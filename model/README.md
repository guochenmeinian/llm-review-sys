## ðŸ“¦ Model Descriptions

This directory contains multiple fine-tuned and DPO-aligned models under two input settings: **full-context** and **sliding-window**. Each setting includes both QLoRA fine-tuned models and DPO-aligned variants with different rejection strategies.

### ðŸ”¹ Full-Context Models

- `full_context_qlora/`  
  Fine-tuned using **QLoRA** on full-paper inputs (~18,000 tokens per sample) using an **H100 GPU** for **2 epochs**.  
  Designed for full-document reasoning tasks where long-range dependency matters.

- `full_context_dpo_base_as_rejected/`  
  Based on `full_context_qlora`, this model was further aligned with **DPO**.  
  - `chosen`: GPT-4 aggregated reviews  
  - `rejected`: Generated by **raw LLaMA3.1**  
  Useful for testing alignment improvements using weak rejection baselines.

- `full_context_dpo_qlora_as_rejected/`  
  Same alignment pipeline, but with `rejected` responses generated by the **QLoRA-finetuned full-context model**, resulting in stronger, more realistic negative samples.  
  This enables evaluation of whether stronger rejections improve DPO preference learning.

---

### ðŸ”¹ Sliding-Window Models

- `sliding_window_qlora/`  
  Fine-tuned with **QLoRA** using **8192-token sliding window segments** (with overlap), trained on a **4090 GPU** for **2 epochs**.  
  Suited for setups where GPU memory is limited.

- `sliding_window_dpo_base_as_rejected/`  
  DPO-aligned version of the sliding window model.  
  - `chosen`: GPT-4 aggregated responses  
  - `rejected`: Generated by **base LLaMA3.1**

---

