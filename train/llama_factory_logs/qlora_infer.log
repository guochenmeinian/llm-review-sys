[INFO|modeling_utils.py:4938] 2025-04-27 03:33:10,958 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/autodl-tmp/LLM-Research/Meta-Llama-3___1-8B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1095] 2025-04-27 03:33:10,961 >> loading configuration file /root/autodl-tmp/LLM-Research/Meta-Llama-3___1-8B-Instruct/generation_config.json
[INFO|configuration_utils.py:1142] 2025-04-27 03:33:10,962 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|2025-04-27 03:33:10] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-04-27 03:33:12] llamafactory.model.adapter:143 >> Merged 1 adapter(s).
[INFO|2025-04-27 03:33:12] llamafactory.model.adapter:143 >> Loaded adapter(s): /root/LLaMA-Factory/saves/openreview/qlora_run_20250427_1
[INFO|2025-04-27 03:33:12] llamafactory.model.loader:143 >> all params: 8,030,261,248
[WARNING|2025-04-27 03:33:12] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.
[INFO|trainer.py:4307] 2025-04-27 03:33:12,999 >> 
***** Running Prediction *****
[INFO|trainer.py:4309] 2025-04-27 03:33:12,999 >>   Num examples = 449
[INFO|trainer.py:4312] 2025-04-27 03:33:12,999 >>   Batch size = 1
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 225/225 [1:31:55<00:00, 24.25s/it]Building prefix dict from the default dictionary ...
Building prefix dict from the default dictionary ...
Dumping model to file cache /tmp/jieba.cache
Dumping model to file cache /tmp/jieba.cache
Loading model cost 0.738 seconds.
Prefix dict has been built successfully.
Loading model cost 0.735 seconds.
Prefix dict has been built successfully.
[INFO|integration_utils.py:831] 2025-04-27 05:06:59,500 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: cg3972 (cg3972-new-york-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.10
wandb: Run data is saved locally in /root/LLaMA-Factory/wandb/run-20250427_050700-g9pvd4d5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run saves/openreview/qlora_eval_20250427_1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/cg3972-new-york-university/llamafactory
wandb: üöÄ View run at https://wandb.ai/cg3972-new-york-university/llamafactory/runs/g9pvd4d5
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 225/225 [1:33:27<00:00, 24.92s/it]
***** predict metrics *****
  predict_bleu-4                 =    52.7685
  predict_model_preparation_time =     0.0051
  predict_rouge-1                =    40.7871
  predict_rouge-2                =    18.2354
  predict_rouge-l                =     24.327
  predict_runtime                = 1:33:46.50
  predict_samples_per_second     =       0.08
  predict_steps_per_second       =       0.04
[INFO|2025-04-27 05:07:02] llamafactory.train.sft.trainer:143 >> Saving prediction results to saves/openreview/qlora_eval_20250427_1/generated_predictions.jsonl
wandb: 
wandb: üöÄ View run saves/openreview/qlora_eval_20250427_1 at: https://wandb.ai/cg3972-new-york-university/llamafactory/runs/g9pvd4d5
wandb: Find logs at: wandb/run-20250427_050700-g9pvd4d5/logs