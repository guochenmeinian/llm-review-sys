model_name_or_path: /root/autodl-tmp/LLM-Research/Meta-Llama-3___1-8B-Instruct

template: llama3
infer_backend: huggingface  # default huggingface or vllm

do_predict: true

eval_dataset: openreview_val
cutoff_len: 128000    # input is very long, use a larger cutoff_len (128k is the default size)
max_new_tokens: 2048
overwrite_cache: true

output_dir: saves/openreview/llama3-8b_vllm_predict_20240507_v1
overwrite_output_dir: true

per_device_eval_batch_size: 1
predict_with_generate: true
