model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
output_dir: ./saved_models/openreview_qlora

# huggingface dataset repo + file paths
hf_dataset_repo: guochenmeinian/openreview_dataset
train_file: llama_factory_qlora.json
eval_file: llama_factory_eval.json

# training config
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 4
num_train_epochs: 3
eval_steps: 100
save_steps: 100
learning_rate: 2e-5

# optional LoRA config
lora_rank: 8
