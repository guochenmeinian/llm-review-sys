import os
import torch
import wandb
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from trl import DPOTrainer, DPOConfig
from transformers import BitsAndBytesConfig  
from peft import PeftModel, prepare_model_for_kbit_training, get_peft_model, LoraConfig 

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# ===== åˆå§‹åŒ– wandb =====
print("ğŸ“¡ åˆå§‹åŒ– wandb...")
wandb.init(
    project="dpo_llama3_project",
    name="qlora_full_context_llama3_vs_dataset"
)

# ===== é…ç½® =====
base_model_path = "meta-llama/Llama-3.1-8B-Instruct"
dataset_repo = "guochenmeinian/openreview_dataset"
dpo_split = "dpo_base"
output_dir = "models/dpo_model_base"

# ===== åŠ è½½ tokenizer =====
print("ğŸ” åŠ è½½ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=False, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# ===== é…ç½® QLoRA é‡åŒ–å‚æ•° =====
print("ğŸ§¬ åŠ è½½ base model + QLoRA adapter...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# ===== åŠ è½½ QLoRA å¾®è°ƒæƒé‡ï¼ˆadapterï¼‰ =====
qlora_model_path = "models/full_context_qlora_model"
model = PeftModel.from_pretrained(base_model, qlora_model_path)
print("âœ… æˆåŠŸåŠ è½½ QLoRA å¾®è°ƒæ¨¡å‹")

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
print("âœ… QLoRA LoRA é…ç½®å®Œæˆ")

# ===== åŠ è½½ DPO æ•°æ®é›† =====
print(f"ğŸ“¦ åŠ è½½æ•°æ®é›† `{dataset_repo}`, split=`{dpo_split}` ...")
dataset = load_dataset(dataset_repo, dpo_split)["train"]
print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(dataset)} æ¡æ ·æœ¬")

# ===== DPO Trainer é…ç½® =====
dpo_config = DPOConfig(
    beta=0.1,  # åå¥½å¼ºåº¦
    max_length=18000,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-6,
    num_train_epochs=3,
    logging_steps=10,
    output_dir=output_dir,
    bf16=True,
    remove_unused_columns=False,
    report_to="wandb",
    run_name="qlora_full_context_llama3_vs_dataset"
)

# ===== åˆ›å»º Trainer =====
# åªä½¿ç”¨å¿…éœ€çš„å‚æ•°
trainer = DPOTrainer(
    model=model,
    eval_dataset=None, # we don't use eval_dataset here
    args=dpo_config,
    train_dataset=dataset,
    ref_model=None, # DPO doesn't require a reference model
    processing_class=tokenizer
)

# ===== è®­ç»ƒ =====
checkpoint_dir = os.path.join(output_dir, "checkpoint-last")
if os.path.isdir(checkpoint_dir):
    print(f"ğŸ”„ å‘ç°å·²æœ‰ checkpointï¼Œå°è¯•ä» {checkpoint_dir} æ¢å¤è®­ç»ƒ...")
    trainer.train(resume_from_checkpoint=True)
else:
    try:
        trainer.train()
    except torch.cuda.OutOfMemoryError as e:
        print("âŒ CUDA OOM! æ‰‹åŠ¨æ¸…ç†ç¼“å­˜ä¸­...")
        torch.cuda.empty_cache()
        raise e

# ===== ä¿å­˜ç»“æœ =====
trainer.model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print("âœ¨ è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³", output_dir)
