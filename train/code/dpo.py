import os
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from trl import DPOTrainer, DPOConfig
from peft import PeftModel
import wandb

# ===== é…ç½® =====
qlora_model_path = "models/full_context_qlora_model"  # æ›¿æ¢ä¸ºä½ çš„ QLoRA æ¨¡å‹è·¯å¾„
dataset_repo = "guochenmeinian/openreview_dataset"
dpo_split = "dpo_base"
output_dir = "models/dpo_model_base"

# ===== åŠ è½½ tokenizer =====
tokenizer = AutoTokenizer.from_pretrained(qlora_model_path, use_fast=False, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# ===== åŠ è½½ QLoRA æ¨¡å‹ =====
base_model = AutoModelForCausalLM.from_pretrained(
    qlora_model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# å¦‚æœä½ ç”¨äº† PEFT å¾®è°ƒï¼ˆLoRAï¼‰ï¼Œå¯è§£æ³¨é‡Šä¸‹é¢è¡Œ
# base_model = PeftModel.from_pretrained(base_model, qlora_model_path)

# ===== åŠ è½½ DPO æ•°æ®é›† =====
dataset = load_dataset(dataset_repo, dpo_split)["train"]

# ===== DPO Trainer é…ç½® =====
dpo_config = DPOConfig(
    beta=0.1,  # åå¥½å¼ºåº¦
    max_length=18000,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-6,
    num_train_epochs=3,
    evaluation_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    logging_steps=10,
    output_dir=output_dir,
    bf16=True,
    remove_unused_columns=False,
    report_to="none",
    report_to="wandb",
    run_name="qlora_full_context_llama3_vs_dataset"
)

# ===== åˆ›å»º Trainer =====
trainer = DPOTrainer(
    model=base_model,
    ref_model=None,  # å¦‚æœä½ æƒ³è®¾ä¸º base æ¨¡å‹çš„ frozen copyï¼Œå¯æŒ‡å®š
    args=dpo_config,
    tokenizer=tokenizer,
    train_dataset=dataset,
    eval_dataset=None  # ä½ å¯ä»¥ç»™ä¸ª eval_setï¼Œä¹Ÿå¯ä»¥çœç•¥
)

# ===== è®­ç»ƒ =====
checkpoint_dir = os.path.join(output_dir, "checkpoint-last")
if os.path.isdir(checkpoint_dir):
    print(f"ğŸ”„ å‘ç°å·²æœ‰ checkpointï¼Œå°è¯•ä» {checkpoint_dir} æ¢å¤è®­ç»ƒ...")
    trainer.train(resume_from_checkpoint=True)
else:
    trainer.train()

# ===== ä¿å­˜ç»“æœ =====
trainer.model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
