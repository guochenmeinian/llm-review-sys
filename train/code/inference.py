import json
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from datasets import load_dataset

# æ¨¡å‹è·¯å¾„
model_name = "meta-llama/Llama-3.1-8B-Instruct"  # æˆ–è€…ä½ çš„å¾®è°ƒæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype=torch.float16)


data = load_dataset("guochenmeinian/openreview_dataset", "dpo")["train"]
print(f"ğŸ“„ æ•°æ®é›†ä¸­å…±æœ‰ {len(data)} ç¯‡è®ºæ–‡æ ·æœ¬")


def run_inference(instruction, input_text, max_input_tokens=128000, max_output_tokens=512):
    # print(tokenizer.__class__)
    # print(tokenizer.pad_token, tokenizer.pad_token_id)
    # print(tokenizer.eos_token, tokenizer.eos_token_id)
    # print(tokenizer.vocab_size)  # å¯¹äº LLaMA3ï¼Œåº”è¯¥æ˜¯ 128256
    # print(tokenizer.special_tokens_map)


    prompt = f"{instruction.strip()}\n\n{input_text.strip()}\n\n### Response:"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=max_input_tokens).to(model.device)

    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_output_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.95,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id
        )
    
    generated_tokens = output_ids[0][inputs['input_ids'].shape[1]:]
    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    return generated_text.strip()

# è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆ.jsonlï¼Œæ¯è¡Œä¸€ä¸ªå®Œæ•´ JSON å¯¹è±¡ï¼‰
output_path = "inference_results_from_dpo_dataset.jsonl"

with open(output_path, "a", encoding="utf-8") as f_out:
    for i, example in enumerate(data):
        print(f"\n===== æ¨ç†ç¬¬ {i+1} æ¡æ ·æœ¬ =====")
        try:
            gen_output = run_inference(example["instruction"], example["input"])
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            gen_output = "ERROR"

        result = {
            "instruction": example["instruction"],
            "input": example["input"],
            "output": example["output"],
            "generated_output": gen_output
        }

        f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
        f_out.flush()  # ğŸ›¡ï¸ å¼ºåˆ¶å†™å…¥ç£ç›˜ï¼Œé˜²æ­¢å¥”æºƒä¸¢å¤±

print(f"\nâœ… æ¨ç†å®Œæˆï¼Œç»“æœå·²å†™å…¥ â†’ {output_path}")