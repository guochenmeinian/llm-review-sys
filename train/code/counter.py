import json
import numpy as np
from transformers import AutoTokenizer
from tqdm import tqdm
from datasets import load_dataset

# ä¿®æ”¹ä¸ºä½ è‡ªå·±çš„è·¯å¾„
model_path = "meta-llama/Llama-3.1-8B-Instruct"

ds = load_dataset("guochenmeinian/openreview_dataset", "eval")["train"]

# åŠ è½½ tokenizerï¼ˆLLaMA ä¸“å±é…ç½®ï¼‰
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)
tokenizer.pad_token = tokenizer.eos_token


print(f"ğŸ“„ åŠ è½½äº† {len(ds)} æ¡æ ·æœ¬")

# ç»Ÿè®¡ token é•¿åº¦
token_lengths = []

for example in tqdm(ds, desc="ğŸ§® Tokenizing"):
    prompt = f"{example['instruction'].strip()}\n\n{example['input'].strip()}"
    response = example['output'].strip()
    full_text = f"{prompt}\n\n### Response:\n{response}"
    token_ids = tokenizer(full_text, add_special_tokens=False)["input_ids"]
    token_lengths.append(len(token_ids))

# è¾“å‡ºç»Ÿè®¡ç»“æœ
token_lengths = np.array(token_lengths)
print("\nğŸ“Š æ ·æœ¬ token é•¿åº¦ç»Ÿè®¡ï¼š")
print(f"æœ€å¤§é•¿åº¦: {token_lengths.max()}")
print(f"æœ€å°é•¿åº¦: {token_lengths.min()}")
print(f"å¹³å‡é•¿åº¦: {token_lengths.mean():.2f}")
print(f"ä¸­ä½æ•°é•¿åº¦: {np.median(token_lengths):.2f}")
print(f"90% æ ·æœ¬ â‰¤ {np.percentile(token_lengths, 90):.0f} tokens")
print(f"95% æ ·æœ¬ â‰¤ {np.percentile(token_lengths, 95):.0f} tokens")
print(f"99% æ ·æœ¬ â‰¤ {np.percentile(token_lengths, 99):.0f} tokens")