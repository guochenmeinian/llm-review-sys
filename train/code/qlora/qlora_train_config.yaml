model_name_or_path: meta-llama/Llama-3.1-8B-Instruct
output_dir: models/dpo-llama3-8b

# huggingface dataset repo + file paths
hf_dataset_repo: guochenmeinian/openreview_dataset

# 数据和模型参数
cutoff_len: 16000

# 训练配置
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 1
num_train_epochs: 3
eval_strategy: steps
eval_steps: 100
save_steps: 100
learning_rate: 2e-5
bf16: true
weight_decay: 0.01
logging_steps: 10
save_total_limit: 2
report_to: none

# LoRA配置
lora_rank: 8
lora_alpha: 32
lora_dropout: 0.05
lora_bias: none

# 量化配置
load_in_4bit: true
bnb_4bit_compute_dtype: bfloat16
bnb_4bit_use_double_quant: true

# wandb
report_to: wandb
wandb_project: llama3_qlora
wandb_run_name: llama3_accelerate_run_1

# deepspeed (optional)
deepspeed: qlora_ds_config.json
