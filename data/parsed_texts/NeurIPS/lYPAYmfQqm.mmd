# Fine-grained Analysis of In-context Linear Estimation:

Data, Architecture, and Beyond

 Yingcong Li

University of Michigan

yingcong@umich.edu

&Ankit Singh Rawat

Google Research NYC

ankitsrawat@google.com

&Samet Oymak

University of Michigan

oymak@umich.edu

###### Abstract

Recent research has shown that Transformers with linear attention are capable of in-context learning (ICL) by implementing a linear estimator through gradient descent steps. However, the existing results on the optimization landscape apply under stylized settings where task and feature vectors are assumed to be IID and the attention weights are fully parameterized. In this work, we develop a stronger characterization of the optimization and generalization landscape of ICL through contributions on architectures, low-rank parameterization, and correlated designs: (1) We study the landscape of 1-layer linear attention and 1-layer H3, a state-space model. Under a suitable correlated design assumption, we prove that both implement 1-step preconditioned gradient descent. We show that thanks to its native convolution filters, H3 also has the advantage of implementing sample weighting and outperforming linear attention in suitable settings. (2) By studying correlated designs, we provide new risk bounds for retrieval augmented generation (RAG) and task-feature alignment which reveal how ICL sample complexity benefits from distributional alignment. (3) We derive the optimal risk for low-rank parameterized attention weights in terms of covariance spectrum. Through this, we also shed light on how LoRA can adapt to a new distribution by capturing the shift between task covariances. Experimental results corroborate our theoretical findings. Overall, this work explores the optimization and risk landscape of ICL in practically meaningful settings and contributes to a more thorough understanding of its mechanics.

Figure 1: We investigate the optimization landscape of in-context learning from the lens of architecture choice, the role of distributional alignment, and low-rank parameterization. The empirical performance (solid curves) are aligned with our theoretical results (dotted curves) from Section 3. More experimental details and discussion are deferred to Section 4.

Introduction

Modern language models exhibit the remarkable ability to learn novel tasks or solve complex problems from the demonstrations provided within their context window (Brown et al., 2020; GeminiTeam et al., 2023; OpenAI, 2023; Touvron et al., 2023). Such _in-context learning_ (ICL) offers a novel and effective alternative to traditional fine-tuning techniques and has become an important feature of LLM with its applications spanning retrieval-augmented generation (Lewis et al., 2020), and reasoning via advanced prompting techniques, such as chain-of-thought (Wei et al., 2022).

ICL ability presents an important research avenue to develop stronger theoretical and mechanistic understanding of large language models. To this aim, there has been significant recent interest in demystifying ICL through the lens of function approximation (Liu et al., 2023a), Bayesian inference (Muller et al., 2021; Xie et al., 2022; Han et al., 2023), and learning and optimization theory (Ahn et al., 2023; Mahankali et al., 2024; Zhang et al., 2024; Duraisamy, 2024). The latter is concerned with understanding the optimization landscape of ICL, which is also crucial for understanding the generalization properties of the model. A notable result in this direction is the observation that linear attention models (Schlag et al., 2021; Von Oswald et al., 2023; Ahn et al., 2023) implement _preconditioned gradient descent_ (PGD) during ICL (Ahn et al., 2023; Mahdavi et al., 2024). While this line of works provide a fresh perspective to ICL, the existing studies do not address many questions arising from real-life applications nor provide guiding principles for various ICL setups motivated by practical considerations.

To this aim, we revisit the theoretical exploration of ICL with linear data model where we feed an in-context prompt containing \(n\) examples \((\bm{x}_{i},y_{i}=\bm{x}_{i}^{T}\bm{\beta}+\xi_{i})_{i=1}^{n}\subset\mathbb{R} ^{d}\times\mathbb{R}\) and a test instance or query \(\bm{x}_{n+1}\in\mathbb{R}^{d}\) to the model, with \(d\) being the feature dimension, \(\bm{\beta}\in\mathbb{R}^{d}\) being the task weight vector, and \((\xi_{i})_{i=1}^{n}\) denoting the noise in individual labels. Given the in-context prompt, the model is tasked to predict \(\hat{y}_{n+1}\) - an estimate for \(y_{n+1}=\bm{x}_{n+1}^{\top}\bm{\beta}+\xi_{n+1}\). We aim to provide answers to the following questions by exploring the loss landscape of ICL:

1. Is the ability to implement gradient-based ICL unique to (linear) attention? Can alternative sequence models implement richer algorithms beyond PGD?
2. In language modeling, ICL often works well with few-shot samples whereas standard linear estimation typically requires \(\mathcal{O}\left(d\right)\) samples. How can we reconcile this discrepancy between classical learning and ICL?
3. To our knowledge, existing works assume linear-attention is fully parameterized, i.e., key and query projections \(\bm{W}_{k},\bm{W}_{q}\in\mathbb{R}^{d\times d}\). What happens when they are low-rank? What happens when there is distribution shift between training and test in-context prompts and we use LoRA (Hu et al., 2022) for adaptation?

In this work, we conduct a careful investigation of these questions. Specifically, we focus on ICL with 1-layer models and make the following contributions:

1. We jointly investigate the landscape of linear attention and H3 (Fu et al., 2023), a widely popular state-space model (SSM). We prove that under correlated design, both models implement 1-step PGD (c.f. Proposition 1) and the alignments in Fig. 0(a) verify that where the dotted curve represents the theoretical PGD result derived from Theorem 1. Our analysis reveals that the gating mechanism in H3 imitates attention. We also empirically show that H3 has the advantage of implementing sample-weighting which allows it to outperform linear attention in temporally-heterogeneous problem settings in Appendix D.
2. Proposition 1 allows for task and features to be correlated to each other as long as odd moments are zero. Through this, we can assess the impact of distributional alignment on the sample complexity of ICL. Specifically, we characterize the performance of _Retrieval Augmented Generation_ (RAG) (c.f. Theorem 2 and Fig. 0(b)) and _Task-Feature Alignment_ (c.f. Theorem 3), where the in-context examples are \(\alpha\)-correlated with either the query or the task vector. For both settings, we prove that alignment amplifies the _effective sample size_ of ICL by a factor of \(\alpha^{2}d+1\), highlighting that aligned data are crucial for the success of ICL in few-shot settings.
3. We show that, under low-rank parameterization, optimal attention-weights still implements PGD according to the truncated eigenspectrum of the fused task-feature covariance (see Section 3.2). We similarly derive risk upper bounds for LoRA adaptation (c.f. Eq. (14) and Fig. 0(c)), and show that, these bounds accurately predict the empirical performance.

Problem Setup and Preliminaries

We begin with a short note on notation. Let bold lowercase and uppercase letters (e.g., \(\bm{x}\) and \(\bm{X}\)) represent vectors and matrices, respectively. The symbol \(\odot\) is defined as the element-wise (Hadamard) product, and \(*\) denotes the convolution operator. \(\bm{1}_{d}\) and \(\bm{0}_{d}\) denote the \(d\)-dimensional all-ones and all-zeros vectors, respectively; and \(\bm{I}_{d}\) denotes the identity matrix of dimension \(d\times d\). Additionally, let \(\bm{\mathrm{tr}}\left(\bm{W}\right)\) denote the trace of the square matrix \(\bm{W}\).

As mentioned earlier, we study the optimization landscapes of 1-layer linear attention (Katharopoulos et al., 2020; Schlag et al., 2021) and H3 (Fu et al., 2023) models when training with prompts containing in-context data following a linear model. We construct the input in-context prompt similar to Ahn et al. (2023); Mahankali et al. (2024); Zhang et al. (2024) as follows.

**Linear data distribution.** Let \((\bm{x},y)\in\mathbb{R}^{d}\times\mathbb{R}\) be a (feature, label) pair generated by a \(d\)-dimensional linear model parameterized by \(\bm{\beta}\in\mathbb{R}^{d}\), i.e., \(y=\bm{x}^{\top}\bm{\beta}+\xi\), where \(\bm{x}\) and \(\bm{\beta}\) are feature and task vectors, and \(\xi\) is the label noise. Given demonstrations \((\bm{x}_{i},y_{i})_{i=1}^{\nu+1}\) sampled from a single \(\bm{\beta}\), define the input in-context prompt

\[\bm{Z}=\begin{bmatrix}\bm{z}_{1}&\dots&\bm{z}_{n}&\bm{z}_{n+1}\end{bmatrix}^{ \top}=\begin{bmatrix}\bm{x}_{1}&\dots&\bm{x}_{n}&\bm{x}_{n+1}\\ y_{1}&\dots&y_{n}&0\end{bmatrix}^{\top}\in\mathbb{R}^{(n+1)\times(d+1)}.\] (1)

Here, we set \(\bm{z}_{i}=\begin{bmatrix}\bm{x}_{i}\\ y_{i}\end{bmatrix}\) for \(i\leq n\) and the last/query token \(\bm{z}_{n+1}=\begin{bmatrix}\bm{x}_{n+1}\\ 0\end{bmatrix}\). Then, given \(\bm{Z}\), the goal of the model is to predict the correct label \(y_{n+1}\) corresponding to \(\bm{x}_{n+1}\). For cleaner notation, when it is clear from context, we drop the subscript \(n+1\) and set \(\bm{x}=\bm{x}_{n+1},~{}\bm{z}=\bm{z}_{n+1}\). Different from the previous work (Ahn et al., 2023; Mahankali et al., 2024; Zhang et al., 2024; Mahdavi et al., 2024) where \((\bm{x}_{i})_{i=1}^{n+1}\) and \(\bm{\beta}\) are assumed to be independent, our analysis focuses on a more general linear setting that captures the dependency between \((\bm{x}_{i})_{i=1}^{n+1}\) and \(\bm{\beta}\).

**Model architectures.** To start with, we first review the architectures of both Transformer and state-space model (SSM). Similar to the previous work (Von Oswald et al., 2023; Ahn et al., 2023; Mahankali et al., 2024; Zhang et al., 2024) and to simplify the model structure, we focus on single-layer models and omit the nonlinearity, e.g., softmax operation and MLP activation, from the Transformer. Given the input prompt \(\bm{Z}\in\mathbb{R}^{(n+1)\times(d+1)}\) in (1), which can be treated as a sequence of \((d+1)\)-dimensional tokens, the single-layer linear attention ATT and H3-like single-layer SSM SSM are denoted by

\[\texttt{ATT}(\bm{Z}) =(\bm{Z}\bm{W}_{q}\bm{W}_{k}^{\top}\bm{Z}^{\top})\bm{Z}\bm{W}_{\nu}\] (2a) \[\texttt{SSM}(\bm{Z}) =\Big{(}(\bm{Z}\bm{W}_{q})\odot\big{(}(\bm{Z}\bm{W}_{k}\odot\bm{ Z}\bm{W}_{\nu})*\bm{f}\big{)}\Big{)}\] (2b)

where \(\bm{W}_{k},~{}\bm{W}_{q},~{}\bm{W}_{\nu}\in\mathbb{R}^{(d+1)\times(d+1)}\) denote the key, query and value weight matrices, respectively. In (2b), the parameter \(\bm{f}\in\mathbb{R}^{n+1}\) is a 1-D convolutional filter that mixes tokens. The Hadamard product \(\odot\) is the gating mechanism (Dauphin et al., 2017) between key and query channels, which is crucial for attention-like feature creation. Thus, (2b) is more generally a gated-convolution layer. For \(\bm{f}\) only, we use indexing \(\bm{f}=[f_{0}~{}\dots~{}f_{n}]^{\top}\in\mathbb{R}^{n+1}\) and given any vector \(\bm{a}\), denote convolution output \((\bm{a}*\bm{f})_{i}=\sum_{j=1}^{i}f_{i-j}a_{j}\). Note that our notation slightly differs from the original H3 model (Fu et al., 2023) in two ways:

1. SSMs provide efficient parameterization of \(\bm{f}\) which would otherwise grow with sequence length. In essence, H3 utilizes a linear state-space model \(\bm{s}_{i}=\bm{A}\bm{s}_{i-1}+\bm{B}\bm{u}_{i}\) and \(y_{i}=\bm{C}\bm{s}_{i}\) with parameters (\(\bm{A}\in\mathbb{R}^{d\times d}\), \(\bm{B}\in\mathbb{R}^{d\times 1},\bm{C}\in\mathbb{R}^{1\times d}\)) from which the filter \(\bm{f}\) is obtained via the impulse response \(f_{i}=\bm{C}\bm{A}^{i}\bm{B}\) for \(i\geq 0\). Here \(d\) is the state dimension and, in practice, \(\bm{A}\) is chosen to be diagonal. Observe that, setting \(d=1\) and \(\bm{A}=\rho,\bm{C}=\bm{B}=1\), SSM reduces to the exponential smoothing \(f_{i}=\rho^{i}\) for \(i\geq 0\). Thus, H3 also captures the all-ones filter as a special instance. As we show in Proposition 1, this simple filter is optimal under independent data model and exactly imitates linear attention. Note that, utilizing a filter \(\bm{f}\) as in (2b) is strictly more expressive than the SSM as it captures all possible impulse responses.
2. H3 also applies a shift SSM to the key embeddings to enable the retrieval of the local context around associative recall hits. We opted not to incorporate this shift operator in our model. This is because unless the features of the neighboring tokens are correlated (which is not the case for the typical independent data model), the entry-wise products between values and shifted keys will have zero mean and be redundant for the final prediction.

We note that we conduct all empirical evaluations with the original H3 model, which displays exact agreement with our theory formalized for (6b), further validating our modeling choice.

### In-context Linear Estimation

We will next study the algorithms that can be implemented by the single-layer attention and state-space models. Through this, we will show that training ATT and SSM with linear ICL data is equivalent to the prediction obtained from one step of optimally-_preconditioned gradient descent_ (PGD) and _sample-weighted preconditioned gradient descent_ (WPGD), respectively. We will further show that under mild assumption, the optimal sample weighting for SSM (e.g., \(\bm{f}\)) is an all-ones vector and therefore, establishing the equivalence among PGD, ATT, and SSM.

**Background: 1-step gradient descent.** Consider minimizing squared loss and solving linear regression using one step of PGD and WPGD. Given \(n\) samples \((\bm{x}_{i},y_{i})_{i=1}^{n}\), define

\[\bm{X}=[\bm{x}_{1}\ \cdots\ \bm{x}_{n}]^{\top}\in\mathbb{R}^{n\times d} \quad\text{and}\quad\bm{y}=[y_{1}\ \cdots\ y_{n}]^{\top}\in\mathbb{R}^{n}.\]

Starting from \(\bm{\beta}_{0}=\bm{0}_{d}\) and letting \(\eta=1/2\) be the step size, a single-step GD preconditioned with weights \(\bm{W}\) returns prediction

\[\hat{y}=\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{y}:=g_{\text{PGD}}(\bm{Z}),\] (3)

and a single-step _sample-weighted_ GD given weights \(\bm{\omega}\in\mathbb{R}^{n}\) and \(\bm{W}\in\mathbb{R}^{d\times d}\) returns prediction

\[\hat{y}=\bm{x}^{\top}\bm{W}\bm{X}^{\top}(\bm{\omega}\odot\bm{y}):=g_{\text{ WPGD}}(\bm{Z}),\] (4)

where \(\bm{Z}\) is defined in (1) consisting of \(\bm{X},\bm{y}\) and \(\bm{x}\). Our goal is to find the optimal \(\bm{W}\), as well as \(\bm{\omega}\) in (4) that minimize the population risks defined as follows.

\[\min_{\bm{W}}\mathcal{L}_{\text{PGD}}(\mathcal{W}) \quad\text{where}\quad\mathcal{L}_{\text{PGD}}(\mathcal{W})= \mathbb{E}\left[(y-g_{\text{PGD}}(\bm{Z}))^{2}\right],\] (5a) \[\min_{\bm{W},\bm{\omega}}\mathcal{L}_{\text{WPGD}}(\mathcal{W}) \quad\text{where}\quad\mathcal{L}_{\text{WPGD}}(\mathcal{W})= \mathbb{E}\left[(y-g_{\text{WPGD}}(\bm{Z}))^{2}\right].\] (5b)

Here, the expectation is over the randomness in \((\bm{x}_{i},\xi_{i})_{i=1}^{n+1}\) and \(\bm{\beta}\), and we use \(\mathcal{W}\) to represent the set of corresponding trainable parameters. The search spaces for \(\bm{\omega}\) and \(\bm{W}\) are \(\mathbb{R}^{n}\) and \(\mathbb{R}^{d\times d}\), respectively.

As per (2), given input prompt \(\bm{Z}\in\mathbb{R}^{(n+1)\times(d+1)}\), either of the underlying models outputs a \((n+1)\)-length sequence. Note that the label for the query \(\bm{x}=\bm{x}_{n+1}\) is excluded from the prompt \(\bm{Z}\). Similar to Ahn et al. (2023); Mahankali et al. (2024), we consider a training objective with a causal mask to ensure inputs cannot attend to their own labels and training can be parallelized. Let \(\bm{Z}_{0}=[\bm{z}_{1}\ \ldots\ \bm{z}_{n}\ 0]^{\top}\) be the features post-causal masking at time/index \(n+1\). Given weights \(\bm{W}_{k},\bm{W}_{q},\bm{W}_{v}\) and the filter \(\bm{f}\) for SSM, predictions at the query token \(\bm{z}=\begin{bmatrix}\bm{x}\\ 0\end{bmatrix}\) take the following forms following sequence-to-sequence mappings in (2):

\[g_{\text{ATT}}(\bm{Z}) =(\bm{z}^{\top}\bm{W}_{q}\bm{W}_{k}^{\top}\bm{Z}_{0}^{\top})\bm{Z }_{0}\bm{W}_{v}\bm{v},\] \[g_{\text{SSM}}(\bm{Z}) =\left((\bm{z}^{\top}\bm{W}_{q})^{\top}\odot((\bm{Z}_{0}\bm{W}_{ k}\odot\bm{Z}_{0}\bm{W}_{v})*\bm{f})_{n+1}\right)\bm{v},\]

where \(\bm{v}\in\mathbb{R}^{d+1}\) is the linear prediction head and \(((\bm{Z}_{0}\bm{W}_{k}\odot\bm{Z}_{0}\bm{W}_{v})*\bm{f})_{n+1}\) returns the last row of the convolution output. Note that SSM can implement the mask by setting \(f_{0}=0\). Now consider the meta learning setting and select loss function to be the squared loss, same as in (5). Thus, the objectives for both models take the following forms.

\[\min_{\bm{W}_{i},\bm{W}_{v},\bm{W}_{v},\bm{x}}\mathcal{L}_{\text{ ATT}}(\mathcal{W}) \quad\text{where}\quad\mathcal{L}_{\text{ATT}}(\mathcal{W})=\mathbb{E}\left[(y-g_ {\text{ATT}}(\bm{Z}))^{2}\right],\] (6a) \[\min_{\bm{W}_{i},\bm{W}_{i},\bm{W}_{i},\bm{x},\bm{f}}\mathcal{L}_{ \text{SSM}}(\mathcal{W})\quad\text{where}\quad\mathcal{L}_{\text{SSM}}( \mathcal{W})=\mathbb{E}\left[(y-g_{\text{SSM}}(\bm{Z}))^{2}\right].\] (6b)

Here, similarly, the expectation subsumes the randomness of \((\bm{x}_{i},\xi_{i})_{i=1}^{n+1}\) and \(\bm{\beta}\) and \(\mathcal{W}\) represents the set of trainable parameters. The search space for matrices \(\bm{W}_{k}\), \(\bm{W}_{q}\), \(\bm{W}_{v}\) is \(\mathbb{R}^{(d+1)\times(d+1)}\), for head \(\bm{v}\) is \(\mathbb{R}^{d+1}\), and for \(\bm{f}\) is \(\mathbb{R}^{n+1}\).

Note that for all the optimization methods (c.f. (5), (6)), to simplify the analysis, we train the models without capturing additional bias terms. Therefore, in the following, we introduce the centralized data assumptions such that the models are trained to make unbiased predictions.

To begin with, a cross moment of random variables is defined as the expectation of a monomial of these variables, with the order of the cross moment being the same as order of the monomial. For example, \(\mathbb{E}[\bm{x}^{\top}\bm{W}\bm{\beta}]\) is a sum of cross-moments of order 2. Then, it motivates the following data assumptions.

**Assumption 1**: _All cross moments of the entries of \((\bm{x}_{i})_{i=1}^{n+1}\) and \(\bm{\beta}\) with odd orders are zero._

**Assumption 2**: _The label noise \((\xi_{i})_{i=1}^{n+1}\) are independent of \((\bm{x}_{i})_{i=1}^{n+1}\) and \(\bm{\beta}\), and their cross moments with odd orders are zero._

Note that compared to Ahn et al. (2023); Mahankali et al. (2024); Zhang et al. (2024), Assumption 1 is more general which also subsumes the dependent distribution settings. In this work, we consider the following three linear models (omitting noise) satisfying Assumption 1. Let \(\bm{\Sigma}_{\bm{\beta}},\bm{\Sigma}_{\bm{x}}\in\mathbb{R}^{d\times d}\) represent the task and feature covariance matrices for independent data, and let \(0\leq\alpha\leq 1\) be the correlation level when considering data dependency. More specific discussions are deferred to Section 3.

* Independent task and data: \(\bm{\beta}\sim\mathcal{N}(0,\bm{\Sigma}_{\bm{\beta}}),\ \bm{x}_{i}\sim \mathcal{N}(0,\bm{\Sigma}_{\bm{x}}),\ \ \text{for all}\ \ 1\leq i\leq n+1\).
* Retrieval augmented generation: \(\bm{\beta},\bm{x}\sim\mathcal{N}(0,\bm{I}_{d}),\ \bm{x}_{i}\ \big{|}\ \bm{x}\sim\mathcal{N}(\alpha\bm{x},(1-\alpha^{2})\bm{I}_{d}),\ \ \text{for all}\ \ 1\leq i\leq n\).
* Task-feature alignment: \(\bm{\beta}\sim\mathcal{N}(0,\bm{I}_{d}),\ \bm{x}_{i}\ \big{|}\ \bm{\beta}\sim\mathcal{N}(\alpha\bm{\beta},\bm{I}_{d}),\ \ \text{for all}\ \ 1\leq i\leq n+1\).

Next, we introduce the following result which establishes the equivalence among optimizing 1-layer linear attention (c.f. (6a)), 1-layer H3 (c.f. (6b)), and 1-step gradient descent (c.f. (5)).

**Proposition 1**: _Suppose Assumptions 1 and 2 hold. Consider the objectives as defined in (5) and (6), and let \(\mathcal{L}_{\mathsf{PGD}}^{\star},\ \mathcal{L}_{\mathsf{HPGD}}^{\star},\ \mathcal{L}_{ \mathsf{ATT}}^{\star}\), and \(\mathcal{L}_{\mathsf{SSR}}^{\star}\) be their optimal risks, respectively. Then,_

\[\mathcal{L}_{\mathsf{PGD}}^{\star}=\mathcal{L}_{\mathsf{ATT}}^{\star}\quad \text{and}\quad\mathcal{L}_{\mathsf{HPGD}}^{\star}=\mathcal{L}_{\mathsf{SSR}}^ {\star}.\]

_Additionally, if the examples \((\bm{x}_{i},y_{i})_{i=1}^{n}\) follow the same distribution and are conditionally independent given \(\bm{x},\bm{\beta}\), then SSM/H3 can achieve the optimal loss using the all-ones filter and \(\mathcal{L}_{\mathsf{PGD}}^{\star}=\mathcal{L}_{\mathsf{SSR}}^{\star}\)._

We defer the proof to Appendix A.1. Proposition 1 establishes that analyzing the optimization landscape of ICL for both single-layer linear attention and the H3 model can be effectively reduced to examining the behavior of a one-step PGD algorithm. Notably, under the independent, RAG and task-feature alignment data settings discussed above, examples \((\bm{x}_{i},y_{i})_{i=1}^{n}\) are independently sampled given \(\bm{x}\) and \(\bm{\beta}\), and we therefore conclude that \(\mathcal{L}_{\mathsf{PGD}}^{\star}=\mathcal{L}_{\mathsf{ATT}}^{\star}= \mathcal{L}_{\mathsf{SSR}}^{\star}\). Leveraging this result, the subsequent section of the paper concentrate on addressing (5a), taking into account various linear data distributions.

While Proposition 1 demonstrates the equivalence of optimal losses, we also study the uniqueness and equivalence of optimal prediction functions. To this end, we analyze the strong convexity of \(\mathcal{L}_{\mathsf{PGD}}(\mathcal{W})\) and derive the subsequent lemmas.

**Lemma 1**: _Suppose Assumption 2 holds and let \(\bm{\xi}=[\xi_{1}\ \xi_{2}\ \cdots\ \xi_{n}]^{\top}\). Then the loss \(\mathcal{L}_{\mathsf{PGD}}(\mathcal{W})\) in (5a) is strongly-convex if and only if \(\mathbb{E}[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{X}\bm{\beta})^{2}]+\mathbb{E }[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})^{2}]\) is strongly-convex. Additionally, let \(g_{\mathsf{PGD}}^{\star}\ g_{\mathsf{ATT}}^{\star}\) be the optimal prediction functions of (5a) and (6a). Then under the conditions of Assumptions 1 and 2, and the strong convexity, \(g_{\mathsf{PGD}}^{\star}=g_{\mathsf{ATT}}^{\star}\)._

**Lemma 2**: _Suppose that the label noise \((\xi_{i})_{i=1}^{n}\) are i.i.d., zero-mean, variance \(\sigma^{2}\) and independent of everything else, and that there is a decomposition \(\bm{x}=\bm{x}_{1}+\bm{x}_{2}\), \(\bm{X}=\bm{X}_{1}+\bm{X}_{2}\), and \(\bm{\beta}=\bm{\beta}_{1}+\bm{\beta}_{2}\) such that either of the following holds_

* \(\sigma>0\)_, and_ \((\bm{x}_{1},\bm{X}_{1})\) _have full rank covariance and are independent of each other and_ \((\bm{x}_{2},\bm{X}_{2})\)_._
* \((\bm{x}_{1},\bm{\beta}_{1},\bm{X}_{1})\) _have full rank covariance and are independent of each other and_ \((\bm{x}_{2},\bm{\beta}_{2},\bm{X}_{2})\)_._

_Then, the loss \(\mathcal{L}_{\mathsf{PGD}}(\mathcal{W})\) in (5a) is strongly-convex._As mentioned above, in this work, we study three specific linear models: with general independent, RAG-related, and task-feature alignment data. Note that for all the three cases, according to Proposition 1, we have \(\mathcal{L}_{\texttt{PGD}}^{\star}=\mathcal{L}_{\texttt{ATT}}^{\star}=\mathcal{ L}_{\texttt{SSM}}^{\star}\). Additionally, the second claim in Lemma 2 holds, and \(\mathcal{L}_{\texttt{PGD}}(\mathcal{W})\) is strongly convex. Therefore, following Lemma 1, we have \(g_{\texttt{PGD}}^{\star}=g_{\texttt{ATT}}^{\star}\). Thanks to the equivalence among PGD, ATT, and SSM, in the next section, we focus on the solution of objective (5a) under different scenarios, which will reflect the optimization landscapes of ATT and SSM models.

## 3 Main Results

In light of Proposition 1, optimizing a single layer linear-attention or H3 model is equivalent to solving the objective (5a). Therefore, in this section, we examine the properties of the one-step PGD in (5a). To this end, we consider multiple problem settings, including distinct data distributions and low-rank training. The latter refers to the scenario where the key and query matrices have rank restrictions, e.g., \(\bm{W}_{k},\bm{W}_{q}\in\mathbb{R}^{(d+1)\times r}\), as well as LoRA-tuning when adapting the model under distribution shift.

### Analysis of Linear Data Models

We first consider the standard independent data setting. We will then examine correlated designs.

**Independent data model.** Let \(\bm{\Sigma}_{\bm{x}}\) and \(\bm{\Sigma}_{\bm{\beta}}\) be the covariance matrices of the input feature and task vectors, respectively, and \(\sigma\geq 0\) be the noise level. We assume

\[\bm{\beta}\sim\mathcal{N}(0,\bm{\Sigma}_{\bm{\beta}}),\quad\bm{x}_{i}\sim \mathcal{N}(0,\bm{\Sigma}_{\bm{x}}),\quad\xi_{i}\sim\mathcal{N}(0,\sigma^{2}),\quad 1\leq i\leq n+1\] (7)

and the label is obtained via \(y_{i}=\bm{x}_{i}^{\top}\bm{\beta}+\xi_{i}\). Our following result characterizes the optimal solution of (5a). Note that the data generated from (7) satisfies the conditions in Proposition 1. Therefore, the same results can be applied to both linear-attention and H3 models.

**Theorem 1**: _Consider independent linear data as defined in (7), and suppose the covariance matrices \(\bm{\Sigma}_{\bm{x}},\bm{\Sigma}_{\bm{\beta}}\) are full rank. Recap the objective from (5a) and let \(\bm{W}_{\star}:=\arg\min_{\bm{W}}\mathcal{L}_{\texttt{PGD}}(\bm{W})\), and \(\mathcal{L}_{\star}=\mathcal{L}_{\texttt{PGD}}(\bm{W}_{\star})\). Additionally, let \(\bm{\Sigma}=\bm{\Sigma}_{\bm{x}}^{1/2}\bm{\Sigma}_{\bm{\beta}}\bm{\Sigma}_{\bm {x}}^{1/2}\) and \(M=\operatorname{\bm{tr}}(\bm{\Sigma})+\sigma^{2}\). Then \(\bm{W}_{\star}\) and \(\mathcal{L}_{\star}\) satisfy_

\[\bm{W}_{\star}=\bm{\Sigma}_{\bm{x}}^{-1/2}\tilde{\bm{W}}_{\star}\bm{\Sigma}_{ \bm{x}}^{-1/2}\qquad\text{and}\qquad\mathcal{L}_{\star}=M-n\operatorname{\bm{ tr}}\left(\bm{\Sigma}\tilde{\bm{W}}_{\star}\right),\] (8)

_where we define \(\tilde{\bm{W}}_{\star}=\left((n+1)\bm{I}_{d}+M\bm{\Sigma}^{-1}\right)^{-1}\)._

**Corollary 1**: _Consider noiseless i.i.d. linear data where \(\bm{\Sigma}_{\bm{x}}=\bm{\Sigma}_{\bm{\beta}}=\bm{I}_{d}\) and \(\sigma=0\). Then, the objective in (5a) returns_

\[\bm{W}_{\star}=\frac{1}{n+d+1}\bm{I}_{d}\qquad\text{and}\qquad\mathcal{L}_{ \star}=d-\frac{nd}{n+d+1}.\]

See Appendix B.2 for proofs. Note that Theorem 1 is consistent with prior work (Ahn et al., 2023, Theorem 1) when specialized to isotropic task covariance, i.e., \(\bm{\Sigma}_{\bm{\beta}}=\bm{I}_{d}\). However, their result is limited as the features and task are assumed to be independent. This prompts us to ask: _What is the optimization landscape with correlated in-context samples?_ Toward this, we consider the following RAG-inspired and task-feature alignment models, where Assumptions 1 and 2 continue to hold and Proposition 1 applies.

**Retrieval augmented generation.** To provide a statistical model of the practical RAG approaches, given the query vector \(\bm{x}_{n+1}=\bm{x}\), we propose to draw ICL demonstrations that are similar to \(\bm{x}\) with the same shared task vector \(\bm{\beta}\). Modeling feature similarity through the cosine angle, RAG should sample the ICL examples \(\bm{x}_{i}\), \(i\leq n\), from the original feature distribution conditioned on the event \(\cos(\bm{x}_{i},\bm{x})\geq\alpha\) where \(\alpha\) is the similarity threshold. As an approximate proxy, under the Gaussian distribution model, we assume that \(\bm{\beta}\sim\mathcal{N}(0,\bm{I}_{d})\), \(\bm{x}\sim\mathcal{N}(0,\bm{I}_{d})\) and that RAG samples \(\alpha\)-correlated demonstrations \((\bm{x}_{i},y_{i})_{i=1}^{n}\) as follows:

\[\bm{x}_{i}\left|\,\bm{x}\sim\mathcal{N}(\alpha\bm{x},(1-\alpha^{2})\bm{I}_{d}),\quad\xi_{i}\sim\mathcal{N}(0,\sigma^{2})\quad\text{and}\quad y_{i}=\bm{x}_{i }^{\top}\bm{\beta}+\xi_{i},\quad 1\leq i\leq n.\] (9)

Note that the above normalization ensures that the marginal feature distribution remains \(\mathcal{N}(0,\bm{I}_{d})\). The full analysis of RAG is provides in Appendix B.3. Specifically, when we carry out the analysis by assuming \(\alpha=\mathcal{O}\left(1/\sqrt{d}\right)\) and \(d/n=\mathcal{O}(1)\) where \(\mathcal{O}(\cdot)\) denotes proportionality, our derivation leads to the following result:

**Theorem 2**: _Consider linear model as defined in (9). Recap the objective from (5a) and let \(\bm{W}_{\star}:=\arg\min_{\bm{W}}\mathcal{L}_{\mathcal{P}\mathcal{O}}(\bm{W})\), and \(\mathcal{L}_{\star}=\mathcal{L}_{\mathcal{P}\mathcal{O}}(\bm{W}_{\star})\). Additionally, let \(\kappa=\alpha^{2}d+1\) and suppose \(\alpha=\mathcal{O}\left(1/\sqrt{d}\right)\), \(d/n=\mathcal{O}\left(1\right)\) and \(d\) is sufficiently large. Then \(\bm{W}_{\star}\) and \(\mathcal{L}_{\star}\) have approximate forms_

\[\bm{W}_{\star}\approx\frac{1}{\kappa n+d+\sigma^{2}}\bm{I}_{d}\qquad\text{and }\qquad\mathcal{L}_{\star}\approx d+\sigma^{2}-\frac{\kappa nd}{\kappa n+d+ \sigma^{2}}.\] (10)

Here, (10) is reminiscent of Corollary 1 and has a surprisingly clean message. Observe that, \(\alpha^{2}d+1\) is the dominant multiplier ahead of \(n\) in both equations. Thus, we deduce that, RAG model follows the same error bound as the independent data model, however, its sample size is amplified by a factor of \(\alpha^{2}d+1\). \(\alpha=0\) reduces to the result of Corollary 1 whereas we need to set \(\alpha=\mathcal{O}\left(1/\sqrt{d}\right)\) for constant amplification. When \(\alpha=1\), RAG achieves the approximate risk \(\mathcal{L}_{\star}\approx 2+\sigma^{2}\), where the constant bias is due to the higher order moments (e.g., the 4'th and 6'th moments) of the standard Gaussian distribution. As \(d\) increases, the normalized loss \(\mathcal{L}_{\star}/d\to 0\). The full analysis of its optimal solution \(\bm{W}_{\star}\) and loss \(\mathcal{L}_{\star}\) are deferred Theorem 4 in Appendix B.3.

**Task-feature alignment**. We also consider another dependent data setting where task and feature vectors are assumed to be correlated. This dataset model has the following motivation: In general, an LLM can generate any token within the vocabulary. However, once we specify the task (e.g. domain of the prompt), the LLM output becomes more deterministic and there are much fewer token candidates. For instance, if the task is "Country", "France" is a viable output compared to "Helium" and vice versa when the task is "Chemistry". Formally speaking, this can be formalized as the input \(\bm{x}\) having a diverse distribution whereas it becomes more predictable conditioned on \(\bm{\beta}\). Therefore, it can be captured through a linear model by making the conditional covariance of \(\bm{x}\left|\bm{\beta}\right.\) to be approximately low-rank. This formalism can be viewed as a _spectral alignment_ between input and task, which is also well-established in deep learning both empirically and theoretically (Li et al., 2020; Arora et al., 2019; Canatar et al., 2021; Cao et al., 2019). Here, we consider such a setting where the shared task vector is sampled as standard Gaussian distribution \(\bm{\beta}\sim\mathcal{N}(0,\bm{I}_{d})\) and letting \(\kappa=\alpha^{2}d+1\), we sample the \(\alpha\)-correlated ICL demonstrations \((\bm{x}_{i},y_{i})_{i=1}^{n+1}\) as follows:

\[\bm{x}_{i}\left|\bm{\beta}\sim\mathcal{N}(\alpha\bm{\beta},\bm{I}_{d}),\quad \xi_{i}\sim\mathcal{N}(0,\sigma^{2})\quad\text{and}\quad y_{i}=\kappa^{-1/2} \bm{x}_{i}^{\top}\bm{\beta}+\xi_{i},\quad 1\leq i\leq n+1.\] (11)

Above, \(\kappa^{-1/2}\) is a normalization factor to ensure that label variance remains invariant to \(\alpha\). To keep the exposition cleaner, we defer the full analysis of its optimal solution \(\bm{W}_{\star}\) and loss \(\mathcal{L}_{\star}\) to Theorem 5 in Appendix B.4. Similar to the RAG setting, by assuming \(\alpha=\mathcal{O}\left(1/\sqrt{d}\right)\) and \(d/n=\mathcal{O}\left(1\right)\), we obtain the following results for the optimal parameter and risk.

**Theorem 3**: _Consider linear model as defined in (11). Recap the objective from (5a) and let \(\bm{W}_{\star}:=\arg\min_{\bm{W}}\mathcal{L}_{\mathcal{P}\mathcal{O}}(\bm{W})\), and \(\mathcal{L}_{\star}=\mathcal{L}_{\mathcal{P}\mathcal{O}}(\bm{W}_{\star})\). Additionally, given \(\kappa=\alpha^{2}d+1\) and suppose \(\alpha=\mathcal{O}\left(1/\sqrt{d}\right)\), \(d/n=\mathcal{O}\left(1\right)\) and \(d\) is sufficiently large. Then \(\bm{W}_{\star}\) and \(\mathcal{L}_{\star}\) have approximate forms_

\[\bm{W}_{\star}\approx\frac{1}{\kappa n+(d+\sigma^{2})/\kappa}\bm{I}_{d}\qquad \text{and}\qquad\mathcal{L}_{\star}\approx d+\sigma^{2}-\frac{\kappa nd}{\kappa n +(d+\sigma^{2})/\kappa}.\] (12)

Similar to (10), (12) contains \(\kappa=\alpha^{2}+1\) multiplier ahead of \(n\), which reduces the in-context sample complexity and setting \(\alpha=0\) reduces to the results of Corollary 1.

### Low-rank Parameterization and LoRA

In this section, we investigate training low-rank models, which assume \(\bm{W}_{k},\bm{W}_{q}\in\mathbb{R}^{(d+1)\times\nu}\) where \(r\) is the rank restriction. Equivalently, we consider objective (5a) under condition \(\text{rank}\left(\bm{W}\right)=r\).

**Lemma 3**: _Consider independent linear data as defined in (7). Recap the objective from (5a) and enforce rank \((\bm{W})\leq r\) and \(\bm{W}^{\top}=\bm{W}\). Let \(\bm{\Sigma}=\bm{\Sigma}_{\bm{x}}^{1/2}\bm{\Sigma}_{\bm{\beta}}\bm{\Sigma}_{\bm{ x}}^{1/2}\) and \(M=\texttt{tr}\left(\bm{\Sigma}\right)+\sigma^{2}\). Denoting \(\lambda_{i}\) to be the \(i\)'th largest eigenvalue of \(\bm{\Sigma}\), we have that_

\[\min_{rank(\bm{W})\leq r,\bm{W}=\bm{W}^{\top}}\mathcal{L}(\bm{W})=M-\sum_{i=1} ^{r}\frac{n\lambda_{i}^{2}}{(n+1)\lambda_{i}+M}.\] (13)Note that \(\mathtt{tr}\left(\bm{\Sigma}\right)=\sum_{i=1}^{d}\lambda_{i}\). Removing the rank constraint and considering noiseless data setting, this reduces to the following optimal risk \(\mathcal{L}_{\star}=\sum_{i=1}^{d}\frac{\lambda_{i}+M}{n+1+M/\lambda_{i}}\). See Appendix C.1 for more details.

**Impact of LoRA:** Based on the above lemma, we consider the impact of LoRA for adapting the pretrained model to a new task distribution under jointly-diagonalizable old and new eigenvalues of \(\bm{\Sigma},~{}\bm{\Sigma}^{new},(\lambda_{i})_{i=1}^{d},(\lambda_{i}^{new})_ {i=1}^{d}\). Consider adapting LoRA matrix to the combined key and value weights in attention, which reflects minimizing the population loss \(\tilde{\mathcal{L}}(\bm{W}_{lora}):=\mathcal{L}(\bm{W}+\bm{W}_{lora})\) in (5a) with fixed \(\bm{W}\). Suppose \(\mathtt{tr}\left(\bm{\Sigma}\right)=\mathtt{tr}\left(\bm{\Sigma}^{new}\right)=M\), \(\sigma=0\) and \(\bm{W}\) is jointly diagonalizable with \(\bm{\Sigma},~{}\bm{\Sigma}^{new}\), then LoRA's risk is upper-bounded by

\[\min_{\mathtt{rank}(\bm{W}_{lora})\leq\tilde{\mathcal{L}}(\bm{W}_{lora})}\leq\min_{|I|\leq,\mathcal{I}\in[d]} \left(\sum_{i\notin I}\frac{\lambda_{i}+M}{n+1+M/\lambda_{i}}+\sum_{i\in I} \frac{\lambda_{i}^{new}+M}{n+1+M/\lambda_{i}^{new}}\right).\] (14)

Note that, the right hand side is provided assuming the optimal LoRA-updated model \(\bm{W}_{lora}\) is also jointly diagonalizable with covariances \(\bm{\Sigma},~{}\bm{\Sigma}^{new}\), and \(\bm{W}\).

## 4 Experiments

We now conduct synthetic experiments to support our theoretical findings and further explore the behavior of different models of interest under different conditions. The experiments are designed to investigate various scenarios, including independent data, retrieval-augmented generation (RAG), task-feature alignment, low-rank parameterization, and LoRA adaption.

**Experimental setting.** We train 1-layer attention and H3 models for solving the linear regression ICL. As described in Section 2, we consider meta-learning setting where task parameter \(\bm{\beta}\) is randomly generated for each training sequence. In all experiments, we set the dimension \(d=20\). Depending on the in-context length (\(n\)), different models are trained to make in-context predictions. We train each model for 10000 iterations with batch size 128 and Adam optimizer with learning rate \(10^{-3}\). Since our study focuses on the optimization landscape, and experiments are implemented via gradient descent, we repeat 20 model trainings from different initialization and results are presented as the minimal test risk among those 20 trails. In all the plots, theoretical predictions are obtained via the corresponding formulae presented in Section 3 and the test risks are normalized by the dimension \(d\).

\(\bullet\)**Equivalence among \(\mathcal{L}_{\mathtt{PGD}}^{\star}\), \(\mathcal{L}_{\mathtt{ATT}}^{\star}\) and \(\mathcal{L}_{\mathtt{SSR}}^{\star}\) (Figure 2).** To verify Proposition 1 as well as Theorem 1, we run random linear regression instances where in-context samples are generated obeying (7). Fig. 1(a) is identical to Fig. 1(a) where we set \(\bm{\Sigma}_{\bm{x}}=\bm{\Sigma}_{\bm{\beta}}=\bm{I}_{d}\) and \(\sigma=0\). In Fig. 1(b), set \(\bm{\Sigma}_{\bm{x}}=\bm{\Sigma}_{\bm{\beta}}=\bm{I}\) and vary noise level \(\sigma^{2}\) from \(0\) to \(0.3\times d\). In Fig. 1(c), we consider noiseless labels, \(\sigma=0\), isotropic feature distribution \(\bm{\Sigma}_{\bm{x}}=\bm{I}_{d}\) and set task covariance to be \(\bm{\Sigma}_{\bm{\beta}}=\gamma\bm{1}\bm{1}^{\top}+(1-\gamma)\bm{I}_{d}\) by choosing \(\gamma\) in \(\{0,0.3,0.6,0.9\}\). Note that in Fig. 1(c), we train a sufficient number of models (greater than 20) to ensure the optimal model is obtained. In all the figures, solid and dashed curves correspond to the

Figure 2: Empirical evidence validates Theorem 1 and Proposition 1. We train 1-layer linear attention and H3 models with prompts containing independent demonstrations following a linear model, and dotted curves are the theory curves following Eq. (8). **(a):** We consider noiseless i.i.d. setting where \(\bm{\Sigma}_{\bm{x}}=\bm{\Sigma}_{\bm{\beta}}=\bm{I}_{d}\) and \(\sigma=0\), with results presented in red (attention) and blue (H3) solid curves. **(b):** We conduct noisy label experiments by choosing \(\sigma\neq 0\). **(c):** Consider non-isotropic task by setting \(\bm{\Sigma}_{\bm{\beta}}=\gamma\bm{1}\bm{1}^{\top}+(1-\gamma)\bm{I}_{d}\). Solid and dashed curves in (b) and (c) represent attention and H3 results, respectively. The alignments in (a), (b) and (c) show the equivalence between attention and H3, validating Theorem 1 and Proposition 1. More experimental details are discussed in Section 4.

ICL results from training 1-layer ATT and SSM models, respectively, and dotted curves are obtained from (8) in Theorem 1. The alignment of solid, dashed and dotted curves validates our Proposition 1 and Theorem 1.

\(\bullet\)**Distributional alignment experiments (Figs. 2(a)-2(b)).** In Figs. 2(a) and 2(b), we generate RAG and task-feature alignment data following (9) and (11), respectively, by setting \(\sigma=0\) and varying \(\alpha\) from 0 to 0.6. Attention training results are displayed in solid curves, and we generate theory curve (dotted) via the \(\mathcal{L_{\star}}\) formula as described in (36) in Appendix B.3 and (42) in Appendix B.4. The empirical alignments corroborate Theorems 4 and 5, further confirming that Proposition 1 is applicable to a broader range of real-world distributional alignment data.

\(\bullet\)**Low-rank (Fig. 2(c)) and LoRA (Fig. 2(d)) experiments.** We also run simulations to verify our theoretical findings in Section 3.2. Consider the independent data setting as described in (7). In Fig. 2(c), we set \(\bm{\Sigma}_{\bm{x}}=\bm{I}_{d}\), \(\sigma=0\) and task covariance to be diagonal with diagonal entries \(c[1\ 2^{-1}\ \cdots\ d^{-1}]^{\top}\) for some normalization constant \(c=d/\sum_{i=1}^{d}i^{-1}\), and parameterize the attention model using matrices \(\bm{W}_{k},\bm{W}_{q}\in\mathbb{R}^{(d+1)\times r}\) and vary \(r\) across the set \(\{1,5,10,20\}\). Results show that empirical (solid) and theoretical (dotted, c.f. (13)) curves overlap. In Fig. 2(d), we implement two phases of training. _Phase 1:_ Setting \(\bm{\Sigma}_{\bm{x}}=\bm{\Sigma}_{\bm{\beta}}=\bm{I}_{d}\) and \(\sigma=0\), we pretrain the model with full rank parameters and obtain weights \(\bm{\hat{W}}_{k},\bm{\hat{W}}_{q},\bm{\hat{W}}_{v}\in\mathbb{R}^{(d+1)\times(d +1)}\). _Phase 2:_ We generate new examples with task covariance \(\bm{\Sigma}_{\bm{\beta}}\) being a diagonal matrix with diagonal entries \(c^{\prime}[2^{-1}\ 2^{-2}\ \cdots\ 2^{-d}]^{\top}\) for some normalization constant \(c^{\prime}=d/\sum_{i=1}^{d}2^{-i}\). Given the rank restriction \(r\), we train additional LoRA parameters \(\bm{W}_{\text{up}},\bm{W}_{\text{down}}\in\mathbb{R}^{(d+1)\times r}\) where \(\bm{W}_{lora}:=\bm{W}_{\text{up}}\bm{W}_{\text{down}}^{\top}\) and (2a) becomes \(\text{ATT}(\bm{Z})=(\bm{Z}(\bm{\hat{W}}_{q}\bm{\hat{W}}_{k}^{\top}+\bm{W}_{ \text{up}}\bm{W}_{\text{down}}^{\top})\bm{Z}^{\top})\bm{\hat{W}}_{v}\). Fig. 2(d) presents the results after two phases of training where dotted curves are drawn from the right hand side of (14) directly. Here, note that since \(\bm{\Sigma},\bm{\Sigma}^{new}\) are diagonal, the right hand side of (14) returns the exact optimal risk of LoRA and the alignments verify it.

## 5 Related Work

There is growing interest in understanding the mechanisms behind ICL (Brown et al., 2020; Liu et al., 2023; Rae et al., 2021) in LLMs due to its success in continuously enabling novel applications for LLMs (GeminiTeam et al., 2023; OpenAI, 2023; Touvron et al., 2023). In the previous work, Garg et al. (2022) explored ICL ability of Transformers. In particular, they considered in-context prompts where each in-context example is labeled by a target function from a given function class, including linear models. A number of works have studied this and related settings to develop a theoretical understanding of ICL (von Oswald et al., 2023; Gatniry et al., 2024; Lin and Lee, 2024; Li et al., 2024; Bai et al., 2024; Akyurek et al., 2023; Zhang et al., 2023; Du et al., 2023). Akyurek et al. (2023) focus on linear regression and provide a construction of Transformer weights that can enable a single step of GD based on in-context examples. Along the similar line, Von Oswald et al. (2023) provide a construction of weights in linear attention-only Transformers that can emulate GD steps on in-context examples for a linear regression task. Similar to this line of work, Dai et al. (2023) argue that pre-trained language models act as meta-optimizer which utilize attention to apply meta-gradients to the original language model based on the in-context examples.

Figure 3: Distributional alignment and low-rank parameterization experiments. **(a)** and **(b)** show the ICL results using data generated via (9) and (11), respectively, by changing \(\alpha\) from 0 to 0.6. In **(c)**, we train low-rank linear attention models by setting \(\bm{W}_{k},\bm{W}_{q}\in\mathbb{R}^{(d+1)\times r}\) and in **(d)**, we apply the low-rank LoRA adaptor, \(\bm{W}_{lora}:=\bm{W}_{\text{up}}\bm{W}_{\text{down}}^{\top}\) where \(\bm{W}_{\text{up}},\bm{W}_{\text{down}}\in\mathbb{R}^{(d+1)\times r}\), to pretrained linear attention models and adjust the LoRA parameters under different task distribution. Solid and dotted curves correspond to the linear attention and theoretical results (c.f. Section 3), respectively, and the alignments validate our theorems in Section 3. More experimental details are discussed in Section 4.

Building on these primarily empirical studies, Zhang et al. (2024); Mahankali et al. (2024); Ahn et al. (2023); Duraisamy (2024) focus on developing a theoretical understanding of Transformers trained to perform ICL. For single-layer linear attention model trained on independent in-context prompts for random linear regression tasks, Mahankali et al. (2024); Ahn et al. (2023) show that the resulting model implements a single step of PGD on in-context examples in a test prompt, thereby corroborating the findings of (Von Oswald et al., 2023). Zhang et al. (2024) study the optimization dynamics of gradient flow while training a single-layer linear attention model on in-context prompts for random linear regression tasks. Similar to Mahankali et al. (2024); Ahn et al. (2023), they show that the trained model implements a single step of GD and PGD for isotropic and anisotropic Gaussian features, respectively. In addition, they also characterize the test-time prediction error for the trained model while highlighting its dependence on train and test prompt lengths.

While our work shares similarities with this line of works, as discussed in our contributions in the introduction, we expand the theoretical understanding of ICL along multiple novel dimensions, which includes the first study of LoRA adaptation for ICL in the presence of a distributional shift. Furthermore, we strive to capture the effect of retrieval augmentation (Lewis et al., 2020, Nakano et al., 2021) on ICL through our analysis. Retrieval augmentation allows for selecting most relevant demonstration out of a large collection for a test instance, e.g., via a dense retrieval model (Izacard et al., 2023), which can significantly outperform the typical ICL setup where fixed task-specific demonstrations are provided as in-context examples (Wang et al., 2022; Basu et al., 2023). Through a careful modeling of retrieval augmentation via correlated design, we show that it indeed has a desirable amplification effect where the effective number in-context examples becomes larger with higher correlation which corresponds to preforming a successful retrieval of query-relevant demonstrations in a practical retrieval augmented setup.

Recently, state space models (SSMs) (Gu et al., 2021, 2021, 2023; Fu et al., 2023; Gu and Dao, 2023) have appeared as potential alternatives to Transformer architecture, with more efficient scaling to input sequence length. Recent studies demonstrate that such SSMs can also perform ICL for simple non-language tasks (Park et al., 2024; Grazzi et al., 2024) as well as complex NLP tasks (Grazzi et al., 2024). That said, a rigorous theoretical understanding of ICL for SSMs akin to Zhang et al. (2024); Mahankali et al. (2024); Ahn et al. (2023) is missing from the literature. In this work, we provide the first such theoretical treatment for ICL with SSMs. Focusing on H3 architecture (Fu et al., 2023), we highlight its advantages over linear attention in specific ICL settings.

## 6 Discussion

In this work, we revisited the loss landscape of in-context learning with 1-layer sequence models. We have established a general connection between ICL and gradient methods that accounts for correlated data, non-attention architectures (specifically SSMs), and the impact of low-rank parameterization including LoRA adaptation. Our results elucidate two central findings: (1) The functions learned by different sequence model architectures exhibit a strong degree of _universality_ and (2) _Dataset and prompt design_, such as RAG, can substantially benefit ICL performance.

**Future directions and limitations.** The results of this work fall short of being a comprehensive theory for ICL in LLMs and can be augmented in multiple directions. First, while the exact equivalence between H3 and linear attention is remarkable, we should examine whether it extends to other SSMs. Secondly, while empirically predictive, our RAG and LoRA analyses are not precise and fully formal. Thirdly, it is desirable to develop a deeper understanding of multilayer architectures and connect to iterative GD methods as in (Ahn et al., 2023; Von Oswald et al., 2023). Finally, we have studied the population risk of ICL training whereas one can also explore the sample complexity of pretraining (Wu et al., 2023; Lu et al., 2024). Moving beyond the theoretically tractable setup of this work, our simplified models are trained on in-context prompts from random initialization. Therefore, this theoretical study doesn't address more challenging in-context learning tasks, such as question answering, where both in-context demonstration and general knowledge from pretraining are required. Future work in this area could also shed light on how certain contexts might elicit undesirable behaviors acquired by an LLM during pretraining, an aspect not covered in our current analysis. This work also studies a theoretical model for retrieval augmentation-based ICL. In a real-life retrieval augmentation-based ICL, one needs to account for the quality of the collection of the retrievable demonstrations and its (negative) impacts on the final predictions.

## Acknowledgements

This work was supported in part by the National Science Foundation grants CCF-2046816, CCF-2403075, the Office of Naval Research award N000142412289, an Adobe Data Science Research award, and a gift by Google Research.

## References

* Ahn et al. (2023) Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. _Advances in Neural Information Processing Systems_, 36, 2023.
* Akyurek et al. (2023) Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=0g0X4H8yH4I.
* Arora et al. (2019) Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In _International Conference on Machine Learning_, pages 322-332. PMLR, 2019.
* Bai et al. (2024) Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. _Advances in neural information processing systems_, 36, 2024.
* Basu et al. (2023) Soumya Basu, Ankit Singh Rawat, and Manzil Zaheer. A statistical perspective on retrieval-based models. In _International Conference on Machine Learning_, pages 1852-1886. PMLR, 2023.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Canatar et al. (2021) Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. _Nature communications_, 12(1):2914, 2021.
* Cao et al. (2019) Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the spectral bias of deep learning. _arXiv preprint arXiv:1912.01198_, 2019.
* Collins et al. (2024) Liam Collins, Adwait Parulekar, Aryan Mokhtari, Sujay Sanghavi, and Sanjay Shakkottai. In-context learning with transformers: Softmax attention adapts to function lipschitzness. _arXiv preprint arXiv:2402.11639_, 2024.
* Dai et al. (2023) Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Findings of the Association for Computational Linguistics: ACL 2023_, pages 4005-4019, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.247. URL https://aclanthology.org/2023.findings-acl.247.
* Dauphin et al. (2017) Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In _International conference on machine learning_, pages 933-941. PMLR, 2017.
* Du et al. (2023) Zhe Du, Haldun Balim, Samet Oymak, and Necmiye Ozay. Can transformers learn optimal filtering for unknown systems? _IEEE Control Systems Letters_, 7:3525-3530, 2023.
* Duraisamy (2024) Karthik Duraisamy. Finite sample analysis and bounds of generalization error of gradient descent in in in-context linear regression. _arXiv preprint arXiv:2405.02462_, 2024.
* Fu et al. (2023) Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=C0ZDy0WYGq.
* Fu et al. (2020)Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598, 2022.
* Gatmiry et al. (2022) Khashayar Gatmiry, Nikunj Saunshi, Sashank J Reddi, Stefanie Jegelka, and Sanjiv Kumar. Can looped transformers learn to implement multi-step gradient descent for in-context learning? In _Forty-first International Conference on Machine Learning_.
* Feam et al. (2023) GeminiTeam, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* Grazzi et al. (2024) Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, and Frank Hutter. Is mamba capable of in-context learning? _arXiv preprint arXiv:2402.03170_, 2024.
* Gu & Dao (2023) Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_, 2023.
* Gu et al. (2021a) Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In _International Conference on Learning Representations_, 2021a.
* Gu et al. (2021b) Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re. Combining recurrent, convolutional, and continuous-time models with linear state space layers. _Advances in neural information processing systems_, 34:572-585, 2021b.
* Han et al. (2023) Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. In-context learning of large language models explained as kernel regression. _arXiv preprint arXiv:2305.12766_, 2023.
* Hollmann et al. (2022) Noah Hollmann, Samuel Muller, Katharina Eggensperger, and Frank Hutter. Tabpfn: A transformer that solves small tabular classification problems in a second. _arXiv preprint arXiv:2207.01848_, 2022.
* Hu et al. (2022) Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=n2eVKeeFYf9.
* Izacard et al. (2023) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. _Journal of Machine Learning Research_, 24(251):1-43, 2023.
* Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In _International conference on machine learning_, pages 5156-5165. PMLR, 2020.
* Lee et al. (2023) Ivan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick. Exploring the relationship between model architecture and in-context learning ability. _arXiv preprint arXiv:2310.08049_, 2023.
* Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in Neural Information Processing Systems_, 33:9459-9474, 2020.
* Li et al. (2020) Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. In _International conference on artificial intelligence and statistics_, pages 4313-4324. PMLR, 2020.
* Li et al. (2023) Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In _International Conference on Machine Learning_, pages 19565-19594. PMLR, 2023.
* Li et al. (2024) Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, and Samet Oymak. Dissecting chain-of-thought: Compositionality through in-context filtering and learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* Li et al. (2020)Ziqian Lin and Kangwook Lee. Dual operating modes of in-context learning. _arXiv preprint arXiv:2402.18819_, 2024.
* Liu et al. (2023a) Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. In _The Eleventh International Conference on Learning Representations_, 2023a. URL https://openreview.net/forum?id=De4FYqjFueZ.
* Liu et al. (2023b) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35, 2023b.
* Lu et al. (2024) Yue Lu, Mary I. Letesy, Jacob A. Zavatone-Veth, Anindita Maiti, and Cengiz Pehlevan. Asymptotic theory of in-context learning by linear attention. _arXiv preprint arXiv:2310.08391_, 2024.
* Mahankali et al. (2024) Arvind V. Mahankali, Tatsunori Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=8p3fu56lKc.
* Mahdavi et al. (2024) Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis. Revisiting the equivalence of in-context learning and gradient descent: The impact of data distribution. In _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 7410-7414. IEEE, 2024.
* Muller et al. (2021) Samuel Muller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. _arXiv preprint arXiv:2112.10510_, 2021.
* Muller et al. (2023) Samuel Muller, Matthias Feurer, Noah Hollmann, and Frank Hutter. Pfns4bo: In-context learning for bayesian optimization. In _International Conference on Machine Learning_, pages 25444-25470. PMLR, 2023.
* Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. _arXiv preprint arXiv:2112.09332_, 2021.
* Olsson et al. (2022) Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_, 2022.
* OpenAI (2023) OpenAI. Gpt-4 technical report. _arXiv preprintarXiv:2303.08774_, 2023.
* Park et al. (2024) Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative study on in-context learning tasks. _International Conference on Machine Learning_, 2024.
* Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. _arXiv preprint arXiv:2112.11446_, 2021.
* Schlag et al. (2021) Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. Linear transformers are secretly fast weight programmers. In _International Conference on Machine Learning_, pages 9355-9366. PMLR, 2021.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* Oswald et al. (2023) Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, pages 35151-35174. PMLR, 2023.
* Oswald et al. (2023) Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering mesa-optimization algorithms in transformers. _arXiv preprint arXiv:2309.05858_, 2023.
* Oswald et al. (2021)Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retrieving from training data. _arXiv preprint arXiv:2203.08773_, 2022.
* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural information processing systems_, 35:24824-24837, 2022.
* Wu et al. (2023) Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? _arXiv preprint arXiv:2310.08391_, 2023.
* Xie et al. (2022) Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=RdJVFCHjUMI.
* Zhang et al. (2023) Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. _arXiv preprint arXiv:2306.09927_, 2023.
* Zhang et al. (2024) Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. _Journal of Machine Learning Research_, 25(49):1-55, 2024.
* Zucchet et al. (2023) Nicolas Zucchet, Seijin Kobayashi, Yassir Akram, Johannes Von Oswald, Maxime Larcher, Angelika Steger, and Joao Sacramento. Gated recurrent neural networks discover attention. _arXiv preprint arXiv:2309.01775_, 2023.

## Appendix A Equivalence among Gradient Descent, Attention, and State-Space Models

In this section, we present the proofs related to Section 2. Recap that given data

\[\bm{X}=[\bm{x}_{1}\ \cdots\ \bm{x}_{n}]^{\top}\in\mathbb{R}^{n \times d},\] \[\bm{\xi}=[\xi_{1}\ \cdots\ \xi_{n}]^{\top}\in\mathbb{R}^{n},\] \[\bm{y}=[y_{1}\ \cdots\ y_{n}]^{\top}=\bm{X}\bm{\beta}+\bm{\xi} \in\mathbb{R}^{n},\] \[\bm{Z}_{0}=[\bm{z}_{1}\ \cdots\ \bm{z}_{n}\ \bm{0}_{d+1}]^{\top}= \begin{bmatrix}\bm{x}_{1}&\ldots&\bm{x}_{n}&\bm{0}_{d}\\ y_{1}&\ldots&y_{n}&0\end{bmatrix}^{\top}\in\mathbb{R}^{(n+1)\times(d+1)},\]

and corresponding prediction functions

\[g_{\text{PGD}}(\bm{Z})=\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{y},\] (15a) \[g_{\text{WGD}}(\bm{Z})=\bm{x}^{\top}\bm{W}\bm{X}^{\top}(\omega \otimes\bm{y}),\] (15b) \[g_{\text{ATT}}(\bm{Z})=(\bm{z}^{\top}\bm{W}_{q}\bm{W}_{t}^{\top} \bm{Z}_{0}^{\top})\bm{Z}_{0}\bm{W}_{v}\bm{v},\] (15c) \[g_{\text{SSH}}(\bm{Z})=\left((\bm{z}^{\top}\bm{W}_{q})^{\top} \odot((\bm{Z}_{0}\bm{W}_{k}\odot\bm{Z}_{0}\bm{W}_{v})*\bm{f})_{n+1}\right)\bm {v},\] (15d)

we have objectives

\[\min_{\bm{W}}\mathcal{L}_{\text{PGD}}(\mathcal{W}) \quad\text{where}\quad\mathcal{L}_{\text{PGD}}(\mathcal{W})=\mathbb{E} \left[(y-g_{\text{PGD}}(\bm{Z}))^{2}\right],\] (16a) \[\min_{\bm{W},\omega}\mathcal{L}_{\text{WGD}}(\mathcal{W}) \quad\text{where}\quad\mathcal{L}_{\text{WGD}}(\mathcal{W})=\mathbb{E} \left[(y-g_{\text{WGD}}(\bm{Z}))^{2}\right],\] (16b) \[\min_{\bm{W}_{i},\bm{W}_{i},\bm{W}_{i},\bm{v}}\mathcal{L}_{ \text{ATT}}(\mathcal{W}) \quad\text{where}\quad\mathcal{L}_{\text{ATT}}(\mathcal{W})=\mathbb{E} \left[(y-g_{\text{ATT}}(\bm{Z}))^{2}\right],\] (16c) \[\min_{\bm{W}_{i},\bm{W}_{i},\bm{W}_{i},\bm{v},\bm{f}}\mathcal{L}_ {\text{SSH}}(\mathcal{W}) \quad\text{where}\quad\mathcal{L}_{\text{SSH}}(\mathcal{W})=\mathbb{E} \left[(y-g_{\text{SSH}}(\bm{Z}))^{2}\right].\] (16d)

Here, the expectation is over the randomness in \((\bm{x}_{i},\xi_{i})_{i=1}^{n}\) and \(\bm{\beta}\), and the search space for \(\bm{W}\) is \(\mathbb{R}^{d\times d}\), for \(\bm{\omega}\) is \(\mathbb{R}^{n}\), for \(\bm{W}_{k},\bm{W}_{q},\bm{W}_{v}\) is \(\mathbb{R}^{(d+1)\times(d+1)}\), for \(\bm{v}\) is \(\mathbb{R}^{d+1}\), and for \(\bm{f}\) is \(\mathbb{R}^{n+1}\).

### Proof of Proposition 1

Consider the problem setting as discussed in Section 2, Proposition 1 can be proven by the following two lemmas.

**Lemma 4**: _Suppose Assumptions 1 and 2 hold. Then, given the objectives (16a) and (16c), we have_

\[\min_{\bm{W}_{\bm{v}},\bm{W}_{\bm{v}},\bm{x}}\mathcal{L}_{\text{ATT}}(\mathcal{ W})=\min_{\bm{W}}\mathcal{L}_{\text{FQD}}(\mathcal{W}).\]

**Proof.** Recap the linear attention estimator from (15c) and denote

\[\bm{W}_{q}\bm{W}_{k}^{\top}=\begin{bmatrix}\bm{\tilde{W}}&\bm{w}_{1}\\ \bm{w}_{2}^{\top}&w\end{bmatrix}\qquad\text{and}\qquad\bm{W}_{\bm{v}}=\begin{bmatrix} \bm{v}_{1}\\ v\end{bmatrix},\]

where \(\bm{\tilde{W}}\in\mathbb{R}^{d\times d},\bm{w}_{1},\bm{w}_{2},\bm{v}_{1}\in \mathbb{R}^{d}\), and \(w,v\in\mathbb{R}\). Then we have

\[g_{\text{ATT}}(\bm{Z}) =(\bm{z}^{\top}\bm{W}_{q}\bm{W}_{k}^{\top}\bm{Z}_{0}^{\top})\bm{Z} _{0}\bm{W}_{\bm{v}}\] \[=[\bm{x}^{\top}\ 0]\begin{bmatrix}\bm{\tilde{W}}&\bm{w}_{1}\\ \bm{w}_{2}^{\top}&w\end{bmatrix}\begin{bmatrix}\bm{X}^{\top}&\bm{0}_{d}\\ \bm{y}^{\top}&0\end{bmatrix}\begin{bmatrix}\bm{X}&\bm{y}\\ \bm{0}_{d}^{\top}&0\end{bmatrix}\begin{bmatrix}\bm{v}_{1}\\ v\end{bmatrix}\] \[=(\bm{x}^{\top}\bm{\tilde{W}}\bm{X}^{\top}+\bm{x}^{\top}\bm{w}_{1 }\bm{y}^{\top})(\bm{X}\bm{v}_{1}+\bm{y}v)\] \[=\bm{x}^{\top}(v\bm{\tilde{W}})\bm{X}^{\top}\bm{y}+\bm{x}^{\top} \bm{w}_{1}\bm{y}^{\top}\bm{X}\bm{v}_{1}+\bm{x}^{\top}\left(\bm{\tilde{W}}\bm{X} ^{\top}\bm{X}\bm{v}_{1}+v\left\|\bm{v}\right\|_{\ell_{2}}^{2}\bm{w}_{1}\right)\] \[=\bm{x}^{\top}(\bm{\tilde{W}}+\bm{w}_{1}\bm{v}_{1}^{\top})\bm{X}^ {\top}\bm{y}+\bm{x}^{\top}\left(\bm{\tilde{W}}\bm{X}^{\top}\bm{X}\bm{v}_{1}+v \left\|\bm{v}\right\|_{\ell_{2}}^{2}\bm{w}_{1}\right)\] \[=\underbrace{\bm{x}^{\top}\bm{\tilde{W}}\bm{X}^{\top}\bm{y}}_{ \tilde{\bm{x}}\pi(\bm{Z})}\underbrace{\bm{x}^{\top}\left(\bm{\tilde{W}}\bm{X} ^{\top}\bm{X}\bm{v}_{1}+v\left\|\bm{v}\right\|_{\ell_{2}}^{2}\bm{w}_{1}\right) }_{\varepsilon},\] (17)

where \(\bm{\tilde{W}}:=v\bm{\tilde{W}}+\bm{w}_{1}\bm{v}_{1}^{\top}\).

We first show that for any given parameters \(\bm{W}_{k},\bm{W}_{q},\bm{W}_{r},\bm{v}\),

\[\mathbb{E}\left[\left(g_{\text{ATT}}(\bm{Z})-y\right)^{2}\right] \geq\mathbb{E}\left[\left(\tilde{g}_{\text{ATT}}(\bm{Z})-y\right)^{2}\right].\] (18)

To this goal, we have

\[\mathbb{E}\left[\left(g_{\text{ATT}}(\bm{Z})-y\right)^{2}\right] -\mathbb{E}\left[\left(\tilde{g}_{\text{ATT}}(\bm{Z})-y\right)^{2}\right] =\mathbb{E}\left[\left(\tilde{g}_{\text{ATT}}(\bm{Z})+\varepsilon -y\right)^{2}\right]-\mathbb{E}\left[\left(\tilde{g}_{\text{ATT}}(\bm{Z})-y \right)^{2}\right]\] \[=\mathbb{E}[\varepsilon^{2}]+2\,\mathbb{E}[\left(\tilde{g}_{\text {ATT}}(\bm{Z})-y\right)\varepsilon]\] (19)

In the following, we consider the expectations of \((a),(b),(c),(d)\) sequentially, which return zeros under Assumptions 1 and 2. Note that since Assumption 1 holds, expectation of any odd _order_ of monomial of the entries of \(\bm{X},\bm{x},\bm{\beta}\) returns zero, i.e., order of \(\bm{x}^{\top}\bm{\beta}\bm{x}\) is \(3\) and therefore \(\mathbb{E}[\bm{x}^{\top}\bm{\beta}\bm{x}]=\bm{0}_{d}\).

\[(a): \mathbb{E}\left[\bm{y}^{\top}\bm{X}\bm{\tilde{W}}^{\top}\bm{x}\bm {x}^{\top}\bm{\tilde{W}}\bm{X}^{\top}\bm{X}\bm{v}_{1}\right]\] \[=\mathbb{E}\left[(\bm{X}\bm{\beta}+\bm{\xi})^{\top}\bm{X}\bm{ \tilde{W}}^{\top}\bm{x}\bm{x}^{\top}\bm{\tilde{W}}\bm{X}^{\top}\bm{X}\bm{v}_{1}\right]\] \[=\mathbb{E}\left[\bm{\beta}^{\top}\bm{X}^{\top}\bm{X}\bm{\tilde{W}} ^{\top}\bm{x}\bm{x}^{\top}\bm{\tilde{W}}\bm{X}^{\top}\bm{X}\bm{v}_{1}\right]+ \mathbb{E}\left[\bm{\xi}^{\top}\bm{X}\bm{\tilde{W}}^{\top}\bm{x}\bm{x}^{\top} \bm{\tilde{W}}\bm{X}^{\top}\bm{X}\bm{v}_{1}\right]\] \[=0.\] \[(b): \mathbb{E}\left[v\left\|\bm{v}\right\|_{\ell_{2}}^{2}\bm{y}^{\top} \bm{X}\bm{\tilde{W}}^{\top}\bm{x}\bm{x}^{\top}\bm{w}_{1}\right]\] \[=\mathbb{E}\left[v(\bm{X}\bm{\beta}+\bm{\xi})^{\top}(\bm{X}\bm{ \beta}+\bm{\xi})(\bm{X}\bm{\beta}+\bm{\xi})^{\top}\bm{X}\bm{\tilde{W}}^{\top}\bm{ x}\bm{x}^{\top}\bm{w}_{1}\right]\] \[=\mathbb{E}\left[v\left\|\bm{\xi}\right\|_{\ell_{2}}^{2}\bm{\xi}^ {\top}\bm{X}\bm{\tilde{W}}^{\top}\bm{x}\bm{x}^{\top}\bm{w}_{1}\right]\] \[=0.\]\[(c): \mathbb{E}\left[y\mathbf{x}^{\top}\widehat{W}\mathbf{X}^{\top} \mathbf{X}\mathbf{r}_{1}\right]\] \[=\mathbb{E}\left[(\boldsymbol{x}^{\top}\boldsymbol{\beta}+\xi) \mathbf{x}^{\top}\widehat{W}\mathbf{X}^{\top}\mathbf{X}\mathbf{r}_{1}\right]\] \[=\mathbb{E}\left[\boldsymbol{\beta}^{\top}\boldsymbol{x} \boldsymbol{x}^{\top}\widehat{W}\mathbf{X}^{\top}\mathbf{X}\mathbf{r}_{1} \right]+\mathbb{E}\left[\xi\mathbf{x}^{\top}\widehat{W}\mathbf{X}^{\top} \mathbf{X}\mathbf{r}_{1}\right]\] \[=0.\] \[(d): \mathbb{E}\left[y\nu\left\|y\right\|_{\ell_{2}}^{2}\mathbf{x}^{ \top}\mathbf{w}_{1}\right]\] \[=\nu\,\mathbb{E}\left[(\boldsymbol{\beta}^{\top}\boldsymbol{x}+ \xi)(\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\xi})^{\top}(\boldsymbol{X }\boldsymbol{\beta}+\boldsymbol{\xi})\mathbf{x}^{\top}\mathbf{w}_{1}\right]\] \[=\nu\,\mathbb{E}\left[\xi\left\|\boldsymbol{\xi}\right\|_{\ell_ {2}}^{2}\mathbf{x}^{\top}\mathbf{w}_{1}\right]\] \[=0.\]

Combining the results with (19) returns that

\[\mathbb{E}\left[(g_{\mathsf{ATT}}(\boldsymbol{Z})-y)^{2}\right]-\mathbb{E} \left[(\bar{g}_{\mathsf{ATT}}(\boldsymbol{Z})-y)^{2}\right]=\mathbb{E}[e^{2}]\geq 0\] (20)

which completes the proof of (18). Therefore, we obtain

\[\min_{\boldsymbol{W}_{q},\boldsymbol{W}_{q},\boldsymbol{W}_{v},\boldsymbol{v }}\mathbb{E}\left[(g_{\mathsf{ATT}}(\boldsymbol{Z})-y)^{2}\right]\geq\min_{ \boldsymbol{W}}\mathbb{E}\left[(\bar{g}_{\mathsf{ATT}}(\boldsymbol{Z})-y)^{2} \right]=\min_{\boldsymbol{W}}\mathbb{E}\left[(g_{\mathsf{PQ}}(\boldsymbol{Z} )-y)^{2}\right].\]

We conclude the proof of this lemma by showing that for any \(\boldsymbol{W}\in\mathbb{R}^{d\times d}\) in \(g_{\mathsf{PQ}}\), there exist \(W_{k},\boldsymbol{W}_{q},\boldsymbol{W}_{v},\boldsymbol{v}\) such that \(g_{\mathsf{ATT}}(\boldsymbol{Z})=g_{\mathsf{PQ}}(\boldsymbol{Z})\). Let

\[\boldsymbol{W}_{k}=\boldsymbol{W}_{v}=\boldsymbol{I}_{d+1},\qquad\boldsymbol{ W}_{q}=\begin{bmatrix}\boldsymbol{W}&\boldsymbol{0}_{d}\\ \boldsymbol{0}_{d}^{\top}&0\end{bmatrix},\quad\text{and}\quad\boldsymbol{v}= \begin{bmatrix}\boldsymbol{0}_{d}\\ 1\end{bmatrix}.\]

Then we obtain

\[g_{\mathsf{ATT}}(\boldsymbol{Z})=\boldsymbol{x}^{\top}\boldsymbol{W}\boldsymbol {X}^{\top}\boldsymbol{y}=g_{\mathsf{PQ}}(\boldsymbol{Z}),\] (21)

which completes the proof. 

**Lemma 5**: _Suppose Assumptions 1 and 2 hold. Then, given the objectives in (16), we have_

\[\min_{\boldsymbol{W}_{q},\boldsymbol{W}_{k},\boldsymbol{W}_{v},\boldsymbol{v }}\mathcal{L}_{\mathsf{SSM}}(\boldsymbol{W})=\min_{\boldsymbol{W},\boldsymbol{ \omega}}\mathcal{L}_{\mathsf{NPGD}}(\boldsymbol{W}).\] (22)

_Additionally, if the examples \((\boldsymbol{x}_{i},y_{i})_{i=1}^{n}\) follow the same distribution and are conditionally independent given \(\boldsymbol{x}\) and \(\boldsymbol{\beta}\), then SSM/H3 can achieve the optimal loss using the all-ones filter and_

\[\min_{\boldsymbol{W},\boldsymbol{\omega}}\mathcal{L}_{\mathsf{NPGD}}( \boldsymbol{W})=\min_{\boldsymbol{W}}\mathcal{L}_{\mathsf{PGD}}(\boldsymbol{ W}).\] (23)

**Proof.** Recap the SSM estimator from (15d) and let

\[\boldsymbol{W}_{q} =\begin{bmatrix}\boldsymbol{w}_{q1}&\boldsymbol{w}_{q2}&\cdots& \boldsymbol{w}_{q,d+1}\end{bmatrix},\] \[\boldsymbol{W}_{k} =\begin{bmatrix}\boldsymbol{w}_{k1}&\boldsymbol{w}_{k2}&\cdots& \boldsymbol{w}_{k,d+1}\end{bmatrix},\] \[\boldsymbol{W}_{v} =\begin{bmatrix}\boldsymbol{w}_{v1}&\boldsymbol{w}_{v2}&\cdots& \boldsymbol{w}_{v,d+1}\end{bmatrix},\]

where \(\boldsymbol{w}_{qj},\boldsymbol{w}_{kj},\boldsymbol{w}_{vj}\in\mathbb{R}^{d+1}\) for \(j\leq d+1\), and let

\[\boldsymbol{v}=\begin{bmatrix}v_{1}\\ v_{2}\\ \cdots\\ v_{d+1}\end{bmatrix},\quad\text{and}\quad\boldsymbol{f}=\begin{bmatrix}f_{0} \\ f_{1}\\ \cdots\\ f_{n}\end{bmatrix}.\]

Then we have

\[g_{\mathsf{SSM}}(\boldsymbol{Z}) =\left((\boldsymbol{z}^{\top}\boldsymbol{W}_{q})^{\top}\odot(( \boldsymbol{Z}_{0}\boldsymbol{W}_{k}\odot\boldsymbol{Z}_{0}\boldsymbol{W}_{ v})\ast\boldsymbol{f})_{n+1}\right)\boldsymbol{v}\] \[=\sum_{i=1}^{n}f_{n+1-i}\cdot\boldsymbol{v}^{\top}\begin{bmatrix} \begin{bmatrix}\boldsymbol{w}_{q1}^{\top}\boldsymbol{z}\\ \cdots\\ \boldsymbol{w}_{d,d+1}^{\top}\boldsymbol{z}\end{bmatrix}\boldsymbol{w}_{v,d+1}^{ \top}\boldsymbol{z}_{i}\end{bmatrix}.\]

[MISSING_PAGE_EMPTY:18]

of monomials of entries of \((\bm{x},\bm{X},\bm{\beta})\) with even orders: e.g., \(y=\bm{x}^{\top}\bm{\beta}+\xi\) where \(\xi\) is of oder \(0\) and \(\bm{x}^{\top}\bm{\beta}\) is of order \(2\).

\[(a): \mathbb{E}\left[\tilde{\delta}(\bm{x},\bm{X},\bm{X})\cdot\bm{x}^{ \top}\tilde{W}\bm{X}\tilde{\bm{y}}\right]\] \[=\mathbb{E}\left[\tilde{\delta}(\bm{x},\bm{X},\bm{X})\cdot\bm{x}^ {\top}\tilde{W}\bm{X}(\bm{X}\bm{\beta}\odot\tilde{\bm{f}})\right]+\mathbb{E} \left[\tilde{\delta}(\bm{x},\bm{X},\bm{X})\cdot\bm{x}^{\top}\tilde{W}\bm{X}( \xi\odot\tilde{\bm{f}})\right]\] \[=\mathbb{E}\left[\tilde{\delta}(\bm{x},\bm{X},\bm{X})\cdot\bm{x}^ {\top}\tilde{W}\bm{X}\right]\mathbb{E}\left[\xi\odot\tilde{\bm{f}}\right]\] \[=0.\]

\[(c): \mathbb{E}\left[\tilde{\bm{w}}^{\top}\bm{x}\bm{y}^{\top}\tilde{ \bm{y}}\cdot\bm{x}^{\top}\tilde{W}\bm{X}\tilde{\bm{y}}\right]\] \[=\mathbb{E}\left[\tilde{\bm{w}}^{\top}\bm{x}(\bm{X}\bm{\beta}+\bm {\xi})^{\top}(\bm{X}\bm{\beta}\odot\tilde{\bm{f}}+\bm{\xi}\odot\tilde{\bm{f}}) \cdot\bm{x}^{\top}\tilde{W}\bm{X}(\bm{X}\bm{\beta}\odot\tilde{\bm{f}}+\bm{\xi} \odot\tilde{\bm{f}})\right]\] \[=0.\]

\[(d): \mathbb{E}\left[y\cdot\tilde{\bm{w}}^{\top}\bm{x}\bm{y}^{\top} \tilde{\bm{y}}\right]\] \[=\mathbb{E}\left[(\bm{x}^{\top}\bm{\beta}+\xi)\cdot\tilde{\bm{w} }^{\top}\bm{x}(\bm{X}\bm{\beta}+\bm{\xi})^{\top}(\bm{X}\bm{\beta}\odot\tilde{ \bm{f}}+\bm{\xi}\odot\tilde{\bm{f}})\right]\] \[=0.\]

Combining the results with (24) results that

\[\mathbb{E}\left[(g_{\texttt{SSSM}}(\bm{Z})-y)^{2}\right]-\mathbb{E}\left[( \tilde{g}_{\texttt{SSM}}(\bm{Z})-y)^{2}\right]=\mathbb{E}\left[(e_{1}+e_{2})^{ 2}\right]\geq 0.\]

Therefore we obtain,

\[\min_{\bm{W}_{v},\bm{W}_{v},\bm{W}_{v},\bm{f}}\mathbb{E}\left[(g_{\texttt{SSM }}(\bm{Z})-y)^{2}\right]\geq\min_{\tilde{W},\tilde{\bm{f}}}\mathbb{E}\left[( \tilde{g}_{\texttt{SSM}}(\bm{Z})-y)^{2}\right]=\min_{\bm{W},\bm{\alpha}} \mathbb{E}\left[(g_{\texttt{WFDD}}(\bm{Z})-y)^{2}\right].\]

Next we show that for any choices of \(\bm{W}\) and \(\bm{\omega}\) in \(g_{\texttt{WFDD}}\), there are \(\bm{W}_{q,k,v},\bm{v},\bm{f}\) such that \(g_{\texttt{SSM}}\equiv g_{\texttt{WFDD}}\). To this end, given \(\bm{\omega}=[\omega_{1}\ \ldots\ \omega_{n}]^{\top}\), let

\[\bm{W}_{q}=\bm{I}_{d+1},\quad\bm{W}_{k}=\begin{bmatrix}\bm{W}^{\top}&\bm{0}_{ d}\\ \bm{0}_{d}^{\top}&0\end{bmatrix},\quad\bm{W}_{v}=\begin{bmatrix}\bm{0}_{d\times d}&\bm{0} _{d}\\ \bm{1}_{d}^{\top}&0\end{bmatrix},\quad\bm{v}=\begin{bmatrix}\bm{1}_{d}\\ 0\end{bmatrix}\quad\text{and}\quad\bm{f}=\begin{bmatrix}0\\ \omega_{n}\\ \omega_{1}\end{bmatrix}.\]

Then we get

\[((\bm{Z}_{0}\bm{W}_{k}\odot\bm{Z}_{0}\bm{W}_{v})\ast\bm{f})_{n+1} =\left(\left(\begin{bmatrix}\bm{X}\bm{W}^{\top}&\bm{0}_{n}\\ \bm{0}_{d}&0\end{bmatrix}\odot\begin{bmatrix}\bm{y}\bm{1}_{d}^{\top}&\bm{0}_{n} \\ \bm{0}_{d}&0\end{bmatrix}\right)\ast\bm{f}\right)_{n+1}\] \[=\begin{bmatrix}\sum_{i=1}^{n}\omega_{i}\cdot y_{i}\bm{W}\bm{x}_{ i}\\ 0\end{bmatrix}\] \[=\begin{bmatrix}\bm{W}\bm{X}^{\top}(\bm{y}\odot\bm{\omega})\\ 0\end{bmatrix},\]

and therefore

\[g_{\texttt{SSM}}(\bm{Z})=\bm{x}^{\top}\bm{W}\bm{X}^{\top}(\bm{y}\odot\bm{ \omega})=g_{\texttt{WFDD}}(\bm{Z}),\]

which completes the proof of (22).

Next, to show (23), for any \(\bm{W}\in\mathbb{R}^{d\times d}\), let \(\mathcal{L}(\bm{\omega})=\mathbb{E}\left[\left(\bm{x}^{\top}\bm{W}\bm{X}^{\top}( \bm{y}\odot\bm{\omega})-y\right)^{2}\right]\). Then we have

\[\frac{\partial\mathcal{L}(\bm{\omega})}{\partial\omega_{i}} =\mathbb{E}\left[\left(\bm{x}^{\top}\bm{W}\sum_{j=1}^{n}\omega_{ j}y_{j}\bm{x}_{j}-y\right)\left(\bm{x}^{\top}\bm{W}y_{j}\bm{x}_{i}\right)\right]\] \[=2\sum_{j=1}^{n}\omega_{j}\,\mathbb{E}\left[(\bm{x}^{\top}\bm{W}y _{j}\bm{x}_{j})(\bm{x}^{\top}\bm{W}y_{j}\bm{x}_{i})\right]-2\,\mathbb{E}\left[ \bm{y}\bm{x}^{\top}\bm{W}y_{i}\bm{x}_{i}\right].\]

Here since \((\bm{x}_{i},y_{i})_{i=1}^{n}\) follow the same distribution and are conditionally independent given \(\bm{x}\) and \(\bm{\beta}\), for any \(i\neq j\neq j^{\prime}\), \(\mathbb{E}\left[(\bm{x}^{\top}\bm{W}y_{j}\bm{x}_{i})^{2}\right]=\mathbb{E} \left[(\bm{x}^{\top}\bm{W}y_{j}\bm{x}_{j})^{2}\right]\) and \(\mathbb{E}\left[(\bm{x}^{\top}\bm{W}y_{j}\bm{x}_{j})(\bm{x}^{\top}\bm{W}y_{j} \bm{x}_{i})\right]=\mathbb{E}\left[(\bm{x}^{\top}\bm{W}y_{j^{\prime}}\bm{x}_{ j})(\bm{x}^{\top}\bm{W}y_{j}\bm{x}_{i})\right]\). Then let

\[\mathbb{E}\left[(\bm{x}^{\top}\bm{W}y_{j}\bm{x}_{j})(\bm{x}^{\top}\bm{W}y_{j} \bm{x}_{i})\right]=\begin{cases}c_{1},&i\neq j\\ c_{2},&i=j\end{cases}\quad\text{and}\quad\mathbb{E}\left[y\bm{x}^{\top}\bm{W}y _{i}\bm{x}_{i}\right]=c_{3},\]

where \((c_{1},c_{2},c_{3}):=(c_{1}(\bm{W}),c_{2}(\bm{W}),c_{3}(\bm{W}))\). We get

\[\frac{\partial\mathcal{L}(\bm{\omega})}{\partial\omega_{i}}=2c_{1}\bm{\omega} ^{\top}\bm{1}_{n}+2(c_{2}-c_{1})\omega_{i}-2c_{3}.\]

If \(c_{2}-c_{1}=0\), then \(\frac{\partial\mathcal{L}(\bm{\omega})}{\partial\omega_{i}}\equiv 2c_{1}\bm{ \omega}^{\top}\bm{1}_{n}-2c_{3}\) for all \(i\leq n\) and any \(\bm{\omega}\in\mathbb{R}^{n}\) achieves the same performance.

If \(c_{2}-c_{1}\neq 0\), setting \(\frac{\partial\mathcal{L}(\bm{\omega})}{\partial\omega_{i}}=0\) returns

\[\omega_{i}=\frac{c_{3}-c_{1}\sum_{j=1}^{n}\omega_{j}}{c_{2}-c_{1}}:=C\quad \text{for all }i\leq n.\]

Therefore the optimal loss is achieved via setting \(\bm{\omega}=C\bm{1}_{n}\). Without loss of generality, we can update \(\bm{W}\to\bm{C}\bm{W}\). Then \(\bm{\omega}=\bm{1}_{n}\), and we obtain

\[\min_{\bm{W},\bm{\omega}}\mathbb{E}\left[\left(\bm{x}^{\top}\bm{W}\bm{X}^{ \top}(\bm{y}\odot\bm{\omega})-y\right)^{2}\right]=\min_{\bm{W}}\mathbb{E} \left[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{y}-y)^{2}\right]\]

which completes the proof of (23). 

### Proof of Lemma 1

**Proof.** Recap the loss \(\mathcal{L}_{\text{PGD}}(\mathcal{W})\) in (16a) and prediction \(g_{\text{PGD}}(\bm{Z})\) in (15a), we have

\[\mathcal{L}_{\text{PGD}}(\mathcal{W}) =\mathbb{E}[(y-g_{\text{PGD}}(\bm{Z}))^{2}]\] \[=\mathbb{E}\left[\left(\bm{x}^{\top}\bm{\beta}+\xi-\bm{x}^{\top} \bm{W}\bm{X}^{\top}(\bm{X}\bm{\beta}+\xi)\right)^{2}\right]\] \[=\mathbb{E}\left[(\bm{x}^{\top}\bm{\beta}-\bm{x}^{\top}\bm{W}\bm{X }^{\top}\bm{X}\bm{\beta})^{2}+2(\bm{x}^{\top}\bm{\beta}-\bm{x}^{\top}\bm{W} \bm{X}^{\top}\bm{X}\bm{\beta})(\xi-\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})+( \xi-\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})^{2}\right]\] \[=\mathbb{E}\left[(\bm{x}^{\top}\bm{\beta}-\bm{x}^{\top}\bm{W}\bm{X }^{\top}\bm{X}\bm{\beta})^{2}+(\xi-\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})^{2} \right]+2\,\mathbb{E}[(\bm{x}^{\top}\bm{\beta}-\bm{x}^{\top}\bm{W}\bm{X}^{ \top}\bm{X}\bm{\beta})(\xi-\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})]\] \[=\underbrace{\mathbb{E}\left[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{X }\bm{\beta})^{2}+(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})^{2}\right]}_{f_{1 }(\bm{W})}\underbrace{-2\,\mathbb{E}[\bm{\beta}^{\top}\bm{x}\bm{x}^{\top}\bm{W} \bm{X}^{\top}\bm{X}\bm{\beta}+\xi\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi}]}_{f _{1}(\bm{W})}+\underbrace{\mathbb{E}[(\bm{x}^{\top}\bm{\beta})^{2}+\xi^{2}]}_{ \text{constant}}\]

where (25) follows Assumption 2. Since \(f_{2}(\bm{W})\) is convex, \(\mathcal{L}_{\text{PGD}}(\mathcal{W})\) is strongly-convex if and only if \(f_{1}(\bm{W})\) is strongly-convex, which completes the proof of strong convexity.

Next, (20) and (21) in the proof of Lemma 4 demonstrate that the optimal loss is achievable and is achieved at \(\varepsilon=0\). Subsequently, (17) indicates that \(g_{\text{ATT}}^{\star}\) has the same form as \(g_{\text{PGD}}^{\star}\). Under the strong convexity assumption, \(g_{\text{PGD}}^{\star}\) is unique, which leads to the conclusion that \(g_{\text{PGD}}^{\star}=g_{\text{ATT}}^{\star}\).

### Proof of Lemma 2

**Proof.** According to Lemma 1, \(\mathcal{L}_{\text{PGD}}(\mathcal{W})\) is strongly-convex as long as either \(\mathbb{E}[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{X}\bm{\beta})^{2}]\) or \(\mathbb{E}[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})^{2}]\) is strongly-convex. Therefore, in this lemma, the two claims correspond to the strong convexity of \(\mathbb{E}[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})^{2}]\) and \(\mathbb{E}[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{X}\bm{\beta})^{2}]\) terms, respectively.

Suppose the decomposition claim holds. Without losing generality, we may assume \((\bm{x}_{1},\bm{\beta}_{1},\bm{X}_{1})\) are zero-mean because we can allocate the mean component to \((\bm{x}_{2},\bm{\beta}_{2},\bm{X}_{2})\) without changing the covariance.

\(\bullet\)**Claim 1:** Let \(\bar{\bm{\Sigma}}_{\bm{x}}=\mathbb{E}[\bm{x}_{1}\bm{x}_{1}^{\top}]\), \(\bar{\bm{\Sigma}}_{\bm{\beta}}=\mathbb{E}[\bm{\beta}_{1}\bm{\beta}_{1}^{\top}]\), and \(\bar{\bm{\Sigma}}_{\bm{X}}=\mathbb{E}[\bm{X}_{1}^{\top}\bm{X}_{1}]\). If the first claim holds, using independence, observe that we can write

\[\mathbb{E}[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})^{2}]=\mathbb{E}[(\bm{x}_{ 1}^{\top}\bm{W}\bm{X}_{1}^{\top}\bm{\xi})^{2}]+\mathbb{E}[(\bm{x}_{1}^{\top} \bm{W}\bm{X}_{2}^{\top}\bm{\xi})^{2}]+\mathbb{E}[(\bm{x}_{2}^{\top}\bm{W}\bm{X }_{1}^{\top}\bm{\xi})^{2}]+\mathbb{E}[(\bm{x}_{2}^{\top}\bm{W}\bm{X}_{2}^{\top} \bm{\xi})^{2}],\]

where the last three terms of the right hand side are convex and the first term obeys

\[\mathbb{E}[(\bm{x}_{1}^{\top}\bm{W}\bm{X}_{1}^{\top}\bm{\xi})^{2}] =\sigma^{2}\,\mathbb{E}[\bm{x}_{1}^{\top}\bm{W}\bm{X}_{1}^{\top} \bm{X}_{1}W^{\top}\bm{x}_{1}]\] \[=\sigma^{2}\text{tr}\left(\mathbb{E}[\bm{x}_{1}\bm{x}_{1}^{\top} \bm{W}\bm{X}_{1}^{\top}\bm{X}_{1}\bm{W}^{\top}]\right)\] \[=\sigma^{2}\text{tr}\left(\bar{\bm{\Sigma}}_{\bm{x}}\bm{\bar{\bm {\Sigma}}}_{\bm{X}}W^{\top}\right)\] \[=\sigma^{2}\left\|\sqrt{\bar{\bm{\Sigma}}_{\bm{x}}}\bm{W}\sqrt{ \bar{\bm{\Sigma}}_{\bm{X}}}\right\|_{F}^{2}.\]

Since noise level \(\sigma>0\), using the full-rankness of covariance matrices \(\bar{\bm{\Sigma}}_{\bm{x}}\) and \(\bar{\bm{\Sigma}}_{\bm{X}}\), we conclude with strong convexity of \(\mathbb{E}[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})^{2}]\):

\(\bullet\)**Claim 2:** Now recall that \(\bar{\bm{\Sigma}}_{\bm{X}}=\mathbb{E}[\bm{X}_{1}^{\top}\bm{X}_{1}]\) and set \(\bm{A}=\bm{X}_{1}^{\top}\bm{X}_{1}-\bar{\bm{\Sigma}}_{\bm{X}}\) and \(\bm{B}=\bm{X}_{2}^{\top}\bm{X}_{2}+\bar{\bm{\Sigma}}_{\bm{X}}\). Observe that \(\mathbb{E}[\bm{A}]=0\). If the second claim holds, \(\mathbb{E}[\bm{X}^{\top}\bm{X}]=\mathbb{E}[\bm{A}+\bm{B}]\). Note that \((\bm{A},\bm{\beta}_{1},\bm{x}_{1})\) are independent of each other and \((\bm{B},\bm{\beta}_{2},\bm{x}_{2})\). Using independence and \(\mathbb{E}[\bm{A}]=0\), similarly write

\[\mathbb{E}[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{X}\bm{\beta})^{2}]=\mathbb{E} [(\bm{x}^{\top}\bm{W}\bm{A}\bm{\beta})^{2}]+\mathbb{E}[(\bm{x}^{\top}\bm{W}\bm {B}\bm{\beta})^{2}].\]

Now using \(\mathbb{E}[\bm{\beta}_{1}]=\mathbb{E}[\bm{x}_{1}]=0\) and their independence from rest, these terms obeys

\[\mathbb{E}[(\bm{x}^{\top}\bm{W}\bm{A}\bm{\beta})^{2}] =\mathbb{E}[(\bm{x}_{1}^{\top}\bm{W}\bm{A}\bm{\beta}_{1})^{2}]+ \mathbb{E}[(\bm{x}_{1}^{\top}\bm{W}\bm{A}\bm{\beta}_{2})^{2}]+\mathbb{E}[(\bm {x}_{2}^{\top}\bm{W}\bm{A}\bm{\beta}_{1})^{2}]+\mathbb{E}[(\bm{x}_{2}^{\top} \bm{W}\bm{A}\bm{\beta}_{2})^{2}]\] \[\mathbb{E}[(\bm{x}^{\top}\bm{W}\bm{B}\bm{\beta})^{2}] =\mathbb{E}[(\bm{x}_{1}^{\top}\bm{W}\bm{B}\bm{\beta}_{1})^{2}]+ \mathbb{E}[(\bm{x}_{1}^{\top}\bm{W}\bm{B}\bm{\beta}_{2})^{2}]+\mathbb{E}[(\bm {x}_{2}^{\top}\bm{W}\bm{B}\bm{\beta}_{1})^{2}]+\mathbb{E}[(\bm{x}_{2}^{\top} \bm{W}\bm{B}\bm{\beta}_{2})^{2}].\]

In both equations, the last three terms of the right hand side are convex. To proceed, we focus on the first terms. Using independence and setting \(\bm{\Sigma}_{\bm{X}}=\mathbb{E}[\bm{X}^{\top}\bm{X}]\geq\bar{\bm{\Sigma}}_{\bm{X}}>0\), we note that

\[\mathbb{E}[(\bm{x}_{1}^{\top}\bm{W}\bm{A}\bm{\beta}_{1})^{2}]+\mathbb{E}[(\bm{x}_ {1}^{\top}\bm{W}\bm{B}\bm{\beta}_{1})^{2}]=\mathbb{E}[(\bm{x}_{1}^{\top}\bm{W} \bm{X}^{\top}\bm{X}\bm{\beta}_{1})^{2}]\]

where \(\bm{x}_{1},\bm{\beta}_{1},\bm{X}\) are independent and full-rank covariance. To proceed, note that

\[\mathbb{E}[(\bm{x}_{1}^{\top}\bm{W}\bm{X}^{\top}\bm{X}\bm{\beta}_{1})^{2}]= \mathbb{E}[(\bm{x}_{1}^{\top}\bm{W}\bm{\Sigma}_{\bm{X}}\bm{\beta}_{1})^{2}]+ \mathbb{E}[(\bm{x}_{1}^{\top}\bm{W}(\bm{X}^{\top}\bm{X}-\bm{\Sigma}_{\bm{X}}) \bm{\beta}_{1})^{2}].\]

Observing the convexity of the right hand side and focusing on the first term, we get

\[\mathbb{E}[(\bm{x}_{1}^{\top}\bm{W}\bm{\Sigma}_{\bm{X}}\bm{\beta}_{1})^{2}]=\text{ tr}\left(\bar{\bm{\Sigma}}_{\bm{x}}\bm{W}\bm{\Sigma}_{\bm{X}}\bar{\bm{\Sigma}}_{\bm{\beta}}\bm{ \Sigma}_{\bm{X}}\bm{W}^{\top}\right)=\left\|\sqrt{\bar{\bm{\Sigma}}_{\bm{X}}\bm{ \Sigma}_{\bm{X}}\sqrt{\bar{\bm{\Sigma}}_{\bm{\beta}}}}\right\|_{F}^{2}.\]

Using the fact that covariance matrices, \(\bar{\bm{\Sigma}}_{\bm{x}},\bm{\Sigma}_{\bm{X}},\bar{\bm{\Sigma}}_{\bm{\beta}}\), are full rank concludes the strong convexity proof of \(\mathbb{E}[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{X}\bm{\beta})^{2}]\). \(\blacksquare\)

## Appendix B Analysis of General Data Distribution

In this section, we provide the proofs in Section 3, which focuses on solving Objective (5a). For the sake of clean notation, let \(\mathcal{L}(\bm{W}):=\mathcal{L}_{\text{PGD}}(\mathcal{W})\) and \(g:=g_{\text{PGD}}\) in this section.

### Supporting Results

We begin by deriving the even moments of random variables.

\(\bullet\)\(2n\)**'th moment of a normally distributed variable:** Let \(u\sim\mathcal{N}(0,\sigma^{2})\). Then we have

\[\mathbb{E}[u^{2n}]=\sigma^{2n}(2n-1)!!.\] (26)

\(\bullet\)\(4\)**'th moment:** Let \(\bm{u}\sim\mathcal{N}(0,\bm{I}_{d})\). Then for any \(\bm{W},\bm{W}^{\prime}\in\mathbb{R}^{d\times d}\), we have

\[\mathbb{E}\left[(\bm{u}^{\top}\bm{W}\bm{u})(\bm{u}^{\top}\bm{W}^{ \prime}\bm{u})\right]\] \[=\mathbb{E}\left[\left(\sum_{i,j=1}^{d}W_{ij}u_{i}u_{j}\right) \left(\sum_{i,j=1}^{d}W^{\prime}_{ij}u_{i}u_{j}\right)\right]\] \[=\mathbb{E}\left[\left(\sum_{i=1}^{d}W_{ii}u_{i}^{2}\right)\left( \sum_{i=1}^{d}W^{\prime}_{ii}u_{i}^{2}\right)\right]+\mathbb{E}\left[\left( \sum_{i\neq j}W_{ij}u_{i}u_{j}\right)\left(\sum_{i\neq j}W^{\prime}_{ij}u_{i} u_{j}\right)\right]\] \[=\sum_{i=1}^{d}W_{ii}W^{\prime}_{ii}\,\mathbb{E}\left[u_{i}^{4} \right]+\sum_{i\neq j}W_{ii}W^{\prime}_{jj}\,\mathbb{E}[u_{i}^{2}]\,\mathbb{E }[u_{j}^{2}]+\sum_{i\neq j}W_{ij}W^{\prime}_{ij}\,\mathbb{E}[u_{i}^{2}]\, \mathbb{E}[u_{j}^{2}]+\sum_{i\neq j}W_{ij}W^{\prime}_{ji}\,\mathbb{E}[u_{i}^{2 }]\,\mathbb{E}[u_{j}^{2}]\] \[=3\sum_{i=1}^{d}W_{ii}W^{\prime}_{ii}+\sum_{i\neq j}W_{ii}W^{ \prime}_{jj}+\sum_{i\neq j}W_{ij}W^{\prime}_{ij}+\sum_{i\neq j}W_{ij}W^{\prime }_{ji}\] \[=\sum_{i,j=1}^{d}W_{ii}W^{\prime}_{jj}+\sum_{i,j=1}^{d}W_{ij}W^{ \prime}_{ij}+\sum_{i,j=1}^{d}W_{ij}W^{\prime}_{ji}\] \[=\text{tr}\left(\bm{W}\right)\text{tr}\left(\bm{W}^{\prime} \right)+\text{tr}\left(\bm{W}^{\prime}\bm{W}^{\prime}\right)+\text{tr}\left( \bm{W}\bm{W}^{\prime}\right).\] (27)

\(\bullet\)\(4\)**'th cross-moment:** Let \(\bm{u},\bm{v}\sim\mathcal{N}(0,\bm{I}_{d})\) and for any \(\bm{W}\in\mathbb{R}^{d\times d}\), let \(\bm{\Lambda}_{\bm{W}}=\bm{W}\odot\bm{I}_{d}\). Then we have

\[\mathbb{E}\left[\left(\bm{u}^{\top}\bm{W}\bm{v}^{\top}\bm{u} \right)^{2}\right]\] \[=\mathbb{E}\left[\left(\sum_{i,j=1}^{d}W_{ij}u_{i}v_{j}^{2} \right)^{2}\left(\sum_{i=1}^{d}u_{i}v_{i}^{2}\right)^{2}\right]\] \[=\mathbb{E}\left[\left(\sum_{i,j=1}^{d}W_{ij}^{2}u_{i}^{2}v_{j}^{ 2}+\sum_{i\neq\sigma}W_{ij}W_{r\,ji}u_{i}v_{j}^{2}+\sum_{j\neq f}W_{ij}W_{ij}u_ {i}^{2}v_{j}v_{j}+\sum_{i^{\prime}\neq i,j\neq h}W_{ij}W_{r\,ji}u_{i}u_{i^{ \prime}}v_{j}v_{j}\right)\left(\sum_{i=1}^{d}u_{i}^{2}v_{i}^{2}+\sum_{i\neq j }u_{i}u_{j}v_{i}v_{j}\right)\right]\] \[=\mathbb{E}\left[\left(\sum_{i,j=1}^{d}W_{ij}^{2}u_{i}^{2}v_{j} ^{2}\right)\left(\sum_{i=1}^{d}u_{i}^{2}v_{i}^{2}\right)+\left(\sum_{i\neq j }W_{ij}W_{ji}u_{i}^{2}u_{j}^{2}v_{i}^{2}\right)\right]\] \[=\mathbb{E}\left[\left(\sum_{i=1}^{d}W_{ii}^{2}u_{i}^{2}v_{i}^{2}+ \sum_{i\neq j}W_{ij}^{2}u_{i}^{2}v_{j}^{2}\right)\left(\sum_{i=1}^{d}u_{i}^{2} v_{i}^{2}\right)\right]+\sum_{i\neq j}W_{ij}W_{ji}\] \[=\mathbb{E}\left[\left(\sum_{i=1}^{d}W_{ii}^{2}u_{i}^{4}v_{i}^{ 4}+\sum_{i\neq j}W_{ii}^{2}u_{i}^{2}v_{i}^{2}v_{j}^{2}\right)\right]+\mathbb{E }\left[\left(\sum_{i\neq j}W_{ji}^{2}u_{i}^{4}v_{j}^{2}v_{i}^{2}+\sum_{i\neq j }W_{ij}^{2}u_{i}^{2}v_{j}^{4}\right)+\sum_{i\neq j\neq h}W_{ij}^{2}u_{i}^{2}v_{ j}^{2}u_{k}^{2}v_{k}^{2}\right]\right]+\sum_{i\neq j}W_{ij}W_{ji}\] \[=9\sum_{i=1}^{d}W_{ii}^{2}+(d-1)\sum_{i=1}^{d}W_{ii}^{2}+6\sum_{i \neq j}W_{ij}^{2}+(d-2)\sum_{i\neq j}W_{ij}^{2}+\sum_{i\neq j}W_{ij}W_{ji}\] \[=3\sum_{i=1}^{d}W_{ii}^{2}+(d+4)\sum_{i,j=1}^{d}W_{ij}^{2}+\sum_ {i,j=1}^{d}W_{ij}W_{ji}\] \[=3\text{tr}\left(\bm{\Lambda}_{\bm{W}}^{2}\right)+(d+4)\text{tr} \left(\bm{W}\bm{W}^{\top}\right)+\text{tr}\left(\bm{W}^{2}\right).\] (28)\(\bullet\) 6'th moment: Let \(\bm{u}\sim\mathcal{N}(0,\bm{I}_{d})\). Then for any \(\bm{W},\bm{W}^{\prime}\in\mathbb{R}^{d\times d}\), we have

\[\mathbb{E}\left[(\bm{u}^{\top}\bm{W}\bm{u})(\bm{u}^{\top}\bm{W}^{ \prime}\bm{u})\|\bm{u}\|_{L_{2}}^{2}\right]\] \[=\mathbb{E}\left[\left(\sum_{i,j=1}^{d}W_{ij}u_{i}u_{j}\right) \left(\sum_{i,j=1}^{d}W_{ij}^{\prime}u_{i}u_{j}\right)\left(\sum_{i=1}^{d}u_{i }^{2}\right)\right]\] \[=\mathbb{E}\left[\left(\sum_{i=1}^{d}W_{ii}u_{i}^{2}\right)\left( \sum_{i=1}^{d}W_{ii}^{\prime}u_{i}^{2}\right)\left(\sum_{i=1}^{d}u_{i}^{2} \right)\right]+\mathbb{E}\left[\left(\sum_{i\neq j}W_{ij}u_{i}u_{j}\right) \left(\sum_{i\neq j}W_{ij}^{\prime}u_{i}u_{j}\right)\left(\sum_{i=1}^{d}u_{i} ^{2}\right)\right]\] \[=\sum_{i=1}^{d}W_{ii}W_{ii}^{\prime}\mathbb{E}\left[u_{i}^{4} \left(\sum_{i=1}^{d}u_{i}^{2}\right)\right]+\sum_{i\neq j}W_{ii}W_{jj}^{\prime }\mathbb{E}\left[u_{i}^{2}u_{j}^{2}\left(\sum_{i=1}^{d}u_{i}^{2}\right)\right]\] \[\quad+\sum_{i\neq j}W_{ij}W_{ij}^{\prime}\mathbb{E}\left[u_{i}^{ 2}u_{j}^{2}\left(\sum_{i=1}^{d}u_{i}^{2}\right)\right]+\sum_{i\neq j}W_{ij}W_{ ij}^{\prime}\mathbb{E}\left[u_{i}^{2}u_{i}^{2}\left(\sum_{i=1}^{d}u_{i}^{2} \right)\right]\] \[=(d+4)\left(3\sum_{i=1}^{d}W_{ii}W_{ii}^{\prime}+\sum_{i\neq j}W_ {ii}W_{jj}^{\prime}+\sum_{i\neq j}W_{ij}W_{ij}^{\prime}+\sum_{i\neq j}W_{ij}W_ {ji}^{\prime}\right)\] (29) \[=(d+4)\left(\sum_{i,j=1}^{d}W_{ii}W_{jj}^{\prime}+\sum_{i,j=1}^{ d}W_{ij}W_{ij}^{\prime}+\sum_{i,j=1}^{d}W_{ij}W_{ii}^{\prime}\right)\] \[=(d+4)\left(\operatorname{tr}\left(\bm{W}\right)\operatorname{ tr}\left(\bm{W}^{\prime}\right)+\operatorname{tr}\left(\bm{W}^{\prime}\bm{W}^{ \top}\right)+\operatorname{tr}\left(\bm{W}\bm{W}^{\prime}\right)\right),\] (30)

where (29) is obtained by following

\[\mathbb{E}\left[u_{i}^{4}\left(\sum_{i=1}^{d}u_{i}^{2}\right) \right]=\mathbb{E}[u^{6}]+(d-1)\,\mathbb{E}[u^{4}]\,\mathbb{E}[u^{2}]=3(d+4),\] \[\mathbb{E}\left[u_{i}^{2}u_{j}^{2}\left(\sum_{i=1}^{d}u_{i}^{2} \right)\right]=2\,\mathbb{E}[u^{4}]\,\mathbb{E}[u^{2}]+(d-2)\,\mathbb{E}[u^{ 2}]\,\mathbb{E}[u^{2}]\,\mathbb{E}[u^{2}]=d+4.\]

\(\bullet\) 8'th moment: Let \(\bm{u}\sim\mathcal{N}(0,\bm{I}_{d})\). Then for any \(\bm{W},\bm{W}^{\prime}\in\mathbb{R}^{d\times d}\), we have

\[\mathbb{E}\left[(\bm{u}^{\top}\bm{W}\bm{u})(\bm{u}^{\top}\bm{W}^{ \prime}\bm{u})\|\bm{u}\|_{L_{2}}^{4}\right]\] \[=\mathbb{E}\left[\left(\sum_{i,j=1}^{d}W_{ij}u_{i}u_{j}\right) \left(\sum_{i,j=1}^{d}W_{ij}^{\prime}u_{i}u_{j}\right)\left(\sum_{i,j=1}^{d}u_ {i}^{2}u_{j}^{2}\right)\right]\] \[=\mathbb{E}\left[\left(\sum_{i=1}^{d}W_{ii}u_{i}^{2}\right)\left( \sum_{i=1}^{d}W_{ii}^{\prime}u_{i}^{2}\right)\left(\sum_{i=1}^{d}u_{i}^{4}+ \sum_{i\neq j}u_{i}^{2}u_{j}^{2}\right)\right]+\mathbb{E}\left[\left(\sum_{i \neq j}W_{ij}u_{i}u_{j}\right)\left(\sum_{i\neq j}W_{ij}^{\prime}u_{i}u_{j} \right)\left(\sum_{i=1}^{d}u_{i}^{4}+\sum_{i\neq j}u_{i}^{2}u_{i}^{2}\right)\right]\] \[=\sum_{i=1}^{d}W_{ii}W_{ii}^{\prime}\mathbb{E}\left[u_{i}^{4} \left(\sum_{i=1}^{d}u_{i}^{4}+\sum_{i\neq j}u_{i}^{2}u_{j}^{2}\right)\right]+ \sum_{i\neq j}W_{ii}W_{jj}^{\prime}\,\mathbb{E}\left[u_{i}^{2}u_{j}^{2}\left( \sum_{i=1}^{d}u_{i}^{4}+\sum_{i\neq j}u_{i}^{2}u_{j}^{2}\right)\right]\] \[\quad+\sum_{i\neq j}W_{ij}W_{ij}^{\prime}\,\mathbb{E}\left[u_{i}^{ 2}u_{i}^{2}u_{j}^{2}\left(\sum_{i=1}^{d}u_{i}^{4}+\sum_{i\neq j}u_{i}^{2}u_{i} ^{2}\right)\right]+\sum_{i\neq j}W_{ij}W_{ji}^{\prime}\,\mathbb{E}\left[u_{i}^{ 2}u_{i}^{2}u_{j}^{2}\left(\sum_{i=1}^{d}u_{i}^{4}+\sum_{i\neq j}u_{i}^{2}u_{j} ^{2}\right)\right]\] \[=(d+4)(d+6)\left(3\sum_{i=1}^{d}W_{ii}W_{ii}^{\prime}+\sum_{i\neq j }W_{ii}W_{jj}^{\prime}+\sum_{i\neq j}W_{ij}W_{ij}^{\prime}+\sum_{i\neq j}W_{ij} W_{ji}^{\prime}\right)\] (31) \[=(d+4)(d+6)\left(\sum_{i,j=1}^{d}W_{ii}W_{jj}^{\prime}+\sum_{i,j=1}^ {d}W_{ij}W_{ij}^{\prime}+\sum_{i,j=1}^{d}W_{ij}W_{ji}^{\prime}\right)\] \[=(d+4)(d+6)\left(\operatorname{tr}\left(\bm{W}\right) \operatorname{tr}\left(\bm{W}^{\prime}\right)+\operatorname{tr}\left(\bm{W}^{ \prime}\bm{W}^{\top}\right)+\operatorname{tr}\left(\bm{W}\bm{W}^{\prime}\right) \right).\] (32)where (31) is obtained by following

\[\mathbb{E}\left[u_{i}^{4}\left(\sum_{r=1}^{d}u_{r}^{4}+\sum_{i^{ \prime}\neq r}u_{r}^{2}u_{r}^{2}\right)\right]\] \[=\mathbb{E}[u^{8}]+(d-1)\,\mathbb{E}[u^{4}]\,\mathbb{E}[u^{4}]+2( d-1)\,\mathbb{E}[u^{6}]\,\mathbb{E}[u^{2}]+(d-1)(d-2)\,\mathbb{E}[u^{4}]\, \mathbb{E}[u^{2}]\,\mathbb{E}[u^{2}]\] \[=105+9(d-1)+30(d-1)+3(d-1)(d-2)\] \[=3(d+4)(d+6),\] \[\mathbb{E}\left[u_{i}^{2}u_{j}^{2}\left(\sum_{r=1}^{d}u_{r}^{4}+ \sum_{i^{\prime}\neq r}u_{r}^{2}u_{r}^{2}\right)\right]\] \[=2\,\mathbb{E}[u^{6}]\,\mathbb{E}[u^{2}]+(d-2)\,\mathbb{E}[u^{4}] (\mathbb{E}[u^{2}])^{2}+2\,\mathbb{E}[u^{4}]\,\mathbb{E}[u^{4}]+4(d-2)\, \mathbb{E}[u^{4}](\mathbb{E}[u^{2}])^{2}+(d-2)(d-3)(\mathbb{E}[u^{2}])^{4}\] \[=30+3(d-2)+18+12(d-2)+(d-2)(d-3)\] \[=(d+4)(d+6).\]

### Independent Data with General Covariance

**Proof of Theorem 1.** Consider a general independent linear model as defined in (7) where \(\bm{\Sigma}_{\bm{x}}\) and \(\bm{\Sigma}_{\bm{\beta}}\) are full-rank feature and task covariance matrices and

\[\bm{x}\sim\mathcal{N}(0,\bm{\Sigma}_{\bm{x}}),\quad\bm{\beta}\sim\mathcal{N}( 0,\bm{\Sigma}_{\bm{\beta}}),\quad\xi\sim\mathcal{N}(0,\sigma^{2}),\quad\text{ and}\quad y=\bm{x}^{\top}\bm{\beta}+\xi.\]

Let

\[\bm{X}=[\bm{x}_{1}\ \cdots\ \bm{x}_{n}]^{\top},\quad\bm{\xi}=[\xi_{1}\ \cdots\ \xi_{n}]^{\top},\quad\text{and}\quad\bm{y}=[y_{1}\ \cdots\ y_{n}]^{\top}=\bm{X}\bm{\beta}+\bm{\xi}.\]

To simplify and without loss of generality, let \(\bm{\tilde{x}}=\bm{\Sigma}_{\bm{x}}^{-1/2}\bm{x}\), \(\bm{\tilde{X}}=\bm{X}\bm{\Sigma}_{\bm{x}}^{-1/2}\), \(\bm{\tilde{\beta}}=\bm{\Sigma}_{\bm{x}}^{1/2}\bm{\beta}\) where we have

\[\bm{\tilde{x}}\sim\mathcal{N}(0,\bm{I}),\qquad\bm{\tilde{\beta}}\sim\mathcal{ N}(0,\bm{\Sigma}_{\bm{x}}^{1/2}\bm{\Sigma}_{\bm{\beta}}\bm{\Sigma}_{\bm{x}}^{1/2})\]

and

\[y=\bm{\tilde{x}}^{\top}\bm{\tilde{\beta}}+\xi,\qquad\bm{y}=\bm{\tilde{X}}\bm {\tilde{\beta}}+\xi.\]

Then recap the loss from (5a), and we obtain

\[\mathcal{L}(\bm{W}) =\mathbb{E}\left[(y-g(\bm{Z}))^{2}\right]\] \[=\mathbb{E}\left[\left(\bm{x}^{\top}\bm{\beta}+\xi-\bm{x}^{\top} \bm{W}\bm{X}^{\top}(\bm{X}\bm{\beta}+\bm{\xi})\right)^{2}\right]\] \[=\mathbb{E}\left[(\bm{x}^{\top}\bm{\beta}-\bm{x}^{\top}\bm{W}\bm{X }^{\top}\bm{X}\bm{\beta})^{2}+2(\bm{x}^{\top}\bm{\beta}-\bm{x}^{\top}\bm{W} \bm{X}^{\top}\bm{X}\bm{\beta})(\xi-\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})+( \xi-\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})^{2}\right]\] \[=\mathbb{E}\left[(\bm{x}^{\top}\bm{\beta}-\bm{x}^{\top}\bm{W}\bm{ X}^{\top}\bm{X}\bm{\beta})^{2}\right]+\mathbb{E}\left[(\bm{x}^{\top}\bm{W}\bm{X}^{ \top}\bm{\xi})^{2}\right]+\sigma^{2},\] (33)

where the last equality comes from the independence of label noise \(\xi,\bm{\xi}\).

We first consider the following term

\[\mathbb{E}\left[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})^{2}\right]=\mathbb{ E}\left[(\bm{\tilde{x}}^{\top}(\bm{\Sigma}_{\bm{x}}^{1/2}\bm{W}\bm{\Sigma}_{ \bm{x}}^{1/2})\bm{\tilde{X}}^{\top}\bm{\xi})^{2}\right]=n\sigma^{2}\cdot \text{tr}\left(\bm{\tilde{W}}\bm{\tilde{W}}^{\top}\right)\]

where we define \(\bm{\tilde{W}}=\bm{\Sigma}_{\bm{x}}^{1/2}\bm{W}\bm{\Sigma}_{\bm{x}}^{1/2}\). Next, focus on the following

\[\mathbb{E}\left[(\bm{x}^{\top}\bm{\beta}-\bm{x}^{\top}\bm{W}\bm{X }^{\top}\bm{X}\bm{\beta})^{2}\right] =\mathbb{E}\left[(\bm{\tilde{x}}^{\top}\bm{\tilde{\beta}}-\bm{ \tilde{x}}^{\top}\bm{\tilde{W}}\bm{\tilde{X}}^{\top}\bm{\tilde{X}}\bm{\tilde{ \beta}})^{2}\right]\] \[=\mathbb{E}\left[(\bm{\tilde{x}}^{\top}\left(\bm{I}-\bm{\tilde{W}} \bm{\tilde{X}}^{\top}\bm{\tilde{X}}\right)\bm{\tilde{\beta}})^{2}\right]\] \[=\text{tr}\left(\mathbb{E}\left[\left(\bm{I}-\bm{\tilde{W}}\bm{ \tilde{X}}^{\top}\bm{\tilde{X}}\right)\bm{\Sigma}\left(\bm{I}-\bm{\tilde{W}}\bm{ \tilde{X}}^{\top}\bm{\tilde{X}}\right)^{\top}\right]\right)\] \[=\text{tr}\left(\bm{\Sigma}\right)-\text{tr}\left(\bm{\Sigma}( \bm{\tilde{W}}+\bm{\tilde{W}}^{\top})\,\mathbb{E}[\bm{\tilde{X}}^{\top}\bm{ \tilde{X}}]\right)+\text{tr}\left(\bm{\tilde{W}}^{\top}\bm{\tilde{W}}\,\mathbb{E }[\bm{\tilde{X}}^{\top}\bm{\tilde{X}}\bm{\Sigma}\bm{\tilde{X}}^{\top}\bm{ \tilde{X}}]\right)\] \[=\text{tr}\left(\bm{\Sigma}\right)-2n\cdot\text{tr}\left(\bm{ \Sigma}\bm{\tilde{W}}\right)+\text{tr}\left(\bm{\tilde{W}}^{\top}\bm{\tilde{W}} \,\mathbb{E}[\bm{\tilde{X}}^{\top}\bm{\tilde{X}}\bm{\Sigma}\bm{\tilde{X}}^{ \top}\bm{\tilde{X}}]\right),\]

where \(\bm{\Sigma}:=\bm{\Sigma}_{\bm{x}}^{1/2}\bm{\Sigma}_{\bm{\beta}}\bm{\Sigma}_{\bm{x}} ^{1/2}\).

Let \(\vec{\bm{x}}_{i}\in\mathbb{R}^{n}\) be the \(i\)'th column of \(\vec{\bm{X}}\) and \(\bm{\Sigma}_{ij}\) be the \((i,j)\)'th entry of \(\bm{\Sigma}\). Then the \((i,j)\) entry of matrix \(\vec{\bm{X}}^{\top}\vec{\bm{X}}\bm{\Sigma}\vec{\bm{X}}^{\top}\vec{\bm{X}}\) is

\[(\vec{\bm{X}}^{\top}\vec{\bm{X}}\bm{\Sigma}\vec{\bm{X}}^{\top}\vec{\bm{X}})_{ij} =\sum_{k=1}^{d}\sum_{p=1}^{d}\bm{\Sigma}_{kp}\bar{\bm{x}}_{i}^{\top} \vec{\bm{x}}_{k}\bar{\bm{x}}_{p}^{\top}\vec{\bm{x}}_{j}.\]

Then we get

\[i\neq j: \quad\mathbb{E}\left[\left(\vec{\bm{X}}^{\top}\vec{\bm{X}}\bm{ \Sigma}\vec{\bm{X}}^{\top}\vec{\bm{X}}\right)_{ij}\right] =\bm{\Sigma}_{ij}\,\mathbb{E}[\vec{\bm{x}}_{i}^{\top}\vec{\bm{x}} _{j}\vec{\bm{x}}_{j}^{\top}\vec{\bm{x}}_{j}]+\bm{\Sigma}_{ji}\,\mathbb{E}[ \vec{\bm{x}}_{i}^{\top}\vec{\bm{x}}_{j}\vec{\bm{x}}_{i}^{\top}\vec{\bm{x}}_{j }]=n^{2}\bm{\Sigma}_{ij}+n\bm{\Sigma}_{ji}\] \[i=j: \quad\mathbb{E}\left[\left(\vec{\bm{X}}^{\top}\vec{\bm{X}}\bm{ \Sigma}\vec{\bm{X}}^{\top}\vec{\bm{X}}\right)_{ii}\right] =\bm{\Sigma}_{ii}\,\mathbb{E}\left[\vec{\bm{x}}_{i}^{\top}\vec{\bm{x}} _{i}\vec{\bm{x}}_{i}^{\top}\vec{\bm{x}}_{i}\right]+\sum_{j\neq i}\bm{\Sigma}_{jj }\,\mathbb{E}\left[\vec{\bm{x}}_{i}^{\top}\vec{\bm{x}}_{j}\vec{\bm{x}}_{j}\vec{ \bm{x}}_{i}\right]\] \[=\bm{\Sigma}_{ii}\,\mathbb{E}\left[(x_{i1}^{2}+\cdots+x_{in}^{2})^ {2}\right]+n\sum_{j\neq i}\bm{\Sigma}_{jj}\] \[=\bm{\Sigma}_{ii}(3n+n(n-1))+n\sum_{j\neq i}\bm{\Sigma}_{jj}\] \[=n\left(\bm{\Sigma}_{ii}(n+1)+\sum_{j=1}^{d}\bm{\Sigma}_{jj}\right)\] \[=n\left(\bm{\Sigma}_{ii}(n+1)+\mathsf{tr}\left(\bm{\Sigma}\right) \right).\]

Therefore

\[\mathbb{E}[\vec{\bm{X}}^{\top}\vec{\bm{X}}\bm{\Sigma}\vec{\bm{X}}^{\top}\vec{ \bm{X}}]=n(n+1)\bm{\Sigma}+n\cdot\mathsf{tr}\left(\bm{\Sigma}\right)\bm{I}.\]

Combining all together results in

\[\mathcal{L}(\bm{W}) =\mathsf{tr}\left(\bm{\Sigma}\right)-2n\mathsf{tr}\left(\bm{ \Sigma}\vec{\bm{W}}\right)+n(n+1)\mathsf{tr}\left(\bm{\Sigma}\vec{\bm{W}}^{ \top}\vec{\bm{W}}\right)+n(\mathsf{tr}\left(\bm{\Sigma}\right)+\sigma^{2}) \mathsf{tr}\left(\vec{\bm{W}}\vec{\bm{W}}^{\top}\right)+\sigma^{2},\] \[=M-2n\mathsf{tr}\left(\bm{\Sigma}\vec{\bm{W}}\right)+n(n+1) \mathsf{tr}\left(\bm{\Sigma}\vec{\bm{W}}^{\top}\vec{\bm{W}}\right)+nM\mathsf{ tr}\left(\vec{\bm{W}}\vec{\bm{W}}^{\top}\right),\] (34)

where \(M:=\mathsf{tr}\left(\bm{\Sigma}\right)+\sigma^{2}\). Setting \(\nabla_{\vec{\bm{W}}}\mathcal{L}(\bm{W})=0\) returns

\[-2n\cdot\bm{\Sigma}+2n(n+1)\cdot\bm{\Sigma}\vec{\bm{W}}+2nM\vec{\bm{W}}=0 \Longrightarrow\vec{\bm{W}}_{\star}=\left((n+1)\bm{I}+M\bm{\Sigma}^{-1}\right) ^{-1}.\]

Then we have

\[\bm{W}_{\star}=\bm{\Sigma}_{\star}^{-1/2}\left((n+1)\bm{I}+M\bm{\Sigma}^{-1} \right)^{-1}\bm{\Sigma}_{\star}^{-1/2}\]

and

\[\mathcal{L}_{\star}=\mathcal{L}(\bm{W}_{\star})=M-n\mathsf{tr}\left(((n+1)\bm{ \Sigma}^{-1}+M\bm{\Sigma}^{-2})^{-1}\right).\]

### Retrieval Augmented Generation with \(\alpha\) Correlation

In this section, we consider the retrieval augmented generation (RAG) linear model similar to (9), where we first draw the query vector \(\bm{x}\) and task vector \(\bm{\beta}\) via

\[\bm{x}\sim\mathcal{N}(0,\bm{I})\quad\text{and}\quad\bm{\beta}\sim\mathcal{N}( 0,\bm{I}).\]

We then draw data \((\bm{x}_{i})_{i=1}^{n}\) to be used in-context according to the rule \(\text{corr\_coef}(\bm{x},\bm{x}_{i})\geq\alpha\geq 0\). Hence, for \(i\leq n\) we sample

\[\bm{x}_{i}\,\big{|}\,\bm{x}\sim\mathcal{N}(\alpha\bm{x},\gamma^{2}\bm{I}), \quad\xi_{i}\sim\mathcal{N}(0,\sigma^{2})\quad\text{and}\quad y_{i}=\bm{x}_{i}^ {\top}\bm{\beta}+\xi_{i},\] (35)

which results in (9) by setting \(\gamma^{2}=1-\alpha^{2}\).

**Theorem 4** (Extended version of Theorem 2): _Consider linear model as defined in (35). Recap the objective from (5a) and let \(\bm{W}_{\star}:=\arg\min_{\bm{W}}\mathcal{L}_{\text{FoE2}}(\bm{W})\), and \(\mathcal{L}_{\star}=\mathcal{L}_{\text{FoE}}(\bm{W}_{\star})\). Then \(\bm{W}_{\star}\) and \(\mathcal{L}_{\star}\) satisfy_

\[\bm{W}_{\star}=c\bm{I}\qquad\text{and}\qquad\mathcal{L}_{\star}=d+\sigma^{2}-cnd (\alpha^{2}(d+2)+\gamma^{2})\] (36)

_where_

\[c=\frac{\alpha^{2}(d+2)+\gamma^{2}}{\alpha^{4}n(d+2)(d+4)+\alpha^{2}\gamma^{2}(d+ 2)(d+2n+3)+\gamma^{4}(d+n+1)+\sigma^{2}(\alpha^{2}(d+2)+\gamma^{2})}.\]

[MISSING_PAGE_FAIL:26]

\[(c): \quad\mathbb{E}\left[\bm{x}^{\top}\bm{\beta}\bm{x}^{\top}\bm{W}\bm{X }^{\top}\bm{X}\bm{\beta}\right]=\mathbb{E}\left[\sum_{i=1}^{n}\bm{x}^{\top}\bm{ \beta}\bm{x}^{\top}\bm{W}(\alpha\bm{x}+\gamma\bm{g}_{i})(\alpha\bm{x}+\gamma\bm {g}_{i})^{\top}\bm{\beta}\right]\] \[=\mathbb{E}\left[\sum_{i=1}^{n}\bm{x}^{\top}\bm{\beta}\bm{x}^{\top }\bm{W}(\alpha^{2}\bm{x}\bm{x}^{\top}+\gamma^{2}\bm{g}_{i}\bm{g}_{i}^{\top}+ \alpha\gamma\bm{x}\bm{g}_{i}^{\top}+\alpha\gamma\bm{g}_{i}\bm{x}^{\top})\bm{ \beta}\right]\] \[=\alpha^{2}n\,\mathbb{E}\left[\bm{x}^{\top}\bm{\beta}\bm{x}^{\top }\bm{W}\bm{x}\bm{x}^{\top}\bm{\beta}\right]+\gamma^{2}n\,\mathbb{E}\left[\bm{x }^{\top}\bm{\beta}\bm{x}^{\top}\bm{W}\bm{g}\bm{g}^{\top}\bm{\beta}\right]\] \[=\alpha^{2}n(d+2)\mathtt{tr}\left(\bm{W}\right)+\gamma^{2}n\mathtt{ tr}\left(\bm{W}\right)\] \[=\left(\alpha^{2}n(d+2)+\gamma^{2}n\right)N_{3}\] \[=A_{3}N_{3}.\]

Here, \((b)\) utilizes the 4'th and 6'th moment results (27) and (30) and we define

\[A_{1} =\alpha^{4}n^{2}(d+4)+\alpha^{2}\gamma^{2}n(2n+d+2)\] \[A_{2} =\alpha^{2}\gamma^{2}n(d+2)+\gamma^{4}n(d+n+1)\] \[A_{3} =\alpha^{2}n(d+2)+\gamma^{2}n.\]

Then combining all together results in

\[\mathcal{L}(\bm{W})=A_{1}N_{1}+A_{2}N_{2}-2A_{3}N_{3}+n\sigma^{2}(\alpha^{2}N _{1}+\gamma^{2}N_{2})+d+\sigma^{2}.\]

To find the optimal solution, set \(\nabla\mathcal{L}(\bm{W})=0\) and we obtain

\[A_{1}\nabla N_{1}+A_{2}\nabla N_{2}-2A_{3}\nabla N_{3}+n\sigma^{2}(\alpha^{2} \nabla N_{1}+\gamma^{2}\nabla N_{2})=0.\] (39)

Note that we have

\[\nabla N_{1} =\nabla\left(\mathtt{tr}\left(\bm{W}\right)^{2}+\mathtt{tr} \left(\bm{W}\bm{W}^{\top}\right)+\mathtt{tr}\left(\bm{W}^{2}\right)\right)=2 \mathtt{tr}\left(\bm{W}\right)\bm{I}+2\bm{W}+2\bm{W}^{\top}\] \[\nabla N_{2} =\nabla\mathtt{tr}\left(\bm{W}\bm{W}^{\top}\right)=2\bm{W}\] \[\nabla N_{3} =\nabla\mathtt{tr}\left(\bm{W}\right)=\bm{I}.\]

Therefore, (39) returns

\[2A_{1}\left(\mathtt{tr}\left(\bm{W}\right)\bm{I}+\bm{W}+\bm{W}^{\top}\right)+ 2A_{2}\bm{W}-2A_{3}+2n\sigma^{2}(\alpha^{2}(\mathtt{tr}\left(\bm{W}\right)\bm{ I}+\bm{W}+\bm{W}^{\top})+\gamma^{2}\bm{W})\bm{I}=0,\] (40)

which implies that the optimal solution \(\bm{W}_{\star}\) has the form of \(c\bm{I}\) for some constant \(c\). Then suppose \(\bm{W}_{\star}=c\bm{I}\), we have \(\mathtt{tr}\left(\bm{W}\right)=cd\) and (40) returns

\[2A_{1}(d+2)c\bm{I}+2A_{2}c\bm{I}-2A_{3}\bm{I}+2n\sigma^{2}(\alpha^{2}(d+2)c \bm{I}+\gamma^{2}c\bm{I})=0\]

\[\implies c =\frac{A_{3}}{A_{1}(d+2)+A_{2}+n\sigma^{2}(\alpha^{2}(d+2)+\gamma^{2})}\] \[=\frac{\alpha^{2}(d+2)+\gamma^{2}}{\alpha^{4}n(d+2)(d+4)+\alpha^ {2}\gamma^{2}(d+2)(d+2n+3)+\gamma^{4}(d+n+1)+\sigma^{2}(\alpha^{2}(d+2)+ \gamma^{2})}.\]

Then the optimal loss is obtained by setting \(\bm{W}_{\star}=c\bm{I}\) and

\[\mathcal{L}_{\star}=\mathcal{L}(\bm{W}_{\star}) =A_{1}c^{2}d(d+2)+A_{2}c^{2}d-2A_{3}cd+n\sigma^{2}c^{2}d(\alpha ^{2}(d+2)+\gamma^{2})+d+\sigma^{2}\] \[=c^{2}d\left(A_{1}(d+2)+A_{2}+n\sigma^{2}(\alpha^{2}(d+2)+\gamma^ {2})\right)-2A_{3}cd+d+\sigma^{2}\] \[=d+\sigma^{2}-A_{3}cd.\]

It completes the proof of (36). Now if assuming \(\alpha=\mathcal{O}\left(1/\sqrt{d}\right)\), \(d/n=\mathcal{O}\left(1\right)\) and sufficiently large dimension \(d\), we have the approximate

\[c \approx\frac{\alpha^{2}d+1}{\alpha^{4}d^{2}n+\alpha^{2}d(d+2n)+(d+ n)+\sigma^{2}(\alpha^{2}d+1)}\] \[=\frac{\alpha^{2}d+1}{(\alpha^{2}d+1)^{2}n+(\alpha^{2}d+1)d+\sigma ^{2}(\alpha^{2}d+1)}\] \[=\frac{1}{(\alpha^{2}d+1)n+d+\sigma^{2}}\]

and

\[\mathcal{L}_{\star}\approx d+\sigma^{2}-\frac{(\alpha^{2}d+1)nd}{(\alpha^{2}d+1 )n+d+\sigma^{2}}.\]

### Task-feature Alignment with \(\alpha\) Correlation

In this section, we consider the task-feature alignment data model similar to (11), where we first draw task vector \(\bm{\beta}\) via

\[\bm{\beta}\sim\mathcal{N}(0,\bm{I}).\]

Then we generate examples \((\bm{x}_{i},y_{i})_{i=1}^{n+1}\) according to the rule \(\text{corr\_coef}(\bm{x}_{i},\bm{\beta})\geq\alpha\geq 0\) via

\[\bm{x}_{i}\,\big{|}\,\bm{\beta}\sim\mathcal{N}(\alpha\bm{\beta},\bm{I}),\quad \xi_{i}\sim\mathcal{N}(0,\sigma^{2})\quad\text{and}\quad y_{i}=\gamma\cdot\bm{x }_{i}^{\top}\bm{\beta}+\xi_{i},\] (41)

which results in (11) by setting \(\gamma^{2}=1/(\alpha^{2}d+1)\).

**Theorem 5** (Extended version of Theorem 3): _Consider linear model as defined in (41). Recap the objective from (5a) and let \(\bm{W}_{\star}:=\arg\min_{\bm{W}}\,\mathcal{L}_{\text{PGD}}(\bm{W})\), and \(\mathcal{L}_{\star}=\mathcal{L}_{\text{PGD}}(\bm{W}_{\star})\). Then \(\bm{W}_{\star}\) and \(\mathcal{L}_{\star}\) satisfy_

\[\bm{W}_{\star}=c\bm{I}\qquad\text{and}\qquad\mathcal{L}_{\star}=d\gamma^{2}( \Delta_{0}\alpha^{2}+1)+\sigma^{2}-cnd\gamma^{2}(\Delta_{1}\alpha^{4}+2\Delta_ {0}\alpha^{2}+1)\] (42)

_where_

\[c=\frac{\Delta_{1}\alpha^{4}+2\Delta_{0}\alpha^{2}+1}{\Delta_{2}\alpha^{6}+ \Delta_{3}\alpha^{4}+\Delta_{4}\alpha^{2}+(d+n+1)+\sigma^{2}(\Delta_{0}\alpha^ {4}+2\alpha^{2}+1)/\gamma^{2}}\]

_and_

\[\begin{cases}\Delta_{0}=d+2\\ \Delta_{1}=(d+2)(d+4)\\ \Delta_{2}=(d+2)(d+4)(d+6)n\\ \Delta_{3}=(d+2)(d+4)(3n+4)\\ \Delta_{4}=(d+2)(3n+d+3)+(d+8).\end{cases}\]

_Suppose \(\alpha=\mathcal{O}\left(1/\sqrt{d}\right)\), \(d/n=\mathcal{O}\left(1\right)\) and \(d\) is sufficiently large. Let \(\kappa=\alpha^{2}d+1\) and \(\gamma^{2}=1/\kappa\). Then \(\bm{W}_{\star}\) and \(\mathcal{L}_{\star}\) have approximate forms_

\[\bm{W}_{\star}\,\approx\frac{1}{\kappa n+(d+\sigma^{2})/\kappa}\qquad\text{ and}\qquad\mathcal{L}_{\star}\,\approx d+\sigma^{2}-\frac{\kappa nd}{\kappa n+(d+ \sigma^{2})/\kappa}.\] (43)

**Proof.** Here, for clean notation and without loss of generality, we define and rewrite (41) via

\[\bm{g}_{i}\sim\mathcal{N}(0,\bm{I}),\quad\xi_{i}\sim\mathcal{N}(0,\sigma^{2}) \quad\text{and}\quad\bm{x}_{i}=\alpha\bm{\beta}+\bm{g}_{i},\quad y_{i}=\gamma \bm{x}_{i}^{\top}\bm{\beta}+\xi_{i}=\gamma\cdot(\alpha\bm{\beta}+\bm{g}_{i})^{ \top}\bm{\beta}+\xi_{i}.\]

Recap the loss function from (5a), we obtain

\[\mathcal{L}(\bm{W}) =\mathbb{E}\left[(y-g(\bm{Z}))^{2}\right]\] \[=\mathbb{E}\left[(\gamma\bm{x}^{\top}\bm{\beta}+\xi-\bm{x}^{\top }\bm{W}\bm{X}^{\top}(\gamma\bm{X}\bm{\beta}+\xi))^{2}\right]\] \[=\mathbb{E}\left[\gamma^{2}(\bm{x}^{\top}\bm{\beta}-\bm{x}^{\top }\bm{W}\bm{X}^{\top}\bm{X}\bm{\beta})^{2}+2\gamma(\bm{x}^{\top}\bm{\beta}-\bm{x }^{\top}\bm{W}\bm{X}^{\top}\bm{X}\bm{\beta})(\xi-\bm{x}^{\top}\bm{W}\bm{X}^{ \top}\bm{\xi})+(\xi-\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})^{2}\right]\] \[=\gamma^{2}\,\mathbb{E}\left[(\bm{x}^{\top}\bm{\beta}-\bm{x}^{\top }\bm{W}\bm{X}^{\top}\bm{X}\bm{\beta})^{2}\right]+\mathbb{E}\left[(\bm{x}^{ \top}\bm{W}\bm{X}^{\top}\bm{\xi})^{2}\right]+\sigma^{2}.\] (44)

Similar to Appendix B.3, to begin with, let

\[N_{1}=\texttt{tr}\left(\bm{W}\right)^{2}+\texttt{tr}\left(\bm{W}\bm{W}^{\top} \right)+\texttt{tr}\left(\bm{W}^{2}\right),\quad N_{2}=\texttt{tr}\left(\bm{W} \bm{W}^{\top}\right),\quad\text{and}\quad N_{3}=\texttt{tr}\left(\bm{W}\right),\]

and additionally, given \(\bm{\Lambda}_{\bm{W}}=\bm{W}\odot\bm{I}\), let

\[N_{4}=3\texttt{tr}\left(\bm{\Lambda}_{\bm{W}}^{2}\right)+(d+4)\texttt{tr} \left(\bm{W}\bm{W}^{\top}\right)+\texttt{tr}\left(\bm{W}^{2}\right).\]

We first focus on the second term in (44)

\[\mathbb{E}\left[(\bm{x}^{\top}\bm{W}\bm{X}^{\top}\bm{\xi})^{2}\right] =\mathbb{E}\left[\left((\alpha\bm{\beta}+\bm{g})^{\top}\bm{W}\sum_ {i=1}^{n}\xi_{i}(\alpha\bm{\beta}+\bm{g}_{i})\right)^{2}\right]\] \[=n\sigma^{2}\,\mathbb{E}\left[\left((\alpha\bm{\beta}+\bm{g})^{ \top}\bm{W}(\alpha\bm{\beta}+\bm{g}^{\prime})\right)^{2}\right]\] \[=n\sigma^{2}\left(\alpha^{4}\,\mathbb{E}\left[(\bm{\beta}^{\top} \bm{W}\bm{\beta})^{2}\right]+2\alpha^{2}\,\mathbb{E}\left[(\bm{\beta}^{\top} \bm{W}\bm{g}^{\prime})^{2}\right]+\mathbb{E}\left[(\bm{g}^{\top}\bm{W}\bm{g}^{ \prime})^{2}\right]\right)\] \[=n\sigma^{2}\left(\alpha^{4}\left(\texttt{tr}\left(\bm{W}\right)^{2 }+\texttt{tr}\left(\bm{W}^{2}\right)+\texttt{tr}\left(\bm{W}\bm{W}^{\top}\right) \right)+(2\alpha^{2}+1)\texttt{tr}\left(\bm{W}\bm{W}^{\top}\right)\right)\] \[=n\sigma^{2}\left(\alpha^{4}N_{1}+(2\alpha^{2}+1)N_{2}\right).\quad \text{(It follows (\ref{eq:2}) and independence of $\bm{\beta},\bm{g},\bm{g}^{\prime}$.)}\]

[MISSING_PAGE_EMPTY:29]

\[(b): \mathbb{E}\left[\left(\bm{g}^{\top}\bm{W}\bm{X}^{\top}\bm{X}\bm{ \beta}\right)^{2}\right]\] (49) \[=\mathbb{E}\left[\left(\sum_{i=1}^{n}\alpha^{2}\bm{g}^{\top}\bm{W} \beta\bm{\beta}^{\top}\beta+\alpha\bm{g}^{\top}\bm{W}\beta\bm{g}_{i}^{\top}\beta+ \alpha\bm{g}^{\top}\bm{W}\bm{g}_{i}\beta^{\top}\beta+\bm{g}^{\top}\bm{W}\bm{g} _{i}\beta^{\top}\beta\right)^{2}\right]\] \[=\alpha^{4}n^{2}\,\mathbb{E}\left[\left(\bm{g}^{\top}\bm{W}\beta \bm{\beta}^{\top}\beta\right)^{2}\right]+\alpha^{2}\,\mathbb{E}\left[\left(\sum _{i=1}^{n}\bm{g}^{\top}\bm{W}\bm{g}_{i}^{\top}\beta\right)^{2}\right]+\alpha^ {2}\,\mathbb{E}\left[\left(\sum_{i=1}^{n}\bm{g}^{\top}\bm{W}\bm{g}_{i}\beta^{ \top}\beta\right)^{2}\right]+\mathbb{E}\left[\left(\sum_{i=1}^{n}\bm{g}^{\top} \bm{W}\bm{g}_{i}\beta^{\top}\beta\right)^{2}\right]\] \[+2\alpha^{2}n\,\mathbb{E}\left[\sum_{i=1}^{n}\bm{g}^{\top}\bm{W} \beta\bm{\beta}^{\top}\bm{W}\bm{g}^{\top}\bm{W}\bm{g}_{i}^{\top}\beta\right]+2 \alpha^{2}\,\mathbb{E}\left[\sum_{i=1}^{n}\bm{g}^{\top}\bm{W}\bm{g}_{i}^{\top} \beta\bm{g}^{\top}\bm{W}\bm{g}_{i}\beta^{\top}\beta\right]\] \[=\alpha^{4}n^{2}\,\mathbb{E}\left[\left(\bm{g}^{\top}\bm{W}\beta \bm{\beta}^{\top}\beta\right)^{2}\right]+\alpha^{2}n\,\mathbb{E}\left[\left( \bm{g}^{\top}\bm{W}\beta\bm{g}^{\top}\beta\right)^{2}\right]+\alpha^{2}n\, \mathbb{E}\left[\left(\bm{g}^{\top}\bm{W}\bm{g}^{\top}\beta\right)^{2}\right] +\mathbb{E}\left[\left(\sum_{i=1}^{n}\bm{g}^{\top}\bm{W}\bm{g}_{i}\beta^{\top} \beta\right)^{2}\right]\] \[+2\alpha^{2}n^{2}\,\mathbb{E}\left[\bm{g}^{\top}\bm{W}\beta\bm{ \beta}^{\top}\beta\bm{g}^{\top}\bm{W}\bm{g}^{\top}\beta\right]+2\alpha^{2}n \,\mathbb{E}\left[\bm{g}^{\top}\bm{W}\bm{g}_{i}^{\top}\beta\bm{g}^{\top}\bm{W }\bm{g}_{i}\beta^{\top}\beta\right]\] \[=\alpha^{4}n^{2}(d+2)(d+4)N_{2}+\alpha^{2}n(d+2)N_{2}+\alpha^{2}nd (d+2)N_{2}+n(d+n+1)N_{2}\] \[\quad+2\alpha^{2}n^{2}(d+2)N_{2}+2\alpha^{2}n(d+2)N_{2}\] (50) \[=\left(\alpha^{2}n(d+2)(\alpha^{2}n(d+4)+2n+d+3)+n(d+n-1)\right)N _{2}\] \[=B_{3}N_{2},\]

where (49) and (50) are obtained using (27), (30) and

\[\mathbb{E}\left[\left(\sum_{i=1}^{n}\bm{g}^{\top}\bm{W}\bm{g}_{i} \bm{g}_{i}^{\top}\bm{\beta}\right)^{2}\right] =n\,\mathbb{E}\left[\left(\bm{g}^{\top}\bm{W}\bm{g}^{\prime}\bm{ g}^{\prime\top}\bm{\beta}\right)^{2}\right]+n(n-1)\,\mathbb{E}\left[\bm{g}^{\top}\bm{W}\bm{g}^{ \prime}\bm{g}^{\prime\top}\bm{\beta}\bm{g}^{\top}\bm{W}\bm{g}^{\prime\prime} \bm{g}^{\prime\prime}\bm{\beta}\right]\] \[=n(d+2)N_{2}+n(n-1)N_{2}=n(n+d+1)N_{2}.\]

\[(c): \mathbb{E}\left[\bm{\beta}^{\top}\bm{\beta}\bm{\beta}^{\top}\bm{ W}\bm{X}^{\top}\bm{X}\bm{\beta}\right]\] \[=n\,\mathbb{E}\left[\bm{\beta}^{\top}\bm{\beta}\bm{\beta}^{\top} \bm{W}(\alpha\bm{\beta}+\bm{g}^{\prime})(\alpha\bm{\beta}+\bm{g}^{\prime})^{ \top}\bm{\beta}\right]\] \[=\alpha^{2}n\,\mathbb{E}\left[\bm{\beta}^{\top}\bm{\beta}\bm{ \beta}^{\top}\bm{W}\bm{\beta}\bm{\beta}^{\top}\bm{\beta}\right]+n\,\mathbb{E} \left[\bm{\beta}^{\top}\bm{\beta}\bm{\beta}^{\top}\bm{W}\bm{g}^{\prime}\bm{g}^{ \prime\top}\bm{\beta}\right]\] \[=\alpha^{2}n(d+2)(d+4)\mathrm{tr}\left(\bm{W}\right)+n(d+2) \mathrm{tr}\left(\bm{W}\right)\] \[=\left(\alpha^{2}n(d+2)(d+4)+n(d+2)\right)N_{3}\] \[=B_{4}N_{3}.\]

\[(d): \mathbb{E}\left[\bm{\beta}^{\top}\bm{g}\bm{g}^{\top}\bm{W}\bm{X}^{ \top}\bm{X}\bm{\beta}\right]\] \[=n\,\mathbb{E}\left[\bm{\beta}^{\top}\bm{g}\bm{g}^{\top}\bm{W}( \alpha\bm{\beta}+\bm{g}^{\prime})(\alpha\bm{\beta}+\bm{g}^{\prime})^{\top}\bm{ \beta}\right]\] \[=\alpha^{2}n\,\mathbb{E}\left[\bm{\beta}^{\top}\bm{g}\bm{g}^{\top }\bm{W}\bm{\beta}\bm{\beta}^{\top}\bm{\beta}\right]+n\,\mathbb{E}\left[\bm{ \beta}^{\top}\bm{g}\bm{g}^{\top}\bm{W}\bm{g}^{\prime}\bm{g}^{\prime\top}\bm{ \beta}\right]\] \[=\alpha^{2}n(d+2)\mathrm{tr}\left(\bm{W}\right)+n\mathrm{tr}\left( \bm{W}\right)\] \[=\left(\alpha^{2}n(d+2)+n\right)N_{3}\] \[=B_{5}N_{3}.\]

Here we define

\[B_{1} =\alpha^{2}n(d+4)(\alpha^{2}n(d+6)+2n+3)+n(n-1)\] \[B_{2} =\alpha^{2}n(d+2)(d+4)\] \[B_{3} =\alpha^{2}n(d+2)(\alpha^{2}n(d+4)+2n+d+3)+n(d+n-1)\] \[B_{4} =\alpha^{2}n(d+2)(d+4)+n(d+2)\] \[B_{5} =\alpha^{2}n(d+2)+n.\]

Then combining all together results in

\[\mathcal{L}(\bm{W}) =\gamma^{2}\left(\alpha^{2}d(d+2)+d+\alpha^{2}(B_{1}N_{1}+B_{2}N_{2 }+nN_{4})+B_{3}N_{2}-2\alpha^{2}B_{4}N_{3}-2B_{5}N_{3}\right)+n\sigma^{2}( \alpha^{4}N_{1}+(2\alpha^{2}+1)N_{2})+\sigma^{2}\] \[=\gamma^{2}\left(\alpha^{2}B_{1}N_{1}+(\alpha^{2}B_{2}+B_{3})N_{2 }-2(\alpha^{2}B_{4}+B_{5})N_{3}+\alpha^{2}nN_{4}\right)+n\sigma^{2}(\alpha^{4}N _{1}+(2\alpha^{2}+1)N_{2})+\gamma^{2}d\left(\alpha^{2}(d+2)+1\right)+\sigma^{2}\]and differentiating it results in

\[\nabla\mathcal{L}(W)=\gamma^{2}\left(\alpha^{2}B_{1}\nabla N_{1}+(\alpha^{2}B_{2} +B_{3})\nabla N_{2}-2(\alpha^{2}B_{4}+B_{5})\nabla N_{3}+\alpha^{2}n\nabla N_{4} \right)+n\sigma^{2}(\alpha^{4}\nabla N_{1}+(2\alpha^{2}+1)\nabla N_{2}).\]

Similar to the proof in Appendix B.3, \(\bm{W}_{\star}\) has the form of \(\bm{W}_{\star}=c\bm{I}\) and we have

\[\nabla N_{1} =\nabla\left(\mathtt{tr}\left(\bm{W}\right)^{2}+\mathtt{tr}\left( \bm{W}\bm{W}^{\top}\right)+\mathtt{tr}\left(\bm{W}^{2}\right)\right)=2\mathtt{ tr}\left(\bm{W}\right)\bm{I}+2\bm{W}+2\bm{W}^{\top}=2c(d+2)\bm{I}\] \[\nabla N_{2} =\nabla\mathtt{tr}\left(\bm{W}\bm{W}^{\top}\right)=2\bm{W}=2c \bm{I}\] \[\nabla N_{3} =\nabla\mathtt{tr}\left(\bm{W}\right)=\bm{I}\] \[\nabla N_{4} =\nabla\left(3\mathtt{tr}\left(\bm{\Lambda}_{W}^{2}\right)+(d+4) \mathtt{tr}\left(\bm{W}\bm{W}^{\top}\right)+\mathtt{tr}\left(\bm{W}^{2}\right)\right)\] \[=6\cdot\mathrm{diag}\left(\bm{\Lambda}_{W}\right)+2(d+4)\bm{W}+2 \bm{W}^{\top}\] \[=2c(d+8)\bm{I}.\]

Therefore, setting \(\nabla\mathcal{L}(W)=0\) returns

\[\gamma^{2}\left(2c(d+2)\alpha^{2}B_{1}+2c(\alpha^{2}B_{2}+B_{3})-2(\alpha^{2} B_{4}+B_{5})+2c(d+8)\alpha^{2}n\right)+2cn\sigma^{2}(\alpha^{4}(d+2)+2\alpha^{2}+1)=0\]

\[\Longrightarrow c =\frac{\alpha^{2}B_{4}+B_{5}}{(d+2)\alpha^{2}B_{1}+(\alpha^{2}B_ {2}+B_{3})+(d+8)\alpha^{2}n+n\sigma^{2}(\alpha^{4}(d+2)+2\alpha^{2}+1)/\gamma^ {2}}\] \[=\frac{\alpha^{4}n(d+2)(d+4)+2\alpha^{2}n(d+2)+n}{\alpha^{4}n^{2} (d+2)(d+4)(3n+4)+\alpha^{2}n(d+2)(3n+d+3)+(d+8))+n(d+n+1)+n\sigma^{2}(\alpha^ {4}(d+2)+2\alpha^{2}+1)/\gamma^{2}}\] \[=\frac{\alpha^{4}(d+2)(d+4)+2\alpha^{2}(d+2)+1}{\alpha^{4}n(d+2) (d+4)(d+6)+\alpha^{4}(d+2)(d+4)(3n+4)+\alpha^{2}((d+2)(3n+d+3)+(d+8))+(d+n+1)+ \sigma^{2}(\alpha^{4}(d+2)+2\alpha^{2}+1)/\gamma^{2}}.\]

Then the optimal loss is obtained by setting \(\bm{W}_{\star}=c\bm{I}\) and

\[\mathcal{L}_{\star}=\mathcal{L}(\bm{W}_{\star})=\gamma^{2}d(\alpha^{2}(d+2)+1) +\sigma^{2}-\gamma^{2}(\alpha^{2}B_{4}+B_{5})cd.\]

It completes the proof of (42). Now if assuming \(\alpha=\mathcal{O}\left(1/\sqrt{d}\right),d/n=\mathcal{O}\left(1\right)\), \(\gamma^{2}=1/(\alpha^{2}d+1)\) and sufficiently large dimension \(d\), we have the approximate

\[c \approx\frac{\alpha^{4}d^{2}+2\alpha^{2}d+1}{n\alpha^{6}d^{3}+3n \alpha^{4}d^{2}+(3n+d)\alpha^{2}d+d+n+\sigma^{2}(\alpha^{4}d+2\alpha^{2}+1)/ \gamma^{2}}\] \[\approx\frac{(\alpha^{2}d+1)^{2}}{n(\alpha^{2}d+1)^{3}+d(\alpha ^{2}d+1)+\sigma^{2}(\alpha^{2}d+1)}\] \[\approx\frac{1}{(\alpha^{2}d+1)n+(d+\sigma^{2})/(\alpha^{2}d+1)}\]

and

\[\mathcal{L}_{\star} \approx\gamma^{2}d(\alpha^{2}d+1)+\sigma^{2}-\frac{\gamma^{2}( \alpha^{2}d+1)^{2}nd}{(\alpha^{2}d+1)n+(d+\sigma^{2})/(\alpha^{2}d+1)}\] \[=d+\sigma^{2}-\frac{(\alpha^{2}d+1)nd}{(\alpha^{2}d+1)n+(d+\sigma ^{2})/(\alpha^{2}d+1)}.\]

## Appendix C Analysis of Low-Rank Parameterization

### Proof of Lemma 3

Proof.: Recall the loss function from (34)

\[\mathcal{L}(\bm{W})=M-2n\mathtt{tr}\left(\bm{\Sigma}\bm{\tilde{W}}\right)+n(n+ 1)\mathtt{tr}\left(\bm{\Sigma}\bm{\tilde{W}}^{\top}\bm{\tilde{W}}\right)+nM \mathtt{tr}\left(\bm{\tilde{W}}\bm{\tilde{W}}^{\top}\right)\]

where \(\bm{\tilde{W}}=\bm{\Sigma}_{x}^{1/2}\bm{W}\bm{\Sigma}_{x}^{1/2}\), \(\bm{\Sigma}=\bm{\Sigma}_{x}^{1/2}\bm{\Sigma}_{\bm{\tilde{p}}}\bm{\Sigma}_{x}^{1 /2}\) and \(M=\mathtt{tr}\left(\bm{\Sigma}\right)+\sigma^{2}\). For any \(\bm{\tilde{W}}\), let us parameterize \(\bm{\tilde{W}}=\bm{U}\bm{E}\bm{U}^{\top}\) where \(\bm{U}\in\mathbb{R}^{d\times r}\) denotes the eigenvectors of \(\bm{\tilde{W}}\) and \(\bm{E}\in\mathbb{R}^{r\times r}\) is a symmetric square matrix. We will first treat \(\bm{U}\) as fixed and optimize \(\bm{E}\). We will then optimize \(\bm{U}\). Fixing \(\bm{U}\), setting \(\bm{\bar{\Sigma}}=\bm{U}^{\top}\bm{\Sigma}\bm{U}\), we obtain

\[\mathcal{L}(\bm{E})=M-2n\texttt{tr}\left(\bm{\bar{\Sigma}}\bm{E}\right)+n(n+1) \texttt{tr}\left(\bm{\bar{\Sigma}}\bm{E}^{2}\right)+nM\texttt{tr}\left(\bm{E} ^{2}\right).\]

Differentiating, we obtain

\[0.5n^{-1}\nabla\mathcal{L}(\bm{E})=-\bm{\bar{\Sigma}}+(n+1)\bm{\bar{\Sigma}} \bm{E}+M\bm{E}.\]

Setting \(\nabla\mathcal{L}(\bm{E})=0\) returns

\[\bm{E}_{\star}=(M\bm{I}+(n+1)\bm{\bar{\Sigma}})^{-1}\bm{\bar{\Sigma}}.\] (51)

Let \(\bar{\lambda}_{i}\) denote the \(i\)'th largest eigenvalue of \(\bm{\bar{\Sigma}}\). Plugging in this value, we obtain the optimal risk as a function of \(\bm{U}\) is given by

\[\mathcal{L}_{\star}(\bm{U}) =M-n\cdot\texttt{tr}\left(\bm{\bar{\Sigma}}\bm{E}_{\star}\right)=M -n\cdot\texttt{tr}\left((MI+(n+1)\bm{\bar{\Sigma}})^{-1}\bm{\bar{\Sigma}}^{2}\right)\] (52) \[=M-n\sum_{i=1}^{r}\frac{\bar{\lambda}_{i}^{2}}{(n+1)\bar{\lambda} _{i}+M}=M-n\sum_{i=1}^{r}\frac{\bar{\lambda}_{i}}{n+1+M\bar{\lambda}_{i}^{-1}}.\] (53)

Now observe that, the right hand side is strictly decreasing function of the eigenvalues \(\bar{\lambda}_{i}\) of \(\bm{\bar{\Sigma}}=\bm{U}^{\top}\bm{\Sigma}\bm{U}\). Thus, to minimize \(\mathcal{L}_{\star}(\bm{U})\), we need to maximize \(\sum_{i=1}^{r}\frac{\bar{\lambda}_{i}}{n+1+M\bar{\lambda}_{i}^{-1}}\). It follows from Cauchy interlacing theorem that \(\bar{\lambda}_{j}\leq\lambda_{i}\) where \(\lambda_{i}\) is the \(i\)'th largest eigenvalue of \(\bm{\Sigma}\) since \(\bm{\bar{\Sigma}}\) is an orthogonal projection of \(\bm{\Sigma}\) on \(\bm{U}\). Consequently, we find the desired bound where

\[\mathcal{L}_{\star}=M-n\sum_{i=1}^{r}\frac{\lambda_{i}}{n+1+M\bar{\lambda}_{i }^{-1}}.\]

The equality holds by setting \(\bm{U}\) to be the top-\(r\) eigenvectors of \(\bm{\Sigma}\) and \(\bm{E}=\bm{E}_{\star}(\bm{U})\) to be the diagonal matrix according to (51). 

## Appendix D Additional Experiments

In this section, we present additional experiments demonstrating that the H3 model can outperform the linear attention model under different training or data settings. The implementation details are consistent with those outlined in Section 4.

\(\bullet\)**H3 outperforms linear attention (Figure 4).** Until now, our analysis has established the equivalence between linear attention and H3 models in solving linear ICL problem. Furthermore, we also investigate settings where H3 could outperform linear attention due to its sample weighting ability. In Figs. 3(a) and 3(b), instead of training separate models to fit the different context lengths, we train a single model with fixed max-length \(n_{\text{max}}\) and loss is evaluated as the average loss given samples from 1 to \(n_{\text{max}}\). Such setting has been wildly studied in the previous ICL work [Garg et al., 2022,

Figure 4: Further comparison for linear attention and H3. In **(a)** and **(b)**, given maximum context lengths \(n_{\text{max}}\), we train linear attention and H3 models to minimize the average loss across all positions \(n\) from 1 to \(n_{\text{max}}\). Averaged test risks are presented in **(c)**. In **(d)**, the task vector \(\bm{\beta}\) evolves gradually over the context positions \(i\leq n\) via \(\bm{\beta}_{i}=(i/n)\bm{\beta}_{1}+(1-i/n)\bm{\beta}_{2}\). In both scenarios, H3 outperforms linear attention benefiting from its additional convolutional filter (c.f. \(\bm{f}\) in (2b)). Implementation details are discussed in Section 4.

Akyurek et al., 2023; Li et al., 2023). We generate data according to (7) with \(\bm{\Sigma}_{\bm{x}}=\bm{\Sigma}_{\bm{\beta}}=\bm{I}_{d}\) and \(\sigma=0\), and train 1-layer linear attention (Fig. 4a) and H3 (Fig. 4b) models with different max-lengths \(n_{\text{max}}=30,50,80\). Comparison between Fig. 4a and 4b shows that 1-layer attention and H3 implement different algorithms in solving the averaged linear regression problem and H3 is more consistent in generalizing to longer context lengths. In Fig. 4c, we plot the averaged risks for each model and H3 outperforms linear attention. Furthermore, in Fig. 4d, we focus on the setting where in-context examples are generated using evolving task vector \(\bm{\beta}\). Specifically, consider that each sequence corresponds to two individual task parameters \(\bm{\beta}^{(1)}\sim\mathcal{N}(0,\bm{I}_{d})\) and \(\bm{\beta}^{(2)}\sim\mathcal{N}(0,\bm{I}_{d})\). Then the \(i\)'th sample is generated via \(\bm{x}_{i}\sim\mathcal{N}(0,\bm{I}_{d})\) and \(y_{i}=\bm{\beta}_{i}^{\top}\bm{x}_{i}\) where \(\bm{\beta}_{i}=\lambda_{i}\bm{\beta}^{(1)}+(1-\lambda_{i})\bm{\beta}^{(2)}\) and \(\lambda_{i}=i/n\). The results are reported in Fig. 4d which again shows that H3 achieves better performance compared to linear attention, as H3 may benefit from the additional convolutional filter (c.f. \(\bm{f}\) in (2b)). Here, dotted curve represent the theoretical results under i.i.d. and noiseless setting, derived from Corollary 1.

## Appendix E Extended Related Work

There is growing interest in understanding the mechanisms behind ICL (Brown et al., 2020; Liu et al., 2023; Rae et al., 2021) in large language models (LLMs) due to its success in continuously enabling novel applications for LLMs (GeminiTeam et al., 2023; OpenAI, 2023; Touvron et al., 2023). Towards this, Xie et al. (2022) explain ICL by language model's ability to perform implicit Bayesian inference where, under specific assumptions on the pre-training data distribution, the model infers a shared latent concept among the in-context examples and leverages the concept to make a prediction. Muller et al. (2021); Hollmann et al. (2022); Muller et al. (2023) introduce prior-data fitted network (PFN) to approximate Bayesian inference on synthetic datasets and use it to perform downstream tasks such as tabular dataset classification. On the other hand, Olsson et al. (2022) posit induction heads as the key mechanism enabling ICL in Transformers. Park et al. (2024) study how various distributional properties of training data aid in the emergence of ICL in Transformers.

In the previous work, Garg et al. (2022) explored ICL ability of Transformers. In particular, they considered in-context prompts where each in-context example is labeled by a target function from a given function class, including linear models. A number of works have studied this and related settings to develop a theoretical understanding of ICL (von Oswald et al., 2023; Gatmiry et al., Collins et al., 2024; Lin and Lee, 2024; Li et al., 2024; Bai et al., 2024; Akyurek et al., 2023; Zhang et al., 2023; Du et al., 2023). Akyurek et al. (2023) focus on linear regression and provide a construction of Transformer weights that can enable a single step of GD based on in-context examples. They further show that Transformers trained on in-context prompts exhibit behaviors similar to the models recovered via explicit learning algorithm on the in-context examples in a prompt. Along the similar line, Von Oswald et al. (2023) provide a construction of weights in linear attention-only Transformers that can emulate GD steps on in-context examples for a linear regression task. Interestingly, they find similarity between their constructed networks and the networks resulting from training on in-context prompts corresponding to linear regression tasks. Similar to this line of work, Dai et al. (2023) argue that pre-trained language models act as meta-optimizer which utilize attention to apply meta-gradients to the original language model based on the in-context examples. Focusing on various NLP tasks, they further connect it to a specific form of explicit fine-tuning that performs gradient updates to the attention-related parameters. Inspired by the connection between linear attention and GD, they developed a novel attention mechanism that mirrors the behavior of GD with momentum. Beyond Transformers, existing work (Lee et al., 2023; Zucchet et al., 2023; Grazzi et al., 2024) demonstrate that other model architectures, such as SSM and RNNs, are also capable of in-context learning (ICL).

Building on these primarily empirical studies, Zhang et al. (2024); Mahankali et al. (2024); Ahn et al. (2023); Duraisamy (2024) focus on developing a theoretical understanding of Transformers trained to perform ICL. For single-layer linear attention model trained on in-context prompts for random linear regression tasks with isotropic Gaussian features and isotropic Gaussian weight vectors, Mahankali et al. (2024); Ahn et al. (2023) show that the resulting model implements a single step of GD on in-context examples in a test prompt, thereby corroborating the findings of (Von Oswald et al., 2023). They also show that the learned model implements a PGD step, when faced with anisotropic Gaussian features, with Mahankali et al. (2024) also considering anisotropic Gaussian weight vectors. Ahn et al. (2023) further study multi-layer model and show that the trained model can implement a generalization of GD++ algorithm, supporting an empirical observation in Von Oswald et al. (2023). On the other hand, Mahankali et al. (2024) extend their single-layer setup to consider suitable non-linear target functions, showing that learned Transformer again implements a single step of GD on lineare regression objective. For a single-layer linear attention model, Zhang et al. (2024) study the optimization dynamics of gradient flow while training such a model on in-context prompts for random linear regression tasks. Despite the non-convexity of the underlying problem, they show the convergence to the global minimum of the population objective. Similar to Mahankali et al. (2024), Ahn et al. (2023), they show that the trained model implements a single step of GD and PGD for isotropic and anisotropic Gaussian features, respectively. In addition, they also characterize the test-time prediction error for the trained model while highlighting its dependence on train and test prompt lengths. Interestingly, Zhang et al. (2024) further explore the effect of various distributional shifts, including the shift in task weight vector distributions between train and test time as well as the covariate shifts among train and test in-context prompts. Interestingly, they find that while linear-attention models are robust to most shifts, they exhibit brittleness to the covariate shifts.

While our work shares similarities with this line of works, as discussed in our contributions in the introduction, we expand the theoretical understanding of ICL along multiple novel dimensions, which includes the first study of LoRA adaptation for ICL in the presence of a distributional shift. Furthermore, we strive to capture the effect of retrieval augmentation (Lewis et al., 2020, Nakano et al., 2021) on ICL through our analysis. Retrieval augmentation allows for selecting most relevant demonstration out of a large collection for a test instance, e.g., via a dense retrieval model (Izacard et al., 2023), which can significantly outperform the typical ICL setup where fixed task-specific demonstrations are provided as in-context examples (Wang et al., 2022, Basu et al., 2023). Through a careful modeling of retrieval augmentation via correlated design, we show that it indeed has a desirable amplification effect where the effective number in-context examples becomes larger with higher correlation which corresponds to preforming a successful retrieval of query-relevant demonstrations in a practical retrieval augmented setup.

Recently, state space models (SSMs) (Gu et al., 2021, 2021, Fu et al., 2023, Gu and Dao, 2023) have appeared as potential alternatives to Transformer architecture, with more efficient scaling to input sequence length. Recent studies demonstrate that such SSMs can also perform ICL for simple non-language tasks (Park et al., 2024, Grazzi et al., 2024) as well as complex NLP tasks (Grazzi et al., 2024). That said, a rigorous theoretical understanding of ICL for SSMs akin to Zhang et al. (2024), Mahankali et al. (2024), Ahn et al. (2023) is missing from the literature. In this work, we provide the first such theoretical treatment for ICL with SSMs. Focusing on H3 architecture (Fu et al., 2023), we highlight its advantages over linear attention in specific ICL settings.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: **All the theoretical contributions claimed in the abstract and introduction along with the underlying data model are presented in Section 2 and Section 3.** Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: **This is a theoretical study which (similar to prior studies in the field) relies on a precise but simplified data model to draw quantitatively precise conclusions. All the assumptions on the data model are clearly stated in Section 2 and Section 3. We have also added a paragraph after the conclusion to specifically highlight various limitations of our work.** Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: **As discussed above, for all of our theoretical results and proofs we state the precise setup and assumptions in Section 2 and Section 3. Due to page limit, proofs are deferred to the supplemental material.** Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: **This is primarily a theoretical work where detailed synthetic experiments (on the same data model studied in our theoretical analysis) have been conducted to corroborate our theoretical findings. We provide sufficient details in Section 4 for reproducing these experiments.** Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: **As discussed above, this paper conducts small scale synthetic experiments to corroborate our theoretical findings. We have provided sufficient details to reproduce these experiments in Section 4.** Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: **All the relevant details for our small scale experiments are provided in Section 4.** Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No]Justification: **This work is a theoretical work studying the optimization landscape of linear attention/H3 under population risk. Then our goal of simulations is to find the optimal solution corresponding to the minimal risks. Therefore, we do not report the error bars and we have included the discussion in the experiment section.** Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: **Our work only focuses on 1-layer attention/H3 model training with hidden dimension \(21\) and maximal context length \(<100\), which can be implemented easily.** Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: **Yes, the authors confirm that the research conducted in the paper conform with the NeurIPS Code of Ethics.** Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: **In its current form, we don't see any specific negative impacts of our theoretical study. However, we have discussed potential broader impacts of the future extensions of this work, e.g., the ones tied to eliciting undesirable behavior of LLMs with in-context learning, while discussing the limitations of the work after the conclusion section.** Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: **The synthetic setup studied in the paper does not pose such risks.** Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets**Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: **The paper does not rely on existing assets.** Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer:[NA] Justification: **The paper does not release new assets such as code, data, or models.** Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: **The paper does not involve crowdsourcing nor research with human subjects.** Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: **The paper does not involve crowdsourcing nor research with human subjects.** Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.