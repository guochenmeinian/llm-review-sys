# Federated Natural Policy Gradient and Actor Critic Methods for Multi-task Reinforcement Learning

 Tong Yang

CMU

Shicong Cen

CMU

Yuting Wei

UPenn

Yuxin Chen

UPenn

Yuejie Chi

CMU

Department of Electrical and Computer Engineering, Carnegie Mellon University; email: shi.cong@andrew.cmu.edu. Department of Statistics and Data Science, Wharton School, University of Pennsylvania; email: ytwei@wharton.upenn.edu. Department of Statistics and Data Science, Wharton School, University of Pennsylvania; email: yuxinc@wharton.upenn.edu. Department of Electrical and Computer Engineering, Carnegie Mellon University; email: yuejiechi@cmu.edu.

###### Abstract

Federated reinforcement learning (RL) enables collaborative decision making of multiple distributed agents without sharing local data trajectories. In this work, we consider a multi-task setting, in which each agent has its own private reward function corresponding to different tasks, while sharing the same transition kernel of the environment. Focusing on infinite-horizon Markov decision processes, the goal is to learn a globally optimal policy that maximizes the sum of the discounted total rewards of all the agents in a decentralized manner, where each agent only communicates with its neighbors over some prescribed graph topology. We develop federated vanilla and entropy-regularized natural policy gradient (NPG) methods in the tabular setting under softmax parameterization, where gradient tracking is applied to estimate the global Q-function to mitigate the impact of imperfect information sharing. We establish non-asymptotic global convergence guarantees under exact policy evaluation, where the rates are nearly independent of the size of the state-action space and illuminate the impacts of network size and connectivity, and further establish its robustness against inexact policy evaluation. We further propose a federated natural actor critic (NAC) method for multi-task RL with function approximation and stochastic policy evaluation, and establish its finite-time sample complexity taking the errors of function approximation into account. To the best of our knowledge, this is the first time that near dimension-free global convergence is established for federated multi-task RL using policy optimization.

## 1 Introduction

Federated reinforcement learning (FRL) is an emerging paradigm that combines the advantages of federated learning (FL) and reinforcement learning (RL) [2, 19], allowing multiple agents to learn a shared policy from local experiences, without exposing their private data to a central server nor other agents. FRL is poised to enable collaborative and efficient decision making in scenarios where data is distributed, heterogeneous, and sensitive, which arise frequently in applications such as edge computing, smart cities, and healthcare [23, 24, 25], to name just a few. As has been observed [19], decentralized training can lead to performance improvements in FL by avoiding communication congestions at busy nodes such as the server, especially under high-latency scenarios. This motivates us to design algorithms for the _fully decentralized_ setting, ascenario where the agents can only communicate with their local neighbors over a prescribed network topology.6

Footnote 6: Our work seamlessly handles the server-client setting as a special case, by assuming the network topology as a fully connected network.

In this work, we study the problem of _federated multi-task RL_[1, 2, 3, 10], where each agent collects its own reward -- possibly unknown to other agents -- corresponding to the local task at hand, while having access to the same dynamics (i.e., transition kernel) of the environment. The collective goal is to learn a shared policy that maximizes the total rewards accumulated from all the agents; in other words, one seeks a policy that performs well in terms of overall benefits, rather than biasing towards any individual task, achieving the Pareto frontier in a multi-objective context. There is no shortage of application scenarios where federated multi-task RL becomes highly relevant. For instance, in healthcare [2], different hospitals may be interested in finding an optimal treatment for all patients without disclosing private data, where the effectiveness of the treatment can vary across different hospitals due to demographical differences. See Appendix B.1 for more application scenarios of our setting.

Nonetheless, despite the promise, provably efficient algorithms for federated multi-task RL remain substantially under-explored, especially in the fully decentralized setting. The heterogeneity of local tasks leads to a higher degree of disagreements between the global value function and local value functions of individual agents. Due to the lack of global information sharing, care needs to be taken to judiciously balance the use of neighboring information (to facilitate consensus) and local data (to facilitate learning) when updating the policy. To the best of our knowledge, very few algorithms are currently available to find the global optimal policy with non-asymptotic convergence guarantees even for tabular infinite-horizon Markov decision processes.

Motivated by the connection with decentralized optimization, it is tempting to take a policy optimization perspective to tackle this challenge. Policy gradient (PG) methods, which seek to learn the policy of interest via first-order optimization methods, play an eminent role in RL due to their simplicity and scalability. In particular, natural policy gradient (NPG) methods [1, 2] are among the most popular variants of PG methods, underpinning default methods used in practice such as trust region policy optimization (TRPO) [11] and proximal policy optimization (PPO) [12]. On the theoretical side, it has also been established recently that the NPG method enjoys fast global convergence to the optimal policy in an almost dimension-free manner [1, 13], where the iteration complexity is nearly independent of the size of the state-action space. These benefits can be translated to their sample-based counterparts such as the natural actor critic (NAC) method [1, 14, 15], where the policies are evaluated via stochastic samples. It is natural to ask:

_Can we develop **federated** NPG and NAC methods with **non-asymptotic global convergence** guarantees for multi-task RL in the fully decentralized setting?_

### Our contributions

Focusing on infinite-horizon Markov decision processes (MDPs), we provide an affirmative answer to the above question, by developing federated NPG (FedNPG) methods for solving both the vanilla and entropy-regularized multi-task RL problems with finite-time global convergence guarantees. While entropy regularization is often incorporated as an effective strategy to encourage exploration during policy learning, solving the entropy-regularized RL problem is of interest in its own right, as the optimal regularized policy possesses desirable robust properties with respect to reward perturbations [1, 2]. Due to the multiplicative update nature of NPG methods under softmax parameterization, it is more convenient to work with the logarithms of local policies in the decentralized setting. In each iteration of the proposed FedNPG method, the logarithms of local policies are updated by a weighted linear combination of two terms (up to normalization): a gossip mixing [17] of the logarithms of neighboring local policies, and a local estimate of the global Q-function tracked via the technique of dynamic average consensus [2], a prevalent idea in decentralized optimization that allows for the use of large constant learning rates [1, 18, 19] to accelerate convergence. We further develop sample-efficient federated NAC (FedNAC) methods that allow for both stochastic policy evaluation and function approximation. Our contributions are as follows.

* We propose FedNPG methods for both the vanilla and entropy-regularized multi-task RL problems, where each agent only communicates with its neighbors and performs local computation using its own reward or task information.

is set as \(\eta=\eta_{1}=\mathcal{O}\left(\frac{(1-\gamma)^{9}(1-\sigma)^{2}\log|\mathcal{A}|}{ TN\sigma}\right)^{1/3}\); for entropy-regularized FedNPG, the learning rate satisfies \(0<\eta<\eta_{0}=\mathcal{O}\left(\frac{(1-\gamma)^{7}(1-\sigma)^{2}\tau}{\sigma N }\right)\).
* Assuming access to exact policy evaluation, we establish that the average iterate of vanilla FedNPG converges globally at a rate of \(\mathcal{O}(1/T^{2/3})\) in terms of the sub-optimality gap for the multi-task RL problem, and that the last iterate of entropy-regularized FedNPG converges globally at a linear rate to the regularized optimal policy. Our convergence theory highlights the impacts of all salient problem parameters (see Table 1 for details), such as the size and connectivity of the communication network. In particular, the iteration complexities of FedNPG are again almost independent of the size of the state-action space, which recover prior results on the centralized NPG methods when the network is fully connected.
* We further demonstrate the stability of the proposed FedNPG methods when policy evaluations are only available in an inexact manner. To be specific, we prove that their convergence rates remain unchanged as long as the approximation errors are sufficiently small in the \(\ell_{\infty}\) sense.
* We go beyond the tabular setting and black-box policy evaluation by proposing FedNAC-- a federated actor critic method for multi-task RL with function approximation and stochastic policy evaluation -- and establish a finite-sample sample complexity on the order of \(\mathcal{O}(1/\varepsilon^{7/2})\) for each agent in terms of the expected sub-optimality gap for the fully decentralized setting.

To the best of our knowledge, the proposed federated NPG and NAC methods are the first policy optimization methods for multi-task RL that achieve near dimension-free global convergence guarantees in terms of iteration and sample complexities, allowing for fully decentralized communication without any need to share local reward/task information. We conduct numerical experiments in a multi-task GridWorld environment to corroborate the efficacy of the proposed methods (see Appendix H). We defer the readers to Appendix A for more related work, and Appendix B.2 for additional discussions on our theoretical contributions.

**Notation.** Boldface small and capital letters denote vectors and matrices, respectively. Sets are denoted with curly capital letters, e.g., \(\mathcal{S}\), \(\mathcal{A}\). We let \((\mathbb{R}^{d},\left\|\cdot\right\|)\) denote the \(d\)-dimensional real coordinate space equipped with norm \(\left\|\cdot\right\|\). The \(\ell_{p}\)-norm of \(\mathbf{v}\) is denoted by \(\left\|\mathbf{v}\right\|_{p}\), where \(1\leq p\leq\infty\), and the spectral norm and the Frobenius norm of a matrix \(\mathbf{M}\) are denoted by \(\left\|\mathbf{M}\right\|_{2}\) and \(\left\|\mathbf{M}\right\|_{\text{F}}\), resp. We let \([N]\) denote \(\left\{1,\ldots,N\right\}\), use \(\mathbf{1}_{N}\) to represent the all-one vector of length \(N\), and denote by \(\mathbf{0}\) a vector or a matrix consisting of all \(0\)'s. We allow the application of functions such as \(\log(\cdot)\) and \(\exp(\cdot)\) to vectors or matrices, with the understanding that they are applied in an element-wise manner.

## 2 Model and backgrounds

**Markov decision processes.** We consider an infinite-horizon discounted Markov decision process (MDP) denoted by \(\mathcal{M}=(\mathcal{S},\mathcal{A},P,r,\gamma)\), where \(\mathcal{S}\) and \(\mathcal{A}\) denote the state space and the action space, respectively, \(\gamma\in[0,1)\) indicates the discount factor, \(P:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\) is the transition kernel, and \(r:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) stands for the reward function. To be more specific, for each state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\) and any state \(s^{\prime}\in\mathcal{S}\), we denote by \(P(s^{\prime}|s,a)\) the transition probability from state

\begin{table}
\begin{tabular}{c c c c} \hline \hline setting & algorithms & iteration complexity & optimality criteria \\ \hline \multirow{2}{*}{unregularized} & NPG [1] & \(\mathcal{O}\left(\frac{1}{(1-\gamma)^{\sigma}}+\frac{\log|\mathcal{A}|}{\eta}\right)\) & \(V^{\star}-V^{\pi^{(0)}}\leq\varepsilon\) \\ \cline{2-4}  & FedNPG (ours) & \(\mathcal{O}\left(\frac{\sigma\sqrt{N}\log|\mathcal{A}|}{(1-\gamma)^{\frac{ \sigma}{2}}}+\frac{1}{(1-\gamma)^{\sigma}}\right)\) & \(\frac{1}{T}\sum_{t=0}^{T-1}\left(V^{\star}-V^{\pi^{(0)}}\right)\leq\varepsilon\) \\ \hline \multirow{2}{*}{regularized} & NPG [14] & \(\mathcal{O}\left(\frac{1}{\tau\eta}\log\left(\frac{1}{\varepsilon}\right)\right)\) & \(V^{\star}_{\star}-V^{\pi^{(0)}}_{\star}\leq\varepsilon\) \\ \cline{2-4}  & FedNPG (ours) & \(\mathcal{O}\left(\max\left\{\frac{1}{\tau\eta},\frac{1}{\tau\eta}\log\left( \frac{1}{\varepsilon}\right)\right\}\right.\) & \(V^{\star}_{\star}-V^{\pi^{(0)}}_{\star}\leq\varepsilon\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Iteration complexities of NPG and FedNPG (ours) methods to reach \(\varepsilon\)-accuracy of the vanilla and entropy-regularized problems, where we assume exact gradient evaluation, and only keep the dominant terms w.r.t. \(\varepsilon\). The policy estimates in the \(t\)-iteration are \(\pi^{(t)}\) and \(\bar{\pi}^{(t)}\) for NPG and FedNPG, respectively, where \(T\) is the number of iterations. Here, \(N\) is the number of agents, \(\tau\leq 1\) is the regularization parameter, \(\sigma\in[0,1]\) is the spectral radius of the network, \(\gamma\in[0,1)\) is the discount factor, \(|\mathcal{A}|\) is the size of the action space, and \(\eta>0\) is the learning rate. The iteration complexities of FedNPG reduce to their centralized counterparts when \(\sigma=0\). For vanilla FedNPG, the learning rate is set as \(\eta=\eta_{1}=\mathcal{O}\left(\frac{(1-\gamma)^{9}(1-\sigma)^{2}\log| \mathcal{A}|}{TN\sigma}\right)^{1/3}\); for entropy-regularized FedNPG, the learning rate satisfies \(0<\eta<\eta_{0}=\mathcal{O}\left(\frac{(1-\gamma)^{7}(1-\sigma)^{2}\tau}{ \sigma N}\right)\).

\(s\) to state \(s^{\prime}\) when action \(a\) is taken, and \(r(s,a)\) the instantaneous reward received in state \(s\) when action \(a\) is taken. Furthermore, a policy \(\pi:\mathcal{S}\to\Delta(\mathcal{A})\) specifies an action selection rule, where \(\pi(a|s)\) specifies the probability of taking action \(a\) in state \(s\) for each \((s,a)\in\mathcal{S}\times\mathcal{A}\).

For any given policy \(\pi\), we denote by \(V^{\pi}:\mathcal{S}\mapsto\mathbb{R}\) the corresponding value function, which is the expected discounted cumulative reward with an initial state \(s_{0}=s\), given by

\[\forall s\in\mathcal{S}:\quad V^{\pi}(s)\coloneqq\mathbb{E}\left[\sum_{t=0}^{ \infty}\gamma^{t}r(s_{t},a_{t})|s_{0}=s\right], \tag{1}\]

where the randomness is over the trajectory generated following the policy \(a_{t}\sim\pi(\cdot|s_{t})\) and the MDP dynamic \(s_{t+1}\sim P(\cdot|s_{t},a_{t})\). We also overload the notation \(V^{\pi}(\rho)\) to indicate the expected value function of policy \(\pi\) when the initial state follows a distribution \(\rho\) over \(\mathcal{S}\), namely, \(V^{\pi}(\rho)\coloneqq\mathbb{E}_{s\sim\rho}\left[V^{\pi}(s)\right]\). Similarly, the Q-function \(Q^{\pi}:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}\) of policy \(\pi\) is defined by

\[Q^{\pi}(s,a)\coloneqq\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t},a_{ t})|s_{0}=s,a_{0}=a\right] \tag{2}\]

for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), which measures the expected discounted cumulative reward with an initial state \(s_{0}=s\) and an initial action \(a_{0}=a\), with expectation taken over the randomness of the trajectory. The optimal policy \(\pi^{\star}\) refers to the policy that maximizes the value function \(V^{\pi}(s)\) for all states \(s\in\mathcal{S}\), which is guaranteed to exist [14]. The corresponding optimal value function and Q-function are denoted as \(V^{\star}\) and \(Q^{\star}\), respectively.

**Entropy-regularized RL.** Entropy regularization [19, 1] is a popular technique in practice that encourages stochasticity of the policy to promote exploration, as well as robustness against reward uncertainties. Mathematically, this can be viewed as adjusting the instantaneous reward based the current policy in use as

\[\forall(s,a)\in\mathcal{S}\times\mathcal{A}:\ r_{\tau}(s,a)\coloneqq r(s,a)- \tau\log\pi(a|s)\,, \tag{3}\]

where \(\tau\geq 0\) denotes the regularization parameter. Typically, \(\tau\) should be too large to outweigh the actual rewards; for ease of presentation, we assume \(\tau\leq\min\left\{1,\,\frac{1}{\log\left|\mathcal{A}\right|}\right\}\)[1]. Equivalently, this amounts to the entropy-regularized (also known as "soft") value function, defined as

\[\forall s\in\mathcal{S}:\quad V^{\pi}_{\tau}(s)\coloneqq V^{\pi}(s)+\tau \mathcal{H}(s,\pi), \tag{4}\]

where

\[\mathcal{H}(s,\pi)\coloneqq\mathbb{E}\left[\sum_{t=0}^{\infty}-\gamma^{t}\log \pi(a_{t}|s_{t})\big{|}s_{0}=s\right]. \tag{5}\]

Analogously, for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), the regularized (or soft) Q-function \(Q^{\pi}_{\tau}\) of policy \(\pi\) is related to the soft value function \(V^{\pi}_{\tau}(s)\) as

\[Q^{\pi}_{\tau}(s,a) =r(s,a)+\gamma\mathbb{E}_{s^{\prime}\in P(\cdot|s,a)}\left[V^{\pi }_{\tau}(s^{\prime})\right]\,, \tag{6a}\] \[V^{\pi}_{\tau}(s) =\mathbb{E}_{a\sim\pi(\cdot|s)}\left[-\tau\pi(a|s)+Q^{\pi}_{\tau }(s,a)\right]\,. \tag{6b}\]

The optimal regularized policy, the optimal regularized value function, and the Q-function are denoted by \(\pi^{\star}_{\tau}\), \(V^{\star}_{\tau}\), and \(Q^{\star}_{\tau}\), respectively.

**Natural policy gradient methods.** Natural policy gradient (NPG) methods lie at the heart of policy optimization, serving as the backbone of popular heuristics such as TRPO [1] and PPO [20]. Instead of directly optimizing the policy over the probability simplex, one often adopts the softmax parameterization, which parameterizes the policy as \(\pi_{\theta}\coloneqq\operatorname{softmax}(\theta)\) or

\[\pi_{\theta}(a|s)\coloneqq\frac{\exp\theta(s,a)}{\sum_{a^{\prime}\in\mathcal{ A}}\exp\theta(s,a^{\prime})} \tag{7}\]

for any \(\theta\): \(\mathcal{S}\times\mathcal{A}\to\mathbb{R}\) and \((s,a)\in\mathcal{S}\times\mathcal{A}\).

In the tabular setting, the update rule of vanilla NPG at the \(t\)-th iteration can be concisely represented as

\[\pi^{(t+1)}(a|s)\propto\pi^{(t)}(a|s)\exp\left(\frac{\eta Q^{(t)}(s,a)}{1- \gamma}\right), \tag{8}\]Turning to the regularized problem, we note that the update rule of entropy-regularized NPG becomes

\[\pi^{(t+1)}(a|s)\propto(\pi^{(t)}(a|s))^{1-\frac{\eta\tau}{\tau-\gamma}}\exp\left( \frac{\eta Q_{\tau}^{(t)}(s,a)}{1-\gamma}\right), \tag{9}\]

where \(\eta\in(0,\frac{1-\gamma}{\tau}]\) is the learning rate, and \(Q_{\tau}^{(t)}=Q_{\tau}^{\pi^{(t)}}\) is the soft Q-function of policy \(\pi^{(t)}\).

## 3 Federated NPG methods for multi-task RL

In this paper, we consider the federated multi-task RL setting, where a set of agents learn collaboratively a single policy that maximizes its average performance over all the tasks using only local computation and communication.

**Multi-task RL.** Each agent \(n\in[N]\) has its own private reward function \(r_{n}(s,a)\) -- corresponding to different tasks -- while sharing the same transition kernel of the environment. The goal is to collectively learn a single policy \(\pi\) that maximizes the global value function given by \(V^{\pi}(s)=\frac{1}{N}\sum_{n=1}^{N}V_{n}^{\pi}(s)\), where \(V_{n}^{\pi}\) is the value function of agent \(n\in[N]\), defined by

\[V_{n}^{\pi}(s)\coloneqq\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}r_{n}(s_{ t},a_{t})|s_{0}=s\right].\]

Clearly, the global value function corresponds to using the average reward of all agents \(r(s,a)=\frac{1}{N}\sum_{n=1}^{N}r_{n}(s,a)\). The global Q-function \(Q^{\pi}(s,a)\) and the agent Q-functions \(Q_{n}^{\pi}(s,a)\) can be defined in a similar manner obeying \(Q^{\pi}(s,a)=\frac{1}{N}\sum_{n=1}^{N}Q_{n}^{\pi}(s,a)\).

In parallel, we are interested in the entropy-regularized setting, where each agent \(n\in[N]\) is equipped with a regularized reward function given by \(r_{\tau,n}(s,a)\coloneqq r_{n}(s,a)-\tau\log\pi(a|s)\). And we define similarly the regularized value functions as

\[V_{\tau,n}^{\pi}(s)\coloneqq\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}r_{ \tau,n}(s_{t},a_{t})|s_{0}=s\right]\]

for all \(n\in[N]\) and \(V_{\tau}^{\pi}(s)=\frac{1}{N}\sum_{n=1}^{N}V_{\tau,n}^{\pi}(s)\), \(\forall s\in\mathcal{S}\). The soft Q-function of agent \(n\) is given by

\[Q_{\tau,n}^{\pi}(s,a)=r_{n}(s,a)+\gamma\mathbb{E}_{s^{\prime}\in P(\cdot|s,a)} \left[V_{\tau,n}^{\pi}(s^{\prime})\right]\,, \tag{10}\]

and the global soft Q-function is given by \(Q_{\tau}^{\pi}(s,a)=\frac{1}{N}\sum_{n=1}^{N}Q_{\tau,n}^{\pi}(s,a)\).

**Federated policy optimization in the fully decentralized setting.** We consider a federated setting with fully decentralized communication, that is, all the agents are synchronized to perform information exchange over some prescribed network topology denoted by an undirected weighted graph \(\mathcal{G}([N],E)\). Here, \(E\) stands for the edge set of the graph with \(N\) nodes -- each corresponding to an agent -- and two agents can communicate with each other if and only if there is an edge connecting them. The information sharing over the graph is best described by a mixing matrix [10], denoted by \(\mathbf{W}=[w_{ij}]\in[0,1]^{N\times N}\), where \(w_{ij}\) is a positive number if \((i,j)\in E\) and 0 otherwise. We also make the following standard assumptions on the mixing matrix.

**Assumption 3.1** (double stochasticity).: The mixing matrix \(\mathbf{W}=[w_{ij}]\in[0,1]^{N\times N}\) is symmetric (i.e., \(\mathbf{W}^{\top}=\mathbf{W}\)) and doubly stochastic (i.e., \(\mathbf{W}\mathbf{1}_{N}=\mathbf{1}_{N}\), \(\mathbf{1}_{N}^{\top}\mathbf{W}=\mathbf{1}_{N}^{\top}\)).

The following standard metric measures how fast information propagates over the graph.

**Definition 3.2** (spectral radius).: The spectral radius of \(\mathbf{W}\) is given as \(\sigma\coloneqq\|\mathbf{W}-\frac{1}{N}\mathbf{1}_{N}\mathbf{1}_{N}^{\top}\|_{2}\in[0,1)\).

The spectral radius \(\sigma\) determines how fast information propagate over the network. For instance, in a fully-connected network, we can achieve \(\sigma=0\) by setting \(\mathbf{W}=\frac{1}{N}\mathbf{1}_{N}\mathbf{1}_{N}^{\top}\). For control of \(1/(1-\sigma)\) regarding different graphs, we refer the readers to [18]. In an Erdos-Renyi random graph, as long as the graph is connected, one has with high probability \(\sigma\asymp 1\). Another immediate consequence is that for any \(\mathbf{x}\in\mathbb{R}^{N}\), letting \(\overline{x}=\frac{1}{N}\mathbf{1}_{N}^{\top}\mathbf{x}\) be its average, we have

\[\|\mathbf{W}\mathbf{x}-\overline{x}\mathbf{1}_{N}\|_{2}\leq\sigma\left\|\mathbf{x}-\overline{ x}\mathbf{1}_{N}\right\|_{2}\,, \tag{11}\]

where the consensus error contracts by a factor of \(\sigma\).

### Proposed federated NPG algorithms

Assuming softmax parameterization, the problem can be formulated as decentralized optimization,

\[\text{(unregularized)}\quad\max_{\theta}\ V^{\pi_{\theta}}(s)=\frac{1 }{N}\sum_{n=1}^{N}V_{n}^{\pi_{\theta}}(s), \tag{12}\] \[\text{(regularized)}\quad\max_{\theta}\ V_{\tau}^{\pi_{\theta}}(s) =\frac{1}{N}\sum_{n=1}^{N}V_{\tau,n}^{\pi_{\theta}}(s), \tag{13}\]

where \(\pi_{\theta}\coloneqq\operatorname{softmax}(\theta)\) subject to communication constraints. Motivated by the success of NPG methods, we aim to develop federated NPG methods to achieve our goal. For notational convenience, let \(\boldsymbol{\pi}^{(t)}\coloneqq\left(\pi_{1}^{(t)},\cdots,\pi_{N}^{(t)}\right) ^{\top}\) be the collection of policy estimates at all agents in the \(t\)-th iteration. Let

\[\overline{\pi}^{(t)}\coloneqq\operatorname{softmax}\left(\frac{1 }{N}\sum_{n=1}^{N}\log\pi_{n}^{(t)}\right), \tag{14}\]

which satisfies that \(\overline{\pi}^{(t)}(a|s)\propto\left(\prod_{n=1}^{N}\pi_{n}^{(t)}(a|s)\right) ^{1/N}\) for each \((s,a)\in\mathcal{S}\times\mathcal{A}\). Therefore, \(\overline{\pi}^{(t)}\) could be seen as the normalized geometric mean of \(\{\pi_{n}^{(t)}\}_{n\in[N]}\). Define the collection of Q-function estimates as \(\boldsymbol{Q}^{(t)}\coloneqq\left(Q_{1}^{\pi_{1}^{(t)}},\cdots,Q_{N}^{\pi_{ N}^{(t)}}\right)^{\top}\) and \(\boldsymbol{Q}_{\tau}^{(t)}\coloneqq\left(Q_{\tau,1}^{\pi_{t}^{(t)}},\cdots,Q_ {\tau,N}^{\pi_{N}^{(t)}}\right)^{\top}\). We shall often abuse the notation and treat \(\boldsymbol{\pi}^{(t)}\), \(\boldsymbol{Q}_{\tau}^{(t)}\) as matrices in \(\mathbb{R}^{N\times|\mathcal{S}||\mathcal{A}|}\), and treat \(\boldsymbol{\pi}^{(t)}(a|s)\), \(\boldsymbol{Q}_{\tau}^{(t)}(a|s)\) as vectors in \(\mathbb{R}^{N}\), for all \((s,a)\in\mathcal{S}\times\mathcal{A}\).

```
1:Input: learning rate \(\eta>0\), iteration number \(T\in\mathbb{N}_{+}\), mixing matrix \(\boldsymbol{W}\in\mathbb{R}^{N\times N}\).
2:Initialize:\(\boldsymbol{\pi}^{(0)},\boldsymbol{T}^{(0)}=\boldsymbol{Q}^{(0)}\).
3:for\(t=0,1,\cdots T-1\)do
4: Update the policy for each \((s,a)\in\mathcal{S}\times\mathcal{A}\): \[\log\boldsymbol{\pi}^{(t+1)}(a|s)=\boldsymbol{W}\Big{(}\log\boldsymbol{\pi}^{ (t)}(a|s)+\frac{\eta}{1-\gamma}\boldsymbol{T}^{(t)}(s,a)\Big{)}-\log \boldsymbol{z}^{(t)}(s)\,,\] (15) where \(\boldsymbol{z}^{(t)}(s)=\sum_{a^{\prime}\in\mathcal{A}}\exp\Big{\{} \boldsymbol{W}\big{(}\log\boldsymbol{\pi}^{(t)}(a^{\prime}|s)+\frac{\eta}{1- \gamma}\boldsymbol{T}^{(t)}(s,a^{\prime})\big{)}\Big{\}}\).
5: Evaluate \(\boldsymbol{Q}^{(t+1)}\).
6: Update the global Q-function estimate for each \((s,a)\in\mathcal{S}\times\mathcal{A}\): \[\boldsymbol{T}^{(t+1)}(s,a)=\boldsymbol{W}\Big{(}\boldsymbol{T}^{(t)}(s,a)+ \underbrace{\boldsymbol{Q}^{(t+1)}(s,a)-\boldsymbol{Q}^{(t)}(s,a)}_{\text{Q- tracking}}\Big{)}\,.\] (16)
7:endfor
```

**Algorithm 1** Federated NPG (FedNPG)

**Vanilla federated NPG methods.** To motivate the algorithm development, observe that the NPG method (cf. (8)) applied to (12) adopts the update rule \(\pi^{(t+1)}(a|s)\propto\pi^{(t)}(a|s)\exp\left(\frac{\eta\sum_{n=1}^{N}Q_{n}^{ \pi_{\theta}^{(t)}}(s,a)}{N(1-\gamma)}\right)^{\top}\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\). Two challenges arise when executing this update rule: the policy estimates are maintained locally without consensus, and the global Q-function are unavailable in the decentralized setting. To address these challenges, we apply the idea of dynamic average consensus [20], where each agent maintains its own estimate \(T_{n}^{(t)}(s,a)\) of the global Q-function, which are collected as vector \(\boldsymbol{T}^{(t)}=\left(T_{1}^{(t)},\cdots,T_{N}^{(t)}\right)^{\top}\). At each iteration, each agent updates its policy estimates based on its neighbors' information via gossip mixing, in addition to a correction term that tracks the difference \(Q_{n}^{\pi_{\theta}^{(t+1)}}(s,a)-Q_{n}^{\pi_{\theta}^{(t)}}(s,a)\) of the local Q-functions between consecutive policy updates. Note that the mixing is applied linearly to the logarithms of local policies, which translates into a multiplicative mixing of the local policies. Algorithm 1 summarizes the detailed procedure of the proposed algorithm written in a compact matrix form, which we dub as federated NPG (FedNPG). Note that the agents do not need to share their reward functions withothers, and agent \(n\in[N]\) will only be responsible to evaluate the local policy \(\pi_{n}^{(t)}\) using the local reward \(r_{n}\).

**Entropy-regularized federated NPG methods.** Moving onto the entropy regularized case, we adopt similar algorithmic ideas to decentralize (9), and propose the federated NPG (FedNPG) method with entropy regularization, summarized in Algorithm 2 (see Appendix C.1). Clearly, the entropy-regularized FedNPG method reduces to vanilla FedNPG in the absence of the regularization (i.e., when \(\tau=0\)).

### Theoretical guarantees

**Global convergence of FedNPG with exact policy evaluation.** We begin with the global convergence of FedNPG (cf. Algorithm 1), stated in the following theorem. The formal statement and proof can be found in Appendix D.3, and see Appendix B.2 for discussions on the technical challenges.

**Theorem 3.3** (Global sublinear convergence of exact FedNPG (informal)).: _Suppose \(\pi_{n}^{(0)},n\in[N]\) are set as the uniform distribution. Then when \(T\geq\frac{128\sqrt{N}\log|\mathcal{A}|\sigma^{4}}{(1-\sigma)^{4}}\) and \(\eta=\left(\frac{(1-\gamma)^{9}(1-\sigma)^{2}\log|\mathcal{A}|}{32TN\sigma^{ 2}}\right)^{1/3}\), we have_

\[\frac{1}{T}\sum_{t=0}^{T-1}\left(V^{\star}(\rho)-V^{\pi^{(t)}}( \rho)\right) \lesssim\frac{V^{\star}(d_{\rho}^{\pi^{*}})}{(1-\gamma)T}+\frac{ N^{1/3}\sigma^{2/3}}{(1-\gamma)^{3}(1-\sigma)^{2/3}}\left(\frac{\log|\mathcal{A}|}{T }\right)^{2/3}\,. \tag{17a}\] \[\left\|\log\pi_{n}^{(t)}-\log\pi^{(t)}\right\|_{\infty} \lesssim\frac{N^{2/3}\sigma^{1/3}}{(1-\gamma)(1-\sigma)^{1/3}} \left(\frac{\log|\mathcal{A}|}{T}\right)^{1/3}. \tag{17b}\]

Theorem 3.3 characterizes the average-iterate convergence of the average policy \(\overline{\pi}^{(t)}\) (cf. (14)) across the agents, which depends logarithmically on the size of the action space, and independently on the size of the state space. Theorem 3.3 indicates that in the server-client setting with \(\sigma=0\), the convergence rate of FedNPG recovers the \(\mathcal{O}(1/T)\) rate, matching that of the centralized NPG established in [1]; on the other end, in the decentralized setting where \(\sigma>0\), FedNPG slows down and eventually converges at the slower \(\mathcal{O}(1/T^{2/3})\) rate.

We state the iteration complexity in Corollary 3.4.

**Corollary 3.4** (Iteration complexity of exact FedNPG).: _To reach \(\frac{1}{T}\sum_{t=0}^{T-1}\left(V^{\star}(\rho)-V^{\pi^{(t)}}(\rho)\right)\leq\varepsilon\), the iteration complexity of FedNPG is at most \(\mathcal{O}\left(\left(\frac{\sigma}{(1-\gamma)^{9/2}(1-\sigma)\varepsilon^{ 3/2}}+\frac{\sigma^{2}}{(1-\sigma)^{4}}\right)\sqrt{N}\log|\mathcal{A}|+\frac {1}{\varepsilon(1-\gamma)^{2}}\right)\)._

**Global convergence of FedNPG with inexact policy evaluation.** In practice, the policies need to be evaluated using samples collected by the agents, where the Q-functions are only estimated approximately. We are interested in gauging how the approximation error impacts the performance of FedNPG, as demonstrated in the following theorem. The formal statement, detailed discussions, and proof of this result is given in Appendix D.4.

**Theorem 3.5** (Global sublinear convergence of inexact FedNPG (informal)).: _Suppose that an estimate \(q_{n}^{\pi_{n}^{(t)}}\) are used in replace of \(Q_{n}^{\pi_{n}^{(t)}}\) in Algorithm 1. Under the assumptions of Theorem 3.3, when \(T\gtrsim\frac{\sqrt{N}\log|\mathcal{A}|\sigma^{4}}{(1-\sigma)^{4}}\) and \(\eta=\left(\frac{(1-\gamma)^{9}(1-\sigma)^{2}\log|\mathcal{A}|}{32TN\sigma^{ 2}}\right)^{1/3}\), we have_

\[\frac{1}{T}\sum_{t=0}^{T-1}\left(V^{\star}(\rho)-V^{\overline{\pi} ^{(t)}}(\rho)\right) \lesssim\frac{V^{\star}(d_{\rho}^{\pi^{*}})}{(1-\gamma)T}+\frac{N^ {1/3}\sigma^{2/3}}{(1-\gamma)^{3}(1-\sigma)^{2/3}}\left(\frac{\log|\mathcal{A} |}{T}\right)^{2/3}\] \[\quad+\frac{1}{(1-\gamma)^{2}}\max_{n\in[N],t\in[T]}\left\|Q_{n}^{ \pi_{n}^{(t)}}-q_{n}^{\pi_{n}^{(t)}}\right\|_{\infty}\,. \tag{18}\]

Equipped with existing sample complexity bounds on policy evaluation, e.g. using a simulator as in [16], this immediate leads to the sample complexity per state-action pair at each agent to find an \(\varepsilon\)-optimal policy is at most

\[\widetilde{\mathcal{O}}\left(\frac{\sqrt{N}}{(1-\gamma)^{11.5}(1-\sigma) \varepsilon^{3.5}}\right) \tag{19}\]for sufficiently small \(\varepsilon\).

**Global convergence of entropy-regularized FedNPG with exact policy evaluation.** Next, we present our global convergence guarantee of entropy-regularized FedNPG with exact policy evaluation (cf. Algorithm 2).

**Theorem 3.6** (Global linear convergence of exact entropy-regularized FedNPG (informal)).: _For any \(\gamma\in(0,1)\) and \(0<\tau\leq 1\), there exists \(\eta_{0}=\min\left\{\frac{1-\gamma}{\tau},\mathcal{O}\left(\frac{(1-\gamma)^{ \gamma}(1-\sigma)^{2}\tau}{\sigma^{2}N}\right)\right\}\), such that if \(0<\eta\leq\eta_{0}\), then we have_

\[\left\|\overline{Q}_{\tau}^{(t)}-Q_{\tau}^{\star}\right\|_{\infty}\leq 2 \gamma C_{1}\rho(\eta)^{t}\qquad\left\|\log\pi_{\tau}^{\star}-\log\overline{ \pi}^{(t)}\right\|_{\infty}\leq\frac{2C_{1}}{\tau}\rho(\eta)^{t}\,, \tag{20}\]

_where \(\overline{Q}_{\tau}^{(t)}:=Q_{\tau}^{\overline{\pi}^{(t)}}\), \(\rho(\eta)\leq\max\{1-\frac{\tau\eta}{2},\frac{3+\sigma}{4}\}<1\), and \(C_{1}\) is some problem-dependent constant. Furthermore, the consensus error satisfies_

\[\forall n\in[N]:\quad\left\|\log\pi_{n}^{(t)}-\log\overline{\pi}^{(t)}\right\| _{\infty}\leq 2C_{1}\rho(\eta)^{t}. \tag{21}\]

The exact expressions of \(C_{1}\) and \(\eta_{0}\) are specified in Appendix D.1. Theorem 3.6 confirms that entropy-regularized FedNPG converges at a linear rate to the optimal regularized policy, which is almost independent of the size of the state-action space, highlighting the positive role of entropy regularization in federated policy optimization. When the network is fully connected, i.e. \(\sigma=0\), the iteration complexity of entropy-regularized FedNPG reduces to \(\mathcal{O}\Big{(}\frac{1}{\eta\tau}\log\frac{1}{\varepsilon}\Big{)}\), matching that of the centralized entropy-regularized NPG established in [14]. When the network is less connected, one needs to be more conservative in the choice of learning rates, leading to a higher iteration complexity, as described in the following corollary.

**Corollary 3.7** (Iteration complexity of exact entropy-regularized FedNPG).: _To reach \(\left\|\log\pi_{\tau}^{\star}-\log\overline{\pi}^{(t)}\right\|_{\infty}\leq\varepsilon\), the iteration complexity of entropy-regularized FedNPG is at most_

\[\widetilde{\mathcal{O}}\left(\max\left\{\frac{2}{\tau\eta},\frac{4}{1-\sigma} \right\}\log\frac{1}{\varepsilon}\right) \tag{22}\]

_up to logarithmic factors. Especially, when \(\eta=\eta_{0}\), the best iteration complexity becomes \(\widetilde{\mathcal{O}}\left(\left(\frac{N\sigma^{2}}{(1-\gamma)^{\gamma}(1- \sigma)^{2}\tau^{2}}+\frac{1}{1-\gamma}\right)\log\frac{1}{\tau\varepsilon}\right)\)._

**Global convergence of entropy-regularized FedNPG with inexact policy evaluation.** Last but not the least, we present the informal convergence results of entropy-regularized FedNPG with inexact policy evaluation, whose formal version can be found in Appendix D.2.

**Theorem 3.8** (Global linear convergence of inexact entropy-regularized FedNPG (informal)).: _Suppose that an estimate \(q_{\tau,n}^{\pi^{(t)}}\) are used in replace of \(Q_{\tau,n}^{\pi^{(t)}}\) in Algorithm 2. Under the assumptions of Theorem 3.6, we have_

\[\left\|\overline{Q}_{\tau}^{(t)}-Q_{\tau}^{\star}\right\|_{\infty}\leq 2 \gamma\Big{(}C_{1}\rho(\eta)^{t}+C_{2}\varepsilon_{q}\Big{)}\,,\quad\left\| \log\pi_{\tau}^{\star}-\log\overline{\pi}^{(t)}\right\|_{\infty}\leq\frac{2} {\tau}\Big{(}C_{1}\rho(\eta)^{t}+C_{2}\varepsilon_{q}\Big{)}\,, \tag{23}\]

_where \(\overline{Q}_{\tau}^{(t)}:=Q_{\tau}^{\overline{\pi}^{(t)}}\), \(\varepsilon_{q}\coloneqq\max_{n\in[N],t\in[T]}\left\|Q_{\tau,n}^{\pi^{(t)}}-q_ {\tau,n}^{\pi^{(t)}}\right\|_{\infty}\), \(\rho(\eta)\leq\max\{1-\frac{\tau\eta}{2},\frac{3+\sigma}{4}\}<1\), and \(C_{1}\), \(C_{2}\) are problem-dependent constants._

## 4 Federated NAC with function approximation and stochastic evaluation

In this section, motivated by the design and analysis of FedNPG, we go beyond the tabular setting and exact policy evaluation, by proposing a federated natural actor-critic (FedNAC) method with function approximation and stochastic policy evaluation. Specifically, we consider the policy with function approximation under softmax parameterization is of the following form:

\[f_{\xi}(a|s)=\frac{\exp(\phi^{\top}(s,a)\mathbf{\xi})}{\sum_{a^{\prime}\in\mathcal{ A}}\exp(\phi^{\top}(s,a^{\prime})\mathbf{\xi})}, \tag{24}\]

for all \((s,a)\in\mathcal{S}\times\mathcal{A}\) and \(\mathbf{\xi}\in\mathbb{R}^{p}\), where \(\phi:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{p}\) is a known feature map. We assume \(\phi\) is bounded over \(\mathcal{S}\times\mathcal{A}\), i.e., there exists \(C_{\phi}>0\) such that \(\left\|\phi(s,a)\right\|_{2}\leq C_{\phi}\) holds for all \((s,a)\in\mathcal{S}\times\mathcal{A}\).

Following [1, 2], given any \(\mathbf{w}\in\mathbb{R}^{p}\), \(Q:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\) and probability distribution \(\zeta\in\Delta(\mathcal{S}\times\mathcal{A})\) over the state-action space, we define the _function approximation error_\(\ell(\mathbf{w},Q,\zeta)\) as follows:

\[\ell(\mathbf{w},Q,\zeta)\coloneqq\mathbb{E}_{(s,a)\sim\zeta}\left[\left(\mathbf{w}^{ \top}\phi(s,a)-Q(s,a)\right)^{2}\right]. \tag{25}\]

By searching for \(\mathbf{w}\) that minimizes \(\ell(\mathbf{w},Q,\zeta)\), it approximates \(Q(s,a)\) using the feature map \(\phi\) with respect to the distribution \(\zeta\).

**Algorithm design.** Let us now discuss the high-level design of FedNAC, which is presented in Algorithm 3, with more details provided in Appendix C.2. At the \(t\)-th iteration (\(t=0,\dots,T-1\)), denote the actor (concerning the policies) parameters of all agents as \(\mathbf{\xi}^{(t)}=(\mathbf{\xi}^{(t)}_{1},\dots,\mathbf{\xi}^{(t)}_{N})^{\top}\in\mathbb{ R}^{N\times p}\), and the critic parameters of all agents as \(\mathbf{w}^{(t)}=(\mathbf{w}^{(t)}_{1},\dots,\mathbf{w}^{(t)}_{N})^{\top}\in\mathbb{R}^{N \times p}\) (concerning the local Q-values) and \(\mathbf{h}^{(t)}=(\mathbf{h}^{(t)}_{1},\dots,\mathbf{h}^{(t)}_{N})^{\top}\in\mathbb{R}^{N \times p}\) (concerning the global Q-values).

* First, the critic parameter \(\mathbf{w}^{(t)}_{n}\) is locally updated at each agent by aiming to minimize \(\ell(\mathbf{w},Q^{(t)}_{n},\tilde{d}^{(t)}_{n})\) (cf. (25)) with gradient descent, where \(Q^{(t)}_{n}\) is the local Q-function of the local policy \(f_{\xi^{(t)}_{n}}\), and \(\tilde{d}^{(t)}_{n}\) is the state-action visitation distribution induced by the local policy \(f_{\xi^{(t)}_{n}}\) and an initial state-action distribution \(\nu\) (determined from the data sampling mechanism, cf. (30)). However, since \(Q^{(t)}_{n}\) is not directly available, it needs to be estimated from samples. Therefore, the critic update takes \(K\) steps of stochastic gradient descent with critic learning rate \(\beta\), given by \[\widetilde{\mathbf{w}}_{k+1}=\widetilde{\mathbf{w}}_{k}-\beta\big{(}\widetilde{\mathbf{w} }_{k}^{\top}\phi(s_{k},a_{k})-\widehat{Q}_{\xi}(s_{k},a_{k})\big{)}\phi(s_{k},a_{k}),\] for \(k=0,\dots,K-1\), where \((s_{k},a_{k})\) is sampled on the local policy \(f_{\xi^{(t)}_{n}}\), and \(\widehat{Q}_{\xi}(s_{k},a_{k})\) is a careful estimate of the Q-value using a trajectory with expected length \(1/(1-\gamma)\) (see Algorithm 5 in Appendix C.2 adopted from [2, Lemma 4]), and \(\widetilde{\mathbf{w}}_{0}=\mathbf{0}\) for simplicity. The final critic is updated as \(\mathbf{w}^{(t)}_{n}=\frac{1}{K}\sum_{k=1}^{K}\widetilde{\mathbf{w}}_{k}\). The total sample complexity of the critic update per iteration is then on the order of \(K/(1-\gamma)\).
* Next, the critic parameter \(\mathbf{h}^{(t)}_{n}\) for estimating the global Q-function can then be estimated by averaging with the neighbors with the Q-tracking term, given by \(\mathbf{h}^{(t)}=\mathbf{W}\left(\mathbf{h}^{(t-1)}+\mathbf{w}^{(t)}-\mathbf{w}^{(t-1)}\right).\)
* Finally, the actor parameter \(\mathbf{\xi}^{(t)}_{n}\) can be updated via averaging with the neighbors along with the policy gradient informed by \(\mathbf{h}^{(t)}_{n}\), given by \(\mathbf{\xi}^{(t+1)}=\mathbf{W}\left(\mathbf{\xi}^{(t)}+\alpha\mathbf{h}^{(t)}\right),\) where \(\alpha\) is the learning rate of the actor.

Note that the sample complexity of FedNAC is on the order of \(KT/(1-\gamma)\). An important aspect of the FedNAC method is that the policy is updated using trajectory data collected via executing the learned policy, which is closer to practice and more challenging to learn than using the generative model.

**Theoretical guarantees.** We first state the assumptions that are needed to guarantee the convergence of Algorithm 3, which are all commonly used in the literature, e.g., [2, 1]. To begin, we require the covariance matrix of the feature map induced by the initial state-action distribution \(\nu\) satisfies the following assumption to guarantee the convergence of the critic.

**Assumption 4.1** (PSD of the covariance matrix of the feature map).: There exists \(\mu>0\) such that \(\mathbb{E}_{(s,a)\sim\nu}\left[\phi(s,a)\phi^{\top}(s,a)\right]=\Sigma_{\nu} \geq\mu\mathbf{I}\).

We also need to ensure that the Q-values can be well approximated by the linear function approximation using feature map \(\phi(s,a)\), which is captured next.

**Assumption 4.2** (Bounded approximation error).: For each \(n\in[N]\), there exists \(\varepsilon^{n}_{\text{approx}}\geq 0\) such that for all \(t\in\mathbb{N}\), it holds that \(\mathbb{E}\left[\ell\left(\mathbf{w}^{(t)}_{\star,n},Q^{(t)}_{n},\tilde{d}^{(t)}_{ n}\right)\right]\leq\varepsilon^{n}_{\text{approx}}\), where \(\mathbf{w}^{(t)}_{\star,n}\coloneqq\arg\min_{\mathbf{w}}\ell\left(\mathbf{w}^{(t)}_{\star,n},Q^{(t)}_{n},\tilde{d}^{(t)}_{n}\right)\).

We denote the average approximation error as \(\bar{\varepsilon}_{\text{approx}}=\frac{1}{N}\sum_{n=1}^{N}\varepsilon^{n}_{ \text{approx}}\). Similar as [2], we need the following assumption that bounds the transfer errors due to distribution shifts.

**Assumption 4.3** (Bounded transfer error).: There exists \(C_{\nu}>0\) such that for all \(n\in[N]\) and \(t\in\mathbb{N}\), it holds that \(\mathbb{E}_{(s,a)\sim\mathcal{X}_{\mathrm{c}}^{(0)}}\left[\left(\frac{h^{\pi}(s,a)}{\mathcal{X}_{\mathrm{c}}^{(0)}(s,a)}\right)^{2}\right]\leq C_{\nu}\), where \(h^{\pi}(s,a)\) is the state-action visitation distribution induced by any policy \(\pi\) from initial state distribution \(\rho\).

Note that if we choose \(\nu(s,a)>0\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), then Assumption 4.3 is guaranteed to hold true (see Lemma E.4 in Appendix E). We are now ready to state the convergence guarantee, whose formal version and proof could be found in Appendix E.

**Theorem 4.4** (Convergence rate of Algorithm 3 (informal)).: _Let \(\mathbf{\xi}_{1}^{(0)}=\cdots=\mathbf{\xi}_{N}^{(0)}\) in FedNAC. Denoting \(\bar{\mathbf{\xi}}^{(t)}\coloneqq\frac{1}{N}\sum_{n=1}^{N}\mathbf{\xi}_{n}^{(t)}\), and \(\bar{f}^{(t)}\coloneqq f_{\bar{\xi}^{(t)}}\) as the average policy. Then under Assumption 3.1, 4.1, 4.2 and 4.3, with appropriately chosen learning rates \(\alpha\) and \(\beta\), as long as the number of actor iterations satisfies_

\[T\gtrsim\max\left\{\frac{\sigma}{\varepsilon^{3/2}(1-\gamma)^{17/4}(1-\sigma)^ {3/2}},\frac{1}{\varepsilon(1-\gamma)},\frac{\sigma^{1/4}}{\varepsilon^{3/4}( 1-\sigma)^{3/8}(1-\gamma)^{7/8}N^{3/8}},\frac{\sigma^{4}}{(1-\gamma)^{2}(1- \sigma)^{6}}\right\}\]

_and the number of critic iterations satisfies \(K=\mathcal{O}\left(\frac{1}{(1-\gamma)^{n}\varepsilon^{2}}\right)\), it holds that_

\[V^{\star}(\rho)-\frac{1}{T}\sum_{t=0}^{T-1}V^{f^{(t)}}(\rho)\lesssim\varepsilon +\frac{\bar{\varepsilon}_{approx}}{1-\gamma}. \tag{26}\]

In the server-client setting when \(\sigma=0\), to reach (26), it suffices to choose \(T=\mathcal{O}\left(\frac{1}{(1-\gamma)\varepsilon}\right)\) and \(K=\mathcal{O}\left(\frac{1}{(1-\gamma)^{6}\varepsilon^{2}}\right)\), leading to a total sample complexity of \(KT/(1-\gamma)=\mathcal{O}\left(\frac{1}{(1-\gamma)^{8}\varepsilon^{3}}\right)\) per agent, and \(T=\mathcal{O}\left(\frac{1}{(1-\gamma)\varepsilon}\right)\) rounds of communication. The sample complexity matches that of (centralized) Q-NPG established in [3] with a single agent. On the other end, in the fully decentralized setting when \(\sigma\) is not close to \(0\), FedNAC requires \(\mathcal{O}\left(\frac{1}{(1-\gamma)^{45/4}\varepsilon^{7/2}(1-\sigma)^{3/2}}\right)\) samples for each agent and \(\mathcal{O}\left(\frac{1}{\varepsilon^{3/2}(1-\gamma)^{17/4}(1-\sigma)^{3/2}}\right)\) rounds of communication to reach (26), for sufficiently small \(\varepsilon\). Encouragingly, the dependency on the accuracy level \(\varepsilon\) -- the dominating factor -- in the sample complexity matches that of FedNPG given in (19) when assuming access to the generative model, which allows query of arbitrary state-action pairs. In contrast, FedNAC only collects on-policy samples, and therefore is much more challenging to guarantee its convergence.

## 5 Conclusions

This work proposes the first provably efficient federated NPG (FedNPG) methods for solving vanilla and entropy-regularized multi-task RL problems in the fully decentralized setting. The established finite-time global convergence guarantees are almost independent of the size of the state-action space up to some logarithmic factor, and illuminate the impacts of the size and connectivity of the network. Furthermore, the proposed FedNPG methods are provably robust vis-a-vis inexactness of local policy evaluations. Last but not least, we also propose FedNAC, which can be viewed as an extension of FedNPG with function approximation and stochastic policy evaluation, and establish its finite-time sample complexity. Future directions include generalizing the framework of federated policy optimization to allow personalized policy learning in a shared environment.

## Acknowledgments and Disclosure of Funding

The work of T. Yang, S. Cen and Y. Chi are supported in part by the grants ONR N00014-19-1-2404, NSF CCF-1901199, CCF-2106778, AFRL FA8750-20-2-0504, and a CMU Cylab seed grant. The work of Y. Wei is supported in part by the the NSF grants DMS-2147546/2015447, CAREER award DMS-2143215, CCF-2106778, and the Google Research Scholar Award. The work of Y. Chen is supported in part by the Alfred P. Sloan Research Fellowship, the Google Research Scholar Award, the AFOSR grant FA9550-22-1-0198, the ONR grant N00014-22-1-2354, and the NSF grants CCF-2221009 and CCF-1907661. S. Cen is also gratefully supported by Wei Shen and Xuehong Zhang Presidential Fellowship, Boeing Scholarship, and JP Morgan Chase PhD Fellowship.

## References

* [AKLM21] A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. _The Journal of Machine Learning Research_, 22(1):4431-4506, 2021.
* [ALRNS19] Z. Ahmed, N. Le Roux, M. Norouzi, and D. Schuurmans. Understanding the impact of entropy on policy optimization. In _International Conference on Machine Learning_, pages 151-160, 2019.
* [Ama98] S.-I. Amari. Natural gradient works efficiently in learning. _Neural computation_, 10(2):251-276, 1998.
* [AR21] A. Anwar and A. Raychowdhury. Multi-task federated reinforcement learning with adversaries. _arXiv preprint arXiv:2103.06473_, 2021.
* [ARB\({}^{+}\)19] M. Assran, J. Romoff, N. Ballas, J. Pineau, and M. Rabbat. Gossip-based actor-learner architectures for deep reinforcement learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* [BM13] F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate o (1/n). _Advances in neural information processing systems_, 26, 2013.
* [BR21] J. Bhandari and D. Russo. On the linear convergence of policy gradient methods for finite MDPs. In _International Conference on Artificial Intelligence and Statistics_, pages 2386-2394. PMLR, 2021.
* [BSGL09] S. Bhatnagar, R. S. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor-critic algorithms. _Automatica_, 45(11):2471-2482, 2009.
* [CCC\({}^{+}\)22a] S. Cen, C. Cheng, Y. Chen, Y. Wei, and Y. Chi. Fast global convergence of natural policy gradient methods with entropy regularization. _Operations Research_, 70(4):2563-2578, 2022.
* [CCC\({}^{+}\)22b] S. Cen, C. Cheng, Y. Chen, Y. Wei, and Y. Chi. Fast global convergence of natural policy gradient methods with entropy regularization. _Operations Research_, 70(4):2563-2578, 2022.
* [CCDX22] S. Cen, Y. Chi, S. S. Du, and L. Xiao. Faster last-iterate convergence of policy optimization in zero-sum Markov games. In _The Eleventh International Conference on Learning Representations_, 2022.
* [CFGW22] J. Chen, J. Feng, W. Gao, and K. Wei. Decentralized natural policy gradient with variance reduction for collaborative multi-agent reinforcement learning. _arXiv preprint arXiv:2209.02179_, 2022.
* [CWC21] S. Cen, Y. Wei, and Y. Chi. Fast policy extragradient methods for competitive games with entropy regularization. _Advances in Neural Information Processing Systems_, 34:27952-27964, 2021.
* [CZC21] Z. Chen, Y. Zhou, and R. Chen. Multi-agent off-policy TDC with near-optimal sample and communication complexity. In _2021 55th Asilomar Conference on Signals, Systems, and Computers_, pages 504-508. IEEE, 2021.
* [CZGB21] T. Chen, K. Zhang, G. B. Giannakis, and T. Basar. Communication-efficient policy gradient methods for distributed reinforcement learning. _IEEE Transactions on Control of Network Systems_, 9(2):917-929, 2021.
* [DAW11] J. C. Duchi, A. Agarwal, and M. J. Wainwright. Dual averaging for distributed optimization: Convergence analysis and network scaling. _IEEE Transactions on Automatic control_, 57(3):592-606, 2011.
* [DLS16] P. Di Lorenzo and G. Scutari. Next: In-network nonconvex optimization. _IEEE Transactions on Signal and Information Processing over Networks_, 2(2):120-136, 2016.

* [EL21] B. Eysenbach and S. Levine. Maximum entropy RL (provably) solves some robust RL problems. In _International Conference on Learning Representations_, 2021.
* [ESM\({}^{+}\)18] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In _International conference on machine learning_, pages 1407-1416. PMLR, 2018.
* [HJ12] R. A. Horn and C. R. Johnson. _Matrix analysis_. Cambridge university press, 2012.
* [Kak01] S. M. Kakade. A natural policy gradient. _Advances in neural information processing systems_, 14, 2001.
* [KDRM22] S. Khodadadian, T. T. Doan, J. Romberg, and S. T. Maguluri. Finite sample analysis of two-time-scale natural actor-critic algorithm. _IEEE Transactions on Automatic Control_, 2022.
* [KJVM21] S. Khodadadian, P. R. Jhunjhunwala, S. M. Varma, and S. T. Maguluri. On the linear convergence of natural policy gradient algorithm. In _2021 60th IEEE Conference on Decision and Control (CDC)_, pages 3794-3799. IEEE, 2021.
* [KMP12] S. Kar, J. M. Moura, and H. V. Poor. Qd-learning: A collaborative distributed strategy for multi-agent reinforcement learning through consensus. _arXiv preprint arXiv:1205.0047_, 2012.
* [KSJM22] S. Khodadadian, P. Sharma, G. Joshi, and S. T. Maguluri. Federated reinforcement learning: Linear speedup under Markovian sampling. In _International Conference on Machine Learning_, pages 10997-11057. PMLR, 2022.
* [Lan23] G. Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes. _Mathematical programming_, 198(1):1059-1106, 2023.
* [LCCC20] B. Li, S. Cen, Y. Chen, and Y. Chi. Communication-efficient distributed optimization in networks with gradient tracking and variance reduction. _The Journal of Machine Learning Research_, 21(1):7331-7381, 2020.
* [LLZ23] G. Lan, Y. Li, and T. Zhao. Block policy mirror descent. _SIAM Journal on Optimization_, 33(3):2341-2378, 2023.
* [LO08] I. Lobel and A. Ozdaglar. Convergence analysis of distributed subgradient methods over random networks. In _2008 46th Annual Allerton Conference on Communication, Control, and Computing_, pages 353-360. IEEE, 2008.
* [LWA\({}^{+}\)23] G. Lan, H. Wang, J. Anderson, C. Brinton, and V. Aggarwal. Improved communication efficiency in federated natural policy gradient via admm-based gradient updates. _arXiv preprint arXiv:2310.19807_, 2023.
* [LWCC23a] G. Li, Y. Wei, Y. Chi, and Y. Chen. Breaking the sample size barrier in model-based reinforcement learning with a generative model. _Operations Research_, 2023.
* [LWCC23b] G. Li, Y. Wei, Y. Chi, and Y. Chen. Softmax policy gradient methods can take exponential time to converge. _Mathematical Programming_, pages 1-96, 2023.
* [LZZ\({}^{+}\)17] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. _Advances in neural information processing systems_, 30, 2017.
* [MA22] M. M Alshater. Exploring the role of artificial intelligence in enhancing academic performance: A case study of chatgpt. _Available at SSRN_, 2022.
* [MBM\({}^{+}\)16] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In _International conference on machine learning_, pages 1928-1937, 2016.

* [MP95] R. D. McKelvey and T. R. Palfrey. Quantal response equilibria for normal form games. _Games and economic behavior_, 10(1):6-38, 1995.
* [MXSS20] J. Mei, C. Xiao, C. Szepesvari, and D. Schuurmans. On the global convergence rates of softmax policy gradient methods. In _International Conference on Machine Learning_, pages 6820-6829. PMLR, 2020.
* [NNXS17] O. Nachum, M. Norouzi, K. Xu, and D. Schuurmans. Bridging the gap between value and policy based reinforcement learning. In _Advances in Neural Information Processing Systems_, pages 2775-2785, 2017.
* [NO09] A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization. _IEEE Transactions on Automatic Control_, 54(1):48-61, 2009.
* [NOR18] A. Nedic, A. Olshevsky, and M. G. Rabbat. Network topology and communication-computation tradeoffs in decentralized optimization. _Proceedings of the IEEE_, 106(5):953-976, 2018.
* [NOS17] A. Nedic, A. Olshevsky, and W. Shi. Achieving geometric convergence for distributed optimization over time-varying graphs. _SIAM Journal on Optimization_, 27(4):2597-2633, 2017.
* [OPA\({}^{+}\)17] S. Omidshafiei, J. Pazis, C. Amato, J. P. How, and J. Vian. Deep decentralized multi-task multi-agent reinforcement learning under partial observability. In _International Conference on Machine Learning_, pages 2681-2690. PMLR, 2017.
* [PN21] S. Pu and A. Nedic. Distributed stochastic gradient tracking methods. _Mathematical Programming_, 187:409-457, 2021.
* [PP08] K. B. Petersen and M. S. Pedersen. The matrix cookbook. _Technical University of Denmark_, 7(15):510, 2008.
* [Put14] M. L. Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* [QL17] G. Qu and N. Li. Harnessing smoothness to accelerate distributed optimization. _IEEE Transactions on Control of Network Systems_, 5(3):1245-1260, 2017.
* [QZLZ21] J. Qi, Q. Zhou, L. Lei, and K. Zheng. Federated reinforcement learning: Techniques, applications, and open challenges. _arXiv preprint arXiv:2108.11887_, 2021.
* [RTR\({}^{+}\)23] M. M. Rahman, H. J. Terano, M. N. Rahman, A. Salamzadeh, and M. S. Rahaman. Chatgpt and academic research: a review and recommendations based on practical examples. _Rahman, M., Terano, HJR, Rahman, N., Salamzadeh, A., Rahaman, S.(2023). ChatGPT and Academic Research: A Review and Recommendations Based on Practical Examples. Journal of Education, Management and Development Studies_, 3(1):1-12, 2023.
* [SEM20] L. Shani, Y. Efroni, and S. Mannor. Adaptive trust region policy optimization: Global convergence and faster rates for regularized MDPs. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 5668-5675, 2020.
* [SLA\({}^{+}\)15] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897, 2015.
* [SWD\({}^{+}\)17] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [WCYW19] L. Wang, Q. Cai, Z. Yang, and Z. Wang. Neural policy gradient methods: Global optimality and rates of convergence. _arXiv preprint arXiv:1909.01150_, 2019.
* [WHM\({}^{+}\)23] J. Wang, J. Hu, J. Mills, G. Min, M. Xia, and N. Georgalas. Federated ensemble model-based reinforcement learning in edge computing. _IEEE Transactions on Parallel and Distributed Systems_, 2023.

* [WJC23] J. Woo, G. Joshi, and Y. Chi. The blessing of heterogeneity in federated q-learning: Linear speedup and beyond. _arXiv preprint arXiv:2305.10697_, 2023.
* [WKNL20] H. Wang, Z. Kaplan, D. Niu, and B. Li. Optimizing federated learning on non-iid data with reinforcement learning. In _IEEE INFOCOM 2020-IEEE Conference on Computer Communications_, pages 1698-1707. IEEE, 2020.
* [WP91] R. J. Williams and J. Peng. Function optimization using connectionist reinforcement learning algorithms. _Connection Science_, 3(3):241-268, 1991.
* [WSJC24] J. Woo, L. Shi, G. Joshi, and Y. Chi. Federated offline reinforcement learning: Collaborative single-policy coverage suffices. In _Forty-first International Conference on Machine Learning_, 2024.
* [Xia22] L. Xiao. On the convergence rates of policy gradient methods. _The Journal of Machine Learning Research_, 23(1):12887-12922, 2022.
* [XML20] T. Xu, Z. Wang, and Y. Liang. Improving sample complexity bounds for actor-critic algorithms. _arXiv preprint arXiv:2004.12956_, 2020.
* [YDG\({}^{+}\)22] R. Yuan, S. S. Du, R. M. Gower, A. Lazaric, and L. Xiao. Linear convergence of natural policy gradient methods with log-linear policies. _arXiv preprint arXiv:2210.01400_, 2022.
* [YLS\({}^{+}\)20] T. Yu, T. Li, Y. Sun, S. Nanda, V. Smith, V. Sekar, and S. Seshan. Learning context-aware policies from multiple smart homes via federated multi-task learning. In _2020 IEEE/ACM Fifth International Conference on Internet-of-Things Design and Implementation (IoTDI)_, pages 104-115. IEEE, 2020.
* [ZAD\({}^{+}\)21] S. Zeng, M. A. Anwar, T. T. Doan, A. Raychowdhury, and J. Romberg. A decentralized policy gradient approach to multi-task reinforcement learning. In _Uncertainty in Artificial Intelligence_, pages 1002-1012. PMLR, 2021.
* [ZBW\({}^{+}\)20] F. Zerka, S. Barakat, S. Walsh, M. Bogowicz, R. T. Leijenaar, A. Jochems, B. Miraglio, D. Townend, and P. Lambin. Systematic review of privacy-preserving distributed machine learning from federated databases in health care. _JCO clinical cancer informatics_, 4:184-200, 2020.
* [ZCH\({}^{+}\)23] W. Zhan, S. Cen, B. Huang, Y. Chen, J. D. Lee, and Y. Chi. Policy mirror descent for regularized reinforcement learning: A generalized framework with linear convergence. _SIAM Journal on Optimization_, 33(2):1061-1091, 2023.
* [ZFL\({}^{+}\)19] H. H. Zhuo, W. Feng, Y. Lin, Q. Xu, and Q. Yang. Federated deep reinforcement learning. _arXiv preprint arXiv:1901.08277_, 2019.
* [ZLK\({}^{+}\)22] R. Zhou, T. Liu, D. Kalathil, P. Kumar, and C. Tian. Anchor-changing regularized natural policy gradient for multi-objective reinforcement learning. _Advances in Neural Information Processing Systems_, 35:13584-13596, 2022.
* [ZM10] M. Zhu and S. Martinez. Discrete-time dynamic average consensus. _Automatica_, 46(2):322-329, 2010.
* [ZRY\({}^{+}\)23] F. Zhao, X. Ren, S. Yang, P. Zhao, R. Zhang, and X. Xu. Federated multi-objective reinforcement learning. _Information Sciences_, 624:811-832, 2023.

Related work

**Global convergence of NPG methods for tabular MDPs.**[1] first establishes a \(\mathcal{O}(1/T)\) last-iterate convergence rate of the NPG method under softmax parameterization with constant step size, assuming access to exact policy evaluation. When entropy regularization is in place, [14] establishes a global linear convergence to the optimal regularized policy for the entire range of admissible constant learning rates using softmax parameterization and exact policy evaluation, which is further shown to be stable in the presence of \(\ell_{\infty}\) policy evaluation errors. The iteration complexity of NPG methods is nearly independent with the size of the state-action space, which is in sharp contrast to softmax policy gradient methods that may take exponential time to converge [15, 16]. [17] proposed a more general framework through the lens of mirror descent for regularized RL with global linear convergence guarantees, which is further generalized in [18, 19]. Earlier analysis of regularized MDPs can be found in [20]. Besides, [21] proves that vanilla NPG also achieves linear convergence when geometrically increasing learning rates are used; see also [14, 1]. [19] developed an anchor-changing NPG method for multi-task RL under various optimality criteria in the centralized setting.

**Convergence and sample complexity results of NAC.** The convergence and sample complexity of a variety of natural actor-critic methods (NACs) are extensively studied in the literature [1, 1, 2, 1, 1, 10]. More pertinent to our work, [1] introduced Q-NPG--a sample version of the NPG method with function approximation under softmax parameterization --and obtained a convergence rate of \(\mathcal{O}(1/\sqrt{T})\). [10] weakens some of its assumptions and improves the convergence rate to \(\mathcal{O}(1/T)\) and gives the \(\widetilde{\mathcal{O}}(1/\varepsilon^{3})\) sample complexity using a constant actor learning rate. The FedNAC method we propose in this paper can be seen as a decentralized version of Q-NPG, and in the server-client setting where the network is fully connected, our convergence rate and sample complexity match those in [10].

**Distributed and federated RL.** There have been a variety of settings being set forth for distributed and federated RL. [23, 24, 25, 26] focused on developing federated versions of RL algorithms to accelerate training, assuming all agents share the same transition kernel and reward function; in particular, [25, 26] established the provable benefits of federated learning in terms of linear speedup. More pertinent to our work, [19, 10] considered the federated multi-task framework, allowing different agents having private reward functions. [19] proposed an empirically probabilistic algorithm that can seek an optimal policy under the server-client setting, while [1] developed new attack methods in the presence of adversarial agents. Recently [15] discussed how to avoid transmitting the Hessian matrix during communication in the server-client setting where all agents share the same reward function. Different from the FRL framework, [18, 19, 10] considered the distributed multi-agent RL setting where the agents interact with a dynamic environment through a multi-agent Markov decision process, where each agent can have their own state or action spaces. [19] developed a decentralized policy gradient method where different agents have different MDPs, where a special case of their setting recovers ours. However, the convergence rate developed in [19] has rather pessimistic dependencies with the size of the state-action space, together with other parameters, without leveraging natural policy gradients and gradient tracking techniques.

**Decentralized first-order optimization algorithms.** Early work of consensus-based first-order optimization algorithms for the fully decentralized setting include but are not limited to [13, 14, 15]. Gradient tracking, which leverages the idea of dynamic average consensus [10] to track the gradient of the global objective function, is a popular method to improve the convergence speed [16, 17, 18, 19, 17].

## Appendix B Additional Discussion

### Application Related to Federated Multi-task RL

In this section, we elaborate more on our motivation and the application scenarios where federated multi-task RL becomes highly relevant.

We first provide some key motivations for our federated multi-task RL setting as follows.

* Efficient knowledge transfer: multi-task RL enables agents to transfer knowledge across related tasks, accelerating learning and improving performance by leveraging experiences gained from one task to another. For instance, in our healthcare example in Section 1, by learning across hospitals with varying demographics, the agent can identify treatment strategies that are effective across diverse patient populations without directly accessing sensitive patient information.
* Generalization and adaptability: agents trained with multi-task RL can generalize their learned policies, adapt to new tasks, and handle diverse environments more effectively, enhancing their robustness and adaptability. In the healthcare example, an optimal treatment over different hospitals better adapts to variations in patient characteristics.
* Resource optimization: training a single policy for multiple tasks optimizes resource usage compared to training separate policies for each task, making it more efficient in scenarios with limited data or computational resources. In the healthcare example, the collaborative approach enhances learning efficiency and scalability while preserving data privacy, particularly in settings where each hospital has limited access to patient information.

Below we provide more application scenarios of our setting.

1. To enhance ChatGPT's performance across different tasks or domains [14, 13], one might consult domain experts to chat and rate ChatGPT's outputs for solving different tasks, and train ChatGPT in a federated manner without exposing private data or feedback of each expert.
2. Our setting is especially suitable for the multi-task problems where each agent only have partial access of the "global" task. There are a lot of such problems. * An example is the problem we consider in our experiments (see Appendix H), where we distributedly train the agents to learn a shared policy to follow a predetermined trajectory while each agent only has partial information of this trajectory. * The above problem could be seen as a simplified version of the Unmanned Aerial Vehicle (UAV) Patrol Mission, each unmanned aerial vehicle (UAV) patrols only in a specific area, and they need to collectively train a strategy utilizing information from the entire patrol range. * In the game setting, different agents aim to train a character to perform well in multiple tasks, and each agent trains on one task.

Despite the promise, provably efficient algorithms for federated multi-task RL remain substantially under-explored, especially in the fully decentralized setting. Our work is the first to provide efficient algorithms with global convergence guarantees for federated multi-task RL.

### Theoretical Contribution

In this section, we stress that while our work is built upon the algorithmic ideas in the distributed learning, reinforcement learning and optimization literature, it is not a strightforward combination and the theoretical analysis is by no means trivial.

One key difficulty is to estimate the global Q-functions using only neighboring information and local data. To address this issue, we invoke the "Q-tracking" step (see Algorithm 1, 2), which is inspired by the gradient tracking method in decentralized optimization. Note that this generalization is highly non-trivial: to the best of our knowledge, the utility of gradient tracking has not been exploited in policy optimization, and the intrinsic nonconcavity issue, together with the use of natural gradients, prevents us from directly using the results from decentralized optimization. It is thus of great value to study if the combination of NPG and gradient tracking could lead to fast globally convergent algorithms as in the standard decentralized optimization literature despite the nonconvexity.

Besides, due to the lack of global information sharing, care needs to be taken to judiciously balance the use of neighboring information (to facilitate consensus) and local data (to facilitate learning) when updating the policy. Compared to the centralized version of our proposed algorithms, a much more delicate theoretical analysis is required to prove our convergence results. For example, the key step to establish the convergence rate of the single-agent exact entropy-regularized NPG is to form the 2nd-order linear system in Eq. (46) in [12], while in our corresponding analysis,a 4th-order linear system in Eq. (49) is needed, where the inequality in each line is non-trivial and requires the introduction of some intricate and novel auxiliary lemmas, see Appendix D.

## Appendix C Omitted Algorithms

### Federated NPG (FedNPG) with entropy regularization

We record the entropy-regularized FedNPG method in Algorithm 2 here due to space limits.

```
1:Input: learning rate \(\eta>0\), iteration number \(T\in\mathbb{N}_{+}\), mixing matrix \(\mathbf{W}\in\mathbb{R}^{N\times N}\), regularization coefficient \(\tau>0\).
2:Initialize:\(\mathbf{\pi}^{(0)}\), \(\mathbf{T}^{(0)}=\mathbf{Q}_{\tau}^{(0)}\).
3:for\(t=0,1,\cdots\)do
4: Update the policy for each \((s,a)\in\mathcal{S}\times\mathcal{A}\): \[\log\mathbf{\pi}^{(t+1)}(a|s)=\mathbf{W}\left(\left(1-\frac{\eta\tau}{1-\gamma} \right)\log\mathbf{\pi}^{(t)}(a|s)+\frac{\eta}{1-\gamma}\mathbf{T}^{(t)}(s,a)\right)- \log\mathbf{z}^{(t)}(s)\,,\] (27) where \(\mathbf{z}^{(t)}(s)=\sum_{a^{\prime}\in\mathcal{A}}\exp\Big{\{}\mathbf{W}\left(\left( 1-\frac{\eta\tau}{1-\gamma}\right)\log\mathbf{\pi}^{(t)}(a^{\prime}|s)+\frac{\eta }{1-\gamma}\mathbf{T}^{(t)}(s,a^{\prime})\right)\Big{\}}\).
5: Evaluate \(\mathbf{Q}_{\tau}^{(t+1)}\).
6: Update the global Q-function estimate for each \((s,a)\in\mathcal{S}\times\mathcal{A}\): \[\mathbf{T}^{(t+1)}(s,a)=\mathbf{W}\Big{(}\mathbf{T}^{(t)}(s,a)+\underbrace{\mathbf{Q}_{\tau}^ {(t+1)}(s,a)-\mathbf{Q}_{\tau}^{(t)}(s,a)}_{\text{Q-tracking}}\Big{)}\,.\] (\(U_{T}\))
7:endfor
```

**Algorithm 2** Federated NPG (FedNPG) with entropy regularization

### Development of FedNAC

For any policy \(\pi\), we let \(d_{s_{0}}^{\pi}\) denote the discounted state visitation distribution of \(\pi\) given an initial state \(s_{0}\in\mathcal{S}\), i.e.,

\[\forall s\in\mathcal{S}:\quad d_{s_{0}}^{\pi}(s)\coloneqq(1-\gamma)\sum_{t=0} ^{\infty}\gamma^{t}\mathbb{P}(s_{t}=s|s_{0})\,. \tag{28}\]

For a distribution \(\rho\in\Delta(\mathcal{S})\), we define \(d_{\rho}^{\pi}(s)=\mathbb{E}_{s_{0}\sim\rho}[d_{s_{0}}^{\pi}(s)]\). We also define the _state-action visitation distribution_\(\bar{d}_{\rho}^{\pi}\) as

\[\bar{d}_{\rho}^{\pi}(s,a)\coloneqq d_{\rho}^{\pi}(s)\pi(a|s)=(1-\gamma)\mathbb{E}_{s_{0} \sim\rho}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathbb{P}(s_{t}=s,a_{t}=a|s_{0}) \right]\,. \tag{29}\]

Furthermore, we extend the definition of \(\bar{d}_{\rho}^{\pi}\) by specifying the initial state-action distribution \(\nu\in\Delta(\mathcal{S}\times\mathcal{A})\) and define

\[\tilde{d}_{\nu}^{\pi}(s,a)\coloneqq(1-\gamma)\underset{(s_{0},a_{0})\sim\nu}{ \mathbb{E}}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathbb{P}(s_{t}=s,a_{t}=a|s_{0},a_{0})\right]\,. \tag{30}\]

Our proposed federated NAC method FedNAC could be seen as a decentralized version of Q-NPG method [1, 2], which we briefly review as follows.

**Q-NPG method.** Q-NPG is a sample version of NPG with function approximation which is suitable for the case where \(\mathcal{S}\) or \(\mathcal{A}\) is large or infinite. We consider the policy with function approximation under softmax parameterization (24).

Given an approximate solution \(\mathbf{w}^{(t)}\) for minimizing the function approximation error \(\ell(\mathbf{w},Q^{f_{\xi^{(t)}}},\bar{d}_{\nu}^{f_{\xi^{(t)}}})\) (see (25)), the Q-NPG update rule \(\mathbf{\xi}^{(t+1)}=\mathbf{\xi}^{(t)}+\alpha\mathbf{w}^{(t)}\), when plugged in parameterization (24), results in the following policy update rule when we set \(\alpha=\eta/(1-\gamma)\):

\[f^{(t+1)}(a|s)\propto f^{(t)}(a|s)\exp\left(\frac{\eta\phi^{\top}(s,a)\mathbf{w}^{(t )}}{1-\gamma}\right)\,, \tag{31}\]

which could be seen as the function approximation version of the update rule (8) of vanilla NPG method.

**Federated NAC method.**_FedNAC_ (describe in Section 4) is presented in Algorithm 3, whose subroutines are written in Algorithm 4, 5. In each iteration \(t\) of FedNAC, each agent \(n\) updates the critic parameter \(\mathbf{w}_{n}^{(t)}\) locally using Algorithm 4, which aims to minimize \(\ell(\mathbf{w},Q_{n}^{(t)},\bar{d}_{n}^{(t)})\) by stochastic gradient descent. Note that since we don't know the Q-function \(Q_{n}^{(t)}\) in the gradients, we need to invoke Algorithm 5[YDG\({}^{+}\)22, Algorithm 3] to give an unbiased estimate \(\widehat{Q}_{n}^{(t)}(s,a)\), where \((s,a)\) is sampled from \(\bar{d}_{n}^{(t)}\) (cf. Theorem E.1). As a consequence, in line 4 of Algorithm 4, we have

\[\mathbb{E}\left[\widehat{\nabla}_{w}\ell(\widetilde{\mathbf{w}}_{k},\widehat{Q}^{ \pi},\bar{d}^{f_{\ell}})\right]=\nabla_{w}\ell(\widetilde{\mathbf{w}}_{k},\widehat {Q}^{\pi},\bar{d}^{f_{\ell}})\,. \tag{32}\]

In each actor iteration, agents share with their neighbors actor and critic parameters, where the tracking scheme is also used.

```
1:Input: number of actor iterations \(T\), number of critic iterations \(K\), actor learning rate \(\alpha\), critic learning rate \(\beta\), discounted factor \(\gamma\in[0,1)\)
2:Initialization: initial state-action distribution \(\nu\), actor parameter \(\mathbf{\xi}^{(0)}=(\mathbf{\xi}_{1}^{(0)\top},\cdots,\mathbf{\xi}_{N}^{(0)\top})^{\top} \in\mathbb{R}^{N\times p}\), \(\mathbf{h}^{(-1)}=\mathbf{w}^{(-1)}=\mathbf{0}\in\mathbb{R}^{N\times p}\)
3:for\(t=0,\cdots,T-1\)do
4: Critic update: \(\mathbf{w}_{n}^{(t)}=\text{ Critic}(K,\nu,\mathbf{\xi}_{n}^{(t)},\gamma,\beta,r_{n})\), \(n\in[N]\) (Algorithm 4)
5: Update the critic parameter for estimating the global Q-function: \[\mathbf{h}^{(t)}=\mathbf{W}\left(\mathbf{h}^{(t-1)}+\mathbf{w}^{(t)}-\mathbf{w}^{(t-1)}\right)\] (33)
6: Actor update: \[\mathbf{\xi}^{(t+1)}=\mathbf{W}\left(\mathbf{\xi}^{(t)}+\alpha\mathbf{h}^{(t)}\right)\] (34)
7:endfor
```

**Algorithm 3** Federated Natural Actor-Critic (FedNAC)

## Appendix D Convergence analysis of FedNPG

For technical convenience, we present first the analysis for entropy-regularized FedNPG and then for vanilla FedNPG.

### Analysis of entropy-regularized FedNPG with exact policy evaluation

To facilitate analysis, we introduce several notation below. For all \(t\geq 0\), we recall \(\overline{\pi}^{(t)}\) as the normalized geometric mean of \(\{\pi_{n}^{(t)}\}_{n\in[N]}\):

\[\overline{\pi}^{(t)}\coloneqq\operatorname{softmax}\left(\frac{1}{N}\sum_{n=1}^ {N}\log\pi_{n}^{(t)}\right)\,, \tag{36}\]

from which we can easily see that for each \((s,a)\in\mathcal{S}\times\mathcal{A}\), \(\overline{\pi}^{(t)}(a|s)\propto\left(\prod_{n=1}^{N}\pi_{n}^{(t)}(a|s)\right)^ {\frac{1}{N}}\). We denote the soft \(Q\)-functions of \(\overline{\pi}^{(t)}\) by \(\overline{Q}_{\tau}^{(t)}\):

\[\overline{Q}_{\tau}^{(t)}\coloneqq\begin{pmatrix}Q_{\tau,1}^{\overline{\pi}^{ (t)}}\\ \vdots\\ Q_{\tau,N}^{\overline{\pi}^{(t)}}\end{pmatrix}\,. \tag{37}\]

In addition, we define \(\widehat{Q}_{\tau}^{(t)}\), \(\overline{Q}_{\tau}^{(t)}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}|}\) and \(\overline{V}_{\tau}^{(t)}\in\mathbb{R}^{|\mathcal{S}|}\) as follows

\[\widehat{Q}_{\tau}^{(t)} \coloneqq\frac{1}{N}\sum_{n=1}^{N}Q_{\tau,n}^{\pi_{n}^{(t)}}\,, \tag{38a}\] \[\overline{Q}_{\tau}^{(t)} \coloneqq Q_{\tau}^{\overline{\pi}^{(t)}}=\frac{1}{N}\sum_{n=1}^ {N}Q_{\tau,n}^{\overline{\pi}^{(t)}}\,.\] (38b) \[\overline{V}_{\tau}^{(t)} \coloneqq V_{\tau}^{\overline{\pi}^{(t)}}=\frac{1}{N}\sum_{n=1}^ {N}V_{\tau,n}^{\overline{\pi}^{(t)}}\,. \tag{38c}\]

For notational convenience, we also denote

\[\alpha\coloneqq 1-\frac{\eta\tau}{1-\gamma}\,. \tag{39}\]

Following [CCC\({}^{+}\)22b], we introduce the following auxiliary sequence \(\{\mathbf{\xi}^{(t)}=(\xi_{1}^{(t)},\cdots,\xi_{N}^{(t)})^{\top}\in\mathbb{R}^{N \times|\mathcal{S}||\mathcal{A}|}\}_{t=0,1,\cdots}\), each recursively defined as

\[\forall(s,a)\in\mathcal{S}\times\mathcal{A}:\quad\mathbf{\xi}^{(0)}(s,a) \coloneqq\frac{\left\lVert\exp\left(Q_{\tau}^{\star}(s,\cdot)/\tau \right)\right\rVert_{1}}{\left\lVert\exp\left(\frac{1}{N}\sum_{n=1}^{N}\log \pi_{n}^{(0)}(\cdot|s)\right)\right\rVert_{1}}\cdot\mathbf{\pi}^{(0)}(a|s)\,, \tag{40a}\] \[\log\mathbf{\xi}^{(t+1)}(s,a) =\mathbf{W}\left(\alpha\log\mathbf{\xi}^{(t)}(s,a)+(1-\alpha)\mathbf{T}^{(t)} (s,a)/\tau\right)\,, \tag{40b}\]

[MISSING_PAGE_EMPTY:20]

_In addition, it holds for all \(t\geq 0\) that_

\[\left\|\overline{Q}_{\tau}^{(t)}-Q_{\tau}^{\star}\right\|_{\infty} \leq\gamma\Omega_{3}^{(t)}+\gamma\Omega_{4}^{(t)}\,, \tag{51}\] \[\left\|\log\overline{\pi}^{(t)}-\log\pi_{\tau}^{\star}\right\|_{ \infty} \leq\frac{2}{\tau}\Omega_{3}^{(t)}\,. \tag{52}\]

Proof.: See Appendix F.1. 

Let \(\rho(\eta)\) denote the spectral norm of \(\mathbf{A}(\eta)\). As \(\mathbf{\Omega}^{(t)}\geq 0\), it is immediate from (49) that

\[\left\|\mathbf{\Omega}^{(t)}\right\|_{2}\leq\rho(\eta)^{t}\big{\|}\mathbf{\Omega}^{(0) }\big{\|}_{2}\,,\]

and therefore we have

\[\left\|\overline{Q}_{\tau}^{(t)}-Q_{\tau}^{\star}\right\|_{\infty}\leq 2\gamma \big{\|}\mathbf{\Omega}^{(t)}\big{\|}_{\infty}\leq 2\gamma\rho(\eta)^{t}\big{\|}\mathbf{ \Omega}^{(0)}\big{\|}_{2}\,,\]

and

\[\left\|\log\overline{\pi}^{(t)}-\log\pi_{\tau}^{\star}\right\|_{\infty}\leq \frac{2}{\tau}\big{\|}\mathbf{\Omega}^{(t)}\big{\|}_{\infty}\leq\frac{2}{\tau}\rho (\eta)^{t}\big{\|}\mathbf{\Omega}^{(0)}\big{\|}_{2}\,.\]

It remains to bound the spectral radius \(\rho(\eta)\), which is achieved by the following lemma.

**Lemma D.3** (Bounding the spectral norm of \(\mathbf{A}(\eta)\)).: _Let_

\[\zeta\coloneqq\frac{(1-\gamma)(1-\sigma)^{2}\tau}{8\left(\tau S_{0}\sigma^{2}+ 10Mc\sigma^{2}/(1-\gamma)+(1-\sigma)^{2}\tau^{2}/16\right)}\,, \tag{53}\]

_where \(S_{0}\coloneqq M\sqrt{N}\left(2+\sqrt{2N}+\frac{M\sqrt{N}}{\tau}\right)\), \(c\coloneqq MN/(1-\gamma)\). For any \(N\in\mathbb{N}_{+},\tau>0,\gamma\in(0,1)\), if_

\[0<\eta\leq\eta_{0}\coloneqq\min\left\{\frac{1-\gamma}{\tau},\zeta\right\}, \tag{54}\]

_then we have_

\[\rho(\eta)\leq\max\left\{\frac{3+\sigma}{4},\frac{1+(1-\alpha)\gamma+\alpha}{2 }\right\}<1\,. \tag{55}\]

Proof.: See Appendix F.2. 

### Analysis of entropy-regularized FedNPG with inexact policy evaluation

We define the collection of _inexact_ Q-function estimates as

\[\mathbf{q}_{\tau}^{(t)}\coloneqq\left(q_{\tau,1}^{\pi^{(t)}_{1}},\cdots,q_{\tau,N }^{\pi^{(t)}_{N}}\right)^{\top},\]

and then the update rule (\(U_{T}\)) should be understood as

\[\mathbf{T}^{(t+1)}(s,a)=\mathbf{W}\left(\mathbf{T}^{(t)}(s,a)+\mathbf{q}_{\tau}^{(t+1)}(s,a)- \mathbf{q}_{\tau}^{(t)}(s,a)\right) \tag{56}\]

in the inexact setting. For notational simplicity, we define \(e_{n}\in\mathbb{R}\) as

\[e_{n}\coloneqq\max_{t\in[T]}\left\|Q_{\tau,n}^{\pi^{(t)}_{n}}-q_{\tau,n}^{\pi^ {(t)}_{n}}\right\|_{\infty}\,,\quad n\in[N]\,, \tag{57}\]

and let \(\mathbf{e}=(e_{1},\cdots,e_{n})^{\top}\). Define \(\widehat{q}_{\tau}^{(t)}\), the approximation of \(\widehat{Q}_{\tau}^{(t)}\) as

\[\widehat{q}_{\tau}^{(t)}\coloneqq\frac{1}{N}\sum_{n=1}^{N}q_{\tau,n}^{\pi^{(t )}_{n}}\,. \tag{58}\]

With slight abuse of notation, we adapt the auxiliary sequence \(\{\overline{\xi}^{(t)}\}_{t=0,\cdots}\) to the inexact updates as

\[\overline{\xi}^{(0)}(s,a) \coloneqq\left\|\exp\left(Q_{\tau}^{\star}(s,\cdot)/\tau\right) \right\|_{1}\cdot\overline{\pi}^{(0)}(a|s)\,, \tag{59a}\] \[\overline{\xi}^{(t+1)}(s,a) \coloneqq\left[\overline{\xi}^{(t)}(s,a)\right]^{\alpha}\exp \left((1-\alpha)\frac{\widehat{q}_{\tau}^{(t)}(s,a)}{\tau}\right)\,,\quad \forall(s,a)\in\mathcal{S}\times\mathcal{A},\ t\geq 0\,. \tag{59b}\]In addition, we define

\[\Omega_{1}^{(t)} \coloneqq\left\lVert u^{(t)}\right\rVert_{\infty}\,, \tag{60a}\] \[\Omega_{2}^{(t)} \coloneqq\left\lVert v^{(t)}\right\rVert_{\infty}\,,\] (60b) \[\Omega_{3}^{(t)} \coloneqq\left\lVert Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t)} \right\rVert_{\infty}\,,\] (60c) \[\Omega_{4}^{(t)} \coloneqq\max\left\{0,-\min_{s,a}\left(\overline{q}_{\tau}^{(t)}( s,a)-\tau\log\overline{\xi}^{(t)}(s,a)\right)\right\}\,, \tag{60d}\]

where

\[u^{(t)}(s,a) \coloneqq\left\lVert\log\mathbf{\xi}^{(t)}(s,a)-\log\overline{\xi}^{( t)}(s,a)\mathbf{1}_{N}\right\rVert_{2}\,, \tag{61}\] \[v^{(t)}(s,a) \coloneqq\left\lVert\mathbf{T}^{(t)}(s,a)-\widehat{q}_{\tau}^{(t)}(s,a)\mathbf{1}_{N}\right\rVert_{2}\,. \tag{62}\]

We let \(\mathbf{\Omega}^{(t)}\) be

\[\mathbf{\Omega}^{(t)}\coloneqq\left(\Omega_{1}^{(t)},\Omega_{2}^{(t)},\Omega_{3}^{ (t)},\Omega_{4}^{(t)}\right)^{\top}\,. \tag{63}\]

With the above preparation, we are ready to state the inexact convergence guarantee of Algorithm 2 in Theorem D.4 below, which is the formal version of Theorem 3.8.

**Theorem D.4**.: _Suppose that \(q_{\tau,n}^{\star(t)}\) are used in replace of \(Q_{\tau,n}^{\star(t)}\) in Algorithm 2. For any \(N\in\mathbb{N}_{+},\tau>0,\gamma\in(0,1)\), there exists \(\eta_{0}>0\) which depends only on \(N,\gamma,\tau,\sigma,|\mathcal{A}|\), such that if \(0<\eta\leq\eta_{0}\) and \(1-\sigma>0\), we have_

\[\left\lVert\overline{Q}_{\tau}^{(t)}-Q_{\tau}^{\star}\right\rVert_ {\infty} \leq 2\gamma\left(\rho(\eta)^{t}\left\lVert\mathbf{\Omega}^{(0)} \right\rVert_{2}+C_{2}\max_{n\in[N],t\in[T]}\left\lVert Q_{\tau,n}^{\star(t)}- q_{\tau,n}^{\star(t)}\right\rVert_{\infty}\right)\,, \tag{64}\] \[\left\lVert\log\pi_{\tau}^{\star}-\log\overline{\pi}^{(t)}\right\rVert _{\infty} \leq\frac{2}{\tau}\left(\rho(\eta)^{t}\left\lVert\mathbf{\Omega}^{( 0)}\right\rVert_{2}+C_{2}\max_{n\in[N],t\in[T]}\left\lVert Q_{\tau,n}^{\star(t )}-q_{\tau,n}^{\star(t)}\right\rVert_{\infty}\right)\,. \tag{65}\]

_Moreover, the consensus errors satisfy:_

\[\forall n\in[N]:\quad\left\lVert\log\pi_{n}^{(t)}-\log\overline{\pi}^{(t)} \right\rVert_{\infty}\leq 2\left(\rho(\eta)^{t}\left\lVert\mathbf{\Omega}^{(0)} \right\rVert_{2}+C_{2}\max_{n\in[N],t\in[T]}\left\lVert Q_{\tau,n}^{\star(t)} -q_{\tau,n}^{\star(t)}\right\rVert_{\infty}\right)\,, \tag{66}\]

_where \(\rho(\eta)\leq\max\{1-\frac{\tau\eta}{2},\frac{3+\sigma}{4}\}<1\) is the same as in Theorem D.1, and \(C_{2}\coloneqq\frac{\sigma\sqrt{N}(2(1-\gamma)+M\sqrt{N}\eta)+2\gamma^{2}+\eta \tau}{(1-\gamma)(1-\rho(\eta))}\)._

From Theorem D.4, we can conclude that if

\[\max_{n\in[N],t\in[T]}\left\lVert Q_{\tau,n}^{\star(t)}-q_{\tau,n}^{\star(t)} \right\rVert_{\infty}\leq\frac{(1-\gamma)(1-\rho(\eta))\varepsilon}{2\gamma \left(\sigma\sqrt{N}(2(1-\gamma)+M\sqrt{N}\eta)+2\gamma^{2}+\eta\tau\right)}\,, \tag{67}\]

then inexact entropy-regularized FedNPG could still achieve \(2\varepsilon\)-accuracy (i.e. \(\left\lVert\overline{Q}_{\tau}^{(t)}-Q_{\tau}^{\star}\right\rVert_{\infty}\leq 2\varepsilon\)) within \(\max\left\{\frac{2}{\tau\eta},\frac{4}{1-\sigma}\right\}\log\frac{2\gamma\| \mathbf{\Omega}^{(0)}\|_{2}}{\varepsilon}\) iterations.

_Remark D.5_.: When \(\eta=\eta_{0}\) (cf. (54) and (53)) and \(\tau\leq 1\), the RHS of (67) is of the order

\[\mathcal{O}\left(\frac{(1-\gamma)\tau\eta_{0}\varepsilon}{\gamma(\gamma^{2}+ \sigma\sqrt{N}(1-\gamma))}\right)=\mathcal{O}\left(\frac{(1-\gamma)^{8}\tau^ {2}(1-\sigma)^{2}\varepsilon}{\gamma(\gamma^{2}+\sigma\sqrt{N}(1-\gamma))( \gamma^{2}N\sigma^{2}+(1-\sigma)^{2}\tau^{2}(1-\gamma)^{6})}\right)\,,\]

which can be translated into a crude sample complexity bound when using fresh samples to estimate the soft Q-functions in each iteration.

The rest of this section outlines the proof of Theorem D.4. We first state a key lemma that tracks the error recursion of Algorithm 2 with inexact policy evaluation, which is a modified version of Lemma D.2.

**Lemma D.6**.: _The following linear system holds for all \(t\geq 0\):_

\[\mathbf{\Omega}^{(t+1)}\leq\mathbf{A}(\eta)\mathbf{\Omega}^{(t)}+\underbrace{ \begin{pmatrix}0\\ \sigma\sqrt{N}\left(2+\frac{M\sqrt{N}\eta}{1-\gamma}\right)\\ \frac{\eta\tau}{1-\gamma}\\ \frac{2\gamma^{2}}{1-\gamma}\end{pmatrix}\left\|\mathbf{e}\right\|_{\infty}}_{=:\bm {b}(\eta)}\,, \tag{68}\]

_where \(\mathbf{A}(\eta)\) is provided in Lemma D.2. In addition, it holds for all \(t\geq 0\) that_

\[\left\|\overline{Q}^{(t)}_{\tau}-Q^{\star}_{\tau}\right\|_{\infty} \leq\gamma\Omega^{(t)}_{3}+\gamma\Omega^{(t)}_{4}\,, \tag{69}\] \[\left\|\log\overline{\pi}^{(t)}-\log\pi^{\star}_{\tau}\right\|_{ \infty} \leq\frac{2}{\tau}\Omega^{(t)}_{3}\,. \tag{70}\]

Proof.: See Appendix F.3. 

By (68), we have

\[\forall t\in N_{+}:\quad\mathbf{\Omega}^{(t)}\leq\mathbf{A}(\eta)^{t}\mathbf{\Omega}^{(0) }+\sum_{s=1}^{t}\mathbf{A}(\eta)^{t-s}\mathbf{b}(\eta)\,,\]

which gives

\[\left\|\mathbf{\Omega}^{(t)}\right\|_{2} \leq\rho(\eta)^{t}\left\|\mathbf{\Omega}^{(0)}\right\|_{2}+\sum_{s=1} ^{t}\rho(\eta)^{t-s}\left\|\mathbf{b}(\eta)\right\|_{2}\left\|\mathbf{e}\right\|_{\infty}\] \[\leq\rho(\eta)^{t}\left\|\mathbf{\Omega}^{(0)}\right\|_{2}+\frac{ \sigma\sqrt{N}(2(1-\gamma)+M\sqrt{N}\eta)+2\gamma^{2}+\eta\tau}{(1-\gamma)(1- \rho(\eta))}\left\|\mathbf{e}\right\|_{\infty}\,. \tag{71}\]

Here, (71) follows from \(\left\|\mathbf{b}(\eta)\right\|_{2}\leq\left\|\mathbf{b}(\eta)\right\|_{1}=\frac{ \sigma\sqrt{N}(2(1-\gamma)+M\sqrt{N}\eta)+2\gamma^{2}+\eta\tau}{1-\gamma} \left\|\mathbf{e}\right\|_{\infty}\) and \(\sum_{s=1}^{t}\rho(\eta)^{t-s}\leq 1/(1-\rho(\eta))\). Recall that the bound on \(\rho(\eta)\) has already been established in Lemma D.3. Therefore we complete the proof of Theorem D.4 by combining the above inequality with (69) and (70) in a similar fashion as before. We omit further details for conciseness.

### Analysis of FedNPG with exact policy evaluation

We state the formal version of Theorem 3.3 below.

**Theorem D.7**.: _Suppose all \(\pi_{n}^{(0)}\) in Algorithm 1 are initialized as uniform distribution. When_

\[0<\eta\leq\eta_{1}\coloneqq\frac{(1-\sigma)^{2}(1-\gamma)^{3}}{8(1+\gamma) \gamma\sqrt{N}\sigma^{2}}\,,\]

_we have_

\[\frac{1}{T}\sum_{t=0}^{T-1}\left(V^{\star}(\rho)-V^{\overline{\pi}^{(t)}}(\rho )\right)\leq\frac{V^{\star}(d_{\rho}^{\pi^{\star}})}{(1-\gamma)T}+\frac{\log \left|\mathcal{A}\right|}{\eta T}+\frac{8(1+\gamma)^{2}\gamma^{2}N\sigma^{2}} {(1-\gamma)^{9}(1-\sigma)^{2}}\eta^{2} \tag{72}\]

_for any fixed state distribution \(\rho\). Furthermore, we have_

\[\forall n\in[N]:\quad\left\|\log\pi_{n}^{(t)}-\log\overline{\pi}^{(t)}\right\| _{\infty}\leq\frac{32N\sigma}{3(1-\gamma)^{4}(1-\sigma)}\eta\,. \tag{73}\]

The rest of this section is dedicated to prove Theorem D.7. Similar to (37), we denote the \(Q\)-functions of \(\overline{\pi}^{(t)}\) by \(\overline{\mathbf{Q}}^{(t)}\):

\[\overline{\mathbf{Q}}^{(t)}\coloneqq\begin{pmatrix}Q_{1}^{\overline{\pi}^{(t)}}\\ \vdots\\ Q_{N}^{\overline{\pi}^{(t)}}\end{pmatrix}\,. \tag{74}\]In addition, similar to (38), we define \(\widetilde{Q}^{(t)}\), \(\overline{Q}^{(t)}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}|}\) and \(\overline{V}^{(t)}\in\mathbb{R}^{|\mathcal{S}|}\) as follows

\[\widetilde{Q}^{(t)} \coloneqq\frac{1}{N}\sum_{n=1}^{N}Q_{n}^{\pi^{(t)}_{n}}\,, \tag{75a}\] \[\overline{Q}^{(t)} \coloneqq Q^{\overline{\pi}^{(t)}}=\frac{1}{N}\sum_{n=1}^{N}Q_{n}^{ \overline{\pi}^{(t)}}\,.\] (75b) \[\overline{V}^{(t)} \coloneqq V^{\overline{\pi}^{(t)}}=\frac{1}{N}\sum_{n=1}^{N}V_{n}^{ \overline{\pi}^{(t)}}\,. \tag{75c}\]

Following the same strategy in the analysis of entropy-regularized FedNPG, we introduce the auxiliary sequence \(\{\mathbf{\xi}^{(t)}=(\xi^{(t)}_{1},\cdots,\xi^{(t)}_{N})^{\top}\in\mathbb{R}^{N \times|\mathcal{S}||\mathcal{A}|}\}\) recursively:

\[\mathbf{\xi}^{(0)}(s,a) \coloneqq\frac{1}{\left\|\exp\left(\frac{1}{N}\sum_{n=1}^{N}\log \pi^{(0)}_{n}(\cdot|s)\right)\right\|_{1}}\cdot\mathbf{\pi}^{(0)}(a|s)\,, \tag{76a}\] \[\log\mathbf{\xi}^{(t+1)}(s,a) =\mathbf{W}\left(\log\mathbf{\xi}^{(t)}(s,a)+\frac{\eta}{1-\gamma}\mathbf{T}^ {(t)}(s,a)\right), \tag{76b}\]

as well as the averaged auxiliary sequence \(\{\overline{\xi}^{(t)}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}|}\}\):

\[\overline{\xi}^{(0)}(s,a) \coloneqq\overline{\pi}^{(0)}(a|s)\,, \tag{77a}\] \[\log\overline{\xi}^{(t+1)}(s,a) \coloneqq\log\overline{\xi}^{(t)}(s,a)+\frac{\eta}{1-\gamma} \widetilde{Q}^{(t)}(s,a)\,,\quad\forall(s,a)\in\mathcal{S}\times\mathcal{A},\ t\geq 0\,. \tag{77b}\]

As usual, we collect the consensus errors in a vector \(\mathbf{\Omega}^{(t)}=\left(\left\|u^{(t)}\right\|_{\infty},\left\|v^{(t)}\right\| _{\infty}\right)^{\top}\), where \(u^{(t)},v^{(t)}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}|}\) are defined as:

\[u^{(t)}(s,a) \coloneqq\left\|\log\mathbf{\xi}^{(t)}(s,a)-\log\overline{\xi}^{(t)} (s,a)\mathbf{1}_{N}\right\|_{2}, \tag{78}\] \[v^{(t)}(s,a) \coloneqq\left\|\mathbf{T}^{(t)}(s,a)-\widehat{Q}^{(t)}(s,a)\mathbf{ 1}_{N}\right\|_{2}. \tag{79}\]

**Step 1: establishing the error recursion.** The next key lemma establishes the error recursion of Algorithm 1.

**Lemma D.8**.: _The updates of FedNPG satisfy_

\[\mathbf{\Omega}^{(t+1)}\leq\underbrace{\begin{pmatrix}\sigma&\frac{\eta}{1-\gamma }\sigma\\ J\sigma&\sigma\left(1+\frac{(1+\gamma)\gamma\sqrt{N}\eta}{(1-\gamma)^{3}}\sigma \right)\end{pmatrix}}_{=:\mathbf{B}(\eta)}\mathbf{\Omega}^{(t)}+\underbrace{\begin{pmatrix}0 \\ \frac{(1+\gamma)\gamma N\sigma}{(1-\gamma)^{3}}\eta\\ \end{pmatrix}}_{=:\mathbf{d}(\eta)} \tag{80}\]

_for all \(t\geq 0\), where_

\[J\coloneqq\frac{2(1+\gamma)\gamma}{(1-\gamma)^{2}}\sqrt{N}\,. \tag{81}\]

_In addition, we have_

\[\phi^{(t+1)}(\eta)\leq\phi^{(t)}(\eta)+\frac{2(1+\gamma)\gamma}{(1-\gamma)^{4} }\eta\big{\|}u^{(t)}\big{\|}_{\infty}-\eta\left(V^{\star}(\rho)-\overline{V}^ {(t)}(\rho)\right)\,, \tag{82}\]

_where_

\[\phi^{(t)}(\eta)\coloneqq\mathbb{E}_{s\sim d_{\mathbf{\xi}}^{\ast}}\left[\mathsf{ KL}\big{(}\pi^{\star}(\cdot|s)\,\|\,\overline{\pi}^{(t)}(\cdot|s)\big{)}\right]-\frac{ \eta}{1-\gamma}\overline{V}^{(t)}(d_{\rho}^{\pi^{\ast}})\,,\quad\forall t\geq 0\,. \tag{83}\]

_Moreover, when \(\eta\leq\eta_{1}\), we have_

\[\forall n\in[N]:\quad\left\|\log\pi^{(t)}_{n}-\log\overline{\pi}^{(t)}\right\| _{\infty}\leq 2\left(\frac{3}{8}\sigma+\frac{5}{8}\right)^{t}\left\|\mathbf{\Omega}^{(0) }\right\|_{2}+\frac{32N\sigma}{3(1-\gamma)^{4}(1-\sigma)}\eta\,. \tag{84}\]

Proof.: See Appendix F.4.

Note that when all \(\pi_{n}^{(0)}\) in Algorithm 1 are initialized as uniform distribution, \(\mathbf{\Omega}^{(0)}=\mathbf{0}\) and (84) indicates (73) in Theorem D.7.

**Step 2: bounding the value functions.** Let \(\mathbf{p}\in\mathbb{R}^{2}\) be defined as:

\[\mathbf{p}(\eta)=\begin{pmatrix}p_{1}(\eta)\\ p_{2}(\eta)\end{pmatrix}\coloneqq\frac{2(1+\gamma)\gamma}{(1-\gamma)^{4}}\begin{pmatrix} \frac{\sigma(1-\gamma)\left(1-\sigma-(1+\gamma)\gamma\sqrt{N}\sigma\eta/(1- \gamma)^{3}\right)\eta}{(1-\gamma)\left(1-\sigma-(1+\gamma)\gamma\sqrt{N} \sigma^{2}\eta/(1-\gamma)^{3}\right)(1-\sigma)-J\sigma^{2}\eta}\\ \frac{\sigma\eta^{2}}{(1-\gamma)\left(1-\sigma-(1+\gamma)\gamma\sqrt{N}\sigma^ {2}\eta/(1-\gamma)^{3}\right)(1-\sigma)-J\sigma^{2}\eta}\end{pmatrix}\,; \tag{85}\]

the rationale for this choice will be made clear momentarily. We define the following Lyapunov function

\[\Phi^{(t)}(\eta)=\phi^{(t)}(\eta)+\mathbf{p}(\eta)^{\top}\mathbf{\Omega}^{(t)}\,,\quad \forall t\geq 0\,, \tag{86}\]

which satisfies

\[\Phi^{(t+1)}(\eta) =\phi^{(t+1)}(\eta)+\mathbf{p}(\eta)^{\top}\mathbf{\Omega}^{(t+1)}\] \[\leq\phi^{(t)}(\eta)+\frac{2(1+\gamma)\gamma}{(1-\gamma)^{4}} \eta\big{\|}u^{(t)}\big{\|}_{\infty}-\eta\left(V^{\star}(\rho)-\overline{V}^{( t)}(\rho)\right)+\mathbf{p}(\eta)^{\top}\left(\mathbf{B}(\eta)\mathbf{\Omega}^{(t)}+\mathbf{d}( \eta)\right)\] \[=\Phi^{(t)}(\eta)+\left[\mathbf{p}(\eta)^{\top}\left(\mathbf{B}(\eta)-\bm {I}\right)+\left(\frac{2(1+\gamma)\gamma}{(1-\gamma)^{4}}\eta,0\right)\right] \mathbf{\Omega}^{(t)}-\eta\left(V^{\star}(\rho)-\overline{V}^{(t)}(\rho)\right)\] \[\qquad+p_{2}(\eta)\frac{(1+\gamma)\gamma N\sigma}{(1-\gamma)^{4} }\eta\,. \tag{87}\]

Here, the second inequality follows from (82). One can verify that the second term vanishes due to the choice of \(\mathbf{p}(\eta)\):

\[\mathbf{p}(\eta)^{\top}\left(\mathbf{B}(\eta)-\mathbf{I}\right)+\left(\frac{2(1+\gamma) \gamma}{(1-\gamma)^{4}}\eta,0\right)=(0,0)\,. \tag{88}\]

Therefore, we conclude that

\[V^{\star}(\rho)-\overline{V}^{(t)}(\rho)\leq\frac{\Phi^{(t)}(\eta)-\Phi^{(t+1 )}(\eta)}{\eta}+p_{2}(\eta)\frac{(1+\gamma)\gamma N\sigma}{(1-\gamma)^{4}}\,.\]

Averaging over \(t=0,\cdots,T-1\),

\[\frac{1}{T}\sum_{t=0}^{T-1}\left(V^{\star}(\rho)-\overline{V}^{( t)}(\rho)\right)\] \[\leq\frac{\Phi^{(0)}(\eta)-\Phi^{(T)}(\eta)}{\eta T}+\frac{2(1+ \gamma)^{2}\gamma^{2}}{(1-\gamma)^{8}}\cdot\frac{N\sigma^{2}\eta^{2}}{(1- \gamma)(1-\sigma-(1+\gamma)\gamma\sqrt{N}\sigma^{2}\eta/(1-\gamma)^{3})(1- \sigma)-\sigma^{2}J\eta}\,. \tag{89}\]

**Step 3: simplifying the expression.** We first upper bound the first term in the RHS of (89). Assuming uniform initialization for all \(\pi_{n}^{(0)}\) in Algorithm 1, we have \(\big{\|}u^{(0)}\big{\|}_{\infty}=\big{\|}v^{(0)}\big{\|}_{\infty}=0\), and

\[\mathbb{E}_{s\sim d_{\rho}^{\star}}\left[\mathsf{KL}\big{(}\pi^{\star}(\cdot|s )\,\|\,\overline{\pi}^{(0)}(\cdot|s)\big{)}\right]\leq\log|\mathcal{A}|.\]

Therefore, putting together relations (86) and (221) we have

\[\frac{\Phi^{(0)}(\eta)-\Phi^{(T)}(\eta)}{\eta T}\leq\frac{\log|\mathcal{A}|}{T \eta}+\frac{1}{T}\left(\mathbf{p}(\eta)^{\top}\mathbf{\Omega}^{(0)}/\eta+\frac{V^{\star }(d_{\rho}^{\star})}{1-\gamma}\right)=\frac{\log|\mathcal{A}|}{T\eta}+\frac{V ^{\star}(d_{\rho}^{\star})}{T(1-\gamma)}\,, \tag{90}\]

To continue, we upper bound the second term in the RHS of (89). Note that

\[\eta\leq\eta_{1}\leq\frac{(1-\sigma)(1-\gamma)^{3}}{2(1+\gamma)\gamma\sqrt{N} \sigma^{2}}\,,\]

which gives

\[\frac{(1+\gamma)\gamma\sqrt{N}\sigma^{2}}{(1-\gamma)^{3}}\eta\leq\frac{1- \sigma}{2}. \tag{91}\]Thus we have

\[(1-\gamma)(1-\sigma-(1+\gamma)\gamma\sqrt{N}\sigma^{2}\eta/(1-\gamma)^ {3})(1-\sigma)-J\sigma^{2}\eta\] \[\geq(1-\gamma)(1-\sigma)^{2}/2-J\sigma^{2}\eta_{1}\] \[\geq(1-\gamma)(1-\sigma)^{2}/4\,, \tag{92}\]

where the first inequality follows from (91) and the second inequality follows from the definition of \(\eta_{1}\) and \(J\). By (92), we deduce

\[\frac{2(1+\gamma)^{2}\gamma^{2}}{(1-\gamma)^{8}}\cdot\frac{N\sigma^{2}\eta^{2 }}{(1-\gamma)(1-\sigma-(1+\gamma)\gamma\sqrt{N}\sigma^{2}\eta/(1-\gamma)^{3})( 1-\sigma)-J\sigma^{2}\eta}\leq\frac{8(1+\gamma)^{2}\gamma^{2}N\sigma^{2}}{(1- \gamma)^{9}(1-\sigma)^{2}}\eta^{2}\,, \tag{93}\]

and our advertised bound (72) thus follows from plugging (90) and (93) into (89).

### Analysis of FedNPG with inexact policy evaluation

We state the formal version of Theorem 3.5 below.

**Theorem D.9**.: _Suppose that \(q_{n}^{\pi^{(t)}}\) are used in replace of \(Q_{n}^{\pi^{(t)}}\) in Algorithm 1. Suppose all \(\pi^{(0)}_{n}\) in Algorithm 1 set to uniform distribution. Let_

\[0<\eta\leq\eta_{1}\coloneqq\frac{(1-\sigma)^{2}(1-\gamma)^{3}}{8(1+\gamma) \gamma\sqrt{N}\sigma^{2}}\,,\]

_we have_

\[\frac{1}{T}\sum_{t=0}^{T-1}\left(V^{\star}(\rho)-V^{\overline{\pi} ^{(t)}}(\rho)\right)\] \[\leq\frac{V^{\star}(d_{\rho}^{\pi^{\star}})}{(1-\gamma)T}+\frac{ \log|\mathcal{A}|}{\eta T}+\frac{8(1+\gamma)^{2}\gamma^{2}N\sigma^{2}}{(1- \gamma)^{9}(1-\sigma)^{2}}\eta^{2}\] \[\qquad+\left[\frac{8(1+\gamma)\gamma}{(1-\gamma)^{5}(1-\sigma)^{2 }}\sqrt{N}\sigma\eta\left(\frac{(1+\gamma)\gamma\eta\sqrt{N}}{(1-\gamma)^{3}}+ 2\right)+\frac{2}{(1-\gamma)^{2}}\right]\max_{n\in[N],t\in[T]}\left\|Q_{n}^{\pi ^{(t)}}-q_{n}^{\pi^{(t)}_{n}}\right\|_{\infty}\]

_for any fixed state distribution \(\rho\). Furthermore, we have_

\[\forall n\in[N]:\quad\left\|\log\pi^{(t)}_{n}-\log\overline{\pi} ^{(t)}\right\|_{\infty}\leq\frac{32}{3(1-\sigma)}\left(\frac{N\sigma}{(1- \gamma)^{4}}\eta+\sqrt{N}\sigma\left(\frac{\eta\sqrt{N}}{(1-\gamma)^{3}}+1 \right)\max_{n\in[N],t\in[T]}\left\|Q_{n}^{\pi^{(t)}}-q_{n}^{\pi^{(t)}_{n}} \right\|_{\infty}\right)\,. \tag{94}\]

We next outline the proof of Theorem D.9. With slight abuse of notation, we again define \(e_{n}\in\mathbb{R}\) as

\[e_{n}\coloneqq\max_{t\in[T]}\left\|Q_{n}^{\pi^{(t)}_{n}}-q_{n}^{\pi^{(t)}_{n} }\right\|_{\infty}\,,\quad n\in[N]\,, \tag{95}\]

and let \(\mathbf{e}=(e_{1},\cdots,e_{n})^{\top}\). We define the collection of _inexact_ Q-function estimates as

\[\mathbf{q}^{(t)}\coloneqq\left(q_{1}^{\pi^{(t)}_{1}},\cdots,q_{N}^{\pi^{(t)}_{N}} \right)^{\top},\]

and then the update rule (16) should be understood as

\[\mathbf{T}^{(t+1)}(s,a)=\mathbf{W}\left(\mathbf{T}^{(t)}(s,a)+\mathbf{q}^{(t+1)}(s,a)-\mathbf{q}^{ (t)}(s,a)\right) \tag{96}\]

in the inexact setting. Define \(\widehat{q}^{(t)}\), the approximation of \(\widehat{Q}^{(t)}\) as

\[\widehat{q}^{(t)}\coloneqq\frac{1}{N}\sum_{n=1}^{N}q_{n}^{\pi^{(t)}_{n}}\,, \tag{97}\]we adapt the averaged auxiliary sequence \(\{\overline{\xi}^{(t)}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}|}\}\) to the inexact updates as follows:

\[\overline{\xi}^{(0)}(s,a) \coloneqq\overline{\pi}^{(0)}(a|s)\,, \tag{98a}\] \[\overline{\xi}^{(t+1)}(s,a) \coloneqq\overline{\xi}^{(t)}(s,a)\exp\left(\frac{\eta}{1-\gamma }\widehat{q}^{(t)}(s,a)\right)\,,\quad\forall(s,a)\in\mathcal{S}\times \mathcal{A},\ t\geq 0\,. \tag{98b}\]

As usual, we define the consensus error vector as \(\mathbf{\Omega}^{(t)}=(\left\|u^{(t)}\right\|_{\infty},\left\|v^{(t)}\right\|_{ \infty})^{\top}\), where \(u^{(t)},v^{(t)}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}|}\) are given by

\[u^{(t)}(s,a) \coloneqq\left\|\log\mathbf{\xi}^{(t)}(s,a)-\log\overline{\xi}^{(t)} (s,a)\mathbf{1}_{N}\right\|_{2}\,, \tag{99}\] \[v^{(t)}(s,a) \coloneqq\left\|\mathbf{T}^{(t)}(s,a)-\widehat{q}^{(t)}(s,a)\mathbf{1}_{ N}\right\|_{2}\,. \tag{100}\]

The following lemma characterizes the dynamics of the error vector \(\mathbf{\Omega}^{(t)}\), perturbed by additional approximation error.

**Lemma D.10**.: _The updates of inexact FedNPG satisfy_

\[\mathbf{\Omega}^{(t+1)}\leq\mathbf{B}(\eta)\mathbf{\Omega}^{(t)}+\mathbf{d}(\eta)+\underbrace{ \left(\frac{0}{\sqrt{N}\sigma\left(\frac{(1+\gamma)\gamma\eta\sqrt{N}}{(1- \gamma)^{3}}+2\right)}\right)\left\|\mathbf{e}\right\|_{\infty}}_{=:\mathbf{e}(\eta)}\,. \tag{101}\]

_In addition, we have_

\[\phi^{(t+1)}(\eta)\leq\phi^{(t)}(\eta)+\frac{2(1+\gamma)\gamma}{(1-\gamma)^{4} }\eta\left\|u^{(t)}\right\|_{\infty}+\frac{2\eta}{(1-\gamma)^{2}}\left\|\mathbf{e} \right\|_{\infty}-\eta\left(V^{\star}(\rho)-\overline{V}^{(t)}(\rho)\right)\,, \tag{102}\]

_where \(\phi^{(t)}(\eta)\) is defined in (83). Moreover, when \(\eta\leq\eta_{1}\), we have_

\[\forall n\in[N]:\quad\left\|\log\pi_{n}^{(t)}-\log\overline{\pi}^{(t)}\right\| _{\infty}\leq 2\left(\frac{3}{8}\sigma+\frac{5}{8}\right)^{t}\left\|\mathbf{ \Omega}^{(0)}\right\|_{2}+\frac{32}{3(1-\sigma)}\left(\frac{N\sigma}{(1- \gamma)^{4}}\eta+\sqrt{N}\sigma\left(\frac{\eta\sqrt{N}}{(1-\gamma)^{3}}+1 \right)\left\|\mathbf{e}\right\|_{\infty}\right)\,. \tag{103}\]

Proof.: See Appendix F.5. 

Similar to (87), we can recursively bound \(\Phi^{(t)}(\eta)\) (defined in (86)) as

\[\Phi^{(t+1)}(\eta) =\phi^{(t+1)}(\eta)+\mathbf{p}(\eta)^{\top}\mathbf{\Omega}^{(t+1)}\] \[\stackrel{{\eqref{eq:g_1}}}{{\leq}}\phi^{(t)}(\eta)+ \frac{2(1+\gamma)\gamma}{(1-\gamma)^{4}}\eta\left\|u^{(t)}\right\|_{\infty}+ \frac{2\eta}{(1-\gamma)^{2}}\left\|\mathbf{e}\right\|_{\infty}-\eta\left(V^{\star} (\rho)-\overline{V}^{(t)}(\rho)\right)\] \[\qquad\qquad+\mathbf{p}(\eta)^{\top}\left(\mathbf{B}(\eta)\mathbf{\Omega}^{( t)}+\mathbf{d}(\eta)+\mathbf{c}(\eta)\right)\] \[=\Phi^{(t)}(\eta)+\underbrace{\left[\mathbf{p}(\eta)^{\top}\left(\mathbf{ B}(\eta)-\mathbf{I}\right)+\left(\frac{2(1+\gamma)\gamma}{(1-\gamma)^{4}}\eta,0 \right)\right]}_{=(0,0)\text{ via \eqref{eq:g_1}}}\mathbf{\Omega}^{(t)}-\eta\left(V^{\star}( \rho)-\overline{V}^{(t)}(\rho)\right)\] \[\qquad\qquad+p_{2}(\eta)\frac{(1+\gamma)\gamma N\sigma}{(1- \gamma)^{4}}\eta+\left[p_{2}(\eta)\sqrt{N}\sigma\left(\frac{(1+\gamma)\gamma \eta\sqrt{N}}{(1-\gamma)^{3}}+2\right)+\frac{2\eta}{(1-\gamma)^{2}}\right] \left\|\mathbf{e}\right\|_{\infty}\,. \tag{104}\]

From the above expression we know that

\[V^{\star}(\rho)-\overline{V}^{(t)}(\rho)\leq\frac{\Phi^{(t)}(\eta)-\Phi^{(t+1 )}(\eta)}{\eta}+p_{2}(\eta)\frac{(1+\gamma)\gamma N\sigma}{(1-\gamma)^{4}}+ \left[p_{2}(\eta)\sqrt{N}\sigma\left(\frac{(1+\gamma)\gamma\sqrt{N}}{(1-\gamma) ^{3}}+\frac{2}{\eta}\right)+\frac{2}{(1-\gamma)^{2}}\right]\left\|\mathbf{e}\right\|_ {\infty}\,,\]which gives

\[\frac{1}{T}\sum_{t=0}^{T-1}\left(V^{*}(\rho)-\overline{V}^{(t)}(\rho) \right)\leq\frac{\Phi^{(0)}(\eta)-\Phi^{(T)}(\eta)}{\eta T}+p_{2}(\eta)\frac{(1+ \gamma)\gamma N\sigma}{(1-\gamma)^{4}}\\ +\left[p_{2}(\eta)\sqrt{N}\sigma\left(\frac{(1+\gamma)\gamma\sqrt{ N}}{(1-\gamma)^{3}}+\frac{2}{\eta}\right)+\frac{2}{(1-\gamma)^{2}}\right]\|\mathbf{e}\|_{\infty} \tag{105}\]

via telescoping. Combining the above expression with (90), (92) and (93), we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\left(V^{*}(\rho)-\overline{V}^{(t)}( \rho)\right)\leq\frac{\log|\mathcal{A}|}{T\eta}+\frac{V^{*}(d_{p}^{*^{*}})}{T( 1-\gamma)}+\frac{8(1+\gamma)^{2}\gamma^{2}N\sigma}{(1-\gamma)^{9}(1-\sigma)^{ 2}}\eta^{2}\\ +\left[\frac{8(1+\gamma)\gamma}{(1-\gamma)^{5}(1-\sigma)^{2}} \sqrt{N}\sigma\eta\left(\frac{(1+\gamma)\gamma\eta\sqrt{N}}{(1-\gamma)^{3}}+2 \right)+\frac{2}{(1-\gamma)^{2}}\right]\|\mathbf{e}\|_{\infty}\;, \tag{106}\]

which establishes (94).

## Appendix E Convergence analysis of FedNAC

Let \(\pi^{*}\) be an optimal policy and does not need to belong to the log-linear policy class. Fix a state distribution \(\rho\in\Delta(\mathcal{S})\) and a state-action distribution \(\nu\). To simplify the notation, we denote \(d_{\rho}^{\pi^{*}}\) as \(d_{\star}\), \(d^{f_{\mathbf{\xi}^{(t)}}}\) as \(d^{(t)}\), \(\bar{d}_{n}^{(t)}\) as \(\bar{d}_{\nu}^{f_{\mathbf{\xi}^{(t)}}}\), and define \(d_{n}^{(t)}\) and \(\bar{d}_{n}^{(t)}\) analogously. We also let \(Q_{n}^{(t)}\) denote \(Q_{n}^{\xi_{n}^{(t)}}\). Define

\[\vartheta_{\rho}\coloneqq\frac{1}{1-\gamma}\left\|\frac{d_{\star}}{\rho}\right\| _{\infty}\geq\frac{1}{1-\gamma} \tag{107}\]

and assume \(\vartheta_{\rho}<\infty\).

We also introduce a weighted KL divergence given by

\[D_{\star}^{(t)}\coloneqq\mathbb{E}_{s\sim d_{\star}}\left[\mathsf{KL}\left( \pi^{*}(\cdot|s)\,\|\,\pi^{(t)}(\cdot|s)\right)\right]\,, \tag{108}\]

where \(\mathsf{KL}\left(\cdot\,\|\,\cdot\right):\mathbb{R}^{|\mathcal{A}|}\times \mathbb{R}^{|\mathcal{A}|}\to\mathbb{R}\) is the Kullback-Leibler (KL) divergence:

\[\forall f,g\in\mathbb{R}^{|\mathcal{A}|}:\quad\mathsf{KL}\left(f\,\|\,g\right) \coloneqq\sum_{a\in\mathcal{A}}f(a)\log\left(\frac{f(a)}{g(a)}\right)\,. \tag{109}\]

Given a state distribution \(\rho\) and an optimal policy \(\pi^{\star}\), we define a state-action measure \(\tilde{d}^{\star}\) as

\[\tilde{d}^{\star}(s,a)\coloneqq d_{\star}(s)\cdot\text{Unif}_{\mathcal{A}}(a )=\frac{d_{\star}(s)}{|\mathcal{A}|}. \tag{110}\]

The following theorem guarantees that for any fixed policy \(\pi\) and state-action distribution \(\nu\in\Delta(\mathcal{S}\times\mathcal{A})\), the Q-Sampler algorithm (cf. Algorithm 5) samples \((s,a)\) from \(\tilde{d}_{\nu}^{\pi}\) and gives an unbiased estimate \(\tilde{Q}^{\pi}(s,a)\) of \(Q^{\pi}(s,a)\), whose proof can be found in [32, Lemma 4].

**Lemma E.1** (Lemma 4 in [32]).: _Consider the output \((s_{h},a_{h})\) and \(\tilde{Q}^{\pi}(s_{h},a_{h})\) of Algorithm 5. It follows that_

\[\mathbb{E}[h+1] =\frac{1}{1-\gamma}\,,\] \[P(s_{h}=s,a_{h}=a) =\tilde{d}_{\nu}^{\pi}(s,a)\,,\] \[\mathbb{E}\left[\tilde{Q}^{\pi}(s_{h},a_{h})|s_{h},a_{h}\right] =Q^{\pi}(s_{h},a_{h})\,.\]To present the convergence results of FedNAC, we further introduce the following notation, where \(t\in\mathbb{N}\) represents the iteration step in FedNAC:

\[\hat{\mathbf{w}}^{(t)} \coloneqq\frac{1}{N}\sum_{n=1}^{N}\mathbf{w}_{n}^{(t)}, \tag{111a}\] \[\mathbf{\bar{\xi}}^{(t)} \coloneqq\frac{1}{N}\sum_{n=1}^{N}\mathbf{\xi}_{n}^{(t)},\] (111b) \[\bar{f}^{(t)} \coloneqq f_{\bar{\xi}^{(t)}},\] (111c) \[f_{n}^{(t)} \coloneqq f_{\xi_{n}^{(t)}},\] (111d) \[\mathbf{w}_{*,n}^{(t)} \in\arg\min_{\mathbf{w}}\ell\left(\mathbf{w},Q_{n}^{(t)},\tilde{d}_{n}^{(t )}\right),\] (111e) \[\hat{\mathbf{w}}_{*}^{(t)} \coloneqq\frac{1}{N}\sum_{n=1}^{N}\mathbf{w}_{*,n}^{(t)}. \tag{111f}\]

For convenience of narration, we introduce the following bounded statistical error assumption.

**Assumption E.2** (Bounded statistical error).: For all \(n\in[N]\), there exists \(\varepsilon_{\text{stat}}^{n}>0\) such that for all \(t\in\mathbb{N}\) in Algorithm 3, we have

\[\mathbb{E}\left[\ell\left(\mathbf{w}_{n}^{(t)},Q_{n}^{(t)},\tilde{d}_{n}^{(t)} \right)-\ell\left(\mathbf{w}_{*,n}^{(t)},Q_{n}^{(t)},\tilde{d}_{n}^{(t)}\right) \right]\leq\varepsilon_{\text{stat}}^{n}. \tag{112}\]

When solving the regression problem with sampling based approaches, we can expect \(\varepsilon_{\text{stat}}^{n}=\mathcal{O}(1/K)\), where \(K\) is the iteration number of Algorithm 4.

**Theorem E.3** (Convergence rate of Critic (Algorithm 4)).: _For Algorithm 4, let \(\mathbf{w}_{0}=\mathbf{0}\) and \(\beta=\frac{1}{2C_{\phi}}\). Then under Assumption 4.1, we have_

\[\mathbb{E}\left[\ell\left(\mathbf{w}_{\text{out}},Q_{\xi},\tilde{d}_{\xi}\right) \right]-\ell\left(\mathbf{w}^{\star},Q_{\xi},\tilde{d}_{\xi}\right)\leq\frac{4}{K} \left(\frac{\sqrt{2p}}{1-\gamma}\left(\frac{C_{\phi}^{2}}{\mu(1-\gamma)}+1 \right)+\frac{C_{\phi}^{2}}{\mu(1-\gamma)^{2}}\right)^{2}, \tag{113}\]

_where \(\mathbf{w}^{\star}\in\arg\min_{\mathbf{w}}\ell\left(\mathbf{w},Q^{\xi},\tilde{d}_{\xi}\right)\)._

The proof of Theorem E.3 is postponed to Appendix G.5.

The following lemma provide a (very pessimistic) upper bound of \(C_{\nu}\) in Assumption 4.3.

**Lemma E.4** (Upper bound of \(C_{\nu}\)).: _If \(\nu(s,a)>0\) for all state-action pairs \((s,a)\in\mathcal{S}\times\mathcal{A}\), then we have_

\[C_{\nu}\leq\frac{1}{(1-\gamma)^{2}\nu_{\min}^{2}}.\]

Proof.: We only need to note that

\[\sqrt{\mathbb{E}_{(s.a)\sim\tilde{d}^{(t)}}\left[\left(\frac{h^{(t)}(s,a)}{ \tilde{d}_{n}^{(t)}(s,a)}\right)^{2}\right]}\leq\max_{(s,a)\in\mathcal{S} \times\mathcal{A}}\frac{h^{(t)}(s,a)}{\tilde{d}_{n}^{(t)}(s,a)}\leq\frac{1}{( 1-\gamma)\nu_{\min}}\,,\]

where the last inequality follows from (27). 

We give some key lemmas which will be used in our proof of Theorem 4.4.

**Lemma E.5** (consensus properties).: _For all \(t\in\mathbb{N}\), we have_

\[\mathbf{\bar{\xi}}^{(t+1)} =\mathbf{\bar{\xi}}^{(t)}+\alpha\hat{\mathbf{w}}^{(t)}, \tag{114}\] \[\frac{1}{N}\mathbf{1}^{\top}\mathbf{h}^{(t)} =\frac{1}{N}\sum_{n=1}^{N}\mathbf{h}_{n}^{(t)}=\hat{\mathbf{w}}^{(t)}. \tag{115}\]Proof.: See Appendix G.7. 

**Lemma E.9** (linear system).: _For any \(t\in\mathbb{N}\), we let \(\mathbf{\Omega}^{(t)}=(\Omega_{1}^{(t)},\Omega_{2}^{(t)})^{\top}\). Then for any \(\zeta>0\), we have_

\[\mathbf{\Omega}^{(t+1)}\leq\mathbf{C}\mathbf{\Omega}^{(t)}+\mathbf{s}, \tag{124}\]

_where_

\[\mathbf{C}=(c_{ij})=\begin{pmatrix}(1+\zeta)\sigma^{2}&\alpha^{2}(1+1/\zeta) \sigma^{2}\\ (1+1/\zeta)\frac{96\sigma^{2}L_{Q}^{2}}{(1-\gamma)\mu}&\sigma^{2}\left(1+\zeta+ (1+1/\zeta)\frac{24L_{Q}^{2}\alpha^{2}}{(1-\gamma)\mu}\right)\end{pmatrix}, \tag{125}\]

_and_

\[\mathbf{s}=\begin{pmatrix}s_{1}\\ s_{2}\end{pmatrix}=\begin{pmatrix}0\\ (1+1/\zeta)\frac{6\sigma^{2}}{(1-\gamma)\mu}\left(N(\bar{\varepsilon}_{\text{ start}}+C_{\nu}\bar{\varepsilon}_{\text{approx}})+4L_{Q}^{2}\left(\frac{\alpha^{2} N\bar{\varepsilon}_{\text{approx}}}{(1-\gamma)\mu}+\frac{\alpha^{2}NC_{Q}^{2}}{\mu^{2}(1- \gamma)^{2}}\right)\right)\end{pmatrix}. \tag{126}\]Proof.: See Appendix G.8. 

Now we are ready to give the formal version of Theorem 4.4 and its proof.

**Theorem E.10** (Convergence rate of FedNAC (formal)).: _Let \(\mathbf{\xi}_{1}^{(0)}=\cdots=\mathbf{\xi}_{N}^{(0)}\) in FedNAC (Algorithm 3), let the \(\mathbf{w}^{(0)}=\mathbf{0}\) and the critic stepsize \(\beta=\frac{1}{2C_{\phi}}\) in Algorithm 4. Then under Assumptions 3.1, 4.1, 4.2 and 4.3, when the actor stepsize satisfies_

\[\alpha\leq\alpha_{1}\coloneqq\frac{(1-\sigma^{2})^{3}\sqrt{(1-\gamma)\mu}}{768 \sqrt{6}\sigma L_{Q}}\,, \tag{127}\]

_where \(L_{Q}\) is defined in Lemma E.7, we have_

\[V^{\star}(\rho)-\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\bar{ V}^{(t)}(\rho)\right]\] \[\leq\frac{D_{\star}^{(0)}+\alpha\vartheta_{\rho}}{T(1-\gamma) \alpha}+\frac{1}{T}\cdot\frac{512\sqrt{6}C_{\phi}\sqrt{C_{\nu}}(\vartheta_{ \rho}+1)\sigma\alpha}{(1-\sigma^{2})^{3/2}(1-\gamma)^{3}\sqrt{N}}\sqrt{\Omega _{2}^{(0)}}\] \[\quad+\left[\frac{2\sqrt{C_{\nu}}(\vartheta_{\rho}+1)}{1-\gamma}+ \sqrt{1+\frac{64C_{\phi}^{2}\alpha^{2}}{(1-\gamma)^{5}\mu}}\cdot\frac{3072 \sqrt{3}C_{\phi}\sqrt{C_{\nu}}(\vartheta_{\rho}+1)\sigma^{2}\alpha}{(1-\sigma^ {2})^{3}(1-\gamma)^{7/2}\sqrt{\mu}}\right]\] \[\quad\quad\cdot\frac{2}{(1-\gamma)^{2}\sqrt{K}}\left((\sqrt{2p}+1 )C_{\phi}^{2}+\sqrt{2p}\mu(1-\gamma)\right)\] \[\quad+\left[\frac{2\sqrt{2C_{\nu}}(\vartheta_{\rho}+1)}{1-\gamma}+ \frac{3072\sqrt{3}C_{\phi}C_{\nu}(\vartheta_{\rho}+1)\sigma^{2}\alpha}{(1- \sigma^{2})^{3}(1-\gamma)^{7/2}\sqrt{\mu}}\right]\sqrt{\varepsilon_{\text{ approx}}}+\frac{6144\sqrt{2}\sigma^{2}C_{\nu}(\vartheta_{\rho}+1)C_{\phi}^{3}\alpha^{2}}{(1- \gamma)^{13/2}\mu^{3/2}(1-\sigma^{2})^{3}}. \tag{128}\]

_Moreover, the consensus errors could be upper bounded by_

\[\mathbb{E}\left\|\mathbf{\xi}^{(t)}-\mathbf{1}_{N}\mathbf{\bar{\xi}}^{(t)\top}\right\|_{ \text{F}}^{2}\leq\left(\frac{49}{64}\sigma^{2}+\frac{15}{64}\right)^{t}\mathbb{ E}\left\|\mathbf{h}^{(0)}-\mathbf{1}_{N}\hat{\mathbf{w}}^{(0)\top}\right\|_{\text{F}}^{2}+\frac{64 \delta(\alpha,K)}{15(1-\sigma^{2})}\,, \tag{129}\]

_where_

\[\delta(\alpha,K)\coloneqq\frac{18\sigma^{2}N}{(1-\sigma^{2})(1-\gamma)\mu} \left(\varepsilon_{\text{start}}+C_{\nu}\varepsilon_{\text{approx}}\right)+ \frac{72\sigma^{2}L_{Q}^{2}N}{(1-\gamma)^{3}\mu^{3}(1-\sigma^{2})}\left((1- \gamma)\mu\varepsilon_{\text{start}}+C_{\phi}^{2}\right)\alpha^{2}\,, \tag{130}\]

_and_

\[\bar{\varepsilon}_{\text{stat}}\leq\frac{4}{(1-\gamma)^{4}K}\left((\sqrt{2p} +1)C_{\phi}^{2}+\sqrt{2p}\mu(1-\gamma)\right)^{2}\,.\]

_Remark E.11_ (Sample and communication complexity).: When \(\sigma>0\) and

\[\alpha=\frac{\sqrt{\mu}(D_{\star}^{(0)})^{1/3}}{6144^{1/3}2^{1/6}C_{\nu}^{1/3} (1+\vartheta_{\rho})^{1/3}C_{\phi}}\cdot\frac{(1-\gamma)^{11/6}(1-\sigma^{2} )}{T^{1/3}\sigma^{2/3}},\]

it follows from Theorem E.10 that

\[V^{\star}(\rho)-\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\bar{ V}^{(t)}(\rho)\right]\] \[\leq\frac{3^{1/3}\cdot 2^{29/6}(D_{\star}^{(0)})^{2/3}C_{\nu}^{1/3}(1+ \vartheta_{\rho})^{1/3}C_{\phi}\sigma^{2/3}}{T^{2/3}(1-\gamma)^{17/6}(1- \sigma^{2})\sqrt{\mu}}+\frac{\vartheta_{\rho}}{(1-\gamma)T}+\frac{2^{17/3}3^ {1/6}C_{\nu}^{1/6}(1+\vartheta_{\rho})^{2/3}\sigma^{1/3}\sqrt{\mu}(D_{\star}^{ (0)})^{1/3}}{T^{4/3}(1-\sigma^{2})^{1/2}(1-\gamma)^{7/6}\sqrt{N}}\] \[\quad+\left[\frac{2\sqrt{C_{\nu}}(\vartheta_{\rho}+1)}{1-\gamma}+ \sqrt{1+\frac{(D_{\star}^{(0)})^{2/3}(1-\sigma^{2})^{2}}{3^{3/2}\cdot 4C_{\nu}^{2/3}(1- \gamma)^{4/3}(1+\vartheta_{\rho})^{1/3}T^{2/3}\sigma^{4/3}}}\cdot\frac{2^{37/6} \cdot 3^{7/6}C_{\nu}^{1/6}(\vartheta_{\rho}+1)^{2/3}\sigma^{4/3}(D_{\star}^{ (0)})^{1/3}}{(1-\sigma^{2})^{2}(1-\gamma)^{5/3}T^{1/3}}\right]\] \[\quad\cdot\frac{2}{(1-\gamma)^{2}\sqrt{K}}\left((\sqrt{2p}+1)C_{ \phi}^{2}+\sqrt{2p}\mu(1-\gamma)\right)\] \[+\left[\frac{2\sqrt{2C_{\nu}}(\vartheta_{\rho}+1)}{1-\gamma}+ \frac{2^{37/6}\cdot 3^{7/6}C_{\nu}^{1/6}(\vartheta_{\rho}+1)^{2/3}\sigma^{4/3}(D_{\star}^{ (0)})^{1/3}}{(1-\sigma^{2})^{2}(1-\gamma)^{5/3}T^{1/3}}\right]\sqrt{\varepsilon_ {\text{approx}}}\,. \tag{131}\]Consequently, we need

\[T\gtrsim\left\{\frac{\sigma}{\varepsilon^{3/2}(1-\gamma)^{17/4}(1-\sigma^{2})^{3/2} },\frac{1}{\varepsilon(1-\gamma)},\frac{\sigma^{1/4}}{\varepsilon^{3/4}(1- \sigma^{2})^{3/8}(1-\gamma)^{7/8}N^{3/8}},\frac{\sigma^{4}}{(1-\gamma)^{2}(1- \gamma^{2})^{6}}\right\}\]

and

\[K=\mathcal{O}\left(\frac{1}{(1-\gamma)^{6}\varepsilon^{2}}\right)\]

such that \(V^{*}(\rho)-\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\bar{V}^{(t)}(\rho) \right]\lesssim\varepsilon+\frac{\bar{\varepsilon}_{\text{approx}}}{1-\gamma}\). In Algorithm 5, each trajectory has the expected length \(1/(1-\gamma)\). Consider only the term where \(\varepsilon\) dominates, FedNAC requires \(\mathcal{O}\left(\frac{1}{(1-\gamma)^{45/4}\varepsilon^{7/2}(1-\sigma^{2})^{3 /2}}\right)\) samples for each agent and \(\mathcal{O}\left(\frac{1}{\varepsilon^{3/2}(1-\gamma)^{17/4}(1-\sigma^{2})^{3 /2}}\right)\) rounds of communication.

On the other end, when \(\sigma=0\), (128) becomes:

\[V^{\star}(\rho)-\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\bar{ V}^{(t)}(\rho)\right] \leq\frac{D_{\star}^{(0)}+\alpha\vartheta_{\rho}}{T(1-\gamma)\alpha }+\frac{4\sqrt{C_{\nu}}(\vartheta_{\rho}+1)}{(1-\gamma)^{3}\sqrt{K}}\left(( \sqrt{2p}+1)C_{\phi}^{2}+\sqrt{2p}\mu(1-\gamma)\right)\] \[\quad+\frac{2\sqrt{2C_{\nu}}(\vartheta_{\rho}+1)}{1-\gamma}\sqrt{ \bar{\varepsilon}_{\text{approx}}}, \tag{132}\]

Consequently, for any fixed \(\alpha>0\), when \(\sigma=0\) or close to \(0\), with \(T=\mathcal{O}\left(\frac{1}{(1-\gamma)\varepsilon}\right)\) and \(K=\mathcal{O}\left(\frac{1}{(1-\gamma)^{6}\varepsilon^{2}}\right)\), FedNAC requires \(KT/(1-\gamma)=\mathcal{O}\left(\frac{1}{(1-\gamma)^{6}\varepsilon^{2}}\right)\) samples for each agent and \(T=\mathcal{O}\left(\frac{1}{(1-\gamma)\varepsilon}\right)\) rounds of communication such that \(V^{\star}(\rho)-\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\bar{V}^{(t)}(\rho) \right]\lesssim\varepsilon+\frac{\bar{\varepsilon}_{\text{approx}}}{1-\gamma}\).

### Proof of Theorem e.10

We suppose Assumptions 3.1, E.2, 4.1, 4.2 and 4.3 holds. By Lemma E.9 and nonnegativity of each entry of \(\mathbf{C}\), \(\mathbf{s}\) and \(\mathbf{\Omega}^{(t)}\) where \(t\in\mathbb{N}\), it's easy to see that

\[\sqrt{\mathbf{\Omega}^{(t+1)}}\leq\sqrt{\mathbf{C}}\sqrt{\mathbf{\Omega}^{(t)}}+\sqrt{\mathbf{ s}}, \tag{133}\]

where \(\sqrt{\cdot}\) is exerted element-wise.

In addition, taking expectation on both sides of (123) and using the act that

\[\mathbb{E}\left[\sqrt{2\left(\bar{\varepsilon}_{\text{approx}}+\frac{L_{Q}^{2} }{N}\left\|\mathbf{\xi}^{(t)}-\mathbf{1}_{N}\bar{\mathbf{\xi}}^{(t)\top}\right\|_{\text{F }}^{2}\right)}\right]\leq\sqrt{2\bar{\varepsilon}_{\text{approx}}}+\sqrt{\frac {2L_{Q}^{2}}{N}\Omega_{1}^{(t)}},\]

we have

\[\vartheta_{\rho}\mathbb{E}[\delta^{(t+1)}]+\frac{\mathbb{E}[D_{ \star}^{(t+1)}]}{(1-\gamma)\alpha} \leq\vartheta_{\rho}\mathbb{E}[\delta^{(t)}]+\frac{\mathbb{E}[D_{ \star}^{(t)}]}{(1-\gamma)\alpha}-\mathbb{E}[\delta^{(t)}]\] \[+\frac{2\sqrt{C_{\nu}}(\vartheta_{\rho}+1)}{1-\gamma}\left(\sqrt{ \bar{\varepsilon}_{\text{stat}}}+\sqrt{2\bar{\varepsilon}_{\text{approx}}}+ \sqrt{\frac{2L_{Q}^{2}}{N}\Omega_{1}^{(t)}}\right). \tag{134}\]

We define the Lyapunov function \(\Phi^{(t)}\) as follows:

\[\Phi^{(t)}\coloneqq\vartheta_{\rho}\mathbb{E}[\delta^{(t)}]+\frac{\mathbb{E}[D _{\star}^{(t)}]}{(1-\gamma)\alpha}+\mathbf{q}^{\top}\sqrt{\mathbf{\Omega}^{(t)}}, \tag{135}\]

where

\[\mathbf{q}=\begin{pmatrix}q_{1}\\ q_{2}\end{pmatrix}=\begin{pmatrix}\frac{2L_{Q}\sqrt{2C_{\nu}}(\vartheta_{\rho}+1 )}{(1-\gamma)\sqrt{N}}\cdot\frac{1}{1-\sqrt{1+\zeta}\sigma-\sqrt{(1+1/\zeta)c _{21}\sigma}\alpha/(1-\sqrt{\bar{\varepsilon}_{22}})}{\frac{2L_{Q}\sqrt{2C_{ \nu}}(\vartheta_{\rho}+1)}{(1-\gamma)\sqrt{N}}\cdot\frac{\sqrt{1+1/\zeta} \sigma\alpha}{(1-\sqrt{1+\zeta}\sigma)(1-\sqrt{\bar{\varepsilon}_{22}})- \sqrt{(1+1/\zeta)c_{21}\sigma}\alpha}}\\ \end{pmatrix}. \tag{136}\]It's straightforward to verify that when \(\zeta=\frac{1-\sigma^{2}}{2}\), we have the entries in \(\mathbf{C}\) (cf. (125)) satisfies

\[c_{11} <\frac{1+\sigma^{2}}{2}, \tag{137}\] \[c_{12} \leq\frac{3\sigma^{2}\alpha^{2}}{1-\sigma^{2}}. \tag{138}\]

Moreover, from \(\alpha\leq\frac{\sqrt{(1-\gamma)\mu}(1-\sigma^{2})}{12\sqrt{2}\sigma L_{Q}}\) we deduce

\[c_{22}\leq\frac{3+\sigma^{2}}{4}, \tag{139}\]

which gives

\[1-\sqrt{c_{22}}\geq 1-\sqrt{\frac{3+\sigma^{2}}{4}}\geq\frac{1-\sigma^{2}}{8}, \tag{140}\]

Also note that \(\alpha\leq\frac{(1-\sigma^{2})^{3}\sqrt{(1-\gamma)\mu}}{768\sqrt{6}\sigma^{2}L _{Q}}\) yields

\[\sqrt{(1+1/\zeta)c_{21}}\sigma\alpha\leq\frac{(1-\sqrt{1+\zeta}\sigma)(1-\sqrt {c_{22}})}{2}.\]

which together with (140) and the fact \(1-\sqrt{1+\zeta}\sigma\geq\frac{1-\sigma^{2}}{4}\) indicates \(q_{1},q_{2}>0\) and that

\[q_{1} \leq\frac{16\sqrt{2}L_{Q}\sqrt{C_{\nu}}(\vartheta_{\rho}+1)}{(1- \sigma^{2})(1-\gamma)\sqrt{N}}, \tag{141}\] \[q_{2} \leq\frac{128\sqrt{6}L_{Q}\sqrt{C_{\nu}}(\vartheta_{\rho}+1)\sigma \alpha}{(1-\sigma^{2})^{5/2}(1-\gamma)\sqrt{N}}. \tag{142}\]

Thus by (133) and (134) we have

\[\Phi^{(t+1)} =\vartheta_{\rho}\mathbb{E}[\delta^{(t+1)}]+\frac{\mathbb{E}[D_{ *}^{(t+1)}]}{(1-\gamma)\alpha}+\mathbf{q}^{\top}\sqrt{\mathbf{\Omega}^{(t+1)}}\] \[\leq\vartheta_{\rho}\mathbb{E}[\delta^{(t)}]+\frac{\mathbb{E}[D_{ *}^{(t)}]}{(1-\gamma)\alpha}-\mathbb{E}[\delta^{(t)}]+\mathbf{q}^{\top}\left(\sqrt{ \mathbf{C}}\sqrt{\mathbf{\Omega}^{(t)}}+\sqrt{\mathbf{s}}\right)\] \[\qquad+\frac{2\sqrt{C_{\nu}}(\vartheta_{\rho}+1)}{1-\gamma}\left( \sqrt{\varepsilon_{\text{stat}}}+\sqrt{2\varepsilon_{\text{approx}}}+\sqrt{ \frac{2L_{Q}^{2}}{N}}\Omega_{1}^{(t)}\right)\] \[=\Phi^{(t)}+\underbrace{\left(\mathbf{q}^{\top}(\sqrt{\mathbf{C}}-\mathbf{I })+\left(\frac{2L_{Q}\sqrt{2C_{\nu}}(\vartheta_{\rho}+1)}{(1-\gamma)\sqrt{N}}, 0\right)\right)}_{=(0,0)}\sqrt{\mathbf{\Omega}^{(t)}}\] \[\qquad+\frac{2\sqrt{C_{\nu}}(\vartheta_{\rho}+1)}{1-\gamma}\left( \sqrt{\varepsilon_{\text{stat}}}+\sqrt{2\varepsilon_{\text{approx}}}\right)+ q_{2}\sqrt{s_{2}}-\mathbb{E}[\delta^{(t)}], \tag{143}\]

which gives

\[\mathbb{E}[\delta^{(t)}]\leq\Phi^{(t)}-\Phi^{(t+1)}+\frac{2\sqrt{C_{\nu}}( \vartheta_{\rho}+1)}{1-\gamma}\left(\sqrt{\varepsilon_{\text{stat}}}+\sqrt{2 \varepsilon_{\text{approx}}}\right)+q_{2}\sqrt{s_{2}}. \tag{144}\]

Summing the above inequality over \(t=0,1,\cdots,T-1\) and divide both sides by \(T\), we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\delta^{(t)}]\leq\frac{\Phi^{(0)}-\Phi^{ (t)}}{T}+\frac{2\sqrt{C_{\nu}}(\vartheta_{\rho}+1)}{1-\gamma}\left(\sqrt{ \varepsilon_{\text{stat}}}+\sqrt{2\varepsilon_{\text{approx}}}\right)+q_{2} \sqrt{s_{2}}. \tag{145}\]

Since

\[s_{2}\leq\frac{18\sigma^{2}N}{(1-\sigma^{2})(1-\gamma)\mu}\left(\bar{ \varepsilon}_{\text{stat}}+C_{\nu}\bar{\varepsilon}_{\text{approx}}\right)+ \frac{72\sigma^{2}L_{Q}^{2}N}{(1-\gamma)^{3}\mu^{3}(1-\sigma^{2})}\left((1- \gamma)\mu\bar{\varepsilon}_{\text{stat}}+C_{\phi}^{2}\right)\alpha^{2}, \tag{146}\]

[MISSING_PAGE_FAIL:34]

### Proof of Theorem e.3

The proof of Theorem E.3 could be found in Appendix C.5 in [33]. We present it for completeness. To prove Theorem E.3, we need the following Theorem G.2.

**Theorem E.12** (Theorem 1 in [19]).: _Consider the following assumptions:_

1. _The observations_ \((\mathbf{a}_{k},\mathbf{b}_{k})\in\mathbb{R}^{p}\times\mathbb{R}^{p}\) _are independent and identically distributed._
2. \(\mathbb{E}\left[\left\|\mathbf{a}_{k}\right\|^{2}\right]^{\gamma}\) _and_ \(\mathbb{E}\left[\left\|\mathbf{b}_{k}\right\|^{2}\right]\) _are finite. The covariance_ \(\mathbb{E}\left[\mathbf{a}_{k}\mathbf{a}_{k}^{\top}\right]\) _is invertible._
3. _The global minimum of_ \(g(w)=\frac{1}{2}\mathbb{E}\left[(\mathbf{w},\mathbf{a}_{k})^{2}-2\langle\mathbf{w},\mathbf{b}_{ k}\rangle\right]\) _is attained at a certain_ \(\mathbf{w}^{\star}\in\mathbb{R}^{p}\)_. Let_ \(\Delta_{k}=\mathbf{b}_{k}-\langle\mathbf{w}^{\star},\mathbf{a}_{k}\rangle\mathbf{a}_{k}\) _denote the residual. We have_ \(\mathbb{E}[\Delta_{k}]=0\)_._
4. \(\exists R>0\) _and_ \(\sigma>0\) _such that_ \(\mathbb{E}\left[\Delta_{k}\Delta_{k}^{\top}\right]\leq\sigma^{2}\mathbb{E} \left[\mathbf{a}_{k}\mathbf{a}_{k}^{\top}\right]\) _and_ \(\mathbb{E}\left[\left\|\mathbf{a}_{k}\right\|^{2}\mathbf{a}_{k}\mathbf{a}_{k}^{\top}\right] \leq R^{2}\mathbb{E}\left[\mathbf{a}_{k}\mathbf{a}_{k}^{\top}\right]\)_._

_Consider the stochastic gradient recursion_

\[w_{k+1}=w_{k}-\eta\left(\langle w_{k},a_{k}\rangle a_{k}-b_{k}\right)\]

_started from \(w_{0}\in\mathbb{R}^{p}\). Let \(w_{\text{out}}=\frac{1}{K}\sum_{k=1}^{K}w_{k}\). When \(\eta=\frac{1}{4R}\)4, we have_

Footnote 4: Here \(\left\|\cdot\right\|\) could be any norm in \(\mathbb{R}^{p}\).

\[\mathbb{E}\left[g(w_{\text{out}})-g(w^{\star})\right]\leq\frac{2}{K}(\sigma \sqrt{p}+R\left\|w_{0}-w^{\star}\right\|)^{2}. \tag{152}\]

In the proof of Theorem E.3 we'll show that for Algorithm 4, the assumptions in Theorem G.2 are all satisfied and thus we can use the result (267).

Proof of Theorem e.3.: We let \(a_{k}\) and \(b_{k}\) in Theorem G.2 be \(\phi(s,a)\) and \(\widehat{Q}_{\xi}\phi(s,a)\) in Algorithm 4, respectively. And we let \(\left\|\cdot\right\|=\left\|\cdot\right\|_{2}\) in Theorem G.2. Since the observations \(\left(\phi(s,a),\widehat{Q}_{\xi}(s,a)\phi(s,a)\right)\in\mathbb{R}^{p}\times \mathbb{R}^{p}\) are i.i.d., (i) is satisfied.

As we assume \(\left\|\phi(s,a)\right\|_{2}\leq C_{\phi}\), \(\mathbb{E}\left[\left\|\phi(s,a)\right\|_{2}^{2}\right]\) is finite. From Assumption 4.1 we know that \(\mathbb{E}\left[\phi(s,a)\phi(s,a)^{\top}\right]\) is invertible.

Let \(H\) be the length of trajectory for estimating \(\widehat{Q}_{\xi}(s,a)\). Then \(\left(\widehat{Q}_{\xi}(s,a)\right)^{2}\) is bounded by

\[\mathbb{E}\left[\left(\widehat{Q}_{\xi}(s,a)\right)^{2}\right] =\mathbb{E}_{(s,a)\sim d_{\nu}^{\pi_{\xi}}}\left[\sum_{\tau=0}^{ \infty}Pr(H=\tau)\mathbb{E}\left[\left(\sum_{t=0}^{\tau}r(s_{t},a_{t})\right) ^{2}\left|H=\tau,s_{0}=s,a_{0}=a\right]\right]\] \[=\mathbb{E}_{(s,a)\sim d_{\nu}^{\pi_{\xi}}}\left[(1-\gamma)\sum_{ \tau=0}^{\infty}\gamma^{\tau}\mathbb{E}\left[\left(\sum_{t=0}^{\tau}r(s_{t},a_ {t})\right)^{2}\left|H=\tau,s_{0}=s,a_{0}=a\right]\right]\right]\] \[\leq\mathbb{E}_{(s,a)\sim d_{\nu}^{\pi_{\xi}}}\left[(1-\gamma) \sum_{\tau=0}^{\infty}\gamma^{\tau}(\tau+1)^{2}\right]\leq\frac{2}{(1-\gamma) ^{2}}\,, \tag{153}\]

from which we deduce \(\mathbb{E}\left[\left\|\widehat{Q}_{\xi}(s,a)\phi(s,a)\right\|_{2}^{2}\right] \leq C_{\phi}^{2}\mathbb{E}\left[\widehat{Q}_{\xi}(s,a)^{2}\right]\) is bounded. Thus (ii) holds.

Furthermore, we introduce the residual

\[\Delta\coloneqq\left(\widehat{Q}_{\xi}(s,a)-\phi(s,a)^{\top}w^{\star}\right) \phi(s,a)\,, \tag{154}\]

then from Lemma 7 in [33] we know that \(\mathbb{E}[\Delta]=\frac{1}{2}\nabla_{w}\ell(w,\widehat{Q}_{\xi},d_{\nu}^{\pi_{ \xi}})=0\), which gives (iii).

To verify (iv), we let \(R=C_{\phi}\) in Theorem G.2, then \(\mathbb{E}\left[\|\phi(s,a)\|_{2}^{2}\phi(s,a)\phi(s,a)^{\top}\right]\leq C_{\phi} ^{2}\mathbb{E}\left[\phi(s,a)\phi(s,a)^{\top}\right]\). Also note that

\[w^{\star} =\left(\mathbb{E}_{(s,a)\sim\mathcal{J}_{\epsilon}^{\pi_{\epsilon }}}\left[\phi(s,a)\phi(s,a)^{\top}\right]\right)^{\dagger}\mathbb{E}_{(s,a)\sim \mathcal{J}_{\epsilon}^{\pi_{\epsilon}}}\left[\widehat{Q}_{\xi}(s,a)\phi(s,a)\right]\] \[\leq\frac{1}{1-\gamma}\left(\mathbb{E}_{(s,a)\sim\nu}\left[\phi(s,a )\phi(s,a)^{\top}\right]\right)^{\dagger}\mathbb{E}_{(s,a)\sim\mathcal{J}_{ \epsilon}^{\pi_{\epsilon}}}\left[\widehat{Q}_{\xi}(s,a)\phi(s,a)\right]\,, \tag{155}\]

from which we deduce

\[\left\|w^{\star}\right\|_{2}\leq\frac{B}{\mu(1-\gamma)^{2}}\,. \tag{156}\]

\[\mathbb{E}\left[\left(\widehat{Q}_{\xi}(s,a)-\phi(s,a)^{\top}w^{ \star}\right)^{2}|s,a\right] =\mathbb{E}\left[\left(\widehat{Q}_{\xi}(s,a)\right)^{2}|s,a \right]-2Q_{\xi}(s,a)\phi(s,a)^{\top}w^{\star}+(\phi(s,a)^{\top}w^{\star})^{2} \tag{157}\] \[\leq\frac{2}{(1-\gamma)^{2}}+\frac{2C_{\phi}^{2}}{\mu(1-\gamma)^{ 3}}+\frac{C_{\phi}^{4}}{\mu^{2}(1-\gamma)^{4}}\] \[\leq\frac{2}{(1-\gamma)^{2}}\left(\frac{C_{\phi}^{2}}{\mu(1-\gamma )}+1\right)^{2}\,. \tag{158}\]

The above expression implies

\[\mathbb{E}\left[\Delta\Delta^{\top}\right] =\mathbb{E}_{(s,a)\sim\mathcal{J}_{\epsilon}^{\pi_{\epsilon}}} \left[\left(\widehat{Q}_{\xi}(s,a)-\phi(s,a)^{\top}w^{\star}\right)^{2}\phi(s, a)\phi(s,a)^{\top}\big{|}s,a\right]\] \[=\mathbb{E}_{(s,a)\sim\mathcal{J}_{\epsilon}^{\pi_{\epsilon}}} \left[\mathbb{E}\left[\left(\widehat{Q}_{\xi}(s,a)-\phi(s,a)^{\top}w^{\star} \right)^{2}\big{|}s,a\right]\phi(s,a)\phi(s,a)^{\top}\right]\] \[\leq\left(\underbrace{\frac{\sqrt{2}}{1-\gamma}\left(\frac{C_{ \phi}^{2}}{\mu(1-\gamma)}+1\right)}_{\sigma}\right)\mathbb{E}[\phi(s,a)\phi(s,a )^{\top}]\,. \tag{159}\]

Therefore, (iv) is verified.

Thus by (267), with stepsize \(\beta=\frac{1}{2C_{\phi}^{2}}\), initialization \(w_{0}=0\) and \(K\) steps of critic updates, we have

\[\mathbb{E}\left[\ell\left(w_{\text{out}},\widehat{Q}_{\xi},\tilde {d}_{\xi}\right)\right]-\ell\left(w^{\star},\widehat{Q}_{\xi},\tilde{d}_{\xi}\right) \leq\frac{4}{K}\left(\sigma\sqrt{p}+C_{\phi}\left\|w^{\star} \right\|_{2}\right)^{2}\] \[\leq\frac{4}{K}\left(\frac{\sqrt{2p}}{1-\gamma}\left(\frac{C_{ \phi}^{2}}{\mu(1-\gamma)}+1\right)+\frac{C_{\phi}^{2}}{\mu(1-\gamma)^{2}} \right)^{2}\,,\]

which gives (113). 

## Appendix F Proof of key lemmas

### Proof of Lemma d.2

Before proceeding, we summarize several useful properties of the auxiliary sequences (cf. (40) and (41)), whose proof is postponed to Appendix G.1.

**Lemma F.1** (Properties of auxiliary sequences \(\{\overline{\xi}^{(t)}\}\) and \(\{\boldsymbol{\xi}^{(t)}\}\)).: \(\{\overline{\xi}^{(t)}\}\) _and \(\{\boldsymbol{\xi}^{(t)}\}\) have the following properties:_

1. \(\boldsymbol{\xi}^{(t)}\) _can be viewed as an unnormalized version of_ \(\boldsymbol{\pi}^{(t)}\)_, i.e.,_ \[\pi_{n}^{(t)}(\cdot|s)=\frac{\xi_{n}^{(t)}(s,\cdot)}{\left\|\xi_{n}^{(t)}(s, \cdot)\right\|_{1}}\,,\ \forall n\in[N],\,s\in\mathcal{S}\,.\] (160)2. _For any_ \(t\geq 0\)_,_ \(\log\overline{\xi}^{(t)}\) _keeps track of the average of_ \(\log\mathbf{\xi}^{(t)}\)_, i.e.,_ \[\frac{1}{N}\mathbf{1}_{N}^{\top}\log\mathbf{\xi}^{(t)}=\log\overline{\xi}^{(t)}\,.\] (161) _It follows that_ \[\forall s\in\mathcal{S},\;t\geq 0:\quad\overline{\pi}^{(t)}(\cdot|s)=\frac{ \overline{\xi}^{(t)}(s,\cdot)}{\left\|\overline{\xi}^{(t)}(s,\cdot)\right\|_{1}}.\] (162)

**Lemma F.2** ([17, Appendix. A.2]).: _For any vector \(\theta=[\theta_{a}]_{a\in\mathcal{A}}\in\mathbb{R}^{|\mathcal{A}|}\), we denote by \(\pi_{\theta}\in\mathbb{R}^{|\mathcal{A}|}\) the softmax transform of \(\theta\) such that_

\[\pi_{\theta}(a)=\frac{\exp(\theta_{a})}{\sum_{a^{\prime}\in \mathcal{A}}\exp(\theta_{a^{\prime}})}\,,\quad a\in\mathcal{A}\,. \tag{163}\]

_For any \(\theta_{1},\theta_{2}\in\mathbb{R}^{|\mathcal{A}|}\), we have_

\[\big{|}\log(\left\|\exp(\theta_{1})\right\|_{1})-\log(\left\|\exp (\theta_{2})\right\|_{1})\big{|}\leq\left\|\theta_{1}-\theta_{2}\right\|_{ \infty}\,, \tag{164}\] \[\left\|\log\pi_{\theta_{1}}-\log\pi_{\theta_{2}}\right\|_{\infty} \leq 2\left\|\theta_{1}-\theta_{2}\right\|_{\infty}\,. \tag{165}\]

**Step 1: bound**\(u^{(t+1)}(s,a)=\big{\|}\log\mathbf{\xi}^{(t+1)}(s,a)-\log\overline{\xi}^{(t+1)}(s,a )\mathbf{1}_{N}\big{\|}_{2}\)**.** By (40b) and (41b) we have

\[u^{(t+1)}(s,a) =\big{\|}\log\mathbf{\xi}^{(t+1)}(s,a)-\log\overline{\xi}^{(t+1)}(s,a )\mathbf{1}_{N}\big{\|}_{2}\] \[=\big{\|}\alpha\Big{(}\mathbf{W}\log\mathbf{\xi}^{(t)}(s,a)-\log\overline{ \xi}^{(t)}(s,a)\mathbf{1}_{N}\Big{)}+(1-\alpha)\Big{(}\mathbf{W}\mathbf{T}^{(t)}(s,a)- \widehat{Q}_{\tau}^{(t)}(s,a)\mathbf{1}_{N}\Big{)}/\tau\big{\|}_{2}\] \[\leq\sigma\alpha\big{\|}\log\mathbf{\xi}^{(t)}(s,a)-\log\overline{\xi }^{(t)}(s,a)\mathbf{1}_{N}\big{\|}_{2}+\frac{1-\alpha}{\tau}\sigma\big{\|}\mathbf{T}^ {(t)}(s,a)-\widehat{Q}_{\tau}^{(t)}(s,a)\mathbf{1}_{N}\big{\|}_{2}\] \[\leq\sigma\alpha\big{\|}u^{(t)}\big{\|}_{\infty}+\frac{1-\alpha }{\tau}\sigma\big{\|}v^{(t)}\big{\|}_{\infty}, \tag{166}\]

where the penultimate step results from the averaging property of \(\mathbf{W}\) (property (11)). Taking maximum over \((s,a)\in\mathcal{S}\times\mathcal{A}\) establishes the bound on \(\Omega_{1}^{(t+1)}\) in (49).

**Step 2: bound**\(v^{(t+1)}(s,a)=\big{\|}\mathbf{T}^{(t+1)}(s,a)-\widehat{Q}_{\tau}^{(t+1)}(s,a)\bm {1}_{N}\big{\|}_{2}\)**.** By (\(U_{T}\)) we have

\[\big{\|}\mathbf{T}^{(t+1)}(s,a)-\widehat{Q}_{\tau}^{(t+1)}(s,a)\mathbf{1} _{N}\big{\|}_{2}\] \[=\Big{\|}\Big{(}\mathbf{W}\mathbf{T}^{(t)}(s,a)-\widehat{Q}_{\tau}^{(t)}(s,a)\mathbf{1}_{N}\Big{)}+\mathbf{W}\left(\mathbf{Q}_{\tau}^{(t+1)}(s,a)-\mathbf{Q}_{\tau}^{(t)} (s,a)\right)+\Big{(}\widehat{Q}_{\tau}^{(t)}(s,a)-\widehat{Q}_{\tau}^{(t+1)}(s,a)\Big{)}\,\mathbf{1}_{N}\Big{\|}_{2}\] \[\leq\sigma\big{\|}\mathbf{T}^{(t)}(s,a)-\widehat{Q}_{\tau}^{(t)}(s,a )\mathbf{1}_{N}\big{\|}_{2}+\sigma\left\|\Big{(}\mathbf{Q}_{\tau}^{(t+1)}(s,a)-\mathbf{Q}_ {\tau}^{(t)}(s,a)\Big{)}+\Big{(}\widehat{Q}_{\tau}^{(t)}(s,a)-\widehat{Q}_{ \tau}^{(t+1)}(s,a)\Big{)}\,\mathbf{1}_{N}\right\|_{2}\] \[\leq\sigma\big{\|}\mathbf{T}^{(t)}(s,a)-\widehat{Q}_{\tau}^{(t)}(s,a )\mathbf{1}_{N}\big{\|}_{2}+\sigma\big{\|}Q_{\tau}^{(t+1)}(s,a)-\mathbf{Q}_{\tau}^{(t)} (s,a)\big{\|}_{2}\,, \tag{167}\]

where the penultimate step uses property (11), and the last step is due to

\[\Big{\|}\Big{(}\mathbf{Q}_{\tau}^{(t+1)}(s,a)-\mathbf{Q}_{\tau}^{(t)}(s,a )\Big{)}+\Big{(}\widehat{Q}_{\tau}^{(t)}(s,a)-\widehat{Q}_{\tau}^{(t+1)}(s,a) \Big{)}\,\mathbf{1}_{N}\Big{\|}_{2}^{2}\] \[=\big{\|}\mathbf{Q}_{\tau}^{(t+1)}(s,a)-\mathbf{Q}_{\tau}^{(t)}(s,a)\big{\|} _{2}^{2}+N(\widehat{Q}_{\tau}^{(t)}(s,a)-\widehat{Q}_{\tau}^{(t+1)}(s,a)\big{)} ^{2}\] \[\qquad-2\sum_{n=1}^{N}\Big{(}Q_{\tau,n}^{\pi^{(t+1)}}(s,a)-Q_{ \tau,n}^{\pi^{(t)}}(s,a)\Big{)}\left(\widehat{Q}_{\tau}^{(t+1)}(s,a)-\widehat{ Q}_{\tau}^{(t)}(s,a)\right)\] \[=\big{\|}\mathbf{Q}_{\tau}^{(t+1)}(s,a)-\mathbf{Q}_{\tau}^{(t)}(s,a) \big{\|}_{2}^{2}-N(\widehat{Q}_{\tau}^{(t)}(s,a)-\widehat{Q}_{\tau}^{(t+1)}(s,a) \big{)}^{2}\] \[\leq\big{\|}\mathbf{Q}_{\tau}^{(t+1)}(s,a)-\mathbf{Q}_{\tau}^{(t)}(s,a) \big{\|}_{2}^{2}\,.\]

**Step 3: bound**\(\big{\|}Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t+1)}\big{\|}_{\infty}\)**.** We decompose the term of interest as

\[Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t+1)} =Q_{\tau}^{\star}-\tau\alpha\log\overline{\xi}^{(t)}-(1-\alpha) \widehat{Q}_{\tau}^{(t)}\] \[=\alpha(Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t)})+(1-\alpha )(Q_{\tau}^{\star}-\overline{Q}_{\tau}^{(t)})+(1-\alpha)(\overline{Q}_{\tau}^{( t)}-\widehat{Q}_{\tau}^{(t)}),\]which gives

\[\big{\|}Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t+1)}\big{\|}_{\infty}\leq\alpha \big{\|}Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t)}\big{\|}_{\infty}+(1- \alpha)\big{\|}Q_{\tau}^{\star}-\overline{Q}_{\tau}^{(t)}\big{\|}_{\infty}+(1- \alpha)\big{\|}\overline{Q}_{\tau}^{(t)}-\widehat{Q}_{\tau}^{(t)}\big{\|}_{ \infty}\,. \tag{168}\]

Note that we can upper bound \(\big{\|}\overline{Q}_{\tau}^{(t)}-\widehat{Q}_{\tau}^{(t)}\big{\|}_{\infty}\) by

\[\big{\|}\overline{Q}_{\tau}^{(t)}-\widehat{Q}_{\tau}^{(t)}\big{\|} _{\infty}= \bigg{\|}\frac{1}{N}\sum_{n=1}^{N}Q_{\tau,n}^{\pi^{(t)}_{,n}}- \frac{1}{N}\sum_{n=1}^{N}Q_{\tau,n}^{\pi^{(t)}}\bigg{\|}_{\infty}\] \[\leq\frac{1}{N}\sum_{n=1}^{N}\big{\|}Q_{\tau,n}^{\pi^{(t)}_{,n}}- Q_{\tau,n}^{\pi^{(t)}}\big{\|}_{\infty}\] \[\leq\frac{M}{N}\sum_{n=1}^{N}\big{\|}\log\xi_{n}^{(t)}-\log \overline{\xi}^{(t)}\big{\|}_{\infty}\leqslant M\big{\|}u^{(t)}\big{\|}_{ \infty}. \tag{169}\]

The last step is due to \(\big{|}\log\xi_{n}^{(t)}(s,a)-\log\overline{\xi}^{(t)}(s,a)\big{|}\leq u^{(t)} (s,a)\), while the penultimate step results from writing

\[\overline{\pi}^{(t)}(\cdot|s) =\text{softmax}\left(\log\overline{\xi}^{(t)}(s,\cdot)\right)\,,\] \[\pi^{(t)}_{n}(\cdot|s) =\text{softmax}\left(\log\xi_{n}^{(t)}(s,\cdot)\right)\,,\]

and applying the following lemma.

**Lemma F.3** (Lipschitz constant of soft Q-function).: _Assume that \(r(s,a)\in[0,1],\forall(s,a)\in\mathcal{S}\times\mathcal{A}\) and \(\tau\geq 0\). For any \(\theta\), \(\theta^{\prime}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}|}\), we have_

\[\|Q_{\tau}^{\pi_{\theta^{\prime}}}-Q_{\tau}^{\pi_{\theta}}\|_{\infty}\leq \underbrace{\frac{1+\gamma+2\tau(1-\gamma)\log|\mathcal{A}|}{(1-\gamma)^{2}} \cdot\gamma}_{=:M}\left\|\theta^{\prime}-\theta\right\|_{\infty}\,. \tag{170}\]

Plugging (169) into (168) gives

\[\big{\|}Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t+1)}\big{\|}_{\infty}\leq \alpha\big{\|}Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t)}\big{\|}_{\infty}+ (1-\alpha)\big{\|}Q_{\tau}^{\star}-\overline{Q}_{\tau}^{(t)}\big{\|}_{\infty} +(1-\alpha)M\big{\|}u^{(t)}\big{\|}_{\infty}\,. \tag{171}\]

**Step 4: bound \(\big{\|}Q_{\tau}^{(t+1)}(s,a)-Q_{\tau}^{(t)}(s,a)\big{\|}_{2}\).**

Let \(w^{(t)}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\) be defined as

\[\forall(s,a)\in\mathcal{S}\times\mathcal{A}:\quad w^{(t)}(s,a)\coloneqq\big{\|} \log\boldsymbol{\xi}^{(t+1)}(s,a)-\log\boldsymbol{\xi}^{(t)}(s,a)-(1-\alpha)V _{\tau}^{\star}(s)\boldsymbol{1}_{N}/\tau\big{\|}_{2}\,. \tag{172}\]

Again, we treat \(w^{(t)}\) as vectors in \(\mathbb{R}^{|\mathcal{S}||\mathcal{A}|}\) whenever it is clear from context. For any \((s,a)\in\mathcal{S}\times\mathcal{A}\) and \(n\in[N]\), by Lemma F.3 it follows that

\[\big{|}Q_{\tau,n}^{\pi^{(t+1)}}(s,a)-Q_{\tau,n}^{\pi^{(t)}}(s,a) \big{|} \leq M\max_{s\in\mathcal{S}}\big{\|}\log\xi_{n}^{(t+1)}(s,\cdot )-\log\xi_{n}^{(t)}(s,\cdot)-(1-\alpha)V_{\tau}^{\star}(s)\boldsymbol{1}_{| \mathcal{A}|}/\tau\big{\|}_{\infty}\] \[\leq M\max_{s\in\mathcal{S}}\max_{a\in\mathcal{A}}w^{(t)}(s,a)\leq M \big{\|}w^{(t)}\big{\|}_{\infty}\,, \tag{173}\]

and consequently

\[\big{\|}\boldsymbol{Q}_{\tau}^{(t+1)}(s,a)-\boldsymbol{Q}_{\tau}^{(t)}(s,a) \big{\|}_{2}\leq M\sqrt{N}\big{\|}w^{(t)}\big{\|}_{\infty}\,. \tag{174}\]

It boils down to control \(\big{\|}w^{(t)}\big{\|}_{\infty}\). To do so, we first note that for each \((s,a)\in\mathcal{S}\times\mathcal{A}\), we have

\[w^{(t)}(s,a)\] \[=\big{\|}\boldsymbol{W}\left(\alpha\log\boldsymbol{\xi}^{(t)}(s, a)+(1-\alpha)\boldsymbol{T}^{(t)}(s,a)/\tau\right)-\log\boldsymbol{\xi}^{(t)}(s,a)-(1- \alpha)V_{\tau}^{\star}(s)\boldsymbol{1}_{N}/\tau\big{\|}_{2}\] \[\stackrel{{(a)}}{{=}}\bigg{\|}\alpha(\boldsymbol{W }-\boldsymbol{I}_{N})\left(\log\boldsymbol{\xi}^{(t)}(s,a)-\log\overline{\xi} ^{(t)}(s,a)\boldsymbol{1}_{N}\right)+(1-\alpha)\left(\boldsymbol{WT}^{(t)}(s,a )/\tau-\log\boldsymbol{\xi}^{(t)}(s,a)-V_{\tau}^{\star}(s)\boldsymbol{1}_{N}/ \tau\right)\bigg{\|}_{2}\] \[\stackrel{{(b)}}{{\leq}}2\alpha\big{\|}\log\boldsymbol {\xi}^{(t)}(s,a)-\log\overline{\xi}^{(t)}(s,a)\boldsymbol{1}_{N}\big{\|}_{2}+ \frac{1-\alpha}{\tau}\big{\|}\boldsymbol{WT}^{(t)}(s,a)-\tau\log\boldsymbol{\xi} ^{(t)}(s,a)-V_{\tau}^{\star}(s)\boldsymbol{1}_{N}\big{\|}_{2} \tag{175}\]

[MISSING_PAGE_FAIL:39]

**Step 5: bound \(\big{\|}\overline{Q}_{\tau}^{(t+1)}-Q_{\tau}^{\star}\big{\|}_{\infty}\).** For any state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\), we observe that

\[Q_{\tau}^{\star}(s,a)-\overline{Q}_{\tau}^{(t+1)}(s,a)\] \[=\gamma\operatorname*{\mathbb{E}}_{s^{\prime}\sim P(\cdot\mid s,a) }\left[\tau\log\left(\left\|\exp\left(\frac{Q_{\tau}^{\star}(s^{\prime},\cdot) }{\tau}\right)\right\|_{1}\right)\right]-\gamma\operatorname*{\mathbb{E}}_{ \begin{subarray}{c}s^{\prime}\sim P(\cdot\mid s,a)\\ a^{\prime}\sim\pi^{(t+1)}(\cdot\mid s^{\prime})\end{subarray}}\left[\overline{Q }_{\tau}^{(t+1)}(s^{\prime},a^{\prime})-\tau\log\overline{\pi}^{(t+1)}(a^{ \prime}|s^{\prime})\right]\,, \tag{183}\]

where the first step invokes the definition of \(Q_{\tau}\) (cf. (6a)), and the second step is due to the following expression of \(V_{\tau}^{\star}\) established in [12]:

\[V_{\tau}^{\star}(s)=\tau\log\left(\left\|\exp\left(\frac{Q_{\tau}^{\star}(s, \cdot)}{\tau}\right)\right\|_{1}\right)\,. \tag{184}\]

To continue, note that by (162) and (41b) we have

\[\log\overline{\pi}^{(t+1)}(a|s) =\log\overline{\xi}^{(t+1)}(s,a)-\log\left(\left\|\overline{\xi}^{ (t+1)}(s,\cdot)\right\|_{1}\right)\] \[=\alpha\log\overline{\xi}^{(t)}(s,a)+(1-\alpha)\frac{\widehat{Q}_ {\tau}^{(t)}(s,a)}{\tau}-\log\left(\left\|\overline{\xi}^{(t+1)}(s,\cdot) \right\|_{1}\right)\,. \tag{185}\]

Plugging (185) into (183) and (181) establishes the bounds on

\[Q_{\tau}^{\star}(s,a)-\overline{Q}_{\tau}^{(t+1)}(s,a) =\gamma\operatorname*{\mathbb{E}}_{s^{\prime}\sim P(\cdot\mid s,a )}\left[\tau\log\left(\left\|\exp\left(\frac{Q_{\tau}^{\star}(s^{\prime},\cdot )}{\tau}\right)\right\|_{1}\right)-\tau\log\left(\left\|\overline{\xi}^{(t+1) }(s^{\prime},\cdot)\right\|_{1}\right)\right]\] \[\quad-\gamma\operatorname*{\mathbb{E}}_{\begin{subarray}{c}s^{ \prime}\sim P(\cdot\mid s,a),\\ a^{\prime}\sim\pi^{(t+1)}(\cdot\mid s^{\prime})\end{subarray}}\left[\overline{Q }_{\tau}^{(t+1)}(s^{\prime},a^{\prime})-\tau\underbrace{\left(\alpha\log \overline{\xi}^{(t)}(s^{\prime},a^{\prime})+(1-\alpha)\frac{\widehat{Q}_{\tau} ^{(t)}(s^{\prime},a^{\prime})}{\tau}\right)}_{=\log\overline{\xi}^{(t+1)}(s^{ \prime},a^{\prime})}\right] \tag{186}\]

for any \((s,a)\in\mathcal{S}\times\mathcal{A}\). In view of property (164), the first term on the right-hand side of (186) can be bounded by

\[\tau\log\left(\left\|\exp\left(\frac{Q_{\tau}^{\star}(s^{\prime},\cdot)}{\tau} \right)\right\|_{1}\right)-\tau\log\left(\left\|\overline{\xi}^{(t+1)}(s^{ \prime},\cdot)\right\|_{1}\right)\leq\left\|Q_{\tau}^{\star}-\tau\log\overline {\xi}^{(t+1)}\right\|_{\infty}.\]

Plugging the above expression into (186), we have

\[0\leq Q_{\tau}^{\star}(s,a)-\overline{Q}_{\tau}^{(t+1)}(s,a)\leq\gamma\big{\|} Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t+1)}\big{\|}_{\infty}-\gamma\min_{s,a} \left(\overline{Q}_{\tau}^{(t+1)}(s,a)-\tau\log\overline{\xi}^{(t+1)}(s,a) \right)\,,\]

which gives

\[\big{\|}Q_{\tau}^{\star}-\overline{Q}_{\tau}^{(t+1)}\big{\|}_{\infty}\leq \gamma\big{\|}Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t+1)}\big{\|}_{\infty}+ \gamma\max\left\{0,-\min_{s,a}\left(\overline{Q}_{\tau}^{(t+1)}(s,a)-\tau\log \overline{\xi}^{(t+1)}(s,a)\right)\right\}. \tag{187}\]

Plugging the above inequality into (171) and (182) establishes the bounds on \(\Omega_{3}^{(t+1)}\) and \(\Omega_{2}^{(t+1)}\) in (49), respectively. **Step 6: bound \(-\min_{s,a}\left(\overline{Q}_{\tau}^{(t+1)}(s,a)-\tau\log\overline{\xi}^{(t+1 )}(s,a)\right)\).** We need the following lemma which is adapted from Lemma 1 in [13]:

**Lemma F.4** (Performance improvement of FedNPG with entropy regularization).: _Suppose \(0<\eta\leq(1-\gamma)/\tau\). For any state-action pair \((s_{0},a_{0})\in\mathcal{S}\times\mathcal{A}\), one has_

\[\overline{V}_{\tau}^{(t+1)}(s_{0})-\overline{V}_{\tau}^{(t)}(s_ {0}) \geq\frac{1}{\eta}\operatorname*{\mathbb{E}}_{\begin{subarray}{c}s \sim\mathcal{A}_{\sigma_{0}}^{(t+1)}\end{subarray}}\left[\alpha\text{KL} \big{(}\overline{\pi}^{(t+1)}(\cdot\mid s_{0})\,\|\,\overline{\pi}^{(t)}( \cdot\mid s_{0})\big{)}+\text{KL}\big{(}\overline{\pi}^{(t)}(\cdot\mid s_{0}) \,\|\,\overline{\pi}^{(t+1)}(\cdot\mid s_{0})\big{)}\right]\] \[\quad-\frac{2}{1-\gamma}\big{\|}\widehat{Q}_{\tau}^{(t)}- \overline{Q}_{\tau}^{(t)}\big{\|}_{\infty}\,, \tag{188}\] \[\overline{Q}_{\tau}^{(t+1)}(s_{0},a_{0})-\overline{Q}_{\tau}^{(t) }(s_{0},a_{0}) \geq-\frac{2\gamma}{1-\gamma}\big{\|}\widehat{Q}_{\tau}^{(t)}- \overline{Q}_{\tau}^{(t)}\big{\|}_{\infty}\,. \tag{189}\]Proof.: See Appendix G.3. 

Using (189), we have

\[\overline{Q}^{(t+1)}_{\tau}(s,a)-\tau\left(\alpha\log\overline{\xi}^{ (t)}(s,a)+(1-\alpha)\frac{\widehat{Q}^{(t)}_{\tau}(s,a)}{\tau}\right)\] \[\geq\overline{Q}^{(t)}_{\tau}(s,a)-\tau\left(\alpha\log\overline{ \xi}^{(t)}(s,a)+(1-\alpha)\frac{\widehat{Q}^{(t)}_{\tau}(s,a)}{\tau}\right)- \frac{2\gamma}{1-\gamma}\big{\|}\widehat{Q}^{(t)}_{\tau}-\overline{Q}^{(t)}_{ \tau}\big{\|}_{\infty}\] \[\geq\alpha\left(\overline{Q}^{(t)}_{\tau}(s,a)-\tau\log\overline{ \xi}^{(t)}(s,a)\right)-\frac{2\gamma+\eta\tau}{1-\gamma}\big{\|}\widehat{Q}^{(t )}_{\tau}-\overline{Q}^{(t)}_{\tau}\big{\|}_{\infty}\,, \tag{190}\]

which gives

\[-\min_{s,a}\left(\overline{Q}^{(t+1)}_{\tau}(s,a)-\tau\log\overline {\xi}^{(t+1)}(s,a)\right)\] \[\leq\alpha\max\left\{0,\min_{s,a}\left(\overline{Q}^{(t)}_{\tau}( s,a)-\tau\log\overline{\xi}^{(t)}(s,a)\right)\right\}+\frac{2\gamma+\eta\tau}{1- \gamma}M\big{\|}u^{(t)}\big{\|}_{\infty}\,. \tag{191}\]

This establishes the bounds on \(\Omega^{(t+1)}_{4}\) in (49).

### Proof of Lemma d.3

Let \(f(\lambda)\) denote the characteristic function. In view of some direct calculations, we obtain

\[f(\lambda) =(\lambda-\alpha)\bigg{\{}\underbrace{(\lambda-\sigma\alpha)( \lambda-\sigma(1+\sigma b\eta))(\lambda-(1-\alpha)\gamma-\alpha)}_{=:f_{0}( \lambda)}\] \[\quad-\frac{\eta\sigma^{2}}{1-\gamma}\underbrace{[S(\lambda-(1- \alpha)\gamma-\alpha)+\gamma cdM\eta+(1-\alpha)(2+\gamma)Mc\eta]}_{=:f_{1}( \lambda)}\bigg{\}} \tag{192}\] \[\quad-\frac{\tau\eta^{3}\gamma}{(1-\gamma)^{2}}\cdot 2cdM\sigma^{2}\,,\]

where, for the notation simplicity, we let

\[b \coloneqq\frac{M\sqrt{N}}{1-\gamma}\,, \tag{193a}\] \[c \coloneqq\frac{MN}{1-\gamma}=\sqrt{N}b\,,\] (193b) \[d \coloneqq\frac{2\gamma+\eta\tau}{1-\gamma}\,. \tag{193c}\]

Note that among all these new notation we introduce, \(S\), \(d\) are dependent of \(\eta\). To decouple the dependence, we give their upper bounds as follows

\[d_{0} \coloneqq\frac{1+\gamma}{1-\gamma}\geq d\,, \tag{194}\] \[S_{0} \coloneqq M\sqrt{N}\left(2+\sqrt{2N}+\frac{M\sqrt{N}}{\tau} \right)\geq S\,, \tag{195}\]

where (194) follows from \(\eta\leq(1-\gamma)/\tau\), and (195) uses the fact that \(\alpha\leq 1\) and \(1-\alpha\leq 1\).

Let

\[\lambda^{\star}\coloneqq\max\left\{\frac{3+\sigma}{4},\frac{1+(1-\alpha) \gamma+\alpha}{2}\right\}. \tag{196}\]

Since \(\mathbf{A}(\rho)\) is a nonnegative matrix, by Perron-Frobenius Theorem (see [12], Theorem 8.3.1), \(\rho(\eta)\) is an eigenvalue of \(\mathbf{A}(\rho)\). So to verify (55), it suffices to show that \(f(\lambda)>0\) for any \(\lambda\in[\lambda^{\star},\infty)\). To do so, in the following we first show that \(f(\lambda^{\star})>0\), and then we prove that \(f\) is non-decreasing on \([\lambda^{\star},\infty)\).

* _Showing_ \(f(\lambda^{\star})>0\). We first lower bound \(f_{0}(\lambda^{\star})\). Since \(\lambda^{\star}\geq\frac{3+\sigma}{4}\), we have \[\lambda^{\star}-\sigma(1+\sigma b\eta)\geq\frac{1-\sigma}{4}\,,\] (197) and from \(\lambda^{\star}\geq\frac{1+(1-\alpha)\gamma+\alpha}{2}\) we deduce \[\lambda^{\star}-(1-\alpha)\gamma-\alpha\geq\frac{(1-\gamma)(1-\alpha)}{2}\] (198) and \[\lambda^{\star}>\frac{1+\alpha}{2}\,,\] (199) which gives \[\lambda^{\star}-\sigma\alpha\geq\frac{1+\alpha}{2}-\sigma\alpha\,.\] (200) Combining (200), (197), (198), we have that \[f_{0}(\lambda^{\star})\geq\frac{1-\sigma}{8}\left(\frac{1+\alpha}{2}-\sigma \alpha\right)\eta\tau\,.\] (201) To continue, we upper bound \(f_{1}(\lambda^{\star})\) as follows. \[f_{1}(\lambda^{\star}) \leq S\tau\eta+\gamma cdM\eta+\frac{2+\gamma}{1-\gamma}cM\tau \eta^{2}\] \[=\eta\left(\tau\left(S+\frac{2+\gamma}{1-\gamma}Mc\eta\right)+ \gamma cdM\right)\,.\] (202) Plugging (201),(202) into (192) and using (199), we have \[f(\lambda^{\star}) >\frac{1-\alpha}{2}\left(f_{0}(\lambda^{\star})-\frac{\eta\sigma^ {2}}{1-\gamma}f_{1}(\lambda^{\star})\right)-\frac{\tau\eta^{3}\gamma}{(1- \gamma)^{2}}\cdot 2cdM\sigma^{2}\] \[\geq\frac{\tau\eta^{2}}{2(1-\gamma)}\left[\frac{1-\sigma}{8}\tau \left(1-\sigma+(1-\alpha)(\sigma-\frac{1}{2})\right)-\frac{\eta\sigma^{2}}{1- \gamma}\left(\tau\left(S+\frac{2+\gamma}{1-\gamma}Mc\sigma\right)+5\gamma cdM \right)\right]\] \[=\frac{\tau\eta^{2}}{2(1-\gamma)}\left[\frac{(1-\sigma)^{2}}{8} \tau-\frac{\eta}{1-\gamma}\left(S\tau\sigma^{2}+\frac{2+\gamma}{1-\gamma}Mc \sigma^{2}\tau\eta+\tau^{2}\left(\frac{1}{2}-\sigma^{2}\right)\cdot\frac{1- \sigma}{8}+5\gamma cdM\sigma^{2}\right)\right]\] \[\geq\frac{\tau\eta^{2}}{2(1-\gamma)}\left[\frac{(1-\sigma)^{2}}{8 }\tau-\frac{\eta}{1-\gamma}\left(S_{0}\tau\sigma^{2}+\frac{(1-\sigma)^{2}}{16 }\tau^{2}+(2+\gamma+5\gamma d_{0})\,cM\sigma^{2}\right)\right]\geq 0\,,\] where the penultimate inequality uses \(\frac{1}{2}-\sigma\leq\frac{1-\sigma}{2}\), and the last inequality follows from the definition of \(\zeta\) (cf. (53)).
* _Proving \(f\) is non-decreasing on \([\lambda^{\star},\infty)\)._ Note that \[\eta\leq\zeta\leq\frac{(1-\gamma)(1-\sigma)^{2}}{8S_{0}\sigma^{2}}\,,\] thus we have \[\forall\lambda\geq\lambda^{\star}:\quad f_{0}^{\prime}(\lambda)-\frac{\eta \sigma^{2}}{1-\gamma}f_{1}^{\prime}(\lambda)\geq(\lambda-\sigma\alpha)( \lambda-\sigma(1+\sigma b\eta))-\frac{\eta}{1-\gamma}S\sigma^{2}\geq 0\,,\] which indicates that \(f_{0}-f_{1}\) is non-decreasing on \([\lambda^{\star},\infty)\). Therefore, \(f\) is non-decreasing on \([\lambda^{\star},\infty)\).

### Proof of Lemma d.6

Note that bounding \(u^{(t+1)}(s,a)\) is identical to the proof in Appendix F.1 and shall be omitted. The rest of the proof also follows closely that of Lemma D.2, and we only highlight the differences due to approximation error for simplicity.

**Step 2: bound \(v^{(t+1)}(s,a)=\big{\|}\mathbf{T}^{(t+1)}(s,a)-\widehat{q}_{\tau}^{(t+1)}(s,a)\mathbf{1}_{N }\big{\|}_{2}\).** Let \(\mathbf{q}_{\tau}^{(t)}\coloneqq\left(q_{\tau,1}^{\pi^{(t)}_{(t)}},\cdots,q_{\tau,N }^{\pi^{(t)}_{(t)}}\right)^{\top}\). Similar to (167) we have

\[\big{\|}\mathbf{T}^{(t+1)}(s,a)-\widehat{q}_{\tau}^{(t+1)}(s,a)\mathbf{1}_ {N}\big{\|}_{2}\] \[\leq\sigma\big{\|}\mathbf{T}^{(t)}(s,a)-\widehat{q}_{\tau}^{(t)}(s,a) \mathbf{1}_{N}\big{\|}_{2}+\sigma\big{\|}\mathbf{q}_{\tau}^{(t+1)}(s,a)-\mathbf{q}_{\tau}^ {(t)}(s,a)\big{\|}_{2}\] \[\leq\sigma\big{\|}\mathbf{T}^{(t)}(s,a)-\widehat{q}_{\tau}^{(t)}(s,a) \mathbf{1}_{N}\big{\|}_{2}+\sigma\big{\|}\mathbf{Q}_{\tau}^{(t+1)}(s,a)-\mathbf{Q}_{\tau}^ {(t)}(s,a)\big{\|}_{2}+2\sigma\left\|\mathbf{e}\right\|_{2}. \tag{203}\]

**Step 3: bound \(\big{\|}Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t+1)}\big{\|}_{\infty}\).** In the context of inexact updates, (168) writes

\[\big{\|}Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t+1)}\big{\|}_{\infty}\leq \alpha\big{\|}Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t)}\big{\|}_{\infty}+ (1-\alpha)\big{\|}Q_{\tau}^{\star}-\overline{Q}_{\tau}^{(t)}\big{\|}_{\infty} +(1-\alpha)\big{\|}\overline{Q}_{\tau}^{(t)}-\widehat{q}_{\tau}^{(t)}\big{\|}_ {\infty}\,.\]

For the last term, following a similar argument in (169) leads to

\[\big{\|}\overline{Q}_{\tau}^{(t)}-\widehat{q}_{\tau}^{(t)}\big{\|} _{\infty} =\left\|\frac{1}{N}\sum_{n=1}^{N}Q_{\tau,n}^{\pi^{(t)}_{(t)}}- \frac{1}{N}\sum_{n=1}^{N}Q_{\tau,n}^{\pi^{(t)}_{(t)}}\right\|_{\infty}+\left\| \frac{1}{N}\sum_{n=1}^{N}\left(Q_{\tau,n}^{\pi^{(t)}_{(t)}}-q_{\tau,n}^{\pi^{( t)}_{(t)}}\right)\right\|_{\infty}\] \[\leq M\cdot\frac{1}{N}\sum_{n=1}^{N}\big{\|}\log\xi_{n}^{(t)}-\log \overline{\xi}^{(t)}\big{\|}_{\infty}+\frac{1}{N}\sum_{n=1}^{N}e_{n}\] \[\leq M\big{\|}u^{(t)}\big{\|}_{\infty}+\left\|\mathbf{e}\right\|_{ \infty}\,.\]

Combining the above two inequalities, we obtain

\[\big{\|}Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t+1)}\big{\|}_{\infty}\leq \alpha\big{\|}Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t)}\big{\|}_{\infty}+ (1-\alpha)\big{\|}Q_{\tau}^{\star}-\overline{Q}_{\tau}^{(t)}\big{\|}_{\infty}+ (1-\alpha)\left(M\big{\|}u^{(t)}\big{\|}_{\infty}+\big{\|}\mathbf{e}\big{\|}_{ \infty}\right)\,. \tag{204}\]

**Step 4: bound \(\big{\|}\mathbf{Q}_{\tau}^{(t+1)}(s,a)-\mathbf{Q}_{\tau}^{(t)}(s,a)\big{\|}_{2}\).** We remark that the bound established in (174) still holds in the inexact setting, with the same definition for \(w^{(t)}\):

\[\big{\|}\mathbf{Q}_{\tau}^{(t+1)}(s,a)-\mathbf{Q}_{\tau}^{(t)}(s,a)\big{\|}_{2}\leq M \sqrt{N}\left\|w^{(t)}\right\|_{\infty}\,. \tag{205}\]

To deal with the approximation error, we rewrite (176) as

\[\Big{\|}\mathbf{WT}^{(t)}(s,a)-\tau\log\mathbf{\xi}^{(t)}(s,a)-V_{\tau}^{ \star}(s)\mathbf{1}_{N}\Big{\|}_{2}\] \[=\Big{\|}\mathbf{WT}^{(t)}(s,a)-\tau\log\mathbf{\xi}^{(t)}(s,a)-\big{(}Q_ {\tau}^{\star}(s,a)-\tau\log\pi_{\tau}^{\star}(a|s)\big{)}\mathbf{1}_{N}\Big{\|} _{2}\] \[\leq\big{\|}\mathbf{WT}^{(t)}(s,a)-Q_{\tau}^{\star}(s,a)\mathbf{1}_{N} \big{\|}_{2}+\tau\big{\|}\log\mathbf{\xi}^{(t)}(s,a)-\log\pi_{\tau}^{\star}(a|s) \mathbf{1}_{N}\big{\|}_{2}\] \[\leq\big{\|}\mathbf{WT}^{(t)}(s,a)-\widehat{q}_{\tau}(s,a)\mathbf{1}_{N }\big{\|}_{2}+\big{\|}\widehat{q}_{\tau}(s,a)\mathbf{1}_{N}-Q_{\tau}^{\star}(s,a) \mathbf{1}_{N}\big{\|}_{2}\] \[\qquad+\tau\big{\|}\log\mathbf{\xi}^{(t)}(s,a)-\log\overline{\pi}^{(t) }(a|s)\mathbf{1}_{N}\big{\|}_{2}+\tau\big{\|}\log\overline{\pi}^{(t)}(a|s)\mathbf{1} _{N}-\log\pi_{\tau}^{\star}(a|s)\mathbf{1}_{N}\big{\|}_{2}\] \[\leq\sigma\left\|\mathbf{T}^{(t)}(s,a)-\widehat{q}_{\tau}^{(t)}(s,a) \mathbf{1}_{N}\right\|_{2}+\sqrt{N}\big{\|}\widehat{q}_{\tau}^{(t)}(s,a)-Q_{\tau}^ {\star}(s,a)\big{|}\] \[\qquad+\tau\left\|\log\mathbf{\xi}^{(t)}(s,a)-\log\overline{\pi}^{(t) }(a|s)\mathbf{1}\right\|_{2}+\tau\sqrt{N}\big{\|}\log\overline{\pi}^{(t)}(a|s)- \log\pi_{\tau}^{\star}(a|s)\big{|}\,, \tag{206}\]

where the second term can be upper-bounded by

\[\big{|}\widehat{q}_{\tau}^{(t)}(s,a)-Q_{\tau}^{\star}(s,a)\big{|} \leq\big{\|}\widehat{Q}_{\tau}^{(t)}-\overline{Q}_{\tau}^{(t)} \big{\|}_{\infty}+\big{\|}\overline{Q}_{\tau}^{(t)}-Q_{\tau}^{\star}\big{\|}_{ \infty}+\big{\|}\widehat{q}_{\tau}^{(t)}(s,a)-\widehat{Q}_{\tau}^{(t)}(s,a) \big{\|}_{\infty}\] \[\leq\big{\|}\widehat{Q}_{\tau}^{(t)}-\overline{Q}_{\tau}^{(t)} \big{\|}_{\infty}+\big{\|}\overline{Q}_{\tau}^{(t)}-Q_{\tau}^{\star}\big{\|}_{ \infty}+\left\|\mathbf{e}\right\|_{\infty}. \tag{207}\]

Combining (207), (206) and the established bounds in (175), (178), (180) leads to

\[w^{(t)}(s,a)\leq \left(2\alpha+(1-\alpha)\cdot\sqrt{2N}\right)\Big{\|}u^{(t)}\Big{\|} _{\infty}+\frac{1-\alpha}{\tau}\left\|v^{(t)}\right\|_{\infty}\] \[\quad+\frac{1-\alpha}{\tau}\cdot\sqrt{N}\left(\left\|\widehat{Q}_{ \tau}^{(t)}-\overline{Q}_{\tau}^{(t)}\right\|_{\infty}+\left\|\overline{Q}_{ \tau}^{(t)}-Q_{\tau}^{\star}\right\|_{\infty}+\left\|\mathbf{e}\right\|_{\infty} \right)+\frac{1-\alpha}{\tau}\cdot 2\sqrt{N}\left\|Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t)}\right\|_{ \infty}\,.\]Combining the above inequality with (205) and (203) gives

\[\left\|v^{(t+1)}\right\|_{\infty} \leq\sigma\left(1+\frac{\eta M\sqrt{N}}{1-\gamma}\sigma\right)\left\| v^{(t)}\right\|_{\infty}+\sigma M\sqrt{N}\Bigg{\{}\left(2\alpha+(1-\alpha)\cdot \sqrt{2N}+\frac{1-\alpha}{\tau}\cdot\sqrt{N}M\right)\left\|u^{(t)}\right\|_{ \infty}\] \[+\frac{1-\alpha}{\tau}\cdot\sqrt{N}\left(\left\|\overline{Q}_{\tau }^{(t)}-Q_{\tau}^{\star}\right\|_{\infty}+\left\|\mathbf{e}\right\|_{\infty}\right) +\frac{1-\alpha}{\tau}\cdot 2\sqrt{N}\big{\|}Q_{\tau}^{\star}-\tau\log\overline{ \xi}^{(t)}\big{\|}_{\infty}\Bigg{\}}+2\sigma\sqrt{N}\left\|\mathbf{e}\right\|_{ \infty}\,. \tag{208}\]

**Step 5: bound \(\left\|\overline{Q}_{\tau}^{(t+1)}-Q_{\tau}^{\star}\right\|_{\infty}\).** It is straightforward to verify that (187) applies to the inexact updates as well:

\[\left\|Q_{\tau}^{\star}-\overline{Q}_{\tau}^{(t+1)}\right\|_{\infty}\leq\gamma \left\|Q_{\tau}^{\star}-\tau\log\overline{\xi}^{(t+1)}\right\|_{\infty}+\gamma \left(-\min_{s,a}\left(\overline{Q}_{\tau}^{(t+1)}(s,a)-\tau\log\overline{\xi} ^{(t+1)}(s,a)\right)\right)\,.\]

Plugging the above inequality into (204) and (208) establishes the bounds on \(\Omega_{3}^{(t+1)}\) and \(\Omega_{2}^{(t+1)}\) in (68), respectively. **Step 6: bound \(-\min_{s,a}\left(\overline{Q}_{\tau}^{(t+1)}(s,a)-\tau\log\overline{\xi}^{(t+1) }(s,a)\right)\).** We obtain the following lemma by interpreting the approximation error \(\mathbf{e}\) as part of the consensus error \(\left\|\widehat{Q}_{\tau}^{(t)}-\overline{Q}_{\tau}^{(t)}\right\|_{\infty}\) in Lemma F.4.

**Lemma F.5** (inexact version of Lemma F.4).: _Suppose \(0<\eta\leq(1-\gamma)/\tau\). For any state-action pair \((s_{0},a_{0})\in\mathcal{S}\times\mathcal{A}\), one has_

\[\overline{V}_{\tau}^{(t+1)}(s_{0})-\overline{V}_{\tau}^{(t)}(s_{0}) \geq\frac{1}{\eta}\mathop{\mathbb{E}}_{s\sim d_{\tau_{0}}^{\mathbf{\pi} ^{(t+1)}}}\left[\alpha\text{KL}\big{(}\overline{\pi}^{(t+1)}(\cdot|s_{0})\, \|\,\overline{\pi}^{(t)}(\cdot|s_{0})\big{)}+\text{KL}\big{(}\overline{\pi}^{( t)}(\cdot|s_{0})\,\|\,\overline{\pi}^{(t+1)}(\cdot|s_{0})\big{)}\right]\] \[\qquad-\frac{2}{1-\gamma}\left(\left\|\widehat{Q}_{\tau}^{(t)}- \overline{Q}_{\tau}^{(t)}\right\|_{\infty}+\left\|\mathbf{e}\right\|_{\infty} \right)\,, \tag{209}\] \[\overline{Q}_{\tau}^{(t+1)}(s_{0},a_{0})-\overline{Q}_{\tau}^{(t) }(s_{0},a_{0}) \geq-\frac{2\gamma}{1-\gamma}\left(\left\|\widehat{Q}_{\tau}^{(t)}- \overline{Q}_{\tau}^{(t)}\right\|_{\infty}+\left\|\mathbf{e}\right\|_{\infty} \right)\,. \tag{210}\]

Using (210), we have

\[\overline{Q}_{\tau}^{(t+1)}(s,a)-\tau\left(\alpha\log\overline{ \xi}^{(t)}(s,a)+(1-\alpha)\frac{\widehat{Q}_{\tau}^{(t)}(s,a)}{\tau}\right)\] \[\geq\overline{Q}_{\tau}^{(t)}(s,a)-\tau\left(\alpha\log\overline {\xi}^{(t)}(s,a)+(1-\alpha)\frac{\widehat{Q}_{\tau}^{(t)}(s,a)}{\tau}\right)- \frac{2\gamma}{1-\gamma}\left(\left\|\widehat{Q}_{\tau}^{(t)}-\overline{Q}_{ \tau}^{(t)}\right\|_{\infty}+\left\|\mathbf{e}\right\|_{\infty}\right)\] \[\geq\alpha\left(\overline{Q}_{\tau}^{(t)}(s,a)-\tau\log\overline {\xi}^{(t)}(s,a)\right)-\frac{2\gamma+\eta\tau}{1-\gamma}\left\|\widehat{Q}_{ \tau}^{(t)}-\overline{Q}_{\tau}^{(t)}\right\|_{\infty}-\frac{2\gamma}{1-\gamma }\left\|\mathbf{e}\right\|_{\infty}\,, \tag{211}\]

which gives

\[-\min_{s,a}\left(\overline{Q}_{\tau}^{(t+1)}(s,a)-\tau\log\overline {\xi}^{(t+1)}(s,a)\right) \tag{212}\] \[\leq-\alpha\min_{s,a}\left(\overline{Q}_{\tau}^{(t)}(s,a)-\tau \log\overline{\xi}^{(t)}(s,a)\right)+\frac{2\gamma+\eta\tau}{1-\gamma}M \left\|u^{(t)}\right\|_{\infty}+\frac{2\gamma}{1-\gamma}\left\|\mathbf{e}\right\| _{\infty}\,.\]

### Proof of Lemma D.8

**Step 1: bound \(u^{(t+1)}(s,a)=\left\|\log\mathbf{\xi}^{(t+1)}(s,a)-\log\overline{\xi}^{(t+1)}(s,a )\mathbf{1}_{N}\right\|_{2}\).** Following the same strategy in establishing (166), we have

\[\left\|\log\mathbf{\xi}^{(t+1)}(s,a)-\log\overline{\xi}^{(t+1)}(s,a) \mathbf{1}_{N}\right\|_{2}\] \[=\left\|\Big{(}\mathbf{W}\log\mathbf{\xi}^{(t)}(s,a)-\log\overline{\xi}^ {(t)}(s,a)\mathbf{1}_{N}\Big{)}+\frac{\eta}{1-\gamma}\left(\mathbf{W}\mathbf{T}^{(t)}(s,a)-\widehat{Q}^{(t)}(s,a)\mathbf{1}_{N}\right)\right\|_{2}\] \[\leq\sigma\left\|\log\mathbf{\xi}^{(t)}(s,a)-\log\overline{\xi}^{(t)} (s,a)\mathbf{1}_{N}\right\|_{2}+\frac{\eta}{1-\gamma}\sigma\left\|\mathbf{T}^{(t)}(s,a)-\widehat{Q}^{(t)}(s,a)\mathbf{1}_{N}\right\|_{2}\,, \tag{213}\]or equivalently

\[\big{\|}u^{(t+1)}\big{\|}_{\infty}\leq\sigma\big{\|}u^{(t)}\big{\|}_{\infty}+\frac{ \eta}{1-\gamma}\sigma\big{\|}v^{(t)}\big{\|}_{\infty}\,. \tag{214}\]

**Step 2: bound \(v^{(t+1)}(s,a)=\big{\|}\mathbf{T}^{(t+1)}(s,a)-\widehat{Q}^{(t+1)}(s,a)\mathbf{1}_{N} \big{\|}_{2}\).** In the same vein of establishing (167), we have

\[\big{\|}\mathbf{T}^{(t+1)}(s,a)-\widehat{Q}^{(t+1)}(s,a)\mathbf{1}_{N} \big{\|}_{2}\] \[\leq\sigma\big{\|}\mathbf{T}^{(t)}(s,a)-\widehat{Q}^{(t)}(s,a)\mathbf{1}_ {N}\big{\|}_{2}+\sigma\big{\|}\mathbf{Q}^{(t+1)}(s,a)-\mathbf{Q}^{(t)}(s,a)\big{\|}_{2}\,, \tag{215}\]

The term \(\big{\|}\mathbf{Q}^{(t+1)}(s,a)-\mathbf{Q}^{(t)}(s,a)\big{\|}_{2}\) can be bounded in a similar way in (174):

\[\big{\|}\mathbf{Q}^{(t+1)}(s,a)-\mathbf{Q}^{(t)}(s,a)\big{\|}_{2}\leq\frac{(1+\gamma) \gamma}{(1-\gamma)^{2}}\sqrt{N}\big{\|}w_{0}^{(t)}\big{\|}_{\infty}\,, \tag{216}\]

where the coefficient \(\frac{(1+\gamma)\gamma}{(1-\gamma)^{2}}\) comes from \(M\) in Lemma F.3 when \(\tau=0\), and \(w_{0}^{(t)}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}|}\) is defined as

\[\forall(s,a)\in\mathcal{S}\times\mathcal{A}:\quad w_{0}^{(t)}(s,a)\coloneqq \left\|\log\mathbf{\xi}^{(t+1)}(s,a)-\log\mathbf{\xi}^{(t)}(s,a)-\frac{\eta}{1-\gamma}V ^{\star}(s)\mathbf{1}_{N}\right\|_{2}\,. \tag{217}\]

It remains to bound \(\big{\|}w_{0}^{(t)}\big{\|}_{\infty}\). Towards this end, we rewrite (175) as

\[w_{0}^{(t)}(s,a)\] \[=\big{\|}\mathbf{W}\left(\log\mathbf{\xi}^{(t)}(s,a)+\frac{\eta}{1-\gamma }\mathbf{T}^{(t)}(s,a)\right)-\log\mathbf{\xi}^{(t)}(s,a)-\frac{\eta}{1-\gamma}V^{\star }(s)\mathbf{1}_{N}\big{\|}_{2}\] \[=\bigg{\|}\big{(}\mathbf{W}-\mathbf{I}\big{)}\left(\log\mathbf{\xi}^{(t)}(s,a )-\log\overline{\xi}^{(t)}(s,a)\mathbf{1}_{N}\right)+\frac{\eta}{1-\gamma}\left( \mathbf{W}\mathbf{T}^{(t)}(s,a)-V^{\star}(s)\mathbf{1}_{N}\right)\bigg{

[MISSING_PAGE_FAIL:46]

[MISSING_PAGE_EMPTY:47]

**Step 2: establish the descent equation.** Note that Lemma F.6 directly applies by replacing \(\widehat{Q}^{(t)}\) with \(\widehat{q}^{(t)}\):

\[\phi^{(t+1)}(\eta)\leq\phi^{(t)}(\eta)+\frac{2\eta}{(1-\gamma)^{2}}\left\| \widehat{q}^{(t)}-\overline{Q}^{(t)}\right\|_{\infty}-\eta\left(V^{\star}(\rho )-\overline{V}^{(t)}(\rho)\right)\,.\]

To bound the middle term, for all \(t\geq 0\), we have

\[\left\|\overline{Q}^{(t)}-\widehat{q}^{(t)}\right\|_{\infty} =\left\|\frac{1}{N}\sum_{n=1}^{N}Q_{n}^{\pi_{n}^{(t)}}-\frac{1}{N }\sum_{n=1}^{N}Q_{n}^{\overline{\pi}^{(t)}}\right\|_{\infty}+\frac{1}{N}\left\| \sum_{n=0}^{N}\left(q_{n}^{\pi_{n}^{(t)}}-Q_{n}^{\pi_{n}^{(t)}}\right)\right\| _{\infty}\] \[\leq\frac{(1+\gamma)\gamma}{(1-\gamma)^{2}}\cdot\frac{1}{N}\sum_{ n=1}^{N}\left\|\log\xi_{n}^{(t)}-\log\overline{\xi}^{(t)}\right\|_{\infty}+ \frac{1}{N}\sum_{n=1}^{N}e_{n}\] \[\leq\frac{(1+\gamma)\gamma}{(1-\gamma)^{2}}\left\|u^{(t)}\right\| _{\infty}+\left\|\mathbf{e}\right\|_{\infty}\,. \tag{229}\]

Hence, (102) is established by combining the above two inequalities.

**Step 4: bound the consensus error.** Similar as (224), here we have

\[\left\|\mathbf{\Omega}^{(t)}\right\|_{2}\leq \rho(\mathbf{B}(\eta))\left\|\mathbf{\Omega}^{(t-1)}\right\|_{2}+(d_{2}( \eta)+c_{2}(\eta))\] \[\leq \cdots\leq\rho^{t}(\mathbf{B}(\eta))\left\|\mathbf{\Omega}^{(0)}\right\| _{2}+\sum_{i=0}^{t-1}\rho^{i}(\mathbf{B}(\eta))\left(\frac{(1+\gamma)\gamma N\sigma }{(1-\gamma)^{4}}\eta+\sqrt{N}\sigmaBy the choice of \(\mathbf{T}^{(0)}\) (line 2 of Algorithm 2), we have \(\overline{T}^{(0)}=\widehat{Q}^{(0)}_{\tau}\) and hence by induction

\[\forall t\geq 0:\quad\overline{T}^{(t)}=\widehat{Q}^{(t)}_{\tau}\,. \tag{233}\]

This implies

\[\log\overline{\xi}^{(t+1)}(s,a)-\alpha\log\overline{\xi}^{(t)}(s,a) =(1-\alpha)\widehat{Q}^{(t)}_{\tau}(s,a)/\tau\] \[=(1-\alpha)\overline{T}^{(t)}(s,a)/\tau\] \[=\frac{1}{N}\mathbf{1}^{\top}\log\mathbf{\xi}^{(t+1)}(s,a)-\alpha\frac{1} {N}\mathbf{1}^{\top}\log\mathbf{\xi}^{(t)}(s,a).\]

Therefore, to prove (161), it suffices to verify the claim for \(t=0\):

\[\frac{1}{N}\mathbf{1}^{\top}\log\mathbf{\xi}^{(0)}(s,a) =\log\left\|\exp\left(Q^{\star}_{\tau}(s,\cdot)/\tau\right)\right\| _{1}+\frac{1}{N}\mathbf{1}^{\top}\log\mathbf{\pi}^{(0)}(a|s)-\log\left\|\exp\left(\frac {1}{N}\sum_{n=1}^{N}\log\pi_{n}^{(0)}(\cdot|s)\right)\right\|_{1}\] \[=\log\left\|\exp\left(Q^{\star}_{\tau}(s,\cdot)/\tau\right)\right\| _{1}+\log\overline{\pi}^{(0)}(a|s)=\log\overline{\xi}^{(0)}(s,a)\,.\]

By taking logarithm over both sides of the definition of \(\overline{\pi}^{(t+1)}\) (cf. (27)), we get

\[\log\overline{\pi}^{(t+1)}(a|s)=\alpha\log\overline{\pi}^{(t)}(a|s)+(1-\alpha) \widehat{Q}^{(t)}(s,a)/\tau-z^{(t)}(s) \tag{234}\]

for some constant \(z^{(t)}(s)\), which deviate from the update rule of \(\log\overline{\xi}^{(t+1)}\) by a global constant shift and hence verifies (162).

### Proof of Lemma F.3

For notational simplicity, we let \(Q^{\theta^{\prime}}_{\tau}\) and \(Q^{\theta}_{\tau}\) denote \(Q^{\pi_{\theta^{\prime}}}_{\tau}\) and \(Q^{\pi_{\theta}}_{\tau}\), respectively. From (6a) we immediately know that to bound \(\left\|Q^{\theta^{\prime}}_{\tau}-Q^{\theta}_{\tau}\right\|_{\infty}\), it suffices to control \(\left|V^{\theta}_{\tau}(s)-V^{\theta^{\prime}}_{\tau}(s)\right|\) for each \(s\in\mathcal{S}\). By (4) we have

\[\left|V^{\theta}_{\tau}(s)-V^{\theta^{\prime}}_{\tau}(s)\right|\leq\left|V^{ \theta}(s)-V^{\theta^{\prime}}(s)\right|+\tau\big{|}\mathcal{H}(s,\pi_{\theta })-\mathcal{H}(s,\pi_{\theta^{\prime}})\big{|}\,, \tag{235}\]

so in the following we bound both terms in the RHS of (235).

**Step 1: bounding \(\left|\mathcal{H}(s,\pi_{\theta})-\mathcal{H}(s,\pi_{\theta^{\prime}})\right|\).** We first bound \(\left|\mathcal{H}(s,\pi_{\theta})-\mathcal{H}(s,\pi_{\theta^{\prime}})\right|\) using the idea in the proof of Lemma 14 in [10]. We let

\[\theta^{(t)}=\theta+t(\theta^{\prime}-\theta)\,,\quad\forall t\in\mathbb{R}\,, \tag{236}\]

and let \(h^{(t)}\in\mathbb{R}^{|\mathcal{S}|}\) be

\[\forall s\in\mathcal{S}:\quad h^{(t)}(s)\coloneqq-\sum_{a\in\mathcal{A}}\pi_{ \theta^{(t)}}(a|s)\log\pi_{\theta^{(t)}}(a|s)\,. \tag{237}\]

Note that \(\left\|h^{(t)}\right\|_{\infty}\leq\log|\mathcal{A}|\). We also denote \(H^{(t)}:\mathcal{S}\rightarrow\mathbb{R}^{|\mathcal{A}|\times|\mathcal{A}|}\) by:

\[\forall s\in\mathcal{S}:\quad H^{(t)}(s)\coloneqq\left.\frac{\partial\pi_{ \theta}(\cdot|s)}{\partial\theta}\right|_{\theta=\theta^{(t)}}=\operatorname{ diag}\{\pi_{\theta^{(t)}}(\cdot|s)\}-\pi_{\theta^{(t)}}(\cdot|s)\pi_{\theta^{(t)}}( \cdot|s)^{\top}\,, \tag{238}\]

then we have

\[\forall s\in\mathcal{S}:\quad\left|\frac{dh^{(t)}(s)}{dt}\right| =\left|\left\langle\frac{\partial h^{(t)}(s)}{\partial\theta^{(t)}( \cdot|s)},\theta^{\prime}(s,\cdot)-\theta(s,\cdot)\right\rangle\right|\] \[=\left|\left\langle H^{(t)}(s)\log\pi_{\theta^{(t)}}(\cdot|s), \theta^{\prime}(s,\cdot)-\theta(s,\cdot)\right\rangle\right|\] \[\leq\left\|H^{(t)}(s)\log\pi_{\theta^{(t)}}(\cdot|s)\right\|_{1} \left\|\theta^{\prime}(s,\cdot)-\theta(s,\cdot)\right\|_{\infty}\,, \tag{239}\]

where \(\frac{\partial h^{(t)}(s)}{\partial\theta^{(t)}(\cdot|s)}\) stands for \(\frac{\partial h^{(t)}(s)}{\partial\theta^{(t)}(\cdot|s)}\big{|}_{\theta= \theta^{(t)}}\). The first term in (239) is further upper bounded as

\[\left\|H^{(t)}(s)\log\pi_{\theta^{(t)}}(\cdot|s)\right\|_{1} =\sum_{a\in\mathcal{A}}\pi_{\theta^{(t)}}(a|s)\left|\log\pi_{ \theta^{(t)}}(a|s)-\pi_{\theta^{(t)}}(\cdot|s)^{\top}\log\pi_{\theta^{(t)}}( \cdot|s)\right|\] \[\leq\sum_{a\in\mathcal{A}}\pi_{\theta^{(t)}}(a|s)\left(\left|\log \pi_{\theta^{(t)}}(a|s)\right|+\left|\pi_{\theta^{(t)}}(\cdot|s)^{\top}\log\pi_{ \theta^{(t)}}(\cdot|s)\right|\right)\] \[=-2\sum_{a\in\mathcal{A}}\pi_{\theta^{(t)}}(a,s)\log\pi_{\theta^ {(t)}}(a|s)\leq 2\log|\mathcal{A}|\,.\]By Lagrange mean value theorem, there exists \(t\in(0,1)\) such that

\[\left|h_{1}(s)-h_{0}(s)\right|=\left|\frac{dh^{(t)}(s)}{dt}\right|\leq 2\log \left|\mathcal{A}\right|\left\|\theta^{\prime}(s,\cdot)-\theta(s,\cdot) \right\|_{\infty}\,,\]

where the inequality follows from (239) and the above inequality. Combining (5) with the above inequality, we arrive at

\[\left|\mathcal{H}(s,\pi_{\theta})-\mathcal{H}(s,\pi_{\theta^{\prime}})\right| \leq\frac{2\log\left|\mathcal{A}\right|}{1-\gamma}\left\|\theta^{\prime}- \theta\right\|_{\infty}\,. \tag{240}\]

**Step 2: bounding \(\left|V^{\theta}(s)-V^{\theta^{\prime}}(s)\right|\).** Similar to the previous proof, we bound \(\left|V^{\theta}(s)-V^{\theta^{\prime}}(s)\right|\) by bounding \(\left|\frac{dV^{\theta(t)}}{dt}(s)\right|\). By Bellman's consistency equation, the value function of \(\pi_{\theta^{(t)}}\) is given by

\[V^{\theta^{(t)}}(s)=\sum_{a\in\mathcal{A}}\pi_{\theta^{(t)}}(a|s)r(s,a)+ \gamma\sum_{a}\pi_{\theta_{\alpha}}(a|s)\sum_{s^{\prime}\in\mathcal{S}} \mathcal{P}(s^{\prime}|s,a)V^{\theta^{(t)}}(s^{\prime})\,,\]

which can be represented in a matrix-vector form as

\[V^{\theta^{(t)}}(s)=e_{s}^{\top}\mathbf{M}_{t}r_{t}\,, \tag{241}\]

where \(e_{s}\in\mathbb{R}^{|\mathcal{S}|}\) is a one-hot vector whose \(s\)-th entry is 1,

\[\mathbf{M}_{t}\coloneqq(\mathbf{I}-\gamma\mathbf{P}_{t})^{-1}\,, \tag{242}\]

with \(\mathbf{P}_{t}\in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}|}\) denoting the induced state transition matrix by \(\pi_{\theta^{(t)}}\)

\[\mathbf{P}_{t}(s,s^{\prime})=\sum_{a\in\mathcal{A}}\pi_{\theta^{(t)}}(a|s) \mathcal{P}(s^{\prime}|s,a)\,, \tag{243}\]

and \(r_{t}\in\mathbb{R}^{|\mathcal{S}|}\) is given by

\[\forall s\in\mathcal{S}:\quad r_{t}(s)\coloneqq\sum_{a\in\mathcal{A}}\pi_{ \theta^{(t)}}(a|s)r(s,a)\,. \tag{244}\]

Taking derivative w.r.t. \(t\) in (241), we obtain [12]

\[\frac{dV^{\theta^{(t)}}(s)}{dt}=\gamma\cdot e_{s}^{\top}\mathbf{M}_{t}\frac{d\mathbf{ P}_{t}}{dt}\mathbf{M}_{t}r_{t}+e_{s}^{\top}\mathbf{M}_{t}\frac{dr_{t}}{dt}\,. \tag{245}\]

We now calculate each term respectively.

* For the first term, it follows that \[\left|\gamma\cdot e_{s}^{\top}\mathbf{M}_{t}\frac{d\mathbf{P}_{t}}{dt}\bm {M}_{t}r_{t}\right| \leq\gamma\left\|\mathbf{M}_{t}\frac{d\mathbf{P}_{t}}{dt}\mathbf{M}_{t}r_{t} \right\|_{\infty}\] \[\leq\frac{\gamma}{1-\gamma}\left\|\frac{d\mathbf{P}_{t}}{dt}\mathbf{M}_{t }r_{t}\right\|_{\infty}\] \[\leq\frac{2\gamma}{1-\gamma}\left\|\mathbf{M}_{t}r_{t}\right\|_{ \infty}\left\|\theta^{\prime}-\theta\right\|_{\infty}\] (246) \[\leq\frac{2\gamma}{(1-\gamma)^{2}}\left\|r_{t}\right\|_{\infty} \left\|\theta^{\prime}-\theta\right\|_{\infty}\] \[\leq\frac{2\gamma}{(1-\gamma)^{2}}\left\|\theta^{\prime}-\theta \right\|_{\infty}\,.\] (247) where the second and fourth lines use the fact \(\|\mathbf{M}_{t}\|_{1}\leq 1/(1-\gamma)\)[13, Lemma 10], and the last line follow from \[\left\|r_{t}\right\|_{\infty}=\max_{s\in\mathcal{S}}\left|\sum_{a\in \mathcal{A}}\pi_{\theta^{(t)}}(a|s)r(s,a)\right|\leq 1.\] We defer the proof of (246) to the end of proof.

* For the second term, it follows that \[\left|e_{s}^{\top}\mathbf{M}_{t}\frac{dr_{t}}{dt}\right|\leq\frac{1}{1-\gamma}\left\| \frac{dr_{t}}{dt}\right\|_{\infty}\leq\frac{1}{1-\gamma}\left\|\theta^{\prime}- \theta\right\|_{\infty}\,.\] (248) where the first inequality follows again from \(\|\mathbf{M}_{t}\|_{1}\leq 1/(1-\gamma)\), and the second inequality follows from \[\left\|\frac{dr_{t}}{dt}\right\|_{\infty}=\max_{s\in\mathcal{S}} \left|\frac{dr_{t}(s)}{dt}\right| =\max_{s\in\mathcal{S}}\left|\left\langle\frac{\partial\pi_{\theta ^{(t)}}(\cdot|s)^{\top}r(s,\cdot)}{\partial\theta^{(t)}(s,\cdot)},\theta^{ \prime}(s,\cdot)-\theta(s,\cdot)\right\rangle\right|\] \[\leq\max_{s\in\mathcal{S}}\left\|\frac{\partial\pi_{\theta^{(t)}} (\cdot|s)^{\top}}{\partial\theta^{(t)}(s,\cdot)}r(s,\cdot)\right\|_{1}\left\| \theta^{\prime}(s,\cdot)-\theta(s,\cdot)\right\|_{\infty}\] \[=\max_{s\in\mathcal{S}}\left(\sum_{a\in\mathcal{A}}\pi_{\theta^{ (t)}}(a|s)\left|r(s,a)-\pi_{\theta^{(t)}}(\cdot|s)^{\top}r(s,\cdot)\right| \right)\left\|\theta^{\prime}(s,\cdot)-\theta(s,\cdot)\right\|_{\infty}\] \[\leq\max_{s\in\mathcal{S}}\max_{\begin{subarray}{c}a\in\mathcal{A }\end{subarray}}\left|r(s,a)-\pi_{\theta^{(t)}}(\cdot|s)^{\top}r(s,\cdot) \right|_{\leq 1\text{ since }r(s,a)\in[0,1]}\left\|\theta^{\prime}(s,\cdot)- \theta(s,\cdot)\right\|_{\infty}\] \[\leq\max_{s\in\mathcal{S}}\left\|\theta^{\prime}(s,\cdot)-\theta( s,\cdot)\right\|_{\infty}=\left\|\theta^{\prime}-\theta\right\|_{\infty}\,.\] (249)

Plugging the above two inequalities into (245) and using Lagrange mean value theorem, we have

\[\left|V^{\theta}(s)-V^{\theta^{\prime}}(s)\right|\leq\frac{1+\gamma}{(1- \gamma)^{2}}\left\|\theta^{\prime}-\theta\right\|_{\infty}\,. \tag{250}\]

**Step 3: sum up.** Combining (250), (240) and (235), we have

\[\forall s\in\mathcal{S}:\quad\left|V^{\theta}_{\tau}(s)-V^{\theta^{\prime}}_{ \tau}(s)\right|\leq\frac{1+\gamma+2\tau(1-\gamma)\log\left|\mathcal{A}\right| }{(1-\gamma)^{2}}\left\|\log\pi-\log\pi^{\prime}\right\|_{\infty}\,. \tag{251}\]

Combining (251) and (6a), (170) immediately follows.

**Proof of (246).** For any vector \(x\in\mathbb{R}^{|\mathcal{S}|}\), we have

\[\left[\frac{d\mathbf{P}_{t}}{dt}x\right]_{s}=\sum_{s^{\prime}\in\mathcal{S}}\sum_{ a\in\mathcal{A}}\frac{d\pi_{\theta^{(t)}}(a|s)}{dt}\mathcal{P}(s^{\prime}|s,a)x (s^{\prime})\,,\]

from which we can bound the \(l_{\infty}\) norm as

\[\left\|\frac{d\mathbf{P}_{t}}{dt}x\right\|_{\infty} \leq\max_{s}\sum_{a\in\mathcal{A}}\sum_{s^{\prime}\in\mathcal{S} }\mathcal{P}(s^{\prime}|s,a)\left|\frac{d\pi_{\theta^{(t)}}(a|s)}{dt}\right| \left\|x\right\|_{\infty}\] \[=\max_{s}\sum_{a\in\mathcal{A}}\left|\frac{d\pi_{\theta^{(t)}}(a| s)}{dt}\right|\left\|x\right\|_{\infty}\] \[\leq 2\left\|\theta^{\prime}-\theta\right\|_{\infty}\left\|x \right\|_{\infty} \tag{252}\]

as desired, where the last line follows from the following fact:

\[\sum_{a\in\mathcal{A}}\left|\frac{d\pi_{\theta^{(t)}}(a|s)}{dt}\right| =\sum_{a\in\mathcal{A}}\left|\left\langle\frac{\partial\pi_{ \theta^{(t)}}(a|s)}{\partial\theta^{(t)}},\theta^{\prime}-\theta\right\rangle\right|\] \[=\sum_{a\in\mathcal{A}}\left|\left\langle\frac{\partial\pi_{ \theta^{(t)}}(a|s)}{\partial\theta^{(t)}(s,\cdot)},\theta^{\prime}(s,\cdot)- \theta(s,\cdot)\right\rangle\right|\] \[=\sum_{a\in\mathcal{A}}\pi_{\theta^{(t)}}(a|s)\left|(\theta^{ \prime}(s,a)-\theta(s,a))-\pi_{\theta^{(t)}}(\cdot|s)^{\top}\left(\theta^{ \prime}(s,\cdot)-\theta(s,\cdot)\right)\right|\] \[\leq\max_{a}\left|\theta^{\prime}(s,a)-\theta(s,a)\right|+\left| \pi_{\theta^{(t)}}(\cdot|s)^{\top}\left(\theta^{\prime}(s,\cdot)-\theta(s, \cdot)\right)\right|\] \[\leq 2\left\|\theta^{\prime}-\theta\right\|_{\infty}\,.\]

[MISSING_PAGE_FAIL:52]

To finish up, applying (256) recursively to expand \(\overline{V}^{(t)}_{\tau}(s_{i})\), \(i\geq 1\) and making use of (257), we arrive at

\[\overline{V}^{(t)}_{\tau}(s_{0})\] \[\leq\sum_{i=1}^{\infty}\gamma^{i}\cdot 2\left\|\delta^{(t)}\right\|_{ \infty}+\operatorname*{\mathbb{E}}_{\begin{subarray}{c}a_{i}\sim\overline{ \mathbf{\pi}}^{(t+1)}\cdot(\cdot|s_{i}),\\ s_{i+1}\sim P(\cdot|s_{i},a_{i}),\forall i\geq 0\end{subarray}}\left[\sum_{i =1}^{\infty}\gamma^{i}\left\{r(s_{i},a_{i})-\tau\log\overline{\mathbf{\pi}}^{(t +1)}(a_{i}|s_{i})\right\}\right.\] \[\qquad-\sum_{i=1}^{\infty}\gamma^{i}\left\{\left(\frac{1-\gamma}{ \eta}-\tau\right)\mathsf{KL}\big{(}\overline{\mathbf{\pi}}^{(t+1)}(\cdot|s_{i} )\,\|\,\overline{\mathbf{\pi}}^{(t)}(\cdot|s_{i})\right)+\frac{1-\gamma}{\eta} \mathsf{KL}\big{(}\overline{\mathbf{\pi}}^{(t)}(\cdot|s_{i})\,\|\,\overline{ \mathbf{\pi}}^{(t+1)}(\cdot|s_{i})\big{)}\right\}\Bigg{\}}\] \[=\frac{2}{1-\gamma}\left\|\delta^{(t)}\right\|_{\infty}+\overline {V}^{(t+1)}_{\tau}(s_{0})\] \[\qquad-\operatorname*{\mathbb{E}}_{s\sim d^{\overline{\mathbf{ \pi}}^{(t+1)}}_{\tau_{0}}}\left[\left(\frac{1}{\eta}-\frac{\tau}{1-\gamma} \right)\mathsf{KL}\big{(}\overline{\mathbf{\pi}}^{(t+1)}(\cdot|s_{i})\,\|\, \overline{\mathbf{\pi}}^{(t)}(\cdot|s_{i})\right)+\frac{1}{\eta}\mathsf{KL} \big{(}\overline{\mathbf{\pi}}^{(t)}(\cdot|s_{i})\,\|\,\overline{\mathbf{\pi}} ^{(t+1)}(\cdot|s_{i})\big{)}\right]\,, \tag{258}\]

where the third line follows since \(\overline{V}^{(t+1)}_{\tau}\) can be viewed as the value function of \(\overline{\mathbf{\pi}}^{(t+1)}\) with adjusted rewards \(\overline{\mathbf{\pi}}^{(t+1)}(s,a)\coloneqq r(s,a)-\tau\log\overline{\mathbf{ \pi}}^{(t+1)}(s|a)\). And (188) follows immediately from the above inequality (258). By (6a) we can easily see that (189) is a consequence of (188).

### Proof of Lemma f.6

We first introduce the famous performance difference lemma which will be used in our proof.

**Lemma G.1** (Performance difference lemma).: _For any policy \(\pi,\pi^{\prime}\in\Delta(\mathcal{A})^{\mathcal{S}}\) and \(\rho\in\Delta(\mathcal{S})\), we have_

\[V^{\pi}(\rho)-V^{\pi^{\prime}}(\rho) =\frac{1}{1-\gamma}\mathbb{E}_{(s,a)\sim\bar{d}^{*}}\left[A^{\pi^ {\prime}}(s,a)\right] \tag{259}\] \[=\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{*}}\left[\langle Q^{\pi^ {\prime}}(s),\pi(s)-\pi^{\prime}(s)\rangle\right]. \tag{260}\]

Proof.: See Lemma 3 in [222]. 

For all \(t\geq 0\), we define the advantage function \(\overline{A}^{(t)}\) as:

\[\forall(s,a)\in\mathcal{S}\times\mathcal{A}:\quad\overline{A}^{(t)}(s,a) \coloneqq\overline{Q}^{(t)}(s,a)-\overline{V}^{(t)}(s)\,. \tag{261}\]

Then for Alg. 1, the update rule of \(\overline{\pi}\) (Eq. (234)) can be written as

\[\log\overline{\mathbf{\pi}}^{(t+1)}(a|s)=\log\overline{\mathbf{\pi}}^{(t)}(a| s)+\frac{\eta}{1-\gamma}\left(\overline{A}^{(t)}(s,a)+\delta^{(t)}(s,a) \right)-\log\widehat{z}^{(t)}(s)\,, \tag{262}\]

where \(\delta^{(t)}\) is defined in (253) and

\[\log\widehat{z}^{(t)}(s) =\log\sum_{a^{\prime}\in\mathcal{A}}\overline{\mathbf{\pi}}^{(t) }(a^{\prime}|s)\exp\left\{\frac{\eta}{1-\gamma}\left(\overline{A}^{(t)}(s,a^{ \prime})+\delta^{(t)}(s,a^{\prime})\right)\right\}\] \[\geq\sum_{a^{\prime}\in\mathcal{A}}\overline{\mathbf{\pi}}^{(t)}( a^{\prime}|s)\log\exp\left\{\frac{\eta}{1-\gamma}\left(\overline{A}^{(t)}(s,a^{ \prime})+\delta^{(t)}(s,a^{\prime})\right)\right\}\] \[=\frac{\eta}{1-\gamma}\sum_{a^{\prime}\in\mathcal{A}}\overline{ \mathbf{\pi}}^{(t)}(a^{\prime}|s)\left(\overline{A}^{(t)}(s,a^{\prime})+\delta ^{(t)}(s,a^{\prime})\right)\] \[=\frac{\eta}{1-\gamma}\sum_{a^{\prime}\in\mathcal{A}}\overline{ \mathbf{\pi}}^{(t)}(a^{\prime}|s)\delta^{(t)}(s,a^{\prime})\geq-\frac{\eta}{1- \gamma}\left\|\delta^{(t)}\right\|_{\infty}\,, \tag{263}\]

where the first inequality follows by Jensen's inequality on the concave function \(\log x\) and the last equality uses \(\sum_{a^{\prime}\in\mathcal{A}}\overline{\mathbf{\pi}}^{(t)}(a^{\prime}|s) \overline{A}^{(t)}(s,a^{\prime})=0\).

For all starting state distribution \(\mu\), we use \(d^{(t+1)}\) as shorthand for \(d^{\overline{\pi}^{(t+1)}}_{\mu}\), the performance difference lemma (Lemma G.1) implies:

\[\overline{V}^{(t+1)}(\mu)-\overline{V}^{(t)}(\mu)\] \[=\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{(t+1)}}\sum_{a\in\mathcal{ A}}\overline{\pi}^{(t+1)}(a|s)\left(\overline{A}^{(t)}(s,a)+\delta^{(t)}(s,a) \right)-\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{(t+1)}}\mathbb{E}_{a\sim \overline{\pi}^{(t+1)}(\cdot|s)}\left[\delta^{(t)}(s,a)\right]\] \[=\frac{1}{\eta}\mathbb{E}_{s\sim d^{(t+1)}}\sum_{a\in\mathcal{A}} \overline{\pi}^{(t+1)}(a|s)\log\frac{\overline{\pi}^{(t+1)}(a|s)\widehat{z}^{( t)}(s)}{\overline{\pi}^{(t)}(a|s)}-\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{(t+1)}} \mathbb{E}_{a\sim\overline{\pi}^{(t+1)}(\cdot|s)}\left[\delta^{(t)}(s,a)\right]\] \[=\frac{1}{\eta}\mathbb{E}_{s\sim d^{(t+1)}}\mathsf{KL}\big{(} \overline{\pi}^{(t+1)}(\cdot|s)\,\|\,\overline{\pi}^{(t)}(\cdot|s)\big{)}+ \frac{1}{\eta}\mathbb{E}_{s\sim d^{(t+1)}}\log\widehat{z}^{(t)}(s)-\frac{1}{1 -\gamma}\mathbb{E}_{s\sim d^{(t+1)}}\mathbb{E}_{a\sim\overline{\pi}^{(t+1)}( \cdot|s)}\left[\delta^{(t)}(s,a)\right]\] \[\geq\frac{1}{\eta}\mathbb{E}_{s\sim d^{(t+1)}}\left(\log\widehat{ z}^{(t)}(s)+\frac{\eta}{1-\gamma}\big{\|}\delta^{(t)}\big{\|}_{\infty}\right)-\frac{2 }{1-\gamma}\big{\|}\delta^{(t)}\big{\|}_{\infty}\,,\]

from which we can see that

\[\overline{V}^{(t+1)}(\mu)-\overline{V}^{(t)}(\mu)\geq-\frac{2}{1-\gamma}\big{\|} \delta^{(t)}\big{\|}_{\infty}\,, \tag{264}\]

where we use (263), and that

\[\overline{V}^{(t+1)}(\mu)-\overline{V}^{(t)}(\mu)\geq\frac{1-\gamma}{\eta} \mathbb{E}_{s\sim\mu}\left(\log\widehat{z}^{(t)}(s)+\frac{\eta}{1-\gamma} \big{\|}\delta^{(t)}\big{\|}_{\infty}\right)-\frac{2}{1-\gamma}\big{\|}\delta ^{(t)}\big{\|}_{\infty}\,, \tag{265}\]

which follows from \(d^{(t+1)}=d^{\overline{\pi}^{(t+1)}}_{\mu}\geq(1-\gamma)\mu\) and the fact that \(\log\widehat{z}^{(t)}(s)+\frac{\eta}{1-\gamma}\big{\|}\delta^{(t)}\big{\|}_{ \infty}\geq 0\) (by (263)).

For any fixed \(\rho\), we use \(d^{\star}\) as shorthand for \(d^{\pi^{\star}}_{\rho}\). By the performance difference lemma (Lemma G.1),

\[V^{\star}(\rho)-\overline{V}^{(t)}(\rho)\] \[=\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\star}}\sum_{a\in \mathcal{A}}\pi^{\star}(a|s)\left(\overline{A}^{(t)}(s,a)+\delta^{(t)}(s,a) \right)-\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\star}}\mathbb{E}_{a\sim\pi^{ \star}(\cdot|s)}\left[\delta^{(t)}(s,a)\right]\] \[=\frac{1}{\eta}\mathbb{E}_{s\sim d^{\star}}\sum_{a\in\mathcal{A }}\pi^{\star}(a|s)\log\frac{\overline{\pi}^{(t+1)}(a|s)\widehat{z}^{(t)}(s)}{ \overline{\pi}^{(t)}(a|s)}-\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\star}} \mathbb{E}_{a\sim\pi^{\star}(\cdot|s)}\left[\delta^{(t)}(s,a)\right]\] \[=\frac{1}{\eta}\mathbb{E}_{s\sim d^{\star}}\left(\mathsf{KL} \big{(}\pi^{\star}(\cdot|s)\,\|\,\overline{\pi}^{(t)}(\cdot|s)\big{)}-\mathsf{ KL}\big{(}\pi^{\star}(\cdot|s)\,\|\,\overline{\pi}^{(t+1)}(\cdot|s)\big{)}+\log \widehat{z}^{(t)}(s)\right)-\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\star}} \mathbb{E}_{a\sim\pi^{\star}(\cdot|s)}\left[\delta^{(t)}(s,a)\right]\] \[\leq\frac{1}{\eta}\mathbb{E}_{s\sim d^{\star}}\left(\mathsf{KL} \big{(}\pi^{\star}(\cdot|s)\,\|\,\overline{\pi}^{(t)}(\cdot|s)\big{)}-\mathsf{ KL}\big{(}\pi^{\star}(\cdot|s)\,\|\,\overline{\pi}^{(t+1)}(\cdot|s)\big{)}+\left( \log\widehat{z}^{(t)}(s)+\frac{\eta}{1-\gamma}\big{\|}\delta^{(t)}\big{\|}_{ \infty}\right)\right)\,, \tag{266}\]

where we use (262) in the second equality.

By applying (265) with \(\mu=d^{\star}\) as the initial state distribution, we have

\[\frac{1}{\eta}\mathbb{E}_{s\sim\mu}\Big{(}\log\widehat{z}^{(t)}(s)+\frac{\eta} {1-\gamma}\big{\|}\delta^{(t)}\big{\|}_{\infty}\Big{)}\leq\frac{1}{1-\gamma} \Big{(}\overline{V}^{(t+1)}(d^{\star})-\overline{V}^{(t)}(d^{\star})\Big{)}+ \frac{2}{(1-\gamma)^{2}}\big{\|}\delta^{(t)}\big{\|}_{\infty}\,.\]

Plugging the above equation into (266), we obtain

\[V^{\star}(\rho)-\overline{V}^{(t)}(\rho)\leq\frac{1}{\eta} \mathbb{E}_{s\sim d^{\star}}\left(\mathsf{KL}\big{(}\pi^{\star}( \cdot|s)\,\|\,\overline{\pi}^{(t)}(\cdot|s)\big{)}-\mathsf{KL}\big{(}\pi^{ \star}(\cdot|s)\,\|\,\overline{\pi}^{(t+1)}(\cdot|s)\big{)}\right)\] \[\qquad+\frac{1}{1-\gamma}\left(\overline{V}^{(t+1)}(d^{\star})- \overline{V}^{(t)}(d^{\star})\right)+\frac{2}{(1-\gamma)^{2}}\big{\|}\delta^{(t )}\big{\|}_{\infty}\,,\]

which gives Lemma F.6.

### Proof of Theorem e.3

The proof of Theorem E.3 could be found in Appendix C.5 in [19]. We present it for completeness. To prove Theorem E.3, we need the following Theorem G.2.

**Theorem G.2** (Theorem I in [13]).: _Consider the following assaumptions:_

1. _The observations_ \((\mathbf{a}_{k},\mathbf{b}_{k})\in\mathbb{R}^{p}\times\mathbb{R}^{p}\) _are independent and identically distributed._
2. \(\mathbb{E}\left[\left\|\mathbf{a}_{k}\right\|^{2}\right]^{8}\) _and_ \(\mathbb{E}\left[\left\|\mathbf{b}_{k}\right\|^{2}\right]\) _are finite. The covariance_ \(\mathbb{E}\left[\mathbf{a}_{k}\mathbf{a}_{k}^{\top}\right]\) _is invertible._
3. _The global minimum of_ \(g(w)=\frac{1}{2}\mathbb{E}\left[\langle\mathbf{w},\mathbf{a}_{k}\rangle^{2}-2\langle \mathbf{w},\mathbf{b}_{k}\rangle\right]\) _is attained at a certain_ \(\mathbf{w}^{\star}\in\mathbb{R}^{p}\)_. Let_ \(\Delta_{k}=\mathbf{b}_{k}-\langle\mathbf{w}^{\star},\mathbf{a}_{k}\rangle\mathbf{a}_{k}\) _denote the residual. We have_ \(\mathbb{E}[\Delta_{k}]=0\)_._
4. \(\exists R>0\) _and_ \(\sigma>0\) _such that_ \(\mathbb{E}\left[\Delta_{k}\Delta_{k}^{\top}\right]\leq\sigma^{2}\mathbb{E} \left[\mathbf{a}_{k}\mathbf{a}_{k}^{\top}\right]\) _and_ \(\mathbb{E}\left[\left\|\mathbf{a}_{k}\right\|^{2}\mathbf{a}_{k}\mathbf{a}_{k}^{\top} \right]\leq R^{2}\mathbb{E}\left[\mathbf{a}_{k}\mathbf{a}_{k}^{\top}\right]\)_._

_Consider the stochastic gradient recursion_

\[\mathbf{w}_{k+1}=\mathbf{w}_{k}-\eta\left(\langle\mathbf{w}_{k},\mathbf{a}_{k}\rangle\mathbf{a}_{k }-\mathbf{b}_{k}\right)\]

_started from \(\mathbf{w}_{0}\in\mathbb{R}^{p}\). Let \(\mathbf{w}_{\text{out}}=\frac{1}{K}\sum_{k=1}^{K}\mathbf{w}_{k}\). When \(\eta=\frac{1}{4R^{2}}\), we have_

\[\mathbb{E}\left[g(\mathbf{w}_{\text{out}})-g(\mathbf{w}^{\star})\right]\leq\frac{2}{K} \left(\sigma\sqrt{p}+R\left\|\mathbf{w}_{0}-\mathbf{w}^{\star}\right\|\right)^{2}. \tag{267}\]

In the proof of Theorem E.3 we'll show that for Algorithm 4, the assumptions in Theorem G.2 are all satisfied and thus we can use the result (267).

Proof of Theorem e.3.: We let \(\mathbf{a}_{k}\) and \(\mathbf{b}_{k}\) in Theorem G.2 be \(\phi(s,a)\) and \(\widehat{Q}_{\xi}\phi(s,a)\) in Algorithm 4, respectively. And we let \(\left\|\cdot\right\|=\left\|\cdot\right\|_{2}\) in Theorem G.2. Since the observations \(\left(\phi(s,a),\widehat{Q}_{\xi}(s,a)\phi(s,a)\right)\in\mathbb{R}^{p}\times \mathbb{R}^{p}\) are i.i.d., (i) is satisfied.

As we assume \(\left\|\phi(s,a)\right\|_{2}\leq C_{\phi}\), \(\mathbb{E}\left[\left\|\phi(s,a)\right\|_{2}^{2}\right]\) is finite. From Assumption 4.1 we know that \(\mathbb{E}\left[\phi(s,a)\phi(s,a)^{\top}\right]\) is invertible.

Let \(H\) be the length of trajectory for estimating \(\widehat{Q}_{\xi}(s,a)\). Then \(\left(\widehat{Q}_{\xi}(s,a)\right)^{2}\) is bounded by

\[\mathbb{E}\left[\left(\widehat{Q}_{\xi}(s,a)\right)^{2}\right] =\mathbb{E}_{(s,a)\sim d_{v}^{\pi_{\xi}}}\left[\sum_{\tau=0}^{ \infty}Pr(H=\tau)\mathbb{E}\left[\left(\sum_{t=0}^{\tau}r(s_{t},a_{t})\right) ^{2}\left|H=\tau,s_{0}=s,a_{0}=a\right]\right]\] \[=\mathbb{E}_{(s,a)\sim d_{v}^{\pi_{\xi}}}\left[(1-\gamma)\sum_{ \tau=0}^{\infty}\gamma^{\tau}\mathbb{E}\left[\left(\sum_{t=0}^{\tau}r(s_{t},a _{t})\right)^{2}\left|H=\tau,s_{0}=s,a_{0}=a\right]\right]\] \[\leq\mathbb{E}_{(s,a)\sim d_{v}^{\pi_{\xi}}}\left[(1-\gamma)\sum_ {\tau=0}^{\infty}\gamma^{\tau}(\tau+1)^{2}\right]\leq\frac{2}{(1-\gamma)^{2}}\,, \tag{268}\]

from which we deduce \(\mathbb{E}\left[\left\|\widehat{Q}_{\xi}(s,a)\phi(s,a)\right\|_{2}^{2}\right] \leq C_{\phi}^{2}\mathbb{E}\left[\widehat{Q}_{\xi}(s,a)^{2}\right]\) is bounded. Thus (ii) holds.

Furthermore, we introduce the residual

\[\Delta\coloneqq\left(\widehat{Q}_{\xi}(s,a)-\phi(s,a)^{\top}w^{\star}\right) \phi(s,a)\,, \tag{269}\]

then from [13, Lemma 7] we know that \(\mathbb{E}[\Delta]=\frac{1}{2}\nabla_{w}\ell(\mathbf{w}^{\star},\widehat{Q}_{\xi}, d_{v}^{\pi_{\xi}})=0\), which gives (iii).

To verify (iv), we let \(R=C_{\phi}\) in Theorem G.2, then \(\mathbb{E}\left[\left\|\phi(s,a)\right\|_{2}^{2}\phi(s,a)\phi(s,a)^{\top} \right]\leq C_{\phi}^{2}\mathbb{E}\left[\phi(s,a)\phi(s,a)^{\top}\right]\). Also note that

\[\mathbf{w}^{\star} =\left(\mathbb{E}_{(s,a)\sim\widetilde{d}_{v}^{\pi_{\xi}}}\left[ \phi(s,a)\phi(s,a)^{\top}\right]\right)^{\dagger}\mathbb{E}_{(s,a)\sim \widetilde{d}_{v}^{\pi_{\xi}}}\left[\widehat{Q}_{\xi}(s,a)\phi(s,a)\right]\] \[\leq\frac{1}{1-\gamma}\left(\mathbb{E}_{(s,a)\sim\nu}\left[ \phi(s,a)\phi(s,a)^{\top}\right]\right)^{\dagger}\mathbb{E}_{(s,a)\sim \widetilde{d}_{v}^{\pi_{\xi}}}\left[\widehat{Q}_{\xi}(s,a)\phi(s,a)\right]\,, \tag{270}\]from which we deduce

\[\left\|\mathbf{w}^{\star}\right\|_{2}\leq\frac{B}{\mu(1-\gamma)^{2}}\,. \tag{271}\]

\[\mathbb{E}\left[\left(\widehat{Q}_{\xi}(s,a)-\phi(s,a)^{\top}\mathbf{w ^{\star}}\right)^{2}|s,a\right] =\mathbb{E}\left[\left(\widehat{Q}_{\xi}(s,a)\right)^{2}|s,a\right] -2Q_{\xi}(s,a)\phi(s,a)^{\top}\mathbf{w}^{\star}+(\phi(s,a)^{\top}w^{ \star})^{2} \tag{272}\] \[\leq\frac{2}{(1-\gamma)^{2}}+\frac{2C_{\phi}^{2}}{\mu(1-\gamma)^{3 }}+\frac{C_{\phi}^{4}}{\mu^{2}(1-\gamma)^{4}}\] \[\leq\frac{2}{(1-\gamma)^{2}}\left(\frac{C_{\phi}^{2}}{\mu(1-\gamma )}+1\right)^{2}\,. \tag{273}\]

The above expression implies

\[\mathbb{E}\left[\Delta\Delta^{\top}\right] =\mathbb{E}_{(s,a)\sim\widetilde{d}_{\xi}^{\tau}\xi}\left[\left( \widehat{Q}_{\xi}(s,a)-\phi(s,a)^{\top}\mathbf{w^{\star}}\right)^{2}\phi(s,a)\phi (s,a)^{\top}\big{|}s,a\right]\] \[=\mathbb{E}_{(s,a)\sim\widetilde{d}_{\xi}^{\tau}\xi}\left[\mathbb{ E}\left[\left(\widehat{Q}_{\xi}(s,a)-\phi(s,a)^{\top}\mathbf{w^{\star}}\right)^{2} \big{|}s,a\right]\phi(s,a)\phi(s,a)^{\top}\right]\] \[\leq\left(\underbrace{\frac{\sqrt{2}}{1-\gamma}\left(\frac{C_{ \phi}^{2}}{\mu(1-\gamma)}+1\right)}_{\sigma}\right)\mathbb{E}[\phi(s,a)\phi(s,a )^{\top}]\,. \tag{274}\]

Therefore, (iv) is verified.

Thus by (267), with stepsize \(\beta=\frac{1}{2C_{\phi}^{2}}\), initialization \(\mathbf{w}_{0}=\mathbf{0}\) and \(K\) steps of critic updates, we have

\[\mathbb{E}\left[\ell\left(\mathbf{w}_{\text{out}},Q_{\xi},\tilde{d}_ {\xi}\right)\right]-\ell\left(\mathbf{w^{\star}},Q_{\xi},\tilde{d}_{\xi}\right) \leq\frac{4}{K}\left(\sigma\sqrt{p}+C_{\phi}\left\|\mathbf{w^{\star} }\right\|_{2}\right)^{2}\] \[\leq\frac{4}{K}\left(\frac{\sqrt{2p}}{1-\gamma}\left(\frac{C_{ \phi}^{2}}{\mu(1-\gamma)}+1\right)+\frac{C_{\phi}^{2}}{\mu(1-\gamma)^{2}} \right)^{2},\]

which gives (113). 

### Proof of Lemma e.7

Proof of Lemma e.7.: For notational simplicity we let \(V^{\xi},V^{\xi^{\prime}}\) denote \(V^{f_{\xi}},V^{f_{\xi^{\prime}}}\), resp. Same as in Lemma F.3, We define \(\xi^{(t)}=\xi+t(\xi^{\prime}-\xi)\) and define \(\mathbf{P}_{t},\mathbf{M}_{t},r_{t}\) by replacing \(\pi_{\xi^{(t)}}\) with \(f_{\xi^{(t)}}\) in (243),(242) and (244), respectively. Define

\[\bar{\phi}_{\xi}(s,a)=\phi(s,a)-\mathbb{E}_{a^{\prime}\sim f_{\xi^{(t)}}}[\phi (s,a^{\prime})],\]

then we have

\[\frac{\partial f_{\xi}(a|s)}{\partial\xi}=f_{\xi}(a|s)\bar{\phi}_{\xi}(s,a)\,. \tag{275}\]

Analogous to (252), we have

\[\left\|\frac{d\mathbf{P}_{t}}{dt}x\right\|_{\infty} \leq\max_{s}\sum_{a\in\mathcal{A}}\sum_{s^{\prime}\in\mathcal{S} }\mathcal{P}(s^{\prime}|s,a)\left|\frac{d\pi_{\xi^{(t)}}(a|s)}{dt}\right\|\left \|x\right\|_{\infty}\] \[=\max_{s}\sum_{a\in\mathcal{A}}\left|\frac{d\pi_{\xi^{(t)}}(a|s) }{dt}\right\|\left\|x\right\|_{\infty}\] \[\leq 2C_{\phi}\left\|\xi^{\prime}-\xi\right\|_{2}\left\|x\right\| _{\infty}\]where the last line follows is due to

\[\sum_{a\in\mathcal{A}}\left|\frac{df_{\xi^{(t)}}(a|s)}{dt}\right| =\sum_{a\in\mathcal{A}}\left|\left\langle\frac{\partial f_{\xi^{(t) }}(a|s)}{\partial\mathbf{\xi}^{(t)}},\mathbf{\xi}^{\prime}-\mathbf{\xi}\right\rangle\right|\] \[=\sum_{a\in\mathcal{A}}f_{\xi^{(t)}}(a|s)\left|\left\langle\bar{ \phi}_{\xi}(s,a),\mathbf{\xi}^{\prime}-\mathbf{\xi}\right\rangle\right|\] \[\leq\sum_{a\in\mathcal{A}}f_{\xi^{(t)}}(a|s)\left\|\bar{\phi}_{ \xi}(s,a)\right\|_{2}\left\|\mathbf{\xi}^{\prime}-\mathbf{\xi}\right\|_{2}\] \[\leq 2C_{\phi}\left\|\mathbf{\xi}^{\prime}-\mathbf{\xi}\right\|_{\infty}\,.\]

Same as (245) in Lemma F.3, we have

\[\frac{dV^{\xi^{(t)}}(s)}{dt}=\gamma\cdot e_{s}^{\top}\mathbf{M}_{t}\frac{d\mathbf{P}_ {t}}{dt}\mathbf{M}_{t}r_{t}+e_{s}^{\top}\mathbf{M}_{t}\frac{dr_{t}}{dt}\,. \tag{276}\]

And similar to (249), we deduce

\[\left\|\frac{dr_{t}}{dt}\right\|_{\infty}=\max_{s\in\mathcal{S}} \left|\frac{dr_{t}(s)}{dt}\right| =\max_{s\in\mathcal{S}}\left|\left\langle\frac{\partial f_{\xi^{( t)}}(\cdot|s)^{\top}r(s,\cdot)}{\partial\mathbf{\xi}^{(t)}},\mathbf{\xi}^{\prime}- \mathbf{\xi}\right\rangle\right|\] \[=\left|\left\langle\sum_{a\in\mathcal{A}}f_{\xi}(a|s)\bar{\phi}_ {\xi}(s,a)r(s,a),\mathbf{\xi}^{\prime}-\mathbf{\xi}\right\rangle\right|\] \[=\sum_{a\in\mathcal{A}}f_{\xi}(a|s)r(s,a)\left|\left\langle\bar{ \phi}_{\xi}(s,a),\mathbf{\xi}^{\prime}-\mathbf{\xi}\right\rangle\right|\] \[\leq 2C_{\phi}\left\|\mathbf{\xi}^{\prime}-\mathbf{\xi}\right\|_{2}\,,\]

which gives

\[\left|e_{s}^{\top}\mathbf{M}_{t}\frac{dr_{t}}{dt}\right|\leq\frac{1}{1-\gamma} \left\|\frac{dr_{t}}{dt}\right\|_{\infty}\leq\frac{2C_{\phi}}{1-\gamma}\left\| \mathbf{\xi}^{\prime}-\mathbf{\xi}\right\|_{2}\,. \tag{277}\]

Following the same steps in (247), we deduce

\[\left|\gamma\cdot e_{s}^{\top}\mathbf{M}_{t}\frac{d\mathbf{P}_{t}}{dt}\mathbf{M}_{t}r_{t} \right|\leq\frac{2\gamma C_{\phi}}{(1-\gamma)^{2}}\left\|\mathbf{\xi}^{\prime}-\bm {\xi}\right\|_{2}\,. \tag{278}\]

Combining the above two expressions (277) and (278) with (276), we deduce

\[\left|V^{\xi}(s)-V^{\xi^{\prime}}(s)\right|\leq\frac{2C_{\phi}(1+\gamma)}{(1- \gamma)^{2}}\left\|\mathbf{\xi}^{\prime}-\mathbf{\xi}\right\|_{2}\,, \tag{279}\]

which implies

\[\forall(s,a)\in\mathcal{S}\times\mathcal{A}:\quad\left|Q^{\xi}(s,a)-Q^{\xi^{ \prime}}(s,a)\right|\leq\frac{2C_{\phi}\gamma(1+\gamma)}{(1-\gamma)^{2}}\left \|\mathbf{\xi}^{\prime}-\mathbf{\xi}\right\|_{2}\,. \tag{280}\]

### Proof of Lemma e.8

This proof is inspired by the proof of [35, Theorem 1]. To give the proof, we first introduce the following three-point descent lemma:

**Lemma G.3** (Three-point descent lemma Lemma 6 in [29]).: _Suppose that \(\mathcal{C}\subset\mathbb{R}^{m}\) is a closed convex set, \(g:\mathcal{C}\to\mathbb{R}\) is a proper, closed, convex function, \(D_{h}(\cdot,\cdot)\) is the Bregman divergence generated by a function \(h\) of Lengendre type and rint \(\text{dom}h\cap\mathcal{C}\neq\emptyset\). For any \(x\in\text{rintdom}h\), let_

\[x^{+}\in\arg\min_{u\in\text{dom}h\cap\mathcal{C}}\{f(u)+D_{h}(u,x)\}\,,\]

_then \(x^{+}\in\text{dom}h\cap\mathcal{C}\) and for any \(u\in\text{dom}h\cap\mathcal{C}\), it holds that_

\[f(x^{+})+D_{h}(x^{+},x)\leq f(u)+D_{h}(u,x)-D_{h}(u,x^{+})\,. \tag{281}\]Proof of Lemma e.8.: By the update rule (114) and the parameterization (24) we know know that

\[\forall(s,a)\in\mathcal{S}\times\mathcal{A}:\quad\bar{f}^{(t+1)}(a|s)=\frac{1}{Z^{ (t)}(s)}f^{(t)}(a|s)\exp\left(\alpha\phi^{\top}(s,a)\hat{\mathbf{w}}^{(t)}\right),\]

where \(Z^{(t)}(s)\) is a normalization coefficient to ensure \(\sum_{a\in\mathcal{A}}f^{(t+1)}(s,a)=1\) for each \(s\in\mathcal{S}\). Note that the above \(\pi^{(t+1)}\) could also be obtained by a mirror descent update:

\[\forall s\in\mathcal{S}:\quad f^{(t+1)}(\cdot|s)=\arg\min_{g\in\Delta(\mathcal{ A})}\left\{-\alpha\langle\Phi(s)\hat{\mathbf{w}}^{(t)},g\rangle+D(g,f^{(t)}(\cdot|s) )\right\}\,, \tag{282}\]

where \(\Phi(s)\in\mathbb{R}^{|\mathcal{A}|\times p}\) is a matrix with rows \(\phi^{\top}(s,a)\in\mathbb{R}^{p}\) for \(a\in\mathcal{A}\), and \(D(\cdot,\cdot)\) denotes the KL divergence defined in (109).

We apply the three-point descent lemma--Lemma G.3 with \(\mathcal{C}=\Delta(\mathcal{A})\), \(f=-\alpha\langle\Phi(s)\hat{\mathbf{w}}^{(t)},\cdot\rangle\) and \(h:\Delta(\mathcal{A})\rightarrow\mathbb{R}\) is the negative entropy with \(h(q)=\sum_{a\in\mathcal{A}}q(a)\log q(a)\) and deduce that for any \(q\in\Delta(\mathcal{A})\), we have

\[-\alpha\langle\Phi(s)\hat{\mathbf{w}}^{(t)},\bar{f}^{(t+1)}(\cdot|s)\rangle+D\left( \bar{f}^{(t+1)}(\cdot|s),\bar{f}^{(t)}(\cdot|s)\right)\leq-\alpha\langle\Phi(s) \hat{\mathbf{w}}^{(t)},q\rangle+D\left(q,\bar{f}^{(t)}(\cdot|s)\right)-D\left(q, \bar{f}^{(t+1)}(\cdot|s)\right)\,.\]

Rearranging terms and dividing both sides by \(-\alpha\), we obtain

\[\langle\Phi(s)\hat{\mathbf{w}}^{(t)},\bar{f}^{(t+1)}(\cdot|s)-q\rangle-\frac{1}{ \alpha}D\left(\bar{f}^{(t+1)}(\cdot|s),\bar{f}^{(t)}(\cdot|s)\right)\geq-\frac {1}{\alpha}D\left(q,\bar{f}^{(t)}(\cdot|s)\right)+\frac{1}{\alpha}D\left(q,\bar {f}^{(t+1)}(\cdot|s)\right)\,. \tag{283}\]

Let \(q=\bar{f}^{(t)}(\cdot|s)\) and \(\pi^{\star}(\cdot|s)\),resp., we have the following two inequalities:

\[\langle\Phi(s)\hat{\mathbf{w}}^{(t)},\bar{f}^{(t+1)}(\cdot|s)-\bar{f}^{(t)}(\cdot| s)\rangle\geq\frac{1}{\alpha}D\left(\bar{f}^{(t+1)}(\cdot|s),\bar{f}^{(t)}( \cdot|s)\right)+\frac{1}{\alpha}D\left(\bar{f}^{(t)}(\cdot|s),\bar{f}^{(t+1)}( \cdot|s)\right)\geq 0\,. \tag{284}\]

\[\langle\Phi(s)\hat{\mathbf{w}}^{(t)},\bar{f}^{(t+1)}(\cdot|s)-\bar{f}^{(t)}(\cdot |s)\rangle+\langle\Phi(s)\hat{\mathbf{w}}^{(t)},\bar{f}^{(t)}(\cdot|s)-\pi^{\star} (\cdot|s)\rangle\] \[\geq-\frac{1}{\alpha}D\left(\pi^{\star}(\cdot|s),\bar{f}^{(t)}( \cdot|s)\right)+\frac{1}{\alpha}D\left(\pi^{\star}(\cdot|s),\bar{f}^{(t+1)}( \cdot|s)\right)\,. \tag{285}\]

Taking expectation w.r.t. distribution \(d^{\star}\) on both sides of (285), we arrive at

\[\mathbb{E}_{s\sim d^{\star}}\left[\langle\Phi(s)\hat{\mathbf{w}}^{(t)},\bar{f}^{(t+ 1)}(\cdot|s)-\bar{f}^{(t)}(\cdot|s)\rangle\right]+\mathbb{E}_{s\sim d^{\star}} \left[\langle\Phi(s)\hat{\mathbf{w}}^{(t)},\bar{f}^{(t)}(\cdot|s)-\pi^{\star}(\cdot |s)\rangle\right]\geq\frac{1}{\alpha}(D_{\star}^{(t+1)}-D_{\star}^{(t)})\,. \tag{286}\]

To simplify the notation we let \(\bar{Q}^{(t)}\) and \(\bar{V}^{(t)}\) denote \(Q^{\bar{f}^{(t)}}\) and \(V^{\bar{f}^{(t)}}\), respectively. Note that the first expectation in the above expression (286) could be upper bounded as follows:

\[\mathbb{E}_{s\sim d^{\star}}\left[\langle\Phi(s)\hat{\mathbf{w}}^{(t)},\bar{f}^{(t+1)}(\cdot|s)-\bar{f}^{(t)}(\cdot|s)\rangle\right]\] \[=\sum_{s\in\mathcal{S}}d^{\star}(s)\langle\Phi(s)\hat{\mathbf{w}}^{(t )},\bar{f}^{(t+1)}(\cdot|s)-\bar{f}^{(t)}(\cdot|s)\rangle\] \[=\sum_{s\in\mathcal{S}}\frac{d^{\star}(s)}{d^{\bar{f}^{(t+1)}}(s)}d ^{\bar{f}^{(k+1)}}(s)\langle\Phi(s)\hat{\mathbf{w}}^{(t)},\bar{f}^{(t+1)}(\cdot|s) -\bar{f}^{(t)}(\cdot|s)\rangle\] \[\leq\vartheta_{\rho}\sum_{s\in\mathcal{S}}d^{\bar{f}^{(k+1)}}(s) \langle\Phi(s)\hat{\mathbf{w}}^{(t)},\bar{f}^{(t+1)}(\cdot|s)-\bar{f}^{(t)}(\cdot|s)\rangle\] \[=\vartheta_{\rho}\sum_{s\in\mathcal{S}}d^{\bar{f}^{(k+1)}}(s) \langle\bar{Q}^{(t)}(s,\cdot),\bar{f}^{(t+1)}(\cdot|s)-\bar{f}^{(t)}(\cdot|s) \rangle+\vartheta_{\rho}\sum_{s\in\mathcal{S}}d^{\bar{f}^{(k+1)}}(s)\langle \bar{\Phi}(s)\hat{\mathbf{w}}^{(t)}-\bar{Q}^{(t)}(s,\cdot),\bar{f}^{(t+1)}(\cdot|s )-\bar{f}^{(t)}(\cdot|s)\rangle\] \[=\vartheta_{\rho}(1-\gamma)\left(\bar{V}^{(t+1)}(\rho)-\bar{V}^{(t )}(\rho)\right)+\vartheta_{\rho}\sum_{s\in\mathcal{S}}d^{\bar{f}^{(k+1)}}(s) \langle\bar{\Phi}(s)\hat{\mathbf{w}}^{(t)}-\bar{Q}^{(t)}(s,\cdot),\bar{f}^{(t+1)} (\cdot|s)-\bar{f}^{(t)}(\cdot|s)\rangle\,, \tag{287}\]where the first inequality uses (27) and the definition of \(\vartheta_{\rho}\) (107) and the last line follows from (260) in Lemma G.1. We separate the second term of the last line into four terms as follows:

\[\begin{split}&\sum_{s\in\mathcal{S}}d^{\bar{f}^{(t+1)}}(s)\langle \bar{\Phi}(s)\hat{\boldsymbol{w}}^{(t)}-\bar{Q}^{(t)}(s,\cdot),\bar{f}^{(t+1)}( \cdot|s)-\bar{f}^{(t)}(\cdot|s)\rangle\\ &=\underbrace{\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}}d^{\bar{ f}^{(t+1)}}(s)\bar{f}^{(t+1)}(a|s)\phi^{\top}(s,a)(\hat{\boldsymbol{w}}^{(t)}- \hat{\boldsymbol{w}}_{\star}^{(t)})}_{(I)}+\underbrace{\sum_{s\in\mathcal{S}} \sum_{a\in\mathcal{A}}d^{\bar{f}^{(t+1)}}(s)\bar{f}^{(t+1)}(a|s)\left(\phi^{ \top}(s,a)\hat{\boldsymbol{w}}_{\star}^{(t)}-\bar{Q}^{(t)}(s,a)\right)}_{(II)} \\ &\quad+\underbrace{\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}}d^{ \bar{f}^{(t+1)}}(s)\bar{f}^{(t)}(a|s)\phi^{\top}(s,a)(\hat{\boldsymbol{w}}_{ \star}^{(t)}-\hat{\boldsymbol{w}}^{(t)})}_{(III)}+\underbrace{\sum_{s\in \mathcal{S}}\sum_{a\in\mathcal{A}}d^{\bar{f}^{(t+1)}}(s)\bar{f}^{(t)}(a|s)\left( \bar{Q}^{(t)}(s,a)-\phi^{\top}(s,a)\hat{\boldsymbol{w}}_{\star}^{(t)}\right)}_{ (IV)}\.\end{split} \tag{288}\]

Applying again Lemma G.1, we deduce the equivalent form of the second expectation in (286) as follows:

\[\begin{split}&\mathbb{E}_{s\sim d^{*}}\left[\langle\Phi(s)\hat{ \boldsymbol{w}}^{(t)},\bar{f}^{(t)}(\cdot|s)-\pi^{\star}(\cdot|s)\rangle\right] \\ &=\mathbb{E}_{s\sim d^{*}}\left[\langle\bar{Q}^{(t)}(s,\cdot),\bar{ f}^{(t)}(\cdot|s)-\pi^{\star}(\cdot|s)\rangle\right]+\mathbb{E}_{s\sim d^{*}} \left[\langle\Phi(s)\hat{\boldsymbol{w}}^{(t)}-\bar{Q}^{(t)}(s,\cdot),\bar{f}^ {(t)}(\cdot|s)-\pi^{\star}(\cdot|s)\rangle\right]\\ &=(1-\gamma)\left(\bar{V}^{(t)}(\rho)-V^{\pi^{*}}(\rho)\right)+ \mathbb{E}_{s\sim d^{*}}\left[\langle\Phi(s)\hat{\boldsymbol{w}}^{(t)}-\bar{Q} ^{(t)}(s,\cdot),\bar{f}^{(t)}(\cdot|s)-\pi^{\star}(\cdot|s)\rangle\right]\,, \end{split} \tag{289}\]

where the second term of the last line could be decomposed into the following terms:

\[\begin{split}&\mathbb{E}_{s\sim d^{*}}\left[\langle\Phi(s)\hat{ \boldsymbol{w}}^{(t)}-\bar{Q}^{(t)}(s,\cdot),\bar{f}^{(t)}(\cdot|s)-\pi^{ \star}(\cdot|s)\rangle\right]\\ &=\underbrace{\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}}d^{ \star}(s)\bar{f}^{(t)}(a|s)\phi^{\top}(s,a)(\hat{\boldsymbol{w}}^{(t)}-\hat{ \boldsymbol{w}}_{\star}^{(t)})}_{(A)}+\underbrace{\sum_{s\in\mathcal{S}}\sum_{a \in\mathcal{A}}d^{\star}(s)\bar{f}^{(t)}(a|s)\left(\phi^{\top}(s,a)\hat{ \boldsymbol{w}}_{\star}^{(t)}-\bar{Q}^{(t)}(s,a)\right)}_{(B)}\\ &\quad+\underbrace{\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}}d^ {\star}(s)\pi^{\star}(a|s)\phi^{\top}(s,a)(\hat{\boldsymbol{w}}_{\star}^{(t)}- \hat{\boldsymbol{w}}^{(t)})}_{(C)}+\underbrace{\sum_{s\in\mathcal{S}}\sum_{a \in\mathcal{A}}d^{\star}(s)\pi^{\star}(a|s)\left(\bar{Q}^{(t)}(s,a)-\phi^{ \top}(s,a)\hat{\boldsymbol{w}}_{\star}^{(t)}\right)}_{(D)}\.\end{split} \tag{290}\]

Plugging (288), (290) into (287) and (289), resp., and making use of (286), we have

\[\begin{split}&\vartheta_{\rho}(1-\gamma)\left(\bar{V}^{(t+1)}(\rho)- \bar{V}^{(t)}(\rho)\right)+(1-\gamma)\left(\bar{V}^{(t)}(\rho)-V^{\pi^{*}}( \rho)\right)\\ &+\vartheta_{\rho}\bigg{(}\underbrace{\sum_{s\in\mathcal{S}} \sum_{a\in\mathcal{A}}d^{\bar{f}^{(t+1)}}(s)\bar{f}^{(t+1)}(a|s)\phi^{\top}(s,a )(\hat{\boldsymbol{w}}^{(t)}-\hat{\boldsymbol{w}}_{\star}^{(t)})}_{(I)}+ \underbrace{\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}}d^{\bar{f}^{(t+1)}}( s)\bar{f}^{(t+1)}(a|s)\left(\phi^{\top}(s,a)\hat{\boldsymbol{w}}_{\star}^{(t)}-\bar{Q}^{(t)}(s,a) \right)}_{(II)}\\ &+\underbrace{\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}}d^{ \bar{f}^{(t+1)}}(s)\bar{f}^{(t)}(a|s)\phi^{\top}(s,a)(\hat{\boldsymbol{w}}^{(t)} -\hat{\boldsymbol{w}}_{\star}^{(t)})}_{(II)}+\underbrace{\sum_{s\in\mathcal{S}} \sum_{a\in\mathcal{A}}d^{\star}(s)\bar{f}^{(t)}(a|s)\left(\bar{Q}^{(t)}(s,a)- \phi^{\top}(s,a)\hat{\boldsymbol{w}}_{\star}^{(t)}\right)}_{(IV)}\\ &+\underbrace{\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}}d^{ \star}(s)\bar{f}^{(t)}(a|s)\phi^{\top}(s,a)(\hat{\boldsymbol{w}}^{(t)}-\hat{ \boldsymbol{w}}_{\star}^{(t)})}_{(A)}+\underbrace{\sum_{s\in\mathcal{S}}\sum_{a \in\mathcal{A}}d^{\star}(s)\bar{f}^{(t)}(a|s)\left(\phi^{\top}(s,a)\hat{ \boldsymbol{w}}_{\star}^{(t)}-\bar{Q}^{(t)}(s,a)\right)}_{(B)}\\ &+\underbrace{\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}}d^{ \star}(s)\pi^{\star}(a|s)\phi^{\top}(s,a)(\hat{\boldsymbol{w}}_{\star}^{(t)}- \hat{\boldsymbol{w}}^{(t)})}_{(C)}+\underbrace{\sum_{s\in\mathcal{S}}\sum_{a \in\mathcal{A}}d^{\star}(s)\pi^{\star}(a|s)\left(\bar{Q}^{(t)}(s,a)-\phi^{ \top}(s,a)\hat{\boldsymbol{w}}_{\star}^{(t)}\right)}_{(D)}\\ &\geq\frac{1}{\alpha}(D_{\star}^{(t+1)}-D_{\star}^{(t)}).\end{split} \tag{291}\]Below we upper bound \(|(I)|\)-\(|(IV)|\) and \(|(A)|\)-\(|(D)|\).

For any \(t\in\mathbb{N}\) and \(n\in[N]\), we define matrix \(\Sigma_{\tilde{d}_{n}^{(t)}}\in\mathbb{R}^{p\times p}\) as

\[\Sigma_{\tilde{d}_{n}^{(t)}}\coloneqq\mathbb{E}_{(s,a)\sim\tilde{d}_{n}^{(t)}} \left[\phi(s,a)\phi^{\top}(s,a)\right]\,, \tag{292}\]

and we define

\[\varepsilon_{\text{stat},n}^{(t)} \coloneqq\ell\left(\mathbf{w}_{n}^{(t)},Q_{n}^{(t)},\tilde{d}_{n}^{( t)}\right)-\ell\left(\mathbf{w}_{\star,n}^{(t)},Q_{n}^{(t)},\tilde{d}_{n}^{(t)} \right)\,, \tag{293}\] \[\varepsilon_{\text{approx},n}^{(t)} \coloneqq\ell\left(\mathbf{w}_{\star,n}^{(t)},Q_{n}^{(t)},\tilde{d}_{ n}^{(t)}\right)\,, \tag{294}\]

then for all \(n\in[N]\), by Assumption E.2 and Assumption 4.2 we have

\[\mathbb{E}\left[\varepsilon_{\text{stat},n}^{(t)}\right]\leq\varepsilon_{ \text{stat}}^{n}\,,\quad\text{and}\quad\mathbb{E}\left[\varepsilon_{\text{ approx},n}^{(t)}\right]\leq\varepsilon_{\text{approx}}^{n}\,. \tag{295}\]

We let \(\tilde{\varepsilon}_{\text{stat}}^{(t)}\coloneqq\frac{1}{N}\sum_{n=1}^{N} \varepsilon_{\text{stat},n}^{(t)}\) and \(\tilde{\varepsilon}_{\text{approx}}^{(t)}\coloneqq\frac{1}{N}\sum_{n=1}^{N} \varepsilon_{\text{approx},n}^{(t)}\). By Cauchy-Schwartz's inequality we have

\[|(I)| \leq\frac{1}{N}\sum_{n=1}^{N}\sum_{(s,a)\in\mathcal{S}\times \mathcal{A}}d^{\tilde{f}^{(t+1)}}(s)\bar{f}^{(t+1)}(a|s)|\phi^{\top}(s,a)(\mathbf{ w}_{n}^{(t)}-\mathbf{w}_{\star,n}^{(t)})|\] \[\leq\frac{1}{N}\sum_{n=1}^{N}\sqrt{\sum_{(s,a)\in\mathcal{S} \times\mathcal{A}}\frac{\left(d^{\tilde{f}^{(t+1)}}(s)\right)^{2}\left(\bar{f }^{(t+1)}(a|s)\right)^{2}}{\tilde{d}_{n}^{(t)}(s,a)}\cdot\sum_{(s,a)\in \mathcal{S}\times\mathcal{A}}\tilde{d}_{n}^{(t)}(s,a)\left(\phi^{\top}(s,a)(\bm {w}_{n}^{(t)}-\mathbf{w}_{\star,n}^{(t)})\right)^{2}}\] \[=\frac{1}{N}\sum_{n=1}^{N}\sqrt{\mathbb{E}_{(s,a)\sim\tilde{d}_{n }^{(t)}}\left[\left(\frac{\left(d^{\tilde{f}^{(t+1)}}(s)\right)\left(\bar{f}^{ (t+1)}(a|s)\right)}{\tilde{d}_{n}^{(t)}(s,a)}\right)^{2}\right]\left\|\mathbf{w}_{ n}^{(t)}-\mathbf{w}_{\star,n}^{(t)}\right\|_{\Sigma_{\tilde{d}_{n}^{(t)}}}^{2}}\] \[\leq\frac{1}{N}\sum_{n=1}^{N}\sqrt{C_{\nu}\left\|\mathbf{w}_{n}^{(t) }-\mathbf{w}_{\star,n}^{(t)}\right\|_{\Sigma_{\tilde{d}_{n}^{(t)}}}^{2}}\] \[\leq\frac{1}{N}\sum_{n=1}^{N}\sqrt{C_{\nu}\varepsilon_{\text{stat },n}^{(t)}}\leq\sqrt{C_{\nu}\varepsilon_{\text{stat}}^{(t)}}\,, \tag{296}\]

where the third inequality follows from Assumption 4.3, the last inequality uses Jensen's inequality, and the penultimate inequality by Assumption E.2 and by noticing that for all \(\mathbf{w}\in\mathbb{R}^{p}\), we have

\[\ell\left(\mathbf{w},Q_{n}^{(t)},\tilde{d}_{n}^{(t)}\right)-\ell\left( \mathbf{w}_{\star,n}^{(t)},Q_{n}^{(t)},\tilde{d}_{n}^{(t)}\right)\] \[=\mathbb{E}_{(s,a)\sim\tilde{d}_{n}^{(t)}}\left[\left(\phi^{\top} (s,a)\mathbf{w}-\phi^{\top}(s,a)\mathbf{w}_{\star,n}^{(t)}+\phi^{\top}(s,a)\mathbf{w}_{ \star,n}^{(t)}-Q_{n}^{(t)}(s,a)\right)^{2}\right]-\ell\left(\mathbf{w}_{\star,n}^{ (t)},Q_{n}^{(t)},\tilde{d}_{n}^{(t)}\right)\] \[=\mathbb{E}_{(s,a)\sim\tilde{d}_{n}^{(t)}}\left[\left(\phi^{\top} (s,a)\mathbf{w}-\phi^{\top}(s,a)\mathbf{w}_{\star,n}^{(t)}\right)^{2}\right]+2\left( \mathbf{w}-\mathbf{w}_{\star,n}^{(t)}\right)^{\top}\mathbb{E}_{(s,a)\sim\tilde{d}_{n}^{( t)}}\left[\left(\phi^{\top}(s,a)\mathbf{w}_{\star,n}^{(t)}-Q_{n}^{(t)}(s,a) \right)\phi(s,a)\right]\] \[=\left\|\mathbf{w}-\mathbf{w}_{\star,n}^{(t)}\right\|_{\Sigma_{\tilde{d} _{n}^{(t)}}}+\left(\mathbf{w}-\mathbf{w}_{\star,n}^{(t)}\right)^{\top}\nabla_{w}\ell \left(\mathbf{w}_{\star,n}^{(t)},Q_{n}^{(t)},\tilde{d}_{n}^{(t)}\right)\] \[\geq\left\|\mathbf{w}-\mathbf{w}_{\star,n}^{(t)}\right\|_{\Sigma_{\tilde{ d}_{n}^{(t)}}}\,, \tag{297}\]

where the last line follows from the first-order optimality condition for the minimum point \(\mathbf{w}_{\star,n}^{(t)}\in\arg\min_{w}\ell\left(\mathbf{w},Q_{n}^{(t)},\tilde{d}_{n}^{(t)}\right)\):

\[\forall\mathbf{w}\in\mathbb{R}^{p}:\quad\left(\mathbf{w}-\mathbf{w}_{\star,n}^{(t)}\right)^ {\top}\nabla_{w}\ell\left(\mathbf{w}_{\star,n}^{(t)},Q_{n}^{(t)},\tilde{d}_{n}^{(t )}\right)\geq 0.\]

Analogous to bounding \(|(I)|\), by simply substituting \(\bar{f}^{(t+1)}\) with \(\bar{f}^{(t)}\) or \(\pi^{\star}\) or substituting \(d^{\bar{f}^{(t+1)}}\) into \(d^{\star}\), we obtain the same upper bound for \(|(III)|\), \(|(A)|\) and \(|(C)|\), i.e.,

\[|(III)|,|(A)|,|(C)|\leq\sqrt{C_{\nu}\varepsilon_{\text{stat}}^{(t)}}\,. \tag{298}\]Now we upper bound \(|(II)|\) as follows:

\[|(II)| \leq\frac{1}{N}\sum_{n=1}^{N}\sum_{(s,a)\in\mathcal{S}\times\mathcal{ A}}d^{\bar{f}^{(t+1)}}(s)\bar{f}^{(t+1)}(a|s)\left(|\phi^{\top}(s,a)\mathbf{w}_{*,n}^{(t )}-Q_{n}^{(t)}(s,a)|+|Q_{n}^{(t)}(s,a)-\bar{Q}^{(t)}(s,a)|\right)\] \[\leq\frac{1}{N}\sum_{n=1}^{N}\sqrt{\sum_{(s,a)\in\mathcal{S} \times\mathcal{A}}\frac{\left(d^{\bar{f}^{(t+1)}}(s)\right)^{2}\left(\bar{f}^{ (t+1)}(a|s)\right)^{2}}{\bar{d}_{n}^{(t)}(s,a)}}.\] \[\qquad\cdot\sqrt{2\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}} \bar{d}_{n}^{(t)}(s,a)\left(\left(\phi^{\top}(s,a)\mathbf{w}_{*,n}^{(t)}-Q_{n}^{(t )}(s,a)\right)^{2}+\left(Q_{n}^{(t)}(s,a)-\bar{Q}^{(t)}(s,a)\right)^{2}\right)}\] \[=\frac{1}{N}\sum_{n=1}^{N}\sqrt{\mathbb{E}_{(s,a)\sim d_{n}^{(t)}} \left[\left(\frac{\left(d^{\bar{f}^{(t+1)}}(s)\right)\left(\bar{f}^{(t+1)}(a|s )\right)}{\bar{d}_{n}^{(t)}(s,a)}\right)^{2}\right]\cdot 2\left(\varepsilon _{\text{approx},n}^{(t)}+L_{Q}^{2}\left\|\mathbf{\xi}_{n}^{(t)}-\bar{\mathbf{\xi}}^{(t )}\right\|_{2}^{2}\right)}\] \[\leq\sqrt{2C_{\nu}\left(\bar{\varepsilon}_{\text{approx}}^{(t)}+ \frac{L_{Q}^{2}}{N}\left\|\mathbf{\xi}^{(t)}-\mathbf{1}(\bar{\mathbf{\xi}}^{(t)})^{\top} \right\|_{\text{F}}^{2}\right)}\,, \tag{299}\]

where \(L_{Q}\) is defined in Lemma E.7, the second line uses Cauchy-Schwartz's inequality and Young's inequality (117) and the last inequality uses Assumption 4.3 and Jensen's inequality.

Analogous to bounding \(|(II)|\), by simply substituting \(\bar{f}^{(t+1)}\) with \(\bar{f}^{(t)}\) or \(\pi^{\star}\) or substituting \(d^{\bar{f}^{(t+1)}}\) into \(d^{\star}\), we obtain the same upper bound for \(|(IV)|\), \(|(B)|\) and \(|(D)|\), i.e.,

\[|(IV)|,|(B)|,|(D)|\leq\sqrt{2C_{\nu}\left(\bar{\varepsilon}_{\text{approx}}^{(t )}+\frac{L_{Q}^{2}}{N}\left\|\mathbf{\xi}^{(t)}-\mathbf{1}(\bar{\mathbf{\xi}}^{(t)})^{\top} \right\|_{\text{F}}^{2}\right)}\,. \tag{300}\]

Plugging (296),(298),(299),(300) into (291) and dividing both sides by \((1-\gamma)\) yield

\[\vartheta_{\rho}\left(\delta^{(t+1)}-\delta^{(t)}\right)+\delta^{(t)}\leq\frac {D_{\star}^{(t)}}{(1-\gamma)\alpha}-\frac{D_{\star}^{(t+1)}}{(1-\gamma)\alpha} +\frac{2\sqrt{C_{\nu}}(\vartheta+1)}{1-\gamma}\left(\sqrt{\bar{\varepsilon}_{ \text{stat}}^{(t)}}+\sqrt{2\left(\bar{\varepsilon}_{\text{approx}}^{(t)}+\frac {L_{Q}^{2}}{N}\left\|\mathbf{\xi}^{(t)}-\mathbf{1}(\bar{\mathbf{\xi}}^{(t)})^{\top}\right\| _{\text{F}}^{2}\right)}\right)\,.\]

Taking expectation on both sides of the above expression and making use of the simple fact that

\[\mathbb{E}\left[\sqrt{x}\right]\leq\sqrt{\mathbb{E}[x]}\,,\]

we reach the conclusion (123).

### Proof of Lemma e.9

Proof of Lemma e.9.: For any \(\zeta>0\), by the actor update rule (34) and (114) we have that

\[\left\|\mathbf{\xi}^{(t+1)}-\mathbf{1}_{N}\bar{\mathbf{\xi}}^{(t+1)\top}\right\|_{\text{F} }^{2} =\left\|\mathbf{W}(\mathbf{\xi}^{(t)}+\alpha\mathbf{h}^{(t)})-\mathbf{1}_{N}(\bar{ \mathbf{\xi}}^{(t)}+\alpha\hat{\mathbf{w}}^{(t)})^{\top}\right\|_{\text{F}}^{2}\] \[\leq(1+\zeta)\sigma^{2}\left\|\mathbf{\xi}^{(t)}-\mathbf{1}_{N}\bar{\mathbf{ \xi}}^{(t)\top}\right\|_{\text{F}}^{2}+\alpha^{2}(1+1/\zeta)\sigma^{2}\left\| \mathbf{h}^{(t)}-\mathbf{1}_{N}\hat{\mathbf{w}}^{(t)\top}\right\|_{\text{F}}^{2}, \tag{301}\]

where the last line follows from Young's inequality (116) and (11). By the gradient tracking step (33), Young's inequality (116) and (11), we have

\[\left\|\mathbf{h}^{(t+1)}-\mathbf{1}\hat{\mathbf{w}}^{(t+1)\top}\right\|_{\text {F}}^{2} =\left\|\mathbf{W}(\mathbf{h}^{(t)}+\mathbf{w}^{(t+1)}-\mathbf{w}^{(t)})-\mathbf{1}\hat{ \mathbf{w}}^{(t)\top}+\mathbf{1}(\hat{\mathbf{w}}^{(t)\top}-\hat{\mathbf{w}}^{(t+1)\top}) \right\|_{\text{F}}^{2}\] \[=\left\|\mathbf{W}\mathbf{h}^{(t)}-\mathbf{1}\hat{\mathbf{w}}^{(t)\top}+\mathbf{W}( \mathbf{w}^{(t+1)}-\mathbf{w}^{(t)})-\mathbf{1}(\hat{\mathbf{w}}^{(t+1)\top}-\hat{\mathbf{w}}^{(t) \top})\right\|_{\text{F}}^{2}\] \[\leq(1+\zeta)\sigma^{2}\left\|\mathbf{h}^{(t)}-\mathbf{1}_{N}\hat{\mathbf{w} }^{(t)\top}\right\|+(1+1/\zeta)\sigma^{2}\left\|\mathbf{w}^{(t+1)}-\mathbf{w}^{(t)}- \mathbf{1}(\hat{\mathbf{w}}^{(t+1)\top}-\hat{\mathbf{w}}^{(t)\top})\right\|_{\text{F}}^{2}\] \[\leq(1+\zeta)\sigma^{2}\left\|\mathbf{h}^{(t)}-\mathbf{1}_{N}\hat{\mathbf{w} }^{(t)\top}\right\|+(1+1/\zeta)\sigma^{2}\left\|\mathbf{w}^{(t+1)}-\mathbf{w}^{(t)} \right\|_{\text{F}}^{2}, \tag{302}\]where the last inequality follows from the fact

\[\left\|\mathbf{w}^{(t+1)}-\mathbf{w}^{(t)}-\mathbf{1}(\hat{\mathbf{w}}^{(t+1)\top}- \hat{\mathbf{w}}^{(t)\top})\right\|_{\mathrm{F}}^{2}\] \[=\left\|\mathbf{w}^{(t+1)}-\mathbf{w}^{(t)}\right\|_{\mathrm{F}}^{2}+N \left\|\hat{\mathbf{w}}^{(t+1)}-\hat{\mathbf{w}}^{(t)}\right\|_{2}^{2}-2\sum_{n=1}^{N} \langle\mathbf{w}_{n}^{(t+1)}-\mathbf{w}_{n}^{(t)},\hat{\mathbf{w}}^{(t+1)}-\hat{\mathbf{w}}^{ (t)}\rangle\] \[=\left\|\mathbf{w}^{(t+1)}-\mathbf{w}^{(t)}\right\|_{\mathrm{F}}^{2}-N \left\|\hat{\mathbf{w}}^{(t+1)}-\hat{\mathbf{w}}^{(t)}\right\|_{2}^{2}\] \[\leq\left\|\mathbf{w}^{(t+1)}-\mathbf{w}^{(t)}\right\|_{\mathrm{F}}^{2}. \tag{303}\]

Then for any \(n\in[N]\), \(t\in\mathbb{N}\) and \(\mathbf{w}\in\mathbb{R}^{p}\), we have

\[\ell(\mathbf{w},Q_{n}^{(t)},\tilde{d}_{n}^{(t)})-\ell(\mathbf{w}_{\star,n} ^{(t)},Q_{n}^{(t)},\tilde{d}_{n}^{(t)})\] \[=\mathbb{E}_{(s,a)\sim\tilde{d}_{n}^{(t)}}\left[\left(\phi^{\top}( s,a)\mathbf{w}-\phi^{\top}(s,a)\mathbf{w}_{\star,n}^{(t)}+\phi^{\top}(s,a)\mathbf{w}_{ \star,n}^{(t)}-Q_{n}^{(t)}(s,a)\right)^{2}\right]-\ell(\mathbf{w}_{\star,n}^{(t)},Q _{n}^{(t)},\tilde{d}_{n}^{(t)})\] \[=\mathbb{E}_{(s,a)\sim\tilde{d}_{n}^{(t)}}\left[\left(\phi^{\top}( s,a)\mathbf{w}-\phi^{\top}(s,a)\mathbf{w}_{\star,n}^{(t)}\right)^{2}\right]+2(\mathbf{w}-\mathbf{w} _{\star,n}^{(t)})^{\top}\mathbb{E}_{(s,a)\sim\tilde{d}_{n}^{(t)}}\left[\left( \phi^{\top}(s,a)\mathbf{w}_{\star,n}^{(t)}-Q_{n}^{(t)}(s,a)\right)\phi(s,a)\right]\] \[=\left\|\mathbf{w}-\mathbf{w}_{\star,n}^{(t)}\right\|_{\Sigma_{\tilde{d} _{n}^{(t)}}}^{2}+(\mathbf{w}-\mathbf{w}_{\star,n}^{(t)})^{\top}\nabla_{w}\ell(\mathbf{w}_{ \star,n}^{(t)},Q_{n}^{(t)},\tilde{d}_{n}^{(t)})\] \[\geq\left\|\mathbf{w}-\mathbf{w}_{\star,n}^{(t)}\right\|_{\Sigma_{\tilde{ d}_{n}^{(t)}}}^{2}\] \[\geq(1-\gamma)\mu\left\|\mathbf{w}-\mathbf{w}_{\star,n}^{(t)}\right\|_{2} ^{2}, \tag{304}\]

where the penultimate line follows from the first-order optimality conditions for the optima \(\mathbf{w}_{\star,n}^{(t)}\):

\[\forall\mathbf{w}\in\mathbb{R}^{p}:\quad(\mathbf{w}-\mathbf{w}_{\star,n}^{(t)})^{\top} \nabla_{w}\ell(\mathbf{w}_{\star,n}^{(t)},Q_{n}^{(t)},\tilde{d}_{n}^{(t)})\geq 0 \tag{305}\]

and the last line is by Assumption 4.1 and (3.2).

Note that

\[\ell(\mathbf{w}_{\star,n}^{(t)},Q_{n}^{(t+1)},\tilde{d}_{n}^{(t+1)})\] \[=\mathbb{E}_{(s,a)\sim\tilde{d}_{n}^{(t+1)}}\left[\left(\phi^{ \top}(s,a)\mathbf{w}_{\star,n}^{(t)}-Q_{n}^{(t+1)}(s,a)\right)^{2}\right]\] \[\leq 2\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\tilde{d}_{n}^{( t)}(s,a)\frac{\tilde{d}_{n}^{(t+1)}(s,a)}{\tilde{d}_{n}^{(t)}(s,a)}(\phi^{\top}(s,a) \mathbf{w}_{\star,n}^{(t)}-Q_{n}^{(t)}(s,a))^{2}+2\mathbb{E}_{(s,a)\sim\tilde{d}_{ n}^{(t+1)}}(Q_{n}^{(t+1)}(s,a)-Q_{n}^{(t)}(s,a))^{2}\] \[\leq 2C_{\nu}\mathbb{E}_{(s,a)\sim\tilde{d}_{n}^{(t)}}(\phi^{\top} (s,a)\mathbf{w}_{\star,n}^{(t)}-Q_{n}^{(t)}(s,a))^{2}+2L_{Q}\left\|\mathbf{\xi}_{n}^{( t+1)}-\mathbf{\xi}_{n}^{(t)}\right\|_{2}^{2}\] \[\leq 2C_{\nu}\varepsilon_{\text{approx}}^{n}+2L_{Q}^{2}\left\|\mathbf{ \xi}_{n}^{(t+1)}-\mathbf{\xi}_{n}^{(t)}\right\|_{2}^{2}, \tag{306}\]

where the second inequality uses Assumption 4.3 and Lemma E.7, and the last line uses Assumption 4.2.

The above equation (306) together with (304) gives

\[\left\|\mathbf{w}_{\star}^{(t+1)}-\mathbf{w}_{\star}^{(t)}\right\|_{ \mathrm{F}}^{2}=\sum_{n=1}^{N}\left\|\mathbf{w}_{\star,n}^{(t+1)}-\mathbf{w}_{\star,n} ^{(t)}\right\|_{2}^{2}\] \[\leq\frac{1}{(1-\gamma)\mu}\sum_{n=1}^{N}\left(\ell(\mathbf{w}_{\star, n}^{(t)},Q_{n}^{(t+1)},\tilde{d}_{n}^{(t+1)})-\ell(\mathbf{w}_{\star,n}^{(t+1)},Q_{n}^{(t+1)}, \tilde{d}_{n}^{(t+1)})\right)\] \[\leq\frac{2}{(1-\gamma)\mu}\left(C_{\nu}\sum_{n=1}^{N} \varepsilon_{\text{approx}}^{n}+L_{Q}^{2}\left\|\mathbf{\xi}^{(t+1)}-\mathbf{\xi}^{(t)} \right\|_{\mathrm{F}}^{2}\right). \tag{307}\]

where \(\mathbf{w}_{\star}^{(t)}\coloneqq(\mathbf{w}_{1}^{(t)},\cdots,\mathbf{w}_{N}^{(t)})^{\top},\forall t\).

Also note that by Assumption E.2 and (304) we have

\[\forall t\in\mathbb{N}:\quad\left\|\mathbf{w}^{(t)}-\mathbf{w}^{(t)}_{\star} \right\|_{\mathrm{F}}^{2}\leq\frac{\sum_{n=1}^{N}\varepsilon_{\text{stat}}^{n}} {(1-\gamma)\mu}. \tag{308}\]

Therefore, by (306) and (308) we have

\[\left\|\mathbf{w}^{(t+1)}-\mathbf{w}^{(t)}\right\|_{\mathrm{F}}^{2} \leq 3\left(\left\|\mathbf{w}^{(t+1)}-\mathbf{w}^{(t+1)}_{\star}\right\|_{ \mathrm{F}}^{2}+\left\|\mathbf{w}^{(t+1)}_{\star}-\mathbf{w}^{(t)}_{\star}\right\|_{ \mathrm{F}}^{2}+\left\|\mathbf{w}^{(t)}-\mathbf{w}^{(t)}_{\star}\right\|_{\mathrm{F}}^ {2}\right)\] \[\leq\frac{6}{(1-\gamma)\mu}\left(N(C_{\nu}\bar{\varepsilon}_{ \text{approx}}+\bar{\varepsilon}_{\text{stat}})+L_{Q}^{2}\left\|\mathbf{\xi}^{(t+ 1)}-\mathbf{\xi}^{(t)}\right\|_{\mathrm{F}}^{2}\right). \tag{309}\]

where the first inequality uses Young's inequality (117).

Note that by the update rule (34), the double stochasticity of the mixing matrix \(\mathbf{W}\) and the consensus property (11) we have

\[\left\|\mathbf{\xi}^{(t+1)}-\mathbf{\xi}^{(t)}\right\|_{\mathrm{F}}^{2}\] \[=\left\|\mathbf{W}(\mathbf{\xi}^{(t)}+\alpha\mathbf{h}^{(t)})-\mathbf{\xi}^{(t)} \right\|_{\mathrm{F}}^{2}\] \[=\left\|(\mathbf{W}-\mathbf{I})(\mathbf{\xi}^{(t)}-\mathbf{1}_{N}\mathbf{\bar{\xi}}^{( t)\top})+\alpha(\mathbf{W}\mathbf{h}^{(t)}-\mathbf{1}_{N}\mathbf{\hat{w}}^{(t)\top})+\alpha\mathbf{1}( \mathbf{\hat{w}}^{(t)}-\mathbf{\hat{w}}^{(t)}_{\star})^{\top}+\mathbf{1}(\mathbf{\hat{w}}^{(t) }_{\star})^{\top}\right\|_{\mathrm{F}}^{2}\] \[\leq 16\left\|\mathbf{\xi}^{(t)}-\mathbf{1}_{N}\mathbf{\bar{\xi}}^{(t)\top} \right\|_{\mathrm{F}}^{2}+4\alpha^{2}\sigma^{2}\left\|\mathbf{h}^{(t)}-\mathbf{1}_{N} \mathbf{\hat{w}}^{(t)\top}\right\|_{\mathrm{F}}^{2}+4\alpha^{2}N\left\|\mathbf{\hat{w }}^{(t)}-\mathbf{\hat{w}}^{(t)}_{\star}\right\|_{2}^{2}+4\alpha^{2}N\left\|\mathbf{ \hat{w}}^{(t)}_{\star}\right\|_{2}^{2}\] \[\leq 16\left\|\mathbf{\xi}^{(t)}-\mathbf{1}_{N}\mathbf{\bar{\xi}}^{(t)\top} \right\|_{\mathrm{F}}^{2}+4\alpha^{2}\sigma^{2}\left\|\mathbf{h}^{(t)}-\mathbf{1}_{N} \mathbf{\hat{w}}^{(t)\top}\right\|_{\mathrm{F}}^{2}+4\alpha^{2}\sum_{n=1}^{N}\left\| \mathbf{w}^{(t)}_{n}-\mathbf{w}^{(t)}_{\star,n}\right\|_{2}^{2}+4\alpha^{2}\sum_{n=1} ^{N}\left\|\mathbf{w}^{(t)}_{\star,n}\right\|_{2}^{2}\] \[\leq 16\left\|\mathbf{\xi}^{(t)}-\mathbf{1}_{N}\mathbf{\bar{\xi}}^{(t)\top} \right\|_{\mathrm{F}}^{2}+4\alpha^{2}\sigma^{2}\left\|\mathbf{h}^{(t)}-\mathbf{1}_{N} \mathbf{\hat{w}}^{(t)\top}\right\|_{\mathrm{F}}^{2}+\frac{4\alpha^{2}N\bar{ \varepsilon}_{\text{stat}}}{(1-\gamma)\mu}+\frac{4\alpha^{2}NC_{\phi}^{2}}{ \mu^{2}(1-\gamma)^{4}}, \tag{310}\]

where the penultimate line uses Jensen's inequality and the last line follows from (304), Assumption E.2 and (271).

Combining (310) and (309) with (302), we deduce

\[\left\|\mathbf{h}^{(t+1)}-\mathbf{1}\mathbf{\hat{w}}^{(t+1)\top}\right\|_{ \mathrm{F}}^{2}\] \[\leq(1+1/\zeta)\frac{96\sigma^{2}L_{Q}^{2}}{(1-\gamma)\mu}\left\| \mathbf{\xi}^{(t)}-\mathbf{1}\mathbf{\bar{\xi}}^{(t)\top}\right\|_{\mathrm{F}}^{2}+\sigma ^{2}\left(1strictly adhere to Assumption 3.1. In Figure 2, we validate the effectiveness of vanilla FedNPG and entropy-regularized FedNPG across different map size \(K\), where we set \(\tau=0,0.005,0.05\), \(\eta=0.1\), \(N=10\), and use a _standard ring graph_ where agent \(n\) receives information from agent \(n+1\) for \(n\in[N-1]\), and agent \(N\) receives information from agent 1, and we set all the weights on each edge of the communication graph to be 0.5. The corresponding mixing matrix of the standard ring graph is as follows:

\[\mathbf{W}=\begin{pmatrix}0.5&0.5&0&0&\cdots&0&0\\ 0&0.5&0.5&0&\cdots&0&0\\ 0&0&0.5&0.5&\cdots&0&0\\ \vdots&\vdots&\vdots&\vdots&&\vdots&\vdots\\ 0&0&0&0&\cdots&0.5&0.5\\ 0.5&0&0&0&\cdots&0&0.5\end{pmatrix}\,. \tag{312}\]

Here, \(\mathbf{W}\) in (312) satisfies the double stochasticity assumption but is not symmetric.

Figure 1: Gridworld experiement. \(N\) agents (\(N=3\) here) aim to learn a shared policy to follow a predetermined path, which is the red dashed line in the complete map. Each agent only has access to partial information about the path and gets reward 1 only at the shaded positions and 0 at other positions. Each agent starts at the top left corner.

Figure 2: **Changing map size \(K\).** we let \(\tau=0,0.005\) and change \(K\) for each \(\tau\). We plot the curves of \((V_{\tau}^{\star}-\overline{V}_{\tau}^{(t)})/V_{\tau}^{\star}\) changing with the iteration number. We can see that both vanilla and entropy-regularized NPG converges to the optimal value function in a few iterations, and the convergence speed is almost the same across different \(K\).

Figure 2 illustrates the normalized sub-optimality gap \((V_{\tau}^{\star}-\overline{V}_{\tau}^{(t)})/V_{\tau}^{\star}\) with respect to the iteration number. It can be seen that both vanilla and entropy-regularized NPG converge to the optimal value function in a few iterations, and the convergence speed is almost the same across different \(K\), i.e. the impact of \(K\) on the convergence speed is minimal.

In Figure 3, we study the performance of our algorithms when the number of agents \(N\) varies. We set \(K=10,20,30\), \(\tau=0.005\), \(\eta=0.1\) and the communication graph to be the standard ring graph. We can see that the convergence speed decreases as \(N\) increases. Same as before, the convergence speed is insensitive to the change of \(K\).

In Figure 4, we illustrate the effect of the communication network topology to our algorithms. To be specific, we change the number of neighbors of each agent (i.e., the number of non-zero entries in each row of \(\mathbf{W}\)) and (i) randomly generalize the weights of the graph such that each row of \(\mathbf{W}\) sum up to 1, i.e.,\(\mathbf{W1}=\mathbf{1}\), see Figure 4(a); (ii) set the non-zero entries in each row of \(\mathbf{W}\) all to be number of neighbors, see Figure 4(b). We fix \(\eta=0.1\), \(K=10\), \(\tau=0.005\). We plot the curves of value functions changing with the iteration number. The green dashed line represents the optimal value. For both 4(a) and 4(b), the convergence speed increase as number of neighbors of each agent increases. FedNPG performs better when using equal weights.

### Discussion on the Experiments

Note that even though there are many existing works in federated RL, none of the existing works, to the best of our knowledge, studies federated multi-tasks RL in the decentralized setting. Therefore,

Figure 4: **Changing communication network topology.** We change the number of neighbors of each agent. (i) In Figure 4(a), we randomly generalize the weights of the graph such that each row of \(\mathbf{W}\) sum up to 1; (ii) In Figure 4(b), we set the non-zero entries in each row of \(\mathbf{W}\) all to be number of neighbors. We plot the curves of value functions changing with the iteration number. The green dashed line represents the optimal value. For both 4(a) and 4(b), the convergence speed increase as number of neighbors increases. FedNPG performs better when using equal weights.

Figure 3: **Changing number of agents \(N\).** we let \(K=10,20,30\) and change \(N\) for each \(K\). We plot the curves of value functions changing with the iteration number. The green dashed line represents the optimal value. We can see that the convergence speed decreases as \(N\) increases. Same as before, the convergence speed is insensitive to the change of \(K\).

we are not able to compare our work with existing works. However, here we include a comparison between FedNPG and a naive baseline without the Q-tracking technique (line 6 in Algorithm 1).

For this plot, we use the standard ring graph (Eq. 312). We fix the size of the maze \(K=30\), learning rate \(\eta=0.1\), and regularity coefficient \(\tau=0.005\). We experiment on different number of agents \(N\) and plot the curves of value function changing with the iteration number. The plot shows that while FedNPG converges within a few iterates, the algorithm without Q-tracking diverges, confirming the positive role of Q-tracking in ensuring convergence.

Figure 5: Comparison between FedNPG and a naive baseline without the Q-tracking technique. The plot shows that while FedNPG converges within a few iterates, the algorithm without Q-tracking diverges, confirming the positive role of Q-tracking in ensuring convergence.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: we clearly state in the abstract and introduction the claims we made, including the contributions made in the paper and important assumptions and limitations. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: we clearly state our assumptions. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: we provide the full set of assumptions and a complete (and correct) proof. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: see Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: The experiments are simple and can be easily reproduced by following the instructions in the paper. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experiment details are included in Section H. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: stochasticity is not critical in our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: the results are irrelevant to the compute resources. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: the research conducted in the paper conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: this is a theoretical paper and it has no societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: the paper aims to provide a better understanding on existing algorithms and thus poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: the paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: the paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. *