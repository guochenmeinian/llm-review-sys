# March: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data

 Pratik Karmakar

School of Computing, National University of Singapore

CNRS@CREATE Ltd, 1 Create Way, Singapore

pratik.karmakar@u.nus.edu

A significant portion of the work has been done as a part of P. Karmakar's masters in Ramakrishna Mission Vivekananda Educational and Research Institute, Belur, India.

Debabrota Basu

Equipe Scool, Univ. Lille, Inria, CNRS, Centrale Lille

UMR 9189- CRIStAL, F-59000 Lille, France

debabrota.basu@inria.fr

###### Abstract

We study design of black-box model extraction attacks that can _send minimal number of queries from_ a _publicly available dataset_ to a target ML model through a predictive API with an aim _to create an informative and distributionally equivalent replica_ of the target. First, we define _distributionally equivalent_ and _Max-Information model extraction_ attacks, and reduce them into a variational optimisation problem. The attacker sequentially solves this optimisation problem to select the most informative queries that simultaneously maximise the entropy and reduce the mismatch between the target and the token models. This leads to _an active sampling-based query selection algorithm_, March, which is _model-oblivious_. Then, we evaluate March on different text and image data sets, and different models, including CNNs and BERT. March extracts models that achieve \(\sim 60-95\%\) of true model's accuracy and uses \(\sim 1,000-8,500\) queries from the publicly available datasets, which are different from the private training datasets. Models extracted by March yield prediction distributions, which are \(\sim 2-4\times\) closer to the target's distribution in comparison to the existing active sampling-based attacks. The extracted models also lead to 84-96\(\%\) accuracy under membership inference attacks. Experimental results validate that March is _query-efficient_, and capable of performing task-accurate, high-fidelity, and informative model extraction.

## 1 Introduction

In recent years, Machine Learning as a Service (MLaaS) is widely deployed and used in industries. In MLaaS [14], an ML model is trained remotely on a private dataset, deployed in a Cloud, and offered for public access through a prediction API, such as Amazon AWS, Google API, Microsoft Azure. An API allows an user, including a potential _adversary_, _to send queries to the ML model and fetch corresponding predictions_. Recent works have shown such models with public APIs can be stolen, or extracted, by designing black-box model extraction attacks [13]. In model extraction attacks, an adversary queries the target model with a query dataset, which might be same or different than the private dataset, collects the corresponding predictions from the target model, and builds areplica model of the target model. The goal is to construct a model which is almost-equivalent to the target model over input space [1].

Often, ML models are proprietary, guarded by IP rights, and expensive to build. These models might be trained on datasets which are expensive to obtain [2] and consist of private data of individuals [1]. Also, extracted models can be used to perform other privacy attacks on the private dataset used for training, such as membership inference [13]. Thus, understanding susceptibility of models accessible through MLaaS presents an important conundrum. This motivates us to _investigate black-box model extraction attacks while the adversary has no access to the private data or a perturbed version of it_[3]. Instead, _the adversary uses a public dataset to query the target model_[14, 15].

Based on the above, we propose a model extraction based on the query-efficient black-box model extraction based on the query-efficient black-box model extraction poses a tension between the number of queries sent to the target model and the accuracy of extracted model [15]. With more queries and predictions, an adversary can build a better replica. But querying an API too much can be expensive, as each query incurs a monetary cost in MLaaS. Also, researchers have developed algorithms that can detect adversarial queries, when they are not well-crafted or sent to the API in large numbers [1, 15]. Thus, designing a query-efficient attack is paramount for practical deployment. Also, it exposes how more information can be leaked from a target model with less number of interactions.

In this paper, _we investigate effective definitions of efficiency of model extraction and corresponding algorithm design for query-efficient black-box model extraction attack with public data, which is oblivious to deployed model and applicable for any datatype._

**Our contributions.** Our investigation yields three contributions.

_1. Formalism: Distributional equivalence and Max-Information extraction._ Often, the ML models, specifically classifiers, are stochastic algorithms. They also include different elements of randomness during training. Thus, rather than focusing on equivalence of extracted and target models in terms of a fixed dataset or accuracy on that dataset [1], we propose _a distributional notion of equivalence._ We propose that if the joint distribution induced by a query generating distribution and corresponding prediction distribution due to both the target and the extracted models are same, they will be called distributionally equivalent (Sec. 3). Another proposal is to reinforce the objective of the attack, i.e. to extract as much information as possible from the target model. This allows us to formulate the Max-Information attack, where the adversary aims to maximise the mutual information between the extracted and target models' distributions. _Our hypothesis is that if we can extract the predictive distribution of a model, it would be enough to replicate other properties of the model (e.g. accuracy) and also to run other attacks (e.g. membership inference), rather than designing specific attacks to replicate the task accuracy or the model weights_. We show that both the attacks can be performed by sequentially solving a single variational optimisation [1] problem (Eqn. (6)).

_2. Algorithm: Adaptive query selection for extraction with_Marich. We propose an algorithm, March (Sec. 4), that optimises the objective of the variational optimisation problem (Eqn. (6)). Given an extracted model, a target model, and previous queries, March adaptively selects a batch of queries enforcing this objective. Then, it sends the queries to the target model, collects the predictions (i.e. the class predicted by target model), and uses them to further train the extracted model (Algo. 1). In order to select the most informative set of queries, it deploys three sampling strategies in cascade. These strategies select: a) the most informative set of queries, b) the most diverse set of queries in the first selection, and c) the final subset of queries where the target and extracted models mismatch the most. Together these strategies allow March to select a small subset of queries that both maximise the information leakage, and align the extracted and target models (Fig. 1).

Figure 1: Black-box model extraction with March.

_3. Experimental analysis._ We perform extensive the most for a given modelevaluation with both image and text datasets, and diverse model classes, such as Logistic Regression (LR), ResNet, CNN, and BERT (Sec. 5). Leveraging March's model-obliviousness, we even extract a ResNet trained on CIFAR10 with a CNN and out-of-class queries from ImageNet. Our experimental results validate that March extracts more accurate replicas of the target model and high-fidelity replica of the target's prediction distributions in comparison to existing active sampling algorithms. While March uses a small number of queries (\(\sim 1k-8.5k\)) selected from publicly available query datasets, the extracted models yield accuracy comparable with the target model while encountering a membership inference attack. This shows that March can extract alarmingly informative models query-efficiently. Additionally, as March can extract the true model's predictive distribution with a different model architecture and a mismatched querying dataset, it allows us to design a model-oblivious and dataset-oblivious approach to attack.

### Related works

**Taxonomy of model extraction.** Black-box model extraction (or model stealing or model inference) attacks aim to _replicate_ of a target ML model, commonly classifiers, deployed in a remote service and accessible through a public API [16]. The replication is done in such a way that the extracted model achieves one of the three goals: a) _accuracy close to that of the target model on the private training data_ used to train the target model, b) _maximal agreement in predictions_ with the target model on the _private training data_, and c) maximal agreement in prediction with the target model over the _whole input domain_. Depending on the objective, they are called _task accuracy_, _fidelity_, and _functional equivalence model extractions_, respectively [3]. Here, _we generalise these three approaches using a novel definition of distributional equivalence and also introduce a novel information-theoretic objective of model extraction which maximises the mutual information between the target and the extracted model over the whole data domain._

**Frameworks of attack design.** Following [16], researchers have proposed multiple attacks to perform one of the three types of model extraction. The attacks are based on two main approaches: _direct recovery_ (target model specific) [17, 18, 3] and _learning_ (target model specific/oblivious). The learning-based approaches can also be categorised into supervised learning strategies, where the adversary has access to both the true labels of queries and the labels predicted by the target model [16, 3], and online active learning strategies, where the adversary has only access to the predicted labels of the target model, and actively select the future queries depending on the previous queries and predicted labels [2, 3, 4]. _As query-efficiency is paramount for an adversary while attacking an API to save the budget and to keep the attack hidden and also the assumption of access true label from the private data is restrictive, we focus on designing an online and active learning-based attack strategy that is model oblivious._

**Types of target model.** While [17, 3] focus on performing attacks against linear models, all others are specific to neural networks [17, 3] and even a specific architecture [19]. In contrast, March is _capable of attacking both linear models and neural networks._ Additionally, March is _model-oblivious_, i.e. it can attack one model architecture (e.g. ResNet) using a different model architecture (e.g. CNN).

**Types of query feedback.** Learning-based attacks often assume access to either the probability vector of the target model over all the predicted labels [16, 18, 19, 20], or the gradient of the last layer of the target neural network [17, 21], which are hardly available in a public API. In contrast, following [20], _we assume access to only the predicted labels of the target model for a set of queries_, which is always available with a public API. Thus, experimentally, we cannot compare with existing active sampling attacks requiring access to the whole prediction vector [21, 18], and thus, compare with a wide-range of active sampling methods that can operate only with the predicted label, such as \(K\)-center sampling, entropy sampling, least confidence sampling, margin sampling etc. [19]. Details are in Appendix C.

**Choices of public datasets for queries.** There are two approaches of querying a target model: _data-free_ and _data-selection based_. In _data-free attacks_, the attacker begins with noise. The informative queries are generated further using a GAN-like model fed with responses obtained from an API [16, 15, 17, 18, 16]. Typically, it requires almost a million queries to the API to start generating sensible query data (e.g. sensible images that can leak from a model trained on CIFAR10). But since one of our main focus is query-efficiency, we focus on _data-selection based attacks_, where an adversary has access to a query dataset to select the queriesfrom and to send it to the target model to obtain predicted labels. In literature, researchers assume three types of query datasets: _synthetically generated samples_ [16], _adversarially perturbed private (or task domain) dataset_ [17, 2], and _publicly available (or out-of-task domain) dataset_[18, 20]. As we do not want to restrict March to have access to the knowledge of the private dataset or any perturbed version of it, _we use publicly available datasets, which are different than the private dataset_. To be specific, we only assume whether we should query the API with images, text, or tabular data and not even the identical set of labels. For example, we experimentally attack models trained on CIFAR10 with ImageNet queries having different classes.

Further discussions on related active sampling algorithms and distinction of March with the existing works are deferred to Appendix C.

## 2 Background: Classifiers, model extraction, membership inference attacks

Before proceeding to the details, we present the fundamentals of a classifier in ML, and two types of inference attacks: Model Extraction (ME) and Membership Inference (MI).

**Classifiers.** A classifier in ML [1] is a function \(f:\mathcal{X}\rightarrow\mathcal{Y}\) that maps a set of input features \(\mathbf{X}\in\mathcal{X}\) to an output \(Y\in\mathcal{Y}\).2 The output space is a finite set of classes, i.e. \(\{1,\dots,k\}\). Specifically, a classifier \(f\) is a parametric function, denoted as \(f_{\theta}\), with parameters \(\theta\in\mathbb{R}^{d}\), and is trained on a dataset \(\mathbf{D}^{T}\), i.e. a collection of \(n\) tuples \(\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}\) generated IID from an underlying distribution \(\mathcal{D}\). Training implies that given a model class \(\mathcal{F}=\{f_{\theta}|\theta\in\Theta\}\), a loss function \(l:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}_{\geq 0}\), and training dataset \(\mathbf{D}^{T}\), we aim to find the optimal parameter \(\theta^{*}\triangleq\operatorname*{arg\,min}_{\theta\in\Theta}\sum_{i=1}^{n} l(f_{\theta}(\mathbf{x}_{i}),y_{i})\). We use cross-entropy, i.e. \(l(f_{\theta}(\mathbf{x}_{i}),y_{i})\triangleq-y_{i}\log(f_{\theta}(\mathbf{x} _{i}))\), as the loss function for classification.

Footnote 2: We denote sets/vectors by **bold** letters, and the distributions by _calligraphic_ letters. We express random variables in UPPERCASE, and an assignment of a random variable in lowercase.

**Model extraction attack.** A model extraction attack is an inference attack where an adversary aims to steal a target model \(f^{T}\) trained on a private dataset \(\mathbf{D}^{T}\) and create another replica of it \(f^{E}\)[16]. In the black-box setting that we are interested in, the adversary can only query the target model \(f^{T}\) by sending queries \(Q\) through a publicly available API and to use the corresponding predictions \(\hat{Y}\) to construct \(f^{E}\). The goal of the adversary is to create a model which is either (a) as similar to the target model as possible for all input features, i.e. \(f^{T}(x)=f^{E}(x)\;\forall x\in\mathcal{X}\)[21, 22] or (b) predicts labels that has maximal agreement with that of the labels predicted by the target model for a given data-generating distribution, i.e. \(f^{E}=\operatorname*{arg\,min}\Pr_{x_{\sim}\mathcal{D}}[l(f^{E}(x),f^{T}(x))]\)[16, 22]. The first type of attacks are called the functionally equivalent attacks. The later family of attacks is referred as the fidelity extraction attacks. The third type of attacks aim to find an extracted model \(f^{E}\) that achieves maximal classification accuracy for the underlying private dataset used to train the \(f^{T}\). These are called task accuracy extraction attacks [16, 20, 20]. In this paper, _we generalise the first two type of attacks by proposing the distributionally equivalent attacks and experimentally show that it yields both task accuracy and fidelity._

**Membership inference attack.** Another popular family of inference attacks on ML models is the Membership Inference (MI) attacks [20, 17]. In MI attack, given a private (or member) dataset \(\mathbf{D}^{T}\) to train \(f^{T}\) and another non-member dataset \(S\) with \(|\mathbf{D}^{T}\cap S|\neq\emptyset\), the goal of the adversary is to infer whether any \(x\in\mathcal{X}\) is sampled from the member dataset \(\mathbf{D}^{T}\) or the non-member dataset \(S\). Effectiveness of an MI attacks can be measured by its accuracy of MI, i.e. the total fraction of times the MI adversary identifies the member and non-member data points correctly. Accuracy of MI attack on the private data using \(f^{E}\) rather than \(f^{T}\) is considered as a measure of effectiveness of the extraction attack [20]. We show that the model \(f^{E}\) extracted using March allows us to obtain similar MI accuracy as that obtained by directly attacking the target model \(f^{T}\) using even larger number of queries. This validates that _the model \(f^{E}\) by March in a black-box setting acts as an information equivalent replica of the target model \(f^{T}\)._

## 3 Distributional equivalence and Max-Information model extractions

In this section, we introduce the distributionally equivalent and Max-Information model extractions. We further reduce both the attacks into a variational optimisation problem.

**Definition 3.1** (Distributionally equivalent model extraction).: For any query generating distribution \(\mathcal{D}^{Q}\) over \(\mathbb{R}^{d}\times\mathcal{Y}\), an extracted model \(f^{E}:\mathbb{R}^{d}\to Y\) is distributionally equivalent to a target model \(f^{T}:\mathbb{R}^{d}\to Y\), if the joint distributions of input features \(Q\in\mathbb{R}^{d}\sim\mathcal{D}^{Q}\) and predicted labels induced by both the models are same almost surely. This means that for any divergence \(D\), two distributionally equivalent models \(f^{E}\) and \(f^{T}\) satisfy \(D(\Pr(f^{T}(Q),Q)\|\Pr(f^{E}(Q),Q))=0\;\forall\;\mathcal{D}^{Q}\).

To ensure query-efficiency in distributionally equivalent model extraction, an adversary aims to choose a query generating distribution \(\mathcal{D}^{Q}\) that minimises it further. If we assume that the extracted model is also a parametric function, i.e. \(f^{E}_{\omega}\) with parameters \(\omega\in\Omega\), we can solve the query-efficient distributionally equivalent extraction by computing

\[(\omega^{*}_{\mathrm{DEq}},\mathcal{D}^{Q}_{\min})\triangleq\operatorname*{ arg\,min}_{\omega\in\Omega}\operatorname*{arg\,min}_{\mathcal{D}^{Q}}D(\Pr(f^{T}_{ \theta^{*}}(Q),Q)\|\Pr(f^{E}_{\omega}(Q),Q)).\] (1)

Equation (1) allows us to choose a different class of models with different parametrisation for extraction till the joint distribution induced by it matches with that of the target model. For example, the extracted model can be a logistic regression or a CNN if the target model is a logistic regression. This formulation also enjoys the freedom to choose the data distribution \(\mathcal{D}^{Q}\) for which we want to test the closeness. Rather the distributional equivalence pushes us to find the best query distribution for which the mismatch between the posteriors reduces the most and to compute an extracted model \(f^{E}_{\omega^{*}}\) that induces the joint distribution closest to that of the target model \(f^{T}_{\theta^{*}}\).

**Connection with different types of model extraction.** For \(D=D_{\mathrm{KL}}\), our formulation extends the fidelity extraction from label agreement to prediction distribution matching, which addresses the future work indicated by [1]. If we choose \(\mathcal{D}^{Q}_{\min}=\mathcal{D}^{T}\), and substitute \(D\) by prediction agreement, distributional equivalence retrieves the fidelity extraction attack. If we choose \(\mathcal{D}^{Q}_{\min}=\operatorname{Unif}(\mathcal{X})\), distributional equivalent extraction coincides with functional equivalent extraction. Thus, a distributional equivalence attack can lead to both fidelity and functional equivalence extractions depending on the choice of query generating distribution \(\mathcal{D}^{Q}\) and the divergence \(D\).

**Theorem 3.2** (Upper bounding distributional closeness).: _If we choose KL-divergence as the divergence function \(D\), then for a given query generating distribution \(\mathcal{D}^{Q}\)_

\[D_{\mathrm{KL}}(\Pr(f^{T}_{\theta^{*}}(Q),Q)\|\Pr(f^{E}_{\omega_{\mathrm{DEq}} }(Q),Q))\leq\min_{\omega}\mathbb{E}_{Q}[l(f^{T}_{\theta^{*}}(Q),f^{E}_{\omega} (Q))]-H(f^{E}_{\omega}(Q)).\] (2)

By variational principle, Theorem 3.2 implies that _minimising the upper bound_ on the RHS leads to an extracted model which minimises the KL-divergence for a chosen query distribution.

**Max-Information model extraction.** Objective of any inference attack is to leak as much information as possible from the target model \(f^{T}\). Specifically, in model extraction attacks, we want to create an informative replica \(f^{E}\) of the target model \(f^{T}\) such that it induces a joint distribution \(\Pr(f^{E}_{\omega}(Q),Q)\), which retains the most information regarding the target's joint distribution. As adversary controls the query distribution, we aim to choose a query distribution \(\mathcal{D}^{Q}\) that maximises information leakage.

**Definition 3.3** (**Max-Information model extraction).** A model \(f^{E}:\mathbb{R}^{d}\to Y\) and a query distribution \(\mathcal{D}^{Q}\) are called a Max-Information extraction of a target model \(f^{T}:\mathbb{R}^{d}\to Y\) and a Max-Information query distribution, respectively, if they maximise the mutual information between the joint distributions of input features \(Q\in\mathbb{R}^{d}\sim\mathcal{D}^{Q}\) and predicted labels induced by \(f^{E}\) and that of the target model. Mathematically, \((f^{E}_{\omega^{*}},\mathcal{D}^{Q}_{\max})\) is a Max-Information extraction of \(f^{T}_{\theta^{*}}\) if

\[(\omega^{*}_{\mathrm{MaxInf}},\mathcal{D}^{Q}_{\max})\triangleq\operatorname* {arg\,max}_{\omega}\operatorname*{arg\,max}_{\mathcal{D}_{Q}}I(\Pr(f^{T}_{ \theta^{*}}(Q),Q)\|\Pr(f^{E}_{\omega}(Q),Q))\] (3)

Similar to Definition 3.1, Definition 3.3 also does not restrict us to choose a parametric model \(\omega\) different from that of the target \(\theta\). It also allows us to compute the data distribution \(\mathcal{D}^{Q}\) for which the information leakage is maximum rather than relying on the private dataset \(\mathbf{D}^{T}\) used for training \(f^{T}\).

**Theorem 3.4** (Lower bounding information leakage).: _For any given distribution \(\mathcal{D}^{Q}\), the information leaked by any Max-Information attack (Equation 3) is lower bounded as:_

\[I(\Pr(f^{T}_{\theta^{*}}(Q),Q)\|\Pr(f^{E}_{\omega^{*}_{\mathrm{MaxInf}}}(Q),Q ))\geq\max_{\omega}-\mathbb{E}_{Q}[l(f^{T}_{\theta^{*}}(Q),f^{E}_{\omega}(Q))]+ H(f^{E}_{\omega}(Q)).\] (4)

By variational principle, Theorem 3.4 implies that _maximising the lower bound_ in the RHS will lead to an extracted model which maximises the mutual information between target and extracted joint distributions for a given query generating distribution.

**Distributionally equivalent and Max-Information extractions: A variational optimisation formulation.** From Theorem 3.2 and 3.4, we observe that the lower and upper bounds of the objective functions of distribution equivalent and Max-Information attacks are negatives of each other. Specifically, \(-D_{\mathrm{KL}}(\Pr(f_{\theta^{*}}^{T}(Q),Q)\|\Pr(f_{\omega_{\mathrm{DEq}}^{E} }^{E}(Q),Q))\geq\max_{\omega}-F(\omega,\mathcal{D}^{Q})\) and \(I(\Pr(f_{\theta^{*}}^{T}(Q),Q)\|\Pr(f_{\omega_{\mathrm{MaxInf}}^{E}}^{E}(Q),Q ))\geq\max_{\omega}F(\omega,\mathcal{D}^{Q})\), where

\[F(\omega,\mathcal{D}^{Q})\triangleq-\mathbb{E}_{Q}[l(f_{\theta^{*}}^{T}(Q),f_ {\omega}^{E}(Q))]+H(f_{\omega}^{E}(Q)).\] (5)

Thus, following a variational approach, we aim to solve an optimisation problem on \(F(\omega,\mathcal{D}^{Q})\) in an online and frequentist manner. We do not assume a parametric family of \(\mathcal{D}^{Q}\). Instead, we choose a set of queries \(Q_{t}\in\mathbb{R}^{d}\) at each round \(t\in T\). This leads to an empirical counterpart of our problem:

\[\max_{\omega\in\omega,Q_{[0,T]}\in\mathbf{D}^{Q}|_{T}|}\hat{F}(\omega,Q_{[0,T ]})\triangleq\max_{\omega,Q_{[0,T]}}-\frac{1}{T}\sum_{t=1}^{T}l(f_{\theta^{*} }^{T}(Q_{t}),f_{\omega}^{E}(Q_{t}))]+\sum_{t=1}^{T}H(f_{\omega}^{E}(Q_{t})).\] (6)

As we need to evaluate \(f_{\theta^{*}}^{T}\) for each \(Q_{t}\), we refer \(Q_{t}\)'s as _queries_, the dataset \(\mathbf{D}^{Q}\subseteq\mathbb{R}^{d}\times\mathcal{Y}\) from where they are chosen as the _query dataset_, and the corresponding unobserved distribution \(\mathcal{D}^{Q}\) as the _query generating distribution_. Given the optimisation problem of Equation 6, we propose an algorithm Marich to solve it effectively.

## 4 Marich: A query selection algorithm for model extraction

In this section, we propose an algorithm, Marich, to solve Equation (6) in an adaptive manner.

**Algorithm design.** We observe that once the queries \(Q_{[0,T]}\) are selected, the outer maximisation problem of Eq. (6) is equivalent to regualrised loss minimisation. Thus, it can be solved using any standard empirical risk minimisation algorithm (e.g. Adam, SGD). Thus, to achieve query efficiency, we focus on designing a query selection algorithm that selects a batch of queries \(Q_{t}\) at round \(t\leq T\):

\[Q_{t}\triangleq\operatorname*{arg\,max}_{Q\in\mathbf{D}^{Q}}-\underbrace{1 }_{t}\sum_{i=1}^{t-1}l(f_{\theta^{*}}^{T}(Q_{i}\cup Q),f_{\omega_{t-1}}^{E}(Q _{i}\cup Q))]+\underbrace{\sum_{i=1}^{t-1}H(f_{\omega_{t-1}}^{E}(Q_{i}\cup Q ))}_{\text{Entropy term}}.\] (7)

```
0: Target model: \(f^{T}\), Query dataset: \(\mathbf{D}^{Q}\), \(\#\)Classes: \(k\)
0: Parameter: \(\#\)initial samples: \(n_{0}\), Training epochs: \(E_{max}\), \(\#\)Batches of queries: \(T\), Query budget: \(B\), Subsampling ratios: \(\gamma_{1},\gamma_{2}\in(0,1]\)
0: Extracted model \(f^{E}\)
1: //* Initialisation of the extracted model*// \(\rhd\) Phase 1
2: \(Q_{0}^{train}\gets n_{0}\) datapoints randomly chosen from \(D^{Q}\)
3: \(Y_{0}^{train}\gets f^{T}(Q_{0}^{train})\)\(\rhd\) Query the target model \(f^{T}\) with \(Q_{0}^{train}\)
4:for epoch \(\leftarrow\) 1 to \(E_{max}\)do
5: \(f_{0}^{E}\leftarrow\) Train \(f^{E}\) with \((Q_{0}^{train},Y_{0}^{train})\)
6:endfor
7: //* Adaptive query selection to build the extracted model*/ \(\rhd\) Phase 2
8:for\(t\gets\) 1 to \(T\)do
9: \(Q_{t}^{entropy}\leftarrow\)EntropySampling\((f_{t-1}^{E},\mathbf{D}^{Q}\setminus Q_{t-1}^{train},B)\)
10: \(Q_{t}^{grad}\leftarrow\)EntropyGradientSampling\((f_{t-1}^{E},Q_{t}^{entropy},\gamma_{1}B)\)
11: \(Q_{t}^{loss}\leftarrow\)LossSampling\((f_{t-1}^{E},Q_{t}^{grad},Q_{t-1}^{train},Y_{t-1}^{train},\gamma_{1}\gamma_{2}B)\)
12: \(Y_{t}^{new}\gets f^{T}(Q_{t}^{loss})\)\(\rhd\) Query the target model \(f^{T}\) with \(Q_{t}^{loss}\)
13: \(Q_{t}^{train}\gets Q_{t-1}^{train}\cup Q_{t}^{loss}\)
14: \(Y_{t}^{train}\gets Y_{t-1}^{train}\cup Y_{t}^{new}\)
15:for epoch \(\leftarrow\) 1 to \(E_{max}\)do
16: \(f_{t}^{E}\leftarrow\) Train \(f_{t-1}^{E}\) with \((Q_{t}^{train},Y_{t}^{train})\)
17:endfor
18:endfor
19:return Extracted model \(f^{E}\gets f_{T}^{E}\) ```

**Algorithm 1**MarichHere, \(f^{E}_{\omega_{t-1}}\) is the model extracted by round \(t-1\). Equation (7) indicates two criteria to select the queries. With the **entropy term**, we want to select a query that maximises the entropy of predictions for the extracted model \(f^{E}_{\omega_{t-1}}\). This allows us to select the queries which are most informative about the mapping between the input features and the prediction space. With the **model-mismatch term**, Eq. (7) pushes the adversary to select queries where the target and extracted models mismatch the most. Thus, minimising the loss between target and extracted models for such a query forces them to match over the whole domain. Algorithm 1 illustrates a pseudocode of Marich (Appendix A).

**Initialisation phase.** To initialise the extraction, we select a set of \(n_{0}\) queries, called \(Q^{train}_{0}\), uniformly randomly from the query dataset \(\mathbf{D}^{Q}\). We send these queries to the target model and collect corresponding predicted classes \(Y^{train}_{0}\) (Line 3). We use these \(n_{0}\) samples of input-predicted label pairs to construct a primary extracted model \(f^{E}_{0}\).

**Active sampling.** As the adaptive sampling phase commences, we select \(\gamma_{1}\gamma_{2}B\) number of queries at round \(t\). To _maximise_ the **entropy term** and _minimise_ the **model-mismatch term** of Eq. (7), we sequentially deploy EntropySampling and LossSampling. To achieve further query-efficiency, we refine the queries selected using EntropySampling by EntropyGradientSampling, which finds the most diverse subset from a given set of queries. Now, we describe the sampling strategies.

EntropySampling. First, we aim to select the set of queries which unveil most information about the mapping between the input features and the prediction space. Thus, we deploy EntropySampling. In EntropySampling, we compute the output probability vectors from \(f^{E}_{t-1}\) for all the query points in \(\mathbf{D}^{Q}\setminus Q^{train}_{t-1}\) and then select top \(B\) points with highest entropy:

\[Q^{entropy}\leftarrow\operatorname*{arg\,max}_{X\subset X_{in},|X|=B}H(f^{E}( X_{in})).\]

Thus, we select the queries \(Q^{entropy}_{t}\), about which \(f^{E}_{t-1}\) is most confused and training on these points makes the model more informative.

EntropyGradientSampling. To be frugal about the number of queries, we refine \(Q^{entropy}_{t}\) to compute the most diverse subset of it. First, we compute the gradients of entropy of \(f^{E}_{t-1}(x)\), i.e. \(\nabla_{x}H(f^{E}_{t-1}(x))\), for all \(x\in Q^{entropy}_{t}\). The gradient at point \(x\) reflects the change at \(x\) in the prediction distribution induced by \(f^{E}_{t-1}\). We use these gradients to embed the points \(x\in Q^{entropy}_{t}\). Now, we deploy K-means clustering to find \(k\) (= #classes) clusters with centers \(C_{in}\). Then, we sample \(\gamma_{1}B\) points from these clusters:

\[Q^{grad}\leftarrow\operatorname*{arg\,min}_{X\subset Q^{entropy}_{t},|X|= \gamma_{1}B}\sum_{x_{i}\in X}\sum_{x_{j}\in C_{in}}\|\nabla_{x_{i}}H(f^{E}(.) )-\nabla_{x_{j}}H(f^{E}(.))\|_{2}^{2}.\]

Selecting from \(k\) clusters ensures diversity of queries and reduces them by \(\gamma_{1}\).

LossSampling. We select points from \(Q^{grad}_{t}\) for which the predictions of \(f^{T}_{\theta^{*}}\) and \(f^{E}_{t-1}\) are most dissimilar. To identify these points, we compute the loss \(l(f^{T}(x),f^{E}_{t-1}(x))\) for all \(x\in Q^{train}_{t-1}\). Then, we select top-\(k\) points from \(Q^{train}_{t-1}\) with the highest loss values (Line 11), and sample a subset \(Q^{loss}_{t}\) of size \(\gamma_{1}\gamma_{2}B\) from \(Q^{grad}_{t}\) which are closest to the \(k\) points selected from \(Q^{train}_{t-1}\). This ensures that \(f^{E}_{t-1}\) would better align with \(f^{T}\) if it trains on the points where the mismatch in predictions are higher.

At the end of Phase 2 in each round of sampling, \(Q^{loss}_{t}\) is sent to \(f^{T}\) for fetching the labels \(Y^{train}_{t}\) predicted by the target model. We use \((Q^{loss}_{t},Y^{loss}_{t})\) along with \((Q^{train}_{t-1},Y^{train}_{t-1})\) to train \(f^{E}_{t-1}\) further. Thus, Marich performs \(n_{0}+\gamma_{1}\gamma_{2}BT\) number of queries through \(T+1\) number of interactions with the target model \(f^{T}\) to create the final extracted model \(f^{E}_{T}\). We experimentally demonstrate effectiveness of the model extracted by Marich to achieve high task accuracy and to act as an informative replica of the target for extracting private information regarding private training data \(\mathbf{D}^{T}\).

**Discussions.** Eq. (7) dictates that the active sampling strategy should try to select queries that maximise the entropy in the prediction distribution of the extracted model, while decreases the mismatch in predictions of the target and the extracted models. We further use the EntropyGradientSampling to choose a smaller but most diverse subset. As Eq. (7) does not specify any ordering between these objectives, one can argue about the sequence of using these three sampling strategies. We choose to use sampling strategies in the decreasing order of runtime complexity as the first strategy selects the queries from the whole query dataset, while the following strategies work only on the already selected queries. We show in Appendix E that LossSampling incurs the highest runtime followed by EntropyGradientSampling, while EntropySampling is significantly cheaper.

_Remark 4.1_.: Previously, researchers have deployed different active sampling methods to efficiently select queries for attacks [PMG\({}^{+}\)17, CCG\({}^{+}\)20, PGS\({}^{+}\)20]. But our derivation shows that the active query selection can be grounded on the objectives of distributionally equivalent and max-information extractions. Thus, though the end result of our formulation, i.e. Marich, is an active query selection algorithm, our framework is different and novel with respect to existing active sampling based works.

## 5 Experimental analysis

Now, we perform an experimental evaluation of models extracted by Marich. Here, we discuss the experimental setup, the objectives of experiments, and experimental results. We defer the source code, extended results, parametric similarity of the extracted models, effects of model-mismatch, details of different samplings, and hyperparameters to Appendix.

**Experimental setup.** We implement a prototype of Marich using Python 3.9 and PyTorch 1.12, and run on a NVIDIA GeForce RTX 3090 24 GB GPU. We perform attacks against _four target models_ (\(f^{T}\)), namely Logistic Regression (LR), CNN [1], ResNet [13], BERT [1], trained on _three private datasets_ (\(\mathbf{D}^{T}\)): MNIST handwritten digits [4], CIFAR10 [KH\({}^{+}\)09] and BBC News, respectively. For model extraction, we use EMNIST letters dataset [1], CIFAR10, ImageNet [10], and AGNews [11], as publicly-available, mismatched query datasets \(\mathbf{D}^{Q}\).

To instantiate task accuracy, we compare accuracy of the extracted models \(f^{E}_{\textsc{Marich}}\) with the target model and models extracted by K-Center (KC) [15], Least-Confidence sampling (LC) [14], Margin sampling (MS) [1, 2], Entropy Sampling (ES) [1], and Random Sampling (RS). To instantiate informativeness of the extracted models [16], we compare the Membership Inference (MI), i.e. MI accuracy and MI agreements (\(\%\) and AUC), performed on the target models, and the models extracted using Marich and competitors with same query budget. For MI, we use in-built membership attack from IBM ART [17]. For brevity, we discuss Best of Competitors (BoC) against Marich for each experiment (except Fig. 2- 3) The objectives of the experiments are:

1. _How do the accuracy of the model extracted using Marich on the private dataset compare with that of the target model, and RS with same query budget?_
2. _How close are the prediction distributions of the model extracted using Marich and the target model? Can Marich produce better replica of target's prediction distribution than other active sampling methods, leading to better distributional equivalence?_
3. _How do the models extracted by Marich behave under Membership Inference (MI) in comparison to the target models, and the models extracted by RS with same budget?_ The MI accuracy achievable by attacking a model acts as a proxy of how informative is the model.
4. _How does the performance of extracted models change if Differentially Private (DP) mechanisms [13] are applied on target model either during training or while answering the queries?_

**Accuracy of extracted models.** Marich extracts LR models with 1,863 and 959 queries selected from EMNIST and CIFAR10, while attacking a target LR model, \(f^{T}_{\text{LR}}\) trained on MNIST (test accuracy: \(90.82\%\)). The models extracted by Marich using EMNIST and CIFAR10 achieve test accuracy \(73.98\%\) and \(86.83\%\) (\(81.46\%\) and \(95.60\%\) of \(f^{T}_{\text{LR}}\)), respectively (Figure 1(a)-1(b)). The models extracted using BoC show test accuracy \(52.60\%\) and \(79.09\%\) (\(57.91\%\) and \(87.08\%\) of \(f^{T}_{\text{LR}}\)), i.e.

Figure 2: Accuracy of the extracted models (mean \(\pm\) std. over 10 runs) w.r.t. the target model using Marich, and competing active sampling methods (KC, LC, MS, ES, RS). Each figure represents (a target model, a query dataset). Models extracted by Marich are closer to the target models.

significantly less than that of Marich. Marich attacks a ResNet, \(f^{T}_{ResNet}\), trained on CIFAR10 (test accuracy: \(91.82\%\)) with 8,429 queries from ImageNet dataset, and extracts a CNN. The extracted CNN shows \(56.11\%\) (\(61.10\%\) of \(f^{T}_{ResNet}\)) test accuracy. But the model extracted using BoC achieves \(42.05\%\) (\(45.79\%\) of \(f^{T}_{ResNet}\)) accuracy (Figure 1(c)). We also attack a CNN with another CNN, which also reflects Marich's improved performance (Figure 1(d)). _To verify_ Marich_'s effectiveness for text data_, we also attack a BERT, \(f^{T}_{BERT}\) trained on BBCNews (test accuracy: \(98.65\%\)) with queries from the AGNews dataset. By using only 474 queries, Marich extracts a model with \(85.45\%\) (\(86.64\%\) of \(f^{T}_{BERT}\)) test accuracy. The model extracted by BoC shows test accuracy \(79.25\%\) (\(80.36\%\) of \(f^{T}_{BERT}\)). _For all the models and datasets,_ Marich _extracts models that achieve test accuracy closer to target models, and are more accurate than models extracted by the other algorithms._

**Distributional equivalence of extracted models.** One of our aims is to extract a distributionally equivalent model of the target \(f^{T}\) using Marich. Thus, in Figure 3, we illustrate the KL-divergence (mean\(\pm\)std. over 10 runs) between the prediction distributions of the target model and the model extracted by Marich. Due to brevity, we show two cases in the main paper: when we attack i) an LR trained on MNIST with EMNIST with an LR, and ii) a ResNet trained on CIFAR10 with ImageNet with a CNN. In all cases, we observe that the models extracted by Marich achieve \(\sim 2-4\times\) lower KL-divergence than the models extracted by all other active sampling methods. _These results show that_ Marich _is extracts high-fidelity distributionally equivalent models than competing algorithms._

**Membership inference with extracted models.** In Table 1, we report _accuracy_, _agreement_ in inference with target model, and _agreement AUC_ of membership attacks performed on different target models and extracted models with different query datasets. The models extracted using Marich demonstrate higher MI agreement with the target models than the models extracted using its competitors in most of the cases. They also achieve MI accuracy close to the target model. _These results indicate that the models extracted by_ Marich _act as informative replicas of the target models._

**Performance against privacy defenses.** We test the impact of DP-based defenses deployed in the target model on the performance of Marich. First, we train four target models on MNIST using _DP-SGD_[1] with privacy budgets \(\varepsilon=\{0.2,0.5,1,\infty\}\) and \(\delta=10^{-5}\). As illustrated in Figure 3(a), accuracy of the models extracted by querying DP target models are \(\sim 2.3-7.4\%\) lower than the model extracted from non-private target models. Second, we apply an _output perturbation_ method [14], where a calibrated Laplace noise is added to the responses of the target model against Marich's queries. This ensures \(\varepsilon\)-DP for the target model. Figure 3(b) shows that performance of the extracted models degrade slightly for \(\varepsilon=2,8\), but significantly for \(\varepsilon=0.25\). Thus, _performance of_ Marich _decreases while operating against DP defenses but the degradation varies depending on the defense mechanism._

\begin{table}
\begin{tabular}{l|l|l|l|l|l|l|l|l|l} \hline \multicolumn{1}{|c|}{**Member dataset**} & \multicolumn{1}{c|}{**Target model**} & \multicolumn{1}{c|}{**Query Dataset**} & \multicolumn{1}{c|}{**Algorithm**} & \multicolumn{1}{c|}{**No-member dataset**} & \multicolumn{1}{c|}{\(\varepsilon\)**Quertex**} & \multicolumn{1}{c|}{MI error} & \multicolumn{1}{c|}{MI error} & \multicolumn{1}{c|}{MI error} & \multicolumn{1}{c|}{MI error} & \multicolumn{1}{c|}{All experiment} & \multicolumn{1}{c|}{All experiment} & \multicolumn{1}{c|}{Accuracy} \\ \hline MNIST & LR & EMNIST & MARICH & EMNIST & \(85.00\)(\(\%\)) & \(\mathbf{34.05}\) & \(\mathbf{90.34}\) & \(\mathbf{90.38}\) & \(\mathbf{72.39}\) \\ \hline MNIST & LR & EMNIST & BaC\({}^{*}\) & EMNIST & \(185.83\)(\(\%\)) & \(\mathbf{84.07}\) & \(\mathbf{90.34}\) & \(\mathbf{90.39}\) & \(\mathbf{72.39}\) \\ \hline MNIST & LR & EMNIST & BaC\({}^{*}\) & EMNIST & \(185.83\)(\(\%\)) & \(\mathbf{84.07}\) & \(\mathbf{90.36}\) & \(\mathbf{80.06}\) & \(\mathbf{72.39}\) \\ \hline MNIST & LR & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 \\ \hline MNIST & LR & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 \\ \hline MNIST & CNN & EMNIST & MARICH & EMNIST & \(61.22\)(\(\%\)) & \(\mathbf{96.02}\) & \(\mathbf{92.79}\) & \(\mathbf{86.71}\) & \(\mathbf{86.03}\) \\ \hline MNIST & CNN & EMNIST & BaC\({}^{*}\) & EMNIST & \(61.67\)(\(\%\)) & \(\mathbf{90.73}\) & \(\mathbf{87.58}\) & \(\mathbf{86.97}\) & \(\mathbf{82.51}\) \\ \hline CIFAR10 & ResNet & EMNIST & \(53.00\)(\(\%\)) & \(\mathbf{10.00}\) & \(\mathbf{10.71}\) & \(\mathbf{91.82}\) \\ \hline CIFAR10 & ResNet & ImageNet & MARICH & ZEMNIST & \(8425\) & \(\mathbf{15.56\)(\(\%\)) & \(\mathbf{90.64}\) & \(\mathbf{93.34}\) & \(\mathbf{74.54}\) & \(\mathbf{84.11}\) \\ \hline CIFAR10 & ResNet & ImageNet & BaC\({}^{*}\) & EMNIST & \(8425\) & \(\mathbf{15.56\)(\(\%\)) & \(\mathbf{90.64}\) & \(\mathbf{85.47}\) & \(\mathbf{72.34}\) & \(\mathbf{60.65}\) \\ \hline BBCNews & BERT & ABCNews & BaC\({}^{*}\) & \(\mathbf{\Delta}\) & \(\mathbf{\Delta}\) & \(\mathbf{100.1

**Summary of results.** From the experimental results, we deduce the following conclusions.

_1. Accuracy._ Test accuracy (on the subsets of private datasets) of the models \(f^{E}_{\text{Markich}}\) are higher than the models extracted with the competing algorithms, and are \(\sim 60-95\%\) of the target models (Fig. 2). This shows effectiveness of Marich as a task accuracy extraction attack, while solving distributional equivalence and max-info extractions.

_2. Distributional equivalence._ We observe that the KL-divergence between the prediction distributions of the target model and \(f^{E}_{\text{Markich}}\) are \(\sim 2-4\times\) lower than the models extracted by other active sampling algorithms. This confirms that Marich conducts more accurate distributionally equivalent extraction than existing active sampling attacks.

_3. Informative replicas: Effective membership inference._ The agreement in MI achieved by attacking \(f^{E}_{\text{Markich}}\) and the target model in most of the cases is higher than that of the BoC* (Table 1). Also, MI accuracy for \(f^{E}_{\text{Markich}}\)'s are \(84.74\%-96.32\%\) (Table 1). This shows that the models extracted by Marich act as informative replicas of the target model.

_4. Query-efficiency._ Table 1 shows that Marich uses only \(959-8,429\) queries from the public datasets, i.e. a small fraction of data used to train the target models. Thus, Marich is significantly query efficient, as existing active learning attacks use \(10\)k queries to commence [PGS\({}^{+}\)20, Table 2].

_5. Performance against defenses._ Performance of Marich decreases with the increasing level of DP applied on the target model, which is expected. But when DP-SGD is applied to train the target, the degradation is little (\(\sim 7\%\)) even for \(\varepsilon=0.2\). In contrast, the degradation is higher when the output perturbation is applied with similar \(\varepsilon\) (\(0.25\)).

_6. Model-obliviousness and out-of-class data._ By construction, Marich is model-oblivious and can use out-of-class public data to extract a target model. To test this flexibility of Marich, we try and extract a ResNet trained on CIFAR10 using a different model, i.e. CNN, and out-of-class data, i.e. ImageNet. We show CNNs extracted by Marich are more accurate, distributionally close, and also lead to higher MI accuracy that the competitors, validating flexibility of Marich.

_7. Resilience to mismatch between \(\mathbf{D}^{T}\)and \(\mathbf{D}^{Q}\)._ For Marich, the datasets \(\mathbf{D}^{T}\)and \(\mathbf{D}^{Q}\)can be significantly different. For example, we attack an MNIST-trained model with EMNIST and CIFAR10 as query datasets. MNIST contains handwritten digits, CIFAR10 contains images of aeroplanes, cats, dogs etc., and EMNIST contains handwritten letters. Thus, the data generating distributions and labels are significantly different between the private and query datasets. We also attack a CIFAR10-trained ResNet with ImageNet as \(\mathbf{D}^{Q}\). CIFAR10 and ImageNet are also known to have very different labels and images. Our experiments demonstrate that Marich can handle data mismatch as well as model mismatch, which is an addendum to the existing model extraction attacks.

## 6 Conclusion and future directions

We investigate the design of a model extraction attack against a target ML model (classifier) trained on a private dataset and accessible through a public API. The API returns only a predicted label for a given query. We propose the notions of distributional equivalence extraction, which extends the existing notions of task accuracy and functionally equivalent model extractions. We also propose an information-theoretic notion, i.e. Max-Info model extraction. We further propose a variational relaxation of these two types of extraction attacks, and solve it using an online and adaptive query selection algorithm, Marich. Marich uses a publicly available query dataset different from the private dataset. We experimentally show that the models extracted by Marich achieve \(56-86\%\) accuracy on the private dataset while using 959 - 8,429 queries. For both text and image data, we demonstrate that the models extracted by Marich act as informative replicas of the target models and also yield high-fidelity replicas of the targets' prediction distributions. Typically, the functional equivalence attacks require model-specific techniques, while Marich is model-oblivious while performing distributional equivalence attack. This poses an open question: is distributional equivalence extraction 'easier' than functional equivalence extraction, which is NP-hard [JCB\({}^{+}\)20]?

**Acknowledgments**

P. Karmakar acknowledges supports of the programme DesCartes and the National Research Foundation, Prime Minister's Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme. D. Basu acknowledges the Inria-Kyoto University Associate Team "RELIANT", and the ANR JCJC for the REPUBLIC project (ANR-22-CE23-0003-01) for support. We also thank Cyriaque Rousselot for the interesting initial discussions.

## References

* [ACG\({}^{+}\)16] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _Proceedings of the 2016 ACM SIGSAC conference on computer and communications security_, pages 308-318, 2016.
* [BBJP18] Lejla Batina, Shivam Bhasin, Dirmanto Jap, and Stipepan Picek. CSI neural network: Using side-channels to recover your artificial neural network information. _CoRR_, abs/1810.09076, 2018.
* [BBZ07] Maria-Florina Balcan, Andrei Broder, and Tong Zhang. Margin based active learning. In _Learning Theory: 20th Annual Conference on Learning Theory, COLT 2007, San Diego, CA, USA; June 13-15, 2007. Proceedings 20_, pages 35-50. Springer, 2007.
* [CATvS17] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik. Emnist: Extending mnist to handwritten letters. In _2017 International Joint Conference on Neural Networks (IJCNN)_, pages 2921-2926, 2017.
* [CCG\({}^{+}\)20] Varun Chandrasekaran, Kamalika Chaudhuri, Irene Giacomelli, Somesh Jha, and Songbai Yan. Exploring connections between active learning and model extraction. In _29th USENIX Security Symposium (USENIX Security 20)_, pages 1309-1326, 2020.
* [CMS11] Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical risk minimization. _Journal of Machine Learning Research_, 12(3), 2011.
* [CSBB\({}^{+}\)18] Jacson Rodrigues Correia-Silva, Rodrigo F Berriel, Claudine Badue, Alberto F de Souza, and Thiago Oliveira-Santos. Copycat cnn: Stealing knowledge by persuading confession with random non-labeled data. In _2018 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2018.
* [DBB18] Ashish Dandekar, Debabrota Basu, and Stephane Bressan. Differential privacy for regularised linear regression. In _Database and Expert Systems Applications: 29th International Conference, DEXA 2018, Regensburg, Germany, September 3-6, 2018, Proceedings, Part II_, pages 483-491. Springer, 2018.
* [DBB21] Ashish Dandekar, Debabrota Basu, and Stephane Bressan. Differential privacy at risk: Bridging randomness and privacy budget. _Proceedings on Privacy Enhancing Technologies_, 1:64-84, 2021.
* [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [DDS\({}^{+}\)09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [Den12] Li Deng. The mnist database of handwritten digit images for machine learning research. _IEEE Signal Processing Magazine_, 29(6):141-142, 2012.
* [DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of cryptography conference_, pages 265-284. Springer, 2006.
* [GBCB16] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. _Deep learning_, volume 1. MIT Press, 2016.
* [Hua21] Kuan-Hao Huang. Deepal: Deep active learning in python. _arXiv preprint arXiv:2111.15258_, 2021.
* [HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.

* [JCB\({}^{+}\)20] Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas Papernot. High accuracy and high fidelity extraction of neural networks. In _29th USENIX security symposium (USENIX Security 20)_, pages 1345-1362, 2020.
* [JG19] Heinrich Jiang and Maya Gupta. Minimum-margin active learning. _arXiv preprint arXiv:1906.00025_, 2019.
* [JSMA19] Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. Prada: protecting against dnn model stealing attacks. In _2019 IEEE European Symposium on Security and Privacy (EuroS&P)_, pages 512-527. IEEE, 2019.
* [KDR\({}^{+}\)21] Krishnateja Killamsetty, Sivasubramanian Durga, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. Grad-match: Gradient matching based data subset selection for efficient deep model training. In _International Conference on Machine Learning_, pages 5464-5474. PMLR, 2021.
* [KH\({}^{+}\)09] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [KS22] Yeachan Kim and Bonggun Shin. In defense of core-set: A density-aware core-set selection for active learning. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 804-812, 2022.
* [KZCI21] Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, and Rishabh Iyer. Retrieve: Core-set selection for efficient and robust semi-supervised learning. _Advances in Neural Information Processing Systems_, 34:14488-14501, 2021.
* [LBH15] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. _nature_, 521(7553):436-444, 2015.
* [LG94] DD Lewis and WA Gale. A sequential algorithmfor training text classifiers. In _SIGIR'94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University_, pages 3-12, 1994.
* [LM05] Daniel Lowd and Christopher Meek. Good word attacks on statistical spam filters. In _CEAS_, volume 2005, 2005.
* [LS06] Mingkun Li and Ishwar K Sethi. Confidence-based active learning. _IEEE transactions on pattern analysis and machine intelligence_, 28(8):1251-1261, 2006.
* [MBL20] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of machine learning models. In _International Conference on Machine Learning_, pages 6950-6960. PMLR, 2020.
* [MHS21] Takayuki Miura, Satoshi Hasegawa, and Toshiki Shibahara. Megex: Data-free model extraction attack against gradient-based explainable ai. _arXiv preprint arXiv:2107.08909_, 2021.
* [MSDH19] Smitha Milli, Ludwig Schmidt, Anca D Dragan, and Moritz Hardt. Model reconstruction from model explanations. In _Proceedings of the Conference on Fairness, Accountability, and Transparency_, pages 1-9, 2019.
* [NSH19] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In _2019 IEEE symposium on security and privacy (SP)_, pages 739-753. IEEE, 2019.
* [NST\({}^{+}\)18] Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser, Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, Ian Molloy, and Ben Edwards. Adversarial robustness toolbox v1.2.0. _CoRR_, 1807.01069, 2018.

* [OSF19] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of black-box models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4954-4963, 2019.
* [PGKS21] Soham Pal, Yash Gupta, Aditya Kanade, and Shirish Shevade. Stateful detection of model extraction attacks. _arXiv preprint arXiv:2107.05166_, 2021.
* [PGS\({}^{+}\)20] Soham Pal, Yash Gupta, Aditya Shukla, Aditya Kanade, Shirish Shevade, and Vinod Ganapathy. Activethief: Model extraction using active learning and unannotated public data. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 865-872, 2020.
* [PMG\({}^{+}\)17] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In _Proceedings of the 2017 ACM on Asia conference on computer and communications security_, pages 506-519, 2017.
* [RGC15] Mauro Ribeiro, Katarina Grolinger, and Miriam AM Capretz. Mlaas: Machine learning as a service. In _2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)_, pages 896-902. IEEE, 2015.
* [RXC\({}^{+}\)21] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning. _ACM computing surveys (CSUR)_, 54(9):1-40, 2021.
* [SAB22] Sunandini Sanyal, Sravanti Addepalli, and R Venkatesh Babu. Towards data-free model stealing in a hard label setting. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15284-15293, 2022.
* [SB12] Joe Staines and David Barber. Variational optimization. _arXiv preprint arXiv:1212.4507_, 2012.
* [Set09] Burr Settles. Active learning literature survey. 2009.
* [SS18] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In _International Conference on Learning Representations_, 2018.
* [SS20] Congzheng Song and Vitaly Shmatikov. Overlearning reveals sensitive attributes. In _8th International Conference on Learning Representations, ICLR 2020_, 2020.
* [SSSS17] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In _2017 IEEE symposium on security and privacy (SP)_, pages 3-18. IEEE, 2017.
* [TMWP21] Jean-Baptiste Truong, Pratyush Maini, Robert J Walls, and Nicolas Papernot. Data-free model extraction. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4771-4780, 2021.
* [TZJ\({}^{+}\)16] Florian Tramer, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine learning models via prediction \(\{\)APIs\(\}\). In _25th USENIX security symposium (USENIX Security 16)_, pages 601-618, 2016.
* [YDY\({}^{+}\)19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. _Advances in neural information processing systems_, 32, 2019.
* [YGFJ18] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In _2018 IEEE 31st computer security foundations symposium (CSF)_, pages 268-282. IEEE, 2018.
* [YSS\({}^{+}\)21] Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya Mironov. Opacus: User-friendly differential privacy library in PyTorch. _arXiv preprint arXiv:2109.12298_, 2021.

* [ZLX\({}^{+}\)22] Jie Zhang, Bo Li, Jianghe Xu, Shuang Wu, Shouhong Ding, Lei Zhang, and Chao Wu. Towards efficient data free black-box adversarial attack. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15115-15125, 2022.
* [ZWL\({}^{+}\)20] Mingyi Zhou, Jing Wu, Yipeng Liu, Shuaicheng Liu, and Ce Zhu. Dast: Data-free substitute training for adversarial attacks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 234-243, 2020.
* [ZZL15] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In _NIPS_, 2015.

## Appendix

### Table of Contents

* A Complete pseudocode of March
* B Theoretical analysis: Proofs of Section 3
* C A review of active sampling strategies
* D Extended experimental analysis
* D.1 Test accuracy of extracted models
* D.2 Fidelity of the prediction distributions of The extracted models
* D.3 Fidelity of parameters of the extracted models
* D.4 Membership inference with the extracted models
* E Significance and comparison of three sampling strategies
* F Performance against differentially private target models
* G Effects of model mismatch
* H Choices of hyperparameters

### Broader impact

In this paper, we design a model extraction attack algorithm, March, that aims to construct a model that has similar predictive distribution as that of a target model. In this direction, we show that popular deep Neural Network (NN) models can be replicated with a few number of queries and only outputs from their predictive API. We also show that this can be further used to conduct membership inference about the private training data that the adversary has no access to. Thus, March points our attention to the vulnerabilities of the popular deep NN models to preserve the privacy of the users, whose data is used to train the deep NN models. Though every attack algorithm can be used adversarially, our goal is not to promote any such adversarial use.

Rather, in the similar spirit as that of the attacks developed in cryptography to help us to design better defenses and to understand vulnerabilities of the computing systems better, we conduct this research to understand the extent of information leakage done by an ML model under modest assumptions. We recommend it to be further used and studied for developing better privacy defenses and adversarial attack detection algorithms.

## Appendix A Complete pseudocode of March

```
0: Target model: \(f^{T}\), Query dataset: \(\mathbf{D}^{Q}\), \(\#\)Classes: \(k\)
0:\(\#\)initial samples: \(n_{0}\), Training epochs: \(E_{max}\), \(\#\)Batches of queries: \(T\), Query budget: \(B\), Subsampling ratios: \(\gamma_{1},\gamma_{2}^{2}\in(0,1]\)
0: Extract model \(f^{E}\)
1: //* Initialisation of the extracted model*/l \(\rhd\) Phase 1
2: \(Q_{t}^{train}\gets n_{0}\) datapoints randomly chosen from \(D^{Q}\)
3: \(Y_{0}^{train}\gets f^{T}(Q_{0}^{train})\)\(\rhd\) Query the target model \(f^{T}\) with \(Q_{0}^{train}\)
4:for epoch \(\leftarrow\) 1 to \(E_{max}\)do
5: \(f_{0}^{E}\leftarrow\) Train \(f^{E}\) with \((Q_{0}^{train},Y_{0}^{train})\)
6:endfor
7: /* Adaptive query selection to build the extracted model*/l \(\rhd\) Phase 2
8:for\(t\gets 1\) to \(T\)do
9: \(Q_{t}^{entropy}\leftarrow\textsc{EntropySampling}(f_{t-1}^{E},\mathbf{D}^{Q} \setminus Q_{t-1}^{train},B)\)
10: \(Q_{t}^{grad}\leftarrow\textsc{GradientSampling}(f_{t-1}^{E},Q_{t}^{entropy}, \gamma_{1}B)\)
11: \(Q_{t}^{loss}\leftarrow\textsc{LossSampling}(f_{t-1}^{E},Q_{t}^{grad},Q_{t-1}^{ train},Y_{t-1}^{train},\gamma_{1}\gamma_{2}B)\)
12: \(Y_{r}^{new}\gets f^{T}(Q_{t}^{loss})\)\(\rhd\) Query the target model \(f^{T}\) with \(Q_{t}^{loss}\)
13: \(Q_{t}^{train}\gets Q_{t-1}^{train}\cup Q_{t}^{loss}\)
14: \(Y_{t}^{train}\gets Y_{t-1}^{train}\cup Y_{r}^{new}\)
15:for epoch \(\leftarrow\) 1 to \(E_{max}\)do
16: \(f_{t}^{E}\leftarrow\) Train \(f_{t-1}^{E}\) with \((Q_{t}^{train},Y_{t}^{train})\)
17:endfor
18:endfor
19:return Extracted model \(f^{E}\gets f_{T}^{E}\)
20:
21:EntropySampling (extracted model: \(f^{E}\), input data points: \(X_{in}\), budget: \(B\))
22: \(Q_{entropy}\leftarrow\arg\max_{X\subset X_{in},|X|=B}H(f^{E}(X_{in}))\)
23:return\(Q_{entropy}\)\(\rhd\) Select \(B\) points with maximum entropy
24:
25:GradientSampling (extracted model: \(f^{E}\), input data points: \(X_{in}\), budget: \(\gamma_{1}B\))
26: \(E\gets H(f^{E}(X_{in}))\)
27: \(G\leftarrow\{\nabla_{x}E\mid x\in X_{in}\}\)
28: \(C_{in}\gets k\) centres of \(G\) computed using K-means
29: \(Q_{grad}\leftarrow\arg\min_{X\subset X_{in},|X|=\gamma_{1}B}\sum_{x_{i}\in X} \sum_{x_{j}\in C_{in}}\|\nabla_{x_{i}}E-\nabla_{x_{j}}E\|_{2}^{2}\)\(\rhd\) Select \(\gamma_{1}B\) points from \(X_{in}\) whose \(\frac{\partial E}{\partial x}\) are closest to that of \(C_{in}\)
30:return\(Q_{grad}\)
31:
32:LossSampling (extracted model: \(f^{E}\), input data points: \(X_{in}\), previous queries: \(Q_{train}\), previous predictions: \(Y_{train}\), budget: \(\gamma_{1}\gamma_{2}B\))
33: \(L\gets l(Y_{train},f^{E}(Q_{train}))\)\(\rhd\) Compute the mismatch vector
34: \(Q_{mis}\leftarrow\textsc{ArgMaxSort}(L,k)\)\(\rhd\) Select top-k mismatching points
35: \(Q_{loss}\leftarrow\arg\min_{X\subset X_{in},|X|=\gamma_{1}\gamma_{2}B}\sum_{x_{i} \in X}\sum_{x_{j}\in Q_{mis}}\|x_{i}-x_{j}\|_{2}^{2}\)\(\rhd\) Select \(\gamma_{1}\gamma_{2}B\) points closest to \(Q_{mis}\)
36:return\(Q_{loss}\) ```

**Algorithm 2**March
The last inequality holds true as the extracted model \(f_{\omega}^{E}\) is trained using the outputs of the target model \(f_{\theta^{*}}^{T}\). Thus, by data-processing inequality, its output distribution possesses less information than that of the target model. Specifically, we know that if \(Y=f(X)\), \(H(Y)\leq H(X)\).

Now, by taking \(\min_{\omega}\) on both sides, we obtain

\[D_{\mathrm{KL}}(\Pr(f_{\theta^{*}}^{T}(Q),Q)\|\Pr(f_{\omega_{ \mathrm{DEq}}}^{E}(Q),Q))\leq\min_{\omega}\mathbb{E}_{Q}[l(f_{\theta^{*}}^{T} (Q),f_{\omega}^{E}(Q))]-H(f_{\omega}^{E}(Q)).\]

Here, \(\omega_{\mathrm{DEq}}^{*}\triangleq\operatorname*{arg\,min}_{\omega}D_{ \mathrm{KL}}(\Pr(f_{\theta^{*}}^{T}(Q),Q)\|\Pr(f_{\omega}^{E}(Q),Q))\). The equality exists if minima of LHS and RHS coincide. 

**Theorem 3.4** (Lower Bounding Information Leakage).: _The information leaked by any Max-Information attack (Equation 3) is lower bounded as follows:_

\[I(\Pr(f_{\theta^{*}}^{T}(Q),Q)\|\Pr(f_{\omega_{\mathrm{MaxInfr}}}^{E}(Q),Q)) \geq\max_{\omega}-\mathbb{E}_{Q}[l(f_{\theta^{*}}^{T}(Q),f_{\omega}^{E}(Q))]+ H(f_{\omega}^{E}(Q)).\]

Proof.: Let us consider the same terminology as the previous proof. Then,

\[I(\Pr(f_{\theta^{*}}^{T}(Q),Q)\|\Pr(f_{\omega}^{E}(Q),Q))\] \[=H(f_{\theta^{*}}^{T}(Q),Q)+H(f_{\omega}^{E}(Q),Q)-H(f_{\theta^{ *}}^{T}(Q),f_{\omega}^{E}(Q),Q)\] \[=H(f_{\theta^{*}}^{T}(Q),Q)+H(f_{\omega}^{E}(Q),Q)-H(f_{\omega}^ {E}(Q),Q|f_{\theta^{*}}^{T}(Q))+H(f_{\theta^{*}}^{T}(Q))\] \[\geq H(f_{\omega}^{E}(Q),Q)-H(f_{\omega}^{E}(Q),Q|f_{\theta^{*}}^ {T}(Q))\] (9) \[\geq H(f_{\omega}^{E}(Q))-H(f_{\omega}^{E}(Q),Q|f_{\theta^{*}}^{T }(Q))\] (10) \[\geq H(f_{\omega}^{E}(Q))-\mathbb{E}_{Q}[l(f_{\omega}^{E}(Q),f_{ \theta^{*}}^{T}(Q))]\] (11)The inequality of Equation 9 is due to the fact that entropy is always non-negative. Equation 10 holds true as \(H(X,Y)\geq\max\{H(X),H(Y)\}\) for two random variables \(X\) and \(Y\). The last inequality is due to the fact that conditional entropy of two random variables \(X\) and \(Y\), i.e. \(H(X|Y)\), is smaller than or equal to their cross entropy, i.e. \(l(X,Y)\) (Lemma B.1).

Now, by taking \(\max_{\omega}\) on both sides, we obtain

\[I(\Pr(f_{\theta^{*}}^{T}(Q),Q)\|\Pr(f_{\omega_{\mathrm{MaxInf}}}^{E}(Q),Q)) \leq\max_{\omega}-\mathbb{E}_{Q}[l(f_{\theta^{*}}^{T}(Q),f_{\omega}^{E}(Q))]+ H(f_{\omega}^{E}(Q)).\]

Here, \(\omega_{\mathrm{MaxInf}}^{*}\triangleq\arg\max_{\omega}I(\Pr(f_{\theta^{*}}^ {T}(Q),Q)\|\Pr(f_{\omega_{\mathrm{MaxInf}}^{*}}^{E}(Q),Q))\). The equality exists if maxima of LHS and RHS coincide. 

**Lemma B.1** (Relating Cross Entropy and Conditional Entropy).: _Given two random variables \(X\) and \(Y\), conditional entropy_

\[H(X|Y)\leq l(X,Y).\] (12)

Proof.: Here, \(H(X|Y)\triangleq-\int\Pr(x,y)\log\frac{\Pr(x,y)}{\Pr(y)}\mathrm{d}\nu_{1}(X) \mathrm{d}\nu_{2}(Y)\) and \(l(X,Y)\triangleq l(\Pr(X),\Pr(Y))=-\int\Pr(x)\ln\Pr(y)\mathrm{d}\nu_{1}(X) \mathrm{d}\nu_{2}(Y)\) denotes the cross-entropy, given reference measures \(\nu_{1}\) and \(\nu_{2}\).

\[l(X,Y) =H(X)+D_{\mathrm{KL}}(\Pr(X)\|\Pr(Y))\] \[=H(X|Y)+I(X;Y)+D_{\mathrm{KL}}(P_{X}\|P_{Y})\] \[\geq H(X|Y)\]

The last inequality holds as both mutual information \(I\) and KL-divergence \(D_{\mathrm{KL}}\) are non-negative functions for any \(X\) and \(Y\).

A review of active sampling strategies

Least Confidence sampling (LC).Least confidence sampling method [13, 14] iteratively selects the subset of \(k\) data points from a data pool, which are most uncertain at that particular instant. The uncertainty function (\(u(\cdot|f_{\omega}):\mathcal{X}\rightarrow[0,1]\)) is defined as

\[u(x|f_{\omega})\triangleq 1-\Pr(\hat{y}|x),\]

where \(\hat{y}\) is the predicted class by a model \(f_{\omega}\) for input \(x\).

Margin Sampling (MS).In margin sampling [1], a subset of \(k\) points is selected from a data pool, such that the subset demonstrates the minimum margin, where \(\operatorname{margin}(\cdot|f_{\omega}):\mathcal{X}\rightarrow[0,1]\) is defined as

\[\operatorname{margin}(x|f_{\omega})\triangleq\Pr(\widehat{y}_{1}(x)|x,f_{ \omega})-\Pr(\widehat{y}_{2}(x)|x,f_{\omega}),\]

where \(f_{\omega}\) is the model, and \(\hat{y}_{1}(x)\) and \(\hat{y}_{2}(x)\) are respectively the highest and the second highest scoring classes returned by \(f_{\omega}\).

Entropy Sampling (ES).Entropy sampling, also known as uncertainty sampling [1], iteratively selects a subset of \(k\) datapoints with the highest uncertainty from a data pool. The uncertainty is defined by the entropy function of the prediction vector, and is computed using all the probabilities returned by the model \(f_{\omega}\) for a datapoint \(x\). For a given point \(x\) and a model \(f_{\omega}\), entropy is defined as

\[\operatorname{entropy}(x|f_{\omega})\triangleq-\sum_{a=1}^{|\mathcal{Y}|}p_{a} \log(p_{a}),\]

where \(p_{a}=\Pr[f_{\omega}(x)=a]\) for any output class \(a\in\{1,\ldots,|\mathcal{Y}|\}\). [1] mention that while using this strategy, _"the initial classifier plays an important role, since without it there may be a long period of random sampling before examples of a low frequency class are stumbled upon"_. This is similar to our experimental observation that ES often demonstrate high variance in its outcomes.

Core-set Algorithms.Here, we discuss some other interesting works in active learning using core-sets, and the issues to directly implement them in our problem.

[13] aims to identify a subset of training dataset for training a specific model. The algorithm needs _white-box access to the model_, and also needs to do a forward pass over the whole training dataset. A white-box sampling algorithm and the assumption of being able to retrieve predictions over the whole training dataset are not feasible in our problem setup. Relaxing the white-box access, [13] proposes to us the average loss over the training dataset or a significantly diverse validation set. Then, the gradient of loss on this training or validation set is compared with that of the selected mini-batch of data points. In a black-box attack, we do not have access to average loss over a whole training or validation dataset. Thus, it is not feasible to deploy the proposed algorithm.

Following another approach, [12] proposes an elegant pre-processing algorithm to select a core-set of a training dataset. This selection further leads to an efficient incremental gradient based training methodology. But in our case, we sequentially query the black-box model to obtain a label for a query point and use them further for training the extracted model. Thus, creating a dataset beforehand and using them further to pre-process will not lead to an adaptive attack and also will not reduce the query budget. Thus, it is out of scope of our work. [14] deploys an auxiliary classifier to first use a subset of labelled datapoints to create low dimensional embeddings. Then according to the query budget, it chooses the points from the sparse regions from each cluster found from the low dimensional embeddings. This is another variant of uncertainty sampling that hypothesises the points from the sparse region are more informative in terms of loss and prediction entropy. This design technique is at the same time model specific, and thus incompatible to our interest. Additionally, this approach also increases the requirement of queries to the target model.

K-Center sampling (KC).Thus, we focus on a version of core-set algorithm, namely K-Center sampling, that is applicable in our context. K-Center sampling as an active learning algorithm that has been originally proposed to train CNNs sample-efficiently [14]. For a given data pool, K-Center sampling iteratively selects the \(k\) datapoints that minimise the core set loss (see Equation 3, [14]) the most for a given model \(f_{\omega}\). In this method, the embeddings (from the model under training) of the data points are used as the representative vectors, and K-Center algorithm is applied on these representative vectors.

Random Sampling (RS).In random sampling, a subset of \(k\) datapoints are selected from a data pool uniformly at random.

In our experiments, for query selection at time \(t\), the extracted model at time \(t-1\), i.e. \(f_{t-1}^{E}\), is used as \(f_{\omega}\), and data pool at time \(t\) is the corresponding query dataset, except the datapoints that has been selected before step \(t\). Hereafter, we deploy a modified version of the framework developed by [10] to run our experiments with the aforementioned active learning algorithms.

Extended experimental analysis

In this section, we step-wise elaborate further experimental setups and results that we skipped for the brevity of space in the main draft. Specifically, we conduct our experiments in six experimental setups. Each experimental setup corresponds to a triplet (target model architecture trained on a private dataset, extracted model architecture, query dataset). Here, we list these six experimental setups in detail

1. A Logistic Regression (LR) model trained on MNIST, a LR model for extraction, EMNIST dataset for querying
2. A Logistic Regression (LR) model trained on MNIST, a LR model for extraction, CIFAR10 dataset for querying
3. A CNN model trained on MNIST, a CNN model for extraction, EMNIST dataset for querying
4. A ResNet model trained on CIFAR10, a CNN model for extraction, ImageNet dataset for querying
5. A ResNet model trained on CIFAR10, a ResNet184 model for extraction, ImageNet dataset for querying

Footnote 4: We begin with a pre-trained ResNet18 model from https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html
6. A BERT model5 trained on BBCNews, a BERT model for extraction, AGNews dataset for querying

Footnote 5: We use the pre-trained BERT model from https://huggingface.co/bert-base-cased

For each of the experimental setups, we evaluate five types of performance evaluations, which are elaborated in Section D.1, D.2.1, D.2.2, D.3, and D.4. While each of the following sections contain illustrations of the different performance metrics evaluating efficacy of the attack and corresponding discussions, Table 2- 4 contain summary of all queries used, accuracy, and membership inference statistics for all the experiments.

### Test accuracy of extracted models

Test accuracy of the extracted model and its comparison with the test accuracy of the target model on a subset of the private training dataset, which was used by neither of these models, is the most common performance metric used to evaluate the goodness of the attack algorithm. The attacks designed solely to optimise this performance metric are called the task accuracy model extraction attacks [3].

With Marich, we aim to extract models that have prediction distributions closest to that of the target model. Our hypothesis is constructing such a prediction distribution lead to a model that also has high accuracy on the private test dataset, since accuracy is a functional property of the prediction distribution induced by a classifier. In order to validate this hypothesis, we compute test accuracies of the target models, and models extracted by Marich and other active sampling algorithms in six experimental setups. We illustrate the evolution curves of accuracies over increasing number of queries in Figure 5.

To compare Marich with other active learning algorithms, we attack the same target models using K-centre sampling, Least Confidence sampling, Margin Sampling, Entropy Sampling, and Random Sampling algorithms (ref. Appendix C) using the same number of queries as used for Marich in each setup.

From Figure 5, we observe that **in most of the cases Marich outperforms all other competing algorithms**.

In this process, Marich uses \(\sim 500-8000\) queries, which is a small fraction of the corresponding query datasets. This also indicates towards the query-efficiency of Marich.

Extraction of a ResNet trained on CIFAR10 with a ResNet18.Along with the five experimental setups mentioned in the paper, we trained a ResNet with CIFAR10 dataset (\(\mathbf{D}^{T}\) here), that shows a test accuracy of \(91.82\%\) on a disjoint test set. We use ImageNet as \(\mathbf{D}^{Q}\) here, to extract a ResNet18 model from the target model. We have restrained from discussing this setup in the main paper due to brevity of space.

Marich extracts a ResNet18 using \(8429\) queries, and the extracted ResNet18 shows test accuracy of \(71.65\pm 0.88\%\). On the other hand, models extracted using Best of Competitors (BoC) using ImageNet queries shows accuracy of \(70.67\pm 0.12\%\).

Figure 5: Comparison of test accuracies achieved by models extracted by different active sampling algorithms.

### Fidelity of the prediction distributions of The extracted models

Driven by the distributional equivalence extraction principle, the central goal of Marich is to construct extracted models whose prediction distributions are closest to the prediction distributions of corresponding target models. From this perspective, in this section, we study the fidelity of the prediction distributions of models extracted by Marich and other active sampling algorithms, namely K-centre sampling, Least Confidence sampling, Margin Sampling, Entropy Sampling, and Random Sampling.

#### d.2.1 KL-divergence between prediction distributions

First, as the metric of distributional equivalence, we evaluate the KL-divergence between the prediction distributions of the models extracted by Marich and other active sampling algorithms. In Figure 6, we report the box-plot (mean, median \(\pm\) 25 percentiles) of KL-divergences (in log-scale) calculated from \(5\) runs for each of \(10\) models extracted by each of the algorithms.

**Results.** Figure 6 shows that the KL-divergence achieved by the prediction distributions of models extracted using Marich are at least \(\sim 2-10\) times less than that of the other competing algorithms. This validates our claim that Marich_yields distributionally closer extracted model \(f^{E}\) from the target model \(f^{T}\) than existing active sampling algorithms.

Figure 6: Comparison of KL-divergences (in log-scale) between the target prediction distributions and prediction distributions of the models extracted by different active learning algorithms.

#### d.2.2 Prediction agreement

In Figure 7, we illustrate the agreement in predictions of \(f^{E}\) with \(f^{T}\) on test datasets using different active learning algorithms. Prediction agreement functions as another metric of fidelity of prediction distributions constructed by extracted models in comparison with those of the target models.

Similar to Figure 6, we report the box-plot (mean, median \(\pm\) 25 percentiles) of prediction agreements (in \(\%\)) calculated from \(5\) runs for each of \(10\) models extracted by each of the algorithms.

**Results.** We observe that the prediction distributions extracted by March achieve almost same to \(\sim 30\%\) higher prediction agreement in comparison with the competing algorithms. Thus, we infer that in this particular case March _achieves better fidelity than the other active sampling algorithms, in some instances, while it is similar to the BoC in some instances_.

Figure 7: Comparison of agreement in predictions (in \(\%\)) between the target model and the models extracted by different active learning algorithms.

### Fidelity of parameters of the extracted models

Generating extracted models with low parametric fidelity is not a main goal or basis of the design principle of Marich. Since parametric fidelity is a popularly studied metric to evaluate goodness of model extraction, in Figure 8, we depict the parametric fidelity of models extracted by different active learning algorithms.

Let \(\mathbf{w}_{E}\) be the parameters of the extracted model and \(\mathbf{w}_{T}\) be the parameters of the target model. We define parametric fidelity as \(F_{\mathbf{w}}\triangleq\log\|\mathbf{w}_{E}-\mathbf{w}_{T}\|_{2}\). Since the parametric fidelity is only computable when the target and extracted models share the same architecture, we report the four instances here where Marich is deployed with the same architecture as that of the target model. For logistic regression, we compare all the weights of the target and the extracted models. For BERT and CNN, we compare between the weights in the last layers of these models.

**Results.** For LR, we observe that the LR models extracted by Marich have \(20-30\) times lower parametric fidelity than the extracted LR models of the competing algorithms. For BERT, the BERT extracted by Marich achieves \(0.4\) times lower parametric fidelity than the Best of Competitors (BoC). As an exception, for CNN, the model extracted by K-center sampling achieves \(0.996\) times less parametric fidelity than that of Marich.

Thus, we conclude that Marich _as a by-product of its distributionally equivalent extraction principle also extracts model with high parametric fidelity_, which is often better than the competing active sampling algorithms.

Figure 8: Comparison of parametric fidelity for Marich and different active learning algorithms.

### Membership inference with the extracted models

A main goal of Marich is to conduct a Max-Information attack on the target model, i.e. to extract an informative replica of its predictive distribution that retains the most information about the private training dataset. Due to lack of any direct measure of informativeness of an extracted model with respect to a target model, we run Membership Inference (MI) attacks using the models extracted by Marich, and other competing active sampling algorithms. High accuracy and agreement in MI attacks conducted on extracted models of Marich and the target models implicitly validate our claim that Marich is able to conduct a Max-Information attack.

**Observation 1.** From Figure 9, we see that in most cases the probability densities of the membership inference are closer to the target model when the model is extracted using Marich, than using all other active sampling algorithms (BoC, Best of Competitors).

Figure 9: Comparison among membership vs. non-membership probability densities for membership attacks against models extracted by Marich, the best of competitors (BoC) and the target model. Each figure represents the model class and query dataset. Memberships and non-memberships inferred from the model extracted by Marich are significantly closer to the target model.

**Observation 2.** In Figure 10, we present the agreements from the member points, nonmember points and overall agreement curves for varying membership thresholds, along with the AUCs of the overall membership agreements. We see that in most cases, the agreement curves for the models extracted using March are above those for the models extracted using random sampling, thus AUCs are higher for the models extracted using March.

**Observation 3.** In Table 2, 3, and 4, we summarise the MI accuracy on the private training dataset, Nonmembership inference accuracy on the private training dataset, Agreement in MI w.r.t. the MI on the target model, and AUC of Agreement in MI with that of the target model for Logistic Regression (LR), CNN, ResNet, and BERT target models. We observe that, while compared with other active sampling algorithms, out of 6 combinations of (target model, extracted model, query dataset) under study, the models extracted by March achieve the highest accuracy in MI and agreement in MI w.r.t. the target model, in most of the instances.

**Results.**_These observations support our claim that model extraction using March gives models are accurate and informative replica of the target model._

Figure 10: Comparison of membership, nonmembership and overall agreements of membership attacks against models extracted by March and the best of competitors with the target model trained with MNIST. Each figure represents the model class and query dataset. Membership agreement of the models extracted by March are higher.

[MISSING_PAGE_EMPTY:28]

[MISSING_PAGE_EMPTY:29]

[MISSING_PAGE_FAIL:30]

Performance against differentially private target models

In this section, we aim to verify performance of Marich against privacy-preserving mechanisms. Specifically, we apply a \((\varepsilon,\delta)\)-Differential Privacy (DP) inducing mechanism [16, 1] on the target model to protect the private training dataset. There are three types of methods to ensure DP: output perturbation [16], objective perturbation [17, 1], and gradient perturbation [1]. Since output perturbation and gradient perturbation methods scale well for nonlinear deep networks, we focus on them as the defense mechanism against Marich's queries.

Gradient perturbation-based defenses.DP-SGD [1] is used to train the target model on the member dataset. This mechanism adds noise to the gradients and clip them while training the target model. We use the default implementation of Opacus [20] to conduct the training in PyTorch.

Following that, we attack the \((\varepsilon,\delta)\)-DP target models using Marich and compute the corresponding accuracy of the extracted models. In Figure 12, we show the effect of different privacy levels \(\varepsilon\) on the achieved accuracy of the extracted Logistic Regression model trained with MNIST dataset and queried with EMNIST dataset. Specifically, we assign \(\delta=10^{-5}\) and vary \(\varepsilon\) in \(\{0.2,0.5,1,2,\infty\}\). Here, \(\varepsilon=\infty\) corresponds to the model extracted from the non-private target model.

We observe that the accuracy of the models extracted from private target models are approximately \(2.3-7.4\%\) lower than the model extracted from the non-private target model. This shows that performance of Marich decreases against DP defenses but not significantly.

Output perturbation-based defenses.Perturbing output of an algorithm against certain queries with calibrated noise, in brief output perturbation, is one of the basic and oldest form of privacy-preserving mechanism [16]. Here, we specifically deploy the Laplace mechanism, where a calibrated Laplace noise is added to the output of the target model generated against some queries. The noise is sampled from a Laplace distribution \(\mathrm{Lap}(0,\frac{\Delta}{\varepsilon})\), where \(\Delta\) is sensitivity of the output and \(\varepsilon\) is the privacy level. This mechanism ensures \(\varepsilon\)-DP.

We compose a Laplace mechanism to the target model while responding to Marich's query and evaluate the change in accuracy of the extracted model as the impact of the defense mechanism. We use a logistic regression model trained on MNIST as the target model. We query it using EMNIST and CIFAR10 datasets respectively. We vary \(\varepsilon\) in \(\{0.25,2,8,\infty\}\). For each \(\varepsilon\) and query dataset, we report the mean and standard deviation of accuracy of the extracted models on a test dataset. Each experiment is run 10 times.

We observe that decrease in \(\varepsilon\), i.e. increase in privacy, causes decrease in accuracy of the extracted model. For EMNIST queries (Figure 12(a)), the degradation in accuracy is around \(10\%\) for \(\varepsilon=2,8\) but we observe a significant drop for \(\varepsilon=0.25\). For CIFAR10 queries (Figure 12(b)), \(\varepsilon=8\) has practically no impact on the performance of the extracted model. But for \(\varepsilon=2\) and \(0.25\), the accuracy of extracted models drop down very fast.

Figure 12: Performance of models extracted by Marich against \((\varepsilon,\delta)\)-differentially private target models trained using DP-SGD. We consider different privacy levels \(\varepsilon\) and \(\delta=10^{-5}\). Accuracy of the extracted models decrease with increase in privacy (decrease in \(\epsilon\)).

Thus, we conclude that output perturbation defends privacy of the target model against Marich for smaller values of \(\varepsilon\). But for larger values of \(\varepsilon\), the privacy-preserving mechanism might not save the target model significantly against Marich.

Figure 13: Performance of models extracted by Marich against target models that perturbs the output of the queries to achieve \(\varepsilon\)-DP. We consider different privacy levels \(\varepsilon\). Accuracy of the extracted models decrease with increase in privacy (decrease in \(\epsilon\)).

[MISSING_PAGE_FAIL:33]

[MISSING_PAGE_FAIL:34]