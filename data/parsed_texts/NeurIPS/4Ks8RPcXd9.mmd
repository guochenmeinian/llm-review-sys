# Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms

Peiyao Xiao\({}^{1}\), Hao Ban\({}^{2}\), Kaiyi Ji\({}^{1}\)

\({}^{1}\)Department of CSE, University at Buffalo

\({}^{2}\) Zuoyi Technology

{peiyaoxi,kaiyiji}@buffalo.edu, bhsimon0810@gmail.com

First two authors contributed equally.

###### Abstract

Multi-objective optimization (MOO) has become an influential framework in many machine learning problems with multiple objectives such as learning with multiple criteria and multi-task learning (MTL). In this paper, we propose a new direction-oriented multi-objective formulation by regularizing the common descent direction within a neighborhood of a direction that optimizes a linear combination of objectives such as the average loss in MTL or a weighted loss that places higher emphasis on some tasks than the others. This formulation includes GD and MGDA as special cases, enjoys the direction-oriented benefit as in CAGrad, and facilitates the design of stochastic algorithms. To solve this problem, we propose Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad) with simple SGD type of updates, and its variant SDMGrad-OS with an efficient objective sampling. We develop a comprehensive convergence analysis for the proposed methods with different loop sizes and regularization coefficients. We show that both SDMGrad and SDMGrad-OS achieve improved sample complexities to find an \(\epsilon\)-accurate Pareto stationary point while achieving a small \(\epsilon\)-level distance toward a conflict-avoidant (CA) direction. For a constant-level CA distance, their sample complexities match the best known \(\mathcal{O}(\epsilon^{-2})\) without bounded function value assumption. Extensive experiments show that our methods achieve competitive or improved performance compared to existing gradient manipulation approaches in a series of tasks on multi-task supervised learning and reinforcement learning. Code is available at https://github.com/ml-opt-lab/sdmgrad.

## 1 Introduction

In recent years, multi-objective optimization (MOO) has drawn intensive attention in a wide range of applications such as online advertising [1], hydrocarbon production [2], autonomous driving [3], safe reinforcement learning [4], etc. In this paper, we focus on the stochastic MOO problem, which takes the formulation of

\[\min_{\theta\in\mathbb{R}^{m}}\bm{L}(\theta):=(\mathbb{E}_{\xi}[L_{1}(\theta; \xi)],\mathbb{E}_{\xi}[L_{2}(\theta;\xi)],...,\mathbb{E}_{\xi}[L_{K}(\theta; \xi)]),\] (1)

where \(m\) is the parameter dimension, \(K\geq 2\) is the number of objectives and \(L_{i}(\theta):=\mathbb{E}_{\xi}L_{i}(\theta;\xi)\). One important example of MOO in eq.1 is the multi-task learning (MTL) [5, 6], whose objectivefunction is often regarded as the average loss over \(K\) objectives.

\[\theta^{*}=\arg\min_{\theta\in\mathbb{R}^{n}}\Big{\{}\bar{L}(\theta)\triangleq \frac{1}{K}\sum_{i=1}^{K}L_{i}(\theta)\Big{\}},\] (2)

where \(L_{i}(\theta)\) is the loss function for task \(i\). However, solving the MOO problem is challenging because it can rarely find a common parameter \(\theta\) that minimizes all individual objective functions simultaneously. As a result, a widely-adopted target is to find the _Pareto stationary point_ at which there is no common descent direction for all objective functions. In this context, a variety of gradient manipulation methods have been proposed, including the multiple gradient descent algorithm (MGDA) [7], PCGrad [8], CAGrad [9] and Nash-MTL [10]. Among them, MGDA updates the model parameter \(\theta\) using a time-varying multi-gradient, which is a convex combination of gradients for all objectives. CAGrad further improves MGDA by imposing a new constraint on the difference between the common descent direction and the average gradient to ensure convergence to the minimum of the average loss of MTL. However, these approaches mainly focus on the deterministic case, and their stochastic versions still remain under-explored.

Stochastic MOO has not been well understood except for several attempts recently. Inspired by MGDA, [11] proposed stochastic multi-gradient (SMG), and established its convergence with convex objectives. However, their analysis requires the batch size to increase linearly. In the more practical nonconvex setting, [12] proposed a correlation-reduced stochastic multi-objective gradient manipulation (CR-MOGM) to address the non-convergence issue of MGDA, CAGrad and PCGrad in the stochastic setting. However, their analysis requires a restrictive assumption on the bounded function value. Toward this end, [13] recently proposed a method named MoCo as a stochastic counterpart of MGDA by introducing a tracking variable to approximate the stochastic gradient. However, their analysis requires that the number \(T\) of iterations is big at an order of \(K^{10}\), where \(K\) is the number of objectives, and hence may be unsuitable for the scenario with many objectives. Thus, it is highly demanding but still challenging to develop efficient stochastic MOO methods with guaranteed convergence in the nonconvex setting under mild assumptions.

### Our Contributions

Motivated by the limitations of existing methods, we propose Stochastic Direction-oriented Multi-objective Gradient descent (SDGMGrad), which is easy to implement, flexible with a new direction-oriented objective formulation, and achieves a better convergence performance under milder assumptions. Our specific contributions are summarized as follows.

**New direction-oriented formulation.** We propose a new MOO objective (see eq. (6)) by regularizing the common descent direction \(d\) within a neighborhood of a direction \(d_{0}\) that optimizes a

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline
**Algorithms** & \begin{tabular}{c} Batch \\ size \\ \end{tabular} & Nonconvex & \begin{tabular}{c} Bounded \\ function value \\ \end{tabular} & 
\begin{tabular}{c} Sample \\ complexity \\ \end{tabular} & CA distance \\ \hline \hline SMG [11] & \(\mathcal{O}(\epsilon^{-2})\) & ✗ & ✗ & \(\mathcal{O}(\epsilon^{-4})\) & N/A \\ \hline CR-MOGM [12] & \(\mathcal{O}(1)\) & ✓ & ✓ & \(\mathcal{O}(\epsilon^{-2})\) & N/A \\ \hline MoCo [13] & \(\mathcal{O}(1)\) & ✓ & ✓ & \(\mathcal{O}(\epsilon^{-2})\) & N/A \\ \hline MoDo [14] & \(\mathcal{O}(1)\) & ✓ & ✗ & \(\mathcal{O}(\epsilon^{-2})\) & \(\mathcal{O}(1)\) \\ \hline SDGMGrad (Theorem 3) & \(\mathcal{O}(1)\) & ✓ & ✗ & \(\mathcal{O}(\epsilon^{-2})\) & \(\mathcal{O}(1)\) \\ \hline SDGMGrad-OS (Theorem 4) & \(\mathcal{O}(1)\) & ✓ & ✗ & \(\mathcal{O}(\epsilon^{-2})\) & \(\mathcal{O}(1)\) \\ \hline \hline MoCo [13] & \(\mathcal{O}(1)\) & ✓ & ✗ & \(\mathcal{O}(\epsilon^{-10})\) & \(\mathcal{O}(\epsilon)\) \\ \hline MoDo [14] & \(\mathcal{O}(1)\) & ✓ & ✗ & \(\mathcal{O}(\epsilon^{-8})\) & \(\mathcal{O}(\epsilon)\) \\ \hline SDGMGrad (Corollary 1) & \(\mathcal{O}(1)\) & ✓ & ✗ & \(\mathcal{O}(\epsilon^{-6})\) & \(\mathcal{O}(\epsilon)\) \\ SDGMGrad-OS (Theorem 2) & \(\mathcal{O}(1)\) & ✓ & ✗ & \(\mathcal{O}(\epsilon^{-6})\) & \(\mathcal{O}(\epsilon)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of different algorithms for stochastic MOO problem to achieve an \(\epsilon\)-accurate Pareto stationary point. MoDo [14] is a concurrent work. Definition of CA distance can be found in Definition 2. \(\mathcal{O}(\cdot)\) omits logarithm factors.

linear combination \(L_{0}(\theta)\) of objectives such as the average loss in MTL or a weighted loss that places higher emphasis on some tasks than the others. This formulation is general to include gradient descent (GD) and MGDA as special cases and takes a similar direction-oriented spirit as in CAGrad [9]. However, different from the objective in CAGrad [9] that enforces the gap between \(d\) and \(d_{0}\) to be small via constraint, our regularized objective is easier to design near-unbiased multi-gradient, which is crucial in developing provably convergent stochastic MOO algorithms.

**New stochastic MOO algorithms.** The proposed SDMGrad is simple with efficient stochastic gradient descent (SGD) type of updates on the weights of individual objectives in the multi-gradient and on the model parameters with (near-)unbiased (multi-)gradient approximations at each iteration. SDMGrad does not require either the linearly-growing batch sizes as in SMG or the extra tracking variable for gradient estimation as in MoCo. Moreover, we also propose SDMGrad-OS (which refers to SDMGrad with objective sampling) as an efficient and scalable counterpart of SDMGrad in the setting with a large number of objectives. SDMGrad-OS samples a subset of data points and objectives simultaneously in all updates and is particularly suitable in large-scale MTL scenarios.

**Convergence analysis and improved complexity.** We provide a comprehensive convergence analysis of SDMGrad for stochastic MOO with smooth nonconvex objective functions. For a constant-level regularization parameter \(\lambda\) (which covers the MGDA case), when an \(\epsilon\)-level conflict-avoidant (CA) distance (see Definition 2) is required, the sample complexities (i.e., the number of samples to achieve an \(\epsilon\)-accurate Pareto stationary point) of the proposed SDMGrad and SDMGrad-OS improve those of MoCo [13] and MoDo [14] by an order of \(\epsilon^{-4}\) and \(\epsilon^{-2}\) (see Table 1), respectively. In addition, when there is no requirement on CA distance, the sample complexities of SDMGrad and SDMGrad-OS are improved to \(\mathcal{O}(\epsilon^{-2})\), matching the existing best result in the concurrent work [14]. For an increasing \(\lambda\), we also show this convergent point reduces to a stationary point of the linear combination \(L_{0}(\theta)\) of objectives.

**Promising empirical performance.** We conduct extensive experiments in multi-task supervised learning and reinforcement learning on multiple datasets and show that SDMGrad can achieve competitive or improved performance compared to existing state-of-the-art gradient manipulation methods such as MGDA, PCGrad, GradDrop, CAGrad, IMTL-G, MoCo, MoDo, Nash-MTL and FAMO, and can strike a better performance balance on different tasks. SDMGrad-OS also exhibits a much better efficiency than SDMGrad in the setting with a large number of tasks due to the efficient objective sampling.

## 2 Related Works

**Multi-task learning.** One important application of MOO is MTL, whose target is to learn multiple tasks with possible correlation simultaneously. Due to this capability, MTL has received significant attention in various applications in computer vision, natural language process, robotics, and reinforcement learning [5; 15; 16; 17]. A group of studies have focused on how to design better MTL model architectures. For example, [18; 19; 20] enhance the MTL models by introducing task-specific modules, an attention mechanism, and different activation functions for different tasks, respectively. Another line of research aims to learn a bunch of smaller models of local tasks split from the original problem, which are then aggregated into a single model via knowledge distillation [21]. Recent several works [22; 23] have explored the connection between MTL and gradient-based meta-learning. [24] and [25] highlight the importance of task grouping and unrelated tasks in MTL, respectively. This paper focuses on a single model by learning multiple tasks simultaneously with novel model-agnostic SDMGrad and SDMGrad-OS methods.

**Gradient-based MOO.** Various gradient manipulation methods have been developed to learn multiple tasks simultaneously. One popular class of approaches re-weight different objectives based on uncertainty [26], gradient norm [27], and training difficulty [28]. MOO-based approaches have received more attention due to their principled designs and training stability. For example, [6] viewed MTL as a MOO problem and proposed an MGDA-type method for optimization. A class of approaches has been proposed to address the gradient conflict problem. Among them, [8] proposed PCGrad by projecting the gradient direction of each task on the norm plane of other tasks. GradDrop randomly dropped out highly conflicted gradients [29], and RotoGrad rotated task gradients to alleviate the conflict [30]. [9] proposed CAGrad by constraining the common direction direction within a local region around the average gradient. These approaches mainly focus on the deterministic setting. In the stochastic case, [13] proposed MoCo as a stochastic counterpart of MGDA, and provided a comprehensive convergence and complexity analysis. More recently, a concurrent work [14] analyzed a three-way trade-off among optimization, generalization, and conflict avoidance, providing an impact on designing the MOO algorithm. In addition, [31] found that scalarization SGD could be incapable of fully exploring the Pareto front compared with MGDA-variant methods. In this paper, we propose a new stochastic MOO method named SDMGrad, which benefits from a direction-oriented regularization and an improved convergence and complexity performance.

**Concurrent work.** A concurrent work [14] proposed a multi-objective approach named MoDo, which uses a similar double sampling strategy (see Section 4.2). However, there are still several differences between this work and [14]. First, our method benefits from a direction-oriented mechanism and is general to include MGDA and CAGrad as special cases, whereas MoDo [14] does not have such features. Second, MoDo takes a single-loop structure, whereas our SDMGrad features a double-loop scheme with an improved sample complexity. Third, [14] focuses more on the trade-off among optimization, generalization and conflict-avoidance, whereas our work focuses more on the optimization efficiency and performance balance in theory and in experiments.

## 3 Preliminaries

### Pareto Stationarity in MOO

Differently from single-objective optimization, MOO aims to find points at which all objectives cannot be further optimized. Consider two points \(\theta\) and \(\theta^{\prime}\). It is claimed that \(\theta\) dominates \(\theta^{\prime}\) if \(L_{i}(\theta)\leq L_{i}(\theta^{\prime})\) for all \(i\in[K]\) and \(L(\theta)\neq L(\theta^{\prime})\). In the general nonconvex setting, MOO aims to find a Pareto stationary point \(\theta\), at which there is no common descent direction for all objectives. In other words, we say \(\theta\) is a Pareto stationary point if \(\text{range}(\nabla L(\theta))\cap(-\mathbb{R}^{K}_{++})=\emptyset\) where \(\mathbb{R}^{K}_{++}\) is the positive orthant cone.

### MGDA and Its Stochastic Variants

**Deterministic MGDA.** The deterministic MGDA algorithm was first studied by [7], which updates the model parameter \(\theta\) along a multi-gradient \(d=\sum_{i=1}^{K}w_{i}^{*}g_{i}(\theta)=G(\theta)w^{*}\), where \(G(\theta)=(g_{1}(\theta),g_{2}(\theta),...,g_{K}(\theta))\) are the gradients of different objectives, and \(w^{*}:=(w_{1}^{*},w_{2}^{*},...,w_{K}^{*})^{T}\) are the weights of different objectives obtained via solving the following problem.

\[w^{*}\in\arg\min_{w}\|G(\theta)w\|^{2}\ \ s.t.\ \ w\in\mathcal{W}:=\{w\in \mathbb{R}^{K}|\mathbf{1}^{T}w=1,w\geq 0\},\] (3)

where \(\mathcal{W}\) is the probability simplex. The deterministic MGDA and its variants such as PCGrad and CAGrad have been well studied, but their stochastic counterparts have not been understood well.

**Stochastic MOO algorithms.** SMG [11] is the first stochastic variant of MGDA by replacing the full gradient \(G(\theta)\) in eq. (3) by its stochastic gradient \(G(\theta;\xi):=(g_{1}(\theta;\xi),...,g_{K}(\theta;\xi))\), and updates the model parameters \(\theta\) along the direction given by

\[d_{\xi}=G(\theta;\xi)w_{\xi}^{*}\ \ s.t.\ \ w_{\xi}^{*}\in\arg\min_{w}\|G( \theta;\xi)w\|^{2}.\]

However, this direct replacement can introduce **biased** multi-gradient estimation, and hence SMG required to increase the batch sizes linearly with the iteration number. To address this limitation, [13] proposed MoCo by introducing an additional tracking variable \(y_{t,i}\) as the stochastic estimate of the gradient \(\nabla L_{i}(\theta)\), which is iteratively updated via

\[y_{t+1,i}=\Pi_{\mathcal{Y}_{i}}\big{(}y_{t,i}-\beta_{t}(y_{t,i}-h_{t,i})\big{)},i=1,2,...,K,\] (4)

where \(\beta_{t}\) is the step size, \(\Pi_{\mathcal{Y}_{i}}\) denotes the projection on a bounded set \(\mathcal{Y}_{i}=\{y\in\mathbb{R}^{m}\|y\|\leq C_{y,i}\}\) for some constant \(C_{y,i}\), and \(h_{t,i}\) denotes stochastic estimator of \(\nabla L_{i}(\theta)\) at t-th iteration. Then, MoCo was shown to achieve an asymptotically unbiased multi-gradient, but with a relatively strong assumption that the number \(T\) of iterations is much larger than the number \(K\) of objectives. Thus, it is important but still challenging to develop provable and easy-to-implement stochastic MOO algorithms with mild assumptions.

Our Method

We first provide a new direction-oriented MOO problem, and then introduce a new stochastic MOO algorithm named SDMGrad and its variant SDMGrad-OS with objective sampling.

### Direction-oriented Multi-objective Optimization

MOO generally targets at finding a direction \(d\) to maximize the minimum decrease across all objectives via solving the following problem.

\[\max_{d\in\mathbb{R}^{m}}\min_{i\in[K]}\Big{\{}\frac{1}{\alpha}(L_{i}(\theta)- L_{i}(\theta-\alpha d))\Big{\}}\approx\max_{d\in\mathbb{R}^{m}}\min_{i\in[K]} \langle g_{i},d\rangle,\] (5)

where the first-order Taylor approximation of \(L_{i}\) is applied at \(\theta\) with a small stepsize \(\alpha\).

In some scenarios, the target is to not only find the above common descent direction but also optimize a specific objective that is often a linear combination \(L_{0}(\theta)=\sum_{i}\widetilde{w}_{i}L_{i}(\theta)\) for some \(\widetilde{w}\in\mathcal{W}\) of objectives. For instance, MTL often takes the averaged loss over tasks as the objective function, and every element in \(\widetilde{w}\) will be set as \(\frac{1}{K}\). In addition, it is quite possible that there is a preference for tasks. In this case, motivated by the framework in [32], we can regard \(\widetilde{w}\) as a preference vector and tend to approach a preferred stationary point along this direction. To address this problem, we propose the following multi-objective problem formulation by adding an inner-product regularization \(\lambda\langle g_{0},d\rangle\) in eq. (5) such that the common descent direction \(d\) stays not far away from a target direction \(g_{0}=\sum_{i}\widetilde{w}_{i}g_{i}\).

\[\max_{d\in\mathbb{R}^{m}}\min_{i\in[K]}\langle g_{i},d\rangle- \frac{1}{2}\|d\|^{2}+\lambda\langle g_{0},d\rangle.\] (6)

In the above eq. (6), the term \(-\frac{1}{2}\|d\|^{2}\) is used to regularize the magnitude of the direction \(d\) and the constant \(\lambda\) controls the distance between the update vector \(d\) and the target direction \(g_{0}\).

Compared to CAGrad.We note that CAGrad also takes a direction-oriented objective but uses a different constraint-enforced formulation as follows.

\[\max_{d\in\mathbb{R}^{m}}\min_{i\in[K]}\langle g_{i},d\rangle\ \ \text{s.t.}\ \ \|d-h_{0}\|\leq c\|h_{0}\|\] (7)

where \(h_{0}\) is the average gradient and \(c\in[0,1)\) is a constant. To optimize eq. (7), CAGrad involves the evaluations of the product \(\|h_{0}\|\|g_{w}\|\) and the relation \(\frac{\|h_{0}\|}{|g_{w}|}\) with \(g_{w}:=\sum_{i}w_{i}g_{i}\), both of which complicate the designs of unbiased stochastic gradient/multi-gradient in the \(w\) and \(\theta\) updates. As a comparison, our formulation in eq. (6), as shown later, admits very simple and provable stochastic algorithmic designs, while still enjoying the direction-oriented benefit as in CAGrad.

To efficiently solve the problem in eq. (6), we then substitute the relation that \(\min_{i\in[K]}\langle g_{i},d\rangle=\min_{w\in\mathcal{W}}\langle\sum_{i=1}^{ K}g_{i}w_{i},d\rangle=\min_{w\in\mathcal{W}}\langle g_{w},d\rangle\) into eq. (6), and obtain an equivalent problem as

\[\max_{d\in\mathbb{R}^{m}}\min_{w\in\mathcal{W}}\langle g_{w}+ \lambda g_{0},d\rangle-\frac{1}{2}\|d\|^{2}.\]

By switching min and max in the above problem, which does not change the solution due to the concavity in \(d\) and the convexity in \(w\), we finally aim to solve \(\min_{w\in\mathcal{W}}\max_{d\in\mathbb{R}^{m}}\langle g_{w}+\lambda g_{0},d \rangle-\frac{1}{2}\|d\|^{2}\), where the solution to the min problem on \(w\) is

\[w_{\lambda}^{*}\in\arg\min_{w\in\mathcal{W}}\frac{1}{2}\|g_{w}+ \lambda g_{0}\|^{2},\] (8)

and the solution to the max problem is \(d^{*}=g_{w_{\lambda}^{*}}+\lambda g_{0}\).

Connection with MGDA and GD.It can be seen from eq. (8) that the updating direction \(d^{*}\) reduces to that of MGDA whenwe set \(\lambda=0\), and is consistent with that of GD for \(\lambda\) large enough. This consistency is also validated empirically in Appendix A.2 by varying \(\lambda\).

### Proposed SDMGrad Algorithm

The natural idea to solve the problem in eq. (8) is to use a simple SGD-type method. The detailed steps are provided in algorithm 1. At each iteration \(t\), we run \(S\) steps of projected SGD with warm-start initialization and with a double-sampling-based stochastic gradient estimator, which is unbiased by noting that

\[\mathbb{E}_{\xi,\xi^{\prime}}[G(\theta_{t};\xi)^{T}\big{(}G(\theta_{t};\xi^{ \prime})w_{t,s}+\lambda g_{0}(\theta_{t};\xi^{\prime})\big{)}]=G(\theta_{t})^{ T}\big{(}G(\theta_{t})w_{t,s}+\lambda g_{0}(\theta_{t})\big{)},\]

where \(\xi,\xi^{\prime}\) are sampled data and \(g_{0}(\theta_{t})=G(\theta_{t})\tilde{w}\) denotes the orientated direction. After obtaining the estimate \(w_{t,S}\), SDMGrad updates the model parameters \(\theta_{t}\) based on the stochastic counterpart \(G(\theta_{t};\zeta)w_{t,S}+\lambda g_{0}(\theta_{t};\zeta)\) of the direction \(d^{*}=g_{w_{1}^{*}}+\lambda g_{0}\) with a stepsize of \(\alpha_{t}\). It can be seen that SDMGrad is simple to implement with efficient SGD type of updates on both \(w\) and \(\theta\) without introducing other auxiliary variables.

```
1:Initialize: model parameters \(\theta_{0}\) and weights \(w_{0}\)
2:for\(t=0,1,...,T-1\)do
3:for\(s=0,1,...,S-1\)do
4: Set \(w_{t,0}=w_{t-1,S}\) (warm start) and sample data \(\xi,\xi^{\prime}\)
5:\(w_{t,s+1}=\Pi_{\mathcal{W}}\big{(}w_{t,s}-\beta_{t,s}[G(\theta_{t};\xi)^{T} \big{(}G(\theta_{t};\xi^{\prime})w_{t,s}+\lambda g_{0}(\theta_{t};\xi^{\prime} )\big{)}]\big{)}\)
6:endfor
7: Sample data \(\zeta\) and \(\theta_{t+1}=\theta_{t}-\alpha_{t}\big{(}G(\theta_{t};\zeta)w_{t,S}+\lambda g_ {0}(\theta_{t};\zeta)\big{)}\)
8:endfor ```

**Algorithm 1** Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad)

### SDMGrad with Objective Sampling (SDMGrad-OS)

Another advantage of SDMGrad is its simple extension via objective sampling to the more practical setting with a large of objectives, e.g., in large-scale MTL. In this setting, we propose SDMGrad-OS by replacing the gradient matrix \(G(\theta_{t};\xi)\) in Algorithm 1 by a matrix \(H(\theta_{t};\xi)\) with randomly sampled columns, which takes the form of

\[H(\theta_{t};\xi,\mathcal{S})=\big{(}h_{1}(\theta_{t};\xi),h_{2}(\theta_{t}; \xi),...,h_{K}(\theta_{t};\xi)\big{)},\] (9)

where \(h_{i}(\theta_{t};\xi)=g_{i}(\theta_{t};\xi)\) with a probability of \(n/K\) or 0 otherwise, \(\mathcal{S}\) corresponds to the randomness by objective sampling, and \(n\) is the expected number of sampled objectives. Then, the stochastic gradient in the \(w\) update is adapted to

\[(\text{Gradient on }w)\quad\frac{K^{2}}{n^{2}}H(\theta_{t};\xi, \mathcal{S})^{T}\big{(}H(\theta_{t};\xi^{\prime},\mathcal{S}^{\prime})w_{t,s} +\lambda h_{0}(\theta_{t};\xi^{\prime},\mathcal{S}^{\prime})\big{)},\]

where \(h_{0}(\cdot)=H(\cdot)\tilde{w}\). Similarly, the updating direction \(d=\frac{K}{n}H(\theta_{t};\zeta,\widetilde{\mathcal{S}})w_{t,S}+\frac{K}{n} \lambda h_{0}(\theta_{t};\zeta,\widetilde{\mathcal{S}})\). In the practical implementation, we first use _np.random.binomial_ to sample a \(0\)-\(1\) sequence \(s\) following a Bernoulli distribution with length \(K\) and probability \(n/K\), and then compute the gradient \(g_{i}(\theta;\xi)\) of \(G(\theta;\xi)\) only if the \(i^{th}\) entry of \(s\) equals to \(1\). This sampling strategy greatly speeds up the training with a large number of objectives, as shown by the reinforcement learning experiments in Section 6.3.

## 5 Main results

### Definitions and Assumptions

We first make some standard definitions and assumptions, as also adopted by existing MOO studies in [11; 12; 13]. Since we focus on the practical setting where the objectives are nonconvex, algorithms are often expected to find an \(\epsilon\)-accurate Pareto stationary point, as defined below.

**Definition 1**.: _We say \(\theta\in\mathbb{R}^{m}\) is an \(\epsilon\)-accurate Pareto stationary point if \(\mathbb{E}\big{[}\min_{w\in\mathcal{W}}\|g_{w}(\theta)\|^{2}\big{]}\leq\epsilon\) where \(\mathcal{W}\) is the probability simplex._

MGDA-variant methods find a direction \(d(\theta)\) (e.g., \(d^{*}=g_{w_{\lambda}^{*}}+\lambda g_{0}\) with \(w_{\lambda}^{*}\) given by eq. (8)) that tends to optimize all objective functions simultaneously, which is called a conflict-avoidant (CA)direction [14]. Thus, it is important to measure the distance between a stochastic direction estimate \(\widehat{d}(\theta)\) and the CA direction, which we call as CA distance, as defined below.

**Definition 2**.: \(\|\mathbb{E}_{\widehat{d}(\theta)}[\widehat{d}(\theta)]-d(\theta)\|\) _denotes the CA distance._

The following assumption imposes the Lipschitz continuity on the objectives and their gradients.

**Assumption 1**.: _For every task \(i\in[K],\ L_{i}(\theta)\) is \(l_{i}\)-Lipschitz continuous and \(\nabla L_{i}(\theta)\) is \(l_{i,1}\)-Lipschitz continuous for any \(\theta\in\mathbb{R}^{m}\)._

We next make an assumption on the bias and variance of the stochastic gradient \(g_{i}(\theta;\xi)\).

**Assumption 2**.: _For every task \(i\in[K]\), the gradient \(g_{i}(\theta;\xi)\) is the unbiased estimate of \(g_{i}(\theta)\). Moreover, the gradient variance is bounded by \(\mathbb{E}_{\xi}[\|g_{i}(\theta;\xi)-g_{i}(\theta)\|^{2}]\leq\sigma_{i}^{2}\)._

**Assumption 3**.: _Assume there exists a constant \(C_{g}>0\) such that \(\|G(\theta)\|\leq C_{g}\)._

The bounded gradient condition in Assumption 3 is necessary to ensure the boundedness of the multi-gradient estimation error, as also adopted by [12, 13].

### Convergence Analysis with Nonconvex Objectives

We first upper-bound the CA distance for our proposed method.

**Proposition 1**.: _Suppose Assumptions 1-3 are satisfied. If we choose \(\beta_{t,s}=c/\sqrt{s}\) and \(S>1\), then_

\[\|\mathbb{E}_{\zeta,w_{t,S}|\theta_{t}}[G(\theta_{t};\zeta)w_{t,S}+\lambda g_ {0}(\theta_{t};\zeta)]-G(\theta_{t})w_{t,\lambda}^{*}-\lambda g_{0}(\theta_{t} )\|\leq\sqrt{\Big{(}\frac{2}{c}+2cC_{1}\Big{)}\frac{2+log(S)}{\sqrt{S}}},\]

_where \(C_{1}=\mathcal{O}(K(1+\lambda))\) and \(c\) is a constant._

Proposition 1 shows that CA distance is decreasing with the number \(S\) of iterations on \(w\) updates. Then, by selecting a properly large \(S\), we obtain the following general convergence result.

**Theorem 1** (SDMGrad).: _Suppose Assumption 1-3 are satisfied. Set \(\alpha_{t}=\alpha=\Theta((1+\lambda)^{-1}K^{-\frac{1}{2}}T^{-\frac{1}{2}})\), \(\beta_{t,s}=c/\sqrt{s}\), where \(c\) is a constant, and \(S=\Theta((1+\lambda)^{-2}T^{2})\). Then the outputs of the proposed SDMGrad algorithm satisfy_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t,\lambda}^{*}+ \lambda g_{0}(\theta_{t})\|^{2}]=\widetilde{\mathcal{O}}((1+\lambda^{2})K^{ \frac{1}{2}}T^{-\frac{1}{2}}),\] (10)

_where \(\widetilde{\mathcal{O}}\) omits the order of \(\log T\)._

Theorem 1 establishes a general convergence guarantee for SDMGrad along the multi-gradient direction \(d^{*}=g_{w_{\lambda}^{*}}+\lambda g_{0}\). Building on Theorem 1, we next show that with different choices of the regularization parameter \(\lambda\), two types of convergence results can be obtained in the following two corollaries.

**Corollary 1** (Constant \(\lambda\)).: _Under the same setting as in Theorem 1, choosing a constant-level \(\lambda>0\), we have \(\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t}^{*}\|^{2}]= \widetilde{\mathcal{O}}(K^{\frac{1}{2}}T^{-\frac{1}{2}})\), where \(w_{t}^{*}\in\operatorname*{arg\,min}_{w\in V}\frac{1}{2}\|G(\theta_{t})w\|^{2}\). To achieve an \(\epsilon\)-accurate Pareto stationary point, each objective requires \(\mathcal{O}(K^{3}\epsilon^{-6})\) samples in \(\xi\) (\(\xi^{\prime}\)) and \(\mathcal{O}(K\epsilon^{-2})\) samples in \(\zeta\), respectively. Meanwhile, the CA distance takes the order of \(\widetilde{\mathcal{O}}(\epsilon)\)._

Corollary 1 covers the MGDA case when \(\lambda=0\). In this setting, the sample complexity \(\mathcal{O}(\epsilon^{-6})\) improves those of MoCo [13] and MoDo [14] by an order of \(\epsilon^{-4}\) and \(\epsilon^{-2}\), respectively, while achieving an \(\epsilon\)-level CA distance.

**Corollary 2** (Increasing \(\lambda\)).: _Under the same setting as in Theorem 1 and choosing \(\lambda=\Theta(T^{\frac{1}{2}})\), we have \(\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|g_{0}(\theta_{t})\|^{2}]=\mathcal{O}( K^{\frac{1}{2}}T^{-\frac{1}{2}}).\) To achieve an \(\epsilon\)-accurate stationary point, each objective requires \(\mathcal{O}(K^{2}\epsilon^{-4})\) samples in \(\xi\) (\(\xi^{\prime}\)) and \(\mathcal{O}(K\epsilon^{-2})\) samples in \(\zeta\), respectively. Meanwhile, the CA distance takes the order of \(\widetilde{\mathcal{O}}(\sqrt{K})\)._Corollary 2 analyzes the case with an increasing \(\lambda=\Theta(T^{\frac{1}{2}})\). We show that SDMGrad converges to a stationary point of the objective \(L_{0}(\theta)=\sum_{i}\widetilde{w}_{i}L_{i}(\theta)\) with an improved sample complexity, but with a worse constant-level CA distance. This justifies the flexibility of our framework that the \(\lambda\) can balance the worst local improvement of individual objectives and the target objective \(L_{0}(\theta)\).

**Convergence under objective sampling.** We provide a convergence analysis for SDMGrad-OS.

**Theorem 2** (SDMGrad-OS).: _Suppose Assumption 1-3 are satisfied. Define \(\gamma=\frac{K}{n}\), \(\alpha_{t}=\alpha=\Theta((1+\lambda^{2})^{-\frac{1}{2}}\gamma^{-\frac{1}{2}}K^ {-\frac{1}{2}}T^{-\frac{1}{2}})\), \(\beta_{t,s}=c/\sqrt{s}\) where \(c\) is a constant, and \(S=\Theta((1+\lambda^{2})^{-2}\gamma^{-2}T^{2})\). Then, by choosing a constant \(\lambda\), the iterates of the proposed SDMGrad-OS algorithm satisfy_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t}^{*}\|^{2}]=\widetilde {\mathcal{O}}(K^{\frac{1}{2}}\gamma^{\frac{1}{2}}T^{-\frac{1}{2}}).\]

Theorem 2 establishes the convergence guarantee for our SDMGrad-OS algorithm, which achieves a per-objective sample complexity of \(\mathcal{O}(\epsilon^{-6})\) comparable to that of SDMGrad, but with a much better efficiency due to the objective sampling, as also validated by the empirical comparison in Table 4.

### Lower sample complexity but constant-level CA distance

Without the requirement on the \(\epsilon\)-level CA distance, we further improve the sample complexity of our method to \(\widetilde{\mathcal{O}}(\epsilon^{-2})\), as shown in the following theorem.

**Theorem 3**.: _Suppose Assumptions 1-3 are satisfied. Set \(S=1\), \(\alpha_{t}=\alpha=\Theta(K^{-\frac{1}{2}}T^{-\frac{1}{2}})\), \(\beta_{t}=\beta=\Theta(K^{-1}T^{-\frac{1}{2}})\) and \(\lambda\) as constant. The iterates of the proposed SDMGrad satisfy_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t}^{*}\|^{2}]=\mathcal{ O}(KT^{-\frac{1}{2}}).\]

Theorem 3 shows that to achieve an \(\epsilon\)-accurate Pareto stationary point, our method requires \(T=\mathcal{O}(K^{2}\epsilon^{-2})\). In this case, each objective requires a number \(\mathcal{O}(K^{2}\epsilon^{-2})\) of samples in \(\xi(\xi^{\prime})\) and \(\zeta\).

**Convergence under objective sampling.** We next analyze the convergence of SDMGrad-OS.

**Theorem 4**.: _Suppose Assumptions 1-3 are satisfied. Set \(S=1\), \(\gamma=\frac{K}{n}\), \(\alpha_{t}=\alpha=\Theta(K^{-\frac{1}{2}}\gamma^{-\frac{1}{2}}T^{-\frac{1}{2}})\), \(\beta_{t}=\beta=\Theta(K^{-1}\gamma^{-1}T^{-\frac{1}{2}})\) and \(\lambda\) as a constant. The iterates of the proposed SDMGrad-OS algorithm satisfy_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t}^{*}\|^{2}]= \mathcal{O}(K\gamma T^{-\frac{1}{2}}).\]

Theorem 4 shows that to achieve an \(\epsilon\)-accurate Pareto stationary point, our algorithm requires \(T=\mathcal{O}(\gamma^{2}K^{2}\epsilon^{-2})\), and each objective requires a number \(\mathcal{O}(\gamma^{2}K^{2}\epsilon^{-2})\) of samples in \(\xi(\xi^{\prime})\) and \(\zeta\).

## 6 Experiments

In this section, we first describe the implementation details of our proposed methods. Then, we demonstrate the effectiveness of the methods under a couple of multi-task supervised learning and reinforcement settings. The experimental details and more empirical results such as the two-objective toy example, consistency with GD and MGDA, and ablation studies over \(\lambda\) can be found in the Appendix A.

### Practical Implementation

**Double sampling.** In supervised learning, double sampling (i.e., drawing two samples simultaneously for gradient estimation) is employed, whereas in reinforcement learning, single sampling is used because double sampling requires to visit the entire episode twice a time, which is much more time-consuming.

**Gradient normalization and rescale.** During the training process, the gradient norms of tasks may change over time. Thus, directly solving the objective in eq. (8) may trigger numerical problems.

Inspired by CAGrad [9], we normalize the gradient of each task and rescale the final update \(d\) by multiplying a factor of \(\frac{1}{1+\lambda}\) to stabilize the training.

**Projected gradient descent.** The computation of the projection to the probability simplex we use is the Euclidean projection proposed by [33], which involves solving a convex problem via quadratic programming. The implementation used in our experiments follows the repository in [34], which is very efficient in practice.

### Supervised Learning

For the supervised learning setting, we evaluate the performance on the Cityscapes [39] and NYU-v2 [40] datasets. The former dataset involves 2 pixel-wise tasks: 7-class semantic segmentation and depth estimation, and the latter one involves 3 pixel-wise tasks: 13-class semantic segmentation, depth estimation and surface normal estimation. Following the experimental setup of [9], we embed a MTL method MTAN [36] into our SDMGrad method, which builds on SegNet [41] and is empowered by a task-specific attention mechanism. We compare SDMGrad with Linear Scalarization (LS) which minimizes the average loss, Scale-invariant (SI) which minimizes the average logarithmic loss, RLW [35], DWA [36], UW [26], MGDA [7], PCGrad [8], GradDrop [29], CAGrad [9], IMTL-G [37], MoCo [13], MoDo [14], Nash-MTL [10], and FAMO [38]. Following [42; 9; 13; 10; 38], we compute two metrics reflecting the overall performance: **(1) \(\bm{\Delta}m\%\)**, the average per-task performance drop versus the single-task (STL) baseline \(b\) to assess method \(m\): \(\Delta m\%=\frac{1}{K}\sum_{k=1}^{K}(-1)^{l_{k}}(M_{m,k}-M_{b,k})/M_{b,k} \times 100\), where \(K\) is the number of metrics, \(M_{b,k}\) is the value of metric \(M_{k}\) obtained by baseline \(b\), and \(M_{m,k}\) obtained by the compared method \(m\). \(l_{k}=1\) if the evaluation metric \(M_{k}\) on task \(k\) prefers a higher value and \(0\) otherwise. **(2) Mean Rank (MR)**: the average rank of each method across all tasks.

We search the hyperparameter \(\lambda\in\{0.1,0.2,\cdots,1.0\}\) for our SDMGrad method and report the results in Table 2 and Table 3. Each experiment is repeated 3 times with different random seeds and the average is reported. It can be seen that our proposed method is able to obtain better or comparable results than the baselines, and in addition, can strike a better performance balance on multiple tasks than other baselines. For example, although MGDA achieves better results on Surface Normal, it performs the worst on both Segmentation and Depth. As a comparison, our SDMGrad method can achieve more balanced results on all tasks.

### Reinforcement Learning

For the reinforcement learning setting, we evaluate the performance on the MT10 benchmarks, which include 10 robot manipulation tasks under the Meta-World environment [43]. Following the experiment setup in [9; 13; 10], we adopt Soft Actor-Critic (SAC) [44] as the underlying training algorithm. We compare SDMGrad with Multi-task SAC [43], Multi-headed SAC [43], Multi-task SAC + Task Encoder [43], PCGrad [8], CAGrad [9], MoCo [13], Nash-MTL [10] and FAMO [38]. We search \(\lambda\in\{0.1,0.2,\cdots,1.0\}\) and provide the success rate and average training time (in sec

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Segmentation} & \multicolumn{2}{c}{Depth} & \multirow{2}{*}{MR \(\downarrow\)} & \multirow{2}{*}{\(\Delta m\%\downarrow\)} \\  & mIoU \(\uparrow\) & \multicolumn{1}{c}{Pix Acc \(\uparrow\)} & & & & \\ \hline STL & 74.01 & 93.16 & 0.0125 & 27.77 & & \\ \hline LS & 75.18 & 93.49 & 0.0155 & 46.77 & 8.50 & 22.60 \\ SI & 70.95 & 91.73 & 0.0161 & 33.83 & 11.50 & 14.11 \\ RLW [35] & 74.57 & 93.41 & 0.0158 & 47.79 & 11.25 & 24.38 \\ DWA [36] & 75.24 & 93.52 & 0.0160 & 44.37 & 8.50 & 21.45 \\ UW [26] & 72.02 & 92.85 & 0.0140 & **30.13** & 7.75 & **5.89** \\ MGDA [7] & 68.84 & 91.54 & 0.0309 & 33.50 & 12.00 & 44.14 \\ PCGrad [8] & 75.13 & 93.48 & 0.0154 & 42.07 & 8.75 & 18.29 \\ GradDrop [29] & 75.27 & 93.53 & 0.0157 & 47.54 & 7.75 & 23.73 \\ CAGGrad [9] & 75.16 & 93.48 & 0.0141 & 37.60 & 7.25 & 11.64 \\ IMTL-G [37] & 75.33 & 93.49 & 0.0135 & 38.41 & 5.25 & 11.10 \\ MoCo [13] & 75.42 & 93.55 & 0.0149 & 34.19 & 4.00 & 9.90 \\ MoDo [14] & 74.55 & 93.32 & 0.0159 & 41.51 & 10.75 & 18.89 \\ Nash-MTL [10] & **75.41** & **93.66** & **0.0129** & 35.02 & **2.75** & 6.82 \\ FAMO [38] & 74.54 & 93.29 & 0.0145 & 32.59 & 7.75 & 8.13 \\ \hline SDMGrad & 74.53 & 93.52 & 0.0137 & 34.01 & 6.25 & 7.79 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Multi-task supervised learning on Cityscapes dataset.

[MISSING_PAGE_FAIL:10]

## References

* [1] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 1930-1939, 2018.
* [2] Junyu You, William Ampomah, and Qian Sun. Development and application of a machine learning based multi-objective optimization workflow for co2-eor projects. _Fuel_, 264:116758, 2020.
* [3] Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou, Qichuan Geng, and Ruigang Yang. The apolloscape open dataset for autonomous driving and its application. _IEEE transactions on pattern analysis and machine intelligence_, 42(10):2702-2719, 2019.
* [4] Philip S Thomas, Joelle Pineau, Romain Laroche, et al. Multi-objective spibb: Seldonian offline policy improvement with safety constraints in finite mdps. _Advances in Neural Information Processing Systems_, 34:2004-2017, 2021.
* [5] Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin Dai, and Luc Van Gool. Multi-task learning for dense prediction tasks: A survey. _IEEE transactions on pattern analysis and machine intelligence_, 44(7):3614-3633, 2021.
* [6] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. _Advances in neural information processing systems_, 31, 2018.
* [7] Jean-Antoine Desideri. Multiple-gradient descent algorithm (mgda) for multiobjective optimization. _Comptes Rendus Mathematique_, 350(5-6):313-318, 2012.
* [8] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. _Advances in Neural Information Processing Systems_, 33:5824-5836, 2020.
* [9] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent for multi-task learning. _Advances in Neural Information Processing Systems_, 34:18878-18890, 2021.
* [10] Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron, Kenji Kawaguchi, Gal Chechik, and Ethan Fetaya. Multi-task learning as a bargaining game. _arXiv preprint arXiv:2202.01017_, 2022.
* [11] Suyun Liu and Luis Nunes Vicente. The stochastic multi-gradient algorithm for multi-objective optimization and its application to supervised machine learning. _Annals of Operations Research_, pages 1-30, 2021.
* [12] Shiji Zhou, Wenpeng Zhang, Jiyan Jiang, Wenliang Zhong, Jinjie Gu, and Wenwu Zhu. On the convergence of stochastic multi-objective gradient manipulation and beyond. _Advances in Neural Information Processing Systems_, 35:38103-38115, 2022.
* [13] Heshan Devaka Fernando, Han Shen, Miao Liu, Subhajit Chaudhury, Keerthiram Murugesan, and Tianyi Chen. Mitigating gradient bias in multi-objective learning: A provably convergent approach. In _The Eleventh International Conference on Learning Representations_, 2023.
* [14] Lisha Chen, Heshan Fernando, Yiming Ying, and Tianyi Chen. Three-way trade-off in multi-objective learning: Optimization, generalization and conflict-avoidance. _arXiv preprint arXiv:2305.20057_, 2023.
* [15] Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. A joint many-task model: Growing a neural network for multiple nlp tasks. _arXiv preprint arXiv:1611.01587_, 2016.
* [16] Sebastian Ruder. An overview of multi-task learning in deep neural networks. _arXiv preprint arXiv:1706.05098_, 2017.

* [17] Yu Zhang and Qiang Yang. A survey on multi-task learning. _IEEE Transactions on Knowledge and Data Engineering_, 34(12):5586-5609, 2021.
* [18] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for multi-task learning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3994-4003, 2016.
* [19] Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. _arXiv preprint arXiv:1711.01239_, 2017.
* [20] Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task reinforcement learning with soft modularization. _Advances in Neural Information Processing Systems_, 33:4767-4777, 2020.
* [21] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* [22] Haoxiang Wang, Han Zhao, and Bo Li. Bridging multi-task learning and meta-learning: Towards efficient training and effective adaptation. In _International Conference on Machine Learning (ICML)_, pages 10991-11002. PMLR, 2021.
* [23] Feiyang Ye, Baijiong Lin, Zhixiong Yue, Pengxin Guo, Qiao Xiao, and Yu Zhang. Multi-objective meta learning. _Advances in Neural Information Processing Systems (NeurIPS)_, 34:21338-21351, 2021.
* [24] Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Efficiently identifying task groupings for multi-task learning. _Advances in Neural Information Processing Systems (NeurIPS)_, 34:27503-27516, 2021.
* [25] Elliot Meyerson and Risto Miikkulainen. The traveling observer model: Multi-task learning through spatial variable embeddings. _arXiv preprint arXiv:2010.02354_, 2020.
* [26] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7482-7491, 2018.
* [27] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In _International conference on machine learning_, pages 794-803. PMLR, 2018.
* [28] Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task prioritization for multitask learning. In _Proceedings of the European conference on computer vision (ECCV)_, pages 270-287, 2018.
* [29] Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, and Dragomir Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign dropout. _Advances in Neural Information Processing Systems_, 33:2039-2050, 2020.
* [30] Adrian Javaloy and Isabel Valera. Rotograd: Gradient homogenization in multitask learning. _arXiv preprint arXiv:2103.02631_, 2021.
* [31] Yuzheng Hu, Ruicheng Xian, Qilong Wu, Qiuling Fan, Lang Yin, and Han Zhao. Revisiting scalarization in multi-task learning: A theoretical perspective. _arXiv preprint arXiv:2308.13985_, 2023.
* [32] Michinari Momma, Chaosheng Dong, and Jia Liu. A multi-objective/multi-task learning framework induced by pareto stationarity. In _International Conference on Machine Learning_, pages 15895-15907. PMLR, 2022.
* [33] Weiran Wang and Miguel A. Carreira-Perpinan. Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application, 2013.
* [34] Adrien Gaidon. Compute euclidean projections on the simplex or l1-ball. GitHub, 2011.

* [35] Baijiong Lin, Feiyang Ye, Yu Zhang, and Ivor W Tsang. Reasonable effectiveness of random weighting: A litmus test for multi-task learning. _arXiv preprint arXiv:2111.10603_, 2021.
* [36] Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1871-1880, 2019.
* [37] Liyang Liu, Yi Li, Zhanghui Kuang, J Xue, Yimin Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang. Towards impartial multi-task learning. iclr, 2021.
* [38] Bo Liu, Yihao Feng, Peter Stone, and Qiang Liu. Famo: Fast adaptive multitask optimization, 2023.
* [39] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In _Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.
* [40] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In _ECCV_, 2012.
* [41] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. _IEEE transactions on pattern analysis and machine intelligence_, 39(12):2481-2495, 2017.
* [42] Kevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas Kokkinos. Attentive single-tasking of multiple tasks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1851-1860, 2019.
* [43] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In _Conference on robot learning_, pages 1094-1100. PMLR, 2020.
* [44] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* [45] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [46] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [47] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_, 2017.
* [48] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* multi task rl algorithms. Github, 2021.
* [50] Jonathan M Borwein. A very complicated proof of the minimax theorem. _Minimax Theory and its Applications_, 1(1):21-27, 2016.
* [51] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In _International conference on machine learning_, pages 71-79. PMLR, 2013.

## Supplementary Materials

### Experiments

### Toy Example

To demonstrate our proposed method can achieve better or comparable performance under stochastic settings, we provide an empirical study on the two-objective toy example used in CAGrad [9]. The two objectives \(L_{1}(x)\) and \(L_{2}(x)\) shown in Figure 1 are defined on \(x=(x_{1},x_{2})^{\top}\in\mathbb{R}^{2}\),

\[L_{1}(x) =f_{1}(x)g_{1}(x)+f_{2}(x)h_{1}(x)\] \[L_{2}(x) =f_{1}(x)g_{2}(x)+f_{2}(x)h_{2}(x),\]

where the functions are given by

\[f_{1}(x) =\max(\tanh(0.5x_{2}),0)\] \[f_{2}(x) =\max(\tanh(-0.5x_{2}),0)\] \[g_{1}(x) =\log\Bigl{(}\max\bigl{(}|0.5(-x_{1}-7)-\tanh(-x_{2})|,0.000005) \Bigr{)}+6\] \[g_{2}(x) =\log\Bigl{(}\max\bigl{(}|0.5(-x_{1}+3)-\tanh(-x_{2})+2|,0.000005 )\Bigr{)}+6\] \[h_{1}(x) =\bigl{(}(-x_{1}+7)^{2}+0.1(-x_{1}-8)^{2}\bigr{)}/10-20\] \[h_{2}(x) =\bigl{(}(-x_{1}-7)^{2}+0.1(-x_{1}-8)^{2}\bigr{)}/10-20.\]

We choose 3 initializations

\[x_{0}\in\{(-8.5,7.5),(-8.5,5),(9,9)\}\]

for different methods and visualize the optimization trajectories in Figure 1. The starting point of every trajectory in Figure 1d-Figure 1h is given by the \(\bullet\) symbol, and the color of every trajectory changes gradually from red to yellow. The gray line illustrates the Pareto front, and the \(\star\) symbol denotes the global optimum. To simulate the stochastic setting, we add zero-mean Gaussian noise to the gradient of each objective for all the methods except MGDA. We adopt Adam optimizer with learning rate of 0.002 and 70000 iterations for each run. As shown, GD can get stuck due to the dominant gradient of a specific objective, which stops progressing towards the Pareto front. PCGrad and CAGrad can also fail to converge to the Pareto front in certain circumstances.

Figure 1: A two-objective toy example.

### Consistency Verification

We conduct the experiment on the multi-task classification dataset Multi-Fashion+MNIST [45]. Each image contained in this dataset is constructed by overlaying two images randomly sampled from MNIST [46] and FashionMNIST [47] respectively. We adopt shrinked Lenet [48] as the shared base-encoder and a task-specific linear classification head for each task. We report the training losses obtained from different methods over 3 independent runs in Figure 2. As illustrated, the performance of SDMGrad with large \(\lambda\) is similar to GD, and the performance when \(\lambda\) is small resembles MGDA. With properly tuned \(\lambda\), lower average training loss can be obtained. Generally, the results confirm the consistency of our formulation with the direction-oriented principle.

The Multi-Fashion+MNIST[45] includes images constructed from FashionMNIST[47] and MNIST[46]. First, select one image from each dataset randomly, then transform the two images into a single image with one put in the top-left corner and the other in bottom-right corner. The dataset contains 120000 training images and 20000 test images. We use SGD optimizer with learning rate 0.001 and train for 100 epochs with batch size 256. We use multi-step scheduler with scale factor 0.1 to decay learning rate every 15 epochs. The projected gradient descent is performed with learning rate of 10 and momentum of 0.5 and 20 gradient descent steps are applied.

### Supervised Learning

We implement the methods based on the library released by [10]. Following [9; 13; 10], we train our method for 200 epochs, using Adam optimizer with learning rate 0.0001 for the first 100 epochs and 0.00005 for the rest. The batch size for Cityscapes and NYU-v2 are 8 and 2 respectively. We compute the averaged test performance over the last 10 epochs as final performance measure. The inner projected gradient descent is performed with learning rate of 10 and momentum of 0.5 and 20 gradient descent steps are applied. The experiments on Cityscapes and NYU-v2 are run on RTX 3090 and Tesla V100 GPU, respectively. We also report additional experiment results over different \(\lambda\) and \(S=1\) in Table 5 and Table 6.

### Reinforcement Learning

Following [9; 13; 10], we conduct the experiments based on MTRL codebase[49]. We train our method for 2 million steps with batch size of 1280. The inner projected gradient descent is performed with learning rate of 10 for MT10 benchmark and 20 gradient descent steps are applied. The method is evaluated once every 10000 steps and the best average test performance over 10 random seeds over the entire training process is reported. We search \(\lambda\in\{0.1,0.2,\cdots,1.0\}\) for MT10 benchmark and the highest success rate is achieved when \(\lambda=0.6\). For our objective sampling strategy, the number of sampled objectives is a random variable obeying binomial distribution whose expectation is \(n\)

Figure 2: Consistency verification on Multi-Fashion+MNIST dataset.

To compare with CAGrad-Fast[9], we choose \(n=4\) for MT10 benchmark. We cite the reported success rates of all baseline methods in Table 4, but independently run each experiment 5 times to calculate the average running time. All experiments on MT10 are run on RTX 2080Ti GPU. We also report additional experiments results over \(S=1\) on MT10 in Table 7.

## Appendix B Notations for Technical Proofs

In this part, we first summarize all the notations that we used in this paper in order to help readers understand. First, in multi-objective optimization, we have \(K\geq 2\) different objectives and each of them has the loss function \(L_{i}(\theta)\). Let \(g_{i}\) denote the gradient of objective \(i\) and \(g_{0}\) denotes the target gradient. \(w=(w_{1},...,w_{K})^{T}\in\mathbb{R}^{K^{\prime}}\ and\ \ \mathcal{W}\) denotes the probability simplex. Other useful notations are listed as below:

\[\theta^{*}=\arg\min_{\theta\in\mathbb{R}^{m}}\Big{\{}L_{0}(\theta)\triangleq \frac{1}{K}\sum_{i=1}^{K}L_{i}(\theta)\Big{\}},\ \ g_{0}=g_{0}(\theta)=G(\theta)\widetilde{w}\]

\begin{table}
\begin{tabular}{l c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Metaworld MT10} \\ \cline{2-3}  & success & \multirow{2}{*}{time} \\  & (mean \(\pm\) stderr) & \\ \hline SDMGrad & **0.84**\(\pm\) 0.10 & 13.6 \\ SDMGrad (S=1) & 0.83 \(\pm\) 0.05 & 11.2 \\ \hline SDMGrad-OS & 0.82 \(\pm\) 0.08 & 9.7 \\ SDMGrad-OS (S=1) & 0.80 \(\pm\) 0.12 & **6.8** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Additional reinforcement learning experiments on Metaworld MT10 benchmarks.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Segmentation} & \multicolumn{2}{c}{Depth} & \multicolumn{4}{c}{Surface Normal} \\ \cline{2-10}  & \multicolumn{2}{c}{mIoU \(\uparrow\)} & \multicolumn{2}{c}{Pix Acc \(\uparrow\)} & \multicolumn{2}{c}{Abs Err \(\downarrow\)} & \multicolumn{2}{c}{Rel Err \(\downarrow\)} & \multicolumn{2}{c}{\(\Delta m\%\downarrow\)} \\ \cline{2-10}  & \multicolumn{2}{c}{mIoU \(\uparrow\)} & \multicolumn{2}{c}{Pix Acc \(\uparrow\)} & \multicolumn{2}{c}{Abs Err \(\downarrow\)} & \multicolumn{2}{c}{Rel Err \(\downarrow\)} & \multicolumn{2}{c}{Within \(t^{c}\uparrow\)} & \multicolumn{2}{c}{\(\Delta m\%\downarrow\)} \\ \hline STL & 74.01 & 93.16 & 0.0125 & 27.77 & & & & & \\ \hline SDMGrad (\(\lambda=0.1\)) & 72.56 & 92.68 & 0.0156 & 40.89 & 18.65 & & & & \\ SDMGrad (\(\lambda=0.2\)) & 74.79 & 93.30 & 0.0149 & 32.46 & & & & & \\ SDMGrad (\(\lambda=0.3\)) & 74.53 & 93.52 & 0.0137 & 34.01 & & & & & \\ SDMGrad (\(\lambda=0.4\)) & 75.10 & 93.48 & 0.0137 & 35.66 & & & & & \\ SDMGrad (\(\lambda=0.5\)) & 74.63 & 93.46 & 0.0131 & 38.99 & & & & & \\ SDMGrad (\(\lambda=0.6\)) & 74.42 & 93.22 & 0.0138 & 38.79 & & & & & \\ SDMGrad (\(\lambda=0.7\)) & 75.06 & 93.42 & 0.0158 & 39.98 & & & & & \\ SDMGrad (\(\lambda=0.8\)) & 74.99 & 93.40 & 0.0155 & 39.65 & & & & & \\ SDMGrad (\(\lambda=0.9\)) & 75.60 & 93.50 & 0.0134 & 43.52 & & & & & \\ SDMGrad (\(\lambda=1.0\)) & 74.50 & 93.47 & 0.0142 & 42.80 & & & & & \\ SDMGrad (\(\lambda=10\)) & 74.17 & 93.13 & 0.0154 & 41.77 & & & & & \\ \hline SDMGrad (\(\lambda=0.3,S=1\)) & 75.41 & 93.62 & 0.0139 & 38.83 & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 5: Additional supervised learning experiments on Cityscapes dataset.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Segmentation} & \multicolumn{2}{c}{Depth} & \multicolumn{2}{c}{\(\Delta m\%\downarrow\)} \\ \cline{2-5}  & \multicolumn{2}{c}{mIoU \(\uparrow\)} & \multicolumn{2}{c}{Pix Acc \(\uparrow\)} & \multicolumn{2}{c}{Abs Err \(\downarrow\)} & \multicolumn{2}{c}{Rel Err \(\downarrow\)} \\ \hline STL & 74.01 & 93.16 & 0.0125 & 27.77 & & \\ \hline SDMGrad (\(\lambda=0.1\)) & 72.56 & 92.68 & 0.0156 & 40.89 & 18.65 & \\ SDMGrad (\(\lambda=0.2\)) & 74.79 & 93.30 & 0.0149 & 32.46 & & & & \\ SDMGrad (\(\lambda=0.3\)) & 74.53 & 93.52 & 0.0137 & 34.01 & & & & \\ SDMGrad (\(\lambda=0.4\)) & 75.10 & 93.48 & 0.0137 & 35.66 & & & & \\ SDMGrad (\(\lambda=0.5\)) & 74.63 & 93.46 & 0.0131 & 38.99 & & & & \\ SDMGrad (\(\lambda=0.6\)) & 74.42 & 93.22 & 0.0138 & 38.79 & & & & \\ SDMGrad (\(\lambda=0.7\)) & 75.06 & 93.42 & 0.0158 & 39.98 & & & & \\ SDMGrad (\(\lambda=0.8\)) & 74.99 & 93.40 & 0.0155 & 39.65 & & & & \\ SDMGrad (\(\lambda=0.9\)) & 75.60 & 93.50 & 0.0134 & 43.52 & & & & \\ SDMGrad (\(\lambda=1.0\)) & 74.50 & 93.47 & 0.0142 & 42.80 & & & & \\ SDMGrad (\(\lambda=10\)) & 74.17 & 93.13 & 0.0154 & 41.77 & & & & & \\ \hline SDMGrad (\(\lambda=0.3,S=1\)) & 75.41 & 93.62 & 0.0139 & 38.83 & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 5: Additional supervised learning experiments on Cityscapes dataset.

\[g_{w} =\sum_{i}w_{i}g_{i}\ \ s.t.\ \ \ \mathcal{W}=\{w:\sum_{i}w_{i}=1\ \ and\ \ w_{i}\geq 0\}\] \[w_{\lambda}^{*} =\arg\min_{w\in\mathcal{W}}\frac{1}{2}\|g_{w}+\lambda g_{0}\|^{2},\ \ w^{*}=\arg\min_{w\in\mathcal{W}}\frac{1}{2}\|g_{w}\|^{2},\ \ w_{t}^{*}=\arg\min_{w\in\mathcal{W}}\frac{1}{2}\|G( \theta_{t})w\|^{2}\] \[w_{t,\lambda}^{*} =\arg\min_{w\in\mathcal{W}}F(w)=\arg\min_{w\in\mathcal{W}}\frac{ 1}{2}\|G(\theta_{t})w+\lambda g_{0}(\theta_{t})\|^{2}\] \[\nabla_{w}F(w) =G(\theta_{t})^{T}(G(\theta_{t})w+\lambda g_{0}(\theta_{t})), \nabla_{w}\widehat{F}(w)=G(\theta_{t};\xi)^{T}(G(\theta_{t},\xi^{\prime})w+ \lambda g_{0}(\theta_{t},\xi^{\prime})).\] (11)

We use \(\mathbb{E}[\cdot]_{A|B}\) to denote taking expectation over \(A\) conditioning on \(B\) and \(\widetilde{\mathcal{O}}\) omits the order of \(\log\).

## Appendix C Detailed proofs for convergence analysis with nonconvex Objectives

We now provide some auxiliary lemmas for proving Proposition 1 and Theorem 1

**Lemma 1**.: _Let \(d^{*}\) be the solution of_

\[\max_{d\in\mathbb{R}^{m}}\min_{i\in[K]}\langle g_{i},d\rangle-\frac{1}{2}\|d\| ^{2}+\lambda\langle g_{0},d\rangle,\]

_then we have_

\[d^{*}=g_{w_{\lambda}^{*}}+\lambda g_{0}.\]

_In addition, \(w_{\lambda}^{*}\) is the solution of_

\[\min_{w\in\mathcal{W}}\frac{1}{2}\|g_{w}+\lambda g_{0}\|^{2}.\]

Proof.: First, it can be seen that

\[\max_{d\in\mathbb{R}^{m}}\min_{i\in[K]}\langle g_{i},d\rangle- \frac{1}{2}\|d\|^{2}+\lambda\langle g_{0},d\rangle\] \[\qquad=\max_{d\in\mathbb{R}^{m}}\min_{w\in\mathcal{W}}\langle \sum_{i}w_{i}g_{i},d\rangle-\frac{1}{2}\|d\|^{2}+\lambda\langle g_{0},d\rangle\] \[\qquad=\max_{d\in\mathbb{R}^{m}}\min_{w\in\mathcal{W}}g_{w}{}^{T} d-\frac{1}{2}\|d\|^{2}+\lambda\langle g_{0},d\rangle.\] (12)

Noting that the problem is concave w.r.t. \(d\) and convex w.r.t \(w\) and using the Von Neumann-Fan minimax theorem [50], we can exchange the min and max problems without changing the solution. Then, we can solve the following equivalent problem.

\[\min_{w\in\mathcal{W}}\max_{d\in\mathbb{R}^{m}}g_{w}{}^{T}d-\frac{1}{2}\|d\|^ {2}+\lambda\langle g_{0},d\rangle\] (13)

Then by fixing \(w\), we have \(d^{*}=g_{w}+\lambda g_{0}\). Substituting this solution to the eq. (13) and rearranging the equation, we turn to solve the following problem.

\[\min_{w\in\mathcal{W}}\frac{1}{2}\|g_{w}+\lambda g_{0}\|^{2}.\]

Let \(w_{\lambda}^{*}\) be the solution of the above problem, and hence the final updating direction \(d^{*}=g_{w_{\lambda}^{*}}+\lambda g_{0}\). Then, the proof is complete. 

**Lemma 2**.: _Suppose Assumption 2-3 are satisfied. According to the definition of \(g_{0}(\theta)\) in eq. (11), we have the following inequalities,_

\[\|g_{0}(\theta)\|\leq C_{g},\ \ \mathbb{E}[\|g_{0}(\theta;\xi)-g_{0}(\theta)\|^{2} ]\leq K\sigma_{0}^{2}.\]

Proof.: Based on the definitions, we have

\[\|g_{0}(\theta)\|=\|G(\theta)\widetilde{w}\|\leq C_{g},\]

where the inequality follows from the fact that \(\|\widetilde{w}\|\leq 1\) and Assumption 3. Then, we have

\[\mathbb{E}_{\xi}[\|g_{0}(\theta;\xi)-g_{0}(\theta)\|^{2}]\leq\mathbb{E}_{\xi }[\|G(\theta;\xi)-G(\theta)\|^{2}]\leq K\sigma_{0}^{2}\]

where \(\sigma_{0}^{2}=\max_{i}\sigma_{i}^{2}\) and the proof is complete.

**Lemma 3**.: _Suppose Assumptions 2-3 are satisfied and recall that \(F(w)=\frac{1}{2}\|G(\theta_{t})w+\lambda g_{0}(\theta_{t})\|^{2}\) is a convex function. Let \(w_{\lambda}^{*}=\arg\min_{w\in\mathcal{W}}\frac{1}{2}\|g_{w}+\lambda g_{0}\|^{2}\) and set step size \(\beta_{t,s}=c/\sqrt{s}\) where \(c>0\) is a constant. Then for any \(S>1\), it holds that,_

\[\mathbb{E}[\|\nabla_{w}\widehat{F}(w)\|]\leq C_{1},\] \[\mathbb{E}[\|G(\theta_{t})w_{S}+\lambda g_{0}(\theta_{t})\|^{2}- \|G(\theta_{t})w_{\lambda}^{*}+\lambda g_{0}(\theta_{t})\|^{2}]\leq (\frac{2}{c}+2cC_{1})\frac{2+log(S)}{\sqrt{S}}\]

_where \(C_{1}=\sqrt{8(K\sigma_{0}^{2}+C_{g}^{2})^{2}+8\lambda^{2}(K\sigma_{0}^{2}+C_{ g}^{2})^{2}}=\mathcal{O}(K+\lambda K)\), \(\nabla_{w}\widehat{F}(w)=G(\theta_{t};\xi)^{T}(G(\theta_{t};\xi^{\prime})w+ \lambda g_{0}(\theta_{t};\xi^{\prime}))\)._

Proof.: This lemma mostly follows from Theorem 2 in [51]. However, we did not take that \(\mathbb{E}[\|\nabla_{w}\widehat{F}(w)\|]\) is bounded by a constant as an assumption. Therefore, we first provide a bound for it in our method. Based on the definition in Equation (11), \(\nabla_{w}\widehat{F}(w)=G(\theta_{t};\xi)^{T}(G(\theta_{t};\xi^{\prime})w+ \lambda g_{0}(\theta_{t};\xi^{\prime}))\). According to the fact that \(\mathbb{E}[X]\leq\sqrt{\mathbb{E}[\|\nabla_{w}\widehat{F}(w)\|^{2}]}=\sqrt{ \mathbb{E}[\|G(\theta_{t};\xi)^{T}(G(\theta_{t};\xi^{\prime})w+\lambda g_{0}( \theta_{t};\xi^{\prime}))\|^{2}]}\)

\[\mathbb{E}[\|\nabla_{w}\widehat{F}(w)\|] \leq\sqrt{\mathbb{E}[\|\nabla_{w}\widehat{F}(w)\|^{2}]}=\sqrt{ \mathbb{E}[\|G(\theta_{t};\xi)^{T}(G(\theta_{t};\xi^{\prime})w+\lambda g_{0}( \theta_{t};\xi^{\prime}))\|^{2}]}\] \[\overset{(i)}{\leq}\sqrt{2\mathbb{E}[\underbrace{\|G(\theta_{t };\xi)^{T}G(\theta_{t};\xi^{\prime})w\|^{2}}_{A}+\lambda^{2}\underbrace{\|G( \theta_{t};\xi)^{T}g_{0}(\theta_{t};\xi^{\prime}))\|^{2}}_{B}}),\] (14)

where \((i)\) follows from the Young's inequality. Next, we provide bounds for \(\mathbb{E}[A]\) and \(\mathbb{E}[B]\), separately:

\[\mathbb{E}[A]\overset{(i)}{\leq} \mathbb{E}[\|(G(\theta_{t};\xi)^{T}-G(\theta_{t})^{T}+G(\theta_{t })^{T})(G(\theta_{t};\xi^{\prime})-G(\theta_{t})+G(\theta_{t}))\|^{2}]\] \[= \mathbb{E}[\|(G(\theta_{t};\xi)^{T}-G(\theta_{t})^{T})(G(\theta_{ t};\xi^{\prime})-G(\theta_{t}))+(G(\theta_{t};\xi)^{T}-G(\theta_{t})^{T})G(\theta_{t})\] \[+G(\theta_{t})^{T}(G(\theta_{t};\xi^{\prime})-G(\theta_{t}))+G( \theta_{t})^{T}G(\theta_{t})\|^{2}]\] \[\overset{(ii)}{\leq} 4\mathbb{E}[\|G(\theta_{t};\xi)^{T}-G(\theta_{t})^{T}\|^{2}\|G( \theta_{t};\xi^{\prime})-G(\theta_{t})\|^{2}+\|G(\theta_{t};\xi)^{T}-G(\theta_ {t})^{T}\|^{2}\|G(\theta_{t})\|^{2}\] \[+\|G(\theta_{t})^{T}\|^{2}\|(G(\theta_{t};\xi^{\prime})-G(\theta_ {t})\|^{2}+\|G(\theta_{t})^{T}G(\theta_{t})\|^{2}]\] \[\overset{(iii)}{\leq} 4K^{2}\sigma_{0}^{4}+8K\sigma_{0}^{2}C_{g}^{2}+4C_{g}^{4}=4(K \sigma_{0}^{2}+C_{g}^{2})^{2},\] (15)

where \((i)\) follows from Cauchy-Schwarz inequality and \(w\in\mathcal{W}\) where \(\mathcal{W}\) is the simplex, \((ii)\) follows from Young's inequality and \((iii)\) follows from Assumption 2 and Assumption 3. Then for term B, we have,

\[\mathbb{E}[B]= \mathbb{E}[\|(G(\theta_{t};\xi)^{T}-G(\theta_{t})^{T}+G(\theta_{ t})^{T})(g_{0}(\theta_{t};\xi^{\prime})-g_{0}(\theta_{t})+g_{0}(\theta_{t}))\|^{2}]\] \[\overset{(i)}{\leq} 4\mathbb{E}[\|(G(\theta_{t};\xi)^{T}-G(\theta_{t})^{T})(g_{0}( \theta_{t};\xi^{\prime})-g_{0}(\theta_{t}))\|^{2}+\|(G(\theta_{t};\xi)^{T}-G( \theta_{t})^{T})g_{0}(\theta_{t})\|^{2}\] \[+\|G(\theta_{t})^{T}(g_{0}(\theta_{t};\xi^{\prime})-g_{0}(\theta_ {t}))\|^{2}+\|G(\theta_{t}^{T})g_{0}(\theta_{t})\|^{2}]\] \[\overset{(ii)}{\leq} 4K^{2}\sigma_{0}^{4}+8K\sigma_{0}^{2}C_{g}^{2}+4C_{g}^{4}=4(K \sigma_{0}^{2}+C_{g}^{2})^{2},\] (16)

where \((i)\) follows from Young's inequality, \((ii)\) follows from Assumption 3 and Lemma 2. Then substituting eq. (15) and eq. (16) into eq. (14), we can obtain,

\[\mathbb{E}[\|\nabla_{w}\widehat{F}(w)\|]\leq\sqrt{8(K\sigma_{0}^{2}+C_{g}^{2})^ {2}+8\lambda^{2}(K\sigma_{0}^{2}+C_{g}^{2})^{2}}=C_{1}.\]

Meanwhile, since \(\mathbb{E}[\|\nabla_{w}\widehat{F}(w)\|]\leq C_{1}\), \(\sup_{w,w^{\prime}}\|w-w^{\prime}\|\leq 1\) and by choosing step size \(\beta_{s}=c/\sqrt{s}\) where \(c>0\) is a constant, we can obtain the following inequality from Theorem 2 in [51]:

\[\mathbb{E}[F(w_{S})-F(w_{\lambda}^{*})]\leq(\frac{1}{c}+cC_{1})\frac{2+log(S)}{ \sqrt{S}}\] (17)

Then after multiplying by 2 on both sides, the proof is complete.

### Proof of Proposition 1

**CA distance.** Now we show the upper bound for the distance to CA direction. Recall that we define the CA distance as \(\|\mathbb{E}_{\zeta,w_{t,S}|\theta_{t}}[G(\theta_{t};\zeta)w_{t,S}+\lambda g_{0}( \theta_{t};\zeta)]-G(\theta_{t})w_{t,\lambda}^{*}-\lambda g_{0}(\theta_{t})\|\).

Proof.: Based on the Jensen's inequality, we have

\[\|\mathbb{E}_{\zeta,w_{t,S}|\theta_{t}}[G(\theta_{t};\zeta)w_{t,S }+\lambda g_{0}(\theta_{t};\zeta)]-G(\theta_{t})w_{t,\lambda}^{*}-\lambda g_{0 }(\theta_{t})\|^{2}\] \[\leq \mathbb{E}_{w_{t,S}|\theta_{t}}\big{[}\big{\|}\mathbb{E}_{\zeta}[ G(\theta_{t};\zeta)w_{t,S}+\lambda g_{0}(\theta_{t};\zeta)]-G(\theta_{t})w_{t, \lambda}^{*}-\lambda g_{0}(\theta_{t})\big{\|}^{2}\big{]}\] \[\overset{(i)}{=} \mathbb{E}[\|G(\theta_{t})w_{t,S}-G(\theta_{t})w_{t,\lambda}^{*} \|^{2}]\] \[= \mathbb{E}[\|G(\theta_{t})w_{t,S}+\lambda g_{0}(\theta_{t})-G( \theta_{t})w_{t,\lambda}^{*}-\lambda g_{0}(\theta_{t})\|^{2}]\] \[= \mathbb{E}[\|G(\theta_{t})w_{t,S}+\lambda g_{0}(\theta_{t})\|^{2} +\|G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{0}(\theta_{t})\|^{2}\] \[-2\mathbb{E}\langle G(\theta_{t})w_{t,S}+\lambda g_{0}(\theta_{t} ),G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{0}(\theta_{t})\rangle]\] \[= \mathbb{E}[\|G(\theta_{t})w_{t,S}+\lambda g_{0}(\theta_{t})\|^{2} +\|G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{0}(\theta_{t})\|^{2}]\] \[-2\mathbb{E}[\langle G(\theta)w_{t,S},G(\theta_{t})w_{t,\lambda} ^{*}+\lambda g_{0}(\theta_{t})\rangle]-2\mathbb{E}[\langle\lambda g_{0}( \theta_{t}),G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{0}(\theta_{t})\rangle]\] \[\overset{(ii)}{\leq} \mathbb{E}[\|G(\theta_{t})w_{t,S}+\lambda g_{0}(\theta_{t})\|^{2} +\|G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{0}(\theta_{t})\|^{2}]\] \[-2\mathbb{E}[\langle G(\theta)w_{t,\lambda}^{*},G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{0}(\theta_{t})\rangle]-2\mathbb{E}[\langle\lambda g_ {0}(\theta_{t}),G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{0}(\theta_{t})\rangle]\] \[= \mathbb{E}[\|G(\theta_{t})w_{t,S}+\lambda g_{0}(\theta_{t})\|^{2} +\|G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{0}(\theta_{t})\|^{2}]\] \[-2\mathbb{E}[\langle G(\theta)w_{t,\lambda}^{*}+\lambda g_{0}( \theta_{t}),G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{0}(\theta_{t})\rangle]\] \[= \mathbb{E}[\|G(\theta_{t})w_{t,S}+\lambda g_{0}(\theta_{t})\|^{2} -\|G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{0}(\theta_{t})\|^{2}]\] \[\overset{(iii)}{\leq} (\frac{2}{c}+2cC_{1})\frac{2+log(S)}{\sqrt{S}}\] (18)

where \((i)\) omits the subscript of taking expectation over \(w_{t,S}\) conditioning on \(\theta_{t}\), \((ii)\) follows from optimality condition that

\[\langle w,G(\theta_{t})^{T}(G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{0}( \theta_{t}))\rangle\geq\langle w_{t,\lambda}^{*},G(\theta_{t})^{T}(G(\theta_{ t})w_{t,\lambda}^{*}+\lambda g_{0}(\theta_{t}))\rangle.\] (19)

\((iii)\) follows from Lemma 3 whenwe choose \(\beta_{t,s}=c/\sqrt{s}\) where \(c\) is a constant. Then take the square root on both sides, the proof is complete. 

### Proof of Theorem 1

**Theorem 5** (Restatement of Theorem 1).: _Suppose Assumptions 1-3 are satisfied. Set \(\alpha_{t}=\alpha=\Theta((1+\lambda)^{-1}K^{-\frac{1}{2}}T^{-\frac{1}{2}})\), \(\beta_{t,s}=c/\sqrt{s}\) where \(c\) is a constant, and \(S=\Theta((1+\lambda)^{-2}T^{2})\). The outputs of the proposed SDMGrad algorithm satisfy_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t,\lambda}^{*}+\lambda g _{0}(\theta_{t})\|^{2}]=\widetilde{\mathcal{O}}((1+\lambda^{2})K^{\frac{1}{2}}T ^{-\frac{1}{2}}).\]

Proof.: Recall that \(d=G(\theta_{t};\zeta)w_{t,S}+\lambda g_{0}(\theta_{t};\zeta)\). According to Assumption 1, we have for any \(i\),

\[L_{i}(\theta_{t+1})+\lambda L_{0}(\theta_{t+1})\leq L_{i}(\theta_{t})+\lambda L _{0}(\theta_{t})+\alpha_{t}\langle g_{i}(\theta_{t})+\lambda g_{0}(\theta_{t}),- d\rangle+\frac{l_{i,1}^{\prime}\alpha_{t}^{2}}{2}\|d\|^{2}.\] (20)

where \(l_{i,1}^{\prime}=l_{i,1}+\lambda\max_{i}l_{i,1}=\Theta(1+\lambda)\). Then we bound the second and third terms separately on the right-hand side (RHS). First, for the second term, conditioning on \(\theta_{t}\) and taking expectation, we have

\[\mathbb{E}[ \langle g_{i}(\theta_{t})+\lambda g_{0}(\theta_{t}),-G(\theta_{t };\zeta)w_{t,S}-\lambda g_{0}(\theta_{t};\zeta)\rangle|\theta_{t}]\] \[= \mathbb{E}[\langle g_{i}(\theta_{t})+\lambda g_{0}(\theta_{t}),-G( \theta_{t})w_{t,S}-\lambda g_{0}(\theta_{t})\rangle|\theta_{t}]\] \[= \mathbb{E}[\langle g_{i}(\theta_{t})+\lambda g_{0}(\theta_{t}),G( \theta_{t})w_{t,\lambda}^{*}-G(\theta_{t})w_{t,S}\rangle-\langle g_{i}(\theta_{t} )+\lambda g_{0}(\theta_{t}),G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{0}( \theta_{t})\rangle|\theta_{t}]\] \[= \mathbb{E}[\langle g_{i}(\theta_{t})+\lambda g_{0}(\theta\[\stackrel{{(i)}}{{\leq}} \mathbb{E}[(l_{i}+\lambda C_{g})\|G(\theta_{t})w^{*}_{t,\lambda}-G( \theta_{t})w_{t,S}\|\|\theta_{t}]-\mathbb{E}[\|G(\theta_{t})w^{*}_{t,\lambda}+ \lambda g_{0}(\theta_{t})\|^{2}|\theta_{t}]\] \[\stackrel{{(ii)}}{{\leq}} (l_{i}+\lambda C_{g})\sqrt{\mathbb{E}[\|G(\theta_{t})w^{*}_{t, \lambda}-G(\theta_{t})w_{t,S}\|^{2}|\theta_{t}]}-\mathbb{E}[\|G(\theta_{t})w^{ *}_{t,\lambda}+\lambda g_{0}(\theta_{t})\|^{2}|\theta_{t}]\] \[\stackrel{{(iii)}}{{\leq}} (l_{i}+\lambda C_{g})\sqrt{(\frac{2}{c}+2cC_{1})\frac{2+log(S)}{ \sqrt{S}}}-\mathbb{E}[\|G(\theta_{t})w^{*}_{t,\lambda}+\lambda g_{0}(\theta_{t })\|^{2}|\theta_{t}]\] (21)

where \((i)\) follows from Cauchy-Schwarz inequality and optimality condition in eq. (19), \((ii)\) follows from the fact that \(\mathbb{E}[X]\leq\sqrt{\mathbb{E}[X^{2}]}\) and \((iii)\) follows from eq. (18).

Then for the third term,

\[\mathbb{E}[\|d\|^{2}]= \mathbb{E}[\|G(\theta_{t};\zeta)w_{t,S}+\lambda g_{0}(\theta_{t}; \zeta)\|^{2}]\] \[= \mathbb{E}[\|G(\theta_{t};\zeta)w_{t,S}-G(\theta_{t})w_{t,S}+G( \theta_{t})w_{t,S}+\lambda g_{0}(\theta_{t};\zeta)-\lambda g_{0}(\theta_{t})+ \lambda g_{0}(\theta_{t})\|^{2}]\] \[\stackrel{{(i)}}{{\leq}} 4\mathbb{E}[\|G(\theta_{t};\zeta)-G(\theta_{t})\|^{2}]+4\mathbb{ E}[\|G(\theta_{t})\|^{2}]+4\lambda^{2}\mathbb{E}[\|g_{0}(\theta_{t};\zeta)-g_{0}( \theta_{t})\|^{2}]\] \[+4\lambda^{2}\mathbb{E}[\|g_{0}(\theta_{t})\|^{2}]\] \[\stackrel{{(ii)}}{{\leq}} \underbrace{4K\sigma_{0}^{2}+4C_{g}^{2}+4\lambda^{2}K\sigma_{0}^ {2}+4\lambda^{2}C_{g}^{2}}_{C_{2}}\] (22)

where \((i)\) follows from Young's inequality, and \((ii)\) follows from Assumption 3 and Lemma 2. Note that \(C_{2}=\mathcal{O}(K+K\lambda^{2})\). Then taking expectation on eq. (20), substituting eq. (21) and eq. (22) into it, and unconditioning on \(\theta_{t}\), we have

\[\mathbb{E}[L_{i}(\theta_{t+1})+\lambda L_{0}(\theta_{t+1})]\] \[\leq \mathbb{E}[L_{i}(\theta_{t})+\lambda L_{0}(\theta_{t})]+\alpha_{t }\mathbb{E}[(g_{i}(\theta_{t})+\lambda g_{0}(\theta_{t}),-d)]+\frac{l^{\prime} _{i,1}\alpha_{t}^{2}}{2}\mathbb{E}[\|d\|^{2}]\] \[\leq \mathbb{E}[L_{i}(\theta_{t})+\lambda L_{0}(\theta_{t})]+\alpha_{ t}(l_{i}+\lambda C_{g})\sqrt{(\frac{2}{c}+2cC_{1})\frac{2+log(S)}{\sqrt{S}}}\] \[-\alpha_{t}\mathbb{E}[\|G(\theta_{t})w^{*}_{t,\lambda}+\lambda g_ {0}(\theta_{t})\|^{2}]+\frac{l^{\prime}_{i,1}\alpha_{t}^{2}}{2}C_{2}\] (23)

Then, choosing \(\alpha_{t}=\alpha\), and rearranging the above inequality, we have

\[\alpha\mathbb{E}[\|G(\theta_{t})w^{*}_{t,\lambda}+\lambda g_{0}( \theta_{t})\|^{2}]\leq \mathbb{E}[L_{i}(\theta_{t})+\lambda L_{0}(\theta_{t})-L_{i}( \theta_{t+1})-\lambda L_{0}(\theta_{t+1})]\] \[+\alpha(l_{i}+\lambda C_{g})\sqrt{(\frac{2}{c}+2cC_{1})\frac{2+ log(S)}{\sqrt{S}}}+\frac{l^{\prime}_{i,1}\alpha^{2}}{2}C_{2}.\]

Telescoping over \(t\in[T]\) in the above inequality yields

\[\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}[\|\!\|G(\theta_{t})w^{*}_{t,\lambda}+\lambda g_{0}( \theta_{t})\|^{2}]\] \[\leq \frac{1}{\alpha T}\mathbb{E}[L_{i}(\theta_{0})-\inf L_{i}(\theta )+\lambda(L_{0}(\theta_{0})-\inf L_{0}(\theta))]+\frac{l^{\prime}_{i,1}\alpha} {2}C_{2}\] \[+(l_{i}+\lambda C_{g})\sqrt{(\frac{2}{c}+2cC_{1})\frac{2+log(S)}{ \sqrt{S}}},\]

If we choose \(\alpha=\Theta((1+\lambda)^{-1}K^{-\frac{1}{2}}T^{-\frac{1}{2}})\) and \(S=\Theta((1+\lambda)^{-2}T^{2})\), we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w^{*}_{t,\lambda}+ \lambda g_{0}(\theta_{t})\|^{2}]=\widetilde{\mathcal{O}}((1+\lambda^{2})K^{ \frac{1}{2}}T^{-\frac{1}{2}}),\]

where \(\widetilde{\mathcal{O}}\) means the order of \(logT\) is omitted. The proof is complete.

### Proof of Corollary 1

Proof.: Since \(\lambda>0\) and \(g_{0}(\theta_{t})=G(\theta_{t})\widetilde{w}\), we have

\[\mathbb{E}[\|G(\theta_{t})w^{*}_{t,\lambda}+\lambda g_{0}(\theta_{t})\|^{2}]=(1+ \lambda)^{2}\mathbb{E}[\|G(\theta_{t})w^{\prime}\|^{2}]\geq(1+\lambda)^{2} \mathbb{E}[\|G(\theta_{t})w^{*}_{t}\|^{2}]\]

where \(w^{\prime}=\frac{1}{1+\lambda}(w^{*}_{1,t,\lambda}+\lambda\widetilde{w}_{1},w^ {*}_{2,t,\lambda}+\lambda\widetilde{w}_{2},...,w^{*}_{K,t,\lambda}+\lambda \widetilde{w}_{K})^{T}\) such that \(w^{\prime}\in\mathcal{W}\). According to parameter selection in Theorem 1 and by choosing a constant \(\lambda\), we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w^{*}_{t}\|^{2}]= \widetilde{\mathcal{O}}(K^{\frac{1}{2}}T^{-\frac{1}{2}}).\] (24)

To achieve an \(\epsilon\)-accurate Pareto stationary point, it requires \(T=\widetilde{\mathcal{O}}(K\epsilon^{-2})\) and each objective requires \(\widetilde{\mathcal{O}}(K^{3}\epsilon^{-6})\) samples in \(\xi\) (\(\xi^{\prime}\)) and \(\widetilde{\mathcal{O}}(K\epsilon^{-2})\) samples in \(\zeta\), respectively. Meanwhile, according to the choice of \(S\) and \(T\), we have the following result for CA distance,

\[\|\mathbb{E}_{\zeta,w_{t,S}|\theta_{t}}[G(\theta_{t},\zeta)w_{t,S}+\lambda g_ {0}(\theta_{t};\zeta)]-G(\theta_{t})w^{*}_{t,\lambda}-\lambda g_{0}(\theta_{t} )\|=\widetilde{\mathcal{O}}(\sqrt{\frac{K}{T}})=\widetilde{\mathcal{O}}(\epsilon)\] (25)

**Remark.** Our algorithm with a constant \(\lambda\) helps mitigate gradient conflict and it guarantees an \(\epsilon-\)accurate Pareto stationary point and the CA distance takes the order of \(\widetilde{O}(\epsilon)\) simultaneously. 

### Proof of Corollary 2

Proof.: According to the inequality \(\|a+b-b\|^{2}\leq 2\|a+b\|^{2}+2\|b\|^{2}\), we have

\[\lambda^{2}\|g_{0}(\theta_{t})\|^{2}\leq 2\|G(\theta_{t})w^{*}_{t,\lambda}+\lambda g_{0}(\theta_{t})\|^{2 }+2\|G(\theta_{t})w^{*}_{t,\lambda}\|^{2}\] \[\leq 2\|G(\theta_{t})w^{*}_{t,\lambda}+\lambda g_{0}(\theta_{t})\|^{2 }+2C_{g}^{2}\]

where the last inequality follows from Assumption 3. Then we take the expectation on the above inequality and sum up it over \(t\in[T]\) such that

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|g_{0}(\theta_{t})\|^{2}]\leq \frac{2}{\lambda^{2}T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w^ {*}_{t,\lambda}+\lambda g_{0}(\theta_{t})\|^{2}]+\frac{C_{g}^{2}}{\lambda^{2}}\] \[= \widetilde{\mathcal{O}}((\lambda^{-2}+1)K^{\frac{1}{2}}T^{-\frac{ 1}{2}}+\lambda^{-2}),\]

where the last inequality follows from Theorem 1. If we choose \(\lambda=\Theta(T^{\frac{1}{2}})\), then we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|g_{0}(\theta_{t})\|^{2}]=\widetilde{ \mathcal{O}}(K^{\frac{1}{2}}T^{-\frac{1}{2}}).\]

To achieve an \(\epsilon\)-accurate stationary point, it requires \(T=\widetilde{\mathcal{O}}(K\epsilon^{-2})\) and each objective requires \(\widetilde{\mathcal{O}}(K^{2}\epsilon^{-4})\) samples in \(\xi\) (\(\xi^{\prime}\)) and \(\widetilde{\mathcal{O}}(K\epsilon^{-2})\) samples in \(\zeta\), respectively. Meanwhile, according to the choice of \(\lambda\), \(S\) and \(T\), we have the following result for CA distance,

\[\|\mathbb{E}_{\zeta,w_{t,S}|\theta_{t}}[G(\theta_{t},\zeta)w_{t,S}+\lambda g_ {0}(\theta_{t};\zeta)]-G(\theta_{t})w^{*}_{t,\lambda}-\lambda g_{0}(\theta_{t} )\|=\widetilde{\mathcal{O}}(\sqrt{\frac{K(1+\lambda)^{2}}{T}})=\widetilde{ \mathcal{O}}(\sqrt{K})\]

**Remark.** With an increasing \(\lambda\), our algorithm approaches GD and it has a faster convergence rate to the stationary point. However, the CA distance takes the order of \(\widetilde{O}(\sqrt{K})\). 

### Proof of Theorem 2

Now we provide the convergence analysis with nonconvex objectives with objective sampling.

**Theorem 6** (Restatement of Theorem 2).: _Suppose Assumptions 1-3 are satisfied. Set \(\gamma=\frac{K}{n}\), \(\alpha_{t}=\alpha=\Theta((1+\lambda^{2})^{-\frac{1}{2}}\gamma^{-\frac{1}{2}}K^{- \frac{1}{2}}T^{-\frac{1}{2}})\), \(\beta_{t,s}=c/\sqrt{s}\) and \(S=\Theta((1+\lambda)^{-2}\gamma^{-2}T^{2})\). Then by choosing a constant \(\lambda\), the iterates of the proposed SDMGrad-OS algorithm satisfy_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w^{*}_{t}\|^{2}]= \widetilde{\mathcal{O}}(K^{\frac{1}{2}}\gamma^{\frac{1}{2}}T^{-\frac{1}{2}}).\]Proof.: Recall that updating direction for \(\theta_{t}\) is \(d^{\prime}=\frac{K}{n}H(\theta_{t};\zeta,\widetilde{S})w_{t,S}+\frac{K}{n} \lambda h_{0}(\theta_{t};\zeta,\widetilde{S})\). Similarly, we have

\[L_{i}(\theta_{t+1})+\lambda L_{0}(\theta_{t+1})\leq L_{i}(\theta_{t})+\lambda L _{0}(\theta_{t})+\alpha_{t}\langle g_{i}(\theta_{t})+\lambda g_{0}(\theta_{t}),-d^{\prime}\rangle+\frac{l^{\prime}_{i,1}\alpha_{t}^{2}}{2}\|d^{\prime}\|^{2}.\] (26)

Then for the inner product term on the RHS of eq. (26), conditioning on \(\theta_{t}\) and taking expectation, we have

\[\mathbb{E}[\langle g_{i}(\theta_{t})+ \lambda g_{0}(\theta_{t}),-d^{\prime}\rangle|\theta_{t}]=\mathbb{E }[\langle g_{i}(\theta_{t})+\lambda g_{0}(\theta_{t}),-G(\theta_{t})w_{t,S}- \lambda g_{0}(\theta_{t})\rangle|\theta_{t}]\] \[\leq (l_{i}+\lambda C_{g})(\sqrt{(\frac{2}{c}+2cC_{1})\frac{2+log(S)}{ \sqrt{S}}})-\mathbb{E}[\|G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{0}(\theta_ {t})\|^{2}|\theta_{t}],\] (27)

where the last inequality follows from eq. (21). Then following the same step as in eq. (22), we can bound the last term on the RHS of eq. (26) as

\[\mathbb{E}[\|d^{\prime}\|^{2}]\leq\underbrace{4\gamma^{2}(1+\lambda^{2})(n \sigma_{0}^{2}+C_{g}^{2})}_{C_{2}^{\prime}}.\] (28)

Then taking expectation on eq. (26), substituting eq. (27) and eq. (28) into it and unconditioning on \(\theta_{t}\), we have

\[\mathbb{E}[L_{i}(\theta_{t+1})+\lambda L_{0}(\theta_{t+1})]\leq \mathbb{E}[L_{i}(\theta_{t})+\lambda L_{0}(\theta_{t})]+\alpha_{t} (l_{i}+\lambda C_{g})\Big{(}\sqrt{(\frac{2}{c}+2cC_{1})\frac{2+log(S)}{\sqrt{S }}}\Big{)}\] \[-\alpha_{t}\mathbb{E}\|G(\theta_{t})w_{t,\lambda}^{*}+\lambda g_{ 0}(\theta_{t})\|^{2}+\frac{l^{\prime}_{i,1}\alpha_{t}^{2}}{2}C_{2}^{\prime}\]

Then choosing \(\alpha_{t}=\alpha\), telescoping the above inequality over \(t\in[T]\), and rearranging the terms, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t,\lambda }^{*}+\lambda g_{0}(\theta_{t})\|^{2}]\leq \frac{1}{\alpha T}\mathbb{E}[L_{i}(\theta_{0})-\inf L_{i}(\theta) +\lambda(L_{0}(\theta_{0})-\inf L_{0}(\theta))]+\frac{l^{\prime}_{i,1}\alpha} {2}C_{2}^{\prime}\] \[+(l_{i}+\lambda C_{g})(\sqrt{(\frac{2}{c}+2cC_{1})\frac{2+log(S)} {\sqrt{S}}})\]

If we choose \(\alpha=\Theta((1+\lambda^{2})^{-\frac{1}{2}}\gamma^{-\frac{1}{2}}K^{-\frac{1}{ 2}}T^{-\frac{1}{2}})\), and \(S=\Theta((1+\lambda)^{-2}\gamma^{-2}T^{2})\), we can get \(\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t,\lambda}^{*}+\lambda g _{0}(\theta_{t})\|^{2}]=\widetilde{\mathcal{O}}((1+\lambda^{2})K^{\frac{1}{2} }\gamma^{\frac{1}{2}}T^{-\frac{1}{2}})\). Furthermore, by choosing \(\lambda\) as constant and following the same step as in Appendix C.3, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t}^{*}\|^{2}]= \widetilde{\mathcal{O}}(K^{\frac{1}{2}}\gamma^{\frac{1}{2}}T^{-\frac{1}{2}}).\]

To achieve an \(\epsilon\)-accurate Pareto stationary point, it requires \(T=\widetilde{\mathcal{O}}(K\gamma\epsilon^{-2})\). In this case, each objective requires a similar number of samples \(\widetilde{\mathcal{O}}(K^{3}\gamma\epsilon^{-6})\) in \(\xi\) (\(\xi^{\prime}\)) and \(\widetilde{\mathcal{O}}(K\gamma\epsilon^{-2})\) samples in \(\zeta\), respectively. As far as we know, this is the first provable objective sampling strategy for stochastic multi-objective optimization. 

## Appendix D Lower sample complexity but higher CA distance

When we do not have requirements on CA distance, we can have a much lower sample complexity. In Algorithm 1, the update process for \(w\) is to reduce the CA distance, which increases the sample complexity. Thus, we will set \(S=1\) to make Algorithm 1 more sample-efficient. In addition, we will use \(w_{t+1}=w_{t,1}\) and \(\beta_{t}\) instead of \(\beta_{t,s}\) in Algorithm 1 for simplicity. The following proof is mostly motivated by Theorem 3 in [14].

### Proof of Theorem 3

**Theorem 7** (Restatement of Theorem 3).: _Suppose Assumptions 1-3 are satisfied and \(S=1\). Set \(\alpha_{t}=\alpha=\Theta(K^{-\frac{1}{2}}T^{-\frac{1}{2}})\), \(\beta_{t}=\beta=\Theta(K^{-1}T^{-\frac{1}{2}})\) and \(\lambda\) as constant. The iterates of the proposed SDMGrad algorithm satisfy,_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t}^{*}\|^{2}]=\mathcal{ O}(KT^{-\frac{1}{2}}).\]

Proof.: Now we define a new function, with a fixed weight \(w\in\mathcal{W}\),

\[l^{\prime}(\theta_{t})=L(\theta_{t})w+\lambda L_{0}(\theta_{t}).\] (29)

For this new function, we have

\[l^{\prime}(\theta_{t+1})\leq l^{\prime}(\theta_{t})+\alpha_{t}\langle G(\theta_{t})w+\lambda g_{0}( \theta_{t}),-d\rangle+\frac{l^{\prime}_{1}\alpha_{t}^{2}}{2}\|d\|^{2}\] \[= l^{\prime}(\theta_{t})+\alpha_{t}\langle G(\theta_{t})w+\lambda g _{0}(\theta_{t}),-G(\theta_{t};\zeta)w_{t+1}-\lambda g_{0}(\theta_{t};\zeta) \rangle+\frac{l^{\prime}_{1}\alpha_{t}^{2}}{2}\|d\|^{2}\]

where \(l^{\prime}_{1}=\max_{i}l_{i,1}+\lambda l_{i,1}\). Then taking expectations over \(\zeta\) on both sides and rearranging the inequality, we have

\[\mathbb{E}[l^{\prime}(\theta_{t+1})]-\mathbb{E}[l^{\prime}(\theta _{t})]\leq \alpha_{t}\mathbb{E}[\langle G(\theta_{t})w+\lambda g_{0}(\theta_ {t}),-G(\theta_{t})w_{t+1}-\lambda g_{0}(\theta_{t})\rangle]+\frac{l^{\prime}_ {1}\alpha_{t}^{2}}{2}\mathbb{E}[\|d\|^{2}]\] \[= -\alpha_{t}\mathbb{E}[\langle G(\theta_{t})w+\lambda g_{0}(\theta _{t}),G(\theta_{t})w_{t+1}-G(\theta_{t})w_{t}\rangle]\] \[-\alpha_{t}\mathbb{E}[\langle G(\theta_{t})w-G(\theta_{t})w_{t},G( \theta_{t})w_{t}+\lambda g_{0}(\theta_{t})\rangle]\] \[-\alpha_{t}\mathbb{E}[\|G(\theta_{t})w_{t}+\lambda g_{0}(\theta _{t})\|^{2}]+\frac{l^{\prime}_{1}\alpha_{t}^{2}}{2}\mathbb{E}[\|d\|^{2}]\] \[\overset{(i)}{\leq} \alpha_{t}\underbrace{\mathbb{E}[\|(G(\theta_{t})w+\lambda g_{0}( \theta_{t}))^{T}G(\theta_{t})\|\|w_{t}-w_{t+1}\|]}_{C}\] \[+\alpha_{t}\underbrace{\mathbb{E}[\langle G(\theta_{t})w_{t}-G( \theta_{t})w,G(\theta_{t})w_{t}+\lambda g_{0}(\theta_{t})\rangle]}_{D}\] \[-\alpha_{t}\mathbb{E}[\|G(\theta_{t})w_{t}+\lambda g_{0}(\theta _{t})\|^{2}]+\frac{l^{\prime}_{1}\alpha_{t}^{2}}{2}\mathbb{E}[\|d\|^{2}],\] (30)

where \((i)\) follows from Cauchy-Schwarz inequlaity. Then we provide bound for term C and term D, respectively. For term C,

\[\mathbb{E}[\|(G(\theta_{t}) w+\lambda g_{0}(\theta_{t}))^{T}G(\theta_{t})\|\|w_{t}-w_{t+1}\|]\] \[= \beta_{t}\mathbb{E}[\|(G(\theta_{t})w+\lambda g_{0}(\theta_{t}))^ {T}G(\theta_{t})\|\|G(\theta_{t};\xi)^{T}(G(\theta_{t};\xi^{\prime})w_{t}+ \lambda g_{0}(\theta;\xi^{\prime}))\|]\] \[\leq \beta_{t}\mathbb{E}[\|(G(\theta_{t})w+\lambda g_{0}(\theta_{t}))^ {T}G(\theta_{t})\|(\|G(\theta_{t};\xi)^{T}(G(\theta_{t};\xi^{\prime})w_{t}\|+ \lambda\|G(\theta_{t};\xi)g_{0}(\theta;\xi^{\prime})\|)]\] \[\leq \beta_{t}(1+\lambda)^{2}C_{g}^{2}(K\sigma_{0}+C_{g})^{2}=\beta_{t }C_{3},\] (31)

where \(C_{3}=\mathcal{O}((1+\lambda)^{2}K^{2})\). Then for term D, we first follow the non-expansive property of projection onto the convex set,

\[\|w_{t+1}-w\|^{2}\leq \|w_{t}-\beta_{t}G(\theta_{t};\xi)^{T}(G(\theta_{t};\xi^{\prime})w _{t}+\lambda g_{0}(\theta_{t};\xi^{\prime}))-w\|^{2}\] \[= \|w_{t}-w\|^{2}-2\beta_{t}\langle w_{t}-w,G(\theta_{t};\xi)^{T}( G(\theta_{t};\xi^{\prime})w_{t}+\lambda g_{0}(\theta_{t};\xi^{\prime}))\rangle\] \[+\beta_{t}^{2}\|G(\theta_{t};\xi)^{T}(G(\theta_{t};\xi^{\prime})w _{t}+\lambda g_{0}(\theta_{t};\xi^{\prime}))\|^{2}\]

Then taking expectation on the above inequality, we can obtain,

\[\mathbb{E}[\|w_{t+1}-w\|^{2}]\leq \mathbb{E}[\|w_{t}-w\|^{2}]-2\beta_{t}\mathbb{E}[\langle w_{t}-w,G (\theta_{t};\xi)^{T}(G(\theta_{t};\xi^{\prime})w_{t}+\lambda g_{0}(\theta_{t};\xi^{ \prime}))\rangle]\]\[\leq \mathbb{E}[\|w_{t}-w\|^{2}]-2\beta_{t}\mathbb{E}[\langle w_{t}-w,G( \theta_{t})^{T}(G(\theta_{t})w_{t}+\lambda g_{0}(\theta_{t}))\rangle]+\beta_{t}^ {2}C_{1}^{2},\]

where the last inequality follows from Lemma 3. Then by rearranging the above inequality, we can obtain,

\[\mathbb{E}[\langle w_{t}-w,G(\theta_{t})^{T}(G(\theta_{t})w_{t}+ \lambda g_{0}(\theta_{t}))\rangle]\leq \frac{1}{2\beta_{t}}\mathbb{E}[\|w_{t}-w\|^{2}-\|w_{t+1}-w\|^{2}]+ \frac{\beta_{t}}{2}C_{1}^{2}\] (32)

Then substituting eq. (31) and eq. (32) into eq. (30), we can obtain,

\[\mathbb{E}[l^{\prime}(\theta_{t+1})-l^{\prime}(\theta_{t})]\leq \alpha_{t}\beta_{t}C_{3}+\frac{\alpha_{t}}{2\beta_{t}}\mathbb{E} [\|w_{t}-w\|^{2}-\|w_{t+1}-w\|^{2}]+\frac{\alpha_{t}\beta_{t}}{2}C_{1}^{2}\] \[-\alpha_{t}\mathbb{E}[\|G(\theta_{t})w_{t}+\lambda g_{0}(\theta _{t})\|^{2}]+\frac{l_{1}^{\prime}\alpha_{t}^{2}}{2}\mathbb{E}[\|d\|^{2}]\] \[\overset{(i)}{\leq} \alpha_{t}\beta_{t}C_{3}+\frac{\alpha_{t}}{2\beta_{t}}\mathbb{E} [\|w_{t}-w\|^{2}-\|w_{t+1}-w\|^{2}]+\frac{\alpha_{t}\beta_{t}}{2}C_{1}^{2}\] \[-\alpha_{t}\mathbb{E}[\|G(\theta_{t})w_{t}+\lambda g_{0}(\theta _{t})\|^{2}]+\frac{l_{1}^{\prime}\alpha_{t}^{2}}{2}C_{2}\] (33)

Then we take \(\alpha_{t}=\alpha\) and \(\beta_{t}=\beta\) as constants, telescope and rearrange the above inequality,

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t}+ \lambda g_{0}(\theta_{t})\|^{2}]\leq \frac{1}{\alpha T}\mathbb{E}[l^{\prime}(\theta_{0})-l^{\prime}( \theta_{T})]+\frac{1}{2\beta T}\mathbb{E}[\|w_{0}-w\|^{2}-\|w_{T}-w\|^{2}]\] \[+\beta(C_{3}+\frac{C_{1}^{2}}{2})+\frac{l_{1}^{\prime}\alpha}{2}C _{2}\] \[\overset{(i)}{\leq} \mathcal{O}(\frac{1}{\alpha T}+\alpha K+\frac{1}{\beta T}+\beta K ^{2}),\] (34)

where \((i)\) follows from that we choose \(\lambda\) as a constant. If we choose \(\alpha=\Theta(K^{-\frac{1}{2}}T^{-\frac{1}{2}})\) and \(\beta=\Theta(K^{-1}T^{-\frac{1}{2}})\), we can get \(\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t}+\lambda g_{0}( \theta_{t})\|^{2}]=\mathcal{O}(KT^{-\frac{1}{2}})\). Furthermore, following the same steps as in Appendix C.3, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t}^{*}\|^{2}]= \mathcal{O}(KT^{-\frac{1}{2}}).\]

To achieve an \(\epsilon\)-accurate Pareto stationary point, it requires \(T=\mathcal{O}(K^{2}\epsilon^{-2})\). In this case, each objective requires a similar number of samples \(\mathcal{O}(K^{2}\epsilon^{-2})\) in \(\xi(\xi^{\prime})\) and \(\zeta\), respectively. 

**Convergence under objective sampling.** We next analyze the convergence of SDMGrad-OS.

**Theorem 8** (Restatement of Theorem 4).: _Suppose Assumptions 1-3 are satisfied and \(S=1\). Set \(\gamma=\frac{K}{n}\), \(\alpha_{t}=\alpha=\Theta(K^{-\frac{1}{2}}\gamma^{-\frac{1}{2}}T^{-\frac{1}{2}})\), \(\beta_{t}=\beta=\Theta(K^{-1}\gamma^{-1}T^{-\frac{1}{2}})\) and \(\lambda\) as a constant. The iterates of the proposed SDMGrad-OS algorithm satisfy,_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t}^{*}\|^{2}]=\mathcal{ O}(K\gamma T^{-\frac{1}{2}}).\]

Proof.: In SDMGrad-OS, the vector for updating \(\theta_{t}\) is \(d^{\prime}=\frac{K}{n}H(\theta_{t};\zeta,\widetilde{S})w_{t+1}+\frac{\lambda K }{n}h_{0}(\theta_{t};\zeta,\widetilde{S})\). Using the same function defined in eq. (29), we have

\[l^{\prime}(\theta_{t+1})\leq l^{\prime}(\theta_{t})+\alpha_{t}\langle G(\theta_{t})w+\lambda g_{0}( \theta_{t}),-d^{\prime}\rangle+\frac{l_{1}^{\prime}\alpha_{t}^{2}}{2}\|d^{ \prime}\|^{2}.\]

Then by taking expectation over \(\zeta\) and \(\widetilde{S}\), we have

\[\mathbb{E}[l^{\prime}(\theta_{t+1})-l^{\prime}(\theta_{t})]\leq \alpha_{t}\mathbb{E}[\langle G(\theta_{t})w+\lambda g_{0}(\theta _{t}),-G(\theta_{t})w_{t+1}+\lambda g_{0}(\theta_{t})\rangle]+\frac{l^{\prime} \alpha_{t}^{2}}{2}\mathbb{E}[\|d^{\prime}\|^{2}]\]\[= \alpha_{t}E[\langle G(\theta_{t})w+\lambda g_{0}(\theta_{t}),G(\theta_ {t})(w_{t}-w_{t+1})\rangle]\] \[+\alpha_{t}\mathbb{E}[\langle G(\theta_{t})w_{t}-G(\theta_{t})w,G( \theta_{t})w_{t}+\lambda g_{0}(\theta_{t})\rangle]\] \[-\mathbb{E}[\|G(\theta_{t})w_{t}+\lambda g_{0}(\theta_{t})\|^{2}] +\frac{l^{\prime}_{t}\alpha_{t}^{2}}{2}\mathbb{E}[\|d^{\prime}\|^{2}]\] \[\leq \alpha_{t}\mathbb{E}[\|(G(\theta_{t})w+\lambda g_{0}(\theta_{t}) )^{T}G(\theta_{t})\|\|w_{t}-w_{t+1}\|]\] \[+\alpha_{t}\mathbb{E}[\langle G(\theta_{t})w_{t}-G(\theta_{t})w,G( \theta_{t})w_{t}+\lambda g_{0}(\theta_{t})\rangle]\] \[-\mathbb{E}[\|G(\theta_{t})w_{t}+\lambda g_{0}(\theta_{t})\|^{2} ]+\frac{l^{\prime}_{t}\alpha_{t}^{2}}{2}\mathbb{E}[\|d^{\prime}\|^{2}]\] (35)

Then following the same steps in eq. (31) and eq. (32), we can obtain,

\[\mathbb{E}[l^{\prime}(\theta_{t+1})-l^{\prime}(\theta_{t})]\leq \alpha_{t}\beta_{t}C^{\prime}_{3}+\frac{\alpha_{t}}{2\beta_{t}} \mathbb{E}[\|w_{t}-w\|^{2}-\|w_{t+1}-w\|^{2}]+\frac{\alpha_{t}\beta_{t}}{2}C^{ \prime 2}_{1}\] \[- \alpha_{t}\mathbb{E}[\|G(\theta_{t})w_{t}+\lambda g_{0}(\theta_{ t})\|^{2}]+\frac{l^{\prime}_{t}\alpha_{t}^{2}}{2}C^{\prime}_{2},\] (36)

where \(C^{\prime 2}_{1}=4\gamma^{4}(1+\lambda^{2})(n\sigma_{0}^{2}+C_{g})^{2}\), \(C^{\prime}_{2}=4\gamma^{2}(1+\lambda^{2})(n\sigma_{0}^{2}+C^{2}_{g})\), and \(C^{\prime}_{3}=\gamma^{2}(1+\lambda)^{2}C^{2}_{g}(n\sigma_{0}^{2}+C_{g})^{2}\). Then we take \(\alpha_{t}=\alpha\) and \(\beta_{t}=\beta\) as constants and telescope the above inequality,

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t}+ \lambda g_{0}(\theta_{t})\|^{2}]\leq \frac{1}{\alpha T}\mathbb{E}[l^{\prime}(\theta_{0})-l^{\prime}( \theta_{T})]+\frac{1}{2\beta T}\mathbb{E}[\|w_{0}-w\|^{2}-\|w_{T}-w\|^{2}]\] \[+\beta(C^{\prime}_{3}+\frac{C^{\prime 2}_{1}}{2})+\frac{l^{ \prime}_{t}\alpha}{2}C^{\prime}_{2}\] \[\stackrel{{(i)}}{{\leq}} \mathcal{O}(\frac{1}{\alpha T}+\alpha\gamma K+\frac{1}{\beta T}+ \beta\gamma^{2}K^{2}),\] (37)

where \((i)\) follows from that we choose \(\lambda\) as constant. Similarly, if we choose \(\alpha=\Theta(K^{-\frac{1}{2}}\gamma^{-\frac{1}{2}}T^{-\frac{1}{2}})\) and \(\beta=\Theta(K^{-1}\gamma^{-1}T^{-\frac{1}{2}})\), we can get \(\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t}+\lambda g_{0}( \theta_{t})\|^{2}]=\mathcal{O}(K\gamma T^{-\frac{1}{2}})\). Furthermore, following the same step as in Appendix C.3, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|G(\theta_{t})w_{t}^{*}\|^{2}]=\mathcal{ O}(K\gamma T^{-\frac{1}{2}}).\]

To achieve an \(\epsilon\)-accurate Pareto stationary point, it requires \(T=\mathcal{O}(\gamma^{2}K^{2}\epsilon^{-2})\). In this case, each objective requires a similar number of samples \(\mathcal{O}(\gamma^{2}K^{2}\epsilon^{-2})\) in \(\xi(\xi^{\prime})\) and \(\zeta\), respectively.