# Neural Injective Functions for Multisets, Measures

and Graphs via a Finite Witness Theorem

 Tal Amir\({}^{1}\) Steven J. Gortler\({}^{2}\) Ilai Avni\({}^{1}\) Ravina Ravina\({}^{1}\) Nadav Dym\({}^{1,3}\)

\({}^{1}\) Faculty of Mathematics, Technion - Israel Institute of Technology, Haifa, Israel

\({}^{2}\) School of Engineering and Applied Sciences, Harvard University, Cambridge, USA

\({}^{3}\) Faculty of Computer Science, Technion - Israel Institute of Technology, Haifa, Israel.

###### Abstract

Injective multiset functions have a key role in the theoretical study of machine learning on multisets and graphs. Yet, there remains a gap between the provably injective multiset functions considered in theory, which typically rely on polynomial moments, and the multiset functions used in practice, which rely on _neural moments_ -- whose injectivity on multisets has not been studied to date.

In this paper, we bridge this gap by showing that moments of neural networks do define injective multiset functions, provided that an analytic non-polynomial activation is used. The number of moments required by our theory is optimal essentially up to a multiplicative factor of two. To prove this result, we state and prove a _finite witness theorem_, which is of independent interest.

As a corollary to our main theorem, we derive new approximation results for functions on multisets and measures, and new separation results for graph neural networks. We also provide two negative results: (1) moments of piecewise-linear neural networks cannot be injective multiset functions; and (2) even when moment-based multiset functions are injective, they can never be bi-Lipschitz.

## 1 Introduction

Multisets are a slight generalization of sets: like sets, they are an unordered collection of elements \(\{\!\{\bm{x}_{1},\ldots,\bm{x}_{k}\}\!\}\), but unlike sets, repetitions are allowed. Multisets arise naturally in many machine-learning tasks. They are the natural way to represent point clouds in \(\mathbb{R}^{3}\), neighborhoods of vertices in graphs, and any other data that has an intrinsic order that is immaterial to the task at hand.

We refer to functions and architectures whose inputs are multisets in \(\mathbb{R}^{d}\) as _multiset functions_ and _multiset architectures_. By definition, these functions do not depend on the order in which the multiset elements are given. This is important not only because the order is irrelevant and thus should not affect the output, but also because otherwise a model may overfit the training data by making predictions based on its intrinsic order.

Multiset architectures are typically constructed using a combination of permutation-invariant operations such as sum- and max-pooling [34], attention mechanisms [23] and sorting [46]. One simple and popular approach, pioneered in the seminal Deep-Sets paper [45], employs multiset functions based solely on sum-pooling. Namely, if the elements of all multisets come from some fixed _alphabet_\(\Omega\), any function \(f:\Omega\rightarrow\mathbb{R}^{m}\) induces a multiset function \(\hat{f}\), to which we refer as the _moment_ of \(f\):

\[\hat{f}\left(\{\!\{\bm{x}_{1},\ldots,\bm{x}_{k}\}\!\}\right)=\sum_{i=1}^{k}f( \bm{x}_{i}).\] (1)While moment functions of the form (1) are simple, they are quite powerful. For example, in [45] it was shown that if \(\Omega\) is countable, and the multisets have no repetitions (so they are just sets), then for an appropriate \(f:\Omega\to\mathbb{R}\), the induced function \(\hat{f}\) maps the input sets _injectively_ to \(\mathbb{R}\).

Injectivity is indeed a desired property for multiset functions. The search for such functions stems from the quest to find an architecture that can approximate _all_ multiset functions. Clearly, if \(\hat{f}\) assigns the same value \(\hat{f}(S_{1})=\hat{f}(S_{2})\) to two different multisets \(S_{1}\neq S_{2}\), then any architecture based on \(\hat{f}\) will yield a poor approximation of a multiset function that assigns different values to \(S_{1}\) and \(S_{2}\).

The authors of [45] showed that injectivity is not only necessary for approximation, but also sufficient: Under the assumption that the alphabet \(\Omega\) is countable, if the moment \(\hat{f}\) of \(f\) maps multisets injectively to \(\mathbb{R}^{m}\), then any multiset function \(F\) can be written as a composition of the form \(F(\{\!\!\{\bm{x}_{1},\ldots,\bm{x}_{k}\}\!\!\})=g\left(\sum_{i=1}^{k}f(\bm{x}_ {i})\right)\). Motivated by this observation, the authors proposed a neural architecture of this form, with the functions \(f\) and \(g\) replaced by Multi-Layer Perceptrons (MLPs). This step was justified by the universal approximation power of MLPs.

These intriguing results inspired further research, mainly focusing on seeking injective multiset functions of the form (1) for _continuous_ alphabets such as \(\Omega=\mathbb{R}^{d}\). Preferably, such functions should (a) have a minimal _embedding dimension_\(m\) while ensuring that \(f:\Omega\to\mathbb{R}^{m}\) induces an injective \(\hat{f}\); and (b) be practical to compute. We next summarize some of these results:

For a countable \(\Omega\), the Deep-Sets paper as well as [44] showed that an embedding dimension \(m=1\) is sufficient. For \(\Omega=\mathbb{R}\), if the multisets contain at most \(n\) elements, and \(f\) is continuous, then an embedding dimension of \(m\geq n\) is necessary and sufficient for injectivity [41, 6, 42].

For \(\Omega=\mathbb{R}^{d}\) and multisets of size at most \(n\), it was shown in [16] that \(m\geq nd\) is necessary for injectivity. As for an upper bound on the required \(m\), while some polynomials discussed in the literature achieve injectivity with a rather high exponential [3, 26, 37] or polynomial [43] dimension \(m\), recent work [9] achieved injectivity with \(m=2nd+1\), using a polynomial \(f\) with randomly chosen coefficients -- thus achieving the lower bound essentially up to a multiplicative factor of \(2\).

While the above works provide injective multiset functions with optimal or near-optimal embedding dimension, these functions are typically polynomials, and not the MLPs used in practice. As mentioned above, many papers [45, 44, 26] justify this by the fact that MLPs can approximate any function, and thus any polynomial. However, using this argument, we have no control on the number of neurons required for injectivity -- which in some cases may be infinite, as we show in Section 4. In this paper, we address this limitation by providing a practical and efficient method to construct functions of the form (1) that are provably injective while having a near-optimal number of neurons. We now state this formally.

### Problem Statement

Let \(\Omega\subseteq\mathbb{R}^{d}\) be a set, to which we refer as an _alphabet_. Denote by \(\mathcal{S}_{\leq n}(\Omega)\) the collection of all multisets \(\{\!\!\{\bm{x}_{1},\ldots,\bm{x}_{k}\}\!\!\}\) with \(\bm{x}_{1},\ldots,\bm{x}_{k}\in\Omega\) and \(k\leq n\). Any function \(f:\Omega\to\mathbb{R}^{m}\) induces a _moment function_\(\hat{f}:\mathcal{S}_{\leq n}(\Omega)\to\mathbb{R}^{m}\) as in (1). If \(\hat{f}\) is injective, we say that \(f\) is _moment injective_ on \(\mathcal{S}_{\leq n}(\Omega)\).

We also consider a natural generalization from multisets to measures, by identifying each multiset \(\{\!\!\{\bm{x}_{1},\ldots,\bm{x}_{k}\}\!\!\}\) with the measure \(\mu=\sum_{i=1}^{k}\delta_{\bm{x}_{i}}\), where \(\delta_{\bm{x}}\) is the Dirac measure that assigns a unit weight to \(\bm{x}\). In this generalized setting, the induced multiset function \(\hat{f}\) of (1) is just the integral of \(f\) with respect to the measure \(\mu\). More generally, we consider signed measures \(\mu=\sum_{i=1}^{n}w_{i}\delta_{\bm{x}_{i}}\), with weights \(w_{i}\in\mathbb{R}\) that can be negative, and points \(\bm{x}_{i}\) that belong to an alphabet \(\Omega\subseteq\mathbb{R}^{d}\). We denote the space1 of all such measures by \(\mathcal{M}_{\leq n}(\Omega)\). A function \(f:\Omega\to\mathbb{R}^{m}\) induces a moment function \(\hat{f}:\mathcal{M}_{\leq n}(\Omega)\to\mathbb{R}^{m}\) defined by

Footnote 1: While we use the term _space_ for \(\mathcal{S}_{\leq n}(\Omega)\) and \(\mathcal{M}_{\leq n}(\Omega)\), note that these are not vector spaces, since the sum of two measures in these spaces might be supported on more than \(n\) points.

\[\hat{f}(\mu)=\int_{\Omega}f(\bm{x})d\mu(\bm{x})=\sum_{i=1}^{n}w_{i}f(\bm{x}_{ i}),\quad\text{where}\quad\mu=\sum_{i=1}^{n}w_{i}\delta_{\bm{x}_{i}}.\] (2)If \(\hat{f}\) is injective, we say that \(f\) is moment-injective on \(\mathcal{M}_{\leq n}(\Omega)\). Naturally, injectivity on \(\mathcal{M}_{\leq n}(\Omega)\) implies injectivity on subsets of this space, such as the space of measures in \(\mathcal{M}_{\leq n}(\Omega)\) that are probability measures, or that have only positive weights. In particular, if \(f\) is moment-injective on \(\mathcal{M}_{\leq n}(\Omega)\), then it is moment-injective on \(\mathcal{S}_{\leq n}(\Omega)\).

To summarize, the main questions we focus on in this paper are:

**Main Questions:** (a) Under what conditions is an MLP \(f\) moment-injective on spaces of multisets \(\mathcal{S}_{\leq n}(\Omega)\) or measures \(\mathcal{M}_{\leq n}(\Omega)\)? (b) How many neurons are needed to achieve this injectivity?

## 2 Main Results

Interestingly, we find that the answers to these two questions largely depend on the activation function. Consider shallow neural networks \(f:\mathbb{R}^{d}\to\mathbb{R}^{m}\) of the form

\[f(\bm{x};\bm{A},\bm{b})=\sigma(\bm{A}\bm{x}+\bm{b}),\quad\bm{A}\in\mathbb{R}^ {m\times d},\ \bm{b}\in\mathbb{R}^{m},\] (3)

with the activation function \(\sigma:\mathbb{R}\to\mathbb{R}\) applied entrywise to \(\bm{A}\bm{x}+\bm{b}\). Suppose that \(\sigma\) is analytic and non-polynomial; such activations include the sigmoid, softplus, tanh, swish and sin. In Section 3 we show that for a large enough \(m\), such networks \(f(\bm{x};\bm{A},\bm{b})\) with random parameters \(\bm{A}\),\(\bm{b}\) are moment-injective on \(\mathcal{M}_{\leq n}(\Omega)\) and on \(\mathcal{S}_{\leq n}(\Omega)\); namely, their induced moment functions \(\hat{f}\) of (2) are injective. This holds for various natural choices of \(\Omega\).

The embedding dimension \(m\) required in (3) depends on the dimension \(d\) of \(\Omega\): For \(\Omega=\mathbb{R}^{d}\), to achieve injectivity on \(\mathcal{S}_{\leq n}(\Omega)\) or \(\mathcal{M}_{\leq n}(\Omega)\), it suffices to take \(m=2nd+1\) or \(m=2n(d+1)+1\) respectively. When \(\Omega\) is countable, \(m=1\) or \(m=2n+1\) are sufficient (corresponding to \(d=0\)). In Appendix C we show that in all these cases, these embedding dimensions are optimal essentially up to a multiplicative factor of two. These results are summarized in Table 1. In Appendix C we also discuss examples where the optimal embedding cardinality for \(\mathcal{M}_{\leq n}(\mathbb{R})\) is obtained.

At the core of our poof of moment injectivity is a theorem which we name the _finite witness theorem_. This theorem enables reducing an infinite family of analytic equality constraints \(\{F\left(\bm{x};\bm{\theta}\right)=0\ |\ \bm{\theta}\in\mathbb{W}\}\) to a finite subset \(\{F\left(\bm{x};\bm{\theta}_{i}\right)=0\ |\ i=1,\dots,m\}\). This theorem generalizes the results in [9], where a special case of it was proved for semialgebraic domains and functions. The theorem we prove here (see Appendix A) applies to a much wider class of domains and functions, among which are analytic functions. In addition to our main result, we use the finite witness theorem to prove moment injectivity of Gaussian functions (Proposition 3.5) and deep MLPs (Proposition 3.6), and we believe it shall find additional applications beyond those discussed in this work.

Negative ResultsWe also prove two negative results for moment-based multiset functions: We show that in contrast to analytic activations, with piecewise linear (PwL) activations, such as ReLU, leaky ReLU and hard arctan, moment injectivity on spaces of measures \(\mathcal{M}_{\leq n}(\Omega)\) with infinite \(\Omega\) is _impossible_. On multiset spaces \(\mathcal{S}_{\leq n}(\Omega)\), moment injectivity with PwL activations can be obtained for some irregular, countable \(\Omega\), such as the alphabet \(\Omega=\Sigma_{\alpha}\) defined in Appendix B, but not for infinite alphabets that arise naturally, like \(\Omega=\mathbb{R}^{d},\mathbb{Z}^{d}\) or \((0,1)^{d}\). These results are summarized in the bottom row of Table 1. The second negative result is that while moments of MLPs with analytic activations can be injective, they can never be stable in the bi-Lipschitz sense. This points to a possible advantage

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Domain & \(\mathcal{M}_{\leq n}(\mathbb{R}^{d})\) & \(\mathcal{S}_{\leq n}(\mathbb{R}^{d})\) & \(\mathcal{M}_{\leq n}(\Sigma)\) & \(\mathcal{S}_{\leq n}(\mathbb{Z}^{d})\) & \(\mathcal{S}_{\leq n}(\Sigma_{\alpha})\) \\ \hline Analytic activation & \(2n(d+1)+1\) & \(2nd+1\) & \(2n+1\) & \(1\) & \(1\) \\ Piecewise-linear activation & \(\infty\) & \(\infty\) & \(\infty\) & \(\infty\) & \(1\) \\ \hline Lower bound & \(n(d+1)\) & \(nd\) & \(n\) & \(1\) & \(1\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: The embedding dimension required for constructing injective functions of measures and multisets. \(\Sigma\subset\mathbb{R}^{d}\) is any infinite countable alphabet. First row: dimensions for which our theorems guarantee injectivity when using analytic non-polynomial activations. Second row: with infinite alphabets, moments of a neural network of any finite size with a piecewise-linear activation cannot be injective, except in the multiset case, with some special countable alphabets such as \(\Sigma_{\alpha}\), defined in Appendix B.1. Third row: lower bounds on the embedding dimension required for injectivity. These bounds show that our results from the first row of the table are optimal essentially up to a factor of two.

of injective multiset functions that are not based on moments, but rather on sorting [3] or max-filters [5]. These multiset functions are not only injective but also bi-Lipschitz.

Implications for learning on multisets and graphsThe result on moment injectivity of MLPs with analytic non-polynomial activation enables us to improve upon two seminal theoretical results in the study of functions on multisets and graphs:

**(a) Universality for multisets.** In Corollaries 6.1 and 6.2, we show that any continuous function on a space of multisets or measures respectively can be presented as a continuous vector-to-vector function composed with a moment function \(\hat{f}\) of an MLP of the form (3). The MLP has the same embedding dimension \(m\) as in Table 1. Essentially, this result replaces the moment-injective polynomials traditionally used in the characterization of multiset functions [45; 42] by MLPs.

**(b) Separation power of Graph Neural Networks.** Famously, the ability of _Message-Passing Neural Networks (MPNNs)_ to separate distinct graphs is at most that of the Weisfeiler-Leman (WL) graph isomorphism test, with equivalence taking place if the multiset functions used in the MPNN are injective [44]. Injective multiset functions are also used in generalizations of this result, such as the equivalence of high-order _Graph Neural Networks (GNNs)_ to high-order WL tests [31; 26], and recent results on geometric GNNs and their corresponding WL tests [15; 16; 25; 33; 8].

Using the fact that an embedding dimension of one is sufficient to achieve injectivity on \(\mathcal{S}_{\leq n}(\Omega)\) with countable \(\Omega\), we show in Theorem 6.3 that standard MPNNs with analytic non-polynomial activations and random parameters have the separation power of WL, even when their architecture only uses a single feature per node. This can be compared on the one hand with the construction in [44], which also requires a single node feature but uses multiset aggregators that are not MLPs, and on the other hand with works that do consider MLPs with ReLU activations [31; 1], but require a number of node features and parameters that depends polylogarithmically on the number of nodes. In contrast, our construction requires a single node feature and a fixed number of parameters (though we have no bound on the number of bits required for achieving separation using floating-point arithmetic). A numerical verification of these results is shown in Figure 1(a), where we show that, on the 600 graphs in the TUDataset [30], MPNNs with three different analytic activations were equivalent to the WL-test even with a single node feature, whereas three different PwL activations were in some cases weaker than WL when a small number of node features was used.

Independently of this work, it was recently proved in [17] that MPNNs with certain analytic activations can separate all trees of depth two, while separation fails with PwL and even piecewise-polynomial activations. Our results here are stronger in that we show separation for _all_ graphs separable by WL, and _all_ analytic non-polynomial activations.

### Notation

We denote vectors by boldface letters, e.g. \(\bm{x},\bm{y}\), and scalars by plain letters \(x,y\). The inner product of \(\bm{a},\bm{x}\) is denoted by \(\bm{a}\cdot\bm{x}\). Throughout this work, the term _measure_ always refers to signed measures.

Figure 1: (a) The number of failures of graph neural networks, with varying hidden dimension and activation, to achieve WL separation on the 600 graphs from the TUDataset [30]. Analytic activations succeed on all graphs, as Theorem 6.3 predicts. (b) The normalized smallest singular value of multiset functions induced by piecewise-linear ReLU-networks and analytic SiLU-networks. Piecewise-linear networks have singularities on squares intersecting the diagonal, leading to non-injectivity. Analytic networks are moment injective, but have singularities on the diagonal, which leads to a non-Lipschitz inverse. See the end of Section 5 for more details.

## 3 Moment injectivity with analytic activations

In this section, we prove moment injectivity for MLPs with analytic non-polynomial activations. We begin by showing that for any non-polynomial function \(\sigma:\mathbb{R}\to\mathbb{R}\), a measure \(\mu\in\mathcal{M}_{\leq n}(\Omega)\) is uniquely determined by the integrals of all functions \(\{\sigma(\bm{a}\cdot\bm{x}+b)\mid\bm{a}\in\mathbb{R}^{d},b\in\mathbb{R}\}\). When this holds, we say that \(\sigma\) is _discriminatory_:

**Definition 3.1**.: Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be a continuous function. We say that \(\sigma\) is _discriminatory_ if for any two signed Borel measures \(\mu,\mu^{\prime}\) on \(\mathbb{R}^{d}\) that are distinct (i.e. \(\mu\neq\mu^{\prime}\)), finite (i.e. \(|\mu\left(A\right)|,|\mu^{\prime}\left(A\right)|<\infty\) for all Borel \(A\subseteq\mathbb{R}^{d}\)) and compactly supported, there exist \(\bm{a}\in\mathbb{R}^{d}\), \(b\in\mathbb{R}\) such that

\[\int_{\mathbb{R}^{d}}\sigma(\bm{a}\cdot\bm{x}+b)d\mu(\bm{x})\neq\int_{\mathbb{ R}^{d}}\sigma(\bm{a}\cdot\bm{x}+b)d\mu^{\prime}(\bm{x}).\] (4)

The definition of discriminatory activation functions comes from2 Cybenko's celebrated paper on the universality of MLPs [7], where it was proved that sigmoid-like activations are discriminatory. This, in turn, was used to prove the universality of MLPs with such activations. In later papers [24; 32] it was shown that universality can be achieved by _all_ continuous non-polynomial activations. In the following simple proposition, we use a reverse argument to that used by Cybenko, and show that activations that allow for universality are automatically discriminatory:

Footnote 2: with a minor change: we do not require that all measures considered are supported on the same fixed compact set.

**Proposition 3.2**.: _Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be a continuous function that is not a polynomial; then \(\sigma\) is discriminatory._

Proof idea.: Suppose that \(\int\sigma(\bm{a}\cdot\bm{x}+b)d\mu=\int\sigma(\bm{a}\cdot\bm{x}+b)d\mu^{\prime}\) for all \(\bm{a},b\). By the universality theorem for shallow MLPs [32], all continuous functions can be approximated by linear combinations of functions of the form \(\sigma(\bm{a}\cdot\bm{x}+b)\). Thus, for any continuous function \(f,\int fd\mu=\int fd\mu^{\prime}\). Since a measure is uniquely determined by its integrals of all continuous functions, \(\mu\) is equal to \(\mu^{\prime}\). 

Next, we shall prove our main result: If \(\sigma\) is _analytic_ and discriminatory, then shallow MLPs of reasonable width with \(\sigma\) as activation are moment injective.

**Theorem 3.3**.: _Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be an analytic non-polynomial function. Let \(n,d\in\mathbb{N}\), and set \(m=2n(d+1)+1\). Then for Lebesgue almost any \(\bm{A}\in\mathbb{R}^{m\times d},\bm{b}\in\mathbb{R}^{m}\), the shallow MLP \(f(\bm{x})=\sigma(\bm{A}\cdot\bm{x}+\bm{b})\) is moment injective on \(\mathcal{M}_{\leq n}(\mathbb{R}^{d})\); namely, the function \(\hat{f}:\mathcal{M}_{\leq n}(\Omega)\to\mathbb{R}^{m}\) given by_

\[\hat{f}\left(\mu\right)=\sum_{i=1}^{n}w_{i}\sigma\left(\bm{A}\bm{x}_{i}+\bm{b} \right)\ \ \text{for}\ \ \mu=\sum_{i=1}^{n}w_{i}\delta_{\bm{x}_{i}}\] (5)

_is injective._

_For moment injectivity on \(\mathcal{S}_{\leq n}(\mathbb{R}^{d})\), it suffices to take \(m=2nd+1\). For \(\mathcal{M}_{\leq n}(\Sigma)\) or \(\mathcal{S}_{\leq n}(\Sigma)\) with countable \(\Sigma\), \(m=2n+1\) and \(m=1\) respectively are sufficient._

Our proof of Theorem 3.3 is based on a separate theorem, which we name the _finite witness theorem_. This theorem enables us to show that, since any two measures can be discriminated by an integral \(\int\sigma(\bm{a}\cdot\bm{x}+b)d\mu(\bm{x})\) for some choice of parameters \(\bm{a}\),\(b\), there exists a finite number of _witness_ parameters \((\bm{a}_{i},b_{i})_{i=1}^{m}\) that are sufficient for discriminating between _any_ two measures. This holds under the assumption that the number of points in both measures is bounded. We shall now state a simple version of this theorem, which suffices for proving Theorem 3.3.

**Theorem 3.4**.: _(Finite Witness Theorem, simple version) Let \(\mathbb{M}\subseteq\mathbb{R}^{L}\) be a countable union of affine sets, each of which is of dimension \(\leq D\). Let \(\mathbb{W}\subseteq\mathbb{R}^{D_{\theta}}\) be open and connected. Let \(F\left(\bm{x};\bm{\theta}\right):\mathbb{M}\times\mathbb{W}\to\mathbb{R}\) be an analytic function. Then for almost any \(\left(\bm{\theta}^{(1)},\ldots,\bm{\theta}^{(2D+1)}\right)\in\mathbb{W}^{2D+1}\), the following set equality holds:_

\[\{(\bm{x},\bm{y})\in\mathbb{M}\times\mathbb{M}\ |\ F\left(\bm{x};\bm{ \theta}\right)=F\left(\bm{y};\bm{\theta}\right),\ \forall\bm{\theta}\in\mathbb{W}\}=\] \[\{(\bm{x},\bm{y})\in\mathbb{M}\times\mathbb{M}\ |\ F\left(\bm{x};\bm{\theta}^{(i)}\right)=F \left(\bm{y};\bm{\theta}^{(i)}\right),\ \forall i=1,\ldots 2D+1\}.\]Using the finite witness theorem, we are now ready to prove Theorem 3.3.

Proof of Theorem 3.3.: Recall that a signed measure \(\mu\in\mathcal{M}_{\leq n}(\mathbb{R}^{d})\) can be parameterized, albeit not uniquely, by a matrix \(\bm{X}=(\bm{x}_{1},\ldots,\bm{x}_{n})\in\mathbb{R}^{d\times n}\) representing \(n\) points in \(\mathbb{R}^{d}\), and a weight vector \(\bm{w}=(w_{1},\ldots,w_{n})\), such that \(\mu=\sum_{i=1}^{n}w_{i}\delta_{\bm{x}_{i}}\). Let \(\mathbb{M}\) be the space of measure parameters

\[\mathbb{M}=\{(\bm{w},\bm{X})\in\mathbb{R}^{n}\times\mathbb{R}^{d\times n}\}.\]

Similarly, let \(\mathbb{W}\) be the space of parameters

\[\mathbb{W}=\{(\bm{a},b)\in\mathbb{R}^{d}\times\mathbb{R}\}.\]

Define \(F:\mathbb{M}\times\mathbb{W}\to\mathbb{R}\) by

\[F(\bm{w},\bm{X};\bm{a},b)=\sum_{i=1}^{n}w_{i}\sigma(\bm{a}\cdot\bm{x}_{i}+b).\] (6)

We now invoke the finite witness theorem. Set \(m=2n(d+1)+1\), and note that \(m=2\dim\left(\mathbb{M}\right)+1\). Recall that \(F\) is analytic. According to Theorem 3.4, for almost any choice of \((\bm{a}_{i},b_{i})_{i=1}^{m}\in\mathbb{W}\),

\[\begin{split}&\{((\bm{w},\bm{X})\,,(\bm{w}^{\prime},\bm{X}^{ \prime}))\in\mathbb{M}\times\mathbb{M}\,\mid\,F\left(\bm{w},\bm{X};\bm{a},b \right)=F\left(\bm{w}^{\prime},\bm{X}^{\prime};\bm{a},b\right),\;\forall\,( \bm{a},b)\in\mathbb{W}\}=\\ &\{((\bm{w},\bm{X})\,,(\bm{w}^{\prime},\bm{X}^{\prime}))\in \mathbb{M}\times\mathbb{M}\,\mid\,F\left(\bm{w},\bm{X};\bm{a}_{i},b_{i}\right) =F\left(\bm{w}^{\prime},\bm{X}^{\prime};\bm{a}_{i},b_{i}\right),\;\forall i=1, \ldots,m\}.\end{split}\] (7)

Let \(\bm{A}\in\mathbb{R}^{m\times d}\) with rows \(\bm{a}_{1},\ldots,\bm{a}_{m}\), and \(\bm{b}=(b_{1},\ldots,b_{m})\). Suppose that \(\bm{A}\),\(\bm{b}\) indeed satisfy (7).

Let \(\mu,\mu^{\prime}\in\mathcal{M}_{\leq n}(\Omega)\) be two measures with parameters \((\bm{w},\bm{X})\), \((\bm{w}^{\prime},\bm{X}^{\prime})\) respectively. Equation (7) implies that if the function \(\hat{f}\) of (5) satisfies \(\hat{f}\left(\mu\right)=\hat{f}\left(\mu^{\prime}\right)\), then \((\bm{w},\bm{X})\), \((\bm{w}^{\prime},\bm{X}^{\prime})\) are not separated by the entire family of functions \(\{F\left(\,\bm{\cdot}\,,\bm{a},b\right)\,\mid\,\bm{a}\in\mathbb{R}^{d},b\in \mathbb{R}\}\). Since \(\sigma\) is discriminatory, this in turn implies that \(\mu=\mu^{\prime}\). This concludes the proof of moment injectivity on \(\mathcal{M}_{\leq n}(\mathbb{R}^{d})\).

If we are only interested in moment injectivity on \(\mathcal{S}_{\leq n}(\mathbb{R}^{d})\), it is sufficient to apply the theorem to

\[\mathbb{M}=\bigcup_{\bm{w}\in\{0,1\}^{n}}\{(\bm{w},\bm{X})\,\mid\,\bm{X}\in \mathbb{R}^{d\times n}\},\]

which is a finite union of affine subspaces of dimension \(D=nd\). Thus, Theorem 3.4 only requires \(m=2nd+1\) to achieve injectivity on \(\mathcal{S}_{\leq n}(\mathbb{R}^{d})\). Similarly, when considering \(\mathcal{M}_{\leq n}(\Sigma)\) with a countable \(\Sigma\), the theorem can be applied to a domain \(\mathbb{M}\) that can be written as a countable union of affine spaces of dimension \(n\), which yields \(m=2n+1\). Finally, \(\mathcal{S}_{\leq n}(\Sigma)\) is a countable union of points, namely zero-dimensional affine subspaces, and therefore \(m=1\) is sufficient in this case. 

### More on the finite witness theorem

The finite witness theorem can be used to prove moment injectivity for functions beyond the activated inner-product form of (5). As an example, we show in the following proposition that Gaussian functions with random parameters are moment injective:

**Proposition 3.5**.: _Let \(n,d\in\mathbb{N}\) and set \(m=2n(d+1)+1\). Let \(\mathbb{W}=(\bm{y},\sigma)\in\mathbb{R}^{d}\times\mathbb{R}_{+}\). Then for Lebesgue almost any \((\bm{y}_{i},\sigma_{i})_{i=1}^{m}\in\mathbb{W}^{m}\), the function_

\[f(\bm{x})=\left(\exp\left(-\frac{\|\bm{x}-\bm{y}_{1}\|^{2}}{\sigma_{1}^{2}} \right),\ldots,\exp\left(-\frac{\|\bm{x}-\bm{y}_{m}\|^{2}}{\sigma_{m}^{2}} \right)\right)\]

_is moment injective on \(\mathcal{M}_{\leq n}(\mathbb{R}^{d})\)._

Proof idea.: Any two measures with bounded support can be separated by the moment of a Gaussian function supported on a small ball around a point where the measures disagree. Thus, a measure in \(\mathcal{M}_{\leq n}(\Omega)\) is uniquely defined by the continuous family of all its Gaussian moments. The finite witness theorem then shows that a finite number \(m\) suffices.

The full version of the finite witness theorem (Theorem A.2), discussed in Appendix A, is more general than Theorem 3.4. In this version, the class of sets admissible as \(\mathbb{M}\) is the class of \(\sigma\)-subanalytic sets. While its definition is technically involved (see Appendix A), this class is quite vast: it includes all open sets, all semialgebraic sets (including affine spaces, polygons, and closed \(\ell_{2}\)-balls), and countable unions thereof. The analyticity assumption on \(F\) is also substantially relaxed to \(\sigma\)-subanalyticity, though this requires an additional condition (13) in the theorem assumptions.

The proof of the finite witness theorem is non-trivial, and we regard it as the main technical contribution of this work. In essence, the proof generalizes a similar result in [9], which only applies to polynomial functions on sets defined by polynomial constraints -- known as _semialgebraic sets_. This class of sets has several nice properties, which the proof in [9] relies on: It is closed under linear projections, finite unions, finite intersections, and complements. Moreover, any semialgebraic set is a finite union of smooth manifolds.

Our generalization from the polynomial to the analytic setting consists of two steps: First, we generalize the theorem to a larger class of sets, called _globally subanalytic sets_, which are known to be an _\(o\)-minimal system_ -- essentially, a family of sets that has the same nice properties of semialgebraic sets mentioned above. This generalization is straightforward; however, it does not allow \(F\) to be an arbitrary analytic function, and thus does not suffice even to prove the weaker version, Theorem 3.4. Our second step is then to observe that our proof carries through also when considering countable unions of globally subanalytic sets, which we name _\(\sigma\)-subanalytic sets_. This, in turn, paves the way to prove the full version of the finite witness theorem.

Using the more general version of the theorem, we can prove the following proposition, which in particular implies moment injectivity of _deep_ networks, provided that the last activation is analytic:

**Proposition 3.6**.: _Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be an analytic non-polynomial function. Let \(n,d\in\mathbb{N}\) and set \(m=2n(d+1)+1\). Let \(f:\mathbb{R}^{d}\to\mathbb{R}^{L}\) be an injective function that is a composition of PwL functions and analytic functions. Then for Lebesgue almost any \(\bm{A}\in\mathbb{R}^{m\times L},\bm{b}\in\mathbb{R}^{m}\), the function \(\sigma(\bm{A}f(\bm{x})+\bm{b})\) is moment injective on \(\mathcal{M}_{\leq n}(\mathbb{R}^{d})\)._

In particular, \(F\) could be a neural network that has increasing widths, linear layers with full rank, and injective activations that are either PwL or analytic (such as leaky ReLU or sigmoid). Therefore, Proposition 3.6 shows that increasing the network depth will not have a negative effect on its moment-injectivity. While this may seem trivial, what is not immediate in this formulation is that the embedding dimension \(m\) depends linearly on \(n\cdot d\) rather than \(n\cdot L\). The reason this is true is that the shallow neural network applied to \(F(\bm{x})\) will only'see' inputs that originate from the set \(F\left(\mathbb{R}^{d}\right)\), and in Appendix A we show that this is a \(\sigma\)-subanalytic set of dimension \(\leq d\).

## 4 Failure of moment injectivity for piecewise-linear functions

In this section, we show that moments of neural networks with piecewise-linear activations (such as ReLU, leaky ReLU and the hard hyperbolic tangent) cannot be injective when the alphabet is infinite, except for some singular cases discussed below.

**Proposition 4.1**.: _Let \(d,m\) and \(n\geq 2\) be natural numbers and \(\Omega\subseteq\mathbb{R}^{d}\) an open set. If \(\bm{\psi}:\mathbb{R}^{d}\to\mathbb{R}^{m}\) is piecewise linear, then it is not moment injective on \(\mathcal{S}_{\leq n}(\Omega)\)._

Proof.: There exists some open \(U\subset\mathbb{R}^{d}\) such that \(\bm{\psi}(\bm{x})\) is of the form \(\bm{\psi}(\bm{x})=\bm{A}\bm{x}+\bm{b}\) in \(U\). Let \(\bm{x}_{0}\in U\) and let \(\bm{d}\neq\bm{0}\in\mathbb{R}^{d}\). For small enough \(\epsilon>0\), we have that \(\bm{x}_{0}+\epsilon\bm{d}\) and \(\bm{x}_{0}-\epsilon\bm{d}\) are in \(U\). It follows that the multisets \(\{\!\bm{x}_{0},\bm{x}_{0}\!\}\) and \(\{\!\bm{x}_{0}-\epsilon\bm{d},\bm{x}_{0}+\epsilon\bm{d}\!\}\) have the same moments:

\[\bm{\psi}(\bm{x}_{0})+\bm{\psi}(\bm{x}_{0})=2\left(\bm{A}\bm{x}_{0}+\bm{b} \right)=\bm{A}(\bm{x}_{0}-\epsilon\bm{d})+\bm{b}+\bm{A}(\bm{x}_{0}+\epsilon \bm{d})+\bm{b}=\bm{\psi}(\bm{x}_{0}-\epsilon\bm{d})+\bm{\psi}(\bm{x}_{0}+ \epsilon\bm{d}).\]

This proves that \(\psi\) is not moment injective on \(\mathcal{S}_{\leq n}(\Omega)\). 

The basic idea behind the above proof is that inside a linear region of \(\bm{\psi}\), different multisets with the same center of mass have the same moments. The same idea can be used to prove failure of moment injectivity of PwL functions on \(\mathcal{M}_{\leq n}(\Omega)\) for _any_ infinite \(\Omega\), and on \(\mathcal{S}_{\leq n}(\mathbb{Z}^{d})\). On the other hand, PwL networks can be moment injective on \(\mathcal{M}_{\leq n}(\Omega)\) with finite \(\Omega\), as well as on \(\mathcal{S}_{\leq n}(\Sigma)\) when \(\Sigma\) is a somewhat pathological infinite countable alphabet. These results are described in Appendix B.

## 5 Failure of bi-Lipschitzness for general moment functions

In Section 3 we have shown that a neural network \(f:\mathbb{R}^{d}\to\mathbb{R}^{m}\) with analytic non-polynomial activation can induce an injective multiset function \(\hat{f}:\mathcal{S}_{\leq n}(\mathbb{R}^{d})\to\mathbb{R}^{m}\). Ideally, we wish such \(\hat{f}\) to be _bi-Lipschitz_, meaning that there exist constants \(0<c\leq C\) such that

\[c\cdot W_{2}(S_{1},S_{2})\leq\|\hat{f}(S_{1})-\hat{f}(S_{2})\|\ \leq C\cdot W_{2}(S_{1},S_{2}), \quad\forall S_{1},S_{2}\in\mathcal{S}_{\leq n}(\mathbb{R}^{d}),\] (8)

where \(W_{2}(S_{1},S_{2})\) is the 2-Wasserstein distance between the two measures \(\mu_{1},\mu_{2}\) that assign uniform weights to the points in \(S_{1},S_{2}\) respectively. Unfortunately, we find that _any_ moment function \(\hat{f}\) induced by some \(f:\mathbb{R}^{d}\to\mathbb{R}^{m}\) cannot be bi-Lipschitz, assuming that \(f\) is differentiable in at least one point.

**Proposition 5.1**.: _Let \(n\geq 2\), \(d,m\in\mathbb{N}\), and let \(f:\mathbb{R}^{d}\to\mathbb{R}^{m}\) be differentiable at some \(\bm{x}_{0}\in\mathbb{R}^{d}\). Then the induced moment function \(\hat{f}:\mathcal{S}_{\leq n}(\mathbb{R}^{d})\to\mathbb{R}^{m}\) defined in (1) is not bi-Lipschitz._

Figure 1(b) illustrates the underlying reason for this failure of bi-Lipschitzness, and its relation to the non-injectivity of PwL moments: consider a shallow neural network \(f:\mathbb{R}\to\mathbb{R}^{10}\) with ReLU activations, and its induced moment function \(\hat{f}\big{(}\{\!\!\{x_{1},x_{2}\}\!\}\!\big{)}=f(x_{1})+f(x_{2})\) on multisets in \(\mathcal{S}_{\leq 2}\left(\mathbb{R}\right)\). The left-hand side visualizes the ratio \(\sigma_{2}/\sigma_{1}\) of the smallest and largest singular values of the differential matrix \(D\hat{f}\). The function \(f\) is PwL, with four linear regions \(I_{1},\ldots,I_{4}\) in \([0,1]\). The linear regions of \(\hat{f}\) in \([0,1]^{2}\) are thus the rectangles \(I_{i}\times I_{j}\). As seen in the figure, there are degeneracies in the rectangles that intersect the diagonal, as for small enough \(\epsilon\), \(\hat{f}(\{\!\!\{x_{0}+\epsilon,x_{0}-\epsilon\}\!\})=\hat{f}(\{\!\!\{x_{0},x_ {0}\}\!\})\) as in the proof of Proposition 4.1. The right-hand side visualizes the same ratio when the analytic SiLU activation is used instead of ReLU. We see that \(D\hat{f}\) is singular on the diagonal. Intuitively, this is because the differentiability of \(f\) implies that it behaves locally like an affine function. This leads to singularities of \(\hat{f}\) on the diagonal, which do not prevent it from being injective, but do prevent it from being bi-Lipschitz. A proof of this phenomenon is given in the appendix. See also Theorem 21 in [4], which independently proved a similar result for general invariant embeddings.

## 6 Applications: Universal Approximation and Graph Separation

As mentioned in the introduction, injective multiset functions can be used to construct multiset architectures with universal approximation power, and to prove separation results for graph neural networks. In this section, we present some immediate corollaries of our results for these two applications. Proofs are in Appendix D.3.

### Universal approximation of functions on multisets and measures

Our first approximation result focuses on multisets of a fixed size \(n\) with an alphabet \(K\subseteq\mathbb{R}^{d}\) that is compact. Any such multiset is determined by a choice of \(n\) vectors in \(K\), possibly with repetitions and irrespective of order. Thus, multiset functions on this space are equivalent to permutation-invariant functions on \(K^{n}\). Using the finite witness theorem and a basic topological argument, we prove:

**Corollary 6.1**.: _Let \(n,d\in\mathbb{N}\) and set \(m=2nd+1\). Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be an analytic non-polynomial function. Let \(K\subseteq\mathbb{R}^{d}\) be a compact set. Then there exist \(\bm{A}\in\mathbb{R}^{m\times d},\bm{b}\in\mathbb{R}^{d}\) such that for any continuous permutation-invariant \(f:K^{n}\to\mathbb{R}\), there exists a continuous \(F:\mathbb{R}^{m}\to\mathbb{R}\) such that_

\[f(\bm{X})=F\left(\sum_{j=1}^{n}\sigma(\bm{A}\bm{x}_{j}+\bm{b})\right),\quad \forall\bm{X}=(\bm{x}_{1},\ldots,\bm{x}_{n})\in K^{n}.\] (9)

Combining Corollary 6.1 with the universality of MLPs, we get that any continuous permutation-invariant function on \(K^{n}\) can be approximated by expressions of the form (9) with \(F\) replaced by an MLP. Similar results were obtained for moments of polynomials rather than of MLPs in [45, 42, 9].

It is worth noting that an analogue of Corollary 6.1 cannot hold with a piecewise-linear \(\sigma\), assuming that \(K\) has a non-empty interior. This is because by Proposition 4.1, any fixed moment function induced by a PwL MLP will not be able to separate all multisets, whereas any two distinct multisets can be separated by some continuous \(f\). Though, with a PwL \(\sigma\), one may approximate any given \(f\) toarbitrary precision, by taking the embedding dimension \(m\) to infinity. In contrast, with an analytic \(\sigma\), we are able to specify a _finite_\(m=2nd+1\) for which exact equality in (9) is guaranteed.

Since our injectivity results on multisets extend to measures, it is natural to seek an extension of the above approximation result to functions defined on measures. Denote by \(\mathcal{P}_{\leq n}(K)\) the space of probability measures supported on \(\leq n\) points in \(K\subseteq\mathbb{R}^{d}\), endowed with the 2-Wasserstein metric.

**Corollary 6.2**.: _Let \(n,d\in\mathbb{N}\) and set \(m=2n(d+1)+1\). Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be analytic and non-polynomial. Let \(K\subseteq\mathbb{R}^{d}\) be compact. Then there exist \(\bm{A}\in\mathbb{R}^{m\times d},\bm{b}\in\mathbb{R}^{m}\) such that for any continuous (in the \(2\)-Wasserstein sense) \(f:\mathcal{P}_{\leq n}(K)\to\mathbb{R}\), there exists a continuous \(F:\mathbb{R}^{m}\to\mathbb{R}\) such that_

\[f(\mu)=F\left(\int_{\bm{x}\in K}\sigma(\bm{A}\bm{x}+\bm{b})d\mu(\bm{x})\right),\quad\forall\mu\in\mathcal{P}_{\leq n}(K).\]

It follows from Corollary 6.2 that any continuous function \(f:\mathcal{P}_{\leq n}(K)\to\mathbb{R}\) with compact \(K\subseteq\mathbb{R}^{d}\) can be approximated to arbitrary precision by functions of the form \(\hat{f}(\mu)=F\left(\int_{\bm{x}\in K}\sigma(\bm{A}\bm{x}+\bm{b})d\mu(\bm{x})\right)\), with \(\bm{A}\in\mathbb{R}^{m\times d},\bm{b}\in\mathbb{R}^{m}\), \(m=2nd+1\), and \(F\) being an MLP.

### Graph separation

We now discuss the implications of Theorem 3.3 for graph separation, using terminology from [44]. Let \(\mathcal{G}_{\leq n}(\Sigma)\) be the collection of all graphs \(G=(V,E,\bm{h}^{(0)})\) with at most \(n\) vertices, endowed with vertex features \(\bm{h}^{(0)}_{v}\in\Sigma\), where \(\Sigma\subseteq\mathbb{R}^{d}\) is a countable alphabet. We consider GIN-like [44] MPNNs that recursively, for \(t=1,\ldots,T\), calculate node features \(\bm{h}^{(t)}_{v}\) from the previous features \(\bm{h}^{(t-1)}_{v}\) by

\[\bm{h}^{(t)}_{v}=\sum_{u\in\mathcal{N}(v)}\sigma\left(\bm{A}^{(t)}\left(\eta^ {(t)}\bm{h}^{(t-1)}_{v}+\bm{h}^{(t-1)}_{u}\right)+\bm{b}^{(t)}\right).\] (10)

After the \(T\) iterations are concluded, a global feature is computed via a _readout function_:

\[\bm{h}_{G}=\sum_{v\in V}\sigma\left(\bm{A}^{(T+1)}\bm{h}^{(T)}_{v}+\bm{b}^{( T+1)}\right).\] (11)

We choose all features \(\bm{h}_{G}\) and \(\bm{h}^{(t)}_{v}\) for \(1\leq t\leq T\), to have the same dimension \(m\). Based on the fact that MPNNs are equivalent to \(1\)-WL when the multiset functions are injective [44], and on our injectivity results for countable alphabets, we prove that:

**Theorem 6.3**.: _Let \(n,d,T\in\mathbb{N}\) and let \(\Sigma\subseteq\mathbb{R}^{d}\) be countable. Let \(m\geq 1\) be any integer. Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be an analytic non-polynomial function. Then for Lebesgue almost any choice of \(\bm{A}^{(t)},\bm{b}^{(t)}\) and \(\eta^{(t)}\), the MPNN defined in (10) and (11) assigns different global features to any pair of graphs \(G_{1},G_{2}\in\mathcal{G}_{\leq n}(\Sigma)\) that can be separated by \(T\) iterations of 1-WL._

Graph separation with continuous featuresUp to now, we have discussed graphs with node features coming from a countable alphabet. Since Theorem 3.3 applies to multisets with continuous alphabets, it can be applied to separation of graphs with continuous node features as well. In particular, the paper [15] explains how the random semialgebraic multiset function from [9] can be used to construct architectures for graphs with continuous features, whose separation power is equivalent to WL tests. Their focus is on showing that the embedding dimension in their construction depends linearly on the dimension of the _graph space_, rather than grows exponentially with the number of message-passing iterations \(T\). Similar results can now be obtained by using our random analytic multiset functions. We leave a full description of these aspects to future work.

## 7 Experiments

Empirical injectivity and bi-Lipschitzness We empirically investigated the injectivity and bi-Lipschitzness of moments of shallow networks of the form (3), by randomly generating a large number of pairs of multisets of \(n\) vectors in \(\mathbb{R}^{d}\), and computing the optimal constants \(c,C\) for which (8) holds for the generated pairs. The ratio \(c/C\) for varying activations and embedding dimension \(m\) is shown in Figure 2. Here \(d=3\) and \(n=1000\). Similar qualitative results were obtained for other values of \(d\) and \(n\); see Figure 4 in Appendix E.

We observe several interesting phenomena: First, at low embedding-dimensions, the ratio \(c/C\) for PwL networks is exactly zero, indicating that they are not injective even on the finite sample set. In contrast, for analytic activations, \(c/C\) is always positive. Indeed, we expect analytic activations to be injective on a finite sample even with embedding dimension \(m=1\), since a finite sample set has an intrinsic dimension of zero. Next, we observe that \(c/C\) naturally improves as \(m\) increases. Finally, we note that even for high \(m\), \(c/C\) is rather small. Indeed, if it were possible to consider _all_ pairs of multisets when computing \(c/C\), we would get zero for all activations and all embedding dimensions, as follows from Theorem 5.1. For additional details on this experiment, see Appendix E.1.

Graph SeparationTo validate Theorem 6.3, we conducted the following experiment: we considered 600 graphs from the TUDataset [30]. On each graph we ran three iterations of the WL test, and three iterations of MPNNs with the Graph Convolutional Layers from [18] with different activations and hidden dimensions. Our goal was to check in how many graphs the MPNNs returned a vertex coloring that differs from the coloring provided by the WL test. The results are shown in Figure 1(a). As seen in the table, with the three _analytic_ activations tested, the vertex coloring of MPNN was always equivalent to 1-WL, even with a hidden dimension of 1. On the other hand, for the three _PwL_ activations, there were inconsistencies in about 1% of the graphs, even with a hidden dimension of 50.

We note that while analytic activations fully succeeded in separation, in some cases the separation was rather weak: while the distance between features of non-equivalent nodes computed by the MPNNs was typically around \(0.1\), the least-separated features had a distance of \(\sim 10^{-7}\). In future work, it could be interesting to investigate whether MPNNs can be trained to yield larger distances between the features of all non-equivalent nodes. Further details on this experiment appear in Appendix E.2.

## 8 Conclusion

We have shown that moments of neural networks with an analytic non-polynomial activation are injective on multisets and measures. We have also shown how this can be harnessed to construct universal approximators for multiset functions, as well as prove separation results for graph neural networks. A key advantage of our approach is that it enables constructing proofs using real models that are used in practice, rather than idealized versions of them as done in previous works.

It may seem tempting, due to our theoretical results, to conclude that analytic activations should perform on multisets better than piecewise-linear activations. We stress that we make no such claim. Indeed, while the separation results in Figure 1(a) corroborate our theory, PwL networks fail to separate only \(1\%\) of the graphs in our experiment. Furthermore, at high embedding dimensions, the empirical bi-Lipschitzness in Figure 2 does not seem to strongly depend on the analyticity of the activation function. Our claim is thus much more modest: we claim that multiset architectures with analytic activations are easier to _theoretically_ analyze, and we hope that pursuing this analysis shall lead to fruitful theoretical and practical insights, which may ultimately benefit multiset architectures with either type of activation.

Lastly, we note that the finite witness theorem, which is presented here as a tool for proving moment injectivity, may prove valuable as a general tool for reducing an infinite number of equality constraints to a finite number, and we believe it will find additional applications beyond the scope of this paper.

**Acknowledgements** N.D. is partially funded by a Horev Fellowship. T.A, R.R. and N.D. are partially funded by ISF grant 272/23.

## References

* [1] Anders Aamand et al. "Exponentially Improving the Complexity of Simulating the Weisfeiler-Lehman Test with Graph Neural Networks". In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 27333-27346.
* [2] Radu Balan, Pete Casazza, and Dan Edidin. "On signal reconstruction without phase". In: _Applied and Computational Harmonic Analysis_ 20.3 (2006), pp. 345-356.
* [3] Radu Balan, Naveed Haghani, and Maneesh Singh. "Permutation invariant representations with applications to graph deep learning". In: _arXiv preprint arXiv:2203.07546_ (2022).
* [4] Jameson Cahill, Joseph W Iverson, and Dustin G Mixon. "Bilipschitz group invariants". In: _arXiv preprint arXiv:2305.17241_ (2023).
* [5] Jameson Cahill et al. "Group-invariant max filtering". In: _arXiv preprint arXiv:2205.14039_ (2022).
* [6] Gabriele Corso et al. "Principal neighbourhood aggregation for graph nets". In: _Advances in Neural Information Processing Systems_ 33 (2020), pp. 13260-13271.
* [7] George Cybenko. "Approximation by superpositions of a sigmoidal function". In: _Mathematics of control, signals and systems_ 2.4 (1989), pp. 303-314.
* [8] Valentino Delle Rose et al. "Three iterations of \((d-1)\)-WL test distinguish non isometric clouds of \(d\)-dimensional points". In: _arXiv e-prints_ (2023), arXiv-2303.
* [9] Nadav Dym and Steven J Gortler. "Low Dimensional Invariant Embeddings for Universal Geometric Learning". In: _arXiv preprint arXiv:2205.02956_ (2022).
* [10] Matthias Fey and Jan E. Lenssen. "Fast Graph Representation Learning with PyTorch Geometric". In: _ICLR Workshop on Representation Learning on Graphs and Manifolds_. 2019.
* [11] Jean Feydy et al. "Interpolating between Optimal Transport and MMD using Sinkhorn Divergences". In: _The 22nd International Conference on Artificial Intelligence and Statistics_. 2019, pp. 2681-2690.
* [12] Hillel Furstenberg. "Ergodic fractal measures and dimension conservation". In: _Ergodic Theory and Dynamical Systems_ 28.2 (2008), pp. 405-422.
* [13] Allen Hatcher. _Algebraic Toplogy_. Cambridge University Press, 2002.
* [14] Michael Hochman. "Lectures on fractal geometry and dynamics". In: _Unpublished lecture notes, Available online at http://math. huji. ac. il/nhochman/courses/fractals-2012/courses-integ-26. pdf_ (2012).
* [15] Snir Hordan et al. "Complete Neural Networks for Euclidean Graphs". In: _arXiv preprint arXiv:2301.13821_ (2023).
* [16] Chaitanya K Joshi et al. "On the expressive power of geometric graph neural networks". In: _arXiv preprint arXiv:2301.09308_ (2023).
* [17] Sammy Khalife and Amitabh Basu. "On the power of graph neural networks and the role of the activation function". In: _arXiv preprint arXiv:2307.04661_ (2023).
* [18] Thomas N. Kipf and Max Welling. "Semi-Supervised Classification with Graph Convolutional Networks". In: _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017. url: https://openreview.net/forum?id=SJU4ayYg1.
* [19] Steven G Krantz and Harold R Parks. _A primer of real analytic functions_. Springer Science & Business Media, 2002.
* [20] M. G. Krein and A. A. Nudelman. _The Markov moment problem and extremal problems_. Translations of mathematical monographs. American Mathematical Society, 1977.
* [21] John M Lee. "Smooth manifolds, Introduction to smooth manifolds". In: _Graduate Texts in Mathematics_ 218 (2013).
* [22] John M Lee and John M Lee. _Introduction to Smooth manifolds_. Springer, 2003.
* [23] Juho Lee et al. "Set transformer: A framework for attention-based permutation-invariant neural networks". In: _International conference on machine learning_. PMLR. 2019, pp. 3744-3753.
* [24] Moshe Leshno et al. "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function". In: _Neural networks_ 6.6 (1993), pp. 861-867.
* [25] Zian Li et al. "Is Distance Matrix Enough for Geometric Deep Learning?" In: _arXiv preprint arXiv:2302.05743_ (2023).

* [26] Haggai Maron et al. "Provably powerful graph networks". In: _Advances in neural information processing systems_ 32 (2019).
* [27] Chris Miller. "Tameness in expansions of the real field". In: _Logic Colloquium_. Vol. 1. 2005, pp. 281-316.
* [28] Boris Samulovich Mityagin. "The zero set of a real analytic function". In: _Mathematical Notes_ 107.3-4 (2020), pp. 529-530.
* [29] Guido F Montufar et al. "On the number of linear regions of deep neural networks". In: _Advances in neural information processing systems_ 27 (2014).
* [30] Christopher Morris et al. "TUDataset: A collection of benchmark datasets for learning with graphs". In: _ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020)_. 2020. arXiv: 2007.08663. URL: www.graphlearning.io.
* [31] Christopher Morris et al. "Weisfeiler and leman go neural: Higher-order graph neural networks". In: _Proceedings of the AAAI conference on artificial intelligence_. Vol. 33. 01. 2019, pp. 4602-4609.
* [32] Allan Pinkus. "Approximation theory of the MLP model in neural networks". In: _Acta numerica_ 8 (1999), pp. 143-195.
* [33] Sergey N Pozdnyakov and Michele Ceriotti. "Incompleteness of graph neural networks for points clouds in three dimensions". In: _Machine Learning: Science and Technology_ 3.4 (2022), p. 045020.
* [34] Charles R Qi et al. "Pointnet: Deep learning on point sets for 3d classification and segmentation". In: _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2017, pp. 652-660.
* [35] Maithra Raghu et al. "On the Expressive Power of Deep Neural Networks". In: _arxiv preprint arXiv:1606.05336_ (2017).
* [36] Walter Rudin. _Real and Complex Analysis_. McGraw-Hill Science/Engineering/Math, 1986.
* [37] Nimrod Segol and Yaron Lipman. "On Universal Equivariant Set Networks". In: _International Conference on Learning Representations_. 2019.
* [38] Guillaume Valette. _On subanalytic geometry_. June 2023.
* [39] Lou Van den Dries and Chris Miller. "Geometric categories and o-minimal structures". In: (1996).
* [40] Cedric Villani et al. _Optimal transport: old and new_. Vol. 338. Springer, 2009.
* [41] Edward Wagstaff et al. "On the limitations of representing functions on sets". In: _International Conference on Machine Learning_. PMLR. 2019, pp. 6487-6494.
* [42] Edward Wagstaff et al. "Universal approximation of functions on sets". In: _Journal of Machine Learning Research_ 23.151 (2022), pp. 1-56.
* [43] Peihao Wang et al. "Polynomial Width is Sufficient for Set Representation with High-dimensional Features". In: _arXiv preprint arXiv:2307.04001_ (2023).
* [44] Keyulu Xu et al. "How Powerful are Graph Neural Networks?" In: _International Conference on Learning Representations_. 2018.
* [45] Manzil Zaheer et al. "Deep sets". In: _Advances in neural information processing systems_ 30 (2017).
* [46] Yan Zhang, Jonathon Hare, and Adam Prugel-Bennett. "FSPool: Learning Set Representations with Featurewise Sort Pooling". In: _International Conference on Learning Representations_.

Finite witness theorem

In this section, we state and prove the full version of the finite witness theorem (Theorem A.2), which is more general than Theorem 3.4 stated in the main text. Before laying out the formal definitions and proofs, we begin by describing the context of these results. While this section makes use of notions from algebraic geometry and real analytical functions, it is self-contained and most of it only requires knowledge of elementary calculus and some topology.

The finite witness theorem is essentially a tool for reducing an infinite, continuously parameterized family of constraints \(p(\bm{z};\bm{\theta})=0\;\forall\bm{\theta}\) to a finite subset of constraints \(p(\bm{z};\bm{\theta}_{i})=0\), \(i=1,\ldots,m\), defined by random parameters \(\bm{\theta}_{1},\ldots,\bm{\theta}_{m}\). This general approach seems to have originated from the famous proof of uniqueness for phase-retrieval measurements in [2]. In that work, functions \(p(\bm{z},\bm{\theta}):\mathbb{C}^{n}\times\mathbb{C}^{n}\to\mathbb{R}\) of the form \(p(\bm{z};\bm{\theta})=|\langle\bm{z},\bm{\theta}\rangle|\) were considered, and it was proved that a finite number of \(\sim 4n\) random measurements \(\bm{\theta}_{i}\) are sufficient to uniquely determine a signal, up to unavoidable global phase ambiguity. The proof in [2] achieves this result by showing that the values \(p(\bm{z};\bm{\theta})\) for _all_ possible \(\bm{\theta}\)s are sufficient to determine the signal uniquely, and then uses a real algebraic-geometric and dimension-counting argument to show that this continuous family of measurements can be replaced by a finite subset, defined by \(m\sim 4n\) random vectors (_witnesses_) \(\{\bm{\theta}_{i}\}_{i=1}^{m}\), without losing information.

In [9], the authors provide a generalization of the results in [2], by defining conditions under which a continuously parameterized family of functions \(p(\bm{z};\bm{\theta})\) that fully determines \(\bm{z}\) up to equivalence, can be replaced by a finite subset \(p(\bm{z};\bm{\theta}_{i})\), \(i=1,\ldots,m\) determined by random witness-vectors \(\bm{\theta}_{i}\). This theorem is based on similar arguments as those used in [2], and on similar assumptions required for machinery from real algebraic geometry. Specifically, sets in [9] are assumed to be _semialgebraic_, which means that they are finite unions of subsets of \(\mathbb{R}^{D}\) that can be defined by polynomial equalities and inequalities. This class of sets includes, for example, finite unions of spheres, balls, and convex polyhedra. The functions in the theorem are assumed to be semialgebraic as well, which means that their graphs are semialgebraic sets. This class of functions includes polynomials, as well as rational functions and piecewise-linear functions.

The main theorem in [9] can be essentially3 formulated as

Footnote 3: To obtain this theorem from the formulation in [9], use the change of variables \(\bm{z}=(\bm{x},\bm{y})\) and \(F(\bm{z};\bm{\theta})=p(\bm{x};\bm{\theta})-p(\bm{y};\bm{\theta})\). The formulation in [9] makes some additional requirements on \(F\) which are relevant to the specific applications considered there, but going through the details of the proof shows that it is sufficient for proving Theorem A.1 as stated here.

**Theorem A.1**.: _Let \(\mathbb{M}\) be a semialgebraic set of dimension \(D\), and let \(F:\mathbb{M}\times\mathbb{R}^{D\bm{\theta}}\to\mathbb{R}\) be a semialgebraic function. Define the set_

\[\mathcal{N}=\{\bm{z}\in\mathbb{M}|\,F(\bm{z};\bm{\theta})=0,\;\forall\bm{ \theta}\in\mathbb{R}^{D\bm{\theta}}\}\]

_and assume that for all \(\bm{z}\in\mathbb{M}\setminus\mathcal{N}\), we have that_

\[\dim\{\bm{\theta}\in\mathbb{R}^{D\bm{\theta}}|\,F(\bm{z};\bm{\theta})=0\}\leq D _{\bm{\theta}}-1,\] (12)

_then for Lebesgue almost every \(\bm{\theta}^{(1)},\ldots,\bm{\theta}^{(D+1)}\),_

\[\mathcal{N}=\{\bm{z}\in\mathbb{M}|\,F(\bm{z};\bm{\theta}^{(i)})=0,\;\forall i =1,\ldots D+1\}.\]

The notion of dimension used in condition (12) and throughout this section is the _Hausdorff dimension_, explained in Appendix A.4 below.

Theorem A.1 is similar to the simple version of the finite witness theorem (Theorem 3.4), with four notable differences: (1) The domain \(\mathbb{M}\) in Theorem 3.4 is a countable union of affine sets; this is not, in general, a semialgebraic set, and thus does not qualify for the conditions of Theorem A.1. (2) The function \(F\) in Theorem 3.4 is analytic. Analytic functions are not necessarily semialgebraic. (3) Theorem A.1 requires the extra condition (12), which does not appear in Theorem 3.4. This condition is not required in Theorem 3.4 because with analytic functions, it is always satisfied; however, it will be required in our full version of the theorem, since it admits a more general class of functions. (4) Theorem A.1 deals with sets of the form \(\{\bm{z}|\,F(\bm{z},\bm{\theta})=0,\forall\bm{\theta}\}\) while Theorem 3.4 deals with sets of the form \(\{(\bm{x},\bm{y})|\,F(\bm{x},\bm{\theta})=F(\bm{y},\bm{\theta}),\forall\bm{ \theta}\}\). This difference is not essential and can be handled by the change of variables \(\bm{z}=(\bm{x},\bm{y})\) and \(\tilde{F}(\bm{x},\bm{y},\bm{\theta})=F(\bm{x},\bm{\theta})-F(\bm{y},\bm{ \theta})\).

To address the first two differences, we shall generalize Theorem A.1 to support a large class of domains \(\mathbb{M}\) and functions \(F\). The admissible domains shall include a vast class of sets, among which are _countable_ unions of semialgebraic sets, as well as all open sets, and sets defined by analytic, rather than polynomial, equations. The admissible functions \(F\) shall include semialgebraic functions, analytic functions, and many other types of functions. To achieve this goal, we use results from the study of _o-minimal structures_ (see below), which aim at finding families of sets that have the same tame properties as semialgebraic sets.

A good starting point to achieve this generalization is to consider the family of _globally subanalytic sets_ (formally defined below). This family contains all semialgebraic sets and is an o-minimal system; consequently, it is possible to generalize Theorem A.1 so that the domain \(\mathbb{M}\) can include all globally subanalytic sets, and the function \(F\) could be any _globally subanalytic function_ (meaning that the graph of \(F\) is a globally subanalytic set). However, this still will not allow for a general analytic \(F\), nor for \(\mathbb{M}\) to include countable unions of affine sets.

To address this, we will show that an analogue to Theorem A.1 holds even when considering _countable unions_ of globally subanalytic sets. We call such sets _\(\sigma\)-subanalytic sets_. To the best of our knowledge, such sets have not been studied to date. The family of \(\sigma\)-subanalytic sets is not an o-minimal structure, since it not closed under taking complements. However, it _is_ closed under linear projections, countable unions, finite intersections and Cartesian products. Moreover, \(\sigma\)-subanalytic sets are countable unions of \(C^{\infty}\) manifolds. We find that these properties are sufficient to generalize Theorem A.1 and obtain a finite witness theorem for the \(\sigma\)-subanalytic category.

Clearly, the class of \(\sigma\)-subanalytic sets is larger than the class of globally subanalytic sets. In particular, it includes countable unions of semialgebraic sets. This enables the domain \(\mathbb{M}\) in the theorem statement to be a countable union of affine spaces, as in Theorem 3.4 from the main text. This also implies that any open set is \(\sigma\)-subanalytic, since it is a countable union of open balls -- which are semialgebraic sets.

As for the function \(F\), our theorem admits all \(\sigma\)-subanalytic functions: functions whose graph is \(\sigma\)-subanalytic. This class includes all analytic functions, as well as all semialgebraic functions. Moreover, we show below that it is closed under composition and other elementary operations.

We now state the full version of the finite witness theorem.

**Theorem A.2** (Finite Witness Theorem, full version).: _Let \(\mathbb{M}\subseteq\mathbb{R}^{p}\), \(\mathbb{W}\subseteq\mathbb{R}^{q}\) be \(\sigma\)-subanalytic sets of dimension \(D\) and \(D_{\boldsymbol{\theta}}\) respectively. Let \(F:\mathbb{M}\times\mathbb{W}\to\mathbb{R}\) be a \(\sigma\)-subanalytic function. Define the set_

\[\mathcal{N}=\{\boldsymbol{z}\in\mathbb{M}\,\mid\,F(\boldsymbol{z}; \boldsymbol{\theta})=0,\;\forall\boldsymbol{\theta}\in\mathbb{W}\}.\]

_Suppose that for all \(\boldsymbol{z}\in\mathbb{M}\setminus\mathcal{N}\)_

\[\dim\{\boldsymbol{\theta}\in\mathbb{W}\,\mid\,F(\boldsymbol{z}; \boldsymbol{\theta})=0\}\leq D_{\boldsymbol{\theta}}-1.\] (13)

_Then for generic \(\big{(}\boldsymbol{\theta}^{(1)},\ldots,\boldsymbol{\theta}^{(D+1)}\big{)} \in\mathbb{W}^{D+1}\),_

\[\mathcal{N}=\{\boldsymbol{z}\in\mathbb{M}\,\mid\,F(\boldsymbol{z}; \boldsymbol{\theta}^{(i)})=0,\;\forall i=1,\ldots D+1\}.\] (14)

_Moreover, if \(\mathbb{W}\) is an open and connected subset of \(\mathbb{R}^{q}\), and \(F(\boldsymbol{z};\boldsymbol{\theta})\) is analytic as a function of \(\boldsymbol{\theta}\) for all fixed \(\boldsymbol{z}\in\mathbb{M}\), then condition (13) is not required, as it is automatically satisfied._

The notion of dimension in (13) is the Hausdorff dimension (discussed below), and the term _generic_ means that the set of \(\big{(}\boldsymbol{\theta}^{(1)},\ldots,\boldsymbol{\theta}^{(D+1)}\big{)}\) for which (14) fails is a subset of \(\mathbb{W}^{D+1}\) whose Hausdorff dimension is strictly lower than \(\dim\big{(}\mathbb{W}^{D+1}\big{)}\). In particular, in the common case where \(\mathbb{W}=\mathbb{R}^{q}\), or is an open subset of \(\mathbb{R}^{q}\), we have that the theorem holds for almost any \(\big{(}\boldsymbol{\theta}^{(1)},\ldots,\boldsymbol{\theta}^{(D+1)}\big{)}\), with respect to the Lebesgue measure on \(\mathbb{R}^{q(D+1)}\).

We now begin to rigorously define the notions discussed in this section and then prove Theorem A.2. In Appendices A.1 to A.4, we construct the theoretical framework step by step. Then, in Appendix A.5 we prove the theorem, and in Appendix A.6 we show that the simple version of the theorem (Theorem 3.4) follows from the full version. Finally, we present in Appendix A.6 several corollaries to the theorem.

### Definitions and Background

#### a.1.1 Globally subanalytic sets: an o-minimal structure

We begin by defining analytic functions, subanalytic sets and functions, and various other related concepts that will be required for our discussion. These definitions are taken from [19] and [38].

**Definition A.3** (Analytic function [19]).: Let \(U\subseteq\mathbb{R}^{D}\) be an open set. We say that \(f:U\to\mathbb{R}\) is _analytic_ if for all \(\bm{z}\in U\), there exists an open ball \(V\subseteq U\) centered at \(\bm{z}\), and \((a_{\alpha})_{\alpha\in\mathbb{N}^{D}}\) such that

\[f(\bm{y})=\sum_{\alpha\in\mathbb{N}^{D}}a_{\alpha}(\bm{y}-\bm{z})^{\alpha} \quad\forall\bm{y}\in V,\] (15)

and the power series in (15) converges absolutely.

Our next goal is to define the family of globally subanalytic sets, which contains all semialgebraic sets; moreover, it is an _o-minimal structure_, meaning that it shares the essential tame properties of semialgebraic sets. This requires some preliminary definitions:

**Definition A.4** (Semianalytic sets [38]).: A subset \(E\subseteq\mathbb{R}^{n}\) is called _semianalytic_ if it is locally defined by finitely many real analytic equalities and inequalities. Namely, for each \(a\in\mathbb{R}^{n}\), there is a neighborhood \(U\) of \(a\), and real analytic functions \(f_{ij},g_{ij}\) on \(U\), where \(i=1,\ldots,r\) and \(j=1\ldots s_{i}\), such that

\[E\cap U=\bigcup_{i=1}^{r}\bigcap_{j=1}^{s_{i}}\{\bm{z}\in U|\,g_{ij}(\bm{z})>0 \text{ and }f_{ij}(\bm{z})=0\}.\] (16)

**Example A.5**.: As shown in Example 1.1.2 in [38], the graph of the analytic function \(f:(0,1)\to\mathbb{R}\) defined by \(f(x)=\sin(1/x)\) is _not_ a semianalytic set. This is because there is no neighborhood of \(a=(0,0)\) for which (16) holds. On the other hand, if \(B\subseteq\mathbb{R}^{n}\) is a closed ball and \(f:B\to\mathbb{R}\) can be extended to an analytic function in an open set containing \(B\), then the graph of \(f\) (as a function defined on \(B\)) will be semianalytic.

The example above suggests that the class of semianalytic sets may be too restrictive for our purposes. More importantly, linear projections of semianalytic sets may not be semianalytic [38], so the family of semianalytic sets does not form an o-minimal system. This can be remedied by defining globally semianalytic sets (which don't form an o-minimal system) and then using these sets to define globally subanalytic sets (which _do_ form an o-minimal system):

**Definition A.6** (Globally semianalytic sets [38]).: A subset \(Z\subseteq\mathbb{R}^{n}\) is _globally semianalytic_ if \(V_{n}(Z)\) is a semianalytic subset of \(\mathbb{R}^{n}\), where \(V_{n}:\mathbb{R}^{n}\to(-1,1)^{n}\) is the homeomorphism defined by

\[V_{n}\left(\bm{z}=(z_{1},\ldots,z_{n})\right)=\left(\frac{z_{1}}{\sqrt{1+|\bm{ z}|^{2}}},\ldots,\frac{z_{n}}{\sqrt{1+|\bm{z}|^{2}}}\right).\]

Globally semianalytic sets can be thought of as semianalytic sets that remain semianalytic when 'compactified' by \(V_{n}\). This is useful to rule out bad behaviour as \(\bm{z}\) goes out to infinity. Important examples of globally semianalytic sets include all bounded semianalytic sets and all semialgebraic sets [38].

Globally semianalytic sets are still not closed under linear projection. Their projections are called globally _subanalytic_ sets:

**Definition A.7** (Globally subanalytic sets [38]).: A subset \(E\subseteq\mathbb{R}^{n}\) is _globally subanalytic_ if it can be presented as a linear projection of a globally semianalytic set; more precisely, if there exists a globally semianalytic set \(Z\subseteq\mathbb{R}^{n+p}\) such that \(E=\pi(Z)\), with \(\pi:\mathbb{R}^{n+p}\to\mathbb{R}^{n}\) being the projection operator that omits the last \(p\) coordinates while leaving the remaining coordinates unchanged.

Globally semianalytic sets are by definition globally subanalytic. Globally subanalytic sets do form an _o-minimal structure_. In particular, they have the following properties:

**Proposition A.8** (Properties of globally subanalytic sets [38]).: _Let \(A,B\subseteq\mathbb{R}^{D}\) and \(C\subseteq\mathbb{R}^{M}\) be globally subanalytic sets. Then:_

1. \(\mathbb{R}^{D}\setminus A\) _is globally subanalytic._2. \(A\cup B\) _is globally subanalytic._
3. \(A\cap B\) _is globally subanalytic._
4. \(A\times C\) _is globally subanalytic._
5. _If_ \(\pi:\mathbb{R}^{D}\to\mathbb{R}^{L}\) _is a linear projection, then_ \(\pi(A)\) _is globally subanalytic._
6. \(A\) _is a finite union of_ \(C^{\infty}\) _manifolds._

The first property in Proposition A.8 is proven in Gabrielov's Complement Theorem; see Theorem 1.8.8 in [38]. The last property is a weaker version of Theorem 1.2.3 in [38]. The remaining properties are proved4 in [38, Basic properties 1.1.8].

Footnote 4: Regarding the fifth property in Proposition A.8, note that it does not follow directly from property 1 in [38] which only discusses a special class of projections, but rather from property 4 in [38] which states that the image of a globally subanalytic set under a globally subanalytic mapping is globally subanalytic. Any linear projection is a semialgebraic mapping, and hence a globally subanalytic mapping.

### \(\sigma\)-subanalytic sets

As mentioned above, the fact that globally subanalytic sets form an o-minimal structure is already sufficient to prove a finite witness theorem for this class. However, this class is still too restrictive for our purposes, as it does not allow us to support arbitrary analytic functions \(F\), as the following example shows:

**Example A.9**.: Analogously to semialgebraic functions, we say that a function is _globally subanalytic_ if its graph is globally subanalytic. Consider the function \(\sin(x)\) defined on the real line. Then by Example 1.1.7 in [38], this function is not globally subanalytic.

Fortunately, we find that a finite witness theorem can be proven for a larger class of sets: countable unions of globally subanalytic sets. We name such sets \(\sigma\)_-subanalytic_ sets. To the best of our knowledge, this class of sets has not been studied to date5. The class of \(\sigma\)-subanalytic sets is rather large: we will show below that it contains all open sets, while the classes discussed previously do not (see Example A.12 below). This is illustrated in the Venn diagram on the left-hand side of Figure 3. Correspondingly, the class of \(\sigma\)-subanalytic functions is larger than the classes of functions considered previously, and in particular it contains _any_ analytic function \(F\) defined on _any_ open set. This is illustrated in the Venn diagram on the right-hand side of Figure 3, and will be discussed rigorously in Appendix A.3 below. Thus, moving to \(\sigma\)_-subanalytic_ sets and functions is the crucial step that enables us to achieve our goal of proving a finite witness theorem for analytic functions (though the theorem covers a much larger class).

Footnote 5: Model theorists and analytic geometers _have_ researched other structures that are larger than o-minimal structures and enjoy some of their tame properties; see, e.g., [39, 27].

We now define \(\sigma\)-subanalytic sets and study their properties.

**Definition A.10** (\(\sigma\)-subanalytic sets).: We say that a subset \(A\subseteq\mathbb{R}^{D}\) is \(\sigma\)_-subanalytic_ if it is a countable union of globally subanalytic subsets of \(\mathbb{R}^{D}\).

The class of \(\sigma\)-subanalytic sets is rather large. For example, any open set in \(\mathbb{R}^{D}\) can be written as a countable union of open balls. Since open balls are semialgebraic sets, it follows that all open sets are \(\sigma\)-subanalytic. Also, any semianalytic set is \(\sigma\)-subanalytic, since semianalytic sets are a countable union of bounded semianalytic sets, and these are globally subanalytic.

#### Properties of \(\sigma\)-subanalytic sets

As the following proposition shows, \(\sigma\)-subanalytic sets inherit most of the properties enjoyed by globally subanalytic sets, described in Proposition A.8.

**Proposition A.11** (Properties of \(\sigma\)-subanalytic sets).: _Assume that \(A,B\subseteq\mathbb{R}^{D}\) and \(C\subseteq\mathbb{R}^{M}\) are \(\sigma\)-subanalytic sets, then_

1. \(A\cup B\) _is_ \(\sigma\)_-subanalytic. More generally, any countable union of_ \(\sigma\)_-subanalytic sets is_ \(\sigma\)_-subanalytic._2. \(A\cap B\) _is_ \(\sigma\)_-subanalytic._
3. \(A\times C\) _is_ \(\sigma\)_-subanalytic._
4. _If_ \(\pi:\mathbb{R}^{D}\to\mathbb{R}^{L}\) _is a linear projection, then_ \(\pi(A)\) _is a_ \(\sigma\)_-subanalytic set._
5. \(A\) _is a_ countable _union of_ \(C^{\infty}\) _manifolds._

Proof.: Let \(A,B\subseteq\mathbb{R}^{D}\) be \(\sigma\)-subanalytic sets. Then \(A=\cup_{n\in\mathbb{N}}A_{n}\) and \(B=\cup_{m\in\mathbb{N}}B_{m}\), where each \(A_{n}\) and \(B_{m}\) is globally subanalytic. We then have

1. A countable union of \(\sigma\)-analytic sets, each of which being itself a countable union of globally subanalytic set, is clearly a countable union of globally subanalytic sets, and therefore is \(\sigma\)-subanalytic by definition.
2. We have that \[A\cap B=(\cup_{n\in\mathbb{N}}A_{n})\cap(\cup_{m\in\mathbb{N}}B_{m})=\cup_{(n,m)\in\mathbb{N}^{2}}(A_{n}\cap B_{m})\] and \(A_{n}\cap B_{m}\) is globally subanalytic as the intersection of globally subanalytic sets.
3. The set \(A\times\mathbb{R}^{M}\) can be presented as \[A\times\mathbb{R}^{M}=(\cup_{n\in\mathbb{N}}A_{n})\times\mathbb{R}^{M}=\cup_{n \in\mathbb{N}}\left(A_{n}\times\mathbb{R}^{M}\right),\] with \(A_{m}\) being globally subanalytic. Since globally subanalytic sets are closed to Cartesian products, each \(A_{n}\times\mathbb{R}^{M}\) is globally subanalytic, and their countable union is \(\sigma\)-subanalytic. Finally, \[A\times C=\left(A\times\mathbb{R}^{M}\right)\cap\left(\mathbb{R}^{D}\times C \right),\] which is \(\sigma\)-subanalytic by part 2.
4. Follows from globally subanalytic sets being closed to projections, since \[\pi\left(\cup_{n\in\mathbb{N}}A_{n}\right)=\cup_{n\in\mathbb{N}}\left(\pi(A_ {n})\right).\]
5. A \(\sigma\)-subanalytic set is a countable union of globally subanalytic sets, each of which is a finite union of \(C^{\infty}\) manifolds. Therefore, it is a countable union of \(C^{\infty}\) manifolds.

Figure 3: Left: The class of semialgebraic sets is contained in the class of globally subanalytic sets, which in turn is contained in the class of \(\sigma\)-subanalytic sets — on which we focus in this paper. One of the advantages of this larger class is that it contains all open sets, whereas the two smaller classes do not. Right: The classes of semialgebraic, globally subanalytic, and \(\sigma\)-subanalytic functions are related in the same way. The class of \(\sigma\)-subanalytic functions is the only one of the three that contains all analytic functions.

In comparison to Proposition A.8, we see that we have lost two properties by moving from globally subanalytic sets to countable unions thereof: Firstly, a \(\sigma\)-subanalytic set is a countable union of manifolds rather than a finite one. Secondly, the complement of a \(\sigma\)-subanalytic set may not be \(\sigma\)-subanalytic, as our next example shows. As it turns out, the properties noted in Proposition A.11 are sufficient for our purposes.

**Example A.12**.: The Cantor set is not \(\sigma\)-subanalytic, since it has fractal Hausdorff dimension and thus is not a countable union of manifolds. However, the complement of the Cantor set, which we denote by \(U\), is an open set and hence is \(\sigma\)-subanalytic. This shows that the class of \(\sigma\)-subanalytic sets is not closed to taking complements, as well as countable intersections: this is since the Cantor set is a countable intersection of finite unions of intervals, which are \(\sigma\)-subanalytic. The set \(U\) is also an example of an open set that is not semialgebraic or globally subanalytic, since these classes of sets _are_ closed to complements, and do not contain the Cantor set.

### \(\sigma\)-subanalytic functions

We now define the class of functions our theorem can admit as the function \(F\): \(\sigma\)-subanalytic functions.

**Definition A.13** (\(\sigma\)-subanalytic function).: Let \(\mathbb{M}\subseteq\mathbb{R}^{D}\) be a \(\sigma\)-subanalytic set. We say that \(f:\mathbb{M}\to\mathbb{R}^{L}\) is a \(\sigma\)-subanalytic function if its graph is a \(\sigma\)-subanalytic subset of \(\mathbb{R}^{D+L}\).

If \(\mathbb{M}\subseteq\mathbb{R}^{D}\) is a semialgebraic set and \(f:\mathbb{M}\to\mathbb{R}^{L}\) is a semialgebraic function, then the graph of \(f\) is semialgebraic, and thus is \(\sigma\)-subanalytic. Therefore, a semialgebraic function \(f\) defined on a semialgebraic domain \(\mathbb{M}\) is a \(\sigma\)-subanalytic function. A similar result holds also for analytic functions defined on open sets, as shown in the following lemma:

**Lemma A.14**.: _If \(U\subseteq\mathbb{R}^{D}\) is open and \(f:U\to\mathbb{R}^{L}\) is analytic, then \(f\) is \(\sigma\)-subanalytic._

Proof.: First recall that since \(U\) is open, it is a \(\sigma\)-subanalytic set. Next, we can write \(U\) as a countable union of closed balls \(B_{i}\), and then the graph of \(f\) can be written as a countable union of sets of the form

\[\{(\bm{x},\bm{y})\in B_{i}\times\mathbb{R}^{L}|\,\bm{y}=f(\bm{x})\}.\]

These subsets of \(U\times\mathbb{R}^{L}\) are compact and semianalytic, and hence they are globally subanalytic. 

#### Properties of \(\sigma\)-subanalytic functions

The following proposition shows that the class of \(\sigma\)-subanalytic functions is closed under composition and elementary arithmetic operations. In particular, this allows for functions that combine analytic and semialgebraic functions via compositions and elementary arithmetic operations.

**Proposition A.15**.: _Let \(A\subseteq\mathbb{R}^{a}\) and \(B\subseteq\mathbb{R}^{b}\) be \(\sigma\)-subanalytic sets._

1. (Composition) _If_ \(f:A\to B\) _and_ \(g:B\to\mathbb{R}^{c}\) _are_ \(\sigma\)_-subanalytic functions, then_ \(g\circ f\) _is a_ \(\sigma\)_-subanalytic function._
2. (Concatenation) _If_ \(f:A\to\mathbb{R}^{m}\) _and_ \(g:A\to\mathbb{R}^{n}\) _are_ \(\sigma\)_-subanalytic functions, then_ \(h:A\to\mathbb{R}^{m+n}\) _given by_ \(h\left(x\right)=\left(f\left(x\right),g\left(x\right)\right)\) _is a_ \(\sigma\)_-subanalytic function._
3. (Addition and multiplication) _If_ \(f,g:A\to\mathbb{R}\) _are_ \(\sigma\)_-subanalytic functions, then_ \(f+g\) _and_ \(f\cdot g\) _are_ \(\sigma\)_-subanalytic functions._

Proof.:
1. To prove that \(g\circ f\) is \(\sigma\)-subanalytic, we need to prove that its graph is a \(\sigma\)-subanalytic set. Indeed, this graph is of the form \[\{(\bm{x},\bm{z})\in A\times\mathbb{R}^{c}\ |\ \ g(f(\bm{x}))=\bm{z}\},\] which is the projection of the intersection \[\{(\bm{x},\bm{y},\bm{z})\in A\times B\times\mathbb{R}^{c}\ |\ \ f(\bm{x})=\bm{y}\} \cap\{(\bm{x},\bm{y},\bm{z})\in A\times B\times\mathbb{R}^{c}\ |\ \ g(\bm{y})=\bm{z}\}\] onto the \((\bm{x},\bm{z})\) coordinates. The two sets in the intersection above are Cartesian products of the graph of \(f\) (respectively \(g\)) with a \(\sigma\)-subanalytic set, and hence are \(\sigma\)-subanalytic.

2. The graph of \(h\) is the intersection of the sets \(\{(\bm{x},\bm{y},\bm{z})|\,f(\bm{x})=\bm{y}\}\) and \(\{(\bm{x},\bm{y},\bm{z})|\,g(\bm{x})=\bm{z}\}\). Each of these two sets is a Cartesian product of a Euclidean space with the graph of a \(\sigma\)-subanalytic function.
3. The functions \(f+g\) and \(f\cdot g\) can be presented as the composition of the function \(\mathbb{R}^{a}\ni\bm{x}\mapsto(f(\bm{x}),g(\bm{x}))\), which is \(\sigma\)-subanalytic, with the addition or multiplications functions. Thus, the claim follows.

### Dimension

The proof of Theorem A.2 is based on a dimension-counting argument. Accordingly, we will need an appropriate definition of dimension for \(\sigma\)-subanalytic sets. It is convenient to work with the Hausdorff dimension, which is defined for _every_ subset of a Euclidean space \(\mathbb{R}^{D}\), and coincides with the standard notions of dimension for vector spaces and manifolds. The definition of Hausdorff dimension can be found, e.g., in [14]. For our purposes, we will only need to use some of its properties, stated below.

**Proposition A.16** (Taken from [14]).: _The Hausdorff dimension has the following properties:_

1. _The Hausdorff dimension of a_ \(k\)_-dimensional_ \(C^{1}\) _submanifold of_ \(\mathbb{R}^{D}\) _is_ \(k\)_._
2. _If_ \(B_{1},B_{2},\ldots\) _are subsets of_ \(\mathbb{R}^{D}\) _then_ \[\dim\left(\cup_{n\in\mathbb{N}}B_{n}\right)=\max_{n\in\mathbb{N}}\dim(B_{n}).\]
3. _For any_ \(B\subseteq\mathbb{R}^{D}\) _and Lipschitz function_ \(f:B\to\mathbb{R}^{C}\)_,_ \(\dim\left(f\left(B\right)\right)\leq\dim\left(B\right)\)_._

Recall that \(\sigma\)-subanalytic sets are countable unions \(A=\cup_{n\in\mathbb{N}}A_{n}\) of \(C^{\infty}\) manifolds. As we saw, their Hausdorff dimension is just the maximal dimension of all manifolds \(A_{n}\). We shall now see that for such sets, the Hausdorff dimension has two nice properties that do not hold for general sets. These properties will be used in the proof of the finite witness theorem.

The first property is the dimension of Cartesian products. For two general sets \(A,B\), it is not always true that \(\dim(A\times B)=\dim(A)+\dim(B)\) (see [14]). However, this does hold if \(A\) and \(B\) are countable unions of manifolds.

**Lemma A.17**.: _If \(A,B\) are countable unions of \(C^{1}\) sub-manifolds of \(\mathbb{R}^{a}\) and \(\mathbb{R}^{b}\), then_

\[\dim(A\times B)=\dim(A)+\dim(B).\]

Proof.: By assumption \(A=\cup_{n\in\mathbb{N}}A_{n}\) and \(B=\cup_{m\in\mathbb{N}}B_{m}\), where \(A_{n},B_{m}\) are \(C^{1}\) manifolds. We have that

\[A\times B=\cup_{(n,m)\in\mathbb{N}^{2}}(A_{n}\times B_{m})\]

is again a countable union of the product manifolds \(A_{n}\times B_{m}\), which are known to be of dimension \(\dim(A_{n})+\dim(B_{m})\) (see e.g. [22]). It follows that the dimension of \(A\times B\) is equal to

\[\max_{(n,m)\in\mathbb{N}^{2}}(\dim(A_{n})+\dim(B_{m}))=\max_{n\in\mathbb{N}} \dim(A_{n})+\max_{m\in\mathbb{N}}\dim(B_{m})=\dim(A)+\dim(B).\]

The second nice property of the Hausdorff dimension for countable unions of manifolds, which does not always hold for general sets, is _dimension conservation_, in the following sense.

**Lemma A.18** (Dimension Conservation).: _Let \(S\subseteq\mathbb{R}^{D_{1}}\) be a countable union of \(C^{\infty}\) manifolds and \(f:\mathbb{R}^{D_{1}}\to\mathbb{R}^{D_{2}}\) a \(C^{\infty}\) function. Then_

\[\dim(S)\leq\dim(f(S))+\max_{t\in f(S)}\dim\left(f^{-1}(t)\cap S\right).\] (17)

For failure of dimension conservation for general sets see, e.g., the discussion in [12].

A good intuition for the concept of dimension conservation comes from the linear case, in which \(f:\mathbb{R}^{D_{1}}\to\mathbb{R}^{D_{2}}\) is a linear transformation and \(S\subseteq\mathbb{R}^{D_{1}}\) is a linear subspace. In this case, the _Rank-Nullity Theorem_ from elementary linear algebra states that

\[\dim(S) =\dim(f(S))+\dim(\mathrm{Kernel}(f)\cap S)\] (18) \[=\dim(f(S))+\dim(f^{-1}(0)\cap S).\]

The same statement also holds if we replace \(f^{-1}(0)\) by \(f^{-1}(t)\), with \(t\) being any point in the image of \(f\). Thus, for linear transformations, any loss of dimension when moving from a set \(S\) to its image, is accounted for by the dimension of the fibers above points in the image. The image conservation in Lemma A.18 is a weaker since it is only an inequality, but it applies to the more general class of \(\sigma\)-semianalytic functions.

Proof of Lemma a.18.: We know that \(S\) is a countable union of manifolds, and at least one of these, which we denote by \(S_{1}\), has the maximal dimension \(\dim(S_{1})=\dim(S)\). This equality shall enable us to argue about \(\dim(S)\) by arguing only about \(\dim(S_{1})\).

Let \(r\) be the maximal rank of the differential of \(f_{|S_{1}}\). Fix some \(s_{1}\in S_{1}\) so that the differential of \(f_{|S_{1}}\) at \(s_{1}\) has rank \(r\). The set of \(s\in S_{1}\) whose differential has rank \(r\) is open, and so there is a neighborhood \(V\) of \(s_{1}\), such that \(V\cap S_{1}\) is a manifold of the same dimension as \(S_{1}\), and the restriction of \(f\) to \(V\cap S_{1}\) has constant rank \(r\). Consider the restriction of \(f\) to \(V\cap S_{1}\), denoted by \(f_{|V\cap S_{1}}\). By the constant rank theorem [21], \(f_{|V\cap S_{1}}\) is a projection, up to a diffeomorphic change of coordinates. Since diffeomorphisms of manifolds preserve dimensions, and projections (like all linear transformations) satisfy dimension conservation as in (18), this implies that locally we have dimension conservation: for all \(t\in f(V\cap S_{1})\),

\[\dim(V\cap S_{1})=\dim\left(f(V\cap S_{1})\right)+\dim\left(f^{-1}(t)\cap V \cap S_{1}\right).\]

Therefore:

\[\dim(S)= \dim(S_{1})=\dim(V\cap S_{1})\] \[= \dim\left(f(V\cap S_{1})\right)+\dim\left(f^{-1}(t)\cap V\cap S_ {1}\right),\quad\forall t\in f(V\cap S_{1})\] \[\leq \dim\left(f(S)\right)+\max_{t\in f(V\cap S_{1})}\dim\left(f^{-1} (t)\cap S\right)\] \[\leq \dim(f(S))+\max_{t\in f(S)}\dim\left(f^{-1}(t)\cap S\right).\]

We conclude our discussion of the Hausdorff dimension with a natural corollary which will be useful later on for our proof of Proposition 3.6.

**Corollary A.19**.: _Let \(\mathbb{M}\subseteq\mathbb{R}^{D}\) be a \(\sigma\)-subanalytic set, and let \(F:\mathbb{M}\to\mathbb{R}^{L}\) a \(\sigma\)-subanalytic function. Then the graph and image of \(F\) are \(\sigma\)-subanalytic sets, and_

\[\dim\left(\mathrm{graph}(F)\right)=\dim(\mathbb{M}),\qquad\dim(F(\mathbb{M}) )\leq\dim(\mathbb{M}).\]

Proof.: Note that by the definition of \(\sigma\)-subanalytic functions, the graph of \(f\) is a \(\sigma\)-subanalytic set. We begin by showing that it has the same dimension as \(\mathbb{M}\). Denote by \(\pi\) the projection from

\[\mathrm{graph}(F)=\{(\boldsymbol{x},F(\boldsymbol{x}))\mid\boldsymbol{x}\in \mathbb{M}\}\]

onto the first coordinate. Firstly, since \(\pi\) is a Lipschitz function, it cannot increase the Hausdorff dimension, so

\[\dim(\mathrm{graph}(F))\geq\dim(\pi(\mathrm{graph}(F)))=\dim(\mathbb{M}).\]

In the other direction, note that for every \((\boldsymbol{x},F(\boldsymbol{x}))\in\mathrm{graph}(F)\) we have that

\[\pi^{-1}\left(\pi\left(\boldsymbol{x},F\left(\boldsymbol{x}\right)\right) \right)=\pi^{-1}\left(\boldsymbol{x}\right)=\{(\boldsymbol{x},F\left( \boldsymbol{x}))\}\]

is a set containing a single point, and thus has dimension zero. Therefore, using Lemma A.18 we have that

\[\dim(\mathrm{graph}(F))\leq\dim(\pi(\mathrm{graph}(F)))+0=\dim(\mathbb{M}),\]

and so we have shown that \(\mathbb{M}\) and the graph of \(F\) have the same dimension. Finally, \(F(\mathbb{M})\) is the projection of the graph of \(F\) onto the second coordinate and so it is a \(\sigma\)-subanalytic set. Moreover, as projections cannot increase the Hausdorff dimension, we obtain that

\[\dim(F(\mathbb{M}))\leq\dim(\mathrm{graph}(F))=\dim(\mathbb{M}),\]

which concludes the proof of the corollary.

### Proof of the finite witness theorem

We are finally ready to prove the full version of the finite witness theorem.

Proof of Theorem a.2.: Let \(m=D+1\). Let \(F_{m}:\mathbb{M}\times\mathbb{W}^{m+1}\to\mathbb{R}^{m+1}\) be given by

\[F_{m}\left(\bm{z},\bm{\theta}_{0},\bm{\theta}_{1},\ldots,\bm{\theta}_{m}\right) =\left(F\left(\bm{z};\bm{\theta}_{0}\right),\ldots,F\left(\bm{z};\bm{\theta}_{ m}\right)\right).\]

Since \(F_{m}\) is a concatenation of \(\sigma\)-subanalytic functions (each such function is the composition of \(F\) with a different linear projection), by Proposition A.15 it is also \(\sigma\)-subanalytic. Therefore, the graph of \(F_{m}\), given by \(\mathcal{A}_{m}\) below, is \(\sigma\)-subanalytic:

\[\mathcal{A}_{m}=\{(\bm{z},\bm{\theta}_{0},\bm{\theta}_{1},\ldots,\bm{\theta}_ {m},s_{0},s_{1},\ldots,s_{m})\in\mathbb{M}\times\mathbb{W}^{m+1}\times \mathbb{R}^{m+1}\,\mid\,F(\bm{z};\bm{\theta}_{i})=s_{i},\;\forall i=0,\ldots,m\}.\]

Next, we can intersect \(\mathcal{A}_{m}\) with the semialgebraic set defined by the equations \(s_{0}\neq 0,s_{1}=s_{2}=\ldots=s_{m}=0\) to obtain the \(\sigma\)-subanalytic set

\[\tilde{\mathcal{A}}_{m}=\{(\bm{z},\bm{\theta}_{0},\bm{\theta}_{1 },\ldots,\bm{\theta}_{m},F(\bm{z};\bm{\theta}_{0}),0,\ldots,0) \in\mathbb{M}\times\mathbb{W}^{m+1}\times\mathbb{R}^{m+1}\mid\] \[F(\bm{z};\bm{\theta}_{0})\neq 0\text{ and }F(\bm{z};\bm{\theta}_{i})=0,\;\forall i=1,\ldots,m\}.\]

We can then remove by projection the last \(m+1\) coordinates and the \(\bm{\theta}_{0}\) coordinate, to obtain the set

\[\mathcal{B}_{m}=\{(\bm{z},\bm{\theta}_{1},\ldots,\bm{\theta}_{m})\in\mathbb{M }\times\mathbb{W}^{m}\mid\,\bm{z}\in\mathbb{M}\setminus\mathcal{N}\text{ and }F(\bm{z};\bm{\theta}_{i})=0,\;\forall i=1,\ldots,m\},\]

which is \(\sigma\)-subanalytic as well. Let \(\pi\) and \(\pi_{\bm{\theta}}\) denote the projections

\[\pi(\bm{z},\bm{\theta}_{1},\ldots,\bm{\theta}_{m})=\bm{z},\qquad\pi_{\bm{ \theta}}(\bm{z},\bm{\theta}_{1},\ldots,\bm{\theta}_{m})=(\bm{\theta}_{1}, \ldots,\bm{\theta}_{m}).\]

The set \(\pi_{\bm{\theta}}(\mathcal{B}_{m})\subseteq\mathbb{W}^{m}\) is exactly the set of \(m\)-tuples \((\bm{\theta}_{1},\ldots,\bm{\theta}_{m})\) that are not sufficient to determine \(\mathcal{N}\). To prove the theorem, we thus need to show that the dimension of \(\pi_{\bm{\theta}}(\mathcal{B}_{m})\) is lower than \(\dim\left(\mathbb{W}^{m}\right)=mD_{\bm{\theta}}\). We do so by bounding the dimension of \(\mathcal{B}_{m}\).

Let \(\bm{z}_{0}\in\pi\left(\mathcal{B}_{m}\right)\). The set \(\pi^{-1}(\bm{z}_{0})\cap\mathcal{B}_{m}\) can be presented as the Cartesian product

\[\pi^{-1}(\bm{z}_{0})\cap\mathcal{B}_{m}=\{\bm{z}_{0}\}\times\underbrace{ \Theta_{\bm{z}_{0}}\times\Theta_{\bm{z}_{0}}\times\ldots\times\Theta_{\bm{z}_{0 }}}_{m\text{ times}},\] (19)

where

\[\Theta_{\bm{z}_{0}}=\{\bm{\theta}\in\mathbb{W}\mid\,F(\bm{z}_{0};\bm{\theta}) =0\}.\]

The set \(\Theta_{\bm{z}_{0}}\) is also \(\sigma\)-subanalytic: the graph of \(F\)

\[\{(\bm{z},\bm{\theta},s)\in\mathbb{M}\times\mathbb{W}\times\mathbb{R}\mid s=F \left(\bm{z};\theta\right)\}\]

is \(\sigma\)-subanalytic by assumption; intersecting it with the space \(\bm{z}=\bm{z}_{0},s=0\), and projecting to the \(\bm{\theta}\) coordinate, yields the set \(\Theta_{\bm{z}_{0}}\). Since \(\Theta_{\bm{z}_{0}}\) is \(\sigma\)-subanalytic, it is a countable union of manifolds. By the theorem assumption (13), the Hausdorff dimension of \(\Theta_{\bm{z}_{0}}\) satisfies

\[\dim(\Theta_{\bm{z}_{0}})\leq D_{\bm{\theta}}-1.\]

Therefore by Lemma A.17 and Equation (19),

\[\dim\left(\pi^{-1}(\bm{z}_{0})\cap\mathcal{B}_{m}\right)=m\dim(\Theta_{\bm{ z}_{0}})\leq mD_{\bm{\theta}}-m.\] (20)

So far, \(\bm{z}_{0}\) is only assumed to be an arbitrary point in \(\pi(\mathcal{B}_{m})\). Now, fix \(\bm{z}_{0}\in\pi(\mathcal{B}_{m})\) to be a maximizer of the left-hand side of Equation (20). By the dimension conservation Lemma A.18,

\[\begin{split}\dim(\mathcal{B}_{m})&\leq\dim(\pi( \mathcal{B}_{m}))+\max_{\bm{z}\in\pi(\mathcal{B}_{m})}\dim\left(\pi^{-1}(\bm{z} )\cap\mathcal{B}_{m}\right)\\ &\stackrel{{(a)}}{{=}}\dim(\pi(\mathcal{B}_{m}))+ \dim\left(\pi^{-1}(\bm{z}_{0})\cap\mathcal{B}_{m}\right)\\ &\stackrel{{(b)}}{{\leq}}\dim(\pi(\mathcal{B}_{m}))+mD_{ \bm{\theta}}-m\\ &\stackrel{{(c)}}{{\leq}}D+mD_{\bm{\theta}}-m=mD_{ \bm{\theta}}-1,\end{split}\] (21)where (a) holds by the choice of \(\bm{z}_{0}\), (b) is by Equation (20), and (c) holds since \(\pi(\mathcal{B}_{m})\subseteq\mathbb{M}\) and \(\dim(\mathbb{M})=D\). Since \(\pi_{\bm{\theta}}\) is Lipschitz, Equation (21) implies that

\[\dim(\pi_{\bm{\theta}}(\mathcal{B}_{m}))\leq\dim(\mathcal{B}_{m})\leq mD_{\bm{ \theta}}-1\]

as required.

Now, suppose that \(\mathbb{W}\) is an open and connected subset of \(\mathbb{R}^{q}\). We need to show that (13) holds for all \(\bm{z}\in\mathbb{M}\setminus\mathcal{N}\). Fix \(\bm{z}\in\mathbb{M}\setminus\mathcal{N}\) and let \(f:\mathbb{W}\to\mathbb{R}\) be given by \(f(\bm{\theta})=F(\bm{z};\bm{\theta})\). Since \(\bm{z}\not\in\mathcal{N}\), there exists some \(\bm{\theta}\) for which \(F(\bm{z};\bm{\theta})\neq 0\). Hence, \(f\) is an analytic function defined on an open connected domain, and is not identically zero. Therefore, its zero-set must be dimension-deficient (see [28, Proposition 3]) and so we obtain (13).

This concludes the proof of the theorem. 

### Corollaries

In this subsection, we present several corollaries to the finite witness theorem, and show that the simple version Theorem 3.4 follows from it as a special case.

A useful application of the finite witness theorem, which often arises in invariant learning, is when one wishes to assert that two points \(\bm{x},\bm{y}\in\mathbb{M}\) are indistinguishable by a parametric family of functions \(F(\,\cdot\,;\bm{\theta})\); namely, that \(F(\bm{x};\bm{\theta})=F(\bm{y};\bm{\theta})\) for all \(\bm{\theta}\in\mathbb{W}\). As the following corollary shows, this can be asserted almost surely by testing whether \(F(\bm{x};\bm{\theta})=F(\bm{y};\bm{\theta})\) on \(2\dim\left(\mathbb{M}\right)+1\) random \(\bm{\theta}\)s.

**Corollary A.20** (Separation by Finite Witnesses).: _Let \(\mathbb{M}\subseteq\mathbb{R}^{p}\), \(\mathbb{W}\subseteq\mathbb{R}^{q}\) be \(\sigma\)-subanalytic sets of dimension \(D\) and \(D_{\bm{\theta}}\) respectively. Let \(F:\mathbb{M}\times\mathbb{W}\to\mathbb{R}\) be a \(\sigma\)-subanalytic function. Define the set_

\[\mathcal{N}=\{(\bm{x},\bm{y})\in\mathbb{M}\times\mathbb{M}\ |\ \,F(\bm{x};\bm{ \theta})=F(\bm{y};\bm{\theta}),\ \forall\bm{\theta}\in\mathbb{W}\}.\]

_Suppose that for all \((\bm{x},\bm{y})\in\mathbb{M}\times\mathbb{M}\setminus\mathcal{N}\)_

\[\dim\{\bm{\theta}\in\mathbb{W}\ |\ \,F(\bm{x};\bm{\theta})=F(\bm{y};\bm{ \theta})\}\leq D_{\bm{\theta}}-1.\] (22)

_Then for generic \(\left(\bm{\theta}^{(1)},\ldots,\bm{\theta}^{(2D+1)}\right)\in\mathbb{W}^{D+1}\),_

\[\mathcal{N}=\{(\bm{x},\bm{y})\in\mathbb{M}\times\mathbb{M}\ |\ \,F(\bm{x};\bm{ \theta}^{(i)})=F(\bm{y};\bm{\theta}^{(i)}),\ \forall i=1,\ldots 2D+1\}.\] (23)

_Moreover, if \(\mathbb{W}\) is an open and connected subset of \(\mathbb{R}^{q}\), and \(F(\bm{x};\bm{\theta})\) is analytic as a function of \(\bm{\theta}\) for all fixed \(\bm{x}\in\mathbb{M}\), then condition (22) is not required, as it is automatically satisfied._

Proof.: Set \(\tilde{\mathbb{M}}=\mathbb{M}\times\mathbb{M}\), and define \(G:\tilde{\mathbb{M}}\times\mathbb{W}\to\mathbb{R}\) by

\[G\left((\bm{x},\bm{y})\,;\bm{\theta}\right)=F(\bm{x};\bm{\theta})-F(\bm{y}; \bm{\theta}).\]

Then \(\tilde{\mathbb{M}}\) is a \(\sigma\)-subanalytic set of dimension \(\dim(\tilde{\mathbb{M}})=2D\), and \(G\) is a \(\sigma\)-subanalytic function. Moreover, if \(F(\bm{x};\bm{\theta})\) is analytic as a function of \(\bm{\theta}\) for any fixed \(\bm{x}\in\mathbb{M}\), then the function \(\bm{\theta}\mapsto G\left((\bm{x},\bm{y})\,;\bm{\theta}\right)\) is analytic for any fixed \((\bm{x},\bm{y})\in\tilde{\mathbb{M}}\). The result follows from applying Theorem A.2 to \(G\). 

Using Corollary A.20, we can now easily prove Theorem 3.4 and Proposition 3.6 from the main text, which we restate here for convenience:

**Theorem 3.4**.: _(Finite Witness Theorem, simple version) Let \(\mathbb{M}\subseteq\mathbb{R}^{L}\) be a countable union of affine sets, each of which is of dimension \(\leq D\). Let \(\mathbb{W}\subseteq\mathbb{R}^{D_{\bm{\theta}}}\) be open and connected. Let \(F\left(\bm{x};\bm{\theta}\right):\mathbb{M}\times\mathbb{W}\to\mathbb{R}\) be an analytic function. Then for almost any \(\left(\bm{\theta}^{(1)},\ldots,\bm{\theta}^{(2D+1)}\right)\in\mathbb{W}^{2D+1}\), the following set equality holds:_

\[\{(\bm{x},\bm{y})\in\mathbb{M}\times\mathbb{M}\ |\ \,F\left(\bm{x};\bm{ \theta}\right)=F\left(\bm{y};\bm{\theta}\right),\ \forall\bm{\theta}\in\mathbb{W}\}=\] \[\{(\bm{x},\bm{y})\in\mathbb{M}\times\mathbb{M}\ |\ \,F\left(\bm{x};\bm{ \theta}^{(i)}\right)=F\left(\bm{y};\bm{\theta}^{(i)}\right),\ \forall i=1,\ldots 2D+1\}.\]

Proof of Theorem 3.4.: Since affine sets are semialgebraic, countable unions of affine sets are \(\sigma\)-subanalytic. Therefore, Theorem 3.4 follows from Corollary A.20. Note that by Corollary A.20, the claim holds for all \((\theta^{(1)},\ldots,\theta^{(2D+1)})\) except for possibly a dimension-deficient subset of \(\mathbb{W}^{D+1}\), which is slightly stronger than the 'for almost any' statement in the theorem.

We now prove Proposition 3.6 from the main text.

**Proposition 3.6**.: _Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be an analytic non-polynomial function. Let \(n,d\in\mathbb{N}\) and set \(m=2n(d+1)+1\). Let \(f:\mathbb{R}^{d}\to\mathbb{R}^{L}\) be an injective function that is a composition of \(\text{PwL}\) functions and analytic functions. Then for Lebesgue almost any \(\bm{A}\in\mathbb{R}^{m\times L},\bm{b}\in\mathbb{R}^{m}\), the function \(\sigma(\bm{A}f(\bm{x})+\bm{b})\) is moment injective on \(\mathcal{M}_{\leq n}(\mathbb{R}^{d})\)._

Proof.: Let \(\mathcal{Y}=f\left(\mathbb{R}^{d}\right)\subseteq\mathbb{R}^{L}\) be the image of \(f\). Since \(f\) is injective, it naturally induces an injective _pushforward_ map of measures \(f_{*}:\mathcal{M}_{\leq n}(\mathbb{R}^{d})\to\mathcal{M}_{\leq n}(\mathcal{Y})\), defined by

\[\mathcal{M}_{\leq n}(\mathbb{R}^{d})\ni\sum_{i=1}^{n}w_{i}\delta_{x_{i}}\mapsto \sum_{i=1}^{n}w_{i}\delta_{f(x_{i})}\in\mathcal{M}_{\leq n}(\mathcal{Y}).\]

Thus, it remains to show that for Lebesgue almost any \(\bm{A}\in\mathbb{R}^{m\times L},\ \bm{b}\in\mathbb{R}^{m}\), the function \(g(\bm{y})=\sigma(\bm{A}\bm{y}+\bm{b})\) is moment-injective on \(\mathcal{M}_{\leq n}(\mathcal{Y})\). To prove this, our first step is to bound the dimension of \(\mathcal{Y}\). By Corollary A.19, since \(f\) is a \(\sigma\)-subanalytic function, \(\mathcal{Y}\) is a \(\sigma\)-subanalytic set, and

\[\dim\left(\mathcal{Y}\right)\leq\dim\left(\mathbb{R}^{d}\right)=d.\]

Let \(\mathbb{M}\) be the space of measure parameters over \(\mathcal{Y}\) with \(n\) points:

\[\mathbb{M}=\{(\bm{w},\bm{Y})\in\mathbb{R}^{n}\times\mathcal{Y}^{n}\},\]

and let \(\mathbb{W}\) be the space of parameters

\[\mathbb{W}=\{(\bm{a},b)\in\mathbb{R}^{L}\times\mathbb{R}\}.\]

Then \(\mathbb{M}\) and \(\mathbb{W}\) are \(\sigma\)-subanalytic sets, \(\dim\left(\mathbb{M}\right)\leq n(d+1)\) and thus \(m=2n(d+1)+1\) is larger or equal to \(2\dim\left(\mathbb{M}\right)+1\). We can now proceed as in the proof of Theorem 3.3:

Define \(F:\mathbb{M}\times\mathbb{W}\to\mathbb{R}\) by

\[F(\bm{w},\bm{Y};\bm{a},b)=\sum_{i=1}^{n}w_{i}\sigma(\bm{a}\cdot\bm{y}_{i}+b).\]

The set \(\mathbb{W}\) is open and connected, and \(F(\bm{w},\bm{Y};\bm{a},b)\) is analytic as a function of \((\bm{a},b)\) for any fixed \((\bm{w},\bm{Y})\in\mathbb{M}\). Therefore, by Corollary A.20, for almost any choice of \(\left(\bm{a}_{i},b_{i}\right)_{i=1}^{m}\in\mathbb{W}\), the following set equality holds:

\[\begin{split}\{((\bm{w},\bm{Y})\,,(\bm{w}^{\prime},\bm{Y}^{ \prime}))\in\mathbb{M}\times\mathbb{M}\ |\ F\left(\bm{w},\bm{Y};\bm{a},b \right)=F\left(\bm{w}^{\prime},\bm{Y}^{\prime};\bm{a},b\right),\ \forall\ (\bm{a},b)\in\mathbb{W}\}=\\ \{((\bm{w},\bm{Y})\,,(\bm{w}^{\prime},\bm{Y}^{\prime}))\in\mathbb{ M}\times\mathbb{M}\ |\ F\left(\bm{w},\bm{Y};\bm{a}_{i},b_{i}\right)=F\left(\bm{w}^{\prime},\bm{Y}^{ \prime};\bm{a}_{i},b_{i}\right),\ \forall i=1,\ldots,m\}.\end{split}\] (24)

Let \(\bm{A}\in\mathbb{R}^{m\times L}\) with rows \(\bm{a}_{1},\ldots,\bm{a}_{m}\), and \(\bm{b}=(b_{1},\ldots,b_{m})\). Suppose that \(\bm{A}\),\(\bm{b}\) indeed satisfy (24). Then, using the same argument as in the proof of Theorem 3.3, we see that the function \(g\) is moment-injective on \(\mathcal{M}_{\leq n}\left(\mathcal{Y}\right)\). This concludes the proof.

Moment Injectivity of piecewise-linear networks

In this appendix, we describe some additional results on moment injectivity of PwL networks. Our results are summarized as follows:

1. PwL networks are not moment injective on \(\mathcal{M}_{\leq n}(\Sigma)\), when \(\Sigma\) is an infinite set.
2. PwL networks are not moment injective on \(\mathcal{S}_{\leq n}(\Omega)\) when \(\Omega=\mathbb{R}^{d}\) (shown in the main text) or \(\Omega=\mathbb{Z}^{d}\).
3. There exist irregular, countable infinite \(\Sigma\), for which PwL networks are moment injective on \(\mathcal{S}_{\leq n}(\Sigma)\) (but not on \(\mathcal{M}_{\leq n}(\Sigma)\) as mentioned above).
4. For _finite_\(\Sigma\), PwL networks can be moment-injective on both \(\mathcal{M}_{\leq n}(\Sigma)\) and \(\mathcal{S}_{\leq n}(\Sigma)\). The number of neurons needed for injectivity depends on \(n\) and on the size of \(\Sigma\).

We now prove these results. We begin with discussing infinite alphabets.

### Failure of moment-injectivity for piecewise-linear functions with infinite alphabets

We begin by showing that PwL moment injectivity is never possible on spaces of _measures_, when the alphabet is _infinite_:

**Proposition B.1**.: _For all natural \(m,d\) and \(n\geq(d+2)/2\), and \(\Omega\subseteq\mathbb{R}^{d}\) that is not finite, if \(\bm{\psi}:\mathbb{R}^{d}\to\mathbb{R}^{m}\) is piecewise linear, then it is not moment injective on \(\mathcal{M}_{\leq n}(\Omega)\)._

Proof.: The proof is based on the fact that if \(\mu=\sum_{i=1}^{n}w_{i}\delta_{\bm{x}_{i}}\) is supported on a single linear region \(L\) of \(\bm{\psi}\), then, denoting the parameters of the affine function corresponding to the region \(L\) by \(\bm{A}\in\mathbb{R}^{d\times m},\bm{b}\in\mathbb{R}^{m}\), we have that

\[\sum_{i=1}^{n}w_{i}\bm{\psi}(\bm{x}_{i})=\bm{A}\left(\sum_{i=1}^{n}w_{i}\bm{x} _{i}\right)+\bm{b}\left(\sum_{i=1}^{n}w_{i}\right).\] (25)

Thus it is sufficient to find two distinct measures in \(\mathcal{M}_{\leq n}(\Omega)\) which are supported in the same linear region, and for which the two sums \(\sum_{i=1}^{n}w_{i}\) and \(\sum_{i=1}^{n}w_{i}\bm{x}_{i}\) give the same value. Since \(\bm{\psi}\) has a finite number of linear regions while \(\Omega\) is infinite, there is some linear region which contains an infinite number of points in \(\Omega\). Thus, we can choose \(d+2\) distinct points \(\bm{x}_{1},\ldots,\bm{x}_{d+2}\) in this region. Since all these points are in \(\mathbb{R}^{d}\), they must be affinely dependent, which means there exist weights \(w_{1},\ldots,w_{d+2}\), not all of which are zero, such that

\[\sum_{j=1}^{d+2}w_{j}\bm{x}_{j}=0\text{ and }\sum_{j=1}^{d+2}w_{j}=0.\]

We can now define \(k=\lceil(d+2)/2\rceil\), and set \(\mu=\sum_{i=1}^{k}w_{i}\delta_{\bm{x}_{i}}\) and \(\mu^{\prime}=\sum_{j=k+1}^{d+2}(-w_{j})\delta_{\bm{x}_{j}}\). According to (25), integration of \(\bm{\psi}\) over \(\mu\) and over \(\mu^{\prime}\) gives the same value, while \(\mu\neq\mu^{\prime}\), and so \(\bm{\psi}\) is not moment injective. 

If we change the setup of Proposition B.1 and consider spaces of _multisets_ rather than spaces of _measures_, we get a somewhat more complicate picture: In the following, we give an example of an irregular, infinite, countable alphabet \(\Sigma_{\alpha}\), for which even the simple identity function \(x\mapsto x\)_is_ moment injective. We shall then show that for the more natural alphabet \(\Sigma=\mathbb{Z}^{d}\), PwL moment injectivity is not possible on \(\mathcal{S}_{\leq n}(\Sigma)\). We define \(\Sigma_{\alpha}=\{\alpha,\alpha^{2},\alpha^{3},\ldots\}\), where \(\alpha\in\mathbb{R}\) is a _transcendental number_, which means that it is not the root of any polynomial with rational coefficients. Examples of such numbers include \(\pi\) and \(e\). In this case, we see that the integral \(\int_{\mathbb{R}}xd\mu(x)\) over any non-zero measure \(\mu=\sum_{i=1}^{n}w_{i}\delta_{\alpha^{i}}\) where \(w_{i}\) are rational numbers, will not be zero. This implies that the identity function is moment injective on \(\mathcal{S}_{\leq n}(\Sigma_{\alpha})\) for any natural \(n\). Note also that if \(n\) is fixed, we can just take \(\alpha=(n+1)^{-1}\), as suggested in [44]. Finally, note that by choosing \(\alpha\) to be positive, we have that the simple piecewise linear MLP \(\mathrm{ReLU}(x)\) is moment injective on \(\mathcal{S}_{\leq n}(\Sigma_{\alpha})\) as well, as \(\mathrm{ReLU}(x)=x\) on the domain \(\Sigma_{\alpha}\).

We now consider the case \(\Sigma=\mathbb{Z}^{d}\):

**Proposition B.2**.: _Let \(d,n\geq 2\) be natural numbers and \(\Sigma=\mathbb{Z}^{d}\). If \(\bm{\psi}:\mathbb{R}^{d}\to\mathbb{R}^{m}\) is piecewise linear, then it is not moment injective on \(\mathcal{S}_{\leq n}(\Sigma)\)._

Proof.: Let us first consider the case where \(d=1\). Since the number of linear regions is finite, there exists a linear region \(L\) that contains an infinite number of natural numbers. From the convexity of \(L\), it follows that \(L\) contains an interval of the form \([N,\infty)\) where \(N\in\mathbb{N}\). Using the same argument as in Proposition 4.1, we conclude that the moments of the multisets \(\{\!\!\{2N,2N\}\!\!\}\) and \(\{\!\!\{N,3N\}\!\!\}\) are the same, and thus \(\bm{\psi}\) is not moment injective. The same argument can be applied in the case \(d>1\) using the identification of \(\mathbb{Z}\) with the elements in \(\mathbb{Z}^{d}\) whose first \(d-1\) coordinates are zero. 

### Piecewise linear network moment injectivity for sets with finite alphabets

We now consider moment injectivity of PwL networks when the alphabet is finite. Recall that when the activations are analytic, moment injectivity is achievable on \(\mathcal{S}_{\leq n}(\Sigma)\) with a single output neuron. For PwL activations, moment injectivity is also possible, but the required number of neurons depends on the cardinality of the alphabet.

**Definition B.3**.: For given \(W,L,d\in\mathbb{N}\), we denote by \(M(W,L,d)\) the maximal number of linear regions that a ReLU network \(f:\mathbb{R}^{d}\to\mathbb{R}\) with maximal width \(W\) and depth \(L\) can have.

**Proposition B.4**.: _If \(n,W,L,d\) are natural numbers with \(n\geq(d+2)/2\), and \(\Sigma\subseteq\mathbb{R}^{d}\) is a finite alphabet satisfying \(|\Sigma|>M(W,L,d)\cdot(d+1)\), then any ReLU neural network \(\bm{\psi}:\mathbb{R}^{d}\to\mathbb{R}^{m}\) of depth \(L\) and maximal width \(W\) will not be injective on \(\mathcal{M}_{\leq n}(\Sigma)\). Moreover, for large enough \(n\), assuming that all points in \(\Sigma\) have rational coordinates, such ReLU networks will also not be moment injective on \(\mathcal{S}_{\leq n}(\Sigma)\)._

Proof.: Since \(M(W,L,d)<\frac{|\Sigma|}{d+1}\), by the pigeonhole principle, we know that there must exist at least one linear region that contains at least \(d+2\) points in \(\Sigma\). Denote these points \(\bm{x}_{1},\bm{x}_{2},...,\bm{x}_{d+2}\). These points are affinely dependent, and so there exist weights \(w_{1},\ldots,w_{d+2}\), not all of which are zero, such that

\[\sum_{j=1}^{d+2}w_{j}\bm{x}_{j}=0\text{ and }\sum_{j=1}^{d+2}w_{j}=0.\] (26)

We can now define \(k=\lceil(d+2)/2\rceil\), and set \(\mu=\sum_{i=1}^{k}w_{i}\delta_{\bm{x}_{i}}\) and \(\mu^{\prime}=\sum_{j=k+1}^{d+2}(-w_{j})\delta_{\bm{x}_{j}}\). According to (25), integration of \(\bm{\psi}\) over \(\mu\) and over \(\mu^{\prime}\) gives the same value, while \(\mu\neq\mu^{\prime}\), and so \(\bm{\psi}\) is not moment injective on \(\mathcal{M}_{\leq n}(\Sigma)\).

We now show that moment injectivity fails also on \(\mathcal{S}_{\leq n}(\Sigma)\) when \(n\) is large enough, under the assumption that \(\Sigma\subset\mathbb{Q}^{d}\). Under this assumption, the above-mentioned points \(\bm{x}_{1},\bm{x}_{2},...,\bm{x}_{d+2}\) are affinely dependent over \(\mathbb{Q}\), and there exist rational weights \(w_{1},\ldots,w_{d+2}\) for which (26) holds.

Now we can multiply (26) by an appropriate integer, so that the \(w_{i}\)s are all integers. Using these new weights, suppose that \(n\geq\sum_{i}|w_{i}|\). Let \(\mu=\sum_{i:w_{i}\geq 0}w_{i}\delta_{\bm{x}_{i}}\) and \(\mu^{\prime}=\sum_{j:w_{j}<0}(-w_{j})\delta_{\bm{x}_{j}}\). Then \(\mu\) and \(\mu^{\prime}\) are two distinct measures that assign the same moment to \(f\). Since the weights of \(\mu\) and \(\mu^{\prime}\) are all natural numbers, these measures correspond to multisets, where the weights denote the number of repetitions of each element. 

Number of linear regionsProposition B.4 gives a lower bound on the number of linear regions needed for moment injectivity. The relationship between a neural network's size and the number of its linear regions is well studied [29, 35]. In particular, it is known that the number of linear regions can be exponentially larger than the number of the parameters. As shown in ([1], Theorem D.6), the maximal number of linear regions \(M(W,L,d)\) of a ReLU network with maximal width \(W\), depth \(L\), and input dimension \(d\), is bounded from above by \(CW^{d\cdot L}\), where \(C>0\) is a constant. Joining this together with Proposition B.4, we see that we cannot have moment injectivity if

\[CW^{d\cdot L}(d+1)<|\Sigma|.\] (27)

This implies that the number of neurons required for injectivity is at least logarithmic in the cardinality of the alphabet. Although this is only a lower bound, we believe that this bound can be achieved using the techniques developed in [1].

We conclude this discussion by providing an _upper bound_ for the number of linear regions needed in the simple case that \(\Sigma=\{\ell_{1}<\ell_{2}<\ldots<\ell_{S}\}\) is a subset of \(\mathbb{R}\), and the depth \(L\) is one. In this case, our bound (27) states that one cannot attain moment injectivity on \(\mathcal{M}_{\leq n}(\Sigma)\) when \(2C\cdot W<S\). In the other direction, we show that moment injectivity _can_ be obtained when \(W=S\).

**Proposition B.5**.: _Let \(\Sigma=\{\ell_{1}<\ell_{2}<\ldots<\ell_{S}\}\), and let \(n\leq S\) be some natural number. Then there exists a shallow ReLU network of width \(W=S\) that is moment injective on \(\mathcal{M}_{\leq n}(\Sigma)\)._

Proof.: Choose some \(t_{1},\ldots,t_{S}\) such that

\[t_{1}<\ell_{1}<t_{2}<\ell_{2}<\ldots<t_{S}<\ell_{S}.\]

Consider the shallow ReLU network

\[x\mapsto\bm{\psi}(x)=\begin{pmatrix}\operatorname{ReLU}(x-t_{1})\\ \operatorname{ReLU}(x-t_{2})\\ \vdots\\ \operatorname{ReLU}(x-t_{S})\end{pmatrix}\]

Suppose that \(\mu=\sum_{i=1}^{S}w_{i}\delta_{\ell_{i}}\) and \(\mu^{\prime}=\sum_{i=1}^{S}w^{\prime}_{i}\delta_{\ell_{i}}\) are two measures such that

\[\int\bm{\psi}(x)d\mu(x)=\int\bm{\psi}(x)d\mu^{\prime}(x).\]

The equality of the last coordinate implies

\[w^{\prime}_{S}(\ell_{S}-t_{S})=\int\operatorname{ReLU}(x-t_{S})d\mu^{\prime}( x)=\int\operatorname{ReLU}(x-t_{S})d\mu(x)=w_{S}(\ell_{S}-t_{S})\]

and so \(w_{S}=w^{\prime}_{S}\). Next, we can consider the equality of the \((S-1)\)th coordinate and show that this implies that \(w_{S-1}=w^{\prime}_{S-1}\). Continuing with this argument recursively, we see that the measures must agree.

Optimal embedding dimension of moment-injective functions

In Appendix C.1, we prove the lower bounds on the embedding dimensions showed in Table 1 for \(\mathcal{M}_{\leq n}(\mathbb{R}^{d}),\mathcal{S}_{\leq n}(\mathbb{R}^{d})\) and \(\mathcal{M}_{\leq n}(\Sigma)\) for countable \(\Sigma\). The remaining lower bounds in the table are equal to one, and thus do not require a proof. For \(\mathcal{S}_{\leq n}(\mathbb{R}^{d})\), these bounds are already known [16; 42], but here we present a slightly different proof, which also applies to spaces of measures that were not considered previously. As a rule, for all the spaces of measures/multisets considered, the lower bound equals to the _intrinsic dimension_ of the space, i.e., the number of continuous parameters required to describe it. For example, \(\mathcal{S}_{\leq n}(\mathbb{R}^{d})\) is parameterized by \(n\) continuous vectors in \(\mathbb{R}^{d}\), with a total dimension of \(n\cdot d\), and by a discrete weight vector that has no influence on the dimension, and thus the lower bound in this case is \(n\cdot d\).

An interesting question that remains open is whether the gap between the embedding dimension achieved here with shallow neural networks, and the best lower bound, can be closed completely. For multisets with features in \([0,1]\), [42] showed that the lower bound of \(n\) moments can be attained using \(n\) polynomial functions. In Appendix C.2, we show that for spaces of measures with features in \(\mathbb{R}\), the lower bound \(2n\) in Table 1 can be attained by \(2n\) functions that form a _T-system_ (see discussion below). Examples of T-systems include the functions \(x,x^{2},\ldots,x^{2n}\), as well as \(2n\) univariate sigmoid neural networks with distinct parameters. Thus, for \(d=1\), we know that the lower bounds on the embedding dimension are optimal. For \(d>1\), it seems that this question is still open.

### Lower bounds

Our lower bounds are based on the following intuitive fact, which is a simple corollary to Brouwer's invariance of domain theorem:

**Proposition C.1**.: _If \(U\subseteq\mathbb{R}^{D}\) is an open set and \(F:U\to\mathbb{R}^{M}\) is continuous and injective, then \(M\geq D\)._

Proof.: Suppose by contradiction that \(M<D\), and let \(\mathbf{0}\in\mathbb{R}^{D-M}\) be a vector of all zeros. Then the function \(\tilde{F}(\bm{x})=(\mathbf{0},F(\bm{x}))\) is a continuous injective function of \(U\subseteq\mathbb{R}^{D}\) into \(\mathbb{R}^{D}\) and by the invariance of domain theorem ([13], Theorem 2B.3) \(\tilde{F}(U)\) should be open. This leads to a contradiction, since arbitrarily small perturbations of the first coordinate of a point \((\mathbf{0},F(\bm{x}))\) in the image of \(\tilde{F}\) will no longer be in the image. 

Based on this proposition, we can prove the lower bounds in Table 1. The following lower bound holds for _any_ alphabet \(\Sigma\).

**Theorem C.2**.: _Let \(\Sigma\subseteq\mathbb{R}^{d}\) with \(|\Sigma|\geq n\). Suppose that \(f:\Sigma\to\mathbb{R}^{m}\) is moment injective on \(\mathcal{M}_{\leq n}(\Sigma)\). Then \(m\geq n\)._

Proof.: Fix \(n\) distinct points \(\bm{x}_{1},\ldots,\bm{x}_{n}\) in \(\Sigma\). The function

\[(w_{1},\ldots,w_{n})\mapsto\sum_{i=1}^{n}w_{i}\delta_{\bm{x}_{i}}\]

maps \(\mathbb{R}^{n}\) injectively into the space of measures \(\mathcal{M}_{\leq n}(\Sigma)\). Since \(f\) is moment injective, it follows that the continuous (in fact, linear) function

\[\mathbb{R}^{n}\ni(w_{1},\ldots,w_{n})\mapsto\sum_{i=1}^{n}w_{i}f(\bm{x}_{i}) \in\mathbb{R}^{m}\]

is injective. Thus, by Proposition C.1 we have that \(m\geq n\). 

When the alphabet is uncountable, we can obtain stronger lower bounds. We now show two lower bounds on the embedding dimension required for moment injectivity of a continuous function \(f\) with the alphabet \(\Omega=\mathbb{R}^{d}\).

**Theorem C.3**.: _Let \(f:\mathbb{R}^{d}\to\mathbb{R}^{m}\) be a continuous function. Then_

1. _If_ \(f\) _is moment injective on_ \(\mathcal{M}_{\leq n}(\mathbb{R}^{d})\)_, then_ \(m\geq n(d+1)\)2. _If_ \(f\) _is moment injective on_ \(\mathcal{S}_{\leq n}(\mathbb{R}^{d})\)_, then_ \(m\geq nd\)_._

Proof.: Choose \(n\) distinct points \(\bm{y}_{1},\ldots,\bm{y}_{n}\in\mathbb{R}^{d}\) and let \(r>0\) be small enough so that the distance between any two distinct points \(\bm{y}_{i},\bm{y}_{j}\) is larger than \(2r\). Then the function

\[(w_{1},\ldots,w_{n},\bm{x}_{i},\ldots,\bm{x}_{n})\mapsto\sum_{i=1}^{n}w_{i} \delta_{\bm{x}_{i}}\]

maps the open set

\[\{(w_{1},\ldots,w_{n},\bm{x}_{i},\ldots,\bm{x}_{n})\ |\ \ w_{i}\neq 0,|\bm{x}_{i}- \bm{y}_{i}|<r,i=1,\ldots,n\}\]

injectively into \(\mathcal{M}_{\leq n}(\mathbb{R}^{d})\). Therefore, the map

\[\mathbb{R}^{n+dn}\ni(w_{1},\ldots,w_{n},\bm{x}_{1},\ldots,\bm{x}_{n})\mapsto \sum_{i=1}^{n}w_{i}f(\bm{x}_{i})\in\mathbb{R}^{m}\]

is injective, and since it is also continuous, Proposition C.1 implies that \(m\geq n(d+1)\).

For the second part of the theorem, where \(f\) is moment injective on \(\mathcal{S}_{\leq n}(\mathbb{R}^{d})\), we can use a similar argument, the only difference being that we fix \(w_{i}=1\). Namely, the map

\[\mathbb{R}^{nd}\ni(\bm{x}_{1},\ldots,\bm{x}_{n})\mapsto\sum_{i=1}^{n}f(\bm{x} _{i})\in\mathbb{R}^{m}\]

is continuous and injective on the open set of \((\bm{x}_{1},\ldots,\bm{x}_{n})\) with \(|\bm{x}_{i}-\bm{y}_{i}|<r\) for all \(i\), and therefore by Proposition C.1\(m\geq nd\).

### T-Systems

We now show how to achieve moment injectivity on \(\mathcal{M}_{\leq n}(\mathbb{R})\) with an _optimal_ embedding dimension. Recall from Table 1 that for this space, our lower bound on the embedding dimension of continuous moment-injective functions is \(2n\), whereas the embedding dimension of the MLPs constructed using our technique is \(4n+1\). We shall now show that this gap can be closed, using the moments of \(2n\) functions that form a _T-system_, which we now define.

**Definition C.4** ([20]).: Let \(\Omega\subseteq\mathbb{R}\). We say that the \(k\) functions \(\tau_{i}:\Omega\to\mathbb{R},\ i=1,\ldots,k\), form a **T-system** on \(\Omega\), if for all pairwise-distinct \(x_{1}\ldots x_{k}\in\Omega\), the square matrix

\[M_{\bm{\tau}}=[\tau_{i}(x_{j})]_{1\leq i,j\leq k}\in\mathbb{R}^{k\times k}\] (28)

is invertible.

An example of a T-system on \(\Omega=\mathbb{R}\) is the standard monomial basis \(\tau_{i}(x)=x^{i-1},i=1,\ldots,k\). With this choice, (28) is the Vandermonde matrix, which is invertible, and hence the monomial basis is a T-system.

We know that the moments of the first \(n\) elements of the standard monomial basis are injective on \(\mathcal{S}_{\leq n}(\mathbb{R})\). The following simple proposition shows that injectivity on all of \(\mathcal{M}_{\leq n}(\mathbb{R})\) can be achieved, at the cost of increasing the number of moments to \(2n\). Moreover, this is true for all T-systems.

**Proposition C.5** (Trivial, based on [20]).: _If \(\bm{\tau}=(\tau_{1},\ldots,\tau_{2n})\) is a **T-system** on \(\Omega\subseteq\mathbb{R}\), then the induced moment function_

\[\hat{\bm{\tau}}(\mu)=\left(\int_{\mathbb{R}}\tau_{1}(x)d\mu(x),\ldots,\int_{ \mathbb{R}}\tau_{2n}(x)d\mu(x)\right)\]

_is injective on \(\mathcal{M}_{\leq n}(\Omega)\)._

Proof.: Let \(\mu,\mu^{\prime}\in\mathcal{M}_{\leq n}(\Omega)\) such that \(\hat{\bm{\tau}}(\mu)=\hat{\bm{\tau}}(\mu^{\prime})\). Then \(\bm{\tau}(\mu-\mu^{\prime})=0\), where \(\mu-\mu^{\prime}\) denotes the difference measure. Since \(\mu-\mu^{\prime}\) is supported on at most \(2n\) points, we can write

\[\mu-\mu^{\prime}=\sum_{i=1}^{2n}w_{i}\delta_{\bm{x}_{i}},\]where the points \(\bm{x}_{i}\) are pairwise distinct, and some of the \(w_{i}\) may be zero. Our goal is to show that the vector

\[\bm{w}=(w_{1},\dots,w_{2n})\]

is all zero. Indeed, the fact that \(\hat{\tau}(\mu-\mu^{\prime})=0\) implies that \(M_{\bm{\tau}}\cdot\bm{w}=0\). And since by assumption \(M_{\bm{\tau}}\) is full rank, we have that \(\bm{w}=0\). 

**Example C.6** (Sigmoid T-systems).: We've seen that the standard monomial basis forms a T-system. Another well-known example [20] is the family of functions

\[\tau_{i}(x)=\frac{1}{x-a_{i}},\quad i=1,\dots,k.\]

If the numbers \(a_{1},\dots,a_{k}\) are pairwise distinct, then \(\bm{\tau}=(\tau_{1},\dots,\tau_{k})\) form a T-system on \(\Omega=\mathbb{R}\setminus\{a_{1},\dots,a_{k}\}\).

We can use these example to obtain an alternative T-system based on the sigmoid activation \(\sigma(x)=(1+e^{-x})^{-1}\). Firstly, using the injectivity of the exponent function, we can deduce from the fact that \(\bm{\tau}\) is a T-system, that for any distinct \(b_{1},\dots,b_{k}\) the functions

\[\hat{\tau}_{i}(x)=\frac{1}{e^{b_{i}}+e^{-x}},\quad i=1,\dots,k\]

form a T-system on \(\Omega=\mathbb{R}\). It follows that the functions

\[\sigma(x+b_{i})=\frac{1}{1+e^{-x-b_{i}}}=e^{-b_{i}}\left(\frac{1}{e^{b_{i}}+e^ {-x}}\right),\quad i=1,\dots,k\]

form a T-system as well. In particular, for all pairwise-distinct \(b_{1},\dots,b_{2n}\), the function

\[x\mapsto\sigma(x+b_{i}),\quad i=1,\dots,2n\]

is moment injective on \(\mathcal{M}_{\leq n}(\mathbb{R})\).

Unfortunately, these results cannot be extended to \(\mathcal{M}_{\leq n}(\mathbb{R}^{d})\) with \(d>1\), as it is known that T-systems can only be defined on subsets of \(\mathbb{R}\) or \(S^{1}\)[20].

Proofs

In this section, we provide the proofs omitted from the main text, except for the proofs of Theorem 3.4 and Proposition 3.6, which appear in Appendix A.6.

### Proofs for Section 3

**Proposition 3.2**.: _Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be a continuous function that is not a polynomial; then \(\sigma\) is discriminatory._

Proof.: Let \(\sigma\) be as in the statement of the proposition, and let \(\mu\) be a signed Borel measure that is finite and is supported on a compact set \(K\subseteq\mathbb{R}^{d}\). Furthermore, assume that \(\int_{\mathbb{R}^{d}}\sigma(\bm{a}\cdot\bm{x}+b)d\mu(\bm{x})=0\) for all \(\bm{a}\) and \(b\). We need to prove that \(\mu=0\).

Note that by the linearity of \(\mu\), we have that \(\int fd\mu=0\) for every \(f\) in the space spanned by functions of the form \(\sigma(\bm{a}\cdot\bm{x}+b)\). Additionally, since functions in this space can approximate any continuous function on \(K\) uniformly ([32]), Propositions 3.3 and 3.8), and \(\mu\) is compactly supported, we have that \(\int fd\mu=0\) for every continuous function. Finally, by the Riesz representation theorem ([36], Theorem 6.19) we know that a signed (and more generally complex) measure on \(K\) is defined uniquely by the integrals of continuous functions and thus \(\mu=0\) as required. 

**Proposition 3.5**.: _Let \(n,d\in\mathbb{N}\) and set \(m=2n(d+1)+1\). Let \(\mathbb{W}=(\bm{y},\sigma)\in\mathbb{R}^{d}\times\mathbb{R}_{+}\). Then for Lebesgue almost any \((\bm{y}_{i},\sigma_{i})_{i=1}^{m}\in\mathbb{W}^{m}\), the function_

\[f(\bm{x})=\left(\exp\left(-\frac{\|\bm{x}-\bm{y}_{1}\|^{2}}{\sigma_{1}^{2}} \right),\ldots,\exp\left(-\frac{\|\bm{x}-\bm{y}_{m}\|^{2}}{\sigma_{m}^{2}} \right)\right)\]

_is moment injective on \(\mathcal{M}_{\leq n}(\mathbb{R}^{d})\)._

Proof.: The proof follows the proof of Theorem 3.3 with some modifications. Set \(m=2n(d+1)+1\). A measure in \(\mathcal{M}_{\leq n}(\mathbb{R}^{d})\) is determined (albeit not uniquely) by a matrix \(\bm{X}=(\bm{x}_{1},\ldots,\bm{x}_{n})\in\mathbb{R}^{d\times n}\) that represents \(n\) points in \(\mathbb{R}^{d}\), and a weight vector \(\bm{w}\in\mathbb{R}^{n}\). Let \(\mathbb{M}\) denote the space of pairs of measure parameters

\[\mathbb{M}=\{(\bm{w},\bm{w}^{\prime},\bm{X},\bm{X}^{\prime})\in\mathbb{R}^{n} \times\mathbb{R}^{n}\times\mathbb{R}^{d\times n}\times\mathbb{R}^{d\times n }\},\]

and let

\[F(\bm{w},\bm{w}^{\prime},\bm{X},\bm{X}^{\prime};\bm{y},\sigma)=\sum_{i=1}^{n} w_{i}\exp\left(-\sigma^{-2}\|\bm{x}_{i}-\bm{y}\|^{2}\right)-\sum_{i=1}^{n}w_{i}^{ \prime}\exp\left(-\sigma^{-2}\|\bm{x}_{i}^{\prime}-\bm{y}\|^{2}\right).\] (29)

We prove the proposition by showing that for Lebesgue almost every \(\bm{y}_{1},\ldots,\bm{y}_{m}\) and \(\sigma_{1},\ldots,\sigma_{m}\),

\[\{(\bm{w},\bm{w}^{\prime},\bm{X},\bm{X}^{\prime})\in\mathbb{M}\ |\ \sum_{i=1}^{n}w_{i}\delta_{\bm{x}_{i}}=\sum_{i=1}^{n}w_{i}^{\prime}\delta_{\bm{ x}_{i}^{\prime}}\}\] \[\stackrel{{(*)}}{{=}}\{(\bm{w},\bm{w}^{\prime},\bm{X },\bm{X}^{\prime})\in\mathbb{M}\ |\ \ F\left(\bm{w},\bm{w}^{\prime},\bm{X},\bm{X}^{\prime};\bm{y},\sigma \right)=0,\ \forall\bm{y}\in\mathbb{R}^{d},\sigma\in\mathbb{R}_{+}\}\] \[\stackrel{{(**)}}{{=}}\{(\bm{w},\bm{w}^{\prime},\bm{X },\bm{X}^{\prime})\in\mathbb{M}\ |\ \ F\left(\bm{w},\bm{w}^{\prime},\bm{X},\bm{X}^{\prime};\bm{y}_{i},\sigma_{i} \right)=0,\ \forall i=1,\ldots,m\}\]

As in the proof of Theorem 3.3, equality (**) follows from the _finite witness theorem_. The equality (*) follows on the one hand from the fact that whenever the measures defined by \((\bm{w},\bm{X})\) and \((\bm{w}^{\prime},\bm{X}^{\prime})\) are the same, necessarily all integrals of functions against these measures are the same, and so \(F\left(\bm{w},\bm{w}^{\prime},\bm{X},\bm{X}^{\prime};\bm{y},\sigma\right)=0\) for all \(\bm{y}\in\mathbb{R}^{d},\sigma\in\mathbb{R}_{+}\).

On the other hand, if the measure \(\mu\) defined by \((\bm{w},\bm{X})\) and the measure \(\mu^{\prime}\) defined by \((\bm{w}^{\prime},\bm{X}^{\prime})\), are _not_ the same, then it is sufficient to show that for some choice of \(\bm{y}\in\mathbb{R}^{d},\sigma\in\mathbb{R}_{+}\) we have \(F(\bm{w},\bm{w}^{\prime},\bm{X},\bm{X}^{\prime};\bm{y},\sigma)\neq 0\). To see this is indeed the case, choose some \(\bm{y}_{0}\in\mathbb{R}^{d}\) which the non-zero matrix \(\mu-\mu^{\prime}\) assigns a non-zero weight \(w_{0}\). Then

\[\lim_{\sigma\to 0,\sigma>0}F(\bm{w},\bm{w}^{\prime},\bm{X},\bm{X}^{\prime};\bm{y} _{0},\sigma)=w_{0}\neq 0.\]

and so for small enough \(\sigma\) we have that \(F(\bm{w},\bm{w}^{\prime},\bm{X},\bm{X}^{\prime};\bm{y}_{0},\sigma)\) which concludes the proof.

### Proofs for Section 5

**Proposition 5.1**.: _Let \(n\geq 2\), \(d,m\in\mathbb{N}\), and let \(f:\mathbb{R}^{d}\to\mathbb{R}^{m}\) be differentiable at some \(\bm{x}_{0}\in\mathbb{R}^{d}\). Then the induced moment function \(\hat{f}:\mathcal{S}_{\leq n}(\mathbb{R}^{d})\to\mathbb{R}^{m}\) defined in (1) is not bi-Lipschitz._

Proof.: We choose some arbitrary \(\bm{d}\in\mathbb{R}^{d}\) with unit norm, and focus on multisets of two elements in \(\mathbb{R}^{d}\) of the form \(S_{\epsilon}=\{\!\!\{\bm{x}_{0}+\epsilon\bm{d},\bm{x}_{0}-\epsilon\bm{d}\}\!\}\). We note that the Wasserstein distance \(W_{2}(S_{\epsilon},S_{0})\) is \(\sqrt{2}\epsilon\). Thus, it is sufficient to show that

\[\lim_{\epsilon\to 0}\frac{\|\hat{f}(S_{\epsilon})-\hat{f}(S_{0})\|}{|\epsilon|}=0,\] (30)

for this implies that there is no positive \(c\) for which (8) holds. Indeed, denoting the differential of \(f\) at \(\bm{x}_{0}\) by \(J\), we have

\[\frac{\|\hat{f}(S_{\epsilon})-\hat{f}(S_{0})\|}{|\epsilon|} =\frac{\|f(\bm{x}_{0}+\epsilon\bm{d})+f(\bm{x}_{0}-\epsilon\bm{d}) -2f(\bm{x}_{0})\|}{|\epsilon|}\] \[=\frac{\|f(\bm{x}_{0}+\epsilon\bm{d})-f(\bm{x}_{0})-\epsilon J\bm {d}+f(\bm{x}_{0}-\epsilon\bm{d})-f(\bm{x}_{0})+\epsilon J\bm{d}\|}{|\epsilon|}\] \[\leq\frac{\|f(\bm{x}_{0}+\epsilon\bm{d})-f(\bm{x}_{0})-\epsilon J \bm{d}\|}{|\epsilon|}+\frac{\|f(\bm{x}_{0}-\epsilon\bm{d})-f(\bm{x}_{0})-(- \epsilon J\bm{d})\|}{|\epsilon|}\overset{\epsilon\to 0}{\to}0,\]

where the convergence to zero of the last expression follows from the definition of differentiability at a point. 

### Proofs for Section 6

We now prove Corollary 6.1:

**Corollary 6.1**.: _Let \(n,d\in\mathbb{N}\) and set \(m=2nd+1\). Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be an analytic non-polynomial function. Let \(K\subseteq\mathbb{R}^{d}\) be a compact set. Then there exist \(\bm{A}\in\mathbb{R}^{m\times d},\bm{b}\in\mathbb{R}^{d}\) such that for any continuous permutation-invariant \(f:K^{n}\to\mathbb{R}\), there exists a continuous \(F:\mathbb{R}^{m}\to\mathbb{R}\) such that_

\[f(\bm{X})=F\left(\sum_{j=1}^{n}\sigma(\bm{A}\bm{x}_{j}+\bm{b})\right),\quad \forall\bm{X}=(\bm{x}_{1},\dots,\bm{x}_{n})\in K^{n}.\] (9)

Proof.: By Proposition 1.3 in [9], it is sufficient to show that there exists \(\bm{A}\in\mathbb{R}^{m\times d},\bm{b}\in\mathbb{R}^{d}\) such that the permutation invariant function

\[(\bm{x}_{1},\dots,\bm{x}_{n})\mapsto\sum_{j=1}^{n}\sigma(\bm{A}\bm{x}_{j}+\bm{ b})\] (31)

is orbit separating. This means that any two element in \(K^{n}\) that are not related by a permutation will be separated by the function in (31). Any pair of elements not related by a permutation correspond to two distinct multisets in \(\mathcal{S}_{\leq n}(\mathbb{R}^{d})\) with exactly \(n\) points. By Theorem 3.3, for almost every choice of \(\bm{A},\bm{b}\) the function in (31) will be injective on \(\mathcal{S}_{\leq n}(\mathbb{R}^{d})\), and thus for such choice this function is indeed invariant and orbit separating. 

We now prove Corollary 6.2:

**Corollary 6.2**.: _Let \(n,d\in\mathbb{N}\) and set \(m=2n(d+1)+1\). Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be analytic and non-polynomial. Let \(K\subseteq\mathbb{R}^{d}\) be compact. Then there exist \(\bm{A}\in\mathbb{R}^{m\times d},\bm{b}\in\mathbb{R}^{m}\) such that for any continuous (in the \(2\)-Wasserstein sense) \(f:\mathcal{P}_{\leq n}(K)\to\mathbb{R}\), there exists a continuous \(F:\mathbb{R}^{m}\to\mathbb{R}\) such that_

\[f(\mu)=F\left(\int_{\bm{x}\in K}\sigma(\bm{A}\bm{x}+\bm{b})d\mu(\bm{x})\right), \quad\forall\mu\in\mathcal{P}_{\leq n}(K).\]Proof.: Our first step is to show that \(\mathcal{P}_{\leq n}(K)\) is compact with respect to the Wasserstein metric. Since \(\mathcal{P}_{\leq n}(K)\) is the image of the compact set

\[Q:=\{(\boldsymbol{w},\boldsymbol{X})\in\mathbb{R}^{n}\times K^{n}\ |\ \sum_{i=1}^{n}w_{i}=1\text{ and }w_{j}\geq 0,,j=1,\ldots,n\}\]

under the function

\[(\boldsymbol{w},\boldsymbol{X})\mapsto\sum_{i=1}^{n}w_{i}\delta_{\boldsymbol {x}_{i}},\]

it is sufficient to show that this function is continuous, as the image of a compact set under a continuous map is compact. Thus, given a sequence of \((\boldsymbol{w}^{(k)},\boldsymbol{X}^{(k)})\) which converges to some \((\boldsymbol{w},\boldsymbol{X})\), we need to show that in Wasserstein space, the measures \(\mu_{k}:=\sum_{i=1}^{n}w_{i}^{(k)}\delta_{\boldsymbol{x}_{i}^{(k)}}\) converge to the measure \(\mu:=\sum_{i=1}^{n}w_{i}\delta_{\boldsymbol{x}_{i}}\). Since the Wasserstein distance metrizes the weak topology on measures ([40], Theorem 6.9), it is sufficient to see that the integral of every continuous \(s:K\to\mathbb{R}\) against the sequence of measures converge to the limit measure. Indeed:

\[\int_{K}s(\boldsymbol{x})d\mu^{(k)}(\boldsymbol{x})=\sum_{i=1}^{n}w_{i}^{(k)} s(\boldsymbol{x}_{i}^{(k)})\to\sum_{i=1}^{n}w_{i}s(\boldsymbol{x}_{i})=\int_{K}s( \boldsymbol{x})d\mu(\boldsymbol{x})\]

Thus we have shown that \(\mathcal{P}_{\leq n}(K)\) is compact.

Next, by Theorem 3.3, for almost every choice of \(\boldsymbol{A},\boldsymbol{b}\) the function

\[q(\mu):=\int_{\boldsymbol{x}\in K}\sigma(\boldsymbol{A}\boldsymbol{x}+ \boldsymbol{b})d\mu(\boldsymbol{x})\]

is injective on \(\mathcal{M}_{\leq n}(\mathbb{R}^{d})\), and so in particular on the compact subset \(\mathcal{P}_{\leq n}(K)\). As \(q\) is also continuous, and a continuous injective function defined on a compact set is a homeomorphism, it follows that \(q^{-1}:q(\mathcal{P}_{\leq n}(K))\to\mathcal{P}_{\leq n}(K)\) is continuous. We then have that \(f=(f\circ q^{-1})\circ q\). We can then use Tietze's extension theorem to extend \(f\circ q^{-1}\) from its compact domain to a continuous function \(F\) defined on all of \(\mathbb{R}^{m}\), and we then obtain \(f=F\circ q\) as required. 

**Theorem 6.3**.: _Let \(n,d,T\in\mathbb{N}\) and let \(\Sigma\subseteq\mathbb{R}^{d}\) be countable. Let \(m\geq 1\) be any integer. Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be an analytic non-polynomial function. Then for Lebesgue almost any choice of \(\boldsymbol{A}^{(t)},\boldsymbol{b}^{(t)}\) and \(\eta^{(t)}\), the MPN defined in (10) and (11) assigns different global features to any pair of graphs \(G_{1},G_{2}\in\mathcal{G}_{\leq n}(\Sigma)\) that can be separated by \(T\) iterations of 1-WL._

Proof.: The message passing iterations discussed in the theorem define new node features \(\boldsymbol{h}_{v}^{(t)}\) from the previous node features \(\boldsymbol{h}_{v}^{(t-1)}\) via

\[\boldsymbol{h}_{v}^{(t)}=\sum_{u\in\mathcal{N}(v)}\sigma\left(\boldsymbol{A} ^{(t)}\left(\eta^{(t)}\boldsymbol{h}_{v}^{(t-1)}+\boldsymbol{h}_{u}^{(t-1)} \right)+\boldsymbol{b}^{(t)}\right).\]

This can be rewritten as

\[\boldsymbol{m}_{v,u}^{(t)} =\eta^{(t)}\boldsymbol{h}_{v}^{(t-1)}+\boldsymbol{h}_{u}^{(t-1)}\] \[\boldsymbol{h}_{v}^{(t)} =f^{(t)}\left(\{\!\{\boldsymbol{h}\boldsymbol{m}_{v,u}^{(t)},|\ u \in\mathcal{N}(v)\}\!\}\right)=\sum_{u\in\mathcal{N}(v)}\sigma\left(\boldsymbol {A}^{(t)}\boldsymbol{m}_{v,u}^{(t)}+\boldsymbol{b}^{(t)}\right),t=1.\ldots,T\]

The final'readout' step creates a global graph features from the node features of the last iteration \(T\) via

\[\boldsymbol{h}_{G}=f^{(T+1)}\left(\{\!\{\boldsymbol{h}_{v}^{(T)}\ |\ \ v\in V \}\!\}\right)=\sum_{v\in V}\sigma\left(\boldsymbol{A}^{(T+1)}\boldsymbol{h}_{ v}^{(T)}+\boldsymbol{b}^{(T+1)}\right)\]

It is known [44] that every pair of graphs \(G,\hat{G}\in\mathcal{G}_{\leq n}(\Sigma)\) which are not be separated by \(T\) iterations of the WL test, will not be separated by our message passing procedure, regardless of the choice of the parameters

\[\boldsymbol{\theta}=\left(\boldsymbol{A}^{(1)},\ldots,\boldsymbol{A}^{(T+1)}, \boldsymbol{b}^{(1)},\ldots,\boldsymbol{b}^{(T+1)},\eta^{(1)}\ldots,\eta^{(T)} \right).\]

[MISSING_PAGE_FAIL:33]

Numerical Experiments: Additional Details

### Empirical injectivity and Bi-Lipschitzness

To generate the results shown in Figure 2 (and Figure 4 below), we ran multiple independent test instances in which we generated two random matrices \(X_{1},X_{2}\in\mathbb{R}^{d\times n}\), representing two sets of \(n\) vectors in \(\mathbb{R}^{d}\). With exact details appearing below, \(X_{1},X_{2}\) were generated such that: (1) Each entry of \(X_{1}\) and \(X_{2}\) has expectation zero and a standard deviation (STD) of \(1\); (2) \(X_{1}\) and \(X_{2}\) differ in exactly \(n_{\Delta}\) randomly chosen columns, with the parameter \(n_{\Delta}\) chosen uniformly at random from \(\{1,\ldots,n\}\); (3) each entry of the \(n_{\Delta}\) nonzero columns of \(\Delta X=X_{2}-X_{1}\) has expectation zero and STD=\(\rho\), with the parameter \(\rho\) itself drawn uniformly (once per test instance) from \([\rho_{\min},\rho_{\max}]\). We used \(\rho_{\min}=0.02\), \(\rho_{\max}=1\). In other words, the relative difference \(\rho\) between non-identical columns of \(X_{1}\) and \(X_{2}\) is chosen at each instance randomly between \(2\%\) and \(100\%\). The motivation for this construction was to test various types and magnitudes of differences between multisets.

We then randomly generated \(\bar{\bm{A}}\in\mathbb{R}^{\bar{m}\times d}\) and \(\bar{\bm{b}}\in\mathbb{R}^{\bar{m}\times 1}\), from which we took subblocks to be used as parameters for the function \(f\left(x;\bm{A},\bm{b}\right)\) of Equation (3): for various values of \(m\in[\bar{m}]\), we took \(\bm{A}_{m}\), \(\bm{b}_{m}\) to be the top \(m\) rows of \(\bar{\bm{A}}\) and \(\bar{\bm{b}}\) respectively, and used them to construct the embedding \(\hat{f}\left(X;\bm{A}_{m},\bm{b}_{m},\sigma\right):\mathbb{R}^{d\times n} \rightarrow\mathbb{R}\):

\[\hat{f}\left(X;\bm{A}_{m},\bm{b}_{m},\sigma\right)=\sum_{i=1}^{n}\sigma\left( \bm{A}_{m}x_{i}+\bm{b}_{m}\right),\]

with \(x_{i}\) denoting the columns of \(X\). The entries of \(\bar{\bm{A}}\) and \(\bar{\bm{b}}\) were drawn from Gaussian distributions chosen such that for each row \(a_{k}\) of \(\bar{\bm{A}}\) and corresponding entry \(b_{k}\) of \(\bm{b}\), and each column \(x_{i}\) of \(X_{1}\) or \(X_{2}\), the input to the activation \(\sigma\), \(a_{k}\cdot x_{i}+b_{k}\), has expectation zero and STD=1; specifically,

\[\mathbb{E}\left[a_{k}\cdot x_{i}\right]=\mathbb{E}\left[b_{k}\right]=0\quad \text{and}\quad\text{STD}\left[a_{k}\cdot x_{i}\right]=\text{STD}\left[b_{k} \right]=\tfrac{1}{\sqrt{2}}.\]

For various activation functions \(\sigma\), we calculated the ratio

\[r\left(X_{1},X_{2}\right)=\frac{\|\hat{f}\left(X_{1};\bm{A}_{m},\bm{b}_{m}, \sigma\right)-\hat{f}\left(X_{2};\bm{A}_{m},\bm{b}_{m},\sigma\right)\|_{2}}{ W_{2}\left(X_{1},X_{2}\right)},\]

with \(W_{2}\left(\cdot,\cdot\right)\) being the \(2\)-Wasserstein distance. We used a Sinkhorn approximation of \(W_{2}\left(\cdot,\cdot\right)\), calculated by the GeomLoss Python library [11]. Finally, for each activation \(\sigma\) and embedding dimension \(m\), we took \(c\) and \(C\) to be the minimum and maximum of \(r\left(X_{1},X_{2}\right)\) respectively over all test instances, and recorded the ratio \(c/C\). In each experimental setting, we ran between 500,000 and 2 million independent instances, depending on the values of \(d\) and \(n\). The results appear in Figure 4. It can be seen that a similar behaviour is exhibited across different values of \(d\) and \(n\). In particular, all PwL activations have \(c/C=0\) at low \(m\) and all analytic activations have positive \(c/C\) in all settings tested.

Probabilistic distributions of dataAt each instance, as mentioned above, we first chose \(n_{\Delta}\sim\text{Uniform}\left[\{1,\ldots,n\}\right]\) and \(\rho\sim\text{Uniform}\left[\rho_{\min},\rho_{\max}\right]\). We then randomly chose \(n_{\Delta}\) columns labelled by \(J\), at which \(X_{1}\), \(X_{2}\) should differ. Let \(I=[n]\setminus J\). Denote by \(X[:,\Lambda]\) the subblock of the matrix \(X\) with columns indexed by \(\Lambda\). The entries of \(X_{1}[:,I]=X_{2}[:,I]\) were drawn i.i.d. from Normal \((0,1)\). For \(X_{1}[:,J]\) and \(X_{2}[:,J]\), we generated two random matrices \(U,V\in\mathbb{R}^{d\times n_{\Delta}}\), each of whose entries are i.i.d. Gaussian with expectation 0, STD=\(\sqrt{1-\tfrac{1}{12}}\left(\rho_{\max}^{2}+\rho_{\max}\rho_{\min}+\rho_{\min} ^{2}\right)\) and STD=1 for \(U\),\(V\) respectively. We then set

\[X_{1}[:,J]=U-\tfrac{1}{2}\rho V,\quad X_{2}[:,J]=U+\tfrac{1}{2}\rho V.\]

Lastly, we generated the entries of \(\bar{\bm{A}}\) and \(\bar{\bm{b}}\) from Normal \(\left(0,\tfrac{1}{\sqrt{2d}}\right)\) and Normal \(\left(0,\tfrac{1}{\sqrt{2}}\right)\) respectively. We now show that: (i) each entry of \(X_{1}[:,J]\) and \(X_{2}[:,J]\) has expectation zero and STD=1, (ii) given \(\rho\), each entry of \(\Delta X[:,J]\) has expectation 0 and STD=\(\rho\), and (iii) All inputs to the activation \(\sigma\) have expectation zero and STD=1. Let \(x_{1},x_{2}\) be two corresponding entries of \(X_{1}[:,J]\), \(X_{2}[:,J]\), and let \(u,v\) be their corresponding entries of \(U,V\). Then

\[x_{1}=u-\tfrac{1}{2}\rho v,\quad x_{2}=u+\tfrac{1}{2}\rho v.\]We now calculate the expectation and variance of \(x_{1}\), \(x_{2}\). Since \(U\), \(V\) and \(\rho\) are independent, we have that

\[\mathbb{E}\left[x_{1}\right]=\mathbb{E}\left[u-\tfrac{1}{2}\rho v\right]= \mathbb{E}\left[u\right]-\tfrac{1}{2}\mathbb{E}\left[\rho\right]\mathbb{E} \left[v\right]=0-\tfrac{1}{2}\frac{\rho_{\min}+\rho_{\max}}{2}\cdot 0=0,\]

and by a similar reasoning, \(\mathbb{E}\left[x_{2}\right]=0\). The variance of \(x_{1}\) is given by:

\[\text{Var}\left[x_{1}\right] =\text{Var}\left[u-\tfrac{1}{2}\rho v\right]=\text{Var}\left[u \right]+\tfrac{1}{4}\left(\text{Var}\left[\rho\right]+\mathbb{E}\left[\rho \right]^{2}\right)\left(\text{Var}\left[v\right]+\mathbb{E}\left[v\right]^{2} \right)-\mathbb{E}\left[\rho\right]^{2}\mathbb{E}\left[v\right]^{2}\] \[=\text{Var}\left[u\right]+\tfrac{1}{4}\left(\frac{\left(\rho_{ \max}-\rho_{\min}\right)^{2}}{12}+\frac{\left(\rho_{\max}+\rho_{\min}\right)^ {2}}{4}\right)\left(1+0\right)-\mathbb{E}\left[\rho\right]^{2}\cdot 0\] \[=\text{Var}\left[u\right]+\tfrac{1}{12}\left(\rho_{\max}^{2}+\rho _{\max}\rho_{\min}+\rho_{\min}^{2}\right)\] \[=\left(1-\tfrac{1}{12}\left(\rho_{\max}^{2}+\rho_{\max}\rho_{\min }+\rho_{\min}^{2}\right)\right)+\tfrac{1}{12}\left(\rho_{\max}^{2}+\rho_{\max} \rho_{\min}+\rho_{\min}^{2}\right)=1.\]

By a similar argument, \(\text{Var}\left[x_{2}\right]=1\). Thus, (i) holds. Let \(\Delta x=x_{2}-x_{1}=\rho v\). Then

\[\mathbb{E}\left[\Delta x\right]=\mathbb{E}\left[x_{2}\right]-\mathbb{E}\left[ x_{1}\right]=0-0=0.\]

Moreover, the conditional variance of \(\Delta x\) given \(\rho\) is

\[\text{Var}\left[\Delta x\mid\rho\right]=\text{Var}\left[\rho v\mid\rho\right] =\rho^{2}\cdot\text{Var}\left[v\mid\rho\right]=\rho^{2}\cdot\text{Var} \left[v\right]=\rho^{2},\]

and thus (ii) holds. Finally, we show that (iii) holds. We already have established that each entry of \(X_{1}\) and of \(X_{2}\) has zero mean and STD=1. Let \(a_{k}\), \(b_{k}\) be a row or \(\bar{\bm{A}}\) and its corresponding entry of \(\bar{\bm{b}}\). Let \(x_{i}\) be an arbitrary column of \(X_{1}\) or \(X_{2}\). Then

\[\mathbb{E}\left[a_{k}\cdot x_{i}\right]=\mathbb{E}\left[\sum_{j=1}^{d}\left(a_ {k}\right)_{j}\cdot\left(x_{i}\right)_{j}\right]=\sum_{j=1}^{d}\mathbb{E} \left[\left(a_{k}\right)_{j}\right]\mathbb{E}\left[\left(x_{i}\right)_{j} \right]=\sum_{j=1}^{d}0\cdot 0=0,\]

Figure 4: Empirical injectivit and bi-Lipschitzness

and by definition \(\mathbb{E}\left[b_{k}\right]=0\). Since each \(\left(a_{k}\right)_{j}\) and \(\left(x_{i}\right)_{j}\) are independent random variables with expectation zero, we have that

\[\text{Var}\left[a_{k}\cdot x_{i}\right]=\text{Var}\left[\sum_{j=1}^{d}\left(a_{k }\right)_{j}\cdot\left(x_{i}\right)_{j}\right]=\sum_{j=1}^{d}\text{Var}\left[ \left(a_{k}\right)_{j}\right]\text{Var}\left[\left(x_{i}\right)_{j}\right]= \sum_{j=1}^{d}\tfrac{1}{2d}\cdot 1=\tfrac{1}{2},\]

and by definition \(\text{Var}\left[b_{k}\right]=\tfrac{1}{2}\). Therefore, (iii) holds.

Computational resourcesAll experiments were run on an NVidia A40 GPU with 48 GB of GPU memory.

### Graph separation

We ran the graph separation experiment using the PyTorch Geometric [10] implementation for 'WL convolutions', and the GCN convolutions of [18], as well as their version of the TUDataset [30]. As initialization for the node features, we only took the node degree.

When moving from the abstract mathematical world to finite precision computing, features that are mathematically equal \(h_{i}=h_{j}\) could end up having slightly different values. To deal with this, all computations were done in double precision. The final features computed by the MPNN were normalized to have an average norm of one, and two features \(h_{i},h_{j}\) were deemed equal if \(\left|h_{i}-h_{j}\right|<10^{-12}\). In Figure 5 we show the minimum and median of the quantity \(\left\|h_{i}-h_{j}\right\|\) over all \((i,j)\) from all graphs for which this norm was larger than the threshold of \(10^{-12}\). This is shown for the SiLU activation, and the values are shown as a function of the hidden dimension used. We see that the minimal non-zero distance was safely larger than the threshold in all examples. Also note that the minimal distance moderately improves as the hidden dimension increases.

Figure 5: Minimal and median distance between non-equivalent features as a function of hidden feature dimension.