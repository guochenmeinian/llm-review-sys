# Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem

Declan Campbell

Princeton Neuroscience Institute

Sunayana Rane

Department of Psychology, Princeton University

Tyler Giallanza

Department of Computer Science, EPFL

Nicolo De Sabbata

Department of Computer Science, Dartmouth College

Kia Ghods

Princeton Neuroscience Institute

Amogh Joshi

Princeton Neuroscience Institute

Alexander Ku

Department of Psychology, Princeton University

Steven M. Frankland

Department of Cognitive Science, Dartmouth College

Thomas L. Griffiths

Jonathan D. Cohen

Taylor Webb

Department of Computer Science, Princeton University

###### Abstract

Recent work has documented striking heterogeneity in the performance of state-of-the-art vision language models (VLMs), including both multimodal language models and text-to-image models. These models are able to describe and generate a diverse array of complex, naturalistic images, yet they exhibit surprising failures on basic multi-object reasoning tasks - such as counting, localization, and simple forms of visual analogy - that humans perform with near perfect accuracy. To better understand this puzzling pattern of successes and failures, we turn to theoretical accounts of the _binding problem_ in cognitive science and neuroscience, a fundamental problem that arises when a shared set of representational resources must be used to represent distinct entities (e.g., to represent multiple objects in an image), necessitating the use of serial processing to avoid interference. We find that many of the puzzling failures of state-of-the-art VLMs can be explained as arising due to the binding problem, and that these failure modes are strikingly similar to the limitations exhibited by rapid, feedforward processing in the human brain.

## 1 Introduction

Recent progress in training large-scale neural networks on internet-scale datasets has led to the creation of AI systems with capabilities rivaling human performance across a broad range of complex tasks. Most recently, this has given rise to an array of vision language models (VLMs), including multimodal language models such as GPT-4v that can generate text descriptions of multimodal text and image inputs [1], and text-to-image models such as DALL-E 3 that can generate images from natural language descriptions [24]. However, despite the considerable success of VLMs across many tasks, these models still perform poorly on several surprisingly simple multi-object reasoning tasks - such as counting [23, 25, 40], relational image generation [7], relational scene understanding [15, 31], and simple visual analogy tasks [20, 38] - on which humans achieve near perfect accuracy.

Drawing from theoretical work both in cognitive science and neuroscience, we turn to the _binding problem_[9, 10, 29, 33, 36] as a potential explanation for these limitations. 'Binding' refers to the ability to associate one feature of an object (e.g., its color) with the other features of that object (e.g., its shape and location), and the 'binding problem' refers to the question of how the brain accomplishes this without interference between the features for different objects. It is widely recognized that thehuman visual system relies on serial processing to solve this problem, iteratively directing attention to individual objects so as to avoid interference [28; 33], and that binding errors arise when it is forced to rely on rapid, parallel visual processing [14; 19; 33]. For example, when human participants are not able to effectively deploy serial processing (e.g., because attention is overloaded, or because speeded judgments are required), they are susceptible to so-called _illusory conjunctions_ (e.g., mistakenly identifying a red square in an image that contains a green square and a red circle) [32].

In this work, we test the hypothesis that the failures exhibited by VLMs on multi-object reasoning tasks are due to representational interference resulting from an inability to manage the binding problem. We first investigate two classic tasks from the cognitive science literature, visual search [33] and numerical estimation [14; 19] (i.e., counting), finding that a wide range of VLMs (including 5 multimodal language models and 4 text-to-image models) exhibit stark capacity constraints similar to those displayed by human observers when forced to make speeded responses. Importantly, although these effects are more pronounced for scenes with more objects, they cannot be explained entirely as a function of the number of objects in a scene. Instead, we find that performance is best explained by the probability of interference given the specific distribution of features and their conjunctions within a scene. Motivated by this observation, we develop a novel scene description benchmark that systematically varies the likelihood of interference, finding that this quantity is highly predictive of binding errors.

We also apply these insights to better understand the limitations of VLMs in visual analogy tasks, introducing a simple input pre-processing technique to reduce the potential for representational interference. We show that this technique improves the performance of VLMs on the task, suggesting that their original limitation on this task may be due to a more basic difficulty with processing multi-object scenes, rather than an inability to process relations. Finally, we discuss the normative factors that underlie the binding problem [2; 9; 22], highlighting the role of compositional representations, which are useful for generalization, but introduce the potential for interference when shared representations are used to process multiple objects at the same time. We argue that, surprisingly, the binding failures exhibited by VLMs imply the presence of compositional representations. Overall, these results highlight the usefulness of cognitive science in helping to understand the limits of large-scale generative models, and suggest the presence of a common set of principles that govern information processing in both artificial systems and human cognition.

## 2 Visual Search

Extensive prior work in cognitive psychology has tested how people process scenes involving multiple objects and under what conditions their performance degrades. These studies demonstrate that performance is not driven solely by the number of objects present in a scene, but also depends on the likelihood of interference among objects given the specific distribution of features and feature conjunctions from which they are composed. This can be seen most directly in research on visual search, where participants are typically tasked with identifying a specific object within a multi-object array. A classic pattern of results arises from a comparison of two conditions: disjunctive and conjunctive search [33]. In disjunctive search (depicted on the left side of Figure 1), the array consists of distractor objects that share one feature with the target (e.g., the distractors are all circles) but differ in a second feature (e.g., the distractors are all _red_ circles in the 2D task variant). Since one of the feature values (the color green) is uniquely assigned to the target object, the distractors present little interference and therefore task performance is invariant to the number of distractors. This condition is therefore sometimes referred to as "popout" search, as the target immediately stands out from the distractors, and the task can thus be performed rapidly without the need for serial processing. Conversely, in conjunctive search (depicted in the middle of Figure 1), there are two types of distractor objects that each share one feature with the target (e.g., half of the distractors are red L-shapes and the other half are green T-shapes). In this case, the target (a green L-shape) possesses no unique feature that easily distinguishes it from the distractors, leading to a significant degree of interference between the distractors and the target. One way to mitigate this is the use of serial search to identify the target. This is suggested by ubiquitiously observed increases in reaction time as a function of the number of distractors, as well as the observation that when participants are prevented from engaging in serial search (e.g., by forcing participants to respond quickly), task performance degrades rapidly as more objects are added to the scene.

### Methods

We tested the extent to which VLMs demonstrate similar capacity constraints to humans in visual search tasks. We evaluated four multimodal language models - GPT-4v, GPT-4o, Gemini Ultra 1.5, and Claude Sonnet 3.5 - on a task involving disjunctive and conjunctive search conditions.1We generated datasets involving either 2D sprites or 3D scenes created in Blender [6] (similar to those found in the CLEVR dataset [13]). The datasets were designed to evaluate the ability of the model to detect the presence of a target object among multiple distractors. In half of the images, a target was present, while in the other half, no target was present.

Footnote 1: We also evaluated an open-source multimodal language model – Llava 1.5 – but performance was very low for these tasks. These results are presented in Supplementary Figure 6.

Each image contained between 4 and 50 distractors. For the disjunctive search task, these consisted of non-overlapping red circles (for the 2D dataset) or green spheres (for the 3D dataset) of a uniform size. Half of the images additionally contained a target object, which was a green circle (for the 2D dataset) or a red sphere (for the 3D dataset). For the conjunctive search task, the 2D dataset consisted of images in which the distractors were either red L-shapes or green T-shapes (randomly selected with equal probability). Half of the images additionally contained a target object, which was a green L-shape. The 3D dataset consisted of images in which the distractors were either green spheres or red cubes (randomly selected with equal probability). Half of the images additionally contained a target object, which was a red sphere. Each of the datasets (2D disjunctive, 2D conjunctive, 3D disjunctive, 3D conjunctive) contained 1000 images.

### Results

We measured the performance of each model by calculating, for each condition, how detection accuracy varied as a function of the number of distractors2. The results indicate that performance in the disjunctive search (i.e., popout) condition was perfect, and invariant to the number of distractors. That is, regardless of the number of distractors, all models achieved perfect accuracy in this condition.

Figure 1: **Visual search tasks and results. Example trials for the 2D (top) and 3D (bottom) variants of the disjunctive (left/red column) and conjunctive (middle/blue column) search conditions. Performance for 2D and 3D task variants are plotted on the right. Results reflect aggregate performance for all four VLMs (GPT-4v, GPT-4o, Gemini Ultra 1.5, and Claude Sonnet 3.5; see Supplementary Figure 5 for separate model results). Error bars denote 95% binomial confidence intervals.**In contrast, in the conjunctive search condition performance was inversely related to the number of objects: for 5 objects, all models displayed an accuracy of \(\sim\)90%, but as the number of objects increased, performance dropped substantially. These results were consistently observed for both the 2D (top panel of Figure 1) and 3D (bottom panel of Figure 1) datasets. These results were also replicated in an alternative version of the disjunctive search task, in which target and distractor colors were varied between trials (Supplementary Figure 7).

The results of these experiments suggest that multimodal language models demonstrate human-like capacity constraints in their ability to perform visual search in multi-object settings. It is important to emphasize that these capacity constraints are not driven solely by the number of objects present within a scene. Like humans, these models demonstrate capacity constraints only in the task conditions that are impacted by interference between the target and distractor objects, consistent with the hypothesis that these capacity constraints arise as a consequence of the binding problem.

## 3 Numerical Estimation

To assess the generality of the human-like capacity constraints observed for VLMs in visual processing, we investigated a simple numerical estimation task (i.e., counting) that has been widely studied in cognitive psychology. Although human observers can precisely count a very large number of items when allowed to explicitly process those items one at a time, their ability to rapidly estimate the number of items in a display is subject to a severe capacity constraint. Studies have found that the number of objects that can be reliably estimated without explicit serial counting (sometimes referred to as "subitizing") is somewhere between 4 and 6 [14; 17; 19; 27; 34]. To determine whether VLMs are subject to similar constraints, we evaluated both multimodal language models (GPT-4v, GPT-4o, Gemini Ultra 1.5, Claude Sonnet 3.5, and Llava 1.5) and text-to-image models (Stable Diffusion Ultra, DALL-E 3, Google Parti, and Google Muse) on a numerical estimation task involving variations of both the number and type of objects. We found that VLMs, across a variety of stimulus and model types, display strikingly similar quantitative capacity limits to those observed in human vision. We also found that these capacity constraints were strongly affected by the variability of features present in an image. This effect is consistent with the hypothesis that these constraints arise due to representational interference: given that objects are represented with a shared set of representational resources, greater feature variability leads to less overlap in the use of these resources, and therefore less opportunities for interference and binding errors.

### Methods

We generated datasets involving both 2D sprites and 3D objects, varying the number of objects per image between 1 and 20. We explored four conditions with varying levels of feature entropy (i.e., feature variability): a low-entropy condition in which all objects in an image had the same color and shape; two medium-entropy conditions in which all objects in an image had the same shape but unique colors, or vice versa; and a high-entropy condition in which all objects in an image had unique colors and shapes. We prompted the multimodal language models to describe the image and then state the number of objects present in it. To test the text-to-image models, we generated a dataset comprising 100 distinct categories, evenly split between common foods (50 categories) and animals (50 categories). We tasked these models with generating images from each category, for which the number of instances of each object ranged from 1 to 10. To assess their ability to generate images with the exact number of objects requested, we conducted a human evaluation study. Participants were asked to count and report the number of objects visible in each generated image. The collected human judgments were then used to quantify the model's accuracy. See Appendix C for further details on human evaluations.

### Results

We measured performance by calculating, for each condition, how accuracy varied with the number of objects present in the scene. The results indicated that, regardless of the type of stimuli used (2D vs. 3D shapes, or animals vs. food), and across two fundamentally different types of vision language model (multimodal language models and text-to-image models), VLMs displayed human-like capacity limits (Figure 2). For both multimodal language models and T2I models, accuracy was very high for scenes involving a relatively small number of objects (1-5), but dropped sharply for scenes involving6 or more objects. Moreover, the multimodal language models exhibited performance consistent with our hypothesis that capacity limits arise due to representational interference across objects (i.e., the binding problem), with overall performance highest in the high-entropy condition (lowest interference), lowest in the low-entropy condition (highest interference), and intermediate in between these two extremes in the medium-entropy conditions. Though there are slight differences between the capacity limits exhibited by these two classes of models, it is striking that they both fall within the subitizing limit of human vision, especially when considering the significant differences in both architecture and training procedures. Furthermore, the effect of feature entropy on these capacity limits is also evident in the high-entropy case.

Figure 2: **Numerical estimation tasks and results.** Top left: Examples of images generated by text-to-image (T2I) models for different numbers and categories of objects. Top right: Performance of T2I models as a function of the number and category of objects. Results reflect an aggregate of four models (Stable Diffusion Ultra, DALL-E 3, Google Parti, and Google Muse). Bottom left: Examples of images (featuring either 2D or 3D objects) used to evaluate numerosity estimation. Feature entropy was varied in four conditions (low entropy, high entropy, and two medium entropy conditions). Bottom middle: Numerosity estimation results for four multimodal language models (GPT-4v, GPT-4o, Gemini Ultra 1.5, Claude Sonnet 3.5; see Supplementary Figure 6 for results with Llava-1.5). Bottom right: Numerosity estimation results plotted as a function of the number of objects in an image, aggregated across all four models (see Supplementary Figure 8 for individual model results). Error bars for all plots reflect 95% binomial confidence intervals.

limits strongly suggests that they are driven by representational interference, arising due an inability to manage the binding problem. To further investigate this hypothesis, we next turned to a novel scene description benchmark that allowed us to systematically investigate the likelihood of representational interference, thus enabling a direct test of the extent to which this factor is responsible for the shortcomings of VLMs.

## 4 Scene Description

Theoretical accounts of the binding problem [9; 33] posit that capacity limits in rapid visual processing arise as a consequence of interference between representations. Given a scene containing multiple objects, and a set of shared features with which to represent those objects, the likelihood of interference will tend to increase as a function of the number of objects in the scene (without the availability of a mechanism for binding features together, e.g., serial processing). However, as emphasized in our experiments on visual search and numerosity estimation, interference is not driven solely by the number of objects, but is also strongly influenced by the specific feature conjunctions present within a scene.

We developed a novel scene description task to further investigate the extent to which VLM performance is driven by representational interference. The task is illustrated in Figure 2(a). For each image, the likelihood of representational interference was quantified as the number of _feature triplets_ present in that image. A feature triplet is defined as any set of three objects for which one pair shares a feature, and another pair shares a different feature. For instance, {green X, green triangle, yellow triangle} is a feature triplet, because the feature 'green' is shared by two objects (the green X and the green triangle), and the feature 'triangle' is shared by two objects (the green triangle and the yellow triangle). Without the ability to accurately bind these features together at the level of objects, such feature triplets create opportunities for representational interference, and thus lead to illusory conjunctions. For instance, the feature triplet {green X, green triangle, yellow triangle} may lead to the erroneous identification of a yellow X. We studied the extent to which the presence of such feature triplets can account for scene description performance in VLMs.

### Methods

As in the previous tasks, we generated datasets involving either 2D sprites or 3D objects. Each scene contained a variable number of objects (10-15 objects for the 2D dataset and 8-12 objects for the 3D dataset), and we systematically varied the number of feature triplets present in each scene. For example, the scene depicted in Figure 2(a) contains three feature triplets (illustrated by the dashed lines). VLMs (GPT-4v, GPT-4o, Gemini Ultra 1.5, and Claude Sonnet 3.5) were prompted to provide a description of the objects in JSON format (see Appendix B for more details). We also generated prompts describing similar scenes (but involving real-world objects) and tested the ability of the T2I models (Stable Diffusion Ultra, DALL-E 3, Google Parti, and Google Muse) to accurately generate these scenes (as assessed by human evaluation; see Appendix C for more details). To obtain a representative sampling of scenes with different triplet counts, we systematically varied the diversity of colors and shapes across trials. This approach ensured adequate sampling of trials with different feature combinations and their associated triplet counts. To ensure reliable performance estimates, we excluded from analysis any triplet counts represented by fewer than 20 trials across all conditions. For the T2I experiments, we additionally excluded trials where models generated more than three extraneous objects not specified in the prompt, as these represented significant deviations from the intended scene structure.

### Results

We measured scene description performance by calculating how the number of errors (quantified as the edit distance between the true description of the scene and the model's description of the scene) varies as a function of the number of objects present in the scene, and the number of feature triplets. The results (Figure 3) confirmed our prediction that performance should vary as a function of the number of triplets. Across multiple stimulus types (2D and 3D objects), and model types (both multimodal language models and text-to-image models), the largest number of errors occurred in the trials where the risk of binding errors was highest (i.e., the trials with the largest number of feature triplets), consistent with the hypothesis that errors would be driven primarily by the formation of illusory conjunctions. These results also showed a pattern of increasing errors as a function of the number of objects, consistent with a general capacity limit on the number of objects that can be accurately represented at the same time. Overall, these results suggest that the capacity limits displayed by VLMs are best explained by an inability to manage the binding problem. In the next section, we apply these insights to better understand the limited visual reasoning capabilities of VLMs.

## 5 Visual Analogy

An open question in studying the performance of VLMs is the extent to which these models can solve analogical reasoning tasks. These tasks are of particular interest given their centrality in human higher-order cognition [11] and their use as measures of human intelligence [30]. Recent work has demonstrated that LLMs have an impressive ability to solve a range of text-based analogical reasoning tasks [37], but initial tests of VLMs have suggested that they often struggle to solve comparable visual forms of these tasks, sometimes performing well below human participants [20; 38].

This leads to the question of why, given the success of LLMs on text-based problems in this domain, VLMs do not display comparable success in solving analogy tasks. One possible explanation,

Figure 3: **Scene description task and results.** A) Example image used in 2D scene description task, illustrating the concept of _feature triplets_: sets of three objects where one pair of objects shares a feature, and another pair shares a different feature. This example contains three feature triplets, demarcated by the dashed lines. 3D scenes were also investigated. B) Scene description results for text-to-image (T2I) models) as a function of the number of feature triplets. C) 2D scene description results for multimodal language models as a function of the number of feature triplets. Left panel illustrates the results aggregated across four models (GPT-4v, GPT-4o, Gemini Ultra 1.5, and Claude Sonnet 3.5). Right panel illustrates the results aggregated across scenes with different numbers of objects. D) 3D scene description results. Error bars represent the standard error of the mean.

suggested by our investigation of the binding problem, is that the difficulty displayed by VLMs on visual reasoning tasks may stem from a more basic difficulty with processing multi-object scenes. In other words, regardless of whether VLMs have the capacity for abstract reasoning necessary to solve analogy tasks, they will likely struggle to solve _visual_ analogy tasks simply because these tasks involve processing multi-object scenes. To distinguish between these two failure modes (inability to process abstract relations vs. inability to process multi-object scenes), we performed experiments using a visual analogy task in which the visual processing demands were explicitly manipulated, and measured the ability of multimodal language models to perform both abstract relational tasks and basic object-level tasks.

### Methods

We generated 200 trials from a simple relational match-to-sample (RMTS) task (Figure 4) using the same 2D sprite stimuli from the previous experiments. We selected a subset of 8 easily recognizable shapes and colors to generate a set of 64 stimuli. For each trial, we chose the source pair by sampling two objects that shared at least one of the two feature dimensions. We then selected the target pairs by sampling two pairs of objects: one which matched the source pair exactly along its relations (the correct target) and one which shared only one of the relations with the source pair (the incorrect target). We manipulated visual processing demands by investigating two conditions, one in which the source and target pairs were presented in a single image (the "unified" condition), and one in which the source and target pairs were presented as separate images presented in sequence (the "decomposed" condition), thereby reducing the chance of binding errors.

We assessed the performance of four VLMs (GPT-4v, GPT-4o, Gemini Ultra 1.5, and Claude Sonnet 3.5) in four tasks: identification of the correct target pair in the full RMTS task (Analogy), decoding of single features from specific individual objects (Single Feature Decoding Task), comprehensive decoding of all features in a given problem (Full Feature Decoding), and decoding of relations between object pairs (Relation Decoding).

### Results

We found that performance on this task was highly variable across VLMs, with some models (Claude Sonnet) showing nearly perfect performance on all tasks (Table 3) and other models (Gemini Ultra) showing poor performance on most tasks (Table 4). These results are consistent with recent work showing mixed success on visual analogy problems [38], and at odds with work claiming that VLMs have no capacity for visual analogy [20]. Interestingly, we also found that many models also struggled on more basic tasks such as identifying the features of the objects present in the image, or identifying the relations for individual pairs of objects.

Figure 4: **Visual analogy task. The Unified and Decomposed conditions present the same object pairs, but in the Decomposed condition it is broken up across three images. The correct target pair must share both relations (shape and color) with the source pair, so the correct answer in this example is Target Pair 2 because it satisfies both the ‘same shape’ and ‘different color’ relations.**

Most importantly, we found that performance was significantly improved in the Decomposed condition (involving separate images for each pair of objects) as compared with the Unified condition (involving a single image with all object pairs). This was the case across all tasks, and for all models, except for the cases in which performance was already at ceiling for the Unified condition. Taken together, these results suggest that the poor performance of VLMs on visual analogy tasks may be a consequence of a more general limitation with processing multi-object scenes, arising due to an inability to manage the binding problem.

## 6 Discussion

We have presented a series of experiments aimed at understanding the limits of vision language models in processing multi-object scenes. Our results suggest that these limitations can all be understood as arising from an inability to manage the binding problem, a fundamental problem associated with compositional coding identified by classic work in cognitive science [10, 33].

Recent theoretical work has formalized this problem within a normative framework [9], suggesting that it arises due to a tension between the learning of compositional representations, and the shared use of such representations to encode multiple objects at the same time. To illustrate this, consider two different schemes for representing multi-object scenes: a _conjunc

\begin{table}
\begin{tabular}{l c c c c} \hline  & \multicolumn{2}{c}{Unified Accuracy} & \multicolumn{2}{c}{Decomposed Accuracy} \\  & Accuracy & 95\% CI & Accuracy & 95\% CI \\ \hline Analogy & 91\% & (86\%, 94\%) & **99\%** & (97\%, 99\%) \\ Relation decoding & 80\% & (73\%, 84\%) & **91\%** & (86\%, 94\%) \\ Full feature decoding & 85\% & (79\%, 89\%) & **100\%** & (100\%, 100\%) \\ Single feature decoding & 95\% & (92\%, 98\%) & 98\% & (96\%, 99\%) \\ \hline \end{tabular}
\end{table}
Table 1: **Visual analogy results: GPT-4.**

\begin{table}
\begin{tabular}{l c c c c} \hline  & \multicolumn{2}{c}{Unified Accuracy} & \multicolumn{2}{c}{Decomposed Accuracy} \\  & Accuracy & 95\% CI & Accuracy & 95\% CI \\ \hline Analogy & 99\% & (96\%, 100\%) & 100\% & (100\%, 100\%) \\ Relation decoding & 88\% & (82\%, 92\%) & **98\%** & (95\%, 99\%) \\ Full feature decoding & 97\% & (84\%, 100\%) & 100\% & (100\%, 100\%) \\ Single feature decoding & 86\% & (81\%, 91\%) & **94\%** & (81\%, 91\%) \\ \hline \end{tabular}
\end{table}
Table 2: **Visual analogy results: GPT-4.**

\begin{table}
\begin{tabular}{l c c c c} \hline  & \multicolumn{2}{c}{Unified Accuracy} & \multicolumn{2}{c}{Decomposed Accuracy} \\  & Accuracy & 95\% CI & Accuracy & 95\% CI \\ \hline Analogy & 99\% & (96\%, 100\%) & 100\% & (100\%, 100\%) \\ Relation decoding & 88\% & (82\%, 92\%) & **98\%** & (95\%, 99\%) \\ Full feature decoding & 97\% & (84\%, 100\%) & 100\% & (100\%, 100\%) \\ Single feature decoding & 86\% & (81\%, 91\%) & **94\%** & (81\%, 91\%) \\ \hline \end{tabular}
\end{table}
Table 3: **Visual analogy results: Claude Sonnet 3.5.**tive_ scheme, involving dedicated representations for every possible conjunction of features (e.g., \(\{\mathrm{redsquare},\mathrm{greencircle},\mathrm{bluetriangle},\mathrm{green square},\ldots\}\)) vs. a _compositional_ scheme, involving the dynamic combination of shared features across different dimensions (e.g., combining the features \(\{\mathrm{red}\}\) and \(\{\mathrm{triangle}\}\) to represent a red triangle). The compositional scheme enables efficient use of finite neural resources, and offers major benefits in terms of generalization (e.g., anything learned about red triangles can then be readily generalized to support inferences about other red objects), but, without a mechanism for dynamically keeping track of the bindings between features, this scheme leads to severe interference, giving rise to the capacity limits observed in cognitive processing. One surprising implication of this perspective is that the binding errors exhibited by VLMs suggest that they have developed compositional representations, perhaps as a consequence of being forced to generalize by their immense and highly diverse training corpora. Without the use of such compositional representations (i.e., if VLMs employed a conjunctive coding scheme), there would be no interference between the representations for different objects, and thus there would be no binding problem. The presence of the binding problem, therefore, implies the presence of compositional representations in VLMs.

It is worth considering how VLMs might be improved so as to enable them to cope with the binding problem. One might think that this could be accomplished through additional fine-tuning on multi-object tasks. However, the theoretical perspective outlined above suggests that, to the extent this alleviates the binding problem, it would do so by eliminating the use of compositional representations, which would then have negative impacts on generalization. The question then is how VLMs might be enhanced to solve the binding problem, while _preserving the benefits of compositional representations_. The most obvious possibility here is to augment VLMs with mechanisms for serial processing of images, of the sort that enable human reasoners to manage the binding problem by selectively attending to individual objects one at a time [22; 33]. A number of methods have been proposed for sequential reasoning over images [12; 35], though none of these methods have yet been deployed at the scale of VLMs. An alternative approach (which may be unique to artificial systems) involves the use of slot-based methods for object-centric representation learning [4; 10; 16], which have been shown to dramatically improve performance in visual reasoning tasks without requiring sequential processing of images [8; 21], but which have also not been scaled to the level of current VLMs. It remains to be seen whether and how these techniques might contribute to improved reasoning in future VLMs, or whether new approaches will be needed to enable human-like visual reasoning.

### Limitations & Future Directions

This study has several limitations that should be considered when interpreting the results. First, we limited our analysis to a relatively small set of tasks. The tasks in our study were selected to illustrate the different settings in which the binding problem may impact performance, while grounding our analysis in well known tasks from cognitive science that have been used to index such capacity constraints in humans. Future work may examine a broader set of tasks such as matrix reasoning tasks [3; 26; 39] that are more diagnostic of the reasoning failures arising due to issues with binding. Second, we primarily investigated proprietary VLMs, for which we do not have detailed knowledge of their training data or architecture, or the ability to directly investigate their internal representations. We chose to focus on these models because they reflect the best-performing current set of VLMs (our experiments with the open-source Llava-1.5 yielded very poor performance on all tasks), but continued progress in the development of open-source VLMs should make it possible to investigate open-source models in future work. Finally, our work is focused particularly on characterizing the capacity constraints of VLMs arising due to issues with feature binding. While we propose a naive approach for improving performance by selectively processing sub-images independently, future work may explore more flexible methods for decomposing complex, multi-object reasoning tasks, especially by exploiting methods for object-centric representation learning [4; 5; 10; 16].

## 7 Acknowledgements

This work was supported in part by Microsoft Azure credits provided to Princeton University. D.C. and T.G. are supported by the National Science Foundation Graduate Research Fellowship Program (NSF GRFP).

## References

* [1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [2] O. Barak, M. Rigotti, and S. Fusi. The sparseness of mixed selectivity neurons controls the generalization-discrimination trade-off. _Journal of Neuroscience_, 33(9):3844-3856, 2013.
* [3] D. Barrett, F. Hill, A. Santoro, A. Morcos, and T. Lillicrap. Measuring abstract reasoning in neural networks. In _International conference on machine learning_, pages 511-520. PMLR, 2018.
* [4] C. P. Burgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. Botvinick, and A. Lerchner. Monet: Unsupervised scene decomposition and representation. _arXiv preprint arXiv:1901.11390_, 2019.
* [5] D. Chen, S. Cahyawijaya, J. Liu, B. Wang, and P. Fung. Subobject-level image tokenization, 2024.
* a 3D modelling and rendering package_. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.
* [7] C. Conwell and T. Ullman. Testing relational understanding in text-guided image generation. _arXiv preprint arXiv:2208.00005_, 2022.
* [8] D. Ding, F. Hill, A. Santoro, M. Reynolds, and M. Botvinick. Attention over learned object embeddings enables complex visual reasoning. _Advances in neural information processing systems_, 34:9112-9124, 2021.
* [9] S. M. Frankland, T. Webb, and J. D. Cohen. No coincidence, george: Capacity-limits as the curse of compositionality. 2021.
* [10] K. Greff, S. Van Steenkiste, and J. Schmidhuber. On the binding problem in artificial neural networks. _arXiv preprint arXiv:2012.05208_, 2020.
* [11] K. J. Holyoak. Analogy and relational reasoning. _The Oxford handbook of thinking and reasoning_, pages 234-259, 2012.
* [12] D. A. Hudson and C. D. Manning. Compositional attention networks for machine reasoning. _arXiv preprint arXiv:1803.03067_, 2018.
* [13] J. Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2901-2910, 2017.
* [14] E. L. Kaufman, M. W. Lord, T. W. Reese, and J. Volkmann. The discrimination of visual number. _The American journal of psychology_, 62(4):498-525, 1949.
* [15] M. Lewis, N. V. Nayak, P. Yu, Q. Yu, J. Merullo, S. H. Bach, and E. Pavlick. Does clip bind concepts? probing compositionality in large image models. _arXiv preprint arXiv:2212.10537_, 2022.
* [16] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Dosovitskiy, and T. Kipf. Object-centric learning with slot attention. _Advances in neural information processing systems_, 33:11525-11538, 2020.
* [17] G. Mandler and B. J. Shebo. Subitizing: an analysis of its component processes. _Journal of experimental psychology: general_, 111(1):1, 1982.
* [18] B. McElree and M. Carrasco. The temporal dynamics of visual search: evidence for parallel processing in feature and conjunction searches. _Journal of Experimental Psychology: Human Perception and Performance_, 25(6):1517, 1999.
* [19] G. A. Miller. The magical number seven, plus or minus two: Some limits on our capacity for processing information. _Psychological review_, 63(2):81, 1956.
* [20] M. Mitchell, A. B. Palmarini, and A. Moskvichev. Comparing humans, gpt-4, and gpt-4v on abstraction and reasoning tasks. _arXiv preprint arXiv:2311.09247_, 2023.
* [21] S. S. Mondal, T. Webb, and J. D. Cohen. Learning to reason over visual objects. _arXiv preprint arXiv:2303.02260_, 2023.

* [22] S. Musslick, A. Saxe, A. N. Hoskin, Y. Sagiv, D. Reichman, G. Petri, and J. D. Cohen. On the rational boundedness of cognitive control: Shared versus separated representations. 2023.
* [23] P. Rahmanzadehegherivi, L. Bolton, M. R. Taesiri, and A. T. Nguyen. Vision language models are blind. _arXiv preprint arXiv:2407.06581_, 2024.
* [24] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.
* [25] S. Rane, A. Ku, J. M. Baldridge, I. Tenney, T. L. Griffiths, and B. Kim. Can generative multimodal models count to ten? In _ICLR 2024 Workshop on Reliable and Responsible Foundation Models_.
* [26] J. C. Raven. _Progressive matrices: A perceptual test of intelligence, individual form_. London: Lewis, 1938.
* [27] S. K. Revkin, M. Piazza, V. Izard, L. Cohen, and S. Dehaene. Does subitizing reflect numerical estimation? _Psychological science_, 19(6):607-614, 2008.
* [28] P. R. Roelfsema. Solving the binding problem: Assemblies form when neurons enhance their firing rate--they don't need to oscillate or synchronize. _Neuron_, 111(7):1003-1019, 2023.
* [29] A. L. Roskies. The binding problem. _Neuron_, 24(1):7-9, 1999.
* [30] R. E. Snow, P. C. Kyllonen, B. Marshalek, et al. The topography of ability and learning correlations. _Advances in the psychology of human intelligence_, 2(S 47):103, 1984.
* [31] T. Thrush, R. Jiang, M. Bartolo, A. Singh, A. Williams, D. Kiela, and C. Ross. Winoground: Probing vision and language models for visio-linguistic compositionality, 2022.
* [32] A. Treisman and H. Schmidt. Illusory conjunctions in the perception of objects. _Cognitive psychology_, 14(1):107-141, 1982.
* [33] A. M. Treisman and G. Gelade. A feature-integration theory of attention. _Cognitive psychology_, 12(1):97-136, 1980.
* [34] L. M. Trick and Z. W. Pylyshyn. Why are small and large numbers enumerated differently? a limited-capacity preattentive stage in vision. _Psychological review_, 101(1):80, 1994.
* [35] M. Vaishnav and T. Serre. Gamr: A guided attention model for (visual) reasoning. _arXiv preprint arXiv:2206.04928_, 2022.
* [36] C. Von Der Malsburg. The correlation theory of brain function. In _Models of neural networks: Temporal aspects of coding and information processing in biological systems_, pages 95-119. Springer, 1994.
* [37] T. Webb, K. J. Holyoak, and H. Lu. Emergent analogical reasoning in large language models. _Nature Human Behaviour_, 7(9):1526-1541, 2023.
* [38] E. Yiu, M. Qraitem, C. Wong, A. N. Majhi, Y. Bai, S. Ginosar, A. Gopnik, and K. Saenko. Kiva: Kid-inspired visual analogies for testing large multimodal models. _arXiv preprint arXiv:2407.17773_, 2024.
* [39] C. Zhang, F. Gao, B. Jia, Y. Zhu, and S.-C. Zhu. Raven: A dataset for relational and analogical visual reasoning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 5317-5327, 2019.
* [40] C. Zhang and S. Wang. Good at captioning, bad at counting: Benchmarking gpt-4v on earth observation data. _arXiv preprint arXiv:2401.17600_, 2024.

[MISSING_PAGE_EMPTY:13]

Figure 8: **Numerosity estimation model results. Individual model performance for 2D and 3D numerosity estimation tasks. Error bars denote 95% binomial confidence intervals.**

Figure 7: **Visual search results with additional control experiment. Results for Claude Sonnet 3.5 on 2D visual search tasks, including disjunctive and conjunctive conditions, and an additional disjunctive search condition (‘Disjunctive Search Control’) in which target and distractor colors were varied between trials.**

Appendix: Prompts for Vision-Language Model Experiments

### Numerical Estimation

1. **2D VLM Prompt** You are presented with an image containing several objects. Your task is to accurately count the number of objects in the image. Follow these instructions carefully: 1. Begin by describing each object in the image. 2. Conclude your response by providing the total count of objects as an integer enclosed in square brackets. Only the number should be enclosed in square brackets.
2. **3D VLM Prompt** You are presented with an image containing several shapes. Your task is to accurately count the number of shapes in the image. Follow these instructions carefully: 1. Begin by describing each shape in the image. 2. Conclude your response by providing the total count of shapes as an integer, enclosed in square brackets. Only the number should be enclosed in square brackets.
3. **T2I Prompt** Render an image with exactly {n} {object_name}, each distinctly separated and easily countable, arranged against a uniform background in photorealistic style.

### Visual Search

1. **2D Disjunctive Search Prompt** You are presented with an image containing several shapes. Your task is to determine if there are any green shapes in the image. Follow these steps carefully: 1. Describe each shape in the image, noting their color. 2. Conclude your response by stating [True] if there are any green shapes, or [ False] if there arenone. Enclose your final answer in square brackets, as shown.
2. **3D Disjunctive Search Prompt** You are presented with an image containing several objects. Your task is to determine if there are any red objects in the image. Follow these steps carefully: 1. Describe each object in the image, noting their color. 2. Conclude your response by stating [True] if there are any red objects, or [ False] if there arenone. Enclose your final answer in square brackets, as shown.
3. **2D Disjunctive Search (Variable Color) Prompt** You are presented with an image containing several shapes. Your task is to determine if all the shapes are the same color. Follow these steps carefully: 1. Describe each shape in the image, noting their color. 2. Conclude your response by stating [True] if all shapes are the same color, or [ False] if there is an "oddball" shape that is a different color. Enclose your final answer in square brackets, as shown.
4. **2D Conjunctive Search** You are presented with an image containing a set of letters, specifically the letters 'L' and 'T'. These letters will appear in either red or green. Your task is to determine if there are any green 'L' in the image. Follow these steps carefully: 1. Describe each shape in the image, noting their color. 2. Conclude your response by stating [True] if the letter 'L' appears in green, or [ False] if there are no green 'L's. Enclose your final answer in square brackets, as shown.

5. 3D Conjunctive Search Prompt

You are presented with an image containing a set of objects, specifically

 spheres and cubes. These objects will appear in either red or green.

 Your task is to determine if there are any redspheres in the image. Follow

 these steps carefully:

1. Describe each object in the image, noting their color.

 2. Conclude your response by stating [True] if a red sphere is present, or [

 False] if there are none. Enclose your final answer in square brackets, as

 shown.

### Scene Description

1. 2D VLM Prompt

The following image contains multiple simple, colored objects.

 The possible shapes that may be present in the image are: <airplane, triangle,

 cloud, X-shape, umbrella, pentagon, heart, star, circle, square, spade,

 scissors, infinity, check mark, right-arrow>.

 The possible colors that may be present in the image are: <red, magenta, salmon

, green, lime, olive, blue, teal, yellow, purple, brown, gray, black, cyan,

 orange>.

 Describe each object in the image in the form of a JSON object detailing the

 color and shape of each item.

 You must answer only with the json array of objects, without any additional

 information or text.

 For example, if the image contains a purple check mark, two green scissors, one

 orange right-arrow, and a teal infinity sign you would write:

 [  {shape="; "check mark", "color": "purple"},

 {shape="; "scissors", "color": "green"},

 {shape="; "scissors", "color": "green"},

 {shape="; "right-arrow", "color": "orange"},

 {shape="; "infinity", "color": "teal"}  ]
2. 3D VLM Prompt

The following image contains multiple simple, colored objects.

 The possible shapes that may be present in the image are: <cone, cylinder, bowl

 donut, sphere, cube, droplet, bowling-pin, coil, crown, snowman, spikey-

 ball.

 The set of colors that may be present in the image are: <red, green, blue,

 yellow, purple, light green, gray, black, light blue, pink, teal, brown>.

 Describe each object in the image in the form of a JSON object, detailing the

 color and shape of each item.

 You must answer only with the json array of objects, without any additional

 information or text.

 For example, if the image contains a brown cube, two green donuts, and a cyan

 spikey-ball, you would write:

 [  {shape="; "cube", "color": "brown"},

 {shape="; "donut", "color": "green"},

 {shape="; "donut", "color": "green"},

 {shape="; "spikey-ball", "color": "cyan"}  ]
3. T2I Prompt

Render an image in photorealistic style with exactly (objects_string) arranged

 against a uniform background, each distinctly separated. Include only these

 objects in the image and nothing else.

[MISSING_PAGE_EMPTY:17]

7. **All Feature Decoding - Unified condition**

Examine the image provided, which depicts six basic, colored shapes arranged into three distinct pairs of objects: the source pair at the top, target pair #1 on the bottom left, and target pair #2 on the bottom right. For each pair, identify the shapes as follows: "object" refers to the left- most object in the pair, and the "object2" to the right-most object in the pair. Return the color and shape of each object in the trial in the json format described below. - Valid shapes: triangle, cloud, cross, heart, circle, square. - Valid colors: red, green, blue, darkorange, purple, and gray.

Your response should be in the following format: { source: { source_object: {shape: circle, color: purple},  source_object2: {shape: circle, color: purple} }, target: { target_object1: {shape: triangle, color: brown},  target1_object2: {shape: triangle, color: brown} }, { target2_object1: {shape: square, color: green},  target2_object2: {shape: square, color: black} } } Response:

8. **All Feature Decoding - Decomposed condition**

Examine the three images provided, which depict six basic, colored shapes arranged into three distinct pairs of objects: the source pair, target pair #1, and target pair #2. For each pair, identify the shapes as follows: "object1" refers to the left- most object in the pair, and the "object2" to the right-most object in the pair. Return the color and shape of each object in the trial in the json format described below. - Valid shapes: triangle, cloud, cross, heart, circle, square. - Valid colors: red, green, blue, darkorange, purple, and gray.

Your response should be in the following format: { source: { source_object1: {shape: circle, color: purple},  source_object2: {shape: circle, color: purple} }, target1: { target1_object1: {shape: triangle, color: brown},  target1_object2: {shape: triangle, color: brown} } }, { target2_object1: {shape: square, color: green},  target2_object2: {shape: square, color: black} } } Response:

## Appendix C Appendix: Human Evaluations

Human evaluations of the text-to-image variants of the counting and binding tasks were conducted on Prolific. Participants were asked to count the number of objects in each T2I counting task image, or asked to match objects to provided labels in each T2I scene description task image. They were also asked to list extraneous objects (generated objects not in the input prompt) when evaluating the binding images. Participants were paid a total estimated wage of approximately $12/hour, and total compensation for the entirety of the human evaluations was approximately $600. All participants provided informed consent. The study was approved by the Princeton University IRB.

## Appendix D Appendix: Computational Resources

Experiments were conducted on closed source models, and therefore did not require any specialized hardware. Generation of the 3D variants of the task were more computationally intensive and were performed in parallel on a local cluster, with 16GB RAM allocated per CPU per job.

Figure 9: **Human Evaluation**

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims made in the abstract and introduction are supported by behavioral evaluation or models and comparisons to previously published work in cognitive science. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We added a dedicated limitations paragraph to the Discussion (Section 6). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: Our work does not involve any novel assumptions that require formal justification. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All details regarding reproducing our experiments and controls are thoroughly described in the methods sections in the text and the Appendix. Furthermore, code for reproducing our experiments in the zip file attached to the OpenReview submission. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: All experiment code is provided in the submission's attached zip file. We will provide a documented version of the code upon publication. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: All methods for testing models is specified in the text and Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report error bars and significance tests for all experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Details on compute are listed in the Appendix (Section C). Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We reviewed the Code of Ethics and confirm that this paper conforms with it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: These are discussed in Section 6. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks as we do not release new models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All models used in the present work are available through commercial APIs, and dataset generation code is shared in the submission. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We release all datasets for GPT-4v and all images generated by DALL-E 3 at the following anonymized OSF repository: https://osf.io/xq9j8/?view_only=57ed8b3838f549758686be8e239c2e5e Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] We include details about human evaluations in the Appendix C. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [Yes] Justification: IRB approval was obtained for our studies, and we include a discussion of potential risks in the human evaluations in Appendix C. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.