# The ALCHEmnist: Automated Labeling 500x CHEaper Than LLM Data Annotators

 Tzu-Heng Huang, Catherine Cao, Vaishnavi Bhargava, Frederic Sala

University of Wisconsin-Madison

{thuang273, ccao35, vbhargava3}@wisc.edu,

fredsala@cs.wisc.edu

###### Abstract

Large pretrained models can be used as annotators, helping replace or augment crowdworkers and enabling distilling generalist models into smaller specialist models. Unfortunately, this comes at a cost: employing top-of-the-line models often requires paying thousands of dollars for API calls, while the resulting datasets are static and challenging to audit. To address these challenges, we propose a simple alternative: rather than directly querying labels from pretrained models, we task models to _generate programs that can produce labels_. These programs can be stored and applied locally, re-used and extended, and cost orders of magnitude less. Our system, **Alchemist**, obtains comparable to or better performance than large language model-based annotation in a range of tasks for a fraction of the cost: on average, improvements amount to a **12.9%** enhancement while the total labeling costs across all datasets are reduced by a factor of approximately **500\(\times\)**. We release our code here: https://github.com/SprocketLab/Alchemist.

## 1 Introduction

One of the most exciting developments in machine learning is the use of large pretrained models to act as _annotators_ or _labelers_[1, 2, 3, 4, 5, 6, 7, 8]. This includes the use of large language models (LLMs) like GPT-4 [9] and Claude 3 [10]. This process offers multiple benefits. First, pretrained models are an efficient way to annotate and have the potential to partially or fully replace expensive human crowdworkers [2, 6, 11, 12]. Second, this approach allows for _distilling_ large models into smaller, task-specific models that can be deployed locally at lower cost [3, 13, 7, 8]. This is additionally important in settings like healthcare and finance where privacy laws require the use of local models.

Despite this promise, pretrained model-based annotation has several drawbacks that stymie its adoption. These drawbacks include

* **High Cost**: Labeling a dataset can be expensive. This is particularly so in cases where each data point consists of many tokens. For example, we find that labeling a moderately-sized dataset [14] with 7,569 data points using GPT-4 costs over $1,200.
* **Lack of Extensibility**: Making even small changes to specifications necessitates re-running the entire pipeline to obtain new labels. This inflexibility means the resulting labels are static.
* **Inability to Audit**: API access to pretrained models does not permit inspecting most aspects of the model. Users must simply accept the provided labels with only minimal additional information. Techniques that ask the model for explanations for its decisions may not be reliable [15, 16, 17].

We address these obstacles through a simple but surprisingly powerful notion. Rather than having pretrained models label data, we task language models to _generate programs that can output labels_. These synthesized programs serve as annotators, capturing the underlying logic used by the models when annotating. In other words, instead of distilling a powerful model to label a dataset (andsubsequently training a smaller model on the labeled data), we _distill directly into code_ (Figure 1). These resulting programs can either make predictions directly or can label training dataset then train a downstream model using it1.

Footnote 1: The latter option is preferable, as these models can often generalize beyond their source of supervision [13]

This simple notion resolves all of the challenges related to pretrained model-based annotation. First, _API calls scale with the number of programs instead of the number of data points._ That is, since we generate programs that can themselves make any number of predictions locally at no cost, we can reduce the number of API calls by orders of magnitude. For example, for the dataset described above [14], the number of GPT-4 calls was reduced from 7,569 (the size of the dataset) to 10 (the number of generated programs), resulting in a massive cost reduction from $1,200 to $0.70, a 1,700-fold decrease. Moreover, code can be easily inspected, corrected, and extended, allowing seamless adaptation when prediction classes or labeling rules change.

While a powerful idea, distilling model into code presents several challenges. First, any particular program may be inaccurate, fail to compile, or may otherwise be flawed, resulting in noisy program outputs. We address this obstacle by applying _weak supervision_, a framework for dataset construction from multiple noisy sources of signal [18, 19, 20, 21]. Next, operating on non-text modalities is challenging. We handle this via a simple two-step approach that first extracts high-level concepts and then uses them in concert with a local feature extractor to enable tractable program generation.

**Contributions.** We propose an alternative approach to replace expensive annotation processes that require repetitive prompting for labels. We developed a system called _Alchemist_ that implements this idea. Empirically, Alchemist improves performance five out of eight datasets, with an average enhancement of 12.9%--while reducing total costs by a factor of approximately \(500\times\). Finally, we introduce and validate extensions that address non-text modalities.

## 2 Related Work

Our work relates to LLM-based annotation, prompting, and the weak supervision framework.

**Using Large Pretrained Models for Data Annotation.** Large pretrained models have demonstrated powerful capabilities using zero-shot prompting across a wide range of tasks [1]. One promising development is their potential to serve as data labelers, which can reduce the cost and human effort in

Figure 1: Examples of generated programs and their prompts. These are synthesized by GPT-4 for spam detection and cancer identification tasks. Programs use regular expressions (left program) and keyword matching (right program) as their labeling logic to classify data points.

data labeling [1; 2; 11; 12]. Existing research in this area mainly focuses on approaches that allow for more efficient inference, enhanced label generation, and distilling into smaller but specialized labelers [3; 4; 5; 6; 7; 8]. However, _scalability_ is the main limitation in these approaches, as making inferences via querying an API for data examples can be cost-prohibitive. To tackle this challenge, rather than prompting for labels repetitively, we propose prompting pretrained models for programs that use synthesized labeling logic and can thus serve as alternative data labelers.

**Prompt Engineering & In-Context Learning.** In-context learning adapts pretrained models to new tasks without additional fine-tuning [1]. It involves providing relevant examples as demonstrations to solve the task, such as pairs of languages for translation [22]. By including task-specific examples, models can better understand the task at hand. Adding a few data points as demonstrations [23] is commonly suggested when models act as data annotators. Moreover, they can be selected [24; 25], retrieved [26], or more efficiently, generated [27]. We explore various types of supplementary information that can be added to Alchemist to help improve program generation and permit more control over the labeling logic used in the programs.

**Weak Supervision Framework.** Weak supervision enables the rapid creation of large training datasets by aggregating cheap-but-noisy signals derived from various labeling sources [18; 19; 21; 28]. These sources can be crafted by domain expertise, using labeling heuristics, or even trained on smaller, weaker classifiers [29; 30; 31; 32]. Recent advancements in code generation open up the potential to automate the heuristic design process. Frameworks such as ScriptoriumWS [33], and DataSculpt [34] have been developed to take advantage of code-generating models [35; 9; 36] to craft weak supervision sources through prompting. While similar in spirit to our approach, these have several drawbacks: ScriptoriumWS requires more human effort in prompt engineering to better guide code-generation models. Both ScriptoriumWS and DataSculpt can perform poorly in tasks requiring specific domain expertise and, most importantly, they do not handle modalities beyond text--unlike Alchemist.

## 3 Alchemist System

We begin by presenting a general annotation workflow in Alchemist, followed by a detailed discussion of each key step.

**General Workflow.** The process is depicted in Fig. 2. First, users select an unlabeled dataset and create simple prompts to instruct language models to generate programs that incorporate labeling logic. These prompts can integrate relevant information and may vary in their design, allowing for the synthesis of multiple programs. Next, given a set of generated programs and their outputs, we apply weak supervision techniques to obtain a set of aggregated labels. Finally, the labeled points can be used to train a distilled model that can be stored and used locally.

### Prompting Strategy

We propose a general and extensible prompt template for querying language models to generate annotator programs. This general template consists of three key components:

* **Task Description**: Provides the model an overview of generated program's desired objectives.
* **Labeling Instructions**: Specifies classes and the expected structure of the program's output.
* **Function Signature**: Describes the function's name and the input types to be used.

Figure 2: Overall workflow for Alchemist.

This simple but general template allows for flexible incorporation of various types of information, enabling the model to generate programs that are tailored to specific requirements. Two sample prompt templates in Alchemist are displayed in Fig 1.

**Using Supplementary Information.** Drawing inspiration from few-shot prompting [37, 1], where users provide demonstrations (i.e., data points with their labels) to enhance generated responses, we explore various types of supplementary information that can be integrated to assist models in synthesizing programs. This approach is particularly useful for scenarios where language models may lack the expertise to generate effective programs, or where specific adaptations in labeling logic are required. Such information can be crafted by users themselves, domain experts or, more efficiently, generated by language models themselves. Additionally, it can be combined with retrieval-augmented generation (RAG) systems [38, 39] to access external knowledge.

We explore various types of supplementary information to assist in code generation, starting with high-level concepts and then progressively looking into more practical details to control programs.

_Dataset and Prediction Class Description._ First, supplementary information can include relevant background details about the purpose for which the dataset was built and high-level information about the dataset, such as definitions for each label class. By providing this context, the language model can better understand the task at hand.

_Data Exemplars._ Furthermore, we recommend including a small number of labeled data examples in the prompt. This can help language models better comprehend the specific problem. Examples act as concrete illustrations of the task, offering a clearer understanding of the expected output. This can be particularly beneficial when dealing with a complex problem.

_Keywords._ Next, labeling logic in programs can make use of keyword-searching techniques (e.g., Fig 1). For instance, in situations such as spam detection or topic classification, certain words or phrases may have a strong correlation with specific classifications. Providing several keywords in the prompt may lead models to create labeling programs that explicitly search for the presence or absence of these keywords. This allows for more targeted and precise labeling.

_Specialized Labeling Rules._ Finally, more prior knowledge such as heuristics, specialized labeling rules, guidance, and domain-specific knowledge can be integrated into the prompt. This information can provide concrete labeling steps on how to label specific classes and offer greater control over the logic implemented in the generated programs.

Figure 3: Alchemist can handle rich modalities through a simple extension. First, a language model identifies task-specific concepts (top). Then, a local multimodal model is used as a feature extractor for these concepts, producing low-dimensional feature vectors that can be ingested by generated labeling programs.

Overall, supplementary context is provided before the task description to enhance language models' understanding of the task. This, in turn, enables models to generate programs that are more effective and tailored to the specific requirements of user needs.

### Dataset Synthesis

While generated programs can efficiently annotate data, these programs may produce outputs that are noisy or inaccurate. However, as such programs may employ different techniques, such as pattern-matching, heuristic rules, or other approaches--each with its own strengths and limitations--there may be _complementary_ signal in their outputs. This means we can aggregate them to mitigate the impact of noise. To do so, we apply weak supervision techniques [18; 19; 20; 21]. This process starts by learning a model of the reliabilities of the programs. Once learned, this model enables aggregating label outputs from different programs into high-quality _pseudolabels_.

Alchemist is compatible with a variety of weak supervision aggregation models, called _label models_, providing flexibility in the choice of the weak supervision approach. For simplicity, in this work, we focus on using the Snorkel framework [19], which is a standard and widely-used approach in the weak supervision community.

### Extensions: Handling Complex Modalities.

Crafting programs that operate over text is relatively easy for large language models. More complex data modalities, however, can be far more challenging. Consider images as an illustrative example. Even employing state-of-the-art multimodal models, e.g., GPT-4o [40] and GPT-4V [9], to seek programs operating over sample images may not produce satisfactory results.

To address this challenge, we extend Alchemist's pipeline to include an intermediate step. Specifically, we convert the raw data (i.e., in our example, image pixels) into a set of features representing high-level concepts. These concepts are obtained by prompting a language model (or, potentially, a multimodal model) to identify task-relevant notions. For example, for a bird categorization task, models may identify "wing shape," "beak shape," or "foot type" as informative concepts for distinguishing between bird species. Next, we use any open-source local multimodal model, like CLIP [41], as a feature extractor for the identified concepts, producing low-dimensional feature vectors that can be easily ingested by generated programs. As such models are free, this does not increase our cost.

Fig. 3 and Fig. 4 present examples of generated high-level concepts and the corresponding programs used for the Waterbirds dataset, where the task is to distinguish between landbird and waterbird species [42]. This simple approach can be applied to any data modality where we have access to a local multimodal model (i.e., a model operating on the modality of interest and text).

Figure 4: Program examples generated by GPT4o on Waterbirds dataset. The left program is synthesized by directly asking for a labeling program when the input is an image (raw pixels), while the right program uses Alchemist’s extension. The former labels birds using the dominant color in the image, which can be predicted incorrectly due to spurious correlations (e.g., background).

## 4 Experiments

We study the capability of Alchemist empirically. Our goals are to validate the following claims:

* **Cost Reduction and Improved Performance (Sec. 4.1):** Alchemist can reduce cost by orders of magnitude, while producing labels of similar or better accuracy.
* **Extendibility to Other Modalities (Sec. 4.2):** Alchemist can operate with modalities beyond text.
* **Use of Supplementary Information (Sec. 4.3):** Incorporating relevant information into prompts enables the generation of better programs, yielding more accurate pseudolabels.
* **More Diverse Programs Can Help (Sec. 4.4):** Increasing the diversity of generated programs created by different labeling logic enables better pseudo labels.
* **Comparing to Human-crafted Programs (Sec 4.5):** Synthesized programs may be more effective in comparison to human-crafted ones.

**Datasets.** We include diverse datasets covering text and image modalities. For text, we include eight datasets that span three different types of language tasks. These include the YouTube [43], SMS [44] datasets for spam classification, IMDb [45], Yelp [45], Finance [46], and French [47] datasets for sentiment analysis, and the MedAbs [48] and Cancer [14] datasets for topic classification. We note that the Finance, French, MedAbs, and Cancer datasets are relatively challenging, with points that require a degree of domain expertise for accurate labeling. For example, the French dataset requires a good understanding of the language. These may pose challenges for pretrained models.

For our extensions to richer modalities, we focus on image tasks. Our evaluation uses the Waterbirds dataset [42]. This dataset is designed to assess models' robustness to spurious correlations and ability to handle distribution shifts. More details are in Appendix A.

### Cost Reduction and Improved Performance

**Setup.** We open our evaluation of Alchemist with text domain datasets and use GPT-3.5 to generate programs. For each dataset, we input pure prompts without supplementary information into GPT-3.5 and generate 10 programs to use. We construct training datasets by aggregating the programs' outputs into pseudolabels with the weak supervision framework Snorkel [19]. We then train a two-layer MLP as a distilled model. We run five times with different random seeds and report their average performance. As our main baseline, we directly use language models to produce annotations per point. The resulting labels are used to train a distilled model for comparison. The prompt template used in our baseline approach and our training settings are provided in Appendix A.

**Expected Results.** We anticipate that Alchemist can generate programs that can produce accurate labels while substantially reducing the expense of API calls.

**Results.** Table 1 presents the distilled model's performance on each testing dataset. We observe that label accuracy is improved on five out of eight datasets, particularly in challenging settings such as the MedAbs, Cancer, and French datasets, outperforming the baseline zero-shot prompting approach. We also report the estimated costs of building training datasets. The costs for zero-shot prompting depend on the number of tokens for the dataset. In contrast, Alchemist only prompts 10 programs for

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{**YouTube**} & \multicolumn{2}{c}{**SMS**} & \multicolumn{2}{c}{**Yelp**} & \multicolumn{2}{c}{**IMb**} \\ \cline{2-9}  & Est. Cost & Accuracy & Est. Cost & F1-score & Est. Cost & Accuracy & Est. Cost & Accuracy \\ \hline Zero-shot Prompting & 0.096 & 0.871 & 0.240 & **0.907** & 3.873 & **0.845** & 3.400 & **0.737** \\ Alchemist with GPT-3.5 & 0.004 & **0.891** & 0.004 & 0.900 & 0.005 & 0.575 & 0.004 & 0.662 \\ \hline \multicolumn{9}{c}{**MedAbs**} & \multicolumn{2}{c}{**Cancer**} & \multicolumn{2}{c}{**Finance**} & \multicolumn{2}{c}{**French**} \\ \cline{2-9}  & Est. Cost & Accuracy & Est. Cost & Accuracy & Est. Cost & Accuracy & Est. Cost & Accuracy \\ \hline Zero-shot Prompting & 1.944 & 0.311 & 15.925 & 0.716 & 0.201 & 0.641 & 0.641 & 0.611 \\ Alchemist with GPT-3.5 & 0.006 & **0.346** & 0.003 & **0.968** & 0.007 & **0.660** & 0.006 & **0.690** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Testing performance of the distilled model is reported for each combination of method and dataset. The estimated cost is obtained by calculating the number of input and output tokens associated with GPT-3.5’s pricing table [49]. Other models may be even more expensive.

each task, resulting in a significant reduction in the costs--by orders of magnitude. This efficiency is the main advantage of Alchemist, _as it allows for the creation of high-quality datasets with minimal expense._ We include ablation studies with other weak supervision models within the Alchemist framework in Appendix C. They successfully demonstrate the flexibility and robustness of using Alchemist.

### Extending Alchemist to Other Modalities

**Setup.** Next, we validate the extension of Alchemist to richer modalities. We consider our approach, where we prompt a multimodal model such as GPT4o and Claude 3, to generate high-level task-specific concepts. We extract features for these concepts by employing CLIP as our local feature extractor. This converts raw pixels into feature vectors for the extracted high-level concepts, producing a set of similarity scores. Armed with these scores, we describe scores associated with their concepts in prompts and ask GPT4o and Claude 3 for 10 programs. As before, we use Snorkel as our aggregation procedure.

_Baselines._ We study two baselines. The first is the vanilla version of Alchemist, where we directly ask GPT4o and Claude 3 to produce code that can operate on images (see left program in Fig. 4). The second is simple zero-shot prompting using CLIP, along with a variant, a group prompting approach that assumes access to spurious information and adds it to the given prompt2.

Footnote 2: https://github.com/google-research/

**Expected Results.** We expect employing our two-step process can enable tractable program generation. In addition, we hypothesize that programs generated in this way are beneficial in targeting salient concepts and reducing the impact of irrelevant or shortcut features, thereby enhancing robustness.

**Results.** We present results in Table 2. Our evaluation focuses on three key metrics: average accuracy, worst group accuracy, and the gap between these two measures. Ideally, a robust model should achieve high average accuracy and high worst group accuracy while minimizing the disparity between the two. First, we see that directly asking programs to use may have very low performance (GPT4o) or may hugely suffer from spurious correlations, destroying worst group performance (Claude 3, CLIP zero-shot). Our method addresses both cases. Compared to baseline methods, Alchemist demonstrates increased worst group accuracy and a reduced gap between the average and worst group accuracies. This is a key strength of Alchemist: _targeting salient concepts to be used as features may help move models away from spurious shortcuts found in the data_. This validates Alchemist's ability to handle complex modalities while improving robustness.

\begin{table}
\begin{tabular}{c l c c} \hline \hline Feature Extractor & Method & Average Accuracy (\(\uparrow\)) & Worst Group Accuracy (\(\uparrow\)) & Gap (\(\downarrow\)) \\ \hline — & **Vanilla Alchemist with GPT4o** & 0.395 & 0.367 & 0.028 \\  & **Vanilla Alchemist with Claude 3** & 0.781 & 0.022 & 0.759 \\ \hline CLIP ViT-B/32 & Zero-shot Prompting & 0.820 & 0.318 & 0.502 \\  & Group Prompting & **0.823** & 0.383 & 0.440 \\ \cline{2-4}  & **Alchemist with GPT4o** & 0.805 & 0.283 & 0.522 \\  & **Alchemist with Claude 3** & 0.774 & **0.463** & **0.410** \\ \hline CLIP ViT-L/14 & Zero-shot Prompting & **0.904** & 0.335 & 0.569 \\  & Group Prompting & 0.791 & 0.240 & 0.551 \\ \cline{2-4}  & **Alchemist with GPT4o** & 0.802 & **0.467** & **0.335** \\  & **Alchemist with Claude 3** & 0.737 & 0.346 & 0.391 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Alchemist on non-text modalities. We experiment with standard Alchemist (top), our proposed extension with two CLIP-based local models as feature extractors, and CLIP prompting baselines. Alchemist achieves comparable performance on average accuracy while improving robustness to spurious correlations.

### Use of Supplementary Information

**Setup.** We test how integrating relevant information into the prompt context can augment generated programs. Instead of manually crafting supplementary information, we harness the power of language models to generate and integrate. This approach is useful for challenging datasets where users may not have the necessary knowledge or expertise to start. We evaluate the effectiveness of this approach, by comparing label model performance using programs generated by two different methods: pure prompting and in-context prompting. In-context prompting involves supplementary information, while pure prompting relies solely on the task description without any additional guidance. We employ GPT-3.5, GPT-4 and Claude 3 as our program sources and synthesize ten for each strategy.

**Expected Results.** We hypothesize that providing supplementary information can enhance task understanding, demonstrate specific labeling logic, and offer concrete steps, ultimately leading to better programs for use.

**Results.** Table 3 presents this comparative analysis on label model performance using different type of information. We observe that by incorporating supplementary information into pure prompts, Alchemist can guide language models to generate more effective programs, which in turn produce more accurate pseudolabels. Improvements are particularly evident in the challenging datasets such as Finance and French. Moreover, this approach can be combined with RAG systems to include external knowledge bases and customize the relevant information. Such flexibility compared to zero-shot prompting is another key strength of Alchemist, as _programs can easily be adapted, augmented, and specialized._

### More Diverse Programs Can Help

**Setup.** As shown in Table 3, incorporating different supplementary information results in varying degrees of additional improvement. Potentially, certain sets of supplementary information allow the

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{YosTube} & \multicolumn{3}{c}{SMS} & \multicolumn{3}{c}{Velp} & \multicolumn{3}{c}{IMOb} \\ \cline{2-13}  & GPT-3.5 & GPT-4 & Claude 3 & GPT-3.5 & GPT-4 & Claude 3 & GPT-3.5 & GPT-4 & Claude 3 & GPT-3.5 & GPT-4 & Claude 3 \\ \hline General Prompt & 0.92 & 0.92 & 0.66 & 0.64 & 0.62 & 0.75 & 0.65 & 0.82 & 0.78 & 0.71 & 0.77 & 0.77 \\ \hline \(+\) Dataset Description & 0.64 & **0.93** & **0.71** & 0.63 & **0.63** & **0.76** & **0.72** & 0.82 & **0.79** & 0.70 & **0.79** & 0.73 \\ \(+\) 5 Data Examples & 0.91 & 0.86 & **0.76** & 0.46 & **0.66** & 0.62 & **0.72** & 0.82 & **0.82** & 0.68 & 0.75 & 0.73 \\ \(+\) Keywords & 0.76 & **0.93** & 0.53 & 0.40 & 0.42 & 0.64 & **0.69** & 0.81 & 0.78 & 0.69 & **0.78** & 0.72 \\ \(+\) Labeling Rules & 0.74 & 0.82 & 0.56 & **0.67** & **0.67** & 0.58 & **0.75** & 0.81 & **0.79** & 0.71 & 0.77 & 0.74 \\ \hline \hline \multicolumn{13}{c}{MedAbs} & \multicolumn{3}{c}{Cancer} & \multicolumn{3}{c}{Finance} & \multicolumn{3}{c}{French} \\ \cline{2-13}  & GPT-3.5 & GPT-4 & Claude 3 & GPT-3.5 & GPT-4 & Claude 3 & GPT-3.5 & GPT-4 & Claude 3 & GPT-3.5 & GPT-4 & Claude 3 \\ \hline General Prompt & 0.52 & 0.53 & 0.55 & 0.71 & 0.73 & 0.59 & 0.66 & 0.49 & 0.56 & 0.65 & 0.55 & 0.56 \\ \hline \(+\) Dataset Description & 0.49 & 0.50 & 0.51 & 0.59 & 0.62 & **0.60** & 0.61 & **0.63** & **0.62** & 0.39 & **0.58** & **0.67** \\ \(+\) 5 Data Examples & **0.53** & **0.54** & 0.55 & 0.55 & 0.57 & **0.63** & 0.60 & **0.50** & **0.60** & 0.40 & **0.69** & 0.44 \\ \(+\) Keywords & **0.55** & **0.55** & 0.55 & 0.55 & 0.46 & 0.66 & **0.62** & **0.65** & **0.69** & **0.66** & **0.67** \\ \(+\) Labeling Rules & 0.52 & **0.55** & **0.56** & 0.61 & 0.59 & **0.63** & 0.66 & **0.56** & **0.67** & 0.65 & **0.66** & 0.33 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Testing performance of the label model is reported for each combination of prompting strategy and dataset. We observe that GPT-4 and Claude 3 (that may possess better comprehension capabilities) exhibit greater enhancements when provided with supplementary information.

Figure 5: Performance is reported using their average performance and standard deviations. Results indicate that the label model is improved when the number of diverse programs increases.

model to specialize better on certain data points than others. We seek to achieve these performance improvements _without_ the need to re-prompt the model with each set of supplementary information. Instead, we collect previously generated programs to obtain a set of programs with greater diversity. We ask: _can Alchemist achieve better performance by modeling more diverse programs?_

We randomly select a set of programs from each category, collect them, and train the label model with their program outputs. Additionally, we increase the number of sampled programs in each category from 4 to 9. We test this approach on the datasets where Alchemist gives comparable or lower performance than zero-shot prompting in our initial experiments in Table 1, namely the SMS, Yelp, and IMDb datasets.

**Expected Results.** By obtaining more diverse programs to use, Alchemist can capture a wider range of perspectives and labeling logic, potentially leading to more accurate pseudolabels.

**Results.** Fig. 5 visualizes the effect on the label model's performance when we increase the diversity in collected programs. It demonstrates a trend and indicates that involving a more diverse set of programs can help to mitigate the impact of individual strategy biases or limitations, leading to the production of better labels.

Overall, results in Sec. 4.3 and in Sec. 4.4 underscore that _the use of supplementary information and involving diverse types of programs can help achieve better performance._

### Comparing to Human-crafted Programs

**Setup.** Lastly, we compare synthesized programs in Alchemist and manually crafted labeling functions in WRENCH [50], which is a widely-used benchmark for evaluating weak supervision methods. We focus on the datasets that overlap between Alchemist and WRENCH. For each dataset, we use pure prompts to query GPT-3.5, GPT-4, and Claude 3 for 10 programs. We then evaluate the performance of the distilled model for both methods. We also include the label model's coverage in our comparison. Higher coverage means that label model can produce more pseudolabels, yielding a larger size of training dataset to use.

**Expected Results.** We expect that synthesized programs may offer some advantages in terms of efficiency and effectiveness compared to human-designed ones.

**Results.** Table 4 presents their comparison. By leveraging the knowledge and capabilities of language models, we find that generated programs offer several advantages, including better coverage (i.e., the ability to label more data points) and comparable, or even better, performance. Generated programs can reduce the need for laborious engineering, which can be time-consuming and often requires a tedious design process to fine-tune labeling logic, such as thresholds and keyword usage. This design process may lead to many undiscovered rules, resulting in lower performance on coverage and potentially limiting the effectiveness of the labeling functions--unlike synthesized programs.

This is particularly evident in the SMS dataset, where WRENCH requires 73 manually crafted labeling functions to obtain high-quality labels, while Alchemist only needs 10 generated programs to obtain comparable performance and higher coverage. This significant reduction highlights the potential of Alchemist to _assist humans in designing labeling functions and make it more accessible to users without extensive domain expertise._

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{**Yelpke**} & \multicolumn{3}{c}{**Yelp**} & \multicolumn{3}{c}{**IDEr**} \\  & Human & \multicolumn{3}{c}{Synthesis} & \multicolumn{3}{c}{Human} & \multicolumn{3}{c}{Synthesis} & \multicolumn{3}{c}{Human} & \multicolumn{3}{c}{Synthesis} & \multicolumn{3}{c}{Human} & \multicolumn{3}{c}{Synthesis} \\  & & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} \\  & & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} \\  & & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} \\ \hline Sum. of Programs & 10 & 10 & 10 & 10 & 73 & 10 & 10 & 10 & 10 & 8 & 10 & 10 & 10 & 8 & 10 & 10 & 5 & 10 & 10 \\ Coverage & 0.89 & 1.09 & 1.09 & 1.09 & 1.10 & 0.41 & 1.06 & 1.00 & 1.00 & 0.83 & 0.78 & 0.99 & 0.80 & 0.88 & 0.89 & 1.00 & 0.98 \\ Performance & 0.85 & **0.80** & **0.89** & 0.72 & 0.89 & **0.60** & **0.83** & 0.99 & 0.76 & 0.87 & **0.82** & **0.83** & 0.73 & 0.66 & **0.75** & 0.70 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Analysis showing that Alchemist can achieve comparable or better accuracy and higher coverage while using fewer programs to label the data.

Conclusion

We propose an alternative approach to costly annotation procedures that require repeated API requests for labels. Our solution introduces a simple notion of prompting programs to serve as annotators. We developed an automated labeling system called Alchemist to embody this idea. Empirically, our results indicate that Alchemist demonstrates comparable or even superior performance compared to language model-based annotation, improving five out of eight datasets with an average enhancement of 12.9%. Notably, Alchemist reduces total costs by a factor of approximately 500. Furthermore, we showcase the system's extensibility to handle more complex modalities while enhancing the robustness of predicted labels. Finally, we confirm that incorporating relevant information can generate better programs, and increasing diversity leads to obtaining higher-quality labels.

## 6 Acknowledgments

We are grateful for the support of the NSF under CCF2106707 (Program Synthesis for Weak Supervision) and the Wisconsin Alumni Research Foundation (WARF). We thank Dyah Adila, Albert Gu, Harit Vishwakarma, and Nicholas Roberts, for their helpful feedback and valuable discussion.

## References

* [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [2] Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. Want to reduce labeling cost? gpt-3 can help. _arXiv preprint arXiv:2108.13487_, 2021.
* [3] Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong. Zerogen: Efficient zero-shot learning via dataset generation. _arXiv preprint arXiv:2202.07922_, 2022.
* [4] Jiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Progen: Progressive zero-shot dataset generation via in-context feedback. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 3671-3683, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
* [5] Jiahui Gao, Renjie Pi, LIN Yong, Hang Xu, Jiacheng Ye, Zhiyong Wu, WEIZHONG ZHANG, Xiaodan Liang, Zhenguo Li, and Lingpeng Kong. Self-guided noise-free data generation for efficient zero-shot learning. In _International Conference on Learning Representations_, 2023.
* [6] Xingwei He, Zhenghao Lin, Yeyun Gong, Alex Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, Weizhu Chen, et al. Annollm: Making large language models to be better crowdsourced annotators. _arXiv preprint arXiv:2303.16854_, 2023.
* [7] Ruoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, and Lei Zou. Llmaaa: Making large language models as active annotators. _arXiv preprint arXiv:2310.19596_, 2023.
* [8] Yu Meng, Martin Michalski, Jiaxin Huang, Yu Zhang, Tarek Abdelzaher, and Jiawei Han. Tuning language models as training data generators for augmentation-enhanced few-shot learning. In _International Conference on Machine Learning_, 2023.
* [9] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [10] Anthropic. Introducing the next generation of claude, Mar 4, 2024.
* [11] Fabrizio Gilardi, Meysam Alizadeh, and Mael Kubli. Chatgpt outperforms crowd workers for text-annotation tasks. _Proceedings of the National Academy of Sciences_, 120(30):e2305016120, 2023.

* [12] Xiaohuan Pei, Yanxi Li, and Chang Xu. Gpt self-supervision for a better data annotator. _arXiv preprint arXiv:2306.04349_, 2023.
* [13] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. _arXiv preprint arXiv:2305.02301_, 2023.
* [14] Tetsuya Sasaki, Firoz Chowdhury, and Sunil Thite. Medical text dataset: Cancer doc classification, 2023.
* [15] Shiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, and Leilani H Gilpin. Can large language models explain themselves? a study of llm-generated self-explanations. _arXiv preprint arXiv:2310.11207_, 2023.
* [16] Andreas Madsen, Sarath Chandar, and Siva Reddy. Can large language models explain themselves? _arXiv preprint arXiv:2401.07927_, 2024.
* [17] Chirag Agarwal, Sree Harsha Tanneru, and Himabindu Lakkaraju. Faithfulness vs. plausibility: On the (un) reliability of explanations from large language models. _arXiv preprint arXiv:2402.04614_, 2024.
* [18] Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher Re. Data programming: Creating large training sets, quickly. _Advances in neural information processing systems_, 29, 2016.
* [19] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Re. Snorkel: Rapid training data creation with weak supervision. In _Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases_, volume 11, page 269. NIH Public Access, 2017.
* [20] Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic Sala, Shreyash Pandey, and Christopher Re. Training complex models with multi-task weak supervision. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 4763-4771, 2019.
* [21] Daniel Fu, Mayee Chen, Frederic Sala, Sarah Hooper, Kayvon Fatahalian, and Christopher Re. Fast and three-rious: Speeding up weak supervision with triplet methods. In _International Conference on Machine Learning_, pages 3280-3291. PMLR, 2020.
* [22] Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. In-context examples selection for machine translation. _arXiv preprint arXiv:2212.02437_, 2022.
* [23] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35, 2023.
* [24] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? _arXiv preprint arXiv:2101.06804_, 2021.
* [25] Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. _arXiv preprint arXiv:2205.14334_, 2022.
* [26] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. _arXiv preprint arXiv:2112.08633_, 2021.
* [27] Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator. _arXiv preprint arXiv:2206.08082_, 2022.
* [28] Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah M. Hooper, Kayvon Fatahalian, and Christopher Re. Fast and three-rious: Speeding up weak supervision with triplet methods. In _Proceedings of the 37th International Conference on Machine Learning_, ICML'20. JMLR.org, 2020.

* [29] Paroma Varma and Christopher Re. Snuba: Automating weak supervision to label training data. In _Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases_, volume 12, page 223. NIH Public Access, 2018.
* [30] Nilaksh Das, Sanya Chaba, Renzhi Wu, Sakshi Gandhi, Duen Horng Chau, and Xu Chu. Goggles: Automatic image labeling with affinity coding. In _Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data_, pages 1717-1732, 2020.
* [31] Benedikt Boecking, Willie Neiswanger, Eric Xing, and Artur Dubrawski. Interactive weak supervision: Learning useful heuristics for data labeling. In _International Conference on Learning Representations_, 2021.
* [32] Nicholas Roberts, Xintong Li, Tzu-Heng Huang, Dyah Adila, Spencer Schoenberg, Cheng-Yu Liu, Lauren Pick, Haotian Ma, Aws Albarghouthi, and Frederic Sala. AutoWS-bench-101: Benchmarking automated weak supervision with 100 labels. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [33] Tzu-Heng Huang, Catherine Cao, Spencer Schoenberg, Harit Vishwakarma, Nicholas Roberts, and Frederic Sala. Scriptoriumws: A code generation assistant for weak supervision. _ICLR Deep Learning for Code Workshop_, 2023.
* [34] Naiqing Guan, Kaiwen Chen, and Nick Koudas. Can large language models design accurate label functions?, 2023.
* [35] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_, 2021.
* [36] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [37] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. _arXiv preprint arXiv:2301.00234_, 2022.
* [38] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in Neural Information Processing Systems_, 33:9459-9474, 2020.
* [39] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. _arXiv preprint arXiv:2312.10997_, 2023.
* [40] Open AI. Hello gpt-4o, Mar 13, 2024.
* [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* [42] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. _arXiv preprint arXiv:1911.08731_, 2019.
* [43] Tulio C Alberto, Johannes V Lochter, and Tiago A Almeida. Tubespam: Comment spam filtering on youtube. In _2015 IEEE 14th international conference on machine learning and applications (ICMLA)_, pages 138-143. IEEE, 2015.
* [44] Tiago A Almeida, Jose Maria G Hidalgo, and Akebo Yamakami. Contributions to the study of sms spam filtering: new collection and results. In _Proceedings of the 11th ACM symposium on Document engineering_, pages 259-262, 2011.

* [45] Wendi Ren, Yinghao Li, Hanting Su, David Kartchner, Cassie Mitchell, and Chao Zhang. Denoising multi-source weak supervision for neural text classification. _arXiv preprint arXiv:2010.04582_, 2020.
* [46] P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. _Journal of the Association for Information Science and Technology_, 65, 2014.
* [47] Abir ELTAIEF. french book reviews, 2023.
* [48] Tim Schopf, Daniel Braun, and Florian Matthes. Evaluating unsupervised text classification: Zero-shot and similarity-based approaches. In _Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval_, NLPIR '22, page 6-15, New York, NY, USA, 2023. Association for Computing Machinery.
* [49] OpenAI. Openai pricing table.
* [50] Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and Alexander Ratner. Wrench: A comprehensive benchmark for weak supervision. _arXiv preprint arXiv:2109.11377_, 2021.
* [51] Dyah Adila, Changho Shin, Linrong Cai, and Frederic Sala. Zero-shot robustification of zero-shot models with foundation models. _arXiv preprint arXiv:2309.04344_, 2023.
* [52] A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the em algorithm. _Applied Statistics_, 28(1):20-28, 1979.
* [53] George B Moody and Roger G Mark. The impact of the mit-bih arrhythmia database. _IEEE engineering in medicine and biology magazine_, 20(3):45-50, 2001.
* [54] Ron Kohavi. Census Income. UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5GP7S.
* [55] Jason A Fries, Paroma Varma, Vincent S Chen, Ke Xiao, Heliodoro Tejeda, Priyanka Saha, Jared Dunnmon, Henry Chubb, Shiraz Maskatia, Madalina Fiterau, et al. Weakly supervised classification of aortic valve malformations using unlabeled cardiac mri sequences. _Nature communications_, 10(1):3111, 2019.

The appendix is organized as follows. First, we provide details about datasets, training settings, and computation resources in Appendix A. Next, in Appendix B we list prompts that we use to query language models. Then, we present ablation studies in Appendix C using other models in weak supervision to work with Alchemist. We include two additional modalities in Appendix D. We incorporate other work [51] for enhancing robustness into Alchemist and present in E. Lastly, we discuss limitations and broader impacts of our work in Appendix F.

## Appendix A Datasets and Implementation Details

We place more details about our datasets and experimental setups here. First, in Table 5 we show task type, prediction classes, and number of training data points in each dataset. MedAbs, Cancer, Finance, and French are considered to be more challenging settings, where these datasets typically need domain expertise to provide labels. Waterbirds is considered to test for a more complex modality.

We employ Snorkel as our label model to aggregate program outputs and report results in the main paper. We show more results using different choices of label model in Appendix C. All the distilled models use the MLP model that is trained with 2 hidden layers, each comprising 32 units, using ReLU activations between layers and no normalization. We run 5 times with different random seeds and report their average performance. We use a A6000 NVidia GPU to run all experiments.

## Appendix B Used Prompts

Next, we present the prompts used to query language models in the baselines and Alchemist.

\begin{table}
\begin{tabular}{l l} \hline \hline Dataset & Zero-shot Prompting (Baseline) \\ \hline YouTube & what is the category of this youtube comment: [text] \\ SMS & what is the category of this sms text: [text] \\ Yelp & what is the sentiment of this restaurant review: [text] \\ IMDb & what is the topic of this abstract: [text] \\ MedAbs & what is the topic of this abstract: [text] \\ Cancer & what is the topic of this document: [text] \\ Finance & what is the sentiment of this news: [text] \\ French & what is the sentiment of this book review: [text] \\ \hline \hline \end{tabular}
\end{table}
Table 6: Prompts for baseline approach are presented.

\begin{table}
\begin{tabular}{l l c c} \hline \hline Dataset & Task Description (Alchemist) & **\#** of Classes & **\#** of Train \\ \hline YouTube [43] & system comment detection & [“span”, "ham"] & 2 & 1686 \\ SMS [46] & spam text detection & [“span”, "ham"] & 2 & 4871 \\ Yelp [45] & restaurant review sentiment classification & [“positive”] & 2 & 30400 \\ IMDB [48] & movie review sentiment classification & [“negative”] & 2 & 20000 \\ MedAbs [48] & medical abstract topic classification & [“neighbors”, “digestive system diseases”, “merous system diseases”, & 5 & 10395 \\ Cancer [14] & biomedical document topic classification & [“carbonic diseases”, “general pathological conditions”] & 3 & 5450 \\ Finance [46] & finance news sentiment classification & [“positive”, “neutral”, “negative”] & 3 & 3488 \\ French [47] & book review sentiment classification & [“positive”, “neutral”, “negative”] & 3 & 6953 \\ Waterbirds [42] & bird species classification & [“landbird”, “waterbird”] & 2 & 5794 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Task descriptions in Alchemist’s prompt are presented.

\begin{table}
\begin{tabular}{l l} \hline \hline Dataset & Zero-shot Prompting (Baseline) \\ \hline YouTube & what is the category of this youtube comment: [text] \\ SMS & what is the category of this sms text: [text] \\ Yelp & what is the sentiment of this restaurant review: [text] \\ IMDb & what is the sentiment of this movie review: [text] \\ MedAbs & what is the topic of this abstract: [text] \\ Cancer & what is the topic of this document: [text] \\ Finance & what is the sentiment of this news: [text] \\ French & what is the sentiment of this book review: [text] \\ \hline \hline \end{tabular}
\end{table}
Table 5: Dataset Table.

First, we show the prompts used for the baseline approach of zero-shot prompting on text datasets in Table 6. In these prompts, the placeholder "[text]" is replaced with individual data points and sent via API calls to obtain labels for each data point.

Next, we present the prompts used in Alchemist in Table 7. The table displays the task description component of each prompt. These descriptions outline the objective of the generated program and are associated with the prediction classes. For the labeling instructions, we directly map the prediction classes to their corresponding class indices and query the language models to output the appropriate class index.

For the image task, we use the prompts ["an image of landbird", "an image of waterbird"] to perform zero-shot prompting using CLIP. In Alchemist, we first query high-level concepts and then combine them with computed scores to prompt LLMs to generate programs. The first step involves the following prompt: "What are the visual primitive concepts to classify "landbird" and "waterbird"? Please organize the primitive concepts by name and use comparisons for the classes. Parse the results into JSON format."

Once we have obtained a set of similarity scores, we use the following prompt: "I have measured similarity scores for the following descriptions as float numbers. If a score is close to 1, it is highly related to the description. If a score is close to 0, it is less related to the description. The descriptions are: ["A bird's foot type is teod, grasping"]; ["A bird's foot type is paddling, swimming"]. Generate a labeling function with input scores to classify landbirds and waterbirds. If it cannot be determined, the function should return -1, but use this cautiously." Descriptions will be replaced by different generated concepts.

## Appendix C Ablation Studies

Alchemist is compatible with a variety of weak supervision aggregation approaches. We report additional results with different choices of label models. Besides Snorkel, we consider three more widely-used label models: Weighted Majority Vote, Dawid-Skene [52], and FlyingSquid (FS) [28]. We reuse our experimental setup from Sec. 4.1 and in Sec. 4.5 and present the performance of the distilled models in Table 8 and in Table 9, respectively.

In Table 8, we observe that the label accuracy is enhanced or achieves comparable performance with different label models, showcasing Alchemist's flexibility in working with various label models. In Table 9, we include compare them with human-crafted labeling functions developed in WRENCH [50]. Similarly, Alchemist obtains higher coverage and achieves comparable or even better label accuracy while reducing the need to craft a large number of programs manually.

Next, we conduct an experiment by varying the temperature in our query APIs and running Alchemist on four different datasets. We train the end model five times with different random seeds and computed the average performance and the variance. Results are shown in Table 10. We observe

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{Youtube} & \multicolumn{2}{c}{SMS} & \multicolumn{2}{c}{Yelp} & IMDB \\ \cline{2-9}  & Est. Cost & Accuracy & Est. Cost & F1-score & Est. Cost & Accuracy & Est. Cost & Accuracy \\ \hline Zero-shot Prompting & 0.096 & 0.871 & 0.240 & 0.907 & 3.873 & **0.845** & 3.400 & **0.737** \\ \hline Weighted Majority Vote & 0.004 & **0.874** & 0.004 & 0.886 & 0.005 & 0.705 & 0.004 & 0.520 \\ Dawid-Skene & 0.004 & 0.864 & 0.004 & 0.895 & 0.005 & 0.682 & 0.004 & 0.507 \\ FlyingSquid & 0.004 & 0.863 & 0.004 & **0.915** & 0.005 & 0.678 & 0.004 & 0.500 \\ Snorkel & 0.004 & **0.891** & 0.004 & 0.900 & 0.005 & 0.575 & 0.004 & 0.662 \\ \hline \hline  & \multicolumn{2}{c}{MedAbs} & \multicolumn{2}{c}{Cancer} & \multicolumn{2}{c}{Finance} & \multicolumn{2}{c}{French} \\ \cline{2-9}  & Est. Cost & Accuracy & Est. Cost & Accuracy & Est. Cost & Accuracy & Est. Cost & Accuracy \\ \hline Zero-shot Prompting & 1.944 & 0.311 & 15.925 & 0.716 & 0.201 & 0.641 & 0.641 & 0.611 \\ \hline Weighted Majority Vote & 0.006 & **0.354** & 0.003 & **0.968** & 0.007 & **0.650** & 0.006 & 0.221 \\ Dawid-Skene & 0.006 & 0.262 & 0.003 & **0.957** & 0.007 & **0.661** & 0.006 & 0.221 \\ FlyingSquid & 0.006 & **0.323** & 0.003 & **0.967** & 0.007 & **0.661** & 0.006 & **0.690** \\ Snorkel & 0.006 & **0.346** & 0.003 & **0.968** & 0.007 & **0.660** & 0.006 & **0.690** \\ \hline \hline \end{tabular}
\end{table}
Table 8: Testing performance of the distilled model is reported for each combination of label model and dataset.

consistent labeling performance across different temperatures (0.0, 0.5, and 1.0), demonstrating Alchemist's stability. Additionally, the stability of generated programs highlights the significance of including aggregation models to handle noisy and diverse outputs, resolve conflicts, and produce final labels.

## Appendix D Richer Modalities

We run Alchemist in two additional modalities in Table 11: time-series (ECG heartbeat classification [53]) and tabular (Census income classification [54]). For ECG heartbeat classification, we generated 10 labeling programs from GPT-4o. For the Census income dataset, we generated program codes for each attribute (e.g., gender, education, age, race). We used Snorkel as our label model. The results demonstrate Alchemist's capability to handle more complex modalities and produce satisfactory performance. We compared Alchemist with human-crafted labeling functions from WRENCH. Alchemist uses fewer labeling functions (programs) and reaches higher labeling performance.

In general, Alchemist will work well with any of these modalities as long as we have access to any cheap local feature extractor. This includes medical imaging tasks: [55] showed manually-crafted simple labeling functions were able to identify heart problems in MRI sequences based on very simple primitives, which could act as the feature extractors for Alchemist.

## Appendix E Improving Robustness

We integrate advancements from existing robustness techniques into Alchemist to further improve accuracy and reduce spurious correlations. Specifically, we consider RoboShot [51], which prompts LLMs for spurious and correct correlation features, then calibrates image embeddings by projecting them to reject or accept concepts. In our setup, we use GPT-4o to identify spurious correlations for

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{**YouTube**} & \multicolumn{4}{c}{**SMS**} & \multicolumn{4}{c}{**French**} & \multicolumn{4}{c}{**Cancer**} \\ \cline{2-13}  & \multicolumn{2}{c}{Label Model} & \multicolumn{2}{c}{End Model} & \multicolumn{2}{c}{Label Model} & \multicolumn{2}{c}{End Model} & \multicolumn{2}{c}{Label Model} & \multicolumn{2}{c}{Label Model} & \multicolumn{2}{c}{Label Model} & \multicolumn{2}{c}{Label Model} & \multicolumn{2}{c}{Label Model} \\ \cline{2-13}  & Mean & Std. & Mean & Std & Mean & Std & Mean & Std. & Mean & Std & Mean & Std & Mean & Std & Mean & Std \\ \hline \multirow{4}{*}{Temperature} & 0.0 & 0.755 & 0.020 & 0.795 & 0.005 & 0.589 & 0.000 & 0.866 & 0.014 & 0.550 & 0.001 & 0.690 & 0.000 & 0.729 & 0.000 & 0.935 & 0.009 \\  & 0.5 & 0.898 & 0.002 & 0.870 & 0.005 & 0.603 & 0.013 & 0.854 & 0.019 & 0.519 & 0.000 & 0.690 & 0.001 & 0.733 & 0.000 & 0.940 & 0.010 \\ \cline{1-1}  & 1.0 & 0.817 & 0.004 & 0.803 & 0.017 & 0.667 & 0.022 & 0.930 & 0.012 & 0.555 & 0.001 & 0.690 & 0.000 & 0.731 & 0.000 & 0.938 & 0.006 \\ \hline \hline \end{tabular}
\end{table}
Table 10: We varied the temperature in GPT-4 API calls, showing **consistent performance** across four datasets. We also trained the end model five times, displaying the average performance and the variance. These results confirm the stability of Alchemist.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{Number of Programs} & Coverage & Weighted Majority Vote & Dawid-Skene & FlyingSquid & Snorkel \\ \hline \multirow{4}{*}{Youtube} & Human-crafted & 10 & 0.89 & 0.88 & 0.84 & 0.87 & 0.85 \\ \cline{2-9}  & GPT-3.5 & 10 & 1.00 & 0.87 & 0.86 & 0.86 & **0.89** \\  & GPT-4 & 10 & 1.00 & 0.85 & **0.88** & **0.87** & **0.89** \\  & Claude 3 & 10 & 1.00 & 0.77 & 0.71 & 0.73 & 0.72 \\ \hline \multirow{4}{*}{SMS} & Human-crafted & 73 & 0.41 & 0.90 & 0.86 & 0.00 & 0.89 \\ \cline{2-9}  & GPT-3.5 & 10 & 1.00 & 0.89 & 0.90 & 0.90 & 0.90 \\  & GPT-4 & 10 & 1.00 & **0.91** & 0.90 & **0.92** & **0.93** \\  & Claude 3 & 10 & 1.00 & **0.91** & **0.92** & **0.92** & 0.89 \\ \hline \multirow{4}{*}{Yelp} & Human-crafted & 8 & 0.83 & 0.75 & 0.83 & 0.77 & 0.76 \\ \cline{2-9}  & GPT-3.5 & 10 & 0.78 & 0.70 & 0.68 & 0.68 & 0.57 \\ \cline{1-1}  & GPT-4 & 10 & 0.99 & 0.73 & **0.81** & 0.72 & 0.82 \\ \cline{1-1}  & Claude 3 & 10 & 0.88 & **0.77** & 0.78 & **0.81** & **0.83** \\ \hline \multirow{4}{*}{IMdb} & Human-crafted & 5 & 0.88 & 0.72 & 0.73 & 0.68 & 0.73 \\ \cline{1-1} \cline{2-9}  & GPT-3.5 & 10 & 0.89 & 0.52 & 0.51 & 0.50 & 0.66 \\ \cline{1-1}  & GPT-4 & 10 & 1.00 & 0.54 & 0.55 & 0.54 & **0.75** \\ \cline{1-1}  & Claude 3 & 10 & 0.98 & 0.59 & 0.64 & 0.60 & 0.70 \\ \hline \hline \end{tabular}
\end{table}
Table 9: We offer a comparison between a wider range of label model options for synthesized programs and those designed by humans.

classifying waterbirds and landbirds, then project image embeddings onto these concept embeddings to reject spurious correlations through subtraction. We compute cosine similarity using the calibrated embeddings to obtain score sets, which are then fed into Alchemist's generated programs. The spurious correlations identified by GPT-4o are ["water background", "land background", "aquatic plants", "trees and bushes"]. Results are displayed in Table 12. This integration using GPT-4o successfully enhances robustness to spurious correlations by improving worst-group accuracies.

## Appendix F Discussion

Techniques to Evaluate Generated Programs.There are many ways to evaluate the quality of generated programs in advance. Expert users can quickly determine whether key problem properties are being used by looking at the code. Besides human inspection, Alchemist includes several automated measurement tools to diagnose generated programs. First, we analyze program outputs to compute coverage, polarity, conflict, and overlap (see 3 for definitions). For example, if coverage (the fraction of data points with at least one label) is below 10%, we discard the program and ask users to generate a new one. Moreover, if a validation dataset is available, Alchemist can run diagnostics to empirically compare accuracy with ground truth, offering more insight into the program's reliability. This data-driven feedback loop ensures tractable program generation. Notably, these tools are not typically accessible with model-based annotation methods.

Footnote 3: https://snorkel.readthedocs.io/en/v0.9.3/packages/_autosummary/labeling/snorkel.labeling.LFAnalysis.html

Limitations.There are two primary limitations in Alchemist. First, the performance of the datasets we test is still dependent on the capabilities of the language model. If the language model's ability to comprehend the given task and generate effective programs is subpar, the labeling performance may suffer. The second limitation arises when dealing with extremely complex tasks. As the complexity of the task increases, the generated code may become longer, more intricate, and harder to understand, posing challenges for developers who take time to validate correctness.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & **ECG Heartbeat Classification** & **Census Income Classification** \\ \cline{2-4}  & Human Crafted & Synthesized by GPT4o & Human Crafted & Synthesized by GPT4o \\ \hline Modality & **Time-Series** & \multicolumn{2}{c}{**Tabular**} \\ \# of Train Set / \# of Test Set & 87554 / 21892 & \multicolumn{2}{c}{30162 / 15060} \\ \# of Classes & 5 & \multicolumn{2}{c}{2} \\ \hline \# of Programs & — & 10 & 83 & **13** \\ Label Model Accuracy & — & **0.827** & 0.681 & **0.725** \\ \hline \hline \end{tabular}
\end{table}
Table 11: We included two new modalities in our evaluation: **time-series and tabular data.** Both performed well using a few generated programs. In the Census dataset, Alchemist outperformed human-crafted labeling functions in WRENCH.

\begin{table}
\begin{tabular}{l l c c c} \hline \hline Feature Extractor & Method & Average Accuracy (\(\uparrow\)) & Worst Group Accuracy (\(\uparrow\)) & Gap (\(\downarrow\)) \\ \hline CLIP ViT-B/32 & Zero-shot Prompting & 0.820 & 0.318 & 0.502 \\  & Group Prompting & 0.823 & 0.383 & 0.440 \\ \cline{2-5}  & **Alchemist with GPT4o** & 0.805 & 0.283 & 0.522 \\  & **RoboShot + Alchemist with GPT4o** & 0.700 & **0.375** & **0.325** \\ \hline CLIP ViT-L/14 & Zero-shot Prompting & 0.904 & 0.335 & 0.569 \\  & Group Prompting & 0.791 & 0.240 & 0.551 \\ \cline{2-5}  & **Alchemist with GPT4o** & 0.802 & 0.467 & 0.335 \\  & **RoboShot + Alchemist with GPT4o** & **0.803** & **0.569** & **0.234** \\ \hline \hline \end{tabular}
\end{table}
Table 12: We integrate Roboshot into Alchemist by querying GPT-4 for spurious correlation features and rejecting them, then reusing the generated programs in Alchemist. This integration improves average accuracy and worst-group performance, **enhancing robustness to spurious correlations.**Broader Impacts.We do not see explicit negative impacts in Alchemist's annotation process. However, generated programs from language models may contain biased labeling logic, toxic content, or malicious functions. To mitigate this, auditing and guardrails may be necessary.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Yes. Our claims are accurately reflect our contributions in data annotation and its scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: Yes. We discuss the limitations of the work in F. Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: This paper is an empirical work. It does not apply to this paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Yes. We report our training settings, and used prompts in the Appendix (See Appendix A and Appendix B). Guidelines:* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Yes. We release our code and data here: :https://github.com/SproketLab/Alchemist.. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We discuss training and testing details in our experiment sections and in Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Re-prompting labels for data points may create a huge expense. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We place the information about computer resources in the Appendix A. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: It follows NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss them and place in Appendix F. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This doesn't apply to this paper.

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Yes. We properly credit data, paper, and ideas that we used in this paper. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We document well about the asset. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: This doesn't apply to this work. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper doesn't involve crowdsourcing and research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.