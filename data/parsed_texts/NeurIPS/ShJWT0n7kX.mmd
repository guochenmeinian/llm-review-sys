# Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling

 Yuanqi Du\({}^{*}\)\({}^{1}\) &Michael Plainer\({}^{*}\)\({}^{2,3,4,5}\) &Rob Brekelmans\({}^{*}\)\({}^{6}\) &Chenru Duan \({}^{7,8}\)

Frank Noe \({}^{4,9,10}\) &Carla P. Gomes \({}^{1}\) &Alan Aspuru-Guzik \({}^{6,11}\) &Kirill Neklyudov \({}^{12,13}\)

\({}^{1}\)Cornell University \({}^{2}\)Zuse School ELIZA \({}^{3}\)Technische Universitat Berlin

\({}^{4}\)Freie Universitat Berlin \({}^{5}\)Berlin Institute for the Foundations of Learning and Data

\({}^{6}\)Vector Institute \({}^{7}\)Massachusetts Institute of Technology \({}^{8}\)Deep Principle, Inc.

\({}^{9}\)Rice University \({}^{10}\)Microsoft Research AI4Science \({}^{11}\)University of Toronto

\({}^{12}\)Universite de Montreal \({}^{13}\)Mila Quebec AI Institute

Equal contribution. Correspondence: k.necludov@gmail.com (Kirill Neklyudov)

###### Abstract

Rare event sampling in dynamical systems is a fundamental problem arising in the natural sciences, which poses significant computational challenges due to an exponentially large space of trajectories. For settings where the dynamical system of interest follows a Brownian motion with known drift, the question of conditioning the process to reach a given endpoint or desired rare event is definitively answered by Doob's \(h\)-transform. However, the naive estimation of this transform is infeasible, as it requires simulating sufficiently many forward trajectories to estimate rare event probabilities. In this work, we propose a variational formulation of Doob's \(h\)-transform as an optimization problem over trajectories between a given initial point and the desired ending point. To solve this optimization, we propose a simulation-free training objective with a model parameterization that imposes the desired boundary conditions by design. Our approach significantly reduces the search space over trajectories and avoids expensive trajectory simulation and inefficient importance sampling estimators which are required in existing methods. We demonstrate the ability of our method to find feasible transition paths on real-world molecular simulation and protein folding tasks.

## 1 Introduction

Conditioning a stochastic process to obey a particular endpoint distribution, satisfy desired terminal conditions, or observe a rare event is a problem with a long history (Schrodinger, 1932; Doob, 1957) and wide-ranging applications from generative modeling (De Bortoli et al., 2021; Chen et al., 2021; Liu et al., 2022, 2023; Somnath et al., 2023) to molecular simulation (Anderson, 2007; Wu et al., 2022; Plainer et al., 2023; Holdijk et al., 2023), drug discovery (Kirmizialtin et al., 2012, 2015; Dickson, 2018), and materials science (Xi et al., 2013; Selli et al., 2016; Sharma et al., 2016).

**Transition Path Sampling.** In this work, we take a particular interest in the problem of _transition path sampling_ (TPS) in computational chemistry (Dellago et al., 2002; Weinan and Vanden-Eijnden, 2010), which attempts to describe how molecules transition between local energy minima or metastable states under random fluctuations or the influence of external forces. Understanding such transitions has numerous applications for combustion, catalysis, battery, material design, and protein folding (Zeng et al., 2020; Klucznik et al., 2024; Blau et al., 2021; Noe et al., 2009; Escobedo et al., 2009).

While the TPS problem is often framed as finding the'most probable path' transitioning between states (Durr and Bach, 1978; Vanden-Eijnden and Heymann, 2008), we build upon connections between TPS and Doob's \(h\)-transform (Das et al., 2019, 2021, 2022; Koehl and Orland, 2022; Singh and Limmer, 2023) and seek to match the _full_ posterior distribution over conditioned processes.

**Doob's \(h\)-Transform.** For Brownian motion diffusion processes, conditioning is known to be achieved by Doob's \(h\)-transform (Doob, 1957; Sarkka and Solin, 2019). However, solving this problem amounts to estimating rare event probabilities or matching a complex target distribution. Approaches which involve simulation of trajectories to construct Monte Carlo expectations or importance sampling estimators (Papaspiliopoulos and Roberts, 2012; Schauer et al., 2017; Yan et al., 2022; Holdijk et al., 2023) can be extremely inefficient if the target event is rare or endpoint distribution is difficult to match. Recent methods based on score matching (Heng et al., 2021) or nonlinear Feynman-Kac formula (Chopin et al., 2023) still require simulation during optimization.

**Variational Formulation of Doob's \(h\)-Transform.** In this work, we propose a variational formulation of Doob's \(h\)-transform as the solution to an optimization on the space of paths of probability distributions. We focus on solving for the Doob transform conditioning on a particular terminal point, which is natural in the TPS setting (see Fig. 1). Taking inspiration from recent bridge matching methods (Peluchetti, 2021, 2023; Liu et al., 2022; Lipman et al., 2022; Shi et al., 2023; Liu et al., 2023a), we propose a parameterization with the following attractive features.

1. **Every Sample Matters.** In contrast to most existing approaches, our training method is _simulation-free_, thereby avoiding computationally wasteful simulation methods to estimate rare-event probabilities and inefficient importance or rejection sampling. We thus refer to our approach as being _sample-efficient_.
2. **Optimization over Sampling.** We propose an expressive variational family of approximations to the conditioned process, which are tractable to sample and can be optimized using neural networks with end-to-end backpropagation.
3. **Problem-Informed Parameterization.** Our parameterization enforces the boundary conditions _by design_, thereby reducing the search space for optimization and efficiently making use of the conditioning information.

We begin by linking the problem of transition path sampling to the Doob's \(h\)-transform and recalling background results in Sec. 2. We present our variational formulation in Sec. 3.1 and detail our optimization algorithm throughout Sec. 3.2. We demonstrate the ability of our approach to achieve comparable performance to Markov Chain Monte Carlo (MCMC) methods with notably improved efficiency on synthetic, and real-world molecular simulation tasks in Sec. 5.

## 2 Background

### Transition Path Sampling

Consider a forward or reference stochastic process with states \(x_{t}\) and the density of transition probability \(\rho_{t+dt}(y|x_{t}=x)\coloneqq\rho(x_{t+dt}=y\,|\,x_{t}=x)\). Starting from an initial point \(x_{0}=A\), the probability density of a discrete-time path is given as

\[\rho(x_{T},\ldots,x_{dt}\,|\,x_{0}=A)=\prod_{t=dt}^{T-dt}\rho(x_{t+dt}\,|\,x_{ t})\cdot\rho(x_{dt}\,|\,x_{0}=A).\] (1)

The problem of rare event sampling aims to condition this reference stochastic process on some event at time \(T\), for example, that the final state belongs to a particular set \(x_{T}\in\mathcal{B}\). We are interested in

Figure 1: Given reference dynamics, transition path sampling seeks to capture the conditional or posterior distribution over paths which reach a terminal set \(x_{T}\in\mathcal{B}\). However, simulating the reference dynamics (blue) can be wasteful since we rarely obtain paths (orange) which reach (the vicinity of) the terminal set \(\mathcal{B}\). This is a major challenge for techniques based on importance sampling or Monte Carlo estimation, even when adding a control term to the reference dynamics. By contrast, our approach optimizes a tractable variational distribution over transition paths with a parameterization which satisfies the initial and terminal conditions by design.

sampling from the entire _transition path_, namely the posterior distribution over intermediate states

\[\rho(x_{T-dt},\ldots,x_{dt}\,|\,x_{0}=A,x_{T}\in\mathcal{B})=\frac{\rho(x_{T}\in \mathcal{B},x_{T-dt}\,\ldots,x_{dt}\,|\,x_{0}=A)}{\rho(x_{T}\in\mathcal{B}\,|\,x _{0}=A)}.\] (2)

Moving to continuous time, we focus on the transition path sampling problem in the case where the reference process is given by a Brownian motion. In particular, we are motivated by applications in computational chemistry (Dellago et al., 2002; Weinan and Vanden-Eijnden, 2010), where the reference process is given by molecular dynamics following either overdamped Langevin dynamics,

\[dx_{t}=-(\gamma M)^{-1}\nabla_{x}U(x_{t})\cdot dt+(\gamma M)^{- \nicefrac{{1}}{{2}}}\sqrt{2k_{B}\mathcal{T}}\cdot dW_{t}\,,\] (3)

or the second-order Langevin dynamics with spatial coordinates \(\bar{x}_{t}\) and velocities \(\bar{v}_{t}\),

\[d\bar{x}_{t}\,=\bar{v}_{t}\cdot dt\,,\qquad d\bar{v}_{t}\,=\Big{(}-M^{-1} \nabla_{x}U(\bar{x}_{t})-\gamma\bar{v}_{t}\Big{)}\cdot dt+M^{-\nicefrac{{1}}{ {2}}}\sqrt{2\gamma k_{B}\mathcal{T}}\cdot dW_{t}\,.\] (4)

for a potential energy function \(U\), where \(W_{t}\) denotes the Wiener process. Note that \(k_{B}\mathcal{T}\) is the Boltzman constant times temperature, \(M\) is the mass matrix, and \(\gamma\) is the friction coefficient.

### Doob's \(h\)-transform

Doob's \(h\)-transform addresses the question of conditioning a reference Brownian motion to satisfy a terminal condition such as \(x_{T}\in\mathcal{B}\), thereby providing an avenue to solve the transition path sampling problem described above. Without loss of generality, and to provide a unified treatment of the dynamics in (3)-(4), we consider the forward or reference stochastic differential equation (SDE),

\[\mathbb{P}^{\text{ref}}_{0:T}:\qquad\qquad dx_{t}=b_{t}(x_{t}) \cdot dt+\Xi_{t}\ dW_{t}\,,\qquad\qquad x_{0}\sim\rho_{0}\,,\] (5)

with drift vector field \(b_{t}:\mathbb{R}^{N}\to\mathbb{R}^{N}\) and diffusion coefficient matrix \(\Xi_{t}\in\mathbb{R}^{N\times N}\) such that \(G_{t}:=\frac{1}{2}\Xi_{t}\Xi_{t}^{T}\) is positive definite.2

Footnote 2: Note that the second-order dynamics in (4) can be represented using

\[x_{t}=\begin{bmatrix}\bar{x}_{t}\\ \bar{v}_{t}\end{bmatrix},\quad\ b_{t}(x_{t})=\begin{bmatrix}\bar{v}_{t}\\ -M^{-1}\nabla_{x}U(\bar{x}_{t})-\gamma\bar{v}_{t}\end{bmatrix},\quad\ G_{t}= \begin{bmatrix}0\\ 0\end{bmatrix}\begin{bmatrix}0\\ M^{-\nicefrac{{1}}{{2}}}\sqrt{2\gamma k_{B}\mathcal{T}}\end{bmatrix}.\]

 We denote the induced path measure as \(\mathbb{P}^{\text{ref}}_{0:T}\in\mathcal{P}(\mathcal{C}([0,T]\to\mathbb{R}^{N}))\), i.e. a measure over continuous functions from time to \(\mathbb{R}^{N}\).

Remarkably, Doob's \(h\)-transform (Doob, 1957; Sarkka and Solin, 2019, Sec. 7.5) shows that conditioning the reference process (5) on \(x_{T}\in\mathcal{B}\) results in another Brownian motion process.

**Proposition 1**.: [Jamison (1975, Thm. 2)] _Let \(h_{\text{B}}(x,t)\coloneqq\rho_{T}(x_{T}\in\mathcal{B}\,|\,x_{t}=x)\) denote the conditional transition probability of the reference process in (5). Then,_

\[\mathbb{P}^{*}_{0:T}:\qquad dx_{t|T}=\Big{(}b_{t}(x_{t|T})+2G_{t} \nabla_{x}\log h_{\text{B}}(x_{t|T},t)\Big{)}\cdot dt+\Xi_{t}\ dW_{t}\qquad x _{0}\sim\rho_{0}\] (6)

_where we use \(x_{t|T}\) to denote a conditional process. The SDE in (6) is associated with the following transition probabilities for \(s<t<T\),_

\[\rho_{t}(y\,|\,x_{s}=x,x_{T}\in\mathcal{B})=\frac{h_{\mathcal{B}}(y,t)}{h_{ \mathcal{B}}(x,s)}\rho_{t}(y\,|\,x_{s}=x),\] (7)

_Note that all of our subsequent results hold for the case when \(\mathcal{B}\) is a point-mass, with the only change being that the \(h\)-function becomes a density, \(h_{B}(x,t)=\rho_{T}(B\,|\,x_{t}=x)\)._

See App. A.1 for proof, and note that (7) is simply an application of Bayes rule \(\rho_{t}(y\,|\,x_{s}=x,x_{T}\in\mathcal{B})=\rho_{T}(x_{T}\in\mathcal{B}|x_{t}= y)\rho_{t}(y\,|\,x_{s}=x)/\rho_{T}(x_{T}\in\mathcal{B}|x_{s}=x)\) with the unconditioned or reference transition probability as the prior. Furthermore, the conditioned transition probabilities in (7) allow us to directly construct the transition path (2). Using Bayes rule, we have

\[\rho(x_{T-dt},\ldots,x_{dt}\,|\,x_{0}=A,x_{T}\in\mathcal{B})=\frac{h_{\mathcal{ B}}(x_{T-dt},T-dt)}{h_{\text{B}}(A,0)}\rho(x_{T-dt}\ldots,x_{dt}\,|\,x_{0}=A)\]

after telescoping cancellation of \(h\)-functions and rewriting the denominator in (2) as \(h_{\text{B}}(A,0)\) Thus, we can solve the TPS problem by exactly solving for the \(h\)-function and simulating the SDE in (6).

Finally, the \(h\)-process and temporal marginals \(\rho_{t}(x|x_{0}=A,x_{T}\in\mathcal{B})\) of the conditioned process satisfy the following forward and backward Kolmogorov equations, which will be useful in deriving our variational objectives in the next section. Note, we use \(\langle\nabla_{x},\star\rangle=\text{div}(\star)\) for the divergence operator, and we use \(\rho_{t|0,T}\) to indicate the dependence on both \(x_{0}=A\) (via the initial condition of (8a)) and \(x_{T}\in\mathcal{B}\) (via the \(h\)-transform \(h_{\text{B}}\)). See App. A.1 for the proof.

**Proposition 2**.: _The following PDEs are obeyed by (a) the density of the conditioned process \(\rho_{t|0,T}(x)\coloneqq\rho_{t}(x\,|\,x_{0}=A,x_{T}\in\mathcal{B})\) and (b) the \(h\)-function \(h_{\mathcal{B}}(x,t)\),_

\[\frac{\partial\rho_{t|0,T}(x)}{\partial t}+\big{\langle}\nabla_{x },\rho_{t|0,T}(x)\big{(}b_{t}(x)+2G_{t}\nabla_{x}\log h_{\mathcal{B}}(x,t) \big{)}\big{\rangle}-\sum_{ij}(G_{t})_{ij}\frac{\partial^{2}}{\partial x_{i} \partial x_{j}}\rho_{t|0,T}(x)=0\,,\] (8a) \[\frac{\partial h_{\mathcal{B}}(x,t)}{\partial t}+\big{\langle} \nabla_{x}h_{\mathcal{B}}(x,t),b_{t}(x)\big{\rangle}+\sum_{ij}(G_{t})_{ij} \frac{\partial^{2}}{\partial x_{i}\partial x_{j}}h_{\mathcal{B}}(x,t)=0\,.\] (8b)

_Reparameterizing (8b) in terms of \(s_{B}(x,t)\coloneqq\log h_{\mathcal{B}}(x,t)\), we can also write_

\[\frac{\partial s_{\mathcal{B}}(x,t)}{\partial t}+\big{\langle} \nabla s_{\mathcal{B}}(x,t),G_{t}\nabla s_{\mathcal{B}}(x,t)\big{\rangle}+ \big{\langle}\nabla s_{\mathcal{B}}(x,t),b_{t}(x)\big{\rangle}+\sum_{ij}(G_{ t})_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}s_{\mathcal{B}}(x,t)=0.\] (8c)

## 3 Method

We first present a novel variational objective whose minimum corresponds to the Doob \(h\)-transform in Sec. 3.1, and then propose an efficient parameterization to solve for the \(h\)-transform in Sec. 3.2.

### Doob's Lagrangian

Consider reference dynamics given in the form of either (3) or (4), with known drift \(b_{t}\) or energy \(U\). We will restrict our attention to conditioning on a terminal rare event of reaching a given endpoint \(x_{T}=B\), along with an initial point \(x_{0}=A\). We approach solving for Doob's \(h\)-transform via a _least action principle_ where, in the following theorem, we define a Lagrangian action whose minimization yields the optimal \(q_{t|0,T}^{*}(x)=\rho_{t|0,T}(x)\) and \(v_{t|0,T}^{*}(x)=\nabla_{x}\log h_{B}(x,t)\) from Prop. 1 and 2.

**Theorem 1**.: _The following Lagrangian action functional has a unique solution which matches the Doob \(h\)-transform in Prop. 2,_

\[\mathcal{S}= \min_{q,v}\int_{0}^{T}dt\ \int dx\ q_{t|0,T}(x)\big{\langle}v_{t|0,T}(x ),G_{t}\ v_{t|0,T}(x)\big{\rangle}\,,\] (9a) \[\text{s.t.} \frac{\partial q_{t|0,T}(x)}{\partial t}=-\big{\langle}\nabla_{x },q_{t|0,T}(x)\big{(}b_{t}(x)+2G_{t}\ v_{t|0,T}(x)\big{)}\big{\rangle}+\sum_{ ij}(G_{t})_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}q_{t|0,T}(x),\] (9b) \[q_{0}(x)=\delta(x-A),\qquad q_{T}(x)=\delta(x-B)\,.\] (9c)

_The optimal \(q_{t|0,T}^{*}(x)\) obeys (8a), and \(v_{t|0,T}^{*}(x)=\nabla_{x}\log h_{B}(x,t)=\nabla_{x}s_{B}(x,t)\) obeys (8b)-(8c)._

This objective will form the basis for our computational approach, with proof of Thm. 1 deferred to App. A.2. We proceed briefly to contextualize our variational objective and highlight several optimization challenges which will be solved by our proposed parameterization in Sec. 3.2.

**Unconstrained Dual Objective.** Introducing Lagrange multipliers to enforce the constraints in (9b)\(\lnot\)(9c) and eliminating \(v_{t|0,T}\), we obtain an alternative, unconstrained version of (9a).

**Corollary 1**.: _The Lagrangian objective in Thm. 1 which solves Doob's \(h\)-transform is equivalent to_

\[\mathcal{S}=\min_{q_{t|0,T}}\max_{z}\ s_{B}(B,T)-s_{B}(A,0)-\int_{0}^{T}\!\!dt \int\!dx\ q_{t|0,T}\Big{(}\frac{\partial s_{B}}{\partial t}+\big{\langle} \nabla s_{B},G_{t}\nabla s_{B}\big{\rangle}+\big{\langle}\nabla s_{B},b_{t} \big{\rangle}+\big{\langle}\nabla,G_{t}\nabla s_{B}\big{\rangle}\Big{)}\]

_if \(q_{t|0,T}\) satisfies (9c). Note \(v_{t|0,T}(x)=\nabla_{x}s_{B}(x,t)\), with \(s_{B}^{*}(x,t)=\log h_{B}(x,t)\) at optimality. 3_

Footnote 3: Compared to (8c), we write \(\sum_{ij}(G_{t})_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}s_{ \mathcal{B}}(x,t)=\langle\nabla,G_{t}\nabla s_{\mathcal{B}}(x,t)\rangle\) for simplicity of notation.

This objective is similar to the objectives optimized by Action Matching methods (Neklyudov et al., 2023, 2024). Notably, the objective in Cor. 1 is expressed _directly_ in terms of the (log) of the \(h\)-function for fixed conditioning information \(x_{T}=B\). We also note that the Hamilton Jacobi-style quantity, whose expectation appears in the final term, is zero at optimality in (8c) of Prop. 2.

**Path Measure Perspective.** We next relate our variational objective in Thm.1 to a KL divergence optimization over path measures. Let \(\mathbb{P}^{\text{ref}}_{0:T}\) denote the law of the reference SDE in (5) with fixed \(\mathbb{P}^{\text{ref}}_{0}=\delta(x_{0}-A)\). Let \(\mathbb{Q}^{v}_{0:T}\) denote the law of a controlled process similar to (6), but with a variational \(v_{t|0,T}\) in place of \(\nabla_{x}\log h_{\mathcal{B}}\),

\[\mathbb{Q}^{v}_{0:T}:\qquad dx_{t}=\left(b_{t}(x_{t|T})+2G_{t}\;v_{t|0,T}(x_{ t|T})\right)\cdot dt+\Xi_{t}\;dW_{t}\,,\qquad x_{0}=A.\] (10)

Note that the density \(q_{t|0,T}\) of \(\mathbb{Q}^{v}_{0:T}\) evolve according to the Fokker-Planck equation in (9b)(Sarkka and Solin, 2019, Sec. 5.2). Using the Girsanov Theorem, the objective in (9a) can then be viewed as a KL divergence minimization over path measures \(\mathbb{Q}^{v}_{0:T}\) which satisfy the boundary constraints.

**Corollary 2**.: _The following Schrodinger Bridge (SB) problem_

\[\mathcal{S}\coloneqq\min_{\mathbb{Q}^{v}_{0:T}\text{ s.t. }\mathbb{Q}^{v}_{0}=\delta_{A},\mathbb{Q}^{v}_{T}=\delta_{B}}D_{KL}[ \mathbb{Q}^{v}_{0:T}:\mathbb{P}^{\text{ref}}_{0:T}]\] (11)

_yields the path measure \(\mathbb{P}^{*}_{0:T}\) associated with the SDE in (6) as its unique minimizing argument. The temporal marginals of \(\mathbb{P}^{*}_{0:T}\) are equal to those which optimize the Lagrangian objective in Thm.1._

Our Lagrangian action minimization thus corresponds to the solution of an SB problem (Schrodinger, 1932; Leonard, 2014) with Dirac delta functions as the endpoint measures. Our objective in (9a) particularly resembles optimal control formulations of SB (Chen et al., 2016, 2021b, Prob. 4.4, 5.3). While it is well-known that the Doob \(h\)-transform (and large deviation theory more generally) plays a role in the solution to SB problems (Jamison, 1975; Leonard, 2014), our interest in the transition path sampling problem leads to specific computational decisions below. See Sec.4 for further discussion.

**Challenges of Optimizing (9a).** We highlight several distinctive features of our problem which inform the development of new computational methods in Sec.3.2.

1. First, we perform optimization over the _first_ argument of the KL divergence in (11), indicating that we need to be able to efficiently sample from the conditioned process in (10) or \(q_{t|0,T}\) in (9). This appears challenging due to the nonlinearity of both the reference and variational drifts, \(b_{t}\) and \(v_{t|0,T}\).
2. For a given \(q_{t|0,T}\), it can be difficult to solve for \(v_{t|0,T}\) which satisfies the Fokker-Planck equation in (9b) or \(\nabla s\) which solves the inner optimization in Cor.1.
3. Finally, we would like to strictly enforce the boundary constraints on \(q_{t|0,T}\) or \(\mathbb{Q}^{v}_{0:T}\) to avoid simulating or wasting computation on trajectories for which \(x_{T}\neq B\).

In fact, our parameterization of \(q_{t|0,T}\) in Sec.3.2 will _completely avoid_ simulation of the SDE in (10) during training (Challenge 1), provide _analytic_ solutions for \(v_{t|0,T}\) satisfying (9b) with a given \(q_{t|0,T}\) (Challenge 2), and _exactly_ enforce the boundary constraints (Challenge 3).

### Computational Approach

We now propose a family of Gaussian (mixture) path parameterizations \(q_{t|0,T}\) which overcome the computational challenges posed in the previous section, while still maintaining expressivity. We present all aspects of our proposed method in the context of the first-order dynamics (3) in Sec.3.2.1, before presenting extensions to mixture paths and the second-order setting (4) in Sec.3.2.2-3.2.3.

#### 3.2.1 First-Order Dynamics and General Approach

**Tractable Drift \(v_{t|0,T}\) for Variational Doob Objective.** We begin by considering a modification of the Fokker-Planck constraint in (9b), with all drift terms absorbed into a single vector field \(u_{t|0,T}\),

\[\frac{\partial q_{t|0,T}(x)}{\partial t}=-\Big{\langle}\nabla_{x},q_{t|0,T}(x )\;u_{t|0,T}(x)\Big{\rangle}+\sum_{ij}(G_{t})_{ij}\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}q_{t|0,T}(x).\] (12)

For arbitrary \(q_{t|0,T}\), solving for _any_\(u_{t|0,T}(x)\) satisfying (12) can be a difficult optimization problem, whose solution is not unique without some cost-minimizing assumption (Neklyudov et al., 2023).

To sidestep this optimization, and address Challenge2, we restrict attention to variational families of \(q_{t|0,T}\in\mathcal{Q}\) where it is _analytically tractable_ to calculate a vector field \(u^{(q,\theta)}_{t|0,T}\) which satisfies (12). We first consider the family of Gaussian paths \(\mathcal{Q}_{G}\), in similar fashion to (conditional) flow matching methods (Lipman et al., 2022; Tong et al., 2023; Liu et al., 2023a), with proof in App.B.

**Proposition 3**.: _For the family of endpoint-conditioned marginals \(q_{t|0,T}(x)=\mathcal{N}(x\,|\,\mu_{t|0,T},\Sigma_{t|0,T})\),_

\[u_{t|0,T}^{(q,\theta)}(x)\coloneqq\frac{\partial\mu_{t|0,T}}{ \partial t}+\bigg{[}\frac{1}{2}\frac{\partial\Sigma_{t|0,T}}{\partial t}\Sigma_ {t|0,T}^{-1}-G_{t}\;\Sigma_{t|0,T}^{-1}\bigg{]}\big{(}x-\mu_{t|0,T}\big{)}\] (13)

_satisfies the Fokker-Planck equation (12) for \(q_{t|0,T}\) and diffusion coefficients \(G_{t}=\frac{1}{2}\Xi_{t}\Xi_{t}^{T}\)._

Given \(u_{t|0,T}^{(q,\theta)}\) corresponding to \(q_{t|0,T}\), we can simply solve for the \(v_{t|0,T}\) satisfying the Fokker-Planck equation in (9b) in our variational Doob objective (Thm. 1). Since \(G_{t}\) was assumed to be invertible and the base drift \(b_{t}\) is known, we have

\[v_{t|0,T}^{(q,\theta)}(x)=\frac{1}{2}(G_{t})^{-1}\Big{(}u_{t|0,T}^{(q,\theta) }(x)-b_{t}(x)\Big{)}.\] (14)

We may now evaluate terms involving \(v_{t|0,T}\) in our Lagrangian objective in (9) using (14) directly, without spending effort to solve an inner minimization over \(v_{t|0,T}\) (thus addressing Challenge 2).

**Optimization over \(q_{t|0,T}\) satisfying Boundary Constraints.** Given the ability to evaluate \(v_{t|0,T}^{(q,\theta)}\) for a given \(q_{t|0,T}\in\mathcal{Q}_{G}\) as above, our variational Doob objective in (9a) reduces to a single optimization over the marginals \(q_{t|0,T}\) of a conditioned process which satisfies the boundary conditions (9c).

We consider parameterizing the mean \(\mu_{t|0,T}\) and covariance \(\Sigma_{t|0,T}\) of our Gaussian path \(q_{t|0,T}\) using a neural network. For simplicity, we consider a diagonal parameterization \(\Sigma_{t|0,T}=\texttt{diag}(\{\sigma_{t|0,T,d}^{2}\}_{d=1}^{D})\). We parameterize a neural network \(\texttt{nnet}_{\theta}:[0,T]\times\mathbb{R}^{D}\times\mathbb{R}^{D}\to \mathbb{R}^{D}\times\mathbb{R}^{D}\) which inputs time \(t\) and boundary conditions \(x_{0}=A,x_{T}=B\), and outputs vectors of mean perturbations and per-dimension variances. Finally, using index notation to separate the output, we construct

\[x_{t|0,T} =\mu_{t|0,T}^{(\theta)}+\Sigma_{t|0,T}^{(\theta)}\;\epsilon,\quad \text{where}\quad\epsilon\sim\mathcal{N}(0,\mathbb{I}_{D}).\] (15a) \[\mu_{t|0,T}^{(\theta)} =\Big{(}1-\frac{t}{T}\Big{)}A+\frac{t}{T}\;B+\frac{t}{T}\Big{(}1- \frac{t}{T}\Big{)}\texttt{nnet}_{\theta}(t,A,B)_{[:D]}\] (15b) \[\Sigma_{t|0,T}^{(\theta)} =\frac{t}{T}\bigg{(}1-\frac{t}{T}\bigg{)}\texttt{diag}\big{(} \texttt{nnet}_{\theta}(t,A,B)_{[D:]}\big{)}+\sigma_{\text{min}}^{2}\mathbb{I} _{D}.\] (15c)

Crucially, our Gaussian parameterization addresses Challenge 1, in that we can easily draw samples \(x_{t|0,T}\sim q_{t|0,T}\) from our variational conditioned process (9b) _without simulating_ the corresponding SDE with nonlinear drift (10). Further, the coefficients in (15b) and (15c) ensure that, as \(t\to 0\) or \(t\to T\), our parameterization satisfies the (smoothed) boundary conditions by design (Challenge 3). Although we add \(\sigma_{\text{min}}^{2}\) to ensure invertibility of \(\Sigma_{t|0,T}\) (see (13)), we preserve \(q_{0}(x_{0})=\mathcal{N}(x_{0}\,|\,A,\sigma_{\text{min}}^{2}\mathbb{I}_{D}) \approx\delta(x_{0}-A)\) and \(q_{T}(x_{T})=\mathcal{N}(x_{T}\,|\,B,\sigma_{\text{min}}^{2}\mathbb{I}_{D}) \approx\delta(x_{T}-B)\).

**Reparameterization Gradients.** Having shown that our parameterization satisfies the constraints (9b)-(9c) by design, we can finally optimize our variational Doob objective with respect to \(q_{t|0,T}\in\mathcal{Q}_{G}\) using the reparameterization trick (Kingma and Welling, 2013; Rezende et al., 2014). In particular, for the expectation at each \(t\) in (9a), we rewrite

\[\nabla_{\theta}\mathbb{E}_{q_{t|0,T}^{(\theta)}(x)}\Big{[}\Big{\langle}v_{t|0,T }^{(q,\theta)}(x),G_{t}\;v_{t|0,T}^{(q,\theta)}(x)\Big{\rangle}\Big{]}=\mathbb{ E}_{\mathcal{N}(\epsilon|0,1_{D})}\Big{[}\nabla_{\theta}\Big{\langle}v_{t|0,T}^{(q, \theta)}\big{(}g(t,\epsilon;\theta)\big{)},G_{t}\;v_{t|0,T}^{(q,\theta)} \big{(}g(t,\epsilon;\theta)\big{)}\Big{\rangle}\Big{]},\]

where \(x=g(t,\epsilon;\theta)\) is the mapping in (15) and \(v_{t|0,T}^{(q,\theta)}\) depends on \(\theta\) via \(\mu_{t|0,T}^{(\theta)}\), \(\Sigma_{t|0,T}^{(\theta)}\) in (13)-(14).

**Full Training Algorithm.** In practice, we sample a batch of times \(\{t_{i}\}_{i=1}^{M}\) uniformly from the interval \(t\in[0,T]\). For each time point, we approximate the gradient using a single-sample estimate of the expectation above (or (9)), which yields a simulation-free training procedure. The full training algorithm is outlined in Alg. 1.

**Sampling of Trajectories.** While we sample directly from \(q_{t|0,T}^{(\theta)}\) during training, we can sample full trajectories which obey this sequence of marginals at test time (Alg. 2). In particular, we simulate SDE trajectories with drift \(u_{t|0,T}^{(q,\theta)}(x)\) and diffusion coefficient \(G_{t}\) using an appropriate solver. Note that this generation scheme sidesteps computationally expensive evaluation of the force field or base drift \(b_{t}(x_{t})\). We visualize example sampling trajectories in Fig. 2.

#### 3.2.2 Second-Order Dynamics

To handle the case of the second-order dynamics in (4), we can adapt our recipe from the previous section with minimal modifications by extending the state space \(x\in\mathbb{R}^{D}\) to include velocities \(\bar{v}\), with \(x=(\bar{x},\bar{v})\in\mathbb{R}^{2D}\). However, note that the dynamics in (4) are no longer stochastic in the spatial coordinates \(\bar{x}\). To ensure invertibility of \(G_{t}\) and existence of the \(h\)-transform, we add a small nonzero diffusion coefficient in the coordinate space \(\bar{x}\), so that the reference process in Eq. (5) is given by

\[x_{t}=\begin{bmatrix}\bar{x}_{t}\\ \bar{v}_{t}\end{bmatrix},\quad b_{t}(x_{t})=\begin{bmatrix}\bar{v}_{t}\\ -M^{-1}\nabla_{x}U(\bar{x}_{t})-\gamma\bar{v}_{t}\end{bmatrix},\quad\Xi_{t}= \begin{bmatrix}\xi_{\text{min}}\mathbb{I}_{D}&0\\ 0&M^{-\nicefrac{{1}}{{2}}}\sqrt{2\gamma k_{B}T}\end{bmatrix}.\] (16)

All steps in our algorithm proceed in similar fashion to Sec. 3.2.1. We now parameterize \(q_{t|0,T}(\bar{x},\bar{v})\) using \(\textsc{nnet}_{\theta}:[0,T]\times\mathbb{R}^{2D}\times\mathbb{R}^{2D}\to \mathbb{R}^{2D}\times\mathbb{R}^{2D}\), which outputs mean perturbations and per-dimension variances to calculate \(u_{t|0,T}^{\bar{x}}\) and \(\Sigma_{t|0,T}^{\bar{x}}\), \(\Sigma_{t|0,T}^{\bar{v}}\) and sample \((\bar{x},\bar{v})\), as in (15). Note that we parameterize \(\Sigma_{t|0,T}^{\bar{x}},\Sigma_{t|0,T}^{\bar{v}}\) separately, matching the block diagonal form of (16). We calculate \(v_{t|0,T}^{(q)}(\bar{x},\bar{v})\coloneqq[v_{t|0,T}^{\bar{x}(q)},v_{t|0,T}^{ \bar{v}(q)}]\) from \(u_{t|0,T}^{(q)}(\bar{x},\bar{v})=[u_{t|0,T}^{\bar{x}(q)},u_{t|0,T}^{\bar{v}(q)}]\) as in (13)-(14), with \(G_{t}^{-1}=(\frac{1}{2}\Xi_{t}\Xi_{t}^{T})^{-1}\) given by (16). The Lagrangian objective in (9) minimizes the norm of the concatenated vector \(v_{t|0,T}^{(q)}(\bar{x},\bar{v})\), which depends on the reference drift \(b_{t}(\bar{x},\bar{v})\) in (16).

#### 3.2.3 Gaussian Mixture Paths

Note that the true Doob \(h\)-transform may not yield marginal distributions which are unimodal Gaussians as in the previous section. To increase the expressivity of our variational family of conditioned processes, we now extend our parameterization to mixtures of Gaussians, \(q_{t|0,T}\in\mathcal{Q}_{\text{MoG}}^{K}\). Given a set of \(K\) mixture weights \(\bm{w}\coloneqq\{w^{k}\}_{k=1}^{K}\) and component Gaussian paths \(\{q_{t|0,T}^{k}\}_{k=1}^{K}\), the following identity allows us to obtain the drift \(u_{t|0,T}^{(q,\theta)}\) of the corresponding mixture distribution \(q_{t|0,T}\).

**Proposition 4**.: _Given a set of processes \(q_{t|0,T}^{k}(x)\) and mixtures weights \(w^{k}\), the vector field satisfying the Fokker-Planck equation in (12) for the mixture \(q_{t|0,T}(x)=\sum_{k}w^{k}q_{t|0,T}^{k}(x)\) is given by_

\[u_{t|0,T}^{(q,\theta)}(x)=\sum_{k=1}^{K}\frac{w^{k}q_{t|0,T}^{k}(x)}{\sum_{j=1 }^{K}w^{j}q_{t|0,T}^{j}(x)}u_{t|0,T}^{(q,k)}(x)\,,\] (17)

_where \(u_{t|0,T}^{(q,k)}(x)\) satisfies the Fokker-Planck equation in (12) for \(q_{t|0,T}^{k}(x)\). This identity holds for both first-order dynamics in spatial coordinates only or second-order dynamics in \(x=(\bar{x},\bar{v})\)._

Finally, we can calculate \(v_{t|0,T}^{(q,\theta)}(x)\) by comparing \(u_{t|0,T}^{(q,\theta)}(x)\) for the mixture of Gaussian path \(q_{t|0,T}\in\mathcal{Q}_{\text{MoG}}^{K}\) to the reference drift \(b_{t}(x)\) as in (14), and proceed to minimize its norm as in (9). In practice, we use Gumbel softmax reparameterization gradients (Maddison et al., 2016; Jang et al., 2017) to optimize the mixture weights \(\{w^{k}\}_{k=1}^{K}\) alongside the neural network parameters \(\{\theta^{k}\}_{k=1}^{K}\) for each Gaussian component \(\{\mu_{t|0,T}^{(\theta)},\Sigma_{t|0,T}^{(\theta)}\}_{k=1}^{K}\) and either first- or second-order dynamics.

## 4 Related Work

**(Aligned) Schrodinger Bridge Matching Methods.** Many existing 'bridge matching' approaches (Shi et al., 2023; Peluchetti, 2021, 2023; Liu et al., 2022; Lipman et al., 2022; Liu et al., 2023b) for SB and generative modeling rely on convenient properties of Brownian bridges and would require calculating \(h\)-transforms to simulate bridges for general reference processes. Our conditionalGaussian path parameterization is similar to Liu et al. (2023); Neklyudov et al. (2024), where analytic bridges are not available for SB problems with nonlinear reference drift or general costs.

Somnath et al. (2023); Liu et al. (2023) attempt to solve the SB problem given access to aligned data \(x_{0},x_{T}\sim q_{0,T}^{\text{data}}\) assumed to be drawn from an optimal coupling. While the method in Somnath et al. (2023) involves approximating an \(h\)-transform, their goal is to obtain an unconditioned vector field \(v_{t}\) to simulate a Markov process. However, De Bortoli et al. (2023) use Doob's \(h\)-transform to argue the learned Markov process will not preserve the empirical coupling unless \(q_{0,T}^{\text{data}}\) is the optimal coupling for the SB problem, and show that an 'augmented' \(v_{0,t}\) which conditions on \(x_{0}\) can correct this issue.

After training on a dataset of \(x_{0},x_{T}\sim q_{0,T}^{\text{data}}\) pairs using our method, we could consider using an (augmented) bridge matching objective (Shi et al., 2023; De Bortoli et al., 2023) to distill our learned \(v_{t|0,T}^{(q)}\) into a vector field \(v_{t}\) or \(v_{0,t}\) which does not condition on the endpoint. Our use of a Gaussian path parameterization with samples from a fixed endpoint coupling and no Markovian step corresponds to a simplified version of the conditional optimal control step in Liu et al. (2023).

**Transition Path Sampling.** We refer to the surveys of Dellago et al. (2002); Weinan and Vanden-Eijnden (2010); Bolhuis and Swenson (2021) for an overview of the TPS problem. Least action principles for TPS have a long history, building upon the Freidlin-Wentzell (Freidlin and Wentzell, 1998) and Onsager-Machlup (Onsager and Machlup, 1953; Durr and Bach, 1978) Lagrangian functionals in the zero-noise limit and finite-noise cases. In particular, the Onsager-Machlup functional relates maximum a posteriori estimators or'most probable (conditioned) paths' to the minimizers of an action functional similar to Thm. 1, where example algorithms include (Vanden-Eijnden and Heymann, 2008; Sheppard et al., 2008). By contrast, our approach targets the _entire_ posterior over transition paths using an expressive variational family. While Lu et al. (2017) provide analysis for the Gaussian family, we draw connections with Doob's \(h\)-transform and extend to mixtures of Gaussians.

Shooting methods are among the most popular for sampling the posterior of transition paths. From a path that satisfies the boundary conditions (obtained, e.g., using high-temperature simulations), shooting picks points and directions to propose alterations, then simulates new trajectories and accepts or rejects using Metropolis-Hastings (MH) (Juraszek and Bolhuis, 2008; Borrero and Dellago, 2016; Jung et al., 2017; Falkner et al., 2023; Jung et al., 2023). While the MCMC corrections yield theoretical guarantees, shooting methods involve expensive molecular dynamics (MD) simulations and need to balance high rejection rates with large changes in trajectories. One-way shooting methods sample paths efficiently but yield highly correlated samples. Two-way shooting methods, which we compare to in Sec. 5, are more expensive but typically sample diverse paths faster. Recent machine learning approaches (e.g. Plainer et al. (2023); Lelievre et al. (2023)) aim to reduce the need for MD.

Finally, various related methods rely on iterative simulation of SDE in (10) during training to learn the control drift term. Yan et al. (2022); Holdijk et al. (2023) are motivated from the perspective of stochastic optimal control, while Das et al. (2021); Rose et al. (2021) develop actor-critic methods using closely-related ideas from soft reinforcement learning. The variational method in Das et al. (2019) optimizes the rate function quantifying the probability of the rare events, while Singh and Limmer (2023) solves the Kolmogorov backward equation to learn the Doob's \(h\)-transform. However, all of these methods may be inefficient if the desired terminal state is sampled infrequently.

## 5 Experiments

We investigate the capabilities of our approach across a variety of different settings. We first illustrate features of our method on toy potentials before continuing to real-world molecular systems, including a commonly-used benchmark system, alanine dipeptide, and a small protein, Chignolin. The code behind our method is available at https://github.com/plainerman/variational-doob. Before diving into the experiments, we introduce the evaluation procedure and baseline methods.

**Evaluation metrics.** In our evaluation, we emphasize two key quantities: accuracy and efficiency. Efficiency is evaluated by the number of calls to the potential energy function, which requires extensive computation and dominates the runtime of larger molecules. For accuracy, we evaluate the log-likelihood of each sampled path and the maximum energy point (saddle point/transition state) along each sampled path. A good method samples many probable paths (i.e., high log-likelihood) and an accurate transition state (i.e., small maximum energy). See App. D for further details.

**Baselines.** We compare our approach against the MCMC-based two-way shooting method with uniform point selection with variable or fixed length trajectories. We found that two-way shooting produced the most diverse path ensembles among possible baselines, although the acceptance probability can be relatively low for systems dominated by diffusive dynamics (Brotzakis and Bolhuis, 2016) and might be improved by better shooting point selection. This baseline gives theoretical guarantees about the ensemble and thus can be considered as a proxy for the ground truth. In that sense, our goal is not to beat two-way shooting but to approximate it with fewer potential evaluations.

### Synthetic Datasets

**Muller-Brown Potential.** The Muller-Brown potential is a popular benchmark to study transition path sampling between metastable states. It consists of three local minima, and we aim to sample transition paths connecting state \(A\) and state \(B\) with a circular state definition. In Fig. 3, we visualize the potential and the sampled paths and can see that the same ensemble is sampled for both our method and two-way shooting. Our method exhibits a slightly reduced variance for unlikely transitions. In Table 1, we can observe that MCMC-based methods require many potential evaluations to achieve a good result, which comes from the low acceptance rate (especially when fixing the lengths of trajectories). Our method requires fewer energy evaluations (1 million vs. 1 billion) while finding paths with similar energy and likelihood. Note that the likelihood for variable approaches has been omitted, as it is governed by the number of steps in the trajectory and cannot be compared directly.

**Gaussian Mixture.** We further consider a potential in which the states are separated by a symmetric high-energy barrier that allows for two distinct reaction channels. In Fig. 3, we observe that a single Gaussian path cannot model a system with multiple modes of transition paths. Nevertheless, this issue can be resolved using a mixture of Gaussian paths, with slightly increased computational cost.

**The Case for Neural Networks.** According to our empirical study, the neural network parameterization of the Gaussian distribution statistics \(\mu_{\,t\,|0,T},\Sigma_{\,t\,|0,T}\) is an invaluable part of our framework. As an ablation, we consider parameterizing \(\mu_{\,t\,|0,T},\Sigma_{\,t\,|0,T}\) as piecewise linear splines whose intermediate points are updated using the same gradient-based optimizer as used for neural network training. In App. D.3, we report results comparing the W1 distance of learned marginals using neural network versus spline parameterizations, observing that splines yield inferior results even after an order of magnitude more potential function evaluations. We thus conclude that spline parameterizations are not competitive for learning transition paths and continue to focus on our neural-network approach.

### Second-order Dynamics and Molecular Systems

**Experiment Setup.** We evaluate our methods on real-world high-dimensional molecular systems governed by the second-order dynamics (4): _alanine dipeptide_ and _Chignolin_. Alanine dipeptide is a well-studied system of 22 atoms (66 total degrees of freedom), where the molecule can be described by two collective variables (CV): the dihedral angles \(\phi\), \(\psi\). Chignolin is a larger system consisting of 10 residues with 138 atoms (414 total degrees of freedom) that cannot be summarized as easily. We use an AMBER14 force field (Maier et al., 2015) implemented in OpenMM (Eastman et al., 2017) but use DMFF (Wang et al., 2023) to backpropagate through the energy evaluations.

\begin{table}
\begin{tabular}{l|c|c c|c c} \hline \hline Method & \# Evaluations (\(\downarrow\)) & Max Energy (\(\downarrow\)) & MinMax Energy (\(\downarrow\)) & Log-Likelihood (\(\uparrow\)) & Max Log-Likelihood (\(\uparrow\)) \\ \hline MCMC (variable) & 3.53M & -13.77 \(\pm\) 16.43 & -40.75 & - & - \\ MCMC & 1.03B & -17.80 \(\pm\) 14.77 & -40.21 & 866.56 \(\pm\) 17.00 & 907.15 \\ Ours & **1.28M** & -14.81 \(\pm\) 13.73 & -40.56 & 858.50 \(\pm\) 17.61 & 909.74 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Transition path sampling experiment for Müller-Brown potential. We report the number of potential evaluations needed to sample 1,000 paths, as well as the maximum energy and the likelihood of each path (including mean and standard deviation). The methods marked with ‘variable’ use a variable length setting. MinMax energy reports the lowest maximum energy across all paths (i.e., energy of lowest transition state).

**Alanine Dipeptide.** In Table 2, we report results for four variants of our models, which either predict Cartesian coordinates or internal coordinates in the form of bond lengths and dihedral angles (compare App. D.4), either with or without Gaussian mixture. For our method, operating in internal coordinates yields better results compared to Cartesian coordinates, where the internal coordinates are distributed similarly to Gaussians and our network does need not learn equivariances (Du et al., 2022). Similarly, Gaussian mixture paths perform slightly better than a single Gaussian path due to the additional expressiveness. We note that paths sampled with Gaussian mixture exhibit a larger variance in max energy as they represent multiple reaction channels.

We find that prior-informed definitions of the desired initial and target states (i.e., CV) are necessary for MCMC to work efficiently with fixed-length trajectories. Finding these CVs in practice is challenging and only possible in this instance because the molecule is small and well-studied. For the larger system size in Table 2, it becomes intractable to use MCMC to connect precise states \(A,B\) ('exact') instead of larger regions ('relaxed'), even with a single trajectory. Variable length MCMC with relaxed endpoint conditions with CV perform well on this task, but our method is competitive using fewer evaluations and more strict boundary conditions. Fixed-length MCMC, even with prior-informed knowledge, can only find 100 trajectories while needing 50 times more potential evaluations compared to variable length.

**Chignolin.** The folding dynamics of Chignolin already pose a challenge and have not yet been well-studied compared to alanine dipeptide. We illustrate the qualitative experimental results for this system in Fig. 4. Operating in Cartesian space, our model samples a feasible transition within 25.6M potential energy evaluation calls and a transition with a duration of \(T=1ps\).

## 6 Conclusion, Limitations and Future Work

In this paper, we propose an efficient computational framework for transition path sampling with Brownian dynamics. We formulate the transition path sampling problem by using Doob's \(h\)-transform to condition a reference stochastic process, and propose a variational formulation for efficient optimization. Specifically, we propose a simulation-free training objective and model parameterization that imposes boundary conditions as hard constraints. We compare our method with MCMC-based baselines and show comparable accuracy with lower computational costs on both synthetic datasets and real-world molecular systems. Our method is currently limited by rigidly defining states A and B to be a point mass with Gaussian noise instead of any arbitrary set. Finally, our method might be improved by accommodating variable length paths.

\begin{table}
\begin{tabular}{l|c|c|c c} \hline Method & States & \# Evaluations (\(\downarrow\)) & Max Energy (\(\downarrow\)) & MinMax Energy (\(\downarrow\)) \\ \hline MCMC (variable length) & CV & 21.02M & 740.0 + 695.79 & 52.37 \\ MCMC* & CV & 1.29B* & 288.46 + 128.31 & 60.52 \\ \hline MCMC (variable length) & relaxed & 187.54M & 412.65 + 334.70 & 26.97 \\ MCMC & relaxed & \textgreater{} 10B & N/A & N/A \\ \hline MCMC (variable length) & exact & \textgreater{} 10B & N/A & N/A \\ MCMC & exact & \textgreater{} 10B & N/A & N/A \\ Ours (Cartesian) & exact & **38.40M** & 726.40 + 0.07 & 726.18 \\ Ours (Cartesian, 2 Mixtures) & exact & 51.20M & 709.38 + 162.37 & 513.72 \\ Ours (Cartesian, 5 Mixtures) & exact & 51.20M & 541.26 + 278.20 & 247.96 \\ Ours (Internal) & exact & **38.40M** & -14.62 + 0.02 & -14.67 \\ Ours (Internal, 2 Mixtures) & exact & 51.20M & -15.38 + 0.14 & -15.54 \\ Ours (Internal, 5 Mixtures) & exact & 51.20M & -15.50 + 0.31 & **-15.95** \\ \hline \end{tabular}
\end{table}
Table 2: Transition path sampling for alanine dipeptide. For MCMC methods, we compare different state definitions of \(\mathcal{A},\mathcal{B}\): ‘CV’ uses \(\phi,\psi\) angles. ‘Exact’ uses a very small threshold of aligned root-mean-square deviation (RMSD) around reference states \(A,B\) (as in Ours). ‘Relaxed’ uses a larger threshold of RMSD around \(A,B\). The method marked with a * only samples 100 paths due to computational limitations, while others sample 1,000. Fields with N/A are intractable as a single trajectory requires more than 1 billion potential evaluations.

Figure 4: Transition path for the protein Chignolin. The energy plot a transition path in which the protein folds in \(T=1,000\) fs, and passes a high energy barrier at \(460fs\) with about \(3,000\) kJ/mol.

## Acknowledgments and Disclosure of Funding

The authors would like to thank Juno Nam and Soojung Yang for spotting the unphysical steric barrier in the original pair of initial and target states, Jungyoon Lee for spotting an error in the energy computation, and Guan-Horng Liu, Maurice Weiler, Hannes Stark and Yanze Wang for helpful discussions. The work of Yuanqi Du and Carla P. Gomes was supported by the Eric and Wendy Schmidt AI in Science Postdoctoral Fellowship, a Schmidt Futures program; the National Science Foundation (NSF), the Air Force Office of Scientific Research (AFOSR); the Department of Energy; and the Toyota Research Institute (TRI). The work of Michael Plainer was supported by the Konrad Zuse School of Excellence in Learning and Intelligent Systems (ELIZA) through the DAAD program Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the German Ministry of Education and Research and by Max Planck Society. The work of Kirill Neklyudov was supported by IVADO.

## References

* Anderson (2007) Anderson, J. B. (2007). Predicting rare events in molecular dynamics. _Advances in Chemical Physics_, 91:381-431.
* Batatia et al. (2023) Batatia, I., Benner, P., Chiang, Y., Elena, A. M., Kovacs, D. P., Riebesell, J., Advincula, X. R., Asta, M., Baldwin, W. J., Bernstein, N., et al. (2023). A foundation model for atomistic materials chemistry. _arXiv preprint arXiv:2401.00096_.
* Blau et al. (2021) Blau, S. M., Patel, H. D., Spotte-Smith, E. W. C., Xie, X., Dwaraknath, S., and Persson, K. A. (2021). A chemically consistent graph architecture for massive reaction networks applied to solid-electrolyte interphase formation. _Chemical science_, 12(13):4931-4939.
* Bolhuis and Swenson (2021) Bolhuis, P. G. and Swenson, D. W. H. (2021). Transition path sampling as markov chain monte carlo of trajectories: Recent algorithms, software, applications, and future outlook. _Advanced Theory and Simulations_, 4(4).
* Borrero and Dellago (2016) Borrero, E. and Dellago, C. (2016). Avoiding traps in trajectory space: Metadynamics enhanced transition path sampling. _The European Physical Journal Special Topics_, 225(8-9):1609-1620.
* Bradbury et al. (2018) Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. (2018). JAX: composable transformations of Python+NumPy programs.
* Brotzakis and Bolhuis (2016) Brotzakis, Z. F. and Bolhuis, P. G. (2016). A one-way shooting algorithm for transition path sampling of asymmetric barriers. _The Journal of Chemical Physics_, 145(16):164112.
* Castellan (1983) Castellan, G. W. (1983). _Physical Chemistry_. Addison-Wesley, Reading, Mass, 3rd ed edition.
* Chen et al. (2021a) Chen, T., Liu, G.-H., and Theodorou, E. (2021a). Likelihood training of schrodinger bridge using forward-backward sdes theory. In _International Conference on Learning Representations_.
* Chen et al. (2016) Chen, Y., Georgiou, T. T., and Pavon, M. (2016). On the relation between optimal transport and schrodinger bridges: A stochastic control viewpoint. _Journal of Optimization Theory and Applications_, 169:671-691.
* Chen et al. (2021b) Chen, Y., Georgiou, T. T., and Pavon, M. (2021b). Stochastic control liaisons: Richard sinkhorn meets gaspard monge on a schrodinger bridge. _Siam Review_, 63(2):249-313.
* Chopin et al. (2023) Chopin, N., Fulop, A., Heng, J., and Thiery, A. H. (2023). Computational doob h-transforms for online filtering of discretely observed diffusions. In _International Conference on Machine Learning_, pages 5904-5923. PMLR.
* Das et al. (2022) Das, A., Kuznets-Speck, B., and Limmer, D. T. (2022). Direct evaluation of rare events in active matter from variational path sampling. _Physical Review Letters_, 128(2):028005.
* Das et al. (2019) Das, A., Limmer, D. T., and Limmer, D. T. (2019). Variational control forces for enhanced sampling of nonequilibrium molecular dynamics simulations. _The Journal of chemical physics_, 151(24).
* Das et al. (2019)Das, A., Rose, D. C., Garrahan, J. P., and Limmer, D. T. (2021). Reinforcement learning of rare diffusive dynamics. _The Journal of Chemical Physics_, 155(13).
* De Bortoli et al. (2023) De Bortoli, V., Liu, G.-H., Chen, T., Theodorou, E. A., and Nie, W. (2023). Augmented bridge matching. _arXiv preprint arXiv:2311.06978_.
* De Bortoli et al. (2021) De Bortoli, V., Thornton, J., Heng, J., and Doucet, A. (2021). Diffusion schrodinger bridge with applications to score-based generative modeling. _Advances in Neural Information Processing Systems_, 34:17695-17709.
* Dellago et al. (2002) Dellago, C., Bolhuis, P. G., and Geissler, P. L. (2002). Transition path sampling. _Advances in chemical physics_, 123:1-78.
* Dickson (2018) Dickson, A. (2018). Mapping the ligand binding landscape. _Biophysical Journal_, 115(9):1707-1719.
* Doob (1957) Doob, J. L. (1957). Conditional brownian motion and the boundary limits of harmonic functions. _Bulletin de la Societe Mathematique de France_, 85:431-458.
* Du et al. (2022) Du, W., Zhang, H., Du, Y., Meng, Q., Chen, W., Zheng, N., Shao, B., and Liu, T.-Y. (2022). Se (3) equivariant graph neural networks with complete local frames. In _International Conference on Machine Learning_, pages 5583-5608. PMLR.
* Durr and Bach (1978) Durr, D. and Bach, A. (1978). The onsager-machlup function as lagrangian for the most probable path of a diffusion process. _Communications in Mathematical Physics_, 60:153-170.
* Eastman et al. (2017) Eastman, P., Swails, J., Chodera, J. D., McGibbon, R. T., Zhao, Y., Beauchamp, K. A., Wang, L.-P., Simmonett, A. C., Harrigan, M. P., Stern, C. D., Wiewiora, R. P., Brooks, B. R., and Pande, V. S. (2017). OpenMM 7: Rapid development of high performance algorithms for molecular dynamics. _PLOS Computational Biology_, 13(7):e1005659.
* Escobedo et al. (2009) Escobedo, F. A., Borrero, E. E., and Araque, J. C. (2009). Transition path sampling and forward flux sampling. applications to biological systems. _Journal of Physics: Condensed Matter_, 21(33):333101.
* Falkner et al. (2023) Falkner, S., Coretti, A., Romano, S., Geissler, P., and Dellago, C. (2023). Conditioning normalizing flows for rare event sampling. _arXiv preprint arXiv:2207.14530_.
* Flamary et al. (2021) Flamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L., Corenflos, A., Fatras, K., Fournier, N., Gautheron, L., Gayraud, N. T., Janati, H., Rakotomamonjy, A., Redko, I., Rolet, A., Schutz, A., Seguy, V., Sutherland, D. J., Tavenard, R., Tong, A., and Vayer, T. (2021). Pot: Python optimal transport. _Journal of Machine Learning Research_, 22(78):1-8.
* Freidlin and Wentzell (1998) Freidlin, M. and Wentzell, A. (1998). _Random perturbations of Dynamical Systems_. Springer.
* Gabrie et al. (2022) Gabrie, M., Rotskoff, G. M., and Vanden-Eijnden, E. (2022). Adaptive monte carlo augmented with normalizing flows. _Proceedings of the National Academy of Sciences_, 119(10):e2109420119.
* Heng et al. (2021) Heng, J., De Bortoli, V., Doucet, A., and Thornton, J. (2021). Simulating diffusion bridges with score matching. _arXiv preprint arXiv:2111.07243_.
* Holdijk et al. (2023) Holdijk, L., Du, Y., Hooft, F., Jaini, P., Ensing, B., and Welling, M. (2023). Stochastic optimal control for collective variable free sampling of molecular transition paths. _Advances in Neural Information Processing Systems_, 36.
* Jamison (1975) Jamison, B. (1975). The markov processes of schrodinger. _Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete_, 32(4):323-331.
* Jang et al. (2017) Jang, E., Gu, S., and Poole, B. (2017). Categorical reparametrization with gumble-softmax. In _International Conference on Learning Representations (ICLR 2017)_. OpenReview. net.
* Jung et al. (2023) Jung, H., Covino, R., Arjun, A., Leitold, C., Dellago, C., Bolhuis, P. G., and Hummer, G. (2023). Machine-guided path sampling to discover mechanisms of molecular self-organization. _Nature Computational Science_, 3(4):334-345.

Jung, H., ichi Okazaki, K., and Hummer, G. (2017). Transition path sampling of rare events by shooting from the top. _The Journal of Chemical Physics_, 147(15).
* Juraszek and Bolhuis (2008) Juraszek, J. and Bolhuis, P. G. (2008). Rate constant and reaction coordinate of trp-cage folding in explicit water. _Biophysical Journal_, 95(9):4246-4257.
* Kingma and Welling (2013) Kingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. _International Conference on Learning Representations_.
* Kirmizialtin et al. (2015) Kirmizialtin, S., Johnson, K. A., and Elber, R. (2015). Enzyme selectivity of HIV reverse transcriptase: Conformations, ligands, and free energy partition. _The Journal of Physical Chemistry B_, 119(35):11513-11526.
* Kirmizialtin et al. (2012) Kirmizialtin, S., Nguyen, V., Johnson, K. A., and Elber, R. (2012). How conformational dynamics of DNA polymerase select correct substrates: Experiments and simulations. _Structure_, 20(4):618-627.
* Klucznik et al. (2024) Klucznik, T., Syntrivanis, L.-D., Bas, S., Mikulak-Klucznik, B., Moskal, M., Szymkuc, S., Mlynarski, J., Gadina, L., Beker, W., Burke, M. D., et al. (2024). Computational prediction of complex cationic rearrangement outcomes. _Nature_, 625(7995):508-515.
* Koehl and Orland (2022) Koehl, P. and Orland, H. (2022). Sampling constrained stochastic trajectories using brownian bridges. _The Journal of Chemical Physics_, 157(5).
* Lelievre et al. (2023) Lelievre, T., Robin, G., Sekkat, I., Stoltz, G., and Cardoso, G. V. (2023). Generative methods for sampling transition paths in molecular dynamics. _ESAIM: Proceedings and Surveys_, 73:238-256.
* Leonard (2014) Leonard, C. (2014). A survey of the schrodinger problem and some of its connections with optimal transport. _Discrete & Continuous Dynamical Systems-A_, 34(4):1533-1574.
* Lipman et al. (2022) Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. (2022). Flow matching for generative modeling. _International Conference on Learning Representations_.
* Liu et al. (2023a) Liu, G.-H., Lipman, Y., Nickel, M., Karrer, B., Theodorou, E., and Chen, R. T. (2023a). Generalized schrodinger bridge matching. In _The Twelfth International Conference on Learning Representations_.
* Liu et al. (2023b) Liu, G.-H., Vahdat, A., Huang, D.-A., Theodorou, E., Nie, W., and Anandkumar, A. (2023b). 1\({}^{2}\)sb: Image-to-image schrodinger bridge. _arXiv preprint arXiv:2302.05872_.
* Liu et al. (2022) Liu, X., Wu, L., Ye, M., and Liu, Q. (2022). Let us build bridges: Understanding and extending diffusion generative models. _arXiv preprint arXiv:2208.14699_.
* Liu et al. (2023c) Liu, X., Wu, L., Ye, M., and qiang liu (2023c). Learning diffusion bridges on constrained domains. In _The Eleventh International Conference on Learning Representations_.
* Lu et al. (2017) Lu, Y., Stuart, A., and Weber, H. (2017). Gaussian approximations for transition paths in brownian dynamics. _SIAM Journal on Mathematical Analysis_, 49(4):3005-3047.
* Maddison et al. (2016) Maddison, C. J., Mnih, A., and Teh, Y. W. (2016). The concrete distribution: A continuous relaxation of discrete random variables. In _International Conference on Learning Representations_.
* Maier et al. (2015) Maier, J. A., Martinez, C., Kasavajhala, K., Wickstrom, L., Hauser, K. E., and Simmerling, C. (2015). ff14sb: improving the accuracy of protein side chain and backbone parameters from ff99sb. _Journal of chemical theory and computation_, 11(8):3696-3713.
* Mehdi et al. (2024) Mehdi, S., Smith, Z., Herron, L., Zou, Z., and Tiwary, P. (2024). Enhanced sampling with machine learning. _Annual Review of Physical Chemistry_, 75.
* Neklyudov et al. (2023) Neklyudov, K., Brekelmans, R., Severo, D., and Makhzani, A. (2023). Action matching: Learning stochastic dynamics from samples. In _International Conference on Machine Learning_.
* Neklyudov et al. (2024) Neklyudov, K., Brekelmans, R., Tong, A., Atanackovic, L., Liu, Q., and Makhzani, A. (2024). A computational framework for solving wasserstein lagrangian flows. _International Conference on Machine Learning_.
* Neklyudov et al. (2020)Noe, F., Olsson, S., Kohler, J., and Wu, H. (2019). Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning. _Science_, 365(6457):eaaw1147.
* Noe et al. (2009) Noe, F., Schutte, C., Vanden-Eijnden, E., Reich, L., and Weikl, T. R. (2009). Constructing the equilibrium ensemble of folding pathways from short off-equilibrium simulations. _Proceedings of the National Academy of Sciences_, 106(45):19011-19016.
* Onsager and Machlup (1953) Onsager, L. and Machlup, S. (1953). Fluctuations and irreversible processes. _Physical Review_, 91(6):1505.
* Papaspiliopoulos and Roberts (2012) Papaspiliopoulos, O. and Roberts, G. (2012). Importance sampling techniques for estimation of diffusion models. _Statistical methods for stochastic differential equations_, 124:311-340.
* Peluchetti (2021) Peluchetti, S. (2021). Non-denoising forward-time diffusions.
* Peluchetti (2023) Peluchetti, S. (2023). Diffusion bridge mixture transports, Schrodinger bridge problems and generative modeling. _arXiv preprint arXiv:2304.00917_.
* Plainer et al. (2023) Plainer, M., Stark, H., Bunne, C., and Gunnemann, S. (2023). Transition path sampling with boltzmann generator-based MCMC moves. In _Generative AI and Biology Workshop_.
* Rezende et al. (2014) Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In _International conference on machine learning_, pages 1278-1286. PMLR.
* Rose et al. (2021) Rose, D. C., Mair, J. F., and Garrahan, J. P. (2021). A reinforcement learning approach to rare trajectory sampling. _New Journal of Physics_, 23(1):013013.
* Rotskoff (2024) Rotskoff, G. M. (2024). Sampling thermodynamic ensembles of molecular systems with generative neural networks: Will integrating physics-based models close the generalization gap? _Current Opinion in Solid State and Materials Science_, 30:101158.
* Sarkka and Solin (2019) Sarkka, S. and Solin, A. (2019). _Applied stochastic differential equations_, volume 10. Cambridge University Press.
* Schauer et al. (2017) Schauer, M., van der Meulen, F., and van Zanten, H. (2017). Guided proposals for simulating multi-dimensional diffusion bridges. _Bernoulli_, 23(4A).
* Schrodinger (1932) Schrodinger, E. (1932). Sur la theorie relativiste de l'electron et l'interpretation de la mecanique quantique. In _Annales de l'institut Henri Poincare_, volume 2, pages 269-310.
* Selli et al. (2016) Selli, D., Boulfelfel, S. E., Schapotschnikow, P., Donadio, D., and Leoni, S. (2016). Hierarchical thermoelectrics: crystal grain boundaries as scalable phonon scatterers. _Nanoscale_, 8(6):3729-3738.
* Sharma et al. (2016) Sharma, N. D., Singh, J., Vijay, A., Samanta, K., Dogra, S., and Bandyopadhyay, A. K. (2016). Pressure-induced structural transition trends in nanocrystalline rare-earth sesquioxides: A raman investigation. _The Journal of Physical Chemistry C_, 120(21):11679-11689.
* Sheppard et al. (2008) Sheppard, D., Terrell, R., and Henkelman, G. (2008). Optimization methods for finding minimum energy paths. _The Journal of chemical physics_, 128(13).
* Shi et al. (2023) Shi, Y., De Bortoli, V., Campbell, A., and Doucet, A. (2023). Diffusion schrodinger bridge matching. _arXiv preprint arXiv:2303.16852_.
* Shoghi et al. (2023) Shoghi, N., Kolluru, A., Kitchin, J. R., Ulissi, Z. W., Zitnick, C. L., and Wood, B. M. (2023). From molecules to materials: Pre-training large generalizable models for atomic property prediction. In _The Twelfth International Conference on Learning Representations_.
* Sidky et al. (2020) Sidky, H., Chen, W., and Ferguson, A. L. (2020). Molecular latent space simulators. _Chemical Science_, 11(35):9459-9467.
* Singh and Limmer (2023) Singh, A. N. and Limmer, D. T. (2023). Variational deep learning of equilibrium transition path ensembles. _The Journal of Chemical Physics_, 159(2).
* Sellan et al. (2016)Smith, J. S., Isayev, O., and Roitberg, A. E. (2017). Ani-1: an extensible neural network potential with dft accuracy at force field computational cost. _Chemical science_, 8(4):3192-3203.
* Somnath et al. (2023) Somnath, V. R., Pariset, M., Hsieh, Y.-P., Martinez, M. R., Krause, A., and Bunne, C. (2023). Aligned diffusion schrodinger bridges. In _Uncertainty in Artificial Intelligence_, pages 1985-1995. PMLR.
* Tong et al. (2023) Tong, A., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Fatras, K., Wolf, G., and Bengio, Y. (2023). Conditional flow matching: Simulation-free dynamic optimal transport. _arXiv preprint arXiv:2302.00482_, 2(3).
* Vanden-Eijnden and Heymann (2008) Vanden-Eijnden, E. and Heymann, M. (2008). The geometric minimum action method for computing minimum energy paths. _The Journal of chemical physics_, 128(6).
* Wang et al. (2018) Wang, H., Zhang, L., Han, J., and Weinan, E. (2018). Deepmd-kit: A deep learning package for many-body potential energy representation and molecular dynamics. _Computer Physics Communications_, 228:178-184.
* Wang et al. (2023) Wang, X., Li, J., Yang, L., Chen, F., Wang, Y., Chang, J., Chen, J., Feng, W., Zhang, L., and Yu, K. (2023). DMFF: An open-source automatic differentiable platform for molecular force field development and molecular dynamics simulation. _Journal of Chemical Theory and Computation_, 19(17):5897-5909.
* Weinan and Vanden-Eijnden (2010) Weinan, E. and Vanden-Eijnden, E. (2010). Transition-path theory and path-finding algorithms for the study of rare events. _Annual review of physical chemistry_, 61(2010):391-420.
* Wu et al. (2022) Wu, L., Gong, C., Liu, X., Ye, M., and Liu, Q. (2022). Diffusion-based molecule generation with informative prior bridges. _Advances in Neural Information Processing Systems_, 35:36533-36545.
* Xi et al. (2013) Xi, L., Shah, M., and Trout, B. L. (2013). Hopping of water in a glassy polymer studied via transition path sampling and likelihood maximization. _The Journal of Physical Chemistry B_, 117(13):3634-3647.
* Yan et al. (2022) Yan, J., Touchette, H., and Rotskoff, G. M. (2022). Learning nonequilibrium control forces to characterize dynamical phase transitions. _Physical Review E_, 105(2):024115.
* Zeng et al. (2020) Zeng, J., Cao, L., Xu, M., Zhu, T., and Zhang, J. Z. (2020). Complex reaction processes in combustion unraveled by neural network-based molecular dynamics simulation. _Nature communications_, 11(1):5713.
* Zhang et al. (2022) Zhang, D., Bi, H., Dai, F.-Z., Jiang, W., Zhang, L., and Wang, H. (2022). Dpa-1: Pretraining of attention-based deep potential model for molecular simulation. _arXiv preprint arXiv:2208.08236_.

Proofs

### Proofs from Sec. 2.2 (Doob's \(h\)-Transform Background)

**Proposition 1**.: [Jamison (1975, Thm. 2)] _Let \(h_{\mathcal{B}}(x,t)\coloneqq\rho_{T}(x_{T}\in\mathcal{B}\,|\,x_{t}=x)\) denote the conditional transition probability of the reference process in (5). Then,_

\[\mathbb{P}^{*}_{0:T}:\qquad dx_{t|T}=\Big{(}b_{t}(x_{t|T})+2G_{t} \nabla_{x}\log h_{\mathcal{B}}(x_{t|T},t)\Big{)}\cdot dt+\Xi_{t}\ dW_{t}\qquad x_{0} \sim\rho_{0}\] (6)

_where we use \(x_{t|T}\) to denote a conditional process. The SDE in (6) is associated with the following transition probabilities for \(s<t<T\),_

\[\rho_{t}(y\,|\,x_{s}=x,x_{T}\in\mathcal{B})=\frac{h_{\mathcal{B}}(y,t)}{h_{ \mathcal{B}}(x,s)}\rho_{t}(y\,|\,x_{s}=x),\] (7)

_Note that all of our subsequent results hold for the case when \(\mathcal{B}\) is a point-mass, with the only change being that the \(h\)-function becomes a density, \(h_{B}(x,t)=\rho_{T}(B\,|\,x_{t}=x)\)._

Proof.: See Jamison (1975) for a simple proof based on Ito's Lemma, assuming smoothness and strict positivity of \(h\). 

**Proposition 2**.: _The following PDEs are obeyed by (a) the density of the conditioned process \(\rho_{t|0,T}(x)\coloneqq\rho_{t}(x\,|\,x_{0}=A,x_{T}\in\mathcal{B})\) and (b) the \(h\)-function \(h_{\mathcal{B}}(x,t)\),_

\[\frac{\partial\rho_{t|0,T}(x)}{\partial t}+\big{\langle}\nabla_{x },\rho_{t|0,T}(x)\big{(}b_{t}(x)+2G_{t}\nabla_{x}\log h_{\mathcal{B}}(x,t) \big{)}\big{\rangle}-\sum_{ij}(G_{t})_{ij}\frac{\partial^{2}}{\partial x_{i} \partial x_{j}}\rho_{t|0,T}(x)=0\,,\] (8a) \[\frac{\partial h_{\mathcal{B}}(x,t)}{\partial t}+\big{\langle} \nabla_{x}h_{\mathcal{B}}(x,t),b_{t}(x)\big{\rangle}+\sum_{ij}(G_{t})_{ij} \frac{\partial^{2}}{\partial x_{i}\partial x_{j}}h_{\mathcal{B}}(x,t)=0\,.\] (8b)

_Reparameterizing (8b) in terms of \(s_{B}(x,t)\coloneqq\log h_{\mathcal{B}}(x,t)\), we can also write_

\[\frac{\partial s_{\mathcal{B}}(x,t)}{\partial t}+\big{\langle} \nabla s_{\mathcal{B}}(x,t),G_{t}\nabla s_{\mathcal{B}}(x,t)\big{\rangle}+ \big{\langle}\nabla s_{\mathcal{B}}(x,t),b_{t}(x)\big{\rangle}+\sum_{ij}(G_{t })_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}s_{\mathcal{B}}(x,t)=0.\] (8c)

Proof.: Let \(p(x_{t+s}=y\,|\,x_{t}=x)\) denote the transition probability of a reference diffusion process

\[\frac{\partial}{\partial s}p(x_{t+s}=y\,|\,x_{t}=x)=-\big{\langle} \nabla_{y},p(x_{t+s}=y\,|\,x_{t}=x)b_{t+s}(y)\big{\rangle}+\sum_{ij}(G_{t})_{ ij}\frac{\partial^{2}}{\partial y_{i}\partial y_{j}}p(x_{t+s}=y\,|\,x_{t}=x),\] (18)

where \((G_{t})_{ij}=\frac{1}{2}\Xi_{t+s}\Xi_{t+s}^{T}\).

Now we condition the process on the end-point value \(x_{T}\in\mathcal{B}\), and we get another kernel, i.e.

\[p(x_{t+s}=y\,|\,x_{t}=x,x_{T}\in\mathcal{B})=\frac{p(x_{T}\in \mathcal{B}\,|\,x_{t+s}=y)}{p(x_{T}\in\mathcal{B}\,|\,x_{t}=x)}p(x_{t+s}=y\,| \,x_{t}=x)\,.\] (19)

We let \(h_{\mathcal{B}}(x,t)=p(x_{T}\in\mathcal{B}\,|\,x_{t}=x)\) denote the conditional probability over the desired endpoint condition given \(x_{t}=x\). According to laws of conditional probability, we can describe how \(h_{\mathcal{B}}(x,t)\) changes in time using the unconditioned transition probability

\[\underbrace{p(x_{T}\in\mathcal{B}\,|\,x_{t}=x)}_{h\,(x,t)}=\int dy \ \underbrace{p(x_{T}\in\mathcal{B}\,|\,x_{t+s}=y)}_{h\,(y,t+s)}p(x_{t+s}=y\,| \,x_{t}=x)\,,\] (20)

we take the derivative \(\frac{\partial}{\partial s}\) on both sides, and we get

\[0=\int dy\ \bigg{[}p(x_{t+s}=y\,|\,x_{t}=x)\frac{\partial h_{\mathcal{B}}(y, t+s)}{\partial s}+\frac{\partial p(x_{t+s}=y\,|\,x_{t}=x)}{\partial s}h_{ \mathcal{B}}(y,t+s)\bigg{]}\,.\] (21)Using the FP equation for the transition probability and integrating by parts, we have

\[0=\int dy\;p(x_{t+s}=y\,|\,x_{t}=x)\Bigg{[}\frac{\partial h_{\mathcal{B}}(y,t+s)}{ \partial s}+\Big{\langle}\nabla_{y}h_{\mathcal{B}}(y,t+s),b_{t}(y)\Big{\rangle} +\sum_{ij}(G_{t})_{ij}\frac{\partial^{2}}{\partial y_{i}\partial y_{j}}h_{ \mathcal{B}}(y,t+s)\Bigg{]}\,.\]

Note that this holds \(\forall x\), hence, we have

\[\frac{\partial h_{\mathcal{B}}(y,t+s)}{\partial s}+\big{\langle}\nabla_{y}h_{ \mathcal{B}}(y,t+s),b_{t+s}(y)\big{\rangle}+\sum_{ij}(G_{t})_{ij}\frac{\partial ^{2}}{\partial y_{i}\partial y_{j}}h_{\mathcal{B}}(y,t+s)=0\,,\]

without any loss of generality we can set \(t=0\)

\[\frac{\partial h_{\mathcal{B}}(y,s)}{\partial s}+\big{\langle}\nabla_{y}h_{ \mathcal{B}}(y,s),b_{s}(y)\big{\rangle}+\sum_{ij}(G_{t})_{ij}\frac{\partial^{ 2}}{\partial y_{i}\partial y_{j}}h_{\mathcal{B}}(y,s)=0\,.\] (22)

as desired to prove the optimality condition in (8b).

To prove (8a), denote \(p(y,s)=p(x_{s}=y\,|\,x_{0}=x)\) and differentiate \(p(x_{s}=y\,|\,x_{0}=x,x_{T}\in\mathcal{B})=\frac{h\,(y,s)}{h\,(x,0)}p(y,s)\) as

\[\frac{\partial}{\partial s}p(x_{s}=y\,|\,x_{0}=x,x_{T}\in \mathcal{B})\] \[=\;\frac{1}{h_{\mathcal{B}}(x,0)}\bigg{[}p(y,s)\frac{\partial h_ {\mathcal{B}}(y,s)}{\partial s}+h_{\mathcal{B}}(y,s)\frac{\partial p(y,s)}{ \partial s}\bigg{]}\] \[=\;\frac{1}{h_{\mathcal{B}}(x,0)}\bigg{[}-\big{\langle}\nabla_{y} h_{\mathcal{B}}(y,s),p(y,s)b_{s}(y)\big{\rangle}-p(y,s)\sum_{ij}(G_{t})_{ij} \frac{\partial^{2}}{\partial y_{i}\partial y_{j}}h_{\mathcal{B}}(y,s)\] \[\qquad\qquad-h_{\mathcal{B}}(y,s)\big{\langle}\nabla_{y},p(y,s)b _{s}(y)\big{\rangle}+h_{\mathcal{B}}(y,s)\sum_{ij}(G_{t})_{ij}\frac{\partial^{ 2}}{\partial y_{i}\partial y_{j}}p(y,s)\bigg{]}\] \[=\;-\bigg{\langle}\nabla_{y},\frac{h_{\mathcal{B}}(y,s)}{h_{ \mathcal{B}}(x,0)}p(y,s)b_{s}(y)\bigg{\rangle}-p(y,s)\bigg{\langle}\nabla_{y}, 2D\nabla_{y}\frac{h_{\mathcal{B}}(y,s)}{h_{\mathcal{B}}(x,0)}\bigg{\rangle}\] \[\qquad\qquad\pm\bigg{\langle}\nabla_{y}p(y,s),2D\nabla_{y}\frac{ h_{\mathcal{B}}(y,s)}{h_{\mathcal{B}}(x,0)}\bigg{\rangle}+\sum_{ij}(G_{t})_{ij} \frac{\partial^{2}}{\partial y_{i}\partial y_{j}}\bigg{(}\frac{h_{\mathcal{B}} (y,s)}{h_{\mathcal{B}}(x,0)}p(y,s)\bigg{)}\,,\]

Note that \(h_{\mathcal{B}}(x,0)\) can be pulled outside the differential operator because it is a function of \(x\). The PDE for the new kernel \(p(y,s\,|\,\mathcal{B})=p(x_{s}=y\,|\,x_{0}=x,x_{T}\in\mathcal{B})\) (conditioned on the end-point) becomes

\[\frac{\partial}{\partial s}p(y,s\,|\,\mathcal{B})=-\big{\langle}\nabla_{y},p(y,s\,|\,\mathcal{B})(b_{s}(y)+2D\nabla_{y}\log h_{\mathcal{B}}(y,s))\big{\rangle} +\sum_{ij}(G_{t})_{ij}\frac{\partial^{2}}{\partial y_{i}\partial y_{j}}p(y,s\,| \,\mathcal{B})\,.\] (23)

which matches the desired PDE in (8a) thereby proving the first two parts of Prop. 2.

Finally, to show (8c), we index time using \(t\) in Eq. (22) and change variables \(h_{\mathcal{B}}(x,t)=e^{s(x,t)}\),

\[\frac{\partial e^{s(x,t)}}{\partial t}+\Big{\langle}\nabla_{x}e^{s(x,t)},b_{t} (x)\Big{\rangle}+\sum_{ij}(G_{t})_{ij}\frac{\partial^{2}}{\partial x_{i} \partial x_{j}}e^{s(x,t)}=0\,.\]

Next, we simplify \(\big{\langle}\nabla,G_{t}\nabla e^{s(x,t)}\big{\rangle}=\big{\langle}\nabla,G_ {t}e^{s(x,t)}\nabla s(x,t)\big{\rangle}=\big{\langle}\nabla e^{s(x,t)},G_{t} \nabla s(x,t)\big{\rangle}+e^{s(x,t)}\big{\langle}\nabla,G_{t}\nabla s(x,t) \big{\rangle}=e^{s(x,t)}\big{\langle}\nabla s(x,t),G_{t}\nabla s(x,t)\big{\rangle} +e^{s(x,t)}\big{\langle}\nabla,G_{t}\nabla s(x,t)\big{\rangle}\) to finally write

\[e^{s(x,t)}\Bigg{(}\frac{\partial s(x,t)}{\partial t}+\big{\langle}\nabla_{x}s(x,t),b_{t}(x)\big{\rangle}+\big{\langle}\nabla s(x,t),G_{t}\nabla s(x,t)\big{ }\big{\rangle}+\sum_{ij}(G_{t})_{ij}\frac{\partial^{2}}{\partial x_{i} \partial x_{j}}s(x,t)\Bigg{)}=0\]

which demonstrates (8c) since the inner term must be zero.

### Proofs from Sec. 3.1 (Lagrangian Action Minimization for Doob's \(h\)-Transform)

We begin by proving Cor. 1, whose proof actually contains the initial steps needed to prove our main theorem Thm. 1. In both proofs, we omit conditioning notation \(q_{t}\gets q_{t|0,T}\) for simplicity and assume \(q_{t}(x)s_{t}(x)\to 0\) vanishes at the boundary \(x\to\pm\infty\), which is used when integrating by parts in \(x\).

**Corollary 1**.: _The Lagrangian objective in Thm. 1 which solves Doob's \(h\)-transform is equivalent to_

\[\mathcal{S}=\min_{q,\,v}\max_{s}\,s_{B}(B,T)-s_{B}(A,0)-\int_{0}^{T}\!dt\, \int\!dx\,q_{t|0,T}\!\left(\frac{\partial s_{B}}{\partial t}+\left\langle \nabla s_{B},G_{t}\nabla s_{B}\right\rangle+\left\langle\nabla s_{B},b_{t} \right\rangle+\left\langle\nabla,G_{t}\nabla s_{B}\right\rangle\right)\]

_if \(q_{t|0,T}\) satisfies (9c). Note \(v_{t|0,T}(x)=\nabla_{x}s_{B}(x,t)\), with \(s_{B}^{*}(x,t)=\log h_{B}(x,t)\) at optimality. 4_

Footnote 4: Compared to (8c), we write \(\sum_{ij}(G_{t})_{ij}\frac{\partial}{\partial x\partial x}s_{\mathcal{B}}(x,t )=\left\langle\nabla,G_{t}\nabla s_{\mathcal{B}}(x,t)\right\rangle\) for simplicity of notation.

Proof.: Consider the following action functional

\[\mathcal{S}= \min_{q,\,v}\int dt\ \int dx\ q_{t}(x)\!\left\langle v_{t}(x),G_{t}v_{t} (x)\right\rangle,\] s.t. \[\frac{\partial q_{t}(x)}{\partial t}=-\big{\langle}\nabla_{x},q _{t}(x)(b_{t}(x)+2G_{t}v_{t}(x))\big{\rangle}+\sum_{ij}(G_{t})_{ij}\frac{ \partial^{2}}{\partial x_{i}\partial x_{j}}q_{t}(x)\,,\] \[q_{0}(x)=\delta(x-A),\ \ q_{1}(x)=\delta(x-B)\,.\]

The Lagrangian of this optimization problem is

\[\mathcal{L}=\int_{0}^{T}dt\ \int dx\ \left[q_{t}\big{\langle}v_{t},G_{t}v_{t} \big{\rangle}+s_{t}\!\left(\frac{\partial q_{t}}{\partial t}+\big{\langle} \nabla,q_{t}(b_{t}+2G_{t}v_{t})\big{\rangle}-\sum_{ij}(G_{t})_{ij}\frac{ \partial^{2}}{\partial x_{i}\partial x_{j}}q_{t}\right)\right],\]

where \(s_{t}\) is the dual variable and we omit the optimization arguments, with \(\mathcal{S}=\min_{q,v}\max_{s}\mathcal{L}\). Swapping the order of optimizations under strong duality, we take the variation with respect to \(v_{t}\) in an arbitrary direction \(\mathfrak{h}_{t}\). Using \(G_{t}=G_{t}^{T}\), we obtain

\[\frac{\delta\mathcal{L}}{\delta v_{t}}[\mathfrak{h}_{t}] =\] (24) \[\implies v_{t}=\nabla s_{t}\,,\]

Substituting into the above, we have

\[\mathcal{L}=\int_{0}^{T}dt\ \int dx\ \bigg{[}s_{t}\frac{\partial q_{t}}{ \partial t}-q_{t}\big{\langle}\nabla s_{t},G_{t}\nabla s_{t}\big{\rangle}+s_{ t}\big{\langle}\nabla,q_{t}b_{t}\big{\rangle}-s_{t}\big{\langle}\nabla,G_{t} \nabla q_{t}\big{\rangle}\bigg{]}\,.\] (25)

Integrating by parts in \(t\) and in \(x\), assuming that \(q_{t}(x)s_{t}(x)\to 0\) as \(x\to\pm\infty\), yields

\[\mathcal{L} =\int dx\ q_{T}s_{T}-\int dx\ q_{0}s_{0}+\int_{0}^{T}dt\ \int dx\left[-q_{t}\frac{\partial s_{t}}{ \partial t}-q_{t}\Big{\langle}\nabla s_{t},G_{t}\nabla s_{t}\Big{\rangle}-q_{ t}\Big{\langle}\nabla s_{t},b_{t}\Big{\rangle}+\Big{\langle}\nabla s_{t},G_{t} \nabla q_{t}\Big{\rangle}\right]\] (26) \[=\] \[=\] \[=\]

where in the second line, we integrate by parts in \(x\) again. Enforcing \(q_{T}(x)=\delta(x-B)\) and \(q_{0}(x)=\delta(x-A)\) and recalling \(\mathcal{S}=\min_{q}\max_{s}\mathcal{L}\) after eliminating \(v_{t}\), we recover the optimization in the statement of the corollary. 

**Theorem**.: _The following Lagrangian action functional has a unique solution which matches the Doob \(h\)-transform in Prop. 2,_

\[\mathcal{S}= \min_{q,\,v}\int_{0}^{T}dt\ \int dx\ q_{t|0,T}(x)\big{\langle}v_{t|0,T}(x),G _{t}\ v_{t|0,T}(x)\big{\rangle}\,,\] (27a) \[\text{s.t.}\ \ \frac{\partial q_{t|0,T}(x)}{\partial t}=-\big{\langle} \nabla_{x},q_{t|0,T}(x)\big{(}b_{t}(x)+2G_{t}\ v_{t|0,T}(x)\big{)}\big{\rangle}+ \sum_{ij}(G_{t})_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}q_{t|0,T} (x),\] (27b) \[q_{0}(x)=\delta(x-A),\qquad q_{T}(x)=\delta(x-B)\,.\] (27c)_Namely, the optimal \(q_{t|0,T}^{*}(x)\) obeys (8a) and the optimal \(v_{t|0,T}^{*}(x)=\nabla_{x}\log h_{\mathcal{B}}(x,t)=\nabla_{x}s(x,t)\) follows (8b) or (8c)._

Proof.: The proof proceeds from (25) above,

\[\mathcal{S}=\min_{q}\max_{s}\mathcal{L}=\min_{q}\max_{s}\int_{0}^{T}dt\ \int dx\ \Big{[}s_{t}\tfrac{\partial q}{\partial t}-q_{t}\big{\langle}\nabla s_{t_{t}}, G_{t}\nabla s_{t}\big{\rangle}+s_{t}\big{\langle}\nabla,q_{t}b_{t}\big{\rangle}-s_{t} \big{\langle}\nabla,G_{t}\nabla q_{t}\big{\rangle}\Big{]}\,.\] (28)

We first show that the optimality condition with respect to \(s_{t}\) yields the Fokker-Planck equation for \(q_{t}\) in Prop. 2 (8a), before deriving the PDE in (8b) as the optimality condition with respect to \(q_{t}\).

Optimality Condition for (27) recovers Prop. 2 (8a): The variation with respect to \(s_{t}\) of (28) is simple, apart from the intermediate term. For a perturbation direction \(\mathfrak{h}_{t}\), we seek

\[\int dx\ \frac{\delta(\boldsymbol{\cdot})}{\delta s_{t}}\ \mathfrak{h}_{t}=\frac{d}{d \varepsilon}\bigg{[}-\int dx\ q_{t}\big{\langle}\nabla(s_{t}+\varepsilon \mathfrak{h}_{t}),G_{t}\nabla(s_{t}+\varepsilon\mathfrak{h}_{t})\big{\rangle} \bigg{]}\bigg{|}_{\varepsilon=0},\]

where \(\boldsymbol{\cdot}\) indicates the functional on the right hand side. Proceeding to differentiate with respect to \(\varepsilon\), we use linearity to pull \(\frac{d}{d\varepsilon}\) inside the integral and apply it first to obtain \(\frac{d}{d\varepsilon}(s_{t}+\varepsilon\mathfrak{h}_{t})=\mathfrak{h}_{t}\). Using the product rule, recognizing the symmetry of terms, and evaluating at \(\varepsilon=0\), we are left with

\[\int dx\ \frac{\delta(\boldsymbol{\cdot})}{\delta s_{t}}\ \mathfrak{h}_{t}=\left[-2\int dx\ q_{t}\big{\langle}\nabla \mathfrak{h}_{t},G_{t}\nabla s_{t}\big{\rangle}\right]\stackrel{{ (i)}}{{=}}\left[\int dx\ \mathfrak{h}_{t}\Big{(}2\big{\langle}\nabla,q_{t}G_{t} \nabla s_{t}\big{\rangle}\Big{)}\right]\] (29)

where in \((i)\) we integrate by parts \(x\).

We are now ready to set the variation of (28) with respect to \(s_{t}\) (in an arbitrary direction \(\mathfrak{h}_{t}\)) equal to zero. Using (29), we have

\[\frac{\delta\mathcal{L}}{\delta s_{t}}[\mathfrak{h}_{t}]=0 =\frac{\partial q_{t}}{\partial t}+2\big{\langle}\nabla,q_{t}G_{t} \nabla s_{t}\big{\rangle}+\big{\langle}\nabla,q_{t}b_{t}\big{\rangle}-\big{ \langle}\nabla,G_{t}\nabla q_{t}\big{\rangle}\] \[\implies 0=\frac{\partial q_{t}}{\partial t}+\Big{\langle}\nabla,q_{t} \Big{(}b_{t}+2G_{t}\nabla s_{t}\Big{)}\Big{\rangle}-\big{\langle}\nabla,G_{t} \nabla q_{t}\big{\rangle}\] (30)

which matches the desired optimality condition for the conditioned process in Prop. 2 (8a).

Optimality Condition for (27) recovers Prop. 2 (8b): Starting again from (28), we take the variation with respect to \(q_{t}\). First, we repeat identical steps (integrate by parts in both \(x\) and \(t\)) to reach (26),

\[\mathcal{L}=\int dx\ q_{T}s_{T}-\int dx\ q_{0}s_{0}-\int_{0}^{T}dt\ \int dx\ q_{t}\bigg{[}\frac{\partial s_{t}}{\partial t}+\big{\langle}\nabla s_{ t},G_{t}\nabla s_{t}\big{\rangle}+\big{\langle}\nabla s_{t},b_{t}\big{\rangle}+ \big{\langle}\nabla,G_{t}\nabla s_{t}\big{\rangle}\bigg{]}\]

where it is now clear that taking the variation with respect to \(q_{t}\) and setting equal to zero yields

\[\frac{\delta\mathcal{L}}{\delta q_{t}}[\mathfrak{h}_{t}]=0=\frac{\partial s_{ t}}{\partial t}+\big{\langle}\nabla s_{t},G_{t}\nabla s_{t}\big{\rangle}+\big{ \langle}\nabla s_{t},b_{t}\big{\rangle}+\big{\langle}\nabla,G_{t}\nabla s_{t} \big{\rangle}\] (31)

which is the desired PDE for \(s(x,t)=\log h_{\mathcal{B}}(x,t)\) in (8c). To obtain (8b), we note an identity used to simplify the last term

\[\sum_{ij}(G_{t})_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\log h_{ t}=\Big{\langle}\nabla,G_{t}\nabla\log h_{t}\Big{\rangle}=\Big{\langle}\nabla, \frac{1}{h_{t}}G_{t}\nabla h_{t}\Big{\rangle}=-\frac{1}{h_{t}^{2}}\Big{\langle} \nabla h_{t},G_{t}\nabla h_{t}\Big{\rangle}+\frac{1}{h_{t}}\Big{\langle}\nabla,G _{t}\nabla h_{t}\Big{\rangle}.\]

Now, substituting \(s(x,t)=\log h_{\mathcal{B}}(x,t)\) into Eq. (31) and abbreviating \(\log h_{\mathcal{B}}(\boldsymbol{\cdot},t)=\log h_{t}(\boldsymbol{\cdot})\), we obtain

\[\frac{1}{h_{t}}\frac{\partial h_{t}}{\partial t}+\frac{1}{h_{t}^{ 2}}\big{\langle}\nabla h_{t},G_{t}\nabla h_{t}\big{\rangle}+\frac{1}{h_{t}} \big{\langle}\nabla h_{t},b_{t}\big{\rangle}-\frac{1}{h_{t}^{2}}\big{\langle} \nabla h_{t},G_{t}\nabla h_{t}\big{\rangle}+\frac{1}{h_{t}}\big{\langle}\nabla,G_{t }\nabla h_{t}\big{\rangle}=0\,,\] \[\implies\quad\frac{\partial h_{t}(x)}{\partial t}+\big{\langle} \nabla h_{t}(x),b_{t}(x)\big{\rangle}+\big{\langle}\nabla,G_{t}\nabla h_{t} \big{\rangle}=0,\] (32)

which matches (8b) as desired.

The last equation defines the backward Kolmogorov equation for the diffusion process with the drift \(b_{t}(x)\) and covariance matrix \(G_{t}\), i.e. the function \(h_{t}(x)\) defines the conditional density \(h_{t}(x)=p(x_{T}\in\mathcal{B}^{\prime}\,|\,x_{t}=x)\) for some set \(\mathcal{B}^{\prime}\), which agrees with the forward process with the same drift and covariance. The boundary condition \(q_{T}(x)=\delta(x-B)\) together with the backward equation define the unique solution to this PDE. Since the PDEs and the boundary conditions are the same as in Doob's \(h\)-transform, we have \(h_{t}(x)=p(x_{T}=B\,|\,x_{t}=x)\)

**Corollary 2**.: _The following SB problem_

\[\mathcal{S}\coloneqq\min_{\mathbb{Q}\text{ s.t. }\mathbb{Q}=\delta,\mathbb{Q}= \delta}D_{KL}[\mathbb{Q}_{0:T}^{v}:\mathbb{P}_{0:T}^{\text{pref}}]\] (11)

_yields the path measure \(\mathbb{P}_{0:T}^{*}\) associated with the SDE in (6) as its unique minimizing argument. The temporal marginals of \(\mathbb{P}_{0:T}^{*}\) are equal to those which optimize the Lagrangian objective in Thm. 1._

Proof.: We use the Girsanov theorem (Sarkka and Solin, 2019, Sec. 7.3) to calculate the KL divergence between the following two Brownian diffusions with fixed initial condition \(x_{0}=A\),

\[\mathbb{P}_{0:T}^{\text{pref}} :\qquad\qquad dx_{t}=b_{t}(x_{t})\cdot dt+\Xi_{t}\ dW_{t}\,,\] (33) \[\mathbb{Q}_{0:T}^{v} :\qquad\qquad dx_{t}=\big{(}b_{t}(x_{t})+2G_{t}\ v_{t|0,T}(x_{t|0,T})\big{)}\cdot dt+\Xi_{t}\ dW_{t}\,,\] (34)

In particular, noting the difference of drifts is \(b_{t}(x_{t})+2G_{t}\ v_{t|0,T}(x_{t})-b_{t}(x_{t})=2G_{t}\ v_{t|0,T}(x_{t})\), the likelihood ratio is given by

\[\frac{d\mathbb{Q}_{0:T}^{v}}{d\mathbb{P}_{0:T}^{\text{pref}}}= \frac{q_{t|0,T}(x_{0},...x_{T})}{\rho(x_{0},...x_{T})}=\exp\Big{\{}-\frac{1}{ 2}\int_{0}^{T}\!\big{\langle}2G_{t}\ v_{t|0,T}(x_{t}),(G_{t})^{-1}\ 2G_{t}\ v_{t|0,T}(x_{t})\big{\rangle}dt\] (35) \[-\int 2\big{(}G_{t}\ v_{t|0,T}(x_{t})\big{)}^{T}G_{t}^{-1}dW_{t} \Big{\}}\]

We finally calculate the KL divergence, noting that, after taking the log, the expectation of the integral \(\int(\cdot)dW_{t}\) in the final term vanishes,

\[D_{KL}[\mathbb{Q}_{0:T}^{v}:\mathbb{P}_{0:T}^{\text{pref}}]=2\int_{0}^{T}dt\ \int dx_{t}\ q_{t|0,T}(x_{t})\ \big{\langle}v_{t|0,T}(x_{t}),G_{t}\ v_{t|0,T}(x_{t})\big{\rangle},\] (36)

which matches (9a) up to a constant factor of 2 does not change the optimum. We finally compare to the constraints in Thm. 1. First, it is clear that the diffusion in (34) satisfies the Fokker-Planck equation in (9b)(Sarkka and Solin, 2019, Sec. 5.2). We respect (9c) by optimizing over endpoint-constrained path measures, which yields

\[\mathcal{S}=\min_{\mathbb{Q}\text{ s.t. }\mathbb{Q}=\delta,\mathbb{Q}= \delta}D_{KL}[\mathbb{Q}_{0:T}^{v}:\mathbb{P}_{0:T}^{\text{pref}}]\] (37)

as desired. 

## Appendix B Gaussian Path Parameterizations

**Proposition 3**.: _For the family of endpoint-conditioned marginals \(q_{t|0,T}(x)=\mathcal{N}(x\,|\,\mu_{t|0,T},\Sigma_{t|0,T})\),_

\[u_{t|0,T}^{(q,\theta)}(x)\coloneqq\frac{\partial\mu_{t|0,T}}{\partial t}+\bigg{[} \frac{1}{2}\frac{\partial\Sigma_{t|0,T}}{\partial t}\Sigma_{t|0,T}^{-1}-G_{t} \ \Sigma_{t|0,T}^{-1}\bigg{]}\big{(}x-\mu_{t|0,T}\big{)}\] (13)

_satisfies the Fokker-Planck equation (12) for \(q_{t|0,T}\) and diffusion coefficients \(G_{t}=\frac{1}{2}\Xi_{t}\Xi_{t}^{T}\)._

Proof.: Consider the following identities for the Gaussian family of marginals \(q_{t}(x)=\mathcal{N}(x|\mu_{t},\Sigma_{t})\), where we omit conditioning \(q_{t}\gets q_{t|0,T}\) for simplicity of notation,

\[\log q_{t}(x) =-\frac{1}{2}(x-\mu_{t})^{T}\Sigma_{t}^{-1}(x-\mu_{t})-\frac{d}{ 2}\log(2\pi)-\frac{1}{2}\log\det\Sigma_{t}\,,\] (38a) \[\nabla_{x}\log q_{t}(x) =-\Sigma_{t}^{-1}(x-\mu_{t})\,,\] (38b) \[\frac{\partial}{\partial t}\log q_{t}(x) =(x-\mu_{t})^{T}\Sigma_{t}^{-1}\frac{\partial\mu_{t}}{\partial t} +\frac{1}{2}(x-\mu_{t})^{T}\Sigma_{t}^{-1}\frac{\partial\Sigma_{t}}{ \partial t}\Sigma_{t}^{-1}(x-\mu_{t})-\frac{1}{2}\text{tr}\bigg{(}\Sigma_{t}^{ -1}\frac{\partial\Sigma_{t}}{\partial t}\bigg{)}\] (38c)

We begin by solving for a vector field \(u_{t}^{\circ}(x)\) that satisfies the continuity equation (where \(u_{t}^{\circ}\) denotes the drift of an ODE)

\[\frac{\partial q_{t}}{\partial t}=-\big{\langle}\nabla_{x},q_{t} u_{t}^{\circ}\big{\rangle}=-q_{t}\big{\langle}\nabla_{x},u_{t}^{\circ}\big{\rangle}+ \big{\langle}\nabla_{x}q_{t},\nabla_{x}u_{t}^{\circ}\big{\rangle}\] \[\implies\frac{\partial}{\partial t}\log q_{t} =-\big{\langle}\nabla_{x},u_{t}^{\circ}\big{\rangle}-\big{\langle}\nabla_{x} \log q_{t},u_{t}^{\circ}\big{\rangle}\] (39)The vector field satisfying this equation is

\[u_{t}^{\text{o}}(x)=\frac{\partial\mu_{t}}{\partial t}+\frac{1}{2}\frac{\partial \Sigma_{t}}{\partial t}\Sigma_{t}^{-1}(x-\mu_{t})\] (40)

which we can confirm using the identities in (38). Indeed, for the terms on the RHS of Eq. (39),

\[-\big{\langle}\nabla_{x},u_{t}^{\text{o}}\big{\rangle}= \ -\frac{1}{2}\text{tr}\bigg{(}\Sigma_{t}^{-1}\frac{\partial \Sigma_{t}}{\partial t}\bigg{)}\,,\] \[-\big{\langle}\nabla_{x}\log q_{t},u_{t}^{\text{o}}\big{\rangle}= \ \bigg{\langle}\Sigma_{t}^{-1}(x-\mu_{t}),\frac{\partial\mu_{t}}{ \partial t}\bigg{\rangle}+\frac{1}{2}(x-\mu_{t})^{T}\Sigma_{t}^{-1}\frac{ \partial\Sigma_{t}}{\partial t}\Sigma_{t}^{-1}(x-\mu_{t})\,.\]

Putting these terms and the time derivative from (38c) into Eq. (39) we conclude the proof.

However, we are eventually interested in finding the formula for the drift \(u_{t}\) that satisfies the Fokker-Planck equation in (12). That is, to describe the same evolution of density \(\frac{\partial q(x)}{\partial t}\), the relationship between \(u_{t}\) and \(u_{t}^{\text{o}}\) is as follows

\[\frac{\partial q_{t}(x)}{\partial t}=-\big{\langle}\nabla_{x},q_ {t}\,u_{t}^{\text{o}}\big{\rangle}= \ -\big{\langle}\nabla_{x},q_{t}\,u_{t}\big{\rangle}+\big{\langle} \nabla_{x},G_{t}\nabla_{x}q_{t}\big{\rangle}\] \[= \ -\big{\langle}\nabla_{x},q_{t}\,u_{t}\big{\rangle}+\big{\langle} \nabla_{x},G_{t}q_{t}\nabla_{x}\log q_{t}\big{\rangle}\] \[= \ -\bigg{\langle}\nabla_{x},q_{t}\underbrace{(u_{t}-G_{t}\nabla _{x}\log q_{t})}_{u^{\text{o}}}\bigg{\rangle}\]

Finally, we use the identities in (38) to obtain

\[u_{t}=\ u_{t}^{\text{o}}+G_{t}\nabla_{x}\log q_{t}= \ \frac{\partial\mu_{t}}{\partial t}+\frac{1}{2}\frac{\partial \Sigma_{t}}{\partial t}\Sigma_{t}^{-1}(x-\mu_{t})-G_{t}\Sigma_{t}^{-1}(x-\mu_ {t})\] \[\Longrightarrow\quad u_{t}= \ \frac{\partial\mu_{t}}{\partial t}+\bigg{[}\frac{1}{2}\frac{ \partial\Sigma_{t}}{\partial t}\Sigma_{t}^{-1}-G_{t}\Sigma_{t}^{-1}\bigg{]}(x- \mu_{t})\]

**Proposition 4**.: _Given a set of processes \(q_{t|0,T}^{k}(x)\) and mixtures weights \(w^{k}\), the vector field satisfying the Fokker-Planck equation in (12) for the mixture \(q_{t|0,T}(x)=\sum_{k}w^{k}q_{t|0,T}^{k}(x)\) is given by_

\[u_{t|0,T}^{(q,\theta)}(x)=\sum_{k=1}^{K}\frac{w^{k}q_{t|0,T}^{k}(x)}{\sum_{j=1 }^{K}w^{j}q_{t|0,T}^{j}(x)}u_{t|0,T}^{(q,k)}(x)\,,\] (17)

_where \(u_{t|0,T}^{(q,k)}(x)\) satisfies the Fokker-Planck equation in (12) for \(q_{t|0,T}^{k}(x)\). This identity holds for both first-order dynamics in spatial coordinates only or second-order dynamics in \(x=(\bar{x},\bar{v})\)._

Proof.: See Peluchetti (2023) Theorem 1 and its proof in their App. A. 

## Appendix C Extended Related Work

### Machine Learning for Molecular Simulation

The main dilemma of molecular dynamics comes from the accuracy and efficiency trade-off--accurate simulation requires solving the Schrodinger equation which is computationally intractable for large systems, while efficient simulation relies on empirical force fields which is inaccurate. Recently, there has been a surge of work in applying machine learning approaches to accelerate molecular simulation. One successful paradigm is machine learning force field (MLFF) which leverages the transferability and efficiency of machine learning methods to fit force/energy prediction models on quantum mechanical data and transfer across different atomic systems Smith et al. (2017); Wang et al. (2018). More recently, increasing attention has been focused on building atomic foundation models to encompass all types of molecular structures Batatia et al. (2023); Shoghi et al. (2023); Zhang et al. (2022).

Sampling is a classical problem in molecular dynamics to draw samples from the Boltzmann distribution of molecular systems. Classical methods mainly rely on Markov chain Monte Carlo (MCMC) or MD which requires long mixing time for multimodal distributions with high energy barriers Rotskoff (2024). Generative models in machine learning demonstrate promises in alleviating this problem by learning to draw independent samples from the Boltzmann distribution of molecular systems (known as Boltzmann generator) Noe et al. (2019). Numerous methods have been developed to utilize generative models as a proposal distribution for escaping local minima in running MCMC methods Gabrie et al. (2022). However, one critical issue is that generative models rely on training from samples. Although recent advances have been developed to learn from unnormalized density (i.e., energy) function, the training inefficiency limits their applicability to solve high-dimensional molecular dynamics problems. To circumvent the curse of dimensionality for the sampling problem, another branch of work study to learn coarse-grained representation with neural networks Sidky et al. (2020). For broader literature of applying machine learning to enhanced sampling, we refer the reader to Mehdi et al. (2024).

## Appendix D Further Experimental Details

### Evaluation Metrics

To assess the quality of our approach in terms of performance and physicalness of paths, we compare them under different metrics to well-established TPS techniques. One important describing factor of a trajectory is the molecule's highest energy during the transition. These high-energy states are often referred as transition states and less likely to occur but they determine importance factors during chemical reaction such as reaction rate. As such, we will look at the maximum energy along the transition path and use it to compare the ensemble of trajectories more efficiently. The main goal is to show that lower energy of the transition states can be sampled by the methods.

However, the maximum energy does not account for the fact that the transition path needs to be sequential, and each step needs to be coherent based on the previous position and momentum. For this, we also compare the likelihood of the paths (i.e., unnormalized density) by computing the probably of being in the start state \(\rho(x_{0})\) and multiplying it with the step probability such that

\[L(x_{0},x_{1},\ldots,x_{N-1})=\rho(x_{0})\cdot\prod_{i=0}^{N-2}\pi(x_{i+1}\,| \,x_{i})\,.\] (41)

For the step probability \(\pi\), we solve the Langevin leap-frog implementation as implemented in OpenMM to solve \(\mathcal{N}(x_{i+1}\,|\,x_{i}+dt\cdot b_{t}(x),dt\sigma_{i}^{2})\). As for the starting probability, we compute the unnormalized density of the Boltzmann distribution for our start state \(z\) and assume that the velocity \(v\) can be sampled independently (Castellan, 1983, Sec. 4.6)

\[\rho(z,v)\propto\exp\!\left(-\frac{U(z)}{k_{B}T}\right)\cdot\mathcal{N}\! \left(v\,|\,0,k_{B}T\cdot M^{-1}\right),\] (42)

with the Boltzmann constant \(k_{B}\) and the diagonal matrix \(M\) containing the mass of each atom.

As for the performance, the number of energy evaluations will be the main determining factor of the runtime for larger molecular systems, especially for proteins. We hence compare the use of the number of energy computations as a proxy for hardware-independent relative measurements. In our tests, this number aligned with the relative runtime of these approaches.

### Toy Potentials

The toy systems move according to the following integration scheme (first-order Euler)

\[x_{t+1}=x_{t}-dt\cdot\nabla_{x}U(x_{t})+\sqrt{dt}\cdot\texttt{diag}(\xi)\cdot \varepsilon,\quad\varepsilon\sim\mathcal{N}(0,1)\,,\] (43)

following the definition of our stochastic system in Sec. 2.2 with a time-independent Wiener process, where \(\xi\) is a constant time-independent standard deviation for all dimensions.

**Muller-Brown.** The underlying Muller-Brown potential that has been used for our experiments can be written as

\[\begin{split} U(x,y)=&-200\cdot\exp\bigl{(}-(x-1)^{2}- 10y^{2}\bigr{)}\\ &-100\cdot\exp\bigl{(}-x^{2}-10\cdot(y-0.5)^{2}\bigr{)}\\ &-170\cdot\exp\bigl{(}-6.5\cdot(0.5+x)^{2}+11\cdot(x+0.5)\cdot(y- 1.5)-6.5\cdot(y-1.5)^{2}\bigr{)}\\ &+15\cdot\exp\bigl{(}0.7\cdot(1+x)^{2}+0.6\cdot(x+1)\cdot(y-1)+ 0.7\cdot(y-1)^{2}\bigr{)}\,.\end{split}\] (44)

We used a first-order Euler integration scheme to simulate transition paths with 275 steps and a \(dt\) of \(10^{-4}s\). \(\xi\) was chosen to be 5 and 1,000 transition paths were simulated. We have used an MLP with four layers and a hidden dimension of 128 each, with swish activations. It has been trained for 2,500 steps with a batch size of 512 and a single Gaussian.

In Fig. 4(a), we compare the likelihood of the sampled paths. We can see that one-way shooting takes time until the path is decorrelated from the initial trajectory, which is shorter and thus has a higher likelihood. All MCMC methods exhibit this behavior, which is typically alleviated by using a warmup period in which all paths are discarded. After that, all methods exhibit similar likelihood, with our method having a slightly lower likelihood. Looking at the transition state (i.e., maximum energy on the trajectory) in Fig. 4(b) reveals that all methods have a similar quality of paths.

We can further analyze the quality of our method by investigating the difference between the "ground truth" marginal \(\rho_{t|0,T}(x)\) and the learned marginal \(q_{t|0,T}(x)\). For this, we compute the Wasserstein W1 distance (Flamary et al., 2021) between the marginal observed by fixed-length two-way shooting (which we assume to be close to the ground truth) and our variational approach. We observe a mean W1 distance of \(0.130\pm 0.026\) and visualize it along the time coordinate \(t\) (in steps) in Fig. 6.

**Dual-Channel Double-Well.** To demonstrate the advantage of mixtures, we have used the two-dimensional potential

\[\begin{split} U(x,y)=&+2\cdot\exp\bigl{(}-(12x^{2}+ 12y^{2})\bigr{)}\\ &-1\cdot\exp\bigl{(}-(12\cdot(x+0.5)^{2}+12y^{2})\bigr{)}\\ &-1\cdot\exp\bigl{(}-(12\cdot(x-0.5)^{2}+12y^{2})\bigr{)}+x^{6}+ y^{6}\,.\end{split}\] (45)

Figure 5: In \(\mathtt{a}\), we compare the log likelihood of sampled trajectories, where a higher likelihood is generally more favorable. The plot in \(\mathtt{b}\) shows the maximum energy of each individual trajectory. A high maximum energy means that the molecule needs to be in an excited state during the transition, making it less likely to occur under lower temperatures.

Figure 6: In this figure, we compare the Wasserstein W1 distance between the marginals. The densities are almost identical at the beginning and end state. Note that at the third local minimum of the Müller-Brown potential along the trajectory (i.e., reached at about \(200\) steps), the marginals align more closely as well.

In this case, we have used \(dt=5*10^{-4}s\) with a transition time of \(T=1s\) and \(\xi=0.1\). As for the MLP, we have used the same structure as in the Muller-Brown example but trained it for 20,000 iterations. The corresponding weights to Prop. 4 are \(w=[\frac{1}{2},\frac{1}{2}]\) and are fixed for this experiment and hence \(w\not\in\theta\).

### Neural Network Ablation Study

In Fig. 7 we compare how different parameterizations of \(\mu^{(\theta)}_{t|0,T}\), and \(\Sigma^{(\theta)}_{t|0,T}\) impact the quality of trajectories on the Muller-Brown potential. For this, we compare linear and cubic splines (with 20 knots) with neural networks. As a metric to estimate the quality, we compare the Wasserstein W1 distance between the learned marginal \(q_{t|0,T}(x)\) and the marginal observed by fixed-length two-way shooting (i.e., baseline). We notice that using linear splines results in the highest W1 distance, while cubic splines improve the quality. Using neural networks, however, yields the best approximation.

We have fixed the computational budget for all systems, which means that we have trained splines for more epochs than the neural network (since they are slower to train). For high-dimensional systems, the runtime is mostly determined by the number of potential evaluations and not the complexity of the architecture. We thus conclude that the additional expressivity provided by neural networks is necessary for more complicated (molecular) systems and does not introduce much computational overhead.

### Molecular Systems

To simulate molecular dynamics, we rely on the AMBER14 force field (amber14/protein.ff14SB Maier et al. (2015)) without a solvent, as implemented in OpenMM (Eastman et al., 2017). As OpenMM does not support auto-differentiation, we do not use OpenMM for the simulations themselves, but utilize DMFF (Wang et al., 2023) which is a differentiable framework implemented in JAX (Bradbury et al., 2018) for molecular simulation. This is needed because during training we compute \(\nabla_{\theta}U\left(x_{t|0,T}\sim\mathcal{N}(\mu^{(\theta)}_{t|0,T},\Sigma^ {(\theta)}_{t|0,T})\right)\), where the concrete \(x_{t|0,T}\) is sampled based on the parameters of the neural network.

For the concrete simulations, we ran them with the timestep \(dt=1fs\), with \(T=1ps\), \(\gamma=1ps\), and \(\text{Temp}=300K\). To compute the MCMC two-way shooting baselines, we use the same settings and consider trajectories as failed, if they exceed 2,000 steps without reaching the target.

**Neural Network Parameterization.** We parameterize our model with neural networks, a 5-layer MLP with ReLU activation function and 256/512 hidden units for alanine dipeptide and Chignolin, respectively. The neural networks are trained using an Adam optimizer with learning rate \(10^{-4}\).

We represent the molecular system in two ways: (1) in Cartesian coordinates, which are the 3D coordinates of each atoms, and with (2) internal coordinate which instead uses bond length, angle and dihedral angle along the molecule, where we use the same parameterization as in (Noe et al., 2019).

Our state definition includes a variance parameter for the initial and target marginal distributions at \(t=0\) and \(t=T\), we choose the variance to be \(10^{-8}\) which almost does not change the energy of the perturbed system.

**Visualization of Transition for Alanine Dipeptide.** In Fig. 8, we show a transition sampled without any noise from the model with internal coordinates and 2 Gaussian mixtures.

Figure 7: We compare the Wasserstein W1 distance between the marginals with different parameterization techniques of \(\mu^{(\theta)}_{t|0,T}\), and \(\Sigma^{(\theta)}_{t|0,T}\).

**Comparison of Sampled Paths During Training.** In our training procedure, the marginal starts with a linear interpolation between \(A\) and \(B\), which produces very unlikely paths with potentially high energy states. In Fig. 9, we compare how the quality of paths changes depending on the number of training iterations (i.e., the number of potential evaluations). We show the curve for a single Gaussian mixture with Cartesian coordinates. Similar trends can be observed in other settings.

**Loss Curves.** In this section, we would like to investigate the training losses of different configurations. For this, we plot the exponential moving average of the loss (\(\alpha=0.001\)) to better highlight the trends of the noisy variational loss. Fig. 10 compares the results of different training settings. We can observe that mixtures can decrease the overall loss, but all model variations converge to a similar loss value.

### Computational Resources

All our experiments involving training were conducted on a single NVIDIA A100 80GB. The baselines themselves were computed on a M3 Pro 12-core CPU.

## Appendix E Societal Impact

Our research concerns the efficient sampling of transition paths which are crucial for a variety of tasks in biology, chemistry, materials science and engineering. Our research could potentially benefit research areas from combustion, catalysis, protein design to battery design. Nevertheless, we do not foresee special potential negative impacts to be discussed here.

Figure 8: Transition path for the alanine dipeptide.

Figure 10: Visualization of the loss for different training setups. These setups are identical to what has been reported in Table 2.

Figure 9: In this figure, we compare the quality of paths based on the current training step (i.e., potential evaluations). We observe that with increasing training time, paths with higher likelihood are sampled.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The major claims made in the abstract and introduction are backed up by the theoretical and empirical results in the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Sec. 6, we discuss potential limitations and future work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: A detailed set of assumptions is presented in Sec. 3 and further proofs for theoretical results are included in App. A. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We disclose all the information needed to reproduce the main experimental results and make our code available at https://github.com/plainerman/variational-doob. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The data we use are synthetic datasets that can be generated by publicly available codes and real-world molecular systems taken from previous literature cited in the paper. Our code is publicly available at https://github.com/plainerman/variational-doob. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We report the experimental details in Sec. 5 and further specify the parameter and settings in App. D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We appropriately report the mean and standard deviation in the experiments where suitable (compare Table 1 and Table 2). Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We include computational resources used to run the model in App. D.5. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, we believe our research conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the potential societal impact of our method in App. E. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not believe that any safe guards needs to be in place, as our approach does not pose any direct risks. As for the data, only publicly available and commonly used resources have been used and, hence, does not pose a risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use public datasets and libraries in this research and have cited the sources. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: [NA] Guidelines: No new assets are introduced in this research.
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing or human subjects are involved in this research. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No human subjects are involved in this research. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.