# Knowledge Composition using Task Vectors with Learned Anisotropic Scaling

 Frederic Z. Zhang\({}^{*}\) Paul Albert\({}^{*}\) Cristian Rodriguez-Opazo

**Anton van den Hengel Ehsan Abbasnejad**

Australian Institute for Machine Learning  The University of Adelaide

{firstname.lastname}@adelaide.edu.au

https://github.com/fredzzhang/atlas

Equal contribution. Listed order was determined by a coin toss. Fred formalised the idea; developed the original codebase; conducted experiments on task arithmetic; and drafted the paper. Paul designed and conducted experiments on few-shot recognition, test-time adaptation, parameter-efficient fine-tuning; investigated numerous properties of task vector compositions; and was crucially involved in every stage of the work.

###### Abstract

Pre-trained models produce strong generic representations that can be adapted via fine-tuning on specialised datasets. The learned weight difference relative to the pre-trained model, known as a task vector, characterises the direction and stride of fine-tuning that enables the model to capture these specialised representations. The significance of task vectors is such that simple arithmetic operations on them can be used to combine diverse representations from different domains. This paper builds on these properties of task vectors and aims to answer (1) whether components of task vectors, particularly parameter blocks, exhibit similar characteristics, and (2) how such blocks can be used to enhance knowledge composition and transfer. To this end, we introduce aTLAS, an algorithm that linearly combines parameter blocks with different learned coefficients, resulting in anisotropic scaling at the task vector level. We show that such linear combinations explicitly exploit the low intrinsic dimensionality of pre-trained models, with only a few coefficients being the learnable parameters. Furthermore, composition of parameter blocks enables modular learning that effectively leverages the already learned representations, thereby reducing the dependency on large amounts of data. We demonstrate the effectiveness of our method in task arithmetic, few-shot recognition and test-time adaptation, with supervised or unsupervised objectives. In particular, we show that (1) learned anisotropic scaling allows task vectors to be more disentangled, causing less interference in composition; (2) task vector composition excels with scarce or no labelled data and is less prone to domain shift, thus leading to better generalisability; (3) mixing the most informative parameter blocks across different task vectors prior to training can reduce the memory footprint and improve the flexibility of knowledge transfer. Moreover, we show the potential of aTLAS as a parameter-efficient fine-tuning method, particularly with less data, and demonstrate that it can be easily scaled up for higher performance.

## 1 Introduction

One practical advantage of neural networks is the fact that knowledge learned from a previous problem, in the form of network weights, can be transferred to solve other related problems. Commonly referred to as transfer learning [6; 73], this technique is often applied when a model trained on a general-purpose dataset--ImageNet [52] for many years--is fine-tuned on other datasets to improveperformance on downstream problems. In the past, classification models [18, 53] have been used as the medium for such knowledge transfer, which played a crucial part in the success of detection and segmentation [66, 67, 19, 51, 68, 68]. In recent years, foundation models [4] trained on broad data, CLIP [47] particularly, have demonstrated strong performance on a multitude of tasks, even when applied in a zero-shot manner. Besides the conventional way of exploiting the knowledge in these models via fine-tuning, recent works [28, 44, 62] have presented more direct measures to manipulate the network weights. In particular, Ilharco et al. [28] showed that, a task vector, defined as the weight difference between a pre-trained and a fine-tuned model, can be used as a carrier of the task-specific knowledge learned via fine-tuning. As such, multiple task vectors, when combined with simple arithmetic, can form a multi-task model that largely retains its performance across all fine-tuning tasks. Linearisation techniques [44], in addition, have been shown to further enhance this compositionality.

Intrigued by this phenomenon, we investigate the potential of task vectors being knowledge carriers in this paper, by learning linear combinations of them (Figure 0(a)) for various problems. In particular, parameter blocks, e.g., weights and biases, tend to encode different learned representations in different layers. We thus learn an independent scaling coefficient per block for more precise adjustments tailored to the unique roles of each parameter block. This results in anisotropic scaling of task vectors (Figure 0(b)), and allows us to exploit their modularity in knowledge composition, granting higher controllability when steering the behaviours of a model for task arithmetic [28].

The potential applications of task vector composition extend beyond model editing. With the coefficients being the only learnable parameters, our method exploits the rich knowledge encapsulated in the task vectors by searching in a low-dimensional coefficient space. As a result, it is a competitive parameter-efficient fine-tuning (PEFT) method, and is particularly effective in cases where labelled data is scarce. This offers new opportunities for few-shot learning [34, 69] and test-time adaptation [35, 57]. Furthermore, for multi-purpose models such as CLIP [47], variants of the model trained with different data sources or fine-tuned on different downstream tasks are often available [26]. These resources constitute a significant knowledge bank, with task vectors being the knowledge carrier. Many learning problems may be simplified to learning a combination of task vectors.

Our primary contribution is a learning algorithm named aTLAS, wherein otherwise complex learning problems can be framed as learning linear combinations of task vectors. The algorithm is broadly applicable to optimising supervised and unsupervised objectives. Its effectiveness is demonstrated in task arithmetic, few-shot recognition, test-time adaptation and parameter-efficient fine-tuning, where we show that (1) learning linear combinations of task vectors directly exploits the low intrinsic dimensionality of pre-trained models [1, 33], resulting in a small number of learnable parameters; (2) standard task vectors, otherwise inferior to linearised variants [44] in task arithmetic, can produce stronger multi-task models with learned anisotropic scaling; (3) aTLAS is effective in low-data regimes, and improves the accuracy of CLIP by \(6.5\) absolute points averaged over \(22\) datasets with unlabelled data; (4) aTLAS is complementary to previous few-shot adaptation methods, in that one third of the examples it improves upon are unique; (5) aTLAS as a few-shot learning method is less prone to domain shift, and achieves better generalisation on out-of-domain datasets; (6) the most informative parameter blocks from different task vectors can be mixed prior to training, allowing for flexible and efficient knowledge transfer under memory constraints; (7) aTLAS is a strong PEFT method when data is limited, and existing PEFT methods such as low-rank adaptations (LoRA) [23] can be seamlessly integrated into aTLAS to improve memory efficiency.

Figure 1: Illustration of (a) learning task vector compositions (\(n=2\), \(\bm{\theta}_{0}\) denotes the weights of a pre-trained model) and (b) the flexibility of anisotropic scaling. Assume a task vector \(\bm{\tau}=\left(\bm{\tau}^{(1)},\bm{\tau}^{(2)}\right)\) has two parameter blocks, learning anisotropic scaling grants more flexibility when combining task vectors.

Models and task vectors

As Ilharco et al. [28] demonstrated, task vectors exhibit many intriguing properties across a wide range of models, such as CLIP [47], GPT-2 [46] and T5-based models [48]. To facilitate more in-depth experimentation and analysis, we focus on the CLIP model in this paper, due to its wide availability and manageable size. In particular, we follow previous practice [44; 28] and acquire task vectors by fine-tuning the image encoder, with the text representations frozen. This ensures that image encoders fine-tuned on different datasets produce features residing in the same representation space, through a common text encoder. The task vectors obtained from these fine-tuned encoders can thus be combined more effectively to form a unified multi-task model.

Formally, denote the CLIP image encoder by \(f:\mathcal{X}\times\Theta\rightarrow\mathcal{Z}\), such that for input image \(\mathbf{x}\in\mathcal{X}\) and parameters \(\boldsymbol{\theta}\in\Theta\), \(\mathbf{z}=f(\mathbf{x};\boldsymbol{\theta})\) is the learned latent representation for the input image. Denote the weights of a pre-trained model by \(\boldsymbol{\theta}_{0}\), and the weights of its fine-tuned variant by \(\boldsymbol{\theta}_{i}\), \(i\in\mathbb{N}^{+}\), where \(i\) indexes a dataset \(\mathcal{D}_{i}\). We follow Ilharco et al. [28] and define a task vector as \(\boldsymbol{\tau}_{i}=\boldsymbol{\theta}_{i}-\boldsymbol{\theta}_{0}\). In addition, we investigate task vectors produced by linearised variants of the image encoder using the first-order Taylor expansion,

\[g(\mathbf{x};\boldsymbol{\theta})\coloneqq f(\mathbf{x};\boldsymbol{\theta}_{ 0})+(\boldsymbol{\theta}-\boldsymbol{\theta}_{0})^{\mathsf{T}}\nabla_{ \boldsymbol{\theta}}f(\mathbf{x};\boldsymbol{\theta}_{0}).\] (1)

Ortiz-Jimenez et al. [44] showed that, task vectors obtained from fine-tuning the linearised variants have low disentanglement errors, and exhibit strong compositional properties.

## 3 Learning task vector compositions

Parameters in a neural network, depending on the depth of the layer, often have different significance. For instance, early layers in convolutional neural networks [18; 53] are known for extracting generic, low-level features, such as edges, corners, etc., while deeper layers produce features more specific to the task. We recognise the non-uniform impacts parameters at different layers can have, and do not perform isotropic scaling on task vectors. Instead, weights, biases and any other forms of parameterisation, which we collectively refer to as _parameter blocks_, will be scaled independently.

### Proposed method: aTLAS

Formally, denote a task vector with \(m\) parameter blocks by \(\boldsymbol{\tau}=\big{(}\boldsymbol{\tau}^{(1)},\ldots,\boldsymbol{\tau}^{ (m)}\big{)}\), where each parameter block \(\boldsymbol{\tau}^{(j)}\) is vectorised, and round brackets denote column vector concatenation. We learn a block diagonal matrix \(\Lambda\), parameterised as

\[\Lambda=\begin{bmatrix}\lambda^{(1)}I^{(1)}&\ldots&\boldsymbol{0}\\ \vdots&\ddots&\vdots\\ \boldsymbol{0}&\ldots&\lambda^{(m)}I^{(m)}\end{bmatrix},\] (2)

where \(\lambda^{(j)}\in\mathbb{R}\) is a learnable coefficient; \(I^{(j)}\) denotes an identity matrix with its number of columns matching the dimension of \(\boldsymbol{\tau}^{(j)}\); and the superscript \(j\in\mathbb{N}^{+}\) indexes a parameter block. This results in anisotropic scaling of a task vector, that is,

\[\Lambda_{i}\boldsymbol{\tau}_{i}=\Big{(}\lambda^{(1)}_{i}\boldsymbol{\tau}^{ (1)}_{i},\ldots,\lambda^{(m)}_{i}\boldsymbol{\tau}^{(m)}_{i}\Big{)},\] (3)

where the subscript \(i\in\mathbb{N}^{+}\) indexes a task vector. As such, assuming a supervised objective, finding the optimal composition of task vectors can be defined as the following optimisation problem

\[\underset{\Lambda_{1},\ldots,\Lambda_{n}}{\text{arg min}}\ \mathbf{E}_{( \mathbf{x},\mathbf{y})\in\mathcal{D}_{t}}\big{[}\mathcal{L}\big{(}f(\mathbf{x };\boldsymbol{\theta}_{0}+\sum_{i=1}^{n}\Lambda_{i}\boldsymbol{\tau}_{i}), \mathbf{y}\big{)}\big{]},\] (4)

where \(\mathcal{L}\) is the loss function for a target task; \(n\) is the number of task vectors; \(\mathbf{y}\) is the labels corresponding to inputs \(\mathbf{x}\); \(\mathcal{D}_{t}\) denotes a target dataset. The number of learnable parameters, as a result, is precisely \(mn\), Let us denote the solution to the aforementioned optimisation problem by \(\{\Lambda^{*}_{i}\}_{i=1}^{n}\). In inference, model \(f(\mathbf{x},\boldsymbol{\theta}_{0}+\sum_{i=1}^{n}\Lambda^{*}_{i}\boldsymbol{ \tau}_{i})\) will be deployed, which incurs no additional computational cost compared to models trained in the conventional way.

In addition, we investigate the task vectors obtained from fine-tuning linearised variants of the model, i.e., \(g(x)\) in Eq. 1. Denote such task vectors by \(\widetilde{\bm{\tau}}\). The learning objective with linearised task vectors can be derived as follows

\[\underset{\Lambda_{1},\ldots,\Lambda_{n}}{\text{arg min}}\ \mathbf{E}_{(\mathbf{x}, \mathbf{y})\in\mathcal{D}_{\text{t}}}\Big{[}\mathcal{L}\Big{(}f(\mathbf{x}; \bm{\theta}_{0})+(\sum_{i=1}^{n}\Lambda_{i}\widetilde{\bm{\tau}}_{i})^{\text{ T}}\nabla_{\bm{\theta}}f(\mathbf{x};\bm{\theta}_{0}),\mathbf{y}\Big{)} \Big{]}.\] (5)

### Relation to intrinsic dimensionality

A notable characteristic of aTLAS is its parameter efficiency. To offer more intuitions, we refer to previous findings [1; 33] that deep neural networks often produce solutions residing in a subspace with much lower intrinsic dimensionality. This is measured by finding a minimum number of \(d\) parameters, such that learning these parameters (\(\bm{\hat{\theta}}\in\mathbb{R}^{d}\)) leads to approximately the same performance as optimising in the full parameter space (\(\bm{\theta}\in\mathbb{R}^{D}\)). This can be expressed as follows

\[\bm{\theta}=\bm{\theta}_{0}+P\bm{\hat{\theta}},\] (6)

where \(\bm{\theta}_{0}\in\mathbb{R}^{D}\) denotes the pre-trained weights and \(P\in\mathbb{R}^{D\times d}\) is a random projection matrix. We demonstrate that learning task vector compositions leads to the same formulation. For brevity of exposition, let us consider compositions at the block level. For the \(j\)-th parameter block, we have

\[\bm{\theta}^{(j)} =\bm{\theta}_{0}^{(j)}+\sum\nolimits_{i=1}^{n}\lambda_{i}^{(j)} \bm{\tau}_{i}^{(j)}\] (7) \[=\bm{\theta}_{0}^{(j)}+\underbrace{\Big{[}\bm{\tau}_{1}^{(j)}, \ldots,\bm{\tau}_{n}^{(j)}\Big{]}}_{\text{projection matrix}}\underbrace{ \Big{[}\lambda_{1}^{(j)},\ldots,\lambda_{n}^{(j)}\Big{]}^{\text{T}}}_{\text{ learnable parameters}}.\] (8)

We draw a parallel between Eqs. 6 and 8 and note that aTLAS explicitly exploits the low intrinsic dimensionality by learning a small set of coefficients. The number of task vectors, i.e., \(n\), is much smaller than the dimension of weight vector \(\bm{\theta}_{i}^{(j)}\), and is analogous to the intrinsic dimensionality \(d\). However, as opposed to using a random projection matrix \(P\), aTLAS constructs the projection matrix from task vectors, making use of the learned representations. To demonstrate its advantage, we use the same number of bases for task vectors2 and random bases3, and show that task vectors consistently achieve higher performance in Figure 2. These results solidify our understanding of task vectors being knowledge carriers. We thus set out to apply aTLAS to various applications.

Footnote 2: A fixed number of task vectors are selected based on the blockwise gradient. Details can be found in Section 5.2 and Appendix D.6.

Footnote 3: Each random basis of the projection is drawn from a Gaussian distribution with the mean and standard deviation to match those of the pre-trained weights in the corresponding parameter block, i.e., \(\bm{\theta}_{0}^{(j)}\).

## 4 Task arithmetic

Task arithmetic [28] is comprised of a few tasks aimed at editing pre-trained models using task vectors. Following previous practice [28; 44], we conduct experiments under the settings of task negation and task addition on eight image classification datasets (details included in Appendix A).

Figure 2: Recognition accuracy versus the number of bases when optimising in a low-dimensional subspace. The accuracy is normalised by that of the fully fine-tuned model. Using task vectors to construct the projection matrix performs consistently better than using random bases on (a) MNIST [32], (b) CIFAR100 [31].

Previous works acquire the optimal isotropic scaling factor on task vectors via a hyper-parameter search on validation sets. As such, we learn anisotropic scaling matrices on the same validation sets, and visualise the learned coefficients to shed light on this mechanism.

### Task negation

Task negation aims to reduce undesired biases, characterised by the performance, on a target task, while maintaining performance on a control dataset, ImageNet [52] in this case. Denote the validation sets for the target and control tasks by \(\mathcal{D}_{t}\) and \(\mathcal{D}_{c}\), respectively. We perform a simultaneous gradient ascent on the target task and gradient descent on the control task, described as follows,

\[\operatorname*{arg\,min}_{\Lambda_{t}}\mathbf{E}_{(\mathbf{x},\mathbf{y}) \in\mathcal{D}_{t}}[-\mathcal{L}(f(\mathbf{x};\boldsymbol{\theta}_{0}+ \Lambda_{t}\boldsymbol{\tau}_{t}),\mathbf{y})]+\mathbf{E}_{(\mathbf{x}, \mathbf{y})\in\mathcal{D}_{c}}[\mathcal{L}(f(\mathbf{x};\boldsymbol{\theta}_{0 }+\Lambda_{t}\boldsymbol{\tau}_{t}),\mathbf{y})],\] (9)

where \(\boldsymbol{\tau}_{t}\) is the task vector for the target dataset, and cross-entropy loss is used. The learning objectives with linearised task vectors can be derived easily based on Eq. 5, and so are omitted.

We summarise the task negation results in Table 1, and show that our method significantly improves upon standard task vectors, while the improvement upon linear task vectors is less prominent. In particular, we observe that weights matrices tend to have much larger negative coefficients, as shown in Figure 2(a). To investigate this, we instead only learn coefficients for the weight matrices, with zero coefficients on other parameter blocks, effectively reducing the number of learnable parameters by two thirds. With ViT-B/32 as the backbone, we observe an average accuracy of \(20.14\) (vs. \(18.76\)) on target tasks and \(61.23\) (vs. \(61.21\)) on the control task, which shows that weight matrices carry majority of the knowledge required for task negation.

### Task addition

Task addition aims at producing a multi-task model using task vectors acquired from a range of datasets. We utilise task vectors from the eight image classification datasets, and learn the anisotropic

\begin{table}
\begin{tabular}{l l l c c c c c c} \hline \hline  & & & \multicolumn{2}{c}{**ViT-B/32**} & \multicolumn{2}{c}{**ViT-B/16**} & \multicolumn{2}{c}{**ViT-L/14**} \\ \cline{4-9} T.V. & Methods & Models & Target (\(\downarrow\)) & Control (\(\uparrow\)) & Target (\(\downarrow\)) & Control (\(\uparrow\)) & Target (\(\downarrow\)) & Control (\(\uparrow\)) \\ \hline n/a & Pre-trained & \(f(\mathbf{x};\theta_{0})\) & 48.14 & 63.35 & 55.48 & 68.33 & 64.89 & 75.54 \\ \hline \multirow{3}{*}{\(\mathbf{x}_{\mathbf{i}}^{\prime}\)} & Search & \(f(\mathbf{x};\theta_{0}+\alpha\boldsymbol{\tau})\) & 23.22 & 60.71 & 19.38 & 64.66 & 19.15 & 72.05 \\  & aTLAS (ours) & \(f(\mathbf{x};\theta_{0}+\Lambda\boldsymbol{\tau})\) & **18.76** & **61.21** & **17.34** & **65.84** & **17.75** & **73.28** \\ \hline \multirow{3}{*}{\(\mathbf{x}_{\mathbf{i}}^{\prime}\)} & Search & \(g(\mathbf{x};\theta_{0}+\alpha\widetilde{\boldsymbol{\tau}})\) & 11.54 & 60.74 & 10.88 & 65.54 & 12.78 & 72.95 \\  & aTLAS (ours) & \(g(\mathbf{x};\theta_{0}+\Lambda\widetilde{\boldsymbol{\tau}})\) & **11.06** & **61.02** & **10.16** & **65.58** & **12.61** & **73.14** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance of task negation averaged across eight datasets. Selected results must maintain at least 95% of the pre-trained accuracy on the control dataset, following previous practice [44]. Best performance in each section is highlighted in bold. Task vector is abbreviated as t.v. Results for each dataset are available in Table 7.

Figure 3: Box-and-whisker plots for the learned coefficients. As each transformer layer consists of a fixed set of parameter blocks, we visualise the distribution of coefficients for these parameter blocks across all layers, for (a) task negation and (b) task addition, as well as (c) distribution of coefficients by layer. We denote the learnable LayerNorm parameters by \(\gamma\) and \(\beta\). Weights and biases are denoted by \(W\) and \(\mathbf{b}\), respective, with attention layer parameters indexed by superscripts and the MLP parameters indexed by subscripts.

scaling matrices with the objectives described in Eqs. 4, 5 using the cross-entropy loss. The training data is comprised of the validation sets for all eight dataset, i.e., \(\mathcal{D}_{t}=\bigcup_{i=1}^{8}\mathcal{D}_{i}\).

Performance comparison against previous methods is shown in Table 2, where our method yields substantial improvements. Interestingly, we note that with previous methods [28; 44], linear task vectors outperform the standard ones in terms of absolute accuracy, while the converse is true with our method. To investigate this, we compute the pairwise disentanglement error \(\xi\)[44], which measures the percentage of data with inconsistent predictions when two task vectors are combined (more details in Appendix C.2). Results in Figure 4 show that standard task vectors with learned anisotropic scaling achieve the lowest average error, indicating less interference in task vector composition. Along with higher fine-tuning accuracy, previously referred to as the non-linear advantage [44], standard task vectors demonstrate stronger performance in task addition.

Furthermore, we again observe that weight matrices have consistently larger coefficients in Figure 2(b), and learning coefficients on weight matrices alone results in an accuracy of \(84.17\) (vs. \(84.98\)) using ViT-B/32. This suggests that weight matrices in transformers are the primary knowledge carrier, which enabled knowledge composition and negation. Note that for better clarity in visualisation, we add \(L_{1}\) regularisation on the learned coefficients during learning, which causes marginal performance drop (\(84.23\) vs. \(84.98\)) but significantly improves interpretability. In addition, we observe substantially higher coefficients on deeper layers (Figure 2(c)). This aligns with our understanding that early layers extract generic features that do not vary significantly across datasets [29], while the deeper layers produce task-specific features and require more careful adaptations.

## 5 Knowledge transfer in low-data regimes

Beyond model editing for task arithmetic, we explore the idea of transferring existing knowledge in task vectors to previously unseen tasks. To this end, we use the CLIP [47] model and a total of \(22\) image classification datasets, each of which produces a task vector. We defer the details of datasets and the process to acquire task vectors to Appendix A. Denote the set of available task vectors by \(T=\{\bm{\tau}_{i}\}_{i=1}^{n}\), and the dataset corresponding to task vector \(\bm{\tau}_{i}\) by \(\mathcal{D}_{i}\). For each target dataset \(\mathcal{D}_{t}\), we

\begin{table}
\begin{tabular}{l l l c c c c c c} \multicolumn{1}{c}{} & & \multicolumn{2}{c}{**ViT-B/32**} & \multicolumn{2}{c}{**ViT-B/16**} & \multicolumn{2}{c}{**ViT-L/14**} \\ \cline{3-10} \multicolumn{1}{c}{T.V.} & Methods & Models & Abs. (\(\uparrow\)) & Rel. (\(\uparrow\)) & Abs. (\(\uparrow\)) & Rel. (\(\uparrow\)) & Abs. (\(\uparrow\)) & Rel. (\(\uparrow\)) \\ \hline n/a & Pre-trained & \(f(\mathbf{x};\theta_{0})\) & 48.14 & - & 55.48 & - & 64.89 & - \\ \hline \multirow{3}{*}{\begin{tabular}{} \end{tabular} } & Search & \(f\big{(}\mathbf{x};\theta_{0}+\alpha\sum_{i}\bm{\tau}_{i}\big{)}\) & 70.12 & 77.24 & 73.63 & 79.85 & 82.93 & 87.92 \\  & aTLAS (ours) & \(f(\mathbf{x};\theta_{0}+\sum_{i}\Lambda_{i}\bm{\tau}_{i})\) & **84.98** & **93.79** & **86.08** & **93.44** & **91.36** & **97.07** \\ \hline \multirow{3}{*}{
\begin{tabular}{} \end{tabular} } & Search & \(g\big{(}\mathbf{x};\theta_{0}+\alpha\sum_{i}\widetilde{\bm{\tau}}_{i}\big{)}\) & 74.67 & 85.17 & 77.51 & 86.21 & 84.75 & 91.86 \\  & aTLAS (ours) & \(g(\mathbf{x};\theta_{0}+\sum_{i}\Lambda_{i}\widetilde{\bm{\tau}}_{i})\) & **83.42** & **95.42** & **85.38** & **95.10** & **88.65** & **96.12** \\ \hline \end{tabular}
\end{table}
Table 2: Performance of task addition averaged across eight datasets. We report the absolute accuracy (Abs.) and the relative accuracy (Rel.) with respect to the fine-tuned model. Best performance in each section is highlighted in bold. Task vector is abbreviated as t.v. Results for each dataset are available in Table 8.

Figure 4: Disentanglement errors between each pair of datasets. Each row reflects the percentage of data in the corresponding dataset that have altered predictions after combining two task vectors. Our method achieves stronger task addition performance as a result of less interference amongst task vectors.

learn task vector compositions using the subset \(T\setminus\{\bm{\tau}_{t}\}\), excluding the task vector for the target dataset to avoid information leakage. We test our method in few-shot and test-time adaptation, to demonstrate its effectiveness in low-data regimes. Notably, we observe that task vectors complement existing few-shot methods. Combining aTLAS with them thus leads to significant improvements.

### Few-shot adaptation

Few-shot recognition requires learning new objects or concepts using a limited amount labelled data--\(k\) per class for \(k\)-shot. Following previous practice [69], we approach this problem by adapting a pre-trained CLIP model [47] to each target dataset \(\mathcal{D}_{t}\). We use the subset of task vectors \(T\setminus\{\bm{\tau}_{t}\}\) and \(k\in\{1,2,4,8,16\}\) images from dataset \(\mathcal{D}_{t}\). During training, we adopt the cross-entropy loss and minimise objectives described in Eqs. 4 and 5 for standard and linear task vectors, respectively.

We compare against Tip-Adapter [69] and LP++ [25] using CLIP with ViT-B/32 backbone, across \(22\) datasets over three random seeds, and summarise the results in Figure 4(a). We show that with \(k=1\), our approach, aTLAS, significantly outperforms previous methods, demonstrating the effectiveness of knowledge transfer with scarce labelled data. More importantly, we note that the idea of task vector composition is highly complementary to those presented in previous methods. As such, combining aTLAS with them results in significant improvements. This is also illustrated in Figure 4(b) as a Venn diagram, where we show the percentage of examples in the validation set that are incorrectly classified by the pre-trained model but correctly classified with few-shot methods. Out of the examples aTLAS improves upon, around half are unique compared against either Tip-Adapter or LP++, demonstrating its complementarity. We also found that standard task vectors generally perform better than their linearised counterparts, and so defer the results of linear task vectors to Appendix D.2.

In addition, due to the low number of learnable parameters, aTLAS exhibits strong generalisability. To demonstrate this, we learn task vector composition on ImageNet [52], and test it on out-of-domain (OOD) datasets: ImageNet-A [22], ImageNet-R [21], ImageNet-sketch [60] and ImageNetV2 [50]. We summarise the results in Figure 4(c), which shows the performance difference against the pre-trained model. Notably, aTLAS is the only method that consistently improves upon the pre-trained model on OOD datasets, and combining aTLAS with other methods can improve their generalisability.

We also test our method and variants integrated with Tip-Adapter and LP++ using other backbones, including ViT-\(\{\)B/16, L/14\(\}\) and ResNet-\(\{50,101\}\), and find that the results are consistent with those for ViT-B/32. More details can be found in Appendix D.3.

### Task vector budget and selection

In practical applications, there may only be a limited number of task vectors available, or the number of task vectors used in training may be restricted due to memory constraints. To this end, we study the influence of task vector budget \(b\) on few-shot recognition performance. We experiment with four selection strategies: (1) random selection; (2) feature-based selection; (3) gradient-based selection; and (4) blockwise gradient-based selection. To elaborate, feature-based selection computes the mean image feature representation of each dataset, and selects \(b\) task vectors from datasets most similar

Figure 5: Few-shot experiment results averaged across \(22\) datasets and three seeds, showing (a) comparison against state-of-the-art few-shot methods with ViT-B/32 backbone and (b) percentage of images in the validation sets that become correctly classified after applying few-shot methods. We also show (c) performance difference compared to pre-trained CLIP model on OOD datasets. More detailed results are included in Appendix D.

to the target dataset. Gradient-based selection computes the gradient with respect to each of the learnable coefficients, and either select entire task vectors with the highest \(L_{1}\) gradient norm, or select task vectors with the highest blockwise gradient for the corresponding parameter block, and repeat the process for all parameter blocks. The blockwise selection therefore allows parameter blocks across different task vectors to be mixed prior to training. More details can be found in Appendix D.6.

For a task vector budget \(b\in\{1,2,5,10,15,21\}\), we summarise the few-shot recognition performance in Figure 6. First, we note that the accuracy of aTLAS does not plateau with the maximum number of task vectors available (21), indicating that more task vectors could be beneficial. Second, we find that selecting task vectors based on feature similarity is a simple yet effective approach with sufficient budgets (\(b>5\)). Selecting whole task vectors with gradient is less effective, generally on par with random selection. Nevertheless, the blockwise variant achieves the best accuracy, particularly for very low budgets (\(b\in\{1,2\}\)), as it is able to exploit knowledge from more task vectors than the budget dictates. We thus deduce that parameter blocks can function as knowledge carriers in isolation, independent of the task vectors to which they belong. In fact, a parameter block \(\bm{\tau}^{(1)}\) as part of the task vector \(\bm{\tau}=\left(\bm{\tau}^{(1)},\ldots,\bm{\tau}^{(m)}\right)\) can be considered as a task vector by itself, i.e., \(\left(\bm{\tau}^{(1)},\bm{0},\ldots,\bm{0}\right)\). This modular nature underscores the potential of task vectors for flexible and efficient knowledge transfer.

### Test-time adaptation

Test-time adaptation (TTA) [35; 57; 59] assumes no labelled data is available for the target task, requiring the model to adapt in an unsupervised fashion. We conduct experiments under the offline adaptation setting, which allows access to the target dataset. We consider three categories of self-supervised techniques for TTA: constrastive objectives, entropy objectives and pseudo labelling. Contrastive objectives align representations of the same image under different data augmentations. For this category, we adopt SimCLR [9], a simple yet effective method. Entropy objectives encourage the pre-trained model to produce confident predictions on unseen datasets by minimising the entropy over the predictions. This technique was previously explored by Yang et al. [65] in model merging. While effective in simpler cases, it can lead to catastrophic collapse in TTA. Therefore, we utilise a state-of-the-art sharpness-aware entropy minimisation algorithm named SAR [43]. Last, we experiment with an unsupervised pseudo-labelling algorithm inspired by FixMatch [54], which we refer as unsupervised FixMatch (UFM). UFM selects an equal number of highly confident examples per class as the labelled set, and then employs FixMatch to produce pseudo-labels from rest of the unlabelled examples. Details are available in Appendix E.

We summarise the results in Table 3 and compare our method, i.e., learning task vector compositions, against the conventional approach of tuning the layer normalisation parameters [43; 57; 59]. We show that under all self-supervised objectives, aTLAS achieves higher accuracy than tuning the LayerNorm. In particular, LayerNorm has 30k learnable parameters with ViT-B/32 while our method only has 3.5k learnable parameters. We note that with the UFM objective, aTLAS performs the best and improves the accuracy by an average of \(6.5\) absolute points over the zero-shot baseline.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Method & Zero-shot & \multicolumn{2}{c}{Contrastive (SimCLR)} & \multicolumn{2}{c}{Entropy (SAR)} & \multicolumn{2}{c}{Pseudo labelling (UFM)} \\ \cline{3-8}  & & LN & aTLAS & LN & aTLAS & LN & aTLAS \\ \hline Accuracy & \(60.4\) & \(60.4\pm 0.0\) & \(62.7\pm 0.1\) & \(61.2\pm 0.1\) & \(62.9\pm 0.0\) & \(62.2\pm 0.1\) & \(\bm{66.9\pm 0.1}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Test-time adaptation accuracy averaged over 22 dataset, with \(\times 1\) standard error over 3 random seeds. LN refers to tuning the LayerNorm layers. CLIP with the ViT-B/32 backbone is used. Highest performance is highlighted in bold.

Figure 6: Few-shot performance of aTLAS with various task vector budgets. The accuracy is averaged across 22 datasets and over three random seeds. Standard deviation \(\times 1\) is overlaid as the error margin. Performance under the 16-shot setting is visualised, while additional detailed results are included in Table 14.

## 6 Relation to parameter-efficient fine-tuning

One of the key advantages of aTLAS is its ability to adapt pre-trained models with few learnable parameters, making it suitable for parameter-efficient fine-tuning (PEFT). Similar to popular PEFT methods such as low-rank adaptation (LoRA) [23], our approach does not introduce additional modules, thereby avoiding an increase in inference complexity. In addition, since only the encoded weight matrices in LoRAs have non-zero weight difference, LoRAs are in fact sparse task vectors. They can thus be seamlessly integrated into our method, significantly reducing the memory cost.

### LoRAs as task vectors

Due to the sparsity and rank deficiency, LoRAs as task vectors may have limited representation capacity and carry less knowledge. Therefore, they may be inferior to standard task vectors for knowledge transfer. We investigate this by learning linear combinations of LoRAs4 using our method, under the settings of few-shot recognition. Results are summarised in Table 4. We first shed light on the impact of sparsity, and compare two variants of our method that either learns linear combinations of all parameter blocks or just the weight matrices. Results show that sparsity results in an accuracy decrease of around \(0.5\%\) on average, except for the one-shot setting. The rank deficiency, on the other hand, causes more substantial accuracy drop. Nevertheless, this can be largely mitigated by increasing the rank. Using a rank of \(64\) leads to similar performance compared to learning compositions of only weight matrices in standard task vectors. In conclusion, while the sparsity and rank deficiency introduce some performance drops, especially in low-shot settings, LoRAs are competitive alternatives to standard task vectors due to their low memory cost.

Footnote 4: Details about the process to acquire LoRAs are included in Appendix D.7.

### Scalability of aTLAS

Despite the parameter efficiency of aTLAS, its performance is not as competitive when sufficient training data is available. To address this, we devise a strategy to flexibly scale up the number of learnable parameters as needed. Specifically, we randomly divide each parameter block into \(K\) partitions, and assign a learnable coefficient to each partition, naturally increasing the number of learnable parameters by \(K\)-fold. We denote these variants by aTLAS \(\times K\). We conduct experiments with these variants using \(\{1,5,10,25,35,50,100\}\%\) of the total available training data across the \(22\) datasets used in Section 5. The results are summarised in Figure 7, showing that our method consistently improves as \(K\) increases. Compared to LoRAs, particularly with limited training data,

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{Shots (\(k\))} & \multicolumn{2}{c}{Standard task vectors} & \multicolumn{4}{c}{LoRAs as task vectors} \\ \cline{2-6}  & All parameter blocks & Weight matrices & Rank=4 & Rank=16 & Rank=64 \\  & (10.7 GB) & (10.5 GB) & (3.3 GB) & (3.4 GB) & (4.1 GB) \\ \hline
1 & \(66.0\pm 0.2\) & \(66.0\pm 0.1\) & \(64.4\pm 0.1\) & \(64.6\pm 0.1\) & \(65.4\pm 0.1\) \\
2 & \(67.7\pm 0.1\) & \(67.0\pm 0.2\) & \(65.7\pm 0.0\) & \(66.6\pm 0.2\) & \(67.4\pm 0.1\) \\
4 & \(70.0\pm 0.0\) & \(69.4\pm 0.2\) & \(68.2\pm 0.0\) & \(68.7\pm 0.1\) & \(69.5\pm 0.2\) \\
8 & \(71.3\pm 0.1\) & \(70.9\pm 0.0\) & \(70.2\pm 0.2\) & \(70.4\pm 0.1\) & \(70.9\pm 0.1\) \\
16 & \(72.8\pm 0.1\) & \(72.3\pm 0.0\) & \(71.7\pm 0.1\) & \(71.8\pm 0.1\) & \(72.0\pm 0.1\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Few-shot recognition performance using standard task vectors or LoRAs as sparse task vectors. Results are averaged across 22 datasets over three seeds, with \(\times 1\) standard deviation. The memory consumption for ViT-B/32 backbone is annotated under each variant. For standard task vectors, we learn compositions on all parameter blocks or weight matrices only. For LoRAs as task vectors, we report results with rank 4, 16 and 64.

Figure 7: Scalability of aTLAS. We compare the accuracy of our method against LoRAs, and vary the amount of training data. Results are averaged over 22 datasets. Detailed results are included in Table 17.

our method achieves higher performance with fewer learnable parameters. With sufficient training data, the variant aTLAS \(\times 1200\) leads to higher performance with a similar number of learnable parameters, as it is able to exploit the knowledge contained in the task vectors that may otherwise be unobtainable from the target dataset.

## 7 Related work

Task vectors and model compositions.Recent studies have demonstrated the possibility of manipulating the behaviours of neural networks directly in the weight space [27; 62; 64]. In particular, task vectors [28], as a carrier of the domain-specific knowledge learned through fine-tuning, exhibit strong compositional properties. Such compositionality can be enhanced via linearisation using first-order Taylor expansion [44], and improves model editing with simple arithmetic, e.g., addition, negation, etc. Yang et al. [65] also investigated the idea of learning layer-wise coefficients to improve task arithmetic. In addition, low-rank adaptations [23], as special forms of task vectors, were shown to also support such arithmetic operations. A recent study [3] also investigated the idea of learning combinations of LoRAs for few-shot recognition.

Model-based transfer learning.One interpretation of transfer learning [73] is to exploit the knowledge encapsulated in a pre-trained model for a target domain. Amongst various sub-modules of a pre-trained model, transferring the feature extractor is the most extensively studied. This ranges from early convolutional neural networks [18; 53] to modern transformers [58], from vision backbones [14; 37] to language models [13; 46]. For vision applications, classification models trained on ImageNet [52] have been used as the medium for knowledge transfer. In recent years, contrastively pre-trained multi-modal models such as CLIP [47] have emerged as a prevalent choice. Such models are trained on large volumes of data by aligning image and language representations, leading to strong baselines well suited for transfer learning. CLIP representations have since been use for medical imaging [70], semantic segmentation [72], satellite imaging [40], etc.

Model adaptation in low-data regimes.The performance of pre-trained models is often constrained when applied to specific tasks with limited labelled data. To address this limitation, extensive research has been conducted on few-shot adaptation of CLIP [47]. These studies focus on various techniques, including prompt engineering [71], feature adaptation [16], and more recently classifier adaptation [25; 69]. In addition to few-shot adaptation, test-time adaptation represents an even more challenging scenario where no annotated data is available. This typically requires leveraging self-supervised objectives to adapt the model, employing methods such as entropy minimisation [35; 43; 59], contrastive learning [8], pseudo labelling [35] and image rotation prediction [57].

## 8 Conclusion

In this paper, we introduced aTLAS, a learning algorithm that leverages the rich knowledge encapsulated in task vectors through learned linear combinations with anisotropic scaling. Unlike conventional methods that learn network parameters, our approach focuses on learning coefficients on task vectors, significantly reducing the number of learnable parameters. We conducted experiments across task arithmetic, few-shot recognition, test-time adaptation and parameter-efficient fine-tuning, demonstrating the effectiveness of our method with supervised and unsupervised objectives. In particular, we highlighted several properties of aTLAS, including low disentanglement error, robustness against domain shift, effectiveness in low-data regimes, complementarity with existing few-shot methods, etc. These properties paved the way for efficient knowledge composition and transfer.

Limitations.As a task vector is defined with respect to a specific pre-trained model, knowledge composition and transfer are not yet feasible across different architectures. This may become possible with suitable projections and remains part of the future work. In addition, combining large numbers of task vectors can consume a substantial amount of GPU memory when training larger models. This can be mitigated by selecting a subset of task vectors, using LoRAs as task vectors or by offloading the computation of task vector composition to CPU, at the cost of training speed decrease. It is also possible to perform task vector composition at bit-width lower than floating point precision, e.g., 4-bit. Similar features are being tested with popular deep learning frameworks such as PyTorch, and we expect the memory requirement of larger models to be less of a constraint in the future.

Acknowledgements.This research is funded in part by the Australian Government through the Australian Research Council (Project DP240103278), and the Centre of Augmented Reasoning at the Australian Institute for Machine Learning, established by a grant from the Department of Education. We would like to thank Stephen Gould for his valuable feedback on the paper.

## References

* Aghajanyan et al. [2021] Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing_, pages 7319-7328. Association for Computational Linguistics, Aug 2021. URL https://aclanthology.org/2021.acl-long.568.
* Arazo et al. [2020] Eric Arazo, Diego Ortego, Paul Albert, Noel E. O'Connor, and Kevin McGuinness. Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning. In _International Joint Conference on Neural Networks (IJCNN)_, 2020. URL https://arxiv.org/pdf/1908.02983.
* Asadi et al. [2024] Nader Asadi, Mahdi Beitollahi, Yasser Khalil, Yinchuan Li, Guojun Zhang, and Xi Chen. Does combining parameter-efficient modules improve few-shot transfer accuracy? _arXiv preprint arXiv:2402.15414_, 2024. URL https://arxiv.org/pdf/2402.15414.
* Bommasani et al. [2021] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjoffsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kaluri, Siddharth Karamcheti, Geoff Keeling, Fersthek Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks of foundation models. _arXiv_, abs/2108.07258, 2021. URL https://arxiv.org/abs/2108.07258.
* Bossard et al. [2014] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101-mining discriminative components with random forests. In _Proceedings of European Conference on Computer Vision_, pages 446-461. Springer, 2014. URL https://link.springer.com/chapter/10.1007/978-3-319-10599-4_29.
* Bozinovski and Fulgosi [1976] Stevo Bozinovski and Ante Fulgosi. The influence of pattern similarity and transfer learning upon the training of a base perceptron b2. _Proceedings of Symposium Informatica_, 3(125), 1976.
* Carion et al. [2020] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In _Proceedings of European Conference on Computer Vision (ECCV)_, pages 213-229, Cham, 2020. Springer International Publishing. URL https://arxiv.org/pdf/2005.12872.
* Chen et al. [2022] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In _Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 295-305, Jun 2022. URL https://arxiv.org/pdf/2204.10377.
* Chen et al. [2020] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A Simple Framework for Contrastive Learning of Visual Representations. In _International Conference on Machine Learning (ICML)_, 2020. URL https://arxiv.org/pdf/2002.05709.
* Cheng et al. [2017] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. _Proceedings of IEEE_, 105(10):1865-1883, Oct 2017. URL https://arxiv.org/abs/1703.00121.
* Cimpoi et al. [2014] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In _Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3606-3613, Columbus, OH, USA, 24-27 Jun 2014. URL https://arxiv.org/abs/1311.3618.
* Coates et al. [2011] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In _Proceedings of International Conference on Artificial Intelligence and Statistics_, pages 215-223. JMLR Workshop and Conference Proceedings, 2011. URL https://proceedings.mlr.press/v15/coates1ia/coates1ia.pdf.

* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 4171-4186, Minneapolis, Minnesota, Jun 2019. URL https://aclanthology.org/N19-1423.pdf.
* Dosovitskiy et al. [2021] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.
* Everingham et al. [2015] Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. The pascal visual objective classes challenges: A retrospective. _International Journal of Computer Vision (IJCV)_, 2015. URL https://link.springer.com/article/10.1007/s11263-014-0733-5.
* Gao et al. [2021] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. CLIP-Adapter: Better vision-language models with feature adapters. _arXiv preprint arXiv:2110.04544_, 2021. URL https://arxiv.org/pdf/2404.02285.
* Griffin et al. [2007] Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007. URL https://authors.library.caltech.edu/records/5sv1j-ytw97.
* 1 Jul 2016. URL https://arxiv.org/pdf/1512.03385.
* He et al. [2017] Kaiming He, Georgia Gkioxari, Pitor Dollar, and Ross Girshick. Mask R-CNN. In _Proceedings of IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 2980-2988, Venice, Italy, 22-29 Oct 2017. URL https://arxiv.org/pdf/1703.06870.
* Helber et al. [2018] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Introducing euroSAT: A novel dataset and deep learning benchmark for land use and land cover classification. In _Proceedings of IEEE International Geoscience and Remote Sensing Symposium_, pages 204-207, Valencia, Spain, 22-27 Jul 2018. URL https://arxiv.org/abs/1709.00029.
* Hendrycks et al. [2021] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In _IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 8340-8349, 2021. URL https://arxiv.org/pdf/2006.16241.
* Hendrycks et al. [2021] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 15262-15271, 2021. URL https://arxiv.org/pdf/1907.07174.
* Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2021. URL https://arxiv.org/pdf/2106.09685.
* Huang et al. [2023] Chensong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. LoRAhub: Efficient cross-task generalization via dynamic lora composition. _arXiv preprint arXiv:2307.13269_, 2023. URL https://arxiv.org/pdf/2307.13269.
* Huang et al. [2024] Yunshi Huang, Fereshteh Shakeri, Jose Dolz, Malik Boudiaf, Houda Bahig, and Ismail Ben Ayed. LP++: A surprisingly strong linear probe for few-shot clip. In _Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2024. URL https://arxiv.org/pdf/2404.02285.
* Ilharco et al. [2021] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. OpenCLIP, Jul 2021. URL https://doi.org/10.5281/zenodo.5143773.
* Ilharco et al. [2022] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 35, pages 29262-29277. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/bc6cddc5d325e1c0f826066c1ad0215-Paper-Conference.pdf.
* Liu et al. [2021]* [28] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In _Proceedings of International Conference on Learning Representations (ICLR)_, Kjigali, Rwanda, 1-5 May 2023. OpenReview.net. URL https://openreview.net/pdf?id=6t0Xwf8-jrj.
* [29] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In _Proceedings of International Conference on Machine Learning (ICML)_, volume 97 of _Proceedings of Machine Learning Research_, pages 3519-3529. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/kornblith19a.html.
* [30] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3D object representations for fine-grained categorization. In _IEEE/CVF International Conference on Computer Vision (ICCV) Workshop on 3D Representation and Recognition_, pages 554-561, Sydney, Australia, 1-8 Dec 2013. URL http://vision.stanford.edu/pdf/3drr13.pdf.
* [31] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [32] Yann LeCun, Corinna Cortez, and Christopher C. J. Burges. The mnist handwritten digit database, 1998. URL http://yann.lecun.com/exdb/mnist/.
* [33] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the instrinsic dimension of objective landscapes. In _Proceedings of International Conference on Learning Representations (ICLR)_, Vancouver, Canada, 30 Apr-3 May 2018. URL https://openreview.net/pdf?id=ryup8-WCW.
* [34] Fei-Fei Li, Robert Fergus, and Pietro Perona. One-shot learning of object categories. _IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)_, 28(4):594-611, 2006. URL http://vision.stanford.edu/documents/Fei-FeiFergusPerona2006.pdf.
* [35] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation. In _Proceedings of International Conference on Machine Learning (ICML)_, volume 119 of _Proceedings of Machine Learning Research_, pages 6028-6039. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/liang20a.html.
* [36] Baijiong Lin. LoRA-Torch: PyTorch reimplementation of LoRA, 2023. URL https://github.com/Baijiong-Lin/LoRA-Torch.
* [37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of IEEE/CVF International Conference on Computer Vision (ICCV)_, 11-17 Oct 2021.
* [38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _Proceedings of International Conference on Learning Representations (ICLR)_, New Orleans, LA, USA, 6-9 may 2019. OpenReview.net. URL https://openreview.net/forum?id=Bkg6RiCqY7.
* [39] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. _arXiv preprint arXiv:1306.5151_, 2013.
* [40] Sangwoo Mo, Minkyu Kim, Kyungmin Lee, and Jinwoo Shin. S-clip: Semi-supervised vision-language learning using few specialist captions. _Advances in Neural Information Processing Systems_, 36, 2024.
* [41] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In _Neural Information Processing Systems (NeurIPS) Workshop on Deep Learning and Unsupervised Feature Learning_, Granada, Spain, 12-17 Dec 2011. URL http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf.
* [42] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _Indian conference on computer vision, graphics & image processing_, pages 722-729. IEEE, 2008.
* [43] Shuaiecheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In _International Conference on Learning Representations (ICLR)_, 2023.
* [44] Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. Task arithmetic in the tangent space: Improved editing of pre-trained models. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 36, pages 66727-66754, New Orleans, LA, USA, 10-16 Dec 2023. Curran Associates, Inc.
** [45] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3498-3505. IEEE, 2012.
* [46] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. _OpenAI blog_, 2019.
* [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _Proceedings of International Conference on Machine Learning (ICML)_, volume 139, pages 8748-8763. Proceedings of Machine Learning Research (PMLR), 18-24 Jul 2021.
* [48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020.
* A gradient-free optimization platform. https://GitHub.com/FacebookResearch/Nevergrad, 2018.
* [50] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In _International Conference on Machine Learning (ICML)_, pages 5389-5400. PMLR, 2019.
* [51] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, _Advances in Neural Information Processing Systems (NeurIPS)_, volume 28, pages 91-99, Montreal, Canada, 7-12 Dec 2015. Curran Associates, Inc.
* [52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Fei-Fei Li. Imagenet large scale visual recognition challenge. _International Journal of Computer Vision (IJCV)_, 115(3):211-252, 2015.
* [53] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In _Proceedings of International Conference on Learning Representations (ICLR)_, San Diego, CA, USA, 7-9 May 2015. OpenReview.net.
* [54] K. Sohn, D. Berthelot, C.-L. L, Z. Zhang, N. Carlini, E. Cubuk, A Kurakin, H. Zhang, and C. Raffel. FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence. _arXiv: 2001.07685_, 2020.
* [55] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. _arXiv preprint arXiv:1212.0402_, 2012.
* [56] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: A multi-class classification competition. In _Proceedings of International Joint Conference on Neural Networks (IJCNN)_, pages 1453-1460, San Jose, CA, USA, 31 Jul-5 Aug 2011. URL https://ieeexplore.ieee.org/document/6033395.
* [57] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In _Proceedings of International Conference on Machine Learning (ICML)_, volume 119 of _Proceedings of Machine Learning Research_, pages 9229-9248. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/sun20b.html.
* [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 30, pages 6000-6010. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
* [59] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2021. URL https://openreview.net/forum?id=uxl3b2Lkr3c.
* [60] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 10506-10518, 2019.

* Welinder et al. [2010] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.
* Wortsman et al. [2022] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _Proceedings of International Conference on Machine Learning (ICML)_, volume 162, pages 23965-23998, 23-29 Jul 2022.
* Xiao et al. [2016] Jianxiong Xiao, Krista A. Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring a large collection of scene categories. _International Journal of Computer Vision (IJCV)_, 119(1):3-22, 2016. ISSN 1573-1405. URL https://doi.org/10.1007/s11263-014-0748-y.
* Yadav et al. [2023] Prateek Yadav, Derek Tam, Leshem Choshen, Colin A Raffel, and Mohit Bansal. TIES-Merging: Resolving interference when merging models. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 36, pages 7093-7115. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/1644c9af28ab7916874f5fd6228a9bcf-Paper-Conference.pdf.
* Yang et al. [2024] Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, and Dacheng Tao. Adamerging: Adaptive model merging for multi-task learning. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2024.
* Zhang et al. [2021] Frederic Z. Zhang, Dylan Campbell, and Stephen Gould. Spatially conditioned graphs for detecting human-object interactions. In _Proceedings of IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 13319-13327, Oct 2021. URL https://arxiv.org/pdf/2012.06060.
* Zhang et al. [2022] Frederic Z. Zhang, Dylan Campbell, and Stephen Gould. Efficient two-stage detection of human-object interactions with a novel unary-pairwise transformer. In _Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 20104-20112, New Orleans, LA, USA, Jun 2022. URL https://arxiv.org/pdf/2112.01838.
* Zhang et al. [2023] Frederic Z. Zhang, Yuhui Yuan, Dylan Campbell, Zhuoyao Zhong, and Stephen Gould. Exploring predicate visual context in detecting of human-object interactions. In _Proceedings of IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 10411-10421, Oct 2023.
* Zhang et al. [2022] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-Adapter: Training-free adaption of CLIP for few-shot classification. In _Proceedings of European Conference on Computer Vision (ECCV)_, pages 493-510, Tel Aviv, Israel, 23-27 Oct 2022. Springer Nature Switzerland. ISBN 978-3-031-19833-5.
* Zhao et al. [2023] Zihao Zhao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Xiang Li, Zhiming Cui, Qian Wang, et al. Clip in medical imaging: A comprehensive survey. _arXiv preprint arXiv:2312.07353_, 2023.
* Zhou et al. [2022] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. _International Journal of Computer Vision (IJCV)_, 130(9):2337-2348, Sep 2022. ISSN 0920-5691. URL https://doi.org/10.1007/s11263-022-01653-1.
* Zhou et al. [2023] Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and Yifan Liu. Zegeclip: Towards adapting clip for zero-shot semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11175-11185, 2023.
* Zhuang et al. [2021] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. _Proceedings of the IEEE_, 109(1):43-76, 2021. ISSN 0018-9219.

Datasets and task vectors

We acquire task vectors by fine-tuning CLIP [47] on a variety of \(22\) image recognition datasets: (1) Stanford Cars [30], (2) DTD [11], (3) EuroSAT [20], (4) GTSRB [56], (5) MNIST [32], (6) RESISC45 [10], (7) SUN397 [63], (8) SVHN [41], (9) CIFAR10 [31], (10) CIFAR100 [31], (11) ImageNet [52], (12) STL10 [12], (13) Food101 [5], (14) Caltech101 [34], (15) Caltech256 [17], (16) FGVC Aircraft [39], (17) Flowers102 [42], (18) Oxford Pets [45], (19) CUB200 [61], (20) PascalVOC [15], (21) Country211 [47], and (22) UCF101 [55]. Fine-tuning was conducted using AdamW optimiser [38], with a learning rate of \(10^{-5}\), batch size of 128 and weight decay of 0.1. Details of the datasets, additional dataset-specific hyper-parameters, and the accuracy after fine-tuning for an assortment of backbones are shown in Table 5. We use the same hyper-parameters for the linearised variants of the model.

To shed light on the semantic relationships amongst datasets, we extract the features of all images for each dataset, and visualise the distributions as ellipses (Figure 8). Specifically, for each dataset, the mean \(\bm{\mu}_{t}\in\mathbb{R}^{d}\) and covariance \(\Sigma_{t}\in\mathbb{R}^{d\times d}\) of image features are computed. Principal component analysis (PCA) is used produce a projection matrix \(P\in\mathbb{R}^{d\times 2}\) from the mean features \(\bm{\mu}_{t}\). Subsequently, the mean and covariance with reduced dimensionality can be expressed as \(P^{\mathsf{T}}\bm{\mu}_{t}\) and \(P^{\mathsf{T}}\Sigma_{t}P\), respectively.

\begin{table}
\begin{tabular}{l l l l l l l l l l l l} \hline \hline \multirow{2}{*}{\#} & \multirow{2}{*}{Datasets} & \multirow{2}{*}{Classes} & \multicolumn{2}{c}{Spilts} & \multicolumn{3}{c}{Epochs} & \multicolumn{3}{c}{Fine-tuned accuracy} \\ \cline{5-11}  & & & & _train_ & _val_ & _test_ & & RN50 & RN101 & VI-B/32 & VI-T/B/16 & VI-T/14 \\ \hline (1) & Cars & 196 & 7.330 & 814 & 8.041 & 35 & 61.92 & 68.41 & 78.26 & 84.14 & 91.67 \\ (2) & DTD & 47 & 3.384 & 376 & 1.880 & 76 & 73.14 & 72.50 & 78.94 & 81.91 & 84.73 \\ (3) & EuroSAT & 10 & 21.600 & 2.700 & 2.700 & 12 & 98.11 & 90.87 & 98.89 & 98.93 & 99.81 \\ (4) & GTSRB & 43 & 23,976 & 2,664 & 12,630 & 11 & 97.33 & 97.51 & 99.14 & 98.84 & 99.30 \\ (5) & MNIST & 10 & 55,000 & 5,000 & 10,000 & 5 & 99.62 & 94.95 & 99.65 & 99.69 & 99.77 \\ (6) & RESISC45 & 45 & 17,010 & 1,890 & 6,300 & 15 & 93.16 & 92.37 & 95.94 & 96.59 & 97.14 \\ (7) & SUN397 & 397 & 17,865 & 1,985 & 19,850 & 14 & 69.65 & 72.26 & 75.40 & 78.12 & 81.98 \\ (8) & SVHN & 10 & 68,257 & 5,000 & 26,032 & 4 & 94.30 & 94.58 & 97.38 & 97.70 & 97.97 \\ (9) & CIFAR10 & 10 & 45,000 & 5,000 & 10,000 & 5 & 93.55 & 94.38 & 90.85 & 98.54 & 99.22 \\ (10) & CIFAR100 & 100 & 45,000 & 5,000 & 10,000 & 6 & 77.55 & 80.15 & 89.09 & 89.95 & 93.01 \\ (11) & ImageNet & 1,000 & 1,276,167 & 5,000 & 5,000 & 10 & 76.01 & 78.19 & 76.41 & 81.33 & 85.52 \\ (12) & STL10 & 10 & 4,500 & 500 & 8,000 & 4 & 90.15 & 95.58 & 98.55 & 92.00 & 99.62 \\ (13) & Food101 & 101 & 70,750 & 5,000 & 25,250 & 15 & 85.14 & 87.22 & 88.68 & 92.85 & 95.37 \\ (14) & Caltech101 & 101 & 6,941 & 694 & 1,736 & 10 & 87.62 & 85.89 & 94.41 & 95.22 & 94.82 \\ (15) & Caltech256 & 257 & 22,037 & 2,448 & 6,122 & 8 & 88.29 & 90.54 & 92.60 & 94.58 & 97.17 \\ (16) & FGVC Aircraft & 100 & 3,334 & 3,333 & 3,333 & 60 & 23.88 & 26.91 & 40.65 & 47.28 & 68.11 \\ (17) & Flowers102 & 102 & 1,020 & 1,020 & 6,149 & 40 & 60.79 & 55.47 & 90.08 & 94.67 & 97.84 \\ (18) & OxfordMITPets & 37 & 3,312 & 368 & 3,669 & 5 & 75.14 & 77.49 & 92.15 & 93.59 & 95.91 \\ (19) & CUB200 & 200 & 5,395 & 599 & 5,794 & 20 & 58.11 & 59.56 & 73.56 & 77.37 & 86.35 \\ (20) & PascalVOC & 20 & 7,844 & 7,818 & 14,976 & 10 & 74.88 & 76.87 & 88.42 & 90.35 & 92.05 \\ (21) & Country211 & 211 & 31,650 & 10,550 & 21,100 & 15 & 19.24 & 20.60 & 21.99 & 27.64Figure 8: visualisation of dataset image feature distributions as ellipses. The mean image features for all datasets are visualised as the ellipse center, with the dimensionality reduced to \(2\) using Principal Component Analysis (PCA). The dimensionality of covariance matrices are also reduced using the same principal components. We show visualisations with (a) \(\times 1\) and (b) \(\times 3\) standard deviations. Pre-trained CLIP [47] with ViT-B/32 is used to extract image features.

[MISSING_PAGE_EMPTY:18]

Figure 9: visualisation of the learned coefficients for (a) standard and (b) linear task vectors in task negation. Note that coefficients for different datasets are learned independently, despite being visualised jointly. Large negative coefficients can be observed on weight matrices. CLIP with ViT-B/32 backbone is used.

Figure 10: Additional box-and-whisker plots for the learned coefficients in task negation, beside previous visualisation on Cars (Figure 2(a)), including results on (a, b) DTD, (c, d) EuroSAT, (e, f) GTSRB, (g, h) MNIST, (i, j) RESISC45, (k, l) SUN397 and (m, n) SVHN.

[MISSING_PAGE_FAIL:21]

coefficients. Given two datasets \(\mathcal{D}_{1},\mathcal{D}_{2}\) and the respective task vectors \(\boldsymbol{\tau}_{1},\boldsymbol{\tau}_{2}\), we overload the definition of function \(f\) to denote the mapping from data space \(\mathcal{X}\) to the label space \(\mathcal{Y}\), and define the disentanglement error as

\[\xi(\boldsymbol{\tau}_{1},\boldsymbol{\tau}_{2}) =\mathbf{E}_{\mathbf{x}\in\mathcal{D}_{1}}\big{[}\delta\big{(}f( \mathbf{x};\boldsymbol{\theta}_{0}+\Lambda_{1}^{\star}\boldsymbol{\tau}_{1}),f(\mathbf{x};\boldsymbol{\theta}_{0}+\Lambda_{1}^{\star}\boldsymbol{\tau}_{1} +\Lambda_{2}^{\star}\boldsymbol{\tau}_{2})\big{)}\big{]},\] (10) \[\xi(\boldsymbol{\tau}_{2},\boldsymbol{\tau}_{1}) =\mathbf{E}_{\mathbf{x}\in\mathcal{D}_{2}}\big{[}\delta\big{(}f( \mathbf{x};\boldsymbol{\theta}_{0}+\Lambda_{2}^{\star}\boldsymbol{\tau}_{2}),f(\mathbf{x};\boldsymbol{\theta}_{0}+\Lambda_{1}^{\star}\boldsymbol{\tau}_{1} +\Lambda_{2}^{\star}\boldsymbol{\tau}_{2})\big{)}\big{]},\] (11)

where \(\Lambda_{1}^{\star},\Lambda_{2}^{\star}\) are the learned coefficients in task addition, and \(\delta\) is defined as

\[\delta(x_{1},x_{2})=\begin{cases}0&x_{1}=x_{2},\\ 1&x_{1}\neq x_{2}.\end{cases}\] (12)

The error metric \(\xi(\boldsymbol{\tau}_{1},\boldsymbol{\tau}_{2})\) measures the percentage of data in dataset \(\mathcal{D}_{1}\), such that when a second task vector \(\boldsymbol{\tau}_{2}\) is added to the model, the predicted labels differ from when only using task vector \(\boldsymbol{\tau}_{1}\). As task vector \(\boldsymbol{\tau}_{1}\) is acquired from dataset \(\mathcal{D}_{1}\), a low disentanglement error indicates that most predictions made by \(\boldsymbol{\tau}_{1}\)--highly likely to be correct--will be retained, thus resulting in higher performance in task addition.

Figure 12: visualisation of the learned coefficients for (a) standard and (b) linear task vectors in task addition. Note that \(L_{1}\) regularisation has been applied to the coefficients during training for better clarity. CLIP with ViT-B/32 backbone is used to produce the results.

## Appendix D Few-shot learning

### Baselines: Tip-Adapter and LP++

Two variants of Tip-Adapter [69] were proposed for few-shot recognition where the weights of the adaptor are either fixed based on features of the few-shot examples or further fine-tuned. We only study the fine-tuned variant due to its higher performance. Tip-Adapter has two hyper-parameters, which in the original paper are optimised through hyper-parameter search on a separate validation set. This practice may not align with the principles of few-shot learning, where access to extensive validation data is typically limited. In addition, Huang et al. [25] note that the performance of Tip-Adapter is very sensitive to these hyper-parameters. We thus opt to learn these two hyper-parameters together with the feature adaptor through gradient descent. The learning rates for the feature adaptor and the hyper-parameters are set to \(10^{-3}\) and \(10^{-1}\), respectively.

For both Tip-Adapter and LP++ [25], we conduct experiments using the publicly available codebase 5. We train both LP++ and Tip-Adapter for 300 epochs on frozen zero-shot features. We apply a cosine annealing decay for Tip-Adapter and maintain fixed learning rates for LP++ as per the official implementation.

Footnote 5: github.com/freeshteshakeri/fewshot-clip-strong-baseline

### linearised task vectors

We report the average few-shot accuracy over the 22 datasets in Table 9, which corresponds to results in Figure 4(a). In particular, we show results with linearised task vectors, as proposed by Ortiz-Jimenez et al. [44]. As highlighted in Section 4, learned anisotropic scaling allows standard task vectors to achieve stronger performance than the linear variants in task addition. For few-shot recognition, we again observe that standard task vectors result in superior performance in most cases. We, however, note the exception that linear task vectors when combined with LP++ achieve higher performance in the 1-shot setting. Nevertheless, the margin over standard task vectors is not very significant, and aTLAS using standard task vectors when integrated with Tip-Adapter is generally a stronger few-shot model.

### Integrating state-of-the-art methods into aTLAS

We use the AdamW [38] optimiser with a learning rate of \(10^{-1}\) and a weight decay of \(10^{-1}\). Our method by itself is trained for \(10\) epochs with ViT backbones and \(30\) epochs with ResNet backbones.

We show that state-of-the-art few-shot methods can be seamlessly integrated into our method, since both Tip-Adapter and LP++ focus on the classifier, while aTLAS improves the feature representations. We experiment with two strategies to combine aTLAS with previous methods, where we either (1) train our method first and use the frozen representations to train a previous method, or (2) train parameters in both methods jointly. Results in Table 10 shows that the joint training strategy results in higher performance, particularly in low-shot settings. We therefore adopt the joint training strategy when combing our method with Tip-Adapter. During training, we adopt different learning rates for different parameter groups, that is, \(10^{-1}\) for learnable coefficients in aTLAS and the hyper-parameters in Tip-Adapter, and \(10^{-3}\) for the adaptor. The joint training takes 20 epochs for ViT backbones and 60 epochs on ResNet backbones, twice the number of epochs when training aTLAS alone.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Shots (\(k\)) & Tip-Adapter & LP++ & \multicolumn{2}{c}{aTLAS} & \multicolumn{2}{c}{aTLAS w/ LP++} & \multicolumn{2}{c}{aTLAS w/ Tip-Adapter} \\ \cline{3-10}  & & & Std. & Lin. & Std. & Lin. & Std. & Lin. \\ \hline
1 & \(64.3\pm 0.2\) & \(64.5\pm 0.1\) & \(66.2\pm 0.2\) & \(64.6\pm 0.1\) & \(68.9\pm 0.2\) & \(\mathbf{69.3\pm 0.1}\) & \(68.7\pm 0.4\) & \(66.7\pm 0.3\) \\
2 & \(67.0\pm 0.1\) & \(67.3\pm 0.1\) & \(67.6\pm 0.1\) & \(65.4\pm 0.1\) & \(71.5\pm 0.1\) & \(67.2\pm 0.2\) & \(\mathbf{71.9\pm 0.2}\) & \(68.9\pm 0.1\) \\
4 & \(69.7\pm 0.1\) & \(69.9\pm 0.1\) & \(70.0\pm 0.0\) & \(66.6\pm 0.1\) & \(73.7\pm 0.1\) & \(70.8\pm 0.1\) & \(\mathbf{74.3\pm 0.1}\) & \(71.6\pm 0.2\) \\
8 & \(71.8\pm 0.1\) & \(72.3\pm 0.1\) & \(71.5\pm 0.0\) & \(68.2\pm 0.1\) & \(75.8\pm 0.1\) & \(73.5\pm 0.1\) & \(\mathbf{76.5\pm 0.1}\) & \(74.2\pm 0.1\) \\
16 & \(73.7\pm 0.1\) & \(74.1\pm 0.1\) & \(72.9\pm 0.1\) & \(69.8\pm 0.1\) & \(77.8\pm 0.0\) & \(76.2\pm 0.1\) & \(\mathbf{78.0\pm 0.1}\) & \(76.7\pm 0.0\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Average accuracy for few-shot recognition over 22 datasets. We report accuracy averaged over 3 random n-shot sample selections, with \(1\times\) standard error. Results are produced using CLIP with ViT-B/32 backbone. For our method, we show results with both standard [28] and linearised [44] task vectors. The best method for each choice of \(k\in\{1,2,4,8,16\}\) is highlighted in bold.

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_EMPTY:25]

### Out-of-domain generalisation

We show detailed results for out-of-domain generalisation over \(k\in\{4,16\}\) shots in Table 13. These results correspond to those presented in Figure 4(c). aTLAS is the only method that consistently improves test accuracy over the zero-shot model on out-of-domain images. When combined with LP++ or Tip-Adapter, aTLAS can be observed to improve the out-of-domain generalisation of these methods.

### Relative significance of individual task vectors

In this section, we examine the informativeness of a task vector across different target datasets. To this end, we apply aTLAS to each of the 22 datasets using only one task vector. For each dataset, we compute the relative accuracy improvement, that is, the accuracy improvement of aTLAS normalised by that of fine-tuning in the full parameter space. Note that aTLAS is applied under the 16-shot setting, while standard fine-tuning uses all training data available. Results are shown in Figure 13. We first note that certain datasets are more prone to accuracy improvement, such as EuroSAT, MNIST, etc., as indicated by the high percentage across entire rows. This is most likely due to the low intrinsic dimensionality of the task. In addition, we highlight the average improvement in the last row. Notably, certain task vectors, e.g., ImageNet task vector, are particularly informative while others, such as those from Flowers102 and OxfordPets are much less so. These results illustrate the varying contributions different task vectors can have depending on the target dataset, which also motivated subsequent efforts on careful task vector selection.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c}{\(k=4\)} & \multicolumn{3}{c}{\(k=16\)} & \multicolumn{3}{c}{\(k=16\)} \\ \cline{2-13}  & Net & Net-A & Net-R & Net-Stech & PusV2 & Net & Net-A & Net-R & Net-Stech & PusV2 \\ \hline Zero-shot & 63.4 & **31.5** & **42.3** & 69.2 & 62.7 & 63.4 & 31.5 & 42.3 & 69.2 & 62.7 \\ aTLAS & 64.7 \(\pm\) 0.0 & 30.8 \(\pm\) 0.0 & 42.3 \(\pm\) 0.0 & **69.9** & 61.4 \(\pm\) 0.0 & 66.5 \(\pm\) 0.0 & 3.7 \(\pm\) 0.1 & **42.9** & 0.74 \(\pm\) 0.0 & 64.7 \(\pm\) 0.0 \\ Tip-Adapter & 64.9 \(\pm\) 0.1 & 30.8 \(\pm\) 0.1 & 41.3 \(\pm\) 0.2 & 68.5 \(\pm\) 0.1 & 63.4 \(\pm\) 0.6 & 66.1 \(\pm\) 0.0 & 29.1 \(\pm\) 0.2 & 40.5 \(\pm\) 0.0 & 67.7 \(\pm\) 0.1 & 63.7 \(\pm\) 0.1 \\ lP++ & 65.7 \(\pm\) 0.0 & 30.1 \(\pm\) 0.2 & 41.0 \(\pm\) 0.2 & 67.8 \(\pm\) 0.1 & 64.4 \(\pm\) 0.0 & 68.0 \(\pm\) 0.0 & 29.2 \(\pm\) 0.1 & 41.0 \(\pm\) 0.0 & 67.0 \(\pm\) 0.1 & 66.0 \(\pm\) 0.0 \\ aTLAS w/ Tip+ & **66.0** \(\pm\) 0.0 & 30.2 \(\pm\) 0.1 & 41.6 \(\pm\) 0.1 & 69.2 \(\pm\) 0.2 & 64.5 \(\pm\) 0.0 & 68.0 \(\pm\) 0.0 & 29.4 \(\pm\) 0.3 & 41.4 \(\pm\) 0.1 & 68.9 \(\pm\) 0.2 & 65.4 \(\pm\) 0.1 \\ aTLAS w/ LP++ & **66.0**\(\pm\) 0.0 & 29.1 \(\pm\) 0.2 & 41.2 \(\pm\) 0.2 & 67.9 \(\pm\) 0.4 & **64.8**\(\pm\) 0.0 & **68.9**\(\pm\) 0.0 & 28.7 \(\pm\) 0.1 & 41.9 \(\pm\) 0.0 & 67.5 \(\pm\) 0.1 & **67.0**\(\pm\) 0.0 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Accuracy of few-shot methods trained on ImageNet [52] and tested on out-of-domain datasets, for \(k\in\{4,16\}\). Results are produced by CLIP with ViT-B/32 backbone and averaged across three random seeds.

Figure 13: Accuracy improvement of aTLAS (16-shot) using one task vector normalised by that of fine-tuning in the full parameter space (all training data). Each column corresponds to a unique task vector, and reflects the relative improvement it leads to on different target datasets. Each row reflects the relative improvement on a dataset, using different task vectors.

### Task vector budget and selection

In this section, we provide details for selecting a budget of \(b\) task vectors with feature-based and gradient-based strategies, as introduced in Section 5.2.

**Feature based selection**. For each dataset \(\mathcal{D}_{i}\), we compute the average image representation \(\bar{\mathbf{z}}_{i}\) of the dataset using the zero-shot model as follows

\[\bar{\mathbf{z}}_{i}=\mathbf{E}_{\mathbf{x}\in\mathcal{D}_{i}}[f(\mathbf{x}; \boldsymbol{\theta}_{0})].\] (13)

Given a target dataset \(\mathcal{D}_{t}\), we simply compute the cosine similarity between its feature representation \(\bar{\mathbf{z}}_{t}\) and that of each other dataset \(\bar{\mathbf{z}}_{i},i\neq t\). Subsequently, \(b\) task vectors corresponding to the datasets with highest similarity will be selected.

**Gradient-based selection**.

Given a target dataset \(\mathcal{D}_{t}\), we may directly compute the gradient with respect to the \(m\) learnable coefficients for each of the \(n\) task vectors. However, as one important motivation behind task vector selection is to reduce memory consumption, using all \(n\) task vectors to compute the gradient defeats the purpose. Therefore, we instead only load a group of \(b\) task vectors (\(b<n\)), compute the gradient with respect to their learnable coefficients, and repeat for other groups. With this sequential computation, the gradient across different groups is not calibrated. Nevertheless, we empirically found this strategy to work well. Denote the partial derivative of the loss on dataset \(\mathcal{D}_{t}\) with respective to a learnable coefficient \(\lambda_{i}^{(j)}\) by \(\dot{\lambda}_{i}^{(j)}\), such that

\[\dot{\lambda}_{i}^{(j)}=\mathbf{E}_{(\mathbf{x},\mathbf{y})\in\mathcal{D}_{t }}\Bigg{[}\frac{\partial\mathcal{L}\Big{(}f\Big{(}\mathbf{x};\boldsymbol{ \theta}_{0}+\sum_{i=1}^{b}\Lambda_{i}\boldsymbol{\tau}_{i}\Big{)},\mathbf{y} \Big{)}}{\partial\lambda_{i}^{(j)}}\Bigg{]}.\] (14)

For the \(i\)-th task vector, we may compute its \(L_{1}\) gradient norm, i.e., \(\left\lVert\dot{\lambda}_{i}^{(1)},\ldots,\dot{\lambda}_{i}^{(m)}\right\rVert_{1}\), and select task vectors with larger gradient. Alternatively, we may select task vectors block by block. Specifically, for the \(j\)-th parameter block, we inspect the absolute values of the partial derivatives for the corresponding coefficients, i.e., \(\left\lvert\dot{\lambda}_{i}^{(j)}\right\rvert\), and select task vectors with higher absolute values. This process is repeated for each parameter block, thus allowing different parameter blocks to have different selections. Crucially, for low budgets, particularly \(b=1\), this enables our method to effectively exploit more task vectors than the budget specifies. The impact of this can be observed in Table 14 (corresponding to Figure 6), that blockwise selection significantly outperforms other methods when the budget is low.

### LoRAs as task vectors

We fine-tune LoRAs for ViT-B/32 using the LoRA-Torch [36] library with ranks 4, 16 and 64. We stop at rank 64 as we do not observe improvements beyond it. We train LoRAs on attention and MLP layers and use the same settings as for full finetuning but with a learning rate of \(10^{-3}\).

Table 15 shows additional results using LoRAs as task vectors. We study learning the effect of fine-tuning the LoRAs task vectors on attention layers only (as done in the original LoRA paper [23]) or on the MLPs. Although the original LoRA paper recommendeds training on the attention layers only [23], we observe that training on MLP layers is important to produce strong LoRA task vectors.

\begin{table}
\begin{tabular}{c l l c c c c c c} \hline \hline \multicolumn{2}{l}{Shots \((k)\)} & Strategy & \(b=0\) & \(b=1\) & \(b=2\) & \(b=5\) & \(b=10\) & \(b=15\) & \(b=21\) \\ \hline \multirow{4}{*}{4} & Random & \(60.39\) & \(63.5\pm 0.0\) & \(64.8\pm 0.1\) & \(67.1\pm 0.2\) & \(69.3\pm 0.1\) & \(69.7\pm 0.1\) & \(70.0\pm 0.0\) \\  & Features & \(60.39\) & \(65.6\pm 0.1\) & \(67.6\pm 0.2\) & \(68.8\pm 0.1\) & \(69.5\pm 0.1\) & \(69.8\pm 0.1\) & \(70.0\pm 0.0\) \\  & Grad. whole & \(60.39\) & \(64.4\pm 0.1\) & \(65.8\pm 0.2\) & \(67.2\pm 0.1\) & \(69.1\pm 0.1\) & \(69.6\pm 0.1\) & \(70.0\pm 0.0\) \\  & Grad. blockwise & \(60.39\) & \(\mathbf{67.3}\pm 0.2\) & \(\mathbf{68.2}\pm 0.0\) & \(\mathbf{68.9}\pm 0.1\) & \(69.1\pm 0.2\) & \(\mathbf{69.7}\pm 0.2\) & \(70.0\pm 0.0\) \\ \hline \multirow{4}{*}{16} & Random & \(60.39\) & \(64.7\pm 0.1\) & \(66.0\pm 0.1\) & \(68.8\pm 0.1\) & \(70.8\pm 0.0\) & \(72.0\pm 0.1\) & \(72.8\pm 0.1\) \\  & Features & \(60.39\) & \(66.2\pm 0.0\) & \(68.1\pm 0.2\) & \(70.3\pm 0.0\) & \(\mathbf{71.7}\pm 0.1\) & \(\mathbf{72.4}\pm 0.1\) & \(72.8\pm 0.1\) \\ \cline{1-1}  & Grad. whole & \(60.39\) & \(65.2\pm 0.1\) & \(66.2\pm 0.1\) & \(68.3\pm 0.2\) & \(71.5\pm 0.1\) & \(72.2\pm 0.1\) & \(72.8\pm 0.1\) \\ \cline{1-1}  & Grad. blockwise & \(60.39\) & \(\mathbf{68.3}\pm 0.1\) & \(\mathbf{69.3}\pm 0.1\) & \(\mathbf{70.5}\pm 0.1\) & \(71.6\pm 0.0\) & \(72.3\pm 0.0\) & \(72.8\pm 0.1\) \\ \hline \hline \end{tabular}
\end{table}
Table 14: Few-shot accuracy when only using a budget of \(b\) task vectors with different selection strategies. We report results for 4 and 16 shots. The results are averaged over 22 datasets and three random seeds. CLIP with ViT-B/32 backbone is used. Highest performance in each section is highlighted in bold.

### Gradient-free optimisation

An alternative to save memory during training is to utilise gradient-free methods to learn the coefficients. We follow previous work on the combination of LoRAs [24] and use the nevergrad [49] library. We observe a memory usage reduction of 60% from 10GB to 4GB calculated using a dedicated pytorch function6. Results for few-shot recognition are summarised in Table 16. We show that although gradient-free optimisation improves upon the zero-shot model, the performance quickly plateaus as the amount of data increases. In addition, learning anisotropic scaling results in worse performance, most likely due to the relatively high number of parameters.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Scaling & Use gradient & Memory (GB) & 0-shot & 1-shot & 2-shot & 4-shot & 8-shot & 16-shot \\ \hline Anisotropic & Yes & 10 & \(60.4\) & \(66.7\pm 0.23\) & \(68.3\pm 0.28\) & \(70.0\pm 0.01\) & \(71.7\pm 0.11\) & \(72.8\pm 0.08\) \\ \hline Isotropic & No & 4 & \(60.4\) & \(\mathbf{63.1}\pm 0.45\) & \(\mathbf{64.2}\pm 0.35\) & \(\mathbf{65.0}\pm 0.12\) & \(\mathbf{65.7}\pm 0.05\) & \(\mathbf{65.4}\pm 0.14\) \\ Anisotropic & No & 4 & \(60.4\) & \(61.3\pm 0.08\) & \(61.5\pm 0.04\) & \(61.5\pm 0.04\) & \(61.6\pm 0.03\) & \(61.6\pm 0.02\) \\ \hline \hline \end{tabular}
\end{table}
Table 16: Few-shot recognition performance with gradient-free optimisation. Results are averaged accuracy over 22 datasets, with \(1\times\) standard error over 3 random seeds.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline Task vector type & Method & 0-shot & 1-shot & 2-shot & 4-shot & 8-shot & 16-shot \\ \hline LoRAs (Att.) & aTLAS & \(60.4\) & \(63.5\pm 0.1\) & \(66.5\pm 0.1\) & \(66.6\pm 0.1\) & \(67.9\pm 0.1\) & \(69.5\pm 0.0\) \\ LoRAs (MLP) & aTLAS & \(60.4\) & \(63.8\pm 0.1\) & \(66.2\pm 0.1\) & \(68.3\pm 0.1\) & \(\mathbf{70.5}\pm 0.1\) & \(\mathbf{71.4}\pm 0.0\) \\ LoRAs (Att. \& MLP) & aTLAS & \(60.4\) & \(\mathbf{64.6}\pm 0.1\) & \(\mathbf{66.6}\pm 0.2\) & \(\mathbf{68.7}\pm 0.1\) & \(70.4\pm 0.1\) & \(\mathbf{71.8}\pm 0.1\) \\ \hline LoRAs (Att. \& MLP) & aTLAS w/ LP++ & \(60.4\) & \(67.1\pm 0.3\) & \(70.9\pm 0.1\) & \(73.4\pm 0.1\) & \(75.9\pm 0.1\) & \(78.2\pm 0.1\) \\ LoRAs (Att. \& MLP) & aTLAS w/ Tip-Adapter & \(60.4\) & \(67.5\pm 0.1\) & \(70.0\pm 0.1\) & \(72.4\pm 0.1\) & \(74.9\pm 0.1\) & \(77.0\pm 0.1\) \\ Standard & aTLAS w/ LP++ & \(60.4\) & \(\mathbf{68.9}\pm 0.2\) & \(\mathbf{71.7}\pm 0.1\) & \(74.1\pm 0.1\) & \(75.8\pm 0.1\) & \(77.9\pm 0.0\) \\ Standard & aTLAS w/ Tip-Adapter & \(60.4\) & \(68.6\pm 0.4\) & \(71.6\pm 0.2\) & \(\mathbf{74.3}\pm 0.1\) & \(\mathbf{76.4}\pm 0.1\) & \(\mathbf{78.2}\pm 0.0\) \\ \hline \hline \end{tabular}
\end{table}
Table 15: Additional few-shot recognition results using LoRAs trained on attention layers, MLP layers or both. Results are averaged across 22 datasets over three seeds, with \(\times 1\) standard deviation. Rank 16 is used for LoRAs.

## Appendix E Unsupervised FixMatch

We provide more details on the Unsupervised FixMatch (UFM) approach in this section. FixMatch [54] utilises a labelled set to guide training, which is given as part of the semi-supervised learning protocol, while we produce a class-balanced "labelled" set from unlabelled images. Given a target dataset \(\mathcal{D}_{t}\) consisting of \(N\) unlabelled images, we first rank the examples by the prediction scores from the zero-shot model across \(C\) classes. We then select the top \(\min(N/C,100)\) examples, that is, at most \(100\) examples per class, as a trusted set in absence of a labelled set. The standard cross-entropy loss is applied to the trusted set. For the rest of the unlabelled images, we use a weakly augmented (Open-CLIP [26] validation augmentations) view of an image to produce pseudo-labels, and incur a loss on the strongly augmented view (Tip-Adapter [69] augmentations). Denote an image with weak augmentation by \(\mathbf{x}\), its strongly augmented view by \(\mathbf{x}^{\star}\), and the predictions made by network by \(\hat{\mathbf{y}}\) and \(\hat{\mathbf{y}}^{\prime}\), respectively, the unsupervised loss can be expressed as

\[\ell_{u}(\hat{\mathbf{y}},\hat{\mathbf{y}}^{\prime}) =-\mathbbm{1}(\max(\sigma(\hat{\mathbf{y}}))>\omega)\ \sigma(\hat{\mathbf{y}})^{\mathsf{T}}\log(\hat{\mathbf{y}}^{\prime}),\] (15) \[\sigma(\hat{\mathbf{y}}) =\frac{\hat{\mathbf{y}}^{0.5}}{\mathbbm{1}^{\mathsf{T}}\hat{ \mathbf{y}}^{0.5}},\] (16)

where \(\mathbbm{1}(\cdot)\) denotes the indicator function, \(\sigma(\cdot)\) performs re-normalisation with adjusted temperature scaling, and \(\omega\) is a confidence threshold that is linearly adjusted from 0.9 to 1 during training. The trusted set is re-estimated at the beginning of each epoch to account for the improving accuracy of the model. In training, images in the trusted set are over-sampled to constitute one fourth of each batch, as this practice prevents the model from diverging due to confirmation bias [2, 54].

## Appendix F Details of aTLAS \(\times K\) variants

Dividing a parameter block into \(K\) random partitions allows us to introduce more learnable coefficients to each block, thus scaling up our method flexibly. One draw back of this approach, however, is that masks for the partitions have to be stored in memory, resulting in a linear memory increase with respect to the size of the parameter block and the value \(K\). To reduce the memory consumption the of aTLAS \(\times K\) variants, we only apply it to LoRAs task vectors. Nevertheless, these memory requirements could most likely be reduced by exploiting sparse matrices or memory efficient matrix indexing techniques, which we plan to investigate in the future.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Method & \# Params & 1\% & 5\% & 10\% & 25\% & 35\% & 50\% & 100\% \\ \hline aTLAS & 2k & 68.5 & 71.5 & 72.6 & 73.6 & 74.6 & 75.4 & 76.4 \\ aTLAS \(\times 5\) & 10k & 69.3 & 72.9 & 74.7 & 76.2 & 76.8 & 77.5 & 78.8 \\ aTLAS \(\times 20\) & 40k & 69.5 & 74.0 & 75.6 & 77.5 & 78.2 & 78.9 & 80.5 \\ aTLAS \(\times 80\) & 160k & 70.2 & 74.7 & 76.2 & 77.9 & 78.9 & 80.0 & 82.0 \\ aTLAS \(\times 1200\) & 2.4M & **71.3** & **75.0** & **76.6** & **78.3** & **80.2** & **81.5** & **83.9** \\ \hline LoRA (rank=16) & 2.4M & 68.8 & 74.1 & 75.6 & 76.8 & 79.0 & 80.6 & 83.6 \\ \hline \hline \end{tabular}
\end{table}
Table 17: Accuracy after fine-tuning on different percentage of training data for variants of aTLAS \(\times K\) and LoRAs [23]. Results are averaged across 22 datasets. Highest accuracy in each section is highlighted in bold.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims made in the introducing summarise findings in Sections 4, 5 and 6. We clearly enunciate our claims and hypothesis by numbering them by order of appearance in the main body of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalise to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss limitations of our work in a paragraph found in the conclusion, Section 8. The dominant limitation we observe to our work is possible memory limitations when applied to larger models with LoRA task vectors. We otherwise tested our approach on a larger array of varied image classification datasets, ranging from simple to larger datasets. Some of the datasets we tested on are specialized while others are more generic. This should ensure that our results are generalisable to a large array of tasks. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognise that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: We do not contribute any theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data is provided or not)? Answer: [Yes] Justification: The algorithms we propose is fully explained in Section 3 with all the information needed to reproduce our results being available the appendix Sections A, D.1, D.3, E and F. Furthermore, the pre-trained models and every dataset we use are publicly available. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data is provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognise that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.

In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have released the entirety of our code base and task vector checkpoint under the link provided in the paper. All the data used in this paper is not owned by us and is publicly available. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimiser, etc.) necessary to understand the results? Answer: [Yes] Justification: All hyper-paramters, number of training samples, optimiser used, model architectures and everything else needed to reproduce our results are available the appendix Sections A, D.1, D.3, E and F. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We perform 3 independent runs in Figures 4(a), 4(b) and 4(c) with standard errors being reported. The standard error is computed as the standard deviation over 3 runs divided by the number of runs (3). This is calculated with the numpy library.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: Although we do not report compute requirements in the paper, on a single A100 or 4090GPU except for ViT-L/14 experiments that were performed on 2 A100. The typical run time for an experiment is 2 hours with ViT-L/14 experiments going up to 8h. We estimate the total compute needed to 1000 GPU-hours for repeated results over 3 seeds. The research project includes failed experiements and iterations on the method that are not reported. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We find no direct societal impact for this research paper as research is conducted on controlled open-sourced datasets. This paper did not study applicability to specialized datasets that can be used to impact society. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimising neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognise that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All datasets we use are credited in Section A.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]Justification: No participants.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognise that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.