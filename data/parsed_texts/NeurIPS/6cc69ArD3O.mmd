# Globally injective and bijective neural operators

Takashi Furuya\({}^{1}\)  Michael Puthawala\({}^{2}\)  Matti Lassas\({}^{3}\)  Maarten V. de Hoop\({}^{4}\)

\({}^{1}\)Shimane University, takashi.furuya0101@gmail.com

\({}^{2}\)South Dakota State University, Michael.Puthawala@sdstate.edu

\({}^{3}\)University of Helsinki, matti.lassas@helsinki.fi

\({}^{4}\)Rice University, mdehoop@rice.edu

###### Abstract

Recently there has been great interest in operator learning, where networks learn operators between function spaces from an essentially infinite-dimensional perspective. In this work we present results for when the operators learned by these networks are injective and surjective. As a warmup, we combine prior work in both the finite-dimensional ReLU and operator learning setting by giving sharp conditions under which ReLU layers with linear neural operators are injective. We then consider the case when the activation function is pointwise bijective and obtain sufficient conditions for the layer to be injective. We remark that this question, while trivial in the finite-rank setting, is subtler in the infinite-rank setting and is proven using tools from Fredholm theory. Next, we prove that our supplied injective neural operators are universal approximators and that their implementation, with finite-rank neural networks, are still injective. This ensures that injectivity is not 'lost' in the transcription from analytical operators to their finite-rank implementation with networks. Finally, we conclude with an increase in abstraction and consider general conditions when subnetworks, which may have many layers, are injective and surjective and provide an exact inversion from a 'linearization.' This section uses general arguments from Fredholm theory and Leray-Schauder degree theory for non-linear integral equations to analyze the mapping properties of neural operators in function spaces. These results apply to subnetworks formed from the layers considered in this work, under natural conditions. We believe that our work has applications in Bayesian uncertainty quantification where injectivity enables likelihood estimation and in inverse problems where surjectivity and injectivity corresponds to existence and uniqueness of the solutions, respectively.

## 1 Introduction

In this work, we produce results at the intersection of two fields: neural operators (NO), and injective and bijective networks. Neural operators [13, 14] are neural networks that take a infinite dimensional perspective on approximation by directly learning an operator between Sobolev spaces. Injectivity and bijectivity on the other hand are fundamental properties of networks that enable likelihood estimation by the change of variables formula, are critical in applications to inverse problems, and are useful properties for downstream applications.

The key contribution of our work is the translation of fundamental notions from the finite-rank setting to the infinite-rank setting. By the 'infinite-dimension setting' we refer to the case when the object of approximation is a mapping between Sobolev spaces. This task, although straight-forward on first inspection, often requires dramatically different arguments and proofs as the topology, analysis and notion of noise are much simpler in the finite-rank case as compared to the infinite-rank case. We see our work as laying the groundwork for the application of neural operators to generative modelsin function spaces. In the context of operator extensions of traditional VAEs (Kingma and Welling, 2013), injectivity of a decoder forces distinct latent codes to correspond to distinct outputs.

Our work draws parallels between neural operators and pseudodifferential operators Taylor (1981), a class that contains many inverses of linear partial differential operators and integral operators. The connection to pseudodifferential operators provided an algebraic perspective to linear PDE Kohn and Nirenberg (1965). An important fact in the analysis of pseudodifferential operators, is that the inverses of certain operators, e.g. elliptic pseudodifferential operators, are themselves pseudodifferential operators. By proving an analogous result in section 4.2, that the inverse of invertible NO are themselves given by NO, we draw an important and profound connection between (non)linear partial differential equations and NO.

We also believe that our methods have applications to the solution of inverse problems with neural networks. The desire to use injective neural networks is one of the primary motivations for this work. These infinite dimensional models can then be approximated by a finite dimensional model without losing discretization invariance, see Stuart (2010). Crucially, discretization must be done at the last possible moment,' or else performance degrades as the discretization becomes finer, see Lassas and Siltanen (2004) and also Saksman et al. (2009). By formulating machine learning problems in infinite dimensional function spaces and then approximating these methods using finite dimensional subspaces, we avoid bespoke ad-hoc methods and instead obtain methods that apply to any discretization.

More details on our motivations for and applications of injectivity & bijectivity of neural operators are given in Appendix A.

### Our Contribution

In this paper, we give a rigorous framework for the analysis of the injectivity and bijectivity of neural operators. Our contributions are as follows:

1. We show an equivalent condition for the layerwise injectivity and bijectivity for linear neural operators in the case of ReLU and bijective activation functions (Section 2). In the particular ReLU case, the equivalent condition is characterized by a directed spanning set (Definition 2).
2. We prove that injective linear neural operators are universal approximators, and that their implementation by finite rank approximation is still injective (Section 3). We note that universal approximation theorem (Theorem 1) in the infinite dimensional case does not require an increase in dimension, which deviate from the finite dimensional case Puthawala et al. (2022a, Thm. 15).
3. We zoom out and perform a more abstract global analysis in the case when the input and output dimensions are the same. In this section we 'coarsen' the notion of layer, and provide a sufficient condition for the surjectivity and bijectivity of nonlinear integral neural operators _with nonlinear kernels_. This application arises naturally in the context of subnetworks and transformers. We construct their inverses in the bijective case (Section 4).

### Related Works

In the finite-rank setting, injective networks have been well-studied, and shown to be of theoretical and practical interest. See Gomez et al. (2017); Kratsios and Bilkopytov (2020); Teshima et al. (2020); Ishikawa et al. (2022); Puthawala et al. (2022a) for general references establishing the usefulness of injectivity or any of the works on flow networks for the utility of injectivity and bijectivity for downstream applications, (Dinh et al., 2016; Siahkoohi et al., 2020; Chen et al., 2019; Dinh et al., 2014; Kingma et al., 2016), but their study in the infinite-rank setting is comparatively underdeveloped. These works, and others, establish injectivity in the finite-rank setting as a property of theoretical and practical interest. Our work extends Puthawala et al. (2022a) to the infinite-dimensional setting as applied to neural operators, which themselves are a generalization of multilayer perceptrons (MLPs) to function spaces. Moreover, our work includes not only injectivity, but also surjectivity in the non-ReLU activation case, which Puthawala et al. (2022a) has not focused on.

Examples of works in these setting include neural operators Kovachki et al. (2021, 2021); LeepONet Lu et al. (2019); Lanthaler et al. (2022), and PCA-Net Bhattacharya et al. (2021); De Hoop et al. (2022). The authors of Alberti et al. (2022) recently proposed continuous generative neural networks (CGNNs), which are convolution-type architectures for generating \(L^{2}(\mathbb{R})\)-functions, and provided the sufficient condition for the global injectivity of their network. Their approach is the wavelet basis expansion, whereas our work relies on an independent choice of basis expansion.

### Networks considered and notation

Let \(D\subset\mathbb{R}^{d}\) be an open and connected domain, and \(L^{2}(D;\mathbb{R}^{h})\) be the \(L^{2}\) space of \(\mathbb{R}^{h}\)-value function on \(D\) given by \(L^{2}(D;\mathbb{R}^{h})\coloneqq\underbrace{L^{2}(D;\mathbb{R})\times\cdots \times L^{2}(D;\mathbb{R})}_{h}=L^{2}(D)^{h}\).

**Definition 1** (Integral and pointwise neural operators).: _We define an integral neural operator \(G:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{out}}\) and layers \(\mathcal{L}_{\ell}:L^{2}(D)^{d_{\ell+1}}\to L^{2}(D)^{d_{\ell+1}}\) by_

\[G:=T_{L+1}\circ\mathcal{L}_{L}\circ\cdots\mathcal{L}_{1}\circ T_{0},\quad( \mathcal{L}_{\ell}v)(x)\coloneqq\sigma(T_{\ell}(v)(x)+b_{\ell}(x)),\]

\[T_{\ell}(v)(x)=W_{\ell}(x)u(x)+\int_{D}k_{\ell}(x,y)u(y),\quad x\in D,\]

_where \(\sigma:\mathbb{R}\to\mathbb{R}\) is a non-linear activation operating element-wise, and \(k_{\ell}(x,y)\in L^{2}(D\times D;\mathbb{R}^{d_{\ell+1}\times d_{\ell}})\) are integral kernels, and \(W_{\ell}\in C(\overline{D};\mathbb{R}^{d_{\ell+1}\times d_{\ell}})\) are pointwise multiplications with matrices, and \(b_{\ell}\in L^{2}(D)^{d_{\ell+1}}\) are bias functions (\(\ell=1,...,L\)). Here, \(T_{0}:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{1}}\) and \(T_{L+1}:L^{2}(D)^{d_{L+1}}\to L^{2}(D)^{d_{out}}\) are mappings (lifting operator) from the input space to the feature space and mappings (projection operator) from the feature spaces to the output space, respectively._

The layers \(T_{0}\) and \(T_{L+1}\) play a special role in the neural operators. They are local linear operators and serve to lift and project the input data from and to finite-dimensional space respectively. These layers may be 'absorbed' into the layers \(\mathcal{L}_{1}\) and \(\mathcal{L}_{L}\) without loss of generality (under some technical conditions), but are not in this text to maintain consistency with prior work. Prior work assumes that \(d_{in}<d_{1}\), we only assume that \(d_{in}\leq d_{1}\) for lifting operator \(T_{0}:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{1}}\). This would seemingly play an important role in the context of injectivity or universality, but we find that our analysis does not require that \(d_{in}<d_{1}\) at all. In fact, as elaborated in Section 3.2, we may take \(d_{in}=d_{\ell}=d_{out}\) for \(\ell=1,\ldots,L\) and our analysis is the same.

## 2 Injective linear neural operator layers with ReLU and bijective activations

In this section we present sharp conditions under which a layer of a neural operator with \(\mathrm{ReLU}\) activation is injective. The Directed Spanning Set (DSS) condition, described by Def. 2 is a generalization of the finite-dimensional DSS (Puthawala et al., 2022) which guarantees layerwise injectivity of \(\mathrm{ReLU}\) layers. Extending this condition from finite to infinite dimensions is not automatic. The finite-dimensional DSS will hold with high probability for random weight matrices if they are expansive enough (Puthawala et al., 2022, Theorem7). However, the infinite-dimensional DSS is much more restrictive than the finite dimensional setting. We then present a less restrictive condition that is met when the activation function is bijective, e.g. a leaky-ReLU activation is used.

Although it may appear that the end-to-end result is strictly stronger than the layerwise result, this is not the case. The layerwise result is an exact characterization, whereas the end-to-end result is sufficient for injectivity, but not necessary. The layerwise analysis is also constructive, and so gives a rough guide for the construction of injective networks, whereas the global analysis is less so. Finally, the layerwise condition has different applications, such as network of stochastic depth, see e.g. Huang et al. (2016); Benitez et al. (2023). End-to-end injectivity by enforcing layerwise injectivity is straightforward, whereas deriving a sufficient condition for networks of any depth is more daunting.

We denote by

\[\sigma(Tv+b)(x):=\begin{pmatrix}\sigma(T_{1}v(x)+b_{1}(x))\\ \vdots\\ \sigma(T_{m}v(x)+b_{m}(x))\end{pmatrix},\quad x\in D,\;v\in L^{2}(D)^{n}\]where \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\) is a non-linear activation function, \(T\in\mathcal{L}(L^{2}(D)^{n},L^{2}(D)^{m})\), and \(b\in L^{2}(D)^{m}\), where \(\mathcal{L}(L^{2}(D)^{n},L^{2}(D)^{m})\) is the space of linear bounded operators from \(L^{2}(D)^{n}\) to \(L^{2}(D)^{m}\). The aim of this section is to characterize the injectivity condition for the operator \(v\mapsto\sigma(Tv+b)\) mapping from \(L^{2}(D)^{n}\) to \(L^{2}(D)^{m}\), which corresponds to layer operators \(\mathcal{L}_{\ell}\). Here, \(T:L^{2}(D)^{n}\to L^{2}(D)^{m}\) is linear.

### ReLU activation

Let \(\mathrm{ReLU}:\mathbb{R}\rightarrow\mathbb{R}\) be ReLU activation, defined by \(\mathrm{ReLU}(s)=\max\{0,s\}\). With this activation function, we introduce a definition which we will find sharply characterizes layerwise injectivity.

**Definition 2** (Directed Spanning Set).: _We say that the operator \(T+b\) has a directed spanning set (DSS) with respect to \(v\in L^{2}(D)^{n}\) if_

\[\mathrm{Ker}\left(T\big{|}_{S(v,T+b)}\right)\cap X(v,T+b)=\{0\},\] (2.1)

_where \(T|_{S(v,T+b)}(v)=(T_{i}v)_{i\in S(v,T+b)}\) and_

\[S(v,T+b):=\left\{i\in[m]\;\Big{|}\;T_{i}v+b_{i}>0\;in\;D\right\},\] (2.2)

\[X(v,T+b):=\left\{u\in L^{2}(D)^{n}\middle|\begin{array}{l}\text{for }i\notin S (v,T+b)\text{ and }x\in D,\\ (i)\;T_{i}v(x)+b_{i}(x)\leq T_{i}u(x)\text{ if }T_{i}v(x)+b_{i}(x)\leq 0,\\ (ii)\;T_{i}u(x)=0\text{ if }T_{i}v(x)+b_{i}(x)>0\end{array}\right\}.\] (2.3)

The name directed spanning set arises from the \(\mathrm{ker}(T|_{S(v,T+b)})\) term of (2.1). The indices of \(S(v,T+b)\) are those that are directed (positive) in the direction of \(v\). If \(T\) restricted to these indices together span \(L^{2}(D)^{n}\), then \(\mathrm{ker}(T|_{S(v,T+b)})\) is \(\{0\}\), and the condition is automatically satisfied. Hence, the DSS condition measures the extent to which the set of indices, which are directed w.r.t. \(v\), form a span of the input space.

**Proposition 1**.: _Let \(T\in\mathcal{L}(L^{2}(D)^{n},L^{2}(D)^{m})\) and \(b\in L^{2}(D)^{m}\). Then, the operator \(\mathrm{ReLU}\circ(T+b):L^{2}(D)^{n}\to L^{2}(D)^{m}\) is injective if and only if \(T+b\) has a DSS with respect to every \(v\in L^{2}(D)^{n}\) in the sense of Definition 2._

See Section B for the proof. Puthawala et al. (2022) has provided the equivalent condition for the injectivity of ReLU operator in the case of the Euclidean space. However, proving analogous results for operators in function spaces require different techniques. Note that because Def. 2 is a sharp characterization of injectivity, it can not be simplified in any significant way. The condition restrictive Def. 2 is, therefore, difficult to relax while maintaining generality. This is because for each function \(v\), multiple components of the function \(Tv+b\) are strictly positive in the entire domain \(D\), and cardinality \(|S(v;T+b)|\) of \(S(v;T+b)\) is larger than \(n\). This observation prompts us to consider the use of bijective activation functions instead of ReLU, such as leaky ReLU function, defined by \(\sigma_{a}(s):=\mathrm{ReLU}(s)-a\;\mathrm{ReLU}(-s)\) where \(a>0\).

### Bijective activation

If \(\sigma\) is injective, then injectivity of \(\sigma\circ(T+b):L^{2}(D)^{n}\to L^{2}(D)^{m}\) is equivalent to the injectivity of \(T\). Therefore, we consider the bijectivity in the case of \(n=m\). As mentioned in Section 1.3, an significant example is \(T=W+K\), where \(W\in\mathbb{R}^{n\times n}\) is injective and \(K:L^{2}(D)^{n}\to L^{2}(D)^{n}\) is a linear integral operator with a smooth kernel. This can be generalized to Fredholm operators (see e.g., Jeribi (2015, Section 2.1.4)), which encompasses the property for identity plus a compact operator. It is well known that a Fredholm operator is bijective if and only if it is injective and its Fredholm index is zero. We summarize the above observation as follows:

**Proposition 2**.: _Let \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\) be bijective, and let \(T:L^{2}(D)^{n}\to L^{2}(D)^{m}\) and \(b\in L^{2}(D)^{m}\). Then, \(\sigma\circ(T+b):L^{2}(D)^{n}\to L^{2}(D)^{m}\) is injective if and only if \(T:L^{2}(D)^{n}\to L^{2}(D)^{m}\) is injective. Furthermore, if \(n=m\) and \(T\in\mathcal{L}(L^{2}(D)^{n},L^{2}(D)^{n})\) is the linear Fredholm operator, then, \(\sigma\circ(T+b):L^{2}(D)^{n}\to L^{2}(D)^{n}\) is bijective if and only if \(T:L^{2}(D)^{n}\to L^{2}(D)^{n}\) is injective with index zero._

We believe that this characterization of layerwise injectivity is considerably less restrictive than Def. 2, and the characterization of bijectivity in terms of Fredholm theory will be particularly useful in establishing operator generalization of flow networks.

Global analysis of injectivity and finite-rank implementation

In this section we consider global properties of the injective and bijective networks that constructed in Section 2. First we construct end-to-end injective networks that are not layerwise injective. By doing this, we may avoid the dimension increasing requirement that would be necessary from a layerwise analysis. Next we show that injective neural operators are universal approximators of continuous functions. Although the punchline resembles that of [Puthawala et al., 2022a, Theorem 15], which relied on Whitney's embedding theorem, the arguments are quite different. The finite-rank case has dimensionality restrictions, as required by degree theory, whereas our infinite-rank result does not. Finally, because all implementations of neural operators are ultimately finite-dimensional, we present a theorem that gives conditions under which finite-rank approximations to injective neural operators are also injective.

### Global analysis

By using the characterization of layerwise injectivity discussed in Section 2, we can compose injective layers to form \(\mathcal{L}_{L}\circ\cdots\mathcal{L}_{1}\circ T_{0}\), a injective network. Layerwise injectivity, however, prevents us from getting injectivity of \(T_{L+1}\circ\mathcal{L}_{L}\circ\cdots\mathcal{L}_{1}\circ T_{0}\) by a layerwise analysis if \(d_{L+1}>d_{out}\), as is common in application [Kovachki et al., 2021b, Pg. 9]. In this section, we consider global analysis and show that \(T_{L+1}\circ\mathcal{L}_{L}\circ\cdots\mathcal{L}_{1}\circ T_{0}\), nevertheless, remains injective. This is summarized in the following lemma.

**Lemma 1**.: _Let \(\ell\in\mathbb{N}\) with \(\ell<m\), and let the operator \(T:L^{2}(D)^{n}\to L^{2}(D)^{m}\) be injective. Assume that there exists an orthogonal sequence \(\{\xi_{k}\}_{k\in\mathbb{N}}\) in \(L^{2}(D)\) and a subspace \(S\) in \(L^{2}(D)\) such that \(\mathrm{Ran}(\pi_{1}T)\subset S\) and_

\[\mathrm{span}\{\xi_{k}\}_{k\in\mathbb{N}}\cap S=\{0\}.\] (3.1)

_where \(\pi_{1}:L^{2}(D)^{m}\to L^{2}(D)\) is the restriction operator defined in (C.1). Then, there exists a linear bounded operator \(B\in\mathcal{L}(L^{2}(D)^{m},L^{2}(D)^{\ell})\) such that \(B\circ T:L^{2}(D)^{n}\to L^{2}(D)^{\ell}\) is injective._

See Section C.1 in Appendix C for the proof. \(T\) and \(B\) correspond to \(\mathcal{L}_{L}\circ\cdots\mathcal{L}_{1}\circ T_{0}\) (from lifting to \(L\)-th layer) and \(T_{L+1}\) (projection), respectively. The assumption (3.1) on the span of \(\xi_{k}\) encodes a subspace distinct from the range of \(T\). In Remark 3 of Appendix C, we provide an example that satisfies the assumption (3.1). Moreover, in Remark 4 of Appendix C, we show the exact construction of the operator \(B\) by employing projections onto the closed subspace, using the orthogonal sequence \(\{\xi_{k}\}_{k\in\mathbb{N}}\). This construction is given by the combination of "Pairs of projections" discussed in Kato (2013, Section I.4.6) with the idea presented in [Puthawala et al., 2022b, Lemma 29].

### Universal approximation

We now show that the injective networks that we consider in this work are universal approximators. We define the set of integral neural operators with \(L^{2}\)-integral kernels by

\[\mathrm{NO}_{L}(\sigma;D,d_{in},d_{out}):=\Big{\{}G:L^{2}(D)^{d_{ in}}\to L^{2}(D)^{d_{out}}:\] \[G=K_{L+1}\circ(K_{L}+b_{L})\circ\sigma\cdots\circ(K_{2}+b_{2}) \circ\sigma\circ(K_{1}+b_{1})\circ(K_{0}+b_{0}),\] \[K_{\ell}\in\mathcal{L}(L^{2}(D)^{d_{\ell}},L^{2}(D)^{d_{\ell+1}} ),\;K_{\ell}:f\mapsto\int_{D}k_{\ell}(\cdot,y)f(y)dy\Big{|}_{D},\;k_{\ell}\in L ^{2}(D\times D;\mathbb{R}^{d_{\ell+1}\times d_{\ell}}),\] \[b_{\ell}\in L^{2}(D;\mathbb{R}^{d_{\ell+1}}),\;d_{\ell}\in \mathbb{N},\;d_{0}=d_{in},\;d_{L+2}=d_{out},\;\ell=0,...,L+2\Big{\}},\] (3.2)

and

\[\mathrm{NO}_{L}^{inj}(\sigma;D,d_{in},d_{out}):=\{G\in\mathrm{NO}_{L}(\sigma; D,d_{in},d_{out}):\text{$G$ is injective}\}.\]

The following theorem shows that \(L^{2}\)-injective neural operators are universal approximators of continuous operators.

**Theorem 1**.: _Let \(D\subset\mathbb{R}^{d}\) be a Lipschitz bounded domain, and \(G^{+}:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{out}}\) be continuous such that for all \(R>0\) there is \(M>0\) so that_

\[\big{\|}G^{+}(a)\big{\|}_{L^{2}(D)^{d_{out}}}\leq M,\;\forall a\in L^{2}(D)^{ d_{in}},\;\|a\|_{L^{2}(D)^{d_{in}}}\leq R,\] (3.3)_We assume that either (i) \(\sigma\in\Lambda_{0}^{L}\cap\mathrm{BA}\) is injective, or (ii) \(\sigma=\mathrm{ReLU}\). Then, for any compact set \(K\subset L^{2}(D)^{d_{in}}\), \(\epsilon\in(0,1)\), there exists \(L\in\mathbb{N}\) and \(G\in\mathrm{NO}_{L}^{inj}(\sigma;D,d_{in},d_{out})\) such that_

\[\sup_{a\in K}\left\|G^{+}(a)-G(a)\right\|_{L^{2}(D)^{d_{out}}}\leq\epsilon.\]

See Section C.3 in Appendix C for the proof. For the definitions of \(\Lambda_{0}^{L}\) and \(\mathrm{BA}\), see Definition 3 in Appendix C. For example, ReLU and Leaky ReLU functions belong to \(\Lambda_{0}^{L}\cap\mathrm{BA}\) (see Remark 5 (i)).

We briefly remark on the proof of Theorem 1 emphasizing how its proof differs from a straightforward extension of the finite-rank case. In the proof we first employ the standard universal approximation theorem for neural operators ((Kovachki et al., 2021b, Theorem 11)). We denote the approximation of \(G^{+}\) by \(\widetilde{G}\), and define the graph of \(\widetilde{G}\) as \(H:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{in}}\times L^{2}(D)^{d_{out}}\). That is \(H(u)=(u,\widetilde{G}(u))\). Next, utilizing Lemma 1, we construct the projection \(Q\) such that \(Q\circ H\) becomes an injective approximator of \(G^{+}\) and belongs to \(\mathrm{NO}_{L}(\sigma;D,d_{in},d_{out})\). The proof for universal approximation theorem is constructive. If, in the future, efficient approximation bounds for neural operators are given, such bounds can likely be used directly in our universality proof to generate corresponding efficient approximation bounds for injective neural operators.

This approach resembles the approach in the finite-rank space Puthawala et al. (2022a, Theorem 15), but unlike that theorem we don't have any dimensionality restrictions. More specifically, in the case of Euclidean spaces \(\mathbb{R}^{d}\), Puthawala et al. (2022a, Theorem 15) requires that \(2d_{in}+1\leq d_{out}\) before all continuous functions \(G^{+}:\mathbb{R}^{d_{in}}\to\mathbb{R}^{d_{out}}\) can be uniformly approximated in compact sets by injective neural networks. When \(d_{in}=d_{out}=1\) this result is not true, as is shown in Remark 5 (iii) in Appendix C using topological degree theory (Cho and Chen, 2006). In contrast, Theorem 1 does not assume any conditions on \(d_{in}\) and \(d_{out}\). Therefore, we can conclude that infinite dimensional case yields better approximation results than the finite dimensional case.

This surprising improvement in restrictions in infinite-dimensions can be elucidated by an analogy to Hilbert's hotel paradox, see (Burger and Starbird, 2004, Sec 3.2). In this analogy, the orthonormal bases \(\{\varphi_{k}\}_{k\in\mathbb{N}}\) and \(\Psi_{j,k}(x)=(\delta_{ij}\varphi_{k}(x))_{i=1}^{d}\) play the part of guests in the hotel with \(\mathbb{N}\) floor, each of which as \(d\) rooms. A key step in the proof of Theorem 1 is that there is a linear isomorphism \(S:L^{2}(D)^{d}\to L^{2}(D)\) (i.e., a rearrangement of guests) which maps \(\Psi_{j,k}\) to \(\varphi_{b(j,k)}\), where \(b:[d]\times\mathbb{N}\to\mathbb{N}\) is a bijection.

### Injectivity-preserving transfer to Euclidean spaces via finite-rank approximation

In the previous section, we have discussed injective integral neural operators. The conditions are given in terms of integral kernels, but such kernels may not actually be implementable with finite width and depth networks, which have a finite representational power. A natural question to ask, therefore, is how should these formal integral kernels be implemented on actual finite rank networks, the so-called implementable neural operators? In this section we discuss this question.

We consider linear integral operators \(K_{\ell}\) with \(L^{2}\)-integral kernels \(k_{\ell}(x,y)\). Let \(\{\varphi_{k}\}_{k\in\mathbb{N}}\) be an orthonormal basis in \(L^{2}(D)\). Since \(\{\varphi_{k}(y)\varphi_{p}(x)\}_{k,p\in\mathbb{N}}\) is an orthonormal basis of \(L^{2}(D\times D)\), integral kernels \(k_{\ell}\in L^{2}(D\times D;\mathbb{R}^{d_{\ell+1}\times d_{\ell}})\) in integral operators \(K_{\ell}\in\mathcal{L}(L^{2}(D)^{d_{\ell}},L^{2}(D)^{d_{\ell+1}})\) has the expansion

\[k_{\ell}(x,y)=\sum_{k,p\in\mathbb{N}}C_{k,p}^{(\ell)}\varphi_{k}(y)\varphi_{ p}(x),\]

where \(C_{k,p}^{(\ell)}\in\mathbb{R}^{d_{\ell+1}\times d_{\ell}}\) whose \((i,j)\)-th component \(c_{k,p,ij}^{(\ell)}\) is given by

\[c_{k,p,ij}^{(\ell)}=(k_{\ell,ij},\varphi_{k}\varphi_{p})_{L^{2}(D\times D)},\]

Here, we denote \((u,\varphi_{k})\in\mathbb{R}^{d_{\ell}}\) by \((u,\varphi_{k})=\left((u_{1},\varphi_{k})_{L^{2}(D)},...,(u_{d_{\ell}}, \varphi_{k})_{L^{2}(D)}\right).\) By truncating by \(N\) finite sums, we approximate \(L^{2}\)-integral operators \(K_{\ell}\in\mathcal{L}(L^{2}(D)^{d_{\ell}},L^{2}(D)^{d_{\ell+1}})\) by finite rank operator \(K_{\ell,N}\) with rank \(N\), having the form

\[K_{\ell,N}u(x)=\sum_{k,p\in[N]}C_{k,p}^{(\ell)}(u,\varphi_{k})\varphi_{p}(x), \;u\in L^{2}(D)^{d_{\ell}}.\]The choice of orthonormal basis \(\{\varphi_{k}\}_{k\in\mathbb{N}}\) is a hyperparameter. If we choose \(\{\varphi_{k}\}_{k}\) as Fourier basis and wavelet basis, then network architectures correspond to Fourier Neural Operators (FNOs) [Li et al., 2020b] and Wavelet Neural Operators (WNOs) [Tripura and Chakraborty, 2023], respectively. We show that Propositions 1, 2 (characterization of layerwise injectivity), and Lemma 1 (global injectivity) all have natural analogues for finite rank operator \(K_{\ell,N}\) in Proposition 5 and Lemma 2 in Appendix D. These conditions applies out-of-the-box to both FNOs and WNOs.

Lemma 2 and Remark 3 in the appendix D give a'recipe' to construct the projection \(B\) such that the composition \(B\circ T\) (interpreted as augmenting finite rank neural operator \(T\) with one layer \(B\)) is injective. The projection \(B\) is constructed by using an orthogonal sequence \(\{\xi_{k}\}_{k\in\mathbb{N}}\) subject to the condition (3.1), which does not over-leap the range of \(T\). This condition is automatically satisfied for any orthogonal base \(\{\varphi_{k}\}_{k\in\mathbb{N}}\). This could yield practical implications in guiding the choice of the orthogonal basis \(\{\varphi_{k}\}_{k\in\mathbb{N}}\) for the neural operator's design.

We also show the universal approximation in the case of finite rank approximation. We denote \(\mathrm{NO}_{L,N}(\sigma;D,d_{in},d_{out})\) by the set of integral neural operators with \(N\) rank, that is the set (3.2) replacing \(L^{2}\)-integral kernel operators \(K_{\ell}\) with finite rank operators \(K_{\ell,N}\) with rank \(N\) (see Definition 4). We define by

\[\mathrm{NO}_{L,N}^{inj}(\sigma;D,d_{in},d_{out}):=\{G_{N}\in\mathrm{NO}_{L,N}( \sigma;D,d_{in},d_{out}):G_{N}\text{ is injective}\}.\]

**Theorem 2**.: _Let \(D\subset\mathbb{R}^{d}\) be a Lipschitz bounded domain, and \(N\in\mathbb{N}\), and \(G^{+}:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{out}}\) be continuous with boundedness as in (3.3). Assume that the non-linear activation function \(\sigma\) is either ReLU or Leaky ReLU. Then, for any compact set \(K\subset L^{2}(D)^{d_{in}}\cap(\mathrm{span}\{\varphi_{k}\}_{k\leq N})^{d_{in}}\), \(\epsilon\in(0,1)\), there exists \(L\in\mathbb{N}\), \(N^{\prime}\in\mathbb{N}\) with_

\[N^{\prime}d_{out}\geq 2Nd_{in}+1,\] (3.4)

_and \(G_{N^{\prime}}\in\mathrm{NO}_{L,N^{\prime}}^{inj}(\sigma;D,d_{in},d_{out})\) such that_

\[\sup_{a\in K}\left\|G^{+}(a)-G_{N^{\prime}}(a)\right\|_{L^{2}(D)^{d_{out}}} \leq\epsilon.\]

See Section D.4 in Appendix D for the proof. In the proof, we make use of Puthawala et al. [2022a, Lemma 29], which gives rise to the assumption (3.4). We do not require any condition on \(d_{in}\) and \(d_{out}\) as well as Theorem 1.

**Remark 1**.: _Observe that in our finite-rank approximation result, we only require that the target function G+ is continuous and bounded, but not smooth. This differs from prior work that requires smoothness of the function to be approximated._

### Limitations

We assume square matrices for the bijectivity and construction of inversions. Weight matrices in actual neural operators are not necessarily square. Lifting and projection are local operators which map the inputs into a higher-dimensional feature space and project back the feature output to the output space. We also haven't addressed possible aliasing effects of our injective operators. We will relax the square assumption and investigate the aliasing of injective operators in future work.

## 4 Subnetworks & nonlinear integral operators: bijectivity and inversion

So far our analysis of injectivity has been restricted to the case where the only source of nonlinearity are the activation functions. In this section we consider a weaker and more abstract problem where nonlinearities can also arise from the integral kernel with surjective activation function, such as leaky \(\mathrm{ReLU}\). Specifically, we consider layers of the form

\[F_{1}(u)=Wu+K(u),\] (4.1)

where \(W\in\mathcal{L}(L^{2}(D)^{n},L^{2}(D)^{n})\) is a linear bounded bijective operator, and \(K:L^{2}(D)^{n}\to L^{2}(D)^{n}\) is a non-linear operator. This arises in the non-linear neural operator construction by Kovachki et al. [2021b] or in Ong et al. [2022] to improve performance of integral autoencoders. In this construction, each layer \(\mathcal{L}_{\ell}\) is written as

\[x\in D,\quad(\mathcal{L}_{\ell}v)(x)=\sigma(W_{\ell}v(x)+K_{\ell}(v)(x)),\quad K _{\ell}(u)(x)=\int_{D}k_{\ell}(x,y,u(x),u(y))u(y)dy,\]where \(W_{\ell}\in\mathbb{R}^{d_{\ell+1}\times d_{\ell}}\) independent of \(x\), and \(K_{\ell}:L^{2}(D)^{d_{\ell}}\to L^{2}(D)^{d_{\ell+1}}\) is the non-linear integral operator.

This relaxing of assumptions is motivated by a desire to obtain theoretical results for both subnetworks and operator transformers. By subnetworks, we mean compositions of layers within a network. This includes, for example, the encoder or decoder block of a traditional VAE. By neural operator we mean operator generalizations of finite-rank transformers, which can be modeled by letting \(K\) be an integral transformation with nonlinear kernel \(k\) of the form

\[k(x,y,v(x),v(y))\equiv\mathrm{softmax}\circ\left\langle Av(x),Bv(y)\right\rangle,\]

where \(A\) and \(B\) are matrices of free parameters, and the (integral) softmax is taken over \(x\). This specific choice of \(k\) can be understood as a natural generalization of the attention mechanism in transformers, see (Kovachki et al., 2021b, Sec. 5.2) for further details.

### Surjectivity and bijectivity

Critical to our analysis is the notion of coercivity. Apart from being a useful theoretical tool, layerwise coercivity of neural networks is a useful property in imaging applications, see e.g.Li et al. (2020)Recall from Showalter (2010, Sec 2, Chap VII) that a non-linear operator \(K:L^{2}(D)^{n}\to L^{2}(D)^{n}\) is coercive if

\[\lim_{\|u\|_{L^{2}(D)^{n}}\to\infty}\langle K(u),\frac{u}{\|u\|_{L^{2}(D)^{n}} }\rangle_{L^{2}(D)^{n}}=\infty.\] (4.2)

**Proposition 3**.: _Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be surjective and \(W:L^{2}(D)^{n}\to L^{2}(D)^{n}\) be linear bounded bijective (then the inverse \(W^{-1}\) is bounded linear), and let \(K:L^{2}(D)^{n}\to L^{2}(D)^{n}\) be a continuous and compact mapping. Moreover, assume that the map \(u\mapsto\alpha u+W^{-1}K(u)\) is coercive with some \(0<\alpha<1\). Then, the operator \(\sigma\circ F_{1}\) is surjective._

See Section E.1 in Appendix E for the proof. An example \(K\) satisfying the coercivity (4.2) condition is given here.

**Example 1**.: _We simply consider the case of \(n=1\), and \(D\subset\mathbb{R}^{d}\) is a bounded interval. We consider the non-linear integral operator_

\[K(u)(x):=\int_{D}k(x,y,u(x))u(y)dy,\;x\in D.\]

_The operator \(u\mapsto\alpha u+W^{-1}K(u)\) with some \(0<\alpha<1\) is coercive when the non-linear integral kernel \(k(x,y,t)\) satisfies certain boundedness conditions. In Examples 2 and 3 in Appendix E, we show that these conditions are met by kernels \(k(x,y,t)\) of the form_

\[k(x,y,t)=\sum_{j=1}^{J}c_{j}(x,y)\sigma(a_{j}(x,y)t+b_{j}(x,y)),\]

_where \(\sigma:\mathbb{R}\to\mathbb{R}\) is the sigmoid function \(\sigma_{s}:\mathbb{R}\to\mathbb{R}\), and \(a,b,c\in C(\overline{D}\times\overline{D})\) or by a wavelet activation function \(\sigma_{wire}:\mathbb{R}\to\mathbb{R}\), see Saragadam et al. (2023), where \(\sigma_{wire}(t)=\text{Im}\left(e^{i\omega t}e^{-t^{2}}\right)\) and \(a,b,c\in C(\overline{D}\times\overline{D})\), and \(a_{j}(x,y)\neq 0\)._

In the proof of Proposition 3, we utilize the Leray-Schauder fix point theorem. By employing the Banach fix point theorem under a contraction mapping condition (4.3), we can obtain bijectivity as follows:

**Proposition 4**.: _Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be bijective. Let \(W:L^{2}(D)^{n}\to L^{2}(D)^{n}\) be bounded linear bijective, and let \(K:L^{2}(D)^{n}\to L^{2}(D)^{n}\). If \(W^{-1}K:L^{2}(D)^{n}\to L^{2}(D)^{n}\) is a contraction mapping, that is, there exists \(\rho\in(0,1)\) such that_

\[\left\|W^{-1}K(u)-W^{-1}K(v)\right\|\leq\rho\left\|u-v\right\|,\;u,v\in L^{2} (D)^{n},\] (4.3)

_then, the operator \(\sigma\circ F_{1}\) is bijective._

See Section E.3 in Appendix E for the proof. We note that if \(K\) is compact linear, then assumption (4.3) implies that \(W+K\) is an injective Fredholm operator with index zero, which is equivalent to \(\sigma\circ(W+K)\) being bijective as observed in Proposition 2. That is, Proposition 4 requires stronger assumptions when applied to the linear case.

Assumption (4.3) implies the the injectivity of \(K\). An interesting example of injective operators arises when \(K\) are Volterra operators. When \(D\subset\mathbb{R}^{d}\) is bounded and \(K(u)=\int_{D}k(x,y,u(y))u(y)dy\), where we denote \(x=(x_{1},\ldots,x_{d})\) and \(y=(y_{1},\ldots,y_{d})\), we recall that \(K\) is a Volterra operator if \(k(x,y,t)\neq 0\) implies \(y_{j}\leq x_{j}\) for all \(j=1,2,\ldots,d\). A well known fact, as discussed in Example 4 in Appendix E, is that if \(K(u)\) is a Volterra operator whose kernel \(k(x,y,t)\in C(\overline{D}\times\overline{D}\times\mathbb{R}^{n})\) is bounded and uniformly Lipschitz in \(t\)-variable then \(F:u\mapsto u+K(u)\) is injective.

**Remark 2**.: _A similar analysis (illustrating coercivity) shows that operators of the following two forms are bijective._

1. _Operators of the form_ \(F(U)\coloneqq\alpha u+K(u)\) _where_ \(K(u)\coloneqq\int_{D}a(x,y,u(x),u(y))dy\) _where_ \(a(x,y,s_{1},s_{2})\) _is continuous and is such that_ \(\exists R>0\)_,_ \(c_{1}<\alpha\) _so that for all_ \(|(s_{1},s_{2})|>R\)_,_ \(\operatorname{sign}(s_{1})\operatorname{sign}(s_{2})a(x,y,s_{1},s_{2})\geq-c_{1}\)_._
2. _Layers of the form_ \((\mathcal{L}_{\ell}v)(x)\coloneqq\sigma_{1}\left(Wu+\sigma_{2}(K(u))\right)\) _where_ \(\sigma_{1}\) _is bijective and_ \(\sigma_{2}\) _bounded._

_Finally we remark that coercivity is bounded preserved by perturbations in a bounded domain. This makes it possible to study non-linear and non-positive perturbations of physical models. For example, in quantum mechanics when a non-negative energy potential \(|\phi|^{4}\) is replaced by a Mexican hat potential \(-C|\phi|^{2}+|\phi|^{4}\), as occurs in the study of magnetization, superconductors and the Higgs field._

### Construction of the inverse of a non-linear integral neural operator

The preceding sections clarified sufficient conditions for surjectivity and bijectivity of the non-linear operator \(F_{1}\). We now consider how to construct the inverse of \(F_{1}\) in a compact set \(\mathcal{Y}\). We find that constructing inverses is possible in a wide variety of settings and, moreover, that the inverses themselves can be given by neural operators. The proof that neural operators may be inverted with other neural operators provides a theoretical justification for Integral Auto Encoder networks [10] where an infinite encoder/decoder pair play a role parallel to those of encoder/decoders in finite-dimensional VAEs [11]. This section proves that the decoder half of a IAE-net is provably able to inverse the encoder half. Our analysis also shows that injective differential operators (as arise in PDEs) and integral operator encoders form a formal algebra under operator composition. We prove this in the rest of this section, but first we summarize the main three steps of the proof.

First, by using the Banach fixed point theorem and invertibility of derivatives of \(F_{1}\) we show that, locally, \(F_{1}\) may be inverted by an iteration of a contractive operator near \(g_{j}=F_{1}(v_{j})\). This makes local inversion simple in balls which cover the set \(\mathcal{Y}\). Second, we construct partition of unity functions \(\Phi_{j}\) that masks the support of each covering ball and allows us to construct one global inverse that simply passes through to the local inverse on the appropriate ball. Third and finally, we show that each function used in both of the above steps are representable using neural operators with distributional kernels.

As a simple case, let us first consider the case when \(n=1\), and \(D\subset\mathbb{R}\) is a bounded interval, and the operator \(F_{1}\) of the form

\[F_{1}(u)(x)=W(x)u(x)+\int_{D}k(x,y,u(y))u(y)dy,\]

where \(W\in C^{1}(\overline{D})\) satisfies \(0<c_{1}\leq W(x)\leq c_{2}\) and the function \((x,y,s)\mapsto k(x,y,s)\) is in \(C^{3}(\overline{D}\times\overline{D}\times\mathbb{R})\) and in \(\overline{D}\times\overline{D}\times\mathbb{R}\) its three derivatives and the derivatives of \(W\) are uniformly bounded by \(c_{0}\), that is,

\[\left\|k\right\|_{C^{3}(\overline{D}\times\overline{D}\times\mathbb{R})}\leq c _{0},\quad\left\|W\right\|_{C^{1}(\overline{D})}\leq c_{0}.\] (4.4)

The condition (4.4) implies that \(F_{1}:H^{1}(D)\to H^{1}(D)\), contains locally Lipschitz smooth functions. Furthermore, \(F_{1}:H^{1}(D)\to H^{1}(D)\) is Frechet differentiable at \(u_{0}\in C(\overline{D})\), and we denote Frechetderivative of \(F_{1}\) at \(u_{0}\) by \(A_{u_{0}}\), which can be written as the integral operator (F.2). We will assume that for all \(u_{0}\in C(\overline{D})\), the integral operator

\[A_{u_{0}}:H^{1}(D)\to H^{1}(D)\text{ is an injective operator}.\] (4.5)

This happens for example when \(K(u)\) is a Volterra operator, see Examples 4 and 5. As the integral operators \(A_{u_{0}}\) are Fredholm operators having index zero, this implies that the operators (4.5) are bijective. The inverse operator \(A_{u_{0}}^{-1}:H^{1}(D)\to H^{1}(D)\) can be written by the integral operator (F.5).

We will consider the inverse function of the map \(F_{1}\) in \(\mathcal{Y}\subset\sigma_{a}(\overline{B}_{C^{1,\alpha}(\overline{D})}(0,R)) =\{\sigma_{a}\circ g\in C(\overline{D}):\|g\|_{C^{1,\alpha}(\overline{D})}\leq R\}\), which is a set of the image of Holder spaces \(C^{n,\alpha}(\overline{D})\) through (leaky) ReLU-type functions \(\sigma_{a}(s)=\operatorname{ReLU}(s)-a\operatorname{ReLU}(-s)\) with \(a\geq 0\). We note that \(\mathcal{Y}\) is a compact subset the Sobolev space \(H^{1}(D)\), and we use the notations \(B_{\mathcal{X}}(g,R)\) and \(\overline{B}_{\mathcal{X}}(g,R)\) as open and closed balls with radius \(R>0\) at center \(g\in\mathcal{X}\) in Banach space \(\mathcal{X}\).

To this end, we will cover the set \(\mathcal{Y}\) with small balls \(B_{H^{1}(D)}(g_{j},\varepsilon_{0})\), \(j=1,2,\ldots,J\) of \(H^{1}(D)\), centered at \(g_{j}=F_{1}(v_{j})\), where \(v_{j}\in H^{1}(D)\). As considered in detail in Appendix F, when \(g\) is sufficiently near to the function \(g_{j}\) in \(H^{1}(D)\), the inverse map of \(F_{1}\) can be written as a limit \((F_{1}^{-1}(g),g)=\lim_{m\to\infty}\mathcal{H}_{j}^{\alpha m}(v_{j},g)\) in \(H^{1}(D)^{2}\), where

\[\mathcal{H}_{j}\left(\begin{array}{c}u\\ g\end{array}\right)\coloneqq\left(\begin{array}{c}u-A_{v_{j}}^{-1}(F_{1}(u) -F_{1}(v_{j}))+A_{v_{j}}^{-1}(g-g_{j})\\ g\end{array}\right),\]

that is, near \(g_{j}\) we can approximate \(F_{1}^{-1}\) as a composition \(\mathcal{H}_{j}^{\alpha m}\) of \(2m\) layers of neural operators.

To glue the local inverse maps together, we use a partition of unity \(\Phi_{\vec{i}}\), \(\vec{i}\in\mathcal{I}\) in the function space \(\mathcal{Y}\), where \(\mathcal{I}\subset\mathbb{Z}^{\ell_{0}}\) is a finite index set. The function \(\Phi_{\vec{i}}\) are given by neural operators

\[\Phi_{\vec{i}}(v,w)=\pi_{1}\circ\phi_{\vec{i},1}\circ\phi_{\vec{i},2}\circ \cdots\circ\phi_{\vec{i},\ell_{0}}(v,w),\quad\text{where}\quad\phi_{\vec{i}, \ell}(v,w)=(F_{y_{\ell},s(\vec{i},\ell),\epsilon_{1}}(v,w),w),\]

where some \(\epsilon_{1}>0\), \(s(\vec{i},\ell)\in\mathbb{R}\) are some suitable values near \(g_{j(\vec{i})}(y_{\ell})\), some \(y_{\ell}\in D\) (\(\ell=1,...,\ell_{0}\)), and \(\pi_{1}\) is the map \(\pi_{1}(v,w)=v\) that maps a pair \((v,w)\) to the first function \(v\). Here, \(F_{z,s,h}(v,w)\) are integral neural operators with distributional kernels \(F_{z,s,h}(v,w)(x)=\int_{D}k_{z,s,h}(x,y,v(x),w(y))dy\), where \(k_{z,s,h}(x,y,v(x),w(y))=v(x)\mathbf{1}_{[s-\frac{1}{2}h,s+\frac{1}{2}h)}(w(y) )\delta(y-z)\), and \(\mathbf{1}_{A}\) is a indicator function of a set \(A\) and \(y\mapsto\delta(y-z)\) is the Dirac delta distribution at the point \(z\in D\). Using these, we can write the inverse of \(F_{1}\) at \(g\in\mathcal{Y}\) as

\[F_{1}^{-1}(g)=\lim_{m\to\infty}\sum_{\vec{i}\in\mathcal{I}}\Phi_{\vec{i}} \mathcal{H}_{j(\vec{i})}^{\alpha m}\left(\begin{array}{c}v_{j(\vec{i})}\\ g\end{array}\right)\quad\text{in }H^{1}(D)\] (4.6)

where \(j(\vec{i})\in\{1,2,\ldots,J\}\).

This result is summarized in following theorem which is proven in Appendix F.

**Theorem 3**.: _Assume that \(F_{1}\) satisfies the above assumptions (4.4) and (4.5) and that \(F_{1}:H^{1}(D)\to H^{1}(D)\) is a bijection. Let \(\mathcal{Y}\subset\sigma_{a}(\overline{B}_{C^{1,\alpha}(\overline{D})}(0,R))\) be a compact subset of the Sobolev space \(H^{1}(D)\), where \(\alpha>0\) and \(a\geq 0\). Then the inverse of \(F_{1}:H^{1}(D)\to H^{1}(D)\) in \(\mathcal{Y}\) can written as a limit (4.6) that is, as a limit of integral neural operators._

## 5 Discussion and Conclusion

In this paper, we provided a theoretical analysis of injectivity and bijectivity for neural operators. In the future, we will further develop applications of our theory, particularly in the areas of generative models and inverse problems and integral autoencoders. We gave a rigorous framework for the analysis of the injectivity and bijectivity of neural operators including when either the \(\operatorname{ReLU}\) activation or bijective activation functions are used. We further proved that injective neural operators are universal approximators and their finite-rank implementation are still injective. Finally, we ended by considering the 'coarser' problem of non-linear integral operators, as arises in subnetworks, operator transformers and integral autoencoders.

## Acknowledgments

ML was partially supported by Academy of Finland, grants 273979, 284715, 312110. M.V. de H. was supported by the Simons Foundation under the MATH + X program, the National Science Foundation under grant DMS-2108175, and the corporate members of the Geo-Mathematical Imaging Group at Rice University.

## References

* Alberti et al. (2022) Giovanni S. Alberti, Matteo Santacesaria, and Silvia Sciutto. Continuous generative neural networks. _arXiv:2205.14627_, 2022.
* Benitez et al. (2023) Jose Antonio Lara Benitez, Takashi Furuya, Florian Faucher, Xavier Tricoche, and Maarten V de Hoop. Fine-tuning neural-operator architectures for training and generalization. _arXiv preprint arXiv:2301.11509_, 2023.
* Bhattacharya et al. (2021) Kaushik Bhattacharya, Bamdad Hosseini, Nikola B Kovachki, and Andrew M Stuart. Model reduction and neural networks for parametric pdes. _The SMAI journal of computational mathematics_, 7:121-157, 2021.
* Burger and Starbird (2004) Edward B Burger and Michael Starbird. _The heart of mathematics: An invitation to effective thinking_. Springer Science & Business Media, 2004.
* Chen et al. (2019) Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and Jorn-Henrik Jacobsen. Residual flows for invertible generative modeling. _Advances in Neural Information Processing Systems_, 32, 2019.
* Cho and Chen (2006) Yeol Je Cho and Yu-Qing Chen. _Topological degree theory and applications_. CRC Press, 2006.
* Hoop et al. (2022) Maarten De Hoop, Daniel Zhengyu Huang, Elizabeth Qian, and Andrew M Stuart. The cost-accuracy trade-off in operator learning with neural networks. _arXiv preprint arXiv:2203.13181_, 2022.
* Dinh et al. (2014) Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. _arXiv preprint arXiv:1410.8516_, 2014.
* Dinh et al. (2016) Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. _arXiv preprint arXiv:1605.08803_, 2016.
* Gilbarg and Trudinger (2001) David Gilbarg and Neil S. Trudinger. _Elliptic partial differential equations of second order_. Classics in Mathematics. Springer-Verlag, Berlin, 2001. ISBN 3-540-41160-7. Reprint of the 1998 edition.
* Gomez et al. (2017) Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network: Backpropagation without storing activations. _Advances in neural information processing systems_, 30, 2017.
* Huang et al. (2016) Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 646-661. Springer, 2016.
* Ishikawa et al. (2022) Isao Ishikawa, Takeshi Teshima, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and Masashi Sugiyama. Universal approximation property of invertible neural networks. _arXiv preprint arXiv:2204.07415_, 2022.
* Jeribi (2015) Aref Jeribi. _Spectral theory and applications of linear operators and block operator matrices_, volume 9. Springer, 2015.
* Kato (2013) Tosio Kato. _Perturbation theory for linear operators_, volume 132. Springer Science & Business Media, 2013.
* Kingma and Welling (2013) Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* Kingma et al. (2016) Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improving variational inference with inverse autoregressive flow. _arXiv preprint arXiv:1606.04934_, 2016.
* Kingma et al. (2014)Joseph J Kohn and Louis Nirenberg. An algebra of pseudo-differential operators. _Communications on Pure and Applied Mathematics_, 18(1-2):269-305, 1965.
* Kovachki et al. [2021a] Nikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra. On universal approximation and error bounds for fourier neural operators. _Journal of Machine Learning Research_, 22:Art-No, 2021a.
* Kovachki et al. [2021b] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. _arXiv preprint arXiv:2108.08481_, 2021b.
* Kratsios and Bilkopytov [2020] Anastasis Kratsios and Ievgen Bilkopytov. Non-euclidean universal approximation. _Advances in Neural Information Processing Systems_, 33:10635-10646, 2020.
* Lanthaler et al. [2022] Samuel Lanthaler, Siddhartha Mishra, and George E Karniadakis. Error estimates for deeponets: A deep learning framework in infinite dimensions. _Transactions of Mathematics and Its Applications_, 6(1):tnac001, 2022.
* Lassas and Siltanen [2004] Matti Lassas and Samuli Siltanen. Can one use total variation prior for edge-preserving bayesian inversion? _Inverse problems_, 20(5):1537, 2004.
* Li et al. [2020a] Housen Li, Johannes Schwab, Stephan Antholzer, and Markus Haltmeier. NETT: solving inverse problems with deep neural networks. _Inverse Problems_, 36(6):065005, 23, 2020a. ISSN 0266-5611. doi: 10.1088/1361-6420/ab6457. URL https://doi.org/10.1088/1361-6420/ab6d57.
* Li et al. [2020b] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _arXiv preprint arXiv:2010.08895_, 2020b.
* Lu et al. [2019] Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. _arXiv preprint arXiv:1910.03193_, 2019.
* Ong et al. [2022] Yong Zheng Ong, Zuowei Shen, and Haizhao Yang. Iae-net: Integral autoencoders for discretization-invariant learning. _arXiv preprint arXiv:2203.05142_, 2022.
* Pinkus [1999] Allan Pinkus. Approximation theory of the mlp model in neural networks. _Acta numerica_, 8:143-195, 1999.
* Puthawala et al. [2022a] Michael Puthawala, Konik Kothari, Matti Lassas, Ivan Dokmanic, and Maarten de Hoop. Globally injective relu networks. _Journal of Machine Learning Research_, 23(105):1-55, 2022a.
* Puthawala et al. [2022b] Michael Puthawala, Matti Lassas, Ivan Dokmanic, and Maarten De Hoop. Universal joint approximation of manifolds and densities by simple injective flows. In _International Conference on Machine Learning_, pages 17959-17983. PMLR, 2022b.
* Saksman et al. [2009] Matti Lassas Saksman, Samuli Siltanen, et al. Discretization-invariant bayesian inversion and besov space priors. _arXiv preprint arXiv:0901.4220_, 2009.
* Saragadam et al. [2023] Vishwanath Saragadam, Daniel LeJeune, Jasper Tan, Gua Balakrishnan, Ashok Veeraraghavan, and Richard G Baraniuk. Wire: Wavelet implicit neural representations. _arXiv preprint arXiv:2301.05187_, 2023.
* Showalter [2010] Ralph E Showalter. _Hilbert space methods in partial differential equations_. Courier Corporation, 2010.
* Siahkoohi et al. [2020] Ali Siahkoohi, Gabrio Rizzuti, Philipp A Witte, and Felix J Herrmann. Faster uncertainty quantification for inverse problems with conditional normalizing flows. _arXiv preprint arXiv:2007.07985_, 2020.
* Stuart [2010] Andrew M Stuart. Inverse problems: a bayesian perspective. _Acta numerica_, 19:451-559, 2010.
* Taylor [1981] M.E. Taylor. _Pseudodifferential Operators_. Princeton mathematical series. Princeton University Press, 1981.
* Taylor et al. [2010]Takeshi Teshima, Isao Ishikawa, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and Masashi Sugiyama. Coupling-based invertible neural networks are universal diffeomorphism approximators. _Advances in Neural Information Processing Systems_, 33:3362-3373, 2020.
* Tripura and Chakraborty (2023) Tapas Tripura and Souvik Chakraborty. Wavelet neural operator for solving parametric partial differential equations in computational mechanics problems. _Computer Methods in Applied Mechanics and Engineering_, 404:115783, 2023.

## Appendix A Motivation behind our injectivity & bijectivity

PDEs-based inverse problems:We consider the following partial differential equations (PDEs) of the form

\[\mathcal{L}_{a}u(x)=f(x),\quad x\in D,\] \[\mathcal{B}u(x)=g(x),\quad x\in\partial D,\]

where \(D\subset\mathbb{R}^{d}\) is a bounded domain, \(\mathcal{L}_{a}\) is partial differential operator with a coefficient \(a\in\mathcal{A}(D;\mathbb{R}^{d_{a}}):=\{a:D\rightarrow\mathbb{R}^{d_{a}}\}\), and \(\mathcal{B}\) is some boundary operator, e.g. Dirichlet or Neumann, and function \(f\) & \(g\) are fixed. When a solution \(u\in\mathcal{U}(D;\mathbb{R}^{d_{u}}):=\{u:D\rightarrow\mathbb{R}^{d_{a}}\}\) is uniquely determined, we may define the solution operator \(G:\mathcal{A}(D;\mathbb{R}^{d_{u}})\rightarrow\mathcal{U}(D;\mathbb{R}^{d_{u}})\) by putting \(G(a)\coloneqq u\). Note that the operator \(G\) is, in general, non-linear even if partial differential Operator \(\mathcal{L}_{a}\) is linear (e.g., \(\mathcal{L}_{a}=\Delta+a\)). The aim of the inverse problem is to evaluate \(G^{-1}\), the inverse of the solution operator. When \(G\) is non-injective, the problem is termed _ill-posed_.

The key link between PDE and our injective neural operators is as follows. Can \(G\) be approximated by injective neural operators \(N_{inj}\)? If so, then \(N_{inj}\) can be a surrogate model to solve an ill-posed inverse problem. In general, this is possible, even if \(G\) is non-injective, per the results in Section 3.

Moreover, by the results of Section 4, we may conduct the inverse \(N_{inj}^{-1}\) of surrogate model \(N_{inj}\) with another neural operator.

Approximation of pushes forward measure:Let \(\mathcal{M}\) be the submanifold of \(X=L^{2}([0,1]^{2})\) or \(X=L^{2}([0,1]^{3})\) corresponding to natural images or 3D medical model. Let \(K\subset\mathbb{R}^{D}\) be a manifold with the same topology as \(\mathcal{M}\), \(\iota:\mathbb{R}^{D}\to X\) be an embedding, and let \(K_{1}=\iota(K)\subset\mathcal{M}\). Given \(\mu\), a measure supported on \(M\), the task is to find a neural operator \(f_{\theta}\colon X\to X\) that maps (pushes forward) the uniform distribution on the model space \(K_{1}\) to \(\mu\) and so thus maps \(K_{1}\) to \(\mathcal{M}\). If \(f_{\theta}\colon X\to X\) is bijective, computing likelihood functions in statistical analysis is made easier via the change of variables formula. Further, we may interpret \(f_{\theta}^{-1}\) as an encoder and \(f_{\theta}\) as the corresponding decoder, which parameterized elements of \(\mathcal{M}\). As everything is formulated in infinite dimension function space \(X\), we obtain discretization invariant methods.

## Appendix B Proof of Proposition 1 in Section 2

Proof.: We use the notation \(T|_{S(v,T+b)}(v)=(T_{i}v)_{i\in S(v,T+b)}\). Assume that \(T+b\) has a DSS with respect to every \(v\in L^{2}(D)^{n}\) in the sense of Definition 2, and that

\[\mathrm{ReLU}(Tv^{(1)}+b)=\mathrm{ReLU}(Tv^{(2)}+b)\quad\text{ in }D,\] (B.1)

where \(v^{(1)},v^{(2)}\in L^{2}(D)^{n}\). Since \(T+b\) has a DSS with respect to \(v^{(1)}\), we have for \(i\in S(v^{(1)},T+b)\)

\[0<\mathrm{ReLU}(T_{i}v^{(1)}+b_{i})=\mathrm{ReLU}(T_{i}v^{(2)}+b_{i})\text{ in }D,\]

which implies that

\[T_{i}v^{(1)}+b_{i}=T_{i}v^{(2)}+b_{i}\text{ in }D.\]

Thus,

\[v^{(1)}-v^{(2)}\in\mathrm{Ker}\left(T\big{|}_{S(v^{(1)},T+b)}\right).\] (B.2)

By assuming (B.1), we have for \(i\notin S(v^{(1)},T)\),

\[\{x\in D\mid T_{i}v^{(1)}(x)+b_{i}(x)\leq 0\}=\{x\in D\mid T_{i}v^{(2)}(x)+b_{i}(x) \leq 0\}.\]

Then, we have

\[T_{i}(v^{(1)}-(v^{(1)}-v^{(2)}))(x)+b_{i}(x)=T_{i}v^{(2)}(x)+b_{i}(x)\leq 0 \text{ if }T_{i}v^{(1)}(x)+b_{i}(x)\leq 0,\]

that is,

\[T_{i}v^{(1)}(x)+b_{i}(x)\leq T_{i}\left(v^{(1)}-v^{(2)}\right)(x)\text{ if }T_{i}v^{(1)}(x)+b_{i}(x)\leq 0.\]In addition,

\[T_{i}(v^{(1)}-v^{(2)})(x)=T_{i}v^{(1)}(x)+b_{i}(x)-\left(Tv^{(2)}(x)+b_{i}(x) \right)=0\text{ if }T_{i}v^{(1)}(x)+b_{i}(x)>0.\]

Thus,

\[v^{(1)}-v^{(2)}\in X(v,T+b).\] (B.3)

Combining (B.2) and (B.3), and (2.1) as \(v=v^{(1)}\), we conclude that

\[v^{(1)}-v^{(2)}=0.\]

Conversely, assume that there exists a \(v\in L^{2}(D)^{n}\) such that

\[\operatorname{Ker}\left(T\big{|}_{S(v,T+b)}\right)\cap X(v,T+b)\neq\{0\}.\]

Then there is \(u\neq 0\) such that

\[u\in\operatorname{Ker}\left(T\big{|}_{S(v,T+b)}\right)\cap X(v,T+b).\]

For \(i\in S(v,T+b)\), we have by \(u\in\operatorname{Ker}(T_{i})\),

\[\operatorname{ReLU}\left(T_{i}(v-u)+b_{i}(x)\right)=\operatorname{ReLU} \left(T_{i}v+b_{i}(x)\right).\]

For \(i\notin S(v,T+b)\), we have by \(u\in X(v,T+b)\),

\[\operatorname{ReLU}\left(T_{i}(v-u)(x)+b_{i}(x)\right) =\left\{\begin{array}{l}0\quad\text{if }T_{i}v(x)+b_{i}(x)\leq 0\\ T_{i}v(x)+b_{i}(x)\quad\text{if }T_{i}v(x)+b_{i}(x)>0\end{array}\right.\] \[=\operatorname{ReLU}\left(T_{i}v(x)+b_{i}(x)\right).\]

Therefore, we conclude that

\[\operatorname{ReLU}\left(T(v-u)+b\right)=\operatorname{ReLU}\left(Tv+b\right),\]

where \(u\neq 0\), that is, \(\operatorname{ReLU}\circ(T+b)\) is not injective. 

## Appendix C Details in Sections 3.1 and 3.2

### Proof of Lemma 1

Proof.: The restriction operator, \(\pi_{\ell}:L^{2}(D)^{m}\to L^{2}(D)^{\ell}\) (\(\ell<m\)), acts as follows,

\[\pi_{\ell}(a,b):=b,\quad(a,b)\in L^{2}(D)^{m-\ell}\times L^{2}(D)^{\ell}.\] (C.1)

Since \(L^{2}(D)\) is a separable Hilbert space, there exists an orthonormal basis \(\{\varphi_{k}\}_{k\in\mathbb{N}}\) in \(L^{2}(D)\). We denote by

\[\varphi_{k,j}^{0}:=\left(0,...,0,\underbrace{\varphi_{k}}_{j-th},0,...,0 \right)\in L^{2}(D)^{m},\]

for \(k\in\mathbb{N}\) and \(j\in[m-\ell]\). Then, \(\{\varphi_{k,j}^{0}\}_{k\in\mathbb{N},j\in[m-\ell]}\) is an orthonormal sequence in \(L^{2}(D)^{m}\), and

\[V_{0} :=L^{2}(D)^{m-\ell}\times\{0\}^{\ell}\] \[=\operatorname{span}\left\{\varphi_{k,j}^{0}\bigm{|}k\in\mathbb{N },\;j\in[m-\ell]\right\}.\]

We define, for \(\alpha\in(0,1)\),

\[\varphi_{k,j}^{\alpha}:=\left(0,...,0,\underbrace{\sqrt{(1-\alpha)}\varphi_{ k}}_{j-th},0,...,0,\sqrt{\alpha}\xi_{(k-1)(m-\ell)+j}\right)\in L^{2}(D)^{m},\] (C.2)

with \(k\in\mathbb{N}\) and \(j\in[m-\ell]\). We note that \(\{\varphi_{k,j}^{\alpha}\}_{k\in\mathbb{N},j\in[m-\ell]}\) is an orthonormal sequence in \(L^{2}(D)^{m}\). We set

\[V_{\alpha}:=\operatorname{span}\left\{\varphi_{k,j}^{\alpha}\bigm{|}k\in \mathbb{N},j\in[m-\ell]\right\}.\] (C.3)It holds for \(0<\alpha<1/2\) that

\[\left\|P_{V_{\alpha}^{\perp}}-P_{V_{\alpha}^{\perp}}\right\|_{\mathrm{op}}<1.\]

Indeed, for \(u\in L^{2}(D)^{m}\) and \(0<\alpha<1/2\),

\[\left\|P_{V_{\alpha}^{\perp}}u-P_{V_{0}^{\perp}}u\right\|_{L^{2}( D)^{m}}^{2}=\left\|P_{V_{\alpha}}u-P_{V_{0}}u\right\|_{L^{2}(D)^{m}}^{2}\] \[=\left\|\sum_{k\in\mathbb{N},j\in[m-\ell]}(u,\varphi_{k,j}^{ \alpha})\varphi_{k,j}^{\alpha}-(u,\varphi_{k,j}^{0})\varphi_{k,j}^{0}\right\|_ {L^{2}(D)^{m}}^{2}\] \[=\left\|\sum_{k\in\mathbb{N},j\in[m-\ell]}(1-\alpha)(u_{j}, \varphi_{k})\varphi_{k}-(u_{j},\varphi_{k})\varphi_{k}\right\|_{L^{2}(D)}^{2}\] \[+\left\|\sum_{k\in\mathbb{N},j\in[m-\ell]}\alpha(u_{m},\xi_{(k-1) (m-\ell)+j})\xi_{(k-1)(m-\ell)+j}\right\|_{L^{2}(D)}^{2}\] \[\leq\alpha^{2}\sum_{j\in[m-\ell]}\sum_{k\in\mathbb{N}}|(u_{j}, \varphi_{k})|^{2}+\alpha^{2}\sum_{k\in\mathbb{N}}|(u_{m},\xi_{k})|^{2}\leq 4 \alpha^{2}\left\|u\right\|_{L^{2}(D)^{m}}^{2},\]

which implies that \(\left\|P_{V_{\alpha}^{\perp}}-P_{V_{0}^{\perp}}\right\|_{\mathrm{op}}\leq 2\alpha\).

We will show that the operator

\[P_{V_{\alpha}^{\perp}}\circ T:L^{2}(D)^{n}\to L^{2}(D)^{m},\]

is injective. Assuming that for \(a,b\in L^{2}(D)^{n}\),

\[P_{V_{\alpha}^{\perp}}\circ T(a)=P_{V_{\alpha}^{\perp}}\circ T(b),\]

is equivalent to

\[T(a)-T(b)=P_{V_{\alpha}}(T(a)-T(b)).\]

Denoting by \(P_{V_{\alpha}}(T(a)-T(b))=\sum_{k\in\mathbb{N},j\in[m-\ell]}c_{k,j}\varphi_{k, j}^{\alpha}\),

\[\pi_{1}(T(a)-T(b))=\sum_{k\in\mathbb{N},j\in[m-\ell]}c_{k,j}\xi_{(k-1)(m-\ell )+j}.\]

From (3.1), we obtain that \(c_{kj}=0\) for all \(k,j\). By injectivity of \(T\), we finally get \(a=b\).

We define \(Q_{\alpha}:L^{2}(D)^{m}\to L^{2}(D)^{m}\) by

\[Q_{\alpha}:=\left(P_{V_{0}^{\perp}}P_{V_{\alpha}^{\perp}}+(I-P_{V_{0}^{\perp} })(I-P_{V_{\alpha}^{\perp}})\right)\left(I-(P_{V_{0}^{\perp}}-P_{V_{\alpha}^{ \perp}})^{2}\right)^{-1/2}.\]

By the same argument as in Section I.4.6 Kato [2013], we can show that \(Q_{\alpha}\) is injective and

\[Q_{\alpha}P_{V_{\alpha}^{\perp}}=P_{V_{0}^{\perp}}Q_{\alpha},\]

that is, \(Q_{\alpha}\) maps from \(\mathrm{Ran}(P_{V_{\alpha}^{\perp}})\) to

\[\mathrm{Ran}(P_{V_{0}^{\perp}})\subset\{0\}^{m-\ell}\times L^{2}(D)^{\ell}.\]

It follows that

\[\pi_{\ell}\circ Q_{\alpha}\circ P_{V_{\alpha}^{\perp}}\circ T:L^{2}(D)^{n} \to L^{2}(D)^{\ell}\]

is injective.

Remarks following Lemma 1

**Remark 3**.: _Lemma 1 and Eqn. (3.1) may be interpreted as saying that if some orthonormal sequence \(\{\xi_{k}\}_{k\in\mathbb{N}}\) exists that doesn't overlap the range of \(T\), then \(T\) may be embedded in a small space without losing injectivity. An example that satisfies (3.1) is the neural operator whose L-th layer operator \(\mathcal{L}_{L}\) consists of the integral operator \(K_{L}\) with continuous kernel function \(k_{L}\), and with continuous activation function \(\sigma\). Indeed, in this case, we may choose the orthogonal sequence \(\{\xi_{k}\}_{k\in\mathbb{N}}\) in \(L^{2}(D)\) as a discontinuous functions sequence 1 so that \(\mathrm{span}\{\xi_{k}\}_{k\in\mathbb{N}}\cap C(D)=\{0\}\). Then, by \(\mathrm{Ran}(\mathcal{L}_{L})\subset C(D)^{d_{L}}\), the assumption (3.1) holds._

Footnote 1: e.g., step functions whose supports are disjoint for each sequence.

**Remark 4**.: _In the proof of Lemma 1, an operator \(B\in\mathcal{L}(L^{2}(D)^{m},L^{2}(D)^{\ell})\),_

\[B=\pi_{\ell}\circ Q_{\alpha}\circ P_{V_{\alpha}^{\perp}},\]

_appears, where \(P_{V_{\alpha}^{\perp}}\) is the orthogonal projection onto orthogonal complement \(V_{\alpha}^{\perp}\) of \(V_{\alpha}\) with_

\[V_{\alpha}:=\mathrm{span}\left\{\varphi_{k,j}^{\alpha}\ \Big{|}\ k\in \mathbb{N},j\in[m-\ell]\right\}\subset L^{2}(D)^{m},\]

_in which \(\varphi_{k,j}^{\alpha}\) is defined for \(\alpha\in(0,1)\), \(k\in\mathbb{N}\) and \(j\in[\ell]\),_

\[\varphi_{k,j}^{\alpha}:=\Bigg{(}0,...,0,\underbrace{\sqrt{(1-\alpha)}\varphi_ {k}}_{j-th},0,...,0,\sqrt{\alpha}\xi_{(k-1)(m-\ell)+j}\Bigg{)}.\]

_Here, \(\{\varphi_{k}\}_{k\in\mathbb{N}}\) is an orthonormal basis in \(L^{2}(D)\). Futhermore, \(Q_{\alpha}:L^{2}(D)^{m}\to L^{2}(D)^{m}\) is defined by_

\[Q_{\alpha}:=\Big{(}P_{V_{0}^{\perp}}P_{V_{\alpha}^{\perp}}+(I-P_{V_{0}^{\perp }})(I-P_{V_{\alpha}^{\perp}})\Big{)}\left(I-(P_{V_{0}^{\perp}}-P_{V_{\alpha}^{ \perp}})^{2}\right)^{-1/2},\]

_where \(P_{V_{0}^{\perp}}\) is the orthogonal projection onto orthogonal complement \(V_{0}^{\perp}\) of \(V_{0}\) with_

\[V_{0}:=L^{2}(D)^{m-\ell}\times\{0\}^{\ell}.\]

_The operator \(Q_{\alpha}\) is well-defined for \(0<\alpha<1/2\) because it holds that_

\[\left\|P_{V_{\alpha}^{\perp}}-P_{V_{0}^{\perp}}\right\|_{\mathrm{op}}<2\alpha.\]

_This construction is given by the combination of "Pairs of projections" discussed in Kato (2013, Section I.4.6) with the idea presented in (Puthawala et al., 2022b, Lemma 29)._

### Proof of Theorem 1

We begin with

**Definition 3**.: _The set of \(L\)-layer neural networks mapping from \(\mathbb{R}^{d}\) to \(\mathbb{R}^{d^{\prime}}\) is_

\[\mathrm{N}_{L}(\sigma;\mathbb{R}^{d},\mathbb{R}^{d^{\prime}}):= \Big{\{}f:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d^{\prime}}\Big{|}f(x)=W_{L} \sigma(\cdots W_{1}\sigma(W_{0}x+b_{0})+b_{1}\cdots)+b_{L},\] \[W_{\ell}\in\mathbb{R}^{d_{\ell+1}\times d_{\ell}},b_{\ell}\in \mathbb{R}^{d_{\ell+1}},d_{\ell}\in\mathbb{N}_{0}(d_{0}=d,\ d_{L+1}=d^{\prime}), \ell=0,...,L\Big{\}},\]

_where \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\) is an element-wise nonlinear activation function. For the class of nonlinear activation functions,_

\[\mathrm{A}_{0}:=\Big{\{}\sigma\in C(\mathbb{R})\Big{|}\exists n\in\mathbb{N} _{0}\text{ s.t. }\mathrm{N}_{n}(\sigma;\mathbb{R}^{d},\mathbb{R})\text{ is dense in }C(K)\text{ for }\forall K\subset\mathbb{R}^{d}\text{ compact}\Big{\}}\]

\[\mathrm{A}_{0}^{L}:=\Big{\{}\sigma\in A_{0}\Big{|}\sigma\text{ is Borel measurable s.t. }\sup_{x\in\mathbb{R}}\frac{|\sigma(x)|}{1+|x|}<\infty\Big{\}}\]\[\mathrm{BA}:=\Big{\{}\sigma\in A_{0}\Big{|}\forall K\subset\mathbb{R}^{d}\text{ compact },\forall\epsilon>0,\text{ and }\forall C\geq\mathrm{diam}(K),\exists n\in\mathbb{N}_{0},\]

\[\exists f\in\mathrm{N}_{n}(\sigma;\mathbb{R}^{d},\mathbb{R}^{d})\text{ s.t. }|f(x)-x|\leq\epsilon,\;\forall x\in K,\text{ and, }|f(x)|\leq C,\;\forall x\in\mathbb{R}^{d}\Big{\}}.\]

_The set of integral neural operators with \(L^{2}\)-integral kernels is_

\[\mathrm{NO}_{L}(\sigma;D,d_{in},d_{out}):=\Big{\{}G:L^{2}(D)^{d_{ in}}\to L^{2}(D)^{d_{out}}\Big{|}\] (C.4) \[\quad G=K_{L+1}\circ(K_{L}+b_{L})\circ\sigma\cdots\circ(K_{2}+b_{ 2})\circ\sigma\circ(K_{1}+b_{1})\circ(K_{0}+b_{0}),\] \[\quad K_{\ell}\in\mathcal{L}(L^{2}(D)^{d_{\ell}},L^{2}(D)^{d_{ \ell+1}}),\;K_{\ell}:f\mapsto\int_{D}k_{\ell}(\cdot,y)f(y)dy\Big{|}_{D},\] \[\quad k_{\ell}\in L^{2}(D\times D;\mathbb{R}^{d_{\ell+1}\times d _{\ell}}),\;b_{\ell}\in L^{2}(D;\mathbb{R}^{d_{\ell+1}}),\] \[\quad d_{\ell}\in\mathbb{N},\;d_{0}=d_{in},\;d_{L+2}=d_{out},\; \ell=0,...,L+2\Big{\}}.\]

Proof.: Let \(R>0\) such that

\[K\subsetneq B_{R}(0),\]

where \(B_{R}(0):=\{u\in L^{2}(D)^{d_{in}}\mid\|u\|_{L^{2}(D)^{d_{in}}}\leq R\}\). By Theorem 11 of Kovachki et al. (2021b), there exists \(L\in\mathbb{N}\) and \(\widetilde{G}\in\mathrm{NO}_{L}(\sigma;D,d_{in},d_{out})\) such that

\[\sup_{a\in K}\Big{\|}G^{+}(a)-\widetilde{G}(a)\Big{\|}_{L^{2}(D)^{d_{out}}} \leq\frac{\epsilon}{2},\] (C.5)

and

\[\Big{\|}\widetilde{G}(a)\Big{\|}_{L^{2}(D)^{d_{out}}}\leq 4M,\quad\text{for }a\in L ^{2}(D)^{d_{in}},\quad\|a\|_{L^{2}(D)^{d_{in}}}\leq R.\]

We write operator \(\widetilde{G}\) by

\[\widetilde{G}=\widetilde{K}_{L+1}\circ(\widetilde{K}_{L}+\widetilde{b}_{L}) \circ\sigma\cdots\circ(\widetilde{K}_{2}+\widetilde{b}_{2})\circ\sigma\circ( \widetilde{K}_{1}+\widetilde{b}_{1})\circ(\widetilde{K}_{0}+\widetilde{b}_{0}),\]

where

\[\widetilde{K}_{\ell}\in\mathcal{L}(L^{2}(D)^{d_{\ell}},L^{2}(D)^{d_{ \ell+1}}),\;\widetilde{K}_{\ell}:f\mapsto\int_{D}\widetilde{k}_{\ell}(\cdot,y) f(y)dy,\] \[\widetilde{k}_{\ell}\in C(D\times D;\mathbb{R}^{d_{\ell+1}\times d _{\ell}}),\;\widetilde{b}_{\ell}\in L^{2}(D;\mathbb{R}^{d_{\ell+1}}),\] \[\quad d_{\ell}\in\mathbb{N},\;d_{0}=d_{in},\;d_{L+2}=d_{out},\; \ell=0,...,L+2.\]

We remark that kernel functions \(\widetilde{k}_{\ell}\) are continuous because neural operators defined in Kovachki et al. (2021b) parameterize the integral kernel function by neural networks, thus,

\[\mathrm{Ran}(\widetilde{G})\subset C(D)^{d_{out}}.\] (C.6)

We define the neural operator \(H:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{in}+d_{out}}\) by

\[H=K_{L+1}\circ(K_{L}+b_{L})\circ\sigma\cdots\circ(K_{2}+b_{2})\circ\sigma\circ (K_{1}+b_{1})\circ(K_{0}+b_{0}),\]

where \(K_{\ell}\) and \(b_{\ell}\) are defined as follows. First, we choose \(K_{inj}\in\mathcal{L}(L^{2}(D)^{d_{in}},L^{2}(D)^{d_{in}})\) as a linear injective integral operator 2.

Footnote 2: For example, if we choose the integral kernel \(k_{inj}\) as \(k_{inj}(x,y)=\sum_{k=1}^{\infty}\vec{\varphi}_{k}(x)\vec{\varphi}_{k}(y)\), then the integral operator \(K_{\mathrm{inj}}\) with the kernel \(k_{\mathrm{inj}}\) is injective where \(\{\vec{\varphi}\}_{k}\) is the orthonormal basis in \(L^{2}(D)^{d_{in}}\).

(i) When \(\sigma_{1}\in\mathrm{A}_{0}^{L}\cap\mathrm{BA}\) is injective,

\[K_{0}=\left(\begin{array}{c}K_{inj}\\ \widetilde{K}_{0}\end{array}\right)\in\mathcal{L}(L^{2}(D)^{d_{in}},L^{2}(D)^{d_ {in}+d_{1}}),\quad b_{0}=\left(\begin{array}{c}O\\ \widetilde{b}_{0}\end{array}\right)\in L^{2}(D)^{d_{in}+d_{1}},\]

\[\vdots\]\[K_{\ell}=\left(\begin{array}{cc}K_{inj}&O\\ O&\widetilde{K}_{\ell}\end{array}\right)\in\mathcal{L}(L^{2}(D)^{d_{in}+d_{\ell }},L^{2}(D)^{d_{in}+d_{\ell+1}}),\quad b_{\ell}=\left(\begin{array}{c}O\\ \widetilde{b}_{\ell}\end{array}\right)\in L^{2}(D)^{d_{in}+d_{\ell+1}},\]

\[(1\leq\ell\leq L),\]

\[\vdots\]

\[K_{L+1}=\left(\begin{array}{cc}K_{inj}&O\\ O&\widetilde{K}_{L+1}\end{array}\right)\in\mathcal{L}(L^{2}(D)^{d_{in}+d_{L+1 }},L^{2}(D)^{d_{in}+d_{out}}),\quad b_{\ell}=\left(\begin{array}{c}O\\ O\end{array}\right)\in L^{2}(D)^{d_{in}+d_{out}}.\]

(ii) When \(\sigma_{1}=\mathrm{ReLU}\),

\[K_{0}=\left(\begin{array}{c}K_{inj}\\ \widetilde{K}_{0}\end{array}\right)\in\mathcal{L}(L^{2}(D)^{d_{in}},L^{2}(D)^ {d_{in}+d_{1}}),\quad b_{0}=\left(\begin{array}{c}O\\ \widetilde{b}_{0}\end{array}\right)\in L^{2}(D)^{d_{in}+d_{1}},\]

\[K_{1}=\left(\begin{array}{cc}K_{inj}&O\\ -K_{inj}&O\\ O&\widetilde{K}_{1}\end{array}\right)\in\mathcal{L}(L^{2}(D)^{d_{in}+d_{1}}, L^{2}(D)^{2d_{in}+d_{2}}),\;b_{0}=\left(\begin{array}{c}O\\ O\\ \widetilde{b}_{1}\end{array}\right)\in L^{2}(D)^{2d_{in}+d_{1}},\]

\[K_{\ell}=\left(\begin{array}{cc}K_{inj}&-K_{inj}\\ -K_{inj}&K_{inj}\\ \widetilde{O}&\widetilde{K}_{\ell}\end{array}\right)\in\mathcal{L}(L^{2}(D)^ {2d_{in}+d_{\ell}},L^{2}(D)^{2d_{in}+d_{\ell+1}}),\]

\[b_{\ell}=\left(\begin{array}{c}O\\ \widetilde{b}_{\ell}\end{array}\right)\in L^{2}(D)^{2d_{in}+d_{\ell+1}},\quad( 2\leq\ell\leq L),\]

\[\vdots\]

\[K_{L}=\left(\begin{array}{cc}K_{inj}&-K_{inj}&\begin{array}{c}O\\ O&-\widetilde{b}_{-}&\begin{array}{c}O\\ \widetilde{b}_{\ell}\end{array}\end{array}\right)\in\mathcal{L}(L^{2}(D)^{2d_{ in}+d_{L}},L^{2}(D)^{d_{in}+d_{L+1}}),\]

\[b_{L}=\left(\begin{array}{c}O\\ \widetilde{b}_{L}\end{array}\right)\in L^{2}(D)^{d_{in}+d_{L+1}},\]

\[K_{L+1}=\left(\begin{array}{cc}K_{inj}&O\\ O&\widetilde{K}_{L+1}\end{array}\right)\in\mathcal{L}(L^{2}(D)^{d_{in}+d_{L+1 }},L^{2}(D)^{d_{in}+d_{out}}),\]

\[b_{L+1}=\left(\begin{array}{c}O\\ O\end{array}\right)\in L^{2}(D)^{d_{in}+d_{out}}.\]

Then, the operator \(H:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{in}+d_{out}}\) has the form of

\[H:=\left\{\begin{array}{cc}\left(\begin{array}{cc}K_{inj}\circ K_{inj} \circ\sigma\circ K_{inj}\circ\cdots\circ\sigma\circ K_{inj}\circ K_{inj}\\ \widetilde{G}\end{array}\right)&\mbox{ in the case of (i)}.\\ \left(\begin{array}{cc}K_{inj}\circ\cdots\circ K_{inj}\\ \widetilde{G}\end{array}\right)&\mbox{ in the case of (ii)}.\\ \end{array}\right)&\mbox{ in the case of (ii)}.\end{array}\right.\]

For the case of (ii), we have used the fact

\[(\begin{array}{cc}I&-I\end{array})\circ\mathrm{ReLU}\circ\left(\begin{array} []{c}I\\ -I\end{array}\right)=I.\]

Thus, in both cases, \(H\) is injective.

In the case of (i), as \(\sigma\in A_{0}^{L}\), we obtain the estimate

\[\left\|\sigma(f)\right\|_{L^{2}(D)^{d_{in}}}\leq\sqrt{2|D|d_{in}}C_{0}+\left\|f \right\|_{L^{2}(D)^{d_{in}}},\;f\in L^{2}(D)^{d_{in}},\]

where

\[C_{0}:=\sup_{x\in\mathbb{R}}\frac{|\sigma(x)|}{1+|x|}<\infty.\]Then we evaluate for \(a\in K(\subset B_{R}(0))\),

\[\begin{split}&\left\|H(a)\right\|_{L^{2}(D)^{d_{in}+d_{out}}}\\ &\leq\left\|\widetilde{G}(a)\right\|_{L^{2}(D)^{d_{out}}}+\left\| K_{inj}\circ K_{inj}\circ\sigma\circ K_{inj}\circ\cdots\circ\sigma\circ K_{inj} \circ K_{inj}(a)\right\|_{L^{2}(D)^{d_{in}}}\\ &\leq 4M+\sqrt{2|D|d_{in}}C_{0}\sum_{\ell=1}^{L}\left\|K_{inj} \right\|_{\mathrm{op}}^{\ell+1}+\left\|K_{inj}\right\|_{\mathrm{op}}^{L+2}R=:C _{H}.\end{split}\] (C.7)

In the case of (ii), we find the estimate, for \(a\in K\),

\[\left\|H(a)\right\|_{L^{2}(D)^{d_{in}+d_{out}}}\leq 4M+\left\|K_{inj}\right\|_{ \mathrm{op}}^{L+2}R<C_{H}.\] (C.8)

From (C.6) (especially, \(\mathrm{Ran}(\pi_{1}H)\subset C(D)\)) and Remark 3, we can choose an orthogonal sequence \(\{\xi_{k}\}_{k\in\mathbb{N}}\) in \(L^{2}(D)\) such that (3.1) holds. By applying Lemma 1, as \(T=H\), \(n=d_{in}\), \(m=d_{in}+d_{out}\), \(\ell=d_{out}\), we find that

\[G:=\underbrace{\pi_{d_{out}}\circ Q_{\alpha}\circ P_{V_{\alpha}^{\perp}}}_{=: B}\circ H:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{out}},\]

is injective. Here, \(P_{V_{\alpha}^{\perp}}\) and \(Q_{\alpha}\) are defined as in Remark 4; we choose \(0<\alpha<<1\) such that

\[\left\|P_{V_{\alpha}^{\perp}}-P_{V_{\alpha}^{\perp}}\right\|_{\mathrm{op}}< \min\left(\frac{\epsilon}{10C_{H}},1\right)=:\epsilon_{0},\]

where \(P_{V_{\alpha}^{\perp}}\) is the orthogonal projection onto

\[V_{0}^{\perp}:=\{0\}^{d_{in}}\times L^{2}(D)^{d_{out}}.\]

By the same argument as in the proof of Theorem 15 in Pithawala et al. (2022a), we can show that

\[\left\|I-Q_{\alpha}\right\|_{\mathrm{op}}\leq 4\epsilon_{0}.\]

Furthermore, since \(B\) is a linear operator, \(B\circ K_{L+1}\) is also a linear operator with integral kernel \(\left(Bk_{L+1}(\cdot,y)\right)(x)\), where \(k_{L+1}(x,y)\) is the kernel of \(K_{L+1}\). This implies that

\[G\in\mathrm{NO}_{L}(\sigma;D,d_{in},d_{out}).\]

We get, for \(a\in K\),

\[\left\|G^{+}(a)-G(a)\right\|_{L^{2}(D)^{d_{out}}}\leq\underbrace{\left\|G^{+} (a)-\widetilde{G}(a)\right\|_{L^{2}(D)^{d_{out}}}}_{(\text{C.S})\leq\frac{ \epsilon}{2}}+\left\|\widetilde{G}(a)-G(a)\right\|_{L^{2}(D)^{d_{out}}}.\] (C.9)

Using (C.7) and (C.8), we then obtain

\[\begin{split}&\left\|\widetilde{G}(a)-G(a)\right\|_{L^{2}(D)^{d_{ out}}}=\left\|\pi_{d_{out}}\circ H(a)-\pi_{d_{out}}\circ Q_{\alpha}\circ P _{V_{\alpha}^{\perp}}\circ H(a)\right\|_{L^{2}(D)^{d_{out}}}\\ &\leq\left\|\pi_{d_{out}}\circ(P_{V_{0}^{\perp}}-P_{V_{\alpha}^{ \perp}}+P_{V_{\alpha}^{\perp}})\circ H(a)-\pi_{d_{out}}\circ Q_{\alpha}\circ P _{V_{\alpha}^{\perp}}\circ H(a)\right\|_{L^{2}(D)^{d_{out}}}\\ &\leq\left\|\pi_{d_{out}}\circ(P_{V_{0}^{\perp}}-P_{V_{\alpha}^{ \perp}})\circ H(a)\right\|_{L^{2}(D)^{d_{out}}}+\left\|\pi_{d_{out}}\circ(I-Q_ {\alpha})\circ P_{V_{\alpha}^{\perp}}\circ H(a)\right\|_{L^{2}(D)^{d_{out}}}\\ &\leq 5\epsilon_{0}\left\|H(a)\right\|_{L^{2}(D)^{d_{in}+d_{out}}} \leq\frac{\epsilon}{2}.\end{split}\] (C.10)

Combining (C.9) and (C.10), we conclude that

\[\sup_{a\in K}\left\|G^{+}(a)-G(a)\right\|_{L^{2}(D)^{d_{out}}}\leq\frac{ \epsilon}{2}+\frac{\epsilon}{2}=\epsilon.\]

### Remark following Theorem 1

**Remark 5**.: _We make the following observations using Theorem 1:_

1. _ReLU and Leaky ReLU functions belong to_ \(\mathrm{A}_{Q}^{L}\cap\mathrm{BA}\) _due to the fact that_ \(\{\sigma\in C(\mathbb{R})\mid\sigma\text{ is not a polynomial}\}\subseteq A_{0}\) _(see Pinkus_ _[_1999_]__), and both the ReLU and Leaky ReLU functions belong to_ \(\mathrm{BA}\) _(see Lemma C.2 in Lanthaler et al._ [_2022_]__). We note that Lemma C.2 in Lanthaler et al._ [_2022_]_ _solely established the case for ReLU. However, it holds true for Leaky ReLU as well since the proof relies on the fact that the function_ \(x\mapsto\min(\max(x,R),R)\) _can be exactly represented by a two-layer ReLU neural network, and a two-layer Leaky ReLU neural network can also represent this function. Consequently, Leaky ReLU is one of example that satisfies (ii) in Theorem_ 1_._
2. _We emphasize that our infinite-dimensional result, Theorem_ 1_, deviates from the finite-dimensional result. Puthawala et al._ [_2022_, Theorem 15] _assumes that_ \(2d_{in}+1\leq d_{out}\) _due to the use of Whitney's theorem. In contrast, Theorem_ 1 _does not assume any conditions on_ \(d_{in}\) _and_ \(d_{out}\)_, that is, we are able to avoid invoking Whitney's theorem by employing Lemma_ 1_._
3. _We provide examples that injective universality does not hold when_ \(L^{2}(D)^{d_{in}}\) _and_ \(L^{2}(D)^{d_{out}}\) _are replaced by_ \(\mathbb{R}^{d_{in}}\) _and_ \(\mathbb{R}^{d_{out}}\)_: Consider the case where_ \(d_{in}=d_{out}=1\) _and_ \(G^{+}:\mathbb{R}\rightarrow\mathbb{R}\) _is defined as_ \(G^{+}(x)=\sin(x)\)_. We can not approximate_ \(G^{+}:\mathbb{R}\rightarrow\mathbb{R}\) _by an injective function_ \(G:\mathbb{R}\rightarrow\mathbb{R}\) _in the set_ \(K=[0,2\pi]\) _in the_ \(L^{\infty}\)_-norm. According to the topological degree theory (see Cho and Chen_ _[_2006_, Theorem 1.2.6(iii)]__), any continuous function_ \(G:\mathbb{R}\rightarrow\mathbb{R}\) _which satisfies_ \(\|G-G^{+}\|_{C([0,2\pi])}<\varepsilon\) _satisfies the equation on both intervals_ \(I_{1}=[0,\pi]\)_,_ \(I_{2}=[\pi,2\pi]\text{ deg}(G,I_{j},s)=\)_deg_\((G^{+},I_{j},s)=1\) _for all_ \(s\in[-1+\varepsilon,1-\varepsilon]\)_,_ \(j=1,2\)_. This implies that_ \(G:I_{j}\rightarrow\mathbb{R}\) _obtains the value_ \(s\in[-1+\varepsilon,1-\varepsilon]\) _at least once. Hence,_ \(G\) _obtains the values_ \(s\in[-1+\varepsilon,1-\varepsilon]\) _at least two times on the interval_ \([0,2\pi]\) _and is it thus not injective. It is worth noting that the degree theory exhibits significant differences between the infinite-dimensional and finite-dimensional cases_ _[_Cho and Chen_,_ 2006_]__)._

## Appendix D Details in Section 3.3

### Finite rank approximation

We consider linear integral operators \(K_{\ell}\) with \(L^{2}\) kernels \(k_{\ell}(x,y)\). Let \(\{\varphi_{k}\}_{k\in\mathbb{N}}\) be an orthonormal basis in \(L^{2}(D)\). Since \(\{\varphi_{k}(y)\varphi_{p}(x)\}_{k,p\in\mathbb{N}}\) is an orthonormal basis of \(L^{2}(D\times D)\), integral kernels \(k_{\ell}\in L^{2}(D\times D;\mathbb{R}^{d_{\ell+1}\times d_{\ell}})\) in integral operators \(K_{\ell}\in\mathcal{L}(L^{2}(D)^{d_{\ell}},L^{2}(D)^{d_{\ell+1}})\) has the expansion

\[k_{\ell}(x,y)=\sum_{k,p\in\mathbb{N}}C_{k,p}^{(\ell)}\varphi_{k}(y)\varphi_{p }(x),\]

then integral operators \(K_{\ell}\in\mathcal{L}(L^{2}(D)^{d_{\ell}},L^{2}(D)^{d_{\ell+1}})\) take the form

\[K_{\ell}u(x)=\sum_{k,p\in\mathbb{N}}C_{k,p}^{(\ell)}(u,\varphi_{k})\varphi_{p }(x),\ u\in L^{2}(D)^{d_{\ell}},\]

where \(C_{k,p}^{(\ell)}\in\mathbb{R}^{d_{\ell+1}\times d_{\ell}}\) whose \((i,j)\)-th component \(c_{k,p,ij}^{(\ell)}\) is given by

\[c_{k,p,ij}^{(\ell)}=(k_{\ell,ij},\varphi_{k}\varphi_{p})_{L^{2}(D\times D)}.\]

Here, we write \((u,\varphi_{k})\in\mathbb{R}^{d_{\ell}}\),

\[(u,\varphi_{k})=\left((u_{1},\varphi_{k})_{L^{2}(D)},...,(u_{d_{\ell}}, \varphi_{k})_{L^{2}(D)}\right).\]

We define \(K_{\ell,N}\in\mathcal{L}(L^{2}(D)^{d_{\ell}},L^{2}(D)^{d_{\ell+1}})\) as the truncated expansion of \(K_{\ell}\) by \(N\) finite sum, that is,

\[K_{\ell,N}u(x):=\sum_{k,p\leq N}C_{k,p}^{(\ell)}(u,\varphi_{k})\varphi_{p}(x).\]Then \(K_{\ell,N}\in\mathcal{L}(L^{2}(D)^{d_{\ell}},L^{2}(D)^{d_{\ell+1}})\) is a finite rank operator with rank \(N\). Furthermore, we have

\[\left\|K_{\ell}-K_{\ell,N}\right\|_{\mathrm{op}}\leq\left\|K_{\ell}-K_{\ell,N} \right\|_{\mathrm{HS}}=\left(\sum_{k,p\geq N}\sum_{i,j}|c_{k,p,ij}^{(\ell)}|^{2} \right)^{1/2},\] (D.1)

which implies that as \(N\rightarrow\infty\),

\[\left\|K_{\ell}-K_{\ell,N}\right\|_{\mathrm{op}}\to 0.\]

### Layerwise injectivity

We first revisit layerwise injectivity and bijectivity in the case of the finite rank approximation. Let \(K_{N}:L^{2}(D)^{n}\to L^{2}(D)^{m}\) be a finite rank operator defined by

\[K_{N}u(x):=\sum_{k,p\leq N}C_{k,p}(u,\varphi_{k})\varphi_{p}(x),\;u\in L^{2}( D)^{n},\]

where \(C_{k,p}\in\mathbb{R}^{m\times n}\) and \((u,\varphi_{p})\in\mathbb{R}^{n}\) is given by

\[(u,\varphi_{p})=\left((u_{1},\varphi_{p})_{L^{2}(D)},...,(u_{n},\varphi_{p})_ {L^{2}(D)}\right).\]

Let \(b_{N}\in L^{2}(D)^{n}\) be defined by

\[b_{N}(x):=\sum_{p\leq N}b_{p}\varphi_{p}(x),\]

in which \(b_{p}\in\mathbb{R}^{m}\). As analogues of Propositions 1 and 2, we obtain the following characterization.

**Proposition 5**.: _(i) The operator_

\[\mathrm{ReLU}\circ(K_{N}+b_{N}):(\mathrm{span}\{\varphi_{k}\}_{k\leq N})^{n} \to L^{2}(D)^{m},\]

_is injective if and only if for every \(v\in(\mathrm{span}\{\varphi_{k}\}_{k\leq N})^{n}\),_

\[\left\{u\in L^{2}(D)^{n}\;\middle|\;\vec{u}_{N}\in\mathrm{Ker}(C_{S,N})\right\} \cap X(v,K_{N}+b_{N})\cap(\mathrm{span}\{\varphi_{k}\}_{k\leq N})^{n}=\{0\}.\]

_where \(S(v,K_{N}+b_{N})\subset[m]\) and \(X(v,K_{N}+b_{N})\) are defined in Definition 2, and_

\[\vec{u}_{N}:=\left((u,\varphi_{p})\right)_{p\leq N}\in\mathbb{R}^{Nn},\;\;C_{S,N}:=\left(C_{k,q}\big{|}_{S(v,K_{N}+b_{N})}\right)_{k,q\in[N]}\in\mathbb{R}^{ N|S(v,K_{N}+b_{N})|\times Nn}.\] (D.2)

_(ii) Let \(\sigma\) be injective. Then the operator_

\[\sigma\circ(K_{N}+b_{N}):(\mathrm{span}\{\varphi_{k}\}_{k\leq N})^{n}\to L^{2} (D)^{m},\]

_is injective if and only if \(C_{N}\) is injective, where_

\[C_{N}:=\left(C_{k,q}\right)_{k,q\in[N]}\in\mathbb{R}^{Nm\times Nn}.\] (D.3)

Proof.: The above statements follow from Propositions 1 and 2 by observing that \(u\in\mathrm{Ker}\left(K_{N}\right)\) is equivalent to (cf. (D.2) and (D.3))

\[\sum_{k,p\leq N}C_{k,p}(u,\varphi_{k})\varphi_{p}=0,\iff C_{N}\vec{u}_{N}=0.\]

### Global injectivity

We revisit global injectivity in the case of finite rank approximation. As an analogue of Lemma 1, we have the following 

**Lemma 2**.: _Let \(N,N^{\prime}\in\mathbb{N}\) and \(n,m,\ell\in\mathbb{N}\) with \(N^{\prime}m>N^{\prime}\ell\geq 2Nn+1\), and let \(T:L^{2}(D)^{n}\to L^{2}(D)^{m}\) be a finite rank operator with \(N^{\prime}\) rank, that is,_

\[\mathrm{Ran}(T)\subset(\mathrm{span}\{\varphi_{k}\}_{k\leq N^{\prime}})^{m},\] (D.4)

_and Lipschitz continuous, and_

\[T:\left(\mathrm{span}\{\varphi_{k}\}_{k\leq N}\right)^{n}\to L^{2}(D)^{m},\]

_is injective. Then, there exists a finite rank operator \(B\in\mathcal{L}(L^{2}(D)^{m},L^{2}(D)^{\ell})\) with rank \(N^{\prime}\) such that_

\[B\circ T:(\mathrm{span}\{\varphi_{k}\}_{k\leq N})^{n}\to(\mathrm{span}\{ \varphi_{k}\}_{k\leq N^{\prime}})^{\ell},\]

_is injective._

Proof.: From (D.4), \(T:L^{2}(D)^{n}\to L^{2}(D)^{m}\) has the form of

\[T(a)=\sum_{k\leq N^{\prime}}(T(a),\varphi_{k})\varphi_{k},\]

where \((T(a),\varphi_{k})\in\mathbb{R}^{m}\). We define \(\mathbf{T}:\mathbb{R}^{Nn}\to\mathbb{R}^{N^{\prime}m}\) by

\[\mathbf{T}(\mathbf{a}):=\left((T(\mathbf{a}),\varphi_{k})\right)_{k\in[N^{ \prime}]}\in\mathbb{R}^{N^{\prime}m},\;\mathbf{a}\in\mathbb{R}^{Nn},\]

where \(T(\mathbf{a})\in L^{2}(D)^{m}\) is defined by

\[T(\mathbf{a}):=T\left(\sum_{k\leq N}a_{k}\varphi_{k}\right)\in L^{2}(D)^{m},\]

in which \(a_{k}\in\mathbb{R}^{n}\), \(\mathbf{a}=(a_{1},...,a_{N})\in\mathbb{R}^{Nn}\).

Since \(T:L^{2}(D)^{n}\to L^{2}(D)^{m}\) is Lipschitz continuous, \(\mathbf{T}:\mathbb{R}^{Nn}\to\mathbb{R}^{N^{\prime}m}\) is also Lipschitz continuous. As \(N^{\prime}m>N^{\prime}\ell\geq 2Nn+1\), we can apply Lemma 29 from Puthawala et al. [2022a] with \(D=N^{\prime}m,m=N^{\prime}\ell,n=Nn\). According to this lemma, there exists a \(N^{\prime}\ell\)-dimensional linear subspace \(\mathbf{V}^{\perp}\) in \(\mathbb{R}^{N^{\prime}m}\) such that

\[\left\|P_{\mathbf{V}^{\perp}}-P_{\mathbf{V}^{\perp}_{0}}\right\|_{\mathrm{op }}<1,\]

and

\[P_{\mathbf{V}^{\perp}}\circ\mathbf{T}:\mathbb{R}^{Nn}\to\mathbb{R}^{N^{\prime }m},\]

is injective, where \(\mathbf{V}^{\perp}_{0}=\{0\}^{N^{\prime}(m-\ell)}\times\mathbb{R}^{N^{\prime }\ell}\). Furthermore, in the proof of Theorem 15 of Puthawala et al. [2022a], denoting

\[\mathbf{B}:=\pi_{N^{\prime}\ell}\circ\mathbf{Q}\circ P_{\mathbf{V}^{\perp}} \in\mathbb{R}^{N^{\prime}\ell\times N^{\prime}m},\]

we are able to show that

\[\mathbf{B}\circ\mathbf{T}:\mathbb{R}^{Nn}\to\mathbb{R}^{N^{\prime}\ell}\]

is injective. Here, \(\pi_{N^{\prime}\ell}:\mathbb{R}^{N^{\prime}m}\to\mathbb{R}^{N^{\prime}\ell}\)

\[\pi_{N^{\prime}\ell}(a,b):=b,\quad(a,b)\in\mathbb{R}^{N^{\prime}(m-\ell)} \times\mathbb{R}^{N^{\prime}\ell},\]

where \(\mathbf{Q}:\mathbb{R}^{N^{\prime}m}\to\mathbb{R}^{N^{\prime}m}\) is defined by

\[\mathbf{Q}:=\left(P_{\mathbf{V}^{\perp}_{0}}P_{\mathbf{V}^{\perp}}+(I-P_{ \mathbf{V}^{\perp}_{0}})(I-P_{\mathbf{V}^{\perp}})\right)\left(I-(P_{\mathbf{ V}^{\perp}_{0}}-P_{\mathbf{V}^{\perp}})^{2}\right)^{-1/2}.\]

We define \(B:L^{2}(D)^{m}\to L^{2}(D)^{\ell}\) by

\[Bu=\sum_{k,p\leq N^{\prime}}\mathbf{B}_{k,p}(u,\varphi_{k})\varphi_{p},\]

where \(\mathbf{B}_{k,p}\in\mathbb{R}^{\ell\times m}\), \(\mathbf{B}=(\mathbf{B}_{k,p})_{k,p\in[N^{\prime}]}\). Then \(B:L^{2}(D)^{m}\to L^{2}(D)^{\ell}\) is a linear finite rank operator with \(N^{\prime}\) rank, and

\[B\circ T:L^{2}(D)^{n}\to L^{2}(D)^{\ell}\]

is injective because, by the construction, it is equivalent to

\[\mathbf{B}\circ\mathbf{T}:\mathbb{R}^{Nn}\to\mathbb{R}^{N^{\prime}\ell},\]

is injective.

### Proof of Theorem 2

**Definition 4**.: _We define the set of integral neural operators with \(N\) rank by_

\[\begin{split}&\operatorname{NO}_{L,N}(\sigma;D,d_{in},d_{out}):= \Big{\{}G_{N}:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{out}}:\\ & G_{N}=K_{L+1,N}\circ(K_{L,N}+b_{L,N})\circ\sigma\cdots\circ(K_ {2,N}+b_{2,N})\circ\sigma\circ(K_{1,N}+b_{1,N})\circ(K_{0,N}+b_{0,N}),\\ & K_{\ell,N}\in\mathcal{L}(L^{2}(D)^{d_{\ell}},L^{2}(D)^{d_{ \ell+1}}),\;K_{\ell,N}:f\mapsto\sum_{k,p\leq N}C^{(\ell)}_{k,p}(f,\varphi_{k}) \varphi_{p},\\ & b_{\ell,N}\in L^{2}(D;\mathbb{R}^{d_{\ell+1}}),\;b_{\ell,N}= \sum_{p\leq N}b^{(\ell)}_{p}\varphi_{m},\\ & C^{(\ell)}_{k,p}\in\mathbb{R}^{d_{\ell+1}\times d_{\ell}},\;b^{ (\ell)}_{p}\in\mathbb{R}^{d_{\ell+1}},\;k,p\leq N,\\ & d_{\ell}\in\mathbb{N},\;d_{0}=d_{in},\;d_{L+2}=d_{out},\;\ell= 0,...,L+2\Big{\}}.\end{split}\] (D.5)

Proof.: Let \(R>0\) such that

\[K\subsetneq B_{R}(0),\]

where \(B_{R}(0):=\{u\in L^{2}(D)^{d_{in}}\mid\|u\|_{L^{2}(D)^{d_{in}}}\leq R\}\). As ReLU and Leaky ReLU function belongs to \(\operatorname{A}_{0}^{L}\cap\operatorname{BA}\), by Theorem 11 of Kovachki et al. (2021), there exists \(L\in\mathbb{N}\) and \(\widetilde{G}\in\operatorname{NO}_{L}(\sigma;D,d_{in},d_{out})\) such that

\[\sup_{a\in K}\left\|G^{+}(a)-\widetilde{G}(a)\right\|_{L^{2}(D)^{d_{out}}}\leq \frac{\epsilon}{3}.\] (D.6)

and

\[\left\|\widetilde{G}(a)\right\|_{L^{2}(D)^{d_{out}}}\leq 4M,\quad\text{for}\;a \in L^{2}(D)^{d_{in}},\quad\left\|a\right\|_{L^{2}(D)^{d_{in}}}\leq R.\]

We write operator \(\widetilde{G}\) by

\[\widetilde{G}=\widetilde{K}_{L+1}\circ(\widetilde{K}_{L}+\widetilde{b}_{L}) \circ\sigma\cdots\circ(\widetilde{K}_{2}+\widetilde{b}_{2})\circ\sigma\circ( \widetilde{K}_{1}+\widetilde{b}_{1})\circ(\widetilde{K}_{0}+\widetilde{b}_{0}),\]

where

\[\begin{split}&\widetilde{K}_{\ell}\in\mathcal{L}(L^{2}(D)^{d_{ \ell}},L^{2}(D)^{d_{\ell+1}}),\;\widetilde{K}_{\ell}:f\mapsto\int_{D} \widetilde{k}_{\ell}(\cdot,y)f(y)dy,\\ &\widetilde{k}_{\ell}\in L^{2}(D\times D;\mathbb{R}^{d_{\ell+1} \times d_{\ell}}),\;\widetilde{b}_{\ell}\in L^{2}(D;\mathbb{R}^{d_{\ell+1}}), \\ & d_{\ell}\in\mathbb{N},\;d_{0}=d_{in},\;d_{L+2}=d_{out},\;\ell=0,...,L+2.\end{split}\]

We set \(\widetilde{G}_{N^{\prime}}\in\operatorname{NO}_{L,N^{\prime}}(\sigma;D,d_{in},d_{out})\) such that

\[\widetilde{G}_{N^{\prime}}=\widetilde{K}_{L+1,N^{\prime}}\circ(\widetilde{K} _{L,N^{\prime}}+\widetilde{b}_{L,N^{\prime}})\circ\sigma\cdots\circ(\widetilde {K}_{2,N^{\prime}}+\widetilde{b}_{2,N^{\prime}})\circ\sigma\circ(\widetilde{K} _{1,N^{\prime}}+\widetilde{b}_{1,N^{\prime}})\circ(\widetilde{K}_{0,N^{\prime }}+\widetilde{b}_{0,N^{\prime}}),\]

where \(\widetilde{K}_{\ell,N^{\prime}}:L^{2}(D)^{d_{\ell}}\to L^{2}(D)^{d_{\ell+1}}\) is defined by

\[\widetilde{K}_{\ell,N^{\prime}}u(x)=\sum_{k,p\leq N^{\prime}}C^{(\ell)}_{k,p} (u,\varphi_{k})\varphi_{p}(x),\]

where \(C^{(\ell)}_{k,p}\in\mathbb{R}^{d_{\ell+1}\times d_{\ell}}\) whose \((i,j)\)-th component \(c^{(\ell)}_{k,p,ij}\) is given by

\[c^{(\ell)}_{k,p,ij}=(\widetilde{k}_{\ell,ij},\varphi_{k}\varphi_{p})_{L^{2}(D \times D)}.\]

Since

\[\left\|\widetilde{K}_{\ell}-\widetilde{K}_{\ell,N^{\prime}}\right\|^{2}_{ \operatorname{op}}\leq\left\|\widetilde{K}_{\ell}-\widetilde{K}_{\ell,N^{ \prime}}\right\|^{2}_{\operatorname{HS}}=\sum_{k,p\geq N^{\prime}+1}\sum_{i,j} |c^{(\ell)}_{k,p,ij}|^{2}\to 0\text{ as }N^{\prime}\to\infty,\]

there is a large \(N^{\prime}\in\mathbb{N}\) such that

\[\sup_{a\in K}\left\|\widetilde{G}(a)-\widetilde{G}_{N^{\prime}}(a)\right\|_{L ^{2}(D)^{d_{out}}}\leq\frac{\epsilon}{3}.\] (D.7)Then, we have

\[\sup_{a\in K}\left\|\widetilde{G}_{N^{\prime}}(a)\right\|_{L^{2}(D)^{d_ {out}}} \leq\sup_{a\in K}\left\|\widetilde{G}_{N^{\prime}}(a)-\widetilde{G}(a) \right\|_{L^{2}(D)^{d_{out}}}+\sup_{a\in K}\left\|\widetilde{G}(a)\right\|_{L^{ 2}(D)^{d_{out}}}\] \[\leq 1+4M.\]

We define the operator \(H_{N^{\prime}}:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{in}+d_{out}}\) by

\[H_{N^{\prime}}(a)=\left(\begin{array}{c}H_{N^{\prime}}(a)_{1}\\ H_{N^{\prime}}(a)_{2}\end{array}\right):=\left(\begin{array}{c}K_{inj,N} \circ\cdots\circ K_{inj,N}(a)\\ \widetilde{G}_{N^{\prime}}(a)\end{array}\right),\]

where \(K_{inj,N}:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{in}}\) is defined by

\[K_{inj,N}u=\sum_{k\leq N}(u,\varphi_{k})\varphi_{k}.\]

As \(K_{inj,N}:(\mathrm{span}\{\varphi_{k}\}_{k\leq N})^{d_{in}}\to L^{2}(D)^{d_{in}}\) is injective,

\[H_{N^{\prime}}:(\mathrm{span}\{\varphi_{k}\}_{k\leq N})^{d_{in}}\to(\mathrm{ span}\{\varphi_{k}\}_{k\leq N})^{d_{in}}\times(\mathrm{span}\{\varphi_{k}\}_{k \leq N^{\prime}})^{d_{out}},\]

is injective. Furthermore, by the same argument (ii) (construction of \(H\)) in the proof of Theorem 1,

\[H_{N^{\prime}}\in NO_{L,N^{\prime}}(\sigma;D,d_{in},d_{out}),\]

because both of two-layer ReLU and Leaky ReLU neural networks can represent the identity map. Note that above \(K_{inj,N}\) is an orthogonal projection, so that \(K_{inj,N}\circ\cdots\circ K_{inj,N}=K_{inj,N}\). However, we write above \(H_{N^{\prime}}(a)_{1}\) as \(K_{inj,N}\circ\cdots\circ K_{inj,N}(a)\) so that it can be considered as combination of \((L+2)\) layers of neural networks.

We estimate that for \(a\in L^{2}(D)^{d_{in}}\), \(\|a\|_{L^{2}(D)^{d_{in}}}\leq R\),

\[\|H_{N^{\prime}}(a)\|_{L^{2}(D)^{d_{in}+d_{out}}}\leq 1+4M+\left\|K_{inj} \right\|_{\mathrm{op}}^{L+2}R=:C_{H}.\]

Here, we repeat an argument similar to the one in the proof of Lemma 2: \(H_{N^{\prime}}:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{in}+d_{out}}\) has the form of

\[H_{N^{\prime}}(a)=\left(\sum_{k\leq N}(H_{N^{\prime}}(a)_{1},\varphi_{k}) \varphi_{k},\;\sum_{k\leq N^{\prime}}(H_{N^{\prime}}(a)_{2},\varphi_{k}) \varphi_{k}\right).\]

where \((H_{N^{\prime}}(a)_{1},\varphi_{k})\in\mathbb{R}^{d_{in}}\), \((H_{N^{\prime}}(a)_{2},\varphi_{k})\in\mathbb{R}^{d_{out}}\). We define \(\mathbf{H}_{N^{\prime}}:\mathbb{R}^{Nd_{in}}\to\mathbb{R}^{Nd_{in}+N^{\prime} d_{out}}\) by

\[\mathbf{H}_{N^{\prime}}(\mathbf{a}):=\left[\left((H_{N^{\prime}}(\mathbf{a})_ {1},\varphi_{k})\right)_{k\in[N]},\;\left((H_{N^{\prime}}(\mathbf{a})_{2}, \varphi_{k})\right)_{k\in[N^{\prime}]}\right]\in\mathbb{R}^{Nd_{in}+N^{\prime} d_{out}},\;\mathbf{a}\in\mathbb{R}^{Nd_{in}},\]

where \(H_{N^{\prime}}(\mathbf{a})=(H_{N^{\prime}}(\mathbf{a})_{1},H_{N^{\prime}}( \mathbf{a})_{2})\in L^{2}(D)^{d_{in}+d_{out}}\) is defined by

\[H_{N^{\prime}}(\mathbf{a})_{1}:=H_{N^{\prime}}\left(\sum_{k\leq N}a_{k}\varphi _{k}\right)_{1}\in L^{2}(D)^{d_{in}},\]

\[H_{N^{\prime}}(\mathbf{a})_{2}:=H_{N^{\prime}}\left(\sum_{k\leq N^{\prime}}a_ {k}\varphi_{k}\right)_{2}\in L^{2}(D)^{d_{out}},\]

where \(a_{k}\in\mathbb{R}^{d_{in}}\), \(\mathbf{a}=(a_{1},...,a_{N})\in\mathbb{R}^{Nd_{in}}\). Since \(H_{N^{\prime}}:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{in}+d_{out}}\) is Lipschitz continuous, \(\mathbf{H}_{N^{\prime}}:\mathbb{R}^{Nd_{in}}\to\mathbb{R}^{N^{\prime}d_{out}}\) is also Lipschitz continuous. As

\[Nd_{in}+N^{\prime}d_{out}>N^{\prime}d_{out}\geq 2Nd_{in}+1,\]

we can apply Lemma 29 of Puthawala et al. (2022a) with \(D=Nd_{in}+N^{\prime}d_{out}\), \(m=N^{\prime}d_{out}\), \(n=Nd_{in}\). According to this lemma, there exists a \(N^{\prime}d_{out}\)-dimensional linear subspace \(\mathbf{V}^{\perp}\) in \(\mathbb{R}^{Nd_{in}+N^{\prime}d_{out}}\) such that

\[\left\|P_{\mathbf{V}^{\perp}}-P_{\mathbf{V}^{\perp}_{d}}\right\|_{op}<\min \left(\frac{\epsilon}{15C_{H_{N}}},\;1\right)=:\epsilon_{0}\]\[P_{\mathbf{V}^{\perp}}\circ\mathbf{H}_{N^{\prime}}:\mathbb{R}^{Nd_{in}}\to \mathbb{R}^{Nd_{in}+N^{\prime}d_{out}},\]

is injective, where \(\mathbf{V}_{0}^{\perp}=\{0\}^{Nd_{in}}\times\mathbb{R}^{N^{\prime}d_{out}}\). Furthermore, in the proof of Theorem 15 of Puthawala et al. (2022), denoting by

\[\mathbf{B}:=\pi_{N^{\prime}d_{out}}\circ\mathbf{Q}\circ P_{\mathbf{V}^{\perp}},\]

we can show that

\[\mathbf{B}\circ\mathbf{H}_{N^{\prime}}:\mathbb{R}^{Nd_{in}}\to\mathbb{R}^{N^{ \prime}d_{out}},\]

is injective, where \(\pi_{N^{\prime}d_{out}}:\mathbb{R}^{Nd_{in}+N^{\prime}d_{out}}\to\mathbb{R}^{N ^{\prime}d_{out}}\)

\[\pi_{N^{\prime}d_{out}}(a,b):=b,\quad(a,b)\in\mathbb{R}^{Nd_{in}}\times \mathbb{R}^{N^{\prime}d_{out}},\]

and \(\mathbf{Q}:\mathbb{R}^{Nd_{in}+N^{\prime}d_{out}}\to\mathbb{R}^{Nd_{in}+N^{ \prime}d_{out}}\) is defined by

\[\mathbf{Q}:=\left(P_{\mathbf{V}_{0}^{\perp}}P_{\mathbf{V}^{\perp}}+(I-P_{ \mathbf{V}_{0}^{\perp}})(I-P_{\mathbf{V}^{\perp}})\right)\left(I-(P_{\mathbf{ V}_{0}^{\perp}}-P_{\mathbf{V}^{\perp}})^{2}\right)^{-1/2}.\]

By the same argument in proof of Theorem 15 in Puthawala et al. (2022), we can show that

\[\left\|I-\mathbf{Q}\right\|_{\mathrm{op}}\leq 4\epsilon_{0}.\]

We define \(B:L^{2}(D)^{d_{in}+d_{out}}\to L^{2}(D)^{d_{out}}\)

\[Bu=\sum_{k,p\leq N^{\prime}}\mathbf{B}_{k,p}(u,\varphi_{k})\varphi_{p},\]

\(\mathbf{B}_{k,p}\in\mathbb{R}^{d_{out}\times(d_{in}+d_{out})}\), \(\mathbf{B}=(\mathbf{B}_{k,p})_{k,p\in[N^{\prime}]}\), then \(B:L^{2}(D)^{d_{in}+d_{out}}\to L^{2}(D)^{d_{out}}\) is a linear finite rank operator with \(N^{\prime}\) rank. Then,

\[G_{N^{\prime}}:=B\circ H_{N^{\prime}}:L^{2}(D)^{d_{in}}\to L^{2}(D)^{d_{out}},\]

is injective because by the construction, it is equivalent to

\[\mathbf{B}\circ\mathbf{H}_{N^{\prime}}:\mathbb{R}^{Nd_{in}}\to\mathbb{R}^{N^{ \prime}d_{out}},\]

is injective. Furthermore, we have

\[G_{N^{\prime}}\in NO_{L,N^{\prime}}(\sigma;D,d_{in},d_{out}).\]

Indeed, \(H_{N^{\prime}}\in NO_{L,N^{\prime}}(\sigma;D,d_{in},d_{out})\), \(B\) is the linear finite rank operator with \(N^{\prime}\) rank, and multiplication of two linear finite rank operators with \(N^{\prime}\) rank is also a linear finite rank operator with \(N^{\prime}\) rank.

Finally, we estimate for \(a\in K\),

\[\left\|G^{+}(a)-G_{N^{\prime}}(a)\right\|_{L^{2}(D)^{d_{out}}}\] (D.8) \[=\underbrace{\left\|G^{+}(a)-\widetilde{G}(a)\right\|_{L^{2}(D)^{ d_{out}}}}_{(D.6)\leq\frac{5}{3}}+\underbrace{\left\|\widetilde{G}(a)- \widetilde{G}_{N^{\prime}}(a)\right\|_{L^{2}(D)^{d_{out}}}}_{(D.7)\leq\frac{5} {3}}+\left\|\widetilde{G}_{N^{\prime}}(a)-G_{N^{\prime}}(a)\right\|_{L^{2}(D) ^{d_{out}}}.\]

Using notation \((a,\varphi_{k})\in\mathbb{R}^{d_{in}}\), and \(\mathbf{a}=((a,\varphi_{k}))_{k\in[N]}\in\mathbb{R}^{Nd_{in}},\) we further estimate for \(a\in K\),

\[\left\|\widetilde{G}_{N^{\prime}}(a)-G_{N^{\prime}}(a)\right\|_{L ^{2}(Q)^{d_{out}}}=\left\|\pi_{d_{out}}H_{N^{\prime}}(a)-B\circ H_{N^{\prime} }(a)\right\|_{L^{2}(Q)^{d_{out}}}\] (D.9) \[=\left\|\pi_{N^{\prime}d_{out}}\mathbf{H}_{N^{\prime}}(\mathbf{a })-\mathbf{B}\circ\mathbf{H}_{N^{\prime}}(\mathbf{a})\right\|_{2}\] \[=\left\|\pi_{N^{\prime}d_{out}}\circ\mathbf{H}_{N^{\prime}}( \mathbf{a})-\pi_{N^{\prime}d_{out}}\circ\mathbf{Q}\circ P_{\mathbf{V}^{\perp}} \circ\mathbf{H}_{N^{\prime}}(\mathbf{a})\right\|_{2}\] \[\leq\left\|\pi_{N^{\prime}d_{out}}\circ(P_{\mathbf{V}_{0}^{\perp}} -P_{\mathbf{V}^{\perp}})\circ\mathbf{H}_{N^{\prime}}(\mathbf{a})-\pi_{N^{ \prime}d_{out}}\circ\mathbf{Q}\circ P_{\mathbf{V}^{\perp}}\circ\mathbf{H}_{N^{ \prime}}(\mathbf{a})\right\|_{2}\] \[\leq\left\|\pi_{N^{\prime}d_{out}}\circ(P_{\mathbf{V}_{0}^{\perp}} -P_{\mathbf{V}^{\perp}})\circ\mathbf{H}_{N^{\prime}}(\mathbf{a})\right\|_{2}+ \left\|\pi_{N^{\prime}d_{out}}\circ(I-\mathbf{Q})\circ P_{\mathbf{V}^{\perp}} \circ\mathbf{H}_{N^{\prime}}(\mathbf{a})\right\|_{2}\] \[\leq 5\epsilon_{0}\underbrace{\left\|\mathbf{H}_{N^{\prime}}( \mathbf{a})\right\|_{2}}_{=\left\|H_{N^{\prime}}(a)\right\|_{L^{2}(D)^{d_{out}} }<C_{H}}\leq\frac{\epsilon}{3},\]

where \(\left\|\cdot\right\|_{2}\) is the Euclidean norm. Combining (D.8) and (D.9), we conclude that

\[\sup_{a\in K}\left\|G^{+}(a)-G_{N^{\prime}}(a)\right\|_{L^{2}(D)^{d_{out}}} \leq\frac{\epsilon}{3}+\frac{\epsilon}{3}+\frac{\epsilon}{3}=\epsilon.\]Details in Section 4.1

### Proof of Proposition 3

Proof.: Since \(W\) is bijective, and \(\sigma\) is surjective, it is enough to show that \(u\mapsto Wu+K(u)\) is surjective. We observe that for \(z\in L^{2}(D)^{n}\),

\[Wu+K(u)=z,\]

is equivalent to

\[H_{z}(u):=-W^{-1}K(u)+W^{-1}z=u.\]

We will show that \(H_{z}:L^{2}(D)^{n}\to L^{2}(D)^{n}\) has a fixed point for each \(z\in L^{2}(D)^{n}\). By the Leray-Schauder theorem, see Gilbarg and Trudinger (2001, Theorem 11.3), \(H:L^{2}(D)\to L^{2}(D)\) has a fixed point if the union \(\bigcup_{0<\lambda\leq 1}V_{\lambda}\) is bounded, where the sets

\[V_{\lambda} :=\{u\in L^{2}(D):\ u=\lambda H_{z}(u)\}\] \[=\{u\in L^{2}(D):\ \lambda^{-1}u=H_{z}(u)\}\] \[=\{u\in L^{2}(D):\ -\lambda^{-1}u=W^{-1}K(u)-W^{-1}z\},\]

are parametrized by \(0<\lambda\leq 1\).

As the map \(u\mapsto\alpha u+W^{-1}K(u)\) is coercive, there is an \(r>0\) such that for \(\|u\|_{L^{2}(D)^{n}}>r\),

\[\frac{\big{\langle}\alpha u+W^{-1}K(u),u\big{\rangle}_{L^{2}(D)^{n}}}{\|u\|_{ L^{2}(D)^{n}}}\geq\|W^{-1}z\|_{L^{2}(D)^{n}}.\]

Thus, we have that for \(\|u\|_{L^{2}(D)^{n}}>r\)

\[\frac{\big{\langle}W^{-1}K(u)-W^{-1}z,u\big{\rangle}_{L^{2}(D)^{n}} }{\|u\|_{L^{2}(D)^{n}}^{2}}\] \[\geq\frac{\big{\langle}\alpha u+W^{-1}K(u),u\big{\rangle}_{L^{2}( D)^{n}}-\big{\langle}\alpha u+W^{-1}z,u\big{\rangle}_{L^{2}(D)^{n}}}{\|u\|_{L^{2} (D)^{n}}^{2}}\] \[\geq\frac{\|W^{-1}z\|_{L^{2}(D)^{n}}}{\|u\|_{L^{2}(D)^{n}}}- \frac{\big{\langle}W^{-1}z,u\big{\rangle}_{L^{2}(D)^{n}}}{\|u\|_{L^{2}(D)^{n}} ^{2}}-\alpha\geq-\alpha>-1,\]

and, hence, for all \(\|u\|_{L^{2}(D)}>r_{0}\) and \(\lambda\in(0,1]\) we have \(u\not\in V_{\lambda}\). Thus

\[\bigcup_{\lambda\in(0,1]}V_{\lambda}\subset B(0,r_{0}).\]

Again, by the Leray-Schauder theorem (see Gilbarg and Trudinger (2001, Theorem 11.3)), \(H_{z}\) has a fixed point. 

### Examples for Proposition 3

**Example 2**.: _We consider the case where \(n=1\) and \(D\subset\mathbb{R}^{d}\) is a bounded interval. We consider the non-linear integral operator,_

\[K(u)(x):=\int_{D}k(x,y,u(x))u(y)dy,\ x\in D,\]

_and \(k(x,y,t)\) is bounded, that is, there is \(C_{K}>0\) such that_

\[|k(x,y,t)|\leq C_{K},\ x,y\in D,\ t\in\mathbb{R}.\]

_If \(\big{\|}W^{-1}\big{\|}_{\mathrm{op}}\) is small enough such that_

\[1>\big{\|}W^{-1}\big{\|}_{\mathrm{op}}\,C_{K}|D|,\]_then, for \(\alpha\in\left(\left\|W^{-1}\right\|_{\mathrm{op}}C_{K}|D|,1\right)\), \(u\mapsto\alpha u+W^{-1}K(u)\) is coercive. Indeed, we have for \(u\in L^{2}(D)\),_

\[\frac{\left\langle\alpha u+W^{-1}K(u),u\right\rangle_{L^{2}(D)}}{ \left\|u\right\|_{L^{2}(D)}}\] \[\geq\alpha\|u\|_{L^{2}(D)}-\left\|W^{-1}\right\|_{\mathrm{op}} \|K(u)\|_{L^{2}(D)}\geq\underbrace{\left(\alpha-\left\|W^{-1}\right\|_{ \mathrm{op}}C_{K}|D|\right)}_{>0}\|u\|_{L^{2}(D)}.\]

_For example, we can consider a kernel_

\[k(x,y,t)=\sum_{j=1}^{J}c_{j}(x,y)\sigma_{s}(a_{j}(x,y)t+b_{j}(x,y)),\]

_where \(\sigma_{s}:\mathbb{R}\rightarrow\mathbb{R}\) is the sigmoid function defined by_

\[\sigma_{s}(t)=\frac{1}{1+e^{-t}}.\]

_T are functions \(a,b,c\in C(\overline{D}\times\overline{D})\) such that_

\[\sum_{j=1}^{J}\|c_{j}\|_{L^{\infty}(D\times D)}<\left\|W^{-1} \right\|_{\mathrm{op}}^{-1}|D|^{-1}.\]

**Example 3**.: _Again, we consider the case where \(n=1\) and \(D\subset\mathbb{R}^{d}\) a is a bounded set. We assume that \(W\in C^{1}(\overline{D})\) satisfies \(0<c_{1}\leq W(x)\leq c_{2}\). For simplicity, we assume that \(|D|=1\). We consider the non-linear integral operator_

\[K(u)(x):=\int_{D}k(x,y,u(x))u(y)dy,\;x\in D,\] (E.1)

_where_

\[k(x,y,t)=\sum_{j=1}^{J}c_{j}(x,y)\sigma_{wire}(a_{j}(x,y)t+b_{j}( x,y)),\] (E.2)

_in which \(\sigma_{wire}:\mathbb{R}\rightarrow\mathbb{R}\) is the wavelet function defined by_

\[\sigma_{wire}(t)=\text{Im}\,(e^{i\omega t}e^{-t^{2}}),\]

_and \(a_{j},b_{j},c_{j}\in C(\overline{D}\times\overline{D})\) are such that the \(a_{j}(x,y)\) are nowhere vanishing functions, that is, \(a_{j}(x,y)\neq 0\) for all \(x,y\in\overline{D}\times\overline{D}\). the and its generalizations (e.g. activation functions which do decay only exponentially)._

_The next lemma holds for any activation function with exponential decay, including the activation function \(\sigma_{wire}\) and settles the key condition for Proposition 3 to hold._

**Lemma 3**.: _Assume that \(|D|=1\) and the activation function \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\) be continuous. Assume that there exists \(M_{1},m_{0}>0\) such that_

\[|\sigma(t)|\leq M_{1}e^{-m_{0}|t|},\quad t\in\mathbb{R}.\]

_Let \(a_{j},b_{j},c_{j}\in C(\overline{D}\times\overline{D})\) be such that \(a_{j}(x,y)\) are nowhere vanishing functions, that is, \(a_{j}(x,y)\neq 0\) for all \(x,y\in\overline{D}\times\overline{D}\). Moreover, let \(K:L^{2}(D)\to L^{2}(D)\) be a non-linear integral operator given in (E.1) with a kernel satisfying (E.2). Let \(\alpha>0\) and \(0<c_{0}\leq W(x)\leq c_{1}\). Then function \(F:L^{2}(D)\to L^{2}(D)\), \(F(u)=\alpha u+W^{-1}K(u)\) is coercive._

Proof.: As \(\overline{D}\) is compact, there is \(a_{0}>0\) such that for all \(j=1,2,\ldots,J\) we have \(|a_{j}(x,y)|\geq a_{0}\) a.e. and \(|b_{j}(x,y)|\leq b_{0}\) a.e. We point out that \(|\sigma(t)|\leq M_{1}\). Next,

\[(\sum_{j=1}^{J}\|W^{-1}c_{j}\|_{L^{\infty}(D\times D)})M_{1} \varepsilon<\frac{\alpha}{4},\] (E.3)we consider \(\lambda>0\) and \(u\in L^{2}(D)\) and the sets

\[D_{1}(\lambda) =\{x\in D:\ |u(x)|\geq\varepsilon\lambda\},\] \[D_{2}(\lambda) =\{x\in D:\ |u(x)|<\varepsilon\lambda\}.\]

Let \(\varepsilon>0\) be such that

\[(\sum_{j=1}^{J}\|W^{-1}c_{j}\|_{L^{\infty}(D\times D)})\cdot M_{1}\varepsilon< \frac{\alpha}{4}.\]

Then, for \(x\in D_{2}(\lambda)\),

\[\sum_{j=1}^{J}\|W^{-1}c_{j}\|_{L^{\infty}(D\times D)}|\sigma(a_{j }(x,y)u(x)+b_{j}(x,y))u(x)|\] \[\leq\sum_{j=1}^{J}\|W^{-1}c_{j}\|_{L^{\infty}(D\times D)}M_{1} \epsilon\lambda\underset{(E.3)}{\leq}\frac{\alpha}{4}\lambda.\]

After \(\varepsilon\) is chosen as in the above, we choose \(\lambda_{0}\geq\max(1,b_{0}/(a_{0}\varepsilon))\) to be sufficiently large so that for all \(|t|\geq\varepsilon\lambda_{0}\) it holds

\[(\sum_{j=1}^{J}\|W^{-1}c_{j}\|_{L^{\infty}(D\times D)})M_{1}\text{exp}(-m_{0} |a_{0}t-b_{0}|)t<\frac{\alpha}{4}.\]

Here, we observe that, as \(\lambda_{0}\geq b_{0}/(a_{0}\varepsilon)\), we have that for all \(|t|\geq\varepsilon\lambda_{0}\), \(a_{0}|t|-b_{0}>0\). Then, when \(\lambda\geq\lambda_{0}\), we have for \(x\in D_{1}(\lambda)\),

\[(\sum_{j=1}^{J}\|W^{-1}c_{j}\|_{L^{\infty}(D\times D)}\Big{|}\sigma\bigg{(}a_{ j}(x,y)u(x)+b_{j}(x,y)\bigg{)}u(x)\bigg{|}\leq\frac{\alpha}{4}.\]

When \(u\in L^{2}(D)\) has the norm \(\|u\|_{L^{2}(D)}=\lambda\geq\lambda_{0}\geq 1\), we have

\[\bigg{|}\int_{D}\int_{D}W(x)^{-1}k(x,y,u(x))u(x)u(y)dxdy\bigg{|}\] \[\leq \int_{D}\bigg{(}\int_{D_{1}}(\sum_{j=1}^{J}\|W^{-1}c_{j}\|_{L^{ \infty}(D\times D)})M_{1}\text{exp}\bigg{(}-m_{0}|a_{0}|u(x)|-b_{0}\bigg{)}|u (x)|dx\bigg{)}|u(y)|dy\] \[+\int_{D}\bigg{(}\int_{D_{2}}\sum_{j=1}^{J}\|W^{-1}c_{j}\|_{L^{ \infty}(D\times D)}|\sigma(a_{j}(x,y)u(x)+b_{j}(x,y))||u(x)|dx\bigg{)}|u(y)|dy\] \[\leq \frac{\alpha}{4}\|u\|_{L^{2}(D)}+\frac{\alpha}{4}\lambda\|u\|_{L^ {2}(D)}\] \[\leq \frac{\alpha}{2}\|u\|_{L^{2}(D)}^{2}.\]

Hence,

\[\frac{\big{\langle}\alpha u+W^{-1}K(u),u\big{\rangle}_{L^{2}(D)}}{\|u\|_{L^{2 }(D)}}\geq\frac{\alpha}{2}\|u\|_{L^{2}(D)},\]

and the function \(u\to\alpha u+W^{-1}K(u)\) is coercive. 

### Proof of Proposition 4

Proof.: (Injectivity) Assume that

\[\sigma(Wu_{1}+K(u_{1})+b)=\sigma(Wu_{2}+K(u_{2})+b).\]where \(u_{1},u_{2}\in L^{2}(D)^{n}\). Since \(\sigma\) is injective and \(W:L^{2}(D)^{n}\to L^{2}(D)^{n}\) is bounded linear bijective, we have

\[u_{1}+W^{-1}K(u_{1})=u_{2}+W^{-1}K(u_{2})=:z.\]

Since the mapping \(u\mapsto z-W^{-1}K(u)\) is contraction (because \(W^{-1}K\) is contraction), by the Banach fixed-point theorem, the mapping \(u\mapsto z-W^{-1}K(u)\) admit a unique fixed-point in \(L^{2}(D)^{n}\), which implies that \(u_{1}=u_{2}\).

(Surjectivity) Since \(\sigma\) is surjective, it is enough to show that \(u\mapsto Wu+K(u)+b\) is surjective. Let \(z\in L^{2}(D)^{n}\). Since the mapping \(u\mapsto W^{-1}z-W^{-1}b-W^{-1}K(u)\) is contraction, by Banach fixed-point theorem, there is \(u^{*}\in L^{2}(D)^{n}\) such that

\[u^{*}=W^{-1}z-W^{-1}b-W^{-1}K(u^{*})\iff Wu^{*}+K(u^{*})+b=z.\]

### Examples for Proposition 4

**Example 4**.: _We consider the case of \(n=1\), and \(D\subset[0,\ell]^{d}\). We consider Volterra operators_

\[K(u)(x)=\int_{D}k(x,y,u(x),u(y))u(y)dy,\]

_where \(x=(x_{1},\ldots,x_{d})\) and \(y=(y_{1},\ldots,y_{d})\). We recall that \(K\) is a Volterra operator if_

\[k(x,y,t,s)\neq 0\implies y_{j}\leq x_{j}\quad\text{for all $j=1,2,\ldots,d$.}\] (E.4)

_In particular, when \(D=(a,b)\subset\mathbb{R}\) is an interval, the Volterra operators are of the form_

\[K(u)(x)=\int_{a}^{x}k(x,y,u(x),u(y))u(y)dy,\]

_and if \(x\) is considered as a time variable, the Volterra operators are causal in the sense that the value of \(K(u)(x)\) at the time \(x\) depends only on \(u(y)\) at the times \(y\leq x\)._

_Assume that \(k(x,y,t,s)\in C(\overline{D}\times\overline{D}\times\mathbb{R}\times\mathbb{ R})\) is bounded and uniformly Lipschitz smooth in the \(t\) and \(s\) variables, that is, \(k\in C(\overline{D}\times\overline{D};C^{0,1}(\mathbb{R}\times\mathbb{R}))\)._

_Next, we consider the non-linear operator \(F:L^{2}(D)\to L^{2}(D)\),_

\[F(u)=u+K(u).\] (E.5)

_Assume that \(u,w\in L^{2}(D)\) are such that \(u+K(u)=w+K(w)\), so that \(w-u=K(u)-K(w)\). Next, we will show that then \(u=w\). We denote and \(D(z_{1})=D\cap([0,z_{1}]\times[0,\ell]^{d-1})\). Then for \(x\in D(z_{1})\) the Volterra property of the kernel implies that_

\[|u(x)-w(x)|\leq\int_{D}|k(x,y,u(x),u(y))u(y)-k(x,y,w(x),w(y))w(y)|dy\] \[\leq\int_{D(z_{1})}|k(x,y,u(x),u(y))u(y)-k(x,y,w(x),u(y))u(y)|dy\] \[\qquad+\int_{D(z_{1})}|k(x,y,w(x),u(y))u(y)-k(x,y,w(x),w(y))u(y)|dy\] \[\qquad+\int_{D(z_{1})}|k(x,y,w(x),w(y))u(y)-k(x,y,w(x),w(y))w(y)|dy\] \[\leq 2\|k\|_{C(\overline{D}\times\overline{D};C^{0,1}(\mathbb{R} \times\mathbb{R}))}\|u-w\|_{L^{2}(D(z_{1}))}\|u\|_{L^{2}(D(z_{1}))}\] \[\qquad\qquad+\|k\|_{L^{\infty}(D\times D\times\mathbb{R}\times \mathbb{R})}\|u-w\|_{L^{2}(D(z_{1}))}\sqrt{|D(z_{1})|},\]_so that for all \(0<z_{1}<\ell\),_

\[\|u-w\|_{L^{2}(D(z_{1}))}^{2}\] \[=\int_{0}^{z_{1}}\bigg{(}\int_{0}^{\ell}\!\!\dots\!\int_{0}^{\ell} \mathbf{1}_{D}(x)|u(x)-w(x)|^{2}dx_{d}dx_{d-1}\dots dx_{2}\bigg{)}dx_{1}\] \[\leq z_{1}\ell^{d-1}\bigg{(}2\|k\|_{C(\overline{D}\times \overline{D};C^{0,1}(\mathbb{R}\times\mathbb{R}))}\|u-w\|_{L^{2}(D(z_{1}))}\|u \|_{L^{2}(D(z_{1}))}\] \[\qquad\qquad\qquad+\|k\|_{L^{\infty}(D\times D\times\mathbb{R} \times\mathbb{R})}\|u-w\|_{L^{2}(D(z_{1}))}\sqrt{|D(z_{1})|}\bigg{)}^{2}\] \[\leq z_{1}\ell^{d-1}\bigg{(}\|k\|_{C(\overline{D}\times \overline{D};C^{0,1}(\mathbb{R}\times\mathbb{R}))}\|u\|_{L^{2}(D)}+\|k\|_{L \infty(D\times D\times\mathbb{R}\times\mathbb{R})}\sqrt{|D|}\bigg{)}^{2}\|u-w \|_{L^{2}(D(z_{1}))}^{2}.\]

_Thus, when \(z_{1}\) is so small that_

\[z_{1}\ell^{d-1}\bigg{(}\|k\|_{C(\overline{D}\times\overline{D}; C^{0,1}(\mathbb{R}^{n}))}\|u\|_{L^{2}(D)}+\|k\|_{L\infty(D\times D\times \mathbb{R}\times\mathbb{R})}\sqrt{|D|}\bigg{)}^{2}<1,\]

_we find that \(\|u-w\|_{L^{2}(D(z_{1}))}=0\), that is, \(u(x)-w(x)=0\) for \(x\in D(z_{1})\). Using the same arguments as above, we see for all \(k\in\mathbb{N}\) that that if \(u=w\) in \(D(kz_{1})\) then \(u=w\) in \(D((k+1)z_{1})\). Using induction, we see that \(u=w\) in \(D\). Hence, the operator \(u\mapsto F(u)\) is injective in \(L^{2}(D)\)._

**Example 5**.: _We consider derivatives of Volterra operators in the domain \(D\subset[0,\ell]^{d}\). Let \(K:L^{2}(D)\to L^{2}(D)\) be a non-linear operator_

\[K(u)=\int_{D}k(x,y,u(y))u(y)dy,\] (E.6)

_where \(k(x,y,t)\) satisfies (E.4), is bounded, and \(k\in C(\overline{D}\times\overline{D};C^{0,1}(\mathbb{R}\times\mathbb{R}))\). Let \(F_{1}:L^{2}(D)\to L^{2}(D)\) be_

\[F_{1}(u)=u+K(u).\] (E.7)

_Then the Frechet derivative of \(K\) at \(u_{0}\in L^{2}(D)\) to the direction \(w\in L^{2}(D)\) is_

\[DF_{1}|_{u_{0}}(w)=w(x)+\int_{D}k_{1}(x,y,u_{0}(y))w(y)dy,\] (E.8)

_where_

\[k_{1}(x,y,u_{0}(y))=u_{0}(y)\frac{\partial}{\partial t}k(x,y,t) \bigg{|}_{t=u_{0}(x)}+k(x,y,u_{0}(y))\] (E.9)

_is a Volterra operator satisfying_

\[k_{1}(x,y,t)\neq 0\implies y_{j}\leq x_{j}\quad\text{for all $j=1,2,\dots,d$.}\] (E.10)

_As seen in Example 4, the operator \(DF_{1}|_{u_{0}}:L^{2}(D)\to L^{2}(D)\) is injective._

## Appendix F Details in Section 4.2

In this appendix, we prove Theorem 3.We recall that in the theorem, weconsider the case when \(n=1\), and \(D\subset\mathbb{R}\) is a bounded interval, and the operator \(F_{1}\) is of the form

\[F_{1}(u)(x)=W(x)u(x)+\int_{D}k(x,y,u(y))u(y)dy,\]

where \(W\in C^{1}(\overline{D})\) satisfies \(0<c_{1}\leq W(x)\leq c_{2}\) and the function \((x,y,s)\mapsto k(x,y,s)\) is in \(C^{3}(\overline{D}\times\overline{D}\times\mathbb{R})\) and in \(\overline{D}\times\overline{D}\times\mathbb{R}\) its three derivatives and the derivatives of \(W\) are uniformly bounded by \(c_{0}\), that is,

\[\|k\|_{C^{3}(\overline{D}\times\overline{D}\times\mathbb{R})} \leq c_{0},\quad\|W\|_{C^{1}(\overline{D})}\leq c_{0}.\] (F.1)We recall that the identical embedding \(H^{1}(D)\to L^{\infty}(D)\) is bounded and compact by Sobolev's embedding theorem.

As we will consider kernels \(k(x,y,u_{0}(y))\), we will consider the non-linear operator \(F_{1}\) mainly as an operator in a Sobolev space \(H^{1}(D)\).

The Frechet derivative of \(F_{1}\) at \(u_{0}\) to direction \(w\), denoted \(A_{u_{0}}w=DF_{1}|_{u_{0}}(w)\) is given by

\[A_{u_{0}}w=W(x)w(x)+\int_{D}k(x,y,u_{0}(y))w(y)dy+\int_{D}u_{0}(y)\,\frac{ \partial k}{\partial u}(x,y,u_{0}(y))w(y)dy.\] (F.2)

The condition (F.1) implies that

\[F_{1}:H^{1}(D)\to H^{1}(D),\] (F.3)

is a locally Lipschitz smooth function and the operator

\[A_{u_{0}}:H^{1}(D)\to H^{1}(D),\]

given in (F.2), is defined for all \(u_{0}\in C(\overline{D})\) as a bounded linear operator.

When \(\mathcal{X}\) is a Banach space, let \(B_{\mathcal{X}}(0,R)=\{v\in\mathcal{X}:\;\|v\|_{\mathcal{X}}<R\}\) and \(\overline{B}_{\mathcal{X}}(0,R)=\{v\in\mathcal{X}:\|v\|_{\mathcal{X}}\leq R\}\) be the open and closed balls in \(\mathcal{X}\), respectively.

We consider the Holder spaces \(C^{n,\alpha}(\overline{D})\) and their image in (leaky) ReLU-type functions. Let \(a\geq 0\) and \(\sigma_{a}(s)=\operatorname{ReLU}(s)-a\operatorname{ReLU}(-s)\). We will consider the image of the closed ball of \(C^{1,\alpha}(\overline{D})\) in the map \(\sigma_{a}\), that is \(\sigma_{a}(\overline{B}_{C^{1,\alpha}(\overline{D})}(0,R))=\{\sigma_{a} \circ g\in C(\overline{D}):\;\|g\|_{C^{1,\alpha}(\overline{D})}\leq R\}\).

We will below assume that for all \(u_{0}\in C(\overline{D})\) the integral operator

\[A_{u_{0}}:H^{1}(D)\to H^{1}(D)\text{ is an injective operator}.\] (F.4)

This condition is valid when \(K(u)\) is a Volterra operator, see Examples 4 and 5. As the integral operators \(A_{u_{0}}\) are Fredholm operators having index zero, this implies that the operators (F.4) are bijective.

The inverse operator \(A_{u_{0}}^{-1}:H^{1}(D)\to H^{1}(D)\) can be written as

\[A_{u_{0}}^{-1}v(x)=\widetilde{W}(x)v(x)-\int_{D}\widetilde{k}_{u_{0}}(x,y)v(y )dy,\] (F.5)

where \(\widetilde{k}_{u_{0}},\partial_{x}\widetilde{k}_{u_{0}}\in C(\overline{D} \times\overline{D})\) and \(\widetilde{W}\in C^{1}(\overline{D})\).

We will consider the inverse function of the map \(F_{1}\) in a set \(\mathcal{Y}\subset\sigma_{a}(\overline{B}_{C^{1,\alpha}(\overline{D})}(0,R))\) that is a compact subset of the Sobolev space \(H^{1}(D)\). To this end, we will cover the set \(\mathcal{Y}\) with small balls \(B_{H^{1}(D)}(g_{j},\varepsilon_{0})\), \(j=1,2,\ldots,J\) of \(H^{1}(D)\), centred at \(g_{j}=F_{1}(v_{j})\), where \(v_{j}\in H^{1}(D)\). We will show below that when \(g\in B_{H^{1}(D)}(g_{j},2\varepsilon_{0})\), that is, \(g\) is \(2\varepsilon_{1}\)-close to the function \(g_{j}\) in \(H^{1}(D)\), the inverse map of \(F_{1}\) can be written as a limit \((F_{1}^{-1}(g),g)=\lim_{m\to\infty}\mathcal{H}_{j}^{\circ m}(v_{j},g)\) in \(H^{1}(D)^{2}\), where

\[\mathcal{H}_{j}\left(\begin{array}{c}u\\ g\end{array}\right)=\left(\begin{array}{c}u-A_{v_{j}}^{-1}(F_{1}(u)-F_{1}(v _{j}))+A_{v_{j}}^{-1}(g-g_{j})\\ g\end{array}\right),\]

that is, near \(g_{j}\) we can approximate \(F_{1}^{-1}\) as a composition \(\mathcal{H}_{j}^{\circ m}\) of \(2m\) layers of neural operators.

To glue the local inverse maps together, we use a partition of unity in the function space \(\mathcal{Y}\) that is given by integral neural operators

\[\Phi_{\vec{i}}(v,w)=\pi_{1}\circ\phi_{\vec{i},1}\circ\phi_{\vec{i},2}\circ \cdots\circ\phi_{\vec{i},\ell_{0}}(v,w),\quad\text{where}\quad\phi_{\vec{i}, \ell}(v,w)=(F_{y_{\ell},s(\vec{i},\ell),\epsilon_{1}}(v,w),w),\]

where \(\vec{i}\) belongs in some finite index set \(\mathcal{I}\subset\mathbb{Z}^{\ell_{0}}\), some \(\epsilon_{1}>0\), some \(y_{\ell}\in D\) (\(\ell=1,...,\ell_{0}\)), \(s(\vec{i},\ell)=i_{\ell}\epsilon_{1}.\pi_{1}(v,w)=v\) that maps a pair \((v,w)\) to the first function \(v\) Here, \(F_{z,s,h}(v,w)\) are integral neural operators with distributional kernels

\[F_{z,s,h}(v,w)(x)=\int_{D}k_{z,s,h}(x,y,v(x),w(y))dy,\]where \(k_{z,s,h}(x,y,v(x),w(y))=v(x)|_{[s-\frac{1}{2},h,s+\frac{1}{2}h)}(w(y))\delta(y-z)\), and \(\mathbf{1}_{A}\) is the indicator function of a set \(A\) and \(y\mapsto\delta(y-z)\) is the Dirac delta distribution at the point \(z\in D\). Using these, we can write the inverse of \(F_{1}\) at \(g\in\mathcal{Y}\) as

\[F_{1}^{-1}(g)=\lim_{m\to\infty}\sum_{\vec{i}\in\mathcal{I}}\Phi_{\vec{i}} \mathcal{H}_{\vec{j}(\vec{i})}^{\mathit{om}}\left(\begin{array}{c}v_{j(\vec{ i})}\\ g\end{array}\right),\] (F.6)

where \(j(\vec{i})\in\{1,2,\ldots,J\}\) are suitably chosen and the limit is taken in the norm topology of \(H^{1}(D)\). This result is summarized in the following theorem, that is a modified version of Theorem 3 for the inverse operator \(F_{1}^{-1}\) in (F.6) where we have refined the partition of unity \(\Phi_{\vec{i}}\) so that we use indexes \(\vec{i}\in\mathcal{I}\subset\mathbb{Z}^{\ell_{0}}\) instead of \(j\in\{1,\ldots,J\}\).

This result is summarized in following theorem:

**Theorem 4**.: _Assume that \(F_{1}\) satisfies the above assumptions (F.1) and (F.4) and that \(F_{1}:H^{1}(D)\to H^{1}(D)\) is a bijection. Let \(\mathcal{Y}\subset\sigma_{a}(\overline{B}_{C^{1,\alpha}(\overline{D})}(0,R))\) be a compact subset of the Sobolev space \(H^{1}(D)\), where \(\alpha>0\) and \(a\geq 0\). Then the inverse of \(F_{1}:H^{1}(D)\to H^{1}(D)\) in \(\mathcal{Y}\) can written as a limit (F.6) that is, as a limit of integral neural operators._

Observe that Theorem 4 includes the case where \(a=1\), in which case \(\sigma_{a}=Id\) and \(\mathcal{Y}\subset\sigma_{a}(\overline{B}_{C^{1,\alpha}(\overline{D})}(0,R))= \overline{B}_{C^{1,\alpha}(\overline{D})}(0,R))\). We note that when \(\sigma_{a}\) is a leaky ReLU-function with parameter \(a>0\), Theorem 4 can be applied to compute the inverse of \(\sigma_{a}\circ F_{1}\) that is given by \(F_{1}^{-1}\circ\sigma_{a}^{-1}\), where \(\sigma_{a}^{-1}=\sigma_{1/a}\). Note that the assumption that \(\mathcal{Y}\subset\sigma_{a}(\overline{B}_{C^{1,\alpha}(\overline{D})}(0,R))\) makes it possible to apply Theorem 4 in the case when one trains deep neural networks having layers \(\sigma_{a}\circ F_{1}\) and the parameter \(a\) of the leaky ReLU-function is a free parameter which is also trained.

Proof.: As the operator \(F_{1}\) can be multiplied by function \(W(x)^{-1}\), it is sufficient to consider the case when \(W(x)=1\).

Below, we use the fact that as \(D\subset\mathbb{R}\), Sobolev's embedding theorem yields that the embedding \(H^{1}(D)\to C(\overline{D})\) is bounded and there is \(C_{S}>0\) such that

\[\|u\|_{C(\overline{D})}\leq C_{S}\|u\|_{H^{1}(D)}.\] (F.7)

For clarity, we denote below the norm of \(C(\overline{D})\) by \(\|u\|_{L^{\infty}(D)}\).

Next we considre the Frechet derivatives of \(F_{1}\). We recall that the 1st Frechet derivative of \(F_{1}\) at \(u_{0}\) is the operator \(A_{u_{0}}\). The 2nd Frechet derivative of \(F_{1}\) at \(u_{0}\) to directions \(w_{1}\) and \(w_{2}\) is

\[D^{2}F_{1}|_{u_{0}}(w_{1},w_{2}) = \int_{D}2\frac{\partial k}{\partial u}(x,y,u_{0}(y))w_{1}(y)w_{2} (y)dy+\int_{D}u_{0}(y)\frac{\partial k^{2}}{\partial u^{2}}(x,y,u_{0}(y))w_{1} (y)w_{2}(y)dy\] \[= \int_{D}p(x,y)w_{1}(y)w_{2}(y)dy,\]

where

\[p(x,y) = 2\frac{\partial k}{\partial u}(x,y,u_{0}(y))+u_{0}(y)\frac{ \partial k^{2}}{\partial u^{2}}(x,y,u_{0}(y)),\] (F.8)

and

\[\frac{\partial}{\partial x}p(x,y) = 2\frac{\partial^{2}k}{\partial u\partial x}(x,y,u_{0}(y))+u_{0} (y)\frac{\partial k^{3}}{\partial u^{2}\partial x}(x,y,u_{0}(y)).\] (F.9)

Thus,

\[\|D^{2}F_{1}|_{u_{0}}(w_{1},w_{2})\|_{H^{1}(D)} \leq 3|D|^{1/2}\|k\|_{C^{3}(D\times D\times\mathbb{R})}(1+\|u_{0}\|_{L ^{\infty}(D)})\|w_{1}\|_{L^{\infty}(D)}\|w_{2}\|_{L^{\infty}(D)}.\]

When we freeze the function \(u\) in kernel \(k\) to be \(u_{0}\), we denote

\[K_{u_{0}}v(x) = \int_{D}k(x,y,u_{0}(y))v(y)dy,\]

**Lemma 4**.: _For \(u_{0},u_{1}\in C(\overline{D})\) we have_

\[\|K_{u_{1}}-K_{u_{0}}\|_{L^{2}(D)\to H^{1}(D)}\leq\|k\|_{C^{2}(D\times D \times\mathbb{R})}|D|\|u_{1}-u_{0}\|_{L^{\infty}(D)}.\]

_and_

\[\|A_{u_{1}}-A_{u_{0}}\|_{L^{2}(D)\to H^{1}(D)}\leq 2\|k\|_{C^{2}(D\times D \times\mathbb{R})}|D|(1+\|u_{0}\|_{L^{\infty}(D)})\|u_{1}-u_{0}\|_{L^{\infty}( D)}.\] (F.11)

Proof.: Denote

\[M_{u_{0}}v(x) = \int_{D}u_{0}(y)\,\frac{\partial k}{\partial u}(x,y,u_{0}(y))v(y)dy,\] \[N_{u_{1},u_{2}}v(x) = \int_{D}u_{1}(y)\,\frac{\partial k}{\partial u}(x,y,u_{2}(y))v(y)dy.\]

We have

\[M_{u_{2}}v-M_{u_{1}}v=(N_{u_{2},u_{2}}v-N_{u_{2},u_{1}}v)+(N_{u_{2},u_{1}}v-N_ {u_{1},u_{1}}v).\]

By Schur's test for continuity of integral operators,

\[\|K_{u_{0}}\|_{L^{2}(D)\to L^{2}(D)} \leq \bigg{(}\sup_{x\in D}\int_{D}|k(x,y,u_{0}(y))|dy\bigg{)}^{1/2} \bigg{(}\sup_{y\in D}\int_{D}|k(x,y,u_{0}(y))|dx\bigg{)}^{1/2}\] \[\leq \|k\|_{C^{0}(D\times D\times\mathbb{R})},\]

and

\[\|M_{u_{0}}\|_{L^{2}(D)\to L^{2}(D)}\] \[\leq \bigg{(}\sup_{x\in D}\int_{D}|u_{0}(y)\,\frac{\partial k}{ \partial u}(x,y,u_{0}(y))|dy\bigg{)}^{1/2}\bigg{(}\sup_{y\in D}\int_{D}|u_{0}( y)\,\frac{\partial k}{\partial u}(x,y,u_{0}(y))|dx\bigg{)}^{1/2}\] \[\leq \|k\|_{C^{1}(D\times D\times\mathbb{R})}\|u\|_{C(\overline{D})},\]

and

\[\|K_{u_{2}}-K_{u_{1}}\|_{L^{2}(D)\to L^{2}(D)}\] \[\leq \bigg{(}\sup_{x\in D}\int_{D}|k(x,y,u_{2}(y))-k(x,y,u_{1}(y))|dy \bigg{)}^{1/2}\] \[\qquad\qquad\times\bigg{(}\sup_{y\in D}\int_{D}|k(x,y,u_{2}(y))-k (x,y,u_{1}(y))|dx\bigg{)}^{1/2}\] \[\leq \bigg{(}\|k\|_{C^{1}(D\times D\times\mathbb{R})}\int_{D}|u_{2}(y)- u_{1}(y))|dy\bigg{)}^{1/2}\] \[\qquad\qquad\times\bigg{(}\|k\|_{C^{1}(D\times D\times\mathbb{R}) }\sup_{y\in D}\int_{D}|u_{2}(y)-u_{1}(y))|dx\bigg{)}^{1/2}\] \[\leq \|k\|_{C^{1}(D\times D\times\mathbb{R})}\bigg{(}\int_{D}|u_{2}(y)- u_{1}(y))|dy\bigg{)}^{1/2}\bigg{(}\sup_{y\in D}\int_{D}|u_{2}(y)-u_{1}(y))|dx \bigg{)}^{1/2}\] \[\leq \|k\|_{C^{1}(D\times D\times\mathbb{R})}\bigg{(}|D|^{1/2}\|u_{2}- u_{1}\|_{L^{2}(D)}^{1/2}\bigg{(}|D|\sup_{y\in D}|u_{2}(y)-u_{1}(y))|\bigg{)}^{1/2}\] \[\leq \|k\|_{C^{1}(D\times D\times\mathbb{R})}|D|^{3/4}\|u_{2}-u_{1}\|_ {L^{2}(D)}^{1/2}\|u_{2}-u_{1}\|_{L^{\infty}(D)}^{1/2}\] \[\leq \|k\|_{C^{1}(D\times D\times\mathbb{R})}|D|\|u_{2}-u_{1}\|_{L^{ \infty}(D)},\]\[\|N_{u_{2},u_{2}}-N_{u_{2},u_{1}}\|_{L^{2}(D)\to L^{2}(D)}\] \[\leq \biggl{(}\sup_{x\in D}\int_{D}|u_{2}(y)k(x,y,u_{2}(y))-u_{2}(y)k(x, y,u_{1}(y))|dy\biggr{)}^{1/2}\] \[\qquad\times\biggl{(}\sup_{y\in D}\int_{D}|u_{2}(y)k(x,y,u_{2}(y) )-u_{2}(y)k(x,y,u_{1}(y))|dx\biggr{)}^{1/2}\] \[\leq \|k\|_{C^{1}(D\times D\times\mathbb{R})}|D|^{3/4}\|u_{2}\|_{C^{0} (D)}\|u_{2}-u_{1}\|_{L^{2}(D)}^{1/2}\|u_{2}-u_{1}\|_{L^{\infty}(D)}^{1/2}\] \[\leq \|k\|_{C^{1}(D\times D\times\mathbb{R})}|D|\|u_{2}\|_{C^{0}(D)}\| u_{2}-u_{1}\|_{L^{\infty}(D)},\]

and

\[\|N_{u_{2},u_{1}}-N_{u_{1},u_{1}}\|_{L^{2}(D)\to L^{2}(D)}\] \[\leq \biggl{(}\sup_{x\in D}\int_{D}|(u_{2}(y)-u_{1}(y))k(x,y,u_{1}(y)) |dy\biggr{)}^{1/2}\] \[\qquad\times\biggl{(}\sup_{y\in D}\int_{D}|(u_{2}(y)-u_{1}(y))k( x,y,u_{1}(y))|dx\biggr{)}^{1/2}\] \[\leq \|k\|_{C^{0}(D\times D\times\mathbb{R})}|D|\|u_{2}-u_{1}\|_{L^{ \infty}(D)},\]

so that

\[\|M_{u_{2}}-M_{u_{1}}\|_{L^{2}(D)\to L^{2}(D)}\] \[\leq \|k\|_{C^{1}(D\times D\times\mathbb{R})}|D|(1+\|u_{2}\|_{C^{0}(D)} )\|u_{2}-u_{1}\|_{L^{\infty}(D)}.\]

Also, when \(D_{x}v=\frac{dv}{dx}\),

\[\|D_{x}\circ K_{u_{0}}\|_{L^{2}(D)\to L^{2}(D)}\] \[\leq \biggl{(}\sup_{x\in D}\int_{D}|D_{x}k(x,y,u_{0}(y))|dy\biggr{)}^{ 1/2}\biggl{(}\sup_{y\in D}\int_{D}|D_{x}k(x,y,u_{0}(y))|dx\biggr{)}^{1/2}\] \[\leq \|k\|_{C^{1}(D\times D\times\mathbb{R})},\]

and

\[\|D_{x}\circ K_{u_{1}}-D_{x}\circ K_{u_{0}}\|_{L^{2}(D)\to L^{2}(D)}\] \[\leq \biggl{(}\sup_{x\in D}\int_{D}|D_{x}k(x,y,u_{1}(y))-D_{x}k(x,y,u_{ 0}(y))|dy\biggr{)}^{1/2}\] \[\qquad\qquad\times\biggl{(}\sup_{y\in D}\int_{D}|D_{x}k(x,y,u_{1} (y))-D_{x}k(x,y,u_{0}(y))|dx\biggr{)}^{1/2}\] \[\leq \biggl{(}\|k\|_{C^{2}(D\times D\times\mathbb{R})}\int_{D}|u_{1}(y )-u_{0}(y))|dy\biggr{)}^{1/2}\] \[\qquad\qquad\times\biggl{(}\|k\|_{C^{2}(D\times D\times\mathbb{R} )}\sup_{y\in D}\int_{D}|u_{1}(y)-u_{0}(y))|dx\biggr{)}^{1/2}\] \[\leq \|k\|_{C^{2}(D\times D\times\mathbb{R})}\biggl{(}\int_{D}|u_{1}(y )-u_{0}(y))|dy\biggr{)}^{1/2}\biggl{(}\sup_{y\in D}\int_{D}|u_{1}(y)-u_{0}(y))| dx\biggr{)}^{1/2}\] \[\leq \|k\|_{C^{2}(D\times D\times\mathbb{R})}\biggl{(}|D|^{1/2}\|u_{1} -u_{0}\|_{L^{2}(D)}^{1/2}\biggr{)}^{1/2}\biggl{(}|D|\sup_{y\in D}|u_{1}(y)-u_{0} (y))|\biggr{)}^{1/2}\] \[\leq \|k\|_{C^{2}(D\times D\times\mathbb{R})}|D|^{3/4}\|u_{1}-u_{0}\|_ {L^{2}(D)}^{1/2}\|u_{1}-u_{0}\|_{L^{\infty}(D)}^{1/2}\] \[\leq \|k\|_{C^{2}(D\times D\times\mathbb{R})}|D|\|u_{1}-u_{0}\|_{L^{ \infty}(D)}^{\infty}.\]

Thus,

\[\|K_{u_{0}}\|_{L^{2}(D)\to H^{1}(D)} \leq \|k\|_{C^{1}(D\times D\times\mathbb{R})},\]\[\|M_{u_{0}}\|_{L^{2}(D)\to H^{1}(D)} \leq \|u_{0}\|_{C^{0}(D)}\|k\|_{C^{1}(D\times D\times\mathbb{R})},\]

and

\[\|K_{u_{1}}-K_{u_{0}}\|_{L^{2}(D)\to H^{1}(D)} \leq \|k\|_{C^{2}(D\times D\times\mathbb{R})}|D|\|u_{1}-u_{0}\|_{L^{ \infty}(D)}.\]

Similarly,

\[\|M_{u_{1}}-M_{u_{0}}\|_{L^{2}(D)\to H^{1}(D)} \leq \|k\|_{C^{2}(D\times D\times\mathbb{R})}|D|(1+\|u_{2}\|_{C^{0}(D )})\|u_{1}-u_{0}\|_{L^{\infty}(D)}.\]

As \(A_{u_{1}}=K_{u_{1}}+M_{u_{1}}\), the claim follows. 

As the embedding \(H^{1}(D)\to C(\overline{D})\) is bounded and has norm \(C_{S}\), Lemma 4 implies that for all \(R>0\) there is

\[C_{L}(R)=2\|k\|_{C^{2}(D\times D\times\mathbb{R})}|D|(1+C_{S}R),\]

such that the map,

\[u_{0}\mapsto DF_{1}|_{u_{0}},\quad u_{0}\in\overline{B}_{H^{1}}(0,R),\] (F.12)

is Lipschitz map \(\overline{B}_{H^{1}}(0,R)\to\mathcal{L}(H^{1}(D),H^{1}(D))\) with a Lipschitz constant \(C_{L}(R)\), that is,

\[\|DF_{1}|_{u_{1}}-DF_{1}|_{u_{2}}\|_{H^{1}(D)\to H^{1}(D)}\leq C_{L}(R)\|u_{1} -u_{2}\|_{H^{1}(D)}.\] (F.13)

As \(u_{0}\mapsto A_{u_{0}}=DF_{1}|_{u_{0}}\) is continuous, the inverse \(A_{u_{0}}^{-1}:H^{1}(D)\to H^{1}(D)\) exists for all \(u_{0}\in C(\overline{D})\), and the embedding \(H^{1}(D)\to C(\overline{D})\) is compact, we have that for all \(R>0\) there is \(C_{B}(R)>0\) such that

\[\|A_{u_{0}}^{-1}\|_{H^{1}(D)\to H^{1}(D)}\leq C_{B}(R),\quad\text{for all }u_{0}\in\overline{B}_{H^{1}}(0,R).\] (F.14)

Let \(R_{1},R_{2}>0\) be such that \(\mathcal{Y}\subset\overline{B}_{H^{1}}(0,R_{1})\) and \(X=F_{1}^{-1}(\mathcal{Y})\subset\overline{B}_{H^{1}}(0,R_{2})\). Below, we denote \(C_{L}=C_{L}(2R_{2})\) and \(C_{B}=C_{B}(R_{2})\).

Next we consider inverse of \(F_{1}\) in \(\mathcal{Y}\). To this end, let us consider \(\varepsilon_{0}>0\), which we choose later to be small enough. As \(\mathcal{Y}\subset\overline{B}_{H^{1}}(0,R)\) is compact there are finite number of elements \(g_{j}=F_{1}(v_{j})\in\mathcal{Y}\), where \(v_{j}\in X\), \(j=1,2,\dots,J\) such that

\[\mathcal{Y}\subset\bigcup_{j=1}^{J}B_{H^{1}(D)}(g_{j},\varepsilon_{0}).\]

We observe that for \(u_{0},u_{1}\in X\),

\[A_{u_{1}}^{-1}-A_{u_{0}}^{-1}=A_{u_{1}}^{-1}(A_{u_{1}}-A_{u_{0}})A_{u_{0}}^{-1},\]

and hence the Lipschitz constant of \(A^{-1}:u\mapsto A_{u}^{-1}\), \(X\to\mathcal{L}(H^{1}(D),H^{1}(D))\) satisfies

\[Lip(A^{-1})\leq C_{A}=C_{B}^{2}C_{L},\] (F.15)

see (F.11).

Let us consider a fixed \(j\) and \(g_{j}\in\mathcal{Y}\). When \(g\) satisfies

\[\|g-g_{j}\|_{H^{1}(D)}<2\varepsilon_{0},\] (F.16)

the equation

\[F_{1}(u)=g,\quad u\in X,\]

is equivalent to the fixed point equation

\[u=u-A_{v_{j}}^{-1}(F_{1}(u)-F_{1}(v_{j}))+A_{v_{j}}^{-1}(g-g_{j}),\]

that is equivalent to the fixed point equation

\[H_{j}(u)=u,\]for the function \(H_{j}:H^{1}(D)\to H^{1}(D)\),

\[H_{j}(u)=u-A_{v_{j}}^{-1}(F_{1}(u)-F_{1}(v_{j}))+A_{v_{j}}^{-1}(g-g_{j}).\]

Note that \(H_{j}\) depends on \(g\), and thus we later denote \(H_{j}=H_{j}^{g}\). We observe that

\[H_{j}(v_{j})=v_{j}+A_{v_{j}}^{-1}(g-g_{j}).\] (F.17)

Let \(u,v\in\overline{B}_{H^{1}}(0,2R_{2})\). We have

\[F_{1}(u)=F_{1}(v)+A_{v}(u-v)+B_{v}(u-v),\quad\|B_{v}(u-v)\|\leq C _{0}\|u-v\|^{2},\]

where, see (F.10),

\[C_{0}=3|D|^{1/2}\|k\|_{C^{3}(D\times D\times\mathbb{R})}(1+2C_{S }R_{2})C_{S}^{2},\]

so that for \(u_{1},u_{2}\in\overline{B}_{H^{1}}(0,2R_{2})\),

\[u_{1}-u_{2}-A_{v_{j}}^{-1}(F_{1}(u_{1})-F_{1}(u_{2}))\] \[= u_{1}-u_{2}-A_{u_{2}}^{-1}(F_{1}(u_{1})-F_{1}(u_{2}))-(A_{u_{2}} ^{-1}-A_{v_{j}}^{-1})(F_{1}(u_{1})-F_{1}(u_{2})),\]

and

\[\|u_{1}-u_{2}-A_{u_{2}}^{-1}(F_{1}(u_{1})-F_{1}(u_{2}))\|_{H^{1}( D)}\] \[= \|A_{u_{2}}^{-1}(B_{u_{2}}(u_{1}-u_{2}))\|_{H^{1}(D)}\] \[\leq \|A_{u_{2}}^{-1}\|_{H^{1}(D)\to H^{1}(D)}\|B_{u_{2}}(u_{1}-u_{2})\|_{H^ {1}(D)}\] \[\leq \|A_{u_{2}}^{-1}\|_{H^{1}(D)\to H^{1}(D)}C_{0}\|u_{1}-u_{2}\|_{H^ {1}(D)}^{2},\] \[\leq C_{B}C_{0}\|u_{1}-u_{2}\|_{H^{1}(D)}^{2},\]

and

\[\|(A_{u_{2}}^{-1}-A_{v_{j}}^{-1})(F_{1}(u_{1})-F_{1}(u_{2}))\|_{H ^{1}(D)}\] \[\leq \|A_{u_{2}}^{-1}-A_{v_{j}}^{-1}\|_{H^{1}(D)\to H^{1}(D)}\|F_{1}(u_{1})- F_{1}(u_{2})\|_{H^{1}(D)}\] \[\leq Lip_{\overline{B}_{H^{1}}(0,2R_{2})\to H^{1}(D)}(A_{-}^{-1})\|u_{2 }-v_{j}\|Lip_{\overline{B}_{H^{1}}(0,2R_{2})\to H^{1}(D)}(F_{1})\|u_{2}-u_{1}\| _{H^{1}(D)}\] \[\leq C_{A}\|u_{2}-v_{j}\|(C_{B}+4C_{0}R_{2})\|u_{2}-u_{1}\|_{H^{1}(D)},\]

see (F.2), and hence, when \(\|u-v_{j}\|\leq r\leq R_{2}\),

\[\|H_{j}(u_{1})-H_{j}(u_{2})\|_{H^{1}(D)}\] \[\leq \|u_{1}-u_{2}-A_{v_{j}}^{-1}(F_{1}(u_{1})-F_{1}(u_{2}))\|_{H^{1}( D)}\] \[\leq \|u_{1}-u_{2}-A_{u_{2}}^{-1}(F_{1}(u_{1})-F_{1}(u_{2}))\|_{H^{1}( D)}+\|(A_{u_{2}}^{-1}-A_{v_{j}}^{-1})(F_{1}(u_{1})-F_{1}(u_{2}))\|_{H^{1}(D)}\] \[\leq \bigg{(}C_{B}C_{0}(\|u_{1}-v_{j}\|_{H^{1}(D)}+\|u_{2}-v_{j}\|_{H^ {1}(D)})+C_{A}(C_{B}+4C_{0}R_{2})\|u_{2}-v_{j}\|\bigg{)}\|u_{2}-u_{1}\|_{H^{1}( D)}\] \[\leq C_{H}r\|u_{2}-u_{1}\|_{H^{1}(D)},\]

where

\[C_{H}=2C_{B}C_{0}+C_{A}(C_{B}+4C_{0}R_{2}).\]

We now choose

\[r=\min(\frac{1}{2C_{H}},R_{2}).\]

We consider

\[\varepsilon_{0}\leq\frac{1}{8C_{B}}\frac{1}{2C_{H}}.\]

Then, we have

\[r\geq 2C_{B}\varepsilon_{0}/(1-C_{H}r).\]Then, we have that \(\text{Lip}_{\overline{B}_{H^{1}}(0,2R_{2})\to H^{1}(D)}(H_{j})\leq a=C_{H}r<\frac{1}{2}\), and

\[r\geq\|A_{v_{j}}^{-1}\|_{H^{1}(D)\to H^{1}(D)}\|g-g_{j}\|_{H^{1}(D)}/(1-a),\]

and for all \(u\in\overline{B}_{H^{1}}(0,R_{2})\) such that \(\|u-v_{j}\|\leq r\), we have \(\|A_{v_{j}}^{-1}(g-g_{j})\|_{H^{1}(D)}\leq(1-a)r\). Then,

\[\|H_{j}(u)-v_{j}\|_{H^{1}(D)} \leq \|H_{j}(u)-H_{j}(v_{j})\|_{H^{1}(D)}+\|H_{j}(v_{j})-v_{j}\|_{H^{1} (D)}\] \[\leq a\|u-v_{j}\|_{H^{1}(D)}+\|v_{j}+A_{v_{j}}^{-1}(g-g_{j})-v_{j}\|_{H ^{1}(D)}\] \[\leq ar+\|A_{v_{j}}^{-1}(g-g_{j})\|_{H^{1}(D)}\leq r,\]

that is, \(H_{j}\) maps \(\overline{B}_{H^{1}(D)}(v_{j},r)\) to itself. By Banach fixed point theorem, \(H_{j}:\overline{B}_{H^{1}(D)}(v_{j},r)\to\overline{B}_{H^{1}(D)}(v_{j},r)\) has a fixed point.

Let us denote

\[\mathcal{H}_{j}\left(\begin{array}{c}u\\ g\end{array}\right)=\left(\begin{array}{c}H_{j}^{g}(u)\\ g\end{array}\right)=\left(\begin{array}{c}u-A_{v_{j}}^{-1}(F_{1}(u)-F_{1}(v_ {j}))+A_{v_{j}}^{-1}(g-g_{j})\\ g\end{array}\right).\]

By the above, when we choose \(\varepsilon_{0}\) to have a value

\[\varepsilon_{0}<\frac{1}{8C_{B}}\frac{1}{2C_{H}},\]

the map \(F_{1}\) has a right inverse map \(\mathcal{R}_{j}\) in \(B_{H^{1}}(g_{j},2\varepsilon_{0})\), that is,

\[F_{1}(\mathcal{R}_{j}(g))=g,\quad\text{for }g\in B_{H^{1}}(g_{j},2\varepsilon_{0}),\] (F.18)

it holds that \(\mathcal{R}_{j}:B_{H^{1}}(g_{j},2\varepsilon_{0})\to\overline{B}_{H^{1}(D)}(v _{j},r)\), and by Banach fixed point theorem it is given by the limit

\[\mathcal{R}_{j}(g)=\lim_{m\to\infty}w_{j,m},\quad g\in B_{H^{1}}(g_{j},2 \varepsilon_{0}),\] (F.19)

in \(H^{1}(D)\), where

\[w_{j,0}=v_{j},\] (F.20) \[w_{j,m+1}=H_{j}^{g}(w_{j,m}).\] (F.21)

We can write for \(g\in B_{H^{1}}(g_{j},2\varepsilon_{0})\),

\[\left(\begin{array}{c}\mathcal{R}_{j}(g)\\ g\end{array}\right)=\lim_{m\to\infty}\mathcal{H}_{j}^{\circ m}\left(\begin{array} []{c}v_{j}\\ g\end{array}\right),\]

where the limit takes space in \(H^{1}(D)^{2}\) and

\[\mathcal{H}_{j}^{\circ m}=\mathcal{H}_{j}\circ\mathcal{H}_{j}\circ\cdots \circ\mathcal{H}_{j},\] (F.22)

is the composition of \(m\) operators \(\mathcal{H}_{j}\). This implies that \(\mathcal{R}_{j}\) can be written as a limit of finite iterations of neural operators \(H_{j}\) (we will consider how the operator \(A_{v_{j}}^{-1}\) can be written as a neural operator below).

As \(\mathcal{Y}\subset\sigma_{a}(\overline{B}_{C^{1,\alpha}(\overline{D})}(0,R))\), there are finite number of points \(y_{\ell}\in D\), \(\ell=1,2,\ldots,\ell_{0}\) and \(\varepsilon_{1}>0\) such that the sets

\[Z(i_{1},i_{2},\ldots,i_{\ell_{0}})=\{g\in\mathcal{Y}:\;(i_{\ell}-\frac{1}{2}) \varepsilon_{1}\leq g(y_{\ell})<(i_{\ell}+\frac{1}{2})\varepsilon_{1},\;\text{ for all }\ell\},\]

where \(i_{1},i_{2},\ldots,i_{\ell_{0}}\in\mathbb{Z}\), satisfy the condition

\[\text{If }(Z(i_{1},i_{2},\ldots,i_{\ell_{0}})\cap\mathcal{Y})\cap B_{H^{1}(D)}(g_{j },\varepsilon_{0})\neq\emptyset\text{ then }Z(i_{1},i_{2},\ldots,i_{\ell_{0}})\cap\mathcal{Y}\subset B_{H^{1}(D)}(g_{j},2 \varepsilon_{0}).\]

To show (F.23), we will below use the mean value theorem for function \(g=\sigma_{a}\circ v\in\mathcal{Y}\), where \(v\in C^{1,\alpha}(\overline{D}).\) First, let us consider the case when the parameter \(a\) of the leaky ReLU function \(\sigma_{a}\) is strictly positive. Without loss of generality, we can assume that \(D=[0,1]\) and \(y_{\ell}=h\ell\), where \(h=1/\ell_{0}\) and \(\ell=0,1,\ldots,\ell_{0}\). We consider \(g\in\mathcal{Y}\cap Z(i_{1},i_{2},\ldots,i_{\ell_{0}})\subset\sigma_{a}(\overline{ B}_{C^{1,\alpha}(\overline{D})}(0,R))\) of the form \(g=\sigma_{a}\circ v\). As \(a\) is non-zero, the inequality \((i_{\ell}-\frac{1}{2})\varepsilon\leq g(y_{\ell})<(i_{\ell}+\frac{1}{2})\varepsilon\) is equivalent to \(\sigma_{1/a}((i_{\ell}-\frac{1}{2})\varepsilon)\leq v(y_{\ell})<\sigma_{1/a}( (i_{\ell}+\frac{1}{2})\varepsilon)\), and thus

\[\sigma_{1/a}(i_{\ell}\varepsilon)-A\varepsilon\leq v(y_{\ell})<\sigma_{1/a}( i_{\ell}\varepsilon)+A\varepsilon,\] (F.24)

where \(A=\frac{1}{2}\max(1,a,1/a)\), that is, for \(g=\sigma_{a}(v)\in Z(i_{1},i_{2},\ldots,i_{\ell_{0}})\) the values \(v(y_{\ell})\) are known within small errors. By applying mean value theorem on the interval \([(\ell-1)h,\ell h]\) for function \(v\) we see that there is \(x^{\prime}\in[(\ell-1)h,\ell h]\) such that

\[\frac{dv}{dx}(x^{\prime})=\frac{v(\ell h)-v((\ell-1)h)}{h},\]

and thus by (F.24),

\[|\frac{dv}{dx}(x^{\prime})-d_{\ell,\vec{i}}|\leq 2A\frac{\varepsilon_{1}}{h},\] (F.25)

where

\[d_{\ell,\vec{i}}=\frac{1}{h}(\sigma_{1/a}(i_{\ell}\varepsilon_{1})-\sigma_{1/ a}((i_{\ell}-1)\varepsilon_{1})),\] (F.26)

Observe that these estimates are useful when \(\varepsilon_{1}\) is much smaller that \(h\). As \(g=\sigma_{a}\circ v\in\mathcal{Y}\subset\sigma_{a}(\overline{B}_{C^{1,\alpha} (\overline{D})}(0,R))\), we have \(v\in\overline{B}_{C^{1,\alpha}(\overline{D})}(0,R)\), so that \(\frac{dv}{dx}\in\overline{B}_{C^{0,\alpha}(\overline{D})}(0,R)\) satisfies (F.25) implies that

\[|\frac{dv}{dx}(x)-d_{\ell,\vec{i}}|\leq 2A\frac{\varepsilon_{1}}{h}+Rh^{ \alpha},\quad\text{for all }x\in[(\ell-1)h,\ell h].\] (F.27)

Moreover, (F.24) and \(v\in\overline{B}_{C^{1,\alpha}(\overline{D})}(0,R)\) imply

\[|v(x)-\sigma_{1/a}(i_{\ell}\varepsilon_{1})|<A\varepsilon_{1}+Rh,\] (F.28)

for all \(x\in[(\ell-1)h,\ell h]\).

Let \(\varepsilon_{2}=\varepsilon_{0}/A\). When we first choose \(\ell_{0}\) to be large enough (so that \(h=1/\ell_{0}\) is small) and then \(\varepsilon_{1}\) to be small enough, we may assume that

\[\max(2A\frac{\varepsilon_{1}}{h}+Rh^{\alpha},A\varepsilon_{1}+Rh)<\frac{1}{8} \varepsilon_{2}.\] (F.29)

Then for any two functions \(g,g^{\prime}\in\mathcal{Y}\cap Z(i_{1},i_{2},\ldots,i_{\ell_{0}})\subset \sigma_{a}(\overline{B}_{C^{1,\alpha}(\overline{D})}(0,R))\) of the form \(g=\sigma_{a}\circ v,g^{\prime}=\sigma_{a}\circ v^{\prime}\) the inequalities (F.27) and (F.28) imply

\[|\frac{dv}{dx}(x)-\frac{dv^{\prime}}{dx}(x)|<\frac{1}{4}\varepsilon_{2},\] (F.30)

for all \(x\in D\). As \(v,v^{\prime}\in\overline{B}_{C^{1,\alpha}(\overline{D})}(0,R)\), this implies

\[\|v-v^{\prime}\|_{C^{1}(\overline{D})}<\frac{1}{2}\varepsilon_{2},\]

As the embedding \(C^{1}(\overline{D})\to H^{1}(D)\) is continuous and has norm less than 2 on the interval \(D=[0,1]\), we see that

\[\|v-v^{\prime}\|_{H^{1}(\overline{D})}<\varepsilon_{2},\]

and thus

\[\|g-g^{\prime}\|_{H^{1}(\overline{D})}<A\varepsilon_{2}=\varepsilon_{0}.\] (F.31)

To prove (F.23), we assume that \((Z(i_{1},i_{2},\ldots,i_{\ell_{0}})\cap\mathcal{Y})\cap B_{H^{1}(D)}(g_{j}, \varepsilon_{0})\neq\emptyset\), and \(g\in Z(i_{1},i_{2},\ldots,i_{\ell_{0}})\cap\mathcal{Y}\). By the assumption, there exists \(g^{*}\in(Z(i_{1},i_{2},\ldots,i_{\ell_{0}})\cap\mathcal{Y})\cap B_{H^{1}(D)}(g _{j},\varepsilon_{0})\). Using (F.31), we have

\[\|g-g_{j}\|_{H^{1}(D)}\leq\|g-g^{*}\|_{H^{1}(D)}+\|g^{*}-g_{j}\|_{H^{1}(D)}\leq 2 \varepsilon_{0}.\]Thus, \(g\in B_{H^{1}(D)}(g_{j},2\varepsilon_{0})\), which implies that the property (F.23) follows.

We next consider the case when the parameter \(a\) of the leaky relu function \(\sigma_{a}\) is zero. Again, we assume that \(D=[0,1]\) and \(y_{\ell}=h\ell\), where \(h=1/\ell_{0}\) and \(\ell=0,1,\ldots,\ell_{0}\). We consider \(g\in\mathcal{Y}\cap Z(i_{1},i_{2},\ldots,i_{\ell_{0}})\subset\sigma_{a}( \overline{B}_{C^{1,\alpha}(\overline{D})}(0,R))\) of the form \(g=\sigma_{a}(v)\) and an interval \([\ell_{1}h,(\ell_{1}+1)h]\subset D\), where \(1\leq\ell_{1}\leq\ell_{0}-2\). We will consider four cases. First, if \(g\) does not obtain the value zero on the interval \([\ell_{1}h,(\ell_{1}+1)h]\) the mean value theorem implies that there is \(x^{\prime}\in[\ell_{1}h,(\ell_{1}+1)h]\) such that \(\frac{dg}{dx}(x^{\prime})=\frac{dv}{dx}(x^{\prime})\) is equal to \(d=(g(\ell_{1}h)-g([(\ell_{1}-1)h))/h\). Second, if \(g\) does not obtain the value zero on either of the intervals \([(\ell_{1}-1)h,\ell_{1}h]\) or \([(\ell_{1}+1)h,(\ell_{1}+2)h]\), we can use the mean value theorem to estimate the derivatives of \(g\) and \(v\) at some point of these intervals similarly to the first case. Third, if \(g\) does not vanish identically on the interval \([\ell_{1}h,(\ell_{1}+1)h]\) but it obtains the value zero on the both intervals \([(\ell_{1}-1)h,\ell_{1}h]\) and \([(\ell_{1}+1)h,(\ell_{1}+2)h]\), the function \(v\) has two zeros on the interval \([(\ell_{1}-1)h,(\ell_{1}+2)h]\) and the mean value theorem implies that there is \(x^{\prime}\in[(\ell_{1}-1)h,(\ell_{1}+2)h]\) such that \(\frac{dv}{dx}(x^{\prime})=0\). Fourth, if none of the above cases are valid, \(g\) vanishes identically on the interval \([\ell_{1}^{\prime}h,(\ell_{1}+1)h]\). In all these cases the fact that \(\|v\|_{C^{1,\alpha}(\overline{D})}\leq R\) implies that the derivative of \(g\) can be estimated on the whole interval \([\ell_{1}h,(\ell_{1}+1)h]\) within a small error. Using these observations we see for any \(\varepsilon_{2},\varepsilon_{3}>0\) that if \(y_{\ell}\in D=[d_{1},d_{2}]\subset\mathbb{R}\), \(\ell=1,2,\ldots,\ell_{0}\) are a sufficiently dense grid in \(D\) and \(\varepsilon_{1}\) to be small enough, then the derivatives of any two functions \(g,g^{\prime}\in\mathcal{Y}\cap Z(i_{1},i_{2},\ldots,i_{\ell_{0}})\subset \sigma_{a}(\overline{B}_{C^{1,\alpha}(\overline{D})}(0,R))\) of the form \(g=\sigma_{a}(v),g^{\prime}=\sigma_{a}(v^{\prime})\) satisfy \(\|g-g^{\prime}\|_{H^{1}([d_{1}+\varepsilon_{3},d_{2}-\varepsilon_{3}])}< \varepsilon_{2}\). As the embedding \(C^{1}([d_{1}+\varepsilon_{3},d_{2}-\varepsilon_{3}])\to H^{1}([d_{1}+ \varepsilon_{3},d_{2}-\varepsilon_{3}])\) is continuous,

\[\|\sigma_{a}(v)\|_{H^{1}([d_{1},d_{1}+\varepsilon_{3}])} \leq c_{a}\|v\|_{C^{1,\alpha}(\overline{D})}\sqrt{\varepsilon}_{3},\] \[\|\sigma_{a}(v)\|_{H^{1}([d_{2}-\varepsilon_{3},d_{2}])} \leq c_{a}\|v\|_{C^{1,\alpha}(\overline{D})}\sqrt{\varepsilon}_{3},\]

and \(\varepsilon_{2}\) and \(\varepsilon_{3}\) can be chosen to be arbitrarily small, we see that the property (F.23) follows. Thus the property (F.23) is shown in all cases.

By our assumptions \(\mathcal{Y}\subset\sigma_{a}(B_{C^{1,\alpha}(\overline{D})}(0,R))\) and hence \(g\in\mathcal{Y}\) implies that \(\|g\|_{C(\overline{D})}\leq AR\). This implies that \(\mathcal{Y}\cap Z(i_{1},i_{2},\ldots,i_{\ell_{0}})\) is empty if there is \(\ell\) such that \(|i_{\ell}|>2AR/\varepsilon_{1}+1\). Thus, there is a finite set \(\mathcal{I}\subset\mathbb{Z}^{\ell_{0}}\) such that

\[\mathcal{Y}\subset\bigcup_{\vec{i}\in\mathcal{I}}Z(\vec{i}),\] (F.32) \[Z(\vec{i})\cap\mathcal{Y}\neq\emptyset,\quad\text{for all }\vec{i}\in \mathcal{I},\] (F.33)

where we use notation \(\vec{i}=(i_{1},i_{2},\ldots,i_{\ell_{0}})\in\mathbb{Z}^{\ell_{0}}\). On the other hand, we have chosen \(g_{j}\in\mathcal{Y}\) such that \(B_{H^{1}(D)}(g_{j},\varepsilon_{0})\), \(j=1,\ldots,J\) cover \(\mathcal{Y}\). This implies that for all \(\vec{i}\in\mathcal{I}\) there is \(j=j(\vec{i})\in\{1,2,\ldots,j\}\) such that there exists \(g\in Z(\vec{i})\cap B_{H^{1}(D)}(g_{j},\varepsilon_{0})\). By (F.23), this implies that

\[Z(\vec{i})\subset B_{H^{1}(D)}(g_{j(\vec{i})},2\varepsilon_{0}).\] (F.34)

Thus, we see that \(Z(\vec{i})\), \(\vec{i}\in\mathcal{I}\) is a disjoint covering of \(\mathcal{Y}\), and by (F.34), in each set \(Z(\vec{i})\cap\mathcal{Y}\), \(\vec{i}\in\mathcal{I}\) the map \(g\to\mathcal{R}_{j}(g)\) we have constructed a right inverse of the map \(F_{1}\).

Below, we denote \(s(\vec{i},\ell)=i_{\ell}\varepsilon_{1}\). Next we construct a partition of unity in \(\mathcal{Y}\) using maps

\[F_{z,s,h}(v,w)(x)=\int_{D}k_{z,s,h}(x,y,v(x),w(y))dy,\]

where

\[k_{z,s,h}(x,y,v(x),w(y))=v(x)\mathbf{1}_{[s-\frac{1}{2}h,s+\frac{1}{2}h)}(w(y) )\delta(y-z).\]

Then,

\[F_{z,s,h}(v,w)(x)=\left\{\begin{aligned} v(x),& \quad\text{if }-\frac{1}{2}h\leq w(z)-s<\frac{1}{2}h,\\ 0,&\quad\text{otherwise}.\end{aligned}\right.\]

Next, for all \(\vec{i}\in\mathcal{I}\) we define the operator \(\Phi_{\vec{i}}\colon H^{1}(D)\times\mathcal{Y}\to H^{1}(D)\),

\[\Phi_{\vec{i}}(v,w)=\pi_{1}\circ\phi_{\vec{i},1}\circ\phi_{\vec{i},2}\circ \cdots\circ\phi_{\vec{i},\ell_{0}}(v,w),\]where \(\phi_{\vec{i},\ell}:H^{1}(D)\times\mathcal{Y}\to H^{1}(D)\times\mathcal{Y}\) are the maps

\[\phi_{\vec{i},\ell}(v,w)=(F_{y_{\ell},s(\vec{i},\ell),\varepsilon_{1}}(v,w),w),\]

and \(\pi_{1}(v,w)=v\) maps a pair \((v,w)\) to the first function \(v\). It satisfies

\[\Phi_{\vec{i}}(v,w)=\left\{\begin{array}{rl}v,&\mbox{ if }-\frac{1}{2} \varepsilon_{1}\leq w(y_{\ell})-s(\vec{i},\ell)<\frac{1}{2}\varepsilon_{1} \mbox{ for all }\ell,\\ 0,&\mbox{ otherwise.}\end{array}\right.\]

Observe that here \(s(\vec{i},\ell)=i_{\ell}\varepsilon_{1}\) is close to the value \(g_{j(\vec{i})}(y_{\ell})\). Now we can write for \(g\in Y\)

\[F_{1}^{-1}(g)=\sum_{\vec{i}\in\mathcal{I}}\Phi_{\vec{i}}(\mathcal{R}_{j(\vec{i })}(g),g),\]

with suitably chosen \(j(\vec{i})\in\{1,2,\ldots,J\}\).

Let us finally consider \(A_{u_{0}}^{-1}\) where \(u_{0}\in C(\overline{D})\). Let us denote

\[\widetilde{K}_{u_{0}}w=\int_{D}u_{0}(y)\,\frac{\partial k}{\partial u}(x,y,u_{ 0}(y))w(y)dy,\]

and \(J_{u_{0}}=K_{u_{0}}+\widetilde{K}_{u_{0}}\) be the integral operator with kernel

\[j_{u_{0}}(x,y)=k(x,y,u_{0}(y))+u_{0}(y)\,\frac{\partial k}{\partial u}(x,y,u_{ 0}(y)).\]

We have

\[(I+J_{u_{0}})^{-1}=I-J_{u_{0}}+J_{u_{0}}(I+J_{u_{0}})^{-1}J_{u_{0}},\]

so that when we write the linear bounded operator

\[A_{u_{0}}^{-1}=(I+J_{u_{0}})^{-1}:H^{1}(D)\to H^{1}(D),\]

as an integral operator

\[(I+J_{u_{0}})^{-1}v(x)=v+\int_{D}m_{u_{0}}(x,y)v(y)dy,\]

we have

\[(I+J_{u_{0}})^{-1}v(x)\] \[= v(x)-J_{u_{0}}v(x)\] \[+\int_{D}\left(\int_{D}\left\{j_{u_{0}}(x,y^{\prime})j_{u_{0}}(y, y^{\prime})+\left(\int_{D}j_{u_{0}}(x,y^{\prime})m_{u_{0}}(y^{\prime},x^{ \prime})j_{u_{0}}(x^{\prime},y)dx^{\prime}\right)\right\}dy^{\prime}\right)v( y)dy\] \[= v(x)-\int_{D}\widetilde{j}_{u_{0}}(x,y)v(y)dy,\]

where

\[\widetilde{j}_{u_{0}}(x,y)=-j_{u_{0}}(x,y)+\int_{D}(j_{u_{0}}(x,y^{\prime})j_ {u_{0}}(y,y^{\prime})dy^{\prime}+\int_{D}\int_{D}j_{u_{0}}(x,y^{\prime})m_{u_ {0}}(y^{\prime},x^{\prime})j_{u_{0}}(x^{\prime},y)dx^{\prime}dy^{\prime}.\]

This implies that the operator \(A_{u_{0}}^{-1}=(I+J_{u_{0}})^{-1}\) is a neural operator, too. Observe that \(\widetilde{j}_{u_{0}}(x,y),\partial_{x}\widetilde{j}_{u_{0}}(x,y)\in C( \overline{D}\times\overline{D})\).

This proves Theorem 3.