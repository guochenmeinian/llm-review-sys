# Sample Complexity of Forecast Aggregation

 Tao Lin

Harvard University

Cambridge, MA 02138

tlin@g.harvard.edu &Yiling Chen

Harvard University

Cambridge, MA 02138

yiling@seas.harvard.edu

###### Abstract

We consider a Bayesian forecast aggregation model where \(n\) experts, after observing private signals about an unknown binary event, report their posterior beliefs about the event to a principal, who then aggregates the reports into a single prediction for the event. The signals of the experts and the outcome of the event follow a joint distribution that is unknown to the principal, but the principal has access to i.i.d. "samples" from the distribution, where each sample is a tuple of the experts' reports (not signals) and the realization of the event. Using these samples, the principal aims to find an \(\varepsilon\)-approximately optimal aggregator, where optimality is measured in terms of the expected squared distance between the aggregated prediction and the realization of the event. We show that the sample complexity of this problem is at least \(\tilde{\Omega}(m^{n-2}/\varepsilon)\) for arbitrary discrete distributions, where \(m\) is the size of each expert's signal space. This sample complexity grows exponentially in the number of experts \(n\). But, if the experts' signals are independent conditioned on the realization of the event, then the sample complexity is significantly reduced, to \(\tilde{O}(1/\varepsilon^{2})\), which does not depend on \(n\). Our results can be generalized to non-binary events. The proof of our results uses a reduction from the distribution learning problem and reveals the fact that forecast aggregation is almost as difficult as distribution learning.

## 1 Introduction

Suppose you want to know whether it will rain tomorrow. A Google search on "weather" returns 40% probability of raining. The weather forecasting app on your phone shows 85%. And one of your friends, who is an expert in meteorology, predicts 65%. How do you aggregate these different predictions into a single, accurate prediction? This problem is called _forecast aggregation_ or _forecast combination_[7, 16, 45]. It has innumerable applications in fields ranging from economics, statistics, operations research, to machine learning, decision theory, and of course, climate science.

A straightforward solution to forecast aggregation is to take the (unweighted) average of all experts' forecasts. Simple as it is, unweighted average performs surprisingly well in practice, as observed by many forecast aggregation works from 1960s to 2020s (e.g., [7, 38, 35, 30, 16, 45, 34]). Naturally, when past data of expert forecasts and event outcomes (e.g., historical weather forecasts and outcomes) are available, one may hope to leverage on such data to learn more accurate aggregators. While adjusting the weights of experts in averaging according to their individual historical accuracy has led to improved accuracy for the aggregated prediction [49], interestingly, more sophisticated data-driven methods [23, 35, 49] were often outperformed by the unweighted average. The dominance of simple aggregators over more sophisticated data-driven methods is observed so often in empirical applications that it is termed "the forecast combination puzzle" [44, p.428].

There are many potential explanations for the forecast combination puzzle [43, 23, 15]. In some scenarios, the past events are different from the future events in fundamental ways (e.g., geopoliticalforecasting) and hence past data may not be informative in predicting the future. Another widely accepted conjecture is that the amount of past data is not large enough for a data-intensive method to perform well. Indeed, the sample sizes in many empirical forecast aggregation works are small under today's "big-data" standard (24 in [27], 69 in [42], 87 in [6], 100 in [30]). However, there are aggregation settings where future events are arguably similar to past events and we do have abundant data -- for instance, the forecasting of weather, stock prices [21], and some forecasting competitions on Kaggle1. Such settings are well-suited for data-driven methods. It is hence tempting to ask _how many_ data are needed for data-driven aggregators to achieve high accuracy in these settings.

Footnote 1: [https://www.kaggle.com/c/m5-forecasting-accuracy](https://www.kaggle.com/c/m5-forecasting-accuracy)

In this paper, we initiate the study of _sample complexity of forecast aggregation_, building upon a standard Bayesian forecasting model [37, 48, 24]. In this model, there are \(n\) experts who observe private signals about an unknown binary event and make posterior predictions about the event. The experts' signals and the event are jointly distributed according to some underlying unknown distribution (information structure) \(P\), which determines the optimal way to aggregate the experts' predictions. With sample access to the unknown distribution \(P\), we ask the following question:

_How many samples do we need to approximately learn the optimal aggregator?_

Our model favors the use of data-driven aggregation methods because historical tasks are i.i.d. as future tasks. We show that however, even in this benign model, optimal aggregation in general needs _exponentially many_ samples. One may thus expect that data-driven methods can hardly perform well in more realistic and non-i.i.d. scenarios. Nevertheless, for some special, yet interesting, families of information structures, the sample complexity of forecast aggregation is significantly lower.

Main results(1) If \(P\) can be an arbitrary discrete distribution, then at least \(\tilde{\Omega}(m^{n-2}/\varepsilon)\) samples are needed, and \(\tilde{O}(m^{n}/\varepsilon^{2})\) samples are sufficient, to learn an \(\varepsilon\)-optimal aggregator with high probability, where \(m\) is the number of signals an expert can possibly observe.2 (2) If the experts' signals are conditionally independent, then the sample complexity is exponentially reduced and surprisingly does not depend on the number of experts or the number of signals: \(\tilde{O}(1/\varepsilon^{2})\) samples are sufficient, and \(\tilde{\Omega}(1/\varepsilon)\) samples are necessary, to learn an \(\varepsilon\)-optimal aggregator with high probability.

Footnote 2: The \(\tilde{O}(\cdot)\) and \(\tilde{\Omega}(\cdot)\) notations omit logarithmic factors.

Main techniquesThe main technical part of our paper is to prove the \(\tilde{\Omega}(m^{n-2}/\varepsilon)\) lower bound on the sample complexity of forecast aggregation for the general case, via a reduction from the _distribution learning_ problem. It is known that learning a discrete distribution with support size \(|S|\) in total variation distance \(\varepsilon\) requires \(\tilde{\Omega}(|S|/\varepsilon^{2})\) samples. By reducing distribution learning to forecast aggregation, we obtain the lower bound on the sample complexity of the latter problem. This reduction is highly nontrivial. To do this reduction we define a new distribution learning problem that is different from the one in the literature, which is of independent interest. This reduction also reveals an interesting fact: learning to aggregate optimally on some distribution is almost as difficult as learning the distribution itself. This is a little surprising because one might initially think that aggregation should be easier than distribution learning - we show that this is not the case.

Structure of the paperWe discuss related works in Section 1.1. Section 2 introduces our model. Section 3 includes some preliminaries, including the distribution learning problem, which we will use in the proof. Section 4 studies the sample complexity for general distributions. Section 5 focuses on the conditional independence case. Section 6 summarizes how our results can be generalized to non-binary (multi-outcome) events. Section 7 concludes and discusses future directions.

### Related Works

Data-driven aggregationData-driven approaches to forecast aggregation date back to perhaps the first paper on forecast aggregation [7] and have been a standard practice in the literature (see, e.g., surveys [16, 45] and more recent works [51, 42, 4, 47, 22, 20]). Many of these works focus on specific weighted average aggregators like _linear pooling_ (weighted arithmetic mean) [51] and _logarithmic pooling_ (normalized weighted geometric mean) [42, 40], and the goal is to estimate the optimal weights from data. However, those weighted average aggregators are not necessarily the optimal (_Bayesian_) aggregator unless the underlying information structure satisfies some strict conditions (e.g., experts' forecasts are equal to the true probability of the event plus normally distributed noises [48]). Our work aims to understand the sample complexity of Bayesian aggregation, namely how many samples are needed to approximate the Bayesian aggregator.

Our work is closely related to a recent work [4] on Bayesian aggregation via _online learning_. For the case of conditionally independent experts, [4] shows that the Bayesian aggregator can be approximated with \(\varepsilon=\tilde{O}(\frac{n}{\sqrt{T}})\) regret. By an online-to-batch conversion, this implies a \(T=\tilde{O}(\frac{n^{2}}{\varepsilon^{2}})\) sample complexity upper bound for the batch learning setting. Our paper studies the batch learning setting. For the conditional independence case, we obtain an improved bound of \(\tilde{O}(\frac{1}{\varepsilon^{2}})\).

Robust forecast aggregationRecent works on "robust forecast aggregation" [2, 39, 18, 3] also study information aggregation problems where the principal does not know the underlying information structure. They take a worst-case approach, assuming that the information structure is chosen adversarially. This often leads to negative results: e.g., a bad approximation ratio [2, 39] or a degenerate maximin aggregator that solely relies on a random expert's opinion [18, 3]. In contrast, we assume sample access to the unknown information structure. Our sample complexity approach is orthogonal and complementary to the robust forecast aggregation approach.

Sample complexity of mechanism designOur work may remind the reader of a long line of research on the sample complexity of revenue maximization in mechanism design (e.g., [17, 20, 36, 19, 5, 11, 29, 10, 25, 50, 28]). In particular, [28] gives a general framework to bound the sample complexity for mechanism design problems that satisfy a "strong monotonicity" property, but this property is not satisfied by our forecast aggregation problem. A key observation from this line of works is that the number of samples needed to learn an \(\varepsilon\)-optimal auction for \(n\geq 1\) independent bidders is increasing in \(n\), because when there are more bidders, although we can obtain higher revenue, the optimal auction benchmark is also improved. We see a similar phenomenon that the sample complexity of forecast aggregation increases with the number of experts in the general case, but not in the case where experts are conditionally independent.

## 2 Model

### Forecast Aggregation

There are \(n\geq 2\) experts and one principal. The principal wants to predict the probability that a binary event \(\omega\in\Omega=\{0,1\}\) happens (\(\omega=1\)), based on information provided by the experts. For example, \(\omega\) may represent whether it will rain tomorrow. We present binary events to simplify notations. All our results can be generalized to multi-outcome events with \(|\Omega|>2\) (see Section 6). We also refer to \(\omega\) as "the state of the world". Each expert \(i=1,\ldots,n\) observes some private signal \(s_{i}\in\mathcal{S}_{i}\) that is correlated with \(\omega\), where \(\mathcal{S}_{i}\) denotes the space of all possible signals of expert \(i\). We assume for now that \(\mathcal{S}_{i}\) is finite, with size \(|\mathcal{S}_{i}|=m\). We relax this assumption in Section 5 where we consider conditionally independent signals. Let \(\mathbf{\mathcal{S}}=\mathcal{S}_{1}\times\cdots\times\mathcal{S}_{n}\) be the joint signal space of all experts; \(|\mathbf{\mathcal{S}}|=m^{n}\). Let \(P\) be a distribution over \(\mathbf{\mathcal{S}}\times\Omega\), namely, a joint distribution of signals \(\mathbf{s}=(s_{1},\ldots,s_{n})\) and event \(\omega\). Since the space \(\mathbf{\mathcal{S}}\times\Omega\) is discrete, we can use \(P(\cdot)\) to denote the probability: \(P(\mathbf{s},\omega)=\Pr_{P}[\mathbf{s},\omega]\). Signals of different experts can be correlated conditioned on \(\omega\). We assume that each expert \(i\) knows the marginal joint distribution of their own signal \(s_{i}\) and \(\omega\), \(P(s_{i},\omega)\). Neither any expert nor the principal knows the entire distribution \(P\). Each expert \(i\) reports to the principal a forecast (or prediction) \(r_{i}\) for the event \(\omega\), which is equal to the conditional probability of \(\omega=1\) given their signal \(s_{i}\):3

Footnote 3: One may wonder whether the experts are willing to report \(r_{i}=P(\omega=1\,|\,s_{i})\)_truthfully_. This can be guaranteed by a _proper scoring rule_. For example, we can reward each expert the Brier score \(C-|r_{i}-\omega|^{2}\) after seeing the realization of \(\omega\)[9]. Each expert maximizes its expected reward by reporting its belief truthfully.

\[r_{i}=P(\omega=1\mid s_{i})=\frac{P(\omega=1)P(s_{i}|\omega=1)}{P(\omega=1)P( s_{i}|\omega=1)+P(\omega=0)P(s_{i}|\omega=0)}. \tag{1}\]

We note that \(r_{i}\) depends on \(s_{i}\) and \(P\), but not on \(\omega\) or other experts' signals \(\mathbf{s}_{-i}\). Let \(\mathbf{r}=(r_{1},\ldots,r_{n})\in[0,1]^{n}\) denote the reports (joint report) of all experts. We sometimes use \(\mathbf{r}_{-i}\) to denote the reports of all experts except \(i\). The principal aggregates the experts' reports \(\mathbf{r}\) into a single forecast \(f(\mathbf{r})\) using some _aggregation function_, or _aggregator_, \(f:[0,1]^{n}\to[0,1]\). We measure the performance of an aggregator by the mean squared loss:

\[L_{P}(f)=\mathbb{E}_{P}\big{[}|f(\mathbf{r})-\omega|^{2}\big{]}. \tag{2}\]

The notation \(\mathbb{E}_{P}[\cdot]\) makes it explicit that the expectation is over the random draw of \((\mathbf{s},\omega)\sim P\) followed by letting \(r_{i}=P(\omega=1\,|\,s_{i})\). We omit \(P\) and write \(\mathbb{E}[\cdot]\) when it is clear from the context.

Let \(f^{*}\) be the optimal aggregator with respect to \(P\), which minimizes \(L_{P}(f)\):

\[f^{*}=\operatorname*{argmin}_{f:[0,1]^{n}\to[0,1]}L_{P}(f)=\operatorname*{ argmin}_{f:[0,1]^{n}\to[0,1]}\mathbb{E}_{P}\big{[}|f(\mathbf{r})-\omega|^{2}\big{]}. \tag{3}\]

We have the following characterization of \(f^{*}\) and \(L_{P}(f)\): \(f^{*}\) is equal to the Bayesian aggregator, which computes the posterior probability of \(\omega=1\) given all the reports \(\mathbf{r}=(r_{1},\ldots,r_{n})\). And the difference between the loss of \(f\) and the loss of \(f^{*}\) is equal to their expected squared difference.

**Lemma 2.1**.: _The optimal aggregator \(f^{*}\) and any aggregator \(f\) satisfy:_

* \(f^{*}(\mathbf{r})=P(\omega=1\,|\,\mathbf{r})\)_, for almost every_ \(\mathbf{r}\)_._
* \(L_{P}(f)-L_{P}(f^{*})=\mathbb{E}_{P}\big{[}|f(\mathbf{r})-f^{*}(\mathbf{r})|^{2}\big{]}\)_._

An aggregator \(f\) is _\(\varepsilon\)-optimal_ (with respect to \(P\)) if \(L_{P}(f)\leq L_{P}(f^{*})+\varepsilon\). By Lemma 2.1, this is equivalent to \(\mathbb{E}_{P}\big{[}|f(\mathbf{r})-f^{*}(\mathbf{r})|^{2}\big{]}\leq\varepsilon\). For an \(\varepsilon\)-optimal \(f\), we also say it \(\varepsilon\)-approximates \(f^{*}\).

**Definition 2.2**.: _An aggregator \(f\) is \(\varepsilon\)-optimal (with respect to \(P\)) if \(\mathbb{E}_{P}\big{[}|f(\mathbf{r})-f^{*}(\mathbf{r})|^{2}\big{]}\leq\varepsilon\)._

Discussion of the benchmark \(f^{*}\)Our benchmark, the Bayesian aggregator \(f^{*}\), is common in the forecast aggregation literature (e.g., [27, 26, 45]). It is stronger than the typical "best expert" benchmark in no-regret learning (e.g., [13, 22]), but weaker than the "omniscient" aggregator that has access to the experts' _signals_: \(f_{\mathrm{omni}}(\mathbf{s})=P(\omega=1\,|\,\mathbf{s})\). If there is a one-to-one mapping between signals \(\mathbf{s}\) and reports \(\mathbf{r}\), then \(f_{\mathrm{omni}}\) and \(f^{*}\) are the same. Otherwise, \(f_{\mathrm{omni}}\) could be much stronger than \(f^{*}\) and an \(\varepsilon\)-approximation to \(f_{\mathrm{omni}}\) using experts' reports only is not always possible.4 In contrast, an \(\varepsilon\)-approximation to \(f^{*}\) is always achievable (in fact, achieved by \(f^{*}\) itself). The difference between \(f^{*}\) and \(f_{\mathrm{omni}}\) is known as the difference between "aggregating forecasts" and "aggregating information sets" in the literature [27, p.198-199], [26, p.168-169], [45, p.143].

Footnote 4: This has been noted by [2, 4, 39]. They give an XOR example where \(\omega=s_{1}\oplus s_{n}\), \(s_{1}\) and \(s_{2}\) are i.i.d. \(\operatorname{Uniform}\{0,1\}\) distributed. The experts always report \(r_{i}=0.5\), \(f_{\mathrm{omni}}(s_{1},s_{2})=\omega\), \(f^{*}(r_{1},r_{2})=0.5\), so \(L_{P}(f_{\mathrm{omni}})=0\) but \(L_{P}(f^{*})=0.25>0\). No aggregator that uses experts’ reports only can do better than \(f^{*}\).

### Sample Complexity of Forecast Aggregation

The principal has access to \(T\) i.i.d. samples of forecasts and event realizations drawn from the underlying unknown distribution \(P\):

\[S_{T}=\big{\{}(\mathbf{r}^{(1)},\omega^{(1)}),\ldots,(\mathbf{r}^{(T)},\omega^{(T)}) \big{\}},\quad(\mathbf{s}^{(t)},\omega^{(t)})\sim P,\quad r_{i}^{(t)}=P(\omega=1\ |\ s_{i}^{(t)}). \tag{4}\]

Here, we implicitly regard \(P\) as a distribution over \((\mathbf{r},\omega)\) instead of \((\mathbf{s},\omega)\). The principal uses samples \(S_{T}\) to learn an aggregator \(\hat{f}=\hat{f}_{S_{T}}\), in order to approximate \(f^{*}\). Our main question is:

_How many samples are necessary and sufficient for finding an \(\varepsilon\)-optimal aggregator \(\hat{f}\)_

_(with probability at least \(1-\delta\) over the random draw of samples)?_

The answer to the above question depends on the family of distributions we are interested in. Let \(\mathcal{P}\) denote a family of distributions over \(\mathbf{\mathcal{S}}\times\Omega\). It could be the set of all distributions over \(\mathbf{\mathcal{S}}\times\Omega\), or in Section 5 we will only consider distributions where the signals are independent conditioned on \(\omega\). We define the sample complexity of forecast aggregation, with respect to \(\mathcal{P}\), formally:

**Definition 2.3**.: _The sample complexity of forecast aggregation (with respect to \(\mathcal{P}\)) is the minimum function \(T_{\mathcal{P}}(\cdot,\cdot)\) of \(\varepsilon,\delta\in(0,1)\), such that: if \(T\geq T_{\mathcal{P}}(\varepsilon,\delta)\), then for any distribution \(P\in\mathcal{P}\), with probability at least \(1-\delta\) over the random draw of \(T\) samples \(S_{T}\) from \(P\) (and over the randomness of the learning procedure if it is randomized), we can obtain an aggregator \(\hat{f}=\hat{f}_{S_{T}}\) satisfying \(\mathbb{E}_{P}[|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|^{2}]\leq\varepsilon\)._The principal is assumed to know the family of distributions \(\mathcal{P}\) but not the specific distribution \(P\in\mathcal{P}\). There should be at least two different distributions in \(\mathcal{P}\). Otherwise, the principal knows what the distribution is and there is no need for learning.

## 3 Preliminaries

In this section, we briefly introduce some notions that will be used in our analysis of the sample complexity of forecast aggregation, including some definitions of distances between distributions, the distribution learning problem, and the distinguishing distributions problem.

Distances between distributionsWe recall two distance metrics for discrete distributions: the _total variation distance_ and the _(squared) Hellinger distance_.

**Definition 3.1**.: _Let \(D_{1},D_{2}\) be two distributions on a discrete space \(\mathcal{X}\)._

* _The_ total variation distance between__\(D_{1}\) and__\(D_{2}\) _is_ \(d_{\mathrm{TV}}(D_{1},D_{2})=\frac{1}{2}\sum_{x\in\mathcal{X}}\big{|}D_{1}(x)-D _{2}(x)\big{|}\)_._
* _The_ squared Hellinger distance _between_ \(D_{1}\) _and_ \(D_{2}\) _is_ \(d_{\mathrm{H}}^{2}(D_{1},D_{2})\;=\;\frac{1}{2}\sum_{x\in\mathcal{X}}\big{(} \sqrt{D_{1}(x)}-\sqrt{D_{2}(x)}\big{)}^{2}=\;1-\sum_{x\in\mathcal{X}}\sqrt{D_{1 }(x)D_{2}(x)}\)_._

The total variation distance has the following well-known property that upper bounds the difference between the expected values of a function on two distributions:

**Fact 3.2**.: _For any function \(h:\mathcal{X}\rightarrow[0,1]\), \(|\mathbb{E}_{x\sim D_{1}}h(x)-\mathbb{E}_{x\sim D_{2}}h(x)|\leq d_{\mathrm{TV} }(D_{1},D_{2})\)._

In Appendix A we give some properties of the Hellinger distance that will be used in the proofs.

Distribution learning in total variation distanceOur analysis of the sample complexity of the forecast aggregation problem will leverage on the sample complexity of another learning problem: _learning discrete distributions in total variation distance_. We review this problem below.

Let \(\mathcal{D}\) be a family of distributions over \(\mathcal{X}\). The _sample complexity of learning distributions in \(\mathcal{D}\) within total variation distance \(\varepsilon\)_ is the minimum function \(T_{\mathcal{D}}^{\mathrm{TV}}(\varepsilon,\delta)\), such that: if \(T\geq T_{\mathcal{D}}^{\mathrm{TV}}(\varepsilon,\delta)\), then for any distribution \(D\in\mathcal{D}\), with probability at least \(1-\delta\) over the random draw of \(T\) samples from \(D\), we can obtain (from the \(T\) samples) a distribution \(\hat{D}\) such that \(d_{\mathrm{TV}}(\hat{D},D)\leq\varepsilon\).

**Proposition 3.3** (e.g., [12, 33]).: _Let \(\mathcal{D}_{\mathrm{all}}\) be the set of all distributions over \(\mathcal{X}\). Then, \(T_{\mathcal{D}_{\mathrm{all}}}^{\mathrm{TV}}(\varepsilon,\delta)=\Theta\big{(} \frac{|\mathcal{X}|+\log(1/\delta)}{\varepsilon^{2}}\big{)}\). In particular, the upper bound can be achieved by using the empirical estimate \(\hat{D}_{\mathrm{emp}}\) (which is the uniform distribution over the \(T\) samples). The lower bound holds regardless of what learning algorithm is used._

Distinguishing distributionsAnother learning problem that we will leverage on is the problem of _distinguishing (two) distributions_: given samples from a distribution randomly chosen from \(\{D_{1},D_{2}\}\), we are to guess whether the samples are from \(D_{1}\) or \(D_{2}\). The sample complexity of distinguishing distributions is characterized by the squared Hellinger distance. It is known that at least \(T=\Omega\big{(}\frac{1}{d_{\mathrm{H}}^{2}(D_{1},D_{2})}\log\frac{1}{\delta} \big{)}\) samples are needed to distinguish two distributions with probability at least \(1-\delta\). See Appendix A for a formal statement of this result.

## 4 Sample Complexity for General Distributions

In this section we characterize the sample complexity of forecast aggregation for general distributions \(P\). We give an exponential (in the number of experts, \(n\)) upper bound and an exponential lower bound on the sample complexity, as follows:

**Theorem 4.1**.: _Let \(\mathcal{P}_{\mathrm{all}}\) be the set of all distributions over \(\boldsymbol{\mathcal{S}}\times\Omega\), with \(|\boldsymbol{\mathcal{S}}|=m^{n}\). Suppose \(n\geq 2\). The sample complexity of forecast aggregation with respect to \(\mathcal{P}_{\mathrm{all}}\) is_

\[O\big{(}\frac{m^{n}+\log(1/\delta)}{\varepsilon^{2}}\big{)}\;\geq\;T_{ \mathcal{P}_{\mathrm{all}}}(\varepsilon,\delta)\;\geq\;\Omega\big{(}\frac{m^ {n-2}+\log(1/\delta)}{\varepsilon}\big{)}. \tag{5}\]

This theorem is for \(n\geq 2\). When there is only one expert (\(n=1\)), there is no need to learn to aggregate because the optimal "aggregator" \(f^{*}\) simply outputs the forecast given by the only expert: \(f^{*}(r_{1})=P(\omega=1\,|\,r_{1})=P(\omega=1\,|\,s_{1})=r_{1}\). The sample complexity is \(0\) in this case.

There is a gap in the dependency on \(\varepsilon\) in the upper bound and the lower bound in Theorem 4.1. We conjecture that the tight dependency on \(\varepsilon\) should be \(\frac{1}{\varepsilon}\) (so the lower bound is tight). See Section 7 for a detailed discussion of this conjecture, where we show that the dependency on \(\varepsilon\) in the upper bound can be improved to \(\frac{1}{\varepsilon}\) for a large family of distributions.

### Proof of the Upper Bound

In this subsection we prove the \(O\big{(}\frac{m^{n}+\log(1/\delta)}{\varepsilon^{2}}\big{)}\) upper bound in Theorem 4.1. This is a direct corollary of the distribution learning result introduced in Section 3.

We regard \(P\) as a distribution over \(\mathbf{r}\) and \(\omega\) instead of over \(\mathbf{s}\) and \(\omega\). Then \(P\) is a discrete distribution with support size at most \(2m^{n}\) because each possible report \(r_{i}\in[0,1]\) corresponds to some discrete signal \(s_{i}\) in \(\mathcal{S}_{i}\). Let \(\hat{P}_{\mathrm{emp}}\) be the empirical distribution of reports and event realizations: \(\hat{P}_{\mathrm{emp}}=\mathrm{Uniform}\big{\{}(\mathbf{r}^{(1)},\omega^{(1)}), \ldots,(\mathbf{r}^{(T)},\omega^{(T)})\big{\}}\). By Proposition 3.3, with probability at least \(1-\delta\) over the random draw of \(T=O\big{(}\frac{2m^{n}+\log(1/\delta)}{\varepsilon^{2}}\big{)}\) samples, we have \(d_{\mathrm{TV}}(\hat{P}_{\mathrm{emp}},P)\leq\varepsilon\). According to Fact 3.2, and by the definition of \(L_{P}(f)\), we have: for any aggregator \(f:[0,1]^{n}\to[0,1]\),

Therefore, if we pick the empirically optimal aggregator \(\hat{f}_{\mathrm{emp}}=\operatorname{argmin}_{f}L_{\hat{P}_{\mathrm{emp}}}(f)\), we get

\[L_{P}(\hat{f}_{\mathrm{emp}})\leq L_{\hat{P}_{\mathrm{emp}}}(\hat{f}_{\mathrm{emp }})+\varepsilon\leq L_{\hat{P}_{\mathrm{emp}}}(f^{*})+\varepsilon\leq L_{P}( f^{*})+2\varepsilon,\]

which means that \(\hat{f}_{\mathrm{emp}}\) is a \(2\varepsilon\)-optimal aggregator for \(P\).

### Proof of the Lower Bound

In this subsection we prove the \(\Omega\big{(}\frac{m^{n-2}+\log(1/\delta)}{\varepsilon}\big{)}\) lower bound in Theorem 4.1. The main idea is a reduction from the distribution learning problem (defined in Section 3) for a specific family \(\mathcal{D}\) of distributions over the joint signal space \(\mathbf{\mathcal{S}}=\mathcal{S}_{1}\times\cdots\times\mathcal{S}_{n}\). We construct a corresponding family of distributions \(\mathcal{P}=\{P_{D}:D\in\mathcal{D}\}\) for the forecast aggregation problem, such that, if we can obtain an \(\varepsilon\)-optimal aggregator \(\hat{f}\) for \(P_{D}\), then we can convert \(\hat{f}\) into a distribution \(\hat{D}\) such that \(d_{\mathrm{TV}}(\hat{D},D)\leq O(\sqrt{\varepsilon})\). We then prove that learning \(\mathcal{D}\) within total variation distance \(\varepsilon_{\mathrm{TV}}=O(\sqrt{\varepsilon})\) requires \(\Omega\big{(}\frac{m^{n-2}+\log(1/\delta)}{\varepsilon^{2}_{\mathrm{TV}}} \big{)}=\Omega\big{(}\frac{m^{n-2}+\log(1/\delta)}{\varepsilon}\big{)}\) samples. This gives the sample complexity lower bound for the forecast aggregation problem for \(\mathcal{P}\) (and hence \(\mathcal{P}_{\mathrm{all}}\)).

We will need a family of distributions \(\mathcal{D}\) that satisfies the following three properties:

**Definition 4.2**.: _We say a family of distributions \(\mathcal{D}\) satisfies_

1. \(B\)-uniformly bounded_, if:_ \(D(\mathbf{s})\leq\frac{B}{|\mathbf{\mathcal{S}}|}=\frac{B}{m^{n}}\), \(\forall\mathbf{s}\in\mathbf{\mathcal{S}}\)_,_ \(\forall D\in\mathcal{D}\)_, where_ \(B\geq 1\) _is a constant._
2. _same marginal across distributions, if: for any_ \(D,D^{\prime}\in\mathcal{D}\)_, any_ \(i\)_, any_ \(s_{i}\in\mathcal{S}_{i}\)_,_ \(D(s_{i})=D^{\prime}(s_{i})\)_._
3. _distinct marginals across signals, if: for any_ \(D\in\mathcal{D}\)_, any_ \(i\)_, any_ \(s_{i}\neq s^{\prime}_{i}\in\mathcal{S}_{i}\)_,_ \(D(s_{i})\neq D(s^{\prime}_{i})\)_._

How do we construct the family \(\mathcal{P}\)? For each distribution \(D\in\mathcal{D}\), we construct distribution \(P_{D}\) as follows: the marginal distribution of \(\omega\) is \(\mathrm{Uniform}\{0,1\}\), i.e., \(P_{D}(\omega=0)=P_{D}(\omega=1)=\frac{1}{2}\); conditioning on \(\omega=0\), the joint signal \(\mathbf{s}\) is uniformly distributed: \(P_{D}(\mathbf{s}\mid\omega=0)=\frac{1}{|\mathbf{\mathcal{S}}|}=\frac{1}{m^{n}}\), \(\forall\mathbf{s}\in\mathbf{\mathcal{S}}\); conditioning on \(\omega=1\), the joint signal is distributed according to \(D\): \(P_{D}(\mathbf{s}\mid\omega=1)=D(\mathbf{s})\), \(\forall\mathbf{s}\in\mathbf{\mathcal{S}}\). The family \(\mathcal{P}\) is \(\{P_{D}:D\in\mathcal{D}\}\).

We show that if we can obtain \(\varepsilon\)-optimal aggregators for distributions in \(\mathcal{P}\), then we can learn the distributions in \(\mathcal{D}\) within total variation distance \((1+B)^{2}\sqrt{\varepsilon}\), and thus the sample complexity of the former is lower bounded by the sample complexity of the latter:

**Lemma 4.3**.: _Let \(\mathcal{D}\) be a family of distributions that satisfies the three properties in Definition 4.2. Let \(\mathcal{P}=\{P_{D}:D\in\mathcal{D}\}\) be defined above. Then, \(T_{\mathcal{P}}(\varepsilon,\delta)\geq T_{\mathcal{D}}^{\mathrm{TV}}\big{(}( 1+B)^{2}\sqrt{\varepsilon},\,\delta\big{)}\)._Proof sketch of Lemma 4.3.: The full proof is in Appendix E.1. We give a sketch here. According to the definition of \(P_{D}\), by Bayes' rule, we have

\[P_{D}(\omega=1\mid\boldsymbol{s})=\frac{\frac{1}{2}P_{D}(\boldsymbol{s}|\omega=1 )}{\frac{1}{2}P_{D}(\boldsymbol{s}|\omega=0)+\frac{1}{2}P_{D}(\boldsymbol{s}| \omega=1)}=\frac{D(\boldsymbol{s})}{\frac{1}{m^{n}}+D(\boldsymbol{s})}. \tag{6}\]

The "distinct marginals across signals" property in Definition 4.2 ensures that there is a one-to-one mapping between signal \(s_{i}\) and report \(r_{i}=P_{D}(\omega=1\!\mid\!s_{i})\), and hence a one-to-one mapping between joint signal \(\boldsymbol{s}\) and joint report \(\boldsymbol{r}\). So, the Bayesian aggregator \(f^{*}\) satisfies \(f^{*}(\boldsymbol{r})=P_{D}(\omega=1\!\mid\!\boldsymbol{r})=P_{D}(\omega=1\! \mid\!\boldsymbol{s})=\frac{D(\boldsymbol{s})}{1/m^{n}+D(\boldsymbol{s})}\). This gives

\[D(\boldsymbol{s})=\frac{1}{m^{n}}\frac{f^{*}(\boldsymbol{r})}{1-f^{*}( \boldsymbol{r})}. \tag{7}\]

Suppose we have obtained an \(\varepsilon\)-optimal aggregator \(\hat{f}\) for \(P_{D}\), \(\mathbb{E}_{P_{D}}\big{[}|\hat{f}(\boldsymbol{r})-f^{*}(\boldsymbol{r})|^{2} \big{]}\leq\varepsilon\), then we convert \(\hat{f}\) into \(\hat{D}\) by letting \(\hat{D}(\boldsymbol{s})=\frac{1}{m^{n}}\frac{f(\boldsymbol{r})}{1-f( \boldsymbol{r})}\). The total variation distance between \(\hat{D}\) and \(D\) is:

\[d_{\mathrm{TV}}(\hat{D},D)=\tfrac{1}{2}\sum_{\boldsymbol{s}\in\boldsymbol{ \mathcal{S}}}\big{|}\hat{D}(\boldsymbol{s})-D(\boldsymbol{s})\big{|}\stackrel{{ \eqref{eq:P_D}}}{{=}}\sum_{\boldsymbol{s}\in\boldsymbol{\mathcal{S}}}\tfrac{1 }{m^{n}}\big{|}\frac{\hat{f}(\boldsymbol{r})}{1-f(\boldsymbol{r})}-\frac{f^{*} (\boldsymbol{r})}{1-f^{*}(\boldsymbol{r})}\big{|}. \tag{8}\]

The "\(B\)-uniformly bounded" property in Definition 4.2 ensures \(D(\boldsymbol{s})=O(\frac{1}{m^{n}})\), which has two consequences: (1) \(P_{D}(\boldsymbol{s})=O(\frac{1}{m^{n}})\); (2) \(f^{*}(\boldsymbol{r})=O(1)\), which implies \(\big{|}\frac{\hat{f}(\boldsymbol{r})}{1-f(\boldsymbol{r})}-\frac{f^{*}( \boldsymbol{r})}{1-f^{*}(\boldsymbol{r})}\big{|}=O\big{(}|\hat{f}(\boldsymbol {r})-f^{*}(\boldsymbol{r})|\big{)}\) due to Lipschitz property of the function \(\frac{x}{1-x}\) for \(x=O(1)\). These two consequences imply

\[d_{\mathrm{TV}}(\hat{D},D) =O\Big{(}\sum_{\boldsymbol{s}\in\boldsymbol{\mathcal{S}}}\tfrac{1 }{m^{n}}\big{|}\hat{f}(\boldsymbol{r})-f^{*}(\boldsymbol{r})\big{|}\Big{)}=O \Big{(}\sum_{\boldsymbol{s}\in\boldsymbol{\mathcal{S}}}P_{D}(\boldsymbol{s}) \big{|}\hat{f}(\boldsymbol{r})-f^{*}(\boldsymbol{r})\big{|}\Big{)}\] \[=O\Big{(}\mathbb{E}\big{[}|\hat{f}(\boldsymbol{r})-f^{*}( \boldsymbol{r})|\big{]}\Big{)}\stackrel{{\text{Jensen's inequality}}}{{\leq}}O\Big{(}\sqrt{\mathbb{E}\big{[}|\hat{f}(\boldsymbol{r})-f^{*}(\boldsymbol{r})|^{2}\big{]}} \Big{)}=O(\sqrt{\varepsilon}).\]

So, we obtain \(d_{\mathrm{TV}}(\hat{D},D)\leq O(\sqrt{\varepsilon})=(1+B)^{2}\sqrt{\varepsilon}\).

There is a subtlety, however: In the distribution learning problem we are given samples from \(D\), which are _signals_\(\{\boldsymbol{s}^{(t)}\}_{t=1}^{T}\). We need to convert them into the corresponding _reports_\(\{\boldsymbol{r}^{(t)}\}_{t=1}^{T}\) as the samples for the forecast aggregation problem. To do this we need to know \(P_{D}\) or \(D\), _which we do not know._ So, we make use of the "same marginal across distributions" property in Definition 4.2 here: because all the distributions \(D\in\mathcal{D}\) have the same marginal probability \(D(s_{i})=D^{\prime}(s_{i})\) (but possibly different joint probabilities \(D(\boldsymbol{s})\neq D^{\prime}(\boldsymbol{s})\)), we are able to compute the report

\[r_{i}^{(t)}=P_{D}(\omega=1\mid s_{i}^{(t)})=\tfrac{\frac{1}{2}P_{D}(s_{i}^{(t) }|\omega=1)}{\frac{1}{2}P_{D}(s_{i}^{(t)}|\omega=0)+\frac{1}{2}P_{D}(s_{i}^{(t )}|\omega=1)}=\tfrac{D(s_{i}^{(t)})}{\frac{1}{m}+D(s_{i}^{(t)})}\]

separately for each expert \(i\) without knowing \(D\), since we know what the family \(\mathcal{D}\) is. This allows us to reduce the distribution learning problem for \(\mathcal{D}\) to the forecast aggregation problem for \(\mathcal{P}\). 

Then, we find a family of distributions \(\mathcal{D}\) that satisfies the three properties in Definition 4.2 and requires many samples to learn.

**Proposition 4.4**.: _There exists a family of distributions \(\mathcal{D}\) that satisfies the three properties in Definition 4.2 (with \(B=e+\frac{1}{2}\)) and requires \(T_{\mathcal{D}}^{\mathrm{TV}}(\varepsilon_{\mathrm{TV}},\delta)=\Omega\big{(} \frac{m^{n-2}+\log(1/\delta)}{e_{\mathrm{TV}}^{2}}\big{)}\) samples to learn._

The above sample complexity is smaller than the \(\Omega\big{(}\frac{|\boldsymbol{\mathcal{S}}|+\log(1/\delta)}{e_{\mathrm{TV}}^ {2}}\big{)}\) lower bound in Proposition 3.3 because we are restricting to a smaller set of distributions than the set of all distributions over \(\boldsymbol{\mathcal{S}}\). The proof of Proposition 4.4 is analogous to a textbook proof of Proposition 3.3, which uses reductions from the distinguishing distributions problem. See details in Appendix E.2.

Finishing the proof of Theorem 4.1:By Lemma 4.3 and Proposition 4.4, plugging in \(\varepsilon_{\mathrm{TV}}=(1+B)^{2}\sqrt{\varepsilon}\) with \(B=e+\frac{1}{2}\), we obtain the lower bound on the sample complexity of forecast aggregation for \(\mathcal{P}\) (and hence for \(\mathcal{P}_{\mathrm{all}}\)):

\[T_{\mathcal{P}}(\varepsilon,\delta)\,\geq\,T_{\mathcal{D}}^{\mathrm{TV}}\big{(}(1 +B)^{2}\sqrt{\varepsilon},\,\delta\big{)}\,=\,\Omega\big{(}\tfrac{m^{n-2}+ \log(1/\delta)}{((1+B)^{2}\sqrt{\varepsilon})^{2}}\big{)}\,=\,\Omega\big{(} \tfrac{m^{n-2}+\log(1/\delta)}{\varepsilon}\big{)}.\]Sample Complexity for Conditionally Independent Distributions

Section 4 proved that learning \(\varepsilon\)-optimal aggregators for all discrete distributions needs exponentially many samples. As shown in the proof, this large sample complexity is because the experts' signals can be arbitrarily correlated conditioned on the event \(\omega\). Accurate estimation of such correlation requires many samples. So, in this section we restrict attentions to the case where the experts' signals are conditionally independent. It turns out that an \(\varepsilon\)-optimal aggregator can be learned using only \(O(\frac{1}{\varepsilon^{2}}\log\frac{1}{\varepsilon\delta})\) samples in this case, which does not depend on \(n\). The assumption of discrete signal space can be relaxed here. We also investigate two special and interesting families of conditionally independent distributions that admit an even smaller sample complexity of \(O(\frac{1}{\varepsilon}\log\frac{1}{\delta})\).

### General Conditionally Independent Distributions

Let \(P\) be a conditionally independent distributions over \(\boldsymbol{\mathcal{S}}\times\Omega\), namely, \(P(\boldsymbol{s}\,|\,\omega)=\prod_{i=1}^{n}P(s_{i}\,|\,\omega)\) for all \(\boldsymbol{s}\in\boldsymbol{\mathcal{S}}\), for \(\omega\in\{0,1\}\). Here, \(\mathcal{S}_{i}\) can be a continuous space, in which case, \(P(\cdot\,|\,\omega)\) represents a density function. We introduce some additional notations. Let \(p=P(\omega=1)\) be the prior probability of \(\omega=1\). For technical convenience we assume \(p\in(0,1)\). Define

\[\rho=\tfrac{p}{1-p}=\tfrac{P(\omega=1)}{P(\omega=0)}\in(0,+\infty). \tag{9}\]

We will be working with ratios like "\(\tfrac{r_{i}}{1-r_{i}}\)" and "\(\tfrac{p}{1-p}\)" a lot in this section. We will use the following characterization of the optimal aggregator \(f^{*}\) for conditionally independent distributions:

**Lemma 5.1** ([8]).: _For conditionally independent distribution \(P\), given signals \(\boldsymbol{s}=(s_{i})_{i=1}^{n}\), with corresponding reports \(\boldsymbol{r}=(r_{i})_{i=1}^{n}\) where \(r_{i}=P(\omega=1|s_{i})\), the posterior probability of \(\omega=1\) is:_

\[f^{*}(\boldsymbol{r})=P(\omega=1\;|\;\boldsymbol{r})=P(\omega=1\;|\; \boldsymbol{s})=\tfrac{1}{1+\rho^{n-1}\prod_{i=1}^{n}\tfrac{1-r_{i}}{r_{i}}}. \tag{10}\]

_(Define \(f^{*}(\boldsymbol{r})=0\) if \(\rho^{n-1}\prod_{i=1}^{n}\tfrac{1-r_{i}}{r_{i}}=+\infty\).)_

Lemma 5.1 implies that one way to learn \(f^{*}\) is to simply learn the value of \(\rho\). If we can learn \(\rho\) with accuracy \(\frac{\sqrt{\varepsilon}}{n}\), then we can learn \(\rho^{n-1}\) with accuracy \(\sqrt{\varepsilon}\) and obtain an \(\hat{f}\) that is \(\varepsilon\)-close to \(f^{*}\) for every possible input \(\boldsymbol{r}\in[0,1]^{n}\). However, by standard concentration inequalities, learning \(\rho\) with accuracy \(\frac{\sqrt{\varepsilon}}{n}\) requires \(\tilde{O}(\frac{n^{2}}{\rho\varepsilon})\) samples, which is larger than the \(\tilde{O}(\frac{1}{\varepsilon^{2}})\) bound we will prove. The key is that we do not actually need \(\hat{f}(\boldsymbol{r})\) to be close to \(f^{*}(\boldsymbol{r})\) for _every_\(\boldsymbol{r}\in[0,1]_{+}^{n}\); we only need the _expectation_\(\mathbb{E}\big{[}|\hat{f}(\boldsymbol{r})-f^{*}(\boldsymbol{r})|^{2}\big{]} \leq\varepsilon\). This allows us to prove a smaller sample complexity bound, using a pseudo-dimension argument.

The main result of this section is that the sample complexity of forecast aggregation with respect to all conditionally independent distributions is between \(\Omega(\frac{1}{\varepsilon}\log\frac{1}{\delta})\) and \(O(\frac{1}{\varepsilon^{2}}\log\frac{1}{\varepsilon\delta})\):

**Theorem 5.2**.: _Let \(\mathcal{P}_{\mathrm{ind}}\) be the set of all conditionally independent distributions over \(\boldsymbol{\mathcal{S}}\times\Omega\). Suppose \(n\geq 2\). The sample complexity of forecast aggregation with respect to \(\mathcal{P}_{\mathrm{ind}}\) is_

\[O\big{(}\tfrac{1}{\varepsilon^{2}}\log\tfrac{1}{\varepsilon\delta}\big{)}\; \geq\;T_{\mathcal{P}_{\mathrm{ind}}}(\varepsilon,\delta)\;\geq\;\Omega\big{(} \tfrac{1}{\varepsilon}\log\tfrac{1}{\delta}\big{)}. \tag{11}\]

We provide the main ideas of the proof of Theorem 5.2 here. The upper bound \(O(\frac{1}{\varepsilon^{2}}\log\frac{1}{\varepsilon\delta})\) is a corollary of our theorem for multi-outcome events (Theorem C.1), so we only give a sketch here. We note that, according to Lemma 5.1, the optimal aggregator has the form \(f^{*}(\boldsymbol{r})=\frac{1}{1+\rho^{n-1}\prod_{i=1}^{n}\frac{1-r_{i}}{r_{i}}}\).

We consider the class of aggregators \(\mathcal{F}=\big{\{}f^{\theta}:f^{\theta}(\boldsymbol{r})=\frac{1}{1+\theta^ {n-1}\prod_{i=1}^{n}\frac{1-r_{i}}{r_{i}}}\big{\}}\) parameterized by \(\theta\in(0,+\infty)\). The class of loss functions \(\mathcal{G}=\big{\{}g^{\theta}:g^{\theta}(\boldsymbol{r},\omega)=|f^{\theta}( \boldsymbol{r})-\omega|^{2}\big{\}}\) associated with \(\mathcal{F}\) has _pseudo-dimension_\(\mathrm{Pdim}(\mathcal{G})=O(1)\). By the known result (e.g., [1]) that the pseudo-dimension gives a sample complexity upper bound on the uniform convergence of a class of functions, we conclude that the empirically optimal aggregator in \(\mathcal{F}\) must be \(O(\varepsilon)\)-optimal on the true distribution (with probability at least \(1-\delta\)), given \(O\big{(}\frac{1}{\varepsilon^{2}}(\mathrm{Pdim}(\mathcal{G})\log\frac{1}{ \varepsilon}+\log\frac{1}{\delta})\big{)}=O(\frac{1}{\varepsilon^{2}}\log \frac{1}{\varepsilon\delta})\) samples.

We prove the \(\Omega(\frac{1}{\varepsilon}\log\frac{1}{\delta})\) lower bound by a reduction from the distinguishing distributions problem (introduced in Section 3). We construct two conditionally independent distributions \(P^{1},P^{2}\) over the space \(\Omega\times\mathbf{\mathcal{S}}\) that differ by \(d_{\mathrm{H}}^{2}(P^{1},P^{2})=O(\varepsilon)\) in squared Hellinger distance. Specifically, \(P^{1}\) has prior \(P^{1}(\omega=1)=0.5-O(\frac{1}{n})+O(\frac{\sqrt{\varepsilon}}{n})\) and \(P^{2}\) has prior \(P^{2}(\omega=1)=0.5-O(\frac{1}{n})-O(\frac{\sqrt{\varepsilon}}{n})\); the conditional distributions of each signal, \(P^{1}(s_{i}\,|\,\omega)\) and \(P^{2}(s_{i}\,|\,\omega)\), differ by \(O(\frac{\varepsilon}{n})\) in squared Hellinger distance; taking the product of \(n\) signals, \(P^{1}(\mathbf{s}\,|\,\omega)\) and \(P^{2}(\mathbf{s}\,|\,\omega)\) differ by \(O(\varepsilon)\). The distinguishing distributions problem asks: given \(T\) samples from either \(P^{1}\) or \(P^{2}\), tell which distribution the samples come from. We show that, if we can solve the forecast aggregation problem, namely, \(\varepsilon\)-approximate \(f^{*}(\mathbf{r})=\frac{1}{1+\rho^{n-1}\prod_{i=1}^{n}\frac{1-r_{i}}{r_{i}}}\), then we can estimate \(\rho\) with accuracy \(O(\frac{\sqrt{\varepsilon}}{n})\), and hence distinguish \(P^{1}\) and \(P^{2}\). But distinguishing \(P^{1}\) and \(P^{2}\) requires \(\Omega(\frac{1}{d_{\mathrm{H}}^{2}(P^{1},P^{2})}\log\frac{1}{\delta})=\Omega( \frac{1}{\varepsilon}\log\frac{1}{\delta})\) samples. This gives the lower bound we claimed. See details in Appendix F.1.

### Strongly and Weakly Informative Experts

While the sample complexity of forecast aggregation for general conditionally independent distributions is \(O(\frac{1}{\varepsilon^{2}}\log\frac{1}{\varepsilon\delta})\), under further assumptions this bound can be improved. In particular, we find two special yet natural families of conditionally independent distributions that admit \(O(\frac{1}{\varepsilon}\log\frac{1}{\delta})\) sample complexity. In these two cases, the experts are either "very informative" or "very noninformative". Roughly speaking, an expert is "very informative" if the conditional distributions of the expert's signal under event \(\omega=0\) and \(\omega=1\) are significantly different, so the expert's prediction \(r_{i}\) is away from the prior \(p\). An expert is "very non-informative" if the opposite is true. Intuitively, an expert being informative should help aggregation and hence reduce the sample complexity. Interestingly though, we show that even if the experts are non-informative the sample complexity of forecast aggregation can also be reduced. See details in Appendix B.

## 6 Extension: Multi-Outcome Events

Our main results regarding the sample complexity of forecast aggregation for binary events (Theorems 4.1 and 5.2) can be generalized to multi-outcome events with \(|\Omega|>2\). We prove that: for general distributions, the sample complexity is \(\tilde{\Omega}(\frac{m^{n-2}}{\varepsilon})=\Omega(\frac{m^{n-2}+\log(1/\delta )}{\varepsilon})\leq T_{\mathcal{P}}(\varepsilon,\delta)\leq O(\frac{|\Omega| m^{n}+\log(1/\delta)}{\varepsilon^{2}})=\tilde{O}(\frac{|\Omega|m^{n}}{ \varepsilon^{2}})\); for conditionally independent distributions, the sample complexity is \(\tilde{\Omega}(\frac{1}{\varepsilon})=\Omega(\frac{1}{\varepsilon}\log\frac{ 1}{\delta})\leq T_{\mathcal{P}_{\mathrm{ind}}}(\varepsilon,\delta)\leq O( \frac{|\Omega|\log|\Omega|}{\varepsilon^{2}}\log\frac{1}{\varepsilon}+\frac{1 }{\varepsilon^{2}}\log\frac{1}{\delta})=\tilde{O}(\frac{|\Omega|}{\varepsilon^ {2}})\). See Appendix C for details.

## 7 Conclusion and Discussion

In this work, we showed an exponential gap between the sample complexity of forecast aggregation for general distributions, \(\tilde{\Omega}(\frac{m^{n-2}}{\varepsilon})\), and conditionally independent distributions, \(\tilde{O}(\frac{1}{\varepsilon^{2}})\). This gap is due to the need of estimating the conditional correlation between experts in the general case, which is not needed in the conditional independence case. Notably, the bound \(\tilde{O}(\frac{1}{\varepsilon^{2}})\) for conditionally independent distributions does not depend on the number of experts.

We discuss the dependency of the sample complexity on \(\varepsilon\) and other directions for future works:

The dependency on \(\varepsilon\)An open question left by our work is the dependency of the sample complexity on the parameter \(\varepsilon\). We conjecture that the tight dependency should be \(\frac{1}{\varepsilon}\) (so our lower bounds are tight). This is supported by the following evidence:

**Theorem 7.1**.: _For the case of \(|\Omega|=2\) and for general distributions, if the distribution \(P\) has a minimum joint probability \(\min_{(\mathbf{s},\omega)\in\mathbf{\mathcal{S}}\times\Omega}P(\mathbf{s},\omega)>\frac{c}{ m^{n}}\) for some \(c>0\), then the sample complexity of forecast aggregation is at most \(O\big{(}\frac{m^{n}}{c\varepsilon}(n\log m+\log\frac{1}{\delta})\big{)}= \tilde{O}\big{(}\frac{nm^{n}}{c\varepsilon}\big{)}\).5_

Footnote 5: This bound has a better dependency on \(\varepsilon\) but worse on \(n\) than the \(O(\frac{m^{n}}{\varepsilon^{2}})\) bound in Theorem 4.1.

In particular, this theorem can be applied to distributions that are close to uniform, where \(P(\mathbf{s},\omega)\approx\frac{1}{|\mathbf{\mathcal{S}}\times\Omega|}=\frac{1}{2m^{n}}\), giving a bound of \(\tilde{O}(\frac{nm^{n}}{\varepsilon})\). Notably, the set of distributions we constructed in the proof of the \(\tilde{\Omega}(\frac{m^{n-2}}{\varepsilon})\) lower bound in Theorem 4.1 are also close to uniform. This means that close-to-uniform distributions have a tight sample complexity bound of the form \(\Theta\big{(}\frac{f(n,m)}{\varepsilon}\big{)}\), not \(\Theta\big{(}\frac{f(n,m)}{\varepsilon^{2}}\big{)}\). Moreover, since close-to-uniform distributions are the "most difficult" distributions to learn in the distribution learning problem, it is likely that they are also the most difficult distributions for the forecast aggregation problem, and therefore the tight sample complexity of forecast aggregation should be determined by the sample complexity for those distributions, which is \(\Theta\big{(}\frac{f(n,m)}{\varepsilon}\big{)}\).

Other future directions
* _The middle ground between fully correlated experts and conditionally independent experts:_ An example is the _partial evidence model_ in [4]. Applying [4]'s results, one can show that the sample complexity of forecast aggregation in the partial evidence model is at most \(\tilde{O}\big{(}\frac{n^{2}}{\varepsilon^{2}}\big{)}\).6 Giving a lower bound for the partial evidence model and exploring other intermediate models is open. Footnote 6: In particular, [4] studies an online learning setting with logarithmic loss. Their regret bound can be converted to our sample complexity bound by an online-to-batch conversion and the Pinsker’s inequality.
* _Weaker benchmark:_ Since the Bayesian aggregator needs exponentially many samples to approximate, can we find a weaker yet meaningful benchmark with a small sample complexity?
* _Samples vs experts:_ In reality, obtaining samples of experts' historical forecasts can be difficult, while recruiting experts is easy. Can we achieve better aggregation by recruiting more experts instead of collecting more samples? How many experts do we need?
* _Eliciting more information:_ Previous works on information elicitation and aggregation have noticed that better aggregation can be achieved by eliciting more information than agents' own predictions, for example, also eliciting each agent's prediction about other agents' predictions (e.g., [41, 47, 31, 14]). One can ask whether and how eliciting more information can help to reduce the sample complexity of information aggregation.
* _Continuous distributions:_ In our model the random variable \(\omega\) to be predicted is discrete. One can study a setting where \(\omega\) is a continuous random variable and the experts report, e.g., the means of their posterior beliefs about \(\omega\). The results for continuous random variables might be very different from the results in this work.
* _Other loss functions:_ We focused on the squared loss \(\mathbb{E}\big{[}|f(\mathbf{r})-\omega|^{2}\big{]}\) due to its popularity in machine learning problems and its useful property that the difference between the squared losses of any aggregator and the optimal aggregator is equal to their expected squared difference (Lemma 2.1). Alternatively, one can consider other loss functions like the logarithmic loss \(\mathbb{E}[\omega\log(f(\mathbf{r}))+(1-\omega)\log(1-f(\mathbf{r}))]\) and the absolute loss \(\mathbb{E}[|f(\mathbf{r})-\omega|]\). There might be some technical challenges in the analysis of sample complexity for those loss functions, though: e.g., the logarithmic loss can be unbounded [4, 40] and the absolute loss does not enjoy a property like Lemma 2.1.

## Acknowledgments and Disclosure of Funding

We would like to thank Yannai Gonczarowski, Ariel Procaccia, Milind Tambe, David Parkes, Bo Waggoner, Rafael Frongillo, Grant Schoenebeck, Yuqing Kong, and anonymous reviewers for their helpful comments. This research is based upon work supported in part by the National Science Foundation under grants no. IIS-2007887 and no. IIS-2147187.

## References

* [1] Martin Anthony and Peter L. Bartlett. _Neural Network Learning: Theoretical Foundations_. Cambridge University Press, 1 edition, November 1999.
* [2] Itai Arieli, Yakov Babichenko, and Rann Smorodinsky. Robust forecast aggregation. _Proceedings of the National Academy of Sciences_, 115(52), December 2018.
* [3] Itai Arieli, Yakov Babichenko, Inbal Talgam-Cohen, and Konstantin Zabarnyi. Universally Robust Information Aggregation for Binary Decisions. In _Proceedings of the 24th ACM Conference on Economics and Computation_, pages 118-118, London United Kingdom, July 2023. ACM.
* [4] Yakov Babichenko and Dan Garber. Learning Optimal Forecast Aggregation in Partial Evidence Environments. _Mathematics of Operations Research_, 46(2):628-641, May 2021.

* [5] Maria-Florina F Balcan, Tuomas Sandholm, and Ellen Vitercik. Sample complexity of automated mechanism design. _Advances in Neural Information Processing Systems_, 29, 2016.
* [6] Jonathan Baron, Barbara A. Mellers, Philip E. Tetlock, Eric Stone, and Lyle H. Ungar. Two Reasons to Make Aggregated Probability Forecasts More Extreme. _Decision Analysis_, 11(2):133-145, June 2014.
* [7] John M. Bates and Clive W. J. Granger. The Combination of Forecasts. _Journal of the Operational Research Society_, 20(4):451-468, December 1969.
* [8] Robert F. Bordley. A Multiplicative Formula for Aggregating Probability Assessments. _Management Science_, 28(10):1137-1148, 1982. Publisher: INFORMS.
* [9] Glenn W. Brier. Verification of Forecasts Expressed in Terms of Probability. _Monthly Weather Review_, 78(1):1-3, January 1950.
* [10] Johannes Brustle, Yang Cai, and Constantinos Daskalakis. Multi-Item Mechanisms without Item-Independence: Learnability via Robustness. In _Proceedings of the 21st ACM Conference on Economics and Computation_, pages 715-761, Virtual Event Hungary, July 2020. ACM.
* [11] Yang Cai and Constantinos Daskalakis. Learning Multi-Item Auctions with (or without) Samples. In _2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 516-527, Berkeley, CA, October 2017. IEEE.
* [12] Clement L. Canonne. A short note on learning discrete distributions, 2020.
* [13] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, Learning, and Games_. Cambridge University Press, Cambridge, 2006.
* [14] Yi-Chun Chen, Manuel Mueller-Frank, and Mallesh Pai. The Wisdom of the Crowd and Higher-Order Beliefs. In _Proceedings of the 24th ACM Conference on Economics and Computation_, pages 450-450, London United Kingdom, July 2023. ACM.
* [15] Gerda Claeskens, Jan R. Magnus, Andrey L. Vasnev, and Wendun Wang. The forecast combination puzzle: A simple theoretical explanation. _International Journal of Forecasting_, 32(3):754-762, July 2016.
* [16] Robert T. Clemen. Combining forecasts: A review and annotated bibliography. _International Journal of Forecasting_, 5(4):559-583, January 1989.
* STOC '14_, pages 243-252, New York, New York, 2014. ACM Press.
* [18] Henrique De Oliveira, Yuhta Ishii, and Xiao Lin. Robust Merging of Information. In _Proceedings of the 22nd ACM Conference on Economics and Computation_, pages 341-342, Budapest Hungary, July 2021. ACM.
* [19] Nikhil R. Devanur, Zhiyi Huang, and Christos-Alexandros Psomas. The sample complexity of auctions with side information. In _Proceedings of the forty-eighth annual ACM symposium on Theory of Computing_, pages 426-439, Cambridge MA USA, June 2016. ACM.
* [20] Peerapong Dhangwatnotai, Tim Roughgarden, and Qiqi Yan. Revenue maximization with a single sample. _Games and Economic Behavior_, 91:318-333, May 2015.
* [21] R. Glen Donaldson and Mark Kamstra. Forecast combining with neural networks. _Journal of Forecasting_, 15(1):49-61, January 1996.
* [22] Rafael Frongillo, Robert Gomez, Anish Thilagar, and Bo Waggoner. Efficient Competitions and Online Learning with Strategic Forecasters. In _Proceedings of the 22nd ACM Conference on Economics and Computation_, pages 479-496, Budapest Hungary, July 2021. ACM.
* [23] Veronique Genre, Geoff Kenny, Aidan Meyler, and Allan Timmermann. Combining expert forecasts: Can anything beat the simple average? _International Journal of Forecasting_, 29(1):108-121, January 2013.
* [24] John Geweke and Charles Whiteman. Chapter 1 Bayesian Forecasting. In _Handbook of Economic Forecasting_, volume 1, pages 3-80. Elsevier, 2006.
* [25] Yannai A. Gonczarowski and S. Matthew Weinberg. The Sample Complexity of Up-to-\(\epsilon\) Multi-dimensional Revenue Maximization. _Journal of the ACM_, 68(3):1-28, March 2021.

* [26] Clive W. J. Granger. Invited review combining forecasts--twenty years later. _Journal of Forecasting_, 8(3):167-173, July 1989.
* [27] Clive W. J. Granger and Ramu Ramanathan. Improved methods of combining forecasts. _Journal of Forecasting_, 3(2):197-204, April 1984.
* [28] Chenghao Guo, Zhiyi Huang, Zhihao Gavin Tang, and Xinzhi Zhang. Generalizing complex hypotheses on product distributions: Auctions, prophet inequalities, and pandora's problem. In _Conference on Learning Theory_, pages 2248-2288. PMLR, 2021.
* STOC 2019_, pages 662-673, Phoenix, AZ, USA, 2019. ACM Press.
* [30] Sunil Gupta and Peter C. Wilton. Combination of Forecasts: An Extension. _Management Science_, 33(3):356-372, 1987. Publisher: INFORMS.
* [31] Yuqing Kong, Yunqi Li, Yubo Zhang, Zhihuan Huang, and Jinzhao Wu. Eliciting Thinking Hierarchy without a Prior. In _Advances in Neural Information Processing Systems_, volume 35, pages 13329-13341. Curran Associates, Inc., 2022.
* [32] Jasper Lee. Lecture 11: Distinguishing (discrete) distributions, 2020.
* [33] Jasper Lee. Lecture 12: Learning Discrete Distributions, 2020.
* [34] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The M4 Competition: 100,000 time series and 61 forecasting methods. _International Journal of Forecasting_, 36(1):54-74, January 2020.
* [35] Spyros Makridakis and Robert L. Winkler. Averages of Forecasts: Some Empirical Results. _Management Science_, 29(9):987-996, September 1983.
* [36] Jamie H Morgenstern and Tim Roughgarden. On the pseudo-dimension of nearly optimal auctions. _Advances in Neural Information Processing Systems_, 28, 2015.
* [37] Peter A. Morris. Decision Analysis Expert Use. _Management Science_, 20(9):1233-1241, 1974. Publisher: INFORMS.
* [38] Paul Newbold and Clive W. J. Granger. Experience with Forecasting Univariate Time Series and the Combination of Forecasts. _Journal of the Royal Statistical Society. Series A (General)_, 137(2):131-146, 1974.
* [39] Eric Neyman and Tim Roughgarden. Are You Smarter Than a Random Expert? The Robust Aggregation of Substitutable Signals. In _Proceedings of the 23rd ACM Conference on Economics and Computation_, pages 990-1012, Boulder CO USA, July 2022. ACM.
* [40] Eric Neyman and Tim Roughgarden. No-regret learning with unbounded losses: The case of logarithmic pooling. _arXiv preprint arXiv:2202.11219_, 2022.
* [41] Drazen Prelec, He Sebastian Seung, and John McCoy. A solution to the single-question crowd wisdom problem. _Nature_, 541(7638):532-535, January 2017.
* [42] Ville A. Satopaa, Jonathan Baron, Dean P. Foster, Barbara A. Mellers, Philip E. Tetlock, and Lyle H. Ungar. Combining multiple probability predictions using a simple logit model. _International Journal of Forecasting_, 30(2):344-356, April 2014.
* [43] Jeremy Smith and Kenneth F. Wallis. A Simple Explanation of the Forecast Combination Puzzle. _Oxford Bulletin of Economics and Statistics_, 71(3):331-355, June 2009.
* [44] James H. Stock and Mark W. Watson. Combination forecasts of output growth in a seven-country data set. _Journal of Forecasting_, 23(6):405-430, September 2004.
* [45] Allan Timmermann. Chapter 4 Forecast Combinations. In _Handbook of Economic Forecasting_, volume 1, pages 135-196. Elsevier, 2006.
* [46] Alexandre B. Tsybakov. _Introduction to nonparametric estimation_. Springer series in statistics. Springer, New York ; London, 2009. OCLC: ccn300399286.
* [47] Juntao Wang, Yang Liu, and Yiling Chen. Forecast Aggregation via Peer Prediction. _Proceedings of the AAAI Conference on Human Computation and Crowdsourcing_, 9(1):131-142, October 2021.

* [48] Robert L. Winkler. Combining Probability Distributions from Dependent Information Sources. _Management Science_, 27(4):479-488, 1981. Publisher: INFORMS.
* [49] Robert L. Winkler and Spyros Makridakis. The Combination of Forecasts. _Journal of the Royal Statistical Society. Series A (General)_, 146(2):150, 1983.
* [50] Chunxue Yang and Xiaohui Bei. Learning Optimal Auctions with Correlated Valuations from Samples. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 11716-11726. PMLR, July 2021.
* [51] Yuhong Yang. Combining Forecasting Procedures: Some Theoretical Results. _Econometric Theory_, 20(1):176-222, 2004. Publisher: Cambridge University Press.

[MISSING_PAGE_FAIL:14]

[MISSING_PAGE_FAIL:15]

Plugging into (12), we get

\[\Pr[j\neq i]\geq\frac{1}{4}\big{(}1-d_{\mathrm{H}}^{2}(D_{1},D_{2}) \big{)}^{2T}.\]

When \(d_{\mathrm{H}}^{2}(D_{1},D_{2})<\frac{1}{2}\), we use the inequality \(1-x\geq e^{-2x}\) for \(0<x<\frac{1}{2}\) to conclude that

\[\Pr[j\neq i]\geq\frac{1}{4}\big{(}e^{-2d_{\mathrm{H}}^{2}(D_{1}, D_{2})}\big{)}^{2T}=\frac{1}{4}e^{-4Td_{\mathrm{H}}^{2}(D_{1},D_{2})}.\qed\]

## Appendix B Special Cases: Strongly and Weakly Informative Experts

In this section we investigate two special families of conditionally independent distributions that admit \(O(\frac{1}{\varepsilon}\log\frac{1}{\delta})\) sample complexity, which is smaller than the \(O(\frac{1}{\varepsilon^{2}}\log\frac{1}{\varepsilon\delta})\) bound for general conditionally independent distributions. In these two cases, the (signals of) experts are either "very informative" or "very non-informative".

**Definition B.1**.: _Let \(\gamma\in[0,\infty]\) be a parameter. For an expert \(i\), we say its signal \(s_{i}\in\mathcal{S}_{i}\) is_

* \(\gamma\)-strongly informative _if either_ \(\frac{P(s_{i}|\omega=1)}{P(s_{i}|\omega=0)}\geq 1+\gamma\) _or_ \(\frac{P(s_{i}|\omega=1)}{P(s_{i}|\omega=0)}\leq\frac{1}{1+\gamma}\) _holds._
* \(\gamma\)-weakly informative _if_ \(\frac{1}{1+\gamma}\leq\frac{P(s_{i}|\omega=1)}{P(s_{i}|\omega=0)}\leq 1+\gamma\)_._

_An expert \(i\) is \(\gamma\)-strongly (or \(\gamma\)-weakly) informative if all of its signals in \(\mathcal{S}_{i}\) are \(\gamma\)-strongly (or \(\gamma\)-weakly) informative.7_

Footnote 7: We note that an expert can be neither \(\gamma\)-strongly informative nor \(\gamma\)-weakly informative for any \(\gamma\).

A signal \(s_{i}\) being \(\gamma\)-strongly (or \(\gamma\)-weakly) informative implies that its corresponding report \(r_{i}\) will be "\(\gamma\)-away from" (or "\(\gamma\)-close to") the prior \(p=P(\omega=1)\), in the "\(\frac{r_{i}}{1-r_{i}}\) and \(\frac{p}{1-p}\)" ratio form. Specifically, if \(s_{i}\) is \(\gamma\)-strongly informative, then from Equation (1) we have

\[\tfrac{r_{i}}{1-r_{i}}=\tfrac{P(s_{i}|\omega=1)}{P(s_{i}|\omega= 0)}\tfrac{p}{1-p}\ \geq(1+\gamma)\rho\ \ \text{ or }\ \leq\tfrac{1}{1+\gamma}\rho. \tag{13}\]

As \(\gamma\) gets larger, a \(\gamma\)-strongly informative signal (expert) is _more_ informative for predicting whether \(\omega=1\) or \(0\). This would make aggregation easier. If \(s_{i}\) is \(\gamma\)-weakly informative, then:

\[\tfrac{1}{1+\gamma}\rho\ \leq\ \tfrac{r_{i}}{1-r_{i}}\ \leq\ (1+\gamma)\rho. \tag{14}\]

As \(\gamma\) gets smaller, a \(\gamma\)-weakly informative signal (expert) is _less_ informative for predicting \(\omega\), but in this case their report \(r_{i}\) will be close to the prior \(p\), which allows us to estimate the \(\rho^{n-1}\) term in the optimal aggregator \(f^{*}(\mathbf{r})=\frac{1}{1+\rho^{n-1}\prod_{i=1}^{n-r_{i}}\frac{1-r_{i}}{r_{i}}}\) better. Those are some intuitions why both strongly and weakly informative signals can reduce the sample complexity of forecast aggregation.

Formally, for \(\gamma\)-strongly informative experts with not-too-small \(\gamma\), we have the following result:

**Theorem B.2**.: _If \(n\geq 32\log\frac{2}{\varepsilon}\) and all experts are \(\gamma\)-strongly informative with \(\frac{\gamma}{1+\gamma}\geq 8\sqrt{\frac{2}{n}\log\frac{2}{\varepsilon}}\), then the sample complexity of forecast aggregation is \(\leq O(\frac{1}{\varepsilon n(\frac{1}{1+\gamma})^{2}}\log\frac{1}{\delta}+ \frac{1}{\varepsilon}\log\frac{1}{\delta})=O(\frac{1}{\varepsilon}\log\frac{1 }{\delta})\)._

We remark that the conditions of the theorem, \(n\geq 32\log\frac{2}{\varepsilon}\) and \(\frac{\gamma}{1+\gamma}\geq 8\sqrt{\frac{2}{n}\log\frac{2}{\varepsilon}}\), are easier to be satisfied when the number of experts \(n\) increases, if the informativeness parameter \(\gamma\) of each expert is a constant or does not decrease with \(n\) (which we believe is a reasonable assumption given that experts are independent of each other). Also, if \(\gamma\) is fixed or increasing, then as \(n\) increases the sample complexity decreases.

The proof of Theorem B.2 is in Appendix G.1. Roughly speaking, we divide each expert's signal set into two sets, \(\mathcal{S}_{i}^{1}\) and \(\mathcal{S}_{i}^{0}\): signals that are more likely to occur under \(\omega=1\) (i.e., \(\frac{P(s_{i}|\omega=1)}{P(s_{i}|\omega=0)}\geq 1+\gamma\)) and under \(\omega=0\) (i.e., \(\frac{P(s_{i}|\omega=1)}{P(s_{i}|\omega=0)}\leq\frac{1}{1+\gamma}\)). If the realized \(\omega\) is \(1\), then one may expect to see \(\Omega\big{(}\frac{\gamma}{1+\gamma}n\big{)}\) more \(\mathcal{S}_{i}^{1}\) signals than the \(\mathcal{S}_{i}^{0}\) signals from the \(n\) experts, because the probabilities of these two types of signals differ by \(\Omega(\frac{\gamma}{1+\gamma})\) for each expert. If \(\omega\) is \(0\) then one may expect to see more \(\mathcal{S}_{i}^{0}\) signals than the \(\mathcal{S}_{i}^{1}\) signals. So, by checking which type of signals are more we can tell whether \(\omega=0\) or \(1\). To tell whether a signal belongs to \(\mathcal{S}_{i}^{1}\) or \(\mathcal{S}_{i}^{0}\), we compare the corresponding report \(\frac{r_{i}}{1-r_{i}}\) with \(\rho\) (namely, \(\frac{r_{i}}{1-r_{i}}\geq(1+\gamma)\rho\iff s_{i}\in\mathcal{S}_{i}^{1}\) and \(\frac{r_{i}}{1-r_{i}}\leq\frac{1}{1+\gamma}\rho\iff s_{i}\in\mathcal{S}_{i}^{0}\)), where \(\rho\) is estimated with accuracy \(\frac{\gamma}{1+\gamma}\) using \(O(\frac{1}{\rho n(\frac{1}{1+\gamma})^{2}}\log\frac{1}{\delta})\) samples. Dealing with the case of \(\rho<\varepsilon\) separately, we obtain the bound \(O(\frac{1}{\varepsilon n(\frac{1}{1+\gamma})^{2}}\log\frac{1}{\delta}+\frac{1 }{\varepsilon}\log\frac{1}{\delta})\).

For \(\gamma\)-weakly informative experts with small \(\gamma\), we have:

**Theorem B.3**.: _If all experts are \(\gamma\)-weakly informative with \(\gamma\leq 1\), then the sample complexity of forecast aggregation is \(\leq\min\left\{O(\frac{m}{\varepsilon}\log\frac{1}{\delta}),O(\frac{1}{ \varepsilon^{2}}\log\frac{1}{\varepsilon\delta})\right\}\), which is \(O(\frac{1}{\varepsilon}\log\frac{1}{\delta})\) if \(\gamma=O(\frac{1}{n})\)._

The \(O(\frac{1}{\varepsilon^{2}}\log\frac{1}{\varepsilon\delta})\) term in the sample complexity follows from the result for general conditionally independent distributions (Theorem 5.2). The \(O(\frac{m}{\varepsilon}\log\frac{1}{\delta})\) term is proved in Appendix G.2. We give the rough idea here using \(\gamma=\frac{1}{n}\) as an example. The proof relies on the observation that \(\mathbb{E}\big{[}\prod_{i=1}^{n}\frac{r_{i}}{1-r_{i}}\mid\omega=0\big{]}=\rho^ {n}\). Since experts are weakly informative, each of their reports \(\frac{r_{i}}{1-r_{i}}\) is around \(\rho\) in the range \([\frac{1}{1+\gamma}\rho,(1+\gamma)\rho]\subseteq[\frac{1}{1+\frac{1}{n}}\rho,( 1+\frac{1}{n})\rho]\). Taking the product, we have \(\prod_{i=1}^{n}\frac{r_{i}}{1-r_{i}}\in[\frac{\rho^{n}}{\varepsilon},e\rho^{n}]\), which is in a bounded range. This allows us to use Chernoff bound to argue that the \(\rho^{n}\) (or \(\rho^{n-1}\)) term in the optimal aggregator \(f^{*}(\mathbf{r})=\frac{1}{1+\rho^{n-1}\prod_{i=1}^{n}\frac{1-r_{i}}{r_{i}}}\) can be estimated, with \(O(\sqrt{\varepsilon})\) accuracy, using only \(O(\frac{\varepsilon}{(\sqrt{\varepsilon})^{2}}\log\frac{1}{\delta})=O(\frac{1 }{\varepsilon}\log\frac{1}{\delta})\) samples of \(\prod_{i=1}^{n}\frac{r_{i}}{1-r_{i}}\). The aggregator using the estimate \(\hat{\rho}^{n-1}\), \(\hat{f}(\mathbf{r})=\frac{1}{1+\hat{\rho}^{n-1}\prod_{i=1}^{n}\frac{1-r_{i}}{r_{i}}}\), turns out to be \(O(\varepsilon)\)-optimal.

## Appendix C Extension: Multi-Outcome Events

In this section we generalize our model from binary events to multi-outcome events. The event space now becomes \(\Omega=\{1,2,\ldots,|\Omega|\}\) with \(|\Omega|>2\). The joint distribution of event \(\omega\in\Omega\) and experts' signals \(\mathbf{s}=(s_{i})_{i=1}^{n}\in\mathbf{\mathcal{S}}\) is still denoted by \(P\), which belongs to some class of distributions \(\mathcal{P}\). The size of each expert's signal space is still assumed to be \(|\mathcal{S}_{i}|=m<+\infty\) for general distributions and can be infinite for conditionally independent distributions (where \(s_{1},\ldots,s_{n}\) are independent conditioned on \(\omega\)). After observing signal \(s_{i}\), expert \(i\) reports its posterior belief of the event given \(s_{i}\), which is \(\mathbf{r}_{i}=(r_{ij})_{j\in\Omega}\) where \(r_{ij}=P(\omega=j\,|\,s_{i})\). An aggregator now is a vector-valued function \(\mathbf{f}=(f_{j})_{j\in\Omega}\) that maps the joint report \(\mathbf{r}=(\mathbf{r}_{i})_{i=1}^{n}=(r_{ij})_{ij}\) to a probability distribution \(\mathbf{f}(\mathbf{r})\) over \(\Omega\), where \(f_{j}(\mathbf{r})\) is the aggregated predicted probability for \(\omega=j\). We assume \(f_{j}(\mathbf{r})\geq 0\) and \(\sum_{j\in\Omega}f_{j}(\mathbf{r})=1\). The definition of the (expected) loss of an aggregator \(\mathbf{f}\) becomes:

\[L_{P}(\mathbf{f})=\mathbb{E}_{P}\Big{[}\,\sum_{j\in\Omega}\big{|}\,f_{j}(\mathbf{r})- \mathbb{1}\,[\omega=j]\,\big{|}^{2}\,\Big{]}. \tag{15}\]

It is easy to see that the optimal aggregator \(\mathbf{f}^{*}=\operatorname*{argmin}_{\mathbf{f}}L_{P}(\mathbf{f})\) in the multi-outcome case is still the Bayesian aggregator (this is a generalization of Lemma 2.1):

\[\mathbf{f}^{*}=(f_{j}^{*})_{j\in\Omega},\quad f_{j}^{*}(\mathbf{r})=P(\omega=j\mid\mathbf{ r}) \tag{16}\]

and the difference between the losses of \(\mathbf{f}\) and \(\mathbf{f}^{*}\) satisfies

\[L_{P}(\mathbf{f})-L_{P}(\mathbf{f}^{*})=\mathbb{E}_{P}\Big{[}\,\sum_{j\in\Omega}\big{|} \,f_{j}(\mathbf{r})-f_{j}^{*}(\mathbf{r})\,\big{|}^{2}\,\Big{]}. \tag{17}\]

So, an \(\varepsilon\)-optimal aggregator \(\mathbf{f}\) is an aggregator that satisfies \(\mathbb{E}_{P}\big{[}\sum_{j\in\Omega}|f_{j}(\mathbf{r})-f_{j}^{*}(\mathbf{r})|^{2} \big{]}\leq\varepsilon\).

Using \(T=T_{\mathcal{P}}(\varepsilon,\delta)\) samples \(\{(\mathbf{r}^{(t)},\omega^{(t)})\}_{t=1}^{T}\) from \(P\), the principal wants to find an \(\varepsilon\)-optimal aggregator \(\hat{\mathbf{f}}\) with probability at least \(1-\delta\), for any distribution \(P\) in a class \(\mathcal{P}\). We give lower bounds and upper bounds on the sample complexity:

**Theorem C.1**.: _The sample complexity of forecast aggregation for multi-outcome events is:_

* _For the class_ \(\mathcal{P}\) _of general distributions:_ \(\Omega\big{(}\frac{m^{n-2}+\log(1/\delta)}{\varepsilon}\big{)}\leq T_{\mathcal{ P}}(\varepsilon,\delta)\leq O\big{(}\frac{|\Omega|m^{n}+\log(1/\delta)}{\varepsilon^{2}} \big{)}\)_._
* _For the class_ \(\mathcal{P}_{\mathrm{ind}}\) _of conditionally independent distributions:_ \(\Omega\big{(}\frac{1}{\varepsilon}\log\frac{1}{\delta}\big{)}\leq T_{\mathcal{ P}_{\mathrm{ind}}}(\varepsilon,\delta)\leq O\big{(}\frac{|\Omega|\log|\Omega|}{\varepsilon^{2}} \log\frac{1}{\varepsilon}+\frac{1}{\varepsilon^{2}}\log\frac{1}{\delta}\big{)}\)_._

### Proof of Theorem c.1

Before proving the theorem, we note that the loss of any aggregator \(\mathbf{f}\) satisfying \(f_{j}(\mathbf{r})\geq 0\) and \(\sum_{j\in\Omega}f_{j}(\mathbf{r})=1\) is bounded by \([0,2]\):

\[0\,\leq\,\sum_{j\in\Omega}\big{|}f_{j}(\mathbf{r})-\mathbb{1}\left[\omega=j\right] \big{|}^{2}\,\leq\,\sum_{j\in\Omega}\big{|}f_{j}(\mathbf{r})-\mathbb{1}\left[\omega =j\right]\big{|}\,\leq\,1+\sum_{j\in\Omega}f_{j}(\mathbf{r})\,=\,2. \tag{18}\]

Similarly,

\[0\,\leq\,\sum_{j\in\Omega}\big{|}f_{j}(\mathbf{r})-f_{j}^{*}(\mathbf{r})\big{|}^{2}\, \leq\,2. \tag{19}\]

#### c.1.1 Lower Bounds

The lower bounds directly follow from the lower bounds for the binary case (Theorem 4.1 and Theorem 5.2) because the binary case is a special case of the multi-outcome case. Specifically, we can regard any binary event distribution \(P\) as a multi-outcome distribution that puts probability only on outcomes \(\{1,2\}\subseteq\Omega\). If we can learn an \(\varepsilon\)-optimal aggregator \(\hat{\mathbf{f}}\) for the multi-outcome case: \(\mathbb{E}\big{[}\sum_{j\in\Omega}|\hat{f}_{j}(\mathbf{r})-f_{j}^{*}(\mathbf{r})|^{2} \big{]}\leq\varepsilon\), then this aggregator satisfies

\[\mathbb{E}\Big{[}\sum_{j\in\{1,2\}}\big{|}\hat{f}_{j}(\mathbf{r})-f_{j}^{*}(\mathbf{r })\big{|}^{2}\Big{]}\,\leq\,\varepsilon\]

\[\iff\mathbb{E}\Big{[}\big{|}\hat{f}_{1}(\mathbf{r})-f_{1}^{*}(\mathbf{r})\big{|}^{2}+ \big{|}(1-\hat{f}_{1}(\mathbf{r}))-(1-f_{1}^{*}(\mathbf{r}))\big{|}^{2}\Big{]}\,=\, \mathbb{E}\Big{[}2\big{|}\hat{f}_{1}(\mathbf{r})-f_{1}^{*}(\mathbf{r})\big{|}^{2}\Big{]} \,\leq\,\varepsilon\]

\[\iff\mathbb{E}\Big{[}\big{|}\hat{f}_{1}(\mathbf{r})-f_{1}^{*}(\mathbf{r})\big{|}^{2} \Big{]}\,\leq\,\frac{\varepsilon}{2}.\]

So, the aggregator \(\hat{f}_{1}(\cdot)\) is an \(\frac{\varepsilon}{2}\)-optimal aggregator for the binary case.

#### c.1.2 Upper Bounds

General distributions: the \(O\big{(}\frac{|\Omega|m^{n}+\log(1/\delta)}{\varepsilon^{2}}\big{)}\) upper bound.The proof for general distributions in the multi-outcome case is the same as the proof for general distributions in the binary case (Section 4.1), except for three differences: (1) \(P\) (regarded as a distribution over reports \(\mathbf{r}\) and event \(\omega\)) now is a discrete distribution with support size at most \(|\Omega|m^{n}\); (2) the loss \(|f(\mathbf{r})-\omega|^{2}\in[0,1]\) in the binary case is replaced by the loss \(\sum_{j\in\Omega}|f_{j}(\mathbf{r})-\mathbb{1}\left[\omega=j\right]|^{2}\in[0,2]\) in the multi-outcome case; (3) the \(\varepsilon\) in the binary case is replaced by \(\frac{\varepsilon}{2}\) because the loss is now upper bounded by \(2\). Thus, the bound \(O\big{(}\frac{m^{n}+\log(1/\delta)}{\varepsilon^{2}}\big{)}\) in the binary case becomes \(O\big{(}\frac{|\Omega|m^{n}+\log(1/\delta)}{(\frac{\varepsilon}{2})^{2}}\big{)} =O\big{(}\frac{|\Omega|m^{n}+\log(1/\delta)}{\varepsilon^{2}}\big{)}\) in the multi-outcome case.

Conditionally independent distributions: the \(O\big{(}\frac{|\Omega|\log|\Omega|}{\varepsilon^{2}}\log\frac{1}{\varepsilon} +\frac{1}{\varepsilon^{2}}\log\frac{1}{\delta}\big{)}\) upper bound.The sample complexity upper bound for conditionally independent distributions is proved by a _pseudo-dimension_ argument. We first show in Lemma C.2 that the optimal aggregator \(\mathbf{f}^{*}\) belongs to some parametric family of aggregators. Then, we upper bound the pseudo-dimension of the loss functions associated with this parametric family of aggregators, which will give the desired sample complexity upper bound.

**Lemma C.2**.: _For multi-outcome conditionally independent distribution \(P\), given signals \(\mathbf{s}=(s_{i})_{i=1}^{n}\), with corresponding reports \(\mathbf{r}=(\mathbf{r}_{i})_{i=1}^{n}=(r_{ij})_{ij}\) where \(r_{ij}=P(\omega=j\,|\,s_{i})\), the posterior probability of \(\omega=j\) satisfies:_

\[f_{j}^{*}(\mathbf{r})=P(\omega=j\;|\;\mathbf{r})=P(\omega=j\;|\;\mathbf{s})=\frac{\frac{1}{P (\omega=j)^{n-1}}\prod_{i=1}^{n}r_{ij}}{\sum_{k\in\Omega}\frac{1}{P(\omega=k) ^{n-1}}\prod_{i=1}^{n}r_{ik}}.\]Proof.: By Bayes' rule and the fact that \(s_{1},\ldots,s_{n}\) are independent conditioning on \(\omega\),

\[P(\omega=j\mid\mathbf{s}) =\frac{P(\omega=j)\prod_{i=1}^{n}P(s_{i}\mid\omega=j)}{\sum_{k\in \Omega}P(\omega=k)\prod_{i=1}^{n}P(s_{i}\mid\omega=k)}\] \[=\frac{\frac{1}{P(\omega=j)^{n-1}}\prod_{i=1}^{n}P(\omega=j)P(s_{i }\mid\omega=j)}{\sum_{k\in\Omega}\frac{1}{P(\omega=k)^{n-1}}\prod_{i=1}^{n}P( \omega=k)P(s_{i}\mid\omega=k)}\] \[=\frac{\frac{1}{P(\omega=j)^{n-1}}\prod_{i=1}^{n}\frac{P(\omega=j )P(s_{i}\mid\omega=j)}{P(s_{i})}}{\sum_{k\in\Omega}\frac{1}{P(\omega=k)^{n-1}} \prod_{i=1}^{n}\frac{P(\omega=k)P(s_{i}\mid\omega=k)}{P(s_{i})}}\] \[=\frac{\frac{1}{P(\omega=j)^{n-1}}\prod_{i=1}^{n}r_{ij}}{\sum_{k \in\Omega}\frac{1}{P(\omega=k)^{n-1}}\prod_{i=1}^{n}r_{ik}}.\]

Since the above expression depends only on \(r_{i}\)'s but not on \(s_{i}\)'s, we have:

\[f_{j}^{*}(\mathbf{r})=P(\omega=j\mid\mathbf{r})=P(\omega=j\mid\mathbf{s})=\frac{\frac{1}{P (\omega=j)^{n-1}}\prod_{i=1}^{n}r_{ij}}{\sum_{k\in\Omega}\frac{1}{P(\omega=k)^ {n-1}}\prod_{i=1}^{n}r_{ik}}.\]

(For the special binary event case,

\[f^{*}(\mathbf{r})=P(\omega=1\,|\,\mathbf{r})=\frac{\frac{1}{P(\omega=1)^{n-1}}\prod_{i =1}^{n}r_{i}}{\frac{1}{P(\omega=1)^{n-1}}\prod_{i=1}^{n}r_{i}+\frac{1}{P( \omega=0)^{n-1}}\prod_{i=1}^{n}(1-r_{i})}=\frac{1}{1+(\frac{P(\omega=1)}{P( \omega=0)})^{n-1}\prod_{i=1}^{n}\frac{1-r_{i}}{r_{i}}}.\]

This proves Lemma 5.1.) 

According to Lemma C.2, the optimal aggregator \(\mathbf{f}^{*}=(f_{j})_{j\in\Omega}\) belongs to the following family of aggregators, parameterized by \(\mathbf{\theta}=(\theta_{j})_{j\in\Omega}\in\mathbb{R}_{+}^{|\Omega|}\):

\[\mathcal{F}=\bigg{\{}\mathbf{f}^{\mathbf{\theta}}=(f_{j}^{\mathbf{\theta}})_{j\in\Omega} \biggm{|}f_{j}^{\mathbf{\theta}}(\mathbf{r})=\frac{\theta_{j}\prod_{i=1}^{n}r_{ij}}{ \sum_{k\in\Omega}\theta_{k}\prod_{i=1}^{n}r_{ik}}\bigg{\}} \tag{20}\]

(The optimal aggregator \(\mathbf{f}^{*}\) has parameter \(\theta_{j}=\frac{1}{P(\omega=j)^{n-1}}\).) Let \(\mathcal{G}\) be the family of loss functions associated with \(\mathcal{F}\), which is also parameterized by \(\mathbf{\theta}\in\mathbb{R}_{+}^{|\Omega|}\),

\[\mathcal{G}=\bigg{\{}g^{\mathbf{\theta}}\biggm{|}g^{\mathbf{\theta}}(\mathbf{r},\omega)= \sum_{j\in\Omega}\big{|}f_{j}^{\mathbf{\theta}}(\mathbf{r})-\mathbb{1}\big{[}\omega= j\big{]}\big{|}^{2}\bigg{\}}. \tag{21}\]

We recall the definition of pseudo-dimension of a family of functions:

**Definition C.3** (e.g., [1]).: _Let \(\mathcal{H}\) be a family of functions from input space \(\mathcal{X}\) to real numbers \(\mathbb{R}\). Let \(x^{(1)},\ldots,x^{(d)}\in\mathcal{X}\) be \(d\) inputs. Let \(t^{(1)},\ldots,t^{(d)}\in\mathbb{R}\) be \(d\) "thresholds". Let \(\mathbf{b}=(b^{(1)},\ldots,b^{(d)})\in\{-1,+1\}^{d}\) be a vector of labels. We say \(\mathbf{b}\) can be generated by \(\mathcal{H}\) (on inputs \(x^{(1)},\ldots,x^{(d)}\) with thresholds \(t^{(1)},\ldots,t^{(d)}\)) if there exists a function \(h_{\mathbf{b}}\in\mathcal{H}\) such that \(h_{\mathbf{b}}(x^{(i)})>t^{(i)}\) if \(b^{(i)}=1\) and \(h_{\mathbf{b}}(x^{(i)})<t^{(i)}\) if \(b^{(i)}=-1\) (namely, \(\operatorname{sign}(h_{\mathbf{b}}(x^{(i)})-t^{(i)})=b^{(i)}\)) for every \(i\in\{1,\ldots,d\}\). The pseudo-dimension of \(\mathcal{H}\), \(\operatorname{Pdim}(\mathcal{H})\), is the largest integer \(d\) for which there exist \(d\) inputs and \(d\) thresholds such that all the \(2^{d}\) label vectors in \(\{-1,+1\}^{d}\) can be generated by \(\mathcal{H}\)._

Pseudo-dimension gives a sample complexity upper bound for the uniform convergence of the empirical means of a family of functions to their true means:

**Theorem C.4** (e.g., [1]).: _Let \(\mathcal{H}\) be a family of functions from \(\mathcal{X}\) to \([0,U]\) with \(\operatorname{Pdim}(\mathcal{H})=d\). For any distribution \(\mathcal{D}\) on \(\mathcal{X}\), with probability at least \(1-\delta\) over the random draw of_

\[T=O\bigg{(}\frac{U^{2}}{\varepsilon^{2}}\Big{(}d\cdot\log\frac{U}{\varepsilon} +\log\frac{1}{\delta}\Big{)}\bigg{)}\]

_samples \(x^{(1)},\ldots,x^{(T)}\) from \(\mathcal{D}\), we have: for every \(h\in\mathcal{H}\), \(\big{|}\mathbb{E}_{x\sim\mathcal{D}}[h(x)]-\frac{1}{T}\sum_{t=1}^{T}h(x^{(t)}) \big{|}\leq\varepsilon\)._

We now upper bound the pseudo-dimension of \(\mathcal{G}\):

**Lemma C.5**.: \(\operatorname{Pdim}(\mathcal{G})\leq O\big{(}|\Omega|\log|\Omega|\big{)}\)_._

Proof.: Suppose \(\operatorname{Pdim}(\mathcal{G})=d\). By definition, there exist \(d\) inputs \((\mathbf{r}^{(1)},\omega^{(1)}),\ldots,(\mathbf{r}^{(d)},\omega^{(d)})\) and \(d\) thresholds \(t^{(1)},\ldots,t^{(d)}\in\mathbb{R}\) such that all the \(2^{d}\) label vectors \(\mathbf{b}\in\{-1,+1\}^{d}\) can be generated by some function \(g^{\mathbf{\theta_{b}}}\in\mathcal{G}\). We will count how many label vectors can actually be generated by \(\mathcal{G}\).

Consider the \(\ell\)-th input \((\mathbf{r}^{(\ell)},\omega^{(\ell)})\) and threshold \(t^{(\ell)}\). To simplify notations we omit superscript \((\ell)\), so we have \((\mathbf{r},\omega)\) and \(t\). We write \(\mathbf{x}=(x_{j})_{j\in\Omega}\) where \(x_{j}=\prod_{i=1}^{n}r_{ij}\). For any function \(g^{\mathbf{\theta}}\in\mathcal{G}\), we have

\[g^{\mathbf{\theta}}(\mathbf{r},\omega) =\sum_{j\in\Omega}\Big{|}\frac{\theta_{j}x_{j}}{\sum_{k\in\Omega} \theta_{k}x_{k}}-\operatorname{\mathbb{1}}[\omega=j]\Big{|}^{2}\] \[=\sum_{j\in\Omega}\Big{(}\frac{\theta_{j}x_{j}}{\sum_{k\in\Omega} \theta_{k}x_{k}}\Big{)}^{2}\ -\ 2\frac{\theta_{\omega}x_{\omega}}{\sum_{k\in\Omega} \theta_{k}x_{k}}\ +\ 1\] \[=\frac{1}{\big{(}\sum_{k\in\Omega}\theta_{k}x_{k}\big{)}^{2}} \Big{[}\sum_{j\in\Omega}(\theta_{j}x_{j})^{2}\ -\ 2\theta_{\omega}x_{\omega} \sum_{k\in\Omega}\theta_{k}x_{k}\ +\ \Big{(}\sum_{k\in\Omega}\theta_{k}x_{k} \Big{)}^{2}\Big{]}.\]

By definition, the set of parameters that give input \((\mathbf{r},\omega)\) label "\(+1\)" is \(\big{\{}\mathbf{\theta}\in\mathbb{R}^{|\Omega|}_{+}\mid g^{\mathbf{\theta}}(\mathbf{r}, \omega)>t\big{\}}\). By the above equation, this set is equal to the set

\[\bigg{\{}\mathbf{\theta}\in\mathbb{R}^{|\Omega|}_{+}\ \bigg{|}\ \sum_{j\in\Omega}( \theta_{j}x_{j})^{2}\ -\ 2\theta_{\omega}x_{\omega}\sum_{k\in\Omega}\theta_{k}x_{k} \ +\ \Big{(}\sum_{k\in\Omega}\theta_{k}x_{k}\Big{)}^{2}>t\Big{(}\sum_{k\in \Omega}\theta_{k}x_{k}\Big{)}^{2}\bigg{\}}.\]

We note that the above set is the solution set to the quadratic form inequality \(\mathbf{\theta}^{\top}A\mathbf{\theta}>0\) for some matrix \(A\in\mathbb{R}^{|\Omega|\times|\Omega|}\). Similarly, the set of parameters that give input \((\mathbf{r},\omega)\) label "\(-1\)" is the solution set to \(\mathbf{\theta}^{\top}A\mathbf{\theta}<0\). These two sets share a boundary: the solution set to \(\mathbf{\theta}^{\top}A\mathbf{\theta}=0\), which is a hyper-ellipsoid in \(\mathbb{R}^{|\Omega|}\). In other words, the input \((\mathbf{r},\omega)\) and threshold \(t\) define a hyper-ellipsoid which divides the parameter space \(\mathbb{R}^{|\Omega|}_{+}\) into two regions such that all the parameters in one region generate the same label for that input.

Enumerating all inputs \((r^{(1)},\omega^{(1)}),\ldots,(r^{(d)},\omega^{(d)})\). They define \(d\) hyper-ellipsoids in total, dividing the parameter space \(\mathbb{R}^{|\Omega|}_{+}\) into several regions. Within each region, all the parameters generate the same label for each input and hence generate the same label vector. So, the number of label vectors that can be generated by all the parameters in \(\mathbb{R}^{|\Omega|}_{+}\) is upper bounded by the number of regions. The number of regions divided by \(d\) hyper-ellipsoids in \(\mathbb{R}^{|\Omega|}_{+}\) is in the order of \(O(d^{|\Omega|})\). Hence, to generate all the \(2^{d}\) label vectors we need \(O(d^{|\Omega|})\geq 2^{d}\). This gives \(d\leq O(|\Omega|\log|\Omega|)\). 

By Theorem C.4 and Lemma C.5, plugging in \(d=\operatorname{Pdim}(\mathcal{G})\leq O(|\Omega|\log|\Omega|)\) and \(U=2\) (because \(g^{\mathbf{\theta}}(\mathbf{r},\omega)\) is bounded by \([0,2]\) according to (18)), we obtain the following: with probability at least \(1-\delta\) over the random draw of

\[T=O\bigg{(}\frac{U^{2}}{\varepsilon^{2}}\Big{(}d\cdot\log\frac{U}{\varepsilon}+ \log\frac{1}{\delta}\Big{)}\bigg{)}=O\bigg{(}\frac{|\Omega|\log|\Omega|}{ \varepsilon^{2}}\log\frac{1}{\varepsilon}+\frac{1}{\varepsilon^{2}}\log \frac{1}{\delta}\bigg{)}\]

samples \((\mathbf{r}^{(1)},\omega^{(1)}),\ldots,(\mathbf{r}^{(T)},\omega^{(T)})\) from \(P\), we have for any \(g^{\mathbf{\theta}}\in\mathcal{G}\),

\[\Big{|}\mathbb{E}_{P}\big{[}g^{\mathbf{\theta}}(\mathbf{r},\omega)\big{]}-\frac{1}{T} \sum_{t=1}^{T}g^{\mathbf{\theta}}(\mathbf{r}^{(t)},\omega^{(t)})\Big{|}\leq\varepsilon.\]

This is equivalent to for any \(\mathbf{f}^{\mathbf{\theta}}\in\mathcal{F}\),

\[\bigg{|}\ \mathbb{E}_{P}\Big{[}\sum_{j\in\Omega}\big{|}f^{\mathbf{\theta}}_{ j}(\mathbf{r})-\operatorname{\mathbb{1}}[\omega=j]\big{|}^{2}\Big{]}-\frac{1}{T} \sum_{t=1}^{T}\sum_{j\in\Omega}\big{|}f^{\mathbf{\theta}}_{j}(\mathbf{r}^{(t)})- \operatorname{\mathbb{1}}[\omega^{(t)}=1]\big{|}^{2}\ \bigg{|}\leq\varepsilon\]

where \(\hat{P}_{\rm emp}\) is the empirical distribution \(\operatorname{Uniform}\big{\{}(\mathbf{r}^{(1)},\omega^{(1)}),\ldots,(\mathbf{r}^{(T)}, \omega^{(T)})\big{\}}\). By the same logic as the proof of the upper bound in Theorem 4.1 (Section 4.1), the empirically optimal aggregator \(\hat{\mathbf{f}}^{*}=\operatorname{argmin}_{\mathbf{f}^{\mathbf{\theta}}\in\mathcal{F}}L_{ \hat{P}_{\rm emp}}(\mathbf{f}^{\mathbf{\theta}})\) is \(2\varepsilon\)-optimal.

Missing Proofs from Section 2

### Proof of Lemma 2.1

To prove the first item \(f^{*}(\mathbf{r})=P(\omega=1\,|\,\mathbf{r})\), we simply note that

\[f^{*}=\operatorname*{argmin}_{f}\mathbb{E}_{\mathbf{r}}\Big{[}\mathbb{E}\big{[}|f( \mathbf{r})-\omega|^{2}\ |\ \mathbf{r}\big{]}\Big{]}\]

should minimize \(\mathbb{E}\big{[}|f(\mathbf{r})-\omega|^{2}\ |\ \mathbf{r}\big{]}\) for almost every \(\mathbf{r}\). This gives \(f^{*}(\mathbf{r})=\mathbb{E}[\omega\,|\,\mathbf{r}]=P(\omega=1\,|\,\mathbf{r})\).

To prove the second item \(L_{P}(f)-L_{P}(f^{*})=\mathbb{E}_{P}\big{[}|f(\mathbf{r})-f^{*}(\mathbf{r})|^{2}\big{]}\), we note that, by the definition of \(L_{P}(\cdot)\) and the fact that \(f^{*}(\mathbf{r})=\mathbb{E}[\omega\,|\,\mathbf{r}]\) proven above,

\[L_{P}(f)-L_{P}(f^{*}) =\mathbb{E}\big{[}|f(\mathbf{r})-\omega|^{2}\big{]}-\mathbb{E}\big{[} |f^{*}(\mathbf{r})-\omega|^{2}\big{]}\] \[=\mathbb{E}\big{[}f(\mathbf{r})^{2}\big{]}-2\mathbb{E}_{\mathbf{r}}\big{[} f(\mathbf{r})\mathbb{E}[\omega|\mathbf{r}]\big{]}-\mathbb{E}\big{[}f^{*}(\mathbf{r})^{2} \big{]}+2\mathbb{E}_{\mathbf{r}}\big{[}f^{*}(\mathbf{r})\mathbb{E}[\omega|\mathbf{r}]\big{]}\] \[=\mathbb{E}\big{[}f(\mathbf{r})^{2}\big{]}-2\mathbb{E}_{\mathbf{r}}\big{[} f(\mathbf{r})\mathbb{E}[\omega|\mathbf{r}]\big{]}-\mathbb{E}\big{[}f^{*}(\mathbf{r})^{2} \big{]}+2\mathbb{E}_{\mathbf{r}}\big{[}f^{*}(\mathbf{r})\mathbb{E}[\omega|\mathbf{r}]\big{]}\] \[=\mathbb{E}\big{[}f(\mathbf{r})^{2}\big{]}-2\mathbb{E}_{\mathbf{r}}\big{[} f(\mathbf{r})f^{*}(\mathbf{r})\big{]}-\mathbb{E}\big{[}f^{*}(\mathbf{r})^{2}\big{]}+2 \mathbb{E}_{\mathbf{r}}\big{[}f^{*}(\mathbf{r})^{2}\big{]}\] \[=\mathbb{E}\big{[}f(\mathbf{r})^{2}-2f(\mathbf{r})f^{*}(\mathbf{r})+f^{*}(\bm {r})^{2}\big{]}\] \[=\mathbb{E}\big{[}|f(\mathbf{r})-f^{*}(\mathbf{r})|^{2}\big{]}.\]

## Appendix E Missing Proofs from Section 4

### Proof of Lemma 4.3

Recall that we have a family \(\mathcal{D}\) of distributions over \(\mathbf{\mathcal{S}}=\mathcal{S}_{1}\times\cdots\times\mathcal{S}_{n}\) satisfying the three properties in Definition 4.2 (\(B\)-uniformly bounded, same marginal across distributions, and distinct marginals across signals). We have constructed from \(\mathcal{D}\) the family of distributions \(\mathcal{P}=\{P_{D}:D\in\mathcal{D}\}\) for the forecast aggregation problem as follows:

\[\begin{cases}P_{D}(\omega=0)=P_{D}(\omega=1)=\dfrac{1}{2},\\ P_{D}(\cdot\ |\ \omega=0)=\operatorname{Uniform}(\mathbf{\mathcal{S}}),&\text{ namely, }\ P_{D}(\mathbf{s}\mid\omega=0)=\dfrac{1}{|\mathbf{\mathcal{S}}|}=\dfrac{1}{m^{n}},\\ P_{D}(\cdot\ |\ \omega=1)=D,&\text{namely, }\ P_{D}(\mathbf{s}\mid\omega=1)=D(\mathbf{s}).\end{cases} \tag{22}\]

We want to show that \(\varepsilon\)-optimal aggregation with respect to \(\mathcal{P}\) will imply \((1+B)^{2}\sqrt{\varepsilon}\) total variation distance learning with respect to \(\mathcal{D}\), and hence \(T_{\mathcal{P}}(\varepsilon,\delta)\geq T_{\mathcal{D}}^{\operatorname{TV}} \big{(}(1+B)^{2}\sqrt{\varepsilon},\,\delta\big{)}\).

First, we have the following observations about \(\mathcal{D}\) and \(\mathcal{P}\):

**Fact E.1**.: _For a family of distributions \(\mathcal{D}\) that satisfies the three properties in Definition 4.2:_

1. _Given signal_ \(s_{i}\)_, expert_ \(i\)_'s report is_ \(r_{i}=P_{D}(\omega=1\ |\ s_{i})=\frac{D(s_{i})}{\frac{n}{n}+D(s_{i})}\)_, which is the same for all distributions_ \(D\in\mathcal{D}\)_._
2. _For every expert_ \(i\)_, given different signals_ \(s_{i}\neq s_{i}^{\prime}\)_, its reports_ \(r_{i}\neq r_{i}^{\prime}\)_. So, there is a one-to-one mapping between_ \(s_{i}\) _and_ \(r_{i}\) _for every_ \(i\in\{1,\dots,n\}\) _and also a one-to-one mapping between the joint signal_ \(\mathbf{s}=(s_{1},\dots,s_{n})\) _and the joint report_ \(\mathbf{r}=(r_{1},\dots,r_{n})\)_._
3. _For any joint signal_ \(\mathbf{s}\)_, with corresponding joint report_ \(\mathbf{r}\)_, we have_ \(D(\mathbf{s})=\frac{f^{*}(\mathbf{r})}{m^{n}(1-f^{*}(\mathbf{r}))}\)_._

Proof.: We prove the three items one by one:

1. By definition, the marginal distribution of joint signal, \(P_{D}(\mathbf{s})\), is \[P_{D}(\mathbf{s})=P_{D}(\omega=0)P_{D}(\mathbf{s}\mid\omega=0)+P_{D}(\omega=1)P_{D}( \mathbf{s}\mid\omega=1)=\dfrac{1}{2}\Big{(}\dfrac{1}{m^{n}}+D(\mathbf{s})\Big{)}.\] (23)Fixing \(s_{i}\), summing over \(\mathbf{s}_{-i}=(s_{1},\ldots,s_{i-1},s_{i+1},\ldots,s_{n})\in\mathbf{\mathcal{S}}_{-i}\), we get \[P_{D}(s_{i})=\frac{1}{2}\Big{(}\frac{|\mathbf{\mathcal{S}}_{-i}|}{m^{n}}+\sum_{\mathbf{s }_{-i}\in\mathbf{\mathcal{S}}_{-i}}D(\mathbf{s})\Big{)}=\frac{1}{2}\Big{(}\frac{1}{m}+ D(s_{i})\Big{)}.\] So, given signal \(s_{i}\), expert \(i\) reports \[r_{i}=P_{D}(\omega=1\mid s_{i})=\frac{P_{z}(\omega=1)P_{D}(s_{i}\mid\omega=1)} {P_{D}(s_{i})}=\frac{D(s_{i})}{\frac{1}{m}+D(s_{i})},\] (24) and this is the same for all \(D\in\mathcal{D}\) since \(D(s_{i})=D^{\prime}(s_{i})\) by the "same marginal across distributions" property.
2. Given \(s_{i}\neq s^{\prime}_{i}\), by the "distinct marginals across signals" property, we have \(D(s_{i})\neq D(s^{\prime}_{i})\). Since \(r_{i}=\frac{D(s_{i})}{\frac{1}{m}+D(s_{i})}\) and \(\frac{x}{\frac{1}{m}+x}\) is a strictly increasing function of \(x\), it follows that \(r_{i}\neq r^{\prime}_{i}\).
3. According to item 2, there is a one-to-one mapping between \(\mathbf{s}=(s_{1},\ldots,s_{n})\) and \(\mathbf{r}=(r_{1},\ldots,r_{n})\); in other words, observing signals \(s_{1},\ldots,s_{n}\) is equivalent to observing reports \(r_{1},\ldots,r_{n}\). Therefore, by Bayes' rule we have \[f^{*}(\mathbf{r})=P_{D}(\omega=1\mid\mathbf{r})=P_{D}(\omega=1\mid\mathbf{s})= \frac{P_{D}(\omega=1)P_{D}(\mathbf{s}\mid\omega=1)}{P_{D}(\mathbf{s})}\] by (22) and (23) \[=\frac{D(\mathbf{s})}{\frac{1}{m^{n}}+D(\mathbf{s})}.\] (25) Rearranging, we obtain \(D(\mathbf{s})=\frac{f^{*}(\mathbf{r})}{m^{n}(1-f^{*}(\mathbf{r}))}\). 

**Claim E.2**.: _If we have an aggregator \(\hat{f}\) that is \(\varepsilon\)-optimal with respect to \(P_{D}\), then we can find a distribution \(\hat{D}\) such that \(d_{\mathrm{TV}}(\hat{D},D)\leq(1+B)^{2}\sqrt{\varepsilon}\)._

Proof.: Because \(D\) is \(B\)-uniformly bounded, from (25) we can verify that \(f^{*}(\mathbf{r})\) satisfies \(f^{*}(\mathbf{r})\leq\frac{B}{1+B}\). So, we can assume \(\hat{f}(\mathbf{r})\leq\frac{B}{1+B}\) as well (if \(\hat{f}(\mathbf{r})>\frac{B}{1+B}\), we can let \(\hat{f}(\mathbf{r})\) be \(\frac{B}{1+B}\); this only reduces the approximation error \(\mathbb{E}\big{[}|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|^{2}\big{]}\)). Define \(\hat{D}\) by letting \(\hat{D}(\mathbf{s})=\frac{\hat{f}(\mathbf{r})}{m^{n}(1-f(\mathbf{r}))}\), \(\forall\mathbf{s}\in\mathbf{\mathcal{S}}\), where \(\mathbf{r}\) is the reports corresponding to \(\mathbf{s}\) (cf., Fact E.1). Then, \(d_{\mathrm{TV}}(\hat{D},D)\) is

\[d_{\mathrm{TV}}(\hat{D},D)=\tfrac{1}{2}\sum_{\mathbf{s}\in\mathbf{\mathcal{ S}}}\big{|}\hat{D}(\mathbf{s})-D(\mathbf{s})\big{|} =\tfrac{1}{2}\sum_{\mathbf{s}\in\mathbf{\mathcal{S}}}\big{|}\frac{\hat{f}( \mathbf{r})}{m^{n}(1-f(\mathbf{r}))}-\frac{f^{*}(\mathbf{r})}{m^{n}(1-f^{*}(\mathbf{r}))} \big{|}\] \[=\tfrac{1}{2m^{n}}\sum_{\mathbf{s}\in\mathbf{\mathcal{S}}}\big{|}\tfrac{ \hat{f}(\mathbf{r})}{1-\hat{f}(\mathbf{r})}-\tfrac{f^{*}(\mathbf{r})}{1-f^{*}(\mathbf{r})}\big{|}.\]

Because \(\hat{f}(\mathbf{r}),f^{*}(\mathbf{r})\leq\frac{B}{1+B}\) and the function \(\frac{x}{1-x}\) has derivative \(\frac{1}{(1-x)^{2}}\leq(1+B)^{2}\) when \(x\leq\frac{B}{1+B}\), we have

\[d_{\mathrm{TV}}(\hat{D},D) \leq\tfrac{1}{2m^{n}}(1+B)^{2}\sum_{\mathbf{s}\in\mathbf{\mathcal{S}}} \big{|}\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})\big{|}\] \[\text{by \eqref{eq:d_TV}}\leq(1+B)^{2}\sum_{\mathbf{s}\in\mathbf{\mathcal{ S}}}P_{D}(\mathbf{s})\big{|}\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})\big{|}\,=\,(1+B)^{2}\mathbb{E}_{P_{D}} \Big{[}\big{|}\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})\big{|}\Big{]}.\]

By Jensen's inequality \((\mathbb{E}[X])^{2}\leq\mathbb{E}[X^{2}]\) and by the assumption that \(\hat{f}\) is \(\varepsilon\)-optimal, we have \(\big{(}\mathbb{E}_{P_{D}}\big{[}|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|\big{]}\big{)}^{ 2}\leq\mathbb{E}_{P_{D}}\big{[}|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|^{2}\big{]}\leq\varepsilon\). Thus,

\[d_{\mathrm{TV}}(\hat{D},D)\leq(1+B)^{2}\sqrt{\varepsilon},\]

which proves the claim. 

Now, we present the reduction from learning \(\mathcal{D}\) in total variation distance to forecast aggregation for \(\mathcal{P}\). We use notations \(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(T)}\in\mathbf{\mathcal{S}}\) to represent the samples from \(D\). From \(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(T)}\) we construct the samples \((\mathbf{r}^{(1)},\omega^{(1)}),\ldots,(\mathbf{r}^{(T)},\omega^{(T)})\) for the forecast aggregation problem. After obtaining a solution \(\hat{f}\) to the latter problem, we convert it into a solution \(\hat{D}\) to the former. Details are as follows:

**Input:**\(T\) i.i.d. samples \(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(T)}\) from an unknown distribution \(D\in\mathcal{D}\).

**Reduction:**

Draw \(T\) samples \(\omega^{(1)},\ldots,\omega^{(T)}\sim\mathrm{Uniform}\{0,1\}\).

For each \(t=1,\ldots,T\), do the following:

If \(\omega^{(t)}=0\), draw \(\mathbf{s}^{(t)}\sim\mathrm{Uniform}(\mathbf{\mathcal{S}})\).

If \(\omega^{(t)}=1\), let \(\mathbf{s}^{(t)}=\mathbf{x}^{(t)}\).

For each \(i\), compute \(r_{i}^{(t)}=\frac{D(s_{i}^{(t)})}{\frac{1}{m}+D(s_{i}^{(t)})}\). Let \(\mathbf{r}^{(t)}=(r_{1}^{(t)},\ldots,r_{n}^{(t)})\).

Feed \(S_{T}=\big{\{}(\mathbf{r}^{(1)},\omega^{(1)}),\ldots,(\mathbf{r}^{(T)},\omega^{(T)}) \big{\}}\) to the forecast aggregation problem. Obtain solution \(\hat{f}\).

Convert \(\hat{f}\) into \(\hat{D}\) according to Claim E.2.

**Output:**\(\hat{D}\).

We remark that, in the second step of the reduction, the report \(r_{i}^{(t)}\) can be computed even if \(D\) is unknown, because the \(D(s_{i}^{(t)})\) is the same for all \(D\in\mathcal{D}\) and hence known.

Using the above reduction, we show that the sample complexity of \(\varepsilon\)-optimal forecast aggregation for \(\mathcal{P}\) cannot be smaller than the sample complexity of learning \(\mathcal{D}\) within total variation distance \((1+B)^{2}\sqrt{\varepsilon}\), which will prove Lemma 4.3:

**Proof of Lemma 4.3:** First, we verify that the distribution of samples \(S_{T}\) in the above reduction is exactly the distribution of \(T\) samples \(\{(\mathbf{r}^{(1)},\omega^{(1)}),\ldots,(\mathbf{r}^{(T)},\omega^{(T)})\}\) from \(P_{D}\). This is because: (1) the distribution of \(\omega^{(t)}\) is \(\mathrm{Uniform}\{0,1\}\), as defined in \(P_{D}\); (2) given \(\omega^{(t)}=0\), the distribution of \(\mathbf{s}^{(t)}\) is \(\mathrm{Uniform}(\mathbf{\mathcal{S}})\), as defined in \(P_{D}\); (3) given \(\omega^{(t)}=1\), the distribution of \(\mathbf{s}^{(t)}\) is the same as the distribution of \(\mathbf{x}^{(t)}\), which is \(D\), because the random draws of \(\omega^{(t)}\) and \(\mathbf{x}^{(t)}\) are independent; (4) according to Fact E.1, the report \(r_{i}^{(t)}=\frac{D(s_{i}^{(t)})}{\frac{1}{m}+D(s_{i}^{(t)})}=P_{D}(\omega=1 \mid s_{i}^{(t)})\), as desired.

Then, by the definition of sample complexity of forecast aggregation, if we are given \(T=T_{\mathcal{P}}(\varepsilon,\delta)\) samples \(S_{T}\) for the forecast aggregation problem, then with probability at least \(1-\delta\) we should be able to find an \(\varepsilon\)-optimal aggregator \(\hat{f}\) with respect to \(P_{D}\). According to Claim E.2, we can convert \(\hat{f}\) into a \(\hat{D}\) such that

\[d_{\mathrm{TV}}(\hat{D},D)\leq(1+B)^{2}\sqrt{\varepsilon}.\]

By the definition of sample complexity \(T_{\mathcal{D}}^{\mathrm{TV}}(\cdot,\delta)\) of distribution learning, \(T\) must be at least

\[T\geq T_{\mathcal{D}}^{\mathrm{TV}}\big{(}(1+B)^{2}\sqrt{\varepsilon},\,\delta \big{)},\]

which proves the lemma.

### Proof of Proposition 4.4

To prove Proposition 4.4 we will construct a family of distributions \(\mathcal{D}\) that satisfies the three properties in Definition 4.2 and requires \(T_{\mathcal{D}}^{\mathrm{TV}}(\varepsilon_{\mathrm{TV}},\delta)=\Omega\big{(} \frac{m^{n-2}+\log(1/\delta)}{\varepsilon_{\mathrm{TV}}^{2}}\big{)}\) samples to learn. For simplicity, we write \(\varepsilon=\varepsilon_{\mathrm{TV}}\). For technical convenience, we assume \(\varepsilon<\frac{1}{40},\delta<0.01\).

#### e.2.1 Part 1: Constructing \(\mathcal{D}\)

We index the distributions \(D_{z}\in\mathcal{D}\) by \(z\); the meaning of \(z\) will be defined later. Without loss of generality, we assume \(|\mathcal{S}_{i}|=m\) to be an even integer, and denote \(\mathcal{S}_{i}=\{1,\ldots,m\}=:S\). We will define \(D_{z}\) to be a distribution over the joint signal space \(\mathbf{\mathcal{S}}=\mathcal{S}_{1}\times\cdots\times\mathcal{S}_{n}=S^{n}=\{1, \ldots,m\}^{n}\). In the following we will call a joint signal \(\mathbf{s}\in S^{n}\) simply a signal. We write a signal \(\mathbf{s}\in S^{n}\) as \(\mathbf{s}=(\mathbf{b},x,y)\), where \(\mathbf{b}\in S^{n-2}\) and \(x,y\in S\). We sort all the \(m^{n}\) signals in \(S^{n}\) by the lexicographical

[MISSING_PAGE_FAIL:24]

Fixing any \(\mathbf{b}\), the numbers of \(\frac{+b\varepsilon c}{W}\) and \(\frac{-z_{b}c\varepsilon}{W}\) in the summation \(\sum_{y=1}^{m}D_{z}(\mathbf{b},x,y)\) are the same. So, they cancel out, and we obtain

\[D_{z}(s_{i})=\sum_{\mathbf{b}\in S^{n-2}}\sum_{y=1}^{m}D_{\mathrm{base}}(\mathbf{b},x, y)=D_{\mathrm{base}}(s_{i}).\]

Finally, if \(i=n\), namely \(s_{i}=y\), then by a similar argument as above we have

\[D_{z}(s_{i})=\sum_{\mathbf{b}\in S^{n-2}}\sum_{x=1}^{m}D_{\mathrm{base}}(\mathbf{b},x, y)=D_{\mathrm{base}}(s_{i}).\]

**Distinct marginals across signals:** By the "same marginal across distributions" property above we have \(D_{z}(s_{i})=D_{\mathrm{base}}(s_{i})\). So, to prove "distinct marginals across signals" we only need to prove \(D_{\mathrm{base}}(s_{i})\neq D_{\mathrm{base}}(s^{\prime}_{i})\) for \(s_{i}\neq s^{\prime}_{i}\). Without loss of generality assume \(s_{i}<s^{\prime}_{i}\). By the definition \(D_{\mathrm{base}}(\mathbf{s})=\frac{\gamma^{\mathrm{num}(\mathbf{s})}}{W}\) and the fact that \(\mathrm{num}(s_{i},\mathbf{s}_{-i})<\mathrm{num}(s^{\prime}_{i},\mathbf{s}_{-i})\) for any \(\mathbf{s}_{-i}\in\mathbf{\mathcal{S}}_{-i}\), we have

\[D_{\mathrm{base}}(s_{i})=\sum_{\mathbf{s}_{-i}\in\mathbf{\mathcal{S}}_{-i}}D_{\mathrm{ base}}(s_{i},\mathbf{s}_{-i})<\sum_{\mathbf{s}_{-i}\in\mathbf{\mathcal{S}}_{-i}}D_{\mathrm{ base}}(s^{\prime}_{i},\mathbf{s}_{-i})=D_{\mathrm{base}}(s^{\prime}_{i}),\]

so \(D_{\mathrm{base}}(s_{i})\neq D_{\mathrm{base}}(s^{\prime}_{i})\). 

#### e.2.2 Part 2: Sample Complexity Lower Bound of Learning \(\mathcal{D}\)

OverviewWe want to prove the proposition that the sample complexity of learning the family of distributions \(D=\{D_{z}\}_{z}\) defined above is at least \(T_{\mathcal{D}}^{\mathrm{TV}}(\varepsilon,\delta)=\Omega\big{(}\frac{m^{n-2}+ \log(1/\delta)}{\varepsilon^{2}}\big{)}\). This proof is analogous to a textbook proof of Proposition 3.3 (the sample complexity for learning _all_ distributions), which uses reductions from the _distinguishing distributions_ problem. Roughly speaking, if one can learn the unknown distribution \(D_{z}\) well then one must be able to guess most of the components of the sign vector \(z=(z_{\mathbf{b}})_{\mathbf{b}\in S^{n-2}}\) correctly, meaning that one can distinguish whether the distribution \(D_{z_{\mathbf{b}}}\) on bucket \(B_{\mathbf{b}}\) is \(D_{z_{\mathbf{b}}=+1}\) or \(D_{z_{\mathbf{b}}=-1}\). However, since \(D_{z_{\mathbf{b}}=+1}\) and \(D_{z_{\mathbf{b}}=-1}\) are "\(O(\varepsilon)\)-close" to each other, distinguishing them requires \(\Omega(\frac{1}{\varepsilon^{2}})\) samples. In average, there are only \(O(\frac{T}{m^{n-2}})\) samples falling into a bucket (because there are \(m^{n-2}\) buckets in total and the distribution \(D_{z}\) is close to uniform). We thus need \(O(\frac{T}{m^{n-2}})=\Omega(\frac{1}{\varepsilon^{2}})\), which gives \(T=\Omega(\frac{m^{n-2}}{\varepsilon^{2}})\). Ignoring logarithmic terms, this proves the proposition.

Formal argumentFirst, we note that if we can learn \(D_{z}\) very well, then we can guess the vector \(z\) correctly for a large fraction of its \(m^{n-2}\) components. Formally, suppose we obtain from \(T\) samples a distribution \(\hat{D}\) such that \(d_{\mathrm{TV}}(\hat{D},D_{z})\leq\varepsilon\). We find the distribution \(D_{w}\) in \(\mathcal{D}\), \(w=(w_{\mathbf{b}})_{\mathbf{b}\in S^{n-2}}\in\{+1,-1\}^{m^{n-2}}\), that is closest to \(\hat{D}\) in total variation distance. By definition, we have \(d_{\mathrm{TV}}(D_{w},\hat{D})\leq d_{\mathrm{TV}}(D_{z},\hat{D})\leq\varepsilon\). Hence, by triangle inequality,

\[d_{\mathrm{TV}}(D_{w},D_{z})\leq d_{\mathrm{TV}}(D_{w},\hat{D})+d_{\mathrm{TV} }(\hat{D},D_{z})\leq 2\varepsilon.\]

Let

\[\#=\big{|}\{\mathbf{b}\in S^{n-2}\mid w_{\mathbf{b}}\neq z_{\mathbf{b}}\}\big{|}\]

be the number of different components of \(w\) and \(z\). We claim that:

**Claim E.4**.: \(d_{\mathrm{TV}}(D_{w},D_{z})\leq 2\varepsilon\) _implies \(\#\leq\frac{2W}{cm^{2}}\)._

Proof.: Whenever we have a different component \(w_{\mathbf{b}}\neq z_{\mathbf{b}}\), this different component contributes the following to the total variation distance between \(D_{w}\) and \(D_{z}\):

\[\frac{1}{2}\sum_{(\mathbf{b},x,y)\in B_{\mathbf{b}}}\big{|}D_{w}(\mathbf{b},x,y)-D_{z}(\bm {b},x,y)\big{|}=\frac{1}{2}\sum_{(\mathbf{b},x,y)\in B_{\mathbf{b}}}\frac{2c \varepsilon}{W}=\frac{m^{2}c\varepsilon}{W}. \tag{28}\]

So, the number of different components of \(w\) and \(z\) is at most \(\frac{d_{\mathrm{TV}}(D_{w},D_{z})}{\frac{m^{2}c\varepsilon}{W}}\leq\frac{2W}{ cm^{2}}\)We first show the \(\Omega(\frac{\log(1/\delta)}{\varepsilon^{2}})\) part in the sample complexity lower bound, and then show the \(\Omega(\frac{m^{n-2}}{\varepsilon^{2}})\) part. Together, they give the lower bound \(\max\{\Omega(\frac{\log(1/\delta)}{\varepsilon^{2}}),\Omega(\frac{m^{n-2}}{ \varepsilon^{2}})\}=\Omega(\frac{m^{n-2}+\log(1/\delta)}{\varepsilon^{2}})\). Consider the distribution \(D_{\boldsymbol{+1}}\in\mathcal{D}\) whose index is the tall "\(+1\)" vector and the distribution \(D_{\boldsymbol{-1}}\in\mathcal{D}\) whose index is the all "\(-1\)" vector. According to (28), the total variation distance between \(D_{\boldsymbol{+1}}\) and \(D_{\boldsymbol{-1}}\) is \(d_{\mathrm{TV}}(D_{\boldsymbol{+1}},D_{\boldsymbol{-1}})=m^{n-2}\cdot\frac{m^ {2}\varepsilon}{W}=\frac{m^{n}\varepsilon\varepsilon}{W}\) because \(\boldsymbol{+1}\) and \(\boldsymbol{-1}\) have \(m^{n-2}\) different components. Since \(W\leq em^{n}\) (26), we have \(d_{\mathrm{TV}}(D_{\boldsymbol{+1}},D_{\boldsymbol{-1}})\geq\frac{\varepsilon }{\varepsilon}>2\varepsilon\). Consider the distinguishing distributions problem (defined in Section 3) where we want to distinguish \(D_{\boldsymbol{+1}}\) and \(D_{\boldsymbol{-1}}\). If we can learn from samples a distribution \(\hat{D}\) that is \(\varepsilon\)-close in total variation distance to the unknown distribution \(D_{\boldsymbol{+1}}\) or \(D_{\boldsymbol{-1}}\), then we can perfectly tell whether the unknown distribution is \(D_{\boldsymbol{+1}}\) or \(D_{\boldsymbol{-1}}\) because the two distributions are more than \(2\varepsilon\)-away from each other in total variation distance. Lemma A.5 implies that, to distinguish \(D_{\boldsymbol{+1}}\) and \(D_{\boldsymbol{-1}}\) with probability \(1-\delta\), the number of samples must be at least \(\Omega(\frac{\log(1/\delta)}{d_{\mathrm{H}}^{2}(D_{\boldsymbol{+1}},D_{ \boldsymbol{-1}})})=\Omega(\frac{\log(1/\delta)}{\varepsilon^{2}})\). This proves the \(\Omega(\frac{\log(1/\delta)}{\varepsilon^{2}})\) part.

We then prove the \(\Omega(\frac{m^{n-2}}{\varepsilon^{2}})\) part. Suppose we first draw the vector \(z\) from \(\{+1,-1\}^{m^{n-2}}\) uniformly at random, then draw \(T\) samples from \(D_{z}\). We obtain the \(D_{w}\) as above. Let's consider the expected number of different components of \(w\) and \(z\) in this two-step random draw procedure:

\[\underset{z,\,T\text{ samples}}{\mathbb{E}}[\#]=\mathbb{E}\Big{[}\sum_{ \boldsymbol{b}\in S^{n-2}}\mathbb{1}\left\{z_{\boldsymbol{b}}\neq w_{ \boldsymbol{b}}\right\}\Big{]}=\sum_{\boldsymbol{b}\in S^{n-2}}\mathbb{E} \big{[}\mathbb{1}\left\{z_{\boldsymbol{b}}\neq w_{\boldsymbol{b}}\right\} \big{]}. \tag{29}\]

We consider each component \(\mathbb{E}\big{[}\mathbb{1}\left\{z_{\boldsymbol{b}}\neq w_{\boldsymbol{b}} \right\}\big{]}\) in the above summation. Suppose that, within the \(T\) samples drawn from \(D_{z}\), \(T_{\boldsymbol{b}}\) of them fall into the bucket \(B_{\boldsymbol{b}}\). So, \(T_{\boldsymbol{b}}\) follows the Binomial\((T,D(B_{\boldsymbol{b}}))\) distribution with

\[D(B_{\boldsymbol{b}})=\sum_{(\boldsymbol{b},x,y)\in B_{\boldsymbol{b}}}D_{z}( \boldsymbol{b},x,y)=\frac{1}{W}\sum_{(\boldsymbol{b},x,y)\in B_{\boldsymbol{b }}}\gamma^{\mathrm{num}(\boldsymbol{b},x,y)}.\]

(Notice that the \(+\frac{z_{\boldsymbol{b}}\varepsilon}{W}\) and \(-\frac{z_{\boldsymbol{b}}\varepsilon}{W}\) cancel out in the summation and hence \(D(B_{\boldsymbol{b}})\) does not depend on \(z\).) Let \(D_{z_{\boldsymbol{b}}}\) denote the "\(B_{\boldsymbol{b}}\)-part" of distribution \(D_{z}\), namely, \(D_{z}\) conditioning on \(B_{\boldsymbol{b}}\):

\[D_{z_{\boldsymbol{b}}}(\boldsymbol{s})=\frac{D_{z}(\boldsymbol{s})}{D(B_{ \boldsymbol{b}})},\;\;\;\forall\boldsymbol{s}\in B_{\boldsymbol{b}}.\]

We think of the random draw of the vector \(z\) and the \(T\) samples as follows: first, we draw \(T_{\boldsymbol{b}}\) from Binomial\((T,D(B_{\boldsymbol{b}}))\); second, we draw \(z_{\boldsymbol{b}}\in\{+1,-1\}\) uniformly at random; third, we draw \(T_{\boldsymbol{b}}\) samples from the conditional distribution \(D_{z_{\boldsymbol{b}}}\); forth, we draw the remaining vector \(z_{-\boldsymbol{b}}\) and the remaining \(T-T_{\boldsymbol{b}}\) samples (which are samples outside of \(B_{\boldsymbol{b}}\)). Only writing the first two steps explicitly, we have

\[\mathbb{E}\big{[}\mathbb{1}\left\{z_{\boldsymbol{b}}\neq w_{ \boldsymbol{b}}\right\}\big{]} =\underset{T_{\boldsymbol{b}}}{\mathbb{E}}\bigg{[}\mathbb{E} \Big{[}\mathbb{E}\big{[}\mathbb{1}\left\{z_{\boldsymbol{b}}\neq w_{ \boldsymbol{b}}\right\}\mid z_{\boldsymbol{b}},T_{\boldsymbol{b}}\big{]} \Big{]}\bigg{]}\] \[=\underset{T_{\boldsymbol{b}}}{\mathbb{E}}\bigg{[}\frac{1}{2} \mathbb{E}\Big{[}\mathbb{1}\left\{z_{\boldsymbol{b}}\neq w_{\boldsymbol{b}} \right\}\mid z_{\boldsymbol{b}}=+1,T_{\boldsymbol{b}}\Big{]}+\frac{1}{2} \mathbb{E}\big{[}\mathbb{1}\left\{z_{\boldsymbol{b}}\neq w_{\boldsymbol{b}} \right\}\mid z_{\boldsymbol{b}}=-1,T_{\boldsymbol{b}}\big{]}\bigg{]}\] \[=\underset{T_{\boldsymbol{b}}}{\mathbb{E}}\bigg{[}\frac{1}{2} \Pr\big{[}z_{\boldsymbol{b}}\neq w_{\boldsymbol{b}}\mid z_{\boldsymbol{b}}=+1,T _{\boldsymbol{b}}\big{]}+\frac{1}{2}\Pr\big{[}z_{\boldsymbol{b}}\neq w_{ \boldsymbol{b}}\mid z_{\boldsymbol{b}}=-1,T_{\boldsymbol{b}}\big{]}\bigg{]}. \tag{30}\]

**Claim E.5**.: _For any \(T_{\boldsymbol{b}}\), \(\frac{1}{2}\Pr\big{[}z_{\boldsymbol{b}}\neq w_{\boldsymbol{b}}\mid z_{ \boldsymbol{b}}=+1,T_{\boldsymbol{b}}\big{]}+\frac{1}{2}\Pr\big{[}z_{ \boldsymbol{b}}\neq w_{\boldsymbol{b}}\mid z_{\boldsymbol{b}}=-1,T_{ \boldsymbol{b}}\big{]}\geq\frac{1}{2}-2c\varepsilon\sqrt{T_{\boldsymbol{b}}}\)._

Proof.: We notice that \(\frac{1}{2}\Pr\big{[}z_{\boldsymbol{b}}\neq w_{\boldsymbol{b}}\mid z_{ \boldsymbol{b}}=+1,T_{\boldsymbol{b}}\big{]}+\frac{1}{2}\Pr\big{[}z_{ \boldsymbol{b}}\neq w_{\boldsymbol{b}}\mid z_{\boldsymbol{b}}=-1,T_{ \boldsymbol{b}}\big{]}\) is the probability that we make a mistake when guessing the sign \(z_{\boldsymbol{b}}\) using \(w_{\boldsymbol{b}}\), if (1) \(z_{\boldsymbol{b}}\) is chosen from \(\{-1,+1\}\) uniformly at random; (2) we are given \(T_{\boldsymbol{b}}\) samples from \(D_{z_{\boldsymbol{b}}}\); (3) we then draw the remaining vector \(z_{-\boldsymbol{b}}\) and the remaining samples; (4) finally, we use the \(D_{w}\) computed from all samples to get \(w_{\boldsymbol{b}}\). The steps (3) and (4) define a randomized function that maps the \(T_{\boldsymbol{b}}\) samples of \(D_{z_{\boldsymbol{b}}}\) to \(w_{\boldsymbol{b}}\in\{0,1\}\), and therefore, according to the first item of Lemma A.5, we have

\[\frac{1}{2}\Pr\big{[}z_{\boldsymbol{b}}\neq w_{\boldsymbol{b}}\mid z_{ \boldsymbol{b}}=+1,T_{\boldsymbol{b}}\big{]}+\frac{1}{2}\Pr\big{[}z_{ \boldsymbol{b}}\neq w_{\boldsymbol{b}}\mid z_{\boldsymbol{b}}=-1,T_{ \boldsymbol{b}}\big{]}\geq\frac{1}{2}-\sqrt{\frac{T_{\boldsymbol{b}}}{2}}d_{ \mathrm{H}}(D_{z_{\boldsymbol{b}}=+1},D_{z_{\boldsymbol{b}}=-1}). \tag{31}\]Then we consider \(d_{\rm H}(D_{z_{\mathbf{b}}=+1},D_{z_{\mathbf{b}}=-1})\). We use Lemma A.2 to do so. For any \(\mathbf{s}\in B_{\mathbf{b}}\), we have, on the one hand,

\[\frac{D_{z_{\mathbf{b}}=+1}(\mathbf{s})}{D_{z_{\mathbf{b}}=-1}(\mathbf{s})}=\frac{ \gamma^{\rm num(\mathbf{s})}\pm c\varepsilon}{\gamma^{\rm num(\mathbf{s})}\mp c \varepsilon}\geq\frac{\gamma^{\rm num(\mathbf{s})}-c\varepsilon}{\gamma^{\rm num( \mathbf{s})}+c\varepsilon}\geq 1-2c\varepsilon,\]

because \(\frac{a-b}{a+b}=1-\frac{2b}{a+b}\geq 1-2b\) for \(a=\gamma^{\rm num(\mathbf{s})}\geq 1\). On the other hand,

\[\frac{D_{z_{\mathbf{b}}=+1}(\mathbf{s})}{D_{z_{\mathbf{b}}=-1}(\mathbf{s})}\leq \frac{\gamma^{\rm num(\mathbf{s})}+c\varepsilon}{\gamma^{\rm num(\mathbf{s})}-c \varepsilon}\leq 1+4c\varepsilon,\]

because \(\frac{a+b}{a-b}=1+\frac{2b}{a-b}\leq 1+4b\) for \(a=\gamma^{\rm num(\mathbf{s})}\geq 1\) and \(b=c\varepsilon\leq\frac{1}{2}\). Therefore, by Lemma A.2 we have

\[d_{\rm H}^{2}(D_{z_{\mathbf{b}}=+1},D_{z_{\mathbf{b}}=-1})\leq\frac{1}{ 2}(4c\varepsilon)^{2}=8c^{2}\varepsilon^{2}. \tag{32}\]

Combining (31) and (32) proves our claim. 

By (30) and Claim E.5, we have \(\mathbb{E}\big{[}\mathbb{1}\{z_{\mathbf{b}}\neq w_{\mathbf{b}}\}\big{]}\geq\underset{ \mathbf{T}_{\mathbf{b}}}{\mathbb{E}}\big{[}\frac{1}{2}-2c\varepsilon\sqrt{T_{\mathbf{b}}} \big{]}\). Summing over \(\mathbf{b}\in S^{n-2}\), (29) becomes

\[\sum_{\mathbf{b}\in S^{n-2}}\mathbb{E}\big{[}\mathbb{1}\left\{z_{\mathbf{ b}}\neq w_{\mathbf{b}}\right\}\big{]}\geq\sum_{\mathbf{b}\in S^{n-2}}\underset{\mathbf{T}_{ \mathbf{b}}}{\mathbb{E}}\Big{[}\frac{1}{2}-2c\varepsilon\sqrt{T_{\mathbf{b}}}\Big{]} =\frac{m^{n-2}}{2}-2c\varepsilon\sum_{\mathbf{b}\in S^{n-2}}\mathbb{ E}[\sqrt{T_{\mathbf{b}}}] \tag{33}\] \[\text{(by Jensen's inequality $\mathbb{E}[\sqrt{X}]\leq\sqrt{ \mathbb{E}[X]}$)} \geq\frac{m^{n-2}}{2}-2c\varepsilon\sum_{\mathbf{b}\in S^{n-2}}\sqrt{ \mathbb{E}[T_{\mathbf{b}}]}.\]

Because \(\mathbb{E}[T_{\mathbf{b}}]=T\cdot D(B_{\mathbf{b}})=\frac{T}{W}\sum_{(\mathbf{b},x,y)\in B _{\mathbf{b}}}\gamma^{\rm num(\mathbf{b},x,y)}\leq\frac{T}{W}m^{2}\gamma^{m^{n}}\), we have

\[\sum_{\mathbf{b}\in S^{n-2}}\mathbb{E}\big{[}\mathbb{1}\left\{z_{\mathbf{b}}\neq w_{ \mathbf{b}}\right\}\big{]}\geq\frac{m^{n-2}}{2}-2c\varepsilon\sum_{\mathbf{b}\in S^{n -2}}\sqrt{\frac{T}{W}m^{2}\gamma^{m^{n}}}=\frac{m^{n-2}}{2}-2c\varepsilon m^{n -2}\sqrt{\frac{T}{W}m^{2}\gamma^{m^{n}}}. \tag{34}\]

Now, let's consider the probability with which we can obtain \(\hat{D}\) such that \(d_{\rm TV}(\hat{D},D_{z})\leq\varepsilon\). We will show that this probability is at most \(0.99<1-\delta\) if \(T\) is less than \(10^{-5}\cdot\frac{m^{n-2}}{\varepsilon^{2}}\). Recall from Claim E.4 that \(d_{\rm TV}(\hat{D},D_{z})\leq\varepsilon\) implies \(\#=\sum_{\mathbf{b}\in S^{n-2}}\mathbb{1}\left\{z_{\mathbf{b}}\neq w_{\mathbf{b}}\right\} \leq\frac{2W}{cm^{2}}\). So, the probability is at most

\[\Pr\Big{[}\sum_{\mathbf{b}\in S^{n-2}}\mathbb{1}\left\{z_{\mathbf{b}}\neq w _{\mathbf{b}}\right\}\leq\frac{2W}{cm^{2}}\Big{]} =\Pr\Big{[}\sum_{\mathbf{b}\in S^{n-2}}\mathbb{1}\left\{z_{\mathbf{b}}=w _{\mathbf{b}}\right\}\geq m^{n-2}-\frac{2W}{cm^{2}}\Big{]}\] (by Markov's inequality) \[\leq\frac{\mathbb{E}\big{[}\sum_{\mathbf{b}\in S^{n-2}}\mathbb{1} \left\{z_{\mathbf{b}}=w_{\mathbf{b}}\right\}\big{]}}{m^{n-2}-\frac{2W}{cm^{2}}}\] (by (34)) \[\text{(by \eqref{eq:d_TV})} \leq\frac{m^{n-2}+2c\varepsilon m^{n-2}\sqrt{\frac{T}{W}m^{2} \gamma^{m^{n}}}}{m^{n-2}-\frac{2W}{cm^{2}}}\] (\(\gamma^{m^{n}}\leq e\) and \(m^{n}\leq W\leq em^{n}\) by (26)) \leq\frac{m^{n-2}+2c\varepsilon m^{n-2}\sqrt{\frac{eT}{m^{n-2}}}} {m^{n-2}-\frac{2em^{n}}{cm^{2}}}\] \[=\frac{\frac{1}{2}+2c\sqrt{\frac{e\varepsilon^{2}}{m^{n-2}}T}}{1- \frac{2c}{c}}<0.99<1-\delta,\]

when \(c=20\), \(T<10^{-5}\cdot\frac{m^{n-2}}{\varepsilon^{2}}\), and \(\delta<0.01\). This means that, in order to obtain such \(\hat{D}\) with probability at least \(1-\delta\), at least \(10^{-5}\cdot\frac{m^{n-2}}{\varepsilon^{2}}\) samples are needed.

Missing Proofs from Section 5

Proof of Theorem 5.2: the \(\Omega\big{(}\frac{1}{\varepsilon}\log\frac{1}{\delta}\big{)}\) Lower Bound

The proof uses a reduction from the distinguishing distributions problem (defined in Section 3). We construct two conditionally independent distributions \(P^{1},P^{2}\) over the space \(\Omega\times\mathcal{S}_{1}\times\cdots\times\mathcal{S}_{n}\) with each \(|\mathcal{S}_{i}|=2\), \(\mathcal{S}_{i}=\{a,b\}\). Given \(T\) samples from either \(P^{1}\) or \(P^{2}\), we want to tell which distribution the samples are coming from. We will show that, if we can solve the forecast aggregation problem, then we can distinguish the two distributions (with high probability), which requires \(T=\Omega(\frac{1}{\ell_{\text{H}}^{2}(P_{1},P_{2})}\log\frac{1}{\delta})= \Omega(\frac{1}{\varepsilon}\log\frac{1}{\delta})\) samples according to Lemma A.5.

Let \(c=32\). We assume \(\varepsilon<2^{-18}\), so that \(c\sqrt{\varepsilon}<\frac{1}{16}\). For \(P^{1}\), we let

\[P^{1}(\omega=1)=0.5-\frac{1}{16n}+\frac{c\sqrt{\varepsilon}}{n}=:p^{1}.\]

For \(P^{2}\), we let

\[P^{2}(\omega=1)=0.5-\frac{1}{16n}-\frac{c\sqrt{\varepsilon}}{n}=:p^{2}.\]

We require that, in the forecast aggregation problem under both distributions \(P^{1}\) and \(P^{2}\), whenever expert \(i\) sees signal \(a\), \(b\), she reports

\[r_{a}=0.5,\hskip 14.226378ptr_{b}=0,\]

respectively. This gives the following conditional probabilities \(P^{1}(\cdot\mid\omega),P^{2}(\cdot\mid\omega)\):

\[\begin{bmatrix}P^{1}(a\mid\omega=0)\\ P^{1}(a\mid\omega=1)\end{bmatrix}=\frac{p^{1}-r_{b}}{r_{a}-r_{b}}\begin{bmatrix} \frac{1-r_{b}}{1-p}\\ \frac{r_{b}}{p^{2}}\end{bmatrix}=\begin{bmatrix}\frac{1-\frac{1}{8n}+\frac{2c \sqrt{\varepsilon}}{n}}{1+\frac{1}{8n}+\frac{2c\sqrt{\varepsilon}}{n}}\\ \end{bmatrix},\hskip 14.226378pt\begin{bmatrix}P^{1}(b\mid\omega=0)\\ P^{1}(b\mid\omega=1)\end{bmatrix}=\begin{bmatrix}\frac{\frac{1}{8n}-\frac{4c \sqrt{\varepsilon}}{n}}{1+\frac{1}{8n}+\frac{2c\sqrt{\varepsilon}}{n}}\\ \end{bmatrix}. \tag{34}\]

\[\begin{bmatrix}P^{2}(a\mid\omega=0)\\ P^{2}(a\mid\omega=1)\end{bmatrix}=\frac{p^{2}-r_{b}}{r_{a}-r_{b}}\begin{bmatrix} \frac{1-r_{b}}{1-p^{2}}\\ \frac{r_{b}}{p^{2}}\end{bmatrix}=\begin{bmatrix}\frac{1-\frac{1}{8n}-\frac{2c \sqrt{\varepsilon}}{n}}{1+\frac{1}{8n}+\frac{2c\sqrt{\varepsilon}}{n}}\\ \end{bmatrix},\hskip 14.226378pt\begin{bmatrix}P^{2}(b\mid\omega=0)\\ P^{2}(b\mid\omega=1)\end{bmatrix}=\begin{bmatrix}\frac{\frac{1}{8n}+\frac{4c \sqrt{\varepsilon}}{n}}{1+\frac{1}{8n}+\frac{2c\sqrt{\varepsilon}}{n}}\\ \end{bmatrix}. \tag{35}\]

Given \(T\) samples from the unknown distribution \(P\in\{P^{1},P^{2}\}\), each of which is a vector of \(\omega^{(t)}\) and all experts' signals \(s_{i}^{(t)}\in\{a,b\}\), we feed the corresponding reports \(r_{i}^{(t)}\in\{r_{a},r_{b}\}\) and \(\omega^{(t)}\) to the forecast aggregation problem and obtain a solution \(\hat{f}\), which is an \(\varepsilon\)-optimal aggregator. We want to use \(\hat{f}\) to estimate the prior \(p=P(\omega=1)\in\{p^{1},p^{2}\}\) so that we can tell apart \(P^{1}\) and \(P^{2}\). Recall from Lemma 5.1 that \(f^{*}(\mathbf{r})=\frac{1}{1+\rho^{n-1}\prod_{i=1}^{n}\frac{1-r_{i}}{r_{i}}}\), where \(\rho=\frac{p}{1-p}\). Writing \(\rho\) in terms of \(f^{*}(\mathbf{r})\), we have

\[\rho=\sqrt[n-1]{\left(\frac{1}{f^{*}(\mathbf{r})}-1\right)\prod_{i=1}^{n}\frac{r_{i }}{1-r_{i}}}.\]

In particular, when \(r_{i}=r_{a}=0.5\) for all \(i\in\{1,\ldots,n\}\), we have:

\[\rho=\sqrt[n-1]{\frac{1}{f^{*}(\mathbf{r}_{0.5})}-1},\hskip 14.226378pt\mathbf{r}_{0.5}=(0.5,\ldots,0.5).\]

So, we estimate \(\rho\) by:

\[\hat{\rho}=\sqrt[n-1]{\frac{1}{\hat{f}(\mathbf{r}_{0.5})}-1}.\]

Now, we want to argue that, if \(\hat{f}\) is \(\varepsilon\)-optimal, then \(|\hat{\rho}-\rho|\) is at most \(O(\frac{\sqrt{\varepsilon}}{n})\). Consider the function:

\[h(x)=\sqrt[n-1]{\frac{1}{x}-1},\]

whose derivative is

\[h^{\prime}(x)=-\frac{1}{n-1}\Big{(}\frac{x}{1-x}\Big{)}^{1-\frac{1}{n-1}}\frac {1}{x^{2}}.\]By definition, we have

\[\big{|}\hat{p}-\rho\big{|}=\big{|}h\big{(}\hat{f}(\mathbf{r}_{0.5})\big{)}-h\big{(}f^{* }(\mathbf{r}_{0.5})\big{)}\big{|}. \tag{36}\]

**Claim F.1**.: \(\frac{1}{2}\leq f^{*}(\mathbf{r}_{0.5})\leq\frac{2}{3}\)_._

Proof.: For \(P\in\{P^{1},P^{2}\}\), its \(\rho=\frac{p}{1-p}\) satisfies

\[1\geq\rho\geq\rho^{n-1}\geq\rho^{n} \geq\left(\frac{0.5-\frac{1}{16n}-\frac{\sqrt{\varepsilon}}{cn}} {0.5+\frac{1}{16n}+\frac{c\sqrt{\varepsilon}}{n}}\right)^{n}\] \[=\left(\frac{1-\frac{8n}{8n}-\frac{2c\sqrt{\varepsilon}}{n}}{1+ \frac{1}{8n}+\frac{2c\sqrt{\varepsilon}}{n}}\right)^{n}>\left(1-2\big{(}\frac {1}{8n}+\frac{2c\sqrt{\varepsilon}}{n}\big{)}\right)^{n}\geq 1-2\big{(}\frac{1}{8 }+2c\sqrt{\varepsilon}\big{)}>\frac{1}{2}, \tag{37}\]

where in the last three transitions we used the inequalities \(\frac{1-x}{1+x}>1-2x\) and \((1-x/n)^{n}\geq 1-x\) for \(x\in(0,1)\) and the fact that \(c\sqrt{\varepsilon}<\frac{1}{16}\). So,

\[f^{*}(\mathbf{r}_{0.5})=\frac{1}{1+\rho^{n-1}}\in\Big{[}\frac{1}{1+1},\frac{1}{1+ \frac{1}{2}}\Big{]}=\Big{[}\frac{1}{2},\frac{2}{3}\Big{]},\]

which proves the claim. 

With Claim F.1, we can without loss of generality assume \(\frac{1}{2}\leq\hat{f}(\mathbf{r}_{0.5})\leq\frac{2}{3}\) as well (otherwise, we can truncate \(\hat{f}(\mathbf{r}_{0.5})\) to this range; this only reduces the approximation error \(\mathbb{E}\big{[}|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|^{2}\big{]}\)).

**Claim F.2**.: _For \(\frac{1}{2}\leq x\leq\frac{2}{3}\), \(|h^{\prime}(x)|\leq\frac{8}{n-1}\)._

Proof.: \[|h^{\prime}(x)|=\frac{1}{n-1}\big{(}\frac{x}{1-x}\big{)}^{1-\frac{1}{n-1}} \frac{1}{x^{2}}\leq\frac{1}{n-1}\Big{(}\frac{\frac{2}{3}}{1-\frac{2}{3}}\Big{)} ^{1-\frac{1}{n-1}}\frac{1}{(\frac{1}{2})^{2}}=\frac{4}{n-1}\cdot 2^{1- \frac{1}{n-1}}\leq\frac{8}{n-1}.\qed\]

**Claim F.3**.: _If \(\hat{f}\) is \(\varepsilon\)-optimal, then \(|\hat{f}(\mathbf{r}_{0.5})-f^{*}(\mathbf{r}_{0.5})|<2\sqrt{\varepsilon}\)._

Proof.: If \(\hat{f}\) is \(\varepsilon\)-optimal, i.e., \(\mathbb{E}\big{[}|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|^{2}\big{]}\leq\varepsilon\), then, by Jensen's inequality \(\mathbb{E}[X^{2}]\geq\mathbb{E}[X]^{2}\), we have

\[\sqrt{\varepsilon}\geq\mathbb{E}\big{[}|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|\big{]}= \sum_{\mathbf{r}}P(\mathbf{r})|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|\geq P(\mathbf{r}_{0.5})|\hat{ f}(\mathbf{r}_{0.5})-f^{*}(\mathbf{r}_{0.5})|. \tag{38}\]

For both \(P\in\{P^{1},P^{2}\}\), we have

\[P(\mathbf{r}_{0.5}) =p\cdot P(\mathbf{r}_{0.5}\mid\omega=1)+(1-p)\cdot P(\mathbf{r}_{0.5}\mid \omega=0)\] \[=p\cdot P(a\mid\omega=1)^{n}+(1-p)\cdot P(a\mid\omega=0)^{n}\] \[\geq p\cdot 1+(1-p)\cdot\left(\frac{1-\frac{1}{8n}-\frac{2c\sqrt{ \varepsilon}}{n}}{1+\frac{1}{8n}+\frac{2c\sqrt{\varepsilon}}{n}}\right)^{n} \stackrel{{\text{\rm\tiny{WP}}(\ref{eq:P0.5})}}{{>}}\frac{1}{2},\]

Plugging \(P(\mathbf{r}_{0.5})>\frac{1}{2}\) into (38), we get \(|\hat{f}(\mathbf{r}_{0.5})-f^{*}(\mathbf{r}_{0.5})|<2\sqrt{\varepsilon}\). 

From (36), Claim F.1, Claim F.2, and Claim F.3, we get

\[\big{|}\hat{\rho}-\rho\big{|}=\big{|}h\big{(}\hat{f}(\mathbf{r}_{0.5})\big{)}-h \big{(}f^{*}(\mathbf{r}_{0.5})\big{)}\big{|}\leq\frac{8}{n-1}\big{|}\hat{f}(\mathbf{r }_{0.5})-f^{*}(\mathbf{r}_{0.5})\big{|}<\frac{8}{n-1}\cdot 2\sqrt{\varepsilon}= \frac{16}{n-1}\sqrt{\varepsilon}. \tag{39}\]

Since \(p=\frac{\rho}{1+\rho}\) as a function of \(\rho\) has a bounded derivative \(\frac{\partial p}{\partial\rho}=\frac{1}{(1+\rho)^{2}}\leq 1\), Equation (39) implies

\[|\hat{p}-p|<\frac{16}{n-1}\sqrt{\varepsilon}\]if we use \(\hat{p}=\frac{\hat{p}}{1+\hat{p}}\) as an estimate of \(p\). This allows us to tell part \(P^{1}\) and \(P^{2}\) because the difference between \(p^{1}\) and \(p^{2}\) is greater than twice of our estimation error \(|\hat{p}-p|\):

\[|p^{1}-p^{2}|=\frac{2c\sqrt{\varepsilon}}{n}=\frac{64\sqrt{\varepsilon}}{n} \geq\frac{64\sqrt{\varepsilon}}{2(n-1)}=\frac{32\sqrt{\varepsilon}}{n-1}>2| \hat{p}-p|.\]

Therefore, we can tell part \(P^{1}\) and \(P^{2}\) by checking whether \(p^{1}\) or \(p^{2}\) is closer to \(\hat{p}\).

Finally, we upper bound the squared Hellinger distance between \(P^{1}\) and \(P^{2}\). This will give the sample complexity lower bound we want.

**Claim F.4**.: \(d_{\mathrm{H}}^{2}(P^{1},P^{2})\leq O(c^{2}\varepsilon)\)_._

Proof.: For the marginal distributions of \(\omega\), \(P^{1}_{\omega}\) and \(P^{2}_{\omega}\), according to Lemma A.2 and the fact that \(1\geq\frac{P^{2}_{\omega}(\omega)}{P^{1}_{\omega}(\omega)}=\frac{1-\frac{1}{8 n}-\frac{2c\sqrt{\varepsilon}}{n}}{1-\frac{1}{8n}+\frac{2c\sqrt{\varepsilon}}{n}}=1 -\frac{\frac{4c\sqrt{\varepsilon}}{n}}{1-\frac{1}{8n}+\frac{2c\sqrt{ \varepsilon}}{n}}=1-O(\frac{c\sqrt{\varepsilon}}{n})\), we have

\[d_{\mathrm{H}}^{2}(P^{1}_{\omega},P^{2}_{\omega})\leq O\big{(}\big{(}\frac{c \sqrt{\varepsilon}}{n}\big{)}^{2}\big{)}=O\Big{(}\frac{c^{2}\varepsilon}{n^{2} }\Big{)}. \tag{40}\]

Given \(\omega=0\) or \(1\), we consider the conditional distributions of each \(s_{i}\), \(P^{1}_{s_{i}|\omega}\) and \(P^{2}_{s_{i}|\omega}\). For \(s_{i}=a\), we have

\[1\geq\frac{P^{2}(a\mid\omega)}{P^{1}(a\mid\omega)} \geq\frac{1-\frac{1}{8n}-\frac{2c\sqrt{\varepsilon}}{n}}{1+\frac{1 }{8n}+\frac{2c\sqrt{\varepsilon}}{n}}\cdot\frac{1+\frac{1}{8n}-\frac{2c\sqrt{ \varepsilon}}{n}}{1-\frac{1}{8n}+\frac{2c\sqrt{\varepsilon}}{n}}\] \[=\frac{1+\frac{1}{8n}-\frac{2c\sqrt{\varepsilon}}{n}}{1+\frac{1}{ 8n}+\frac{2c\sqrt{\varepsilon}}{n}}\cdot\frac{1-\frac{1}{8n}-\frac{2c\sqrt{ \varepsilon}}{n}}{1-\frac{1}{8n}+\frac{2c\sqrt{\varepsilon}}{n}}\] \[(\frac{a-x}{a+x}\geq 1-\frac{2x}{a}) \geq\Big{(}1-\frac{\frac{4c\sqrt{\varepsilon}}{n}}{1+\frac{1}{ 8n}}\Big{)}\cdot\Big{(}1-\frac{\frac{4c\sqrt{\varepsilon}}{n}}{1-\frac{1}{8n} }\Big{)}=1-O\big{(}\frac{c\sqrt{\varepsilon}}{n}\big{)}.\]

For \(s_{i}=b\), we have

\[1\geq\frac{P^{1}(b\mid\omega)}{P^{2}(b\mid\omega)}\geq\frac{\frac{1}{4n}-\frac {4c\sqrt{\varepsilon}}{n}}{1+\frac{1}{8n}-\frac{2c\sqrt{\varepsilon}}{n}}\cdot \frac{1+\frac{1}{8n}+\frac{2c\sqrt{\varepsilon}}{n}}{\frac{1}{4n}+\frac{4c \sqrt{\varepsilon}}{n}}\geq\frac{\frac{1}{4n}-\frac{4c\sqrt{\varepsilon}}{n}} {\frac{1}{4n}+\frac{4c\sqrt{\varepsilon}}{n}}=\frac{1-16c\sqrt{\varepsilon}}{1 +16c\sqrt{\varepsilon}}=1-O(c\sqrt{\varepsilon}).\]

So, \(d_{\mathrm{H}}^{2}(P^{1}_{s_{i}|\omega},P^{2}_{s_{i}|\omega})\) can be upper bounded as follows:

\[d_{\mathrm{H}}^{2}(P^{1}_{s_{i}|\omega},P^{2}_{s_{i}|\omega}) =\frac{1}{2}\bigg{[}\Big{(}\sqrt{P^{1}(a\mid\omega)}-\sqrt{P^{2} (a\mid\omega)}\Big{)}^{2}+\Big{(}\sqrt{P^{1}(b\mid\omega)}-\sqrt{P^{2}(b\mid \omega)}\Big{)}^{2}\bigg{]}\] \[=\frac{1}{2}\bigg{[}P^{1}(a\mid\omega)\Big{(}1-\sqrt{\frac{P^{2} (a\mid\omega)}{P^{1}(a\mid\omega)}}\Big{)}^{2}+P^{2}(b\mid\omega)\Big{(}1- \sqrt{\frac{P^{1}(b\mid\omega)}{P^{2}(b\mid\omega)}}\Big{)}^{2}\bigg{]}\] \[\leq\frac{1}{2}\bigg{[}1\cdot\Big{(}1-\sqrt{1-O\big{(}\frac{c \sqrt{\varepsilon}}{n}\big{)}}\Big{)}^{2}+\Big{(}\frac{1}{4n}+\frac{4c\sqrt{ \varepsilon}}{n}\Big{)}\cdot\Big{(}1-\sqrt{1-O(c\sqrt{\varepsilon})}\Big{)}^{2} \bigg{]}\] (since \[1-\sqrt{1-x}\leq x\)) \(\leq\frac{1}{2}\bigg{[}O\big{(}\frac{c\sqrt{\varepsilon}}{n}\big{)}^{2}+O \big{(}\frac{1}{n}\big{)}\cdot O(c\sqrt{\varepsilon})^{2}\bigg{]}\)

\[=O\Big{(}\frac{c^{2}\varepsilon}{n}\Big{)}. \tag{41}\]

Since \(P^{1}=P^{1}_{\omega}\cdot\prod_{i=1}^{n}P^{1}_{s_{i}|\omega}\) and \(P^{2}=P^{2}_{\omega}\cdot\prod_{i=1}^{n}P^{2}_{s_{i}|\omega}\), we have, by Lemma A.3 and Lemma A.4,

\[d_{\mathrm{H}}^{2}(P^{1},P^{2}) \leq d_{\mathrm{H}}^{2}(P^{1}_{\omega},P^{2}_{\omega})+\max_{ \omega\in\{0,1\}}\Big{\{}d_{\mathrm{H}}^{2}\big{(}\prod_{i=1}^{n}P^{1}_{s_{i} |\omega},\ \prod_{i=1}^{n}P^{2}_{s_{i}|\omega}\big{)}\Big{\}}\] \[\leq d_{\mathrm{H}}^{2}(P^{1}_{\omega},P^{2}_{\omega})+\max_{ \omega\in\{0,1\}}\Big{\{}n\cdot d_{\mathrm{H}}^{2}(P^{1}_{s_{i}|\omega},\ P^{2}_{s_{ i}|\omega})\Big{\}}\] (40) and (41) \[=O\Big{(}\frac{c^{2}\varepsilon}{n^{2}}\Big{)}+\max_{\omega\in\{0,1\}}\Big{\{}n\cdot O\Big{(}\frac{c^{2}\varepsilon}{n}\Big{)}\Big{\}}\] \[=O(c^{2}\varepsilon).\qed\]Therefore, according to Lemma A.5, to tell apart \(P^{1}\) and \(P^{2}\) with probability at least \(1-\delta\) we need at least

\[T=\Omega\Big{(}\frac{1}{d_{\mathrm{H}}^{2}(P^{1},P^{2})}\log\frac{1}{\delta} \Big{)}=\Omega\Big{(}\frac{1}{c^{2}_{\varepsilon}}\log\frac{1}{\delta}\Big{)}\]

samples. This concludes the proof.

## Appendix G Missing Proofs from Section B

### Proof of Theorem b.2

#### g.1.1 Additional Notations and Lemmas

We introduce some additional notations and lemmas for the proof. Let \(\mu_{0}\) be the expected average report of all experts conditioning on \(\omega=0\):

\[\mu_{0}=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}[r_{i}\mid\omega=0]=\frac{1}{n}\sum _{i=1}^{n}\mathbb{E}_{s_{i}\mid\omega=0}\big{[}P(\omega=1\mid s_{i})\mid\omega =0\big{]}, \tag{42}\]

which is equal to the expected prediction of \(\omega\) given expert \(i\)'s signal \(s_{i}\) where \(s_{i}\) is distributed conditioning on \(\omega=0\), averaged over all experts. Symmetrically, let

\[\mu_{1}=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}[1-r_{i}\mid\omega=1]=\frac{1}{n} \sum_{i=1}^{n}\mathbb{E}_{s_{i}\mid\omega=1}\big{[}P(\omega=0\mid s_{i})\mid \omega=1\big{]}. \tag{43}\]

Recall that \(p=P(\omega=1)\).

**Fact G.1**.: \((1-p)\mu_{0}=p\mu_{1}\)_._

Proof.: For each expert \(i\), by the law of total expectation and the fact that \(r_{i}=P(\omega=1\!\mid\!s_{i})\), we have the following equations:

\[(1-p)\cdot\mathbb{E}[r_{i}\mid\omega=0]+p\cdot\mathbb{E}[r_{i}\mid \omega=1] =P(\omega=0)\cdot\mathbb{E}[r_{i}\mid\omega=0]+P(\omega=1)\cdot \mathbb{E}[r_{i}\mid\omega=1]\] \[=\mathbb{E}[r_{i}]\] \[=\mathbb{E}_{s_{i}}\big{[}P(\omega=1\!\mid\!s_{i})\big{]}=P( \omega=1)=p.\]

Subtracting \(p\) from both sides, we get

\[(1-p)\cdot\mathbb{E}[r_{i}\mid\omega=0]-p\cdot\mathbb{E}[1-r_{i}\mid\omega=1]=0.\]

Averaging over all experts \(i\in\{1,\ldots,n\}\), we conclude that

\[(1-p)\cdot\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}[r_{i}\mid\omega=0]-p\cdot\frac{ 1}{n}\sum_{i=1}^{n}\mathbb{E}[1-r_{i}\mid\omega=1]=0.\qed\]

**Lemma G.2**.: _If \((1-p)\mu_{0}=p\mu_{1}<\frac{\varepsilon}{2}\), then the averaging aggregator \(f_{avg}(\boldsymbol{r})=\frac{1}{n}\sum_{i=1}^{n}r_{i}\) is \(\varepsilon\)-optimal._

Proof.: If \((1-p)\mu_{0}=p\mu_{1}<\frac{\varepsilon}{2}\), then the expected loss of \(f_{avg}\) is at most

\[L_{P}(f_{avg}) =\mathbb{E}\big{[}|f_{avg}-\omega|^{2}\big{]}\] \[=p\mathbb{E}\big{[}(f_{avg}(\boldsymbol{r})-1)^{2}\mid\omega=1 \big{]}+(1-p)\mathbb{E}\big{[}(f_{avg}(\boldsymbol{r})-0)^{2}\mid\omega=0 \big{]}\] \[\leq p\mathbb{E}\big{[}1-f_{avg}(\boldsymbol{r})\mid\omega=1 \big{]}+(1-p)\mathbb{E}\big{[}f_{avg}(\boldsymbol{r})\mid\omega=0\big{]}\] \[=p\mathbb{E}\big{[}1-\frac{1}{n}\sum_{i=1}^{n}r_{i}\mid\omega=1 \big{]}+(1-p)\mathbb{E}\big{[}\frac{1}{n}\sum_{i=1}^{n}r_{i}\mid\omega=0\big{]}\] \[=p\mu_{1}+(1-p)\mu_{0}\] \[<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon,\]

which implies that \(f_{avg}\) is \(\varepsilon\)-optimal.

The following lemma says that \(O(\frac{1}{\varepsilon}\log\frac{1}{\delta})\) samples are sufficient to tell whether the mean of a random variable is below \(\varepsilon\) or above \(\frac{\varepsilon}{2}\):

**Lemma G.3**.: _Given \(T=\frac{40}{\varepsilon}\log\frac{2}{\delta}\) i.i.d. samples \(X^{(1)},\ldots,X^{(T)}\) of a random variable \(X\in[0,1]\) with unknown mean \(\mathbb{E}[X]=\mu\), with probability at least \(1-\delta\) we can tell whether \(\mu<\varepsilon\) or \(\mu\geq\frac{\varepsilon}{2}\). This can be done by checking whether the empirical mean \(\hat{\mu}=\frac{1}{T}\sum_{t=1}^{T}X^{(t)}\) is \(<\frac{3}{4}\varepsilon\) or \(\geq\frac{3}{4}\varepsilon\)._

Proof.: If \(\mu\geq\varepsilon\), using the multiplicative version of Chernoff bound we have

\[\Pr\left[\hat{\mu}<\frac{3}{4}\varepsilon\right]\leq\Pr\left[\hat{\mu}<\frac{3 }{4}\mu\right]\leq e^{-\frac{(\frac{4}{3})^{2}\mu T}{2}}\leq e^{-\frac{\varepsilon T }{32}}\leq\delta.\]

Namely, with probability at least \(1-\delta\), it holds that

\[\hat{\mu}\geq\frac{3}{4}\varepsilon.\]

If \(\mu<\varepsilon\), then using the additive version of Chernoff-Hoeffding theorem, we have

\[\Pr\left[\hat{\mu}<\mu-\frac{\varepsilon}{4}\right] \leq e^{-D(\mu-\frac{\varepsilon}{4}||\mu)T},\] \[\Pr\left[\hat{\mu}>\mu+\frac{\varepsilon}{4}\right] \leq e^{-D(\mu+\frac{\varepsilon}{4}||\mu)T},\]

where \(D(x||y)=x\ln\frac{x}{y}+(1-x)\ln\frac{1-x}{1-y}\). Using the inequality \(D(x||y)\geq\frac{(x-y)^{2}}{2y}\) for \(x\leq y\) and \(D(x||y)\geq\frac{(x-y)^{2}}{2x}\) for \(x\geq y\), we obtain:

\[\Pr\left[\hat{\mu}<\mu-\frac{\varepsilon}{4}\right] \leq e^{-\frac{(\frac{4}{3})^{2}}{2\mu}T}\leq e^{-\frac{(\frac{4}{ 3})^{2}}{2\varepsilon}T}=e^{-\frac{\varepsilon}{4\delta}T}\leq\frac{\delta}{2},\] \[\Pr\left[\hat{\mu}>\mu+\frac{\varepsilon}{4}\right] \leq e^{-\frac{(\frac{4}{3})^{2}}{2(\mu+\frac{\varepsilon}{4})}T}\leq e^{- \frac{(\frac{4}{3})^{2}}{2(\frac{\varepsilon}{4})}T}=e^{-\frac{\varepsilon}{4 \delta}T}\leq\frac{\delta}{2}.\]

By a union bound, with probability at least \(1-\delta\), we have

\[|\hat{\mu}-\mu|\leq\frac{\varepsilon}{4}.\]

Combining the case of \(\mu\geq\varepsilon\) and \(\mu<\varepsilon\), we conclude that: with probability at least \(1-\delta\),

* If \(\hat{\mu}<\frac{3}{4}\varepsilon\), then we must have \(\mu<\varepsilon\).
* If \(\hat{\mu}\geq\frac{3}{4}\varepsilon\), then we have \(\mu\geq\varepsilon\) or \(\varepsilon>\mu\geq\hat{\mu}-\frac{\varepsilon}{4}\geq\frac{\varepsilon}{2}\). In either case, we have \(\mu\geq\frac{\varepsilon}{2}\).

The last lemma we will use shows how to estimate the unknown value of \(\rho=\frac{p}{1-p}=\frac{P(\omega=1)}{P(\omega=0)}\) with accuracy \(\Delta\) using \(T=O(\frac{1}{n\Delta^{2}}\log\frac{1}{\delta})\) samples. Notice that, if one simply uses the empirical value \(\hat{\rho}=\frac{\sum_{t=1}^{T}\frac{1}{\{\omega^{(t)}=1\}}}{\sum_{t=1}^{T}\frac {1}{\{\omega^{(t)}=0\}}}\) to estimate \(\rho\), then by Chernoff bound this needs \(T=O(\frac{1}{\Delta^{2}}\log\frac{1}{\delta})\) samples, which is larger than what we claim by a factor of \(n\). This sub-optimality is because one only uses the \(\omega^{(t)}\)'s in the samples to estimate \(\rho\), wasting the reports \(r_{i}^{(t)}\)'s. By using \(r_{i}^{(t)}\)'s to estimate \(\rho\), we can reduce the number of samples by a factor of \(n\). The basic idea is the following: According to Fact G.1, we have \(\rho=\frac{p}{1-p}=\frac{\mu_{0}}{\mu_{1}}=\frac{\frac{p}{1-\varepsilon}r_{i} \lfloor\omega=0\rfloor}{\frac{p}{\lfloor\sum_{i=1}^{n}(1-r_{i})\rfloor\omega= 1}}\). The numerator \(\mathbb{E}[\sum_{i=1}^{n}r_{i}\mid\omega=0]\) and the denominator \(\mathbb{E}[\sum_{i=1}^{n}(1-r_{i})\mid\omega=1]\) can be estimated from samples of \(r_{i}^{(t)}\)'s where \(\omega^{(t)}=0\) and \(1\) respectively. The total number of \(r_{i}^{(t)}\)'s is \(Tn\), because we have \(n\) experts per sample. This reduces the needed number of samples by a factor of \(n\). Formally:

**Lemma G.4**.: _For conditionally independent distribution \(P\), we can estimate \(\rho=\frac{P(\omega=1)}{P(\omega=0)}\) with accuracy \(\frac{|\hat{\rho}-\rho|}{\rho}\leq\Delta<1\) (equivalently, \(\frac{\hat{\rho}}{\rho}\in 1\pm\Delta\)) with probability at least \(1-\delta\) using_

\[T=O\bigg{(}\frac{1}{(1-p)\mu_{0}\cdot n\cdot\Delta^{2}}\log\frac{1}{\delta}\;+ \;\frac{1}{\min\{p,1-p\}}\log\frac{1}{\delta}\bigg{)}\]samples of \((\omega^{(t)},\mathbf{r}^{(t)})\)'s, by letting \(\hat{\rho}=\frac{\frac{1}{\#_{0}}\sum_{t:\omega^{(t)}=0}\sum_{i=1}^{n}r_{i}^{(t)}} {\frac{1}{\#_{1}}\sum_{t:\omega^{(t)}=1}\sum_{i=1}^{n}(1-r_{i}^{(t)})}\), where \(\#_{0}\) and \(\#_{1}\) are the numbers of samples with \(\omega^{(t)}=0\) and \(1\) respectively._

Proof.: Recall that \(p=P(\omega=1)\), \(1-p=P(\omega=0)\), and \(\rho=\frac{p}{1-p}\). According to Fact G.1 (which says \((1-p)\mu_{0}=p\mu_{1}\)), we have

\[\rho=\frac{p}{1-p}=\frac{\mu_{0}}{\mu_{1}}=\frac{\sum_{i=1}^{n}\mathbb{E}[r_{i }\mid\omega=0]}{\sum_{i=1}^{n}\mathbb{E}[1-r_{i}\mid\omega=1]}. \tag{44}\]

Consider the following way of estimating \(\rho\) from \(T\) samples \((\omega^{(t)},\mathbf{r}^{(t)})_{t=1}^{T}\): Let \(\#_{0}\), \(\#_{1}\) be the numbers of samples where \(\omega^{(t)}=0,1\), respectively:

\[\#_{0}=\sum_{t=1}^{T}\mathbb{I}\left\{\omega^{(t)}=0\right\},\ \ \ \ \#_{1}=\sum_{t=1}^{T}\mathbb{I}\left\{\omega^{(t)}=1\right\}.\]

We let

\[\hat{\rho}=\frac{\frac{\frac{1}{\#_{0}}\sum_{t:\omega^{(t)}=0}\sum_{i=1}^{n}r_{ i}^{(t)}}{\frac{1}{\#_{1}}\sum_{t:\omega^{(t)}=1}\sum_{i=1}^{n}(1-r_{i}^{(t)})}}. \tag{45}\]

Now, we compare the \(\hat{\rho}\) in (45) and the \(\rho\) in (44): we see that \(\frac{1}{\#_{0}}\sum_{t:\omega^{(t)}=0}\sum_{i=1}^{n}r_{i}^{(t)}\) is an (unbiased) estimate of the numerator \(\sum_{i=1}^{n}\mathbb{E}[r_{i}\mid\omega=0]=n\mu_{0}\) and that \(\frac{1}{\#_{1}}\sum_{t:\omega^{(t)}=1}\sum_{i=1}^{n}(1-r_{i}^{(t)})\) is an (unbiased) estimate of the denominator \(\sum_{i=1}^{n}\mathbb{E}[1-r_{i}\mid\omega=1]=n\mu_{1}\). We use Chernoff bounds to argue that the accuracy of the two estimates is within \(\Delta\) with high probability if \(\#_{0}\) and \(\#_{1}\) are big enough. Suppose that, when drawing the \(T\) samples, we draw all the \(\omega^{(t)}\)'s first (and hence \(\#_{0},\#_{1}\) are determined), and then draw all the \(r_{i}^{(t)}\)'s. After all the \(\omega^{(t)}\)'s are drawn, the \(r_{i}^{(t)}\)'s become independent, because the signals \(s_{i}^{(t)}\)'s are conditionally independent given \(\omega^{(t)}\). Therefore, we can use Chernoff bounds:

\[\Pr\Big{[}\left|\frac{1}{\#_{0}}\sum_{t:\omega^{(t)}=0}\sum_{i=1} ^{n}r_{i}^{(t)}-n\mu_{0}\right|>\Delta n\mu_{0}\Big{]}\leq 2e^{-\frac{\#_{0}n \mu_{0}\Delta^{2}}{3}},\] \[\Pr\Big{[}\left|\frac{1}{\#_{1}}\sum_{t:\omega^{(t)}=1}\sum_{i= 1}^{n}(1-r_{i}^{(t)})-n\mu_{1}\right|>\Delta n\mu_{1}\Big{]}\leq 2e^{-\frac{\#_{1}n \mu_{1}\Delta^{2}}{3}}.\]

Requiring \(\delta\geq 2e^{-\frac{\#_{0}n\mu_{0}\Delta^{2}}{3}}\) and \(\delta\geq 2e^{-\frac{\#_{1}n\mu_{1}\Delta^{2}}{3}}\), namely,

\[\#_{0}\geq\frac{3}{n\mu_{0}\Delta^{2}}\log\frac{2}{\delta},\ \ \ \ \ \#_{1}\geq\frac{3}{n\mu_{1}\Delta^{2}}\log\frac{2}{\delta}, \tag{46}\]

we have, with probability at least \(1-2\delta\), both of the following hold:

\[\frac{1}{\#_{0}}\sum_{t:\omega^{(t)}=0}\sum_{i=1}^{n}r_{i}^{(t)}\ \in\ (1\pm \Delta)n\mu_{0},\ \ \ \ \ \frac{1}{\#_{1}}\sum_{t:\omega^{(t)}=1}\sum_{i=1}^{n}(1-r_{i}^{(t)})\ \in\ (1\pm \Delta)n\mu_{1}, \tag{47}\]

Then, we argue that (46) can be satisfied with high probability if \(T\) is large enough. This is again done by a Chernoff bound: since \(\mathbb{E}[\#_{j}]=\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\left\{\omega^{(t)}=j \right\}]=T\cdot P(\omega=j)\), for \(j=0,1\), we have

\[\Pr\Big{[}\big{|}\#_{0}-T(1-p)\big{|}\geq\tfrac{1}{2}T(1-p)\big{]}\leq 2e^{- \frac{T(1-p)(\frac{1}{2})^{2}}{3}},\ \ \ \ \Pr\Big{[}\big{|}\#_{1}-Tp\big{|}\geq\tfrac{1}{2}Tp\Big{]}\leq 2e^{-\frac{Tp(\frac{1}{2})^{2}}{3}}. \tag{48}\]

So, if we are given

\[T\geq\frac{12}{\min\{p,1-p\}}\log\frac{2}{\delta} \tag{49}\]samples, then we can ensure that with probability at least \(1-2\delta\), it holds \(\#_{0}\geq\frac{1}{2}T(1-p)\) and \(\#_{1}\geq\frac{1}{2}Tp\). Then, in order for (46) to be satisfied, we can let

\[\frac{1}{2}T(1-p)\geq\frac{3}{n\mu_{0}\Delta^{2}}\log\frac{2}{\delta},\quad\quad \frac{1}{2}Tp\geq\frac{3}{n\mu_{1}\Delta^{2}}\log\frac{2}{\delta}.\]

This gives

\[T\;\geq\;\max\left\{\frac{6}{(1-p)\mu_{0}\cdot n\Delta^{2}}\log \frac{2}{\delta},\;\;\frac{6}{p\mu_{1}\cdot n\Delta^{2}}\log\frac{2}{\delta} \right\}\;\stackrel{{(1-p)\mu_{0}=p\mu_{1}}}{{=}}\frac{6}{(1-p) \mu_{0}\cdot n\Delta^{2}}\log\frac{2}{\delta}. \tag{50}\]

Both (49) and (50) are satisfied when

\[T\;\geq\;\frac{6}{(1-p)\mu_{0}\cdot n\Delta^{2}}\log\frac{2}{\delta}\;+\; \frac{12}{\min\{p,1-p\}}\log\frac{2}{\delta}.\]

To conclude, if we are given \(T=\frac{6}{(1-p)\mu_{0}\cdot n\Delta^{2}}\log\frac{2}{\delta}+\frac{12}{\min\{ p,1-p\}}\log\frac{2}{\delta}\) samples, then with probability at least \(1-4\delta\), (47) holds, which implies

\[\hat{\rho}\;\in\;\frac{(1\pm\Delta)\mu_{0}}{(1\pm\Delta)\mu_{1}}\;=\;\frac{(1 \pm\Delta)}{(1\pm\Delta)}\rho\;\subseteq\;(1\pm 4\Delta)\rho\quad\Longrightarrow \quad\frac{|\hat{\rho}-\rho|}{\rho}\leq 4\Delta,\]

for \(\Delta<\frac{1}{4}\). 

#### g.1.2 The Proof

We want to show the \(O(\frac{1}{\varepsilon n(\frac{1}{1+\gamma})^{2}}\log\frac{1}{\delta}+\frac{1 }{\varepsilon}\log\frac{1}{\delta})\) sample complexity upper bound for the case where experts have \(\gamma\)-strongly informative signals.

We first use \(O(\frac{1}{\varepsilon}\log\frac{1}{\delta})\) samples tell whether \((1-p)\mu_{0}=p\mu_{1}<\frac{\varepsilon}{2}\) or \((1-p)\mu_{0}=p\mu_{1}\geq\frac{\varepsilon}{4}\). We note that

\[(1-p)\mu_{0}=P(\omega=0)\cdot\mathbb{E}\Big{[}\frac{1}{n}\sum_{i=1}^{n}r_{i} \mid\omega=0\Big{]}=\mathbb{E}\Big{[}\mathbb{1}\{\omega=0\}\cdot\frac{1}{n}\sum _{i=1}^{n}r_{i}\Big{]},\]

which is the expectation of the random variable \(X=\mathbb{1}\{\omega=0\}\cdot\frac{1}{n}\sum_{i=1}^{n}r_{i}\). So, according to Lemma G.3, we can tell whether \((1-p)\mu_{0}<\frac{\varepsilon}{2}\) or \(\geq\frac{\varepsilon}{4}\) with probability at least \(1-\delta\) using \(O(\frac{1}{\varepsilon}\log\frac{1}{\delta})\) samples of \(X\). If \((1-p)\mu_{0}=p\mu_{1}<\frac{\varepsilon}{2}\), then according to Lemma G.2, the averaging aggregator \(f_{avg}(\mathbf{r})=\frac{1}{n}\sum_{i=1}^{n}r_{i}\) is \(\varepsilon\)-optimal. We hence obtained an \(\varepsilon\)-optimal aggregator in this case. So, in the following proof, we assume \((1-p)\mu_{0}=p\mu_{1}\geq\frac{\varepsilon}{4}\).

For each expert \(i\), let \(\mathcal{S}^{1}_{i}=\{s_{i}\in\mathcal{S}_{i}:\frac{P(s_{i}\mid\omega=1)}{P(s _{i}\mid\omega=0)}\geq 1+\gamma\}\) be its set of \(\gamma\)-strongly informative signals that are more likely to be realized under \(\omega=1\) than under \(\omega=0\). Let \(\mathcal{S}^{0}_{i}=\mathcal{S}_{i}\setminus\mathcal{S}^{1}_{i}=\{s_{i}\in \mathcal{S}_{i}:\frac{P(s_{i}\mid\omega=1)}{P(s_{i}\mid\omega=0)}\leq\frac{1} {1+\gamma}\}\) be the set of signals that are more likely to be realized under \(\omega=0\). Since \(\frac{r_{i}}{1-r_{i}}=\frac{P(s_{i}\mid\omega=1)}{P(s_{i}\mid\omega=0)}\rho\) by Equation (1), whenever an expert receives a signal in \(\mathcal{S}^{1}_{i}\), its report satisfies

\[\frac{r_{i}}{1-r_{i}}\geq(1+\gamma)\rho,\quad\forall s_{i}\in\mathcal{S}^{1}_{i }; \tag{51}\]

and whenever it receives a signal in \(\mathcal{S}^{0}_{i}\), its report satisfies

\[\frac{r_{i}}{1-r_{i}}\leq\frac{1}{1+\gamma}\rho,\quad\forall s_{i}\in \mathcal{S}^{0}_{i}. \tag{52}\]

We will use the notation \(P(\mathcal{S}^{u}_{i}\mid\omega)=P(s_{i}\in\mathcal{S}^{u}_{i}\mid\omega)= \sum_{s_{i}\in\mathcal{S}^{u}_{i}}P(s_{i}\mid\omega)\), for \(u\in\{0,1\}\). Given a set of \(n\) signals \(s_{1},\ldots,s_{n}\), one per expert, we let \(X^{1}=\sum_{i=1}^{n}\mathbb{1}\{s_{i}\in\mathcal{S}^{1}_{i}\}\) be the total number of signals that belong to the \(\mathcal{S}^{1}_{i}\) sets, and similarly let \(X^{0}=\sum_{i=1}^{n}\mathbb{1}\{s_{i}\in\mathcal{S}^{0}_{i}\}\). We have \(X^{0}+X^{1}=n\), and by definition,

\[\mathbb{E}[X^{1}\mid\omega=1] =\;\sum_{i=1}^{n}P(\mathcal{S}^{1}_{i}\mid\omega=1)\;\geq\;(1+ \gamma)P(\mathcal{S}^{1}_{i}\mid\omega=0)\;=\;(1+\gamma)\mathbb{E}[X^{1}\mid \omega=0]. \tag{53}\] \[\mathbb{E}[X^{0}\mid\omega=0] =\;\sum_{i=1}^{n}P(\mathcal{S}^{0}_{i}\mid\omega=0)\;\geq\;(1+ \gamma)P(\mathcal{S}^{0}_{i}\mid\omega=1)\;=\;(1+\gamma)\mathbb{E}[X^{0}\mid \omega=1], \tag{54}\]

**Claim G.5**.: _At least one of \(\mathbb{E}[X^{1}\mid\omega=1]\) and \(\mathbb{E}[X^{0}\mid\omega=0]\) is \(\geq\frac{n}{2}\)._

Proof.: Suppose on the contrary both \(\mathbb{E}[X^{1}\mid\omega=1]\) and \(\mathbb{E}[X^{0}\mid\omega=0]\) are \(<\frac{n}{2}\). Then, from (54) we have

\[\mathbb{E}[X^{0}\mid\omega=1]\leq\frac{1}{1+\gamma}\mathbb{E}[X^{0}\mid\omega=0 ]<\frac{n}{2}.\]

This implies \(n=\mathbb{E}[X^{0}+X^{1}\mid\omega=1]<\frac{n}{2}+\frac{n}{2}=n\), a contradiction. 

Let \(u\in\{0,1\}\) be an index such that

\[\mathbb{E}[X^{u}\mid\omega=u]\geq\frac{n}{4}. \tag{55}\]

Claim G.5 guarantees that such a \(u\) exists. We construct a "hypothetical" aggregator \(f_{\mathrm{hypo}}\) that, having access to \(\rho\) and \(\mathbb{E}[X^{u}\mid\omega]\), predicts whether \(\omega=0\) or \(1\) by counting the number \(X^{u}\) of signals that belong to the \(\mathcal{S}_{i}^{u}\) sets and comparing it with its expectations under \(\omega=0\) and \(1\), \(\mathbb{E}[X^{u}\mid\omega=0]\) and \(\mathbb{E}[X^{u}\mid\omega=1]\), respectively. Specifically, given reports \(\mathbf{r}=(r_{1},\ldots,r_{n})\) as input, with corresponding unobserved signals \(\mathbf{s}=(s_{1},\ldots,s_{n})\), \(f_{\mathrm{hypo}}\) does the following:

1. If \(u=1\), count how many reports \(r_{i}\)'s satisfy \(\frac{r_{i}}{1-r_{i}}\geq(1+\gamma)\rho\); If \(u=0\), count how many reports \(r_{i}\)'s satisfy \(\frac{r_{i}}{1-r_{i}}\leq\frac{1}{1+\gamma}\rho\). According to (51) and (52), this number is exactly equal to the number of signals that belong to the \(\mathcal{S}_{i}^{u}\) sets, \(X^{u}\).
2. Then, check whether \(X^{u}\) is closer (in terms of absolute difference) to \(\mathbb{E}[X^{u}\mid\omega=u]\) or \(\mathbb{E}[X^{u}\mid\omega=1-u]\). If \(X^{u}\) is closer to \(\mathbb{E}[X^{u}\mid\omega=u]\), output \(f_{\mathrm{hypo}}(\mathbf{r})=u\); otherwise, output \(f_{\mathrm{hypo}}(\mathbf{r})=1-u\).

We claim that \(f_{\mathrm{hypo}}\) is \(\varepsilon\)-optimal.

**Claim G.6**.: _Given \(\frac{\gamma}{1+\gamma}\geq 8\sqrt{\frac{2}{n}\log\frac{2}{\varepsilon}}\) and \(\mathbb{E}[X^{u}\mid\omega=u]\geq\frac{n}{4}\), \(f_{\mathrm{hypo}}\) is \(\varepsilon\)-optimal._

Proof.: Given either \(\omega=0\) or \(1\), consider the conditional random draw of signals \(s_{1},\ldots,s_{n}\). Because \(X^{u}=\sum_{i=1}^{n}\mathbb{I}\{s_{i}\in\mathcal{S}_{i}^{u}\}\) and the random variables \(\mathbb{I}\{s_{i}\in\mathcal{S}_{i}^{u}\}\), \(i=1,\ldots,n\), are \([0,1]\)-bounded and independent conditioning \(\omega\), by Hoeffding's inequality we have

\[\Pr\Big{[}\Big{|}\,X^{u}-\mathbb{E}[X^{u}\mid\omega]\,\Big{|}\geq a\,\Big{|}\, \omega\Big{]}\leq 2e^{-\frac{2a^{2}}{n}}.\]

Let

\[a=\sqrt{\frac{n}{2}\log\frac{2}{\varepsilon}}. \tag{56}\]

Then with probability at least \(1-2e^{-\frac{2a^{2}}{n}}=1-\varepsilon\), it holds

\[\big{|}\,X^{u}-\mathbb{E}[X^{u}\mid\omega]\,\big{|}<a. \tag{57}\]

Consider the difference between \(\mathbb{E}[X^{u}\mid\omega=u]\) and \(\mathbb{E}[X^{u}\mid\omega=1-u]\). By (53) and (54), we have

\[\mathbb{E}[X^{u}\mid\omega=1-u]\;\leq\;\frac{1}{1+\gamma}\mathbb{E}[X^{u}\mid \omega=u]\;=\;\Big{(}1-\frac{\gamma}{1+\gamma}\Big{)}\mathbb{E}[X^{u}\mid\omega =u].\]

By the assumption \(\mathbb{E}[X^{u}\mid\omega=u]\geq\frac{n}{4}\),

\[\mathbb{E}[X^{u}\mid\omega=1-u]\;\leq\;\mathbb{E}[X^{u}\mid\omega=u]-\frac{ \gamma}{1+\gamma}\cdot\frac{n}{4}.\]

By the assumption \(\frac{\gamma}{1+\gamma}\geq 8\sqrt{\frac{2}{n}\log\frac{2}{\varepsilon}}\), we have \(\frac{\gamma}{1+\gamma}\cdot\frac{n}{4}\;\geq\;8\sqrt{\frac{2}{n}\log\frac{2}{ \varepsilon}}\cdot\frac{n}{4}\;=4\sqrt{\frac{n}{2}\log\frac{2}{\varepsilon}}= 4a\). Therefore

\[\mathbb{E}[X^{u}\mid\omega=u]-\mathbb{E}[X^{u}\mid\omega=1-u]\geq 4a. \tag{58}\]

Because we already had \(\big{|}\,X^{u}-\mathbb{E}[X^{u}\mid\omega]\,\big{|}<a\) (which happened with probability at least \(1-\varepsilon\)), if \(X^{u}\) turns out to be closer to \(\mathbb{E}[X^{u}\mid\omega=u]\) it must be that \(\omega=u\); if \(X^{u}\) turns out to be closer to \(\mathbb{E}[X^{u}\mid\omega=1-u]\) it must be that \(\omega=1-u\). In either case, our output \(f_{\mathrm{hypo}}(\mathbf{r})\) is equal to\(\omega\), having a loss \(0\). If \(\left|X^{u}-\mathbb{E}[X^{u}\mid\omega]\right|<a\) did not happen, our loss is at most \(1\). Therefore, the expected loss of our aggregator \(f_{\mathrm{hypo}}\) is at most

\[L_{P}(f_{\mathrm{hypo}})=\mathbb{E}_{\omega}\Big{[}\mathbb{E}\big{[}|f_{ \mathrm{hypo}}(\boldsymbol{r})-\omega|^{2}\mid\omega\big{]}\Big{]}\leq\mathbb{ E}_{\omega}\Big{[}(1-\varepsilon)\cdot 0+\varepsilon\cdot 1\Big{]}=\varepsilon.\]

Since the expected loss of the optimal aggregator \(f^{*}\) is non-negative, \(f_{\mathrm{hypo}}\) is \(\varepsilon\)-optimal. 

In the remaining proof, we show how to use samples to learn a "real" aggregator \(\hat{f}\) that implements the same functionality as the hypothetical aggregator \(f_{\mathrm{hypo}}\) and hence is \(\varepsilon\)-optimal. We have two learning tasks: First, we need to _estimate_\(\rho\), so that we can implement the step (1) of \(f_{\mathrm{hypo}}\) which tells apart \(\frac{r_{i}}{1-r_{i}}\geq(1+\gamma)\rho\) and \(\frac{r_{i}}{1-r_{i}}\leq\frac{1}{1+\gamma}\rho\). Second, we need to _find an index \(u\in\{0,1\}\) such that \(\mathbb{E}[X^{u}\mid\omega=u]\geq\frac{n}{4}\) and estimate \(\mathbb{E}[X^{u}\mid\omega=u]\)_, so that we can implement the step (2) of \(f_{\mathrm{hypo}}\) which tells whether \(X^{u}\) is closer to \(\mathbb{E}[X^{u}\mid\omega=u]\) or \(\mathbb{E}[X^{u}\mid\omega=1-u]\). We show that these two tasks can be achieved using \(O(\frac{1}{\varepsilon n(\frac{1}{1+\gamma})^{2}}\log\frac{1}{\delta}\,+\, \frac{1}{\varepsilon}\log\frac{1}{\delta})\) samples, with probability at least \(1-O(\delta)\).

Task 1: estimate \(\rho\), using \(T_{1}=O(\frac{1}{\varepsilon n(\frac{1}{1+\gamma})^{2}}\log\frac{1}{\delta}\,+ \,\frac{1}{\varepsilon}\log\frac{1}{\delta})\) samples.We want to use samples to obtain an estimate \(\hat{\rho}\) of \(\rho\) such that \(\frac{1}{1+\gamma}\rho<\hat{\rho}<(1+\gamma)\rho\). So, by checking whether \(\frac{r_{i}}{1-r_{i}}>\hat{\rho}\) or \(\frac{r_{i}}{1-r_{i}}<\hat{\rho}\) we can tell apart \(\frac{r_{i}}{1-r_{i}}\geq(1+\gamma)\rho\) and \(\frac{r_{i}}{1-r_{i}}\leq\frac{1}{1+\gamma}\rho\). Using Lemma G.4 with \(\Delta=\frac{\gamma}{1+\gamma}\), we obtain a \(\hat{\rho}\) such that

\[\hat{\rho}\in(1\pm\Delta)\rho,\]

with probability at least \(1-\delta\) using

\[T_{1}=O\bigg{(}\frac{1}{(1-p)\mu_{0}n\Delta^{2}}\log\frac{1}{\delta}\,+\, \frac{1}{\min\{p,1-p\}}\log\frac{1}{\delta}\bigg{)}\leq O\bigg{(}\frac{1}{ \varepsilon n(\frac{\gamma}{1+\gamma})^{2}}\log\frac{1}{\delta}\,+\,\frac{1}{ \varepsilon}\log\frac{1}{\delta}\bigg{)}\]

samples (recall that we have \(\min\{p,1-p\}\geq(1-p)\mu_{0}=p\mu_{1}\geq\frac{\varepsilon}{4}\)). The \(\hat{\rho}\) then satisfies

\[\hat{\rho}<\Big{(}1+\frac{\gamma}{1+\gamma}\Big{)}\rho<(1+\gamma)\rho\quad \text{and}\quad\hat{\rho}>\Big{(}1-\frac{\gamma}{1+\gamma}\Big{)}\rho=\frac{1 }{1+\gamma}\rho,\]

as desired.

Task 2: find \(u\) such that \(\mathbb{E}[X^{u}\mid\omega=u]\geq\frac{n}{4}\) and estimate \(\mathbb{E}[X^{u}\mid\omega=u]\), using \(T_{2}=O(\frac{1}{\varepsilon}\log\frac{1}{\delta})\) samples.First, we show how to use \(T_{2}=O(\frac{1}{\varepsilon}\log\frac{1}{\delta})\) samples to estimate both \(\mathbb{E}[X^{0}\mid\omega=0]\) and \(\mathbb{E}[X^{1}\mid\omega=1]\) with accuracy \(a=\sqrt{\frac{n}{2}\log\frac{2}{\varepsilon}}\). By the same argument as in the proof of Lemma G.4 (Equations 48 and 49), we know that with probability at least \(1-2\delta\) over the random draws of

\[T_{2}\geq\frac{12}{\min\{p,1-p\}}\log\frac{2}{\delta} \tag{59}\]

samples, the numbers of samples \((\omega^{(t)},r_{1}^{(t)},\ldots,r_{n}^{(t)})\)'s where \(\omega^{(t)}=0\) and \(\omega^{(t)}=1\), denoted by \(\#_{0}\) and \(\#_{1}\), must satisfy

\[\#_{0}\geq\frac{1}{2}(1-p)T_{2},\qquad\#_{1}\geq\frac{1}{2}pT_{2}.\]

We consider the samples where \(\omega^{(t)}=0\). There are \(\#_{0}n\) total number of \(r_{i}^{(t)}\)'s. Suppose we have accomplished Task 1. Then, for each \(r_{i}^{(t)}\), we can tell whether the corresponding signal \(s_{i}^{(t)}\) belongs to \(\mathcal{S}_{i}^{0}\) by checking whether \(\frac{r_{i}^{(t)}}{1-r_{i}^{(t)}}<\hat{\rho}\). So, we can exactly compute the total number of such signals in the \(t\)-th sample, \(X^{0(t)}=\sum_{i=1}^{n}\mathbb{1}\left\{s_{i}^{(t)}\in\mathcal{S}_{i}^{0}\right\}\), whose expected value is \(\mathbb{E}[X^{0}\mid\omega=0]\). Because signals are independent given \(\omega^{(t)}=0\), by Hoeffding's inequality we have

\[\Pr\bigg{[}\Big{|}\sum_{t:\omega^{(t)}=0}\underbrace{\sum_{i=1}^{n}\mathbb{1} \left\{s_{i}^{(t)}\in\mathcal{S}_{i}^{0}\right\}}_{X^{0(t)}}-\#_{0}\mathbb{E}[X ^{0}\mid\omega=0]\Big{|}\,\geq\,\#_{0}a\bigg{]}\leq 2e^{-\frac{2(\#_{0}a)^{2}}{\#_{0}n}} =2e^{-\frac{2\#_{0}a^{2}}{n}}.\]

[MISSING_PAGE_FAIL:37]

Given reports \(\mathbf{r}=(r_{1},\ldots,r_{n})\) as input, we let \(\hat{f}\) do the following:

1. If \(u=1\), now how many reports \(r_{i}\)'s satisfy \(\frac{r_{i}}{1-r_{i}}>\hat{\rho}\); If \(u=0\), count how many reports \(r_{i}\)'s satisfy \(\frac{r_{i}}{1-r_{i}}<\hat{\rho}\). Let this number be \(X\);
2. Then, check whether \(X>M\) or \(X\leq M\). If \(X>M\), output \(\hat{f}(\mathbf{r})=u\); otherwise, output \(\hat{f}(\mathbf{r})=1-u\).

We argue that \(\hat{f}\) implements the same functionality as \(f_{\mathrm{hypo}}\): (1) In Task 1 we got \(\frac{1}{1+\gamma}\rho<\hat{\rho}<(1+\gamma)\rho\). So, by checking whether \(\frac{r_{i}}{1-r_{i}}>\hat{\rho}\) or \(<\hat{\rho}\) we can exactly tell whether \(\frac{r_{i}}{1-r_{i}}\geq(1+\gamma)\rho\) or \(\leq\frac{1}{1+\gamma}\rho\). Hence, we have \(X=X^{u}\), the number of signals that belong to the \(\mathcal{S}_{i}^{u}\) sets. (2) Recall from Equation (57) that with probability at least \(1-\varepsilon\), \(X\) is \(a\)-close to its expectation \(\mathbb{E}[X^{u}\mid\omega]\). Then, according to Claim G.7, \(X>M\) implies that \(X\) is closer to \(\mathbb{E}[X^{u}\mid\omega=u]\); \(X<M\) implies that \(X\) is closer to \(\mathbb{E}[X^{u}\mid\omega=1-u]\). So, \(\hat{f}\) implements both of the two steps in \(f_{\mathrm{hypo}}\). Hence, according to Claim G.6, \(\hat{f}\) is \(\varepsilon\)-optimal.

### Proof of Theorem b.3

According to Lemma 5.1, the optimal aggregator is

\[f^{*}(\mathbf{r})=\frac{1}{1+\rho^{n-1}\prod_{i=1}^{n}\frac{1-r_{i}}{r_{i}}}, \tag{61}\]

where \(\rho=\frac{p}{1-p}\). We claim that an approximately optimal aggregator can be obtained by first estimating \(\rho\) from samples and then use the aggregator with the estimate \(\hat{\rho}\):

\[\hat{f}(\mathbf{r})=\frac{1}{1+\hat{\rho}^{n-1}\prod_{i=1}^{n}\frac{1-r_{i}}{r_{i} }}. \tag{62}\]

**Claim G.8**.: _If \(\frac{|\hat{\rho}-\rho|}{\rho}\leq\frac{2\sqrt{\varepsilon}}{n-1}<\frac{1}{2}\), then the aggregator \(\hat{f}\) defined above is \(\varepsilon\)-optimal._

Proof.: Consider the function \(g(\hat{\rho})=\frac{1}{1+\hat{\rho}^{n-1}\prod_{i=1}^{n}\frac{1-r_{i}}{r_{i}}}\) (where \(\hat{\rho}\) is the variable and \(r_{i}\)'s are constants). We claim that

\[|g^{\prime}(\hat{\rho})|\leq\frac{n-1}{4\hat{\rho}}. \tag{63}\]

To see this, we note that if \(r_{i}=0\) for some \(i\) then \(g(\hat{\rho})=0\) and \(g^{\prime}(\hat{\rho})=0\). Otherwise, we let \(y=\prod_{i=1}^{n}\frac{1-r_{i}}{r_{i}}<+\infty\) and take the derivative with respect to \(\hat{\rho}\),

\[g^{\prime}(\hat{\rho})=-\frac{1}{(1+\hat{\rho}^{n-1}y)^{2}}(n-1)\hat{\rho}^{n- 2}y=-(n-1)\frac{(\hat{\rho}^{\frac{n}{2}-1}\sqrt{y})^{2}}{(1+\hat{\rho}^{n-1} y)^{2}}=-(n-1)\frac{1}{\left(\frac{1}{\hat{\rho}^{\frac{n}{2}-1}\sqrt{y}}+\hat{\rho}^{ \frac{n}{2}}\sqrt{y}\right)^{2}}.\]

By the AM-GM inequality \(a+b\geq 2\sqrt{ab}\), we get

\[|g^{\prime}(\hat{\rho})|\leq(n-1)\frac{1}{\left(2\sqrt{\frac{1}{\hat{\rho}^{ \frac{n}{2}-1}\sqrt{y}}\cdot\hat{\rho}^{\frac{n}{2}}\sqrt{y}}\right)^{2}}=(n-1 )\frac{1}{4\hat{\rho}},\]

as claimed.

Using (63), we have, for \(\hat{\rho}\geq\frac{\rho}{2}\),

\[|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|=|g(\hat{\rho})-g(\rho)|\leq\frac{n-1}{4\min\{ \hat{\rho},\rho\}}\cdot|\hat{\rho}-\rho|\leq\frac{n-1}{2}\cdot\frac{|\hat{\rho} -\rho|}{\rho}. \tag{64}\]

So, to obtain \(\varepsilon\)-approximation \(\mathbb{E}\big{[}|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|^{2}\big{]}\leq\varepsilon\), we can require \(|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|\leq\frac{n-1}{2}\cdot\frac{|\hat{\rho}-\rho|}{ \rho}\leq\sqrt{\varepsilon}\). This can be satisfied if the error in estimating \(\rho\) is at most \(\frac{|\hat{\rho}-\rho|}{\rho}\leq\frac{2\sqrt{\varepsilon}}{n-1}\).

We then show how to use \(O(\frac{\gamma n}{\varepsilon}\log\frac{1}{\delta})\) samples to estimate the value of \(\rho\) with \(O(\frac{\sqrt{\varepsilon}}{n-1})\) accuracy, which will give us an \(\varepsilon\)-optimal aggregator according to Claim G.8. Recall from (14) that when signals are \(\gamma\)-weakly informative, the reports always satisfy

\[\frac{1}{1+\gamma}\rho\leq\frac{r_{i}}{1-r_{i}}\leq(1+\gamma)\rho. \tag{65}\]

The following observation is the key:

**Lemma G.9**.: _For each expert \(i\), we have_

* \(\mathbb{E}\big{[}\,\frac{r_{i}}{1-r_{i}}\mid\omega=0\,\big{]}=\rho\)_;_
* \(\mathbb{E}\big{[}\,\frac{1-r_{i}}{r_{i}}\mid\omega=1\,\big{]}=\frac{1}{\rho}\)_._

_As corollaries, for \(k\) conditionally independent reports \(r_{1},\ldots,r_{k}\), we have \(\mathbb{E}\big{[}\prod_{i=1}^{k}\frac{r_{i}}{1-r_{i}}\mid\omega=0\,\big{]}=\rho ^{k}\) and \(\mathbb{E}\big{[}\prod_{i=1}^{k}\frac{1-r_{i}}{r_{i}}\mid\omega=1\,\big{]}= \frac{1}{\rho^{k}}\)._

Proof.: Because \(\frac{r_{i}}{1-r_{i}}=\frac{P(s_{i}|\omega=1)}{P(s_{i}|\omega=0)}\rho\) (from (13)), we have

\[\mathbb{E}\Big{[}\,\frac{r_{i}}{1-r_{i}}\,\big{|}\,\omega=0\,\Big{]}=\sum_{s_{ i}\in\mathcal{S}_{i}}P(s_{i}\mid\omega=0)\frac{P(s_{i}\mid\omega=1)}{P(s_{i} \mid\omega=0)}\rho=\sum_{s_{i}\in\mathcal{S}_{i}}P(s_{i}\mid\omega=1)\rho=\rho.\]

For conditionally independent \(r_{1},\ldots,r_{k}\), we have

\[\mathbb{E}\Big{[}\,\prod_{i=1}^{k}\frac{r_{i}}{1-r_{i}}\,\big{|}\,\omega=0\, \Big{]}=\prod_{i=1}^{k}\mathbb{E}\Big{[}\,\frac{r_{i}}{1-r_{i}}\,\big{|}\, \omega=0\,\Big{]}=\rho^{k}.\]

Similarly, we can prove \(\mathbb{E}\big{[}\frac{1-r_{i}}{r_{i}}\mid\omega=1\,\big{]}=\frac{1}{\rho}\) and \(\mathbb{E}\big{[}\prod_{i=1}^{k}\frac{1-r_{i}}{r_{i}}\mid\omega=1\,\big{]}= \frac{1}{\rho^{k}}\). 

Let \(\Delta=\frac{\sqrt{\varepsilon}}{3\gamma n}\) and suppose we are given \(T=\frac{6\varepsilon}{\gamma n\Delta^{2}}\log\frac{2}{\delta}=\frac{54 \varepsilon\gamma n}{\varepsilon}\log\frac{2}{\delta}=O(\frac{\gamma n}{ \varepsilon}\log\frac{1}{\delta})\) samples. Suppose when drawing the samples we first draw the events \(\omega^{(t)}\)'s, and then draw the reports \(r_{i}^{(t)}\)'s conditioning on \(\omega^{(t)}\) being \(0\) or \(1\). After the first step, the numbers of samples with \(\omega^{(t)}=0\) and \(\omega^{(t)}=1\) are determined, which we denote by \(\#_{0}\) and \(\#_{1}\). Since \(\#_{0}+\#_{1}=T\), one of them must be at least \(T/2\). We argue that whether \(\#_{0}\geq\frac{T}{2}\) or \(\#_{1}\geq\frac{T}{2}\) we can estimate \(\rho^{1/\gamma}\) with accuracy \(3\Delta\). (For simplicity, we assume that \(1/\gamma\) is an integer.)

* If \(\#_{0}\geq T/2\), then we consider the \(\#_{0}n\) reports \(r_{i}^{(t)}\)'s in the samples with \(\omega^{(t)}=0\). We divide these \(\#_{0}n\) reports evenly into \(\#_{0}n\gamma\) groups, each of size \(1/\gamma\), denoted by \(G_{1},\ldots,G_{\#_{0}n\gamma}\). Consider the product of \(\frac{r_{i}^{(t)}}{1-r_{i}^{(t)}}\)'s in a group \(G_{j}\): because \(r_{i}^{(t)}\)'s are independent given \(\omega=0\), by Lemma G.9 we have \[\mathbb{E}\Big{[}\prod_{r_{i}^{(t)}\in G_{j}}\frac{r_{i}^{(t)}}{1-r_{i}^{(t) }}\,\big{|}\,\omega=0\Big{]}=\rho^{1/\gamma}.\] Using (65) and the inequality \((1+\gamma)^{1/\gamma}\leq e\), we have \[\frac{1}{e}\rho^{1/\gamma}\,\leq\,\frac{1}{(1+\gamma)^{1/\gamma}}\rho^{1/ \gamma}\,\leq\prod_{r_{i}^{(t)}\in G_{j}}\frac{r_{i}^{(t)}}{1-r_{i}^{(t)}}\, \leq\,(1+\gamma)^{1/\gamma}\rho^{1/\gamma}\,\leq\,e\rho^{1/\gamma}.\] Let \(X_{j}\) be the random variable \(\frac{1}{e\rho^{1/\gamma}}\prod_{r_{i}^{(t)}\in G_{j}}\frac{r_{i}^{(t)}}{1-r_{ i}^{(t)}}\). From the above equation and inequality we have \(\mathbb{E}[X_{j}]=\frac{1}{e}\) and \(X_{j}\in[\frac{1}{e^{2}},1]\subseteq[0,1]\). So, by Chernoff bound, \[\Pr\Big{[}\,\frac{1}{\#_{0}n\gamma}\sum_{j=1}^{\#_{0}n\gamma}X_{j}\,\in\,(1 \pm\Delta)\frac{1}{e}\,\big{|}\,\omega=0\Big{]}\geq 1-2e^{-\frac{\#_{0}n \gamma\Delta^{2}}{3e}}\geq 1-2e^{-\frac{T\gamma n\Delta^{2}}{3e}}=1-\delta,\]given our choice of \(T\). Multiplying \(\frac{1}{\#_{0}n\gamma}\sum_{j=1}^{\#_{0}n\gamma}X_{j}\) by \(e\rho^{1/\gamma}\), we obtain the following estimate of \(\rho^{1/\gamma}\): \[\hat{\rho}_{0}^{1/\gamma}\;:=\;\frac{1}{\#_{0}n\gamma}\sum_{j=1}^{\#_{0}n \gamma}\prod_{r_{i}^{(t)}\in G_{j}}\frac{r_{i}^{(t)}}{1-r_{i}^{(t)}}\;\in\;(1 \pm\Delta)\rho^{1/\gamma}.\] Dividing by \(\rho^{1/\gamma}\), we get \((\frac{\hat{\rho}_{0}}{\rho})^{1/\gamma}\in 1\pm\Delta\).
* If \(\#_{1}\geq T/2\), then by considering the \(\#_{1}n\) reports in the samples with \(\omega^{(t)}=1\), dividing them into \(\#_{1}n\gamma\) groups of size \(1/\gamma\), \(H_{1},\ldots,H_{\#_{1}n\gamma}\), and similarly defining random variable \(Y_{j}=\frac{e^{1/\gamma}}{e}\prod_{r_{i}^{(t)}\in H_{j}}\frac{1-r_{i}^{(t)}}{ r_{i}^{(t)}}\), we obtain the following estimate of \((\frac{1}{\rho})^{1/\gamma}\): \[(\frac{1}{\hat{\rho}_{1}})^{1/\gamma}\;:=\;\frac{1}{\#_{1}n\gamma}\sum_{j=1}^{ \#_{1}n\gamma}\prod_{r_{i}^{(t)}\in H_{j}}\frac{1-r_{i}^{(t)}}{r_{i}^{(t)}}\; \in\;(1\pm\Delta)(\frac{1}{\rho})^{1/\gamma}.\] Multiplying by \(\rho^{1/\gamma}\), we get \((\frac{\rho}{\rho_{1}})^{1/\gamma}\in 1\pm\Delta\). Taking the reciprocal and noticing that \(\frac{1}{1\pm\Delta}\subseteq 1\pm 3\Delta\) when \(\Delta<\frac{1}{3}\), we obtain \((\frac{\hat{\rho}_{1}}{\rho})^{1/\gamma}\in 1\pm 3\Delta\).

From the discussion above we obtained an estimate \(\hat{\rho}\in\{\hat{\rho}_{0},\hat{\rho}_{1}\}\) of \(\rho\) such that \((\frac{\hat{\rho}}{\rho})^{1/\gamma}\in 1\pm 3\Delta\). Raising to the power of \(\gamma\), and using the inequality \((1-x)^{\gamma}\geq 1-x\gamma\) and \((1+x)^{\gamma}\leq e^{x\gamma}\leq 1+2x\gamma\) for \(x\gamma\leq 1\), we get

\[\frac{\hat{\rho}}{\rho}\;\in\;(1\pm 3\Delta)^{\gamma}\;\subseteq\;[1-3\Delta \gamma,\;e^{3\Delta\gamma}]\;\subseteq\;[1-3\Delta\gamma,\;1+6\Delta\gamma].\]

In particular, this implies \(\frac{|\hat{\rho}-\rho|}{\rho}\leq 6\Delta\gamma=\frac{2\sqrt{e}}{n}\). Then, according to Claim G.8, the aggregator \(\hat{f}\) defined by \(\hat{f}(\boldsymbol{r})=\frac{1}{1+\hat{\rho}^{n-1}\prod_{i=1}^{n}\frac{1-r_{i }}{r_{i}}}\) is \(\varepsilon\)-optimal. We hence obtained an \(\varepsilon\)-optimal aggregator.

## Appendix H Missing Proofs from Section 7

### Proof of Theorem 7.1

Regard \(P\) as a joint distribution over reports \(\boldsymbol{r}=(r_{1},\ldots,r_{n})\) and the state \(\omega\), where \(r_{i}\) is sampled by first sampling \(s_{i}\in\mathcal{S}_{i}\) and then letting \(r_{i}=P(\omega=1\,|\,s_{i})\). Since \(|\mathcal{S}_{i}|=m\), there are at most \(m\) different values of \(r_{i}\) that can be sampled, so there are at most \(2m^{n}\) different tuples of \((\boldsymbol{r},\omega)\) in the support of \(P\). For each such tuple \((\boldsymbol{r},\omega)\), consider the empirical probability of this tuple:

\[\hat{P}(\boldsymbol{r},\omega)=\frac{1}{T}\sum_{t=1}^{T}\mathbb{1}\big{[}( \boldsymbol{r}^{(t)},\omega^{(t)})=(\boldsymbol{r},\omega)\big{]}.\]

By the Chernoff bound, we have

\[\Pr\big{[}\big{|}\hat{P}(\boldsymbol{r},\omega)-P(\boldsymbol{r},\omega)\big{|} >\Delta P(\boldsymbol{r},\omega)\big{]}\leq 2e^{-\frac{TP(\boldsymbol{r},\omega) \Delta^{2}}{3}}.\]

Using a union bound for all the \(2m^{n}\) tuples and the fact that \(P(\boldsymbol{r},\omega)\geq P(\boldsymbol{s},\omega)>\frac{c}{m^{n}}\) (where \(\boldsymbol{s}\in\boldsymbol{\mathcal{S}}\) are some signals that generate \(\boldsymbol{r}\)), we have

\[\big{|}\hat{P}(\boldsymbol{r},\omega)-P(\boldsymbol{r},\omega)\big{|}\leq \Delta P(\boldsymbol{r},\omega) \tag{66}\]

holds for all tuples \((\boldsymbol{r},\omega)\) except with probability at most

\[2m^{n}\cdot 2e^{-\frac{TP(\boldsymbol{r},\omega)\Delta^{2}}{3}}\leq 4m^{n} \cdot e^{-\frac{7\Delta^{2}}{3m^{n}}}=\delta\]

if

\[T\geq\frac{3m^{n}}{c\Delta^{2}}\log\frac{4m^{n}}{\delta}. \tag{67}\]Assuming (66) holds, we consider the "empirical" Bayesian aggregator:

\[\hat{f}(\mathbf{r})=\frac{\hat{P}(\mathbf{r},\omega)}{\hat{P}(\mathbf{r})}.\]

Since (66) implies \(\hat{P}(\mathbf{r},\omega)\in(1\pm\Delta)P(\mathbf{r},\omega)\) and \(\hat{P}(\mathbf{r})\in(1\pm\Delta)P(\mathbf{r})\), we have

\[\hat{f}(\mathbf{r})\geq\frac{1-\Delta}{1+\Delta}\cdot\frac{P(\mathbf{r},\omega)}{P(\bm {r})}=\big{(}1-\frac{2\Delta}{1+\Delta}\big{)}f^{*}(\mathbf{r})\geq(1-2\Delta)f^{* }(\mathbf{r})\]

and

\[\hat{f}(\mathbf{r})\leq\frac{1+\Delta}{1-\Delta}\cdot\frac{P(\mathbf{r},\omega)}{P(\bm {r})}=\big{(}1+\frac{2\Delta}{1-\Delta}\big{)}f^{*}(\mathbf{r})\leq(1+4\Delta)f^{* }(\mathbf{r}),\]

if \(\Delta<\frac{1}{2}\). Putting these two inequalities together:

\[|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|\leq 4\Delta f^{*}(\mathbf{r}).\]

We note that this holds for all possible \(\mathbf{r}\) in the support of \(P\). So, the expected approximation error of \(\hat{f}\) is at most:

\[\mathbb{E}\big{[}|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|^{2}\big{]} =\sum_{\mathbf{r}}P(\mathbf{r})|\hat{f}(\mathbf{r})-f^{*}(\mathbf{r})|^{2}\] \[\leq\