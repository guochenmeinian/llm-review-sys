# Shuffling Gradient-Based Methods for

Nonconvex-Concave Minimax Optimization

 Quoc Tran-Dinh

Department of Statistics and Operations Research

The University of North Carolina at Chapel Hill

quoctd@email.unc.edu

&Trang H. Tran

School of OR and Information Engineering

Cornell University, Ithaca, NY

http7@cornell.edu

&Lam M. Nguyen

IBM Research, Thomas J. Watson Research Center

Yorktown Heights, NY

LamSw given.MLTD@ibm.com

###### Abstract

This paper aims at developing novel shuffling gradient-based methods for tackling two classes of minimax problems: _nonconvex-linear_ and _nonconvex-strongly concave_ settings. The first algorithm addresses the nonconvex-linear minimax model and achieves the state-of-the-art oracle complexity typically observed in nonconvex optimization. It also employs a new shuffling estimator for the "hyper-gradient", departing from standard shuffling techniques in optimization. The second method consists of two variants: _semi-shuffling_ and _full-shuffling_ schemes. These variants tackle the nonconvex-strongly concave minimax setting. We establish their oracle complexity bounds under standard assumptions, which, to our best knowledge, are the best-known for this specific setting. Numerical examples demonstrate the performance of our algorithms and compare them with two other methods. Our results show that the new methods achieve comparable performance with SGD, supporting the potential of incorporating shuffling strategies into minimax algorithms.

## 1 Introduction

Minimax problems arise in various applications across generative machine learning, game theory, robust optimization, online learning, and reinforcement learning (e.g., [1, 2, 3, 5, 12, 13, 17, 19, 21, 25, 35, 40]). These models often involve stochastic settings or large finite-sum objective functions. To tackle these problems, existing methods frequently adapt stochastic gradient descent (SGD) principles to develop algorithms for solving the underlying minimax problems [4, 13]. For instance, in generative adversarial networks (GANs), early algorithms employed stochastic gradient descent-ascent methods where two routines, each using an SGD loop, ran iteratively [13]. However, practical implementations of SGD often incorporate shuffling strategies, as seen in popular deep learning libraries like TensorFlow and PyTorch. This has motivated recent research on developing shuffling techniques specifically for optimization algorithms [4, 5, 8, 16, 26, 32, 38]. Our work builds upon this trend by developing shuffling methods for two specific classes of minimax problems.

**Problem statement.** In this paper, we study the following minimax optimization problem:

\[\min_{w\in\mathbb{R}^{p}}\max_{u\in\mathbb{R}^{q}}\Big{\{}\mathcal{L}(w,u):=f(w )+\mathcal{H}(w,u)-h(u)\equiv f(w)+\tfrac{1}{n}\sum_{i=1}^{n}\mathcal{H}_{i}(w,u)-h(u)\Big{\}},\] (1)where \(f:\mathbb{R}^{p}\to\mathbb{R}\cup\{+\infty\}\) is a proper, closed, and convex function, \(\mathcal{H}_{i}:\mathbb{R}^{p}\times\mathbb{R}^{q}\to\mathbb{R}\) are smooth for all \(i\in[n]:=\{1,2,\cdots,n\}\), and \(h:\mathbb{R}^{q}\to\mathbb{R}\cup\{+\infty\}\) is also a proper, closed, and convex function. In this paper, we will focus on two classes of problems in (1), overlapped to each other.

* \(\mathcal{H}_{i}\) is nonconvex in \(w\) and linear in \(u\) as \(\mathcal{H}_{i}(w,u):=\langle F_{i}(w),Ku\rangle\) for a given function \(F_{i}:\mathbb{R}^{p}\to\mathbb{R}^{m}\) and a matrix \(K\in\mathbb{R}^{q\times m}\) for all \(i\in[n]\) and \((w,u)\in\mathrm{dom}\left(\mathcal{L}\right)\).
* \(\mathcal{H}_{i}\) is nonconvex in \(w\) and \(\mathcal{H}_{i}(w,\cdot)-h(\cdot)\) is strongly concave in \(u\) for all \((w,u)\in\mathrm{dom}\left(\mathcal{L}\right)\).

Although (NC) looks more general than (NL), both cases can be overlapped, but one is not a special case of the other. Under these two settings, our approach will rely on a _bilevel optimization_ approach, where the lower-level problem is to solve \(\max_{u}\mathcal{L}(w,u)\), while the upper-level one is \(\min_{w}\mathcal{L}(w,u)\).

**Challenges.** The setting (NL) is a special case of stochastic nonconvex-concave minimax problems because the objective term \(\mathcal{H}(w,u):=\langle F(w),Ku\rangle\) is linear in \(u\). It is equivalent to the compositional model (CO) described below. However, if \(h\) is only merely convex and not strongly convex (e.g., the indicator of a standard simplex), then \(\Phi_{0}\) in (CO) becomes nonsmooth regardless of \(F\)'s properties. This presents our first challenge. A natural approach to address this issue, as discussed in Section 2, is to smooth \(\Phi_{0}\). The second challenge arises from the composition between the outer function \(h^{*}\) and the finite sum \(F(\cdot)\) in (CO). Unlike standard finite-sum optimization, this composition prevents any direct use of existing techniques, requiring a novel approach for algorithmic development and analysis. The third challenge involves unbiased estimators for gradients or "hyper-gradients" in minimax problems. Most existing methods rely on unbiased estimators for objective gradients, with limited work exploring biased estimators. While biased estimators can be used, they require variance reduction properties (see, e.g., [10]). The setting (NC) faces the same second and third challenges as the setting (NL). Additionally, when reformulating it as a minimization problem using a bilevel optimization approach (3), constructing a shuffling estimator for the "hyper-gradient" \(\nabla\Phi_{0}\) becomes unclear. This requires solving the lower-level maximization problem (2). Therefore, it remains an open question whether shuffling gradient-type methods can be extended to this bilevel optimization approach to address (1). In this paper, we address the following research question:

_Can we efficiently develop shuffling gradient methods to solve (1) for both \((\mathrm{NL})\) and \((\mathrm{NC})\) settings?_

Our attempt to tackle this question leads to a novel way of constructing shuffling estimators for the hyper-gradient \(\nabla\Phi_{0}\) or its smoothed counterpart. This allows us to develop two shuffling gradient-based algorithms with rigorous theoretical guarantees on oracle complexity, matching state-of-the-art complexity results in shuffling-type algorithms for nonconvex optimization.

**Related work.** Shuffling optimization algorithms have gained significant attention in optimization and machine communities, demonstrating advantages over standard SGDs, see, e.g., [4, 5, 8, 16, 26, 32, 38]. Nevertheless, applying these techniques to minimax problems like (1) remains challenging, with limited existing literature (e.g., [3, 8, 11]). Das _et al._ in [8] explored a specific case of (1) without nonsmooth terms \(f\) and \(h\), assuming strong monotonicity and \(L\)-Lipschitz continuity of the gradient \(\nabla\mathcal{H}:=[\nabla_{w}\mathcal{H},-\nabla_{u}\mathcal{H}]\) of the joint objective \(\mathcal{H}\). Their algorithm simplifies to a shuffling variant of fixed-point iteration or a gradient descent-ascent scheme, not applicable to our settings. Cho and Yun in [3] built upon [8] by relaxing the strong monotonicity to Polyak-Lojasiewicz (PL) conditions. This work is perhaps the most closely related one to our algorithm, Algorithm 2, for the (NC) setting. Note that the method in [3] exploits Nash's equilibrium perspective with a simultaneous update, which is different from our alternative update. Moreover, [3] only considers the noncomposite case with \(f=0\) and \(h=0\). Though we only focus on a nonconvex-strongly-concave setting (NC), our results here can be extended to the PL condition as in [3]. Very recently, Konstantinos _et al._ in [11] introduced shuffling extragradient methods for variational inequalities, which encompass convex-concave minimax problems as a special case. However, this also falls outside the scope of our work due to the nonconvexity of (1) in \(w\). Again, all the existing works in [3, 8, 11] utilize a Nash's equilibrium perspective, while ours leverages a bilevel optimization technique. Besides, in contrast to our sampling-without-replacement approach, stochastic and randomized methods (i.e. using i.i.d. sampling strategies) have been extensively studied for minimax problems, see, e.g., [9, 14, 15, 18, 22, 23, 31, 37, 42]. A comprehensive comparison can be found, e.g., in [3].

**Contribution.** Our main contribution can be summarized as follows.

* For setting (NL), we suggest to reformulate (1) into a compositional minimization and exploit a smoothing technique to treat this reformulation. We propose a new way of constructing shuffling estimators for the "hyper-gradient" \(\nabla\Phi_{\gamma}\) (cf. (10)) and establish their properties.

2. We propose a novel shuffling gradient-based algorithm (_cf._ Algorithm 1) to approximate an \(\epsilon\)-KKT point of (1) for the setting (NL). Our method requires \(\mathcal{O}(n\epsilon^{-3})\) evaluations of \(F_{i}\) and \(\nabla F_{i}\) under the strong convexity of \(h\), and \(\mathcal{O}(n\epsilon^{-7/2})\) evaluations of \(F_{i}\) and \(\nabla F_{i}\) without the strong convexity of \(h\), for a desired accuracy \(\epsilon>0\).
3. For setting (NC), we develop two variants of the shuffling gradient method: _semi-shuffling_ and _full-shuffling_ schemes (_cf._ Algorithm 2). The semi-shuffling variant combines both gradient ascent and shuffling gradient methods to construct a new algorithm, which requires \(\mathcal{O}(n\epsilon^{-3})\) evaluations of both \(\nabla_{w}\mathcal{H}_{i}\) and \(\nabla_{u}\mathcal{H}_{i}\). The full-shuffling scheme allows to perform both shuffling schemes on the maximization and the minimization alternatively, requiring either \(\mathcal{O}(n\epsilon^{-3})\) or \(\mathcal{O}(n\epsilon^{-4})\) evaluations of \(\nabla_{u}\mathcal{H}_{i}\) depending on our assumptions, while maintaining \(\mathcal{O}(n\epsilon^{-3})\) evaluations of \(\nabla_{w}\mathcal{H}_{i}\) for a given desired accuracy \(\epsilon>0\).

If a random shuffling strategy is used in our algorithms, then the oracle complexity in all the cases presented above is improved by a factor of \(\sqrt{n}\). Our settings (NL) and (NC) of (1) are different from existing works [3; 8; 11], as we work with general nonconvexity in \(w\), and linearity or [strong] concavity in \(u\), and both \(f\) and \(h\) are possibly nonsmooth. Our algorithms are not reduced or similar to existing shuffling methods for optimization, but we use shuffling strategies to form estimators for the hyper-gradient \(\nabla\Phi_{0}\) in (5). The oracle complexity in both settings (NL) and (NC) is similar to the ones in nonconvex optimization and in a special case of (1) from [3] (up to a constant factor).

**Paper outline.** The rest of this paper is organized as follows. Section 2 presents our bilevel optimization approach to (1) and recalls necessary preliminary results. Section 3 develops our shuffling algorithm to solve the setting (NL) of (1) and establishes its convergence. Section 4 proposes new shuffling methods to solve the setting (NC) and investigates their convergence. Section 5 presents numerical experiments, while technical proofs and supporting results are deferred to Supp. Docs.

**Notations.** For a function \(f\), we use \(\operatorname{dom}\left(f\right)\) to denote its effective domain, and \(\nabla f\) for its gradient or Jacobian. If \(f\) is convex, then \(\nabla f\) denotes a subgradient, \(\partial f\) is its subdifferential, and \(\operatorname{prox}_{f}\) is its proximal operator. We use \(\mathcal{F}_{t}\) to denote \(\sigma(w_{0},w_{1},\cdots,w_{t})\), a \(\sigma\)-algebra generated by random vectors \(w_{0},w_{1},\cdots,w_{t}\), \(\mathbb{E}_{t}[\cdot]=\mathbb{E}[\cdot|\mathcal{F}_{t}]\) is a conditional expectation, and \(\mathbb{E}[\cdot]\) is the full expectation. As usual, \(\mathcal{O}(\cdot)\) denotes Big-O notation in the theory of algorithm complexity.

## 2 Bilevel Optimization Approach and Preliminary Results

Our approach relies on a bilevel optimization technique [9] in contrast to Nash's game viewpoint [24], which treats the maximization as a lower level and the minimization as an upper level problem.

### Bilevel optimization approach

The minimax model (1) is split into a _lower-level_ (_i.e. a follower_) _maximization problem_ of the form:

\[\begin{split}\Phi_{0}(w)&:=\max_{u\in\mathbb{R}^{q} }\bigl{\{}\mathcal{H}(w,u)-h(u)\equiv\frac{1}{n}\sum_{i=1}^{n}\mathcal{H}_{i} (w,u)-h(u)\bigr{\}},\\ u_{0}^{*}(w)&:=\operatorname*{\arg\!\max}_{u\in \mathbb{R}^{q}}\bigl{\{}\mathcal{H}(w,u)-h(u)\equiv\frac{1}{n}\sum_{i=1}^{n} \mathcal{H}_{i}(w,u)-h(u)\bigr{\}}.\end{split}\] (2)

For \(\Phi_{0}\) defined by (2), then the _upper-level_ (_i.e. the leader_) _minimization problem_ can be written as

\[\Psi_{0}^{*}:=\min_{w\in\mathbb{R}^{p}}\Big{\{}\Psi_{0}(w):=\Phi_{0}(w)+f(w) \Big{\}}.\] (3)

Clearly, this approach is sequential, and only works if \(\Phi_{0}\) is well-defined, i.e. (2) is globally solvable. Hence, the concavity of \(\mathcal{H}(w,\cdot)-h(\cdot)\) w.r.t. to \(u\) is crucial for this approach as stated below. However, this assumption can be relaxed to a global solvability of (2) combined with a PL condition as in [3].

**Assumption 1** (Basic).: _Problems_ (1) and (3) _satisfy the following assumptions for all \(i\in[n]\):_

1. \(\Psi_{0}^{*}:=\inf_{w}\Psi_{0}(w)>-\infty\)_._
2. \(\mathcal{H}_{i}\) _is differentiable w.r.t._ \((w,u)\in\operatorname{dom}\left(\mathcal{L}\right)\) _and_ \(\mathcal{H}_{i}(w,\cdot)\) _is concave in_ \(u\) _for any_ \(w\)_._
3. _Both_ \(f:\mathbb{R}^{p}\to\mathbb{R}\cup\{+\infty\}\) _and_ \(h:\mathbb{R}^{q}\to\mathbb{R}\cup\{+\infty\}\) _are proper, closed, and convex._

This assumption remains preliminary. To develop our algorithms, we will need more conditions on \(\mathcal{H}_{i}\) and possibly on \(f\) and \(h\), which will be stated later. In addition, we can work with a sublevel set

\[\mathcal{L}_{\Psi_{0}}(w_{0}):=\{w\in\operatorname{dom}\left(\Psi_{0}\right): \Psi_{0}(w)\leq\Psi_{0}(w_{0})\}\] (4)of \(\Psi_{0}\) for a given initial point \(w_{0}\) from our methods. If \(u_{0}^{*}(w)\) is uniquely well-defined for given \(w\in\mathcal{L}_{\Psi_{0}}(w_{0})\), then by the well-known Danskin's theorem, \(\Phi_{0}\) is differential at \(w\) and its gradient is

\[\nabla\Phi_{0}(w)=\nabla_{w}\mathcal{H}(w,u_{0}^{*}(w))=\tfrac{1}{n}\sum_{i=1} ^{n}\nabla_{w}\mathcal{H}_{i}(w,u_{0}^{*}(w)).\] (5)

We adopt the term "hyper-gradient" from bilevel optimization to name \(\nabla\Phi_{0}\) in this paper.

### Technical assumptions and properties of \(\Phi_{0}\) for nonconvex-linear setting (NL)

\((\mathrm{a})\)_Compositional minimization formulation_. If \(\mathcal{H}_{i}(w,u):=\langle F_{i}(w),Ku\rangle\) as in setting (NL), then (1) is equivalently reformulated into the following _nonconvex compositional minimization_ problem:

\[\min_{w\in\mathbb{R}^{p}}\Big{\{}\Psi_{0}(w):=f(w)+\Phi_{0}(w)=f(w)+h^{*} \Big{(}\tfrac{1}{n}\sum_{i=1}^{n}K^{T}F_{i}(w)\Big{)}\Big{\}},\] (CO)

where \(h^{*}(v):=\sup_{u}\{\langle v,u\rangle-h(u)\}\), the Fenchel conjugate of \(h\), and \(\Phi_{0}(w)=h^{*}(K^{T}F(w))\). If \(h\) is not strongly convex, then \(h^{*}\) is convex but possibly nonsmooth.

\((\mathrm{b})\)_Technical assumptions_. To develop our algorithms, we also need the following assumptions.

**Assumption 2**.: \(h\) _is \(\mu_{h}\)-strongly convex with \(\mu_{h}\geq 0\), and \(\mathrm{dom}(h)\) is bounded by \(M_{h}<+\infty\)._

**Assumption 3** (For \(F_{i}\)).: _For setting \((\mathrm{NL})\) with \(\mathcal{H}_{i}(w,u):=\langle F_{i}(w),Ku\rangle\)\((i\in[n])\), assume that_

* \(F_{i}\) _is continuously differentiable, and its Jacobian_ \(\nabla F_{i}\) _is_ \(L_{F_{i}}\)_-Lipschitz continuous._
* \(F_{i}\) _is also_ \(M_{F_{i}}\)_-Lipschitz continuous or equivalently, its Jacobian_ \(\nabla F_{i}\) _is_ \(M_{F_{i}}\)_-bounded._
* _There exists a positive constant_ \(\sigma_{J}\in(0,+\infty)\) _such that_ \[\tfrac{1}{n}\sum_{i=1}^{n}\|\nabla F_{i}(w)-\nabla F(w)\|^{2}\leq\sigma_{J}^{2 },\quad\forall w\in\mathrm{dom}\left(F\right).\] (6)

Assumption 2 allows \(\mu_{h}=0\) that also covers the non-strong convexity of \(h\). Assumption 3 is rather standard to develop gradient-based methods for solving (1). Under Assumption 3, the finite-sum \(F\) is also \(M_{F}\)-Lipschitz continuous and the Jacobian \(\nabla F\) of \(F\) is also \(L_{F}\)-Lipschitz continuous with

\[M_{F}:=\max\{M_{F_{i}}:i\in[n]\}\quad\text{and}\quad L_{F}:=\max\{L_{F_{i}}:i \in[n]\}.\] (7)

Condition (6) can be relaxed to the form \(\tfrac{1}{n}\sum_{i=1}^{n}\|\nabla F_{i}(w)-\nabla F(w)\|^{2}\leq\sigma_{J}^{ 2}+\Theta_{J}\|\nabla\Phi_{0}(w)\|^{2}\) for some \(\Theta_{J}\geq 0\), where \(\nabla\Phi_{0}\) is a [sub]gradient of \(\Phi_{0}\) or \(\Phi_{\gamma}\) (its smoothed approximation). Moreover, under Assumption 3, if \(\mu_{h}>0\), then \(\nabla h^{*}\) is \(L_{h^{*}}\)-Lipschitz continuous with \(L_{h^{*}}:=\tfrac{1}{\mu_{h}}\). Thus it is possible (see [9]) to prove that \(\Phi_{0}\) is differentiable, and \(\nabla\Phi_{0}\) is also \(L_{\Phi_{0}}\)-Lipschitz continuous with \(L_{\Phi_{0}}:=M_{h}\|K\|L_{F}+\tfrac{M_{F}^{2}\|K\|^{2}}{\mu_{h}}\) as a consequence of Lemma 4 when \(\gamma\downarrow 0^{+}\) in Supp. Doc. A.

\((\mathrm{c})\)_Smoothing technique for lower-level maximization problem_ (2). If \(h\) is only merely convex (i.e. \(\mu_{h}=0\)), then (2) may not be uniquely solvable, leading to the possible non-differentiability of \(\Phi_{0}\). Let us define the following convex function:

\[\phi_{0}(v):=\max_{u\in\mathbb{R}^{q}}\left\{\langle v,Ku\rangle-h(u)\right\}=h ^{*}(K^{T}v).\] (8)

Then, \(\Phi_{0}\) in (2) or (CO) can be written as \(\Phi_{0}(w)=\phi_{0}(F(w))=\phi_{0}\left(\tfrac{1}{n}\sum_{i=1}^{n}F_{i}(w)\right)\). Our goal is to smooth \(\phi_{0}\) if \(h\) is not strongly convex, leading to

\[\left\{\begin{aligned} \phi_{\gamma}(v)&:=\max_{u} \left\{\langle v,Ku\rangle-h(u)-\gamma b(u)\right\},\\ u_{\gamma}^{*}(v)&:=\operatorname*{argmax}_{u}\left\{ \langle v,Ku\rangle-h(u)-\gamma b(u)\right\},\end{aligned}\right.\] (9)

where \(\gamma>0\) is a given smoothness parameter and \(b:\mathbb{R}^{q}\to\mathbb{R}\) is a proper, closed, and \(1\)-strongly convex function such that \(\mathrm{dom}(h)\subseteq\mathrm{dom}(b)\). We also denote \(D_{b}:=\sup\{\|\nabla b(u)\|:u\in\mathrm{dom}\left(h\right)\}\). In particular, if we choose \(b(u):=\tfrac{1}{2}\|u-\bar{u}\|^{2}\) for a fixed \(\bar{u}\), then \(u_{\gamma}^{*}(v)=\mathrm{prox}_{h/\gamma}(\bar{u}-K^{T}v)\).

Using \(\phi_{\gamma}\), problem (CO) can be approximated by its smoothed formulation:

\[\min_{w\in\mathbb{R}^{p}}\Bigl{\{}\Psi_{\gamma}(w):=f(w)+\Phi_{\gamma}(w)=f(w)+ \phi_{\gamma}(F(w))\equiv f(w)+\phi_{\gamma}\Big{(}\tfrac{1}{n}\sum_{i=1}^{n}F_{ i}(w)\Big{)}\Bigr{\}}.\] (10)

To develop our method, one key step is to approximate the hyper-gradient of \(\Phi_{\gamma}\) in (10), where

\[\nabla\Phi_{\gamma}(w) = \nabla F(w)^{T}\nabla\phi_{\gamma}(F(w))=\tfrac{1}{n}\sum_{i=1}^{ n}\nabla F_{i}(w)^{T}\nabla\phi_{\gamma}(F(w)).\] (11)

Then, \(\nabla\Phi_{\gamma}\) is \(L_{\Phi_{\gamma}}\)-Lipschitz continuous with \(L_{\Phi_{\gamma}}:=M_{h}\|K\|L_{F}+\tfrac{M_{F}^{2}\|K\|^{2}}{\mu_{h}+\gamma}\) (see Lemma 4).

### Technical assumptions and properties of \(\Phi_{0}\) for the nonconvex-strongly-concave setting

To develop our shuffling gradient-based algorithms for solving (1) under the nonconvex-strongly-concave setting (NC), we impose the following assumptions.

**Assumption 4** (For \(\mathcal{H}_{i}\)).: \(\mathcal{H}_{i}\) _for all \(i\in[n]\) in (1) satisfies the following conditions:_

* _For any given_ \(w\) _such that_ \((w,u)\in\operatorname{dom}\left(\mathcal{H}\right)\)_,_ \(\mathcal{H}_{i}(w,\cdot)\) _is_ \(\mu_{H}\)_-strongly concave w.r.t._ \(u\)_._
* \(\nabla\mathcal{H}_{i}\) _is_ \((L_{w},L_{u})\)_-Lipschitz continuous, i.e. for all_ \((w,u),(\hat{w},\hat{u})\in\operatorname{dom}\left(\mathcal{H}\right)\)_:_ \[\|\nabla\mathcal{H}_{i}(w,u)-\nabla\mathcal{H}_{i}(\hat{w},\hat{u})\|^{2}\leq L _{w}^{2}\|w-\hat{w}\|^{2}+L_{u}^{2}\|u-\hat{u}\|^{2}.\] (12)
* _There exist two constants_ \(\Theta_{w}\geq 0\) _and_ \(\sigma_{w}\geq 0\) _such that for_ \((w,u)\in\operatorname{dom}\left(\mathcal{H}\right)\)_, we have_ \[\tfrac{1}{n}\sum_{i=1}^{n}\|\nabla_{w}\mathcal{H}_{i}(w,u)-\nabla_{w}\mathcal{ H}(w,u)\|^{2}\,\leq\,\Theta_{w}\|\nabla_{w}\mathcal{H}(w,u)\|^{2}+\sigma_{w}^{2}.\] (13) _There exist two constants_ \(\Theta_{u}\geq 0\) _and_ \(\sigma_{u}\geq 0\) _such that for all_ \((w,u)\in\operatorname{dom}\left(\mathcal{H}\right)\)_, we have_ \[\tfrac{1}{n}\sum_{i=1}^{n}\|\nabla_{u}\mathcal{H}_{i}(w,u)-\nabla_{u}\mathcal{ H}(w,u)\|^{2}\,\leq\,\Theta_{u}\|\nabla_{u}\mathcal{H}(w,u)\|^{2}+\sigma_{u}^{2}.\] (14)

Assumption 4(a) makes sure that our lower-level maximization of (1) is well-defined. Assumption 4(b) and (c) are standard in shuffling gradient-type methods as often seen in nonconvex optimization [9].

**Lemma 1** (Smoothness of \(\Phi_{0}\)).: _Under Assumptions 2 and 4, \(u_{0}^{*}(\cdot)\) in (2) is \(\kappa\)-Lipschitz continuous with \(\kappa:=\frac{L_{w}}{\mu_{H}+\mu_{h}}\). Moreover, \(\nabla\Phi_{0}\) in (5) is \(L_{\Phi_{0}}\)-Lipschitz continuous with \(L_{\Phi_{0}}:=(1+\kappa)L_{w}\)._

### Approximate KKT points and approximate stationary points

\((\mathrm{a})\) _Exact and approximate KKT points and stationary points._ A pair \((w^{\star},u^{\star})\in\operatorname{dom}\left(\mathcal{L}\right)\) is called a KKT (Karush-Kuhn-Tucker) point of (1) if

\[0\in\nabla_{w}\mathcal{H}(w^{\star},u^{\star})+\partial f(w^{\star})\quad \text{and}\quad 0\in-\nabla_{u}\mathcal{H}(w^{\star},u^{\star})+\partial h(u^{ \star}).\] (15)

Given a tolerance \(\epsilon>0\), **our goal** is to find an \(\epsilon\)-approximate KKT point \((\widehat{w},\widehat{u})\) of (1) defined as

\[r_{w}\in\nabla_{w}\mathcal{H}(\widehat{w},\widehat{u})+\partial f(\widehat{w }),\quad r_{u}\in-\nabla_{u}\mathcal{H}(\widehat{w},\widehat{u})+\partial h( \widehat{u}),\quad\text{and}\quad\mathbb{E}\big{[}\|[r_{w},r_{u}]\|^{2}\big{]} \leq\epsilon^{2}.\] (16)

A vector \(w^{\star}\in\operatorname{dom}\left(\Psi_{0}\right)\) is said to be a stationary point of (3) if

\[0\in\nabla\Phi_{0}(w^{\star})+\partial f(w^{\star}).\] (17)

Since \(f\) is possibly nonsmooth, we can define a stationary point of (3) via a gradient mapping as:

\[\mathcal{G}_{\eta}(w):=\eta^{-1}\big{(}w-\operatorname{prox}_{\eta f}(w-\eta \nabla\Phi_{0}(w))\big{)},\] (18)

where \(\eta>0\) is given. It is well-known that \(\mathcal{G}_{\eta}(w^{\star})=0\) iff \(w^{\star}\) is a stationary point of (3). Again, since we cannot exactly compute \(w^{\star}\), we expect to find an \(\epsilon\)-stationary point \(\widehat{w}_{T}\) of (3) such that \(\mathbb{E}\big{[}\|\mathcal{G}_{\eta}(\widehat{w}_{T})\|^{2}\big{]}\leq \epsilon^{2}\) for a given tolerance \(\epsilon>0\).

\((\mathrm{b})\) _Constructing an approximate stationary point and KKT point from algorithms._ Our algorithms below generate a sequence \(\{\widehat{w}_{t}\}_{t\geq 0}^{T}\) such that \(\frac{1}{T+1}\sum_{t=0}^{T}\mathbb{E}\big{[}\|\mathcal{G}_{\eta}(\widetilde{w} _{t})\|^{2}\big{]}\leq\epsilon^{2}\). Hence, we construct an \(\epsilon\)-stationary point \(\widehat{w}_{T}\) using one of the following two options:

\[\widehat{w}_{T}:=\widetilde{w}_{t_{\star}},\ \text{ where }\left\{\begin{array}{ll}t_{\star}:= \operatorname{argmin}\{\|\mathcal{G}_{\eta}(\widetilde{w}_{t})\|:0\leq t\leq T \},&\text{(Option 1) \ or}\\ t_{\star}\text{ is uniformly randomly chosen from }\{0,1,\cdots,T\}\end{array}\right.\] (19)

Clearly, we have \(\mathbb{E}\big{[}\|\mathcal{G}_{\eta}(\widehat{w}_{T})\|^{2}\big{]}\leq\frac{1 }{T+1}\sum_{t=0}^{T}\mathbb{E}\big{[}\|\mathcal{G}_{\eta}(\widetilde{w}_{t}) \|^{2}\big{]}\leq\epsilon^{2}\). We need the following result.

**Lemma 2**.: \((\mathrm{a})\) _If \((w^{\star},u^{\star})\) is a KKT point of (1), then \(w^{\star}\) is a stationary point of (3). Conversely, if \(w^{\star}\) is a stationary point of (3), then \((w^{\star},u_{0}^{\star}(w^{\star}))\) is a KKT point of (1)._

\((\mathrm{b})\) _If \(\widehat{w}_{T}\) is an \(\epsilon\)-stationary point of (3) and \(\nabla\Phi_{0}\) is \(L_{\Phi_{0}}\)-Lipschitz continuous, then \((\overline{w}_{T},\overline{u}_{T})\) is an \(\hat{\epsilon}\)-KKT point of (1), where \(\overline{w}_{T}:=\operatorname{prox}_{\eta f}(\widehat{w}_{T}-\eta\nabla\Phi_{0} (\widehat{w}_{T}))\), \(\overline{u}_{T}:=u_{0}^{\star}(\overline{w}_{T})\), and \(\hat{\epsilon}:=(1+L_{\Phi_{0}}\eta)\epsilon\)._

\((\mathrm{c})\) _If \(\widehat{w}_{T}\) is an \(\epsilon\)-stationary point of (10), then \((\overline{w}_{T},\overline{u}_{T})\) is an \(\hat{\epsilon}\)-KKT point of (1), where \(\overline{w}_{T}:=\operatorname{prox}_{\eta f}(\widehat{w}_{T}-\eta\nabla\Phi_{ \gamma}(\widehat{w}_{T}))\), \(\overline{u}_{T}:=u_{\gamma}^{\star}(F(\overline{w}_{T}))\), and \(\hat{\epsilon}:=\max\{(1+L_{\Phi_{\gamma}}\eta)\epsilon,\gamma D_{b}\}\)._

Lemma 2 allows us to construct an \(\hat{\epsilon}\)-approximate KKT point \((\overline{w}_{T},\overline{u}_{T})\) of (1) from an \(\epsilon\)-stationary point \(\widehat{w}_{T}\) of either (3) or its smoothed problem (10), where \(\hat{\epsilon}=\mathcal{O}(\max\{\epsilon,\gamma\})\).

### Technical condition to handle the possible nonsmooth term \(f\)

To handle the nonsmooth term \(f\) of (1) in our algorithms we require one more condition as in [5].

**Assumption 5**.: _Let \(\Phi_{\gamma}\) be defined by (10), which reduces to \(\Phi_{0}\) given by (2) as \(\gamma\downarrow 0^{+}\), and \(\mathcal{G}_{\eta}\) be defined by (18). Assume that there exist two constants \(\Lambda_{0}\geq 1\) and \(\Lambda_{1}\geq 0\) such that:_

\[\|\nabla\Phi_{\gamma}(w)\|^{2}\leq\Lambda_{0}\|\mathcal{G}_{\eta}(w)\|^{2}+ \Lambda_{1},\quad\forall w\in\operatorname{dom}\left(\Phi_{0}\right).\] (20)

If \(f=0\), then \(\mathcal{G}_{\eta}(w)\equiv\nabla\Phi_{\gamma}(w)\), and Assumption 5 automatically holds with \(\Lambda_{0}=1\) and \(\Lambda_{1}=0\). If \(f\neq 0\), then it is crucial to have \(\Lambda_{0}\geq 1\) in (20). Let us consider two examples to see why?

1. If \(f\) is \(M_{f}\)-Lipschitz continuous (e.g., \(\ell_{1}\)-norm), then (20) also holds with \(\Lambda_{0}:=1+\nu>1\) and \(\Lambda_{1}:=\frac{1+\nu}{\nu}M_{f}\) for a given \(\nu>0\).
2. If \(f=\delta_{\mathcal{W}}\), the indicator of a nonempty, closed, convex, and bounded set \(\mathcal{W}\), then Assumption 5 also holds by the same reason as in Example (i) (see Supp. Doc. A).

## 3 Shuffling Gradient Method for Nonconvex-Linear Minimax Problems

We first propose a new construction using shuffling techniques to approximate the true gradient \(\nabla\Phi_{\gamma}\) in (11) for any \(\gamma\geq 0\). Next, we propose our algorithm and analyze its convergence.

### The shuffling gradient estimators for \(\nabla\Phi_{\gamma}\)

**Challenges.** To evaluate \(\nabla\Phi_{\gamma}(w)\) in (11), we need to evaluate both \(\nabla F(w)\) and \(F(w)\) at each \(w\). However, in SGD or shuffling gradient methods, we want to approximate both quantities at each iteration. Note that this gradient can be written in a finite-sum \(\frac{1}{n}\sum_{i=1}^{n}\nabla F_{i}(w)^{T}\nabla\phi_{\gamma}(F(w))\) (see (11)), but every summand requires \(\nabla\phi_{\gamma}(F(w))\), which involves the full evaluation of \(F\).

**Our estimators.** Let \(F_{\pi^{(t)}(i)}(w_{i-1}^{(t)})\) and \(\nabla F_{\hat{\pi}^{(t)}(i)}(w_{i-1}^{(t)})\) be the function value and the Jacobian component evaluated at \(w_{i-1}^{(t)}\) respectively for \(i\in[n]\), where \(\pi^{(t)}=(\pi^{(t)}(1),\pi^{(t)}(2),\cdots,\pi^{(t)}(n))\) and \(\hat{\pi}^{(t)}=(\hat{\pi}^{(t)}(1),\hat{\pi}^{(t)}(2),\cdots,\hat{\pi}^{(t) }(n))\) are two permutations of \([n]:=\{1,2,\cdots,n\}\). We want to use these quantities to approximate the function value \(F(w_{0}^{(t)})\) and its Jacobian \(\nabla F(w_{0}^{(t)})\) of \(F\) at \(w_{0}^{(t)}\), respectively, where \(w_{0}^{(t)}\) the iterate vector at the beginning of each epoch \(t\).

For function value \(F(w_{0}^{(t)})\), we suggest the following approximation at each _inner iteration_\(i\in[n]\):

\[\textbf{Option 1:}\qquad\quad F_{i}^{(t)}\,:=\,\frac{1}{n}\left[\sum_{j=1}^{i}F_{ \pi^{(t)}(j)}(w_{j-1}^{(t)})+\sum_{j=i+1}^{n}F_{\pi^{(t)}(j)}(w_{0}^{(t)}) \right].\] (21)

Alternative to (21), for all \(i\in[n]\), we can simply choose another option:

\[\textbf{Option 2:}\qquad\quad F_{i}^{(t)}\,:=\,\frac{1}{n}\sum_{j=1}^{n}F_{j}(w_{ 0}^{(t)})=\frac{1}{n}\sum_{j=1}^{n}F_{\pi^{(t)}(j)}(w_{0}^{(t)}).\] (22)

For Jacobian \(\nabla F(w_{0}^{(t)})\), we suggest to use the following standard shuffling estimator for all \(i\in[n]\):

\[\nabla F_{i}^{(t)}:=\nabla F_{\hat{\pi}^{(t)}(i)}(w_{i-1}^{(t)}).\] (23)

For \(F_{i}^{(t)}\) from (21) (or (22)) and for \(\nabla F_{i}^{(t)}\) from (23), we form an approximation of \(\nabla\Phi_{\gamma}(w_{0}^{(t)})\) as

\[\widetilde{\nabla}\Phi_{\gamma}(w_{i-1}^{(t)}):=(\nabla F_{i}^{(t)})^{T}\nabla \phi_{\gamma}(F_{i}^{(t)})\equiv(\nabla F_{i}^{(t)})^{T}Ku_{\gamma}^{*}(F_{i}^ {(t)}).\] (24)

**Discussion.** The estimator \(F_{i}^{(t)}\) for \(F\) requires \(n-i\) more function evaluations \(F_{\pi^{(t)}(j)}(w_{0}^{(t)})\) at each epoch \(t\). The first option (21) for \(F\) uses \(2n\) function evaluations \(F_{i}\), while the second one in (22) only needs \(n\) function evaluations at each epoch \(t\geq 0\). However, (21) uses the most updated information up to the _inner iteration_\(i\) compared to (22), which is expected to perform better. The Jacobian estimator \(\nabla F_{i}^{(t)}\) is standard and only uses one sample or a mini-batch at each iteration \(i\).

### The shuffling gradient-type algorithm for nonconvex-linear setting (NL)

We propose Algorithm 1, a shuffling gradient-type method, to approximate a stationary point of (10).

**Discussion.** First, the cost per epoch of Algorithm 1 consists of either \(2n\) or \(n\) function evaluations \(F_{i}\), and \(n\) Jacobian evaluations \(\nabla F_{i}\). Compare to standard shuffling gradient-type methods, e.g., in [8], Algorithm 1 has either \(n\) more evaluations of \(F_{i}\) or the same cost. Second, when implementingAlgorithm 1, we do not need to evaluate the full Jacobian \(\nabla F_{i}^{(t)}\), but rather the product of matrix \((\nabla F_{i}^{(t)})^{T}\) and vector \(\nabla\Phi_{\gamma}(F_{i}^{(t)})\) as \(\widetilde{\nabla}\Phi_{\gamma}(w_{i-1}^{(t)}):=(\nabla F_{i}^{(t)})^{T}\nabla \Phi_{\gamma}(F_{i}^{(t)})\). Evaluating this matrix-vector multiplication is much more efficient than evaluating the full Jacobian \(\nabla F_{i}^{(t)}\) and \(\nabla\Phi_{\gamma}(F_{i}^{(t)})\) individually. Third, thanks to Assumption 5, the proximal step \(\widetilde{w}_{t}:=\operatorname{prox}_{\eta_{t}f}(w_{n}^{(t)})\) is only required at the end of each epoch \(t\). This significantly reduces the computational cost if \(\operatorname{prox}_{\eta_{t}f}\) is expensive.

### Convergence Analysis of Algorithm 1 for Nonconvex-Linear Setting (NL)

Now, we are ready to state the convergence result of Algorithm 1 in a short version: Theorem 1. The full version of this theorem is Theorem 6, which can be found in Supp. Doc. B.

**Theorem 1**.: _Suppose that Assumptions 1, 2, 3, and 5 holds for the setting \((\mathrm{NL})\) of (1) and \(\epsilon>0\) is a sufficiently small tolerance. Let \(\{\widetilde{w}_{t}\}\) be generated by Algorithm 1 after \(T=\mathcal{O}(\epsilon^{-3})\) epochs using arbitrarily permutations \(\pi^{(t)}\) and \(\tilde{\pi}^{(t)}\) and a learning rate \(\eta_{t}=\eta:\mathcal{O}(\epsilon)\)\((see\) Theorem 6 in Supp. Doc. B for the exact formulas of \(T\) and \(\eta\)). Then, we have \(\frac{1}{T+1}\sum_{t=0}^{T}\|\mathcal{G}_{\eta_{t}}(\widetilde{w}_{t})\|^{2} \leq\epsilon^{2}\)._

_Alternatively, if \(\{\widetilde{w}_{t}\}\) is generated by Algorithm 1 after \(T:=\mathcal{O}(n^{-1/2}\epsilon^{-3})\) epochs using two random and independent permutations \(\pi^{(t)}\) and \(\hat{\pi}^{(t)}\) and a learning rate \(\eta_{t}=\eta:\mathcal{O}(n^{1/2}\epsilon)\)\((see\) Theorem 6 in Supp. Doc. B for the exact formulas). Then, we have \(\frac{1}{T+1}\sum_{t=0}^{T}\mathbb{E}[\|\mathcal{G}_{\eta_{t}}(\widetilde{w}_ {t})\|^{2}]\leq\epsilon^{2}\)._

Our first goal is to approximate a stationary point \(w^{\star}\) of (CO) as \(\mathbb{E}[\|\mathcal{G}_{\eta}(\widetilde{w})\|^{2}]\leq\epsilon^{2}\), while Algorithm 1 only provides an \(\epsilon\)-stationary of (10). For a proper choice of \(\gamma\), it is also an \(\epsilon\)-stationary point of (3).

**Corollary 1**.: _Let \(\widehat{w}_{T}\) defined by (19) be generated from \(\{\widetilde{w}_{t}\}\) of Algorithm 1. Under the conditions of Theorem 1 and any permutations \(\pi^{(t)}\) and \(\hat{\pi}^{(t)}\), the following statements hold._

* _If_ \(h\) _is_ \(\mu_{h}\)_-strongly convex with_ \(\mu_{h}>0\)_, then we can set_ \(\gamma=0\)_, and Algorithm_ 1 _requires_ \(\mathcal{O}(n\epsilon^{-3})\) _evaluations of_ \(F_{i}\) _and_ \(\nabla F_{i}\) _to achieve an_ \(\epsilon\)_-stationary_ \(\widehat{w}_{T}\) _of (_3_)._
* _If_ \(h\) _is only convex_ \((\)_i.e._ \(\mu_{h}=0\)\()\)_, then we can set_ \(\gamma:=\mathcal{O}(\epsilon)\)_, and Algorithm_ 1 _needs_ \(\mathcal{O}(n\epsilon^{-7/2})\) _evaluations of_ \(F_{i}\) _and_ \(\nabla F_{i}\) _to achieve an_ \(\epsilon\)_-stationary_ \(\widehat{w}_{T}\) _of (_3_)._

_If, in addition, \(\pi^{(t)}\) and \(\hat{\pi}^{(t)}\) are sampled uniformly at random without replacement and independently, and \(\Lambda_{1}=\mathcal{O}(n^{-1})\), then the numbers of evaluations of \(F_{i}\) and \(\nabla F_{i}\) are reduced by a factor of \(\sqrt{n}\)._

## 4 Shuffling Method for Nonconvex-Strongly Concave Minimax Problems

In this section, we develop shuffling gradient-based methods to solve (1) under the nonconvex-strongly concave setting (NC). Since this setting does not cover the nonconvex-linear setting (NL) in Section 3 as a special case, we need to treat it separately using different ideas and proof techniques.

### The construction of algorithm

Unlike the linear case with \(\mathcal{H}_{i}(w,u)=\langle F_{i}(w),Ku\rangle\) in Section 3, we cannot generally compute the solution \(u_{0}^{\star}(\widetilde{w}_{t-1})\) in (2) exactly for a given \(\widetilde{w}_{t-1}\). We can only approximate \(u_{0}^{\star}(\widetilde{w}_{t-1})\) by some \(\widetilde{u}_{t}\). This leads to another level of inexactness in an approximate "hyper-gradient" \(\widetilde{\nabla}\Phi_{0}(w_{i-1}^{(t)})\) defined by

\[\widetilde{\nabla}\Phi_{0}(w_{i-1}^{(t)}):=\nabla_{w}\mathcal{H}_{\hat{\pi}^{( t)}(i)}(w_{i-1}^{(t)},\widetilde{u}_{t}).\] (25)There are different options to approximate \(u_{0}^{*}(\widetilde{w}_{t-1})\). We propose two options below, but other choices are possible, including accelerated gradient ascent methods and stochastic algorithms [6, 20].

\((\mathrm{a}_{1})\) **Gradient ascent scheme for the lower-level problem.** We apply a standard gradient ascent scheme to update \(\widetilde{u}_{t}\): _Starting from \(s=0\) with \(u_{0}^{(t)}:=\widetilde{u}_{t-1}\), at each epoch \(s=1,\cdots,S\), we update_

\[\widehat{u}_{s}^{(t)}\,:=\,\mathrm{prox}_{\widehat{u}_{t}h}\big{(}\widehat{u }_{s-1}^{(t)}+\tfrac{\widehat{u}_{t}}{n}\sum_{i=1}^{n}\nabla_{u}\mathcal{H}_{ i}(\widetilde{w}_{t-1},\widehat{u}_{s-1}^{(t)})\big{)},\] (26)

_for a given learning rate \(\widehat{\eta}_{t}>0\). Then, we finally output \(\widetilde{u}_{t}:=\widehat{u}_{S}^{(t)}\) to approximate \(u_{0}^{*}(\widetilde{w}_{t-1})\)._

To make our method more flexible, we allow to perform either only _one iteration_ (i.e. \(S=1\)) or _multiple iterations_ (i.e. \(S>1\)) of (26). Each iteration \(s\) requires \(n\) evaluations of \(\nabla_{u}\mathcal{H}_{i}\).

\((\mathrm{a}_{2})\) **Shuffling gradient ascent scheme for the lower-level problem.** We can also construct \(\widetilde{u}_{t}\) by a _shuffling gradient ascent scheme_. Again, we allow to run either only _one epoch_ (i.e. \(S=1\)) or _multiple epochs_ (i.e. \(S>1\)) of the shuffling algorithm to update \(\widetilde{u}_{t}\), leading to the following scheme: _Starting from \(s:=1\) with \(\widehat{u}_{0}^{(t)}:=\widetilde{u}_{t-1}\), at each epoch \(s=1,2,\cdots,S\), having \(\widehat{u}_{s-1}^{(t)}\), we generate a permutation \(\pi^{(s,t)}\) of \([n]\) and run a shuffling gradient ascent scheme as_

\[\left\{\begin{array}{l}u_{0}^{(s,t)}:=\widehat{u}_{s-1}^{(t)},\\ \mbox{For }i=1,2,\cdots,n\mbox{, update}\\ u_{i}^{(s,t)}:=u_{i-1}^{(s,t)}+\tfrac{\widehat{u}_{t}}{n}\nabla_{u} \mathcal{H}_{\pi^{(s,t)}(i)}(\widetilde{w}_{t-1},u_{i-1}^{(s,t)}),\\ \widehat{u}_{s}^{(t)}:=\mathrm{prox}_{\widehat{\eta}_{t}h}(u_{n}^{(s,t)}). \end{array}\right.\] (27)

_At the end of the \(S\)-th epoch, we output \(\widetilde{u}_{t}:=\widehat{u}_{S}^{(t)}\) as an approximation to \(u_{0}^{*}(\widetilde{w}_{t-1})\)._ Here, we use the same learning rate \(\widehat{\eta}_{t}\) for all epochs \(s\in[S]\). Each epoch \(s\) requires \(n\) evaluations of \(\nabla_{u}\mathcal{H}_{i}\).

\((\mathrm{b})\) **Shuffling gradient descent scheme for the upper-level minimization problem.** Having \(\widetilde{u}_{t}\) from either (26) or (27), we run a _shuffling gradient descent epoch_ to update \(\widetilde{w}_{t}\) from \(\widetilde{w}_{t-1}\) as

\[\left\{\begin{array}{l}w_{0}^{(t)}:=\widetilde{w}_{t-1},\\ \mbox{For }i=1,2,\cdots,n\mbox{, update}\\ w_{i}^{(t)}:=w_{i-1}^{(t)}-\tfrac{\widehat{u}_{t}}{n}\widetilde{\nabla}\Phi_{0} (w_{i-1}^{(t)})\equiv w_{i-1}^{(t)}-\tfrac{\widehat{u}_{t}}{n}\nabla_{w} \mathcal{H}_{\hat{\pi}^{(t)}(i)}(w_{i-1}^{(t)},\widetilde{u}_{t}),\\ \widetilde{w}_{t}:=\mathrm{prox}_{\widehat{\eta}_{t}f}(w_{n}^{(t)}).\end{array}\right.\] (28)

These two steps (26) (or (27)) in \(u\) and (28) in \(w\) are implemented alternatively for \(t=1,\cdots,T\).

\((\mathrm{c})\) **The full algorithm.** Combining both steps (26) (or (27)) and (28), we can present an _alternating shuffling proximal gradient algorithm_ for solving (1) as in Algorithm 2.

```
1:Initialization: Choose an initial point \((\widetilde{w}_{0},\widetilde{u}_{0})\in\mathrm{dom}\left(\mathcal{L}\right)\).
2:for\(t=1,2,\cdots,T\)do
3: Compute \(\widetilde{u}_{t}\) using either (26) or (27).
4: Set \(w_{0}^{(t)}:=\widetilde{w}_{t-1}\) and generate a permutation \(\hat{\pi}^{(t)}\) of \([n]\).
5:for\(i=1,\cdots,n\)do
6: Evaluate \(\widetilde{\nabla}\Phi_{0}(w_{i-1}^{(t)}):=\nabla_{w}\mathcal{H}_{\hat{\pi}^ {(t)}(i)}(w_{i-1}^{(t)},\widetilde{u}_{t})\).
7: Update \(w_{i}^{(t)}:=w_{i-1}^{(t)}-\tfrac{\widehat{u}_{t}}{n}\widetilde{\nabla}\Phi_{0} (w_{i-1}^{(t)})\).
8:endfor
9: Compute \(\widetilde{w}_{t}:=\mathrm{prox}_{\widehat{\eta}_{t}f}(w_{n}^{(t)})\).
10:endfor ```

**Algorithm 2** (Alternating Shuffling Proximal Gradient Algorithm for Solving (1) under setting (NC))

**Discussion.** Algorithm 2 has a similar form as Algorithm 1, where \(u_{0}^{*}(\widetilde{w}_{t-1})\) is approximated by \(\widetilde{u}_{t}\). In Algorithm 1, \(u_{0}^{*}(\widetilde{w}_{t-1})\) is approximated by \(u_{\gamma}^{*}(F_{i}^{(t)})\). Moreover, Algorithm 1 solves the smoothed problem (10) of (3), while Algorithm 2 directly solves (3). Depending on the choice of method to approximate \(u_{0}^{*}(\widetilde{w}_{t-1})\), we obtain different variants of Algorithm 2. We have proposed two variants:* **Semi-shuffling variant:** We use (26) for computing \(\widetilde{u}_{t}\) to approximate \(u_{0}^{*}(\widetilde{w}_{t-1})\).
* **Full-shuffling variant:** We use (27) for computing \(\widetilde{u}_{t}\) to approximate \(u_{0}^{*}(\widetilde{w}_{t-1})\).

Note that Algorithm 2 works in an alternative manner, where it approximates \(u_{0}^{*}(\widetilde{w}_{t-1})\) up to a certain accuracy before updating \(\widetilde{w}_{t}\). This alternating update is very natural and has been widely applied to solve minimax optimization as well as bilevel optimization problems, see, e.g., [1; 9; 13].

### Convergence analysis

Now, we state the convergence of both variants of Algorithm 2: _semi-shuffling_ and _full-shuffling_ variants. The full proof of the following theorems can be found in Supp. Doc. C.

\((\mathrm{a})\) _Convergence of the semi-shuffling variant._ Our first result is as follows.

**Theorem 2**.: _Suppose that Assumptions 1, 2, 4, and 5 hold for (1), and \(\mathcal{G}_{\eta}\) is defined by (18). Let \(\{(\widetilde{w}_{t},\widetilde{u}_{t})\}\) be generated by Algorithm 2 using the **gradient ascent scheme** (26) with \(\eta:=\mathcal{O}(\epsilon)\) explicitly given in Theorem 8 of Supp. Doc. C, \(\hat{\eta}\in(0,\frac{2}{L_{u}+\mu_{h}}]\), \(S:=\mathcal{O}\big{(}\frac{1}{\hat{\eta}}\big{(}\mu_{h}+\frac{4L_{u}\mu_{H}}{ L_{u}+\mu_{H}}\big{)}^{-1}\big{)}=\mathcal{O}(1)\), and \(T:=\mathcal{O}(\epsilon^{-3})\) explicitly given in Theorem 8. Then, we have \(\frac{1}{T+1}\sum_{t=0}^{T}\|\mathcal{G}_{\eta}(\widetilde{w}_{t})\|^{2}\leq \epsilon^{2}\)._

_Consequently, Algorithm 2 requires \(\mathcal{O}(n\epsilon^{-3})\) evaluations of both \(\nabla_{w}\mathcal{H}_{i}\) and \(\nabla_{u}\mathcal{H}_{i}\) to achieve an \(\epsilon\)-stationary point \(\widetilde{w}_{T}\) of (3) computed by (19)._

Note that Theorem 2 holds for both \(S>1\) and \(S=1\) (i.e. we perform only one iteration of (26)).

\((\mathrm{b})\) _Convergence of the full-shuffling variant - The case \(S>1\) with multiple epochs._ We state our results for two separated cases: only \(\mathcal{H}_{i}\) is \(\mu_{H}\)-strongly convex, and only \(h\) is \(\mu_{h}\)-strongly convex.

**Theorem 3** (Strong convexity of \(\mathcal{H}_{i}\)).: _Suppose that Assumptions 1, 2, 4, and 5 hold, and \(\mathcal{H}_{i}\) is \(\mu_{H}\)-strongly concave with \(\mu_{H}>0\) for \(i\in[n]\), but \(h\) is only merely convex._

_Let \(\{(\widetilde{w}_{t},\widetilde{u}_{t})\}\) be generated by Algorithm 2 using \(S\) epochs of the **shuffling routine** (27) and fixed learning rates \(\eta_{t}=\eta:=\mathcal{O}(\epsilon)\) as given in Theorem 8 of Supp. Doc. C for a given \(\epsilon>0\), \(\hat{\eta}_{t}:=\hat{\eta}=\mathcal{O}(\epsilon)\), \(S:=\lfloor\frac{\ln(7/2)}{\mu_{H}\hat{\eta}}\rfloor\), and \(T:=\mathcal{O}(\epsilon^{-3})\). Then, we have \(\frac{1}{T+1}\sum_{t=0}^{T}\|\mathcal{G}_{\eta}(\widetilde{w}_{t})\|^{2}\leq \epsilon^{2}\)._

_Consequently, Algorithm 2 requires \(\mathcal{O}(n\epsilon^{-3})\) evaluations of \(\nabla_{w}\mathcal{H}_{i}\) and \(\mathcal{O}(n\epsilon^{-4})\) evaluations of \(\nabla_{u}\mathcal{H}_{i}\) to achieve an \(\epsilon\)-stationary point \(\widetilde{w}_{T}\) of (3) computed by (19)._

**Theorem 4** (Strong convexity of \(h\)).: _Suppose that Assumptions 1, 2, 4, and 5 hold for (1), and \(h\) is \(\mu_{h}\)-strongly convex with \(\mu_{h}>0\), but \(\mathcal{H}_{i}\) is only merely concave for all \(i\in[n]\). Then, under the same settings as in Theorem 3, but with \(S:=\lfloor\frac{\ln(7/2)}{\mu_{h}\hat{\eta}}\rfloor\), the conclusions of Theorem 3 still hold._

\((\mathrm{c})\) _Convergence of the full-shuffling variant - The case \(S=1\) with one epoch._ Both Theorems 3 and 4 require \(\mathcal{O}(n\epsilon^{-4})\) evaluations of \(\nabla_{u}\mathcal{H}_{i}\). To improve this complexity, we need two additional assumptions but can perform only one epoch of (27), i.e. \(S=1\).

**Assumption 6**.: _Let \(\hat{\mathcal{G}}_{\eta}(u):=\eta^{-1}(u-\mathrm{prox}_{\eta h}(u+\eta\nabla_{u }\mathcal{H}(w,u)))\) be the gradient mapping of \(\psi(w,\cdot):=-\mathcal{H}(w,\cdot)+h(\cdot)\). Assume that there exist \(\hat{\Lambda}_{0}\geq 1\) and \(\hat{\Lambda}_{1}\geq 0\) such that_

\[\|\nabla_{u}\mathcal{H}(w,u)\|^{2}\leq\hat{\Lambda}_{0}\|\hat{\mathcal{G}}_{ \eta}(u)\|^{2}+\hat{\Lambda}_{1},\quad\forall(w,u)\in\mathrm{dom}\left( \mathcal{L}\right).\] (29)

Clearly, if \(h=0\), then \(\hat{\mathcal{G}}_{\eta}(u)=-\nabla_{u}\mathcal{H}(w,u)\) and (20) automatically holds for \(\hat{\Lambda}_{0}=1\) and \(\hat{\Lambda}_{1}=0\). Assumption 6 is similar to Assumption 5, and it is required to handle the \(\mathrm{prox}\) operator of \(h\) in (27).

**Assumption 7**.: _For \(f\) in (1), there exists \(L_{f}\geq 0\) such that_

\[f(y)\leq f(x)+\langle f^{\prime}(x),y-x\rangle+\tfrac{L_{f}}{2}\|y-x\|^{2}, \quad\forall x,y\in\mathrm{dom}\left(f\right),\;f^{\prime}(x)\in\partial f(x).\] (30)

Clearly, if \(f\) is \(L_{f}\)-smooth, then (30) holds. If \(f\) is also convex, then (30) implies that \(f\) is \(L_{f}\)-smooth.

Under these additional assumptions, we have the following result.

**Theorem 5**.: _Suppose that Assumptions 1, 2, 4, 5, 6, and 7 hold and \(\mathcal{G}_{\eta}\) is defined by (18). Let \(\{(\widetilde{w}_{t},\widetilde{u}_{t})\}\) be generated by Algorithm 2 using **one epoch**\((S=1)\) of the **shuffling routine** (27), and fixed learning rates \(\eta_{t}=\eta:=\mathcal{O}(\epsilon)\) as in Theorem 9 of Supp. Doc. C for a given \(\epsilon>0\), \(\hat{\eta}_{t}:=\hat{\eta}=30\kappa^{2}\eta\), and \(T:=\mathcal{O}(\epsilon^{-3})\), where \(\kappa:=\frac{L_{u}}{\mu_{H}+\mu_{h}}\). Then, we have \(\frac{1}{T+1}\sum_{t=0}^{T}\|\mathcal{G}_{\eta}(\widetilde{w}_{t})\|^{2}\leq \epsilon^{2}\).__Consequently, Algorithm 2 requires \(\mathcal{O}(n\epsilon^{-3})\) evaluations of both \(\nabla_{w}\mathcal{H}_{i}\) and of \(\nabla_{u}\mathcal{H}_{i}\) to achieve an \(\epsilon\)-stationary point \(\widehat{w}_{T}\) of (3) computed by (19)._

Similar to Algorithm 1, if \(\pi^{(s,t)}\) and \(\hat{\pi}^{(t)}\) are generated randomly and independently, \(\Lambda_{1}=\mathcal{O}(1/n)\), and \(\hat{\Lambda}_{1}=\mathcal{O}(1/n)\), then our complexity stated above can be improved by a factor of \(\sqrt{n}\). Nevertheless, we omit this analysis. Finally, we can combine each Theorem 2, 3, 4 or 5 and Lemma 2 to construct an \(\hat{\epsilon}\)-KKT point of (1). Theorem 5 has a better complexity than Theorems 3 and 4, but requires stronger assumptions. Algorithm 2 is also different from the one in [3] both in terms of algorithmic form and the underlying problem to be solved, while achieving the same oracle complexity.

## 5 Numerical Experiments

We perform some experiments to illustrate Algorithm 1 and compare it with two existing and related algorithms. Further details and additional experiments can be found in Supp. Doc. D.

We consider the following regularized stochastic minimax problem studied, e.g., in [9; 33]:

\[\min_{w\in\mathbb{R}^{p}}\Big{\{}\max_{1\leq j\leq m}\big{\{}\frac{1}{n}\sum_ {i=1}^{n}F_{i,j}(w)\big{\}}+\frac{\lambda}{2}\|w\|^{2}\Big{\}},\] (31)

where \(F_{i,j}:\mathbb{R}^{p}\times\Omega\to\mathbb{R}_{+}\) can be viewed as the loss of the \(j\)-th model for data point \(i\in[n]\). If we define \(\phi_{0}(v):=\max_{1\leq j\leq m}\{v_{j}\}\) and \(f(w):=\frac{\lambda}{2}\|w\|^{2}\), then (31) can be reformulated into (3). Since \(v_{j}\geq 0\), we have \(\phi_{0}(v):=\max_{1\leq j\leq m}\{v_{j}\}=\|v\|_{\infty}=\max_{\|u\|_{1}\leq 1 }\langle v,u\rangle\), which is nonsmooth. Thus we can smooth \(\phi_{0}\) as \(\phi_{\gamma}(v):=\max_{\|u\|_{1}\leq 1}\{\langle v,u\rangle-(\gamma/2)\|u\|^{2}\}\) using \(b(u):=\frac{1}{2}\|u\|^{2}\).

Here, we apply our problem (31) to solve a model selection problem in binary classification with nonnegative nonconvex losses, see, e.g., [41]. Each function \(F_{i,j}\) belongs to \(4\) different nonconvex losses (\(m=4\)): \(F_{i,1}(w,\xi):=1-\tanh(b_{i}\langle a_{i},w\rangle)\), \(F_{i,2}(w,\xi):=\log(1+\exp(-b_{i}\langle a_{i},w\rangle))-\log(1+\exp(-b_{i} \langle a_{i},w\rangle-1))\), \(F_{i,3}(w,\xi):=(1-1/(\exp(-b_{i}\langle a_{i},w\rangle)+1))^{2}\), and \(F_{i,4}(w,\xi):=\log(1+\exp(-b_{i}\langle a_{i},w\rangle))\) (see [41] for more details), where \((a_{i},b_{i})\) represents data samples.

We implement 4 algorithms: our SGM with 2 options, SGD from [10], and Prox-Linear from [11]. We test these algorithms on two datasets from LIBSVM [6]. We set \(\lambda:=10^{-4}\) and update the smooothing parameter \(\gamma_{t}\) as \(\gamma_{t}:=\frac{1}{2(t+1)^{1/3}}\). The learning rate \(\eta\) for all algorithms is finely tuned from \(\{100,50,10,5,1,0.5,0.1,0.05,0.01,0.001,0.0001\}\), and the results are shown in Figure 1 for **w8a** and **rev1** datasets using \(k_{b}=32\) blocks. The details of this experiment is given in Supp. Doc. D.

As shown in Figure 1, the two variants of our SGM have a comparable performance with SGD and Prox-Linear, providing supportive evidence for using shuffling strategies in minimax algorithms.

## 6 Conclusions

This work explores a bilevel optimization approach to address two prevalent classes of nonconvex-concave minimax problems. These problems find numerous applications in practice, including robust learning and generative AIs. Motivated by the widespread use of shuffling strategies in implementing gradient-based methods within the machine learning community, we develop novel shuffling-based algorithms for solving these problems under standard assumptions. The first algorithm uses a non-standard shuffling strategy and achieves the state-of-the-art oracle complexity typically observed in nonconvex optimization. The second algorithm is also new, flexible, and offers a promising possibility for further exploration. Our results are expected to provide theoretical justification for incorporating shuffling strategies into minimax optimization algorithms, especially in nonconvex settings.

Figure 1: The performance of 4 algorithms for solving (31) on two datasets after 200 epochs.

## Acknowledgments and Disclosure of Funding

This work was partly supported by the National Science Foundation (NSF): NSF-RTG grant No. NSF DMS-2134107 and the Office of Naval Research (ONR), grant No. N00014-23-1-2588.

## References

* [1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In _International Conference on Machine Learning_, pages 214-223, 2017.
* [2] M. G. Azar, I. Osband, and R. Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pages 263-272. PMLR, 2017.
* [3] A. Ben-Tal, L. El Ghaoui, and A. Nemirovski. _Robust optimization_. Princeton University Press, 2009.
* [4] A. Beznosikov, E. Gorbunov, H. Berard, and N. Loizou. Stochastic gradient descent-ascent: Unified theory and new efficient methods. In _International Conference on Artificial Intelligence and Statistics_, pages 172-235. PMLR, 2023.
* [5] K. Bhatia and K. Sridharan. Online learning with dynamics: A minimax perspective. _Advances in Neural Information Processing Systems_, 33:15020-15030, 2020.
* [6] C.-C. Chang and C.-J. Lin. LIBSVM: A library for Support Vector Machines. _ACM Transactions on Intelligent Systems and Technology_, 2:27:1-27:27, 2011.
* [7] H. Cho and C. Yun. SGDA with shuffling: faster convergence for nonconvex-PL minimax optimization. _The 11th International Conference on Learning Representations_, pp. 1-10, 2022.
* [8] A. Das, B. Scholkopf, and M. Muehlebach. Sampling without replacement leads to faster rates in finite-sum minimax optimization. _Advances in Neural Information Processing Systems_, 35:6749-6762, 2022.
* [9] S. Dempe. _Foundations of Bilevel Programming_. Springer Science & Business Media, 2002.
* [10] D. Driggs, J. Liang, and C.-B. Schonlieb. On biased stochastic gradient estimation. _Journal of Machine Learning Research_, vol. 23, no. 24, pp. 1-43, 2022.
* [11] K. Emmanouilidis, R. Vidal, and N. Loizou. Stochastic extragradient with random reshuffling: Improved convergence for variational inequalities. In _International Conference on Artificial Intelligence and Statistics_, pages 3682-3690. PMLR, 2024.
* [12] G. Gidel, H. Berard, G. Vignoud, P. Vincent, and S. Lacoste-Julien. A variational inequality perspective on generative adversarial networks. _International Conference on Learning Representations_, pp. 1-10, 2019.
* [13] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In _Advances in neural information processing systems_, pages 2672-2680, 2014.
* [14] E. Gorbunov, H. Berard, G. Gidel, and N. Loizou. Stochastic extragradient: General analysis and improved rates. In _International Conference on Artificial Intelligence and Statistics_, pages 7865-7901. PMLR, 2022.
* [15] E. Y. Hamedani, A. Jalilzadeh, N. S. Aybat, and U. V. Shanbhag. Iteration complexity of randomized primal-dual methods for convex-concave saddle point problems. _arXiv preprint arXiv:1806.04118_, 2018.
* [16] J. Z. HaoChen and S. Sra. Random shuffling beats SGD after finite epochs. _International Conference on Machine Learning_, pp. 2624-2633, 2019.
* [17] E. Ho, A. Rajagopalan, A. Skvortsov, S. Arulampalam, and M. Piraveenan. Game theory in defence applications: A review. _Sensors_, 22(3):1032, 2022.

* [18] Y. Hsieh, F. Iutzeler, J. Malick, and P. Mertikopoulos. Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling. _Advances in Neural Information Processing Systems_, 33:16223-16234, 2020.
* [19] Abdul Jabbar, Xi Li, and Bourahla Omar. A survey on generative adversarial networks: Variants, applications, and training. _ACM Computing Surveys (CSUR)_, 54(8):1-49, 2021.
* [20] G. Lan. _First-order and Stochastic Optimization Methods for Machine Learning_. Springer, 2020.
* [21] F. Lin, X. Fang, and Z. Gao. Distributionally robust optimization: A review on theory and applications. _Numerical Algebra, Control & Optimization_, 12(1):159, 2022.
* [22] N. Loizou, H. Berard, G. Gidel, I. Mitliagkas, and S. Lacoste-Julien. Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity. _Advances in Neural Information Processing Systems_, 34:19095-19108, 2021.
* [23] L. Luo, H. Ye, and T. Zhang. Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems. _Advances in Neural Information Processing Systems_, vol. 33, pp. 20566-20577, 2020.
* [24] Z. Luo, J. Pang, and D. Ralph. _Mathematical Programs with Equilibrium Constraints_. Cambridge University Press, Cambridge, 1996.
* [25] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018.
* [26] Q. Meng, W. Chen, Y. Wang, Z.-M. Ma, and T.-Y. Liu. Convergence analysis of distributed stochastic gradient descent with shuffling. _Neurocomputing_, 337:46-57, 2019.
* [27] K. Mishchenko, A. Khaled, and P. Richtarik. Random reshuffling: Simple analysis with vast improvements. _Advances in Neural Information Processing Systems_, 33:17309-17320, 2020.
* [28] K. Mishchenko, A. Khaled, and P. Richtarik. Proximal and federated random reshuffling. In _International Conference on Machine Learning_, pages 15718-15749. PMLR, 2022.
* [29] Y. Nesterov. _Introductory lectures on convex optimization: A basic course_, volume 87 of _Applied Optimization_. Kluwer Academic Publishers, 2004.
* [30] L. M. Nguyen, Q. Tran-Dinh, D. T. Phan, P. H. Nguyen, and M. van Dijk. A unified convergence analysis for shuffling-type gradient methods. _Journal of Machine Learning Research_, 22(207):1-44, 2021.
* [31] B. Palaniappan and F. Bach. Stochastic variance reduction methods for saddle-point problems. In _Advances in Neural Information Processing Systems_, pages 1416-1424, 2016.
* [32] I. Safran and O. Shamir. How good is SGD with random shuffling? _Conference on Learning Theory_, pp. 3250-3284, 2020.
* [33] A. Shapiro and A. Kleywegt. Minimax analysis of stochastic problems. _Optim. Methods Softw._, 17(3):523-542, 2002.
* [34] Q. Tran-Dinh, D. Liu, and L. M. Nguyen. Hybrid variance-reduced SGD algorithms for nonconvex-concave minimax problems. _The 34th Conference on Neural Information Processing Systems (NeurIPs 2020)_, 2020.
* [35] J. Wang, T. Zhang, S. Liu, P.-Y. Chen, J. Xu, M. Fardad, and B. Li. Adversarial attack generation empowered by min-max optimization. _Advances in Neural Information Processing Systems_, 34:16020-16033, 2021.
* [36] M. Wang, E. Fang, and L. Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. _Math. Program._, 161(1-2):419-449, 2017.

* [37] J. Yang, N. Kiyavash, and N. He. Global convergence and variance-reduced optimization for a class of nonconvex-nonconcave minimax problems. _arXiv preprint arXiv:2002.09621_, 2020.
* [38] B. Ying, K. Yuan, and A. H. Sayed. Convergence of variance-reduced stochastic learning under random reshuffling. _arXiv preprint arXiv:1708.01383_, 2(3):6, 2017.
* [39] J. Zhang and L. Xiao. Stochastic variance-reduced prox-linear algorithms for nonconvex composite optimization. _Mathematical Programming_, pp. 1-43, 2022.
* [40] K. Zhang, Z. Yang, and T. Basar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. _Handbook of reinforcement learning and control_, pages 321-384, 2021.
* [41] L. Zhao, M. Mammadov, and J. Yearwood. From convex to nonconvex: a loss function analysis for binary classification. In _IEEE International Conference on Data Mining Workshops (ICDMW)_, pages 1281-1288. IEEE, 2010.
* [42] R. Zhao. Optimal stochastic algorithms for convex-concave saddle-point problems. _arXiv preprint arXiv:1903.01687_, 2019.

#### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our claims made in the abstract reflects our contribution stated in the introduction, see the "Contribution" paragraph in the introduction section. Our contribution consists of two algorithms, Algorithm 1 and Algorithm 2, and their theoretical convergence guarantees stated in the subsequent theorems. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: This paper has limitation as it only focuses on two classes of minimax problems defined in (1). Yes, we only consider two classes of minimax problems: nonconvex-linear (NL) and convex-strongly concave (NC), covered by our assumption, Assumptions 1 to 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We state all required assumptions in Assumptions 1 to 5. Our theoretical results stated in each theorem also refers to these assumptions when required. Our full proofs are given in Supp. Docs. due to space limit, and we believe that our technical proofs are correct. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide the details of our experiments, including mathematical models, the detailed implementation of algorithms, the choice of parameters, and datasets. We also upload the code with examples to run and verify. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our data is available online from LIBSVM. The code is implemented in Python. The code for all experiments is also provided with instruction. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Supp. Doc. D provides all the details of our experiments, including how to select parameters, and how to report our results. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not have such a result to report. Guidelines:* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Our experiments were run on a MacBook Pro. 2.8GHz Quad-Core Intel Core I7, 16Gb Memory specified at the beginning of Supp. Doc. D. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our data is publicly available online from LIBSVM. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]Justification: We do not yet know if our paper has an immediate broader impact. However, since our problems and our algorithms are sufficiently general, we hope they will create broader impacts. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not have our own real data or specific model that has a high risk for misuse. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Our code is open-source and will be made available online under a standard public license. Guidelines:

* The answer NA means that the paper does not use existing assets.

* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: It does not have new asset. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: It does not relate to crowdsourcing experiments and research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: It does not require any approval.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.