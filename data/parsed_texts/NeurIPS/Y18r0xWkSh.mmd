# Posterior Contraction Rates for Matern Gaussian Processes on Riemannian Manifolds

 Paul Rosa

University of Oxford

&Viacheslav Borovitskiy

ETH Zurich

Alexander Terenin

University of Cambridge

and Cornell University

&Judith Rousseau

University of Oxford

###### Abstract

Gaussian processes are used in many machine learning applications that rely on uncertainty quantification. Recently, computational tools for working with these models in geometric settings, such as when inputs lie on a Riemannian manifold, have been developed. This raises the question: can these intrinsic models be shown theoretically to lead to better performance, compared to simply embedding all relevant quantities into \(\mathbb{R}^{d}\) and using the restriction of an ordinary Euclidean Gaussian process? To study this, we prove optimal contraction rates for intrinsic Matern Gaussian processes defined on compact Riemannian manifolds. We also prove analogous rates for extrinsic processes using trace and extension theorems between manifold and ambient Sobolev spaces: somewhat surprisingly, the rates obtained turn out to coincide with those of the intrinsic processes, provided that their smoothness parameters are matched appropriately. We illustrate these rates empirically on a number of examples, which, mirroring prior work, show that intrinsic processes can achieve better performance in practice. Therefore, our work shows that finer-grained analyses are needed to distinguish between different levels of data-efficiency of geometric Gaussian processes, particularly in settings which involve small data set sizes and non-asymptotic behavior.

## 1 Introduction

Gaussian processes provide a powerful way to quantify uncertainty about unknown regression functions via the formulation of Bayesian learning. Motivated by applications in the physical and engineering sciences, a number of recent papers [11, 9, 10, 37] have studied how to extend this model class to spaces with geometric structure, in particular Riemannian manifolds including important special cases such as spheres and Grassmannians [4], hyperbolic spaces and spaces of positive definite matrices [5], as well as general manifolds approximated numerically by a mesh [11].

These Riemannian Gaussian process models are starting to be applied for statistical modeling, and decision-making settings such as Bayesian optimization. For example, in a robotics setting, Jaquier et al. [27] has shown that using Gaussian processes with the correct geometric structure allows one to learn quantities such as the orientation of a robotic arm with less data compared to baselines. The same model class has also been used by Coveney et al. [15] to perform Gaussian process regression on a manifold which models the geometry of a human heart for downstream applications in medicine.

Given these promising empirical results, it is important to understand whether these learning algorithms have good theoretical properties, as well as their limitations. Within the Bayesian framework, a natural way to quantify data-efficiency and generalization error is to posit a data-generating mechanism model and study if--and how fast--the posterior distribution concentrates around the true regression function as the number of observations goes to infinity.

Within the Riemannian setting, it is natural to compare _intrinsic_ methods, which are formulated directly on the manifold of interest, with _extrinsic_ ones, which require one to embed the manifold within a higher-dimensional Euclidean space. For example, the two-dimensional sphere can be embedded into the Euclidean space \(\mathbb{R}^{3}\): intrinsic Gaussian processes model functions on the sphere while extrinsic ones model functions on \(\mathbb{R}^{3}\), which are then restricted to the sphere. Are the former more efficient than the latter? Since embeddings--even isometric ones--at best only preserve distances locally, they can induce spurious dependencies, as points can be close in the ambient space but far away with respect to the intrinsic geodesic distance: this is illustrated in Figure 1. In cases where embeddings significantly alter distances, one can expect intrinsic models to perform better, and it is therefore interesting to quantify such differences.

In other settings, the manifold on which the data lies can be unknown, which makes using intrinsic methods directly no longer possible. There, one would like to understand how well extrinsic methods can be expected to perform. According to the _manifold hypothesis_[18], it is common for perceptual data such as text and images to concentrate on a lower-dimensional submanifold within, for instance, pixel space or sequence space. It is therefore also interesting to investigate how Gaussian process models--which, being kernel-based, are simpler than for instance deep neural networks--perform in such scenarios, at least in the asymptotic regime.

In this work, we develop geometric analogs of the Gaussian process posterior contraction theorems of van der Vaart and van Zanten [56]. More specifically, we derive posterior contraction rates for three main geometric model classes: (1) the intrinsic Riemannian Matern Gaussian processes, (2) truncated versions of the intrinsic Riemannian Matern Gaussian processes, which are used in practice to avoid infinite sums, and (3) the extrinsic Euclidean Matern Gaussian processes under the assumption that the data lies on a compact Riemannian manifold. In all cases, we focus on IID randomly-sampled input points--commonly referred to as _random design_ in the literature--and contraction in the sense of the \(L^{2}(p_{0})\) distance, defined in Section 2. We focus on _compact_ Riemannian manifolds: this allows one to define Matern Gaussian processes through their Karhunen-Loeve expansions, which requires a discrete spectrum for the Laplace-Beltrami operator--see for instance Borovitskiy et al. [11] and Chavel [13], Chapter 1--and is a common setting in statistics [39].

Contributions.We show that all three classes of Gaussian processes lead to optimal procedures, in the minimax sense, as long as the smoothness parameter of the kernel is aligned with the regularity of the unknown function. While this result is natural--though non trivial--in the case of intrinsic Matern processes, it is rather remarkable that it also holds for extrinsic ones. This means that in order to understand their differences better, finite-sample considerations are necessary. We therefore present experiments that compute the worst case errors numerically. These experiments highlight that intrinsic models are capable of achieving better performance in the small-data regime. We conclude with a discussion of why these results--which might at first seem counterintuitive--are very natural

Figure 1: Samples from different Matern Gaussian processes on different manifolds, namely a one-dimensional dumbbell-shaped manifold and a two-dimensional sphere. Notice that the values across the dumbbellâ€™s bottleneck can be very different for the intrinsic process in (a), despite being very close in the ambient Euclidean distance and in contrast to the situation for the extrinsic model in (b). On the other hand, there is little qualitative difference between (c) and (d), since the embedding produces a reasonably-good global approximation to geodesic distances on the sphere.

when viewed from an appropriate mathematical perspective: they suggest that optimality is perhaps best seen as a basic property or an important guarantee that any sensible model should satisfy.

## 2 Background

_Gaussian process regression_ is a Bayesian approach to regression where the modeling assumptions are \(y_{i}=f(x_{i})+\varepsilon_{i}\), with \(\varepsilon_{i}\sim\mathrm{N}(0,\sigma_{\varepsilon}^{2})\), \(x_{i}\in X\), and \(f\) is assigned a Gaussian process prior. A _Gaussian process_ is a random function \(f:X\to\mathbb{R}\) for which all finite-dimensional marginal distributions are multivariate Gaussian. The distribution of such a process is uniquely determined by its _mean function_\(m(\cdot)=\mathbb{E}(f(\cdot))\) and _covariance kernel_\(k(\cdot,\cdot^{\prime})=\mathrm{Cov}(f(\cdot),f(\cdot^{\prime}))\), hence we write \(f\sim\mathrm{GP}(m,k)\).

For Gaussian process regression, the posterior distribution given the data is also a Gaussian process with probability kernel \(\Pi(\cdot\mid\bm{x},\bm{y})=\mathrm{GP}(m_{\Pi(\cdot\mid\bm{x},\bm{y})},k_{ \Pi(\cdot\mid\bm{x},\bm{y})})\), see Rasmussen and Williams [41],

\[m_{\Pi(\cdot\mid\bm{x},\bm{y})}(\cdot)=\mathbf{K}_{(\cdot):\bm{ x}}(\mathbf{K}_{\bm{x}\bm{x}}+\sigma_{\varepsilon}^{2}\mathbf{I})^{-1}\bm{y},\] (1) \[k_{\Pi(\cdot\mid\bm{x},\bm{y})}(\cdot,\cdot^{\prime})=\mathbf{K}_ {(\cdot,\cdot^{\prime})}-\mathbf{K}_{(\cdot):\bm{x}}(\mathbf{K}_{\bm{x}\bm{x} }+\sigma_{\varepsilon}^{2}\mathbf{I})^{-1}\mathbf{K}_{\bm{x}(\cdot^{\prime})}.\] (2)

These quantities describe how incorporating data updates the information contained within the Gaussian process. We will be interested studying the case where \(X\) is a Riemannian manifold, but first review the existing theory on the asymptotic behaviour of the posterior when \(X=[0,1]^{d}\).

### Posterior Contraction Rates

Posterior contraction results describe how the posterior distribution concentrates around the true data generating process, as the number of observations increases, so that it eventually uncovers the true data-generating mechanism. The area of _posterior asymptotics_ is concerned with understanding conditions under which this does or does not occur, with questions of _posterior contraction rates_--how fast such convergence occurs--being of key interest. At present, there is a well-developed literature on posterior contraction rates, see Ghosal and van der Vaart [20] for a review.

In the context of Gaussian process regression with _random design_, which is the focus of this paper, the true data generating process is assumed to be of the form

\[y_{i}\mid x_{i}\sim\mathrm{N}(f_{0}(x_{i}),\sigma_{\varepsilon}^{2}) x_{i}\sim p_{0}\] (3)

where \(f_{0}\in\mathcal{F}\subseteq\mathbb{R}^{X}\), a class of real-valued functions, and \(\mathrm{N}(\mu,\sigma^{2})\) denotes the Gaussian with moments \(\mu,\sigma^{2}\). Note that, in this particular variant, these equations exactly mirror those of the Gaussian process model's likelihood, including the use of the same noise variance \(\sigma_{\varepsilon}^{2}\) in both cases: in this paper, we focus on the particular case where \(\sigma_{\varepsilon}\) is known in advance. This setting is restrictive, one can extend to an unknown \(\sigma_{\varepsilon}>0\) using techniques that are not specific to our geometric setting: for instance, the approach of [55] allows to handle an unknown \(\sigma_{\varepsilon}\) if one assumes an upper and lower bound on it and keep the same contraction rates. In practice, more general priors, including ones that do not assume an upper or lower bound on \(\sigma_{\varepsilon}\), can be used, such as a conjugate one like in Banerjee [6]--these can also be analyzed to obtain contraction rates, albeit with additional considerations. The generalization error for prediction in such models is strongly related to the _weighted \(L^{2}\) loss_ given by

\[\left\lVert f-f_{0}\right\rVert_{L^{2}(p_{0})}=\left(\int_{X}\lvert f(x)-f_{0} (x)\rvert^{2}\,\mathrm{d}p_{0}(x)\right)^{1/2}\] (4)

which is arguably the natural way of measuring discrepancy between \(f\) and \(f_{0}\), given the fact that the covariates \(x_{i}\) are sampled from \(p_{0}\). The posterior contraction rate is then defined as

\[\mathbb{E}_{\bm{x},\bm{y}}\,\mathbb{E}_{f\sim\Pi(\cdot\mid\bm{x},\bm{y})} \lVert f-f_{0}\rVert_{L^{2}(p_{0})}^{2}\] (5)

where \(\mathbb{E}_{f\sim\Pi(\cdot\mid\bm{x},\bm{y})}(\cdot)\) denotes expectation under the posterior distribution while \(\mathbb{E}_{\bm{x},\bm{y}}(\cdot)\) denotes expectation under the true data generating process.1 In the case of covariates distributed on \([0,1]^{d}\), posterior contraction rates have been derived under Matern Gaussian process priors [47] in van der Vaart and van Zanten [56], who showed the following result.

Footnote 1: Note that other notions of posterior contraction can be found in the literature, see Ghosal and van der Vaart [20] and Rousseau [42] for examples that are slightly weaker than the definition we work with.

**Result 1** (Theorem 2 of van der Vaart and van Zanten [56]).: _In the Bayesian regression model, let \(f\) be a mean-zero Matern Gaussian process prior on \(\mathbb{R}^{d}\) with amplitude \(\sigma_{f}^{2}\), length scale \(\kappa\), and smoothness \(\nu>d/2\). Assume that the true data generating process is given by (3), where \(p_{0}\) has a Lebesgue density on \(X=[0,1]^{d}\) which is bounded from below and above by \(0<c_{p_{0}}<C_{p_{0}}<\infty\), respectively. Let \(f_{0}\in H^{\beta}\cap\mathcal{CH}^{\beta}\) with \(\beta>d/2\), where \(H^{\beta}\) and \(\mathcal{CH}^{\beta}\) the Sobolev and Holder spaces, respectively. Then there exists a constant \(C>0\), which does not depend on \(n\) but does depend on \(d\), \(\sigma_{f}^{2}\), \(\nu\), \(\kappa\), \(\beta\), \(p_{0}\), \(\sigma_{e}^{2}\), \(\|f_{0}\|_{H^{\beta}(\mathcal{M})}\), and \(\|f_{0}\|_{\mathcal{CH}^{\beta}(\mathcal{M})}\), such that_

\[\mathbb{E}_{\bm{x},\bm{y}}\,\mathbb{E}_{f\sim\Pi(\cdot|\bm{x},\bm{y})}\|f-f_{0 }\|_{L^{2}(p_{0})}^{2}\leq Cn^{-\frac{2\min(\beta,\nu)}{2\nu+d}}\] (6)

_and, moreover, the posterior mean satisfies_

\[\mathbb{E}_{\bm{x},\bm{y}}\,\|m_{\Pi(\cdot|\bm{x},\bm{y})}-f_{0}\|_{L^{2}(p_{ 0})}^{2}\leq Cn^{-\frac{2\min(\beta,\nu)}{2\nu+d}}.\] (7)

Note that \(m_{\Pi(\cdot|\bm{x},\bm{y})}\) is the Bayes estimator [52] of \(f\) associated to the weighted \(L^{2}\) loss and that the second inequality above is a direct consequence of the first. Therefore the posterior contraction rate implies the same convergence rate for \(m_{\Pi(\cdot|\bm{x},\bm{y})}\). The best rate is attained when \(\beta=\nu\): that is, when true smoothness and prior smoothness match--which is known to be minimax optimal in the problem of estimating \(f_{0}\): see Tsybakov [52]. In this paper, we extend this result to the manifold setting.

### Related Work and Current State of Affairs

The formalization of posterior contraction rates of Bayesian procedures dates back to the work of Schwartz [46] and Le Cam [29], but has been extensively developed since the seminal paper of Ghosal et al. [19] for various sampling and prior models, see for instance [20; 42] for reviews. This includes, in particular, work on Gaussian process priors [54; 56; 57; 43; 49]. Most of the results in the literature, however, assume Euclidean data: as a consequence, contraction properties of Bayesian models under manifold assumptions are still poorly understood, with exception of some recent developments in both density estimation [7; 8; 60] and regression [63; 60].

The results closest to ours are those of Yang and Dunson [63] and Castillo et al. [12]. In the former, the authors use an extrinsic length-scale-mixture of squared exponential Gaussian processes to achieve optimal contraction rates with respect to the weighted \(L^{2}\) norm, using a completely different proof technique compared to us, and their results are restricted to \(f_{0}\) having Holder smoothness of order less than or equal to two. On the other hand Castillo et al. [12] consider, as an intrinsic process on the manifold, a hierarchical Gaussian process based on its heat kernel and provide posterior contraction rates. For the Matern class, Li et al. [30] presents results which characterize the asymptotic behavior of kernel hyperparameters: our work complements these results by studying contraction of the Gaussian process itself toward the unknown ground-truth function. One can also study analogous discrete problems: Dunson et al. [17] and Sanz-Alonso and Yang [45] present posterior contraction rates for a specific graph Gaussian process model in a semi-supervised setting. In the next section, we present our results on Matern processes, defined either by restriction of an ambient process or by an intrinsic construction, and discuss their implications.

## 3 Posterior Contraction Rates on Compact Riemannian Manifolds

We now study posterior contraction rates for Matern Gaussian processes on manifolds, which are arguably the most-widely-used Gaussian process priors in both the Euclidean and Riemannian settings. We begin by more precisely describing our geometric setting before stating our key results and discussing their implications. From now on, we write \(X=\mathcal{M}\), to emphasize that the covariate space is a manifold.

**Assumption 2**.: _Assume that \(\mathcal{M}\subset\mathbb{R}^{D}\) is a smooth, compact submanifold (without boundary) of dimension \(d<D\) equipped with the standard Riemannian volume measure \(\mu\)._

We denote \(|\mathcal{M}|=\int_{\mathcal{M}}d\mu(x)\) for volume of \(\mathcal{M}\). With this geometric setting defined, we will need to describe regularity assumptions in terms of functional spaces on the manifold \(\mathcal{M}\). We work with Holder spaces \(\mathcal{CH}^{\gamma}(\mathcal{M})\), defined using charts via the usual Euclidean Holder spaces, the Sobolev spaces \(H^{s}(\mathcal{M})\), and Besov spaces \(B^{s}_{\infty,\infty}(\mathcal{M})\) which are one of the ways of generalizingthe Euclidean Holder spaces of smooth functions to manifolds. We follow Coulhon et al. [14] and Castillo et al. [12], and define these spaces using the Laplace-Beltrami operator on \(\mathcal{M}\) in Appendix A.

Recall that the data-generating process is given by (3), with \(f_{0}\) as the true regression function and \(p_{0}\) as the distribution of the covariates.

**Assumption 3**.: _Assume that \(p_{0}\) is absolutely continuous with respect to \(\mu\), and that its density, denoted by \(p_{0}\), satisfies \(c\leq p_{0}\leq C\) for \(0<c,C<\infty\). Assume the regression function \(f_{0}\) satisfies \(f_{0}\in H^{\beta}(\mathcal{M})\cap B^{\beta}_{\infty,\infty}(\mathcal{M})\) for some \(\beta>d/2\), and that \(\sigma_{\varepsilon}^{2}>0\) is fixed and known._

This setting can be extended to handle unknown variance \(\sigma_{\varepsilon}\) by putting a prior on \(\sigma_{\varepsilon}\), following the strategy of Salomond [44] and Naulet and Barat [36]. Since we are focused primarily on the impact of the manifold, we do not pursue this here. With the setting fully defined, we proceed to develop posterior contraction results for different types of Matern Gaussian process priors: intrinsic, intrinsic truncated and extrinsic.

### Intrinsic Matern Gaussian Processes

We now introduce the first geometric Gaussian process prior under study--the Riemannian Matern kernel of Whittle [62], Lindgren et al. [33], and Borovitskiy et al. [11]. This process was originally defined using stochastic partial differential equations: here, we present it by its Karhunen-Loeve expansion, to facilitate comparisons with its truncated analogs presented in Section 3.2.

**Definition 4** (Intrinsic Matern prior).: _Let \(\nu>0\), and let \((\lambda_{j},f_{j})_{j\geq 0}\) be the eigenvalues and orthonormal eigenfunctions of the Laplace-Beltrami operator on \(\mathcal{M}\), in increasing order. Define the intrinsic Riemannian Matern Gaussian process through its Karhunen-Loeve expansion to be_

\[f(\cdot)=\frac{\sigma_{f}^{2}}{C_{\nu,\kappa}}\sum_{j=1}^{\infty}\biggl{(} \frac{2\nu}{\kappa^{2}}+\lambda_{j}\biggr{)}^{-\frac{\nu+d/2}{2}}\xi_{j}f_{j}(\cdot) \xi_{j}\sim\mathrm{N}(0,1)\] (8)

_where \(\nu,\kappa,\sigma_{f}^{2}\) are positive parameters and \(C_{\nu,\kappa}\) is the normalization constant, chosen such that \(\frac{1}{|\mathcal{M}|}\int_{M}\mathrm{Var}(f(x))\,\mathrm{d}\mu(x)=\sigma_{f} ^{2}\), where \(\mathrm{Var}\) denotes the variance._

The covariance kernels of these processes are visualized in Figure 2. With this prior, and the setting defined in Section 3, we are ready to present our first result: this model attains the desired optimal posterior contraction rate as soon as the regularity of the ground-truth function matches the regularity of the Gaussian process, as described by the parameter \(\nu\).

**Theorem 5**.: _Let \(f\) be a Riemannian Matern Gaussian process prior of Definition 4 with smoothness parameter \(\nu>d/2\) and let \(f_{0}\) satisfy Assumption 3. Then there is a \(C>0\) such that_

\[\mathbb{E}_{\bm{x},\bm{y}}\,\mathbb{E}_{f\sim\Pi(\cdot\mid\bm{x},\bm{y})}\|f- f_{0}\|_{L^{2}(p_{0})}^{2}\leq Cn^{-\frac{2\min(\beta,\nu)}{2\nu+d}}.\] (9)

All proofs are given in Appendix B. Our proof follows the general approach of van der Vaart and van Zanten [56], by first proving a contraction rate with respect to the distance \(n^{-1/2}\|f(\bm{x})-f_{0}(\bm{x})\|_{\mathbb{R}^{n}}\) at input locations \(\bm{x}\), and then extending the result to the true \(L^{2}\)-distance by applying a suitable concentration inequality. The first part is obtained by studying the _concentration function_, which is known to be the key quantity to control in order to derive contraction rates of Gaussian process priors--see Ghosal and van der Vaart [20] and van der Vaart and van Zanten [57] for an overview.

Figure 2: Different Matern kernels \(k(\bullet,x)\) on different manifolds.

Given our regularity assumptions on \(f_{0}\), the most difficult part lies in controlling the small-ball probabilities \(\Pi[\left\|f\right\|_{\mathcal{C}(\mathcal{M})}<\varepsilon\right]\): we handle this by using results relating this quantity with the entropy of an RKHS unit ball with respect to the uniform norm. Since our process' RKHS is related to the Sobolev space \(H^{\nu+d/2}(\mathcal{M})\) which admits a description in terms of charts, we apply results on the entropy of Sobolev balls in the Euclidean space to conclude the first part. Finally, to extend the rate to the true \(L^{2}(p_{0})\) norm, following van der Vaart and van Zanten [56], we prove a Holder-type property for manifold Matern processes, and apply Bernstein's inequality. Together, this gives the claim.

This result is good news for the intrinsic Matern model: it tells us that asymptotically it incorporates the data as efficiently as possible at least in terms of posterior contraction rates, given that its regularity matches the regularity of \(f_{0}\). An inspection of the proof shows that the constant \(C>0\) can be seen to depend on \(d\),\(\sigma_{f}^{2}\), \(\nu\), \(\kappa\), \(\beta\), \(p_{0}\sigma_{\varepsilon}^{2}\), \(\left\|f_{0}\right\|_{H^{\beta}(\mathcal{M})}\), \(\left\|f_{0}\right\|_{B^{\beta}_{\infty\infty}(\mathcal{M})}\), and \(\left\|f_{0}\right\|_{\mathcal{C}\mathcal{H}^{\beta}(\mathcal{M})}\). Theorem 5 extends to the case where the norm is raised to any power \(q>1\) rather than the second power, with the right-hand side raised to the same power: see Appendix B for details. We now consider variants of this prior that can be implemented in practice.

### Truncated Matern Gaussian Processes

The Riemannian Matern prior's covariance kernel cannot in general be computed exactly, since Definition 4 involves an infinite sum. Arguably the simplest way to implement these processes numerically is to truncate the respective infinite series in the Karhunen-Loeve expansion by taking the first \(J\) terms, which is also optimal in an \(L^{2}(\mathcal{M})\)-sense.

Note that the truncated prior is a randomly-weighted finite sum of Laplace-Beltrami eigenfunctions, which have different smoothness properties compared to the original prior: the truncated prior takes its values in \(\mathcal{C}^{\infty}(\mathcal{M})\) since the eigenfunctions of \(\mathcal{M}\) are smooth--see for instance De Vito et al. [16]. Nevertheless, if the truncation level is allowed to grow as the sample size increases, then the regularity of the process degenerates and one gets a function with essentially-finite regularity in the limit.

Truncated random basis expansions have been studied extensively in the Bayesian literature in the Euclidean setting--see for instance Arbel et al. [2] and Yoo et al. [64] or Ghosal and van der Vaart [20], Chapter 11 for examples with priors based on wavelet expansions. It is known that truncating the expansion at a high enough level usually allows one to retain optimality. Instead of truncating deterministically, it is also possible to put a prior on the truncation level and resort to MCMC computations which would then select the optimal number of basis functions adaptively, at the expense of a more computationally intensive method--this is done, for instance, in van der Meulen et al. [53] in the context of drift estimation for diffusion processes. Random truncation has been proven to lead in many contexts to adaptive posterior contraction rates, meaning that although the prior does not depend on the smoothness \(\beta\) of \(f_{0}\), the posterior contraction rate--up to possible \(\ln n\) terms--is of order \(n^{-\beta/(2\beta+d)}\): see for instance Arbel et al. [2] and Rousseau and Szabo [43].

By analogy of the Euclidean case with its random Fourier feature approximations [40], we can call the truncated version of Definition 4 the _manifold Fourier feature_ model, for which we now present our result.

**Theorem 6**.: _Let \(f\) be a Riemannian Matern Gaussian process prior on \(\mathcal{M}\) with smoothness parameter \(\nu>d/2\), modified to truncate the infinite sum to at least \(J_{n}\geq cn^{\frac{d(\min(1,\nu/\beta))}{2\nu+d}}\) terms, and let \(f_{0}\) satisfy Assumption 3. Then there is a \(C>0\) such that_

\[\mathbb{E}_{\boldsymbol{x},\boldsymbol{y}}\,\mathbb{E}_{f\sim\Pi(\cdot| \boldsymbol{x},\boldsymbol{y})}\|f-f_{0}\|_{L^{2}(p_{0})}^{2}\leq Cn^{-\frac{2 \min(\beta,\nu)}{2\nu+d}}.\] (10)

The proof is essentially-the-same as the non-truncated Matern, but involves tracking dependence of the inequalities on the truncation level \(J_{n}\), which implicitly defines a sequence of priors rather than a single fixed prior.

This result is excellent news for the intrinsic models: it means that they inherit the optimality properties of the limiting one, even in the absence of the infinite sum--in spite of the fact that the corresponding finite-truncation prior places its probability on \(\mathcal{C}^{\infty}(\mathcal{M})\). Again, the constant \(C>0\) can be seen to depend on \(d\),\(\sigma_{f}^{2}\), \(\nu\),\(\kappa\),\(\beta\),\(p_{0}\),\(\sigma_{\varepsilon}^{2}\), \(\left\|f_{0}\right\|_{H^{\beta}(\mathcal{M})}\), \(\left\|f_{0}\right\|_{B^{\beta}_{\infty\infty}(\mathcal{M})}\), and \(\left\|f_{0}\right\|_{\mathcal{C}\mathcal{H}^{\beta}(\mathcal{M})}\). This concludes our results for the intrinsic Riemannian Matern priors. We now study what happens if, instead of working with a geometrically-formulated model, we simply embed everything into \(\mathbb{R}^{d}\) and formulate our models there.

### Extrinsic Matern Gaussian Processes

The results of the preceding sections provide good reason to be excited about the intrinsic Riemannian Matern prior: the rates it obtains match the usual minimax rates seen for the Euclidean Matern prior and Euclidean data, provided that we match the smoothness \(\nu\) with the regularity of \(f_{0}\). Another possibility is to consider an extrinsic Gaussian process, that is, a Gaussian process defined over an ambient space. This has been considered by Yang and Dunson [63] for instance for the square-exponential process, in an adaptive setting where one does not assume that the regularity \(\beta\) of \(f_{0}\) is explicitly known, but where \(\beta\leq 2\). In this section we prove a non-adaptive analog of this result for the Matern process.

**Definition 7** (Extrinsic Matern prior).: _Assume that the manifold \(\mathcal{M}\) is isometrically embedded in the Euclidean space \(\mathbb{R}^{D}\), such that we can regard \(\mathcal{M}\) as a subset of \(\mathbb{R}^{D}\). Consider the Gaussian process with zero mean and kernel given by restricting onto \(\mathcal{M}\) the standard Euclidean Matern kernel_

\[k_{\nu,\kappa,\sigma_{f}^{2}}(x,x^{\prime})=\sigma_{f}^{2}\frac{2^{1-\nu}}{ \Gamma(\nu)}\bigg{(}\sqrt{2\nu}\frac{\left\|x-x^{\prime}\right\|_{\mathbb{R}^ {D}}}{\kappa}\bigg{)}^{\nu}K_{\nu}\bigg{(}\sqrt{2\nu}\frac{\left\|x-x^{\prime} \right\|_{\mathbb{R}^{D}}}{\kappa}\bigg{)}\] (11)

_where \(\sigma_{f},\kappa,\nu>0\) and \(K_{\nu}\) is the modified Bessel function of the second kind [22]._

Since the extrinsic Matern process is defined in a completely agnostic way with respect to the manifold geometry, we would expect it to be less performant when \(\mathcal{M}\) is known. However, it turns out that the extrinsic Matern process converges at the same rate as the intrinsic one, as given in the following claim.

**Theorem 8**.: _Let \(f\) be a mean-zero extrinsic Matern Gaussian process prior with smoothness parameter \(\nu>d/2\) on \(\mathcal{M}\), and let \(f_{0}\) satisfy Assumption 3. Then for some \(C>0\) we have_

\[\mathbb{E}_{\boldsymbol{x},\boldsymbol{y}}\,\mathbb{E}_{f\sim\Pi(\cdot| \boldsymbol{x},\boldsymbol{y})}\|f-f_{0}\|_{L^{2}(p_{0})}^{2}\leq Cn^{-\frac{ 2\min(\beta,\nu)}{2\nu+d}}.\] (12)

Theorem 8 is a surprising result because the optimal rates in this setting only require the knowledge of the regularity \(\beta\), but not the knowledge of the manifold or the intrinsic dimension. More precisely, the prior is not designed to be an adaptive prior, since it is a fixed Gaussian process, but it surprisingly adapts to the dimension of the manifold, and thus to the manifold.

The proof is also based on control of concentration functions. The main difference is that, although the ambient process has a well known RKHS--the Sobolev space \(H^{s+D/2}(\mathbb{R}^{D})\)--the restricted process has a non-explicit RKHS, which necessitates further analysis. We tackle this issue by using results from Grosse and Schneider [24] relating manifold and ambient Sobolev spaces by linear bounded trace and extension operators, and from Yang and Dunson [63] describing a general link between the RKHS of an ambient process and its restriction. This allows us to show that the restricted process has an RKHS that is actually norm-equivalent to the Sobolev space \(H^{\nu+d/2}(\mathcal{M})\), which allows us to conclude the result in the same manner as in the intrinsic case.

As consequence, our argument applies _mutatis mutandis_ in any setting where suitable trace and extension theorems apply, with the Riemannian Matern case corresponding to the usual Sobolev results. In particular, our arguments therefore apply directly to other processes possessing similar RKHSs, such as for instance various kernels defined on the sphere--see e.g. Wendland [61], Chapter 17 and Hubbert et al. [26]. The constant \(C>0\) can be seen to depend on \(d\),\(D\),\(\sigma_{f}^{2}\), \(\nu\),\(\kappa\),\(\beta\),\(p_{0}\),\(\sigma_{\varepsilon}^{2}\), \(\left\|f_{0}\right\|_{H^{\beta}(\mathcal{M})}\),\(\left\|f_{0}\right\|_{B^{\beta}_{\infty}(\mathcal{M})}\),\(\left\|f_{0}\right\|_{\mathcal{CH}^{\beta}(\mathcal{M})}\)--notice that here \(C\) depends implicitly on \(D\) because of the presence of trace and extension operator continuity constants. We now proceed to understand the significance of the overall results.

### Summary of Results

As a consequence of our previous results, fixing a single common data generating distribution determined by \(p_{0},f_{0}\), under suitable conditions the intrinsic Matern process, its truncated version, and the extrinsic Matern process all possess the _same_ posterior contraction rate with respect to the \(L^{2}(p_{0})\)-norm, which depends on \(d\), \(\nu\), and \(\beta\), and is optimal if the regularities of \(f_{0}\) and the prior match. These results imply the following immediate corollary, which follows by convexity of \(\left\|\cdot\right\|_{L^{2}(p_{0})}^{2}\) using Jensen's inequality.

**Corollary 9**.: _Under the assumptions of Theorems 5, 6 and 8, it follows that, for some \(C>0\)_

\[\mathbb{E}_{\bm{x},\bm{y}}\left\|m_{\Pi(\cdot|\bm{x},\bm{y})}-f_{0}\right\|_{L^ {2}(p_{0})}^{2}\leq Cn^{-\frac{2\min(\beta,\nu)}{2\nu+d}}\] (13)

_where \(m_{\Pi(\cdot|\bm{x},\bm{y})}\) is the posterior mean given a particular value of \(\left(x_{i},y_{i}\right)_{i=1}^{n}\)._

When \(\nu=\beta\), the optimality of the rates we present in the manifold setting can be easily inferred by lower bounding the \(L^{2}\)-risk of the posterior mean by the \(L^{2}\)-risk over a small subset of \(\mathcal{M}\) and using charts, which translates the problem into the Euclidean framework for which the rate is known to be optimal--see for instance Tsybakov [52].

To contextualize this, observe that even in cases where the geometry of the manifold is non-flat, the asymptotic rates are unaffected by the choice of the prior's length scale \(\kappa\)--in either the intrinsic, or the extrinsic case--but only by the smoothness parameter \(\nu\). Indeed, the RKHS of the process is only determined--up to norm equivalence--by \(\nu\), which plays an important role in the proofs. This, and the fact that extrinsic processes attain the same rates, implies that the study of asymptotic posterior contraction rates _cannot detect geometry_ in our setting, as was already hinted by Yang and Dunson [63]. Hence, in the geometric setting, optimal posterior contraction rates should be thought of more as a basic property that any reasonable model should satisfy. Differences in performance will be down to constant factors alone--but as we will see, these can be significant. To understand these differences, we turn to empirical analysis.

## 4 Experiments

From Theorems 5, 6 and 8, we know that intrinsic and extrinsic Gaussian processes exhibit the same posterior contraction rates in the asymptotic regime. Here, we study how these rates manifest themselves in practice, by examining how worst-case errors akin to those of Corollary 9 behave numerically. Specifically, we consider the pointwise worst-case error

\[v^{(\tau)}(t)=\sup_{\|f_{0}\|_{\mu^{\nu+d/2}}\leq 1}\mathbb{E}_{\varepsilon_{ i}\sim\mathrm{N}(0,\sigma_{\varepsilon}^{2})}\big{|}m_{\Pi(\cdot|\bm{x},\bm{y})} ^{(\tau)}(t)-f_{0}(t)\big{|}^{2}\] (14)

where \(m_{\Pi(\cdot|\bm{x},\bm{y})}^{(\tau)}\) is the posterior mean corresponding to the zero-mean Matern Gaussian process prior with smoothness \(\nu\), length scale \(\kappa\), amplitude \(\sigma_{f}^{2}\), which is intrinsic if \(\tau=\mathrm{i}\) or extrinsic if \(\tau=\mathrm{e}\). We use a Gaussian likelihood with noise variance \(\sigma_{\varepsilon}^{2}\) and observations \(y_{i}=f_{0}(x_{i})+\varepsilon_{i}\), and examine this quantity as a function of the evaluation location \(t\in\mathcal{M}\). By allowing us to assess how error varies in different regions of the manifold, this provides us with a fine-grained picture of how posterior contraction behaves.

One can show that \(v^{(\tau)}\) may be computed without numerically solving an infinite-dimensional optimization problem. Specifically, (14) can be calculated, in the respective intrinsic and extrinsic cases, using

\[v^{(\mathrm{i})}(t) =k^{(\mathrm{i})}(t,t)-\mathbf{K}^{(\mathrm{i})}_{\mathbf{i} \mathbf{X}}\Big{(}\mathbf{K}^{(\mathrm{i})}_{\mathbf{X}\mathbf{X}}+\sigma_{ \varepsilon}^{2}\mathbf{I}\Big{)}^{-1}\mathbf{K}^{(\mathrm{i})}_{\mathbf{X} \mathbf{t}}\] (15) \[v^{(\mathrm{e})}(t) \approx(\mathbf{K}^{(\mathrm{i})}_{\mathbf{i}\mathbf{X}^{\prime }}-\bm{\alpha}_{t}\mathbf{K}^{(\mathrm{i})}_{\mathbf{X}\mathbf{X}^{\prime}})( \mathbf{K}^{(\mathrm{i})}_{\mathbf{X}^{\prime}\mathbf{X}^{\prime}})^{-1}( \mathbf{K}^{(\mathrm{i})}_{\mathbf{X}^{\prime}\mathbf{t}}-\mathbf{K}^{( \mathrm{i})}_{\mathbf{X}^{\prime}\mathbf{X}}\bm{\alpha}_{t}^{\top})+\sigma_{ \varepsilon}^{2}\bm{\alpha}_{t}\bm{\alpha}_{t}^{\top}\] (16)

where, for the extrinsic case, \(\bm{\alpha}_{t}=\mathbf{K}^{(\mathrm{i})}_{\mathbf{i}\mathbf{X}}(\mathbf{K}^ {(\mathrm{e})}_{\mathbf{X}\mathbf{X}}+\sigma_{\varepsilon}^{2}\mathbf{I})^{-1}\), and \(\mathbf{X}^{\prime}\) is a set of points sampled uniformly from the manifold \(\mathcal{M}\), the size of which determines approximation quality. The intrinsic expression is simply the posterior variance \(k^{(\mathrm{i})}_{\Pi(\cdot|\bm{x},\bm{y})}(t,t)\), and its connection with worst-case error is a well-known folklore result mentioned somewhat implicitly in, for instance, Mutny and Krause [35]. The extrinsic expression is very-closely-related, and arises by numerically approximating a certain RKHS norm. A derivation of both is given in Appendix F. To assess the approximation error of this formula, we also consider an analog of (16) but instead defined for the intrinsic model, and compare it to (15): in all cases, the difference between the exact and approximate expression was found to be smaller than differences between models. By computing these expressions, we therefore obtain, up to numerics, the pointwise worst-case expected error in our regression model.

For \(\mathcal{M}\) we consider three settings: a dumbbell-shaped manifold, a sphere, and the dragon manifold from the Stanford 3D scanning repository. In all cases, we perform computations by approximatingthe manifold using a mesh, and implementing the truncated Karhunen-Loeve expansion with \(J=500\) eigenpairs obtained from the mesh. We fix smoothness \(\nu=\frac{5}{2}\), amplitude \(\sigma_{f}^{2}=1\), and noise variance \(\sigma_{\varepsilon}^{2}=0.0005\), for both the intrinsic and extrinsic Matern Gaussian processes. Since the interpretation of the length scale parameter is manifold-specific, for the intrinsic Gaussian processes we set \(\kappa=200\) for the dumbbell, \(\kappa=0.25\) for the sphere, and \(\kappa=0.05\) for the dragon manifold. In all cases, this yielded functions that were neither close to being globally-constant, nor resembled noise. Each experiment was repeated \(10\) times to assess variability. Complete experimental details are given in Appendix G.2

Footnote 2: Code available at: https://github.com/aterenin/geometric_asymptotics.

The length scales \(\kappa\) are defined differently for intrinsic and extrinsic Matern kernels: in particular, using the same length scale in both models can result in kernels behaving very differently. To alleviate this, for the extrinsic process, we set the length scale by maximizing the extrinsic process' marginal likelihood using the full dataset generated by the intrinsic process, except in the dumbbell's case where the full dataset is relatively small, and therefore a larger set of 500 points was used instead. This allows us to numerically match intrinsic and extrinsic length scales to ensure a reasonably-fair comparison.

Figure 3 shows the mean, and spatial standard deviation of \(v_{\tau}(t)\), where by _spatial standard deviation_ we mean the sample standard deviation computed with respect to locations in space, rather than with respect to different randomly sampled datasets. From this, we see that on the dumbbell and dragon manifold--whose geometry differs significantly from the respective ambient Euclidean spaces--intrinsic models obtain better mean performance. The standard deviation plot reveals that intrinsic models have errors that are less-variable across space. This means that extrinsic models exhibit higher errors in some regions rather than others--such as, for instance, regions where embedded Euclidean and Riemannian distances differ--whereas in intrinsic models the error decays in a more spatially-uniform manner.

In contrast, on the sphere, both models perform similarly. Moreover, both the mean and spatial standard deviation decrease at approximately the same rates, indicating that the extrinsic model's predictions are correct about-as-often as the intrinsic model's, as a function of space. This confirms the view that, since the sphere does not possess any bottleneck-like areas where embedded Euclidean distances are extremely different from their Riemannian analogs, it is significantly less affected by differences coming from embeddings.

Figure 3: Worst-case error estimates for the intrinsic and extrinsic processes, on the _dumbbell_, _sphere_, and _dragon_ manifolds (lower is better, \(y\) axis is in the logarithmic scale). We see that, on the dumbbell and dragon manifold, intrinsic models achieve lower expected errors than extrinsic models for the ranges considered (top), and that their expected error consistently varies less as a function of space (bottom). In contrast, on the sphere, both models achieve similar performance, with differences between models falling within the range of variability caused by different random number seeds. We also see that the difference between computing the pointwise worst-case error exactly and approximately, in the intrinsic case where computing this difference is possible, is small in all cases.

In total, our experiments confirm that there are manifolds on which geometric models can perform significantly better than non-geometric models. This phenomenon was also noticed in Dunson et al. [17], where a prior based on the eigendecomposition of a random geometric graph, which can be thought as an approximation of our intrinsic Matern processes, is compared to a standard extrinsic Gaussian process. In our experiments, we see this through expected errors, mirroring prior results on Bayesian optimization performance. From our theoretical results, such differences cannot be captured through posterior contraction rates, and therefore would require sharper technical tools, such as non-asymptotic analysis, to quantify theoretically.

## 5 Conclusion

In this work, we studied the asymptotic behavior of Gaussian process regression with different classes of Matern processes on Riemannian manifolds. By using various results on Sobolev spaces on manifolds we derived posterior contraction rates for intrinsic Matern process defined via their Karhunen-Loeve decomposition in the Laplace-Beltrami eigenbasis, including processes arising from truncation of the respective sum which can be implemented in practice. Next, using trace and extension theorems which relate manifold and Euclidean Sobolev spaces, we derived similar contraction rates for the restriction of an ambient Matern process in the case where the manifold is embedded in Euclidean space. These theoretical asymptotic results were supplemented by experiments on several examples, showing significant differences in performance between intrinsic and extrinsic methods in the small sample size regime when the manifold's geometric structure differs from the ambient Euclidean space. Our work therefore shows that capturing such differences cannot be done through asymptotic contraction rates, motivating and paving the way for further work on non-asymptotic error analysis to capture empirically-observed differences between extrinsic and intrinsic models.

## Acknowledgments

The authors are grateful to Mojmir Mutny and Prof. Andreas Krause for fruitful discussions concerning this work. PR and JR were supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 834175). VB was supported by an ETH Zurich Postdoctoral Fellowship. AT was supported by Cornell University, jointly via the Center for Data Science for Enterprise and Society, the College of Engineering, and the Ann S. Bowers College of Computing and Information Science.

## References

* Aliprantis and Border [2006] C. D. Aliprantis and K. C. Border. _Infinite Dimensional Analysis: A Hitchhiker's Guide_. Springer, 2006. Cited on page 29.
* Arbel et al. [2013] J. Arbel, G. Gayraud, and J. Rousseau. Bayesian Optimal Adaptive Estimation Using a Sieve Prior. _Scandinavian Journal of Statistics_, 2013. Cited on page 29.
* Aubin [1998] T. Aubin. _Some nonlinear problems in Riemannian geometry_. Springer, 1998. Cited on page 29.
* Azangulov et al. [2022] I. Azangulov, A. Smolensky, A. Terenin, and V. Borovitskiy. Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces I: the compact case. _arXiv:2208.14960_, 2022. Cited on page 29.
* Azangulov et al. [2023] I. Azangulov, A. Smolensky, A. Terenin, and V. Borovitskiy. Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces II: non-compact symmetric spaces. _arXiv:2301.13088_, 2023. Cited on page 29.
* Banerjee [2020] S. Banerjee. Modeling massive spatial datasets using a conjugate Bayesian linear modeling framework. _Spatial Statistics_, 2020. Cited on page 29.
* Berenfeld et al. [2022] C. Berenfeld, P. Rosa, and J. Rousseau. Estimating a density near an unknown manifold: a Bayesian nonparametric approach. _arXiv:2205.15717_, 2022. Cited on page 29.
* Bhattacharya and Dunson [2010] A. Bhattacharya and D. B. Dunson. Nonparametric Bayesian density estimation on manifolds with applications to planar shapes. _Biometrika_, 2010. Cited on page 29.
* Borovitskiy et al. [2021] V. Borovitskiy, I. Azangulov, A. Terenin, P. Mostowsky, M. P. Deisenroth, and N. Durrande. Matern Gaussian processes on graphs. In _International Conference on Artificial Intelligence and Statistics_, 2021. Cited on page 29.

* [10] V. Borovitskiy, M. R. Karimi, V. R. Somnath, and A. Krause. Isotropic Gaussian Processes on Finite Spaces of Graphs. In _Artificial Intelligence and Statistics_, 2023. Cited on page 1.
* [11] V. Borovitskiy, A. Terenin, P. Mostowsky, and M. P. Deisenroth. Matern Gaussian processes on Riemannian manifolds. _Advances in Neural Information Processing Systems_, 2020. Cited on pages 1, 2, 5, 15, 23, 25.
* [12] I. Castillo, G. Kerkyacharian, and D. Picard. Thomas Bayes' walk on manifolds. _Probability Theory and Related Fields_, 2013. Cited on pages 4, 5, 15, 32, 33.
* [13] I. Chavel. _Eigenvalues in Riemannian Geometry_. Elsevier, 1984. Cited on pages 2, 14.
* [14] T. Coulhon, G. Kerkyacharian, and P. Petrushev. Heat Kernel Generated frames in the setting of Dirichlet Spaces. _Journal of Fourier Analysis and Applications_, 2012. Cited on pages 5, 15, 32.
* [15] S. Coveney, C. Corrado, C. H. Roney, D. O'Hare, S. E. Williams, M. D. O'Neill, S. A. Niederer, R. H. Clayton, J. E. Oakley, and R. D. Wilkinson. Gaussian Process Manifold Interpolation for Probabilistic Atrial Activation Maps and Uncertain Conduction Velocity. _Philosophical Transactions of the Royal Society A_, 2020. Cited on page 1.
* [16] E. De Vito, N. Mucke, and L. Rosasco. Reproducing kernel Hilbert spaces on manifolds: Sobolev and Diffusion spaces. _Analysis and Applications_, 2021. Cited on pages 6, 14, 15, 17, 23, 25.
* [17] D. B. Dunson, H. Wu, and N. Wu. Graph Based Gaussian Processes on Restricted Domains. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 2021. Cited on pages 4, 10.
* [18] C. Fefferman, S. Mitter, and H. Narayanan. Testing the Manifold Hypothesis. _Journal of the American Mathematical Society_, 2016. Cited on page 2.
* [19] S. Ghosal, J. K. Ghosh, and A. van der Vaart. Convergence Rates of Posterior Distributions. _The Annals of Statistics_, 2000. Cited on page 4.
* [20] S. Ghosal and A. van der Vaart. _Fundamentals of Nonparametric Bayesian Inference_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2017. Cited on pages 3-6, 17, 21, 28, 31.
* [21] E. Gine and R. Nickl. _Mathematical Foundations of Infinite-Dimensional Statistical Models_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2015. Cited on pages 14, 15, 32.
* [22] I. S. Gradshteyn and I. M. Ryzhik. _Table of Integrals, Series, and Products_. Academic Press, 7th edition, 2014. Cited on page 7.
* [23] A. Grigoryan. _Heat Kernel and Analysis on Manifolds_. American Mathematical Society, 2009. Cited on page 14.
* [24] N. Grosse and C. Schneider. Sobolev spaces on Riemannian manifolds with bounded geometry: General coordinates and traces. _Mathematische Nachrichten_, 2013. Cited on pages 7, 14, 24, 32.
* [25] E. Hebey. _Sobolev spaces on Riemannian manifolds_, volume 1635. Springer, 1996. Cited on page 14.
* [26] S. Hubbert, E. Porcu, C. J. Oates, and M. Girolami. Sobolev Spaces, Kernels and Discrepancies over Hyperspheres. _Transactions on Machine Learning Research_, 2023. Cited on page 7.
* [27] N. Jaquier, V. Borovitskiy, A. Smolensky, A. Terenin, T. Asfour, and L. Rozo. Geometry-aware bayesian optimization in robotics using Riemannian Matern kernels. In _Conference on Robot Learning_, 2022. Cited on page 1.
* [28] M. Kanagawa, P. Hennig, D. Sejdinovic, and B. K. Sriperumbudur. Gaussian processes and kernel methods: A review on connections and equivalences. _arXiv:1807.02582_, 2018. Cited on pages 24, 25.
* [29] L. Le Cam. Convergence of Estimates Under Dimensionality Restrictions. _The Annals of Statistics_, 1973. Cited on page 4.
* [30] D. Li, W. Tang, and S. Banerjee. Inference for Gaussian Processes with Matern Covariogram on Compact Riemannian Manifolds. _Journal of Machine Learning Research_, 2023. Cited on page 4.
* [31] W. V. Li and W. Linde. Approximation, Metric Entropy and Small Ball Estimates for Gaussian Measures. _The Annals of Probability_, 1999. Cited on pages 32, 33.

* [32] M. Lifshits. _Lectures on Gaussian processes_. Springer, 2012. Cited on page 17.
* [33] F. Lindgren, H. Rue, and J. Lindstrom. An explicit link between Gaussian fields and Gaussian Markov random fields: the stochastic partial differential equation approach. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 2011. Cited on page 5.
* [34] S. V. Lototsky and B. L. Rozovsky. _Stochastic partial differential equations_. Springer, 2017. Cited on page 26.
* [35] M. Mutny and A. Krause. Experimental design for linear functionals in reproducing kernel hilbert spaces. _Advances in Neural Information Processing Systems_, 2022. Cited on page 8.
* [36] Z. Naulet and E. Barat. Some Aspects of Symmetric Gamma Process Mixtures. _Bayesian Analysis_, 2018. Cited on page 5.
* [37] M. Niu, P. Cheung, L. Lin, Z. Dai, N. D. Lawrence, and D. B. Dunson. Intrinsic Gaussian processes on complex constrained domains. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 2018. Cited on page 1.
* [38] T. Pinder and D. Dodd. GPJax: A Gaussian Process Framework in JAX. _Journal of Open Source Software_, 2022. Cited on page 35.
* [39] E. Porcu, M. Bevilacqua, R. Schaback, and C. J. Oates. The Matern Model: A Journey through Statistics, Numerical Analysis and Machine Learning. _arXiv:2303.02759_, 2023. Cited on page 2.
* [40] A. Rahimi and B. Recht. Random Features for Large-Scale Kernel Machines. _Advances in Neural Information Processing Systems_, 2007. Cited on page 6.
* [41] C. E. Rasmussen and C. K. I. Williams. _Gaussian Processes for Machine Learning_. MIT Press, 2006. Cited on page 3.
* [42] J. Rousseau. On the frequentist properties of Bayesian nonparametric methods. _Annual Review of Statistics and its Application_, 2016. Cited on pages 3, 4.
* [43] J. Rousseau and B. Szabo. Asymptotic behaviour of the empirical Bayes posteriors associated to maximum marginal likelihood estimator. _The Annals of Statistics_, 2017. Cited on pages 4, 6.
* [44] J. Salomond. Testing Un-Separated Hypotheses by Estimating a Distance. _Bayesian Analysis_, 2018. Cited on page 5.
* [45] D. Sanz-Alonso and R. Yang. Unlabeled Data Help in Graph-Based Semi-Supervised Learning: A Bayesian Nonparametrics Perspective. _Journal of Machine Learning Research_, 2022. Cited on page 4.
* [46] L. Schwartz. On Bayes procedures. _Zeitschrift fur Wahrscheinlichkeitstheorie und Verwandte Gebiete_, 1965. Cited on page 4.
* [47] M. L. Stein. _Interpolation of Spatial Data: Some Theory for Kriging_. Springer, 1999. Cited on page 3.
* [48] R. S. Strichartz. Analysis of the Laplacian on the complete Riemannian manifold. _Journal of functional analysis_, 52(1):48-79, 1983. Cited on page 14.
* [49] W. Tang, L. Zhang, and S. Banerjee. On identifiability and consistency of the nugget in Gaussian spatial process models. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 2021. Cited on page 4.
* [50] H. Triebel. _Theory of Function Spaces I_. Birkhauser, 1983. Cited on pages 15, 27.
* [51] H. Triebel. _Theory of Function Spaces II_. Birkhauser, 1992. Cited on pages 14, 15.
* [52] A. B. Tsybakov. _Introduction to Nonparametric Estimation_. Springer, 2008. Cited on pages 4, 8.
* [53] F. van der Meulen, M. Schauer, and H. van Zanten. Reversible jump MCMC for nonparametric drift estimation for diffusion processes. _Computational Statistics and Data Analysis_, 2014. Cited on page 6.
* [54] A. van der Vaart and H. van Zanten. Adaptive Bayesian estimation using a Gaussian random field with inverse Gamma bandwidth. _The Annals of Statistics_, 2009. Cited on page 4.
* [55] A. van der Vaart and H. van Zanten. Bayesian inference with rescaled Gaussian process priors. _Electronic Journal of Statistics_, 2007. Cited on page 3.
* [56] A. van der Vaart and H. van Zanten. Information Rates of Nonparametric Gaussian Process Methods. _Journal of Machine Learning Research_, 2011. Cited on pages 2-6, 17-22, 30, 31.
* [57] A. van der Vaart and H. van Zanten. Rates of contraction of posterior distributions based on Gaussian process priors. _The Annals of Statistics_, 2008. Cited on pages 4, 5, 17.

* [58] A. van der Vaart and J. Wellner. _Weak Convergence and Empirical Processes_. Springer Series in Statistics. Springer Verlag, 1996. Cited on page 22.
* [59] H. van Zanten and A. van der Vaart. Reproducing kernel Hilbert spaces of Gaussian priors. _Institute of Mathematical Statistics Collections_, 2008. Cited on pages 17, 30, 32.
* [60] X. Wang and G. Lerman. Nonparametric Bayesian regression on manifolds via Brownian motion. _arXiv:1507.06710_, 2015. Cited on page 4.
* [61] H. Wendland. _Scattered Data Approximation_. Cambridge University Press, 2004. Cited on pages 7, 25.
* [62] P. Whittle. Stochastic Processes in Several Dimensions. _Bulletin of the International Statistical Institute_, 1963. Cited on page 5.
* [63] Y. Yang and D. B. Dunson. Bayesian manifold regression. _The Annals of Statistics_, 2016. Cited on pages 4, 7, 8.
* [64] W. W. Yoo, V. Rivoirard, and J. Rousseau. Adaptive supremum norm posterior contraction: Wavelet spike-and-slab and anisotropic Besov spaces. _arXiv:1708.01909_, 2017. Cited on page 6.

Preliminaries

Here we describe preliminaries necessary for Appendices B to E. This includes some basic properties of the Laplace-Beltrami operator on compact manifolds, partitions of unity subordinate to atlases, function spaces such as Holder, Sobolev and Besov spaces, general Gaussian random elements on Banach spaces, and a certain technical lemma. Hereinafter, the expression \(a\lesssim b\) means \(a\leq Cb\) for some constant \(C>0\) whose value is irrelevant to our claims.

### Laplace-Beltrami Operator and Subordinate Partitions of Unity

Let \(\mathcal{M}\) denote a compact connected Riemannian manifold without boundary of dimension \(d\in\mathbb{Z}_{>0}\). The Laplace-Beltrami operator \(\Delta\) on \(\mathcal{M}\) is self-adjoint and positive semi-definite [48, Theorem 2.4]. Let \((L^{2}(\mathcal{M}),\langle\cdot,\cdot\rangle_{L^{2}(\mathcal{M})})\) denote the Hilbert space of square integrable (equivalence classes of) functions on \(\mathcal{M}\) with respect to the standard Riemannian volume measure.

By standard theory [13, 23], there exists an orthonormal basis \(\left\{f_{j}\right\}_{j=0}^{\infty}\) of \(L^{2}(\mathcal{M})\) consisting of the eigenfunctions of \(\Delta\), namely \(\Delta f_{j}=-\lambda_{j}f_{j}\), with \(\lambda_{j}\geq 0\). We assume that the pairs \((\lambda_{j},f_{j})\) are sorted such that \(0=\lambda_{0}\leq\lambda_{j}\leq\lambda_{j+1}\). The growth of \(\lambda_{j}\) can be characterized as follows.

**Result 10** (Weyl's Law).: _There exists a constant \(C>0\) such that for all \(j\) large enough we have_

\[C^{-1}j^{2/d}\leq\lambda_{j}\leq Cj^{2/d}.\] (17)

Proof.: See Chavel [13], Chapter 1. 

Following De Vito et al. [16] and Grosse and Schneider [24] we fix a family \(\mathcal{T}=\left(\mathcal{U}_{l},\phi_{l},\chi_{l}\right)_{l=1}^{L}\), where \(L\in\mathbb{Z}_{>0}\), the local coordinates \(\phi_{l}:\mathcal{U}_{l}\subset\mathcal{M}\to\mathcal{V}_{l}=\phi_{l}( \mathcal{U}_{l})\subset\mathbb{R}^{d}\) are smooth diffeomorphisms, and the functions \(\chi_{l}\) form a partition of unity subordinate to \(\left\{\mathcal{U}_{l}\right\}_{l=1}^{L}\), that is \(\chi_{l}\in\mathcal{C}^{\infty}(\mathcal{M})\), \(\operatorname{supp}(\chi_{l})\subset\mathcal{U}_{l},0\leq\chi_{l}\leq 1\) and \(\sum_{l}\chi_{l}=1\)--here, we can choose \(L\) finite by compactness of \(\mathcal{M}\). For convenience and without loss of generality, we assume that \(\mathcal{V}_{l}=(0,1)^{d}\). With this, we can start defining function spaces on \(\mathcal{M}\).

Holder Spaces \(\mathcal{CH}^{\gamma}\) and the spaces of continuous functions \(\mathcal{C}\) and smooth functions \(\mathcal{C}^{\infty}\)

Consider an arbitrary domain \(\mathcal{X}\subseteq\mathbb{R}^{d}\) or \(\mathcal{X}\subseteq\mathcal{M}\). We denote the class of infinitely differentiable functions on \(\mathcal{X}\) by \(\mathcal{C}^{\infty}(\mathcal{X})\). Let \(\mathcal{C}^{k}(\mathcal{X})\) denote the Banach space of \(k\in\mathbb{Z}_{>0}\) times continuously differentiable functions on \(\mathcal{X}\) with finite norm

\[\|f\|_{\mathcal{C}^{k}(\mathcal{X})}=\sup_{x\in\mathcal{X}}\bigl{|}\nabla^{k} f(x)\bigr{|}\] (18)

where \(\nabla^{k}\) is the \(k\)th covariant derivative, as in Hebey [25], Section 2.1 and Aubin [3], Definition 2.2. We also write \(\mathcal{C}(\mathcal{X})=\mathcal{C}^{0}(\mathcal{X})\) for the space of continuous functions on \(\mathcal{X}\).

The Euclidean Holder spaces \(\mathcal{CH}^{\gamma}(\mathbb{R}^{d})\), where \(\gamma=k+\alpha\) with \(k\in\mathbb{Z}_{\geq 0}\) and \(0<\alpha\leq 1\), are defined by3

Footnote 3: HÃ¶lder spaces are often also denoted by \(\mathcal{C}^{\gamma}\) as well, with \(\gamma\in\mathbb{R}_{>0}\). Since, using this formulation, they do _not_ coincide with \(\mathcal{C}^{k}(\mathcal{X})\) when \(k=\gamma\in\mathbb{Z}_{\geq 0}\), we use the notation \(\mathcal{CH}^{\gamma}\) to avoid confusion.

\[\mathcal{CH}^{\gamma}(\mathbb{R}^{d})=\left\{f\in\mathcal{C}^{k}( \mathbb{R}^{d}):\|f\|_{\mathcal{C}^{\mathcal{H}^{\gamma}}(\mathbb{R}^{d})}< \infty\right\}\] (19)

where

\[\|f\|_{\mathcal{CH}^{\gamma}(\mathbb{R}^{d})}=\|f\|_{\mathcal{C}^{k}( \mathbb{R}^{d})}+\sup_{\bm{x},\bm{y}\in\mathbb{R}^{d},\,\bm{x}\neq\bm{y}}\frac {|f(\bm{x})-f(\bm{y})|}{\|\bm{x}-\bm{y}\|_{\mathbb{R}^{d}}^{\alpha}}.\] (20)

More information on these definitions may be found, for instance, in Gine and Nickl [21] and Triebel [51]. We now turn to the manifold versions of the Holder spaces.

**Definition 11** (Holder spaces).: _For all \(\gamma>0\) we define the Holder space \(\mathcal{CH}^{\gamma}(\mathcal{M})\) on the manifold \(\mathcal{M}\) to be the space of all \(f:\mathcal{M}\to\mathbb{R}\) satisfying_

\[\|f\|_{\mathcal{CH}^{\gamma}(\mathcal{M})}=\sum_{l=1}^{L}\bigl{\|}(\chi_{l}f) \circ\phi_{l}^{-1}\bigr{\|}_{\mathcal{CH}^{\gamma}(\mathbb{R}^{q})}<\infty.\] (21)

Since the charts \(\phi_{l}\) are smooth, Definition 11 can be easily seen to be independent of the chosen atlas, with equivalence of norms.

### Sobolev and Besov Spaces

We now introduce the manifold versions of the Sobolev and Besov spaces, whose definitions in the standard Euclidean case may be found, for instance, in Triebel [51]. For Sobolev spaces we use the Bessel-potential-based definition, following De Vito et al. [16].

**Definition 12** (Sobolev spaces).: _For any \(s>0\) we define the Sobolev space \(H^{s}(\mathcal{M})\) on the manifold \(\mathcal{M}\) as the Hilbert space of functions \(f\in L^{2}(\mathcal{M})\) such that \(\|f\|_{H^{s}(\mathcal{M})}^{2}=\langle f,f\rangle_{H^{s}(\mathcal{M})}<\infty\) where_

\[\langle f,g\rangle_{H^{s}(\mathcal{M})}=\sum_{j=0}^{\infty}(1+\lambda_{j})^{s }\langle f,f_{j}\rangle_{L^{2}(\mathcal{M})}\langle g,f_{j}\rangle_{L^{2}( \mathcal{M})}.\] (22)

**Remark 13**.: _It is easy to see that substituting \((1+\lambda_{j})^{s}\) in (22) with \(\beta(\alpha+\lambda_{j})^{s}\) or with \((\alpha+\beta\lambda_{j}^{s})\) for any \(\alpha,\beta>0\) results in the same set of functions and an equivalent norm. The former follows from Borovitskiy et al. [11], eq. (109). The latter follows from the Binomial Theorem._

For Besov spaces we follow Coulhon et al. [14] and Castillo et al. [12] and define them in terms of approximations by low-frequency functions. We fix a function \(\Phi\in\mathcal{C}^{\infty}(\mathbb{R}_{\geq 0},\mathbb{R}_{\geq 0})\) such that \(K=\mathrm{supp}(\Phi)\subset[0,2]\) and \(\Phi(x)=1\) for \(x\in[0,1]\). We also define the functions \(\Phi_{j}(x)=\Phi\bigl{(}2^{-j}x\bigr{)}\).

Coulhon et al. [14], Corollary 3.6 shows that the operators \(\Phi_{j}(\sqrt{\Delta})\) defined by

\[\Phi_{j}\Bigl{(}\sqrt{\Delta}\Bigr{)}f=\sum_{j\geq 0}\Phi_{j}\Bigl{(}\sqrt{ \lambda_{j}}\Bigr{)}\langle f_{j},f\rangle f_{j}\] (23)

are bounded in the space \(L^{p}(\mathcal{M})\) for all \(1\leq p\leq\infty\).4 Moreover, the same result also shows that we can express any \(f\in L^{p}(\mathcal{M})\) as \(f=\lim_{j\to\infty}\Phi_{j}(\sqrt{\Delta})f\) in \(L^{p}(\mathcal{M})\). \(\Phi_{j}(\sqrt{\Delta})f\) can intuitively be considered as a version of \(f\) filtered by a low-pass filter. The next definition introduces the Besov spaces \(B^{s}_{p,q}(\mathcal{M})\), which are formulated in terms of quality-of-approximation by low-frequency functions.

Footnote 4: The space \(L^{p}(\mathcal{M})\) is the Banach space of functions (or rather their equivalence classes) that are integrable when raised to the power \(p<\infty\) or essentially bounded for \(p=\infty\). See for instance Triebel [50] for details.

**Definition 14** (Besov spaces).: _For any \(s>0\) and \(1\leq p,q\leq\infty\) we define the Besov space \(B^{s}_{p,q}(\mathcal{M})\) on the manifold \(\mathcal{M}\) as the space of functions \(f\in L^{p}(\mathcal{M})\) such that \(\|f\|_{B^{s}_{p,q}(\mathcal{M})}<\infty\) where_

\[\|f\|_{B^{s}_{p,q}(\mathcal{M})}=\begin{cases}\|f\|_{L^{p}(\mathcal{M})}+ \Bigl{(}\sum_{j\geq 0}\Bigl{(}2^{js}\|\Phi_{j}(\sqrt{\Delta})f-f\|_{L^{p}( \mathcal{M})}\Bigr{)}^{q}\Bigr{)}^{1/q}&\text{if }q<+\infty\\ \|f\|_{L^{p}(\mathcal{M})}+\sup_{j\geq 0}2^{js}\|\Phi_{j}(\sqrt{\Delta})f-f\|_{L ^{p}(\mathcal{M})}&\text{if }q=+\infty.\end{cases}\] (24)

The classical Besov spaces \(B^{s}_{2,2}\) coincide with the Sobolev spaces \(H^{s}\) on \(\mathbb{R}^{d}\), in the sense that they define the same set of functions and equivalent norms--see for instance Gine and Nickl [21] section 4.3.6--and even on manifolds if one follows the construction of Triebel [51], pages 7.3-7.4 for Besov spaces. Since our definition of the Besov spaces is somewhat non-standard, we present a proof.

**Proposition 15**.: _For all \(s>0\), \(H^{s}(\mathcal{M})=B^{s}_{2,2}(\mathcal{M})\) as sets and there exist two constants \(C_{1},C_{2}>0\) such that for all \(f\in H^{s}(\mathcal{M})=B^{s}_{2,2}(\mathcal{M})\) we have_

\[C_{1}\|f\|_{H^{s}(\mathcal{M})}\leq\|f\|_{B^{s}_{2,2}(\mathcal{M})}\leq C_{2}\| f\|_{H^{s}(\mathcal{M})}.\] (25)Proof.: It is enough to prove (25), the rest will follow automatically. The main technical tools used in the proof are Result 10 and summation by parts. First, we prove the upper bound. Denote the support set of \(\Phi\) by \(K=\operatorname{supp}(\Phi)\). Notice that

\[\big{(}\|f\|_{B^{s}_{2,2}(\mathcal{M})}-\|f\|_{L^{2}(\mathcal{M})} \big{)}^{2}= \sum_{j\geq 0}2^{2js}\big{\|}\Phi_{j}\Big{(}\sqrt{\Delta}\Big{)}f-f \big{\|}_{L^{2}(\mathcal{M})}^{2}\] (26) \[= \sum_{j\geq 0}2^{2js}\sum_{l:\sqrt{\lambda_{l}}\not\in\mathbb{Z}^{2j }K}\big{|}\langle f_{l},f\rangle_{L^{2}(\mathcal{M})}\big{|}^{2}\] (27) \[\leq \sum_{j\geq 0}2^{2js}\sum_{l:\sqrt{\lambda_{l}}>2^{j}}\big{|}\langle f _{l},f\rangle_{L^{2}(\mathcal{M})}\big{|}^{2}.\] (28)

The last inequality results from the fact that \([0,1]\subset K\). By Weyl's Law (Result 10), there exists a constant \(C>0\) such that \(\lambda_{l}\leq Cl^{2/d}\). Without loss of generality, we can assume that \(C=2^{2r}\), \(r\in\mathbb{Z}_{>0}\). Hence \(\sqrt{\lambda_{l}}>2^{j}\) implies \(l>2^{d(j-r)}\), thus we have

\[\big{(}\|f\|_{B^{s}_{2,2}(\mathcal{M})}-\|f\|_{L^{2}(\mathcal{M}) }\big{)}^{2}\leq \sum_{j\geq 0}2^{2js}\sum_{l>2^{d(j-r)}}\big{|}\langle f_{l},f \rangle_{L^{2}(\mathcal{M})}\big{|}^{2}\] (29) \[= \sum_{0\leq j\leq r}2^{2js}\sum_{l>2^{d(j-r)}}\big{|}\langle f_{l },f\rangle_{L^{2}(\mathcal{M})}\big{|}^{2}+\sum_{j>r}2^{2js}\sum_{l>2^{d(j-r)} }\big{|}\langle f_{l},f\rangle_{L^{2}(\mathcal{M})}\big{|}^{2}\] (30) \[\leq r2^{2rs}\|f\|_{L^{2}(\mathcal{M})}^{2}+2^{2rs}\sum_{j>0}2^{2js} \sum_{l>2^{d(j}}\big{|}\langle f_{l},f\rangle_{L^{2}(\mathcal{M})}\big{|}^{2}.\] (31)

Now let \(R_{j}=\sum_{l>2^{d}}\big{|}\langle f_{l},f\rangle_{L^{2}(\mathcal{M})}\big{|} ^{2}\) and \(S_{J}=\sum_{j=1}^{J}2^{2js}\leq\frac{2^{2s}}{2^{2s}-1}2^{2Js},S_{0}=0\). Write

\[\sum_{j>0}2^{2js}\sum_{l>2^{d}}\big{|}\langle f_{l},f\rangle_{L^ {2}(\mathcal{M})}\big{|}^{2}= \sum_{j>0}(S_{j}-S_{j-1})R_{j}\] (32) \[= \sum_{j>0}S_{j}(R_{j}-R_{j+1})\] (33) \[= \sum_{j>0}S_{j}\sum_{2^{d_{j}}<l\leq 2^{d(j+1)}}\big{|}\langle f _{l},f\rangle_{L^{2}(\mathcal{M})}\big{|}^{2}\] (34) \[\leq \frac{2^{2s}}{2^{2s}-1}\sum_{j>0}2^{2js}\sum_{2^{d_{j}}<l\leq 2 ^{d(j+1)}}\big{|}\langle f_{l},f\rangle_{L^{2}(\mathcal{M})}\big{|}^{2}\] (35) \[\leq \frac{2^{2s}}{2^{2s}-1}\sum_{j>0}\ \ \sum_{2^{d_{j}}<l\leq 2^{d(j+1)}}l ^{2s/d}\big{|}\langle f_{l},f\rangle_{L^{2}(\mathcal{M})}\big{|}^{2}\] (36) \[\leq \frac{c^{s}2^{2s}}{2^{2s}-1}\sum_{j>0}\ \ \sum_{2^{d_{j}}<l\leq 2 ^{d(j+1)}}\lambda_{l}^{s}\big{|}\langle f_{l},f\rangle_{L^{2}(\mathcal{M})} \big{|}^{2}\] (37) \[= \frac{c^{s}2^{2s}}{2^{2s}-1}\sum_{l>2^{d}}\lambda_{l}^{s}\big{|} \langle f_{l},f\rangle_{L^{2}(\mathcal{M})}\big{|}^{2}\] (38) \[\leq \frac{c^{s}2^{2s}}{2^{2s}-1}\sum_{l\geq 0}\lambda_{l}^{s}\big{|} \langle f_{l},f\rangle_{L^{2}(\mathcal{M})}\big{|}^{2}\] (39)

where we have used Result 10 to get existence of a \(c\) such that \(l^{2/d}\leq c\lambda_{l}\). This proves the upper bound. The proof for the lower bound is similar. 

Proposition 15 provides a characterization of the Sobolev spaces \(H^{s}(\mathcal{M})\). There is yet another characterization of these spaces that will be useful later, in terms of charts. We present this characterization as part of the following result.

**Theorem 16**.: _On the Sobolev space \(H^{s}(\mathcal{M})\), the following norms are equivalent:_

\[\left\|f\right\|_{H^{s}(\mathcal{M})} =\left(\sum_{j=0}^{\infty}(1+\lambda_{j})^{s}\langle f,f_{j}\rangle _{L^{2}(\mathcal{M})}^{2}\right)^{1/2}\] (40) \[\left\|f\right\|_{B^{s}_{2,2}(\mathcal{M})} =\left\|f\right\|_{L^{2}(\mathcal{M})}+\left(\sum_{j\geq 0}\Bigl{(}2^ {js}\|\Phi_{j}(\sqrt{\Delta})f-f\|_{L^{2}(\mathcal{M})}\Bigr{)}^{2}\right)^{1/2}\] (41) \[\left\|f\right\|_{H^{\pi}_{\mathcal{T}}(\mathcal{M})} =\left(\sum_{l=1}^{L}\bigl{\|}(\chi_{l}f)\circ\phi_{l}^{-1}\bigr{\|} _{H^{s}(\mathbb{R}^{d})}^{2}\right)^{1/2}\] (42)

Proof.: The equivalence between \(\left\|\cdot\right\|_{H^{s}(\mathcal{M})}\) and \(\left\|f\right\|_{B^{s}_{2,2}(\mathcal{M})}\) is given by Proposition 15. The proof of equivalence between \(\left\|\cdot\right\|_{H^{s}(\mathcal{M})}\) and \(\left\|f\right\|_{H^{s}_{\mathcal{T}}(\mathcal{M})}\) can be found in De Vito et al. [16]. 

### Gaussian Random Elements

Here we recall the definition of a Gaussian process as a Banach-space-valued random variable, following for instance van Zanten and van der Vaart [59].

**Definition 17** (Gaussian random element).: _Let \((\mathbb{B},\left\|\cdot\right\|_{\mathbb{B}})\) be a Banach space, and \(f\) be a Borel random variable with values in \(\mathbb{B}\) almost surely. We say that \(f\) is a Gaussian random element if \(b^{*}(f)\) is a univariate Gaussian random variable for every bounded linear functional \(b^{*}:\mathbb{B}\to\mathbb{R}\)._

Random variables of this kind are also sometimes called _Gaussian in the sense of duality_. One should think of a Gaussian random element as a generalization of a Gaussian process, but which is better-behaved from a function-analytic point of view and in particular does not require the process to be an actual function--as opposed to, for instance, a measure or a distribution. Many connections between the usual Gaussian processes and Gaussian random elements exist, see Lifshits [32], Ghosal and van der Vaart [20], Appendix I, van der Vaart and van Zanten [57] for details. The following observation about Gaussian random elements will be useful later.

**Lemma 18**.: _A Gaussian process \(f\) on the manifold \(\mathcal{M}\) with almost surely continuous sample paths is a Gaussian random element in the Banach space \(\Bigl{(}\mathcal{C}(\mathcal{M}),\left\|\cdot\right\|_{\mathcal{C}(\mathcal{ M})}\Bigr{)}\) of continuous functions on \(\mathcal{M}\)._

Proof.: Since \(\mathcal{C}(\mathcal{M})\) is separable, this follows from Lemma I.6 in Ghosal and van der Vaart [20]. 

### A Technical Lemma

In order to apply Bernstein's inequality when going from the error at input locations to the \(L^{2}(p_{0})\) error, we will use the following technical extrapolation lemma.

**Lemma 19**.: _For any function \(g:\mathcal{M}\to\mathbb{R}\), a number \(\gamma\in\mathbb{R}_{>0}\setminus\mathbb{Z}_{>0}\) and a density \(p_{0}:\mathcal{M}\to\mathbb{R}_{>0}\) with \(1\lesssim p_{0}\), we have_

\[\left\|g\right\|_{L^{\infty}(\mathcal{M})}\lesssim\left\|g\right\|_{\mathcal{C} \mathcal{H}^{s}(\mathcal{M})}^{\frac{d}{2+\gamma d}}\left\|g\right\|_{L^{2}(p _{0})}^{\frac{2\gamma}{2+\gamma d}}.\] (43)

Proof.: We use Lemma 15 from van der Vaart and van Zanten [56] and push it through charts. More precisely we have, using \(B^{\gamma}_{\infty,\infty}\bigl{(}[0,1]^{D}\bigr{)}=\mathcal{C}\mathcal{H}^{ \gamma}\bigl{(}[0,1]^{D}\bigr{)}\) for \(\gamma\notin\mathbb{Z}_{>0}\), that

\[\left\|g\right\|_{L^{\infty}(\mathcal{M})} \leq\sum_{l}\bigl{\|}(\chi_{l}g)\circ\phi_{l}^{-1}\bigr{\|}_{L^{ \infty}(\mathcal{V}_{l})}\] (44) \[\lesssim\max_{l}\bigl{\|}(\chi_{l}g)\circ\phi_{l}^{-1}\bigr{\|}_ {\mathcal{C}\mathcal{H}^{\gamma}(\mathcal{V}_{l})}^{\frac{d}{2+\gamma d}} \bigl{\|}(\chi_{l}g)\circ\phi_{l}^{-1}\bigr{\|}_{L^{2}(\mathcal{V}_{l})}^{ \frac{2\gamma}{2+\gamma d}}.\] (45)

By definition of the the manifold Holder spaces, this gives

\[\left\|g\right\|_{L^{\infty}(\mathcal{M})}\lesssim\left\|g\right\|_{\mathcal{ C}\mathcal{H}^{\gamma}(\mathcal{M})}^{\frac{d}{2+\gamma d}}\max_{l}\bigl{\|}( \chi_{l}g)\circ\phi_{l}^{-1}\bigr{\|}_{L^{2}(\mathcal{V}_{l})}^{\frac{2\gamma} {2+\delta}}.\] (46)

[MISSING_PAGE_FAIL:18]

Result 10 (Weyl's Law) and properties of \(\Phi\), we have

\[\|f\|_{\mathbb{H}_{n}}^{2} \lesssim\|f\|_{H^{\nu+d/2}(\mathcal{M})}^{2}\] (52) \[=\sum_{l\geq 0}(1+\lambda_{l})^{\nu+d/2}\Phi^{2}\Big{(}2^{-j} \sqrt{\lambda_{l}}\Big{)}\Big{|}\langle f_{l},f_{0}\rangle_{L^{2}(\mathcal{M}) }\Big{|}^{2}\] (53) \[\leq\sum_{l:\sqrt{\lambda_{l}}\leq 2^{j+1}}(1+\lambda_{l})^{\nu+d /2-\beta}(1+\lambda_{l})^{\beta}\Big{|}\langle f_{l},f_{0}\rangle_{L^{2}( \mathcal{M})}\Big{|}^{2}\] (54) \[\leq 2^{(j+2)(\nu+d/2-\beta)}\sum_{l:\sqrt{\lambda_{l}}\leq 2^{j+ 1}}(1+\lambda_{l})^{\beta}\Big{|}\langle f_{l},f_{0}\rangle_{L^{2}(\mathcal{M}) }\Big{|}^{2}\] (55) \[\leq 2^{(j+2)(\nu+d/2-\beta)}\sum_{l\geq 0}(1+\lambda_{l})^{ \beta}\Big{|}\langle f_{l},f_{0}\rangle_{L^{2}(\mathcal{M})}\Big{|}^{2}\] (56) \[=2^{(j+2)(\nu+d/2-\beta)}\|f_{0}\|_{H^{\beta}(\mathcal{M})}^{2}\] (57) \[\lesssim\varepsilon^{-\frac{1}{\beta}(\nu+d/2-\beta)}\|f_{0}\|_{ H^{\beta}(\mathcal{M})}^{2}\lesssim\varepsilon^{-\frac{2}{\beta}(\nu+d/2-\beta)} \|f_{0}\|_{H^{\beta}(\mathcal{M})}^{2}.\] (58)

Our assumption \(\nu\geq\beta\) implies that

\[\frac{2}{\beta}(\nu-\beta+d/2)\geq\frac{d}{\beta}\geq\frac{d}{\nu}.\] (59)

Hence, we have \(\varepsilon^{-d/\nu}\leq\varepsilon^{-\frac{2}{\beta}(\nu-\beta+d/2)}\) which gives us \(\varphi_{f_{0}}(\varepsilon)\lesssim\varepsilon^{-\frac{2}{\beta}(\nu-\beta+d /2)}\). It is then easy to check that \(\varepsilon_{n}=Mn^{-\frac{\beta}{2\nu+d}}\) satisfies \(\varphi_{f_{0}}(\varepsilon_{n})\leq n\varepsilon_{n}^{2}\) for \(M>0\) large enough. 

From this, we deduce an upper bound on the error in _the empirical \(L^{2}\) norm_\(\left\|\cdot\right\|_{n}\), that is, on the Euclidean distance between the posterior Gaussian process \(f\) and the ground truth function \(f_{0}\) evaluated at data locations \(x_{i}\).

**Lemma 22**.: _Let \(\Pi_{n}\) denote the prior in either Theorem 5, Theorem 6 or Theorem 8 with smoothness parameter \(\nu>d/2\). Fix \(f_{0}\in H^{\beta}(\mathcal{M})\cap B_{\infty,\infty}^{\beta}(\mathcal{M})\) with \(\beta>0\). Then_

\[\mathbb{E}_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\|f-f_{0}\|_{n}^{2}\leq \varepsilon_{n}^{2}\] (60)

_for \(\varepsilon_{n}\propto n^{-\frac{\min(\nu,\beta)}{2\nu+d}}\) with constant depending on \(f_{0},\nu\) but not on \(\bm{x}\)._

Proof.: By Theorem 21 for \(\varepsilon_{n}\) a multiple of \(n^{-\frac{\min(\beta,\nu)}{2\nu+d}}\), we have \(\varphi_{f_{0}}(\varepsilon_{n})\leq n\varepsilon_{n}^{2}\). By virtue of this, the proof of Theorem 1 and Proposition 11 of van der Vaart and van Zanten [56] imply the result. Indeed, the proof of Theorem 1 relies solely on the fact that \(\varphi_{f_{0}}(\varepsilon_{n}/2)\leq n\varepsilon_{n}^{2}\) and an application of van der Vaart and van Zanten [56], Proposition 11. We have \(\varphi_{f_{0}}(\varepsilon_{n})\leq n\varepsilon_{n}^{2}\leq n(2\varepsilon_ {n})^{2}\) and hence the condition is satisfied with \(\varepsilon_{n}\) replaced by \(2\varepsilon_{n}\). 

We now turn to the proofs of our main results, Theorems 5, 6 and 8, which for convenience we restate below. The idea of these proofs is to extend the result of Lemma 22 from input locations to the whole manifold \(\mathcal{M}\) using an appropriate concentration inequality. To this end, the proof closely follows the one of Theorem 2 in van der Vaart and van Zanten [56], but relies on Lemma 22 proved above and on the concentration inequality we prove in Appendix D along with some important sample differentiablity properties of the prior processes.

**Theorem 5**.: _Let \(f\) be a Riemannian Matern Gaussian process prior of Definition 4 with smoothness parameter \(\nu>d/2\) and let \(f_{0}\) satisfy Assumption 3. Then there is a \(C>0\) such that_

\[\mathbb{E}_{\bm{x},\bm{y}}\,\mathbb{E}_{f\sim\Pi(\cdot|\bm{x},\bm{y})}\|f-f_{ 0}\|_{L^{2}(p_{0})}^{2}\leq Cn^{-\frac{2\min(\beta,\nu)}{2\nu+d}}.\] (9)

**Theorem 6**.: _Let \(f\) be a Riemannian Matern Gaussian process prior on \(\mathcal{M}\) with smoothness parameter \(\nu>d/2\), modified to truncate the infinite sum to at least \(J_{n}\geq cn^{\frac{d(\min(1,\nu/\beta))}{2\nu+d}}\) terms, and let \(f_{0}\) satisfy Assumption 3. Then there is a \(C>0\) such that_

\[\mathbb{E}_{\bm{x},\bm{y}}\,\mathbb{E}_{f\sim\Pi(\cdot|\bm{x},\bm{y})}\|f-f_{ 0}\|_{L^{2}(p_{0})}^{2}\leq Cn^{-\frac{2\min(\beta,\nu)}{2\nu+d}}.\] (10)

**Theorem 8**.: _Let \(f\) be a mean-zero extrinsic Matern Gaussian process prior with smoothness parameter \(\nu>d/2\) on \(\mathcal{M}\), and let \(f_{0}\) satisfy Assumption 3. Then for some \(C>0\) we have_

\[\mathbb{E}_{\bm{x},\bm{y}}\,\mathbb{E}_{f\sim\Pi(\cdot|\bm{x},\bm{y})}\|f-f_{0} \|_{L^{2}(p_{0})}^{2}\leq Cn^{-\frac{2\min(\beta,\nu)}{2\nu+d}}.\] (12)

Proof of Theorems 5, 6 and 8.: To ease notation, we denote the expectations under the true data generating process by \(\mathbb{E}_{\bm{x},\bm{y}}=\mathbb{E}_{\bm{x},\bm{y}}^{(f_{0})}\) and \(\mathbb{E}_{\bm{y}|\bm{x}}=\mathbb{E}_{\bm{y}|\bm{x}}^{(f_{0})}\), omitting the superscript \((\cdot)^{f_{0}}\). Take \(\varepsilon_{n}\propto n^{-\frac{\min(\beta,\nu)}{2\nu+d}}\) satisfying \(\varphi_{f_{0}}(\varepsilon_{n}/2)\leq n\varepsilon_{n}^{2}\), noting that such a rate exists by Theorem 21. Then, for each \(n\), there exists an element \(f_{n}\in\mathbb{H}_{n}\), where \(\mathbb{H}_{n}\) is the RKHS corresponding to \(\Pi_{n}\), satisfying

\[\left\|f_{n}\right\|_{\mathbb{H}_{n}}^{2}\leq n\varepsilon_{n}^{2} \left\|f_{n}-f_{0}\right\|_{L^{\infty}(\mathcal{M})}\leq\varepsilon_{n}/2.\] (61)

Write

\[\varepsilon_{n}^{-2}\,\mathbb{E}_{\bm{x},\bm{y}}\,\mathbb{E}_{f \sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\|f-f_{0}\|_{L^{2}(p_{0})}^{2} \lesssim\varepsilon_{n}^{-2}\,\mathbb{E}_{\bm{x},\bm{y}}\, \mathbb{E}_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\|f_{n}-f_{0}\|_{L^{2}(p_{0})}^ {2}\] (62) \[\quad+\varepsilon_{n}^{-2}\,\mathbb{E}_{\bm{x},\bm{y}}\,\mathbb{E }_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\|f-f_{n}\|_{L^{2}(p_{0})}^{2}\] (63) \[\lesssim 1+\varepsilon_{n}^{-2}\,\mathbb{E}_{\bm{x},\bm{y}}\, \mathbb{E}_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\|f-f_{n}\|_{L^{2}(p_{0})}^{2}.\] (64)

Thus, we can work with \(f-f_{n}\) instead of \(f-f_{0}\). Define \(\mathcal{B}(r)=\left\{\left\|f-f_{n}\right\|_{L^{2}(p_{0})}>\varepsilon_{n}r\right\}\). Then

\[\varepsilon_{n}^{-2}\,\mathbb{E}_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\|f-f_{n} \|_{L^{2}(p_{0})}^{2}.=\int_{0}^{\infty}2r\,\mathbb{P}_{f\sim\Pi_{n}(\cdot|\bm{ x},\bm{y})}(\mathcal{B}(r))\,\mathrm{d}r.\] (65)

Fix a \(\gamma\) such that \(d/2<\gamma<\nu,\gamma\notin\mathbb{Z}_{>0}\) and \(s>0,\tau>0\) and define

\[\mathcal{B}^{(\mathrm{I})}(r) =\left\{2\|f-f_{n}\|_{n}>\varepsilon_{n}r\right\}\] (66) \[\mathcal{B}^{(\mathrm{II})}(r) =\left\{\left\|f\right\|_{\mathcal{CH}^{\gamma}(\mathcal{M})}> \tau\sqrt{n}\varepsilon_{n}r^{s}\right\}\] (67) \[\mathcal{B}^{(\mathrm{III})}(r) =\mathcal{B}(r)\setminus\Big{(}\mathcal{B}^{(\mathrm{I})}(r) \cup\mathcal{B}^{(\mathrm{II})}(r)\Big{)}.\] (68)

Then \(\mathcal{B}(r)\subseteq\mathcal{B}^{(\mathrm{I})}(r)\cup\mathcal{B}^{( \mathrm{II})}(r)\cup\mathcal{B}^{(\mathrm{III})}(r)\), and thus for an indexed family of events \(\mathcal{A}_{r}\) to be chosen later, we have

\[\varepsilon_{n}^{-2}\,\mathbb{E}_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y })}\|f-f_{n}\|_{L^{2}(p_{0})}^{2}\lesssim\int_{0}^{\infty}r\,\mathbb{P}_{f\sim \Pi_{n}(\cdot|\bm{x},\bm{y})}\Big{(}\mathcal{B}^{(\mathrm{I})}(r)\Big{)}\, \mathrm{d}r+\int_{0}^{\infty}r\mathbb{1}_{\mathcal{A}_{r}^{c}}\,\mathrm{d}r\] (69) \[\quad+\int_{0}^{\infty}r\mathbb{1}_{\mathcal{A}_{r}}\,\mathbb{P} _{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\Big{(}\mathcal{B}^{(\mathrm{II})}(r) \Big{)}\,\mathrm{d}r+\int_{0}^{\infty}r\mathbb{1}_{\mathcal{A}_{r}}\,\mathbb{P }_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\Big{(}\mathcal{B}^{(\mathrm{III})}(r) \Big{)}\,\mathrm{d}r.\] (70)

For the first term, by Lemma 22 applied conditionally on \(\bm{x}\), for which we got a bound on the integrated empirical \(L^{2}\)-norm uniformly on the design points, we have

\[\mathbb{E}_{\bm{x},\bm{y}}\int_{0}^{\infty}r\,\mathbb{P}_{f\sim\Pi_{n}(\cdot| \bm{x},\bm{y})}\Big{(}\mathcal{B}^{(\mathrm{I})}(r)\Big{)}\,\mathrm{d}r \lesssim\varepsilon_{n}^{-2}\,\mathbb{E}_{\bm{x},\bm{y}}\,\mathbb{E}_{f\sim\Pi _{n}(\cdot|\bm{x},\bm{y})}\|f-f_{0}\|_{n}^{2}\lesssim\varepsilon_{n}^{-2} \varepsilon_{n}^{2}=1.\] (71)

Moreover, by Lemma 14 of van der Vaart and van Zanten [56] applied with \(r\) in the notation of the reference being equal to \(\sqrt{n}\varepsilon_{n}r^{s}\), for each \(r>0\), the event

\[\mathcal{A}_{r}(\bm{x}) =\left\{\bm{u}\in\mathbb{R}^{n}\!\!:\!\!\int\frac{p_{\bm{y}|\bm{x }}^{(f)}(\bm{u})}{p_{\bm{y}|\bm{x}}^{(f_{0})}(\bm{u})}\,\mathrm{d}\Pi_{n}(f) \geq e^{-n\varepsilon_{n}^{2}r^{2s}}\,\mathbb{P}_{f\sim\Pi_{n}}\Big{(}\|f\!-\!f _{0}\|_{L^{\infty}(\mathcal{M})}\!<\!\varepsilon_{n}r^{s}\!\Big{)}\!\right\}\] (72) \[\quad\geq\left\{\bm{u}\in\mathbb{R}^{n}\!\!:\!\!\!\int\frac{p_{\bm {y}|\bm{x}}^{(f)}(\bm{u})}{p_{\bm{y}|\bm{x}}^{(f_{0})}(\bm{u})}\,\mathrm{d}\Pi_{n}(f) \geq e^{-n\varepsilon_{n}^{2}r^{2s}}\,\mathbb{P}_{f\sim\Pi_{n}}(\|f\!-\!f_{0}\|_{n} \!<\!\varepsilon_{n}r^{s}\!)\!\right\}\] (73)

is such that

\[p_{\bm{y}|\bm{x}}^{(f_{0})}[\mathcal{A}_{r}^{c}(\bm{x})]\leq e^{-n\varepsilon_{ n}^{2}r^{2s}/8}.\] (74)It should be noted that the \(\sqrt{n}\) factor disappears because of the discrepancy between the empirical \(L^{2}\) norm \(\left\|\cdot\right\|_{n}\) and the Euclidean norm used in van der Vaart and van Zanten [56]. By Fubini's Theorem and since \(ne_{n}^{2}\geq n^{\frac{d}{2\nu+d}}\geq 1\), the second term is bounded by

\[\mathbb{E}_{\bm{x},\bm{y}}\int_{0}^{\infty}r\mathbb{1}_{\mathcal{A }^{s}_{r}(\bm{x})}\,\mathrm{d}r =\int_{0}^{\infty}r\,\mathbb{E}_{\bm{x}}\big{[}\mathbb{E}_{\bm{y} |\bm{x}}\big{[}\mathbb{1}_{\mathcal{A}^{s}_{r}(\bm{x})}\big{]}\big{]}\, \mathrm{d}r\] (75) \[\leq\int_{0}^{\infty}re^{-ne_{n}^{2}r^{2s}/8}\,\mathrm{d}r\] (76) \[\leq\int_{0}^{\infty}re^{-r^{2s}/8}\,\mathrm{d}r=C_{s}<\infty.\] (77)

It remains to bound the last two terms. By Bayes' Rule, we have the equality

\[\mathbb{P}_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\Big{(}\|f\|_{ \mathcal{C}\mathcal{H}^{\gamma}(\mathcal{M})}>\tau\sqrt{n}\varepsilon_{n}r^{ s}\Big{)} =\frac{\int_{\|f\|_{\mathcal{C}\mathcal{H}^{\gamma}(\mathcal{M}) }>\tau\sqrt{n}\varepsilon_{n}r^{s}}p_{\bm{y}|\bm{x}}^{(f)}(\bm{y})\,\mathrm{d} \Pi_{n}(f)}{\int p_{\bm{y}|\bm{x}}^{(f)}(\bm{y})\,\mathrm{d}\Pi_{n}(f)}\] (78) \[=\frac{\int_{\|f\|_{\mathcal{C}\mathcal{H}^{\gamma}(\mathcal{M}) }>\tau\sqrt{n}\varepsilon_{n}r^{s}}\frac{p_{\bm{y}|\bm{x}}^{(f)}(\bm{y})}{p_{ \bm{y}|\bm{x}}^{(f)}(\bm{y})}\,\mathrm{d}\Pi_{n}(f)}{\int\frac{p_{\bm{y}|\bm{x }}^{(f)}(\bm{y})}{p_{\bm{y}|\bm{x}}^{(f)}(\bm{y})}\,\mathrm{d}\Pi_{n}(f)}\] (79)

therefore for \(\bm{y}\in\mathcal{A}_{r}(\bm{x})\), we have

\[\mathbb{P}_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\Big{(}\|f\|_{ \mathcal{C}\mathcal{H}^{\gamma}(\mathcal{M})}>\tau\sqrt{n}\varepsilon_{n}r^{ s}\Big{)}\] (80) \[\leq\frac{e^{n\varepsilon_{n}^{2}r^{2s}}}{\mathbb{P}_{f\sim\Pi_{ n}}\Big{(}\|f-f_{0}\|_{L^{\infty}(\mathcal{M})}<\varepsilon_{n}r^{s}\Big{)}}\int_{ \|f\|_{\mathcal{C}\mathcal{H}^{\gamma}(\mathcal{M})}>\tau\sqrt{n}\varepsilon_ {n}r^{s}}\frac{p_{\bm{y}|\bm{x}}^{(f)}(\bm{y})}{p_{\bm{y}|\bm{x}}^{(f)}(\bm{y })}\,\mathrm{d}\Pi_{n}(f).\] (81)

Hence taking expectations, using Tonelli's Theorem, and \(\mathbb{E}_{\bm{x},\bm{y}}\frac{p_{\bm{y}|\bm{x}}^{(f)}(\bm{y})}{p_{\bm{y}| \bm{x}}^{(f)}(\bm{y})}=1\) gives

\[\mathbb{E}_{\bm{x},\bm{y}}\Big{[}\mathbb{1}_{\mathcal{A}_{r}( \bm{x})}\,\mathbb{P}_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\Big{(}\|f\|_{ \mathcal{C}\mathcal{H}^{\gamma}(\mathcal{M})}>\tau\sqrt{n}\varepsilon_{n}r^{ s}\Big{)}\Big{]}\] (82) \[\leq\frac{e^{n\varepsilon_{n}^{2}r^{2s}}}{\mathbb{P}_{f\sim\Pi_{ n}}\Big{(}\|f-f_{0}\|_{L^{\infty}(\mathcal{M})}<\varepsilon_{n}r^{s}\Big{)}}\, \mathbb{E}_{\bm{x},\bm{y}}\int_{\|f\|_{\mathcal{C}\mathcal{H}^{\gamma}( \mathcal{M})}>\tau\sqrt{n}\varepsilon_{n}r^{s}}\frac{p_{\bm{y}|\bm{x}}^{(f)}( \bm{y})}{p_{\bm{y}|\bm{x}}^{(f)}(\bm{y})}\,\mathrm{d}\Pi_{n}(f)\] (83) \[=\frac{e^{n\varepsilon_{n}^{2}r^{2s}}}{\mathbb{P}_{f\sim\Pi_{n}} \Big{(}\|f-f_{0}\|_{L^{\infty}(\mathcal{M})}<\varepsilon_{n}r^{s}\Big{)}}\int_{ \|f\|_{\mathcal{C}\mathcal{H}^{\gamma}(\mathcal{M})}>\tau\sqrt{n}\varepsilon_{ n}r^{s}}\mathbb{E}_{\bm{x},\bm{y}}\frac{p_{\bm{y}|\bm{x}}^{(f)}(\bm{y})}{p_{\bm{y}|\bm{x}}^{(f)}( \bm{y})}\,\mathrm{d}\Pi_{n}(f)\] (84) \[=\frac{e^{n\varepsilon_{n}^{2}r^{2s}}}{\mathbb{P}_{f\sim\Pi_{n}} \Big{(}\|f-f_{0}\|_{L^{\infty}(\mathcal{M})}<\varepsilon_{n}r^{s}\Big{)}}\, \mathbb{P}_{f\sim\Pi_{n}}\Big{(}\|f\|_{\mathcal{C}\mathcal{H}^{\gamma}( \mathcal{M})}>\tau\sqrt{n}\varepsilon_{n}r^{s}\Big{)}.\] (85)

Therefore, the third term can be bounded as

\[\mathbb{E}_{\bm{x},\bm{y}}\int_{0}^{\infty}r\mathbb{1}_{\mathcal{A }_{r}}\,\mathbb{P}_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\Big{(}\mathcal{B}^{( \Pi)}(r)\Big{)}\,\mathrm{d}r\] (86) \[\leq\int_{0}^{\infty}r\frac{e^{n\varepsilon_{n}^{2}r^{2s}}}{ \mathbb{P}_{f\sim\Pi_{n}}\Big{(}\|f-f_{0}\|_{L^{\infty}(\mathcal{M})}< \varepsilon_{n}r^{s}\Big{)}}\,\mathbb{P}_{f\sim\Pi_{n}}\Big{(}\|f\|_{\mathcal{C} \mathcal{H}^{\gamma}(\mathcal{M})}>\tau\sqrt{n}\varepsilon_{n}r^{s}\Big{)}\, \mathrm{d}r.\] (87)

Now, using Lemma 31, for a possibly small constant \(c>0\) independent of \(n\), we have

\[\mathbb{P}_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\Big{(}\|f\|_{\mathcal{C} \mathcal{H}^{\gamma}(\mathcal{M})}>\tau\sqrt{n}\varepsilon_{n}r^{s}\Big{)}\leq e ^{-cr^{2}ne_{n}^{2}r^{2s}}.\] (88)

Moreover, by using the bound on the concentration function in Theorem 21 and Ghosal and van der Vaart [20], Proposition 11.19, we can assume that

\[\mathbb{P}_{f\sim\Pi_{n}}\Big{[}\|f-f_{0}\|_{L^{\infty}(\mathcal{M})}< \varepsilon_{n}r^{s}\Big{]}\geq e^{-c^{-1}ne_{n}^{2}r^{2s}}.\] (89)Therefore, the third term is bounded by

\[\mathbb{E}_{\bm{x},\bm{y}}\int_{0}^{\infty}r\mathbb{1}_{\mathcal{A}_{ r}(\bm{x})}\,\mathbb{P}_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\Big{(}\mathcal{B}^{( \mathrm{II})}(r)\Big{)}\,\mathrm{d}r \leq\int_{0}^{\infty}re^{-n(cr^{2}-1)e_{n}^{2}r^{2s}}e^{c^{-1}n e_{n}^{2}r^{2s}}\,\mathrm{d}r\] (90) \[\leq\int_{0}^{\infty}re^{-r^{2s}}\,\mathrm{d}r<\infty\] (91)

if \(\tau^{2}c>2+c^{-1}\). It remains to bound the last term. We have, by the same arguments as above, that

\[\mathbb{E}_{\bm{x},\bm{y}}\int_{0}^{\infty}r\mathbb{1}_{\mathcal{A }_{r}(\bm{x})}\,\mathbb{P}_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\Big{(}\mathcal{B }^{(\mathrm{III})}(r)\Big{)}\,\mathrm{d}r\] (92) \[=\mathbb{E}_{\bm{x},\bm{y}}\int_{0}^{\infty}r\mathbb{1}_{\mathcal{ A}_{r}(\bm{x})}\] (93) \[\quad\times\mathbb{P}_{f\sim\Pi_{n}(\cdot|\bm{x},\bm{y})}\Big{(} \|f\|_{\mathcal{C}\mathcal{H}^{\gamma}(\mathcal{M})}\leq\tau\sqrt{n}\varepsilon _{n}r^{s},2\|f-f_{n}\|_{n}\leq\varepsilon_{n}r\leq\|f-f_{n}\|_{L^{2}(p_{0})} \Big{)}\,\mathrm{d}r\] (94) \[\leq\int_{0}^{\infty}r\frac{e^{nc_{n}^{2}r^{2s}}}{\mathbb{P}_{f \sim\Pi_{n}}\Big{(}\|f-f_{0}\|_{L^{\infty}(\mathcal{M})}<\varepsilon_{n}r^{s} \Big{)}}\] (95) \[\quad\times\mathbb{E}_{\bm{x}}\,\mathbb{P}_{f\sim\Pi_{n}}\Big{(} \|f\|_{\mathcal{C}\mathcal{H}^{\gamma}(\mathcal{M})}\leq\tau\sqrt{n} \varepsilon_{n}r^{s},2\|f-f_{n}\|_{n}\leq\varepsilon_{n}r\leq\|f-f_{n}\|_{L^{2} (p_{0})}\Big{)}\,\mathrm{d}r\] (96) \[\leq\int_{0}^{\infty}re^{(c+1)n\varepsilon_{n}^{2}r^{2s}}\] (97) \[\quad\times\int_{\|f\|_{\mathcal{C}\mathcal{H}^{\gamma}( \mathcal{M})}\leq\tau\sqrt{n}\varepsilon_{n}r^{s},\varepsilon_{n}r\leq\|f-f_{ n}\|_{L^{2}(p_{0})}}\mathbb{E}_{\bm{x}}\,\mathbb{1}_{\|f-f_{n}\|_{L^{2}(p_{0})} \geq 2\|f-f_{n}\|_{n}}\,\mathrm{d}\Pi_{n}(f)\,\mathrm{d}r.\] (98)

As the squared empirical \(L^{2}\)-norm is a Monte Carlo approximation of the true \(L^{2}\)-norm, the probability in the integrand can be controlled via a concentration inequality. As in van der Vaart and van Zanten [56], we use Bernstein's inequality [58, Lemma 2.2.9]. For a collection \(Y_{1},\ldots Y_{n}\) of random variables such that \(\mathbb{E}\,Y_{i}=0\) and \(Y_{i}\in[-M,M]\) almost surely for some constant \(M>0\), this inequality asserts

\[\mathbb{P}(|Y_{1}+\ldots+Y_{n}|>x)\leq 2\exp\biggl{(}-\frac{1}{2}\frac{x^{2}}{v+ Mx/3}\biggr{)}\] (99)

where \(v\geq\mathrm{Var}(Y_{1}+\ldots+Y_{n})\). We put \(Y_{i}=\frac{1}{n}(f(x_{i})-f_{n}(x_{i}))^{2}-\frac{1}{n}\|f-f_{n}\|_{L^{2}(p_{ 0})}^{2}\) where \(x_{i}\) are IID with \(x_{i}\sim p_{0}\). It is easy to check that \(\mathbb{E}\,Y_{i}=0\) and \(Y_{i}\in[-M,M]\) for \(M=\frac{1}{n}\|f-f_{n}\|_{L^{\infty}(\mathcal{M})}\). Furthermore, \(v=\frac{1}{n}\|f-f_{n}\|_{L^{\infty}(\mathcal{M})}^{2}\|f-f_{n}\|_{L^{2}( \mathcal{M})}^{2}\) upper-bounds the variance of the respective sum, since

\[\mathrm{Var}(Y_{1}+\ldots+Y_{n}) =\frac{1}{n}\,\mathrm{Var}_{x\sim p_{0}}(f(x)-f_{n}(x))^{2}\leq \frac{1}{n}\mathbb{E}_{x\sim p_{0}}(f(x)-f_{n}(x))^{4}\] (100) \[\leq\frac{\|f-f_{n}\|_{L^{\infty}(\mathcal{M})}^{2}}{n}\,\mathbb{ E}_{x\sim p_{0}}(f(x)-f_{n}(x))^{2}=v.\] (101)

Using Bernstein's inequality with \(Y_{i}\), \(M\), \(v\) as above and with \(x=\frac{3}{4}\|f-f_{n}\|_{L^{2}(p_{0})}^{2}\), we have

\[\mathbb{E}_{\bm{x}}\,\mathbb{1}_{\|f-f_{n}\|_{L^{2}(p_{0})}\geq 2\|f-f_{ n}\|_{n}} =\mathbb{E}_{\bm{x}}\,\mathbb{1}_{\|f-f_{n}\|_{L^{2}(p_{0})}^{2} \leq 4\|f-f_{n}\|_{n}^{2}}\] (102) \[=\mathbb{E}_{\bm{x}}\,\mathbb{1}_{\|f-f_{n}\|_{n}^{2}-\|f-f_{n}\| _{L^{2}(p_{0})}^{2}\leq-\frac{3}{4}\|f-f_{n}\|_{L^{2}(p_{0})}^{2}}\] (103) \[\lesssim\exp\biggl{(}-\frac{1}{2}\frac{x^{2}}{v+Mx/3}\biggr{)}\] (104) \[=\exp\Biggl{(}-\frac{\frac{9}{32}\|f-f_{n}\|_{L^{2}(p_{0})}^{4}}{ \|f-f_{n}\|_{L^{\infty}(\mathcal{M})}^{2}}\Biggr{)}\] (105) \[=\exp\Biggl{(}-\frac{9n}{32}\frac{4}{5}\frac{\|f-f_{n}\|_{L^{2}(p_ {0})}^{2}}{\|f-f_{n}\|_{L^{\infty}(\mathcal{M})}^{2}}\Biggr{)}\] (106) \[=\exp\Biggl{(}-\frac{9n}{40}\frac{\|f-f_{n}\|_{L^{2}(p_{0})}^{2}}{ \|f-f_{n}\|_{L^{\infty}(\mathcal{M})}^{2}}\Biggr{)}.\] (107)Moreover, by Lemma 19, since \(\gamma\notin\mathbb{Z}_{>0}\), we have

\[\|f-f_{n}\|_{L^{\infty}(\mathcal{M})}\lesssim\|f-f_{n}\|_{\mathcal{CH}^{\gamma}( \mathcal{M})}^{\frac{d}{2\gamma+d}}\|f-f_{n}\|_{L^{2}(p_{0})}^{\frac{2\gamma}{2 \gamma+d}}.\] (108)

Using the Sobolev Embedding Theorem, given in De Vito et al. [16], Theorem 4, \(\|f-f_{n}\|_{\mathcal{CH}^{\gamma}(\mathcal{M})}\lesssim\|f_{n}\|_{\mathbb{H}}+ \|f\|_{\mathcal{CH}^{\gamma}(\mathcal{M})}\lesssim\tau\sqrt{n}\varepsilon_{n}r ^{s}\) whenever \(\|f\|_{\mathcal{CH}^{\gamma}(\mathcal{M})}\leq\tau\sqrt{n}\varepsilon_{n}r^{s}\). Therefore, for a constant \(c>0\), when \(\|f\|_{\mathcal{CH}^{\gamma}(\mathcal{M})}\leq\tau\sqrt{n}\varepsilon_{n}r^{s}\) and \(\varepsilon_{n}r\leq\|f-f_{n}\|_{L^{2}(p_{0})}\), we have that

\[\mathbb{E}_{\bm{x}}\operatorname{\mathbb{1}}_{\|f-f_{n}\|_{L^{2} (\mathcal{M})}\geq 2\|f-f_{n}\|_{n}} \lesssim\exp\left(-cn\frac{\|f-f_{n}\|_{L^{2}(p_{0})}^{2d}}{\|f- f_{n}\|_{L^{2}(\mathcal{M})}^{\frac{2d}{2\gamma+d}}\|f-f_{n}\|_{L^{2}(p_{0})}^{ \frac{4\gamma}{2\gamma+d}}}\right)\] (109) \[\leq e^{-c\tau-\frac{2d}{2\gamma+d}n^{\frac{2\gamma}{2\gamma+d}}r ^{\frac{2d}{2\gamma+d}(1-s)}}.\] (110)

Hence, we can bound the last term as

\[\mathbb{E}_{\bm{x},\bm{y}}\int_{0}^{\infty}r\mathbb{1}_{\mathcal{ A}_{r}}\operatorname{\mathbb{P}}_{f\sim\Pi_{n}(\cdot\mid\bm{x},\bm{y})}\left( \mathcal{B}^{(\mathrm{III})}(r)\right)\mathrm{d}r\] (111) \[\lesssim\int_{0}^{\infty}re^{(c+1)ne_{n}^{2}r^{s}}e^{-c\tau^{- \frac{2d}{2\gamma+d}}n^{\frac{2\gamma}{2\gamma+d}}r^{\frac{2d}{2\gamma+d}(1-s) }}\mathrm{d}r.\] (112)

We have \(n^{\frac{2\gamma}{2\gamma+d}}=n\Big{(}n^{-\frac{d/2}{2\gamma+d}}\Big{)}^{2}\). Since \(\varepsilon_{n}\lesssim n^{-\frac{\min(\nu,\beta)}{2\nu+d}}\) and \(\min(\nu,\beta)>d/2\), we have \(n\varepsilon_{n}^{2}\lesssim n^{\frac{2\gamma}{2\gamma+d}}\) for some \(\gamma\in(d/2,\nu)\). Moreover, for this choice of \(\gamma\) and \(s\) small enough we have \(\frac{2d}{2\gamma+d}(1-s)\geq 2s\), which proves that for some constants \(C,C^{\prime},C^{\prime\prime}>0\) the fourth term is bounded by

\[C\int_{0}^{\infty}re^{-C^{\prime\prime}r^{C^{\prime\prime}}}\mathrm{d}r<\infty.\] (113)

This concludes the proof. 

**Remark 23**.: _Following the proof, it is easy to see that \(\|f-f_{0}\|_{L^{2}(p_{0})}^{2}\) on the left-hand side of Equations (9), (10) and (12) can be replaced with \(\|f-f_{0}\|_{L^{2}(p_{0})}^{q}\) for any \(q>1\), changing the exponent \(-\frac{2\min(\beta,\nu)}{2\nu+d}\) on the right-hand side to \(-\frac{q\min(\beta,\nu)}{2\nu+d}\)._

## Appendix C Characterizing Reproducing Kernel Hilbert Spaces of Matern Kernels

We start by describing the reproducing kernel Hilbert spaces (RKHSs) of the (truncated) intrinsic Matern processes, proving that they coincide with (certain subspaces of) the manifold Sobolev spaces. We follow the ideas of Borovitskiy et al. [11], where the same was shown somewhat implicitly. We consider the more-involved case of the extrinsic Matern processes immediately after.

The next lemma describes the RKHS of the intrinsic Matern processes, including truncated variants. This result is easy to obtain since we have defined them in terms of the Karhunen-Loeve expansions.

**Lemma 24**.: _Let \(\mathbb{H}_{J}\) be the RKHS of the intrinsic Matern Gaussian process with smoothness parameter \(\nu\) truncated at the level \(J\in\mathbb{Z}_{>0}\cup\{\infty\}\), and let \(\{f_{j}\}_{j=0}^{\infty}\) be the orthonormal basis of Laplace-Beltrami eigenfunctions. The space \(\mathbb{H}_{J}\) is norm-equivalent--with constants depending only on \(\nu,\kappa\) and \(\sigma_{f}^{2}\)--to the set of functions \(f=\sum_{j=1}^{J}b_{j}f_{j}\) with \(b_{j}\in\mathbb{R}\), equipped with the inner product_

\[\Big{\langle}\sum_{j=1}^{J}b_{j}f_{j},\sum_{j=1}^{J}b_{j}^{\prime}f_{j}\Big{\rangle} _{\mathbb{H}_{J}}=\sum_{j=1}^{J}(1+\lambda_{j})^{\nu+d/2}b_{j}b_{j}^{\prime}.\] (114)

_In particular, \(\mathbb{H}_{J}\subset H^{\nu+d/2}(\mathcal{M})\) for all \(J\), and for every \(h\in\mathbb{H}_{J}\) we have \(\|h\|_{\mathbb{H}_{J}}=\|h\|_{H^{\nu+d/2}(\mathcal{M})}\)._

Proof.: By direct computation, the covariance \(k\) of the (truncated) intrinsic Gaussian process is

\[k(x,x^{\prime})=\frac{\sigma_{f}^{2}}{C_{\nu,\kappa}}\sum_{j=1}^{J}\!\left( \frac{2\nu}{\kappa^{2}}+\lambda_{j}\right)^{-(\nu+d/2)}f_{j}(x)f_{j}(x^{\prime}).\] (115)Hence, the covariance operator \(K:L^{2}(\mathcal{M})\to L^{2}(\mathcal{M})\) defined by

\[(Kf)(x)=\int_{\mathcal{M}}k(x,x^{\prime})f(x^{\prime})\,\mathrm{d}x^{\prime}\] (116)

is diagonal in the basis \(\left\{f_{j}\right\}_{j=1}^{J}\), with \(Kf_{j}=\frac{\sigma_{f}^{2}}{C_{\nu,\kappa}}\big{(}\frac{2\nu}{\kappa^{2}}+ \lambda_{j}\big{)}^{-(\nu+d/2)}f_{j}\). Then, Kanagawa et al. [28], Theorem 4.2 implies that \(\mathbb{H}_{J}\) consists of functions of form \(f=\sum_{j=1}^{J}a_{j}f_{j}\) satisfying

\[\left\|f\right\|_{\mathbb{H}_{J}}^{2}=\frac{\sigma_{f}^{2}}{C_{\nu,\kappa}} \sum_{j=1}^{J}\biggl{(}\frac{2\nu}{\kappa^{2}}+\lambda_{j}\biggr{)}^{\nu+d/2} \left|a_{j}\right|^{2}<\infty.\] (117)

Using the elementary inequality \(\min\bigl{(}\frac{2\nu}{\kappa^{2}},1\bigr{)}\leq\frac{2\nu}{\kappa^{2}}+ \lambda_{j}\leq\max\bigl{(}\frac{2\nu}{\kappa^{2}},1\bigr{)}\), we find that this space is norm-equivalent to the space \(H_{J}^{\nu+d/2}(\mathcal{M})\) of functions \(f=\sum_{j=1}^{J}a_{j}f_{j}\) satisfying

\[\left\|f\right\|_{H_{J}^{\nu+d/2}(\mathcal{M})}^{2}=\sum_{j=1}^{J}(1+\lambda_ {j})^{\nu+d/2}|a_{j}|^{2}<\infty\] (118)

where the comparison constants \(\sqrt{\frac{\sigma_{f}^{2}}{C_{\nu,\kappa}}\min\bigl{(}1,\frac{2\nu}{\kappa^{ 2}}\bigr{)}}\) and \(\sqrt{\frac{\sigma_{f}^{2}}{C_{\nu,\kappa}}\max\bigl{(}1,\frac{2\nu}{\kappa^{ 2}}\bigr{)}}\) only depend on the parameters \(\nu\), \(\kappa\), \(\sigma_{f}^{2}\). 

The next two theorems will be useful to characterize the RKHS of the extrinsic Matern process on \(\mathcal{M}\). We start by a lemma relating the RKHS of the restriction of a Gaussian process to the original one.

**Lemma 25**.: _Assume that \(k\) is a kernel on \(\mathbb{R}^{d}\), \(f\sim\mathrm{GP}(0,k)\) with almost surely continuous sample paths and \(\widetilde{\mathbb{H}}\) is the RKHS of \(k\). If \(\mathcal{M}\subseteq\mathbb{R}^{d}\) is a submanifold, then the RKHS \(\mathbb{H}\) corresponding to the restricted process \(f_{|\mathcal{M}}\) is the set of all restrictions \(g_{|\mathcal{M}}\) of functions \(g\in\widetilde{\mathbb{H}}\) equipped with the norm_

\[\left\|h\right\|_{\mathbb{H}}=\inf_{g\in\widetilde{\mathbb{H}},\ g_{|\mathcal{M }}=h}\left\|g\right\|_{\widetilde{\mathbb{H}}}.\] (119)

_Moreover there always exists an element \(g\in\widetilde{\mathbb{H}}\) such that \(g_{|\mathcal{M}}=f\) and \(\left\|g\right\|_{\widetilde{\mathbb{H}}}=\left\|f\right\|_{\mathrm{H}}\)._

Proof.: Lemma 5.1 in Yang and Dunson [63]. 

The last result will be used to characterize the RKHS of the extrinsic Matern Gaussian processes using trace and extension operators. The second ingredient for this is the following.

**Theorem 26**.: _If \(s>\frac{D-d}{2}\) then the restriction operator extends to a bounded linear map \(\mathrm{Tr}:H^{s}\bigl{(}\mathbb{R}^{D}\bigr{)}\to H^{s-\frac{D-d}{2}}( \mathcal{M})\). Moreover, for every \(u>0\) there exists a bounded right inverse \(\mathrm{Ex}:H^{u}(\mathcal{M})\to H^{u+\frac{D-d}{2}}\bigl{(}\mathbb{R}^{D} \bigr{)}\) such that \(\mathrm{Tr}\circ\mathrm{Ex}=\mathrm{id}_{H^{u}(\mathcal{M})}\) where \(\mathrm{Tr}\) corresponds to \(s=u+\frac{D-d}{2}\)._

Proof.: Theorem 4.10 in Grosse and Schneider [24]. 

The last two results allow us to characterize the RKHS of the extrinsic Matern process on \(\mathcal{M}\).

**Proposition 27**.: _The RKHS \(\mathbb{H}\) of a restricted extrinsic Matern process \(f\) with smoothness parameter \(\nu\) on \(\mathcal{M}\) is norm-equivalent to the Sobolev space \(H^{\nu+d/2}(\mathcal{M})\)._

Proof.: Using Lemma 25, the RKHS \(\mathbb{H}\) can be characterized as the set of functions \(f:\mathcal{M}\to\mathbb{R}\) that are the restrictions of some \(g\in\widetilde{\mathbb{H}}\), where \(\widetilde{\mathbb{H}}\) is the RKHS of the ambient Matern process \(\tilde{f}\), with

\[\left\|f\right\|_{\mathbb{H}}=\inf_{g\in\widetilde{\mathbb{H}},\ g_{|\mathcal{M }}=f}\left\|g\right\|_{\widetilde{\mathbb{H}}}.\] (120)Since \(\bar{\mathbb{H}}\) is norm-equivalent to the Sobolev space7\(H^{\nu+D/2}\big{(}\mathbb{R}^{D}\big{)}\)--see for instance Kanagawa et al. [28]--by Theorem 26, for every \(f\in\mathbb{H}\) we have

Footnote 7: Note that this norm-equivalence is the only property of the Gaussian process we use in the proofs. Any other Gaussian process satisfying this, including ones different from the Matern processes of Borovitskiy et al. [11], would also work. This is of potential interest since other Euclidean kernels, such as Wendland kernels [61], are known to possess RKHSs which are norm-equivalent to those of the Matern kernel.

\[\left\|f\right\|_{\mathbb{H}}\lesssim\left\|\mathrm{Ex}(f)\right\|_{H^{\nu+D/ 2}(\mathbb{R}^{D})}\lesssim\left\|f\right\|_{H^{\nu+D/2-\frac{D-d}{2}}(\mathcal{ M})}=\left\|f\right\|_{H^{\nu+d/2}(\mathcal{M})}.\] (121)

Similarly, for every \(g\in\widetilde{\mathbb{H}}\) with \(g_{|\mathcal{M}}=f\), we have

\[\left\|f\right\|_{H^{\nu+d/2}(\mathcal{M})}=\left\|g_{|\mathcal{M}}\right\|_{ H^{\nu+d/2}(\mathcal{M})}\lesssim\left\|g\right\|_{H^{\nu+D/2}(\mathbb{R}^{D})} \lesssim\left\|g\right\|_{\widetilde{\mathbb{H}}}.\] (122)

Hence, taking the infimum, we obtain

\[\left\|f\right\|_{H^{\nu+d/2}(\mathcal{M})}\lesssim\inf_{g\in\widetilde{ \mathbb{H}},\ g_{|\mathcal{M}}=f}\left\|g\right\|_{\widetilde{\mathbb{H}}}= \left\|f\right\|_{\mathbb{H}}.\] (123)

The claim follows. 

## Appendix D Concentration and Sample Path Regularity

In this section, we prove that intrinsic, truncated intrinsic and extrinsic Matern processes are sub-Gaussian, uniformly with respect to the truncation parameter in the case of the truncated intrinsic Matern process, and live in Holder spaces with appropriate exponents. On our way to proving this, we characterize sample path regularity of (truncated) intrinsic Matern processes, which is of independent interest. A simple way to do this would be to build on the results of Appendix C, using Driscoll's Theorem--given in Kanagawa et al. [28], Theorem 4.9--and the Sobolev Embedding Theorem--De Vito et al. [16], Theorem 4--but that would only give us that the sample paths are almost surely in \(\mathcal{CH}^{\gamma}(\mathcal{M})\) for every \(0<\gamma<\nu-d/2,\gamma\notin\mathbb{N}\), whereas here we improve the range of index to \(\gamma<\nu\).

Kolmogorov's continuity criterion is a standard tool to show that a given stochastic process has a Holder continuous version: we re-prove it here because we will need a form of the result which gives explicit control of the Holder norms, which is not usually included in the respective statement.

**Lemma 28** (Kolmogorov's continuity criterion).: _If \(g\sim\Pi\) is a zero-mean Gaussian process on \([0,1]^{d}\) with_

\[\mathbb{E}\left|g(x)-g(y)\right|^{2}\leq C\|x-y\|_{\mathbb{R}^{d}}^{2\rho}\] (124)

_for some \(0<\rho\leq 1\) and \(C>0\), then there exists a version of \(g\) with samples paths in \(\mathcal{CH}^{\alpha}\big{(}[0,1]^{d}\big{)}\) for every \(0<\alpha<\rho\). Moreover for every \(\alpha<\rho\) this version satisfies \(\mathbb{E}\left\|g\right\|_{\mathcal{CH}^{\alpha}([0,1]^{d})}^{2}\leq C^{\prime}\) where \(C^{\prime}<+\infty\) depends only on \(C,\rho,\alpha\) and \(d\)._

Proof.: Take \(x,y\in[0,1]^{d},M>0\) and \(q\in\mathbb{N}\). Since the random variable \(g(x)-g(y)\) is Gaussian we have

\[\mathbb{E}\left|g(x)-g(y)\right|^{2q}=\frac{(2q)!}{2^{q}q!}\Big{(}\mathbb{E} \left|g(x)-g(y)\right|^{2}\Big{)}^{q}\leq C_{q}\|x-y\|_{\mathbb{R}^{d}}^{2\rho q}\] (125)

where we have defined \(C_{q}=C^{q}\frac{(2q)!}{2^{q}q!}\). The reason for considering the the \(2q\)th power will become clear later in the proof. By Markov's inequality, for every \(x,y\in[0,1]^{d}\) we have

\[\mathbb{P}(|g(x)-g(y)|>u)\leq C_{q}u^{-2q}\|x-y\|_{\mathbb{R}^{d}}^{2q\rho}.\] (126)

Now, take \(X=\cup_{k\geq 0}X_{k},X_{k}=2^{-k}\mathbb{Z}^{d}\cap[0,1]^{d}\). Then, the previous inequality applied to any adjacent \(x,y\in X_{k}\), where we see \(X_{k}\) as a graph where two vertices are connected if they differ by \(2^{-k}\), and \(u=M2^{-k\alpha}\), implies

\[\mathbb{P}\big{(}|g(x)-g(y)|>M2^{-k\alpha}\big{)}\leq C_{q}M^{-2q}2^{-2kq(\rho -\alpha)}.\] (127)

[MISSING_PAGE_FAIL:26]

where \(x,y\in\mathcal{V}_{l}\) and \(K(x,y)=\mathrm{Cov}(f(x),f(y))\) is the covariance kernel of \(f\). Let \(\widetilde{\mathbb{H}}_{l}\) be the RKHS induced by \(\widetilde{K}_{l}\). We seek to apply Lemma 28 to \(h_{l}\). For all \(x,y\in\mathcal{V}_{l}\), where we recall that we can assume that \(\mathcal{V}_{l}=(0,1)^{d}\), we have

\[\mathbb{E}_{f\sim\Pi_{n}}|h_{l}(x)-h_{l}(y)|^{2} =\widetilde{K}_{l}(x,x)+\widetilde{K}_{l}(y,y)-2\widetilde{K}_{l} (x,y)\] (139) \[=\Big{\|}\widetilde{K}_{l}(x,\cdot)-\widetilde{K}_{l}(y,\cdot) \Big{\|}_{\widetilde{\mathbb{H}}_{l}}^{2}\] (140) \[=\sup_{\|\varphi\|_{\widetilde{\mathbb{H}}_{l}}=1}\!\!\left| \Big{\langle}\widetilde{K}_{l}(x,\cdot)-\widetilde{K}_{l}(y,\cdot),\varphi \Big{\rangle}_{\widetilde{\mathbb{H}}_{l}}\right|^{2}\] (141) \[=\sup_{\|\varphi\|_{\widetilde{\mathbb{H}}_{l}}=1}\!\!\left| \varphi(x)-\varphi(y)\right|^{2}\] (142) \[\leq\sup_{\|\varphi\|_{\widetilde{\mathbb{H}}_{l}}=1}\!\!\left\| \varphi\right\|_{\mathcal{CH}^{\nu}(\mathcal{V}_{l})}^{2}\!\!\left\|x-y\right\| _{\mathbb{R}^{d}}^{2\nu}\] (143)

where \(\|\varphi\|_{\mathcal{CH}^{\nu}(\mathcal{V}_{l})}^{2}\) can potentially be infinite or unbounded: we will show this is not the case. To do this it suffices to show that we have a continuous embedding \(\widetilde{\mathbb{H}}_{l}\hookrightarrow\mathcal{CH}^{\nu}(\mathcal{V}_{l})\), that is \(\|\cdot\|_{\mathcal{CH}^{\nu}(\mathcal{V}_{l})}\lesssim\|\cdot\|_{\widetilde {\mathbb{H}}_{l}}\). The RKHS \(\widetilde{\mathbb{H}}_{l}\) is by definition the completion of

\[\left\{\sum_{i=1}^{p}\alpha_{i}\widetilde{K}_{l}(x_{i},\cdot):p \geq 1,\alpha_{i}\in\mathbb{R},x_{i}\in\mathcal{V}_{l}\right\}\] (144) \[=\left\{\sum_{i=1}^{p}\alpha_{i}\big{(}\chi_{l}\circ\phi_{l}^{-1} \big{)}(x_{i})\big{(}\chi_{l}\circ\phi_{l}^{-1}\big{)}(\cdot)K\big{(}\phi_{l} ^{-1}(x_{i}),\phi_{l}^{-1}(\cdot)\big{)}:p\geq 1,\alpha_{i}\in\mathbb{R},x_{i} \in\mathcal{V}_{l}\right\}\] (145)

with respect to the topology induced by the RKHS norm

\[\left\|\sum_{i=1}^{p}\alpha_{i}\widetilde{K}_{l}(x_{i},\cdot)\right\|_{ \widetilde{\mathbb{H}}_{l}}^{2}=\sum_{i,j=1}^{p}\alpha_{i}\alpha_{j}\big{(} \chi_{l}\circ\phi_{l}^{-1}\big{)}(x_{i})\big{(}\chi_{l}\circ\phi_{l}^{-1} \big{)}(x_{j})K\big{(}\phi_{l}^{-1}(x_{i}),\phi_{l}^{-1}(x_{j})\big{)}.\] (146)

Denote the RKHS of \(K\) by \(\mathbb{H}\). By Theorem 16, and by the equality \(\|\cdot\|_{\mathbb{H}}=\|\cdot\|_{H^{\nu+d/2}(\mathcal{M})}\) on \(\mathbb{H}\) which follows by Lemma 24, we have

\[\left\|\sum_{i=1}^{p}\alpha_{i}\widetilde{K}_{l}(x_{i},\cdot) \right\|_{H^{\nu+d/2}(\mathbb{R}^{d})}^{2}\] (147) \[\quad=\left\|\sum_{i=1}^{p}\alpha_{i}\big{(}\chi_{l}\circ\phi_{l} ^{-1}\big{)}(x_{i})\big{(}\chi_{l}\circ\phi_{l}^{-1}\big{)}(\cdot)K(\phi_{l}^{ -1}(x_{i}),\phi_{l}^{-1}(\cdot))\right\|_{H^{\nu+d/2}(\mathbb{R}^{d})}^{2}\] (148) \[\lesssim\left\|\sum_{i=1}^{p}\alpha_{i}\big{(}\chi_{l}\circ\phi_{l }^{-1}\big{)}(x_{i})K(\phi_{l}^{-1}(x_{i}),\cdot)\right\|_{H^{\nu+d/2}( \mathcal{M})}^{2}\] (149) \[=\left\|\sum_{i=1}^{p}\alpha_{i}\big{(}\chi_{l}\circ\phi_{l}^{-1} \big{)}(x_{i})K(\phi_{l}^{-1}(x_{i}),\cdot)\right\|_{\mathbb{H}}^{2}\] (150) \[=\sum_{i,j=1}^{p}\alpha_{i}\alpha_{j}\big{(}\chi_{l}\circ\phi_{l} ^{-1}\big{)}(x_{i})\big{(}\chi_{l}\circ\phi_{l}^{-1}\big{)}(x_{j})K\big{(}\phi_{ l}^{-1}(x_{i}),\phi_{l}^{-1}(x_{j})\big{)}\] (151) \[=\left\|\sum_{i=1}^{p}\alpha_{i}\widetilde{K}_{l}(x_{i},\cdot) \right\|_{\widetilde{\mathbb{H}}_{l}}^{2}.\] (152)

Therefore, we have a continuous embedding \(\widetilde{\mathbb{H}}_{l}\hookrightarrow H^{\nu+d/2}\big{(}\mathbb{R}^{d}\big{)}\) with \(\|\cdot\|_{H^{\nu+d/2}(\mathbb{R}^{d})}\lesssim\|\cdot\|_{\widetilde{\mathbb{ H}}_{l}}^{2}\) on \(\widetilde{\mathbb{H}}_{l}\). By the Sobolev Embedding Theorem in \(\mathbb{R}^{d}\)--see for instance Triebel [50], Section 2.7.1,Remark 2--we have \(B_{2,2}^{\nu+d/2}\big{(}\mathbb{R}^{d}\big{)}=H^{\nu+d/2}\big{(}\mathbb{R}^{d} \big{)}\hookrightarrow\mathcal{CH}^{\nu}(\mathbb{R}^{d})\), which implies \(\widetilde{\mathbb{H}}_{l}\hookrightarrow\mathcal{CH}^{\nu}\big{(}\mathbb{R}^{ d}\big{)}\) by composition. Thus, there exists a constant \(C=C_{\nu}\) such that

\[\mathbb{E}_{f\sim\Pi_{n}}\big{|}h_{l}(x)-h_{l}(y)\big{|}^{2}\leq C\|x-y\|_{ \mathbb{R}^{d}}^{2\nu}\] (153)

for \(x,y\in\mathcal{V}_{l}\). Hence, by applying Lemma 28, there exists a version \(\tilde{h}_{l}\) of \(h_{l}\) with almost surely \(\alpha\)-Holder continuous sample paths for every \(\alpha<\nu\). Now consider \(\tilde{f}=\sum_{l=1}^{L}\tilde{h}_{l}\circ\phi_{l}\). Then \(\tilde{f}\) is a version of \(f\). We proceed to bound \(\mathbb{E}_{f\sim\Pi_{n}}\Big{[}\|\tilde{f}\|_{\mathcal{CH}^{\alpha}( \mathcal{M})}^{2}\Big{]}\). For any \(1\leq l,r\leq L\) write

\[\Big{|}\tilde{h}_{r}(\phi_{r}(\phi_{l}^{-1}(x)))-\tilde{h}_{r}(\phi_{r}(\phi_{ l}^{-1}(y)))\Big{|}\leq K\big{|}\phi_{r}(\phi_{l}^{-1}(x))-\phi_{r}(\phi_{l}^{-1}(y)) \big{|}^{\alpha}\leq CK|x-y|^{\alpha}\] (154)

where \(C\) is the Lipshitz constant of \(\phi_{r}\circ\phi_{l}^{-1}\) which is well defined and finite because this composition is a diffeomorphism and \(K\) is a random constant with \(\mathbb{E}_{f\sim\Pi_{n}}\,K^{2}\leq C_{\alpha,\nu,d}\). Hence

\[\mathbb{E}_{f\sim\Pi_{n}}\big{\|}\tilde{f}\big{\|}_{\mathcal{CH}^ {\alpha}(\mathcal{M})}^{2} =\mathbb{E}_{f\sim\Pi_{n}}\sum_{l=1}^{L}\Bigl{\|}\Big{(}\chi_{l} \tilde{f}\Big{)}\circ\phi_{l}^{-1}\Bigr{\|}_{\mathcal{CH}^{\alpha}(\mathbb{R}^ {d})}^{2}\] (155) \[=\mathbb{E}_{f\sim\Pi_{n}}\sum_{l=1}^{L}\Biggl{\|}\Bigg{(}\chi_{l }\sum_{r=1}^{L}\tilde{h}_{r}\circ\phi_{r}\Bigg{)}\circ\phi_{l}^{-1}\Biggr{\|}_ {\mathcal{CH}^{\alpha}(\mathbb{R}^{d})}^{2}\] (156) \[\lesssim\sum_{l=1}^{L}\sum_{r=1}^{L}\mathbb{E}_{f\sim\Pi_{n}} \Big{\|}\tilde{h}_{r}\circ\phi_{r}\circ\phi_{l}^{-1}\Big{\|}_{\mathcal{CH}^{ \alpha}(\mathbb{R}^{d})}^{2}\lesssim\mathbb{E}_{f\sim\Pi_{n}}\,K^{2}\leq C_{ \alpha,\nu,d}.\] (157)

which gives the \(\nu\leq 1\) case.

We now turn to the general case. The proof will be similar to the one of Ghosal and van der Vaart [20], Proposition I.3 although we need to control the Holder norms and work through charts since our Gaussian processes are supported on manifolds. Assume for simplicity that \(d=1,1<\nu\leq 2\), otherwise it suffices to introduce coordinates and to proceed by induction on \(\lfloor\nu\rfloor\). Let \(l\in\{1,\ldots,L\}\), and as before define \(\widetilde{K}_{l}(x,y)=\big{(}\chi_{l}\circ\phi_{l}^{-1}\big{)}(x)\big{(}\chi _{l}\circ\phi_{l}^{-1}\big{)}(y)K\big{(}\phi_{l}^{-1}(x),\phi_{l}^{-1}(y) \big{)}\) the covariance kernel of \(h_{l}=(\chi_{l}f)\circ\phi_{l}^{-1}\) as well as \(\widetilde{\mathbb{H}}_{l}\) its RKHS.

First, let us construct an \(L^{2}(\Omega)\)-derivative \(\dot{h}_{l}\) of \(h_{l}\), where \(L^{2}(\Omega)\) is the space of random variables with finite variance with \(\left\langle a,b\right\rangle_{L^{2}(\Omega)}=\mathbb{E}(ab)\). This derivative is a square integrable process on \(\mathcal{V}_{l}\) such that

\[\mathbb{E}_{f\sim\Pi_{n}}\bigg{|}\frac{h_{l}(x+\delta)-h_{l}(x)}{h}-\dot{h}_{l }(x)\bigg{|}^{2}\to 0\] (158)

as \(h\to 0\), for all \(x\in\mathcal{V}_{l}\). For this, we will first show that \(\frac{\partial\widetilde{K}_{l}}{\partial x}(x,\cdot)\in\widetilde{\mathbb{E} }_{l}\) for every \(x\in\mathcal{V}_{l}\)--here \(\frac{\partial\widetilde{K}_{l}}{\partial x}\) denotes the derivative of the function \(\widetilde{K}_{l}(\cdot,\cdot^{\prime})\) with respect to the first argument--and that

\[\left\|\frac{\partial\widetilde{K}_{l}}{\partial x}(x,\cdot)-\frac{\partial \widetilde{K}_{l}}{\partial x}(x^{\prime},\cdot)\right\|_{\widetilde{\mathbb{H }}_{l}}\leq C_{\nu}|x-x^{\prime}|^{\nu-1}.\] (159)We first show that \(\frac{\widetilde{K}_{l}(x+\delta,\cdot)-\widetilde{K}_{l}(x,\cdot)}{h}\) is a Cauchy net8 in \(\widetilde{\mathbb{H}}_{l}\). We have

Footnote 8: See for instance Aliprantis and Border [1] for a review of Cauchy nets.

\[\left\|\frac{\widetilde{K}_{l}(x+\delta,\cdot)-\widetilde{K}_{l}( x,\cdot)}{h}-\frac{\widetilde{K}_{l}(x+\delta^{\prime},\cdot)-\widetilde{K}_{l}(x, \cdot)}{h^{\prime}}\right\|_{\widetilde{\mathbb{H}}_{l}}\] (160) \[\quad=\sup_{\|\varphi\|_{\widetilde{\mathbb{H}}_{l}}=1}\left\langle \frac{\widetilde{K}_{l}(x+\delta,\cdot)-\widetilde{K}_{l}(x,\cdot)}{h}-\frac{ \widetilde{K}_{l}(x+\delta^{\prime},\cdot)-\widetilde{K}_{l}(x,\cdot)}{h^{ \prime}},\varphi\right\rangle_{\widetilde{\mathbb{H}}_{l}}\] (161) \[\quad=\sup_{\|\varphi\|_{\widetilde{\mathbb{H}}_{l}}=1}\left( \frac{\varphi(x+\delta)-\varphi(x)}{h}-\frac{\varphi(x+\delta^{\prime})- \varphi(x)}{h^{\prime}}\right)\] (162) \[\quad=\sup_{\|\varphi\|_{\widetilde{\mathbb{H}}_{l}}=1}\int_{0}^ {1}[\varphi^{\prime}(x+th)-\varphi^{\prime}(x+th^{\prime})]\,\mathrm{d}t\] (163) \[\quad\leq\sup_{\|\varphi\|_{\widetilde{\mathbb{H}}_{l}}=1}\| \varphi^{\prime}\|_{\mathcal{C}\mathcal{H}^{\nu-1}(\mathcal{V}_{l})}|h-h^{ \prime}|^{\nu-1}\] (164) \[\quad\leq\sup_{\|\varphi\|_{\widetilde{\mathbb{H}}_{l}}=1}\| \varphi\|_{\mathcal{C}\mathcal{H}^{\nu}(\mathcal{V}_{l})}|h-h^{\prime}|^{\nu-1}\] (165)

where in (163) the derivative \(\varphi^{\prime}\) exists because, exactly as in the case \(\nu\leq 1\), we can show show that \(\widetilde{\mathbb{H}}_{l}\hookrightarrow\mathcal{CH}^{\nu}\big{(}\mathbb{R}^{ d}\big{)}\). This also implies that for a constant \(C=C_{\nu}\) we have

\[\left\|\frac{\widetilde{K}_{l}(x+\delta,\cdot)-\widetilde{K}_{l}(x,\cdot)}{h}- \frac{\widetilde{K}_{l}(x+\delta^{\prime},\cdot)-\widetilde{K}_{l}(x,\cdot)}{ h^{\prime}}\right\|_{\widetilde{\mathbb{H}}_{l}}\leq C|h-h^{\prime}|^{\nu-1}.\] (166)

As \(|h-h^{\prime}|^{\nu-1}\to 0\) when \(h,h^{\prime}\to 0\), because \(\nu>1\), this proves that \(\frac{\widetilde{K}_{l}(x+\delta,\cdot)-\widetilde{K}_{l}(x,\cdot)}{h}\) is a Cauchy net in \(\widetilde{\mathbb{H}}_{l}\): by completeness of \(\widetilde{\mathbb{H}}_{l}\) it converges in \(\widetilde{\mathbb{H}}_{l}\) to a limit. Since by general properties of RKHSs, convergence in \(\widetilde{\mathbb{H}}_{l}\) implies pointwise convergence, the limit satisfies

\[\lim_{h\to 0}\frac{\widetilde{K}_{l}(x+\delta,y)-\widetilde{K}_{l}(x,y)}{h}= \frac{\partial\widetilde{K}_{l}}{\partial x}(x,y).\] (167)

Hence the partial derivative \(\frac{\partial\widetilde{K}_{l}}{\partial x}(x,y)\) exists for all \(y\) and \(\frac{\partial\widetilde{K}_{l}}{\partial x}(x,\cdot)\in\widetilde{\mathbb{H}}_ {l}\). Moreover, by the isometry \(L^{2}(\Omega)\ni h_{l}(x)\mapsto\mathbb{E}_{f\sim\Pi_{l}}\,h_{l}(x)h_{l}( \cdot)=\widetilde{K}_{l}(x,\cdot)\in\widetilde{\mathbb{H}}_{l}\), we deduce that \(h_{l}\) is actually \(L^{2}(\Omega)\)-differentiable, with an \(L^{2}(\Omega)\)-derivative denoted as \(\dot{h}_{l}\), and that the derivative process \(\dot{h}_{l}\) is Gaussian, as it is an \(L^{2}(\Omega)\)-limit of Gaussian random variables, satisfying \(\mathbb{E}_{f\sim\Pi_{n}}\,\dot{h}_{l}(x)\dot{h}_{l}(y)=\left\langle\frac{ \partial\widetilde{K}_{l}}{\partial x}(x,\cdot),\frac{\partial\widetilde{K}_{l }}{\partial x}(y,\cdot)\right\rangle_{\widetilde{\mathbb{H}}_{l}}\)

Having established the existence of an \(L^{2}(\Omega)\)-derivative \(\dot{h}_{l}\) of the process \(h_{l}\), we would like now to show that \(\dot{h}_{l}\) possesses a \((\gamma-1)\)-regular version for every \(\gamma<\nu\). For this, we would like to apply Lemma 28 to \(\dot{h}_{l}\). Notice that, by isometry, for all \(h>0\) we have

\[\mathbb{E}_{f\sim\Pi_{n}}\Big{|}\dot{h}_{l}(x)-\dot{h}_{l}(y)\Big{|} ^{2} =\left\|\frac{\partial\widetilde{K}_{l}}{\partial x}(y,\cdot)- \frac{\partial\widetilde{K}_{l}}{\partial x}(x,\cdot)\right\|_{\widetilde{ \mathbb{H}}_{l}}^{2}\] (168) \[\leq 3\left\|\frac{\widetilde{K}_{l}(y+\delta,\cdot)-\widetilde{K} _{l}(y,\cdot)}{h}-\frac{\partial\widetilde{K}_{l}}{\partial x}(y,\cdot)\right\|_{ \widetilde{\mathbb{H}}_{l}}^{2}\] (169) \[\quad+3\left\|\frac{\widetilde{K}_{l}(x+\delta,\cdot)-\widetilde{ K}_{l}(x,\cdot)}{h}-\frac{\partial\widetilde{K}_{l}}{\partial x}(x,\cdot) \right\|_{\widetilde{\mathbb{H}}_{l}}^{2}\] (170) \[\quad+3\left\|\frac{\widetilde{K}_{l}(x+\delta,\cdot)-\widetilde{ K}_{l}(x,\cdot)}{h}-\frac{\widetilde{K}_{l}(y+\delta,\cdot)-\widetilde{K}_{l}(y, \cdot)}{h}\right\|_{\widetilde{\mathbb{H}}_{l}}^{2}.\] (171)Therefore, by the same arguments as above, we have

\[\left(\mathbb{E}_{f\sim\Pi_{n}}\Big{|}\dot{h}_{l}(x)-\dot{h}_{l}(y) \Big{|}^{2}\right)^{1/2} \lesssim\liminf_{h\to 0}\!\left\|\frac{\widetilde{K}_{l}(x+\delta, \cdot)-\widetilde{K}_{l}(x,\cdot)}{h}-\frac{\widetilde{K}_{l}(y+\delta,\cdot)- \widetilde{K}_{l}(y,\cdot)}{h}\right\|_{\widetilde{\mathbb{H}}_{l}}\] (172) \[\leq\liminf_{h\to 0}\sup_{\|\nu\|_{\widetilde{h}_{l}}=1} \int_{0}^{1}\!\!\left|\varphi^{\prime}(x+th)-\varphi^{\prime}(y+th)\right| \mathrm{d}t\] (173) \[\leq\liminf_{h\to 0}C_{\nu}|x-y|^{\nu-1}=C_{\nu}|x-y|^{\nu-1}\] (174)

where the transition from the second-to-last line to the last line is similar to (163)-(165).

Therefore, we can apply Lemma 28 to \(\dot{h}_{l}\), and find a version \(\tilde{h}^{\prime}_{l}\) of \(\dot{h}_{l}\) with sample paths in \(\mathcal{CH}^{\alpha-1}(\mathcal{V}_{l})\) almost surely for all \(\alpha<\nu\), such that

\[\mathbb{E}_{f\sim\Pi_{n}}\big{\|}\tilde{h}^{\prime}_{l}\big{\|}_{\mathcal{CH }^{\alpha-1}(\mathcal{V}_{l})}^{2}\leq C_{\nu,\alpha}<+\infty\qquad\qquad \qquad\qquad\alpha<\nu.\] (175)

This gives Holder regularity of the respective derivatives: we now integrate these to obtain a Holder-regular version of the process itself. Take any \(c_{l}\in(0,1)\) and consider \(\tilde{h}_{l}=h_{l}(c_{l})+\int_{c_{l}}^{c}\dot{h}^{\prime}_{l}(t)\,\mathrm{d}t\). Then since \(\tilde{h}^{\prime}_{l}\) is almost surely in \(\mathcal{CH}^{\alpha-1}(\mathcal{V}_{l})\), \(\tilde{h}_{l}\) is has sample paths almost surely in \(\mathcal{CH}^{\alpha}(\mathcal{V}_{l})\). Moreover, it is easy to check using our previous results that \(\tilde{h}_{l}\) has an \(L^{2}(\Omega)\)-derivative given by \(\tilde{h}^{\prime}_{l}\). This implies that \(\tilde{h}_{l}\) is a version of \(h_{l}\).

To conclude the argument, we construct \(\tilde{f}\) from the obtained parts, by pulling \(\tilde{h}_{l}\) back from the charts to the manifold. Consider \(\tilde{f}=\sum_{l=1}^{L}\tilde{h}_{l}\circ\phi_{l}\). Arguing as in the case \(\nu\leq 1\), we see that \(\tilde{f}\) is a version of \(f\) with \(\mathcal{CH}^{\alpha}(\mathcal{M})\) sample paths for every \(\alpha<\nu\), and for every \(\alpha<\nu\) we have \(\mathbb{E}_{f\sim\Pi_{n}}\|\tilde{f}\|_{\mathcal{CH}^{\alpha}(\mathcal{M})}^{2} \leq C_{\alpha,\nu}<+\infty\). This gives the claim. 

With this, it is easy to prove that all Matern Gaussian processes considered in this paper can be seen as Gaussian random elements in the Banach space \(\big{(}\mathcal{C}(\mathcal{M}),\|\cdot\|_{\mathcal{C}(\mathcal{M})}\big{)}\) of continuous functions on \(\mathcal{M}\). This allows us to use the same proof scheme as in van der Vaart and van Zanten [56] through the control of the _concentration functions_ defined in Appendix B. It is also important that we work with Gaussian random elements in \(\mathcal{C}(\mathcal{M})\)--and not only with the classical notion of Gaussian process, as the concentration functions are defined using the _Gaussian random element RKHS_ defined in van Zanten and van der Vaart [59], which can potentially be different from the classical RKHS. Fortunately, when the process is a Gaussian random element in \(\mathcal{C}(\mathcal{M})\), van Zanten and van der Vaart [59], Theorem 2.1 implies that the two notions of RKHS coincide.

**Corollary 30**.: _The intrinsic Matern Gaussian processes of Definition 4, their truncated versions as in Theorem 6 as well as the extrinsic Matern Gaussian processes of Definition 7 are Gaussian random elements in \(\big{(}\mathcal{C}(\mathcal{M}),\|\cdot\|_{\mathcal{C}(\mathcal{M})}\big{)}\)._

Proof.: By Lemma 18, it suffices to show that the processes have almost surely continuous sample paths. Since Euclidean Matern Gaussian processes have continuous sample paths, this implies the same for their restrictions, the extrinsic Matern Gaussian processes on \(\mathcal{M}\). For the intrinsic Matern process, we apply Lemma 29. 

Using Lemma 29 and known properties of Euclidean Matern processes, we now show, in a sense, that all of the Matern processes presented in this paper are sub-Gaussian, in a manner which holds uniformly with respect to the truncation parameter in the case of the truncated intrinsic Matern process, and live in Holder spaces with appropriate exponents. This result is used to control Holder norms when going from the error at input locations to the \(L^{2}(p_{0})\)-error. We use the notation \(\Pi_{n}\) to emphasize that the prior depends on \(n\) when we consider a truncated intrinsic Matern process.

**Lemma 31**.: _For \(f\sim\Pi_{n}\) with \(\Pi_{n}\) the prior in either Definition 4, Theorem 6 or Definition 7, for every \(\nu>0\) and \(\gamma<\nu,\gamma\notin\mathbb{Z}_{>0}\), there exists a constant \(\sigma(f)=\sigma_{\gamma}(f)\) independent of \(n\) we have for \(x>0\) that_

\[\mathbb{P}\Big{(}\|f\|_{\mathcal{CH}^{\gamma}(\mathcal{M})}>(x+1)\sigma(f) \Big{)}\leq 2e^{-x^{2}/2}.\] (176)Proof.: We start with the restriction \(f\) of an extrinsic Matern process \(\tilde{f}\) to \(\mathcal{M}\), as in Definition 7. By van der Vaart and van Zanten [56], Section 3.1, for every \(\gamma<\nu\) we have \(\tilde{f}\in\mathcal{CH}^{\gamma}([0,1]^{D})\) almost surely. By Ghosal and van der Vaart [20], Lemma I.7, for every \(\gamma<\nu\), \(\tilde{f}\) is a Gaussian random element in the Banach space \(\mathcal{CH}^{\gamma}([0,1]^{D})\). In particular, by the Borell-TIS inequality [20, Proposition I.8] we have for \(x>0\) that

\[\mathbb{P}\Big{(}\big{\|}\tilde{f}\big{\|}_{\mathcal{CH}^{\gamma}([0,1]^{D})}> (x+1)\sigma\big{(}\tilde{f}\big{)}\Big{)}\leq 2e^{-x^{2}/2}\] (177)

where \(\sigma\big{(}\tilde{f}\big{)}=\Big{(}\mathbb{E}\,\big{\|}\tilde{f}\big{\|}_{ \mathcal{CH}^{\gamma}([0,1]^{D})}^{2}\Big{)}^{1/2}<\infty\). Since \(\mathcal{M}\) is smooth, the restriction \(f\) also satisfies for \(x>0\) the expression

\[\mathbb{P}\Big{(}\|f\|_{\mathcal{CH}^{\gamma}(\mathcal{M})}>(x+1)\sigma(f) \Big{)}\leq 2e^{-x^{2}/2}\] (178)

perhaps for a possibly larger constant \(\sigma(f)\). Finally, the case of the intrinsic Matern process \(f\sim\Pi_{n}\) truncated at \(J_{n}\in\mathbb{Z}_{>0}\cup\{\infty\}\) follows in the same manner, as we have shown in Lemma 29 that \(\sup_{n\geq 1}\mathbb{E}_{f\sim\Pi_{n}}\|f\|_{\mathcal{CH}^{\alpha}(\mathcal{M })}^{2}\leq C_{\alpha,\nu}\). 

## Appendix E Small Ball Asymptotics

Here, we bound the probability that a Matern Gaussian process lies in the \(\varepsilon\)-ball with respect to the \(L^{\infty}\)-norm for small \(\varepsilon\). For a Banach space \(\mathbb{B}\), an element \(x\in\mathbb{B}\) and a number \(r\in\mathbb{R}_{>0}\), let us denote the closed \(r\)-ball around \(x\) by \(B_{r}^{\mathbb{B}}(x)\). We start by an upper bound on the metric entropy of Sobolev balls on \(\mathcal{M}\) with respect to the uniform norm.

**Lemma 32** (Entropy of Sobolev balls).: _For any \(s>0\) define the \(\varepsilon\)-covering number of \(A\subseteq H^{s}(\mathcal{M})\) with respect to the norm \(\|\cdot\|_{L^{\infty}(\mathcal{M})}\) by_

\[N\Big{(}\varepsilon,A,\|\cdot\|_{L^{\infty}(\mathcal{M})}\Big{)}=\operatorname* {arg\,min}_{J\in\mathbb{Z}_{>0}}\bigg{\{}\exists h_{1},..,h_{J}\in A:A\subset \bigcup_{j=1}^{J}B_{\varepsilon}^{L^{\infty}(\mathcal{M})}(h_{j})\bigg{\}}.\] (179)

_Then for any \(s>d/2\), there exist \(C,\varepsilon_{0}>0\) such that for every \(\varepsilon\leq\varepsilon_{0}\)_

\[\ln N\Big{(}\varepsilon,B_{1}^{H^{s}(\mathcal{M})}(0),\|\cdot\|_{L^{\infty}( \mathcal{M})}\Big{)}\leq C\varepsilon^{-\frac{d}{s}},\] (180)

_where the left-hand side of the inequality above, as a function of \(\varepsilon\), is called the metric entropy of the Sobolev ball \(B_{1}^{H^{s}(\mathcal{M})}(0)\) with respect to the uniform norm \(\|\cdot\|_{L^{\infty}(\mathcal{M})}\)._

Proof.: Using charts we will reduce the problem to the entropy of the unit ball of the Sobolev space \(H^{s}\big{(}[0,1]^{d}\big{)}\) for which the upper bound is known. Take \(f\in B_{1}^{H^{s}(\mathcal{M})}(0)\) and consider approximations of \(f\) by \(g\) of the form

\[g=\sum_{l=1}^{L}\chi_{l}(g_{l}\circ\phi_{l})\] (181)

for some functions \(g_{l}:\mathcal{V}_{l}\to\mathbb{R}\) where \(\mathcal{V}_{l}\subseteq\mathbb{R}^{d}\). We have

\[\big{\|}f-g\big{\|}_{L^{\infty}(\mathcal{M})} =\big{\|}\sum_{l=1}^{L}\chi_{l}(g_{l}\circ\phi_{l}-f)\big{\|}_{L^{ \infty}(\mathcal{M})}\leq\sum_{l=1}^{L}\big{\|}\chi_{l}(g_{l}\circ\phi_{l}-f) \big{\|}_{L^{\infty}(\mathcal{U}_{l})}\] (182) \[\leq\sum_{l=1}^{L}\big{\|}g_{l}\circ\phi_{l}-f\big{\|}_{L^{\infty} (\mathcal{U}_{l})}\leq\sum_{l=1}^{L}\big{\|}g_{l}-f\circ\phi_{l}^{-1}\big{\|}_{L ^{\infty}(\mathcal{V}_{l})}\] (183) \[\leq L\max_{1\leq l\leq L}\big{\|}g_{l}-f\circ\phi_{l}^{-1}\big{\|} _{L^{\infty}([0,1]^{d})}.\] (184)

This means that to approximate \(f\) by \(g\) uniformly on \(\mathcal{M}\), we need to choose the functions \(g_{l}\) that approximate \(f\circ\phi_{l}^{-1}\) well with respect to the uniform norm on \([0,1]^{d}\).

Next, we show that the functions \(f\circ\phi_{l}^{-1}\) are contained in an Euclidean Sobolev ball of radius \(R\), with \(R\) depending only on \(\nu\) and the atlas. To do this we use Grosse and Schneider [24], Lemma 2.1 to write

\[\Big{\|}f\circ\phi_{l}^{-1}\Big{\|}_{H^{s}([0,1]^{d})} =\Big{\|}\sum_{l^{\prime}=1}^{L}(\chi_{l^{\prime}}f)\circ\phi_{l} ^{-1}\Big{\|}_{H^{s}([0,1]^{d})}\leq\sum_{l^{\prime}=1}^{L}\Big{\|}(\chi_{l^{ \prime}}f)\circ\phi_{l}^{-1}\Big{\|}_{H^{s}([0,1]^{d})}\] (185) \[=\sum_{l^{\prime}=1}^{L}\Big{\|}(\chi_{l^{\prime}}f)\circ\phi_{l^ {\prime}}^{-1}\circ\phi_{l^{\prime}}\circ\phi_{l}^{-1}\Big{\|}_{H^{s}([0,1]^{ d})}\] (186) \[\lesssim\sum_{l^{\prime}=1}^{L}\Big{\|}(\chi_{l^{\prime}}f)\circ \phi_{l^{\prime}}^{-1}\Big{\|}_{H^{s}([0,1]^{d})}\lesssim\|f\|_{H^{s}(\mathcal{ M})}.\] (187)

Note, importantly, that the remark just above Grosse and Schneider [24], Lemma 2.1 allows us to consider Besov spaces \(B^{s}_{2,2}\) coinciding with the Sobolev spaces \(H^{s}\) instead of the Besov spaces \(B^{s}_{2,\infty}\)--to get from the second line to the third. Note also that the constant hidden behind the notation \(\lesssim\) in the last line where we use Theorem 16 is the radius \(R\). Without loss of generality we assume \(R=1\). By the Euclidean counterpart of the result we are proving [21, Theorem 4.3.36], we have

\[\ln N\Big{(}\varepsilon,B_{1}^{H^{s}[0,1]^{d}}(0),\|\cdot\|_{L^{\infty}([0,1]^ {d})}\Big{)}\lesssim\varepsilon^{-\frac{d}{s}}.\] (188)

Let \(h_{1},..,h_{J}\in B_{1}^{H^{s}([0,1]^{d})}(0)\) be such that \(B_{1}^{H^{s}([0,1]^{d})}(0)\subset\bigcup_{j=1}^{J}B_{\varepsilon/L}^{L^{ \infty}([0,1]^{d})}(h_{k})\). Then for any \(f\in B_{1}^{H^{s}(\mathcal{M})}(0)\) there exists a sequence \(\{j_{l}\}_{l=1}^{L}\subseteq\{1,..,J\}\) such that

\[\big{\|}f-\sum_{l=1}^{L}\chi_{l}(h_{j_{l}}\circ\phi_{l})\big{\|}_{L^{\infty}( \mathcal{M})}<L\frac{\varepsilon}{L}=\varepsilon.\] (189)

This shows that \(N\big{(}\varepsilon,B_{1}^{H^{s}(\mathcal{M})}(0),\|\cdot\|_{L^{\infty}( \mathcal{M})}\big{)}\leq LJ\), where \(L\) is just the number of charts, thereby proving the claim. 

For the related _diffusion spaces_[16], the RKHS corresponding to the heat (diffusion) kernels, Castillo et al. [12] uses the results of Coulhon et al. [14] to bound the entropy in terms of a wavelet frame instead of relying on charts. We believe this alternative proof scheme should work in our case as well. However, we could not, to the best of our effort, get a tight-enough bound for the Sobolev spaces by directly using the results of Coulhon et al. [14] and therefore we chose to rely on charts instead.

Having established regularity properties for our prior processes, we now turn to the _small ball problem_: we want to find sharp lower bounds on \(\mathbb{P}(\|f\|_{L^{\infty}(\mathcal{M})}<\varepsilon)\) where \(f\sim\Pi\) is our prior process. This will be crucial in order to control the _concentration functions_ used in Appendix B. In fact, it is well-known that this problem is closely related to the estimation of the metric entropy of the unit ball of the RKHS of \(f\) with respect to the uniform norm: see Li and Linde [31] for details. Since we have already characterized the RKHS of our processes in Proposition 27 and Lemma 24, we are able to lower bound the small-ball probabilities. The technicality here involves getting a bound uniform in the truncation parameter for the truncated Matern process, as the truncated Matern process is a sequence of priors rather than a fixed prior.

**Lemma 33**.: _If \(f\sim\Pi_{n}\) where \(\Pi_{n}\) is the prior in either Definition 4 and Theorem 6 or Definition 7 with smoothness parameter \(\nu>d/2\), then there exist two constants \(C,\varepsilon_{0}>0\) that do not depend on \(n\) such that for all \(\varepsilon\leq\varepsilon_{0}\) we have \(-\ln\mathbb{P}\Big{(}\|f\|_{L^{\infty}(\mathcal{M})}<\varepsilon\Big{)}\leq C \varepsilon^{-\frac{d}{\nu}}\)._

Proof.: Because the processes are Gaussian random elements in \(\mathcal{C}(\mathcal{M})\) by Lemma 18, their stochastic process RKHS given by Proposition 27 coincide with their _Gaussian random element RKHS_ defined in van Zanten and van der Vaart [59]. Hence, for the non-truncated intrinsic and the extrinsic Matern processes, the result follows by a direct application of Lemma 32 and Li and Linde [31], Theorem 1.2. For the intrinsic Matern process truncated at \(J_{n}\) it is not immediately clear that the constants \(C,\varepsilon_{0}\) can be taken independent of \(n\), so we go through the proof of Li and Linde [31], Proposition 3.1 to see that this is, in fact, the case. We first need a crude upper bound of the form

\[-\ln\mathbb{P}\Big{(}\|f\|_{L^{\infty}(\mathcal{M})}<\varepsilon\Big{)}\leq c \varepsilon^{-c}\] (190)

[MISSING_PAGE_FAIL:33]

Expressions for Pointwise Worst-case Errors

Let \(k\) be a kernel on some abstract input domain \(\mathcal{X}\), and let \(\mathbb{H}_{k}\) be the respective RKHS. Consider \(n\) input values \(\mathbf{X}\subseteq\mathcal{X}\) and let \(\sigma_{\varepsilon}^{2}>0\) be the noise variance. Define

\[m_{k,\mathbf{X},f,\varepsilon}(t) =\mathbf{K}_{t\mathbf{X}}(\mathbf{K}_{\mathbf{X}\mathbf{X}}+ \sigma_{\varepsilon}^{2}\mathbf{I})^{-1}(f(\mathbf{X})+\varepsilon),\] (202) \[v^{\mathrm{(i)}}(t) =v_{k,\mathbf{X}}(t)=k(t,t)-\mathbf{K}_{t\mathbf{X}}\big{(} \mathbf{K}_{\mathbf{X}\mathbf{X}}+\sigma_{\varepsilon}^{2}\mathbf{I}\big{)}^{ -1}\mathbf{K}_{\mathbf{X}t}.\] (203)

**Proposition 34**.: _With notation above_

\[v^{\mathrm{(i)}}(t)=\sup_{f\in\mathcal{H}_{k},\left\|f\right\|_{\mathbb{H}_{ k}}\leq 1}\mathbb{E}_{\bm{\varepsilon}\sim\mathrm{N}(\bm{0},\sigma_{ \varepsilon}^{2}\mathbf{I})}{\left|f(t)-m_{k,\mathbf{X},f,\varepsilon}(t) \right|^{2}}.\] (204)

Proof.: To simplify notation, we shorten \(\mathbb{E}_{\bm{\varepsilon}\sim\mathrm{N}(\bm{0},\sigma_{\varepsilon}^{2} \mathbf{I})}\) to \(\mathbb{E}\) and denote \(\bm{\alpha}=\mathbf{K}_{t\mathbf{X}}(\mathbf{K}_{\mathbf{X}\mathbf{X}}+ \sigma_{\varepsilon}^{2}\mathbf{I})^{-1}\). First of all, by direct computation,

\[\mathbb{E}\,m_{k,\mathbf{X},f,\varepsilon}(t) =\bm{\alpha}f(\mathbf{X}),\] (205) \[\mathbb{E}\,m_{k,\mathbf{X},f,\varepsilon}(t)^{2} =\bm{\alpha}f(\mathbf{X})f(\mathbf{X})^{\top}\bm{\alpha}^{\top}+ \sigma_{\varepsilon}^{2}\bm{\alpha}\bm{\alpha}^{\top}.\] (206)

Write

\[\mathbb{E}{\left|f(t)-m_{k,\mathbf{X},f,\varepsilon}(t)\right|^{2} =f(t)^{2}-2f(t)\,\mathbb{E}\,m_{k,\mathbf{X},f,\varepsilon}(t)+ \mathbb{E}\,m_{k,\mathbf{X},f,\varepsilon}(t)^{2}\] (207) \[=f(t)^{2}-2f(t)\bm{\alpha}f(\mathbf{X})+\bm{\alpha}f(\mathbf{X}) f(\mathbf{X})^{\top}\bm{\alpha}^{\top}+\sigma_{\varepsilon}^{2}\bm{\alpha}\bm{ \alpha}^{\top}\] (208) \[=(f(t)-\bm{\alpha}f(\mathbf{X}))^{2}+\sigma_{\varepsilon}^{2}\bm {\alpha}\bm{\alpha}^{\top}\] (209) \[=\left\langle k(t,\cdot)-\sum_{j=1}^{n}\alpha_{j}k(x_{j},\cdot),f \right\rangle_{\mathbb{H}_{k}}^{2}+\sigma_{\varepsilon}^{2}\bm{\alpha}\bm{ \alpha}^{\top}.\] (210)

As \(\left\|g\right\|_{\mathbb{H}_{k}}=\sup_{f\in\mathbb{H}_{k},\left\|f\right\|_{ \mathbb{H}_{k}}\leq 1}\langle g,f\rangle_{\mathbb{H}_{k}}\), implying \(\sup_{f\in\mathbb{H}_{k},\left\|f\right\|_{\mathbb{H}_{k}}\leq 1}\langle g,f \rangle_{\mathbb{H}_{k}}^{2}=\left\|g\right\|_{\mathbb{H}_{k}}^{2}\), we have

\[\sup_{\begin{subarray}{c}f\in\mathcal{H}_{k}\\ \left\|f\right\|_{\mathbb{H}_{k}}\leq 1\end{subarray}}\mathbb{E}{\left|f(t)-m_{k, \mathbf{X},f,\varepsilon}(t)\right|^{2}} =\left\|k(t,\cdot)-\sum_{j=1}^{n}\alpha_{j}k(x_{j},\cdot)\right\| _{\mathbb{H}_{k}}^{2}+\sigma_{\varepsilon}^{2}\bm{\alpha}\bm{\alpha}^{\top}\] (211) \[=k(t,t)-2\bm{\alpha}\mathbf{K}_{\mathbf{X}t}+\underbrace{\bm{ \alpha}\mathbf{K}_{\mathbf{X}\mathbf{X}}\bm{\alpha}^{\top}+\sigma_{ \varepsilon}^{2}\bm{\alpha}\bm{\alpha}^{\top}}_{\bm{\alpha}\mathbf{K}_{ \mathbf{X}t}}\] (212) \[=k(t,t)-\bm{\alpha}\mathbf{K}_{\mathbf{X}t}=\underbrace{k(t,t)- \mathbf{K}_{t\mathbf{X}}(\mathbf{K}_{\mathbf{X}\mathbf{X}}+\sigma_{ \varepsilon}^{2}\mathbf{I})^{-1}\mathbf{K}_{\mathbf{X}t}}_{v_{k,\mathbf{X}}(t )}.\] (213)

We now move to the misspecified case. Consider the RKHS \(\mathcal{H}_{c}\) for some other kernel \(c:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) instead of \(\mathbb{H}_{k}\). Then, continuing from (210), write

\[\sup_{\begin{subarray}{c}f\in\mathcal{H}_{c}\\ \left\|f\right\|_{\mathbb{H}_{k}}\leq 1\end{subarray}}\mathbb{E}{\left|f(t)-m_{k, \mathbf{X},f,\varepsilon}(t)\right|^{2}}=\left\|c(t,\cdot)-\sum_{j=1}^{n} \alpha_{j}c(x_{j},\cdot)\right\|_{\mathbb{H}_{c}}^{2}+\sigma_{\varepsilon}^{2} \bm{\alpha}\bm{\alpha}^{\top}.\] (214)

The next question is how to compute the norm on the right-hand side. There is not much hope of doing this exactly in the misspecified case: thus, we consider approximations. To this end, we take some large set of locations \(\mathbf{X}^{\prime}\subseteq\mathcal{X}\). Then we use \(\left\|g\right\|_{\mathbb{H}_{c}}^{2}\approx g(\mathbf{X}^{\prime})^{\top} \mathbf{C}_{\mathbf{X}^{\prime}\mathbf{X}^{\prime}}^{-1}g(\mathbf{X}^{\prime})\) for \(g(\cdot)=c(t,\cdot)-\sum_{j=1}^{n}\alpha_{j}c(x_{j},\cdot)\). As a result, we obtain the approximation

\[\sup_{\begin{subarray}{c}f\in\mathcal{H}_{c}\\ \left\|f\right\|_{\mathbb{H}_{c}}\leq 1\end{subarray}}\mathbb{E}{\left|f(t)-m_{k, \mathbf{X},f,\varepsilon}(t)\right|^{2}}\approx g(\mathbf{X}^{\prime})^{\top} \mathbf{C}_{\mathbf{X}^{\prime}\mathbf{X}^{\prime}}^{-1}g(\mathbf{X}^{\prime})+ \sigma_{\varepsilon}^{2}\bm{\alpha}\bm{\alpha}^{\top}=\tilde{v}_{k,c,\mathbf{X} }(t)=v^{\mathrm{(e)}}(t)\] (215)

where \(v^{\mathrm{(e)}}(t)\) was first introduced in Section 4To compute spatial averages of this quantity, let \(g_{t}(\cdot)=c(t,\cdot)-\sum_{j=1}^{n}\alpha_{j}c(x_{j},\cdot)\), the same as \(g\) before, but now with explicit dependence on \(t\). Similarly, put \(\bm{\alpha}_{t}=\mathbf{K}_{t\mathbf{X}}(\mathbf{K}_{\mathbf{X}\mathbf{X}}+ \sigma_{\varepsilon}^{2}\mathbf{I})^{-1}\). Then

\[g_{t}(\mathbf{X}^{\prime}) =\mathbf{C}_{\mathbf{X}^{\prime}\,t}-\mathbf{C}_{\mathbf{X}^{ \prime}\,\mathbf{X}}\bm{\alpha}_{t}^{\top}=\mathbf{C}_{\mathbf{X}^{\prime}\,t }-\mathbf{C}_{\mathbf{X}^{\prime}\,\mathbf{X}}(\mathbf{K}_{\mathbf{X}\mathbf{X }}+\sigma_{\varepsilon}^{2}\mathbf{I})^{-1}\mathbf{K}_{\mathbf{X}t}\] (216) \[g_{t}(\mathbf{X}^{\prime})^{\top}\mathbf{C}_{\mathbf{X}^{\prime }\mathbf{X}^{\prime}}^{-1}g_{t}(\mathbf{X}^{\prime}) =(\mathbf{C}_{t\,\mathbf{X}^{\prime}}-\bm{\alpha}_{t}\mathbf{C}_{ \mathbf{X}\,\mathbf{X}^{\prime}})\mathbf{C}_{\mathbf{X}^{\prime}\mathbf{X}^{ \prime}}^{-1}\big{(}\mathbf{C}_{\mathbf{X}^{\prime}\,t}-\mathbf{C}_{\mathbf{X }^{\prime}\,\mathbf{X}}\bm{\alpha}_{t}^{\top}\big{)}.\] (217)

From here we can also deduce that

\[\frac{1}{|\mathbf{X}^{\prime}|}\sum_{t\in\mathbf{X}^{\prime}} \tilde{v}_{k,c,\mathbf{X}}(t) =\frac{1}{|\mathbf{X}^{\prime}|}\sum_{t\in\mathbf{X}^{\prime}}g_{ t}(\mathbf{X}^{\prime})^{\top}\mathbf{C}_{\mathbf{X}^{\prime}\mathbf{X}^{ \prime}}^{-1}g_{t}(\mathbf{X}^{\prime})\] (218) \[=\frac{1}{|\mathbf{X}^{\prime}|}\operatorname{tr}\bigl{(}g_{ \mathbf{X}^{\prime}}(\mathbf{X}^{\prime})^{\top}\mathbf{C}_{\mathbf{X}^{ \prime}\mathbf{X}^{\prime}}^{-1}g_{\mathbf{X}^{\prime}}(\mathbf{X}^{\prime}) \bigr{)}\] (219)

where \(g_{\mathbf{X}^{\prime}}(\mathbf{X}^{\prime})=\mathbf{C}_{\mathbf{X}^{\prime} \,\mathbf{X}^{\prime}}-\mathbf{C}_{\mathbf{X}^{\prime}\,\mathbf{X}}(\mathbf{K} _{\mathbf{X}\mathbf{X}}+\sigma_{\varepsilon}^{2}\mathbf{I})^{-1}\mathbf{K}_{ \mathbf{X}\mathbf{X}^{\prime}}\).

## Appendix G Full Experimental Details

All of our kernels were computed using GPJax[38] and the geometric kernels library.10 We use three manifolds, each represented by a mesh: (i) a dumbbell-shaped manifold represented as a mesh with \(1556\) nodes, (ii) a sphere represented by an icosahedral mesh with \(2562\) nodes, and (iii) the Stanford dragon mesh, preprocessed to keep only its largest connected component, which has \(100179\) nodes. For the sphere, we also considered a finer icosahedral mesh with \(10242\) nodes, but this was found to have virtually no effect on the computed pointwise expected errors.

Footnote 10: See https://Gpjax.readthedocs.io and https://geometric-kernels.github.io.

We use extrinsic Matern and Riemannian Matern kernels with the following hyperparameters: \(\sigma_{f}^{2}=1\) and \(\sigma_{e}^{2}=0.0005\). For the truncated Karhunen-Loeve expansion, we used \(J=500\) eigenpairs obtained from the mesh. We selected smoothness values to ensure norm-equivalence of the intrinsic and extrinsic kernels' reproducing kernel Hilbert spaces, which was \(\nu=5/2\) for the intrinsic model, and \(\nu=5/2+d/2\) for the extrinsic model, where \(d\) is the manifold's dimension. We used different length scales for each manifold: \(\kappa=200\) for the dumbbell, \(\kappa=0.25\) for the sphere, and \(\kappa=0.05\) for the dragon, selected to ensure that the Gaussian processes were neither approximately constant, nor white-noise-like. We considered data sizes of \(N=50\), \(N=500\), and \(N=1000\), respectively, for the dumbbell, sphere, and dragon, sampled uniformly from the mesh's nodes, which in each case resulted in a reasonably-uniform distribution of points across the manifold. Finally, for the extrinsic pointwise error approximation, we used a subset \(\mathbf{X}^{\prime}\) uniformly sampled from each mesh's nodes, of size equal to the data size. For each respective test set, we used the full mesh. Each experiment was repeated for \(10\) different seeds.

To set the length scales for the extrinsic process, we used maximum marginal likelihood optimization on the full data, except for the dumbbell whose full data size is small and for which we instead generated a larger set consisting of \(500\) points. We optimzied only the length scale, leaving all other hyperparameters fixed. We used ADAM with a learning rate of \(0.005\), and an initialization equal to the length scale \(\kappa\) of the intrinsic model, except for the dumbbell where this lead to divergence and we instead used an initial value of \(\kappa/4\). We ran the optimizer for a total of \(1000\) steps. With these settings, we found empirically that maximum marginal likelihood optimization always converged.