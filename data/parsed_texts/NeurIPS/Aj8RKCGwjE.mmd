# AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields

Louis Serrano\({}^{1}\)  Thomas X Wang\({}^{1}\)  Etienne Le Naour\({}^{1,2}\)

Jean-Noel Vittaut\({}^{3}\)  Patrick Gallinari\({}^{1,4}\)

\({}^{1}\)Sorbonne Universite, CNRS, ISIR, 75005 Paris, France

\({}^{2}\)EDF R&D, Palaiseau, France

\({}^{3}\)Sorbonne Universite, CNRS, LIP6, 75005 Paris, France

\({}^{4}\)Criteo AI Lab, Paris, France

Corresponding author: louis.serrano@isir.upmc.fr

###### Abstract

We present AROMA (Attentive Reduced Order Model with Attention), a framework designed to enhance the modeling of partial differential equations (PDEs) using local neural fields. Our flexible encoder-decoder architecture can obtain smooth latent representations of spatial physical fields from a variety of data types, including irregular-grid inputs and point clouds. This versatility eliminates the need for patching and allows efficient processing of diverse geometries. The sequential nature of our latent representation can be interpreted spatially and permits the use of a conditional transformer for modeling the temporal dynamics of PDEs. By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training. AROMA's superior performance in simulating 1D and 2D equations underscores the efficacy of our approach in capturing complex dynamical behaviors. _Github page_: https://github.com/LouisSerrano/aroma

## 1 Introduction

In recent years, many deep learning (DL) surrogate models have been introduced to approximate solutions to partial differential equations (PDEs) (Lu et al., 2021; Li et al., 2021; Brandstetter et al., 2022; Stachenfeld et al., 2022). Among these, the family of neural operators has been extensively adopted and tested across various scientific domains, demonstrating the potential of data-centric DL models in science (Pathak et al., 2022; Vinuesa & Brunton, 2022).

Neural Operators were initially constrained by discretization and domain geometry limitations. Recent advancements, such as neural fields (Yin et al., 2022; Serrano et al., 2023) and transformer architectures (Li et al., 2023; Hao et al., 2023), have partially addressed these issues, improving both dynamic modeling and steady-state settings. However, Neural Fields struggle to model spatial information and local dynamics effectively, and existing transformer architectures, while being flexible, are computationally expensive due to their operation in the original physical space and require large training datasets.

Our hypothesis is that considering spatiality is essential in modeling spatio-temporal phenomena, yet applying attention mechanisms directly is computationally expensive. We propose a new framework that models the dynamics in a reduced latent space, encoding spatial information compactly, by one or two orders of magnitude relative to the original space. This approach addresses both the complexity issues of transformer architectures and the spatiality challenges of Neural Fields.

Our novel framework leverages attention blocks and neural fields, resulting in a model that is easy to train and achieves state-of-the-art results on most datasets, particularly for complex geometries, without requiring prior feature engineering. To the best of our knowledge, we are the first to propose a fully attention-based architecture for processing domain geometries and unrolling dynamics. Compared to existing transformer architectures for PDEs, our framework first encapsulates the domain geometry and observation values in a compact latent representation, efficiently forecasting the dynamics at a lower computational cost. Transformer-based methods such as (Li et al., 2023; Hao et al., 2023) unroll the dynamics in the original space, leading to high complexity.

Our contributions are summarized as follows:

* A principled and versatile encode-process-decode framework for solving PDEs that operate on general input geometries, including point sets, grids, or meshes, and can be queried at any location within the spatial domain.
* A new spatial encode / process / decode approach: Variable-size inputs are mapped onto a fixed-size compact latent token space that encodes local spatial information. This latent representation is further processed by a transformer architecture that models the dynamics while exploiting spatial relations both at the local token level and globally across tokens. The decoding exploits a conditional neural field, allowing us to query forecast values at any point in the spatial domain of the equation.
* We include stochastic components at the encoding and processing levels to enhance stability and forecasting accuracy.
* Experiments performed on representative spatio-temporal forecasting problems demonstrate that AROMA is on par with or outperforms state-of-the-art baselines in terms of both accuracy and complexity.

## 2 Problem setting

In this paper, we focus on time-dependent PDEs defined over a spatial domain \(\Omega\) (with boundary \(\partial\Omega\)) and temporal domain \([0,T]\). In the general form, their solutions \(\bm{u}(x,t)\) satisfy the following constraints :

\[\frac{\partial\bm{u}}{\partial t} =F\left(\nu,t,x,\bm{u},\frac{\partial\bm{u}}{\partial x},\frac{ \partial^{2}\bm{u}}{\partial x^{2}},\dots\right),\quad\forall x\in\Omega, \forall t\in(0,T]\] (1) \[\mathcal{B}(\bm{u})(t,x) =0\quad\forall x\in\partial\Omega,\forall t\in(0,T]\] (2) \[\bm{u}(0,x) =\bm{u}^{0}\quad\forall x\in\Omega\] (3)

where \(\nu\) represents a set of PDE coefficients, Equations (2) and (3) represent the constraints with respect to the boundary and initial conditions. We aim to learn, using solutions data obtained with classical solvers, the evolution operator \(\mathcal{G}\) that predicts the state of the system at the next time step: \(\bm{u}^{t+\Delta t}=\mathcal{G}(\bm{u}^{t})\). We have access to training trajectories obtained with different initial conditions, and we want to generate accurate trajectory rollouts for new initial conditions at test time. A rollout is obtained by the iterative application of the evolution operator \(\bm{u}^{m\Delta t}=\mathcal{G}^{m}(\bm{u}^{0})\).

## 3 Model Description

### Model overview

We provide below an overview of the global framework and each component is described in a subsequent section. The model comprises three key components, as detailed in Figure 1.

* **Encoder**\(\mathcal{E}_{w}:\bm{u}^{t}_{\mathcal{X}}\to\bm{Z}^{t}\). The encoder takes input values \(\bm{u}^{t}_{\mathcal{X}}\) sampled over the domain \(\Omega\) at time \(t\), where \(\mathcal{X}\) denotes the discrete sample space and could be a grid, an irregular mesh or a point set. \(\bm{u}^{t}_{\mathcal{X}}\) is observed at locations \(\bm{x}\) = (\(x_{1}\),...\(x_{N}\)), with values \(\bm{u}^{t}=(\bm{u}^{t}(\bm{x}_{1}),\cdots,\bm{u}^{t}(\bm{x}_{N}))\). \(N\) is the number of observations and can vary across samples. \(\bm{u}^{t}_{\mathcal{X}}\) is projected through a cross attention mechanism onto a set of \(M\) tokens \(\bm{Z}^{t}=(\bm{z}^{t}_{1},\cdots,\bm{z}^{t}_{M})\) with \(M\) a fixed parameter. This allowsmapping any discretized input \(\bm{u}_{\mathcal{X}}^{t}\) onto a fixed dimensional latent representation \(\bm{Z}^{t}\) encoding implicit local spatial information from the input domain. The encoder is trained as a VAE and \(\bm{Z}^{t}\) is sampled from a multivariate normal statistics as detailed in Section 3.2.
* **Latent time-marching refiner \(\mathcal{R}_{\theta}:\bm{Z}^{t}\rightarrow\hat{\bm{Z}}^{t+\Delta t}\).** We model the dynamics in the latent space through a transformer. The dynamics can be unrolled auto-regressively in the latent space for any time horizon without requiring to project back in the original domain \(\Omega\). Self-attention operates on the latent tokens, which allows modeling global spatial relations between the local token representations. The transformer is enriched with a conditional diffusion mechanism operating between two successive time steps of the transformer. We experimentally observed that this probabilistic model was more robust than a baseline deterministic transformer for temporal extrapolation.
* **Decoder \(\mathcal{D}_{\psi}:\hat{\bm{Z}}^{t+\Delta t}\rightarrow\hat{\bm{u}}^{t+\Delta t}\).** The decoder uses the latent tokens \(\hat{\bm{Z}}^{t+\Delta t}\) to approximate the function value \(\hat{\bm{u}}^{t+\Delta t}(x)=\mathcal{D}_{\psi}(x,\hat{\bm{Z}}^{t+\Delta t})\) for any query coordinate \(x\in\Omega\). We therefore denote \(\hat{\bm{u}}^{t+\Delta t}=\mathcal{D}_{\psi}(\bm{Z}^{t+\Delta t})\) the predicted function.

InferenceWe encode the initial condition and unroll the dynamics in the latent space by successive denoisings: \(\hat{\bm{u}}^{m\Delta t}=\mathcal{D}_{\psi}\circ\mathcal{R}_{\theta}^{m}\circ \mathcal{E}_{w}(\bm{u}^{0})\). We then decode along the trajectory to get the reconstructions. We outline the full inference pipeline in Figure 1 and detail its complexity analysis in Appendix C.1.

TrainingWe perform a two-stage training: we first train the encoder and decoder, secondly train the refiner. This is more stable than end-to-end training.

### Encoder-decoder description

The encoder-decoder components are jointly trained using a VAE setting. The encoder is specifically designed to capture local input observation from any sampled point set in the spatial domain and encodes this information into a fixed number of tokens. The decoder can be queried at any position in the spatial domain, irrespective of the input sample.

EncoderThe encoder maps an arbitrary number \(N\) of observations \((\bm{x},\bm{u}(\bm{x})):=((x_{1},\bm{u}(x_{1})),\ldots,(x_{N},\bm{u}(x_{N}))\) onto a latent representation \(\bm{Z}\) of fixed size \(M\) through the following series of transformations:

Figure 1: **AROMA inference**: The discretization-free encoder compresses the information of a set of \(N\) input values to a sequence of \(M\) latent tokens, where \(M<N\). The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token \(Z^{t}\) represents \(u_{t}\) and \(Z^{t+\Delta t}\) is the prediction corresponding to \(u_{t+\Delta t}\).

where \((\bm{\gamma}(\bm{x}),\bm{v}(\bm{x}))=((\gamma(x_{1}),v(x_{1})),\ldots,(\gamma(x_{N }),v(x_{N})))\), and \(h\ll d\).

**(i) Embed positions and observations**: Given an input sequence of coordinate-value pairs \((x_{1},\bm{u}(x_{1})),\ldots,(x_{N},\bm{u}(x_{N}))\), we construct sequences of positional embeddings \(\bm{\gamma}=(\gamma(x_{1}),\ldots,\gamma(x_{N}))\) and value embeddings \(\bm{v}=(v(x_{1}),\ldots,v(x_{N}))\), where \(\gamma(x)=\textsc{FourierFeatures}(x;\omega)\) and \(v(x)=\textsc{Linear}(\bm{u}(x))\), with \(\omega\) a fixed set of frequencies. These embeddings are aggregated onto a smaller set of learnable query tokens \(\mathbf{T}=(T_{1},\ldots,T_{M})\) and then \(\mathbf{T}^{\prime}=(T_{1}^{\prime},\ldots,T_{M}^{\prime})\) with \(M\) fixed, to compress the information and encode the geometry and spatial latent representations.

**(ii) Encode geometry**: Geometry-aware tokens \(\mathbf{T}\) are obtained with a multihead cross-attention layer and a feedforward network (FFN), expressed as \(\mathbf{T}^{\text{geo}}=\mathbf{T}+\textsc{FFN}(\textsc{CrossAttention}( \mathbf{Q}=\mathbf{W}_{Q}\mathbf{T},\mathbf{K}=\mathbf{W}_{K}\bm{\gamma}, \mathbf{V}=\mathbf{W}_{V}\bm{\gamma}))\). This step does not include information on the observations, ensuring that similar geometries yield similar query tokens \(\mathbf{T}^{\text{geo}}\) irrespective of the \(\bm{u}\) values.

**(iii) Encode observations**: The \(\mathbf{T}^{\text{geo}}\) tokens are then used to aggregate the observation values via a cross-attention mechanism: \(\mathbf{T}^{\text{obs}}=\mathbf{T}^{\text{geo}}+\textsc{FFN}(\textsc{CrossAttention}( \mathbf{Q}=\mathbf{W}_{Q}^{\prime}\mathbf{T}^{\text{geo}},\mathbf{K}=\mathbf{ W}_{K}^{\prime}\bm{\gamma},\mathbf{V}=\mathbf{W}_{V}^{\prime}\bm{v}))\). Here, the values contain information on the observation values, and the keys contain information on the observation locations.

**(iv) Reduce channel dimension and sample \(\bm{Z}\)**: The information in the channel dimension of \(\mathbf{T}^{\prime}\) is compressed using a bottleneck linear layer. To avoid exploding variance in this compressed latent space, we regularize it with a penalty on the \(L_{2}\) norm of the latent code \(\|\bm{Z}\|^{2}\). Introducing stochasticity through a variational formulation further helps to regularize the auto-encoding and obtain smoother representations for the forecasting step. For this, we learn the components of a Gaussian multivariate distribution \(\bm{\mu}=\textsc{Linear}(\mathbf{T}^{\text{obs}})\) and \(\log(\bm{\sigma})=\textsc{Linear}(\mathbf{T}^{\text{obs}})\) from which the final token embedding \(\bm{Z}\) is sampled.

DecoderThe decoder's role is to reconstruct \(\hat{\bm{u}}^{t+\Delta t}\) from \(\hat{\bm{Z}}^{t+\Delta t}\), see Figure 1. Since training is performed in two steps ("encode-decode" first and then "process"), the decoder is trained to reconstruct \(\hat{\bm{u}}^{t}\) for input \(\bm{u}^{t}\). One proceeds as follows. **(i) Increase channel dimensions and apply self-attention**: The decoder first lifts the latent tokens \(\bm{Z}\) to a higher channel dimension (this is the reverse operation of the one performed by the encoder) and then apply several layers of self-attention to get tokens \(\bm{Z}^{{}^{\prime}}\). **(ii) Cross-attend**: The decoder applies cross-attention to obtain feature vectors that depend on the query coordinate \(x\), \((\mathbf{f}_{q}^{u}(x))=\textsc{CrossAttention}(\mathbf{Q}=\mathbf{W}_{Q}( \gamma_{q}(x)),\mathbf{K}=\mathbf{W}_{K}\bm{Z}^{{}^{\prime}},\mathbf{V}= \mathbf{W}_{V}\bm{Z}^{{}^{\prime}})\), where \(\gamma_{q}\) is a Fourier features embedding of bandwidth \(\omega_{q}\). **(iii) Decode with MLP**: Finally, we use a small MLP to decode this feature vector and obtain the reconstruction \(\hat{\bm{u}}(x)=\text{MLP}(\mathbf{f}_{q}^{u}(x))\). In contrast with existing neural field methods for dynamics modeling, the feature vector here is local. In practice, one uses multiple cross attentions to get feature vectors with different frequencies (see Appendix Figures 7 and 8 for further details).

TrainingThe encoder and decoder are jointly optimized as a variational autoencoder (VAE) (Kingma & Welling, 2013) to minimize the following objective : \(\mathcal{L}=\mathcal{L}_{\text{recon}}+\beta\cdot\mathcal{L}_{KL}\); where \(\mathcal{L}_{\text{recon}}=\text{MSE}(u_{\mathcal{X}}^{t},\hat{u}_{\mathcal{X}} ^{t})\) is the reconstruction loss between the input and the reconstruction \(\mathcal{D}_{\psi}(\bm{Z}^{t},\mathcal{X})\) on the grid \(\mathcal{X}\), with \(\bm{Z}^{t}\sim\mathcal{N}(\bm{\mu}^{t},(\bm{\sigma}^{t})^{2})\) and \(\bm{\mu}^{t},\bm{\sigma}^{t}=\mathcal{E}_{w}(u_{\mathcal{X}}^{t})\). The KL divergence loss \(\mathcal{L}_{\text{KL}}=D_{\text{KL}}(\mathcal{N}(\bm{\mu}^{t},(\bm{\sigma}^{t}) ^{2})\,||\mathcal{N}(0,I))\) helps regularize the network and prevents overfitting. We found that using a variational formulation was essential to obtain smooth latent representations while training the encoder-decoder.

### Transformer-based diffusion

Modeling the dynamics is performed in the latent \(\bm{Z}\) space. This space encodes spatial information present in the original space while being a condensed, smaller-sized representation, allowing for reduced complexity dynamics modeling. As indicated, the dynamics can be unrolled auto-regressively in this space for any time horizon without the need to map back to the original space. We use absolute positional embeddings \(E_{\text{pos}}\) and a linear layer to project onto a higher dimensional space: \(\bm{Z}_{[0]}=\texttt{Linear}(\bm{Z})+E_{\text{pos}}\). The backbone then applies several self-attention blocks, which process tokens as follows:

\[\bm{Z}_{[l+1]}\leftarrow\bm{Z}_{[l]}+\texttt{Attention}(\texttt{ LayerNorm}(\bm{Z}_{[l]}))\] (4) \[\bm{Z}_{[l+1]}\leftarrow\bm{Z}_{[l+1]}+\texttt{FFN}(\texttt{ LayerNorm}(\bm{Z}_{[l+1]})\] (5)

We found out that adding a diffusion component to the transformer helped enhance the stability and allowed longer forecasts. Diffusion steps are inserted between two time steps \(t\) and \(t+\Delta t\) of the time-marching process transformer. The diffusion steps are denoted by \(k\) and are different from the ones of the time-marching process (several diffusion steps \(k\) are performed between two time-marching steps \(t\) and \(t+\Delta t\)).

We then use a conditional diffusion transformer architecture close to Peebles and Xie (2023) for \(\mathcal{R}_{\theta}\), where we detail the main block in Appendix B. At diffusion step \(k\), the input to the network is a sequence stacking the tokens at time \(t\) and the current noisy targets estimate \((\bm{Z}^{t},\tilde{\bm{Z}}_{k}^{t+\Delta t})\). See Appendix B, Figure 4 and Figure 5 for more details. To train the diffusion transformer \(\mathcal{R}_{\theta}\), we freeze the encoder and decoder, and use the encoder to sample pairs of successive latent tokens \((\bm{Z}^{t},\bm{Z}^{t+\Delta t})\). We employ the "v-predict" formulation of DDPM (Salimans and Ho, 2022) for training and sampling.

## 4 Experiments

In this section, we systematically evaluate the performance of our proposed model across various experimental settings, focusing on its ability to handle dynamics on both regular and irregular grids. First, we investigate the dynamics on regular grids, where we benchmark our model against state-of-the-art neural operators, including Fourier Neural Operators (FNO), ResNet, Neural Fields, and Transformers. This comparison highlights the efficacy of our approach in capturing complex spatio-temporal patterns on structured domains. Second, we extend our analysis to dynamics on irregular grids and shared geometries, emphasizing the model's extrapolation capabilities in data-constrained regimes. Here, we compare our results with Neural Fields and Transformers, demonstrating the robustness of our model in handling less structured and more complex spatial configurations. Lastly, we assess the model's capacity to process diverse geometries and underlying spatial representations by comparing its performance on irregular grids and different geometries. This evaluation highlights the flexibility and generalization ability of our model in encoding and learning from varied spatial domains, showcasing its potential in accurately representing and predicting dynamics across a wide range of geometric settings. We include additional results from ablation studies in Appendix C.6.

Figure 2: Spatial interpretation of the tokens through cross attention between \(T^{geo}\) and \(\bm{\gamma}(x)\) for each \(x\) in the domain. Here we visualize the cross-attention of three different tokens for a given head. The cross attentions can have varying receptive fields depending on the geometries.

### Dynamics on regular grids

We begin our analysis with dynamics modeling on regular grid settings. Though our model is targeted for complex geometries, we believe this scenario remains an important benchmark to assess the efficiency of surrogate models.

Datasets\(\bullet\)**1D Burgers' Equation** (_Burgers_): Models shock waves, using a dataset with periodic initial conditions and forcing term as in Brandstetter et al. (2022). It includes 2048 training and 128 test trajectories, at resolutions of \((250,100)\). We create sub-trajectories of 50 timestamps and treat them independently. \(\bullet\)**2D Navier Stokes Equation**: for a viscous and incompressible fluid. We use the data from Li et al. (2021). The equation is expressed with the vorticity form on the unit torus: \(\frac{\partial w}{\partial t}+u\cdot\nabla w=\nu\Delta w+f\), \(\nabla u=0\) for \(x\in\Omega,t>0\), where \(\nu\) is the viscosity coefficient. We consider two different versions \(\nu=10^{-4}\) (_Navier-Stokes_\(1\times 10^{-4}\)) and \(\nu=10^{-5}\) (_Navier-Stokes_\(1\times 10^{-5}\)), and use train and test sets of \(1000\) and \(200\) trajectories with a base spatial resolution of size \(64\times 64\). We consider a horizon of \(T=30\) for \(\nu=10^{-4}\) and \(T=20\) for \(\nu=10^{-5}\) since the phenomenon is more turbulent. At test time, we use the vorticity at \(t_{0}=10\) as the initial condition.

SettingWe train all the models with supervision on the next state prediction to learn to approximate the time-stepping operator \(\bm{u}^{t+\Delta t}=\mathcal{G}(\bm{u}^{t})\). At test time, we unroll the dynamics auto-regressively with each model and evaluate the prediction with a relative \(L_{2}\) error defined as \(L_{2}^{\text{test}}=\frac{1}{N_{\text{test}}}\sum_{j\in\text{test}}\frac{|| \bm{u}^{\text{regressive}}_{j}-\bm{u}^{\text{regressive}}_{j}||_{2}}{||\bm{u}^ {\text{regressive}}_{j}||_{2}}\).

BaselinesWe use a diverse panel of baselines including state of the art regular-grid methods such as FNO (Li et al., 2021) and ResNet (He et al., 2016; Lippe et al., 2023), flexible transformer architectures such as OFormer (Li et al., 2023), and GNOT (Hao et al., 2023), and finally neural-field based methods with DINO (Yin et al., 2022) and CORAL (Serrano et al., 2023).

ResultsTable 1 presents a comparison of model performance on the _Burgers_, _Navier-Stokes1e-4_, and _Navier-Stokes1e-5_ datasets, with metrics reported in Relative \(L_{2}\). Our method, AROMA, demonstrates excellent performance across the board, highlighting its ability to capture the dynamics of turbulent phenomena, as reflected in the _Navier-Stokes_ datasets.

In contrast, DINO and CORAL, both global neural field models, perform poorly in capturing turbulent phenomena, exhibiting significantly higher errors compared to other models. This indicates their limitations in handling complex fluid dynamics. On the other hand, AROMA outperforms GNOT on all datasets, though it performs reasonably well compared to the neural field based method.

Regarding the regular-grid methods, ResNet shows suboptimal performance in the pure teacher forcing setting, rapidly accumulating errors over time during inference. FNO stands out as the best baseline, demonstrating competitive performance on all datasets. We hypothesize that FNO's robustness to error accumulation during the rollout can be attributed to its Fourier block, which effectively cuts off high-frequency components. Overall, the results underscore AROMA's effectiveness and highlight the challenges Neural Field-based models face in accurately modeling complex phenomena.

### Dynamics on irregular grids with shared geometries

We continue our experimental analysis with dynamics on unstructured grids, where we observe trajectories only through sparse spatial observations over time. We adopt a data-constrained regime and show that our model can still be competitive with existing Neural Fields in this scenario.

DatasetsTo evaluate our framework, we utilize two fluid dynamics datasets commonly used as a benchmark for this task (Yin et al., 2022; Serrano et al., 2023) with unique initial conditions for each trajectory: \(\bullet\)**2D Navier-Stokes Equation** (_Navier-Stokes_\(1\times 10^{-3}\)): We use the same equation as

\begin{table}
\begin{tabular}{l c c c} \hline \hline Model & _Burgers_ & _Navier-Stokes_ & _Navier-Stokes_ \\  & & \(1\times 10^{-4}\) & \(1\times 10^{-5}\) \\ \hline FNO & \(5.00\times 10^{-2}\) & \(1.53\times 10^{-1}\) & \(\bm{1.24\times 10^{-1}}\) \\ ResNet & \(8.50\times 10^{-2}\) & \(3.77\times 10^{-1}\) & \(2.56\times 10^{-1}\) \\ \hline DINO & \(4.57\times 10^{-1}\) & \(7.25\times 10^{-1}\) & \(3.72\times 10^{-1}\) \\ CORAL & \(6.20\times 10^{-2}\) & \(3.77\times 10^{-1}\) & \(3.11\times 10^{-1}\) \\ \hline GNOT & \(1.28\times 10^{-1}\) & \(1.85\times 10^{-1}\) & \(1.65\times 10^{-1}\) \\ OFormer & \(4.92\times 10^{-2}\) & \(1.36\times 10^{-1}\) & \(2.40\times 10^{-1}\) \\ \hline AROMA & \(\bm{3.65\times 10^{-2}}\) & \(\bm{1.05\times 10^{-1}}\) & \(\bm{1.24\times 10^{-1}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Model Performance Comparison** - Test results. Metrics in Relative \(L_{2}\).

in Section 4.1 but with a higher viscosity coefficient \(\nu=1e-3\). We have 256 trajectories of size 40 for training and 32 for testing. We used a standard resolution of 64x64. \(\bullet\)**3D Shallow-Water Equation** (_Shallow-Water_): This equation approximates fluid flow on the Earth's surface. The data includes the vorticity \(w\) and height \(h\) of the fluid. The training set comprises 64 trajectories of size 40, and the test set comprises 8 trajectories with 40 timestamps. We use a standard spatial resolution of \(64\times 128\).

**Setting \(\bullet\) Temporal Extrapolation**: For both datasets, we split trajectories into two equal parts of 20 timestamps each. The first half is denoted as _In-t_ and the second half as _Out-t_. The training set consists of _In-t_. During training, we supervise with the next state only. During testing, the model unrolls the dynamics from a new initial condition (IC) up to the end of _Out-t_, i.e. for 39 steps. Evaluation within the _In-t_ horizon assesses the model's ability to forecast within the training regime. The _Out-t_ evaluation tests the model's extrapolation capabilities beyond the training horizon. \(\bullet\)**Sparse observations**: For the train and test set we randomly select \(\pi\) percent of the available regular mesh to create a unique grid for each trajectory, both in the train and in the test. The grid is kept fixed along a given trajectory. While each grid is different, they maintain the same level of sparsity across trajectories. In our case, \(\pi=100\%\) amounts to the fully observable case, while in \(\pi=25\%\) each grid contains around 1020 points for _Navier-Stokes_\(1\times 10^{-3}\) and 2040 points for _Shallow-Water_.

BaselinesWe compare our model to OFormer (Li et al., 2023), GNOT (Hao et al., 2023), and choose DINO (Yin et al., 2022) and CORAL (Serrano et al., 2023) as the neural field baselines.

Training and evaluationDuring training, we only use the data from the training horizon (_In-t_). At test time, we evaluate the models to unroll the dynamics for new initial conditions in the training horizon (_In-t_) and for temporal extrapolation (_Out-t_).

ResultsTable 2 demonstrates that AROMA consistently achieves low MSE across all levels of observation sparsity and evaluation horizons for both datasets. Overall, our method performs best with some exceptions. On _Shallow-Water_ our model is slightly outperformed by CORAL in the fully observed regime, potentially because of a lack of data. Similarly, on _Navier-Stokes_\(1\times 10^{-3}\) CORAL has slightly better scores in the very sparse regime \(\pi=5\%\). Overall, this is not surprising as meta-learning models excel in data-constrained regimes. We believe our geometry-encoding block is crucial for obtaining good representations of the observed values in the sparse regimes, potentially explaining the performance gap with GNOT and OFormer.

### Dynamics on different geometries

Finally, we extend our analysis to learning dynamics over varying geometries.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{\(\mathcal{X}_{tr}\downarrow\mathcal{X}_{te}\)} & \multirow{2}{*}{dataset \(\rightarrow\)} & _Navier-Stokes_\(1\times 10^{-3}\) & \multicolumn{2}{c}{_Shallow-Water_} \\ \cline{3-6}  & & _In-t_ & _Out-t_ & _In-t_ & _Out-t_ \\ \hline \multirow{4}{*}{\(\pi=100\%\)} & DINO & \(2.51\times 10^{-2}\) & \(9.91\times 10^{-2}\) & \(4.15\times 10^{-4}\) & \(3.55\times 10^{-3}\) \\  & CORAL & \(5.76\times 10^{-4}\) & \(3.00\times 10^{-3}\) & \(\mathbf{2.12\times 10^{-5}}\) & \(\mathbf{6.00\times 10^{-4}}\) \\  & OFormer & \(7.76\times 10^{-3}\) & \(6.39\times 10^{-2}\) & \(1.00\times 10^{-2}\) & \(2.23\times 10^{-2}\) \\  & GNOT & \(3.21\times 10^{-4}\) & \(2.33\times 10^{-3}\) & \(2.48\times 10^{-4}\) & \(2.17\times 10^{-3}\) \\  & AROMA & \(\mathbf{1.32\times 10^{-4}}\) & \(\mathbf{2.23\times 10^{-3}}\) & \(3.10\times 10^{-5}\) & \(8.75\times 10^{-4}\) \\ \hline \multirow{4}{*}{\(\pi=25\%\)} & DINO & \(3.27\times 10^{-2}\) & \(1.40\times 10^{-1}\) & \(4.12\times 10^{-4}\) & \(3.26\times 10^{-3}\) \\  & CORAL & \(1.54\times 10^{-3}\) & \(1.07\times 10^{-2}\) & \(3.77\times 10^{-4}\) & \(1.44\times 10^{-3}\) \\ \cline{1-1}  & OFormer & \(3.73\times 10^{-2}\) & \(1.60\times 10^{-1}\) & \(6.19\times 10^{-3}\) & \(1.40\times 10^{-2}\) \\ \cline{1-1}  & GNOT & \(2.07\times 10^{-2}\) & \(6.24\times 10^{-2}\) & \(8.91\times 10^{-4}\) & \(4.66\times 10^{-3}\) \\ \cline{1-1}  & AROMA & \(\mathbf{7.02\times 10^{-4}}\) & \(\mathbf{6.31\times 10^{-3}}\) & \(\mathbf{1.49\times 10^{-4}}\) & \(\mathbf{1.02\times 10^{-3}}\) \\ \hline \multirow{4}{*}{\(\pi=5\%\)} & DINO & \(3.63\times 10^{-2}\) & \(1.35\times 10^{-1}\) & \(4.47\times 10^{-3}\) & \(9.88\times 10^{-3}\) \\  & CORAL & \(\mathbf{2.87\times 10^{-3}}\) & \(\mathbf{1.48\times 10^{-2}}\) & \(2.72\times 10^{-3}\) & \(6.58\times 10^{-3}\) \\ \cline{1-1} irregular grid & OFormer & \(3.23\times 10^{-2}\) & \(1.12\times 10^{-1}\) & \(8.67\times 10^{-3}\) & \(1.72\times 10^{-2}\) \\ \cline{1-1}  & GNOT & \(7.43\times 10^{-2}\) & \(1.89\times 10^{-1}\) & \(5.05\times 10^{-3}\) & \(1.49\times 10^{-2}\) \\ \cline{1-1}  & AROMA & \(4.73\times 10^{-3}\) & \(2.01\times 10^{-2}\) & \(\mathbf{1.93\times 10^{-3}}\) & \(\mathbf{3.14\times 10^{-3}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Temporal Extrapolation** - Test results. Metrics in MSE.

DatasetsWe evaluate our model on two problems involving non-convex domains, as described by Pfaff et al. (2021). Both scenarios involve fluid dynamics in a domain with an obstacle, where the area near the boundary conditions (BC) is more finely discretized. The boundary conditions are specified by the mesh, and the models are trained with various obstacles and tested on different, yet similar, obstacles. \(\bullet\)**Cylinder (_CylinderFlow_)**: This dataset simulates water flow around a cylinder using a fixed 2D Eulerian mesh, representing _incompressible_ fluids. For each node \(j\) in the mesh \(\mathcal{X}\), we have data on the node position \(x^{(j)}\), momentum \(w(x^{(j)})\), and pressure \(p(x^{(j)})\). Our task is to learn the mapping from \((w_{t}(x),p_{t}(x))_{x\in\mathcal{X}}\) to \((w_{t+\Delta t}(x),p_{t+\Delta t}(x))_{x\in\mathcal{X}}\) for a fixed \(\Delta t\). \(\bullet\)**Airfoil** (_AirfoilFlow_)**: This dataset simulates the aerodynamics around an airfoil, relevant for _compressible_ fluids. In addition to the data available in the Cylinder dataset, we also have the fluid density \(\rho(x^{(j)})\) for each node \(j\). Our goal is to learn the mapping from \((w_{t}(x),p_{t}(x),\rho_{t}(x))_{x\in\mathcal{X}}\) to \((w_{t+\Delta t}(x),p_{t+\Delta t}(x),\rho_{t+\Delta t}(x))_{x\in\mathcal{X}}\). Each example in the dataset corresponds to a unique mesh. On average, there are 5233 nodes per mesh for _AirfoilFlow_ and 1885 for _CylinderFlow_. We temporally subsample the original trajectories by taking one timestamp out of 10, forming trajectories of 60 timestamps. We use the first 40 timestamps for training (_In-t_) and keep the last 20 timestamps for evaluation (_Out-t_).

SettingWe train all the models with supervision on the next state prediction. At test time, we unroll the dynamics auto-regressively with each model and evaluate the prediction with a mean squared error (MSE) both in the training horizon _(In-t)_ and beyond the training horizon _(Out-t)_.

ResultsThe results in Table 3 show that AROMA outperforms other models in predicting flow dynamics on both _CylinderFlow_ and _AirfoilFlow_ geometries, achieving the lowest MSE values across all tests. This indicates AROMA's superior ability to encode geometric features accurately. Additionally, AROMA maintains stability over extended prediction horizons, as evidenced by its consistently low _Out-t_ MSE values.

### Long rollouts and uncertainty quantification

After training different models on _Burgers_, we compare them on long trajectory rollouts. We start from \(t_{0}=50\) (i.e. use a numerical solver for 50 steps), and unroll our dynamics auto-regressively for 200 steps. Note that all the models were only trained to predict the next state. We plot the correlation over rollout steps of different methods, including our model without the diffusion process, in Figure 3. We can clearly see the gain in stability in using the diffusion for long rollouts. Still, the predictions will eventually become uncorrelated over time as the solver accumulates errors compared with the numerical solution. As we employ a generative model, we can generate several rollouts and estimate the uncertainty of the solver with standard deviations. We can see in Appendix Figure 11 that this uncertainty increases over time. This uncertainty is not a guarantee that the solution lies within the bounds, but is an indication that the model is not confident in its predictions.

Figure 3: Correlation over time for long rollouts with different methods on _Burgers_

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model & \multicolumn{2}{c}{_CylinderFlow_} & \multicolumn{2}{c}{_AirfoilFlow_} \\ \cline{2-5}  & _In-t_ & _Out-t_ & _In-t_ & _Out-t_ \\ \hline CORAL & \(4.458\times 10^{-2}\) & \(8.695\times 10^{-2}\) & \(1.690\times 10^{-1}\) & \(3.420\times 10^{-1}\) \\ DINO & \(1.349\times 10^{-1}\) & \(1.576\times 10^{-1}\) & \(3.770\times 10^{-1}\) & \(4.740\times 10^{-1}\) \\ OFormer & \(5.020\times 10^{-1}\) & \(1.080\times 10^{0}\) & \(5.620\times 10^{-1}\) & \(7.620\times 10^{-1}\) \\ AROMA & \(\mathbf{1.480\times 10^{-2}}\) & \(\mathbf{2.780\times 10^{-2}}\) & \(\mathbf{5.720\times 10^{-2}}\) & \(\mathbf{1.940\times 10^{-1}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Dynamics on different geometries** - Test results. MSE on normalized data.

Related Work

Our model differs from existing models in the field of operator learning and more broadly from existing neural field architectures. The works most related to ours are the following.

Neural Fields for PDENeural Fields have recently emerged as powerful tools to model dynamical systems. DINO (Yin et al., 2022) is a space-time continuous architecture based on a modulated multiplicative filter network (Fathony et al., 2021) and a NeuralODE (Chen and Zhang, 2019) for modeling the dynamics. DINO is capable of encoding and decoding physical states on irregular grids thanks to the spatial continuity of the INR and through auto-decoding (Park et al., 2019). CORAL is another neural-field based architecture, which tackles the broader scope of operator learning, also builds on meta-learning (Zintgraf et al., 2019; Dupont et al., 2022) to freely process irregular grids. CORAL and DINO are the most similar works to ours, as they are both auto-regressive and capable of processing irregular grids. On the other hand Chen et al. (2022) and Haghberger et al. (2024) make use of spatio-temporal Neural Fields, for obtaining smooth and compact latent representations in the first or to directly predict trajectory solutions within a temporal horizon in the latter. Moreover, they either use a CNN or rely on patches for encoding the observations and are therefore not equipped for the type of tasks AROMA is designed for.

Transformers for PDESeveral PDE solvers leverage transformers and cross-attention as a backbone for modeling PDEs. Transformers, which operate on token sequences, provide a natural solution for handling irregular meshes and point sets. Li et al. (2023) and Hao et al. (2023) introduced transformer architectures tailored for operator learning. Hao et al. (2023) incorporated an attention mechanism and employed a mixture of experts strategy to address multi-scale challenges. However, their architecture relies on linear attention without reducing spatial dimensions, resulting in linear complexity in sequence size, but quadratic in the hidden dimensions, which can be prohibitive for deep networks and large networks. Similarly, Li et al. (2023) utilized cross-attention to embed both regular and irregular meshes into a latent space and applied a recurrent network for time-marching in this latent space. Nonetheless, like GNOT, their method operates point-wise on the latent space. Transolver (Wu et al., 2024) decomposes a discrete input function into a mixture of "slices," each corresponding to a prototype in a mixture model, with attention operating in this latent space. This approach, akin to our model, reduces complexity. However, it has not been designed for temporal problems. (Alkin et al., 2024) recently proposed a versatile model capable of operating on Eulerian and Lagrangian (particles) representations. They reduce input dimensionality by aggregating information from input values onto "supernodes" selected from the input mesh via message passing while decoding is performed with a Perceiver-like architecture. In contrast, AROMA performs implicit spatial encoding with cross-attention to encode the geometry and aggregate obsevation values. Finally, their training involves complex end-to-end optimization, whereas we favor two simple training steps that are easier to implement.

## 6 Conclusion and Limitations

AROMA offers a novel and flexible neural operator approach for modeling the spatio-temporal evolution of physical processes. It is able to deal with general geometries and to forecast at any position of the spatial domain. It incorporates in an encode-process-decode framework attention mechanisms, a latent diffusion transformer for spatio-temporal dynamics and neural fields for decoding. Thanks to a very compact spatial encoding, its complexity is lower than most SOTA models. Experiments with small-size datasets demonstrate its effectiveness. Its reduced complexity holds potential for effective scaling to larger datasets. As for the limitations, the performance of AROMA are still to be demonstrated on larger and real world examples. Moreover, like all dynamical models that operate over a latent space, the reconstruction capabilities of the decoder is a bottleneck for the rollout accuracy. Since the encoder and decoder are learning spatial relationships from scratch, conferring the framework a high flexibility, the training efficiency does not match that of CNN-based auto-encoders on regular grids. We therefore believe there could be further improvements to be made to achieve a similar performance while keeping the same level of flexibility. Finally, even though our model has some potential for uncertainty modeling, this aspect has still to be further explored and analyzed.

## Acknowledgements

We acknowledge the financial support provided by DL4CLIM (ANR-19-CHIA-0018-01), DEEP-NUM (ANR-21-CE23-0017-02), PHLUSIM (ANR-23-CE23-0025-02), and PEPR Sharp (ANR-23-PEIA-0008, ANR, FRANCE 2030). This work was granted access to the HPC resources of IDRIS under the allocations 2023-AD011013522R1, 2023-AD011013332R1, 2023-AD011015133R1, 2023-A0161015133 made by GENCI.

## References

* Alkin et al. (2024) Benedikt Alkin, Andreas Furst, Simon Schmid, Lukas Gruber, Markus Holzleitner, and Johannes Brandstetter. Universal Physics Transformers. 2024. URL http://arxiv.org/abs/2402.12365.
* Bauer et al. (2023) Matthias Bauer, Emilien Dupont, Andy Brock, Dan Rosenbaum, Jonathan Schwarz, and Hyunjik Kim. Spatial functa: Scaling functa to imagenet classification and generation. _CoRR_, abs/2302.03130, 2023.
* Brandstetter et al. (2022) Johannes Brandstetter, Daniel E Worrall, and Max Welling. Message passing neural pde solvers. _International Conference on Learning Representations_, 2022.
* Chen et al. (2022) Peter Yichen Chen, Jinxu Xiang, Dong Heon Cho, Yue Chang, G A Pershing, Henrique Teles Maia, Maurizio Chiaromotte, Kevin Carlberg, and Eitan Grinspun. Crom: Continuous reduced-order modeling of pdes using implicit neural representations. _International Conference on Learning Representation_, 6 2022. URL http://arxiv.org/abs/2206.02607.
* Chen and Zhang (2019) Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, 2019-June, 2019. ISSN 10636919. doi: 10.1109/CVPR.2019.00609.
* Dupont et al. (2022) Emilien Dupont, Hyunjik Kim, S. M. Ali Eslami, Danilo Rezende, and Dan Rosenbaum. From data to functa: Your data point is a function and you can treat it like one. _Proceedings of the 39 th International Conference on Machine Learning_, 1 2022. URL http://arxiv.org/abs/2201.12204.
* Fathony et al. (2021) Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico Kolter. Multiplicative filter networks. _International Conference on Learning Representations._, 2021.
* Hagnberger et al. (2024) Jan Hagnberger, Marimuthu Kalimuthu, Daniel Musekamp, and Mathias Niepert. Vectorized Conditional Neural Fields: A Framework for Solving Time-dependent Parametric Partial Differential Equations. In _Proceedings of the 41st International Conference on Machine Learning (ICML 2024)_, 2024.
* Hao et al. (2023) Zhongkai Hao, Zhengyi Wang, Hang Su, Chengyang Ying, Yinpeng Dong, Songming Liu, Ze Cheng, Jian Song, and Jun Zhu. Gnot: A general neural operator transformer for operator learning. In _International Conference on Machine Learning_, pp. 12556-12569. PMLR, 2023.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 770-778, 2016.
* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* Kingma and Welling (2013) Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* Lee et al. (2023) Doyup Lee, Chiheon Kim, Minsu Cho, and Wook-Shin Han. Locality-aware generalizable implicit neural representation. _Advances in Neural Information Processing Systems_, 2023.
* Li et al. (2023) Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations' operator learning. _Transactions on Machine Learning Research (April/2023)_, 2023.
* Li et al. (2020)Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _International Conference on Learning Representations._, 10 2021. URL http://arxiv.org/abs/2010.08895.
* Lippe et al. [2023] Phillip Lippe, Bastiaan S Veeling, Paris Perdikaris, Richard E Turner, and Johannes Brandstetter. Pde-refiner: Achieving accurate long rollouts with neural pde solvers. _arXiv preprint arXiv:2308.05732_, 2023.
* Lu et al. [2021] Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. _Nat Mach Intell_, 3:218-229, 10 2021. doi: 10.1038/s42256-021-00302-5. URL http://arxiv.org/abs/1910.03193http://dx.doi.org/10.1038/s42256-021-00302-5.
* Oord et al. [2017] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. _arXiv preprint arXiv:1711.00937_, 2017.
* Park et al. [2019] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, 2019-June, 2019. ISSN 10636919. doi: 10.1109/CVPR.2019.00025.
* Pathak et al. [2022] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators. _arXiv preprint arXiv:2202.11214_, 2022.
* Peebles and Xie [2023] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 4195-4205, 2023.
* Pfaff et al. [2021] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W. Battaglia. Learning mesh-based simulation with graph networks. _International Conference on Learning Representations._, 10 2021. URL http://arxiv.org/abs/2010.03409.
* Rolinek et al. [2019] Michal Rolinek, Dominik Zietlow, and Georg Martius. Variational autoencoders pursue pca directions (by accident). In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 12406-12415, 2019.
* Cachay et al. [2023] Salva Ruhling Cachay, Bo Zhao, Hailey Joren, and Rose Yu. DYffusion: a dynamics-informed diffusion model for spatiotemporal forecasting. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.
* Salimans and Ho [2022] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. _arXiv preprint arXiv:2202.00512_, 2022.
* Serrano et al. [2023] Louis Serrano, Lise Le Boudec, Armand Kassai Koupai, Thomas X Wang, Yuan Yin, Jean-Noel Vittaut, and Patrick Gallinari. Operator learning with neural fields: Tackling pdes on general geometries. _Advances in Neural Information Processing Systems_, 2023.
* Stachenfeld et al. [2022] Kimberly Stachenfeld, Drummond B. Fielding, Dmitrii Kochkov, Miles Cranmer, Tobias Pfaff, Jonathan Godwin, Can Cui, Shirley Ho, Peter Battaglia, and Alvaro Sanchez-Gonzalez. Learned coarse models for efficient turbulence simulation. _International Conference on Learning Representation_, 2022.
* Vinuesa and Brunton [2022] Ricardo Vinuesa and Steven L Brunton. Enhancing computational fluid dynamics with machine learning. _Nature Computational Science_, 2(6):358-366, 2022.
* Wu et al. [2024] Haixu Wu, Huakun Luo, Haowen Wang, Jianmin Wang, and Mingsheng Long. Transolver: A Fast Transformer Solver for PDEs on General Geometries. 2024. URL http://arxiv.org/abs/2402.02366.
* Yin et al. [2022] Yuan Yin, Matthieu Kirchmeyer, Jean-Yves Franceschi, Alain Rakotomamonjy, and Patrick Gallinari. Continuous pde dynamics forecasting with implicit neural representations. _International Conference on Learning Representations_, 9 2022. URL http://arxiv.org/abs/2209.14855.
* Zhang et al. [2021]Luisa Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast context adaptation via meta-learning. _36th International Conference on Machine Learning, ICML 2019_, 2019-June, 2019.

## Appendix A Extended Related Work

Diffusion models for PDERecently, diffusion models have experienced significant growth and success in generative tasks, such as image or video generation (Ho et al., 2020). This success has motivated their application to physics prediction. Ruhling Cachay et al. (2023) propose DYffusion, a framework that adapts the diffusion process to spatio-temporal data for forecasting on long-time roll-outs, by performing diffusion-like timesteps in the physical time dimension. PDE-Refiner (Lippe et al., 2023) is a CNN-based method that uses diffusion to stabilize prediction rollouts over long trajectories. Compared to these methods, we perform diffusion in a latent space, reducing the computational cost; and leverage the advanced modeling capabilities of transformers.

Local Neural FieldsWe are not the first work that proposes to leverage locality to improve the design of neural fields. In a different approach, Bauer et al. (2023) proposed a grid-based latent space where the modulation function \(\phi\) is dependent on the query coordinate \(x\). This concept enables the application of architectures with spatial inductive biases for generation on the latent representations, such as a U-Net Denoiser for diffusion processes. Similarly, Lee et al. (2023) developed a locality-aware, generalizable Implicit Neural Representation (INR) with demonstrated capabilities in generative modeling. Both of these architectures assume regular input structures, be it through patching methods or grid-based layouts.

## Appendix B Implementation details

Diffusion transformerWe illustrate how our diffusion transformer is trained and used at inference in Figure 4 and Figure 5. We provide the diffusion step \(k\) which acts as a conditioning input for the diffusion model. We use an exponential decrease for the noise level as in Lippe et al. (2023) i.e. \(\alpha_{k}=1-\sigma_{\text{min}}^{k/K}\). We use the same diffusion transformer block as in Peebles and Xie (2023), which relies on amplitude and shift modulations from the diffusion timestamp \(k\):

\[\alpha^{(1)},\beta^{(1)},\gamma^{(1)} \leftarrow\text{MLP}_{1}(k)\] (6) \[\alpha^{(2)},\beta^{(2)},\gamma^{(2)} \leftarrow\text{MLP}_{2}(k)\] (7) \[\bm{Z}_{[l+1]} \leftarrow\bm{Z}_{[l]}+\alpha^{(1)}\cdot\texttt{Attention}(\gamma^ {(1)}\cdot\texttt{LayerNorm}(\bm{Z}_{[l]})+\beta^{(1)})\] (8) \[\bm{Z}_{[l+1]} \leftarrow\bm{Z}_{[l+1]}+\alpha^{(2)}\cdot\texttt{FFN}(\gamma^{(2 )}\cdot\texttt{LayerNorm}(\bm{Z}_{[l+1]}+\beta^{(2)})\] (9)Encoder-DecoderWe provide a more detailed description of the encoder-decoder pipeline in Figure 6.

Local INRWe show the implementation of our local INR, both with single-band frequency and multi-band frequency, in Figure 7 and Figure 8. The cross-attention mechanism enables to retrieve a local feature vector \(\mathbf{f}_{q}(x)\) for each query position \(x\). We then use an MLP to decode this feature vector to retrieve the output value. In practice, we retrieve several feature vectors corresponding each to separate frequency bandwidths. In this case, we concatenate the feature vectors before decoding them with the MLP.

Figure 4: During training, we noise the next-step latent tokens \(\bm{Z}^{t+\Delta t}\) and train the transformer to predict the “velocity” of the noise. Each DIT block is implemented as in Peebles and Xie (2023).

Figure 6: Architecture of our encoder and decoder. We regularize the architecture as a variational auto-encoder. Cross-attention layers are used to aggregate the \(N\) observations into \(M\) latent tokens, and to expand the \(M\) processed tokens to the queried values. We use a bottleneck layer to reduce the channel dimension of the latent space.

Figure 7: Single-band local INR decoder

Figure 8: Multi-band local INR decoder

### Hyperparameters

We detail the values of the hyperparameters used on each dataset: Table 5 presents the hyperparameters of the Encoder-Decoder, while Table 4 presents the hyperparameters of the Diffusion Transformer. We use a cosine scheduler for the tuning learning rate for both trainings, with an initial maximum learning rate of \(10^{-3}\) annealing to \(10^{-5}\). All experiments were performed with an NVIDIA TITAN RTX.

For the diffusion transformer, we use \(K=3\) diffusion steps for all experiments and only vary the minimum noise \(\sigma_{\text{min}}\).

For the encoder-decoder, we have the following hyperparameters:

* hidden_dim: The number \(h\) of neurons at each hidden layer.
* num_self_attentions: The number of Self Attention layers used for the decoder.
* num_latents: The number \(M\) of latent tokens used to spatially project the objervations and geometries.
* latent_dim: The dimension \(c\) of each latent token.
* latent_heads: The number of heads use for the Self Attention layers.
* latent_dim_head: The dimension of each head in a Self Attention layer.
* cross_heads: The number of heads use for the Cross Attention layers.
* cross_dim_head: The dimension of each head in a Cross Attention layer.
* dim: The number of neurons used in the MLP decoder.
* depth_inr: The number of layers in the MLP decoder.
* frequencies: The different frequencies used for the local INR. We use base 2 for all experiments and select 16 frequencies in logarithmic scale per level. For example, [3, 4, 5] means that we construct 3 frequency embedding vectors, the first \(\gamma_{1}(x)=(\cos(2^{0}\pi x),\sin(2^{0}\pi x),\ldots,\cos(2^{3}\pi x),\sin( 2^{3}\pi x))\), for the second \(\gamma_{2}=(\cos(2^{3}\pi x),\sin(2^{3}\pi x),\ldots,\cos(2^{4}\pi x),\sin(2^{ 4}\pi x))\), and for the third \(\gamma_{3}=(\cos(2^{4}\pi x),\sin(2^{4}\pi x),\ldots,\cos(2^{5}\pi x),\sin(2^{ 5}\pi x))\)
* dropout_sequence: The ratio of points that are ignored by the encoder.
* feature_dim: The dimension of the feature vector.
* encode_geo: If we use a cross-attention block to encode the geometry.
* max_encoding_freq: The maximum frequency used for the frequency embedding \(\gamma\) of the encoder.
* kl_weight: The weight \(\beta\) used for the VAE training.
* epochs: Number of training epochs.

The most important hyperparameter of the encoder-decoder is the number of tokens \(M\) that are used to aggregate the observations and geometries. We show the impact it has on the quality of reconstructions in Table 6.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Hyperparameters & Burgers & NS1e-3 & NS1e-4 & NS1e-5 & Shallow-water & Cylinder-Flow & Airfoil-Flow \\ \hline hidden\_size & 128 & 128 & 128 & 128 & 128 & 128 & 128 \\ depth & 4 & 4 & 4 & 4 & 4 & 4 & 4 \\ num\_heads & 4 & 4 & 4 & 4 & 4 & 4 & 4 \\ mlp\_ratio & 4.0 & 4.0 & 4.0 & 4.0 & 4.0 & 4.0 & 4.0 \\ min.noise & 1e-2 & 1e-2 & 1e-3 & 1e-3 & 1e-3 & 1e-3 \\ denoising\_steps & 3 & 3 & 3 & 3 & 3 & 3 & 3 \\ epochs & 2000 & 2000 & 2000 & 2000 & 2000 & 2000 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Diffusion Transformer Hyperparameters for Different Datasets

## Appendix C Additional results

### Time complexity analysis

We denote \(N\) as the number of observations of \(\bm{u}\), \(M\) as the number of tokens used to compress the information, \(T\) as the number of autoregressive calls in the rollout, \(K\) as the number of refinement steps, and \(d\) as the number of channels used in the attention mechanism. The most computationally expensive operations in our architecture are the cross-attention and self-attention blocks. For simplification, we omit the geometry encoding block in this study.

The cost of the cross-attention in the encoder is \(O(NMd)\), and similarly, the cost of the cross-attention in the decoder is \(O(NMd)\). Let \(L_{1}\) and \(L_{2}\) represent the number of layers in the decoder and diffusion transformer, respectively. The cost of the self-attention layers in the decoder is \(O(L_{1}M^{2}d)\), while in the diffusion transformer, it is \(O(4L_{2}M^{2}d)\).

To unroll the dynamics, we encode the initial condition, obtain the predictions in the latent space, and then decode in parallel, yielding a total cost of \(O((2N+4KTL_{2}M+L_{1}M)Md)\). As expected, our architecture has linear complexity in the number of observations through the cross-attention layers. In contrast, GNOT relies on linear attention, resulting in a time complexity of \(O((LN)d^{2})\) for each prediction, where \(L\) is the depth of the network. At inference, the cost per step along a trajectory is \(LNd^{2}\) for GNOT, compared to \(4KL_{2}M^{2}d\) for AROMA.

For instance, using \(K=3\), \(M=64\), \(N=4096\), and \(d=128\), GNOT's cost is approximately \(10\) times that of AROMA for each prediction throughout the rollout. Therefore AROMA is more efficient when \(M\ll N\).

### Encoding interpretation

We provide in Figure 9 a qualitative analysis through cross-attention visualizations how the geometry encoding block helps to capture the geometry of the domain. In the first cross-attention block, the query tokens \(\mathbf{T}\) are not aware of the geometry and therefore attend to large regions of the domains. This lets the model understand, where the boundaries of the domain are and therefore where the cylinder is. Once the query tokens have aggregated the mesh information, the cross attention between \(\mathbf{T}^{\text{geo}}\) and the positions are sharper and depend on the geometry.

### Example rollouts

We show examples of rollout predictions using AROMA on _Burgers_ dataset in Figure 10, on _Navier-Stokes_\(1\times 10^{-3}\) dataset in Figure 12 and on _CylinderFlow_ in Figure 13. AROMA returns predictions that remain stable and accurate, even outside the training time horizon.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Hyperparameters & Burgers & NS1e-3 & NS1e-4 & NS1e-5 & Shallow-water & Cylinder-Flow & Airfoil-Flow \\ \hline hidden\_dim & 128 & 128 & 128 & 128 & 128 & 128 & 128 \\ num\_self\_attentions & 2 & 2 & 2 & 3 & 2 & 2 & 3 \\ num\_latents & 32 & 32 & 256 & 256 & 32 & 64 & 64 \\ latent\_dim & 8 & 16 & 16 & 16 & 16 & 16 & 16 \\ latent\_heads & 4 & 4 & 4 & 4 & 4 & 4 & 4 \\ latent\_dim\_head & 32 & 32 & 32 & 32 & 32 & 32 & 32 \\ cross\_heads & 4 & 4 & 4 & 4 & 4 & 4 & 4 \\ cross\_dim\_head & 32 & 32 & 32 & 32 & 32 & 32 & 32 \\ dim & 128 & 128 & 128 & 128 & 64 & 128 & 128 \\ depth\_inr & 3 & 3 & 3 & 3 & 3 & 3 & 3 \\ frequencies & \([3,4,5]\) & \([2,3]\) & \([3,4,5]\) & \([3,4,5]\) & \([2,3]\) & \([3,4,5]\) & \([3,4,5]\) \\ dropout\_sequence & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\ feature\_dim & 16 & 16 & 16 & 16 & 16 & 16 & 16 \\ encode\_geo & False & True & False & False & True & True & True \\ max\_encoding\_freq & 4 & 4 & 4 & 4 & 5 & 4 & 5 \\ kl\_weight & 1e-4 & 1e-4 & 1e-4 & 1e-5 & 1e-5 & 1e-5 & 1e-5 \\ epochs & 5000 & 5000 & 5000 & 5000 & 5000 & 5000 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hyperparameters of the Encoder-Decoder for Different DatasetsFigure 11: Uncertainty of AROMA over rollout steps. The blue line is the mean prediction while the blue shade represents the mean \(\pm\)\(3\times\) standard deviation.

Figure 10: Test example long rollout trajectory with AROMA on _Burgers_. Left is the predicted trajectory and right is the ground truth.

Figure 9: Evolution of the cross-attention maps between the geometry encoding stage and the observation encoding stage. Blue means the cross-attention value is close to zero while yellow means the cross-attention score is close to one.

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_FAIL:20]

Figure 16: Perturbation analysis on _Burgers_. Token 1.

Figure 17: Perturbation analysis on _Burgers_. Token 2.

Figure 18: Perturbation analysis on _Burgers_. Token 3.

Figure 19: Perturbation analysis on _Burgers_. Token 5.

Figure 21: Perturbation analysis on _Burgers_. Token 7.

Figure 22: Perturbation analysis on _Burgers_. Token 8.

Figure 20: Perturbation analysis on _Burgers_. Token 6.

Figure 21: Perturbation analysis on _Burgers_. Token 7.

### Ablation studies

Number of tokens \(M\)We show the impact of the number of latent tokens on the _Navier-Stokes_\(1\times 10^{-4}\) dataset in Table 6. We train our auto-encoder with 10000 trajectories. We can see that the performance increases with the number of tokens.

Auto-encoding vs VAEOur framework can also be used without the KL regularization, and could potentially be employed with other forms of regularization, such as L2 regularizaton or vector-quantization (Oord et al., 2017). We investigated in Table 7 the impact the KL regularization had on the overall rollout performance, and selected an autoencoder with L2 regularization (weight decay) as baseline. Our conclusion is that using an autoencoder with L2 regularization is a viable alternative to the VAE in some cases for achieving a smooth latent space. The autoencoder demonstrated superior performance on two datasets (_Burgers_ and _Navier-Stokes_\(1\times 10^{-4}\), explained by its lower reconstruction errors, which translate into better rollout performance. However, for the more challenging _Navier-Stokes_\(1\times 10^{-5}\) case, the autoencoder's latent space exhibited high variance, which may explain the observed performance difference with the VAE.

No-diffusion vs diffusionAs an ablation, we also measured the influence of the diffusion formulation on the rollout accuracy by comparing to the same transformer architecture trained directly with an MSE on the mean tokens. The deterministic version of AROMA shows consistently robust performance and even surpasses the diffusion version on the _Navier-Stokes_\(1\times 10^{-4}\) case (Table 7). This demonstrates that the latent tokens obtained with AROMA contain meaningful information for dynamics modeling. On the other hand, the deterministic version yields less accurate long rollouts on _Burgers_ or _KS_ in Figure 23 and Figure 3. Note that using diffusion allows us to model the trajectory distribution, which opens the way to infer statistics on this distribution. This is key, for example, when modeling uncertainty, which is a critical problem for these models.

Latent MLP vs Latent TransformerModeling interactions at the local and global levels is key to learn the dynamics faithfully. Experiments using MLPs (Table 7) as time steppers which do not consider interactions between tokens lead to significantly lower performance compared to transformers.

### Kuramoto-Sivashinsky : a failure case

We conducted additional experiments on a chaotic 1D PDE, the Kuramoto-Sivashinsky (_KS_) equation. We found that AROMA currently struggles with dynamics that exhibit chaotic phenomena and non-decaying spectra, as shown in Figure 23. The primary limitation appears to be the reconstruction capabilities of the encoder-decoder. For the _KS_ equation, we found that obtaining reconstructions with an MSE in the range of 1e-10 to 1e-12 was necessary for accurate spectrum reconstruction. Like all models leveraging a reduced latent representation space, AROMA inherently loses some of

\begin{table}
\begin{tabular}{c c} \hline \hline
**\#Latent Tokens** & **Test Reconstruction error** \\ \hline
64 & 0.02664 \\
128 & 0.0123 \\
256 & 0.01049 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Influence of the number of latent tokens on the test reconstruction capabilities on _Navier-Stokes_\(1\times 10^{-4}\). Performance in Relative \(L_{2}\) Error.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Model & _Burgers_ & _Navier-Stokes_ & _Navier-Stokes_ \\  & & \(1\times 10^{-4}\) & \(1\times 10^{-5}\) \\ \hline AROMA + auto-encoding & \(\mathbf{3.43\times 10^{-2}}\) & \(\mathbf{5.02\times 10^{-2}}\) & \(2.10\times 10^{-1}\) \\ \hline AROMA w/o diffusion & \(4.31\times 10^{-2}\) & \(7.50\times 10^{-2}\) & \(1.28\times 10^{-1}\) \\ AROMA + mlp & \(1.11\times 10^{-1}\) & \(1.00\times 10^{9}\) & \(8.25\times 10^{-1}\) \\ \hline AROMA & \(3.65\times 10^{-2}\) & \(1.05\times 10^{-1}\) & \(\mathbf{1.24\times 10^{-1}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Ablation Study**. Metrics in Relative \(L_{2}\) on the test set.

[MISSING_PAGE_FAIL:24]

Figure 24: Latent space dynamics on _Navier-Stokes 1e-3_ - Mean tokens over time. Each color line is a different token channel.

Figure 25: Latent space dynamics on _Navier-Stokes_ - Logvar tokens over time. Each color line is a different token channel.

Figure 26: Latent space dynamics on _Navier-Stokes_ - Predicted tokens over time. Each color line is a different token channel.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We conducted extensive experiments on multiple settings to obtain robust results across several datasets. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: There is a limitation section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: No theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Detailed hyperparameters and code. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: Everything will be made available upon acceptation. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the settings are disclosed explicitly. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: No error bars yet. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Type of gpu detailed. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes we conformed to the code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: No social impacts mentioned. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: This paper causes no such risk. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All datasets mentioned, and code also credited when appropriate. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [No] Justification: There is no such dataset. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper is not related to research on this topic. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No such risk. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.