# Contextual Active Model Selection

Xuefeng Liu\({}^{1}\)

Fangfang Xia\({}^{2}\)

Rick L. Stevens\({}^{1,2}\)

Yuxin Chen\({}^{1}\)

\({}^{1}\)Department of Computer Science

 University of Chicago

\({}^{2}\)Argonne National Laboratory

Correspondence to: Xuefeng Liu <xuefeng@uchicago.edu>.

###### Abstract

While training models and labeling data are resource-intensive, a wealth of pre-trained models and unlabeled data exists. To effectively utilize these resources, we present an approach to actively select pre-trained models while minimizing labeling costs. We frame this as an _online contextual active model selection_ problem: At each round, the learner receives an unlabeled data point as a context. The objective is to adaptively select the best model to make a prediction while limiting label requests. To tackle this problem, we propose CAMS, a contextual active model selection algorithm that relies on two novel components: (1) a contextual model selection mechanism, which leverages context information to make informed decisions about which model is likely to perform best for a given context, and (2) an active query component, which strategically chooses when to request labels for data points, minimizing the overall labeling cost. We provide rigorous theoretical analysis for the regret and query complexity under both adversarial and stochastic settings. Furthermore, we demonstrate the effectiveness of our algorithm on a diverse collection of benchmark classification tasks. Notably, CAMS requires substantially less labeling effort (less than 10%) compared to existing methods on CIFAR10 and DRIFT benchmarks, while achieving similar or better accuracy.

## 1 Introduction

As pre-trained models become increasingly prevalent in a variety of real-world machine learning applications [2; 11; 53], there is a growing demand for label-efficient approaches for model selection, especially when facing varying data distributions and contexts at run time. Oftentimes, no single pre-trained model achieves the best performance for every context, and a proper approach is to construct a policy for adaptively selecting models for specific contexts [48]. For instance, in medical diagnosis and drug discovery, accurate predictions are of paramount importance. The diagnosis of diseases through pathologist or the determination of compound chemical properties through lab testing can be costly and time-consuming. Different models may excel in analyzing different types of pathological images [1; 3; 23] or chemical compounds [17; 32; 46]. Furthermore, in many real-world applications, the collection of labels for model evaluation can be expensive and data instances may arrive as a stream rather than all at once. This scenario necessitates _cost-effective_ and _robust_ online algorithms capable of determining the most efficient model selection policy even when faced with a limited supply of labels, a scenario not fully addressed by previous works that typically assume access to all labels [6; 7; 27; 54].

Recently, the problem of online model selection with the consideration of label acquisition costs was studied in a _context-free_ setting by Karimi et al. [39]. However, this approach doesn't fully capture the dynamics of data contexts that are essential in many applications. Recognizing this gap, in this paper, we consider a more general problem setting that incorporates context information for adaptive model selection. We introduce CAMS, an algorithm for active model selection that dynamically adapts to the data context to choose the most suitable models for an arbitrary data stream. As highlighted inTable 1, CAMS aims to address the need for adaptive and effective model selection, by bridging the gap between contextual bandits, online learning, and active learning.

Our key contributions are summarized as follows:

* We investigate a novel problem which we refer to as _contextual active model selection_, and introduce a novel principled algorithm that features two key technical components: (1) a _contextual online model selection_ procedure, designed to handle both stochastic and adversarial settings, and (2) an _active query_ strategy. The proposed algorithm is designed to be robust to heterogeneous data streams, accommodating both stochastic and adversarial online data streaming scenarios.
* We provide rigorous theoretical analysis on the _regret_ and _query complexity_ of the proposed algorithms. We establish regret upper bounds for both adversarial and stochastic data streams under limited label costs. Our regret upper bounds are within constant factors of the existing lower bounds for online learning problems with expert advice under the full information setting.
* Empirically, we demonstrate the effectiveness and robustness of our approach on a variety of online model selection tasks spanning different application domains (from generic ML benchmarks such as CIFAR10 to domain-specific tasks in biomedical analysis), data scales (ranging from 80 to 10K), data modalities (i.e., tabular, image, and graph-based data), and label types (binary or multiclass labels). For the tasks evaluated, (1) CAMS outperforms all competing baselines by a significant margin. (2) Asymptotically, CAMS performs no worse than the best single model. (3) CAMS is not only robust to adversarial data streams but also can efficiently recover from "malicious experts" (i.e. inferior pre-trained models).

## 2 Related Work

Contextual bandits.Classical bandit algorithms (e.g., [6; 7]) aim to find the best arm(s) through a sequence of actions. When side information (e.g., user profile for recommender systems or environmental context for experimental design) is available, many bandit algorithms can be lifted to the contextual setting: For example, EXP4 [7; 9; 52] considers the bandit setting with expert advice: At each round, experts announce their predictions of which actions are the most promising for the given context, and the goal is to construct a expect selection policy that competes with the best expert from hindsight. In bandit problems, the learner only gets to observe the reward for each action taken. In contrast, for the online model selection problem considered in this work--where an action corresponds to choosing a model to make prediction on an incoming data point--we get to see the loss/reward of _all_ models on the labeled data point. By utilizing the information from unchosen arms, it could significantly reduce the cumulative regret. In this regard, this work aligns more closely with online learning with _full information_ setting, where the learner has access to the loss of all the arms at each round (e.g. as considered in the Hedge algorithm [13; 14; 27; 36]).

Online learning with full information.A clear distinction between our work and online learning is that we assume the labels of the online data stream are not readily available but can be acquired at each round with a cost. In addition, the learner only observes the loss incurred by all models on a data point when it decides to query its label. In contrast, in the canonical online learning setting, labels arrive with the data and one gets to observe the loss of all candidate models at each round. Similar setting also applies to other online learning problems, such as online boosting or bagging. A related work to ours is online learning with label-efficient prediction [16], which proposes an online learning algorithm with matching upper and lower bounds on the regret. However, they consider a fixed query probability that leads to a linear query complexity. Our algorithm, inspired by uncertainty sampling in active learning, achieves an improved query complexity with the adaptive query strategy while maintaining a comparable regret.

\begin{table}
\begin{tabular}{l|l l l l l l l} \hline \multirow{2}{*}{**Setup Algorithm**} & **Online bagging** & **Helge** & **EXP3** & **EXP4** & **Query by Committee** & **ModelPicker** & **CAMS** \\  & [54] & [27] & [7] & [7] & [65] & [39] & (ours) \\  & bagging & online learning & bandit & contextual bandits & active learning & model selection & (ours) \\ \hline model selection\({}^{\dagger}\) & \(\times\) & ✓ & ✓ & \(\times\) & ✓ & ✓ & ✓ \\ \hline full-information & ✓ & ✓ & \(\times\) & \(\times\) & ✓ & ✓ & ✓ \\ \hline active queries & \(\times\) & \(\times\) & \(\times\) & \(\times\) & ✓ & ✓ & ✓ \\ \hline context-aware & \(\times\) & \(\times\) & \(\times\) & ✓ & \(\times\) & \(\times\) & ✓ \\ \hline \end{tabular}

* We regard “arm” as “modroids” when comparing CAMS against bandit algorithms, such as EXP3EXP4.
* Online ensemble learning aims to build a competing model by aggregating multiple models rather than selecting the best model (for a given context).

\end{table}
Table 1: Comparing CAMS against related work in terms of problem setup.

Stream-based Active learning.Active learning aims to achieve a target learning performance with fewer training examples [64]. The active learning framework closest to our setting is query-by-committee (QBC) [65], in particular under the stream-based setting [35, 47]. QBC maintains a committee of hypotheses; each committee member votes on the label of an instance, and the instances with the maximal disagreement among the committee are considered the most informative labels. Note that existing stream-based QBC algorithms are designed and analyzed assuming i.i.d. data streams. In comparison, our work uses a different query strategy as well as a novel model recommendation strategy, which also applies to the adversarial setting.

Active model selection.Active model selection captures a broad class of problems where model evaluations are expensive, either due to (1) the cost of evaluating (or "probing") a model, or (2) the cost of annotating a training example. Existing works under the former setting [49, 59] and online setting [21, 66] often ignore context information and data annotation cost, and only consider _partial_ feedback on the models being evaluated/ probed on i.i.d. data. The goal is to identify the best model with as few model probes as possible. This is quite different from our problem setting which considers the full information setting as well as non-negligible data annotation cost. [71] proposes that the optimal model choice is influenced by the sample size rather than any individual sample feature. [44] addresses the active model selection problem, however both works do not adopt a stream-based approach. For the later, apart from Karimi et al. [39], an online contextual-free model selection work, as shown in Table 1, most existing works assume a pool-based setting where the learner can choose among the pool of unlabeled data [4, 29, 43, 60, 61, 68, 76], and the goal is to identify the best model with a minimal set of labels.

## 3 Problem Statement

Notations.Let \(\mathcal{X}\) be the input domain and \(\mathcal{Y}:=\{0,\ldots,c-1\}\) be the set of \(c\) possible class labels for each input instance. Let \(\mathcal{F}=\{f_{1},\ldots,f_{k}\}\) be a set of \(k\) pre-trained classifiers over \(\mathcal{X}\times\mathcal{Y}\). A model selection policy \(\pi:\mathcal{X}\rightarrow\Delta^{k-1}\) maps any input instance \(\textbf{{x}}\in\mathcal{X}\) to a distribution over the pre-trained classifiers \(\mathcal{F}\), specifying the probability \(\pi\left(\textbf{{x}}\right)\) of selecting each classifier under input \(x\). Here, \(\Delta^{k-1}\) denotes the \(k\)-dimensional probability simplex \(\left\{\textbf{{w}}\in\mathbb{R}^{k}:\left|\textbf{{w}}\right|=1,\textbf{{w} }\geq 0\right\}\). One can interpret a policy \(\pi\) as an "expert" that suggests which model to select for a given _context_ \(x\).

Let \(\Pi\) be a collection of model selection policies. In this paper, we propose an _extended policy set_\(\Pi^{*}:=\Pi\cup\{\pi_{i}^{\text{const}},\ldots,\pi_{k}^{\text{const}}\}\) which also includes constant policies that always suggest a fixed model. Here, \(\pi_{j}^{\text{const}}(\cdot):=\epsilon_{j}\), and \(\textbf{{e}}_{j}\in\Delta^{k-1}\) denotes the canonical basis vector with \(e_{j}=1\). Unless otherwise specified, we assume \(\Pi\) is finite with \(|\Pi|=n\), and \(|\Pi^{*}|\leq n+k\). As a special case, when \(\Pi=\emptyset\), our problem reduces to the contextual-free setting.

The contextual active model selection protocol.Assume that the learner knows the set of classifiers \(\mathcal{F}\) as well as the set of model selection policies \(\Pi\). At round \(t\), the learner receives a data instance \(\textbf{{x}}_{t}\in\mathcal{X}\) as the context for the current round, and computes the predicted label \(\hat{y}_{t,j}=f_{j}\left(\textbf{{x}}_{t}\right)\) for each pre-trained classifier indexed by \(j\in[k]\). Denote the vector of predicted labels by all \(k\) models by \(\tilde{\textbf{{y}}}_{t}:=[\hat{y}_{t,1},\ldots,\hat{y}_{t,k}]^{\top}\). Based on previous observations, the learner identifies a model/classifier \(f_{j_{t}}\) and makes a prediction \(\hat{y}_{t,j_{t}}\) for the instance \(\textbf{{x}}_{t}\). Meanwhile, the learner can obtain the true label \(y_{t}\)_only if_ it decides to query \(\textbf{{x}}_{t}\). Upon observing \(y_{t}\), the learner incurs a _query cost_, and receives a (full) loss vector \(\boldsymbol{\ell}_{t}=\mathbb{I}_{\{\tilde{y}_{t}\neq y_{t}\}}\), where the \(j\)th entry \(\ell_{t,j}:=\mathbb{I}_{\{\hat{y}_{t,j}\neq y_{t}\}}\) corresponds to the 0-1 loss for model \(j\in[k]\) at round \(t\). The learner can then use the queried labels to adjust its model selection criterion for future rounds.

Performance metric.If \(\textbf{{x}}_{t}\) is misclassified by the model \(j_{t}\) selected by learner at round \(t\), i.e. \(\hat{y}_{t,j_{t}}\neq y_{t}\), it will be counted towards the _cumulative loss_ of the learner, regardless of the learner making a query. Otherwise, no loss will be incurred for that round. For a learning algorithm \(\mathcal{A}\), its cumulative loss over \(T\) rounds is defined as \(L_{T}^{\mathcal{A}}:=\sum_{t=1}^{T}\ell_{t,j_{t}}\).

In practice, the choice of model \(j_{t}\) at round \(t\) by the learner \(\mathcal{A}\) could be random: For _stochastic_ data streams where \(\left(\textbf{{x}},y\right)\) arrives i.i.d., the learner may choose different models for different random realizations of \(\left(\textbf{{x}}_{t},y_{t}\right)\). For the _adversarial_ setting where the data stream \(\left\{\left(\textbf{{x}}_{t},y_{t}\right)\right\}_{t\geq 1}\) is chosen by an adversary before each round, the learner may randomize its choice of model to avoid a constant loss at each round [33]. Therefore, due to the randomness of \(L_{T}^{\mathcal{A}}\), we consider the _expected_ cumulative loss \(\mathbb{E}[L_{T}^{\mathcal{A}}]\) as a key performance measure of the learner \(\mathcal{A}\). To characterize the progress of \(\mathcal{A}\), we consider the _regret_--formally defined as follows-- as the difference between the cumulative loss received by the learner and the loss if the learner selects the "best policy" \(\pi^{*}\in\Pi^{*}\) in hindsight.

For stochastic data streams, we assume that each policy \(i\) recommends the _most probable_ model w.r.t. \(\pi_{i}(\bm{x}_{t})\) for context \(\bm{x}_{t}\). We use \(\text{maxind}(\bm{w}):=\arg\max_{j:w_{j}\in\bm{w}}w_{j}\) to denote the index of the maximal-value entry2 of \(\bm{w}\). Since \((\bm{x},y)\) are drawn i.i.d., we define \(\mu_{i}=\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}_{\bm{x}_{t},y_{t}}\big{[}\ell_{t, \text{maxind}(\pi_{i}(\bm{x}_{t}))}\big{]}\). This leads to the pseudo-regret for the stochastic setting over \(T\) rounds, defined as

Footnote 2: Assume ties are broken randomly.

\[\overline{\mathcal{R}}_{T}\left(\mathcal{A}\right)=\mathbb{E}[L_{T}^{\mathcal{ A}}]-T\min_{i\in[\|\Pi^{*}\|]}\mu_{i}.\] (1)

In an adversarial setting, since the data stream (and hence the loss vector) is determined by an adversary, we consider the reference best policy to be the one that minimizes the loss on the adversarial data stream, and the expected regret

\[\mathcal{R}_{T}\left(\mathcal{A}\right)=\mathbb{E}[L_{T}^{\mathcal{A}}]-\min_ {i\in[\|\Pi^{*}\|]}\sum_{t=1}^{T}\tilde{\ell}_{t,i},\] (2)

where \(\tilde{\ell}_{t,i}:=\left\langle\pi_{i}\left(\bm{x}_{t}\right),\bm{\ell}_{t}\right\rangle\) denotes the expected loss if the learner commits to policy \(\pi_{i}\), randomizes and selects \(j_{t}\sim\pi_{i}\left(\bm{x}_{t}\right)\) (and receives loss \(\ell_{t,j_{t}}\)) at round \(t\). Our goal is to devise a principled online active model selection strategy to minimize the regret as defined in (1) or (2), while maintaining a low total query cost. For convenience, we refer the readers to App. B for a summary of the notations used in this paper.

## 4 Contextual Active Model Selection

In this section, we introduce our main algorithm for both stochastic and adversarial data streams.

**Contextual model selection.** Our key insight underlying the contextual model selection strategy extends from the _online learning with expert advice_ framework [13, 27]. We start by appending the constant policies that always pick single pre-trained _models_ to form the extended policy set \(\Pi^{*}\) (Line 3, in Fig. 1). This allows CAMS to be at least as competitive as the best model. Then, at each round, CAMS maintains a probability distribution over the (extended) policy set \(\Pi^{*}\), and updates those according to the observed loss for each policy. We use \(\bm{q}_{t}:=(q_{t,i})_{i\in[\Pi^{*}]}\) to denote the probability distribution over \(\Pi^{*}\) at \(t\). Specifically, the probability \(q_{t,i}\) is computed based on the exponentially weighted cumulative loss, i.e. \(q_{t,i}\propto\exp\left(-\eta_{t}\tilde{L}_{t-1,i}\right)\) where \(\tilde{L}_{t,i}:=\sum_{\tau=1}^{t}\tilde{\ell}_{\tau,i}\) denotes the cumulative loss of policy \(i\).

Figure 1: The Contextual Active Model Selection (CAMS) algorithm

For adversarial data streams, it is natural for both the online learner and the model selection policies to randomize their actions to avoid linear regret [33]. Following this insight, CAMS randomly samples a policy \(i_{t}\sim\bm{q}_{t}\), and--based on the current context \(\bm{x}_{t}\)--samples a classifier \(j_{t}\sim\pi_{i_{t}}\left(\bm{x}_{t}\right)\) to recommend at round \(t\).

Under the stochastic setting, CAMS adopts a weighted majority strategy [45] when selecting models. The vector of each model's weighted votes from the policies, \(\bm{w}_{t}=\sum_{i\in\left\|\Pi^{*}\right\|}q_{t,i}\pi_{i}(\bm{x}_{t})\), is interpreted as a distribution induced by the weighted policy. The model \(j_{t}=\text{maxind}(\bm{w}_{t})\) which receives the highest probability becomes the recommended model at round \(t\). This deterministic model selection strategy is commonly used in stochastic online optimization [33]. An alternative strategy is to take a randomized approach as in the adversarial setting, or take a Follow-the-Leader approach [42] and go with the most probable model recommended by the most probable policy (i.e. use \(\bm{w}_{t}=\pi_{\text{maxind}(\bm{q}_{t})}(\bm{x}_{t})\)).As shown in experimental results section and further discussed in Appendix (outperformance over the best policy/expert section), CAMS outperforms these policies in a wide range of practical applications. The model selection steps are detailed in Line 5-9 in Fig. 1.

**Active queries.** Under a limited budget, we intend to query the labels of those instances that exhibit significant disagreement among the pre-trained models \(\mathcal{F}\). To achieve this goal, we design an adaptive query strategy with query probability \(z_{t}\). Concretely, given context \(\bm{x}_{t}\), model predictions \(\hat{\bm{y}}_{t}\) and model distribution \(\bm{w}_{t}\), we denote by \(\hat{\ell}_{t}^{y}:=\left\langle\bm{w}_{t},\mathbb{I}\left\{\hat{\bm{y}}_{t} \neq y\right\}\right\rangle\) as the expected loss if the true label is \(y\). We characterize the model disagreement as

\[\mathfrak{E}\left(\hat{\bm{y}}_{t},\bm{w}_{t}\right):=\frac{1}{c}\sum_{y\in \mathcal{Y},\hat{\ell}_{t}^{y}\in\left(0,1\right)}\bar{\ell}_{t}^{y}\log_{c} \frac{1}{\bar{\ell}_{t}^{y}}.\] (3)

Intuitively, when \(\bar{\ell}_{t}^{y}\) is close to \(0\) or \(1\), there is little disagreement among the models in labeling \(\bm{x}_{t}\) as \(y\), otherwise there is significant disagreement. We capture this insight with function \(h(x)=-x\log x\). Since the label \(y_{t}\) is unknown upfront when receiving \(\bm{x}_{t}\), we iterate through all the possible labels \(y\in\mathcal{Y}\) and take the average value as in Eq. (3). Note that \(\mathfrak{E}\) takes a similar algebraic form to the entropy function, although it does not inherit the information-theoretic interpretation.

With the model disagreement term defined above, we consider an adaptive query probability3

Footnote 3: For convenience of discussion, we assume that those rounds where all policies in \(\Pi^{*}\) select the same models or all models \(\mathcal{F}\) make the same predictions are removed as a precondition.

\[z_{t}=\max\big{\{}\delta_{0}^{t},\mathfrak{E}\left(\hat{\bm{y}}_{t},\bm{w}_{t }\right)\big{\}},\] (4)

where \(\delta_{0}^{t}=\frac{1}{\sqrt{t}}\in\left(0,1\right]\) is an adaptive lower bound on the query probability to encourage exploration at an early stage. The query strategy is summarized in Line 10-14 in Fig. 1.

Model updates.Now define \(U_{t}\sim\text{Ber}\left(z_{t}\right)\) as a binary query indicator that is sampled from a Bernoulli distribution parametrized by \(z_{t}\). Upon querying the label \(y_{t}\), one can calculate the loss for each model \(f_{j}\in\mathcal{F}\) as \(\ell_{t,j}=\mathbb{I}\left\{\hat{y}_{t,j}\neq y_{t}\right\}\). Since CAMS does not query all the i.i.d. examples, we introduce an unbiased loss estimator for the models, defined as \(\hat{\ell}_{t,j}=\frac{\ell_{t,j}}{z_{t}}U_{t}\). The unbiased loss of policy \(\pi_{i}\in\Pi^{*}\) can then be computed as \(\tilde{\ell}_{t,i}=\left\langle\pi_{i}(\bm{x}_{t}),\hat{\ell}_{t,j}\right\rangle\). In the end, CAMS computes the (unbiased) cumulative loss of policy \(\pi_{i}\) as \(\tilde{L}_{T,i}=\sum_{t=1}^{T}\tilde{\ell}_{t,i}\), which is used to update the policy probability distribution in next round. Pseudocode for the model update steps is summarized in Line 15-21 in Fig. 1.

Remark.CAMS runs efficiently with time complexity \(O\left(nk\right)\) per round and space complexity \(O\left(\left(n+k\right)\cdot k\right)\). At each round, each model selection policy specifies a probability distribution over the models for the given context. When these distributions correspond to constant Dirac delta distributions (regardless of the context), the problem reduces to the context-free problem investigated by Karimi et al. [39].

## 5 Theoretical Analysis

We now present theoretical bounds on the regret (defined in Eq. (1) and Eq. (2), respectively) and the query complexity of CAMS for both the stochastic and the adversarial settings.

### Stochastic setting

Under the stochastic setting, the cumulative loss of CAMS over T rounds--as specified by the Recommend procedure--is \(L_{T}^{\text{CAMS}}=\sum_{t=1}^{T}\hat{\ell}_{t,\text{maxind}(\boldsymbol{w}_{t})}\) where recall \(\boldsymbol{w}_{t}=\sum_{i\in\left|\Pi^{*}\right|}q_{t,i}\pi_{i}(\boldsymbol{x} _{t})\) is the probability distribution over \(\mathcal{F}\) induced by the weighted policy.

Let \(i^{*}=\arg\min_{i\in\left|\left|\Pi^{*}\right|\right|}\mu_{i}\) be the index of the best policy (\(\mu_{i}\) denotes the expected loss of policy \(i\), as defined in problem statement section. The cumulative expected loss of policy \(i^{*}\) is \(T\mu_{i^{*}}\); therefore the expected pseudo-regret (Eq. (1)) is \(\overline{\mathcal{R}}_{T}\left(\text{CAMS}\right)=\mathbb{E}\left[\sum_{t=1} ^{T}\hat{\ell}_{t,\text{maxind}(\boldsymbol{w}_{t})}\right]-T\mu_{i^{*}}\).

Define \(\Delta:=\min_{i\neq i^{*}}(\mu_{i}-\mu_{i^{*}})\) as the minimal sub-optimality gap4 in terms of the expected loss against the best policy \(i^{*}\). Furthermore, let \(\boldsymbol{w}_{t}^{t}:=\pi_{i^{*}}(\boldsymbol{x}_{t})\) be probability distribution over \(\mathcal{F}\) induced by policy \(i^{*}\) at round \(t\). We define \(\gamma:=\min_{\boldsymbol{x}_{t}}\{\max_{w_{j}\in\boldsymbol{w}_{t}^{*}}w_{j} -\max_{w_{j}\in\boldsymbol{w}_{t}^{*},j\neq\text{maxind}(\boldsymbol{w}_{t}^{ *})}w_{j}\}\) (5) as the minimal probability gap between the most probable model and the rest (assuming no ties) induced by the best policy \(i^{*}\). We further define \(b=p_{\min}\log_{c}\left(1/p_{\min}\right)\), where \(p_{\min}=\min_{s,i}\pi\left(\mathbf{x}_{s}\right)\) denotes the minimal model selection probability by any policy5. As our first main theoretical result, we show that, without exhaustively querying the labels of the stochastic stream, CAMS achieves constant expected regret.

Footnote 4: w.l.o.g. assume there is a single best policy, and thus \(\Delta>0\).

Footnote 5: We assume \(p_{\min}>0\) per the policy regularization criterion in Appendix C.3. (cf. Algorithm 1 on “Regularized policy \(\bar{\pi}(\mathbf{x}_{t})\))”.

**Theorem 1**.: _(Regret) In the stochastic environment, with probability at least \(1-\delta\), CAMS achieves constant expected pseudo regret \(\overline{\mathcal{R}}_{T}\left(\text{CAMS}\right)=\left(\frac{\ln\left|\Pi^{*} \right|-1+\sqrt{\ln\left|\Pi^{*}\right|\cdot 2b^{2}\ln\frac{q}{4}}}{\sqrt{\ln\left|\Pi^{*} \right|\Delta}}\right)^{2}\)._

Note that in the stochastic setting, a lower bound of \(\Omega\left(\left(\log\Pi^{*}\right)/\Delta\right)\) was shown in Mourtada and Gaiffas [50] for online learning problems with expert advice under the full information setting (i.e. assuming labels are given for all data points in the stochastic stream). To establish the proof of Theorem 1, we consider a novel procedure to connect the weighted policy by CAMS to the best policy \(\pi_{i^{*}}\). Conceptually, we would like to show that, after a _constant_ number of rounds \(\tau_{\text{const}}\), with high probability, the model selected by CAMS (Line 32) will be the same as the one selected by the best policy \(i^{*}\). In that way, the expected pseudo regret will be dominated by the maximal cumulative loss up to \(\tau_{\text{const}}\). Toward this goal, we first bound the weight of the best policy \(w_{t,i^{*}}\) as a function of \(t\), by choosing a proper learning rate \(\eta_{t}\left(\text{CAMS},\text{Line }23\right)\). Then, we identify a constant threshold \(\tau_{\text{const}}\), beyond which CAMS exhibits the same behavior as \(\pi_{i^{*}}\) with high probability. Finally, we obtain the regret bound by inspecting the regret at the two stages separately. The formal statement of Theorem 1 and the detailed proof are deferred to App. E.1.

Next, we provide an upper bound on the query complexity in the stochastic setting.

**Theorem 2**.: _(Query Complexity). For \(c\)-class classification problems, with probability at least \(1-\delta\), the expected number of queries made by CAMS over \(T\) rounds is upper bounded by \(\left(\left(\frac{\ln\frac{\left|\Pi^{*}\right|-1}+\sqrt{\ln\left|\Pi^{*} \right|\cdot 2b^{2}\ln\frac{q}{4}}}{\sqrt{\ln\left|\Pi^{*}\right|\Delta}}\right)^{ 2}+T\mu_{i^{*}}\right)\frac{\ln T}{c\ln c}\)._

Theorem 2 is built upon Theorem 1, where the the key idea behind the proof is to relate the number of updates to the regret. When \(T\mu_{i^{*}},\tilde{L}_{T,*}\) are regarded as constants (given by an oracle), the query-complexity bound is then sub-linear _w.r.t._\(T\). Note that the number of class labels \(c\) affects the quality of the query complexity bound. The intuition behind this result is, with larger number of classes, _each query may carry more information upon observation_. For instance, in an extreme case where only one expert always recommends the best model and others gives random recommendations of models (and predicts random labels), having more classes lowers the chance of a model making the correct guess, and therefore helps to "filter out" those suboptimal experts in fewer rounds--hence being more query efficient. We defer the proof of Theorem 2 to App. E.2.

### Adversarial setting

Now we consider the adversarial setting. Let \(\tilde{L}_{T,*}:=\min_{i\in\left[\left|\Pi^{*}\right|\right]}\sum_{t=1}^{T} \tilde{\ell}_{t,i}\) be the cumulative loss of the best policy. The expected regret (Eq. (2)) for CAMS equals to \(\mathcal{R}_{T}\left(\text{CAMS}\right)=\left(\frac{1}{\sqrt{\ln\left|\Pi^{*} \right|}}\right)^{2}\).

\(\mathbb{E}\big{[}\sum_{t=1}^{T}\langle\bm{q}_{t},\tilde{\bm{\ell}}_{t}\rangle\big{]} -\tilde{L}_{T,*}\). We show that under the adversarial setting, CAMS achieves sub-linear regret in \(T\) without accessing all labels.

**Theorem 3**.: _(Regret) Let \(c\) be the number of classes and \(\rho_{t}\) be specified as Line 26-27 in the SetRate procedure. Under the adversarial setting, the expected regret of CAMS is bounded by \(2c\sqrt{\ln c/\max\{\rho_{T},\sqrt{1/T}\}}\cdot\sqrt{T\log|\Pi^{*}|}\)._

The proof is provided in App. F.1. Assuming \(\rho_{t}\) to be a constant, our regret upper bound in Theorem 3 matches (up to constants) the lower bound of \(\Omega\left(\sqrt{T\ln|\Pi^{*}|}\right)\) for online learning problems with expert advice under the full information setting [15; 63] (i.e. assuming labels are given for all data points). Hereby, the decaying learning rate \(\eta_{t}\) as specified in Line 27 is based on two parameters, where \(1/\sqrt{t}\) corresponds to the lower bound \(\delta_{0}^{t}\) on the query probability, and \(\rho_{t}\triangleq 1-\max_{\tau\in[t-1]}\langle\bm{w}_{\tau},\mathbb{I}\left\{ \tilde{\bm{y}}_{\tau}=y_{\tau}\right\}\)) (6) is a (data-dependent) term that is chosen to reduce the impact of the randomized query strategy on the regret bound (especially when \(t\) is large). Intuitively, \(\rho_{t}\) relates to the skewness of the policy where the \(\max\) term corresponds to the maximal probability of most probable mispredicted label over \(t\) rounds. Note that in theory \(\rho_{t}\) can be small (e.g. CAMS may choose a constant policy \(\pi_{i}^{\text{const}}\in\Pi^{*}\) that mispredict the label for \(\bm{x}_{t}\), which leads to \(\rho_{t}=0\)); in such cases, our result still translates to a sublinear regret bound of \(O(c\sqrt{\log c}\cdot T^{\frac{1}{4}}\sqrt{\log|\Pi^{*}|})\). Furthermore, in practice, we consider to "regularize" the policies (App. D.4) to ensure that probability a policy selecting any model is bounded away from 0.

Finally, the following theorem (proof in App. F.2) establishes a query complexity bound of CAMS.

**Theorem 4**.: _(Query Complexity). Under the adversarial setting, the expected query complexity over \(T\) rounds is \(O\left(\ln T\left(\sqrt{\frac{T\log|\Pi^{*}|}{\max\{\rho_{T},\sqrt{1/T}\}}}+ \tilde{L}_{T,*}\right)\right).\)_

## 6 Experiments

Datasets.We evaluate our approach using five datasets: (1) CIFAR10 [41] contains 60,000 images from 10 different balanced classes. (2) DRIFT [73] is a tabular dataset with 128-dimensional features, based on 13,910 chemical sensor measurements of 6 types of gases at various concentration levels. (3) VERTEBRAL [5] is a biomedical tabular dataset which classifies 310 patients into three classes (Normal, Spondylolisthesis, Disk Hernia) based on 6 attributes. (4) HIV [74] contains over 40,000 compounds annotated with molecular graph features and binary labels (active, inactive) indicating their ability to inhibit HIV replication. (5) CovType [24] has 580K samples and contains details including slope, aspect, elevation, measurements of area, and type of forest cover.

Policy sets.We construct the policy sets \(\Pi\) for each dataset following a procedure similar to Meta-selector [48]. In this approach, a set of recommender algorithms is considered, and Meta-selector assigns varying ratings to these algorithms based on the specific user. Concretely, we first construct a set of models trained on different subsamples from each dataset. We then construct a set of policies, which include _malicious_, _normal_, _random_, and _biased_ policy types for each dataset

Figure 2: **Main results. Comparison of CAMS with 7 baselines across 4 diverse benchmarks in terms of cost effectiveness. We plot the cumulative loss as we increase the query cost for a fixed number of rounds \(T\) and maximal query cost \(B\) (from left to right: \(T=10000,3000,80,4000\), and \(B=1200,2000,80,2000\)). CAMS outperforms all baselines. Algorithms: 4 contextual {Oracle, CQBC, CIWAL, CAMS} and 4 non-contextual baselines {RS, QBC, IWAL, MP} are included (see Section ). 90% confident interval are indicated in shades.**

based on different models and features. Details on the classifiers and policies are provided in the supplemental materials. The _malicious_ policy provides contrary advice; the _random_ policy provides random advice; the _biased_ policy provides biased advice by training on a biased distribution for classifying specific classes. The _normal_ policy gives reasonable advice, being trained under a standard process on the training set. We represent the output of the \(i_{th}\) policy as \(\pi_{i}\left(\bm{x}_{t}\right)\), indicating the rewards distribution of all the base classifiers on \(\bm{x}_{t}\). In total, we create 80, 10, 6, 4 classifiers and 85, 11, 17, 20 policies for CIFAR10, DRIFT, VERTEBRAL, and HIV, respectively.

Baselines.We evaluate CAMS against both _contextual_ and _non-contextual_ active model selection baselines. We consider four _non-contextual_ baselines: (1) Random Query Strategy (RS) which queries the instance label with a fixed probability \(\frac{\hat{y}}{T}\); (2) Model Picker (MP) [39] that employs variance-based active sampling with a coin-flip query probability \(\max\left\{v\left(\hat{\bm{y}}_{t},\bm{w}_{t}\right),\eta_{t}\right\}\), where the variance term is defined as \(v\left(\hat{\bm{y}}_{t},\bm{w}_{t}\right)=\max_{y\in Y}\hat{\ell}_{t}^{y}\left( 1-\hat{\ell}_{t}^{y}\right)\); (3) Query by Committee (QBC) implementing committee-based sampling [22]; and (4) Importance Weighted Active Learning (IWAL) [8] that calculates query probability based on labeling disagreements of surviving classifiers. Since no _contextual_ baselines exist yet, we propose contextual versions of QBC and IWAL as (5) CQBC and (6) CIWAL. Both extensions maintain their respective original query strategies but incorporate the context into the cumulative rewards. For _model selection_, CAMS, MP, CQBC, and CIWAL recommend the classifier with the highest probability. The other baselines use Follow-the-Leader (FTL), recommending the model with the minimum cumulative loss for past queried instances. Finally, we add (7) Oracle to represent the best single policy with the minimum cumulative loss, with the same query strategy as CAMS.

### Main results

Fig. 2 visualizes the _cost effectiveness_ of CAMS and the baselines. Here, we define _cost effectiveness_ as the measure of how quickly the cumulative loss decreases in response to an increase in query cost. Fig. 2 demonstrates that CAMS outperforms all the comparison methods across all benchmarks. Remarkably, it outperforms even the oracle on the VERTEBRAL (Fig. 1(c)) and HIV (Fig. 1(d)) benchmarks with fewer than 10 and 20 queries, respectively. In the case of the VERTEBRAL benchmark, CAMS outperforms the best baseline in query cost by a margin of \(20\%\), despite the fact that 11 out of the 17 experts provided malicious or random advice. This level of performance is attained by utilizing an active query strategy to retrieve highly informative data, thereby maximizing the differentiation between models and policies within the constraints of a limited budget. Additionally, the model selection strategy allows for effectively combining the expertise among the experts.

Figure 3: **Ablation studies.** (a) Comparing three query strategies {CAMS, variance-based, random} under same model selection policy. (b) Comparing the increasing rate of CAMS’ query cost over other baselines. (c) Comparing CAMS with MP in context-free environment. (d) Evaluating the performance of CAMS under a pure adversarial setting. (e) Large dataset. (f,g) Adjustable query probability. (h) CAMS outperforms the best single policy. The ablation study (a)-(d) is conducted on CIFAR10. For additional results on other benchmarks, please refer to the supplemental material.

### Ablation studies

**Effectiveness of active querying.** In Fig. 2(a) and Fig. 2(b), we perform ablation studies to demonstrate the effectiveness of our active query strategy. We fix the model recommendation strategy as the one used by CAMS, and compare three query strategies: (1) CAMS, (2) the state-of-the-art variance-based query strategy from Model Picker [39] (referred to as "variance"), and (3) a random query strategy. Figure 2(a) demonstrates that CAMS has the fastest convergence rate in terms of cumulative loss on CIFAR10, implying effective use of queried labels. Furthermore, CAMS not only achieves the minimum cumulative loss but also incurs significantly lower query costs, with reductions of 71% and 95% compared to the variance and random strategies respectively as showed in Fig. 2(b). This suggests that CAMS selectively queries data to optimize policy improvement, whereas the other strategies may query unnecessary labels, including potentially noisy or uninformative ones, which impede policy improvement and convergence.

**Robustness.** In Fig. 2, 2(c), 2(d), 2(e), 2(f), and 2(g), CAMS exhibits robustness in a variety of environmental settings. Firstly, As shown in Fig. 2, CAMS outshines other methods in a contextual environment, whereas in Fig. 2(c), a non-contextual (no experts) environment, it achieves comparable performance to the state-of-the-art Model Picker in identifying the best classifier. Secondly, CAMS is robust in both stochastic and adversarial environments. As demonstrated in Fig. 2, CAMS surpasses other methods in a stochastic environment. Additionally, as illustrated in Fig. 2(d), in a worst-case adversarial environment, CAMS effectively recovers from adversarial actions and approaches the performance of the best classifier (see App. G.5). We further observe that CAMS demonstrates robustness to varying scales of data, where the online stream sizes range from 80 to 10K (Fig. 2) to 100K (Fig. 2(e), where we randomly sample 100K samples from the CovType dataset [24]).

In Fig. 2, we assume that the stream length \(T\) is hidden and not used as input to CAMS. Under the stochastic setting, however, knowing \(T\) can provide additional information that one can leverage to optimize the query probability, thereby giving an advantage to some of the baseline algorithms (e.g. random). As an ablation study, in Fig. 2(f) and Fig. 2(g), we assume the stochastic setting where the total length \(T\) of the online stream is given. Given the stream length \(T\) and query budget \(b\), we may optimize each algorithm by scaling their query probabilities, so that each algorithm allocates its query budget to the top \(b\) informative labels in the entire online stream based on its own query criterion. CAMS still outperform the baselines under the setting.

**Improvement over the best classifier and policy.** Fig. 2(h) demonstrates that when provided with good policies, CAMS formulates a stronger policy which incurs no regret. CAMS has the potential to outperform an oracle, especially in rounds where the oracle does not make the optimal recommendation. For instance, in the stochastic version of CAMS (as shown in lines 22-23 and 30-32 of Fig. 1), CAMS recommends a model using a weighted majority vote among all policies, enabling the formation of a new policy in each round by amalgamating the strengths of each sub-optimal policy. This adaptive strategy can potentially outperform any single policy. Moreover, in most real-world scenarios and conducted experiments (as depicted in App. G.6), data streams may not be strictly stochastic, and therefore no single policy consistently performs the best. In such cases, CAMS's weighted policy may find an enhanced combination of "advices", leading to improved performance.

## 7 Conclusion

We introduced CAMS, an online contextual active model selection framework based on a novel model selection and active query strategy. The algorithm was motivated by many real-world use cases that need to make decision by taking both contextual information and the cost into consideration. We have demonstrated CAMS's compelling performance of using the minimum query cost to learn the optimal contextual model selection policy on several diverse online model selection tasks. In addition to the promising empirical performance, we also provided rigorous theoretical guarantees on the regret and query complexity for both stochastic and adversarial settings. We hope our work can inspire future works to handle more complex real-world model selection tasks (e.g. beyond classification or non-uniform loss functions, etc. where our analysis does not readily apply).

## Acknowledgements

This work is supported in part by the RadBio-AI project (DE-AC02-06CH11357), U.S. Department of Energy Office of Science, Office of Biological and Environment Research, the IMPROVE project under contract (75N91019F00134, 75N91019D00024, 89233218CNA000001, DE-AC02-06-CH11357, DE-AC52-07NA27344, DE-AC05-00OR22725), the Laboratory Directed Research and Development (LDRD) funding from Argonne National Laboratory provided by the Director, Office of Science, of the U.S. Department of Energy under Contract No. DE-AC02-06CH11357, the Exascale Computing Project (17-SC-20-SC), a collaborative effort of the U.S. Department of Energy Office of Science and the National Nuclear Security Administration, the University of Chicago Joint Task Force Initiative, the AI-Assisted Hybrid Renewable Energy, Nutrient, and Water Recovery project (DOE DE-EE0009505), and the National Science Foundation under Grant No. IIS 2313131 and IIS 2332475.

## References

* Abd-Ellah et al. [2018] Mahmoud Khaled Abd-Ellah, Ali Ismail Awad, Ashraf AM Khalaf, and Hesham FA Hamed. Two-phase multi-model automatic brain tumour diagnosis system from magnetic resonance images using convolutional neural networks. _EURASIP Journal on Image and Video Processing_, 2018(1):1-10, 2018.
* Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* Aggarwal et al. [2021] Ravi Aggarwal, Viknesh Sounderajah, Guy Martin, Daniel SW Ting, Alan Karthikesalingam, Dominic King, Hutan Ashrafian, and Ara Darzi. Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis. _NPJ digital medicine_, 4(1):65, 2021.
* Ali et al. [2014] Alnur Ali, Rich Caruana, and Ashish Kapoor. Active learning with model selection. In _Proceedings of the AAAI conference on artificial intelligence_, volume 28, 2014.
* Asuncion and Newman [2007] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.
* Auer et al. [2002] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine learning_, 47(2):235-256, 2002.
* Auer et al. [2002] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. _SIAM journal on computing_, 32(1):48-77, 2002.
* Beygelzimer et al. [2009] Alina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learning. In _Proceedings of the 26th annual international conference on machine learning_, pages 49-56, 2009.
* Beygelzimer et al. [2011] Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pages 19-26. JMLR Workshop and Conference Proceedings, 2011.
* Breiman [2001] Leo Breiman. Random forests. _Machine learning_, 45(1):5-32, 2001.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Bubeck et al. [2012] Sebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends(r) in Machine Learning_, 5(1):1-122, 2012.
* Burtini et al. [2015] Giuseppe Burtini, Jason Loeppky, and Ramon Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. _arXiv preprint arXiv:1510.00757_, 2015.
* Cesa-Bianchi and Lugosi [2006] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge university press, 2006.
* Cesa-Bianchi et al. [1997] Nicolo Cesa-Bianchi, Yoav Freund, David Haussler, David P Helmbold, Robert E Schapire, and Manfred K Warmuth. How to use expert advice. _Journal of the ACM (JACM)_, 44(3):427-485, 1997.

* [16] Nicolo Cesa-Bianchi, Gabor Lugosi, and Gilles Stoltz. Minimizing regret with label efficient prediction. _IEEE Transactions on Information Theory_, 51(6):2152-2162, 2005.
* [17] Austin Clyde, Xuefeng Liu, Thomas Brettin, Hyunseung Yoo, Alexander Partin, Yadu Babuji, Ben Blaiszik, Jamaludin Mohd-Yusof, Andre Merzky, Matteo Turilli, et al. Ai-accelerated protein-ligand docking for sars-cov-2 is 100-fold faster with no significant change in detection. _Scientific Reports_, 13(1):2105, 2023.
* [18] Corinna Cortes and Vladimir Vapnik. Support-vector networks. _Machine learning_, 20(3):273-297, 1995.
* [19] Thomas Cover and Peter Hart. Nearest neighbor pattern classification. _IEEE transactions on information theory_, 13(1):21-27, 1967.
* [20] Jan Salomon Cramer. The origins of logistic regression. 2002.
* [21] Ashok Cutkosky, Christoph Dann, Abhimanyu Das, Claudio Gentile, Aldo Pacchiano, and Manish Purohit. Dynamic balancing for model selection in bandits and rl. In _International Conference on Machine Learning_, pages 2276-2285. PMLR, 2021.
* [22] Ido Dagan and Sean P Engelson. Committee-based sampling for training probabilistic classifiers. In _Machine Learning Proceedings 1995_, pages 150-157. Elsevier, 1995.
* [23] Jiao Du, Weisheng Li, Ke Lu, and Bin Xiao. An overview of multi-modal medical image fusion. _Neurocomputing_, 215:3-20, 2016.
* [24] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.
* [25] Joseph L Durant, Burton A Leland, Douglas R Henry, and James G Nourse. Reoptimization of mdl keys for use in drug discovery. _Journal of chemical information and computer sciences_, 42(6):1273-1280, 2002.
* [26] Ronald A Fisher. The statistical utilization of multiple measurements. _Annals of eugenics_, 8(4):376-386, 1938.
* [27] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. _Journal of computer and system sciences_, 55(1):119-139, 1997.
* [28] Yoav Freund, Robert Schapire, and Naoki Abe. A short introduction to boosting. _Journal-Japanese Society For Artificial Intelligence_, 14(771-780):1612, 1999.
* [29] Jacob R Gardner, Gustavo Malkomes, Roman Garnett, Kilian Q Weinberger, Dennis Barbour, and John P Cunningham. Bayesian active model selection with an application to automated audiometry. In _Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 2_, pages 2386-2394, 2015.
* [30] Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized trees. _Machine learning_, 63(1):3-42, 2006.
* [31] David J Hand and Keming Yu. Idiot's bayes--not so stupid after all? _International statistical review_, 69(3):385-398, 2001.
* [32] Katja Hansen, Franziska Biegler, Raghunathan Ramakrishnan, Wiktor Pronobis, O Anatole Von Lilienfeld, Klaus-Robert Muller, and Alexandre Tkatchenko. Machine learning predictions of molecular properties: Accurate many-body potentials and nonlocality in chemical space. _The journal of physical chemistry letters_, 6(12):2326-2331, 2015.
* [33] Elad Hazan. Introduction to online convex optimization. _arXiv preprint arXiv:1909.05207_, 2019.
* [34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [35] Shen-Shyang Ho and Harry Wechsler. Query by transduction. _IEEE transactions on pattern analysis and machine intelligence_, 30(9):1557-1571, 2008.
* [36] Steven CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. Online learning: A comprehensive survey. _Neurocomputing_, 459:249-289, 2021.

* [37] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. _arXiv preprint arXiv:1704.04861_, 2017.
* [38] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.
* [39] Mohammad Reza Karimi, Nezihre Merve Gurel, Bojan Karlas, Johannes Rausch, Ce Zhang, and Andreas Krause. Online active model selection for pre-trained classifiers. In _International Conference on Artificial Intelligence and Statistics_, pages 307-315. PMLR, 2021.
* [40] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* [41] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [42] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [43] Rui Leite and Pavel Brazdil. Active testing strategy to predict the best classification algorithm via sampling and metalearning. In _ECAI_, pages 309-314, 2010.
* [44] Weizhi Li, Gautam Dasarathy, Karthikeyan Natesan Ramamurthy, and Visar Berisha. Finding the homology of decision boundaries with active learning. _Advances in Neural Information Processing Systems_, 33:8355-8365, 2020.
* [45] Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. _Information and computation_, 108(2):212-261, 1994.
* [46] Ying-tao Liu, Yi Li, Zi-fu Huang, Zhi-jian Xu, Zhuo Yang, Zhu-xi Chen, Kai-xian Chen, Ji-ye Shi, and Wei-liang Zhu. Multi-algorithm and multi-model based drug target prediction and web server. _Acta Pharmacologica Sinica_, 35(3):419-431, 2014.
* [47] Chen Change Loy, Timothy M Hospedales, Tao Xiang, and Shaogang Gong. Stream-based joint exploration-exploitation active learning. In _2012 IEEE Conference on Computer Vision and Pattern Recognition_, pages 1560-1567. IEEE, 2012.
* [48] Mi Luo, Fei Chen, Pengxiang Cheng, Zhenhua Dong, Xiuqiang He, Jiashi Feng, and Zhenguo Li. Metaselector: Meta-learning for recommendation with user-level adaptive model selection. In _Proceedings of The Web Conference 2020_, pages 2507-2513, 2020.
* [49] Omid Madani, Daniel J Lizotte, and Russell Greiner. Active model selection. _arXiv preprint arXiv:1207.4138_, 2012.
* [50] Jaouad Mourtada and Stephane Gaiffas. On the optimality of the hedge algorithm in the stochastic regime. _Journal of Machine Learning Research_, 20:1-28, 2019.
* [51] Mohamad T Musavi, Wahid Ahmed, Khue Huang Chan, Kathleen B Faris, and Donald M Hummels. On the training of radial basis function classifiers. _Neural networks_, 5(4):595-603, 1992.
* [52] Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. _arXiv preprint arXiv:1506.03271_, 2015.
* [53] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* [54] Nikunj Cza and Stuart J Russell. Online bagging and boosting. In _International Workshop on Artificial Intelligence and Statistics_, pages 229-236. PMLR, 2001.
* [55] J. Ross Quinlan. Induction of decision trees. _Machine learning_, 1(1):81-106, 1986.
* [56] Carl Edward Rasmussen. Gaussian processes in machine learning. In _Summer school on machine learning_, pages 63-71. Springer, 2003.
* [57] Ryan M Rifkin and Ross A Lippert. Notes on regularized least squares. 2007.
* [58] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. _Journal of chemical information and modeling_, 50(5):742-754, 2010.

* [59] Marlesson RO Santana, Luckeciano C Melo, Fernando HF Camargo, Bruno Brandao, Anderson Soares, Renan M Oliveira, and Sandor Caetano. Contextual meta-bandit for recommender systems selection. In _Fourteenth ACM Conference on Recommender Systems_, pages 444-449, 2020.
* [60] Christoph Sawade, Niels Landwehr, Steffen Bickel, and Tobias Scheffer. Active risk estimation. In _ICML_, 2010.
* [61] Christoph Sawade, Niels Landwehr, and Tobias Scheffer. Active comparison of prediction models. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 25. Curran Associates, Inc., 2012.
* [62] Nick Schneider, Florian Piewak, Christoph Stiller, and Uwe Franke. Regnet: Multimodal sensor registration using deep neural networks. In _2017 IEEE intelligent vehicles symposium (IV)_, pages 1803-1810. IEEE, 2017.
* [63] Yevgeny Seldin and Gabor Lugosi. A lower bound for multi-armed bandits with expert advice. In _13th European Workshop on Reinforcement Learning (EWRL)_, 2016.
* [64] Burr Settles. Active learning literature survey. 2009.
* [65] H Sebastian Seung, Manfred Opper, and Haim Sompolinsky. Query by committee. In _Proceedings of the fifth annual workshop on Computational learning theory_, pages 287-294, 1992.
* [66] Naman Shukla, Arinbjorn Kolbeinsson, Lavanya Marla, and Kartik Yellepeddi. Adaptive model selection framework: An application to airline pricing. _arXiv preprint arXiv:1905.08874_, 2019.
* [67] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* [68] Masashi Sugiyama and Neil Rubens. A batch ensemble approach to active learning with model selection. _Neural Networks_, 21(9):1278-1286, 2008.
* [69] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1-9, 2015.
* [70] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _International conference on machine learning_, pages 6105-6114. PMLR, 2019.
* [71] Stephanie Van Der Pas and Peter Grunwald. Almost the best of three worlds: Risk, consistency and optional stopping for the switch criterion in nested model selection. _Statistica Sinica_, pages 229-253, 2018.
* [72] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. _arXiv preprint arXiv:1710.10903_, 2017.
* [73] Alexander Vergara, Shankar Vembu, Tuba Ayhan, Margaret A Ryan, Margie L Homer, and Ramon Huerta. Chemical gas sensor drift compensation using classifier ensembles. _Sensors and Actuators B: Chemical_, 166:320-329, 2012.
* [74] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical science_, 9(2):513-530, 2018.
* [75] Zhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong Li, Zhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et al. Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism. _Journal of medicinal chemistry_, 63(16):8749-8760, 2019.
* [76] Chicheng Zhang and Kamalika Chaudhuri. Beyond disagreement-based agnostic active learning. _Advances in Neural Information Processing Systems_, 27:442-450, 2014.

Impact Statements

This paper introduces a novel framework for adaptive model selection in label-efficient learning. By integrating robust online learning with active query strategies, our algorithm effectively adapts to varying data contexts and minimizes labeling efforts, crucial in domains requiring swift and accurate decisions, such as disease identification and financial predictions. Ethically, the framework's design promotes efficient and context-aware model selection, reducing potential biases associated with context-ignorant model selections. No major ethical concerns are anticipated, given the algorithm's generality and focus on solving practical problems.

## Appendix B Table of Notations Defined in the Main Paper

\begin{table}
\begin{tabular}{l l} \hline \hline
Summary of Regret and Query Complexity Bounds

We summarize the regret and query complexity bounds (if applicable) of related algorithms in Table 3.

_Remark 5_.: When \(T\mu_{i^{*}}\), \(\hat{L}_{T,*}\) are regarded as constants (given by an oracle), the query-complexity bound is then sub-linear _w.r.t._\(T\).

_Remark 6_.: Note that the number of class labels \(c\) affects the quality of the query complexity bound. The intuition behind this result is, with larger number of classes, _each query may carry more information upon observation_. For instance, in an extreme case where only one expert always recommends the best model and others gives random recommendations of models (and predicts random labels), having more classes lowers the chance of a model making the correct guess, and therefore helps to "filter out" those suboptimal experts in fewer rounds--hence being more query efficient.

_Remark 7_.: To prove the practical feasibility of CAMS, we have analyzed its time and space complexity. Our analysis shows that CAMS has a time complexity of \(O\left(Tnk\right)\) in total or \(O(nk)\) per round (due to the Recommend procedure under the stochastic setting), and a space complexity of \(O\left((n+k)\cdot k\right)\). Here, \(T\) refers to the online horizon, \(n\) denotes the number of policies, and \(k\) denotes the number of models. Taking into account these complexities, we can confirm that CAMS is practically feasible.

## Appendix D Supplemental Materials on Experimental Setup

### Baselines

Model Picker (MP)Model Picker [39] is a context-free online active model selection method inspired by EXP3. Model Picker aims to find the best classifier in hindsight while making a small number of queries. For query strategy, it uses a variance-based active learning sampling method to select the most informative label to query to differentiate a pool of models, where the variance is defined as \(v\left(\hat{\bm{y}}_{t},\bm{w}_{t}\right)=\max_{y\in\mathcal{Y}}\bar{\ell}_{t} ^{y}\left(1-\bar{\ell}_{t}^{y}\right)\). The coin-flip query probability is defined as \(\max\left\{v\left(\hat{\bm{y}}_{t},\bm{w}_{t}\right),\eta_{t}\right\}\) when \(v\left(\hat{\bm{y}}_{t},\bm{w}_{t}\right)\neq 0\), or 0 otherwise. For model recommendation, it uses an exponential weight algorithm to recommend the model with minimal exponential cumulative loss based on the past queried labels at each round.

Query by Committee (QBC)For query strategy, we have adapted the method of [22] as a disagreement-based selective sampling query strategy for online streaming data. We treat each classifier as a committee member and compute the query probability by measuring disagreement between models for each instance. The query function is coin-flip by vote entropy probability \(-\frac{1}{\log\min\left(k,|C|\right)}\sum_{c}\frac{V(c,x)}{k}\log\frac{V(c,x)} {k}\), where \(V\left(c,x\right)\) stands for the number of committee members assigning a class c for input context x and k is the number of committee. For the model recommendation part, we use the method of Follow-the-Leader (FTL) [42], which greedily recommends the model with the minimum cumulative loss for past queried instances.

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Algorithm** & **Regret** & **Query Complexity** \\ \hline Exp3 [42] & \(2\sqrt{Tk\log k}\) & – \\ \hline Exp3.p & \(5.15\sqrt{nT\log\frac{n}{\delta}}\) & – \\ \hline Exp4 [42] & \(\sqrt{2Tk\log n}\) & – \\ \hline Exp4.p & \(6\sqrt{kT\ln\frac{n}{\delta}}\) & – \\ \hline Model Picker\({}_{\text{\tiny{product}}}\) & \(62\max_{\Delta}\lambda/k\left(\lambda^{2}\log k\right)\) & \(\sqrt{2T\log k}(1+4\frac{c}{\lambda})\) \\ \(\lambda=\min_{j\in[k]\setminus(\cdot)}\Delta_{j}^{2}/\delta_{j}\) & \(\lambda=\frac{1}{2}\log\left(k\right)\) \\ \hline Model Picker\({}_{\text{\tiny{splectural}}}\) & \(2\sqrt{2T\log k}\) & \(5\sqrt{T\log k}+2L_{T,*}\) \\ \hline \multirow{2}{*}{\(\mathbf{CAMS}_{\text{\tiny{STOCHATE}}}\)} & \(\left(\frac{\ln\frac{|T|-1}{2}+\sqrt{\ln|T|\cdot 24^{2}\ln\frac{1}{\delta}}}{\sqrt{\ln|T| \cdot|\Delta}}\right)^{2}\) & \(\left(\left(\frac{\ln\frac{|T|-1}{2}+\sqrt{\ln|T|\cdot 24^{2}\ln\frac{1}{\delta}}}{\sqrt{ \ln|T|\cdot|\Delta}}\right)^{2}+T_{\mu_{*}}\right)\frac{\ln T}{\c\ln c}\) \\ \hline \multirow{2}{*}{\(\mathbf{CAMS}_{\text{\tiny{splectural}}}\)} & \multirow{2}{*}{\(2c\sqrt{\ln c/\max\left(\rho_{T},\sqrt{1/T}\right)}\cdot\sqrt{T\log\left| \Pi^{*}\right|}\)} & \multirow{2}{*}{\(O\left(\left(\sqrt{\frac{T\log|T|-1}{\max\left(\rho_{T},\sqrt{1/T} \right)}+\hat{L}_{T,*}\right)\left(\ln T\right)}\right)\)} \\ \hline \hline \end{tabular}
\end{table}
Table 3: Regret and query complexity bounds. For the notations in this table: \(i^{*}\) is the model with the highest expected accuracy; \(\theta_{j}=\mathbb{P}\left[\ell_{\cdot,j}\neq\ell_{\cdot,i^{*}}\right]\) is the probability that exactly one of \(j\) and \(i^{*}\) correctly classifies a sample; \(\gamma\) and \(\rho_{T}\) are defined in Eq. (5) and (6), respectively. \(b=p_{\min}\log_{c}\left(1/p_{\min}\right)\), where \(p_{\min}=\min_{s,i}\pi(\mathbf{x}_{s})\) denotes the minimal model selection probability by any policy.

Importance Weighted Active Learning (IWAL)We have implemented [8] as the IWAL baseline. For the query strategy part, IWAL computes an adaptive rejection threshold for each instance and assigns an importance weight to each classifier in the hypothesis space \(\mathcal{H}_{t}\). IWAL retains the classifiers in the hypothesis space according to their weighted error versus the current best classifier's weighted error at round \(t\). The query probability is calculated based on labeling disagreements of surviving classifiers through function \(\max_{i,j\in\mathcal{H}_{t},\,y\in[c]}\ell_{t,i}^{(y)}-\ell_{t,j}^{(y)}\). For model recommendation, we also adopt the Follow-the-Leader (FTL) strategy.

Random Query Strategy (RS)The RS method queries the label of incoming instances by the coin-flip fixed probability \(\frac{b}{T}\). It also uses the FTL strategy based on queried instances for model recommendation.

Contextual Query by Committee (CQBC)We have created a contextual variant of QBC termed CQBC, which has the same entropy query strategy as the original QBC. For model recommendation, we combine two model selection strategies. The first strategy calculates the cumulative reward of each classifier based on past queries and normalizes it as a probability simplex vector. We also adopt Expt4's arm recommending vector to use contextual information. Finally, we compute the element-wise product of the two vectors and normalize it to be CQBC's model recommendation vector. At each round, CQBC would recommend the top model based on the classifiers' historical performance on queried instances and the online advice matrix for streaming data.

Contextual Importance Weighted Active Learning (CIWAL)We have created a variant version of importance-weighted active learning. Similar to CQBC, CIWAL adopts the query strategy from IWAL and converts the model selection strategy to be contextual. For model selection, we incorporate Exp4's arm recommendation strategy based on the side-information advice matrix and each classifier's historical performance according to queried instances. We compute the element-wise product of the two vectors as the model selection vector of CIWAL and normalize it as a weighted vector. Finally, CIWAL recommends the classifier with the highest weight.

 Oracle:Among all the given policies, oracle represents the best single policy that achieves the minimum cumulative loss, and it has the same query strategy as CAMS.

### Details on policies and classifiers

We constructed different expert-model configurations to reflect the cases in real-world applications6. This section lists the collection of policies and models used in our experiments.

Footnote 6: To list a few other scenarios beyond the ones used in the paper: In _healthcare_, models could be the treatments, experts could be the doctors and the context could be the condition of a patient. For any patient (context), doctors (experts) will have their own advice on the treatment (model) recommendation for this patient based on their past experience. In the _finance_ domain, models could be trading strategies, experts could be portfolio managers, and the context could be the stock/equity. Some trading strategies (models) might work well for the information technology sector, and some other models might work well for the energy sector, so depending on the sector of stock (context), different portfolio managers (experts) might have their own advice on different trading strategies (models) based their past trading experience.

Cifar10:We have constructed 80 diversified classifiers based on VGG [67], ResNet [34], DenseNet [38], GoogLeNet [69]. We have also used EfficientNet [70], MobileNets [37], RegNet [62], and ResNet to construct 85 diversified policies.

Drift:We have constructed ten classifiers using Decision Tree [55], SVM [18], AdaBoost [28], Logistic Regression [20], KNN [19] models. We have also created 8 diversified policies with multilayer perceptron (MLP) models of different layer configurations: (128, 30, 10); (128, 60, 30, 10); (128, 120, 30, 10); (128, 240, 120, 30, 10).

Vertebral:We have built six classifiers using Random Forest [10], Gaussian Process [56], linear discriminant analysis [26], Naive Bayes [31] algorithms. We have constructed policies by using standard scikit-learn built-in models including Random Forest Classifier, Extra Trees Classifier [30], Decision Tree Classifier, Radius Neighbors Classifier [51], Ridge Classifier [57] and K-Nearest-Neighbor classifiers.

HIV:We have used graph convolutional networks (GCN) [40], Graph Attention Networks (GAT) [72], AttentiveFP [75], and Random Forest to construct 4 classifiers. We have also used various feature representations of molecules such as MACCS key [25], ECFP2, ECFP4, and ECFP6 [58] molecular fingerprints to build 6 MLP-based policies, respectively.

CovType:We have built 6 classifiers using Random Forest, Gaussian Process, linear discriminant analysis, Naive Bayes algorithms. We have constructed 17 policies by using standard scikit-learn built-in models including Random Forest Classifier, Extra Trees Classifier, Decision Tree Classifier, Radius Neighbors Classifier, Ridge Classifier and K-Nearest-Neighbor classifiers.

### Implementation details

We build our evaluation pipeline on top of prior work [39] around the four benchmark datasets. Specifically,

* _Context_\(\bm{x}_{t}\) is the raw context of the data (e.g., the 32x32 image for CIFAR10).
* _Predictions_\(\hat{\bm{y}}_{t}\) contain the predicted label vector of all the classifiers' predictions according to the online context \(x_{t}\).
* _Oracle_ contains the true label \(y_{t}\) of \(\bm{x}_{t}\).
* _Advice matrix_ contains all policies' probability distribution \(\lambda\) over all the classifiers on context \(x_{t}\).

To adapt to an online setting, we sequentially draw random \(T\) i.i.d. instances \(\bm{x}_{1:T}\) from the test pool and define it as a realization. For a fair comparison, all algorithms receive data instances in the same order within the same realization.

```
1:Input: context \(\bm{x}_{t}\), Models \(\mathcal{F}\), policy \(\pi\in\Pi^{*}\)
2:\(\eta=\sum_{j=1}^{|\mathcal{F}|}\left([\pi\left(x_{t}\right)]_{j}-\frac{1}{| \mathcal{F}|}\right)^{2}\)
3:return\(\frac{\pi_{i}\left(x_{t}\right)+\eta}{1+|\mathcal{F}|\cdot\eta}\) ```

**Algorithm 1** Regularized policy \(\overline{\pi}\left(\bm{x}_{t}\right)\)

### Regularized policy

As discussed in adversarial section, we wish to ensure that the probability a policy selecting any model is bounded away from 0 so that the regret bound in Theorem 3 is non vacuous. In our experiments, we achieve this goal by applying a regularized policy \(\overline{\pi}\) as shown in Algorithm 1.

### Summary of datasets and models

We summarize the attributes of datasets, the models, and the model selection policies as follows.

### Hyperparameters

We performed our experiments on a Linux server with 80 Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz and total 528 Gigabyte memory.

By considering the resource of server, We set 100 realizations and 3000 stream-size for DRIFT, 20 realizations and 10000 stream-size for CIFAR10, 200 realizations and 4000 stream size for HIV,

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline
**dataset** & **classification** & **total instances** & **test set** & **stream size** & **classifier** & **policy** \\ \hline CIFAR10 & 10 & 60000 & 10000 & 10000 & 80 & 85 \\ DRIFT & 6 & 13910 & 3060 & 3000 & 10 & 11 \\ VERTEBRAL & 3 & 310 & 127 & 80 & 6 & 17 \\ HIV & 2 & 40000 & 4113 & 4000 & 4 & 20 \\ CovType & 55 & 580000 & 100000 & 100000 & 6 & 17 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Attributes of benchmark datasets300 realization and 80 stream-size for VERTEBRAL. In each realization, we randomly selected stream-size aligned data from testing-set and make it as online streaming data which is the input of each algorithm. Thus, we got independent result for each realization.

A small realization number would increase the variance of the results due to the randomness of stream order. A large realization number would make the result be more stable but at the cost of increasing computational cost (time, memory, etc.). We chose the realization number by balancing both aspects.

## Appendix E Proofs for the Stochastic Setting

In this section, we focus on the stochastic setting. We first prove the regret bound presented in Theorem 1 and then prove the query complexity presented in Theorem 2 for Algorithm 1.

### Proof of Theorem 1

Before providing the proof of Theorem 1, we first introduce the following lemma.

**Lemma 8**.: _Fix \(\tau\in(0,1)\). Let \(q_{t,i^{*}}\) be the probability of the optimal policy \(i^{*}\) maintained by Algorithm 1 at \(t\), and let \(b=p_{\min}\log_{\mathcal{C}}\left(1/p_{\min}\right)\), where \(p_{\min}=\min_{s,i}\pi(\mathbf{x}_{s})\) denotes the minimal model selection probability by any policy7. When \(t\geq\left(\frac{\ln\frac{(\lvert\Pi^{*}\rvert-1)\tau}{1-\tau}}{\sqrt{\ln \lvert\Pi^{*}\rvert}\left(\Delta-\sqrt{\frac{\lvert\Pi^{*}\rvert}{t}\ln\frac{ 2}{t}}\right)}\right)^{2}\), with probability at least \(1-\delta\), it holds that \(q_{t,i^{*}}\geq\tau\)._

Footnote 7: We assume \(p_{\min}>0\) per the policy regularization criterion in Appendix C.3. (cf. Algorithm 1 on “Regularized policy \(\bar{\pi}(\mathbf{x}_{t})\))”.

Proof of Lemma 8.: W.l.o.g, we assume \(\mu_{1}\leq\mu_{2}\leq\ldots\mu_{n+k}\). Recall that we define \(\Delta=\min_{i\neq i^{*}}\Delta_{i}=\mu_{2}-\mu_{1}=\frac{\mathbb{E}[\widetilde {L}_{t,2}-\widetilde{L}_{t,1}]}{t}\), and \(\pi_{1}\) is the policy with the minimal expected loss.

Define

\[\delta_{t}\triangleq\tilde{\ell}_{t-1,i^{\prime}}-\tilde{\ell}_{t-1,1}.\] (7)

where \(i^{\prime}\triangleq\arg\min_{i\neq 1}\tilde{L}_{t-1,i}\) denotes the index of the best empirical policy up to \(t-1\) other than \(\pi_{1}\). Therefore for \(i\geq 2\), it holds that

\[\widetilde{L}_{t-1,i^{\prime}}-\widetilde{L}_{t-1,i}=\sum_{s=1}^{t-1}\delta_{ s}\leq 0.\]

We have \(q_{t,i^{*}}=q_{t,1}=\frac{\exp\left(-\eta_{t}\widetilde{L}_{t-1,1}\right)}{ \sum_{i=1}^{\lvert\Pi^{*}\rvert}\exp\left(-\eta_{t}\widetilde{L}_{t-1,i}\right)}\) as the weight of optimal expert at round \(t\). Therefore

\[q_{t,i^{*}}=q_{t,1} =\frac{\exp\left(-\eta_{t}\widetilde{L}_{t-1,1}\right)}{\sum_{i =1}^{\lvert\Pi^{*}\rvert}\exp\left(-\eta_{t}\widetilde{L}_{t-1,i}\right)}\] \[\overset{(a)}{=}\frac{\exp\left(-\eta_{t}\widetilde{L}_{t-1,1}+ \eta_{t}\widetilde{L}_{t-1,i^{\prime}}\right)}{\sum_{i=1}^{\lvert\Pi^{*} \rvert}\exp\left(-\eta_{t}\widetilde{L}_{t-1,i}+\eta_{t}\widetilde{L}_{t-1,i^ {\prime}}\right)}\] \[\overset{(b)}{=}\frac{\exp\left(\eta_{t}\sum_{s=1}^{t}\delta_{s} \right)}{\exp\left(\eta_{t}\sum_{s=1}^{t}\delta_{s}\right)+\sum_{i=2}^{\lvert \Pi^{*}\rvert}\exp\left(-\eta_{t}\widetilde{L}_{t-1,i}+\eta_{t}\widetilde{L}_{t -1,i^{\prime}}\right)}\] \[\geq\frac{\exp\left(\eta_{t}\sum_{s=1}^{t}\delta_{s}\right)}{\exp \left(\eta_{t}\sum_{s=1}^{t}\delta_{s}\right)+\lvert\Pi^{*}\rvert-1}\]

where step \((a)\) is by dividing the cumulative loss of sub-optimal policy \(\pi_{i^{\prime}}\) and step (b) is by the definition of \(\delta_{t}\) in Equation (7).

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_FAIL:20]

where step (a) by setting \(\delta=\frac{1}{T}\), and where \(\gamma\) in Eq. (10) is the min gap. 

### Proof of Theorem 2

In this section, we analyze the query complexity of CAMS in the stochastic setting, where we take a similar approach as proposed by Karimi et al. [39] for the context-free model selection problem. Our main idea is to derive from query indicator \(U_{t}\) and query probability \(z_{t}\). We first used Lemma 10 to bound the expected number of queries \(\sum_{t=1}^{T}U_{t}\) by the sum of query probability as \(\sum_{t=1}^{T}\delta_{0}^{t}+\sum_{t=1}^{T}\mathfrak{E}\left(\hat{\bm{y}}_{t}, \bm{w}_{t}\right)\). Then we used Lemma 11 to bound the first item (which corresponds to the lower bound of query probability over \(T\) rounds) and applied Lemma 12 to bound the second term (which characterizes the model disagreement). Finally, we combined the upper bounds on the two parts to reach the desired result.

**Lemma 10**.: _The query complexity of Algorithm 1 is upper bounded by_

\[\mathbb{E}\left[\sum_{t=1}^{T}\left(\frac{1}{\sqrt{t}}+\frac{\sum_{y\in \mathcal{Y}}\langle\mathbf{w}_{t},\bm{\ell}_{t}^{y}\rangle\log_{|\mathcal{Y} |}\frac{1}{\langle\mathbf{w}_{t},\bm{\ell}_{t}^{y}\rangle}}{|\mathcal{Y}|} \right)\right].\] (12)

Proof.: Now we have model disagreement defined in Eq. (3), the query probability defined in Eq. (4), and the query indicator \(U\). Let us assume, at each round, we have query probability \(z_{t}>0\), which indicates we will not process the instance that all the models' prediction are the same.

At round \(t\), from query probability Eq. (4), we have

\[z_{t} =\max\left\{\delta_{0}^{t},\mathfrak{E}\left(\hat{\bm{y}}_{t}, \bm{w}_{t}\right)\right\}\] \[\leq \delta_{0}^{t}+\mathfrak{E}\left(\hat{\bm{y}}_{t},\bm{w}_{t} \right),\]

where the inequality is by applying that \(\forall A,B\geq 0,\max\{A,B\}\leq A+B\).

Thus, in total round \(T\), we could get the following equation as the cumulative query cost,

\[\mathbb{E}\left[\sum_{t=1}^{T}U_{t}\right]\leq \mathbb{E}\left[\sum_{t=1}^{T}\left(\frac{1}{\sqrt{t}}+\frac{\sum_ {y\in\mathcal{Y}}\langle\bm{w}_{t},\bm{\ell}_{t}^{y}\rangle\log_{|\mathcal{Y} |}\frac{1}{\langle\bm{w}_{t},\bm{\ell}_{t}^{y}\rangle}}{|\mathcal{Y}|}\right) \right],\] (13)

where the inequality is by inputting \(\delta_{0}^{t}=\frac{1}{\sqrt{t}}\) and Eq. (3). 

**Lemma 11**.: \(\sum_{t=1}^{T}\frac{1}{\sqrt{t}}\leq 2\sqrt{T}\)_._

Proof.: We can bound the LHS as follows:

\[\sum_{t=1}^{T}\frac{1}{\sqrt{t}} =\sum_{t=1}^{\lfloor\sqrt{T}\rfloor}\frac{1}{\sqrt{t}}+\sum_{t= \lfloor\sqrt{T}\rfloor+1}^{T}\frac{1}{\sqrt{t}}\] \[\leq\sqrt{T}+\sum_{t=\lfloor\sqrt{T}\rfloor+1}^{T}\frac{1}{\sqrt{ T}}\] \[=\sqrt{T}+\left(T-\sqrt{T}\right)\frac{1}{\sqrt{T}}\] \[\leq 2\sqrt{T}.\]

**Lemma 12**.: _Denote the true label at round \(t\) by \(y_{t}\), and define \(p_{t,y}:=\sum_{j\in[k]}\mathbb{I}\left\{\hat{y}_{t,j}=y\right\}w_{j}\). Further define \(R_{t}:=\sum_{t}1-p_{t,y_{t}}\) as the expected cumulative loss of Algorithm 1 at \(t\). Then_

\[\sum_{t=1}^{T}\frac{\sum_{y\in\mathcal{Y}}\langle\mathbf{w}_{t},\bm{\ell}_{t}^ {y}\rangle\log_{|\mathcal{Y}|}\frac{1}{\langle\mathbf{w}_{t},\bm{\ell}_{t}^{ y}\rangle}}{|\mathcal{Y}|}\leq\frac{R_{T}\cdot\left(\log_{|\mathcal{Y}|} \frac{T^{2}(|\mathcal{Y}|-1)}{R_{T}^{2}}\right)}{|\mathcal{Y}|}.\]Proof of Lemma 12.: Suppose at round \(t\), the true label is \(y_{t}\). \(\sum_{y\neq y_{t}}p_{t,y}=1-p_{t,y_{t}}=1-\left\langle\sum_{i\in|\Pi^{*}|}q_{t,i} \pi_{i}(\bm{x}_{t}),\bm{\ell}_{t}\right\rangle=r_{t}\),

\[\frac{\sum_{y\in\mathcal{Y}}\langle\bm{w}_{t},\bm{\ell}_{t}^{y} \rangle\log_{|\mathcal{Y}|}\frac{1}{\langle\bm{w}_{t},\bm{\ell}_{t}^{y}\rangle }}{|\mathcal{Y}|} =\frac{(1-p_{t,y_{t}})\log_{|\mathcal{Y}|}\frac{1}{1-p_{t,y_{t}}} }{|\mathcal{Y}|}+\frac{\sum_{y\neq y_{t}}\left(1-p_{t,y}\right)\log_{|\mathcal{ Y}|}\frac{1}{1-p_{t,y}}}{|\mathcal{Y}|}\] \[\overset{(a)}{\leq}\frac{(1-p_{t,y_{t}})\log_{|\mathcal{Y}|}\frac {1}{1-p_{t,y_{t}}}}{|\mathcal{Y}|}+(|\mathcal{Y}|-1)\,\frac{\frac{(1-p_{t,y_{ t}})}{|\mathcal{Y}|-1}\log_{|\mathcal{Y}|}\frac{|\mathcal{Y}|-1}{1-p_{t,y_{t}}}}{| \mathcal{Y}|}\] \[\leq\frac{(1-p_{t,y_{t}})\log_{|\mathcal{Y}|}\frac{1}{1-p_{t,y_{ t}}}}{|\mathcal{Y}|}+\frac{(1-p_{t,y_{t}})\log_{|\mathcal{Y}|}\frac{|\mathcal{Y}|-1} {1-p_{t,y_{t}}}}{|\mathcal{Y}|}\] \[=\frac{(1-p_{t,y_{t}})\log_{|\mathcal{Y}|}\frac{|\mathcal{Y}|-1} {\left(1-p_{t,y_{t}}\right)^{2}}}{|\mathcal{Y}|}\] \[\overset{(b)}{\leq}\frac{r_{t}\log_{|\mathcal{Y}|}\frac{| \mathcal{Y}|-1}{r_{t}^{2}}}{|\mathcal{Y}|},\]

where step \((a)\) is by applying Jensen's inequality and using \(1-p_{t,y}=\frac{1-p_{t,y_{t}}}{|\mathcal{Y}|-1}\), and step \((b)\) is by replacing the expected loss \(1-p_{t,y_{t}}\) by its short-hand notation \(r_{t}\).

Recall that we define the expected cumulative loss as \(R_{T}=\sum_{t=1}^{T}r_{t}\). Since when \(r_{t}\in[0,1]\), \(\frac{r_{t}\log_{|\mathcal{Y}|}\frac{|\mathcal{Y}|-1}{r_{t}^{2}}}{|\mathcal{Y} |}\) is concave, we get

\[\sum_{t=1}^{T}\frac{\sum_{y\in\mathcal{Y}}\langle\bm{w}_{t},\bm{\ell}_{t}^{y} \rangle\log_{|\mathcal{Y}|}\frac{1}{\langle\bm{w}_{t},\bm{\ell}_{t}^{y}\rangle }}{|\mathcal{Y}|}\leq\frac{T\left(\frac{\sum_{r_{t}}r_{t}}{T}\right)\left( \log_{|\mathcal{Y}|}\frac{|\mathcal{Y}|-1}{\frac{r_{t}^{2}}{T}\sum_{r}r_{t}} \right)}{|\mathcal{Y}|}=\frac{R_{T}\left(\log_{|\mathcal{Y}|}\frac{T^{2}(| \mathcal{Y}|-1)}{R_{T}^{2}}\right)}{|\mathcal{Y}|}.\] (14)

Since \(R_{T}\) is the cumulative loss up to round \(T\), \(T\)'s incremental rate is no less than \(R_{T}\)'s incremental rate. Thus, \(R_{T}\leq T\) and \(\frac{T_{t}}{R_{t}}\leq\frac{T_{t+1}}{R_{t+1}}\). So we get Eq. (14). 

Now we are ready to prove Theorem 2.

Proof of Theorem 2.: From Lemma 10, we get the following equation as the cumulative query cost

\[\mathbb{E}\left[\sum_{t=1}^{T}U_{t}\right]\leq\mathbb{E}\left[\sum_{t=1}^{T} \left(\frac{1}{\sqrt{t}}+\frac{\sum_{y\in\mathcal{Y}}\langle\bm{w}_{t},\bm{ \ell}_{t}^{y}\rangle\log_{|\mathcal{Y}|}\frac{1}{\langle\bm{w}_{t},\bm{\ell}_{ t}^{y}\rangle}}{|\mathcal{Y}|}\right)\right].\]

Let us assume the expected total loss of best policy is \(T\mu_{i^{*}}\). From Theorem 1, we get

\[\mathbb{E}\left[R_{T}\right]=\mathbb{E}\left[\sum_{t=1}^{T}r_{t}\right]\leq \left(\frac{\ln\frac{|\Pi^{*}|-1}{\gamma}+\sqrt{\ln|\Pi^{*}|\cdot 2b^{2}\ln\frac{2}{ \delta}}}{\sqrt{\ln|\Pi^{*}|\Delta}}\right)^{2}+T\mu_{i^{*}}.\]

Plugging this result into the query complexity bound given by Lemma 11 and Lemma 12, we have \[\mathbb{E}\left[\sum_{t=1}^{T}U_{t}\right] \leq 2\sqrt{T}+\frac{\left(\left(\frac{\ln\frac{|\Pi^{*}|-1}{\gamma} +\sqrt{\ln|\Pi^{*}|\cdot 2b^{2}\ln\frac{2}{\delta}}}{\sqrt{\ln|\Pi^{*}|\Delta}} \right)^{2}+T\mu_{i^{*}}\right)}{|\mathcal{Y}|}\log_{|\mathcal{Y}|}\frac{T^{2} \left(|\mathcal{Y}|-1\right)}{\left(\left(\frac{\ln\frac{|\Pi^{*}|-1}{\gamma} +\sqrt{\ln|\Pi^{*}|\cdot 2b^{2}\ln\frac{2}{\delta}}}{\sqrt{\ln|\Pi^{*}|\Delta}} \right)^{2}+T\mu_{i^{*}}\right)^{2}}\] \[\leq\frac{\left(\left(\frac{\ln\frac{|\Pi^{*}|-1}{\gamma}+\sqrt{ \ln|\Pi^{*}|\cdot 2b^{2}\ln\frac{2}{\delta}}}{\sqrt{\ln|\Pi^{*}|\Delta}} \right)^{2}+T\mu_{i^{*}}\right)\left(\log_{|\mathcal{Y}|}\left(T|\mathcal{Y} |\right)\right)}{|\mathcal{Y}|}\] \[=\frac{\left(\left(\frac{\ln\frac{|\Pi^{*}|-1}{\gamma}+\sqrt{\ln |\Pi^{*}|\cdot 2b^{2}\ln\frac{2}{\delta}}}{\sqrt{\ln|\Pi^{*}|\Delta}} \right)^{2}+T\mu_{i^{*}}\right)\ln\left(T\right)}{|\mathcal{Y}|\ln|\mathcal{Y }|}\] \[\stackrel{{(a)}}{{=}}\frac{\left(\left(\frac{\ln \frac{|\Pi^{*}|-1}{\gamma}+\sqrt{\ln|\Pi^{*}|\cdot 2b^{2}\ln\frac{2}{\delta}}}{ \sqrt{\ln|\Pi^{*}|\Delta}}\right)^{2}+T\mu_{i^{*}}\right)\ln\left(T\right)}{ c\ln c},\]

where \(\gamma\) is defined as Eq. (10) and step (a) by applying \(c=|\mathcal{Y}|\). 

## Appendix F Proofs for the Adversarial Setting

In this section, we first prove the regret bound presented in Theorem 3 and then prove the query complexity bound presented in Theorem 4 for Algorithm 1 in the adversarial setting. Lemma 13 builds upon the proof of the hedge algorithm [27], but with an _adaptive_ learning rate.

### Proof of Theorem 3

**Lemma 13**.: _Consider the setting of Algorithm 1, Let us define \(h_{t,i}=\exp\left(-\eta_{t}\tilde{L}_{t-1,i}\right)\,\forall i\in|\Pi^{*}|\) as exponential cumulative loss of policy \(i\), \(\eta_{t}\) is the adaptive learning rate and \(\mathbf{q}_{t}\) is the probability distribution of policies, then_

\[\log\frac{\sum_{i\in[|\Pi^{*}|]}h_{T+1,i}}{\sum_{i\in[|\Pi^{*}|]}h_{1,i}}\leq -\sum_{t=1}^{T}\eta_{t}\sum_{i=1}^{|\Pi^{*}|}q_{t,i}\widetilde{\ell}_{t,i}+ \sum_{t=1}^{T}\frac{\eta_{t}^{2}}{2}\sum_{i=1}^{|\Pi^{*}|}q_{t,i}\left( \widetilde{\ell}_{t,i}\right)^{2}.\]

Proof.: We first bound the following term

\[\frac{\sum_{i\in[|\Pi^{*}|]}h_{t+1,i}}{\sum_{i\in[|\Pi^{*}|]}h_{ t,i}} =\sum_{i=1}^{|\Pi^{*}|}\frac{h_{t+1,i}}{\sum_{i\in[|\Pi^{*}|]}h_{ t,i}}\] \[=\sum_{i=1}^{|\Pi^{*}|}q_{t,i}\exp\left(-\eta_{t}\widetilde{\ell }_{t,i}\right)\] \[\leq \sum_{i=1}^{|\Pi^{*}|}q_{t,i}\left(1-\eta_{t}\widetilde{\ell}_{t,i}+\frac{\eta_{t}^{2}\left(\widetilde{\ell}_{t,i}\right)^{2}}{2}\right)\] \[=1-\eta_{t}\sum_{i=1}^{|\Pi^{*}|}q_{t,i}\widetilde{\ell}_{t,i}+ \frac{\eta_{t}^{2}}{2}\sum_{i=1}^{|\Pi^{*}|}q_{t,i}\left(\widetilde{\ell}_{t,i }\right)^{2},\]

where the inequality is by applying that for \(x\leq 0\), we have \(e^{x}\leq 1+x+\frac{x^{2}}{2}\).

By taking \(\log\) on both side, we get

\[\log\frac{\sum_{i\in[|\Pi^{*}|]}h_{t+1,i}}{\sum_{i\in[|\Pi^{*}|]}h_{t,i}} \leq\log\left(1-\eta_{t}\sum_{i=1}^{|\Pi^{*}|}q_{t,i}\widetilde{ \ell}_{t,i}+\frac{\eta_{t}^{2}}{2}\sum_{i=1}^{|\Pi^{*}|}q_{t,i}\left(\widetilde {\ell}_{t,i}\right)^{2}\right)\] \[\stackrel{{(a)}}{{\leq}}-\eta_{t}\sum_{i=1}^{|\Pi^ {*}|}q_{t,i}\widetilde{\ell}_{t,i}+\frac{\eta_{t}^{2}}{2}\sum_{i=1}^{|\Pi^{*}| }q_{t,i}\left(\widetilde{\ell}_{t,i}\right)^{2},\]

where step \((a)\) is by applying that \(\log\left(1+x\right)\leq x\), when \(x\geq-1\).

Now summing over \(t=1:T\) yields:

\[\log\frac{\sum_{i\in[|\Pi^{*}|]}h_{T+1,i}}{\sum_{i\in[|\Pi^{*}|] }h_{1,i}} =\sum_{t=1}^{T}\log\frac{\sum_{i\in[|\Pi^{*}|]}h_{t+1,i}}{\sum_{i \in[|\Pi^{*}|]}h_{t,i}}\] \[\leq-\sum_{t=1}^{T}\eta_{t}\sum_{i=1}^{|\Pi^{*}|}q_{t,i} \widetilde{\ell}_{t,i}+\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{2}\sum_{i=1}^{|\Pi^{* }|}q_{t,i}\left(\widetilde{\ell}_{t,i}\right)^{2}.\]

**Lemma 14**.: _Consider the setting of Algorithm 1. Let \(p_{t,y}=\sum_{j\in[k]}\mathbb{I}\left\{\hat{y}_{t,j}=y\right\}w_{j}\). The query probability \(z_{t}\) satisfies_

\[z_{t}\geq\frac{1}{|\mathcal{Y}|\ln|\mathcal{Y}|}\left(p_{t,y_{t}}\left(1-p_{t, y_{t}}\right)+p_{t,y}\left(1-p_{t,y}\right)\right),\forall y\neq y_{t}.\]

Proof.: We first bound the query probability term

\[z_{t} =\max\left\{\delta_{0}^{t},\mathfrak{E}(\hat{y}_{t},\textbf{w}_{t })\right\}\] \[=\max\{\delta_{0}^{t},\frac{1}{|\mathcal{Y}|}\sum_{y\in\mathcal{Y }}\langle\textbf{w}_{t},\boldsymbol{\ell}_{t}^{y}\rangle\log_{|\mathcal{Y}|} \frac{1}{\langle\textbf{w}_{t},\boldsymbol{\ell}_{t}^{y}\rangle}\}\] \[=\max\{\delta_{0}^{t},\frac{1}{|\mathcal{Y}|}\sum_{y\in\mathcal{Y }}\left(1-p_{t,y}\right)\cdot\ln\frac{1}{1-p_{t,y}}\frac{1}{\ln|\mathcal{Y}|}\}\] \[\stackrel{{(a)}}{{\geq}}\max\{\delta_{0}^{t},\frac{1} {|\mathcal{Y}|}\sum_{y\in\mathcal{Y}}\left(1-p_{t,y}\right)\cdot p_{t,y}\cdot \frac{1}{\ln|\mathcal{Y}|}\}\] \[=\max\{\delta_{0}^{t},\frac{1}{|\mathcal{Y}|\ln|\mathcal{Y}|}\sum _{y\in\mathcal{Y}}\left(1-p_{t,y}\right)\cdot p_{t,y}\}\] \[\stackrel{{(b)}}{{\geq}}\frac{1}{|\mathcal{Y}|\ln| \mathcal{Y}|}\left(p_{t,y_{t}}\left(1-p_{t,y_{n}}\right)+p_{t,y}\left(1-p_{t, y}\right)\right),\forall y\neq y_{t},\]

where step \((a)\) is by applying \(\ln\left(1+x\right)\geq\frac{x}{1+x}\) for \(x>-1\),

\[\ln\frac{1}{1-p_{t,y}}=\ln\left(1+\frac{p_{t,y}}{1-p_{t,y}}\right)\geq\frac{ \frac{p_{t,y}}{1-p_{t,y}}}{1-p_{t,y}}=p_{t,y},\]

and where step \((b)\) is by applying \(\forall a,b\in\mathbb{R},\max\left\{a,b\right\}\geq a\).

Proof of Theorem 3.: By applying Lemma 13, we got

\[\log\frac{\sum_{i\in[|\Pi^{*}|]}h_{T+1,i}}{\sum_{i\in[|\Pi^{*}|]}h_{1,i}}\leq- \sum_{t=1}^{T}\eta_{t}\sum_{i=1}^{|\Pi^{*}|}q_{t,i}\widetilde{\ell}_{t,i}+ \sum_{t=1}^{T}\frac{\eta_{t}^{2}}{2}\sum_{i=1}^{|\Pi^{*}|}q_{t,i}\left( \widetilde{\ell}_{t,i}\right)^{2}.\]For any policy \(s\), we have a lower bound

\[\log\frac{\sum_{i\in[\left|\Pi^{*}\right|]}h_{T+1,i}}{\sum_{i\in[ \left|\Pi^{*}\right|]}h_{1,i}} \geq\log\frac{h_{T+1,s}}{\sum_{i\in[\left|\Pi^{*}\right|]}h_{1,i}}\] \[\overset{(a)}{=}\log\frac{h_{T+1,s}}{\left|\Pi^{*}\right|}\] \[=-\log{(n+k)}{-}\eta_{T}\sum_{t=1}^{T}\widetilde{\ell}_{t,s},\] (15)

where step \((a)\) in Eq. (15) is by initializing \(\widetilde{\bm{L}}_{0}=0\), \(e^{0}=1\), and \(\sum_{i\in[\left|\Pi^{*}\right|]}\bm{h}_{1}=e^{(-\eta_{t}\widetilde{\bm{L}}_{ 0})}=\left|\Pi^{*}\right|\).

Thus, we have

\[-\sum_{t=1}^{T}\eta_{t}\sum_{i=1}^{\left|\Pi^{*}\right|}q_{t,i} \widetilde{\ell}_{t,i}+\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{2}\sum_{i=1}^{\left| \Pi^{*}\right|}q_{t,i}\left(\widetilde{\ell}_{t,i}\right)^{2} \geq-\log{(n+k)}{-}\eta_{T}\sum_{t=1}^{T}\widetilde{\ell}_{t,s}\] \[\sum_{t=1}^{T}\eta_{t}\sum_{i=1}^{\left|\Pi^{*}\right|}q_{t,i} \widetilde{\ell}_{t,i}{-}\eta_{T}\sum_{t=1}^{T}\widetilde{\ell}_{t,s} \leq\log{(n+k)}+\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{2}\sum_{i=1}^{ \left|\Pi^{*}\right|}q_{t,i}\left(\widetilde{\ell}_{t,i}\right)^{2}\] \[\eta_{T}\sum_{t=1}^{T}\sum_{i=1}^{\left|\Pi^{*}\right|}q_{t,i} \widetilde{\ell}_{t,i}{-}\eta_{T}\sum_{t=1}^{T}\widetilde{\ell}_{t,s} \overset{(b)}{\leq}\log{(n+k)}+\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{2}\sum_{i=1 }^{\left|\Pi^{*}\right|}q_{t,i}\left(\widetilde{\ell}_{t,i}\right)^{2}\] \[\sum_{t=1}^{T}\sum_{i=1}^{\left|\Pi^{*}\right|}q_{t,i} \widetilde{\ell}_{t,i}-\sum_{t=1}^{T}\widetilde{\ell}_{t,s} \overset{(c)}{\leq}\frac{\log{\left|\Pi^{*}\right|}}{\eta_{T}}+\frac{1}{ \eta_{T}}\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{2}\sum_{i=1}^{\left|\Pi^{*}\right|} q_{t,i}\left(\widetilde{\ell}_{t,i}\right)^{2},\]

where step \((b)\) is by applying

\[\eta_{T}\sum_{t=1}^{T}\sum_{i=1}^{\left|\Pi^{*}\right|}q_{t,i} \widetilde{\ell}_{t,i}{-}\eta_{T}\sum_{t=1}^{T}\widetilde{\ell}_{t,s}\leq\sum _{t=1}^{T}\eta_{t}\sum_{i=1}^{\left|\Pi^{*}\right|}q_{t,i}\widetilde{\ell}_{t, i}{-}\eta_{T}\sum_{t=1}^{T}\widetilde{\ell}_{t,s},\]

and step \((c)\) is by dividing \(\eta_{T}\) on both side.

Because we have

\[\mathbb{E}_{T}\bigg{[}q_{t,i}\left(\widetilde{\ell}_{t,i}\right)^ {2}\bigg{]} =q_{t,i}\mathbb{E}_{T}\bigg{[}\left(\pi_{i}\left(\bm{x}_{t}\right) \cdot\widehat{\bm{\ell}}_{t}\right)^{2}\bigg{]}\] \[=q_{t,i}\left(P\left(U_{t}=1\right)\left(\pi_{i}\left(\bm{x}_{t} \right)\cdot\frac{\bm{\ell}_{t}}{z_{t}}\right)^{2}+P\left(U_{t}=0\right)\cdot 0\right)\] \[=q_{t,i}\left(z_{t}\left(\pi_{i}\left(\bm{x}_{t}\right)\cdot\frac{ \bm{\ell}_{t}}{z_{t}}\right)^{2}\right)\] \[=\frac{q_{t,i}}{z_{t}}\left(\pi_{i}\left(\bm{x}_{t}\right)\cdot \bm{\ell}_{t}\right)^{2}\] \[\leq\frac{q_{t,i}}{z_{t}}\pi_{i}\left(\bm{x}_{t}\right)\cdot\bm{ \ell}_{t}\] \[=\frac{q_{t,i}}{z_{t}}\langle\pi_{i}\left(\bm{x}_{t}\right),\bm{ \ell}_{t}\rangle,\]

it leads to \[\sum_{t=1}^{T}\sum_{i=1}^{|\Pi^{*}|}q_{t,i}\widetilde{\ell}_{t,i}- \sum_{t=1}^{T}\widetilde{\ell}_{t,s} \leq\frac{\log|\Pi^{*}|}{\eta_{T}}+\frac{1}{\eta_{T}}\sum_{t=1}^{ T}\frac{\eta_{t}^{2}}{2}\sum_{i=1}^{|\Pi^{*}|}\frac{q_{t,i}}{z_{t}}\langle\pi_{i} \left(\boldsymbol{x}_{t}\right),\boldsymbol{\ell}_{t}\rangle\] \[\overset{(d)}{\leq}\frac{\log|\Pi^{*}|}{\eta_{T}}+\frac{1}{\eta _{T}}\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{2}\frac{\langle\boldsymbol{w}_{t}, \boldsymbol{\ell}_{t}\rangle}{z_{t}},\]

where step \((d)\) is by applying \(\sum_{i=1}^{|\Pi^{*}|}q_{t,i}\langle\pi_{i}\left(\boldsymbol{x}_{t}\right), \boldsymbol{\ell}_{t}\rangle=\langle\boldsymbol{w}_{t},\boldsymbol{\ell}_{t}\rangle\).

So we have,

\[\sum_{t=1}^{T}\sum_{i=1}^{|\Pi^{*}|}q_{t,i}\widetilde{\ell}_{t,i} -\sum_{t=1}^{T}\widetilde{\ell}_{t,s} \leq\frac{\log|\Pi^{*}|}{\eta_{T}}+\frac{1}{\eta_{T}}\sum_{t=1}^{ T}\frac{\eta_{t}^{2}}{2}\frac{\langle\boldsymbol{w}_{t},\boldsymbol{\ell}_{t} \rangle}{z_{t}}\] \[\overset{(e)}{\leq}\frac{\log|\Pi^{*}|}{\eta_{T}}+\frac{1}{\eta _{T}}\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{2}\frac{1-p_{t,y_{t}}}{z_{t}}\] \[\overset{(f)}{\leq}\frac{\log|\Pi^{*}|}{\eta_{T}}+\frac{1}{\eta _{T}}\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{2}\frac{1-p_{t,y_{t}}}{\mathcal{Y}_{0} \left(\left(1-p_{t,y_{t}}\right)p_{t,y_{t}}+\left(1-p_{t,y}\right)p_{t,y} \right)}\] \[\leq\frac{\log|\Pi^{*}|}{\eta_{T}}+\frac{1}{\eta_{T}}\sum_{t=1}^{ T}\frac{\eta_{t}^{2}}{2}\frac{1}{\mathcal{Y}_{0}\left(p_{t,y_{t}}+\frac{1-p_{t,y}}{ 1-p_{t,y_{t}}}p_{t,y}\right)},\]

where step \((e)\) is by using \(\langle\boldsymbol{w}_{t},\boldsymbol{\ell}_{t}\rangle=1-p_{t,y_{t}}\) and step \((f)\) by using Lemma 14 and get lower bound of \(z_{t}\) as \(\frac{1}{|\mathcal{Y}|\ln|\mathcal{Y}|}\left(p_{t,y_{t}}\left(1-p_{t,y_{t}} \right)+p_{t,y}\left(1-p_{t,y}\right)\right)\) and applying \(\frac{1}{|\mathcal{Y}|\ln|\mathcal{Y}|}=\mathcal{Y}_{0}\).

If \(p_{t,y_{t}}\geq\frac{1}{|\mathcal{Y}|}\),

\[p_{t,y_{t}}+\frac{1-p_{t,y}}{1-p_{t,y_{t}}}p_{t,y}\geq\frac{1}{|\mathcal{Y}|}.\]

If \(p_{t,y_{t}}<\frac{1}{|\mathcal{Y}|}\), \(\exists y,p_{t,y}\to 1\), \(\delta_{1}^{t}=1-\max_{y,\tau\in[t]}p_{\tau,y}\). Let \(p_{t,\hat{y}}=\max_{y}p_{t,y}\). Thus, we have \(w_{\hat{y}}>\frac{1}{|\mathcal{Y}|}\) and

\[p_{t,y_{t}}+\frac{1-p_{t,y}}{1-p_{t,y_{t}}}p_{t,y}\geq p_{t,y_{t}}+w_{\hat{y}} \frac{\delta_{1}^{t}}{1-p_{t,y_{t}}}\geq 0+\frac{1}{|\mathcal{Y}|}\frac{ \delta_{1}^{t}}{1}=\frac{\delta_{1}^{t}}{|\mathcal{Y}|}.\]

Therefore

\[\max\{p_{t,y_{t}}+p_{t,y}\frac{1-p_{t,y}}{1-p_{t,y_{t}}}\}=\left\{\begin{array} []{ll}\frac{1}{|\mathcal{Y}|}&\mbox{if }p_{t,y_{t}}\geq\frac{1}{|\mathcal{Y}|},\\ \frac{\delta_{1}^{t}}{|\mathcal{Y}|}&\mbox{if }p_{t,y_{t}}<\frac{1}{|\mathcal{Y}|}. \end{array}\right.\]So we have

\[\sum_{t=1}^{T}\sum_{i=1}^{|\Pi^{*}|}q_{t,i}\widetilde{\ell}_{t,i}- \sum_{t=1}^{T}\widetilde{\ell}_{t,s} \leq\frac{\log|\Pi^{*}|}{\eta_{T}}+\frac{1}{\eta_{T}}\sum_{t=1}^{ T}\frac{\eta_{t}^{2}}{2}\frac{1}{\mathcal{Y}_{0}\left(p_{t,y_{t}}+\frac{1- \bm{w}_{y}}{1-p_{t,y_{t}}}p_{t,y}\right)}\] \[\overset{(g)}{\leq}\frac{\log|\Pi^{*}|}{\eta_{T}}+\frac{1}{\eta_ {T}}\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{2}\frac{1}{\max\{\mathcal{Y}_{0}\frac{ \delta_{1}^{t}}{|\mathcal{Y}|},\delta_{0}^{t}\}}\] \[=\frac{\log|\Pi^{*}|}{\eta_{T}}+\frac{1}{\eta_{T}}\sum_{t=1}^{T} \frac{\eta_{t}^{2}}{2}\frac{|\mathcal{Y}|^{2}\ln|\mathcal{Y}|}{\max\{\delta_{ 1}^{t},\delta_{0}^{t}|\mathcal{Y}|^{2}\ln|\mathcal{Y}|\}}\] \[\overset{(h)}{\leq}\frac{\log|\Pi^{*}|}{\eta_{T}}+\frac{1}{\eta_ {T}}\sum_{t=1}^{T}\frac{\eta_{t}^{2}}{2}\cdot\frac{|\mathcal{Y}|^{2}\ln| \mathcal{Y}|}{\frac{\delta_{1}^{t}+\delta_{0}^{t}|\mathcal{Y}|^{2}\ln| \mathcal{Y}|}{2}}\] \[=\frac{\log|\Pi^{*}|}{\eta_{T}}+\frac{1}{\eta_{T}}\sum_{t=1}^{T} \eta_{t}^{2}\frac{1}{\delta_{1}^{t}+\delta_{0}^{t}|\mathcal{Y}|^{2}\ln| \mathcal{Y}|}\cdot|\mathcal{Y}|^{2}\ln|\mathcal{Y}|,\]

where step \((g)\) is by getting the lower bound of \(z_{t}\) as \(\frac{\delta_{1}^{t}}{|\mathcal{Y}|}\leq\frac{1}{|\mathcal{Y}|}\), \(\delta_{0}^{t}\leq\frac{\delta_{0}^{t}}{1-p_{t,y_{t}}}\) and step \((h)\) is by applying \(\max\{A,B\}\geq\frac{A+B}{2}\).

Let us define \(\rho_{t}\triangleq\min_{\tau\in[t]}\delta_{1}^{*}=1-\max_{c,\tau\in[t]}p_{t,y}^ {T}\). We get

\[\mathbb{E}_{T}[\mathcal{R}_{T}] \leq\frac{\log|\Pi^{*}|}{\eta_{T}}+\frac{1}{\eta_{T}}\sum_{t=1}^{ T}\log|\Pi^{*}|\cdot\frac{1}{T}\leq\frac{2\log|\Pi^{*}|}{\eta_{T}}\]

Let \(\eta_{t}=\sqrt{\frac{\rho_{t}+\delta_{0}^{t}|\mathcal{Y}|^{2}\ln|\mathcal{Y}| }{|\mathcal{Y}|^{2}\ln|\mathcal{Y}|}}\cdot\sqrt{\frac{\log|\Pi^{*}|}{T}}\), we obtain

\[\mathbb{E}_{T}[\mathcal{R}_{T}] \leq\frac{2\sqrt{\log|\Pi^{*}|}\cdot\sqrt{T}\cdot\sqrt{|\mathcal{ Y}|^{2}\ln|\mathcal{Y}|}}{\sqrt{\rho_{T}+\delta_{0}^{T}|\mathcal{Y}|^{2}\ln| \mathcal{Y}|}}\] \[\leq 2|\mathcal{Y}|\sqrt{\frac{T\ln|\mathcal{Y}|\log|\Pi^{*}|}{ \max\{\rho_{T},\sqrt{1/T}\}}}\]

where the last inequality is due to the fact that

\[\rho_{T}+\delta_{0}^{T}|\mathcal{Y}|^{2}\ln|\mathcal{Y}|>\max\{\rho_{T}, \delta_{0}^{T}\}=\max\{\rho_{T},\sqrt{1/T}\}\]

which completes the proof. 

### Proof of Theorem 4

Proof of Theorem 4.: From Lemma 10, we get the following equation as the cumulative query cost

\[\mathbb{E}\left[\sum_{t=1}^{T}U_{t}\right]\leq\mathbb{E}\left[\sum_{t=1}^{T} \left(\frac{1}{\sqrt{t}}+\frac{\sum_{y\in\mathcal{Y}}\langle\bm{w}_{t},\bm{ \ell}_{t}^{y}\rangle\log_{|\mathcal{Y}|}\frac{1}{\langle\bm{w}_{t},\bm{\ell}_{t }^{y}\rangle}}{|\mathcal{Y}|}\right)\right].\]

Let us assume the expected total loss of best policy is \(\tilde{L}_{T,*}\). Thus, from Theorem 3, we get the expected cumulative loss

\[\mathbb{E}\left[R_{T}\right]=\mathbb{E}\left[\sum_{t=1}^{T}r_{t}\right]\leq 2 |\mathcal{Y}|\sqrt{\frac{T\ln|\mathcal{Y}|\log|\Pi^{*}|}{\max\{\rho_{T},\sqrt{ 1/T}\}}}+\tilde{L}_{T,*}.\]Now plugging the regret bound \(\mathcal{R}_{T}\) proved in Theorem 3 into the query complexity bound given by Lemma 12, we have

\[\sum_{t=1}^{T}\frac{\sum_{y\in\mathcal{Y}}\langle\bm{w}_{t},\bm{ \ell}_{t}^{y}\rangle\log_{|\mathcal{Y}|}\frac{1}{\langle\bm{w}_{t},\bm{\ell}_{ t}^{y}\rangle}}{|\mathcal{Y}|} \leq\frac{\left(2|\mathcal{Y}|\sqrt{\frac{T\ln|\mathcal{Y}|\log|\Pi^{*}|}{ \max\{\rho_{T},\sqrt{1/T}\}}}+\tilde{L}_{T,*}\right)\left(\log_{|\mathcal{Y}|} \frac{T^{2}(|\mathcal{Y}|-1)}{\left(2|\mathcal{Y}|\sqrt{\frac{T\ln|\mathcal{Y} |\log|\Pi^{*}|}{\max\{\rho_{T},\sqrt{1/T}\}}}+\tilde{L}_{T,*}\right)^{2}}\right) }{|\mathcal{Y}|}\] \[\leq\frac{\left(2|\mathcal{Y}|\sqrt{\frac{T\ln|\mathcal{Y}|\log| \Pi^{*}|}{\max\{\rho_{T},\sqrt{1/T}\}}}+\tilde{L}_{T,*}\right)\left(\log_{| \mathcal{Y}|}T|\mathcal{Y}|\right)}{|\mathcal{Y}|}\] \[=\frac{\left(2|\mathcal{Y}|\sqrt{\frac{T\ln|\mathcal{Y}|\log| \Pi^{*}|}{\max\{\rho_{T},\sqrt{1/T}\}}}+\tilde{L}_{T,*}\right)\left(\log_{| \mathcal{Y}|}T+1\right)}{|\mathcal{Y}|}.\]

Finally, by applying query complexity upper bound of Lemma 11, we got

\[\mathbb{E}\left[\sum_{t=1}^{T}U_{t}\right]\leq 2\sqrt{T}+\frac{\left(2| \mathcal{Y}|\sqrt{\frac{T\ln|\mathcal{Y}|\log|\Pi^{*}|}{\max\{\rho_{T},\sqrt{1/ T}\}}}+\tilde{L}_{T,*}\right)\left(\log_{|\mathcal{Y}|}T+1\right)}{| \mathcal{Y}|}.\]

Since the second term on the RHS dominates the upper bound, we have

\[O\left(\mathbb{E}\left[\sum_{t=1}^{T}U_{t}\right]\right)=O\left(\frac{\left( \sqrt{\frac{T\log|\Pi^{*}|}{\max\{\rho_{T},\sqrt{1/T}\}}}+\tilde{L}_{T,*} \right)\left(\ln T\right)}{\sqrt{\ln\left(|\mathcal{Y}|\right)}}\right) \stackrel{{(a)}}{{=}}O\left(\left(\sqrt{\frac{T\log|\Pi^{*}|}{ \max\{\rho_{T},\sqrt{1/T}\}}}+\tilde{L}_{T,*}\right)\left(\ln T\right)\right),\]

where step (a) is obtained by suppressing constant coefficients involving \(|\mathcal{Y}|\) into the \(O\) notation.

Additional Experiments

In this section, we further evaluate CAMS and provide additional experimental results (complementary to the main results presented in Fig. 2) under the following scenarios:

1. In App. G.1, we demonstrate that CAMS outperforms the baselines on a large scale dataset as well.
2. In App. G.2, we perform ablation study of three query strategies CAMS (entropy), variance and random strategy. CAMS _(Entropy)_ achieves the minimum cumulative loss for CIFAR10, DRIFT, and VERTEBRAL under the same query cost and outperform the other query strategies.
3. In a _mixture of experts_ environment, CAMS converges to the best policy and outperforms all others (App. G.3);
4. In a _non-contextual_ (no experts) environment, CAMS has approximately equal performance as Model Picker to reach the best classifier effectively (App. G.4);
5. In an _adversarial_ environment, CAMS can efficiently recover from the adversary and approach the performance of the best classifier (App. G.5);
6. In a _complete sub-optimal expert_ environment, a variant of the CAMS algorithm, namely CAMS-MAX, which deterministically picks the most probable policy and selects the most probable model, outperforms CAMS-Random-Policy, which randomly samples a policy and selects the most probable model (App. G.7 & App. G.8). However, CAMS-MAX at most approaches the performance of the best policy. In contrast, perhaps surprisingly, CAMS is able to outperform the best policy on both VERTEBRAL and HIV (App. G.6).
7. In App. G.9, we summarize the maximum query cost under a fixed number of realizations with its associated cumulative loss for all baselines (exclude oracle) on all benchmarks in experiment section.
8. In App. G.10, we compare the query complexity of each baselines and demonstrate that CAMS has the lowest query cost increasing rate on CIFAR10, DRIFT and VERTEBRAL dataset.
9. In previous studies, we assume that the data comes in an online format. In App. G.11, we assume we know the data stream length ahead and applying the scaling parameter to each algorithm to query their top data points from hindsight. CAMS still outperforms all the baselines.
10. In App. G.12, we compare CAMS with CAMS-nonactive, a greedy version (query label for each incoming data point). Although CAMS query much less data, it still performs equally well or even better than the greedy version.
11. In App. G.13, we demonstrates that CAMS can achieve negative RCL on all benchmarks, which means it outperforms any algorithms that chase the best classifier where the horizontal 0 line represents the performance benchmark of best classifier.

### Performance of CAMS at scale: Experimental results on CovType

We scaled up our experiments on a larger dataset, CovType [24]. The CovType dataset offers details about different types of forest cover in the United States. It contains details including slope, aspect, elevation, measurements of the wilderness area, and the type of forest cover. CovType has 580K samples, of which 100K instances were chosen at random as online stream for testing. Fig. 4 demonstrated that CAMS outperforms all baselines which is consistent with the existing results in experiment section.

### Query strategies ablation comparison

Using the same CAMS model recommendation section, we compare three query strategies: the adaptive model-disagreement-based query strategy in Line 10-14 of Fig. 1 (referred to as _entropy_ in the following), the variance-based query strategy from Model Picker [39] (referred to as _variance_), and a random query strategy. Fig. 5 shows that CAMS's adaptive query strategy has the sharpest converge rate on cumulative loss, which demonstrates the effectiveness of the queried labels. Moreover, _entropy_ achieves the minimum cumulative loss for CIFAR10, DRIFT, and VERTEBRAL under the same query cost. For the HIV dataset, there is no clear winner between _entropy_ and _variance_ since the mean of their performance lie within the error bar of each other for the most part.

### Comparing CAMS with each individual expert

We evaluate CAMS by comparing it with all the policies available in various benchmarks. The policies in each benchmark are summarized in App. D.2 and Table D.5. The empirical results in Fig. 6 demonstrate that CAMS could efficiently outperform all policies and converge to the performance of the best policy with only slight increase in query cost in all benchmarks. In particular, on the VERTEBRAL and HIV benchmarks, CAMS even outperforms the best policy.

### Comparing CAMS against Model Picker in a context-free environment

CAMS outperforms Model Picker in Fig. 2, by leveraging the context information for adaptive model selection. In a context-free environment, \(\Pi=\{\varnothing\}\), so \(\Pi^{*}:=\{\pi_{1}^{\text{const}},\dots,\pi_{k}^{\text{const}}\}\), where \(\pi_{j}^{\text{const}}(\cdot):=\bm{e}_{j}\) represents a policy that only recommends a fixed model. In this case, selecting the best policy to CAMS equals selecting the best single model. Fig. 7 demonstrates that the mean of CAMS and Model Picker lies in the shades of each other, which means CAMS has approximately the same performance as model picker considering the randomness on all benchmarks.

### Robustness against malicious experts in adversarial environments

When given only malicious and random advice policies, the conventional contextual online learning from experts advice framework will be trapped in the malicious or random advice. In contrast, CAMS could efficiently identify these policies and avoid taking advice from them. Meanwhile, it also successfully identifies the best classifier to learn to reach its best performance.

Figure 4: Comparing CAMS with 7 baselines on CovType in terms of relative cumulative loss, query complexity, and cost effectiveness. CAMS outperforms all baselines. **(Left)** Performance measured by relative cumulative loss (i.e. loss against the best classifier) under a fixed query cost \(B\) (where \(B=1000\)). **(Middle)** Number of queries and **(Right)** Performance of cumulative loss by increasing the query cost, for a fixed number of rounds \(T\) (where \(T=100,000\)) and maximal query cost \(B\) (where \(B=5000\) ). **Algorithms**: 4 contextual {Oracle, CQBC, CIWAL, CAMS} and 4 non-contextual baselines {RS, QBC, IWAL, MP} are included (see Section ). 90% confident interval are indicated in shades.

Figure 5: Ablation study of three query strategies (entropy, variance, random) for 4 diverse benchmarks based on the same model recommendation strategy. Under the same query cost constraint, CAMS’s entropy-based strategy exceeds the performance of the other two strategies on non-binary benchmarks in terms of query cost and cumulative lost. 90% confident intervals are indicated in shades.

The _novelty_ in CAMS that enables this robustness is that we add the constant policies \(\{\pi_{1}^{\text{const}},\dots,\pi_{k}^{\text{const}}\}\) into the policy set \(\Pi\) to form the new set as \(\Pi^{*}\). To illustrate the performance difference, we have created a variant of CAMS by adapting to the conventional approach (named CAMS-conventional). Fig. 8 demonstrates that CAMS could outperform all the malicious and random policies and converge to the performance of the best classifier. **CAMS-_conventional:_** We create the CAMS-conventional algorithm as the CAMS using policy set \(\Pi\), not \(\Pi^{*}\).

Figure 8: Evaluating the robustness of CAMS compared to the conventional learning from experts’ advice (CAMS-conventional) in a complete malicious and random policies environment. When no good policy is available, CAMS could recover from malicious advice and successfully approach the performance of the best classifier. In contrast, the conventional approach will be trapped in malicious advice. 90% confident intervals are indicated in shades.

Figure 6: Comparing CAMS with every single policy (only plotted top performance policies in Figure). CAMS could approach the best expert and exceed all others with limited queries. In particular, on VERTEBRAL and HIV Benchmarks, CAMS outperforms the best expert. 90% confident intervals are indicated in shades.

Figure 7: Comparing the model selection strategy of CAMS and Model Picker baseline based on the same variance-based query strategy in a context-free environment. CAMS has approximately the same performance as Model Picker on all the benchmarks. 90% confident intervals are indicated in shades.

### Outperformance over the best policy/expert

We also observe that CAMS does not stop at approaching the best policy or classifier performance. Sometimes, it even outperforms all the policies and classifiers, and Fig. 9 demonstrates such a case. To demonstrate the advantage of CAMS, we create two variant versions of CAMS: (1) CAMS-MAX (App. G.7), (2) CAMS-Random-Policy (App. G.8). CAMS-MAX and CAMS-Random-Policy use the same algorithm as CAMS in adversarial settings but have different model selection strategies for ablation study in the stochastic settings.

We evaluate the three algorithms on VERTEBRAL and HIV benchmarks in terms of (a) _normal policies_ (Fig. 9 Left), (b) _classifiers_ (Fig. 9 Middle), and (c) _malicious and random policies_ (Fig. 9 Right). In the normal policies column, we only compare the policies with regular policies giving helpful advice. In the classifier column, we compare them with the performance of classifiers only. In the malicious and random policies column, we compare them with unreasonable policies only.

Fig. 9 demonstrates that all three algorithms could outperform the malicious/random policies. However, CAMS-Random-Policy does not outperform the best classifier while both CAMS and CAMS-MAX can on both benchmarks. CAMS-MAX approaches the performance of the best policy but does not outperform the best policy on both benchmarks. Finally, perhaps surprisingly, CAMS outperforms the best policy (Oracle) on both benchmarks and continues to approach the hypothetical, optimal policy (with 0 cumulative loss).

This surprising factor is contributed by the adaptive weighted policy of CAMS, which adaptively creates a better policy by combining the advantage of each sub-optimal policy and classifier to reach the performance of the hypothetical, optimal policy (defined as \(\sum_{t=1}^{T}\min_{i\in[n+k]}\widetilde{\ell}_{t,i}\)). The second reason could be that the benchmark we created, or any real-world cases, will not be strictly in a stochastic setting (in which a single policy outperforms all others or has lower \(\mu\) in every round). The weight policy strategy can make a better combination of advice for this case.

Figure 9: Comparing CAMS, CAMS-MAX and CAMS-Random-Policy with top policies and classifiers in the VERTEBRA and HIV benchmarks. They outperform all the malicious/random policies. Moreover, CAMS and CAMS-MAX outperform the best classifier. Finally, only CAMS even exceeds the best policy (Oracle) in both benchmarks and continues approaching the hypothetical, optimal policy (0 cumulative loss). 90% confident intervals are indicated in shades.

### The CAMS-MAX algorithm

CAMS-MAX is a variant of CAMS. In an adversarial setting, they share the same algorithm. However, in a stochastic setting, CAMS-MAX gets the index \(i^{*}\) of max value in the probability distribution of policy \(\bm{q}\), and selects the model with the max value in \(\pi_{i^{*}}\) (\(\bm{x}_{t}\)) to recommendation. The difference is marked as blue color in Fig. 10.

### The CAMS-Random-Policy algorithm

```
1:Input: Models \(\mathcal{F}\), policies \(\Pi^{*}\), #rounds \(T\), budget \(b\)
2:Initialize loss \(\tilde{\bm{L}}_{0}\gets 0\); query cost \(C_{0}\gets 0\)
3:for\(t=1,2,...,T\)do
4: Receive \(\bm{x}_{t}\)
5:\(\eta_{t}\leftarrow\textsc{SetRate}(t,\bm{x}_{t},|\Pi^{*}|)\)
6: Set \(q_{t,i}\propto\exp\left(-\eta_{t}\tilde{\bm{L}}_{t-1,i}\right)\forall i\in| \Pi^{*}|\)
7:\(j_{t}\leftarrow\textsc{Recommend}(\bm{x}_{t},\bm{q}_{t})\)
8: Output \(\hat{y}_{t,j_{0}}\sim f_{t,j_{0}}\) as the prediction for \(\bm{x}_{t}\)
9: Compute \(z_{t}\) in Eq. (4)
10: Sample \(U\sim\textsc{Set}(z_{t})\)
11:if\(\bm{U}_{t}=1\) and \(C_{t}\leq\bm{0}\)then
12: Query the label \(y_{t}\)
13:\(C_{t}\gets C_{t-1}+1\)
14: Compute \(\bm{\ell}_{t}\): \(\ell_{t,j}=\mathbb{I}\left\{\hat{y}_{t,j}\neq y_{t}\right\},\forall j\in| \mathcal{F}|\)
15: Estimate model loss: \(\hat{\ell}_{t,j}=\frac{\ell_{t,j}}{z_{t}},\forall j\in|\mathcal{F}|\)
16:\(\tilde{\bm{\ell}}_{t}\): \(\hat{\ell}_{t,i}\sim\langle\pi_{i}(\bm{x}_{t}),\hat{\ell}_{t,j}\rangle,\forall i \in|[\Pi^{*}|]\)
17:\(\tilde{\bm{L}}_{t}=\tilde{\bm{L}}_{t-1}+\tilde{\bm{\ell}}_{t}\)
18:else
19:\(\tilde{\bm{L}}_{t}=\tilde{\bm{L}}_{t-1}\)
20:\(C_{t}\gets C_{t-1}\)
21:endif
22:endfor ```

**Algorithm 10** The CAMS-MAX Algorithm

### The CAMS-Random-Policy algorithm

CAMS-Random-Policy is a variant of CAMS. It shares the same algorithm with CAMS in an adversarial environment. However, it uses a random sampling policy method in a stochastic setting.

Figure 11: The CAMS-Random-Policy Algorithm

Figure 10: The CAMS-MAX AlgorithmIt randomly samples the policy from the probability distribution of policy \(\bm{q}\), and selects the model with max value in \(\pi_{i^{*}}\left(\bm{x}_{t}\right)\) to recommendation. The difference is marked as blue color in Fig. 11.

### Maximal queries from experiments

Table 6 in this section summarizes the maximum query cost for a given data stream (of fixed total size), with its associated cumulative loss for all baselines (exclude Oracle) on all benchmarks in experiment section. The result in this table is slightly different from the query complexity curves of Fig. 2 (Middle). The curve in Fig. 2 (Middle) takes the average value, while the table takes the maximal value from a fixed number of simulations. CAMS wins over all baselines (other than Oracle) in terms of query cost on CIFAR10, DRIFT, and VERTEBRAL benchmarks. CAMS outperforms all baselines in terms of cumulative loss on DRIFT, VERTEBRAL, and HIV benchmarks. In particular, CAMS outperforms both cumulative loss and query cost on the DRIFT and VERTEBRAL benchmarks.

### Query complexity

To achieve the same level of prediction accuracy (measured by average cumulative loss over a fixed number of rounds), CAMS incurs less than 10% of the label cost of the best competing baselines on CIFAR10 (10K examples), and 68% the cost on VERTEBRAL (see Fig. 12); Fig. 128 and Table 6 also demonstrate the compelling effectiveness of CAMS's query strategy outperforming all baselines in terms of query cost in VERTEBRAL, DRIFT, and CIFAR10 benchmarks, which is consistent with our query complexity bound in Theorem 2.

Footnote 8: We also consider variants for each algorithm (other than Random and Oracle) where we scale the query probabilities based on the early-phase performance and observe similar behavior. See App. G.11 for the corresponding results.

### Fine-tuning the query probabilities for stochastic streams

For the experimental results we reported in the main paper, we consider a streaming setting where the data arrives online in an _arbitrary order_ and _arbitrary length_. Therefore, for both CAMS and

Figure 12: Comparing CAMS with 7 baselines on 4 diverse benchmarks in terms of query complexity (Number of queries). CAMS outperforms all baselines for a fixed number of rounds \(T\) (where \(T=10000,3000,80,4000\) from left to right) and maximal query cost \(B\) (where \(B=1200,2000,80,2000\) from left to right). **Algorithms:** 4 contextual {Oracle, CQBC, CIWAL, CAMS} and 4 non-contextual baselines {RS, QBC, IWAL, MP} are included (see Section ). 90% confident interval are indicated in shades.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Algorithm** & **CIFAR10** & **DRIFT** & **VERTEBRAL** & **HIV** \\ \hline _Max queries, Cumulative loss_ & _1200, 10000_ & _2000, 3000_ & _80, 80_ & _2000, 4000_ \\ \hline RS & 1200, 2916 & 2000, 766 & 80, 19 & 2000, 143 \\ \hline QBC & 1200, 2857 & 1904, 771 & 72, 20 & 2000, 139 \\ \hline IWAL & 1200, 2854 & 2000, 760 & 80, 19 & 690, 140 \\ \hline MP & 1200, 2885 & 493, 803 & 33, 25 & 153, 148 \\ \hline CQBC & 1200, 2284 & 1900, 744 & 68, 13 & 2000, 124 \\ \hline CIWAL & 1200, 2316 & 2000, 746 & 80, 12 & 690, 124 \\ \hline
**CAMS** & **348**, 2348 & **251, 710** & **32, 11** & 782, **112** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Maximal queries from experiments

[MISSING_PAGE_FAIL:35]

### Relative Cumulative Loss

Relative cumulative loss (RCL).At round \(t\), we define RCL as \(L_{t,j_{i}}-L_{t,j^{*}}\), where \(L_{t,j^{*}}\) stands for the cumulative loss (CL) of the policy always selecting the best classifier, and \(L_{t,j_{i}}\) stands for the CL of policy \(i\).

The RCL under the same query cost for all baselines is shown in Fig. 15. The loss trajectory demonstrates that CAMS efficiently adapts to the best policy after only a few rounds and outperforms all baselines in all benchmarks. The result also demonstrates that CAMS can achieve negative RCL on all benchmarks, which means it outperforms any algorithms that chase the best classifier, as the horizontal 0 line represents the performance benchmark of best classifier. This empirical result aligns with Theorem 1 that, in the worst scenario, if the best classifier is the best policy, CAMS will achieve its performance. Otherwise, CAMS will reach a better policy and incurs no regret.

CAMS could achieve such performance because when an Oracle fails to achieve 0 loss over all instances and contexts, CAMS has the opportunity to outperform the Oracle _in those rounds Oracle does not make the best recommendation_. For instance, the stochastic version of CAMS (Line 22-23; Line 30-32 in Fig. 1) may achieve this by recommending a model using the weighted majority vote among all policies. Therefore, one can view CAMS as adaptively constructing a new policy at each

Figure 14: Comparing CAMS (in red) with CAMS-nonactive (in blue) on 4 diverse benchmarks in terms of query complexity, and cost effectiveness. CAMS outperforms or performs equally well to CAMS-nonactive with much less queried labels for all benchmarks. **(Top)** Number of queries and **(Bottom)** Performance of cumulative loss by increasing the query cost, for a fixed number of rounds \(T\) (where \(T=5000,3000,80,4000\) from left to right) and maximal query cost \(B\) (where \(B=T=5000,3000,80,4000\) from left to right). 90% confident interval are indicated in shades.

Figure 15: Comparing CAMS with 7 baselines on 4 diverse benchmarks in terms of loss trajectory. CAMS outperforms all baselines. Performance measured by relative cumulative loss (i.e. loss against the best classifier) under a fixed query cost \(B\) (where \(B=200,400,30,400\) from left to right). **Algorithms: 4 contextual {Oracle, CQBC, CIWAL, CAMS}** and 4 non-contextual baselines {RS, QBC, IWAL, MP} are included (see Section ). 90% confident interval are indicated in shades.

[MISSING_PAGE_FAIL:37]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We list the paper's contribution and scope clearly at the last paragraph of Section 1 Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed our limitation at Section 7 future direction. It summarizes the limitations of our approach and we hope to address in the future work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We have listed all the details of modeling approaches, proof assumptions, and complete proofs in Section 5, and Appendix E, F. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide the code and data in the supplementary material with a readme.txt for reproducing the results. Experiment details are listed in Section 6 and Appendix G, D.6. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the code and data in the supplementary material with a readme.txt for reproducing the results. Experiment details are listed in Section 6 and Appendix G, D.6. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experiment details are listed in Section 6 and Appendix G, D.6. In addition, we provide the code and data in the supplementary material with a readme.txt for reproducing the results. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide standard deviation in tables and confidence interval in figures. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: It can be found at the supplementary Appendix D.6. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The codes are anonymous as well. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: It has been discussed at Abstract and Section 1. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.