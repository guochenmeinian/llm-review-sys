# Outlier-Robust Wasserstein DRO

 Sloan Nietert

Cornell University

nietert@cs.cornell.edu &Ziv Goldfeld

Cornell University

goldfeld@cornell.edu &Soroosh Shafiee

Cornell University

shafiee@cornell.edu

###### Abstract

Distributionally robust optimization (DRO) is an effective approach for data-driven decision-making in the presence of uncertainty. Geometric uncertainty due to sampling or localized perturbations of data points is captured by Wasserstein DRO (WDRO), which seeks to learn a model that performs uniformly well over a Wasserstein ball centered around the observed data distribution. However, WDRO fails to account for non-geometric perturbations such as adversarial outliers, which can greatly distort the Wasserstein distance measurement and impede the learned model. We address this gap by proposing a novel outlier-robust WDRO framework for decision-making under both geometric (Wasserstein) perturbations and non-geometric (total variation (TV)) contamination that allows an \(\varepsilon\)-fraction of data to be arbitrarily corrupted. We design an uncertainty set using a certain robust Wasserstein ball that accounts for both perturbation types and derive minimax optimal excess risk bounds for this procedure that explicitly capture the Wasserstein and TV risks. We prove a strong duality result that enables tractable convex reformulations and efficient computation of our outlier-robust WDRO problem. When the loss function depends only on low-dimensional features of the data, we eliminate certain dimension dependencies from the risk bounds that are unavoidable in the general setting. Finally, we present experiments validating our theory on standard regression and classification tasks.

## 1 Introduction

The safety and effectiveness of various operations rely on making informed, data-driven decisions in uncertain environments. Distributionally robust optimization (DRO) has emerged as a powerful framework for decision-making in the presence of uncertainties. In particular, Wasserstein DRO (WDRO) captures uncertainties of geometric nature, e.g., due to sampling or localized (adversarial) perturbations of the data points. The WDRO problem is a two-player zero-sum game between a learner (decision-maker), who chooses a decision \(\theta\in\Theta\), and Nature (adversary), who chooses a distribution \(\nu\) from an ambiguity set defined as the \(p\)-Wasserstein ball of a prescribed radius around the observed data distribution \(\tilde{\mu}\). Namely, WDRO is given by1

Footnote 1: Here, \(\mathsf{W}_{p}(\mu,\nu):=\inf_{\pi\in\Pi(\mu,\nu)}\big{(}\int\|x-y\|^{p}d\pi(x, y)\big{)}^{1/p}\) is the \(p\)-Wasserstein metric between \(\mu\) and \(\nu\), where \(\Pi(\mu,\nu)\) is the set of all their couplings.

\[\inf_{\theta\in\Theta}\sup_{\nu:\,\mathsf{W}_{p}(\nu,\tilde{\mu})\leq\rho}\mathbb{ E}_{Z\sim\nu}[\ell(\theta,Z)],\] (1)

whose solution \(\hat{\theta}\in\Theta\) is chosen to minimize risk over the Wasserstein ball with respect to (w.r.t.) the loss function \(\ell\). WDRO has received considerable attention in many fields, including machine learning [6; 22; 45; 48; 59], estimation and filtering [36; 37; 46], and chance constraint programming [12; 55]. In many practical scenarios, the observed data may be contaminated by non-geometric perturbations, such as adversarial outliers. Unfortunately, the WDRO problem from (1) is not suited for handling thisissue, as even a small fraction of outliers can greatly distort the \(\mathsf{W}_{p}\) measurement and impede decision-making. In this work, we address this gap by proposing a novel outlier-robust WDRO framework that can learn well-performing decisions even in the presence of outliers. We couple it with a comprehensive theory of excess risk bounds, statistical guarantees, and computationally-tractable reformulations, as well as supporting numerical results.

### Contributions

We consider a scenario where the observed data distribution \(\tilde{\mu}\) is subject to both geometric (Wasserstein) perturbations and non-geometric (total variation (TV)) contamination, which allows an \(\varepsilon\)-fraction of data to be arbitrarily corrupted. Namely, if \(\mu\) is the true (unknown) data distribution, then the Wasserstein perturbation maps it to some \(\mu^{\prime}\) with \(\mathsf{W}_{p}(\mu^{\prime},\mu)\leq\rho\), and the TV contamination step further produces \(\tilde{\mu}\) with \(\|\tilde{\mu}-\mu^{\prime}\|_{\mathsf{TV}}\leq\varepsilon\) (e.g., in the special case of the Huber model [28], \(\tilde{\mu}=(1-\varepsilon)\mu^{\prime}+\varepsilon\alpha\) where \(\alpha\) is an arbitrary noise distribution). To enable robust decision-making under this model, we replace the Wasserstein ambiguity set in (1) with a ball w.r.t. the recently proposed outlier-robust Wasserstein distance \(\mathsf{W}_{p}^{\varepsilon}\)[38, 39]. The \(\mathsf{W}_{p}^{\varepsilon}\) distance (see (2) ahead) filters out the \(\varepsilon\)-fraction of mass from the contaminated distribution that contributed most to the transportation cost, and then measures the \(\mathsf{W}_{p}\) distance post-filtering. To obtain well-performing solutions for our WDRO problem, the \(\mathsf{W}_{p}^{\varepsilon}\) ball is intersected with a set that encodes standard moment assumptions on the uncorrupted data distribution, which are necessary for meaningful outlier-robust estimation guarantees.

We establish minimax optimal excess risk bounds for the decision \(\hat{\theta}\) that solves the proposed outlier-robust WDRO problem. The bounds control the gap \(\mathbb{E}[\ell(\hat{\theta},Z)]-\mathbb{E}[\ell(\theta_{\star},Z)]\), where \(Z\sim\mu\) follows the true data distribution and \(\theta_{\star}=\operatorname*{argmin}_{\theta}\mathbb{E}[\ell(\theta,Z)]\) is the optimal decision, subject to regularity properties of \(\ell_{\star}=\ell(\theta_{\star},\cdot)\). In turn, our bounds imply that the learner can make effective decisions using outlier-robust WDRO based on the contaminated observation \(\tilde{\mu}\), so long as \(\ell_{\star}\) has low variational complexity. The bounds capture this complexity using the Lipschitz or Sobolev seminorms of \(\ell_{\star}\) and clarify the distinct effect of each perturbation (Wasserstein versus TV) on the quality of the learned \(\hat{\theta}\) solution. We further establish their minimax optimality when \(p=1\), by providing a matching lower bound in the setting when an adversary picks a class of Lipschitz functions over which the learner must perform uniformly well. The excess risk bounds become looser as the data dimension \(d\) grows. We show that this degradation is alleviated when the loss function depends on the data only through \(k\)-dimensional affine features, by providing risk bounds that adapt to \(k\) instead of \(d\).

We then move to study the computational side of the problem, which may initially appear intractable due to non-convexity of the constraint set. We resolve this via a cheap preprocessing step that computes a coarse robust estimate of the mean [34] and replaces the original constraint set (that involves the true mean) with a version centered around the estimate. We adapt our excess risk bounds to this formulation and then prove a strong duality theorem. The dual form is reminiscent of the one for classical WDRO with adaptations reflecting the constraint to the clean distribution family and the partial transportation under \(\mathsf{W}_{p}^{\varepsilon}\). Under additional convexity conditions on the loss, we further derive an efficiently-computable, finite-dimensional, convex reformulation. The optimization results are also adapted to the setting with low-dimensional features. Using the developed machinery, we present experiments that validate our theory on simple regression/classification tasks and demonstrate the superiority of the proposed approach over classical WRDO, when the observed data is contaminated.

### Related Work

Distributionally robust optimization.The Wasserstein distance has emerged as a powerful tool for modeling uncertainty in the data generating distribution. It was first used to construct an ambiguity set around the empirical distribution in [40]. Recent advancements in convex reformulations and approximations of the WDRO problem, as discussed in [8; 20; 35], have brought notable computational advantages. Additionally, WDRO is linked to various forms of variation [2; 9; 19; 43] and Lipschitz [7; 11; 44] regularization, which contribute to its success in practice. Robust generalization guarantees can also be provided by WDRO via measure concentration argument or transportation inequalities [18; 30; 31; 51; 53; 54]. Several works have raised concerns regarding the sensitivity of standard DRO to outliers [24; 27; 58]. An attempt to address this was proposed in [56] using a refined risk function based on a family of \(f\)-divergences. This formulation aims to prevent DRO from overfitting to potential outliers but is not robust to geometric perturbations. Further, their risk bounds require a moment condition to hold uniformly over \(\Theta\), in contrast to our bounds that depend only on \(\theta_{\star}\). We are able to address these limitations by setting a WDRO framework based on partial transportation. While partial OT has been previously used in the context of DRO problems, it was introduced to address stochastic programs with side information in [17] rather than to account for outlier robustness. Another closely related line of work is presented in [4; 5], where the ambiguity set is constructed using an \(f\)-divergence to mitigate statistical errors and the Prokhorov distance to handle outlier data. The proposed model is both computationally efficient and statistically reliable. However, they have not investigated its minimax optimality or robustness against the Huber contamination model, which we aim to do in this paper. Additionally, a best-case favorable analysis approach has been proposed in [29] to address outlier data. This approach is an alternative to the worst-case distributionally robust method. However, it requires solving a non-convex optimization problem, significantly impacting its scalability, and is not accompanied by any proof of minimax optimality.

Robust statistics.The problem of learning from data under TV \(\varepsilon\)-corruptions dates back to [28]. Over the years, various robust and sample-efficient estimators, particularly for mean and scale parameters, have been developed in the robust statistics community; see [41] for a comprehensive survey. The theoretical computer science community, on the other hand, has focused on developing computationally efficient estimators that achieve optimal estimation rates in high dimensions [13; 16]. Relatedly, the probably approximate correct (PAC) learning framework has been well-studied in similar models [1; 10]. Recently, [58] developed a unified robust estimation framework based on minimum distance estimation that gives sharp population-limit and promising finite-sample guarantees for mean and covariance estimation, as well linear regression. Their analysis centers on a generalized resilience quantity, which is essential to our work. We are unaware of any results in the settings above which extend to combined TV and \(\mathsf{W}_{p}\) corruptions. Finally, our analysis relies on the outlier-robust Wasserstein distance from [38; 39], which was shown to yield an optimal minimum distance estimate for robust distribution estimation under \(\mathsf{W}_{p}\) loss.

## 2 Preliminaries

Notation.Consider a closed, non-empty set \(\mathcal{Z}\subseteq\mathbb{R}^{d}\) equipped with the Euclidean norm \(\|\cdot\|\). A continuously differentiable function \(f:\mathcal{Z}\to\mathbb{R}\) is called \(\alpha\)-smooth if \(\|\nabla f(z)-\nabla f(z^{\prime})\|\leq\alpha\|z-z^{\prime}\|\), for all \(z,z^{\prime}\in\mathcal{Z}\). The perspective function of a lower semi-continuous (l.s.c.) and convex function \(f\) is \(P_{f}(x,\lambda)\coloneqq\lambda f(x/\lambda)\) for \(\lambda>0\), with \(P_{f}(x,\lambda)=\lim_{\lambda\to 0}\lambda f(x/\lambda)\) when \(\lambda=0\). The convex conjugate of \(f\) is \(f^{*}(y)\coloneqq\sup_{x\in\mathbb{R}^{d}}y^{\top}x-f(x)\). We denote by \(\chi_{\mathcal{Z}}\) the indicator function of \(\mathcal{Z}\), that is, \(\chi_{\mathcal{Z}}(z)=0\) if \(z\in\mathcal{Z}\) and \(\chi_{\mathcal{Z}}(z)=\infty\) otherwise. The convex conjugate of \(\chi_{\mathcal{Z}}\), denoted by \(\chi_{\mathcal{Z}}^{*}\), is termed as the support function of \(\mathcal{Z}\).

We use \(\mathcal{M}(\mathcal{Z})\) for the set of signed Radon measures on \(\mathcal{Z}\) equipped with the TV norm \(\|\mu\|_{\mathsf{TV}}\coloneqq\frac{1}{2}|\mu|(\mathcal{Z})\), and write \(\mu\leq\nu\) for set-wise inequality. The class of Borel probability measures on \(\mathcal{Z}\) is denoted by \(\mathcal{P}(\mathcal{Z})\). We write \(\mathbb{E}_{\mu}[f(Z)]\) for expectation of \(f(Z)\) with \(Z\sim\mu\); when clear from the context, the random variable is dropped and we write \(\mathbb{E}_{\mu}[f]\). Let \(\Sigma_{\mu}\) denote the covariance matrix of \(\mu\in\mathcal{P}_{2}(\mathcal{Z})\). Define \(\mathcal{P}_{p}(\mathcal{Z})\coloneqq\{\mu\in\mathcal{P}(\mathcal{Z}):\inf_{z \in\mathcal{Z}}\mathbb{E}_{\mu}[\|Z-z_{0}\|^{p}]<\infty\}\). The push-forward of \(f\) through \(\mu\in\mathcal{P}(\mathcal{Z})\) is \(f_{\#}\mu(\cdot)\coloneqq\mu(f^{-1}(\cdot))\), and, for \(\mathcal{A}\subseteq\mathcal{P}(\mathcal{Z})\), write \(f_{\#}\mathcal{A}\coloneqq\{f_{\#}\mu:\mu\in\mathcal{A}\}\). The \(p\)th order homogeneous Sobolev (semi)norm of continuously differentiable \(f:\mathcal{Z}\to\mathbb{R}\) w.r.t. \(\mu\) is \(\|f\|_{H^{1,p}(\mu)}\coloneqq\mathbb{E}_{\mu}[\|\nabla f\|^{p}]^{1/p}\). The set of integers up to \(n\in\mathbb{N}\) is denote by \([n]\); we also use the shorthand \([x]_{+}=\max\{x,0\}\). We write \(\lesssim,\gtrsim,\asymp\) for inequalities/equality up to absolute constants.

Classical and outlier-robust Wasserstein distances.For \(p\in[1,\infty)\), the \(p\)-_Wasserstein distance_ between \(\mu,\nu\in\mathcal{P}_{p}(\mathcal{Z})\) is \(\mathsf{W}_{p}(\mu,\nu)\coloneqq\inf_{\pi\in\Pi(\mu,\nu)}\bigl{(}\mathbb{E}_{ \pi}\bigl{[}\|X-Y\|^{p}\bigr{]}\bigr{)}^{1/p}\), where \(\Pi(\mu,\nu)\coloneqq\{\pi\in\mathcal{P}(\mathcal{Z}^{2}):\,\pi(\cdot\times \mathcal{Z})=\mu,\,\pi(\mathcal{Z}\times\cdot)=\nu\}\) is the set of all their couplings. Some basic properties of \(\mathsf{W}_{p}\) are (see, e.g., [42; 52]): (i) \(\mathsf{W}_{p}\) is a metric on \(\mathcal{P}_{p}(\mathcal{Z})\); (ii) the distance is monotone in the order, i.e., \(\mathsf{W}_{p}\leq\mathsf{W}_{q}\) for \(p\leq q\); and (iii) \(\mathsf{W}_{p}\) metrizes weak convergence plus convergence of \(p\)th moments: \(\mathsf{W}_{p}(\mu_{n},\mu)\to 0\) if and only if \(\mu_{n}\stackrel{{ w}}{{\rightarrow}}\mu\) and \(\int\|x\|^{p}d\mu_{n}(x)\to\int\|x\|^{p}d\mu(x)\).

To handle corrupted data, we employ the _\(\varepsilon\)-outlier-robust \(p\)-Wasserstein distance2_, defined by

Footnote 2: While not a metric, \(\mathsf{W}_{p}^{\varepsilon}\) is symmetric and satisfies an approximate triangle inequality ([39], Proposition 3).

\[\mathsf{W}_{p}^{\varepsilon}(\mu,\nu)\coloneqq\inf_{\begin{subarray}{c}\mu^{ \prime}\in\mathcal{P}(\mathbb{R}^{d})\\ \|\mu^{\prime}-\mu\|_{\mathsf{TV}}\leq\varepsilon\end{subarray}}\mathsf{W}_{p }(\mu^{\prime},\nu)=\inf_{\begin{subarray}{c}\nu^{\prime}\in\mathcal{P}( \mathbb{R}^{d})\\ \|\nu^{\prime}-\nu\|_{\mathsf{TV}}\leq\varepsilon\end{subarray}}\mathsf{W}_{p }(\mu,\nu^{\prime}).\] (2)

The second equality is a useful consequence of Lemma 4 in [39] (see Appendix A for details, along with an interpretation of \(\mathsf{W}_{p}^{\varepsilon}\) as a partial OT distance).

Robust statistics.Resilience is a standard sufficient condition for population-limit robust statistics bounds [49; 58]. The _\(p\)-Wasserstein resilience_ of a measure \(\mu\in\mathcal{P}(\mathcal{Z})\) is defined by

\[\tau_{p}(\mu,\varepsilon)\coloneqq\sup_{\mu^{\prime}\leq\frac{1}{1-\varepsilon }}\sup_{\mu^{\prime}\leq\frac{1}{1-\varepsilon}\mu}\mathsf{W}_{p}(\mu^{\prime },\mu),\]

and that of a family \(\mathcal{G}\subseteq\mathcal{P}(\mathbb{R})\) by \(\tau_{p}(\mathcal{G},\varepsilon)\coloneqq\sup_{\mu\in\mathcal{G}}\tau_{p}(\mu,\varepsilon)\). The relation between \(\mathsf{W}_{p}\) resilience and robust estimation is formalized in the following proposition.

**Proposition 1** (Robust estimation under \(\mathsf{W}_{p}\) resilience [39]).: _Fix \(0\leq\varepsilon\leq 0.49\). For any clean distribution \(\mu\in\mathcal{G}\subseteq\mathcal{P}(\mathcal{Z})\) and corrupted measure \(\tilde{\mu}\in\mathcal{P}(\mathcal{Z})\) such that \(\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu},\mu)\leq\rho\), the minimum distance estimate \(\tilde{\mu}=\operatorname*{argmin}_{\nu\in\mathcal{G}}\mathsf{W}_{p}^{ \varepsilon}(\nu,\tilde{\mu})\) satisfies \(\mathsf{W}_{p}(\tilde{\mu},\mu)\lesssim\rho+\tau_{p}(\mathcal{G},2\varepsilon)\).3_

Footnote 3: If a minimizer does not exist for either problem, an infimizing sequence will achieve the same guarantee.

Throughout, we focus on the bounded covariance class \(\mathcal{G}_{\mathrm{cov}}\coloneqq\big{\{}\mu\in\mathcal{P}(\mathcal{Z}):\, \Sigma_{\mu}\preceq I_{d}\big{\}}\).

**Proposition 2** (\(\mathsf{W}_{p}\) resilience bound for \(\mathcal{G}_{\mathrm{cov}}\)[39]).: _Fixing \(0\leq\varepsilon\leq 0.99\) and \(1\leq p\leq 2\), we have \(\tau_{p}(\mathcal{G}_{\mathrm{cov}},\varepsilon)\lesssim\sqrt{d}\,\varepsilon^ {1/p-1/2}\)._

## 3 Outlier-robust WDRO

We perform stochastic optimization with respect to an unknown data distribution \(\mu\), given access only to a corrupted version \(\tilde{\mu}\). We allow both localized Wasserstein perturbations, that map \(\mu\) to some \(\mu^{\prime}\) with \(\mathsf{W}_{p}(\mu,\mu^{\prime})\leq\rho\), and \(\mathsf{TV}\)\(\varepsilon\)-contamination that takes \(\mu^{\prime}\) to \(\tilde{\mu}\) with \(\|\tilde{\mu}-\mu^{\prime}\|_{\mathsf{TV}}\leq\varepsilon\). Equivalently, both perturbations are captured by \(\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu},\mu)\leq\rho\).4 To simplify notation, we henceforth suppress the dependence of the loss function \(\ell\) on the model parameters \(\theta\in\Theta\), writing \(\ell\) for \(\ell(\theta,\cdot)\) for a specific function and \(\mathcal{L}=\{\ell(\theta,\cdot)\}_{\theta\in\Theta}\) for the whole class. Our full model is as follows.

Footnote 4: We defer explicit modeling of sampling to Section 3.2 but note that the following results immediately transfer to the \(n\)-sample setting so long as \(\rho\) is taken to be larger than \(\mathsf{W}_{p}(\tilde{\mu}_{n},\mu)\) with high probability.

**Setting A:** Fix a \(p\)-Wasserstein radius \(\rho\geq 0\) and \(\mathsf{TV}\) contamination level \(\varepsilon\in[0,0.49]\). Let \(\mathcal{L}\subseteq\mathbb{R}^{\mathcal{Z}}\) be a family of real-valued loss functions on \(\mathcal{Z}\), such that each \(\ell\in\mathcal{L}\) is l.s.c. with \(\sup_{z\in\mathcal{Z}}\frac{\ell(z)}{1+\|z\|^{p}}<\infty\), and fix a class \(\mathcal{G}\subseteq\mathcal{P}_{p}(\mathcal{Z})\) encoding distributional assumptions. We consider the following model:

1. Nature selects a distribution \(\mu\in\mathcal{G}\), unknown to the learner;
2. The learner observes a corrupted measure \(\tilde{\mu}\in\mathcal{P}(\mathcal{Z})\) such that \(\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu},\mu)\leq\rho\);
3. The learner selects a decision \(\hat{\ell}\in\mathcal{L}\) and suffers excess risk \(\mathbb{E}_{\mu}[\hat{\ell}]-\inf_{\ell\in\mathcal{L}}\mathbb{E}_{\mu}[\ell]\).

We seek a decision-making procedure for the learner which provides strong excess risk guarantees when \(\ell_{\star}=\operatorname*{argmin}_{\ell\in\mathcal{L}}\mathbb{E}_{\mu}[\ell]\) is appropriately "simple." To achieve this, we introduce the _\(\varepsilon\)-outlier-robust \(p\)-Wasserstein DRO problem_:

\[\inf_{\ell\in\mathcal{L}}\sup_{\nu\in\mathcal{G}:\,\mathsf{W}_{p}^{\varepsilon}( \tilde{\mu},\nu)\leq\rho}\mathbb{E}_{\nu}[\ell].\] (OR-WDRO)

### Excess Risk Bounds

We quantify the excess risk of decisions made using OR-WDRO for the two most popular choices of order, \(p=1,2\). Proofs are provided in Supplement B.

**Theorem 1** (OR-WDRO risk bound).: _Under Setting \(A\), let \(\hat{\ell}\) minimize (OR-WDRO). Then, writing \(c=2(1-\varepsilon)^{-1/p}\), the excess risk is bounded by_

\[\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}]\leq \begin{cases}\|\ell_{\star}\|_{\mathrm{Lip}}\big{(}c\rho+2\tau_{1}(\mathcal{G},2\varepsilon)\big{)},&p=1,\ell_{\star}\text{ Lipschitz}\\ \|\ell_{\star}\|_{\hat{H}^{1,2}(\mu)}\big{(}c\rho+2\tau_{2}(\mathcal{G},2 \varepsilon)\big{)}+\frac{1}{2}\alpha\big{(}c\rho+2\tau_{2}(\mathcal{G},2 \varepsilon)\big{)}^{2},&p=2,\ell_{\star}\text{ $\alpha$-smooth}\end{cases}.\]

Note that \(c=O(1)\) since \(\varepsilon\leq 0.49\). These bounds imply that the learner can make effective decisions when \(\ell_{\star}\) has low variational complexity5. In contrast, there are simple regression settings with TV corruption that drive the excess risk of standard WDRO to infinity. Our proof derives both results as a special case of a general bound in terms of the \(\mathsf{W}_{p}\) regularizer, defined by \(\mathcal{R}_{\mu,p}(\rho;\ell)\coloneqq\sup_{\nu^{\prime}\in\mathcal{P}( \mathcal{Z}):\mathsf{W}_{p}(\nu^{\prime},\nu^{\prime})\leq\rho}\mathbb{E}_{ \nu^{\prime}}[\ell]-\mathbb{E}_{\nu}[\ell]\). Introduced in [18], this quantity appears implicitly throughout the WDRO literature. In particular, for each \(\ell\in\mathcal{L}\), we derive the following bound:

Footnote 5: The same bounds hold up to \(\varepsilon\) additive slack if \(\ell_{\star}\) is only \(\varepsilon\)-approximately optimal for (OR-WDRO).

\[\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell]\leq\mathcal{R}_{\mu,p} \big{(}\underbrace{c\rho}_{\mathsf{W}_{p}}+\underbrace{2\tau_{p}(\mathcal{G},2\varepsilon)}_{\mathsf{TV}};\hat{\ell}\big{)},\] (3)

whose radius reveals the effect of each perturbation (viz. Wasserstein versus TV) on the quality of the decision. The first bound of the theorem follows by plugging in \(p=1\) and controlling \(\mathcal{R}_{\mu,1}\) via Kantorovich duality. The second bound uses \(p=2\) and controls \(\mathcal{R}_{\mu,2}\) by replacing \(\ell\) with its Taylor expansion about \(Z\sim\mu\). We now instantiate Theorem 1 for the bounded covariance class \(\mathcal{G}_{\mathrm{cov}}\).

**Corollary 1** (Risk bounds for \(\mathcal{G}_{\mathrm{cov}}\)).: _Under the setting of Theorem 1 with \(\mathcal{G}\subseteq\mathcal{G}_{\mathrm{cov}}\), we have_

\[\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}]\lesssim \begin{cases}\|\ell_{\star}\|_{\mathrm{Lip}}\big{(}\rho+\sqrt{d\varepsilon} \,\big{)},&p=1,\ell_{\star}\text{ Lipschitz}\\ \|\ell_{\star}\|_{\hat{H}^{1,2}(\mu)}(\rho+\sqrt{d}\,)+\alpha(\rho^{2}+d),&p=2,\ell_{\star}\text{ $\alpha$-smooth}\end{cases}.\]

Since \(\mathcal{G}_{\mathrm{cov}}\) encodes second moment constraints, \(\tau_{2}(\mathcal{G}_{\mathrm{cov}},\varepsilon)\asymp d\) is independent of \(\varepsilon\). Therefore, the first bound is preferable as \(\varepsilon\to 0\) if \(\|\ell_{\star}\|_{\hat{H}^{1,2}(\mu)}\approx\|\ell_{\star}\|_{\mathrm{Lip}}\), while the second is better when \(\varepsilon=\Omega(1)\) and \(\|\ell_{\star}\|_{\hat{H}^{1,2}(\mu)}\ll\|\ell_{\star}\|_{\mathrm{Lip}}\).6 Distinct trade-offs are observed under stronger tail bounds like sub-Gaussianity, i.e., for \(\mathcal{G}_{\mathrm{subG}}\coloneqq\{\mu\in\mathcal{P}(\mathcal{Z}):\, \mathbb{E}_{\mu}[e^{(\theta^{\top}(Z-\mathbb{E}[Z])^{2})}]\leq 2,\,\forall\theta \in\mathbb{S}^{d-1}\}\).

Footnote 6: Under \(\mathsf{W}_{p}^{c}\) perturbations, one may perform outlier-robust WDRO using any \(p^{\prime}\in[1,p]\), which may be advantageous in terms of the TV component of the excess risk.

**Corollary 2** (Risk bounds for \(\mathcal{G}_{\mathrm{subG}}\)).: _Under the setting of Theorem 1 with \(\mathcal{G}\subseteq\mathcal{G}_{\mathrm{subG}}\), the excess risk \(\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}]\) is bounded up to constants by_

\[\begin{cases}\|\ell_{\star}\|_{\mathrm{Lip}}\Big{(}\rho+\sqrt{d+\log\frac{1}{ \varepsilon}}\,\varepsilon\,\Big{)},&p=1,\ell_{\star}\text{ Lipschitz}\\ \|\ell_{\star}\|_{\hat{H}^{1,2}(\mu)}\Big{(}\rho+\sqrt{(d+\log\frac{1}{ \varepsilon})\varepsilon}\,\Big{)}+\alpha\Big{(}\rho^{2}+\big{(}d+\log\frac{1}{ \varepsilon}\big{)}\,\varepsilon\,\Big{)},&p=2,\ell_{\star}\text{ $\alpha$-smooth}\end{cases}.\]

**Remark 1** (Comparison to MDE under \(\mathsf{W}_{p}^{c}\)).: We note that the excess risk \(\mathcal{R}_{\mu,p}\big{(}c\rho+2\tau_{p}(\mathcal{G},2\varepsilon);\ell_{ \star}\big{)}\) from (3) can alternatively be obtained by performing standard \(p\)-WDRO with an expanded radius \(c\rho+2\tau_{1}(\mathcal{G},2\varepsilon)\) around the minimum distance estimate \(\hat{\mu}=\operatorname*{argmin}_{\nu\in\mathcal{G}}\mathsf{W}_{1}^{c}(\tilde{ \mu},\nu)\). However, obtaining \(\hat{\mu}\) is an expensive preprocessing step, and we are unaware of any efficient algorithms for such MDE in the finite-sample setting. In Supplement D, we explore recentering WDRO around a tractable estimate obtained from iterative filtering [15], but find the resulting risk to be highly suboptimal. Furthermore, the improvements to our risk bounds under low-dimensional structure, which are derived in Section 4, do not extend to decisions obtained from these alternative procedures.

We now show that Theorem 1 cannot be improved in general. In particular, the first bound is minimax optimal over Lipschitz loss families when \(\mu\in\mathcal{G}_{\mathrm{cov}}\).

**Proposition 3** (Lower bound).: _Fix \(\mathcal{Z}=\mathbb{R}^{d}\) and \(\varepsilon\in[0,0.49]\). For any \(L\geq 0\), there exists a family \(\mathcal{L}\subseteq\mathrm{Lip}_{L}(\mathbb{R}^{d})\), independent of \(\varepsilon\), such that for any decision rule \(\mathsf{D}:\mathcal{P}(\mathcal{Z})\to\mathcal{L}\) there exists a pair \((\mu,\tilde{\mu})\in\mathcal{G}_{\mathrm{cov}}\times\mathcal{P}(\mathcal{Z})\) with \(\mathsf{W}_{1}^{e}(\tilde{\mu},\mu)\leq\rho\) satisfying \(\mathbb{E}_{\mu}[\mathsf{D}(\tilde{\mu})]-\inf_{\ell\in\mathcal{L}}\mathbb{E}_{ \mu}[\ell]\gtrsim L\big{(}\rho+\sqrt{d\varepsilon}\,\big{)}\)._

Each family \(\mathcal{L}\) encodes a multivariate regression problem. Our proof combines a one-dimensional lower bound of [49] for linear regression with lower bounds of [39] for robust estimation under \(\mathsf{W}_{1}\).

### Statistical Guarantees

We next formalize a finite-sample model and adapt our excess risk bounds to it.

**Setting B:** Fix \(\rho,\varepsilon,\mathcal{L},\mathcal{G}\) as in Setting A, and let \(Z_{1},\ldots,Z_{n}\) be identically and independently distributed (i.i.d.) according to \(\mu\in\mathcal{G}\), with empirical measure \(\hat{\mu}_{n}=\frac{1}{n}\sum_{i=1}^{n}\delta_{Z_{i}}\). Upon observing these clean samples, Nature applies a \(\mathsf{W}_{p}\) perturbation of size \(\rho_{0}\), producing \(\{Z^{\prime}_{i}\}_{i=1}^{n}\) with empirical measure \(\mu^{\prime}_{n}\) such that \(\mathsf{W}_{p}(\hat{\mu}_{n},\mu^{\prime}_{n})\leq\rho_{0}\). Finally, Nature corrupts up to \(\lfloor\varepsilon n\rfloor\) samples to obtain \(\{\tilde{Z}_{i}\}_{i=1}^{n}\) with empirical measure \(\tilde{\mu}_{n}\) such that \(\|\tilde{\mu}_{n}-\mu^{\prime}\|_{\mathsf{TV}}=\frac{1}{n}\sum_{i=1}^{n}\mathds {1}\{\tilde{Z}_{i}\neq Z_{i}\}\leq\varepsilon\). Equivalently, the final dataset satisfies \(\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}_{n},\hat{\mu}_{n})\leq\rho_{0}\).7 The learner is now tasked with selecting \(\tilde{\ell}\in\mathcal{L}\) given \(\tilde{\mu}_{n}\).

Footnote 7: In general, \(\{\tilde{Z}_{i}\}_{i=1}^{n}\) may be any measurable function of \(\{Z_{i}\}_{i=1}^{n}\) and independent randomness such that \(\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}_{n},\hat{\mu}_{n})\leq\rho_{0}\).

The results from Section 3 apply whenever \(\rho\geq\rho_{0}+\mathsf{W}_{p}(\mu,\hat{\mu}_{n})\). In particular, we obtain the following corollary as an immediate consequence of Theorem 1 and Theorem 3.1 of [32].

**Corollary 3** (Finite-sample risk bounds).: _Under Setting B, fix \(\hat{\ell}\in\mathcal{L}\) minimizing (OR-WDRO) centered at \(\tilde{\mu}=\tilde{\mu}_{n}\) with \(\rho\geq\rho_{0}+100\,\mathbb{E}[\mathsf{W}_{p}(\mu,\hat{\mu}_{n})]\). Then the excess risk bounds of Theorem 1 hold with probability at least 0.99. If \(\mathcal{G}\in\{\mathcal{G}_{\text{cov}},\mathsf{G}_{\text{subG}}\}\), \(p=1\), and \(d\geq 3\), or if \(\mathcal{G}=\mathcal{G}_{\text{subG}}\), \(p=2\), and \(d\geq 5\), then \(\mathbb{E}[\mathsf{W}_{p}(\mu,\hat{\mu}_{n})]\lesssim\sqrt{d}n^{-1/d}\)._

**Remark 2** (Smaller radius).: In the classic WDRO setting with \(\rho_{0}=\varepsilon=0\), the radius \(\rho\) can be taken significantly smaller than \(n^{-1/d}\) if \(\mathcal{L}\) and \(\mu\) are sufficiently well-behaved. For example, [18] proves that \(\rho=\widetilde{O}(n^{-1/2})\) gives meaningful risk bounds when \(\mu\) satisfies a \(T_{2}\) transportation inequality.8 While this high-level condition may be hard to verify in practice, Supplement E shows that this improvement can be lifted to an instance of our outlier-robust WDRO problem.

Footnote 8: We say that \(\mu\in T_{2}(\tau)\) if \(\mathsf{W}_{2}(\nu,\mu)\leq\sqrt{\tau\mathsf{H}(\nu\|\mu)}\), for all \(\nu\in\mathcal{P}_{2}(\mathcal{Z})\), where \(\mathsf{H}(\nu\|\mu)\) is relative entropy.

### Tractable Reformulations and Computation

For computation, we restrict to \(\mu\in\mathcal{G}_{\text{cov}}\). Initially, (OR-WDRO) may appear intractable, since \(\mathcal{G}_{\text{cov}}\) is non-convex when viewed as a subset of the cone \(\mathcal{M}_{+}(\mathcal{Z})\). Moreover, enforcing membership to this class is non-trivial. To remedy these issues, we use a cheap preprocessing step to obtain a robust estimate \(z_{0}\in\mathcal{Z}\) of the mean \(\mathbb{E}_{\mu}[Z]\), and we optimize over the modified class \(\mathcal{G}_{2}(\sigma,z_{0})\coloneqq\big{\{}\nu\in\mathcal{P}(\mathcal{Z}): \mathbb{E}_{\nu}[\|Z-z_{0}\|^{2}]\leq\sigma^{2}\big{\}}\), with \(\sigma\gtrsim\|z_{0}-\mathbb{E}_{\mu}[Z]\|+\sqrt{d}\) taken so that \(\mu\in\mathcal{G}_{2}(\sigma,z_{0})\). Finally, for technical reasons, we switch to the one-sided robust distance \(\mathsf{W}_{p}^{\varepsilon}(\mu\|\nu)\coloneqq\inf_{\mu^{\prime}\in\mathcal{P} (\mathbb{R}^{d}):\mu^{\prime}\leq\frac{1}{1-\varepsilon}\mu}\mathsf{W}_{p}( \mu^{\prime},\nu)\). Altogether, we arrive at the modified DRO problem

\[\inf_{\ell\in\mathcal{L}}\sup_{\nu\in\mathcal{G}_{2}(\sigma,z_{0}):\mathsf{W} _{p}^{\varepsilon}(\tilde{\mu}_{n}\|\nu)\leq\rho}\mathbb{E}_{\nu}[\ell],\] (4)

which, as stated next, admits risk bounds matching Corollary 1 up to empirical approximation error.

**Proposition 4** (Risk bound for modified problem).: _Consider Setting B with \(\mathcal{G}\subseteq\mathcal{G}_{\text{cov}}\). Fix \(z_{0}\in\mathcal{Z}\) such that \(\|z_{0}-\mathbb{E}_{\mu}[Z]\|\leq E=O(\rho_{0}+\sqrt{d})\), and suppose that \(\mathsf{W}_{p}(\hat{\mu}_{n},\mu)\leq\delta\). Take \(\tilde{\ell}\) minimizing (4) with \(\rho=(\rho_{0}+\delta)(1-\varepsilon)^{-1/p}+\tau_{p}(\mathcal{G}_{\text{cov}},\varepsilon)\) and \(\sigma=\sqrt{d}+E\). We then have_

\[\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}]\lesssim\begin{cases} \|\ell_{\star}\|_{\text{Lip}}\big{(}\rho_{0}+\sqrt{d\varepsilon}+\delta\big{)},&p=1,\ell_{\star}\text{ Lipschitz}\\ \|\ell_{\star}\|_{\hat{H}^{1,2}(\mu)}\big{(}\rho_{0}+\sqrt{d}+\delta\big{)}+ \alpha\big{(}\rho_{0}+\sqrt{d}+\delta\big{)}^{2},&p=2,\ell_{\star}\text{ $\alpha$-smooth}\end{cases}.\]

Parameters \(\rho,\sigma\) are taken so that \(\mu\in\mathcal{G}_{2}(\sigma,z_{0})\) and \(\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}_{n}\|\mu)\leq\rho\). Noting this, the proof mirrors that of Theorem 1, using a \(\mathsf{W}_{p}\) resilience bound for \(\mathcal{G}_{2}(\sigma,z_{0})\). To ensure \(\mathsf{W}_{p}(\hat{\mu}_{n},\mu)\leq\delta\) with decent probability, one should take \(\delta\) to be an upper bound on \(\sup_{\nu\in\mathcal{G}}\mathbb{E}[\mathsf{W}_{p}(\hat{\nu}_{n},\nu)]\). When \(p=2\), this quantity is only finite if \(\mathcal{Z}\) is bounded or if \(\mathcal{G}\) encodes stronger tail bounds than \(\mathcal{G}_{\text{cov}}\) (see, e.g., [32]).

For efficient computation, we must specify a robust mean estimation algorithm to obtain \(z_{0}\) and a procedure for solving (4). The former is achieved by taking a coordinate-wise trimmed mean.

**Proposition 5** (Coarse robust mean estimation).: _Consider Setting \(B\) with \(\mathcal{G}\subseteq\mathcal{G}_{\text{cov}}\) and \(\varepsilon\leq 1/3\). For \(n=\Omega(\log(d))\), there is a trimmed mean procedure, which applied coordinate-wise to \(\{\tilde{Z}_{i}\}_{i=1}^{n}\), returns \(z_{0}\in\mathbb{R}^{d}\) with \(\|z_{0}-\mathbb{E}_{\mu}[Z]\|\lesssim\sqrt{d}+\rho_{0}\) with probability at least \(0.99\), in time \(\tilde{O}(d)\)._More sophisticated methods, e.g., iterative filtering [15], achieve dimension-free estimation guarantees at the cost of additional sample and computational complexity. We will return to these techniques in Section 4, but overlook them for now since they do not impact worst-case excess risk bounds.

We next show that that the inner maximization problem of (4) can be simplified to a minimization problem involving only two scalars provided the following assumption holds.

**Assumption 1** (Slater condition I).: Given the distribution \(\tilde{\mu}_{n}\) and the fixed point \(z_{0}\), there exists \(\nu_{0}\in\mathcal{P}(\mathcal{Z})\) such that \(\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}_{n}\|\nu_{0})<\rho\) and \(\mathbb{E}_{\nu_{0}}[\|Z-z_{0}\|^{2}]<\sigma^{2}\). Additionally, we require \(\rho>0\).

Notice that Assumption 1 indeed holds for \(\nu_{0}=\mu\) as applied in Proposition 4.

**Proposition 6** (Strong duality).: _Under Assumption 1, for any \(\ell\in\mathcal{L}\) and \(z_{0}\in\mathbb{R}^{d}\), we have_

\[\sup_{\begin{subarray}{c}\nu\in\mathcal{G}_{2}(\sigma,z_{0}):\\ \mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}_{n}\|\nu)\leq\rho\end{subarray}} \mathbb{E}_{\nu}[\ell]=\inf_{\begin{subarray}{c}\lambda_{1},\lambda_{2}\in \mathbb{R}_{+}\\ \alpha\in\mathbb{R}\end{subarray}}\lambda_{1}\sigma^{2}+\lambda_{2}\rho^{p}+ \alpha+\frac{1}{1-\varepsilon}\,\mathbb{E}_{\tilde{\mu}_{n}}\left[\,\bar{ \ell}(\cdot\,;\lambda_{1},\lambda_{2},\alpha)\right],\] (5)

_where \(\bar{\ell}(z;\lambda_{1},\lambda_{2},\alpha)\coloneqq\sup_{\xi\in\mathcal{Z}} \,\big{[}\,\ell(\xi)-\lambda_{1}\|\xi-z_{0}\|^{2}-\lambda_{2}\|\xi-z\|^{p}- \alpha\big{]}_{+}\)._

The minimization problem over \((\lambda_{1},\lambda_{2},\alpha)\) is an instance of stochastic convex optimization, where the expectation of the implicit function \(\bar{\ell}\) is taken w.r.t. the contaminated empirical measure \(\tilde{\mu}_{n}\). In contrast, the dual reformulation for classical WDRO only involves \(\lambda_{2}\) and takes the expectation of the implicit function \(\underline{\ell}(z;\lambda_{2})\coloneqq\sup_{\xi\in\mathcal{Z}}\ell(\xi)- \lambda_{2}\|\xi-z\|^{p}\) w.r.t. \(\tilde{\mu}_{n}\). The additional \(\lambda_{1}\) variable above is introduced to account for the clean family \(\mathcal{G}_{2}(\sigma,z_{0})\), and the use of partial transportation under \(\mathsf{W}_{p}^{\varepsilon}\) results in the introduction of the operator \([\cdot]_{+}\) and the decision variable \(\alpha\).

**Remark 3** (Connection to conditional value at risk (CVaR)).: The CVaR of a Borel measurable loss function \(\ell\) acting on a random vector \(Z\sim\mu\in\mathcal{P}(\mathcal{Z})\) with risk level \(\varepsilon\in(0,1)\) is defined as

\[\mathrm{CVaR}_{1-\varepsilon,\mu}[\ell(Z)]=\inf_{\alpha\in\mathbb{R}}\alpha+ \frac{1}{1-\varepsilon}\,\mathbb{E}_{Z\sim\mu}\,\big{[}[\ell(Z)-\alpha]_{+} \big{]}.\]

CVaR is also known as expected shortfall and is equivalent to the conditional expectation of \(\ell(Z)\), given that it is above an \(\varepsilon\) threshold. This concept is often used in finance to evaluate the market risk of a portfolio. With this definition, the result of Proposition 6 can be written as

\[\sup_{\begin{subarray}{c}\nu\in\mathcal{G}_{2}(\sigma,z_{0}):\\ \mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}_{n}\|\nu)\leq\rho\end{subarray}} \mathbb{E}_{\nu}[\ell]=\inf_{\lambda_{1},\lambda_{2}\in\mathbb{R}_{+}}\lambda _{1}\sigma^{2}+\lambda_{2}\rho^{p}+\mathrm{CVaR}_{1-\varepsilon,\tilde{\mu}_{n }}\Bigg{[}\sup_{\xi\in\mathcal{Z}}\,\ell(\xi)-\lambda_{1}\|\xi-z_{0}\|^{2}- \lambda_{2}\|\xi-Z\|^{p}\Bigg{]}.\]

When \(\varepsilon\to 0\) and \(\sigma\to\infty\), whence CVaR reduces to expected value and the constrained class \(\mathcal{G}_{2}(\sigma,z_{0})\) expands to \(\mathcal{P}(\mathcal{Z})\), the dual formulation above reduces to that of classical WDRO [8, 21].

Evaluating \(\bar{\ell}\) requires solving a maximization problem, which could be in itself challenging. To overcome this, we impose additional convexity assumptions, which are standard for WDRO [35, 43].

**Assumption 2** (Convexity condition).: The loss \(\ell\) is a pointwise maximum of finitely many concave functions, i.e., \(\ell(\xi)=\max_{j\in[J]}\ell_{j}(\xi)\), for some \(J\in\mathbb{N}\), where \(\ell_{j}\) is real-valued, l.s.c., and concave9. The set \(\mathcal{Z}\) is closed and convex. The atoms of \(\tilde{\mu}_{n}\) are in the relative interior of \(\mathcal{Z}\).

Footnote 9: Generally, any continuous function can be approximated arbitrarily well by a maximum of finitely many concave functions. However, the number of functions needed may be arbitrarily large in general. Fortunately, some losses like the \(\ell_{\infty}\)-norm \(\|z\|_{\infty}\!=\!\max_{i\in[d],a\in\{\pm 1\}}\sigma z_{i}\) require only \(\mathrm{poly}(d)\) pieces.

**Theorem 2** (Convex reformulation).: _Under Assumption 1, for any \(\ell\in\mathcal{L}\) satisfying Assumption 2 and \(z_{0}\in\mathbb{R}^{d}\), we have_

\[\sup_{\begin{subarray}{c}\nu\in\mathcal{G}_{q}(\sigma,z_{0}):\\ \mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}_{n}\|\nu)\leq\rho\end{subarray}} \mathbb{E}_{\nu}[\ell]=\left\{\begin{array}{ll}\inf&\lambda_{1}\sigma^{2}+ \lambda_{2}\rho^{p}+\alpha+\frac{1}{n(1-\varepsilon)}\sum_{i\in[n]}s_{i}\\ \mathrm{s.t.}&\alpha\!\in\!\mathbb{R},\lambda_{1},\lambda_{2}\!\in\!\mathbb{R}_ {+},s,\tau_{ij}\!\in\!\mathbb{R}_{n}^{\varepsilon},\zeta_{ij}^{\ell},\zeta_{ij }^{\ell},\zeta_{ij}^{\mathsf{W}},\zeta_{ij}^{\mathcal{Z}}\!\in\!\mathbb{R}^{d },\,\forall i\in[n],\forall j\in[J]\\ &s_{i}\geq(-\ell_{j})^{*}(\zeta_{ij}^{\ell})+z_{0}^{\top}\zeta_{ij}^{\mathcal{G} }+\tau_{ij}\\ &\qquad\qquad+\tilde{Z}_{i}^{\top}\zeta_{ij}^{\mathsf{W}}+P_{h}(\zeta_{ij}^{ \mathsf{W}},\lambda_{2})+\chi_{\mathcal{Z}}^{\star}(\zeta_{ij}^{\mathcal{Z}})- \alpha,&\forall i\in[n],\forall j\in[J]\\ &\zeta_{ij}^{\ell}+\zeta_{ij}^{\mathcal{G}}+\zeta_{ij}^{\mathsf{W}}+\zeta_{ij}^{ \mathcal{Z}}=0,\;\|\zeta_{ij}^{\mathcal{G}}\|^{2}\leq\lambda_{1}\tau_{ij},& \forall i\in[n],\forall j\in[J],\end{array}\right.\]

_where \(P_{h}\) is the perspective function (i.e., \(P_{h}(\zeta,\lambda)=\lambda h(\zeta/\lambda)\)) of_\[h(\zeta)\coloneqq\begin{cases}\chi_{\{z\in\mathbb{R}^{d}:\|z\|\leq 1\}}(\zeta),&p=1 \\ \frac{(p-1)^{p-1}}{p^{p}}\|\zeta\|^{\frac{p}{p-1}},&p>1.\end{cases}\] (6)

The minimization problem in Theorem 2 is a finite-dimensional convex program. In Section 5, we use this result in conjunction with Proposition 5 to efficiently perform outlier-robust WDRO.

We conclude this section by characterizing the worst-case distribution, i.e., the optimal adversarial strategy, for our outlier-robust WDRO problem. To that end, we need the primal formulation below.

**Theorem 3** (Worst-case distribution).: _Under Assumption 1, for any \(\ell\in\mathcal{L}\) satisfying Assumption 2 and \(z_{0}\in\mathbb{R}^{d}\), we have_

\[\sup_{\begin{subarray}{c}\nu\in\mathcal{G}_{d}(\sigma_{z_{0}}):\\ W_{p}^{\star}(\hat{\mu}_{n}\|\nu)\leq\rho\end{subarray}}\mathbb{E}_{\nu}[\ell]= \left\{\begin{array}{ll}\max&-\sum_{(i,j)\in[n]\times[J]}P_{-\ell_{j}}(\xi_{ ij},q_{ij})\\ \mathrm{s.t.}&q_{ij}\in\mathbb{R}_{+},\,\xi_{ij}\in q_{ij}\cdot\mathcal{Z}& \forall i\in[n],\forall j\in[J]\\ &\sum_{j\in[J]}q_{ij}\leq\frac{1}{n(1-\varepsilon)}&\forall i\in[n]\\ &\sum_{(i,j)\in[n]\times[J]}q_{ij}=1\\ &\sum_{(i,j)\in[n]\times[J]}P_{\|\cdot\|^{p}}(\xi_{ij}-q_{ij}\tilde{Z}_{i},q_{ ij})\leq\rho\\ &\sum_{(i,j)\in[n]\times[J]}P_{\|\cdot\|^{2}}(\xi_{ij}-q_{ij}z_{0},q_{ij})\leq \sigma^{2}\end{array}\right.\]

_The discrete distribution \(\nu^{\star}=\sum_{(i,j)\in\mathcal{Q}}q_{ij}^{\star}\delta_{\xi_{ij}^{\star} /q_{ij}}\) achieves the worst-case expectation on the left-hand side, where \((q_{ij}^{\star},\xi_{ij}^{\star})_{(i,j)\in[n]\times[J]}\) are optimizers of the maximization problem on the right and \(\mathcal{Q}:=\{(i,j)\in[n]\times[J]:q_{ij}^{\star}>0\}\)._

The maximization problem from Theorem 3 is the conjugate dual of the minimization in Theorem 2. Subsequently, we propose a systematic approach for constructing a discrete distribution based on a solution derived from the maximization problem that achieves the worst-case expected loss.

**Remark 4** (Comparison to WDRO worst-case distribution).: Recall that our robust WDRO approach reduces to the classic WDRO approach as \(\varepsilon=0\) and \(\sigma\to\infty\). Consequently, this implies that the constraints \(\sum_{j\in[J]}q_{ij}\leq 1/(n(1-\varepsilon))\) and \(\sum_{(i,j)\in[n]\times[J]}P_{\|\cdot\|^{2}}(\xi_{ij}-q_{ij}z_{0},q_{ij})\leq\sigma ^{2}\) can be dropped under this specific choice of \(\varepsilon\) and \(\sigma\). As a result, our construction simplifies to the approach presented in [35, Theorem 4.4] for WDRO problems.

**Remark 5** (Parameter tuning).: In practice, \(\varepsilon\), \(\rho_{0}\), and the relevant tail bound may be unknown. Thus, in Appendix F, we consider learning under Setting B with \(\mathcal{G}=\mathcal{G}_{\mathrm{cov}}(\sigma)\) for potentially unknown \(\varepsilon\), \(\sigma\), and \(\rho_{0}\). First, we observe that knowledge of upper bounds on these parameters is sufficient to attain risk bounds scaling in terms of said upper bounds. This approach avoids meticulous parameter tuning but may result in suboptimal risk. To efficiently match our risk bounds with known parameters, we show that it is necessary and sufficient to know \(\rho_{0}\) and at least one of \(\varepsilon\) or \(\sigma\) (up to constant factors).

## 4 Low-Dimensional Features

While Proposition 3 shows that the excess risk bounds from Theorem 1 cannot be improved in general, finer guarantees can be derived when the optimal loss function depends only on \(k\)-dimensional affine features of the data. Defining \(\mathcal{G}^{(k)}\) as the union of the projections \(\{U_{\#}\mu:\mu\in\mathcal{G}\}\) over \(U\in\mathbb{R}^{k\times d}\) with \(UU^{\top}=I_{k}\)10, we improve the excess risk bound of Theorem 1 for this setting.

Footnote 10: If \(\mathcal{G}\) is closed under isometries, like \(\mathcal{G}_{\mathrm{cov}}\), then \(\mathcal{G}^{(k)}=\{U_{\#}\mu:\mu\in\mathcal{G}\}\) for any such \(U\).

**Theorem 4** (Excess risk bound).: _Under Setting A, let \(\hat{\ell}\) minimize (OR-WDRO), and assume that \(\ell_{\star}=\underline{\ell}\circ A\) for an affine map \(A:\mathbb{R}^{d}\to\mathbb{R}^{k}\) and some \(\underline{\ell}:\mathbb{R}^{k}\to\mathbb{R}\). Writing \(c\!=\!2(1\!-\!\varepsilon)^{-1/p}\), we have_

\[\mathbb{E}_{\mu}[\hat{\ell}\,]\!-\!\mathbb{E}_{\mu}[\ell_{\star}]\leq\!\left\{ \begin{array}{ll}\|\ell_{\star}\|_{\mathrm{Lip}}\big{(}c\rho+2\tau_{1}( \mathcal{G}^{(k)},2\varepsilon)\big{)},&p\!=\!1,\ell_{\star}\text{ Lipschitz}\\ \|\ell_{\star}\|_{\hat{H}^{1.2}(\mu)}\big{(}c\rho\!+\!2\tau_{2}(\mathcal{G}^{(k )},2\varepsilon)\big{)}\!+\!\frac{1}{2}\alpha\big{(}c\rho\!+\!2\tau_{2}(\mathcal{ G}^{(k)},2\varepsilon)\big{)}^{2},&p\!=\!2,\ell_{\star}\text{ $\alpha$-smooth}\end{array}\right..\]

This dependence on \(\mathcal{G}^{(k)}\) rather than \(\mathcal{G}=\mathcal{G}^{(d)}\) is a substantial improvement when \(k\ll d\).

**Corollary 4** (Risk bounds for \(\mathcal{G}_{\mathrm{cov}}\)).: _Under the setting of Theorem 4 with \(\mathcal{G}\subseteq\mathcal{G}_{\mathrm{cov}}\), we have_

\[\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}]\lesssim\begin{cases} \left\|\ell_{\star}\right\|_{\mathrm{Lip}}\bigl{(}\rho+\sqrt{k\varepsilon} \,\bigr{)},&p=1,\ell_{\star}\text{ Lipschitz}\\ \left\|\ell_{\star}\right\|_{\dot{H}^{1,2}(\mu)}(\rho+\sqrt{k}\,)+\alpha(\rho^ {2}+k),&p=2,\ell_{\star}\text{ $\alpha$-smooth}.\end{cases}\]

We again have a matching lower bound for the Lipschitz setting, this time using \(k\)-variate regression.

**Proposition 7** (Lower bound).: _Fix \(\mathcal{Z}=\mathbb{R}^{d}\) and \(\varepsilon\in[0,0.49]\). For any \(L\geq 0\), there exists a family \(\mathcal{L}\subseteq\mathrm{Lip}_{L}(\mathbb{R}^{d})\), independent of \(\varepsilon\), such that each \(\ell\in\mathcal{L}\) decomposes as \(\ell=\ell\circ A\) for \(A\in\mathbb{R}^{k\times d}\) and \(\underline{\ell}:\mathbb{R}^{k}\to\mathbb{R}\), and such that for any decision rule \(\mathsf{D}:\mathcal{P}(\mathcal{Z})\to\mathcal{L}\) there exists a pair \((\mu,\tilde{\mu})\in\mathcal{G}_{\mathrm{cov}}\times\mathcal{P}(\mathcal{Z})\) with \(\mathsf{W}_{1}^{\varepsilon}(\tilde{\mu},\mu)\leq\rho\) satisfying \(\mathbb{E}_{\mu}[\mathsf{D}(\tilde{\mu})]-\inf_{\ell\in\mathcal{L}}\mathbb{E }_{\mu}[\ell]\gtrsim L\bigl{(}\rho+\sqrt{k\varepsilon}\,\bigr{)}\)._

For computation, we turn to a slightly modified \(n\)-sample contamination model. Our analysis for the low-dimensional case only supports additive TV corruptions (sometimes called Huber contamination).

**Setting \(\mathsf{B}^{\prime}\):** Fix \(\rho,\varepsilon,\mathcal{L},\mathcal{G}\) as in Setting A, and fix \(m=\lceil(1-\varepsilon)n\rceil\) for some \(n\in\mathbb{N}\). Let \(Z_{1},\ldots,Z_{m}\) be drawn i.i.d. from \(\mu\in\mathcal{G}\), with empirical measure \(\hat{\mu}_{m}=\frac{1}{m}\sum_{i=1}^{m}\delta_{Z_{i}}\). Upon observing these clean samples, Nature applies a \(\mathsf{W}_{p}\) perturbation of size \(\rho_{0}\), producing \(\{Z_{i}^{\prime}\}_{i=1}^{m}\) with empirical measure \(\mu_{m}^{\prime}\) such that \(\mathsf{W}_{p}(\hat{\mu}_{m},\mu_{m}^{\prime})\leq\rho_{0}\). Finally, Nature adds \(\lfloor\varepsilon n\rfloor\) samples to obtain \(\{\tilde{Z}_{i}\}_{i=1}^{n}\) with empirical measure \(\tilde{\mu}_{n}\) such that \(\mu_{m}^{\prime}\leq\frac{1}{1-\varepsilon}\tilde{\mu}_{n}\). Equivalently, the final dataset satisfies \(\mathsf{W}_{p}^{\varepsilon}(\hat{\mu}_{n}\|\hat{\mu}_{m})\leq\rho_{0}\).

As before, we modify (OR-WDRO) using a centered alternative to \(\mathcal{G}_{\mathrm{cov}}\). Defining \(\mathcal{G}_{\mathrm{cov}}(\sigma,z_{0})\coloneqq\bigl{\{}\mu\in\mathcal{P}( \mathcal{Z}):\mathbb{E}_{\mu}[(Z-z_{0})(Z-z_{0})^{\top}]\preceq\sigma^{2}I_{d} \bigr{\}}\), we consider the outlier-robust WDRO problem

\[\inf_{\ell\in\mathcal{L}}\sup_{\nu\in\mathcal{G}_{\mathrm{cov}}(\sigma,z_{0}) :\mathsf{W}_{p}^{\varepsilon}(\hat{\mu}_{m}\|\nu)\leq\rho}\mathbb{E}_{\nu}[ \ell].\] (7)

To start, we provide a corresponding risk bound which matches Corollary 4 when \(k=O(1)\).

**Proposition 8** (Risk bound for modified problem).: _Consider Setting \(\mathsf{B}^{\prime}\) with \(\mathcal{G}\subseteq\mathcal{G}_{\mathrm{cov}}\), and assume \(\ell_{\star}=\underline{\ell}\circ A\) for affine \(A:\mathbb{R}^{d}\to\mathbb{R}^{k}\) and \(\underline{\ell}:\mathbb{R}^{k}\to\mathbb{R}\). Fix \(z_{0}\in\mathcal{Z}\) such that \(\|z_{0}-\mathbb{E}_{\mu}[Z]\|\leq E=O(\rho_{0}+1)\), and assume \(\mathsf{W}_{p}(\hat{\mu}_{m},\mu)\leq\delta\). If \(\hat{\ell}\) minimizes (7) with \(\rho=\rho_{0}+\delta\) and \(\sigma=1+E\), then_

\[\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}]\lesssim\begin{cases} \left\|\ell_{\star}\right\|_{\mathrm{Lip}}\bigl{(}\sqrt{k}\rho_{0}+\sqrt{k \varepsilon}+\delta\,\bigr{)},&p=1,\ell_{\star}\text{ Lipschitz}\\ \left\|\ell_{\star}\right\|_{\dot{H}^{1,2}(\mu)}\bigl{(}\sqrt{k}\rho_{0}+ \sqrt{k}+\delta\bigr{)}+\alpha\bigl{(}\sqrt{k}\rho_{0}+\sqrt{k}+\delta\bigr{)} ^{2},&p=2,\ell_{\star}\text{ $\alpha$-smooth}\end{cases}.\]

Here, the stronger requirement for the robust mean estimate, the restriction to additive contamination, and the need to optimize over the centered \(\mathcal{G}_{\mathrm{cov}}\) class rather than \(\mathcal{G}_{2}\) all stem from the fact that the resilience term \(\tau_{p}((\mathcal{G}_{\mathrm{cov}})_{k},\varepsilon)\) scales with \(\sqrt{k}\) rather than \(\sqrt{d}\). Fortunately, efficient computation is still possible. First, we employ iterative filtering [15] for dimension-free robust mean estimation.

**Proposition 9** (Refined robust mean estimation).: _Consider Setting \(\mathsf{B}\) or \(\mathsf{B}^{\prime}\) with \(\mathcal{G}=\mathcal{G}_{\mathrm{cov}}\) and \(\varepsilon\leq 1/12\). For \(n=\tilde{\Omega}(d)\), there exists an iterative filtering algorithm which takes \(\tilde{\mu}_{n}\) as input, runs in time \(\tilde{O}(nd^{2})\), and outputs \(z_{0}\in\mathbb{R}^{d}\) such that \(\|z_{0}-\mathbb{E}_{\mu}[Z]\|\lesssim\rho_{0}+1\) with probability at least 0.99._

The analysis requires care when \(p=1\), since \(\mathsf{W}_{1}\) perturbations can arbitrarily increase the initial covariance bound. Fortunately, this increase can be controlled by trimming out a few samples.

Next, we show that computing the inner worst-case expectation in (7) can be simplified into a minimization problem involving only a scalar and a positive semidefinite matrix provided the following assumption holds (which is indeed the case in the setting of Proposition 8).

**Assumption 3** (Slater condition II).: Given the distribution \(\tilde{\mu}_{n}\) and fixed point \(z_{0}\), there exists \(\nu_{0}\in\mathcal{P}(\mathcal{Z})\) such that \(\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}_{n}\|\nu_{0})<\rho\) and \(\mathbb{E}_{\nu_{0}}[(Z-z_{0})(Z-z_{0})^{\top}]\prec\!\sigma^{2}I_{d}\). Further, we require \(\rho\!>\!0\).

**Proposition 10** (Strong duality).: _Under Assumption 3, for any \(\ell\in\mathcal{L}\) and \(z_{0}\in\mathbb{R}^{d}\), we have_

\[\sup_{\begin{subarray}{c}\nu\in\mathcal{G}_{\mathrm{cov}}(\sigma,z_{0}):\\ \mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}_{n}\|\nu)\leq\rho\end{subarray}}\mathbb{E }_{\nu}[\ell]=\inf_{\begin{subarray}{c}\Lambda_{1}\in\mathcal{Q}_{+}^{d}\\ \lambda_{2}\in\mathbb{R}_{+},\alpha\in\mathbb{R}\end{subarray}}-z_{0}^{\top} \Lambda_{1}z_{0}\!+\!\sigma^{2}\operatorname{Tr}[\Lambda_{1}]\!+\!\lambda_{2} \rho^{p}\!+\!\alpha\!+\!\frac{1}{1-\varepsilon}\,\mathbb{E}_{\tilde{\mu}_{n}} \bigl{[}\overline{\ell}(\,\cdot\,;\Lambda_{1},\lambda_{2},\alpha)\bigr{]},\]

_where \(\overline{\ell}(z;\Lambda_{1},\lambda_{2},\alpha)\coloneqq\sup_{\xi\in\mathcal{Z}} \left[\ell(\xi)-\xi^{\top}\Lambda_{1}\xi+2\xi^{\top}\Lambda_{1}z_{0}-\lambda_ {2}\|\xi-z\|^{p}-\alpha\right]_{+}\)._

The minimization problem over the variables \((\Lambda_{1},\lambda_{2},\alpha)\) belongs to the class of stochastic convex optimization problems. As before, we show that under the convexity condition from Assumption 2 we obtain a tractable reformulation that does not involve an extra optimization problem for evaluating \(\overline{\ell}\).

[MISSING_PAGE_EMPTY:10]

## References

* [1] D. Angluin and P. Laird. Learning from noisy examples. _Machine Learning_, 2:343-370, 1988.
* [2] D. Bartl, S. Drapeau, J. Obloj, and J. Wiesel. Robust uncertainty sensitivity analysis. _arXiv preprint arXiv:2006.12022_, 2020.
* [3] A. Ben-Tal and M. Teboulle. An old-new concept of convex risk measures: The optimized certainty equivalent. _Mathematical Finance_, 17(3):449-476, 2007.
* [4] A. Bennouna, R. Lucas, and B. Van Parys. Certified robust neural networks: Generalization and corruption resistance. In _International Conference on Machine Learning_, 2023.
* [5] A. Bennouna and B. Van Parys. Holistic robust data-driven decisions. _arXiv preprint arXiv:2207.09560_, 2022.
* [6] J. Blanchet, P. W. Glynn, J. Yan, and Z. Zhou. Multivariate distributionally robust convex regression under absolute error loss. In _Advances in Neural Information Processing Systems_, 2019.
* [7] J. Blanchet, Y. Kang, and K. Murthy. Robust Wasserstein profile inference and applications to machine learning. _Journal of Applied Probability_, 56(3):830-857, 2019.
* [8] J. Blanchet and K. Murthy. Quantifying distributional model risk via optimal transport. _Mathematics of Operations Research_, 44(2):565-600, 2019.
* [9] J. Blanchet, K. Murthy, and N. Si. Confidence regions in Wasserstein distributionally robust estimation. _Biometrika_, 109(2):295-315, 2022.
* [10] N. H. Bshouty, N. Eiron, and E. Kushilevitz. PAC learning with nasty noise. _Theoretical Computer Science_, 288(2):255-275, 2002.
* [11] R. Chen and I. C. Paschalidis. A robust learning approach for regression models based on distributionally robust optimization. _Journal of Machine Learning Research_, 19(1):517-564, 2018.
* [12] Z. Chen, D. Kuhn, and W. Wiesemann. Data-driven chance constrained programs over Wasserstein balls. _Operations Research (Forthcoming)_, 2022.
* [13] Y. Cheng, I. Diakonikolas, and R. Ge. High-dimensional robust mean estimation in nearly-linear time. In _ACM-SIAM Symposium on Discrete Algorithms_, 2019.
* [14] L. Deng. The MNIST database of handwritten digit images for machine learning research. _IEEE Signal Processing Magazine_, 29(6):141-142, 2012.
* [15] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Being robust (in high dimensions) can be practical. In _International Conference on Machine Learning_, 2017.
* [16] I. Diakonikolas and D. Kane. _Algorithm High-Dimensional Robust Statistics_. Cambridge University Press, 2022.
* [17] A. Esteban-Perez and J. M. Morales. Distributionally robust stochastic programs with side information based on trimmings. _Mathematical Programming_, 195(1-2):1069-1105, 2022.
* [18] R. Gao. Finite-sample guarantees for Wasserstein distributionally robust optimization: Breaking the curse of dimensionality. _Operations Research (Forthcoming)_, 2022.
* [19] R. Gao, X. Chen, and A. J. Kleywegt. Wasserstein distributionally robust optimization and variation regularization. _Operations Research (Forthcoming)_, 2022.
* [20] R. Gao and A. Kleywegt. Distributionally robust stochastic optimization with Wasserstein distance. _Mathematics of Operations Research_, 48(2):603-655, 2023.
* [21] R. Gao and A. Kleywegt. Distributionally robust stochastic optimization with wasserstein distance. _Mathematics of Operations Research_, 48(2):603-655, 2023.

* Gao et al. [2018] R. Gao, L. Xie, Y. Xie, and H. Xu. Robust hypothesis testing using Wasserstein uncertainty sets. In _Advances in Neural Information Processing Systems_, 2018.
* Gurobi Optimization [2023] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. URL https://www.gurobi.com.
* Hashimoto et al. [2018] T. Hashimoto, M. Srivastava, H. Namkoong, and P. Liang. Fairness without demographics in repeated loss minimization. In _International Conference on Machine Learning_, 2018.
* Hiriart-Urruty and Lemarechal [2004] J.-B. Hiriart-Urruty and C. Lemarechal. _Fundamentals of Convex Analysis_. Springer, 2004.
* Hopkins et al. [2020] S. Hopkins, J. Li, and F. Zhang. Robust and heavy-tailed mean estimation made simple, via regret minimization. In _Advances in Neural Information Processing Systems_, 2020.
* Hu et al. [2018] W. Hu, G. Niu, I. Sato, and M. Sugiyama. Does distributionally robust supervised learning give robust classifiers? In _International Conference on Machine Learning_, 2018.
* Huber [1964] P. J. Huber. Robust Estimation of a Location Parameter. _The Annals of Mathematical Statistics_, 35(1):73-101, 1964.
* Jiang and Xie [2023] N. Jiang and W. Xie. Distributionally favorable optimization: A framework for data-driven decision-making with endogenous outliers. _Optimization Online_, 2023.
* Kwon et al. [2020] Y. Kwon, W. Kim, J.-H. Won, and M. C. Paik. Principled learning method for Wasserstein distributionally robust optimization with local perturbations. In _International Conference on Machine Learning_, 2020.
* Lee and Raginsky [2018] J. Lee and M. Raginsky. Minimax statistical learning with Wasserstein distances. In _Advances in Neural Information Processing Systems_, 2018.
* 798, 2020.
* Lofberg [2004] J. Lofberg. YALMIP: A toolbox for modeling and optimization in MATLAB. In _IEEE international conference on robotics and automation_, pages 284-289. IEEE, 2004.
* Lugosi and Mendelson [2021] G. Lugosi and S. Mendelson. Robust multivariate mean estimation: the optimality of trimmed mean. _The Annals of Statistics_, 49(1):393-410, 2021.
* Mohajerin Esfahani and Kuhn [2018] P. Mohajerin Esfahani and D. Kuhn. Data-driven distributionally robust optimization using the Wasserstein metric: Performance guarantees and tractable reformulations. _Mathematical Programming_, 171(1-2):115-166, 2018.
* Nguyen et al. [2022] V. A. Nguyen, D. Kuhn, and P. Mohajerin Esfahani. Distributionally robust inverse covariance estimation: The Wasserstein shrinkage estimator. _Operations Research_, 70(1):490-515, 2022.
* Nguyen et al. [2023] V. A. Nguyen, S. Shafieezadeh-Abadeh, D. Kuhn, and P. Mohajerin Esfahani. Bridging Bayesian and minimax mean square error estimation via Wasserstein distributionally robust optimization. _Mathematics of Operations Research_, 48(1):1-37, 2023.
* Nietert et al. [2022] S. Nietert, R. Cummings, and Z. Goldfeld. Outlier-robust optimal transport: duality, structure, and statistical analysis. In _International Conference on Artificial Intelligence and Statistics_, 2022.
* Nietert et al. [2023] S. Nietert, R. Cummings, and Z. Goldfeld. Robust estimation under the Wasserstein distance. _arXiv preprint arXiv:2302.01237_, 2023.
* Pflug and Wozabal [2007] G. Pflug and D. Wozabal. Ambiguity in portfolio selection. _Quantitative Finance_, 7(4):435-442, 2007.
* Ronchetti and Huber [2009] E. M. Ronchetti and P. J. Huber. _Robust Statistics_. John Wiley & Sons Hoboken, 2009.
* Santambrogio [2015] F. Santambrogio. _Optimal Transport for Applied Mathematicians_. Springer, 2015.

* [43] S. Shafieezadeh-Abadeh, L. Aolaritei, F. Dorfler, and D. Kuhn. New perspectives on regularization and computation in optimal transport-based distributionally robust optimization. _arXiv preprint arXiv:2303.03900_, 2023.
* [44] S. Shafieezadeh-Abadeh, D. Kuhn, and P. Mohajerin Esfahani. Regularization via mass transportation. _Journal of Machine Learning Research_, 20(103):1-68, 2019.
* [45] S. Shafieezadeh-Abadeh, P. Mohajerin Esfahani, and D. Kuhn. Distributionally robust logistic regression. In _Advances in Neural Information Processing Systems_, 2015.
* [46] S. Shafieezadeh-Abadeh, V. A. Nguyen, D. Kuhn, and P. Mohajerin Esfahani. Wasserstein distributionally robust Kalman filtering. In _Advances in Neural Information Processing Systems_, 2018.
* [47] A. Shapiro. On duality theory of conic linear problems. In M. A. Goberna and M. A. Lopez, editors, _Semi-Infinite Programming_, pages 135-165. Kluwer Academic Publishers, 2001.
* [48] A. Sinha, H. Namkoong, and J. Duchi. Certifying some distributional robustness with principled adversarial training. In _International Conference on Learning Representations_, 2018.
* [49] J. Steinhardt, M. Charikar, and G. Valiant. Resilience: A criterion for learning in the presence of arbitrary outliers. In _Innovations in Theoretical Computer Science Conference_, 2018.
* [50] J. F. Sturm. Using SeDuMi 1.02, A MATLAB toolbox for optimization over symmetric cones. _Optimization Methods and Software_, 11(1-4):625-653, 1999.
* [51] Z. Tu, J. Zhang, and D. Tao. Theoretical analysis of adversarial learning: A minimax approach. In _Advances in Neural Information Processing Systems_, 2019.
* [52] C. Villani. _Topics in Optimal Transportation_. American Mathematical Society, 2003.
* [53] R. Volpi, H. Namkoong, O. Sener, J. Duchi, V. Murino, and S. Savarese. Generalizing to unseen domains via adversarial data augmentation. In _Advances in Neural Information Processing Systems_, pages 5339-5349, 2018.
* [54] Y. Wang, X. Ma, J. Bailey, J. Yi, B. Zhou, and Q. Gu. On the convergence and robustness of adversarial training. In _International Conference on Machine Learning_, 2019.
* [55] W. Xie. On distributionally robust chance constrained programs with Wasserstein distance. _Mathematical Programming_, 186(1):115-155, 2021.
* [56] R. Zhai, C. Dan, Z. Kolter, and P. Ravikumar. DORO: Distributional and outlier robust optimization. In _International Conference on Machine Learning_, 2021.
* [57] J. Zhen, D. Kuhn, and W. Wiesemann. A unified theory of robust and distributionally robust optimization via the primal-worst-equals-dual-best principle. _Operations Research (Forthcoming)_, 2023.
* 2283, 2022.
* [59] S. Zhu, L. Xie, M. Zhang, R. Gao, and Y. Xie. Distributionally robust weighted k-nearest neighbors. In _Advances in Neural Information Processing Systems_, 2022.

Preliminary Results on Robust OT and Wasserstein DRO

We first recall properties of the robust Wasserstein distance \(\mathsf{W}_{p}^{\varepsilon}\) which will be used throughout the supplement. To start, we show that our definition coincides with another based on partial OT, considered in [39]. In what follows, we fix \(p\geq 1\), write \(c\mathcal{P}(\mathcal{Z}):=\{c\mu:\mu\in\mathcal{P}(\mathcal{Z})\}\), and, for \(\mu,\nu\in c\mathcal{P}(\mathcal{Z})\), we define \(\Pi(\mu,\nu):=c\Pi(\mu/c,\nu/c)\) and \(\mathsf{W}_{p}(\mu,\nu)^{p}:=c\mathsf{W}_{p}(\mu/c,\nu/c)\).

**Lemma 1** (\(\mathsf{W}_{p}^{\varepsilon}\) as partial OT).: _Fix \(\varepsilon\in[0,1]\). For any \(\mu,\nu\in\mathcal{P}(\mathcal{Z})\), we have_

\[\mathsf{W}_{p}^{\varepsilon}(\mu,\nu)\coloneqq\inf_{\begin{subarray}{c}\mu^{ \prime}\in\mathcal{P}(\mathcal{Z})\\ \|\mu^{\prime}-\mu\|_{\mathsf{TV}}\leq\varepsilon\end{subarray}}\mathsf{W}_{p }(\mu^{\prime},\nu)=\inf_{\begin{subarray}{c}\nu^{\prime}\in\mathcal{P}( \mathcal{Z})\\ \|\nu^{\prime}-\nu\|_{\mathsf{TV}}\leq\varepsilon\end{subarray}}\mathsf{W}_{p }(\mu,\nu^{\prime})=\inf_{\begin{subarray}{c}\mu^{\prime},\nu^{\prime}\in(1- \varepsilon)\mathcal{P}(\mathcal{Z})\\ \mu^{\prime}\leq\mu,\,\nu^{\prime}\leq\nu\end{subarray}}\mathsf{W}_{p}(\mu^{ \prime},\nu^{\prime}).\]

Proof.: We write \(\underline{\mathsf{W}}_{p}^{\varepsilon}(\mu,\nu)\) for the rightmost expression; this is definition of \(\mathsf{W}_{p}^{\varepsilon}\) considered in [39]. We first show that \(\underline{\mathsf{W}}_{p}^{\varepsilon}(\mu,\nu)\leq\mathsf{W}_{p}^{ \varepsilon}(\mu,\nu)\). Fix any \(\mu^{\prime}\) feasible for the \(\mathsf{W}_{p}^{\varepsilon}\) problem. Then, by the approximate triangle inequality for \(\underline{\mathsf{W}}_{p}^{\varepsilon}\) (Proposition 3 of [39]), we have

\[\underline{\mathsf{W}}_{p}^{\varepsilon}(\mu,\nu)\leq\underline{\mathsf{W}}_{p }^{\varepsilon}(\mu,\mu^{\prime})+\mathsf{W}_{p}(\mu^{\prime},\nu)\leq\mathsf{ W}_{p}(\mu^{\prime},\nu).\]

Indeed, writing \(c\coloneqq(\mu\wedge\mu^{\prime})(\mathcal{Z})\geq 1-\varepsilon\), the last inequality uses that \(\underline{\mathsf{W}}_{p}^{\varepsilon}(\mu,\mu^{\prime})\leq\mathsf{W}_{p}( \frac{1-\varepsilon}{c}\mu\wedge\mu^{\prime},\frac{1-\varepsilon}{c}\mu\wedge \mu^{\prime})=0\). Infimizing over feasible \(\mu^{\prime}\) gives that \(\underline{\mathsf{W}}_{p}^{\varepsilon}(\mu,\nu)\leq\mathsf{W}_{p}^{ \varepsilon}(\mu,\nu)\).

For the other direction, take any \(\mu^{\prime},\nu^{\prime}\) feasible for the \(\underline{\mathsf{W}}_{p}^{\varepsilon}\) problem. Let \(\pi\in\Pi(\mu^{\prime},\nu^{\prime})\) be any optimal coupling for the \(\mathsf{W}_{p}(\mu^{\prime},\nu^{\prime})\) problem, and write \(\mu^{\prime\prime}=\mu^{\prime}+(\nu-\nu^{\prime})\in\mathcal{P}(\mathcal{Z})\). Defining the coupling \(\pi^{\prime}=\pi+(\operatorname{Id},\operatorname{Id})_{\#}(\nu-\nu^{\prime}) \in\Pi(\mu^{\prime\prime},\nu)\), we compute

\[\mathsf{W}_{p}(\mu^{\prime\prime},\nu)^{p}\leq\int_{\mathcal{Z}\times \mathcal{Z}}\|x-y\|^{p}\,\mathrm{d}\pi^{\prime}(x,y)=\int_{\mathcal{Z}\times \mathcal{Z}}\|x-y\|^{p}\,\mathrm{d}\pi(x,y)=\mathsf{W}_{p}(\mu^{\prime},\nu^{ \prime}).\]

By construction, \(\|\mu^{\prime\prime}-\mu\|_{\mathsf{TV}}\leq\varepsilon\), and so \(\mathsf{W}_{p}^{\varepsilon}(\mu,\nu)\leq\mathsf{W}_{p}(\mu^{\prime},\nu^{ \prime})\). Infimizing over feasible \(\mu^{\prime},\nu^{\prime}\) gives that \(\mathsf{W}_{p}^{\varepsilon}(\mu,\nu)\leq\underline{\mathsf{W}}_{p}^{ \varepsilon}(\mu,\nu)\). 

We thus inherit several results for \(\underline{\mathsf{W}}_{p}\) given in [39].

**Lemma 2** (Approximate triangle inequality [39]).: _If \(\mu,\nu,\kappa\in\mathcal{P}(\mathcal{Z})\) and \(\varepsilon_{1},\varepsilon_{2}\in[0,1]\), then_

\[\mathsf{W}_{p}^{\varepsilon_{1}+\varepsilon_{2}}(\mu,\nu)\leq\mathsf{W}_{p}^{ \varepsilon_{1}}(\mu,\kappa)+\mathsf{W}_{p}^{\varepsilon_{2}}(\kappa,\nu).\]

**Lemma 3** (\(\mathsf{W}_{p}^{\varepsilon}\) modulus of continuity, [39], Lemma 3).: _For any \(\mathcal{G}\subseteq\mathcal{P}(\mathcal{Z})\), we have_

\[\sup_{\begin{subarray}{c}\alpha,\beta\in\mathcal{G}\\ \mathsf{W}_{p}^{\varepsilon}(\alpha,\beta)\leq\rho\end{subarray}}\mathsf{W}_{p} (\alpha,\beta)\leq(1-\varepsilon)^{-1/p}\rho+2\tau_{p}(\mathcal{G},\varepsilon).\]

**Lemma 4** (One-sided vs. two-sided \(\mathsf{W}_{p}^{\varepsilon}\)).: _For \(\mu,\nu\in\mathcal{P}(\mathcal{Z})\), we have_

\[\mathsf{W}_{p}^{\varepsilon}(\mu\|\nu)\leq(1-\varepsilon)^{-1/p}\mathsf{W}_{p} ^{\varepsilon}(\mu,\nu)+\tau_{p}(\nu,\varepsilon).\]

Proof.: Fix any \(\mu^{\prime},\nu^{\prime}\in(1-\varepsilon)\mathcal{P}(\mathcal{Z})\) with \(\mu^{\prime}\leq\mu\) and \(\nu^{\prime}\leq\nu\). By design, we have

\[\mathsf{W}_{p}^{\varepsilon}(\mu\|\nu) \leq\mathsf{W}_{p}\big{(}\tfrac{1}{1-\varepsilon}\mu^{\prime}, \tfrac{1}{1-\varepsilon}\nu^{\prime}\big{)}+\mathsf{W}_{p}\big{(}\tfrac{1}{1- \varepsilon}\nu^{\prime},\nu\big{)}\] \[=(1-\varepsilon)^{-1/p}\mathsf{W}_{p}(\mu^{\prime},\nu^{\prime})+ \tau_{p}(\nu,\varepsilon)\]

Infimizing over \(\mu^{\prime}\) and \(\nu^{\prime}\) and applying Lemma 1 gives the lemma. 

Next, we specify explicit constants for the \(\mathsf{W}_{p}\) resilience of \(\mathcal{G}_{\mathrm{cov}}\). Our analysis goes through the related notion of mean resilience [49], defined by \(\tau(\mu,\varepsilon)=\sup_{\mu^{\prime}\in\mathcal{P}(\mathcal{Z});\mu^{ \prime}\leq\frac{1}{1-\varepsilon}\mu}\|\,\mathbb{E}_{\mu^{\prime}}[Z]- \mathbb{E}_{\mu}[Z]\|\). We say that \(Z\sim\mu\in\mathcal{P}(\mathcal{Z})\) is \((\tau_{0},\varepsilon)\)-resilient in mean or under \(\mathsf{W}_{p}\) if \(\mu\in\tau(\tau_{0},\varepsilon)\) or \(\mu\in\tau_{p}(\tau_{0},\varepsilon)\).

**Lemma 5** (\(\mathsf{W}_{p}\) resilience for \(\mathcal{G}_{2}\) and \(\mathcal{G}_{\mathrm{cov}}\)).: _Fix \(\varepsilon\in[0,1)\) and \(\sigma\geq 0\). For \(1\leq p\leq 2\), we have \(\tau_{p}(\mathcal{G}_{2}(\sigma),\varepsilon)\leq 4\sigma\varepsilon^{1/p-1/2}(1- \varepsilon)^{-1/p}\). Moreover, we have \(\tau_{p}(\mathcal{G}_{\mathrm{cov}}(\sigma),\varepsilon)\leq\tau_{p}( \mathcal{G}_{2}(\sqrt{d}\sigma),\varepsilon)\)._Proof.: Fix any \(Z\sim\mu\in\mathcal{G}_{2}(\sigma,z_{0})\). By definition, we have \(\mathbb{E}[(\|Z-z_{0}\|^{p})^{2/p}]=\mathbb{E}[\|Z-z_{0}\|^{2}]\leq\sigma^{2}=( \sigma^{p})^{2/p}\). Thus, standard bounds (e.g., Lemma E.2 of [58]) give that \(\|Z-z_{0}\|^{p}\) is \((\sigma^{p}\varepsilon^{1-p/2}(1-\varepsilon)^{-1},\varepsilon)\)-resilient in mean. By Lemma 7 of [39], we thus have that \(Z\) is \((2\sigma\varepsilon^{1/p-1/2}(1-\varepsilon)^{-1/p}+2\varepsilon^{1/p}\sigma, \varepsilon)\)-resilient under \(\mathsf{W}_{p}\). This gives the first result. For the second, we observe that for \(Z\sim\mu\in\mathcal{G}_{\mathrm{cov}}(\sigma)\), we have \(\mathbb{E}[\|Z-\mathbb{E}[Z]\|^{2}]=\mathrm{tr}(\Sigma_{\mu})\leq\sqrt{d}\sigma\). 

Lastly, we turn to the \(\mathsf{W}_{p}\) regularizer.

**Lemma 6** (Controlling \(\mathsf{W}_{p}\) regularizer, [18], Lemmas 1 and 2).: _For any \(\ell\in\mathcal{L}\), we have \(\mathcal{R}_{\nu,1}(\rho;\ell)\leq\rho\|\ell\|_{\mathrm{Lip}}\), with equality if \(\ell\) is convex. For \(\alpha\)-smooth \(\ell\), we have \(|\mathcal{R}_{\nu,2}(\rho;\ell)-\rho\|\ell\|_{\dot{H}^{1,2}(\nu)}|\leq\frac{1} {2}\alpha\rho^{2}\)._

The factor of 1/2 under smoothness was not present in Lemma 2 of [18], since the proof only used that \(|\ell(z)-\ell(z_{0})-\nabla(z_{0})^{\top}(z-z_{0})|\leq\alpha\|z-z_{0}\|^{2}\), instead of the tight upper bound of \(\frac{\alpha}{2}\|z-z_{0}\|^{2}\). This quantity naturally bounds the excess risk of standard WDRO.

**Lemma 7** (WDRO excess risk bound).: _Under Setting \(A\) with \(\varepsilon=0\), the standard WDRO estimate \(\hat{\ell}=\operatorname*{argmin}_{\ell\in\mathcal{L}}\sup_{\nu\in\mathcal{P}( \mathcal{Z}):\mathsf{W}_{p}(\tilde{\mu},\nu)\leq\rho}\mathbb{E}_{\nu}[\ell]\) satisfies \(\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}]\leq\mathcal{R}_{ \mu,p}(2\rho;\ell_{\star})\)._

Proof.: We bound

\[\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}] \leq\sup_{\begin{subarray}{c}\nu\in\mathcal{P}(\mathcal{Z})\\ \mathsf{W}_{p}(\nu,\tilde{\mu})\leq\rho\end{subarray}}\mathbb{E}_{\nu}[\hat{ \ell}]-\mathbb{E}_{\mu}[\ell_{\star}]\] ( \[\mathsf{W}_{p}(\mu,\tilde{\mu})\leq\rho\] ) \[\leq\sup_{\begin{subarray}{c}\nu\in\mathcal{P}(\mathcal{Z})\\ \mathsf{W}_{p}(\nu,\tilde{\mu})\leq\rho\end{subarray}}\mathbb{E}_{\nu}[\ell_{ \star}]-\mathbb{E}_{\mu}[\ell_{\star}]\] (optimality of \[\hat{\ell}\] ) \[\leq\sup_{\begin{subarray}{c}\nu\in\mathcal{P}(\mathcal{Z})\\ \mathsf{W}_{p}(\nu,\tilde{\mu})\leq 2\rho\end{subarray}}\mathbb{E}_{\nu}[\ell_{ \star}]-\mathbb{E}_{\mu}[\ell_{\star}]\] ( \[\mathsf{W}_{p}\] triangle inequality) \[=\mathcal{R}_{\mu,p}(2\rho;\ell_{\star}),\]

as desired. 

Note that this bound does not incorporate the distributional assumptions encoded by \(\mathcal{G}\).

## Appendix B Proofs for Section 3

### Proof of Theorem 1

We compute

\[\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell] \leq\sup_{\begin{subarray}{c}\nu\in\mathcal{G}\\ \mathsf{W}_{p}^{\mathsf{e}}(\tilde{\mu},\nu)\leq\rho\end{subarray}}\mathbb{E}_ {\nu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell]\] ( \[\mu\in\mathcal{G}\], \[\mathsf{W}_{p}^{\mathsf{e}}(\tilde{\mu},\mu)\leq\rho\] ) \[\leq\sup_{\begin{subarray}{c}\nu\in\mathcal{G}\\ \mathsf{W}_{p}^{\mathsf{e}}(\tilde{\mu},\nu)\leq\rho\end{subarray}}\mathbb{E}_ {\nu}[\ell]-\mathbb{E}_{\mu}[\ell]\] (Lemma 2) \[\leq\sup_{\begin{subarray}{c}\nu\in\mathcal{G}\\ \mathsf{W}_{p}(\nu,\mu)\leq\varepsilon\rho+2\tau_{p}(\mathcal{G},2\varepsilon) \end{subarray}}\mathbb{E}_{\nu}[\ell]-\mathbb{E}_{\mu}[\ell]\] (Lemma 3) \[\leq\sup_{\begin{subarray}{c}\nu\in\mathcal{P}(\mathcal{Z})\\ \mathsf{W}_{p}(\nu,\mu)\leq\varepsilon\rho+2\tau_{p}(\mathcal{G},2\varepsilon) \end{subarray}}\mathbb{E}_{\nu}[\ell]-\mathbb{E}_{\mu}[\ell]\] ( \[\mathcal{G}\subseteq\mathcal{P}(\mathcal{Z})\] ) \[=\mathcal{R}_{\mu,p}\big{(}c\rho+2\tau_{p}(\mathcal{G},2\varepsilon );\ell\big{)}.\]

Combining this bound with Lemma 6 gives the theorem.

### Proof of Corollary 1

The corollary follows as an immediate consequence of Theorem 1 and the resilience bounds of Proposition 2.

### Proof of Corollary 2

The corollary follows as an immediate consequence of Theorem 1 and the resilience bound \(\tau_{p}(\mathcal{G}_{\mathrm{subG}},\varepsilon)\lesssim\sqrt{d+p+\log\frac{1}{ \varepsilon}}\,\varepsilon^{1/p}\) established in [39, Theorem 2].

### Proof of Proposition 3

For ease of presentation, suppose \(d=2m\) is even. Consider \(\mathbb{R}^{d}\) as \(\mathbb{R}^{m}\times\mathbb{R}^{m}\), fix \(w\in\mathbb{R}^{m}\) with \(\|w\|=\rho\), and let \(\mathcal{L}\) consist of the following loss functions:

\[\ell_{+,0}(x,y) \coloneqq L\|x+y\|\] \[\ell_{-,0}(x,y) \coloneqq L\|x-y\|\] \[\ell_{+,1}(x,y) \coloneqq L\|x+y-w\|\] \[\ell_{-,1}(x,y) \coloneqq L\|x-y+w\|.\]

Fixing corrupted measure \(\tilde{\mu}=\delta_{0}\), we consider the following candidates for the clean measure \(\mu\):

\[\mu_{+,0} \coloneqq(1-\varepsilon)\delta_{0}+\varepsilon(\mathrm{Id},- \mathrm{Id})_{\#}\mathcal{N}(0,I_{d}/\varepsilon)\] \[\mu_{-,0} \coloneqq(1-\varepsilon)\delta_{0}+\varepsilon(\mathrm{Id},+ \mathrm{Id})_{\#}\mathcal{N}(0,I_{d}/\varepsilon)\] \[\mu_{+,1} \coloneqq(1-\varepsilon)\delta_{(0,w)}+\varepsilon(\mathrm{Id},- \mathrm{Id}+w)_{\#}\mathcal{N}(0,I_{d}/\varepsilon)\] \[\mu_{-,1} \coloneqq(1-\varepsilon)\delta_{(0,w)}+\varepsilon(\mathrm{Id}, \mathrm{Id}+w))_{\#}\mathcal{N}(0,I_{d}/\varepsilon),\]

where \(\mathrm{Id}:x\mapsto x\) is the identity map. By design, \(\mathsf{W}^{\varepsilon}_{1}(\tilde{\mu}\|\mu_{+,0}),\mathsf{W}^{\varepsilon} _{1}(\tilde{\mu}\|\mu_{-,0}),\mathsf{W}^{\varepsilon}_{1}(\tilde{\mu}\|\mu_{+, 1})\), and \(\mathsf{W}^{\varepsilon}_{1}(\tilde{\mu}\|\mu_{-,1})\) are all at most \(\rho\) and \(\mu_{+,0},\mu_{-,0},\mu_{+,1},\mu_{-,1}\in\mathcal{G}_{\mathrm{cov}}\). Moreover,

\[\mathbb{E}_{\mu_{+,0}}[\ell_{+,0}] =\mathbb{E}_{\mu_{-,0}}[\ell_{-,0}]=\mathbb{E}_{\mu_{+,1}}[\ell_{ +,1}]=\mathbb{E}_{\mu_{-,1}}[\ell_{-,1}]=0\] \[\mathbb{E}_{\mu_{+,0}}[\ell_{-,0}] =\mathbb{E}_{\mu_{-,0}}[\ell_{+,0}]=\mathbb{E}_{\mu_{+,1}}[\ell_{ -,1}]=\mathbb{E}_{\mu_{-,1}}[\ell_{+,1}]=2L\varepsilon\,\mathbb{E}_{Z\sim \mathcal{N}(0,I_{d}/\varepsilon)}[\|Z\|]\gtrsim L\sqrt{d\varepsilon}\] \[\mathbb{E}_{\mu_{+,0}}[\ell_{+,1}] =\mathbb{E}_{\mu_{+,1}}[\ell_{+,0}]=\mathbb{E}_{\mu_{-,0}}[\ell_{ -,1}]=\mathbb{E}_{\mu_{-,1}}[\ell_{-,0}]=L\|w\|=L\rho.\]

Thus, for any \(\hat{\ell}=\mathsf{D}(\tilde{\mu})\in\mathcal{L}\), there exists \(\mu\in\{\mu_{+,0},\mu_{-,0},\mu_{+,1},\mu_{-,1}\}\) such that

\[\mathbb{E}_{\mu}[\hat{\ell}]-\inf_{\ell\in\mathcal{L}}\mathbb{E}_{\mu}[\ell]= \mathbb{E}_{\mu}[\hat{\ell}]\gtrsim L\max\{\rho,\sqrt{d\varepsilon}\}\asymp L \big{(}\rho+\sqrt{d\varepsilon}\big{)}.\qed\]

### Proof of Proposition 4

Since \(\mu\in\mathcal{G}_{\mathrm{cov}}\), we have

\[\mathbb{E}_{\mu}\big{[}\|Z-z_{0}\|^{2}\big{]}^{\frac{1}{2}} \leq\mathbb{E}_{\mu}[\|Z-\mathbb{E}_{\mu}[Z]\|^{2}]^{\frac{1}{2}}+ \|\,\mathbb{E}_{\mu}[Z]-z_{0}\|\] \[=\mathrm{tr}(\Sigma_{\mu})^{\frac{1}{2}}+\|\,\mathbb{E}_{\mu}[Z]-z _{0}\|\] \[\leq\sqrt{d}+\|\,\mathbb{E}_{\mu}[Z]-z_{0}\|\leq\sigma.\]

Consequently, we have \(\mu\in\mathcal{G}_{2}(\sigma,z_{0})\). Next, we bound

\[\mathsf{W}^{\varepsilon}_{p}(\tilde{\mu}_{n}\|\mu) \coloneqq\inf_{\begin{subarray}{c}\nu\in\mathcal{P}(\mathcal{Z}) \\ \nu\leq\frac{1}{1-\varepsilon}\tilde{\mu}_{n}\end{subarray}}\mathsf{W}_{p}(\nu,\mu)\] \[\leq\inf_{\begin{subarray}{c}\nu,\mu^{\prime}\in\mathcal{P}( \mathcal{Z})\\ \nu\leq\frac{1}{1-\varepsilon}\tilde{\mu}_{n}\end{subarray}}\mathsf{W}_{p}(\nu, \mu^{\prime})+\mathsf{W}_{p}(\mu^{\prime},\mu)\] \[\leq\inf_{\begin{subarray}{c}\nu,\mu^{\prime}\in\mathcal{P}( \mathcal{Z})\\ \nu\leq\frac{1}{1-\varepsilon}\tilde{\mu}_{n}\end{subarray}}\mathsf{W}_{p}(\nu, \mu^{\prime})+\tau_{p}(\mu,\varepsilon)\] (definition of \[\tau_{p}\] )\[=(1-\varepsilon)^{-\frac{1}{p}}\mathsf{W}_{p}^{*}(\tilde{\mu}_{n},\mu) +\tau_{p}(\mu,\varepsilon)\] (Lemma 1) \[\leq(1-\varepsilon)^{-\frac{1}{p}}(\rho_{0}+\delta)+\tau_{p}( \mathcal{G}_{\mathrm{cov}},\varepsilon)=\rho.\]

Writing \(\mathcal{G}^{\prime}=\mathcal{G}_{2}(\sigma,z_{0})\) and mirroring the proof of Theorem 1, we have for each \(\ell\in\mathcal{L}\) that

\[\mathbb{E}_{\mu}[\hat{\ell}\,]-\mathbb{E}_{\mu}[ \ell] \leq\sup_{\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}_{n}\|\nu)\leq\rho} \mathbb{E}_{\nu}[\hat{\ell}\,]-\mathbb{E}_{\mu}[\ell]\] \[\leq\sup_{\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}_{n}\|\nu)\leq \rho}\mathbb{E}_{\nu}[\ell]-\mathbb{E}_{\mu}[\ell]\] ( \[\hat{\ell}\] optimal for ( 4 )) \[\leq\sup_{\nu\in\mathcal{G}^{\prime}\atop\mathsf{W}_{p}^{ \varepsilon}(\nu,\mu)\leq 2\rho}\mathbb{E}_{\nu}[\ell]-\mathbb{E}_{\mu}[\ell]\] (Lemma 2) \[\leq\sup_{\nu\in\mathcal{G}^{\prime}\atop\mathsf{W}_{p}(\nu,\mu) \leq c\rho+2\tau_{p}(\mathcal{G}^{\prime},2\varepsilon)}\mathbb{E}_{\nu}[\ell] -\mathbb{E}_{\mu}[\ell]\] (Lemma 3) \[\leq\sup_{\nu\in\mathcal{P}(\mathbb{Z})\atop\mathsf{W}_{p}(\nu, \mu)\leq c\rho+2\tau_{p}(\mathcal{G}^{\prime},2\varepsilon)}\mathbb{E}_{\nu}[ \ell]-\mathbb{E}_{\mu}[\ell]\] \[=\mathcal{R}_{\mu,p}\big{(}c\rho+2\tau_{p}(\mathcal{G}^{\prime},2\varepsilon);\ell\big{)}\] \[\leq\mathcal{R}_{\mu,p}\big{(}c\rho+8\sigma(2\varepsilon)^{\frac {1}{p}-\frac{1}{2}}(1-2\varepsilon)^{-\frac{1}{p}};\ell\big{)}.\] (Lemma 5)

When \(p=1\), we bound the regularizer radius by

\[c\rho+8\sigma\sqrt{2\varepsilon}(1-2\varepsilon)^{-1}\lesssim\rho_{0}+ \delta+\tau_{1}(\mathcal{G}_{\mathrm{cov}},\varepsilon)+(\sqrt{d}+\rho_{0}) \sqrt{\varepsilon}\lesssim\rho_{0}+\delta+\sqrt{d\varepsilon},\]

using Lemma 5. Similarly, when \(p=2\), we bound the radius by

\[c\rho+8\sigma(1-2\varepsilon)^{-\frac{1}{2}}\lesssim\rho_{0}+\delta+\tau_{2 }(\mathcal{G}_{\mathrm{cov}},\varepsilon)+(\sqrt{d}+\rho_{0})\lesssim\rho_{0} +\delta+\sqrt{d}.\]

Taking \(\ell=\ell_{\star}\) and applying Lemma 7 gives the proposition.

### Proof of Proposition 5

To start, we fix \(d=1\). Given \(0\leq\gamma<1/2\) and \(\nu\in\mathcal{P}(\mathbb{R})\) with cumulative distribution function (CDF) \(F_{\nu}\), define the \(\gamma\)-trimming \(\mathsf{T}_{\gamma}(\nu)\in\mathcal{P}(\mathbb{R})\) as the law of \(F_{\nu}^{-1}(U)\), where \(U\sim\mathrm{Unif}([\gamma,1-\gamma])\), and let \(m_{\gamma}(\nu)\coloneqq\mathbb{E}_{T_{\gamma}(\nu)}[Z]\) denote the \(\gamma\)-trimmed mean. If \(\nu=\frac{1}{|A|}\sum_{a\in A}\delta_{a}\) is uniform over a finite set \(A=\{a_{1}<a_{2}<\cdots<a_{n}\}\) and \(\gamma n\) is an integer, we have \(m_{\gamma}(\nu)=\frac{1}{(1-2\gamma)n}\sum_{i=\gamma n+1}^{(1-\gamma)n}a_{i}\). Our robust mean estimate when \(d=1\) is \(z_{0}=m_{\gamma}(\tilde{\mu}_{n})\) with \(\gamma=1/3\). The smaller choice of \(\gamma=\varepsilon\) gives tighter guarantees at the cost of increased sample complexity; we keep the larger choice since we only require a coarse estimate.

**Lemma 8**.: _Consider Setting B with \(d=1\), \(\mathcal{G}=\mathcal{G}_{\mathrm{cov}}\), \(\rho_{0}=0\), \(\varepsilon\leq 1/3\). Fix sample size \(n=\Omega(\log(1/\delta))\), for \(0<\delta<1/2\). Then, \(\|m_{\gamma}(\tilde{\mu}_{n})-\mathbb{E}_{\mu}[Z]\|\lesssim 1\) with probability at least \(1-\delta\)._

Proof.: This follows by Proposition 1.18 of [16] applied to the distribution \(\mu\) with corruption fraction \(\gamma\), \(\varepsilon^{\prime}=4\gamma/3<1/2\), and resilience bound \(\tau(\mathcal{G}_{\mathrm{cov}},2\varepsilon^{\prime})\lesssim\sqrt{\gamma}\lesssim 1\). 

Now, since we are free to permute the order of the TV and \(\mathsf{W}_{p}\) corruptions (see Lemma 1), there exist \(\{W_{i}\}_{i=1}^{n}\subseteq\mathbb{R}\) with empirical measure \(\nu_{n}\in\mathcal{P}(\mathbb{R})\) such that \(\|\nu_{n}-\tilde{\mu}_{n}\|_{\mathsf{TV}}\leq\varepsilon\) and \(\mathsf{W}_{p}(\nu_{n},\tilde{\mu}_{n})\leq\rho_{0}\). By Lemma 8, we have \(|m_{\gamma}(\nu_{n})-\mathbb{E}_{\mu}[Z]|\lesssim 1\). Of course, we do not observe \(\nu_{n}\), so this result is not immediately useful. To apply this fact, we use that \(\mathsf{T}_{\gamma}\) is an approximate Wasserstein contraction.

**Lemma 9**.: _If \(\alpha,\beta\in\mathcal{P}(\mathbb{R})\) and \(0\leq\gamma<1/2\), then \(\mathsf{W}_{p}(\mathsf{T}_{\gamma}(\alpha),\mathsf{T}_{\gamma}(\beta))\leq(1-2 \gamma)^{-1/p}\mathsf{W}_{p}(\alpha,\beta)\)._

Proof.: Writing \(F_{\alpha}\) and \(F_{\beta}\) for the CDFs of \(\alpha\) and \(\beta\), respectively, we compute

\[\mathsf{W}_{p}(\mathsf{T}_{\gamma}(\alpha),\mathsf{T}_{\gamma}(\beta))^{p}= \frac{1}{1-2\gamma}\int_{\gamma}^{1-\gamma}|F_{\alpha}^{-1}(t)-F_{\beta}^{-1}(t) |^{p}\,\mathrm{d}t\]\[\leq\frac{1}{1-2\gamma}\int_{0}^{1}|F_{\alpha}^{-1}(t)-F_{\beta}^{-1}(t)|^{p} \,\mathrm{d}t=\frac{1}{1-2\gamma}\mathsf{W}_{p}(\alpha,\beta)^{p},\]

as desired. 

Applying Lemma 9, we bound

\[|m_{\gamma}(\tilde{\mu}_{n})-\mathbb{E}_{\mu}[Z]| \leq|m_{\gamma}(\nu_{n})-\mathbb{E}_{\mu}[Z]|+|m_{\gamma}(\nu_{n} )-m_{\gamma}(\tilde{\mu}_{n})|\] \[\leq|m_{\gamma}(\nu_{n})-\mathbb{E}_{\mu}[Z]|+\mathsf{W}_{1}( \mathsf{T}_{\gamma}(\nu_{n}),\mathsf{T}_{\gamma}(\tilde{\mu}_{n}))\] \[\lesssim 1+\mathsf{W}_{1}(\nu_{n},\tilde{\mu}_{n})\] \[\lesssim 1+\rho_{0}.\]

This matches the proposition statement when \(d=1\).

For general \(d>1\), we propose the coordinate-wise trimmed estimate \(z_{0}\in\mathbb{R}^{d}\) given by \((z_{0})_{i}=m_{\gamma}(\mathbf{e}_{i\ \ \#}^{\top}\tilde{\mu}_{n})\). Plugging in \(\delta\gets 1/(100d)\) into Lemma 8 and taking a union bound over coordinates, we condition on the \(0.99\) probability event that the one-dimensional bound holds for all coordinates. We can then bound

\[\|z_{0}-\mathbb{E}_{\mu}[Z]\|^{2} =\sum_{i=1}^{d}\bigl{(}m_{\gamma}(\mathbf{e}_{i\ \ \#}^{\top}\tilde{\mu}_{n})-\mathbb{E}_{\mu}[\mathbf{e}_{i \ \#}^{\top}Z]\bigr{)}^{2}\] \[\lesssim d+\sum_{i=1}^{d}\mathsf{W}_{1}(\mathbf{e}_{i\ \#}^{\top}\nu_{n},\mathbf{e}_{i\ \#}^{\top}\tilde{\mu}_{n})^{2}\] \[\leq d+\mathsf{W}_{1}(\nu_{n},\tilde{\mu}_{n})^{2}\] \[\leq d+\rho_{0}^{2},\]

as desired. The penultimate inequality is a consequence of the reverse Minkowski inequality.

**Lemma 10**.: _Fix \(\alpha,\beta\in\mathcal{P}(\mathbb{R}^{d})\) Write \(\alpha_{i}=\mathbf{e}_{i\ \ \#}^{\top}\alpha\) and \(\beta_{i}=\mathbf{e}_{i\ \#}^{\top}\beta\), \(i\in[d]\), for their coordinate-wise marginals. We then have_

\[\sum_{i=1}^{d}\mathsf{W}_{1}(\alpha_{i},\beta_{i})^{2}\leq\mathsf{W}_{1}( \alpha,\beta)^{2}.\] (8)

Proof.: Take \((X,Y)\) to be an optimal coupling for the \(\mathsf{W}_{1}(\alpha,\beta)\) problem. Writing \(\Delta_{i}=\|Y_{i}-X_{i}\|^{2}\) for \(i\in[d]\), the right hand side of (8) can be written as the \(L^{1/2}\) norm \(\|\sum_{i=1}^{d}\Delta_{i}\|_{1/2}\). We then bound

\[\sum_{i=1}^{d}\mathsf{W}_{1}(\alpha_{i},\beta_{i})^{2}\leq\sum_{i=1}^{d}\| \Delta_{i}\|_{1/2}\leq\left\|\sum_{i=1}^{d}\Delta_{i}\right\|_{1/2},\]

where the final inequality follows by the reverse Minkowski inequality for \(L^{1/2}\). 

### Proof of Proposition 6

We have

\[\sup_{\begin{subarray}{c}\nu\in\mathcal{G}_{2}(\sigma,z_{0}):\\ \mathsf{W}_{p}^{\mathsf{c}}(\tilde{\mu}_{n}\|\nu)\leq\rho\end{subarray}}\mathbb{ E}_{\nu}[\ell] =\sup_{\begin{subarray}{c}\mu^{\prime},\nu\in\mathcal{P}(\mathcal{Z})\\ \pi\in\Pi(\mu^{\prime},\nu)\end{subarray}}\left\{\mathbb{E}_{\nu}[\ell]:\ \ \mathbb{E}_{\nu}[\|Z^{ \prime}-Z\|^{p}]\leq\rho^{p},\ \right\}\] \[=\sup_{\begin{subarray}{c}\nu_{1},\ldots,\nu_{n}\in\mathcal{P}( \mathcal{Z})\\ \nu_{1},\ldots,\nu_{n}\in\mathcal{P}(\mathcal{Z})\end{subarray}}\left\{\sum_{i \in[n]}m_{i}\,\mathbb{E}_{\nu_{i}}[\ell]:\ \ \sum_{i\in[n]}m_{i}\,\mathbb{E}_{\nu_{i}}[\|Z_{i}-Z_{i}\|^{p}]\leq\rho^{p}, \right\},\]

where the first equality follows from the definitions of \(\mathcal{G}_{2}(\sigma,z_{0})\) and \(\mathsf{W}_{p}^{\mathsf{c}}(\tilde{\mu}_{n}\|\nu)\). The second equality holds because \(\tilde{\mu}_{n}=\frac{1}{n}\sum_{i\in[n]}\delta_{Z_{i}}\), which implies that the distributions \(\mu^{\prime},\nu\) and \(\pi\) take the form \(\mu^{\prime}=\sum_{i\in[n]}m_{i}\partial_{\tilde{Z}_{j}}\), \(\nu=\sum_{i\in[n]}m_{i}\nu_{i}\), and \(\pi=\sum_{i\in[n]}m_{i}\delta_{\tilde{Z}_{j}}\otimes\nu_{i}\), respectively. Note that the distribution \(\nu_{i}\) models the probability distribution of the random variable \(Z\) condition on the event that \(Z^{\prime}=\tilde{z}_{i}\). Using the definition of the expectation operator and introducing the positive measure \(\nu_{i}^{\prime}=m_{i}\nu_{i}\) for every \(i\in[n]\), we arrive at

\[\sup_{\begin{subarray}{c}\nu\in\mathcal{G}_{2}(\sigma,z_{0}):\\ W_{p}^{\mathsf{c}}(\tilde{\mu}_{n},\|\nu)\leq\rho\end{subarray}}\mathbb{E}_{ \nu}[\ell]=\sup_{\begin{subarray}{c}m\in\mathbb{R}^{n}\\ \nu_{1}^{\prime},\ldots,\nu_{n}^{\prime}\geq 0\end{subarray}}\left\{\sum_{i \in[n]}\mathbb{E}_{\nu_{1}^{\prime}}[\ell]:\begin{array}{l}\sum_{i\in[n]} \int_{Z}\|z_{i}-\tilde{Z}_{i}\|^{p}d\nu_{i}^{\prime}(z_{i})\leq\rho^{p},\\ 0\leq m_{i}\leq\frac{1}{n(1-\epsilon)},\;\forall i\in[n],\\ \sum_{i\in[n]}m_{i}=1\\ \int_{Z}d\nu_{i}^{\prime}(z_{i})=m_{i},\;\forall i\in[n]\end{array}\right\},\]

where the second equality follows from strong duality, which holds because the Slater condition outlined in [47, Proposition 3.4] is satisfied thanks to Assumption 1. The proof concludes by removing the decision variables \(r\) and \(s\) and using the definition of \(\tilde{\mu}_{n}\). 

### Proof of Theorem 2

The proof requires the following preparatory lemma. We say that the function \(f\) is proper if \(f(x)>-\infty\) and \(\mathrm{dom}(f)\neq\emptyset\).

**Lemma 11**.: _The followings hold._

1. _Let_ \(f(x)=\lambda g(x-x_{0})\)_, where_ \(\lambda\geq 0\) _and_ \(g:\mathbb{R}^{d}\to\mathbb{R}\) _is l.s.c. and convex. Then,_ \(f^{*}(y)=x_{0}^{\top}y+\lambda g^{*}(y/\lambda)\)_._
2. _Let_ \(f(x)=\|x\|^{p}\) _for some_ \(p\geq 1\)_. Then,_ \(f^{*}(y)=h(y)\)_, where the function_ \(h\) _is defined as in (_6_)._
3. _Let_ \(f(x)=x^{\top}\Sigma x\) _for some_ \(\Sigma\succ 0\)_. Then,_ \(f^{*}(y)=\frac{1}{4}y^{\top}\Sigma^{-1}y\)_._

Proof.: The claims follows from [25, SSE, Proposition 1.3.1 ], [57, Lemma B.8 (ii)] and [25, SSE, Example 1.1.3], respectively. 

Proof of Theorem 2.: By Proposition 6 and exploiting the definition of \(\tilde{\mu}_{n}\), we have

\[\sup_{\begin{subarray}{c}\nu\in\mathcal{G}_{2}(\sigma,z_{0}):\\ W_{p}^{\mathsf{c}}(\tilde{\mu}_{n},\|\nu)\leq\rho\end{subarray}}\mathbb{E}_{ \nu}[\ell]=\left\{\begin{array}{ll}\inf&\lambda_{1}\sigma^{2}+\lambda_{2} \rho^{p}+\alpha+\frac{1}{n(1-\epsilon)}\sum_{i\in[n]}s_{i}\\ \text{s.t.}&\alpha\in\mathbb{R},\;\lambda_{1},\lambda_{2}\in\mathbb{R}_{+},\;s \in\mathbb{R}_{+}^{n}\\ &s_{i}\geq\sup_{\xi\in\mathcal{Z}}\;\ell(\xi)-\lambda_{1}\|\xi-z_{0}\|^{2}- \lambda_{2}\|\xi-\tilde{Z}_{i}\|^{p}-\alpha\quad\forall i\in[n]\\ &\\ =\left\{\begin{array}{ll}\inf&\lambda_{1}\sigma^{2}+\lambda_{2}\rho^{p}+ \alpha+\frac{1}{n(1-\epsilon)}\sum_{i\in[n]}s_{i}\\ \text{s.t.}&\alpha\in\mathbb{R},\;\lambda_{1},\lambda_{2}\in\mathbb{R}_{+},\;s \in\mathbb{R}_{+}^{n}\\ &\\ &s_{i}\geq\sup_{\xi\in\mathcal{Z}}\;\ell_{j}(\xi)-\lambda_{1}\|\xi-z_{0}\|^{2} \\ &\\ &-\lambda_{2}\|\xi-\tilde{Z}_{i}\|^{p}-\alpha\quad\forall i\in[n],\forall j \in[J]\end{array}\right.\] (9)

where the second equality follows form Assumption 2. For any fixed \(i\in[n]\) and \(j\in[J]\), we have

\[\sup_{\xi\in\mathcal{Z}}\;\ell_{j}(\xi)-\lambda_{1}\|\xi-z_{0}\|^ {2}-\lambda_{2}\|\xi-\tilde{Z}_{i}\|^{p}-\alpha\] \[= \left\{\begin{array}{ll}\inf&(-\ell_{j})^{*}(\zeta_{ij}^{ \ell})+z_{0}^{\top}\zeta_{ij}^{\mathcal{G}}+\tau_{ij}+\tilde{Z}_{i}^{\top} \zeta_{ij}^{\mathsf{W}}+P_{h}(\zeta_{ij}^{\mathsf{W}},\lambda_{2})+\chi_{ \mathcal{Z}}^{*}(\zeta_{Z}^{\mathcal{G}})-\alpha\\ \text{s.t.}&\tau_{ij}\in\mathbb{R}_{+}^{n},\;\zeta_{ij}^{\ell},\zeta_{ij}^{ \mathsf{W}},\zeta_{ij}^{\mathsf{Z}}\in\mathbb{R}^{d},\;\zeta_{ij}^{\ell}+ \zeta_{ij}^{\mathsf{G}}+\zeta_{ij}^{\mathsf{W}}+\zeta_{ij}^{\mathcal{Z}}=0,\; \|\zeta_{ij}^{\mathcal{G}}\|^{2}\leq\lambda_{1}\tau_{ij}\end{array}\right.\]where the equality is a result of strong duality due to [57, Theorem 2] and Lemma 11. The claim follows by substituting all resulting dual minimization problems into (9) and eliminating the corresponding minimization operators. 

### Proof of Theorem 3

Thanks to Remark 3, we have

\[\sup_{\begin{subarray}{c}\nu\in\mathcal{G}_{2}(\sigma,z_{0}):\\ \mathsf{W}_{p}^{e}(\mu_{n}\|\nu)\leq\rho\end{subarray}}\mathbb{E}_{\nu}[\ell]\] \[=\inf_{\lambda_{1},\lambda_{2}\in\mathbb{R}_{+}}\lambda_{1} \sigma^{2}+\lambda_{2}\rho^{p}+\mathrm{CVaR}_{1-\varepsilon,\tilde{\mu}_{n}} \Bigg{[}\sup_{\xi\in\mathcal{Z}}\,\ell(\xi)-\lambda_{1}\|\xi-z_{0}\|^{2}- \lambda_{2}\|\xi-Z\|^{p}\Bigg{]}\] \[=\inf_{\lambda_{1},\lambda_{2}\in\mathbb{R}_{+}}\sup_{m\in \mathcal{M}_{\varepsilon}}\lambda_{1}\sigma^{2}+\lambda_{2}\rho^{p}+\sum_{i \in[n]}m_{i}\Bigg{[}\sup_{\xi\in\mathcal{Z}}\,\ell(\xi)-\lambda_{1}\|\xi-z_{ 0}\|^{2}-\lambda_{2}\|\xi-\tilde{Z}_{i}\|^{p}\Bigg{]},\]

where \(\mathcal{M}_{\varepsilon}:=\{m\in\mathbb{R}_{+}^{n}:m_{i}\leq 1/(n(1- \varepsilon)),\forall i\in[n],\sum_{i\in[n]}m_{i}=1\}\), and the equality follows from the primal representation of the CVaR as a coherent risk measure [3, Example 4.1] and the fact that \(\tilde{\mu}_{n}\) is discrete. By Assumption 2, we have

\[\sup_{m\in\mathcal{M}_{\varepsilon}}\sum_{i\in[n]}m_{i}\left[ \sup_{\xi\in\mathcal{Z}}\,\ell(\xi)-\lambda_{1}\|\xi-z_{0}\|^{2}-\lambda_{2}\| \xi-\tilde{Z}_{i}\|^{p}\right]\] \[=\sup_{m\in\mathcal{M}_{\varepsilon}}\sup_{\xi_{ij}\in\mathcal{Z} }\sum_{i\in[n]}m_{i}\bigg{[}\max_{j\in[J]}\ell(\xi_{ij})-\lambda_{1}\|\xi_{ij }-z_{0}\|^{2}-\lambda_{2}\|\xi_{ij}-\tilde{Z}_{i}\|^{p}\bigg{]}\] \[=\sup_{q\in\mathcal{Q}_{\varepsilon}}\sup_{\xi_{ij}\in\mathcal{Z }}\sum_{(i,j)\in[n]\times[J]}q_{ij}\Big{[}\ell(\xi_{ij})-\lambda_{1}\|\xi_{ij }-z_{0}\|^{2}-\lambda_{2}\|\xi_{ij}-\tilde{Z}_{i}\|^{p}\Big{]},\]

where \(\mathcal{Q}_{\varepsilon}:=\{q\in\mathbb{R}_{+}^{n\times J}:\sum_{j\in[J]}q_{ ij}\leq\frac{1}{n(1-\varepsilon)},\forall i\in[n],\sum_{(i,j)\in[n]\times[J]}q_{ ij}=1\}\), and the last equality easily follows by introducing the variables \(q_{ij}\) as a means to merge the variables \(m_{i}\) and the maximum operator. Note that the final supremum problem is nonconvex as we have bi-linearity between \(q_{ij}\) and \(\xi_{ij}\). Using the definition of the perspective function and the simple variable substitution \(\xi_{ij}+\xi_{ij}/q_{ij}\), however, one can convexify this problem and arrive at

\[\sup_{\begin{subarray}{c}\nu\in\mathcal{G}_{2}(\sigma,z_{0}):\\ \mathsf{W}_{p}^{e}(\mu_{n}\|\nu)\leq\rho\end{subarray}}\mathbb{E}_{\nu}[\ell] =\inf_{\lambda_{1},\lambda_{2}\in\mathbb{R}_{+}}\sup_{\begin{subarray}{c}q \in\mathcal{Q}_{\varepsilon}\\ \xi_{ij}\in q_{ij}:\mathcal{Z}\end{subarray}}\Big{\{}\lambda_{1}\sigma^{2}+ \lambda_{2}\rho^{p}-\sum_{(i,j)\in[n]\times[J]}P_{-\ell_{j}}(\xi_{ij},q_{ij})\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad-\lambda_ {1}P_{\|\cdot\|^{2}}(\xi_{ij}-q_{ij}z_{0},q_{ij})-\lambda_{2}P_{\|\cdot\|^{p}} (\xi_{ij}-q_{ij}\tilde{Z}_{i},q_{ij})\Big{\}}.\]

Note that strong duality holds similar to the proof of [57, Section 6]. This allows us to interchange the infimum and supremum without changing the optimal value of the problem. Then, infimizing over \(\lambda_{1}\) and \(\lambda_{2}\), and noticing that the resulting supremum problem is solvable, since the feasible set is compact, conclude the first part of the proof. Following the discussion in [57, SS 6], it is easy to show that the proposed discrete distribution \(\nu^{\star}\) solves the worst-case expectation problem. The details are omitted for brevity. This concludes the proof.

## Appendix C Proofs for Section 4

### Proof of Theorem 4

We start with the following lemma.

**Lemma 12**.: _Under the setting of Theorem 4, we may decompose \(\ell_{\star}=\tilde{\ell}\circ Q\) for \(Q\in\mathbb{R}^{k\times d}\) with \(QQ^{\top}=I_{k}\) and some \(\tilde{\ell}:\mathbb{R}^{k}\to\mathbb{R}^{d}\). For any such decomposition, we have_

\[\sup_{\begin{subarray}{c}\nu\in\mathcal{G}\\ \mathsf{W}_{p}^{e}(\nu,\tilde{\mu})\leq\rho\end{subarray}}\mathbb{E}_{\nu}[\ell_ {\star}]=\sup_{\begin{subarray}{c}\nu\in\mathcal{G}_{k}\\ \mathsf{W}_{p}^{e}(\nu,Q_{\tilde{\mu}})\leq\rho\end{subarray}}\mathbb{E}_{\nu}[ \tilde{\ell}].\]Proof.: To start, we decompose \(A(z)=RQz+z_{0}\), where \(Q\in\mathbb{R}^{k\times d}\) with \(QQ^{\top}=I_{k}\), \(R\in\mathbb{R}^{k\times k}\), and \(z_{0}\in\mathbb{R}^{k}\). Note that the orthogonality condition ensures that \(Q^{\top}\) isometrically embeds \(\mathbb{R}^{k}\) into \(\mathbb{R}^{d}\). We then choose \(\tilde{\ell}(w)=\underline{\ell}(Rw+z_{0})\).

Next, given \(\nu\in\mathcal{G}\), we have \(Q_{\#}\nu\in\mathcal{G}^{(k)}\) with \(\mathsf{W}_{p}^{\varepsilon}(Q_{\#}\nu,Q_{\#}\tilde{\mu})\leq\mathsf{W}_{p}^{ \varepsilon}(\nu,\tilde{\mu})\), and \(\mathbb{E}_{\nu}[\ell]=\mathbb{E}_{Q_{\#}\nu}[\tilde{\ell}]\). Thus, the RHS supremum is always at least as large as the LHS. It remains to show the reverse.

Fix \(\nu\in\mathcal{G}_{k}\) with \(\mathsf{W}_{p}^{\varepsilon}(\nu,Q_{\#}\tilde{\mu})\). Take any \(\nu^{\prime}\in\mathcal{P}(\mathbb{R}^{k})\) with \(\mathsf{W}_{p}(\nu,\nu^{\prime})\leq\rho\) and \(\|\nu^{\prime}-Q_{\#}\tilde{\mu}\|_{\mathsf{TV}}\leq\varepsilon\). Write \(\kappa=Q_{\#}^{\top}\nu\in\mathcal{G}\) and \(\kappa^{\prime}=Q_{\#}^{\top}\nu^{\prime}\). Since \(Q^{\top}\) is an isometric embedding, we have \(\kappa\in\mathcal{G}\), \(\mathsf{W}_{p}(\kappa,\kappa^{\prime})=\mathsf{W}_{p}(\nu,\nu^{\prime})\leq\rho\), and \(\|\kappa^{\prime}-\tilde{\mu}\|_{\mathsf{TV}}=\|\nu^{\prime}-Q_{\#}\tilde{\mu }\|_{\mathsf{TV}}\leq\varepsilon\). Finally, we have \(\mathbb{E}_{\nu}[\ell]=\mathbb{E}_{\kappa}[\tilde{\ell}]\). Thus, the RHS supremum is no greater than the LHS, and we have the desired equality. 

Writing \(\mu_{k}=Q_{\#}\mu\), we mirror the proof of Theorem 1 and bound

\[\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}] \leq\sup_{\begin{subarray}{c}\nu\in\mathcal{G}\\ \mathsf{W}_{p}^{\varepsilon}(\tilde{\mu},\nu)\leq\rho\end{subarray}}\mathbb{ E}_{\nu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}]\] ( \[\mu\in\mathcal{G}\], \[\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu},\mu)\leq\rho\] ) \[\leq\sup_{\begin{subarray}{c}\nu\in\mathcal{G}\\ \mathsf{W}_{p}^{\varepsilon}(\tilde{\mu},\nu)\leq\rho\end{subarray}}\mathbb{ E}_{\nu}[\ell_{\star}]-\mathbb{E}_{\mu}[\ell_{\star}]\] ( \[\hat{\ell}\] optimal for (OR-WDRO)) \[=\sup_{\begin{subarray}{c}\nu\in\mathcal{G}^{(k)}\\ \mathsf{W}_{p}^{\varepsilon}(Q_{\#}\tilde{\mu})\leq\rho\end{subarray}}\mathbb{ E}_{\nu}[\tilde{\ell}]-\mathbb{E}_{\mu_{k}}[\tilde{\ell}]\] (Lemma 12 ) \[\leq\sup_{\begin{subarray}{c}\nu\in\mathcal{G}^{(k)}\\ \mathsf{W}_{p}^{\varepsilon}(Q_{\#}\tilde{\mu})\leq\rho\end{subarray}}\mathbb{ E}_{\nu}[\tilde{\ell}]-\mathbb{E}_{\mu_{k}}[\tilde{\ell}]\] (Lemma 2 ) \[\leq\sup_{\begin{subarray}{c}\nu\in\mathcal{G}^{(k)}\\ \mathsf{W}_{p}^{2\varepsilon}(\nu,\mu_{k})\leq 2\rho\end{subarray}}\mathbb{E}_{\nu}[\tilde{\ell}]- \mathbb{E}_{\mu_{k}}[\tilde{\ell}]\] (Lemma 3 ) \[\leq\sup_{\begin{subarray}{c}\nu\in\mathcal{P}(\mathcal{Z})\\ \mathsf{W}_{p}(\nu,\mu_{k})\leq c\rho+2\tau_{p}(\mathcal{G}^{(k)},2 \varepsilon)\end{subarray}}\mathbb{E}_{\nu}[\tilde{\ell}]-\mathbb{E}_{\mu_{k}}[ \tilde{\ell}]\] ( \[\mathcal{G}\subseteq\mathcal{P}(\mathcal{Z})\] ) \[=\mathcal{R}_{\mu_{\pm},p}\big{(}c\rho+2\tau_{p}(\mathcal{G}^{(k)},2\varepsilon);\tilde{\ell}\,\big{)}.\]

To obtain the theorem, we apply Lemma 6 and observe that \(\|\tilde{\ell}\|_{\mathrm{Lip}}=\|\ell_{\star}\|_{\mathrm{Lip}}\) and \(\|\tilde{\ell}\|_{\tilde{H}^{1,2}(\mu_{k})}=\|\ell_{\star}\|_{\tilde{H}^{1,2}( \mu)}\) (since \(Q^{\top}\) is an isometric embedding from \(\mathbb{R}^{k}\) into \(\mathbb{R}^{d}\)). 

### Proof of Corollary 4

This follows as an immediate consequence of Theorem 4 and Corollary 1. 

### Proof of Proposition 7

We simply instantiate the lower bound construction from Proposition 3 in \(\mathbb{R}^{k}\), viewed as a subspace of \(\mathbb{R}^{d}\). Extending each \(\ell\in\mathcal{L}\) to \(\mathbb{R}^{d}\) by \(\ell(z)=\ell(z_{1:k})\), the same lower bound applies with \(d\gets k\). 

### Proof of Proposition 8

For \(Z\sim\mu\in\mathcal{G}_{\mathrm{cov}}\) and \(z_{0},w\in\mathbb{R}^{d}\), we bound

\[\mathbb{E}\big{[}w^{\top}(Z-z_{0})(Z-z_{0})^{\top}w\big{]}^{\frac{ 1}{2}} =\mathbb{E}\big{[}w^{\top}(Z-z_{0})^{2}\big{]}^{\frac{1}{2}}\] \[\leq\mathbb{E}\big{[}w^{\top}(Z-\mathbb{E}[Z])^{2}\big{]}^{\frac{ 1}{2}}+|w^{\top}(\mathbb{E}[Z]-z_{0})|\] \[\leq\|w\|(1+\|z_{0}-\mathbb{E}[Z]\|).\]

Consequently, we have \(\mu\in\mathcal{G}_{\mathrm{cov}}(1+\|z_{0}-\mathbb{E}[Z]\|,z_{0})\subseteq \mathcal{G}_{\mathrm{cov}}(\sigma,z_{0})\). Moreover, we have \(\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}_{n},\mu)\leq\mathsf{W}_{p}(\nu^{ \prime}_{m},\mu)\leq\rho_{0}+\delta=\rho\). Decomposing \(\ell_{\star}=\tilde{\ell}\circ Q\) as in Lemma 12 and writing \(\mu_{k}=Q_{\#}\mu\), the approach applied in the proof of Theorem 4 gives

\[\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}]\leq\mathcal{R}_{\mu _{k},p}\big{(}c\rho+2\tau_{p}(Q_{\#}\mathcal{G}_{\mathrm{cov}}(\sigma,z_{0}),2 \varepsilon);\hat{\ell}\big{)}\]\[\leq\|\,\mathbb{E}_{\alpha_{k}}[Z]-\mathbb{E}_{\mu}[Z]\|+O(1+\rho_{0})\] \[\leq O(1+\rho_{0}),\]

as desired. 

### Proof of Proposition 10

We have

\[\sup_{\begin{subarray}{c}\nu\in\mathcal{G}_{\mathrm{cov}}(\sigma, \varepsilon_{0}):\\ W^{\varepsilon}_{p}(\tilde{\mu}_{n}\|\nu)\leq\rho\end{subarray}}\mathbb{E}_{ \nu}[\ell]=\sup_{\begin{subarray}{c}\mu^{\prime},\nu\in\mathcal{P}(Z)\\ \pi\in\Pi(\mu^{\prime},\nu)\end{subarray}}\left\{\begin{array}{ll}\mathbb{E}_{ \nu}[(Z-z_{0})(Z-z_{0})^{\top}]\preceq\sigma I_{d},\\ \begin{subarray}{c}\mathbb{E}_{\pi}[\|Z^{\prime}-Z\|^{p}]\leq\rho^{p},\\ \mu^{\prime}\leq\frac{1}{1-\varepsilon}\tilde{\mu}_{n}\end{array}\right\}\]\[=\sup_{\begin{subarray}{c}m\in\mathbb{R}^{n}\\ \nu_{1},\dots,\nu_{n}\in\mathcal{P}(Z)\end{subarray}}\left\{\sum_{i\in[n]}m_{i} \,\mathbb{E}_{\nu_{i}}[(Z_{i}-z_{0})(Z_{i}-z_{0})^{\top}]\preceq\sigma I_{d}, \right.\] \[=\sup_{\begin{subarray}{c}m\in\mathbb{R}^{n}\\ \nu_{1},\dots,\nu_{n}\geq 0\end{subarray}}\left\{\sum_{i\in[n]}m_{i}\,\mathbb{E}_{ \nu_{i}}[\ell]:\begin{array}{l}\sum_{i\in[n]}m_{i}\,\mathbb{E}_{\nu_{i}}[ \|\tilde{Z}_{i}-Z_{i}\|^{p}]\leq\rho^{p},\\ 0\leq m_{i}\leq\frac{1}{n(1-\epsilon)},\;\forall i\in[n],\\ \sum_{i\in[n]}m_{i}=1\end{array}\right\},\]

where the first equality follows from the definitions of \(\mathcal{G}_{\rm cov}(\sigma,z_{0})\) and \(\mathsf{W}_{p}^{\epsilon}(\tilde{\mu}_{n}\|\nu)\). The second and the third equalities follow from the same variable substitution as in the proof of Proposition 6. The last optimization problem admits the dual form

\[\inf_{\begin{subarray}{c}\Lambda_{1}\in\mathbb{Q}_{+}^{d},\lambda_{2}\in \mathbb{R}_{+}\\ r,s\in\mathbb{R}^{n},\alpha\in\mathbb{R}\end{subarray}}\left\{\begin{array}{ l}-z_{0}^{\top}\Lambda_{1}z_{0}+\sigma\,\operatorname{Tr}[\Lambda_{1}]+ \lambda_{2}\rho^{p}+\alpha+\frac{\sum_{i\in[n]}s_{i}}{n(1-\epsilon)}:\\ s_{i}\geq\max\{0,r_{i}-\alpha\},\quad\forall i\in[n],\\ r_{i}\geq\ell(\xi)-\xi^{\top}\Lambda_{1}\xi+2\xi^{\top}\Lambda_{1}z_{0}-\lambda _{2}\|\xi-\tilde{Z}_{i}\|^{p},\quad\forall\xi\in\mathcal{Z},\forall i\in[n] \end{array}\right\}.\]

Strong duality holds thanks to Assumption 3 and [47, Proposition 3.4]. The proof of the second claim concludes by removing the decision variables \(r\) and \(s\) and using the definition of \(\tilde{\mu}_{n}\). 

### Proof of Theorem 5

By Proposition 10 and exploiting the definition of \(\tilde{\mu}_{n}\), we have

\[\sup_{\begin{subarray}{c}\nu\in\mathcal{G}_{\rm cov}(\sigma,z_{0 }):\\ \mathsf{W}_{p}^{\epsilon}(\tilde{\mu}_{n}\|\nu)\leq\rho\end{subarray}}\mathbb{E }_{\nu}[\ell]\] \[= \left\{\begin{array}{rl}\inf&-z_{0}^{\top}\Lambda_{1}z_{0}+ \sigma\operatorname{Tr}[\Lambda_{1}]+\lambda_{2}\rho^{p}+\alpha+\frac{1}{n(1- \varepsilon)}\sum_{i\in[n]}s_{i}\\ \text{s.t.}&\Lambda_{1}\in\mathbb{Q}_{+}^{d},\,\lambda_{2}\in\mathbb{R}_{+},\,s \in\mathbb{R}_{+}^{n}\\ &s_{i}\geq\sup_{\xi\in\mathcal{Z}}\;\ell(\xi)-\xi^{\top}\Lambda_{1}\xi+2\xi^{ \top}\Lambda_{1}z_{0}-\lambda_{2}\|\xi-\tilde{Z}_{i}\|^{p}-\alpha\quad\forall i \in[n]\end{array}\right.\] \[= \left\{\begin{array}{rl}\inf&-z_{0}^{\top}\Lambda_{1}z_{0}+ \sigma\operatorname{Tr}[\Lambda_{1}]+\lambda_{2}\rho^{p}+\frac{1}{n(1- \varepsilon)}\sum_{i\in[n]}s_{i}\\ \text{s.t.}&\Lambda_{1}\in\mathbb{Q}_{+}^{d},\,\lambda_{2}\in\mathbb{R}_{+},\,s \in\mathbb{R}_{+}^{n}\\ &s_{i}\geq\sup_{\xi\in\mathcal{Z}}\;\ell_{j}(\xi)-\xi^{\top}\Lambda_{1}\xi+2 \xi^{\top}\Lambda_{1}z_{0}-\lambda_{2}\|\xi-\tilde{Z}_{i}\|^{p}-\alpha\quad \forall i\in[n],\forall j\in[J]\end{array}\right.\] (10)

where the second equality follows form Assumption 2. For any fixed \(i\in[n]\) and \(j\in[J]\), we have

\[\sup_{\xi\in\mathcal{Z}}\;\ell_{j}(\xi)-\xi^{\top}\Lambda_{1}\xi+ 2\xi^{\top}\Lambda_{1}z_{0}-\lambda_{2}\|\xi-\tilde{Z}_{i}\|^{p}-\alpha\] \[= \left\{\begin{array}{rl}\inf&(-\ell_{j})^{*}(\zeta_{ij}^{\ell} )+\frac{1}{4}(\zeta_{ij}^{\mathcal{G}})^{\top}\Lambda_{1}^{-1}\zeta_{ij}^{ \mathcal{G}}+\tilde{Z}_{i}^{\top}\zeta_{ij}^{\mathsf{W}}+\lambda_{2}h(\zeta_{ ij}^{\mathsf{W}}/\lambda_{2},p)+\chi_{\mathcal{Z}}^{*}(\zeta_{ij}^{ \mathcal{G}})-\alpha\\ \text{s.t.}&\zeta_{ij}^{\ell},\zeta_{ij}^{\mathcal{G}},\zeta_{ij}^{\mathsf{W}},\zeta_{ij}^{\mathcal{Z}}\in\mathbb{R}^{d},\;\zeta_{ij}^{\ell}+\zeta_{ij}^{ \mathcal{G}}+\zeta_{ij}^{\mathcal{W}}+\zeta_{ij}^{\mathcal{Z}}=2\Lambda_{1}z_{0} \end{array}\right.\]

where the equality is a result of strong duality due to [57, Theorem 2] and Lemma 11. The claim follows by introducing the epigraph variable \(\tau_{ij}\) for the term \(\frac{1}{4}(\zeta_{ij}^{\mathcal{G}})^{\top}\Lambda_{1}^{-1}\zeta_{ij}^{ \mathcal{G}}\), and substituting all resulting dual minimization problems into (10) and eliminating the corresponding minimization operators. Note that by the Schur complement argument, we have

\[\frac{1}{4}(\zeta_{ij}^{\mathcal{G}})^{\top}\Lambda_{1}^{-1}\zeta_{ij}^{ \mathcal{G}}\leq\tau_{ij}\iff\begin{bmatrix}\Lambda_{1}&\zeta_{ij}^{\mathcal{G }}\\ (\zeta_{ij}^{\mathcal{G}})^{\top}&4\tau_{ij}\end{bmatrix}\succeq 0,\]

which implies that the resulting reformulation is indeed convex.

Comparison to WDRO with Expanded Radius around Minimum Distance Estimate (Remark 1)

First, we prove the claimed excess risk bound for WDRO with an expanded radius around the minimum distance estimate \(\hat{\mu}=\hat{\mu}(\tilde{\mu},\mathcal{G},\varepsilon)\coloneqq\operatorname*{ argmin}_{\nu\in\mathcal{G}}\mathsf{W}_{p}^{e}(\nu,\tilde{\mu})\). We write \(c=2(1-\varepsilon)^{-1/p}\) as in Theorem 1.

**Lemma 14**.: _Under Setting A, let \(\hat{\ell}=\operatorname*{argmin}_{\ell\in\mathcal{L}}\sup_{\nu\in\mathcal{P} (Z):\mathsf{W}_{p}(\nu,\hat{\mu})\leq\rho^{\prime}}\mathbb{E}_{\nu}[\ell]\), for the expanded radius \(\rho^{\prime}\coloneqq c\rho+2\tau_{p}(\mathcal{G},2\varepsilon)\). We then have \(\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}]\leq\mathcal{R}_{ \mu,p}(c\rho+2\tau_{p}(\mathcal{G},2\varepsilon);\ell_{\star})\)._

Proof.: Since \(\mathsf{W}_{p}^{e}(\mu,\tilde{\mu})\leq\rho\) and \(\mu\in\mathcal{G}\), we have \(\mathsf{W}_{p}^{e}(\hat{\mu},\tilde{\mu})\leq\rho\). Thus, Lemma 2 gives that \(\mathsf{W}_{p}^{2\varepsilon}(\hat{\mu},\mu)\leq 2\rho\). By Lemma 3, we then have \(\mathsf{W}_{p}(\hat{\mu},\mu)\leq c\rho+2\tau_{p}(\mathcal{G},2\varepsilon)\), and so Lemma 7 gives the desired result. 

In practice, we are unaware of efficient finite-sample algorithms to compute \(\hat{\mu}\). For the class \(\mathcal{G}_{\mathrm{cov}}\), we instead propose the spectral reweighing estimate \(\hat{\mu}=\hat{\mu}(\tilde{\mu},\varepsilon)\coloneqq\operatorname*{argmin}_{ \nu\in\mathcal{P}_{2}(Z),\,\nu\leq\frac{1}{1-2\varepsilon}\hat{\mu}}\|\Sigma_{ \nu}\|_{\mathrm{op}}\), where \(\|\cdot\|_{\mathrm{op}}\) is the matrix operator norm (see [26] for varied applications of spectral reweighing). In practice, when \(\tilde{\mu}=\tilde{\mu}_{n}\) is an \(n\)-sample empirical measure, one can efficiently obtain a feasible measure \(\nu\) whose objective value is optimal up to constant factors for the problem with \(\varepsilon\gets 3\varepsilon\), using the iterative filtering algorithm [15]. We work with the exact minimizer \(\tilde{\mu}\) for convenience, but our results are robust to such approximate solutions.

**Lemma 15**.: _Under Setting A with \(\mathcal{G}=\mathcal{G}_{\mathrm{cov}}\), \(p=1\), and \(0<\varepsilon\leq 0.2\), we have \(\mathsf{W}_{1}(\tilde{\mu},\mu)\lesssim\sqrt{d}\rho+\sqrt{d\varepsilon}\), and this bound is tight; that is, there exists an instance \((\mu,\tilde{\mu})\in\mathcal{G}_{\mathrm{cov}}\times\mathcal{P}(\mathbb{R}^{d})\) with \(\mathsf{W}_{1}^{e}(\mu,\tilde{\mu})\leq\rho\) such that \(\mathsf{W}_{1}(\tilde{\mu},\mu)\gtrsim\sqrt{d}\rho+\sqrt{d\varepsilon}\). Consequently, the WDRO estimate \(\hat{\ell}=\operatorname*{argmin}_{\ell\in\mathcal{E}}\mathsf{sup}_{\nu\in \mathcal{P}(Z):\mathsf{W}_{1}(\nu,\tilde{\mu})\leq\tilde{\rho}}\mathbb{E}_{ \nu}[\ell]\) satisfies \(\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}]\leq\mathcal{R}_{ \mu,1}(O(\sqrt{d}\rho+\sqrt{d\varepsilon});\ell_{\star})\)._

Proof.: **Upper bound:** For the upper bound on \(\mathsf{W}_{1}\) estimation error, fix any \(\tilde{\mu}\in\mathcal{P}(\mathbb{R}^{d})\) with \(\mathsf{W}_{1}^{e}(\tilde{\mu},\mu)\leq\rho\). Take any \(\mu^{\prime}\in\mathcal{P}(\mathbb{R}^{d})\) such that \(\mathsf{W}_{1}(\mu^{\prime},\mu)\leq\rho\) and \(\|\mu^{\prime}-\tilde{\mu}\|_{\mathsf{TV}}\leq\varepsilon\). By Lemma 13, there exists \(\alpha\in\mathcal{P}(\mathbb{R}^{d})\) with \(\mathsf{W}_{1}(\alpha,\mu)\leq\rho\), \(\mathsf{W}_{2}(\alpha,\mu)\leq\rho\sqrt{2/\varepsilon}\), and \(\|\alpha-\tilde{\mu}\|_{\mathsf{TV}}\leq 2\varepsilon\). Fixing an optimal coupling \(\pi\in\Pi(\alpha,\mu)\) for the \(\mathsf{W}_{2}(\alpha,\mu)\) problem and letting \((Z,W)\sim\pi\), we bound

\[\|\Sigma_{\alpha}\|_{\mathrm{op}}^{\frac{1}{2}}=\sup_{\theta\in\mathbb{S}^{d-1 }}\mathbb{E}\big{[}\theta^{\top}(Z-\mathbb{E}[Z])^{2}\big{]}^{\frac{1}{2}} \leq\sup_{\theta\in\mathbb{S}^{d-1}}\mathbb{E}\big{[}\theta^{\top}(Z-\mathbb{E }[W])^{2}\big{]}^{\frac{1}{2}}\leq\|\Sigma_{\mu}\|_{\mathrm{op}}^{\frac{1}{2} }+\mathsf{W}_{2}(\alpha,\mu)\]

Thus, \(\alpha\in\mathcal{G}_{\mathrm{cov}}(1+\rho\sqrt{2/\varepsilon})\). Write \(\beta\coloneqq\frac{1}{(\alpha\wedge\tilde{\mu})(\mathbb{S}^{d})}\alpha\wedge \tilde{\mu}\), and note that this midpoint measure is feasible for the problem defining \(\tilde{\mu}\). Hence, we have

\[\|\Sigma_{\tilde{\mu}}\|_{\mathrm{op}}\leq\|\Sigma_{\beta}\|_{\mathrm{op}}\leq \sup_{\theta\in\mathbb{S}^{d-1}}\mathbb{E}_{\beta}\big{[}(\theta^{\top}(Z- \mathbb{E}_{\alpha}[Z]))^{2}\big{]}\leq\frac{1}{1-2\varepsilon}\|\Sigma_{ \alpha}\|_{\mathrm{op}}\leq\frac{1}{1-2\varepsilon}(1+\rho\sqrt{2/\varepsilon})^{ 2},\]

and so \(\tilde{\mu}\in\mathcal{G}_{\mathrm{cov}}\big{(}(1-2\varepsilon)^{-1/2}(1+\rho \sqrt{2/\varepsilon})\big{)}\). Moreover, we have \(\|\tilde{\mu}-\alpha\|_{\mathsf{TV}}\leq 4\varepsilon\). Thus, using Lemma 5 and the fact that \(4\varepsilon\) is bounded away from 1, we bound

\[\mathsf{W}_{1}(\tilde{\mu},\alpha)\leq\mathsf{W}_{1}\Big{(}\tilde{\mu},\tfrac{1 }{(\tilde{\mu}\wedge\alpha)(\mathbb{R}^{d})}\tilde{\mu}\wedge\alpha\Big{)}+ \mathsf{W}_{1}\Big{(}\tfrac{1}{(\tilde{\mu}\wedge\alpha)(\mathbb{R}^{d})} \tilde{\mu}\wedge\alpha\Big{)}\lesssim\sqrt{d}\rho+\sqrt{d\varepsilon}.\]

By the triangle inequality, we have \(\mathsf{W}_{1}(\tilde{\mu},\mu)\lesssim\sqrt{d}\rho+\sqrt{d\varepsilon}\). The risk bound follows by Lemma 7. Taking the final error measurement using distance between means instead of \(\mathsf{W}_{1}\), we observe that \(\|\mathbb{E}_{\tilde{\mu}}[Z]-\mathbb{E}_{\mu}[Z]\|_{2}\lesssim\rho+\sqrt{\varepsilon}\).

**Lower bound:** To see that this guarantee cannot be improved, fix clean measure \(\mu=\delta_{0}\in\mathcal{G}_{\mathrm{cov}}\), and consider the corrupted measure \(\tilde{\mu}=(1-3\varepsilon)\delta_{0}+2\varepsilon\big{(}\frac{1}{2}\delta_{ \frac{\rho}{2}\varepsilon_{\star}\mathbf{e}_{1}}+\frac{1}{2}\delta_{-\frac{ \rho}{2}\varepsilon_{\star}\mathbf{e}_{1}}\big{)}+\varepsilon\mathcal{N}\big{(} 0,\tfrac{\rho^{2}}{100\varepsilon^{2}}I_{d}\big{)}\), constructed so that \(\mathsf{W}_{1}^{e}(\tilde{\mu},\mu)\leq\rho\). Intuitively, iterative filtering seeks to drive down the operator norm of covariance matrix and will thus focus on removing mass from the second mixture component.

To formalize this, we decompose the output of filtering as \((1-2\varepsilon)\tilde{\mu}=(1-3\varepsilon-\tau)\delta_{0}+\alpha+\beta\), where \(0\leq\tau\leq 2\varepsilon\) and \(\alpha,\beta\in\mathcal{M}_{+}(\mathbb{R}^{d})\) such that \(\alpha\leq 2\varepsilon\big{(}\frac{1}{2}\delta_{\frac{\rho}{2}\varepsilon_{\star} \mathbf{e}_{1}}+\frac{1}{2}\delta_{-\frac{\rho}{2}\varepsilon_{\star}}\mathbf{e}_{1} \big{)}\) and \(\beta\leq\varepsilon\mathcal{N}\big{(}0,\tfrac{\rho^{2}}{100\varepsilon^{2}}I_{d} \big{)}\). We have \(\tau+(2\varepsilon-\alpha(\mathbb{R}^{d}))+(\varepsilon-\beta(\mathbb{R}^{d}))=2\varepsilon\) by the definition of \(\tilde{\mu}\). Further note that \(\sqrt{\varepsilon}+\rho\lesssim\rho\), by the bounds above. Now suppose for sake of contradiction that \(\beta(\mathbb{R}^{d})\leq\varepsilon/2\). Then \(\alpha(\mathbb{R}^{d})\geq\varepsilon/2\), and so we bound

\[\|\Sigma_{\hat{\mu}}\|_{\mathrm{op}}^{\frac{1}{2}} \geq\sqrt{\mathbb{E}_{\hat{\mu}}[Z_{1}^{2}]}-\|\,\mathbb{E}_{\hat{ \mu}}[Z]\|\] \[\geq\sqrt{\frac{\varepsilon}{2(1-2\varepsilon)}\frac{\rho^{2}}{4 \varepsilon^{2}}}-\|\,\mathbb{E}_{\hat{\mu}}[Z]\|\] \[\geq\frac{\rho}{\sqrt{8\varepsilon}}-O(\rho).\]

On the other hand, another feasible outcome for spectral reweighing is \(\mu^{\prime}=\frac{1}{1-\varepsilon}(1-3\varepsilon)\delta_{0}+\varepsilon \mathcal{N}\big{(}0,\frac{\rho^{2}}{100\varepsilon^{2}}I_{d}\big{)}\), for which we have \(\|\Sigma_{\mu^{\prime}}\|_{\mathrm{op}}^{1/2}=\frac{\rho}{10\sqrt{\varepsilon}}\). Since \(10>\sqrt{8}\), this contradicts optimality of \(\hat{\mu}\) if \(\varepsilon\leq c\rho^{2}\) for a sufficiently small constant \(c\). However, if \(\varepsilon>c\rho^{2}\), then a lower bound of \(\Omega(\sqrt{d\varepsilon})\) suffices. This bound holds even without Wasserstein perturbations; see the \(\mathcal{G}_{\mathrm{cov}}\) lower risk bound of \(\Omega(\sqrt{d\varepsilon})\) in Theorem 2 of [38].

We now suppose that \(\beta(\mathbb{R}^{d})\geq\varepsilon/2\). Let \(Z\sim\mathcal{N}\big{(}0,\frac{\rho^{2}}{100\varepsilon^{2}}\big{)}\) and write \(F\) for the CDF of \(\|Z\|^{2}\) (which has a scaled \(\chi_{d}^{2}\) distribution). We then have \(\int\|z\|\,\mathrm{d}\beta(z)\geq\varepsilon\,\mathbb{E}\big{[}\|Z\|\mid\|Z\|^{ 2}\leq F^{-1}(1/2)\big{]}\gtrsim\sqrt{d}\rho\), using concentration of \(\chi_{d}^{2}\) about its mean. Thus, \(\mathsf{W}_{1}(\hat{\mu},\mu)\geq\mathbb{E}_{\hat{\mu}}[\|Z\|]-\mathbb{E}_{\mu }[\|Z\|]\gtrsim\sqrt{d}\rho\). 

## Appendix E Smaller Robustness Radius for Outlier-Robust WDRO (Remark 2)

In the classical WDRO setting with \(\rho_{0}=\varepsilon=0\), the radius \(\rho\) can often be taken significantly smaller than \(n^{-1/d}\) if \(\mathcal{L}\) and \(\mu\) are sufficiently well-behaved. In particular, when \(\mu\) satisfies a \(T_{2}\) transportation inequality, [18] proves that \(\rho=\widetilde{O}(n^{-1/2})\) gives meaningful risk bounds. Recall that \(\mu\in T_{2}(\tau)\) if

\[\mathsf{W}_{2}(\nu,\mu)\leq\sqrt{\tau\mathsf{H}(\nu\|\mu)},\quad\forall\nu\in \mathcal{P}_{2}(\mathcal{Z}),\]

where \(\mathsf{H}(\nu\|\mu)\coloneqq\int_{\mathcal{Z}}\log(d\nu/d\mu)d\nu\) denotes relative entropy when \(\nu\ll\mu\) (and is \(+\infty\) otherwise). We note that \(T_{2}\) is implied by the log-Sobolev inequality, which holds for example when \(\mu\) has strongly log-concave density. Under \(T_{2}\), [18] shows the following.

**Proposition 11** (Example 3 in [18]).: _Fix \(\mathcal{Z}=\mathbb{R}^{d}\times\mathbb{R}\), \(\tau,B>0\), and an \(\alpha\)-smooth and \(L\)-Lipschitz function \(f:\mathbb{R}\to\mathbb{R}\). Consider the parameterized family of loss functions \(\mathcal{L}=\{(x,y)\mapsto\ell_{\theta}(x,y)=f(\theta^{\top}x-y):\theta\in\Theta\}\), where \(\Theta\subset\{\theta\in\mathbb{R}^{d}:\|\theta\|\leq B\}\). Fix \(\mu\in\mathcal{P}(\mathcal{Z})\) whose first marginal \(\mu_{X}=\mu(\cdot\times\mathbb{R})\) satisfies \(\mu_{X}\in T_{2}(\tau)\) and such that \(\inf_{\theta\in\Theta}\mathbb{E}_{\mu}[f^{\prime}(\theta^{\top}X-Y)^{2}]>0\). Write_

\[\sigma=\sup_{\theta\in\Theta}\frac{\mathbb{E}_{\mu}[f^{\prime}(\theta^{\top}X,Y)^{4}]^{\frac{1}{2}}}{\mathbb{E}_{\mu}[f^{\prime}(\theta^{\top}X,Y)^{2}]} \leq\frac{L^{2}}{\inf_{\theta\in\Theta}\mathbb{E}_{\mu}[f^{\prime}(\theta^{\top} X,Y)^{2}]}<\infty.\]

_For \(t>0\), define_

\[\rho_{n} =\sqrt{\frac{\tau t\big{(}1+d\log(2+2Bn)\big{)}}{n}}\Bigg{(}1+ \sigma\sqrt{\frac{2t(1+d\log(2+2Bn))}{n}}\Bigg{)},\] \[\delta_{n} =\frac{2L+2B\alpha\mathbb{E}_{\mu}[\|X\|]+B^{2}\alpha^{2}\mathrm{ Var}_{\mu}(\|X\|)+\rho_{n}\sqrt{\mathbb{E}_{\mu}\big{[}(L+B\alpha\|X\|)^{2}\big{]}+ \mathrm{Var}_{\mu}\big{(}(L+B\alpha\|X\|)^{2}\big{)}}}{n},\] \[\eta_{n} =\frac{2\alpha B^{2}\tau t\big{(}1+d\log(2+2Bn)\big{)}}{n}.\]

_Then, with probability at least \(1-2/n-2e^{-t}\), we have_

\[|\mathbb{E}_{\mu}[\ell_{\theta}]-\mathbb{E}_{\hat{\mu}_{n}}[\ell_{\theta}]|\leq \mathcal{R}_{\hat{\mu},2}(\rho_{n};\ell_{\theta})+\delta_{n}+\eta_{n}\quad \forall\theta\in\Theta.\] (11)

We note that (11) is stated without the absolute value on the right hand side, but that this strengthened result holds due to the discussion after [18, Theorem 1]. This generalization bound immediately gives the following excess risk bound.

**Corollary 5**.: _Assume \(n\geq 800\). Fix \(\rho_{n}\), \(\delta_{n}\), \(\eta_{n}\), and \(\mathcal{L}\) as in Proposition 11 with \(t=7\), and take \(\ell_{\hat{\theta}}\in\mathcal{L}\) minimizing (1) with \(p=2\) and radius \(\rho=\rho_{n}\). Then, with probability at least \(0.99\), we have_

\[\mathbb{E}_{\mu}[\ell_{\hat{\theta}}]-\mathbb{E}_{\mu}[\ell_{\theta}]\lesssim \rho_{n}\|\ell_{\theta}\|_{\hat{H}^{1,2}(\mu)}+\alpha\rho_{n}^{2}+\delta_{n}+ \eta_{n}\quad\forall\theta\in\Theta.\]

Proof.: By Proposition 11 and [18, Remark 1], we have

\[\mathbb{E}_{\mu}[\ell_{\hat{\theta}}]-\mathbb{E}_{\mu}[\ell_{\theta}]\leq 2 \mathcal{R}_{\hat{\mu}_{n},2}(\rho_{n};\ell_{\theta})+2\delta_{n}+2\eta_{n} \quad\forall\theta\in\Theta,\]

with probability at least \(1-2/n-2e^{-t}\geq 0.995\). Since \(\ell_{\theta}\) is \(\alpha\)-smooth, Lemma 6 gives that

\[\mathbb{E}_{\mu}[\ell_{\hat{\theta}}]-\mathbb{E}_{\mu}[\ell_{\theta}]\leq 2 \rho_{n}\|\ell_{\theta}\|_{\hat{H}^{1,2}(\hat{\mu}_{n})}+2\alpha\rho_{n}^{2}+2 \delta_{n}+2\eta_{n}\quad\forall\theta\in\Theta,\]

with probability at least 0.995. By Markov's inequality, we can substitute \(\hat{\mu}_{n}\) with \(\mu\) at the cost of a constant factor blow-up in excess risk and a decrease in the confidence probability to, say, 0.99. 

In the example above, excess risk is controlled by the 2-Wasserstein regularizer with radius \(\rho_{n}=O(n^{-1/2})\), up to \(O(n^{-1})\) correction terms, which is significantly smaller that the typical radius size of \(O(n^{-1/d})\). We shall now lift this improvement to the outlier-robust setting. Similar to Proposition 4, we perform outlier-robust DRO with a modified choice of \(\mathcal{A}\). This time, writing \(\mathcal{G}_{2}(\sigma)=\cup_{z_{0}\in\mathcal{Z}}\mathcal{V}_{2}(\sigma,z_{0})\), we have the following.

**Proposition 12** (Outlier-robust WDRO under \(T_{2}\)).: _Assume \(n\geq 800\) and \(\mu\in\mathcal{G}_{\mathrm{cov}}\). Fix \(\rho_{n}\), \(\delta_{n}\), \(\eta_{n}\), and \(\mathcal{L}\) as in Proposition 11 with \(t=8\), and take \(\ell_{\hat{\theta}}\) minimizing (OR-WDRO) with center \(\hat{\mu}_{n}\), radius \(\rho=\rho_{0}+15\rho_{n}+200\sqrt{d}\), and \(\mathcal{A}=\mathcal{G}_{2}(15\sqrt{d}+\rho_{n})\). Then, with probability at least 0.99, we have_

\[\mathbb{E}_{\mu}[\ell_{\hat{\theta}}]-\mathbb{E}_{\mu}[\ell_{\theta}]\lesssim \|\ell_{\theta}\|_{\hat{H}^{1,2}(\mu)}\big{(}\rho_{0}+\rho_{n}+\sqrt{d}\big{)} +\alpha\big{(}\rho_{0}+\rho_{n}+\sqrt{d}\big{)}^{2}+\delta_{n}+\eta_{n}\quad \forall\theta\in\Theta.\]

Proof.: Noting that \(\mathcal{G}_{\mathrm{cov}}\subseteq\mathcal{G}_{2}(\sqrt{d})\), we have by Markov's inequality that \(\hat{\mu}_{n}\in\mathcal{G}_{2}(15\sqrt{d})\) with probability at least 0.995. In other words, there exists \(z_{0}\in\mathcal{Z}\) such that \(\mathsf{W}_{2}(\hat{\mu}_{n},\delta_{z_{0}})\leq 15\sqrt{d}\). Thus, for any \(\nu\in\mathcal{P}(\mathcal{Z})\) with \(\mathsf{W}_{2}(\hat{\mu}_{n},\nu)\leq\rho_{n}\), we have \(\mathsf{W}_{2}(\nu,\delta_{z_{0}})\leq 15\sqrt{d}+\rho_{n}\), and so \(\nu\in\mathcal{G}_{2}(15\sqrt{d}+\rho_{n})\). By Lemmas 4 and 5, this implies that

\[\mathsf{W}_{2}^{\varepsilon}(\tilde{\mu}_{n}\|\nu) \leq\mathsf{W}_{2}^{\varepsilon}(\tilde{\mu}_{n},\nu)+\tau_{2}( \nu,\varepsilon)\] \[\leq\rho_{0}+\rho_{n}+8(15\sqrt{d}+\rho_{n})(1-\varepsilon)^{-1/2}\] \[<\rho.\]

Next, by Proposition 11, with probability at least \(1-1/400+2e^{-8}\geq 0.9985\), we have for each \(\theta\in\Theta\) that

\[\mathbb{E}_{\mu}[\ell_{\hat{\theta}}]-E_{\mu}[\ell_{\theta}] \leq\mathbb{E}_{\hat{\mu}_{n}}[\ell_{\hat{\theta}}]+\mathcal{R}_{ \hat{\mu}_{n},2}(\rho_{n};\ell_{\hat{\theta}})-\mathbb{E}_{\mu}[\ell_{\theta}] +\delta_{n}+\eta_{n}\] \[=\mathbb{E}_{\hat{\mu}_{n}}[\ell_{\hat{\theta}}]+\left(\sup_{ \begin{subarray}{c}\nu\in\mathcal{P}(2)\\ \mathsf{W}_{2}(\hat{\mu}_{n},\nu)\leq\rho_{n}\end{subarray}}\mathbb{E}_{\nu}[ \ell_{\hat{\theta}}]-\mathbb{E}_{\hat{\mu}_{n}}[\ell_{\hat{\theta}}]\right)- \mathbb{E}_{\mu}[\ell_{\theta}]+\delta_{n}+\eta_{n}\] \[=\mathbb{E}_{\hat{\mu}_{n}}[\ell_{\hat{\theta}}]+\left(\sup_{ \begin{subarray}{c}\nu\in\mathcal{G}_{2}(15\sqrt{d}+\rho_{n})\\ \mathsf{W}_{2}(\hat{\mu}_{n},\nu)\leq\rho_{n}\end{subarray}}\mathbb{E}_{\nu}[ \ell_{\hat{\theta}}]-\mathbb{E}_{\hat{\mu}_{n}}[\ell_{\hat{\theta}}]\right)- \mathbb{E}_{\mu}[\ell_{\theta}]+\delta_{n}+\eta_{n}\] \[\leq\mathbb{E}_{\hat{\mu}_{n}}[\ell_{\hat{\theta}}]+\left(\sup_{ \begin{subarray}{c}\nu\in\mathcal{G}_{2}(15\sqrt{d}+\rho_{n})\\ \mathsf{W}_{2}^{\varepsilon}(\hat{\mu}_{n}|\nu)\leq\rho\end{subarray}}\mathbb{E}_{ \nu}[\ell_{\hat{\theta}}]-\mathbb{E}_{\hat{\mu}_{n}}[\ell_{\hat{\theta}}]\right)- \mathbb{E}_{\mu}[\ell_{\theta}]+\delta_{n}+\eta_{n}.\]

Let \(c=2\big{(}\frac{1-\varepsilon}{1-2\varepsilon}\big{)}^{1/p}\). Using optimality of \(\hat{\ell}\) and Lemma 2, we further bound\[\mathbb{E}_{\mu}[\ell_{\hat{\theta}}]-E_{\mu}[\ell_{\theta}] \leq\left(\sup_{\nu\in\mathcal{G}_{2}(15\sqrt{d}+\rho_{n})\atop\mathsf{ W}_{2}^{\ell}(\hat{\mu}_{n}\|\nu)\leq\rho}\mathbb{E}_{\nu}[\ell_{\theta}]- \mathbb{E}_{\hat{\mu}_{n}}[\ell_{\theta}]\right)+\mathbb{E}_{\hat{\mu}_{n}}[ \ell_{\theta}]-\mathbb{E}_{\mu}[\ell_{\theta}]+\delta_{n}+\eta_{n}\] \[\leq\left(\sup_{\nu\in\mathcal{G}_{2}(15\sqrt{d}+\rho_{n})\atop \mathsf{W}_{2}^{\ell}(\hat{\mu}_{n}\|\nu)\leq\rho}\mathbb{E}_{\nu}[\ell_{\theta }]-\mathbb{E}_{\hat{\mu}_{n}}[\ell_{\theta}]\right)+\mathcal{R}_{\hat{\mu}_{n},2}(\rho_{n};\ell_{\theta})+2\delta_{n}+2\eta_{n}\] \[\leq\left(\sup_{\nu\in\mathcal{G}_{2}(15\sqrt{d}+\rho_{n})\atop \mathsf{W}_{2}^{\ell}(\hat{\mu}_{n},\nu)\leq\rho}\mathbb{E}_{\nu}[\ell_{ \theta}]-\mathbb{E}_{\hat{\mu}_{n}}[\ell_{\theta}]\right)+\mathcal{R}_{\hat{ \mu}_{n},2}(\rho_{n};\ell_{\theta})+2\delta_{n}+2\eta_{n}\] \[\leq\left(\sup_{\nu\in\mathcal{G}_{2}(15\sqrt{d}+\rho_{n})\atop \mathsf{W}_{2}(\nu,\hat{\mu}_{n})\leq\varepsilon\rho+\tau_{2}(\hat{\mu}_{n},2 \varepsilon)+\tau_{2}(\mathcal{A},2\varepsilon)}\right)+\mathcal{R}_{\hat{\mu} _{n},2}(\rho_{n};\ell_{\theta})+2\delta_{n}+2\eta_{n}\] \[\leq\mathcal{R}_{\hat{\mu}_{n},2}\big{(}c\rho+\tau_{2}(\hat{\mu} _{n},2\varepsilon)+\tau_{2}(\mathcal{A},2\varepsilon);\ell_{\theta}\big{)}+ \mathcal{R}_{\hat{\mu}_{n},2}(\rho_{n};\ell_{\theta})+2\delta_{n}+2\eta_{n}\] \[\leq\|\ell_{\theta}\|_{\hat{H}^{1,2}(\hat{\mu}_{n})}\big{(}\rho_{ 0}+\rho_{n}+\sqrt{d}\big{)}+\alpha\big{(}\rho_{0}+\rho_{n}+\sqrt{d}\big{)}^{2} +\delta_{n}+\gamma_{n}.\]

As in Corollary 5, we can substitute \(\hat{\mu}_{n}\) with \(\mu\) at the cost of a constant factor increase in excess risk along with a small decrease in the confidence probability (in this case, sufficiently small such that the total failure probability is at most \(0.01\)). 

The main goal of Proposition 12 was to demonstrate that one can expect improved excess risk bounds for outlier-robust WDRO in situations where such improvements hold for standard WDRO. We conjecture that similar guarantees hold for additional settings and under milder assumptions like the \(T_{1}\) inequality, but leave such refinements for future work. In particular, for the class \(\mathcal{G}_{\mathrm{cov}}\), it would be desirable to prove such bounds when \(p=1\), so that the TV contribution to the risk vanishes as \(\varepsilon\to 0\).

## Appendix F Parameter Tuning (Remark 5)

To clarify the parameter selection process, we consider Setting B with the class \(\mathcal{G}=\mathcal{G}_{\mathrm{cov}}(\sigma)\), \(p=1\), and \(\varepsilon\leq 1/3\). We aim to efficiently achieve excess risk

\[\mathbb{E}_{\mu}[\hat{\ell}]-\mathbb{E}_{\mu}[\ell_{\star}]\lesssim\|\ell_{ \star}\|_{\mathrm{Lip}}\big{(}\rho_{0}+\sigma\sqrt{d\varepsilon}+\sigma\sqrt{d} n^{-1/d}\big{)},\] (12)

matching Proposition 4, when

\[\hat{\ell}=\operatorname*{argmin}_{\ell\in\mathcal{L}}\sup_{\nu\in\mathcal{G} _{2}(\hat{\nu},z_{0}):\,\mathsf{W}_{1}^{\ell}(\hat{m}_{n},\nu)\leq\hat{\rho}} \mathbb{E}_{\nu}[\ell]\] (13)

for some parameter guesses \(\hat{\sigma}\), \(\hat{\varepsilon}\), and \(\hat{\rho}\), and a robust mean estimate \(z_{0}\). First, we observe that the coordinate-wise trimmed mean estimate from Proposition 5 is computed without knowledge of the parameters, so it is safe to assume that \(\|z_{0}-\mathbb{E}_{\mu}[Z]\|\lesssim\sqrt{d}+\rho_{0}\). If the parameter guesses are conservative, i.e., \(\hat{\sigma}\geq\sigma\), \(\hat{\varepsilon}\geq\varepsilon\), and \(\hat{\rho}\geq\rho_{0}+\mathsf{W}_{1}(\mu,\hat{\mu}_{n})\), then we may still employ Proposition 4. If they are not too large, i.e., \(\hat{\sigma}\lesssim\sigma\), \(\hat{\varepsilon}\lesssim\varepsilon\), and \(\hat{\rho}\leq\rho_{0}+\sigma\sqrt{d}n^{-1/d}\), this gives the desired excess risk.

We now explore what prior knowledge of \(\sigma\), \(\varepsilon\), and \(\rho_{0}\) is needed to obtain such guesses. First, we show that effective learning is impossible without knowledge of \(\rho_{0}\), even for standard WDRO (i.e., \(\varepsilon=0\), \(\sigma=\infty\)). For ease of presentation, we present the following lower bound without sampling.

**Lemma 16**.: _There exists a family of loss functions \(\mathcal{L}\) over \(\mathbb{R}\) such that, for any \(C>0\) and decision rule \(\mathsf{D}:\mathcal{P}(\mathbb{R})\to\mathcal{L}\), there are \(\mu,\tilde{\mu}\in\mathcal{P}(\mathbb{R})\) such that \(\mathbb{E}_{\mu}[\mathsf{D}(\tilde{\mu})]>C(\inf_{\ell\in\mathcal{L}}\mathbb{E }_{\mu}[\ell]+\mathsf{W}_{1}(\mu,\tilde{\mu})\|\ell\|_{\mathrm{Lip}})\)._

Proof.: Let \(\mathcal{L}=\{\ell_{\theta}:\theta>0\}\), where \(\ell_{\theta}(z)\coloneqq z/\theta+\theta\). By design, we have \(\|\ell_{\theta}\|_{\mathrm{Lip}}=1/\theta\). Let \(\tilde{\mu}=\delta_{0}\) and write \(\mathsf{D}(\tilde{\mu})=\ell_{\hat{\theta}}\) for some \(\hat{\theta}>0\). We then set \(\mu=\delta_{\rho}\) for \(\rho=10\hat{\theta}^{2}C^{2}\). This gives

\[\mathbb{E}_{\mu}[\mathsf{D}(\tilde{\mu})]=\ell_{\hat{\theta}}(\rho)=\frac{\rho} {\hat{\theta}}+\hat{\theta}>\frac{\rho}{\hat{\theta}}=10C^{2}\hat{\theta},\]\[\inf_{\ell\in\mathcal{L}}\mathbb{E}_{\mu}[\ell]+\mathsf{W}_{1}(\mu,\tilde{\mu})\| \ell\|_{\text{Lip}}=\inf_{\theta>0}\ell_{\theta}(\rho)+\frac{\rho}{\theta}=\inf_ {\theta>0}\frac{2\rho}{\theta}+\theta=2\sqrt{2\rho}=\sqrt{80}C\hat{\theta}.\]

Thus, we have \(\mathbb{E}_{\mu}[\mathsf{D}(\tilde{\mu})]>\inf_{\ell\in\mathcal{L}}\mathbb{E}_ {\mu}[\ell]+\mathsf{W}_{1}(\mu,\tilde{\mu})\|\ell\|_{\text{Lip}}\), as desired. 

Thus, we assume in what follows that \(\hat{\rho}=\rho_{0}\) is known. Moreover, we require knowledge of at least one of \(\varepsilon\) and \(\sigma\). If both are unknown, then is information theoretically impossible to meaningfully distinguish inliers from outliers (see Exercise 1.7b of [16] for a discussion of this issue in the setting of robust mean estimation). If \(\varepsilon\) is known, then we can choose \(\hat{\sigma}\) as \(2^{i}\) for the smallest \(i\) such that the supremum of Eq. (13) is feasible (or, equivalently, such that the associated dual is bounded for some fixed \(\ell\in\mathcal{L}\)). We can overshoot by at most a factor of two and thus achieve the desired risk bound. Using binary search, this adds a multiplicative overhead logarithmic in the ratio of the initial guess for \(\sigma\) and its true value. The same approach can be employed if \(\sigma\) is known but not \(\varepsilon\).

## Appendix G Additional Experiments

We now provide several experiments in addition to those in the main body. Code is again provided at https://github.com/sbnietert/outlier-robust-WDRO.

First, in Fig. 3, we extend the experiment summarized Fig. 2 (top) to include runs with \(\hat{\varepsilon}=\varepsilon\) and varied Wasserstein radius \(\hat{\rho}\in\{\rho/2,\rho,2\rho\}\). For this simple learning problem and perturbation model, we find that the choice of \(\hat{\rho}\) plays a minor role in the resulting excess risk. We emphasize that, in the worst case, selection of \(\hat{\rho}\) can be critical, as demonstrated by Lemma 16.

Next, we consider linear classification with the hinge loss, i.e. \(\mathcal{L}=\{\ell_{\theta}(x,y)=\max\{0,1-y(\theta^{\top}x)\}:\theta\in \mathbb{R}^{d-1}\}\). This time (to ensure that the resulting optimization problem is convex), our approach supports Euclidean Wasserstein perturbations in the feature space, but no Wasserstein perturbations in the label space (this corresponds to using \(\mathcal{Z}=\mathbb{R}^{d-1}\times\mathbb{R}\) equipped with the (extended) norm \(\|(x,y)\|=\|x\|_{2}+\infty\cdot\mathds{1}\{y\neq 0\}\). We consider clean data \((X,Y=\operatorname{sign}(\theta_{\star}^{\top}X))\sim\mu\), where \(X\sim\mathcal{N}(0,I_{d-1})\). The corrupted data \((\tilde{X},\tilde{Y})\sim\tilde{\mu}\) satisfies \((\tilde{X},\tilde{Y})=(X+\rho e_{1},Y)\) with probability \(1-\varepsilon\) and \((\tilde{X},\tilde{Y})=(100X,-Y)\) with probability \(\varepsilon\), so that \(\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}\|\mu)\leq\rho\). In Figure 4 (left), we fix \(d=10\) and compare the excess risk \(\mathbb{E}_{\mu}[\ell_{\hat{\theta}}^{\top}]-\mathbb{E}_{\mu}[\ell_{\theta_{ \star}}]\) of standard WDRO and outlier-robust WDRO with \(\mathcal{G}=\mathcal{G}_{2}\), as described by Proposition 4 and implemented via Theorem 2. The results are averaged over \(T=20\) runs for sample size \(n\in\{10,20,50,75,100\}\). We note that this example cannot drive the excess risk of standard WDRO to infinity, so the separation between standard and outlier-robust WDRO is less striking than regression, though still present.

We further present results for multivariate regression. This time, we consider \(\mathcal{Z}=\mathbb{R}^{d\times k}\) equipped with the \(\ell_{2}\) norm, and use the loss family \(\mathcal{L}=\{\ell_{M}(x,y)=\|Mx-y\|_{1}:M\in\mathbb{R}^{k\times d}\}\). We consider clean data \((X,Y=M_{\star}^{\top}X)\sim\mu\), where \(M_{\star}\in\mathbb{R}^{k\times d}\) and \(X\) have standard normal entries. The corrupted data \((\tilde{X},\tilde{Y})\sim\tilde{\mu}\) satisfies \((\tilde{X},\tilde{Y})=(X+\rho e_{1},Y)\) with probability \(1-\varepsilon\) and \((\tilde{X},\tilde{Y})=(10X,-100M_{\star}X)\) with probability \(\varepsilon\), so that \(\mathsf{W}_{p}^{\varepsilon}(\tilde{\mu}\|\mu)\leq\rho\). In Figure 4 (right), we fix \(d=10\) and \(k=3\), and compare the excess risk \(\mathbb{E}_{\mu}[\ell_{\hat{M}}]-\mathbb{E}_{\mu}[\ell_{M_{\star}}]\) of standard WDRO and outlier-robust WDRO with \(\mathcal{G}=\mathcal{G}_{2}\), as described by Proposition 4 and implemented via Theorem 2. The results are averaged over \(T=10\) runs for sample size \(n\in\{10,20,50,75,100\}\). We are restricted to low \(k\) since the \(\ell_{1}\) norm in the losses is expressed as the maximum of \(2^{k}\) concave functions (specifically, we use that \(\ell_{M}(x,y)=\max_{\alpha\in\{-1,1\}^{k}}\alpha^{\top}(Mx-y)\)).

Finally, we turn to a classification task with image data. We train a robust linear classifier to distinguish

Figure 3: Excess risk of standard WDRO and outlier-robust WDRO for linear regression under \(\mathsf{W}_{p}\) and TV corruptions, with varied sample size and dimension.

between the MNIST [14] digits "3" and "8" when 10% of training labels are flipped uniformly at random, using outlier-robust WDRO with \(\mathcal{G}=\mathcal{G}_{2}\) and the hinge loss, as above, applied with \(\varepsilon=0.1\) and \(\rho=0.01\). To ensure tractability, we first pass the images through principal component analysis to reduce the input to 50 dimensions, and we again use Theorem 2 for implementation. In Fig. 5, we plot the classification accuracy of our robust classifier compared to one learned via standard WDRO for training set size \(n\in\{10,20,50,100,150\}\), averaged over \(T=10\) runs. In this case, we do not witness a noticeable improvement over standard WDRO. We suspect that the relevant \(\mathcal{G}_{2}\) class is too large to be of significant use in this high-dimensional setting.

For all experiments, confidence bands are plotted representing the top and bottom 10% quantiles among 100 bootstrapped means from the \(T\) runs. The additional experiments were performed on an M1 Macbook Air with 16GB RAM in roughly 30 minutes each.

Figure 4: Excess risk of standard WDRO and outlier-robust WDRO for classification and multivariate linear regression under W\({}_{p}\) and TV corruptions, with varied sample size.

Figure 5: Classification accuracy of standard WDRO and outlier-robust WDRO predictors for MNIST digit classification under random label flips, with varied number of training samples.