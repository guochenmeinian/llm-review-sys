Leveraging Environment Interaction for Automated PDDL Translation and Planning with Large Language Models

Sadegh Mahdavi

University of British Columbia

smahdavi@ece.ubc.ca

&Raquel Aoki

Borealis AI

raquel.aoki@borealisai.com

&Keyi Tang

Borealis AI

keyi.tang@borealisai.com

&Yanshuai Cao

Borealis AI

yanshuai.cao@borealisai.com

Work performed while interning at Borealis AI.

###### Abstract

Large Language Models (LLMs) have shown remarkable performance in various natural language tasks, but they often struggle with planning problems that require structured reasoning. To address this limitation, the conversion of planning problems into the Planning Domain Definition Language (PDDL) has been proposed as a potential solution, enabling the use of automated planners. However, generating accurate PDDL files typically demands human inputs or correction, which can be time-consuming and costly. In this paper, we propose a novel approach that leverages LLMs and environment feedback to automatically generate PDDL domain and problem description files without the need for human intervention. Our method introduces an iterative refinement process that generates multiple problem PDDL candidates and progressively refines the domain PDDL based on feedback obtained from interacting with the environment. To guide the refinement process, we develop an Exploration Walk (EW) metric, which provides rich feedback signals for LLMs to update the PDDL file. We evaluate our approach on \(10\) PDDL environments. We achieve an average task solve rate of 66% compared to a 29% solve rate by GPT-4's intrinsic planning with chain-of-thought prompting. Our work enables the automated modeling of planning environments using LLMs and environment feedback, eliminating the need for human intervention in the PDDL translation process and paving the way for more reliable LLM agents in challenging problems. Our code is available at https://github.com/BorealisAI/llm-pddl-planning

## 1 Introduction

Large language models (LLMs) have demonstrated remarkable success across various domains, including mathematics, coding, and even the bar exam [1]. These models excel at understanding and generating natural language, offering flexibility and adaptability to a wide range of tasks. However, when it comes to planning and long-horizon reasoning, LLMs have shown limited performance [8; 28], despite some promising results [3].

Planning is a crucial aspect of intelligence that involves reasoning to find a sequence of actions to achieve a desired goal state from an initial state. The Planning Domain Definition Language (PDDL) [18] is a widely used formalism for describing planning problems. PDDL provides a structured way to define the domain, which includes the types of objects, predicates, and actions, as well as the problem instance, which specifies the initial state and goal conditions. PDDL enables the application of search-based algorithms, such as breadth-first search (BFS) or A\({}^{*}\) search, which can guarantee to find a valid solution if one exists. However, the downside of PDDL is that it requires a well-defined and structured domain and problem definition, which can be challenging to create, especially for complex scenarios. Figure 1 showcases snippets of some PDDL problems and domain files along with an action plan produced by a classical planner.

Recent studies explored combining the strengths of LLMs and PDDL-based planning [15; 7; 9]. The idea is to leverage LLM for translation from natural language (NL) problem descriptions into PDDL formal descriptions, and then use a classical planner to solve the translated PDDL problem [9]. This hybrid approach could theoretically take advantage of the flexibility of NL input and the correctness guarantees provided by the classical planner. If the translation from NL to PDDL is accurate, the resulting plan is guaranteed to be valid.

Unfortunately, existing approaches have not been able to generate both PDDL problem and domain descriptions with reasonable success rates without humans in the loop, as we shall elaborate in Sec. 2. While translating PDDL problems is feasible given the domain PDDL description [15], generating domain PDDL from NL correctly is a more nuanced and challenging problem. To do so requires identifying causally relevant objects to design predicates, as well as their inter-relationships, in a way that accurately reflects the possible states and transitions of the environment. A small error, for example in predicate design, could lead to entirely incorrect domain description and failed planning (see Appendix A.2 for a real example). Guan et al. [9] take a step toward this goal relying on human-in-the-loop to detect and correct mistakes made by LLMs.

In this work, we develop a fully automated method for generating PDDL domain and problem definitions using LLMs and environment feedback without relying on human intervention. Intuitively, our method lets an LLM build hypothetical "mental models" of the environment, in the form of proposed PDDL domain descriptions. The LLM then verifies and updates the "mental model" by observing discrepancies between the feasibility of actions under its "mental model" and the real environment. This method enables LLMs to use classical planners to solve complex planning problems whose solutions may require hundreds or thousands of steps that all need to be correct.

We first highlight the challenges of this task and then propose our solution. In particular, our contributions are as follows:

* We demonstrate that even small modifications to PDDL domains can render plan search infeasible, limiting the feedback information for LLMs to perform in context update.
* To address this, we introduce a new Exploration Walk (EW) metric, which is a smooth similarity measure between two domains by comparing the executability of random action sequences sampled from one domain on the other. Crucially, EW only requires access to the action interface and executability of the environments, not directly the ground-truth PDDL.
* We propose an EW-guided tree search approach that leverages LLMs to generate and refine the PDDL domain and problem files iteratively and automatically.
* We evaluate our method on \(10\) challenging PDDL domains, where a number of them are from the International Planning Competition, and show that it outperforms a baseline that generates PDDL files in a single attempt without refinement. Our method solves 7 out of 10 environments, achieving an average task solve rate of 66% and average EW score of 0.84, compared to 34% task solve rate and 0.53 EW score for the baseline, and 29% solve rate by GPT-4 (gpt-4-1106-preview)'s intrinsic planning with chain-of-thought prompting.

To the best of our knowledge, this is the first work that enables modeling a planning environment via PDDL translation using LLMs and environment interaction, without the need for human intervention.

## 2 Related Work

**LLMs and Classical Planning.** There has been recent interest in integrating LLMs with PDDL [15; 28; 9; 7; 30; 23; 10; 20; 26], and more generally neural networks with PDDL [24; 2]. Silver et al. [25] leverage LLMs to take domain PDDLs and problem PDDL specifications, and synthesize a Python function to generate domain-specific plans, as a replacement for search-based planning. Liuet al. [15] show that using LLMs to translate problem specification to PDDL, and using classical solvers results into a higher planning accuracy that using LLM directly as a planner. Dagan et al. [7] consider a similar setting, but assume that the list of objects is partially observable, and the LLM needs to interact with the world to observe the list of objects. All of the mentioned works, however, assume that a domain PDDL files is already provided. Oswald et al. [20] generate domain PDDL from natural language and propose heuristics for comparing PDDL action domains. However, their approach assumes that predicates are provided, whereas our work makes no such assumption. Additionally, Oswald et al. [20] rely on ground-truth problem instances for domain compatibility evaluation, whereas we directly translate problem PDDL without any such assumptions. Guan et al. [9] translate both Domain and Problem from natural language description but rely on human experts to correct mistakes in the domain translation before generating problem PDDLs. In this work, our goal is to lift the human-intervention assumption, and instead, use domain interaction for evaluation and verification. See Table 1 for a summary of related work comparison.

**Direct Reasoning with LLMs.** Recent research has explored eliciting direct reasoning capabilities within Large Language Models (LLMs). This reasoning can be either entirely direct [31; 29] or partially direct with the assistance of basic external tools [16]. However, the primary limitation of these approaches lies in the inherent tendency of auto-regressive LLMs to produce errors in long-horizon reasoning tasks [28]. Even a minor mistake in a single reasoning step can lead to cascading errors, ultimately resulting in an incorrect final answer [8]. When applied to classical planning, this approach delegates the entire plan generation process to an LLM instead of leveraging a dedicated classical planner. Studies have demonstrated that this strategy is suboptimal compared to generating PDDL code directly [9; 15], highlighting the importance of incorporating classical planning tools for faithful plan generation in classical planning tasks.

**External Reasoning and Code Generation.** This last line of work focuses on generating executable code from natural language instructions such as SQL or Python code generation [4; 19; 17; 5; 16; 32]. Here, the LLM often acts as a code translator, and the reasoning logic lies within the generated code. Chen et al. [4] show that LLMs are capable of Python code generation from docstrings to high accuracy. The authors also find that taking multiple code samples from an LLM and picking the best samples results in an accuracy boost. Later works show that iterative refinement of LLM responses improves the accuracy on the downstream task [17; 5], especially given external feedback such as unit tests or human feedback. Our work is related to code generation as we produce structured PDDL files. However, our setting presents three challenges: (1) there are two types of PDDL files, in contrast to a single Python script, and the two files need to be consistent with each other; (2) more importantly, getting external feedback and the evaluation of a generated PDDL code is not as easy as python unit tests, and as we show in Section 4.3, (domain generation) errors are abundant and hard to trace; (3) LLMs are trained with a lot more Python code compared to PDDL, as the later is much scarcer.

## 3 Notation and Background

**Notation.** We denote \(\mathbbm{1}[\cdot]\) as the indicator function. The notation \(1:N\) refers to the sequence of integers ranging from \(1\) to \(N\). For a set \(\mathcal{A}\), we define \(\mathcal{A}^{*}\) as the set comprising all possible sequences of elements drawn from \(\mathcal{A}\), and define \(2^{\mathcal{A}}\) as the power set of \(\mathcal{A}\).

**PDDL.** Planning Domain Definition Language (PDDL) is a formal language used to describe and specify planning problems for automated planning. Here, we have two types of PDDL files: (1) _Domain PDDL_, which defines possible _predicates_ (_i.e._, states), and _actions_ in the environment. Executing each action requires some _precondition_ (_i.e._, a set of predicates to have a specific value), and the execution leads to some _effect_ (_i.e._, a change in the values of some predicates). (2) _Problem PDDL_, which contains a set of initial predicates and a set of goal predicates.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Method(s)** & **Translate Problem** & **Translate Domain** & **No Human Intervention** \\ \hline LLM+P [15], LLM-DP [7] & ✓\({}^{*}\) & \(\times\) & ✓ \\ \hline LLM World Models [9] & ✓ & ✓ & \(\times\) \\ \hline Ours & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of comparison to most closely related prior studies.*Require at least one problem instance to be translated by a human into the target domain as an in-context example.

The problem PDDL instantiates the domain definition PDDL to form a concrete environment. Together, the planning problem is fully defined and formalized. A _classical planner_ takes in both files and searches for a plan based on the provided specification. A _plan_ is a sequence of actions, starting from the initial state, leading to a state satisfying the goal conditions, with each action respecting the rules of the environment. Formally, let \(\mathcal{D},\mathcal{P},\mathcal{A}\) be the set of all possible domains, problems, and actions, respectively. Then, given a domain \(d\in\mathcal{D}\) and problem \(p\in\mathcal{P}\), a classical planner \(C:\mathcal{D}\times\mathcal{P}\rightarrow\mathcal{A}^{*}\cup\{\bot\}\) takes in domain \(d\) and plan \(p\), and produces a plan \(q:=C(d,p)\) which is either set of actions from \(\mathcal{A}^{*}\), or a planning error \(\bot\). A planning error may be due to an infeasible plan search (_i.e._, plan not found), syntax errors, or incompatible domain and problem. A plan validator verifies whether a plan \(q\) is executable and achieves the desired problem goal given a domain PDDL \(d\) and problem PDDL \(p\), _i.e._, whether \(q\) solves the planning problem instance. The validator function, denoted as \(V_{d,p}(q):\mathcal{A}^{*}\rightarrow\{0,1\}\), is \(1\) if the plan is valid, and \(0\) otherwise. For convenience, we assume \(V_{d,p}(\bot)=0\). Similarly, we define plan execution checker \(E_{d,p}:\mathcal{A}^{*}\rightarrow\{0,1\}\), which only checks whether an action sequence is executable in a domain or not. Note that the _difference between \(V\) and \(E\) is that the former checks for both plan executability and goal satisfaction, while the latter only checks for plan executability._ We also define \(\mathcal{S}\) as the set of all possible states. Function \(A_{d,p}:\mathcal{S}\to 2^{\mathcal{A}}\) delineates the set of legal actions given the current states (_i.e._, actions that would not immediately result in \(E_{d,p}\) returning 0). The function \(S_{d,p}:\mathcal{A}\times\mathcal{S}\rightarrow\mathcal{S}\) denotes the state transition function ( i.e., \(S_{d,p}(a,s)\) determines the subsequent state given the current state \(s\) and action \(a\)). Finally, we denote the initial state induced by \(d\) and \(p\) to be \(s_{d,p,0}\in\mathcal{S}\). See Table 3 in the Appendix for a summary of notations.

To illustrate the definitions with an example, consider the Grippers [13] environment with several rooms containing robots and boxes. Robots can move balls between rooms using their left and right grippers. Given an initial setting of robots and balls in different rooms, the main goal is to move specific balls to specific rooms using the robots.

Figure 1: Snippets of PDDL domain, problem, and plan.

Method

Given an environment \(e\), its domain NL description and a task NL description, the environment's object list and action interface, our goal is to model the environment by generating a domain PDDL \(\hat{d}\in\mathcal{D}\) and a problem PDDL \(\hat{p}\in\mathcal{P}\), such that applying a classical planner \(C\) on the PDDL files produces a valid plan for the environment, _i.e._, \(C(\hat{d},\hat{p})\) is a valid plan for \(e\), _i.e._, \(V_{d,p}(C(\hat{d},\hat{p}))=1\).

### Setup

For evaluation, we assume there exists a ground truth domain PDDL \(d\in\mathcal{D}\), and a corresponding problem instance \(p\in\mathcal{P}\). However, the ground truth is not directly compared to generated \(\hat{d},\hat{p}\), but to validate the plan \(\hat{q}:=C(\hat{d},\hat{p})\) by executing the validator of the ground-truth environment, \(V_{d,p}(\hat{q})\).

Formally, for each environment \(e\) with domain PDDL \(d\in\mathcal{D}\), and \(N\) tasks with their corresponding ground-truth problem PDDLs \(p_{1:N}:=(p_{1},p_{2},\ldots,p_{N}),p_{1:N}\in\mathcal{P}^{N}\), our goal is to generate a domain PDDL \(\hat{d}\), and a sequence of task PDDLs \(\hat{p}_{1:N}:=(\hat{p}_{1},\hat{p}_{2},\ldots,\hat{p}_{N})\) such that the average solve rate \(\overline{V}\) is maximized:

\[\operatorname*{argmax}_{\hat{d}\in\mathcal{D},\hat{p}_{1:N}\in\mathcal{P}^{N} }\overline{V}(\hat{d},\hat{p}_{1:N};e):=\frac{1}{N}\sum_{i=1}^{N}V_{d,p_{i}} \left(C(\hat{d},\hat{p}_{i})\right).\] (1)

Generating accurate \(\hat{d}\) and \(\hat{p}_{1:N}\) in one attempt is often impractical [9], and some form of feedback is required to refine the response. Guan et al. [9] leverage human expert feedback on \(\hat{d}\) to correct the generated domain. However, human feedback may not always be reliable and is not scalable. Before introducing our method that relies on environment feedback instead, we first state our assumptions:

**Assumption 1** (Environment access): _We assume the list of objects and action interfaces are known. Furthermore, we assume that executability and verifiability of actions can be observed (through the functions \(E_{d,p}\) and \(V_{d,p}\))._

**Assumption 2** (Natural language description): _We assume the natural language descriptions of the domain and task are both given._

The action interfaces are equivalent to APIs available to LLM agents. So it is reasonable to assume that the exact API call signatures are known. On the other hand, one may wonder why the object list, which appears in problem PDDLs as illustrated in Figure 1 needs to be assumed to be given, when the NL problem description should describe the objects involved in the planning tasks. This is because the NL description may not refer to the object instances using exactly the same label as the environment induced by \(d\) and \(p\). If \(p\) refers to a robot as robot1 but the user specifying the natural language problem description calls it Jarvis, then the environment only recognizes robot1 and not Jarvis, so the LLM would have no way to correct this mistake due to trivial name mismatch. See Appendix A.1 for a detailed example of our assumptions on the Grippers environment.

Note that our assumptions do not require the underlying environment to be a PDDL environment, but it can be any environment as long as PDDL is expressive enough to capture the working mechanisms of the environment. For digital agents in virtual environments, the list of objects and action interfaces are just different data objects and APIs available. The assumptions could even hold true for physical agents in the real world, provided recognition and control are sufficiently accurate. In this work, we focus on PDDL environments only, although our framework is more general.

### Difficulty of domain PDDL generation

Generating the correct domain PDDL is challenging, as small mistakes could make the plan search fail. To demonstrate this brittleness, we simulate random omission of \(k\) terms, where \(0\leq k\leq 10\), from the action precondition and effects of the original domain \(d\). For instance, in the case of the Grippers (Figure 1), we may create a new synthetic domain by removing the (at robby?r?to) term from the effects of the move action. Namely, we define \(\hat{d}_{k}\sim\mathbb{P}_{k}(d)\), where \(\mathbb{P}_{k}(d)\) represents the uniform random removal of \(k\) terms. Then, for each generated \(\hat{d}_{k}\), coupled with the ground truth task PDDLs, we compute whether the classical planner is able to find a plan without error and compute the _Plan-Not-Found_ rate under \(k\) omissions, PNF\({}_{k}\), of the environment.

We empirically measure the value of \(\text{PNF}_{k}\) using Monte-Carlo estimation on \(15\) environments. As shown in Figure 1(a), \(\text{PNF}_{1}\) has an average of \(0.14\) among different environments. This means that on average \(14\%\) of the terms in domain PDDLs are so critical that removing them results in a plan-not-found error. This situation is exacerbated for larger \(k\): at \(k=3\), the average \(\text{PNF}_{k}\) reaches around \(0.3\). In practice, the problem PDDL \(\hat{p}_{i}\) also needs to be generated, and the generated domain \(\hat{d}\) may have extra terms, both of which may further increase the planning-not-found rate.

### Domain alignment measure via Exploration Walk metrics

Whenever the plan search fails, absolutely no information is available to the LLM about which part of the problem or domain has issues. This is because the underlying search algorithm (such as BFS and A\({}^{*}\)) fails and as a result, it does not produce any output. For example, with BFS, it enumerates all paths (possibly several thousand paths or more), and finds none satisfy the goal conditions, leaving the plan search without any useful insights. As an alternative, we introduce the Exploration Walk (EW): a smooth feedback signal that provides incremental feedback for LLM in-context learning. EW both provides a mechanism to gather richer feedback information that feeds into LLM context for PDDL refinement, as well as computing a smooth scoring metric that to compare multiple PDDLs and guide the refinement process forward.

Intuitively, the idea is to take legal random action sequences and verify their executability under LLM's "mental model" environment induced by an LLM-generated PDDL domain. This is analogous to the _retrodiction_ step in scientific methodology, where existing observations and experimental data need to be explained by the existing model.

And in the other direction, EW takes executable random action sequences from an LLM-generated PDDL domain and verifies whether they are correct in the real environment. This is analogous to _hypothesis testing_ in scientific methodology, where new predictions are verified experimentally.

We now describe the EW and EW metrics formally. We define an Exploration Walk of length \(T\) to be any action sequence sampled from a strictly positive distribution \(\mathbb{P}_{d,p,T}\) over executable \(T\)-step action sequences in \(\mathcal{A}^{*}\) corresponding to domain \(d\) and task \(p\). We assume the probability of non-executable action sequences to be zero under \(\mathbb{P}_{d,p,T}\). In other words, \(\forall q_{1:T}\), \(\mathbb{P}_{d,p,T}(q_{1:T})>0\) iff \(E_{d,p}(q_{1:T})=1\).

For the rest of this paper, we use the simplest possible EW, with a uniform distribution over valid actions at each step. Note that to sample uniform random EW from the ground truth environment induced by \(d\) and \(p\), we do not need direct access to the full \(d\) and \(p\). We only need the list of objects in \(p\) and the action interface in \(d\), and executability checker \(E_{d,p}\), consistent with our Assumption 1. At each step, running \(E_{d,p}\) on all possible actions yields the legal actions at that step for EW.

Given an EW distribution, we define an EW metric using the fractions of executability of EW walks from one domain under another, averaged over all different lengths.

Figure 2: (a) Effect of the number of removed terms on plan search failure. Each gray line shows the \(\text{PNF}_{k}\) (Plan-Not-Found) metric for one environment. The red line is the average of all 15 environments. (b) Correlation between average exploration walk (EW) score and average domain difference. The \(x\)-axis shows how many terms each pair of domains differs in. The \(y\)-axis shows the average EW score over various pairs. All the domains show the average monotonicity of the EW score with respect to term difference.

**Definition 1** (EW Metrics): _Let \(p_{1:N}\) and \(\hat{p}_{1:N}\) be problems in domain \(d\) and \(\hat{d}\) respectively, such that the set of objects in \(p_{j}\) and \(\hat{p}_{j}\) are consistent. We define the one-sided measure \(m_{d-\hat{d}}\) and the symmetric one \(m_{d\leftrightarrow\hat{d}}\) for the degree of alignment between two domains \(d\) and \(\hat{d}\) as:_

\[m_{d\rightarrow\hat{d}}(p_{1:N},\hat{p}_{1:N}) :=\frac{1}{NT_{\text{max}}}\sum\nolimits_{j=1}^{N}\sum\nolimits_ {T=1}^{T_{\text{max}}}\mathbb{E}_{q\sim\mathbb{P}_{d,p_{j},T}}\left[E_{\hat{d},\hat{p}_{j}}(q)\right]\] \[m_{d\leftrightarrow\hat{d}}(p_{1:N},\hat{p}_{1:N}) :=2\big{/}\big{(}1/m_{d\rightarrow\hat{d}}(p_{1:N},\hat{p}_{1:N}) +1/m_{\hat{d}\rightarrow\hat{d}}(\hat{p}_{1:N},p_{1:N})\big{)},\] (2)

_where \(T_{\text{max}}\) is the largest EW walk length._

\(m_{d\rightarrow\hat{d}}\) measures what fraction of EWs sampled from domain \(d\) are executable on the domain \(\hat{d}\). Then, \(m_{d\leftrightarrow\hat{d}}\) takes the harmonic mean of \(m_{d\rightarrow\hat{d}}\) and \(m_{\hat{d}\rightarrow\hat{d}}\) to produce the final EW measure. This metric has two favourable properties: (1) it ensures that \(m_{d\leftrightarrow\hat{d}}=m_{\hat{d}\leftrightarrow d}\), thereby providing a consistent measure of similarity regardless of the order of domain comparison. (2) the harmonic mean is resistant to trivial domain similarity inflation. By employing the harmonic mean rather than the arithmetic mean, the symmetric EW metric prevents domains that are overly permissive (e.g., domains where all actions are permissible without any preconditions) from being similar to more restrictive domains. For example, in a scenario where domain \(\hat{d}\) allows all possible actions without restrictions, \(m_{d\rightarrow\hat{d}}=1\). An arithmetic mean in this context would yield \(m_{d\leftrightarrow\hat{d}}\geq 0.5\), overestimating the similarity. In contrast, the harmonic mean results in \(m_{d\leftrightarrow\hat{d}}=\epsilon\), where (\(\epsilon\ll 1\)) for most cases.

Note that while the PDDL problems \(p_{1:N}\) and \(\hat{p}_{1:N}\) appear in the definition of EW metrics, we only use the fact there are aligned object sets in them. We could also use an arbitrarily sampled object list to form an \(\tilde{P}\) and pair \(\tilde{P}\) with \(D\) and \(\hat{D}\) for EW metrics. But since for PDDL generation, we already generate \(\hat{p}_{1:N}\), it is more convenient to use them.

Importantly, EW metrics can be computed without direct access to the full ground truth domain \(d\) and problems \(p\)'s. As established before, to sample uniform random EW, we just need access to the object list and action interface, plus the environment executability checker of the source domain. So even for \(m_{d\rightarrow\hat{d}}\), where the EW action sequences come from \(d\), we do not need more than what is available through Assumption 1.

To demonstrate the relationship between \(m_{d\leftrightarrow\hat{d}}\) and domain disparity, we use the same simulated random omission study setup from Sec. 4.2. For a pair of modified domains, we count the number of terms that differ, and inspect \(m_{d\leftrightarrow\hat{d}}\) as function of increasing number of differing terms in Figure 2 for six example domains (see Figure 4 in the Appendix for the full set). We observe that, on average, a greater discrepancy in the number of terms between two domains correlates with a reduced EW score \(m_{d\leftrightarrow\hat{d}}\). This observation provides additional support to the use of the EW score as an effective measure for domain differences.

### Leveraging LLMs to generate PDDL files

We now show our overall LLM-based method for PDDL generation using the EW score to guide and measure the progress of domain generation. To illustrate the process, we first focus on a domain \(d\) with a single task \(p\). Recall that we are given NL description of the environment domain \(d_{\text{NL}}\) and problem \(p_{\text{NL}}\) (Assumption 2), as well as the object list in \(p\) and action interface from \(d\) (Assumption 1). Then, by using \(d_{\text{NL}}\), \(p_{\text{NL}}\), and access to environment action feedback, we seek to generate \(\hat{d}\in\mathcal{D},\hat{p}\in\mathcal{P}\).

Our method starts by initializing templated \(\hat{d}^{(0)}\) based on action interfaces and templated \(\hat{p}^{(0)}\) using object list. Example template \(\hat{d}^{(0)}\) and \(\hat{p}^{(0)}\) are shown in Listings 6 and 4 of Appendix A.1. We then use an LLM to improve the initial \(\hat{d}^{(0)}\) and \(\hat{p}^{(0)}\).

Given that domain PDDL files are typically more complex than problem PDDL files, our strategy prioritizes the generation of a problem PDDL file \(\hat{p}\) first, followed by the domain \(\hat{d}\). This approach enables us to assess the quality of the generated domain immediately. Moreover, prior works on code generation [4], tree-of-thought [31], and self-debug [5] have found that taking multiple samples from the LLM response and taking the best response leads to better performance. However, they often require an evaluation metric on the generated response (such as unit test cases, or execution traces). Here, we use the EW metric introduced in Section 4.3 to serve as an evaluator of the generated domain. These considerations lead to our proposed Algorithm 1. We emphasize again that theground-truth domain and problem \(d,p\) are only used to take exploration walks and evaluate a plan through the environment in 1.

```
0: Natural language descriptions \(d_{\text{NL}}\), \(p_{\text{NL}}\), environment action interface.
1:\(\hat{p}^{(1)},\hat{p}^{(2)},\dots,\hat{p}^{(n_{p})}\leftarrow\text{LLM}_{n_{p}}(p_ {\text{NL}})\) {Problem PDDL candidates}
2:for\(i=1,2,\dots,n_{p}\)do
3:\(h^{(i)}\leftarrow[\hat{p}^{(i)},d_{\text{NL}}]\) {Keep a history of conversation}.
4:\(\hat{d}^{(i)}_{\text{best}}\gets d_{\text{NL}}\) {Initialize with an empty template}.
5:for\(c=1,2,\dots,c_{\text{max}}\)do
6:\(\hat{d}^{(i,1)},\hat{d}^{(i,2)},\dots,\hat{d}^{(i,n_{d})}\leftarrow\text{LLM}_ {n_{d}}(h^{(i)})\)
7:\(\hat{d}^{(c)}\leftarrow\operatorname*{argmax}_{\hat{d}\in\{\hat{d}^{(i,1)}, \dots,\hat{d}^{(i,n_{d})}\}}m_{d\leftrightarrow\hat{d}}\left(p,\hat{p}^{(i)}\right)\) {Evaluate LLM responses using EW}
8:\(f^{(c)}\leftarrow\text{Natural language feedback from EW on }d,p\).
9:\(h^{(i)}\gets h^{(i)}+[\hat{d}^{(c)},f^{(c)}]\)
10:\(\hat{d}^{(i)}_{\text{best}}\leftarrow\operatorname*{argmax}_{\hat{d}\in\{\hat{d} ^{(c)},\hat{d}_{\text{best}}\}}m_{d\leftrightarrow\hat{d}}\left(p,\hat{p}^{(i)}\right)\)
11:endfor
12:endfor
13:\(\hat{d},\hat{p}\leftarrow\operatorname*{argmax}_{\{(\hat{d}^{(i)},\hat{p}^{( i)})\mid i=1,2,\dots,n_{p}\}}m_{d\leftrightarrow\hat{d}^{(i)}_{\text{best}}}(p, \hat{p}^{(i)})\)
14:return\(\hat{d},\hat{p}\) {Return the final refined domain and problem PDDLs} ```

**Algorithm 1** Generating Domain PDDL and Problem PDDL Using Environment Feedback

Note that each environment contains \(N>1\) problems, therefore, we need to translate all problem instances into PDDL. Similar to Liu et al. [15], given one problem \(p_{1_{\text{NL}}}\) and its generated translation \(\hat{p}_{1}\), we translate the rest of the problems \(p_{2:N_{\text{NL}}}\) in a one-shot manner. That is, we generate \(\hat{p}_{i}:=\text{LLM}_{1}\left(p_{1_{\text{NL}}},\hat{p}_{1},p_{i_{\text{NL }}}\right)\) as the final problem translation for problem \(i\) for all \(2\leq i\leq N\).

## 5 Experiments

**Dataset.** We consider PDDL files from real environments, taking nine domains from a combination of domain PDDLs from Liu et al. [15] and Seipp et al. [22]. The LLM may have seen the mentioned domains in its pre-training data, which is a common issue for current benchmarks. To mitigate this issue, we also modify the original Grippers domain, and create a modified domain called "Grippers-ood" domain, to ensure no LLM has seen it previously. We generate natural domain descriptions for

Figure 3: Overview of our method. _Right:_ The process begins with natural language descriptions translated into problem PDDL by the LLM (red arrows). Then a domain is generated and refined through iterative cycles involving exploration walks in the environment, interaction with a classical planner, and feedback from the LLM (blue/black arrows). _Left:_ The iterative refinement process depicted on the right corresponds to single paths in the structures shown on the left. Each node represents a state in the refinement process, with arrows indicating problem translation (red), domain refinement (blue).

all PDDL files by back-translating them using GPT-4 and manually inspecting and modifying the translations for correctness. For each environment, we consider one domain PDDL \(d\) and \(N=10\) problem PDDLs \(p_{1:N}\). We use one problem for domain translation and EW evaluation, and all problems for evaluating a final domain response. We reserve the Blocksworld environment as an in-context example for prompting the LLM. As such, we do not evaluate the Blocksworld environment itself in our evaluations. See Appendices A.1 and C for more details on dataset curation.

**Feedback Format.** The natural language feedback given to LLM is in the following form: [Action sequence] [State description]. That is, we first provide LLM with the sequence of actions taken from one exploration walk, up until one action fails. Then, we provide the environment state description from the last step. We show an example of environment feedback and LLM response for the Termes environment in Listings 9 in the Appendix. We deliberately choose a simple feedback format to maintain the general applicability of our framework.

**Baselines and Metrics.** We use GPT-4 [1] (gpt-4-1106-preview) as the LLM since models with lower capability may struggle with syntax errors [9]. We consider the following methods: (1, 2) **Intrinsic Planning (CoT):** where the language model generates a complete plan without the help of any external planning library, based on the given descriptions, both with and without chain-of-thought prompting. This baseline does not leverage any classical planner or PDDL translation. (3) **P&D Chain:** Our proposed method (Algorithm 1) with \(n_{d}=n_{p}=1\). (4) **P&D Tree:** Our proposed method with multiple response generations (\(n_{d}=10,n_{p}=5\)). (5) **P&D Tree + DomProp:** Our proposed method with multiple response generations and domain proposals for each problem (see Appendix B.2). Following prior works [17; 5], we set a maximum conversation turns of \(c_{\text{max}}=4\).

We run each algorithm for four seeds and compute the Best@4 metric, which takes the highest score among the four seeds. We report two metrics: (1) tasks solved2, measuring the fraction of the \(N=10\) tasks successfully solved (Eq. (1)), and (2) EW score, comparing the final domain through running exploration walks on all \(N\) problems (Eq. (2) with \(T_{\text{max}}=10\)). We use the original fast-downward [11] library for planning, the modified fast-downward library from text-world [6] for python-compatible state explorations, and the VAL [12] library to validate plans.

Footnote 2: Note that a perfect task solve rate does not guarantee exact domain equivalency of the generated domain to the ground truth domain.

**Results.** Table 2 shows the final results on various environments. We consider a domain generation to be solved if a method achieves \(>0.5\) solve rate since we observe the rest of the errors are problem translation errors rather than domain translation errors. Our proposed method solves 7 out of 10 domains, compared to 3 solved by the Intrinsic CoT baseline. We also generally observe the correlation of EW score with task solve rate. Particularly, even when the task solve rate is zero, the EW metric shows signs of progress, _e.g._, in domains such as Barman and Childsnack where all task solve rates are zero, the EW metric shows a clear distinction between method performances. Moreover, when the EW metric is high, such as \(1.0\), we observe a generated PDDL domain to be very close to the ground-truth domain, and differing in very few predicates. For instance, in the case of the "Hiking" environment, the P&D Chain achieves zero solve rate, but a perfect EW score, which we observe perfect solution in the case of P&D Tree.

**Computational Cost.** For the results in Table 2 using the GPT-4 model, we used \(12.40\) million input tokens and \(8.73\) million output tokens. Computing the EW is relatively negligible compared to the cost of LLM inference. In our experiments, computing the EW score for a single domain-problem pair takes less than two minutes on a 64-core server CPU.

## 6 Conclusion

In this work, we present a novel approach for modeling planning environments via PDDL translation using large language models (LLMs) and environment feedback, without relying on human intervention. The key contributions include introducing the Exploration Walk (EW) metric to measure domain similarity and guide domain refinement, and an iterative method that leverages LLMs to generate and refine PDDL domain and problem files. Evaluation on 10 real-world PDDL domains demonstrates the effectiveness of the proposed approach, outperforming a baseline that generates PDDL files in a single attempt without refinement. The method solves 7 out of 10 environments, achieving an average task solve rate of 66% and an average EW score of 0.84.

The current limitations include potentially insufficient and efficient exploration caused by random EW. More sophisticated EW strategies could improve the success rate while lowering the cost in the future. For example, strategies from the reinforcement learning literature (_e.g._, [27; 21]) could be adapted to improve exploration efficiency and success rates. Another limitation is that we have only applied the framework to PDDL environments, despite it being applicable to digital or even physical environments. We hope this work will inspire further research at the intersection of language models and planning, enabling the development of more advanced and autonomous planning systems.

## References

* Achiam et al. [2023] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* Ahmetoglu et al. [2022] A. Ahmetoglu, M. Y. Seker, J. Piater, E. Oztop, and E. Ugur. Deepsym: Deep symbol generation and rule learning for planning from unsupervised robot interaction. _Journal of Artificial Intelligence Research_, 75:709-745, 2022.
* Bubeck et al. [2023] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* Chen et al. [2021] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_, 2021.
* Chen et al. [2024] X. Chen, M. Lin, N. Scharli, and D. Zhou. Teaching large language models to self-debug. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=KuPixIqPiq.
* Cote et al. [2018] M.-A. Cote, A. Kadar, X. Yuan, B. Kybartas, T. Barnes, E. Fine, J. Moore, R. Y. Tao, M. Hausknecht, L. E. Asri, M. Adada, W. Tay, and A. Trischler. Textworld: A learning environment for text-based games. _CoRR_, abs/1806.11532, 2018.
* Dagan et al. [2023] G. Dagan, F. Keller, and A. Lascarides. Dynamic planning with a llvm. _arXiv preprint arXiv:2308.06391_, 2023.
* Dziri et al. [2024] N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y. Lin, S. Welleck, P. West, C. Bhagavatula, R. Le Bras, et al. Faith and fate: Limits of transformers on compositionality. _Advances in Neural Information Processing Systems_, 36, 2024.
* Guan et al. [2023] L. Guan, K. Valmeekam, S. Sreedharan, and S. Kambhampati. Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=zDbsSscmuj.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & Intrinsic & Intrinsic & P\&D Chain & P\&D Tree & P\&D Tree + DomProp \\  & No CoT & CoT & (\(n_{d}=1,n_{p}=1\)) & (\(n_{d}=10,n_{p}=5\)) & (\(n_{d}=10,n_{p}=5\)) \\ \hline Barman & 0.00 / – & 0.00 / – & 0.00 / 0.93 & 0.00 / 1.00 & 0.00 / 1.00 \\ Childsnack & 0.00 / – & 0.00 / – & 0.00 / 0.57 & 0.00 / 1.00 & 0.00 / 1.00 \\ Driverlog & 0.00 / – & 0.00 / – & 0.00 / 0.05 & 0.00 / 0.05 & 0.00 / 0.60 \\ Floortile & 0.00 / – & 0.00 / – & 0.00 / 0.07 & 0.90 / 0.94 & 0.00 / 0.07 \\ Grippers & 0.40 / – & 0.60 / – & 0.10 / 0.39 & 1.00 / 1.00 & 1.00 / 1.00 \\ Grippers-ood & 0.30 / – & 0.30 / – & 0.30 / 0.35 & 0.70 / 0.72 & 1.00 / 1.00 \\ Hiking & 0.00 / – & 0.00 / – & 0.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 \\ Miconic & 0.90 / – & 1.00 / – & 1.00 / 0.84 & 1.00 / 0.85 & 1.00 / 1.00 \\ Movie & 1.00 / – & 1.00 / – & 1.00 / 0.07 & 1.00 / 0.85 & 1.00 / 0.86 \\ Termes & 0.00 / – & 0.00 / – & 1.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 \\ \hline
**Average** & 0.26 / – & 0.29 / – & 0.34 / 0.53 & **0.66** / 0.84 & 0.60 / **0.85** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Best@4 (Tasks solved / Exploration Walk) for different domains. For intrinsic planning no domain is generated, therefore the EW score is not defined.

* Han et al. [2024] M. Han, Y. Zhu, S.-C. Zhu, Y. N. Wu, and Y. Zhu. Interpret: Interactive predicate learning from language feedback for generalizable task planning. In _Robotics: Science and Systems (RSS)_, 2024.
* Helmert [2006] M. Helmert. The fast downward planning system. _Journal of Artificial Intelligence Research_, 26:191-246, 2006.
* Howey et al. [2004] R. Howey, D. Long, and M. Fox. Val: Automatic plan validation, continuous effects and mixed initiative planning using pddl. In _Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence_, ICTAI '04, page 294-301, USA, 2004. IEEE Computer Society. ISBN 076952236X. doi: 10.1109/ICTAI.2004.120. URL https://doi.org/10.1109/ICTAI.2004.120.
* IPC [1998] IPC. International planning competition, 1998. URL https://www.icaps-conference.org/competitions/.
* Kojima et al. [2022] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. In _Advances in Neural Information Processing Systems_, volume 35, pages 22199-22213, 2022.
* Liu et al. [2023] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. Llm+p: Empowering large language models with optimal planning proficiency. _ArXiv_, abs/2304.11477, 2023. URL https://api.semanticscholar.org/CorpusID:258298051.
* Lyu et al. [2023] Q. Lyu, S. Havaldar, A. Stein, L. Zhang, D. Rao, E. Wong, M. Apidianaki, and C. Callison-Burch. Faithful chain-of-thought reasoning. In J. C. Park, Y. Arase, B. Hu, W. Lu, D. Wijaya, A. Purwarianti, and A. A. Krisnadhi, editors, _Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 305-329, Nusa Dua, Bali, Nov. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. ijcnlp-main.20. URL https://aclanthology.org/2023.ijcnlp-main.20.
* Madaan et al. [2023] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, and P. Clark. Self-refine: Iterative refinement with self-feedback. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=S37hDerQLB.
* McDermott [2000] D. McDermott. The 1998 ai planning systems competition. _AI Magazine_, 21(2):35-55, 2000.
* Ni et al. [2023] A. Ni, S. Iyer, D. Radev, V. Stoyanov, W.-t. Yih, S. I. Wang, and X. V. Lin. Lever: Learning to verify language-to-code generation with execution. In _Proceedings of the 40th International Conference on Machine Learning (ICML'23)_, 2023.
* Oswald et al. [2024] J. Oswald, K. Srinivas, H. Kokel, J. Lee, M. Katz, and S. Sohrabi. Large language models as planning domain generators. In _34th International Conference on Automated Planning and Scheduling_, 2024. URL https://openreview.net/forum?id=C88wQlV0aJ.
* Sagar et al. [2024] S. Sagar, A. Tapia, and R. Senanayake. Failures are fated, but can be faded: Characterizing and mitigating unwanted behaviors in large-scale vision and language models. _arXiv preprint arXiv:2406.07145_, 2024.
* Seipp et al. [2022] J. Seipp, A. Torralba, and J. Hoffmann. PDDL generators. https://doi.org/10.5281/zenodo.6382173, 2022.
* Silver et al. [2022] T. Silver, V. Hariprasad, R. S. Shuttleworth, N. Kumar, T. Lozano-Perez, and L. P. Kaelbling. PDDL planning with pretrained large language models. In _NeurIPS 2022 Foundation Models for Decision Making Workshop_, 2022. URL https://openreview.net/forum?id=1QMMUB4zfl.
* Silver et al. [2023] T. Silver, R. Chitnis, N. Kumar, W. McClinton, T. Lozano-Perez, L. Kaelbling, and J. B. Tenenbaum. Predicate invention for bilevel planning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 12120-12129, 2023.
* Silver et al. [2024] T. Silver, S. Dan, K. Srinivas, J. Tenenbaum, L. Kaelbling, and M. Katz. Generalized planning in PDDL domains with pretrained large language models. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2024.
* Stein and Koller [2023] K. Stein and A. Koller. Autoplanbench:: Automatically generating benchmarks for llm planners from pddl. _arXiv preprint arXiv:2311.09830_, 2023.

* Sutton and Barto [2018] R. S. Sutton and A. G. Barto. _Reinforcement Learning: An Introduction_. A Bradford Book, Cambridge, MA, USA, 2018. ISBN 0262039249.
* Valmeekam et al. [2023] K. Valmeekam, M. Marquez, A. Olmo, S. Sreedharan, and S. Kambhampati. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023. URL https://openreview.net/forum?id=YXogl4uQUO.
* Wei et al. [2022] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural information processing systems_, 35:24824-24837, 2022.
* Xie et al. [2023] Y. Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh. Translating natural language to planning goals with large-language models. _arXiv preprint arXiv:2302.05128_, 2023.
* Yao et al. [2023] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. R. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=5Xc1ecx01h.
* Zhang et al. [2023] T. Zhang, T. Yu, T. Hashimoto, M. Lewis, W.-t. Yih, D. Fried, and S. Wang. Coder reviewer reranking for code generation. In _International Conference on Machine Learning_, pages 41832-41846. PMLR, 2023.

## Appendix A Dataset

### Dataset Details.

**Dataset Examples.** We provide an example of each file for the Grippers environment: (1) The ground-truth domain \(d\) (Listing 1) of ground truth PDDL domain (2) One ground-truth problem \(p\) (Listing 2) (3) Domain natural language description along with a PDDL template for action interfaces \(d_{\text{NL}}\). (Listing 5 and 6) (4) Problem natural language description along with a PDDL template with the list of objects (Listings 3 and 4)

```
1(define(domaingripper-strips)
2(:requirements:strips:typing)
3(:typesroomobjrobotgripper)
4(:predicates(at-robby?r-robot?x-room)
5(at?o-obj?x-room)
6(free?r-robot?g-gripper)
7(carry?r-robot?o-obj?g-gripper))
8
9(:actionmove
10:parameters(?r-robot?from?to-room)
11:precondition(and(at-robby?r?from))
12:effect(and(at-robby?r?to)
13(not(at-robby?r?from))))
14
15(:actionpick* ```
*parameters(?r-robot?obj-obj?room-room?g-gripper)
*precondition(and(at?obj?room)(at-robby?r?room)(free?r?g))
*effect(and(carry?r?obj?g)
*(not(at?obj?room))
*(not(free?r?g)))
*(:actiondrop
*parameters(?r-robot?obj-obj?room-room?g-gripper)
*precondition(and(carry?r?obj?g)(at-robby?r?room))
*effect(and(at?obj?room)
*(free?r?g)
*(not(carry?r?obj?g)))) ```

Listing 1: Grippers domain PDDL [15].

```
*(define(problemgripper-2-3-4)
*(:domaingripper-strips)
*(:objectsrobot1robot2-robot
*gripper1lgripper1rgripper2lgripper2-gripper
*room1room2room3-room
*ball1ball2ball3ball4-obj)
*(:init
*(at-robbyrobot1room2)

Figure 4: Correlation between average exploration walk score and average domain difference

(freerobot1gripper1)
10(freerobot1gripper1)
11(at-robbyrobot2room3)
12(freerobot2gripper2)
13(freerobot2lgripper2)
14(atball1room3)
15(atball2room1)
16(atball3room1)
17(atball4room3)
18)
19(:goal
20(and
21(atball1room2)
22(atball2room2)
23(atball3room3)
24(atball4room3)
25)
26) ) ```

Listing 2: Grippers problem PDDL.

```
1Youcontroltworobots,eachequippedwithalleftandright gripper,capableofmovingobjects(balls)betweendifferentrooms.
2
3Initially:
4-Robot1isinroom2andbothitsgrippers(rgripper1andlgripper1)arefree.
5-Robot2isinroom3andbothitsgrippers(rgripper2andlgripper2)arefree.
6-Ball1andBall4areinroom3.
7-Ball2andBall3areinroom1.
8
9Yourgoalistoachievethefollowingconfiguration:
10-Ball1mustbecmovedtoroom2.
11-Ball2mustbecmovedtoroom2.
12-Ball3mustremaininroom3.
13-Ball4mustremaininroom3. ```

Listing 3: Grippers problem natural language.

```
1(define(problemgripper-2-3-4)
2(:domaingripper-strips)
3(:objectslgripper1lgripper2rgripper1rgripper2-gripperball1ball2ball3ball4-objrobot1robot2-robotroom1room2room3-room)
4(:init)
5(:goal(and))
6) ```

Listing 4: Grippers problem template PDDL.

```
1Thegripperdomaininvolvesaworldwithmultiplerooms,robots,andobjects(balls).Eachrobothastwogrippersthatcanbeusedtopickupanddropobjects.Thegoalistomoveobjectsfromtheirinitiallocationstothedesiredgoallocationsusingtherobotsandtheirgrippers.
2
3Thedomainincludesthreeactions:
4
51.move:Thisactionallowsarobottomovefromoneroomtonanother.Thepreconditionisthattherobotmustbeinthestartingroom.Theeffectisthattherobotisnolongerinthestartingroomandisnowinthedestinationroom.
6
72.pick:Thisactionallowsarobottopickupanobjectusingoneofitsgrippers.Thepreconditionsarethattheobjectandtherobotmustbeinthesamerroom,andthespecifiedgrippermustbefree(notholdinganyobject).Theeffectisthattherobotisnowcarryingtheobjectwiththespecifiedgripper,theobjectisnolongerintheroom,andthegripperisnolongerfree.
8
93.drop:Thisactionallowsarobottodropanobjectitiscarryinginacyclificroomusingoneofitsgrippers.Thepreconditionsarethattherobotmustbecarryingtheobjectwiththespecifiedgripperandtherobotmustbeinthespecifiedroom.Theeffectisthattheobjectisnowintheroom,thegripperisfree,andtherobotisnolongercarryingtheobjectwiththatgripper.

```
1(define(domaingripper-strips)
2(:requirements:strips:typing)
3(:typesroomobjrobotgripper)
4(:predicates)
5
6(:actionmove
7:parameters(?r-robot?from?to-room)
8:precondition()
9:effect())
10
11(:actionpick
12:parameters(?r-robot?o-obj?room-room?g-gripper)
13:precondition()
14:effect())
15
16(:actiondrop
17:parameters(?r-robot?o-obj?room-room?g-gripper)
18:precondition()
19:effect()) ```

Listing 5: Grippers domain natural language.

```
1(moverobot2room3room1)
2(pickrobot2ball2room1lgripper2)
3(moverobot2room1room2)
4(droprobot2ball2room2lgripper2) ```

Listing 6: Grippers domain PDDL template.

* [command=
* 5{moverobot1room2room1} (pickrobot1ball3room1lgripper1) (moverobot1room1room3) (pickrobot1ball1room3rgripper1) (droprobot1ball3room3lgripper1) (moverobot1room3room2) (droprobot1ball1room2rgripper1) ```

Listing 7: Grippers problem plan example.

### Criticality of predicate design.

Here, we give an example on the delicacy of predicate design. Consider the Grippers environment, where each robot has two grippers: left gripper and right gripper. In our experiments, one of the main predicates that the LLM incorrectly generates is the free predicate (see Listing 8). This predicate keeps track of whether a gripper is free or not. Therefore, at first sight, (free?g - gripper) seems a natural choice to show a particular gripper is not occupied and hence is capable of picking a ball. However, when designed this way, in contrast to (free?r - robot?g - gripper) (missing the robot argument), this small detail causes the final domain to be entirely wrong! The reason is that there would no longer be any association between a robot and its two grippers. Therefore, on the incorrect domain, one robot will be able to pickup an object with the gripper of another robot! In fact, we observe that this incorrect design for the free predicate, is the reason behind the failure of the "P&D Chain" method in Table 2.

We provide one more example from the Barman environment, illustrating the criticality of predicate design. The Barman environment involves actions related to manipulating containers (e.g., shot glasses, shakers) to prepare and serve drinks using various ingredients. One of the key predicates used in the domain is (used?c - container?b - beverage), which keeps track of which beverage has been used in a specific container. This is important for actions like refilling or cleaning, where knowing the specific beverage type is essential to ensure conformation to the environment rules (_e.g._, a container can be refilled only with the beverage that it already had, otherwise, it needs to be cleaned first). However, we have observed that when the LLM generates the domain, it sometimes mistakenly omits the beverage argument, simplifying the predicate to (used?c - container). At first glance, this might seem like a harmless simplification, as the container usage is still tracked. However, this change results in significant problems in the overall domain behavior. Since the beverage is no longer specified, the domain can no longer differentiate between containers used for different types of beverages. This leads to situations where a container that has already been used for one beverage can be incorrectly treated as if it can hold another beverage without requiring proper cleaning or resetting actions. Such a mistake can cause the final domain to generate invalid plans, as the planner will fail to ensure that containers are used properly with respect to their contents, leading to cascading errors in tasks like mixing drinks, cleaning containers, or pouring from shakers.

```
1{define(problemgripper-2-3-4)
2(:domaingripper-strips)
3(:objectsrobot1robot2-robot
4rgripper1lgripper1rgripper2lgripper2-gripper

Figure 5: Histogram of average number of lines of domains in [22].

[MISSING_PAGE_FAIL:18]

more importantly, sometimes modifies incorrect parts of the domain. For instance, the LLM had a high tendency towards changing the action interface signature, despite the instructions explicitly mentioning not to change the signature. As such, we provide a python interface for the LLM to modify a domain. The interface provides the LLM with the following two functions:

``` add_or_update_predicates( predicates:List[str] ) modify_action( action_name:str, new_preconditions:List[str], new_effects:List[str] ) ```

The first function adds predicates to the list of already created predicates, and the second one modifies the preconditions and effects of a particular action. Guan et al. [9] use a similar approach where they generate the domain PDDL one action at a time, and gradually create predicates. However, our python function interface allows for more flexibility, such as more convenient implementation as well as enabling the LLM to modify an action several times, or introduce predicates in between reasoning steps.

**Domain Rating.** Our main domain rating originates from the EW metric. When generating domain refinement strategies, the LLM may make mistakes hence failing before even the EW metric could be computed. For instance, the modification may be invalid, containing syntax error, or failing to fill parts of the template. To facilitate incorporating these into the EW metric strategy, we create the following rating system for each domain refinement modification:

``` Description&Rating ``` ExplorationWalkExecutable&\(0\leq\)EWScore\(\leq 1\) Noinitialactionpossible&\(-1\) Invaliddomainmodification(_e.g._, undefinedpredicates)&\(-2\) Domainsanitycheckfailure(_e.g._, emptyeffectlist)&\(-3\) Invaliddomainmodification&\(-4\) Nodomainmodification&\(-5\) ```

Figure 6: Overview of our method with domain proposal. To generate a problem PDDL, the LLM first drafts a domain proposal to find suitable predicates for the problem PDDL. Then, the draft is discarded, and the domain refinement stage starts.

when the EW metric is perfect (_i.e._, equals \(1.0\)), we also run the planning on the environment (_i.e._, evaluate \(V_{d,p}(.)\)) and stop early if the plan is valid.

```
1User:
2Incorrect.Theenvironmentreturnedthefollowingerror:
3
4Sampledasetofconsecutiverandomactionsfromthegroundtruthenvironment,buttheactionsarenotexecutableinthegeneratedenvironment.
5Executingthefollowingactionssequentiallyontheenvironment:
6(movepos-2-0pos-3-0n0)
7(movepos-3-0pos-2-0n0)
8(create-blockpos-2-0)
9(movepos-2-0pos-3-0n0)
10(movepos-3-0pos-2-0n0)
11(place-blockpos-2-0pos-1-0n0n1)
12(remove-blockpos-2-0pos-1-0n1n0)
13(place-blockpos-2-0pos-2-1n0n1)
14(remove-blockpos-2-0pos-2-1n1n0)
15(place-blockpos-2-0pos-3-0n0n1)
16(create-blockpos-2-0)
17(place-blockpos-2-0pos-2-1n0n1)
18(create-blockpos-2-0)
19(place-blockpos-2-0)
20(create-blockpos-2-0)
21(move-uppos-2-0n0pos-1-0n1)
22
23Result:Errorwhenexecutingtheaction(move-uppos-2-0n0pos-1-0n1).
24Currentstate:Therobotisatpositionpos-2-0.
25Thisactionisnotexecutableontheenvironment.
26
27Pleasereasonabouttheissuewithyourgeneratedcode.Thecurrentdomainpddlissafollows:
28
29{"pddl
30[CurrentGeneratedPDDL]
31{"pddl
32
33Inyourresponse,pleasegeneratenewcodetofixtheissue.
34
35Assistant:
36Itsseemsthatthereisanissuewiththe'move-up'actionprecondition.Thepreconditionfor'move-up'shouldcheckthatthedestinationposition'?hto'isonelessthanthestartingposition'?hfrom',nottheotherwayaround.Thisisbecausetherobotismovingtoahigherposition,sothedestinationheightshouldbegreaterthanthestartingheight.
37
38Similarly,thepreconditionfor'move-down'shouldcheckthatthedestinationposition'?hto'isonelessthanthestartingposition'?hfrom',indicatingthattherobotismovingtoalowerposition.

[MISSING_PAGE_FAIL:21]

* 3DONOTtranslatetheproblemPDDLfiles,onlyuseproblemPDDL tounderstandthedomain.ALWAYSwrapyourcodeinthe appropriatemarkdownsyntax.
* 4Twoexamplesareprovidedbelow.
* 5Q:
* 6DomainPDDL:
* 7<<'pddl
* 8(define(domaingripper-strips)
* 9(:requirements:strips:typing)
* 10(:typesroomobjrobotgripper)
* 11(:predicates(at-robby?r-robot?x-room)
* 12(at?o-obj?x-room)
* 13(free?r-robot?g-gripper)
* 14(carry?r-robot?o-obj?g-gripper))
* 15(:actionmove
* 16:parameters(?r-robot?from?to-room)
* 17:precondition(and(at-robby?r?from))
* 18:effect(and(at-robby?r?to)
* 19(not(at-robby?r?from))))
* 10(:actionpick
* 11:parameters(?r-robot?obj-obj?room-room?g-gripper)
* 12:precondition(and(at?obj?room)(at-robby?r?room)(free?r?g))
* 13:effect(and(carry?r?obj?g)
* 14(not(at?obj?room))
* 15(not(free?r?g))))
* 16(:actiondrop
* 17:parameters(?r-robot?obj-obj?room-room?g-gripper)
* 18:precondition(and(carry?r?obj?g)(at-robby?r?room))
* 19:effect(and(at?obj?room)
* 11(free?r?g)))
* 12(not(carry?r?obj?g))))
* 13(:problemPDDL:
* 14<<'pddl
* 15(define(problemgripper-2-4-6)
* 16(:domaingripper-strips)
* 17(:objectsrobot1robot2-robot
* 18rgripper1gripper1gripper2lgripper2-gripper
* 19room1room2room3room4-room
* 10ball1ball2ball3ball4ball5ball6-obj)
* 11(:init
* 12(at-robbyrobot1room2)
* 13(freerobot1rgripper1)
* 14(freerobot1gripper1)
* 15(at-robbyrobot2room3)
* 16(freerobot2rgripper2)
* 17(freerobot2lgripper2)
* 18(atballroom3)
* 19(atball2room1)
* 20(atball3room3)* (atball14room2)
* (atball15room4)
* (atball6room4)
* (:goal
* (and
* (atball11room4)
* (atball2room1)
* (atball3room1)
* (atball4room2)
* (atball5room1)
* (atball6room1)
* )
* (:cc
* A:
* (:markdown
* The gripper domain involves a world with multiple rooms, robots, and objects (balls). Each robot has two grippers that can be used to pick up and drop objects. The goal is to move objects from their initial locations to the desired goal locations using the robots and their grippers.
* The domain includes three actions:
* (11. move: This action allows arobot to move from one room to another. The precondition is that the robot must be in the starting room. The effect is that the robot is no longer in the starting room and is now in the destination room.
* (22. pick: This action allows arobot to pick up an object using one of its grippers. The preconditions are that the object and the robot must be in the same room, and the specified gripper must be free (not holding any object). The effect is that the robot is now carrying the object with the specified gripper, the object is no longer in the room, and the gripper is no longer free.
* (33. drop: This action allows arobot to drop an object is carrying in a specific room using one of its grippers. The preconditions are that the robot must be carrying the object with the specified gripper and the robot must be in the specified room. The effect is that the object is now in the room, the gripper is free, and the robot is no longer carrying the object with that gripper.
* (34.
* Q:
* Domain PDDL:
* (:ddl
* (define (domain child-snack)
* (:requirements :typing :equality)
* (:types child bread-portion content-portions sandwichray place)

[MISSING_PAGE_FAIL:24]

* }
* :effect(and(not(ontray?s?t))
* (served?c)))
* (:actionserve_sandwich
* :parameters(?s-sandwich?c-child?t-tray?p-place)
* :precondition(and(not_allergic_gluten?c)
* (waiting?c?p)
* (ontray?s?t)
* (at?t?p))
* :effect(and(not(ontray?s?t))
* (served?c)))
* (:actionmove_tray
* :parameters(?t-tray?p1?p2-place)
* :precondition(and(at?t?p1))
* :effect(and(not(at?t?p1))
* (at?t?p2)))
* }
* * *
* ProblemPDDL:
* * *'pddl
* ; child-snacktaskwith6childrenand0.4glutenfactor
* ;constantfactorof1.3
* ;randomseed:234324
* (define(problemprob-snack)
* (:domainchild-snack)
* (:objects child1child2child3child4child5child6-child
* (bread1 bread2bread3bread4bread5bread6-bread-portion
* (content1content2content3content4content6-content-portion
* (tray1tray2-tray
* (table1table2table3-place
* (sandw1sandw2sandw3sandw4sandw5sandw6sandw7sandw8-sandwich
* ) )
* (:init
* (atray1kitchen)
* (atray2kitchen)
* (at_kitchen_bread1)
* (at_kitchen_bread2)
* (at_kitchen_bread3)
* (at_kitchen_bread4)
* (at_kitchen_bread4)
* (at_kitchen_bread5)
* (at_kitchen_bread6)
* (at_kitchen_content1)
* (at_kitchen_content2)
* (at_kitchen_content3)
* (at_kitchen_content4)
* (at_kitchen_content5)
* (at_kitchen_content6)
* (no_gluten_bread2)
* (no_gluten_breadbread5)
* (no_gluten_bread5)
* (no_gluten_content3)

[MISSING_PAGE_FAIL:26]

thesandwichmustnotalreadyexist.Theeffectofthisactionisthatthebreadandcontentarenolongervailableinthekitchen,andasandwichismowavailableinthekitchen.
* 239.put_on_tray:Thisactionisforplacingasandwichonataray.Thepreconditionsarethatthesandwichmustbeinthekitchenandthetraymustbeatthekitchen.Theeffectisthatthesandwichisnolongerinthekitchenandisnowonthetray.
* 2304.serve_sandwich_no_gluten:Thisactionisforservingagluten-freesandwichtoachildwhoisallergictogluten.Thepreconditionsarethatthechildisallergictogluten,waitingataplace,andthereisagluten-freesandwichonatrayatthesamplace.Theeffectisthatthesandwichisnolongeronthetrayandthechildiserved.
* 2315.serve_sandwich:Thisactionisforservingasandwichtoachildwhoisnotallergictogluten.Thepreconditionsarethatthechildisnotallergictogluten,waitingataplace,andthereisasandwichonatrayatthesamplace.Theeffectisthatthesandwichisnolongeronthetrayandthechildiserved.
* 2326.move_tray:Thisactionallowsatraytobemovedfromoneplacetoanother.Thepreconditionisthatthetrayisatthestartingplace.Theeffectisthatthetrayisnolongeratthestartingplaceandisnowatthedestinationplace.
* 2337.* 234Q:
* 235DomainPDDL:
* 236*'pddl
* 237(define(domaintermes)
* 238(:requirements:typing:negative-preconditions)
* 239(:types
* 240num-object
* 241position-object
* 242)
* 243(:predicates
* 244(height?p-position?h-numb)
* 245(at?p-position)
* 246(has-block)
* 247;
* 248;staticpredicates
* 249(SUCC?n1-numb?n2-numb)
* 250(NEIGHBOR?p1-position?p2-position)
* 251(IS-DEPOT?p-position)
* 252)
* 253(:actionmove
* 254:parameters(?from-position?to-position?h-numb)
* 255:precondition
* 256(and
* 257(at?from)
* 258(NEIGHBOR?from?to)
* 259(height?from?h)(height?to?h)
261 )
262 :effect
263 (and
264 (not (at?from))
265 (at?to)
266 )
267 )
268 (:action move-up
269 :parameters (?from - position?hfrom - numb?to - position?hto - numb)
270 :precondition
271 (and
272 (at?from)
273 (NEIGHBOR?from?to)
274 (height?from?hfrom)
275 (height?to?hto)
276 (SUCC?hto?hfrom)
277 )
278 :effect
279 (and
280 (not (at?from))
281 (at?to)
282 )
283 )
284 (:action move-down
285 :parameters (?from - position?hfrom - numb?to - position?hto - numb)
286 :precondition
287 (and
288 (at?from)
289 (NEIGHBOR?from?to)
290 (height?from?hfrom)
291 (height?to?hto)
292 (SUCC?hfrom?hto)
293 )
294 :effect
295 (and
296 (not (at?from))
297 (at?to)
298 )
299 )
300 (:action place-block
301 :parameters (?rpos - position?bpos - position?hbefore - numb?hafter - numb)
302 :precondition
303 (and
304 (at?rpos)
305 (NEIGHBOR?rpos?bpos)
306 (height?rpos?hbefore)
307 (height?bpos?hbefore)
308 (SUCC?hafter?hbefore)
309 (has-block)
310 (not (IS-DEPOT?bpos))* 311 )
* 312 :effect
* 313 (and
* 314 (not (height?bpos?hbefore))
* 315 (height?bpos?hafter)
* 316 (not (has-block))
* 317 )
* 318 )
* 319 (:actionremove-block

* 321 :precondition
* 322 (and
* 323 (at?rpos)
* 324 (NEIGHBOR?rpos?bpos)
* 325 (height?rpos?hafter)
* 326 (height?bpos?hbefore)
* 327 (SUCC?hbefore?hafter)
* 328 (not (has-block))
* 329 )
* 330 :effect
* 331 (and
* 332 (not (height?bpos?hbefore))
* 333 (height?bpos?hafter)
* 334 (has-block)
* 335 )
* 336 )
* 337 (:actioncreate-block

* 339 :precondition
* 340 (and
* 341 (at?p)
* 342 (not (has-block))
* 343 (IS-DEPOT?p)
* 344 )
* 345 :effect
* 346 (and
* 347 (has-block)
* 348 )
* 349 )
* 350 (:actiondestroy-block

* 352 :precondition
* 353 (and
* 354 (at?p)
* 355 (has-block)
* 356 (IS-DEPOT?p)
* 357 )
* 358 :effect
* 359 (and
* 360 (not (has-block))
* 361 )
* 362 )
* 363 )

[MISSING_PAGE_EMPTY:30]

[MISSING_PAGE_EMPTY:31]

* 3.move-down:Thisactionallowstherobottomovefromahigherpositiontoaneighboringlowermosition.Thepreconditionsarethattherobotisattentastingposition,thestartingpositionisaneighbortothedestinationposition,thestartingpositionhasacertainheight,andthedestinationposition'sheightisonelessthanthestartingposition'sheight.Theeffectisthattherobotisnologeratthestartingpositionandisnowatthedestinationposition.
* 4.place-block:Thisactionallowstherobottoplaceablockataneighboringposition,increasingtheheightofthantpositionbyone.Thepreconditionsarethattherobotisatapositionnexttotheblockposition,bothpositionshavethesameheight,therobothasablock,andtheblockpositionisnotadepot.Theeffectisthattheheightoftheblockpositionisincreasedbyone,andtherobotnolongerhasablock.
* 5.remove-block:Thisactionallowstherobottoremoveablockfromaneighboringposition,decreasingtheheightofthantpositionbyone.Thepreconditionsarethattherobotisatapositionnexttotheblockposition,therobot'spositionisoneheightunithigherthantheblockposition,andtherobotdoesnothaveablock.Theeffectisthattheheightoftheblockpositionisdecreasedbyone,andtherobotnowhasablock.
* 6.create-block:Thisactionallowstherobottocreatablockatadepot.Thepreconditionsarethattherobotisatthedepotanddoesnothaveablock.Theeffectisthattherobotnowhasablock.
* 7.destroy-block:Thisactionallowstherobottodestroyablockatadepot.Thepreconditionsarethattherobotistathedepotandhasablock.Theeffectisthattherobotnolongerhasablock.
* 6.* 7Q:
* 8DomainPDDL:
* 9.pdf
* 10.TarargetDomainPDDLCode]
* 11.TarargetProblemPDDLCode]
* 12.TarargetProblemPDDLCode]
* 13.TarargetProblemTemplate,withdomainPDDLandproblemPDDLplaceholdersforeachtargetdomain.SomePDDLcreditcommentsareomitedforclarity.

```
1Yourtaskistotranslateg problemPDDLfilesintonaturallanguage.Ensurethattheresultingdescriptioncoversallinitialstateandgoalconditions.
2DONOTbelazyinyourresponse,beextremelypreciseinyourdescriptionsuchthatallconditionsarecoveredinyourdescriptionandthereisnoambiguityinyourdescription.
3Ifyoudnotfindanycommonruleaboutsomeconditions,listalloftthem.

* 4Fortheinitialconditions,startwith"Initially:",andforthegoalconditions,startwith"Yourgoalisto".
* 5ALWAYSwrapyourcodeintheappropriatemarkdownsyntax.
* 6Twoexamplesareprovidedbelow.
* 7Q:
* 8DomainDescription:
* 9"""arkdown
* 10TheTermesdomainisaplanningdomainthatusimulatesthebehaviorofroboticagents(inspiredbytermites)thatcanmovearound,pickupblocks,stackthemetbuildstructures,andremoveblocksfromstructures.Thedomainincludesactionsformovingtherobot,placingandremovingblocks,andcreatinganddestroyingblocksata depot.
* 11Theactionsdefinedinthisdomaininclude:
* 12.move:Thisactionallowstherobottomovefromonepositiontomotheratthesameheight.Thepreconditionsarethattherobotistatthestartingposition,thestartingpositionisaneighborothedestinationposition,andbothpositionshavethesameheight.Theeffectisthattherobotisnologeratthestartingpositionandisnowatthedestinationposition.
* 13.move-up:Thisactionallowstherobottomovefromalowerpositiontoaneighboringhigherposition.Thepreconditionsarethattherobotistatthestartingposition,thestartingpositionhasacertainheight,andthedestinationposition'sheightisonelessthanthestartingposition'sheight.Theeffectisthattherobotismolongeratthestartingpositionandisnowatthedestinationposition.
* 14.move-down:Thisactionallowstherobottomovefromahigherpositiontoaneighboringlowperposition.Thepreconditionsarethattherobotistatthestartingposition,thestartingpositionisaneightotothedestinationposition,thestartingpositionhasacertainheight,andthedestinationposition'sheightisonelessthanthestartingposition'sheight.Theeffectisthattherobotismolongeratthestartingpositionandisnowatthedestinationposition.
* 154.place-block:Thisactionallowstherobottopaceablockataeighboringposition,increasingtheheightofthatpositionbyone.Thepreconditionsarethattherobotisatapositionnexttotheblockposition,bothpositionshavethesameheight,therobothasablock,andtheblockpositionisnotadepot.Theeffectisthattheheightoftheblockpositionisincreasedbyone,andtherobotnolongerhasablock.
* 165.remove-block:Thisactionallowstherobottoremoveablockfromaneighboringposition,decreasingtheheightofthatpositionbyone.Thepreconditionsarethattherobotisatapositionnexttotheblockposition,therobot'spositionisoneheightunithigherthantheblockposition,andtherobotdoesnothaveablock.Theeffect

[MISSING_PAGE_EMPTY:34]

[MISSING_PAGE_POST]

S-DEPDT pos-2-0)
* (:goal
* (and
* (height pos-0-0 n0)
* (height pos-0-1 n0)
* (height pos-0-2 n0)
* (height pos-1-0 n0)
* (height pos-1-1 n1)
* (height pos-1-2 n0)
* (height pos-2-0 n0)
* (height pos-2-1 n0)
* (height pos-2-2 n0)
* (not (has-block))
* (not (has-block))
* )
* )
* * *
* (00 A:
* (*'markdown
* (01) You control arobot capable of building structures by moving and manipulating blocks.
* (03) The environment consists of a grid of positions and two heights, numbered 0 and 1. Therobot can move at the same height, move up one height, or movedown one height. It can also place or remove a block at a neighboring position, or create or destroy a block at the depot. A block's height increases by one when placed and decreases by one when removed.
* (04) Initially:
* (05) All positions on the grid have a height of 0.
* (06) Therobot is at position pos-2-0, which is the depot.
* (07) Therobot does not have a block.
* (08) The positions have the following neighboring relationships:
* pos-0-0 neighbors pos-1-0 and pos-0-1
* 10-pos-0-1neighborspos-1-1,pos-0-0,andpos-0-2
* 11-pos-0-2neighborspos-1-2andpos-0-1
* 12-pos-1-0neighborspos-0-0,pos-2-0,andpos-1-1
* 13-pos-1-1neighborspos-0-1,pos-2-1,pos-1-0,andpos-1-2
* 14-pos-1-2neighborspos-0-2,pos-2-2,andpos-1-1
* 15-pos-2-0neighborspos-1-0andpos-2-1,andisthedepot
* 16-pos-2-1neighborspos-1-1,pos-2-0,andpos-2-2
* 17-pos-2-2neighborspos-1-2andpos-2-1
* 18-Thereisasuccessorrelationshipbetweenhenumbersn1andn0.
* 19Yourgoalistochaiveethefollowingconfiguration:
* 20-Theheightatpos-1-1needstobe1.
* 21-Allotherpositionsmustremainatheight0.
* 22-Therobotshouldnothaveablockattheendofthetask.
* 23<<
* Q:
* DomainDescription:
* <<'markdown
* Thesatellitedomainisdesignedtomodeltheoperationofsatellitesthatcantakeimagesofvarioustargetsindifferentmodes.Eachsatelliteisequippedwithinstrumentsthatcanbetwuredonandoff,calibrated,andusedtotakeimages.Thedomainincludesactionsforturningthesatellitetopinatdifferentdirections,switchinginstrumentsonandoff,calibratinginstruments,andtakingimages.
* Theactionsdefinedinthisdomaininclude:
* 291.turn_to:Thisactionchangesthedirectionthesatelliteispointing.Thepreconditionsarethatthesatellitemustbepointingatapreviousdirection,andboththenewandpreviousdirectionsarevalid.Theeffectistathethesatelliteisnowpointingatthenewdirectionandnolongerpointingatthepreviousdirection.
* 302.switch_on:Thisactionturnsonaninstrumentonboardstatellite.Thepreconditionsarethattheinstrumentmustbeonboardthesatelliteandtheremustbepoweravailableonthesatellite.Theeffectistathetheinstrumentispoweredon,itisnolongercalibrated,andthesatellitenolongerhaspoweravailable.
* 313.switch_off:Thisactionturnsoffaninstrumentonboardthesatellite.Thepreconditionsarethattheinstrumentmustbeonboardthesatelliteanditmustbepoweredon.Theeffectisthatthesatellitehaspoweravailableandtheinstrumentisnolongerpoweredon.
* 324.calibrate:Thisactioncalibratesaninstrumentonboardthesatellite.Thepreconditionsarethatthesatellitemustbepointingatacablibrationtargetfortheinstrument,theinstrumentmustbeonboardthesatelliteandpoweredon.Theeffectistathetheinstrumentiscalibrated.
* 335.take_image:Thisactionusesaninstrumentonboardstatellitetotalangeinaspecificmodeofadirectionthesatelliteispointingat.Thepreconditionsarethatthesatellitemustbepointingatthedirection,

[MISSING_PAGE_FAIL:37]

* [184] (calibration_target instrument0 Star0)
* [185] (on_board instrument0 satellite0)
* [186] (power_avail satellite0)
* [187] (pointing satellite0 Star8)
* [188] (satellitesatellite1)
* [189] (instrument instrument1)
* [190] (supports instrument1 infrared0)
* [191] (supports instrument1 infrared1)
* [192] (calibration_target instrument1 GroundStation3)
* [193] (instrument instrument2)
* [194] (supports instrument2 infrared1)
* [195] (supports instrument2 infrared0)
* [196] (calibration_target instrument2 Star2)
* [197] (instrument instrument3)
* [198] (supports instrument3 spectrograph4)
* [199] (supports instrument3 infrared1)
* [200] (supports instrument3 thermograph2)
* [201] (calibration_target instrument3 Star0)
* [202] (on_board instrument1 satellite1)
* [203] (on_board instrument2 satellite1)
* [204] (on_board instrument3 satellite1)
* [205] (power_avail satellite1)
* [206] (pointing satellite1 GroundStation3)
* [207] (satellitesatellite2)
* [208] (instrument4)
* [209] (supports instrument4 infrared1)
* [210] (supports instrument4 image3)
* [211] (supports instrument4 infrared0)
* [212] (calibration_target instrument4 Star2)
* [213] (instrumentument5)
* [214] (supports instrument5 thermograph2)
* [215] (supports instrument5 spectrograph4)
* [216] (calibration_target instrument5 Star0)
* [217] (instrument instrument6)
* [218] (supports instrument6 infrared0)
* [219] (calibration_target instrument6 GroundStation3)
* [220] (on_board instrument4 satellite2)
* [221] (on_board instrument5 satellite2)
* [222] (on_board instrument6 satellite2)
* [223] (power_avail satellite2)
* [224] (pointing satellite2 Star4)
* [225] (satellitesatellite3)
* [226] (instrumentinstrument7)
* [227] (supports instrument7 image3)
* [228] (calibration_target instrument7 Star2)
* [229] (on_board instrument7 satellite3)
* [230] (power_avail satellite3)
* [231] (pointing satellite3 Phenomenon9)
* [232] (satellitesatellite4)
* [233] (instrumentishment8)
* [234] (supports instrument8 infrared0)
* [235] (supports instrument8 spectrograph4)
* [236] (supports instrument8 infrared1)
* [237] (calibration_target instrument8 Star2)(on_boardinstrument8 satellite4)
* (power_availsetellite4)
* (pointing satellite4 Phenomenon9)
* (mode thermograph2)
* (mode image3)
* (mode infrared1)
* (mode spectrograph4)
* (mode infrared0)
* (direction Star1)
* (direction Star4)
* (direction Star0)
* (direction GroundStation3)
* (direction Star2)
* (direction Star5)
* (direction Planet6)
* (direction Phenomenon7)
* (direction Star8)
* (direction Phenomenon9)
* (direction Star10)
* (direction Star11)
* (direction Star12)
* (direction Planet13)
* (direction Planet14)
* (direction Phenomenon15)
* (direction Planet16)
* (direction Star17)
* (direction Star18)
* (direction Planet19)
* (direction
* (pointing satellite0 Phenomenon9)
* (pointing satellite1 Star4)
* (pointing satellite4 Star11)
* (have_imageStar5 image3)
* (have_imagePlanet6 infrared1)
* (have_imagePhenomenon7 infrared1)
* (have_imageStar8 image3)
* (have_imageStar10 thermograph2)
* (have_imageStar11 infrared1)
* (have_imagePlanet13 spectrograph4)
* (have_imagePlanet14 thermograph2)
* (have_imagePhenomenon15 infrared0)
* (have_imagePlanet16 image3)
* (have_imageStar17 infrared0)
* )

* A:

* Youareoperatingaconsellationofsatellitestaskedwithtakingimagesofvarioustargetsindifferentmodes.
* Initially:
* Therearefivesatellites(satellite0tostallite4)withcorrespondinginstruments(instrument0tostinstrument8).

- Eachinstrumportspecificmodesandhasa calibrationtarget:

* Instrument1supportsinfrared0andinfrared1,targeting GroundStation3.
* Instrument2supportsinfrared1andinfrared0,targeting Star2.
* Instrument3supportsspectrograph4,infrared1,and thermograph2,targetingStar0.
* Instrument4supportsinfrared1,image3,andinfrared0, targetingStar2.
* Instrument5supportsthermograph2andspectrograph4, targetingStar0.

* Instrument8supportsinfrared0,spectrograph4,and infrared1,targetingStar2.
* Instrumentsareonboardtheirrespectivelyastellites,and allsatelliteshavepoweravailable.

* Therearevariousmodes(thermograph2,image3,infrared1, spectrograph4,infrared0)anddirections(Star1toStar18, GroundStation3, Planet6,Phenomenon7,Phenomenon9, Planet13, Planet14,Phenomenon15, Planet16, Planet19).
* 308 Yourgoalisto:

* 324 Toachievethesegoals,youwillneedtoturnthesatellites topointatthecorrectdirections,switchonand calibratethemecessaryinstrments,andtakeimagesusingthecalibratedinstrumentsinthesupportedmodes.

* 326 Q:
* 327 DomainDescription:

* 47 markdown
* 329 [TargetDomainNaturalLanguageDescription]* [31] Problem PDDL:
* [32] <<'pddl
* [Target Problem PDDL Code]
* [33] <<'
* A: ```

Listing 11: Domain back-translation prompt template, with domain natural language description and problem PDDL placeholders for each target domain.

```
1Yourtaskistogeneratelyothpredicatedescriptorforeachenvironment.Youaregiventhenaturallanguage descriptionofthedomainalongwiththePDDLcode.
2Q:
3DomainDescription:
4"""arkdown
5Therobothasfouractions:pickup,putdown,stack,and unstack.Thedomainassumesaworldwherethereareast ofblocksthatcanbestackedontopofeachother,anarm thatcanholdoneblockatatime,andatablewhere blockscanbeplaced.
6Theactionsdefinedinthisdomaininclude:
7pickup:allowsthearmtopickupapablockfromthetableifit isclearandthearmisempty.Afterthepickupaction, thearmwillbeholdingtheblock,andtheblockwillno longerbententableorclear.
8putdown:allowsthearmtoputdownablockonthetableifit isholdingablock.Aftertheputdownaction,thearmwill beempty,andtheblockwillbeonthetableandclear.
9stack:allowsthearmtostackablockontopofanotherblock ifthearmisholdingthetopblockandthebottomblock isclear.Afterthestackaction,thearmwillbeempty, thetopblockwillbeontopofthebottomblock,andthe bottomblockwillnolongerbeclear.
10unstack:allowsthearmtounstackablockfromontopof anotherblockifthearmisemptyandthetopblockis clear.Aftertheunstackaction,thearmwillbeholdingthetopblock,thetopblockwillnolongerbeontopof thebottomblock,andthebottomblockwillbeclear.
11"""
12DomainPDDL:
13"""pddl
14(define(domainblocksworld-4ops)
15(:requirements:strips)
16(:predicates(clear?x)
17(on-table?x)
18(arm-empty)
19(holding?x)
20(on?x?y))
21(:actionpickup
22:parameters(?ob)
23:precondition(and(clear?ob)(on-table?ob)(arm-empty))
24:effect(and(holding?ob)(not(clear?ob))(not(on-table?ob))
25(not(arm-empty))))

[MISSING_PAGE_FAIL:42]

* [75]else:
* [76]raiseValueError(f"Unknownpredicate: {predicate_name}")
* [77]<<< [78]Q:
* [90]DomainDescription:
* [10]<'markdown
* [11][TargetDomainNaturalLanguageDescription]
* [12]<< [13]DomainPDDL:
* [14]<'>ddl
* [15][TargetDomainPDDLCode]
* [16]<< [17]A:

Listing 12: Predicate translation python code generation prompt.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims are supported by experiments. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In the conclusion section, we discuss limitations of our work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: Our paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe our method in Algorithm 1, and Appendix B. The code will be made publicly available upon publication. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: The code will be made publicly available upon publication. We explain our method in Algorithm 1. For the data, the PDDL files are publicly available, and we provide examples on how to obtain natural language descriptions. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Our method uses pre-trained large language models, and we provide examples in Algorithm 1 on how we prompt the LLMs. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Figure 2b and Figure 4 show standard error. Table 2 does not contain any confidence intervals as the experiments become computationally expensive. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In the experiments section, we reported the number of tokens used to create Table 2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The setup used in our work does not have direct societal impact as it is centered around generating PDDL code. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work is a fundamental work and poses no such risk. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the sources from which we use library, data, and code. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The code will be made publicly available upon publication Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.