# ReactZyme: A Benchmark for

Enzyme-Reaction Prediction

 Chenqing Hua\({}^{1,3}\)1 Bozitao Zhong\({}^{2}\)1 Sitao Luan\({}^{1,3}\)

 Liang Hong \({}^{2}\) Guy Wolf \({}^{3,4}\) Doina Precup \({}^{1,3,5}\) Shuangjia Zheng\({}^{2}\)2

\({}^{1}\)McGill; \({}^{2}\)SJTU; \({}^{3}\)Mila; \({}^{4}\)UdeM; \({}^{5}\)DeepMind

Footnote 1: Co-authorship

Footnote 2: Correspondence to: chenqing.hua@mail.mcgill.ca; shuangjia.zheng@sjtu.edu.cn

###### Abstract

Enzymes, with their specific catalyzed reactions, are necessary for all aspects of life, enabling diverse biological processes and adaptations. Predicting enzyme functions is essential for understanding biological pathways, guiding drug development, enhancing bioproduct yields, and facilitating evolutionary studies. Addressing the inherent complexities, we introduce a new approach to annotating enzymes based on their catalyzed reactions. This method provides detailed insights into specific reactions and is adaptable to newly discovered reactions, diverging from traditional classifications by protein family or expert-derived reaction classes. We employ machine learning algorithms to analyze enzyme reaction datasets, delivering a much more refined view on the functionality of enzymes. Our evaluation leverages the largest enzyme-reaction dataset to date, derived from the SwissProt and Rhea databases with entries up to January 8, 2024. We frame the enzyme-reaction prediction as a retrieval problem, aiming to rank enzymes by their catalytic ability for specific reactions. With our model, we can _recruit proteins for novel reactions_ and _predict reactions in novel proteins_, facilitating enzyme discovery and function annotation (https://github.com/WillHua127/ReactZyme).

## 1 Introduction

Enzymes, as catalysts of biological systems, are the workhorses of various biological functions [35, 52, 13] (Fig. 1a). They accelerate and regulate nearly all chemical processes and metabolic pathways in organisms, from simple bacteria to complex mammals [53, 18]. The ability to understand and manipulate enzyme functions is fundamental to numerous scientific and industrial fields, including biosynthesis, where enzymes help to produce complex organic molecules [16, 42], and synthetic biology, where they are engineered to create novel biological pathways [19, 34, 24]. Furthermore, they can break down pollutants, thus playing a significant role in bio-remediation efforts [57, 75]. In the realm of protein evolution, examining enzyme functions across the tree of life enhances our understanding of the evolutionary processes that sculpt metabolic networks and enable organisms to adapt to their environments [31, 20, 11, 54]. As such, gaining insights into enzyme function is not merely an academic pursuit in life sciences but a necessity for practical applications in medicine, agriculture, and environmental management.

The current methodologies for enzyme annotation primarily rely on established databases and classifications such as KEGG Orthology (KO), Enzyme Commission (EC) numbers, and Gene Ontology (GO) annotations, each with its specific focus and methodology [65] (Fig. 1b). For instance, the EC system categorizes enzymes based on the chemical reactions they catalyze, providing a hierarchical numerical classification [4]. KO links gene products to their functional orthologs across different species [48], whereas GO offers a broader ontology for describing the roles of genes and proteins in any organism [12].

Despite their widespread use, these systems have notable limitations. The EC classification, while widely used, sometimes groups vastly different enzymes under the same category or subdivides similar ones excessively, based on the substrates they interact with--leading to ambiguities in enzyme function characterization. GO annotations, although comprehensive, frequently lack specificity in defining enzyme functions and suffer from an underdeveloped database structure. Similarly, KO tends to categorize based on gene or protein families rather than specific functions, potentially assigning different identifiers to proteins with identical functions [15; 50].

Given these challenges, we propose a novel benchmark and a new enzyme-reaction dataset to learn enzymes more accurately by focusing on their catalyzed reactions directly rather than solely on gene family or human-assigned function types. The ReactZyme codes and dataset can be found on https://github.com/WillHua127/ReactZyme & https://zenodo.org/records/13635807. Our approach also leverages machine learning techniques--graph representation learning and protein language models--to analyze enzyme reaction data, providing a more nuanced understanding of enzyme functionality. This method aims to overcome the limitations of current annotation systems by offering a clearer, more consistent categorization of enzymes based on their biochemical roles, which could significantly enhance both academic research and industrial applications in enzyme technology. To this end, we summarize our ReactZyme enzyme-reaction dataset in Section 3 and the approach in Section 4 with a method visualization in Fig. 2, and introduce and the retrieval challenge and experiments in Section 5.

## 2 Related Work

**Protein Function Annotation**. Protein function annotation is a foundational task in bioinformatics, typically utilizing databases like Gene Ontology (GO), Enzyme Commission (EC) numbers, and KEGG Orthology (KO) annotations [12; 4; 48]. Traditional methods such as BLAST, PSI-BLAST, and eggNOG rely on sequence alignments and similarities to infer function [3; 2; 29]. Recently, deep learning has introduced innovative approaches for protein function prediction [56; 39; 8]. There are 2 types of protein function prediction model, one uses only protein sequence as their input, while the other also uses experimentally-determined or predicted protein structure as input. Generally, these methods typically predict EC or GO information to approximate protein functions, distinct from describing the exact catalysed reaction.

**Protein-Ligand Interaction Prediction**. Protein-ligand interaction prediction is another related area, with numerous models designed to identify potential bindings between proteins and ligands [10; 25; 73]. Most existing models, such as those for drug-target interaction (DTI), focus on stable bindings critical for therapeutic efficacy [72; 14], which differs from substrate-enzyme interactions where binding does not necessarily result in catalysis. Some models have also tackled the specific challenge of enzyme-substrate prediction, including the ESP model [37; 38]. This area differs from drug-target interactions, underscoring the unique dynamics of enzyme-substrate relationships where the interaction may not always lead to stable binding.

**Protein-Ligand Structure Prediction**. The protein-ligand structure prediction task, also referred to as ligand docking, has evolved with new methodologies emerging [14; 80; 1; 26]. Traditional docking methods like Vina [63], Gold [70], and Glide [17] have been complemented by deep learning approaches such as EquiBind [60], TankBind [43], E3Bind [81], UniMol [83], and DiffDock [14]. Moreover, recent advances in protein-ligand structure prediction, such as AlphaFold 3 [1], RFAA

Figure 1: Overview of the enzyme-reaction prediction task. (a) Illustration of the enzymatic reaction process: substrate binds to the enzyme; formation of the enzyme-substrate complex; release of the product, leaving the enzyme for another catalytic cycle. (b) Current methods for enzyme reaction prediction: Search for annotated enzymes (e.g. sequence-based BLAST [2], structure-based FoldSeek [67]); prediction of EC/GO annotation (e.g. CLEAN [77]); enzyme-reaction prediction (ReactZyme).

[36], and Umol [9], provide detailed structural models of protein-ligand complexes, but they do not specifically address the functional interactions between enzymes and substrates. These methods are crucial for structure-based models but offer limited insight into the functional dynamics essential for understanding enzyme activity.

**Graph Representation Learning for Bioinformatics**. Graph representation learning emerges as a potent strategy for representing and learning about proteins and molecules, focusing on structured, non-Euclidean data [58; 47; 45; 46; 28; 44]. In this context, proteins and molecules can be effectively modeled as 2D graphs or 3D point clouds, where nodes correspond to individual atoms or residues, and edges represent interactions between them [21; 82; 27; 78]. Indeed, representing proteins and molecules as graphs or point clouds offers a valuable approach for gaining insights into and learning the fundamental geometric and chemical mechanisms governing protein-ligand interactions. This representation allows for a more comprehensive exploration of the intricate relationships and structural features within protein-ligand structures [64; 30; 79].

## 3 ReactZyme Dataset

### Dataset

**Overview**. Our study utilizes a comprehensive dataset compiled from the SwissProt and Rhea databases [7; 5]. SwissProt, a curated subset of the UniProt database, has been selected for its high-quality, human-derived functional annotations of protein sequences. This section of UniProt is particularly valuable for its expert-reviewed entries, which ensure reliable and accurate functional data, making it ideal for our analysis. Rhea is employed for its precise mapping from enzymes to specific catalyzed functions, offering detailed descriptions of biochemical reactions. The ReactZyme dataset can be downloaded via https://zenodo.org/records/11494913.

**Data Collection**. The SwissProt and Rhea dataset are downloaded on January 8, 2024, and includes data entries up to this date, providing the most recent and comprehensive data available for our study. We selectively exclude water molecules and unspecific functional groups that could mask the true molecular structures. Conversely, we keep metal ions, gas molecules, and other small molecules because of their potential to bind to proteins, a characteristic that presents a valuable learning feature for our model. To this end, the total dataset comprises \(178,463\) positive enzyme-reaction pairs, including \(178,327\) unique enzymes and \(7,726\) unique reactions.

**Compare to Other Datasets**. There are two datasets related to the enzyme-reaction prediction task. The first one is from ESP [37], which used GO annotation database for UniProt dataset, lay emphasis on the substrate binding to the enzyme. The ESP dataset contains \(18,351\) enzyme-substrate pairs with experimental evidence for substrate binding, contains \(12,156\) unique enzymes and \(1,379\) unique molecules. The other dataset is from EnzymeMap [23], which used as training set in CLIPZyme [51]. EnzymeMap is a high-quality dataset of atom mapped and balanced enzymatic reaction, with enzyme information from BRENDA [59]. This dataset contains \(46,356\) enzyme-driven reactions, including \(16,776\) distinct reactions and \(12,749\) enzymes. A comparison is illustrated in Table 1.

**ReactZyme Limitation**. While ReactZyme has the advantage of containing significantly more data than both ESP and EnzymeMap, it has some limitations. Notably, it lacks atom-mapping data, and the number of reactions is smaller than in EnzymeMap. This reduction in reaction count is because some reactions in ReactZyme are represented using functional groups rather than the full substrate. Futhermore, ReactZyme may not include sufficient coverage of the entirety of space of proteins and reactions in practical use. ReactZyme can be developed further for more practical interest in enzyme and substrate design.

### Data Split

We provide three dataset splits based on time, enzyme similarity, and reaction similarity. For each data split, \(10\%\) of the training data are randomly sampled for validation.

**Time Split**. The first data-split method is based on a specific date. We split the training and test samples by selecting enzyme-reaction pairs before \(2010\)-\(12\)-\(31\), for training and pairs after this date

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c} \hline Dataset & \#Pair & \#Enzyme & \#Molecule/Reaction & SubstrateInfo & ProductInfo & ReactionInfo & Atom-Mapping \\ \hline ESP & \(18,351\) & \(12,156\) & \(1,379\) & ✓ & ✗ & ✗ & ✗ \\ \hline EnzymeMap & \(46,356\) & \(12,749\) & \(16,776\) & ✓ & ✓ & ✓ & ✓ \\ \hline ReactZyme & \(178,463\) & \(178,327\) & \(7,726\) & ✓ & ✓ & ✓ & ✗ \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of ESP, EnzymeMap, and ReactZymefor testing. This results in \(166,175\) training pairs and \(12,287\) test pairs, approximately a \(93\%/7\%\) training/test ratio. The training samples include \(166,084\) unique enzymes and \(7,726\) unique reactions, while the test samples include \(12,277\) unique enzymes and \(2,634\) unique reactions.

**Enzyme Similarity**. The second data-split method is based on enzyme similarity. We ensure that enzymes in the training set do not appear in the test set, using the Levenshtein distance [6] for sequence-based protein sequence comparison, ensuring at least \(60\%\) sequence difference between training and test set enzymes. This results in \(169,724\) training pairs and \(8,739\) test pairs, approximately a \(95\%/5\%\) training/test ratio. The training samples include \(169,596\) unique enzymes and \(7,726\) unique reactions, while the test samples include \(8,734\) unique unseen enzymes and \(1,573\) unique reactions.

**Reaction Similarity**. The third data-split method is based on reaction similarity, calculated by the Needleman-Wunsch algorithm on SMILES. We ensure that reactions in the training set do not appear in the test set. This results in \(163,771\) training pairs and \(14,692\) test pairs, approximately a \(91\%/9\%\) training/test ratio. The training samples include \(163,651\) unique enzymes and \(7,340\) unique reactions, while the test samples include \(14,688\) unique enzymes and \(386\) unique unseen reactions.

**Negative Sample**. A common method involves designating all enzymes within a training set that are not annotated for catalyzing a specific reaction as negative samples [51]. Nevertheless, given the extensive size of our dataset, we opt for a strategy centered on enzyme and reaction similarity to construct negative samples. Specifically, for each verified positive enzyme-reaction pair, we identify the top-k enzymes that closely resemble the positive enzyme but do not have annotations for catalyzing the reaction, using them as negative samples. Similarly, we select the top-k reactions that are similar to the positive reaction but are not catalyzed by the positive enzyme, to serve as additional negative samples (k=1000). This method effectively narrows down the size of negative samples while retaining those of significance for both training and testing purposes. Despite our approach, the construction of negative samples still presents an unresolved challenge, remaining as an open question for future development.

## 4 ReactZyme Approach

We conceptualize the prediction of enzyme-substrate/product as a retrieval task, where it seeks to rank a given list of enzyme proteins according to their catalytic efficacy for a specified chemical reaction [51]. The overarching goal is to understand the intricate interactions between enzymes and chemical reactions. To this end, we formulate strategies for the representation of the reactions and proteins to enhance the generalization capabilities of machine learning models in the retrieval task. More specifically, we highlight the development of representation methods that capture structural and functional subtleties of enzymes and reactions, which play a central role in predicting enzyme-substrate compatibility and catalytic potential. Our approach is visualized in Fig. 2.

Figure 2: Our methodology begins with the computation of conformations for structural insights from given reactions. Similarly, for enzymes, we employ AlphaFold to obtain their structures. Then, molecule encoders are used to transcribe 2D molecular graphs alongside their 3D geometry. For the initialization of enzyme features, protein language models are employed. The substrates and products are refined through cross-attention and the merged to form a single reaction representation. Enzyme features are further refined using an equivariant-GNN. These enzyme embeddings, along with reaction embeddings, are processed through an encoder-decoder to establish pair-wise relationships. And, a probability matrix between enzymes and reactions is computed to facilitate retrieval.

### Multi-View Reaction Representation

In representing the substrate and product of catalytic reactions, we employ both string and graph representations to capture the transition from substrates to products. Diverging from the previous enzyme datasets, such as CLEAN [77] and CLIPZyme [51], our dataset uniquely offers a combination of graph and geometric data representations. This allows the structural and functional information that is inherent in reactions to be captured in a more fine-grained manner, hence portraying a rich and informative description of the catalytic processes.

**SMILES**. Following CLEAN [77] and CLIPZyme [51], we continue to use SMILES [71] for representing substrates and products. This method is highly useful for its simplicity and ease of interpretation. Such representation concisely shows the substrate-to-product conversion process and uses some linear notation, which is particularly adept at conveying structural changes in a straightforward manner.

**Graph and Conformation**. Graph representation for substrates and products can capture the structural and functional information that is not typically included in string representations [33; 40; 74]. In these graphs, atoms are represented as nodes, while bonds are viewed as edges. Formally, consider a molecular graph denoted as \(\mathcal{G}=(\mathcal{V},\mathcal{E})\), \(\mathcal{V}\in\mathbb{R}^{N\times d_{v}}\) represents atom (node) features with each \(\bm{v}_{i}\in\mathcal{V}\) denotes one-hot encoded atom type, and \(\mathcal{E}\in\mathbb{R}^{N\times N\times d_{e}}\) represents edge (bond) features with each \(\bm{e}_{ij}\in\mathcal{E}\) denotes one-hot encoded bond type and connectivity. In addition to the graph representations for reactions, we use molecular conformations to incorporate geometric information. Formally, consider a molecular conformation denoted as \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{X})\), \(\mathcal{X}\in\mathbb{R}^{N\times 3}\) denotes additional geometric features, specifically atom positions. These conformations are computed through molecular force field optimization [62].

Once obtaining the graph representations \(\mathcal{G}_{s}=(\mathcal{V}_{s},\mathcal{E}_{s},\mathcal{X}_{s}),\mathcal{G} _{p}=(\mathcal{V}_{p},\mathcal{E}_{p},\mathcal{X}_{p})\) for substrates and products, respectively, we proceed to compute reaction embeddings. Consider a graph neural network denoted as \(\phi\), we first use it to separately encode the graph representations as

\[\hat{\mathcal{V}}_{s},\hat{\mathcal{E}}_{s} =\phi(\mathcal{V}_{s},\mathcal{E}_{s},\mathcal{X}_{s}),\;\hat{ \mathcal{V}}_{s}\in\mathbb{R}^{N_{s}\times d_{v}^{T}},\hat{\mathcal{E}}_{s} \in\mathbb{R}^{N_{s}\times N_{s}\times d_{v}^{T}},\] (1) \[\hat{\mathcal{V}}_{p},\hat{\mathcal{E}}_{p} =\phi(\mathcal{V}_{p},\mathcal{E}_{p},\mathcal{X}_{p}),\;\hat{ \mathcal{V}}_{p}\in\mathbb{R}^{N_{p}\times d_{v}^{T}},\hat{\mathcal{E}}_{p} \in\mathbb{R}^{N_{p}\times N_{p}\times d_{v}^{T}},\] (2)

where \(\hat{\mathcal{V}},\hat{\mathcal{E}}\) denotes the updated node and edge representations, respectively. It then becomes challenging to formulate 'transitions' between substrates and products. One method to address this challenge is by constructing a pseudo-transition state graph denoted \(\mathcal{G}_{t}=(\mathcal{V}_{t},\mathcal{E}_{t})\), by adding the bond features for edges connecting the same pairs of nodes in the reactants and the products. Then the graph neural network \(\phi\) can be used to update the transition graphs, and final reaction embedding can be computed by taking the aggregated node features, as \(\bm{r}=\texttt{Aggregate}(\hat{\mathcal{V}}_{t})\in\mathbb{R}^{d_{r}}\). The concept of creating a pseudo-transition state graph is adopted in CLIPZyme [51].

However, we take a more direct approach by computing cross-attention between substrates and products to formulate the 'transitions', as follows:

\[\hat{\mathcal{V}}_{s}=\texttt{softmax}\left(\frac{(\hat{\mathcal{V}}_{s},W_{ \mathcal{G}}^{2}(\hat{\mathcal{V}}_{p},W_{\mathcal{E}}^{T})^{T}}{\sqrt{d_{r}} }\right)(\hat{\mathcal{V}}_{p}W_{\mathcal{E}})\in\mathbb{R}^{N_{s}\times d _{v}},\;\hat{\mathcal{V}}_{p}=\texttt{softmax}\left(\frac{(\hat{\mathcal{V}}_{p }W_{\mathcal{G}}^{2}(\hat{\mathcal{V}}_{p},W_{\mathcal{E}}^{T})^{T}}{\sqrt{d_{ r}}}\right)(\hat{\mathcal{V}}_{s}W_{\mathcal{V}}^{p})\in\mathbb{R}^{N_{p} \times d_{r}}.\] (3)

In here, the 'transitions' are learned through an attention mechanism that considers the pairwise relationships between atoms in substrates and atoms in products, and the edge features \(\hat{\mathcal{E}}_{s},\hat{\mathcal{E}}_{p}\) can be additionally used as attention biases in transformers [69]. And the final reaction embedding is computed by taking the average of node features, as \(\bm{r}=\texttt{Mean}([\hat{\mathcal{V}}_{s},\hat{\mathcal{V}}_{p}])\in \mathbb{R}^{d_{r}}\). In practice, for the choice of graph neural networks to process the structural information of substrate and product graphs \(\mathcal{G}=(\mathcal{V},\mathcal{E})\), we choose to use Molecule Attention Transformer-2D (MAT-2D) [49] and UniMol-2D [83]; and with additional geometric features \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{X})\), we choose to use MAT-3D and UniMol-3D.

### Enzyme Representation

When representing enzymes involved in catalytic reactions, we draw upon advancements in both protein structures and protein language models. This approach shares similarities with CLIPZyme [51], where we utilize a equivariant graph neural network to leverage information of protein structures. However, we are different in the additional use of a structure-based protein language model, where the protein embeddings are computed based on structure-aware sequence tokens.

**Protein Language Model Initialization**. Each protein is represented as a residue-level point cloud in Euclidean space, denoted as \(\mathcal{G}_{e}=(\mathcal{V}_{e},\mathcal{X}_{e},\mathcal{S}_{e})\), where \(\mathcal{S}_{e}\) represents the protein sequence and \(\mathcal{V}_{e}\in\mathbb{R}^{N_{e}\times d_{e}}\) represents residue features. Each residue \(\bm{v}_{i}\in\mathcal{V}_{e}\) can be initialized either with a one-hot encoded residue type or using embeddings from a protein language model (PLM). The protein structure is denoted as \(\mathcal{X}_{e}\in\mathbb{R}^{N_{e}\times 3}\), which can be initialized using AlphaFold [32] or by searching against the AlphaFold database [68]. In practice, we use two protein language models, one using vanilla residue sequences and another using structure-aware residue sequences. The first PLM is the ESM model [41], which results in node features for each protein as \(\mathcal{V}_{e}^{\texttt{EM}}\in\mathbb{R}^{N_{e}\times 1280}\). To enhance our understanding of protein behavior, we employ a second structure-based protein language model called SaProt[61], which differs from ESM by taking structure-aware sequence tokens rather than vanilla sequence tokens. It is achieved this by first aligning the protein structures using FoldSeek[66]. The updated protein sequence after FoldSeek alignment is denoted as \(\hat{\mathcal{S}}_{e}\), representing the structure-aware protein sequence. And SaProt computes structure-aware residue features, resulting in node features for each protein as \(\mathcal{V}_{e}^{\texttt{SAP}}\in\mathbb{R}^{N_{e}\times 1280}\).

The final protein embedding is computed by taking the average of node features as, \(\bm{e}_{\texttt{ESM}}=\texttt{Mean}(\mathcal{V}_{e}^{\texttt{ESM}})\in \mathbb{R}^{1280}\) and \(\bm{e}_{\texttt{SAP}}=\texttt{Mean}(\mathcal{V}_{e}^{\texttt{SAP}})\in \mathbb{R}^{1280}\).

**GNN Encoding**. In addition to these embeddings, we utilize an equivariant graph neural network to encode the protein graphs \(\mathcal{G}_{e}^{\texttt{ESM}}=(\mathcal{V}_{e}^{\texttt{ESM}},\mathcal{X}_{e },\mathcal{S}_{e})\) and \(\mathcal{G}_{e}^{\texttt{SAP}}=(\mathcal{V}_{e}^{\texttt{SAP}},\mathcal{X}_{e },\mathcal{S}_{e})\). We employ the Frame Averaging Neural Network (FANN), denoted as \(\psi\), to learn SE(3)-invariant node features [55]. This approach possesses the effectiveness and efficiency advantage when dealing with large graphs. The frame averaging operation is achieved by first projecting the protein structure \(\hat{\mathcal{X}}_{e}\) onto a set of eight frames \(\mathcal{U}_{e}\in\mathcal{F}(\mathcal{X}_{e})\). These frames are constructed using Principal Component Analysis (PCA). Suppose \(\bm{u}_{1},\bm{u}_{2},\bm{u}_{3}\) denote the three principal components of a covariance matrix \(\Sigma_{e}=(\mathcal{X}_{e}-\mu_{e})^{T}(\hat{\mathcal{X}}_{e}-\mu_{e})\), where \(\mu_{e}\) denotes the Center-of-Mass of \(\mathcal{X}_{e}\). The frame set \(\mathcal{F}(\mathcal{X}_{e})\) is defined as \(\mathcal{F}(\mathcal{X}_{e})=\{\pm\bm{u}_{1},\pm\bm{u}_{2},\pm\bm{u}_{3}\}\). Then the frame averaging operation computes SE(3)-invariant node features \(\hat{\mathcal{V}}_{e}\), as follows:

\[\hat{\mathcal{V}}_{e}=\frac{1}{|\mathcal{F}(\mathcal{X}_{e})|}\sum_{\mathcal{ U}_{e}\in\mathcal{F}(\mathcal{X}_{e})}\psi(\mathcal{V}_{e},(\mathcal{X}_{e}-\mu_{e}) \mathcal{U}_{e})\in\mathbb{R}^{N_{e}\times 1280}.\] (4)

And the final GNN-encoded protein embedding is computed by taking the average of node features as, \(\bm{e}_{\texttt{ESM}}^{\texttt{ES}}=\texttt{Mean}(\hat{\mathcal{V}}_{e}^{ \texttt{ESM}})\in\mathbb{R}^{1280}\) and \(\bm{e}_{\texttt{SAP}}^{\texttt{ES3}}=\texttt{Mean}(\hat{\mathcal{V}}_{e}^{ \texttt{SAP}})\in\mathbb{R}^{1280}\).

### Enzyme-Reaction Prediction

Once we have the reaction and enzyme embeddings \(\bm{r},\bm{e}\), designing models to learn the interactions between enzymes and reactions becomes quite flexible. While approaches like Transformer and attention mechanisms can be used to learn pairwise relationships from positive and negative enzyme-reaction pairs [69; 49], or Bidirectional Recurrent Neural Network (Bi-RNN) can capture enzyme-reaction interactions sequentially [76; 22], we take a more direct approach by employing an MLP network. Consider the input reaction embedding of dimension \(d_{r}\), the reaction encoder is a 4-layer Multi-Layer Perceptron (MLP) as:

\[\small\bm{z}_{r}=\texttt{ReactionEnc}(\bm{r})=W_{4}(\texttt{SiLU}_{3}(\texttt {LN}_{3}(W_{3}(\texttt{SiLU}_{2}(\texttt{LN}_{2}(W_{2}(\texttt{SiLU}_{1}( \texttt{LN}_{1}(W_{1}\bm{r}+B_{1}))))+B_{2})))+B_{3})))+B_{4}\in\mathbb{R}^{256},\] (5)

where \(W_{1}\in\mathbb{R}^{d_{r}\times 512},B_{1}\in\mathbb{R}^{512},W_{2}\in\mathbb{R}^{512 \times 256},B_{2}\in\mathbb{R}^{256},W_{3},W_{4}\in\mathbb{R}^{256\times 256},B_{3},B_{4} \in\mathbb{R}^{256}\). The enzyme encoder, denoted as EnzymeEnc, has a similar architecture, with only modification in the first-layer MLP as \(W_{1}\in\mathbb{R}^{1280\times 512},B_{1}\in\mathbb{R}^{512}\). And the encoded reaction and enzyme representations have the dimension of \(256\), as \(\bm{z}_{r},\bm{z}_{e}\in\mathbb{R}^{256}\).

The decoder network is a 4-layer MLP that takes the encoded enzyme-reaction pair and computes the prediction score:

\[\small\bm{y}=\texttt{Decoder}(\bm{z}_{r},\bm{z}_{e})=W_{4}(W_{3}(\texttt{SiLU }(W_{2}(\texttt{SiLU}(W_{1}([\bm{z}_{r},\bm{z}_{e}])+B_{1}))+B_{2}))+B_{3})) \in\mathbb{R},\] (6)

where \(W_{1}\in\mathbb{R}^{512\times 256},B_{1}\in\mathbb{R}^{256},W_{2}\in\mathbb{R}^{256 \times 128},B_{2}\in\mathbb{R}^{128},W_{3}\in\mathbb{R}^{128\times 64},B_{3}\in \mathbb{R}^{64},W_{4}\in\mathbb{R}^{64\times 1}\). In Appendix C, we further compare the simple MLP-decoder network with Transformer- and Bi-RNN-decoder networks (in Tables 9, 10, and 11), showing their retrieval performance.

Benchmarking on ReactZyme Dataset

### Primary Empirical Evaluation

**Baseline Overview**. We summarize the baseline models used for the enzyme-reaction retrieval task. For reaction representation, we employ Molecule Attention Transformer-2D (MAT-2D) [49], and UniMol-2D [83] for 2D molecular graphs, as well as MAT-3D and UniMol-3D for 3D molecular conformations. For enzyme representation, we employ ESM [41] and a structure-aware protein language model, SaProt[61]. Additionally, we use an equivariant graph neural network (FANN[55]) to enhance residue-level representations.

**Metrics**. In the evaluation of the enzyme-reaction retrieval task, we use several metrics: Top-k Accuracy, Top-k Accuracy-N, Mean Rank, and Mean Reciprocal Rank (MRR). (1) Top-k Accuracy quantifies the proportion of instances where the correct enzyme (or reaction) is ranked within the model's top-k predictions, irrespective of its exact position. (2) Top-k Accuracy-N refines this by assessing the frequency at which the correct enzyme (or reaction) is not only within the top-k predictions but also occupies the precise rank specified by N within this subset. For instance, with k=1, the correct enzyme must be the model's foremost prediction. (3) Mean Rank calculates the average position of the correct enzyme in the retrieval list, with lower values indicating better performance. (4) MRR evaluates how quickly the correct enzyme is retrieved by averaging the reciprocal ranks of the first correct enzyme across all reactions, ranging from \(0\) to \(1\), with higher values indicating better performance. More details and implementations can be found in Appendix A.

**Results**. We present the average results of baseline models for time-based, enzyme similarity-based, and reaction similarity-based splits in Tables 2, 3, and 4, respectively. The top-performing results are highlighted in green, orange, and purple for each split type. In Table 2(a), ranking reactions for each enzyme, the vanilla ESM with 2D molecular graphs (MAT-2D + ESM) achieves \(32.46\%\) top-1 accuracy, \(40.47\) mean rank and \(0.455\) MRR. These results improve with molecular conformations and enzyme structure augmentation (UniMol-3D + ESM + GNN Encoding). For enzyme ranking per reaction (Table 2(b)), MAT-2D + ESM, MAT-2D + ESM) achieves \(21.75\%\) top-1 accuracy, \(165.31\) mean rank, and \(0.179\) MRR, with slight improvements using molecular conformations (MAT-3D + ESM). Similar improvements are seen in the enzyme similarity-based split. In Table 3(a), MAT-2D + SaProt achieves \(66.91\%\) top-1 accuracy, \(5.44\) mean rank and \(0.773\) MRR, which further improves with molecular conformations (UniMol-3D + ESM). In Table 3(b), MAT-2D + SaProt achieves \(39.99\%\) top-1 accuracy, \(23.59\) mean rank, and \(0.288\) MRR. With molecular conformations (UniMol-3D + ESM), accuracy and MRR improve slightly, though the mean rank drops. Reaction similarity-based splits pose significant challenges, especially for unseen reactions. In Table 4(a), MAT-2D + ESM achieves \(9.41\%\) top-1 accuracy, \(39.91\) mean rank and \(0.200\) MRR. Adding molecular conformations and enzyme structure augmentation (UniMol-3D + ESM + GNN Encoding) yields minimal improvement. Conversely, in Table 4(b), MAT-2D + ESM alone is sufficient.

\begin{table}

\end{table}
Table 2: Average results of baseline models of _time-based split_. Top results are highlighted in green, orange, and purple, respectively.

**Summary**. It is evident that the tasks associated with the time-based and enzyme similarity-based splits are less challenging than the reaction similarity-based split. This is reflected by higher top-k accuracy, improved mean rank, and a greater Mean Reciprocal Rank (MRR), indicating increased confidence. The likely reason is that the training set for the time-based and enzyme similarity-based splits includes all reactions, whereas the test set for the reaction similarity-based split contains numerous unseen reactions. This makes the task significantly more demanding, yet it provides an excellent opportunity to evaluate the generalization capabilities of prediction models. Deep learning models employing 2D and 3D graph representations for reactions and enzymes prove effective in learning enzyme-reaction interactions, which are crucial for accurate enzyme-reaction prediction. Vanilla models such as ESM, when reactions augmented with MAT-2D and UniMol-2D, have shown promising results. These outcomes can be further enhanced by incorporating molecular conformation data (MAT-3D and UniMol-3D). Additionally, the use of an equivariant model (GNN Encoding) to represent enzyme structures has led to further improvements in prediction accuracy. This suggests that structural information plays a significant role in enzyme-reaction prediction tasks, a finding that was not observed in previous EC classification tasks. These methods prioritize enzyme functionality over mere gene family classification or human-assigned reaction categories.

\begin{table}

\end{table}
Table 4: Average results of baseline models of _reaction-similarity-based split_. Top results are highlighted in green, orange, and purple, respectively.

\begin{table}

\end{table}
Table 3: Average results of baseline models of _enzyme-similarity-based split_. Top results are highlighted in green, orange, and purple, respectively.

### Classic Annotation Method - BLAST

**Method**. To predict the reaction of an enzyme using BLAST, we employ BLASTp with default parameters. The training set sequences are used as the target database, while the test set sequences serve as the query. We use the following commands:

\begin{tabular}{l} Bash Command \(\rightarrow\) bash makeblastdb -in train.fasta -dbtype prot parse_seqids -out train_db blastp -query test.fasta -db train_db -outfmt "6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitstcore" -out results.tsv \\ \end{tabular}

If BLASTp finds a match between the test set and training set sequences, we set the corresponding value to \(1\), indicating that the sequences likely share the same reaction. If there is no match found, the value is set to \(0\), indicating no predicted reaction match.

For reaction-based sequence searches, where the reaction is known in the training set, we use the training set sequences as the query to search against the test set, applying the same criteria for setting the values.

**Results**. We compare the average neural network and BLAST results for time-, enzyme similarity-, and reaction similarity-based splits in Tables 5, 6, and 7, respectively. We highlight best performing models and use different colors distinguish between Top-k Accuracy, Mean Rank, and MRR.

**Analysis**. In the time-based split, we observe that the performance of Neural Networks and BLAST are quite similar in terms of Top-k Accuracy, Mean Rank, and MRR. The comparable performance of BLAST may be attributed to the presence of some enzyme and reaction sequences in the training

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c c} \hline \hline \multicolumn{1}{c}{**Data/Communications/Programming/NNN**} & \multicolumn{1}{c}{**Model**} & \multicolumn{set that reappear in the test set, or to similar enzyme and reaction clusters. However, in the enzyme-similarity-based split, BLAST falls significantly short of the results achieved by Neural Networks. This disparity arises because many test enzyme sequences are either unseen or substantially different from those in the training set.

In the reaction-similarity-based split, BLAST exhibits nearly \(0\%\) top-k accuracy, along with extremely high mean ranks and low MRRs. This outcome suggests that BLAST's predictions are almost random guesses, indicating that the model does not effectively leverage the enzyme-reaction pairs from the training data. In contrast, Neural Networks still excel in identifying the underlying patterns necessary for accurate enzyme and reaction retrieval. Overall, Neural Networks outperform the classical BLAST annotation method, highlighting their potential to advance enzyme-reaction prediction tasks.

### Potential Strategy

In Table 8, we report the accuracy and AUROC of prediction models on positive and negative samples for enzyme-reaction prediction. While these metrics are secondary to the retrieval results discussed in Section 5.1, a strong correlation is evident between the retrieval performance and the ROC scores. Notably, the ROC scores for the reaction similarity-based split are lower compared to those for the time- and enzyme similarity-based splits. This pattern is similar in the retrieval results, underscoring the heightened difficulty of the reaction similarity-based task.

### Further Evaluation

We present further experiments in the Appendices for deeper evaluation and comparison. In Appendix C, we compare MLP, Transformer, and Bi-RNN decoder networks. Given the presence of annotated negative samples, we explore a contrastive learning approach in Appendix D. We also compare to the CLIPZyme pseudo-graph approach in Appendix E. And for a better description of chemical environment of reactants and product, we compare with fingerprint features in Appendix F.

## 6 Conclusion

In this paper, we introduce ReactZyme, a new benchmark for enzyme-reaction prediction. Unlike previous methods that rely on protein sequence or structure similarity or provide EC/GO annotations to predict reaction, our approach directly evaluates the mapping between enzymes and their catalyzed reactions. These enzyme-reaction prediction methods are able to handle protein with novel reactions and to discover proteins that catalyze unreported reactions. We evaluate the performance of several baselines on the ReactZyme. While the baselines demonstrate competitive results on time- and enzyme-similarity-based splits, the reaction-similarity-based split remains particularly challenging. This difficulty may arise from the presence of many unseen reactions in the test set of the reaction-similarity-based split. One potential avenue for improvement is to explore contrastive learning techniques to address this challenge. However, we acknowledge that this remains an open problem for researchers in our community to tackle.

The ReactZyme benchmark facilitates the evaluation of models working with protein and molecule representations, which requires a comprehensive understanding in both modalities. Models demonstrating high performance in enzyme-reaction prediction can be further leveraged for protein function prediction and enzyme discovery. This includes identifying key enzymes in biosynthesis and discovering potent enzymes for degrading emerging pollutants, for these reactions that have not been previously found in enzymes.

\begin{table}
\begin{tabular}{l|c|c c c c c} \hline \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{**Acc \& ROC**} & \multicolumn{1}{c|}{**Time**} & \multicolumn{1}{c|}{**Enzyme**} & \multicolumn{1}{c}{**Reaction**} \\ \hline
**Model** & **GENN Encoding** & Acc & ROC & Acc & ROC & Acc & ROC \\ \hline \multirow{3}{*}{**PMT-20 + EBM**} & \(\mathcal{X}\) & 0.9904 & 0.8635 & 0.9897 & 0.8793 & 0.9715 & 0.5914 \\  & 0.9747 + Sartori & \(\mathcal{X}\) & 0.9734 & 0.8327 & 0.9880 & 0.8533 & 0.9719 & 0.5780 \\  & 0.9837 & 0.8937 & 0.8958 & 0.9837 & 0.8727 & 0.9683 & 0.8599 \\  & 0.9636 & 0.8268 & 0.9784 & 0.8498 & 0.9727 & 0.6019 \\  & 0.9708 & 0.8460 & 0.9846 & 0.8787 & 0.9723 & 0.5691 \\  & 0.9765 & 0.8546 & 0.9850 & 0.8617 & 0.9751 & 0.5823 \\ \hline \multirow{3}{*}{**PMT-30 + EBM**} & \(\mathcal{X}\) & 0.9871 & 0.8630 & 0.9836 & 0.8617 & 0.9743 & 0.6041 \\  & 0.9864 & 0.8271 & 0.9707 & 0.8520 & 0.9718 & 0.5884 \\  & 0.9802 & 0.8552 & 0.9991 & 0.8800 & 0.9729 & 0.6091 \\  & 0.9751 & 0.8490 & 0.9737 & 0.8538 & 0.9732 & 0.5907 \\  & 0.9903 & 0.8747 & 0.9879 & 0.8010 & 0.9821 & 0.6285 \\  & 0.9843 & 0.8558 & 0.9828 & 0.8622 & 0.9786 & 0.5970 \\ \hline \end{tabular}
\end{table}
Table 8: Average accuracy and AUROC of baseline models for enzyme-reaction prediction. Top results are highlighted in green, orange, and purple, respectively.

## Acknowledgement

Chenqing Hua thanks to the FACS-Acuity Project of Canada (No. 10242), Shuangjia Zheng thanks to the National Natural Science Foundation of China (No. 62402314) and Aureka Bio. We extend our gratitude to Zuobai Zhang for his valuable discussions and insights, although he could not be included as a co-author due to some extreme factors. We also thank Connor Coley for raising concerns related to the data source at the early stage, which led to improvements in the dataset introduction.

## References

* [1] J. Abramson, J. Adler, J. Dunger, R. Evans, T. Green, A. Pritzel, O. Ronneberger, L. Willmore, A. J. Ballard, J. Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. _Nature_, pages 1-3, 2024.
* [2] S. F. Altschul, W. Gish, W. Miller, E. W. Myers, and D. J. Lipman. Basic local alignment search tool. _Journal of molecular biology_, 215(3):403-410, 1990.
* [3] S. F. Altschul, T. L. Madden, A. A. Schaffer, J. Zhang, Z. Zhang, W. Miller, and D. J. Lipman. Gapped blast and psi-blast: a new generation of protein database search programs. _Nucleic acids research_, 25(17):3389-3402, 1997.
* [4] A. Bairoch. The enzyme database in 2000. _Nucleic acids research_, 28(1):304-305, 2000.
* [5] P. Bansal, A. Morgat, K. B. Axelsen, V. Muthukrishnan, E. Coudert, L. Aimo, N. Hyka-Nouspikel, E. Gasteiger, A. Kerhorun, T. B. Neto, et al. Rhea, the reaction knowledgebase in 2022. _Nucleic acids research_, 50(D1):D693-D700, 2022.
* [6] B. Berger, M. S. Waterman, and Y. W. Yu. Levenshtein distance, sequence comparison and biological database search. _IEEE transactions on information theory_, 67(6):3287-3294, 2020.
* [7] B. Boeckmann, A. Bairoch, R. Apweiler, M.-C. Blatter, A. Estreicher, E. Gasteiger, M. J. Martin, K. Michoud, C. O'Donovan, I. Phan, et al. The swiss-prot protein knowledgebase and its supplement trembl in 2003. _Nucleic acids research_, 31(1):365-370, 2003.
* [8] R. Bonetta and G. Valentino. Machine learning techniques for protein function prediction. _Proteins: Structure, Function, and Bioinformatics_, 88(3):397-413, 2020.
* [9] P. Bryant, A. Kelkar, A. Guljas, C. Clementi, and F. Noe. Structure prediction of protein-ligand complexes from sequence information with umol. _Nature Communications_, 15(1):4536, 2024.
* [10] A. Bushuiev, R. Bushuiev, A. Filkin, P. Kouba, M. Gabrielova, M. Gabriel, J. Sedlar, T. Pluskal, J. Damborsky, S. Mazurenko, et al. Learning to design protein-protein interactions with enhanced generalization. _arXiv preprint arXiv:2310.18515_, 2023.
* [11] E. Campbell, M. Kaltenbach, G. J. Correy, P. D. Carr, B. T. Porebski, E. K. Livingstone, L. Afriat-Jurnou, A. M. Buckle, M. Weik, F. Hollfelder, et al. The role of protein dynamics in the evolution of new enzyme function. _Nature chemical biology_, 12(11):944-950, 2016.
* [12] G. O. Consortium. The gene ontology (go) database and informatics resource. _Nucleic acids research_, 32(suppl_1):D258-D261, 2004.
* [13] R. A. Copeland. _Enzymes: a practical introduction to structure, mechanism, and data analysis_. John Wiley & Sons, 2023.
* [14] G. Corso, H. Stark, B. Jing, R. Barzilay, and T. Jaakkola. Diffdock: Diffusion steps, twists, and turns for molecular docking. _arXiv preprint arXiv:2210.01776_, 2022.
* [15] D. Devos and A. Valencia. Practical limits of function prediction. _Proteins: Structure, Function, and Bioinformatics_, 41(1):98-107, 2000.
* [16] J.-L. Ferrer, M. Austin, C. Stewart Jr, and J. Noel. Structure and function of enzymes involved in the biosynthesis of phenylpropanoids. _Plant Physiology and Biochemistry_, 46(3):356-370, 2008.
* [17] R. A. Friesner, J. L. Banks, R. B. Murphy, T. A. Halgren, J. J. Klicic, D. T. Mainz, M. P. Repasky, E. H. Knoll, M. Shelley, J. K. Perry, et al. Glide: a new approach for rapid, accurate docking and scoring. 1. method and assessment of docking accuracy. _Journal of medicinal chemistry_, 47(7):1739-1749, 2004.

* [18] J. Gao, S. Ma, D. T. Major, K. Nam, J. Pu, and D. G. Truhlar. Mechanisms and free energies of enzymatic reactions. _Chemical reviews_, 106(8):3188-3209, 2006.
* [19] H. M. Girvan and A. W. Munro. Applications of microbial cytochrome p450 enzymes in biotechnology and synthetic biology. _Current opinion in chemical biology_, 31:136-145, 2016.
* [20] M. E. Glasner, J. A. Gerlt, and P. C. Babbitt. Evolution of enzyme superfamilies. _Current opinion in chemical biology_, 10(5):492-497, 2006.
* [21] V. Gligorijevic, P. D. Renfrew, T. Kosciolek, J. K. Leman, D. Berenberg, T. Vatanen, C. Chandler, B. C. Taylor, I. M. Fisk, H. Vlamakis, et al. Structure-based protein function prediction using graph convolutional networks. _Nature communications_, 12(1):3168, 2021.
* [22] E. Hajiramezanali, A. Hasanzadeh, K. Narayanan, N. Duffield, M. Zhou, and X. Qian. Variational graph recurrent neural networks. _Advances in neural information processing systems_, 32, 2019.
* [23] E. Heid, D. Probst, W. H. Green, and G. K. Madsen. Enzymemap: curation, validation and data-driven prediction of enzymatic reactions. _Chemical Science_, 14(48):14229-14242, 2023.
* [24] C. E. Hodgman and M. C. Jewett. Cell-free synthetic biology: thinking outside the cell. _Metabolic engineering_, 14(3):261-269, 2012.
* [25] C. Hua, C. Coley, G. Wolf, D. Precup, and S. Zheng. Effective protein-protein interaction exploration with pipretrieval. _arXiv preprint arXiv:2402.03675_, 2024.
* [26] C. Hua, S. Luan, J. Fu, and D. Precup. Multi-dataset multi-task framework for learning molecules and protein-target interactions properties. 2022.
* [27] C. Hua, S. Luan, M. Xu, R. Ying, J. Fu, S. Ermon, and D. Precup. Mudiff: Unified diffusion for complete molecule generation. _arXiv preprint arXiv:2304.14621_, 2023.
* [28] C. Hua, G. Rabusseau, and J. Tang. High-order pooling for graph neural networks with tensor decomposition. _Advances in Neural Information Processing Systems_, 35:6021-6033, 2022.
* [29] J. Huerta-Cepas, D. Szklarczyk, D. Heller, A. Hernandez-Plaza, S. K. Forslund, H. Cook, D. R. Mende, I. Letunic, T. Rattei, L. J. Jensen, et al. eggnog 5.0: a hierarchical, functionally and phylogenetically annotated orthology resource based on 5090 organisms and 2502 viruses. _Nucleic acids research_, 47(D1):D309-D314, 2019.
* [30] C. Isert, K. Atz, and G. Schneider. Structure-based drug design with geometric deep learning. _Current Opinion in Structural Biology_, 79:102548, 2023.
* [31] R. A. Jensen. Enzyme recruitment in evolution of new function. _Annual review of microbiology_, 30(1):409-425, 1976.
* [32] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Zidek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* [33] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley. Molecular graph convolutions: moving beyond fingerprints. _Journal of computer-aided molecular design_, 30:595-608, 2016.
* [34] J. D. Keasling. Manufacturing molecules through metabolic engineering. _Science_, 330(6009):1355-1358, 2010.
* [35] J. Kraut. How do enzymes work? _Science_, 242(4878):533-540, 1988.
* [36] R. Krishna, J. Wang, W. Ahern, P. Sturmfels, P. Venkatesh, I. Kalvet, G. R. Lee, F. S. Morey-Burrows, I. Anishchenko, I. R. Humphreys, et al. Generalized biomolecular modeling and design with rosettafold all-atom. _Science_, 384(6693):ead2528, 2024.
* [37] A. Kroll, S. Ranjan, M. K. Engqvist, and M. J. Lercher. A general model to predict small molecule substrates of enzymes based on machine and deep learning. _Nature communications_, 14(1):2787, 2023.
* [38] A. Kroll, Y. Rousset, X.-P. Hu, N. A. Liebrand, and M. J. Lercher. Turnover number predictions for kinetically uncharacterized enzymes using machine and deep learning. _Nature Communications_, 14(1):4139, 2023.
* [39] M. Kulmanov and R. Hoehndorf. Deepgoplus: improved protein function prediction from sequence. _Bioinformatics_, 36(2):422-429, 2020.

* [40] J. Li, D. Cai, and X. He. Learning graph-level representation for drug discovery. _arXiv preprint arXiv:1709.03741_, 2017.
* [41] Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, R. Verkuil, O. Kabeli, Y. Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. _Science_, 379(6637):1123-1130, 2023.
* [42] W. Liu and P. Wang. Cofactor regeneration for sustainable enzymatic biosynthesis. _Biotechnology advances_, 25(4):369-384, 2007.
* [43] W. Lu, Q. Wu, J. Zhang, J. Rao, C. Li, and S. Zheng. Tankbind: Trigonometry-aware neural networks for drug-protein binding structure prediction. _Advances in neural information processing systems_, 35:7236-7249, 2022.
* [44] S. Luan, C. Hua, Q. Lu, L. Ma, L. Wu, X. Wang, M. Xu, X.-W. Chang, D. Precup, R. Ying, et al. The heterophilic graph learning handbook: Benchmarks, models, theoretical analysis, applications and challenges. _arXiv preprint arXiv:2407.09618_, 2024.
* [45] S. Luan, C. Hua, Q. Lu, J. Zhu, M. Zhao, S. Zhang, X.-W. Chang, and D. Precup. Is heterophily a real nightmare for graph neural networks to do node classification? _arXiv preprint arXiv:2109.05641_, 2021.
* [46] S. Luan, C. Hua, Q. Lu, J. Zhu, M. Zhao, S. Zhang, X.-W. Chang, and D. Precup. Revisiting heterophily for graph neural networks. _Advances in neural information processing systems_, 35:1362-1375, 2022.
* [47] S. Luan, M. Zhao, C. Hua, X.-W. Chang, and D. Precup. Complete the missing half: Augmenting aggregation filtering with diversification for graph convolutional networks. _arXiv preprint arXiv:2008.08844_, 2020.
* [48] X. Mao, T. Cai, J. G. Olyarchuk, and L. Wei. Automated genome annotation and pathway identification using the kegg orthology (ko) as a controlled vocabulary. _Bioinformatics_, 21(19):3787-3793, 2005.
* [49] L. Maziarka, T. Danel, S. Mucha, K. Rataj, J. Tabor, and S. Jastrzebski. Molecule attention transformer. _arXiv preprint arXiv:2002.08264_, 2020.
* [50] A. G. McDonald and K. F. Tipton. Fifty-five years of enzyme classification: advances and difficulties. _The FEBS journal_, 281(2):583-592, 2014.
* [51] P. G. Mikhael, I. Chinn, and R. Barzilay. Clipzyme: Reaction-conditioned virtual screening of enzymes. _arXiv preprint arXiv:2402.06748_, 2024.
* [52] Y. Murakami, J.-i. Kikuchi, Y. Hisaeda, and O. Hayashida. Artificial enzymes. _Chemical reviews_, 96(2):721-758, 1996.
* [53] H. Neurath and K. A. Walsh. Role of proteolytic enzymes in biological regulation (a review). _Proceedings of the National Academy of Sciences_, 73(11):3825-3832, 1976.
* [54] G. P. Pinto, M. Corbella, A. O. Demkiv, and S. C. L. Kamerlin. Exploiting enzyme evolution for computational protein design. _Trends in Biochemical Sciences_, 47(5):375-389, 2022.
* [55] O. Puny, M. Atzmon, H. Ben-Hamu, I. Misra, A. Grover, E. J. Smith, and Y. Lipman. Frame averaging for invariant and equivariant network design. _arXiv preprint arXiv:2110.03336_, 2021.
* [56] J. Y. Ryu, H. U. Kim, and S. Y. Lee. Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers. _Proceedings of the National Academy of Sciences_, 116(28):13996-14001, 2019.
* [57] A. Saravanan, P. S. Kumar, D.-V. N. Vo, S. Jeevanantham, S. Karishma, and P. Yaashikaa. A review on catalytic-enzyme degradation of toxic environmental pollutants: Microbial enzymes. _Journal of Hazardous Materials_, 419:126451, 2021.
* [58] V. G. Satorras, E. Hoogeboom, and M. Welling. E (n) equivariant graph neural networks. In _International conference on machine learning_, pages 9323-9332. PMLR, 2021.
* [59] I. Schomburg, A. Chang, O. Hofmann, C. Ebeling, F. Ehrentreich, and D. Schomburg. Brenda: a resource for enzyme data and metabolic information. _Trends in biochemical sciences_, 27(1):54-56, 2002.
* [60] H. Stark, O. Ganea, L. Pattanaik, R. Barzilay, and T. Jaakkola. Equibind: Geometric deep learning for drug binding structure prediction. In _International conference on machine learning_, pages 20503-20521. PMLR, 2022.

* [61] J. Su, C. Han, Y. Zhou, J. Shan, X. Zhou, and F. Yuan. Saprot: protein language modeling with structure-aware vocabulary. _bioRxiv_, pages 2023-10, 2023.
* [62] P. Tosco, N. Stiefl, and G. Landrum. Bringing the mmff force field to the rdkit: implementation and validation. _Journal of cheminformatics_, 6:1-4, 2014.
* [63] O. Trott and A. J. Olson. Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. _Journal of computational chemistry_, 31(2):455-461, 2010.
* [64] J. Tubiana, D. Schneidman-Duhovny, and H. J. Wolfson. Scannet: an interpretable geometric deep learning model for structure-based protein binding site prediction. _Nature Methods_, 19(6):730-739, 2022.
* [65] A. Valencia. Automatic annotation of protein function. _Current opinion in structural biology_, 15(3):267-274, 2005.
* [66] M. van Kempen, S. S. Kim, C. Tumescheit, M. Mirdita, J. Lee, C. L. Gilchrist, J. Soding, and M. Steinegger. Fast and accurate protein structure search with foldseek. _Nature Biotechnology_, pages 1-4, 2023.
* [67] M. Van Kempen, S. S. Kim, C. Tumescheit, M. Mirdita, J. Lee, C. L. Gilchrist, J. Soding, and M. Steinegger. Fast and accurate protein structure search with foldseek. _Nature biotechnology_, 42(2):243-246, 2024.
* [68] M. Varadi, S. Anyango, M. Deshpande, S. Nair, C. Natasgia, G. Yordanova, D. Yuan, O. Stroe, G. Wood, A. Laydon, et al. Alphafold protein structure database: massively expanding the structural coverage of protein-sequence space with high-accuracy models. _Nucleic acids research_, 50(D1):D439-D444, 2022.
* [69] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [70] M. L. Verdonk, J. C. Cole, M. J. Hartshorn, C. W. Murray, and R. D. Taylor. Improved protein-ligand docking using gold. _Proteins: Structure, Function, and Bioinformatics_, 52(4):609-623, 2003.
* [71] D. Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. _Journal of chemical information and computer sciences_, 28(1):31-36, 1988.
* [72] M. Wen, Z. Zhang, S. Niu, H. Sha, R. Yang, Y. Yun, and H. Lu. Deep-learning-based drug-target interaction prediction. _Journal of proteome research_, 16(4):1401-1409, 2017.
* [73] L. Wu, Y. Tian, Y. Huang, S. Li, H. Lin, N. V. Chawla, and S. Z. Li. Mape-ppi: Towards effective and efficient protein-protein interaction prediction via microenvironment-aware protein embedding. _arXiv preprint arXiv:2402.14391_, 2024.
* [74] H.-C. Yi, Z.-H. You, D.-S. Huang, and C. K. Kwoh. Graph representation learning in bioinformatics: trends, methods and applications. _Briefings in Bioinformatics_, 23(1):bbab340, 2022.
* [75] S. Yoshida, K. Hiraga, T. Takehana, I. Taniguchi, H. Yamaji, Y. Maeda, K. Toyohara, K. Miyamoto, Y. Kimura, and K. Oda. A bacterium that degrades and assimilates poly (ethylene terephthalate). _Science_, 351(6278):1196-1199, 2016.
* [76] J. You, R. Ying, X. Ren, W. Hamilton, and J. Leskovec. Graphrnn: Generating realistic graphs with deep auto-regressive models. In _International conference on machine learning_, pages 5708-5717. PMLR, 2018.
* [77] T. Yu, H. Cui, J. C. Li, Y. Luo, G. Jiang, and H. Zhao. Enzyme function prediction using contrastive learning. _Science_, 379(6639):1358-1363, 2023.
* [78] O. Zhang, Y. Huang, S. Cheng, M. Yu, X. Zhang, H. Lin, Y. Zeng, M. Wang, Z. Wu, H. Zhao, et al. Deep geometry handling and fragment-wise molecular 3d graph generation. _arXiv preprint arXiv:2404.00014_, 2024.
* [79] O. Zhang, J. Jin, H. Lin, J. Zhang, C. Hua, Y. Huang, H. Zhao, C.-Y. Hsieh, and T. Hou. Ecloudgen: Access to broader chemical space for structure-based molecule generation. _bioRxiv_, pages 2024-06, 2024.

* [80] X. Zhang, O. Zhang, C. Shen, W. Qu, S. Chen, H. Cao, Y. Kang, Z. Wang, E. Wang, J. Zhang, et al. Efficient and accurate large library ligand docking with karmadock. _Nature Computational Science_, 3(9):789-804, 2023.
* [81] Y. Zhang, H. Cai, C. Shi, B. Zhong, and J. Tang. E3bind: An end-to-end equivariant network for protein-ligand docking. _arXiv preprint arXiv:2210.06069_, 2022.
* [82] Z. Zhang, M. Xu, A. Jamasb, V. Chenthamarakshan, A. Lozano, P. Das, and J. Tang. Protein representation learning by geometric structure pretraining. _arXiv preprint arXiv:2203.06125_, 2022.
* [83] G. Zhou, Z. Gao, Q. Ding, H. Zheng, H. Xu, Z. Wei, L. Zhang, and G. Ke. Uni-mol: A universal 3d molecular representation learning framework. 2023.

Metrics

The code for evaluation follows:

```
1importtorch
2
3defenzyme_reaction_evaluation(logits, labels):
4#logits=(n_enzyme, n_reaction); labels=(n_enzyme, n_reaction)
5
6#computeargsortaccordingtopigitsvalues
7asrt=torch.argsort(logits, dim=1,descending=True,stable=True)
8#ifallzeros,randomlypermute
9if(logits==0).all(dim=-1).sum():
10rand_perm=torch.stack([torch.randperm(logits.size(1))for_ inrange(logits.size(0))])
11indices=torch.where((logits==0).all(dim=-1)==1)[0]
12asrt[indices]=rand_perm[indices]
13
14ranking=torch.empty(logits.shape[0],logits.shape[1],dtype=torch.long).scatter_(1,asrt,torch.arange(logits.shape[1]). repeat(logits.shape[0],1))
15ranking=(ranking+1).to(labels.device)
16
17#computenearank
18mean_rank=(ranking*labels.float()).sum(dim=-1)/(labels.sum( dim=-1)))
19mean_rank=mean_rank.mean(dim=0)
20
21#computemrr
22mrr=(1.0/ranking*labels.float()).sum(dim=-1)/(labels.sum( dim=-1))#(num_seq)
23mrr=mrr.mean(dim=0)
24
25top_accs=[]
26top_accs_n=[]
27forkin[1,2,3,4,5,10,20,50]:
28#computetop-kacc
29top_acc=((ranking<=k)*labels.float()).sum(dim=-1)>0).float()
30top_acc=top_acc.mean(dim=0)
31top_accs.append(top_acc)
32
33#computetop-kacc-n
34top_acc_n=((ranking<=k)*labels.float()).sum(dim=-1)/k
35top_acc_n=top_acc_n.mean(dim=0)
36top_accs_n.append(top_acc_n)
37
38returntop_accs[0],top_accs[1],top_accs[2],top_accs[3],top_accs[4],top_accs[5],top_accs[6],top_accs[7],top_accs_n[0],top_accs_n[1],top_accs_n[2],top_accs_n[3],top_accs_n[4],top_accs_n[5],top_accs_n[6],top_accs_n[7],mean_rank,mrr ```

Listing 1: Pytorch Implementation for Enzyme-Reaction Prediction.

We employ Top-k Accuracy, Top-k Accuracy-N, Mean Rank, and Mean Reciprocal Rank (MRR) to evaluate the enzyme-reaction retrieval task.

Top-k Accuracy measures the percentage of cases where the correct enzyme (or reaction) is included within the top-k predictions made by the model; and it does not necessarily have to be the first prediction, as long as it is within the top-k. For Top-k Accuracy, the formula could be:

\[\texttt{Top-k Accuracy}=\frac{\texttt{Number of correct enzymes in top-k predictions}}{\texttt{Total number of predictions}}\]

Top-k Accuracy-N measures how often the correct enzyme (or reaction) is not just within the top-k predictions, but also at the correct rank within those top-k. For example, if k=1, then the correct enzyme must be the model's top prediction. For Top-k Accuracy-N, the formula might look like:

Top-k Accuracy-N = \(\dfrac{\text{Number of correct enzymes at correct rank in top-k predictions}}{\text{Total number of predictions}}\)

Mean Rank calculates the average position of the correct enzyme in the retrieval list, with lower values indicating better performance.

MRR evaluates how quickly the correct enzyme is retrieved by averaging the reciprocal ranks of the first correct enzyme across all reactions, ranging from \(0\) to \(1\), with higher values indicating better performance.

Appendix B Terminology of enzyme-reaction prediction, Enzyme-function prediction, enzyme-substrate/product prediction, and enzyme annotation

The terms or the concepts of 'enzyme reaction prediction', 'enzyme function prediction', 'enzyme substrate/product prediction', and 'enzyme annotation' may not be clearly delineated in the main section. In here, we aim to explain and address these concerns. There are indeed different types of annotations for enzyme, with function annotation being one of them. A reaction is part of the function, as not all functions map directly to a reaction. An enzyme reaction includes multiple features, such as substrate, product, and conditions (including the catalyst). This distinction helps clarify the various concepts like enzyme reaction prediction, function prediction, and substrate/product prediction.

## Appendix C Experiments on Transformer and Bi-RNN Networks

In Section 4, we choose to use an encoder-decoder network over directly employment of Transformer or Bi-RNN. Here, we explain the intuition behind the use of the encoder-decocoder design over the transformer-like architectures. The encoder network, at the low-hierarchical level, aims to learn individual representations for enzymes and reactions, respectively. And the decoder network, at the high-hierarchical level, aims to learn the contacts or the interactions between any enzyme-reaction pair. Thus, in principle, the decoder could be any network that learns the interactions between enzymes and reactions.

**Results**. In Section 4, we choose to use a MLP as the decoder network, here, we employ the Transformer and Bi-RNN as the decoder network for further evaluation. We compare the average results of baseline models by MLP, Transformer, Bi-RNN for time-based, enzyme similarity-based, and reaction similarity-based splits in Tables 9, 10, and 11, respectively.

**Analysis.** We observe significant performance improvements when using Transformer and Bi-RNN as the decoder networks. Specifically, Bi-RNN demonstrates superior performance on both time- and enzyme-similarity-based splits, while Transformer also shows better and stronger performance compared to the MLP decoder on these two splits. However, neither Transformer nor Bi-RNN provide substantial improvements on the reaction similarity-based split, with any gains being incremental at best. This suggests that, despite the significant advancements on the other two splits, the reaction-based split remains extremely challenging and requires considerable effort to address. Given that Transformer and Bi-RNN are designed to handle sequential and tokenized data, they are inherently more powerful than MLP for this enzyme-substrate/product prediction task. A promising direction for

\begin{table}

\end{table}
Table 9: Comparisons between MLP, Transformer, Bi-RNN on _time-based split_. (a) Given the enzyme, the list of candidate reactions is evaluated (**enzymes, #Fractions**).

future work would be to design enzyme-reaction-specific Transformer or Bi-RNN models tailored for this retrieval task.

## Appendix D Experiments on Contrastive Learning

**Results**. In this section, we compare the average results of baseline models and the contrastive learning approach for time-based, enzyme similarity-based, and reaction similarity-based splits in Tables 12, 13, and 14, respectively. For enzyme-reaction prediction, contrastive learning can be used to learn embeddings or representations of enzymes and reactions that are predictive of their interactions. Positive pairs are optimized to have similar representations, while the negative pairs are optimized to be distinct in the embedding space.

**Summary**. For contrastive learning approach, an additional contrastive optimization goal is used to make positive pairs similar and negative pairs distinct. However, we do not observe significant improvements in performance using contrastive learning on our dataset. This suggests that while contrastive learning can be a powerful tool, its impact on our specific task and dataset may be limited, possibly due to the characteristics of our synthesized dataset or the dense method employed.

\begin{table}

\end{table}
Table 10: Comparisons between MLP, Transformer, Bi-RNN on _enzyme-similarity-based split_.

\begin{table}

\end{table}
Table 11: Comparisons between MLP, Transformer, Bi-RNN on _reaction-similarity-based split_.

[MISSING_PAGE_FAIL:19]

**Analysis**. The pseudo-graph approach may capture some hidden atomic-level transition pattern from molecular substrates to molecular products. The approach captures the atom and bond similarities and differences, learning more of the hidden patterns in catalytic reactions, therefore resulting in a performance increase on reaction-similarity-based split. However, such hidden pattern may not be important or significant when more reaction information are provided to us, thus no performance increase or incremental change on time-based and enzyme-similarity-based splits.

## Appendix F Experiments on Fingerprint Features

In addition to the use of one-hot encoded atomic and bond features, we study the encodings of using fingerprints generated by RDKit to describe the chemical environments of reactants and products.

**Results**. We compare the average results of baseline models and the fingerprint features for time-based, enzyme similarity-based, and reaction similarity-based splits in Tables 18, 19, and 20, respectively.

**Analysis**. We observe there is no significant performance increase when using fingerprint features to describe the chemical environments on time- and enzyme-similarity-based splits. And there is a slight improvement on reaction-similarity-based split. The experimental pattern is similar to the observation in using pseudo-graphs for transition states. Using fingerprint features may be useful when the reaction features play a more dense role in the prediction task; it helps capture some hidden atomic-level information than the one-hot graph encoded features.

\begin{table}

\end{table}
Table 17: Comparisons between baselines and CLIPZyme on _reaction-similarity-based split_.
Appendix G No Significant Improvement with Molecular Conformations: An Intuitive Explanation from \(3Di\) Perspective

In our paper, we compared models like ESM (without structure) and SaProt (with structure), as well as models with 2D or 3D molecular conformation information. The results showed inconsistent performance when structural features were included in different tasks. We believe that this might be because the fact that SaProt encodes only \(3Di\) information, which lacks the detailed structural features necessary to accurately model enzyme functional sites. For molecules, due to their smaller sizes, the difference between 2D and 3D information might be minimal. This could explain the limited performance gains observed in experiments.

Furthermore, it is important to consider the scale of the ReactZyme dataset in comparison to previous studies. The dataset comprises more than 100,000 enzyme-substrate pairs, which is an order of magnitude larger than the typical datasets used in similar studies (around 10,000 pairs). The increased size and diversity of our dataset may dilute the impact of molecular conformation information on the overall performance. While the incorporation of this information has resulted in only a modest improvement, it remains a valuable aspect of our work.

Moreover, we recognize this as a current limitation and believe that there is potential for further optimization in the utilization of molecular conformations and structural data. Future work could explore more sophisticated methods to leverage this information, potentially leading to more substantial performance gains in enzyme-reaction prediction tasks.

## Appendix H Further Dataset Statistics

In Section 3, we describe the enzyme-similarity split using the Levenshtein distance, ensuring that enzymes in the training and test sets differ by at least \(60\%\) in sequence. While this approach guarantees that the test set enzymes are distinct from those in the training set, it does not necessarily ensure that the test set is representative or meaningfully distinct in terms of enzyme clustering. To work on the concern, we apply MMseq2 alignment to the test set enzyme sequences to analyze their clustering patterns. The results show that \(72.7\%\) of the test enzymes have at least a \(30\%\) sequence difference, \(40.7\%\) have at least a \(50\%\) sequence difference, and \(14.5\%\) have at least a \(70\%\) sequence difference. These statistics suggest that while there is substantial diversity in the test set, additional considerations may be necessary to ensure that it accurately reflects the broader enzyme landscape rather than being skewed by unrepresentative outliers.

Similarly, we introduce the reaction-similarity split using the Needleman-Wunsch algorithm applied to SMILES, ensuring that reactions in the test set are distinct and do not overlap with those in the training set. We apply Needleman-Wunsch algorithm to the SMILES of test set reactions to analyze their clustering patterns. The results show that \(92.3\%\) of the test enzymes have at least a \(10\%\) SMILES difference, \(60.9\%\) have at least a \(30\%\) SMILES difference, and \(14.5\%\) have at least a \(50\%\) SMILES difference. These results indicate a significant level of diversity in the test set reactions, although additional considerations might be necessary to ensure the representativeness and typicality of the test set in capturing the broader reaction space.

\begin{table}

\end{table}
Table 20: Comparisons between baselines and fingerprint features on _reaction-similarity-based split_.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [N/A].
* [N/A] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes]" is generally preferable to "[No]", it is perfectly acceptable to answer "[No]" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No]" or "[N/A]" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Full experiments Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Section 4 & 5Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [N/A] Justification: NA Guidelines: The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Provided with code amnd dataset links for checking Guidelines: * The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Full code access Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ** Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All discussed in Section 4 Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Average experimental results Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Single A40 GPU Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Well confirmed Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Well discussed Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [N/A] Justification: NA Guidelines: * The answer NA means that the paper poses no such risks.

* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Full credits Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [N/A] Justification: NA Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [N/A] Justification: NAGuidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [N/A] Justification: NA Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.