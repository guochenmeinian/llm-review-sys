# Normalizing flow neural networks by JKO scheme

 Chen Xu

School of Industrial and

Systems Engineering

Georgia Tech

&Xiuyuan Cheng

Department of Mathematics

Duke University

&Yao Xie

School of Industrial and

Systems Engineering

Georgia Tech

###### Abstract

Normalizing flow is a class of deep generative models for efficient sampling and likelihood estimation, which achieves attractive performance, particularly in high dimensions. The flow is often implemented using a sequence of invertible residual blocks. Existing works adopt special network architectures and regularization of flow trajectories. In this paper, we develop a neural ODE flow network called JKO-iFlow, inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which unfolds the discrete-time dynamic of the Wasserstein gradient flow. The proposed method stacks residual blocks one after another, allowing efficient block-wise training of the residual blocks, avoiding sampling SDE trajectories and score matching or variational learning, thus reducing the memory load and difficulty in end-to-end training. We also develop adaptive time reparameterization of the flow network with a progressive refinement of the induced trajectory in probability space to improve the model accuracy further. Experiments with synthetic and real data show that the proposed JKO-iFlow network achieves competitive performance compared with existing flow and diffusion models at a significantly reduced computational and memory cost.

## 1 Introduction

Generative models have wide applications in statistics and machine learning to infer data-generating distributions and to sample from the model distributions learned from the data. In addition to widely used deep generative models such as variational auto-encoders (VAE) Kingma and Welling (2014, 2019) and generative adversarial networks (GAN) Goodfellow et al. (2014); Gulrajani et al. (2017), normalizing flow models Kobyzev et al. (2020) have been popular and with a great potential. The flow model learns the data distribution via an invertible mapping \(F\) between the data density \(p_{X}\) in \(\mathbb{R}^{d}\) and the target standard multivariate Gaussian density \(p_{Z}\), \(Z\sim\mathcal{N}(0,I_{d})\). While flow models, once trained, can be utilized for efficient data sampling and explicit likelihood evaluation, training of such models is often difficult in practice. To alleviate such difficulties, many prior works (Dinh et al., 2015, 2017; Kingma and Dhariwal, 2018; Behrmann et al., 2019; Ruthotto et al., 2020; Onken et al., 2021), among others, have explored designs of training objectives, network architectures, and computational techniques.

Among various flow models, continuous normalizing flow (CNF) transports the data density to a target distribution through continuous dynamics, primarily neural ODE (Chen et al., 2018) models. CNF

Figure 1: Comparison of JKO-iFlow (proposed) and standard CNF models. In contrast to most existing CNF models, JKO-Flow learns the unique deterministic transport equation corresponding to the diffusion process by directly performing block-wise training of a neural ODE model.

models have shown promising performance on generative tasks Grathwohl et al. (2019); Kobyzev et al. (2020). However, a known computational challenge of CNF models is model regularization, primarily due to the non-uniqueness of the flow transport. Without additional regularization, the trained CNF model such as FFJORD Grathwohl et al. (2019) may follow a less regular trajectory in the probability space, see Figure 1(b), which may worsen the generative performance. While regularization of flow models has been developed using different techniques, including using spectral normalization Behrmann et al. (2019) and optimal transport Liutkus et al. (2019); Onken et al. (2021); Finlay et al. (2020); Xu et al. (2022); Huang et al. (2023), only using regularization may not resolve the non-uniqueness of the flow. Besides regularization, several practical difficulties remain when training CNF models, particularly the high computational cost. In many settings, CNF models consist of stacked blocks, and each can be complex. _End-to-end training_ of such deep models often places a high demand on computational resources and memory consumption.

In this work, we propose JKO-iFlow, an invertible normalizing flow network that unfolds the Wasserstein gradient flow via a neural ODE model inspired by the JKO-scheme Jordan et al. (1998). The JKO scheme, see (5), can be viewed as a proximal step to minimize the Kullback-Leibler (KL) divergence between the current density and the equilibrium density. It recovers the solution of the Fokker-Planck equation (FPE) in the limit of small step size. The proposed JKO-iFlow model thus can be viewed as trained to learn the _unique_ transport map following the FPE which flows from the data distribution toward the normal equilibrium and gives a smooth trajectory of density evolution, see Figure 1(a). Unlike most CNF models, where all the residual blocks are trained end-to-end, each block in the JKO-iFlow network implements one step in the JKO scheme to learn the deterministic transport map by minimizing an objective of that block given the trained previous blocks. The block-wise training significantly reduces memory and computational load. Theoretically, with a small step size, the discrete-time transport map approximates the continuous solution of FPE, which leads to the invertibility of each trained residual block. This leaves the residual blocks to be completely general, such as graph neural network layers and convolutional neural network layers, depending on the structure of data considered in the problem. The theoretical need for small step sizes does not incur a restriction in practice, whereby one can use step size not exceeding a certain maximum value when adopting the neural ODE integration. We further introduce time reparameterization with progressive refinement in computing the flow network, where each block corresponds to a representative point along the density evolution trajectory in the space of probability measures. The algorithm adaptively chooses the number of blocks and step sizes.

The proposed approach is related to the diffusion models (Song and Ermon, 2019; Ho et al., 2020; Block et al., 2020; Song et al., 2021) yet differs fundamentally in that our approach is a type of flow models, which directly computes the data likelihood, and such likelihood estimation is essential for statistical inference. While the diffusion models can also indirectly obtain likelihood estimation, they are more designed as samplers. In terms of implementation, our approach trains a neural ODE model without SDE sampling (injection of noise) nor learning of score matching. It also differs from previous works on progressive training of ResNet generative models Johnson and Zhang (2019); Fan et al. (2022) in that the model trains an invertible flow mapping and avoids inner loops of variational learning. We refer to Section 1.1 for more discussions on related works. Empirically, JKO-iFlow yields competitive performance as other CNF models with significantly less computation. The model is also compatible with general equilibrium density \(p_{Z}\) having a parametrized potential \(V\), exemplified by the application to the conditional generation setting where \(p_{Z}\) is replaced with Gaussian mixtures Xu et al. (2022). In summary, the contributions of the work include

\(\bullet\) We propose an invertible neural ODE model where each residual block corresponds to a JKO step, and the training objective can be computed from pushed data samples through the previous blocks. The residual block has a general form, and the invertibility is ensured due to the regularity and continuity of the approximate solution of the FPE.

\(\bullet\) We develop a block-wise procedure to train the JKO-iFlow model, which can adaptively determine the number of blocks. We also propose to adaptively reparameterize the computed trajectory in the probability space with refinement, which improves the model accuracy and the overall computational efficiency.

\(\bullet\) We show that JKO-iFlow greatly reduces memory and computational cost when achieving competitive or better generative performance and likelihood estimation compared to existing flow and diffusion models on simulated and real data.

### Related works

For deep generative models, popular approaches include generative adversarial networks (GAN) (Goodfellow et al., 2014; Gulrajani et al., 2017; Isola et al., 2017) and variational auto-encoder (VAE)(Kingma and Welling, 2014, 2019). Apart from known training difficulties (e.g., mode collapse (Salimans et al., 2016) and posterior collapse (Lucas et al., 2019)), these models do not provide likelihood or inference of data density. The normalizing flow framework (Kobyzev et al., 2020) has been extensively developed, including continuous flow (Grathwohl et al., 2019), Monge-Ampere flow (Zhang et al., 2018), discrete flow (Chen et al., 2019), and extension to non-Euclidean data (Liu et al., 2019; Mathieu and Nickel, 2020; Xu et al., 2022). Efforts have been made to develop novel invertible mapping structures (Dinh et al., 2017; Papamakarios et al., 2017) and regularize the flow trajectories by transport cost (Finlay et al., 2020; Onken et al., 2021; Ruthotto et al., 2020; Xu et al., 2022; Huang et al., 2023). Despite such efforts, the model and computational challenges of normalizing flow models include regularization and the large model size when using a large number of residual blocks, which cannot be determined a priori, and the associated memory and computational load.

In parallel to continuous normalizing flows, which are neural ODE models, neural SDE models become an emerging tool for generative tasks. Diffusion process and Langevin dynamics in deep generative models have been studied in score-based generative models (Song and Ermon, 2019; Ho et al., 2020; Block et al., 2020; Song et al., 2021) under different settings. Specifically, these models estimate the score function (i.e., the gradient of the log probability density with respect to data) of data distribution via neural network parametrization, which may encounter challenges in learning and sampling high dimensional data and call for special techniques (Song and Ermon, 2019). The recent work of Song et al. (2021) developed reverse-time SDE sampling for score-based generative models and adopted the connection to neural ODE to compute the likelihood; using the same idea of backward SDE, Zhang and Chen (2021) proposed joint training of forward and backward neural SDEs. Theoretically, latent diffusion Tzen and Raginsky (2019, 2020) was used to analyze neural SDE models. The current work focuses on a neural ODE model where the deterministic vector field \(\mathbf{f}(x,t)\) is to be learned from data following a JKO scheme of the FPE. Rather than neural SDE, our approach involves no sampling of SDE trajectories nor learning of the score function, and it learns an invertible residual network directly. In contrast, diffusion-based models derive the ODE model from the learned diffusion model to achieve explicit likelihood computation. For example, Song et al. (2021) derived neural ODE model from the learned score function of the diffused data marginal distributions for all \(t\). We experimentally obtain competitive or improved performance against the diffusion model on simulated two-dimensional and high-dimensional tabular data.

JKO-inspired deep models have been studied in several recent works. (Bunne et al., 2022) reformulated the JKO step for minimizing an energy function over convex functions. JKO scheme has also been used to discretize Wasserstein gradient flow to learn a deep generative model in (Alvarez-Melis et al., 2022; Mokrov et al., 2021), which adopted input convex neural networks (ICNN) (Amos et al., 2017). ICNN, as a special type of network architecture, may have limited expressiveness (Rout et al., 2022; Korotin et al., 2021). In addition to using the gradient of ICNN, (Fan et al., 2022) proposed parametrizing the transport in a JKO step by a residual network but identified difficulty in calculating the push-forward distribution. The approach in (Fan et al., 2022) also relies on a variational formulation, which requires training an additional network similar to the discriminator in GAN using inner loops. The idea of progressive additive learning in training generative ResNet, namely training ResNet block-wisely by a variational loss, dates back to Johnson and Zhang (2019) under the GAN framework. Our method trains an invertible neural-ODE flow network that flows from data density to the normal one and backward, which enables the computation of model likelihood as in other neural-ODE approaches. The objective in each JKO step to minimize KL divergence can also be computed directly without any inner-loop training, see Section 4.

Compared to score-based neural SDE methods, our approach is closer to the more recent flow-based models related to diffusion models (Lipman et al., 2023; Albergo and Vanden-Eijnden, 2023; Boffi and Vanden-Eijnden, 2023). These works proposed to learn the transport equation (a deterministic ODE) corresponding to the SDE process. Specifically, (Lipman et al., 2023; Albergo and Vanden-Eijnden,2023] matched the velocity field from interpolated distributions between initial and terminal ones; [Boffi and Vanden-Eijnden, 2023] proposed to learn the score \(\nabla\log\rho_{t}(x)\) (where \(\rho_{t}\) solves the FPE and is unknown _a priori_) to solve high-dimensional FPE. Using Stein's identity (which is equivalent to the derivation in Section 3.2), the step-wise training objective in [Boffi and Vanden-Eijnden, 2023] optimizes to learn the score without the need to simulate the entire SDE. The idea of approximating the solution of FPE by a deterministic transport equation dates back to the 90s Degond and Mustieles [1990], Degond and Mas-Gallic [1989], and has been used in kernel-based solver of FPE in [Maoutsa et al., 2020] and studied via a self-consistency equation in [Shen et al., 2022]. While our approach learns an equivalent velocity field at the infinitesimal time step, see section 3.2, the formulation in JKO-iFlow is at a finite time-step motivated by the JKO scheme. We also mention that an independent concurrent work [Vidal et al., 2023] proposed a similar block-wise training algorithm using the JKO scheme under the framework of [Onken et al., 2021] and demonstrated benefits in avoiding tuning the penalty hyperparameter associated with the KL-divergence objective. Our model is applied to generative tasks of high-dimensional data, including image data, and we also develop additional techniques for computing the flow probability trajectory.

For the expressiveness of deep generative models, approximation properties of deep neural networks for representing probability distributions have been developed in several works. Lee et al. [2017] established approximation by composition of Barron functions [Barron, 1993]; Bailey and Telgarsky [2018] developed space-filling approach, which was generalized in Perekrestenko et al. [2020, 2021]; Lu and Lu [2020] constructed a deep ReLU network with guaranteed approximation under integral probability metrics, using techniques of empirical measures and optimal transport. These results show that deep neural networks can provably transport one source distribution to a target one with sufficient model capacity under certain regularity conditions of the pair of densities. Our JKO-iFlow model potentially leads to a constructive approximation analysis of the neural ODE flow model.

## 2 Preliminaries

_Normalizing flow_. A normalizing flow can be mathematically expressed via a density evolution equation of \(\rho(x,t)\) such that \(\rho(x,0)=p_{X}\) and as \(t\) increases \(\rho(x,t)\) approaches \(p_{Z}\sim\mathcal{N}(0,I_{d})\)[Tabak and Vanden-Eijnden, 2010]. Given an initial distribution \(\rho(x,0)\), such a flow typically is not unique. We consider when the flow is induced by an ODE of \(x(t)\) in \(\mathbb{R}^{d}\)

\[\dot{x}(t)=\mathbf{f}(x(t),t),\] (1)

where \(x(0)\sim p_{X}\). The marginal density of \(x(t)\) is denoted as \(p(x,t)\), and it evolves according to the continuity equation (Liouville equation) of (1) written as

\[\partial_{t}p+\nabla\cdot(p\mathbf{f})=0,\quad p(x,0)=p_{X}(x).\] (2)

_Ornstein-Uhlenbeck (OU) process_. Consider a Langevin dynamic denoted by the SDE \(dX_{t}=-\nabla V(X_{t})dt+\sqrt{2}dW_{t}\), where \(V\) is the potential of the equilibrium density \(p_{Z}\). We focus on the case of normal equilibrium, that is, \(V(x)=|x|^{2}/2\) and then \(p_{Z}\propto e^{-V}\). In this case, the process is known as the (multivariate) OU process. Suppose \(X_{0}\sim p_{X}\), and let the density of \(X_{t}\) be \(\rho(x,t)\) also denoted as \(\rho_{t}(\cdot)\). The Fokker-Planck equation (FPE) describes the evolution of \(\rho_{t}\) towards the equilibrium \(p_{Z}\) as follows, where \(V(x):=|x|^{2}/2\),

\[\partial_{t}\rho=\nabla\cdot(\rho\nabla V+\nabla\rho),\quad\rho(x,0)=p_{X}(x).\] (3)

Under generic conditions, \(\rho_{t}\) converges to \(p_{Z}\) exponentially fast. For Wasserstein-2 distance and the standard normal \(p_{Z}\), classical argument gives that (take \(C=1\) in Eqn (6) of Bolley et al. [2012])

\[W_{2}(\rho_{t},p_{Z})\leq e^{-t}W_{2}(\rho_{0},p_{Z}),\quad t>0.\] (4)

_JKO scheme_. The seminal work Jordan et al. [1998] established a time discretization scheme of the solution to (3) by the gradient flow to minimize \(\mathrm{KL}(\rho||p_{Z})\) under the Wasserstein-2 metric in probability space. Denote by \(\mathcal{P}\) the space of all probability densities on \(\mathbb{R}^{d}\) with a finite second moment. The JKO scheme computes a sequence of distributions \(p_{k}\), \(k=0,1,\cdots,\) starting from \(p_{0}=\rho_{0}\in\mathcal{P}\). With step size \(h>0\), the scheme at the \(k\)-th step is written as

\[p_{k+1}=\arg\min_{\rho\in\mathcal{P}}F[\rho]+\frac{1}{2h}W_{2}^{2}(p_{k},\rho),\] (5)

where \(F[\rho]:=\mathrm{KL}(\rho||p_{Z})\). It was proved in Jordan et al. [1998] that as \(h\to 0\), the solution \(p_{k}\) converges to the solution \(\rho(\cdot,kh)\) of (3) for all \(k\), and the convergence \(\rho_{(h)}(\cdot,t)\to\rho(\cdot,t)\) is strongly in \(L^{1}(\mathbb{R}^{d},(0,T))\) for finite \(T\) where \(\rho_{(h)}\) is piece-wise constant interpolated from \(p_{k}\).

JKO scheme by neural ODE

Given i.i.d. observed data samples \(X_{i}\in\mathbb{R}^{d}\), \(i=1,\ldots,N\), drawn from some unknown density \(p_{X}\), the goal is to train an invertible neural network to transport the density \(p_{X}\) to an _a priori_ specified density \(p_{Z}\) in \(\mathbb{R}^{d}\), where each data sample \(X_{i}\) is mapped to a code \(Z_{i}\). A prototypical choice of \(p_{Z}\) is the standard multivariate Gaussian \(\mathcal{N}(0,I_{d})\). In this work, we leave the potential of \(p_{Z}\) abstract and denote by \(V\), that is, \(p_{Z}\propto e^{-V}\) and \(V(x)=|x|^{2}/2\) for normal \(p_{Z}\). By a slight abuse of notation, we denote by \(p_{X}\) and \(p_{Z}\) both the distributions and the density functions of data \(X\) and code \(Z\) respectively.

### Objective of JKO step

We are to specify \(\mathbf{f}(x,t)\) in the ODE (1), to be parametrized and learned by a neural ODE, such that the induced density evolution of \(p(x,t)\) converges to \(p_{Z}\) as \(t\) increases. We start by dividing the time horizon \([0,T]\) into finite subintervals with step size \(h\), let \(t_{k}=kh\) and \(I_{k+1}:=[t_{k},t_{k+1})\). Define \(p_{k}(x):=p(x,kh)\), namely the density of \(x(t)\) at \(t=kh\). The solution of (1) determined by the vector-field \(\mathbf{f}(x,t)\) on \(t\in I_{k+1}\) (assuming the ODE is well-posed (Sideris, 2013)) gives a one-to-one mapping \(T_{k+1}\) on \(\mathbb{R}^{d}\), s.t. \(T_{k+1}(x(t_{k}))=x(t_{k+1})\) and \(T_{k+1}\) transports \(p_{k}\) into \(p_{k+1}\), i.e., \((T_{k})_{\#}p_{k-1}=p_{k}\), where we denote by \(T_{\#}p\) the push-forward of distribution \(p\) by \(T\), such that \((T_{\#}p)(A)=p(T^{-1}(A))\) for a measurable set \(A\). In other words, the mapping \(T_{k+1}\) is the solution map of the ODE from time \(t_{k}\) to \(t_{k+1}\).

Suppose we can find \(\mathbf{f}(\cdot,t)\) on \(I_{k+1}\) such that the corresponding \(T_{k+1}\) solves the JKO scheme (5), then with small \(h\), \(p_{k}\) approximates the solution to the Fokker-Planck equation 3, which then flows towards \(p_{Z}\). By the Monge formulation of the Wasserstein-2 distance between \(p\) and \(q\) as \(W_{2}^{2}(p,q)=\min_{T:T_{\#}p_{k}=q}\mathbb{E}_{\kappa\sim p}\|x-T(x)\|^{2}\), solving for the transported density \(p_{k}\) by (5) is equivalent to solving for the transport \(T_{k+1}\) by

\[T_{k+1}=\arg\min_{T:\mathbb{R}^{d}\to\mathbb{R}^{d}}F[T]+\frac{1}{2h}\mathbb{ E}_{x\sim p_{k}}\|x-T(x)\|^{2},\] (6)

where \(F[T]=\mathrm{KL}(T_{\#}p_{k}\|p_{Z})\). The equivalence between (5) and (6) is proved in Lemma A.1.

Furthermore, the following proposition gives that, once \(p_{k}\) is determined by \(\mathbf{f}(x,t)\) for \(t\leq t_{k}\), the value of \(F[T]\) can be computed from \(\mathbf{f}(x,t)\) on \(t\in I_{k+1}\) only. The counterpart for convex function-based parametrization of \(T_{k}\) was given in Theorem 1 of (Mokrov et al., 2021), where the computation using the change-of-variable differs as we adopt an invertible neural ODE approach here. The proof is left to Appendix A.

**Proposition 3.1**.: _Given \(p_{k}\), up to a constant \(c\) independent from \(\mathbf{f}(x,t)\) on \(t\in I_{k+1}\),_

\[\mathrm{KL}(T_{\#}p_{k}\|p_{Z})=\mathbb{E}_{x(t_{k})\sim p_{k}}\left(V(x(t_{k +1}))-\int_{t_{k}}^{t_{k+1}}\nabla\cdot\mathbf{f}(x(s),s)ds\right)+c.\] (7)

By Proposition 3.1, the minimization (6) is equivalent to

\[\min_{\{\mathbf{f}(x,t)\}_{t\in I_{k+1}}}\mathbb{E}_{x(t_{k})\sim p_{k}}\big{(} V(x(t_{k+1}))-\int_{t_{k}}^{t_{k+1}}\nabla\cdot\mathbf{f}(x(s),s)ds+\frac{1}{2h} \|x(t_{k+1})-x(t_{k})\|^{2}\big{)},\] (8)

where \(x(t_{k+1})=x(t_{k})+\int_{t_{k}}^{t_{k+1}}\mathbf{f}(x(s),s)ds\). Taking a neural ODE approach, we parametrize \(\{\mathbf{f}(x,t)\}_{t\in I_{k+1}}\) as a residual block with parameter \(\theta_{k+1}\), and then (8) is reduced to minimizing over \(\theta_{k+1}\). This leads to a block-wise learning algorithm to be introduced in Section 4, where we further allow the step-size \(h\) to vary for different \(k\) as well.

### Infinitesimal optimal \(\mathbf{f}(x,t)\)

In each JKO step of (8), let \(p=p_{k}\) denote the current density, \(q=p_{Z}\) be the target equilibrium density. In this subsection, we show that the optimal \(\mathbf{f}\) in (8) with small \(h\) reveals the difference between score functions between target and current densities. Thus, minimizing the objective (8) searches for a neural network parametrization of the score function \(\nabla\log\rho_{t}\) implicitly, in contrast to diffusion-based models which learn the score function explicitly (Ho et al., 2020; Song et al.,2021), e.g., via denoising score matching. At infinitesimal \(h\), this is equivalent to solving the FPE by learning a deterministic transport equation as in (Boffi and Vanden-Eijnden, 2023; Shen et al., 2022).

Consider general equilibrium distribution \(q\) with a differentiable potential \(V\). To analyze the optimal pushforward mapping in the small \(h\) limit, we shift the time interval \([kh,(k+1)h]\) to be \([0,h]\) to simplify the notation. Then (8) is reduced to

\[\min_{\{\mathbf{f}(x,t)\}_{t\in[0,h)}}\mathbb{E}_{x(0)\sim p}\left(V(x(h))- \int_{0}^{h}\nabla\cdot\mathbf{f}(x(s),s)ds+\frac{1}{2h}\|x(h)-x(0)\|^{2} \right),\] (9)

where \(x(h)=x(0)+\int_{0}^{h}\mathbf{f}(x(s),s)ds\). In the limit of \(h\to 0+\), formally, \(x(h)-x(0)=h\mathbf{f}(x(0),0)+O(h^{2})\), and suppose \(V\) of \(q\) is \(C^{2}\), \(V(x(h))=V(x(0))+h\nabla V(x(0))\cdot\mathbf{f}(x(0),0)+O(h^{2})\). For any differentiable density \(\rho\), the (Stein) score function is defined as \(\mathbf{s}_{\rho}=\nabla\log\rho\), and we have \(\nabla V=-\mathbf{s}_{q}\). Taking the formal expansion of orders of \(h\), the objective in (9) is written as

\[\mathbb{E}_{x\sim p}\left(V(x)+h\left(-\mathbf{s}_{q}(x)\cdot\mathbf{f}(x,0)- \nabla\cdot\mathbf{f}(x,0)+\frac{1}{2}\|\mathbf{f}(x,0)\|^{2}\right)+O(h^{2}) \right).\] (10)

Note that \(\mathbb{E}_{x\sim p}V(x)\) is independent of \(\mathbf{f}(x,t)\), and the \(O(h)\) order term in (10) is over \(\mathbf{f}(x,0)\) only, thus the minimization of the leading term is equivalent to

\[\min_{\mathbf{f}\left(\cdot\right)=\mathbf{f}\left(\cdot,0\right)}\mathbb{E}_{ x\sim p}\left(-T_{q}\mathbf{f}+\frac{1}{2}\|\mathbf{f}\|^{2}\right),\quad T_{q} \mathbf{f}:=\mathbf{s}_{q}\cdot\mathbf{f}+\nabla\cdot\mathbf{f},\] (11)

where \(T_{q}\) is known as the Stein operator (Stein, 1972). The \(T_{q}\mathbf{f}\) in (11) echoes that the derivative of KL divergence with respect to transport map gives Stein operator (Liu and Wang, 2016). The Wasserstein-2 regularization gives an \(L^{2}\) regularization in (11). Let \(L^{2}(p)\) be the \(L^{2}\) space on \((\mathbb{R}^{d},p(x)dx)\), and for vector field \(\mathbf{v}\) on \(\mathbb{R}^{d}\), \(\mathbf{v}\in L^{2}(p)\) if \(\int|\mathbf{v}(x)|^{2}p(x)dx<\infty\). One can verify that, when both \(\mathbf{s}_{p}\) and \(\mathbf{s}_{q}\) are in \(L^{2}(p)\), the minimizer of (11) is

\[\mathbf{f}^{*}\left(\cdot,0\right)=\mathbf{s}_{q}-\mathbf{s}_{p}.\]

This shows that the infinitesimal optimal \(\mathbf{f}(x,t)\) equals the difference between the score functions of the equilibrium and the current density.

### Invertibility of flow model and expressiveness

At time \(t\) the current density of \(x(t)\) is \(\rho_{t}\), the analysis in Section 3.2 implies that the optimal vector field \(\mathbf{f}(x,t)\) has the expression as

\[\mathbf{f}(x,t)=\mathbf{s}_{q}-\mathbf{s}_{\rho_{t}}=-\nabla V-\nabla\log\rho _{t}.\] (12)

With this \(\mathbf{f}(x,t)\), the Liouville equation (2) coincides with the FPE (3). This is consistent with the JKO scheme with a small \(h\) recovering the solution to the FPE. Under proper regularity condition of \(V\) and the initial density \(\rho_{0}\), the r.h.s. of (12) is also regular over space and time. This leads to two consequences, in approximation and in learning: Approximation-wise, the regularity of \(\mathbf{f}(x,t)\) allows to construct a \(k\)-th residual block in the flow network to approximate \(\{\mathbf{f}(x,t)\}_{t\in I_{k}}\) when there is sufficient model capacity, by classical universal approximation theory of shallow networks (Barron, 1993; Yarotsky, 2017). We further discuss the approximation analysis based on the proposed model in the last section.

For learning, when properly trained with sufficient data, the neural ODE vector field \(\mathbf{f}(x,t;\theta_{k})\) will learn to approximate (12). This can be viewed as inferring the score function of \(\rho_{t}\), and also leads to the invertibility of the trained flow net in theory: Suppose the trained \(\mathbf{f}(x,t;\theta_{k})\) is close enough to (12); it will also have bounded Lipschitz constant. Then the residual block is invertible as long as the step size \(h\) is sufficiently small, e.g. less than \(1/L\) where \(L\) is the Lipschitz bound of \(\mathbf{f}(x,t;\theta_{k})\). In practice, we typically use smaller \(h\) than needed merely by invertibility (allowed by the model budget) so that the flow network can more closely track the FPE of the diffusion process. The invertibility of the proposed model is numerically verified in experiments (see Table A.1).

## 4 Training of JKO-iFlow net

The proposed JKO-iFlow model allows progressive learning of the residual blocks in the neural-ODE model in a block-wise manner (Section 4.1). We also introduce two techniques to improve the training of the trajectories in probability space (Section 4.2), illustrated in a vector space in Appendix B.3.

### Block-wise training

Note that the training of \((k+1)\)-th block in (8) can be conducted once the previous \(k\) blocks are trained. Specifically, with finite training data \(\{X_{i}=x_{i}(0)\}_{i=1}^{n}\), the expectation \(\mathbb{E}_{x(t)\sim p_{k}}\) in (8) is replaced by the sample average over \(\{x_{i}(kh)\}_{i=1}^{n}\) which can be computed from the previous \(k\) blocks. Note that for each given \(x(t)=x(t_{k})\), both \(x(t_{k+1})\) and the integral of \(\nabla\cdot\mathbf{f}\) in (8) can be computed by a numerical neural ODE integrator. Following previous works, we use the Hutchinson trace estimator (Hutchinson, 1989; Grathwohl et al., 2019) to compute the quantity \(\nabla\cdot\mathbf{f}\) in high dimensions, and we also propose a finite-difference approach to reduce the computational cost (details in Appendix B.2). Applying the numerical integrator in computing (8), we denote the resulting \(k\)-th residual block abstractly as \(f_{\theta_{k}}\) with trainable parameters \(\theta_{k}\).

This leads to a block-wise training of the normalizing flow network, as summarized in Algorithm 1. The sequence of time stamps \(t_{k}\) is given by specifying the time steps \(h_{k}:=t_{k+1}-t_{k}\), which we allow to differ across \(k\). The choice of the sequence \(h_{k}\) is initialized by a geometric sequence starting from \(h_{0}\) with maximum stepsize \(h_{\max}\), see Appendix B.1. In the special case where the multiplying factor is one, the sequence of \(h_{k}\) gives a constant step size. The adaptive choice of \(h_{k}\) with refinement (by adding more blocks) will be introduced in Section 4.2. Regarding the termination criterion \(\mathrm{Ter}(\mathrm{k})\) in line 2 of Algorithm 1, we monitor the ratio \(\mathbb{E}_{x\sim p_{k-1}}\|x-T_{k}(x)\|^{2}/\mathbb{E}_{x\sim p_{k-1}}\|T_{k }(x)\|^{2}\) and terminate when it is below some threshold \(\epsilon\), set as 0.01 in all experiments. In practice, when training the \(k\)-th block, both the averages of \(\|x-T_{k}(x)\|^{2}\) and \(\|T_{k}(x)\|^{2}\) are computed by empirically averaging over the training samples (in the last epoch) at no additional computational cost. Lastly, line 5 of training a "free block" (i.e., the block without the \(W_{2}\) regularization) is to flow the push-forward density \(p_{L}\) closer to the target density \(p_{Z}\), where the former is obtained through the first \(L\) blocks.

```
0: Time stamps \(\{t_{k}\}\), training data, termination criterion \(\mathrm{Ter}\) and tolerance level \(\epsilon\), maximal number of blocks \(L_{\max}\).
1: Initialize \(k=1\).
2:while\(\mathrm{Ter}(\mathrm{k})>\epsilon\) and \(k\leq L_{\max}\)do
3: Optimize \(f_{\theta_{k}}\) upon minimizing (8) with mini-batch sample approximation, given \(\{f_{\theta_{i}}\}_{i=1}^{k-1}\). Set \(k\gets k+1\).
4:endwhile
5:\(L\gets k\). Optional: Optimize \(f_{\theta_{L+1}}\) without \(W_{2}\) regularization. ```

**Algorithm 1** Block-wise JKO-iFlow training

The block-wise training significantly reduces the memory and computational load since only one block is trained when optimizing (8) regardless of flow depth. Therefore, one can use larger training batches and potentially more expensive numerical integrators within a certain memory budget for higher accuracy. We also empirically observe that training each block using standard back-propagation (rather than the adjoint method in neural ODE) gives a comparable result at a lower cost. To ensure the invertibility of the trained JKO-iFlow network, we further break the time interval \([t_{k-1},t_{k})\) into 3 or 5 subintervals to compute the neural ODE integration, e.g., by Runge-Kutta-4. We empirically verify small inversion errors on test samples.

### Computation of trajectories in probability space

We adopt two additional computational techniques to facilitate learning of the trajectories in the probability space, represented by the sequence of densities \(p_{k}\), \(k=1,\ldots,L\), associated with the \(L\) residual blocks of the proposed normalizing flow network. The two techniques are illustrated in Figure 2. Further details and illustrations of the approach can be found in Appendix B.

\(\bullet\)_Trajectory reparameterization._ We empirically observe fast decay of the movements \(W_{2}^{2}(T_{\#}p_{k},p_{k})\) when \(h_{k}\) is set to be constant, that is, initial blocks transport the densities much further than the later ones. This is consistent with the exponential convergence of the Fokker-Planck flow, see (4), but unwanted in the algorithm because in order to train the current block, the flow model needs to transport data through all previous blocks, and yet the later blocks trained using constant step size barely contribute to the density transport. Hence, instead of having constant \(h_{k}\), we _reparameterize_ the values of \(t_{k}\) through an adaptive procedure based on the \(W_{2}\) distance at each block. The procedure uses an adaptive approach to encourage the \(W_{2}\) movement in each block to be more even across the \(L\) blocks, where the retraining of the trajectory can be potentially warm-started by the previous trajectory in iteration.

\(\bullet\)_Progressive refinement_. The performance of CNF models is typically improved with a larger number of residual blocks, corresponding to a smaller step size. The smallness of stepsize \(h\) also ensures the invertibility of the flow model, in theory and also in practice. However, directly training the model with a small non-adaptive stepsize \(h\) may result in long computation time and convergence to the normal density \(q_{Z}\) only after a large number of blocks, where the choice of \(h_{k}\) is not as efficient as after adaptive reparameterization. We introduce a refinement approach to increase the number of blocks progressively, where each time interval \([t_{k-1},t_{k})\) splits into two halves, and the number of blocks doubles after the adaptive reparameterization of the trajectory converges at the coarse level. The new trajectory at the refined level is again trained with adaptive reparameterization, where the residual blocks can be warm-started from the coarse-level model to accelerate the convergence. The trajectory refinement allows going to a smaller step size \(h_{k}\), which benefits the accuracy of the JKO-iFlow model including the numerical accuracy in integrating each neural ODE block.

## 5 Experiment

In this section, we examine the proposed JKO-iFlow model on simulated and real datasets, including both unconditional and conditional generation tasks. Codes are available at https://github.com/hamrel-cxu/JKO-iFlow.

### Baselines and metrics

We compare five alternatives, including four CNF models and one diffusion model. The first two CNF models are FFJORD (Grathwohl et al., 2019) and OT-Flow (Onken et al., 2021), which are continuous-time flow (neural-ODE models). The next two CNF models are IResNet (Behrmann et al., 2019) and IGNN (Xu et al., 2022), which are discrete in time (ResNet models). The diffusion model baseline is the score-based neural SDE (Song et al., 2021), which we call "ScoreSDE." Details about the experimental setup, including dataset construction and neural network architecture and training can be found in Appendix C.2.

The accuracy of trained generative models is evaluated by two quantitative metrics, the negative log-likelihood (NLL) metric, and the kernel _maximum mean discrepancy_ (MMD) (Gretton et al., 2012a) metric, including MMD-1, MMD-m, and MMD-c for constant, median distance, and custom bandwidth respectively. The test threshold \(\tau\) is computed by bootstrap, where an MMD metric less than \(\tau\) indicates that the generated distribution is evaluated by the MMD test to be the same as the true data distribution (achieving \(\alpha=0.05\) test level). See details in Appendix C.1. The computational cost is measured by the number of mini-batch stochastic gradient descent steps and the training

Figure 3: Results on two-dimensional simulated datasets by JKO-iFlow and competitors.

Figure 2: Diagram illustrating trajectory reparameterization and refinement. Top: the original trajectory under three blocks via Algorithm 1. Bottom: the trajectory under six blocks after reparameterization and refinement, which renders the \(W_{2}\) movements more even.

[MISSING_PAGE_FAIL:9]

FIDs compared to most CNF baselines (Grathwohl et al., 2019; Behrmann et al., 2019; Chen et al., 2019; Finlay et al., 2020).

### Conditional generation

The problem aims to generate input samples \(X\) given a label \(Y\) from the conditional distribution \(X|Y\) to be learned from data. We follow the approach in IGNN (Xu et al., 2022). In this setting, the JKO-iFlow network pushes from the distribution \(X|Y=k\) to the class-specific component in the Gaussian mixture of \(H|Y=k\), see Figure A.7b and Appendix C.2.4 for more details. We apply JKO-iFlow to the Solar ramping dataset and compare it with the original IGNN model, and both models use graph neural network layers in the residual blocks. The results are shown in Figure A.8, where both the NLL and MMD-m metrics indicate the superior performance of JKO-iFlow and is consistent with the visual comparison.

## 6 Discussion

The work can be extended in several directions. The application to larger-scale image datasets and larger graphs will enlarge the scope of usage. To overcome the computational challenge faced by neural ODE models for high dimensional input, e.g., images of higher resolution, one would need to improve the training efficiency of the backpropagation in neural ODE in addition to the dimension reduction techniques by VAE as been explored here. Another possibility is to combine the JKO-iFlow scheme with other backbone flow models that are more suitable for the specific tasks. Meanwhile, it would be interesting to extend the method to other problems for which CNF models have proven to be effective. Examples include multi-dimensional probabilistic regression (Chen et al., 2018), a plug-in to deep architectures such as StyleFlow (Abdal et al., 2021), and the application to Mean-field Games (Huang et al., 2023).

Theoretically, the expressiveness of the flow model to generate a regular data distribution can be analyzed based on Section 3.3. To sketch a road map, a block-wise approximation guarantee of \(\mathbf{f}(x,t)\) as in (12) can lead to approximation of the Fokker-Planck flow (3), which pushes forward the density to be \(\varepsilon\)-close to normality in \(T=\log(1/\varepsilon)\) time, see (4). Reversing the time of the ODE then leads to an approximation of the initial density \(\rho_{0}=p_{X}\) by flowing backward in time from \(T\) to zero. Further analysis under technical assumptions is left to future work. During the time that this paper was being published, the convergence analysis of the proposed model was studied in Cheng et al. (2023).

Figure 4: Generated samples of MNIST, CIFAR10, and Imagenet-32 by JKO-iFlow model in latent space. We select 2 images per class for CIFAR10 and 1 image per class for Imagenet-32. The FIDs are shown in subxaptions. Uncurated samples are shown in Figure A.6.

## Acknowledgement

The authors thank Jianfeng Lu, Yulong Lu, and Yiping Lu for helpful discussions. The work is partially supported by NSF DMS-2134037. C.X. and Y.X. are partially supported by an NSF CAREER CCF-1650913, and NSF DMS-2134037, CMMI-2015787, CMMI-2112533, DMS-1938106, and DMS-1830210 and the Coca-Cola Foundation. XC is also partially supported by NSF DMS-2237842 and Simons Foundation.

## References

* Abdal et al. (2021) Rameen Abdal, Peihao Zhu, Niloy J Mitra, and Peter Wonka. Styleflow: Attribute-conditioned exploration of stylegan-generated images using conditional continuous normalizing flows. _ACM Transactions on Graphics (ToG)_, 40(3):1-21, 2021.
* Albergo and Vanden-Eijnden (2023) Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In _The Eleventh International Conference on Learning Representations_, 2023.
* Alvarez-Melis et al. (2022) David Alvarez-Melis, Yair Schiff, and Youssef Mroueh. Optimizing functionals on the space of probabilities with input convex neural networks. _Transactions on Machine Learning Research_, 2022. ISSN 2835-8856.
* Amos et al. (2017) Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In _International Conference on Machine Learning_, pages 146-155. PMLR, 2017.
* Arcones and Gine (1992) Miguel A Arcones and Evarist Gine. On the bootstrap of u and v statistics. _The Annals of Statistics_, pages 655-674, 1992.
* Bailey and Telgarsky (2018) Bolton Bailey and Matus J Telgarsky. Size-noise tradeoffs in generative networks. _Advances in Neural Information Processing Systems_, 31, 2018.
* Barron (1993) Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. _IEEE Transactions on Information theory_, 39(3):930-945, 1993.
* Behrmann et al. (2019) Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and Jorn-Henrik Jacobsen. Invertible residual networks. In _International Conference on Machine Learning_, pages 573-582. PMLR, 2019.
* Block et al. (2020) Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and langevin sampling. _arXiv preprint arXiv:2002.00107_, 2020.
* Boffi and Vanden-Eijnden (2023) Nicholas M Boffi and Eric Vanden-Eijnden. Probability flow solution of the fokker-planck equation. _Machine Learning: Science and Technology_, 4(3):035012, 2023.
* Bolley et al. (2012) Francois Bolley, Ivan Gentil, and Arnaud Guillin. Convergence to equilibrium in wasserstein distance for fokker-planck equations. _Journal of Functional Analysis_, 263(8):2430-2457, 2012.
* Bunne et al. (2022) Charlotte Bunne, Laetitia Papaxanthos, Andreas Krause, and Marco Cuturi. Proximal optimal transport modeling of population dynamics. In _International Conference on Artificial Intelligence and Statistics_, pages 6511-6528. PMLR, 2022.
* Chen et al. (2018) Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. _Advances in neural information processing systems_, 31, 2018.
* Chen et al. (2019) Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and Jorn-Henrik Jacobsen. Residual flows for invertible generative modeling. _Advances in Neural Information Processing Systems_, 32, 2019.
* Cheng et al. (2023) Xiuyuan Cheng, Jianfeng Lu, Yixin Tan, and Yao Xie. Convergence of flow-based generative models via proximal gradient descent in wasserstein space. _arXiv preprint arXiv:2310.17582_, 2023.
* Defferrard et al. (2016) Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. _Advances in neural information processing systems_, 29, 2016.
* Defferrard et al. (2017)Pierre Degond and S Mas-Gallic. The weighted particle method for convection-diffusion equations. i. the case of an isotropic viscosity. _Mathematics of computation_, 53(188):485-507, 1989.
* Degond and Mustieles (1990) Pierre Degond and Francisco-Jose Mustieles. A deterministic approximation of diffusion equations using particles. _SIAM Journal on Scientific and Statistical Computing_, 11(2):293-310, 1990.
* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Deng (2012) Li Deng. The mnist database of handwritten digit images for machine learning research. _IEEE Signal Processing Magazine_, 29(6):141-142, 2012.
* Dinh et al. (2015) Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components estimation. In Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings_, 2015.
* Dinh et al. (2017) Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017.
* Esser et al. (2021) Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12873-12883, 2021.
* Fan et al. (2022) Jiaojiao Fan, Qinsheng Zhang, Amirhossein Taghvaei, and Yongxin Chen. Variational Wasserstein gradient flow. In _Proceedings of the 39th International Conference on Machine Learning_, pages 6185-6215. PMLR, 2022.
* Fey and Lenssen (2019) Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In _ICLR Workshop on Representation Learning on Graphs and Manifolds_, 2019.
* Finlay et al. (2020) Chris Finlay, Jorn-Henrik Jacobsen, Levon Nurbekyan, and Adam Oberman. How to train your neural ode: the world of jacobian and kinetic regularization. In _International conference on machine learning_, pages 3154-3164. PMLR, 2020.
* Goodfellow et al. (2014) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In _NIPS_, 2014.
* Grathwohl et al. (2019) Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. Scalable reversible generative models with free-form continuous dynamics. In _International Conference on Learning Representations_, 2019.
* Gretton et al. (2012) Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alex Smola. A kernel two-sample test. _J. Mach. Learn. Res._, 13:723-773, 2012a.
* Gretton et al. (2012) Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil, Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. _Advances in neural information processing systems_, 25, 2012b.
* Gulrajani et al. (2017) Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved training of wasserstein gans. In _NIPS_, 2017.
* Heusel et al. (2017) Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.
* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* Huang et al. (2021) Chin-Wei Huang, Jae Hyun Lim, and Aaron C Courville. A variational perspective on diffusion-based generative models and score matching. _Advances in Neural Information Processing Systems_, 34:22863-22876, 2021.
* Huang et al. (2020)Han Huang, Jiajia Yu, Jie Chen, and Rongjie Lai. Bridging mean-field games and normalizing flows with trajectory regularization. _Journal of Computational Physics_, page 112155, 2023.
* Simulation and Computation_, 18:1059-1076, 1989.
* Isola et al. (2017) Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5967-5976, 2017.
* Johnson and Zhang (2019) Rie Johnson and Tong Zhang. A framework of composite functional gradient methods for generative adversarial models. _IEEE transactions on pattern analysis and machine intelligence_, 43(1):17-32, 2019.
* Jordan et al. (1998) Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker-planck equation. _SIAM journal on mathematical analysis_, 29(1):1-17, 1998.
* Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015.
* Kingma and Welling (2014) Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun, editors, _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings_, 2014.
* Kingma and Welling (2019) Diederik P. Kingma and Max Welling. An introduction to variational autoencoders. _Foundations and Trends(r) in Machine Learning_, 12(4):307-392, 2019.
* Kingma and Dhariwal (2018) Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. _Advances in neural information processing systems_, 31, 2018.
* Kobyzev et al. (2020) Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker. Normalizing flows: An introduction and review of current methods. _IEEE transactions on pattern analysis and machine intelligence_, 43(11):3964-3979, 2020.
* Korotin et al. (2021) Alexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Safin, and Evgeny Burnaev. Wasserstein-2 generative networks. In _International Conference on Learning Representations_, 2021.
* Krizhevsky and Hinton (2009) Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009.
* Lee et al. (2017) Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora. On the ability of neural nets to express distributions. In _Conference on Learning Theory_, pages 1271-1296. PMLR, 2017.
* Lipman et al. (2023) Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In _The Eleventh International Conference on Learning Representations_, 2023.
* Liu et al. (2019) Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing flows. _Advances in Neural Information Processing Systems_, 32, 2019.
* Liu and Wang (2016) Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. _Advances in neural information processing systems_, 29, 2016.
* Liutkus et al. (2019) Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert Stoter. Sliced-wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions. In _International Conference on Machine Learning_, pages 4104-4113. PMLR, 2019.
* Lu and Lu (2020) Yulong Lu and Jianfeng Lu. A universal approximation theorem of deep neural networks for expressing probability distributions. _Advances in neural information processing systems_, 33:3094-3105, 2020.
* Liu et al. (2019)

[MISSING_PAGE_FAIL:14]

* Song and Ermon (2019) Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in Neural Information Processing Systems_, 32, 2019.
* Song et al. (2021) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2021.
* Stein (1972) Charles Stein. A bound for the error in the normal approximation to the distribution of a sum of dependent random variables. In _Proceedings of the sixth Berkeley symposium on mathematical statistics and probability, volume 2: Probability theory_, volume 6, pages 583-603. University of California Press, 1972.
* Sutherland et al. (2017) Danica J. Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy. In _International Conference on Learning Representations_, 2017.
* Tabak and Vanden-Eijnden (2010) Esteban G Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent of the log-likelihood. _Communications in Mathematical Sciences_, 8(1):217-233, 2010.
* Tzen and Raginsky (2019a) Belinda Tzen and Maxim Raginsky. Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit. _arXiv preprint arXiv:1905.09883_, 2019a.
* Tzen and Raginsky (2019b) Belinda Tzen and Maxim Raginsky. Theoretical guarantees for sampling and inference in generative models with latent diffusions. In _Conference on Learning Theory_, pages 3084-3114. PMLR, 2019b.
* Vidal et al. (2023) Alexander Vidal, Samy Wu Fung, Luis Tenorio, Stanley Osher, and Levon Nurbeyan. Taming hyperparameter tuning in continuous normalizing flows using the jko scheme. _Scientific Reports_, 13(1):4501, 2023.
* Xu et al. (2022) Chen Xu, Xiuyuan Cheng, and Yao Xie. Invertible neural networks for graph prediction. _IEEE Journal on Selected Areas in Information Theory_, 3(3):454-467, 2022.
* Yarotsky (2017) Dmitry Yarotsky. Error bounds for approximations with deep relu networks. _Neural Networks_, 94:103-114, 2017.
* Zhang et al. (2018) Linfeng Zhang, Lei Wang, et al. Monge-Ampere flow for generative modeling. _arXiv preprint arXiv:1809.10188_, 2018.
* Zhang and Chen (2021) Qinsheng Zhang and Yongxin Chen. Diffusion normalizing flow. _Advances in Neural Information Processing Systems_, 34:16280-16291, 2021.

Proofs

### Proofs in Section

**Lemma A.1**.: _Suppose \(p\) and \(q\) are two densities on \(\mathbb{R}^{d}\) in \(\mathcal{P}\), the following two problems_

\[\min_{\rho\in\mathcal{P}}L_{\rho}[\rho]=\operatorname{KL}(\rho||q)+\frac{1}{2h} W_{2}^{2}(p,\rho),\] (13)

\[\min_{T:\mathbb{R}^{d}\to\mathbb{R}^{d}}L_{T}[T]=\operatorname{KL}(T_{\#}p||q )+\frac{1}{2h}\mathbb{E}_{x\sim p}\|x-T(x)\|^{2},\] (14)

_have the same minimum and_

_(a) If_ \(T^{*}:\mathbb{R}^{d}\to\mathbb{R}^{d}\) _is a minimizer of (_14_), then_ \(\rho^{*}=(T^{*})_{\#}p\) _is a minimizer of (_13_)._

_(b) If_ \(\rho^{*}\) _is a minimizer of (_13_), then the optimal transport from_ \(p\) _to_ \(\rho^{*}\) _minimizes (_14_)._

Proof of Lemma a.1.: Let the minimum of (14) be \(L_{T}^{*}\), and that of (13) be \(L_{\rho}^{*}\).

Proof of (a): Suppose \(L_{T}\) achieves minimum at \(T^{*}\), then \(T^{*}\) is the optimal transport from \(p\) to \(\rho^{*}=(T^{*})_{\#}p\) because otherwise \(L_{T}\) can be further improved. By definition of \(L_{\rho}\), we have \(L_{T}^{*}=L_{T}[T^{*}]=L_{\rho}|\rho^{*}|\geq L_{\rho}^{*}\). We claim that \(L_{T}^{*}=L_{\rho}^{*}\). Otherwise, there is another \(\rho^{\prime}\) such that \(L_{\rho}[\rho^{\prime}]<L_{T}^{*}\). Let \(T^{\prime}\) be the optimal transport from \(p\) to \(\rho^{\prime}\), and then \(L_{T}[T^{\prime}]=L_{\rho}|\rho^{\prime}|<L_{T}^{*}\), contradicting with that \(L_{T}^{*}\) is the minimum of \(L_{T}\). This also shows that \(L_{\rho}[\rho^{*}]=L_{T}^{*}=L_{\rho}^{*}\), that is, \(\rho^{*}\) is a minimizer of \(L_{\rho}\).

Proof of (b): Suppose \(L_{\rho}\) achieves minimum at \(\rho^{*}\). Let \(T^{*}\) be the OT from \(p\) to \(\rho^{*}\), then \(\mathbb{E}_{x\sim p}|x-T^{*}(x)|^{2}=W_{2}(p,\rho^{*})^{2}\), and then \(L_{T}[T^{*}]=L_{\rho}[\rho^{*}]=L_{\rho}^{*}\) which equals \(L_{T}^{*}\) as proved in (a). This shows that \(T^{*}\) is a minimizer of \(L_{T}\). 

Proof of Proposition 3.1.: Given \(p_{k}\) being the density of \(x(t)\) at \(t=kh\), recall that \(T\) is the solution map from \(x(t)\) to \(x(t+h)\). We denote \(\rho_{t}:=p_{k}\), and \(\rho_{t+h}:=T_{\#}p_{k}\). By definition,

\[\operatorname{KL}(T_{\#}p_{k}||p_{Z})=\mathbb{E}_{x\sim\rho_{t+h}}(\log\rho_{t +h}(x)-\log p_{Z}(x)).\] (15)

Because \(p_{Z}\propto e^{-V}\), \(V(x)=|x|^{2}/2\), we have \(\log p_{Z}(x)=-V(x)+c_{1}\) for some constant \(c_{1}\). Thus

\[\mathbb{E}_{x\sim\rho_{t+h}}\log p_{Z}(x)=\mathbb{E}_{x(t)\sim\rho_{t}}\log p _{Z}(x(t+h))=c_{1}-\mathbb{E}_{x(t)\sim\rho_{t}}V(x(t+h)).\] (16)

To compute the first term in (15), note that

\[\mathbb{E}_{x\sim\rho_{t+h}}\log\rho_{t+h}(x)=\mathbb{E}_{x(t)\sim\rho_{t}} \log\rho_{t+h}(x(t+h)),\] (17)

and by the expression (called "instantaneous change-of-variable formula" in normalizing flow literature (Chen et al., 2018), which we derive directly below)

\[\frac{d}{dt}\log\rho(x(t),t)=-\nabla\cdot\mathbf{f}(x(t),t),\] (18)

we have that for each value of \(x(t)\),

\[\log\rho_{t+h}(x(t+h))=\log\rho(x(t+h),t+h)=\log\rho(x(t),t)-\int_{t}^{t+h} \nabla\cdot\mathbf{f}(x(s),s)ds.\]

Inserting back to (17), we have

\[\mathbb{E}_{x\sim\rho_{t+h}}\log\rho_{t+h}(x)=\mathbb{E}_{x(t)\sim\rho_{t}} \log\rho_{t}(x(t))-\mathbb{E}_{x(t)\sim\rho_{t}}\int_{t}^{t+h}\nabla\cdot \mathbf{f}(x(s),s)ds).\]

The first term is determined by \(\rho_{t}=p_{k}\), and thus is a constant \(c_{2}\) independent from \(\mathbf{f}(x,t)\) on \(t\in[kh,(k+1)h]\). Together with (16), we have shown that

\[\text{r.h.s. of \eqref{eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq

[MISSING_PAGE_EMPTY:17]

Let the space be \(\mathbb{R}^{d}\), and \(F:\mathbb{R}^{d}\rightarrow\mathbb{R}\) be a differential landscape. Given a sequence of positive stepsize \(h_{k}\), one can compute a sequence of representative points \(x_{k}\) which discretizes the gradient descent trajectory of minimizing \(F\) (towards a local minimum \(x^{*}\)). Specifically, we have

\[x_{k+1}=\arg\min_{x}F(x)+\frac{1}{2h_{k}}\|x-x_{k}\|_{2}^{2},\] (21)

starting from some fixed \(x_{0}\).

#### b.3.1 Trajectory reparameterization

Starting from an initial sequence of step-size \(\{h_{k}\}_{k=1}^{L}\) as in Section B.1, the reparameterization technique implements an iterative scheme to update the sequence of \(h_{k}\) adaptively. We call the \(j\)-th iteration 'Iter-\(j\)', and denote the sequence by \(\{h_{k}^{(j)}\}_{k=1}^{L}\). The corresponding solution points \(x_{k}\) by solving (21) with \(h_{k}^{(j)}\) are denoted as \(x_{k}^{(j)}\). In addition to the constant \(h_{\max}\), we need another algorithmic parameter \(\eta\in(0,1)\), and we set \(\eta=0.3\) for the vector-space example in computation.

We always have \(x_{0}^{(j)}=x_{0}\) for all 'Iter-\(j\)'. In the \(j\)-th iteration of the parametrization, we compute the following

1. Compute the _arclength_ as \[S_{k}^{(j)}:=\|x_{k-1}^{(j)}-x_{k}^{(j)}\|_{2},\quad k=1,\cdots,L.\] (22)
2. Compute the average arclength \[\bar{S}:=\sum_{k=1}^{L}S_{k}^{(j)}/L.\]
3. Update the stepsize for \(k=1,\ldots,L\) as \[h_{k}^{(j+1)}:=\min\{h_{k}^{(j)}+\eta(\bar{S}h_{k}^{(j)}/S_{k}^{(j)}-h_{k}^{(j )}),h_{\max}\}\]
4. Solve (21) using \(h_{k}^{(j+1)}\) to obtain \(x_{k}^{(j+1)}\).

We terminate the iteration when the arclength \(\{S_{k}^{(j)}\}\) are approximately equal. An illustration is given as Iter-12 in the upper panel of Figure A.1. After the reparameterization iterations, we solve an additional \(x_{L+1}\) by minimizing \(F(x)\) starting from \(x_{L}\). This corresponds to optimizing the "free-block" in Algorithm 1.

#### b.3.2 Progressive refinement

The scheme can be computed using a positive integer refinement factor. For simplicity, we use factor 2 throughout this work.

Given a sequence of \(\{h_{k}\}_{k=1}^{L}\) and the representative points \(\{x_{k}\}_{k=1}^{L}\), the refinement scheme computes a refined trajectory having \(k=1,\cdots,2L\):

1. Compute the refined stepsizes as \[\tilde{h}_{2k-1}=\tilde{h}_{2k}=h_{k}/2,\quad k=1,\ldots,L\]
2. Compute the representative points \(\tilde{x}_{k}\) by solving (21) using \(\tilde{h}_{k}\), possibly warm-starting by \(\tilde{x}_{2k}=x_{k}\) and \(\tilde{x}_{2k-1}=(x_{k}+x_{k-1})/2\).

We then apply the trajectory reparameterization iterations as in Section B.3.1 to the refined trajectory till convergence, and the free-block ending point is also recomputed. An illustration is given as r-Iter-10 in the upper panel of Figure A.1.

### Trajectory improvement in probability space

We apply the two techniques to solve the JKO step (8), which is the counterpart of (21) as a gradient descent scheme with proximal steps. The representative points \(x_{k}\) are replaced with transported distributions \(p_{k}\) which form a sequence of points in \(\mathcal{P}\), and the optimization is over the parametrization of \(p_{k}\) induced by the neural network flow mapping consisting of the first \(k\) residual blocks.

It remains to define the arclength in (22) to implement the two techniques. Because we equip \(\mathcal{P}\) with the Wasserstein-2 metric, we compute (omitting the iteration index \(j\) in the notation)

\[S_{k}=W_{2}(p_{k-1},p_{k})=(\mathbb{E}_{x\sim p_{k-1}}\|x-T_{k}(x)\|^{2})^{1/2},\] (23)

where the transport mapping \(T_{k}(x)=x+\int_{t_{k-1}}^{t_{k}}f_{\theta_{k}}(x(s),s)ds\) and can be computed from the \(k\)-th block. In practice, the expectation \(\mathbb{E}_{x\sim p_{k-1}}\) in (23) is computed by a finite-sample average on the training set.

At last, the optimal warm-start of \(p_{k}\) in the refinement is implemented by inheriting the parameters \(\theta_{k}\) of the trained blocks.

## Appendix C Experimental details

### Quantitative evaluation metrics

Besides visual comparison, we adopt two quantitative metrics to evaluate the performance of generative models, the negative log-likelihood (NLL) metric Grathwohl et al. (2019) and the maximum mean discrepancy (MMD) (Gretton et al., 2012a) metric.

Figure A.1: The upper panel shows the arc lengths and the mean and standard deviation of arc lengths over 12 reparameterization iterations, one refinement, and an additional 10 reparameterization iterations. The lower panel visualizes the trajectory consisting of 17 solution points at "r-Iter-10", where the free point as the 17-th point computes the approximate minimum by \(x_{17}=x_{L+1}=[-1.911,0.105]\).

#### c.1.1 NLL metric

Our computation of the NLL metric follows the standard procedure for neural ODE normalizing flow models Chen et al. (2018); Grathwohl et al. (2019), where the evolution of density \(\rho(x(t))\) can be computed via integrating \(\nabla\cdot\mathbf{f}(x(t),t)\) over time due to the instantaneous change-of-variable formula (18).

Specifically, our model flows from \(t_{0}=0\) to \(t_{L+1}=T\), where \(x(0)\sim p_{0}\) the data distribution, and we train the flow model to make \(x(T)\) follow a normal density \(q\). The model density at data sample \(x\) is expressed as \(\rho(x,0)=(T_{\theta}^{-1})_{\#}q(x)\) where \(T_{\theta}^{-1}\) is the inverse model flow mapping from \(x(T)\) to \(x(0)\). Thus the model log-likelihood can be expressed as

\[\log\rho(x(0),0)=\log q(x(T),T)+\int_{0}^{T}\nabla\cdot\mathbf{f}(x(s),s)ds.\]

In practice, our trained flow model has \(L\) blocks where each block \(f_{\theta_{k}}\) represents \(\mathbf{f}(x,t)\) on \([t_{k-1},t_{k})\) and is parametrized by \(\theta_{k}\). The log-likelihood at test sample \(x\) is then computed by

\[\mathrm{LL}(x)=-\frac{1}{2}(\|x(t_{L+1})\|_{2}^{2}+d\log(2\pi))+\sum_{k=1}^{L +1}\int_{t_{k-1}}^{t_{k}}\nabla\cdot f_{\theta_{k}}(x(s),s)ds,\] (24)

where

\[x(t_{k})=x(t_{k-1})+\int_{t_{k-1}}^{t_{k}}f_{\theta_{k}}(x(s),s)ds\]

starting from \(x(0)=x\). Both the integration of \(f_{\theta_{k}}\) and \(\nabla\cdot f_{\theta_{k}}\) are computed using the numerical scheme of neural ODE. We report NLL in the natural unit of information (i.e., \(\log\) with base \(e\), known as "nats") in all our experiments.

#### c.1.2 MMD metrics

Note that the normalizing flow models use NLL (on training set) as the training objective. In contrast, the MMD metric is an impartial evaluation metric as it is not used to train JKO-iFlow or any competing methods. Given two set of data samples \(\bm{X}:=\{x_{i}\}_{i=1}^{N}\) and \(\bm{\tilde{X}}:=\{\tilde{x}_{j}\}_{j=1}^{M}\) and a kernel function \(k(x,\tilde{x})\), the (squared) kernel MMD (Gretton et al., 2012a) is defined as

\[\text{MMD}(\bm{X},\bm{\tilde{X}}):=\frac{1}{N^{2}}\sum_{i=1}^{N}\sum_{j=1}^{N }k(x_{i},x_{j})+\frac{1}{M^{2}}\sum_{i=1}^{M}\sum_{j=1}^{M}k(\tilde{x}_{i}, \tilde{x}_{j})-\frac{2}{NM}\sum_{i=1}^{N}\sum_{j=1}^{M}k(x_{i},\tilde{x}_{j}),\] (25)

When a generative model is trained, we generate \(M\) i.i.d. data samples by the model to construct the set \(\bm{\tilde{X}}\), and we form the set \(\bm{X}\) using \(N\) true data samples (from the test set). MMD metrics with other choices of kernels are possible Gretton et al. (2012b); Sutherland et al. (2017); Schrab et al. (2023). In all experiments here, we use the Gaussian kernel \(k(x,\tilde{x})=\exp\{-\|x-\tilde{x}\|^{2}/2h^{2}\}\) to stay consistent with reported baselines from (Onken et al., 2021), where \(h>0\) is the bandwidth parameter. We use three ways of setting the bandwidth parameter \(h\):

* Constant bandwidth: \(h=h_{c}=1\). The resulting MMD is denoted as 'MMD-1'.
* Median bandwidth (Gretton et al., 2012a): let \(h=h_{m}\) be the median of \(\|x_{i}-x_{j}\|\) for all distinct \(i\) and \(j\). The median distance is computed from the dataset \(X\). The resulting MMD is denoted as 'MMD-m'.
* Custom bandwidth: on certain datasets when we can use prior knowledge to decide on the bandwidth, we will custom the choice of \(h\) (typically smaller than the median distance, due to that theoretically smaller bandwidth may lead to a more powerful MMD test to distinguish the difference in the underlying distributions) while ensuring that we use large enough \(M\) and \(N\) to compute the MMD metric. We call the metric 'MMD-c'.

On all datasets, we use at most \(N=10K\) test samples as \(\mathbf{X}\), and for each trained model, we generate \(M=10K\) test samples to form the dataset \(\tilde{\mathbf{X}}\) to compute the MMD value defined as in (25). Note that the MMD metric as a measure of distance between two distributions is significant when above a test threshold \(\tau\), which is defined as the upper (\(1-\alpha\))-quantile of the distribution of the MMD

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_FAIL:22]

[MISSING_PAGE_FAIL:23]

To ensure a fair comparison against diffusion models, we perform additional experiments using different noise schedulers \(\beta(t)\) and \(\bar{\beta}_{\max}\) in ScoreSDE on MINIBOONE, following the noise scheduler suggestions in DDPM [Ho et al., 2020]. Note that DDPM can be viewed as a discrete-time version of the variance-preserving ScoreSDE model. As shown in Table A.4, the performance of ScoreSDE is indeed sensitive to the noise schedule. However, the best NLL of ScoreSDE 17.45 from the table is still noticeably higher than the NLL of 12.55 obtained by JKO-iFlow on MINIBOONE in Table A.2, and JKO-iFlow is trained using ten times less number of batches. To improve the performance of ScoreSDE on this example, we use ScoreSDE without restrictions on modeling and computation. Specifically, we consider a larger network following the setup in [Albergo and Vanden-Eijnden, 2023], where the network has 4 hidden layers with 512 hidden nodes per layer and consists of 831K parameters in total. We then train this larger model for 100K batches using ScoreSDE with various noise schedulers, choosing noise schedulers that performed best based on results in Table A.4. From the results reported in Table A.5, we see that the testing NLL by ScoreSDE can be as low as 10.47, which is among the state-of-the-art values reported in Table A.3. Nevertheless, we want to highlight that the proposed JKO-iFlow can obtain competitive results (i.e., 10.55 in Table A.3) with much smaller models: each JKO-iFlow block has only 2 hidden layers with 128 hidden nodes per layer, and we trained 4 blocks that contain 112K parameters in total. We also trained JKO-iFlow for only 2.72K batches, rather than the 100K batches we used for ScoreSDE.

#### c.2.3 Image generation with pre-trained variational auto-encoder

We perform image generation on MNIST [Deng, 2012], CIFAR10 [Krizhevsky and Hinton, 2009], and Imagenet-32 [Deng et al., 2009] datasets. We do so in the latent space of a pre-trained VAE, where we discuss the details below.

Figure A.4: Generative quality on tabular datasets via PCA projection of generated samples. The generative quality in general aligns with the quantitative metrics in Table 1 and A.2.

\begin{table}
\begin{tabular}{l c c c} \hline Noise scheduler \(\backslash\)\(\bar{\beta}_{\max}\) & 5 & 10 & 15 \\ \hline Linear & 11.33 (0.44) & 10.84 (1.17) & 10.47 (0.13) \\ Quadratic & 12.88 (0.40) & 11.11 (0.24) & 11.05 (0.49) \\ \hline \end{tabular}
\end{table}
Table A.5: Testing NLL per noise scheduler and \(\bar{\beta}_{\max}\) combination of ScoreSDE on MINIBOONE, using a larger network with longer training. The table format is identical to that of Table A.4.

_VAE as data pre-processing:_ We train deep VAEs in an adversarial manner following (Esser et al., 2021), and use pre-trained VAEs to pre-process the input images \(X\). Specifically, the encoder \(\mathcal{E}:X\rightarrow(\mu(X),\Sigma(X))\) of the VAE maps a RGB-image \(X\) to parameters of a multivariate Gaussian distribution \(\mathcal{N}(\mu(X),\Sigma(X))\) in a lower dimension \(\tilde{d}\). Then, given any random latent code \(\tilde{X}\sim\mathcal{N}(\mu(X),\Sigma(X))\), the decoder \(\mathcal{D}:\tilde{X}\rightarrow\hat{X}\) of the VAE is trained so that \(X\approx\hat{X}\) for the reconstructed image \(\tilde{X}=\mathcal{D}(\tilde{X})\). On MNIST, we let the latent-space dimension \(\tilde{d}=20\), and on CIFAR10 and ImageNet32, we let the latent-space dimension \(\tilde{d}=192\). There are 5.6M parameters in the VAE encoder and 8.25M parameters in the VAE decoder. Given a trained VAE, we then train the JKO-iFlow model \(T_{\theta}\) to transport invertibly between the distribution of random latent codes \(\tilde{X}\) defined over all training images \(X\) and the standard multivariate Gaussian distribution in \(\mathbb{R}^{\tilde{d}}\).

_Training and test data:_ Training data: 60K images in MNIST, 50K images in CIFAR10, and 1.28M images in Imagenet-32. Test data: 10K images in MNIST and CIFAR10, and 50K images in Imagenet-32.

_Choice of \(h_{k}\):_ on MNIST, we specify \(\{h_{0}=1,\rho=1,h_{\max}=1\}\). on CIFAR10, we specify \(\{h_{0}=1,\rho=1,h_{\max}=1\}\). On Imagenet-32, we specify \(\{h_{0}=1,\rho=1.1,h_{\max}=\infty\}\). We train \(L=6\) blocks on MNIST, and we train \(L=8\) blocks on CIFAR10 and Imagenet-32. On CIFAR10 and Imagenet-32, we also scale \(h_{k}=T\cdot h_{k}/\sum_{j}h_{j}\) so \(\sum_{k}h_{k}=T\). We let \(T=0.8\) on CIFAR10 and \(T=0.4\) on Imagenet-32.

_Network architecture:_ We use the softplus activation with \(\beta=20\) for all hidden layers. On MNIST, we use fully connected residual blocks with three hidden layers at 256 hidden nodes. On CIFAR10 and Imagenet-32, we parametrize each \(\mathbf{f}_{b_{b}}\) as a concatenation of convolution layers and transposed convolution layers. Specifically:

* CIFAR10: convolution layers have channels 3-64-128-256-256 with kernel size 3 and strides 1-1-2-1. Transposed convolution layers have channels 256-256-128-64-3 with kernel size 3-4-3-3 and strides 1-2-1-1. Total 2.1M parameters.
* Imagenet-32: convolution layers have channels 3-64-128-256-512 with kernel size 3 and strides 1-1-2-1. Transposed convolution layers have channels 512-256-128-64-3 with kernel size 3-4-3-3 and strides 1-2-1-1. Total 3.3M parameters.

We remark that because inputs into the JKO-iFlow blocks are latent-space codes that have much lower dimensions than the original image, our JKO-iFlow blocks are simpler and lighter in design than models in previous NeuralODE (Grathwohl et al., 2019; Finlay et al., 2020) and diffusion model works (Ho et al., 2020; Song et al., 2021; Boffi and Vanden-Eijnden, 2023). For instance, the DDPM model (Ho et al., 2020) on CIFAR10 has 35.7M parameters with more sophisticated model designs.

_Training specifics:_ On MNIST, we fix the batch size to be 2000 during training, and we train 15K batches per JKO-iFlow block. We fix the learning rate to be 1e-3.

On CIFAR10 and Imagenet-32, we fix the batch size to be 512 during training, and we train 75K batches per JKO-iFlow block. The time per batch is 0.18 seconds on Imagenet-32 and 0.15 seconds on CIFA10. The total time on Imagenet-32 is 30 hours and on CIFAR10 is 24 hours. When training the blocks, the initial learning rate lr0 is decreased by a constant factor of 0.9 every 2500 batches.

Figure A.5: Reparametrization iterations of JKO-iFlow model on MINIBOONE. (a) After 7 reparameterization iterations, a trajectory of more uniform \(W_{2}\) movements is obtained. (b) Generated samples by the models before and after the moving iterations (visualized in the first two principal components computed from test data).

We let lr0 be 1e-3 when training the first 5 blocks on Imagenet-32 and decrease it to be 8e-4 on blocks 6-8. We let lr0 be 1e-3 when training the first 2 blocks on CIFAR10 and decrease it to be 7.5e-4 on the rest 6 blocks. Additionally, we use gradient clipping of max norm 1 after collecting gradients on each mini-batch.

_Additional results:_ We perform the following steps to curate generated samples for CIFAR10 and Imagenet-32, which are shown in Figure 4, we first train an image classifier (i.e., VGG-16 [Simonyan and Zisserman, 2015]) on the training data. Then, we generate a large number of uncurated images (200K for Imagenet-32 and 20K for CIFAR10), classify them using the pre-trained classifier, and sort them based on top-1 predicted probability. Out of the top 20K images for Imagenet-32 and top 750 images for CIFAR10 upon sorting, we manually select images that we think most resemble the given predicted class. Figure A.6 further shows uncurated images on CIFAR10 and Imagenet-32 by the same JKO-iFlow model after training.

#### c.2.4 Conditional generation

To modify the JKO-iFlow to apply to the conditional generation task, we follow the framework in Xu et al. [2022], which trains a single flow mapping from \(X\) to \(H\) that pushes to match each component of the distribution associated with a distinct output label value \(Y=k\), \(k=1,\cdots,K\). Specifically, for a data-label pair \(\{X_{i},Y_{i}\}\), consider the continuous ODE trajectory \(x(t)\) starting from \(x(0)=X_{i}\), the per-sample training objective is by changing the term \(V(x(t_{k+1}))\) in (8) to be \(V_{Y_{i}}(x(t_{k+1}))\), where \(V_{k}(\cdot)\) is the potential of the Gaussian mixture component \(H|Y=k\). Because the Gaussian

Figure A.6: Uncurated generated samples of CIFAR10 and Imagenet-32 by JKO-iFlow in latent space.

Figure A.7: Unconditional and conditional generation on simulated toy datasets by JKO-iFlow. We color samples in (b) by the class label \(Y\) taking binary values. In both (a) and (b), the generated samples are close to the true samples in distribution. In (b), the pushforward distribution by JKO-iFlow, denoted as \(H|Y\), is also close to the target Gaussian mixture distribution \(H|Y\).

mixture is parametrized by mean vectors (the covariance is isotropic with fixed variance) (Xu et al., 2022), the expression of \(V_{k}(\cdot)\) is a quadratic function with explicit expression.

#### Training and test data:

* For the simulated two-moon data, we re-sample 5K training samples every epoch, for a total of 40 epochs per block. The batch size is 1000.
* The solar dataset is retrieved from the National Solar Radiation Database (NSRDB), following the data pre-processing in (Xu et al., 2022). The dataset has a 1K training samples and a 1K test samples. The batch size is 500, and each block is trained for 50 epochs.

#### Choice of \(h_{k}\):

For both datasets, \(h_{0}=1,\rho=1,h_{\max}=3\). The reparametrization and refinement techniques are not used.

#### MMD metric:

The MMD values are computed for each specific value of \(Y\) (across all graph nodes). When \(Y\) is fixed, we retrieve test samples from true data, denoted as \(\bm{X}|Y\), and generate model distribution samples, denoted as \(\bm{\tilde{X}}|Y\), and then compute the MMD values the same as in the unconditional case. Due to the relatively small sample size of \(\bm{X}|Y\) (679 and 144 respectively), we report the average values of MMD-m and threshold \(\tau\) over 50 replicas. In each replica, we subsample 90% observations of \(\bm{X}|Y\) and generate \(M=2000\) samples to form \(\bm{\tilde{X}}|Y\). For the results in Figure A.8, for the \(Y\) in plots (a)(b), \(N=612\) samples are randomly sampled from \(\bm{X}|Y\) in each replica, and the standard deviation of MMD-m values is less than the 2e-3 level. For the \(Y\) in plots (c)(d), \(N=130\) samples are randomly sampled from \(\bm{X}|Y\) in each replica, and the standard deviation of MMD-m values is less than the 5e-3 level. The median distance kernel bandwidth computed on the entire \(\bm{X}|Y\) is used in computing the MMD values.

#### Network and activation:

Regarding design of residual blocks,

* On two-moon data, we use fully-connected residual blocks with two hidden layers under 128 hidden nodes. The activation is Tanh. We train four residual blocks. The learning rate is 5e-3.
* On solar data, we follow the same design of residual blocks as (Xu et al., 2022). More precisely, each residual block contains one Chebnet (Defferrard et al., 2016) layer with degree 3, followed by two fully-connected layers with 64 hidden nodes. The activation is ELU. We train 11 residual blocks. The learning rate is 5e-3.