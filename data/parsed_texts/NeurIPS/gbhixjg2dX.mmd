# Synthetic Combinations:

A Causal Framework for Combinatorial Interventions

 Abhinneet Agarwal

Department of Statistics

University of California, Berkeley

aa3797@berkeley.edu

&Anish Agarwal

Department of IEOR

Columbia University

aa5194@columbia.edu

Work done while post-doc at Amazon, Core AI.

Suhas Vijaykumar

Amazon, Core AI

suhasv@mit.edu

Work done while at MIT.

###### Abstract

We consider a setting where there are \(N\) heterogeneous units and \(p\) interventions. Our goal is to learn unit-specific potential outcomes for any combination of these \(p\) interventions, i.e., \(N\times 2^{p}\) causal parameters. Choosing a combination of interventions is a problem that naturally arises in a variety of applications such as factorial design experiments and recommendation engines (e.g., showing a set of movies that maximizes engagement). Running \(N\times 2^{p}\) experiments to estimate the various parameters is likely expensive and/or infeasible as \(N\) and \(p\) grow. Further, with observational data there is likely confounding, i.e., whether or not a unit is seen under a combination is correlated with its potential outcome. We study this problem under a novel model that imposes latent structure across _both_ units and combinations of interventions. Specifically, we assume latent similarity in potential outcomes across units (i.e., the matrix of potential outcomes is rank \(r\)) and regularity in how combinations of interventions interact (i.e., the coefficients in the Fourier expansion of the potential outcomes is \(s\) sparse). We establish identification for all \(N\times 2^{p}\) parameters despite unobserved confounding. We propose an estimation procedure, Synthetic Combinations, and establish finite-sample consistency under precise conditions on the observation pattern. We show that Synthetic Combinations is able to consistently estimate unit-specific potential outcomes given a total of \(\text{poly}(r)\times\big{(}N\!+\!s^{2}p\big{)}\) observations. In comparison, previous methods that do not exploit structure across both units and combinations have poorer sample complexity that scales as \(\min(N\times s^{2}p,\ \text{poly}(r)\times(N+2^{p}))\).

## 1 Introduction

Modern-day decision makers--in settings from e-commerce to public policy to medicine--often must select a combination of actions, and would like to do so in a highly personalized manner. Examples include recommending a curated basket of items to customers on a commerce platform, deciding on a combination of therapies for a medical patient, enacting a collection of socio-economic policies for a specific geographic location, conjoint analysis in surveys, selecting important feature sets for machine learning models, etc. Despite the ubiquity of this setting, it comes with significant empirical challenges: with \(p\) interventions and \(N\) units, a decision maker must evaluate \(N\times 2^{p}\) potential combinations in order to confirm the optimal personalized policy. With large \(N\) and even with relatively small \(p\) (due to exponential dependence), it becomes infeasible to run that many experiments; in observational data there is the additional challenge of potential unobserved confounding. Current methods tackle this problem by following one of two approaches: (i) they impose structure on how combinations of interventions interact, or (ii) they assume latent similarityin potential outcomes across units. However, as we discuss in detail below, these approaches require a large number of observations to estimate all \(N\!\times\!2^{p}\) potential outcomes because they do not exploit structure across both units and combinations. This motivates the question: _how can one effectively share information across both units and combinations of interventions?_

**Contributions.** Our contributions may be summarized as follows. **(1)** For a unit \(n\!\in\![N]\), we represent its potential outcomes over the \(2^{p}\) combinations as a Boolean function from \(\{-1,1\}^{p}\) to \(\mathbb{R}\), expressed in the Fourier basis. To impose structure across combinations, we assume that for a unit \(n\), the linear coefficients \(\boldsymbol{\alpha}_{n}\!\in\!\mathbb{R}^{2^{p}}\) induced by this Fourier basis representation is \(s\)-sparse, i.e., has at most \(s\) non-zero entries. To impose structure across units, we assume that this matrix of Fourier coefficients across units \(\mathcal{A}\!=\![\boldsymbol{\alpha}_{n}]_{n\in[N]}\!\in\!\mathbb{R}^{N\times 2^ {p}}\) has rank \(r\). This simultaneous sparsity and low-rank assumption allows the researcher to share information across both units and combinations. **(2)** We establish identification for the \(N\!\times\!2^{p}\) potential outcomes of interest, which requires that any confounding is mediated by the (unobserved) matrix of Fourier coefficients \(\mathcal{A}\). **(3)** We design a two-step algorithm "Synthetic Combinations" and prove it consistently estimates the various causal parameters, despite potential unobserved confounding. The first step of Synthetic Combinations, termed "horizontal regression", learns the structure across combinations of interventions--assuming sparsity in the Fourier coefficients--via the Lasso. The second step, termed "vertical regression", learns the structure across units--assuming the matrix of Fourier coefficients are low-rank--via principal component regression (PCR). **(4)** We show that Synthetic Combinations is able to consistently estimate unit-specific potential outcomes given a total of \(\text{poly}(r)\!\times\!\big{(}N\!+\!s^{2}p\big{)}\) observations (ignoring logarithmic factors). This improves over previous methods that do not exploit structure across both units and combinations, which have sample complexity scaling as \(\min(N\!\times\!s^{2}p,\ \ \text{poly}(r)\!\times\!(N\!+\!2^{p}))\). A summary of the sample complexities required for different methods can be found in Table 1. A key technical challenge in our proofs is analyzing how the error induced in the first step of Synthetic Combinations percolates through to the second step. To tackle it, we reduce the problem to that of high-dimensional error-in-variables regression with linear model misspecification, and do a novel analysis of this statistical setting.

## 2 Related Work

**Learning over Combinations.** To place structure on the space of combinations, we use tools from the theory of learning (sparse) Boolean functions, in particular the Fourier transform. Sparsity of the Fourier transform was proposed as a complexity measure to characterize and design learning algorithms for low-depth trees, low-degree polynomials, and small circuits [12, 26]. Learning Boolean functions is now a central topic in learning theory, and is closely related to many important questions in ML more broadly; see e.g. [31] for discussion of the \(k\)-Junta problem and its relation to relevant feature selection. We refer to O'Donnell [34, Chapter 3] for further background on this area. In this paper, we focus on [32] which showed that Lasso can be used to efficiently learn sparse Boolean functions, an essential property in our setting since the dimension of the function class grows exponentially.

**Matrix Completion.** We build on the observation that imputing counterfactual outcomes in the presence of a latent factor structure can be equivalently expressed as low-rank matrix completion [9, 8, 5]. The observation that low-rank matrices may typically be recovered from a small fraction of the entries by nuclear-norm minimization has had a major impact on modern statistics [15, 35, 13]. In the noisy setting, proposed estimators have generally proceeded by minimizing risk subject to a nuclear-norm penalty, such as in the SoftImpute algorithm of [30], or minimizing risk subject to a rank constraint as in the hard singular-value thresholding (HSVT) algorithms analyzed by [27, 28, 23, 16]. We refer the reader to [19, 33] for a comprehensive overview of this vast literature.

\begin{table}
\begin{tabular}{|l|c|c|c|} \hline Learning algorithm & Exploits structure across combinations (\(\|\boldsymbol{\alpha}_{n}\|_{0}\!=\!s\)) & Exploits structure across units (rank\((\mathcal{A})\!=\!r\)) & Sample complexity \\ \hline Lasso & ✓ & ✗ & \(O\big{(}N\!\times\!s^{2}p\big{)}\) \\ \hline Matrix Completion & ✗ & ✓ & \(O\big{(}\text{poly}(r)\!\times\!(N\!+\!2^{p})\big{)}\) \\ \hline Synthetic Combinations & ✓ & ✓ & \(O\big{(}\text{poly}(r)\!\times\!(N\!+\!s^{2}p)\big{)}\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of sample complexity of Synthetic Combinations to other methods.

**Econometrics/causal inference.** There is a rich literature on how to learn personalized treatment effects for heterogeneous units. This problem is of particular importance in the social sciences and in medicine, where experimental data is limited, and has led to several promising approaches including instrumental variables, difference-in-differences, regression discontinuity, and others; see [7; 25] for an overview. Of particular interest to us here is the "synthetic control" method [1], which exploits an underlying factor structure to effectively "share" counterfactual information between treatment and control units. Building on [1], recent work has shown that the same underlying structure can be used to estimate treatment effects with multiple treatments _and_ heterogeneous units despite unmeasured confounding. The resulting framework, called "synthetic interventions" (SI), uses the factor representation to efficiently share information across similar (yet distinct) units and treatments [5]. We generalize the SI framework to settings where treatments are combinations of interventions and most treatments have no units that receive it, but there is structure across combinations of interventions. In doing so, we enable its use in highly practical cases where multiple interventions are delivered simultaneously, such as recommender systems, medical treatment regimens, and factorial design experiments.

## 3 Setup and Model

In this section, we first describe requisite notation, background on the Fourier expansion of real-valued functions over booleans, and how it relates to potential outcomes over combinations.

### Notation

**Representation of Combinations as Binary Vectors**. Let \([p]=\{1,...\,p\}\) denote the set of \(p\) interventions. Denote by \(\Pi\) the power set of \([p]\), i.e., the set of all possible combinations of \(p\) interventions, where we note \(|\Pi|=2^{p}\). Then, any given combination \(\pi\in\Pi\) induces the following binary representation \(\mathbf{v}(\pi)\in\{-1,1\}^{p}\) defined as follows: \(\mathbf{v}(\pi)_{i}=2\,\mathbf{1}\{i\in\pi\}-1\).

**Fourier Expansion of Boolean Functions.** Let \(\mathcal{F}_{\text{bool}}=\{f:\{-1,1\}^{p}\rightarrow\mathbb{R}\}\) be the set of all real-valued functions defined on the hypercube \(\{-1,1\}^{p}\). Then \(\mathcal{F}_{\text{bool}}\) forms a Hilbert space defined by the following inner product: for any \(f,g\in\mathcal{F}_{\text{bool}}\), \(\langle f,g\rangle_{B}=\frac{1}{2^{p}}\sum_{\mathbf{x}\in\{-1,1\}^{p}}f( \mathbf{x})g(\mathbf{x})\). This inner product induces the norm \(\langle f,f\rangle_{B}\coloneqq\|f\|_{B}^{2}=\frac{1}{2^{p}}\sum_{\mathbf{x} \in\{-1,1\}^{p}}f^{2}(\mathbf{x})\). We construct an orthonormal basis for \(\mathcal{F}_{\text{bool}}\) as follows: for each subset \(S\subset[p]\), define a basis function \(\chi_{S}(\mathbf{x})=\prod_{i\in S}x_{i}\) where \(x_{i}\) is the \(i^{\text{th}}\) coefficient of \(\mathbf{x}\in\{-1,1\}^{p}\). One can verify that for any \(S\subset[p]\) that \(\|\chi_{S}\|_{B}=1\), and that \(\langle\chi_{S},\chi_{S^{\prime}}\rangle_{B}=0\) for any \(S^{\prime}\neq S\). Since \(|\{\chi_{S}:S\subset[p]\}|=2^{p}\), the functions \(\chi_{S}\) are an orthonormal basis of \(\mathcal{F}_{\text{bool}}\).

Hence, any \(f\in\mathcal{F}_{\text{bool}}\) can be expressed via the following "Fourier" decomposition: \(f(\mathbf{x})=\sum_{S\subset[p]}\,\alpha_{S}\chi_{S}(\mathbf{x}),\) where the Fourier coefficient \(\alpha_{S}\) is given by computing \(\alpha_{S}=\langle f,\chi_{S}\rangle_{B}\). We will refer to \(\chi_{S}\) as the Fourier character. Define \(\boldsymbol{\alpha}_{f}=[\alpha_{S}]_{S\in[p]}\in\mathbb{R}^{2^{p}}\) and \(\boldsymbol{\chi}(x)=\lfloor\chi_{S}(\mathbf{x})\rfloor_{S\in[p]}\in\{-1,1\} ^{2^{p}}\) as the vector of Fourier coefficients and characters respectively. Hence any function \(f\dvtx\{-1,1\}^{p}\rightarrow\mathbb{R}\) can be re-expressed as follows: \(f(\mathbf{x})=\langle\boldsymbol{\alpha}_{f},\boldsymbol{\chi}(x)\rangle\). For \(\pi\in\Pi\), abbreviate \(\chi_{S}(\mathbf{v}(\pi))\) and \(\boldsymbol{\chi}(\mathbf{v}(\pi))\) as \(\chi_{S}^{\pi}\) and \(\boldsymbol{\chi}^{\pi}\) respectively.

**Observed and potential outcomes.** Let \(Y_{n}^{(\pi)}\in\mathbb{R}\) denote the _potential outcome_ for unit \(n\) under combination \(\pi\) and \(Y_{n\pi}\in\{\mathbb{R}\cup\star\}\) as the _observed outcome_, where \(\star\) indicates a missing value, i.e., the outcome associated with the unit-combination pair \((n,\pi)\) was not observed. Let \(\mathbf{Y}=[Y_{n\pi}]\in\{\mathbb{R}\cup\star\}^{N\times 2^{p}}\). Let \(\mathcal{D}\subset[N]\times[2^{p}]\), refer to the subset of unit-combination pairs we do observe, i.e.,

\[Y_{n\pi}=\begin{cases}Y_{n}^{(\pi)},&\text{if }(n,\pi)\in\mathcal{D}\\ \star,&\text{otherwise}.\end{cases} \tag{1}\]

Note that (1) implies stable unit treatment value assignment (SUTVA) holds. Let \(\Pi_{S}\subseteq\Pi\) denote a subset of combinations. For a given unit \(n\), let \(\mathbf{Y}_{\Pi_{S},n}=[Y_{n\pi_{i}}:\pi_{i}\in\Pi_{S}]\in\{\mathbb{R}\cup\star \}^{|\Pi_{S}|}\) represent the vector of observed outcomes for all \(\pi\in\Pi_{S}\). Similarly, let \(\mathbf{Y}_{n}^{(\Pi_{S})}=[Y_{n}^{(\pi_{i})}:\pi_{i}\in\Pi_{S}]\in\mathbb{R}^{| \Pi_{S}|}\) represent the vector of potential outcomes. Denote \(\boldsymbol{\chi}(\Pi_{S})=[\boldsymbol{\chi}^{\pi_{i}}:\pi_{i}\in\Pi_{S}] \in\{-1,1\}^{|\Pi_{S}|\times 2^{p}}\).

### Model & Target Causal Parameter

Define \(Y_{n}^{(\cdot)}\!:\!\pi\!\rightarrow\!\mathbb{R}\) as a real-valued function over the hypercube \(\{-1,1\}^{p}\) associated with unit \(n\). It takes as input a combination \(\pi\), converts it to a \(p\)-dimensional binary vector \(\mathbf{v}(\pi)\), and outputs a real number \(Y_{n}^{(\pi)}\). Given the discussion in Section 3.1, it follows that \(Y_{n}^{(\pi)}\) always has the representation \(\langle\boldsymbol{\alpha}_{n},\boldsymbol{\chi}^{\pi}\rangle\) for some \(\boldsymbol{\alpha}_{n}\!\in\!\mathbb{R}^{2^{p}}\). Thus, without any loss of generality, the \(\boldsymbol{\alpha}_{n}\) are unit-specific latent variables (Fourier coefficients) encoding the treatment response function. Below, we state our key assumption on these induced Fourier coefficients.

**Assumption 3.1** (Potential Outcome Model).: _For any unit-combination pair \((n,\pi)\), we assume it has the following representation,_

\[Y_{n}^{(\pi)}\!=\!\langle\boldsymbol{\alpha}_{n},\boldsymbol{\chi}^{\pi} \rangle\!+\!\epsilon_{n}^{\pi}, \tag{2}\]

_where \(\boldsymbol{\alpha}_{n}\!\in\!\mathbb{R}^{2^{p}}\) and \(\boldsymbol{\chi}^{\pi}\!\in\!\{-1,1\}^{2^{p}}\) are the Fourier coefficients and characters, respectively. We assume the following properties: (a) low-rank: the matrix \(\mathcal{A}\!=\![\boldsymbol{\alpha}_{n}]_{n\in[N]}\) has rank \(r\!\in\![\min\{N,\!2^{p}\}]\); (b) sparsity: \(\boldsymbol{\alpha}_{n}\) is \(s\)-sparse (i.e. \(\|\boldsymbol{\alpha}_{n}\|_{0}\!\leq\!s\), where \(s\!\in\![2^{p}]\)) for every unit \(n\!\in\![N]\); (c) \(\epsilon_{n}^{\pi}\) is a residual term specific to \((n,\!\pi)\) which satisfies \(\mathbb{E}[\epsilon_{n}^{\pi}\mid\mathcal{A}]\!=\!0\)._

The assumption is then that each \(\boldsymbol{\alpha}_{n}\)\(s\)-sparse and \(\mathcal{A}\) is rank-\(r\); \(\epsilon_{n}^{\pi}\) is the residual from this sparse and low-rank approximation, and it serves as the source of uncertainty in our model. Given \(\mathbb{E}[\epsilon_{n}^{\pi}\mid\mathcal{A}]\!=\!0\), the matrix \(\mathbb{E}[\mathbf{Y}_{N}^{(\Pi)}]\!=\![\mathbb{E}[\mathbf{Y}_{n}^{(\Pi)}]\!:\! n\!\in\![N]]\!\in\!\mathbb{R}^{2^{p}\times N}\) also is rank \(r\), where the expectation is defined with respect to \(\epsilon_{n}^{\pi}\). This is because \(\mathbb{E}[\mathbf{Y}_{N}^{(\Pi)}]\) can be written as \(\mathbb{E}[\mathbf{Y}_{N}^{(\Pi)}]\!=\!\boldsymbol{\chi}(\Pi)\mathcal{A}^{T}\), and since \(\boldsymbol{\chi}(\Pi)\) is an invertible matrix, \(\text{rank}(\mathbb{E}[\mathbf{Y}_{N}^{(\Pi)}])\!=\!\text{rank}(\mathcal{A})\). The low-rank property places _structure across units_; that is, we assume there is sufficient similarity across units so that \(\mathbb{E}[\mathbf{Y}_{n}^{(\Pi)}]\) for any unit \(n\) can be written as a linear combination of \(r\) other rows of \(\mathbb{E}[\mathbf{Y}_{N}^{(\Pi)}]\). This is a standard assumption used to encode latent similarity across units in matrix completion and its related applications (e.g., recommendation engines).

Sparsity establishes _unit-specific_ structure; that is, we assume that the potential outcomes for a given user only depend on a small subset of the functions \(\{\chi_{S}\!:\!S\!\subset\![p]\}\). We emphasize that this subset of functions can be different across units. As discussed in Section 2, sparsity is commonly employed when studying the learnability of Boolean functions. In the context of recommendation engines, sparsity is implied if the ratings for a set of goods only depend on a small number of combinations of items within that set. Sparsity is also often assumed implicitly in factorial design experiments, where analysts typically only include pairwise interaction effects between interventions and ignore higher-order interactions [24]. We discuss further applications of combinatorial inference in greater detail in Appendix A.

Next, we present an assumption that formalizes the dependence (i.e., confounding) between the missingness pattern induced by the treatment assignments \(\mathcal{D}\) and the potential outcomes \(Y_{n}^{(\pi)}\), and provide an interpretation of the induced data generating process (DGP) for potential outcomes.

**Assumption 3.2** (Selection on Fourier coefficients).: _For all \(n\!\in\![N]\) and \(\pi\!\in\!\Pi\), \(Y_{n}^{(\pi)}\!\perp\!\mathcal{D}\!\mid\!\mathcal{A}\)._

**DGP.** Given Assumptions 3.1 and Assumption 3.2, the DGP can be summarized as follows: (i) unit-specific latent Fourier coefficients \(\mathcal{A}\) are either deterministic or sampled from an unknown distribution; we will condition on this quantity throughout. (ii) Given \(\mathcal{A}\), we sample mean-zero random variables \(\epsilon_{n}^{\pi}\), and generate potential outcomes according to our model \(Y_{n}^{(\pi)}\!=\!\langle\boldsymbol{\alpha}_{n},\!\chi^{\pi}\rangle\!+\! \epsilon_{n}^{\pi}\). (iii) \(\mathcal{D}\) is allowed to depend on unit-specific latent Fourier coefficients \(\mathcal{A}\) (i.e. \(\mathcal{D}\!=\!f(\mathcal{A})\)). We define all expectations w.r.t. noise, \(\epsilon_{n}^{\pi}\). This DGP introduces unobserved confounding since \(Y_{n}^{(\pi)}\!\not\perp\!\mathcal{D}\). However, this DGP does imply that Assumption 3.2 holds, i.e., conditional on the Fourier coefficients \(\mathcal{A}\), the potential outcomes are independent of the treatment assignments \(\mathcal{D}\). This conditional independence condition can be thought of as "selection on latent Fourier coefficients", which generalizes the widely made assumption of "selection on observables." The latter requires that potential outcomes are independent of treatment assignments conditional on _observed_ covariates-we reemphasize that \(\mathcal{A}\) is unobserved.

**Target parameter.** For any unit-combination pair \((n,\!\pi)\), we aim to estimate \(\mathbb{E}[Y_{n}^{(\pi)}\!\mid\!\mathcal{A}]\), where the expectation is w.r.t. \(\epsilon_{n}^{\pi}\), and we condition on the set of Fourier coefficients \(\mathcal{A}\).

## 4 Identification of Potential Outcomes

We show that \(\mathbb{E}[Y_{n}^{(\pi)}\mid\mathcal{A}]\) can be written as a function of observed outcomes, i.e., we establish identification of our target causal parameter. As discussed earlier, our model allows for _unobserved confounding_: whether or not a unit is seen under a combination may be correlated with its potential outcome under that combination due to unobserved factors, as long as certain conditions are met. We introduce necessary notation and assumption required for our result. For a unit \(n\in[N]\), denote the subset of combinations we observe them under as \(\Pi_{n}\subseteq\Pi\). For \(\pi\in\Pi\), let \(\tilde{\chi}_{n}^{\pi}\in\mathbb{R}^{2^{p}}\) denote the vector where we zero out all coordinates of \(\boldsymbol{\chi}^{\pi}\in\mathbb{R}^{2^{p}}\) that correspond to the coefficients of \(\boldsymbol{\alpha}_{n}\) which are zero. For example, if \(\boldsymbol{\alpha}_{n}=(1,1,0,0,...0)\) and \(\boldsymbol{\chi}^{\pi}=(1,1,...1)\), then \(\tilde{\chi}_{n}^{\pi}=(1,1,0,...0)\). We the make the following assumption.

**Assumption 4.1** (Donor Units).: _We assume there exists a set of "donor units" \(\mathcal{I}\subset[N]\), such that the following two conditions hold:_

1. _Horizontal span inclusion: For any donor unit_ \(u\in\mathcal{I}\) _and combination_ \(\pi\in\Pi\)_, suppose_ \(\tilde{\boldsymbol{\chi}}_{u}^{\pi}\in span(\tilde{\boldsymbol{\chi}}_{u}^{ \pi_{i}}:\pi_{i}\in\Pi_{u})\)_. That is, there exists exists_ \(\beta_{\Pi_{u}}^{\pi}\in\mathbb{R}^{|\Pi_{u}|}\) _such that_ \(\tilde{\boldsymbol{\chi}}_{u}^{\pi}=\sum_{\pi_{i}\in\Pi_{u}}\beta_{\pi_{i}}^{ \pi}\tilde{\boldsymbol{\chi}}_{u}^{\pi_{i}}\)_._
2. _Linear span inclusion: For any unit_ \(n\in[N]\setminus\mathcal{I}\)_, suppose_ \(\boldsymbol{\alpha}_{n}\in span(\boldsymbol{\alpha}_{u}:u\in\mathcal{I})\)_. That is, there exists_ \(\mathbf{w}^{n}\) _such that_ \(\boldsymbol{\alpha}_{n}=\sum_{u\in\mathcal{I}}w_{u}^{n}\boldsymbol{\alpha}_{u}\)__

Horizontal span inclusion requires that the set of observed combinations for any donor unit is "diverse" enough that the projection of the Fourier characteristic for a target intervention is in the span of characteristics of observed interventions. Linear span inclusion requires that the donor set is diverse enough such that the Fourier coefficient of any unit is in the span of the Fourier coefficients of the donor set.

### Identification Result

Given these assumptions, we now present our identification theorem.

**Theorem 4.2**.: _Let Assumptions 3.1, 3.2, 4.1 hold. Given \(\beta_{\Pi_{u}}^{\pi}\) and \(\mathbf{w}_{u}^{n}\) defined in Assumption 4.1, we have_

_(a) Donor units: For_ \(u\in\mathcal{I}\)_, and_ \(\pi\in\Pi\)_,_ \(\mathbb{E}[Y_{u}^{(\pi)}\mid\mathcal{A}]=\sum_{\pi_{u}\in\Pi_{u}}\beta_{\pi_{u} }^{\pi}\mathbb{E}[Y_{u,\pi_{u}}\mid\mathcal{A},\mathcal{D}]\)_._

_(b) Non-donor units: For_ \(n\in[N]\setminus\mathcal{I}\)_, and_ \(\pi\notin\Pi_{n}\)_,_ \(\mathbb{E}[Y_{n}^{(\pi)}\mid\mathcal{A}]=\sum_{u\in\mathcal{I},\pi_{u}\in\Pi_{ u}}w_{u}^{n}\beta_{\pi_{u}}^{\pi}\mathbb{E}[Y_{u,\pi_{u}}\mid\mathcal{A}, \mathcal{D}]\)_._

Theorem 4.2 gives conditions under which the donor set \(\mathcal{I}\) and the observation pattern \(\mathcal{D}\) are sufficient to recover the full set of unit specific potential outcomes \(\mathbb{E}[Y_{u}^{(\pi)}|\mathcal{A}]\) in the noise-free limit. Part (a) establishes that for every donor unit \(u\in\mathcal{I}\), the causal estimand can be written as a function of its _own_ observed outcomes \(\mathbb{E}[\mathbf{Y}_{\Pi_{u}}]\), given knowledge of \(\beta_{\Pi_{u}}^{\pi}\). Part (b) states that the target causal estimand \(\mathbb{E}[Y_{n}^{(\pi)}]\) for a non-donor unit and combination \(\pi\) can be written as a linear combination of the outcomes of the donor set \(\mathcal{I}\), given knowledge of \(\mathbf{w}_{n}^{n}\). Previous works that establish identification under a latent factor model requires a growing number of donor units to be observed under all treatments [6]. This is infeasible in our setting because _the vast majority of combinations have no units that receive it_. As a result, we have to first to identify the outcomes of donor units under all combinations (part (a)), before transferring them to non-donor units (part (b)). In order to do so, Theorem 4.2 suggests that the key quantities in estimating \(\mathbb{E}[Y_{n}^{(\pi)}]\) for any unit-combination pair \((n,\pi)\) are \(\beta_{\Pi_{u}}^{\pi}\) and \(\mathbf{w}_{u}^{n}\). In the following section, we provide an algorithm to estimate both \(\beta_{\Pi_{u}}^{\pi}\) and \(\mathbf{w}_{u}^{n}\), as well as concrete ways of determining the donor set \(\mathcal{I}\).

## 5 The Synthetic Combinations Estimator

We now describe the Synthetic Combinations estimator, a simple and flexible two-step procedure for estimating our target causal parameter. See Figure 1 for a pictorial representation.

**Step 1: Horizontal Regression.** We denote the vector of observed responses \(\mathbf{Y}_{n,\Pi_{n}}=[Y_{n\pi}:\pi\in\Pi_{n}]\in\mathbf{R}^{|\Pi_{n}|}\) for any unit \(n\) as \(\mathbf{Y}_{\Pi_{n}}\). Then, for every unit \(u\) in the donor set \(\mathcal{I}\), we estimate \(\mathbb{E}[Y_{u}^{(\pi)}]\)via the Lasso, i.e., by solving the following convex program with penalty parameter \(\lambda_{u}\):

\[\hat{\mathbf{\alpha}}_{u}\!=\!\operatorname*{argmin}_{\mathbf{\alpha}}\frac{1}{|\Pi_{u}| }\|\mathbf{Y}_{\Pi_{u}}\!-\!\mathbf{\chi}(\Pi_{u})\mathbf{\alpha}\|_{2}^{2}\!+\!\lambda _{u}\|\mathbf{\alpha}\|_{1} \tag{3}\]

where recall that \(\mathbf{\chi}(\Pi_{u})=[\mathbf{\chi}^{\pi}:\pi\in\Pi_{u}]\in\mathbb{R}^{|\Pi_{u}|\times 2 ^{p}}\). Then, for any donor unit-combination pair \((u,\pi)\), let \(\hat{\mathbb{E}}[Y_{u}^{(\pi)}]\!=\!\langle\hat{\mathbf{\alpha}}_{u}\mathbf{\chi}^{\pi}\rangle\) denote the estimate of the potential outcome \(\mathbb{E}[Y_{u}^{(\pi)}]\).

**Step 2: Vertical Regression.** Next, we estimate potential outcomes for all units \(n\!\in\![N]\setminus\mathcal{I}\). To do so, we define some required notation. For \(\Pi_{S}\!\subseteq\!\Pi\), define the vector of estimated potential outcomes \(\hat{\mathbb{E}}[\mathbf{Y}_{u}^{(\Pi_{S})}]\!=\![\hat{\mathbb{E}}[Y_{u}^{(\pi )}]\!:\!\pi\!\in\!\Pi^{S}]\!\in\!\mathbb{R}^{|\Pi_{S}|}\). Additionally, let \(\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{S})}]\!=\![\hat{\mathbb{E}}[ \mathbf{Y}_{u}^{(\Pi_{S})}]\!:\!u\!\in\!\mathcal{I}]\!\in\!R^{|\Pi_{S}|\times| \mathcal{I}|}\).

_Step 2(a): Principal Component Regression._ Perform a singular value decomposition (SVD) of \(\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\) to get \(\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\!=\!\sum_{l=1}^{\min(| \Pi_{n}|,|\mathcal{I}|)}\hat{s}_{l}\hat{\mathbf{\mu}}_{l}\hat{\mathbf{\nu}}_{l}^{T}\). Using a hyper-parameter \(\kappa\!\leq\!\min(|\Pi_{n}|,|\mathcal{I}|)\)3, compute \(\hat{\mathbf{w}}^{n}\!\in\!\mathbb{R}^{|\mathcal{I}|}\) as follows:

Footnote 3: Both \(\lambda\) and \(\kappa\) can be chosen in a data-driven manner (e.g., via CV) as discussed in [17] and [4] respectively.

\[\hat{\mathbf{w}}^{n}\!=\!\left(\sum_{l=1}^{\kappa}\!\hat{s}_{l}^{-1}\hat{\mathbf{ \nu}}_{l}\hat{\mathbf{\mu}}_{l}^{T}\right)\mathbf{Y}_{\Pi_{n}} \tag{4}\]

_Step 2(b): Estimation._ Using \(\hat{\mathbf{w}}^{n}\!=\![\hat{w}_{u}^{n}\!:\!u\!\in\!\mathcal{I}]\), we have the following estimate for any \(\pi\!\in\!\Pi\)

\[\hat{\mathbb{E}}[Y_{n}^{(\pi)}]\!=\!\sum_{u\in\mathcal{I}}\!\hat{w}_{u}^{n} \hat{\mathbb{E}}[Y_{u}^{(\pi)}] \tag{5}\]

**Suitability of Lasso and PCR.** Lasso is appropriate for the horizontal regression because \(\mathbf{\alpha}_{u}\) is sparse. However, Synthetic Combinations allows for any ML algorithm (e.g., neural networks, random forests) to be used in the first step. This flexibility allows an analyst to tailor the horizontal learning procedure, and include prior information. However, we leave this model-agnostic analysis as future work, and focus on the Lasso for simplicity. PCR is appropriate for the vertical regression because \(\mathcal{A}\) is low rank. As [3; 4] show, PCR implicitly regularizes the regression by adapting to the rank of the covariates (\(\mathbf{Y}_{\Pi_{n}}\)), i.e., the out-of-sample error of PCR scales with \(r\) rather than the ambient covariate dimension.

**Determining Donor Set \(\mathcal{I}\).** Synthetic Combinations requires the existence of a subset of units \(\mathcal{I}\!\subset\![N]\) such that we are able to (i) accurately estimate their potential outcomes under all possible combinations, and (ii) transfer these estimated outcomes to a unit \(n\!\in\![N]\setminus\mathcal{I}\). Theoretically, we detail sufficient conditions on the observation pattern such that we are able to perform (i) and (ii) accurately via the Lasso and PCR respectively. In practice, we recommend the following to determine \(\mathcal{I}\). For every unit \(n\!\in\![N]\), learn a separate Lasso model \(\mathbf{\alpha}_{n}\) and assess its performance through cross-validation (CV). Assign units with low CV error (with a pre-determined threshold) as the donor set \(\mathcal{I}\)

Figure 1: (a) depicts an example of a particular observation pattern with outcome for unit-combination pair \((n,\pi)\) missing. (b) demonstrates horizontal regression for donor unit \(u\) where we estimate potential outcome \(\mathbb{E}[Y_{u}^{(\pi)}]\). (c) visualizes vertical regression where we transfer estimated outcomes from the donor set \(\mathcal{I}\) to \((n,\pi)\).

and estimate outcomes \(\hat{\mathbb{E}}[Y_{u}^{(\pi)}]\) for every unit \(u\in\mathcal{I}\) and \(\pi\in\Pi\). For non-donor units, PCR performance can also be assessed via k-fold CV. For units with low PCR error, linear span inclusion (Assumption 4.1(b)) and the assumptions required for the generalization for PCR likely hold, and hence we estimate their potential outcomes as in (5). For units with large PCR error, it is either unlikely that these set of assumptions holds or that \(|\Pi_{n}|\) is not large enough (i.e., additional experiments need to be run for this unit), and hence we do not recommend estimating their counterfactuals.

## 6 Synthetic Combinations Theoretical Analysis

In this section, we establish finite-sample consistency of Synthetic Combinations, starting with a discussion of the additional assumptions required for our results.

### Additional Assumptions

**Assumption 6.1** (Bounded Potential Outcomes).: _We assume that \(\mathbb{E}[Y_{n}^{(\pi)}]\in[-1,1]\) for any unit-combination pair \((n,\pi)\)._

**Assumption 6.2** (Sub-Gaussian Noise).: _Conditioned on \(\mathcal{A}\), for any unit-combination pair \((n,\pi)\), \(\epsilon_{n}^{\pi}\) are independent zero-mean sub-Gaussian random variables with \(\text{Var}[\epsilon_{n}^{\pi}\mid\mathcal{A}]\leq\sigma^{2}\) and \(\|\epsilon_{n}^{\pi}\mid\mathcal{A}\|_{\psi_{2}}\leq C\sigma\) for some constant \(C>0\)._

**Assumption 6.3** (Incoherence of Donor Fourier characteristics).: _For every unit \(u\in\mathcal{I}\), assume \(\mathbf{\chi}(\Pi_{u})\) satisfies incoherence: \(\left\|\frac{\mathbf{\chi}(\Pi_{u})^{T}\mathbf{\chi}(\Pi_{u})}{|\Pi_{u}|}-\mathbf{I}_ {2p}\right\|_{\infty}\leq\frac{C^{\prime}}{s}\), for a universal constant \(C^{\prime}>0\)._

To define our next set of assumptions, we introduce necessary notation. For any subset of combinations \(\Pi_{S}\subset\Pi\), let \(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{S})}]=[\mathbb{E}[\mathbf{Y}_{u}^{( \Pi_{S})}]\dvtx u\in\mathcal{I}]\in\mathbb{R}^{|\Pi_{S}|\times|\mathcal{I}|}\).

**Assumption 6.4** (Donor Unit Balanced Spectrum).: _For a given unit \(n\in[N]\setminus\mathcal{I}\), let \(r_{n}\) and \(s_{1}...s_{r_{n}}\) denote the rank and non-zero singular values of \(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}\mid\mathcal{A}]\). We assume that the singular values are well-balanced, i.e., for universal constants \(c,c^{\prime}>0\), we have that \(s_{r_{n}}/s_{1}\geq c\), and \(\|\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}\mid\mathcal{A}]\|_{F}^{2} \geq c^{\prime}|\Pi_{n}||\mathcal{I}|\)._

**Assumption 6.5** (Subspace Inclusion).: _For a given unit \(n\in[N]\setminus\mathcal{I}\) and intervention \(\pi\in\Pi\setminus\Pi_{n}\), assume that \(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}]\) lies within the row-span of \(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\)_

Assumption 6.3 is necessary for finite-sample consistency when estimating \(\mathbf{\alpha}_{n}\) via the Lasso estimator (3), and is commonly made when studying the Lasso [36]. Incoherence can also seen as a inclusion criteria for a unit \(n\) to be included in the donor set \(\mathcal{I}\). Assumption 6.3 for example holds (with high probability) if \(\Pi_{u}\) is chosen uniformly at random and grows as \(\omega(s^{2}p)\) as shown in Lemma 2 of [32]. Assumption 6.4 requires that the non-zero singular values of \(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}\mid\mathcal{A}]\) are well-balanced. This assumption is standard when studying PCR [3; 5], and within the econometrics literature [10; 22]. It can also be empirically validated by plotting the spectrum of \(\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\); if the singular spectrum of \(\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\) displays a natural elbow point, then Assumption 6.4 is likely. Assumption 6.5 is also commonly made when analyzing PCR [4; 5; 2]. It can be thought of as a "causal transportability" condition from the model learnt using \(\Pi_{n}\) to the interventions \(\pi\in\Pi\setminus\Pi_{n}\). That is, subspace inclusion allows us to generalize well, and accurately estimate \(\mathbb{E}[Y_{n}^{(\pi)}\mid\mathcal{A}]\) using \(\langle\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}],\hat{\mathbf{w} }^{n}\rangle\).

### Finite Sample Consistency

The following result establishes finite-sample consistency of Synthetic Combinations. Without loss of generality, we will focus on estimating the pair of quantities \((\mathbb{E}[Y_{u}^{(\pi)}],\mathbb{E}[Y_{n}^{(\pi)}])\) for a given donor unit \(u\in\mathcal{I}\), and non-donor unit \(n\in[N]\setminus\mathcal{I}\) under treatment assignment \(\pi\in\Pi\). To simplify notation, we will use \(O_{p}\) notation: for any sequence of random vectors \(X_{n}\), \(X_{n}\) = \(O_{p}(\gamma_{n})\) if, for any \(\epsilon>0\), there exists constants \(c_{\epsilon}\) and \(n_{\epsilon}\) such that \(\mathbb{P}(\|X_{n}\|_{2}\geq c_{\epsilon}\gamma_{n})\leq\epsilon\) for every \(n\geq n_{\epsilon}\). Similarly, we define \(\tilde{O_{p}}(\gamma_{n})\) which suppresses logarithmic terms. We will further absorb dependencies on \(\sigma\) into \(\tilde{O_{p}}(\cdot)\).

**Theorem 6.6** (Finite Sample Consistency of Synthetic Combinations).: _Conditioned on \(\mathcal{A}\), let Assumptions 3.1-4.1, and 6.1-6.5 hold. Then, the following statements hold._1. _For the given donor unit-combination pair_ \((u,\pi)\)_, let the Lasso regularization parameter satisfy_ \(\lambda_{u}\!=\!\Omega(\sqrt{\frac{p}{|\Pi_{u}|}})\)_. Then, we have that:_ \(|\hat{\mathbb{E}}[Y_{u}^{(\pi)}]\!-\!\mathbb{E}[Y_{u}^{(\pi)}]|\!=\!\tilde{O}_{p }\!\left(\sqrt{\frac{s^{2}p}{|\Pi_{u}|}}\right)\)_._
2. _For the given unit-combination pair_ \((n,\pi)\) _where_ \(n\!\in\![N]\setminus\!\mathcal{I}\)_, let_ \(\kappa\!=\!\text{rank}(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{u})}])\!:=\!r _{n}\)_. Then, provided that_ \(\min_{u\in\mathcal{I}}\!|\Pi_{u}|\!:=\!M\!=\!\omega(r_{n}^{2}s^{2}p)\)_, we have that:_ \[\left|\hat{\mathbb{E}}[Y_{n}^{(\pi)}]\!-\!\mathbb{E}[Y_{n}^{(\pi)}]\right|\!= \!\tilde{O}_{p}\!\left(\frac{r_{n}^{2}\sqrt{s^{2}p}}{\sqrt{M\!\times\!\min\{| \Pi_{n}|,|\mathcal{I}|\}}}\!+\!\frac{r_{n}^{2}s^{2}p\sqrt{|\mathcal{I}|}}{M}\! +\!\frac{r_{n}}{|\Pi_{n}|^{1/4}}\right)\]

Establishing Theorem 6.6 requires a novel analysis of error-in-variables (EIV) linear regression. Specifically, the general EIV linear model is as follows: \(Y=\mathbf{X}\beta+\epsilon\), \(\mathbf{Z}=\mathbf{X}+\mathbf{H}\), where \(Y\) and \(\mathbf{Z}\) are observed. In our case, \(Y=\mathbf{Y}_{\Pi_{n}}\), \(\mathbf{X}=\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\), \(\beta=\mathbf{w}^{n}\), \(\mathbf{Z}=\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\), and \(\mathbf{H}\) is the error arising in estimating \(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\) via the Lasso. Typically one assumes that \(\mathbf{H}\) is is a matrix of independent sub-gaussian noise. Our analysis requires a novel worst-case analysis of \(\mathbf{H}\) (due to the 2-step regression of Lasso and then PCR), in which each entry of is \(\mathbf{H}\) simply bounded.

Next, we describe the conditions placed on \(\Pi_{n}\), \(M\), \(\mathcal{I}\) to achieve consistent estimation of \((\mathbb{E}[Y_{u}^{(\pi)}],\!\mathbb{E}[Y_{n}^{(\pi)}])\). To simplify the discussion, we assume that \(\min\{|\Pi_{n}|,|\mathcal{I}|\}\!=\!|\mathcal{I}|\). This condition can be enforced in practice by simply picking a subset of donor units such that \(|\mathcal{I}|\leq|\Pi_{n}|\) when performing PCR (i.e., step 2 of Synthetic Combinations). Given this assumption, it can be verified that \(|\Pi_{n}|\) and \(M\) need to scale as \(\omega(r_{n}^{4})\) and \(\omega(r_{n}^{4}s^{2}p)\) respectively to achieve \(\max\!\left(|\hat{\mathbb{E}}[Y_{u}^{(\pi)}]\!-\!\mathbb{E}[Y_{u}^{(\pi)}]|, |\hat{\mathbb{E}}[Y_{n}^{(\pi)}]\!-\!\mathbb{E}[Y_{n}^{(\pi)}]|\right)\!=\! \tilde{o}_{p}(1)\). Next, we present a corollary that discusses how quickly the parameters \(r_{n}\), \(s\), and \(p\) can grow with the number of observations \(|\Pi_{n}|\) and \(M\).

**Corollary 6.7**.: _With the set-up of Theorem 6.6, if (a) \(r_{n}=o(|\Pi_{n}|^{1/4})\) and (b) \(s=o\!\left(\sqrt{M/(p\!\max\{|\Pi_{n}|,|\mathcal{I}|\})}\right)\) then \(\max\!\left(|\hat{\mathbb{E}}[Y_{u}^{(\pi)}]\!-\!\mathbb{E}[Y_{u}^{(\pi)}]|,| \hat{\mathbb{E}}[Y_{n}^{(\pi)}]\!-\!\mathbb{E}[Y_{n}^{(\pi)}]|\right)=\tilde{o} _{p}(1)\) as \(M\),\(|\Pi_{n}|\),\(|\mathcal{I}|\!\rightarrow\!\infty\)._

Corollary 6.7 quantifies how \(s\),\(r_{n}\) can scale with the number of observations to achieve consistency. That is, Corollary 6.7 reflects the maximum "complexity" allowed for a given sample size.

### Sample Complexity

We discuss the sample complexity of Synthetic Combinations to estimate all \(N\!\times\!2^{p}\) causal parameters, and compare it to that of other methods. To ease our discussion, we will ignore dependence on logarithmic factors and \(\sigma\). Even if potential outcomes \(Y_{n}^{(\pi)}\) were observed for all unit-combination pairs, consistently estimating \(\mathbb{E}[\mathbf{Y}_{N}^{(\Pi)}]\) is not trivial. This is because, we only get to observe a single and noisy version \(Y_{n\pi}\!=\!(\boldsymbol{\alpha}_{n},\!\mathcal{X}^{\pi})\!+\!\epsilon_{n}^{\pi}\). Hypothetically, if we observe \(K\) independent samples of \(Y_{n\pi}\) for a given \((n,\!\pi)\), denoted by \(Y_{n\pi}^{1}\),...,\(Y_{n\pi}^{K}\), the maximum likelihood estimator would be the empirical average \(\frac{1}{K}\sum_{i=1}^{K}Y_{n\pi}^{i}\). The empirical average would concentrate around \(\mathbb{E}[Y_{n}^{(\pi)}]\) at a rate \(O(1/\sqrt{K})\) and hence would require \(K\!=\!\Omega(\delta^{-2})\) samples to estimate \(\mathbb{E}[Y_{n}^{(\pi)}]\) within error \(O(\delta)\). Therefore, this naive (unimplementable) solution would require \(N\!\times\!2^{p}\!\times\!\delta^{-2}\) observations to estimate \(\mathbb{E}[\mathbf{Y}_{N}^{(\Pi)}]\).

On the other hand, Synthetic Combinations produces consistent estimates of the potential outcome despite being given _at most only a single noisy sample_ of each potential outcome. As the discussion after Theorem 6.6 shows, if \(|\mathcal{I}|\!\leq\!|\Pi_{n}|\), Synthetic Combinations requires \(|\mathcal{I}|\!\times\!r^{4}s^{2}p/\delta^{2}\) observations for the donor set, and \((N-|\mathcal{I}|)\!\times\!r^{4}/\delta^{4}\) observations for the non-donor units to achieve an estimation error of \(O(\delta)\) for all \(N\!\times\!2^{p}\) causal parameters. Hence, we have that the number of observations required to achieve an estimation error of \(O(\delta)\) for all pairs \((n,\!\pi)\) scales as \(O\!\left(\text{poly}(r/\delta)\times(N\!+\!s^{2}p)\right)\).

**Sample Complexity Comparison to Other Methods.**_Horizontal regression:_ An alternative algorithm would be to learn an individual model for each unit \(n\in[N]\). That is, run a separate horizontal regression via the Lasso for every unit. This alternative algorithm has sample complexity that scales at least as \(O(N\!\times\!s^{2}p/\delta^{2})\) rather than \(O\!\left(\text{poly}(r)/\delta^{4}\!\times\!\left(N\!+\!s^{2}p\right)\right)\) required by Synthetic Combinations. It suffers because it does not utilize any structure across units (i.e., the low-rank property of \(\mathcal{A}\)), whereas Synthetic Combinations captures the similarity between units via PCR.

_Matrix completion:_ Synthetic Combinations can be thought of as a matrix completion method; estimating \(\mathbb{E}[Y_{n}^{(\pi)}]\) is equivalent to imputing \((n,\!\pi)\)-th entry of the observation matrix \(\mathbf{Y}\!\in\!\{\mathbb{R}\cup\star\}^{N\times 2^{p}}\), where recall \(\star\) denotes an unobserved unit-combination outcome. Under the low-rank property (Assumption 3.1(b)) and various models of missingness (i.e., observation patterns), recent works on matrix completion [14, 29, 6] (see Section 2 for an overview) have established that estimating \(\mathbb{E}[Y_{n}^{(\pi)}]\) to an accuracy \(O(\delta)\) requires at least \(O(\text{poly}(r/\delta)\!\times\!(N\!+\!2^{p}))\) samples. This is because matrix completion techniques do not leverage the sparsity of \(\mathbf{\alpha}_{n}\). Moreover, matrix completion results typically report error in the Frobenius norm, whereas we give entry-wise guarantees. This leads to an extra factor of \(s\) in our analysis as it requires proving convergence for \(\|\hat{\mathbf{\alpha}_{n}}-\mathbf{\alpha}_{n}\|_{1}\) rather than \(\|\hat{\mathbf{\alpha}_{n}}-\mathbf{\alpha}_{n}\|_{2}\). Hence, Synthetic Combinations combines the best of both approaches by leveraging the structure of the potential outcomes _and_ the similarity across units.

**Natural Lower Bound on Sample Complexity.** We provide an informal discussion on the lower bound sample-complexity to estimate all \(N\times 2^{p}\) potential outcomes. As established in Lemma E.1, \(\mathcal{A}\) has at most \(rs\) non-zero columns. Counting the parameters in the singular value decomposition of \(\mathcal{A}\), only \(r\!\times\!(N\!+\!rs)\) free parameters are required to be estimated. Hence, a natural lower bound on the sample complexity scales as \(O(Nr\!+\!r^{2}s)\). Hence, Synthetic Combinations is only sub-optimal by a factor (ignoring logarithmic factors) of \(sp\) and \(\text{poly}(r)\). As discussed earlier, an additional factor of \(s\) can be removed if we focus on deriving Frobenius norm error bounds. It remains as interesting future work to derive estimation procedures that are able to achieve this lower bound.

## 7 Conclusion

In this work, we formulate a causal inference framework for combinatorial interventions, a setting that is ubiquitous in practice. We propose a model that imposes both unit-specific structure, and latent similarity across units. Under this model, we propose an estimation procedure, Synthetic Combinations, that exploits the sparsity and low-rankness of the Fourier coefficients to efficiently estimate all \(N\!\times\!2^{p}\) causal parameters. We formally establish finite-sample consistency of Synthetic Combinations in an observational setting. Our work also naturally suggests future directions for research such as extending Synthetic Combinations to permutations over items (i.e., rankings), or providing an analysis of Synthetic Combinations that is agnostic to the horizontal regression algorithm used. A related line of work to the question above is also deriving estimation algorithms that can achieve the sample complexity lower bound discussed in Section 6.3.

## 8 Acknowledgements

We thank Alberto Abadie, Peng Ding, Giles Hooker, Devavrat Shah, Vasilis Syrgkanis, and Bin Yu for useful discussions and feedback.

## References

* [1] A. Abadie, A. Diamond, and J. Hainmueller. Synthetic control methods for comparative case studies: Estimating the effect of california's tobacco control program. _Journal of the American statistical Association_, 105(490):493-505, 2010.
* [2] A. Agarwal and R. Singh. Causal inference with corrupted data: Measurement error, missing values, discretization, and differential privacy. _arXiv preprint arXiv:2107.02780_, 2021.
* [3] A. Agarwal, D. Shah, D. Shen, and D. Song. On robustness of principal component regression. _Advances in Neural Information Processing Systems_, 32, 2019.
* [4] A. Agarwal, D. Shah, and D. Shen. On principal component regression in a high-dimensional error-in-variables setting. _arXiv preprint arXiv:2010.14449_, 2020.
* [5] A. Agarwal, D. Shah, and D. Shen. Synthetic interventions. _arXiv preprint arXiv:2006.07691_, 2020.
* [6] A. Agarwal, M. Dahleh, D. Shah, and D. Shen. Causal matrix completion. _arXiv preprint arXiv:2109.15154_, 2021.

* [7] J. D. Angrist, G. W. Imbens, and D. B. Rubin. Identification of causal effects using instrumental variables. _Journal of the American statistical Association_, 91(434):444-455, 1996.
* [8] S. Athey, M. Bayati, N. Doudchenko, G. Imbens, and K. Khosravi. Matrix completion methods for causal panel data models. _Journal of the American Statistical Association_, pages 1-41, 2021.
* [9] J. Bai and S. Ng. Matrix completion, counterfactuals, and factor analysis of missing data. _arXiv preprint arXiv:1910.06677_, 2019.
* [10] J. Bai and S. Ng. Matrix completion, counterfactuals, and factor analysis of missing data. _Journal of the American Statistical Association_, 116(536):1746-1763, 2021.
* [11] M. Bertrand and S. Mullainathan. Are emily and greg more employable than lakisha and jamal? a field experiment on labor market discrimination. _American economic review_, 94(4):991-1013, 2004.
* [12] Y. Brandman, A. Orlitsky, and J. Hennessy. A spectral lower bound technique for the size of decision trees and two-level and/or circuits. _IEEE Transactions on Computers_, 39(2):282-287, 1990.
* [13] E. Candes and B. Recht. Exact matrix completion via convex optimization. _Communications of the ACM_, 55(6):111-119, 2012.
* [14] E. J. Candes and Y. Plan. Matrix completion with noise. _Proceedings of the IEEE_, 98(6):925-936, 2010.
* [15] E. J. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix completion. _IEEE Transactions on Information Theory_, 56(5):2053-2080, 2010.
* [16] S. Chatterjee. Matrix estimation by universal singular value thresholding. 2015.
* [17] D. Chetverikov, Z. Liao, and V. Chernozhukov. On cross-validated lasso in high dimensions. _The Annals of Statistics_, 49(3):1300-1317, 2021.
* [18] T. Dasgupta, N. S. Pillai, and D. B. Rubin. Causal inference from 2 k factorial designs by using potential outcomes. _Journal of the Royal Statistical Society: Series B: Statistical Methodology_, pages 727-753, 2015.
* [19] M. A. Davenport and J. Romberg. An overview of low-rank matrix recovery from incomplete observations. _IEEE Journal of Selected Topics in Signal Processing_, 10(4):608-622, 2016. doi: 10.1109/JSTSP.2016.2539100.
* [20] E. Duflo, R. Glennerster, and M. Kremer. Using randomization in development economics research: A toolkit. _Handbook of development economics_, 4:3895-3962, 2007.
* [21] S. Eriksson and D.-O. Rooth. Do employers use unemployment as a sorting criterion when hiring? evidence from a field experiment. _American economic review_, 104(3):1014-1039, 2014.
* [22] J. Fan, W. Wang, and Y. Zhong. An l\({}_{\infty}\) eigenvector perturbation bound and its application to robust covariance estimation. _Journal of Machine Learning Research_, 18(207):1-42, 2018.
* [23] M. Gavish and D. L. Donoho. The optimal hard threshold for singular values is \(4/\sqrt{3}\). _IEEE Transactions on Information Theory_, 60(8):5040-5053, 2014.
* [24] E. George, W. G. Hunter, and J. S. Hunter. _Statistics for experimenters: design, innovation, and discovery_. Wiley, 2005.
* [25] G. W. Imbens and D. B. Rubin. _Causal inference in statistics, social, and biomedical sciences_. Cambridge University Press, 2015.
* [26] M. Karpovsky. _Finite Orthogonal Series in Design of Digital Devices_. John Wiley & Sons, Inc., Hoboken, NJ, USA, 1976. ISBN 0471731889.
* [27] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. _Advances in neural information processing systems_, 22, 2009.

* [28] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. _IEEE transactions on information theory_, 56(6):2980-2998, 2010.
* [29] W. Ma and G. H. Chen. Missing not at random in matrix completion: The effectiveness of estimating missingness probabilities under a low nuclear norm assumption. _Advances in neural information processing systems_, 32, 2019.
* [30] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. _The Journal of Machine Learning Research_, 11:2287-2322, 2010.
* [31] E. Mossel, R. O'Donnell, and R. P. Servedio. Learning juntas. In _Proceedings of the thirty-fifth annual ACM symposium on Theory of computing_, pages 206-212, 2003.
* [32] S. Negahban and D. Shah. Learning sparse boolean polynomials. In _2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 2032-2036. IEEE, 2012.
* [33] L. T. Nguyen, J. Kim, and B. Shim. Low-rank matrix completion: A contemporary survey. _IEEE Access_, 7:94215-94237, 2019.
* [34] R. O'Donnell. Some topics in analysis of boolean functions. In _Proceedings of the fortieth annual ACM symposium on Theory of computing_, pages 569-578, 2008.
* [35] B. Recht. A simpler approach to matrix completion. _Journal of Machine Learning Research_, 12(12), 2011.
* [36] P. Rigollet and J.-C. Hutter. High dimensional statistics. _Lecture notes for course 18S997_, 813 (814):46, 2015.
* [37] P.-A. Wedin. Perturbation bounds in connection with singular value decomposition. _BIT Numerical Mathematics_, 12(1):99-111, 1972.
* [38] C. J. Wu and M. S. Hamada. _Experiments: planning, analysis, and optimization_. John Wiley & Sons, 2011.
* [39] A. Zhao and P. Ding. Regression-based causal inference with factorial experiments: estimands, model specifications and design-based properties. _Biometrika_, 109(3):799-815, 2022.

Combinatorial Inference Applications

In this section, we discuss how classical models and applications could relate to our proposed potential outcome model. In particular, we discuss two well-studied models for functions over combinations: low-degree Boolean polynomials and \(k\)-Juntas. We also discuss applications such as factorial design experiments and recommendation systems.

**Low-degree Boolean Polynomials.** A special instance of sparse Boolean functions is a low-degree polynomial which we define as follows. For a positive integer \(d\!\leq\!p\), \(f\!:\!\{\!-\!1,\!1\!\}^{p}\!\rightarrow\!\mathbb{R}\) is a \(d\)-degree polynomial if its Fourier transform satisfies the following for any input \(\mathbf{x}\!\in\!\{\!-\!1,\!1\!\}^{p}\)

\[f(\mathbf{x})\!=\sum_{S\subset\left\lfloor p\right\rfloor,\left\lvert S \right\rvert\leq d}\alpha_{S}\chi_{S}(\mathbf{x}). \tag{6}\]

In this setting, we see that \(s\!\leq\!\sum_{i=0}^{d}\binom{p}{i}\!\approx\!p^{d}\). That is, degree \(d\)-polynomials impose sparsity on the potential outcome by limiting the degree of interaction between interventions.

**Applications of low-degree Boolean polynomials.** We discuss two applications when the potential outcomes can be modeled as a low-degree Boolean polynomial.

_Factorial Design Experiments._ Factorial design experiments consist of \(p\) treatments where each treatment arm can take on a discrete set of values, and units are assigned different combinations of these treatments. In the special case that each treatment arm only takes on two possible values, this experimental design mechanism is referred to as a \(2^{p}\) factorial experiment. Factorial design experiments are widely employed in the social sciences, agricultural and industrial applications [20, 18, 38]. A common strategy to determine treatment effects in this setting is to assume that the potential outcome only depends on main effects and pairwise interactions with higher-order interactions being negligible [11, 21, 24]. That is, analysts implicitly enforce sparsity of \(\boldsymbol{\alpha}_{n}\) by setting \(d\!=\!2\). We refer the reader to [39] for a detailed discussion of various estimation strategies and their validity in different settings. The model we propose (see Assumption 3.1) captures these various modeling choices commonly used in factorial design experiments by imposing sparsity on \(\boldsymbol{\alpha}_{n}\). Further, as we show later, Synthetic Combinations can adapt to low-degree polynomials without pre-specifying the degree, \(d\), i.e., it automatically adapts to the inherent level of interactions in the data. The additional assumption we make however is that there is structure across units, i.e., the matrix \(\mathcal{A}\!=\![\boldsymbol{\alpha}_{n}]_{n\in[N]}\) is low-rank.

_Optimal Team Selection._ Another use for combinatorial inference is to find the optimal configuration for a team. For example, a soccer team can be interested in finding the configuration of positions (i.e., defensive, midfield, offensive players) that leads to the highest winning percentage. Here, units \(n\) are different teams, and combinations \(\pi\) represent possible configurations of the team. The potential outcomes \(\mathbb{E}[Y_{n}^{(\pi)}]\) represent the winning percentage for a given team \(n\) under a particular configuration \(\pi\), with the coefficients of \(\boldsymbol{\alpha}_{n}\) representing the effect of different configurations on the win rate. In this setting, enforcing sparsity of \(\boldsymbol{\alpha}_{n}\) via low-degree polynomials may be appropriate because the number of wins may depend on all the players, however only interactions between players of the same type matter while higher-order interactions between players of different types can be negligible. Returning to the example of a soccer team, the win rate can depend strongly on the synergy of the offensive players, whereas interactions between offensive and defensive players have a smaller effect. Further, since different teams might play in similar ways and can have the same optimal configuration, one potential way to capture this structure across teams is by placing a low-rank structure on \(\mathcal{A}\).

\(k\)**-Juntas.** Another special case of sparse Boolean functions are \(k\)-Juntas which only depend on \(k\!<\!p\) input variables. More formally, a function \(f\!:\!\{\!-\!1,\!1\!\}^{p}\!\rightarrow\!\mathbb{R}\) is a \(k\)-junta if there exists a set \(K\!\subset\![p]\) with \(\left\lvert K\right\rvert\!=\!k\) such that the fourier expansion of \(f\) can be represented as follows

\[f(\mathbf{x})\!=\!\sum_{S\subset K}\alpha_{S}\chi_{S}(\mathbf{x}) \tag{7}\]

Therefore, in the setting of the \(k\)-Junta, the sparsity index \(s\leq 2^{k}\). In contrast to low-degree polynomials where the function depends on all \(p\) variables but limits the degree of interaction between variables, \(k\)-Juntas only depend on \(k\) variables but allow for arbitrary interactions amongst them.

**Applications of \(k\)-Juntas.** We discuss two applications when the potential outcomes can be modeled as a \(k\)-Junta.

_Recommendation Systems._ Recommendation platforms such as _Netflix_ are often interested in recommending a combination of movies that maximizes a user's engagement with a platform. Here, units \(n\) can be individual users, and \(\pi\) represents combinations of different movies. The potential outcomes \(\mathbb{E}[Y_{n}^{(\pi)}]\) are the engagement levels for a unit \(n\) when presented with a combination of movies \(\pi\), with \(\boldsymbol{\alpha}_{n}\) representing the preferences for that user. In this setting, a user's engagement with the platform may only depend on a small subset of movies. For example, a user who is only interested in fantasy will only remain on the platform if they are recommended movies such as _Harry Potter_. Under this behavioral model, the potential outcomes (i.e., engagement levels) can be modeled as a \(k\)-Junta with the non-zero coefficients of \(\boldsymbol{\alpha}_{n}\) representing the combinations of the \(k\) movies that affect engagement level for a user \(n\). Our potential outcome observational model (Assumption 3.1) captures this form of sparsity while also reflecting the low-rank structure commonly assumed when studying recommendation systems. Once again, our results show Synthetic Combinations can adapt to \(k-\)Juntas without pre-specifying the subset of features \(K\), i.e., it automatically learns the important features for a given user.

_Knock-down Experiments in Genomics._ A key task in genomics is to identify which set of genes are responsible for a phenotype (i.e., physical trait of interest such as blood pressure) in a given individual. To do so, geneticists use knock-down experiments which measure the difference in the phenotype after eliminating the effect of a set of genes in an individual. To encode this process in the language of combinatorial causal inference, we can think of units \(n\) as different individuals, an action as knocking a particular gene out, and \(\pi\) as a combination of genes that are knocked out. The potential outcomes \(\mathbb{E}[Y_{n}^{(\pi)}]\) is the expression of the phenotype for a unit \(n\) when the combination of genes \(\pi\) are eliminated via knock-down experiments, and the coefficients of \(\boldsymbol{\alpha}_{n}\) represent the effect of different combination of genes on the phenotype. A typical assumption in genomics is that phenotypes only depend on a small set of genes and their interactions. In this setting, we can model this form of sparsity by thinking of the potential outcome function as a \(k\)-junta, as well as capturing the similarity between the effect of genes on different individuals via our low-rank assumption.

## Appendix B Proof of Theorem 4.2

Proof.: Below, the symbol \(\overset{\Delta X}{=}\) and \(\overset{DX}{=}\) imply that the equality follows from Assumption \(X\) and Definition \(X\), respectively. We begin with the proof of Theorem 4.2 (a). _Proof of Theorem 4.2 (a):_ For a donor unit \(u\!\in\!\mathcal{I}\) and \(\pi\!\in\!\Pi\!\setminus\!\Pi_{u}\), we have

\[\mathbb{E}[Y_{u}^{(\pi)}\mid\mathcal{A}] \overset{A3.1}{=}\mathbb{E}[\langle\boldsymbol{\alpha}_{u}, \boldsymbol{\chi}^{\pi}\rangle\!+\!\epsilon_{u}^{\pi}\mid\mathcal{A}]\] \[\overset{A3.1(c)}{=}\langle\boldsymbol{\alpha}_{u},\!\boldsymbol{ \chi}^{\pi}\rangle\mid\mathcal{A},\mathcal{D}\] \[\overset{A4.1(a)}{=}\langle\boldsymbol{\alpha}_{u},\!\sum_{\pi_{u }\in\Pi_{u}}\beta_{\pi_{u}}^{\pi}\boldsymbol{\tilde{X}}_{u}^{\pi_{u}}\rangle \mid\mathcal{A},\mathcal{D}\] \[=\!\sum_{\pi_{u}\in\Pi_{u}}\!\beta_{\pi_{u}}^{\pi}\langle \boldsymbol{\alpha}_{u},\!\boldsymbol{\tilde{X}}_{u}^{\pi_{u}}\rangle\mid \mathcal{A},\mathcal{D}\] \[\overset{A3.1(c)}{=}\!\sum_{\pi_{u}\in\Pi_{u}}\!\beta_{\pi_{u}}^ {\pi}\mathbb{E}[\langle\boldsymbol{\alpha}_{u},\!\boldsymbol{\tilde{X}}_{u}^{ \pi_{u}}\rangle\!+\!\epsilon_{u}^{\pi_{u}}\mid\mathcal{A},\mathcal{D}]\] \[\overset{A3.1}{=}\!\sum_{\pi_{u}\in\Pi_{u}}\!\beta_{\pi_{u}}^{ \pi}\mathbb{E}[Y_{u,\pi_{u}}\mid\mathcal{A},\mathcal{D}]\]Proof of Theorem 4.2 (b):.: For a donor unit \(n\in[N]\setminus I^{D}\) and \(\pi\in\Pi\setminus\Pi_{n}\), we have

\[\mathbb{E}[Y_{n}^{(\pi)}\mid\mathcal{A}] \stackrel{{ A3.1}}{{=}}\mathbb{E}[\langle\mathbf{\alpha}_ {n},\mathbf{\chi}^{\pi}\rangle+\epsilon_{n}^{\pi}\mid\mathcal{A}]\] \[\stackrel{{ A3.1(c)}}{{=}}\langle\mathbf{\alpha}_{n},\mathbf{ \chi}^{\pi}\rangle\mid\mathcal{A}\] \[=\langle\mathbf{\alpha}_{n},\mathbf{\chi}^{\pi}\rangle\mid\mathcal{A}, \mathcal{D}\] \[\stackrel{{ A4.1(b)}}{{=}}\langle\sum_{u\in\mathcal{I }}w_{u}^{n}\mathbf{\alpha}_{u},\mathbf{\chi}^{\pi}\rangle\mid\mathcal{A},\mathcal{D}\] \[=\!\sum_{u\in\mathcal{I}}\!\!w_{u}^{n}\langle\mathbf{\alpha}_{u},\mathbf{ \chi}^{\pi}\rangle\mid\mathcal{A},\mathcal{D}\] \[\stackrel{{ A3.1(c)}}{{=}}\!\sum_{u\in\mathcal{I}}w_ {u}^{n}\mathbb{E}[\langle\mathbf{\alpha}_{u},\mathbf{\chi}^{\pi}\rangle+\epsilon_{u}^ {\pi}\mid\mathcal{A},\mathcal{D}]\] \[\stackrel{{ A3.1}}{{=}}\sum_{u\in\mathcal{I}}w_{u}^{ n}\mathbb{E}[Y_{u}^{(\pi)}\mid\mathcal{A},\mathcal{D}]\] \[=\!\sum_{u\in\mathcal{I}\pi_{u}\in\Pi_{u}}\!\!w_{u}^{n}\beta_{\pi_ {u}}^{\pi}\mathbb{E}[Y_{u,\pi_{u}}\mid\mathcal{A},\mathcal{D}]\]

where the last equality follows from Theorem 4.2 (a). 

## Appendix C Proof of Theorem 6.6

### Proof of Theorem 6.6 (a)

We have that

\[\hat{\mathbb{E}}[Y_{u}^{(\pi)}]-\mathbb{E}[Y_{u}^{(\pi)}] =\langle\hat{\mathbf{\alpha}}^{u},\mathbf{\chi}^{\pi}\rangle-\langle\mathbf{ \alpha}_{u},\mathbf{\chi}^{\pi}\rangle\] \[=\langle\hat{\mathbf{\alpha}}_{u}-\mathbf{\alpha}_{u},\mathbf{\chi}^{\pi}\rangle\] \[\leq\|\hat{\mathbf{\alpha}}_{u}-\mathbf{\alpha}_{u}\|_{1}\|\mathbf{\chi}^{\pi}\| _{\infty}\] \[=\|\hat{\mathbf{\alpha}}_{u}-\mathbf{\alpha}_{u}\|_{1} \tag{8}\]

To finish the proof, we quote the following Theorem which we adapt to our notation.

**Theorem C.1** (Theorem 2.18 in [36]).: _Fix the number of samples \(n\geq 2\). Assume that the linear model \(Y=\mathbf{X}\theta^{*}+\epsilon\), where \(\mathbf{X}\in\mathbb{R}^{n\times d}\) and \(\epsilon\) is a sub-gaussian random variable with noise variance \(\sigma^{2}\). Moreover, assume that \(\|\theta^{*}\|_{0}\leq k\), and that \(\mathbf{X}\) satisfies the incoherence condition (Assumption 6.3) with parameter \(k\). Then, the lasso estimator \(\hat{\theta}^{L}\) with regularization parameter defined by_

\[2\tau =8\sigma\sqrt{\log(2d)/n}+8\sigma\sqrt{\log(1/\delta)/n}\]

_satisfies_

\[\|\theta^{*}-\hat{\theta}^{L}\|_{2}^{2}\leq k\sigma^{2}\frac{\log( 2d/\delta)}{n} \tag{9}\]

_with probability at least \(1-\delta\)._

Further, as in established in the proof of Theorem 2.18 in [36], \(\|\theta^{*}-\hat{\theta}^{L}\|_{1}\leq\sqrt{k}\|\theta^{*}-\hat{\theta}^{L}\|_ {2}\). Note that the set-up of Theorem C.1 holds in our setting with the following notational changes: \(Y=\mathbf{Y}_{\Pi_{u}}\), \(\mathbf{X}=\mathbf{\chi}(\Pi_{u})\), \(\theta^{*}=\mathbf{\alpha}_{u}\), \(\hat{\theta}^{L}=\hat{\mathbf{\alpha}}_{u}\), \(k=s\) as well as our assumptions on the regularization parameter \(\lambda_{u}\) and that \(\mathbf{\epsilon}_{u}^{\pi}\) is sub-gaussian (Assumption 6.2). Applying Theorem C.1 gives us

\[\|\hat{\mathbf{\alpha}}_{u}-\mathbf{\alpha}_{u}\|_{1}=O_{p}\left(\sqrt{ \frac{s^{2}p}{|\Pi_{u}|}}\right)\]

Substituting this bound into (8) yields the claimed result.

### Proof of Theorem 6.6 (b)

For any matrix \(\mathbf{A}\) with orthonormal columns, let \(\mathcal{P}_{A}=\mathbf{A}\mathbf{A}^{T}\) denote the projection matrix on the subspace spanned by the columns of \(\mathbf{A}\). Define \(\tilde{\mathbf{w}}^{n}=\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_{n})}}\mathbf{w}^{n}\), where \(\mathbf{V}_{\mathcal{I}}^{(\Pi_{n})}\) are the right singular vectors of \(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\). Let \(\Delta_{w}^{n}=\tilde{\mathbf{w}}^{n}-\tilde{\mathbf{w}}^{n}\in\mathbb{R}^{| \mathcal{I}|}\), and \(\Delta_{\mathcal{I}}^{\pi}=\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}]- \mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}]\in\mathbb{R}^{|\mathcal{I}|}\). Denote \(\Delta_{E}=\max_{u\in\mathcal{I},\pi\in\Pi}\bigl{|}\hat{\mathbb{E}}[Y_{u}^{( \pi)}]-\mathbb{E}[Y_{u}^{(\pi)}]\bigr{|}\). In order to proceed, we first state the following result,

**Lemma C.2**.: _Let the set-up of Theorem 6.6 hold. Then, we have_

\[\mathbb{E}[Y_{n}^{(\pi)}]=\langle\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}],\tilde{\mathbf{w}}^{n}\rangle\]

Using Lemma C.2, and the notation established above, we have

\[\bigl{|}\hat{\mathbb{E}}[Y_{n}^{(\pi)}]-\mathbb{E}[Y_{n}^{(\pi)}] \bigr{|}= \bigl{|}\langle\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}],\hat{\mathbf{w}}^{n}\rangle-\langle\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}],\tilde{\mathbf{w}}^{n}\rangle\bigr{|}\] \[\leq \bigl{|}\langle\Delta_{\mathcal{I}}^{\pi},\tilde{\mathbf{w}}^{n} \rangle\bigr{|}+\bigl{|}\langle\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}], \Delta_{w}^{n}\rangle\bigr{|}+\bigl{|}\langle\Delta_{w}^{n},\Delta_{\mathcal{I} }^{\pi}\rangle\bigr{|} \tag{10}\]

From Assumption 6.5, it follows that \(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}]=\mathcal{P}_{V_{\mathcal{I}}^{( \Pi_{n})}}\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}]\), where \(\mathbf{V}_{\mathcal{I}}^{(\Pi_{n})}\) are the right singular vectors of \(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\). Using this in (10) gives us

\[\bigl{|}\hat{\mathbb{E}}[Y_{n}^{(\pi)}]-\mathbb{E}[Y_{n}^{(\pi)}]\bigr{|}\leq \bigl{|}\langle\Delta_{\mathcal{I}}^{\pi},\tilde{\mathbf{w}}^{n}\rangle\bigr{|} +\Bigl{|}\langle\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}],\mathcal{P}_{V_{ \mathcal{I}}^{(\Pi_{n})}}\Delta_{w}^{n}\rangle\Bigr{|}+\bigl{|}\langle\Delta_{ w}^{n},\Delta_{\mathcal{I}}^{\pi}\rangle\bigr{|} \tag{11}\]

Below, we bound the three terms on the right-hand-side of (11) separately. Before we bound each term, we state a Lemma that will be useful to help us establish the results.

**Lemma C.3**.: _Let Assumptions 3.1, 4.1, 6.1, and 6.4 hold. Then, \(\|\tilde{\mathbf{w}}^{n}\|_{2}\lesssim\sqrt{\frac{r_{n}}{|\mathcal{I}|}}\)_

Bounding Term 1.: By Holder's inequality and Lemma C.3, we have that

\[|\langle\Delta_{\mathcal{I}}^{\pi},\tilde{\mathbf{w}}^{n}\rangle|\leq\|\tilde{ \mathbf{w}}^{n}\|_{1}\|\Delta_{\mathcal{I}}^{\pi}\|_{\infty}\leq\|\tilde{ \mathbf{w}}^{n}\|_{1}\Delta_{E}\leq\sqrt{|\mathcal{I}|}\|\tilde{\mathbf{w}}^{n} \|_{2}\Delta_{E}\lesssim\sqrt{r_{n}}\Delta_{E} \tag{12}\]

This concludes the analysis for the first term.

Bounding Term 2.: By Cauchy-Schwarz and Assumption 6.1 we have,

\[\bigl{|}\langle\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}], \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_{n})}}\Delta_{w}^{n}\rangle\bigr{|}\leq \|\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}]\|_{2}\|\mathcal{P}_{V_{ \mathcal{I}}^{(\Pi_{n})}}\Delta_{w}^{n}\|_{2}\] \[\leq \sqrt{|\mathcal{I}|}\|\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_{n})}} \Delta_{w}^{n}\|_{2} \tag{13}\]

We now state a lemma that will help us conclude our bound of Term 2. The proof is given in Appendix D.3.

**Lemma C.4**.: _Let the set-up of Theorem 6.6 hold. Then,_

\[\|\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_{n})}}\Delta_{w}^{n}\|_{2}\] \[= \tilde{O}_{p}\left(r_{n}^{2}\Biggl{[}\frac{\Delta_{E}}{\sqrt{| \mathcal{I}|}\min\{\sqrt{|\Pi_{n}|},\sqrt{|\mathcal{I}|}\}}+\Delta_{E}^{2} \Biggr{]}+\frac{r_{n}^{3/2}\Delta_{E}}{\sqrt{|\mathcal{I}|}}+\frac{r_{n} \Bigl{(}1+\sqrt{\Delta_{E}}r_{n}^{1/4}\Bigr{)}}{\sqrt{|\mathcal{I}|}\|\Pi_{n }|^{1/4}}\right) \tag{14}\]

Incorporating the result of the lemma above into (13) gives us

\[\bigl{|}\langle\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}], \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_{n})}}\Delta_{w}^{n}\rangle\bigr{|}\] \[= \tilde{O}_{p}\left(r_{n}^{2}\Biggl{[}\frac{\Delta_{E}}{\min\{ \sqrt{|\Pi_{n}|},\sqrt{|\mathcal{I}|}\}}+\sqrt{|\mathcal{I}|}\Delta_{E}^{2} \Biggr{]}+r_{n}^{3/2}\Delta_{E}+\frac{r_{n}\Bigl{(}1+\sqrt{\Delta_{E}}r_{n}^{1/ 4}\Bigr{)}}{|\Pi_{n}|^{1/4}}\right) \tag{15}\]Bounding Term 3.By Holder's inequality, we have that

\[|\langle\Delta_{w}^{n},\Delta_{\mathcal{I}}^{\pi}\rangle| \leq \|\Delta_{w}^{n}\|_{2}\|\Delta_{\mathcal{I}}^{\pi}\|_{2} \tag{16}\] \[\leq \sqrt{|\mathcal{I}|}\|\Delta_{w}^{n}\|_{2}\|\Delta_{\mathcal{I}} ^{\pi}\|_{\infty}\] \[\leq \sqrt{|\mathcal{I}|}\Delta_{E}\|\Delta_{w}^{n}\|_{2}\]

We now state a proposition that will help us conclude our proof of Term 3. The proof is given in Appendix D.4.

**Proposition C.5**.: _Let the set-up of Theorem 6.6 hold. Then, conditioned on \(\mathcal{A}\), we have_

\[\hat{\mathbf{w}}^{n}-\tilde{\mathbf{w}}^{n}= \tilde{O}_{p}\Bigg{(}r_{n}\Bigg{[}\frac{\|\tilde{\mathbf{w}}^{n}\| _{2}}{\min\{\sqrt{|\Pi_{n}|},\sqrt{|\mathcal{I}|}\}}+\Delta_{E}\Bigg{]}\Bigg{)} \tag{18}\]

As a result of Proposition C.5 and Lemma C.3, we have the following bound for Term 3,

\[|\langle\Delta_{w}^{n},\Delta_{\mathcal{I}}^{\pi}\rangle|= \tilde{O}_{p}\Bigg{(}r_{n}^{3/2}\Bigg{[}\frac{\Delta_{E}}{\min\{\sqrt{|\Pi_{n }|},\sqrt{|\mathcal{I}|}\}}+\sqrt{|\mathcal{I}|}\Delta_{E}^{2}\Bigg{]}\Bigg{)} \tag{19}\]

Collecting Terms.Combining equations (12), (15), (19) gives us

\[\Big{|}\hat{\mathbb{E}}[Y_{n}^{(\pi)}]-\mathbb{E}[Y_{n}^{(\pi)}] \Big{|} \tag{20}\] \[= \tilde{O}_{p}\Bigg{(}r_{n}^{2}\Bigg{[}\frac{\Delta_{E}}{\min\{ \sqrt{|\Pi_{n}|},\sqrt{|\mathcal{I}|}\}}+\sqrt{|\mathcal{I}|}\Delta_{E}^{2} \Bigg{]}+r_{n}^{3/2}\Delta_{E}+\frac{r_{n}\Big{(}1+\sqrt{\Delta_{E}}r_{n}^{1/4 }\Big{)}}{|\Pi_{n}|^{1/4}}\Bigg{)}\]

By Theorem 6.6 (a), we have that

\[\Delta_{E}= \max_{u\in\mathcal{I}}O_{p}\Bigg{(}\sqrt{\frac{s^{2}p}{|\Pi_{u}|}} \Bigg{)}= O_{p}\Bigg{(}\sqrt{\frac{s^{2}p}{M}}\Bigg{)} \tag{21}\]

where we remind the reader that \(M=\min_{u\in\mathcal{I}}|\Pi_{u}|\). Substituting (21), and our assumption that \(M=\omega(r_{n}^{2}s^{2}p)\) into (20), we get

\[\Big{|}\hat{\mathbb{E}}[Y_{n}^{(\pi)}]-\mathbb{E}[Y_{n}^{(\pi)}] \Big{|}= \tilde{O}_{p}\Bigg{(}\frac{r_{n}^{2}\sqrt{s^{2}p}}{\sqrt{M\times\min\{|\Pi_{ n}|,|\mathcal{I}|\}}}+\frac{r_{n}^{2}s^{2}p\sqrt{|\mathcal{I}|}}{M}+\frac{r_{n}}{| \Pi_{n}|^{1/4}}\Bigg{)} \tag{22}\]

## Appendix D Proofs of Helper Lemmas for Theorem 6.6.

In this section we provide proofs of Lemmas C.2, C.3, C.4, and Proposition C.5 which were required for the Proof of Theorem 6.6.

### Proof of Lemma C.2

By Assumption 3.1 and 4.1 (b), we have

\[\mathbb{E}[Y_{n}^{(\pi)}] = \mathbb{E}[\langle\mathbf{\alpha}_{n},\mathbf{\chi}^{\pi}\rangle+\epsilon _{n}^{\pi}]\] \[= \mathbb{E}[\langle\mathbf{\alpha}_{n},\mathbf{\chi}^{\pi}\rangle]\] \[= \sum_{u\in\mathcal{I}}w_{n}^{n}\mathbb{E}[\langle\mathbf{\alpha}_{u},\mathbf{\chi}^{\pi}\rangle]\] \[= \sum_{u\in\mathcal{I}}w_{n}^{n}\mathbb{E}[Y_{u}^{(\pi)}]\] \[= \mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}]^{T}\mathbf{w}^{n}\]

From Assumption 6.5, we have that \(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\pi)}]= \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_{n})}}\mathbb{E}[\mathbf{Y}_{\mathcal{I}} ^{(\pi)}]\). Substituting this into the equation above completes the proof.

### Proof of Lemma C.3

For simplicity, denote \(\mathbb{E}[\mathbf{Y}_{n}^{(\Pi_{n})}]=\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\). By definition, \(\tilde{\mathbf{w}}^{n}\) is the solution to the following optimization program

\[\min_{\mathbf{w}\in\mathbb{R}^{|\mathcal{I}|}} \|\mathbf{w}\|_{2}\] s.t. \[\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\!=\!\mathbb{E}[\mathbf{Y}_{ \mathcal{I}}^{(\Pi_{n})}]\mathbf{w} \tag{23}\]

Let \(\mathbf{U}_{\mathcal{I}}^{(\Pi_{n})}\), \(\mathbf{\Sigma}_{\mathcal{I}}^{(\Pi_{n})}\), \(\mathbf{V}_{\mathcal{I}}^{(\Pi_{n})}\) denote the SVD of \(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\). Further, let \(\mathbf{U}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}\), \(\mathbf{\Sigma}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}\), \(\mathbf{V}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}\) denote the rank \(r_{n}\) truncation of the SVD. Then, define \(\mathbf{w}_{r_{n}}=\mathbf{V}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}(\mathbf{\Sigma} _{\mathcal{I}}^{(\Pi_{n}),r_{n}})^{\dagger}(\mathbf{U}_{\mathcal{I}}^{(\Pi_{n }),r_{n}})^{T}\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\), where \(\dagger\) is pseudo-inverse. We first show that \(\mathbf{w}_{r_{n}}\) is a solution to (23).

\[\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\mathbf{w}_{r_{n}} =\!\left(\mathbf{U}_{\mathcal{I}}^{(\Pi_{n})}\mathbf{\Sigma}_{ \mathcal{I}}^{(\Pi_{n})}(\mathbf{V}_{\mathcal{I}}^{(\Pi_{n})})^{T}\right) \mathbf{V}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}(\mathbf{\Sigma}_{\mathcal{I}}^{(\Pi _{n}),r_{n}})^{\dagger}(\mathbf{U}_{\mathcal{I}}^{(\Pi_{n}),r_{n}})^{T}\mathbb{ E}[\mathbf{Y}^{(\Pi_{n})}]\] \[=\!\left(\sum_{i=1}^{r_{n}}\!s_{i}u_{i}v_{i}^{T}\right)\left(\sum_ {j=1}^{r_{n}}\!\frac{1}{s_{j}}v_{j}u_{j}^{T}\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\right)\] \[=\!\sum_{i,j=1}^{r_{n}}\!\frac{s_{i}}{s_{j}}u_{i}v_{i}^{T}v_{j}u_ {j}^{T}\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\] \[=\!\sum_{i=1}^{r_{n}}\!u_{i}u_{i}^{T}\mathbb{E}[\mathbf{Y}^{(\Pi_ {n})}]\] \[=\!\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\]

Next, we bound \(\|\mathbf{w}_{r_{n}}\|_{2}\) using Assumptions 6.1 and 6.4 as follows

\[\|\mathbf{w}_{r_{n}}\|_{2} \!\leq\!\|(\mathbf{\Sigma}_{\mathcal{I}}^{(\Pi_{n}),r_{n}})^{ \dagger}\|_{2}\|\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\|_{2}\] \[\leq\!\frac{\sqrt{|\Pi_{n}|}}{s_{r_{n}}(\mathbb{E}[\mathbf{Y}_{ \mathcal{I}}^{(\Pi_{n})}])}\] \[\leq\!\sqrt{\frac{cr_{n}}{|\mathcal{I}|}}\]

Therefore, we have

\[\|\tilde{\mathbf{w}}^{n}\|_{1}\!\leq\!\sqrt{|\mathcal{I}|}\|\tilde{\mathbf{w}} _{r_{n}}\|_{2}\!\leq\!\sqrt{cr_{n}}\]

### Proof of Lemma C.4

First, we introduce some necessary notation required for the proof. Let \(\hat{\mathbb{E}}\left[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}\right]=\hat{ \mathbf{U}}_{\mathcal{I}}^{(\Pi_{n})}\hat{\mathbf{S}}_{\mathcal{I}}^{(\Pi_{n}) }\hat{\mathbf{V}}_{\mathcal{I}}^{(\Pi_{n})}\) denote the rank \(r_{n}\) SVD of \(\hat{\mathbb{E}}\left[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}\right]\). Then, to establish Lemma C.4, consider the following decomposition:

\[\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_{n})}}\Delta_{w}^{n}\!=\!\left(\mathcal{P}_ {V_{\mathcal{I}}^{(\Pi_{n})}}\!-\!\mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_{n} )}}\right)\Delta_{w}^{n}\!+\!\mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_{n})}} \Delta_{w}^{n}\]

We bound each of these terms separately again.

_Bounding Term 1_. We have

\[\|\!\left(\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_{n})}}\!-\!\mathcal{P}_{\hat{V}_{ \mathcal{I}}^{(\Pi_{n})}}\right)\Delta_{w}^{n}\|_{2}\!\leq\!\left\|\mathcal{P}_ {V_{\mathcal{I}}^{(\Pi_{n})}}\!-\!\mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_{n} )}}\right\|_{\infty}\!\|\Delta_{w}^{n}\|_{2} \tag{24}\]

**Theorem D.1** (Wedin's Theorem [37]).: _Given \(\mathbf{A},\mathbf{B}\in\mathbb{R}^{m\times n}\), let \((\mathbf{U},\mathbf{V}),(\hat{\mathbf{U}},\hat{\mathbf{V}})\) denote their respective left and right singular vectors. Further, let \((\mathbf{U}_{k},\mathbf{V}_{k})\!\in\!\)(respectively, \((\hat{\mathbf{U}}_{k},\hat{\mathbf{V}}_{k})\)) correspond to the truncation of \((\mathbf{U},\mathbf{V})\) (respectively, \((\hat{\mathbf{U}},\hat{\mathbf{V}})\)), respectively, that retains the columns correspondng to the top \(k\) singular values of \(\mathbf{A}\) (respectively, \(\mathbf{B}\)). Let \(s_{i}\) represent the \(i\)-th singular values of \(A\). Then, \(\max(\|\mathcal{P}_{U_{k}}\!-\!\mathcal{P}_{\hat{U}_{k}}\|_{\infty},\|\mathcal{P} _{V_{k}}\!-\!\mathcal{P}_{\hat{V}_{k}}\|_{\infty})\!\leq\!\frac{\|\mathbf{A}- \mathbf{B}\|_{\infty}}{s_{k}-s_{k+1}}\)._Applying Theorem D.1 gives us

\[\max\biggl{(}\biggl{\|}\mathcal{P}_{U_{2}^{(\Pi_{n})}}-\mathcal{P}_{ \hat{U}_{2}^{(\Pi_{n})}}\biggr{\|}_{\text{op}},\biggl{\|}\mathcal{P}_{V_{2}^{( \Pi_{n})}}-\mathcal{P}_{\hat{V}_{2}^{(\Pi_{n})}}\biggr{\|}_{\text{op}}\biggr{)} \leq\frac{2\|\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]-\hat{ \mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\|_{\text{op}}}{s_{r_{n}}-s_{ r_{n}+1}}\] \[\leq\frac{2\sqrt{|\mathcal{I}||\Pi_{n}||}\|\mathbb{E}[\mathbf{Y}_{ \mathcal{I}}^{(\Pi_{n})}]-\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}) }]\|_{\text{max}}}{s_{r_{n}}-s_{r_{n}+1}}\] \[=\frac{2\sqrt{|\mathcal{I}||\Pi_{n}|}\Delta_{E}}{s_{r_{n}}-s_{r_{n }+1}}\] \[=\frac{2\sqrt{|\mathcal{I}||\Pi_{n}|}\Delta_{E}}{s_{r_{n}}} \tag{25}\]

where the last equality follows from the fact that \(\text{rank}(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{\Pi_{n}}])=r_{n}\), hence \(s_{r_{n}+1}=0\). Now, plugging Assumption 6.4, (25) into (24) gives us

\[\max\biggl{(}\biggl{\|}\mathcal{P}_{V_{2}^{(\Pi_{n})}}-\mathcal{ P}_{\hat{V}_{2}^{(\Pi_{n})}}\biggr{\|}_{\text{op}},\biggl{\|}\mathcal{P}_{V_{2}^{( \Pi_{n})}}-\mathcal{P}_{\hat{V}_{2}^{(\Pi_{n})}}\biggr{\|}_{\text{op}}\biggr{)} \leq C\sqrt{r_{n}}\Delta_{E} \tag{26}\]

Substituting the result of Proposition C.5 and (26) into (24) gives us

\[\biggl{\|}\Bigl{(}\mathcal{P}_{V_{2}^{(\Pi_{n})}}-\mathcal{P}_{ \hat{V}_{2}^{(\Pi_{n})}}\Bigr{)}\Delta_{w}^{n}\biggr{\|}_{2}= O_{p}\biggl{(}r_{n}^{2}\biggl{[}\frac{\Delta_{E}}{\sqrt{|\mathcal{I}|\min \{\sqrt{|\Pi_{n}|},\sqrt{|\mathcal{I}|\}}}}+\Delta_{E}^{2}\biggr{]}\biggr{)} \tag{27}\]

To further simplify (27), we substitute the result of Lemma C.3

Substituting the Lemma C.3 into (27) gives us

\[\biggl{\|}\Bigl{(}\mathcal{P}_{V_{2}^{(\Pi_{n})}}-\mathcal{P}_{ \hat{V}_{2}^{(\Pi_{n})}}\Bigr{)}\Delta_{w}^{n}\biggr{\|}_{2}= O_{p}\biggl{(}r_{n}^{2}\biggl{[}\frac{\Delta_{E}}{\sqrt{|\mathcal{I}|\min \{\sqrt{|\Pi_{n}|},\sqrt{|\mathcal{I}|\}}}}+\Delta_{E}^{2}\biggr{]}\biggr{)} \tag{28}\]

_Bounding Term 2._ We introduce some necessary notation required to bound the second term. Let \(\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}]=\sum_{l=1}^{r_{n}} \hat{s}_{l}\hat{\mu}_{l}\hat{\nu}_{l}^{\prime}\) denote the \(r_{n}\) decomposition of \(\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\). Let, \(\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}]\!=\!\hat{\mathbf{ U}}_{\mathcal{I}}^{(\Pi_{n})}\hat{\mathbf{S}}_{\mathcal{I}}^{(\Pi_{n})}(\hat{ \mathbf{V}}_{\mathcal{I}}^{(\Pi_{n})})^{T}\). Further, define \(\boldsymbol{\epsilon}^{\Pi_{n}}\!=\![\epsilon_{n}^{\pi}\!\cdot\!\pi\!\in\!\Pi_{ n}]\!\in\!\mathbb{R}^{|\Pi_{n}|}\).

Then to begin, note that since \(\hat{\mathbf{V}}_{\mathcal{I}}^{(\Pi_{n})}\) is a isometry, we have that

\[\|\mathcal{P}_{\hat{V}_{2}^{(\Pi_{n})}}\Delta_{w}^{n}\|_{2}^{2}\!=\!\|(\hat{ \mathbf{V}}_{\mathcal{I}}^{(\Pi_{n})})^{T}\Delta_{w}^{n}\|_{2}^{2}\]

To upper bound \(\|(\hat{\mathbf{V}}_{\mathcal{I}}^{(\Pi_{n})})^{T}\Delta_{w}^{n}\|_{2}^{2}\) as follows, consider

\[\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\Delta_{w}^{n}\|_{2}^{ 2}\!=\!\Bigl{(}(\hat{\mathbf{V}}_{\mathcal{I}}^{(\Pi_{n})})^{T}\Delta_{w}^{n} \Bigr{)}^{T}\Bigl{(}\hat{\mathbf{S}}_{\mathcal{I}}^{(\Pi_{n})}\Bigr{)}^{2} \Bigl{(}(\hat{\mathbf{V}}_{\mathcal{I}}^{(\Pi_{n})})^{T}\Delta_{w}^{n}\Bigr{)} \!\geq\!\hat{s}_{r_{n}}^{2}\|(\hat{\mathbf{V}}_{\mathcal{I}}^{(\Pi_{n})})^{T} \Delta_{w}^{n}\|_{2}^{2}\]

Using the two equations above gives us

\[\|\mathcal{P}_{\hat{V}_{2}^{(\Pi_{n})}}\Delta_{w}^{n}\|_{2}^{2}\!\leq\!\frac{\| \hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}]\Delta_{w}^{n}\|_{2 }^{2}}{\hat{s}_{r_{n}}^{2}} \tag{29}\]

To bound the numerator in (29), note that by definition \(\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]=\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n })}]\tilde{\mathbf{w}}^{n}\). Using this observation, we have

\[\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \Delta_{w}^{n}\|_{2}^{2}\!\leq 2\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \hat{\mathbf{w}}^{n}-\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\|_{2}^{2}\!+\!2\|\hat{ \mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}]\tilde{\mathbf{w}}^{n}- \mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\|_{2}^{2}\] \[= 2\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \tilde{\mathbf{w}}^{n}-\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\|_{2}^{2}\!+\!2\|( \hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}]\!-\!\mathbb{E}[ \mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}])\tilde{\mathbf{w}}^{n}\|_{2}^{2} \tag{30}\]

To proceed, we then use the following inequality: for any \(\mathbf{A}\!\in\!\mathbb{R}^{a\times b}\!,\!v\!\in\!\mathbb{R}^{b}\), we have

\[\|\mathbf{A}v\|_{2}\!=\!\|\sum_{j=1}^{b}\!\mathbf{A}_{\cdot j}v_{j}\|_{2}\!\leq\! \biggl{(}\max_{j\leq b}\!\|\mathbf{A}_{\cdot j}\|_{2}\biggr{)}\left(\sum_{j=1}^{b} v_{j}\right)=\|\mathbf{A}\|_{2,\infty}\|v\|_{1} \tag{31}\]Substituting (30) into (29) and then applying inequality (31) gives us

\[\|\mathcal{P}_{\hat{Y}_{\mathcal{I}}^{(\Pi_{n})}}\Delta_{w}^{n}\|_{2}^{2}\!\leq\! \frac{2}{\hat{s}_{r_{n}}^{2}}\Big{(}\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^ {(\Pi_{n}),r_{n}}]\hat{\mathbf{w}}^{n}\!-\!\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\|_ {2}^{2}\!+\!\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}]\!- \!\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\|_{2,\infty}^{2}\|\hat{ \mathbf{w}}^{n}\|_{1}^{2}\Big{)} \tag{32}\]

Next, we bound \(\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}]\hat{\mathbf{w}}^ {n}\!-\!\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\|_{2}^{2}\). To this end, note that Assumption 3.1 implies that

\[\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \hat{\mathbf{w}}^{n}\!-\!\mathbf{Y}^{(\Pi_{n})}\|_{2}^{2}\] \[= \|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \hat{\mathbf{w}}^{n}\!-\!\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\!-\!\mathbf{\epsilon}^ {\Pi_{n}}\|_{2}^{2}\] \[= \|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \hat{\mathbf{w}}^{n}\!-\!\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\|_{2}^{2}\!+\!\| \mathbf{\epsilon}^{\Pi_{n}}\|_{2}^{2}\!-\!2\langle\hat{\mathbb{E}}[\mathbf{Y}_ {\mathcal{I}}^{(\Pi_{n}),r_{n}}]\hat{\mathbf{w}}^{n}\!-\!\mathbb{E}[\mathbf{Y} ^{(\Pi_{n})}],\!\mathbf{\epsilon}^{\Pi_{n}}\rangle \tag{33}\]

Next, we proceed by calling upon Property 4.1 of [4] which states that \(\hat{\mathbf{w}}^{n}\) as given by (4) is the unique solution to the following convex program:

\[\min_{\begin{subarray}{c}\mathbf{w}\in\mathbb{R}^{[2]}\\ \mathbf{w}\in\mathbb{R}^{[2]}\end{subarray}}\|\mathbf{w}\|_{2}\] \[\text{s.t.}\ \mathbf{w}\!\in\!\operatorname*{argmin}_{\begin{subarray}{c} \mathbf{w}\in\mathbb{R}^{[2]}\end{subarray}}\!\|\mathbf{Y}^{(\Pi_{n})}\!-\! \hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}]\mathbf{w}\|_{2}^ {2} \tag{34}\]

Using this property, we have that

\[\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \hat{\mathbf{w}}^{n}\!-\!\mathbf{Y}^{(\Pi_{n})}\|_{2}^{2}\] \[\leq \|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \tilde{\mathbf{w}}^{n}\!-\!\mathbb{Y}^{(\Pi_{n})}\|_{2}^{2}\] \[= \|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \tilde{\mathbf{w}}^{n}\!-\!\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}] \!-\!\mathbf{\epsilon}^{\Pi_{n}}\|_{2}^{2}\] \[= \|(\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \!-\!\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}])\tilde{\mathbf{w}}^{n}\! \|_{2}^{2}\!+\!\|\mathbf{\epsilon}^{\Pi_{n}}\|_{2}^{2}\!-\!2\langle(\hat{ \mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}]\!-\!\mathbb{E}[ \mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}])\tilde{\mathbf{w}}^{n},\!\epsilon^{\Pi_ {n}}\rangle \tag{35}\]

Substituting (33) and (35) into (32) and using (31), we get

\[\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \hat{\mathbf{w}}^{n}\!-\!\mathbb{E}[\mathbf{Y}^{(\Pi_{n})}]\|_{2}^{2}\] \[\leq \|(\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \!-\!\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}])\tilde{\mathbf{w}}^{n}\| _{2}^{2}\!+\!2\langle(\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{ n}}])\Delta_{w}^{n},\!\mathbf{\epsilon}^{\Pi_{n}}\rangle\] \[\leq \|(\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \!-\!\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}])\|_{2,\infty}^{2}\| \tilde{\mathbf{w}}^{n}\|_{1}^{2}\!+\!2\langle(\hat{\mathbb{E}}[\mathbf{Y}_{ \mathcal{I}}^{(\Pi_{n}),r_{n}}])\Delta_{w}^{n},\!\mathbf{\epsilon}^{\Pi_{n}}\rangle\]

Then substituting this equation into (32) gives us

\[\|\mathcal{P}_{\hat{Y}_{\mathcal{I}}^{(\Pi_{n})}}\Delta_{w}^{n}\|_{2}^{2}\!\leq \!\frac{4}{\hat{s}_{r_{n}}^{2}}\Big{(}\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I} }^{(\Pi_{n}),r_{n}}]\!-\!\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\|_{2, \infty}^{2}\|\tilde{\mathbf{w}}^{n}\|_{1}^{2}\!+\!\langle\hat{\mathbb{E}}[ \mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}]\Delta_{w}^{n},\!\mathbf{\epsilon}^{\Pi_ {n}}\rangle\Big{)} \tag{36}\]

We state three lemmas that help us bound the equation above with their proofs given in Appendices D.3.1, D.3.2 and D.3.3 respectively.

**Lemma D.2**.: _Let the set-up of Theorem 6.6 hold. Then,_

\[\hat{s}_{r_{n}}\!-\!s_{r_{n}}\!=\!O_{p}(1)\]

**Lemma D.3**.: _Let the set-up of Theorem 6.6 hold. Then for a universal constant \(C\!>\!0\), we have_

\[\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}] \!-\!\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\|_{2,\infty}\!\leq\!C\sqrt{ r_{n}|\Pi_{n}}|\Delta_{E} \tag{37}\]

**Lemma D.4**.: _Let the set-up of Theorem 6.6 hold. Then,_

\[\langle\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}]\Delta_{w}^{n},\!\mathbf{\epsilon}^{\Pi_{n}}\rangle\!=\!O_{p}\Big{(}\sqrt{|\Pi_{n}|}\!+\!r_{n}\!+\! \|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}]\!-\!\mathbb{E}[ \mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\|_{2,\infty}\|\tilde{\mathbf{w}}^{n}\|_{1}\Big{)} \tag{38}\]

Using the results of three lemmas above, applying Assumption 6.4 in (36), and then simplifying gives us

\[\|\mathcal{P}_{\hat{Y}_{\mathcal{I}}^{(\Pi_{n})}}\Delta_{w}^{n}\|_{2}\!=\!O_{p} \!\left(\frac{r_{n}\Delta_{E}\|\tilde{\mathbf{w}}^{n}\|_{1}}{\sqrt{|\mathcal{I}|}} \!+\!\frac{r_{n}\Big{(}1\!+\!\sqrt{\Delta_{E}\|\tilde{\mathbf{w}}^{n}\|_{1}} \Big{)}}{\sqrt{|\mathcal{I}|}|\Pi_{n}|^{1/4}}\right) \tag{39}\]This concludes the proof for term 2. Using the result of Lemma C.3, we have that \(\|\hat{\mathbf{w}}^{n}\|_{1}\leq\sqrt{r_{n}}\). Substituting this into (39) gives us

\[\|\mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_{n})}}\Delta_{w}^{n}\|_{2}=O_{p} \left(\frac{r_{n}^{3/2}\Delta_{E}}{\sqrt{|\mathcal{I}|}}+\frac{r_{n}\Big{(}1+ \sqrt{\Delta_{E}}r_{n}^{1/4}\Big{)}}{\sqrt{|\mathcal{I}|}||\Pi_{n}|^{1/4}}\right) \tag{40}\]

_Collecting Terms._ Combining the results of (28) and (40), gives us

\[\|\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_{n})}}\Delta_{w}^{n}\|_{2}\] \[=\tilde{O_{p}}\left(r_{n}^{2}\Bigg{[}\frac{\Delta_{E}}{\sqrt{| \mathcal{I}|}\min\{\sqrt{|\mathcal{I}|}\}}+\Delta_{E}^{2}\Bigg{]}+\frac{r_{n} ^{3/2}\Delta_{E}}{\sqrt{|\mathcal{I}|}}+\frac{r_{n}\Big{(}1+\sqrt{\Delta_{E}} r_{n}^{1/4}\Big{)}}{\sqrt{|\mathcal{I}|}||\Pi_{n}|^{1/4}}\right)\]

#### d.3.1 Proof of Lemma d.2

We first state Weyl's inequality which will be useful for us to establish the results.

**Theorem D.5** (Weyl's Inequality).: _Given two matrices \(\mathbf{A},\mathbf{B}\in\mathbb{R}^{m\times n}\), let \(s_{i}\) and \(\hat{s}_{i}\) denote the \(i\)-th singular values of \(\mathbf{A}\) and \(\mathbf{B}\) respectively. Then, for all, \(i\leq\min\{n,m\}\), we have \(|s_{i}-\hat{s}_{i}|\leq\|\mathbf{A}-\mathbf{B}\|_{op}\)_

Using Weyl's inequality gives us

\[|\hat{s}_{r_{n}}-s_{r_{n}}| \leq\|\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]-\hat{E}[ \mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\|_{op}\] \[\leq\sqrt{|\mathcal{I}||\Pi_{n}|}\Delta_{E}\]

Using the inequality above and Assumption 6.4, we have

\[\hat{s}_{r_{n}}\geq s_{r_{n}}-\sqrt{|\mathcal{I}|\Pi_{n}}\Delta_{E}\] \[= s_{r_{n}}\Bigg{(}1-\frac{\sqrt{|\mathcal{I}|\Pi_{n}}\Delta_{E}} {s_{r_{n}}}\Bigg{)}\] \[\geq s_{r_{n}}(1-\sqrt{r_{n}}\Delta_{E})\]

Then, substituting (21) into the equation above gives us that

\[\frac{\hat{s}_{r_{n}}}{s_{r_{n}}}\geq\left(1-C\sqrt{\frac{r_{n}s^{2}p}{M}}\right)\]

holds with high probability for some universal constant \(C\geq 0\). Finally, using the assumption that \(M=\omega(r_{n}s^{2}p)\) yields the claimed result.

#### d.3.2 Proof of Lemma d.3

For notational simplicity, let \(\hat{\mathbf{U}}_{r_{n}},\hat{\mathbf{S}}_{r_{n}},\hat{\mathbf{V}}_{r_{n}}\) denote \(\hat{\mathbf{U}}_{\mathcal{I}}^{(\Pi_{n})},\hat{\mathbf{S}}_{\mathcal{I}}^{( \Pi_{n})},\hat{\mathbf{V}}_{\mathcal{I}}^{(\Pi_{n})}\) respectively. For a matrix \(\mathbf{A}\), let \(A_{\cdot,j}\) denote its \(j\)-th column. Additionally, denote \(\Delta_{j}=\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})_{\cdot}r_{n}} ]_{\cdot,j}-\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{\cdot,j}\). Then,

\[\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})_{\cdot}r_{n} }]_{\cdot,j}-\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{\cdot,j}\] \[= \Big{(}\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})_{\cdot }r_{n}}]_{\cdot,j}-\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T} \mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{\cdot,j}\Big{)}+\Big{(}\hat{ \mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\mathbb{E}[\mathbf{Y}_{\mathcal{ I}}^{(\Pi_{n})}]_{\cdot,j}-\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{\cdot,j} \Big{)}\]

We have that \(\Big{(}\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})_{\cdot}r_{n}}]_{ \cdot,j}-\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\mathbb{E}[ \mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{\cdot,j}\Big{)}\) belongs to the subspace spanned by the column vectors of \(\hat{U}_{r_{n}}\), while \(\Big{(}\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\mathbb{E}[\mathbf{ Y}_{\mathcal{I}}^{(\Pi_{n})}]_{\cdot,j}-\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{ \cdot,j}\Big{)}\) belongs to its orthogonal complement. Therefore,

\[\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})_{\cdot}r_{ n}}]_{\cdot,j}-\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{\cdot,j}\|_{2}^{2}\] \[= \Big{\|}\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})_{ \cdot}r_{n}}]_{\cdot,j}-\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T} \mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{\cdot,j}\Big{\|}_{2}^{2}+\Big{\|} \hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\mathbb{E}[\mathbf{Y}_{ \mathcal{I}}^{(\Pi_{n})}]_{\cdot,j}-\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n })}]_{\cdot,j}\Big{\|}_{2}^{2}\]Bounding \(\left\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n}}]_{..j}\!-\! \hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\mathbb{E}[\mathbf{Y}_{ \mathcal{I}}^{(\Pi_{n})}]_{..j}\right\|_{2}^{2}\). Observe that, we have

\[\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\hat{\mathbb{E }}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{..j} =\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\hat{\mathbb{ E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{\mathbf{ej}}\!=\!\hat{\mathbf{U}}_{r_{n}} \hat{\mathbf{U}}_{r_{n}}^{T}\hat{\mathbb{U}}_{\mathcal{I}}^{(\Pi_{n})}\hat{ \mathbf{S}}_{\mathcal{I}}^{(\Pi_{n})}\hat{\mathbf{V}}_{\mathcal{I}}^{(\Pi_{n} )}\mathbf{ej}\] \[=\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{S}}_{r_{n}}\hat{\mathbf{V}} _{r_{n}}^{T}\mathbf{ej}\!=\!\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n }),r_{n}}]_{..j}\]

Therefore, we have

\[\left\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n}),r_{n} }]_{..j}\!-\!\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\mathbb{E}[ \mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{..j}\right\|_{2}^{2} =\left\|\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\hat {\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{..j}\!-\!\hat{\mathbf{U}} _{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_ {n})}]_{..j}\right\|_{2}^{2}\] \[\leq\left\|\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T} \right\|_{2}^{2}\left\|\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_ {..j}\!-\!\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{..j}\right\|_{2}^{2}\] \[\leq\!\left\|\Pi_{n}\Delta_{E}^{2}\right.\]

Bounding \(\left\|\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\mathbb{E}[\mathbf{ Y}_{\mathcal{I}}^{(\Pi_{n})}]_{..j}\!-\!\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{( \Pi_{n})}]_{..j}\right\|_{2}^{2}\). Note that \(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{..j}=\mathbf{U}_{\mathcal{I }}^{(\Pi_{n})}\Big{(}\mathbf{U}_{\mathcal{I}}^{(\Pi_{n})}\Big{)}^{T}\mathbb{E} [\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{..j}\). Using Assumption 6.1 and (26), we have

\[\left\|\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\mathbb{ E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{..j}\!-\!\mathbb{E}[\mathbf{Y}_{ \mathcal{I}}^{(\Pi_{n})}]_{..j}\right\|_{2}^{2} \leq\left\|\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\! -\!\mathbf{U}_{\mathcal{I}}^{(\Pi_{n})}\Big{(}\mathbf{U}_{\mathcal{I}}^{(\Pi_{ n})}\Big{)}^{T}\right\|_{2}^{2}\|\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]_{..j}\|_{2}^{2}\] \[\leq\left\|\hat{\mathbf{U}}_{r_{n}}\hat{\mathbf{U}}_{r_{n}}^{T}\! -\!\mathbf{U}_{\mathcal{I}}^{(\Pi_{n})}\Big{(}\mathbf{U}_{\mathcal{I}}^{(\Pi_{ n})}\Big{)}^{T}\right\|_{2}^{2}\!\!

## Appendix E Sparsity of Matrix of Fourier Coefficients

In this section, we prove that the matrix of Fourier coefficients \(\mathcal{A}\) has at most \(r\)s non-zero columns. This result helps establish that the lower bound on the sample complexity for estimating all \(N\times 2^{p}\) parameters is \(O(Nr+r^{2}s)\) as discussed in 6.3.

**Lemma E.1**.: _The number of nonzero columns of \(\mathcal{A}\), which we have denoted \(p^{\prime}\), satisfies \(p^{\prime}\leq rs\)._

Proof.: Let \(\mathcal{A}_{t}\) denote the sub-matrix of \(\mathcal{A}\) formed by taking the first \(t\) rows. Further, let \(p^{\prime}_{t}\) denote the number of nonzero columns of \(\mathcal{A}_{t}\). We proceed by induction, that is we show for all \(t\),

\[p^{\prime}_{t}-s\cdot\operatorname{rank}(\mathcal{A}_{t})\leq 0\]

In the case of \(t=1\), either the first row is the zero vector, in which case \(p^{\prime}_{1}=\operatorname{rank}(\mathcal{A}_{1})=0\), or else \(\operatorname{rank}(\mathcal{A}_{1})=1\) and \(p^{\prime}_{1}\leq s\) since the first row is at most \(s\)-sparse. Then, for general \(t\), note that

\[\operatorname{rank}(\mathcal{A}_{t-1})\leq\operatorname{rank}(\mathcal{A}_{t} )\leq\operatorname{rank}(\mathcal{A}_{t-1})+1.\]

If \(\operatorname{rank}(\mathcal{A}_{t})=\operatorname{rank}(\mathcal{A}_{t-1})\) then we must also have \(p^{\prime}_{t-1}=p^{\prime}_{t}\) and the inductive hypothesis holds. Otherwise, \(\operatorname{rank}(\mathcal{A}_{t})=\operatorname{rank}(\mathcal{A}_{t-1})+1\). Note that the \(t^{\text{th}}\) row of \(\mathcal{A}\) has only \(s\) nonzero entries, so \(p^{\prime}_{t}\leq p^{\prime}_{t-1}+s\). In this case, we have that

\[p^{\prime}_{t}-s\cdot\operatorname{rank}(\mathcal{A}_{t})= p^{\prime}_{t}-s\cdot(\operatorname{rank}(\mathcal{A}_{t-1})+1)\] \[\leq (p^{\prime}_{t-1}+s)-s\cdot(\operatorname{rank}(\mathcal{A}_{t-1 })+1)\leq 0\]

where the last inequality holds due to the inductive hypothesis. Since \(\operatorname{rank}(\mathcal{A})=r\), we have that \(p^{\prime}\leq rs\). This completes the proof.

\begin{table}
\begin{tabular}{||c c||} \hline Notation of [2] & Our Notation \\ \hline \(\mathbf{Y}\) & \(\mathbf{Y}_{\Pi_{n}}\) \\ \hline \(\mathbf{X}\) & \(\mathbb{E}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\) \\ \hline \(\mathbf{Z}\) & \(\hat{\mathbb{E}}[\mathbf{Y}_{\mathcal{I}}^{(\Pi_{n})}]\) \\ \hline \(n\) & \(|\Pi_{n}|\) \\ \hline \(p\) & \(|\mathcal{I}|\) \\ \hline \(\boldsymbol{\beta}^{*}\) & \(\bar{\mathbf{w}}\) \\ \hline \(\beta\) & \(\bar{\mathbf{w}}\) \\ \hline \(\Delta_{E}\) & \(\Delta_{E}\) \\ \hline \(r\) & \(r\) \\ \hline \(\phi^{lr}\) & \(0\) \\ \hline \(A\) & \(1\) \\ \hline \(K\) & \(0\) \\ \hline \(K_{a}\),\(\kappa\),\(\bar{\sigma}\) & \(C\sigma\) \\ \hline \(\rho_{min}\) & \(1\) \\ \hline \end{tabular}
\end{table}
Table 2: A summary of the main notational differences between our setting and that of [2].