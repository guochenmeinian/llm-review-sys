# Estimating Causal Effects Identifiable from a Combination of Observations and Experiments

Yonghan Jung\({}^{1}\), Ivan Diaz\({}^{2}\), Jin Tian\({}^{3}\), and Elias Bareinboim\({}^{4}\)

###### Abstract

Learning cause and effect relations is arguably one of the central challenges found throughout the data sciences. Formally, determining whether a collection of observational and interventional distributions can be combined to learn a target causal relation is known as the problem of _generalized identification_ (or _g-identification_) [Lee et al., 2019]. Although g-identification has been well understood and solved in theory, it turns out to be challenging to apply these results in practice, in particular when considering the estimation of the target distribution from finite samples. In this paper, we develop a new, general estimator that exhibits multiply robustness properties for g-identifiable causal functionals. Specifically, we show that any g-identifiable causal effect can be expressed as a function of generalized multi-outcome sequential back-door adjustments that are amenable to estimation. We then construct a corresponding estimator for the g-identification expression that exhibits robustness properties to bias. We analyze the asymptotic convergence properties of the estimator. Finally, we illustrate the use of the proposed estimator in experimental studies. Simulation results corroborate the theory.

## 1 Introduction

Performing causal inferences is a crucial aspect of scientific research with broad applications ranging from the social sciences to economics, biology to medicine. It provides a set of principles and tools to draw causal conclusions from a combination of observations and experiments. Two significant tasks in the realization of these inferences are causal effect identification and estimation. _Causal effect identification_ concerns determining the conditions under which one can infer the causal effect \(P(Y=y|do(X=x))\) (shortly, \(P(y|do(x))\)) of the treatment \(X=x\) on the outcome \(Y=y\) from a combination of available data distributions and a causal graph depicting the data-generating process [Pearl, 2000, Bareinboim and Pearl, 2016]. _Causal effect estimation_ aims to develop an estimator for the identified causal effect expression using a set of finite samples.

Recent advances in the literature on generalized causal effect identification (g-identification) have developed algorithms that can identify causal effects by using a set of observational and experimental distributions and a causal graph. The result is an expression of the causal effect as a function of available observational and experimental distributions [Bareinboim and Pearl, 2012, Lee et al., 2019]. For concreteness, consider some practical scenarios that exemplify g-identification.

**Example 1**.: Many studies have investigated how a training program's eligibility (\(X\)) affects future salary (\(Y\)) (e.g.,[Glynn and Kashin, 2017]). Actual registration in the program (\(Z\)) determines the salary, and experimental studies have looked into how Z affects Y (e.g., [LaLonde, 1986]). Eligibility is determined by past average income (\(W\)), which is associated with both \(Z\) and \(Y\). The causalgraph in Fig. 0(a) shows the data-generating process, with bidirected edges indicating unmeasured confounders affecting the variables. According to Lee et al. (2019), the causal effect \(P(y|do(x))\) can be identified by combining the experimental distribution on \(Z\) (denoted \(P(\cdot|do(z))\)) with the observational distribution \(P\). It's given as \(P(y|do(x))=\sum_{z,w}P(y|do(z))P(z|w,x)P(w)\). 

**Example 2**.: There have been many experimental studies on the effect of an antihypertensive drug (\(X_{1}\)) on blood pressure (\(W\)) (e.g., Hansson et al. (1999)) and on the effect of using an anti-diabetic drug (\(X_{2}\)) on cardiovascular disease (\(Y\)) (e.g., Ajjan and Grant (2006), Kumar et al. (2016)). \(R\) is a set of mediators. Their relations are depicted in Fig. 0(b). Recent studies report that simultaneously taking antihypertensive and anti-diabetic drugs may be harmful (Ferrannini and Cushman, 2012). This motivates the study of the combined causal effect of both treatments (i.e., \(P(y|do(x_{1},x_{2}))\)) by combining the two experimental studies (i.e., from \(P(\cdot|do(x_{1}))\) and \(P(\cdot|do(x_{2}))\)). According to Lee et al. (2019), it turns out that \(P(y|do(x_{1},x_{2}))=\sum_{r,w}P(y|r,w,do(x_{2}))P(r|x_{2},do(x_{1}))\sum_{x_{ 2}^{\prime}}P(w|r,x_{2}^{\prime},do(x_{1}))P(x_{2}^{\prime}|do(x_{1}))\), which means that the joint treatment effects can be computed using the two experimental studies on \(X_{1}\) and \(X_{2}\). 

On the other hand, causal effect estimation has mainly focused on limited identification scenarios, relying on stringent assumptions such as the no unmeasured confounder assumption. Beyond these restrictions, recent progress has been made in developing statistically appealing estimators from observational data for any identification functional given by the complete identification algorithms (Jung et al., 2020, 2021, 2021, 2022, 2022, 2022, 2022). While these estimators are capable of estimating any identification expression from observational data, they are not yet sufficiently advanced to estimate g-identification, which involves multiple observations and experiments.

Recently, Jung et al. (2023) generalized existing doubly robust estimators (Mises, 1947, Bickel et al., 1993, Robins and Rotnitzky, 1995, Bang and Robins, 2005, Robins et al., 2009, van der Laan and Gruber, 2012, Luedtke et al., 2017, Chernozhukov et al., 2018, Rotnitzky et al., 2021) to estimate covariate adjustments (e.g., back-door adjustment (Pearl, 1995), sequential back-door (SBD) adjustment (Pearl and Robins, 1995) or multi-outcome SBD (mSBD) (Jung et al., 2021)) in the g-identification setting, where the expression is in the form of covariate adjustment involving multiple experimental distributions. However, the covariate adjustments only cover a limited portion of all g-identifiability scenarios as in Examples (1,2). On a different thread, Xia et al. (2023) developed a neural network-based estimation framework capable of taking a combination of observational/experimental data. Still, the derived estimators do not possess the doubly robustness property. In other words, there is still a gap between g-identification and causal effect estimation.

In this paper, our goal is to bridge the gap between g-identification and causal effect estimation. Specifically, this paper presents a framework for estimating identification expressions using multiple sets of samples from both observational and interventional distributions. This framework is a generalization of the results in Jung et al. (2021) since our results reduce to theirs when only observational data is available. Furthermore, our work subsumes the results in Jung et al. (2023) when the identification functional takes the form of covariate adjustments.

The contributions of our paper are as follows:

1. We show that any causal effects identifiable by g-identification can be expressed as a function of generalized mSBD adjustments. We provide a systematic procedure for specifying the function.

Figure 1: Causal graphs of examples 1 and 2. The nodes representing the treatment and the outcome are marked in blue and red, respectively.

2. After developing a doubly robust estimator for generalized mSBD adjustments, we construct an estimation framework for any g-identifiable causal effects, which enjoys multiply robustness against model misspecification and bias. Experimental studies corroborate our results.

### Preliminaries

We use bold letters (\(\mathbf{X}\)) to denote a random vector and a random value. Each random vector is represented with a capital letter (\(\mathbf{X}\)) and its realized value with a small letter (\(\mathbf{x}\)). Given a set \(\mathbf{X}=\{X_{1},\cdots,X_{n}\}\) aligned by an order \(\prec\) such that \(X_{i}\prec X_{j}\) for \(i<j\), we denote \(\overline{\mathbf{X}}^{i}\coloneqq\{X_{1},\cdots,X_{i}\}\) and \(\overline{\mathbf{X}}^{i,j}\coloneqq\{X_{i},\cdots,X_{j}\}\). For a discrete vector \(\mathbf{X}\), we use \(\mathbbm{1}_{\mathbf{x}}(\mathbf{X})\) to represent the indicator function such that \(\mathbbm{1}_{\mathbf{x}}(\mathbf{X})=1\) if \(\mathbf{X}=\mathbf{x}\); \(\mathbbm{1}_{\mathbf{x}}(\mathbf{X})=0\) otherwise. We use \([n]\coloneqq\{1,\cdots,n\}\) a collection of index. For a discrete vector \(\mathbf{V}\), we use \(P(\mathbf{v})\coloneqq P(\mathbf{V}=\mathbf{v})\) where \(P\) is a distribution. We use \(\mathbb{E}_{P}\left[f(\mathbf{V})\right]\coloneqq\sum_{\mathbf{v}\in\mathfrak{ ev}}f(\mathbf{V})P(\mathbf{v})\) for a function \(f\), where \(\mathfrak{Sw}\) denote the support of \(\mathbf{V}\). We will use \(\mathfrak{D}_{\mathbf{V}}\) to denote the domain of \(\mathbf{V}\). For a sample set \(D\coloneqq\{\mathbf{V}_{(i)}\}_{i=1}^{n}\) where \(\mathbf{V}_{(i)}\) denotes the \(i\)th samples, we use \(\mathbb{E}_{D}\left[f(\mathbf{V})\right]\coloneqq(1/n)\sum_{i=1}^{n}f(\mathbf{ V}_{(i)})\). We use \(\|f\|_{P}\coloneqq\sqrt{\mathbb{E}_{P}\left[\{f(\mathbf{V})\}^{2}\right]}\). If a function \(\widehat{f}\) is a consistent estimator of \(f\) having a rate \(r_{n}\), we will use \(\widehat{f}-f=o_{P}(r_{n})\). We will say \(\hat{f}\) is \(L_{2}\)-consistent if \(\|\hat{f}-f\|_{P}=o_{P}(1)\). We will use \(\widehat{f}-f=O_{P}(1)\) if \(\widehat{f}-f\) is bounded in probability. Also, \(\widehat{f}-f\) is said to be bounded in probability at rate \(r_{n}\) if \(\widehat{f}-f=O_{P}(r_{n})\). We use the typical graph terminology \(pa(\mathbf{C})_{G},ch(\mathbf{C})_{G},de(\mathbf{C})_{G},an(\mathbf{C})_{G}\) to represent the union of \(\mathbf{C}\) with its parents, children, descendants, ancestors in the graph \(G\). We use \(\text{pre}(\mathbf{C};G)\) to denote the union of the predecessors of \(C_{i}\in\mathbf{C}\) given a topological order \(\prec_{G}\) over a graph \(G\). We use \(G(\mathbf{C})\) to denote the subgraph of \(G\) over \(\mathbf{C}\). Throughout the paper, we will assume a fixed topological order \(\prec_{G}\) over \(\mathbf{V}\) on \(G\). 

**Structural Causal Models (SCMs).** We use Structural Causal Models (SCMs) as our framework (Pearl, 2000; Bareinboim et al., 2022). An SCM \(\mathcal{M}\) is a quadruple \(\mathcal{M}=\langle\mathbf{U},\mathbf{V},P(\mathbf{U}),F\rangle\). \(\mathbf{U}\) is a set of exogenous (latent) variables following a joint distribution \(P(\mathbf{U})\). \(\mathbf{V}\) is a set of endogenous (observable) variables whose values are determined by functions \(F=\{f_{V_{i}}\}_{V_{i}\in\mathbf{V}}\) such that \(V_{i}\gets f_{V_{i}}(pa_{i},u_{i})\) where \(PA_{i}\subseteq V\) and \(U_{i}\subseteq U\). Each SCM \(\mathcal{M}\) induces a distribution \(P(\mathbf{V})\) and a causal graph \(G=G(\mathcal{M})\) over \(\mathbf{V}\) in which there exists a directed edge from every variable in \(PA_{i}\) to \(V_{i}\) and dashed-bidirected arrows encode common latent variables (e.g., see Fig. 0(a)). Performing an intervention fixing \(\mathbf{X}=\mathbf{x}\) is represented through the do-operator, \(do(\mathbf{X}=\mathbf{x})\), which encodes the operation of replacing the original equations of \(X\) (i.e., \(f_{X}(pa_{x},u_{x})\)) by the constant \(x\) for all \(X\in\mathbf{X}\) and induces an interventional distribution \(P(\mathbf{V}|do(\mathbf{x}))\). 

**Experimental Distributions and Samples** To clarify the connection between the experimental samples where the randomization is applied to \(\mathbf{Z}\subseteq\mathbf{V}\) and the distribution \(P_{\mathbf{z}}(\mathbf{V}\backslash\mathbf{z})\), we introduce the notation \(P_{\sigma(\mathbf{Z})}(\mathbf{V})\) where \(\sigma(\mathbf{Z})\) denotes that \(\mathbf{Z}\) is randomized. The distribution \(P_{\sigma(\mathbf{Z})}(\mathbf{V})\) is a distribution induced by the SCM in which the original equation \(Z\gets f_{Z}(pa_{z},u_{z})\) for \(Z\in\mathbf{Z}\) is replaced to the function assigning the value to \(Z=z\) at random without depending on other endogenous variables \(PA_{Z}\); e.g., \(Z=1\) and \(0\) at probability \(0.5\) for each. We note that \(P\coloneqq P_{\sigma(\emptyset)}\) when observational. For any set \(\mathbf{A},\mathbf{B},\mathbf{Z}\subseteq\mathbf{V}\), the interventional distribution can be represented as \(P(\mathbf{A}|do(\mathbf{z}),\mathbf{B})=P_{\sigma(\mathbf{Z})}(\mathbf{A}| \mathbf{Z}=\mathbf{z},\mathbf{B})\) by the definition of the \(do\)-operator and \(P_{\sigma(\mathbf{Z})}\) distribution. We use \(P_{\mathbf{z}}(\mathbf{A}|\mathbf{B})\coloneqq P_{\sigma(\mathbf{Z})}( \mathbf{A}|\mathbf{Z}=\mathbf{z},\mathbf{B})\) to highlight that the distribution is induced from the randomization and conditioning on \(\mathbf{Z}=\mathbf{z}\). The experimental samples from randomization \(\sigma(\mathbf{Z})\) induces samples \(D_{\sigma(\mathbf{Z})}\) following \(P_{\sigma(\mathbf{Z})}(\mathbf{V})\). We use \(D_{\mathbf{z}}\) to denote the subsample of \(D_{\sigma(\mathbf{Z})}\) fixing \(\mathbf{Z}=\mathbf{z}\), which follows \(P_{\mathbf{z}}(\mathbf{V})\). 

**g-identification.** Let \(\mathbb{Z}\coloneqq\{\mathbf{Z}_{i}\}_{i=1}^{m}\) denote a collection of variables where \(\mathbf{Z}_{i}\) can be an empty set. Let \(\mathbb{P}\coloneqq\{P_{\sigma(\mathbf{Z}_{i})}(\mathbf{V}),\ \mathbf{Z}_{i}\in\mathbb{Z}\}\), a collection of distributions inducing experimental samples from trials randomizing \(\mathbf{Z}_{i}\in\mathbb{Z}\). A causal effect \(P(\mathbf{y}|do(\mathbf{x}))\) is said to be \(g\)_-identifiable_ from \(\mathbb{P}\) in a causal graph \(G\) if \(P(\mathbf{y}|do(\mathbf{x}))\) is uniquely computable from the combination of distributions in \(\mathbb{P}\) in any SCM that induces \(G\)(Lee et al., 2019, Def. 4). The complete g-identification algorithm developed by Lee et al. (2019) identifies the causal effect by decomposing so-called _confounded components_ (c-component). A _c-component_ is a maximal set of variables where every pair is connected by a bidirectional path composed of bidirectional edges (\(V_{i}\leftrightarrow V_{j}\)). For example, graphs in Figs. 0(a, 0(b)) form a single c-component since bidirectional paths connect any pairs of variables. For any sets \(\mathbf{C}\subseteq\mathbf{V}\), the quantity \(Q[\mathbf{C}]\coloneqq P(\mathbf{c}|do(\mathbf{v}\backslash\mathbf{c}))\) is called a _c-factor_. To identify the causal effect \(P(\mathbf{y}|do(\mathbf{x}))\) from \(\mathbb{P}\) and \(G\), the g-identification algorithm in [Lee et al., 2019, Algo. 1] (and rewrote in Algo. 1) rewrites the causal effect as a marginalization over a product of c-factors, \(P(\mathbf{y}|do(\mathbf{x}))=\sum_{\mathbf{d}\setminus\mathbf{y}\in\mathfrak{S}_ {\mathbf{D}\setminus\mathbf{Y}}}\prod_{i=1}^{k_{d}}Q[\mathbf{D}_{i}]\), where \(\mathbf{D}\coloneqq an(\mathbf{Y})_{G(\mathbf{V}\setminus\mathbf{X})}\) and \(\mathbf{D}_{i}\) are c-components in \(G(\mathbf{D})\), and identifies each \(Q[\mathbf{D}_{i}]\) from \(\mathbb{P}\). 

### Problem Statement

This paper aims to develop an estimation framework for the g-identifiable causal effect \(P(\mathbf{y}|do(\mathbf{x}))\) identified as a function of distributions in \(\mathbb{P}\) from experimental samples \(\mathbb{D}\coloneqq\{D_{\mathbf{z}_{i}}\sim P_{\sigma(\mathbf{Z}_{i})}( \mathbf{V})\in\mathbb{P}\}\). We impose the following regularity assumptions:

**Assumption 1** (Regularity).: _For variables \(\mathbf{V}\) and distributions \(P_{\sigma(\mathbf{Z})}\in\mathbb{P}\), the following conditions hold: (1) All variables in \(\mathbf{V}\) are discrete; (2) \(P_{\sigma(\mathbf{Z})}(\mathbf{v})>c,\forall\mathbf{v}\in\mathfrak{D}\mathbf{V}\) for some \(c\in(0,1)\)._

We discuss the relaxation of the regularity assumption in Appendix C. This relaxation allows some subset of variables in \(\mathbf{V}\) can be a mixture of continuous and discrete random variables. Due to space constraints, all proofs are provided in Appendix B.

## 2 Expressing Causal Effects as a Combination of mSBD Adjustments

In this section, we present an algorithm that expresses any g-identifiable causal effects as a combination of _marginalization/multiplication/divisions_ of adjustment functionals defined in the following. We begin by formally defining the generalized multi-outcome sequential back-door adjustment (g-mSBD) functional, which strictly generalizes the mSBD adjustment proposed by Jung2021:

**Definition 1** (generalized-mSBD adjustment (g-mSBD)).: Let \((\mathbf{W},\mathbf{R})\) be a disjoint pair in \(\mathbf{V}\) topologically ordered as \((\mathbf{W},\mathbf{R})=\{\mathbf{R}_{0},W_{1},\cdots,\mathbf{R}_{m-1},W_{m}, \mathbf{R}_{m}\}\) by \(\prec_{G}\), where \(\mathbf{R}_{i}\) can be empty. Let \(\overline{\mathbf{W}}^{i-1}\coloneqq\{W_{j}\}_{j=1}^{i-1}\) and \(\overline{\mathbf{R}}^{i-1}\coloneqq\{\mathbf{R}_{j}\}_{j=0}^{i-1}\) for \(\forall i\in[m]\). Let \(\mathbf{C}\subseteq\mathbf{W}\). Let \(\mathbb{Z}_{0}\subseteq\mathbb{Z}\) be some set such that \(\forall\mathbf{Z}\in\mathbb{Z}_{0}\), \(\mathbf{W}\cap\mathbf{Z}=\emptyset\). Let \(\mathsf{seq}(\mathbb{Z}_{0})\) denote a sequence \((\mathbf{z}_{1},\cdots,\mathbf{z}_{m})\) where \(\mathbf{z}_{i}\) denotes some realization of \(\mathbf{Z}_{i}\in\mathbb{Z}_{0}\) (same \(\mathbf{z}_{i}\) could appear multiple times in the sequence). Then, the g-mSBD adjustment is expressed as an operator \(A_{0}[\mathbf{W},\mathbf{C},\mathbf{R};\mathbb{Z}_{0},\mathsf{seq},G](\mathbf{ w}\backslash\mathbf{c},\mathbf{r})\) defined by

\[A_{0}[\mathbf{W},\mathbf{C},\mathbf{R};\mathbb{Z}_{0},\mathsf{seq}](\mathbf{w} \backslash\mathbf{c},\mathbf{r})\coloneqq\sum_{\mathbf{c}\in\mathfrak{S}_{ \mathbf{C}}}\prod_{i:W_{i}\in\mathbf{W}}P_{\mathbf{z}_{i}}(w_{i}|\overline{ \mathbf{w}}^{i-1},\overline{\mathbf{r}}^{i-1}\backslash\mathbf{z}_{i}). \tag{1}\]

The g-mSBD adjustment specializes to the mSBD adjustment [Jung et al., 2021b] when \(\mathbb{Z}_{0}=\emptyset\). The g-mSBD adjustment can be viewed as a variant of the g-formula [Robins, 1986] involving multiple distributions. The power of the g-mSBD adjustment lies in its ability to express the c-factor:

**Lemma 1** (c-component Identification [Jung et al., 2021b]).: _Let \(\mathbf{S}\) denote a \(c\)-component in \(G_{i}\coloneqq G(\mathbf{V}\backslash\mathbf{Z}_{i})\) for some \(\mathbf{Z}_{i}\in\mathbb{Z}\). Let \(\mathbf{R}\coloneqq pa(\mathbf{S})_{G_{i}}\backslash\mathbf{S}\). Let \((\mathbf{S},\mathbf{R})\) be ordered as \((\mathbf{R}_{0},S_{1},\cdots,\mathbf{R}_{m-1},S_{m})\) by \(\prec_{G}\). Let \(\mathbf{A}\subseteq\mathbf{S}\) denote a set satisfying \(\mathbf{A}=an(\mathbf{A})_{G_{i}(\mathbf{S})}\). Let \(\mathbf{C}\coloneqq(\mathbf{S}\backslash\mathbf{A})\). Let \(\mathbb{Z}_{0}\coloneqq\{\mathbf{Z}_{i}\}\) and \(\mathsf{seq}(\mathbb{Z}_{0})\) be a sequence of \(\mathbf{z}_{i}\) repeating \(m\) times. Then, the \(c\)-factor \(Q[\mathbf{A}]\) is g-identifiable as follows:_

\[Q[\mathbf{A}]=A_{0}[\mathbf{S},\mathbf{C},\mathbf{R};\mathbb{Z}_{0}\coloneqq\{ \mathbf{Z}_{i}\},\mathsf{seq}](\mathbf{a},\mathbf{r})=\sum_{\mathbf{c}\in \mathfrak{S}_{\mathbf{C}}}\prod_{j:V_{j}\in\mathbf{S}}P_{\mathbf{z}_{i}}(v_{j }|\overline{\mathbf{s}}^{j-1},\overline{\mathbf{r}}^{j-1}\backslash\mathbf{z}_{ i}). \tag{2}\]

We propose an identification algorithm, Algo. 1, which expresses any causal effect as a combination of marginalizations, multiplications, and divisions of g-mSBD operators. Here are some results used for the g-mSBD operation. An example of using these results is provided in Appendix A.

**Lemma 2** (Marginalization).: _Let \(A_{0}[\mathbf{W},\mathbf{C},\mathbf{R};\mathbb{Z}_{0},\mathsf{seq}](\mathbf{w} \backslash\mathbf{c},\mathbf{r})\) denote the g-mSBD operator in Def. 1. Let \(\mathbf{W}_{0}\subseteq\mathbf{W}\backslash\mathbf{C}\). Let \(\mathbf{W}_{\textit{mar}}\subseteq\{\mathbf{W}_{0},\mathbf{C}\}\) denote the vector formed by the following procedure: Starting from \(\mathbf{W}_{\textit{mar}}=\emptyset\), for \(j=m,\cdots,1\), \(\mathbf{W}_{\textit{mar}}=\mathbf{W}_{\textit{mar}}\cup\{W_{j}\}\) if (1) \(W_{j}\in\{\mathbf{W}_{0},\mathbf{C}\}\) and (2) \(\exists k\in\{j,\cdots,m\}\) such that \(\mathbf{R}_{j},\cdots,\mathbf{R}_{k-1}=\emptyset\), \(\overline{\mathbf{W}}^{k+1:m}\subseteq\mathbf{W}_{\textit{mar}}\), and \(\mathbf{Z}_{k}=\cdots=\mathbf{Z}_{j}\) and \(\mathbf{z}_{k}=\cdots=\mathbf{Z}_{j}\) and \(\mathbf{z}_{k}=\cdots=\mathbf{Z}_{j}\). Let \(\mathbf{W}^{\prime}\coloneqq\mathbf{W}\backslash\mathbf{W}_{\textit{mar}}\), \(\mathbf{R}^{\prime}\coloneqq\textit{pre}(\mathbf{W}^{\prime};G)\cap\mathbf{R}\) and \(\mathbf{C}^{\prime}\coloneqq\{\mathbf{W}_{0},\mathbf{C}\}\backslash\mathbf{W}_{ \textit{mar}}\). Let \(\mathbb{Z}^{\prime}\subseteq\mathbb{Z}_{0}\) denote the collection of \(\mathbf{Z}_{i}\) corresponding to the variable in \(\mathbf{W}^{\prime}\), and \(\mathsf{seq}^{\prime}\) the corresponding sequence. Then,_

\[\sum_{\mathbf{w}_{0}\in\mathfrak{S}_{W_{0}}}A_{0}[\mathbf{W},\mathbf{C}, \mathbf{R};\mathbb{Z}_{0},\mathsf{seq}](\mathbf{w}\backslash\mathbf{c}, \mathbf{r})=A_{0}[\mathbf{W}^{\prime},\mathbf{C}^{\prime},\mathbf{R}^{\prime}; \mathbb{Z}^{\prime},\mathsf{seq}^{\prime}](\mathbf{w}^{\prime}\backslash \mathbf{c}^{\prime},\mathbf{r}^{\prime}). \tag{3}\]This lemma provides a graphical criterion where \(\sum_{\mathbf{w}_{0}\in\mathfrak{S}_{W_{0}}}A_{0}[\mathbf{W},\mathbf{C},\mathbf{R}; \mathbb{Z}_{0},\mathtt{seq}](\mathbf{w}\backslash\mathbf{c},\mathbf{r})\) is given as a g-mSBD operator.

```
Input:\(\mathbf{x},\mathbf{y},\mathbb{Z}\coloneqq\{\mathbf{Z}_{i}\},\mathbb{P}\coloneqq\{P_{ \mathbf{o}^{\prime}(\mathbf{Z}_{i})}(\mathbf{V}),\ \forall\mathbf{Z}_{i}\in\mathbb{Z}\},G\) Output:Expression of \(P(\mathbf{y}|do(\mathbf{x}))\) w.r.t. distributions in \(\mathbb{P}\)
1If\(\exists\mathbf{Z}_{i}\in\mathbb{Z}\) such that \(P(\mathbf{y}|do(\mathbf{x}))=P_{\mathbf{z}_{i}}(\mathbf{y})\) for some \(\mathbf{z}_{i}\in\mathfrak{D}_{\mathbf{Z}_{i}}\), thenreturn\(P_{\mathbf{z}_{i}}(\mathbf{y})\).
2 Let \(\mathbf{V}\gets an(\mathbf{Y})\); \(P(\mathbf{v})\gets P(an(\mathbf{Y}))\); and \(G\gets G(an(\mathbf{Y}))\).
3 Let \(\mathbf{D}\coloneqq an(\mathbf{Y})_{G(\mathbf{V}\backslash\mathbf{X})}\).
4 Find the \(C\)-component of \(G(\mathbf{D})\): \(\mathbf{D}_{1},\cdots\mathbf{D}_{k_{d}}\).
5foreach\(\mathbf{D}_{j}\in\{\mathbf{D}_{1},\cdots\mathbf{D}_{k_{d}}\}\)do
6foreach\(\mathbf{Z}_{i}\in\mathbb{Z}\)do
7 Find the c-component \(\mathbf{S}_{j}^{i}\) in \(G(\mathbf{V}\backslash\mathbf{Z}_{i})\) such that \(\mathbf{D}_{j}\subseteq\mathbf{S}_{j}^{i}\).
8\(Q[\mathbf{S}_{j}^{i}]=A_{0}[\mathbf{S}_{j}^{i},\emptyset,\mathbf{R}_{j}^{i} ;\mathbb{Z}_{j}^{i}\coloneqq\{\mathbf{Z}_{i}\},\mathtt{seq}_{j}^{i}](\mathbf{ s}_{j}^{i},\mathbf{r}_{j}^{i})\), where \(\mathbf{R}_{j}^{i}\coloneqq pa(\mathbf{S}_{j}^{i})_{G(\mathbf{V}\backslash \mathbf{Z}_{i})}\backslash\mathbf{S}_{j}^{i}\). // By Lemma 1
9 Run \(Q[\mathbf{D}_{j}]=\mathtt{subID}(\mathbf{D}_{j},\mathbf{S}_{j}^{i},Q[\mathbf{ S}_{j}^{i}],G(\mathbf{S}_{j}^{i}))\).
10If\(Q[\mathbf{D}_{j}]\neq\mathtt{FAIL}\), then break.
11
12 end if
13
14If\(Q[\mathbf{D}_{j}]=\mathtt{FAIL}\), then return FAIL.
15
16 end if
17\(P(\mathbf{y}|do(\mathbf{x}))=\sum_{\mathbf{d}\backslash\mathbf{y}\in\mathfrak{S} _{\mathbf{D}\backslash\mathbf{V}}}\prod_{j=1}^{k_{d}}Q[\mathbf{D}_{j}]\). // Apply Lemmas (2,3,4) if viable return\(P(\mathbf{y}|do(\mathbf{x}))\)
18Procedure\(\mathtt{subID}(\mathbf{C},\mathbf{T},Q[\mathbf{T}],G(\mathbf{T}))\)
19 Let \(\mathbf{A}\coloneqq an(\mathbf{C})_{G(\mathbf{T})}=\{A_{1},A_{2},\cdots,A_{n_{a }}\}\) such that \(A_{1}\prec_{G}\cdots\prec_{G}A_{n_{a}}\) in \(G(\mathbf{T})\).
20 Let \(Q[\mathbf{A}]=\sum_{\mathbf{t}\backslash\mathbf{a}\in\mathfrak{S}_{\mathbf{T }\backslash\mathbf{A}}}Q[\mathbf{T}]\). // Apply Lemma 2 if viable
21If\(\mathbf{A}=\mathbf{C}\), then return\(Q[\mathbf{A}]\).
22
23If\(\mathbf{A}=\mathbf{T}\), then return\(\mathtt{FAIL}\).
24else
25 Let \(\mathbf{S}\) be the c-component in \(G(\mathbf{A})\) such that \(\mathbf{C}\subseteq\mathbf{S}\).
26 Let \(Q[\mathbf{S}]\coloneqq\prod_{\{i:A_{i}\in\mathbf{S}\}}\frac{\sum_{b_{i+1}\in \mathbf{c}_{\mathbf{B}_{i+1}}}Q[\mathbf{A}]}{\sum_{b_{i}\in\mathbf{c}_{\mathbf{ B}_{i}}}Q[\mathbf{A}]}\) for \(\mathbf{B}_{i}\coloneqq\mathbf{A}\backslash\overline{\mathbf{A}}^{i-1}\). // Apply Lemmas (2,3,4) if viable return\(\mathtt{subID}\left(\mathbf{C},\mathbf{S},Q[\mathbf{S}],G(\mathbf{S})\right)\)
27 end if
```

**Algorithm 1**\(\mathrm{GID}\left(\mathbf{x},\mathbf{y},\mathbb{Z},\mathbb{P},G\right)\)

The \(C\)-component of \(G(\mathbf{D})\): \(\mathbf{D}_{1},\cdots\mathbf{D}_{k_{d}}\).

The \(Q[\mathbf{D}_{j}]\neq\mathtt{FAIL}\), then break.

This lemma provides a graphical criterion where \(\sum_{\mathbf{w}_{0}\in\mathfrak{S}_{W_{0}}}A_{0}[\mathbf{W},\mathbf{C}, \mathbf{R};\mathbb{Z}_{0},\mathtt{seq}](\mathbf{w}\backslash\mathbf{c},\mathbf{ r})\) is given as a g-mSBD operator.

**Lemma 3** (**Multiplication)**.: _Let \(A_{0}^{i}\coloneqq A_{0}[\mathbf{W}_{i},\emptyset,\mathbf{R}_{i};\mathbb{Z}_{i},\mathtt{seq}^{i}](\mathbf{w}_{i},\mathbf{r}_{i})\coloneqq\prod_{j=1}^{m^{i}}P_ {\mathbf{z}_{j}^{i}}(w_{i,j}|\overline{\mathbf{w}}_{i}^{j-1},\overline{\mathbf{ r}}_{i}^{j-1}\backslash\mathbf{z}_{j}^{i})\) for \(i\in\{1,2\}\) where \(\mathtt{seq}^{i}\coloneqq(\mathbf{z}_{j}^{i})_{j=1}^{m^{i}}\). Let \(\mathbf{W}\coloneqq\mathbf{W}_{1}\cup\mathbf{W}_{2}\). Let \(\mathbf{R}\coloneqq(\mathbf{R}_{1}\cup\mathbf{R}_{2})\backslash\mathbf{W}\). Let \((\mathbf{W},\mathbf{R})\) be ordered by \(\prec_{G}\). Let \(\mathbb{Z}\coloneqq\mathbb{Z}_{1}\cup\mathbb{Z}_{2}\). Assume the following: (1) \(\mathbf{W}_{1}\cap\mathbf{W}_{2}=\emptyset\); and (2) \(\forall W_{j}\in\mathbf{W},\exists W_{i,k}\in\mathbf{W}_{i}\) such that \((\overline{\mathbf{W}}^{j-1},\overline{\mathbf{R}}^{j-1})=(\overline{\mathbf{W}}_ {i}^{k-1},\overline{\mathbf{R}}_{i}^{k-1})\)._

_Let \(\mathtt{seq}\coloneqq(\mathbf{z}_{j})_{j:W_{j}\in\mathbf{W}}\) where \(\mathbf{z}_{j}=\mathbf{z}_{k}^{i}\) for all \(j\). Then,_

\[A_{0}^{1}\times A_{0}^{2}=A_{0}[\mathbf{W},\emptyset,\mathbf{R};\mathbb{Z}, \mathtt{seq}](\mathbf{w},\mathbf{r})=\prod_{j:W_{j}\in\mathbf{W}}P_{\mathbf{z}_{j }}(w_{j}|\overline{\mathbf{w}}^{j-1},\overline{\mathbf{r}}^{j-1}\backslash\mathbf{z}_{j}). \tag{4}\]

This lemma provides a graphical criterion where a product \(A_{0}^{1}\times A_{0}^{2}\) is given as a g-mSBD operator.

**Lemma 4** (**Division)**.: _Let \(A_{0}^{i}\coloneqq A_{0}[\mathbf{W}_{i},\emptyset,\mathbf{R}_{i};\mathbb{Z}_{i},\mathtt{seq}^{i}](\mathbf{w}_{i},\mathbf{r}_{i})\coloneqq\prod_{j=1}^{m^{i}}P_ {\mathbf{z}_{j}^{i}}(w_{i,j}|\overline{\mathbf{w}}_{i}^{j-1},\overline{\mathbf{ r}}_{i}^{j-1}\backslash\mathbf{z}_{j}^{i})\) for \(i\in\{1,2\}\) where \(\mathtt{seq}^{i}\coloneqq(\mathbf{z}_{j}^{i})_{j=1}^{m^{i}}\). Let \(\mathbf{W}\coloneqq\mathbf{W}_{1}\backslash\mathbf{W}_{2}\). Let \(\mathbf{R}\coloneqq(\mathbf{R}_{1}\cup\mathbf{W}_{2})\cap\mathit{pre}(\mathbf{W};G)\). Assume the following: (1) \(\mathbf{W}_{2}\subseteq\mathbf{W}_{1}\); and (2) \(\forall W_{j}\in\mathbf{W},\exists W_{1,k}\in\mathbf{W}_{1}\) such that \((\overline{\mathbf{W}}^{j-1},\overline{\mathbf{R}}^{j-1})=(\overline{\mathbf{W}}_ {1}^{k-1},\overline{\mathbf{R}}_{1}^{k-1})\), \(\mathbf{Z}_{i,k}=\mathbf{Z}_{j}\) and \(\mathbf{z}_{i,k}=\mathbf{z}_{j}\). Then,_

\[A_{0}^{1}/A_{0}^{2}=A_{0}[\mathbf{W},\emptyset,\mathbf{R};\mathbb{Z}_{1}, \mathtt{seq}^{1}](\mathbf{w},\mathbf{r})=\prod_{j:W_{j}\in\mathbf{W}}P_{\mathbf{z}_{j }}(w_{j}|\overline{\mathbf{w}}^{j-1},\overline{\mathbf{r}}^{j-1}\backslash\mathbf{z}_{j}). \tag{5}\]This lemma provides a graphical criterion where a product \(A_{0}^{1}/A_{0}^{2}\) is given as a g-mSBD operator.

We have rewritten the identification algorithm proposed by Lee et al. (2019) as Algo. 1 to express the g-identifiable causal effect as a combination of marginalizations, multiplications, and divisions of g-mSBD. It's worth of noting that the identification algorithm proposed by Lee et al. (2019) and Algo. 1 are equivalent.

**Theorem 1** (Expression of g-Identifiable Causal Effects).: _Algo. 1 returns any g-identifiable causal effects as a function of a set \(\{A_{0}^{k}\}\) of g-mSBD adjustment operators in the form_

\[P(\mathbf{y}|do(\mathbf{x}))=f(\{A_{0}^{k}\}_{k=1}^{K}), \tag{6}\]

_where the function \(f(\cdot)\) applies marginalization, multiplication, or division over g-mSBD operators in \(\{A_{0}^{k}\}\) as specified by Algo. 1._

For concreteness, we demonstrate the application of Algo. 1 for Figs. ((a,b)), where the effects \(P(\mathbf{y}|do(\mathbf{x}))\) are g-identifiable. Detailed and visually friendly demonstrations are described in Appendix A.

**Example 3** (Application of Algo. 1 to Example 1).: Note \(\mathbb{Z}=\{\emptyset,Z\}\). **Line 3-4: \(\mathbf{D}=\{Z,Y\}\)** where \(\mathbf{D}_{1}\coloneqq\{Z\}\) and \(\mathbf{D}_{2}\coloneqq\{Y\}\). **Line 5-13:** Identify \(Q[\mathbf{D}_{1}]\) from \(\mathbf{Z}_{1}=\emptyset\) as follow. Note \(\mathbf{D}_{1}\subseteq\mathbf{S}^{0}\), where \(\mathbf{S}^{0}\coloneqq\mathbf{V}\) where \(Q[\mathbf{S}^{0}]=A_{0}[\mathbf{S}^{0},\emptyset,\emptyset;\mathbb{Z}^{0} \coloneqq\emptyset,\emptyset][\mathbf{s}^{0},\emptyset)=P(\mathbf{v})\). Run \(Q[\mathbf{D}_{1}]=\texttt{subID}(\mathbf{D}_{1},\mathbf{S}^{0},Q[\mathbf{S}^ {0}],G)\) and obtain \(Q[\mathbf{D}_{1}]=A_{0}^{1}\coloneqq A_{0}[\{W,Z\},W,X;\emptyset,\emptyset](z, x)=\sum_{w\in\mathbf{S}^{0}_{W}}P(z|x,w)P(w)\). Lemmas (2,3,4) are used in running the sub-procedure. Now, identify \(Q[\mathbf{D}_{2}]\) from \(\mathbf{Z}_{2}=\{Z\}\) as follow. Note \(\mathbf{D}_{2}\subseteq\mathbf{S}^{1}\coloneqq\{W,X,Y\}\), the c-component in \(G(\mathbf{V}\backslash Z)\). Note \(Q[\mathbf{S}^{1}]=A_{0}[\mathbf{S}^{1},\emptyset,\emptyset;\mathbb{Z}^{1} \coloneqq\{Z\},\texttt{seq}^{1}][\mathbf{s}^{1},\emptyset)=P_{z}(w,x,y)\), where \(\texttt{seq}^{1}=(z,z,z)\). We run \(Q[\mathbf{D}_{2}]=\texttt{subID}(\mathbf{D}_{2},\mathbf{S}^{1},Q[\mathbf{S}^ {1}],G(\mathbf{S}^{1}))\), and obtain \(Q[\mathbf{D}_{2}]=A_{0}^{2}\coloneqq A_{0}[Y,\emptyset,\emptyset;\mathbb{Z}^ {1},\texttt{seq}^{1}][y,\emptyset)=P_{z}(y)\). Lemma 2 is used in the sub-procedure. **Line 14-15: \(P(y|do(x))=\sum_{z\in\mathbf{S}^{x}_{2}}A_{0}^{1}A_{0}^{2}\)**. 

**Example 4** (Application of Algo. 1 to Example 2).: Note \(\mathbb{Z}=\{X_{1},X_{2}\}\). **Line 3-4: \(\mathbf{D}=\{R,W,Y\}\)** where \(\mathbf{D}_{1}\coloneqq\{R\}\), \(\mathbf{D}_{2}\coloneqq\{W\}\), and \(\mathbf{D}_{3}=\{Y\}\). **Line 5-13:** In \(G(\mathbf{V}\backslash X_{1})\), \(\mathbf{D}_{1}=\mathbf{S}^{1}_{1}\coloneqq\{R\}\). \(Q[\mathbf{D}_{1}]=Q[\mathbf{S}^{1}_{1}]=A_{0}^{1}\coloneqq A_{0}[R,\emptyset,X _{2};\mathbb{Z}^{1}\coloneqq\{X_{1}\},\texttt{seq}^{1}](r,x_{2})=P(r|do(x_{1} ),x_{2})\), where \(\texttt{seq}^{1}=(x_{1})\). In \(G(\mathbf{V}\backslash X_{1})\), \(\mathbf{D}_{2}\subseteq\mathbf{S}^{1}_{2}\coloneqq\{X_{2},W,Y\}\). \(Q[\mathbf{S}^{1}_{2}]=A_{0}[\mathbf{S}^{1}_{2},\emptyset,R;\mathbb{Z}^{2} \coloneqq\{X_{1}\},\mathbf{seq}^{2}](\mathbf{s}^{1}_{2},r)=P_{x_{1}}(x_{2})P_ {x_{1}}(w|x_{2},r)P_{x_{1}}(y|x_{2},w,r)\) where \(\texttt{seq}^{2}=(x_{1},x_{1},x_{1})\). Run \(Q[\mathbf{D}_{2}]=\texttt{subID}(\mathbf{D}_{2},\mathbf{S}^{1}_{2},Q[\mathbf{S }^{1}_{2}],G(\mathbf{S}^{1}_{2}))=A_{0}^{2}\coloneqq A_{0}[\{X_{2},W\},X_{2},R;\mathbb{Z}^{2},\texttt{seq}^{2}](w,r)=\sum_{x^{\prime}_{2}\in\mathbf{S}^{x} _{2}}P_{x_{1}}(w|r,x^{\prime}_{2})P_{x_{1}}(x^{\prime}_{2})\) where \(\texttt{seq}^{2}=(x_{1},x_{1})\). Lemma 2 is used in the sub-procedure. Since \(\texttt{subID}(\mathbf{D}_{3},\mathbf{S}^{1}_{2},Q[\mathbf{S}^{1}_{2}],G( \mathbf{S}^{1}_{2}))\) return FAIL, we find the c-component \(\mathbf{S}^{1}_{2}\coloneqq\{Y\}\) where \(\mathbf{D}_{3}=\mathbf{S}^{1}_{2}\). Note \(Q[\mathbf{D}_{3}]=Q[\mathbf{S}^{1}_{2}]=A_{0}^{3}\coloneqq A_{0}[Y,\emptyset, \{R,W\};\mathbb{Z}^{3}\coloneqq\{X_{2}\},\texttt{seq}^{3}](y,\{r,w\})=P_{x_{2} }(y|w,r)\), where \(\texttt{seq}^{3}=(x_{2})\). **Line 14-15:** Applying Lemma 3, \(A_{0}^{13}\coloneqq A_{0}^{1}\times A_{0}^{3}=A_{0}[\{R,Y\},\emptyset,\{X_{2},W\};\mathbb{Z}^{13}\coloneqq\{X_{1},X_{2}\},\texttt{seq}^{13}](\{r,y\},\{x_{2},w\})=P_{x_{1}}(r|x_{2},P_{x_{2}}(y|r,w)\), where \(\texttt{seq}^{13}=(x_{1},x_{2})\). Then, \(P(y|do(x_{1},x_{2}))=\sum_{r,w\in\mathbf{S}_{R,W}}A_{0}^{13}A_{0}^{2}\). 

## 3 Estimating g-Identifiable Causal Effects

In this section, we develop an estimator for \(P(\mathbf{y}|do(\mathbf{x}))\) using samples \(\mathbb{D}\coloneqq\{D_{\sigma(\mathbf{Z}_{i})}\sim P_{\sigma(\mathbf{Z}_{i} )}(\mathbf{V})\in\mathbb{P}\}\) obtained from randomized experiments and observations (where \(\mathbf{Z}_{i}=\emptyset\)). We use \(P_{\sigma(\mathbf{Z})}\) instead of \(P_{\mathbf{z}}\) to highlight the distribution \(D_{\sigma(\mathbf{Z}_{i})}\in\mathbb{D}\) follows.

We first introduce an estimator for the g-mSBD adjustment that exhibits the doubly robust property. The nuisance parameters for the g-mSBD adjustment are defined as follows:

**Definition 2** (**Nuisances for g-mSBD)**.: _Nuisances for g-mSBD \(A_{0}\) in Eq. (1) are \(\{\mu_{0}^{i+1},\pi_{0}^{i}\}_{i=1}^{m-1}\) defined as follows. Let \(\mu_{0}^{m+1}=\mu^{m+1}\coloneqq\mathbbm{1}_{\mathbf{w}\backslash\mathbf{c}}( \mathbf{W}\backslash\mathbf{C})\). For \(i=m-1,\cdots,1\),_

\[\mu_{0}^{i+1}(\overline{\mathbf{W}}^{i},\overline{\mathbf{R}}^{1: i}) \coloneqq\mathbb{E}_{P_{\sigma(\mathbf{Z}_{i+1})}}\left[\mu_{0}^{i+2}(\overline{ \mathbf{W}}^{i+1},\mathbf{r}_{i+1},\overline{\mathbf{R}}^{1:i})|\overline{ \mathbf{W}}^{i},\overline{\mathbf{R}}^{1:i},\mathbf{r}_{0},\mathbf{z}_{i+1}\right] \tag{7}\] \[\pi_{0}^{i}(\overline{\mathbf{W}}^{i},\overline{\mathbf{R}}^{1: i}) \coloneqq\frac{P_{\sigma(\mathbf{Z}_{i})}(\overline{\mathbf{W}}^{i},\overline{\mathbf{R}}^{1: i-1}|\mathbf{z}_{i},\mathbf{r}_{0})}{P_{\sigma(\mathbf{Z}_{i+1})}(\overline{\mathbf{W}}^{i}, \overline{\mathbf{R}}^{1:i-1}|\mathbf{z}_{i+1},\mathbf{r}_{0})}\frac{\mathbbm{1}_{ \mathbf{r}_{i}}(\mathbf{R}_{i})}{P_{\sigma(\mathbf{Z}_{i+1})}(\mathbf{R}_{i}| \overline{\mathbf{W}}^{i},\overline{\mathbf{R}}^{1:i-1},\mathbf{z}_{i+1},\mathbf{r} _{0})}. \tag{8}\]

**Remark 1** (**Simplification of Nuisances**).: _Although the nuisances \(\pi_{0}^{i}\) may seem complicated, they can be simplified in several important special cases. For example, \(\pi_{0}^{i}=\mathbbm{1}_{\mathbf{r}_{i}}(\mathbf{R}_{i})/P_{\sigma(\mathbf{Z})}( \mathbf{R}_{i}|\overline{\mathbf{W}}^{i},\overline{\mathbf{R}}^{i-1},\mathbf{z}, \mathbf{r}_{0})\) if \(\mathbb{Z}=\{\mathbf{Z}\}\) for any \(\mathbf{Z}\subseteq\mathbf{V}\) where \(\mathbf{Z}\) is possibly empty._In general, employing off-the-shelf classification methods for density ratio estimation is feasible, leveraging the techniques outlined in Section 5.4 of Diaz et al. (2021).

We now introduce a g-mSBD estimator exhibiting the robustness properties using these nuisances. This estimator is motivated by the double/debiased machine-learning style estimators (Chernozhukov et al., 2018, 2022):

**Definition 3** (**DR-g-mSBD Estimators)**.: Let \(D_{\sigma(\mathbf{Z}_{i})}\) for \(\mathbf{Z}_{i}\in\mathbb{Z}\) denote the experimental samples from randomizing the variable \(\mathbf{Z}_{i}\). Let \(\overline{D}_{\mathbf{z}_{i}}\) for \(\mathbf{z}_{i}\in\mathfrak{D}_{\mathbf{Z}_{i}}\) denote the subsamples of \(D_{\sigma(\mathbf{Z}_{i})}\) fixing \(\mathbf{R}_{0}\backslash\mathbf{Z}_{i}=\mathbf{r}_{0}\backslash\mathbf{z}_{i}\) and \(\mathbf{Z}_{i}=\mathbf{z}_{i}\). The DR-g-mSBD estimator \(\hat{A}\) for the g-mSBD adjustment \(A_{0}[\mathbf{W},\mathbf{C},\mathbf{R};\mathbb{Z}_{0}\coloneqq\{\mathbf{Z}_{i }\}_{i=1}^{m},\mathtt{seq}\coloneqq(\mathbf{z}_{i})_{i=1}^{m}](\mathbf{w} \backslash\mathbf{c},\mathbf{r})\) is defined as follows:

1. Randomly partition \(\overline{D}_{\mathbf{z}_{i}}\) into \(\{\overline{D}_{\mathbf{z}_{i},\ell}\}_{\ell\in[L]}\); i.e., \(\overline{D}_{\mathbf{z}_{i}}=\cup_{\ell=1}^{L}\overline{D}_{\mathbf{z}_{i}, \ell}\), \(\forall\mathbf{Z}_{i}\in\mathbb{Z}\) and \(\mathbf{z}_{i}\in\mathfrak{D}_{\mathbf{Z}_{i}}\).
2. For each fold \(\ell\in[L]\), let \(\mu_{\ell}^{i+1}\) denote learned \(\mu_{0}^{i+1}\) using \(\overline{D}_{\mathbf{z}_{i+1}}\backslash\overline{D}_{\mathbf{z}_{i+1},\ell}\) for \(i=m,\cdots,2\); and \(\pi_{\ell}^{i}\) learned \(\pi_{0}^{i}\) for \(i=1,\cdots,m-1\). Define \(\hat{\mu}_{\ell}^{i+1}\coloneqq\mu_{\ell}^{i+1}(\overline{\mathbf{W}}^{i}, \mathbf{r}_{i},\overline{\mathbf{R}}^{1:i-1})\) and \(\overline{\pi}_{\ell}^{i}\coloneqq\prod_{j=1}^{i}\pi_{\ell}^{j}\).
3. Estimate \(\hat{A}\coloneqq\hat{A}(\{\mu_{\ell}^{j+1},\pi_{\ell}^{j}\}_{j\in[m-1],\ell\in [L]})\coloneqq(1/L)\sum_{\ell=1}^{L}\hat{A}_{\ell}(\{\mu_{\ell}^{j+1},\pi_{ \ell}^{j}\}_{j\in[m-1]})\) where \[\hat{A}_{\ell}\coloneqq\hat{A}_{\ell}(\{\mu_{\ell}^{j+1},\pi_{\ell}^{j}\}_{j \in[m-1]})\coloneqq\sum_{j=1}^{m-1}\mathbb{E}_{\overline{D}_{\mathbf{z}_{j+1},\ell}}\left[\overline{\pi}_{\ell}^{j}\{\hat{\mu}_{\ell}^{j+2}-\mu_{\ell}^{j+1 }\}\right]+\mathbb{E}_{\overline{D}_{\mathbf{z}_{1},\ell}}\left[\bar{\mu}_{ \ell}^{2}\right],\] (9) where \(\mathbb{E}_{\overline{D}_{\mathbf{z}_{j},\ell}}\). \([\cdot]\) is an empirical average over samples \(\overline{D}_{\mathbf{z}_{j},\ell}\).

We now analyze the doubly robustness property of this estimator.

**Proposition 1** (**Asymptotic Analysis of g-mSBD Estimators)**.: _Assume that the nuisance estimates \(\mu_{\ell}^{i}\) and \(\pi_{\ell}^{i}\) are \(L_{2}\)-consistent; i.e., \(\|\mu_{\ell}^{i+1}-\mu_{0}^{i+1}\|_{P_{\sigma(\mathbf{Z}_{i+1})}}=o_{P_{\sigma (\mathbf{z}_{i+1})}}(1)\), \(\|\hat{\mu}_{\ell}^{i+2}-\hat{\mu}_{0}^{i+2}\|_{P_{\sigma(\mathbf{z}_{i+1})}}= o_{P_{\sigma(\mathbf{z}_{i+1})}}(1)\) and \(\|\pi_{\ell}^{i}-\pi_{0}^{i}\|_{P_{\sigma(\mathbf{z}_{i+1})}}=o_{P_{\sigma( \mathbf{z}_{1})}}(1)\). Let \(n_{i}\coloneqq\left|\overline{D}_{\mathbf{z}_{i}}\right|\) for \(i\in\{1,\cdots,m\}\). Then,_

\[\hat{A}-A_{0}=\sum_{i=1}^{m}R_{i}+\frac{1}{L}\sum_{\ell=1}^{L}\sum_{i=1}^{m-1 }O_{P_{\sigma(\mathbf{z}_{i+1})}}\left(\|\mu_{\ell}^{i+1}-\mu_{0}^{i+1}\|\|\pi_ {\ell}^{i}-\pi_{0}^{i}\|\right), \tag{10}\]

_where \(R_{i}\) is a random variable such that \(n_{i}^{1/2}R_{i}\) converges in distribution to a mean-zero normal random variable._

This estimator possesses a doubly robustness property since the estimator is bounded in probability at rate \(n^{-1/2}\) (for \(n\coloneqq\min\{n_{1},\cdots,n_{m}\}\), whenever \(O_{P_{\sigma(\mathbf{z}_{i+1})}}\left(\|\mu_{\ell}^{i+1}-\mu_{0}^{i+1}\|\|\pi_{ \ell}^{i}-\pi_{0}^{i}\|\right)=O_{P}(n_{i+1}^{-1/2})\) for all \(i\).

We now construct an estimator for the g-identification expression using the DR-g-mSBD estimator defined in Def. 3. The resulting estimator is called the MR-gID estimator:

**Definition 4** (**MR-gID Estimator)**.: The MR-gID estimator \(\hat{\psi}\) for the identification expression of the causal effect \(\psi_{0}\coloneqq f(\{A_{0}^{k}\}_{k=1}^{K})\) in Theorem 1 is given as follows: For each \(A_{0}^{k}\) composing \(f(\{A_{0}^{k}\}_{k=1}^{K})\), let \(\hat{A}^{k}\coloneqq\hat{A}^{k}(\{\mu_{k,\ell}^{j+1},\pi_{k,\ell}^{j}\}_{j\in[m ^{k}-1],\ell\in[L]})\) denote the DR-g-mSBD estimator with nuisance estimates \(\{\mu_{k,\ell}^{j+1},\pi_{k,\ell}^{j}\}\) for the true nuisances \(\{\mu_{k,0}^{j+1},\pi_{k,0}^{j}\}\). Then,

\[\hat{\psi}\coloneqq f(\{\hat{A}^{k}\}_{k=1}^{K}). \tag{11}\]

We impose assumptions on the identification expression and its nuisances for further analysis.

**Assumption 2** (**Analysis of MR-gID )**.: _The identification function \(f(\{A^{k}\}_{k=1}^{K})\) in Thm. 1 and each nuisances \(\{\mu_{k,\ell}^{i+1},\pi_{k,\ell}^{i}\}_{k,\ell}\) for \(\hat{A}^{k}\) satisfy the following properties:_

1. _Twice differentiability:_ \(f(\{A^{k}\}_{k=1}^{K})\) _is twice continuously Frechet differentiable w.r.t._ \(\{A^{k}\}_{k=1}^{K}\) _w.r.t._ \(\{A^{k}\}_{k=1}^{K}\)2. _Boundedness_: \(\forall k\in[K]\) and \(\forall\mathbf{Z}_{i}\in\mathbb{Z}\), \(\nabla_{\alpha^{k}}f(\{A^{j}_{0}\}_{j=1}^{K})[\hat{A}^{k}-A^{k}_{0}]=O_{P_{\sigma (\mathbf{Z}_{i})}}(\hat{A}^{k}-A^{k}_{0})\).
3. \(L_{2}\)**-Consistency**: \(\|\mu^{i+1}_{k,\ell}-\mu^{i+1}_{k,0}\|_{P_{\sigma(\mathbf{Z}^{k}_{i+1})}}=o_{P_ {\sigma(\mathbf{Z}^{k}_{i+1})}}(1)\), \(\|\hat{\mu}^{i+2}_{k,\ell}-\hat{\mu}^{i+2}_{k,0}\|_{P_{\sigma(\mathbf{Z}^{k}_{ i+1})}}=o_{P_{\sigma(\mathbf{Z}^{k}_{i+1})}}(1)\), \(\|\pi^{i}_{k,\ell}-\pi^{i}_{k,0}\|_{P_{\sigma(\mathbf{Z}^{k}_{i+1})}}=o_{P_{ \sigma(\mathbf{Z}^{k}_{i+1})}}(1)\), and \(\|\hat{\mu}^{2}_{k,\ell}-\hat{\mu}^{2}_{k,0}\|_{P_{\sigma(\mathbf{Z}^{k}_{i}) }}=o_{P_{\sigma(\mathbf{Z}^{k}_{i})}}(1)\).

Assumption 2 is imposed to limit the error of the MR-gID, which is a linear function of the errors of each DR-g-mSBD estimator.

**Theorem 2** (Asymptotic Analysis of MR-gID).: _Suppose Assumption 2 holds. Let \(n_{k,i}\coloneqq|\overline{D}_{\mathbf{z}^{k}_{i}}|\) for \(\mathbf{Z}^{k}_{i}\in\mathbb{Z}\) and \(\mathbf{z}^{k}_{i}\in\mathfrak{D}_{\mathbf{Z}^{k}_{i}}\). Let \(\hat{\psi}\) denote the MR-gID estimator in Def. 4 for the causal effect \(\psi_{0}\coloneqq f(\{A^{k}_{0}\}_{k=1}^{K})\) in Theorem 1. Then, the error of \(\hat{\psi}\) is given as_

\[\hat{\psi}-\psi_{0}=\sum_{k=1}^{K}\sum_{i=1}^{m^{k}}O_{P_{\sigma( \mathbf{Z}^{k}_{i})}}(n_{k,i}^{-1/2})+\frac{1}{L}\sum_{k=1}^{K}\sum_{\ell=1}^{ L}\sum_{i=1}^{m^{k-1}}O_{P_{\sigma(\mathbf{Z}^{k}_{i})}}(\|\mu^{i+1}_{k,\ell}- \mu^{i+1}_{k,0}\|\|\pi^{i}_{k,\ell}-\pi^{i}_{k,0}\|). \tag{12}\]

We highlight that the MR-gID \(\hat{\psi}\) exhibits robustness property since \(\hat{\psi}-\psi_{0}\) for \(\psi_{0}=P(\mathbf{y}|do(\mathbf{x}))\) is bounded at rate \(n^{-1/2}\) (for \(n=\min\{n_{k,i}\}\) and \(P\in\mathbb{P}\)) even when all nuisances \(\{\mu^{i+1}_{k,\ell},\pi^{i}_{k,\ell}\}\) are bounded at slower \(n^{-1/4}\) rate. Furthermore, the MR-gID estimator exhibits multiply robustness, as the error of the MR-gID is a linear function of the error of DR-g-mSBD, which demonstrates the doubly robustness property. Formally,

**Corollary 2** (**Multiply Robustness** (Corollary of Thm. 2)).: _Suppose (1) Assumption 2 holds; (2) Either \(\pi^{i}_{k,\ell}=\pi^{i}_{k,0}\) or \(\mu^{j}_{k,\ell}=\mu^{j}_{k,0}\) for \(j=i+1,\cdots,m^{k}\) for all \(i,\ell,k\); and (3) all nuisances \(\{\pi^{i}_{k,\ell},\mu^{i+1}_{k,\ell}\}_{i,\ell,k}\) are bounded by some constant. Then, the MR-gID \(\hat{\psi}\) (Def. 4) is consistent to \(\psi_{0}\)._

For concreteness, we illustrate the application of Thm. 2 for Examples (1, 2). Detailed procedures are provided in Appendix A.

**Example 5** (Application of Thm. 2 to Example 1).: _Recall that \(P(y|do(x))=f(\{A^{1}_{0},A^{2}_{0}\})\coloneqq\sum_{z\in\mathfrak{S}_{Z}}A^{1} _{0}A^{2}_{0}\). The nuisance set for \(A^{1}_{0}\) is \(\mu^{2}_{1,0}(X,W)\coloneqq\mathbb{E}_{P}\left[\mathbb{1}_{z}(\bar{Z})|X,W\right]\) and \(\pi^{1}_{1,0}(X,W)\coloneqq\mathbb{1}_{x}(X)/P(X|W)\). Then, the estimator for \(A^{1}_{0}\) is \(\hat{A}^{1}\coloneqq\hat{A}^{1}(\{\mu^{1}_{1,\ell},\pi^{1}_{1,\ell}\}_{\ell\in[ L]})\) defined in Def. 3. The nuisance set for \(A^{2}_{0}\) is \(\mu^{2}_{2,0}\coloneqq\mathbb{E}_{P_{\sigma(Z)}}\left[\mathbb{1}_{y}(Y)\right]\). Then, the estimator for \(A^{2}_{0}\) is \(\hat{A}^{2}\coloneqq\hat{A}^{2}(\{\mu^{2}_{2,\ell},\pi^{1}_{\ell\in[L]}\}\). Then, the estimator is constructed as Def. 4, as \(f(\{\hat{A}^{1},\hat{A}^{2}\})\). By Thm. 2, the error of the estimator is \(O_{P}(n_{0}^{-1/2})+O_{P}(n_{s}^{-1/2})+(1/L)\sum_{\ell=1}^{L}O_{P}(\|\mu^{2 }_{1,\ell}-\mu^{2}_{1,0}\|\|\pi^{1}_{1,\ell}-\pi^{1}_{1,0}\|)\), where \(n_{0}\coloneqq|D|\) and \(n_{z}\coloneqq|D_{z}|\) where \(D\sim P\) and \(D_{z}\sim P_{z}\)._

**Example 6** (**Application of Thm. 2 to Example 2)**.: _Recall that \(P(y|do(x_{1},x_{2}))=f(\{A^{2}_{0},A^{13}_{0}\})=\sum_{r,w\in\mathfrak{S}_{R,W} }A^{2}_{0}A^{13}_{0}\). The nuisance set for \(A^{2}_{0}\) is \(\mu^{2}_{2,0}\coloneqq\mathbb{E}_{P_{z_{1}}}\left[\mathbb{1}_{w}(W)|R,X_{2}\right]\) and \(\pi^{1}_{1,0}\coloneqq\mathbb{1}_{r}(R)/P_{x_{1}}(R|X_{2})\). Then, the estimator for \(A^{2}_{0}\) is \(\hat{A}^{2}\coloneqq\hat{A}^{2}(\{\mu^{2}_{2,\ell},\pi^{1}_{1,\ell}\}_{\ell\in[ L]})\). The nuisance set for \(A^{13}_{0}\) is \(\mu^{2}_{13,0}\coloneqq\mathbb{E}_{P_{z_{2}}}\left[\mathbb{1}_{r,y}(R,Y)|R,W\right]\) and \(\pi^{1}_{13,0}\coloneqq\frac{P_{\sigma(X_{1})}(R|x_{2},x_{1})}{P_{\sigma(X_{2}) }(R|x_{2})}\frac{1_{w}(W)}{P_{\sigma(X_{2})}(W|R,x_{2})}\). Then, the estimator is \(\hat{A}^{13}=\hat{A}^{13}(\{\mu^{2}_{13,0},\pi^{1}_{13,0}\}_{\ell\in[L]})\). Then, the estimator for \(A^{13}\) is \(f(\{\hat{A}^{13},\hat{A}^{2}\})\). By Thm. 2, the error of the estimator is \(O_{P_{\sigma(X_{1})}}(n_{1}^{-1/2})+O_{P_{\sigma(X_{2})}}(n_{2}^{-1/2})+(1/L) \sum_{\ell=1}^{L}\{O_{P_{\sigma(X_{1})}}(\|\mu^{2}_{2,\ell}-\mu^{2}_{2,0}\|\| \pi^{2}_{2,\ell}-\pi^{2}_{2,0}\|)+O_{P_{\sigma(X_{2})}}(\|\mu^{2}_{13,\ell}- \mu^{2}_{13,0}\|\|\pi^{1}_{13,\ell}-\pi^{1}_{13,0}\|)\}\), where \(n_{1}\coloneqq|D_{1}|\) and \(n_{2}\coloneqq|D_{2}|\) where \(D_{1}\sim P_{x_{1}}\) and \(D_{2}\sim P_{x_{2}}\)._

## 4 Experiments

In this section, we demonstrate the MR-gID estimator from Definition (4) through Examples (1,2) and Project STAR dataset[14, 15]. For each example, the proposed estimator is constructed using a dataset \(\mathbb{D}\coloneqq\{D_{\mathbf{Z}_{i}},\ \mathbf{Z}_{i}\in\mathbb{Z}\}\) simulated from an underlying SCM. Our goal is to provide empirical evidence of the fast convergence behavior and the robustness property of the proposed estimator compared to competing baseline estimators. We consider two standard baselines in the literature: the'regression-based estimator (reg)' only uses the regression nuisance parameters \(\mu^{2}\) for \(\mu^{2}_{0}\) defined in Def. 2, and the 'probability weighting-based estimator (pw)' only uses the probability weighting parameters \(\pi^{m-1}\) for \(\pi^{m-1}_{0}\) defined in Def. 2, while our MR-gID uses both in estimating the g-mSBD operators \(A^{k}\) composing \(f(\{A^{k}\})\) in Thm. 1. Details of the regression-based ('reg') and the probability weighting-based ('pw') estimators are provided in Appendix A. The details of the simulation are in Appendix (D, E).

### Synthetic Dataset Analysis

**Accuracy Measure.** We compare the proposed estimator ('mr') in Def. 4 to the regression-based estimator ('reg') and the probability weighting-based estimator ('pw'). In particular, we use \(T^{\text{est}}(\mathbf{x})\) for \(\text{est}\in\{\text{reg},\text{pw},\text{mr}\}\) to denote the g-ID estimators that leverage regression-based ('reg'), probability weighting-based ('pw'), and MR-gID in estimating each operator \(A^{k}\) in the identification expression \(f(\{A^{k}\})\) of the causal effect \(P(\mathbf{y}|do(\mathbf{x}))\). We assess the quality of the estimators by computing the _average absolute error_\(\text{AAE}^{\text{est}}\coloneqq\frac{1}{|\mathfrak{D}_{\mathbf{X}}|}\sum_{ \mathbf{x}\in\mathfrak{D}_{\mathbf{X}}}|T^{\text{est}}(\mathbf{x})-P(\mathbf{ y}|do(\mathbf{x}))|\) where \(|\mathfrak{D}_{\mathbf{X}}|\) is the cardinality of \(\mathfrak{D}_{\mathbf{X}}\). Nuisance functions are estimated using gradient boosting models called XGBoost (Chen and Guestrin, 2016). We ran 100 simulations for each \(n=\{200,400,600,800,1000\}\) for \(n\coloneqq|D_{\mathbf{Z}}|\) for \(\forall\mathbf{Z}\subseteq\mathbb{Z}\). We label the box-plot for these AAEs as 'AAE-plot'.

**Experimental Setup.** We evaluate the \(\text{AAE}^{\text{est}}\) for Examples (1,2) in four scenarios:

* **(Scenario 1)** There were no noises in estimating nuisances.
* **(Scenario 2)** We introduced a converging noise \(\epsilon\) in estimating the nuisance, decaying at a \(n^{-\alpha}\) rate (i.e., \(\epsilon\sim\text{Normal}(n^{-\alpha},N^{-2\alpha})\)) for \(\alpha=1/4\) to emphasize the errors induced by the finiteness of samples. This scenario is inspired by the experimental design discussed in (Kennedy, 2020).
* simulated by training the model with a random matrix having the same dimension as the input matrix.
* **(Scenario 4)** Nuisance \(\{\pi^{i}_{k,\ell}\}_{\ell,k,i}\) are estimated incorrectly as in Scenario 3.

In Scenario 1, we aim to show that all estimators \(T^{\text{reg}},T^{\text{pw}},T^{\text{mr}}\) are converging to the true causal quantity \(P(\mathbf{y}|do(\mathbf{x}))\). In Scenario 2, we aim to show that the MR-gID estimator exhibits fast convergence behavior compared to competing estimators. In Scenario (3,4), our goal is to highlight the multiply robustness property of the MR-gID estimator.

Figure 2: AAE Plots for Examples (1,2) for Scenarios {1,2,3,4} depicted in the Experimental Setup section. The \(x\)-axis and \(y\)-axis are the number of samples and AAE, respectively.

**Experimental Results.** The AAE plots for all scenarios are presented in Fig. 2. All the estimators ('reg', 'pw','mr') converge in Scenario 1 as the sample size grows. In Scenario 2, where the estimated nuisances are controlled to be bounded in probability at \(n^{-1/4}\) rate, the proposed MR-gID \(\hat{\psi}\) outperforms the other two estimators by achieving fast convergence. This result corroborates the robustness property in Thm. 2. In Scenarios (3,4), where the estimated nuisances for \(\{\mu^{i}\}_{i=2}^{m}\) or \(\{\pi^{i}\}_{i=1}^{m-1}\) are wrongly specified, the MR-gID estimator converges while other estimators fail to converge. This result corroborates the multiply robustness property in Coro. 2.

### Project STAR Dataset

This section provides an overview of the analysis using Project STAR dataset [25, 26]. Project STAR investigated the impact of teacher/student ratios on academic achievement for students in kindergarten through third grade. The dataset \(D\) includes class size (\(X_{1}\)), the academic outcome in kindergarten (\(W\)) for kindergarten, the academic outcome in second grade (\(R\)), class size (\(X_{2}\)), and the academic outcome for the third grade (\(Y\)). We assume that the SCM \(\mathcal{M}\) underlying Project STAR dataset \(D\) can be depicted in Figure 2(a). The target quantity is \(\mathbb{E}[Y|do(x_{1},x_{2})]\) where \(P_{x_{1},x_{2}}(y)=\sum_{r\in\mathfrak{D}_{R}}P_{x_{1}}(r)\sum_{x_{1}^{\prime },w\in\mathfrak{D}_{X_{1},W}}P_{x_{2}}(y|x_{1}^{\prime},w,r,x_{2})P_{x_{2}}(x _{1}^{\prime},w)\). The detailed procedures are in Appendix E.

**Experimental Setup.** We generate two datasets \(D_{1}\) and \(D_{2}\) from the original dataset \(D\) to demonstrate the gID estimation. \(D_{1}\) is a random subsample of \(D\) with only \(\{X_{1},W,R\}\) and follows \(P_{\sigma(X_{1})}(X_{1},W,R)\). \(D_{2}\) is constructed by resampling from \(D\) in a way that the confounding bias between \(X_{1}\) and \(W\), and \(X_{1}\) and \(Y\) presents, following \(P_{\sigma(X_{2})}(X_{1},W,X_{2},R,Y)\). We conducted 100 simulations by generating new instances of \(D_{1}\) and \(D_{2}\) to create the AAE plot. Estimators were constructed solely from \(D_{1}\) and \(D_{2}\), with \(D\) used exclusively to construct the ground-truth estimate.

**Experimental Results.** We evaluated the AAE(r) of estimators \(T^{\text{sat}}\) for \(\text{est}\in\{\text{reg},\text{pw},\text{mr}\}\). The AAE plots for scenarios (2,3,4) are in Figs. 2(b,c,d). Our findings indicate that the MR-gID estimator \(T^{\text{mr}}\) consistently provided reliable estimates for the ground-truth quantity.

## 5 Conclusions

We present a framework for estimating the causal effect \(P(\mathbf{y}|do(\mathbf{x}))\) by combining multiple observational and experimental datasets and a causal graph \(G\). We introduce the generalized multi-outcome sequential back-door adjustment (g-mSBD) operator (Def. 1) and its operations. We show that any g-identifiable causal effects can be expressed as a function of the g-mSBD operators as specified in Algo. 1 (Thm. 1). We then develop an estimator called DR-g-mSBD (Def. 3) for the g-mSBD operator and analyze its statistical properties in Prop. 1. Based on the DR-g-mSBD estimator, we develop the MR-gID estimator (Def. 4) and analyze its statistical properties (Thm. 2 and Coro. 2) which exhibits fast convergence and multiply-robustness. Our experimental results demonstrate that the MR-gID estimator is a consistent and robust estimator of \(P(\mathbf{y}|do(\mathbf{x}))\) against model misspecification and slow convergence.

Figure 3: A graph and the AAE-plot for Project STAR.

## Acknowledgement

This research was supported in part by the NSF, ONR, AFOSR, DoE, Amazon, JP Morgan, and The Alfred P. Sloan Foundation.

## References

* Aijan and Grant (2006) Ramzi A Ajjan and Peter J Grant. Cardiovascular disease prevention in patients with type 2 diabetes: The role of oral anti-diabetic agents. _Diabetes and Vascular Disease Research_, 3(3):147-158, 2006.
* Bang and Robins (2005) Heejung Bang and James M Robins. Doubly robust estimation in missing data and causal inference models. _Biometrics_, 61(4):962-973, 2005.
* Bareinboim and Pearl (2012) Elias Bareinboim and Judea Pearl. Causal inference by surrogate experiments: z-identifiability. In _In Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence_, pages 113-120. AUAI Press, 2012.
* Bareinboim and Pearl (2016) Elias Bareinboim and Judea Pearl. Causal inference and the data-fusion problem. _Proceedings of the National Academy of Sciences_, 113(27):7345-7352, 2016.
* Bareinboim et al. (2022) Elias Bareinboim, Juan D Correa, Duligur Ibeling, and Thomas Icard. On pearls hierarchy and the foundations of causal inference. In _Probabilistic and causal inference: the works of judea pearl_, pages 507-556. 2022.
* Bhattacharya et al. (2022) Rohit Bhattacharya, Razieh Nabi, and Ilya Shpitser. Semiparametric inference for causal effects in graphical models with hidden variables. _Journal of Machine Learning Research_, 23:1-76, 2022.
* Bickel et al. (1993) Peter J Bickel, Chris AJ Klaassen, Peter J Bickel, Yaacov Ritov, J Klaassen, Jon A Wellner, and YA'Acov Ritov. _Efficient and adaptive estimation for semiparametric models_, volume 4. Johns Hopkins University Press Baltimore, 1993.
* Blanchard and Bruning (2015) Philippe Blanchard and Erwin Bruning. _Mathematical methods in Physics: Distributions, Hilbert space operators, variational methods, and applications in quantum physics_, volume 69. Birkhauser, 2015.
* Chen and Guestrin (2016) Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 785-794, 2016.
* Chernozhukov et al. (2018) Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters: Double/debiased machine learning. _The Econometrics Journal_, 21(1), 2018.
* Chernozhukov et al. (2022) Victor Chernozhukov, Juan Carlos Escanciano, Hidehiko Ichimura, Whitney K Newey, and James M Robins. Locally robust semiparametric estimation. _Econometrica_, 90(4):1501-1535, 2022.
* Crump et al. (2009) Richard K Crump, V Joseph Hotz, Guido W Imbens, and Oscar A Mitnik. Dealing with limited overlap in estimation of average treatment effects. _Biometrika_, 96(1):187-199, 2009.
* Diaz et al. (2021) Ivan Diaz, Nicholas Williams, Katherine L Hoffman, and Edward J Schenck. Nonparametric causal effects based on longitudinal modified treatment policies. _Journal of the American Statistical Association_, pages 1-16, 2021.
* Ferrannini and Cushman (2012) Ele Ferrannini and William C Cushman. Diabetes and hypertension: the bad companions. _The Lancet_, 380(9841):601-610, 2012.
* Gentzel et al. (2021) Amanda M Gentzel, Purva Pruthi, and David Jensen. How and why to use experimental data to evaluate methods for observational causal inference. In _International Conference on Machine Learning_, pages 3660-3671. PMLR, 2021.
* Glynn and Kashin (2017) Adam N Glynn and Konstantin Kashin. Front-door difference-in-differences estimators. _American Journal of Political Science_, 61(4):989-1002, 2017.
* Glynn and Kashin (2018)Lennart Hansson, Lars H Lindholm, Tord Ekbom, Bjorn Dahlof, Jan Lanke, Bengt Schersten, PO Wester, Thomas Hedner, Ulf de Faire, STOP-Hypertension-2 Study Group, et al. Randomised trial of old and new antihypertensive drugs in elderly patients: cardiovascular mortality and morbidity the swedish trial in old patients with hypertension-2 study. _The Lancet_, 354(9192):1751-1756, 1999.
* Hill (2011) Jennifer L Hill. Bayesian nonparametric modeling for causal inference. _Journal of Computational and Graphical Statistics_, 20(1):217-240, 2011.
* Huang and Valtorta (1999) Yimin Huang and Marco Valtorta. Identifiability in causal bayesian networks: A sound and complete algorithm. In _Proceedings of the national conference on artificial intelligence_, volume 21, page 1149. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.
* Jung et al. (2020a) Yonghan Jung, Jin Tian, and Elias Bareinboim. Estimating causal effects using weighting-based estimators. In _Proceedings of the 34th AAAI Conference on Artificial Intelligence_, 2020a.
* Jung et al. (2020b) Yonghan Jung, Jin Tian, and Elias Bareinboim. Learning causal effects via weighted empirical risk minimization. _Advances in Neural Information Processing Systems_, 33, 2020b.
* Jung et al. (2021a) Yonghan Jung, Jin Tian, and Elias Bareinboim. Estimating identifiable causal effects on markov equivalence class through double machine learning. In _Proceedings of the 38th International Conference on Machine Learning_, 2021a.
* Jung et al. (2021b) Yonghan Jung, Jin Tian, and Elias Bareinboim. Estimating identifiable causal effects through double machine learning. In _Proceedings of the 35th AAAI Conference on Artificial Intelligence_, 2021b.
* Jung et al. (2023) Yonghan Jung, Jin Tian, and Elias Bareinboim. Estimating joint treatment effects by combining multiple experiments. In _Proceedings of the 40th International Conference on Machine Learning_, 2023. URL [https://proceedings.mlr.press/v202/jung23c.html](https://proceedings.mlr.press/v202/jung23c.html).
* Kennedy (2020) Edward H Kennedy. Optimal doubly robust estimation of heterogeneous causal effects. _arXiv preprint arXiv:2004.14497_, 2020.
* Kennedy et al. (2020) Edward H Kennedy, Sivaraman Balakrishnan, Max GSell, et al. Sharp instruments for classifying compliers and generalizing causal effects. _Annals of Statistics_, 48(4):2008-2030, 2020.
* Krueger and Whitmore (2001) Alan B Krueger and Diane M Whitmore. The effect of attending a small class in the early grades on college-test taking and middle school test results: Evidence from project star. _The Economic Journal_, 111(468):1-28, 2001.
* Kumar et al. (2016) R Kumar, DM Kerins, and T Walther. Cardiovascular safety of anti-diabetic drugs. _European Heart Journal-Cardiovascular Pharmacotherapy_, 2(1):32-43, 2016.
* LaLonde (1986) Robert J LaLonde. Evaluating the econometric evaluations of training programs with experimental data. _The American economic review_, pages 604-620, 1986.
* Lee et al. (2019) Sanghack Lee, Juan D Correa, and Elias Bareinboim. General identifiability with arbitrary surrogate experiments. In _Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence_. AUAI Press, 2019.
* Louizos et al. (2017) Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling. Causal effect inference with deep latent-variable models. _Advances in neural information processing systems_, 30, 2017.
* Luedtke et al. (2017) Alexander R Luedtke, Oleg Sofrygin, Mark J van der Laan, and Marco Carone. Sequential double robustness in right-censored longitudinal models. _arXiv preprint arXiv:1705.02459_, 2017.
* Mises (1947) R v Mises. On the asymptotic distribution of differentiable statistical functions. _The annals of mathematical statistics_, 18(3):309-348, 1947.
* Pearl (1995) J. Pearl. Causal diagrams for empirical research. _Biometrika_, 82(4):669-710, 1995.
* Pearl (2000) Judea Pearl. _Causality: Models, Reasoning, and Inference_. Cambridge University Press, New York, 2000. 2nd edition, 2009.
* Pearl (2000)Judea Pearl and James Robins. Probabilistic evaluation of sequential plans from causal models with hidden variables. In _Proceedings of the Eleventh conference on Uncertainty in artificial intelligence_, pages 444-453, 1995.
* Robins (1986) James Robins. A new approach to causal inference in mortality studies with a sustained exposure periodapplication to control of the healthy worker survivor effect. _Mathematical modelling_, 7(9-12):1393-1512, 1986.
* Robins et al. (2009) James Robins, Lingling Li, Eric Tchetgen, and Aad W van der Vaart. Quadratic semiparametric von mises calculus. _Metrika_, 69:227-247, 2009.
* Robins and Rotnitzky (1995) James M Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression models with missing data. _Journal of the American Statistical Association_, 90(429):122-129, 1995.
* Rotnitzky et al. (2017) Andrea Rotnitzky, James Robins, and Lucia Babino. On the multiply robust estimation of the mean of the g-functional. _arXiv preprint arXiv:1705.08582_, 2017.
* Rotnitzky et al. (2021) Andrea Rotnitzky, Ezequiel Smucler, and James M Robins. Characterization of parameters with a mixed bias property. _Biometrika_, 108(1):231-238, 2021.
* Schanzenbach (2006) Diane Whitmore Schanzenbach. What have researchers learned from project star? _Brookings papers on education policy_, (9):205-228, 2006.
* Stock et al. (2003) James H Stock, Mark W Watson, et al. _Introduction to econometrics_, volume 104. Addison Wesley Boston, 2003.
* Tian and Pearl (2003) Jin Tian and Judea Pearl. On the identification of causal effects. Technical Report R-290-L, 2003.
* van der Laan and Gruber (2012) Mark J van der Laan and Susan Gruber. Targeted minimum loss based estimation of causal effects of multiple time point interventions. _The international journal of biostatistics_, 8(1), 2012.
* Van der Vaart (2000) Aad W Van der Vaart. _Asymptotic statistics_, volume 3. Cambridge university press, 2000.
* Vansteelandt et al. (2007) Stijn Vansteelandt, Andrea Rotnitzky, and James Robins. Estimation of regression models for the mean of repeated outcomes under nonignorable nonmonotone nonresponse. _Biometrika_, 94(4):841-860, 2007.
* Xia et al. (2023) K. Xia, Y. Pan, and E. Bareinboim. Neural causal models for counterfactual identification and estimation. In _The 11th International Conference on Learning Representations_, Feb 2023.
* Xia et al. (2021) Kevin Xia, Kai-Zhan Lee, Yoshua Bengio, and Elias Bareinboim. The causal-neural connection: Expressiveness, learnability, and inference. _Advances in Neural Information Processing Systems_, 34, 2021.
* Zhang and Bareinboim (2019) Junzhe Zhang and Elias Bareinboim. Near-optimal reinforcement learning in dynamic treatment regimes. _Advances in Neural Information Processing Systems_, 32, 2019.

**Supplement to "Estimating Causal Effects Identifiable from a Combination of Observations and Experiments"**

###### Contents

* 1 Introduction
	* 1.1 Preliminaries
	* 1.2 Problem Statement
* 2 Expressing Causal Effects as a Combination of mSBD Adjustments
* 3 Estimating g-Identifiable Causal Effects
* 4 Experiments
	* 4.1 Synthetic Dataset Analysis
	* 4.2 Project STAR Dataset
* 5 Conclusions
* A Further Details
* A.1 Example 3
* A.2 Example 4
* A.3 Example 5
* A.3.1 Specification of Nuisances
* A.3.2 Construction of Estimators
* A.4 Example 6
* A.4.1 Specification of Nuisances
* A.4.2 Construction of Estimators
* A.5 Details on Regression-based (REG) and Probability Weighting-based (PW) estimators.
* A.5.1 Regression-based Estimator
* A.5.2 Probability-weighting based Estimator
* B Proofs
* B.1 Proof of Lemma 1
* B.2 Proof of Lemma 2
* B.3 Proof of Lemma 3
* B.4 Proof of Lemma 4
* B.5 Proof of Theorem 1
* B.6 Proof of Proposition 1
* B.7 Proof of Theorem 2
* B.8 Proof of Corollary 2
* C Discussion
* C.1 Relaxation of Discreteness Assumption
* C.2 Sequential Doubly Robustness: \(2^{m-1}\) robustness versus \(m\)-robustness
* D Details of Experiments
* D.1 Designs of Simulations
* D.1.1 Example 1
* D.1.2 Example 2
* E Project STAR: Estimating Joint Effects of Class Sizes to Academic Outcomes
Further Details

We restate the notation here. To clarify the relationship between the experimental samples, where randomization is applied to \(\mathbf{Z}\subseteq\mathbf{V}\), and the distribution \(P_{\mathbf{z}}(\mathbf{V}\backslash\mathbf{z})\), we introduce the notation \(P_{\sigma(\mathbf{Z})}(\mathbf{V})\), where \(\sigma(\mathbf{Z})\) indicates that \(\mathbf{Z}\) has been randomized. The distribution \(P_{\sigma(\mathbf{Z})}(\mathbf{V})\) is derived from the Structural Causal Model (SCM), where the original equation \(Z\gets f_{Z}(pa_{z},u_{z})\) for \(Z\in\mathbf{Z}\) is replaced by a function that assigns a value to \(Z=z\) randomly, independent of other endogenous variables. For example, assigning \(Z=1\) and \(0\) with a probability of 0.5 each.

It should be noted that when considering observational data, \(P\coloneqq P_{\sigma(\emptyset)}\). For any sets \(\mathbf{A}\), \(\mathbf{B}\), and \(\mathbf{Z}\subseteq\mathbf{V}\), the interventional distribution can be represented as \(P(\mathbf{A}|\mathrm{do}(\mathbf{z}),\mathbf{B})=P_{\sigma(\mathbf{Z})}( \mathbf{A}|\mathbf{Z}=\mathbf{z},\mathbf{B})\) according to the definition of the do-operator and the \(P_{\sigma(\mathbf{Z})}\) distribution. To emphasize that the distribution is induced from randomization and conditioning on \(\mathbf{Z}=\mathbf{z}\), we use \(P_{\mathbf{z}}(\mathbf{A}|\mathbf{B})\coloneqq P_{\sigma(\mathbf{Z})}(\mathbf{ A}|\mathbf{Z}=\mathbf{z},\mathbf{B})\). The experimental samples obtained from randomization \(\sigma(\mathbf{Z})\) lead to samples \(D_{\sigma(\mathbf{Z})}\) that follow \(P_{\sigma(\mathbf{Z})}(\mathbf{V})\). We denote the subsample of \(D_{\sigma(\mathbf{Z})}\), where \(\mathbf{Z}=\mathbf{z}\) is fixed, as \(D_{\mathbf{z}}\), which follows \(P_{\mathbf{z}}(\mathbf{V})\). \(\blacksquare\)

### Example 3

We provide a detailed illustration of Example 3, demonstrating the application of Lemmas (2,3,4).

**Input:**\(\mathbf{x}=\{x\}\), \(\mathbf{y}\coloneqq\{y\}\), \(\mathbb{Z}\coloneqq\{\emptyset,Z\}\). The goal is to identify \(P(y|do(x))\) from \(\mathbb{P}\) which contains \(P\) and \(P_{\sigma(Z)}(\mathbf{V})\). In the identification, \(P(\mathbf{V})\) and \(P_{z}(\mathbf{V}|Z)\coloneqq P_{\sigma(Z)}(\mathbf{V}|z)\) for \(z\in\mathfrak{D}_{Z}\) are used.

**Line 3-4**: Since \(\mathbf{V}\backslash\mathbf{X}=\{Z,Y\}\), \(\mathbf{D}=an(Y)_{G(Z,Y)}=\{Z,Y\}\). Let \(\mathbf{D}_{1}\coloneqq\{Z\}\) and \(\mathbf{D}_{2}\coloneqq\{Y\}\).

We now run **Line 5-13**. We first run \(\mathbf{D}_{1}=\{Z\}\) and \(\mathbf{Z}_{1}=\emptyset\). Then,

1. **Line 7**: The c-component \(\mathbf{S}_{1}^{1}=\mathbf{V}=\{W,X,Z,Y\}\) includes \(\mathbf{D}_{1}\).
2. **Line 8**: The c-factor \(Q[\mathbf{S}_{1}^{1}]\) is identified as \[Q[\mathbf{S}_{1}^{1}]=A_{0}[\mathbf{S}_{1}^{1},\emptyset,\emptyset;\mathbb{Z }_{1}^{1}\coloneqq\emptyset,\emptyset](\mathbf{s}_{1}^{1},\emptyset)=P(w)P( x|w)P(z|x,w)P(y|w,x,z).\]
3. **Line 9**: Run \(Q[\mathbf{D}_{1}]=\textsc{subID}(\mathbf{D}_{1},\mathbf{S}_{1}^{1},Q[\mathbf{ S}_{1}^{1}],G(\mathbf{S}_{1}^{1}))\). 1. **Line a.(2-3):**\(\mathbf{A}=an(Z)_{G(\mathbf{V})}=\{W,X,Z\}\). Then, by Lemma 2, \[Q[\mathbf{A}]=\sum_{y\in\mathfrak{S}_{Y}}Q[\mathbf{S}]=A_{0}[\{W,X,Z\}, \emptyset,\emptyset;\emptyset,\emptyset](\{w,x,z\},\emptyset)=P(w)P(x|w)P(z| x,w).\] 2. **Line a.(7-8):** Note \(\mathbf{S}=\{W,Z\}\) is a c-component in \(G(\mathbf{A})\) containing \(\mathbf{D}_{1}=\{Z\}\). Then, \[Q[\mathbf{S}]=(\sum_{x,z\in\mathfrak{S}_{X},Z}Q[\mathbf{A}])\times\frac{Q[ \mathbf{A}]}{\sum_{z\in\mathfrak{D}_{Z}}Q[\mathbf{A}]}.\] By Lemma 2, \[\sum_{x,z\in\mathfrak{S}_{X},Z}Q[\mathbf{A}] =A_{0}[W,\emptyset,\emptyset;\emptyset,\emptyset](w,\emptyset )=P(w),\] \[\sum_{z\in\mathfrak{S}_{Z}}Q[\mathbf{A}] =A_{0}[\{W,X\},\emptyset,\emptyset;\emptyset,\emptyset](\{w,x \},\emptyset,\emptyset)=P(w)P(x|w).\] By Lemma 4, \[\frac{Q[\mathbf{A}]}{\sum_{z\in\mathfrak{S}_{Z}}Q[\mathbf{A}]} =\frac{A_{0}[\{W,X,Z\},\emptyset,\emptyset;\emptyset,\emptyset](\{w,x,z\},\emptyset)}{A_{0}[\{W,X\},\emptyset,\emptyset;\emptyset,\emptyset](\{w,x\},\emptyset,\emptyset)}\] \[=A_{0}[Z,\emptyset,\{W,X\};\emptyset,\emptyset](z,\{w,x\})\] \[=P(z|w,x).\]By Lemma 3, \[Q[\mathbf{S}] =A_{0}[W,\emptyset,\emptyset,\emptyset;\emptyset,\emptyset](w, \emptyset)\times A_{0}[Z,\emptyset,\{W,X\};\emptyset,\emptyset](z,\{w,x\})\] \[=A_{0}[\{W,Z\},\emptyset,X;\emptyset,\emptyset](\{w,z\},x)\] \[=P(w)P(z|w,x),\] because the order is \(W\prec_{G}X\prec_{G}Z\). 3. **Line a.9:** Run \(Q[\mathbf{D}_{1}]=\textsc{subID}(\mathbf{D}_{1},\mathbf{S},Q[\mathbf{S}],G( \mathbf{S}))\). 4. **Line a.(2-3):**\(\mathbf{A}=an(\mathbf{D}_{1})_{G(\mathbf{S})}=\{Z\}=\mathbf{D}_{1}\). Then, by Lemma 2, \[Q[\mathbf{D}_{1}] =\sum_{w\in\mathcal{W}}Q[\mathbf{S}]\] \[=A_{0}^{1}\coloneqq A_{0}[\{W,Z\},W,X;\emptyset,\emptyset](z,x)\] \[=\sum_{w\in\mathfrak{Sw}_{W}}P(w)P(z|w,x).\]

We now run **Line 5-13**. We first run \(\mathbf{D}_{2}=\{Y\}\) and \(\mathbf{Z}_{1}=\emptyset\). We note that it fails since the sub-procedure \(\textsc{subID}(\mathbf{D}_{2},\mathbf{S}_{1}^{1},Q[\mathbf{S}_{1}^{1}],G( \mathbf{S}_{1}^{1}))\) fails. Specifically, \(\mathbf{A}\coloneqq an(\mathbf{D}_{2})_{G(\mathbf{S}_{1}^{1})}=\mathbf{V}= \mathbf{S}_{1}^{1}\). Therefore, by **Line a.5**, the procedure fails.

We now run \(\mathbf{D}_{2}\) with \(\mathbf{Z}_{2}=\{Z\}\).

1. **Line 7**: The c-component \(\mathbf{S}_{2}^{1}=\mathbf{V}\backslash Z=\{W,X,Y\}\).
2. **Line 8**: \(Q[\mathbf{S}_{2}^{1}]=A_{0}[\mathbf{S}_{2}^{1},\emptyset,\emptyset,\mathscr{Z }_{2}^{1}=\{Z\},\mathsf{seq}_{2}^{1}](\mathbf{s}_{2}^{1},\emptyset)=P_{z}(w)P_{ z}(x|w)P_{z}(y|x,w)\), where \(\mathsf{seq}_{2}^{1}(W_{j})=(z,z,z)\).
3. **Line 9**: We run \(Q[\mathbf{D}_{2}]=\textsc{subID}(\mathbf{D}_{2},\mathbf{S}_{2}^{1},Q[\mathbf{S }_{2}^{1}],G(\mathbf{S}_{2}^{1}))\).
4. **Line a.(2-3):**\(\mathbf{A}=an(\mathbf{D}_{2})_{G(\mathbf{S}_{2}^{1})}=\{Y\}=\mathbf{D}_{2}\). Then, by Lemma 2 \[Q[\mathbf{D}_{2}] =\sum_{w,x\in\mathfrak{Sw}_{W,X}}Q[\mathbf{S}_{2}^{1}]\] (A.1) \[=\sum_{w,x\in\mathfrak{Sw}_{W,X}}A_{0}[\{W,X,Y\},\emptyset, \emptyset,\mathbb{Z}_{2}^{1}=\{Z\},\mathsf{seq}_{2}^{1}](y;\emptyset)\] (A.2) \[=A_{0}[Y,\emptyset,\emptyset;\mathbb{Z}_{2}^{1}=\{Z\},\mathsf{seq }_{2}^{1}](\{y\};\emptyset)\] (A.3) \[=P_{z}(y).\] (A.4)

Let

\[Q[\mathbf{D}_{1}] =A_{0}^{1}\coloneqq A_{0}[\{W,Z\},W,X;\emptyset,\emptyset](z,x)\] (A.5) \[Q[\mathbf{D}_{2}] =A_{0}^{2}\coloneqq A_{0}[Y,\emptyset,\emptyset;\mathbb{Z}_{2}^{ 1}=\{Z\},\mathsf{seq}_{2}^{1}](\{y\};\emptyset).\] (A.6)

By **Line 14**,

\[P(y|do(x))=\sum_{z\in\mathfrak{Sw}_{Z}}Q[\mathbf{D}_{1}]Q[\mathbf{D}_{2}]=\sum _{z\in\mathfrak{Sw}_{Z}}A_{0}^{1}A_{0}^{2}.\] (A.7)

### Example 4

We provide a detailed illustration of Example 4, demonstrating the application of Lemmas (2,3,4).

**Input**: \(\mathbf{x}=\{x_{1},x_{2}\}\), \(\mathbf{y}\coloneqq\{y\}\), \(\mathbb{Z}\coloneqq\{X_{1},X_{2}\}\). The goal is to identify \(P(y|do(x_{1},x_{2}))\) from \(\mathbb{P}\coloneqq\{P_{\sigma(X_{1})}(\mathbf{V}),P_{\sigma(X_{2})}(\mathbf{V })\}\). Specifically, two distributions \(P_{x_{1}}(\mathbf{V}\backslash X_{1})\) and \(P_{x_{2}}(\mathbf{V}\backslash X_{2})\) will be used in the identification task.

**Line 3-4**: \(\mathbf{D}=an(Y)_{G(R,W,Y)}=\{R,W,Y\}\). Let \(\mathbf{D}_{1}\coloneqq\{R\}\), \(\mathbf{D}_{2}\coloneqq\{W\}\) and \(\mathbf{D}_{3}=\{Y\}\).

**Line 5-13**: Consider \(\mathbf{D}_{1}=\{R\}\) and \(\mathbf{Z}_{1}\coloneqq\{X_{1}\}\). Note \(\mathbf{S}^{1}_{1}\coloneqq\{R\}=\mathbf{D}_{1}\) is a c-component in \(G(\mathbf{V}\backslash X_{1})\). Therefore, \(Q[\mathbf{D}_{1}]=Q[\mathbf{S}^{1}_{1}]\), where

\[Q[\mathbf{D}^{1}_{1}]=A^{1}_{0}\coloneqq A_{0}[R,\emptyset,X_{2};\mathbb{Z}^{1 }_{1}\coloneqq\{X_{1}\},\mathtt{seq}^{1}_{1}](r,x_{2})=P_{x_{1}}(r|x_{2}),\] (A.8)

where \(\mathtt{seq}^{1}_{1}\coloneqq(x_{1})\).

**Line 5-13**: We now consider \(\mathbf{D}_{3}=\{Y\}\). Note that \(Q[\mathbf{D}_{3}]\) is not identifiable from \(P_{x_{1}}(\mathbf{V}\backslash X_{1})\). To witness, consider the c-component \(\mathbf{S}^{1}_{3}\coloneqq\{W,X_{2},Y\}\) in \(G(\mathbf{V}\backslash X_{1})\). Then, the sub-procedure \(\mathtt{subID}(\mathbf{D}_{3},\mathbf{S}^{1}_{3},Q[\mathbf{S}^{1}_{3}],G( \mathbf{S}^{1}_{3}))\) fails because failure condition in line a.5 is triggered. Specifically, \(an(Y)_{G(\mathbf{S}^{1}_{3})}=\mathbf{S}^{1}_{3}\).

Therefore, we consider \(\mathbf{D}_{3}=\{Y\}\) with \(\mathbf{Z}_{3}\coloneqq x_{2}\). Note \(\mathbf{S}^{2}_{3}\coloneqq\{Y\}=\mathbf{D}_{3}\) is a c-component in \(G(\mathbf{V}\backslash X_{2})\). Therefore, \(Q[\mathbf{D}_{3}]=Q[\mathbf{S}^{2}_{3}]\) which is given by line 8:

\[Q[\mathbf{D}_{3}] =A_{0}[Y,\emptyset,\{R,W\};\mathbb{Z}^{2}_{3}\coloneqq\{X_{2}\}, \mathtt{seq}^{2}_{3}=(x_{2})](y,\{r,w\})\] (A.9) \[=P_{x_{2}}(y|r,w).\] (A.10)

**Line 5-13**: Consider \(\mathbf{D}_{2}=\{W\}\) and \(\mathbf{Z}_{1}\coloneqq X_{1}\). Note that \(Q[\mathbf{D}_{2}]\) is not identifiable from \(P_{x_{2}}(\mathbf{V}\backslash X_{2})\). To witness, consider the c-component \(\mathbf{S}^{2}_{2}\coloneqq\{X_{1},W\}\) in \(G(\mathbf{V}\backslash X_{2})\). Then, the sub-procedure \(\mathtt{subID}(\mathbf{D}_{2},\mathbf{S}^{2}_{2},Q[\mathbf{S}^{2}_{2}],G( \mathbf{S}^{2}_{2}))\) fails because failure condition in line a.5 is triggered. Specifically, \(an(W)_{G(\mathbf{S}^{2}_{2})}=\mathbf{S}^{2}_{2}\).

Therefore, we consider \(\mathbf{D}_{2}=\{W\}\) with \(\mathbf{Z}_{1}\coloneqq X_{1}\).

1. Note \(\mathbf{S}^{1}_{2}=\{X_{2},W,Y\}\) is a c-component in \(G(\mathbf{V}\backslash X_{1})\) containing \(\mathbf{D}_{2}\). Then, \[Q[\mathbf{S}^{1}_{2}] =A_{0}[\{X_{2},W,Y\},\emptyset,R;\mathbb{Z}^{1}_{2}\coloneqq\{X _{1}\},\mathtt{seq}^{1}_{2}=(x_{1},x_{1},x_{1})](\{x_{2},w,y\},r)\] (A.11) \[=P_{x_{1}}(x_{2})P_{x_{1}}(w|r,x_{2})P_{x_{1}}(y|x_{2},w,r).\] (A.12)
2. Run \(Q[\mathbf{D}_{2}]=\mathtt{subID}(\mathbf{D}_{2},\mathbf{S}^{1}_{2},Q[\mathbf{ S}^{1}_{2}],G(\mathbf{S}^{1}_{2}))\).
3. **Line a.(2-3): \(\mathbf{A}=an(W)_{G(\mathbf{S}^{1}_{2})}=\{W\}=\mathbf{D}_{2}\)**. Then, \[Q[\mathbf{D}_{2}] =\sum_{y,x_{2}\in\mathfrak{S}_{Y,X_{2}}}Q[\mathbf{S}^{1}_{2}]\] (A.13) \[=\sum_{y,x_{2}\in\mathfrak{S}_{Y,X_{2}}}A_{0}[\{X_{2},W,Y\}, \emptyset,R;\mathbb{Z}^{1}_{2},\mathtt{seq}^{1}_{2}](\{x_{2},w,y\},r)\] (A.14) \[=A_{0}[\{X_{2},W\},X_{2},R;\mathbb{Z}^{1}_{2},\mathtt{seq}^{1}_{2 }=(x_{1},x_{1})](w,r)\] (A.15) \[=\sum_{x_{2}^{\prime}\in\mathcal{X}_{2}}P_{x_{1}}(x_{2})P_{x_{1} }(w|r,x_{2}^{\prime}).\] (A.16)

Also, by Lemma 3,

\[Q[\mathbf{D}_{1}]Q[\mathbf{D}_{3}]\] (A.17) \[=A_{0}^{13}\] (A.18) \[\coloneqq A_{0}[R,\emptyset,X_{2};\mathbb{Z}^{1}_{1}\coloneqq\{X _{1}\},\mathtt{seq}^{1}_{1}](r,x_{2})\times A_{0}[Y,\emptyset,\{R,W\};\mathbb{Z }^{2}_{3}\coloneqq\{X_{2}\},\mathtt{seq}^{2}_{3}](y,\{r,w\})\] (A.19) \[=A_{0}[\{R,Y\},\emptyset,\{X_{2},W\};\mathbb{Z}^{13}\coloneqq\{X _{1},X_{2}\},\mathtt{seq}^{13}=(x_{1},x_{2}),G](\{r,y\},\{x_{2},w\})\] (A.20) \[=P_{x_{1}}(r|x_{2})P_{x_{2}}(y|r,w).\] (A.21)

Finally,

\[P(y|do(x_{1},x_{2}))=\sum_{r,w\in\mathfrak{S}_{R,W}}A_{0}^{2}A_{0}^{13}.\] (A.22)

### Example 5

#### a.3.1 Specification of Nuisances

Recall that the topological order of the variable is \(W\prec_{G}X\prec_{G}Z\prec_{G}Y\). Also, \(P(y|do(x))=\sum_{z\in\mathfrak{S}_{Z}}A_{0}^{1}A_{0}^{2}\) where

\[A_{0}^{1} \coloneqq A_{0}[\{W,Z\},W,X;\emptyset,\emptyset](z,x)=\sum_{w\in \mathfrak{S}_{W}}P(z|x,w)P(w)\] (A.23) \[A_{0}^{2} \coloneqq A_{0}[Y,\emptyset,\emptyset;\{Z\},(z)](y,\emptyset)=P_{z }(y)\coloneqq P_{\sigma(Z)}(y|z).\] (A.24)

That is,

\[P(y|do(x))=f(A_{0}^{1},A_{0}^{2})\coloneqq\sum_{z\in\mathfrak{S}_{Z}}A_{0}^{1}A_ {0}^{2}.\] (A.25)

By leveraging the definition of the nuisance in Def. 2, the nuisance composing \(A_{0}^{1}\) is \(\{\mu_{1,0}^{2},\pi_{1,0}^{1}\}\) which are defined as follow:

\[\mu_{1,0}^{2}(X,W) \coloneqq\mathbb{E}_{P}\left[\mathbb{1}_{z}(Z)|X,W\right],\] \[\pi_{1,0}^{1}(X,W) \coloneqq\mathbb{1}_{x}(X)/P(X|W).\]

The nuisance composing \(A_{0}^{2}\) is \(\{\mu_{2,0}^{2}\}\) which is \(\mu_{2,0}^{2}\coloneqq\mathbb{E}_{P_{z}}\left[\mathbb{1}_{y}(Y)\right]\).

#### a.3.2 Construction of Estimators

We apply the procedure in Def. 3 to construct estimators \(\hat{A}^{1}\) and \(\hat{A}^{2}\) for \(A_{0}^{1}\) and \(A_{0}^{2}\). We choose \(L=2\). We first construct \(\hat{A}^{1}\) for the fixed \(\{z,x\}\in\mathfrak{O}_{Z,X}\). We note that \(\hat{A}_{\ell}^{1}\) for \(\ell\in\{1,2\}\) is given as follow: For a fixed \(z,x\),

\[\hat{A}_{\ell}^{1} \coloneqq\mathbb{E}_{D_{\ell}}\left[\pi_{\ell}^{1}(X,W)\{\mathbb{1 }_{z}(Z)-\mu_{1,\ell}^{2}(X,W)\}+\mu_{1,\ell}^{2}(x,W)\right],\] (A.26)

and

\[\hat{A}^{1}=1/L\sum_{\ell=1}^{L}\hat{A}_{\ell}^{1},\] (A.27)

where \(\pi^{1},\mu^{2}\) are nuisances estimated using \(D\backslash D_{\ell}\). Specifically, \(\mu_{1,\ell}^{2}(X,W)\) is obtained by using the XGBoost [Chen and Guestrin, 2016] regression model which regresses \(\mathbb{1}_{z}(Z)\) onto the \(\{X,W\}\) using \(D\backslash D_{\ell}\). \(\mu_{1,\ell}^{2}(x,W)\) is evaluated from \(D_{\ell}\) after fixing a column for \(X\) to \(x\). In similar, \(\pi^{1}\) as follow: we first model \(P(X|W)\) by regressing \(X\) onto \(W\) from the data \(D\backslash D_{\ell}\) using the XGBoost [Chen and Guestrin, 2016]. Then, we evaluate \(\pi_{1,0}^{1}(X,W)\) by plugging in the trained \(P(X|W)\).

We now construct \(\hat{A}^{2}\) for the fixed \(z\) and \(y\). We first take the subsamples \(D_{z}\) from the experimental samples \(D_{\sigma(Z)}\in\mathbb{D}\), where \(D_{z}\) is the sample where \(Z=z\). Then, we compute the following:

\[\hat{A}^{2}=\mu_{2}^{2}=\mathbb{E}_{D_{z}}\left[\mathbb{1}_{y}(Y)\right].\] (A.28)

Then, following Def. 4, the MR-gID is constructed as follow:

\[f(\hat{A}^{1},\hat{A}^{2})=\sum_{z\in\mathfrak{D}_{Z}}\hat{A}^{1}\hat{A}^{2}.\] (A.29)

### Example 6

#### a.4.1 Specification of Nuisances

Recall that the topological order of the variable is \(X_{1}\prec_{G}X_{2}\prec_{G}R\prec_{G}W\prec_{G}Y\). Also,

\[A_{0}^{2} \coloneqq A_{0}[\{X_{2},W\},\{X_{2}\},R;\mathbb{Z}^{2}=\{X_{1}\}, \mathtt{seq}^{2}=(x_{1},x_{1})](w,r)\] (A.30) \[= \sum_{x_{\ell}^{\prime}\in\mathfrak{S}_{X_{2}}}P_{x_{1}}(w|r,x_{ \ell}^{\prime})P_{x_{1}}(x_{2}^{\prime})\] (A.31) \[A_{0}^{13} \coloneqq A_{0}[\{R,Y\},\emptyset,\{X_{2},W\};\mathbb{Z}^{13}=\{X _{1},X_{2}\},\mathtt{seq}^{13}=(x_{1},x_{2})](\{r,y\},\{x_{2},w\})\] (A.32) \[= P_{x_{1}}(r|x_{2})P_{x_{2}}(y|r,w).\] (A.33)

Then,

\[P(y|do(x_{1},x_{2}))=\sum_{r,w\in\mathfrak{S}_{R,W}}A_{0}^{2}A_{0}^{13}.\] (A.34)

The nuisance composing \(A_{0}^{2}\) is \(\{\mu_{2,0}^{2},\pi_{2,0}^{1}\}\) which are defined as follow:

\[\mu_{2,0}^{2}(R,X_{2}) \coloneqq\mathbb{E}_{P_{x_{1}}}\left[\mathbb{1}_{w}(W)|R,X_{2} \right],\] (A.35) \[\pi_{2,0}^{1}(R,X_{2}) \coloneqq\mathbb{1}_{r}(R)/P_{x_{1}}(R|X_{2}).\] (A.36)

The nuisance composing \(A_{0}^{13}\) is \(\{\mu_{2,0}^{2},\pi_{2,0}^{1}\}\) which are defined as follow:

\[\mu_{13,0}^{2}(R,W) \coloneqq\mathbb{E}_{P_{x_{2}}}\left[\mathbb{1}_{r,y}(R,Y)|R,W\right]\] (A.37) \[=\mathbb{E}_{P_{\sigma(X_{2})}}\left[\mathbb{1}_{r,y}(R,Y)|R,W,x_{ 2}\right]\] (A.38) \[=\mathbb{1}_{r}(R)\mathbb{E}_{P_{\sigma(X_{2})}}\left[\mathbb{1}_ {y}(Y)|R,W,x_{2}\right],\] (A.39)

and

\[\pi_{13,0}^{1}(X_{2},W) \coloneqq\frac{P_{\sigma(X_{1})}(R|x_{2},x_{1})}{P_{\sigma(X_{2} )}(R|x_{2})}\frac{\mathbb{1}_{w}(W)}{P_{\sigma(X_{2})}(W|R,x_{2})}.\] (A.40)

#### a.4.2 Construction of Estimators

We apply the procedure in Def. 3 to construct estimators \(\hat{A}^{2}\) and \(\hat{A}^{13}\) for \(A_{0}^{2}\) and \(A_{0}^{13}\). We choose \(L=2\). We first construct \(\hat{A}^{2}\) for the fixed \(\{w,r,x_{1}\}\). We note that \(\hat{A}^{2}\coloneqq(1/L)\sum_{\ell=1}^{L}\hat{A}_{\ell}^{2}\) for \(\ell\in\{1,2\}\) where \(\hat{A}_{\ell}^{2}\) is given as follow:

\[\hat{A}_{\ell}^{2} \coloneqq\mathbb{E}_{D_{x_{1},\ell}}\left[\pi_{2,\ell}^{1}(R,X_{2 })\{\mathbb{1}_{w}(W)-\mu_{2,\ell}^{2}(R,X_{2})\}+\mu_{2,\ell}^{2}(r,X_{2}) \right],\] (A.41)

where \(D_{x_{1}}\) is a subsample of \(D_{\sigma(X_{1})}\) fixing \(X_{1}=x_{1}\), and \(\pi_{2,\ell}^{1},\mu_{2,\ell}^{2}\) are nuisances trained using \(D_{x_{1}}\backslash D_{x_{1},\ell}\). We note that \(\mu_{2,\ell}^{2}(R,X_{2})\) is constructed by regressing \(\mathbb{1}_{w}(W)\) onto \(\{R,X_{2}\}\). Also, \(\pi_{2,\ell}^{1}(R,X_{2})\) is constructed by regressing \(R\) onto \(X_{2}\).

We now construct \(\hat{A}^{13}\coloneqq(1/L)\sum_{\ell=1}^{L}\hat{A}_{\ell}^{13}\) for \(\ell\in\{1,2\}\) where \(\hat{A}_{\ell}^{13}\) is given as follow:

\[\hat{A}_{\ell}^{13} \coloneqq\mathbb{E}_{D_{x_{2},\ell}}\left[\pi_{13,\ell}^{1}(X_{2 },W)\{\mathbb{1}_{r,y}(R,Y)-\mu_{13,\ell}^{2}(R,W)\}\right]+\mathbb{E}_{ \overline{D}_{x_{1},\ell}}\left[\mu_{13,\ell}^{2}(R,w)\right]\right],\] (A.42)

where \(D_{x_{1}}\) is a subsample of \(D_{\sigma(X_{1})}\) fixing \(X_{1}=x_{1}\), and \(\overline{D}_{x_{1}}\) is a subsample of \(D_{x_{1}}\) fixing \(X_{2}=x_{2}\). \(\pi_{2,\ell}^{1},\mu_{2,\ell}^{2}\) are nuisances trained using \(\overline{D}_{x_{1}}\backslash\overline{D}_{x_{1},\ell}\) and \(\overline{D}_{x_{2}}\backslash\overline{D}_{x_{2},\ell}\).

### Details on Regression-based (REG) and Probability Weighting-based (PW) estimators.

In this section, we provide details on two alternative g-ID estimators used in Sec. 4: \(T^{\text{reg}}\coloneqq f(\{\hat{A}^{k,\text{reg}}\}_{k=1}^{K}\})\) ('regression-based estimators') where \(\hat{A}^{k,\text{reg}}\) denotes the regression-based estimator for the g-mSBD operator, and \(T^{\mathfrak{pw}}=f((\hat{A}^{k,\mathfrak{pw}})_{k=1}^{K})\) ('probability weighting-based estimators) where \(\hat{A}^{k,\mathfrak{reg}}\) denotes the probability weighting-based estimator for the g-mSBD operator.

#### a.5.1 Regression-based Estimator

The regression-based g-mSBD estimator is defined as follows:

**Definition A.1** (Regression-based g-mSBD Estimator).: Let \(D_{\sigma(\mathbf{Z}_{i})}\) for \(\mathbf{Z}_{i}\in\mathbb{Z}\) denote the experimental samples from randomizing the variable \(\mathbf{Z}_{i}\). Let \(\overline{D}_{\mathbf{z}_{i}}\) for \(\mathbf{z}_{i}\in\mathfrak{D}_{\mathbf{z}_{i}}\) denote the subsamples of \(D_{\sigma(\mathbf{Z}_{i})}\) fixing \(\mathbf{R}_{0}\backslash\mathbf{Z}_{i}=\mathbf{r}_{0}\backslash\mathbf{z}_{i}\) and \(\mathbf{Z}_{i}=\mathbf{z}_{i}\). A regression-based estimator \(\hat{A}^{\text{reg}}\) for the g-mSBD adjustment \(A_{0}[\mathbf{W},\mathbf{C},\mathbf{R};\mathbb{Z}_{0}\coloneqq\{\mathbf{Z}_{i }\}_{i=1}^{m},\mathtt{seq}\coloneqq(\mathbf{z}_{i})_{i=1}^{m}](\mathbf{w} \backslash\mathbf{c},\mathbf{r})\) is given as follows:

1. Randomly partition \(\overline{D}_{\mathbf{z}_{i}}\) into \(\{\overline{D}_{\mathbf{z}_{i},\ell}\}_{\ell\in[L]}\); i.e., \(\overline{D}_{\mathbf{z}_{i}}=\cup_{\ell=1}^{L}\overline{D}_{\mathbf{z}_{i},\ell },\;\forall\mathbf{Z}_{i}\in\mathbb{Z}\) and \(\mathbf{z}_{i}\in\mathfrak{D}_{\mathbf{z}_{i}}\).
2. For each fold \(\ell\in[L]\), let \(\mu_{\ell}^{i+1}\) denote learned \(\mu_{0}^{i+1}\) using \(\overline{D}_{\mathbf{z}_{i+1}}\backslash\overline{D}_{\mathbf{z}_{i+1},\ell}\) for \(i=m,\cdots,2\). Define \(\hat{\mu}_{\ell}^{i+1}\coloneqq\mu_{\ell}^{i+1}(\overline{\mathbf{W}}^{i}, \mathbf{r}_{i},\overline{\mathbf{R}}^{1:i-1})\).
3. Estimate \(\hat{A}^{\text{reg}}\coloneqq\hat{A}^{\text{reg}}(\{\mu_{\ell}^{j+1}\}_{j\in[ m-1],\ell\in[L]})\coloneqq(1/L)\sum_{\ell=1}^{L}\hat{A}^{\text{reg}}_{\ell}(\{ \mu_{\ell}^{j+1}\}_{j\in[m-1]})\) where \[\hat{A}^{\text{reg}}_{\ell}\coloneqq\hat{A}^{\text{reg}}_{\ell}(\{\mu_{\ell}^ {j+1}\}_{j\in[m-1]})\coloneqq\mathbb{E}_{\overline{D}_{\mathbf{z}_{1},\ell}} \left[\tilde{\mu}_{\ell}^{2}\right].\] (A.43)

The error of the regression-based estimator is given as follows:

**Proposition A.1** (Error Analysis of the regression-based g-mSBD estimator).: _Suppose \(\|\mu_{\ell}^{2}-\mu_{0}^{2}\|_{P_{\sigma(\mathbf{Z}_{1})}}=o_{P_{\sigma( \mathbf{Z}_{1})}}(1)\). Then,_

\[\hat{A}^{\text{reg}}-A_{0}=O_{P_{\sigma(\mathbf{Z}_{1})}}(n_{1}^{-1/2})+\frac{ 1}{L}\sum_{\ell=1}^{L}O_{P_{\sigma(\mathbf{Z}_{1})}}(\|\mu_{\ell}^{2}-\mu_{0} ^{2}\|).\] (A.44)

Proof of Proposition a.1.: We note that

\[A_{0}=\mathbb{E}_{P_{\sigma(\mathbf{Z}_{1})}}\left[\tilde{\mu}_{0}^{2}|\mathbf{ z}_{1},\mathbf{r}_{0}\right]\] (A.45)

by the analysis in Lemma S.2. Therefore, by Lemma S.7,

\[\hat{A}^{\text{reg}}_{\ell}-A_{0} =\mathbb{E}_{\overline{D}_{\mathbf{z}_{1},\ell}-P_{\sigma(\mathbf{ Z}_{1})|r_{0},\mathbf{z}_{1}}}\left[\tilde{\mu}_{0}^{2}\right]\] (A.46) \[+\mathbb{E}_{\overline{D}_{\mathbf{z}_{1},\ell}-P_{\sigma(\mathbf{ Z}_{1})|r_{0},\mathbf{z}_{1}}}\left[\tilde{\mu}_{\ell}^{2}-\tilde{\mu}_{0}^{2}\right]\] (A.47) \[+\mathbb{E}_{P_{\sigma(\mathbf{Z}_{1})}}\left[\tilde{\mu}_{\ell}^{ 2}-\tilde{\mu}_{0}^{2}|\mathbf{r}_{0},\mathbf{z}_{1}\right].\] (A.48)

By the central limit theorem,

\[\text{Eq.~{}\eqref{eq:A46}}=O_{P_{\sigma(\mathbf{Z}_{1})}}(n_{1,\ell}^{-1/2}),\] (A.49)

where \(n_{1,\ell}\coloneqq|\overline{D}_{\mathbf{z}_{1},\ell}|\).

By [Kennedy et al., 2020, Lemma 2] and the given assumption that \(\|\mu_{\ell}^{2}-\mu_{0}^{2}\|_{P_{\sigma(\mathbf{Z}_{1})}}=o_{P_{\sigma( \mathbf{Z}_{1})}}(1)\),

\[\text{Eq.~{}\eqref{eq:A47}}=O_{P_{\sigma(\mathbf{Z}_{1})}}(1/n_{1,\ell}^{-1/2}).\] (A.50)

Finally, by applying Cauchy-Schwarz inequality,

\[\text{Eq.~{}\eqref{eq:A48}}=O_{P_{\sigma(\mathbf{Z}_{1})}}(\|\tilde{\mu}_{ \ell}^{2}-\tilde{\mu}_{0}^{2}\|).\] (A.51)Finally,

\[\hat{A}^{\text{reg}}-A_{0} =\frac{1}{L}\sum_{\ell=1}^{L}(\hat{A}_{\ell}^{\text{reg}}-A_{0})\] (A.52) \[=\frac{1}{L}\sum_{\ell=1}^{L}\Big{(}O_{P_{\sigma(\mathbf{Z}_{1})}}( n^{-1/2_{1,\ell}})+O_{P_{\sigma(\mathbf{Z}_{1})}}(\|\hat{\mu}_{\ell}^{2}-\hat{ \mu}_{0}^{2}\|)\Big{)}\] (A.53) \[=O_{P_{\sigma(\mathbf{Z}_{1})}}(n^{-1/2_{1}})+\frac{1}{L}\sum_{ \ell=1}^{L}O_{P_{\sigma(\mathbf{Z}_{1})}}(\|\mu_{\ell}^{2}-\mu_{0}^{2}\|).\] (A.54)

#### a.5.2 Probability-weighting based Estimator

In this section, we define and analyze the probability weighting-based g-mSBD estimator. The probability-weighting-based estimator is defined as follows:

**Definition A.2** (**Probability-weighting-based g-mSBD Estimator)**.: Let \(D_{\sigma(\mathbf{Z}_{i})}\) for \(\mathbf{Z}_{i}\in\mathbb{Z}\) denote the experimental samples from randomizing the variable \(\mathbf{Z}_{i}\). Let \(\overline{D}_{\mathbf{z}_{i}}\) for \(\mathbf{z}_{i}\in\mathfrak{D}_{\mathbf{Z}_{i}}\) denote the subsamples of \(D_{\sigma(\mathbf{Z}_{i})}\) fixing \(\mathbf{R}_{0}\backslash\mathbf{Z}_{i}=\mathbf{r}_{0}\backslash\mathbf{z}_{i}\) and \(\mathbf{Z}_{i}=\mathbf{z}_{i}\). A probability weighting-based estimator \(\hat{A}^{\text{pw}}\) for the g-mSBD adjustment \(A_{0}[\mathbf{W},\mathbf{C},\mathbf{R};\mathbb{Z}_{0}\coloneqq\{\mathbf{Z}_{i }\}_{i=1}^{m},\mathbf{seq}\coloneqq(\mathbf{z}_{i})_{i=1}^{m}](\mathbf{w} \backslash\mathbf{c},\mathbf{r})\) is given as follows:

1. Randomly partition \(\overline{D}_{\mathbf{z}_{i}}\) into \(\{\overline{D}_{\mathbf{z}_{i},\ell}\}_{\ell\in[L]}\); i.e., \(\overline{D}_{\mathbf{z}_{i}}=\cup_{\ell=1}^{L}\overline{D}_{\mathbf{z}_{i}, \ell},\;\forall\mathbf{Z}_{i}\in\mathbb{Z}\) and \(\mathbf{z}_{i}\in\mathfrak{D}_{\mathbf{Z}_{i}}\).
2. For each fold \(\ell\in[L]\), let \(\pi_{\ell}^{i}\) denote learned \(\pi_{0}^{i}\) using \(\overline{D}_{\mathbf{z}_{i}}\backslash\overline{D}_{\mathbf{z}_{i},\ell}\) for \(i=m-1,\cdots,1\). Let \(\overline{\pi}_{\ell}^{m-1}\coloneqq\prod_{i=1}^{m-1}\pi_{\ell}^{i}\).
3. Estimate \(\hat{A}^{\text{pw}}\coloneqq\hat{A}^{\text{pw}}(\{\pi_{\ell}^{j}\}_{j\in[m-1],\ell\in[L]})\coloneqq(1/L)\sum_{\ell=1}^{L}\hat{A}_{\ell}^{\text{pw}}(\{\pi_{ \ell}^{j}\}_{j\in[m-1]})\) where \[\hat{A}_{\ell}^{\text{pw}}\coloneqq\hat{A}_{\ell}^{\text{pw}}(\{\pi_{\ell}^{j }\}_{j\in[m-1]})\coloneqq\mathbb{E}_{\overline{D}_{\overline{\mathbf{z}}_{m, \ell}}}\left[\overline{\pi}_{\ell}^{m-1}\mathbb{1}_{\mathbf{w}\backslash \mathbf{c}}(\mathbf{W}\backslash\mathbf{C})\right].\] (A.55)

**Lemma S.1** (**Representation of the g-mSBD operator using Probability Weighting)**.: _The g-mSBD adjustment \(A_{0}\) in Def. 1 can be represented as_

\[A_{0}=\mathbb{E}_{P_{\sigma(\mathbf{Z}_{m})}}\left[\overline{\pi}_{0}^{m-1} \mathbb{1}_{\mathbf{w}\backslash\mathbf{c}}(\mathbf{W}\backslash\mathbf{C})| \mathbf{z}_{m},\mathbf{r}_{0}\right].\] (A.56)

Proof of Lemma s.1.: It suffices to show that, for \(k=m-1,\cdots,1\),

\[\mathbb{E}_{P_{\sigma(\mathbf{z}_{k+1})}}\left[\overline{\pi}_{0}^{k}\hat{\mu }_{0}^{k+2}|\mathbf{z}_{k+1},\mathbf{r}_{0}\right]=\mathbb{E}_{P_{\sigma( \mathbf{z}_{k})}}\left[\overline{\pi}_{0}^{k}\hat{\mu}_{0}^{k+1}|\mathbf{z}_{ k},\mathbf{r}_{0}\right].\] (A.57)

If this holds, Lemma S.1 can be shown as follows:

\[A_{0} =\mathbb{E}_{P_{\sigma(\mathbf{z}_{1})}}\left[\tilde{\mu}_{0}^{2} |\mathbf{z}_{1},\mathbf{r}_{0}\right]\] \[=\mathbb{E}_{P_{\sigma(\mathbf{z}_{2})}}\left[\overline{\pi}_{0} ^{1}\hat{\mu}_{0}^{3}|\mathbf{z}_{2},\mathbf{r}_{0}\right]\] \[=\mathbb{E}_{P_{\sigma(\mathbf{z}_{m})}}\left[\overline{\pi}_{0} ^{m-1}\hat{\mu}_{0}^{m+1}|\mathbf{z}_{m},\mathbf{r}_{0}\right]\] \[=\mathbb{E}_{P_{\sigma(\mathbf{z}_{m})}}\left[\overline{\pi}_{0} ^{m-1}\mathbb{1}_{\mathbf{w}\backslash\mathbf{c}}(\mathbf{W}\backslash \mathbf{C})|\mathbf{z}_{m},\mathbf{r}_{0}\right].\]

Eq. (A.57) holds as follows:

\[\mathbb{E}_{P_{\sigma(\mathbf{z}_{k+1})}}\left[\overline{\pi}_{0} ^{k}\hat{\mu}_{0}^{k+2}|\mathbf{z}_{k+1},\mathbf{r}_{0}\right]\] \[=\mathbb{E}_{P_{\sigma(\mathbf{z}_{k+1})}}\left[\overline{\pi}_{0} ^{k}\hat{\mu}_{0}^{k+1}|\mathbf{z}_{k+1},\mathbf{r}_{0}\right]\] \[=\mathbb{E}_{P_{\sigma(\mathbf{z}_{k})}}\left[\overline{\pi}_{0} ^{k-1}\hat{\mu}_{0}^{k+1}|\mathbf{z}_{k},\mathbf{r}_{0}\right].\]

This completes the proof.

Equipped with Lemma S.1, we analyze the error of the probability-weighting-based estimator as follow:

**Proposition A.2** (Error Analysis of the probability weighting-based g-mSBD estimator).: _Suppose \(\|\{\pi_{\ell}^{1}\tau_{\ell}^{2}-\pi_{0}^{1}\tau_{0}^{2}\}\|_{P_{\sigma(\mathbf{ Z}_{2})}}=o_{P_{\sigma(\mathbf{Z}_{2})}}(1)\). Then,_

\[\hat{A}^{\text{pw}}-A_{0}=O_{P_{\sigma(\mathbf{Z}_{i})}}(n_{m}^{-1/2})+\frac{1} {L}\sum_{\ell=1}^{L}O_{P_{\sigma(\mathbf{Z}_{m})}}(\|\overline{\pi}_{\ell}^{m} -\overline{\pi}_{0}^{m}\|).\] (A.58)

Proof of Proposition a.2.: By Lemma S.7 and Assumption 1,

\[\hat{A}_{\ell}^{\text{pw}}-A_{0} =\mathbb{E}_{\overline{\mathcal{D}}_{\mathbf{z}_{m,\ell}}-P_{ \sigma(\mathbf{Z}_{m})|\mathbf{z}_{m,\tau_{0}}}}\left[\overline{n}_{0}^{m}1_ {\mathbf{w}\backslash\mathbf{c}}(\mathbf{W}\backslash\mathbf{C})\right]\] (A.59) \[+\mathbb{E}_{\overline{\mathcal{D}}_{\mathbf{z}_{m,\ell}}-P_{ \sigma(\mathbf{Z}_{m})|\mathbf{z}_{m,\tau_{0}}}}\left[(\overline{\pi}^{m}- \overline{\pi}_{0}^{m})1_{\mathbf{w}\backslash\mathbf{c}}(\mathbf{W}\backslash \mathbf{C})\right]\] (A.60) \[+\mathbb{E}_{P_{\sigma(\mathbf{Z}_{m})|\mathbf{z}_{m,\tau_{0}}}} \left[(\overline{\pi}^{m}-\overline{\pi}_{0}^{m})1_{\mathbf{w}\backslash\mathbf{ c}}(\mathbf{W}\backslash\mathbf{C})\right].\] (A.61)

By the central limit theorem,

\[\text{Eq.~{}\eqref{eq:A.59}}=O_{P_{\sigma(\mathbf{Z}_{m})}}(n_{m,\ell}^{-1/2}).\] (A.62)

By (Kennedy et al., 2020, Lemma 2) and the given assumption,

\[\text{Eq.~{}\eqref{eq:A.60}}=O_{P_{\sigma(\mathbf{Z}_{m})}}(n_{m,\ell}^{-1/2}).\] (A.63)

Finally, by applying Cauchy-Schwarz inequality,

\[\text{Eq.~{}\eqref{eq:A.61}}=O_{P_{\sigma(\mathbf{Z}_{m})}}(\|\{\overline{ \pi}^{m}-\overline{\pi}_{0}^{m}\}\|).\] (A.64)

This completes the proof. 

## Appendix B Proofs

### Proof of Lemma 1

**Lemma 1** (**c-component Identification**(Jung et al., 2021b)).: _Let \(\mathbf{S}\) denote a \(c\)-component in \(G_{i}\coloneqq G(\mathbf{V}\backslash\mathbf{Z}_{i})\) for some \(\mathbf{Z}_{i}\in\mathbb{Z}\). Let \(\mathbf{R}\coloneqq pa(\mathbf{S})_{G_{i}}\backslash\mathbf{S}\). Let \((\mathbf{S},\mathbf{R})\) be ordered as \((\mathbf{R}_{0},S_{1},\cdots,\mathbf{R}_{m-1},S_{m})\) by \(\prec_{G}\). Let \(\mathbf{A}\subseteq\mathbf{S}\) denote a set satisfying \(\mathbf{A}=an(\mathbf{A})_{G_{i}(\mathbf{S})}\). Let \(\mathbf{C}\coloneqq(\mathbf{S}\backslash\mathbf{A})\). Let \(\mathbb{Z}_{0}\coloneqq\

[MISSING_PAGE_FAIL:24]

\(\mathbf{R}\coloneqq(\mathbf{R}_{1}\cup\mathbf{R}_{2})\setminus\mathbf{W}\). Let \((\mathbf{W},\mathbf{R})\) be ordered by \(\prec_{G}\). Let \(\mathbb{Z}\coloneqq\mathbb{Z}_{1}\cup\mathbb{Z}_{2}\). Assume the following: (1) \(\mathbf{W}_{1}\cap\mathbf{W}_{2}=\emptyset\); and (2) \(\forall W_{j}\in\mathbf{W},\exists W_{i,k}\in\mathbf{W}\) such that \((\overline{\mathbf{W}}^{j-1},\overline{\mathbf{R}}^{j-1})=(\overline{\mathbf{W }}_{i}^{k-1},\overline{\mathbf{R}}_{i}^{k-1})\). Let \(\mathsf{seq}\coloneqq(\mathbf{z}_{j})_{j:W_{j}\in\mathbf{W}}\) where \(\mathbf{z}_{j}=\mathbf{z}_{k}^{i}\) for all \(j\). Then,_

\[A_{0}^{1}\times A_{0}^{2}=A_{0}[\mathbf{W},\emptyset,\mathbf{R};\mathbb{Z}, \mathsf{seq}](\mathbf{w},\mathbf{r})=\prod_{j:W_{j}\in\mathbf{W}}P_{\mathbf{z }_{j}}(w_{j}|\overline{\mathbf{w}}^{j-1},\overline{\mathbf{r}}^{j-1}\setminus \mathbf{z}_{j}). \tag{4}\]

Proof of Lemma3.: \[A_{0}[\mathbf{W},\emptyset,\mathbf{R};\mathbb{Z},\mathsf{seq}]( \mathbf{w},\mathbf{r})\] (B.14) \[=\prod_{j:W_{j}\in\mathbf{W}}P_{\mathbf{z}_{j}}(w_{j}|\overline{ \mathbf{w}}^{j-1},\overline{\mathbf{r}}^{j-1}\setminus\mathbf{z}_{j})\] (B.15) \[=\prod_{k:W_{1,k}\in\mathbf{W}_{1}\text{s}_{1}W_{1,k}=W_{j}}P_{ \mathbf{z}_{k}^{1}}(w_{1,k}|\overline{\mathbf{w}}_{1}^{j-1},\overline{\mathbf{ r}}_{1}^{j-1}\setminus\mathbf{z}_{j}^{1})\times\prod_{k:W_{2,k}\in\mathbf{W}_{2} \text{s}_{1}W_{2,k}=W_{j}}P_{\mathbf{z}_{k}^{2}}(w_{2,k}|\overline{\mathbf{w} }_{2}^{j-1},\overline{\mathbf{r}}_{2}^{j-1}\setminus\mathbf{z}_{j}^{2})\] (B.16) \[=\prod_{j=1}^{m^{1}}P_{\mathbf{z}_{j}^{1}}(w_{1,j}|\overline{ \mathbf{w}}_{1}^{j-1},\overline{\mathbf{r}}_{1}^{j-1}\setminus\mathbf{z}_{j}^ {1})\times\prod_{j=1}^{m^{2}}P_{\mathbf{z}_{j}^{2}}(w_{2,j}|\overline{\mathbf{ w}}_{2}^{j-1},\overline{\mathbf{r}}_{2}^{j-1}\setminus\mathbf{z}_{j}^{2})\] (B.17) \[=A_{0}^{1}\times A_{0}^{2}.\] (B.18)

### Proof of Lemma4

**Lemma 4** (Division).: _Let \(A_{0}^{i}\coloneqq A_{0}[\mathbf{W}_{i},\emptyset,\mathbf{R}_{i};\mathbb{Z}_ {i},\mathsf{seq}^{i}](\mathbf{w}_{i},\mathbf{r}_{i}):=\prod_{j=1}^{m^{i}}P_{ \mathbf{z}_{j}^{i}}(w_{i,j}|\overline{\mathbf{w}}_{i}^{j-1},\overline{\mathbf{ r}}_{i}^{j-1}\setminus\mathbf{z}_{j}^{j})\) for \(i\in\{1,2\}\) where \(\mathsf{seq}^{i}\coloneqq(\mathbf{z}_{j}^{i})_{j=1}^{m^{i}}\). Let \(\mathbf{W}\coloneqq\mathbf{W}_{1}\setminus\mathbf{W}_{2}\). Let \(\mathbf{R}\coloneqq(\mathbf{R}_{1}\cup\mathbf{W}_{2})\cap\text{pre}(\mathbf{W };G)\). Assume the following: (1) \(\mathbf{W}_{2}\subseteq\mathbf{W}_{1}\); and (2) \(\forall W_{j}\in\mathbf{W},\exists W_{1,k}\in\mathbf{W}_{1}\) such that \((\overline{\mathbf{W}}^{j-1},\overline{\mathbf{R}}^{j-1})=(\overline{\mathbf{ W}}_{1}^{k-1},\overline{\mathbf{R}}_{1}^{k-1})\), \(\mathbf{Z}_{i,k}=\mathbf{Z}_{j}\) and \(\mathbf{z}_{i,k}=\mathbf{z}_{j}\). Then,_

\[A_{0}^{1}/A_{0}^{2}=A_{0}[\mathbf{W},\emptyset,\mathbf{R};\mathbb{Z}_{1}, \mathsf{seq}^{1}](\mathbf{w},\mathbf{r})=\prod_{j:W_{j}\in\mathbf{W}}P_{ \mathbf{z}_{j}}(w_{j}|\overline{\mathbf{w}}^{j-1},\overline{\mathbf{r}}^{j-1 }\setminus\mathbf{z}_{j}). \tag{5}\]

Proof of Lemma4.: \[A_{0}^{1}/A_{0}^{2} =\frac{\prod_{j=1}^{m^{1}}P_{\mathbf{z}_{j}^{1}}(w_{1,j}| \overline{\mathbf{w}}_{1}^{j-1},\overline{\mathbf{r}}_{1}^{j-1}\setminus \mathbf{z}_{j}^{1})}{\prod_{j=1}^{m^{2}}P_{\mathbf{z}_{j}^{2}}(w_{2,j}| \overline{\mathbf{w}}_{2}^{j-1},\overline{\mathbf{r}}_{2}^{j-1}\setminus \mathbf{z}_{j}^{2})}\] (B.19) \[=\prod_{k:W_{k}\in\mathbf{W}_{1}\setminus\mathbf{W}_{2}}P_{ \mathbf{z}_{k}^{1}}(w_{1,k}|\overline{\mathbf{w}}_{1}^{k-1},\overline{\mathbf{ r}}_{1}^{k-1}\setminus\mathbf{z}_{k}^{1})\] (B.20) \[=\prod_{k:W_{k}\in\mathbf{W}_{1}\setminus\mathbf{W}_{2}}P

### Proof of Theorem 1

**Theorem 1** (**Expression of g-Identifiable Causal Effects**).: _Algo. 1 returns any g-identifiable causal effects as a function of a set \(\{A_{0}^{k}\}\) of g-mSBD adjustment operators in the form_

\[P(\mathbf{y}|do(\mathbf{x}))=f(\{A_{0}^{k}\}_{k=1}^{K}), \tag{6}\]

_where the function \(f(\cdot)\) applies marginalization, multiplication, or division over g-mSBD operators in \(\{A_{0}^{k}\}\) as specified by Algo. 1._

Proof of Theorem 1.: Throughout the proof, we refer to the algorithm developed in (Lee et al., 2019, Algo. 1) as the "standard gID" algorithm, in comparison to our gID algorithm presented in Algo. 1. It is established that the standard gID algorithm is sound, as stated in (Lee et al., 2019, Theorem 2). This means that if the algorithm returns an identification expression, it must be correct. Furthermore, the standard gID algorithm is proven to be complete (Lee et al., 2019, Theorem 3). In other words, the causal effect \(P(\mathbf{y}|do(\mathbf{x}))\) is identifiable from \(\mathbb{P}\) and the causal graph \(G\) if and only if the standard gID algorithm does not return FAIL.

In our proof, we will show the soundness and completeness of Algo. 1 based on the foundation provided by the standard gID algorithm.

Algo. 1 is sound - If Algo. 1 returns an expression \(f(\{A_{0}^{k}\}_{k=1}^{K})\), then it holds that \(f(\{A_{0}^{k}\}_{k=1}^{K})=P(\mathbf{y}|do(\mathbf{x}))\). The soundness of Algo. 1 is derived from the soundness of Tian's c-factor operation, as demonstrated in (Tian and Pearl, 2003, Lemmas (3,4)) and Lemma 1.

We will now show that Algo. 1 is complete. Suppose there exists an input \((\mathbf{x},\mathbf{y},\mathbb{Z},\mathbb{P},G)\) for which the standard gID algorithm does not return FAIL while Algo. 1 does return FAIL. This implies the existence of \(\mathbf{D}_{j}\) such that \(Q[\mathbf{D}_{j}]\) is not identifiable from all \(Q[\mathbf{S}_{j}^{i}]\) where \(\mathbf{D}_{j}\) is a c-component in \(G(\mathbf{D})\), and \(\mathbf{S}_{j}^{i}\) is the c-component in \(G(\mathbf{V}\backslash\mathbf{Z}_{i})\) that contains \(\mathbf{D}_{j}\). This observation is a consequence of the soundness and completeness of the subID procedure, as established in (Huang and Valtorta, 2006, Theorem 1).

It should be noted that \(Q[\mathbf{D}_{j}]\) is not identifiable from \(\mathbf{S}_{j}^{i}\) for all \(\{i:\mathbf{Z}_{i}\in\mathbb{Z}\}\) only when there exists a c-component \(\mathbf{T}_{j}^{i}\) in \(G(\mathbf{S}_{j}^{i})\) that serves as an ancestral set of \(\mathbf{D}_{j}\) and includes \(\mathbf{D}_{j}\). However, in such a scenario, the standard gID algorithm fails due to lines 12 and 13 of the algorithm. This contradicts the initial assumption that the standard gID algorithm does not return FAIL. Consequently, Algo. 1 returns FAIL whenever the standard gID algorithm does so. The completeness of the standard gID algorithm implies that Algo. 1 is complete in the g-identification task.

The fact that \(f(\cdot)\) is a function involving marginalization, multiplications, and divisions of g-mSBD (generalized modified single back-door) operators is a consequence of applying Lemmas (2, 3, 4) within the algorithm. These lemmas establish the properties and operations of the g-mSBD operators, which are then utilized in the construction of \(f(\cdot)\) in Algo. 1.

### Proof of Proposition 1

We first restate the g-mSBD adjustment, its nuisances, and the g-mSBD estimator here:

**Definition 1** (**generalized-mSBD adjustment (g-mSBD)**).: Let \((\mathbf{W},\mathbf{R})\) be a disjoint pair in \(\mathbf{V}\) topologically ordered as \((\mathbf{W},\mathbf{R})=\{\mathbf{R}_{0},W_{1},\cdots,\mathbf{R}_{m-1},W_{m}, \mathbf{R}_{m}\}\) by \(\prec_{G}\), where \(\mathbf{R}_{i}\) can be empty. Let \(\overline{\mathbf{W}}^{i-1}\coloneqq\{W_{j}\}_{j=1}^{i-1}\) and \(\overline{\mathbf{R}}^{i-1}\coloneqq\{\mathbf{R}_{j}\}_{j=0}^{i-1}\) for \(\forall i\in[m]\). Let \(\mathbf{C}\subseteq\mathbf{W}\). Let \(\mathbb{Z}_{0}\subseteq\mathbb{Z}\) be some set such that \(\forall\mathbf{Z}\in\mathbb{Z}_{0},\mathbf{W}\cap\mathbf{Z}=\emptyset\). Let \(\mathbf{seq}(\mathbb{Z}_{0})\) denote a sequence \((\mathbf{z}_{1},\cdots,\mathbf{z}_{m})\) where \(\mathbf{z}_{i}\) denotes some realization of \(\mathbf{Z}_{i}\in\mathbb{Z}_{0}\) (same \(\mathbf{z}_{i}\) could appear multiple times in the sequence). Then, the g-mSBD adjustment is expressed as an operator \(A_{0}[\mathbf{W},\mathbf{C},\mathbf{R};\mathbb{Z}_{0},\mathbf{seq},G](\mathbf{ w}\backslash\mathbf{c},\mathbf{r})\) defined by

\[A_{0}[\mathbf{W},\mathbf{C},\mathbf{R};\mathbb{Z}_{0},\mathbf{seq}](\mathbf{w} \backslash\mathbf{c},\mathbf{r})\coloneqq\sum_{\mathbf{c}\in\mathbf{\Theta} _{\mathbf{C}}}\prod_{i:W_{i}\in\mathbf{W}}P_{\mathbf{z}_{i}}(w_{i}|\overline{ \mathbf{w}}^{i-1},\overline{\mathbf{r}}^{i-1}\backslash\mathbf{z}_{i}). \tag{1}\]

**Definition 2** (**Nuisances for g-mSBD**).: _Nuisances for g-mSBD \(A_{0}\) in Eq. (1) are \(\{\mu_{0}^{i+1},\pi_{0}^{i}\}_{i=1}^{m-1}\) defined as follows. Let \(\mu_{0}^{m+1}=\mu^{m+1}\coloneqq\mathbbm{1}_{\mathbf{w}\setminus\mathbf{c}} \left(\mathbf{W}\setminus\mathbf{C}\right)\). For \(i=m-1,\cdots,1\),_

\[\mu_{0}^{i+1}(\overline{\mathbf{W}}^{i},\overline{\mathbf{R}}^{1: i}) \coloneqq\mathbb{E}_{P_{\sigma(\mathbf{Z}_{i+1})}}\left[\mu_{0}^{i+2}( \overline{\mathbf{W}}^{i+1},\mathbf{r}_{i+1},\overline{\mathbf{R}}^{1:i})| \overline{\mathbf{W}}^{i},\overline{\mathbf{R}}^{1:i},\mathbf{r}_{0},\mathbf{z }_{i+1}\right] \tag{7}\] \[\pi_{0}^{i}(\overline{\mathbf{W}}^{i},\overline{\mathbf{R}}^{1:i}) \coloneqq\frac{P_{\sigma(\mathbf{Z}_{i})}(\overline{\mathbf{W}}^ {i},\overline{\mathbf{R}}^{1:i-1}|\mathbf{z}_{i},\mathbf{r}_{0})}{P_{\sigma( \mathbf{Z}_{i+1})}(\overline{\mathbf{W}}^{i},\overline{\mathbf{R}}^{1:i-1}| \mathbf{z}_{i+1},\mathbf{r}_{0})}\frac{1_{\mathbf{r}_{i}}(\mathbf{R}_{i})}{P_ {\sigma(\mathbf{Z}_{i+1})}(\mathbf{R}_{i}|\overline{\mathbf{W}}^{i},\overline {\mathbf{R}}^{1:i-1},\mathbf{z}_{i+1},\mathbf{r}_{0})}. \tag{8}\]

**Definition 3** (**DR-g-mSBD Estimators**).: Let \(D_{\sigma(\mathbf{Z}_{i})}\) for \(\mathbf{Z}_{i}\in\mathbb{Z}\) denote the experimental samples from randomizing the variable \(\mathbf{Z}_{i}\). Let \(\overline{D}_{\mathbf{z}_{i}}\) for \(\mathbf{z}_{i}\in\mathfrak{D}_{\mathbf{z}_{i}}\) denote the subsamples of \(D_{\sigma(\mathbf{Z}_{i})}\) fixing \(\mathbf{R}_{0}\setminus\mathbf{Z}_{i}=\mathbf{r}_{0}\setminus\mathbf{z}_{i}\) and \(\mathbf{Z}_{i}=\mathbf{z}_{i}\). The DR-g-mSBD estimator \(\hat{A}\) for the g-mSBD adjustment \(A_{0}[\mathbf{W},\mathbf{C},\mathbf{R};\mathbb{Z}_{0}\coloneqq\{\mathbf{Z}_{i }\}_{i=1}^{m},\mathtt{seq}\coloneqq(\mathbf{z}_{i})_{i=1}^{m}](\mathbf{w} \setminus\mathbf{c},\mathbf{r})\) is defined as follows:

1. Randomly partition \(\overline{D}_{\mathbf{z}_{i}}\) into \(\{\overline{D}_{\mathbf{z}_{i},\ell}\}_{\ell\in[L]}\); i.e., \(\overline{D}_{\mathbf{z}_{i}}=\cup_{\ell=1}^{L}\overline{D}_{\mathbf{z}_{i},\ell}\), \(\forall\mathbf{Z}_{i}\in\mathbb{Z}\) and \(\mathbf{z}_{i}\in\mathfrak{D}_{\mathbf{z}_{i}}\).
2. For each fold \(\ell\in[L]\), let \(\mu_{\ell}^{i+1}\) denote learned \(\mu_{0}^{i+1}\) using \(\overline{D}_{\mathbf{z}_{i+1}}\setminus\overline{D}_{\mathbf{z}_{i+1},\ell}\) for \(i=m,\cdots,2\); and \(\pi_{\ell}^{i}\) learned \(\pi_{0}^{i}\) for \(i=1,\cdots,m-1\). Define \(\tilde{\mu}_{\ell}^{i+1}\coloneqq\mu_{\ell}^{i+1}(\overline{\mathbf{W}}^{i}, \mathbf{r}_{i},\overline{\mathbf{R}}^{1:i-1})\) and \(\overline{\pi}_{\ell}^{i}\coloneqq\prod_{j=1}^{i}\pi_{\ell}^{j}\).
3. Estimate \(\hat{A}\coloneqq\hat{A}(\{\mu_{\ell}^{j+1},\pi_{\ell}^{j}\}_{j\in[m-1],\ell \in[L]})\coloneqq(1/L)\sum_{\ell=1}^{L}\hat{A}_{\ell}(\{\mu_{\ell}^{j+1}, \pi_{\ell}^{j}\}_{j\in[m-1]})\) where \[\hat{A}_{\ell}\coloneqq\hat{A}_{\ell}(\{\mu_{\ell}^{j+1},\pi_{\ell}^{j}\}_{j \in[m-1]})\coloneqq\sum_{j=1}^{m-1}\mathbb{E}_{\overline{D}_{\mathbf{z}_{j+1 },\ell}}\left[\overline{\pi}_{\ell}^{j}\{\hat{\mu}_{\ell}^{j+2}-\mu_{\ell}^{j +1}\}\right]+\mathbb{E}_{\overline{D}_{\mathbf{z}_{1},\ell}}\left[\tilde{\mu} _{\ell}^{2}\right],\] (9) where \(\mathbb{E}_{\overline{D}_{\mathbf{z}_{j},\ell}}\left[\cdot\right]\) is an empirical average over samples \(\overline{D}_{\mathbf{z}_{j},\ell}\).

We analyze the bias of the g-mSBD estimator using the following results:

**Lemma S.2** (**Representation of g-mSBD**).: _The g-mSBD adjustment \(A_{0}\) in Def. 1 can be represented as_

\[A_{0}=\sum_{i=1}^{m-1}\mathbb{E}_{P_{\sigma(\mathbf{Z}_{i+1})}}\left[\overline{ \pi}_{0}^{i}\{\hat{\mu}_{0}^{i+2}-\mu_{0}^{i+1}\}|\mathbf{z}_{i+1},\mathbf{r}_{ 0}\right]+\mathbb{E}_{P_{\sigma(\mathbf{Z}_{1})}}\left[\tilde{\mu}_{0}^{2}| \mathbf{z}_{1},\mathbf{r}_{0}\right],\] (B.25)

_where \(\tilde{\mu}_{\ell}^{i+1}(\overline{\mathbf{W}}^{i},\overline{\mathbf{R}}^{1:i- 1})\coloneqq\mu_{\ell}^{i+1}(\overline{\mathbf{W}}^{i},\mathbf{r}_{i}, \overline{\mathbf{R}}^{1:i-1})\) and \(\overline{\pi}^{i}\coloneqq\prod_{j=1}^{i}\pi^{j}\) as defined in Def. 3._

Proof of Lemma s.2.: Throughout the proof, we will use \(\mathbf{w}^{\prime}\backslash\mathbf{c}^{\prime}\) as some realization of \(\mathbf{W}\setminus\mathbf{C}\). Recall that \(\mathbbm{1}_{\mathbf{w}\setminus\mathbf{c}}(\mathbf{w}^{\prime}\backslash \mathbf{c}^{\prime})=1\) when \(\mathbf{w}^{\prime}\backslash\mathbf{c}^{\prime}=\mathbf{w}\backslash\mathbf{c}\) and zero otherwise. We first recall that

\[A_{0}=\sum_{\mathbf{w}^{\prime}\in\mathfrak{C}_{\mathbf{W}}}\prod_{i:\mathbf{W} _{i}\in\mathbf{W}}\mathbbm{1}_{\mathbf{w}\setminus\mathbf{c}}(\mathbf{w}^{ \prime}\backslash\mathbf{c}^{\prime})P_{\sigma(\mathbf{Z}_{i})}(w^{\prime}_{i}| \overline{\mathbf{w}^{\prime}}^{i-1},\mathbf{\mathbf{r}}^{i-1},\mathbf{z}_{i}),\] (B.26)

by the definition of the experimental distribution \(P_{\sigma(\mathbf{Z}_{i})}\).

For all \(i=1,\cdots,m-1\),

\[\mathbb{E}_{P_{\sigma(\mathbf{Z}_{i+1})}}\left[\overline{\pi}_{0} ^{i}(\overline{\mathbf{W}}^{i},\overline{\mathbf{R}}^{1:i})\{\tilde{\mu}_{0}^{i +2}(\overline{\mathbf{W}}^{i+1},\overline{\mathbf{R}}^{1:i})-\mu_{0}^{i+1}( \overline{\mathbf{W}}^{i},\overline{\mathbf{R}}^{1:i}))|\mathbf{z}_{i+1}, \mathbf{r}_{0}\right]\] (B.27) \[\stackrel{{\ref{def:g-mSBD}}}{{=}}\mathbb{E}_{P_{ \sigma(\mathbf{Z}_{i+1})}}\left[\overline{\pi}_{0}^{i}(\overline{\mathbf{W}}^{i}, \overline{\mathbf{R}}^{1:i})\{\tilde{\mu}_{0}^{i+2}(\overline{\mathbf{W}}^{i+1},\overline{\mathbf{R}}^{1:i})|\overline{\mathbf{W}}^{i},\overline{\mathbf{R}}^{1 :i},\mathbf{z}_{i+1},\mathbf{r}_{0}\right]\] (B.28) \[\stackrel{{\ref{def:g-mSBD}}}{{=}}\mathbb{E}_{P_{ \sigma(\mathbf{Z}_{i+1})}}\left[\overline{\pi}_{0}^{i}(\overline{\mathbf{W}}^{i}, \overline{\mathbf{R}}^{1:i})\{\mu_{0}^{i+1}(\overline{\mathbf{W}}^{i}, \overline{\mathbf{R}}^{1:i})-\mu_{0}^{i+1}(\overline{\mathbf{W}}^{i},\overline{ \mathbf{R}}^{1:i})\}|\mathbf{z}_{i+1},\mathbf{r}_{0}\right]\] (B.29) \[=0,\] (B.30)

where the equation \(\stackrel{{\ref{def:g-mSBD}}}{{=}}\) holds by the total law of expectation, and \(\stackrel{{\ref{def:g-mSBD}}}{{=}}\) holds by the definition of \(\tilde{\mu}_{0}^{i+1}\).

It suffices to show that

\[\mathbb{E}_{P_{\sigma(\mathbf{Z}_{1})}}\left[\mu_{0}^{2}(\overline{\mathbf{W}}^{1}, \overline{\mathbf{r}}^{1})|\mathbf{z}_{1},\mathbf{r}_{0}\right]=A_{0}=\sum_{ \mathbf{w}^{\prime}\in\mathfrak{O}_{\overline{\mathbf{W}}^{k+1:m}}}\mathbb{1}_{ \mathbf{w}\backslash\mathbf{c}}(\mathbf{w}^{\prime}\backslash\mathbf{c}^{\prime })\prod_{j=1}^{m}P_{\sigma(\mathbf{Z}_{j})}(w^{\prime}_{j}|\overline{\mathbf{w} ^{\prime}}^{j-1},\overline{\mathbf{r}}^{j-1},\mathbf{z}_{j}).\] (B.31)

To prove the equation, we show that, for all \(k=m,m-1,\cdots,2\),

\[\mu_{0}^{k}(\overline{\mathbf{W}}^{k-1},\overline{\mathbf{R}}^{1:k-1})=\sum_{ \overline{\mathbf{w}}^{\prime k:m}\in\mathfrak{O}_{\overline{\mathbf{W}}^{k:m} }}\mathbb{1}_{\mathbf{w}\backslash\mathbf{c}}(\mathbf{w}^{\prime}\backslash \mathbf{c}^{\prime})\prod_{j=k}^{m}P_{\sigma(\mathbf{Z}_{j})}(w^{\prime}_{j}| \overline{\mathbf{W}}^{k-1},\overline{\mathbf{R}}^{1:k-1},\overline{\mathbf{ w}}^{k:j-1},\overline{\mathbf{r}}^{k:j-1},\mathbf{r}_{0},\mathbf{z}_{j}).\] (B.32)

This equation holds when \(k=m\), because

\[\mu_{0}^{m}(\overline{\mathbf{W}}^{m-1},\overline{\mathbf{R}}^{1: m-1}) \coloneqq\mathbb{E}_{P_{\sigma(\mathbf{Z}_{m})}}\left[\mathbb{1}_{ \mathbf{w}\backslash\mathbf{c}}(\mathbf{w}^{\prime}\backslash\mathbf{c}^{\prime })\big{|}\overline{\mathbf{W}}^{m-1},\overline{\mathbf{R}}^{1:m-1},\mathbf{r} _{0},\mathbf{z}_{m}\right]\] (B.33) \[=\sum_{w^{\prime}_{m}\in\mathfrak{O}_{\overline{\mathbf{W}}^{m-1 :m}}}\mathbb{1}_{\mathbf{w}\backslash\mathbf{c}}(\mathbf{w}^{\prime}\backslash \mathbf{c}^{\prime})P_{\sigma(\mathbf{Z}_{m})}(w^{\prime}_{m}|\overline{ \mathbf{W}}^{m-1},\overline{\mathbf{R}}^{1:m-1},\mathbf{r}_{0},\mathbf{z}_{m}).\] (B.34)

For \(k=m-1\),

\[\mu_{0}^{m-1}(\overline{\mathbf{W}}^{m-2},\overline{\mathbf{R}}^ {1:m-2})\] (B.35) \[\coloneqq\mathbb{E}_{P_{\sigma(\mathbf{Z}_{m-1})}}\left[\tilde{ \mu}_{0}^{m}(\overline{\mathbf{W}}^{m-1},\mathbf{r}_{m-1},\overline{\mathbf{R }}^{1:m-2})|\overline{\mathbf{W}}^{m-2},\overline{\mathbf{R}}^{1:m-2},\mathbf{ z}_{m-1},\mathbf{r}_{0}\right]\] (B.36) \[=\sum_{\overline{\mathbf{w}}^{\prime m}-1:m}\in\mathfrak{O}_{ \overline{\mathbf{W}}^{m-1:m}}\mathbb{1}_{\mathbf{w}\backslash\mathbf{c}}( \mathbf{w}^{\prime}\backslash\mathbf{c}^{\prime})\prod_{j=m-1}^{m}P_{\sigma( \mathbf{Z}_{j})}(w^{\prime}_{j}|\overline{\mathbf{w}}^{\prime m-1:j-1}, \overline{\mathbf{r}}^{m-1:j-1},\overline{\mathbf{W}}^{m-2},\overline{\mathbf{ R}}^{1:m-2},\mathbf{z}_{j},\mathbf{r}_{0})\] (B.37)

Based on this observation, we make the following induction hypothesis: Suppose, for a fixed \(k\in\{2,\cdots,m\}\), the following holds:

\[\mu_{0}^{k+1}(\overline{\mathbf{W}}^{k},\overline{\mathbf{R}}^{1:k})\stackrel{{ \text{induction}}}{{=}}\sum_{\overline{\mathbf{w}}^{k+1:m}\in\mathfrak{O}_{ \overline{\mathbf{W}}^{k+1:m}}}\mathbb{1}_{\mathbf{w}\backslash\mathbf{c}}( \mathbf{w}^{\prime}\backslash\mathbf{c}^{\prime})\prod_{j=k+1}^{m}P_{\sigma( \mathbf{Z}_{j})}(w^{\prime}_{j}|\overline{\mathbf{W}}^{k},\overline{\mathbf{ R}}^{1:k},\overline{\mathbf{w}}^{\prime k+1:j-1},\mathbf{r}^{k+1:j-1},\mathbf{z}_{j}, \mathbf{r}_{0}).\] (B.38)

Then, the induction hypothesis holds for \(k-1\) as follows:

\[\mu_{0}^{k}(\overline{\mathbf{W}}^{k-1},\overline{\mathbf{R}}^{1: k-1})\] (B.39) \[\coloneqq\mathbb{E}_{P_{\sigma(\mathbf{Z}_{k+1})}}\left[\tilde{ \mu}^{k+1}(\overline{\mathbf{W}}^{k},\overline{\mathbf{R}}^{1:k-1},\mathbf{r} _{k})|\overline{\mathbf{W}}^{k-1},\overline{\mathbf{R}}^{1:k-1},\mathbf{z}_{k +1},\mathbf{r}_{0}\right]\] (B.40) \[=\sum_{\overline{\mathbf{w}}^{\prime k:m}\in\mathfrak{O}_{ \overline{\mathbf{W}}^{k:m}}}\mathbb{1}_{\mathbf{w}\backslash\mathbf{c}}( \mathbf{w}^{\prime}\backslash\mathbf{c}^{\prime})\prod_{j=k+1}^{m}P_{\sigma( \mathbf{Z}_{j})}(w^{\prime}_{j}|\overline{\mathbf{W}}^{k-1},\overline{ \mathbf{R}}^{1:k-1},\overline{\mathbf{w}}^{\prime k:j-1},\overline{\mathbf{r}}^{ k:j-1},\mathbf{z}_{j},\mathbf{r}_{0})P_{\sigma(\mathbf{Z}_{k})}(w^{\prime}_{k}| \overline{\mathbf{W}}^{k-1},\overline{\mathbf{R}}^{1:k-1},\mathbf{z}_{k}, \mathbf{r}_{0})\] (B.41) \[=\sum_{\overline{\mathbf{w}}^{\prime k:m}\in\mathfrak{O}_{\overline{ \mathbf{W}}^{k:m}}}\mathbb{1}_{\mathbf{w}\backslash\mathbf{c}}(\mathbf{w}^{ \prime}\backslash\mathbf{c}^{\prime})\prod_{j=k}^{m}P_{\sigma(\mathbf{Z}_{j})}(w^{ \prime}_{j}|\overline{\mathbf{W}}^{k-1},\overline{\mathbf{R}}^{1:k-1}, \overline{\mathbf{w}}^{\prime k:j-1},\overline{\mathbf{r}}^{k:j-1},\mathbf{z}_{ j},\mathbf{r}_{0}).\] (B.42)

Also, we already checked that the induction hypothesis holds for \(k=m\). Therefore, the hypothesis holds for all \(k=2,\cdots,m\):

\[\mu_{0}^{k}(\overline{\mathbf{W}}^{k-1},\overline{\mathbf{R}}^{1:k-1})=\sum_{ \overline{\mathbf{w}}^{\prime k:m}\in\mathfrak{O}_{\overline{\mathbf{W}}^{k:m}}} \mathbb{1}_{\mathbf{w}\backslash\mathbf{c}}(\mathbf{w}^{\prime}\backslash \mathbf{c}^{\prime})\prod_{j=k}^{m}P_{\sigma(\mathbf{Z}_{j})}(w^{\prime}_{j}| \overline{\mathbf{W}}^{k-1},\overline{\mathbf{R}}^{1:k-1},\overline{\mathbf{w}}^{ \prime k:j-1},\overline{\mathbf{r}}^{k:j-1},\mathbf{z}_{j},\mathbf{r}_{0}).\] (B.43)

[MISSING_PAGE_FAIL:29]

Therefore,

\[\text{Eq.~{}\eqref{eq:B3}}+\text{Eq.~{}\eqref{eq:B3}}\] \[=\text{Eq.~{}\eqref{eq:B3}}+\text{Eq.~{}\eqref{eq:B3}}\] \[=\mathbb{E}_{P_{\sigma}(\mathbf{z}_{i+1})}\left[\overline{\pi}^{i} \{\hat{\mu}_{0}^{i+2}-\mu^{i+1}\}|\mathbf{z}_{i+1},\mathbf{r}_{0}\right]+\text{ Eq.~{}\eqref{eq:B3}}\] \[=\mathbb{E}_{P_{\sigma}(\mathbf{z}_{i+1})}\left[\overline{\pi}^{i} \{\mu_{0}^{i+1}-\mu^{i+1}\}|\mathbf{z}_{i+1},\mathbf{r}_{0}\right]+\text{Eq.~{} \eqref{eq:B3}}\] \[=\mathbb{E}_{P_{\sigma}(\mathbf{z}_{i+1})}\left[\overline{\pi}^{i }\{\mu_{0}^{i+1}-\mu^{i+1}\}|\mathbf{z}_{i+1},\mathbf{r}_{0}\right]+\mathbb{E} _{P_{\sigma}(\mathbf{z}_{i+1})}\left[\overline{\pi}^{i-1}\pi_{0}^{i}\{\mu^{i+ 1}-\mu_{0}^{i+1}\}|\mathbf{z}_{i+1},\mathbf{r}_{0}\right]\] \[=\mathbb{E}_{P_{\sigma}(\mathbf{z}_{i+1})}\left[\overline{\pi}^{i -1}\{\mu_{0}^{i+1}-\mu^{i+1}\}\{\pi^{i}-\pi_{0}^{i}\}|\mathbf{z}_{i+1},\mathbf{ r}_{0}\right].\] (B.56)

Finally,

**Lemma S.4** (Bias Analysis of g-mSBD Estimators (2)).: _Let \(\overline{A}\) be the quantity defined as_

\[\overline{A}\coloneqq\sum_{i=1}^{m-1}\mathbb{E}_{P_{\sigma}(\mathbf{z}_{i+1})} \left[\overline{\pi}^{i}\{\hat{\mu}^{i+2}-\mu^{i+1}\}|\mathbf{z}_{i+1},\mathbf{ r}_{0}\right]+\mathbb{E}_{P_{\sigma}(\mathbf{z}_{1})}\left[\hat{\mu}^{2}|\mathbf{z}_{1}, \mathbf{r}_{0}\right].\] (B.57)

_Then, for \(k=3,\cdots,m-1\),_

\[\overline{A}-A_{0} =\sum_{r=k}^{m-1}\mathbb{E}_{P_{\sigma}(\mathbf{z}_{r+1})}\left[ \overline{\pi}^{r-1}\{\mu_{0}^{r+1}-\mu^{r+1}\}\{\pi^{r}-\pi_{0}^{r}\}| \mathbf{z}_{r+1},\mathbf{r}_{0}\right]+\mathbb{E}_{P_{\sigma}(\mathbf{z}_{k})} \left[\overline{\pi}^{k-1}\{\hat{\mu}_{0}^{k+1}-\mu^{k}\}|\mathbf{z}_{k}, \mathbf{r}_{0}\right]\] \[+\sum_{i=1}^{k-2}\mathbb{E}_{P_{\sigma}(\mathbf{z}_{i+1})}\left[ \overline{\pi}^{i}\{\hat{\mu}^{i+2}-\mu^{i+1}\}|\mathbf{z}_{i+1},\mathbf{r}_{0 }\right]+\mathbb{E}_{P_{\sigma}(\mathbf{z}_{1})}\left[\hat{\mu}^{2}-\hat{\mu}_ {0}^{2}|\mathbf{z}_{1},\mathbf{r}_{0}\right].\]

Proof of Lemma s.4.: The equation holds for \(k=m-1\). It can be shown as follows:

\[\overline{A}-A_{0}\] \[=\sum_{i=1}^{m-1}\mathbb{E}_{P_{\sigma}(\mathbf{z}_{i+1})}\left[ \overline{\pi}^{i}\{\hat{\mu}^{i+2}-\mu^{i+1}\}|\mathbf{z}_{i+1},\mathbf{r}_{0 }\right]+\mathbb{E}_{P_{\sigma}(\mathbf{z}_{1})}\left[\hat{\mu}^{2}-\hat{\mu}_ {0}^{2}|\mathbf{z}_{1},\mathbf{r}_{0}\right]\] \[=\mathbb{E}_{P_{\sigma}(\mathbf{z}_{m+1})}\left[\overline{\pi}^{ m}\{\hat{\mu}_{0}^{m+2}-\mu^{m+1}\}|\mathbf{z}_{m+1},\mathbf{r}_{0}\right]+ \mathbb{E}_{P_{\sigma}(\mathbf{z}_{m})}\left[\overline{\pi}^{m-1}\{\hat{\mu}^ {m+1}-\mu^{m}\}|\mathbf{z}_{m},\mathbf{r}_{0}\right]\] (B.58) \[+\sum_{i=1}^{m-2}\mathbb{E}_{P_{\sigma}(\mathbf{z}_{i+1})}\left[ \overline{\pi}^{i}\{\hat{\mu}^{i+2}-\mu^{i+1}\}|\mathbf{z}_{i+1},\mathbf{r}_{0 }\right]+\mathbb{E}_{P_{\sigma}(\mathbf{z}_{1})}\left[\hat{\mu}^{2}-\hat{\mu}_ {0}^{2}|\mathbf{z}_{1},\mathbf{r}_{0}\right].\] (B.59)

Then,

\[\text{Eq.~{}\eqref{eq:B3}}\] \[\overset{\text{Lemms~{}\ref{eq:B3}}}{=}\mathbb{E}_{P_{\sigma}( \mathbf{z}_{m+1})}\left[\overline{\pi}^{m-1}\{\mu_{0}^{m+1}-\mu^{m+1}\}\{\pi^{m} -\pi_{0}^{m}\}|\mathbf{z}_{m+1},\mathbf{r}_{0}\right]+\mathbb{E}_{P_{\sigma}( \mathbf{z}_{m})}\left[\overline{\pi}^{m-1}\{\hat{\mu}_{0}^{m}-\mu^{m-1}\}| \mathbf{z}_{m},\mathbf{r}_{0}\right].\] (B.60)

Therefore,

\[\overline{A}-A_{0}\] \[=\text{Eq.~{}\eqref{eq:B3}}+\sum_{i=1}^{m-2}\mathbb{E}_{P_{ \sigma}(\mathbf{z}_{i+1})}\left[\overline{\pi}^{i}\{\hat{\mu}^{i+2}-\mu^{i+1}\}| \mathbf{z}_{i+1},\mathbf{r}_{0}\right]+\mathbb{E}_{P_{\sigma}(\mathbf{z}_{1})} \left[\hat{\mu}^{2}-\hat{\mu}_{0}^{2}|\mathbf{z}_{1},\mathbf{r}_{0}\right].\] (B.61)For any fixed \(k+1\in\{m-1,\cdots,4\}\), suppose the following holds:

\[\overline{A}-A_{0}\] \[=\sum_{r=k+1}^{m-1}\mathbb{E}_{P_{\sigma(\mathbf{z}_{k+1})}}\left[ \overline{\pi}^{r-1}\{\mu_{0}^{r}-\mu^{r}\}\{\pi^{r+1}-\pi_{0}^{r+1}\}|\mathbf{ z}_{r+1},\mathbf{r}_{0}\right]\] (B.62) \[+\mathbb{E}_{P_{\sigma(\mathbf{z}_{k+1})}}\left[\overline{\pi}^{k }\{\hat{\mu}_{0}^{k+2}-\mu^{k+1}\}|\mathbf{z}_{k+1},\mathbf{r}_{0}\right]\] (B.63) \[+\sum_{i=1}^{k-1}\mathbb{E}_{P_{\sigma(\mathbf{z}_{i+1})}}\left[ \overline{\pi}^{i}\{\hat{\mu}^{i+2}-\mu^{i+1}\}|\mathbf{z}_{i+1},\mathbf{r}_{ 0}\right]\] (B.64) \[+\mathbb{E}_{P_{\sigma(\mathbf{z}_{1})}}\left[\tilde{\mu}^{2}- \tilde{\mu}_{0}^{2}|\mathbf{z}_{1},\mathbf{r}_{0}\right].\] (B.65)

We note that this holds when \(k=m-2\), as shown in Eq. (B.61). We will now show that it will hold for \(k\), too. First,

Eq. (B.63) \[+\text{Eq.~{}\eqref{eq:k_k_1}}\] (B.66) \[=\mathbb{E}_{P_{\sigma(\mathbf{z}_{k+1})}}\left[\overline{\pi}^{k }\{\hat{\mu}_{0}^{k+2}-\mu^{k+1}\}|\mathbf{z}_{k+1},\mathbf{r}_{0}\right]+ \mathbb{E}_{P_{\sigma(\mathbf{z}_{k})}}\left[\overline{\pi}^{k-1}\{\hat{\mu}^{k +2}-\mu^{k+1}\}|\mathbf{z}_{k},\mathbf{r}_{0}\right]\] \[+\sum_{i=1}^{k-2}\mathbb{E}_{P_{\sigma(\mathbf{z}_{i+1})}}\left[ \overline{\pi}^{i}\{\hat{\mu}^{i+2}-\mu^{i+1}\}|\mathbf{z}_{i+1},\mathbf{r}_{0 }\right],\] \[\overset{\text{Lemma~{}S.3}}{\mathbb{E}}_{P_{\sigma(\mathbf{z}_{ k+1})}}\left[\overline{\pi}^{k-1}\{\mu_{0}^{k+1}-\mu^{k+1}\}\{\pi^{k}-\pi_{0}^{k} \}|\mathbf{z}_{k+1},\mathbf{r}_{0}\right]+\mathbb{E}_{P_{\sigma(\mathbf{z}_{k} )}}\left[\overline{\pi}^{k-1}\{\hat{\mu}_{0}^{k+1}-\mu^{k}\}|\mathbf{z}_{k}, \mathbf{r}_{0}\right]\] \[+\sum_{i=1}^{k-2}\mathbb{E}_{P_{\sigma(\mathbf{z}_{i+1})}}\left[ \overline{\pi}^{i}\{\hat{\mu}^{i+2}-\mu^{i+1}\}|\mathbf{z}_{i+1},\mathbf{r}_{ 0}\right].\] (B.67)

Therefore,

\[\overline{A}-A_{0}\] \[=\text{Eq.~{}\eqref{eq:k_1}}+\text{Eq.~{}\eqref{eq:k_1}}+\text{Eq.~ {}\eqref{eq:k_1}}+\text{Eq.~{}\eqref{eq:k_1}}\] \[=\text{Eq.~{}\eqref{eq:k_1}}+\text{Eq.~{}\eqref{eq:k_1}}+\text{Eq.~ {}\eqref{eq:k_1}}\] \[=\sum_{r=k+1}^{m}\mathbb{E}_{P_{\sigma(\mathbf{z}_{r+1})}}\left[ \overline{\pi}^{r-1}\{\mu_{0}^{r+1}-\mu^{r+1}\}\{\pi^{r}-\pi_{0}^{r}\}|\mathbf{ z}_{r+1},\mathbf{r}_{0}\right]+\mathbb{E}_{P_{\sigma(\mathbf{z}_{k+1})}}\left[ \overline{\pi}^{k-1}\{\mu_{0}^{k+1}-\mu^{k_{1}}\}\{\pi^{k}-\pi_{0}^{k}\}| \mathbf{z}_{k+1},\mathbf{r}_{0}\right]\] \[+\mathbb{E}_{P_{\sigma(\mathbf{z}_{k})}}\left[\overline{\pi}^{k-1 }\{\hat{\mu}_{0}^{k+1}-\mu^{k}\}|\mathbf{z}_{k},\mathbf{r}_{0}\right]\] \[+\sum_{i=1}^{k-2}\mathbb{E}_{P_{\sigma(\mathbf{z}_{i+1})}}\left[ \overline{\pi}^{i}\{\hat{\mu}^{i+2}-\mu^{i+1}\}|\mathbf{z}_{i+1},\mathbf{r}_{0}\right]\] \[+\text{Eq.~{}\eqref{eq:k_1}}\]

Therefore, the equation holds for \(k\), too. This completes the proof. 

**Lemma S.5** (Bias Analysis of g-mSBD Estimators (3)).: \(\mathbb{E}_{P_{\sigma(\mathbf{z}_{2})}}\left[\pi^{1}\{\hat{\mu}_{0}^{3}-\mu^{2 }\}|\mathbf{z}_{2},\mathbf{r}_{0}\right]-\mathbb{E}_{P_{\sigma(\mathbf{z}_{1} )}}\left[\tilde{\mu}^{2}-\tilde{\mu}_{0}^{2}|\mathbf{z}_{1},\mathbf{r}_{0} \right]=\mathbb{E}_{P_{\sigma(\mathbf{z}_{2})}}\left[\{\mu_{0}^{2}-\mu^{2}\}\{ \pi^{1}-\pi_{0}^{1}\}|\mathbf{z}_{2},\mathbf{r}_{0}\right].\)Proof of Lemma s.5.: Note that

\[\mathbb{E}_{P_{\sigma(\mathbf{Z}_{1})}}\left[\hat{\mu}^{2}(\mathbf{W }_{1},\mathbf{R}_{0})-\hat{\mu}_{0}^{2}(\mathbf{W}_{1},\mathbf{R}_{0})|\mathbf{ z}_{1},\mathbf{r}_{0}\right]\] \[=\mathbb{E}_{P_{\sigma(\mathbf{Z}_{2})}}\left[\frac{P_{\sigma( \mathbf{Z}_{1})}(\mathbf{W}_{1},\mathbf{R}_{0}|\mathbf{z}_{1},\mathbf{r}_{0})}{ P_{\sigma(\mathbf{Z}_{2})}(\mathbf{W}_{1},\mathbf{R}_{0}|\mathbf{z}_{2}, \mathbf{r}_{0})}\{\hat{\mu}^{2}(\mathbf{W}_{1},\mathbf{R}_{0})-\hat{\mu}_{0}^{ 2}(\mathbf{W}_{1},\mathbf{R}_{0})\}\middle|\mathbf{z}_{2},\mathbf{r}_{0}\right]\] \[=\mathbb{E}_{P_{\sigma(\mathbf{Z}_{2})}}\left[\frac{P_{\sigma( \mathbf{Z}_{1})}(\mathbf{W}_{1},\mathbf{R}_{0}|\mathbf{z}_{1},\mathbf{r}_{0})} {P_{\sigma(\mathbf{Z}_{2})}(\mathbf{W}_{1},\mathbf{R}_{0}|\mathbf{z}_{2}, \mathbf{r}_{0})}\frac{\mathbb{1}_{r_{1}}(\mathbf{R}_{1})}{P_{\sigma(\mathbf{Z }_{2})}(\mathbf{R}_{1}|\mathbf{W}_{0},\mathbf{z}_{2},\mathbf{r}_{0})}\{\mu^{2} (\mathbf{W}_{1},\mathbf{R}_{1})-\mu_{0}^{2}(\mathbf{W}_{1},\mathbf{R}_{1})\} \middle|\mathbf{z}_{2},\mathbf{r}_{0}\right]\] \[=\mathbb{E}_{P_{\sigma(\mathbf{Z}_{2})}}\left[\pi_{0}^{1}(\mathbf{ W}_{1},X_{1})\{\mu^{2}(\mathbf{W}_{1},X_{1})-\mu_{0}^{2}(\mathbf{W}_{1},X_{1}) \}|\mathbf{z}_{2},\mathbf{r}_{0}\right].\]

Therefore,

\[\mathbb{E}_{P_{\sigma(\mathbf{Z}_{2})}}\left[\pi^{1}\{\hat{\mu}_{ 0}^{3}-\mu^{2}\}|\mathbf{z}_{2},\mathbf{r}_{0}\right]-\mathbb{E}_{P_{\sigma( \mathbf{Z}_{1})}}\left[\hat{\mu}^{2}-\hat{\mu}_{0}^{2}|\mathbf{z}_{1},\mathbf{ r}_{0}\right]\] \[=\mathbb{E}_{P_{\sigma(\mathbf{Z}_{2})}}\left[\pi^{1}\{\mu_{0}^{ 2}-\mu^{2}\}|\mathbf{z}_{2},\mathbf{r}_{0}\right]-\mathbb{E}_{P_{\sigma( \mathbf{Z}_{1})}}\left[\hat{\mu}^{2}-\hat{\mu}_{0}^{2}|\mathbf{z}_{1},\mathbf{ r}_{0}\right]\] \[=\mathbb{E}_{P_{\sigma(\mathbf{Z}_{2})}}\left[\pi^{1}\{\mu_{0}^{ 2}-\mu^{2}\}|\mathbf{z}_{2},\mathbf{r}_{0}\right]-\mathbb{E}_{P_{\sigma( \mathbf{Z}_{2})}}\left[\pi_{0}^{1}\{\mu_{0}^{2}-\mu^{2}\}|\mathbf{z}_{2}, \mathbf{r}_{0}\right]\] \[=\mathbb{E}_{P_{\sigma(\mathbf{Z}_{2})}}\left[\{\mu_{0}^{2}-\mu^{ 2}\}\{\pi^{1}-\pi_{0}^{1}\}|\mathbf{z}_{2},\mathbf{r}_{0}\right].\]

**Lemma S.6** (**Bias Analysis of g-mSBD Estimators**).: _Let \(\overline{A}\) be the quantity defined as_

\[\overline{A}\coloneqq\sum_{i=1}^{m-1}\mathbb{E}_{P_{\sigma(\mathbf{Z}_{i+1})}} \left[\overline{\pi}^{i}\{\hat{\mu}^{i+2}-\mu^{i+1}\}|\mathbf{z}_{i+1},\mathbf{ r}_{0}\right]+\mathbb{E}_{P_{\sigma(\mathbf{Z}_{1})}}\left[\hat{\mu}^{2}|\mathbf{z}_{1}, \mathbf{r}_{0}\right],\] (B.68)

_where \(\pi^{i},\mu^{i}\) are arbitrary nuisances for true nuisances \(\pi_{0}^{i}\) and \(\mu_{0}^{i}\) defined in Def. 2. Let \(A_{0}\) denote the g-mSBD in Def. 1. Then,_

\[\overline{A}-A_{0}=\sum_{r=1}^{m-1}O_{P_{\sigma(\mathbf{Z}_{r+1})}}\left(\| \pi^{r}-\pi_{0}^{r}\|\|\mu^{r+1}-\mu_{0}^{r+1}\|\right).\] (B.69)

Proof of Lemma s.6.: By Lemmas (S.3,S.4,S.5).

We also use the following results which are used by Kennedy et al. [2020].

**Lemma S.7** (**Decomposition**).: _Let \(\mathcal{D}\sim P\) denote a finite sample set following a distribution \(P\). Let \(h(V;\eta)\) denote an arbitrary random function taking \(\eta\) as a nuisance. For any \(\eta,\eta_{0}\),_

\[\mathbb{E}_{\mathcal{D}}\left[h(\mathbf{V};\eta)\right]-\mathbb{E }_{P}\left[h(\mathbf{V};\eta_{0})\right]\] (B.70) \[=\mathbb{E}_{\mathcal{D}-P}\left[h(V;\eta_{0})\right]+\mathbb{E}_ {\mathcal{D}-P}\left[h(V;\eta)-h(V;\eta_{0})\right]+\mathbb{E}_{P}\left[h(V; \eta)-h(V;\eta_{0})\right].\] (B.71)

Proof of Lemma s.7.: 

**Lemma S.8** (**Continuous Mapping Theorem for \(L_{2}(p)\))**.: _Let \(X_{n},X\) denote a random sequence defined on a metric space \(S\). Suppose a function \(g:S\to S^{\prime}\) (where \(S^{\prime}\) is another metric space) is bounded and continuous almost everywhere. Then,_

\[X_{n}\overset{L_{2}(P)}{\rightarrow}X\implies g(X_{n})\overset{L_{2}(P)}{ \rightarrow}g(X).\] (B.75)

[MISSING_PAGE_EMPTY:33]

By the central limit theorem, we note that \(R^{a}_{i,\ell}\) for \(i=1,\cdots,m\) is a random variable such that \(n^{1/2}_{i,\ell}R^{a}_{i,\ell}\) converges in distribution to a mean-zero normal random variable. Therefore,

\[\text{Eq.~{}\eqref{eq:R1}}=\sum_{i=1}^{m}R^{a}_{i,\ell},\] (B.84)

also behaves the same.

We now analyze the second term. Define

\[R^{b}_{1,\ell}\coloneqq\mathbb{E}_{\overline{\mathcal{D}}_{\mathbf{z}_{1,\ell} }-P_{\sigma(\mathbf{z}_{1})|\mathbf{z}_{1,\sigma_{0}}}}\left[\tilde{\mu}_{\ell }^{2}-\tilde{\mu}_{0}^{2}\right],\] (B.85)

and for \(i=1,2,\cdots,m-1\),

\[R^{b}_{i+1,\ell}\coloneqq\mathbb{E}_{\overline{\mathcal{D}}_{ \mathbf{z}_{i+1,\ell}}-P_{\sigma(\mathbf{z}_{i+1})|\mathbf{z}_{i+1,\sigma_{0}} }}\left[\overline{\pi}_{\ell}^{i}\{\dot{\mu}_{\ell}^{i+2}-\mu_{\ell}^{i+1}\}- \overline{\pi}_{0}^{i}\{\dot{\mu}_{0}^{i+2}-\mu_{0}^{i+1}\}\right].\] (B.86)

By [14, Lemma 2] and the continuous mapping theorem in Lemma S.8,

\[R^{b}_{1,\ell}=O_{P_{\sigma(\mathbf{z}_{1})}}\left(\frac{\|\dot{\mu}_{\ell}^{2 }-\dot{\mu}_{0}^{2}\|}{\sqrt{n_{1,\ell}}}\right),\] (B.87)

and for \(i=1,\cdots,m-1\),

\[R^{b}_{i+1,\ell}=O_{P_{\sigma(\mathbf{z}_{i+1})}}\left(\frac{\| \overline{\pi}_{\ell}^{i}\{\dot{\mu}_{\ell}^{i+2}-\mu_{\ell}^{i+1}\}-\overline {\pi}_{0}^{i}\{\dot{\mu}_{0}^{i+2}-\mu_{0}^{i+1}\}\|}{\sqrt{n_{i+1,\ell}}} \right).\] (B.88)

Under the given assumption, for \(i=1,2,\cdots,m\),

\[R^{b}_{i,\ell}=o_{P_{\sigma(\mathbf{z}_{i})}}(1),\] (B.89)

and

\[\text{Eq.~{}\eqref{eq:R1}}=\sum_{i=1}^{m}o_{P_{\sigma(\mathbf{z}_{i})}}(1).\] (B.90)

Define

\[R_{i,\ell}\coloneqq R^{a}_{i,\ell}+R^{b}_{i,\ell}.\] (B.91)

Then, \(R_{i,\ell}\) is also a random variable such that \(n^{1/2}_{i,\ell}R_{i,\ell}\) converges in distribution to a mean-zero normal random variable, by Slutsky's theorem.

We now analyze the third term. By Lemma S.6, the third term can be analyzed as follow:

\[\text{Eq.~{}\eqref{eq:R1}}=\sum_{i=1}^{m-1}O_{P_{\sigma(\mathbf{z}_{i+1})}} \left(\|\mu_{\ell}^{i+1}-\mu_{0}^{i+1}\|\|\pi_{\ell}^{i}-\pi_{0}^{i}\|\right).\] (B.92)

Therefore,

\[\hat{A}_{\ell}-A_{0} =\text{Eq.~{}\eqref{eq:R1}}+\text{Eq.~{}\eqref{eq:R1}}+\text{Eq.~ {}\eqref{eq:R1}}\] (B.93) \[=\sum_{i=1}^{m}R_{i,\ell}+\sum_{i=1}^{m-1}O_{P_{\sigma(\mathbf{z}_ {i+1})}}\left(\|\mu_{\ell}^{i+1}-\mu_{0}^{i+1}\|\|\pi_{\ell}^{i}-\pi_{0}^{i}\| \right).\] (B.94)Define \(R_{i}\coloneqq(1/L)\sum_{\ell=1}^{L}R_{i,\ell}\). We note that \(R_{i}\) is a random variable such that \(n_{i}^{1/2}R_{i}\) converges in distribution to a mean-zero normal random variable. Then,

\[\hat{A}-A_{0} =\frac{1}{L}\sum_{\ell=1}^{L}\{\hat{A}_{\ell}-A_{0}\}\] (B.95) \[=\sum_{i=1}^{m}R_{i}+\frac{1}{L}\sum_{\ell=1}^{L}\sum_{i=1}^{m-1}O _{P_{\sigma}(\mathbf{z}_{i+1})}\left(\|\pi_{\ell}^{i}-\pi_{0}^{i}\|\|\mu_{\ell} ^{i+1}-\mu_{0}^{i+1}\|\right).\] (B.96)

### Proof of Theorem 2

We first restate the definition of the MR-gID estimator, the theorem, and its corresponding assumptions.

**Definition 4** (**MR-gID Estimator**).: The MR-gID estimator \(\hat{\psi}\) for the identification expression of the causal effect \(\psi_{0}\coloneqq f(\{A_{0}^{k}\}_{k=1}^{K})\) in Theorem 1 is given as follows: For each \(A_{0}^{k}\) composing \(f(\{A_{0}^{k}\}_{k=1}^{K})\), let \(\hat{A}^{k}\coloneqq\hat{A}^{k}(\{\mu_{k,\ell}^{j+1},\pi_{k,\ell}^{j}\}_{j\in [m^{k}-1],\ell\in[L]})\) denote the DR-gD estimator with nuisance estimates \(\{\mu_{k,\ell}^{j+1},\pi_{k,\ell}^{j}\}\) for the true nuisances \(\{\mu_{k,0}^{j+1},\pi_{k,0}^{j}\}\). Then,

\[\hat{\psi}\coloneqq f(\{\hat{A}^{k}\}_{k=1}^{K}). \tag{11}\]

**Assumption 2** (**Analysis of MR-gID**).: _The identification function \(f(\{A^{k}\}_{k=1}^{K})\) in Thm. 1 and each nuisances \(\{\mu_{k,\ell}^{i+1},\pi_{k,\ell}^{i}\}_{k,\ell}\) for \(\hat{A}^{k}\) satisfy the following properties:_

1. _Twice differentiability__:_ \(f(\{A^{k}\}_{k=1}^{K})\) _is twice continuously Frechet differentiable w.r.t._ \(\{A^{k}\}_{k=1}^{K}\) _w.r.t._ \(\{A^{k}\}_{k=1}^{K}\)_._
2. _Boundedness__:_ \(\forall k\in[K]\) _and_ \(\forall\mathbf{Z}_{i}\in\mathbb{Z}\)_,_ \(\nabla_{A^{k}}f(\{A_{0}^{j}\}_{j=1}^{K})[\hat{A}^{k}-A_{0}^{k}]=O_{P_{\sigma}( \mathbf{z}_{i})}(\hat{A}^{k}-A_{0}^{k})\)_._
3. \(L_{2}\)_-Consistency__:_ \(\|\mu_{k,\ell}^{i+1}-\mu_{k,0}^{i+1}\|_{P_{\sigma}(\mathbf{z}_{k+1}^{k})}=o_{P _{\sigma}(\mathbf{z}_{k+1}^{k})}(1)\)_,_ \(\|\hat{\mu}_{k,\ell}^{i+2}-\hat{\mu}_{k,0}^{i+2}\|_{P_{\sigma}(\mathbf{z}_{k+ 1}^{k})}=o_{P_{\sigma}(\mathbf{z}_{k+1}^{k})}(1)\)_,_ \(\|\pi_{k,\ell}^{i}-\pi_{k,0}^{i}\|_{P_{\sigma}(\mathbf{z}_{k+1}^{k})}=o_{P_{ \sigma}(\mathbf{z}_{k+1}^{k})}(1)\)_, and_ \(\|\hat{\mu}_{k,\ell}^{2}-\hat{\mu}_{k,0}^{2}\|_{P_{\sigma}(\mathbf{z}_{k}^{1} )}=o_{P_{\sigma}(\mathbf{z}_{k}^{1})}(1)\)_._

**Theorem 2** (**Asymptotic Analysis of MR-gID**).: _Suppose Assumption 2 holds. Let \(n_{k,i}\coloneqq|\overline{D}_{\mathbf{z}_{i}^{k}}|\) for \(\mathbf{Z}_{i}^{k}\in\mathbb{Z}\) and \(\mathbf{z}_{i}^{k}\in\mathfrak{D}_{\mathbf{Z}_{i}^{k}}\). Let \(\hat{\psi}\) denote the MR-gID estimator in Def. 4 for the causal effect \(\psi_{0}\coloneqq f(\{A_{0}^{k}\}_{k=1}^{K})\) in Theorem 1. Then, the error of \(\hat{\psi}\) is given as_

\[\hat{\psi}-\psi_{0}=\sum_{k=1}^{K}\sum_{i=1}^{m^{k}}O_{P_{\sigma}(\mathbf{z}_{ i}^{k})}(n_{k,i}^{-1/2})+\frac{1}{L}\sum_{k=1}^{K}\sum_{\ell=1}^{L}\sum_{i=1}^{m^ {k}-1}O_{P_{\sigma}(\mathbf{z}_{i}^{k})}(\|\mu_{k,\ell}^{i+1}-\mu_{k,0}^{i+1} \|\|\pi_{k,\ell}^{i}-\pi_{k,0}^{i}\|). \tag{12}\]

Proof of Theorem 2.: We first define the following notation. For a map \(g(x)\), we will use \(\nabla_{x}g(x_{0})[h]\coloneqq\lim_{t\to 0}(g(x_{0}+th)-g(x_{0}))/t\). We first note that by the definition of Frechet Derivative-based Taylor expansion (Blanchard and Bruning, 2015, Def. 34.1) and the given assumption ('Twice differentiability'), the error \(\hat{\psi}-\psi_{0}\) can be represented as follow:

\[\hat{\psi}-\psi_{0}=\sum_{k=1}^{K}\nabla_{A^{k}}f(\{A_{0}^{j}\}_{j=1}^{K})[\hat {A}^{k}-A_{0}^{k}]+o(\hat{A}^{k}-A_{0}^{k}),\] (B.97)

where, by Prop. 1,

\[\hat{A}^{k}-A_{0}^{k}=\sum_{i=1}^{m^{k}}O_{P_{\sigma}(\mathbf{z}_{i}^{k})}(n_{k,i}^{-1/2})+\frac{1}{L}\sum_{\ell=1}^{L}\sum_{i=1}^{m^{k}-1}O_{P_{\sigma}( \mathbf{z}_{k+1}^{k})}(\|\mu_{k,\ell}^{i+1}-\mu_{k,0}^{i+1}\|\|\pi_{k,\ell}^{i }-\pi_{k,0}^{i}\|).\] (B.98)Therefore, by the big O in probability calculus [Van der Vaart, 2000, Chap. 2],

\[o(\hat{A}^{k}-A_{0}^{k}) =o\left(\sum_{i=1}^{m^{k}}O_{P_{\sigma(\mathbf{z}_{i}^{k})}}(n_{k,i} ^{-1/2})+\frac{1}{L}\sum_{\ell=1}^{L}\sum_{i=1}^{m^{k}-1}O_{\overline{P}_{ \sigma(\mathbf{z}_{i+1}^{k})}}(\|\mu_{k,\ell}^{i+1}-\mu_{k,0}^{i+1}\|\|\pi_{k, \ell}^{i}-\pi_{k,0}^{i}\|).\right)\] (B.99) \[=\sum_{i=1}^{m^{k}}o_{P_{\sigma(\mathbf{z}_{i}^{k})}}(n_{k,i}^{-1/ 2})+\frac{1}{L}\sum_{\ell=1}^{L}\sum_{i=1}^{m^{k}-1}o_{P_{\sigma(\mathbf{z}_{i +1}^{k})}}(\|\mu_{k,\ell}^{i+1}-\mu_{k,0}^{i+1}\|\|\pi_{k,\ell}^{i}-\pi_{k,0}^ {i}\|).\] (B.100)

By applying the given assumption ('Boundedness'), we have the following:

\[\nabla_{A^{k}}f(\{A_{0}^{j}\}_{j=1}^{m})[\hat{A}^{k}-A_{0}^{k}] =\sum_{i=1}^{m^{k}}O_{P_{\sigma(\mathbf{z}_{i}^{k})}}(n_{k,i}^{-1 /2})\] \[+\frac{1}{L}\sum_{\ell=1}^{L}\sum_{i=1}^{m^{k}-1}O_{P_{\sigma( \mathbf{z}_{i+1}^{k})}}(\|\mu_{k,\ell}^{i+1}-\mu_{k,0}^{i+1}\|\|\pi_{k,\ell}^{ i}-\pi_{k,0}^{i}\|).\] (B.101)

Therefore,

\[\hat{\psi}-\psi_{0} =\sum_{k=1}^{K}\nabla_{A^{k}}f(\{A_{0}^{j}\}_{j=1}^{K})[\hat{A}^{ k}-A_{0}^{k}]+o(\hat{A}^{k}-A_{0}^{k})\] (B.102) \[=\sum_{k=1}^{K}\sum_{i=1}^{m^{k}}O_{P_{\sigma(\mathbf{z}_{i}^{k}) }}(n_{k,i}^{-1/2})+\frac{1}{L}\sum_{k=1}^{K}\sum_{\ell=1}^{L}\sum_{i=1}^{m^{k}- 1}O_{P_{\sigma(\mathbf{z}_{i}^{k})}}(\|\mu_{k,\ell}^{i+1}-\mu_{k,0}^{i+1}\|\| \pi_{k,\ell}^{i}-\pi_{k,0}^{i}\|)\] (B.103) \[+\sum_{k=1}^{K}\sum_{i=1}^{m^{k}}o_{P_{\sigma(\mathbf{z}_{i}^{k}) }}(n_{k,i}^{-1/2})+\frac{1}{L}\sum_{k=1}^{K}\sum_{\ell=1}^{L}\sum_{i=1}^{m^{k}- 1}o_{P_{\sigma(\mathbf{z}_{i}^{k})}}(\|\mu_{k,\ell}^{i+1}-\mu_{k,0}^{i+1}\|\| \pi_{k,\ell}^{i}-\pi_{k,0}^{i}\|)\] (B.104) \[=\sum_{k=1}^{K}\sum_{i=1}^{m^{k}}O_{P_{\sigma(\mathbf{z}_{i}^{k}) }}(n_{k,i}^{-1/2})+\frac{1}{L}\sum_{k=1}^{K}\sum_{\ell=1}^{L}\sum_{i=1}^{m^{k}- 1}O_{P_{\sigma(\mathbf{z}_{i}^{k})}}(\|\mu_{k,\ell}^{i+1}-\mu_{k,0}^{i+1}\|\| \pi_{k,\ell}^{i}-\pi_{k,0}^{i}\|).\] (B.105)

### Proof of Corollary 2

**Corollary 2** (**Multiply Robustness** (Corollary of Thm. 2)).: _Suppose (1) Assumption 2 holds; (2) Either \(\pi_{k,\ell}^{i}=\pi_{k,0}^{i}\) or \(\mu_{k,\ell}^{i}=\mu_{k,0}^{j}\) for \(j=i+1,\cdots,m^{k}\) for all \(i,\ell,k\); and (3) all nuisances \(\{\pi_{k,\ell}^{i},\mu_{k,\ell}^{i+1}\}_{i,\ell,k}\) are bounded by some constant. Then, the MR-gID \(\hat{\psi}\) (Def. 4) is consistent to \(\psi_{0}\)._

Proof of Corollary 2.: We first note that \(f\) is a continuous function under the twice differentiability condition in Assumption 2. Suppose each \(\hat{A}^{k}\) is a consistent estimator of \(A_{0}^{k}\) under given conditions. Then, by the continuous mapping theorem, \(f(\{\hat{A}^{k}\}_{k=1}^{K})\) is consistent to \(P_{\mathbf{x}}(\mathbf{y})=f(\{A_{0}^{k}\}_{k=1}^{K})\). Therefore, it suffices to show that each \(\hat{A}^{k}\) is a consistent estimator of \(A_{0}^{k}\).

We first recall that \(\hat{A}^{k}\coloneqq(1/L)\sum_{\ell=1}^{L}\hat{A}^{k}_{\ell}\) by Def. 4. By applying Lemma S.7, \(\hat{A}^{k}_{\ell}-A^{k}_{0}\) can be rewritten as follow:

\[\hat{A}^{k}_{\ell}-A_{0}\] \[=\sum_{i=1}^{m-1}\mathbb{E}_{\overline{D}_{\pi^{k}_{i+1},\ell}-P_{ \sigma(\mathbf{z}^{k}_{i+1})|\pi^{k}_{i+1},\nu_{0}}}\left[\overline{\pi}^{i}_{k,\ell}\{\hat{\mu}^{i+2}_{k,0}-\mu^{i+1}_{k,0}\}\right]+\mathbb{E}_{\overline{D} _{\pi^{k}_{i,\ell}}-P_{\sigma(\mathbf{z}^{k}_{i})|\pi^{k}_{i},\nu_{0}}}\left[ \hat{\mu}^{2}_{k,0}\right]\] (B.106) \[+\sum_{i=1}^{m-1}\mathbb{E}_{\overline{D}_{\pi^{k}_{i+1},\ell}-P_ {\sigma(\mathbf{z}^{k}_{i+1})|\pi^{k}_{i+1},\nu_{0}}}\left[\overline{\pi}^{i}_ {k,\ell}\{\hat{\mu}^{i+2}_{k,\ell}-\mu^{i+1}_{k,\ell}\}-\overline{\pi}^{i}_{k,0}\{\hat{\mu}^{i+2}_{k,0}-\mu^{i+1}_{k,0}\}\right]\] \[+\mathbb{E}_{\overline{D}_{\pi^{k}_{i,\ell}}-P_{\sigma(\mathbf{z} ^{k}_{i})|\pi^{k}_{i},\nu_{0}}}\left[\hat{\mu}^{2}_{k,\ell}-\hat{\mu}^{2}_{k,0 }\right]\] (B.107) \[+\sum_{i=1}^{m-1}\mathbb{E}_{P_{\sigma(\mathbf{z}^{k}_{i+1})}} \left[\overline{\pi}^{i}_{k,\ell}\{\hat{\mu}^{i+2}_{k,\ell}-\mu^{i+1}_{k,\ell} \}-\overline{\pi}^{i}_{k,0}\{\hat{\mu}^{i+2}_{k,0}-\mu^{i+1}_{k,0}\}|\mathbf{r} _{0},\mathbf{z}^{k}_{i+1}\right]\] \[+\mathbb{E}_{P_{\sigma(\mathbf{z}^{k}_{i})}}\left[\hat{\mu}^{2}_{ k,\ell}-\hat{\mu}^{2}_{k,0}|\mathbf{r}_{0},\mathbf{z}^{k}_{i+1}\right]\] (B.108)

We first note that all term in Eq. (B.106) converges in the mean-zero normal distribution and is bounded in probability at \(n^{-1/2}_{i+1,\ell,k}\) rate. Therefore,

\[\text{Eq.~{}\eqref{eq:P_exp}}=\sum_{i=1}^{m^{k}}O_{P_{\sigma( \mathbf{z}^{k}_{i})}}(n^{-1/2}_{i,\ell,k}).\] (B.109)

We now analyze the second term. We note that, by (Kennedy et al., 2020, Lemma 2),

\[\mathbb{E}_{\overline{D}_{\pi^{k}_{i+1},\ell}-P_{\sigma(\mathbf{z }^{k}_{i+1})|\pi^{k}_{i+1},\nu_{0}}}\left[\overline{\pi}^{i}_{k,\ell}\{\hat{ \mu}^{i+2}_{k,\ell}-\mu^{i+1}_{k,\ell}\}-\overline{\pi}^{i}_{k,0}\{\hat{\mu}^ {i+2}_{k,0}-\mu^{i+1}_{k,0}\}\right]\] (B.110) \[=O_{P_{\sigma(\mathbf{z}^{k}_{i+1})}}\left(\frac{\|\overline{\pi} ^{i}_{k,\ell}\{\hat{\mu}^{i+2}_{k,\ell}-\mu^{i+1}_{k,\ell}\}-\overline{\pi}^{i} _{k,0}\{\hat{\mu}^{i+2}_{k,0}-\mu^{i+1}_{k,0}\}\|}{\sqrt{n_{i+1,\ell,k}}}\right).\] (B.111)

We note that \(\|\overline{\pi}^{i}_{k,\ell}\{\hat{\mu}^{i+2}_{k,\ell}-\mu^{i+1}_{k,\ell}\}- \overline{\pi}^{i}_{k,0}\{\hat{\mu}^{i+2}_{k,0}-\mu^{i+1}_{k,0}\}\|\) is bounded by some constant by the given condition. Therefore, it is bounded in probability at \(n^{-1/2}_{i+1,\ell,k}\) rate. By the same analysis, \(\mathbb{E}_{\overline{D}_{\pi^{k}_{i,\ell}}-P_{\sigma(\mathbf{z}^{k}_{1})|\pi^{ k}_{i},\nu_{0}}}\left[\hat{\mu}^{2}_{k,\ell}-\hat{\mu}^{2}_{k,0}\right]\) is bounded in probability at \(1/\sqrt{n_{1,\ell,k}}\)-rates. Therefore,

\[\text{Eq.~{}\eqref{eq:P_exp}}=\sum_{i=1}^{m^{k}}O_{P_{\sigma( \mathbf{z}^{k}_{i})}}(n^{-1/2}_{i,\ell,k}).\] (B.112)

Finally, under the given assumption, the third term can be analyzed by Lemma S.6 and is zero:

\[\text{Eq.~{}\eqref{eq:P_exp}}=\sum_{i=1}^{m^{k}-1}O_{P_{\sigma( \mathbf{z}^{k}_{i+1})}}\left(\|\mu^{i+1}_{k,\ell}-\mu^{i+1}_{k,0}\|\|\pi^{i}_{ k,\ell}-\pi^{i}_{k,0}\|\right)=0.\] (B.113)

Therefore,

\[\hat{A}^{k}_{\ell}-A_{0}=\text{Eq.~{}\eqref{eq:P_exp}}+\text{Eq.~{}\eqref{eq: P_exp}}+\text{Eq.~{}\eqref{eq:P_exp}}=\sum_{i=1}^{m}\sum_{i=1}^{m^{k}}O_{P_{ \sigma(\mathbf{z}^{k}_{i})}}(n^{-1/2}_{i,\ell,k}),\] (B.114)and, finally,

\[\hat{A}-A_{0} =\frac{1}{L}\sum_{\ell=1}^{L}\{\hat{A}_{\ell}^{k}-A_{0}\}\] (B.115) \[=\frac{1}{L}\sum_{\ell=1}^{L}\sum_{i=1}^{m^{k}}O_{P_{\sigma(\mathbf{ Z}_{i}^{k})}}(n_{i,\ell,k}^{-1/2})\] (B.116) \[=\sum_{i=1}^{m^{k}}O_{P_{\sigma(\mathbf{Z}_{i}^{k})}}(n_{i,k}^{-1/ 2})\] (B.117) \[=\sum_{i=1}^{m}o_{P_{\sigma(\mathbf{Z}_{i}^{k})}}(1),\] (B.118)

where the last equation holds since \(O_{P}(n^{-\alpha})=o_{P}(1)\) when \(\alpha>0\). 

## Appendix C Discussion

### Relaxation of Discreteness Assumption

In this paper, we made the strong assumption that all variables are discrete. However, this assumption does not hold in general. In this section, we relax the assumption to a certain degree while ensuring that the proposed estimators and corresponding error analysis remain applicable without sacrificing generality.

First, we define a set of variables, denoted as \(\mathtt{disc}\), which must be discrete in order to apply the proposed estimators and leverage the error analyses presented in the paper.

**Definition 4** (**Discreteness set \(\mathtt{disc}\)**).: For some given inputs \((\mathbf{x},\mathbf{y},\mathbb{Z},\mathbb{P},G)\), suppose

\[f(\{A_{0}^{k}\}_{k=1}^{K})=\mathtt{GID}(\mathbf{x},\mathbf{y}, \mathbb{Z},\mathbb{P},G),\] (C.1)

where each \(A_{0}^{k}\) is specified as

\[A_{0}^{k}\coloneqq A_{0}[\mathbf{W}^{k},\mathbf{C}^{k},\mathbf{R} ^{k};\mathbb{Z}^{k},\mathtt{seq}^{k}](\mathbf{w}^{k}\backslash\mathbf{c}^{k}, \mathbf{r}_{k}),\] (C.2)

for \(\mathbf{W}^{k},\mathbf{R}^{k}\subseteq\mathbf{V}\) and \(\mathbb{Z}^{k}\subseteq\mathbb{Z}\). Then, the discreteness set \(\mathtt{disc}(\{A_{0}^{k}\}_{k=1}^{K})\) is defined as follow:

\[\mathtt{disc}(\{A_{0}^{k}\}_{k=1}^{K})\coloneqq\bigcup_{k=1}^{K} \{(\mathbf{W}^{k}\backslash\mathbf{C}^{k})\cup\mathbf{R}^{k}\cup\mathbb{Z}^{k }\}.\] (C.3)

For Example 1, the discreteness set is given as

\[\mathtt{disc}(\{A_{0}^{1},A_{0}^{2}\})=(Z,X)\cup(Y,Z)=\{Z,X,Y\} =\mathbf{V}\backslash\{W\}.\] (C.4)

For Example 2, the discreteness set is given as

\[\mathtt{disc}(\{A_{0}^{2},A_{0}^{13}\})=(W,R,X_{1})\cup(R,Y,X_{2},W,X_{1})=\{X_{1},X_{2},R,W,Y\}=\mathbf{V}\] (C.5)

Equipped with the discreteness set, we relax the assumption as follows:

**Assumption 1.1** (**Relaxed Regularity)**.: _For variables \(\mathbf{V}\) and the Radon-Nikodym derivative \(p_{\sigma(\mathbf{Z})}\) of \(P_{\sigma(\mathbf{Z})}\) for \(\mathbf{Z}\in\mathbb{Z}\), the following conditions hold:_

1. _All variables in_ \(\mathtt{disc}(\{A_{0}^{k}\})\) _are discrete;_
2. \(p_{\sigma(\mathbf{Z})}(\mathbf{v})>c,\forall\mathbf{v}\in\mathfrak{D}_{ \mathbf{V}}\) _for some_ \(c\in(0,1)\)_._

We note that the proposed estimator is well-defined and corresponding error analyses Theorem 2 and Corollary 2 hold true under the relaxed assumption:

**Lemma S.9** (**Well-defined MR-gID Estimator under Relaxed Regularity**).: _The MR-gID estimator in Definition 4 is pathwise-differentiable under Assumption 1.1 and Assumption 2._

Proof of Lemma s.9.: For \(k=1,\cdots,K\), define

\[\overline{A}^{k}\coloneqq\sum_{i=1}^{m^{k}-1}\mathbb{E}_{P_{\sigma(\mathbf{z}_{ i+1}^{k})}}\left[\mathbf{\pi}_{k}^{i}\{\hat{\mu}_{k}^{i+2}-\mu_{k}^{i+1}\}|\mathbf{z}_{ i+1}^{k},\mathbf{r}_{0}^{k}\right]+\mathbb{E}_{P_{\sigma(\mathbf{z}_{k}^{k})}} \left[\hat{\mu}_{k}^{2}|\mathbf{z}_{1}^{k},\mathbf{r}_{0}^{k}\right].\] (C.6)

To establish the pathwise-differentiability of the MR-gID estimator \(f(\{\hat{A}^{k}\}_{k=1}^{K})\) as defined in Definition 4, it is sufficient to ensure the pathwise-differentiability of individual \(\overline{A}^{k}\). Under Assumption 1.1, \(\mu_{k}^{m^{k}+1}\coloneqq\mathbbm{1}_{\mathbf{w}^{k}}\varsigma^{k}(\mathbf{ W}^{k}\backslash\mathbf{C}^{k})\) is well-defined since \((\mathbf{W}^{k}\backslash\mathbf{C}^{k})\in\texttt{disc}(\{A_{0}^{k}\}_{R=1}^{ K})\) are discrete. Also, each \(\mathbbm{1}_{\pi_{i}^{k}}(\mathbf{R}_{i}^{k})\) in each \(\pi_{i}^{k}\) are well-defined since \(\mathbf{R}_{i}^{k}\in\texttt{disc}(\{A_{0}^{k}\}_{R=1}^{K})\) are directly, the conditional expectation \(\mathbb{E}_{P_{\sigma(\mathbf{z}_{k}^{k})}}[\cdot|\mathbf{z}_{i}^{k},\mathbf{r} _{0}^{k}]\) is well-defined since \(\mathbf{Z}_{i}^{k}\in\texttt{disc}(\{A_{0}^{k}\}_{R=1}^{K})\) are discrete. Also, under the positivity condition stated in Assumption 1.1, \(\overline{A}^{k}\) in Eq. (C.6) is pathwise-differentiable. By combining this with Assumption 2, we conclude that the MR-gID estimator is pathwise-differentiable. 

### Sequential Doubly Robustness: \(2^{m-1}\) robustness versus \(m\)-robustness

In this section, we discuss the practical properties of the proposed doubly robust g-mSBD estimator in Def. 3. We recall that the estimator is doubly robust by the analysis in Lemma S.6 as follows:

**Lemma S.6** (**Bias Analysis of g-mSBD Estimators**).: _Let \(\overline{A}\) be the quantity defined as_

\[\overline{A}\coloneqq\sum_{i=1}^{m-1}\mathbb{E}_{P_{\sigma(\mathbf{z}_{i+1})}} \left[\overline{\pi}^{i}\{\hat{\mu}^{i+2}-\mu^{i+1}\}|\mathbf{z}_{i+1}, \mathbf{r}_{0}\right]+\mathbb{E}_{P_{\sigma(\mathbf{z}_{1})}}\left[\hat{\mu}^ {2}|\mathbf{z}_{1},\mathbf{r}_{0}\right],\] (B.68)

_where \(\pi^{i},\mu^{i}\) are arbitrary nuisances for true nuisances \(\pi_{0}^{i}\) and \(\mu_{0}^{i}\) defined in Def. 2. Let \(A_{0}\) denote the g-mSBD in Def. 1. Then,_

\[\overline{A}-A_{0}=\sum_{r=1}^{m-1}O_{P_{\sigma(\mathbf{z}_{r+1})}}\left(\| \pi^{r}-\pi_{0}^{r}\|\|\mu^{r+1}-\mu_{0}^{r+1}\|\right).\] (B.69)

The term in Eq. (B.69) exhibits doubly robustness, becoming zero when either \(\pi^{r}=\pi_{0}^{r}\) or \(\mu^{r+1}=\mu_{0}^{r+1}\) hold for all \(r=1,2,\dots,m-1\). This phenomenon is referred to as the sequential doubly robustness (Luedtke et al., 2017), or \(2^{m-1}\) robustness in the sense that there are \(2^{m-1}\) ways to make Eq. (B.69) zero (Vansteelandt et al., 2007; Rotnitzky et al., 2017).

While the proposed doubly robust g-mSBD estimator defined in Definition 3 exhibits doubly robustness, as shown in Proposition 1, it does not satisfy \(2^{m-1}\) robustness. This is due to the dependencies between \(\mu_{\ell}^{r}\) and \(\mu_{\ell}^{s}\) for \(s\in\{r+1,\cdots,m\}\). Specifically, if \(\mu_{\ell}^{s}\) is misspecified for some \(s>r\), it renders the case \(\mu_{\ell}^{r}=\mu_{0}^{r}\) impossible. Consequently, instead of having \(2^{m-1}\) possibilities, there are only \(m\) ways to make Eq. (B.69) equal to zero. For each \(r=1,\cdots,m-1\), this requires either \(\pi_{\ell}^{r}=\pi_{0}^{r}\) or \(\mu_{\ell}^{s}=\mu_{0}^{s}\) for \(s>r\). This condition is referred to as \(m\)-robustness. In summary, the doubly robust g-mSBD estimator achieves \(m\)-robustness instead of \(2^{m-1}\) robustness. We acknowledge that an interesting open direction is to explore ways to enhance the doubly robust g-mSBD estimator to attain \(2^{m-1}\) robustness, building upon the findings presented in Luedtke et al. (2017).

## Appendix D Details of Experiments

As described in Sec. 4, we used the XGBoost (Chen and Guestrin, 2016) as a model for estimating nuisances \(\mu,\pi,\{\mu^{i}\}_{i=2}^{m},\{\pi^{i}\}_{i=1}^{m}\). We implemented the model using Python. In modeling nuisance using the XGBoost, we used the commandxgboost.XGBClassifier(eval_metric='logloss')1 to use the XGBoost with the default parameter settings. In estimating the weight, we set the weight \(\pi_{\ell}^{i}=10\) whenever the estimated weight is over \(10\)(Crump et al., 2009). For Example 1, the dimension of \(W\) is set as \(|W|=10\). We chose \(L=2\). All variables are set to be binary. We compute the effect of \(P(Y=1|do(\mathbf{x}))\).

Footnote 1: Detailed parametrization of parameters including learning rates, maximum depth of the trees, etc. are explained in [https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier).

### Designs of Simulations

In this section, we present the structural causal models (SCMs) utilized for generating the dataset. Furthermore, we include a segment of the code employed to generate the dataset.

#### d.1.1 Example 1

We define the following structural causal models for Example 1:

\[U_{W},U_{WZ},U_{WY},U_{XY} \sim\texttt{normal}(0,1),\] \[\mathbf{W} \coloneqq f_{W}(U_{WZ},U_{WY},U_{W}),\] \[X \coloneqq f_{X}(\mathbf{W},U_{XY}),\] \[Z \coloneqq f_{Z}(W,X,U_{WZ}),\] \[Y \coloneqq f_{Y}(Z,U_{WY},U_{XY}),\]

where

\[f_{W}(U_{WZ},U_{WY}) \coloneqq\left\lfloor\frac{1}{1+\exp(U_{W}-U_{WZ}+U_{WY})}\right\rfloor,\] \[f_{X}(\mathbf{W},U_{XY}) \coloneqq\left\lfloor\frac{1}{1+\exp(\mathbf{c}_{X}^{\intercal} \mathbf{W}+U_{XY})}\right\rfloor,\] \[f_{Z}(W,X,U_{WZ}) \coloneqq\left\lfloor\frac{1}{1+\exp(U_{WZ}\cdot\mathbf{c}_{Z}^{ \intercal}\mathbf{W}+4X-2+U_{WZ})}\right\rfloor\] \[f_{Y}(Z,U_{WY},U_{XY}) \coloneqq\left\lfloor\frac{1}{1+\exp(4Z-2+0.5U_{WY}-U_{XY})} \right\rfloor,\]

where the coefficient vector \(\mathbf{c}_{X},\mathbf{c}_{Z}\) are such that their \(i\)th element is \(0\) if \(i\) is an even number and \(1\) otherwise.

#### d.1.2 Example 2

We define the following structural causal models for Example 2:

\[U_{X_{1},X_{2}},U_{X_{1},W},U_{X_{1},R},U_{X_{2},W},U_{X_{2},Y} \sim\texttt{normal}(0,1),\] \[X_{1} \coloneqq f_{X_{1}}(U_{X_{1},X_{2}}),\] \[X_{2} \coloneqq f_{X_{2}}(U_{X_{1},X_{2}}),\] \[R \coloneqq f_{R}(U_{X_{1},R},X_{1},X_{2}),\] \[W \coloneqq f_{W}(U_{X_{1},W},U_{X_{2},W},X_{1},R),\] \[Y \coloneqq f_{Y}(U_{X_{2},Y},X_{2},R,W),\]where

\[f_{X_{1}}(U_{X_{1},X_{2}}) \coloneqq\left\lfloor\frac{1}{1+\exp(2U_{X_{1},X_{2}}-1)}\right\rfloor,\] \[f_{X_{2}}(U_{X_{1},X_{2}}) \coloneqq\left\lfloor\frac{1}{1+\exp(3U_{X_{1},X_{2}}+1)}\right\rfloor,\] \[f_{R}(U_{X_{1},R},X_{1},X_{2}) \coloneqq\left\lfloor\frac{1}{1+\exp(U_{X_{1},R}(2X_{2}-1)+2X_{1} +3X_{2}+U_{X_{1},R}-4)}\right\rfloor,\] \[f_{W}(U_{X_{1},W},U_{X_{2},W},X_{1},R) \coloneqq\left\lfloor\frac{1}{1+\exp(U_{X_{1W}}(2X_{1}-1)+(4R-2)+ U_{X_{2},W}}\right\rfloor,\] \[f_{Y}(U_{X_{2},Y},X_{2},R,W) \coloneqq\left\lfloor\frac{1}{1+\exp(0.5U_{X_{2},Y}(2R-1)-2X_{2}+ 2W+U_{X_{2},Y}-2)}\right\rfloor.\]

## Appendix E Project STAR: Estimating Joint Effects of Class Sizes to Academic Outcomes

We applied the proposed estimators to the Project STAR dataset [15, 16]. Project STAR is an experimental study investigating teacher/student ratios' impact on academic achievement for kindergarten through third-grade students. In the study, students were randomly assigned to three different class sizes: small-size classes, regular classes, and large-size classes. The objective was to evaluate how class size affects academic outcomes [16]. In our analysis, we used the dataset introduced in the online complement of Stock et al. [16].

Project STAR Dataset.We denote the Project STAR dataset as \(D\). The dataset \(D\) includes the following information: class size for kindergarten (\(X_{1}\)), the academic outcome in kindergarten (\(W\)), the academic outcome in second grade (\(R\)), class size for third grade (\(X_{2}\)), the academic outcome in the third grade (\(Y\)), free lunch receiving for kindergarten (\(C_{1}\)), gender (\(C_{2}\)), ethnicity (\(C_{3}\)) and free lunch receiving for the third grade (\(C_{4}\)). We will use \(\mathbf{C}_{1}\coloneqq(C_{1},C_{2})\) and \(\mathbf{C}_{2}\coloneqq(C_{3},C_{4})\).

Assumption on Dataset.We assume that the SCM \(\mathcal{M}\) generating the variables \((X_{1},W,R,X_{2},Y,\mathbf{C}_{1},\mathbf{C}_{2})\) induces a causal graph depicted in Figure 3(a). Here, we mark \(\mathbf{C}_{1},\mathbf{C}_{2}\) as gray to denote that these variables will be considered latent; i.e., these variables will not be used in the data analysis.

Project STAR dataset \(D\) is a longitudinal experimental study randomizing \(X_{1}\) and \(X_{2}\); i.e., the dataset is induced by the submodel \(M_{x_{1},x_{2}}\) for \(x_{1},x_{2}\in\mathfrak{D}_{X_{1},X_{2}}\), represented in Fig. 3(b). The samples for variables \(\{X_{1},W\}\) follow a distribution \(P_{\sigma(X_{1})}(X_{1},W,R)=P_{\sigma(X_{1},X_{2})}(X_{1},W,R)\), and the samples for variables \(\{X_{1},W,R,X_{2},Y\}\) follow a distribution \(P_{\sigma(X_{1},X_{2})}(X_{1},W,R,X_{2},Y)\). To demonstrate, we will describe how to generate the sample fol

Figure 4: Example causal graphs for Section E. Nodes representing the treatment and outcome are marked in blue and red, respectively.

lowing a distribution \(P_{\sigma(X_{2})}(X_{1},W,R,X_{2},Y)\) by creating unmeasured confounding bias between \(X_{1}\) and \(W\), and \(X_{1}\) and \(Y\), which is depicted by Fig. E.4c.

Creation of Datasets from Marginal Experiments.In this empirical study, we create two datasets from this dataset: \(D_{1}\) and \(D_{2}\). The dataset \(D_{1}\) is a random subsample of \(D\) only including \(\{X_{1},W\}\). Then, \(D_{1}\) follows \(P_{\sigma(X_{1})}(X_{1},W,R)\).

To construct the dataset \(D_{2}\) following the marginal experimental distribution \(P_{\sigma(X_{2})}(X_{1},W,R,X_{2},Y)\), the confounding bias between \(X_{1}\) and \(W\) and \(X_{1}\) and \(Y\) should be introduced. To do so, we follow a standard procedure for introducing confounding bias from experimental studies used in Hill (2011); Louizos et al. (2017); Zhang and Bareinboim (2019); Gentzel et al. (2021).

A setting for the standard procedure is the following. For any arbitrary random variable \(X,Y,Z,W\) such that \(X\to Y\), \(Z\to Y\), and there are no arrows into \(X\), the dataset \(D\coloneqq\{(X_{(i)},Y_{(i)},Z_{(i),W_{(i)}}:i=1,\cdots,n\}\) is given. In \(D\), \(Z\) is not a confounding variable since \(Z\perp\!\!\!\perp X\). The goal is to generate a new dataset \(D^{\prime}\coloneqq\{(X_{(j)},Y_{(j)},Z_{(j)}:j=1,\cdots,n^{\prime}\}\) where \(Z\) serves as a confounding variable between \(X\) and \(Y\). The procedure named \(\texttt{IntroduceConfounding}(X,Y;Z,D)\) is given as follows: Initialize \(D^{\prime}=\{\}\). For \(i=1,\cdots,n\), do the followings:

1. Generate the Bernoulli Random \(B_{(i)}\) with parameter \(P(X_{(i)}|Z_{(i)})\).
2. If \(B_{(i)}=1\), include \((X_{(i)},Y_{(i)},Z_{(i)},W_{(i)})\) in \(D^{\prime}\).

Finally, we exclude \(Z\) is removed from \(D^{\prime}\). By doing so, we introduce unmeasured confounding bias between \(X\) and \(Y\) in \(D^{\prime}\); i.e., \(D^{\prime}=\texttt{IntroduceConfounding}(X,Y;Z,D)\).

To generate the dataset \(D_{2}\sim P_{\sigma(X_{2})}(X_{1},W,R,X_{2},Y)\) from \(D\sim P_{\sigma(X_{1},X_{2})}(X_{1},W,R,X_{2},Y,\mathbf{C}_{1},\mathbf{C}_{2})\), we have to introduce the unmeasured confounding bias between \(X_{1}\) and \(W\), and \(X_{1}\) and \(Y\). We do this by \(D^{\prime}_{2}=\texttt{IntroduceConfounding}(X_{1},W;\mathbf{C}_{1},D)\). Then, \(D^{\prime}_{2}\) is a variable containing \((X_{1},W,R,X_{2},\mathbf{C}_{2},Y)\) and there is a confounding bias between \(X_{1}\) and \(W\). Then, we set \(D_{2}=\texttt{IntroduceConfounding}(X_{1},Y;\mathbf{C}_{2},D^{\prime}_{2})\). Then, \(D_{2}\) is a variable containing \((X_{1},W,R,X_{2},Y)\) and there is a confounding bias between \(X_{1}\) and \(Y\), and \(X_{1}\) and \(W\). A causal graph Fig. E.4c depicted the dependencies in \(D_{2}\).

Goal.In this empirical study, we aim to study the joint effect of the class size for kindergarten (\(X_{1}\)) and the third grade (\(X_{2}\)) on the third grade's academic outcome (\(Y\)); i.e., \(\mathbb{E}\left[Y|do(x_{1},x_{2})\right]\). Since \(D\) is a longitudinal experimental dataset following \(P_{\texttt{and}(X_{1},X_{2})}(C,X_{1},W,X_{2},Y)\), the ground-truth \(\mathbb{E}\left[Y|do(x_{1},x_{2})\right]\) is estimated as \(\mathbb{E}_{D}\left[Y|\texttt{1}_{x_{1},x_{2}}(X_{1},X_{2})\right]/\mathbb{E} _{D}\left[\texttt{1}_{x_{1},x_{2}}(X_{1},X_{2})\right]\).

Causal Effect Identification.We identify \(P(y|do(x_{1},x_{2}))\) through Algo. 1.

1. **Line 3**: Set \(\mathbf{D}\coloneqq an(Y)_{G(\mathbf{V}\setminus\{X_{1},X_{2}\})}=\{Y,R,W\}\).
2. **Line 4**: Set \(\mathbf{D}_{1}\coloneqq\{R\},\mathbf{D}_{2}\coloneqq\{W\},\mathbf{D}_{3} \coloneqq\{Y\}\).

Each \(Q[\mathbf{D}_{1}]=Q[W],Q[\mathbf{D}_{2}]=Q[R],Q[\mathbf{D}_{3}]=Q[Y]\) are identified as follows: For \(Q[\mathbf{D}_{1}]\),

1. **Line 7**: For \(\mathbf{D}_{1},\mathbf{Z}_{1}\coloneqq\{X_{1}\}\), \(\mathbf{z}_{1}\coloneqq\{x_{1}\}\), \(\mathbf{S}_{1}^{1}\coloneqq\{W\}\).
2. **Line 8**: Set \(Q[\mathbf{S}_{1}^{1}]=A_{0}[\mathbf{S}_{1}^{1},\emptyset,\mathbf{R}_{1}^{1}; \mathbb{Z}_{1}^{1}=\{X_{1}\},\mathsf{seq}_{1}^{1}=(x_{1})](w,x_{1})\) where \(\mathbf{R}_{1}^{1}\coloneqq\emptyset\).
3. **Line a.4**: Since \(\mathbf{S}_{1}^{1}=\mathbf{D}_{1}\), we set \[Q[\mathbf{D}_{1}]=Q[\mathbf{S}_{1}^{1}]\] \[=A_{0}^{W}\] \[\coloneqq A_{0}[W,\emptyset,\emptyset;\mathbb{Z}_{1}^{1}=\{X_{1} \},\mathsf{seq}_{1}^{1}=(x_{1})](w,\emptyset)\] \[=P_{\sigma(X_{1})}(w|x_{1})=P_{x_{1}}(w).\]

For \(Q[\mathbf{D}_{2}]=Q[R]\),1. **Line 7**: For \(\mathbf{D}_{2}\), \(\mathbf{Z}_{2}\coloneqq\{X_{1}\}\), \(\mathbf{z}_{2}\coloneqq\{x_{1}\}\), \(\mathbf{S}^{1}_{2}\coloneqq\{R\}\).
2. **Line 8**: Set \(Q[\mathbf{S}^{1}_{2}]=A_{0}[\mathbf{S}^{1}_{2},\emptyset,\mathbf{R}^{1}_{2}; \mathbb{Z}^{1}_{2}=\{X_{1}\},\mathsf{seq}^{1}_{2}=(x_{1})](r,x_{1})\) where \(\mathbf{R}^{1}_{1}\coloneqq\{W\}\).
3. **Line a.4**: Since \(\mathbf{S}^{1}_{2}=\mathbf{D}_{2}\), we set \[Q[\mathbf{D}_{2}]=Q[\mathbf{S}^{1}_{2}]\] \[=A^{R}_{0}\] \[\coloneqq A_{0}[\mathbf{S}^{1}_{2},\emptyset,\{W\};\mathbb{Z}^{1} _{2}=\{X_{1}\},\mathsf{seq}^{1}_{2}=(x_{1})](w,r)\] \[=P_{\sigma(X_{1})}(r|r,x_{1})=P_{x_{1}}(r|w).\]

For \(Q[\mathbf{D}_{3}]=Q[Y]\),

1. **Line 7**: For \(\mathbf{D}_{3}\), \(\mathbf{Z}_{3}\coloneqq\{X_{2}\}\), \(\mathbf{z}_{3}\coloneqq\{x_{2}\}\), \(\mathbf{S}^{2}_{1}\coloneqq\{Y,X_{1},W\}\).
2. **Line 8**: Set \(Q[\mathbf{S}^{2}_{1}]=A_{0}[\mathbf{S}^{2}_{1},\emptyset,\mathbf{R}^{1}_{1}; \mathbb{Z}^{2}_{1}=\{X_{2}\},\mathsf{seq}^{2}_{1}=(x_{2})](y,r)\) where \(\mathbf{R}^{2}_{1}\coloneqq\{W\}\); i.e., \[Q[\mathbf{S}^{2}_{1}] =A_{0}[\{X_{1},W,Y\},\emptyset,\{R\};\mathbb{Z}^{2}_{1}=\{X_{2}\},\mathsf{seq}^{2}_{1}=(x_{2})]((x_{1},w,y),r)\] \[=P_{\sigma(X_{2})}(y|x_{1},w,r,x_{2})P_{\sigma(X_{2})}(x_{1},w|x_ {2}).\]
3. **Line a.2**: Set \(\mathbf{A}\coloneqq an(\mathbf{D}_{3})_{G(\mathbf{S}^{2}_{1})}=\{Y\}\).
4. **Line a.3**: \(Q[\mathbf{A}]=A_{0}[\{Y\},\{X_{1},W\},\{R\};\mathbb{Z}^{2}_{1}=\{X_{2}\}, \mathsf{seq}^{2}_{1}=(x_{2})](y,r)\).
5. **Line a.4**: Since \(\mathbf{A}=\mathbf{D}_{2}\), we set \[Q[\mathbf{D}_{2}]=Q[\mathbf{A}]\] \[=A^{Y}_{0}\] \[\coloneqq A_{0}[\{Y\},\{X_{1},W\},\{R\};\mathbb{Z}^{2}_{1}=\{X_{ 2}\},\mathsf{seq}^{2}_{1}=(x_{2})](y,r)\] (E.1) \[=\sum_{x^{\prime}_{1},w\in\mathfrak{D}_{X_{1},W}}P_{\sigma(X_{2}) }(y|x^{\prime}_{1},w,r,x_{2})P_{\sigma(X_{2})}(x^{\prime}_{1},w|x_{2}).\]

Then, by **Line 14**,

\[P(y|do(x_{1},x_{2})) =\sum_{r,w\in\mathfrak{D}_{R,W}}Q[W]Q[R]Q[Y]\] \[=\sum_{r,w\in\mathfrak{D}_{R,W}}A^{W}_{0}A^{R}_{0}A^{Y}_{0}.\]

By Lemma 3,

\[A^{WR}_{0} \coloneqq A^{W}_{0}A^{R}_{0}\] \[=A_{0}[\{R,W\},\emptyset,\emptyset;\mathbb{Z}^{1}_{2}=\{X_{1}\}, \mathsf{seq}^{1}_{2}=(x_{1},x_{1})]((w,r),\emptyset)\] \[=P_{\sigma(X_{1})}(w,r|x_{1})=P_{x_{1}}(w,r).\]

Therefore,

\[P(y|do(x_{1},x_{2})) =\sum_{r,w\in\mathfrak{D}_{R,W}}A^{W}_{0}A^{R}_{0}A^{Y}_{0}\] \[=\sum_{r,w\in\mathfrak{D}_{R,W}}A^{WR}_{0}A^{Y}_{0}\] \[=\sum_{r\in\mathfrak{D}_{R}}(\sum_{w\in\mathfrak{D}_{W}}A^{WR}_{0 })A^{Y}_{0},\]

where the last equation holds since \(A^{Y}_{0}\) is not a function of \(W\). By Lemma 2

\[A^{R^{\prime}}_{0}\coloneqq\sum_{w\in\mathfrak{D}_{W}}A^{WR}_{0}=A_{0}[R, \emptyset,\emptyset;\{X_{1}\},(x_{1})](r,\emptyset)=P_{\sigma(X_{1})}(r|x_{1} )=P_{x_{1}}(r).\]Therefore,

\[P(y|do(x_{1},x_{2})) =\sum_{r\in\mathfrak{D}_{R}}(\sum_{w\in\mathfrak{D}_{W}}A_{0}^{WR})A_ {0}^{Y}\] \[=\sum_{r\in\mathfrak{D}_{R}}A_{0}^{R^{\prime}}A_{0}^{Y}.\]

Causal Effect Estimation.Here, we only describe the nuisance for \(A_{0}^{Y}\) in Eq. (E.1), since estimating \(A_{0}^{R^{\prime}}=P_{x_{1}}(r)\) is trivial. We define the nuisance as follows: For the fixed \(x_{2}\in\mathfrak{D}_{X_{2}}\),

\[\mu_{0}(X_{1},W,R) \coloneqq\mathbb{E}_{P_{x_{2}}}\left[Y|X_{1},W_{1},C\right],\] (E.2) \[\pi_{0}(R|X_{1},W) \coloneqq\frac{\mathbb{1}_{r}(R)}{P_{x_{2}}(R|X_{1},W)}.\] (E.3)

Then, \(A_{0}^{Y}\) in Eq. (E.1) can be expressed as follows:

\[\text{Eq. \eqref{eq:P_x_1}}\] (E.4) \[=\mathbb{E}_{P_{x_{2}}}\left[Y\frac{\mathbb{1}_{r}(R)}{\pi_{0}(R| X_{1},W)}\right],\;\text{or}\,,\] (E.5) \[=\mathbb{E}_{P_{x_{2}}}\left[\mu_{0}(X_{1},W,r)\right],\;\text{or}\,,\] (E.6) \[=\mathbb{E}_{P_{x_{2}}}\left[\frac{\mathbb{1}_{r}(R)}{\pi_{0}(R|X _{1},W)}\{Y-\mu_{0}(X_{1},W,R)\}+\mu_{0}(X_{1},W,r)\right].\] (E.7)

We then construct the regression-based, probability weighting-based, and MR-gID (MR) \(T^{\text{reg}},T^{\text{pw}},T^{\text{mr}}\) using the following procedure.

1. For each fixed \(x_{2}\in\mathfrak{D}_{X_{2}}\) and a sample set \(D_{x_{2}}\) for \(i\in\{1,2\}\), randomly split the sample as \(D_{x_{2},t}\) and \(D_{x_{2},e}\).
2. Use \(D_{x_{2},t}\) to train the model for learning nuisances in Eq. (E.2) and Eq. (E.3). Let \(\mu(X_{1},W,R)\) and \(\pi(R|X_{1},W)\) denote the learnt models. We use the XGBoost [Chen and Guestrin, 2016] to learn the model.
3. Then, each estimator is defined as follows: \[T^{\text{reg}} \coloneqq\mathbb{E}_{D_{x_{2},e}}\left[\mu(X_{1},W,r)\right]\] (E.8) \[T^{\text{pw}} \coloneqq\mathbb{E}_{D_{x_{2},e}}\left[\frac{\mathbb{1}_{r}(R)}{ \pi(R|X_{1},W)}Y\right]\] (E.9) \[T^{\text{mr}} \coloneqq\mathbb{E}_{D_{x_{2},e}}\left[\frac{\mathbb{1}_{r}(R)}{ \pi(R|X_{1},W)}\{Y-\mu(X_{1},W,R)\}\right]+\mathbb{E}_{D_{x_{2},e}}\left[\mu(X _{1},W,r)\right].\] (E.10)

Experimental ResultsAs described in the Experimental Setup section (Sec. 4), we evaluated the AAE\({}^{\text{est}}\) of estimators \(T^{\text{est}}\) for set \(\in\{\text{reg},\text{pw},\text{mr}\}\) in Scenarios {1, 2, 3, 4}. The AAE plots for all

Figure E.5: AAE Plot for Project STAR dataset analysis for Scenarios {1,2,3,4}.

cases can be seen in Fig. E.5. In this particular scenario, the sample size was not varied since the sample itself was externally given.

In Case 2, we introduced variation by adjusting the size of the converging noise \(\epsilon\), which follows a normal distribution \(\text{Normal}(n^{-\alpha},n^{-2\alpha})\) for \(n\in\{200,400,600,800,1000\}\). It was observed that the MR-gID estimator \(T^{\text{mr}}\) outperformed the other two estimators by achieving fast convergence, as demonstrated in Theorem 2. For Scenarios [3, 4], the DML estimator \(T^{\text{mr}}\) exhibited doubly robust properties, as illustrated in Corollary 2.