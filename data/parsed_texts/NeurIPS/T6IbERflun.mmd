# Correlating Variational Autoencoders Natively For Multi-View Imputation

 Ella S. C. Orme

Department of Mathematics

Imperial College London

ella.orme18@imperial.ac.uk

&Marina Evangelou

Department of Mathematics

Imperial College London

m.evangelou@imperial.ac.uk

&Ulrich Paquet

African Institute for Mathematical Sciences, South Africa

ulrich@aims.ac.za

###### Abstract

Multi-view data from the same source often exhibit correlation. This is mirrored in correlation between the latent spaces of separate variational autoencoders (VAEs) trained on each data-view. A multi-view VAE approach is proposed that incorporates a joint prior with a non-zero correlation structure between the latent spaces of the VAEs. By enforcing such correlation structure, more strongly correlated latent spaces are uncovered. Using conditional distributions to move between these latent spaces, missing views can be imputed and used for downstream analysis. Learning this correlation structure involves maintaining validity of the prior distribution, as well as a successful parameterization that allows end-to-end learning.

## 1 Introduction

Data from multiple sources describing the same subjects arises in a wealth of settings. This can be clinical information of patients alongside genetic information and scan data. Datasets consisting of multiple views are referred to as multi-view or multi-modal data. There are instances where not all views are always available for every realisation. For example, a patient may miss an appointment or machinery may falter, resulting in no reading for a specific view. Missing data results in smaller usable datasets and reduced statistical power with many methods only applicable to full datasets [1] and can result in reduced performance in downstream analysis [2]. This manuscript presents a multi-view imputation approach, where its aim is to impute the realisations of a missing view by incorporating the information learnt from the other view.

The proposed multi-view imputation method, named Joint Prior Variational Autoencoder (JPVAE), is based on variational autoencoders (VAEs) [3], with the views connected solely through a joint prior on the VAEs' latent embeddings. Standard autoencoders seek to encode a latent representation of data, and from this encoding reconstruct the original data via a decoder. Multi-view approaches allow the latent representation of a missing view to be obtained, from which the reconstruction can be used as an imputation of the missing view. Several multi-view imputation approaches exist in the literature based on autoencoders [4]. However, as variational autoencoders learn a continuous embedding, they provide better interpolation of the latent space than standard autoencoders, making them a more suitable approach for an imputation method. The proposed joint prior in JPVAE incorporates a non-zero correlation structure that is found to increase the observed correlation between views in the latent spaces. This allows for successful movement between latent spaces, improving the ability to impute missing views.

Various approaches have been proposed extending VAEs to the multi view case, including those by [5, 6, 7, 8], with missing data imputation included as a feature of these methods. Most recently, proposed methods differ via their method of approximating a joint posterior. Daunhawer et al. [9] discuss the undesirable upper bound these methods put on a lower limit of the log-likelihood used as the objective function in VAEs. In contrast to existing approaches, which largely assume some joint posterior and/or shared latent space, our work has the novelty that is based on a joint prior between the latent variables. To the best of our knowledge, this is the first attempt made to correlate the latent spaces of VAEs natively through a joint prior.

JPVAE is most similar in structure to the private version of Deep Variational Canonical Correlation Analysis (VCCA-private), a multi-view VAE approach by Wang et al. [10]. However their model contains both private and shared variables in the latent space and doesn't have a suitable approach for imputing missing views. JPVAE can also be seen as an application of the ideas from self-supervised learning techniques such as Barlow Twins [11], where the embeddings from noisy versions of the same input are driven to be highly correlated via a loss function.

Figure 1 illustrates an application of the proposed approach JPVAE, on imputing view 2 of the data that corresponds to the bottom half of MNIST digits using the top half of the digits data (view 1). Through the proposed approach, reconstructions of missing views can be obtained. This is achieved through the conditional distribution between the two latent variables, as illustrated in Figure 2. By incorporating a joint prior with a non-zero cross-correlation structure, we observe better quality results in imputing view 2. Further realisations of image imputation, including imputation of view 1 from view 2, can be found in Appendix A.5.

### Contributions

A novel multi-view imputation approach based on variational autoencoders, named JPVAE, is proposed that in contrast to existing work assumes a joint prior between the latent variables.1 As the multiple views are correlated, the generated latent spaces are found also to be correlated. JPVAE

Figure 1: Imputation of the bottom half of MNIST digits (view 2 of the data) using the top half of the image (view 1) on a JPVAE model trained with (a) independent priors (completely separate VAEs) and (b) a joint prior with learnt correlation structure between latent spaces. The cross entropy loss between true bottom half of image and imputation is 109.1 in (a) and 93.04 in (b). A classifier trained on the concatenation of the reconstruction of view 1 and imputation of view 2 achieves average testing accuracy of 79.92% (1.0) in (a) and 87.45% (0.27) in (b). See Appendix A.5 for further details.

takes advantage of this correlation and whilst marginals for each VAE are assumed to follow a standard normal distribution, a joint prior is assumed with a non-zero cross correlation. The imposed correlation structure is learnt natively by forcing a stronger correlation in the latent space. Section 2 presents the proposed method and related theorems that showcase the validity of the proposed method. As part of our work, we present and prove theorems that enable the parameterization of differentiable positive definite matrices that allow end-to-end learning. Through our conducted experiments presented in Section 3, learning the correlation structure leads to improved imputation ability, lower reconstruction loss and better predictive likelihood. Imputed views from JPVAE can be used for downstream tasks, with improved classification accuracy demonstrated. Lastly, better VAE models are learnt with JPVAE preventing posterior collapse, a common problem observed with VAEs [12].

### Notation

Throughout this paper matrices are denoted by capital letters and column vectors are denoted by lower case letters, both emboldened. \(\bm{X}=\{\mathbf{x}_{i}\}_{i=1}^{n}\) with vector \(\mathbf{x}_{i}\in\mathbb{R}^{c}\) represents an entire dataset in \(\mathbb{R}^{n\times c}\), with \(\mathbf{x}_{i}\) an individual realisation. A diagonal matrix with vector \(\bm{a}\) along the diagonal is represented by \(\mathrm{diag}(\bm{a})\). A block diagonal matrix with matrices \(\bm{A}_{i}\) along the diagonal is represented by \(\mathrm{bdiag}(\bm{A}_{i})\). Vertical concatenation of column vectors \(\bm{a}\) and \(\bm{b}\) is denoted by \((\bm{a};\bm{b})\) i.e. \((\bm{a};\bm{b})=(\bm{a}^{T};\bm{b}^{T})^{T}\). The eigenvalues and singular values of matrix \(\bm{A}\) are denoted by \(\lambda_{i}(\bm{A})\) and \(\sigma_{i}(\bm{A})\) respectively, indexed by subscript \(i\) in order of decreasing magnitude. \(\mathbf{I}\) denotes an identity matrix of relevant dimension.

## 2 Multi-view variational autoencoder with a joint prior

This section presents the novel multi-view VAE approach JPVAE where each view has a separate associated VAE. Before discussing the proposed approach in detail, the main concepts of VAEs are introduced. Subsequently the single-view VAE is extended to the multi-view setting and the joint prior is presented. The different components of JPVAE are presented including theoretical work on matrix parameterization which enables end-to-end learning.

### Variational autoencoder

A standard VAE seeks to encode data \(\bm{X}\) in a probabilistic latent space and decode from this space to reconstructed data \(\tilde{\bm{X}}\)[3]. The goal is for \(\tilde{\bm{X}}\) to be as close as possible to \(\bm{X}\). As the latent space is a probabilistic embedding rather than one obtained by a deterministic mapping, it has the advantage that

Figure 2: Workflow to obtain reconstruction \(\tilde{\bm{x}}_{1}\) and imputation \(\tilde{\bm{x}}_{2|1}\) from input \(\bm{x}_{1}\) only, on pre-trained JPVAE network. Trapeziums represent encoders/decoders and rectangles represent vectors. The structures for view 1 and 2 are shown in blue and orange respectively. The expectation step is shown in red.

it can be explored fully, including the sampling of new realisations from the same distribution as \(\hat{\bm{X}}\). These realisations are assumed to be taken from the same underlying distribution as \(\bm{X}\), \(p_{\theta}(\cdot)=p(\cdot|\theta)\) where \(\theta\) is the set of parameters defining the distribution.

VAEs are a type of variational Bayesian method that seek to find a lower bound for this marginal probability, \(p_{\theta}(\mathbf{X})\), through a Bayesian framework. The likelihood of \(\bm{x}\in\mathbb{R}^{c}\) given latent variable \(\bm{z}\in\mathbb{R}^{d}\) is denoted by \(p_{\theta}(\cdot|\bm{z})\) and the prior for the latent variables, usually taken to be a standard normal, is denoted by \(p_{\theta}(\bm{z})\). The encoder and decoder are neural networks that seek to learn the posterior distribution of \(\bm{z}\) given \(\bm{x}\), \(p_{\theta}(\cdot|\bm{x})\), and the likelihood of \(\bm{x}\) given \(\bm{z}\) respectively, given the assumed prior over the latent space. In order to make the posterior learnable, an _approximate_ posterior distribution is used that usually is a multivariate normal distribution. The approximation \(p_{\theta}(\cdot|\bm{x})\approx q_{\phi}(\cdot|\bm{x})\) is made where \(\phi\) denotes the parameters of the probabilistic encoder, \(q_{\phi}(\cdot|\bm{x})\). The encoder maps an input \(\bm{x}\) to a mean vector \(\bm{\mu}(\mathbf{x})\in\mathbb{R}^{d}\) and to the log of the vector of variances, \(\bm{\sigma}^{2}(\mathbf{x})\in\mathbb{R}^{d}\). A sample is drawn from the approximate posterior distribution and this is used as an input to the neural network which acts as the decoder, \(D_{\theta}\). As \(\phi\) appears within the distribution of the latent variables, derivatives cannot simply be taken inside the expectation term. Differentiability of the loss function is required for the loss-driven parameter update steps and therefore a reparamaterization trick needs to be implemented [3].

Instead of drawing \(\bm{z}\) directly, \(\bm{\epsilon}\sim N(\bm{0},\bm{I})\) is drawn and \(\bm{z}=\bm{\mu}(\mathbf{x})+\mathrm{diag}(\bm{\sigma}^{2}(\mathbf{x}))\bm{ \epsilon}\) is determined. The distribution over which we take expectation is now independent of the parameters for which we take derivatives, and derivative update steps may now be implemented. The neural network decoder then seeks to reconstruct \(\bm{x}\) from the latent variable \(\bm{z}\).

A maximum likelihood principle is then applied to the marginal likelihood of the data \(p_{\theta}(\bm{X})\) to obtain estimates for the parameters within the encoders and decoders. As the likelihood is intractable a lower bound on the log-likelihood, known as the Evidence Lower Bound (ELBO) is instead maximised, given by:

\[\mathcal{L}(\theta,\phi)=\mathbb{E}_{\bm{z}\sim q_{\phi}(\cdot|\bm{x})}\left[ \ln(p_{\theta}(\bm{x}|\bm{z}))\right]-D_{KL}(q_{\phi}(\cdot|\bm{x})||p_{\theta }(\cdot))\] (1)

where \(D_{KL}(r|s)\) denotes the Kullback-Leibler (KL) divergence between distributions \(r\) and \(s\)[13]. \(\mathbb{E}_{z\sim q_{\phi}(\cdot|\bm{x})}(\cdot)\) denotes the expectation with respect to the conditional distribution of \(\bm{z}\) given \(\bm{x}\). Maximising the ELBO is equivalent to finding a balance between an approximate posterior that is close to the prior and putting weight on the latent variable space that maximises the likelihood of the data given these same variables, \(\ln(p_{\theta}(\bm{x}|\bm{z}))\). Without the KL term, the delta function is returned as the approximate posterior and the autoencoder is recovered.

With this VAE formulation the KL term often becomes very small or 'vanishes' leading to the posterior equalling the prior (posterior collapse). This is referred to as KL vanishing and leads to a decoder that is largely independent of the latent variables. See [14] for further discussion of this problem. To combat this, we implement KL annealing - a procedure where a weight \(\beta\) is introduced on the KL term and gradually increased, typically from 0, as learning occurs [15]. The objective function becomes:

\[\mathcal{L}_{\beta}(\theta,\phi)=\mathbb{E}_{\bm{z}\sim q_{\phi}(\cdot|\bm{x})} \left[\ln(p_{\theta}(\bm{x}|\bm{z}))\right]-\beta D_{KL}(q_{\phi}(\cdot|\bm{x} )||p_{\theta}(\cdot)).\] (2)

It is now assumed two data views with shared row dimension are present and represented by \(\bm{X}=\{\bm{X}_{1},\bm{X}_{2}\}\) with \(\bm{X}_{i}\in\mathbb{R}^{N\times c_{i}}\). For brevity, the sample subscript is dropped and \(\bm{x}_{i}\in\mathbb{R}^{c_{i}}\) refers to an individual realisation from view \(i\). Here \(N\) is the number of rows in both views and \(c_{i}\) is the number of columns in view \(i=1,2\). This shared row dimension implies the views are paired, with row \(j\) representing the same individual/sample in both views. This allows for meaningful correlation in the latent space. The notation introduced in this section for single view VAEs is used in the following sections where multi-view VAEs are presented. Subscripts are used to denote the relevant views e.g. the encoder and decoder in view \(i\) are represented by \(E_{i\phi_{i}}\) and \(D_{i\theta_{i}}\) respectively.

### Joint prior variational autoencoder

As the different views in a multi-view dataset are from a common source, correlation exists between the views. This translates to correlation between the latent spaces of each view from independently trained VAEs. JPVAE takes advantage of this correlation, enforcing the relationship between the two views via a joint prior on the latent variables, as illustrated in Figure 3. Enforcing the proposed correlation structure between the latent spaces ensures we can move from the original space where data are highly correlated in a non-linear fashion, to a space where the correlation is linear, and back to the reconstructed space that has a non-linear correlation. The marginal prior on the latent variables corresponding to each view is a standard normal, as with the traditional VAE. However, a cross-covariance matrix \(\bm{C}\) is assumed between the latent variables \(\bm{z}_{1}\) and \(\bm{z}_{2}\). Having separate VAE structures for each view assumes conditional independence in both directions: (a) given the data the latent variables are independent, and (b) given the latent variables the data are independent. This allows the unique features of each view to be encoded and decoded.

As the latent spaces are linearly correlated, it is possible to move between them via the conditional distribution, as illustrated in Figure 2. This allows for imputation of missing views, obtaining a reconstruction of \(\bm{x}_{2}\) solely from realisation \(\bm{x}_{1}\) (and vice versa). If a joint prior is not assumed, separate VAEs are trained on each view and there is no correlation enforced between latent spaces. Whilst some correlation exists between the latent spaces, it is not as strong, and therefore may not produce as accurate reproductions, as illustrated in the numerical experiments of Section 3.

### Objective function

By assuming that latent variables for each view are independent given the data, the approximated posterior distributed can be expressed as:

\[q_{\phi}(\bm{z}|\bm{x})=q_{(\phi_{1},\phi_{2})}((\bm{z}_{1},\bm{z}_{2})|(\bm{ x}_{1},\bm{x}_{2}))=q_{\phi_{1}}(\bm{z}_{1}|\bm{x}_{1})q_{\phi_{2}}(\bm{z}_{2}| \bm{x}_{2}).\] (3)

The individual approximate posteriors \(q_{\phi_{i}}(\cdot|\bm{x}_{i})\) are multivariate Gaussians with mean and covariance matrix determined by the output of \(E_{i\phi_{i}}\). For input \(\bm{x}_{i}\), these are \(\bm{\mu}_{i}(\bm{x}_{i})\) and \(\operatorname{diag}(\bm{\sigma}_{i}^{2}(\bm{x}_{i}))\) respectively. As the posteriors are assumed to be independent, the joint distribution is multivariate Gaussian with mean \((\bm{\mu}_{1}(\bm{x}_{1});\bm{\mu}_{2}(\bm{x}_{2}))\). The covariance matrix is represented by \(\operatorname{diag}(\bm{\Sigma}_{i}(\bm{x}_{i}))\) with \(\bm{\Sigma}_{i}(\bm{x}_{i})=\operatorname{diag}(\bm{\sigma}_{i}^{2}(\bm{x}_{i}))\). Similarly, by assuming the data is independent given the latent variables, the likelihood function can be expressed as:

\[p_{\theta}(\bm{x}|\bm{z})=p_{(\theta_{1},\theta_{2})}((\bm{x}_{1},\bm{x}_{2}) |(\bm{z}_{1},\bm{z}_{2}))=p_{\theta_{1}}(\bm{x}_{1}|\bm{z}_{1})p_{\theta_{2}} (\bm{x}_{2}|\bm{z}_{2}).\] (4)

This is equivalent to having separate encoders and decoders for each view but with a joint prior. Assuming independence means the expectation term in the ELBO can be separated into terms corresponding to the separate views:

\[\mathbb{E}_{\bm{z}\sim q_{\phi}(\cdot|\bm{x})}\left[\operatorname{ln}p_{\theta }(\bm{x}|\bm{z})\right]=\mathbb{E}_{\bm{z}_{1}\sim q_{\phi_{1}}(\cdot|\bm{x}_ {1})}\left[\operatorname{ln}p_{\theta_{1}}(\bm{x}_{1}|\bm{z}_{1})\right]+ \mathbb{E}_{\bm{z}_{2}\sim q_{\phi_{2}}(\cdot|\bm{x}_{2})}\left[\operatorname{ ln}p_{\theta_{2}}(\bm{x}_{2}|\bm{z}_{2})\right].\]

Figure 3: Workflow to obtain reconstructions \(\tilde{\bm{x}}_{1}\) and \(\tilde{\bm{x}}_{2}\) from realisations \(\bm{x}_{1}\) and \(\bm{x}_{2}\) using a learnt JPVAE structure. Trapeziums present the encoders and decoders of the two views. Vectors are presented by the rectangles. The structures for view 1 and 2 are shown in blue and orange respectively. The prior on the latent variables is shown in red.

The objective function therefore becomes:

\[\mathcal{L}_{\beta}(\theta_{1},\phi_{1},\theta_{2},\phi_{2})= \mathbb{E}_{\bm{z}_{1}\sim q_{\phi_{1}}(\cdot|\bm{x})}\left[\text{ ln}p_{\theta_{1}}(\bm{x}_{1}|\bm{z}_{1})\right]+\mathbb{E}_{\bm{z}_{1}\sim q_{ \phi_{2}}(\cdot|\bm{x})}\left[\text{ln}p_{\theta_{2}}(\bm{x}_{2}|\bm{z}_{2})\right]\] (5) \[-\beta D_{KL}(q_{\phi_{1}}(\cdot|\bm{x}_{1})q_{\phi_{2}}(\cdot|\bm {x}_{2})\parallel p_{\theta}(\cdot)).\]

### Joint prior

There are assumed to be \(n_{i}\) latent variables in the VAE for view \(i\), represented by the random vector \(\bm{z}_{i}\in\mathbb{R}^{n_{i}}\). Without loss of generality it is that assumed \(n_{1}\leq n_{2}\). The joint prior distribution of the random vector \(\bm{z}=(\bm{z}_{1},\bm{z}_{2})\in\mathbb{R}^{n_{1}+n_{2}}\) is assumed to be a multivariate normal with mean \(\bm{0}\) and covariance matrix, \(\bm{\Sigma}_{C}\):

\[\bm{\Sigma}_{C}=\begin{pmatrix}\bm{I}_{1}&\bm{C}^{T}\\ \bm{C}&\bm{I}_{2}\end{pmatrix}\] (6)

where \(\bm{C}\in\mathbb{R}^{n_{2}\times n_{1}}\) is the cross-covariance matrix encapsulating the relationship between the two latent spaces. \(\bm{I}_{i}\) is the \(n_{i}\times n_{i}\) identity matrix. As \(\bm{\Sigma}_{C}\) is a covariance matrix, it needs to satisfy two conditions: symmetry and positive semi-definiteness. With the defined structure it is clearly symmetric and so it is sufficient to require \(\bm{\Sigma}_{C}\) to be a positive semi-definite matrix to ensure it is a well-defined covariance matrix. For a covariance matrix with structure as defined in Eq. 6, it is useful to note that \(\bm{\Sigma}_{C}\) is positive definite if and only if \(\bm{I}_{1}-\bm{C}^{T}\bm{C}\) (or \(\bm{I}_{2}-\bm{C}\bm{C}^{T}\)) is positive definite [16]. Additionally, as (a) a real symmetric matrix is positive (semi) definite if and only if all of its eigenvalues are positive (non-negative) [17, p. 51] and (b) a matrix is invertible if and only if all of its eigenvalues are non-zero, covariance matrix \(\bm{\Sigma}_{C}\) (and \(\bm{I}_{1}-\bm{C}^{T}\bm{C}/\bm{I}_{2}-\bm{C}\bm{C}^{T}\)) is positive definite if and only if it is invertible.

### Kullback-Leibler divergence term

As discussed earlier, for calculating the ELBO for the proposed prior, the KL divergence between \(q_{\phi_{1}}(\cdot|\bm{x}_{1})q_{\phi_{2}}(\cdot|\bm{x}_{2})\) and the prior on \((\bm{z}_{1},\bm{z}_{2})\), \(p_{\mathcal{C}}\), is required. The following general result on KL divergence between multivariate Gaussians is applied in this setting.

Between two \(k\)-dimensional multivariate Gaussians \(r\) and \(s\) with respective means \(\bm{\mu}_{r}\) and \(\bm{\mu}_{s}\) and respective covariance matrices \(\bm{\Sigma}_{r}\) and \(\bm{\Sigma}_{s}\), the KL divergence is given by [18]:

\[D_{KL}(s||r)=\frac{1}{2}\left[\ln\frac{|\bm{\Sigma}_{r}|}{|\bm{ \Sigma}_{s}|}-k+\left(\bm{\mu}_{s}-\bm{\mu}_{r}\right)^{T}\bm{\Sigma}_{r}^{-1} \left(\bm{\mu}_{s}-\bm{\mu}_{r}\right)+\mathrm{tr}\left\{\bm{\Sigma}_{r}^{-1} \bm{\Sigma}_{s}\right\}\right].\] (7)

As this assumes the existence of \(\bm{\Sigma}_{r}^{-1}\) and \(\bm{\Sigma}_{s}^{-1}\), both covariance matrices must be positive definite.

By utilising the result of Eq. 7 with the prior over \(\bm{z}=(\bm{z}_{1},\bm{z}_{2})\) set as \(r=p_{\mathcal{C}}\), and the approximate posterior distribution as \(s=q_{\phi_{1}}(\cdot|\bm{x}_{1})q_{\phi_{2}}(\cdot|\bm{x}_{2})\), the KL term of Eq. 2 is obtained. Explicitly, \(p_{\mathcal{C}}=N(\bm{0},\bm{\Sigma}_{C})\) and \(q_{\phi_{1}}(\cdot|\bm{x}_{1})q_{\phi_{2}}(\cdot|\bm{x}_{2})=N((\bm{\mu}_{1}( \bm{x}_{1}),\bm{\mu}_{2}(\bm{x}_{2})),\bm{\Sigma}_{q})\) with \(\bm{\Sigma}_{q}=\mathrm{bdiag}(\bm{\Sigma}_{i})\). For the utilisation of Eq. 7, it is assumed that \(\bm{\Sigma}_{C}\) is positive definite and the variances of the approximate posterior are positive.

Using the block matrix inverse result from (19, p. 18) (stated in Appendix A.1.1) the inverse of \(\bm{\Sigma}_{C}\) is given by:

\[\bm{\Sigma}_{C}^{-1}=\begin{pmatrix}\bm{D}_{1}&-\bm{D}_{1}\bm{C}^{T}\\ -\bm{D}_{2}\bm{C}&\bm{D}_{2}\end{pmatrix}\] (8)

where \(\bm{D}_{1}=(\bm{I}_{1}-\bm{C}^{T}\bm{C})^{-1}\) and \(\bm{D}_{2}=(\bm{I}_{2}-\bm{C}\bm{C}^{T})^{-1}\). These inverses are guaranteed to exist, given the assumption of positive definiteness on \(\bm{\Sigma}_{C}\). Using Eq. 8 and properties of the trace, we obtain:

\[\mathrm{tr}\left\{\bm{\Sigma}_{C}^{-1}\bm{\Sigma}_{q}\right\}= \mathrm{tr}\left\{\begin{pmatrix}\bm{D}_{1}\bm{\Sigma}_{1}&-\bm{D}_{1}\bm{C}^{T} \bm{\Sigma}_{2}\\ -\bm{D}_{2}\bm{C}\bm{\Sigma}_{1}&\bm{D}_{2}\bm{\Sigma}_{2}\end{pmatrix}\right\} =\mathrm{tr}\left\{\bm{D}_{1}\bm{\Sigma}_{1}\right\}+\mathrm{tr}\left\{\bm{D}_{2 }\bm{\Sigma}_{2}\right\}.\]

Additionally, as \(\bm{\Sigma}_{\bm{C}}\) is a block matrix, \(|\bm{\Sigma}_{C}|=|\bm{I}_{1}-\bm{C}^{T}\bm{C}|=1/|\bm{D}_{1}|=1/|\bm{D}_{2}|\)[20, p. 114]. Similarly. as \(\bm{\Sigma}_{q}\) is a diagonal matrix we have \(|\bm{\Sigma}_{q}|=|\bm{\Sigma}_{1}||\bm{\Sigma}_{2}|\). Writing \(\bm{\mu}_{i}=\bm{\mu}_{i}(\bm{x}_{i})\) and notingthat \(\bm{\mu}_{p}=0\), we obtain:

\[D_{KL}(q_{\phi_{1}}(\cdot|\bm{x}_{1})q_{\phi_{2}}(\cdot|\bm{x}_{2}) ||p_{\bm{C}}) =\frac{1}{2}\left[\bm{\mu}_{1}^{T}\bm{D}_{1}\bm{\mu}_{1}-\ln|\bm{ \Sigma}_{1}|-n_{1}+\operatorname{tr}\left\{\bm{D}_{1}\bm{\Sigma}_{1}\right\}\right]\] (9) \[\qquad+\frac{1}{2}\left[\bm{\mu}_{2}^{T}\bm{D}_{2}\bm{\mu}_{2}- \ln|\bm{\Sigma}_{2}|-n_{2}+\operatorname{tr}\left\{\bm{D}_{2}\bm{\Sigma}_{2} \right\}\right]\] \[\qquad-\frac{1}{2}\left[\ln|\bm{D}_{1}|+\bm{\mu}_{1}^{T}\bm{D}_{ 1}\bm{C}^{T}\bm{\mu}_{2}+\bm{\mu}_{2}^{T}\bm{D}_{2}\bm{C}\bm{\mu}_{1}\right].\]

### Matrix parameterization

The matrix \(\bm{C}\) is unknown and therefore must be either chosen apriori or be optimised. In our work, \(\bm{C}\) is updated along with all other parameters at each update step, using the ELBO function as the loss function. By doing this, two challenges were faced and addressed. Firstly, \(C\) must be updated in such a way that \(\Sigma_{C}\) is a valid covariance matrix. Secondly, a differentiable parameterization of \(C\) is needed to allow for end-to-end learning. Conditions for validity of the update step are outlined in the following theorems.

**Theorem 2.1**.: \(\bm{\Sigma}_{C}\) _defined as in Eq. 6 is positive semi-definite if and only if all singular values of \(C\) are bounded by 1._

Proof.: Due to previous remarks, it is equivalent to show that \(\bm{I}_{1}-\bm{C}^{T}\bm{C}\) is positive semi-definite if and only if the singular values of \(\bm{C}\) are bounded by 1. Further, this is equivalent to the showing that eigenvalues of \(\bm{I}_{1}-\bm{C}^{T}\bm{C}\) are non-negative if and only if the eigenvalues of \(C\) are bounded by 1.

The eigenvalues of \(\bm{C}^{T}\bm{C}\) are given by \(\{\sigma_{k}^{2}(\bm{C})\}_{k=1}^{n_{1}}\)[21] meaning the eigenvalues of \(-\bm{C}^{T}\bm{C}\) are given by \(\{-\sigma_{k}^{2}(\bm{C})\}_{k=1}^{n_{1}}\). Applying Weyl's Theorem [19, p. 242] (stated in Appendix A.1.2) to \(\bm{I}_{1}-\bm{C}^{T}\bm{C}\) implies:

\[\lambda_{1}(\bm{I}_{1})+\lambda_{n_{1}+1-k}(-\bm{C}^{T}\bm{C}) \leq\lambda_{k}(\bm{I}_{1}-\bm{C}^{T}\bm{C})\leq\lambda_{n_{1}}( \bm{I}_{1})+\lambda_{n_{1}+1-k}(-\bm{C}^{T}\bm{C})\] (10) \[\implies 1\leq\lambda_{k}(\bm{I}_{1}-\bm{C}^{T}\bm{C})-\lambda_{n_{1}+1-k}( -\bm{C}^{T}\bm{C})\leq 1\]

which combined with the relations above gives \(\lambda_{k}(\bm{I}_{1}-\bm{C}^{T}\bm{C})=1+\lambda_{n_{1}+1-k}(-\bm{C}^{T}\bm{ C})=1-\sigma_{n_{1}+1-k}^{2}(\bm{C})\). Therefore all eigenvalues of \(\bm{I}_{1}-\bm{C}^{T}\bm{C}\) are non-negative if and only if all singular values of \(C\) are bounded by 1. 

If the inequality on \(\sigma_{1}(\bm{C})\) is replaced by a strict inequality then \(\bm{\Sigma}_{C}\) is guaranteed to be positive definite. This is clear as all eigenvalues of \(\bm{I}_{1}-\bm{C}^{T}\bm{C}\) are now positive which ensures positive definiteness. This guarantees the applicability of Eq. 9. A further restriction, outlined in Theorem 2.2, can be made which enables a different parameterization of \(C\) which assumes a scaled orthogonal relationship between the views.

**Theorem 2.2**.: _If \(\bm{C}=\alpha\tilde{\bm{C}}\in\mathbb{R}^{n_{2}\times n_{1}}\) where \(|\alpha|\leq 1\) and \(\tilde{\bm{C}}\) is a semi-orthogonal matrix (\(\tilde{\bm{C}}^{T}\tilde{\bm{C}}=I_{1}\)) then \(\bm{\Sigma}_{C}\) is positive semi-definite._

Proof.: Given the assumed structure on \(\bm{C}\), we have the following:

\[\bm{I}_{1}-\bm{C}^{T}\bm{C}=\bm{I}_{1}-\alpha^{2}\tilde{\bm{C}}^{T}\tilde{\bm{ C}}=(1-\alpha^{2})\bm{I}_{1}.\] (11)

As \(\bm{I}_{1}\) is positive semi definite, if \(|\alpha|\leq 1\) then, so too is \(\bm{I}_{1}-\bm{C}^{T}\bm{C}\). 

Alternatively, this could be seen as a corollary to Theorem 2.1 - due to the semi-orthogonality of \(\tilde{\bm{C}}\), all singular values of \(\tilde{\bm{C}}\) are equal to 1 which means \(\sigma_{i}(\bm{C})=\alpha\) with \(|\alpha|\leq 1\). Again, if the condition on \(\alpha\) is replaced with a strict inequality (\(|\alpha|<1\)) this guarantees applicability of Eq. 9. A simplification of Eq. 9 assuming this orthogonality condition can be found in Appendix A.2.

To ensure applicability of Eq. 9, we can either require (a) \(\bm{C}=\alpha\tilde{\bm{C}}\) with \(\tilde{\bm{C}}\) an orthogonal matrix and \(|\alpha|<1\) or (b) \(\sigma_{1}(\bm{C})<1\). The latter requires a matrix factorisation of \(\bm{C}\) which includes singular values. The most obvious approach is to use a singular value decomposition (SVD) of \(\bm{C}\) _i.e._\(\bm{C}=\bm{U}\bm{S}\bm{V}^{T}\) where \(\bm{U}\) and \(\bm{V}\) are orthogonal matrices and \(\bm{S}\) is the matrix of singular values i.e. \(\bm{S}=\mathrm{diag}(\sigma_{k}(\bm{C}))\). These singular values can be parameterized by \(\sigma_{k}(\bm{C})=(1-\exp(-\sigma_{k})/(1+\exp(-\sigma_{k}))\) for \(\sigma_{k}\in\mathbb{R}\). To fully parameterize \(\bm{C}\) with the singular value constraint, a parameterization of orthogonal matrices \(\bm{U}\) and \(\bm{V}\) is needed. Similarly, a parameterization of orthogonal \(\tilde{\bm{C}}\) is needed for option (a). The following assumes \(n_{1}=n_{2}=n\).

As discussed by Shepard et al. [22], there are four popular parameterizations of orthogonal matrices. All methods in their preliminary form parameterize at most a subset of the orthogonal matrices (at maximum those with determinant +1 or -1). These parameterizations must therefore be extended to map to the entire orthogonal matrix space. As it is the only one-to-one mapping, the rational Cayley transform has been chosen for use within JPVAE. In its original form, the Cayley transform tells us that for all orthogonal matrices with no eigenvalues equal to 1, there exists a unique skew-symmetric matrix \(\bm{A}\in\mathbb{R}^{m\times m}\) such that \(\bm{O}=(\bm{I}+\bm{A})(\bm{I}-\bm{A})^{-1}\)[22]. To extend the parameterization to the full space of orthogonal matrices, an extra matrix consisting of 1s and \(-1\)s along the diagonal is needed [23; 24]. Let \(\bm{J}=\mathrm{diag}([\bm{1}_{m-r};-\bm{1}_{r}])\) where \(r=\mathrm{floor}(m/(1+\exp(-s)))\) for \(s\in\mathbb{R}\). Then \(\bm{O}=\bm{J}(\bm{I}+\bm{A})(\bm{I}-\bm{A})^{-1}\) is a mapping onto the space of orthogonal matrices. \(\tilde{\bm{C}}\) is therefore parameterized in full by \((n-1)^{2}/2+1\) parameters: \(\left\{\left\{a_{ij}\in\mathbb{R}:i<j\quad\text{for}\quad i,j\in\left\{1, \cdots,n\right\}\right\},\left\{s\in\mathbb{R}\right\}\right\}\).

### Imputation

Once the model has been learned on the training data, missing views can be imputed. Assume data is available for view \(j\), but not view \(i\), given \(\bm{x}_{j}\) we can impute the missing value of \(\bm{x}_{i}\). Data \(\bm{x}_{j}\) is fed into the encoder for view \(j\), \(E_{j\phi}\), and latent variables are sampled giving \(\bm{z}_{j}=\bm{a}\). An estimate of \(\bm{z}_{i}\) can be obtained using the conditional distribution of \(\bm{z}_{i}\) given \(\bm{z}_{j}\). This is fed through decoder \(i\), \(D_{i\theta}\), to produce an estimate of \(\bm{x}_{i}\) given \(\bm{x}_{j}\), \(\tilde{\bm{x}}_{i|j}\).

The joint marginal of \(\bm{z}_{1}\) and \(\bm{z}_{2}\) is assumed to be a multivariate normal with mean \([\bm{\mu}_{1};\bm{\mu}_{2}]\) and covariance matrix \(\bm{\Sigma}\). The maximum likelihood estimator for the mean and covariance are obtained and denoted by \([\hat{\bm{\mu}_{1}};\bm{\mu}_{2}]\) and \(\hat{\bm{\Sigma}}\) respectively. As any subset of variables from a multivariate normal conditioned on a known second subset of variables also follows a multivariate normal distribution, the conditional distribution can be found explicitly. For \(i\neq j\), the distribution of \(\bm{z}_{i}\) given \(\bm{z}_{j}=\bm{a}\) is observed is:

\[\bm{z}_{i}|\bm{z}_{j}=\bm{a}\sim N\left(\hat{\bm{\mu}}_{i}+\hat{\bm{\Sigma}}_{ ij}\hat{\bm{\Sigma}}_{jj}^{-1}(\bm{a}-\hat{\bm{\mu}}_{j}),\hat{\bm{\Sigma}}_{ii}- \hat{\bm{\Sigma}}_{ij}\hat{\bm{\Sigma}}_{jj}^{-1}\hat{\bm{\Sigma}}_{ji}\right)\] (12)

where \(\hat{\bm{\Sigma}}_{kl}\in\mathbb{R}^{n_{k}\times n_{l}}\) is the submatrix of \(\hat{\bm{\Sigma}}\) corresponding to the variables associated with view \(k\) and view \(l\). The conditional mean \(\mathbb{E}(\bm{z}_{i}|\bm{z}_{j}=\bm{a})=\hat{\bm{\mu}}_{i}+\hat{\bm{\Sigma}}_{ ij}\hat{\bm{\Sigma}}_{jj}^{-1}(\bm{a}-\hat{\bm{\mu}}_{j})\) is then used as an estimate of the latent variables in latent space \(i\) and fed into \(D_{i\theta}\) to obtain \(\tilde{\bm{x}}_{i|j}\), the imputed value of \(\bm{x}_{i}\) given \(\bm{x}_{j}\).

## 3 Numerical experiments

Through a series of experiments the performance of JPVAE is explored for both imputation purposes as well as for downstream analyses like classification. As JPVAE enables imputation of missing views (\(\tilde{\mathbf{X}}_{i|j}\)), this ability is investigated alongside reconstruction of views (\(\tilde{\mathbf{X}}_{i}\)).

A multi-view dataset was created from the binary version of the popular MNIST dataset, which consists of handwritten digits from \(0\) to \(9\)[25]. For each image, the top half was taken as view 1 and the bottom half was taken as view 2. This dataset of halved images is referred to as hvdMNIST. The hvdMNIST dataset has the desirable property of having a strong correlation between views. The dataset contains \(50,000\) training images and \(10,000\) test images. Experiments are repeated with 5 different random seeds and the average and standard deviation reported. Details on the model architecture and training details can be found in Appendices A.3 and A.4 respectively.

Two variants of JPVAE are explored, where in each one the validity of \(\Sigma_{C}\) is enforced via different ways. The first variant imposes a bound on singular values (such that \(\sigma_{1}(\bm{C})<1\)). The second variant enforces a scaled orthogonality where \(\bm{C}\bm{C}^{T}=\bm{C}^{T}\bm{C}=\alpha^{2}\bm{I}\), with the value of \(\alpha\) set as \(0.95\). The two variants of JPVAE are compared with the \(\bm{C}=\bm{0}\) case, which corresponds to completely disjoint VAEs for each view.

Without explicitly learning a correlation structure, correlation is present between the two generated latent spaces. By incorporating a joint prior with a non-zero cross-correlation as presented in Eq. 6into the loss function, JPVAE increases the correlation between views in the latent space as shown in Figure 4. Table 1 illustrates the improvement in the ability to reconstruct view 2 given view 1 and vice versa when correlation structure is learnt. Enforcing the orthogonality restriction improves the performance of JPVAE compared with simply applying the singular value bound.

The improved performance of the method is enabled by the learnt correlation that prevents posterior collapse. Following the work by [26], the phenomenon of posterior collapse is indicated by the percentage of active units (AU). The activity of unit \(u\) in the latent space is measured by \(\mathrm{A}_{u}=\mathrm{Cov}_{\mathbf{x}}\left(\mathbb{E}_{u\sim q(u|\mathbf{x} )}[u]\right)\). A unit is considered active, _i.e._ to not have suffered from posterior collapse, if \(A_{u}\geq 10^{-2}\). A higher percentage of AUs are preserved when \(\bm{C}\) is learnt, with the best case scenario observed with the orthogonality constraint (Table 2). Wang et al. [12] proved that posterior collapse is equivalent to latent variable non-identifiability. This indicates that by enforcing the orthogonality restriction, we make the latent variable space identifiable.

The imputed views, \(\tilde{\bm{X}}_{1|2}\) and \(\tilde{\bm{X}}_{2|1}\), can be used for downstream tasks. If not all views are available for an individual, this allows techniques requiring all views to be applied. As an illustration of the performance of the imputed data in downstream tasks, a basic multi-layer perceptron classifier was

\begin{table}
\begin{tabular}{l c c c} \hline  & \multicolumn{2}{c}{Reconstruction} & \multicolumn{2}{c}{Imputation} \\ \cline{2-4}  & \(\tilde{\bm{X}}_{1}\) & \(\tilde{\bm{X}}_{2}\) & \(\tilde{\bm{X}}_{1|2}\) & \(\tilde{\bm{X}}_{2|1}\) \\ \hline \(\bm{C}=\bm{0}\) & 24.64 (0.37) & 25.56 (0.24) & 114.1 (2.3) & 127.5 (3.2) \\ \(\sigma_{1}(\bm{C})<1\) & 24.08 (0.44) & 25.02 (0.21) & 106.6 (1.4) & 117.4 (4.1) \\ \(\bm{C}\bm{C}^{T}=\bm{C}^{T}\bm{C}=\alpha^{2}\bm{I}\) & **23.41** (0.29) & **23.98** (0.31) & **97.25** (1.9) & **106.6** (1.9) \\ \hline \end{tabular}
\end{table}
Table 1: Average reconstruction losses across the entire dataset. Best results in bold, standard deviation in brackets.

\begin{table}
\begin{tabular}{l c} \hline  & AU \\ \hline \(\bm{C}=\bm{0}\) & 61 (1.4) \\ \(\sigma_{1}(\bm{C})<1\) & 66 (1.4) \\ \(\bm{C}\bm{C}^{T}=\bm{C}^{T}\bm{C}=\alpha^{2}\bm{I}\) & **98.5** (2.2) \\ \hline \end{tabular}
\end{table}
Table 2: The percentage (%) of active units (AU) across latent spaces \((\bm{z}_{1},\bm{z}_{2})\), out of a total 40. Best results in bold, standard deviation in brackets.

Figure 4: Empirical cross-correlation between view 1 and view 2 in the latent spaces. The left plot represents empirical cross-correlation for \(\bm{C}=\bm{0}\) and the right shows the same for \(\bm{C}\) learnt with the orthogonality restriction imposed. The Frobenius norm of these matrices are 1.703 and 3.448 respectively.

trained and tested on different combinations of reconstructed and imputed views. The performance of a classifier trained on the reconstructed data indicates that when \(\bm{C}\) is learnt, the relevant signal remains present in both the reconstructed and the imputed data. The performance on imputed data is greatly improved by learning \(\bm{C}\), and in particular by enforcing the orthogonality constraint (Table 3; results with standard deviation can be found in Figure 9.).

Not only does learning correlation structure improve the ability to impute data, the reconstruction loss and classification accuracy for reconstructed data \(\bm{x}_{i}\) is improved compared with those scores seen when training each view on separate VAEs (Table 1). This may be due to the increased use of the latent space, as evidenced by the higher percentage of active units. Whilst a classifier trained on the imputed data from \(\bm{C}=\bm{0}\) demonstrates the retention of signal, the low accuracy seen for a classifier trained on reconstructed data and high reconstruction loss indicates that it does not retain the specific signature of the input. For example, taking the top half of a digit '2' as the input, it may correctly reconstruct the bottom half of a realisation of the digit '2' but not correctly reconstruct the specific realisation (as seen in Figure 0(a)).

Using the joint prior we see that the view with stronger signal (view 1) is bolstering the classification of the view with weaker signal (view 2). Whilst classification accuracy on \(\bm{x}_{1}\) is higher than that on \(\bm{x}_{2}\), the accuracy on the imputed data \(\tilde{\bm{X}}_{2|1}\) sees a smaller drop, and is greater than that of \(\tilde{\bm{X}}_{1|2}\). Notably, the accuracy on imputed data \((\tilde{\bm{X}}_{2|1},\tilde{\bm{X}}_{2|1})\) is comparable to that on \((\tilde{\bm{X}}_{2},\tilde{\bm{X}}_{2})\) whilst the same for view 1 experiences a drop in performance.

## 4 Conclusions

A novel multi-view VAE approach has been proposed that natively strengthens the correlation between latent spaces via a joint prior. This is the first time that a connection between multi-view VAEs is made through a joint prior rather than a joint posterior, as has previously been implemented in the literature. Theoretical guarantees and parameterizations are presented that allow for end-to-end learning. By simultaneously preventing posterior collapse, JPVAE returns superior models and demonstrates a promising ability to impute missing data suitable for downstream tasks.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{3}{c}{View 1} & \multicolumn{3}{c}{View 2} \\ \cline{2-7}  & \((\tilde{\bm{X}}_{1},\tilde{\bm{X}}_{1})\) & \((\tilde{\bm{X}}_{1},\tilde{\bm{X}}_{1|2})\) & \((\tilde{\bm{X}}_{1|2},\tilde{\bm{X}}_{1|2})\) & \((\tilde{\bm{X}}_{2},\tilde{\bm{X}}_{2})\) & \((\tilde{\bm{X}}_{2},\tilde{\bm{X}}_{2|1})\) & \((\tilde{\bm{X}}_{2|1},\tilde{\bm{X}}_{2|1})\) \\ \hline \(\bm{C}=\bm{0}\) & 89.07 & 47.21 & 77.22 & 85.88 & 51.83 & 78.81 \\ \(\sigma_{1}(\bm{C})<1\) & 89.67 & 64.50 & 80.22 & 86.46 & 67.06 & 82.38 \\ \(\bm{C}\bm{C}^{T}=\bm{C}^{T}\bm{C}=\alpha^{2}\bm{I}\) & 90.31 & 77.22 & 83.54 & 87.80 & 76.55 & 86.30 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results for \((\bm{Y},\bm{Z})\) represent classification accuracy % for model trained on the training split of \(\bm{Y}\) and tested on the test split of \(\bm{Z}\). Accuracies for \((\tilde{\bm{X}}_{1},\tilde{\bm{X}}_{1})\) and \((\tilde{\bm{X}}_{2},\tilde{\bm{X}}_{2})\) with standard deviation in brackets are \(93.59\%\) (\(0.25\)) and \(90.83\%\) (\(0.23\)) respectively. Best results in bold. For clarity, results with standard deviation reported can be found in Figure 9.

## References

* Flores et al. [2023] Javier E. Flores, Daniel M. Claborne, Zachary D. Weller, Bobbie-Jo M. Webb-Robertson, Katrina M. Waters, and Lisa M. Bramer. Missing data in multi-omics integration: Recent advances through artificial intelligence. _Frontiers in Artificial Intelligence_, 6, 2023. ISSN 2624-8212. doi: 10.3389/frai.2023.1098308. URL https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2023.1098308.
* Lin et al. [2016] Dongdong Lin, Jigang Zhang, Jingyao Li, Chao Xu, Hong-Wen deng, and Yu-Ping Wang. An integrative imputation method based on multi-omics datasets. _BMC Bioinformatics_, 17, 06 2016. doi: 10.1186/s12859-016-1122-6.
* Kingma and Welling [2014] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In _2nd International Conference on Learning Representations_, 2014. URL http://arxiv.org/abs/1312.6114.
* Song et al. [2020] Meng Song, Jonathan Greenbaum, Joseph Luttrell, Weihua Zhou, Chong Wu, Hui Shen, Ping Gong, Chaoyang Zhang, and Hong-Wen Deng. A review of integrative imputation for multimics datasets. _Frontiers in Genetics_, 11, 2020.
* Sutter et al. [2020] Thomas Sutter, Imant Daunhawer, and Julia Vogt. Multimodal generative learning utilizing jensen-shannon-divergence. _Advances in neural information processing systems_, 33:6100-6110, 2020.
* Shi et al. [2019] Yuge Shi, N. Siddharth, Brooks Paige, and Philip H. S. Torr. Variational mixture-of-experts autoencoders for multi-modal deep generative models. In _Proceedings of the 33rd International Conference on Neural Information Processing Systems_, 2019.
* Wu and Goodman [2018] Mike Wu and Noah Goodman. Multimodal generative models for scalable weakly-supervised learning. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, page 5580-5590, 2018.
* Kalafut et al. [2023] Noah Cohen Kalafut, Xiang Huang, and Daireng Wang. Joint variational autoencoders for multimodal imputation and embedding. _Nature Machine Intelligence_, 5:631-642, 2023.
* Daunhawer et al. [2022] Imant Daunhawer, Thomas M. Sutter, Kieran Chin-Cheong, Emanuele Palumbo, and Julia E Vogt. On the limitations of multimodal VAEs. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=w-CPUXXrAj.
* Wang et al. [2016] Weiran Wang, Xinchen Yan, Honglak Lee, and Karen Livescu. Deep variational canonical correlation analysis. _arXiv preprint arXiv:1610.03454_, 2016.
* Zbontar et al. [2021] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In _38th International Conference on Machine Learning, ICML 2021_, pages 12310-12320. ML Research Press, 2021.
* Wang et al. [2021] Yixin Wang, David Blei, and John P Cunningham. Posterior collapse and latent variable non-identifiability. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 5443-5455, 2021.
* 86, 1951. doi: 10.1214/aoms/1177729694.
* Fu et al. [2019] Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, and Lawrence Carin. Cyclical annealing schedule: A simple approach to mitigating KL vanishing. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 240-250. Association for Computational Linguistics, 2019. doi: 10.18653/v1/N19-1021. URL https://aclanthology.org/N19-1021.
* Bowman et al. [2015] Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. In _Conference on Computational Natural Language Learning_, 2015.

* Boyd and Vandenberghe [2004] Stephen Boyd and Lieven Vandenberghe. _Convex optimization_, chapter Mathematical background, page 650. Cambridge University Press, 2004.
* Petersen and Pedersen [2012] K. B. Petersen and M. S. Pedersen. The matrix cookbook, Nov 2012. URL http://www2.compute.dtu.dk/pubdb/pubs/3274-full.html. Version 20121115.
* Pardo [2018] L. Pardo. _Statistical Inference Based on Divergence Measures_. Statistics: A Series of Textbooks and Monographs. CRC Press, 2018. ISBN 9781420034813.
* Horn and Johnson [2012] Roger A. Horn and Charles R. Johnson. _Matrix Analysis_. Cambridge University Press, 2 edition, 2012.
* Abadir and Magnus [2005] K.M. Abadir and J.R. Magnus. _Matrix Algebra_. Econometric Exercises. Cambridge University Press, 2005. ISBN 9781139443647.
* Demmel [1997] James W. Demmel. _Applied Numerical Linear Algebra_, chapter Linear Least Squares Problems, pages 110-111. Society for Industrial and Applied Mathematics, 1997. doi: 10.1137/1.9781611971446.ch3.
* Shepard et al. [2015] Ron Shepard, Scott R. Brozell, and Gergely Gidofalvi. The representation and parametrization of orthogonal matrices. _The Journal of Physical Chemistry A_, 119(28):7924-7939, 07 2015.
* Ferrar [1960] W.L. Ferrar. _Algebra: a Textbook of Determinants, Matrices and Algebraic Forms_. Oxford University Press, 1960. URL https://archive.org/details/algebra032104mbp/page/n3/mode/2up.
* Khuri and Good [1989] A.I. Khuri and I.J. Good. The parameterization of orthogonal matrices: a review mainly for statisticians. _South African Statistical Journal_, 23, 1989.
* Lecun et al. [1998] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.
* Burda et al. [2015] Yuri Burda, Roger Baker Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. _ICLR_, 2015.
* Kingma and Ba [2017] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017. URL https://arxiv.org/abs/1412.6980.

## Appendix A Appendix / supplemental material

The Appendix contains additional results and figures to supplement the main body of text.

Theoretical results used within the manuscript are presented in Section A.1. This is followed by a simplification of the KL term when orthogonality constraints are assumed which is outlined in Section A.2. The model architecture and training details implemented in the numerical experiments are detailed in Sections A.4 and A.3 respectively. Lastly, additional results from the conducted numerical experiments can be found in Section A.5. All results throughout this manuscript are given to 4 significant figures, with standard deviations reported to 2 significant figures.

### Theoretical results

Theoretical results utilised within the paper are now presented.

#### a.1.1 Block matrix inverse

The inverse of a block matrix, which is needed in Section 2.5 to determine the inverse of \(\bm{\Sigma}_{C}\) in order to derive the KL term, is now outlined.

For a matrix \(\bm{A}\) with block matrix structure

\[\begin{pmatrix}\bm{A}_{11}&\bm{A}_{12}\\ \bm{A}_{21}&\bm{A}_{22}\end{pmatrix}\] (13)

the following result holds [19, p. 18]:

**Lemma A.1**.: _Assuming all relevant inverses exist, the inverse of \(\bm{A}\) as defined in Eq. 13 is given by:_

\[\begin{pmatrix}(\bm{A}_{11}-\bm{A}_{12}\bm{A}_{22}^{-1}\bm{A}_{21})^{-1}&\bm{A}_ {11}^{-1}\bm{A}_{12}(\bm{A}_{21}\bm{A}_{11}^{-1}\bm{A}_{12}-\bm{A}_{22})^{-1}\\ \bm{A}_{22}^{-1}\bm{A}_{21}(\bm{A}_{12}\bm{A}_{22}^{-1}\bm{A}_{21}-\bm{A}_{11} )^{-1}&(\bm{A}_{22}-\bm{A}_{21}\bm{A}_{11}^{-1}\bm{A}_{12})^{-1}\end{pmatrix}.\] (14)

Applying this with \(\bm{A}_{ii}=\bm{I}_{i}\), \(\bm{A}_{12}=\bm{C}^{T}\) and \(\bm{A}_{21}=\bm{C}\) gives the result in the text.

#### a.1.2 Weyl's inequality

The following inequality is used within the proof of Theorem 2.1 and concerns the sequence of eigenvalues of matrices. In contrast to the rest of the paper, the eigenvalues are indexed in non-increasing order, not non-increasing order of magnitude. For matrix \(\bm{M}\) these are denoted by \(\{\hat{\lambda}_{j}(\bm{M})\}_{j=1}^{n}\).

Weyl's inequality [19, p. 242, Corollary 4.3.15] says:

**Lemma A.2**.: _Let \(\bm{A},\bm{B}\in\mathbb{R}^{n\times n}\) be symmetric matrices. The following inequality holds_

\[\hat{\lambda}_{j}(\bm{A})+\hat{\lambda}_{1}(\bm{B})\leq\hat{ \lambda}_{j}(\bm{A}+\bm{B})\leq\hat{\lambda}_{j}(\bm{A})+\hat{\lambda}_{n}(\bm {B}),\quad j=1,\ldots,n\] (15)

Notice that \(\lambda_{k}(\bm{I}_{1})=\hat{\lambda}_{j}(\bm{I}_{1})=1\) for all \(j,k\), and that as \(-\bm{C}^{T}\bm{C}\) is negative semi-definite all eigenvalues are non-positive. This means \(\hat{\lambda}_{j}(-\bm{C}^{T}\bm{C})=\lambda_{n-j}(-\bm{C}^{T}\bm{C})\). Applying this lemma with \(\bm{A}=-\bm{C}^{T}\bm{C}\) and \(\bm{B}=\bm{I}_{1}\) gives Eq. 10.

### KL term simplification

Under the orthogonality assumption on \(\bm{C}\), the KL term derived in Eq. 9 can be simplified. Specifically, if \(\bm{C}\bm{C}^{T}=\bm{C}^{T}\bm{C}=\alpha^{2}\bm{I}\) for some \(\alpha\in(0,1)\) then \(\bm{D}_{1}=\bm{D}_{2}=\gamma\bm{I}\) where \(\bm{I}=\bm{I}_{1}=\bm{I}_{2}\) and \(\gamma=1/(1-\alpha^{2})\). Therefore Eq. 9 reduces to:

\[D_{KL}(q_{\phi_{1}}(\cdot|\bm{x}_{1})q_{\phi_{2}}(\cdot|\bm{x}_{1 })||p_{\bm{C}})=\frac{1}{2} \left[\gamma\bm{\mu}_{1}^{T}\bm{\mu}_{1}-\ln|\bm{\Sigma}_{1}|-n+ \gamma\operatorname{tr}\left\{\bm{\Sigma}_{1}\right\}\right]\] (16) \[+\frac{1}{2}\left[\gamma\bm{\mu}_{2}^{T}\bm{\mu}_{2}-\ln|\bm{ \Sigma}_{2}|-n+\gamma\operatorname{tr}\left\{\bm{\Sigma}_{2}\right\}\right]\] \[-\frac{1}{2}\left[n\ln|\gamma|+2\gamma\bm{\mu}_{1}^{T}\bm{C}\bm{ \mu}_{2}\right].\]

### Model architecture

All encoders and decoders for the JPVAE, as well as the neural network used for classification, consist of two dense layers with 512 units each. The latent distribution layers consists of two 20 unit dense layers which each parameterize the mean and the log of the variances of 20 normal random variables. The decoders output layer parameterizes the distribution of a Bernouilli random variable for each pixel. The classifier output layer parameterizes the categorical distribution with 10 outcomes.All activation functions were ReLu.

### Training details

The JPVAE used an Adam optimiser [27] with learning rate 0.001, trained for 30 epochs with a batch size of 32 and used binary cross entropy as the reconstruction error. The cyclical KL annealing schedule as introduced by [14] is implemented, with \(M=30\).

The classifier is trained for 50 epochs with a batch size of 32, step size of 0.01, except for on original data where it is trained for 15 epochs to prevent overfitting. Cross entropy loss is used as the loss function.

[MISSING_PAGE_FAIL:14]

Figure 6: Imputation of the top half of MNIST digits (view 1 of the data) using the bottom half of the image (view 2) on a JPVAE model trained with (a) independent priors (completely separate VAEs) and (b) a joint prior with learnt correlation structure between latent spaces. The cross entropy loss between true top half of image and imputation is 117.8 in (a) and 100.2 in (b).

Figure 7: Additional realisation of an imputation of the top half of MNIST digits (view 1 of the data) using the bottom half of the image (view 2) on a JPVAE model trained with (a) independent priors (completely separate VAEs) and (b) a joint prior with learnt correlation structure between latent spaces. The cross entropy loss between true top half of image and imputation is 114.3 in (a) and 104.1 in (b).

Figure 8: Results for \([\bm{Y}_{1};\bm{Y}_{2}]\) represent classification accuracy % for model trained on the training split of \([\hat{\bm{X}}_{1};\hat{\bm{X}}_{2}]\) and tested on the test split of \([\bm{Y}_{1};\bm{Y}_{2}]\) (the column wise concatenation of \(\bm{Y}_{1}\) and \(\bm{Y}_{2}\)). Accuracy for \([\bm{X}_{1};\bm{X}_{2}]\) with standard deviation in brackets is \(98.04\%\) (\(0.074\)). Error bars present +/- one standard deviation.

Figure 9: Results for \((\bm{Y},\bm{Z})\) represent classification accuracy % for model trained on the training split of \(\bm{Y}\) and tested on the test split of \(\bm{Z}\). Accuracies for \((\bm{X}_{1},\bm{X}_{1})\) and \((\bm{X}_{2},\bm{X}_{2})\) with standard deviation in brackets are \(93.59\%\) (\(0.25\)) and \(90.83\%\) (\(0.23\)) respectively. Error bars present +/- one standard deviation.