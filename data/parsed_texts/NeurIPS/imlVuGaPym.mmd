# Equivariant Quantum Neural Networks for Image Classification

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We propose an Equivariant Quantum Neural Network (EQNN) architecture that leverages symmetries commonly present in image data, specifically roto-reflection symmetries. By incorporating symmetries such as rotations and reflections into the quantum neural network's design, we can significantly reduce the number of trainable parameters, thereby decreasing the model's complexity and improving its efficiency. This method enhances learning capabilities with smaller datasets while also promoting better generalization. We evaluate the performance of our model using standard benchmark datasets for image classification and compare it against other quantum models.

## 1 Introduction

In the fast-evolving field of machine learning, incorporating symmetries into model architectures has proven to be a highly effective method for introducing inductive biases. These biases play a key role in improving both how efficiently models are trained and how well they generalize to new data. Symmetry integration allows models to better utilize the intrinsic patterns within the data, thereby reducing the need for large datasets and extensive pre-processing. Geometric machine learning has shown that the incorporation of symmetries in models significantly simplifies optimization tasks, leading to faster training and improved performance in a wide range of applications.

In recent years, the combination of quantum computing and geometric machine learning has given rise to a new subfield called geometric quantum machine learning (GQML), which brings symmetries into quantum model architectures. A promising development in this area is the use of Equivariant Quantum Neural Networks (EQNNs), which have shown potential in overcoming challenges unique to quantum computing. One such challenge is the barren plateau problem, which hinders optimization in quantum circuits. EQNNs aim to preserve symmetry while leveraging the power of quantum computing, opening new possibilities for tackling tasks such as image classification and pattern recognition.

This work focuses on embedding roto-reflection symmetries into quantum convolutional neural networks (QCNNs) to create equivariant quantum convolutional neural networks (EQCNNs). These models are specifically designed to be equivariant under geometric transformations, such as 90\({}^{\circ}\) rotations and reflections over both the X and Y axes. By embedding these symmetries directly into the architecture, we aim to improve the model's capacity for recognizing patterns and classifying images more accurately and efficiently while reducing the need for large training datasets compared to models that do not leverage symmetry.

Equivariant models offer several key advantages. First, by reducing the number of parameters needed in the model, they streamline the learning process, making training faster and less computationally demanding. This reduction in parameters also helps prevent overfitting, ensuring thatthe model does not memorize specific details of the training data but instead generalizes well to unseen data. Additionally, because the model is designed to be invariant to certain symmetries in the data, it reduces the number of possible outputs, allowing the model to learn more efficiently even with limited data, and without requiring data augmentation techniques. Another benefit is weight-sharing, which further reduces the number of parameters that need to be optimized, leading to improved computational efficiency.

Despite these advantages, there are important considerations when using equivariant models. A critical challenge is ensuring that the data itself reflects the symmetries incorporated into the model. If the data does not exhibit these symmetries, the model's expressivity will be limited, potentially leading to suboptimal training outcomes. In such scenarios, enforcing equivariance may constrain the model's ability to learn effectively, as it would be restricted to a space that does not align with the true structure of the data. Therefore, it is crucial to ensure alignment between the symmetries embedded in the model and the characteristics of the data being used.

### Quantum Machine Learning

Quantum Machine Learning (QML) is an emerging field at the intersection of quantum computing and machine learning. QML seeks to utilize the unique properties of quantum systems, such as superposition and entanglement, to potentially surpass classical machine learning algorithms in terms of speed and efficiency, particularly on noisy intermediate-scale quantum devices (NISQ).

One of the most widely studied approaches in QML is the Quantum Neural Network (QNN), which is a quantum counterpart to classical neural networks. QNNs are typically implemented using Variational Quantum Algorithms (VQAs), which combine quantum circuits with trainable parameters optimized through classical feedback loops.

The main components of a QNN include the following:

* Data Embedding: A classical input is mapped into a quantum state through a quantum feature map \(\phi:X\to H\), where \(H\) is a Hilbert space and \(x\rightarrow|\phi(x)\rangle\) represents a classical input transformed into a quantum state via a unitary operation \(U_{\phi}(x)\).
* Ansatz (Variational Quantum Circuit): A variational quantum circuit consists of quantum gates with trainable parameters that are adjusted during the training process to optimize the model. Typically, these circuits use rotation gates that apply tunable rotations to qubits.
* Measurement: Once the quantum state is prepared, one or more qubits are measured to obtain the output. The measurement is typically performed with respect to the Pauli-\(Z\) observable, yielding expectation values that contribute to the final prediction.

In a QNN, predictions are obtained by measuring the expectation values of certain observables:

\[y(\mathbf{x})=\langle\psi(\theta,x)|O|\psi(\theta,x)\rangle\] (1)

These hybrid quantum-classical models have demonstrated promising results in various applications, offering a potential solution to quantum machine learning's scalability and trainability issues.

### Equivariant Quantum Neural Networks

An Equivariant Quantum Neural Network (EQNN) is a type of QNN designed to respect the symmetries present in the data. For image classification tasks, incorporating roto-reflection symmetries (such as 90\({}^{\circ}\) rotations and reflections) can reduce the model's complexity by ensuring that the output remains invariant under these transformations.

To build an EQNN, each component of the QNN (data embedding, ansatz, and measurement) must satisfy the symmetry conditions. Specifically, an equivariant embedding transforms classical data into quantum states that reflect the symmetry of the dataset. The ansatz is designed using quantum gates that respect these symmetries, and the measurement is carried out with respect to an invariant observable. The objective is to ensure that the model's output remains unchanged under symmetry transformations, i.e.,

[MISSING_PAGE_FAIL:3]

#### 2.2.2 Equivariant Ansatz

Once the data is embedded, we apply a quantum circuit designed to be equivariant with respect to roto-reflection symmetries. Using the Twirling Method, we identified a set of quantum gates that preserve these symmetries.

Twirling formula. Let \(V_{g}\) be a unitary representation of G. Then,

\[T_{V}[X]=\frac{1}{|G|}\sum_{g\in G}V_{g}XV_{g}^{\dagger}\] (7)

defines a projector onto the set of operators commuting with all elements of the representation, i.e.,

\[[T_{V}[X],V_{g}]=0,\text{ for all }X\text{ and }g\in\text{G}\] (8)

This is the same as \(\mathcal{U}(\theta)V_{g}=V_{g}\mathcal{U}(\theta)\).

Using this formula, we find that the quantum gates that are equivariant are the following:

\[T_{V}=\{Y1Y2,Z1Z2,X1,X2\}.\]

This is the equivariant gateset that ensures that each gate respects the underlying symmetries of the data, reducing the search space during optimization and improving the efficiency of the model.[1]

Using these quantum gates, we define the \(U2\_equiv\) convolutional filter, which serves as the foundation for constructing the equivariant quantum model. To ensure equivariance, we follow the structure outlined in Figure 1, where each yellow block represents a \(U2\_equiv\) convolutional filter. It is important to note that the same filter with identical parameters must be applied across all the qubits, a technique known as weight sharing. This convolutional filter has six trainable parameters, and due to weight sharing, each layer utilizes only these six parameters.

#### 2.2.3 Invariant Observable

Finally, the quantum state is measured by calculating the expectation values of the Pauli-Z observable for each qubit. [3] These measurements are used for image classification, ensuring that the model's output remains invariant under the symmetries considered. The observable satisfies the condition

\[V_{g}^{\dagger}OV_{g}=O\] (9)

which guarantees its invariance under the group \(G\).

## 3 Results

In this work, we trained multiple quantum models utilizing the Mean Square Error (MSE) as the cost function, with the Nesterov optimizer to enhance convergence. [2] The learning rate was set to 0.01, and all models were trained for a total of 200 epochs to ensure sufficient optimization of the parameters with training-test data of 80/20. The experiments were carried out on an Acer Nitro 5

Figure 1: Architecture used to construct left) QCNN and right) EQCNN.

(2020) laptop, equipped with an Intel Core i5 10th generation processor, 12 GB of RAM, and an Nvidia GeForce GTX 1650 Ti graphics card.

For the equivariant quantum model, we implemented a network architecture composed of three quantum convolutional layers, 18 trainable parameters, designed to maintain symmetry properties. For the other quantum models, we experimented with varying numbers of convolutional filters and layer configurations to explore different feature extraction capabilities.

The entire project was developed using the Pennylane framework, which facilitated the integration of quantum circuits with machine learning techniques. All simulations were executed using quantum simulators, which allowed us to test the models in ideal quantum environments.

A GitHub repository with open-source code and detailed instructions for reproducing the project will be made available and linked here once the work is accepted.

Figure 3: Examples of the symmetry operations that we are considering using up) MNIST and down) Fashion-MNIST datasets.

Figure 2: Quantum convolutional filters used to build the equivariant and no-equivariant models.

## 4 Conclusions

By embedding roto-reflection symmetries into our EQCNN, we achieve a more efficient model for image classification and it can take advantage of the NISQ quantum computers era. This approach reduces the parameter space, making the model more data-efficient and improving generalization. We show the effectiveness of our model on benchmark datasets such as MNIST and Fashion-MNIST, demonstrating its potential for applications in quantum machine learning with classical data.

Despite the advances presented, our approach has certain limitations. First, the proposed equivariant quantum model is particularly effective for datasets that exhibit specific symmetries, such as roto-reflections. Its effectiveness may be reduced for datasets that do not display such symmetries. Additionally, as the complexity of the dataset increases or very large datasets are used, scaling the equivariant quantum model becomes more challenging due to the nature of the equivariant ansatz. This limitation may impact the efficiency and performance of the model in broader practical applications.

## References

* (1)
* Chang et al. (2023) Chang, S.Y., Grossi, M., Le Saux, B., Vallecorsa, S. (2023). Approximately equivariant quantum neural network for p4m group symmetries in images. In 2023 IEEE International Conference on Quantum Computing and Engineering (QCE), pp. 229-235. https://doi.org/10.1109/QCE57702.2023.00033.
* Hur et al. (2022) 2. Hur, T., Kim, L., Park, D.K. (2022). Quantum convolutional neural network for classical data classification. Quantum Machine Intelligence, 4(1), 3. https://doi.org/10.1007/s42484-021-00061-x.
* West and Sevior (2023) 3. West, M.T., Sevior, M., Usman, M. (2023). Reflection equivariant quantum neural networks for enhanced image classification. Machine Learning: Science and Technology, 4(3), 035027. https://doi.org/10.1088/2632-2153/acf096.
* Nguyen et al. (2024) 4. Nguyen, Q.T., Schatzki, L., Braccia, P., Ragone, M., Coles, P.J., Sauvage, F., Larocca, M., Cerezo, M. (2024). Theory for equivariant quantum neural networks. PRX Quantum, 5(2), 020328. https://doi.org/10.1103/PRXQuantum.5.020328.
* Meyer et al. (2023) 5. Meyer, J.J., Mularski, M., Gil-Fuster, E., Mele, A.A., Arzani, F., Wilms, A., Eisert, J. (2023). Exploiting symmetry in variational quantum machine learning. PRX Quantum, 4(1), 010328. https://doi.org/10.1103/PRXQuantum.4.010328.
* Cong et al. (2019) 6. Cong, I., Choi, S., Lukin, M.D. (2019). Quantum convolutional neural networks. Nature Physics, 15(12), 1273-1278. https://doi.org/10.1038/s41567-019-0648-8.
* Das and Caruso (2024) 7. Das, S., Caruso, F. (2024). Permutation-equivariant quantum convolutional neural networks. arXiv, April 28, 2024. http://arxiv.org/abs/2404.18198.

Figure 4: Loss plot comparison among different quantum models. Left) using MNIST. Right) Using Fashion-MNIST.