# Replicability in Learning: Geometric Partitions and Sperner-KKM Lemma

Jason Vander Woude

Sandia National Laboratories

jasonvwoude@gmail.com &Peter Dixon

University of Toronto, Mississauga

tooplark@gmail.com &A. Pavan

Iowa State University

pavan@cs.iastate.edu &Jamie Radcliffe

University of Nebraska-Lincoln

jamie.radcliffe@unl.edu &N. V. Vinodchandran

University of Nebraska-Lincoln

vinod@unl.edu

###### Abstract

This paper studies replicability in machine learning tasks from a geometric viewpoint. Recent works have revealed the role of geometric partitions and Sperner's lemma (and its variations) in designing replicable learning algorithms and in establishing impossibility results.

A partition \(\mathcal{P}\) of \(\mathbb{R}^{d}\) is called a \((k,\varepsilon)\)-secluded partition if for every \(\vec{p}\in\mathbb{R}^{d}\), an \(\varepsilon\)-radius ball (with respect to the \(\ell_{\infty}\) norm) centered at \(\vec{p}\) intersects at most \(k\) members of \(\mathcal{P}\). In relation to replicable learning, the parameter \(k\) is closely related to the _list complexity_, and the parameter \(\varepsilon\) is related to the sample complexity of the replicable learner. Construction of secluded partitions with better parameters (small \(k\) and large \(\varepsilon\)) will lead to replicable learning algorithms with small list and sample complexities.

Motivated by this connection, we undertake a comprehensive study of secluded partitions and establish near-optimal relationships between \(k\) and \(\varepsilon\).

1. We show that for any \((k,\varepsilon)\)-secluded partition where each member has at most unit measure, it must be that \(k\geq(1+2\varepsilon)^{d}\), and consequently, for the interesting regime \(k\in[2^{d}]\) it must be that \(\varepsilon\leq\frac{\log_{d}(k)}{d}\).
2. To complement this upper bound on \(\varepsilon\), we show that for each \(d\in\mathbb{N}\) and for each viable \(k\in[2^{d}]\), a construction of a \((k,\varepsilon)\)-secluded (unit cube) partition with \(\varepsilon\geq\frac{\log_{d}(k)}{d}\cdot\frac{1}{8\log_{d}(d+1)}\). This establishes the optimality of \(\varepsilon\) within a logarithmic factor.
3. Finally, we adapt our proof techniques to obtain a new "neighborhood" variant of the cubical KKM lemma (or cubical Sperner's lemma): For any coloring of \([0,1]^{d}\) in which no color is used on opposing faces, it holds for each \(\varepsilon\in(0,\frac{1}{2}]\) that there is a point where the open \(\varepsilon\)-radius \(\ell_{\infty}\)-ball intersects at least \((1+\frac{2}{3}\varepsilon)^{d}\) colors. While the classical Sperner/KKM lemma guarantees the existence of a point that is "adjacent" to points with \((d+1)\) distinct colors, the neighborhood version guarantees the existence of a small neighborhood with exponentially many points with distinct colors.

## 1 Introduction

Can we design learning algorithms that are replicable? Typically, learning algorithms observe samples from an unknown distribution and output a hypothesis. Since different runs of a learning algorithmmay observe different samples, the algorithm may output different hypotheses on different runs, making standard learning algorithms _non-replicable_. Several recent works have been investigating various notions of replicability in learning algorithms. Intuitively, a replicable learning algorithm should output the same canonical hypothesis, on multiple runs (with high probability). However, it has been observed that such an ideal notion of replicability may not be achievable even in simple learning tasks such as learning one-dimensional thresholds. This led to relaxed definitions of replicability including \(\rho\)-replicability [31], stability [10], global-stability [24], and list-replicability [17].

A significant insight that emerged form these works is the profound connection between geometry and algorithmic replicability [17; 13; 31; 12]. These works use geometric partitions to design replicable algorithms and employ (variants of) Sperner/KKM lemma, including Poincare-Miranda and Borsuk-Ulam theorems, to obtain lower bound results. In particular, in [17], the authors use the notion of _secluded partitions_ of \(\mathbb{R}^{d}\)[44] to design replicable learning algorithms with small list complexity. The works of [13; 12; 17] used Sperner/KKM, Poincare-Miranda and Borsuk-Ulam theorems to obtain lower bounds on list complexity and stability parameters for various learning tasks.

Motivated by these connections we undertake an in-depth investigation into _secluded partitions_. Our first contribution is a comprehensive understanding of the optimality of secluded partition constructions. Our second contribution is the discovery of a new neighborhood variant of the Sperner/KKM lemma.

Secluded partitions have found applications beyond replicable learning including deterministic rounding, pseudodeterministic algorithms, and quantum computaion [44; 7]. Moreover, the notion of secluded partitions is simple and natural and is rooted in the works of Lebesgue and Brouwer [36; 8]. Thus, our investigation of secluded partitions should be seen as a fundamental endeavor.

The applicability of the Sperner/KKM lemma, and other equivalent results such as Brouwer's fixed point theorem, the Poincare-Miranda theorem, and the Lebesgue covering theorem is not just limited to the area of replicable algorithms. These lemmas and their variants have found numerous applications in various contexts: distributed and parallel computing [2; 4; 39; 28; 42; 6], communication complexity [21; 11], computational complexity [25; 35; 40; 16], algorithmic game theory [14; 15], and fair-division [37; 38; 5]. We expect that the neighborhood variant we established will also be useful in computer science applications.

## 2 Preliminaries

A learning algorithm is _\(k\)-list replicable_ if the output of the learning algorithm belongs to a list \(\mathcal{L}\) consisting of at most \(k\) good hypotheses with high probability. Below is a more formal definition from [17].

Let \(\mathcal{X}\) be a domain over which a family of distributions \(\mathcal{D}\) are defined, let \(\mathcal{H}\) be a set (representing hypotheses), and \(\mbox{\em err}:\mathcal{D}\times\mathcal{H}\rightarrow[0,\infty)\) be an error function. A learning algorithm on input \(\varepsilon,\delta\) observes \(m\) samples from a distribution \(D\in\mathcal{D}\) and learns a hypothesis \(h\in\mathcal{H}\) with a small error \(\mbox{\em err}(D,h)\leq\varepsilon\).

**Definition 2.1** (List Replicability).: _Let \(k\in\mathbb{N}\), \(\varepsilon\in(0,\infty)\), and \(\delta\in[0,1]\). A learning algorithm \(A\) is \((k,\varepsilon,\delta)\)-list replicable if there exists \(n\in\mathbb{N}\) such that for every \(D\in\mathcal{D}\), there exists a list \(L\subseteq\mathcal{H}\) of size at most \(k\) such that for all \(h\in L\), \(\mbox{\em err}(D,h)\leq\varepsilon\), and_

\[\Pr_{s\sim D^{n}}[A(s,\varepsilon,\delta)\in L]\geq 1-\delta.\]

_For \(k\in\mathbb{N}\), we call \(A\)\(k\)-list replicable if for all \(\varepsilon\in(0,\infty)\) and \(\delta\in(0,1]\), \(A\) is \((k,\varepsilon,\delta)\)-list replicable. We say that \(n\) is the sample complexity of \(A\) and \(k\) is the list complexity of \(A\)._

The above definition captures the ideas over multiple runs of the learning algorithm, we may see at most \(k\) different hypotheses. Note that the ideal scenario is when \(k=1\). A generic goal is to design list replicable algorithms with small list and sample complexities. The work of [17] designed list-replicable algorithms for various learning tasks including a general theorem that any concept class that is learnable with \(k\) non-adaptive statistical queries has a \(k+1\)-list replicable algorithm.

A key ingredient in their list replicable algorithms is the geometric notion of secluded partitions that we define next. Given a point \(\vec{p}\in\mathbb{R}^{d}\), let \(\overline{B}_{\infty}(\varepsilon,\vec{p})\) (\(B^{\circ}_{\infty}(\varepsilon,\vec{p})\)) denote the closed (respectively, open) \(\varepsilon\)-ball around \(\vec{p}\) in the \(\ell_{\infty}\) norm.

**Definition 2.2** ([44]).: _A partition \(\mathcal{P}\) of \(\mathbb{R}^{d}\) is called a \((k,\varepsilon)\)-secluded partition if for every \(\vec{p}\in\mathbb{R}^{d}\), the ball \(\overline{B}_{\infty}(\varepsilon,\vec{p})\) intersects at most \(k\) members of \(\mathcal{P}\). The parameters \(k\) and \(\varepsilon\) are called the degree and tolerance respectively._

**Remark.** To avoid trivial partitions where each point is a partition member or the entire \(\mathbb{R}^{d}\) is a single partition member, all the partitions considered in this work have non-zero, bounded measure partition members.

It is easy to see that the standard grid partition of \(\mathbb{R}^{d}\) with unit cubes is \((2^{d},\frac{1}{2})\)-secluded. The following result [44, 30] improves the degree parameter substantially.

**Theorem 2.3**.: _There is a \((d+1,\frac{1}{2d})\)-secluded partition where each partition member is a unit cube._

Sperner/KKM Lemma, Poincare-Miranda Theorem, and Borsuk-Ulam Theorem can be viewed as _fixed point theorems_ and are known to be equivalent to each other (in the sense that any of these theorems can be derived from the other theorem). We state the Sperner/KKM Lemma below. First we introduce the necessary definitions and notation.

Recall that a \(d\)-dimensional cube is the set \([0,1]^{d}\). For a set of colors \(C\), a coloring is a mapping \(\chi:[0,1]^{d}\to C\). For a color \(c\in C\), let \(X_{c}\) denote the set of points assigned color \(c\) by \(\chi\). That is, \(X_{c}=\chi^{-1}(c)\). _A coloring \(\chi\) is a Sperner/KKM coloring if no two points from opposite faces of the cube gets the same color._ That is, for every \(c\in C\) the set \(X_{c}\) has the property that for each coordinate \(i\in[d]\), the projection \(\pi_{i}(X_{c})=\{x_{i}:\vec{x}\in X_{c}\}\) does not contain both \(0\) and \(1\).

**Theorem 2.4** (Sperner/KKM).: _Given a valid Sperner/KKM coloring of \([0,1]^{d}\) by finitely many colors, there exists a point in the closure of at least \(d+1\) different colors._

## 3 Our Contributions

This section provides an overview of the results established in this work.

**Secluded Partitions and replicability.** The relationship between replicability and partitions can be best abstracted by considering \(d\)-coin bias estimation problem [17]: given \(d\) coins with unknown biases, estimate the biases of each coin within an additive error of \(\nu\). The work of [17] showed that a \((k,\varepsilon)\)-partition of \(\mathbb{R}^{d}\) yields a \(k\)-list replicable algorithm for this task. They used a known construction of \((d+1,\frac{1}{2d})\)-secluded partition of \(\mathbb{R}^{d}\) from Theorem 2.3to obtain a \(d+1\)-list replicable algorithm for this task. However, there is a _cost_ to replicability. The sample complexity of the replicable algorithm blows up by a factor of \(O(d^{2})\) (compared to the sample complexity of the non-replicable algorithm). In general a \((k,\varepsilon)\)-secluded partition gives rise to a \(k\)-list replicable algorithm whose sample complexity blows up by a factor of \(O(\frac{1}{\varepsilon^{2}})\) (compared to the "non-replicable" algorithms).

Thus constructions of secluded partitions with low-degree (\(k\)) and high tolerance (\(\varepsilon\)), will lead to list replicable algorithms with _low list and sample complexities_. It is also known that the degree parameter \(k\) must be at least \(d+1\)[44]. This leads to the following fundamental questions: Can we design secluded partitions that substantially improve the tolerance with little or no degradation of the degree? For example, consider the \((d+1,\frac{1}{2d})\)-secluded partition from Theorem 2.3. Can we improve this and construct a \((d+1,\omega(\frac{1}{d}))\)-secluded partition? Or can we loosen the degree requirement from \(k=d+1\) to \(k\in\text{poly}(d)\) in favor of improving the tolerance from \(\varepsilon\in\Theta(\frac{1}{d})\) to \(\varepsilon\in\omega(\frac{1}{d})\)? Our first contribution is the following upper bound result on the tolerance parameter.

**Theorem 3.1**.: _Let \(d\in\mathbb{N}\), \(\varepsilon\in[0,\infty)\), and \(\mathcal{P}\) a partition of \(\mathbb{R}^{d}\) such that every member has outer Lebesgue measure at most \(1\). Then there exists some \(\vec{p}\in\mathbb{R}^{d}\) such that \(B^{\circ}_{\infty}(\varepsilon,\vec{p})\) intersects at least \((1+2\varepsilon)^{d}\) members of \(\mathcal{P}\). Thus, if \(\mathcal{P}\) is a \((k,\varepsilon)\)-secluded partition, then \(k\geq(1+2\varepsilon)^{d}\). Consequently, if \(k\leq 2^{d}\), then it must be that \(\varepsilon\leq\frac{\log_{4}(k)}{d}\)._

This result shows that even if one relaxes \(k\in\text{poly}(d)\), \(\varepsilon\in O(\frac{\log d}{d})\). This shows that the \((d+1,\frac{1}{2d})\)-secluded partition construction is near optimal in terms of the tolerance parameter, for degree \(d+1\). Stated in terms of replicability, this result implies one can not hope to design list-replicable algorithms with improved sample complexity using a secluded partitions approach.

Our second result is a construction of secluded partitions for various choices of the degree parameter \(k\) with tolerance parameter \(\varepsilon\) almost matching the bound from Theorem 3.1. Until this work, we knewof secluded partitions for only two choices of the degree parameter \(k\): the standard grid partition of \(\mathbb{R}^{d}\) that is \((2^{d},\frac{1}{2})\)-secluded and the \((d+1,\frac{1}{2d})\) partition from Theorem 2.3. We did not know secluded partition constructions for other choices of \(k\). Our second main contribution is the construction of near-optimal secluded partitions for all choices of \(k\).

**Theorem 3.2**.: _Let \(d\in\mathbb{N}\) and \(k\in[2^{d}]\setminus[d]\). Then there exists a \((k,\varepsilon)\)-secluded unit cube partition of \(\mathbb{R}^{d}\) with \(\varepsilon\geq\frac{\log_{4}(k)}{8d\log_{4}(d+1)}\)._

Note that by Theorem 3.1, \(\varepsilon\leq\frac{\log_{4}k}{d}\) and the above construction achieves \(\varepsilon\geq\frac{\log_{4}(k)}{8d\log_{4}(d+1)}\). Thus this construction is optimal up to log factors. As a corollary of this construction, we obtain a smooth tradeoff between list and sample complexities for the problem of estimating the bias of \(d\)-coins

**Corollary 3.3**.: _For the \(d\)-coin bias estimation problem, there exists a \(k\)-list replicable algorithm with sample complexity \(\tilde{O}\big{(}\frac{d^{2}\log^{2}d}{\nu^{2}\log^{2}k}\big{)}\), for any \(k\in[2^{d}]\setminus[d]\), per coin._

For example, if we allow \(k=2^{\sqrt{d}}\), then the sample complexity is \(\tilde{O}(\frac{d}{\nu^{2}})\), per coin.

Sperner/KKM Lemma and replicability.While the geometric tool of secluded partitions has been used to design list replicable algorithms, interestingly works that establish lower bounds on the list complexity of replicable algorithms [17; 13; 12] employ geometric/topological tools such as Sperner/KKM Lemma, Poincare-Miranda Theorem, and Borsuk-Ulam Theorem. For example, the work [17] used Sperner/KKM lemma to establish a lower bound of \(d+1\) on the list complexity for the problem of estimating the bias of \(d\) coins as well as for the \(d\)-dimensional threshold learning problem. The work of [13] used Poincare-Miranda Theorem to establish a lower bound on the list complexity of classes with VC-Dimension \(d\). As our third contribution, we generalize the Sperner/KKM Lemma and obtain a neighborhood variant.

**Theorem 3.4** (Neighborhood Sperner/KKM Lemma).: _Given a Sperner/KKM coloring of \([0,1]^{d}\), for any \(\varepsilon\in(0,\frac{1}{2}]\), there exists a point \(\vec{p}\in[0,1]^{d}\) such that \(B_{\infty}^{\circ}\left(\varepsilon,\vec{p}\right)\) contains at least \(\left(1+\frac{2}{3}\varepsilon\right)^{d}\) points with distinct colors._

Sperner/KKM Lemma(Lemma 2.4) states that in valid coloring of the \(d\)-dimensional hypercube, there is a point \(\vec{p}\) whose closure has at least \(d+1\) colors. That is, for every \(\varepsilon>0\), the \(\varepsilon\)-ball around \(\vec{p}\) intersects at least \(d+1\) colors. Our theorem is a quantitative generalization of this result. It states that the \(\varepsilon\)-ball around \(\vec{p}\) intersects exponentially many colors--at least \((1+\frac{2}{3}\varepsilon)^{d}\) many colors.

## 4 Related Work

One of the first works that studied replicability in the context of learning algorithms is the seminal work of Bun, Livny, and Moran [10]; they used the term _global stability_ to capture this notion. A learning algorithm \(A\) to be \((n,\eta)\)-_globally stable_ with respect to a distribution \(D\) if there is a hypothesis \(h\) such that \(\Pr_{S\sim D^{n}}(A(S)=h)\geq\eta\), here \(\eta\) is called the _stability parameter_. They used the notion of stability, combined with the work of [3], to obtain the equivalence between online learnability and differentially private PAC learnability. Ghazi, Kumar and Manurangsi [24] generalized the notion of stability to pseudo-global stability and list-global stability. Impagliazzo, Lei, Pitassi, and Sorrell [31] introduced the notion of \(\rho\)-replicability and designed replicable algorithms for various learning tasks. One of their replicable algorithms uses a partition/tiling known as "foams tiling" [34]. Dixon, Pavan, Vander Woude and Vinodchandran [17] studied the notions of _list replicability_ and _certificate replicability_ as a measure of the degree of (non)-replicability. Chase, Moran and Yehudayof [13] related the notions list complexity and stability. They established that a learning task has list complexity \(k\) if and only if its stability parameter is \(1/k\). They also established lower bounds the list-complexity (upper bound on the stability) on the PAC-learnability of classes with bounded VC-dimension. In [12], the authors use Borsuk-Ulam theorem to establish impossibility results for replicable agnostic PAC learning.

In the context of randomized algorithms, the notion of replicability is studied under the terminology _pseudodeterminism_. This notion was introduced by Gat and Goldwasser [23] and has been extended to notions called multi-pseudodeterminism [26] and influential-bit algorithms [27]. The study of replicability in the context of learning has been receiving growing attention over the past few yearsand researchers have been investigating this notion under various scenarios. The notion of replicability in the context of stochastic bandits and reinforcement learning has been studied in [19; 33; 18], and the work of [1] studies replicability for optimization problems. Other very recent works include those reported in [9; 29; 32]. A notion that is related to replicability is that of reproducibility. The article by [41] distinguishes these two notions.

## 5 Proof Sketches of Main Results

The main technical tools that we use come from measure theory and the geometry of numbers and include generalized Brunn-Minkowski Inequality and ideas from the standard proof of Blichfeldt's theorem. The generalized Brunn-Minkowski inequality gives a lower bound on the measure of a Minkowski sum of sets (\(A+B=\left\{\vec{a}+\vec{b}\colon\vec{a}\in A,\vec{b}\in B\right\}\)) based on the measures of those sets. We use the following version of the statement from [22, Equation 11]. Here \(m(A)\) is the Lebesgue measure of a set \(A\subseteq\mathbb{R}^{d}\).

**Theorem 5.1** (Generalized Brunn-Minkowski Inequality).: _Let \(d\in\mathbb{N}\) and \(A,B\subseteq\mathbb{R}^{d}\) be Lebesgue measurable such that \(A+B\) is also Lebesgue measurable. Then_

\[m(A+B)\geq\left[m(A)^{\frac{1}{d}}+m(B)^{\frac{1}{d}}\right]^{d}.\]

A common technique in the proof of Blichfeldt's theorem is to use an averaging argument to show that if a set \(A\) is covered by a large family of other sets, then some point in \(A\) is covered many times.

### Proof Sketch of Theorem 3.1

We present the high-level ideas and the intuition behind the proof of Theorem 3.1. In the appendix, we provide a complete proof. Figure 1 serves as a visual.

The goal is to find some point \(\vec{p}\in\mathbb{R}^{d}\) such that \(B^{\circ}_{\infty}(\varepsilon,\vec{p})\) intersects at least \((1+2\varepsilon)^{d}\) members of the partition. Instead of directly trying to establish this, we take a critical change of perspective: for any \(\vec{p}\in\mathbb{R}^{d}\) and \(X\in\mathcal{P}\) (or really any \(X\subseteq\mathbb{R}^{d}\)), it holds that \(B^{\circ}_{\infty}(\varepsilon,\vec{p})\cap X\neq\emptyset\) if and only if \(\vec{p}\in\bigcup_{\vec{x}\in X}B^{\circ}_{\infty}(\varepsilon,\vec{x})\). Thus, what we do is to "replace" every member \(X\) of the partition with the enlarged set \(\bigcup_{\vec{x}\in X}B^{\circ}_{\infty}(\varepsilon,\vec{x})\) and try to find a point \(\vec{p}\) that belongs to at least \((1+2\varepsilon)^{d}\) of these enlarged sets. To achieve this, we take inspiration from a common proof of Blichfeldt's theorem--specifically, the following result which says that if we have a collection of sets \(A_{1},A_{2},A_{3},\ldots\) which are subsets of another set \(S\), then there is a point in \(S\) occurring in multiple \(A_{i}\)s provided together the \(A_{i}\)s have enough volume/measure. We can in fact give a lower bound on the number of \(A_{i}\)s to which such a point belongs to. The following is the formal claim of this known result.

**Proposition 5.1** (Continuous Multi-Pigeonhole Principle).: _Let \(d\in\mathbb{N}\) and \(S\subset\mathbb{R}^{d}\) be bounded and measurable. Let \(\mathcal{A}\) be a family of measurable subsets of \(S\), and let \(k=\left\lceil\frac{\sum_{A\subseteq A}m(A)}{m(S)}\right\rceil\). Then if \(k<\infty\), there exists \(\vec{p}\in S\) such that \(\vec{p}\) belongs to at least \(k\) members of \(\mathcal{A}\). (And if \(k=\infty\), then for any \(n\in\mathbb{N}\) there exists \(\vec{p}^{(n)}\in S\) such that \(\vec{p}^{(n)}\) belongs to at least \(n\) members of \(\mathcal{A}\).)_

There is an immediate issue we have to deal with to be able to use Proposition 5.1 for our application. We would like to take \(\mathcal{A}\) to be the indexed collection of enlarged partition members: \(\mathcal{A}=\left\{\bigcup_{\vec{x}\in X}B^{\circ}_{\infty}(\varepsilon,\vec{ x})\right\}_{X\in\mathcal{P}}\), but then all we know is that each of these is a subset of \(S=\mathbb{R}^{d}\) which is not bounded. This is a simple enough issue to deal with using a standard measure theory technique of considering instead a sequence \(S_{1},S_{2},S_{3},\ldots\) of sets which _are_ bounded and get larger and larger so that \(\bigcup_{n=1}^{\infty}S_{n}=\mathbb{R}^{d}\); we work with each of these sets individually and then try to use a limiting argument to pass the result back to \(S=\mathbb{R}^{d}\). Specifically, we will take \(S_{n}=[-n,n]^{d}\) as illustrated in the first two panes of Figure 1. The third pane of Figure 1 illustrates that we will specifically consider the partition of \(S_{n}\) induced by \(\mathcal{P}\) which we denote by \(\mathcal{S}_{n}\). That is, the induced partition \(\mathcal{S}_{n}\) is the set \(\left\{X\cap S_{n}\colon X\in\mathcal{P}\text{ and }X\cap S_{n}\neq\emptyset\right\}\). Then for each \(S_{n}\) we consider a collection \(\mathcal{A}_{n}\) of the enlarged members of the induced partition: \(\mathcal{A}_{n}=\left\{A_{Y}\right\}_{Y\in\mathcal{S}_{n}}\) where \(A_{Y}\stackrel{{\mathrm{def}}}{{=}}\bigcup_{\vec{y}\in Y}B^{\circ }_{\infty}(\varepsilon,\vec{y})\). Note that each \(A_{Y}\) is a subset of \(S^{\prime}_{n}\stackrel{{\mathrm{def}}}{{=}}[-(n+\varepsilon),n+ \varepsilon]^{d}\) as in the fourth pane of Figure 1.

However, there remains one other issue to deal with to utilize Proposition 5.1--for each \(n\), we have to have some lower bound on the expression \(\left\lceil\frac{\sum_{A_{Y}\in\mathcal{A}_{n}}m(A_{Y})}{m(S_{n}^{\prime})}\right\rceil\). We know that \(m(S_{n}^{\prime})=(2(n+\varepsilon))^{d}\), and using the fact that \(\mathcal{S}_{n}\) is a partition of \(S_{n}=[-n,n]^{d}\), we have the following if \(\mathcal{S}_{n}\) is countable1 (meaning finite or countably infinite):

Footnote 1: If \(\mathcal{S}_{n}\) is uncountable, one of the equalities in the chain becomes the wrong inequality as we get that \(\sum_{Y\in\mathcal{S}_{n}}m(Y)\leq m\left(\bigsqcup_{Y\in\mathcal{S}_{n}}Y\right)\) by B.2 in B.

\[\sum_{A_{Y}\in\mathcal{A}_{n}}m(A_{Y})\geq\sum_{Y\in\mathcal{S}_{n}}m(Y)=m \left(\bigsqcup_{Y\in\mathcal{S}_{n}}Y\right)=m(S_{n}),\]

but this is not nearly good enough, because it just gives

\[\left\lceil\frac{\sum_{A_{Y}\in\mathcal{A}_{n}}m(A)}{m(S_{n}^{\prime})}\right \rceil\geq\left\lceil\frac{m(S_{n})}{m(S_{n}^{\prime})}\right\rceil\geq\left \lceil\frac{(2n)^{d}}{(2(n+\varepsilon))^{d}}\right\rceil=\left\lceil\left( \frac{n}{n+\varepsilon}\right)^{d}\right\rceil=1\]

whereas we want it \(\geq(1+2\varepsilon)^{d}\). Basically, this lower bound is not good because we did not account for the fact that each \(A_{Y}\in\mathcal{A}_{n}\) is enlarged from \(Y\in\mathcal{S}_{n}\). Thus, we want some way to give for each \(Y\in\mathcal{S}_{n}\), a lower bound on the measure of the enlarged set \(A_{Y}\). One might observe that enlarging with an \(\varepsilon\)-ball looks something like scaling by a factor of \(1+\varepsilon\), and since the Lebesgue measure (i.e. typical notion of volume/measure in \(\mathbb{R}^{d}\)) has the property that scaling by \((1+\varepsilon)\) increases the measure by a factor of \((1+\varepsilon)^{d}\), we might be able to show that the enlarged version of each member increases by a factor of \((1+\varepsilon)^{d}\) (which is basically what we are looking to get).

Figure 1: Pane 1: A partition of \(\mathbb{R}^{2}\). Pane 2: We consider only members of the partition which intersect \([-n,n]^{2}\). Pane 3: The partition that \(\mathcal{P}\) induces on \([-n,n]^{2}\). Pane 4: We enlarge each member by placing an \(\varepsilon\)-ball at each point of the member. These enlarged elements are still contained within \([-(n+\varepsilon),n+\varepsilon]^{2}\). Pane 5: The sum of areas of the expanded members is “significantly” more than the area of \([-(n+\varepsilon),n+\varepsilon]^{2}\).

This intuition holds, though the actual reason is not related to scaling, and is dependent on the members having measure at most \(1\). Rather, we use a specialized adaption of Theorem 5.1 to show that

\[m(A_{Y})\geq m(Y)\cdot(1+2\varepsilon)^{d} \tag{1}\]

holds. Now that we have dealt with both issues that arise with trying to apply Proposition 5.1, we can consider a fixed \(n\) and can continue. We proceed in two cases: (1) the interesting case in which \(\mathcal{S}_{n}\) has only countably many members, and (2) the nearly trivial case in which the partition \(\mathcal{S}_{n}\) contains uncountably many members. In case (1) we have

\[\left\lceil\frac{\sum_{A_{Y}\in\mathcal{A}_{n}}m(A_{Y})}{m(S_{n}^ {\prime})}\right\rceil =\left\lceil\frac{\sum_{Y\in\mathcal{S}_{n}}m(A_{Y})}{m(S_{n}^{ \prime})}\right\rceil\] (Re-index) \[\geq\left\lceil\frac{\sum_{Y\in\mathcal{S}_{n}}\left[m(Y)\cdot(1 +2\varepsilon)^{d}\right]}{m(S_{n}^{\prime})}\right\rceil\] ( Equation 1) \[=\left\lceil\frac{(1+2\varepsilon)^{d}\cdot\sum_{Y\in\mathcal{S} _{n}}m(Y)}{m(S_{n}^{\prime})}\right\rceil\] (Linearity of summation) \[=\left\lceil\frac{(1+2\varepsilon)^{d}\cdot m\left(\bigsqcup_{Y \in\mathcal{S}_{n}}Y\right)}{m(S_{n}^{\prime})}\right\rceil\] (Countable additivity of measures) \[=\left\lceil\frac{(1+2\varepsilon)^{d}\cdot m(S_{n})}{m(S_{n}^{ \prime})}\right\rceil\] ( \[S_{n}=\bigsqcup_{Y\in\mathcal{S}_{n}}Y\] ) \[=\left\lceil(1+2\varepsilon)^{d}\cdot\left(\frac{n}{n+\varepsilon }\right)^{d}\right\rceil\] ( \[\frac{m(S_{n})}{m(S_{n}^{\prime})}=(\frac{n}{n+\varepsilon})^{d}\] as above)

Since (for fixed \(d\) and \(\varepsilon\)) we have \(\lim_{n\to\infty}\left(\frac{n}{n+\varepsilon}\right)^{d}=1\), then \(\lim_{n\to\infty}(1+2\varepsilon)^{d}\cdot\left(\frac{n}{n+\varepsilon}\right) ^{d}=(1+2\varepsilon)^{d}\), so because there is a ceiling involved, we can take \(N\in\mathbb{N}\) to be large enough that \(\left\lceil(1+2\varepsilon)^{d}\cdot\left(\frac{N}{N+\varepsilon}\right)^{d} \right\rceil=\left\lceil(1+2\varepsilon)^{d}\right\rceil\), so by Proposition 5.1, there is a point \(\vec{p}\in S_{n}^{\prime}\) that is contained in at least \((1+2\varepsilon)^{d}\) many sets in \(\mathcal{A}_{N}\), and by our change of perspective, this point \(\vec{p}\) has the property that \(B_{\infty}^{\circ}(\varepsilon,\vec{p})\) intersects at least \((1+2\varepsilon)^{d}\) many members of \(\mathcal{P}\).

In case (2) where some \(\mathcal{S}_{N}\) contains uncountably many members, then we completely ignore the lower bound for \(m(A_{Y})\) in Equation 1 because it might be that lots of members \(Y\) (possibly all of them) have measure \(0\), and so that bound only tells us that \(m(A_{Y})\geq 0\). Instead, we note that \(Y\) is at least non-empty, so contains at least one point \(\vec{y}\), and thus \(A_{Y}\supseteq B_{\infty}^{\circ}(\varepsilon,\vec{y})=\prod_{i=1}^{d}[y_{i}- \varepsilon,y_{i}+\varepsilon]\), and so \(m(A_{Y})\geq(2\varepsilon)^{d}\). Thus, \(\left\lceil\frac{\sum_{A_{Y}\in\mathcal{A}_{N}}m(A_{Y})}{m(S_{N}^{\prime})} \right\rceil=\infty\), so by Proposition 5.1, there is a point \(\vec{p}\in S_{N}^{\prime}\) that is contained in at least \((1+2\varepsilon)^{d}\) many sets in \(\mathcal{A}_{N}\), and by our change of perspective, this point \(\vec{p}\) has the property that \(B_{\infty}^{\circ}(\varepsilon,\vec{p})\) intersects at least \((1+2\varepsilon)^{d}\) many members of \(\mathcal{P}\).

### Proof of Theorem 3.2

The partitions that we construct, to establish Theorem 3.2, are of a very natural form: we build a partition of a large dimension \(d\) space, by splitting up the coordinates into smaller sets, and separately partitioning each set of coordinates. In the end, the smaller partitions will be known partition constructions from Theorem 2.3. We will define the construction very generically. We need the observation that if a partition is \((k,\varepsilon)\)-secluded, then we can _increase_\(k\) to \(k^{\prime}\) and _decrease_\(\varepsilon\) to \(\varepsilon^{\prime}\) and the partition is trivially \((k^{\prime},\varepsilon^{\prime})\)-secluded. We refer to this property as the "monotonicity" property.

**Definition 5.2** (Partition Product).: _Let \(d_{1},\ldots,d_{n}\in\mathbb{N}\) and \(\mathcal{P}_{1},\ldots,\mathcal{P}_{n}\) be partitions of \(\mathbb{R}^{d_{1}},\ldots,\mathbb{R}^{d_{n}}\) respectively. Letting \(d=\sum_{i=1}^{n}d_{n}\), we define the product partition of \(\mathbb{R}^{d}\) as_

\[\prod_{i=1}^{n}\mathcal{P}_{i}\stackrel{{\mathrm{def}}}{{=}} \left\{\prod_{i=1}^{n}X_{i}\colon X_{i}\in\mathcal{P}_{i}\right\}\]_where \(\prod_{i=1}^{n}X_{i}\) is viewed as a subset of \(\mathbb{R}^{d}\)._

We specifically stated that \(\prod_{i=1}^{n}X_{i}\) is viewed as a subset of \(\mathbb{R}^{d}\), because technically it is a subset of \(\prod_{i=1}^{n}\mathbb{R}^{d_{i}}\), but this is naturally isomorphic to \(\mathbb{R}^{d}=\mathbb{R}^{\sum_{i=1}^{n}d_{i}}\). For example, technically, if \(d_{1}=d_{2}=d_{3}=2\), then the elements of \(\prod_{i=1}^{n}\mathbb{R}^{d_{i}}\) are of the form \(\langle\langle x_{1},x_{2}\rangle,\langle x_{3},x_{4}\rangle,\langle x_{5},x_{ 6}\rangle\rangle\), but this is trivially isomorphic to \(\mathbb{R}^{6}\) by instead considering the element as \(\langle x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}\rangle\).

We can now show that if we take a product of partitions, and we have a guarantee for each \(\mathcal{P}_{i}\) that it is \((k_{i},\varepsilon_{i})\)-secluded, then we can guarantee the product partition is \((k,\varepsilon)\)-secluded where \(k\) is the product of the \(k_{i}\)'s and \(\varepsilon\) is the minimum of the \(\varepsilon_{i}\)'s.

**Proposition 5.2** (Product Partition Exclusion Guarantees).: _Let \(n\in\mathbb{N}\). For each index \(i\in[n]\), let \(d_{i},k_{i}\in\mathbb{N}\), \(\varepsilon_{i}\in(0,\infty)\) and \(\mathcal{P}_{i}\) be a \((k_{i},\varepsilon_{i})\)-secluded partition of \(\mathbb{R}^{d_{i}}\). Then the product partition \(\mathcal{P}=\prod_{i=1}^{n}\mathcal{P}_{i}\) is a \((k,\varepsilon)\)-secluded partition of \(\mathbb{R}^{d}\) where \(d=\sum_{i=1}^{n}d_{i}\), and \(k=\prod_{i=1}^{n}k_{i}\), and \(\varepsilon=\min_{i\in[n]}\varepsilon_{i}\)._

Proof Sketch.: The basic idea is that for any point \(\vec{p}\in\mathbb{R}^{d}\), we consider how many members of \(\mathcal{P}\) intersect \(\overline{B}_{\varepsilon}(\vec{p})\). Conceptually we think of \(\vec{p}\) as a sequence \(\langle\vec{p}^{(i)}\rangle_{i=1}^{n}\) of \(n\) points where the \(i\)th point \(\vec{p}^{(i)}\) belongs to \(\mathbb{R}^{d_{i}}\). Because we are working with the \(\ell_{\infty}\) norm (that is the norm used by the definition of secluded), the \(\varepsilon\) ball around \(\vec{p}\) is the product of the \(\varepsilon\) balls around each \(\vec{p}^{(i)}\) which is smaller than the product of \(\varepsilon_{i}\) balls around each \(\vec{p}^{(i)}\) because we chose \(\varepsilon\) as the minimum size. Thus, if the \(\varepsilon\) ball around \(\vec{p}\) intersects a member \(X\) of the partition \(\mathcal{P}\), then conceptually viewing \(X\) as a sequence \(\langle X_{i}\rangle_{i=1}^{n}\) where \(X_{i}\) is a member of \(\mathcal{P}_{i}\), it must be for each \(i\in[n]\) that the \(\varepsilon\) ball around \(\vec{p}^{(i)}\) intersects \(X_{i}\) (and thus so does the \(\varepsilon_{i}\) ball since \(\varepsilon_{i}\geq\varepsilon\)). This means (for each \(i\in[n]\)) that \(X_{i}\) is one of at most \(k_{i}\) members of \(\mathcal{P}_{i}\) because at most \(k_{i}\) members of \(\mathcal{P}_{i}\) intersect the \(\varepsilon_{i}\) ball around \(\vec{p}^{(i)}\) (by definition of \(\mathcal{P}_{i}\) being \((k_{i},\varepsilon_{i})\)-secluded). Thus \(X\) is one of at most \(\prod_{i=1}^{n}k_{i}=k\) members of \(\mathcal{P}\). That is, there are at most \(k\) members of \(\mathcal{P}\) that intersect the \(\varepsilon\) ball around \(\vec{p}\) which is the definition of \(\mathcal{P}\) being \((k,\varepsilon)\)-secluded. 

Utilizing the construction above, we will now take the unit cube partition from Theorem 2.3 for each \(\mathbb{R}^{d_{i}}\) and take their product to obtain a new partition. Since the dimension of each \(d_{i}\) is smaller than the dimension \(d\), this allows us to get a larger value of \(\varepsilon_{i}\) for each partition, and thus a larger value of \(\varepsilon\) for the partition of \(\mathbb{R}^{d}\) than if we had used one of the original partitions. The price we pay for this is that the value of \(k\) also increases.

We establish the following lemma.

**Lemma 5.3**.: _Let \(d\in\mathbb{N}\) and \(d^{\prime}\in[d]\).There exists a \((k,\varepsilon)\)-secluded unit cube partition of \(\mathbb{R}^{d}\) where \(k=(d^{\prime}+1)^{\lceil\frac{d}{d^{\prime}}\rceil}\) and \(\varepsilon=\frac{1}{2d^{\prime}}\)._

Proof.: Let \(n=\lceil\frac{d}{d^{\prime}}\rceil\). By [44], let \(\mathcal{P}^{\prime}\) be a \((d^{\prime}+1,1/2d^{\prime})\)-secluded unit cube partition of \(\mathbb{R}^{d^{\prime}}\). By Proposition 5.2\(\mathcal{P}=\prod_{i=1}^{n}\mathcal{P}^{\prime}\) is a \((k,\varepsilon)\)-secluded unit cube partition of \(\mathbb{R}^{n\cdot d^{\prime}}\) where \(k=(d^{\prime}+1)^{n}\) and \(\varepsilon=\frac{1}{2d^{\prime}}\). Since \(n\cdot d^{\prime}=\lceil\frac{d}{d^{\prime}}\rceil\cdot d^{\prime}\geq d\), this trivially (by ignoring extra coordinates) gives a partition of \(\mathbb{R}^{d}\) with these same properties which completes the proof. 

Finally, we are ready to prove Theorem 3.2.

Proof.: Let \(d^{\prime}\in[d]\) be the minimum integer such that \((d^{\prime}+1)^{\lceil\frac{d}{d^{\prime}}\rceil}\leq k\) and let \(\varepsilon=\frac{1}{2d^{\prime}}\). By Lemma 5.3 and monotonicity, let \(\mathcal{P}\) be a \((k,\varepsilon)\)-secluded unit cube partition of \(\mathbb{R}^{d}\). Now we prove the bound on \(\varepsilon\) in two cases: either \(d^{\prime}=1\) or \(d^{\prime}\geq 2\).

In the case that \(d^{\prime}=1\), then \(\varepsilon=\frac{1}{2}\) and \(k\geq(d^{\prime}+1)^{\lceil\frac{d}{d^{\prime}}\rceil}=2^{d}\) and by hypothesis \(k\leq 2^{d}\), so we have equality and we conclude that

\[\frac{\log_{4}(k)}{4d\log_{4}(d+1)}=\frac{d/2}{4d\log_{4}(d+1)}=\frac{1}{8\log_ {4}(d+1)}\leq\frac{1}{8\cdot\frac{1}{2}}\leq\frac{1}{2}=\varepsilon\]

which proves the bound on \(\varepsilon\) in the first case.

In the other case, we have \(d\geq d^{\prime}\geq 2\). Let \(d^{\prime\prime}=d^{\prime}-1>0\) and \(\delta=\frac{1}{2d^{\prime\prime}}\). Note that

\[\frac{\varepsilon}{\delta}=\frac{2d^{\prime\prime}}{2d^{\prime}}=\frac{d^{\prime }-1}{d^{\prime}}=1-\frac{1}{d^{\prime}}\geq 1-\frac{1}{2}=\frac{1}{2}\]

so \(\varepsilon\geq\frac{1}{2}\delta\). By our choice of \(d^{\prime}\) and because \(d^{\prime\prime}<d^{\prime}\), it must be that \(k\leq(d^{\prime\prime}+1)^{\lceil\frac{d}{d^{\prime\prime}}\rceil}\). Noting that \(\lceil\frac{d}{d^{\prime\prime}}\rceil\leq\frac{d}{d^{\prime\prime}}+1=\frac{d+ d^{\prime\prime}}{d^{\prime\prime}}\leq\frac{2d}{d^{\prime\prime}}\), we have

\[k\leq(d^{\prime\prime}+1)^{\lceil\frac{d}{d^{\prime\prime}}\rceil}\leq(d^{ \prime\prime}+1)^{\frac{2d}{d^{\prime\prime}}}=(d^{\prime\prime}+1)^{4d\delta }\leq(d^{\prime\prime}+1)^{8d\varepsilon}\leq(d+1)^{8d\varepsilon}\]

By taking the logarithm of each side and solving for \(\varepsilon\), we obtain the desired result that \(\varepsilon\geq\frac{\log_{4}(k)}{8d\log_{4}(d+1)}\).

### Proof Discussion of Theorem 3.4

Interestingly, the proof of the neighborhood Sperner/KKM lemma relies on the techniques developed to prove Theorem 3.1: for each color \(c\in C\), let \(X_{c}\) be the set of points that are colored \(c\). We union an \(\varepsilon\)-ball at each point in \(X_{c}\), to obtain an enlarged version \(A_{X_{c}}\) of \(X_{c}\). Now, as before, by the Continuous Pigeonhole Principle (Corollary 5.1 ), there is a point that belongs to many of the enlarged sets (and so the ball located at that point intersects many of the original color sets). However, there are some additional issues that arise on the unit cube that don't arise in \(\mathbb{R}^{d}\).

In the discussion in the proof sketch of Theorem 3.1, the enlarged set was not contained in the original region (denoted \(S_{n}\)) and we needed to consider a larger region (denote \(S^{\prime}_{n}\)) to contain them. In \(\mathbb{R}^{d}\) we could deal with this via a limiting argument so that the ratio of the volume change \(m(S_{n})/m(S^{\prime}_{n})\) tends to \(1\) (i.e. it became negligible when ceilings were involved). If one enlarges every color in a unit cube \([-\frac{1}{2},\frac{1}{2}]^{d}\) in the same way, the measure of each color is guaranteed to increase by a factor of \((1+2\varepsilon)^{d}\) as before, but also the smallest set that contains all of these enlargements is the unit cube \([-\frac{1}{2}-\varepsilon,\frac{1}{2}+\varepsilon]^{d}\) which increased in measure by a factor of \((1+2\varepsilon)^{d}\) compared to the original cube, so nothing has been gained! Obviously, there will be an overlap of the enlargements, but the bounds given by the generalized Brunn-Minkowsi inequality tell us no additional information.

We resolve this by employing a trick of first extending the coloring directly to \([-\frac{1}{2}-\varepsilon,\frac{1}{2}+\varepsilon]^{d}\) in a natural way that ensures each color is bounded away from the boundary so that we can perform an enlargement which is non-uniform (it enlarges only toward one of the \(2^{d}\) orthants) and still have the enlarged color set contained in \([-\frac{1}{2}-\varepsilon,\frac{1}{2}+\varepsilon]^{d}\). This means we end up knowing that each modified color has increased in measure by at least a factor of \((1+\frac{2}{3}\varepsilon)^{d}\) and that the modified containing region has not changed in measure at all.

## 6 Conclusion and Open Questions

This work is a comprehensive study of secluded partitions. Prior to this work, it was known that the \((d+1,O(\frac{1}{d}))\)-secluded partitions of [30, 44] were optimal in regards to the degree parameter (\(k=d+1\)). However, it was unknown if they were optimal in the tolerance parameter \(\varepsilon\) for this choice of \(k\). The present work showed that these constructions are optimal in \(\varepsilon\) up to a logarithmic factors. Furthermore, they remain optimal within a logarithmic factor even if we allow the degree \(k\) to be polynomial in the dimension \(d\). We also constructed secluded partitions, optimal up to logarithmic factors, for all \(k\) between \(d+1\) and \(2^{d}\).

This work raises a few open problems. At first glance, it might seem to complete the study of secluded partitions; however, it only establishes near-matching bounds for the \(\ell_{\infty}\) norm. In Appendix A.8, we present upper bounds on \(\varepsilon\) in terms of \(k\) for \(\ell_{p}\) norms, but no known constructions approach these bounds. Developing near-optimal partitions for other norms and exploring their applications to replicability would be an intriguing direction for future research

In replicable algorithm design, there appears to be a _cost_ to achieve replicability--sample complexity blow-up. This work showed that this blow-up in sample complexity is unavoidable in list replicability if one uses secluded partitions method. Can we establish a generic lower bound on the sample complexity of list replicable learning algorithms?While Theorem 3.4 gives a new "neighborhood" variant of the Sperner/KKM lemma, the color bound of \((1+\frac{2}{3}\varepsilon)^{d}\) is not tight for small \(\varepsilon\) because the standard cubical Sperner/KKM lemma (2.4) shows that even for arbitrarily small \(\varepsilon\), the color bound is at least \(d+1\). A compelling research goal is to improve on Theorem 3.4 so that the standard cubical Sperner/KKM lemma follows as a special case. Finally, finding applications of the neighborhood Sperner/KKM lemma is an interesting research direction.

## 7 Acknowledgements

We thank the anonymous reviewers for their valuable suggestions, which improved the presentation of this paper. Vinodchandran's work is partly supported by NSF grants 2130608, 2342244, and a UNL Grand Challenges Grant. Pavan's work is supported in part by NSF grants 2130536 and 2342245. Jason Vander Woude's contributions were made during his time at University of Nebraska, Lincoln and was partly supported by NSF grant 2130608.

## References

* December 9, 2022_, 2022.
* [2] Dan Alistarh, Faith Ellen, and Joel Rybicki. Wait-free approximate agreement on graphs, March 2021. arXiv:2103.08949 [cs].
* [3] Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite littlestone dimension. In Moses Charikar and Edith Cohen, editors, _Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019_, pages 852-860. ACM, 2019.
* [4] Martin Biely, Peter Robinson, and Ulrich Schmid. Easy Impossibility Proofs for k-Set Agreement in Message Passing Systems, March 2011. arXiv:1103.3671 [cs].
* [5] Vittorio Bilo, Ioannis Caragiannis, Michele Flammini, Ayumi Igarashi, Gianpiero Monaco, Dominik Peters, Cosimo Vinci, and William S. Zwicker. Almost Envy-Free Allocations with Connected Bundles. _Games and Economic Behavior_, 131:197-221, January 2022. arXiv:1808.09406 [cs, econ].
* [6] Elizabeth Borowsky and Eli Gafni. Generalized FLP impossibility result for t-resilient asynchronous computations. In S. Rao Kosaraju, David S. Johnson, and Alok Aggarwal, editors, _Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, May 16-18, 1993, San Diego, CA, USA_, pages 91-100. ACM, 1993.
* [7] Sergey Bravyi, Natalie Parham, and Minh Tran. Identity check problem for shallow quantum circuits, 2024.
* 152, 1913.
* [9] Mark Bun, Marco Gaboardi, Max Hopkins, Russell Impagliazzo, Rex Lei, Toniann Pitassi, Satchit Sivakumar, and Jessica Sorrell. Stability is stable: Connections between replicability, privacy, and adaptive generalization. In Barna Saha and Rocco A. Servedio, editors, _Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June 20-23, 2023_, pages 520-527. ACM, 2023.
* [10] Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. In Sandy Irani, editor, _61st IEEE Annual Symposium on Foundations of Computer Science, FOCS 2020, Durham, NC, USA, November 16-19, 2020_, pages 389-402. IEEE, 2020.

* [11] J. Y. Cai, R. Lipton, L. Longpre, M. Ogihara, K. Regan, and D. Sivakumar. Communication complexity of key agreement on small ranges. In _STACS_, pages 38-49, 1995.
* [12] Zachary Chase, Bogdan Chornomaz, Shay Moran, and Amir Yehudayoff. Local borsuk-ulam, stability, and replicability, 2023.
* [13] Zachary Chase, Shay Moran, and Amir Yehudayoff. Replicability and stability in learning. _CoRR_, abs/2304.03757, 2023.
* [14] Xi Chen and Xiaotie Deng. On the complexity of 2D discrete fixed point problem. _Theor. Comput. Sci._, 410(44):4448-4456, 2009.
* [15] Constantinos Daskalakis, Paul W. Goldberg, and Christos H. Papadimitriou. The complexity of computing a nash equilibrium. _SIAM J. Comput._, 39(1):195-259, 2009.
* 24, 2022_, pages 1552-1565. ACM, 2022.
* [17] Peter Dixon, A. Pavan, Jason Vander Woude, and N. V. Vinodchandran. List and certificate complexities in replicable learning. _CoRR_, abs/2304.02240, 2023.
* 16, 2023_, 2023.
* [19] Hossein Esfandiari, Alkis Kalavasis, Amin Karbasi, Andreas Krause, Vahab Mirrokni, and Grigoris Velegkas. Replicable bandits. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Rigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023.
* [20] Gerald B. Folland. _Real Analysis: Modern Techniques and Their Applications_. John Wiley & Sons, April 1999. Google-Books-ID: N8jVDwAAQBAJ.
* [21] Anat Ganor, Karthik C. S., and Domotor Palvolgyi. On Communication Complexity of Fixed Point Computation, May 2022. arXiv:1909.10958 [cs].
* [22] R. J. Gardner. The Brunn-Minkowski inequality. _Bulletin of the American Mathematical Society_, 39(03):355-406, April 2002.
* [23] E. Gat and S. Goldwasser. Probabilistic search algorithms with unique answers and their cryptographic applications. _Electronic Colloquium on Computational Complexity (ECCC)_, 18:136, 2011.
* [24] Badih Ghazi, Ravi Kumar, and Pasin Manurangsi. User-level differentially private learning via correlated sampling. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 20172-20184, 2021.
* [25] Paul W. Goldberg. The Complexity of the Path-following Solutions of Two-dimensional Sperner/Brouwer Functions, June 2015. arXiv:1506.04882 [cs].
* [26] Oded Goldreich. Multi-pseudodeterministic algorithms. _Electronic Colloquium on Computational Complexity (ECCC)_, 26:12, 2019.
* [27] Ofer Grossman and Yang P. Liu. Reproducibility and pseudo-determinism in log-space. In _Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019_, pages 606-620. SIAM, 2019.
* [28] Maurice Herlihy and Nir Shavit. The topological structure of asynchronous computability. _J. ACM_, 46(6):858-923, 1999.

* [29] Max Hopkins, Russell Impagliazzo, Daniel Kane, Sihan Liu, and Christopher Ye. Replicability in high dimensional statistics. _CoRR_, abs/2406.02628, 2024.
* [30] William M. Hoza and Adam R. Klivans. Preserving Randomness for Adaptive Algorithms. _arXiv:1611.00783 [cs]_, June 2018. arXiv: 1611.00783.
* [31] Russell Impagliazzo, Rex Lei, Toniann Pitassi, and Jessica Sorrell. Reproducibility in Learning. _arXiv:2201.08430 [cs]_, January 2022. arXiv: 2201.08430.
* [32] Alkis Kalavasis, Amin Karbasi, Kasper Green Larsen, Grigoris Velegkas, and Felix Zhou. Replicable learning of large-margin halfspaces. In _Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024_. OpenReview.net, 2024.
* 16, 2023_, 2023.
* [34] Guy Kindler, Ryan O'Donnell, Anup Rao, and Avi Wigderson. Spherical Cubes and Rounding in High Dimensions. In _2008 49th Annual IEEE Symposium on Foundations of Computer Science_, pages 189-198, Philadelphia, PA, USA, October 2008. IEEE.
* [35] Shiva Kintali. Scarf is Ppad-Complete, May 2009. arXiv:0812.1601 [cs].
* [36] Henri Lebesgue. Sur la non-applicabilite de deux domaines appartenant respectivement a des espaces an etn+p dimensions. _Mathematische Annalen_, 70(2):166-168, June 1911.
* [37] Frederic Meunier and Francis Edward Su. Multilabeled versions of Sperner's and Fan's lemmas and applications, May 2019. arXiv:1801.02044 [cs, math].
* [38] Frederic Meunier and Shira Zerbib. Envy-free cake division without assuming the players prefer nonempty pieces, January 2019. arXiv:1804.00449 [cs, math].
* [39] Susumu Nishimura. Proving Unsolvability of Set Agreement Task with Epistemic mu-Calculus, May 2022. arXiv:2205.06452 [cs].
* [40] Christos H. Papadimitriou. On graph-theoretic lemmata and complexity classes (extended abstract). In _31st Annual Symposium on Foundations of Computer Science, St. Louis, Missouri, USA, October 22-24, 1990, Volume II_, pages 794-801. IEEE Computer Society, 1990.
* [41] Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Lariviere, Alina Beygelzimer, Florence d'Alche Buc, Emily Fox, and Hugo Larochelle. Improving reproducibility in machine learning research(a report from the neurips 2019 reproducibility program). _Journal of Machine Learning Research_, 22(164):1-20, 2021.
* [42] Michael E. Saks and Fotios Zaharoglou. Wait-free k-set agreement is impossible: The topology of public knowledge. _SIAM J. Comput._, 29(5):1449-1483, 2000.
* [43] Waclaw Sierpinski. Sur la question de la mesurabilite de la base de M. Hamel. _Fundamenta Mathematicae_, 1:105-111, 1920. Publisher: Instytut Matematyczny Polskiej Akademii Nauk.
* [44] Jason Vander Woude, Peter Dixon, A. Pavan, Jamie Radcliffe, and N. V. Vinodchandran. Geometry of rounding, 2022. 10.48550/arXiv.2211.02694.

## Appendix A Complete Proofs

### Notation

The following is a list of some of the notation we will use in this paper.

* We use \(\mathbb{N}\) to denote the natural numbers starting with \(1\).
* We continue to use \(\overline{B}_{\upharpoonright\upharpoonright\upharpoonright}(\varepsilon,\vec{p})\), \(B^{\circ}_{\upharpoonright\upharpoonright\upharpoonright}(\varepsilon,\vec{p})\), \(\overline{B}_{\infty}(\varepsilon,\vec{p})\), and \(B^{\circ}_{\infty}(\varepsilon,\vec{p})\) as before.
* For two sets \(A,B\subseteq\mathbb{R}^{d}\) we write \(A+B\) to represent the Minkowski sum \(A+B\stackrel{{\mathrm{def}}}{{=}}\left\{\vec{a}+\vec{b}\colon\vec{ a}\in A,\ \vec{b}\in B\right\}\). We also may write \(\vec{a}+B\) to mean \(\left\{\vec{a}+\vec{b}\colon\vec{b}\in B\right\}\) for some fixed vector \(\vec{a}\).
* We will use \(v_{\upharpoonright\upharpoonright\upharpoonright\upharpoonright,d}\) to represent the Borel/Lebesgue measure of the unit radius ball in \(\mathbb{R}^{d}\) with respect to a general norm \(\left\lVert\cdot\right\rVert\). This is a normalization factor that appears in some results.

### Proof Theorem 3.1

In this section, we present a complete proof of 3.1. We begin with some prerequisite results in A.2.1. Then, in A.2.2 we establish a more generic theorem that can handle any norm. Theorem 3.1 follows as immediate corollary this generic result.

#### a.2.1 Prerequisite Results

In this section, we will deal with arbitrary norms of \(\mathbb{R}^{d}\). We point out the well-known fact that all norms on \(\mathbb{R}^{d}\) are equivalent in the sense that they all generate the same topology on \(\mathbb{R}^{d}\). Given two norms \(\left\lVert\cdot\right\rVert^{a}\) and \(\left\lVert\cdot\right\rVert^{b}\) on \(\mathbb{R}^{d}\), there exists fixed constants \(c_{d},C_{d}\in(0,\infty)\) such that for all vectors \(\vec{x}\in\mathbb{R}^{d}\), it holds that \(c_{d}\|\vec{x}\|^{a}\leq\|\vec{x}\|^{b}\leq C_{d}\|\vec{x}\|^{a}\). Thus the collection of open sets in \(\mathbb{R}^{d}\) is the same no matter which norm we are using. This also means that the Borel and Lebesgue \(\sigma\)-algebras on \(\mathbb{R}^{d}\) are the same no matter which norm is used, and thus balls with respect to any norm on \(\mathbb{R}^{d}\) are measurable.

We begin with four simple facts. The first fact will later allow us to pass a result through a limit since the answer will be an integer.

**Fact A.1**.: _For any \(\alpha\in\mathbb{R}\), there exists \(\gamma\in\mathbb{R}\) such that \(\gamma<\alpha\) and \(\left\lceil\gamma\right\rceil=\left\lceil\alpha\right\rceil\)._

The next fact says that the Minkowski sum of a set \(X\) and an open ball at the origin can be viewed as a union of open balls positioned at each point of \(X\).

**Fact A.2**.: _For any normed vector space, given a set \(X\) and \(\varepsilon\in[0,\infty)\), then_

\[X+B^{\circ}_{\upharpoonright\upharpoonright\upharpoonright}(\varepsilon,\vec{0 })=\bigcup_{\vec{x}\in X}B^{\circ}_{\upharpoonright\upharpoonright\upharpoonright \upharpoonright}(\varepsilon,\vec{x}).\]

_The same can be said replacing open balls with closed balls._

The third fact says that we can decompose a ball into a Minkowski sum of two smaller balls.

**Fact A.3**.: _For any normed vector space, and any \(\alpha,\beta\in(0,\infty)\), it holds that \(B^{\circ}_{\upharpoonright\upharpoonright\upharpoonright}(\alpha,\vec{0})+B^{ \circ}_{\upharpoonright\upharpoonright\upharpoonright\upharpoonright}(\beta, \vec{0})=B^{\circ}_{\upharpoonright\upharpoonright\upharpoonright\upharpoonright \upharpoonright}(\alpha+\beta,\vec{0})\)._

The fourth fact, while also very simple, is the key change of perspective that allowed us to prove the main results of this section. It says that if we are checking which sets \(X\) in our partition intersect an \(\varepsilon\)-ball located at \(\vec{p}\) (in order to see how many there are), we can instead enlarge each member of the partition by taking its Minkowski sum with the origin-centered \(\varepsilon\)-ball, and check which of these enlarged members contain the point \(\vec{p}\).

**Fact A.4**.: _For any normed vector space, for any set \(X\), for any vector \(\vec{p}\), and any \(\varepsilon>0\), the following are equivalent:_

1. \(B^{\circ}_{\upharpoonright\upharpoonright\upharpoonright}(\varepsilon,\vec{p}) \cap X\neq\emptyset\)__
2. \(\vec{p}\in X+B^{\circ}_{\upharpoonright\upharpoonright\upharpoonright\upharpoonright }(\varepsilon,\vec{0})\)__Now we introduce the result which is the connection to the above-mentioned key change of perspective. The result says to consider a bounded, (measurable) subset \(S\subseteq\mathbb{R}^{d}\) (so it has finite measure) and a collection \(\mathcal{A}\) of (measurable) subsets of \(S\). If we compute the sum of measures of all members in the collection \(\mathcal{A}\) (i.e. intuitively the total volume that they take up), and compare this to the measure/volume of \(S\), then whatever this ratio is, we can find a point in \(S\) covered by that many members of the collection \(\mathcal{A}\). For example, in the simplest case that the total measure of members of \(\mathcal{A}\) is larger than the measure of \(S\), then there is no way for all of the members of \(\mathcal{A}\) to be disjoint, so there has to be some point covered by two members. This simple case can be viewed as a continuous version of the pigeonhole principle.

In the more generic case, this result should be intuitively true by an averaging argument: if every point of \(S\) is covered only \(n\) times, then the total measure of members in \(\mathcal{A}\) is at most \(n\cdot m(S)\), so if the ratio of total measure in \(\mathcal{A}\) to the measure of \(S\) is large, then \(n\) must also be large. This more general version is a sort of continuous multi-pigeonhole principle.

**Proposition A.1** (Continuous Multi-Pigeonhole Principle).: _Let \(d\in\mathbb{N}\) and \(S\subset\mathbb{R}^{d}\) be bounded and measurable. Let \(\mathcal{A}\) be a family of measurable subsets of \(S\), and let \(k=\left\lceil\frac{\sum_{\mathcal{A}\in\mathcal{A}}m(A)}{m(S)}\right\rceil\). Then if \(k<\infty\), there exists \(\vec{p}\in S\) such that \(\vec{p}\) belongs to at least \(k\) members of \(\mathcal{A}\). (And if \(k=\infty\), then for any \(n\in\mathbb{N}\) there exists \(\vec{p}^{(n)}\in S\) such that \(\vec{p}^{(n)}\) belongs to at least \(n\) members of \(\mathcal{A}\).)_

We first encountered this result as the main ingredient in the standard proof of Blichfeldt's theorem (which was the source of motivation for our main technique), but many of the sources we found where proofs of Blichfeldt's theorem are presented did not prove the result above except in special cases, so for convenience and completion, we provide a proof in B in two parts: B.4, and B.5.

The next ingredient that we need is a way to measure how large the Minkowski sum in Fact A.4 is. In order to utilize Proposition A.1 we need a lower bound on the measures, and we can obtain one using the generalized Brunn-Minkowski inequality stated below.

**Theorem A.5** (Generalized Brunn-Minkowski Inequality).: _Let \(d\in\mathbb{N}\) and \(A,B\subseteq\mathbb{R}^{d}\) be Lebesgue measurable such that \(A+B\) is also Lebesgue measurable. Then_

\[m(A+B)\geq\left[m(A)^{\frac{1}{2}}+m(B)^{\frac{1}{2}}\right]^{d}.\]

This version of the statement can be obtained from [22, Equation 11]; in that survey, Gardner states this theorem with a requirement that the sets be bounded, but in the following paragraph notes that this is not necessary and the requirement is only stated for convenience of the presentation in that survey.

In the theorem, the requirement that \(A+B\) is Lebesgue measurable is not a triviality; Gardner discusses that there exist known Lebesgue measurable sets \(A\) and \(B\) such that the Minkowski sum \(A+B\) is not Lebesgue measurable as shown in [43]. The next result gives us a way to circumvent this issue in our application even if the members of our partition are not measurable by taking \(B\) to be an open set so that the sum \(A+B\) is open (and thus measurable), and using the outer measure of \(A\) so that we don't need the assumption that \(A\) is measurable.

**Lemma A.6**.: _Let \(d\in\mathbb{N}\) and let \(\mathbb{R}^{d}\) be equipped with any norm \(\left\|\cdot\right\|\). Let \(Y\subseteq\mathbb{R}^{d}\), and \(\varepsilon\in(0,\infty)\). Then \(Y+B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon,\vec{0})\) is open (and thus Borel measurable), and \(m(Y+B^{\circ}(\varepsilon,\vec{0}))\geq\left(m_{out}(Y)^{\frac{1}{d}}+ \varepsilon\cdot(v_{\mathbb{I}\cdot\mathbb{I}},d)^{\frac{1}{d}}\right)^{d}\)._

Proof.: By Fact A.2, for any \(\varepsilon^{\prime\prime}\in(0,\infty)\), \(Y+B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon^{\prime\prime},\vec{0})= \bigcup_{\vec{y}\in Y}B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon^{ \prime\prime},\vec{y})\) which is a union of open sets, so is itself open and thus Borel measurable. Now, for any \(\varepsilon^{\prime}\in(0,\varepsilon)\), observe that by Fact A.3, \(B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon,\vec{0})=B^{\circ}_{ \mathbb{I}\cdot\mathbb{I}}(\varepsilon-\varepsilon^{\prime},\vec{0})+B^{\circ} _{\mathbb{I}\cdot\mathbb{I}}(\varepsilon^{\prime},\vec{0})\) and thus, this sum is measurable because it is an open ball. Using this equality and the associativity of the Minkowski sum, we have

\[Y+B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon,\vec{0})=Y+\left[B^{\circ }_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon-\varepsilon^{\prime},\vec{0})+B^{ \circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon^{\prime},\vec{0})\right]= \left[Y+B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon-\varepsilon^{ \prime},\vec{0})\right]+B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon^{ \prime},\vec{0}).\]Thus, we have the following chain of inequalities (each justified after it is stated):

\[m\left(Y+B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon,\vec{0})\right) =m\left(\left[Y+B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon- \varepsilon^{\prime},\vec{0})\right]+B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}( \varepsilon^{\prime},\vec{0})\right)\] (Open, measurable, equality above) \[\geq\left(m\left(Y+B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}( \varepsilon-\varepsilon^{\prime},\vec{0})\right)^{\frac{1}{d}}+m\left(B^{ \circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon^{\prime},\vec{0})\right)^{ \frac{1}{d}}\right)^{d}\]

The above comes from the Theorem A.5 noting that as demonstrated above, terms of the sum \(\left[Y+B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon-\varepsilon^{\prime },\vec{0})\right]\) and \(B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon^{\prime},\vec{0})\) are open and thus measurable, and the same holds for the sum itself \(\left(Y+B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon,\vec{0})\right)\). We continue.

\[\geq\left(m_{out}\left(Y\right)^{\frac{1}{d}}+m\left(B^{\circ}_{\mathbb{I} \cdot\mathbb{I}}(\varepsilon^{\prime},\vec{0})\right)^{\frac{1}{d}}\right)^{d}\]

The above inequality comes from the definition of the outer measure of \(Y\) being the infimum of the measures of all measurable supersets of \(Y\). Since \(Y\subseteq Y+B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon^{\prime},\vec{0})\), we get the inequality above. Continuing, we have the following:

\[=\left(m_{out}\left(Y\right)^{\frac{1}{d}}+m\left(\varepsilon^{ \prime}\cdot B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(1,\vec{0})\right)^{\frac{1} {d}}\right)^{d}\quad\text{(Scaling of norm-based balls)}\] \[=\left(m_{out}\left(Y\right)^{\frac{1}{d}}+\left[(\varepsilon^{ \prime})^{d}\cdot m\left(B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(1,\vec{0}) \right)\right]^{\frac{1}{d}}\right)^{d}\] (Scaling for Lebesgue measure) \[=\left(m_{out}\left(Y\right)^{\frac{1}{d}}+\varepsilon^{\prime}\cdot(v_{ \mathbb{I}\cdot\mathbb{I},d})^{\frac{1}{d}}\right)^{d}\quad\text{(Algebra and }v_{\mathbb{I}\cdot\mathbb{I},d}\overset{\text{def}}{=}m\left(B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(1,\vec{0})\right)\]

Since the inequality above holds for all \(\varepsilon^{\prime}\in(0,\varepsilon)\), it must also hold in the limit (keeping \(d\) and \(Y\) fixed):

\[m\left(Y+B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon,\vec{0})\right) \geq\lim_{\varepsilon^{\prime}\to\varepsilon}\left[\left(m_{out}\left(Y\right)^{ \frac{1}{d}}+\varepsilon^{\prime}\cdot(v_{\mathbb{I}\cdot\mathbb{I},d})^{\frac{1}{d}}\right)^{d}\right]=\left(m_{out}\left(Y\right)^{\frac{1}{d}}+\varepsilon\cdot(v_{\mathbb{I}\cdot\mathbb{I},d})^{\frac{1}{d}}\right)^{d}\]

which concludes the proof. 

At this point, we are nearly in a position to prove the main result of this section, but we need one last result which gives an inequality that we will compose with the Theorem A.5. The result below can be interpreted as saying that for appropriate parameters, we can essentially factor our the "\(x\)" in \((x^{1/d}+\alpha)^{d}\) to get the no larger expression \(x(1+\alpha)^{d}\).

**Lemma A.7**.: _For \(d\in[1,\infty)\), \(x\in[0,1]\), and \(\alpha\in[0,\infty)\), it holds that \((x^{1/d}+\alpha)^{d}\geq x(1+\alpha)^{d}\)._

Proof.: If \(x=0\), then the result is trivial. Otherwise \(x\in(0,1]\), so \(x^{1/d}\in(0,1]\). Because \(\alpha\geq 0\), \(\frac{\alpha}{x^{1/d}}\geq\alpha\), so

\[(x^{1/d}+\alpha)^{d}=\left(x^{1/d}\left(1+\frac{\alpha}{x^{1/d}}\right)\right)^{d}=x \left(1+\frac{\alpha}{x^{1/d}}\right)^{d}\geq x(1+\alpha)^{d}.\]

#### a.2.2 A generic result for all Norms, and Theorem 3.1

We establish Theorem 3.1, by proving the following result.

**Theorem A.8** (\(\varepsilon\)-Neighborhoods for Measure Bounded Partitions and Arbitrary Norm).: _theorem Let \(d\in\mathbb{N}\), \(M\in(0,\infty)\), and \(\mathcal{P}\) a partition of \(\mathbb{R}^{d}\) such that every member has outer Lebesgue measure at most \(M\). Let \(\mathbb{R}^{d}\) be equipped with any norm \(\left\|\cdot\right\|\). For every \(\varepsilon\in(0,\infty)\), there exists \(\vec{p}\in\mathbb{R}^{d}\) such that \(B^{\circ}_{\mathbb{I}\cdot\mathbb{I}}(\varepsilon,\vec{p})\) intersects at least \(k=\left\lceil\left(1+\varepsilon\left(\frac{v_{\mathbb{I}\cdot\mathbb{I},d}}{M} \right)^{1/d}\right)^{d}\right\rceil\) members of the partition where \(v_{\mathbb{I}\cdot\mathbb{I},d}\overset{\text{def}}{=}m\left(B^{\circ}_{ \mathbb{I}\cdot\mathbb{I}}(1,\vec{0})\right)\) is the Lebesgue measure of the \(\left\|\cdot\right\|\) unit ball._Proof.: Throughout the proof, all lengths will be with respect to \(\left\lVert\cdot\right\rVert\), so we will eliminate the clutter by neglecting to use the \(\left\lVert\cdot\right\rVert\) subscript anywhere in the proof. We will also be working in a single dimension \(d\) throughout the proof, so we also drop the \(d\) subscript from \(v\) throughout.

Consider the following for each \(n\in\mathbb{N}\). Let \(S_{n}=B^{\circ}(n,\vec{0})\) and \(S_{n}^{\prime}=B^{\circ}(n+\varepsilon,\vec{0})=S_{n}+B^{\circ}(\varepsilon, \vec{0})\) and \(\mathcal{S}\) be the partition of \(S_{n}\) induced2 by \(\mathcal{P}\). For each \(Y\in\mathcal{S}_{n}\), let \(C_{Y}=Y+B^{\circ}(\varepsilon,\vec{0})\). By Lemma A.6, \(C_{Y}\) is measurable, and \(m(C_{Y})\geq\left(m_{out}\left(Y\right)^{\frac{1}{d}}+\varepsilon\cdot v^{ \frac{1}{d}}\right)^{d}\). Also observe that \(C_{Y}\subseteq S_{n}^{\prime}\). Now consider the following inequalities:

Footnote 2: I.e. \(\mathcal{S}=\{X\cap S_{n}\colon X\in\mathcal{P}\text{ and }X\cap S_{n}\neq \emptyset\}\). That is, \(\mathcal{S}\) is the partition of \(S_{n}\) obtained by intersecting every member of \(\mathcal{P}\) with the new domain \(S_{n}\) and keeping those that have non-empty intersections.

\[m(C_{Y}) \geq\left(m_{out}\left(Y\right)^{\frac{1}{d}}+\varepsilon\cdot v^ {\frac{1}{d}}\right)^{d}\] (Above) \[=\left(M^{1/d}\left[\frac{m_{out}\left(Y\right)^{\frac{1}{d}}}{M^ {1/d}}+\frac{\varepsilon\cdot v^{\frac{1}{d}}}{M^{1/d}}\right]\right)^{d}\] (Algebra) \[=M\left(\left[\frac{m_{out}\left(Y\right)}{M}\right]^{\frac{1}{d}} +\frac{\varepsilon\cdot v^{\frac{1}{d}}}{M^{1/d}}\right)^{d}\] (Algebra) \[\geq M\cdot\frac{m_{out}\left(Y\right)}{M}\cdot\left(1+\frac{ \varepsilon\cdot v^{\frac{1}{d}}}{M^{1/d}}\right)^{d}\] (A.7 since \[\frac{m_{out}\left(Y\right)}{M}\in[0,1]\] ) \[=m_{out}\left(Y\right)\cdot\left(1+\frac{\varepsilon\cdot v^{ \frac{1}{d}}}{M^{1/d}}\right)^{d}\] (Simplify)

Informally, the above shows that for each \(Y\in\mathcal{S}_{n}\), the set \(Y+B^{\circ}(\varepsilon,\vec{0})\) has substantially more (outer) measure than \(Y\) does--specifically a factor of \(\left(1+\frac{\varepsilon\cdot v^{\frac{1}{d}}}{M^{1/d}}\right)^{d}\). We will extend this to unsurprisingly show that this implies that \(\left\{Y+B^{\circ}(\varepsilon,\vec{0})\right\}_{Y\in\mathcal{S}_{n}}\) also has this same factor more (outer) measure than \(\mathcal{S}_{n}\) does, observing that \(\mathcal{S}_{n}\) has total (outer) measure of about \(m(S_{n})\) since \(\mathcal{S}_{n}\) is a partition of \(S_{n}\) (any discrepancy is due to non-measurable members in \(\mathcal{S}_{n}\))

Formally, we claim that there exists a finite subfamily \(\mathcal{F}_{n}\subseteq\mathcal{S}_{n}\) such that

\[\sum_{Y\in\mathcal{F}_{n}}m\left(Y+B^{\circ}(\varepsilon,\vec{0})\right)\geq \left(1+\frac{\varepsilon\cdot v^{\frac{1}{d}}}{M^{1/d}}\right)^{d}\cdot m(S_{ n}).\]

To see this, first consider the case that \(\mathcal{S}_{n}\) has infinite cardinality. Let \(\mathcal{F}_{n}\subset\mathcal{S}_{n}\) be any subfamily of finite cardinality at least \(\left(1+\frac{\varepsilon\cdot v^{\frac{1}{d}}}{M^{1/d}}\right)^{d}\cdot m(S_{ n})\cdot\frac{1}{\varepsilon^{d}v}\). This gives

\[\sum_{Y\in\mathcal{F}_{n}}m\left(Y+B^{\circ}(\varepsilon,\vec{0})\right)\geq \sum_{Y\in\mathcal{F}_{n}}m\left(B^{\circ}(\varepsilon,\vec{0})\right)\]where this inequality is because \(Y+B^{\circ}(\varepsilon,\vec{0})\) is a superset of some translation of \(B^{\circ}(\varepsilon,\vec{0})\) since \(Y\neq\emptyset\). Continuing, we use the standard fact that \(m\left(B^{\circ}(\varepsilon,\vec{0})\right)=m\left(\varepsilon\cdot B^{\circ} (1,\vec{0})\right)=\varepsilon^{d}\cdot m\left(B^{\circ}(1,\vec{0})\right)= \varepsilon^{d}v\):

\[\geq\sum_{Y\in\mathcal{F}_{n}}\varepsilon^{d}v\] \[=\left[\left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/d}} \right)^{d}\cdot m(S_{n})\cdot\frac{1}{\varepsilon^{d}v}\right]\cdot \varepsilon^{d}v\qquad\text{ (Cardinality of $\mathcal{F}_{n}$)}\] \[=\left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/d}}\right) ^{d}\cdot m(S_{n}).\] (Simplify)

Now consider the other (more interesting) case where \(\mathcal{S}_{n}\) has finite cardinality3. Take \(\mathcal{F}_{n}=\mathcal{S}_{n}\) so that

Footnote 3: In fact this case also works if \(\mathcal{S}_{n}\) is countable even though we have already dealt with that case.

\[\sum_{Y\in\mathcal{F}_{n}}m\left(Y+B^{\circ}(\varepsilon,\vec{0})\right) =\sum_{Y\in\mathcal{S}_{n}}m\left(Y+B^{\circ}(\varepsilon,\vec{0} )\right)\] ( \[\mathcal{F}_{n}=\mathcal{S}_{n}\] ) \[\geq\sum_{Y\in\mathcal{S}_{n}}m_{out}\left(Y\right)\cdot\left(1+ \frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/d}}\right)^{d}\] (Above) \[=\left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/d}}\right) ^{d}\cdot\sum_{Y\in\mathcal{S}_{n}}m_{out}\left(Y\right)\qquad\text{ (Linearity of summation)}\] \[\geq\left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/d}}\right) ^{d}\cdot m_{out}\left(\bigcup_{Y\in\mathcal{S}_{n}}Y\right)\]

where the above inequality is due the the countable subaddativity property of outer measures which states that a countable sum of outer measures of sets is at least as large as the outer measure of the union of the sets. In the last step we get

\[=\left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/d}}\right) ^{d}\cdot m\left(S_{n}\right)\qquad\text{ (\begin{subarray}{c}\bigsqcup_{Y\in\mathcal{F}_{n}}Y=S_{n} \text{ is measurable}\end{subarray})}\]

Thus, regardless of whether \(\mathcal{S}_{n}\) has infinite or finite cardinality, there exists a finite subfamily \(\mathcal{F}_{n}\subseteq\mathcal{S}_{n}\) such that

\[\sum_{Y\in\mathcal{F}_{n}}m\left(Y+B^{\circ}(\varepsilon,\vec{0})\right)\geq \left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/d}}\right)^{d}\cdot m(S_{ n}).\]

Fix such a subfamily \(\mathcal{F}_{n}\), and let \(\mathcal{A}_{n}=\left\{Y+B^{\circ}(\varepsilon,\vec{0})\right\}_{Y\in\mathcal{F }_{n}}\) be a family indexed4 by \(\mathcal{F}_{n}\). Note that for each \(A_{Y}\stackrel{{\text{def}}}{{=}}Y+B^{\circ}(\varepsilon,\vec{0} )\in\mathcal{A}_{n}\), since \(Y\subseteq S_{n}=B^{\circ}(n,\vec{0})\), we have \(A_{Y}\subseteq S_{n}+B^{\circ}(\varepsilon,\vec{0})=S^{\prime}_{n}\).

Thus, by B.55, there is a point \(\vec{p}^{(n)}\in S^{\prime}_{n}\) which belongs to at least \(k_{n}\)-many sets in \(\mathcal{A}_{n}\) where

Footnote 5: We are taking \(X\) in B.5 to be \(S^{\prime}_{n}\) in this proof, and taking \(\mu\) to be \(m\) and taking \(\mathcal{A}\) to be \(\mathcal{A}_{n}\). We have that \(\sum_{\mathcal{A}\in\mathcal{A}_{n}}m(A)<\infty\) because \(|\mathcal{A}_{n}|=|\mathcal{F}_{n}|\) is finite, and each \(A\in\mathcal{A}_{n}\) is a subset of \(S^{\prime}_{n}\), so has finite measure.

\[k_{n}\stackrel{{\mathrm{def}}}{{=}}\left\lceil\frac{\sum_{Y\in \mathcal{F}_{n}}m\left(Y+B^{\circ}(\varepsilon,\vec{0})\right)}{m(S^{\prime}_ {n})}\right\rceil \geq\frac{\left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/ d}}\right)^{d}\cdot m\left(S_{n}\right)}{m(S^{\prime}_{n})}\] (Above) \[=\left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/d}}\right)^ {d}\cdot\frac{m\left(B^{\circ}(n,\vec{0})\right)}{m\left(B^{\circ}(n+ \varepsilon,\vec{0})\right)}\] (Def'n of

\[S_{n}\]

 and

\[S^{\prime}_{n}\]

) \[=\left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/d}}\right)^ {d}\cdot\frac{n^{d}\cdot v}{(n+\varepsilon)^{d}\cdot v}\] (Standard scaling fact) \[=\left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/d}}\right)^ {d}\cdot\left(\frac{n}{n+\varepsilon}\right)^{d}.\] (Simplify)

Since \(\vec{p}^{(n)}\) belongs to at least \(k_{n}\)-many sets in \(\mathcal{A}_{n}\), this by definition means that there are at least \(k_{n}\)-many sets \(Y\in\mathcal{F}_{n}\) such that \(\vec{p}^{(n)}\in Y+B^{\circ}(\varepsilon,\vec{0})\), so by Fact A.4, we have \(Y\cap B^{\circ}(\varepsilon,\vec{p}^{(n)})\neq\emptyset\). For each such \(Y\) (since \(Y\in\mathcal{F}_{n}\subseteq\mathcal{S}_{n}\)), there is a distinct6\(X_{Y}\in\mathcal{P}\) such that \(Y\subseteq X_{Y}\) and thus \(X_{Y}\cap B^{\circ}(\varepsilon,\vec{0})\neq\emptyset\) showing that there are at least \(k_{n}\)-many sets in \(\mathcal{P}\) which intersect \(B^{\circ}(\varepsilon,\vec{0})\).

Footnote 6: I.e. for \(Y\neq Y^{\prime}\in\mathcal{S}_{n}\) we have that \(X_{Y},X_{Y^{\prime}}\in\mathcal{P}\) and \(X_{Y}\neq X_{Y^{\prime}}\) so this mapping of \(Y\)’s to \(X\)’s is injective, so we have at least the same cardinality of \(X\)’s with the desired property as \(Y\)’s with the desired property.

For the last step of the proof, we perform a limiting process on \(n\). By Fact A.1, let \(\gamma\in\mathbb{R}\) such that \(\gamma<\left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/d}}\right)^{d}\) and \(\lceil\gamma\rceil=\left\lceil\left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}} }{M^{1/d}}\right)^{d}\right\rceil\). Then, because

\[\lim_{n\to\infty}k_{n}\geq\lim_{n\to\infty}\left(1+\frac{\varepsilon\cdot v ^{\frac{1}{2}}}{M^{1/d}}\right)^{d}\cdot\left(\frac{n}{n+\varepsilon}\right)^ {d}=\left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/d}}\right)^{d}>\gamma,\]

we can take \(N\in\mathbb{N}\) sufficiently large so that

\[k_{N}\geq\left(1+\frac{\varepsilon\cdot v^{\frac{1}{2}}}{M^{1/d}}\right)^{d} \cdot\left(\frac{N}{N+\varepsilon}\right)^{d}>\gamma.\]

Then considering the point \(\vec{p}^{(N)}\), we have by the choice of \(\gamma\) and the fact that \(k_{N}\) is an integer that

\[k_{N}=\lceil k_{N}\members of \(\mathcal{P}\), and thus trivially the closed ball \(\overline{B}_{\infty}(\varepsilon,\vec{p})\) does as well. Thus, if \(\mathcal{P}\) is \((k,\varepsilon)\)-secluded (meaning by definition that for every \(\vec{p}\in\mathbb{R}^{d}\) it holds that \(\overline{B}_{\infty}(\varepsilon,\vec{p})\) intersects at most \(k\) members of \(\mathcal{P}\)) then it must be that \(k\geq(1+2\varepsilon)^{d}\).

For the consequence, if \(k\leq 2^{d}\), then this implies \(\varepsilon\leq\frac{1}{2}\). Then taking the logarithm of both sides of our inequality and using the fact that \(\log_{4}(1+2x)\geq x\) for \(x\in[0,\frac{1}{2}]\) (see footnote7), we have

Footnote 7: One can verify that the function \(\log_{4}(1+2x)-x\) is concave on its domain \((-\frac{1}{2},\infty)\) using the second derivative and note that it is \(0\) at \(x=0\) and \(x=\frac{1}{2}\) (which is why the base \(4\) logarithm was chosen) so it is non-negative on \([0,\frac{1}{2}]\).

\[\log_{4}(k)\geq d\log_{4}(1+2\varepsilon)\geq d\varepsilon\]

showing that \(\varepsilon\leq\frac{\log_{4}(k)}{d}\). 

We state the following interesting corollary when \(k(d)\) is polynomial.

**Corollary A.9**.: _Let \(\langle\mathcal{P}_{d}\rangle_{d=1}^{\infty}\) be a sequence of \((k(d),\varepsilon(d))\)-secluded partitions of \(\mathbb{R}^{d}\) such that every member of each \(\mathcal{P}_{d}\) has outer Lebesgue measure at most 1. If \(k(d)\in\mathsf{poly}(d)\) then \(\varepsilon(d)\in O(\frac{\ln d}{d})\) (where the hidden constant can be taken to be anything exceeding the polynomial degree of \(k\))._

Proof.: Since \(k(d)\in\mathsf{poly}(d)\), then there are constants \(C,n\) such that for sufficiently large \(d\), we have \(k(d)\leq Cd^{n}\) which for sufficiently large \(d\) is less than \(2^{d}\) so by 3.1, for sufficiently large \(d\) we have

\[\varepsilon(d)\leq\frac{\ln(k(d))}{d}\leq\frac{n\ln(Cd)}{d}\in O\left(\frac{ \ln(d)}{d}\right).\]

More specifically, for any \(n^{\prime}>n\) we have for large enough \(d\) that \((n^{\prime}-n)\ln(d)\geq n\ln(C)\), so for large enough \(d\) we have

\[\varepsilon(d)\leq\frac{n\ln(Cd)}{d}=\frac{n[\ln(C)+\ln(d)]}{d}\leq\frac{(n^{ \prime}-n)\ln(d)+n\ln(d)}{d}=\frac{n^{\prime}\ln(d)}{d}\]

showing that the hidden constant can be taken to be any \(n^{\prime}\) larger than the degree \(n\) of \(k\). 

### A Family of Near Optimal Constructions

The partitions that we construct in this section are of a very natural form: we build a partition of a large dimension \(d\) space, by splitting up the coordinates into smaller sets, and separately partitioning each set of coordinates. In the end, the smaller partitions will be known partition constructions [44]. We will define the construction very generically. We need two basic results. The following observation notes that if a partition is \((k,\varepsilon)\)-secluded, then we can _increase_\(k\) to \(k^{\prime}\) and _decrease_\(\varepsilon\) to \(\varepsilon^{\prime}\) and the partition is trivially \((k^{\prime},\varepsilon^{\prime})\)-secluded.

**Observation A.10** (Monotonicity in \(k\) and \(\varepsilon\)).: _Let \(d\in\mathbb{N}\), \(k,k^{\prime}\in\mathbb{N}\) with \(k^{\prime}\geq k\), \(\varepsilon,\varepsilon^{\prime}\in[0,\infty)\) with \(\varepsilon^{\prime}\leq\varepsilon\), and \(\mathcal{P}\) a \((k,\varepsilon)\)-secluded partition of \(\mathbb{R}^{d}\). Then \(\mathcal{P}\) is also a \((k^{\prime},\varepsilon^{\prime})\)-secluded partition of \(\mathbb{R}^{d}\)._

Proof.: Since \(\mathcal{P}\) is \((k,\varepsilon)\)-secluded, by definition every \(\varepsilon\)-ball intersects at most \(k\) members of \(\mathcal{P}\), so trivially every (no larger) \(\varepsilon^{\prime}\)-ball intersects at most \(k^{\prime}\geq k\) members of \(\mathcal{P}\). 

We will frequently refer to the above observation just using the phrase "by monotonicity, \(\mathcal{P}\) is \((k^{\prime},\varepsilon^{\prime})\)-secluded"

**Fact A.11** (Trivial \(k\) for Unit Cube Partitions).: _Let \(d\in\mathbb{N}\), \(\varepsilon\in[0,\infty)\), and \(\mathcal{P}\) be any unit cube partition of \(\mathbb{R}^{d}\). Then \(\mathcal{P}\) is \((k,\varepsilon)\)-secluded for \(k=\left\lfloor(2+2\varepsilon)^{d}\right\rfloor\)._

Proof.: Consider any point \(\vec{p}\in\mathbb{R}^{d}\). Observe that for any \(X\in\mathcal{P}\), \(X\) is a unit cube, so \(\operatorname{diam}_{\infty}(X)=1\), so if \(X\) intersects \(\overline{B}_{\infty}(\varepsilon,\vec{p})\), then \(X\subseteq\overline{B}_{\infty}(1+\varepsilon,\vec{p})\).

Because (1) each \(X\in\mathcal{P}\) has measure 1, and (2) every pair of members are disjoint (because \(\mathcal{P}\) is a partition), and (3) the measure of \(\overline{B}_{\infty}(1+\varepsilon,\vec{p})=\vec{p}+[-1-\varepsilon,1+ \varepsilon]^{d}\) is \((2+2\varepsilon)^{d}\), it follows that at most \(\left\lfloor(2+2\varepsilon)^{d}\right\rfloor\) members of \(\mathcal{P}\) are a subset of \(\overline{B}_{\infty}(1+\varepsilon,\vec{p})\) and thus at most \(\left\lfloor(2+2\varepsilon)^{d}\right\rfloor\) members of \(\mathcal{P}\) intersect \(\overline{B}_{\infty}(\varepsilon,\vec{p})\) which shows that \(\mathcal{P}\) is \((k,\varepsilon)\)-secluded for \(k=\left\lfloor(2+2\varepsilon)^{d}\right\rfloor\) as claimed.

#### a.3.1 Construction

**Definition A.12** (Partition Product).: _Let \(d_{1},\ldots,d_{n}\in\mathbb{N}\) and \(\mathcal{P}_{1},\ldots,\mathcal{P}_{n}\) be partitions of \(\mathbb{R}^{d_{1}},\ldots,\mathbb{R}^{d_{n}}\) respectively. Letting \(d=\sum_{i=1}^{n}d_{n}\) we define the product partition of \(\mathbb{R}^{d}\) as_

\[\prod_{i=1}^{n}\mathcal{P}_{i}\operatorname*{\stackrel{{\mathrm{ def}}}{{=}}}\left\{\prod_{i=1}^{n}X_{i}\colon X_{i}\in\mathcal{P}_{i}\right\}\]

_where \(\prod_{i=1}^{n}X_{i}\) is viewed as a subset of \(\mathbb{R}^{d}\)._

We specifically stated that \(\prod_{i=1}^{n}X_{i}\) is viewed as a subset of \(\mathbb{R}^{d}\), because technically it is a subset of \(\prod_{i=1}^{n}\mathbb{R}^{d_{i}}\), but this is naturally isomorphic to \(\mathbb{R}^{d}=\mathbb{R}\Sigma_{i=1}^{n}d_{i}\). For example, technically, if \(d_{1}=d_{2}=d_{3}=2\), then the elements of \(\prod_{i=1}^{n}\mathbb{R}^{d_{i}}\) are of the form \(\langle\langle x_{1},x_{2}\rangle,\langle x_{3},x_{4}\rangle,\langle x_{5},x_{ 6}\rangle\rangle\), but this is trivially isomorphic to \(\mathbb{R}^{6}\) by instead considering the element as \(\langle x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}\rangle\).

Also observe (shown below) that if the original partitions were unit cube partitions, then the product partition is also a unit cube partition.

**Fact A.13** (Unit Cube Preservation).: _If \(d_{1},\ldots,d_{n}\in\mathbb{N}\) and \(\mathcal{P}_{1},\ldots,\mathcal{P}_{n}\) are unit cube partitions of \(\mathbb{R}^{d_{1}},\ldots,\mathbb{R}^{d_{n}}\) respectively, then \(\prod_{i=1}^{n}\mathcal{P}_{i}\) is also a unit cube partition._

Proof.: Each member of \(\prod_{i=1}^{n}\mathcal{P}_{i}\) is of the form \(\prod_{i=1}^{n}X_{i}\) where \(X_{i}\in\mathcal{P}_{i}\). Since \(\mathcal{P}_{i}\) is a unit cube partition, each \(X_{i}\) is a product of translates of \([0,1)\), and thus \(\prod_{i=1}^{n}X_{i}\) is also a product of translates of \([0,1)\), so the member is a unit cube. 

If all partitions \(\mathcal{P}_{1},\ldots,\mathcal{P}_{n}\) are "efficiently computable" in the sense that given an arbitrary point, \(\vec{x}\in\mathbb{R}^{d_{i}}\) there is an efficient algorithm that computes a representation of the member of \(\mathcal{P}_{i}\) containing \(\vec{x}\), then the product partition is also "efficiently computable" because given some point \(\vec{y}\in\mathbb{R}^{d}\), the member that it is contained in can be found by determining which member of \(\mathcal{P}_{1}\) the point \(\langle y_{i}\rangle_{i=1}^{d_{1}}\) is in, and independently determining which member of \(\mathcal{P}_{2}\) the point \(\langle y_{i}\rangle_{i=d_{1}+1}^{d_{1}+d_{2}}\) is in, etc. The member of \(\prod_{i=1}^{d}\mathcal{P}_{i}\) that contains \(\vec{y}\) is just the product of members. This is an important property for using partitions as the basis of rounding schemes because an algorithm must determine which member/equivalence class a point is in (even if just implicitly). Because the partitions of [44] are "efficiently computable" so are the partitions constructed here.

We can now show that if we take a product of partitions, and we have a guarantee for each \(\mathcal{P}_{i}\) that it is \((k_{i},\varepsilon_{i})\)-secluded, then we can guarantee the product partition is \((k,\varepsilon)\)-secluded where \(k\) is the product of the \(k_{i}\)'s and \(\varepsilon\) is the minimum of the \(\varepsilon_{i}\)'s.

**Proposition A.2** (Product Partition Exclusion Guarantees).: _Let \(n\in\mathbb{N}\). For each index \(i\in[n]\), let \(d_{i},k_{i}\in\mathbb{N}\), \(\varepsilon_{i}\in(0,\infty)\) and \(\mathcal{P}_{i}\) be a \((k_{i},\varepsilon_{i})\)-secluded partition of \(\mathbb{R}^{d_{i}}\). Then the product partition \(\mathcal{P}=\prod_{i=1}^{n}\mathcal{P}_{i}\) is a \((k,\varepsilon)\)-secluded partition of \(\mathbb{R}^{d}\) where \(d=\sum_{i=1}^{n}d_{i}\), and \(k=\prod_{i=1}^{n}k_{i}\), and \(\varepsilon=\min_{i\in[n]}\varepsilon_{i}\)._

Proof Sketch.: The basic idea is that for any point \(\vec{p}\in\mathbb{R}^{d}\), we consider how many members of \(\mathcal{P}\) intersect \(\overline{B}_{\varepsilon}(\vec{p})\). Conceptually8, we think of \(\vec{p}\) as a sequence \(\langle\vec{p}^{(i)}\rangle_{i=1}^{n}\) of \(n\) points where the \(i\)th point \(\vec{p}^{(i)}\) belongs to \(\mathbb{R}^{d_{i}}\). Because we are working with the \(\ell_{\infty}\) norm (that is the norm used by the definition of secluded), the \(\varepsilon\) ball around \(\vec{p}\) is the product of the \(\varepsilon\) balls around each \(\vec{p}^{(i)}\) which is smaller than the product of \(\varepsilon_{i}\) balls around each \(\vec{p}^{(i)}\) because we chose \(\varepsilon\) as the minimum size. Thus, if the \(\varepsilon\) ball around \(\vec{p}\) intersects a member \(X\) of the partition \(\mathcal{P}\), then conceptually viewing \(X\) as a sequence \(\langle X_{i}\rangle_{i=1}^{n}\) where \(X_{i}\) is a member of \(\mathcal{P}_{i}\), it must be for each \(i\in[n]\) that the \(\varepsilon\) ball around \(\vec{p}^{(i)}\) intersects \(X_{i}\) (and thus so does the \(\varepsilon_{i}\) ball since \(\varepsilon_{i}\geq\varepsilon\)). This means (for each \(i\in[n]\)) that \(X_{i}\) is one of at most \(k_{i}\) members of \(\mathcal{P}_{i}\) because at most \(k_{i}\) members of \(\mathcal{P}_{i}\) intersect the \(\varepsilon_{i}\) ball around \(\vec{p}^{(i)}\) (by definition of \(\mathcal{P}_{i}\) being \((k_{i},\varepsilon_{i})\)-secluded). Thus \(X\) is one of at most \(\prod_{i=1}^{n}k_{i}=k\) members of \(\mathcal{P}\). That is, there are at most \(k\) members of \(\mathcal{P}\) that intersect the \(\varepsilon\) ball around \(\vec{p}\) which is the definition of \(\mathcal{P}\) being \((k,\varepsilon)\)-secluded.

[MISSING_PAGE_FAIL:21]

### A Neighborhood Sperner/KKM Lemma: Theorem 3.4

In this section, we restate and prove our neighborhood variant of the Sperner/KKM/Lebesgue result on the cube. The proof idea is illustrated in Figure 2.

**Theorem A.15** (Neighborhood Sperner/KKM Lemma).: _Given a Sperner/KKM coloring of \([0,1]^{d}\), for any \(\varepsilon\in(0,\frac{1}{2}]\), there exists a point \(\vec{p}\in[0,1]^{d}\) such that \(B^{\circ}_{\infty}(\varepsilon,\vec{p})\) contains at least \(\left(1+\frac{2}{3}\varepsilon\right)^{d}\) points with distinct colors._

Proof.: For convenience, we will assume that the cube is \([-\frac{1}{2},\frac{1}{2}]^{d}\) rather than \([0,1]^{d}\). Let \(C\) be a finite set (of colors) and \(\chi\colon[-\frac{1}{2},\frac{1}{2}]^{d}\to C\) be a Sperner/KKM coloring of the unit cube \([-\frac{1}{2},\frac{1}{2}]^{d}\).

Figure 2: (a) shows a (finite) coloring \(\chi\) of the unit cube \([-\frac{1}{2},\frac{1}{2}]^{2}\) such that no color includes points on opposite edges. (b) shows the natural extension \(\gamma\) of that coloring to \([-\frac{1}{2}-\varepsilon,\frac{1}{2}+\varepsilon]^{2}\). The extension is obtained by mapping each point \(\vec{y}\) to the point \(\vec{x}\) for which each coordinate value \(y_{i}\) is restricted to be within \([-\frac{1}{2},\frac{1}{2}]\), and then \(\vec{y}\) is given whatever color \(\vec{x}\) had. (c), (e), and (g) show three of the five colors and demonstrate that there is at least one quadrant of the \(\varepsilon\)-ball that can be Minkowski summed with the color so that the sum remains a subset of the extended cube. For red it is the lower right quadrant, for purple it is the upper right, and for gray it could be the upper left (shown) or the upper right. (d), (f), and (h) show the resulting Minkowski sum for each color. Utilizing the Brunn-Minkowski inequality, this set will have substantially greater area—by a factor of at least \((1+\frac{\varepsilon}{1+\varepsilon})^{2}\).

[MISSING_PAGE_FAIL:23]

Proof of Claim.: Let \(\vec{v}\) be an orientation given in A.16 for color \(c\). We get the following chain of containments:

\[Y_{c}+B_{\vec{v}} =Y_{c}+\left(\prod_{i=1}^{d}-v_{i}\cdot(0,\varepsilon)\right)\] (Def'n of

\[B_{\vec{v}}\]

) \[\subseteq\left(\prod_{i=1}^{d}v_{i}\cdot(-\tfrac{1}{2},\tfrac{1} {2}+\varepsilon]\right)+\left(\prod_{i=1}^{d}-v_{i}\cdot(0,\varepsilon)\right)\] (A.16) \[=\left(\prod_{i=1}^{d}v_{i}\cdot(-\tfrac{1}{2},\tfrac{1}{2}+ \varepsilon]\right)+\left(\prod_{i=1}^{d}v_{i}\cdot(-\varepsilon,0)\right)\] (Factor a negative) \[=\prod_{i=1}^{d}v_{i}\cdot(-\tfrac{1}{2}-\varepsilon,\tfrac{1}{2 }+\varepsilon)\] (Minkowski sum of rectangles) \[\subseteq[-\tfrac{1}{2}-\varepsilon,\tfrac{1}{2}+\varepsilon]^{d}.\] (

This proves the claim. 

We also claim that \(Y_{c}+B_{\vec{v}}\) has a substantial measure.

**Subclaim A.18**.: _For each color \(c\in C\) and any orientation \(\vec{v}\in\{-1,1\}^{d}\), the set \(Y_{c}+B_{\vec{v}}\) is Borel measurable and \(m(Y_{c}+B_{\vec{v}})\geq m_{out}(Y_{c})\cdot\left(1+\tfrac{\varepsilon}{1+ \varepsilon}\right)^{d}\)._

Proof of Claim.: Let \(M=(1+\varepsilon)^{d}\) which is the measure of \(\prod_{i=1}^{d}v_{i}\cdot(-\tfrac{1}{2},\tfrac{1}{2}+\varepsilon]\), and because by A.16, \(Y_{c}\) is a subset of this set, we have \(m_{out}(Y_{c})\leq M\).

We have that \(Y_{c}+B_{\vec{v}}\) is Borel measurable and that \(m\left(Y_{c}+B_{\vec{v}}\right)\geq\left(m_{out}(Y_{c})^{\frac{1}{d}}+ \varepsilon\right)^{d}\) by A.6 (see details13). Thus, we have the following chain of inequalities:

Footnote 13: Note that for the \(\ell_{\infty}\) norm, the measure of the unit ball is \(v_{\|\cdot\|_{\infty},d}=2^{d}\). Then note that \(B_{\vec{v}}\) is an open orthant of an \(\varepsilon\) ball with respect to \(\ell_{\infty}\), so is in fact itself an \(\frac{\varepsilon}{2}\) ball with respect to \(\ell_{\infty}\). This is why we get “\(\varepsilon\)” instead of the “\(2\varepsilon\)” in A.6. We could translate this open ball to the origin and translate the set \(Y_{c}\) accordingly to get the same Minkowski sum without changing the measures, and after doing so we could apply A.6 verbatim.

\[m(Y_{c}+B_{\vec{v}}) \geq\left(m_{out}(Y_{c})^{1/d}+\varepsilon\right)^{d}\] (Above) \[=M\cdot\left(\frac{m_{out}(Y_{c})^{1/d}}{M^{1/d}}+\frac{ \varepsilon}{M^{1/d}}\right)^{d}\] (Factor out

\[M\]

) \[\geq M\cdot\left(\frac{m_{out}(Y_{c})}{M}\right)\cdot\left(1+ \frac{\varepsilon}{M^{1/d}}\right)^{d}\] (A.7) \[=m_{out}(Y_{c})\cdot\left(1+\frac{\varepsilon}{1+\varepsilon} \right)sum of measures of sets in \(\mathcal{A}\), we have the following:

\[\sum_{A\in\mathcal{A}}m(A) =\sum_{c\in C}m\left(Y_{c}+B_{\vec{v}(c)}\right)\] (Def'n of

\[\mathcal{A}\]

; measurability was shown above) \[\geq\left(1+\frac{\varepsilon}{1+\varepsilon}\right)^{d}\cdot\sum_{ c\in C}m_{out}(Y_{c})\] (A.18 and linearity of summation) \[\geq\left(1+\frac{\varepsilon}{1+\varepsilon}\right)^{d}\cdot m_{ out}\left(\bigcup_{c\in C}Y_{c}\right)\] (Countable/finite subaddativity of outer measures) \[=\left(1+\frac{\varepsilon}{1+\varepsilon}\right)^{d}\cdot m_{ out}\left([-\tfrac{1}{2}-\varepsilon,\tfrac{1}{2}+\varepsilon]^{d}\right)\] (The

\[Y_{c}\]

's partition

\[[-\tfrac{1}{2}-\varepsilon,\tfrac{1}{2}+\varepsilon]^{d}\] \[=\left(1+\frac{\varepsilon}{1+\varepsilon}\right)^{d}\cdot(1+2 \varepsilon)^{d}\] (Evaluate outer measure)

By A.17, each member of \(\mathcal{A}\) is a subset of \([-\tfrac{1}{2}-\varepsilon,\tfrac{1}{2}+\varepsilon]^{d}\), so by 5.1, there exists a point \(\vec{p}\in[-\tfrac{1}{2}-\varepsilon,\tfrac{1}{2}+\varepsilon]^{d}\) that belongs to at least

\[\left\lceil\frac{\left(1+\frac{\varepsilon}{1+\varepsilon}\right)^{d}\cdot(1 +2\varepsilon)^{d}}{(1+2\varepsilon)^{d}}\right\rceil=\left\lceil\left(1+\frac {\varepsilon}{1+\varepsilon}\right)^{d}\right\rceil\]

sets in \(\mathcal{A}\). That is, \(\vec{p}\) belongs to \(Y_{c}+B_{\vec{v}(c)}\) for at least \(\left\lceil\left(1+\frac{\varepsilon}{1+\varepsilon}\right)^{d}\right\rceil\) colors \(c\in C\). For each such color \(c\), it follows that \(\vec{p}+(-\varepsilon,\varepsilon)^{d}\) intersects \(Y_{c}\) (see justification14). Note that with respect to the \(\ell_{\infty}\) norm, \(\vec{p}+(-\varepsilon,\varepsilon)^{d}=B_{\infty}^{\circ}(\varepsilon,\vec{p})\) showing that \(B_{\infty}^{\circ}(\varepsilon,\vec{p})\) contains points of at least \(\left\lceil\left(1+\frac{\varepsilon}{1+\varepsilon}\right)^{d}\right\rceil\) colors (according to the coloring of \(\gamma\) since we are discussing sets \(Y_{c}\)).

Footnote 14: If \(\vec{p}\in Y_{c}+B_{\vec{v}(c)}\subseteq Y_{c}+(-\varepsilon,\varepsilon)^{d}\), then by definition of Minkowski sum there exists \(\vec{y}\in Y_{c}\) and \(\vec{b}\in(-\varepsilon,\varepsilon)^{d}\) such that \(\vec{p}=\vec{y}+\vec{b}\) so \(Y_{c}\ni\vec{y}=\vec{p}-\vec{b}\in\vec{p}+(-\varepsilon,\varepsilon)^{d}\) demonstrating that these two sets contain a common point.

What we really want, though, is a point in the unit cube that has this property rather than a point in the extended cube, and we want it with respect to the original coloring \(\chi\) rather than the extended coloring \(\gamma\). We will show that the point \(\vec{f}\left(\vec{p}\right)\) suffices.

**Subclaim A.19**.: _If \(c\in C\) is a color for which \(B_{\infty}^{\circ}(\varepsilon,\vec{p})\cap Y_{c}\neq\emptyset\), then also \(B_{\infty}^{\circ}(\varepsilon,\vec{f}\left(\vec{p}\right))\cap X_{c}\neq\emptyset\)._

Proof of Claim.: Let \(\vec{y}\in B_{\infty}^{\circ}(\varepsilon,\vec{p})\cap Y_{c}\). Then because \(\vec{y}\in B_{\infty}^{\circ}(\varepsilon,\vec{p})\), we have \(\|\vec{y}-\vec{p}\|_{\infty}<\varepsilon\), so for each coordinate \(i\in[d]\), \(|y_{i}-p_{i}|<\varepsilon\). It is easy to analyze the \(9\) cases (or \(3\) by symmetries) arising in the definition of \(f\) to see that this implies \(|f(y_{i})-f(p_{i})|<\varepsilon\) as well (i.e. \(f\) maps pairs of values in its domain so that they are no farther apart), thus \(\|\vec{f}\left(\vec{y}\right)-\vec{f}\left(\vec{p}\right)\|_{\infty}<\varepsilon\) and thus \(\vec{f}\left(\vec{y}\right)\in B_{\infty}^{\circ}(\varepsilon,\vec{f}\left( \vec{p}\right))\).

Also, as justified in a prior footnote12, for any \(\vec{y}\in Y_{c}\) we have \(\vec{f}\left(\vec{y}\right)\in X_{c}\) so that \(\vec{f}\left(\vec{y}\right)\in B_{\infty}^{\circ}(\varepsilon,\vec{f}\left( \vec{p}\right))\cap X_{c}\) which shows that the intersection is non-empty. 

Footnote 12: If \(\vec{p}\in Y_{c}+B_{\vec{v}(c)}\subseteq Y_{c}+(-\varepsilon,\varepsilon)^{d}\), then by definition of Minkowski sum there exists \(\vec{y}\in Y_{c}\) and \(\vec{b}\in(-\varepsilon,\varepsilon)^{d}\) such that \(\vec{p}=\vec{y}+\vec{b}\) so \(Y_{c}\ni\vec{y}=\vec{p}-\vec{b}\in\vec{p}+(-\varepsilon,\varepsilon)^{d}\) demonstrating that these two sets contain a common point.

Thus, because \(B_{\infty}^{\circ}(unique color since any pair of corners belong to an opposite pair of faces on the cube. For this reason it is trivial that for \(\varepsilon>\frac{1}{2}\) there is a point \(\vec{p}\) such that \(B_{\infty}^{\circ}(\varepsilon,\vec{p})\) intersects at least \(2^{d}\) colors: just let \(\vec{p}\) be the midpoint of the unit cube. Thus, the only interesting case is \(\varepsilon\in(0,\frac{1}{2}]\), and for such \(\varepsilon\) we have \(1+\varepsilon\leq\frac{3}{2}\) and thus \(\frac{\varepsilon}{1+\varepsilon}\geq\frac{2}{3}\varepsilon\) showing that \(\left(1+\frac{\varepsilon}{1+\varepsilon}\right)^{d}\geq(1+\frac{2}{3} \varepsilon)^{d}\). This completes the proof of the theorem. 

## Appendix B Measure Theory

Throughout this section, we use the word "countable" to mean finite or countably infinite.

**Fact B.1** (\(\ell_{\infty}\) Diameter Ball).: _Let \(d\in\mathbb{N}\) and \(X\subseteq\mathbb{R}^{d}\) be a bounded set with diameter \(D\) (with respect to \(\ell_{\infty}\)). Then there exists \(\vec{p}\in\mathbb{R}^{d}\) such that \(X\subseteq\overline{B}_{D/2}(\vec{p})\). As a consequence, \(m_{out}(X)\leq D^{d}\) where \(m_{out}\) denotes outer Lebesgue measure._

Proof Sketch.: For each coordinate \(i\in[d]\), consider the set \(X_{i}=\{\pi_{i}(\vec{x})\colon\vec{x}\in X\}\subseteq\mathbb{R}\) of the \(i\)th coordinates of each point in \(X\). The infimum and supremum are distance at most \(D\) apart, because otherwise there would be points \(\vec{y},\vec{z}\in X\) such that \(|\pi_{i}(\vec{z})-\pi_{i}(\vec{y})|>D\) which means \(\|\vec{z}-\vec{y}\|_{\infty}>D\). Thus taking \(\vec{p}=\langle\frac{\inf(X_{i})+\sup(X_{i})}{2}\rangle_{i=1}^{d}\) we have \(X\subseteq\prod_{i=1}^{d}[\inf(X_{i}),\sup(X_{i})]\subseteq\vec{p}+[-\frac{D}{ 2},\frac{D}{2}]^{d}=\overline{B}(D/2,\vec{p})\). 

**Fact B.2**.: _If \(\mu\) is a measure and \(\mathcal{A}\) is a (possibly uncountable) family of pairwise disjoint measurable sets, then_

\[\mu(\bigsqcup_{A\in\mathcal{A}}A)\geq\sum_{A\in\mathcal{A}}\mu(A).\]

Proof.: By definition of the arbitrary summation (c.f. [20, p. 11]) we have

\[\sum_{A\in\mathcal{A}}\mu(A)\operatorname*{\stackrel{{\mathrm{ def}}}{{=}}}\sup\left\{\sum_{A\in\mathcal{F}}\mu(A):\mathcal{F}\subseteq \mathcal{A},\ \mathcal{F}\ \text{finite}\right\}\]

and for any \(\mathcal{F}\subseteq\mathcal{A}\) we have

\[\mu(\bigsqcup_{A\in\mathcal{A}}A)\geq\mu(\bigsqcup_{A\in\mathcal{F}}A)=\sum_{ A\in\mathcal{F}}\mu(A).\]

Thus \(\mu(\bigsqcup_{A\in\mathcal{A}}A)\) is an upper bound for the set \(\left\{\sum_{A\in\mathcal{F}}\mu(A):\mathcal{F}\subseteq\mathcal{A},\ \mathcal{F}\ \text{finite}\right\}\) and thus greater than or equal to the supremum. 

**Fact B.3** (Interchange of Countable Sums with Non-negative Terms).: _If \(I,J\) are countable sets, and \(a_{i,j}\geq 0\) for all \((i,j)\in I\times J\), then_

\[\sum_{i\in I}\sum_{j\in J}a_{i,j}=\sum_{j\in J}\sum_{i\in I}a_{i,j}\]

Proof.: This can be proved directly via basic analysis methods if \(I\) and \(J\) are assumed to be \(\mathbb{N}\) and the definition of the infinite sum as a limit of finite sums is used. Alternatively, viewing the summation as an integral over a countable measure space, this can be viewed as a corollary to Tonelli's theorem. 

**Lemma B.4** (Upper Bound Measure of Multiplicity).: _Let \(n\in\mathbb{N}\). Let \(X\) be a measurable set in some measure space (the measure being denoted by \(\mu\)) and let \(\mathcal{A}\) be a countable family of measurable subsets of \(X\) such that for each \(x\in X\), \(x\) belongs to at most \(n\) members of \(\mathcal{A}\). Then_

\[\sum_{A\in\mathcal{A}}\mu(A)\leq n\cdot\mu(X).\]

Proof.: 15 For ease of indexing, assume \(\mathcal{A}=\left\{A_{i}\right\}_{i=1}^{\infty}\). Let \(I_{A_{i}}\) denote the indicator function for \(A_{i}\), and note that by hypothesis, for any \(k\in\mathbb{N}\), the function \(\sum_{i=1}^{k}I_{A_{i}}\) is bounded above by the constant 

[MISSING_PAGE_FAIL:27]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Rigorous proofs of all results stated in the abstract and introduction are provided in the main body/appendix. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [NA]. Justification: This is a theoretical paper. The open problems and future research directions are discussed. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Complete proofs of all the theorems are provided in the appendix. The main body discusses proof ideas. No assumptions in the theorem statements are made. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: Our paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: Our paper does not include experiments requiring code. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: Our paper does not include experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.

7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Our paper does not include experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Our paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes], Justification: There are no potential harms caused by this research work. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our paper has no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Our paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.