# Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts

Pritam Sarkar\({}^{1,2}\) Ahmad Beirami\({}^{3}\) Ali Etemad\({}^{1,3}\)

\({}^{1}\)Queen's University \({}^{2}\)Vector Institute \({}^{3}\)Google Research

{pritam.sarkar, ali.etemad}@queensu.ca beirami@google.com

###### Abstract

Video self-supervised learning (VSSL) has made significant progress in recent years. However, the exact behavior and dynamics of these models under different forms of distribution shift are not yet known. In this paper, we comprehensively study the behavior of six popular self-supervised methods (\(\bm{v}\)-SimCLR, \(\bm{v}\)-MoCo, \(\bm{v}\)-BYOL, \(\bm{v}\)-SimSiam, \(\bm{v}\)-DINO, \(\bm{v}\)-MAE) in response to various forms of natural distribution shift, i.e., (\(i\)) context shift, (\(ii\)) viewpoint shift, (\(iii\)) actor shift, (\(iv\)) source shift, (\(v\)) generalizability to unknown classes (zero-shot), and (\(vi\)) open-set recognition. To perform this extensive study, we carefully craft a test bed consisting of \(17\) in-distribution and out-of-distribution benchmark pairs using available public datasets and a series of evaluation protocols to stress-test the different methods under the intended shifts. Our study uncovers a series of intriguing findings and interesting behaviors of VSSL methods. For instance, we observe that while video models generally struggle with context shifts, \(\bm{v}\)-MAE and supervised learning exhibit more robustness. Moreover, our study shows that \(\bm{v}\)-MAE is a strong temporal learner, whereas contrastive methods, \(\bm{v}\)-SimCLR and \(\bm{v}\)-MoCo, exhibit strong performances against viewpoint shifts. When studying the notion of open-set recognition, we notice a trade-off between closed-set and open-set recognition performance if the pretrained VSSL encoders are used without finetuning. We hope that our work will contribute to the development of robust video representation learning frameworks for various real-world scenarios. The project page and code are available at: https://pritamqu.github.io/OOD-VSSL.

## 1 Introduction

Self-supervised learning has achieved tremendous success in learning strong and meaningful representations in various video domains, such as action recognition [1; 2; 3; 4; 5; 6; 7], action localization [8], video summarization [9], and video captioning [10; 11; 12]. Considering the diversity and complexity of the video domain, real-world deployment of video-based intelligent systems requires to understand the model performance under distribution shifts. Distribution shifts may occur due to differences in contextual information, viewpoint, geographical location, and the presence of unknown classes with respect to the training data, among others.

Despite the vast amount of work on VSSL [13; 14; 15; 16; 17; 18; 5; 1; 19; 8; 12], a number of fundamental questions regarding the out-of-distribution behavior and dynamics of VSSL methods remain unanswered. To date, there have been no comprehensive studies of these aspects, which we attempt to address in this paper. Specifically, we pose and answer the following questions:

1. How do the learned spatial and temporal representations vary based on different VSSL pretraining methodologies? How robust are these representations to different distribution shifts?

**Q2.**: Considering recent findings about the robustness of finetuning on the generalizability of large language models (LLMs) [20; 21], we pose the question: How does finetuning influence the out-of-distribution (OoD) generalization and zero-shot performance of VSSL?
**Q3.**: How do VSSL methods perform on open-set problems (where test samples can be from classes previously unknown to the model)? And what is the relationship between performance in closed-set vs. open-set recognition?
**Q4.**: Do different VSSL methods exhibit comparable decision-making patterns ('decision similarity') given the same training conditions? And how is this impacted by different distribution shifts?

To address these questions, we consider six different self-supervised learning algorithms, (_i_) SimCLR [22], (_ii_) MOCO-v3 [23], (_iii_) BYOL [24], (_iv_) SimSiam [25], (_v_) DINO [26], and (_vi_) MAE [27], along with fully supervised learning, and analyze their behaviors in various OoD settings. We select these methods to cover three key categories of self-supervision, namely _contrastive_ (SimCLR, MoCo-v3), _non-contrastive_ (BYOL, SimSiam, DINO), and _generative_ (MAE), approaches. In particular, these methods represent fundamental approaches on which numerous variants have been built and therefore represent many existing nuanced solutions in the area [2; 3; 4; 5; 6; 7; 1; 19; 8; 12]. For distribution shifts, we study a series of different possibilities which occur in videos in real-world settings due to changes in context (e.g., real vs. mime actions) [28; 29; 30; 31; 32], viewpoint (e.g, third-person vs. ego-centric view) [33; 34; 35; 36], actor (e.g., real-world vs. synthetic) [37; 38], and data sources (e.g., different datasets) [39; 40]. Moreover, we evaluate the generalizability of the VSSL methods on unseen classes, i.e., zero-shot recognition [41; 42; 43] and open-set recognition [44; 45; 46; 47]. To perform this investigation, we design a comprehensive study consisting of \(17\) in-distribution and out-of-distribution (InD-OoD) dataset pairs and examine the dynamics of VSSL under different distribution shifts using a variety of evaluation protocols including linear evaluation, finetuning, unsupervised clustering, and zero-shot recognition. Moreover, for a fair comparison, the VSSL methods are pretrained in identical experimental setups. We provide a high-level overview comparing the performance of all methods across all shifts in Figure 1. To the best of our knowledge, this is the first work to study VSSL under distribution shift to this depth.

In summary, our contributions are as follows:

* We present the first comprehensive and large-scale systematic investigation of real-world distribution shifts on VSSL methods. Our study encompasses \(2\) large-scale pretraining datasets, \(7\) video learning algorithms, \(17\) InD-OoD dataset pairs, as well as \(3\) toy datasets. Our thorough evaluation involves a total of \(269\) experiments, covering various evaluation protocols including linear evaluation, finetuning, unsupervised clustering, zero-shot recognition, and open-set recognition.
* Our study uncovers a series of intriguing behaviors and relationships of various VSSL methods, shedding light on the strengths and weaknesses that often go unnoticed in InD validation. Moreover, our investigation provides a comprehensive and impartial perspective on the effectiveness of supervised vs. self-supervised pretraining.

## 2 Related work

The analysis of robustness is a well-established area in computer vision, with an extensive body of research dedicated to image-based models [48; 49; 50; 51; 52]. Despite the growing popularity of video models in different domains, detailed comparative studies on their robustness remains

Figure 1: We present a **high-level overview** of different video learning methods depicting their performance under different distribution shifts. We normalized the eval. metric of each method to that of the highest performing method in each OoD scenario and averaged the results over multiple OoD datasets. See Appendix D.6 for more details.

under-explored. We come across two recent works [53; 54] that study the behavior of video models (supervised) against synthetic perturbations. An initial study [54] explores the performance of video models against spatial corruptions like noise, blur, color jittering and others. In a subsequent work, [53] extends the analysis of video models on temporal perturbations like frame reversal, jumbling, and freezing among others. In particular, our study focuses on real-world distribution shifts, which we find crucial as synthetic perturbations yield little or no consistency to the natural distribution shifts of real data [49; 48]. Moreover, none of the prior works attempts to understand the behavior of _video self-supervised_ methods under various forms of distribution shift.

## 3 Preliminaries

**Contrastive methods.** We study two popular contrastive methods: \(\bm{v}\)-SimCLR1 and \(\bm{v}\)-MoCo. These methods learn representations by maximizing the similarity of positive pairs while minimizing the similarity of negative pairs using the InfoNCE loss [22; 23; 55], where the similarity scores are calculated using L2-normalized cosine distance. The implementation of our \(\bm{v}\)-MoCo is based on MoCo-v3 [23], and it uses a momentum target encoder to fetch the key embeddings for the corresponding query embeddings. The momentum encoder is progressively updated from the online encoder using an exponential moving average (EMA) technique [56]. In contrast, \(\bm{v}\)-SimCLR directly uses the online encoder to compute the similarity scores from the embeddings of the given views. Additionally, \(\bm{v}\)-MoCo employs a predictor head, unlike \(\bm{v}\)-SimCLR.

Footnote 1: \(\bm{v}\) denotes the video variant of the image-based self-supervised method.

**Non-contrastive methods.** We study three popular non-contrastive methods \(\bm{v}\)-BYOL, \(\bm{v}\)-SimSiam, and \(\bm{v}\)-DINO. These methods learn meaningful representations by minimizing the distance between positive views of a given sample, without requiring any negative pairs. \(\bm{v}\)-BYOL uses an architecture similar to \(\bm{v}\)-MoCo, but instead of optimizing an InfoNCE loss, it minimizes the L2-normalized cosine distance of the positive views [24; 25]. \(\bm{v}\)-SimSiam is an ablation variant of \(\bm{v}\)-BYOL that does not use a target encoder and directly obtains the embeddings corresponding to the views from the online encoder. The setup of \(\bm{v}\)-DINO is also similar to \(\bm{v}\)-BYOL, but it introduces 'Centering' [26] to prevent against model collapse, instead of using a predictor head like \(\bm{v}\)-BYOL and \(\bm{v}\)-SimSiam.

**Generative method.** Finally, we also study \(\bm{v}\)-MAE which aims to learn representations through reconstructions of heavily masked inputs. \(\bm{v}\)-MAE employs an autoencoder architecture [27; 1; 57] where the encoder compresses the input information, which is then reconstructed by the decoder while minimizing the L2-reconstruction error [27].

We provide additional details and design specifics for each method in Appendix B.

## 4 Distribution Shifts

Let \(p_{y}\) denote a probability distribution over labels. Let \(p_{x|y}\) denote a class conditional density of input \(x\) given labels \(y\). We draw \(y\) from \(p_{y}\) and then \(x\) from \(p_{x|y}(x|y)\). In the case of InD, there is no distribution shift and the validation set is drawn from the exact same distribution as above. We consider two types of distribution shifts in this paper: input-based and output-based. For **Input shift**, we assume that the class conditional density shifts from \(p_{x|y}\) to \(q_{x|y}\). To measure the OoD performance, we choose \(q_{x|y}\) such that there is no overlap between the distributions of the unlabelled pretraining data and OoD validation sets. For **Output shift**, we assume that the label distribution shifts from \(p_{y}\) to \(q_{y}\). Note that we are particularly interested in cases where the support set of \(q_{y}\) might be different from that of \(p_{y}\), i.e., novel classes appear at test time.

We study four different input shifts, (_i_) context shift, (_ii_) viewpoint shift, (_iii_) actor shift, and (_iv_) source shift. **Context shift** or out-of-context refers to when the scenic background or contextual information is misleading or absent, e.g., mime actions. Humans possess a deep understanding of actions and can thus recognize actions without much context. However, vision models tend to rely heavily on contextual cues and background objects to make predictions [28; 58; 59]. As a result, models may generalize poorly when contextual information is absent or different from what the model has been trained on. **Viewpoint shift** refers to the change in the viewpoint or perspective of the camera that captures the scene. Some of the popular viewpoints in the video domains include third-person view, egocentric view, bird's-eye view, top-down view, and surveillance camera view,among a few others. In this work, we examine the generalizability of models trained on third-person views to other viewpoints including egocentric, surveillance camera, and top-down views. Further, we study generalizability under **actor shift**, which occurs when the 'type' of actors changes between training and test sets. These actor-type shifts can include human-to-animal shifts or synthetic-to-real shifts. We consider human actions in the real world as InD while animal actions and synthetic videos (e.g., video games, animation) are considered OoD. Lastly, we study the distribution shifts caused due to the changes in the data sources, referred to as **source shift**. As discussed in [39; 40], datasets of similar classes also exhibit distribution shifts due to the difference in curation strategies, annotation schemes, demographics, geographical locations, and other factors, even when none of the specific shifts mentioned earlier apply. To assess the generalizability of video models under real-world distribution shifts, we also investigate their performance when faced with multiple forms of distribution shifts occurring simultaneously. For example, we evaluate the model's ability to handle 'top-down synthetic videos' which causes both viewpoint and actor shifts concurrently. A few examples are presented in Figure 2.

We also investigate the dynamics of video models under output shifts. We perform **zero-shot** recognition on both regular actions and unusual or rare actions. An example of rare actions would be 'hammering watermelon' as opposed to 'cutting watermelon', as shown in Figure 2. Lastly, we evaluate performance in **open-set** problems where models are required to distinguish between known vs. unknown classes while correctly predicting the known ones [60; 45; 44; 46]. Deep learning models are known to struggle in such scenarios due to their tendency towards over-confident predictions. We note that existing literature has only evaluated VSSL methods under closed-set scenarios, neglecting the importance of their performance in real-world open-set settings.

## 5 Experiment setup

**Benchmarks.** We use two large-scale video action datasets, Kinetics400 [61] and Kinetics700 [62], for pretraining the video models. The results presented in the main paper use Kinetics400 for pre-training, while we provide additional results based on pretraining with Kinetics700 in Appendix D. To evaluate the video models under distribution shifts, we use a total of 12 real-world benchmarks, comprising: **Mimetics10** and **Mimetics50**[28] as the out-of-context validation sets; **CharadesEgo**[36], **TinyiTat-v2**[63], and **Sim3Action**[64] to investigate viewpoint shifts (egocentric, surveillance camera, and top-down views, respectively); **ActorShift**[65] and **Sims4action**[64], for actor shifts (animal and synthetic domains, respectively); **UCF101**[66] and **HMDB51**[67] for source shift; **UCF101**[66], **HMDB51**[67], and **RareAct**[68] for zero-shot recognition; **UCF101** and **HMDB51** for open-set recognition while using **Kinetics400** and **UCF101** as closed-set. For each OoD validation set, we create an InD training and validation set to measure the change in performance. We construct the InD splits using **Kinetics400**[61], **Kinetics700**[62], **MIT-v2**[69], and **CharadesEgo**[36]. Finally, we also use 3 toy datasets to conduct experiments in controlled

Figure 2: **Sample video frames** of distribution shifts. In these examples, the left frames of each category represent an InD sample and the right frames represent an OoD sample.

setups, including **ToyBox**[70], **COIL**[34], **STL-10**[71]. Additional details of the benchmarks can be found in Appendix C.

**Pretraining.** To ensure a fair comparison between the VSSL methods, we pretrain them in identical setups with necessary adjustments in hyperparameters. Although some of the methods studied in this work are already available in the literature [3, 5, 72, 1], they follow a variety of experiment setups including different architectures (e.g., R2+1D [73], R3D [72], TSM [74], ViT [75]), inputs, and others. Therefore, they could not be directly adopted for our experiments and are instead re-implemented. Specifically, we keep the encoder, inputs, batch sizes, and maximum number of pretraining epochs the same for all the methods. Furthermore, VSSL methods are tuned based on the InD validation split, with no exposure to OoD validation sets. We use the ViT-Base [75, 76] as the encoder, with a patch size of \(4\times 16^{2}\). Amongst the \(6\) VSSL methods studied in this paper, all of them use a Siamese [77] architecture other than \(\bm{v}\)-MAE. Therefore, the contrastive and non-contrastive methods are fed with \(2\) random spatio-temporal augmented crops from the original videos, similar to [72]. For a fair comparison, the \(\bm{v}\)-MAE which requires a single view as input, is fed with \(3\!\times\!32\!\times\!112^{2}\) inputs, while the other VSSL methods are fed with \(3\!\times\!16\!\times\!112^{2}\) inputs per view. Additional details for VSSL pretraining can be found in Appendix B.

**Evaluation.** To study input-based distribution shifts, we perform linear evaluation and finetuning using the InD training splits, followed by evaluating on both InD and OoD validation splits. We follow the standard protocol used in [72, 2, 4, 6, 78, 79, 7] for linear evaluation and finetuning. To evaluate the models on zero-shot recognition, we follow [41, 42] and jointly train video-text encoders using the Word2Vec [80] word embeddings. Lastly, we follow [45] for open-set recognition. We report top-1 accuracy for multi-class classification and mean average precision (meanAP) for multi-label multi-class classification. Moreover, for open-set problems, we report the area under the ROC curve (AUC) for distinguishing known vs. unknown, and accuracy for measuring closed-set performance. Please find more details in Appendix C.

## 6 Findings

**Q1: Dynamics of learned spatial and temporal representations under distribution shifts**

Our experiments on context shift show that video models greatly suffer in out-of-context generalization, as OoD performance significantly drops for all of the methods as presented in Table 1. Notice that \(\bm{v}\)-Supervised achieves the best OoD performance under linear evaluation (**Lin.**) and \(\bm{v}\)-MAE achieves the best when finetuned (**FT**), for both benchmarks. Intuitively speaking, the models need to learn strong temporal dynamics to generalize well under context shift, as the background or contextual information may be misleading or absent. \(\bm{v}\)-MAE and \(\bm{v}\)-Supervised show strong temporal learning capabilities as they learn _time-variant_ representations. This is in contrast to the other (contrastive and non-contrastive) methods which encourage learning _time-invariant_ or time-persistent representations by minimizing the embedding distance of the positive pairs sampled from two different timestamps. Additionally, our statistical analysis in Appendix D.7 confirms the higher robustness of \(\bm{v}\)-Supervised and \(\bm{v}\)-MAE against context shift (10 class) in both linear and finetuned setups. Moreover, in context shift with 50 classes, \(\bm{v}\)-Supervised exhibits more robustness in linear evaluation, while \(\bm{v}\)-MAE is more robust when finetuned.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{2}{c}{**Context (10 class)**} & \multicolumn{2}{c}{**Context (50 class)**} \\ \cline{2-5}  & Lin. & FT & Lin. & FT \\ \cline{2-5}  & OoD & InD & OoD & InD & OoD \\ \hline \hline \(\bm{v}\)-Supervised & **37.0\(\bm{\pm 1.5}\)** & **41.2** & **92.7** & **15.0\(\bm{\pm 1.5}\)** & 66.3\(\bm{\pm 1.5}\) & 19.0 78.1 \\ \hline \(\bm{v}\)-SimCLR & 31.4\(\bm{\pm 1.5}\) & 92.5\(\bm{\sim}\) & 35.3 & 94.4 & 14.9\(\bm{\pm 1.5}\) & 71.7\(\bm{\pm 1.5}\) & 19.1 81.8 \\ \(\bm{v}\)-MoCo & 29.7\(\bm{\pm 1.5}\) & 92.2\(\bm{\sim}\) & 39.0 & 94.2 & **15.0\(\bm{\pm 1.5}\)** & 74.0\(\bm{\pm 1.5}\) & 20.5 81.8 \\ \(\bm{v}\)-BYOL & 31.6\(\bm{\pm 1.5}\) & 89.3\(\bm{\pm 1.5}\) & **41.2** & 6.6 & 14.4\(\bm{\pm 1.5}\) & 71.8\(\bm{\pm 1.5}\) & 21.1 80.8 \\ \(\bm{v}\)-SimSim & 30.8\(\bm{\pm 1.5}\) & 89.5\(\bm{\sim}\) & **40.4** & 93.8 & 13.8\(\bm{\pm 1.5}\) & 67.9\(\bm{\pm 1.5}\) & 19.0 7.8 \\ \(\bm{v}\)-DINO & 38.4\(\bm{\pm 1.5}\) & 90.4\(\bm{\sim}\) & 40.4 & 93.6 & 13.0\(\bm{\pm 1.5}\) & 68.7\(\bm{\pm 1.5}\) & 19.0 78.8 \\ \(\bm{v}\)-MAE & 33.3\(\bm{\pm 1.5}\) & 82.9\(\bm{\sim}\) & **41.2** & 95.2 & 12.3\(\bm{\pm 1.5}\) & 56.4\(\bm{\pm 1.5}\) & **26.0** & 81.4 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison under **context** shift.

Figure 3: Comparing VSSL methods on **distangled** (a) temporal and (b) spatial representations.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

representations. Such datasets could potentially even diminish the effectiveness of robust pretrained models. For instance, in the case of \(\bm{v}\)-BYOL, the performance of the frozen encoder exceeds that of the finetuned model, as seen in Table 3 (HMDB/UCF).

To further investigate scenarios where finetuning may impair VSSL pretraining, we evaluate the video models on temporally perturbed inputs (e.g., freeze frames, random sampling, reverse sampling) in a controlled setup. As the VSSL methods are never trained with temporally perturbed inputs, this constitutes a distribution shift. The results presented in Figure 6 show that end-to-end finetuning of the contrastive and non-contrastive video encoders diminishes the time-invariance of the learned representations, which decreases robustness to temporal perturbations. As a result, in such cases, the frozen encoders perform better than the finetuned ones. Moreover, the frozen \(\bm{v}\)-MAE shows the worst performance under temporal shifts as it learns time-variant representations through self-supervised pretraining.

**Highlights:** (\(a\)) As opposed to LLMs, finetuning generally helps VSSL in both InD and OoD. (\(b\)) The benefits of finetuning largely vary between different VSSL methods and the type of distribution shifts. (\(c\)) Finetuning provides more benefits against actor shifts in both animal and synthetic domains in comparison to viewpoint shifts like egocentric and surveillance camera views. (\(d\)) Finetuning degrades robustness to temporal perturbations as it impairs the time-invariant representations of contrastive and non-contrastive methods. It can also degrade performance under source shift depending on the quality of the training benchmark.

### Q3: Closed-set vs. open-set performance

We perform open-set recognition in two setups: (1) we use the Kinetics400 for closed-set recognition, while the non-overlapping classes from the UCF101 and HMDB51 are used for open-set. (2) we use UCF101 and the non-overlapping classes of HMDB for closed-set and open-set recognition, respectively. To perform a comprehensive analysis on the performance of closed-set vs. open-set, we adopt two approaches, i.e, single-objective (closed-set only with cross-entropy error) and joint-objective (closed-set and open-set recognition simultaneously with DEAR loss [45]) variants.

Figure 5: **Linear vs. finetuned performance comparison under real-world distribution shifts. We measure the improvements as the difference between finetuned and linear evaluation accuracy.**

The results in Table 6 (a) demonstrate that the models struggle in open-set recognition when using Kinetics400 as closed-set in comparison to UCF101. This is due to the large number of known classes in Kinetics400 compared to the unknowns (\(400\) known classes in Kinetics400 vs. \(31\) and \(22\) unknown classes from UCF and HMDB, respectively). Therefore, the models tend to become over-confident and make false predictions. Evident from Table 6 (a), contrastive methods are robust to open-set recognition in all setups. This is likely due to the auxiliary information contributed by the negative samples used in these methods. To verify this hypothesis, we analyze the open macro-F1 scores vs. openness of the video models following [45, 44] by incrementally adding more unknown classes. The results in Figure 7 (a-c) show that both \(\bm{v}\)-SimCLR and \(\bm{v}\)-MoCo consistently achieve better performances compared to others. An additional statistical analysis on the performance of contrastive methods in open-set recognition is presented in Appendix D.7, which confirms their better performance relative to other methods.

Next, instead of end-to-end finetuning, we use the frozen encoders to train a linear head for open-set recognition. The top \(3\) closed-set performers, \(\bm{v}\)-BYOL, \(\bm{v}\)-MoCo, and \(\bm{v}\)-SimCLR (see Figure 7 (d) or U101\({}^{*}\) in Table 6 (b)), show the worst performance in open-set recognition. However, the pretrained encoders, \(\bm{v}\)-SimSiam, \(\bm{v}\)-DINO, and \(\bm{v}\)-Supervised, which are considered weaker in closed-set, perform better in open-set recognition, which might be due to their lack of over-confidence. \(\bm{v}\)-DINO outperforms other methods in open-set recognition achieving \(79.4\%\) while the performance of \(\bm{v}\)-SimCLR and \(\bm{v}\)-MoCo drops to almost chance levels. In comparison, \(\bm{v}\)-MoCo reports \(84.9\%\) in standard closed-set recognition, whereas \(\bm{v}\)-DINO achieves \(80.9\%\). The results presented in Figure 7 (d) suggest that there is a trade-off between closed-set and open-set recognition performances, particularly when frozen encoders are used without finetuning.

**Highlights:** (\(a\)) Contrastive methods demonstrate superior performance in open-set recognition when finetuned. (\(b\)) There is a trade-off between closed-set and open-set recognition performance when frozen pretrained encoders are used. (\(c\)) Strong frozen encoders (\(\bm{v}\)-MoCo, \(\bm{v}\)-SimCLR) have no open-set generalization performance due to their over-confident predictions. (\(d\)) On the other hand, slightly weak VSSL frozen encoders (\(\bm{v}\)-DINO, \(\bm{v}\)-SimSiam) show better open-set performance, while \(\bm{v}\)-MAE seems to perform poorly in both settings.

\begin{table}

\end{table}
Table 6: Comparison in **open-set** recognition. \(\bm{v}\)-Supervised trained from scratch with DEAR loss failed to converge in K/U and K/H. Here, closed/open-set pairs are denoted as K/H: Kinetics/HMDB, K/U: Kinetics/UCF, U/H: UCF/HMDB, CE: cross-entropy error.

Figure 7: (a-c) Comparing **open macro-F1 scores** of finetuned models vs. **openness** (openness is measured as the ratio of unknown to known classes). (d) The relationships between **closed-set** and **open-set** recognition performance of frozen pretrained encoders.

### Q4: Decision similarity under distribution shifts

To measure 'decision similarity' between the video models, we evaluate whether the models make similar predictions, regardless of their correctness. The results presented in Figure 8 demonstrate that decision similarity varies between the VSSL methods both InD and OoD, but is generally lower in OoD settings. Specifically, while we observe only a slight decrease in decision similarity in the case of animal domain actor shift, we observe significant drops in decision similarity in the case of context shifts and source shifts. Moreover, the results reveal that the decision similarity between supervised and self-supervised methods significantly drops under distribution shifts, indicating that under such shifts, VSSL methods make considerably different decisions than supervised learning. We also observe low decision similarity between \(\bm{v}\)-MAE and other VSSL methods, which can be due to the generative nature of \(\bm{v}\)-MAE vs. the others. Lastly, we note that contrastive methods tend to have greater decision similarities with each other than the similarities observed among non-contrastive methods. Additional results are presented in Appendix D.

**Highlights:** (\(a\)) The decision similarity of video models decreases under distribution shifts, which further varies based on the type of shift. (\(b\)) Context and source shifts cause the most dissimilarity between decisions. (\(c\)) Overall, the predictions between the supervised and self-supervised methods, as well as between \(\bm{v}\)-MAE and other VSSL methods exhibit the least similarity.

## 7 Discussion and Summary

**Limitations.** We consider contrastive, non-contrastive, and generative VSSL methods in our work, as they are well-established and have demonstrated strong performance in previous studies [72; 1; 3; 5] on various video benchmarks [1; 87; 72]. However, there exists another category of VSSL methods which uses pretext tasks for self-supervision, e.g., rotation prediction [88], frame and clip order prediction [14; 89], motion prediction [90], and others [91; 92; 93; 94]. While these methods are not included in our study, they are worth exploring in future research. Moreover, our work primarily focuses on various _self-supervised methods_ as opposed to network architectures. It would be valuable to further investigate VSSL under distribution shifts with larger Transformer [95] or convolutional [96] architectures, which we could not perform due to resource constraints. Nonetheless, our findings serve as a foundation for future studies. We discuss the broader impact of our work in Appendix A.

**Summary.** In this work, we thoroughly examine the behavior of VSSL methods under real-world distribution shifts that commonly occur in videos due to changes in context, viewpoint, actor, and source. Moreover, our study delves into investigating the generalizability of VSSL methods in zero-shot and open-set recognition. To rigorously evaluate the robustness of video models, we introduce a comprehensive OoD test bed curated from existing literature. This test bed is carefully designed to stress test the robustness of video models and provide a comprehensive evaluation of their capabilities. Our study uncovers a wide range of interesting dynamics of various VSSL methods under different distribution shifts, which can be instrumental in guiding future research and algorithm development in video representation learning. To the best of our knowledge, this is the first work to systematically investigate VSSL under real-world distribution shifts.

Figure 8: The **decision similarity** between the video models in InD (top) vs. OoD (bottom). The lighter color indicates less similarity.

[MISSING_PAGE_FAIL:11]

* [16] Dezhao Luo, Chang Liu, Yu Zhou, Dongbao Yang, Can Ma, Qixiang Ye, and Weiping Wang. Video cloze procedure for self-supervised spatio-temporal learning. In _AAAI_, 2020.
* [17] Dahun Kim, Donghyeon Cho, and In So Kweon. Self-supervised video representation learning with space-time cubic puzzles. In _AAAI_, 2019.
* [18] Fei Wu, Qingzhong Wang, Jiang Bian, Ning Ding, Feixiang Lu, Jun Cheng, Dejing Dou, and Haoyi Xiong. A survey on video action recognition in sports: Datasets, methods and applications. _IEE Transactions on Multimedia_, 2022.
* [19] Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou, and Lu Yuan. Bevt: Bert pretraining of video transformers. _arXiv preprint arXiv:2112.01529_, 2021.
* [20] Beliz Gunel, Jingfei Du, Alexis Conneau, and Veselin Stoyanov. Supervised contrastive learning for pre-trained language model fine-tuning. In _ICLR_, 2021.
* [21] Hyundong Cho, Chinnadhurai Sankar, Christopher Lin, Kaushik Sadagopan, Shahin Shayandeh, Asli Celikyilmaz, Jonathan May, and Ahmad Beirami. Know thy strengths: Comprehensive dialogue state tracking diagnostics. In _Findings of the ACL: EMNLP 2022_, pages 5345-5359, 2022.
* [22] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _ICML_, pages 1597-1607, 2020.
* [23] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In _ICCV_, pages 9640-9649, 2021.
* [24] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Pires, Zhaohan Guo, Mohammad Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. In _NeurIPS_, 2020.
* [25] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In _CVPR_, pages 15750-15758, 2021.
* [26] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _ICCV_, pages 9650-9660, 2021.
* [27] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. _arXiv preprint arXiv:2111.06377_, 2021.
* [28] Philippe Weinzaepfel and Gregory Rogez. Mimetics: Towards understanding human actions out of context. _IJCV_, 129(5), 2021.
* [29] Jihoon Chung, Yu Wu, and Olga Russakovsky. Enabling detailed action recognition evaluation through video dataset augmentation. _NeurIPS_, 35:39020-39033, 2022.
* [30] Jinpeng Wang, Yuting Gao, Ke Li, Yiqi Lin, Andy J Ma, Hao Cheng, Pai Peng, Feiyue Huang, Rongrong Ji, and Xing Sun. Removing the background by adding the background: Towards background robust self-supervised video representation learning. In _CVPR_, pages 11804-11813, 2021.
* [31] Jinwoo Choi, Chen Gao, Joseph CE Messou, and Jia-Bin Huang. Why can't i dance in the mall? learning to mitigate scene bias in action recognition. _NeurIPS_, 32, 2019.
* [32] Manlin Zhang, Jinpeng Wang, and Andy J Ma. Suppressing static visual cues via normalizing flows for self-supervised video representation learning. In _AAAI_, volume 36, pages 3300-3308, 2022.
* [33] Yong Jae Lee, Joydeep Ghosh, and Kristen Grauman. Discovering important people and objects for egocentric video summarization. In _CVPR_, pages 1346-1353, 2012.
* [34] Sheila J. Nayar. Columbia object image library (coil100). 1996.

* [35] Ugur Demir, Yogesh S Rawat, and Mubarak Shah. Tinyvirat: Low-resolution video action recognition. In _ICPR_, pages 7387-7394, 2021.
* [36] Gunnar A Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek Alahari. Charades-ego: A large-scale dataset of paired third and first person videos. _arXiv preprint arXiv:1804.09626_, 2018.
* [37] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In _CVPR_, pages 8494-8502, 2018.
* [38] Gul Varol, Ivan Laptev, Cordelia Schmid, and Andrew Zisserman. Synthetic humans for action recognition from unseen viewpoints. _IJCV_, 129(7):2264-2287, 2021.
* [39] Angelina Wang, Alexander Liu, Ryan Zhang, Anat Kleiman, Leslie Kim, Dora Zhao, Iroha Shirai, Arvind Narayanan, and Olga Russakovsky. Revise: A tool for measuring and mitigating bias in visual datasets. _IJCV_, 130(7):1790-1810, 2022.
* [40] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In _CVPR_, pages 1521-1528, 2011.
* [41] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In _CVPR_, pages 4613-4623, 2020.
* [42] Alec Kerrigan, Kevin Duarte, Yogesh Rawat, and Mubarak Shah. Reformulating zero-shot action recognition for multi-label actions. _NeurIPS_, 34:25566-25577, 2021.
* [43] Farhad Pourpanah, Moloud Abdar, Yuxuan Luo, Xinlei Zhou, Ran Wang, Chee Peng Lim, Xi-Zhao Wang, and QM Jonathan Wu. A review of generalized zero-shot learning methods. _PAMI_, 2022.
* [44] Abhijit Bendale and Terrance E Boult. Towards open set deep networks. In _CVPR_, pages 1563-1572, 2016.
* [45] Wentao Bao, Qi Yu, and Yu Kong. Evidential deep learning for open set action recognition. In _ICCV_, pages 13349-13358, 2021.
* [46] Walter J Scheirer, Anderson de Rezende Rocha, Archana Sapkota, and Terrance E Boult. Toward open set recognition. _PAMI_, 35(7):1757-1772, 2012.
* [47] Chuanxing Geng, Sheng-jun Huang, and Songcan Chen. Recent advances in open set recognition: A survey. _PAMI_, 43(10):3614-3631, 2020.
* [48] Florian Wenzel, Andrea Dittadi, Peter Vincent Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik Zietlow, David Kernert, Chris Russell, Thomas Brox, Bernt Schiele, Bernhard Scholkopf, and Francesco Locatello. Assaying out-of-distribution generalization in transfer learning, 2022.
* [49] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. _NeurIPS_, 33:18583-18599, 2020.
* [50] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena, Krishnamurthy Dj Dvijotham, and Ali Taylan Cemgil. A fine-grained analysis on distribution shift. In _ICLR_, 2022.
* [51] Haotian Ye, Chuanlong Xie, Tianle Cai, Ruichen Li, Zhenguo Li, and Liwei Wang. Towards a theoretical framework of out-of-distribution generalization. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _NeurIPS_, volume 34, pages 23519-23531, 2021.
* [52] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations, 2019.

* [53] Madeline C Schiappa, Naman Biyani, Shruti Vyas, Hamid Palangi, Vibhav Vineet, and Yogesh Rawat. Large-scale robustness analysis of video action recognition models. _arXiv preprint arXiv:2207.01398_, 2022.
* [54] Chenyu Yi, Siyuan Yang, Haoliang Li, Yap-peng Tan, and Alex Kot. Benchmarking the robustness of spatial-temporal models against corruptions. _arXiv preprint arXiv:2110.06513_, 2021.
* [55] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* [56] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. _NeurIPS_, 30, 2017.
* [57] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked autoencoders as spatiotemporal learners. _arXiv preprint arXiv:2205.09113_, 2022.
* [58] Yi Li and Nuno Vasconcelos. Repair: Removing representation bias by dataset resampling, 2019.
* [59] Yingwei Li, Yi Li, and Nuno Vasconcelos. Resound: Towards action recognition without representation bias. In _ECCV_, September 2018.
* [60] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need. _arXiv preprint arXiv:2110.06207_, 2021.
* [61] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. _arXiv preprint arXiv:1705.06950_, 2017.
* [62] Lucas Smaira, Joao Carreira, Eric Noland, Ellen Clancy, Amy Wu, and Andrew Zisserman. A short note on the kinetics-700-2020 human action dataset. _arXiv preprint arXiv:2010.10864_, 2020.
* [63] Praveen Tirupattur, Aayush J Rana, Tushar Sangam, Shruti Vyas, Yogesh S Rawat, and Mubarak Shah. Tinyaction challenge: Recognizing real-world low-resolution activities in videos. _arXiv preprint arXiv:2107.11494_, 2021.
* [64] Alina Roitberg, David Schneider, Aulia Djamal, Constantin Seibold, Simon Reiss, and Rainer Stiefelhagen. Let's play for action: Recognizing activities of daily living by learning from life simulation video games. In _IROS_, pages 8563-8569, 2021.
* [65] Yunhua Zhang, Hazel Doughty, Ling Shao, and Cees GM Snoek. Audio-adaptive activity recognition across video domains. In _CVPR_, pages 13791-13800, 2022.
* [66] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. _arXiv preprint arXiv:1212.0402_, 2012.
* [67] Hildegard Kuehne, Hueihan Jhuang, Estibaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In _ICCV_, pages 2556-2563, 2011.
* [68] Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Andrew Zisserman. Rareact: A video dataset of unusual interactions. _arXiv preprint arXiv:2008.01018_, 2020.
* [69] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. _PAMI_, 42(2):502-508, 2019.
* [70] Xiaohan Wang, Tengyu Ma, James Ainooson, Seunghwan Cha, Xiaotian Wang, Azhar Molla, and Maithilee Kunda. The toybox dataset of egocentric visual object transformations. _arXiv preprint arXiv:1806.06034_, 2018.
* [71] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In _ICAIS_, pages 215-223. PMLR, 2011.

* [72] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. In _CVPR_, pages 3299-3309, 2021.
* [73] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In _CVPR_, pages 6450-6459, 2018.
* [74] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In _ICCV_, pages 7083-7093, 2019.
* [75] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [76] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. Vivit: A video vision transformer. In _ICCV_, pages 6836-6846, 2021.
* [77] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Sackinger, and Roopak Shah. Signature verification using a" siamese" time delay neural network. _NeurIPS_, 6, 1993.
* [78] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. _NeurIPS_, 34, 2021.
* [79] Pedro Morgado, Ishan Misra, and Nuno Vasconcelos. Robust audio-visual instance discrimination. In _CVPR_, pages 12934-12945, 2021.
* [80] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. _arXiv preprint arXiv:1301.3781_, 2013.
* [81] Adria Recasens, Pauline Luc, Jean-Baptiste Alayrac, Luyu Wang, Florian Strub, Corentin Tallec, Mateusz Malinowski, Viorica Patraucean, Florent Altche, Michal Valko, et al. Broaden your views for self-supervised video learning. _arXiv preprint arXiv:2103.16559_, 2021.
* [82] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelovic, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. Self-supervised multimodal versatile networks. _NeurIPS_, 2(6):7, 2020.
* [83] Senthil Purushwalkam and Abhinav Gupta. Demystifying contrastive self-supervised learning: Invariances, augmentations and dataset biases. _NeurIPS_, 33:3407-3418, 2020.
* [84] Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep representations and image clusters. In _CVPR_, 2016.
* [85] Uno Fang, Man Li, Jianxin Li, Longxiang Gao, Tao Jia, and Yanchun Zhang. A comprehensive survey on multi-view clustering. _IEEE Transactions on Knowledge and Data Engineering_, pages 1-20, 2023.
* [86] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, and Sangdoo Yun. What do self-supervised vision transformers learn? _ICLR_, 2023.
* [87] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In _ICCV_, pages 6202-6211, 2019.
* [88] Longlong Jing, Xiaodong Yang, Jingen Liu, and Yingli Tian. Self-supervised spatiotemporal feature learning via video rotation prediction. _arXiv preprint arXiv:1811.11387_, 2018.
* [89] Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang. Self-supervised spatiotemporal learning via video clip order prediction. In _CVPR_, pages 10334-10343, 2019.

* [90] Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Yunhui Liu, and Wei Liu. Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics. In _CVPR_, pages 4006-4015, 2019.
* [91] Haodong Duan, Nanxuan Zhao, Kai Chen, and Dahua Lin. Transrank: Self-supervised video representation learning via ranking-based transformation recognition. In _CVPR_, pages 3000-3010, 2022.
* [92] Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuffle and learn: unsupervised learning using temporal order verification. In _ECCV_, pages 527-544, 2016.
* [93] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In _CVPRW_, pages 0-0, 2019.
* [94] Dahun Kim, Donghyeon Cho, and In So Kweon. Self-supervised video representation learning with space-time cubic puzzles. In _AAAI_, volume 33, pages 8545-8552, 2019.
* [95] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasminj Bastings, Mark Patrick Collier, Alexey Gritsenko, Vignhesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters, 2023.
* [96] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11976-11986, 2022.
* [97] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In _CVPR_, pages 4613-4623, 2020.
* [98] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [99] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. _arXiv preprint arXiv:1904.09237_, 2019.
* [100] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pages 402-419. Springer, 2020.
* [101] Filip Ilic, Thomas Pock, and Richard P Wildes. Is appearance free action recognition possible? In _ECCV_, pages 156-173. Springer, 2022.
* [102] Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased representations with biased representations. In _International Conference on Machine Learning_, pages 528-539. PMLR, 2020.
* [103] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Z XU, Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentric video-language pretraining. _NeurIPS_, 35:7575-7586, 2022.
* [104] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. _arXiv preprint arXiv:1910.01108_, 2019.
* [105] Shreyank N Gowda, Laura Sevilla-Lara, Kiyoon Kim, Frank Keller, and Marcus Rohrbach. A new split for evaluating true zero-shot action recognition. In _Pattern Recognition_, pages 191-205. Springer, 2022.

* [106] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [107] Harold W Kuhn. The hungarian method for the assignment problem. _Naval research logistics quarterly_, 1955.
* [108] Xiaohan Wang, Tengyu Ma, James Ainooson, Seunghwan Cha, Xiaotian Wang, Azhar Molla, and Maithilee Kunda. Seeing neural networks through a box of toys: The toybox dataset of visual object transformations, 2018.

**Appendix**

The organization of the supplementary material is as follows:

* A: Broader impact
* B: Video self-supervised learning
* B.1 Methods
* B.2 Implementation details
* B.3 Additional insights
* C: Distribution shifts
* C.1 Overview
* C.2 Context shift
* C.3 Viewpoint shift
* C.4 Actor shift
* C.5 Source shift
* C.6 Zero-shot recognition
* C.7 Open-set recognition
* C.8 Toy experiments
* D: Additional experiments and results
* D.1 Pretrained on Kinetics700
* D.2 Temporal dynamics
* D.3 Comparing InD vs. OoD performance under natural distribution shifts
* D.4 Effect of synthetic perturbations
* D.5 Effect of finetuning in out-of-distribution generalization
* D.6 High-level overview
* D.7 Statistical analyses
* D.8 Exploring performance variation of VSSL methods across action classes
Broader impact

In this work, we conduct a thorough study on the behavior of Video Self-supervised Learning (VSSL) methods in the face of distribution shifts. We believe this work has far-reaching implications for advancing the field of video representation learning and its real-world applications. This work is of utmost importance primarily for the following two reasons.

**Real-world deployment:** Video models have a wide range of real-world applications across various domains. These applications include surveillance and security, autonomous vehicles, sports analysis, video content recommendation, human-computer interaction, video captioning and summarization, among many others. However, in real-world scenarios, distribution shifts pose a significant challenge, where the distribution of the data that the system encounters may differ from the training data distribution. Understanding how VSSL algorithms behave under real-world distributional shifts is crucial for ensuring their reliable performance and generalization capabilities for real-world applications. By studying VSSL methods under distribution shifts, we identify the limitations and challenges faced by the popular VSSL methods. We believe our work is a step towards enabling the development of robust and reliable solutions for real-world deployment.

**Algorithmic robustness:** Our study provides valuable insights into the algorithmic robustness of VSSL methods, advancing our understanding of VSSL algorithms. By systematically exploring the behavior of these methods under distribution shifts, we have uncovered intriguing findings and observed interesting behaviors which are previously unknown. Our work not only highlights the strengths of VSSL methods but also identifies their weaknesses in different scenarios. We find that different VSSL methods exhibit varying levels of robustness when confronted with different types of distribution shifts. Some methods demonstrate resilience against certain shifts, while they prove to be vulnerable to others. These findings pave the way for future research to develop robust frameworks that leverage the strengths of different approaches, thereby enhancing the overall performance of VSSL algorithms.

Video self-supervised learning

### Methods

In the following section, we provide a comprehensive overview of our implementation of the VSSL methods explored in this study. We define an encoder \(f(\cdot)\), which is composed of a Transformer backbone \(\theta(\cdot)\) and an MLP projection head \(h(\cdot)\). We further define the auxiliary network components, a target encoder \(f_{t}(\cdot)\) (utilized in \(\bm{v}\)-MoCo, \(\bm{v}\)-BYOL, and \(\bm{v}\)-DINO), a decoder \(f_{d}(\cdot)\) (employed in \(\bm{v}\)-MAE), and a predictor head \(f_{p}(\cdot)\) (utilized in \(\bm{v}\)-BYOL, \(\bm{v}\)-SimSiam, and \(\bm{v}\)-MoCo). Note, \(f_{t}(\cdot)\) and \(f_{d}(\cdot)\) are built on the Transformer architecture, while \(f_{p}(\cdot)\) utilizes an MLP head. For given samples \(\bm{v}\), we generate two augmented views as \(\bm{v}_{1}\) and \(\bm{v}_{2}\), where \(\bm{v}_{1}\) and \(\bm{v}_{2}\) are randomly sampled from different timestamps and differently augmented. Moreover, an augmented view \(\bm{v}_{i}\in\mathbb{R}^{T\times H\times W\times C}\) can be expressed into a sequence of P spatio-temporal patches of size \(t\times h\times w\times C\), where, \(\text{P}=T/t\times H/h\times W/w\). We project the spatio-temporal patches to a linear layer followed by feeding to the encoders. Following, we briefly summarize the training algorithms of these VSSL methods along with their design differences. An overview of these frameworks is presented in Figure S1.

\(\bm{v}\)**-SimCLR.** The objective of \(\bm{v}\)-SimCLR is to maximize the similarity between positive pairs and minimize the similarity between negative pairs using the InfoNCE loss function. Given two augmented views \(v_{1}\) and \(v_{2}\), we obtain \(z_{1}\) and \(z_{2}\) as \(f(v_{1})\) and \(f(v_{2})\). The L2 normalized cosine similarity scores, referred to as \(\mathrm{logits}\), are then calculated as \(\mathrm{sim}(z_{1},z_{2})\), where \(\mathrm{sim}(z_{1},z_{2})=\frac{z_{1}^{\top}z_{2}}{||z_{1}||||z_{2}||}\). The pretraining loss is calculated as \(\mathcal{L}_{simclr}=\mathrm{InfoNCE}(\mathrm{logits})\) as per Equation S1, where \(k+\) refers to the positive embeddings (i.e., embeddings of the augmented views) and \(k-\) refers to the negative embeddings (embeddings of other samples in a minibatch).

\[\mathrm{InfoNCE}(\mathrm{logits})=-\log\frac{\sum_{k\in(k^{+})}\exp(\mathrm{ logits}/\tau)}{\sum_{k\in\{k^{+},k^{-}\}}\exp(\mathrm{logits}/\tau)}\text{, where $\tau$ is temperature.}\] (S1)\(\bm{v}\)**-MoCo.** Similar to \(\bm{v}\)-SimCLR, \(\bm{v}\)-MoCo also calculates InfoNCE loss for given query-key pairs. We obtain the query embeddings as \(p_{1}=f_{p}(f(\bm{v}_{1}))\) and its corresponding key embeddings are obtained as \(zz_{2}=\mathrm{sg}(f_{t}(\bm{v}_{2}))\), where \(\mathrm{sg}\) refers to stop-gradient operation. In Equation S1, the positive and negative key embeddings \(\{k+,k-\}\in zz_{2}\) and the query embedding \(q\in p_{1}\). To calculate a symmetrized loss we also obtain \(p_{2}\) and \(zz_{1}\). The training objective \(\mathcal{L}_{moco}\) is defined as \(\mathrm{InfoNCE}(\mathrm{sim}(p_{1},zz_{2}))+\mathrm{InfoNCE}(\mathrm{sim}(p_{2 },zz_{1}))\). The encoder \(f\) is updated using \(\mathcal{L}_{moco}\), and the target encoder \(f_{t}\) is updated using the exponential moving average (EMA) [56] as \(f_{t}\gets\alpha f+(1-\alpha)f\), where \(\alpha\) is the EMA coefficient.

\(\bm{v}\)**-BYOL.** Different from \(\bm{v}\)-SimCLR and \(\bm{v}\)-MoCo, \(\bm{v}\)-BYOL does not require any negative pairs and only relies on positive views. Therefore, we obtain \(p_{1}\) and \(zz_{2}\) as \(f_{p}(f(\bm{v}_{1}))\) and \(\mathrm{sg}(f_{t}(\bm{v}_{2}))\) to calculate the similarity scores as \(\mathrm{sim}(p_{1},zz_{2})\). Moreover, to obtain a symmetrized loss, we also calculate \(\mathrm{sim}(p_{2},zz_{1})\) and define total loss \(\mathcal{L}_{byl}=-\mathrm{sim}(p_{1},zz_{2})-\mathrm{sim}(p_{2},zz_{1})\). Please note, we use \(\mathcal{L}_{byl}\) to train the \(f\), and the weights of \(f_{t}\) are updated using an EMA similar to \(\bm{v}\)-MoCo.

\(\bm{v}\)**-SimSiam.** The approach for \(\bm{v}\)-SimSiam is very similar to the \(\bm{v}\)-BYOL, except it does not use \(f_{t}\), and directly uses the representations of the \(f\) as the target. We obtain \(p_{1}\) and \(z_{2}\) as \(f_{p}(f(\bm{v}_{1}))\) and \(\mathrm{sg}(f(\bm{v}_{2}))\) to calculate the similarity scores as \(\mathrm{sim}(p_{1},z_{2})\). Similarly, we also obtain \(\mathrm{sim}(p_{2},z_{1})\) and define total loss \(\mathcal{L}_{stimsiam}=-\mathrm{sim}(p_{1},z_{2})-\mathrm{sim}(p_{2},z_{1})\).

\(\bm{v}\)**-DINO.** Similar to \(\bm{v}\)-BYOL and \(\bm{v}\)-MoCo, \(\bm{v}\)-DINO uses a target encoder \(f_{t}(\cdot)\) but does not use a predictor head. We obtain \(z_{1}\) and \(zz_{2}\) as \(f(\bm{v}_{1})\) and \(\mathrm{sg}(f_{t}(\bm{v}_{2})-\mathbf{C})\) respectively, where \(\mathbf{C}\) refers to the Centering operation which can be interpreted as adding a bias term, preventing the model from collapsing [26]. Next, we calculate the cross-entropy (\(\mathrm{CE}\)) error between \(z_{1}\) and \(zz_{2}\) as \(\mathrm{CE}(z_{1},zz_{2})=-\mathrm{softmax}(zz_{2}/\tau_{t})\log(\mathrm{softmax }(z_{1}/\tau_{s}))\), where \(\tau_{t}\) and \(\tau_{s}\) denote the temperatures used to sharpen the representations of the target and base encoder respectively. Finally, we calculate loss \(\mathcal{L}_{dino}=\mathrm{CE}(z_{1}+zz_{2})+\mathrm{CE}(z_{2},zz_{1})\) which is used to update \(f\) and the weights of \(f_{t}\) are updated using EMA similar to \(\bm{v}\)-BYOL and \(\bm{v}\)-MoCo.

\(\bm{v}\)**-MAE.** Unlike the methods mentioned above, \(\bm{v}\)-MAE takes a single view and performs masking and reconstruction using an autoencoder architecture [27; 1; 57]. Moreover, different from the other models, the encoder does not contain a projection head, i.e., \(f=\theta\), instead uses a decoder \(f_{d}\) for reconstruction. For a given sample \(\bm{v}\), we obtain masked tokens \(\bm{v}^{[m]}\) as \(\{\bm{v}^{i}|m^{i}=1\}_{i=1}^{P}\) and the corrupted inputs as \(\hat{\bm{v}}=\{\bm{v}|m^{i}=0\}_{i=1}^{P}\). We calculate the reconstruction loss as \(\mathcal{L}_{mae}=(f_{d}(\theta(\hat{\bm{v}}))-\bm{v}^{[m]})^{2}\), which is used to train the \(\theta\) and \(f_{d}\).

After pretraining using the aforementioned methods, we discard the auxiliary network components and utilize only \(\theta\) for downstream evaluations.

### Implementation details

**Datasets.** We use 2 popular large-scale datasets Kinetics400 [61] and Kinetics700 [62] for pretraining. Kinetics400 consists of \(240\)K samples including \(400\) action classes, whereas, Kinetics700 consists of \(537\)K training samples spread over \(700\) action classes. Following [97; 42], we discard the overlapping classes between Kinetics700 and UCF101 [66], HMDB51 [67] from pretraining, which results in approximately \(480\)K samples spread over \(663\) action classes. We follow such a setup so that the pretrained encoder can be used for zero-shot recognition on full UCF101 and HMDB51.

**Pretraining.** To ensure a fair comparison between the VSSL methods, we pretrain them in identical setups with necessary adjustments in hyperparameters. Specifically, we keep the encoder, inputs, batch size, and maximum number of pretraining epochs identical for all the methods. However, we adjust the data augmentation strategy and auxiliary network components (e.g., predictor head, projector head, decoder) as required to achieve their best performance on 2 validation sets Kinetics400 and UCF101. For example, the projector head of \(\bm{v}\)-DINO is based on the configuration proposed in [26], whereas, the projector head for the other Siamese frameworks is made of an MLP head similar to [24; 25]. The details of the frameworks are presented in Table S1. Considering the widespread use of Transformers in vision, VSSL methods are implemented using ViT-B [75] as the encoder with a patch size of \(4\times 16^{2}\). Videos are downsampled to \(8\) FPS and resized to \(3\times 112^{2}\) frames. Except for \(\bm{v}\)-MAE, all VSSL methods are fed with \(16\) frames in each view and \(32\) frames are fed as input to \(\bm{v}\)-MAE. We use AdamW [98; 99] optimizer with cosine learning rate scheduler and train all methods up to \(800\) epochs with a batch size of \(768\). We present the hyperparameters in Table S3. All the methods are pretrained using 8 V100 32 GB GPUs in parallel. We present the computational specificsin Table S2, which shows that \(\bm{v}\)-MAE is the most computationally efficient framework amongst all the VSSL methods.

### Additional insights

**Aggressive cropping.** As discussed in the main paper, aggressive cropping leads to viewpoint invariance, therefore we experiment with various multi-crop ratios commonly used in the literature [22, 72] across all six VSSL methods. We present the best setups corresponding to individual methods in Table S3. We observe that aggressive cropping negatively impacts the performance of \(\bm{v}\)-MAE. This is primarily because reconstructing a local crop serves as a weak pseudo task due to limited pixel-level variation, resulting in suboptimal representation learning. Additionally, as depicted in Figure S2, the impact of cropping varies between contrastive and non-contrastive methods. Very high aggressive cropping ratios (\(0.08-1.0\)) prove beneficial for contrastive methods, while slightly less aggressive cropping ratios (\(0.2-0.766\)) yield better results for non-contrastive methods.

**Pretraining epoch vs. accuracy.** We track the performance of different videl learning methods on two downstream benchmarks, Kinetics400 and split-1 of UCF101. In particular, Kinetics400 is used for finetuning and UCF101 for linear evaluation. The results are presented in Figure S3 shows that while \(\bm{v}\)-MAE achieves the lowest linear evaluation accuracy, it shows the best performance when finetuned. Moreover, \(\bm{v}\)-BYOL, \(\bm{v}\)-SimCLR, and \(\bm{v}\)-MoCo exhibit superior performance compared to \(\bm{v}\)-DINO and \(\bm{v}\)-SimSiam in both the linear evaluation and finetuning setups. Interestingly, contrastive methods \(\bm{v}\)-SimCLR and \(\bm{v}\)-MoCo show comparable performance among them, however, there is a notable performance gap between the non-contrastive methods. Specifically, \(\bm{v}\)-BYOL outperforms both \(\bm{v}\)-SimSiam and \(\bm{v}\)-DINO by a significant margin in both the linear evaluation and finetuning setups. Moreover, Figure S3 (a) indicates minimal or no improvements between \(600\) to \(800\) epochs. Therefore, we refrain from training the models beyond \(800\) epochs.

**Spatial vs. temporal dependency.** In addition to the out-distribution tests discussed in the main paper, we also conduct several interesting analyses to understand the dependency of these VSSLmethods towards spatial vs. temporal representations. To explore the spatial vs. temporal dependency of the VSSL methods, we use UCF101 as the base validation dataset and create \(3\) synthetic variant of it, (_i_) **No Temporal**: we completely remove the temporal information from the input and treat single frames as an input. In particular, we obtain \(80\) frames per video that represent \(10\) seconds of video sampled at \(8\) FPS. (_ii_) **Partial Appearance**: Next, to suppress the spatial information, we obtain the optical flow [100] from the original videos. (_iii_) **No Appearance**: Following [101], we also obtain the videos of no appearance, i.e., the static frames have no meaning and actions are present in the temporal dimension. We present sample frames of these variants in Figure S4.

The results are presented in Table S4 show several intriguing observations. First, removing the temporal dimension from the videos leads to a significant drop in performance in all the methods. Amongst all the video models, \(\bm{v}\)-MAE suffers the worst in the absence of temporal information in both linear and finetune setups, which shows its overly dependent nature on temporal information and it is a weak spatial learner (also discussed in the main paper). Second, in the absence of temporal information, the frozen encoder outperforms the finetuned variant even in the case of \(\bm{v}\)-Supervised. We also notice the performance of the non-contrastive methods significantly varies under such setups, for example, while \(\bm{v}\)-BYOL achieves the best performance in linear evaluation, \(\bm{v}\)-DINO performs worse other than \(\bm{v}\)-MAE. Additionally, our tests under partial or no appearance reveal that none of the video models is capable of understanding actions just from the temporal information. In fact, the performance of video models drops near chance level ( \(1\%\)) in no appearance setup and achieve slightly better (\(3-7\%\)) in partial appearance setup.

Distribution Shifts

### Overview

We provide a summary of our proposed out-of-distribution test bed in Table S5, and the details of our setup are described in the following subsections. Due to the large number of experiments conducted across various setups, we will release configuration files for individual evaluation on the project website rather than sharing them here.

### Context shift

We use the two splits of Mimetics (i.e., Mimetics10 [102] and Mimetics50 [28]) as the OoD validation benchmark to study video models under context shift. Mimetics consists of a subset of action classes from Kinetics400, where the context in the videos is either partial or misleading or completely absent. We train the pretrained encoders using videos of corresponding action classes from Kinetics400, and subsequently perform InD and OoD validations using Kinetics400 and Mimetics, respectively. For linear evaluation, we extract the fixed features by taking the mean of all the patches from the last layer of the encoder. We then employ an SVM with a linear kernel for classification. In the finetuning stage, we add a fully-connected layer on top of the encoder and train it end-to-end using the AdamW optimizer and cross-entropy error. The top-1 accuracy is reported for both InD and OoD validation. Please note, the different VSSL methods are individually tuned to achieve their best performance.

### Viewpoint shift

We study three types of viewpoint shifts, (_i_) egocentric view, (_ii_) surveillance camera view, and (_iii_) top-down view. We use CharadesEgo [36], TinyVirat-v2 [63], and Sims4Action [64] as the OoD benchmark for egocentric view, low-resolution surveillance camera view, and top-down view respectively. CharadesEgo is a collection of paired third-person and first-person videos of human activities. Therefore, we use the corresponding third-person videos from CharadesEgo [36] for InD training and validation split while the egocentric view is used as OoD validation. We use MiT-v2 [69] to create the corresponding InD split for TinyVirat-v2 and Sims4Action [64]. MiT-v2 is a large-scale video benchmark of 1M videos spread over \(339\) different human actions. We use the videos of overlapping classes between MiT-v2 and TinyVirat-v2 to study surveillance camera viewpoint shift.

Similarly, the videos of overlapping classes between MiT-v2 and Sims4Action are used to study top-down viewpoint shifts. The class assignments between InD and OoD benchmarks are mentioned at the end of this subsection.

For training the models on CharadesEgo, we adopt the approach proposed in [103]. In this setup, instead of using one-hot encoded labels, we utilize the text descriptions associated with the videos. We generate tokens from these descriptions and train a video-text encoder using an InfoNCE objective function [55]. In particular, we use the DistilBERT base [104] as the text encoder along with the pretrained ViT-B is used as the video encoder. We redirect the readers to [103], to know more about the training scheme of CharadesEgo. In the case of linear evaluation, the above-mentioned setup remains the same, except we keep the video encoder frozen. Moreover, our training strategy with MiT-v2 to study surveillance camera viewpoint and top-down viewpoint shifts follow a standard finetuning setup using cross-entropy error [72; 2; 4; 6; 78; 79; 7]. However, due to the difference in the nature of the datasets, as TinyVirat contains multiple actions per video, whereas, MiT-v2 consists of single actions per video, we make an adjustment in the prediction layer. Instead of using a Softmax layer, we utilize a Sigmoid layer to predict multiple possible outputs for a given input video. We set the threshold for classification at 0.5. Similarly, the setup for linear evaluation on TinyVirat remains the same, except the video encoder is kept frozen. Next, in the case of Sims4Action, we use SVM for linear evaluation. We report mean average precision and F1-score for CharadesEgo and TinyVirat, respectively, and top-1 accuracy for the rest of them.

**TinyVirat-v2 and MiT-v2.** opening: opening, pull: pulling, activity_carrying: carrying, entering: entering, exiting: exiting, loading: loading, talking: talking, activity_running: running, riding: riding, closing: closing, activity_walking: walking, push: pushing, activity_standing: standing, specialized_talking_phone: telephoning.

**Sims4Action and MiT-v2.** cook: cooking, drink: drinking, eat: eating, read_book: reading, use_phone: telephoning, walk: walking.

### Actor shift

We study actor shift in two setups, i.e., animal domain and synthetic domain. We use the ActorShift [65] as the OoD validation set to test generalizability on animal actions, while using the video of overlapping classes from Kinetics700 [62] as InD training and validation. We use SVM for linear evaluation and follow standard techniques [72; 2; 4; 6; 78; 79; 7] as discussed earlier for finetuning. We report top-1 accuracy for both InD and OoD validation. We present the overlapping classes between Kinetics700 and ActorShift at the end of this subsection. The setup for synthetic domain shift is already discussed in the previous subsection (Viewpoint shift).

**ActorShift and Kinetics700.** sleeping, watching tv, eating, drinking, swimming, running, opening door.

### Source shift

To study source shift, we use the videos from the common classes of UCF101 and HMDB51. While using the UCF as the training split, we use the corresponding videos from HMDB as the OoD validation set and vice versa. Following standard protocol [72; 2; 4; 6; 78; 79; 7], we use SVM for linear evaluation and the pretrained encoder along with a fully-connected layer for finetuning. We report the top-1 accuracy for both setups. The class assignments are as follows.

**UCF101 and HMDB51.** FrisbeeCatch: catch, RockClimbingIndoor: climb, Diving: dive, (BasketballDunk, Basketball): dribble, Fencing: fencing, GolfSwing: golf, (HandstandPushups, Handstand-Walking): handstand, (LongJump, JumpingJack): jump, SoccerPenalty: kick_ball, PullUps: pullup, Punch: punch, PushUps: pushup, Biking: ride_bike, HorseRiding: ride_horse, ThrowDiscus: throw, WalkingWithDog: walk, Archery: shoot_bow,

### Zero-shot

We perform zero-shot in two setups: (1) we train the video-text encoder on Kinetics400, while the non-overlapping classes from UCF101 and HMDB51 are used for OoD validation [105]. (2) We train the models using the videos from Kinetics700 that do not share classes with UCF101 and HMDB51,while the full UCF101 and HMDB51 are used for OoD validation. Moreover, in both cases, we also use RareAct as the OoD benchmark, which consists of rare or unusual video actions.

Since, the pretraining of VSSL methods does not employ language, first, a video-text encoder is jointly trained to learn the mapping between video and text representations. Following [42], we use Word2Vec [80] to extract the word embeddings from the labels, which are then fed to a text encoder consists of an MLP head. We measure the similarity between the video and text pairs as the dot product of L2 normalized video-text embeddings, referred to as logits. Next, the video text encoders are trained to minimize the Softmax cross-entropy error using the obtained logits and the class labels. We redirect readers to [42] for additional details of the training method. Please note, we also try with a stronger language model (e.g., BERT[106], DistilBERT[104]) than Word2Vec, however, it does not improve the performance, in fact, slightly worsens it. Since the labels in Kinetics are typically composed of only a few words (mostly less than 2), the average pooled word embeddings work well in such cases. This finding is consistent with the observations of [42]. Moreover, we perform zero-shot in both setups when the pretrained video encoder is kept frozen and finetuned in an end-to-end manner.

### Open-set

We perform open-set recognition in two setups: (1) We use the Kinetics400 for closed-set recognition, while the non-overlapping classes from the UCF101 and HMDB51 are used for open-set, similar splits that have been used in zero-shot. (2) We use UCF101 and the non-overlapping classes of HMDB for closed-set and open-set recognition, respectively. We obtain the non-overlapping classes by manual inspection, presented at the end of this subsection.

To enable the models for open-set recognition, we follow [45] and train the pretrained encoders with the DEAR loss which aims to calibrate the uncertainty to ensure the models show high uncertainty towards the unknown and low uncertainty towards the known. We redirect readers to [45] for more information about open-set training. We conduct open-set recognition in both finetuned and linear evaluation setups. We observe that both supervised training and linear evaluation setups using VSSL encoders did not converge when Kinetics400 was used for closed set recognition. This is likely because the models tend to become over-confident due to the large number of known classes in Kinetics400 compared to the unknowns. To measure the models' performance we report the area under the ROC curve (AUC) for open-set recognition and accuracy for closed-set recognition.

**HMDB51.** The non-overlapping classes from HMDB51 used in open-set recognition while using UCF101 as the closed-set: brush_hair, cartwheel, chew, clap, climb_stairs, drink, eat, fall_floor, flic_flac, hit, hug, kick, kiss, laugh, pick, pour, push, run, shake_hands, shoot_ball, shoot_bow, shoot_gun, sit, situp, smile, smoke, somersault, stand, swing_baseball, sword_exercise, talk, turn, wave.

### Toy experiments

In all of our toy experiments, we use the pretrained frozen encoders to extract the fixed representations, which are then used for K-means clustering2. Additionally, we apply the Hungarian algorithm [107] for assigning the formed cluster to the targets and measure the accuracy. The details of the datasets used to conduct the toy experiments are as follows.

Footnote 2: sklearn.cluster.KMeans

#### c.8.1 ToyBox

The ToyBox [108] consists of egocentric views of \(9\) distinct temporal transformations of different toy objects. The transformations include positive rotations, negative rotations, and translations across the \(x\), \(y\), and \(z\) axis. A few representative frames illustrating such transformations are presented in Figure S5. Using the pretrained frozen encoders, we aim to correctly cluster different transformations across various objects. Moreover, as the static spatial information is non-discriminative to the temporal transformations, we aim to form clusters solely based on temporal representations. The main motivation behind this experiment is to disentangle spatial representations from the embedding space to independently evaluate the models on their ability to learn temporal dynamics.

#### c.8.2 Coil100

We use COIL100 [34] to study viewpoint invariance which consists of images of different objects captured from different camera angles. We particularly choose an image-based dataset for this experiment to restrict any discriminatory temporal features in the embedding space that may attribute to the outcome of the models' prediction. We present a few representative frames in Figure S6

#### c.8.3 Stl10

We use an image dataset STL10 [71] to test the robustness of the pretrained encoder against low-resolution inputs. We systematically reduce the resolution from \(112^{2}\) to \(16^{2}\), please see a few examples in Figure S7. Similar to our viewpoint shift setup, we choose an image dataset to restrict any influence of temporal representations in the outcome.

### License of Dataset

In Table S6, we summarize the licensing terms for the datasets used in this study.

Figure S7: Samples from STL10 presenting low-resolution inputs.

\begin{tabular}{l l l} \hline \hline
**Dataset** & **License** & **Link** \\ \hline CharadesEgo & Non-commercial use & https://prior.allenai.org/projects/ \\  & & charades-ego \\ Moments-in-Time-v2 & Non-commercial use & http://moments.csail.mit.edu/ \\ Kinetics & CC BY 4.0 & https://www.deepmind.com/open-source/ \\  & & kinetics \\ HMDB51 & CC BY 4.0 & https://serre-lab.clps. \\  & & brown.edu/resource/ \\  & & hmdb-a-large-human-motion-database/ \\ ToyBox & CC BY 4.0 & https://aivaslab.github.io/toybox/ \\ Mimetics & Open access & https://europe.naverlabs.com/research/ \\  & & computer-vision/mimetics/ \\ UCF101 & Open access & https://www.crcv.ucf.edu/data/UCF101. \\ TinyVirat-v2 & Open access & https://www.crcv.ucf.edu/ \\  & & tiny-actions-challenge-cvpr2021/ \\  & & \#tabtwo \\ COIL100 & Open access & https://www.cs.columbia.edu/CAVE/ \\  & & software/softlib/coil-100.php \\ STL-10 & Open access & https://cs.stanford.edu/~acoates/ \\ ActorShift & MIT & https://uvaauas.figshare.com/articles/ \\  & & dataset/ActorShift_zip/19387046 \\ Sims4Action & MIT & https://github.com/aroitberg/ \\ RareAct & Apache & https://github.com/antoine77340/ \\  & & RareAct \\ \hline \hline \end{tabular}
\end{table}
Table S6: Licensing terms for the datasets used in this study.

Additional Experiments and Results

### Pretrained on Kinetics700

In addition to the Kinetics400 used in the main paper, we further utilize the Kinetics700 to evaluate the benefits of incorporating more diverse data for out-of-distribution generalization. We use a subset of Kinetics700 comprising 480K samples, which doubles the size of Kinetics400. We use this subset to pretrain the VSSL methods for the same number of total iterations as Kinetics400. Unless stated otherwise, we compare the performance between Kinetics400 and Kinetics700 pretraining in linear evaluation, averaging over \(3\) trials. In Table S7 and Figure S8, we present a high-level overview showing the impact of using more diverse data in pretraining. Table S7 highlights that non-contrastive methods (\(\bm{v}\)-BYOL, \(\bm{v}\)-SimSiam, \(\bm{v}\)-DINO) benefit more from the inclusion of diverse data compared to the contrastive approaches (\(\bm{v}\)-SimCLR, \(\bm{v}\)-MoCo) and \(\bm{v}\)-MAE. Interestingly, we observe a slight decrease in the performance of \(\bm{v}\)-MAE in this setting. This could be attributed to several factors, such as the pretrained encoder ViT-B reaching saturation in the pretraining setup of \(\bm{v}\)-MAE, resulting in no improvements in performance from the additional data. To address this, a larger backbone like ViT-L or ViT-H, as used in prior works [57, 1], could be explored. Unfortunately, resource limitations prevented us from adopting a larger network to test this. Detailed results are presented in Tables S8, S9, S10, S11 and S12.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{4}{c}{**UCF/HMDB**} & \multicolumn{4}{c}{**HMDB/UCF**} \\ \cline{2-10}  & \multicolumn{2}{c}{Kinetics400} & \multicolumn{2}{c}{Kinetics700} & \multicolumn{2}{c}{Kinetics400} & \multicolumn{2}{c}{Kinetics400} & \multicolumn{2}{c}{Kinetics700} \\ \cline{2-10}  & \multicolumn{2}{c}{OoD(H) InD(U)} & \multicolumn{2}{c}{OoD(H) InD(U)} & \multicolumn{2}{c}{OoD(U)} & \multicolumn{2}{c}{InD(H)} & \multicolumn{2}{c}{OoD(U)} & \multicolumn{2}{c}{InD(H)} \\ \hline \hline \(\bm{v}\)-SimCLR & 47.7 & 96.0 & 45.1 & 96.5 & 55.1 & 82.0 & 55.1 & 83.6 \\ \(\bm{v}\)-MoCo & **51.5** & 97.1 & 48.3 & 96.3 & 57.2 & 84.5 & 54.6 & 82.6 \\ \(\bm{v}\)-BYOL & 51.4 & 94.5 & **50.5** & 96.8 & **63.1** & 79.9 & **61.1** & 82.8 \\ \(\bm{v}\)-SimSiam & 46.1 & 92.6 & 48.0 & 92.5 & **52.2** & 76.2 & **53.1** & 80.5 \\ \(\bm{v}\)-DINO & 49.3 & 94.3 & 50.3 & 95.8 & **53.8** & 77.5 & 58.2 & 81.8 \\ \(\bm{v}\)-MAE & 39.5 & 89.2 & **39.5** & 89.3 & **39.1** & 72.8 & **37.5** & 72.4 \\ \hline \hline \end{tabular}
\end{table}
Table S8: Comparison of video models under **context** shift when pretrained with Kinetics400 (#\(240\)K) vs. Kinetics700 (#\(480\)K). We highlight the best result in each scenario in **bold**. Interestingly, we observe a consistent benefit from the inclusion of more diverse data in the InD validation sets of both splits. However, the improvements in OoD are not consistently observed.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{4}{c}{**Cunet (10 class)**} & \multicolumn{4}{c}{**Context (50 class)**} \\ \cline{2-10}  & Kinetics400 & Kinetics700 & Kinetics400 & Kinetics700 \\ \cline{2-10}  & OoD InD & OoD InD & OoD InD & OoD InD & OoD InD & OoD InD & OoD InD \\ \hline \hline \(\bm{v}\)-SimCLR & 31.4 & 92.5 & 30.2 & 91.2 & **14.9** & 71.7 & 15.3 & 73.3 \\ \(\bm{v}\)-MoCo & 29.7 & 92.2 & 27.7 & 93.1 & **15.0** & 74.0 & 14.1 & 75.0 \\ \(\bm{v}\)-BYOL & 31.6 & 89.3 & **32.4** & 91.4 & 14.4 & 71.8 & **15.8** & 75.2 \\ \(\bm{v}\)-SimSiam & 30.8 & 89.5 & **33.6** & 91.4 & **13.8** & 67.9 & 14.7 & 70.3 \\ \(\bm{v}\)-DINO & **34.8** & 90.4 & **34.3** & 91.0 & 13.0 & 68.7 & 14.5 & 72.3 \\ \(\bm{v}\)-MAE & 33.3 & 82.9 & **31.9** & 83.4 & **12.3** & 56.4 & 12.7 & 54.5 \\ \hline \hline \end{tabular}
\end{table}
Table S9: Comparison of video models under **viewpoint** and **actor** shifts, when pretrained with Kinetics400 (#\(240\)K) vs. Kinetics700 (#\(480\)K). We highlight the best results in each scenario in **bold**. The results demonstrate that adding diverse pretraining data significantly improves the performance under egocentric viewpoint shifts and top-down synthetic domain shifts. However, less significant improvements are observed for surveillance viewpoint shifts and animal domain actor shifts. Moreover, the performance of \(\bm{v}\)-MAE deteriorates when pretrained with Kinetics700.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c}{**UCF**} & \multicolumn{2}{c}{**HMDB**} & \multicolumn{2}{c}{**RareAct**} & \multicolumn{2}{c}{Kinetics400 (InD)} \\ \cline{2-7}  & K400 & K700 & K400 & K700 & K400 & K700 & K400 & K700 \\ \hline \hline \(\bm{\upsilon}\)-SimCLR & **37.2** & 39.0 & 18.6 & 19.0 & 7.7 & 9.4 & 56.8 & 57.8 \\ \(\bm{\upsilon}\)-MoCo & 35.2 & 40.6 & 19.5 & **21.6** & **8.7** & 9.4 & 58.4 & 59.5 \\ \(\bm{\upsilon}\)-BYOL & 33.0 & **44.0** & **22.4** & 21.1 & 7.5 & **9.9** & 57.4 & 61.0 \\ \(\bm{\upsilon}\)-SimSiam & 34.0 & 39.2 & 18.8 & 20.3 & 7.7 & 8.3 & 50.4 & 52.3 \\ \(\bm{\upsilon}\)-DINO & 34.3 & 41.1 & 17.2 & 20.0 & 8.1 & 9.5 & 53.4 & 58.3 \\ \(\bm{\upsilon}\)-MAE & 25.5 & 24.1 & 14.2 & 17.2 & 5.8 & 6.2 & 35.9 & 35.4 \\ \hline \hline \end{tabular}
\end{table}
Table S12: Comparison of video models in **open-set** recognition, when pretrained with Kinetics400 (#\(240\)K) vs. Kinetics700 (#\(480\)K). We highlight the best results in each scenario in **bold**. The results exhibit a very similar trend in both pretraining setups. For example, while \(\bm{\upsilon}\)-SimCLR and \(\bm{\upsilon}\)-MoCo perform fairly well in closed-set, they show near chance-level performance in open-set. \(\bm{\upsilon}\)-MAE performs poorly in both open-set and closed-set, in both pretraining setups. \(\bm{\upsilon}\)-DINO achieves the best open-set performance in both setups while retaining decent closed-set performance.

Figure S8: We present a high-level depiction of the models’ performance when **pretrained with Kinetics400 vs. Kinetics700**. When pretrained with Kinetics700, the trend line of InD performance (blue dashed line) is slightly higher than that of the OoD performance (brown dashed line).

### Temporal dynamics

In this subsection, we provide the additional results of our experiment on the temporal dynamics of VSSL methods. We present the performance on temporal transformation recognition in Figure S9. Moreover, we present the performance of VSSL methods in classifying the same objects from static videos, in Figure S10.

Figure S9: Evaluating performance of video models on egocentric **transformation recognition** using ToyBox [70]. The models are used to classify a total of 9 types of temporal transformations including positive rotations, negative rotations, and transformations across the \(x\), \(y\), and \(z\) axes. In all of the cases, \(\bm{v}\)-MAE  consistently shows superior performance.

Figure S10: Evaluating performance of video models in egocentric **object recognition**. The models are used to classify the videos of static objects including 4 types of animals, household items, cars, and all of them together (a total of 12 classes). In all cases \(\bm{v}\)-MAE consistently exhibits the lowest performance. Among all the categories, the models perform significantly better in classifying household objects than animals or vehicles. This is likely because the pretraining data Kinetics400 consist of several human actions involving household objects e.g., kitchen utensils, etc.

### Comparing InD vs. OoD performance under natural distribution shifts

To gain a high-level understanding of the models' performance in InD vs. OoD, we visualize their accuracy in a 2D space in Figure S11. It is noteworthy that superior InD performance does not necessarily translate to better OoD performance. We observe cases where models exhibit similar InD performance but significantly differ in their performance under distribution shifts. Ablation variants of Figure S11 are presented in Figure S12. Statistical analyses on the robustness of video models are presented in Appendix D.7.

Figure S12: An ablated view of **InD vs. OoD** performance under natural distribution shifts, utilizing both frozen (empty markers) and finetuned (filled markers) encoders. Here, \(y=mx+c\) shows a linear fit for the given data points (i.e., projected InD and OoD performance metrics). The data points above linear fit indicate more robustness. (**a and b**) \(\bm{v}\)-Supervised demonstrates superior performance in linear evaluation, while \(\bm{v}\)-MAE achieves the best results when finetuned. On the other hand, although \(\bm{v}\)-MoCo and \(\bm{v}\)-SimCLR show strong performance in InD validation, they exhibit weaker generalization in out-of-context scenarios. (**c, d, and e**) Overall, \(\bm{v}\)-SimCLR and \(\bm{v}\)-MoCo perform better in viewpoint shifts in both linear and finetuning. \(\bm{v}\)-MAE (finetuned) shows a single instance of superior results for the specific case of low-resolution surveillance camera shift due to its robustness in low-resolution inputs. (**f**) In animal domain actor shift, \(\bm{v}\)-BYOL achieves the best results in linear evaluation, whereas, \(\bm{v}\)-SimCLR outperforms others when finetuned. (**g and h**) \(\bm{v}\)-BYOL achieves the best performance under source shifts in all setups. (**i and j**) Frozen \(\bm{v}\)-BYOL achieves the best zero-shot recognition on UCF but performs poorly on HMDB. On the other hand, the frozen \(\bm{v}\)-SimCLR performs the best on HMDB, while generalizing poorly on UCF. (**k**) Overall, video models generalize poorly in zero-shot recognition of unusual actions (RareAct).

### Effect of synthetic perturbations

In addition to the natural distribution shifts, we further extend our work investigating the performance of VSSL methods under synthetic perturbations. We follow the setup proposed in [53] to create the synthetic perturbation of varying severity on a scale of \(1\) to \(5\). We apply a total of \(16\) different synthetic perturbations belonging to the common spatio-temporal augmentation techniques such as noise addition, blurring, temporal perturbation, and camera motion. We test the robustness of the frozen encoders against synthetic perturbations on \(2\) benchmarks UCF101 and Kinetics400, presented in Figures S13 and S14 respectively. The results demonstrate that with increasing perturbation severity, the performance of video models deteriorates. However, it is important to acknowledge that certain visual augmentations, such as noise and blur, are already applied during the self-supervised pretraining phase. As a result, they may not be considered as distribution shifts within the scope of our setup. Furthermore, it is neither possible to train self-supervised methods without such augmentation techniques. Despite these considerations, we present these results for the sake of completeness.

Since spatial perturbations are extensively covered in literature, we refrain from explaining them here and redirect readers to [53, 54, 49]. Instead, we briefly discuss the temporal perturbations [53]. 'Sampling': using different frame rates than used in training; 'Reverse sampling': reversing the temporal order with different sampling rates; 'Jumble': shuffling frames in segments; 'Box jumble': shuffling the segments instead of frames; 'Freezing': randomly freezing video frames; 'Random shuffle': randomly shuffle the frame orders, please note random shuffle has just one severity level.

Figure S13: Spatio-temporal perturbations on UCF101 (linear evaluation). When subjected to extreme perturbations, the performance of VSSL methods experiences a significant decline, reaching near-chance levels. Notably, \(\bm{v}\)-MAE exhibits the worst performance across all setups. Among all methods, \(\bm{v}\)-BYOL demonstrates superiority in scenarios involving blur and temporal perturbations, while \(\bm{v}\)-MoCo outperforms others in rotations and translations. Furthermore, \(\bm{v}\)-DINO excels when tested on noisy videos.

Figure S14: Spatio-temporal perturbations on Kinetics400 (linear evaluation). When subjected to extreme perturbations, the performance of VSSL methods experiences a significant decline, reaching near-chance levels. Notably, \(\bm{v}\)-MAE exhibits the worst performance across all setups. Among all methods, \(\bm{v}\)-MoCo demonstrates superiority in scenarios involving blur, rotations, and temporal perturbations, while \(\bm{v}\)-BYOL outperforming the others when tested on noisy videos.

[MISSING_PAGE_FAIL:41]

Figure S15: A detailed comparison of the impact of **finetuning on InD vs. OoD**. For optimal viewing, please rotate 90 degrees to the left. The results demonstrate that the benefits of finetuning are highly dependent on the VSSL method and type of distribution shift. Our analysis reveals that finetuning tends to be more advantageous in scenarios involving actor shifts (animal domain and synthetic domain). Conversely, its benefits are relatively less pronounced under viewpoint shifts and zero-shot recognition. Moreover, the impact of finetuning in source shift and context shift is mixed. For example, while we notice significant benefits in UCF to HMDB shift, finetuning in fact worsens the performance in the case of HMDB to UCF shift. Additionally, finetuning is more beneficial when evaluated on a smaller benchmark with just 10 out-of-context classes. However, the improvements are relatively modest in a more challenging setup involving 50 out-of-context classes.

Figure S16: Comparing the performance of video models in **linear evaluation vs. finetuning**. In the plot, the filled markers represent InD results, while the empty markers represent the OoD results. Overall, we observe that finetuning consistently leads to improved performance, as indicated by the data points lying above the diagonal line (\(y=x\)), with only one exception. It is worth noting that the average improvement in InD performance (shown by the blue dashed line) is higher compared to the improvement in OoD performance (shown by the brown dashed line).

Figure S17: An ablated view of **linear vs. finetuning** performance comparison. Our results demonstrate that improved InD performance does not necessarily translate to better OoD performance. We observe cases where models exhibit similar InD performance but significantly differ in their performance under distribution shifts, e.g., please see the linear results in d, e, f, and i.

#### d.5.2 In-distribution overfitting due to finetuning

As mentioned in the main paper, while finetuning enhances overall performance, it is susceptible to in-distribution overfitting. In other words, while additional training leads to improved performance on InD validation, it adversely affects OoD performance. We present a few examples of such instances in Figure S18. In practice, we apply early stopping to tackle such overfitting.

### High-level overview

The high-level overview presented in Figure 1 summarizes our findings from Tables 1, 2, 3, 4, and 6(a) of the main paper. To create the overview plot, we first normalize the OoD performance (e.g., accuracy, mAP) of each experiment to that of the best-performing method. Thus, the best performing method gets \(1.0\) and all other methods get a score in \([0,1]\). Next, we group the experiments based on their super-categories and then take their average score. For example, in the case of source shift, we compute this average by grouping the normalized scores of \(4\) experiments, namely linear and finetuning evaluation of both UCF/HMDB and HMDB/UCF shifts. We follow similar steps for all the shifts (details are given below), resulting in a \(7\times 6\) matrix, where \(7\) represents the number of video methods, and \(6\) represents the total number of shifts. This final matrix is then used to color-code and present a high-level summary of all methods across all shifts.

* Context shift: Table 1
* Context shift (10 classes) linear; best performing method: \(\bm{v}\)-Supervised
* Context shift (10 classes) finetune; best performing methods: \(\bm{v}\)-Supervised, \(\bm{v}\)-BYOL, \(\bm{v}\)-MAE
* Context shift (50 classes) linear; best performing methods: \(\bm{v}\)-Supervised, \(\bm{v}\)-MoCo
* Context shift (50 classes) finetune; best performing method: \(\bm{v}\)-MAE
* Viewpoint shift: Table 2
* Viewpoint shift (egocentric) linear; best performing method: \(\bm{v}\)-MoCo
* Viewpoint shift (egocentric) finetune; best performing method: \(\bm{v}\)-MoCo
* Viewpoint shift (surveillance+low resolution) linear; best performing method: \(\bm{v}\)-SimCLR
* Viewpoint shift (surveillance+low resolution) finetune; best performing method: \(\bm{v}\)-MAE
* Viewpoint+actor shift (top-down+synthetic) linear; best performing method: \(\bm{v}\)-SimCLR
* Viewpoint+actor shift (top-down+synthetic) finetune; best performing method: \(\bm{v}\)-SimCLR
* Actor shift: Table 2
* Viewpoint+actor shift (top-down+synthetic) linear; best performing method: \(\bm{v}\)-SimCLR
* Viewpoint+actor shift (top-down+synthetic) finetune; best performing method: \(\bm{v}\)-SimCLR
* Actor shift (animal) linear; best performing method: \(\bm{v}\)-BYOL
* Actor shift (animal) finetune; best performing method: \(\bm{v}\)-SimCLR
* Source shift: Table 3
* Source shift (UCF/HMDB) linear; best performing method: \(\bm{v}\)-MoCo
* Source shift (UCF/HMDB) finetune; best performing method: \(\bm{v}\)-BYOL
* Source shift (HMDB/UCF) linear; best performing method: \(\bm{v}\)-BYOL
* Source shift (HMDB/UCF) finetune; best performing method: \(\bm{v}\)-BYOL
* Zero-shot recognition: Table 4
* Zero-shot (UCF) linear; best performing method: \(\bm{v}\)-SimCLR
* Zero-shot (UCF) finetune; best performing method: \(\bm{v}\)-MoCo
* Zero-shot (HMDB) linear; best performing method: \(\bm{v}\)-BYOL
* Zero-shot (HMDB) finetune; best performing method: \(\bm{v}\)-MAE
* Zero-shot (RareAct) linear; best performing method: \(\bm{v}\)-MoCo
* Zero-shot (RareAct) finetune; best performing method: \(\bm{v}\)-SimSiam
* Open-set recognition: Table 6(a)
* Open-set (K400/UCF) finetune; best performing method: \(\bm{v}\)-SimCLR
* Open-set (K400/HMDB) finetune; best performing method: \(\bm{v}\)-MoCo
* Open-set (U101/HHDB) finetune; best performing method: \(\bm{v}\)-MoCo

[MISSING_PAGE_FAIL:47]

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Distribution shift** & **Method 1** & **Method 2** & \multicolumn{2}{c}{**Adjusted \(\Delta_{\text{OOD}}\)**} \\ \cline{3-5}  & & **Lin.** & **FT.** \\ \hline \hline \multirow{8}{*}{Viewpoint (ego.)} & \(\bm{v}\)-SimCLR & \(\bm{v}\)-BYOL & 0.21 & 0.20 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-SimSiam & 0.35 & 1.28 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-DINO & 0.24 & 1.16 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-Supervised & 0.71 & 1.44 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-MAE & 0.65 & 2.01 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-BYOL & 0.56 & 0.69 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-SimSiam & 0.71 & 1.78 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-DINO & 0.60 & 1.66 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-Supervised & 1.06 & 1.94 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-MAE & 1.00 & 2.51 \\ \hline \multirow{8}{*}{Viewpoint (surv.+low res)} & \(\bm{v}\)-SimCLR & \(\bm{v}\)-BYOL & 2.50 & 2.77 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-SimSiam & 0.98 & 0.51 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-DINO & 2.22 & 1.84 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-Supervised & 0.89 & 0.10 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-MAE & 0.21 & -0.16 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-BYOL & 0.49 & 1.44 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-SimSiam & -1.03 & -0.81 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-DINO & 0.22 & 0.52 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-Supervised & -1.12 & -1.22 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-MAE & -1.79 & -1.48 \\ \hline \multirow{8}{*}{View+Act (t-down+syn.)} & \(\bm{v}\)-SimCLR & \(\bm{v}\)-BYOL & 1.87 & 6.30 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-SimSiam & -0.88 & 6.70 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-DINO & 2.06 & 11.5 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-Supervised & 8.45 & 16.0 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-MAE & -3.93 & -0.60 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-BYOL & 1.81 & 4.80 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-SimSiam & -0.94 & 5.20 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-DINO & 2.00 & 10.0 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-Supervised & 8.39 & 14.5 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-MAE & -3.99 & -2.10 \\ \hline \hline \end{tabular}
\end{table}
Table S16: Comparative statistical analysis of the robustness of \(\bm{v}\)-MAE  and \(\bm{v}\)-Supervised  in learning **temporal dynamics** (original numbers from Figure 3(a)).

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Distribution shift** & **Method 1** & **Method 2** & **Adjusted \(\bm{\Delta_{\mathrm{OOD}}}\)** \\ \hline \multirow{8}{*}{View+Act (t-down+syn.)} & \(\bm{v}\)-Supervised & \(\bm{v}\)-BYOL & -7.11 & -9.70 \\  & \(\bm{v}\)-Supervised & \(\bm{v}\)-SimSiam & -9.87 & -9.30 \\  & \(\bm{v}\)-Supervised & \(\bm{v}\)-DINO & -6.92 & -4.50 \\  & \(\bm{v}\)-Supervised & \(\bm{v}\)-SimCLR & -10.6 & -16.0 \\  & \(\bm{v}\)-Supervised & \(\bm{v}\)-MoCo & -9.26 & -14.5 \\  & \(\bm{v}\)-Supervised & \(\bm{v}\)-MAE & -12.9 & -16.6 \\ \hline \hline \end{tabular}
\end{table}
Table S18: Comparative statistical analysis of the robustness of contrastive methods (\(\bm{v}\)-SimCLR  and \(\bm{v}\)-MoCo) in learning **viewpoint invariance** (original numbers from Figure 4(a)).

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Distribution shift** & **Method 1** & **Method 2** & \multicolumn{2}{c}{**Adjusted \(\mathbf{\Delta_{OOD}}\)**} \\ \hline \hline \multirow{8}{*}{Kinetics400/UCF} & \(\bm{v}\)-SimCLR & \(\bm{v}\)-BYOL & 2.44 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-SimSiam & 2.42 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-DINO & 2.94 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-MAE & 7.85 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-BYOL & 0.72 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-SimSiam & 0.70 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-DINO & 1.22 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-MAE & 6.13 \\ \hline \multirow{8}{*}{Kinetics400/HMDB} & \(\bm{v}\)-SimCLR & \(\bm{v}\)-BYOL & 0.04 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-SimSiam & 2.12 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-DINO & 1.03 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-MAE & 5.09 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-BYOL & 0.76 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-SimSiam & 2.84 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-DINO & 1.75 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-MAE & 5.82 \\ \hline \multirow{8}{*}{UCF101/HMDB} & \(\bm{v}\)-SimCLR & \(\bm{v}\)-BYOL & 2.58 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-SimSiam & 4.29 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-DINO & 1.31 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-MAE & 11.38 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-BYOL & 3.20 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-SimSiam & 4.91 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-DINO & 1.94 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-MAE & 12.0 \\ \hline \hline \end{tabular}
\end{table}
Table S21: Comparative statistical analysis of the robustness of \(\bm{v}\)-BYOL under **source shift** (original numbers from Table 3).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{**Distribution shift**} & \multirow{2}{*}{**Method 1**} & \multirow{2}{*}{**Method 2**} & \multicolumn{2}{c}{**Adjusted \(\mathbf{\Delta_{OOD}}\)**} \\ \cline{3-5}  & & & **Lin.** & **FT.** \\ \hline \hline \multirow{7}{*}{UCF101/HMDB} & \(\bm{v}\)-BYOL & \(\bm{v}\)-SimSiam & -5.02 & -0.24 \\  & \(\bm{v}\)-BYOL & \(\bm{v}\)-DINO & -4.01 & -2.13 \\  & \(\bm{v}\)-BYOL & \(\bm{v}\)-MAE & -2.89 & -1.56 \\  & \(\bm{v}\)-BYOL & \(\bm{v}\)-SimCLR & -5.37 & 0.95 \\  & \(\bm{v}\)-BYOL & \(\bm{v}\)-MoCo & -2.70 & -5.30 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-BYOL & 2.31 & 0.24 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-DINO & -0.42 & -1.89 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-MAE & 0.70 & -1.32 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-SimCLR & -1.79 & 1.19 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-MoCo & 0.89 & -5.06 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-BYOL & 1.78 & 2.13 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-SimSiam & -1.96 & 1.89 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-MAE & 0.17 & 0.57 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-SimCLR & -2.31 & 3.08 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-MoCo & 0.36 & -3.17 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-BYOL & 0.28 & 1.56 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-SimSiam & -3.46 & 1.32 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-DINO & -2.45 & -0.57 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-SimCLR & -3.81 & 2.51 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-MoCo & -1.14 & -3.74 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-BYOL & 2.40 & -0.95 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-SimSiam & -1.34 & -1.19 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-DINO & -0.32 & -3.08 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-MAE & 0.79 & -2.51 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-MoCo & 0.99 & -6.25 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-BYOL & -1.40 & 5.30 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-SimSiam & -5.14 & 5.06 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-DINO & -4.12 & 3.17 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-MAE & -3.01 & 3.74 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-SimCLR & -5.49 & 6.25 \\ \hline \hline \end{tabular}
\end{table}
Table S24: Comparative statistical analysis of the robustness of slightly weak frozen encoders (\(\bm{v}\)-DINO, \(\bm{v}\)-SimSiam, and \(\bm{v}\)-Supervised) in **open-set recognition** in linear evaluation (the original numbers from Table 6(b)).

\begin{table}
\begin{tabular}{l l l c c} \hline \hline \multirow{2}{*}{**Distribution shift**} & \multirow{2}{*}{**Method 1**} & \multirow{2}{*}{**Method 2**} & \multicolumn{2}{c}{**Adjusted \(\Delta_{\text{OOD}}\)**} \\ \cline{3-5}  & & & **Lin.** & **FT.** \\ \hline \hline  & \(\bm{v}\)-BYOL & \(\bm{v}\)-SimSiam & -0.18 & 1.05 \\  & \(\bm{v}\)-BYOL & \(\bm{v}\)-DINO & 2.17 & -0.01 \\  & \(\bm{v}\)-BYOL & \(\bm{v}\)-MAE & 0.55 & -1.58 \\  & \(\bm{v}\)-BYOL & \(\bm{v}\)-SimCLR & 1.64 & 0.06 \\  & \(\bm{v}\)-BYOL & \(\bm{v}\)-MoCo & 1.27 & 2.40 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-BYOL & -2.61 & -1.05 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-DINO & 1.48 & -1.06 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-MAE & -0.14 & -2.63 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-SimCLR & 0.94 & -0.99 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-MoCo & 0.58 & 1.35 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-BYOL & -4.35 & 0.01 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-SimSiam & -2.61 & 1.06 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-MAE & -1.88 & -1.57 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-SimCLR & -0.80 & 0.07 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-MoCo & -1.16 & 2.41 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-BYOL & -3.15 & 1.58 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-SimSiam & -1.42 & 2.63 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-DINO & 0.94 & 1.57 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-SimCLR & 0.40 & 1.64 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-MoCo & 0.04 & 3.98 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-BYOL & -5.14 & -0.06 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-SimSiam & -3.40 & 0.99 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-DINO & -1.05 & -0.07 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-MAE & -2.67 & -1.64 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-MoCo & -1.95 & 2.34 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-BYOL & -5.55 & -2.40 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-SimSiam & -3.81 & -1.35 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-DINO & -1.46 & -2.41 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-MAE & -3.08 & -3.98 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-SimCLR & -1.99 & -2.34 \\ \hline \hline \end{tabular}
\end{table}
Table S25 Part 2. Comparative statistical analysis of the robustness of all methods in **zero-shot recognition** (original numbers from Table 4).

\begin{table}
\begin{tabular}{l l l c c} \hline \hline \multirow{2}{*}{**Distribution shift**} & \multirow{2}{*}{**Method 1**} & \multirow{2}{*}{**Method 2**} & \multicolumn{2}{c}{**Adjusted \(\mathbf{\Delta_{\mathrm{OOD}}}\)**} \\ \cline{3-5}  & & & **Lin.** & **FT.** \\ \hline \hline \multirow{5}{*}{\(\bm{v}\)-BYOL} & \(\bm{v}\)-SimSiam & -1.19 & -0.95 \\  & \(\bm{v}\)-BYOL & \(\bm{v}\)-DINO & -1.36 & -0.31 \\  & \(\bm{v}\)-BYOL & \(\bm{v}\)-MAE & -0.82 & -0.43 \\  & \(\bm{v}\)-BYOL & \(\bm{v}\)-SimCLR & -0.59 & -0.09 \\  & \(\bm{v}\)-BYOL & \(\bm{v}\)-MoCo & -1.38 & -0.20 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-BYOL & 0.59 & 0.95 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-DINO & -0.43 & 0.64 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-MAE & 0.11 & 0.52 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-SimCLR & 0.33 & 0.86 \\  & \(\bm{v}\)-SimSiam & \(\bm{v}\)-MoCo & -0.45 & 0.75 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-BYOL & 0.79 & 0.31 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-SimSiam & -0.08 & -0.64 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-MAE & 0.30 & -0.12 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-SimCLR & 0.53 & 0.22 \\  & \(\bm{v}\)-DINO & \(\bm{v}\)-MoCo & -0.26 & 0.11 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-BYOL & -0.39 & 0.43 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-SimSiam & -1.25 & -0.52 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-DINO & -1.41 & 0.12 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-SimCLR & -0.65 & 0.34 \\  & \(\bm{v}\)-MAE & \(\bm{v}\)-MoCo & -1.43 & 0.23 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-BYOL & -0.26 & 0.09 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-SimSiam & -1.12 & -0.86 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-DINO & -1.28 & -0.22 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-MAE & -0.75 & -0.34 \\  & \(\bm{v}\)-SimCLR & \(\bm{v}\)-MoCo & -1.3 & -0.11 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-BYOL & 0.50 & 0.20 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-SimSiam & -0.36 & -0.75 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-DINO & -0.52 & -0.11 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-MAE & 0.01 & -0.23 \\  & \(\bm{v}\)-MoCo & \(\bm{v}\)-SimCLR & 0.24 & 0.11 \\ \hline \hline \end{tabular}
\end{table}
Table S25 Part 3. Comparative statistical analysis of the robustness of all methods in **zero-shot recognition** (original numbers from Table 4).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{**Distribution shift**} & \multirow{2}{*}{**Method 1**} & \multirow{2}{*}{**Method 2**} & \multicolumn{2}{c}{**Adjusted \(\boldsymbol{\Delta_{\mathrm{OOD}}}\)**} \\ \cline{3-5}  & & & **Lin.** & **FT.** \\ \hline \hline  & \(\boldsymbol{v}\)-Supervised & \(\boldsymbol{v}\)-SimCLR & -1.86 & -3.00 \\  & \(\boldsymbol{v}\)-Supervised & \(\boldsymbol{v}\)-MoCo & -1.50 & -1.20 \\  & \(\boldsymbol{v}\)-Supervised & \(\boldsymbol{v}\)-BYOL & -2.46 & -1.20 \\  & \(\boldsymbol{v}\)-Supervised & \(\boldsymbol{v}\)-SimSiam & -3.24 & -2.00 \\  & \(\boldsymbol{v}\)-Supervised & \(\boldsymbol{v}\)-DINO & -1.70 & -1.90 \\  & \(\boldsymbol{v}\)-Supervised & \(\boldsymbol{v}\)-MAE & 0.39 & -2.40 \\  & \(\boldsymbol{v}\)-SimCLR & \(\boldsymbol{v}\)-Supervised & 0.74 & 3.00 \\  & \(\boldsymbol{v}\)-SimCLR & \(\boldsymbol{v}\)-MoCo & -0.2 & 1.80 \\  & \(\boldsymbol{v}\)-SimCLR & \(\boldsymbol{v}\)-BYOL & -1.16 & 1.80 \\  & \(\boldsymbol{v}\)-SimCLR & \(\boldsymbol{v}\)-SimSiam & -1.94 & 1.00 \\  & \(\boldsymbol{v}\)-SimCLR & \(\boldsymbol{v}\)-DINO & -0.40 & 1.10 \\  & \(\boldsymbol{v}\)-SimCLR & \(\boldsymbol{v}\)-MAE & 1.69 & 0.60 \\  & \(\boldsymbol{v}\)-MoCo & \(\boldsymbol{v}\)-Supervised & 0.09 & 1.20 \\  & \(\boldsymbol{v}\)-MoCo & \(\boldsymbol{v}\)-SimCLR & -1.21 & -1.80 \\  & \(\boldsymbol{v}\)-MoCo & \(\boldsymbol{v}\)-BYOL & -1.82 & 0.00 \\  & \(\boldsymbol{v}\)-MoCo & \(\boldsymbol{v}\)-SimSiam & -2.59 & -0.80 \\  & \(\boldsymbol{v}\)-MoCo & \(\boldsymbol{v}\)-DINO & -1.05 & -0.70 \\  & \(\boldsymbol{v}\)-MoCo & \(\boldsymbol{v}\)-MAE & 1.04 & -1.20 \\  & \(\boldsymbol{v}\)-BYOL & \(\boldsymbol{v}\)-Supervised & 1.58 & 1.20 \\  & \(\boldsymbol{v}\)-BYOL & \(\boldsymbol{v}\)-SimCLR & 0.28 & -1.80 \\  & \(\boldsymbol{v}\)-BYOL & \(\boldsymbol{v}\)-MoCo & 0.64 & 0.00 \\  & \(\boldsymbol{v}\)-BYOL & \(\boldsymbol{v}\)-SimSiam & -1.10 & -0.80 \\  & \(\boldsymbol{v}\)-BYOL & \(\boldsymbol{v}\)-DINO & 0.45 & -0.70 \\  & \(\boldsymbol{v}\)-BYOL & \(\boldsymbol{v}\)-MAE & 2.54 & -1.20 \\  & \(\boldsymbol{v}\)-SimSiam & \(\boldsymbol{v}\)-Supervised & 1.27 & 2.00 \\  & \(\boldsymbol{v}\)-SimSiam & \(\boldsymbol{v}\)-SimCLR & -0.03 & -1.00 \\  & \(\boldsymbol{v}\)-SimSiam & \(\boldsymbol{v}\)-MoCo & 0.33 & 0.80 \\  & \(\boldsymbol{v}\)-SimSiam & \(\boldsymbol{v}\)-ByOL & -0.63 & 0.80 \\  & \(\boldsymbol{v}\)-SimSiam & \(\boldsymbol{v}\)-DINO & 0.13 & 0.10 \\  & \(\boldsymbol{v}\)-SimSiam & \(\boldsymbol{v}\)-MAE & 2.22 & -0.40 \\  & \(\boldsymbol{v}\)-DINO & \(\boldsymbol{v}\)-Supervised & 0.67 & 1.90 \\  & \(\boldsymbol{v}\)-DINO & \(\boldsymbol{v}\)-SimCLR & -0.63 & -1.10 \\  & \(\boldsymbol{v}\)-DINO & \(\boldsymbol{v}\)-MoCo & -0.27 & 0.70 \\  & \(\boldsymbol{v}\)-DINO & \(\boldsymbol{v}\)-BYOL & -1.23 & 0.70 \\  & \(\boldsymbol{v}\)-DINO & \(\boldsymbol{v}\)-SimSiam & -2.01 & -0.10 \\  & \(\boldsymbol{v}\)-DINO & \(\boldsymbol{v}\)-MAE & 1.63 & -0.50 \\  & \(\boldsymbol{v}\)-MAE & \(\boldsymbol{v}\)-Supervised & -1.60 & 2.40 \\  & \(\boldsymbol{v}\)-MAE & \(\boldsymbol{v}\)-SimCLR & -2.90 & -0.60 \\  & \(\boldsymbol{v}\)-MAE & \(\boldsymbol{v}\)-MoCo & -2.55 & 1.20 \\  & \(\boldsymbol{v}\)-MAE & \(\boldsymbol{v}\)-BYOL & -3.51 & 1.20 \\  & \(\boldsymbol{v}\)-MAE & \(\boldsymbol{v}\)-SimSiam & -4.28 & 0.40 \\  & \(\boldsymbol{v}\)-MAE & \(\boldsymbol{v}\)-DINO & -2.74 & 0.50 \\ \hline \hline \end{tabular}
\end{table}
Table S26: Comparative statistical analysis of the robustness of all methods under **animal domain actor shift** (original numbers from Table 2).

### Exploring performance variation of VSSL methods across action classes

To provide a deeper understanding, we conduct a comprehensive analysis examining the impact of distribution shifts on the models' prediction across various action classes. Our investigation focuses on evaluating the performance of the finetuned models across different categories in both the InD and OoD. The findings are presented as follows:

* Figure S19 Context shift (10 classes)
* Figure S20 Context shift (50 classes)
* Figure S21 Viewpoint shift (egocentric)
* Figure S22 Viewpoint shift (surveillance camera+low resolution)
* Figure S23 Viewpoint + actor shift (top-down + synthetic)
* Figure S24 Actor shift (animal)
* Figure S25 Source shift (UCF to HMDB)
* Figure S26 Source shift (HMDB to UCF)
* Figure S28 Zero-shot (UCF)
* Figure S27 Zero-shot (HMDB)
* Figure S29 Zero-shot (RareAct)
* Figure S30 Open-set (UCF/HMDB)
* Figure S32 Open-set (Kinetics400/UCF)
* Figure S31 Open-set (Kinetics400/HMDB)Figure S19: **Context shift (10 classes)**: In-distribution vs. out-of-distribution performance comparison per action class. For optimal viewing, please zoom in. All of the contrastive and non-contrastive methods fail to identify ‘canoeing’ in OoD, whereas, \(\bm{v}\)-Supervised and \(\bm{v}\)-MAE make a few correct predictions. In another extreme example, \(\bm{v}\)-MAE makes a few correct predictions for ‘surfing water’, while other methods completely fail. In particular, VSSL methods demonstrate relatively good performance in identifying certain action classes in out-of-context scenarios, such as ‘golf driving’ and ‘playing volleyball’.

Figure S20: **Context shift (50 classes)**: In-distribution vs. out-of-distribution performance comparison per action class. For optimal viewing, please zoom in. Some of the top-performing action classes under context shift include ‘brushing teeth’, ‘catching or throwing frisbee’, ‘juggling balls’, ‘juggling soccer ball’, and ‘skipping rope’. It is noteworthy that all of these action classes involve high temporal motion and cyclic patterns. This observation leads us to speculate that the presence of these attributes may assist video models in recognizing actions in out-of-context settings.

Figure S21: **Viewpoint shift (egocentric)**: In-distribution vs. out-of-distribution performance comparison per action class. For optimal viewing, please zoom in and rotate 90 degrees to the left. A few examples of poor OoD generalization include action classes such as: 'lying on a bed', 'fixing a vacuum', and'someone is closing something', among a few others. We also note a few instances, where models show very similar InD and OoD performance, such as 'holding some food’, 'holding a boom', and 'taking food from somewhere', among a few others.

Figure S22: **Viewpoint shift (surveillance view + low resolution)**: In-distribution vs. out-of-distribution performance comparison per action class. For optimal viewing, please zoom in. Interestingly, both \(\bm{v}\)-BYOL and \(\bm{v}\)-SimSiam perform well in identifying ‘running’ but failed to identify ‘riding’. On the other hand, \(\bm{v}\)-SimCLR and \(\bm{v}\)-MoCo correctly identify ‘riding’ but fail to predict ‘running’. Overall, models show poor performance in distinguishing actions with subtle differences like ‘closing’ vs. ‘opening’, ‘entering’ vs. ‘exiting’, and ‘pull’ vs. ‘push’.

Figure S23: **Viewpoint + actor shift (top-down + synthetic)**: In-distribution vs. out-of-distribution performance comparison per action class. For optimal viewing, please zoom in. The models exhibit particularly poor performance in identifying the action classes ‘eat’, followed by ‘drink’. Conversely, they demonstrate relatively better performance in identifying actions such as ‘read book’, ‘use phone’, and ‘walk’.

Figure S24: **Actor shift (animal)**: In-distribution vs. out-of-distribution performance comparison per action class. For optimal viewing, please zoom in. The models exhibit particularly poor performance in identifying action classes ‘drinking’ and ‘running’. Conversely, they perform well in ‘predicting eating’, ‘opening door’, ‘swimming’, and ‘watching tv’.

Figure S25: **Source shift (UCF to HMDB)**: In-distribution vs. out-of-distribution performance comparison per action class. For optimal viewing, please zoom in. The results demonstrate the challenges faced by the video models in generalizing across similar action classes from different datasets. For instance, the models trained on UCF videos which include ‘soccer penalty’, struggle to generalize to the ‘kick ball’ class in HMDB. Similarly, the ‘walking with dog’ class in UCF does not aid in generalizing to ‘walk’ in HMDB.

Figure S26: **Source shift (HMDB to UCF)**: In-distribution vs. out-of-distribution performance comparison per action class. For optimal viewing, please zoom in. We notice large variability in performances across different methods. For example, \(\bm{v}\)-Supervised performs very poorly in identifying the action ‘dive’, whereas, all the self-supervised methods perform reasonably well. Furthermore, while \(\bm{v}\)-BYOL and \(\bm{v}\)-DINO demonstrate strong performance in identifying the action ‘walk’, the other methods struggle to achieve a similar level of accuracy. Overall, the VSSL methods tend to exhibit poor generalization capabilities for actions such as ‘jump’, ‘throw’, and ‘dribble’, among others.

Figure S27: **Zero-shot recognition (HMDB)**: In-distribution vs. out-of-distribution performance comparison per action class. For optimal viewing, please zoom in. The video models overall generalize well on action classes such as ‘chew’, ‘handstand’, ‘kick’, and ‘smile’, whereas, they show poor generalizability on other action classes such as ‘fencing’, ‘flic’ flac’, ‘pick’, ‘run’, and ‘turn’ among others. Moreover, their performance largely varies across different actions.

Figure S28: **Zero-shot recognition (UCF)**: In-distribution vs. out-of-distribution performance comparison per action class. For optimal viewing, please zoom in. The video models overall generalize well on action classes such as ‘apply lipstick’, ‘rafting’, ‘ice dancing’, and ‘sumo wrestling’, whereas, they show poor generalizability on action classes such as ‘fencing’, ‘front crawl’, ‘hammering’, ‘mixing’, and ‘rowing’ among others. Moreover, their performance largely varies across different actions.

Figure S29: **Zero-shot recognition (RareAct)**: In-distribution vs. out-of-distribution performance comparison per action class. For optimal viewing, please zoom in and rotate 90 degrees to the left. The models tend to generalize poorly on unusual actions. Moreover, compared to other zero-shot setups, we notice the maximum variability in model performance across different action classes.

Figure S30: Confusion matrices for open-set recognition, using **UCF101 as closed-set and HMDB as open-set**. The x-axis represents the ground truth and the y-axis represents the predicted labels. We highlight known classes with green and unknown classes with red. Among all the methods \(\bm{v}\)-SimSiam and \(\bm{v}\)-MAE show worse performance in identifying ‘unknown’ classes from HMDB as ‘unknown’.

Figure S31: Confusion matrices of open-set recognition, using **Kinetics400 as closed-set and HMDB as open-set.** The x-axis represents the ground truth and the y-axis represents the predicted labels. We highlight known classes with green and unknown classes with red. The results exhibit that all the video models struggle in identifying ‘unknown’ classes from HMDB as ‘unknown’. This is likely due to their over-confident predictions.

Figure S32: Confusion matrices of open-set recognition, using **Kinetics400 as closed-set and UCF as open-set**. The x-axis represents the ground truth and the y-axis represents the predicted labels. We highlight known classes with green and unknown classes with red. The results exhibit that all the video models struggle in identifying ‘unknown’ classes from UCF as ‘unknown’. This is likely due to their over-confident predictions.