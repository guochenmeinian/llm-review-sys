# Wasserstein Gradient Boosting: A Framework for Distribution-Valued Supervised Learning

 Takuo Matsubara

The University of Edinburgh

Edinburgh, EH9 3JZ

takuo.matsubara@ed.ac.uk

###### Abstract

Gradient boosting is a sequential ensemble method that fits a new weaker learner to pseudo residuals at each iteration. We propose Wasserstein gradient boosting, a novel extension of gradient boosting, which fits a new weak learner to alternative pseudo residuals that are Wasserstein gradients of loss functionals of probability distributions assigned at each input. It solves distribution-valued supervised learning, where the output values of the training dataset are probability distributions. In classification and regression, a model typically returns, for each input, a point estimate of a parameter of a noise distribution specified for a response variable, such as the class probability parameter of a categorical distribution specified for a response label. A main application of Wasserstein gradient boosting in this paper is tree-based evidential learning, which returns a distributional estimate of the response parameter for each input. We empirically demonstrate the competitive performance of the probabilistic prediction by Wasserstein gradient boosting in comparison with existing uncertainty quantification methods.

## 1 Introduction

Gradient boosting is a celebrated machine learning algorithm that has achieved considerable success with tabular data [1]. Gradient boosting has been extensively used for point forecasts and probabilistic classification, yet a relatively small number of studies have been concerned with the predictive uncertainty of gradient boosting. Predictive uncertainty of machine learning models plays a growing role in today's real-world production systems [2]. It is vital for safety-critical systems, such as medical diagnoses [3] and autonomous driving [4], to assess the potential risk of their actions that partially or entirely rely on predictions from their models. Gradient boosting has already been applied in a diverse range of real-world applications, including click prediction [5], ranking systems [6], scientific discovery [7], and data competition [8]. There is a pressing need for methodology to harness the power of gradient boosting to predictive uncertainty quantification.

In classification and regression, we typically specify a noise distribution \(p(y\mid\theta)\) of a response variable \(y\) and use a model to return a point estimate \(\theta(x)\) of the response parameter for each input \(x\). In recent years, the importance of capturing uncertainty in the model output \(\theta(x)\) has increasingly been emphasised [2]. A variety of approaches have been proposed to obtain a distributional estimate \(p(\theta\mid x)\) of the response parameter for each input \(x\) [e.g. 9, 10, 11]. For example, Bayesian neural networks (BNNs) quantify uncertainty in network weights and propagate it to the space of network outputs. Marginalising the predictive distribution \(p(y\mid\theta)\) over the distributional estimate \(p(\theta\mid x)\) has been demonstrated to confer enhanced predictive accuracy and robustness against adversarial attacks [11]. Furthermore, the dispersion of the distributional estimate has been used as a powerful indicator for out-of-distribution (OOD) detection [12].

In this context, a line of research based on the concept of _evidential learning_ has recently gained significant attention [11, 13, 14, 15]. The idea can be broadly interpreted as making use of the 'individual-level' posterior \(p(\theta\mid y_{i})\) of the response parameter \(\theta\) conditional on each individual datum \(y_{i}\), which arises from the response-distribution likelihood \(p(y_{i}\mid\theta)\) and a user-specified prior \(p(\theta)\). If each individual-level posterior falls into a closed form characterised by some hyperparameter, neural networks can be trained by using the finite-dimensional hyperparameter as a target value for each input. Outstanding performance and computational efficiency of the existing approaches have been delivered in a wide spectrum of engineering and medical applications [16, 17, 18, 19]. However, the existing approaches are limited to neural networks and to the case where every individual-level posterior is in closed form so that the finite-dimensional hyperparameter can be predicted by proxy. In general, posterior distributions are known only up to their normalising constants and, therefore, require an approximation typically by particles [20].

Without closed-form expression, each individual-level posterior needs to be treated as an infinite-dimensional output for each input. This challenge poses the following fundamental question:

Consider a supervised learning setting whose outputs are probability distributions. Given a training set of input values and output distributions \(\{x_{i},\mu_{i}\}_{i=1}^{D}\), can we build a model that receives an input \(x\) and returns a _nonparametric_ prediction of the output distribution?

Motivated by this question, we propose a general framework of Wasserstein gradient boosting (WGBoost). WGBoost receives an input and returns a particle approximation of the output distribution. Figure 1 illustrates inputs and outputs of WGBoost. In this paper, we focus on application of WGBoost to evidential learning, where the individual-level posterior \(p(\theta\mid y_{i})\) of the response parameter \(\theta\) is used as the output distribution \(\mu_{i}\) for each input \(x_{i}\) in the training set. Figure 2 compares the pipeline of evidential learning based on WGBoost with that of Bayesian learning.

**Contributions**: Our contributions are summarised as follows:

1. **Methodology of WGBoost**: Section 2 establishes the general framework of WGBoost. It is a novel family of gradient boosting that returns a set of particles that approximates an output distribution assigned at each input. In contrast to standard gradient boosting that fits a weak learner to the gradient of a loss function, WGBoost fits a weak learner to the estimated Wasserstein gradient of a loss functional over probability distributions.
2. **Application to Evidential Learning**: Section 3 establishes tree-based evidential learning based on WGBoost, with the loss functional specified by the Kullback-Leibler (KL) divergence. Following modern gradient-boosting libraries [21, 22] that uses second-order gradient boosting (c.f. Section 2.2), we implement a concrete second-order WGBoost algorithm built on an approximate Wasserstein gradient and Hessian of the KL divergence.
3. **Experiment on Real-world Data**: Section 4 demonstrates the performance of probabilistic regression, and classification with OOD detection, on real-world tabular datasets. To the author's knowledge, WGBoost is the first framework that enables evidential learning for (i) boosted tree models and (ii) cases without closed form of individual-level posteriors.

Figure 1: Illustration of WGBoost trained on a set \(\{x_{i},\mu_{i}\}_{i=1}^{10}\) whose inputs are 10 grid points in \([-3.5,3.5]\) and each output distribution is a normal distribution \(\mu_{i}(\theta)=\mathcal{N}(\theta\mid\sin(x_{i}),0.5)\) over \(\theta\in\mathbb{R}\). The blue area indicates the \(95\)% high probability region of the conditional distribution \(\mathcal{N}(\theta\mid\sin(x),0.5)\). WGBoost returns \(N=10\) particles (red lines) to predict the output distribution for each input \(x\). This illustration uses the Gaussian kernel regressor for every weaker learner.

## 2 Wasserstein Gradient Boosting

This section establishes the general formulation of WGBoost. Section 2.1 recaps the notion of Wasserstein gradient flows, a 'gradient' system of probability distributions that minimises an objective functional in the space of probability distributions. Section 2.2 recaps the notion of gradient boosting, a sequential ensemble method that fits a new weak learner to the 'gradient' of the remaining loss at each iteration. Section 2.3 combines the above two notions to establish WGBoost, a novel family of gradient boosting that enables to solve distribution-valued supervised learning.

Notation and SettingLet \(\mathcal{X}\) and \(\mathcal{Y}\) denote the space of inputs and responses in classification and regression. Suppose \(\Theta=\mathbb{R}^{d}\). Let \(\mathcal{P}_{2}\) be the 2-Wasserstein space i.e. a set of all probability distributions on \(\Theta\) with finite second moment equipped with the Wasserstein metric [23]. We identify a probability distribution in \(\mathcal{P}_{2}\) with its density whenever it exits. Denote by \(\odot\) and \(\odot\), respectively, elementwise multiplication and elementwise division of two vectors in \(\mathbb{R}^{d}\). Let \(\nabla\) be the gradient operator. Let \(\nabla_{\text{d}}^{2}\) be a second-order gradient operator that takes the second derivative at each coordinate i.e. \(\nabla_{\text{d}}^{2}f(\theta)=[\partial^{2}f(\theta)/\partial\theta_{1}^{2},\dots,\partial^{2}f(\theta)/\partial\theta_{d}^{2}]^{\mathrm{T}}\in\mathbb{R }^{d}\).

### Wasserstein Gradient Flow

In the Euclidean space, a gradient flow of a function \(f\) means a curve of points \(x_{t}\) that solves a differential equation \((d/dt)x_{t}=-\nabla f(x_{t})\) from some initial value \(x_{0}\). That is the continuous-time limit of gradient descent, which minimises the function \(f\) as \(t\to\infty\). A Wasserstein gradient flow means a curve of probability distributions \(\mu_{t}\) minimising a given functional \(\mathcal{F}\) on the 2-Wasserstein space \(\mathcal{P}_{2}\) from some initial distribution \(\mu_{0}\). The Wasserstein gradient flow \(\mu_{t}\) is characterised as a solution of a partial differential equation, known as the _continuity equation_:

\[\frac{d}{dt}\mu_{t}=-\nabla\cdot(\mu_{t}\nabla_{W}\mathcal{F}(\mu_{t}))\quad \text{ given}\quad\mu_{0}\in\mathcal{P}_{2},\] (1)

where \(\nabla_{W}\mathcal{F}(\mu):\Theta\to\Theta\) denotes the _Wasserstein gradient_ of \(\mathcal{F}\) at \(\mu\)[24; 25]. Appendix A recaps the derivation of the Wasserstein gradient, presenting the examples for several functionals.

One of the elegant properties of the Wasserstein gradient flow is casting the infinite-dimensional optimisation of the functional \(\mathcal{F}\) as a finite-dimensional particle update [23]. The continuity equation (1) can be reformulated as a dynamical system of a random variable \(\theta_{t}\sim\mu_{t}\), such that

\[\frac{d}{dt}\theta_{t}=-\left[\nabla_{W}\mathcal{F}(\mu_{t})\right](\theta_{t} )\quad\text{given}\quad\theta_{0}\sim\mu_{0},\] (2)

in the sense that the law \(\mu_{t}\) of such a random variable \(\theta_{t}\) is a weak solution of the continuity equation. Consider the case where the initial measure \(\mu_{0}\) is set to the empirical distribution \(\hat{\mu}_{0}\) of \(N\) particles \(\{\theta_{0}^{n}\}_{n=1}^{N}\). Discretising the continuous-time system (2) by the Euler method with a small step size \(\nu>0\) yields an iterative update scheme of \(N\) particles \(\{\theta_{m}^{n}\}_{n=1}^{N}\) from step \(m=0\):

\[\begin{bmatrix}\theta_{m+1}^{1}\\ \vdots\\ \theta_{m+1}^{N}\end{bmatrix}=\begin{bmatrix}\theta_{m}^{1}\\ \vdots\\ \theta_{m}^{N}\end{bmatrix}+\nu\begin{bmatrix}-[\nabla_{W}\mathcal{F}(\hat{ \mu}_{m})](\theta_{m}^{1})\\ \vdots\\ -[\nabla_{W}\mathcal{F}(\hat{\mu}_{m})](\theta_{m}^{N})\end{bmatrix},\] (3)

Figure 2: Comparison of the pipeline of (a) Bayesian learning and (b) evidential learning based on WGBoost. The former uses the (global-level) posterior \(p(w\mid\{x_{i},y_{i}\}_{i=1}^{D})\) of the model parameter \(w\) conditional on all data, and samples multiple models from it. The latter uses the individual-level posterior \(p(\theta\mid y_{i})\) of the response parameter \(\theta\) as the output distribution of the training set, and trains WGBoost that returns a particle-based distributional estimate \(p(\theta\mid x)\) of \(\theta\) for each input \(x\).

where \(\hat{\mu}_{m}\) denotes the empirical distribution of the particles \(\{\theta_{m}^{n}\}_{n=1}^{N}\) at step \(m\).

In practice, it is common that the Wasserstein gradient of a chosen functional \(\mathcal{F}\) is not well-defined for empirical distributions. In such case, the particle update scheme (3) is not directly applicable because it depends on the Wasserstein gradient \(\nabla_{W}\mathcal{F}(\hat{\mu}_{m})\) at the empirical distribution \(\hat{\mu}_{m}\). For example, the KL divergence \(\mathcal{F}(\mu)=\mathrm{KL}(\mu\mid\pi)\) with a reference distribution \(\pi\) leads to the Wasserstein gradient \([\nabla_{W}\mathcal{F}(\mu)](\theta)=-(\nabla\log\pi(\theta)-\nabla\log\mu( \theta))\) ill-defined if \(\mu\) is an empirical distribution. Hence, the particle update scheme (3) is often performed with the estimated or approximated Wasserstein gradient well-defined for empirical distributions (e.g. 26; 27; 28; 29; 30). A main application of WGBoost in Section 3 uses the'smoothed' Wasserstein gradient of the KL divergence [26].

### Gradient Boosting

Gradient boosting [31] is a sequential ensemble method of \(M\) weak learners \(f_{1},\ldots,f_{M}\). It iteratively constructs an ensemble \(F_{m}\) of \(m\) weak learners \(f_{1},\ldots,f_{m}\) from step \(m=0\) to \(M\). Given the current ensemble \(F_{m}\) at step \(m\), it trains a new weak learner \(f_{m+1}\) to construct the next ensemble by

\[F_{m+1}(x)=F_{m}(x)+\nu f_{m+1}(x),\] (4)

where \(\nu\) is a shrinkage hyperparameter called a _learning rate_. The initial state of the ensemble \(F_{0}(x)\) at step \(m=0\) is typically set to a constant that best fits the data. Any learning algorithm can be used as a weak learner in principle, although tree-based algorithms are most used [32].

The fundamental idea of gradient boosting is to train the new weak learner \(f_{m+1}\) to approximate the negative gradient of the remaining error of the current ensemble \(F_{m}\). Suppose that a loss function \(L\) measures the remaining error \(R_{i}(F_{m}(x_{i})):=L(F_{m}(x_{i}),y_{i})\) for each output vector \(y_{i}\in\mathbb{R}^{d}\). The new weak learner \(f_{m+1}\) is fitted to the set \(\{x_{i},g_{i}\}_{i=1}^{D}\) whose target variable \(g_{i}\) is each specified by

\[g_{i}:=-\nabla R_{i}(F_{m}(x_{i}))\in\mathbb{R}^{d}.\]

The target \(g_{i}\) is often called a _pseudo residual_. For each data input \(x_{i}\), the boosting scheme (4) updates the output of the current ensemble \(F_{m}(x_{i})\) in the steepest descent direction of the error \(R_{i}(F_{m}(x_{i}))\). Although [31] originally suggested an additional line search to determine a scaling constant of each weak learner, the line search has been reported to have a negligible influence on performance [33].

In modern gradient-boosting libraries, such as XGBoost [21] and LightGBM [22], the standard practice is to use the diagonal (coordinatewise) Newton direction of the remaining error \(R_{i}(F_{m}(x_{i}))\) in lieu of the negative gradient \(g_{i}\). The new base leaner \(f_{m+1}\) is instead fitted to the set \(\{x_{i},g_{i}\oslash h_{i}\}_{i=1}^{n}\), where the negative gradient \(g_{i}\) is divided elementwise by the Hessian diagonal \(h_{i}\) given by

\[h_{i}:=\nabla_{\mathrm{d}}^{2}R_{i}(F_{m}(x_{i}))\in\mathbb{R}^{d}.\]

The target variable \(g_{i}\oslash h_{i}\) is the diagonal Newton direction that minimises the second-order Taylor approximation of the remaining error \(R_{i}(F_{m}(x_{i}))\) for each coordinate independently. Combining the second-order gradient boosting framework with tree-based weak learners has demonstrated exceptional scalability and performance [34; 35]. Although it is possible to use the 'full' Newton direction as the target variable of each weak learner, the impracticality of the full Newton direction has been pointed out (e.g. 36; 37). In addition, the coordinatewise computability of the diagonal Newton direction is suitable for popular gradient-boosting tree algorithms [36].

### General Formulation of Wasserstein Gradient Boosting

Now we consider the setting of distribution-valued supervised learning, where we are given a training set of input vectors and output distributions \(\{x_{i},\mu_{i}\}_{i=1}^{D}\subset\mathcal{X}\times\mathcal{P}_{2}\). Our goal is to construct a model that receives an input and returns a set of \(N\) particles whose empirical distribution approximates the output distribution. We specify a loss functional \(\mathrm{D}(\cdot\mid\cdot)\) between two probability distributions--such as the KL divergence--to measure the remaining error \(\mathcal{F}_{i}(\cdot)=\mathrm{D}(\cdot\mid\mu_{i})\) for each \(i\)-th training output distribution \(\mu_{i}\). Our idea is to combine gradient boosting with Wasserstein gradient, where we iteratively construct a set of \(N\) boosting ensembles \(F_{m}^{1},\ldots,F_{m}^{N}\) from step \(m=0\) to \(M\).

Here, the output \(F_{m}^{n}(x)\) of each \(n\)-th boosting ensemble represents the \(n\)-th output particle for an input \(x\). Given the current set of \(N\) ensembles \(F_{m}^{1},\ldots,F_{m}^{N}\) at step \(m\), WGBoost trains a set of new weak learners \(f^{1}_{m+1},\ldots,f^{N}_{m+1}\) and computes the next set of \(N\) ensembles by

\[\begin{bmatrix}F^{1}_{m+1}(x)\\ \vdots\\ F^{N}_{m+1}(x)\end{bmatrix}=\begin{bmatrix}F^{1}_{m}(x)\\ \vdots\\ F^{N}_{m}(x)\end{bmatrix}+\nu\begin{bmatrix}f^{1}_{m+1}(x)\\ \vdots\\ f^{N}_{m+1}(x)\end{bmatrix}\] (5)

where \(\nu\) is a learning rate. Similarly to standard gradient boosting, we specify the initial state of \(N\) ensembles \(F^{1}_{0},\ldots,F^{N}_{0}\) at step \(m=0\) by a set of constants. Throughout, denote by \(\hat{\mu}_{m,i}\) the empirical distribution of the \(N\) output particles \(F^{1}_{m}(x_{i}),\ldots,F^{N}_{m}(x_{i})\) for each \(i\)-th training input \(x_{i}\).

As discussed in Section 2.1, the Wasserstein gradient often needs to be estimated for empirical distributions. For better presentation, let \(\mathcal{G}_{i}(\mu)\) denote an estimate of the Wasserstein gradient \(\nabla_{W}\mathcal{F}_{i}(\mu)\) of the \(i\)-th remaining error \(\mathcal{F}_{i}(\mu)\), which is well-defined for any distribution \(\mu\). If the Wasserstein gradient \(\nabla_{W}\mathcal{F}_{i}(\mu)\) is originally well-defined for any distribution \(\mu\), it is a trivial choice of the estimate, i.e., \(\mathcal{G}_{i}(\mu)=\nabla_{W}\mathcal{F}_{i}(\mu)\). Otherwise, any suitable estimate can be used as \(\mathcal{G}_{i}(\mu)\). The foundamental idea of WGBoost is to train the \(n\)-th new learner \(f^{n}_{m+1}\) to approximate the estimated Wasserstein gradient \(-\mathcal{G}_{i}(\hat{\mu}_{m,i})\) evaluated at the \(n\)-th boosting output \(F^{n}_{m}(x_{i})\) for each \(x_{i}\), so that,

\[\begin{bmatrix}f^{1}_{m+1}(x_{i})\\ \vdots\\ f^{N}_{m+1}(x_{i})\end{bmatrix}\approx\begin{bmatrix}-\left[\mathcal{G}_{i} \left(\hat{\mu}_{m,i}\right)\right]\left(F^{1}_{m}(x_{i})\right)\\ \vdots\\ -\left[\mathcal{G}_{i}\left(\hat{\mu}_{m,i}\right)\right]\left(F^{N}_{m}(x_{i })\right)\end{bmatrix}.\]

For each data input \(x_{i}\), the boosting scheme (5) approximates the particle update scheme (3) for the output particles \(F^{1}_{m}(x_{i}),\ldots,F^{N}_{m}(x_{i})\) under the estimated Wasserstein gradient. The output particles are updated in the direction to decrease the remaining error \(\mathcal{F}_{i}(\hat{\mu}_{m,i})=\mathrm{D}(\hat{\mu}_{m,i}\mid\mu_{i})\) at each step \(m\).

Algorithm 1 summarises the general procedure of WGBoost. See Figure 1 for illustration of WGBoost. In Section 3, we choose the KL divergence as a loss functional \(\mathrm{D}\) and use a kernel smoothing estimate of the Wasserstein gradient. See Appendix A for the Wasserstein gradient of other divergences.

**Remark 1** (**Stochastic WGBoost)**.: Stochastic gradient boosting [38] uses only a randomly sampled subset of data to fit a new weak learner at each step \(m\) to reduce the computational cost. The same subsampling approach can be applied for WGBoost whenever the dataset is large.

**Remark 2** (**Second-Order WGBoost)**.: If any estimate of the Wasserstein 'Hessian' of the remaining error \(\mathcal{F}_{i}\) is available, the Newton direction of \(\mathcal{F}_{i}\) may also be computable [e.g. 39, 40]. Implementation of a second-order WGBoost algorithm is immediate by plugging such a Newton direction into \(\mathcal{G}_{i}(\mu)\) in Algorithm 1. Our default WGBoost algorithm for tree-based evidential learning is built on a diagonal approximate Newton direction of the KL divergence, aligning with the standard practice in modern gradient-boosting libraries to use the diagonal Newton direction.

``` Input: training set \(\{x_{i},\mu_{i}\}_{i=1}^{D}\) of input \(x_{i}\in\mathcal{X}\) and output distribution \(\mu_{i}\in\mathcal{P}_{2}\) Parameter: loss \(\mathrm{D}\), estimate \(\mathcal{G}_{i}(\mu)\) of the Wasserstein gradient \(\nabla_{W}\,\mathrm{D}(\mu\mid\mu_{i})\), particle number \(N\), iteration \(M\), learning rate \(\nu\), weak learner \(f\), initial constants \((\vartheta^{1}_{0},\ldots,\vartheta^{N}_{0})\) Output: set of \(N\) boosting ensembles \((F^{1}_{M},\ldots,F^{N}_{M})\) at final step \(M\) \((F^{1}_{0}(\cdot),\ldots,F^{N}_{0}(\cdot))\leftarrow(\vartheta^{1}_{0},\ldots, \vartheta^{N}_{0})\)\(\triangleright\) set initial state of \(N\) boosting ensembles for\(m\gets 0,\ldots M-1\)do for\(i\gets 1,\ldots,D\)do \(\hat{\mu}_{m,i}\leftarrow\) empirical distribution of output values \((F^{1}_{m}(x_{i}),\ldots,F^{N}_{m}(x_{i}))\) for input \(x_{i}\) for\(n\gets 1,\ldots,N\)do \(g^{n}_{i}\leftarrow-\left[\mathcal{G}_{i}(\hat{\mu}_{m,i})\right](F^{n}_{m}(x_ {i}))\)\(\triangleright\) compute target value of \(n\)-th new weak learner end for for\(n\gets 1,\ldots,N\)do \(f^{n}_{m+1}\leftarrow\) fit\(\left(\,\{x_{i},g^{n}_{i}\}_{i=1}^{D}\,\right)\)\(\triangleright\) fit \(n\)-th new weak learner \(F^{n}_{m+1}(\cdot)\gets F^{n}_{m}(\cdot)+\nu f^{n}_{m+1}(\cdot)\)\(\triangleright\) set next state of \(n\)-th boosting ensemble end ```

**Algorithm 1**Wasserstein Gradient Boosting

**Remark 3** (**Second-Order WGBoost)**.: If any estimate of the Wasserstein 'Hessian' of the remaining error \(\mathcal{F}_{i}\) is available, the Newton direction of \(\mathcal{F}_{i}\) may also be computable [e.g. 39, 40]. Implementation of a second-order WGBoost algorithm is immediate by plugging such a Newton direction into \(\mathcal{G}_{i}(\mu)\) in Algorithm 1. Our default WGBoost algorithm for tree-based evidential learning is built on a diagonal approximate Newton direction of the KL divergence, aligning with the standard practice in modern gradient-boosting libraries to use the diagonal Newton direction.

Application to Evidential Learning

This section provides our default setting to implement a concrete WGBoost algorithm for evidential learning, which enables classification and regression with predictive uncertainty. The individual-level posterior \(p(\theta\mid y_{i})\) of a response distribution \(p(y\mid\theta)\) is used as the output distribution \(\mu_{i}\) of the training set \(\{x_{i},\mu_{i}\}_{i=1}^{D}\). Section 3.1 recaps derivation of the individual-level posterior \(p(\theta\mid y_{i})\), followed by Section 3.2 discussing the default choice of the prior. We choose the KL divergence as a loss functional of WGBoost. Section 3.3 recaps a widely-used estimate of the Wasserstein gradient of the KL divergence based on kernel smoothing [26]. A further advantage of the kernel smoothing estimate is that the approximate Wasserstein Hessian is available, with which Section 3.4 establishes a second-order WGBoost algorithm similarly to modern gradient-boosting libraries.

### Derivation of Individual-Level Posteriors and Predictive Distribution

Suppose that a response distribution \(p(y\mid\theta)\) of a response variable \(y\) is specified, as is typically done for probabilistic prediction. Suppose also that a prior distribution \(p_{i}(\theta)\) of the response parameter \(\theta\) is specified for each individual data input \(x_{i}\). For each individual data pair \((x_{i},y_{i})\), the response-distribution likelihood \(p(y_{i}\mid\theta)\) and the prior \(p_{i}(\theta)\) determine the individual-level posterior

\[p(\theta\mid y_{i})\propto p(y_{i}\mid\theta)p_{i}(\theta)\]

by Bayes' theorem. This individual-level posterior is set to the output distribution \(\mu_{i}\) of the training set \(\{x_{i},\mu_{i}\}_{i=1}^{D}\) of WGBoost. The framework of WGBoost then constructs a model that returns a particle approximation of the output distribution \(\mu_{i}(\cdot)=p(\cdot\mid y_{i})\) for each data input \(x_{i}\).

For a new input \(x\), the constructed WGBoost model provides a set of particles \((\theta^{1}(x),\ldots,\theta^{N}(x))\) as a distributional prediction \(p(\theta\mid x)\) of the response parameter \(\theta\). We can define a predictive distribution \(p(y\mid x)\) of the response \(y\) for the new input \(x\) via marginalisation of the output particles:

\[p(y\mid x)=\int_{\Theta}p(y\mid\theta)p(\theta\mid x)d\theta=\frac{1}{N}\sum_ {i=1}^{N}p\left(y\mid\theta^{i}(x)\right).\] (6)

We can also define a point prediction \(\hat{y}\) for the new input \(x\) via the individual-level Bayes action \(\hat{y}=\text{argmin}_{y\in\mathcal{Y}}\;\int_{\Theta}U(y,\theta)p(\theta\mid x )d\theta\), which minimises the average of some error \(U:\mathcal{Y}\times\Theta\to\mathbb{R}\). For example, the Bayes action \(\hat{y}\) is simply the mean of the output particles if \(U(y,\theta)=(y-\theta)^{2}\).

In general, the explicit form of the individual-level posterior \(p(\theta\mid y_{i})\) is known only up to the normalising constant. Our full algorithm in Section 3.4 requires no normalising constant of the individual-level posterior \(p(\theta\mid y_{i})\). Our algorithm depends only on the log-gradient of the individual-level posterior \(\nabla\log p(\theta\mid y_{i})\) that cancels any constant term by the gradient. Hence, knowing the form of the response-distribution likelihood \(p(y_{i}\mid\theta)\) and the prior \(p_{i}(\theta)\) suffices.

**Remark 3** (Difference from Bayesian Learning).: Given a response distribution \(p(y\mid\theta)\) and a model \(\theta=f(x,w)\) with the parameter \(w\), Bayesian learning of the model \(f\) means the use of the posterior \(p(w\mid\{x_{i},y_{i}\}_{i=1}^{D})\) over \(w\) conditional on all data. The predictive distribution \(p(y\mid x)\) of the response \(y\) is defined via marginalisation over \(w\): \(\int_{\Theta}p(y\mid\theta=f(x,w))p(w\mid\{x_{i},y_{i}\}_{i=1}^{D})dw\). In contrast, WGBoost returns a distributional prediction \(p(\theta\mid x)\) of the response parameter \(\theta\), circumventing the marginalisation over the model parameter \(w\) that can be ultra-high dimensional.

### Choice of Individual-Level Priors

The prior distribution \(p_{i}(\theta)\) of the response parameter \(\theta\) is specified at each individual data input \(x_{i}\). The approach to eliciting the prior may differ, depending on whether past data are available. If past data are available, they can be utilised to elicit a reasonable prior for unobserved data. If no past data are available, we recommend the use of a noninformative prior that have been developed as a sensible choice of prior in the absence of past data; see (e.g. 41) for the introduction. To avoid numerical errors, if a noninformative prior is improper (i.e. nonintegrable), we recommend the use of a proper probability distribution that approximates the noninformative prior sufficiently well.

**Example 1** (Normal Location-Scale Response).: Consider a scalar-valued response variable \(y\in\mathbb{R}\) for regression. A normal location-scale response distribution \(\mathcal{N}(y\mid m,\sigma)\) has the mean and scale parameters \(m\in\mathbb{R}\) and \(\sigma\in(0,\infty)\). A typical noninformative prior of \(m\) and \(\sigma\) are given by,respectively, \(1\) and \(1/\sigma\) which are improper. At every data point \((x_{i},y_{i})\), we use a normal prior \(\mathcal{N}(m\mid 0,\sigma_{0})\) over \(m\) and an inverse gamma prior \(\mathrm{IG}(\sigma\mid\alpha_{0},\beta_{0})\) over \(\sigma\), with the hyperparameters \(\sigma_{0}=10\) and \(\alpha_{0}=\beta_{0}=0.01\), which approximate the non-informative priors.

**Example 2** (**Categorical Response**).: Consider a label response variable \(y\in\{1,\ldots,k\}\) for \(k\)-class classification. A categorical response distribution \(\mathcal{C}(y\mid q)\) has the class probability parameter \(q=(q_{1},\ldots,q_{k})\) in the \(k\)-dimensional simplex \(\Delta_{k}\). If \(k=2\), it corresponds to the Bernoulli distribution. A typical noninformative prior of \(q\) is given by \(1/(q_{1}\times\cdots\times q_{k})\) which are improper. At every data point \((x_{i},y_{i})\), we use the logistic normal prior--a multivariate generalisation of the logit normal distribution [42]--over \(q\) with the mean \(0\) and identity covariance matrix scaled by \(10\).

**Remark 4** (**Reparametrisation and Standardisation**).: Section 2 supposed \(\Theta=\mathbb{R}^{d}\) for some dimension \(d\) without no loss of generality. Any parameter that lies in a subset of the Euclidean space (e.g. \(\sigma\) in Example 1) can be reparametrised as one in the Euclidean space (e.g. \(\log\sigma\)). Appendix D details the reparametrisation used for Examples 1 and 2. In addition, if one's dataset has scalar outputs of a low or high order of magnitude, we recommend standardising the outputs.

### Approximate Wasserstein Gradient of KL Divergence

We consider the KL divergence \(\mathrm{KL}(\mu\mid\mu_{i})\) as a loss functional of WGBoost. One challenge of the KL divergence is that the resulting Wasserstein gradient \(\left[\mathcal{G}_{i}^{\mathrm{KL}}(\mu)\right](\theta):=-\left(\nabla\log \mu_{i}(\theta)-\nabla\log\mu(\theta)\right)\) is not well-defined when \(\mu\) is an empirical distribution. A particularly successful solution--which originates in [43] and has been applied in wide contexts [26; 44; 45]--is to smooth the Wasserstein gradient through a kernel integral operator \(\int_{\Theta}[\mathcal{G}_{i}^{\mathrm{KL}}(\mu)](\theta^{*}k(\theta,\theta^ {*})d\mu(\theta^{*})\)[46]. By integration-by-part (see [e.g. 43]), the smoothed Wasserstein gradient, denoted \(\mathcal{G}_{i}^{*}(\mu)\), falls into the following form that is well-defined for any distribution \(\mu\):

\[\left[\mathcal{G}_{i}^{*}(\mu)\right](\theta):=-\mathbb{E}_{\theta^{*}\sim \mu}\Big{[}\nabla\log\mu_{i}(\theta^{*})k(\theta^{*},\theta)+\nabla k(\theta^ {*},\theta)\Big{]}\in\mathbb{R}^{d},\] (7)

where \(\nabla k(\theta^{*},\theta)\) denotes the gradient of \(k\) with respect to the first argument \(\theta^{*}\). An approximate Wasserstein gradient flow based on the smoothed Wasserstein gradient \(\mathcal{G}_{i}^{*}(\mu)\) is called the Stein variational gradient descent [43] or kernelised Wasserstein gradient flow [47]. In most cases, the kernel \(k\) is set to the Gaussian kernel \(k(\theta,\theta^{*})=\exp(-\|\theta-\theta^{*}\|^{2}/h)\) with the scale \(h>0\). Appendix B discusses a choice of kernel. This work uses the Gaussian kernel with \(h=0.1\) throughout.

Another common approach to approximating the Wasserstein gradient flow of the KL divergence is the Langevin diffusion approach [48]. The discretised algorithm, called the unadjusted Langevin algorithm [49], is a stochastic particle update scheme that adds a Gaussian noise at every iteration. However, several known challenges, such as asymptotic bias and slow convergence, often necessitate an ad-hoc adjustment of the algorithm [48]. Appendix B discusses a variant of WGBoost built on the Langevin algorithm, although it is not considered the default implementation.

### Second-Order Implementation of WGBoost

We use a diagonal (coordinatewise) approximate Wasserstein Newton direction of the KL divergence, following the standard practice in modern gradient-boosting libraries [21; 22] to use the diagonal Newton direction of a loss. Similarly to smoothed Wasserstein gradient \(\mathcal{G}_{i}^{*}(\mu)\), the approximate Wasserstein Hessian of the KL divergence \(\mathrm{KL}(\mu\mid\mu_{i})\) can be obtained through the kernel smoothing. The diagonal of the approximate Wasserstein Hessian, denoted \(\mathcal{H}_{i}^{*}(\mu)\), is defined by

\[\left[\mathcal{H}_{i}^{*}(\mu)\right](\theta):=\mathbb{E}_{\theta^{*}\sim\mu} \Big{[}-\nabla_{\mathrm{d}}^{2}\log\mu_{i}(\theta^{*})k(\theta,\theta^{*})^{2 }+\nabla k(\theta,\theta^{*})\odot\nabla k(\theta,\theta^{*})\Big{]}\in \mathbb{R}^{d}.\] (8)

The diagonal approximate Wasserstein Newton direction of the KL divergence is then defined by \(-\left[\mathcal{G}_{i}^{*}(\mu)\right](\cdot)\oslash\left[\mathcal{H}_{i}^{*}( \mu)\right](\cdot)\). Appendix C provides the derivation based on [39] who derived the Newton direction of the KL divergence in the context of nonparametric variational inference.

The second-order WGBoost algorithm is established by plugging it into \(\mathcal{G}_{i}(\mu)\) in Algorithm 1, that is,

\[\left[\mathcal{G}_{i}(\mu)\right](\cdot)=\left[\mathcal{G}_{i}^{*}(\mu)\right] (\cdot)\oslash\left[\mathcal{H}_{i}^{*}(\mu)\right](\cdot).\] (9)

Algorithm 1 under the choice (9) is considered our default WGBoost algorithm for evidential learning. We refer this algorithm to as the _Wasserstein-boosted evidential learning_ (WEvidential). The explicit pseudocode is provided in Algorithm 2 for full clarity.

``` Input: dataset \(\{x_{i},y_{i}\}_{i=1}^{D}\) of input \(x_{i}\) and response \(y_{i}\) of classification or regression Parameter : individual-level posterior \(p(\theta\mid y_{i})\) of response distribution \(p(y\mid\theta)\), particle number \(N\), iteration \(M\), learning rate \(\nu\), weak learner \(f\), initial constants \(\{\vartheta_{0}^{n}\}_{n=1}^{N}\) Output: set of \(N\) boosting ensembles \((F_{M}^{1},\ldots,F_{M}^{N})\) at final step \(M\) \((F_{0}^{1}(\cdot),\ldots,F_{0}^{N}(\cdot))\leftarrow(\vartheta_{0}^{1},\ldots, \vartheta_{0}^{N})\)\(\triangleright\) set initial state of \(N\) boostings for\(m\gets 0,\ldots,M-1\)do for\(i\gets 1,\ldots,D\)do \((\theta_{1}^{1},\ldots,\theta_{N}^{N})\leftarrow(F_{1}^{1}(x_{i}),\ldots,F_{m }^{N}(x_{i}))\)\(\triangleright\) get output particles for \(i\)-th data input for\(n\gets 1,\ldots,N\)do \(g_{i}^{n}\leftarrow\frac{1}{N}\sum_{k=1}^{N}\nabla\log p(\theta_{i}^{k}\mid y_ {i})k(\theta_{i}^{k},\theta_{i}^{n})+\nabla k(\theta_{i}^{k},\theta_{i}^{n})\) \(h_{i}^{n}\leftarrow\frac{1}{N}\sum_{k=1}^{N}-\nabla_{\text{d}}^{2}\log p( \theta_{i}^{k}\mid y_{i})k(\theta_{i}^{k},\theta_{i}^{n})^{2}+\nabla k(\theta_ {i}^{k},\theta_{i}^{n})\odot\nabla k(\theta_{i}^{k},\theta_{i}^{n})\)  end for for for\(n\gets 1,\ldots,N\)do \(f_{m+1}^{n}\leftarrow\text{fit}\big{(}\left\{x_{i},g_{i}^{n}\odot h_{i}^{n} \right\}_{i=1}^{D}\big{)}\)\(\triangleright\) fit \(n\)-th new weak learner \(F_{m+1}^{n}(\cdot)\gets F_{m}^{n}(\cdot)+\nu f_{m+1}^{n}(\cdot)\)\(\triangleright\) set next state of \(n\)-th boosting  end for  end for ```

**Algorithm 2**Wasserstein-Boosted Evidential Learning

**Remark 5** (**Computation)**.: The diagonal Newton direction (9) has the computational complexity \(\mathcal{O}(N\times d)\) same as that of the smoothed Wasserstein gradient. Hence, there is essentially _no reason not to use_ the diagonal Newton direction (9) instead of the smoothed Wasserstein gradient. Although it is possible to use the full Newton direction with no diagonal approximation, the computation requires the inverse and product of \((N\times d)\times(N\times d)\) matrices that result in the complexity up to \(\mathcal{O}(N^{3}\times d^{3})\). Appendix D presents a simulation study to compare computational time and convergence speed of four WGBoost algorithms built on different estimates of the Wasserstein gradient.

## 4 Experiment on Real-world Tabular Data

We empirically demonstrate the performance of the WGBoost algorithm through three experiments using real-world tabular data. The first application illustrates the output of WGBoost through a simple conditional density estimation. The second application benchmarks the probabilistic regression performance. The third application demonstrates the classification and OOD detection performance. The source code is available in https://github.com/takuomatsubara/WGBoost.

**Common Hyperparameters** Throughout, we set the number of output particles \(N\) to \(10\) and set each weak learner \(f\) to the decision tree regressor [50] with maximum depth \(1\) for Section 4.1 and \(3\) for the rest. We set the learning rate \(\nu\) to \(0.1\) for regression and \(0.4\) for classification, unless otherwise stated. Appendix E contains further details, including a choice of the initial constant \(\{\vartheta_{0}^{n}\}_{n=1}^{N}\).

### Illustrative Conditional Density Estimation

We illustrate the output of WEvidential by estimating a conditional density \(p(y\mid x)\) from one-dimensional scalar inputs and outputs \(\{x_{i},y_{i}\}_{i=1}^{D}\). The normal output distribution \(\mathcal{N}(y\mid m,\sigma)\) and the prior \(p_{i}(m,\sigma)\) in Example 1 were used to define the individual-level posterior \(p(m,\sigma\mid y_{i})\), in which case the output of the WGBoost algorithm is a set of \(10\) particles \(\{(m^{n}(x),\sigma^{n}(x))\}_{n=1}^{10}\) of the mean and scale parameters for each input \(x\). We chose the number of weak learners \(M\), drawing on an early-stopping approach used in [32], where we held out 20% of the training set as a validation set and chose the number \(1\leq M\leq 4000\) achieving the least validation error. Once the number \(M\) was chosen, WEvidential was trained again using all the entire training set.

The conditional density is estimated using the predictive distribution (6) by WEvidential. We used two real-world datasets, _bone mineral density_[51] and _old faithful geyser_[52]. Figure 3 depicts the result for the former dataset, demonstrating that the WGBoost algorithm captures the heterogeneity of the conditional density on each input well. The result for the latter dataset is contained in Appendix E.1.

### Probabilistic Regression Benchmark

We examine the regression performance of WEvidential using a standard benchmark protocol that originated in [53] and has been used in a number of subsequent works [10; 9; 32]. The benchmark protocol uses real-world tabular datasets from the UCI machine learning repository [54], each with one-dimensional scalar responses. As in Section 4.1, the normal response distribution \(\mathcal{N}(y\mid m,\sigma)\) and the prior \(p_{i}(m,\sigma)\) in Example 1 were used to define the individual-level posterior \(p(m,\sigma\mid y_{i})\).

We followed the data splitting protocol in [53] and randomly held out 10% of each dataset as a test set. The negative log likelihood (NLL) is measured by using the predictive distribution (6). The root mean squared error (RMSE) is measured by using the point prediction by the mean value of the predictive distribution. We chose the number of weak learners \(M\) by the same approach as in Section 4.1. We repeated this procedure 20 times for each dataset, except the _protein_ and _year msd_ datasets for which we repeated five times and once. For the year msd dataset only, we subsampled 10% of data to fit each weak learner and used the learning rate 0.01 due to the large dataset size.

Table 1 compares the performance of WEvidential with five other methods: Monte Carlo Dropout (MCDropout) [9], Deep Ensemble (DEnsemble) [10], Concrete Dropout (CDropout) [55], Natural Gradient Boosting (NGBoost) [32], and Deep Evidential Regression (DEvidential) [13]. Appendix E provides further details on the experiment and a limited yet additional comparison. The WGBoost algorithm achieves the best score or a score sufficiently close to the best score most often.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Dataset & Criteria & WEvidential & MCDropout & DEnsemble & CDropout & NGBoost & DEvidential \\ \hline \multirow{3}{*}{\begin{tabular}{c} boston \\ concrete \\ energy \\ kin8nm \\ \end{tabular} } & \(\mathbf{2.47\pm 0.16}\) & \(2.46\pm 0.06\) & \(\mathbf{2.41\pm 0.25}\) & \(2.72\pm 0.01\) & \(\mathbf{2.43\pm 0.15}\) & \(\mathbf{2.35\pm 0.06}\) \\  & \(\mathbf{2.83\pm 0.11}\) & \(3.04\pm 0.02\) & \(3.06\pm 0.18\) & \(3.51\pm 0.00\) & \(3.04\pm 0.17\) & \(3.01\pm 0.02\) \\ \cline{2-6}  & \(\mathbf{0.53\pm 0.08}\) & \(1.99\pm 0.02\) & \(1.38\pm 0.22\) & \(2.30\pm 0.00\) & \(\mathbf{6.00\pm 0.45}\) & \(1.39\pm 0.06\) \\ \cline{2-6}  & -0.44 \(\pm\) 0.03 & -0.95 \(\pm\) 0.01 & -1.20 \(\pm\) 0.02 & -0.65 \(\pm\) 0.00 & -0.49 \(\pm\) 0.02 & -\(\mathbf{1.24\pm 0.01}\) \\ \cline{2-6}  & NLL & -5.47 \(\pm\) 0.03 & -3.80 \(\pm\) 0.01 & -5.63 \(\pm\) 0.05 & **-5.87\(\pm\) 0.05** & -5.34 \(\pm\) 0.04 & -5.73 \(\pm\) 0.07 \\ \cline{2-6}  & power & \(\mathbf{2.60\pm 0.04}\) & \(2.80\pm 0.01\) & \(2.79\pm 0.04\) & \(2.75\pm 0.01\) & \(2.79\pm 0.11\) & \(2.81\pm 0.07\) \\ \cline{2-6}  & \(2.70\pm 0.01\) & \(2.89\pm 0.00\) & \(2.83\pm 0.02\) & \(2.81\pm 0.00\) & \(2.81\pm 0.03\) & \(\mathbf{2.63\pm 0.00}\) \\ \cline{2-6}  & wine & \(\mathbf{0.95\pm 0.08}\) & \(0.93\pm 0.01\) & \(\mathbf{0.94\pm 0.12}\) & \(1.70\pm 0.00\) & \(\mathbf{0.91\pm 0.06}\) & \(\mathbf{0.89\pm 0.05}\) \\ \cline{2-6}  & yacht & \(\mathbf{0.16\pm 0.24}\) & \(1.55\pm 0.03\) & \(1.18\pm 0.21\) & \(1.75\pm 0.00\) & \(\mathbf{0.20\pm 0.26}\) & \(1.03\pm 0.19\) \\ \cline{2-6}  & year msd & \(3.50\pm 0.01\) & \(3.59\pm 0.01\) & \(\mathbf{3.35\pm 0.01}\) & NA\(\pm\) NA & NA \(\pm\) NA \\ \hline \multirow{3}{*}{
\begin{tabular}{c} boston \\ concrete \\ energy \\ kin8nm \\ \end{tabular} } & \(\mathbf{2.78\pm 0.60}\) & \(2.97\pm 0.19\) & \(\mathbf{3.28\pm 1.00}\) & \(\mathbf{2.65\pm 0.17}\) & \(\mathbf{2.94\pm 0.53}\) & \(3.06\pm 0.16\) \\ \cline{2-6}  & \(\mathbf{4.15\pm 0.52}\) & \(5.23\pm 0.12\) & \(6.03\pm 0.58\) & \(4.46\pm 0.16\) & \(5.06\pm 0.61\) & \(5.85\pm 0.15\) \\ \cline{2-6}  & \(\mathbf{0.42\pm 0.07}\) & \(1.66\pm 0.04\) & \(2.09\pm 0.29\) & \(0.46\pm 0.02\) & \(\mathbf{0.46\pm 0.06}\) & \(2.06\pm 0.10\) \\ \cline{2-6}  & \(\mathbf{0.15\pm 0.00}\) & \(0.10\pm 0.00\) & \(0.09\pm 0.00\) & \(\mathbf{0.07\pm 0.00}\) & \(0.16\pm 0.00\) & \(0.09\pm 0.00\) \\ \cline{2-6}  & \(\mathbf{0.00\pm 0.00}\) & \(0.01\pm 0.00\) & \(\mathbf{0.00\pm 0.00}\) & \(\mathbf{0.00\pm 0.00}\) & \(\mathbf{0.00\pm 0.00}\) & \(\mathbf{0.00\pm 0.00}\) \\ \cline{2-6}  & power & \(\mathbf{3.19\pm 0.25}\) & \(4.02\pm 0.04\) & \(4.11\pm 0.17\) & \(3.70\pm 0.04\) & \(3.79\pm 0.18\) & \(4.23\pm 0.09\) \\ \cline{2-6}  & protein & \(4.09\pm 0.02\) & \(4.36\pm 0.01\) & \(4.71\pm 0.06\) & \(\mathbf{3.35\pm 0.02}\) & \(4.33\pm 0.03\) & \(4.64\pm 0.03\) \\ \cline{2-6}  & wine & \(\mathbf{0.61\pm 0.05}\) & \(\mathbf{0.62\pm 0.01}\) & \(\mathbf{0.64\pm 0.04}\) & \(0.62\pm 0.00\) & \(\mathbf{0.63\pm 0.04}\) & \(\mathbf{0.61\pm 0.02}\) \\ \cline{2-6}  & yacht & \(\mathbf{0.48\pm 0.18}\) & \(1.11\pm 0.09\) & \(1.58\pm 0.48\) & \(0.57\pm 0.05\) & \(\mathbf{0.50\pm 0.20}\) & \(1.57\pm 0.56\) \\ \cline{2-6}  & year msd & \(9.11\pm 0.00\) & \(\mathbf{8.85\pm 0.01}\) & \(8.89\pm 0.00\) & NA\(\pm\) NA & NA & NA \(\pm\) NA \\ \hline \hline \end{tabular}
\end{table}
Table 1: The NLLs and RMSEs for each dataset, where the best score is underlined and the scores whose standard deviation ranges include the best score are in bold. Results of MCDropout, DEnsembles, CDropout, NGBoost, and DEvidential were reported in [9; 10], [55], [32] and [13] respectively.

Figure 3: Conditional density estimation for the bone mineral density dataset (grey dots) by WEvidential, where the normal response distribution \(\mathcal{N}(y\mid m,\sigma)\) is used for the response variable \(y\). Left: distributional estimate (10 particles) of the location parameter \(\{m^{n}(x)\}_{n=1}^{10}\) for each input. Right: estimated conditional density (6) through marginalisation of the output particles \(\{(m^{n}(x),\sigma^{n}(x))\}_{n=1}^{10}\).

### Classification and Out-of-Distribution Detection

We examine the classification and anomaly OOD detection performance of WEvidential on two real-world tabular datasets, _segment_ and _sensorless_, following the protocol used in [14]. The categorical response distribution \(\mathcal{C}(y\mid q)\) and the prior \(p_{i}(q)\) in Example 2 were used to define the individual-level posterior \(p(q\mid y_{i})\), in which case the output of the WGBoost algorithm is a set of \(10\) particles \(\{q^{n}\}_{n=1}^{10}\) of the class probability parameter \(q\) in the simplex \(\Delta^{k}\) for each input \(x\). We set the number of weak learners \(M\) to \(4000\) without early stopping to reduce the computational cost.

The segment and sensorless datasets have 7 and 11 classes in total. For the segment dataset, the data subset that belongs to the last class was kept as the OOD samples. For the sensorless dataset, the data subset that belongs to the last two classes was kept as the OOD samples. For each dataset, 20% of the non-OOD samples is held out as a test set to measure the classification accuracy. There exist several ways of defining a OOD score for each input [56]. For the WGBoost algorithm, the inverse of the maximum norm of the output-particle variance was used as the OOD score. We measured the OOD detection performance by the area under the precision recall curve (PR-AUC), viewing non-OOD test data as the positive class and OOD data as the negative class. We repeated this procedure five times.

Table 2 compares the performance of WEvidential with four other methods: MCDropout, DEnsemble, and Distributional Distillation (DDistillation) [57], and Posterior Network (PNetwork) [14]. Appendix E provides further details on the experiment. Figure 4 exemplifies how the dispersion of the output particles differ between OOD and non-OOD inputs. WEvidential demonstrates a high classification and OOD detection accuracy simultaneously. Although PNetwork has the best OOD detection performance for the sensorless dataset, the performance of the WGBoost algorithm also exceeds 80%, which is distinct from MCDropout, DEnsemble, and DDistillation.

## 5 Discussion

This work established the general framework of WGBoost and developed the concrete algorithm WEvidential for evidential learning. The established framework of WGBoost offers exciting avenues for future research. Important directions for future study include (i) exploring alternative loss functionals to the KL divergence, (ii) investigating the convergence properties, and (iii) evaluating robustness of obtained predictive uncertainty in comparison to other methods. A particular limitation of WGBoost may arise when data are not tabular, as is the case of standard gradient boosting. These questions require careful examination and are critical for future study.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Dataset & Criteria & WEvidential & MCDropout & DEnsemble & DDistillation & PNetwork \\ \hline \multirow{3}{*}{segment} & Accuracy & \(96.57\pm 0.6\) & \(95.25\pm 0.1\) & \(\mathbf{97.27\pm 0.1}\) & \(96.21\pm 0.1\) & \(96.92\pm 0.1\) \\  & OOD & \(\mathbf{99.67\pm 0.2}\) & \(43.11\pm 0.6\) & \(58.13\pm 1.7\) & \(35.83\pm 0.4\) & \(96.74\pm 0.9\) \\ \cline{1-1}  & Accuracy & \(\mathbf{99.54\pm 0.1}\) & \(89.32\pm 0.2\) & \(99.37\pm 0.0\) & \(93.66\pm 1.5\) & \(99.52\pm 0.0\) \\ \cline{1-1}  & OOD & \(81.13\pm 5.3\) & \(40.61\pm 0.7\) & \(50.62\pm 0.1\) & \(31.17\pm 0.2\) & \(\mathbf{88.65\pm 0.4}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: The classification accuracies and OOD detection PR-AUCs for each dataset, where the best score is underlined and in bold. The results other than WEvidential were reported in [14].

Figure 4: Examples of the output particles (red dot) of WEvidential on the segment dataset, where the coloured area indicate the kernel density estimation of the output particles for each class.

## References

* [1] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. _Information Fusion_, 81:84-90, 2022.
* [2] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U. Rajendra Acharya, Vladimir Makarenkov, and Saeid Nahavandi. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. _Information Fusion_, 76:243-297, 2021.
* [3] Eric Topol. High-performance medicine: the convergence of human and artificial intelligence. _Nature Medicine_, 25:44-56, 2019.
* [4] Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu. A survey of deep learning techniques for autonomous driving. _Journal of Field Robotics_, 37(3):362-386, 2020.
* [5] Matthew Richardson, Ewa Dominowska, and Robert Ragno. Predicting clicks: estimating the click-through rate for new ads. In _Proceedings of the 16th International Conference on World Wide Web_, page 521-530, 2007.
* [6] Christopher Burges. From ranknet to lambdarank to lambdamart: An overview. _Learning_, 11, 2010.
* [7] Byron P. Roe, Hai-Jun Yang, Ji Zhu, Yong Liu, Ion Stancu, and Gordon McGregor. Boosted decision trees as an alternative to artificial neural networks for particle identification. _Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment_, 543(2):577-584, 2005.
* [8] James Bennett and Stan Lanning. The Netflix prize. In _Proceedings of the KDD Cup Workshop 2007_, pages 3-6, 2007.
* [9] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 1050-1059. PMLR, 2016.
* [10] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _Advances in Neural Information Processing Systems_, volume 30, 2017.
* [11] Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classification uncertainty. In _Advances in Neural Information Processing Systems_, volume 31, 2018.
* [12] Jakob Gawlikowski, Cedrique Rovile Nijeutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Muhammad Shahzad, Wen Yang, Richard Bamler, and Xiao Xiang Zhu. A survey of uncertainty in deep neural networks. _Artificial Intelligence Review_, 56:1513-1589, 2023.
* [13] Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep evidential regression. In _Advances in Neural Information Processing Systems_, volume 33, pages 14927-14937, 2020.
* [14] Bertrand Charpentier, Daniel Zugner, and Stephan Gunnemann. Posterior network: Uncertainty estimation without OOD samples via density-based pseudo-counts. In _Advances in Neural Information Processing Systems_, volume 33, pages 1356-1367. Curran Associates, Inc., 2020.
* [15] Dennis Thomas Ulmer, Christian Hardmeier, and Jes Frellsen. Prior and posterior networks: A survey on evidential deep learning methods for uncertainty estimation. _Transactions on Machine Learning Research_, 2023.
* [16] Edouard Capellier, Franck Davoine, Veronique Cherfaoui, and You Li. Evidential deep learning for arbitrary lidar object classification in the context of autonomous driving. In _2019 IEEE Intelligent Vehicles Symposium (IV)_, pages 1304-1311, 2019.

* [17] Patrick Hemmer, Niklas Kuhl, and Jakob Schoffer. Deal: Deep evidential active learning for image classification. In _2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)_, pages 865-870, 2020.
* [18] Ava P. Soleimany, Alexander Amini, Samuel Goldman, Daniela Rus, Sangeeta N. Bhatia, and Connor W. Coley. Evidential deep learning for guided molecular property prediction and discovery. _ACS Central Science_, 7(8):1356-1367, 2021.
* [19] Jakob Gawlikowski, Sudipan Saha, Anna Kruspe, and Xiao Xiang Zhu. An advanced Dirichlet prior network for out-of-distribution detection in remote sensing. _IEEE Transactions on Geoscience and Remote Sensing_, 60:1-19, 2022.
* [20] Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. _Bayesian Data Analysis_. Chapman and Hall/CRC, 3rd ed. edition, 2013.
* [21] Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, KDD '16, page 785-794, 2016.
* [22] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. LightGBM: A highly efficient gradient boosting decision tree. In _Advances in Neural Information Processing Systems_, volume 30, 2017.
* [23] C'edric Villani. _Topics in Optimal Transportation_. Americal Mathematical Society, 2003.
* [24] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savare. _Gradient Flows In Metric Spaces and in the Space of Probability Measures_. Birkhauser Basel, 2005.
* [25] Filippo Santambrogio. {Euclidean, metric, and Wasserstein} gradient flows: an overview. _Bulletin of Mathematical Sciences_, 7:87-154, 2017.
* [26] Qiang Liu. Stein variational gradient descent as gradient flow. In _Advances in Neural Information Processing Systems_, volume 30, 2017.
* [27] Jos'e Antonio Carrillo, Katy Craig, and Francesco S. Patacchini. A blob method for diffusion. _Calculus of Variations and Partial Differential Equations_, 58(53), 2019.
* [28] Yifei Wang, Peng Chen, and Wuchen Li. Projected Wasserstein gradient descent for high-dimensional Bayesian inference. _SIAM/ASA Journal on Uncertainty Quantification_, 10(4):1513-1532, 2022.
* [29] Dimitra Maoutsa, Sebastian Reich, and Manfred Opper. Interacting particle solutions of Fokker-Planck equations through gradient-log-density estimation. _Entropy (Basel)_, 22(8):802, 2020.
* [30] Ye He, Krishnakumar Balasubramanian, Bharath K. Sriperumbudur, and Jianfeng Lu. Regularized Stein variational gradient flow. _arXiv:2211.07861_, 2022.
* [31] Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. _The Annals of Statistics_, 29(5):1189-1232, 2001.
* [32] Tony Duan, Avati Anand, Daisy Yi Ding, Khanh K. Thai, Sanjay Basu, Andrew Ng, and Alejandro Schuler. XGBoost: Natural gradient boosting for probabilistic prediction. In _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 2690-2700. PMLR, 2020.
* 505, 2007.
* [34] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? In _Advances in Neural Information Processing Systems_, volume 35, pages 507-520, 2022.
* [35] Piotr Florek and Adam Zagdanski. Benchmarking state-of-the-art gradient boosting algorithms for classification. _arXiv:2305.17094_, 2023.

* [36] Zhendong Zhang and Cheolkon Jung. GBDT-MO: Gradient-boosted decision trees for multiple outputs. _IEEE Transactions on Neural Networks and Learning Systems_, 32(7):3156-3167, 2021.
* [37] Tianqi Chen, Sameer Singh, Ben Taskar, and Carlos Guestrin. Efficient Second-Order Gradient Boosting for Conditional Random Fields. In _Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics_, volume 38 of _Proceedings of Machine Learning Research_, pages 147-155. PMLR, 2015.
* [38] Jerome H. Friedman. Stochastic gradient boosting. _Computational Statistics & Data Analysis_, 38(4):367-378, 2002.
* [39] Gianluca Detommaso, Tiangang Cui, Youssef Marzouk, Alessio Spantini, and Robert Scheichl. A Stein variational Newton method. In _Advances in Neural Information Processing Systems_, volume 31, 2018.
* [40] Yifei Wang and Wuchen Li. Information Newton's flow: second-order optimization method in probability space. _arXiv:2001.04341_, 2020.
* [41] Malay Ghosh. Objective priors: An introduction for frequentists. _Statistical Science_, 26(2):187-202, 2011.
* [42] J. Aitchison and S. M. Shen. Logistic-normal distributions: Some properties and uses. _Biometrika_, 67(2):261-272, 1980.
* [43] Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian inference algorithm. In _Advances in Neural Information Processing Systems_, volume 29, 2016.
* [44] Dilin Wang, Zhe Zeng, and Qiang Liu. Stein variational message passing for continuous graphical models. In _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 5219-5227. PMLR, 2018.
* [45] Alexander Lambert, Fabio Ramos, Byron Boots, Dieter Fox, and Adam Fishman. Stein variational model predictive control. In _Proceedings of the 2020 Conference on Robot Learning_, volume 155 of _Proceedings of Machine Learning Research_, pages 1278-1297. PMLR, 2021.
* [46] Anna Korba, Adil Salim, Michael Arbel, Giulia Luise, and Arthur Gretton. A non-asymptotic analysis for Stein variational gradient descent. In _Advances in Neural Information Processing Systems_, volume 33, pages 4672-4682, 2020.
* [47] Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, and Philippe Rigollet. SVGD as a kernelized Wasserstein gradient flow of the chi-squared divergence. In _Advances in Neural Information Processing Systems_, volume 33, pages 2098-2109, 2020.
* [48] Andre Wibisono. Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem. In _Proceedings of the 31st Conference On Learning Theory_, volume 75 of _Proceedings of Machine Learning Research_, pages 2093-3027. PMLR, 2018.
* 363, 1996.
* [50] Leo Breiman, Jerome Friedman, R.A. Olshen, and Charles J. Stone. _Classification and Regression Trees_. Chapman and Hall/CRC, 1984.
* [51] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. _The Elements of Statistical Learning_. Springer New York, 2009.
* [52] Sanford Weisberg. _Applied Linear Regression_. John Wiley & Sons, 1985.
* [53] Jose Miguel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of Bayesian neural networks. In _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 1861-1869. PMLR, 2015.

* [54] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
* [55] Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. In _Advances in Neural Information Processing Systems_, volume 30, 2017.
* [56] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. _arXiv:2110.11334_, 2024.
* [57] Andrey Malinin, Bruno Mlodozeniec, and Mark Gales. Ensemble distribution distillation. In _International Conference on Learning Representations_, 2020.
* [58] Filippo Santambrogio. _Optimal Transport for Applied Mathematicians_. Birkhauser Cham, 2015.
* [59] Mingxuan Yi and Song Liu. Bridging the gap between variational inference and Wasserstein gradient flows, 2023.
* [60] Michael Arbel, Anna Korba, Adil Salim, and Arthur Gretton. Maximum mean discrepancy gradient flow. In _Advances in Neural Information Processing Systems_, volume 32, 2019.
* [61] Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the Fokker-Planck equation. _SIAM Journal on Mathematical Analysis_, 29(1):1-17, 1998.
* [62] Grigorios A. Pavliotis. _Stochastic Processes and Applications_. Springer New York, 2014.
* [63] Vern I. Paulsen and Mrinal Raghupathi. _An Introduction to the Theory of Reproducing Kernel Hilbert Spaces_. Cambridge University Press, 2016.
* [64] Alex Leviyev, Joshua Chen, Yifei Wang, Omar Ghattas, and Aaron Zimmerman. A stochastic Stein variational Newton method. _arXiv:2204.09039_, 2022.
* [65] Alex Smola, Arthur Gretton, Le Song, and Bernhard Scholkopf. A Hilbert space embedding for distributions.
* [66] Shun ichi Amari. _Information Geometry and Its Applications_. Springer Tokyo, 2016.
* [67] Pavel Izmailov, Wesley J. Maddox, Polina Kirichenko, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Subspace inference for bayesian deep learning. In _Proceedings of 35th Conference on Uncertainty in Artificial Intelligence_, 2019.

**Appendix**

This appendix contains the technical and experiment details referred to in the main text. Appendix A recaps derivation of the Wasserstein gradient. Appendix B discusses the variant of WGBoost built on the unadjusted Langevin algorithm. Appendix C derives the diagonal approximate Wasserstein Newton direction used for WEvidential. Appendix D provides simulation studies on kernel choice of WEvidential and comparison of WGBoost algorithms built on different estimate of the Wasserstein gradient. Appendix E contains the additional details of the experiment presented in the main text.

## Appendix A Derivation and Example of Wasserstein Gradient

This section recaps derivation of the Wasserstein gradient of a functional \(\mathcal{F}\), with examples of common divergences. The Wasserstein gradient depends on a function on \(\Theta\) called the _first variation_[24]. The first variation \(\delta\mathcal{F}(\mu)/\delta\mu\) of the functional \(\mathcal{F}\) at \(\mu\) is a function on \(\Theta\) that satisfies

\[\lim_{\epsilon\to 0^{+}}\frac{\mathcal{F}(\mu+\epsilon\nu)-\mathcal{F}(\mu)}{ \epsilon}=\int_{\Theta}\frac{\delta\mathcal{F}(\mu)}{\delta\mu}(\theta)\nu( \theta)d\theta\]

for all signed measure \(\nu\) s.t. \(\mu+\epsilon\nu\in\mathcal{P}_{2}\) for all \(\epsilon\) sufficiently small. The Wasserstein gradient \(\nabla_{W}\mathcal{F}(\mu)\) of the functional \(\mathcal{F}\) at \(\mu\) is derived as the gradient of the first variation (see [e.g. 24]):

\[[\nabla_{W}\mathcal{F}(\mu)](\theta):=\nabla\frac{\delta\mathcal{F}(\mu)}{ \delta\mu}(\theta).\]

It is common to suppose that the functional \(\mathcal{F}\) consists of three energies, which are determined by functions \(U:\mathbb{R}\to\mathbb{R}\), \(V:\Theta\to\mathbb{R}\), and \(W:\Theta\to\mathbb{R}\) respectively, such that

\[\mathcal{F}(\mu)=\underbrace{\int_{\Theta}U(\mu(\theta))d\theta}_{\text{ internal energy}}+\underbrace{\int_{\Theta}V(\theta)\mu(\theta)d\theta}_{\text{potential energy}}+\underbrace{\frac{1}{2}\int_{\Theta\times\Theta}W(\theta-\theta^{\prime})\mu( \theta)d\theta\mu(\theta^{\prime})d\theta^{\prime}}_{\text{interaction energy}}.\]

For a functional \(\mathcal{F}\) that falls into the above form, the Wasserstein gradient is derived as

\[[\nabla_{W}\mathcal{F}(\mu)]\left(\theta\right)=\nabla U^{\prime}(\mu( \theta))+\nabla V(\theta)+\int_{\Theta}\nabla W(\theta-\theta^{\prime})\mu( \theta^{\prime})d\theta^{\prime}\]

where \(U^{\prime}\) is the derivative of \(U:\mathbb{R}\to\mathbb{R}\)[23]. The KL divergence \(\mathcal{F}(\mu)=\mathrm{KL}(\mu\mid\pi)\) of a distribution \(\pi\) falls into the form with \(U(x)=x\log x\), \(V(\theta)=-\log\pi(\theta)\), and \(W(\theta)=0\), where

\[\mathrm{KL}(\mu\mid\pi)=\int_{\Theta}\log\mu(\theta)\mu(\theta)d\theta+\int_{ \Theta}-\log\pi(\theta)\mu(\theta)d\theta.\]

Table 3 presents examples of Wasserstein gradients of common divergences \(\mathcal{F}(\mu)=\mathrm{D}(\mu\mid\pi)\).

In the context of Bayesian inference, the KL divergence is particularly useful among many divergences. The Wasserstein gradient of the KL divergence requires no normalising constant of a posterior distribution \(\pi\). This is because the Wasserstein gradient depends only on the log-gradient of the posterior \(\nabla\log\pi(\theta)=\nabla\pi(\theta)/\pi(\theta)\) of the target \(\pi\), in which case the normalising constant of the target \(\pi\) is cancelled out by fraction. Hence, any posterior known only up to the normalising constant can be used as the target distribution \(\pi\) in the Wasserstein gradient of the KL divergence.

\begin{table}
\begin{tabular}{c c} \hline \hline Divergence \(\mathcal{F}(\mu)=\mathrm{D}(\mu\mid\pi)\) & Wasserstein gradient \([\nabla_{W}\mathcal{F}(\mu)]\left(\theta\right)\) \\ \hline \(\mathrm{KL}(\mu\mid\pi)\) & \(-(\nabla\log\pi(\theta)-\nabla\log\mu(\theta))\) \\ \(\mathrm{Chi}^{2}(\mu\mid\pi)\) & \(2\nabla(\mu(\theta)/\pi(\theta))\) \\ Alpha\((\mu\mid\pi)\) & \((\mu(\theta)/\pi(\theta))^{\alpha-1}\nabla(\mu(\theta)/\pi(\theta))\) \\ MMD\((\mu\mid\pi)\) & \(\int_{\Theta}\nabla k(\theta,\theta^{\prime})\mu(\theta)d\theta-\int_{\Theta} \nabla k(\theta,\theta^{\prime})\pi(\theta)d\theta\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Wasserstein gradients of four divergences: the KL divergence [58], the chi-squared divergence [47], the alpha divergence [59], and the maximum mean discrepancy [60].

Langevin Gradient Boosting for KL Divergence

If a chosen functional \(\mathcal{F}\) on \(\mathcal{P}_{2}\) is the KL divergence \(\mathcal{F}(\mu)=\mathrm{KL}(\mu\mid\pi)\) of a target distribution \(\pi\), the continuity equation (1) admits an equivalent representation as the Fokker-Planck equation [61]:

\[\frac{d}{dt}\mu_{t}=\nabla\cdot(\mu_{t}\nabla\log\pi)+\Delta\mu_{t}\quad\text{ given}\quad\mu_{0}\in\mathcal{P}_{2}\] (10)

where \(\Delta\) denotes the Laplacian operator. Recall that the original continuity equation (1) can be reformulated as the deterministic differential equation (2) of a random variable \(\theta_{t}\sim\mu_{t}\). In contrast, the Fokker-Planck equation (10) can be reformulated as a stochastic differential equation of a random variable \(\theta_{t}\sim\mu_{t}\), known as the overdamped Langevin dynamics [62]:

\[d\theta_{t}=\nabla\log\pi(\theta_{t})dt+\sqrt{2}dB_{t}\quad\text{ given}\quad\theta_{0}\sim\mu_{0},\] (11)

where \(B_{t}\) denotes a standard Brownian motion. Note that the deterministic system (2) in the case of the KL divergence and the above stochastic system (11) are equivalent at population level, in a sense that the law of the random variable \(\theta_{t}\) in both the systems solves the two equivalent equations.

At the algorithmic level, however, discretisation of each system leads to different particle update schemes. Set the initial distribution \(\mu_{0}\) in (11) to the empirical distribution \(\hat{\mu}_{0}\) of \(N\) initial particles \(\{\theta_{0}^{n}\}_{n=1}^{N}\). Discretising the stochastic system (11) by the Euler-Maruyama method with a step size \(\nu>0\) yields a stochastic update scheme of particles \(\{\theta_{m}^{n}\}_{n=1}^{N}\) from step \(m=0\):

\[\begin{bmatrix}\theta_{m+1}^{1}\\ \vdots\\ \theta_{m+1}^{N}\end{bmatrix}=\begin{bmatrix}\theta_{m}^{1}\\ \vdots\\ \theta_{m}^{N}\end{bmatrix}+\nu\begin{bmatrix}\nabla\log\pi(\theta_{m}^{1})+ \sqrt{2/\nu}\;\xi^{1}\\ \vdots\\ \nabla\log\pi(\theta_{m}^{N})+\sqrt{2/\nu}\;\xi^{N}\end{bmatrix},\]

where each \(\xi^{n}\) denotes a realisation from a standard normal distribution on \(\mathbb{R}^{d}\). The above updating scheme of each \(n\)-th particle is known as the unadjusted Langevin algorithm [49]. We can define a variant of WGBoost by replacing the term \(\mathcal{G}_{i}(\mu)\) in Algorithm 1 with \(\nabla\log\mu_{i}(\cdot)+\sqrt{2/\nu}\;\xi_{i}\) where \(\mu_{i}\) is an output distribution at each \(x_{i}\) and \(\xi_{i}\) is a realisation from a standard normal distribution. The procedure is summarised in Algorithm 3, which we call Langevin gradient boosting (LGBoost).

``` Input: training set \(\{x_{i},\mu_{i}\}_{i=1}^{D}\) of input \(x_{i}\in\mathcal{X}\) and output distribution \(\mu_{i}\in\mathcal{P}_{2}\) Parameter :particle number \(N\), iteration \(M\), rate \(\nu\), weak learner \(f\), initial constants \((\vartheta_{0}^{1},\dots,\vartheta_{0}^{N})\) Output: set of \(N\) boosting ensembles \((F_{M}^{1},\dots,F_{M}^{N})\) at final step \(M\) \((F_{0}^{1}(\cdot),\dots,F_{0}^{N}(\cdot))\leftarrow(\vartheta_{0}^{1},\dots, \vartheta_{0}^{N})\) for\(m\gets 0,\dots,M-1\)do for\(n\gets 1,\dots,N\)do for\(i\gets 1,\dots,D\)do \(g_{i}^{n}\leftarrow\nabla\log\mu_{i}(F_{m}^{n}(x_{i}))+\sqrt{2/\nu}\;\xi_{i}^{n}\quad \text{where}\quad\xi_{i}^{n}\sim\mathcal{N}(0,I_{d})\) end for\(f_{m+1}^{n}\leftarrow\text{fit}\left(\{x_{i},g_{i}^{n}\}_{i=1}^{D}\right)\) \(F_{m+1}^{n}(\cdot)\gets F_{m}^{n}(\cdot)+\nu f_{m+1}^{n}(\cdot)\)  end for end ```

**Algorithm 3**Langevin Gradient Boosting

## Appendix C Derivation of Approximate Wasserstein Newton Direction

This section derives the diagonal approximate Wasserstein Newton direction based on the kernel smoothing. The approximate Wasserstein Newton direction of the KL divergence was derived in [39] under a different terminology--simply, the Newton direction--from a viewpoint of nonparametric variational inference. We place their result in the context of approximate Wasserstein gradient flows. Appendix C.1 shows the derivation of the smoothed Wasserstein gradient and Hessian. Appendix C.2 defines the Newton direction built upon the smoothed Wasserstein gradient and Hessian, following the derivation in [39]. Appendix C.3 derives the diagonal approximation of the Newton direction.

### Smoothed Wasserstein Gradient and Hessian

Consider the one-dimensional case \(\Theta=\mathbb{R}\) for simplicity. For a map \(T:\mathbb{R}\to\mathbb{R}\) and a distribution \(\mu\in\mathcal{P}_{2}\), let \(\mu_{t}\) be the pushforward of \(\mu\) under the transform \(\theta\mapsto\theta+tT(\theta)\) defined with a time-variable \(t\in\mathbb{R}\). This means that \(\mu_{t}\) is a distribution obtained by change-of-variable applied for \(\mu\). The Wasserstein gradient of a functional \(\mathcal{F}(\mu)\) can be associated with the time derivative \((d/dt)\mathcal{F}(\mu_{t})\)[23]. In what follows, we focus on the KL divergence \(\mathcal{F}(\mu)=\operatorname{KL}(\mu\mid\pi)\) as a loss functional. Under a condition \(T\in L^{2}(\mu)\), the time derivative at \(t=0\) satisfies the following equality

\[\frac{d}{dt}\operatorname{KL}(\mu_{t}\mid\pi)\Big{|}_{t=0}=\int_{\Theta}\;T( \theta)\left[\mathcal{G}^{\operatorname{KL}}(\mu)\right](\theta)\;d\mu( \theta)=\left\langle T,\mathcal{G}^{\operatorname{KL}}(\mu)\right\rangle_{L^ {2}(\mu)},\] (12)

where \(\mathcal{G}^{\operatorname{KL}}(\mu)\) denotes the Wasserstein gradient of \(\mathcal{F}(\mu)=\operatorname{KL}(\mu\mid\pi)\) with the target distribution \(\pi\) made implicit. It gives an interpretation of the Wasserstein gradient as the steepest-descent direction because the decay of the KL divergence at \(t=0\) is maximised when \(T=-\mathcal{G}^{\operatorname{KL}}(\mu)\).

The'smoothed' Wasserstein gradient can be derived by restricting the transform map \(T\) to a more regulated Hilbert space than \(L^{2}(\mu)\). A reproducing kernel Hilbert space (RKHS) \(H\) associated with a kernel function \(k:\mathbb{R}\times\mathbb{R}\to\mathbb{R}\) is the most common choice of such a Hilbert space [e.g. 26]. An important property of the RKHS \(H\) is that any function \(f\in H\) satisfies the _reproducing property_\(f(\theta)=\langle f(\cdot),k(\theta,\cdot)\rangle_{H}\) under the associated kernel \(k\) and inner product \(\langle\cdot,\cdot\rangle_{H}\)[63]. As discussed in [e.g. 46], applying the reproducing property in (12) under the condition \(T\in H\) and exchanging the integral order, the time derivative satisfies an alternative equality as follows:

\[\frac{d}{dt}\operatorname{KL}(\mu_{t}\mid\pi)\Big{|}_{t=0} =\int_{\Theta}\;\left\langle T(\cdot),k(\theta,\cdot)\right\rangle _{H}\left[\mathcal{G}^{\operatorname{KL}}(\mu)\right](\theta)\;d\mu(\theta)\] \[=\left\langle T(\cdot),\int_{\Theta}\left[\mathcal{G}^{ \operatorname{KL}}(\mu)\right](\theta)k(\theta,\cdot)d\mu(\theta)\right\rangle _{H}=\left\langle T,\mathcal{G}^{*}(\mu)\right\rangle_{H}\] (13)

where \([\mathcal{G}^{*}(\mu)](\cdot):=\int_{\Theta}[\mathcal{G}^{\operatorname{KL}} (\mu)](\theta)k(\theta,\cdot)d\mu(\theta)\) corresponds to the smoothed Wasserstein gradient used in the main text. The decay of the KL divergence at \(t=0\) is maximised by \(T=-\mathcal{G}^{*}(\mu)\).

Similarly, the Wasserstein Hessian of the functional \(\mathcal{F}(\mu)\) can be associated with the second time derivative \((d^{2}/dt^{2})\mathcal{F}(\mu_{t})\)[23]. As discussed in [e.g. 46], the Wasserstein Hessian of the KL divergence, denoted \(\operatorname{Hess}(\mu)\), is an operator over functions \(T\in L^{2}(\mu)\) that satisfies

\[\frac{d^{2}}{dt^{2}}\operatorname{KL}(\mu_{t}\mid\pi)\Big{|}_{t=0}=\left\langle T,\operatorname{Hess}(\mu)T\right\rangle_{L^{2}(\mu)}.\] (14)

See [46] for the explicit form of the Wasserstein Hessian. In the same manner as the smoothed Wasserstein gradient, applying the reproducing property in (14) under the condition \(T\in H\) and exchanging the integral order, the second time derivative satisfies an alternative equality as follows:

\[\frac{d^{2}}{dt^{2}}\operatorname{KL}(\mu_{t}\mid\pi)\Big{|}_{t=0}=\left\langle T (\star_{1}),\left\langle\left[\operatorname{Hess}^{*}(\mu)\right](\star_{1}, \star_{2}),T(\star_{2})\right\rangle_{H}\right\rangle_{H}\] (15)

where \([\operatorname{Hess}^{*}(\mu)](\star_{1},\star_{2}):=\left\langle k(\star_{1},\cdot),\operatorname{Hess}(\mu)k(\star_{2},\cdot)\right\rangle_{L^{2}(\mu)}\) is the smoothed Wasserstein Hessian and the symbols \(\star_{1}\) and \(\star_{2}\) denote the variables to which each of the two inner products is taken.

In the multidimensional case \(\Theta=\mathbb{R}^{d}\), the transport map \(T\) is a vector-valued function \(T:\mathbb{R}^{d}\to\mathbb{R}^{d}\), where a similar derivation can be repeated by replacing \(L^{2}(\mu)\) and \(H\) with the product space of \(d\) independent copies of \(L^{2}(\mu)\) and \(H\). It follows from Proposition 1 and Theorem 1 in [39]--which derives the explicit form of (13) and (15) under their terminology, first and second variations--that the explicit form of the smoothed Wasserstein gradient and Hessian is given by

\[\left[\mathcal{G}^{*}(\mu)\right](\cdot) =\mathbb{E}_{\theta\sim\mu}\Big{[}-\nabla\log\pi(\theta)k(\theta,\cdot)-\nabla k(\theta,\cdot)\Big{]}\in\mathbb{R}^{d},\] \[\left[\operatorname{Hess}^{*}(\mu)\right](\star_{1},\star_{2}) =\mathbb{E}_{\theta\sim\mu}\Big{[}-\nabla^{2}\log\pi(\theta)k( \theta,\star_{1})k(\theta,\star_{2})+\nabla k(\theta,\star_{1})\otimes\nabla k (\theta,\star_{2})\Big{]}\in\mathbb{R}^{d\times d}\]

where \(\nabla^{2}\) denotes an operator to take the Jacobian of the gradient--i.e., \(\nabla^{2}f(\theta)\) is the Hessian matrix of \(f\) at \(\theta\)--and \(\otimes\) denotes the outer product of two vectors. Note that both the smoothed Wasserstein gradient and Hessian are well-defined for any distribution \(\mu\) including empirical distributions.

### Approximate Wasserstein Newton Direction

In the Euclidean space, the Newton direction of an objective function is a direction s.t. the second-order Taylor approximation of the function is minimised. Similarly, [39] characterised the Newton direction \(T^{*}:\mathbb{R}^{d}\to\mathbb{R}^{d}\) of the KL divergence \(\mathrm{KL}(\mu\mid\pi)\) as a solution of the following equation

\[\left\langle\left\langle\left[\mathrm{Hess}^{*}(\mu)\right](\star_{1},\star_{2 }),T^{*}(\star_{2})\right\rangle_{H}+\left[\mathcal{G}^{*}(\mu)\right](\star_ {1}),V(\star_{1})\right\rangle_{H}=0\quad\text{for all}\quad V\in H.\]

Here \(\Theta=\mathbb{R}^{d}\) and \(H\) is the product space of \(d\) independent copies of the RKHS of a kernel \(k\). To obtain a closed-form solution, [39] supposed that the Newton direction \(T^{*}\) can be expressed in a form \(T^{*}(\cdot)=\sum_{i=1}^{n}W^{n}k(\cdot,\theta^{n})\) dependent on a set of each particle \(\theta^{n}\in\Theta\) and associated vector-valued coefficient \(W^{n}\in\mathbb{R}^{d}\). Once the set of the particles is given, the set of the associated vector-valued coefficients is determined by solving the following simultaneous linear equation

\[\begin{bmatrix}\sum_{n=1}^{N}[\mathrm{Hess}^{*}(\mu)](\theta^{1},\theta^{n}) \cdot W^{n}\\ \vdots\\ \sum_{n=1}^{N}[\mathrm{Hess}^{*}(\mu)](\theta^{N},\theta^{n})\cdot W^{n} \end{bmatrix}=\begin{bmatrix}-[\mathcal{G}^{*}(\mu)](\theta^{1})\\ \vdots\\ -[\mathcal{G}^{*}(\mu)](\theta^{N})\end{bmatrix}.\] (16)

These equations (16) can be rewritten in a matrix form [64]. Let \(K:=N\times d\). Define a block matrix \(\mathbf{H}\in\mathbb{R}^{K\times K}\) and a block vector \(\mathbf{G}\in\mathbb{R}^{K}\) by the following partitioning

\[\mathbf{H}=\left(\begin{array}{c|c|c}\mathbf{H}_{11}&\cdots&\mathbf{H}_{1N} \\ \hline\vdots&\ddots&\vdots\\ \hline\mathbf{H}_{N1}&\cdots&\mathbf{H}_{NN}\end{array}\right)\quad\text{ and}\quad\quad\mathbf{G}=\left(\begin{array}{c}\mathbf{G}_{1}\\ \hline\vdots\\ \hline\mathbf{G}_{N}\end{array}\right)\]

with each block specified as \(\mathbf{H}_{ij}:=[\mathrm{Hess}^{*}(\mu)](\theta^{i},\theta^{j})\in\mathbb{R}^ {d\times d}\) and \(\mathbf{G}_{i}:=[\mathcal{G}^{*}(\mu)](\theta^{i})\in\mathbb{R}^{d}\). Define a block matrix \(\mathbf{K}\in\mathbb{R}^{K\times K}\) and a block vector \(\mathbf{W}\in\mathbb{R}^{K}\) by the following partitioning

\[\mathbf{K}:=\left(\begin{array}{c|c|c}\mathbf{K}_{11}&\cdots&\mathbf{K}_{1N} \\ \hline\vdots&\ddots&\vdots\\ \hline\mathbf{K}_{N1}&\cdots&\mathbf{K}_{NN}\end{array}\right)\quad\text{ and}\quad\quad\mathbf{W}:=\left(\begin{array}{c}W^{1}\\ \hline\vdots\\ \hline W^{N}\end{array}\right)\]

with each block of \(\mathbf{K}\) specified as \(\mathbf{K}_{ij}:=\mathbf{I}_{d}\times k(\theta^{i},\theta^{j})\in\mathbb{R}^ {d\times d}\), where \(\mathbf{I}_{d}\) denotes the \(d\times d\) identity matrix. Notice that \(\mathbf{W}\) is a block vector that aligns the vector-valued coefficients \(\{W^{n}\}_{n=1}^{N}\). Using these notations, the optimal coefficients that solve (16) is simply written as \(\mathbf{W}=-\mathbf{H}^{-1}\mathbf{G}\)[64].

Given the optimal coefficients \(\mathbf{W}=-\mathbf{H}^{-1}\mathbf{G}\), the Newton direction \(T^{*}(\theta^{n})\) evaluated at the given particle \(\theta^{n}\) for each \(n=1,\ldots,N\) can be written in the following block vector form

\[\left(\begin{array}{c}T^{*}(\theta^{1})\\ \hline\vdots\\ \hline T^{*}(\theta^{N})\end{array}\right)=-\left(\begin{array}{c|c|c} \mathbf{K}_{11}&\cdots&\mathbf{K}_{1N}\\ \hline\vdots&\ddots&\vdots\\ \hline\mathbf{K}_{N1}&\cdots&\mathbf{K}_{NN}\end{array}\right)\left(\begin{array} []{c|c|c}\mathbf{H}_{11}&\cdots&\mathbf{H}_{1N}\\ \hline\vdots&\ddots&\vdots\\ \hline\mathbf{H}_{N1}&\cdots&\mathbf{H}_{NN}\end{array}\right)^{-1}\left( \begin{array}{c}\mathbf{G}_{1}\\ \hline\vdots\\ \hline\mathbf{G}_{N}\end{array}\right)\] (17)

To distinguish from the standard Newton direction in the Euclidean space, we call (17) the approximate Wasserstein Newton direction. The approximate Wasserstein Newton direction yields a second-order particle update scheme. Suppose we have particles \(\{\theta^{n}_{m}\}_{n=1}^{N}\) to be updated at each step \(m\). At each step \(m\), define the above matrices \(\mathbf{H}\) and \(\mathbf{G}\) with the empirical distribution \(\mu=\hat{\pi}_{m}\) of the particles \(\{\theta^{n}_{m}\}_{n=1}^{N}\). Replacing the Wasserstein gradient in the particle update scheme (3) by the approximate Wasserstein Newton direction (17) provides the second-order update scheme in [39].

### Diagonal Approximate Wasserstein Newton Direction

We derive the diagonal approximation of the approximate Wasserstein Newton direction, which we used for our second-order WGBoost algorithm. A few approximations of the approximate Wasserstein Newton direction were discussed in [39] for better performance of their particle algorithm. We derive the diagonal approximation so that no matrix product and inversion will be involved. Specifically, we replace the matrices \(\mathbf{K}\) and \(\mathbf{H}\) in (17) by the diagonal approximations \(\hat{\mathbf{K}}\) and \(\hat{\mathbf{H}}\), that is,

\[\hat{\mathbf{K}}=\left(\begin{array}{c|c|c}\mathbf{I}_{d}&\cdots&\mathbf{0} \\ \hline\vdots&\ddots&\vdots\\ \hline\mathbf{0}&\cdots&\mathbf{I}_{d}\end{array}\right)\quad\text{and}\quad \hat{\mathbf{H}}=\left(\begin{array}{c|c|c}\mathbf{h}_{11}&\cdots&\mathbf{0} \\ \hline\vdots&\ddots&\vdots\\ \hline\mathbf{0}&\cdots&\mathbf{h}_{NN}\end{array}\right),\]where \(\mathbf{K}_{nn}=\mathbf{I}_{d}\times k(\theta^{n},\theta^{n})=\mathbf{I}_{d}\) for the Gaussian kernel \(k\) used in this work, and the matrix \(\mathbf{h}_{nn}\in\mathbb{R}^{d\times d}\) denotes the diagonal approximation of the diagonal block \(\mathbf{H}_{nn}\) of \(\mathbf{H}\).

Recall that \(\mathbf{H}_{nn}=[\mathrm{Hess}^{*}(\mu)](\theta^{n},\theta^{n})\). Denote by \(\mathrm{Diag}(\mathbf{A})\) the diagonal of a square matrix \(\mathbf{A}\). The diagonal approximation \(\mathbf{h}_{nn}\) is a diagonal matrix whose diagonal is \(\mathrm{Diag}(\mathbf{H}_{nn})\). We plug the diagonal approximations \(\hat{\mathbf{K}}\) and \(\hat{\mathbf{H}}\) in (17). It follows from inverse and multiplication properties of diagonal matrices that the approximate Wasserstein Newton direction turns into a form

\[\left(\begin{array}{c}\dfrac{T^{*}(\theta^{1})}{\vdots}\\ \dfrac{T^{*}(\theta^{N})}{T^{*}(\theta^{N})}\end{array}\right)=-\left(\begin{array} []{c|c|c}\dfrac{\mathbf{h}_{11}}{\cdots}&\mathbf{0}\\ \vdots&\ddots&\vdots\\ \hline\mathbf{0}&\cdots&\mathbf{h}_{NN}\end{array}\right)^{-1}\left(\begin{array} []{c}\dfrac{\mathbf{G}_{1}}{\vdots}\\ \vdots\\ \dfrac{\mathbf{G}_{N}}{\mathbf{G}_{N}}\end{array}\right)=\left(\begin{array} []{c}\dfrac{-\mathbf{G}_{1}\oslage\mathrm{Diag}\left(\mathbf{H}_{11}\right)} {\vdots}\\ \vdots\\ \hline-\mathbf{G}_{N}\oslage\mathrm{Diag}\left(\mathbf{H}_{NN}\right)\end{array} \right).\] (18)

At an arbitrary particle location \(\theta\), denote by \([\mathcal{H}^{*}(\mu)](\theta)\) the diagonal of the smoothed Wasserstein Hessian \([\mathrm{Hess}^{*}(\mu)](\theta,\theta)\). It is straightforward to see that the diagonal can be written as

\[[\mathcal{H}^{*}(\mu)]\left(\cdot\right)=\mathbb{E}_{\theta\sim\mu}\Big{[}- \nabla_{\mathrm{d}}^{2}\log\pi(\theta)k(\theta,\cdot)^{2}+\nabla k(\theta, \cdot)\odot\nabla k(\theta,\cdot)\Big{]}.\]

Notice that \(\mathrm{Diag}\left(\mathbf{H}_{nn}\right)=[\mathcal{H}^{*}(\mu)]\left(\theta^ {n}\right)\) by definition. It therefore follows from the formula (18) with \(\mathbf{G}_{n}=[\mathcal{G}^{*}(\mu)](\theta^{n})\) and \(\mathrm{Diag}\left(\mathbf{H}_{nn}\right)=[\mathcal{H}^{*}(\mu)](\theta^{n})\) that the diagonal approximate Wasserstein Newton direction at an arbitrary particle location \(\theta\) can be independently computed by

\[-[\mathcal{G}^{*}(\mu)](\theta)\oslage\mathcal{H}^{*}(\mu)]\left(\theta\right).\]

We used this direction in Section 3. In the main text, the diagonal approximate Wasserstein Newton direction is defined for each loss functional \(\mathcal{F}_{i}(\cdot)=\mathrm{D}(\cdot\mid\mu_{i})\), with \(\pi=\mu_{i}\), using the smoothed Wasserstein gradient \(\mathcal{G}^{*}_{i}(\mu)\) and the diagonal of the smoothed Wasserstein Hessian \(\mathcal{H}^{*}_{i}(\mu)\) for each i-th output distribution \(\mu_{i}\).

## Appendix D Simulation Study for WEvidential

This section provides simulation studies on kernel choice of WEvidential and comparison of different estimate of the Wasserstein gradient to use in the WGBoost framework.

### Choice of Kernel

We used the kernel smoothing estimate of the Wasserstein gradient of the KL divergence in order to built WEvidential. The smoothed Wasserstein gradient originates in an approximate Wasserstein gradient flow called Stein variational gradient descent (SVGD) [43]. It is fairly common in SVGD in practice to use the Gaussian kernel \(k(\theta,\theta^{*})=\exp(-\|\theta-\theta^{*}\|^{2}/h)\) with the scale \(h>0\). For WEvidential, the scale \(h\) may be viewed as a hyperparameter to choose. We recommend using the value of the scale s.t. \(0.01\leq h\leq 1.0\) in general. This work uses the value \(h=0.1\) throughout the experiments in the main text. One may opt for performing more advanced tuning of the scale \(h\).

We provide a simulation study to examine sensitivity to the scale value. We prepared a synthetic dataset \(\{x_{i},\mu_{i}\}_{i=1}^{D}\) whose inputs are 200 gird points on the interval \([-3.5,3.5]\) and output distributions are normal distributions \(\mu_{i}(\theta)=\mathcal{N}(\theta\mid\sin(x_{i}),0.5)\) conditional on each \(x_{i}\). We used WEvidential with different kernel scales \(h=0.001,0.01,0.1,1.0,100\) and fitted it to the synthetic dataset \(\{x_{i},\mu_{i}\}_{i=1}^{D}\). The decision tree regressor with the maximum depth \(3\) was used for each weak learner. The learning rate and the number of weak learners were set to \(0.1\) and \(100\). The initial constant \(\{\vartheta^{n}\}_{n=1}^{10}\) of WEvidential was set to 10 grid points in the interval \([-10,10]\).

We computed the output of WEvidential for 500 grid points in the interval \([-3.5,3.5]\). We used the maximum mean discrepancy (MMD) [65] to measure the approximation error between the empirical distribution \(\hat{\mu}_{i}\) of the output of WEvidential and the output distribution \(\mu_{i}\):

\[\mathrm{MMD}^{2}\left(\hat{\mu}_{i},\mu_{i}\right)=\mathbb{E}_{\theta\sim\hat{ \mu}_{i},\theta^{\prime}\sim\hat{\mu}_{i}}[k_{\mathrm{D}}(\theta,\theta^{\prime })]-2\mathbb{E}_{\theta\sim\hat{\mu}_{i},\theta^{\prime}\sim\mu_{i}}[k_{ \mathrm{D}}(\theta,\theta^{\prime})]+\mathbb{E}_{\theta\sim\mu_{i},\theta^{ \prime}\sim\mu_{i}}[k_{\mathrm{D}}(\theta,\theta^{\prime})]\]

where \(k_{\mathrm{D}}\) is a Gaussian kernel \(k_{\mathrm{D}}(\theta,\theta^{\prime})=\exp(-(\theta-\theta^{\prime})^{2}/s)\) with the scale \(s=0.025\). The total approximation error was measured by the MMD averaged over all the inputs. Figure 5 shows the total approximation error of WEvidential for each scale value \(h\) in the common log scale, together with examples of the output of WEvidential for different values of \(h\). It demonstrates that the total error is minimised by \(h=0.1\) and stays in a relatively small value range for \(0.01\leq h\leq 1.0\).

### Comparison of Different Wasserstein Gradient Estimates

We compare four WGBoost algorithms built on different estimates of the Wasserstein gradient of the KL divergence. The first three algorithms set the estimate \(\mathcal{G}_{i}(\mu)\) in Algorithm 1 to, respectively,

1. the smoothed Wasserstein gradient in (7);
2. the diagonal approximate Wasserstein Newton direction in (8);
3. the full approximate Wasserstein Newton direction in (17).

The fourth algorithm is LGBoost in Appendix B which is rather a variant of WGBoost. The first and third algorithms are called the first-order WEvidential and the full-Newton WEvidential, respectively. The second algorithm is our default WEvidential presented in section 3. The first-order WEvidential is implemented by removing \(h_{i}^{n}\) and replacing \(g_{i}^{n}\oslash h_{i}^{n}\) with \(g_{i}^{n}\) in Algorithm 2. The full-Newton WEvidential is implemented by replacing \(g_{i}^{n}\oslash h_{i}^{n}\) in Algorithm 2 with \(v_{i}^{n}\) computed by the following Algorithm 4, where \(\nabla^{2}f(\theta)\) denotes the Hessian matrix of a function \(f:\Theta\to\mathbb{R}\) at \(\theta\).

We prepared the same synthetic dataset \(\{x_{i},\mu_{i}\}_{i=1}^{D}\) as Appendix D.1 and fitted each algorithm to the dataset. We computed the output of each algorithm for 500 grid points in the interval \([-3.5,3.5]\) and measured the approximation error by the MMD in the same manner as Appendix D.1. The decision tree regressor with the maximum depth \(3\) was used for each weak learner, and the learning rate was set to \(0.1\). We used an increasing number of weak learners up to \(100\) weak learners, in order to observe the decay of the approximation error and the increase of the computational time. The initial constant \(\{\vartheta^{n}\}_{n=1}^{10}\) for each algorithm was set to 10 grid points in the interval \([-10,10]\), which sufficiently differs from the output distributions so that the decay of the approximation error is clear.

Figure 6 shows the approximation error and the computational time of each algorithm with respect to the increasing number of weak learners, together with the output of each algorithm with 100 weak learners trained. It demonstrates that WEvidential and full-Newton WEvidential reduce the approximation error most efficiently, while full-Newton WEvidential takes the longest computational time among others. As in Algorithm 4, the computation of the full approximate Wasserstein Newton direction requires the inverse and product of the \((N\times d)\times(N\times d)\) block matrices, where \(N\) denotes the particle number \(N\) and \(d\) denotes the particle dimension. The error decay of LGBoost is not only slow but also shows stochasticity due to the Gaussian noise used in the algorithm. We therefore recommend our default WEvidential for better performance and efficient computation.

Figure 5: The total MMD error and example outputs of WEvidential for different kernel scales. Panel (a): the total MMD error for different scale values \(h=0.001,0.01,0.1,1.0,10,100\) both plotted in the common log scale. Panel (b): the output of WEvidential for \(h=0.1\). Panel (c): the output of WEvidential for \(h=0.01\). Panel (d): the output of WEvidential for \(h=100\).

## Appendix E Additional Details of Main Experiments

This section describes additional details of the applications in Section 4. All the experiments were performed with x86-64 CPUs, where some of them were parallelised up to 10 CPUs and the rest uses 1 CPU. The scripts to reproduce all the experiments are provided in the source code. AppendicesE.1 to E.3 describe additional details of the applications in Sections 4.1 to 4.3 respectively. AppendixE.4 describes a choice of initial constants \(\{\vartheta_{0}^{n}\}_{n=1}^{N}\) for the WGBoost algorithm used in Section 4.

### Detail of Section 4.1

The normal response distribution \(\mathcal{N}(y\mid m,\sigma)\) in Example 1 was used in Section 4.1. The normal response distribution has the scale parameter \(\sigma\) that lies only in the positive domain of the Euclidean space \(\mathbb{R}\). We reparametrised it as one in the Euclidean space \(\mathbb{R}\) by the log transform \(\sigma^{\prime}:=\log(\sigma)\), which is the standard practice in Bayesian computation [20]. The inverse of the log transform

Figure 6: The approximation error and computational time of the four different WGBoost algorithms. Panel (a): the approximation error of each algorithm measured by the MMD averaged over the inputs with respect to the number of weak learners. Panel (b): the computational time with respect to the number of weak learners in common logarithm scale. Panel (c)-(f): the outputs of the four algorithms each with 100 weak learners used.

is the exponential transform \(\sigma=\exp(\sigma^{\prime})\). The change of variable formula tells the form of the individual-level posterior on \((m,\sigma^{\prime})\) up to the normalising constant. Under the prior in Example 1, the individual-level posterior on \((m,\sigma^{\prime})\) conditional on each individual response \(y_{i}\) is given by

\[p(m,\sigma^{\prime}\mid y_{i})\propto\exp\left(-\frac{1}{2}\frac{(y_{i}-m)^{2} }{\exp(\sigma^{\prime})^{2}}\right)\times\exp\left(-\frac{1}{2}\frac{m^{2}}{10 ^{2}}\right)\times\frac{1}{\exp(\sigma^{\prime})^{1.01}}\exp\left(-\frac{0.01 }{\exp(\sigma^{\prime})}\right),\]

where the Jacobian determinant \(|d\sigma/d\sigma^{\prime}|=\exp(\sigma^{\prime})\) is used for the change of variable. Figure 7 shows the output of WEvidential for the old faithful geyser dataset and the conditional density estimated through (6).

### Detail of Section 4.2

Section 4.2 used the same reparametrisation of the normal response parameter \((m,\sigma)\) as that of Appendix E.2. The NLL and RMSE of each algorithm on test data \(\{x_{i},y_{i}\}_{i=1}^{D}\) were computed by

\[\text{NLL}=-\frac{1}{D}\sum_{i=1}^{D}\log p(y_{i}\mid x_{i})\quad\text{and} \quad\text{RMSE}=\sqrt{\frac{1}{D}\sum_{i=1}^{D}(y_{i}-\hat{y}_{i})^{2}}\]

where \(p(y_{i}\mid x_{i})\) and \(\hat{y}_{i}\) denote the predictive distribution and the point prediction provided by the algorithm. For WEvidential, we standardised the responses in the training data, in which case standardisation of test data were performed as \(y^{\prime}_{i}=(y_{i}-y^{\text{train}}_{\text{mean}})/y^{\text{train}}_{ \text{std}}\) with the mean \(y^{\text{train}}_{\text{mean}}\) and standard deviation \(y^{\text{train}}_{\text{std}}\) of the training responses. Hence WEvidential provided the predictive distribution \(p(y^{\prime}_{i}\mid x_{i})\) and the point prediction \(\hat{y}^{\prime}_{i}\) for the standardised responses \(y^{\prime}_{i}\). With no loss of generality, the NLL and RMSE for the original responses \(y_{i}\) can be computed as follows:

\[\text{NLL}=-\frac{1}{D}\sum_{i=1}^{D}\log p(y_{i}\mid x_{i})=- \frac{1}{D}\sum_{i=1}^{D}\log p(y^{\prime}_{i}\mid x_{i})+\log y^{\text{train }}_{\text{std}},\] (19) \[\text{RMSE}=\sqrt{\frac{1}{D}\sum_{i=1}^{D}(y_{i}-(y^{\text{train }}_{\text{mean}}+y^{\text{train}}_{\text{std}}\times\hat{y}^{\prime}_{i}))^{ 2}}=y^{\text{train}}_{\text{std}}\sqrt{\frac{1}{D}\sum_{i=1}^{D}(y^{\prime}_ {i}-\hat{y}^{\prime}_{i})^{2}},\] (20)

where the equality of the NLL follows from the change of variable \(p(y_{i}\mid x_{i})=p(y^{\prime}_{i}\mid x_{i})/y^{\text{train}}_{\text{std}}\) and the equality of the RMSE follows from rearranging the terms.

We provide a brief description of each algorithm used in Section 4.2. For all the algorithms, the normal response distribution \(p(y\mid m,\sigma)\) was specified as the response distribution, where the algorithm produces a point or distributional estimate of the response parameter \((m,\sigma)\) at each input \(x\).

* MCDropout [9] trains a single neural network \(F\) while dropping out each network weight with some Bernoulli probability. It can be interpreted as a variational approximation of a Bayesian neural network. It generates multiple subnetworks \(\{F^{n}\}_{n=1}^{N}\) by subsampling the network weights by the dropout. The predictive distribution \(p(y\mid x)\) is given by the model averaging \((1/N)\sum_{i=1}^{N}p(y\mid(m,\sigma)=F^{n}(x))\) for each input \(x\).
* DEnsemble [10] simply trains independent copies \(\{F^{n}\}_{n=1}^{N}\) of a neural network \(F\) in parallel. It is one of the mainstream approaches to uncertainty quantification based on deep learning. The predictive distribution is given by the model averaging as in MCDropout.

Figure 7: Conditional density estimation for the old faithful geyser dataset (grey dots) by WEvidential. Left: distributional estimate (10 particles) of the location parameter for each input. Right: estimated density by the predictive distribution (6) based on the output particles.

* CDropout [55] consider a continuous relaxation of the Bernoulli random variable used in MCDropout to optimise the Bernoulli probability of the dropout. It generates multiple sub-networks \(\{F^{n}\}_{n=1}^{N}\) by subsampling the network weights by the dropout with the optimised probability. The predictive distribution is the same as MCDropout.
* NGBoosting [32] is a family of gradient booting that use the natural gradient [66] of the response distribution as a target variable of each weak learner. In contrast to other methods, NGBoost outputs a single value \(F(x)\) to be plugged into the response-distribution parameter. The predictive distribution \(p(y\mid x)\) is given by \(p(y\mid(m,\sigma)=F(x))\) for each input \(x\).
* DEvidential [13] extends deep evidential learning [11], originally proposed in classification settings, to regression settings. It considers the case where the individual-level posterior of the response distribution falls into a conjugate parametric form, and predicts the hyperparameter of the individual-level posterior by a neural network. The predictive distribution is also given in a conjugate closed-form.

The algorithms used in Section 4.2 are computationally efficient uncertainty quantification methods that are commonly-used in practice. In the following, we provide a limited yet additional comparison of WEvidential with other Bayesian neural networks (BNNs) and a kernel-based model, Gaussian process (GP). We used a subset of the datasets in Section 4, which were used in [67] who employed BNNs with one hidden layer of 50 units. We additionally used a large-scale dataset, _keggd_. For the keggd dataset only, we reported the NLL and RMSE of the normalised outputs without the adjustment (19) and (20), in line with [67]. WEvidential was compared with BNNs learned by variational inference with the reparametrisation trick (VI), deterministic variational inference (DVI), stochastic weight averaging Gaussian (SWAG), principal component analysis subspace inference (PCA+VI), and two-layer deep Gaussian process with 50 induced points trained via expectation propagation (DGP1-50). Table 1 summarises the result.

### Detail of Section 4.3

We reparametrised the parameter of the categorical response distribution \(\mathcal{C}(y\mid q)\) used in Section 4.3, similarly to the normal response distribution used in Sections 4.1 and 4.2. The categorical response distribution has a class probability parameter \(q=[q_{1},\ldots,q_{k}]\) in the simplex \(\Delta_{k}\). We applied the log-ratio transform \(q^{\prime}:=[\log(q_{1}/q_{k}),\ldots,\log(q_{k-1}/q_{k})]\in\mathbb{R}^{k-1}\) that maps from the simplex \(\Delta_{k}\) to the Euclidean space \(\mathbb{R}^{k-1}\)[42]. The inverse of the log-ratio transform is the logistic transform

\[q=\left[\frac{\exp(q^{\prime}_{1})}{z_{k}},\ldots,\frac{\exp(q^{\prime}_{k-1}) }{z_{k}},\frac{1}{z_{k}}\right]\in\Delta_{k}\quad\text{where}\quad z_{k}=1+ \sum_{j=1}^{k-1}\exp(q^{\prime}_{j}).\]

The logistic normal distribution on the original parameter \(q\) corresponds to a normal distribution on the transformed parameter \(q^{\prime}\)[42]. The change of variable formula tells the individual-level posterior on \(q^{\prime}\) up to the normalising constant. Under the prior in Example 2, the individual-level posterior

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Dataset & Criteria & WEvidential & VI & DVI & SWAG & PCA+VI & DGP1-50 \\ \hline \multirow{3}{*}{\begin{tabular}{} \end{tabular} } & \(2.47\pm 0.16\) & \(2.43\pm 0.03\) & \(2.41\pm 0.02\) & \(2.76\pm 0.13\) & \(2.72\pm 0.13\) & \(2.33\pm 0.06\) \\  & \(2.83\pm 0.11\) & \(3.04\pm 0.02\) & \(3.06\pm 0.01\) & \(3.01\pm 0.09\) & \(2.99\pm 0.10\) & \(3.13\pm 0.03\) \\ \cline{2-7}  & \(0.53\pm 0.08\) & \(2.38\pm 0.02\) & \(1.01\pm 0.06\) & \(1.70\pm 1.50\) & \(1.72\pm 1.59\) & \(1.32\pm 0.03\) \\ \cline{2-7}  & -5.47 \(\pm\) 0.03 & -5.87 \(\pm\) 0.29 & -6.29 \(\pm\) 0.04 & -6.71 \(\pm\) 0.11 & -6.71 \(\pm\) 0.11 & -3.60 \(\pm\) 0.33 \\ \cline{2-7}  & \(0.16\pm 0.24\) & \(1.68\pm 0.04\) & \(0.47\pm 0.03\) & \(0.40\pm 0.42\) & \(0.40\pm 0.42\) & \(1.39\pm 0.14\) \\ \cline{2-7}  & -0.91 \(\pm\) 0.04 & - & - & -1.08 \(\pm\) 0.04 & -1.09 \(\pm\) 0.03 & - \\ \hline \multirow{3}{*}{
\begin{tabular}{} \end{tabular} } & \(2.78\pm 0.60\) & - & - & \(3.52\pm 0.98\) & \(3.46\pm 0.95\) & - \\  & \(4.15\pm 0.52\) & - & - & \(5.23\pm 0.42\) & \(5.14\pm 0.42\) & - \\ \cline{2-7}  & \(0.42\pm 0.07\) & - & - & \(1.59\pm 0.27\) & \(1.59\pm 0.27\) & - \\ \cline{2-7}  & \(0.00\pm 0.00\) & - & - & \(0.00\pm 0.00\) & \(0.00\pm 0.00\) & - \\ \cline{2-7}  & \(0.48\pm 0.18\) & - & - & \(0.97\pm 0.38\) & \(0.97\pm 0.38\) & - \\ \cline{2-7}  & \(0.24\pm 0.01\) & - & - & \(0.13\pm 0.03\) & \(0.13\pm 0.03\) & - \\ \hline \hline \end{tabular}
\end{table}
Table 4: The NLLs and RMSEs for each dataset, where the best score is underlined. The results other than that of WEvidential were reported in [67].

\(p(q^{\prime}\mid y_{i})\) conditional on each individual observed response \(y_{i}\) is given by

\[p(q^{\prime}\mid y_{i})\propto\left(\frac{1}{z_{k}}\right)^{[y_{i}=k]}\times\prod _{j=1}^{k-1}\left(\frac{\exp(q^{\prime}_{j})}{z_{k}}\right)^{[y_{i}=j]}\times \exp\left(-\frac{1}{2}\frac{\|q^{\prime}\|^{2}}{10^{2}}\right),\]

where \([y_{i}=j]\) is \(1\) if \(y_{i}\) is the \(j\)-th class label and \(0\) otherwise.

We provide a brief description of each algorithm used in Section 4.3. MCDropout and DEnsmemble are described in Appendix E.2.

* DDistillation [57] predicts the parameter of a Dirichlet distribution over the simplex \(\Delta_{k}\) by a neural network using the output of DEnsmemble. The output of multiple networks in DEnsmemble is distilled into the Dirichlet distribution controlled by one single network.
* PNetwork [14] considers the case where the individual-level posterior of the categorical response distribution falls into a Dirichlet distribution similarly to deep evidential learning [11]. It predicts the hyperparameter of the individual-level posterior given in the form of the Dirichlet distribution.

For the OOD detection, the OOD score we used was the inverse of the maximum norm of the variance of the WEvidential output. There are other quantities that are possible to use as the OOD score. For example, the entropy of the predictive distribution \(p(y\mid x)\) is a quantity computable for any probabilistic classification method. We compared the OOD detection performance of WEvidential with that of NGBoost and Random Forest (RForest) based on the entropy of their predictive distributions. For reference, we also computed the OOD detection performance of WEvidential based on the entropy of the predictive distribution. For NGBoost, the decision tree regressor was used for each weak learner and 4000 weak learners were trained with the learning rate 0.01. For RForest, the maximum depth 3 was used. Table 5 shows the result on the segment dataset, demonstrating that the performance of WEvidential is higher, to a large margin, than that of NGBoost and RForest based on the entropy.

To investigate on the effective learning rate of WEvidential for classification, we performed a simple simulation study of WEvidential using the synthetic _madelon_ dataset. We created 600 data points of three-dimensional inputs and three-class labels. The data subset that belongs to the last class was kept as the OOD samples. We randomly held out 20% of non-OOD samples as a test set to measure the test classification accuracy. We trained WEvidential using different learning rates from 0.1 to 1.0 while fixing the number of weak learners to 4000. Table 6 shows that the result for the learning rate 0.4 demonstrates the best classification accuracy and the third best OOD detection performance. We also trained WEvidential using different numbers of weak learners from 1000 to 4000 while fixing the learning rate to 0.4. Table 7 shows that the result for the number of weak learners 4000 demonstrates the best classification accuracy and the second best OOD detection performance. While the most effective learning rate differs depending on each dataset, we drew on the insight obtained from the simulation study and set the learning rate to 0.4 for classification.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline Criteria & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & 1.0 \\ \hline Accuracy & 92.50 & 92.50 & 92.50 & 93.75 & 91.25 & 86.25 & 90.00 & 75.00 & 85.00 & 80.00 \\ OOD & 29.85 & 34.79 & 35.26 & 41.77 & 40.52 & 42.69 & 35.21 & 35.58 & 42.04 & 36.05 \\ \hline \hline \end{tabular}
\end{table}
Table 6: The classification accuracy and OOD detection performance of WEvidential on the synthetic dataset for different learning rates, where the number of weak learners is fixed to 4000.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Dataset & Criteria & WEvidential & WEvidential (Entropy) & NGBoost & RForest \\ \hline \multirow{2}{*}{segment} & Accuracy & 96.57 ± 0.6 & 96.57 ± 0.6 & 94.09 ± 0.6 & 88.18 ± 1.6 \\  & OOD & 99.67 ± 0.2 & 98.96 ± 0.4 & 89.96 ± 1.3 & 66.51 ± 1.1 \\ \hline \hline \end{tabular}
\end{table}
Table 5: The OOD detection performance of WEvidential, NGBoost, and RForest on the segment dataset, where WEvidential (Entropy) indicates the result of WEvidential based on the entropy.

### Choice of Initial State of WGBoost

In standard gradient boosting, the initial state at step \(m=0\) is specified by a constant that most fits given data. Similarly, we specified the initial state \(\{\vartheta_{0}^{n}\}_{n=1}^{N}\) of WEvidential by a set of constants that most fits the output distributions in average. We find such a set of constants by performing an approximate Wasserstein gradient flow averaged over all the output distributions. Specifically, given the term \(\mathcal{G}_{i}(\mu)\) in Algorithm 1, we define \(\bar{\mathcal{G}}(\mu):=(1/D)\sum_{i=1}^{D}\mathcal{G}_{i}(\mu)\) and perform the update scheme of a set of \(N\) particles \(\{\bar{\vartheta}_{m}^{n}\}_{n=1}^{N}\):

\[\begin{bmatrix}\bar{\vartheta}_{m+1}^{1}\\ \vdots\\ \bar{\vartheta}_{m+1}^{N}\end{bmatrix}=\begin{bmatrix}\bar{\vartheta}_{m}^{1} \\ \vdots\\ \bar{\vartheta}_{m}^{N}\end{bmatrix}+\nu_{0}\begin{bmatrix}-[\bar{\mathcal{G} }(\hat{\pi}_{m})](\bar{\vartheta}_{m}^{1})\\ \vdots\\ -[\bar{\mathcal{G}}(\hat{\pi}_{m})](\bar{\vartheta}_{m}^{N})\end{bmatrix}\]

with the learning rate \(\nu_{0}=0.01\) up to the maximum step number \(m=5000\). The initial particle locations for this update scheme were sampled from a standard normal distribution. We specified the initial state \(\{\vartheta_{0}^{n}\}_{n=1}^{N}\) by the set of particles \(\{\bar{\vartheta}_{m}^{n}\}_{n=1}^{N}\) obtained though this scheme at \(m=5000\).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Criteria & 1000 & 2000 & 3000 & 4000 \\ \hline Accuracy & 91.25 & 92.50 & 91.25 & 93.75 \\ OOD & 43.96 & 37.70 & 39.84 & 41.77 \\ \hline \hline \end{tabular}
\end{table}
Table 7: The classification accuracy and OOD detection performance of WEvidential on the synthetic dataset for different numbers of weak learners, where the learning rate is fixed to 0.4.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claim made in the abstract and introduction were elaborated in the subsequent section and empirically demonstrated through the experiments. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: A concise discussion on the important remaining questions and the potential limitation were provided in the last section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]. Justification: This work primarily focused on the methodological development of the proposed algorithm, empirically demonstrating the performance on the various benchmarks. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The computational procedure of the proposed algorithm was clarified in the main text and appendix, with the setting to reproduce the experiments detailed. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The source code is available online with the instruction, and also submitted in the supplementary file. For the blind review process, the source code URL was masked in the manuscript. The source code URL will be unmasked after the review process. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the experimental setting was clarified in the main text and appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All the experiments to measure the predictive performance of the proposed algorithm were repeated several times to estimate the standard deviation of the score. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The computer resource was clarified in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The conducted research conforms the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This work proposed a generic algorithm that is not tied to specific applications. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This work proposed a generic algorithm. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The credit and license were appropriately included in the text and source code. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes]. Justification: The documentation and license were included in the source code. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.