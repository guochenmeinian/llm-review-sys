# BioTrove: A Large Curated Image Dataset Enabling AI for Biodiversity

Chih-Hsuan Yang\({}^{1,*}\)

Ben Feuer\({}^{2,*}\)

Zaki Jubery\({}^{1}\)

Zi K. Deng\({}^{3}\)

Andre Nakkab\({}^{2}\)

Md Zahid Hasan\({}^{1}\)

Shivani Chiranjeevi\({}^{1}\)

Kelly Marshall\({}^{2}\)

Nirmal Baishnab\({}^{1}\)

Asheesh K Singh\({}^{1}\)

Arti Singh\({}^{1}\)

Soumik Sarkar\({}^{1}\)

Nirav Merchant\({}^{3}\)

Chinmay Hegde\({}^{2}\)

Baskar Ganapathysubramanian\({}^{1}\)

###### Abstract

We introduce BioTrove, the largest publicly accessible dataset designed to advance AI applications in biodiversity. Curated from the iNaturalist platform and vetted to include only research-grade data, BioTrove contains 161.9 million images, offering unprecedented scale and diversity from three primary kingdoms: _Antimalia_ ("animals"), _Fungi_ ("fungi"), and _Plantae_ ("plants"), spanning approximately 366.6K species. Each image is annotated with scientific names, taxonomic hierarchies, and common names, providing rich metadata to support accurate AI model development across diverse species and ecosystems.

We demonstrate the value of BioTrove by releasing a suite of CLIP models trained using a subset of 40 million captioned images, known as BioTrove-Train. This subset focuses on seven categories within the dataset that are underrepresented in standard image recognition models, selected for their critical role in biodiversity and agriculture: _Aves_ ("birds"), _Arachinda_ ("spiders/ticks/mites"), _Insecta_ ("insects"), _Plantae_ ("plants"), _Fungi_ ("fungi"), _Mollusca_ ("snails"), and _Reptilia_ ("snakes/lizards"). To support rigorous assessment, we introduce several new benchmarks and report model accuracy for zero-shot learning across life stages, rare species, confounding species, and multiple taxonomic levels.

We anticipate that BioTrove will spur the development of AI models capable of supporting digital tools for pest control, crop monitoring, biodiversity assessment, and environmental conservation. These advancements are crucial for ensuring food security, preserving ecosystems, and mitigating the impacts of climate change. BioTrove is publicly available, easily accessible, and ready for immediate use.

## 1 Introduction

AI advances are poised to play a crucial role in biodiversity conservation, ecology management, and agriculture. Already, AI tools have been shown to enable automated species identification, monitoring of ecological changes, and optimization of crop management [36, 5]. However, standard AI approaches for biodiversity applications persistently face major challenges. Training datasets are labor-intensive and costly to create; they cover only a narrow set ofvisual concepts; standard vision models excel at single tasks but require extensive retraining for new tasks; models often struggle with generalizing to unseen labels and new environments, limiting their effectiveness in real-world applications [34, 14]. Models that perform well on benchmarks often fail in the wild [12, 1]. Standard computer vision datasets (ImageNet and its successors) have significant limitations, including incorrectly labeled images, geographical and cultural biases, and overlapping or ill-defined labels, all of which impair the development of high-performant AI models [24]. Consequently, there is a critical need for large, diverse, accurately annotated datasets that are specific to biodiversity, ecology, and agricultural research [27, 23].

In response to this need, several datasets have been introduced. Perhaps the most well-known (raw) pool of biodiversity images on the Web is iNaturalist [42], from which several curated datasets have been sourced, among them being iNat2021 [41] with 2.7M images of over 10,000 species of plants, animals, and fungi. However, insects (which comprise a very large fraction of extant species) are under-represented in this dataset. IP102 [44], Insecta [10], and the more recent BioScan-1M [13], are alternative datasets that focus on the Insecta Class. Perhaps the latest advance in such research is TreeOfLife-10M [39], which is currently the state-of-the-art dataset of text-annotated biological images, comprising 10M images with approximately 450K unique taxonomic classes.

In this paper, we contribute to advancing biodiversity AI research by curating and releasing **BioTrove**, a dataset comprising **161.9 million captioned images** across approximately **366.6K species**. This dataset surpasses all previous collections in both scale and diversity, representing the largest publicly available, "AI-ready" dataset of curated biodiversity images. Each image in BioTrove is paired with language data and spans a diverse range of

Figure 1: **Top Seven Phyla in the BioTrove Dataset. This figure displays the seven most frequently occurring phyla within BioTrove, which is curated to include data exclusively from the three primary kingdoms: Animalia, Plantae, and Fungi. For each phylum, the five most common species are shown, including their scientific names, common names, and the number of images per species. The phyla are ordered by species diversity, with the most diverse phylum on the right and the least diverse on the left.**

taxonomic groups, including _Reptilia_ (reptiles), _Plantae_ (plants), _Mollusca_ (mollusks), _Mammalia_ (mammals), _Insecta_ (insects), _Fungi_ (fungi), _Aves_ (birds), _Arachnida_ (archinds), _Animalia_ (animals), _Amphibia_ (amphibians), and _Actinoptergii_ (ray-finned fish). The dataset spans global regions, supporting robust training across diverse environmental contexts. Representative examples are shown in Figure 1, and additional details are provided on the project website.

Each image in BioProve originates from the iNaturalist community science platform [42] and is annotated with detailed metadata, including the common name, scientific name, and complete taxonomic hierarchy. This curated metadata provides research-grade high-quality text annotations that enhance AI model training. Additionally, we open-source a data management pipeline, BioProve-Process, to facilitate interaction with BioProve metadata. With BioProve-Process, researchers can efficiently filter and balance data by selecting specific taxonomic categories, adjusting for taxonomy level, and managing species distribution to reduce skewness. This enables users to create custom subsets that align with their research goals while maintaining consistency in species representation.

To showcase the capabilities of BioProve, we introduce two technical contributions. First, we train and release BioProve-CLIP, a suite of vision-language foundation models, using a subset, BioProve-Train, consisting of approximately 40M images from BioProve and representing around 33K species. This subset, constructed with BioProve-Process, includes diverse taxa, specifically focusing on birds (_Aves_), spiders/ticks/mites (_Arachnida_), insects (_Insecta_), plants (_Plantae_), fungi (_Fungi_), snails (_Mollusca_), and snakes/lizards (_Reptilia_). These taxonomic classes were selected to capture a broad range of species--outside of charismatic megafauna--that critically impact biodiversity. The models exhibit robust generalization capabilities, demonstrating high zero-shot and few-shot performance on unseen taxa when using either common or scientific names. We anticipate that BioProve-CLIP will serve as a valuable foundation for biodiversity-related applications and can be further fine-tuned for specific research needs.

Second, we rigorously quantify the performance of our foundation models on five existing fine-grained image classification benchmarks, as well as on three newly curated test datasets. We find that BioProve-CLIP models comfortably achieve the state-of-the-art in certain settings, while both the original (OpenAI) CLIP model as well as BioCLIP [39] excel in certain other settings. We analyze these findings in further detail below, but overall we hope that our dataset can be used by the AI community as a testbed for further algorithmic and scaling research in fine-grained image recognition.

The remainder of this paper is organized as follows. Section 2 introduces the BioProve dataset, the dataset's salient characteristics, and a comparison with previous work. Section 3 details our curation methodology. Section 4 introduces our newly proposed test datasets and their characteristics. Section 5 details our new BioProve-CLIP models and their benchmark performance relative to previous work. Section 6 concludes with a discussion of limitations and potential future directions.

## 2 The BioProve Dataset

Characteristics.BioProve comprises over 161.9 million images spanning 372,966 species. This dataset is an order of magnitude larger than existing biodiversity datasets, such as the state-of-the-art TreeOfLife-10M dataset, which it surpasses in _scale_ by a factor of nearly 13.5\(\times\) while maintaining comparable _species diversity_. Figure 1 shows representative image samples, while Figure 2 displays the distribution of samples across the seven major categories with the most frequently observed species. Additionally, Figure 3 illustrates the range of phyla, taxonomic classes, orders, and families represented in the dataset.

BioProve includes only research-grade data and publicly accessible licensed content for research purposes from iNaturalist, which designates observations as research-grade once they meet strict validation criteria. To qualify, two or more experienced iNaturalist community members--naturalists, biologists, or citizen scientists--must agree on the species identification. Additionally, the observation must meet other requirements, such as a clear photograph and precise geolocation data. Recent experiments have shown that iNaturalist's Research Grade observations achieve approximately 97% accuracy, underscoring the reliability of this community-based validation process [17]. Furthermore, iNaturalist continuously enhances data quality by refining validator criteria and implementing new data quality assessment measures, ensuring BioTrove remains a robust dataset for scientific use.

Each image sample in BioTrove is enriched with detailed, curated metadata that facilitates efficient filtering by species count and taxonomic information. The metadata includes common names, scientific names, and hierarchical taxonomic data, which enhances the usability of the dataset for AI model training. For the complete list of metadata fields, see Table 1.

Along with the dataset, we also release our data curation tooling pipeline: BioTroveProcess, which enables users to easily access and manipulate the dataset. This pipeline allows researchers to select specific categories across different taxonomic levels, visualize data distributions, and effectively manage class imbalance according to their needs. It facilitates the downloading of specific images by their URLs and provides image-text pairs as well as user-defined chunks to support various AI applications. BioTroveProcess thus enables users to define custom subsets of BioTrove with ease, making the dataset fully AI-ready and reducing barriers to follow-up research in biodiversity-focused AI.

Dual-language text descriptions.We adopt both common and scientific names since Latin is a low-resource language, and current AI models do not perform well on scientific names alone in a zero-shot manner. We found that a well-structured text description that integrates common names, scientific names, and detailed taxonomic hierarchies facilitates the learning of relationships between Latin and English terms, thereby improving the models' applicability in scientific contexts [6, 38, 43]. Moreover, incorporating the taxonomic hierarchy enables models to more effectively associate visual data with taxonomic terminology [25, 2]. This matches the guidelines suggested by BioCLIP [39] to enhance model performance and generalization. **Privacy Measures:** The images of BioTrove were sourced from the

Figure 2: **Distribution of the BioTrove dataset.** (a) Size of the top seven Phyla in the BioTrove dataset. (b) Species counts for the top seven Phyla. (c) The 40 highest occurring species in entire BioTrove dataset.

iNaturalist Open Dataset, whose metadata included Personally Identifiable Information (PII). This included information about observers, such as their usernames and sometimes their real names if they have chosen to share that information publicly. We removed all such fields to ensure that no PII is present in the metadata associated with BioProve samples, ensuring the privacy of all contributors. **License:** During curation, we took care to include only images from iNaturalist Open Data, which are all licensed under either the CC0, or CC-BY, or CC-BY-NC licenses. This ensures that all our images are available for public research purposes. **Offensive Content:** Some of our URLs may point to images that users could find disturbing or harmful, such as photos of dead or dismembered animals. We retained these types of

\begin{table}
\begin{tabular}{l l} \hline \hline Text Type & Description \\ \hline Common Name & Vernacular name (e.g., Western honey bee) \\ Scientific Name & Genus and species (e.g., _Apis mellifera_) \\ Taxonomic Name & Flattened taxonomy concatenated into a single string \\ Taxonomic Rank & Specific level in the hierarchy (e.g., subspecies, species) \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Annotations provided in the BioProve Dataset.**

Figure 3: **Treemap diagram of the BioProve dataset, starting from Kingdom. The nested boxes represent phyla, (taxonomic) classes, orders, and families. Box size represents the relative number of samples.**

images since they sometimes can provide valuable scientific data about wildlife, including information on predation events, roadkill, and other occurrences relevant to conservation and biodiversity studies. Although iNaturalist relies on user contributions and community moderation to maintain the quality and appropriateness of the data, we acknowledge that the vast and diverse nature of the data means that some offensive or inappropriate content might be present.

Our closest comparisons are with BioScan-1M (which appeared in NeurIPS 2023 Datasets and Benchmarks) and TreeOfLife-10M (which will appear in CVPR 2024). BioScan-1M focuses solely on the Insecta Class and provides scientific names, taxonomic ranks, as well as DNA barcodes. The TreeOfLife-10M dataset comprises 10.4 million images, integrating data from iNat2021 [41], BioScan-1M, and a fresh set of image samples sourced from the Encyclopedia of Life (EOL). It also supports dual-language labels and detailed taxonomic hierarchies and was used to train the BioCLIP vision-language model. See Table 2 for essential differences.

## 3 Data Collection and Curation Methodology

Challenges with iNaturalist Open Data.All of BioProve is sourced from the iNaturalist Open Data community science platform, which (in all) comprises over 280M biodiversity-relevant observations shared by users. However, there are still significant gaps in usability for AI research. The photos and metadata, although easily downloadable, are provided in four separate metadata sheets that are not ready to use. Taxa information is encoded as numerical IDs, requiring additional API calls and non-trivial lookups to convert these into common or scientific names. The multiple metadata sheets structure is fragmented across four separate files--photos, taxa, observations, and observers--adding complexity to data integration. Managing data balance and filtering out species with too few images can lead to biases toward common (charismatic) species and an imbalanced training process.

Curation of BioProve.The iNaturalist Open Dataset comprises a collection of 284.2 million images stored on an AWS S3 bucket as of 2024-09-27, with associated metadata provided across four separate CSV files (photos, observations, taxa, and observers). Details on each of these files are presented in Section A.5 in the Appendix. Although these files contain a wealth of valuable information, they are structured for rapid retrieval rather than AI-readiness. To address this, we curate the metadata into a streamlined, AI-optimized format.

We populate an SQL database with each CSV file as an individual SQL table, then create an aggregated table by joining photos, observations, and taxa on their relational columns, discarding irrelevant columns. In this aggregate table, we add a new column populated with

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline
**Feature** & **BioProve** & **TreeOfLife** & **BioScan** \\ \hline
**Size** & 161.9 million images & 10.4 million images & 1.1 million images \\ \hline
**Diversity** & 366.6K species & 454.1K species & 8.3K \\ \hline
**Labels** & Dual language & Dual language & Single language \\
**Provided** & (common and scientific names), detailed & (common and scientific names), detailed & (scientific names), taxonomic ranks \\  & taxonomic hierarchies & taxonomic hierarchies & (family to species), \\  & & & DNA barcodes \\ \hline
**Data Source** & iNaturalist Open & iNaturalist, Encyclopedia of Life & Specimens from \\  & Dataset & (EOL), BioScan-1M & Malaise traps, DNA barcodes matched to \\  & & & \\ \hline
**Key Features** & Ready-to-use pipeline, reduce class imbalance, high-quality annotations, supports BioProve-CLIP & Rich hierarchical representations, comprehensive metadata, supports BioCLIP & Focus on insects, high-resolution images, detailed taxonomic annotation, DNA codes \\ \hline \end{tabular}
\end{table}
Table 2: **Comparison** of BioProve with existing biodiversity datasets.

the Amazon S3 URL and generate individual columns for taxonomic kingdom, phylum, class, order, family, genus, and species.

BioTrove includes only research-grade images from the _Animalia_, _Plantae_, and _Fungi_ kingdoms, filtering out other domains to maintain a clear biodiversity focus. To achieve this filtering, we apply strict taxonomic criteria, ensuring only these three kingdoms are represented. The iNaturalist metadata files lack common names, so we reconstruct this information by cross-referencing species names from the iNaturalist Taxonomy DarwinCore Archive, updated monthly. This enriched metadata, including common names, is then appended to the SQL table. The final curated dataset is exported as parquet files, available for public access on HuggingFace.

Data Filtering and Preprocessing.As outlined, BioTrove includes structured metadata that is both comprehensive and easy to work with, featuring full taxonomic information and direct URLs to image files. To further support accessibility, we release an accompanying software pipeline that allows users to filter specific categories, visualize data distributions, and manage dataset imbalances effectively. These tools make it simple for researchers to interact with BioTrove, creating tailored subsets based on their specific needs. The iNaturalist data, sourced from citizen science contributions, has inherent variability in species representation, with some species documented extensively and others less so. To address this, our tools enable user-defined filters to exclude species with fewer than a set number of images and to cap image counts per species, thus supporting more balanced model training.

To further mitigate dataset imbalances (detailed in our experiments section), we use a semi-global shuffling strategy in which the data is organized into chunked tar files. These files are shuffled, divided into smaller groups, and then merged into larger batches to ensure a balanced species distribution within each batch. This method enhances dataset integrity, helping to prevent the overrepresentation of any single species across the batches.

## 4 Models and Benchmarks

We now showcase and demonstrate the utility of the BioTrove dataset by creating and benchmarking BioTroveCLIP, a new suite of vision-language foundation models for biodiversity.

### BioTrove-Train

BioTrove-Train is a curated subset comprising approximately 40M samples and 33K species, focused specifically on seven taxonomic categories: _Aves_, _Anchnida_, _Insecta_, _Plantae_, _Fungi_, _Mollusca_, and _Reptilia_. As discussed previously, the BioTrove dataset is accompanied by a flexible pipeline that enables users to apply customized filtering to select specific categories or subsets based on research needs, thereby allowing researchers to generate their own training datasets. For BioTrove-Train, these seven categories were pre-selected due to their significant impact on biodiversity and agricultural ecosystems, as well as their relative underrepresentation in standard image recognition models. Unlike megafauna, which are typically well-represented in existing models, these categories address unique challenges in biodiversity-focused AI.

This subset comprises data posted on iNaturalist prior to 2024-01-27. We applied strict filtering criteria to ensure high-quality data, excluding species with fewer than 30 images and capping the maximum number of images per species at 50,000. To maintain balance, we employed a semi-global shuffling method, organizing the data into mini-batches of approximately 50,000 samples. From these, 95% were randomly selected for training and validation, while the remaining 5% were reserved for testing. Detailed statistics can be found in Table 3.

### New Benchmarks

We created three new benchmark datasets, all of which are non-overlapping curated subsets of the BioTrove dataset. These benchmarks focus on fine-grained image classification within the seven taxonomic categories: _Aves_, _Arachnida_, _Insecta_, _Plantae_, _Fungi_, _Mollusca_, and _Reptilia_. All benchmarks presented here are independent and strictly within these seven categories, without overlapping with each other or with the BioTrove-Train subset.

Additionally, we report results on several established benchmarks from the literature (see Table 4).

**BioTrove-Balanced.** To ensure balanced species representation across the seven key taxonomic categories, we curate the BioTrove-Balanced benchmark. Each category includes up to 500 species, with 50 images per species, resulting in a total of 112,209 images. This balanced dataset provides a consistent foundation for model performance evaluations. The exact species counts for each category are detailed in Table 7 (see Appendix).

**BioTrove-Unseen.** To assess the ability of models to generalize to previously unseen species within the seven categories, we curated the BioTrove-Unseen benchmark. This dataset includes species from BioTrove-Train with fewer than 30 instances, ensuring they were unseen during training. Each species is represented by at least 10 images, with a total of 11,983 images. This benchmark tests the models' robustness on rare species not encountered during training.

**BioTrove-LifeStages.** The BioTrove-LifeStages benchmark evaluates the model's ability to recognize species across different developmental stages, focusing on insect species that exhibit significant visual variations throughout their life cycle. This dataset contains 20 labels representing four life stages (egg, larva, pupa, and adult) for five distinct insect species. The data was collected via the observation export feature on the iNaturalist platform between February 1, 2024, and May 20, 2024, ensuring no overlap with the training dataset. This benchmark allows for comprehensive evaluations of model performance across various life stages (see Figure 4).

Figure 4: (a) Example images from BioTrove-Unseen. (b) BioTrove-Life-Stages with 20 class labels: four life stages (egg, larva, pupa, and adult) for five distinct insect species.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Dataset** & **Description** & **Images** & **Unique Classes** \\ \hline TreeOfLife-10M & \begin{tabular}{c} Dataset combines a subset \\ of iNaturalist, \\ Encyclopedia of Life (EOL), BioScan-1M. \\ One subset of BioTrove \\ Train \\ \end{tabular} & 10.4M & 454,103 \\ BioTrove-Train & \begin{tabular}{c} BioTrove- \\ One subset of BioTrove \\ with size 40M. \\ \end{tabular} & 39.9M & 33,364 \\ \hline \hline \end{tabular} 
\begin{tabular}{c c c} \hline \hline
**Level** & **Uniques** \\ \hline kingdom & 3 \\ phylum & 14 \\ class & 50 \\ order & 311 \\ family & 1692 \\ genus & 11506 \\ species & 33364 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Training data sources used in BioTrove-Train and Diversity in Different Taxonomy Levels. We integrate taxonomic labels into the images.**

[MISSING_PAGE_FAIL:9]

BioProve-Train. For BioCLIP-Rare, the dataset is a subset from EOL which BioCLIP did not see before, but TreeOfLife contains the majority of the EOL dataset.

**Limitations**. We also evaluated all models on the challenging Confounding-species benchmark introduced in [4], but find that all models perform at or below random chance and do not report results here; this could be an interesting avenue for follow-up work.

In Table 8 in the Appendix, we report model performance at different levels of the taxonomic hierarchy. Generally, we find that models trained on web-scraped data perform better with common names, whereas models trained on specialist datasets perform better when using scientific names. Additionally, models trained on web-scraped data excel at classifying at the highest taxonomic level (kingdom), while models begin to benefit from specialist datasets like BioProve-Train and Tree-of-Life-10M at the lower taxonomic levels (order and species). From a practical standpoint, this is not problematic: BioProve-CLIP is highly accurate at the species level, and higher-level taxa can be deterministically derived from lower ones.

Addressing these limitations will further enhance the applicability of models like BioProve-CLIP in real-world biodiversity monitoring tasks.

## 6 Concluding Discussion

We introduce BioProve, the largest publicly accessible dataset designed to advance AI for biodiversity applications. This dataset, curated from the iNaturalist community science platform, includes 161.9 million images, surpassing existing datasets in scale by an order of magnitude. We anticipate that BioProve will enable the development of AI models that can enable various digital tools ranging from pest control strategies, crop monitoring, and worldwide biodiversity assessment and environmental conservation.

We also believe that BioProve can be used as a unique testbed for measuring progress on fine-grained image recognition. The success of BioProve-CLIP on BioProve-Unseen underscores the importance of scaling up per-category sample size, or vertical scaling [9], in achieving high accuracy on long-tailed extreme-imbalance classification. However, BioCLIP continues to exhibit superior performance on several datasets, and we believe that this is because TreeofLife-10M contains an order-of-magnitude more _classes_ (species) than BioProve-Train. We invite the AI community to create new subsets of BioProve with varying degrees of balance and species diversity and use our tooling to measure model performance against current benchmarks.

## Acknowledgements

We acknowledge support from the AI Research Institutes program supported by NSF and USDA-NIFA under AI Institute for Resilient Agriculture, Award No. 2021-67021-35329, and the NAIRR program for computing support.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**Model** & **BTU** & **BTB** & **BCR** & **F** & **I2** & **B** & **LS** & **DW** & **Weighted Avg.** \\ \hline
**OpenAI-B** & 12.9 \(\pm\) 0.6 & 7.3 \(\pm\) 0.15 & 10.9 \(\pm\) 0.56 & 11.5 \(\pm\) 1.98 & 10.2 \(\pm\) 0.93 & 50.0 \(\pm\) 0.33 & **56.5 \(\pm\) 3.97 & 10.3 \(\pm\) 0.45 & 14.7 \\
**BioCLIP-B** & **68.2** \(\pm\) 0.83 & 62.2 \(\pm\) 0.28 & **30.2** \(\pm\) 0.82 & 45.1 \(\pm\) 308 & **20.8 \(\pm\) 1.25** & **68.7 \(\pm\) 0.30 & 18.0 \(\pm\) 3.07 & **19.9 \(\pm\) 0.59** & 58.5 \\
**MetaCLIP-L** & 24.9 \(\pm\) 0.77 & 15.4 \(\pm\) 0.21 & 20.5 \(\pm\) 0.72 & 24.6 \(\pm\) 2.67 & 16.1 \(\pm\) 1.13 & **70.1 \(\pm\) 0.30** & **64.3 \(\pm\) 3.83 & 14.7 \(\pm\) 0.52 & 25.0 \\ \hline
**BT-CLIP-O** & 47.1 \(\pm\) 0.89 & **91.1** \(\pm\) 0.17 & 22.9 \(\pm\) 0.75 & 43.2 \(\pm\) 3.07 & 16.5 \(\pm\) 1.14 & 47.8 \(\pm\) 0.33 & 28.0 \(\pm\) 3.59 & 17.0 \(\pm\) 0.56 & **70.8** \\
**BT-CLIP-B** & **53.8** \(\pm\) 0.89 & **82.2** \(\pm\) 0.22 & **23.7** \(\pm\) 0.75 & **53.9** \(\pm\) 0.09 & **16.9** \(\pm\) 1.15 & 57.1 \(\pm\) 0.32 & 15.0 \(\pm\) 2.86 & 18.4 \(\pm\) 0.57 & 67.2 \\
**BT-CLIP-M** & 44.3 \(\pm\) 0.89 & **91.1** \(\pm\) 0.17 & 21.8 \(\pm\) 0.74 & **54.7** \(\pm\) 3.09 & 5.1 \(\pm\) 0.68 & 42.5 \(\pm\) 0.32 & 26.3 \(\pm\) 3.52 & **49.9** \(\pm\) 0.74 & **69.5** \\ \hline \hline \end{tabular}
\end{table}
Table 5: **BioProve-CLIP performance on various benchmarks. The top three rows are pre-trained checkpoints: OpenAI-B refers to OpenAI’s ViT-B-16 model, BioCLIP-B refers to the BioCLIP ViT-B-16 model, and MetaCLIP-L refers to the MetaCLIP-cc ViT-L-14 model. The bottom three rows are BioProve-CLIP models fine-tuned on different checkpoints: BT-Clip-O (from OpenAI-B), BT-Clip-B (from BioCLIP-B), and BT-Clip-M (from MetaCLIP-L). Benchmark abbreviations: BTU (Birotove-Unseen, n=300), BTB (Birotove-Balanced, n=2253), BCR (BioCLIP-Rare, n=400), F (Fungi, n=25), I2 (Inscets-2, n=102), B (Birds-525, n=525), LS (Life-Stages, n=20), and DW (DeepWeeds, n=9). 95% confidence intervals (\(\pm\)) are included.**

## References

* Alcorn et al. [2019] Michael A Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, and Anh Nguyen. Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4845-4854, 2019.
* Chen et al. [2021] Siqi Chen, Yijie Pei, Zunwang Ke, and Wushour Silamu. Low-resource named entity recognition via the pre-training model. _Symmetry_, 13(5):786, 2021.
* Chiavassa and Kraft [2024] Juan A Chiavassa and Martin Kraft. The fair-device-an ai image recognition-based non-lethal and generalist monitoring system for insect biodiversity in agriculture. In _44. GIL-Jahrestagung, Biodiversitat fordern durch digitale Landwirtschaft_, pages 209-214. Gesellschaft fur Informatik eV, 2024.
* Chiranjeevi et al. [2023] Shivani Chiranjeevi, Mojdeh Sadaati, Zi K Deng, Jayanth Koushik, Talukder Z Jubery, Daren Mueller, Matthew E O Neal, Nirav Merchant, Aarti Singh, Asheesh K Singh, Soumik Sarkar, Arti Singh, and Baskar Ganapathysubramanian. Deep learning powered real-time identification of insects using citizen science data, 2023.
* Chiu et al. [2020] Mang Tik Chiu, Xingqian Xu, Yunchao Wei, Zilong Huang, Alexander G Schwing, Robert Brunner, Hrant Khachatrian, Hovnatan Karapetyan, Ivan Dozier, Greg Rose, et al. Agriculture-vision: A large aerial image database for agricultural pattern analysis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2828-2838, 2020.
* De Langhe et al. [2024] Loic De Langhe, Orphee De Clercq, and Veronique Hoste. Unsupervised authorship attribution for medieval latin using transformer-based embeddings. In _Proceedings of the Third Workshop on Language Technologies for Historical and Ancient Languages (LT4HALA)@ LREC-COLING-2024_, pages 57-64, 2024.
* Cecco et al. [2021] Grace J Di Cecco, Vijay Barve, Michael W Belitz, Brian J Stucky, Robert P Guralnick, and Allen H Hurlbert. Observing the observers: how participants contribute data to inaturalist and implications for biodiversity science. _BioScience_, 71(11):1179-1188, 2021.
* Biodiversity Information Facility [2023] Global Biodiversity Information Facility. Gbif backbone taxonomy. https://www.gbif.org/dataset/d7dddbf4-2cf0-4f39-9b2a-bb099caae36c, 2023.
* Feuer and Hegde [2023] Ben Feuer and Chinmay Hegde. Exploring dataset-scale indicators of data quality. _ArXiv_, abs/2311.04016, 2023. URL https://api.semanticscholar.org/CorpusID:265042924.
* Feuer et al. [2024] Benjamin Feuer, Ameya Joshi, Minsu Cho, Shivani Chiranjeevi, Zi Kang Deng, Aditya Balu, Asheesh K. Singh, Soumik Sarkar, Nirav Merchant, Arti Singh, Baskar Ganapathysubramanian, and Chinmay Hegde. Zero-shot insect detection via weak language supervision. _The Plant Phenome Journal_, 7(1):e20107, 2024. URL https://acess.onlinelibrary.wiley.com/doi/abs/10.1002/ppj2.20107.
* Gabeff et al. [2024] Valentin Gabeff, Marc Russwurm, Devis Tuia, and Alexander Mathis. Wildclip: Scene and animal attribute retrieval from camera trap data with domain-adapted vision-language models. _International Journal of Computer Vision_, pages 1-17, 2024.
* Geirhos et al. [2018] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. _arXiv preprint arXiv:1811.12231_, 2018.
* Gharaee et al. [2024] Zahra Gharaee, ZeMing Gong, Nicholas Pellegrino, Iuliia Zarublieva, Joakim Bruslund Haurum, Scott Lowe, Jaclyn McKeown, Chris Ho, Joschka McLeod, Yi-Yun Wei, et al. A step towards worldwide biodiversity assessment: The bioscan-1m insect dataset. _Advances in Neural Information Processing Systems_, 36, 2024.
* Guldenring and Nalpantidis [2021] Ronja Guldenring and Lazaros Nalpantidis. Self-supervised contrastive learning on agricultural images. _Computers and Electronics in Agriculture_, 191:106510, 2021.

* [15] iNaturalist. Year in review 2019. https://www.inaturalist.org/blog/29540-year-in-review-2019, 2019.
* [16] iNaturalist. inaturalist 2019 stats. https://www.inaturalist.org/stats/2019, 2019.
* [17] iNaturalist. A second experiment to learn about the accuracy of inaturalist observations, 2023. URL https://www.inaturalist.org/blog/90263-a-second-experiment-to-learn-about-the-accuracy-of-inaturalist-observations. Accessed: 2024-10-28.
* [18] Dhiraj D. Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy, Bharat Kaul, and Pradeep Dubey. A study of BFLOAT16 for deep learning training. _CoRR_, abs/1905.12322, 2019. URL http://arxiv.org/abs/1905.12322.
* [19] Vamsi Krishna Kommineni, Jens Katteg, Jitendra Gaikwad, Susanne Tautenhahn, and Birgitta Koenig-ries. The role of the clip model in analysing herbarium specimen images. _Biodiversity Information Science and Standards_, 7, 2023.
* [20] Xiang Li, Congcong Wen, Yuan Hu, and Nan Zhou. Rs-clip: Zero shot remote sensing scene classification via contrastive vision-language supervision. _International Journal of Applied Earth Observation and Geoinformation_, 124:103497, 2023.
* [21] Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E. Gonzalez, and Ion Stoica. Tune: A research platform for distributed model selection and training. _CoRR_, abs/1807.05118, 2018. URL http://arxiv.org/abs/1807.05118.
* [22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.
* [23] Yuzhen Lu, Dong Chen, Ebenezer Olaniyi, and Yanbo Huang. Generative adversarial networks (gans) for image augmentation in agriculture: A systematic review. _Computers and Electronics in Agriculture_, 200:107208, 2022.
* [24] Alexandra Sasha Luccioni and David Rolnick. Bugs in the data: How imagenet misrepresents biodiversity. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 14382-14390, 2023.
* [25] Mourad Mars. From word embeddings to pre-trained language models: A state-of-the-art walkthrough. _Applied Sciences_, 12(17):8805, 2022.
* [26] Chao Mou, Aokang Liang, Chunying Hu, Fanyu Meng, Baixun Han, and Fu Xu. Monitoring endangered and rare wildlife in the field: A foundation deep learning model integrating human knowledge for incremental recognition with few data and low cost. _Animals_, 13(20):3168, 2023.
* [27] Jorg Muller, Oliver Mitesser, H Martin Schaefer, Sebastian Seibold, Annika Busse, Peter Kriegel, Dominik Rabl, Rudy Gelis, Alejandro Arteaga, Juan Freile, et al. Soundscapes and deep learning enable tracking biodiversity recovery in tropical forests. _Nature communications_, 14(1):6191, 2023.
* [28] K Denise Kendall Niemiller, Mark A Davis, and Matthew L Niemiller. Addressing 'biodiversity naivety'through project-based learning using inaturalist. _Journal for Nature Conservation_, 64:126070, 2021.
* [29] Alex Olsen, Dmitry Konovalov, Bronson Philippa, Peter Ridd, Jake Wood, Jamie Johns, Wesley Banks, Benjamin Girgenti, O.P. Kenny, James Whinney, Brendan Calvert, Mostafa Rahimi Azghadi, and Ron White. Deepweeds: A multiclass weed species image dataset for deep learning. _Scientific Reports_, 9, 02 2019. doi: 10.1038/s41598-018-38343-3.

* [30] Lukas Picek, Milan Sulc, Jiri Matas, Thomas S Jeppesen, Jacob Heilmann-Clausen, Thomas Lasssoe, and Tobias Froslev. Danish fungi 2020-not just another image recognition dataset. In _Proceedings of the IEEE Winter Conference on Applications of Computer Vision_, pages 1525-1535, 2022.
* image classification, 05 2023. URL https://www.kaggle.com/datasets/gpiosenka/100-bird-species.
* [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. _CoRR_, abs/2103.00020, 2021. URL https://arxiv.org/abs/2103.00020.
* [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [34] DB Roy, J Alison, TA August, M Belisle, K Bjerge, JJ Bowden, MJ Bunsen, F Cunha, Q Geissmann, K Goldmann, et al. Towards a standardized framework for ai-assisted, image-based monitoring of nocturnal insects. _Philosophical Transactions of the Royal Society B_, 379(1904):20230108, 2024.
* [35] Atriya Sen, Beckett Sterner, Nico Franz, Caleb Powel, and Nathan Upham. Combining machine learning & reasoning for biodiversity data intelligence. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 14911-14919, 2021.
* [36] Kadukothanahally Nagaraju Shivaprakash, Niraj Swami, Sagar Mysorekar, Roshni Arora, Aditya Gangadharan, Karishma Vohra, Madegowda Jadeyegowda, and Joseph M Kiesecker. Potential for artificial intelligence (ai) and machine learning (ml) applications in biodiversity conservation, managing forests, and related services in india. _Sustainability_, 14(12):7154, 2022.
* [37] Daniele Silvestro, Stefano Goria, Thomas Sterner, and Alexandre Antonelli. Improving biodiversity protection through artificial intelligence. _Nature sustainability_, 5(5):415-424, 2022.
* [38] Rachele Sprugnoli, Giovanni Moretti, and Marco Passarotti. Building and comparing lemma embeddings for latin. classical latin versus thomas aquinas. _IJCoL. Italian Journal of Computational Linguistics_, 6(6-1):29-45, 2020.
* [39] Samuel Stevens, Jiaman Wu, Matthew J Thompson, Elizabeth G Campolongo, Chan Hee Song, David Edward Carlyn, Li Dong, Wasila M Dahdl, Charles Stewart, Tanya Berger-Wolf, et al. Bioclip: A vision foundation model for the tree of life. _arXiv preprint arXiv:2311.18803_, 2023.
* [40] Nathan Stringham and Mike Izbicki. Evaluating word embeddings on low-resource languages. In _Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems_, pages 176-186, 2020.
* [41] Shem Unger, Mark Rollins, Allison Tietz, and Hailey Dumais. inaturalist as an engaging tool for identifying organisms in outdoor activities. _Journal of Biological Education_, 55(5):537-547, 2021.
* [42] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 8769-8778, 2018.
* [43] Rini Wijayanti, Masayu Leylia Khodra, Kridanto Surendro, and Dwi H Widyantoro. Learning bilingual word embedding for automatic text summarization in low resource language. _Journal of King Saud University-Computer and Information Sciences_, 35(4):224-235, 2023.

* [44] Xiaoping Wu, Chi Zhan, Yukun Lai, Ming-Ming Cheng, and Jufeng Yang. IP102: A large-scale benchmark dataset for insect pest recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 8787-8796, 2019.
* [45] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data, 2024.

Appendix

### Background on CLIP and zero-shot classification

Unlike traditional vision models, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) examples, leveraging natural language supervision to enhance generalization [33]. CLIP's approach allows it to learn from a wide variety of images and their associated textual descriptions, making it more flexible and general compared to standard vision models. This flexibility is crucial for in various domains, including biodiversity monitoring and agriculture. For instance, CLIP models analyze digital plant specimen images, aiding in pre-processing and filtering for further analysis for agriculture purposes [19, 20]. As for biodiversity, WildCLIP and KICIP facilitate wildlife observation and monitoring with high accuracy and effectiveness in data-sparse settings [11, 26]. These examples underscore the importance of developing and utilizing comprehensive datasets to fully leverage the capabilities of CLIP models in advancing biodiversity and agricultural research.

### The value of taxonomic information

Taxonomic classification, the hierarchical arrangement of organisms into categories based on shared characteristics, is foundational in biological sciences. Taxonomy underpins various scientific, ecological, and agricultural applications. It allows for precise identification and classification of species, which is fundamental for understanding biodiversity and monitoring ecosystems. For instance, accurate species identification can aid in tracking invasive species, as noted in studies such as [37]. In agriculture, detailed taxonomic information helps in identifying pests and beneficial species, thereby improving pest control strategies and crop management; supports ecological research by providing insights into species interactions, distribution patterns, and evolutionary relationships [13]; and is essential for policy-making and conservation planning [35].

### Scientific versus common names

Although we identify the importance and need to include taxonomic information in the dataset for biodiversity, one potential challenge is the fact that this information is mostly in Latin for which text embedding models often exhibit suboptimal performance due to its status as a low-resource language [40]. Nonetheless, Latin remains indispensable as it is the standard for representing scientific names and taxonomic classifications. We therefore integrate common names, scientific names, and detailed taxonomic hierarchies. We believe that such an "all-encompassing" approach facilitates the learning of relationships between Latin and English terms, thereby improving the models' applicability in scientific contexts [6, 38, 43]. Furthermore, incorporating taxonomic data into the training process significantly enhances the multimodal capabilities of the models, enabling them to associate visual data with taxonomic terminology [25, 2].

### iNaturalist, iNaturalist Open Data

iNaturalist is an online social network for sharing biodiversity information and learning about nature. It serves as a crowdsourced species identification system and organism occurrence recording tool. Users from around the world upload images, making the continuously updated dataset valuable for AI applications in biodiversity and research. Each photo includes detailed metadata: copyright status, location, uploader, time, and taxonomic classification. This diversity in image sources makes iNaturalist an excellent dataset for training AI models intended for real-world applications [41, 28, 7, 3]. Despite its vast and diverse data, iNaturalist is not directly optimized for AI researchers: arranging this data for use in AI models like CLIP is not straightforward. Each photo has its own page on the iNaturalist website, making it difficult to download images along with all the necessary information in a streamlined manner.

The iNaturalist Open Dataset aims to address some of these challenges. It is one of the world's largest public datasets of photos of living organisms, structured as a "bucket" of images stored using Amazon Web Service's Simple Storage Service (S3). The dataset includes multiple resized versions of each photo, allowing users to download the size most useful to their research.

Additionally, the dataset provides four tab-separated CSV files representing observations, observers, photos, and taxa_id. These files are generated monthly, capturing a snapshot of the continually changing iNaturalist data. The images in the iNaturalist Open Dataset are licensed under either CC0, CC-BY, or CC-BY-NC and are open for public research. Photos with a CC0 license can be attributed as "[observer name or login], no rights reserved (CC0)". Photos with other Creative Commons licenses can be attributed as "[observer name or login], some rights reserved ([license abbreviation])".

### iNaturalist Details

Each image in the iNaturalist Open Dataset can be associated with its appropriate metadata through a group of four metadata CSV files, representing photos, observations, taxa, and observers.

The photos metadata file contain nine distinct columns of metadata information of each photo. Of these columns, only photo_id and observation_uuid are relevant for us. The value of photo_id is a identifier number used to access individual photos, the photo's iNaturalist page can be found by constructing a URL in this format: https://www.inaturalist.org/photos/[photo_id]. The value of observation_uuid indicates which observation the photo is associated with, it is used to map the photos metadata to the observations metadata.

An observation represents one user submission of a species encounter to the iNaturalist website. One observation can have multiple photos of the same species but never multiple species. The observation metedata file contains eight distinct columns of metadata information on each observation. The columns relevant to us are observation_uuid, quality grade, and taxon_id. Each observation is given a unique number identifier indicated by its observation_uuid. iNaturalist has its own system to determining the quality of an observation and its associated photos, quality_grade represents this and can range from "Casual", "Research Grade", or "Needs ID". The value taxon_id indicates the species is represented in the observation, it is used to map the observations metadata to the taxa metadata.

The taxa metadata file contains information about each specific taxon in iNaturalist, it has has six distinct metadata columns. The columns relevant to us are taxon_id, name, ancestry, and active. Each specific taxon in iNaturalist has a unique identifier number associated with it, this is its taxon_id. This taxon_id will map to the scientific name of the taxon which is represented in the name metadata column. Each taxon also has associated with it a taxonomic ancestry, this is represented as a string of taxon_ids concatenated together with "\(\backslash\)" like so "48460/1/47115/47584/1051154". The active column indicated whether the taxon is currently in use in iNaturalist.

The observer metadata file contains information about each user within the iNaturalist site. For the purpose of machine learning research none of its three metadata columns are relevant.

While the iNaturalist Open Dataset metadata files provide a plethora of interesting information, its structure makes it inherently cumbersome to use for research. To solve this, we aggregate and process the iNaturalist metadata into a concise and streamlined format for easy query and usage.

First, the respective CSV files are used to populate a SQL database with each CSV file as its own SQL table. A new aggregate SQL table is created that joins the photos, observations, and taxa tables on its relational columns. Only the metadata columns we deemed relevant are kept and the extraneous non-useful metadata columns are discarded.

One of the difficulties working with the base iNaturalist metadata files is that it does not contain the image URL, information that is critical in image downloads. We include a new column in the aggregated metadata table that explicitly links to the Amazon S3 URL in which the image is hosted.

The BioProve metadata file used for model training contains the metadata columns phylum, class, order, family, genus, species, scientific_name, common_name for the seven BioProve categories _Aves_, _Arachida_, _Insecta_, _Plantae_, _Fungi_, _Mollusca_, and _Reptilia_. To ensure that only images and metadata from the seven BioProve categories appear in our final dataset we use the taxa table to find the taxon in our categories then use it in a SQL query on the ancestry column of our aggregated metadata table.

The taxonomic rank columns are also found utilizing the ancestry metadata column. A difficulty in working with the ancestry metadata is present in that there is not a clear indication of what taxonomic rank a taxon id represents the ancestry string. This problem is exacerbated due to the presence of taxonomic ranks and dsub ranks whose presence is variable across different species. As such, a custom function is applied to each row to dynamically find the rank of each taxon id in the ancestry and then appropriately populate the taxon id to a metadata column of that rank. This process results in all taxonomies rank represented as metadata columns; only phyllum, class, order, family, genus and species are kept in the BioProve metadata file.

The scientific name of a species is found using the name metadata column of our aggregated metadata table. The common name of a species is also useful metadata information. Unfortunately, the iNaturalist Open Data metadata files do not contain the common name information of a species. To address this, we curate a lookup table of the common names in our dataset. This is obtained from the iNaturalist Taxonomy DarwinCore Archive, Having obtained the common names for each species, we append it to the BioProve-specific metadata.

### Composition of BioProve and Related Datasets

In Table 6, we compare BioProve with existing large-scale biodiversity datasets. BioProve comprises 161.9 million research-grade images, representing approximately 372,966 species, and significantly surpasses other datasets in terms of both diversity and scale.

### Composition of BioProve-Train

See Figure 5 and Table 7.

### BioProve-CLIP training details

We use BioProve-Train to train new CLIP-style foundation models, and then evaluate them on zero-shot image classification tasks. Following the implementation of Stevens et al. [39], we utilize a ViT-B/16 architecture initialized from the OpenAI pretrained weights for our main model, and train for 40 epochs.

\begin{table}
\begin{tabular}{|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|} \hline
**Dataset** & **BioProve** & **Wildlife Insights** & **TreeOfLife** & **BioScan** & **Naturalsit 2017 [42]** & **Naturalist 2019 [16, 15]** & **GBIF Backbone**[8] \\ \hline
**Size** & 161.9 million images & 148.8 million images (52.6M wildlife images) & 10.43I images & 1.13I images & 675,170 images & 13.1M images & 7.5M records \\ \hline
**Diversity** & 366.6K species & 3,682 species & 454.11K species & 8.3K species & 5,089 species & 166.8K species & Millions of species \\ \hline
**Labels** & Common/scientific names, taxonomic hierarchies & Species, location, timestamps, behavioral tags & Common/scientific names, taxonomic hierarchies & Common/scientific names, taxonomic hierarchies & Common/scientific names, taxonomic hierarchies & Common/scientific names, taxonomic hierarchies & Common/scientific names, taxonomic hierarchies & Common/scientific names, taxonomic hierarchies & Common/scientific names, taxonomic hierarchies & Species names, OTU libraries \\ \hline
**Data** & iNaturalist Open Dataset & Camera traps, sensors & Naturalist, BioScan-1M & Malae trap, Specimens, DNA-bordered & Naturalist & Naturalist & Naturalist & Catalogue of Life, iIDL, UNITE, WebHLS, etc. \\ \hline
**Key Features** & AI-ready pipeline, high-quality annotations, supports, BioProve-CLIP & Automated processing, AI species & Rich hierarchical data, metadata, species supports & Instruct-focused, taxonomic data, taxonomic data, BioProve-CLIP & Inbalanced classes, taxonomic data, face-guined & Large-scale species data, face-guined & Comprehensive taxonomy; cross-referencing \\ \hline
**Al-Ready** & Yes & Yes & Yes & Yes & No & No & No \\ \hline \end{tabular}
\end{table}
Table 6: Comparison of BioProve with other biodiversity datasets.

In addition, we also train a ViT-L/14 model from the MetaCLIP [45] checkpoint for 12 epochs, and a ViT-B/16 from the BioCLIP checkpoint for 8 epochs. We select the AdamW optimizer from Loshchilov and Hutter [22] along with a cosine learning rate scheduler, as this has previously been shown to perform well for CLIP pretraining [32]. We conduct twenty rounds of hyperparameter optimization using Ray Tune [21] to determine the optimal learning rate, \(\beta_{1}\), \(\beta_{2}\) and weight decay settings.

We train our models for a combined 10 days on 8xH100 nodes in bfloat16 precision [18] with gradient checkpointing, computing loss with local features, and utilizing static graph optimization for DDP.

### Additional BioTrove-CLIP results

In Table 8, we report model performance at different levels of the taxonomic hierarchy. Generally, we find that models trained on web-scraped data perform better with common names, whereas models trained on specialist datasets perform better when using scientific names. Additionally, models trained on web-scraped data excel at classifying at the highest taxonomic level (kingdom), while models begin to benefit from specialist datasets like BioTrove-Train and Tree-of-Life-10M at the lower taxonomic levels (order and species).

However, BioTrove-CLIP shows a performance decline at taxonomic levels below the species level. This is likely because our training metadata structure allows for classifications solely by referring to species information. From a practical standpoint, this is not problematic for the species in our test set since BioTrove-CLIP is highly accurate at the species level, and higher-level taxa can be deterministically derived from the lower ones.

\begin{table}
\begin{tabular}{|c|c|} \hline
**Category** & **Number of Unique Species** \\ \hline Kingdom: Fungi & 281 \\ Kingdom: Plantae & 500 \\ Phylum: Mollusca & 147 \\ Class: Insecta & 500 \\ Class: Arachnida & 136 \\ Class: Reptilia & 189 \\ Class: Aves & 500 \\ \hline \end{tabular}
\end{table}
Table 7: Number of Unique Species in Each Category in BioTrove-Balanced.

Figure 5: BioTrove-Train Dataset Analysis: a) Consistent category distribution across BioTrove-Train and BioTrove-116M datasets. b) Species exhibit a long-tailed distribution. c) Impact of local vs. semi-global shuffling on species representation within training minibatches.

Furthermore, the OpenCLIP and MetaCLIP baselines outperform BioTrove-CLIP on the life stages benchmark. This highlights the importance of retaining the general linguistic capabilities of the pretrained CLIP models for hybrid tasks.

### Additional BioTrove-CLIP Comparative Analysis

We conducted a comparative evaluation of the top-1 and top-5 zero-shot accuracy of the BioCLIP model, which was trained exclusively on the iNaturalist 2021 (iNat21) dataset, and the BioTrove-CLIP model, initialized from BioCLIP checkpoints originally trained on the TreeOfLife dataset. The comparison highlights the performance differences across various benchmarks, as presented in Table 9.

Our analysis shows that models trained on the BioTrove dataset consistently outperform those trained solely on iNat21, particularly in benchmarks such as BioTrove-Unseen, _Fungi_, and _Insects-2_. While certain benchmarks like Life-Stages and DeepWeeds show moderate differences, the results emphasize the advantages of training on BioTrove, leading to enhanced model accuracy and robustness.

The following table provides detailed performance metrics for both models across various benchmarks, comparing their top-1 and top-5 accuracy scores with associated confidence intervals.

As demonstrated, the model trained on BioTrove exhibits superior performance in most categories, particularly when evaluated on rare and unseen species, underscoring the importance of diverse and large-scale datasets like BioTrove for enhancing biodiversity AI models.

\begin{table}
\begin{tabular}{l r r r r} \hline \hline
**Benchmark** & **BC-iNat21** & **Top-1** & **BC-iNat21** & **Top-5** & **BT-Clip Top-1** & **BT-Clip Top-5** \\ \hline BioTrove Unseen & 0.2100 & 0.3470 & 0.5380 & 0.8220 \\ _Fungi_ & 0.4420 & 0.7550 & 0.5390 & 0.7590 \\ Life-Stages & 0.2867 & 0.8617 & 0.1500 & 0.8600 \\ DeepWeeds & 0.2057 & 0.6897 & 0.1840 & 0.5740 \\ _Insects-2_ & 0.0103 & 0.0483 & 0.1690 & 0.5710 \\ _Birds-525_ & 0.5030 & 0.6330 & 0.5710 & 0.7540 \\ BioCLIP-Rare & 0.1490 & 0.2790 & 0.2370 & 0.7600 \\ BioTrove Balanced & 0.5020 & 0.6450 & 0.5180 & 0.6610 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Performance Comparison Across Benchmarks: This table compares the performance of BC-iNat21 (trained solely on the iNaturalist 2021 dataset) and BT-Clip (trained from the BioCLIP checkpoint, originally trained on the TreeOfLife dataset). Metrics include Top-1 Accuracy and Top-5 Accuracy.

\begin{table}
\begin{tabular}{l r r r r} \hline \hline
**Benchmark** & **BC-iNat21** & **BC-iNat21** & **BT-Clip** & **BT-Clip** \\  & **Top-1 Acc.** & **Top-5 Acc.** & **Top-1 Acc.** & **Top-5 Acc.** \\ \hline BioTrove-Unseen & 0.2100 & 0.3470 & 0.5380 & 0.8220 \\ _Fungi_ & 0.4420 & 0.7550 & 0.5390 & 0.7590 \\ Life-Stages & 0.2867 & 0.8617 & 0.1500 & 0.8600 \\ DeepWeeds & 0.2057 & 0.6897 & 0.1840 & 0.5740 \\ _Insects-2_ & 0.0103 & 0.0483 & 0.1690 & 0.5710 \\ _Birds-525_ & 0.5030 & 0.6330 & 0.5710 & 0.7540 \\ BioCLIP-Rare & 0.1490 & 0.2790 & 0.2370 & 0.7600 \\ BioTrove Balanced & 0.5020 & 0.6450 & 0.5180 & 0.6610 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Performance Comparison Across Benchmarks: This table compares the performance of BC-iNat21 (trained solely on the iNaturalist 2021 dataset) and BT-Clip (trained from the BioCLIP checkpoint, originally trained on the TreeOfLife dataset). Metrics include Top-1 Accuracy and Top-5 Accuracy. BC-iNat21 refers to BioCLIP (iNat21), and BT-Clip refers to BioTrove-CLIP (BioCLIP checkpoint from TreeOfLife).

### Checklist

1. For all authors... 1. Do the main claims made in the abstract, and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Section 5. 3. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Section 4.3. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 4.3 and the Appendix. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Training a single model took several weeks and we do not have the resources to train multiple models with the same hyperparameters. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.3.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] See Section A.4. section 2-1 and 3, we mentioned Inat open dataset. we mention INAT open dataset; how to cite it? 2. Did you mention the license of the assets? [Yes] See Section 2. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] See Section 2. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Section 2. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Section 2.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]