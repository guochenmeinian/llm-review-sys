# On Theoretical Limits of Learning with Label Differential Privacy

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Label differential privacy (DP) is designed for learning problems with private labels and public features. Although various methods have been proposed for learning under label DP, the theoretical limits remain unknown. The main challenge is to take infimum over all possible learners with arbitrary model complexity. In this paper, we investigate the fundamental limits of learning with label DP under both central and local models. To overcome the challenge above, we derive new lower bounds on testing errors that are adaptive to the model complexity. Our analyses indicate that \(\epsilon\)-local label DP only enlarges the sample complexity with respect to \(\epsilon\), without affecting the convergence rate over the sample size \(N\), except the case with heavy-tailed label. Under the central model, the performance loss due to the privacy mechanism is further weakened, such that the additional sample complexity becomes negligible. Overall, our analysis validates the promise of learning under the label DP from a theoretical perspective and shows that the learning performance can be significantly improved by weakening the DP definition to only labels.

## 1 Introduction

Many modern machine learning tasks require sensitive training samples that need to be protected from leakage [1]. As a standard approach for privacy protection, differential privacy (DP) [2] has been extensively studied [3; 4; 5; 6; 7; 8; 9]. However, the learning performances under original DP definition are usually far from satisfactory [10; 11; 12; 13]. Therefore, researchers attempt to design weakened DP requirements, under which the performances can be significantly improved, while still securing sensitive information. Under such background, label DP has emerged in recent years [14], which regards features as public, while only labels are sensitive and need to be protected. Such setting is realistic in many applications, such as computational advertising [15], recommendation systems [16] and medical diagnosis [17]. These tasks usually use some basic demographic information as features, which can be far less sensitive.

Despite various approaches for learning with label DP [14; 18; 19; 20; 21], the fundamental limits are still unknown. An interesting question is: By weakening the DP definitions to only labels, how much accuracy improvement is possible? From an information-theoretic perspective [22], the underlying limits of statistical problems are characterized by the minimax lower bound, which takes the supremum over all possible distributions from a general class, and infimum over all learners. Deriving minimax lower bounds for learning under the label DP is challenging in two aspects. Firstly, under label DP, each sample has both public (i.e. the feature) and private (i.e. the label) components. Directly applying the methods for original DP [23; 24; 25; 26; 27] treats all components as private, and thus does not yield tight results. Secondly, the classical packing method [47] is only suitable for fixed model structures with fixed dimensionality. However, to establish lower bounds, one needs to take infimum over all possible learners with arbitrary model complexity.

In this paper, we investigate the theoretical limits of classification and regression problems under label DP. Our analysis involves both central and local models. For each problem, we derive the information-theoretic minimax lower bound of the risk function over a wide class of distributions satisfying the \(\beta\)-Holder smoothness and the \(\gamma\)-Tsybakov margin assumption [28] (see Assumption 1 for details). The general idea is to convert the problem to multiple hypothesis testing. To overcome the challenges above, we provide a bound of Kullback-Leibler divergence over joint distributions of private and public random variables, which is tighter than the bound between fully private variables. Moreover, under the central model, instead of using the packing method, we develop a new lower bound on the minimum testing error for each pair of hypotheses based on the group privacy property [4], which is suitable for arbitrary model complexity. After deriving minimax lower bounds, we also propose algorithms with matching upper bounds to validate the tightness of our results.

The results are shown in Table 1, in which the third row refers to the bounds under the original local DP definition, while the fourth row lists the non-private baselines. To the best of our knowledge, minimax rates under central DP have not been established, and are thus not listed here. The main findings are summarized as follows.

* Under \(\epsilon\)-local label DP, for classification and regression with bounded label noise, the sample complexity is larger by a factor of \(O(1/\epsilon^{2})\). However, the convergence rate remains unaffected, which is in clear contrast with the original DP, under which the convergence rate is slower.
* Under \(\epsilon\)-local label DP constraint, for regression with heavy-tailed label noise, the convergence rate of risk over \(N\) becomes slower, indicating that heavy-tailed labels increase the difficulty of privacy protection.
* Under \(\epsilon\)-central label DP constraint, the performance loss caused by the privacy mechanism becomes further weakened. The risk only increases by a term that decays faster than the non-private rate, indicating that the additional sample complexity caused by the privacy mechanism becomes negligible with large \(N\).

In general, our analysis provides a theoretical perspective of understanding label DP. The result shows that by weakening the DP definition to protecting labels only, the learning performances can be significantly improved.

## 2 Related Work

**Label DP.** Under the local model, labels are randomized before training. The simplest method is randomized response [30]. An important improvement is proposed in [14], called RRWithPrior, which incorporates prior distribution. [19] proposes ALIBI, which further improves randomized response by generating soft labels through Bayesian inference. There are also several methods for regression under label DP [31; 18]. Under central label DP, [20] proposes a clustering approach. [19] proposes private aggregation of teacher ensembles (PATE), which is then further improved in [21].

**Minimax analysis for public data.** Minimax theory provides a rigorous framework for the best possible performance of an algorithm given some assumptions. Classical methods include Le Cam [32], Fano [33] and Assouad [34]. Using these methods, minimax lower bounds have been widely established for both classification and regression problems [35; 36; 37; 38; 39; 40; 41; 28]. If the feature vector has bounded support, then the minimax rate of classification and regression are \(O(N^{-\frac{\beta(\gamma+1)}{2\beta+d}})\) and \(O(N^{-\frac{2\beta}{2\beta+d}})\), respectively.

\begin{table}
\begin{tabular}{c c c} \hline  & Regression & Regression \\  & Bounded label noise & Unbounded label noise \\ \hline Local & \(\tilde{O}\left((N(\epsilon^{2}\wedge 1))^{-\frac{\beta(\gamma+1)}{2\beta+d}} \right)\) & \(\tilde{O}\left((N(\epsilon^{2}\wedge 1)^{-\frac{2\beta}{d+2}})\right)\) & \(O\left((N\epsilon^{2})^{-\frac{2\beta(p-1)}{2\beta+d(p-1)}}\lor N^{-\frac{2 \beta}{2\beta+d}}\right)\) \\ Central & \(\tilde{O}\left(N^{-\frac{\beta(\gamma+1)}{2\beta+d}}+(\epsilon N)^{-\frac{ \beta(\gamma+1)}{\beta+d}}\right)\) & \(O\left(N^{-\frac{2\beta}{2\beta+d}}+(\epsilon N)^{-\frac{2\beta}{d+3}}\right)\) & \(O\left(N^{-\frac{2\beta}{2\beta+d}}+(\epsilon N)^{-\frac{2\beta(p-1)}{\beta+d(p- 1)}}\right)\) \\ Local full & \(O((N(\epsilon^{2}\wedge 1)^{-\frac{\beta(\gamma+1)}{2\beta+d}})\) & \(O((N(\epsilon^{2}\wedge 1))^{-\frac{\beta}{2\beta+d}})\) & \(O((N(\epsilon^{2}\wedge 1))^{-\frac{\beta(p-1)}{\beta+d(p-1)}})\) \\ Non-priv. & \(O(N^{-\frac{\beta(\gamma+1)}{2\beta+d}})\) & \(O(N^{-\frac{2\beta}{2\beta+d}})\) & \(O(N^{-\frac{2\beta}{2\beta+d}})\) \\ \hline \end{tabular}
\end{table}
Table 1: Minimax rate of convergence under label differential privacy. \(d\) is the dimension of features.

**Minimax analysis for private data.** Under the local model, [42] finds the relation between label DP and stochastic query. [23] and [24] develop the variants of Le Cam, Fano, and Assouad's method under local DP. Lower bounds are then established for various statistical problems, such as mean estimation [43, 44, 45, 46], classification [26] and regression [27]. Under central model, for pure DP, the standard approach is the packing method [47], which is then used in hypothesis testing [48], mean estimation [49, 50], and learning of distributions [51, 52, 53]. There are also several works on approximate DP, such as [54, 55].

This work studies the theoretical limits of label DP, under which each sample is a mixture of public feature and private labels, thus existing methods can not be directly applied here. Under the central model, the minimax analysis becomes more challenging, since the packing method is only suitable for fixed model structures (i.e. the dimensionality of model output is fixed), while we need to find the minimum possible error over all possible learners with arbitrary output dimensions. As a result, the lower bounds of general classification and regression problems have not been established even under the original DP definition. To overcome such challenge, we develop a new approach to bound the error of hypothesis testing (see Lemma 1 in Appendix D).

## 3 Preliminaries

In this section, we show some necessary definitions, background information, and notations.

### Label DP

To begin with, we review the definition of DP. Suppose the dataset consists of \(N\) samples \((\mathbf{x}_{i},y_{i})\), \(i=1,\ldots,N\), in which \(\mathbf{x}_{i}\in\mathcal{X}\) is the feature vector, while \(y_{i}\in\mathcal{Y}\subset\mathbb{R}^{d}\) is the label.

**Definition 1**.: _(Differential Privacy (DP) [2]) Let \(\epsilon\geq 0\). A randomized function \(\mathcal{A}:(\mathcal{X},\mathcal{Y})^{N}\rightarrow\Theta\) is \(\epsilon\)-DP if for any two adjacent datasets \(D,D^{\prime}\in(\mathcal{X},\mathcal{Y})^{N}\) and any \(S\subseteq\Theta\),_

\[P(\mathcal{A}(D)\in S)\leq e^{\epsilon}P(\mathcal{A}(D^{\prime})\in S),\] (1)

_in which \(D\) and \(D^{\prime}\) are adjacent if they differ only on a single sample, including both the feature vector and the label._

In machine learning tasks, the output of \(\mathcal{A}\) is the model parameters, while the input is the training dataset. Definition 1 requires that both features and labels are privatized. Consider that in some applications, the features may be much less sensitive, the notion of label DP is defined as follows.

**Definition 2**.: _(Central label DP) A randomized function \(\mathcal{A}\) is \(\epsilon\)-label DP if for any two datasets \(D\) and \(D^{\prime}\) that differ on the label of only one training sample and any \(S\subseteq\Theta\), (1) holds._

Compared with Definition 1, Definition 2 only requires the output to be insensitive to the replacement of a label. Therefore label DP is a weaker requirement. Correspondingly, the local label DP is defined as follows.

**Definition 3**.: _(Local label DP) A randomized function \(M:(\mathcal{X},\mathcal{Y})\rightarrow\mathcal{Z}\) is \(\epsilon\)-local label DP if_

\[\sup_{y,y^{\prime}\in\mathcal{Y}S\subseteq\mathcal{Z}}\ln\frac{P(M(\mathbf{x },y)\in S)}{P(M(\mathbf{x},y^{\prime})\in S)}\leq\epsilon.\] (2)

Definition 3 requires that each label is privatized locally before running any machine learning algorithms. It is straightforward to show that local label DP ensures central label DP. To be more precise, we have the following proposition.

**Proposition 1**.: _Let \(\mathbf{z}_{i}=M(\mathbf{x}_{i},y_{i})\) for \(i=1,\ldots,N\). If \(\mathcal{A}\) is a function of \((\mathbf{x}_{i},\mathbf{z}_{i})\), \(i=1,\ldots,N\), then \(\mathcal{A}\) is \(\epsilon\)-label DP._

### Risk of Classification and Regression

In supervised learning problems, given \(N\) samples \((\mathbf{X}_{i},Y_{i})\), \(i=1,\ldots,N\) drawn from a common distribution, the task is to learn a function \(g:\mathcal{X}\rightarrow\mathcal{Y}\). For a loss function \(l:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}\), the goal is to minimize the _risk function_, which is defined as the expectation of loss function between the predicted value and the ground truth:

\[R=\mathbb{E}[l(\hat{Y},Y)].\] (3)The minimum risk among all function \(g\) is called Bayes risk, i.e. \(R^{*}=\min_{g}\mathbb{E}[l(g(\mathbf{X},Y))]\). In practice, the sample distribution is unknown, and we need to learn \(g\) from samples. Therefore, the risk of any practical classifiers is larger than Bayes risk. The gap \(R-R^{*}\) is called excess risk, and we hope that \(R-R^{*}\) to be as small as possible. Now we discuss classification and regression problems separately.

_1) Classification._ For classification problems, the size of \(\mathcal{Y}\) is finite. For convenience, we denote \(\mathcal{Y}=[K]\), in which \([K]:=\{1,\ldots,K\}\). In this paper, we use \(0-1\) loss, i.e. \(l(\hat{Y},Y)=\mathbf{1}(\hat{Y}\neq Y)\), then \(R=\text{P}(\hat{Y}\neq Y)\). Define \(K\) functions \(\eta_{1},\ldots,\eta_{K}\) as the conditional class probabilities:

\[\eta_{k}(\mathbf{x})=\text{P}(Y=k|\mathbf{X}=\mathbf{x}),k=1,\ldots,K.\] (4)

Under this setting, the Bayes optimal classifier and the corresponding Bayes risk is

\[c^{*}(\mathbf{x}) = \underset{j\in[K]}{\arg\max}\eta_{j}(\mathbf{x}),\] (5) \[R^{*}_{cls} = \text{P}(c^{*}(\mathbf{X})\neq Y).\] (6)

_2) Regression._ Now we consider the case with \(\mathcal{Y}\) having infinite size. We use \(\ell_{2}\) loss in this paper, i.e. \(l(\hat{Y},Y)=(\hat{Y}-Y)^{2}\). Then the Bayes risk is

\[R^{*}_{reg}=\mathbb{E}[(Y-\eta(\mathbf{X}))^{2}].\] (7)

Then the following proposition gives a bound of the excess risk for classification and regression problems.

**Proposition 2**.: _For any classifier \(c:\mathcal{X}\rightarrow[K]\), the excess risk of classification is bounded by_

\[R_{cls}-R^{*}_{cls}=\int(\eta^{*}(\mathbf{x})-\mathbb{E}[\eta_{c(\mathbf{x})} (\mathbf{x})])f(\mathbf{x})d\mathbf{x}.\] (8)

_For any regression estimate \(\hat{\eta}:\mathcal{X}\rightarrow\mathcal{Y}\), the excess risk of regression is bounded by_

\[R_{reg}-R^{*}_{reg}=\mathbb{E}[(\hat{\eta}(\mathbf{X})-\eta(\mathbf{X}))^{2}].\] (9)

The proof of Proposition 2 is shown in Appendix A. Finally, we state some basic assumptions that will be used throughout this paper.

**Assumption 1**.: _There exists some constants \(L\), \(\beta\), \(C_{T}\), \(\gamma\), \(c\), \(D\) and \(\theta\in(0,1]\) such that_

_(a) For all \(j\in[K]\) and any \(\mathbf{x}\), \(\mathbf{x}^{\prime}\), \(|\eta_{j}(\mathbf{x})-\eta_{j}(\mathbf{x}^{\prime})|\leq L\left\|\mathbf{x}- \mathbf{x}^{\prime}\right\|^{\beta}\);_

_(b) For any \(t>0\), \(\text{P}\left(0<\eta^{*}(\mathbf{X})-\eta_{s}(\mathbf{X})<t\right)\leq C_{T}t ^{\gamma},\) in which \(\eta_{s}(\mathbf{x})\) is the second largest one among \(\{\eta_{1}(\mathbf{x}),\ldots,\eta_{K}(\mathbf{x})\}\);_

_(c) The feature vector \(\mathbf{X}\) has a probability density function (pdf) \(f\) which is bounded from below, i.e. \(f(\mathbf{x})\geq c\);_

_(d) For all \(r<D\), \(V_{r}(\mathbf{x})\geq\theta v_{d}r^{d},\) in which \(V_{r}(\mathbf{x})\) is the volume (Lebesgue measure) of \(B(\mathbf{x},r)\cap\mathcal{X}\), \(v_{d}\) is the volume of a unit ball._

Assumption 1 (a) requires that all \(\eta_{j}\) are Holder continuous. This condition is common in literatures about nonparametric statistics [28]. (b) is generalized from the Tsybakov noise assumption for binary classification, which is commonly used in many existing works in the field of both nonparametric classification [29, 37, 40, 41] and differential privacy [26, 27]. If \(K=2\), then \(\eta^{*}\) and \(\eta_{s}\) refer to the larger and smaller class conditional probability, respectively. An intuitive understanding of (b) is that in the majority of the support, the maximum value among \(\{\eta_{1}(\mathbf{x}),\ldots,\eta_{K}(\mathbf{x})\}\) should have some gap to the second largest one. With sufficiently large sample size and model complexity, assumption (b) ensures that for test samples within the majority of the support \(\mathcal{X}\), the algorithm is highly likely to correctly identify the class with the maximum conditional probability. Therefore, in (b), we only care about \(\eta^{*}(\mathbf{x})\) and \(\eta_{s}(\mathbf{x})\), while other classes with small conditional probabilities can be ignored. (c) is usually called "strong density assumption" in existing works [39, 40], which is quite strong. It is possible to relax this assumption so that the theoretical analysis becomes suitable for general cases. However, we do not focus on such generalization in this paper. Assumption (d) prevents the corner of the support \(\mathcal{X}\) from being too sharp. In the remainder of this section, denote \(\mathcal{F}_{cls}\) as the set of all pairs \((f,\eta)\) satisfying Assumption 1.

[MISSING_PAGE_FAIL:5]

According to Definition 3, \(M\) is \(\epsilon\)-local label DP. For the performance guarantee (14), according to Proposition 2, we need to bound \(\eta^{*}(\mathbf{x})-\mathbb{E}[\eta_{c(\mathbf{x})}(\mathbf{x})]\) for each \(\mathbf{x}\). If \(\eta^{*}(\mathbf{x})-\eta_{s}(\mathbf{x})\) is large, then with high probability, \(c(\mathbf{x})=c^{*}(\mathbf{x})\), and then \(\eta^{*}(\mathbf{x})=\eta_{c(\mathbf{x})}(\mathbf{x})\). Thus we mainly consider the case with small \(\eta^{*}(\mathbf{x})-\eta_{s}(\mathbf{x})\). The details of proof are shown in Appendix C. 

The lower bound (10) and the upper bound (14) match up to a logarithm factor, indicating that the results are tight. Now we comment on the results.

**Remark 1**.: _1) **Comparison with non-private bound.** The classical minimax lower bound for non-private classification problem is \(N^{-\frac{\beta(\gamma+1)}{2\beta+d}}\). Therefore, the lower bound (10) reaches the non-private bound with \(\epsilon\gtrsim 1\). With small \(\epsilon\), \(N\) training samples with privatized labels roughly equals \(N\epsilon^{2}\) non-privatized samples in terms of performance._

_2) **Comparison with local DP that protects both features and labels.** In this case, the optimal excess risk is \((N\epsilon^{2})^{-\beta(\gamma+1)/(2\beta+2d)}\lor N^{-\beta(\gamma+1)/(2 \beta+d)}\), which is worse than the right hand side of (10). Such result indicates that compared with classical DP, label DP incurs significantly weaker performance loss._

_3) **Comparison with other baseline methods.** If we use the randomized response method instead of the privacy mechanism (11), then the performance decreases sharply with the number of classes \(K\). Several methods have been proposed to improve the randomized response method, such as RRWithPrior [14] and ALBII [19]. However, these methods are not guaranteed in theory._

### Central Label DP

_1) Lower bound._ The following theorem shows the minimax lower bound under the central label DP.

**Theorem 3**.: _Denote \(\mathcal{A}_{\epsilon}\) as the set of all learning algorithms satisfying \(\epsilon\)-label DP (Definition 2). Then_

\[\inf_{\mathcal{A}\in\mathcal{A}_{\epsilon}(f,\eta)\in\mathcal{F}_{cls}}(R_{cls }-R_{cls}^{*})\gtrsim N^{-\frac{\beta(\gamma+1)}{2\beta+d}}+(\epsilon N)^{- \frac{\beta(\gamma+1)}{\beta+d}}.\] (16)

Proof.: (Outline) Lower bounds under central DP are usually constructed by packing method [47], which works for fixed output dimensions. However, to achieve a desirable bias and variance tradeoff, the model complexity needs to increase with \(N\). In our proof, we still divide the support into \(G\) bins and construct two hypotheses for each bin, but we develop a new tool (see Lemma 1) to give a lower bound of the minimum error of hypothesis testing. We then use the group privacy property [4] to get the overall lower bound. The details can be found in Appendix D. 

_2) Upper bound._ Now we show that (16) is achievable. Similar to the local label DP problem, now divide the support into \(G\) bins, such that the length of each bin is \(h\). Now the classification within the \(l\)-th bin follows a exponential mechanism [56]:

\[\text{P}(c_{l}=j|\mathbf{X}_{1:N},Y_{1:N})=\frac{e^{\epsilon n_{lj}/2}}{\sum_{ k=1}^{K}e^{\epsilon n_{lk}/2}},\] (17)

in which \(n_{lj}=\sum_{i=1}^{N}\mathbf{1}(\mathbf{X}_{i}\in B_{l},Y_{i}=j)\). Then let \(c(\mathbf{x})=c_{l}\) for \(\mathbf{x}\in B_{l}\). The excess risk is bounded in the next theorem.

**Theorem 4**.: _The privacy mechanism (17) is \(\epsilon\)-label DP. Moreover, under Assumption 1, if \(h\) scales as \(h\sim(\ln K/\epsilon N)^{\frac{\beta}{\beta+d}}+(\ln K/N)^{\frac{\beta}{2\beta +d}}\), then the excess risk can be bounded as follows:_

\[R-R^{*}\lesssim\left(\frac{N}{\ln K}\right)^{-\frac{\beta(\gamma+1)}{2\beta+d }}+\left(\frac{\epsilon N}{\ln K}\right)^{-\frac{\beta(\gamma+1)}{\beta+d}}.\] (18)

Proof.: (Outline) The privacy guarantee of the exponential mechanism has been analyzed in [4]. Following these existing analyses, it can be shown that (17) is \(\epsilon\)-label DP. It remains to show (18). Note that if \(\eta^{*}(\mathbf{x})-\eta_{s}(\mathbf{x})\) is large, then the difference between the largest and the second largest one from \(\{n_{lj}|j=1,\ldots,K\}\) will also be large. From (17), the following inequality holds with high probability: \(c_{l}=\arg\max_{j}n_{lj}=\arg\max_{j}\eta_{j}(\mathbf{x})=c^{*}(\mathbf{x})\), which means that the classifier makesoptimal prediction. Hence we mainly consider the case with small \(\eta^{*}(\mathbf{x})-\eta_{s}(\mathbf{x})\). The details of the proof can be found in Appendix E. 

The upper and lower bounds match up to logarithmic factors. In (18), the first term is just the non-private convergence rate, while the second term \((\epsilon N)^{-\frac{\beta(\gamma+1)}{\beta+d}}\) can be regarded as the additional risk caused by the privacy mechanism. It decays faster with \(N\) compared with the first term, thus the additional performance loss caused by the privacy mechanism becomes negligible as \(N\) increases. This result is crucially different from the local model, under which the privacy mechanism always induces higher sample complexity by a factor of \(O(1/(\epsilon^{2}\wedge 1))\).

## 5 Regression with Bounded Noise

Now we analyze the theoretical limits of regression problems under local and central label DP. Throughout this section, we assume that the label is restricted within a bounded interval.

**Assumption 2**.: _Given any \(\mathbf{x}\in\mathcal{X}\), \(P(|Y|<T|\mathbf{X}=\mathbf{x})=1\)._

Assumption 1 remains the same here. In the remainder of this section, denote \(\mathcal{F}_{reg1}\) as the set of \((f,\eta)\) that satisfies Assumption 1 and 2.

### Local Label DP

_1) Lower bound._ Theorem 5 shows the minimax lower bound.

**Theorem 5**.: _Denote \(\mathcal{M}_{\epsilon}\) as the set of all privacy mechanisms satisfying \(\epsilon\)-label DP. Then_

\[\inf_{\hat{\eta}}\inf_{M\in\mathcal{M}_{\epsilon}(f,\eta)\in \mathcal{F}_{reg1}}(R_{reg}-R_{reg}^{*})\gtrsim(N(\epsilon^{2}\wedge 1))^{- \frac{2\beta}{d+2\beta}}.\] (19)

The proof of Theorem 5 is similar to that of Theorem 1, except for some details in hypotheses construction and the final bound of excess risk. The details are shown in Appendix F.

_2) Upper bound._ The privacy mechanism is \(Z=Y+W\), in which \(W\sim\mathrm{Lap}(2T/\epsilon)\). Then the privacy mechanism satisfies \(\epsilon\)-label DP. In this case, the real regression function \(\eta(\mathbf{x})\) can be estimated using the nearest neighbor approach. Let

\[\hat{\eta}(\mathbf{x})=\frac{1}{k}\sum_{i\in\mathcal{N}_{k}( \mathbf{x})}Z_{i},\] (20)

in which \(\mathcal{N}_{k}(\mathbf{x})\) is the set of \(k\) nearest neighbors of \(\mathbf{x}\) among \(\mathbf{X}_{1},\ldots,\mathbf{X}_{N}\).

**Theorem 6**.: _The method described above is \(\epsilon\)-local label DP. Moreover, with \(k\sim N^{\frac{2\beta}{d+2\beta}}(\epsilon\wedge 1)^{-\frac{2d}{d+2\beta}}\), then under Assumption 1 and 2,_

\[R_{reg}-R_{reg}^{*}\lesssim(N(\epsilon^{2}\wedge 1))^{-\frac{2\beta}{d+2 \beta}}.\] (21)

Proof.: (Outline) Since \(|Y|<T\), \(W\sim\mathrm{Lap}(2T/\epsilon)\), it is obvious that \(Z=Y+W\) is \(\epsilon\)-local label DP. For the performance (21), the bias can be bounded by the \(k\) nearest neighbor distances based on Assumption 1(a). The variance of \(\hat{\eta}(\mathbf{x})\) scales inversely with \(k\). An appropriate \(k\) can be selected to achieve a good tradeoff between bias and variance. The details are shown in Appendix G. 

From standard minimax analysis on regression problems, the non-private convergence rate is \(N^{-2\beta/(d+2\beta)}\). From Theorem 5 and 6, the privatization process makes sample complexity larger by a \(O(1/\epsilon^{2})\) factor.

### Central Label DP

_1) Lower bound._ The following theorem shows the minimax lower bound.

**Theorem 7**.: _Let \(\mathcal{A}_{\epsilon}\) be the set of all algorithms satisfying \(\epsilon\)-central DP. Then_

\[\inf_{\mathcal{A}\in\mathcal{A}_{\epsilon}(f,\eta)\in\mathcal{F} _{reg1}}(R_{reg}-R_{reg}^{*})\gtrsim N^{-\frac{2\beta}{2\beta+d}}+(\epsilon N )^{-\frac{2\beta}{d+\beta}}.\] (22)2) Upper bound.: For each bin \(B_{l}\), let \(n_{l}=\sum_{i=1}^{N}\mathbf{1}(\mathbf{X}_{i}\in B_{l})\) be the number of samples in \(B_{l}\). If \(n_{l}>0\), then

\[\hat{\eta}_{l}=\frac{1}{n_{l}}\sum_{i=1}^{N}\mathbf{1}(\mathbf{X}_{i}\in B_{l})Y _{i}+W_{l},\] (23)

in which \(W_{l}\sim\mathrm{Lap}(2/(n_{l}\epsilon))\). If \(n_{l}=0\), i.e. no sample falls in \(B_{l}\), then just let \(\hat{\eta}_{l}=0\). For all \(\mathbf{x}\in B_{l}\), let \(\hat{\eta}(\mathbf{x})=\hat{\eta}_{l}\). The excess risk can be bounded with the following theorem.

**Theorem 8**.: (23) _is \(\epsilon\)-label DP. Moreover, under Assumption 1 and 2, if \(h\) scales as \(h\sim N^{-\frac{1}{2\beta+d}}+(\epsilon N)^{-\frac{1}{d+\beta}}\), then the excess risk is bounded by_

\[R-R^{*}\lesssim N^{-\frac{2\beta}{2\beta+d}}+(\epsilon N)^{-\frac{2\beta}{d+ \beta}}.\] (24)

The upper and lower bounds match, indicating that the results are tight. Again, the second term in (24) converges faster than the first one with respect to \(N\), the performance loss caused by privacy constraints becomes negligible as \(N\) increases.

## 6 Regression with Heavy-tailed Noise

In this section, we consider the case such that the noise has tails. We make the following assumption.

**Assumption 3**.: _For all \(\mathbf{x}\in\mathcal{X}\), \(\mathbb{E}[|Y|^{p}|\mathbf{X}=\mathbf{x}]\leq M_{p}\) for some \(p\geq 2\)._

Instead of requiring \(|Y|<T\) for some \(T\), now we only assume that the \(p\)-th order moment is bounded. For non-private cases, given fixed noise variance, the tail does not affect the mean squared error of regression. As a result, as long as \(p\geq 2\), the convergence rate of regression risk is the same as the case with bounded noise. However, the label DP requires the output to be insensitive to the worst case replacement of labels, which can be harder if the noise has tails. To achieve \(\epsilon\)-DP, the clipping radius decreases with \(\epsilon\), thus the noise strength needs to grow faster than \(O(1/\epsilon)\). As a result, the convergence rate becomes slower than the non-private case. In the remainder of this section, denote \(\mathcal{F}_{reg2}\) as the set of \((f,\eta)\) that satisfies Assumption 1 and 3.

### Local Label DP

_1) Lower bound._ In earlier sections about classification and regression with bounded noise, the impact of privacy mechanisms is only a polynomial factor on \(\epsilon\), while the convergence rate of excess risk with respect to \(N\) is not changed. However, this rule no longer holds when the noise has heavy tails.

**Theorem 9**.: _Denote \(\mathcal{M}_{\epsilon}\) as the set of all privacy mechanisms satisfying \(\epsilon\)-label DP. Then for small \(\epsilon\),_

\[\inf_{\hat{\eta}}\inf_{M\in\mathcal{M}_{\epsilon}(f,\eta)\in\mathcal{F}}(R_{ reg}-R_{reg}^{*})\gtrsim(N(e^{\epsilon}-1)^{2})^{-\frac{2\beta(p-1)}{2p \beta+d(p-1)}}+N^{-\frac{2\beta}{2\beta+d}}.\] (25)

_2) Upper bound._ Since now the noise has unbounded distribution, without preprocessing, the sensitivity is unbounded, thus simply adding noise to \(Y\) can no longer protect the privacy. Therefore, a solution is to clip \(Y\) into \([-T,T]\), and add noise proportional to \(T/\epsilon\) to achieve \(\epsilon\)-local label DP. Such truncation will inevitably introduce some bias. To achieve a tradeoff between clipping bias and sensitivity, the value of \(T\) needs to be tuned carefully. Based on such intuition, the method is precisely stated as follows. Let \(Z_{i}=Y_{Ti}+W_{i}\), in which \(Y_{Ti}\) is the truncation of \(Y_{i}\), i.e. \(Y_{Ti}=(Y_{i}\wedge T)\vee(-T)\), and \(W\sim\mathrm{Lap}(2T/\epsilon)\). The result is shown in the next theorem.

**Theorem 10**.: _The method above is \(\epsilon\)-local label DP. Moreover, with \(k\sim(N\epsilon^{2})^{\frac{2\beta}{2p\beta+d(p-1)}}\lor N^{\frac{2\beta}{2 \beta+d}}\), and \(T\sim(k\epsilon^{2})^{\frac{1}{2p\beta}}\), the risk is bounded by_

\[R_{reg}-R_{reg}^{*}\lesssim(N\epsilon^{2})^{-\frac{2\beta(p-1)}{2p\beta+d(p-1) }}+N^{-\frac{2\beta}{2\beta+d}}.\] (26)

Proof.: (Outline) It can be shown that the clipping bias scales as \(T^{2(1-p)}\). To meet the \(\epsilon\)-label DP, an additional error that scales as \(T/\epsilon\) is needed. By averaging over \(k\) nearest neighbors, the variance caused by noise \(W\) scales with \(T^{2}/(k\epsilon^{2})\). From standard analysis on nearest neighbor methods [29], the non-private mean squared error scales as \(1/k+(k/N)^{2\beta/d}\). Put all these terms together, Theorem 10 can be proved. Details can be found in Appendix K.

With the limit of \(p\rightarrow\infty\), the problem reduces to the case with bounded noise, and the growth rate of \(k\) and the convergence rate of risk are the same as those in Theorem 6. For finite \(p\), \(2\beta(p-1)/(2p\beta+d(p-1))<2\beta/(2\beta+d)\), thus the convergence rate becomes slower due to the privacy mechanism.

### Central Label DP

_1) Lower bound._ The minimax lower bound is shown in Theorem 11.

**Theorem 11**.: _The minimax lower bound is_

\[\inf_{\mathcal{A}\in\mathcal{A}_{*}(f,\eta)\in\mathcal{F}_{reg2}}(R_{reg}-R_{ reg}^{*})\gtrsim N^{-\frac{2\beta}{2\beta+d}}+\left(\epsilon N\right)^{-\frac{2 \beta(p-1)}{p\beta+d(p-1)}}\] (27)

_2) Upper bound._ Now we derive the upper bound. To restrict the sensitivity, instead of estimating with (23) directly, now we calculate an average of clipped label values:

\[\hat{\eta}_{l}=\frac{1}{n_{l}}\sum_{i=1}^{N}\mathbf{1}(\mathbf{X}_{i}\in B_{l} )\operatorname{Clip}(Y_{i},T)+W_{l},\] (28)

in which \(W_{l}\sim\operatorname{Lap}(2T/(n_{l}\epsilon))\). Then for all \(\mathbf{x}\in B_{l}\), let \(\hat{\eta}(\mathbf{x})=\hat{\eta}_{l}\). The following theorem bounds the excess risk.

**Theorem 12**.: (28) _is \(\epsilon\)-label DP. Moreover, under Assumption 1 and 3, if \(h\) and \(T\) scales as \(h\sim N^{-\frac{2\beta}{2\beta+d}}+\left(\epsilon N\right)^{-\frac{2\beta}{p \beta+d(p-1)}}\), and \(T\sim(\epsilon Nh^{d})^{1/p}\), then the excess risk can be bounded by_

\[R_{reg}-R_{reg}^{*}\lesssim N^{-\frac{2\beta}{2\beta+d}}+\left(\epsilon N \right)^{-\frac{2\beta(p-1)}{p\beta+d(p-1)}}.\] (29)

The proof of Theorem 11 and 12 follow that of Theorem 7 and 8. The details are shown in Appendix L and M respectively. With \(p=2\), the right hand side of (29) becomes \(\left(\epsilon\wedge 1\right)^{-\frac{2\beta}{2\beta+d}}\), indicating that the privacy constraint blows up the sample complexity by a constant factor. With larger \(p\), the second term in (29) becomes negligible compared with the first one.

The theoretical analyses in this section are summarized as follows. In general, with fixed noise variance, if the label noise is heavy-tailed, while the non-private convergence rates remain unaffected, the additional risk caused by privacy mechanisms becomes significantly higher, indicating the difficulty of privacy protection for heavy-tailed distributions.

## 7 Conclusion

In this paper, we have derived the minimax lower bounds of learning under label DP for both central and local models. Furthermore, we propose methods whose upper bounds match these lower bounds. The results indicate the theoretical limits of learning under the label DP. From these results, it is discovered that under local label DP constraints, the sample complexity blows up by a factor of at least \(O(1/\epsilon^{2})\). Under central label DP requirements, the additional error caused by privacy mechanisms is significantly smaller. Finally, it is shown that for regression problem with heavy-tailed label distribution, the additional risk induced by privacy requirement becomes inevitably higher.

**Limitations:** The limitations of our work include the following aspects. Some assumptions can be weakened. For example, current analysis assumes that feature distributions have bounded supports, which may be extended to the unbounded case. One can let the bin splitting and nearest neighbor method be adaptive in the tails of features, such as [41]. Moreover, the bounds derived in this paper require that samples increase exponentially with dimensionality. However, in practice, the performance of learning under the label DP can be quite well even in high dimensions. The discrepancy can be explained by the fact that the minimax lower bound considers the worst-case distribution over a wide range of distributions. However, in most realistic cases, the distributions satisfy significantly better properties. A better modeling is to assume that these samples lie on a low dimensional manifold [57, 58]. In this case, it is possible to achieve a much better convergence rate. Finally, it is not sure whether approximate DP (i.e. \(\left(\epsilon,\delta\right)\)-DP) can improve the convergence rates.

## References

* [1]R. Bao, J. Zhang, D. Wu, et al. (2024) Privacy inference attack and defense in centralized and federated learning: a comprehensive survey. IEEE Transactions on Artificial Intelligence. Cited by: SS1.
* [2]C. Dwork, F. F. McSherry, K. Nissim, et al. (2006) Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pp. 265-284. Cited by: SS1.
* [3]M. Abadi, A. Chu, I. Goodfellow, et al. (2016) Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pp. 308-318. Cited by: SS1.
* [4]C. Dwork, A. Roth, et al. (2014) The algorithmic foundations of differential privacy. Foundations and Trends(r) in Theoretical Computer Science9 (3-4), pp. 211-407. Cited by: SS1.
* [5]R. Bassily, A. Smith, A. Thakurta (2014) Private empirical risk minimization: efficient algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of Computer Science, pp. 464-473. Cited by: SS1.
* [6]R. Bassily, V. Feldman, K. Talwar, et al. (2019) Private stochastic convex optimization with optimal rates. Advances in Neural Information Processing Systems32. Cited by: SS1.
* [7]D. Wang, H. Xiao, S. Devadas, et al. (2020) On differentially private stochastic convex optimization with heavy-tailed data. In International Conference on Machine Learning, pp. 10081-10091. Cited by: SS1.
* [8]H. Tsai, V. Feldman, T. Koren, et al. (2021) Private stochastic convex optimization: optimal rates in l1 geometry. In International Conference on Machine Learning, pp. 393-403. Cited by: SS1.
* [9]R. Das, S. Kale, Z. Xu, et al. (2023) Beyond uniform lipschitz condition in differentially private optimization. In International Conference on Machine Learning, pp. 7066-7101. Cited by: SS1.
* [10]F. Tramer and B. Boneh (2021) Differentially private learning needs better features (or much more data). In International Conference on Learning Representations, Cited by: SS1.
* [11]Z. Bu, J. Mao, S. Xu (2022) Scalable and efficient training of large convolutional neural networks with differential privacy. Advances in Neural Information Processing Systems35, pp. 38305-38318. Cited by: SS1.
* [12]S. De, L. Berrada, J. Hayes, et al. (2022) Unlocking high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650. Cited by: SS1.
* [13]J. Wei, X. Bao, X. Xiao, et al. (2022) Dpis: an enhanced mechanism for differentially private sgd with importance sampling. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, pp. 2885-2899. Cited by: SS1.
* [14]B. Ghazi, N. Golowich, R. Kumar, et al. (2021) Deep learning with label differential privacy. Advances in Neural Information Processing Systems34, pp. 27131-27145. Cited by: SS1.
* [15]H. B. McMahan, G. Holt, D. Sculley, et al. (2013) Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1222-1230. Cited by: SS1.
* [16]F. McSherry and I. Mironov (2009) Differentially private recommender systems: building privacy into the netlix prize contenders. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 627-636. Cited by: SS1.
* [17]A. Russone, A. B. Kasadha, S. Stumpf, et al. (2020) Trust, identity, privacy, and security considerations for designing a peer data sharing platform between people living with hiv. Proceedings of the ACM on Human-Computer Interaction4 (CSCW2), pp. 1-27. Cited by: SS1.
* [18]B. Ghazi, R. Kamath, et al. (2022) Regression with label differential privacy. In The Eleventh International Conference on Learning Representations, Cited by: SS1.
* [19]M. Malek Esmaeili, K. Mironov, et al. (2021) Antipodes of label differential privacy: rate and alibi. Advances in Neural Information Processing Systems34, pp. 6934-6945. Cited by: SS1.
* [20]H. Esfandiari, V. Mirrokni, et al. (2022) Label differential privacy via clustering. In International Conference on Artificial Intelligence and Statistics, pp. 7055-7075. Cited by: SS1.

* Tang et al. [2022] Tang, X., M. Nasr, S. Mahloujifar, et al. Machine learning with differentially private labels: Mechanisms and frameworks. _Proceedings on Privacy Enhancing Technologies_, 2022.
* Cover [1999] Cover, T. M. _Elements of information theory_. John Wiley & Sons, 1999.
* Duchi et al. [2013] Duchi, J. C., M. I. Jordan, M. J. Wainwright. Local privacy and statistical minimax rates. In _2013 IEEE 54th Annual Symposium on Foundations of Computer Science_, pages 429-438. IEEE, 2013.
* [24] --. Minimax optimal procedures for locally private estimation. _Journal of the American Statistical Association_, 113(521):182-201, 2018.
* Gopi et al. [2020] Gopi, S., G. Kamath, J. Kulkarni, et al. Locally private hypothesis selection. In _Conference on Learning Theory_, pages 1785-1816. PMLR, 2020.
* Berrett [2019] Berrett, T., C. Butucea. Classification under local differential privacy. _arXiv preprint arXiv:1912.04629_, 2019.
* Berrett et al. [2021] Berrett, T. B., L. Gyorfi, H. Walk. Strongly universally consistent nonparametric regression and classification with privatised data. _Electronic Journal of Statistics_, 15:2430-2453, 2021.
* Tsybakov [2009] Tsybakov, A. B. _Introduction to Nonparametric Estimation_. 2009.
* Audibert and Tsybakov [2007] Audibert, J.-Y., A. B. Tsybakov. Fast learning rates for plug-in classifiers. _Annals of Statistics_, 2007.
* Warner [1965] Warner, S. L. Randomized response: A survey technique for eliminating evasive answer bias. _Journal of the American Statistical Association_, 60(309):63-69, 1965.
* Badanidiyuru Varadaraja et al. [2023] Badanidiyuru Varadaraja, A., B. Ghazi, P. Kamath, et al. Optimal unbiased randomizers for regression with label differential privacy. _Advances in Neural Information Processing Systems_, 36, 2023.
* LeCam [1973] LeCam, L. Convergence of estimates under dimensionality restrictions. _The Annals of Statistics_, pages 38-53, 1973.
* Verdu et al. [1994] Verdu, S., et al. Generalizing the fano inequality. _IEEE Transactions on Information Theory_, 40(4):1247-1251, 1994.
* Assouad [1983] Assouad, P. Deux remarques sur l'estimation. _Comptes rendus des seances de l'Academie des sciences. Serie 1, Mathematique_, 296(23):1021-1024, 1983.
* Yang [1999] Yang, Y. Minimax nonparametric classification. i. rates of convergence. _IEEE Transactions on Information Theory_, 45(7):2271-2284, 1999.
* [36] --. Minimax nonparametric classification. ii. model selection for adaptation. _IEEE Transactions on Information Theory_, 45(7):2285-2292, 1999.
* Chaudhuri and Dasgupta [2014] Chaudhuri, K., S. Dasgupta. Rates of convergence for nearest neighbor classification. _Advances in Neural Information Processing Systems_, 27, 2014.
* Yang and Tokdar [2015] Yang, Y., S. T. Tokdar. Minimax-optimal nonparametric regression in high dimensions. _The Annals of Statistics_, pages 652-674, 2015.
* Doring et al. [2018] Doring, M., L. Gyorfi, H. Walk. Rate of convergence of \(k\)-nearest-neighbor classification rule. _Journal of Machine Learning Research_, 18(227):1-16, 2018.
* Gadat et al. [2016] Gadat, S., T. Klein, C. Marteau. Classification in general finite dimensional spaces with the k-nearest neighbor rule. _Annals of Statistics_, 2016.
* Zhao and Lai [2021] Zhao, P., L. Lai. Minimax rate optimal adaptive nearest neighbor classification and regression. _IEEE Transactions on Information Theory_, 67(5):3155-3182, 2021.
* Kasiviswanathan et al. [2011] Kasiviswanathan, S. P., H. K. Lee, K. Nissim, et al. What can we learn privately? _SIAM Journal on Computing_, 40(3):793-826, 2011.
* Li et al. [2023] Li, M., T. B. Berrett, Y. Yu. On robustness and local differential privacy. _The Annals of Statistics_, 51(2):717-737, 2023.
* Feldman et al. [2020] Feldman, V., T. Koren, K. Talwar. Private stochastic convex optimization: optimal rates in linear time. In _Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing_, pages 439-449. 2020.
* Duchi and Rogers [2019] Duchi, J., R. Rogers. Lower bounds for locally private estimation via communication complexity. In _Conference on Learning Theory_, pages 1161-1191. PMLR, 2019.

* [46] Huang, Z., Y. Liang, K. Yi. Instance-optimal mean estimation under differential privacy. _Advances in Neural Information Processing Systems_, 34:25993-26004, 2021.
* [47] Hardt, M., K. Talwar. On the geometry of differential privacy. In _Proceedings of the forty-second ACM symposium on Theory of computing_, pages 705-714. 2010.
* [48] Bun, M., G. Kamath, T. Steinke, et al. Private hypothesis selection. _Advances in Neural Information Processing Systems_, 32, 2019.
* [49] Narayanan, S. Better and simpler lower bounds for differentially private statistical estimation. _arXiv preprint arXiv:2310.06289_, 2023.
* [50] Kamath, G., V. Singhal, J. Ullman. Private mean estimation of heavy-tailed distributions. In _Conference on Learning Theory_, pages 2204-2235. PMLR, 2020.
* [51] Kamath, G., J. Li, V. Singhal, et al. Privately learning high-dimensional distributions. In _Conference on Learning Theory_, pages 1853-1902. PMLR, 2019.
* [52] Alabi, D., P. K. Kothari, P. Tankala, et al. Privately estimating a gaussian: Efficient, robust, and optimal. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, pages 483-496. 2023.
* [53] Arbas, J., H. Ashtiani, C. Liaw. Polynomial time and private learning of unbounded gaussian mixture models. In _International Conference on Machine Learning_, pages 1018-1040. 2023.
* [54] Bun, M., J. Ullman, S. Vadhan. Fingerprinting codes and the price of approximate differential privacy. In _Proceedings of the forty-sixth annual ACM symposium on Theory of computing_, pages 1-10. 2014.
* [55] Kamath, G., A. Mouzakis, V. Singhal. New lower bounds for private estimation and a generalized fingerprinting lemma. _Advances in neural information processing systems_, 35:24405-24418, 2022.
* [56] McSherry, F., K. Talwar. Mechanism design via differential privacy. In _48th Annual IEEE Symposium on Foundations of Computer Science (FOCS'07)_, pages 94-103. IEEE, 2007.
* [57] Kpotufe, S. k-nn regression adapts to local intrinsic dimension. _Advances in neural information processing systems_, 24, 2011.
* [58] Carter, K. M., R. Raich, A. O. Hero III. On local intrinsic dimension estimation and its applications. _IEEE Transactions on Signal Processing_, 58(2):650-663, 2009.

Proof of Proposition 2

From (5) and (6), the Bayes risk is

\[R^{*}_{cls}=\text{P}(Y\neq c^{*}(\mathbf{X}))=\int\text{P}(Y\neq c^{*}(\mathbf{ x})|\mathbf{X}=\mathbf{x})f(\mathbf{x})d\mathbf{x}=\int(1-\eta^{*}(\mathbf{x}))f( \mathbf{x})d\mathbf{x}.\] (30)

The risk of classifier \(c\) is

\[R_{cls}=\text{P}(Y\neq c(\mathbf{X}))=\mathbb{E}\left[\int\left(1-\eta_{c( \mathbf{x})}(\mathbf{x})\right)f(\mathbf{x})d\mathbf{x}\right].\] (31)

From (31) and (6),

\[R_{cls}-R^{*}_{cls}=\int(\eta^{*}(\mathbf{x})-\mathbb{E}[\eta_{c(\mathbf{x})} (\mathbf{x})])f(\mathbf{x})d\mathbf{x}.\] (32)

The proof is complete.

## Appendix B Proof of Theorem 1

In this section, we prove the minimax lower bound of multi-class classification. The problem with \(K\) classes with \(K>2\) is inherently harder than that with \(K=2\). Therefore, we just need to prove the lower bound for binary classification, in which \(\mathcal{Y}=\{1,2\}\). Let

\[\eta(\mathbf{x})=\eta_{2}(\mathbf{x})-\eta_{1}(\mathbf{x}).\] (33)

Since \(\eta_{1}(\mathbf{x})+\eta_{2}(\mathbf{x})=1\) always holds, we have

\[\eta_{1}(\mathbf{x})=\frac{1-\eta(\mathbf{x})}{2},\eta_{2}(\mathbf{x})=\frac{ 1+\eta(\mathbf{x})}{2}.\] (34)

Therefore, \(\eta(\mathbf{x})\) captures the conditional distribution of \(Y\) given \(\mathbf{x}\).

Find \(G\) disjoint cubes \(B_{1},\ldots,B_{G}\subset\mathcal{X}\), such that the length of each cube is \(h\). Denote \(\mathbf{c}_{1},\ldots,\mathbf{c}_{G}\) as the centers of these cubes. Let \(\phi(\mathbf{u})\) be some function supported at \([-1/2,1/2]^{d}\), such that

\[0\leq\phi(\mathbf{u})\leq 1.\] (35)

Let \(f(\mathbf{x})=c\) over \(\mathbf{x}\in\mathcal{X}\). For \(\mathbf{v}\in\mathcal{V}:=\{-1,1\}^{m}\), let

\[\eta_{\mathbf{v}}(\mathbf{x})=\sum_{k=1}^{m}v_{k}\phi\left(\frac{\mathbf{x}- \mathbf{c}_{k}}{h}\right)h^{\beta}.\] (36)

It can be proved that if for some constant \(C_{M}\),

\[m\leq C_{M}h^{\gamma\beta-d},\] (37)

then for any \(\eta=\eta_{\mathbf{v}}\), \(\eta_{1}\) and \(\eta_{2}\) satisfies Assumption 1(b). Denote

\[\hat{v}_{k}=\operatorname*{arg\,max}_{s\in\{-1,1\}}\;\int_{B_{k}}\phi\left( \frac{\mathbf{x}-\mathbf{c}_{k}}{h}\right)\mathbf{1}(\operatorname*{sign}( \hat{\eta}(\mathbf{x}))=s)f(\mathbf{x})d\mathbf{x}.\] (38)

Then the excess risk is bounded by

\[R-R^{*} = \int|\eta_{\mathbf{v}}(\mathbf{x})|\text{P}(\operatorname*{sign} (\hat{\eta}(\mathbf{x}))\neq\operatorname*{sign}(\eta_{\mathbf{v}}(\mathbf{x} )))f(\mathbf{x})d\mathbf{x}\] (39) \[\geq \sum_{k=1}^{m}\int_{B_{k}}|\eta_{\mathbf{v}}(\mathbf{x})|\text{P} (\operatorname*{sign}(\hat{\eta}(\mathbf{x}))\neq\operatorname*{sign}(\eta_{ \mathbf{v}}(\mathbf{x})))f(\mathbf{x})d\mathbf{x}\] \[= \sum_{k=1}^{m}h^{\beta}\int_{B_{k}}\phi\left(\frac{\mathbf{x}- \mathbf{c}_{k}}{h}\right)\text{P}(\operatorname*{sign}(\hat{\eta}(\mathbf{x} )))f(\mathbf{x})d\mathbf{x}.\]

If \(\hat{v}_{k}\neq v_{k}\), then from (38),

\[\int_{B_{k}}\phi\left(\frac{\mathbf{x}-\mathbf{c}_{k}}{h}\right)\mathbf{1}( \operatorname*{sign}(\hat{\eta}(\mathbf{x})))f(\mathbf{x})d\mathbf{x}\geq\int _{B_{k}}\phi\left(\frac{\mathbf{x}-\mathbf{c}_{k}}{h}\right)\mathbf{1}( \operatorname*{sign}(\hat{\eta}(\mathbf{x}))=v_{k})f(\mathbf{x})d\mathbf{x}.\] (40)

[MISSING_PAGE_EMPTY:14]

From (46) and (47),

\[\delta\leq N\left[(e^{\epsilon}-1)^{2}\wedge 3\right]h^{2\beta+d}\left\|\phi \right\|_{2}^{2}.\] (48)

Let

\[h\sim\left(N\left(\epsilon^{2}\wedge 1\right)\right)^{-\frac{1}{2\beta+d}}.\] (49)

Then \(\delta\lesssim 1\). From (45), with \(m\sim h^{\gamma\beta-d}\),

\[\inf_{\hat{\mathbf{v}}}\inf_{M\in\mathcal{M}_{\epsilon}}\max_{\mathbf{v}\in \mathcal{V}}\hskip-1.422638pt\mathbb{E}[\rho_{H}(\hat{\mathbf{v}},\mathbf{v})] \gtrsim h^{\gamma\beta-d}.\] (50)

Hence

\[\inf_{\hat{Y}}\inf_{M\in\mathcal{M}_{\epsilon}(f,\eta)\in\mathcal{P}}(R-R^{*}) \gtrsim h^{\beta+d}h^{\gamma\beta-d}\sim h^{\beta(\gamma+1)}\sim\left[N\left( \epsilon^{2}\wedge 1\right)\right]^{-\frac{\beta(\gamma+1)}{2\beta+4}}.\] (51)

The proof is complete.

## Appendix C Proof of Theorem 2

Denote

\[n_{l}=\sum_{i=1}^{N}\mathbf{1}(\mathbf{X}_{i}\in B_{l}),\] (52)

and for \(\mathbf{Z}=M(\mathbf{X},Y)\), let

\[\tilde{\eta}_{j}(\mathbf{x}) := \mathbb{E}[\mathbf{Z}(j)|\mathbf{X}=\mathbf{x}]\] (53) \[= \frac{e^{\frac{\epsilon}{2}}}{e^{\frac{\epsilon}{2}}+1}\eta_{j}( \mathbf{x})+\frac{1}{e^{\frac{\epsilon}{2}}+1}(1-\eta_{j}(\mathbf{x}))\]

as the number of training samples whose feature vectors fall in \(B_{l}\), and

\[v_{lj}:=\frac{1}{n_{l}}\sum_{i:\mathbf{X}_{i}\in B_{l}}\tilde{\eta}_{j}( \mathbf{X}_{i}).\] (54)

Recall (12) that defines \(S_{lj}\). From Hoeffding's inequality,

\[\text{P}\left(|S_{lj}-n_{l}v_{lj}|>t|\mathbf{X}_{1:N}\right)\leq 2\exp\left[- \frac{2t^{2}}{n_{l}}\right],\] (55)

in which \(\mathbf{X}_{1:N}\) denotes \(\mathbf{X}_{1},\ldots,\mathbf{X}_{N}\).

Define

\[v_{l}^{*}:=\max_{j}v_{lj},\] (56)

and

\[c_{l}^{*}:=\arg\max_{j}\,v_{lj}.\] (57)

Now we bound \(\text{P}(v_{l}^{*}-v_{lc_{l}}>t)\), in which \(c_{l}\) is defined in (13). \(c_{l}\) can be viewed as the prediction at the \(l\)-th bin. We would like to show that the even if the prediction is wrong, the value (i.e. conditional probability) of the predicted class is close to the ground truth. \(v_{l}^{*}-v_{lc_{l}}>t\) only if \(\exists j\), \(v_{l}^{*}-v_{lj}>t\), and \(S_{lj}>S_{lc_{l}^{*}}\). Therefore either \(S_{lj}-n_{l}v_{lj}>t/2\) or \(S_{lc_{l}^{*}}-n_{l}v_{l}^{*}>t/2\) holds. Hence

\[\text{P}\left(v_{l}^{*}-v_{lc_{l}}\geq t\right)\leq\text{P}\left(\exists j,|S _{lj}-n_{l}v_{lj}|\geq\frac{1}{2}n_{l}t\right)\leq 2K\exp\left(-\frac{1}{2}n_{l }t^{2}\right).\] (58)

Define

\[t_{0}=\sqrt{\frac{2\ln(2K)}{n_{l}}}.\] (59)Then

\[v_{l}^{*}-\mathbb{E}[v_{lc_{l}}|\mathbf{X}_{1:N}] = \int_{0}^{1}\mathsf{P}(v_{l}^{*}-v_{lc_{l}}>t)dt\] (60) \[\leq t_{0}+\int_{t_{0}}^{\infty}2K\exp\left(-\frac{1}{2}n_{l}t^{2} \right)dt\] \[\stackrel{{(a)}}{{\leq}} t_{0}+2\sqrt{\frac{2\pi}{n_{l}}}K\exp\left(-\frac{1}{2}n_{l}t_{0}^{2}\right)\] \[= \sqrt{\frac{2\ln(2K)}{n_{l}}}+\sqrt{\frac{2\pi}{n_{l}}}\] \[\leq 3\sqrt{\frac{\ln(2K)}{n_{l}}}.\]

In (a), we use the inequality

\[\int_{t}^{\infty}e^{-\frac{\pi^{2}}{2\sigma^{2}}}du\leq\sqrt{2\pi}\sigma e^{- \frac{\sigma^{2}}{2\sigma^{2}}}.\] (61)

Now we bound the excess risk.

\[R-R^{*} = \int\left(\eta^{*}(\mathbf{x})-\mathbb{E}[\eta_{lc(\mathbf{x})} (\mathbf{x})]\right)f(\mathbf{x})d\mathbf{x}\] (62) \[= \sum_{l=1}^{G}\int_{B_{l}}\left(\eta^{*}(\mathbf{x})-\mathbb{E}[ \eta_{c(\mathbf{x})}(\mathbf{x})]\right)f(\mathbf{x})d\mathbf{x}.\]

We need to bound \(\int_{B_{l}}\left(\eta^{*}(\mathbf{x})-\mathbb{E}[\eta_{c(\mathbf{x})}( \mathbf{x})]\right)f(\mathbf{x})d\mathbf{x}\) for each \(l\). From Assumption 1(a), for any \(\mathbf{x},\mathbf{x}^{\prime}\in B_{l}\), the distance is bounded by \(\|\mathbf{x}-\mathbf{x}^{\prime}\|\leq\sqrt{d}L\). Thus

\[|\eta_{j}(\mathbf{x})-\eta_{j}(\mathbf{x}^{\prime})|\leq L_{d}h^{\beta},\] (63)

in which \(L_{d}\) is defined as \(L_{d}:=L\sqrt{d}\). From (63) and (53),

\[|\tilde{\eta}_{j}(\mathbf{x})-\tilde{\eta}_{j}(\mathbf{x}^{\prime})|\leq\frac{ e^{\frac{e}{2}}-1}{e^{\frac{e}{2}}+1}L_{d}h^{\beta}.\] (64)

Define

\[\tilde{\eta}^{*}(\mathbf{x})=\max_{j}\tilde{\eta}_{j}(\mathbf{x}),\] (65)

then

\[\eta^{*}(\mathbf{x})-\mathbb{E}[\eta_{c_{l}}(\mathbf{x})|\mathbf{ X}_{1:N}] \leq \frac{e^{\frac{e}{2}}+1}{e^{\frac{e}{2}}-1}\left(\tilde{\eta}^{*} (\mathbf{x})-\mathbb{E}[\tilde{\eta}_{c_{l}}(\mathbf{x})|\mathbf{X}_{1:N}]\right)\] (66) \[\leq \frac{e^{\frac{e}{2}}+1}{e^{\frac{e}{2}}-1}\left(v_{l}^{*}- \mathbb{E}[v_{lc_{l}}|\mathbf{X}_{1:N}]\right)+2L_{d}h^{\beta}\] \[\leq 3\frac{e^{\frac{e}{2}}+1}{e^{\frac{e}{2}}-1}\sqrt{\frac{2\ln(2K )}{n_{l}}}+2L_{d}h^{\beta}.\]

Take integration over cube \(B_{l}\), we get

\[\int_{B_{l}}\left(\eta^{*}(\mathbf{x})-\mathbb{E}[\eta_{c_{l}}( \mathbf{x})]\right)f(\mathbf{x})d\mathbf{x}\] (67) \[\leq \mathsf{P}\left(n_{l}<\frac{1}{2}Np(B_{l})\right)\int_{B_{l}} \left(\eta^{*}(\mathbf{x})-\mathbb{E}[\eta_{c_{l}}(\mathbf{x})|n_{l}<\frac{1} {N}p(B_{l})]\right)f(\mathbf{x})d\mathbf{x}\] \[+\int_{B_{l}}\left(\eta^{*}(\mathbf{x})-\mathbb{E}[\eta_{c_{l}}( \mathbf{x})|n_{l}\geq\frac{1}{N}p(B_{l})]\right)f(\mathbf{x})d\mathbf{x}\] \[\leq p(B_{l})e^{-\frac{1}{2}(1-\ln 2)Np(B_{l})}+\left[3\frac{e^{\frac{e}{2}}+1}{e ^{\frac{e}{2}}-1}\sqrt{\frac{2\ln(2K)}{Np(B_{l})}}+2L^{d}h^{\beta}\right]p(B_{ l}),\]in which \(p(B_{l})=\mbox{P}(\mathbf{X}\in B_{l})\) is the probability mass of \(B_{l}\). Moreover, define

\[\Delta_{l}=\inf_{\mathbf{x}\in B_{l}}\left(\eta^{*}(\mathbf{x})- \eta_{s}(\mathbf{x})\right),\] (68)

and

\[\tilde{\Delta}_{l}=\inf_{\mathbf{x}\in B_{l}}\left(\tilde{\eta}^{ *}(\mathbf{x})-\tilde{\eta}_{s}(\mathbf{x})\right)=\frac{e^{\frac{\xi}{2}}-1}{ e^{\frac{\xi}{2}}+1}\Delta_{l},\] (69)

in which the \(\tilde{\eta}_{s}\) is the second largest value of \(\tilde{\eta}_{j}\) among \(j=1,\ldots,K\), which follows the definition of \(\eta_{s}\).

If \(\Delta_{l}>0\), then \(c^{*}(\mathbf{x})\) is the same over \(B_{l}\). Then either \(v_{l}^{*}-v_{lc_{l}}=0\) or \(v_{l}^{*}-v_{lc_{l}}\geq\Delta_{l}\) holds. Hence

\[\tilde{\eta}^{*}(\mathbf{x})-\mathbb{E}[\tilde{\eta}_{c_{l}}( \mathbf{x})|\mathbf{X}_{1:N}]\] (70) \[= \int_{0}^{1}\mbox{P}\left(\tilde{\eta}^{*}(\mathbf{x})-\tilde{ \eta}_{c_{l}}(\mathbf{x})>t|\mathbf{X}_{1:N}\right)dt\] \[\leq \int_{0}^{1}\mbox{P}\left(v_{l}^{*}-v_{lc_{l}}>t-2L_{d}h^{\beta }\frac{e^{\frac{\xi}{2}}+1}{e^{\frac{\xi}{2}}-1}|\mathbf{X}_{1:N}\right)dt\] \[\leq \int_{0}^{\tilde{\Delta}_{l}+2L_{d}h^{\beta}}\mbox{P}(v_{l}^{*}- v_{lc_{l}}\geq\Delta_{l})dt+\int_{\tilde{\Delta}_{l}+2L_{d}h^{\beta}}^{\infty}2K \exp\left[-\frac{1}{2}n_{l}(t-2L_{d}h^{\beta})^{2}\right]dt\] \[\leq 2K\exp\left(-\frac{1}{2}n_{l}\tilde{\Delta}_{l}^{2}\right)( \tilde{\Delta}_{l}+2L_{d}h^{\beta}\frac{e^{\frac{\xi}{2}}+1}{e^{\frac{\xi}{2} }-1})+2K\sqrt{\frac{2\pi}{n_{l}}}\exp\left(-\frac{1}{2}n_{l}\tilde{\Delta}_{l}^ {2}\right)\] \[= \left[2K\left(\tilde{\Delta}_{l}+2L_{d}h^{\beta}\frac{e^{\frac{ \xi}{2}}+1}{e^{\frac{\xi}{2}}-1}\right)+2K\sqrt{\frac{2\pi}{n_{l}}}\right] \exp\left(-\frac{1}{2}n_{l}\tilde{\Delta}_{l}^{2}\right).\]

Take expectation over \(\mathbf{X}_{1:N}\), we get

\[\int_{B_{l}}(\eta^{*}(\mathbf{x})-\mathbb{E}[\eta_{c_{l}}( \mathbf{x})])f(\mathbf{x})d\mathbf{x}\leq p(B_{l})e^{-\frac{1}{2}(1-\ln 2)Np(B_{l})}\] (71) \[+2Kp(B_{l})\left(\Delta_{l}+2L_{d}h^{\beta}+\frac{e^{\frac{\xi}{2 }}+1}{e^{\frac{\xi}{2}}-1}\sqrt{\frac{2\pi}{Np(B_{l})}}\right)\exp\left[-\frac{ 1}{2}Np(B_{l})\Delta_{l}^{2}\left(\frac{e^{\frac{\xi}{2}}-1}{e^{\frac{\xi}{2}} +1}\right)^{2}\right].\]

Define

\[a_{l}=\left[3\frac{e^{\frac{\xi}{2}}+1}{e^{\frac{\xi}{2}}-1}\sqrt {\frac{2\ln(2K)}{cNh^{d}}}+2L_{d}h^{\beta}\right]p(B_{l}),\] (72)

and

\[b_{l}=2Kp(B_{l})\left(\Delta_{l}+2L_{d}h^{\beta}+\frac{e^{\frac{ \xi}{2}}+1}{e^{\frac{\xi}{2}}-1}\sqrt{\frac{2\pi}{cNh^{d}}}\right)\exp\left[- \frac{1}{2}cNh^{d}\Delta_{l}^{2}\left(\frac{e^{\frac{\xi}{2}}-1}{e^{\frac{\xi} {2}}+1}\right)^{2}\right].\] (73)

From Assumption 1(c), \(p(B_{l})\geq cNh^{d}\). Therefore, from (67) and (71)

\[R-R^{*} \leq \sum_{l=1}^{G}\left[p(B_{l})e^{-\frac{1}{2}(1-\ln 2)Np(B_{l})}+\min \{a_{l},b_{l}\}\right]\] (74) \[\leq e^{-\frac{1}{2}(1-\ln 2)cNh^{d}}+\sum_{l=1}^{G}\min\{a_{l},b_{l}\}.\]

It remains to bound \(\sum_{l=1}^{G}\min\{a_{l},b_{l}\}\). Note that for all \(\mathbf{x}\in B_{l}\), \(\eta^{*}(\mathbf{x})-\eta_{s}(\mathbf{x})\leq\Delta_{l}+2L_{d}h^{\beta}\). Thus

\[\sum_{l:\Delta_{l}\leq u}p(B_{l})\leq\mbox{P}\left(\eta^{*}(\mathbf{X})-\eta_ {s}(\mathbf{X})\leq u+2L_{d}h^{\beta}\right)\leq M(u+2L_{d}h^{\beta})^{\gamma}.\] (75)Let

\[\Delta_{0}=\frac{e^{\frac{\epsilon}{2}}+1}{e^{\frac{\epsilon}{2}}-1}\sqrt{\frac{2 \ln(2K)}{cNh^{d}}},\] (76)

and

\[I_{0} = \{l|\Delta_{l}\leq\Delta_{0}\},\] (77) \[I_{k} = \{l|2^{k-1}\Delta_{0}<\Delta_{l}\leq 2^{k}\Delta_{0}\},k=1,2,\ldots\] (78)

Then

\[\min_{l\in I_{0}}\{a_{l},b_{l}\} \leq \sum_{l\in I_{0}}a_{l}\] (79) \[\leq \left(\sum_{l:\Delta_{l}\leq\Delta_{0}}p(B_{l})\right)\left[3 \frac{e^{\frac{\epsilon}{2}}+1}{e^{\frac{\epsilon}{2}}-1}\sqrt{\frac{2\ln(2K) }{cNh^{d}}}+2L_{d}h^{\beta}\right]\] \[\leq M(\Delta_{0}+2L_{d}h^{\beta})^{\gamma}\left[3\frac{e^{\frac{ \epsilon}{2}}+1}{e^{\frac{\epsilon}{2}}-1}\sqrt{\frac{2\ln(2K)}{cNh^{d}}}+2L_ {d}h^{\beta}\right]\] \[\lesssim \left(\frac{1}{\epsilon^{2}\wedge 1}\frac{\ln K}{Nh^{d}} \right)^{\frac{\gamma+1}{2}}+h^{\beta(\gamma+1)}.\]

For \(I_{k}\) with \(k\geq 1\),

\[\min_{l\in I_{k}}\{a_{l},b_{l}\} \leq \sum_{l\in I_{k}}b_{l}\] (80) \[\leq \left(\sum_{l:\Delta_{l}\leq 2^{k}\Delta_{0}}p(B_{l})\right) \cdot 2K\left(2^{k}\Delta_{0}+2L_{d}h^{\beta}+\Delta_{0}\right)\exp\left[- \frac{1}{2}\left(\frac{e^{\frac{\epsilon}{2}}-1}{e^{\frac{\epsilon}{2}}+1} \right)^{2}cNh^{d}2^{2k-2}\Delta_{0}^{2}\right]\] \[\leq M(2^{k}\Delta_{0}+2L_{d}h^{\beta})^{\gamma}\left((2^{k}+1) \Delta_{0}+2L_{d}h^{\beta}\right)(2K)^{-2^{2k-2}+1}\] \[\leq M(\Delta_{0}+2L_{d}h^{\beta})^{\gamma+1}2^{k\gamma+k-2^{2k-2}+2}.\]

It is obvious that there exists a finite constant \(C^{\prime}<\infty\) that depends on \(\gamma\), such that

\[\sum_{k=1}^{\infty}2^{k\gamma+k-2^{2k-2}+2}\leq C^{\prime}.\] (81)

Therefore

\[\sum_{k=1}^{\infty}\sum_{l\in I_{k}}\min\{a_{l},b_{l}\}\lesssim \left(\frac{1}{\epsilon^{2}\wedge 1}\frac{\ln K}{Nh^{d}}\right)^{\frac{\gamma+1}{2}}+h^ {\beta(\gamma+1)}.\] (82)

Combine (74), (79) and (82),

\[R-R^{*}\lesssim\left(\frac{1}{\epsilon^{2}\wedge 1}\frac{\ln K}{Nh^{d}} \right)^{\frac{\gamma+1}{2}}+h^{\beta(\gamma+1)}.\] (83)

To minimize the overall excess risk, let

\[h\sim\left(\frac{N(\epsilon^{2}\wedge 1)}{\ln K}\right)^{-\frac{1}{2 \beta+d}},\] (84)

then

\[R-R^{*}\lesssim\left(\frac{N(\epsilon^{2}\wedge 1)}{\ln K}\right)^{-\frac{ \beta(\gamma+1)}{2\beta+d}}.\] (85)

Compare to the simple random response method, the bin splitting avoids the polynomial decrease over \(K\).

Proof of Theorem 3

We still divide the support as the local label DP setting, except that the value of \(h\) is different, which will be specified later in this section. Note that (42) still holds here. Let \(\mathbf{V}\) takes values from \(\{-1,1\}^{m}\) randomly with equal probability, and \(V_{k}\) is the \(k\)-th element. Then \(\eta_{\mathbf{V}}(\mathbf{x})\) is a random function. The corresponding random output of hypothesis testing is denoted as \(\hat{V}_{k}\), which is calculated by (38). Then

\[\inf_{\mathcal{A}\in\mathcal{A}_{*}(f,\eta)\in\mathcal{F}_{el_{s} }}(R-R^{*}) \geq \frac{1}{2}ch^{\beta+d}\left\|\phi\right\|_{1}\inf_{\mathcal{A} \in\mathcal{A}_{*}}\max_{\mathbf{v}\in\hat{V}}\sum_{k=1}^{m}\text{P}(\hat{v}_{ k}\neq v_{k})\] (86) \[\geq \frac{1}{2}h^{\beta+d}\left\|\phi\right\|_{1}\inf_{\mathcal{A} \in\mathcal{A}_{*}}\sum_{k=1}^{m}\text{P}(\hat{V}_{k}\neq V_{k})\] \[= \frac{1}{2}h^{\beta+d}\left\|\phi\right\|_{1}\sum_{k=1}^{m}\inf_{ \mathcal{A}\in\mathcal{A}_{*}}\text{P}(\hat{V}_{k}\neq V_{k}),\]

in which the last step holds since \(\hat{V}_{k}\) for different \(k\) are calculated independently.

It remains to give a lower bound of \(\text{P}(\hat{V}_{k}\neq V_{k})\). Denote \(n_{k}\) as the number of samples falling in \(B_{k}\), \(\bar{Y}_{k}\) as the average label values in \(B_{k}\):

\[n_{k} := \sum_{i=1}^{N}\mathbf{1}(\mathbf{X}_{i}\in B_{k}),\] (87) \[\bar{Y}_{k} := \frac{1}{n_{k}}\sum_{i=1}^{N}Y_{i}\mathbf{1}(X_{i}\in B_{k}).\] (88)

Moreover, define

\[a_{k} := \frac{1}{n_{k}}\sum_{i=1}^{N}|\eta(\mathbf{X}_{i})|\mathbf{1}( \mathbf{X}_{i}\in B_{k})\] (89) \[= \frac{h^{\beta}}{n_{k}}\sum_{i=1}^{N}\phi\left(\frac{\mathbf{X}_ {i}-\mathbf{c}_{k}}{h}\right)\mathbf{1}(\mathbf{X}_{i}\in B_{k}),\]

in which the last step comes from (36). Then

\[\mathbb{E}[\bar{Y}_{k}|\mathbf{X}_{1:N},V_{k}]=V_{k}a_{k},\] (90)

in which \(\mathbf{X}_{1:N}\) means \(\mathbf{X}_{1},\ldots,\mathbf{X}_{N}\). We then show the following lemma.

**Lemma 1**.: _If \(0\leq t\leq\ln 2/(\epsilon n_{k})\), and \(n_{k}t\) is an integer, then_

\[\text{P}(\hat{V}_{k}=1|\mathbf{X}_{1:N},\bar{Y}_{k}=-t)+\text{P}(\hat{V}_{k}=- 1|\mathbf{X}_{1:N},\bar{Y}_{k}=t)\geq\frac{2}{3}.\] (91)

Proof.: Construct \(D^{\prime}\) by changing the label values of \(l=n_{k}t\) items from these \(n_{k}\) samples falling in \(B_{k}\), from \(-1\) to \(1\). Then the average label values in \(B_{k}\) is denoted as \(\bar{Y}_{k}^{\prime}\) after such replacement. \(\hat{V}_{k}\) also becomes \(\hat{V}_{k}^{\prime}\). Then from the \(\epsilon\)-label DP requirement,

\[\text{P}(\hat{V}_{k}=1|\mathbf{X}_{1:N},\bar{Y}_{k}=-t) \stackrel{{(a)}}{{\geq}} e^{-l\epsilon}\text{P}\left(\hat{V}_{k}^{\prime}=1|\mathbf{X}_{1:N}, \bar{Y}_{k}^{\prime}=-t+\frac{2l}{n_{k}}\right)\] (92) \[\stackrel{{(b)}}{{\geq}} e^{-l\epsilon}\text{P}\left(\hat{V}_{k}=1|\mathbf{X}_{1:N}, \bar{Y}_{k}=-t+\frac{2l}{n_{k}}\right)\] \[\geq e^{-n_{k}t\epsilon}\left[1-\text{P}\left(\hat{V}_{k}=-1|\mathbf{ X}_{1:N},\bar{Y}_{k}=-t+\frac{2l}{n_{k}}\right)\right]\] \[\geq \frac{1}{2}\left[1-\text{P}\left(\hat{V}_{k}=-1|\mathbf{X}_{1:N},\bar{Y}_{k}=t\right)\right].\]in which (a) uses the group privacy property. The Hamming distance between \(D\) and \(D^{\prime}\) is \(l\), thus the ratio of probability between \(D\) and \(D^{\prime}\) is within \([e^{-le},e^{le}]\). (b) holds because the algorithm does not change after changing \(D\) to \(D^{\prime}\). Similarly,

\[\text{P}(\hat{V}_{k}=-1|\mathbf{X}_{1:N},\bar{Y}_{k}=t)\geq\frac{1}{2}\left[1- \text{P}\left(\hat{V}_{k}=1|\mathbf{X}_{1:N},\bar{Y}_{k}=-t\right)\right].\] (93)

Then (91) can be shown by adding up (92) and (93). 

Now we use Lemma 1 to bound the excess risk. With sufficiently large \(n_{k}\), \(\hat{Y}_{k}\) will be close to Gaussian distribution with mean \(a_{k}\). To be more rigorous, by Berry-Esseen theorem [2], for some absolute constant \(C_{E}\),

\[\text{P}\left(\bar{Y}_{k}\leq a_{k}|\mathbf{X}_{1:N},V_{k}=1\right)\geq\frac{ 1}{2}-\frac{C_{E}}{\sqrt{n_{k}}}.\] (94)

Similarly,

\[\text{P}\left(\bar{Y}_{k}\geq-a_{k}|\mathbf{X}_{1:N},V_{k}=-1\right)\geq\frac {1}{2}-\frac{C_{E}}{\sqrt{n_{k}}}.\] (95)

We first analyze cubes with

\[n_{k}>16C_{E}^{2},a_{k}<\frac{\ln 2}{\epsilon n_{k}}.\] (96)

Under condition (96), the right hand side of (94) and (95) are at least \(1/4\). Therefore

\[\text{P}(\hat{V}_{k}\neq V_{k}|\mathbf{X}_{1:N}) = \frac{1}{2}\text{P}(\hat{V}_{k}=1|\mathbf{X}_{1:N},V_{k}=-1)+\frac {1}{2}\text{P}(\hat{V}_{k}=-1|\mathbf{X}_{1:N},V_{k}=1)\] (97) \[\geq \frac{1}{8}\text{P}\left(\hat{V}_{k}=1|\mathbf{X}_{1:N},\bar{Y}_ {k}\geq-\frac{\ln 2}{\epsilon n_{k}}\right)+\frac{1}{8}\text{P}\left(\hat{V}_{k}=-1| \mathbf{X}_{1:N},\bar{Y}_{k}\leq\frac{\ln 2}{\epsilon n_{k}}\right)\] \[\geq \frac{1}{12}.\]

From (86),

\[\inf_{\mathcal{A}\in\mathcal{A}_{\epsilon}}\sup_{(f,\eta)\in\mathcal{F}_{cls} }(R-R^{*})\geq\frac{1}{2}h^{\beta+d}\left\|\phi\right\|_{1}\sum_{k=1}^{m} \frac{1}{12}\text{P}\left(a_{k}<\frac{\ln 2}{\epsilon n_{k}},n_{k}>16C_{E}^{2}\right)\] (98)

From (35), (89) and (87), \(a_{k}\leq h^{\beta}\). Therefore

\[\inf_{\mathcal{A}\in\mathcal{A}_{\epsilon}}\sup_{(f,\eta)\in\mathcal{F}_{cls} }(R-R^{*})\geq\frac{1}{24}h^{\beta+d}\left\|\phi\right\|_{1}\sum_{k=1}^{m} \text{P}\left(16C_{E}^{2}<n_{k}<\frac{\ln 2}{\epsilon h^{\beta}}\right).\] (99)

Recall that each cube has probability mass \(ch^{d}\). Select \(h\) such that

\[2Nch^{d}=\frac{\ln 2}{\epsilon h^{\beta}}.\] (100)

From Chernoff inequality, \(16C_{E}^{2}<n_{k}<\ln 2/(\epsilon h^{\beta})\) holds with high probability. (100) yields

\[h\sim(\epsilon N)^{-\frac{1}{d+\beta}}.\] (101)

Recall the bound of \(m\) in (37). Let \(m\sim h^{\gamma\beta-d}\), then (99) becomes

\[\inf_{\mathcal{A}\in\mathcal{A}_{\epsilon}(f,\eta)\in\mathcal{F}_ {cls}}(R-R^{*}) \gtrsim h^{\beta(\gamma+1)}\] (102) \[\gtrsim (\epsilon N)^{-\frac{\beta(\gamma+1)}{d+\beta}}.\]

Moreover, the standard lower bound for classification [28] is

\[\inf_{\mathcal{A}\in\mathcal{A}_{\epsilon}(f,\eta)\in\mathcal{F}_{cls}}(R-R^{ *})\gtrsim N^{-\frac{\beta(\gamma+1)}{2\beta+d}}.\] (103)

Therefore

\[\inf_{\mathcal{A}\in\mathcal{A}_{\epsilon}(f,\eta)\in\mathcal{F}_{cls}}(R-R^{ *}) \gtrsim N^{-\frac{\beta(\gamma+1)}{2\beta+d}}+(\epsilon N)^{-\frac{ \beta(\gamma+1)}{d+\beta}}.\] (104)Proof of Theorem 4

Denote

\[n_{l}^{*}=\max_{j}n_{lj},\] (105)

\[n_{l}:=\sum_{j=1}^{K}n_{lj}=\sum_{i=1}^{N}\mathbf{1}(\mathbf{X}_{i}\in B_{l}).\] (106)

For all \(j\) such that \(n_{l}^{*}-n_{lj}>t\),

\[\text{P}(c_{l}=j|\mathbf{X}_{1:N},Y_{1:N}) = \frac{e^{cn_{lj}/2}}{\sum_{k=1}^{K}e^{cn_{lk}/2}}\] (107) \[\leq \frac{e^{cn_{l}^{*}/2}}{\sum_{k=1}^{K}e^{cn_{lk}/2}}e^{-\frac{1}{ 2}\epsilon t}\] \[\leq e^{-\frac{1}{2}\epsilon t}.\]

Therefore

\[\text{P}(n_{l}^{*}-n_{lc_{l}}>t)=\sum_{j:n_{l}^{*}-n_{lj}>t}\text{P}(c_{l}=j| \mathbf{X}_{1:N},Y_{1:N})\leq Ke^{-\frac{1}{2}\epsilon t}.\] (108)

Hence

\[\mathbb{E}[n_{l}^{*}-n_{lc_{l}}] = \int_{0}^{\infty}\text{P}(n_{l}^{*}-n_{lj}>t)dt\] (109) \[\leq \int_{0}^{2\ln K/\epsilon}1dt+\int_{2\ln K/\epsilon}^{\infty}Ke^ {-\frac{1}{2}\epsilon t}dt\] \[= \frac{2}{\epsilon}(\ln K+1).\]

Define

\[v_{lj}=\frac{1}{n_{l}}\sum_{i=1}^{N}\mathbf{1}(\mathbf{X}_{i}\in B_{l})\eta_{ j}(\mathbf{X}_{i}),\] (110)

then

\[\mathbb{E}[n_{lj}|\mathbf{X}_{1:N}]=n_{l}v_{lj}.\] (111)

From Hoeffding's inequality,

\[\text{P}(|n_{lj}-n_{l}v_{lj}|>t)\leq 2e^{-\frac{1}{2n_{l}}t^{2}}.\] (112)

Thus

\[\mathbb{E}\left[\max_{j}|n_{lj}-n_{l}v_{lj}|\right] = \int_{0}^{\infty}\text{P}\left(\cup_{j=1}^{K}\left\{|n_{lj}-n_{l} v_{lj}|>t\right\}\right)dt\] (113) \[\leq \int_{0}^{\infty}\min\left(1,2Ke^{-\frac{1}{2n_{l}}t^{2}}\right)dt\] \[= \sqrt{2n_{l}\ln(2K)}+\int_{\sqrt{2n_{l}\ln(2K)}}^{\infty}2Ke^{- \frac{1}{2n_{l}}t^{2}}dt\] \[< 2\sqrt{2n_{l}\ln(2K)},\]

in which the last step uses the inequalit \(\int_{t}^{\infty}e^{-u^{2}/(2\sigma^{2})}du\leq\sqrt{2\pi}\sigma e^{-t^{2}/(2 \sigma^{2})}\). Then

\[\mathbb{E}[v_{l}^{*}-v_{lc_{l}}|\mathbf{X}_{1:N}] = \frac{1}{n_{l}}\mathbb{E}[n_{l}v_{l}^{*}-n_{l}v_{lc_{l}}]\] (114) \[= \frac{1}{n_{l}}\mathbb{E}\left[n_{l}^{*}-n_{lc_{l}}+n_{l}v_{l}^{* }-n_{l}^{*}+n_{lc_{l}}-n_{l}v_{lc_{l}}\right]\] \[\leq \frac{1}{n_{l}}\mathbb{E}[n_{l}^{*}-n_{lc_{l}}]+\frac{2}{n_{l}} \mathbb{E}\left[\max_{j}|n_{lj}-n_{l}v_{lj}|\right]\] \[\leq \frac{2}{\epsilon n_{l}}(\ln K+1)+4\sqrt{\frac{2\ln(2K)}{n_{l}}}.\]By Holder continuity assumption (Assumption 1(a)), for \(\mathbf{x}\in B_{l}\),

\[|v_{lj}-\eta_{j}(\mathbf{x})|\leq\frac{1}{n_{l}}\sum_{i=1}^{N}\mathbf{1}(\mathbf{ X}_{i}\in B_{l})|\eta_{j}(\mathbf{X}_{i})-\eta_{j}(\mathbf{x})|\leq L_{d}h^{\beta},\] (115)

in which \(L_{d}=L\sqrt{d}\), \(L\) is the constant in Assumption 1(a). Thus

\[\mathbb{E}[\eta^{*}(\mathbf{x})-\eta_{c_{l}}(\mathbf{x})|\mathbf{X}_{1:N}]\leq \frac{2}{\epsilon n_{l}}(\ln K+1)+4\sqrt{\frac{2\ln(2K)}{n_{l}}}+2L_{d}h^{ \beta}.\] (116)

Now take integration over \(B_{l}\).

\[\int_{B_{l}}\left(\eta^{*}(\mathbf{x})-\mathbb{E}[\eta_{c_{l}}( \mathbf{x})]\right)f(\mathbf{x})d\mathbf{x}\] \[\leq p\left(n_{l}<\frac{1}{2}Np(B_{l})\right)\int_{B_{l}}\left(\eta^{ *}(\mathbf{x})-\mathbb{E}\left[\eta_{c_{l}}(\mathbf{x})|n_{l}<\frac{1}{2}Np(B _{l})\right]\right)f(\mathbf{x})d\mathbf{x}\] \[+\int_{B_{l}}\left(\eta^{*}(\mathbf{x})-\mathbb{E}\left[\eta_{c _{l}}(\mathbf{x})|n_{l}\geq\frac{1}{2}Np(B_{l})\right]\right)f(\mathbf{x})d \mathbf{x}\] \[\leq p(B_{l})\exp\left[-\frac{1}{2}(1-\ln 2)Np(B_{l})\right]+\left[ \frac{2(\ln K+1)}{\epsilon Np(B_{l})}+4\sqrt{\frac{2\ln(2K)}{Np(B_{l})}}+2L_{d }h^{\beta}\right]p(B_{l}),\] (117)

in which \(p(B_{l})=\text{P}(\mathbf{X}\in B_{l})=\int_{B_{l}}f(\mathbf{x})d\mathbf{x}\). (117) is the central label DP counterpart of (67). The remainder of the proof follows arguments of the local label DP. We omit detailed steps. The result is

\[R-R^{*}\lesssim\left(\frac{\ln K}{\epsilon Nh^{d}}+\sqrt{\frac{\ln K}{Nh^{d}} }+h^{\beta}\right)^{\gamma+1}.\] (118)

Let

\[h\sim\left(\frac{\ln K}{\epsilon N}\right)^{\frac{1}{\beta+d}}+\left(\frac{ \ln K}{N}\right)^{\frac{1}{2\beta+d}},\] (119)

then

\[R-R^{*}\lesssim\left(\frac{\ln K}{\epsilon N}\right)^{\frac{\beta(\gamma+1)} {\beta+d}}+\left(\frac{\ln K}{N}\right)^{\frac{\beta(\gamma+1)}{2\beta+d}}.\] (120)

The proof is complete.

## Appendix F Proof of Theorem 5

Find \(G\) cubes in the support and the length of each cube is \(h\). Let \(\phi(\mathbf{u})\) be the same as the classification case shown in appendix B. For \(\mathbf{v}\in\mathcal{V}:=\{-1,1\}^{G}\), let

\[\eta_{\mathbf{v}}(\mathbf{x})=\sum_{k=1}^{K}v_{k}\phi\left(\frac{\mathbf{x}- \mathbf{c}_{k}}{h}\right)h^{\beta}.\] (121)

Let \(\text{P}(Y=1|\mathbf{x})=(1+\eta_{\mathbf{v}}(\mathbf{x}))/2\), \(\text{P}(Y=-1|\mathbf{x})=(1-\eta_{\mathbf{v}}(\mathbf{x}))\), then \(\eta(\mathbf{x})=\mathbb{E}[Y|\mathbf{x}]=\eta_{\mathbf{v}}(\mathbf{x})\).

The overall volume of the support is bounded. Thus, we have

\[G\leq C_{G}h^{-d}\] (122)

for some constant \(C_{G}\).

Denote

\[\hat{v}_{k}=\text{sign}\left(\int_{B_{k}}\hat{\eta}(\mathbf{x})\phi\left(\frac {\mathbf{x}-\mathbf{c}_{k}}{h}\right)f(\mathbf{x})d\mathbf{x}\right),\] (123)then the excess risk is bounded by

\[R = \mathbb{E}\left[\left(\hat{\eta}(\mathbf{X})-\eta_{\mathbf{v}}( \mathbf{X})\right)^{2}\right]\] (124) \[= \sum_{k=1}^{K}\int_{B_{k}}\mathbb{E}\left[\left(\hat{\eta}( \mathbf{x})-\eta_{\mathbf{v}}(\mathbf{x})\right)^{2}\right]f(\mathbf{x})d \mathbf{x}.\]

If \(\hat{v}_{k}\neq v_{k}\), from (123),

\[\int_{B_{k}}\left(\hat{\eta}(\mathbf{x})-v_{k}\phi\left(\frac{ \mathbf{x}-\mathbf{c}_{k}}{h}\right)h^{\beta}\right)^{2}f(\mathbf{x})d\mathbf{ x}\geq\int_{B_{k}}\left(\hat{\eta}(\mathbf{x})+v_{k}\phi\left(\frac{\mathbf{x}- \mathbf{c}_{k}}{h}\right)h^{\beta}\right)^{2}f(\mathbf{x})d\mathbf{x}.\] (125)

Therefore, if \(\hat{v}_{k}\neq v_{k}\), then

\[\int_{B_{k}}\left(\hat{\eta}(\mathbf{x})-\eta_{\mathbf{v}}( \mathbf{x})\right)^{2}d\mathbf{x}\geq\frac{1}{2}\int_{B_{k}}\phi^{2}\left( \frac{\mathbf{x}-\mathbf{c}_{k}}{h}\right)h^{2\beta}f(\mathbf{x})d\mathbf{x} =\frac{1}{2}ch^{2\beta+d}\left\|\phi\right\|_{2}^{2}.\] (126)

Therefore

\[R-R^{*} \geq \mathbb{E}\left[\frac{1}{2}ch^{2\beta+d}\left\|\phi\right\|_{2}^{ 2}\mathbf{1}(\hat{v}_{k}\neq v_{k})\right]\] (127) \[= \frac{1}{2}ch^{2\beta+d}\left\|\phi\right\|_{2}^{2}\mathbb{E}[ \rho_{H}(\hat{\mathbf{v}},\mathbf{v})].\]

Similar to the classification problem analyzed in Appendix B, let

\[h\sim\left(N(\epsilon\wedge 1)^{2}\right)^{-\frac{1}{2\beta+d}},\] (128)

then \(\delta\lesssim 1\), and

\[\inf_{\hat{\mathbf{v}}}\sup_{M\in\mathcal{M}_{\epsilon}}\max_{ \mathbf{v}\in\mathcal{V}}\mathbb{E}[\rho_{H}(\hat{\mathbf{v}},\mathbf{v})] \gtrsim G\sim h^{-d}.\] (129)

Thus

\[\inf_{\hat{\eta}}\inf_{M\in\mathcal{M}_{\epsilon}}\sup_{P_{X,Y} \in\mathcal{F}_{reg1}}R\gtrsim h^{2\eta+d}h^{-d}\sim h^{2\beta}\sim(N(\epsilon \wedge 1)^{2})^{-\frac{2\beta}{2\beta+d}}.\] (130)

## Appendix G Proof of Theorem 6

According to Assumption 2, \(|Y|<T\) with probability \(1\), thus \(\mathrm{Var}[Y|\mathbf{x}]\leq T^{2}\) for any \(\mathbf{x}\). A Laplacian distribution with parameter \(\lambda\) has variance \(2\lambda^{2}\), thus

\[\mathrm{Var}[W]=2\lambda^{2}=2\left(\frac{2T}{\epsilon}\right)^{ 2}=\frac{8T^{2}}{\epsilon^{2}}.\] (131)

Hence

\[\mathrm{Var}[Z]=\mathrm{Var}[Y]+\mathrm{Var}[W]\leq T^{2}\left( 1+\frac{8}{\epsilon^{2}}\right).\] (132)

Now we analyze the bias first.

\[\mathbb{E}[\hat{\eta}(\mathbf{x})]=\mathbb{E}\left[\frac{1}{k} \sum_{i\in\mathcal{N}_{k}(\mathbf{x})}Z_{i}\right]=\mathbb{E}\left[\frac{1}{k }\sum_{i\in\mathcal{N}_{k}(\mathbf{x})}\eta(\mathbf{X}_{i})\right].\] (133)Thus

\[|\mathbb{E}[\hat{\eta}(\mathbf{x})]-\eta(\mathbf{x})| \leq \mathbb{E}\left[\frac{1}{k}\sum_{i\in\mathcal{N}_{k}(\mathbf{x})}| \eta(\mathbf{X}_{i})-\eta(\mathbf{x})|\right]\] (134) \[\leq \mathbb{E}\left[\frac{1}{k}\sum_{i\in\mathcal{N}_{k}(\mathbf{x})} \min\left\{L\left\|\mathbf{X}_{i}-\mathbf{x}\right\|^{\beta},2T\right\}\right]\] \[\leq \mathbb{E}\left[\frac{1}{k}\sum_{i\in\mathcal{N}_{k}(\mathbf{x})} \min\left\{L\rho^{\beta}(\mathbf{x}),2T\right\}\right]\] \[\leq 2T\mathsf{P}(\rho(\mathbf{x})>r_{0})+Lr_{0}^{\beta}\] \[\leq 2Te^{-(1-\ln 2)k}+L\left(\frac{2k}{Ncv_{d}\theta}\right)^{\frac{ \beta}{d}}\] \[\leq C_{1}\left(\frac{k}{N}\right)^{\frac{\beta}{d}},\]

for some constant \(C_{1}\).

It remains to bound the variance.

\[\mathrm{Var}[\hat{\eta}(\mathbf{x})]=\mathbb{E}\left[\mathrm{Var}\left[\hat{ \eta}(\mathbf{x})|\mathbf{X}_{1},\ldots,\mathbf{X}_{N}\right]\right]+\mathrm{ Var}[\mathbb{E}[\hat{\eta}(\mathbf{x})]|\mathbf{X}_{1},\ldots,\mathbf{X}_{N}].\] (135)

For the first term in (135),

\[\mathrm{Var}[\hat{\eta}(\mathbf{x})|\mathbf{X}_{1},\ldots,\mathbf{ X}_{N}] = \mathrm{Var}\left[\frac{1}{k}\sum_{i\in\mathcal{N}_{k}(\mathbf{x} )}Z_{i}|\mathbf{X}_{1},\ldots,\mathbf{X}_{N}\right]\] (136) \[= \frac{1}{k^{2}}\sum_{i\in\mathcal{N}_{k}(\mathbf{x})}\mathrm{Var} [Z_{i}|\mathbf{X}_{1},\ldots,\mathbf{X}_{N}]\] \[\leq \frac{1}{k}T^{2}\left(1+\frac{8}{\epsilon^{2}}\right).\]

For the second term in (135),

\[\mathrm{Var}[\mathbb{E}[\hat{\eta}(\mathbf{x})|\mathbf{X}_{1}, \ldots,\mathbf{X}_{N}]] = \mathrm{Var}\left[\frac{1}{k}\sum_{i\in\mathcal{N}_{k}(\mathbf{x} )}\eta(\mathbf{X}_{i})\right]\] (137) \[\leq \mathbb{E}\left[\left(\frac{1}{k}\sum_{i\in\mathcal{N}_{k}( \mathbf{x})}\eta(\mathbf{X}_{i})-\eta(\mathbf{x})\right)^{2}\right]\] \[= \frac{1}{k}\sum_{i\in\mathcal{N}_{k}(\mathbf{x})}\mathbb{E}\left[ (\eta(\mathbf{X}_{i})-\eta(\mathbf{x}))^{2}\right]\] \[\leq \frac{1}{k}\sum_{i\in\mathcal{N}_{k}(\mathbf{x})}\mathbb{E}\left[ \min\left\{L^{2}\left\|\mathbf{X}_{i}-\mathbf{x}\right\|^{2\beta},4T^{2} \right\}\right]\] \[\leq 4T^{2}e^{-(1-\ln 2)k}+L^{2}r_{0}^{2\beta}\] \[\leq C_{1}^{2}\left(\frac{k}{N}\right)^{\frac{2\beta}{d}}.\]

Therefore (135) becomes

\[\mathrm{Var}[\hat{\eta}(\mathbf{x})]\leq\frac{1}{k}T^{2}\left(1+\frac{8}{ \epsilon^{2}}\right)+C_{1}^{2}\left(\frac{k}{N}\right)^{\frac{2\beta}{d}}.\] (138)Combine the analysis of bias and variance,

\[\mathbb{E}[(\hat{\eta}(\mathbf{x})-\eta(\mathbf{x}))^{2}]\leq\frac{1}{k}T^{2} \left(1+\frac{8}{\epsilon^{2}}\right)+2C_{1}^{2}\left(\frac{k}{N}\right)^{\frac {2\beta}{d}}.\] (139)

Therefore the overall risk is bounded by

\[R=\mathbb{E}[(\hat{\eta}(\mathbf{X})-\eta(\mathbf{X}))^{2}]\lesssim\frac{1}{k }T^{2}\left(1+\frac{8}{\epsilon^{2}}\right)+2C_{1}^{2}\left(\frac{k}{N}\right) ^{\frac{2\beta}{d}}.\] (140)

The optimal growth rate of \(k\) over \(N\) is

\[k\sim N^{\frac{2\beta}{d+2\beta}}(\epsilon\wedge 1)^{-\frac{2\beta}{d+2\beta}}.\] (141)

Then the convergence rate of the overall risk becomes

\[R\lesssim(N(\epsilon\wedge 1)^{2})^{-\frac{2\beta}{d+2\beta}}.\] (142)

## Appendix H Proof of Theorem 7

From (127),

\[R-R^{*} \geq \frac{1}{2}ch^{2\beta+d}\left\|\phi\right\|_{2}^{2}\mathbb{E}[ \rho_{H}(\hat{\mathbf{V}},\mathbf{V})]\] (143) \[= \frac{1}{2}ch^{2\beta+d}\left\|\phi\right\|_{2}^{2}\sum_{k=1}^{G} \text{P}(\hat{V}_{k}\neq V_{k}).\]

Follow the analysis of lower bounds of classification in Appendix D, let \(h\) scales as (101), then \(\text{P}(\hat{V}_{k}\neq V_{k})\gtrsim 1\). Moreover, \(G\sim h^{-d}\). Hence

\[\inf_{\mathcal{A}\in\mathcal{A}_{\epsilon}(f,\eta)\in\mathcal{F}_{reg1}}(R-R^ {*})\gtrsim h^{2\beta}\sim(\epsilon N)^{-\frac{2\beta}{d+\beta}}.\] (144)

Moreover, note that the non-private lower bound of regression is

\[\inf_{\mathcal{A}\in\mathcal{A}_{\epsilon}(f,\eta)\in\mathcal{F}_{reg1}}(R-R^ {*})\gtrsim N^{-\frac{2\beta}{2\beta+d}}.\] (145)

Combine (144) and (145),

\[\inf_{\mathcal{A}\in\mathcal{A}_{\epsilon}(f,\eta)\in\mathcal{F}_{reg1}}(R-R^ {*})\gtrsim N^{-\frac{2\beta}{2\beta+d}}+(\epsilon N)^{-\frac{2\beta}{d+\beta }}.\] (146)

## Appendix I Proof of Theorem 8

_1) Analysis of bias._ Note that

\[\mathbb{E}[\hat{\eta}_{l}|\mathbf{X}_{1:N}]=\mathbb{E}[Y|\mathbf{X}\in B_{l} ]=\frac{1}{p(B_{l})}\int\eta(\mathbf{u})f(\mathbf{u})d\mathbf{u}.\] (147)

Therefore, for all \(\mathbf{x}\in B_{l}\),

\[|\mathbb{E}[\hat{\eta}_{l}|\mathbf{X}_{1:N}]-\eta(\mathbf{x})| \leq \frac{1}{p(B_{l})}\int|\eta(\mathbf{u})-\eta(\mathbf{x})|f( \mathbf{u})d\mathbf{u}\] (148) \[\leq L_{d}h^{\beta}.\]

Therefore for all \(\mathbf{x}\in B_{l}\),

\[|\mathbb{E}[\hat{\eta}_{l}]-\eta(\mathbf{x})|\leq L_{d}h^{\beta}.\] (149)

_2) Analysis of variance._ If \(n_{l}>0\),

\[\text{Var}\left[\frac{1}{n_{l}}\sum_{i=1}^{N}\mathbf{1}(\mathbf{X}_{i}\in B_{ l})Y_{i}|\mathbf{X}_{1:N}\right]=\frac{1}{n_{l}}\,\text{Var}[Y|\mathbf{X}\in B_{l}] \leq\frac{1}{n_{l}}.\] (150)Therefore

\[\mathrm{Var}\left[\frac{1}{n_{l}}\sum_{i=1}^{N}\mathbf{1}(\mathbf{X}_ {i}\in B_{l})Y_{i}\right] \leq \mathsf{P}\left(n_{l}<\frac{1}{2}Np(B_{l})\right)+\mathsf{P}\left(n _{l}\geq\frac{1}{2}Np(B_{l})\right)\frac{2}{Np(B_{l})}\] (151) \[\leq \exp\left[-\frac{1}{2}(1-\ln 2)Np(B_{l})\right]+\frac{2}{Nch^{d}}.\]

Similarly,

\[\mathrm{Var}[W_{l}] \leq \mathsf{P}\left(n_{l}<\frac{1}{2}Np(B_{l})\right)\frac{1}{ \epsilon^{2}}+\mathsf{P}\left(n_{l}\geq\frac{1}{2}Np(B_{l})\right)\frac{8}{ \left(\frac{1}{2}Np(B_{l})\right)^{2}\epsilon^{2}}\] (152) \[\lesssim \frac{1}{N^{2}h^{2d}\epsilon^{2}}.\]

The mean squared error can then be bounded by the bounds of bias and variance.

\[\mathbb{E}\left[(\hat{\eta}(\mathbf{x})-\eta(\mathbf{x}))^{2}\right]\lesssim h ^{2\beta}+\frac{1}{Nh^{d}}+\frac{1}{N^{2}h^{2d}\epsilon^{2}}.\] (153)

Let

\[h\sim N^{-\frac{1}{2\beta+d}}+(\epsilon N)^{-\frac{1}{d+\beta}}.\] (154)

Then

\[R-R^{*}\lesssim N^{-\frac{2\beta}{2\beta+d}}+(\epsilon N)^{-\frac{2\beta}{d+ \beta}}.\] (155)

## Appendix J Proof of Theorem 9

Now we prove the minimax lower bound of nonparametric regression under label DP constraint. We focus on the case in which \(\epsilon\) is small.

Similar to the steps of the proof of Theorem 5 in Appendix F, we find \(B\) cubes in the support. The definition of \(\eta_{\mathbf{v}}\), \(\hat{v}_{k}\) are also the same as (121) and (123). Compared with the case with bounded noise, now \(Y\) can take values in \(\mathbb{R}\).

For given \(\mathbf{x}\), let

\[Y=\left\{\begin{array}{cl}T&\mbox{with probability}\quad\frac{1}{2}\left( \frac{M_{p}}{Tp}+\frac{\eta_{\mathbf{v}}(\mathbf{x})}{T}\right)\\ 0&\mbox{with probability}\quad\quad 1-\frac{M_{p}}{Tp}\\ -T&\mbox{with probability}\quad\frac{1}{2}\left(\frac{M_{p}}{Tp}-\frac{\eta_{ \mathbf{v}}(\mathbf{x})}{T}\right).\end{array}\right.\] (156)

In Appendix F about the case with bounded noise, \(T\) is a fixed constant. However, here \(T\) is not fixed and will change over \(N\). It is straightforward to show that the distribution of \(Y\) in (156) satisfies Assumption 3:

\[\mathbb{E}[|Y|^{p}|\mathbf{x}]=M_{p}.\] (157)

Moreover, by taking expectation over \(Y\), it can be shown that \(\eta_{\mathbf{v}}\) is still the regression function:

\[\mathbb{E}[Y|\mathbf{x}]=\eta_{\mathbf{v}}(\mathbf{x}).\] (158)

Let

\[T=\left(\frac{1}{2}M_{p}h^{-\beta}\right)^{\frac{1}{p-1}}.\] (159)

Here we still define

\[\delta=\sup_{M\in\mathcal{M}_{\epsilon}\mathbf{v},\mathbf{v}^{\prime}:\rho_{H }(\mathbf{v},\mathbf{v}^{\prime})=1}D(P_{(X,Z)_{1,N}|\mathbf{v}}||P_{(X,Z)_{1,N}|\mathbf{v}^{\prime}}).\] (160)Without loss of generality, suppose that \(v_{1}=v_{1}^{\prime}\) for \(i\neq 1\). Then

\[D(P_{(X,Z)_{1:N}|\mathbf{v}}||P_{(X,Z)_{1:N}|\mathbf{v}^{\prime}}) = ND(P_{X,Z|\mathbf{v}}||P_{X,Z|\mathbf{v}^{\prime}})\] (161) \[= N\int_{B_{1}}f(\mathbf{x})D(P_{Z|\mathbf{X},\mathbf{v}}||P_{Z| \mathbf{X},\mathbf{v}^{\prime}})d\mathbf{x}\] \[\leq N\int_{B_{1}}f(\mathbf{x})(e^{\epsilon}-1)^{2}\mathbb{T}\mathbb{ V}^{2}\left(P_{Z|X,\mathbf{v}},P_{Z|X,\mathbf{v}^{\prime}}\right)d\mathbf{x}\] \[= N\int_{B_{1}}f(\mathbf{x})(e^{\epsilon}-1)^{2}\eta_{\mathbf{v}}^ {2}(\mathbf{x})\frac{1}{T^{2}}d\mathbf{x}\] \[= N(e^{\epsilon}-1)^{2}\frac{h^{2\beta}}{T^{2}}\int_{B_{1}}f( \mathbf{x})\phi^{2}\left(\frac{\mathbf{x}-\mathbf{c}_{1}}{h}\right)d\mathbf{x}\] \[= N(e^{\epsilon}-1)^{2}h^{2\beta+d}\left\|\phi\right\|_{2}^{2}T^{-2}\] \[= N(e^{\epsilon}-1)^{2}\left\|\phi\right\|_{2}^{2}\left(\frac{1}{ 2}M_{p}\right)^{-\frac{2}{p-1}}h^{2\beta+d+\frac{2\beta}{p-1}}.\]

Let

\[h\sim(N(e^{\epsilon}-1)^{2})^{-\frac{p-1}{2p\beta+d(p-1)}},\] (162)

then \(\delta\lesssim 1\). Hence

\[\inf_{\hat{\eta}}\inf_{M\in\mathcal{M}_{\epsilon}(f,\eta)\in\mathcal{F}}R \gtrsim h^{2\beta}\sim(N(e^{\epsilon}-1)^{2})^{-\frac{2\beta(p-1)}{2p\beta+d( p-1)}}.\] (163)

## Appendix K Proof of Theorem 10

Define

\[\eta_{T}(\mathbf{x}):=\mathbb{E}[Y_{T}|\mathbf{x}].\] (164)

Then

\[\hat{\eta}(\mathbf{x})-\eta(\mathbf{x})=\eta_{T}(\mathbf{x})-\eta(\mathbf{x}) +\mathbb{E}[\hat{\eta}(\mathbf{x})]-\eta_{T}(\mathbf{x})+\hat{\eta}(\mathbf{x })-\mathbb{E}[\hat{\eta}(\mathbf{x})].\] (165)

Therefore

\[\mathbb{E}\left[(\hat{\eta}(\mathbf{x})-\eta(\mathbf{x}))^{2}\right] \leq 3(\eta_{T}(\mathbf{x})-\eta(\mathbf{x}))^{2}+3(\mathbb{E}[\hat{ \eta}(\mathbf{x})]-\eta_{T}(\mathbf{x}))^{2}+3\operatorname{Var}[\hat{\eta}( \mathbf{x})]\] (166) \[:= 3(I_{1}+I_{2}+I_{3}).\]

Now we bound \(I_{1}\), \(I_{2}\) and \(I_{3}\) separately.

**Bound of \(I_{1}\).** We show the following lemma (which will also be used later).

**Lemma 2**.: \[|\eta_{T}(\mathbf{x})-\eta(\mathbf{x})|\leq\frac{M_{p}}{p-1}T^{1-p}.\] (167)

Proof.: Firstly, we decompose \(\eta_{T}(\mathbf{x})\) and \(\eta(\mathbf{x})\):

\[\eta_{T}(\mathbf{x})=\mathbb{E}[Y_{T}|\mathbf{x}]=\mathbb{E}[Y\mathbf{1}(-T \leq Y\leq T)|\mathbf{x}]+T\mathsf{P}(Y>T|\mathbf{x})-TP(Y<T|\mathbf{x}),\] (168)

\[\eta(\mathbf{x})=\mathbb{E}[Y|\mathbf{x}]=\mathbb{E}[Y\mathbf{1}(-T\leq Y \leq T)|\mathbf{x}]+\mathbb{E}[Y\mathbf{1}(Y>T)|\mathbf{x}]-\mathbb{E}[Y \mathbf{1}(Y<T)|\mathbf{x}].\] (169)

The first term is the same between (168) and (169). Therefore we only need to compare the second and the third term.

\[\mathbb{E}[Y\mathbf{1}(Y>T)|\mathbf{x}] = \int_{0}^{T}\mathsf{P}(Y>T|\mathbf{x})dt+\int_{T}^{\infty}\mathsf{ P}(Y>T|\mathbf{x})dt\] (170) \[\leq T\mathsf{P}(Y>T|\mathbf{x})+\int_{T}^{\infty}M_{p}t^{-p}dt\] \[= T\mathsf{P}(Y>T|\mathbf{x})+\frac{M_{p}}{p-1}T^{1-p}.\]Therefore

\[\mathbb{E}[Y\mathbf{1}(Y>T)|\mathbf{x}]-T\text{P}(Y>T|\mathbf{x})\leq \frac{M_{p}}{p-1}T^{1-p}.\] (171)

Similarly,

\[TP(Y<T|\mathbf{x})-\mathbb{E}[Y\mathbf{1}(Y<T)|\mathbf{x}]\leq \frac{M_{p}}{p-1}T^{1-p}.\] (172)

A Combination of these two inequalities yields the (167). 

With Lemma 2,

\[I_{1}\leq\frac{M_{p}^{2}}{(p-1)^{2}}T^{2(1-p)}.\] (173)

**Bound of \(I_{2}\).** Follow the steps in (134),

\[I_{2}\leq C_{1}^{2}\left(\frac{k}{N}\right)^{\frac{2\beta}{d}}.\] (174)

**Bound of \(I_{3}\).** We decompose \(\mathrm{Var}[\hat{\eta}(\mathbf{x})]\) as following:

\[\mathrm{Var}[\hat{\eta}(\mathbf{x})]=\mathbb{E}[\mathrm{Var}[ \hat{\eta}(\mathbf{x})|\mathbf{X}_{1},\ldots,\mathbf{X}_{N}]]+\mathrm{Var}[ \mathbb{E}[\hat{\eta}(\mathbf{x})|\mathbf{X}_{1},\ldots,\mathbf{X}_{N}]].\] (175)

For the first term in (175), from Assumption 3, \(\mathbb{E}[|Y|^{p}|\mathbf{x}]\leq M_{p}\). Since \(p\geq 2\), we have \(\mathbb{E}[Y^{2}|\mathbf{x}]=M_{p}^{\frac{2}{p}}\). Therefore

\[\mathrm{Var}[Z_{i}|\mathbf{X}_{1},\ldots,\mathbf{X}_{N}]=\mathrm{ Var}[Y_{T}]+\mathrm{Var}[W]\leq M_{p}^{\frac{2}{p}}+\frac{8T^{2}}{\epsilon^{2}}.\] (176)

Recall (20), we have

\[\mathrm{Var}[\hat{\eta}(\mathbf{x})|\mathbf{X}_{1},\ldots,\mathbf{ X}_{N}] = \frac{1}{k^{2}}\sum_{i\in\mathcal{N}_{k}(\mathbf{x})}\mathrm{Var }[Z_{i}|\mathbf{X}_{1},\ldots,\mathbf{X}_{N}]\] (177) \[\leq \frac{1}{k}\left(M_{p}^{\frac{2}{p}}+\frac{8T^{2}}{\epsilon^{2}} \right).\]

For the second term in (175), (137) still holds, thus

\[\mathrm{Var}[\mathbb{E}[\hat{\eta}(\mathbf{x})|\mathbf{X}_{1}, \ldots,\mathbf{X}_{N}]]\leq C_{1}^{2}\left(\frac{k}{N}\right)^{\frac{2\beta}{d }},\] (178)

and

\[I_{3}\leq\frac{1}{k}\left(M_{p}^{\frac{2}{p}}+\frac{8T^{2}}{ \epsilon^{2}}\right)+C_{1}^{2}\left(\frac{k}{N}\right)^{\frac{2\beta}{d}}.\] (179)

Plug (173), (174) and (179) into (166), and take expectations, we get

\[R = \mathbb{E}[(\hat{\eta}(\mathbf{X})-\eta(\mathbf{X}))^{2}]\] (180) \[\lesssim T^{2(1-p)}+\frac{1}{k}+\frac{T^{2}}{k\epsilon^{2}}+\left(\frac{k }{N}\right)^{\frac{2\beta}{d}}.\]

Let

\[T\sim(k\epsilon^{2})^{\frac{1}{2p}},k\sim(N\epsilon^{2})^{\frac{2p \beta}{d(p-1)+2p\beta}}\lor N^{\frac{2\beta}{2\beta+d}},\] (181)

then

\[R\lesssim(N\epsilon^{2})^{-\frac{2\beta(p-1)}{d(p-1)+2p\beta}}\lor N^{-\frac{ 2\beta}{2\beta+d}}.\] (182)Proof of Theorem 11

Let \(Y\) be distributed as (156). Recall Lemma 1 for the problem of classification and regression with bounded noise.

Now we show the corresponding lemma for regression with unbounded noise.

**Lemma 3**.: _If \(0\leq t\leq T\ln 2/(\epsilon n_{k})\), and \(n_{k}t/T\) is an integer, then_

\[\text{P}(\hat{V}_{k}=1|\mathbf{X}_{1:N},\bar{Y}_{k}=-t)+\text{P}(\hat{V}_{k}=- 1|\mathbf{X}_{1:N},\bar{Y}_{k}=t)\geq\frac{2}{3}.\] (183)

Here we briefly explain the condition \(n_{k}t\) is an integer. Recall the definition of \(\bar{Y}_{k}\) in (88). Now since \(Y\) take values in \(\{-T,0,T\}\), \(n_{k}\bar{Y}_{k}/T\) must be an integer. Therefore, in Lemma 3, we only need to consider the case such that \(n_{k}t/T\) is an integer.

Proof.: The proof follows the proof of Lemma 1 closely. We provide the proof here for completeness. Construct \(D^{\prime}\) by changing the label values of \(l=n_{k}t/T\) items from these \(n_{k}\) samples falling in \(B_{k}\), from \(-T\) to \(T\). Then the average label values in \(B_{k}\) is denoted as \(\bar{Y}^{\prime}_{k}\) after such replacement. \(\hat{V}_{k}\) also becomes \(\hat{V}^{\prime}_{k}\). Then from the \(\epsilon\)-label DP requirement,

\[\text{P}(\hat{V}_{k}=1|\mathbf{X}_{1:N},\bar{Y}_{k}=-t) \stackrel{{(a)}}{{\geq}} e^{-l\epsilon}\text{P}\left(\hat{V}^{\prime}_{k}=1|\mathbf{X}_{1:N},\bar{Y}^{\prime}_{k}=-t+\frac{2l}{n_{k}}\right)\] \[\stackrel{{(b)}}{{\geq}} e^{-l\epsilon}\text{P}\left(\hat{V}_{k}=1|\mathbf{X}_{1:N},\bar{Y}_{k}=-t+ \frac{2l}{n_{k}}\right)\] \[\geq e^{-n_{k}t\epsilon}\left[1-\text{P}\left(\hat{V}_{k}=-1|\mathbf{ X}_{1:N},\bar{Y}_{k}=-t+\frac{2l}{n_{k}}\right)\right]\] \[\geq \frac{1}{2}\left[1-\text{P}\left(\hat{V}_{k}=-1|\mathbf{X}_{1:N},\bar{Y}_{k}=t\right)\right].\] (184)

in which (a) uses the group privacy property. The Hamming distance between \(D\) and \(D^{\prime}\) is \(l\), thus the ratio of probability between \(D\) and \(D^{\prime}\) is within \([e^{-l\epsilon},e^{l\epsilon}]\). (b) holds because the algorithm does not change after changing \(D\) to \(D^{\prime}\). Similarly,

\[\text{P}(\hat{V}_{k}=-1|\mathbf{X}_{1:N},\bar{Y}_{k}=t)\geq\frac{1}{2}\left[1 -\text{P}\left(\hat{V}_{k}=1|\mathbf{X}_{1:N},\bar{Y}_{k}=-t\right)\right].\] (185)

Then (183) can be shown by adding up (184) and (185). 

We then follow the proof of Theorem 3 in Appendix D. (101) becomes

\[h\sim\left(\frac{\epsilon N}{T}\right)^{-\frac{1}{d+\beta}}.\] (186)

In (156), note that \(\text{P}(Y=T)\geq 0\) and \(\text{P}(Y=-T)\geq 0\). Therefore \(M_{p}/T^{p}\geq\eta_{\mathbf{v}}(\mathbf{x})/T\). This requires \(h^{\beta}T^{p-1}\leq M_{p}\). Let \(T\sim h^{-\frac{\beta}{p-1}}\), then

\[h\sim\left(\epsilon N\right)^{-\frac{1}{d+\beta}}h^{\frac{\beta}{(d+\beta|(p- 1)}},\] (187)

i.e.

\[h\sim\left(\epsilon N\right)^{-\frac{p-1}{p\beta+d(p-1)}}.\] (188)

Combine with standard minimax rate, the lower bound of regression with unbounded noise is

\[\inf_{\mathcal{A}\in\mathcal{A}_{c}(f,\eta)\in\mathcal{F}_{reg2}}(R-R^{*}) \gtrsim N^{-\frac{2\beta}{2\beta+d}}+\left(\epsilon N\right)^{-\frac{2\beta(p- 1)}{p\beta+d(p-1)}}.\] (189)Proof of Theorem 12

_1) Analysis of bias._ Note that Lemma 2 still holds here. Moreover, recall (149). Therefore

\[|\mathbb{E}[\hat{\eta}_{l}]-\eta(\mathbf{x})|\leq|\mathbb{E}[\hat{\eta}_{l}-\eta _{T}(\mathbf{x})]|+|\eta_{T}(\mathbf{x})-\eta(\mathbf{x})|\leq L_{d}h^{\beta}+ \frac{M_{p}}{p-1}T^{1-p}.\] (190)

_2) Analysis of variance._ Similar to (151), it can be shown that

\[\mathrm{Var}\left[\frac{1}{n_{l}}\sum_{i=1}^{N}\mathbf{1}(\mathbf{X}_{i}\in B _{l})Y_{i}\right]\lesssim\frac{1}{Nh^{d}}.\] (191)

Moreover, the noise variance can be bounded by

\[\mathrm{Var}[W_{l}]\lesssim\frac{T^{2}}{N^{2}h^{2d}\epsilon^{2}}.\] (192)

The mean squared error is then bounded by

\[\mathbb{E}\left[\left(\hat{\eta}(\mathbf{x})-\eta(\mathbf{x})\right)^{2} \right]\lesssim h^{2\beta}+T^{2(1-p)}+\frac{T^{2}}{N^{2}h^{2d}\epsilon^{2}}+ \frac{1}{Nh^{d}}.\] (193)

Let \(T\sim(\epsilon Nh^{d})^{1/p}\), then

\[R-R^{*}=\mathbb{E}\left[\left(\hat{\eta}(\mathbf{X})-\eta(\mathbf{X})\right)^ {2}\right]\lesssim h^{2\beta}+\frac{1}{Nh^{d}}+(\epsilon Nh^{d})^{-2(1-1/p)}.\] (194)

To minimize (194), let

\[h\sim N^{-\frac{1}{2\beta+d}}+(\epsilon N)^{-\frac{p-1}{p\beta+d(p-1)}},\] (195)

then

\[R-R^{*}\lesssim N^{-\frac{2\beta}{2\beta+d}}+(\epsilon N)^{-\frac{2\beta(p-1) }{p\beta+d(p-1)}}.\] (196)NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main contribution (i.e. proposing a new Huber loss minimization approach which is more suitable to realistic cases, and providing theoretical analysis) has been made clear in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: It is explained at the end of conclusion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Proofs are shown in the appendix, and intuition is provided in the paper. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: This is a theoretical paper without experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

Answer: [NA]

Justification: This is a theoretical paper without experiments.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This is a theoretical paper without experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: No experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: No experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our paper does not violate code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper is foundational and theoretical research and not tied to particular applications. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper has no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.