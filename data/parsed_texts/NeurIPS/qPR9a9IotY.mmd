# Finding Symmetry in Neural Network Parameter Spaces

 Bo Zhao

University of California, San Diego

bozhao@ucsd.edu

&Nima Dehmamy

IBM Research

nima.dehnamy@ibm.com

Robin Walters

Northeastern University

r.walters@northeastern.edu

&Rose Yu

University of California, San Diego

roseyu@ucsd.edu

###### Abstract

Parameter space symmetries, or loss-invariant transformations, are important for understanding neural networks' loss landscape, training dynamics, and generalization. However, identifying the full set of these symmetries remains a challenge. In this paper, we formalize data-dependent parameter symmetries and derive their infinitesimal form, which enables an automated approach to discover symmetry across different architectures. Our framework systematically uncovers parameter symmetries, including previously unknown ones. We also prove that symmetries in smaller subnetworks can extend to larger networks, allowing the discovery of symmetries in small architectures to generalize to more complex models.

## 1 Introduction

Parameter space symmetry, or loss-invariant transformation of parameters, influences various aspects of deep learning theory. Continuous symmetry connects groups to their orbits, revealing important topological properties such as the dimension (Zhao et al., 2023) and connectedness (Zhao et al., 2023) of the minimum. Parameter symmetry also influences training dynamics through the associated conserved quantities of gradient flow (Kunin et al., 2021) and by steering stochastic gradient descent towards certain favored solutions (Ziyin, 2024). Additionally, symmetry provides a tool to perform optimization within a loss level set, with successful applications in accelerating optimization (Armenta et al., 2023; Zhao et al., 2022) and improving generalization (Zhao et al., 2024). Other applications of parameter space symmetry include model compression (Ganev et al., 2022; Sourek et al., 2021) and reducing the search space for efficient sampling in Bayesian neural networks (Wiese et al., 2023).

Despite the wide range of applications, our knowledge of parameter space symmetries is limited. In particular, known symmetries often cannot account for all loss-invariant parameter transformation. While several frameworks have been developed to unify known symmetries, whether the symmetries in current literature are complete remains an open question. Due to a lack of systematic approach, current practice typically requires deriving symmetries from scratch for every new architecture, creating barriers for wider application that leverages parameter symmetries. In this paper, we discuss an automated approach to directly learn the symmetry groups and their group actions on the parameter space of neural networks. We show that large networks often have symmetries inherited from its components or subnetworks. This view suggests that searching for symmetries in small networks is an effective approach to identify a significant number of symmetries in modern architectures.

Our main contributions are:

* Formal definitions of data-dependent parameter symmetries and their infinitesimal form.

* An approach to identify symmetries in the parameter space of large networks from known symmetries in smaller subnetworks.
* A framework that discovers symmetry in neural network parameter spaces.

## 2 Data-dependent group action and symmetry

Let \(\Theta\) be the space of parameters and \(\mathcal{D}\) be the space of data. In this paper, we consider loss functions of the form \(L:\Theta\times\mathcal{D}\rightarrow\mathbb{R}\), which map parameters and a single data point to a real number. By abuse of notation, we allow \(L\) to simultaneously process multiple data points. Specifically, we sometimes define \(L:\Theta\times\mathcal{D}^{d}\rightarrow\mathbb{R}^{d}\) for \(d\in\mathbb{Z}^{+}\) data points.

Let \(G\) be a group. Consider a map \(a\), which defines a map for every data batch of size \(d\in\mathbb{Z}^{+}\):

\[a:\mathcal{D}^{d} \rightarrow(G\times\Theta\rightarrow\Theta)\] \[X \mapsto(a_{X}:g,\theta\mapsto\theta^{\prime}).\] (1)

The map \(a\) is a group action on \(\Theta\) if it satisfies the following axioms:

identity: \[a_{X}(I,\theta)=\theta,\qquad\forall X\in\mathcal{D}^{d},\ \forall \theta\in\Theta.\] associative law: \[a_{X}(g_{2},a_{X}(g_{1},\theta))=a_{X}(g_{2}g_{1},\theta),\qquad \forall g_{1},g_{2}\in G,\ \ \forall X\in\mathcal{D}^{d},\ \ \forall\theta\in\Theta.\]

A group action \(a\) is a parameter space symmetry of \(L\) if it additionally satisfies

loss invariance: \[L(a_{X}(g,\theta),X)=L(\theta,X),\qquad\forall g\in G,\ \ \forall X\in\mathcal{D}^{d},\ \ \forall\theta\in\Theta.\]

A function \(L\) has a \(G\)-symmetry if there exists a loss-invariant group action \(a\). We refer to \(G\) as a symmetry group of \(L\). Additionally, the action \(a\) is termed a data-dependent group action or symmetry if the map (1) has a non-trivial dependency on \(X\). That is, \(a\) is data-dependent if there exists \(X_{1},X_{2}\in\mathcal{D}^{d}\), such that \(a_{X_{1}}\neq a_{X_{2}}\). We derive an infinitesimal version of parameter space symmetries in Appendix B.

## 3 Building symmetries from known ones

One way to identify symmetries in a large network is by examining its components or subnetworks. Despite often having billions of parameters, neural networks typically consist of a limited set of functional families, such as fully connected layers, attention mechanisms, and activation functions. This modular view suggests a mechanism by which symmetries in networks with fewer layers might extend to those in deeper networks. Additionally, within similar types of networks, it may be possible to extrapolate symmetries found in narrower layers to wider ones.

By focusing on symmetries in small architectures and using them to infer symmetries in larger ones, we circumvent the complexity associated with direct handling of high-dimensional parameter spaces. This approach not only simplifies the discovery of symmetries in large-scale networks but also provides a systematic method for using symmetries in smaller subnetworks to understand those in more extensive architectures. Proofs and further discussions can be found in Appendix C.

When a loss function \(L\) depends on a subset of parameter exclusively through a subnetwork \(f\), any symmetries that preserves \(f\) will also preserve the original network \(L\):

**Proposition 3.1**.: _Let \(L:\Theta\times\mathcal{D}^{d}\rightarrow\mathbb{R}^{d}\) be a function, where the parameter space \(\Theta\) is a product space \(\Theta=\Theta_{1}\times\Theta_{2}\), with spaces \(\Theta_{1},\Theta_{2}\). Suppose there exist functions \(h:\Theta_{1}\times\mathcal{D}^{d}\to S\), \(f:\Theta_{2}\times S\to T\), and \(j:(\Theta_{1}\times T)\times\mathcal{D}^{d}\rightarrow\mathbb{R}^{d}\), such that for every \(\theta=(\theta_{1},\theta_{2})\in\Theta\) and \(X\in\mathcal{D}^{d}\), \(L(\theta,X)=j\big{(}(\theta_{1},f\big{(}\theta_{2},h(\theta_{1},X)\big{)}),X \big{)}\). If \(a:S\rightarrow(G\times\Theta_{2}\rightarrow\Theta_{2})\) is a \(G\)-symmetry of \(f\), then there is an induced \(G\)-symmetry of \(L\), \(a^{\prime}:\mathcal{D}^{d}\rightarrow(G\times\Theta\rightarrow\Theta)\), defined by \(a^{\prime}_{X}(g,(\theta_{1},\theta_{2}))=\big{(}\theta_{1},a_{h(\theta_{1},X )}(g,\theta_{2})\big{)}\)._

The relationship between the functions in the proposition is described by the commutative diagram below, where \(p_{1}:\Theta\rightarrow\Theta_{1}\), \(p_{2}:\Theta\rightarrow\Theta_{2}\) are projections onto \(\Theta_{1}\) and \(\Theta_{2}\), \(i_{1}:\Theta_{1}\rightarrow\Theta_{1}\) and \(i_{2}:\Theta_{2}\rightarrow\Theta_{2}\) are identity maps, and \(X\in\mathcal{D}^{d}\) represents a batch of data. When \(L\) can be decomposed in this way, the function \(h\) does not depend on \(\Theta_{2}\), and the function \(j\) depends on \(\Theta_{2}\) only through the output of \(f\). This effectively confines \(L\)'s dependency on \(\Theta_{2}\) to the transformation defined by \(f\), ensuring that any transformation on \(\Theta_{2}\) not altering the output of \(f\) will not affect the output of \(L\). Consequently, symmetries identified in the smaller network \(f\) can be extrapolated to the larger network \(L\).

Proposition 3.1 can be applied to construct symmetries in larger networks from those in smaller ones (Corollary C.2, C.1 in Appendix C). Figure 1 shows the subset of parameters (\(\Theta_{2}\)) the symmetry applies to in the corollaries.

Note that this approach does not explore the emergence of new, more complex symmetries that may arise as the neural network scale up in size. Notably, there are cases where there exists a \(G\) symmetry over its input space, but group actions on individual layers are not loss-invariant (Kvinge et al. (2022)). Nevertheless, studying smaller and simpler networks remains a effective strategy to obtain a significant number of symmetries in larger networks, and is a first step in characterizing the complete set of symmetries in modern architectures.

## 4 Automatic Discovery of Parameter Symmetries

Formulating symmetries in the infinitesimal form makes them easier to learn using an automatic framework, as it defines a set of local conditions for a function to be a symmetry. Using the infinitesimal symmetry derived in Section B.1, we construct an automated framework for discovering parameter space symmetries.

Enforcing Loss Invariance and Group Axioms.Given a function \(L\), our goal is to find a symmetry \(a\) and a set of Lie algebra elements \(h\) corresponding to a symmetry group of \(L\). We parameterize \(a\) using a neural network with learnable parameters, and set \(h\) to be learnable as well. We define the following loss terms that quantify the deviation from loss invariance and the group axioms (identity and associativity law):

\[L_{\text{invariance}} =\mathbb{E}_{x,\theta}|D_{\theta}L|_{\theta,X}\circ D_{g}a_{X}|_{ I,\theta}(h)|\] (2) \[L_{\text{id}} =\mathbb{E}_{x,\theta}\|a_{x}(I,\theta)-\theta\|_{2}\] (3)

The two loss terms bias the action towards being loss-invariant and preserving identity. By minimizing \(L_{\text{Lie\,,deriv}}\), we ensure that the learned symmetry \(a\) and the Lie algebra element \(h\) satisfy the infinitesimal symmetry condition (Theorem B.1). Minimizing \(L_{\text{id}}\) enforces the identity axiom. By focusing on the Lie algebras, we enforce the loss invariance and group structure at the infinitesimal level. This formulation allows us to avoid computing exponential maps.

Regularizations.To prevent the group action to be the identity function, we encourage the infinitesimal action to be nonzero. In implementation, we include the following regularization term

Figure 1: If a network contains substructures with known symmetry, we can infer the same symmetry for the large network. (a) Symmetry from narrower networks. (b) Symmetry from shallower networks.

to encourage the norm of the infinitesimal action to be around a fixed positive real number \(\beta\): \(L_{\text{reg\_id}}=\min_{a,h}\mathbb{E}_{\theta}|\beta-\|D_{g}a_{X}|_{I,\theta}(h )\||\).

When learning multiple generators simultaneously, we want them to be orthogonal. Following Yang et al. (2023), we do this by including the following cosine similarity between each pair of the \(k\) generators in the loss function: \(L_{\text{reg\_h,orth}}=\sum_{1\leq i<j\leq k}\frac{h_{i}\cdot h_{j}}{\|h_{i}\| \|h_{j}\|}\).

Finally, we encourage sparsity of \(h\) for easier interpretation, with \(L_{\text{reg\_h,sparse}}=\sum_{k,j}|h_{kj}|\).

The final training objective is a weighted average of the above loss and regularization terms, with hyperparameters \(\gamma_{1},...,\gamma_{6}\in\mathbb{R}^{+}\):

\[\min_{h,a}\gamma_{1}L_{\text{invariance}}+\gamma_{2}L_{\text{id}}+\gamma_{3}L_{ \text{reg\_id}}+\gamma_{4}L_{\text{reg\_h,orth}}+\gamma_{5}L_{\text{reg\_h, sparse}}.\] (4)

### Learned data-independent symmetries

In the first set of tasks, we see if our method can learn generators for architectures with already known data-independent symmetries. We consider two-layer networks in the form of \(L(W_{1},W_{2},X,Y)=\|W_{2}\sigma(W_{1}X)-Y\|^{2}\), where \(W_{2}\in\mathbb{R}^{m\times h},W_{1}\in\mathbb{R}^{h\times n}\) are parameters, \(X\in\mathbb{R}^{n\times k}\), \(Y\in\mathbb{R}^{m\times k}\) are data, and \(\sigma\) is a homogeneous activation function.

During training, we train the generators \(h\) and the group action \(a\) under objective (4). We parametrize \(a\) using a 4-layer MLP with hidden dimensions 64, 64, 64. The group action \(a\) takes a group element, parameter, and data as input and outputs transformed parameters. We use 10000 training samples, each containing a randomly generated set of parameters and data. We set the learning rate as \(10^{-3}\) with decay 0.6 every 1000 steps, and the weights for the multi-objective loss as \(\gamma_{1}=10\), \(\gamma_{2}=\gamma_{4}=\gamma_{5}=1\), and \(\gamma_{6}=0.1\).

As a proof of concept, we training a group action and a single generator \(h\in\mathbb{R}^{2\times 2}\) for the two-layer architecture with \(m=h=n=k=1\) and \(\sigma\) being the identity function. Figure 2 visualizes the learned generator, which matches the expected generator that generates the rescaling group.

Note that, however, we do not impose constraints on the group action (in particular, not enforcing linear actions). Hence we do not expect the learned generators to look similar to the elements of the Lie algebra infinitesimal generators of the symmetry group in general. For example, the action \(a\) can be a composition of two function, the first transforming learned generators to the set of actual generators, and the second performing the group action. We find that our method can learn the generators and group actions for wider two-layer homogeneous architectures as well. More examples of learned generators for larger architectures can be found in Appendix D.

### Learned data-dependent Symmetries

As a more practical application of our framework, we attempt to uncover data-dependent symmetries from architectures where no continuous symmetry is known before. We apply our framework to learn generators and loss-invariant group actions for two-layer neural network with sigmoid and tanh activation function, as well as a three-layer neural network with skip connection.

Specifically, we aim to learn symmetries in the two-layer networks defined in the previous section, but replacing \(\sigma\) by sigmoid or tanh. Our objective is again to find a set of generators \(h\) and a group action \(a\) that minimizes (4). We use 10000 training samples, each containing a randomly generated set of parameters and data. We set the learning rate as \(10^{-3}\) and the weights for the multi-objective loss as \(\gamma_{1}=1\), \(\gamma_{2}=\gamma_{4}=10\), \(\gamma_{5}=1\), and \(\gamma_{6}=0.1\).

Figure 3 shows the learned generators for data-dependent symmetries in a two-layer sigmoid MLP with parameters dimensions \(W_{1}\in\mathbb{R}^{3\times 3},W_{2}\in\mathbb{R}^{3\times 1}\) and data \(X\in\mathbb{R}^{3\times 1},Y\in\mathbb{R}^{1\times 1}\). Figure 6 in the Appendix shows the training curve. Since sigmoid networks have no data-independent continuous symmetry, this set of symmetries are data-dependent, indicating that our method successfully learns data-dependent symmetries for this architecture.

Figure 2: Generator for a two-layer linear MLP with scalar parameters and data.

Figure 4 shows the learned generators for data-dependent symmetries in a three-layer tanh MLP with parameters dimensions \(W_{1}\in\mathbb{R}^{2\times 2},W_{2}\in\mathbb{R}^{2\times 2},W_{3}\in\mathbb{R}^{ 2\times 1}\) and data \(X\in\mathbb{R}^{1\times 2},Y\in\mathbb{R}^{1\times 1}\). The generators indicate the existence of symmetries that act on non-contiguous layers, which has not been discovered in previous literature.

## 5 Discussion

While our discovery framework suggests that there are previously unknown data-dependent symmetries in various neural network architectures, the existence and number of symmetries in neural network parameter spaces remain open questions. Whether the number of symmetries is affected by existence of symmetry in data or changes during training are also interesting directions. Future work will examine the structure of learned symmetry, such as the dimension of Lie algebras.

## References

* Armenta et al. (2023) Marco Armenta, Thierry Judge, Nathan Painchaud, Youssef Skandarani, Carl Lemaire, Gabriel Gibeau Sanchez, Philippe Spino, and Pierre-Marc Jodoin. Neural teleportation. _Mathematics_, 11(2):480, 2023.
* Badrinarayanan et al. (2015) Vijay Badrinarayanan, Bamdev Mishra, and Roberto Cipolla. Symmetry-invariant optimization in deep networks. _arXiv preprint arXiv:1511.01754_, 2015.
* Benton et al. (2020) Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural networks from training data. _Advances in neural information processing systems_, 33:17605-17616, 2020.
* Bensouss et al. (2015)An Mei Chen, Haw-minn Lu, and Robert Hecht-Nielsen. On the geometry of feedforward neural network error surfaces. _Neural computation_, 5(6):910-927, 1993.
* Dehmamy et al. (2021) Nima Dehmamy, Robin Walters, Yanchen Liu, Dashun Wang, and Rose Yu. Automatic symmetry discovery with lie algebra convolutional network. _Advances in Neural Information Processing Systems_, 34:2503-2515, 2021.
* Du et al. (2018) Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. _Neural Information Processing Systems_, 2018.
* Gabel et al. (2023) Alex Gabel, Victoria Klein, Riccardo Valperga, Jeroen SW Lamb, Kevin Webster, Rick Quax, and Efstratios Gavves. Learning lie group symmetry transformations with neural networks. In _Topological, Algebraic and Geometric Learning Workshops 2023_, pages 50-59. PMLR, 2023.
* Ganev et al. (2022) Iordan Ganev, Twan van Laarhoven, and Robin Walters. Universal approximation and model compression for radial neural networks. _arXiv preprint arXiv:2107.02550v2_, 2022.
* Grigsby et al. (2023) Elisenda Grigsby, Kathryn Lindsey, and David Rolnick. Hidden symmetries of relu networks. In _International Conference on Machine Learning_, pages 11734-11760. PMLR, 2023.
* Gruver et al. (2022) Nate Gruver, Marc Anton Finzi, Micah Goldblum, and Andrew Gordon Wilson. The lie derivative for measuring learned equivariance. In _The Eleventh International Conference on Learning Representations_, 2022.
* Karjol et al. (2024) Pavan Karjol, Rohan Kashyap, Aditya Gopalan, and A. P. Prathosh. A unified framework for discovering discrete symmetries. In _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics_, volume 238 of _Proceedings of Machine Learning Research_, pages 793-801. PMLR, 2024.
* Krippendorf and Syvaeri (2020) Sven Krippendorf and Marc Syvaeri. Detecting symmetries with neural networks. _Machine Learning: Science and Technology_, 2(1):015010, 2020.
* Kunin et al. (2021) Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. In _International Conference on Learning Representations_, 2021.
* Kvinge et al. (2022) Henry Kvinge, Tegan Emerson, Grayson Jorgenson, Scott Vasquez, Tim Doster, and Jesse Lew. In what ways are deep neural networks invariant and how should we measure this? _Advances in Neural Information Processing Systems_, 35:32816-32829, 2022.
* Moskalev et al. (2022) Artem Moskalev, Anna Sepliarskaia, Ivan Sosnovik, and Arnold Smeulders. Liegg: Studying learned lie group generators. _Advances in Neural Information Processing Systems_, 35:25212-25223, 2022.
* Moskalev et al. (2023) Artem Moskalev, Anna Sepliarskaia, Erik J Bekkers, and Arnold WM Smeulders. On genuine invariance learning without weight-tying. In _Topological, Algebraic and Geometric Learning Workshops 2023_, pages 218-227. PMLR, 2023.
* Portilheiro (2023) Vasco Portilheiro. Quantifying lie group learning with local symmetry error. In _NeurIPS 2023 Workshop on Symmetry and Geometry in Neural Representations_, 2023.
* Romero and Lohit (2022) David W Romero and Suhas Lohit. Learning partial equivariances from data. _Advances in Neural Information Processing Systems_, 35:36466-36478, 2022.
* Shaw et al. (2024) Ben Shaw, Abram Magner, and Kevin R Moon. Symmetry discovery beyond affine transformations. _arXiv preprint arXiv:2406.03619_, 2024.
* Sonoda et al. (2023) Sho Sonoda, Hideyuki Ishi, Isao Ishikawa, and Masahiro Ikeda. Joint group invariant functions on data-parameter domain induce universal neural networks. In _NeurIPS 2023 Workshop on Symmetry and Geometry in Neural Representations_, 2023.
* Surek et al. (2021) Gustav Sourek, Filip Zelezny, and Ondrej Kuzelka. Lossless compression of structured convolutional models via lifting. In _International Conference on Learning Representations_, 2021.
* Urbano and Romero (2023) Alonso Urbano and David W Romero. Self-supervised detection of perfect and partial input-dependent symmetries. _arXiv preprint arXiv:2312.12223_, 2023.
* Sosnovik et al. (2021)Jonas Gregor Wiese, Lisa Wimmer, Theodore Papamarkou, Bernd Bischl, Stephan Gunnemann, and David Rugamer. Towards efficient mcmc sampling in bayesian neural networks by exploiting symmetry. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD): Research Track_, pages 459-474, 2023.
* Yang et al. (2023a) Jianke Yang, Nima Dehmamy, Robin Walters, and Rose Yu. Latent space symmetry discovery. _arXiv preprint arXiv:2310.00105_, 2023a.
* Yang et al. (2023b) Jianke Yang, Robin Walters, Nima Dehmamy, and Rose Yu. Generative adversarial symmetry discovery. _International Conference on Machine Learning_, 2023b.
* Zhao et al. (2022) Bo Zhao, Nima Dehmamy, Robin Walters, and Rose Yu. Symmetry teleportation for accelerated optimization. _Advances in Neural Information Processing Systems_, 2022.
* Zhao et al. (2023a) Bo Zhao, Nima Dehmamy, Robin Walters, and Rose Yu. Understanding mode connectivity via parameter space symmetry. In _UniReps: the First Workshop on Unifying Representations in Neural Models_, 2023a.
* Zhao et al. (2023b) Bo Zhao, Iordan Ganev, Robin Walters, Rose Yu, and Nima Dehmamy. Symmetries, flat minima, and the conserved quantities of gradient flow. _International Conference on Learning Representations_, 2023b.
* Zhao et al. (2024) Bo Zhao, Robert M Gower, Robin Walters, and Rose Yu. Improving convergence and generalization using parameter symmetries. _International Conference on Learning Representations_, 2024.
* Zhou et al. (2021) Allan Zhou, Tom Knowles, and Chelsea Finn. Meta-learning symmetries by reparameterization. _International Conference on Learning Representations_, 2021.
* Ziyin (2024) Liu Ziyin. Symmetry leads to structured constraint of learning. In _International Conference on Machine Learning_. PMLR, 2024.

## Appendix A Related Work

Parameter space symmetry.Parameter symmetries are loss-invariant transformations on neural network parameters, often in the form of group actions. Symmetry is present in many neural networks. Known symmetries include invertible linear transformations in linear networks, rescaling in homogeneous networks (Badrinarayanan et al., 2015; Du et al., 2018), radial rescaling in radial neural networks (Ganev et al., 2022), and translation in softmax and scaling in batchnorm functions (Kunin et al., 2021). In tanh neural networks (Chen et al., 1993), only permutation and sign flip symmetries preserve the loss function. ReLU networks, however, possess symmetries beyond the well-known rescaling (Grigsby et al., 2023). The existence and number of symmetries in most other architectures remain an open question.

Data-dependent symmetry.While the above symmetries leave the loss unchanged on all data, a relaxed definition, data-dependent symmetry, only requires loss invariance on a subset of data. Zhao et al. (2023) found examples of such symmetries with nontrivial data dependency, although these symmetries are complicated, limited to minibatches of size one, and difficult generalize across different architectures. This motivates an automated symmetry discovery framework, which, in principle, can find symmetries of arbitrary form in arbitrary architectures. The concept of a symmetry dependent on data has also appeared in adjacent fields. For example, (Moskalev et al., 2023) observe that learned data invariance in neural networks is strongly conditioned on data and breaks under data distribution drift; Sonoda et al. (2023) define a joint group action on data and parameters as part of a new proof of universal approximation theory.

Discovering and measuring symmetry.Various work explores learning continuous symmetries by identifying generators of Lie groups (Krippendorf and Syvaeri, 2020; Moskalev et al., 2022; Dehmamy et al., 2021; Yang et al., 2023; Gabel et al., 2023), including cases with nonlinear group actions (Yang et al., 2023; Shaw et al., 2024). We build on this approach to discover data-dependent group action in high-dimensional parameter spaces. While learning discrete symmetry (Zhou et al., 2021; Karjol et al., 2024) and distributions of symmetry (Benton et al., 2020; Romero and Lohit, 2022; Urbano and Romero, 2023) are also relevant, they are not the primary focus of this paper.

Extracted symmetry is often evaluated locally, by measuring function changes under infinitesimal symmetry transformations (Gruver et al., 2022) or by comparing tangent spaces of orbits under the learned group and the true symmetry group (Portilheiro, 2023). We adopt the local invariance of loss functions under symmetry transformation, similar to that defined in (Gruver et al., 2022; Moskalev et al., 2022), as the minimization objective in learning data-dependent group actions.

## Appendix B Infinitesimal Symmetry and Examples

### Infinitesimal Symmetry

We derive an infinitesimal version of parameter space symmetries. For the automatic symmetry discovery framework in Section 4, this definition allows us to learn the group elements and actions without computing matrix exponential, which is expensive, during training.

The following theorem shows that the derivative of the loss function \(L\) with respect to the parameters \(\theta\) vanishes in the directions generated by the symmetry group's infinitesimal transformations. In other words, the loss function is invariant to small changes along these symmetric directions in parameter space.

**Theorem B.1**.: _Let \(a:\mathcal{D}^{d}\rightarrow(G\times\Theta\rightarrow\Theta)\) be a parameter space symmetry of a loss function \(L:\Theta\times\mathcal{D}^{d}\rightarrow\mathbb{R}^{d}\). Let \(D_{\theta}L|_{\theta,X}:T_{\theta}\Theta\rightarrow\mathbb{R}^{d}\) be the derivative of \(L\) with respect to \(\theta\), and \(D_{g}a_{X}|_{I,\theta}:\mathfrak{g}\to T_{\theta}\Theta\) be the derivative of \(a_{X}(g,\theta)\) with respect to \(g\). Then, for all \(\theta\in\Theta\), \(X\in\mathcal{D}^{d}\), and \(h\in\mathfrak{g}\)_

\[D_{\theta}L|_{\theta,X}\circ D_{g}a_{X}|_{I,\theta}\circ h=0.\] (5)Proof sketch.: Consider a smooth curve \(\gamma(t)=a_{X}(\exp(ht),\theta)\) in \(\Theta\), where \(h\in\mathfrak{g}\) and \(t\in\mathbb{R}\). Then, since \(L\) is invariant under \(a\), \(L(\gamma(t),X)=L(\theta,X),\forall t\in\mathbb{R}\). The result follows from differentiating both sides with respect to \(t\) at \(t=0\) and applying chain rules. 

Proof.: Since \(a\) is a symmetry of \(L\), we have

\[L(a_{X}(g,\theta),X)=L(\theta,X),\quad\forall g\in G,\quad\forall\theta\in \Theta,\quad\forall X\in\mathcal{D}^{d}.\]

Consider a smooth curve \(\gamma(t)=a_{X}(\exp(ht),\theta)\) in \(\Theta\), where \(h\in\mathfrak{g}\) and \(t\in\mathbb{R}\). Then, since \(L\) is invariant under \(a\),

\[L(\gamma(t),X)=L(\theta,X),\quad\forall t\in\mathbb{R}.\]

Differentiating both sides with respect to \(t\) at \(t=0\), we get

\[\left.\frac{d}{dt}L(\gamma(t),X)\right|_{t=0}=0.\]

Applying the chain rule,

\[\left.\frac{d}{dt}L(\gamma(t),X)\right|_{t=0}=D_{\theta}L|_{\theta,X}\left( \left.\frac{d\gamma(t)}{dt}\right|_{t=0}\right).\]

Now, compute \(\left.\frac{d\gamma(t)}{dt}\right|_{t=0}\) using the chain rule:

\[\left.\frac{d\gamma(t)}{dt}\right|_{t=0}=\left.\frac{d}{dt}a_{X}(\exp(ht), \theta)\right|_{t=0}=D_{g}a_{X}|_{I,\theta}\left(\left.\frac{d}{dt}\exp(ht) \right|_{t=0}\right).\]

Since \(\exp\) is the exponential map from \(\mathfrak{gl}(n)\) to \(GL(n)\), and \(h\in\mathfrak{gl}(n)\), we have

\[\left.\frac{d}{dt}\exp(ht)\right|_{t=0}=h.\]

Therefore,

\[\left.\frac{d\gamma(t)}{dt}\right|_{t=0}=D_{g}a_{X}|_{I,\theta}(h).\]

Putting it all together,

\[D_{\theta}L|_{\theta,X}\left(D_{g}a_{X}|_{I,\theta}(h)\right)=0.\]

Equation 5 states that the gradient of the loss function \(L\) with respect to the parameters \(\theta\) is orthogonal to the directions in parameter space generated by the infinitesimal symmetry transformations \(D_{g}a_{X}\big{|}_{I,\theta}(h)\). This orthogonality implies that moving along these symmetric directions does not change the loss to first order, reflecting the invariance of \(L\) under the group action.

Assuming that \(\Theta=\mathbb{R}^{n}\), then for a single data point (\(d=1\)), we can write (5) in coordinates as

\[D_{\theta}L|_{\theta,X}\left(D_{g}a_{X}|_{I,\theta}(h)\right)=\sum_{i=1}^{dim (\Theta)}\sum_{k=1}^{dim(\mathfrak{g})}\frac{\partial L}{\partial\theta_{i}} \left(\left.D_{g}a_{X}\right|_{I,\theta}\right)_{ik}h_{k}=0.\] (6)

When \(\Theta=\mathbb{R}^{n}\) and \(G\) is a subgroup of \(\mathrm{GL}(n)\) with a linear, data-independent symmetry \(a_{x}(g,\theta)=g\theta\) for all \(x\in X\), (6) reduces to the equation in Theorem 3.1 in (Moskalev et al., 2022). With \((D_{g}a)_{ijk}=\frac{\partial a_{i}}{\partial g_{jk}}=\delta_{ij}\theta_{k}\), we have

\[\left.\frac{dL(\exp(h\cdot t)\cdot\theta)}{dt}\right|_{t=0}=\sum_{i=1}^{n}\sum _{j=1}^{n}\sum_{k=1}^{n}\frac{\partial L}{\partial\theta_{i}}\left(\left.D_{g }a\right|_{I,\theta}\right)_{ijk}h_{jk}=\sum_{i=1}^{n}\sum_{k=1}^{n}\frac{ \partial L}{\partial\theta_{i}}\theta_{k}h_{ik}.\] (7)

Our symmetry acts on parameters instead of data, but otherwise this matches Theorem 3.1 in (Moskalev et al., 2022).

### Alternative Option for Discovery Objectives

A more straightforward training objective exponentiates the Lie algebra to obtain group elements, before enforcing loss invariance and group axioms:

\[\min_{h,a}L_{\text{invariance\_int}}+L_{\text{id\_int}}+L_{\text{ assoc\_int}}\]

with

\[L_{\text{invariance\_int}}=\mathbb{E}_{x,\theta,t}\|L\left(a_{x}( exp(ht),\theta),x\right)-L(\theta,x)\|\] \[L_{\text{id\_int}}=\mathbb{E}_{x,\theta}\|a_{x}(I,\theta)- \theta\|\] \[L_{\text{assoc\_int}}=\sum_{h_{1},h_{2}\in\mathfrak{B}}\mathbb{E} _{x,\theta}\left\|a_{\exp(h_{1})X}(\exp(h_{2}),a_{X}(\exp(h_{1}),\theta))-a_{ X}(\exp(h_{2})\exp(h_{1}),\theta)\right\|.\]

Similarly to the infinitesimal version, this objective also directly enforces the necessary group structures. We adopt the infinitesimal formulation to avoid the computational overhead of evaluating exponential maps.

## Appendix C Building symmetries from known ones

This section contains the proofs for results in Section 3.

**Proposition 3.1**.: _Let \(L:\Theta\times\mathcal{D}^{d}\rightarrow\mathbb{R}^{d}\) be a function, where the parameter space \(\Theta\) is a product space \(\Theta=\Theta_{1}\times\Theta_{2}\), with spaces \(\Theta_{1},\Theta_{2}\). Suppose there exist functions \(h:\Theta_{1}\times\mathcal{D}^{d}\to S\), \(f:\Theta_{2}\times S\to T\), and \(j:(\Theta_{1}\times T)\times\mathcal{D}^{d}\rightarrow\mathbb{R}^{d}\), such that for every \(\theta=(\theta_{1},\theta_{2})\in\Theta\) and \(X\in\mathcal{D}^{d}\), \(L(\theta,X)=j\big{(}(\theta_{1},f\big{(}\theta_{2},h(\theta_{1},X)\big{)}),X \big{)}\). If \(a:S\rightarrow(G\times\Theta_{2}\rightarrow\Theta_{2})\) is a \(G\)-symmetry of \(f\), then there is an induced \(G\)-symmetry of \(L\), \(a^{\prime}:\mathcal{D}^{d}\rightarrow(G\times\Theta\rightarrow\Theta)\), defined by \(a^{\prime}_{X}(g,(\theta_{1},\theta_{2}))=\big{(}\theta_{1},a_{h(\theta_{1},X) }(g,\theta_{2})\big{)}\)._

Proof.: We need to show that \(a^{\prime}\) satisfies the identity and associative law of a group action and preserves \(L\).

Since \(a\) is a group action on \(\Theta_{2}\), it satisfies the identity axiom \(a_{h(\theta_{1},X)}(I,\theta_{2})=\theta_{2}\). Applying this in the definition of \(a^{\prime}\), we get \(a^{\prime}_{X}(I,(\theta_{1},\theta_{2}))=(\theta_{1},a_{h(\theta_{1},X)}(I, \theta_{2}))=(\theta_{1},\theta_{2})\).

Since \(a\) is a group action on \(\Theta_{2}\), it satisfies the associative law \(a_{h(\theta_{1},X)}(g_{2}g_{1},\theta_{2})=a_{h(\theta_{1},X)}(g_{2},a_{h( \theta_{1},X)}(g_{1},\theta_{2}))\), for all \(g_{1},g_{2}\in G\). It follows that \(a^{\prime}\) also satisfies the associative law: \(a^{\prime}_{X}(g_{2}g_{1},(\theta_{1},\theta_{2}))=(\theta_{1},a_{h(\theta_{1},X)}(g_{2}g_{1},\theta_{2}))=(\theta_{1},a_{h(\theta_{1},X)}(g_{2},a_{h(\theta_ {1},X)}(g_{1},\theta_{2})))=a^{\prime}_{X}(g_{2},a^{\prime}_{X}(g_{1},(\theta_ {1},\theta_{2})))\)

Finally, since \(a\) is a symmetry of \(f\), we have \(f(a_{h(\theta_{1},X)}(g,\theta_{2}),h(\theta_{1},X))=f(\theta_{2},h(\theta_{1},X))\), for all \(g\in G\). It follows that \(a^{\prime}\) preserves the value of \(L\): \(L(a^{\prime}_{X}(g,\theta),X)=j\big{(}(\theta_{1},f\big{(}a_{h(\theta_{1},X)}(g,\theta_{2}),h(\theta_{1},X)\big{)}),X\big{)}=j\big{(}(\theta_{1},f\big{(} \theta_{2},h(\theta_{1},X)\big{)}),X\big{)}=L(\theta,X)\). 

The first corollary describes how symmetries identified in narrower networks also apply to wider networks. A function \(\sigma:\mathbb{R}^{h\times k}\rightarrow\mathbb{R}^{h\times k}\) is row-wise if, for any matrix \(A\in\mathbb{R}^{h\times k}\) with rows \(\{a_{i}\in\mathbb{R}^{k}\}_{i=1}^{h}\), the output matrix \(\sigma(A)\) has rows \(\{\sigma_{row}(a_{i})\in\mathbb{R}^{k}\}_{i=1}^{h}\), where \(\sigma_{row}:\mathbb{R}^{k}\rightarrow\mathbb{R}^{k}\) applies independently on each row of \(A\). Element-wise function is a special case of row-wise functions. For fully connected networks with row-wise activation functions, identifying a symmetry in one architecture suggests that the same symmetry will apply to wider versions of that architecture.

**Corollary C.1**.: _Consider a network parameter space \(\Theta(m,h,n)=\mathbb{R}^{m\times h}\times\mathbb{R}^{h\times n}\) and data space \(\mathcal{D}(n,k)=\mathbb{R}^{n\times k}\). Let \(\sigma:\mathbb{R}^{h\times k}\rightarrow\mathbb{R}^{h\times k}\) be a row-wise function. Consider a function \(L_{mnhk}:\Theta(m,h,n)\times\mathcal{D}(n,k)\rightarrow\mathbb{R}^{m\times k}\), defined as \(L_{mnhk}((U,V),X)=U\sigma(VX)\) for \(U\in\mathbb{R}^{m\times h}\), \(V\in\mathbb{R}^{h\times n}\), and \(X\in\mathbb{R}^{n\times k}\). If there is a \(G\)-symmetry of \(L_{mnhk}\), then there is a \(G\)-symmetry of \(L_{mnhk^{\prime}k}\) with any \(h^{\prime}>h\)._Proof.: The function \(L_{mnh^{\prime}k}\) can be decomposed into

\[U(\sigma(VX))_{ik} =U_{ij}\sigma(VX)_{jk}\] \[=\sum_{j=1}^{h}\sum_{l=1}^{n}U_{ij}\sigma(V_{jl}X_{lk})\] \[=\sum_{j=1}^{h}\sum_{l=1}^{n}U_{ij}\sigma(V_{jl}X_{lk})+\sum_{j=h +1}^{h^{\prime}}\sum_{l=1}^{n}U_{ij}\sigma(V_{jl}X_{lk})\] (8)

Note that for all \(i,k\), the first term depends only on the first \(h\) columns of \(U\) and first \(h\) rows of \(V\), and the second terms depends only on the rest of the columns and rows of \(U\) and \(V\). Denoting the first \(h\) columns of \(U\) as \(U_{1:h}\), the rest of the columns of \(U\) as \(U_{h+1:h^{\prime}}\), the first \(h\) rows of \(V\) as \(V_{1:h}\), and the rest of the rows of \(V\) as \(V_{h+1:h^{\prime}}\), we have

\[L_{mnh^{\prime}k}((U,V),X)=L_{mnhk}((U_{1:h},V_{1:h}),X)+L_{mn(h^{\prime}-h)k }((U_{h+1:h^{\prime}},V_{h+1:h^{\prime}}),X).\] (9)

Let \(\Theta_{1}=\mathbb{R}^{m\times h}\times\mathbb{R}^{h\times n}\) and \(\Theta_{2}=\mathbb{R}^{m\times(h^{\prime}-h)}\times\mathbb{R}^{(h^{\prime}-h) \times n}\). Then \(\Theta(m,h^{\prime},n)=\Theta_{1}\times\Theta_{2}\). Let \(S=(\mathbb{R}^{m\times k}\times\mathcal{D}^{d})\) and \(T=\mathbb{R}^{m\times k}\times\mathbb{R}^{m\times k}\). Define the following three functions

\[h:\Theta_{1}\times\mathcal{D}^{d}\rightarrow(\mathbb{R}^{m\times k }\times\mathcal{D}^{d})\] \[f:\Theta_{2}\times(\mathbb{R}^{m\times k}\times\mathcal{D}^{d}) \rightarrow\mathbb{R}^{m\times k}\times\mathbb{R}^{m\times k}\] \[j:(\Theta_{1}\times(\mathbb{R}^{m\times k}\times\mathbb{R}^{m \times k}))\times\mathcal{D}^{d}\rightarrow\mathbb{R}^{m\times k}\] (10)

by

\[h((U_{1:h},V_{1:h}),X) =(L_{mnhk}((U_{1:h},V_{1:h}),X),X)\] \[f((U_{h+1:h^{\prime}},V_{h+1:h^{\prime}}),(Y,X)) =\big{(}L_{mn(h^{\prime}-h)k}((U_{h+1:h^{\prime}},V_{h+1:h^{ \prime}}),X),Y\big{)}\] \[j(\big{(}(U_{1:h},V_{1:h}),(Y^{\prime},Y)\big{)},X) =Y^{\prime}+Y.\] (11)

Then \(L_{mnh^{\prime}k}(\theta,X)=j\big{(}(\theta_{1},f\big{(}\theta_{2},h(\theta_{ 1},X)\big{)}),X\big{)}\) for all \(\theta=(\theta_{1},\theta_{2})\in\Theta\) and \(X\in\mathcal{D}^{d}\). Since \(L_{mnhk}\) has a symmetry, \(f\) has the same symmetry. By Proposition 3.1, \(L_{mnh^{\prime}k}\) also has the same symmetry. 

The next corollary shows that symmetries of a subset of layers are also symmetries in the entire network. Both corollary can be proved by partitioning the dimensions the parameter space and defining corresponding functions that compose \(L\), before applying Proposition 3.1. Figure 1 shows the subset of parameters (\(\Theta_{2}\)) the symmetry applies to in the corollaries. These are the subnetworks where symmetries are assume to be known, which the larger network inherits.

**Corollary C.2**.: _Let \(\Theta=\Theta_{1}\times...\times\Theta_{l}\) be a parameter space. Consider a list of spaces \(\Phi_{0}=\mathcal{D}^{d}\), \(\Phi_{l}=\mathbb{R}^{d}\), and \(\Phi_{1}\),..., \(\Phi_{l-1}\). Let \(L:\Theta\times\mathcal{D}^{d}\rightarrow\mathbb{R}^{d}\) be a function defined recursively by \(\{L_{i}\}_{i=1}^{l}\) with \(L_{i}:\Theta_{i}\times\Phi_{i-1}\rightarrow\Phi_{i}\), such that \(L=\phi_{l}\) where \(\phi_{i}=L_{i}(\theta_{i},\phi_{i-1})\in\Phi_{i}\) and \(\phi_{0}=X\). If for some \(1\leq i\leq l\), \(L_{i}\) has a \(G\)-symmetry, then \(L\) has a \(G\)-symmetry._

Proof.: Define functions

\[h:(\Theta_{1}\times...\times\Theta_{i-1}\times\Theta_{i+1}\times...\times\Theta_{l})\times\mathcal{D}^{d}\rightarrow\Phi_{i-1}\] \[f:\Theta_{i}\times\Phi_{i-1}\rightarrow\Phi_{i}\] \[j:(\Theta_{1}\times...\times\Theta_{i-1}\times\Theta_{i+1}\times...\times\Theta_{l})\times\Phi_{i}\times\mathcal{D}^{d}\rightarrow\mathbb{R}^{d}\] (12)

by

\[h((\theta_{1},...,\theta_{i-1},\theta_{i+1},...,\theta_{l}),X) =L_{i-1}(\theta_{i-1},X),\quad\text{computed using }(\theta_{1},...,\theta_{i-1})\] \[f(\theta_{i},\phi_{i-1}) =L_{i}(\theta_{i},\phi_{i-1})\] \[j((\theta_{1},...,\theta_{i-1},\theta_{i+1},...,\theta_{l}),\phi_ {i},X) =L_{l}(\theta_{l},X),\quad\text{computed using }(\theta_{l},...,\theta_{i+1})\text{ and }\phi_{i}.\] (13)

Then \(L((\theta_{1},...,\theta_{l}),X)=j\big{(}(\theta_{1},...,\theta_{i-1},\theta_ {i+1},...,\theta_{l}),f(\theta_{i},h((\theta_{1},...,\theta_{i-1},\theta_{i+1},...,\theta_{l}),X)),X\big{)}\) for all \(\theta=(\theta_{1},\theta_{2})\in\Theta\) and \(X\in\mathcal{D}^{d}\). By Proposition 3.1, if \(f=L_{i}\) has a \(G\)-symmetry, \(L\) also has a \(G\)-symmetry.

In addition to obtaining symmetries from those in smaller networks, we can also get symmetries for a loss function over data batches with a certain size, if we know there is a symmetry for this function over larger data batches. Concretely, if there exists a group action that preserves loss for all data batches of size \(d\in\mathbb{Z}^{+}\), then that group action preserves loss for all data batches of size \(d^{\prime}<d\).

**Proposition C.3**.: _Let \(L_{d}:\Theta\times\mathcal{D}^{d}\to\mathbb{R}^{d}\) be a function that is applied pointwise on each of \(d\) data points in a data batch. If \(L_{d}\) admits a \(G\)-symmetry, then \(L_{d^{\prime}}\) admits a \(G\)-symmetry for all \(d^{\prime}<d\)._

Proof.: Suppose that \(L_{d}\) has a \(G\)-symmetry. Let \(a:\mathcal{D}^{d}\to(G\times\Theta\to\Theta),X_{d}\mapsto(a_{X_{d}}:g,\theta \mapsto\theta^{\prime})\) be the corresponding group action. Define \(a^{\prime}:\mathcal{D}^{d^{\prime}}\to(G\times\Theta\to\Theta)\) by \(X_{d^{\prime}}\mapsto(a_{t(X_{d^{\prime}})}:g,\theta\mapsto\theta^{\prime})\), where \(t:\mathcal{D}^{d^{\prime}}\to\mathcal{D}^{d}\) appends \(d-d^{\prime}\) random data points to its input. Clearly, \(a^{\prime}\) satisfies the identity and associate axiom and preserves loss. Therefore, \(a^{\prime}\) is a \(G\)-symmetry of \(L_{d^{\prime}}\). 

## Appendix D Additional Experiment Details

Figure 5: Learned generators for a two-layer linear MLP with parameters dimensions \(W_{2}\in\mathbb{R}^{\mathbb{F}\times 2},W_{1}\in\mathbb{R}^{2\times 1}\) and data \(X,Y\in\mathbb{R}\).