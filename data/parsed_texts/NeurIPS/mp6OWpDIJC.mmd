# Autonomous Agents for Collaborative Task under Information Asymmetry

 Wei Liu\({}^{\star}\)\({}^{\dagger}\) Chenxi Wang\({}^{\star}\)\({}^{\dagger}\) Yifei Wang\({}^{\star}\) Zihao Xie\({}^{\star}\) Rennai Qiu\({}^{\star}\) Yufan Dang\({}^{\star}\)

Zhuoyun Du\({}^{\star}\) Weize Chen\({}^{\star}\) Cheng Yang\({}^{\clubsuit}\) Chen Qian\({}^{\star}\)\({}^{\clubsuit}\)

\({}^{\star}\)Tsinghua University Peng Cheng Laboratory, China

thinkwee2767@gmail.com albertyang33@gmail.com qianc62@gmail.com

###### Abstract

Large Language Model Multi-Agent Systems (LLM-MAS) have greatly progressed in solving complex tasks. It communicates among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents' collaborations are leveraged to perform multi-person tasks, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed _iAgents_, which denotes _Informative Multi-Agent Systems_. In _iAgents_, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. _iAgents_ employs a novel agent reasoning mechanism, _InfoNav_, to navigate agents' communication towards effective information exchange. Together with _InfoNav_, _iAgents_ organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce _InformativeBench_, the first benchmark tailored for evaluating LLM agents' task-solving ability under information asymmetry. Experimental results show that _iAgents_ can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes1.

Footnote 1: Available on https://github.com/thinkwee/iAgents.

\(\dagger\): Equal Contributions.

\(\boxplus\): Corresponding Authors.

\(\dagger\): Corresponding Authors.

\(\dagger\): Corresponding Authors.

## 1 Introduction

There has been notable progress in autonomous agents driven by the Large Language Model (LLM), especially in developing communicative agents for completing collaborative tasks [53, 45, 12, 38, 16, 43, 26, 63, 36], as shown in Figure 0(a). In these multi-agent systems (MAS), multiple agents are created through role-play prompting [25] to imitate the ability of human experts and form a _virtual entity_ (e.g., an agent company or hospital) to provide solutions derived from agents' communication. Agents share context in the _virtual entity_ to facilitate collective decision-making.

Since autonomous communication among agents has achieved significant success in discussing, decomposing, and resolving various complex tasks, the natural idea is to upgrade the tasks for agentsfrom single-person to multi-person, where agents work on behalf of multiple human users and solve the collaboration task among these users. An intuitive solution is to assign each user an agent and perform autonomous collaborations among these agents. However, in such a setting, a new challenge arises. This challenge involves dealing with asymmetry [46, 44] in various types of information (environment, goals, and mind state) [66, 65, 34, 4, 54, 7] since each agent can only observe the information of its human user. Previous LLM-MAS are not suitable for handling this scenario, because 1) human information is sensitive and private, so the asymmetry can not be resolved by directly collecting all information into one place and sharing it as the context for MAS. 2) Human information is dynamic so it can not be easily memorized during pre-training and activated accurately through role-play prompting in MAS to avoid asymmetry. Essentially, agents' cooperation in previous MAS has adopted an introspective approach within the _virtual entity_ (an agent hospital/town/software company), which struggles to deal with asymmetry in human information.

To bridge gaps in such asymmetry, agents need to retrieve information from humans and proactively exchange information, creating a new ecosystem combining the human and the agent network. Therefore, we propose the concept of _iAgents_ (_Informative Multi-Agent Systems_) for achieving this kind of collaboration, as shown in Figure 1b. _iAgents_ utilizes a new agent reasoning method (_InfoNav_) to model the agents' minds and navigate communication among agents toward proactive information exchange. Furthermore, a new memory mechanism is designed to provide agents with accurate and comprehensive information for exchange. Additionally, we introduced _InformativeBench_, the first benchmark evaluating agents' collaboration ability under information asymmetry. It includes both information-seeking tasks within large social networks and algorithm-like reasoning tasks over a small network. Our contributions can be summarized as follows:

1. We raise the research problem of information asymmetry in the multi-agent system for enhancing human collaboration, which is the first to shift the research perspective in this area from a holistic system view to individuals within the system. It gives a new vision to the human-agent collaboration relationship.
2. We propose the _iAgents_ framework to deal with the information asymmetry in a multi-agent system. Equipped with _InfoNav_ and improved memory mechanism, _iAgents_ could perform effective communication and collaboration within a social network (shown in Figure 6) of 140 individuals and 588 relationships, and across over 30 dialogues they searched nearly 70,000 messages and resolved the task within 3 minutes.
3. We introduce the first multi-agent information asymmetry benchmark, _InformativeBench_. Agents with some state-of-the-art LLM backends achieved an average accuracy of 50.48% on _InformativeBench_, with the most challenging task achieving only 22.8% accuracy, which reveals both potential promise and challenges in this direction.

Figure 1: Comparison between previous MAS (left) and _iAgents_ (right). The visibility range of information for each agent is highlighted with a colored background. On the left, all agents share all information (colored background of _Virtual Company_). On the right, each agent could only see information about its human user (separated colored backgrounds), and _iAgents_ is designed to deal with such kind of information asymmetry.

Related Work

**Agents based on Large Language Models (LLMs)** Originate from ancient Greek philosophy, where an "agent" denoted a being capable of intentional action, driven by mental states such as desires and beliefs [40]. As AI progresses, this concept integrates into the simulation and understanding of intelligent behavior [50]. Traditionally, agent research focused on some specific tasks [18; 23; 42], but the emergence of LLMs has shifted this focus. GPT-4, for instance, is recognized for achieving a form of general intelligence [4], prompting exploration into equipping LLMs with agency and intrinsic motivation [53]. Studies introduce frameworks for LLM-based agents, including memorisation [62], decision-making, perception, and action modules, leveraging the inherent autonomy, reactivity, pro-activeness, and social ability of LLMs [48]. Notable applications include utilizing single-agent systems and multi-agent systems for task solving and simulation [38; 16; 43; 26; 63; 31; 37]. However, most of this research has primarily focused on agent capabilities, often neglecting interaction and cooperation paradigms. This highlights a critical research gap [3], warranting further exploration in this area.

**Human-Agent and Multi-Agent Cooperation Paradigms** To ensure that agents align with human objectives [22; 32], human-agent cooperation is crucial. Two main paradigms of human-agent cooperation are the Equal-Partnership and the Instructor-Executor. The former emphasizes agents as communicators who understand human emotions [13], while the latter highlights the human's guiding role, with agents following instructions [10]. However, single-agent systems face limitations, such as the inability to collaborate, learn from social interactions, and function effectively in complex scenarios [41; 28; 53]. Research suggests that multi-agent systems with each agent holding specific functions, can stimulate stronger intelligence [30; 1; 39]. Collaborative multi-agent systems can efficiently handle complex tasks [29; 27; 9]. Recent studies focus on scenarios with information asymmetry among agents. For instance, [66] explores social intelligence among agents achieving private goals based on common scene information. Additionally, [65] develops an evaluation framework to simulate social interactions with LLMs. The study finds that learning from omniscient simulations enhances interaction naturalness but doesn't improve goal achievement in cooperative scenarios. Many real-world scenarios involve information asymmetry, posing challenges for multi-agent systems.

**Reasoning** In previous research, agents are tasked with providing accurate information to human users in human-machine collaboration. Due to the nature of language models, the output of information relies on the user's input and the previously decoded content serving as rationale context. Therefore, some work on reasoning has explored how to improve the organization and expression of this rationale context [47; 58; 2; 8], to enhance the accuracy of output information. However, some research has also found that the reasoning process of LLMs differs from that of humans [56; 20; 5; 33]. For questions relying on internal knowledge within LLMs to answer, agents do not necessarily solve problems step by step like humans. Instead, compared to steps, having context with sufficient information content is more important. For scenarios discussed in this paper, we focus on machine-to-machine communication, relying on external knowledge of LLMs to collaboratively answer questions. This poses different requirements for LLM reasoning, especially in terms of how to promote information flow so sufficient information is included in the context provided to LLM. Agents need to actively [17] and accurately acquire, provide, and ask for information.

## 3 Method

### Problem Formulation

Without loss of generality, we formalize tasks in social networks that require information exchange for collaboration as a Question Answer (QA) task. The rationales \(R\) necessary for answering the question \(Q\) are distributed in different human information (\(I_{1},I_{2}\)) across the social network, which leads to information asymmetry. Consequently, agents (\(A_{1},A_{2}\)) of two individuals are required to collaborate, update the rationale set (\(R_{1},R_{2}\)) that they hold through communication \(C\), and by combining their rationales, they can reason and obtain the answer. The whole process can be formulated as:

\[Ans =Reasoning(Q,R)\] (1) \[R =R_{1}\cup R_{2}\] (2) \[R_{1},R_{2} =C(I_{1},I_{2},A_{1},A_{2})\] (3)

### Overview

As shown in Figure 2, agents need to actively retrieve information from humans and exchange it with other agents. The communication can be represented as:

\[C_{n}=\{U_{1},U_{2},...,U_{n}\}\] (4)

where \(U\) denotes an utterance in the communication \(C\), and \(n\) is the maximum number of communication turns. Agents take turns making utterances to advance towards task resolution. Following the classical definition [59; 53], where agents observe the environment, think to make decisions, and then take action, we can organize agents' communication similarly. Each agent's behavior in one communication turn involves a pipeline of 1) _observing_ the current communication progress \(C\) and their held rationales \(R\), 2) _thinking_ about how to update the rationale to \(R^{new}\) and what \(query\) to make for retrieving information from humans, and 3) _acting_ by retrieving information and making an utterance based on it. This pipeline can be formalized as:

\[U_{i}=\left\{\begin{array}{cc}Act_{A_{1}}(Think_{A_{1}}(Obs^{i}_{A_{1}}))&i \%2==1\\ \\ Act_{A_{2}}(Think_{A_{2}}(Obs^{i}_{A_{2}}))&else\end{array}\right.\] (5)

where

\[Obs^{i}_{A}=\{R,C_{i-1}\}\] (6)

\[Think_{A}(Obs^{i}_{A})=\{query,R^{new}\}\] (7)

\[Act_{A}(Think_{A})=A(query(I))=U\] (8)

To ensure each generated utterance provides valuable information and eliminates asymmetry, how to exchange information and what information to exchange is crucial. To deal with these two questions, we use the _InfoNav_ mechanism to guide communication towards effective information exchange. Furthermore, we introduce the _Mixed Memory_ mechanism which organizes human information into Fuzzy and Clear Memory for accurate and comprehensive retrieval. Additionally, each agent can initiate new communication \(C^{new}\) within their subnetwork, which means the communication \(C\) may be recursive and can diffuse among the social network:

\[C_{n}=\{C_{1}^{new},C_{2}^{new},...,C_{m}^{new},U_{1},U_{2},...,U_{n}\}\] (9)

For example, if Alice's agent wants to collaborate with Bob's agent, Bob's agent might respond "Hold on, I can ask Charlie's agent for help."

### InfoNav

As shown in Equation 6, the agent needs to be aware of its rationale set and ongoing communication to effectively advance the conversation. While the status of the rationale set can be implicitly inferred

Figure 2: Overall architecture of _iAgents_. From left to right, 1) each individual in the social network is equipped with an agent, and 2) two human users invoke their agents to solve a task, each initially holding the information that is visible to its human user. Then 3) agents automatically raise communication and exchange necessary information on behalf of human users. Finally, 4) agents perform a consensus check on their planning completed by _InfoNav_ to solve the task.

from utterances, this inference is often unreliable for LLM Agents. This unreliability can lead to incorrect states and then generate meaningless utterances, such as repetitive questioning or redundant thanking, which makes it harder to infer rationale and creates a vicious cycle. To address this, we propose the _InfoNav_ mechanism. _InfoNav_ plans and tracks the status of the agent's rationale set explicitly for better navigating the communication. Before each utterance, the agent reviews its plan to identify which unknown rationale to inquire about and then updates the plan based on the responses received. Figure 3 shows an example of _InfoNav_ in action. Initially, we prompt the agent to generate a plan \(P\) outlining the rationales needed to answer question \(Q\). Since the agent has no information at the beginning, all rationales in the plan are marked as unknown:

\[P(r_{1}^{u},...,r_{m}^{u})=Prompt(Q)\] (10)

where \(r^{u}\) denotes unknown rationales. During communication, if the agent gets the information of one rationale, it will update the status of this rationale from "unknown" to "known" and fill this information into the rationale placeholder in the planning text. The plan is written in fluent natural language, making it explicit and effective for prompting the model. Therefore, using _InfoNav_, the rationale set \(R\) in equation 6 is rewritten to plan \(P\), and the updated rationale \(R^{new}\) is replaced to the plan with filled rationales \(P(r^{k})\), where \(r^{k}\) represents known rationales:

\[Obs_{A}=\{P(r^{u}),C\}\] (11)

\[P(r^{k})=Think_{A}(Obs_{A})\] (12)

After multiple turns of communication, both sides finish the update of their plans. Agents then unify collected rationales and discard conflicting ones to reach an answer, denoted as "Consensus Reasoning". Thus, equations 1 to 3 rewrite to:

\[Ans =Reasoning(Q,R)\] (13) \[R =Consensus(P_{1}(R_{1}),P_{2}(R_{2}))\] (14) \[P_{1}(R_{1}),P_{2}(R_{2}) =C(I_{1},I_{2},A_{1},A_{2})\] (15)

Previous reasoning methods[47, 58, 2, 8] focused on providing accurate plans. In contrast, _InfoNav_ emphasizes navigating communication and information exchange with plans. The plan in _InfoNav_ can be seen as a generalization of Dialogue Status Tracking (DST)[15, 51, 14] in conventional task-oriented dialogue systems. It also generalizes the concept of software in multi-agent software generation frameworks like ChatDev[38] or MetaGPT [16]. The plan maintains progress in task-solving, guiding agents to share information during communication.

### Mixed Memory

In _iAgents_, agents are navigated by _InfoNav_ to retrieve human information and share it with other agents for collaboration. Retrieval of human information is necessary since 1) human's lifelong information can not be stored in the "long context" (such as 128k tokens) of LLM, and 2) even though the information required for a single-turn conversation can fit into the context, the accumulation of information over multiple turns can lead to context explosion. It is also challenging to organize human information which is diverse in format and complex to understand. We propose organizing

Figure 3: A case of the task asking two agents to find the longest activity among all schedules. _InfoNav_ navigates the communication by providing a plan to the agent. It first 1) asks the agent to make a plan on what information is needed, then 2) fills the placeholder in this plan during communication. Finally it 3) performs a consensus check on the completed plan to 4) get the answer.

human information into two types of agent memories: Clear Memory and Fuzzy Memory. These memories facilitate reactive retrieval, ensuring accurate and comprehensive rationale extraction, as shown in Figure 2.

Clear Memory (\(Mem_{C}\)) stores information in a structured format to facilitate precise retrieval. Clear memory faithfully preserves the original information (\(I\)) and supports accurate retrieval. Additionally, it enables information retrieval from multiple spans across different chat sessions (\(s\)), capturing evolving changes in rationales.

However, Clear Memory's strict exact-match requirements complicate the retrieval process. It also struggles to provide cohesive context. To address these issues, we introduce Fuzzy Memory (\(Mem_{F}\)). Fuzzy memory stores summarized session texts (\(I_{s}\)) and uses embedding-based ANN retrieval [21]. Although both fuzzy memory and reflection [31] produce summary-like text, we emphasize objective summarization of information to facilitate session-level retrieval, rather than subjective generalizations to aid in planning. While this approach may lose some details, it offers a comprehensive context and enables robust, semantic-based retrieval. Therefore, the retrieval action in Equation 8 can be rewritten to involve both memory types:

\[Act_{A_{k}}(Think_{A_{k}}) =A_{k}(query_{k}(I_{k}))\] (16) \[=A_{k}(SQL(Mem_{C}),ANN(Mem_{F}))\] (17)

What's more, the query of these two kinds of memories is decided by agents based on observations of previous executions, which means agents can reactively adjust their queries. Combining these two kinds of memory facilitates agents to cross-verify the retrieved information and provides _InfoNav_ with comprehensive and accurate rationales.

## 4 InformativeBench

To the best of our knowledge, there is no benchmark or dataset tailored for information asymmetry in the collaboration task among communicative agents. In this paper, we construct _InformativeBench_, the first benchmark to evaluate agent collaboration tasks featuring information asymmetry in social networks. It includes two categories with a total of five datasets. Details, including the scale, distribution, and metrics of the datasets, are provided in section C. What's more, recent studies have found that _LLM continuously ingests internet data so static benchmarks can be easily memorized and overfitted_[64, 55, 61]. Hence, two pipelines for constructing _InformativeBench_ are easy to realize and can be generalized to more domains for constant and dynamic evaluations. They are Needle-Oriented and Reasoning-Oriented pipelines, as shown in Figure 4.

**Needle-Oriented Pipeline** A "needle"[11] is inserted into the social network, and agents are tasked with finding this "needle" information. This evaluates their ability to share and locate information. The dataset can be created by splitting the needle and spreading it into the network, or by collecting pieces from the network and combining them. For the split method, the _Needle in the Persona (NP)

Figure 4: Two kinds of tasks in the _InformativeBench_. Each agent can only see the information (marked with different colors) of the human that it works on behalf of, which generates information asymmetry. Agents are 1) asked to find the needle information within the network or 2) reason to get an answer which is the output of an algorithm running on distributed information in the network.

dataset modifies the dialogue in the SPC dataset[19] by adding a common or opposite persona to two individuals' personas. Agents are asked to find this persona. For the combination method, the _FriendsTV_ dataset reconstructs the social network from the entire Season 1 script of _Friends_[49], involving 140 characters with 588 relationships, and combines two questions in the FriendsQA dataset [57; 24] as "needle pieces" to generate new question. This dataset, the largest in _InformativeBench_, features sarcasm, plot twists, and complex relationships for simulating real-world challenges.

**Reasoning-Oriented Pipeline** Humans are assigned different pieces of information, which serve as inputs for an algorithm (such as sorting or merging). Agents must reason to get the answer which is the algorithm's output. Therefore, the algorithm serves as an automatic verifier for information asymmetric reasoning. In _InformativeBench_, this is represented by the _Schedule_ dataset, which develops a program for assigning different schedules to individuals. Agents are presented with algorithmic problems of varying difficulties, and the program automatically verifies the correctness of their solutions. The datasets include questions of three levels of difficulty: _Easy_) calculate the number of conflicting schedules between two people, _Medium_) find the longest activity among six people, and _Hard_) find the longest common free period among six people.

## 5 Experimental Setup

We generically treat chat histories as human information. This approach simplifies modeling information asymmetry in social networks. Other types of information, such as knowledge bases, documents, or web content, can all be organized in mixed memory so _iAgents_ is adaptable to all these kinds of information. We conduct all experiments with a maximum of 10 communication turns for agents. The experiments use gpt-4-0125-preview, gpt-3.5-turbo-16k, gemini-1.0-pro-latest, and Claude-sonnet 2 as LLM backends. The temperature is set to 0.2. For Fuzzy Memory, we use gpt-4-0125-preview to summarize session text and OpenAI text-embedding-3-small to generate embeddings for ANN embedding search. We use precision as the metric for questions in the NP, ScheduleEasy, and FriendsTV datasets. For the ScheduleMedium and ScheduleHard datasets, we use F1 and IoU as the metrics, corresponding to the algorithm used. Details about the metrics are shown in Section C.3. For the Schedule and NP datasets, we do not activate mixed memory since the information scale is small and can be fully loaded in the LLM context. Additionally, for the Schedule dataset, we do not activate the agent's ability to initiate new communication due to the small scale of the social network.

Footnote 2: as of 20240501.

## 6 Result

### _InformativeBench_ Evaluations

We first comprehensively assessed the performance of _iAgents_ using some state-of-the-art LLMs on _InformativeBench_, as shown in Table 1. GPT-4 achieves over 50% accuracy across most datasets, indicating its potential to work on behalf of humans for cooperation. However, smaller-scale LLMs still face significant challenges in solving cooperation problems in information asymmetry. Most models could only achieve about 50% precision on the easiest NP task. For the Schedule dataset, as questions become harder, performance drops, with most models solving less than 20% of the hardest questions. The FriendsTV dataset introduces a large social network, requiring agents to use external memory to retrieve rationale from extensive human information. Most LLMs struggle to exceed 40%

\begin{table}
\begin{tabular}{c|c c c|c c} \hline \hline \multirow{2}{*}{**LLM Backend**} & \multicolumn{2}{c|}{**Reasoning-Oriented (Schedule Dataset)**} & \multicolumn{2}{c}{**Needle-Oriented**} \\  & **Easy** & **Medium** & **Hard** & **NP** & **FriendsTV** \\ \hline GPT 4 & 56.67\% & 51.00\% & 22.80\% & 64.00\% & 57.94\% \\ GPT 3.5 & 36.67\% & 18.00\% & 12.25\% & 51.00\% & 35.71\% \\ Claude Sonnet & 43.33\% & 17.44\% & 18.66\% & 50.00\% & 34.13\% \\ Gemini 1.0 & 26.67\% & 22.33\% & 14.40\% & 40.00\% & 28.57\% \\ \hline \hline \end{tabular}
\end{table}
Table 1: Evaluation results of _iAgents_ on _InformativeBench_ with different LLM backends.

accuracy in this dataset. Thus, while previous studies show impressive performance when agents are omniscient, collaborating in information asymmetry remains challenging.

### Ablation Study

We conducted ablation experiments on several key designs of the _iAgents_ framework, as detailed in Table 2. Analyzing the FriendsTV dataset revealed that incorporation of the mixed memory mechanism led to a performance increase ranging from 2.38% to 6.34%, surpassing the impact of _InfoNav_, which resulted in only a 0.8% performance increase. This discrepancy underscores the greater significance of effective retrieval over reasoning during communication in large social networks with mass information. Notably, the ablation of both memory mechanisms emphasized the indispensability of mixed memory. The introduction of recursive communication exhibited the most significant performance gain (12.7%), primarily due to the challenges posed by the vast social network in the FriendsTV dataset. By actively introducing new communications within ongoing dialogues, agents could acquire and corroborate information, thus significantly enhancing performance. This highlights the imperative of scalability in our proposed framework for addressing real-world problems.

For the NP and Schedule datasets, the main challenge lies in facilitating effective multi-turn communication to exchange information for reasoning. Therefore, _InfoNav_ emerged as pivotal in enhancing performance, resulting in performance increases ranging from 15% to 26%. When agents relied solely on initialized prompts to navigate multi-turn communication, they struggled to exchange information effectively to accomplish tasks. This deficiency was particularly evident in datasets like Schedule, which emphasize logical reasoning and computation. Across all difficulty levels, agents without the _InfoNav_ mechanism failed to achieve accuracy exceeding 10%.

### Analysis on Agents' Behaviour

_InfoNav_**Behaviour** We examined how agents utilize _InfoNav_ for information exchange during multi-turn communication. Notably, we calculated the average number of unknown rationales solved each time _InfoNav_ updated the plan and the proportion of rationales passed in consensus reasoning. Moreover, some rationales were solved in a "Fake Solved" hallucination, where agents filled in the rationale as "solved, which is unknown". We also documented the frequency of such occurrences. Table 3 shows that agents who propose fewer rationales to seek and achieve a higher solved ratio are more likely to accomplish the task. Interestingly, agents often fill multiple rationales concurrently rather than sequentially. Those agents with higher instances of synchronous completions suggest a deeper understanding of the task and greater confidence in filling rationales. Furthermore, the occurrence of Fake Solved instances is lower among agents who predict tasks correctly. The consensus ratio is also higher when agents successfully complete the task. It denotes that the information obtained by the two collaborating agents is relatively accurate and free of contradictions, thus increasing the likelihood of arriving at the correct conclusion through their final reasoning. Besides, we observed that agents not only propose rationales but also task states, such as the completion status of specific actions. The completion rates of these rationales and states are positively correlated with task success. In essence, the utilization of _InfoNav_ by agents mirrors human intuition, emphasizing first careful planning, then proactive and accurate information exchange.

\begin{table}
\begin{tabular}{l|c c c|c c} \hline \hline \multirow{2}{*}{**Experiment**} & \multicolumn{3}{c|}{**Reasoning-Oriented (Schedule Dataset)**} & \multicolumn{3}{c}{**Needle-Oriented**} \\  & **Easy** & **Medium** & **Hard** & **NP** & **FriendsTV** \\ \hline iAgents (Full Model) & 36.67\% & 18.00\% & 12.25\% & 51.00\% & 35.71\% \\ \hline _Ablation on Info/Nav:_ & & & & & \\ \hline w/o InfoNav & 10.00\% & 3.56\% & 7.34\% & 39.00\% & 34.92\% \\ \hline _Ablation on other mechanisms (Limited Applicability):_ & & & & \\ \hline w/o Recursive Comm & – & – & – & 48.00\% & 23.02\% \\ w/o Fuzzy Memory & – & – & – & – & 29.37\% \\ w/o Clear Memory & – & – & – & – & 33.33\% \\ \hline \hline \end{tabular}
\end{table}
Table 2: Ablation study on _iAgents_. Dashes (–) indicate: (1) _iAgents_ on Reasoning-Oriented dataset does not equip other mechanisms, hence no ablation needed; (2) For NP dataset, _iAgents_ does not utilize Mixed Memory hence there is no ablation.

**Memory Behaviour** Similarly, we explored how agents adapt their memory retrieval strategies during communication. We examined three parameters in clear memory queries: the context window, which determines the breadth of contextual messages; the total message retrieval limit; and the size of the query keywords set. For fuzzy memory, we analyzed two parameters: the number of queried responses (topk) and the length of the query text. These findings are illustrated in Figure 5. Our analysis revealed several notable trends. The majority of agents do not change their behavior during communication. However, when agents decide to change their behavior, we observed that they tended to increase the amount of retrieved information over time. This augmentation trend was particularly pronounced on the overall message retrieval limit, where the frequency of "increase" actions surpassed that of "decrease" actions by nearly threefold. Furthermore, agents who completed tasks exhibited a more conservative approach, with a lower proportion of behavioral changes compared to agents unable to complete tasks. This phenomenon may be attributed to the difficulty of certain tasks, making agents continuously refine their strategies in pursuit of the required information.

### Analysis on Real World Concern

We studied two significant challenges in extending the _iAgents_ to real-world applications. Firstly, we investigated whether the agent can effectively respond to human input without being overly influenced by factual knowledge obtained during pre-training [52, 35]. Secondly, we explored the agent's ability to engage in communication while upholding human privacy. Our experiments were conducted using the GPT3.5 model on the FriendsTV dataset.

**Prior Distraction** The FriendsTV contains information that could be memorized by LLM from the Internet, hence it is perfect for analyzing prior distractions. We anonymized the names of the primary characters in the dataset, for example, renaming "Rachel" to "Alice". The performance of the agents on this anonymized dataset decreased from 35.71% to 32.54%, suggesting that to some extent, agents can reason based on user-provided information rather than solely relying on knowledge memorized in pre-training. It may need further advancements, such as model unlearning [60], to fully address this issue.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline \multirow{2}{*}{**Sample**} & **\#Rationales** & **\#Rationales Solved** & **Rationales Solved** & **Rate Solved** & **Consensus** \\  & **in**_InfoNav_ & **per Update** & **Ratio** & **Ratio** & **Ratio** \\ \hline Predict Right & 5.29 & 2.04 & 84.75\% & 3.49\% & 70.52\% \\ Predict Wrong & 5.63 & 1.69 & 67.23\% & 5.40\% & 62.70\% \\ All & 5.45 & 1.87 & 76.22\% & 4.42\% & 66.20\% \\ \hline \hline \end{tabular}
\end{table}
Table 3: Analysis _InfoNav_ behaviour on the trajectory of _iAgents_ using GPT4 as backend. When agents successfully complete the task, the static collected from their trajectory proves that they better utilize the _InfoNav_ mechanism, since the rationale solved ratio, synchronous completions of rationales, and consensus ratio are higher, and present fewer fake solved hallucinations.

Figure 5: The figure depicts the distribution of different behaviors of agents in adjusting memory retrieval based on the progress of communication. Agents predominantly tend to maintain parameters unchanged, but when changes occur, they tend to increase parameters to gain more information.

**Privacy Concern** In investigating whether agents can communicate without compromising privacy, we conducted an experiment involving modifications to the agent's system prompt, emphasizing the importance of privacy preservation in utterances. The agent then utilized vague expressions such as "somebody/somewhere" and disclosed only relevant entity information. This adjustment led to a performance drop from 35.71% to 30.95%, indicating the ongoing challenge of achieving collaboration while ensuring privacy. It's important to note that we solely adjusted privacy settings on the output side, rather than restricting agent access to human information on the input side. This decision was made because setting access permissions might inadvertently reveal prior task-related information. Thus, the real challenge lies in appropriately regulating access to information based on task requirements, akin to teaching the agent to retrieve necessary information accurately. Additionally, absolute privacy protection is impractical, as absolute privacy protection amounts to forgoing problem-solving through collaboration.

## 7 Limitations

While the _iAgents_ framework introduces innovative multi-agent collaboration, it has several limitations and challenges. **Privacy Issues**: As discussed in Section 6.4, we examined the performance of agents communicating and collaborating under privacy constraints, highlighting the trade-off between privacy and collaboration. We define three privacy levels. L1: Users fully share personal information, allowing maximum efficiency for _iAgents_. L2: Users keep their personal information private. _iAgents_ can handle this situation by deploying an edge-side small language model agent for information acquisition. L3: Users demand maximal privacy, with both personal and agent communication handled locally on private devices, which is still a challenge for small language model agents. **Network Modeling**: The current framework initiates communications based on user relevance but lacks nuanced modeling of human social networks and collaboration history. Enhancing network topology through added or removed nodes and incorporating past interactions could improve communication efficiency. **Human-Agent Interaction**: Although _iAgents_ target full autonomy, human involvement for verification remains necessary in real-world scenarios. Processes must be designed to prompt user feedback and adjust agent strategies accordingly, ensuring alignment with user preferences. **Cost**: The high input token consumption (about 30,000 tokens per task) required for _iAgents_ to handle human information is a challenge. However, advancements in long-context models, which extend input token length, present opportunities to reduce the cost of scaling _iAgents_.

## 8 Conclusion

This paper revisits the ecological role of agents within human society, where agents act on behalf of humans in communication to complete collaborative tasks. A primary focus lies in addressing the challenge of information asymmetry. We introduce a novel paradigm for designing multi-agent systems, termed _iAgents_, for addressing information asymmetry. Furthermore, we introduce a benchmark to evaluate the agents' collaboration ability under information asymmetry thoroughly. Going forward, we aim to confront several key challenges to successfully implement this system in the real world for augmenting human productivity, including deploying lightweight models at the edge to address privacy concerns and devising new Human-Computer Interaction paradigms for autonomous and controllable communication among agents, etc. _iAgents_ does not role-play to replace human experts but consistently attributes the value of information to humans and we believe it can facilitate the productivity of human society within a secure and controllable framework.

## 9 Acknowledgements

The work was supported by the Postdoctoral Fellowship Program of CPSF under Grant Number GZB20230348 and the Tencent Rhino-Bird Focused Research Program. We wish to express our profound appreciation to Professor Zhiyuan Liu and Professor Maosong Sun from the Department of Computer Science at Tsinghua University for their detailed guidance and critical insights, which have been instrumental to the success of this work.

## References

* [1] P. G. Balaji and D. Srinivasan. _An Introduction to Multi-Agent Systems_, pages 1-27. Springer Berlin Heidelberg, Berlin, Heidelberg, 2010.
* [2] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 17682-17690, 2024.
* [3] Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. _arXiv preprint arXiv:2304.05332_, 2023.
* [4] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* [5] Changyu Chen, Xiting Wang, Ting-En Lin, Ang Lv, Yuchuan Wu, Xin Gao, Ji-Rong Wen, Rui Yan, and Yongbin Li. Masked thought: Simply masking partial reasoning steps can improve mathematical reasoning learning of language models. _arXiv preprint arXiv:2403.02178_, 2024.
* [6] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In _The Twelfth International Conference on Learning Representations_, 2024.
* [7] Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence. _arXiv preprint arXiv:2407.07061_, 2024.
* [8] Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought generation. _arXiv preprint arXiv:2311.04254_, 2023.
* [9] Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan Dang, Weize Chen, and Cheng Yang. Multi-agent software development through cross-team collaboration. _arXiv preprint arXiv:2406.08979_, 2024.
* [10] Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn. _CoRR_, abs/2306.08640, 2023.
* Pressure Testing LLMs.
* [12] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges. _arXiv preprint arXiv:2402.01680_, 2024.
* [13] Masum Hasan, Cengiz Ozel, Sammy Potter, and Ehsan Hoque. Sapien: Affective virtual agents powered by large language models*. In _2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)_. IEEE, September 2023.
* [14] Michael Heck, Carel van Niekerk, Nurul Lubis, Christian Geishauser, Hsien-Chin Lin, Marco Moresi, and Milica Gasic. Trippy: A triple copy strategy for value independent neural dialog state tracking. In _21th Annual Meeting of the Special Interest Group on Discourse and Dialogue_, page 35, 2020.
* [15] Matthew Henderson, Blaise Thomson, and Steve Young. Word-based dialog state tracking with recurrent neural networks. In _Proceedings of the 15th annual meeting of the special interest group on discourse and dialogue (SIGDIAL)_, pages 292-299, 2014.
* [16] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. _arXiv preprint arXiv:2308.00352_, 2023.

* [17] Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, and Bryan Hooi. Uncertainty of thoughts: Uncertainty-aware planning enhances information seeking in large language models. _arXiv preprint arXiv:2402.03271_, 2024.
* [18] Francois F. Ingrand, Michael P. Georgeff, and Anand S. Rao. An architecture for real-time reasoning and system control. _IEEE Expert: Intelligent Systems and Their Applications_, 7(6):34-44, dec 1992.
* [19] Pegah Jandaghi, XiangHai Sheng, Xinyi Bai, Jay Pujara, and Hakim Sidahmed. Faithful persona-based conversational dataset generation with large language models. _arXiv preprint arXiv:2312.10007_, 2023.
* [20] Mingyu Jin, Qinghai Yu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, et al. The impact of reasoning step length on large language models. _arXiv preprint arXiv:2401.04925_, 2024.
* [21] Jeff Johnson, Matthijs Douze, and Herve Jegou. Billion-scale similarity search with gpus. _IEEE Transactions on Big Data_, 7(3):535-547, 2019.
* [22] Zachary Kenton, Tom Everitt, Laura Weidinger, Jason Gabriel, Vladimir Mikulik, and Geoffrey Irving. Alignment of language agents. _CoRR_, abs/2103.14659, 2021.
* [23] Douglas B Lenat. Enabling agents to work together. _Communications of the ACM_, 37(7):126-142, 1994.
* [24] Changmao Li and Jinho D Choi. Transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 5709-5714, 2020.
* [25] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbulilin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large language model society. _Advances in Neural Information Processing Systems_, 36, 2024.
* [26] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and Yang Liu. Agent hospital: A simulacrum of hospital with evolvable medical agents. _arXiv preprint arXiv:2405.02957_, 2024.
* [27] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. _arXiv preprint arXiv:2305.19118_, 2023.
* [28] Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Diyi Yang, and Soroush Vosoughi. Training socially aligned language models on simulated social interactions. In _The Twelfth International Conference on Learning Representations_, 2023.
* [29] Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. _arXiv preprint arXiv:2307.04738_, 2023.
* [30] Marvin Minsky. _Society of mind_. Simon and Schuster, 1988.
* [31] Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, pages 1-22, 2023.
* [32] Metty Paul, Leandros Maglaras, Mohamed Amine Ferrag, and Iman Almomani. Digitization of healthcare sector: A study on privacy and security concerns. _ICT Express_, 9(4):571-588, 2023.
* [33] Jacob Pfau, William Merrill, and Samuel R Bowman. Let's think dot by dot: Hidden computation in transformer language models. _arXiv preprint arXiv:2404.15758_, 2024.
* [34] David Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? _Behavioral and brain sciences_, 1(4):515-526, 1978.

* [35] Siya Qi, Yulan He, and Zheng Yuan. Can we catch the elephant? the evolvement of hallucination evaluation on natural language generation: A survey. _arXiv preprint arXiv:2404.12041_, 2024.
* [36] Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Zihao Xie, YiFei Wang, Weize Chen, Cheng Yang, Xin Cong, Xiaoyin Che, Zhiyuan Liu, and Maosong Sun. Experiential co-learning of software-developing agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5628-5640, Bangkok, Thailand, August 2024. Association for Computational Linguistics.
* [37] Chen Qian, Jiahao Li, Yufan Dang, Wei Liu, YiFei Wang, Zihao Xie, Weize Chen, Cheng Yang, Yingli Zhang, Zhiyuan Liu, et al. Iterative experience refinement of software-developing agents. _arXiv preprint arXiv:2405.04219_, 2024.
* [38] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communative agents for software development. In _The 62nd Annual Meeting of the Association for Computational Linguistics_, 2024.
* [39] Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Scaling large-language-model-based multi-agent collaboration. _arXiv preprint arXiv:2406.07155_, 2024.
* [40] Markus Schlosser. Agency. In Edward N. Zalta, editor, _The Stanford Encyclopedia of Philosophy_. Metaphysics Research Lab, Stanford University, Winter 2019 edition, 2019.
* [41] Significant Gravitas. AutoGPT.
* [42] Richard S Sutton, Andrew G Barto, et al. Reinforcement learning. _Journal of Cognitive Neuroscience_, 11(1):126-134, 1999.
* [43] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning. _arXiv preprint arXiv:2311.10537_, 2023.
* [44] Michael Tomasello. _The cultural origins of human cognition_. Harvard university press, 2009.
* [45] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. _Frontiers of Computer Science_, 18(6):1-26, 2024.
* [46] Max Weber. _Max Weber: Selections in Translation_. Cambridge University Press, 1978.
* [47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 24824-24837. Curran Associates, Inc., 2022.
* [48] Lilian Weng. Llm-powered autonomous agents. _lilianweng.github.io_, Jun 2023.
* [49] Wikipedia contributors. Friends (tv series) -- Wikipedia, the free encyclopedia, 2024.
* [50] Michael Wooldridge and Nicholas R. Jennings. Intelligent agents: theory and practice. _The Knowledge Engineering Review_, 10(2):115-152, 1995.
* [51] Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher, and Pascale Fung. Transferable multi-domain state generator for task-oriented dialogue systems. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 808-819, 2019.
* [52] Kevin Wu, Eric Wu, and James Zou. How faithful are rag models? quantifying the tug-of-war between rag and llms' internal prior. _arXiv preprint arXiv:2404.10198_, 2024.

* [53] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. The rise and potential of large language model based agents: A survey. _CoRR_, abs/2309.07864, 2023.
* [54] Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, and Yulan He. Opentom: A comprehensive benchmark for evaluating theory-of-mind reasoning capabilities of large language models. _arXiv preprint arXiv:2402.06044_, 2024.
* [55] Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in large language models. _arXiv preprint arXiv:2404.18824_, 2024.
* [56] Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language models latently perform multi-hop reasoning? _arXiv preprint arXiv:2402.16837_, 2024.
* [57] Zhengzhe Yang and Jinho D Choi. Friendsqa: Open-domain question answering on tv show transcripts. In _Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue_, pages 188-197, 2019.
* [58] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 11809-11822. Curran Associates, Inc., 2023.
* [59] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In _International Conference on Learning Representations (ICLR)_, 2023.
* [60] Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. In _Socially Responsible Language Modelling Research_, 2023.
* [61] Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, et al. A careful examination of large language model performance on grade school arithmetic. _arXiv preprint arXiv:2405.00332_, 2024.
* [62] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism of large language model based agents. _arXiv preprint arXiv:2404.13501_, 2024.
* [63] Zhiling Zheng, Oufan Zhang, Christian Borgs, Jennifer T Chayes, and Omar M Yaghi. Chatgpt chemistry assistant for text mining and the prediction of mof synthesis. _Journal of the American Chemical Society_, 145(32):18048-18062, 2023.
* [64] Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. Don't make your llm an evaluation benchmark cheater. _CoRR_, abs/2311.01964, 2023.
* [65] Xuhui Zhou, Zhe Su, Tiwalayo Eisape, Hyunwoo Kim, and Maarten Sap. Is this the real life? is this just fantasy? the misleading success of simulating social interactions with llms. _arXiv preprint arXiv:2403.05020_, 2024.
* [66] Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, and Maarten Sap. SOTOPIA: Interactive evaluation for social intelligence in language agents. In _The Twelfth International Conference on Learning Representations_, 2024.

FriendsTV Social Network Visualization

Figure 6 illustrates the social network reconstructed from the plot of the entire first season of the Friends TV series in the FriendsTV dataset. _Friends_ revolves around six main characters: Ross, Rachel, Monica, Joey, Phoebe, and Chandler. In the social network, these six protagonists are the important nodes with the most connections. Edges connecting them are displayed in different colors. The entire social network comprises 140 nodes and 588 edges, with an average node degree of 4.243, a network diameter of 6, and an average path length of 2.189. Similar to real-world social networks, this network is highly sparse. Many characters may appear in the same scene without interacting with each other, resulting in a network density of 0.061, which brings challenges for resolving information asymmetry.

## Appendix B Notations

Table 4 presents a comprehensive list of all symbol notations employed in this paper, encompassing those utilized in the formalized description of the methodology as well as in the ablation and analysis experiments.

## Appendix C _InformativeBench_ Details

### Question Distribution

Figure 7 presents the distribution of problem types across the three datasets in _InformativeBench_. The majority of the questions in _InformativeBench_ are of the "What" and "Who" types, which have objective ground truth and lack ambiguity. In the Schedule dataset, questions are categorized into three difficulty levels, with each difficulty level corresponding to a different type of question: "What", "How Many", and "How Long".

Figure 6: The visualization of social network in FriendsTV dataset. The connection of the six main characters is labeled with different colors.

### Question Sample

Table 5 provides examples of problems from five datasets:

\begin{table}
\begin{tabular}{c c} \hline \hline Notation & Definition \\ \hline \(Q\) & Question \\ \(Ans\) & Answer to the question \\ \(R\) & Full rationale set to answer the question \\ \(A\) & Agents \\ \(C\) & Communication among agents \\ \(U\) & Utterance in the communication \\ \(R_{1},R_{2}\) & Rational subset hold by agents \\ \(R^{new}\) & Updated rationale set \\ \(I\) & Information from human \\ \(query\) & query to retrieve human information \\ \(Act\) & Action taken by Agents \\ \(Think\) & Think process taken by Agents \\ \(Obs\) & Observation from Agents \\ \(P\) & Agent’s Planning \\ \(r^{u}\) & Unknown Rationales in the Plan \\ \(r^{k}\) & Known Rationales in the Plan \\ \(Mem_{D}\) & Distinct Memory \\ \(Mem_{F}\) & Fuzzy Memory \\ \hline \hline \end{tabular}
\end{table}
Table 4: Main notations used in this paper.

\begin{table}
\begin{tabular}{c|c} \hline \hline Dataset & Question Sample \\ \hline NP & What fantasy series does Alice enjoy that Dave is indifferent about? \\ \hline Schedule & Calculate how many activities need to be deleted at least so that there are no overlapping \\ Easy & activities between you and me? \\ \hline Schedule & Please find out the activity with longest duration on the schedule of all people \\ Medium & Please find out when all our friends can join together today and list all free time spans. \\ \hline Schedule & Who is concerned about the impact of the blackout on their family, given the context of a \\ Hard & widespread power outage affecting Manhattan? \\ \hline \hline \end{tabular}
\end{table}
Table 5: Question sample in the _InformativeBench_.

Figure 7: The distribution of question types in the _InformativeBench_.

1. **Needle in the Persona**. In a segment of multi-party casual conversation among Alice, Bob, Charlie, and Dave, "needle information" related to a fantasy series is inserted. Questions are then posed to Bob and Dave's agents to identify this needle information.
2. **Schedule**. Each person is assigned a daily schedule. Questions of varying difficulty require agents to collaborate to discuss overlapping schedules for two human users, the longest schedule among multiple human users, and common free time for multiple human users.
3. **FriendsTV**. Based on questions from the FriendsQA data, new questions are synthesized. For instance, there is a scene in the third act of the seventh episode of the first season of Friends where a blackout occurs. Agents need to combine the rationales and answers to the questions "Where did the blackout happen?" and "Who was worried about grandmother being affected by the blackout?" to locate this scene in the script of the first season and find the relevant characters.

### Metrics

In this section, we outline the evaluation metrics for all datasets and how to automate the evaluation process.

1. **Needle in the Persona**. We use **accuracy** to evaluate the agents' performance on the Needle in the Persona dataset, defined as the number of correctly answered questions divided by the total number of questions. All questions are in the form of "what," "where," and "who," hence the ground truth is objectively unique. However, due to potential variations in the expression of names, locations, or other nouns, and the possibility that the agent's response may be a complete sentence containing reasoning or additional information, it is impractical to determine correctness through exact matches. We utilize GPT-4 to judge whether the agent's prediction aligns with the ground truth. It is important to note that, unlike other methods using LLMs for evaluation, we do not rely on GPT-4's own knowledge to determine the correctness of answers since we have an objective ground truth. GPT-4 is merely used to assess whether the agent's prediction and the ground truth refer to the same entity. We also manually verify GPT-4's judgments to ensure they align perfectly with human evaluations. The GPT-4 evaluation prompt is as follows: _You are an experienced human labeler for reading comprehension tasks. Given a ground truth answer and a model prediction, you have to judge whether the model prediction is correct. The question is [question]. The ground truth answer is {ground_truth}. The model prediction is [prediction]. Return 1 if the model prediction is correct else 0. the model prediction may be a little different on the expression, as long as the meaning or key entity is correct, the answer can be regarded as correct._
2. **Schedule**. In the Schedule dataset, we define metrics based on specific algorithmic problems rather than simple correctness judgments, providing more continuous metrics to evaluate agents' abilities in finer granularity. 1. ScheduleEasy. Under this difficulty level, agents are required to determine the minimum number of activities to delete to resolve scheduling conflicts between two human users' calendars, returning a numerical value. We also use **accuracy** for evaluation but normalize the agent's response through regularization and GPT-4 prompting to extract the specific numerical value. The agent's response is considered correct only when the numerical value matches exactly. 2. ScheduleMedium. Agents are tasked with identifying the longest activities in each person's schedule, and in dataset configurations, there are often multiple longest activities. Agents need to return all possible names of the longest activities. We employ the **F1 score** for evaluation, as agents may miss some activities or erroneously recall others. Additionally, we use GPT-4 prompting to normalize the agent's response, mapping activity names mentioned in their response to a uniform representation present in the entire set of activities. 3. ScheduleHard. Agents are required to identify the free time slots for all individuals and enumerate them. To compare multiple time slots in the ground truth and agents' predictions, we analogize to the **Intersection-over-Union (IoU)** metric used in the computer vision domain for object detection, generalizing it to one-dimensional time slots. We calculate the ratio of the intersection duration between the predicted time slots and the ground truth time slots to the union duration. For example, if the predicted time slot is from 9 AM to 12 PM and the ground truth time slot is from 10 AM to 2 PM, then IoU is the duration of 2 hours divided by the duration of 5 hours, resulting in 0.4.
3. **FriendsTV**. The metrics design for the FriendsTV dataset is identical to Needle in the Persona, employing **accuracy** as the evaluation metric, as they share the same question types.

### Benchmark Statistic

Table 6 presents detailed statistics of five datasets in _InformativeBench_, including the number of question-answer pairs and the scale of social networks. We utilize the FriendsTV dataset to simulate real-world challenges, providing a large-scale social network to test agents' writing abilities. The other datasets simulate smaller social networks, focusing on enabling agents to exchange information to solve complex reasoning tasks. The difficulty for agents in collaborating within human social networks lies not only in the scale of the social network (information acquisition) but also in effective communication (information exchange). Therefore, we designed datasets of varying scales and difficulties to comprehensively evaluate agents. As the social networks in datasets other than FriendsTV are relatively simple with limited information, we did not enable the MixedMemory mechanism in experiments with these datasets.

## Appendix D _InformativeBench_ Pipeline

### Needle-Oriented

#### d.1.1 Needle in the Persona Pipeline

The SPC dataset [19] is a dialogue dataset based on LLM. Researchers construct and sample the personas of both individuals in the dialogue, then prompt the LLM to generate coherent conversations that are faithful with these personas. Here, a persona refers to the background information of a individual, such as experiences, interests, occupations, demographic attributes, etc. We use Split-Needle pipeline to split a new persona into two sentences then add it to the dialogues in SPC dataset to construct _Needle in the Persona_ dataset. An example is shown in Figure 8 and 9.

Specifically, we randomly select two sets of dialogues, involving four individuals, to construct a small social network. Additionally, we randomly choose a new persona as the needle information to be injected into two individuals' profile within this social network, and modify the dialogues to reflect this persona injection. Finally, we have the agents verify each other's dialogue information to identify the needle information. The detailed pipeline is as follows:

1. Two samples are selected from the original SPC dataset, comprising two sets of dialogues (e.g., Alice-Bob, Charlie-Dave).
2. A new shared persona is added for Alice and Dave, for example: 1. They both recently developed an interest in fishing. 2. Alternatively, contrasting personas can be added, such as Alice enjoys eating vegetables, while Dave strongly dislikes them.
3. Modify the dialogues of Alice-Bob and Charlie-Dave to reflect the addition of the new persona mentioned above.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline Dataset & Needle in the Persona & Schedule Easy & Schedule Medium & Schedule Hard & FriendsTV \\ \hline Pipeline & Needle & Reasoning & Reasoning & Reasoning & Needle \\ \#QA & 100 & 30 & 30 & 30 & 222 \\ \#Individuals & 4 & 4 & 6 & 6 & 140 \\ \#Relationships & 5 & 3 & 5 & 588 \\ Need External Memory & No & No & No & No & Yes \\ Metrics & Precision & Precision & F1 & IoU & Precision \\ \hline \hline \end{tabular}
\end{table}
Table 6: Statistic of _InformativeBench_.

Figure 8: Example of Needle in the Persona dataset, part 1.

[MISSING_PAGE_FAIL:20]

4. Have Bob and Charlie common their respective agents and initiate an automatic communication to identify this persona.

#### d.1.2 FriendsTV Pipeline

Similar to the Needle in the Persona dataset, the FriendsTV dataset also focuses on querying Needle Information injected within social networks. Derived from the transcripts of the first season of the TV show "Friends", the FriendsTV dataset generate a dialogues dataset among characters in TV series. If two characters engage in dialogue during the first season, they are considered friends, thus automatically constructing a large-scale social network.

Unlike the Needle in the Persona dataset, we do not split needle information in the FriendsTV dataset. Instead, we synthesize a new Needle information question and answer based on two questions from the FriendsQA [57] dataset. The context, questions, and answers were manually annotated through crowdsourcing by the authors of the original FriendsQA paper. This was a remarkable project that spanned several years. We are very grateful for the contributions of the authors of FriendsQA. The original Friends script is publicly available online and can be accessed through multiple channels (GitHub, Kaggle). An example of our FriendsTV dataset is illustrated in Figure 10. Specifically,

1. **Annotate Script.** We divide each episode script into scenes then query GPT4 to annotate the listener of each utterance. We manually check with GPT assistance to make sure the correctness of listener labeling. At last, we explodes the utterance into 1v1 dialogues (e.g a utterance from A, B to C, D would be exploded into four utterances between AC, AD, BC, BD).
2. **Normalization**. We norm the abbr name back to the full name ("mon" -> "monica") and replace pronoun ("her father" -> "Rachel's father"). Some utterances have multiple listeners that be labeled as "A/B", "A&B", "A and B", "A, B". We explode all these utterance with multiple utterances having single listener. The labeled listener "ALL" is replaced to all characters in this scene except. We also manually check and remove some notes that accidentally transformed into the scripts (such as some introduction on the speaker/listener)
3. **Generate Needle Information**. Two questions with the same context and different participants in the FriendsQA dataset are chosen. We prompt GPT4 to generate a new QA based on these two questions. The new question needs the answers of two original questions to reason and since the participants are different for original questions, the information hence is in asymmetry.

### Reasoning-Oriented

The Reasoning-Oriented pipeline leverages algorithmic problems to assess the cooperative ability of agents. Given an algorithm, such as divide and conquer, dynamic programming, or segment tree, we distribute the algorithm's input as information to different individuals in the social network. Then, we pose questions regarding the algorithm, hoping that agents can collaborate to produce the algorithm's output. It is worth noting that such pipelines are not intended to evaluate whether agents can collaborate to solve algorithms, as collaboration through natural language is certainly less efficient than utilizing algorithms directly on collected information. The algorithm serves as a perfect verifier on questions with distributed inputs. What's more, it is easy to utilize algorithm for differentiating tasks of varying difficulty to comprehensively evaluate agent capabilities. Additionally, algorithmic problems usually can provide continuous metrics, rather than just binary classification metrics of correctness, which detail the performance of agents. In _InformativeBench_, the algorithmic input information consists of the schedules of different individuals, where schedules can be formalized as multiple time intervals. Then, we examine agents' performance on algorithmic problems such as overlapping intervals, longest common intervals, etc. Specifically,

1. **Generate Schedule** 1. **Activity Pool Setting** We established a pool of single-person activities and a pool of multi-person activities. In the former, each activity requires only one participant, and the only attribute of the activity is its duration. In the latter, each activity requires multiple participants, and the attributes of the activity include its duration and the required number of participants. Additionally, we set up a routine activity pool, which Figure 10: Example of FriendsTV dataset. We show related dialogue, but agents do not have direct access to it and they have to retrieve these dialogue as context from memories about the whole Friends season 1 stories.

Figure 11: Example of ScheduleHard dataset, part 1.

Figure 12: Example of ScheduleHard dataset, part 2.

includes activities such as having breakfast. In the routine activity pool, each activity has attributes including its duration and a range of allowable start times. 2. **Individual Attribute Setting** To distinguish the schedule of each individual, we randomly set an activity preference vector for each individual. This vector consists of 0s and 1s, corresponding to all activities in the activity pool, indicating whether the individual is willing to participate in the activity. Additionally, for each individual, we set a time vector, which consists of 0s and 1s, indicating whether the time slot is occupied. The time vector is based on half-hour units and is 48-dimensional. 3. **Schedule Generation Process** For each group of individuals participating in the experiment, multi-person activities are allocated first. The number of multi-person activities, n, is set based on the number of individuals in each experimental batch. Next, n activities are randomly selected from the multi-person activity pool. Based on each individual's activity preference vector, participants are assigned to each multi-person activity as needed. The activity is then updated in the schedule of the participants, and the corresponding position of the time vector is set to 1. Subsequently, routine activities are generated for each individual, and the corresponding position of the time vector is also set to 1. Finally, for each individual, we use two pointers to traverse all the free time in their schedule and arrange single-person activities for them based on their needs and activity preference vector.
2. **Generate Dialogue** We generate pairwise dialogues within groups of individuals with symmetric information, enabling them to become aware of the schedules of all other individuals within the group. Specifically, we prompt GPT-4 in generating dialogues that exchange schedule information. This process includes the following key requirements: 1) For multi-person activities that person1 and person2 both participate in, the dialogue can mention the names of the other participants in that activity. 2) For multi-person activities that only person1 or person2 participates in, the dialogue should not mention the names of the other participants in that activity. 3) The generated dialogue needs to follow a certain format, for example, when person1 speaks to person2, the format should be "person1 to person2: ". Finally, the dialogue returned by GPT-4 is split into individual messages, and the sender, receiver, and message text of each message are written into the database for subsequent memory retrieval.
3. **Generate Question** 1. **Easy** Agents are asked to calculate how many intervals can be deleted at least so there are no overlapping time intervals between two individuals. It requires agents to share and examine the schedule and then reason to calculate the number of overlapping intervals. 2. **Medium** Agents are asked to find the longest schedules of all individuals in the network. It requires agents to traverse all intervals from the human schedules and exchange to find the longest one. 3. **Hard** Agents need to find out all intervals that everybody is free. It needs to collect all intervals and reasons to find the intervals with no activity interval covers.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: the paper elaborates the limitations in section 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: the paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: the paper describes the steps to generate all datasets in section 4 and section D. The paper describes methods in section 3. The paper describes the experimental setup in section 5. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: the paper provides open access to the data and code on https://github.com/thinkwee/iAgents, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: the paper does not include training. The paper describes all the hyperparameters setup in section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: error bars are not reported. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: the paper discussed the token cost in section 7. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: the paper discuss both potential positive societal impacts and negative societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: the paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: the paper cites all the papers related to our dataset. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: new assets introduced in the paper are well documented and the documentation is provided alongside the assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.