# Posterior Inferred, Now What?

Streamlining Prediction in Bayesian Deep Learning

Rui Li Marcus Klasson Arno Solin Martin Trapp

Department of Computer Science, Aalto University, Finland

{firstname.lastname}@aalto.fi

###### Abstract

The rising interest in Bayesian deep learning (BDL) has led to a plethora of methods for estimating the posterior distribution. However, efficient computation of inferences, such as predictions, has been largely overlooked with Monte Carlo integration remaining the standard. In this work we examine streamlining prediction in BDL through a single forward pass without sampling. For this we use local linearisation on activation functions and local Gaussian approximations at linear layers. Thus allowing us to analytically compute an approximation to the posterior predictive distribution. We showcase our approach for both MLP and transformer architectures and assess its performance on regression and classification tasks.

See also extended paper at https://arxiv.org/abs/2411.18425.

## 1 Introduction

Through the success of machine learning models in real-world applications, ensuring their reliability and robustness has become a key concern. In particular, in applications such as aided medical diagnosis [1], autonomous driving [18], or supporting scientific discovery [22], providing reliable predictions, identifying failure modes, and identify how to reduce uncertainties of the system is vital. Uncertainty quantification is at the core of these topics with Bayesian deep learning (BDL, [27, 20]) providing a promising paradigm for assessing uncertainties effectively and efficiently.

The central goal in BDL is to make inferences w.r.t. the posterior distribution over the probabilistic model (the parameters or the function itself). For example, to compute the expected prediction, estimate model uncertainties, or use it within acquisition functions in active learning. For this, we need to first estimate the posterior distribution and secondly make inferences of interest based on the estimated posterior. While both of these steps typically involve intractable integration, only the first step has seen significant progress in recent years [3, 16, 4]. For the second step, in case of a Laplace approximation (LA, [11]), globally linearising the model function around the maximum _a posteriori_ (MAP) estimate to perform inferences [13, 8] has shown promise in providing good predictive uncertainty. However, for all other posterior approximation methods, sampling based approximations remain to be the default. Given the high dimensionality of neural networks, sophisticated sampling methods are usually computationally prohibited and vanilla Monte-Carlo sampling is typically employed.

In this work, we tackle this problem by streamlining the prediction in BDL through local linearisation of activation functions and local Gaussian approximations at linear layers. Instead of a sample based approximation, which requires multiple re-evaluations of the network, we analytically approximate the posterior predictive distribution in a single forward pass through the network, making our methods well-suited for large-scale applications. Moreover, in contrast to global linearisation, our method is suitable for more complex inference tasks as the neural network function becomes locally linear with respect to the inputs. Empirically, we find that local linearisation and local Gaussianapproximation of neural networks to provide accurate predictive uncertainties and predictions, while being conceptually simple. Fig. 1 shows the posterior predictive densities for our proposal, compared to sampling based approximations and global linearisation in case of a Laplace approximation.

The contributions of our work can be summarised as follows: _(i)_ We propose a sampling-free and deterministic method for approximating the posterior predictive distribution through local linearisation of activation functions and local Gaussian approximations in neural networks. _(ii)_ We show how to exploit different covariance structures of the approximate posterior and present a streamlined prediction path for both MLP and transformer architectures. _(iii)_ We evaluate our method on regression and classification tasks and find that our method result in good predictive performance.

## 2 Related Work

**Inferring Posterior in Bayesian Deep Learning** There has been many methods developed which can be roughly grouped into three categories: _(i)_ Laplace approximation based methods: Starting from [11] where simple post-hoc Laplace approximation (LA) has shown promising results, LA has gained increasing attention ever since. Recent works applied LA methods in various applications, such as large language models [29; 10] and dynamic neural networks [17]. _(ii)_: Variational inference (VI) based methods: [3] showed mean-field VI (MFVI) could improve generalisation in small-scale neural network and [24] showed MFVI is effective for large-scale neural networks as well. _(iii)_: Others: Monte Carlo Dropout [6] aims to estimate predictive uncertainty by interpreting dropout in neural networks as a form of Bayesian approximation. Deep ensemble [12] combines the outputs of multiple independently trained models to capture predictive uncertainty. Stochastic Weight Averaging-Gaussian [16], which extends Stochastic Weight Averaging [9] by capturing the posterior distribution of model weights using a Gaussian approximation.

**Making Prediction in Bayesian Deep Learning** Little work has been done and the usual go-to solution is Monte Carlo Estimation. For Laplace approximation, [8] proposed the linearised LA by performing a global linearisation and has shown promise in providing useful predictive uncertainties.

## 3 Methods

In Bayesian deep learning (BDL), predicting the output \(y\) (_e.g._, class label, regression value) for an input \(\bm{x}\in\mathcal{X}\) is performed by _marginalizing_ out the model parameters \(\bm{\theta}\) of the neural network \(f_{\bm{\theta}}(\cdot)\) instead of trusting a single point estimate, _i.e._,

\[p(y\mid\bm{x},\mathcal{D})=\int_{\bm{\theta}}p(y\mid f_{\bm{\theta}}(\bm{x})) \,p(\bm{\theta}\mid\mathcal{D})\,\mathrm{d}\bm{\theta},\] (1)

where \(\mathcal{D}=\{(\bm{x}_{n},y_{n})\}_{n=1}^{N}\) denotes the training data and the posterior distribution \(p(\bm{\theta}\mid\mathcal{D})=\frac{p(\bm{\theta},\mathcal{D})}{p(\mathcal{D })}\) is given by Bayes' rule. However, for most neural networks integrating over the high-dimensional parameter space is intractable, necessitating the use of approximations to compute the posterior distribution \(p(\bm{\theta}\mid\mathcal{D})\) and the posterior predictive distribution \(p(y\mid\bm{x},\mathcal{D})\).

Figure 1: Ours gives better predictive uncertainties and decision boundaries compared with sampling in both Laplace approximation (LA) and mean-field variational inference (MFVI), while having matching performance with global linearised model (GLM) in LA.

Let the weights and biases of the \(m^{\text{th}}\) linear layer of the network \(f\) be denoted as \(\bm{W}^{(m)}\in\mathbb{R}^{\operatorname{D_{\textit{m}}}\times\operatorname{D_{ \textit{n}}}}\) and \(\bm{b}^{(m)}\in\mathbb{R}^{\operatorname{D_{\textit{m}}}}\), respectively. Then the pre-activation \(\bm{h}^{(m)}\) is given as \(\bm{h}^{(m)}=\bm{W}^{(m)}\bm{a}^{(m-1)}+\bm{b}^{(m)}\), where \(\bm{a}^{(m-1)}\in\mathbb{R}^{\operatorname{D_{\textit{n}}}}\) is the activation of the previous layer. In case \(m=1\), then \(\bm{a}^{(0)}\) corresponds to the input \(\bm{x}\). We further denote the \(k^{\text{th}}\) element of \(\bm{h}^{(m)}\) as \(h_{k}^{(m)}=\sum_{i=1}^{\operatorname{D_{\textit{n}}}}W_{ki}^{(m)}a_{i}^{(m- 1)}+b_{k}^{(m)}\) and drop the superscript if it is clear from the context.

Given an approximate posterior distribution \(q(\bm{\theta})\) with \(\bm{\theta}=\{\bm{W}^{(m)},\bm{b}^{(m)}\}_{m=1}^{M}\), we aim to compute the probability distribution of the activation \(\bm{a}^{(m)}\) of each layer \(m\). For this, we need to estimate the distribution of the pre-activation \(\bm{h}^{(m)}\) and then compute an approximation to the activation \(\bm{a}^{(m)}\) after application of a non-linear activation function \(g(\cdot)\).

Approximating the pre-activation distributionIn case the activation \(\bm{a}^{(m-1)}\) is deterministically give, _i.e._, for the input layer, we can compute the distribution over pre-activations analytically as a consequence of the stability of stable distributions under linear transformations [21]. However, for hidden layers the distribution over pre-activations is generally not of the same family as the posterior distribution [28]. Nevertheless, we will apply a local Gaussian approximation to the pre-activation at every hidden layer. Specifically, we make the assumption:

**Assumption 3.1**.: _Assume that the activations of the previous layer \(a_{i}^{(m-1)}\) and parameters of the \(m^{\text{th}}\) layer are independent._

Then followed by a Gaussian approximation of \(a_{i}^{(m-1)}\,W_{ki}^{(m)}\) for each \(i\) and each \(k\), the mean of the pre-activation \(\bm{h}^{(m)}\) is given as:

\[\mathbb{E}\left[\bm{h}^{(m)}\right]=\mathbb{E}\left[\bm{W}^{(m)}\right]\mathbb{ E}\left[\bm{a}^{(m-1)}\right]+\mathbb{E}\left[\bm{b}^{(m)}\right],\] (2)

and the covariance between the \(k^{\text{th}}\) and the \(j^{\text{th}}\) hidden unit is computed as:

\[\mathbb{C}\text{ov}\left[h_{k}^{(m)},h_{l}^{(m)}\right]= \underset{1\leq i,j\leq\operatorname{D_{\textit{n}}}}{\sum} \mathbb{C}\text{ov}\left[a_{i}^{(m-1)}W_{ki}^{(m)},a_{j}^{(m-1)}W_{lj}^{(m)} \right]+\mathbb{C}\text{ov}\left[b_{k}^{(m)},b_{l}^{(m)}\right]\] \[+ \underset{1\leq i\leq\operatorname{D_{\textit{n}}}}{\sum}\mathbb{ E}\left[a_{i}^{(m-1)}\right]\left(\mathbb{C}\text{ov}\left[W_{ki}^{(m)},b_{l}^{(m)} \right]+\mathbb{C}\text{ov}\left[W_{ki}^{(m)},b_{k}^{(m)}\right]\right),\] (3)

where

\[\mathbb{C}\text{ov}\left[a_{i}^{(m-1)}W_{ki}^{(m)},a_{j}^{(m-1)}W _{lj}^{(m)}\right] =\mathbb{E}\left[a_{i}^{(m-1)}\right]\mathbb{E}\left[a_{j}^{(m-1) }\right]\mathbb{C}\text{ov}\left[W_{ki}^{(m)},W_{lj}^{(m)}\right]\] \[+\mathbb{E}\left[W_{ki}^{(m)}\right]\mathbb{E}\left[W_{lj}^{(m)} \right]\mathbb{C}\text{ov}\left[a_{i}^{(m-1)},a_{j}^{(m-1)}\right]\] \[+\mathbb{C}\text{ov}\left[a_{i}^{(m-1)},a_{j}^{(m-1)}\right]\mathbb{ C}\text{ov}\left[W_{ki}^{(m)},W_{lj}^{(m)}\right].\] (4)

Depending on the structure of the covariance matrix, we can further simplify the computation of the covariance matrix.

Approximating the activation distributionLet \(g(\cdot)\) denote a non-linear activation function computing \(\bm{a}=g(\bm{h})\) for a pre-activation \(\bm{h}\). Inspired by the application of local linearisation in Bayesian filtering [_e.g._, 23], we use a first order Taylor expansion of \(g(\cdot)\) at the mean of the pre-activation \(\mathbb{E}\left[\bm{h}\right]\). Specifically, we approximate \(g(\bm{h})\) using

\[g(\bm{h})\approx g(\mathbb{E}\left[\bm{h}\right])+\bm{J}_{g}|_{\bm{h}=\mathbb{E} \left[\bm{h}\right]}(\bm{h}-\mathbb{E}\left[\bm{h}\right]),\] (5)

where \(\bm{J}_{g}|_{\bm{h}=\mathbb{E}\left[\bm{h}\right]}\) is the Jacobian of \(g(\cdot)\) at \(\bm{h}=\mathbb{E}\left[\bm{h}\right]\). Then, as stable distributions are closed under linear transformations, the distribution of \(\bm{a}\) can be computed analytically and is given as follows in case of a Gaussian distributed, _i.e._,

\[\bm{a}\sim\mathcal{N}(g(\mathbb{E}\left[\bm{h}\right]),\bm{J}_{g}|_{\bm{h}= \mathbb{E}\left[\bm{h}\right]}^{\top}\bm{\Sigma}_{\bm{h}}\bm{J}_{g}|_{\bm{h}= \mathbb{E}\left[\bm{h}\right]}).\] (6)

Note that the quality of the local linearisation will depend on the scale of the distribution over the input \(\bm{h}\). For ReLU activation functions, Petersen et al. [21] have shown that local linearisation provides the optimal Gaussian approximation of a univariate Gaussian distribution in total variation. For classification tasks, we employ a probit approximation [14, 11].

Combining local Gaussian approximations for linear layers and local linearisation for non-linear activation functions results in a tractable approximation to the posterior predictive distribution, which can be computed in a single forward pass. Fig. 2 illustrates our streamlined prediction for multi-layer perceptrons (MLP) and attention blocks in tranformers, for a detailed description on the approach for transformers see App. B.6.

**Covariance Structure** Computing the full covariance of the posterior is usually infeasible due to high computational and memory cost. Diagonal approximation and Kronecker-factorization of the covariance/precision are two of the most common approaches. For diagonal covariance, calculating the posterior predictive distribution is straightforward, see App. B.2 for details. In case of Kronecker factors, we developed a tailored block retrieval method for efficient propagation of uncertainties, see App. B.3 for details. Note that other covariance structures can exploited in a similar fashion.

**Computational Complexity** We will briefly discuss the computational complexity of our method for the case of full covariance. Observe from Eqs. (3) and (4) that the computational cost to obtain \((\mathsf{Cov}[h_{k},h_{l}])\) is \(\mathcal{O}(\mathsf{D}_{\text{in}}^{(l)^{2}})\). Therefore, computing the output covariance at the \(l^{\text{th}}\) linear layer will be in the order of \(\mathcal{O}(\mathsf{D}_{\text{out}}^{(l)^{2}}\mathsf{D}_{\text{in}}^{(l)^{2}})\). For element-wise activation functions, the computational cost will be \(\mathcal{O}(\mathsf{D}_{\text{out}}^{(l)^{2}})\). Hence, we obtain a total cost of \(\mathcal{O}(\sum_{l=1}^{L}\mathsf{D}_{\text{out}}^{(l)^{2}}\mathsf{D}_{\text {in}}^{(l)^{2}}+\mathsf{D}_{\text{out}}^{(l)^{2}})\) for a network with \(L\) layers. By exploiting the covariance structure, the total computational cost can be substantially reduced.

## 4 Experiments

We adopt the Laplace approximation (LA, [15]) and mean-field variational inference [MFVI, 2] for approximating the posterior distribution of the network parameters. We compare our method using local Gaussian approximation and local linearisation against Monte Carlo (MC) sampling and a global linearised model [GLM, 8]. For MFVI, we adopt the IVON optimiser [24] to obtain the posterior approximation, which has been shown to be effective and scalable to large-scale classification tasks. Here, we compare our method against MC sampling from the posterior to make predictions as done in Shen et al. [24]. For the MFVI and LA sampling baselines, we used \(1,000\) MC samples in the regression and MLP classification experiments, and \(50\) MC samples for the ViT classification experiments. For our method, we additionally fit a scale factor, multiplied to the predictive variance, by minimizing the negative log predictive density (NLPD) on the validation set. This is necessary, as the predictive variance in case of deep and wide network with diagonal covariance structure can be large. We use a paired \(t\)-test with \(p=0.05\) and bold results with significant statistical difference.

**Regression** We experiment with multi-layer perceptron (MLP) for regression. See App. C.1 for experiment setup details and additional results. We use full covariance for LA. As shown in Table Table 1, for MFVI our proposal (Ours) result in better performance than sampling on \(8\) data sets and matches the performance on the remaining \(3\) data sets. For LA, our approach obtains better performance than sampling on all data sets.

**Classification** For MLP, we train it from scratch and treat all layers Bayesian. For ViT, we fine-tune the MLP layers in the last two blocks in a pre-trained Vision transformer (ViT) base model [5] and

Figure 2: Illustration our approach for different network architectures. In MLPs, we can directly apply local Gaussian approximations and local linearisation of each layer. The distribution over activations is then propagated to the next layer. In attention layers, we treat the query \(\bm{Q}\) and key \(\bm{K}\) deterministically and only treat the value \(\bm{V}\) as a random quantity, resulting in a straightforward propagation path. The resulting distribution is then propagated to the subsequent MLP layer.

later treat them Bayesian. See App. C.2 for experiment setup details and additional results. With LA, we use a Kronecker-factorized covariance for MLPs and a diagonal covariance for ViT models. As shown in Table 2, for both MLP and ViT, we obtain better performance when compared with sampling and GLM.

Robustness to Out-of-distributionWe now assess the robustness to out-of-distribution (OOD) data for our method and the baselines. In Fig. 3, we take the ViT network fine-tuned on CIFAR-10 and evaluate its predictive entropy on the SVHN data set [19]. Our method can distinguish between in-distribution and OOD data better than the LA MAP and MFVI Sampling. Although our method underfits on the in-distribution data, the separation between is clear for the OOD data similar. For results on MLP, see App. C.2.

## 5 Discussion & Conclusion

In this work, we proposed to streamline prediction in Bayesian deep learning by local linearisation and local Gaussian approximations. For this, we discussed the propagation in different neural network architectures and covariance structures. We showed through a series of experiments that our method obtains high predictive performance, obtain good predictive uncertainties, and can distinguish between in-distribution and OOD data. In future work, we aim to extend our approach to other network architectures, such as convolutional layers, and utilize our approach in more complex inference tasks.

\begin{table}
\begin{tabular}{l c|c|c|c|c|c} \hline \hline  & & \multicolumn{2}{c|}{_MFVI (Diagonal Covariance)_} & \multicolumn{2}{c}{_LaL (Keen. Cox. for MLP. Diag. Cox. for ViT)_} \\  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\  & \multicolumn{1}{c|}{} & Sampling & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \hline MNIST & MLP & \(0.179\pm 0.014\) & \(\mathbf{0.086}_{1.005}\) & \(0.210\pm 0.003\) & \(\mathbf{0.089}_{1.004}\) & \(\mathbf{0.089}_{1.005}\) \\ FMNIST & MLP & \(2.010\pm 0.051\) & \(\mathbf{0.529}_{1.011}\) & \(0.556\pm 0.008\) & \(0.548\pm 0.018\) & \(\mathbf{0.397}\pm 0.010\) \\ \hline CIFAR-10 & ViT & \(0.124\pm 0.011\) & \(\mathbf{0.080}_{1.005}\) & \(0.169\pm 0.004\) & \(\mathbf{0.089}_{1.005}\) & \(\mathbf{0.088}\pm 0.006\) \\ CIFAR-100 & ViT & \(0.480\pm 0.018\) & \(\mathbf{0.437}\pm 0.013\) & \(1.043\pm 0.010\) & \(0.602\pm 0.011\) & \(\mathbf{0.457}\pm 0.012\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Negative log predictive density \(\downarrow\) on classification data sets. Ours results in better or matching performance when compared with sampling, indicating the effectiveness of our approximation.

\begin{table}
\begin{tabular}{l c|c|c|c|c|c} \hline \hline  & & \multicolumn{2}{c|}{_MFVI (Diagonal Covariance)_} & \multicolumn{2}{c}{_LaL (Keen. Cox. for MLP. Diag. Cox. for ViT)_} \\  & \multicolumn{1}{c|}{} & Sampling & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\  & \multicolumn{1}{c|}{} & Sampling & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\  & \multicolumn{1}{c|}{} & Sampling & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \hline SNIST & MLP & \(0.179\pm 0.014\) & \(\mathbf{0.086}_{1.005}\) & \(0.210\pm 0.003\) & \(\mathbf{0.089}_{1.004}\) & \(\mathbf{0.089}_{1.005}\) \\ FMNIST & MLP & \(2.010\pm 0.051\) & \(\mathbf{0.529}_{1.011}\) & \(0.556\pm 0.008\) & \(0.548\pm 0.018\) & \(\mathbf{0.397}\pm 0.010\) \\ \hline CIFAR-10 & ViT & \(0.124\pm 0.011\) & \(\mathbf{0.080}_{1.005}\) & \(0.169\pm 0.004\) & \(\mathbf{0.089}_{1.005}\) & \(\mathbf{0.088}\pm 0.006\) \\ CIFAR-100 & ViT & \(0.480\pm 0.018\) & \(\mathbf{0.437}\pm 0.013\) & \(1.043\pm 0.010\) & \(0.602\pm 0.011\) & \(\mathbf{0.457}\pm 0.012\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Negative log predictive density \(\downarrow\) on classification data sets. Ours results in better or matching performance when compared with sampling, indicating the effectiveness of our approximation.

Figure 3: Kernel density plots over the predictive entropy from a ViT network finetuned on CIFAR-10 (blue, in-distribution) and data from SVHN (red, out-of-distribution). Our method results in a clear separation between the in- and out-of-distribution data.

## Acknowledgements

AS and RL acknowledge funding from the Research Council of Finland (grant number 339730). MT acknowledges funding from the Research Council of Finland (grant number 347279). MK acknowledges funding from the Finnish Center for Artificial Intelligence (FCAI). We acknowledge CSC - IT Center for Science, Finland, for awarding this project access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through CSC. We acknowledge the computational resources provided by the Aalto Science-IT project.

## References

* [1] E. Begoli, T. Bhattacharya, and D. Kusnezov. The need for uncertainty quantification in machine-assisted medical decision making. _Nature Machine Intelligence_, 1(1):20-23, 2019.
* [2] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. _Journal of the American statistical Association_, 112(518):859-877, 2017.
* [3] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural network. In _Proceedings of the 32nd International Conference on Machine Learning (ICML)_, Proceedings of Machine Learning Research, pages 1613-1622. PMLR, 2015.
* effortless bayesian deep learning. In _Advances in Neural Information Processing Systems (NeurIPS) 34_, volume 34, pages 20089-20103. Curran Associates, Inc., 2021.
* [5] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations (ICLR)_, 2021.
* [6] Y. Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In _Proceedings of the 33th International Conference on Machine Learning (ICML)_, volume 48 of _Proceedings of Machine Learning Research_, pages 1050-1059. PMLR, 2016.
* [7] M. N. Gibbs. _Bayesian Gaussian processes for regression and classification_. Phd thesis, University of Cambridge, 1998.
* [8] A. Immer, M. Korzepa, and M. Bauer. Improving predictions of bayesian neural nets via local linearization. In _Proceedings of the twenty forth International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 130 of _Proceedings of Machine Learning Research_, pages 703-711. PMLR, 2021.
* [9] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G. Wilson. Averaging weights leads to wider optima and better generalization. In _Proceedings of the 34th Conference on Uncertainty in Artificial Intelligence_, pages 876-885. AUAI Press, 2018.
* [10] P. J. Kampen, G. R. Als, and M. R. Andersen. Towards scalable bayesian transformers: Investigating stochastic subset selection for nlp. In _Proceedings of the 40th Conference on Uncertainty in Artificial Intelligence_. AUAI Press, 2024.
* [11] A. Kristiadi, M. Hein, and P. Hennig. Being bayesian, even just a bit, fixes overconfidence in relu networks. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, volume 119 of _Proceedings of Machine Learning Research_, pages 5436-5446. PMLR, 2020.
* [12] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _Advances in Neural Information Processing Systems (NeurIPS) 30_, volume 30, pages 6402-6413. Curran Associates, Inc., 2017.
* [13] N. D. Lawrence. _Variational inference in probabilistic models_. PhD thesis, University of Cambridge, 2001.

* [14] D. J. MacKay. Bayesian interpolation. _Neural computation_, 4(3):415-447, 1992.
* [15] D. J. MacKay. Bayesian methods for backpropagation networks. In _Models of neural networks III: association, generalization, and representation_, pages 211-254. Springer, 1996.
* [16] W. J. Maddox, P. Izmailov, T. Garipov, D. P. Vetrov, and A. G. Wilson. A simple baseline for bayesian uncertainty in deep learning. In _Advances in Neural Information Processing Systems (NeurIPS) 32_, volume 32, pages 13132-13143. Curran Associates, Inc., 2019.
* [17] L. Meronen, M. Trapp, A. Pilzer, L. Yang, and A. Solin. Fixing overconfidence in dynamic neural networks. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 2680-2690, 2024.
* [18] R. Michelmore, M. Wicker, L. Laurenti, L. Cardelli, Y. Gal, and M. Kwiatkowska. Uncertainty quantification with statistical guarantees in end-to-end autonomous driving control. In _2020 IEEE international conference on robotics and automation (ICRA)_, pages 7344-7350. IEEE, 2020.
* [19] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, A. Y. Ng, et al. Reading digits in natural images with unsupervised feature learning. In _NIPS workshop on deep learning and unsupervised feature learning_, 2011.
* [20] T. Papamakou, M. Skoularidou, K. Palla, L. Aitchison, J. Arbel, D. B. Dunson, M. Filippone, V. Fortuin, P. Hennig, J. M. Hernandez-Lobato, A. Hubin, A. Immer, T. Karaletsos, M. E. Khan, A. Kristiadi, Y. Li, S. Mandt, C. Nemeth, M. A. Osborne, T. G. J. Rudner, D. Rugamer, Y. W. Teh, M. Welling, A. G. Wilson, and R. Zhang. Position: Bayesian deep learning is needed in the age of large-scale ai. In _Proceedings of the 41st International Conference on Machine Learning (ICML)_, volume 235 of _Proceedings of Machine Learning Research_. PMLR, 2024.
* [21] F. Petersen, A. A. Mishra, H. Kuehne, C. Borgelt, O. Deussen, and M. Yurochkin. Uncertainty quantification via stable distribution propagation. In _International Conference on Learning Representations (ICLR)_, 2024.
* [22] A. F. Psaros, X. Meng, Z. Zou, L. Guo, and G. E. Karniadakis. Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons. _Journal of Computational Physics_, 477:111902, 2023.
* [23] S. Sarkka and L. Svensson. _Bayesian filtering and smoothing_, volume 17. Cambridge university press, 2023.
* [24] Y. Shen, N. Dahheim, B. Cong, P. Nickl, G. M. Marconi, B. C. E. M. Raoul, R. Yokota, I. Gurevych, D. Cremers, M. E. Khan, and T. Mollenhoff. Variational learning is effective for large deep networks. In _Proceedings of the 41st International Conference on Machine Learning (ICML)_, volume 235 of _Proceedings of Machine Learning Research_. PMLR, 2024.
* [25] D. J. Spiegelhalter and S. L. Lauritzen. Sequential updating of conditional probabilities on directed graphical structures. _Networks_, 20(5):579-605, 1990.
* [26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems (NeurIPS) 30_, volume 30. Curran Associates, Inc., 2017.
* [27] A. G. Wilson and P. Izmailov. Bayesian deep learning and a probabilistic perspective of generalization. In _Advances in Neural Information Processing Systems (NeurIPS) 33_, volume 33, pages 4697-4708. Curran Associates, Inc., 2020.
* [28] P. Wolinski and J. Arbel. Gaussian pre-activations in neural networks: Myth or reality? _arXiv preprint arXiv:2205.12379_, 2022.
* [29] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison. Bayesian low-rank adaptation for large language models. In _International Conference on Learning Representations (ICLR)_, 2024.

## Appendix B Derivations

We derive the approximate posterior predictive distribution form in this section. App. B.1 is for the case where the covariance has full structure in linear layer. App. B.2 is for the case where the covariance has diagonal structure in linear layer. App. B.3 is for the case where the covariance has Kronecker-factorised structure in linear layer. App. B.4 is the derivation for activation layers. App. B.5 describes the probt approximation for approximate the posterior prediction for classification. App. B.6 describes how to apply our method for the transformer.

### Derivation for General Covariance Structure

Denote the weight and bias of the \(m^{\text{th}}\) linear layer as \(\bm{W}^{(m)}\in\mathbb{R}^{\mathrm{D}_{\text{out}}\times\mathrm{D}_{\text{in}}}\) and \(\bm{b}^{(m)}\in\mathbb{R}^{\mathrm{D}_{\text{out}}}\) respectively, and its input as \(\bm{a}^{(m-1)}\in\mathbb{R}^{\mathrm{D}_{\text{in}}}\). The pre-activation is then given as \(\bm{h}^{(m)}=\bm{W}^{(m)}\bm{a}^{(m-1)}+\bm{b}^{(m)}\) with its \(k^{\text{th}}\) element being \(h_{k}^{(m)}=\sum_{i=1}^{\mathrm{D}_{\text{in}}}W_{ki}^{(m)}a_{i}^{(m-1)}+b_{k} ^{(m)}\).

We make the following assumptions to obtain a tractable distribution on the pre-activation:

* Assumption 1: We assume each \(a_{i}^{(m-1)}W_{ki}^{(m)}\) is a Gaussian distribution.
* Assumption 2: We assume that the activations of the previous layer \(a_{i}^{(m-1)}\) and parameters of the \(m^{\text{th}}\) layer are independent.

\begin{table}
\begin{tabular}{l l} \hline \hline \(\bm{x}\) & lowercase bolder letter, vector \\ \(\bm{W}\) & uppercase bold letter, matrix \\ \(\mathcal{D}\) & set \\ \(x_{i}\) & \(i^{\text{th}}\) element of \(\bm{x}\) \\ \(W_{ki}\) & \(k^{\text{th}}\) row, \(i^{\text{th}}\) column of \(\bm{W}\) \\ \(\bm{W}[k,:]\) & \(k^{\text{th}}\) row of a matrix \\ \(k\), \(l\) & dimension of the output \\ \(i\), \(j\) & dimension of the input \\ \(d\) & data feature dimension \\ \(n,N\) & number of data points \\ \(C\) & total number of classes \\ \(m\) & layer index \\ \hline \hline \end{tabular}
\end{table}
Table 3: Notation.

From assumption 1, because now \(a_{i}^{(m-1)}W_{ki}^{(m)}\) and \(b_{k}^{(m)}\) are all Gaussian distributions, \(h_{k}^{(m)}\) will follow Gaussian distribution as well. We call this local Gaussian approximation as we approximate each local component \(a_{i}^{(m-1)}W_{ki}^{(m)}\) with a Gaussian. As now each \(h_{k}^{(m)}\) is a Gaussian, \(\bm{h}^{(m)}\) will be jointly Gaussian. We derive its mean and covariance and drop the layer index if it is clear from the context.

Derivation of meanAs \(a_{i}\) is assumed to be uncorrected with \(W_{ki}\), we have

\[\mathbb{E}\left[h_{k}\right] =\mathbb{E}\left[\sum_{i=1}^{\mathrm{D_{in}}}W_{ki}a_{i}+b_{k}\right]\] (7) \[=\sum_{i=1}^{\mathrm{D_{in}}}\mathbb{E}\left[W_{ki}a_{i}+b_{k}\right]\] (8) \[=\sum_{i=1}^{\mathrm{D_{in}}}\mathbb{E}\left[W_{ki}a_{i}\right]+ \mathbb{E}\left[b_{k}\right]\] (9) \[\approx\sum_{i=1}^{\mathrm{D_{in}}}\mathbb{E}\left[W_{ki}\right] \mathbb{E}\left[a_{i}\right]+\mathbb{E}\left[b_{k}\right].\] (Assumption 2)

Derivation of covarianceThe covariance between the \(k^{\text{th}}\) and \(l^{\text{th}}\) pre-activation can be written as

\[\mathbb{C}\mathrm{ov}\left[h_{k},h_{l}\right] =\mathbb{C}\mathrm{ov}\left[\sum_{i=1}^{\mathrm{D_{in}}}a_{i}W_{ ki}+b_{k},\sum_{i=1}^{\mathrm{D_{in}}}a_{i}W_{li}+b_{l}\right]\] (10) \[=\mathbb{C}\mathrm{ov}\left[\sum_{i=1}^{\mathrm{D_{in}}}a_{i}W_{ ki},\sum_{i=1}^{\mathrm{D_{in}}}a_{i}W_{li}\right]+\mathbb{C}\mathrm{ov}\left[ \sum_{i=1}^{\mathrm{D_{in}}}a_{i}W_{ki},b_{l}\right]+\mathbb{C}\mathrm{ov} \left[\sum_{i=1}^{\mathrm{D_{in}}}a_{i}W_{li},b_{k}\right]\] (11) \[=\sum_{1\leq i,j\leq D_{in}}\mathbb{C}\mathrm{ov}\left[a_{i}W_{ki},a_{j}W_{lj}\right]+\sum_{1\leq i\leq D_{in}}(\mathbb{C}\mathrm{ov}\left[a_{i}W _{ki},b_{l}\right]+\mathbb{C}\mathrm{ov}\left[a_{i}W_{li},b_{k}\right])\] \[\qquad\qquad\qquad\qquad\qquad+\mathbb{C}\mathrm{ov}\left[b_{k},b_{l}\right]\] (12)

We first derive the form of \(\mathbb{C}\mathrm{ov}[a_{i}W_{ki},a_{i}W_{li}]\):

\[\mathbb{C}\mathrm{ov}\left[a_{i}W_{ki},a_{j}W_{lj}\right]\] \[=\mathbb{E}\left[(a_{i}W_{ki}-\mathbb{E}\left[a_{i}W_{ki}\right]) (a_{j}W_{lj}-\mathbb{E}\left[a_{j}W_{lj}\right])\right]\] (13) \[=\mathbb{E}\left[a_{i}W_{ki}a_{j}W_{lj}-a_{i}W_{ki}\mathbb{E} \left[a_{j}W_{lj}\right]-\mathbb{E}\left[a_{i}W_{ki}\right]a_{j}W_{lj}+ \mathbb{E}\left[a_{i}W_{ki}\right]\mathbb{E}\left[a_{j}W_{lj}\right]\right]\] (14) \[=\mathbb{E}\left[a_{i}a_{j}W_{ki}W_{lj}\right]-\mathbb{E}\left[a_{ i}W_{ki}\right]\mathbb{E}\left[a_{j}W_{lj}\right]-\mathbb{E}\left[a_{i}W_{ki}\right] \mathbb{E}\left[a_{j}W_{lj}\right]+\mathbb{E}\left[a_{i}W_{ki}\right]\mathbb{ E}\left[a_{j}W_{lj}\right]\] (15) \[\approx\mathbb{E}\left[a_{i}a_{j}\right]\mathbb{E}\left[W_{ki}W_{ lj}\right]-\mathbb{E}\left[a_{i}\right]\mathbb{E}\left[W_{ki}\right]\mathbb{E} \left[a_{j}\right]\mathbb{E}\left[W_{lj}\right]\] (Assumption 2) \[=(\mathbb{E}\left[a_{i}\right]\mathbb{E}\left[a_{j}\right]+ \mathbb{C}\mathrm{ov}\left[a_{i},a_{j}\right])(\mathbb{E}\left[W_{ki}\right] \mathbb{E}\left[W_{lj}\right]+\mathbb{C}\mathrm{ov}\left[W_{ki},W_{lj}\right])\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad-\mathbb{E}\left[a_{i}\right]\mathbb{E}\left[W_{ ki}\right]\mathbb{E}\left[a_{j}\right]\mathbb{E}\left[W_{lj}\right]\] (16) \[=\mathbb{E}\left[a_{i}\right]\mathbb{E}\left[a_{j}\right]\mathbb{C} \mathrm{ov}\left[W_{ki},W_{lj}\right]+\mathbb{E}\left[W_{ki}\right]\mathbb{E} \left[W_{lj}\right]\mathbb{C}\mathrm{ov}\left[a_{i},a_{j}\right]+\mathbb{C} \mathrm{ov}\left[a_{i},a_{j}\right]\mathbb{C}\mathrm{ov}\left[W_{ki},W_{lj} \right].\] (17)Then we drive the form of \(\mathbb{Cov}[a_{i}W_{ki},b_{l}]\):

\[\mathbb{Cov}\left[a_{i}W_{ki},b_{l}\right] =\mathbb{E}\left[\left(a_{i}W_{ki}-\mathbb{E}\left[a_{i}W_{ki}\right] \right)\left(b_{l}-\mathbb{E}\left[b_{l}\right]\right)\right]\] (18) \[\approx\mathbb{E}\left[\left(a_{i}W_{ki}-\mathbb{E}\left[a_{i} \right]\mathbb{E}\left[W_{ki}\right]\right)\left(b_{l}-\mathbb{E}\left[b_{l} \right]\right)\right]\] (Assumption 2) \[=\mathbb{E}\left[a_{i}W_{ki}b_{l}-a_{i}W_{ki}\mathbb{E}\left[b_{l }\right]-\mathbb{E}\left[a_{i}\right]\mathbb{E}\left[W_{ki}\right]b_{l}+ \mathbb{E}\left[a_{i}\right]\mathbb{E}\left[W_{ki}\right]\mathbb{E}\left[b_{l }\right]\right]\] (19) \[=\mathbb{E}\left[a_{i}W_{ki}b_{l}\right]-\mathbb{E}\left[a_{i} \right]\mathbb{E}\left[W_{ki}\right]\mathbb{E}\left[b_{l}\right]\] (20) \[\approx\mathbb{E}\left[a_{i}\right]\mathbb{E}\left[W_{ki}b_{l} \right]-\mathbb{E}\left[a_{i}\right]\mathbb{E}\left[W_{ki}\right]\mathbb{E} \left[b_{l}\right]\] (Assumption 2) \[=\mathbb{E}\left[a_{i}\right]\left(\mathbb{E}\left[W_{ki}\right] \mathbb{E}\left[b_{l}\right]+\mathbb{Cov}\left[W_{ki},b_{l}\right]\right)- \mathbb{E}\left[a_{i}\right]\mathbb{E}\left[W_{ki}\right]\mathbb{E}\left[b_{l }\right]\] (21) \[=\mathbb{E}\left[a_{i}\right]\mathbb{Cov}\left[W_{ki},b_{l}\right].\] (22)

Putting it together, we have \(\mathbb{Cov}[h_{k},h_{l}]=\)

\[\underset{1\leq i,j\leq\mathrm{D}_{n}}{\sum}\mathbb{Cov}\left[a_{i}W_{ki},a_{ j}W_{lj}\right]+\sum_{i=1}^{\mathrm{D}_{n}}\left(\mathbb{E}\left[a_{i}\right] \mathbb{Cov}\left[W_{ki},b_{l}\right]+\mathbb{E}\left[a_{i}\right]\mathbb{Cov }\left[W_{li},b_{k}\right]\right)+\mathbb{Cov}\left[b_{k},b_{l}\right],\] (23)

where \(\mathbb{Cov}[a_{i}W_{ki},a_{j}W_{lj}]=\)

\[\mathbb{E}\left[a_{i}\right]\mathbb{E}\left[a_{j}\right]\mathbb{Cov}\left[W_{ ki},W_{lj}\right]+\mathbb{E}\left[W_{ki}\right]\mathbb{E}\left[W_{lj}\right] \mathbb{Cov}\left[a_{i},a_{j}\right]+\mathbb{Cov}\left[a_{i},a_{j}\right] \mathbb{Cov}\left[W_{ki},W_{lj}\right].\] (24)

Note that \(\sum_{1\leq i,j\leq\mathrm{D}_{n}}\mathbb{Cov}[a_{i}W_{ki},a_{j}W_{lj}]\) in Eq. (23) could be rewrite into the form of matrix multiplication for efficient implementation:

\[\underset{1\leq i,j\leq\mathrm{D}_{n}}{\sum}\mathbb{Cov}\left[a_{i }W_{ki},a_{j}W_{lj}\right]\] (25) \[= \underset{1\leq i,j\leq\mathrm{D}_{n}}{\sum}\mathbb{E}\left[a_{i }\right]\mathbb{E}\left[a_{j}\right]\mathbb{Cov}\left[W_{ki},W_{lj}\right]+ \mathbb{E}\left[W_{ki}\right]\mathbb{E}\left[W_{lj}\right]\mathbb{Cov}\left[a_ {i},a_{j}\right]+\mathbb{Cov}\left[a_{i},a_{j}\right]\mathbb{Cov}\left[W_{ki}, W_{lj}\right]\] (26) \[= \sum\left[\begin{array}{cccc}\mathbb{E}\left[a_{1}\right]\mathbb{E }\left[a_{1}\right]\mathbb{Cov}\left[W_{k1},W_{11}\right]&\ldots&\mathbb{E} \left[a_{1}\right]\mathbb{E}\left[a_{\mathrm{D}_{n}}\right]\mathbb{Cov}[W_{k1 },W_{\mathrm{D}_{n}}]\\ \vdots&\vdots&\vdots&\vdots\\ \mathbb{E}\left[a_{\mathrm{D}_{n}}\right]\mathbb{E}\left[a_{1}\right]\mathbb{ Cov}\left[W_{k}\,\mathrm{D}_{n},W_{11}\right]&\ldots&\mathbb{E}\left[a_{1}\right] \mathbb{E}\left[a_{\mathrm{D}_{n}}\right]\mathbb{Cov}\left[W_{k}\,\mathrm{D}_{ n},W_{1}\,\mathrm{D}_{n}\right]\end{array}\right]\] (27) \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad

[MISSING_PAGE_EMPTY:11]

However, for posterior stored in Kronecker product form, we will have \(\operatorname{D_{in}}\times\operatorname{D_{in}}\) numbers of \(\operatorname{D_{out}}\times\operatorname{D_{out}}\) matrix, which complicates the retrieval of \(\operatorname{\mathbb{C}ov}[\boldsymbol{W}[k,:],\boldsymbol{W}[l,:]]\).

### Derivation for Activation Layers

For \(\boldsymbol{a}=g(\boldsymbol{h})\) where \(\boldsymbol{h}\sim\mathcal{N}(\boldsymbol{h};\operatorname{\mathbb{E}}\left[ \boldsymbol{h}\right],\boldsymbol{\Sigma}_{h})\) and \(g(\cdot)\) is the activation function, we use local linearisation to approximate the distribution of \(\boldsymbol{a}\). Specifically, we do a first-order Taylor expansion on \(g(\cdot)\) at \(\operatorname{\mathbb{E}}\left[\boldsymbol{h}\right]\):

\[\boldsymbol{a} =g(\boldsymbol{h})\] (45) \[\approx g(\operatorname{\mathbb{E}}\left[\boldsymbol{h}\right])+ \boldsymbol{J}_{g}|_{\boldsymbol{h}=\operatorname{\mathbb{E}}\left[ \boldsymbol{h}\right]}(\boldsymbol{h}-\operatorname{\mathbb{E}}\left[ \boldsymbol{h}\right]).\] (46)

Given that Gaussian distribution is closed under linear transformation, we have

\[\boldsymbol{h}\sim \mathcal{N}(\operatorname{\mathbb{E}}\left[\boldsymbol{h}\right],\boldsymbol{\Sigma}_{h})\] (47) \[\boldsymbol{h}-\operatorname{\mathbb{E}}\left[\boldsymbol{h}\right]\sim \mathcal{N}(\boldsymbol{0},\boldsymbol{\Sigma}_{h})\] (48) \[\boldsymbol{J}_{g}|_{\boldsymbol{h}=\operatorname{\mathbb{E}} \left[\boldsymbol{h}\right]}(\boldsymbol{h}-\operatorname{\mathbb{E}}\left[ \boldsymbol{h}\right])\sim \mathcal{N}(\boldsymbol{0},\boldsymbol{J}_{g}|_{\boldsymbol{h}= \operatorname{\mathbb{E}}\left[\boldsymbol{h}\right]}\top\boldsymbol{\Sigma}_ {h}\boldsymbol{J}_{g}|_{\boldsymbol{h}=\operatorname{\mathbb{E}}\left[ \boldsymbol{h}\right]})\] (49) \[g(\operatorname{\mathbb{E}}\left[\boldsymbol{h}\right])+ \boldsymbol{J}_{g}|_{\boldsymbol{h}=\operatorname{\mathbb{E}}\left[ \boldsymbol{h}\right]}(\boldsymbol{h}-\operatorname{\mathbb{E}}\left[ \boldsymbol{h}\right])\sim \mathcal{N}(g(\operatorname{\mathbb{E}}\left[\boldsymbol{h} \right]),\boldsymbol{J}_{g}|_{\boldsymbol{h}=\operatorname{\mathbb{E}}\left[ \boldsymbol{h}\right]}\top\boldsymbol{\Sigma}_{h}\boldsymbol{J}_{g}|_{ \boldsymbol{h}=\operatorname{\mathbb{E}}\left[\boldsymbol{h}\right]})\] (50) \[\boldsymbol{a} \underset{\text{approx}}{\sim}\mathcal{N}(\boldsymbol{a};g( \operatorname{\mathbb{E}}\left[\boldsymbol{h}\right]),\boldsymbol{J}_{g}|_{ \boldsymbol{h}=\operatorname{\mathbb{E}}\left[\boldsymbol{h}\right]}\top \boldsymbol{\Sigma}_{h}\boldsymbol{J}_{g}|_{\boldsymbol{h}=\operatorname{ \mathbb{E}}\left[\boldsymbol{h}\right]}).\] (51)

### Probit Approximation for Classification

Following [4], in classification we treat the logits before last layer activation (softmax) as model output \(f\). Then we can use probit approximation to get posterior predictive:

Binary [14, 25]

\[p\left(y^{*}\mid x^{*}\right) =\int_{\mathbb{R}}\operatorname{sigmoid}\left(f^{*}\right) \mathcal{N}\left(f^{*}\mid\mu^{*},{\sigma^{*}}^{2}\right)\mathrm{d}f^{*}\] (52) \[\approx\int\Phi\left(f^{*}\right)\mathcal{N}\left(f^{*}\mid\mu^{ *},{\sigma^{*}}^{2}\right)\mathrm{d}f^{*}\] (53) \[=\sigma\left(\frac{\mu^{*}}{\sqrt{1+\frac{\pi}{8}{\sigma^{*}}^{2}} }\right).\] (54)

Multi-class [7]

\[p\left(\mathbf{y}^{*}\mid\mathbf{x}^{*}\right) =\int_{\mathbb{R}^{C}}\operatorname{softmax}\left(\mathbf{f}^{*} \right)\mathcal{N}\left(\mathbf{f}^{*}\mid\mu^{*},\boldsymbol{\Sigma}^{*} \right)\mathrm{d}\mathbf{f}^{*}\] (55) \[\overset{\text{j-th element}}{\approx} \frac{\exp\left(\tau_{i}\right)}{\sum_{j=1}^{C}\exp\left(\tau_{j} \right)},\text{ where }\tau_{j}=\frac{\mu_{j}^{*}}{\sqrt{1+\frac{\pi}{8}\Sigma_{jj}^{*}}}.\]

### Transformer Block

There are four components in each transformer block [26]: (1) multi-head attention; (2) MLP; (3) layer normalisation; and (4) residual connection. For MLP blocks, the propagation is the same as described above. For layer normalisation, as Gaussian distribution is closed under linear transformation, push distribution over it is straightforward. For residual connection, we assume the input and output are independent. We describe how to push distribution through attention layers below. Note for computational reasons, we always assume the input has diagonal covariance.

**Attention Block** Given an input \(\boldsymbol{H}\in\mathbb{R}^{T\times D}\) where \(T\) is the number of tokens in the input sequence and \(D\) is the dimension of each token, denote the query, key and value matrices as \(\boldsymbol{W}_{Q}\in\mathbb{R}^{D\times D}\), \(\boldsymbol{W}_{K}\in\mathbb{R}^{D\times D}\), \(\boldsymbol{W}_{V}\in\mathbb{R}^{D\times D}\) respectively, the key, query and value in an attention blocks are

\[\boldsymbol{Q}=\boldsymbol{H}\boldsymbol{W}_{Q},\quad\boldsymbol{K}=\boldsymbol{ H}\boldsymbol{W}_{K},\quad\boldsymbol{V}=\boldsymbol{H}\boldsymbol{W}_{V},\] (56)and the output of attention block is

\[\text{Attention}(\bm{H})=\text{Softmax}(\frac{\bm{Q}\bm{K}^{\top}}{\sqrt{D}})\bm{V}.\] (57)

When the input \(\bm{H}\) is a distribution, \(\bm{Q}\), \(\bm{K}\) and \(\bm{V}\) will all be distributions as well. As pushing a distribution over a softmax activation requires further approximation, we ignore the distribution over \(\bm{Q}\) and \(\bm{K}\) for computational reasons and compute their value by using the mean of input:

\[\bm{Q}=\mathbb{E}\left[\bm{H}\right]\mathbb{E}\left[\bm{W}_{Q}\right],\quad \bm{K}=\mathbb{E}\left[\bm{H}\right]\mathbb{E}\left[\bm{W}_{K}\right].\] (58)

For \(\bm{V}\), for simplicity we describe our approximation for a single token \(\bm{h}\) whose value is \(\bm{v}=\bm{W}_{V}\bm{h}\) with \(k^{\text{th}}\) element being \(v_{k}=\sum_{i=1}^{D}W_{V_{k_{i}}}h_{i}\). Assuming \(\bm{h}\) is a Gaussian, the covariance between the \(k^{\text{th}}\) and the \(l^{\text{th}}\) value is

\[\mathbb{C}\text{ov}\left[v_{k},v_{l}\right] =\mathbb{C}\text{ov}\left[\sum_{i=1}^{D}W_{V_{k_{i}}}h_{i},\sum_ {j=1}^{D}W_{V_{l_{j}}}h_{j}\right]\] (59) \[=\sum_{i=1}^{D}\sum_{j=1}^{D}\mathbb{C}\text{ov}\left[W_{V_{k_{i} }}h_{i},W_{V_{l_{j}}}h_{j}\right].\] (60)

We have

\[\mathbb{C}\text{ov}\left[v_{k},v_{l}\right] =\sum_{i=1}^{D}\sum_{j=1}^{D}\mathbb{C}\text{ov}\left[W_{V_{k_{i} }}h_{i},W_{V_{l_{j}}}h_{j}\right]\] (definition) \[=\sum_{i=1}^{D}\sum_{j=1}^{D}W_{V_{k_{i}}}W_{V_{l_{j}}}\text{ Cov}\left[h_{i},h_{j}\right]\] ( \[\bm{W}_{V}\] deterministic) \[\approx\sum_{i=1}^{D}W_{V_{k_{i}}}W_{V_{l_{i}}}\,\mathbb{V} \text{ar}\left[h_{i}\right].\] (ignore correlation between \[\bm{h}\] for computational reason) \[\mathbb{V}\text{ar}\left[v_{k}\right] =\sum_{1\leq i,j\leq D}\mathbb{C}\text{ov}\left[W_{V_{k_{i}}}h_{ i},W_{V_{k_{j}}}h_{j}\right]\] (definition) \[\approx\sum_{1\leq i,j\leq D}\left(\mathbb{E}\left[h_{i}\right] \mathbb{E}\left[h_{j}\right]+\mathbb{C}\text{ov}\left[h_{i},h_{j}\right] \right)\mathbb{C}\text{ov}\left[W_{ki},W_{kj}\right]+\mathbb{E}\left[W_{ki} \right]\mathbb{E}\left[W_{kj}\right]\mathbb{C}\text{ov}\left[h_{i},h_{j}\right]\] (assumption A2) \[=\sum_{1\leq i\leq D}\left(\mathbb{E}\left[h_{i}\right]^{2}+ \mathbb{V}\text{ar}\left[h_{i}\right]\right)\mathbb{V}\text{ar}\left[W_{ki} \right]+\mathbb{E}\left[W_{ki}\right]^{2}\mathbb{V}\text{ar}\left[h_{i}\right].\] ( \[\bm{W}_{V}\] is isotropic Gaussian) ( 61 )

Once we have the distribution over \(\bm{V}\), the distribution over \(\text{Attention}(\bm{H})\) becomes a distribution of linear combination of Gaussian, which is tractable.

Then for multi-head attention, we assume each attention head's output is independent, which allows us to compute the distribution over the final output in tractable form. As we assume all input is isotropic, here we only need to compute the variance for each dimension.

## Appendix C Experiment

### Regression

Table 4 gives the UCI regression data set information and the neural network structure we used. For all neural networks, we use ReLU activation function. In Table 5 we report the Root Mean Square

Error (RMSE), Ours results in matching or better performance compared with sampling and GLM, indicating the effectiveness of our method. Note that as the mean of the posterior prediction of our method is the same as the prediction made by setting the weights of the neural network to be the mean of the posterior, we result in the same prediction as GLM of LA, and hence the same performance.

### Classification

Table 6 gives the classification data sets information and the neural network structure we used. We use ReLU activation for MLP. For ViT, we make the MLP block in the last two transformer block and the classification head Bayesian, and treat the rest of the weight deterministically. In Table 7 we report the test accuracy, our method results in matching or better performance compared with sampling and GLM, indicating the effectiveness of our method.

In Fig. 4, we show kernel density plots over the predictive entropy of an FMNIST-trained MLP evaluated on MNIST. Our method can distinguish between in-distribution and OOD data better than the LA MAP and MFVI Sampling. Although our method underfits on the in-distribution data, the separation between is clear for the OOD data similar.

\begin{table}
\begin{tabular}{l l l} \hline \hline Dataset Name & \((n,d)\) & Network Structure \\ \hline MNIST & (50000, 784) & \(d\)-128-64-10 \\ FMNIST & (50000, 784) & \(d\)-128-64-10 \\ CIFAR-10 & (50000, 3, 32, 32) & ViT-base \\ CIFAR-100 & (50000, 3, 32, 32) & ViT-base \\ \hline \hline \end{tabular}
\end{table}
Table 6: Classification experiment setup.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline  & & \multicolumn{2}{c|}{_MFVI (Diag, Cov.)_} & \multicolumn{2}{c|}{_Laplace Approximation (Full Cov.)_} \\  & \((n,d)\) & Sampling & Ours & Sampling & GLM & Ours \\ \hline Servo & (167, 4) & \(0.749\pm 0.147\) & \(\bm{0.740\pm 0.143}\) & \(1.632\pm 0.233\) & \(\bm{0.658\pm 0.141}\) & \(\bm{0.658\pm 0.141}\) \\ LD & (345, 5) & \(\bm{0.884\pm 0.273}\) & \(\bm{0.881\pm 0.272}\) & \(\bm{0.989\pm 0.441}\) & \(\bm{0.977\pm 0.418}\) & \(\bm{0.977\pm 0.418}\) \\ AM & (398, 7) & \(\bm{0.415\pm 0.115}\) & \(\bm{0.417\pm 0.113}\) & \(0.505\pm 0.105\) & \(\bm{0.371\pm 0.103}\) & \(\bm{0.371\pm 0.103}\) \\ REV & (414, 6) & \(\bm{0.563\pm 0.096}\) & \(\bm{0.562\pm 0.095}\) & \(0.789\pm 0.130\) & \(\bm{0.532\pm 0.104}\) & \(\bm{0.532\pm 0.104}\) \\ FF & (517, 12) & \(\bm{0.874\pm 1.123}\) & \(\bm{0.874\pm 1.124}\) & \(\bm{0.910\pm 0.824}\) & \(\bm{0.852\pm 0.792}\) & \(\bm{0.852\pm 0.792}\) \\ ITT & (1020, 33) & \(\bm{0.818\pm 0.007}\) & \(\bm{0.497\pm 0.066}\) & \(0.560\pm 0.075\) & \(\bm{0.507\pm 0.072}\) & \(\bm{0.507\pm 0.072}\) \\ CCS & (1030, 8) & \(\bm{0.472\pm 10.026}\) & \(\bm{0.476\pm 10.06}\) & \(0.494\pm 10.020\) & \(\bm{0.301\pm 0.057}\) & \(\bm{0.301\pm 0.057}\) \\ ASN & (1503, 5) & \(0.568\pm 0.062\) & \(\bm{0.560\pm 0.092}\) & \(0.550\pm 0.069\) & \(\bm{0.352\pm 0.055}\) & \(\bm{0.352\pm 0.055}\) \\ CAC & (1994, 127) & \(\bm{0.571\pm 0.105}\) & \(0.585\pm 0.092\) & \(1.481\pm 0.167\) & \(\bm{0.703\pm 0.101}\) & \(\bm{0.703\pm 0.101}\) \\ PT & (5875, 19) & \(0.601\pm 0.067\) & \(\bm{0.590\pm 0.068}\) & \(0.479\pm 0.081\) & \(\bm{0.410\pm 0.076}\) & \(\bm{0.410\pm 0.076}\) \\ CCCP & (9568, 4) & \(\bm{0.241\pm 0.038}\) & \(\bm{0.241\pm 0.038}\) & \(0.358\pm 0.041\) & \(\bm{0.224\pm 0.037}\) & \(\bm{0.224\pm 0.037}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Root Mean Square Error \(\downarrow\) on UCI regression data sets. Ours results in better or matching performance compared with sampling and GLM, indicating the effectiveness of our method.

\begin{table}
\begin{tabular}{l c|c|c|c|c|c} \hline \hline  & & \multicolumn{2}{c|}{_MFVI (Diag. Cov.)_} & \multicolumn{2}{c}{_LA (Kron. Cov. for MLP, Diag. Cov. for ViT)_} \\  & & Sampling & Ours & Sampling & GLM & Ours \\ \hline MNIST & MLP & \(\mathbf{0.974}_{\pm 0.002}\) & \(\mathbf{0.974}_{\pm 0.002}\) & \(0.972_{\pm 0.002}\) & \(\mathbf{0.975}_{\pm 0.002}\) & \(\mathbf{0.975}_{\pm 0.002}\) \\ FMNIST & MLP & \(\mathbf{0.843}_{\pm 0.004}\) & \(\mathbf{0.842}_{\pm 0.004}\) & \(0.868_{\pm 0.004}\) & \(\mathbf{0.882}_{\pm 0.003}\) & \(\mathbf{0.881}_{\pm 0.003}\) \\ \cline{2-7} CIFAR-10 & ViT & \(\mathbf{0.978}_{\pm 0.001}\) & \(\mathbf{0.978}_{\pm 0.001}\) & \(0.971_{\pm 0.002}\) & \(\mathbf{0.974}_{\pm 0.002}\) & \(\mathbf{0.976}_{\pm 0.002}\) \\ CIFAR-100 & ViT & \(\mathbf{0.896}_{\pm 0.003}\) & \(\mathbf{0.895}_{\pm 0.003}\) & \(0.855_{\pm 0.004}\) & \(0.873_{\pm 0.003}\) & \(\mathbf{0.884}_{\pm 0.003}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Accuracy \(\uparrow\) on classification data sets. Ours results in better or matching performance compared with sampling and GLM, indicating the effectiveness of our method.

Figure 4: Kernel density plots over the predictive entropy from an MLP trained on FMNIST (blue, in-distribution) and data from MNIST (red, out-of-distribution). Our method results in a clear separation between the in- and out-of-distribution data.