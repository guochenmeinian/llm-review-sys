# Harnessing Hard Mixed Samples with Decoupled Regularizer

 Zicheng Liu\({}^{1,2,}\)1 &Siyuan Li\({}^{1,2,}\)1 &Ge Wang\({}^{1,2}\) &Chen Tan\({}^{1,2}\) &Lirong Wu\({}^{1,2}\) &Stan Z. Li\({}^{2,}\)2 &AI Lab, Research Center for Industries of the Future, Hangzhou, China; \({}^{1}\)Zhejiang University; \({}^{2}\)Westlake University; \(\{\)liuzicheng; lisiyuan; wangge; tanchen; lirongwu; stan.zq.li\(\}\)

Footnote 1: Equal contribution. \({}^{\dagger}\)Stan Z. Li (Stan.ZQ.Li@westlake.edu.cn) is the corresponding author.

###### Abstract

Mixup is an efficient data augmentation approach that improves the generalization of neural networks by smoothing the decision boundary with mixed data. Recently, _dynamic_ mixup methods have improved previous _static_ policies effectively (_e.g._, linear interpolation) by maximizing target-related salient regions in mixed samples, but excessive additional time costs are not acceptable. These additional computational overheads mainly come from optimizing the mixed samples according to the mixed labels. However, we found that the extra optimizing step may be redundant because label-mismatched mixed samples are informative hard mixed samples for deep models to localize discriminative features. In this paper, we thus are not trying to propose a more complicated _dynamic_ mixup policy but rather an efficient mixup objective function with a decoupled regularizer named Decoupled Mixup (DM). The primary effect is that DM can adaptively utilize those hard mixed samples to mine discriminative features without losing the original smoothness of mixup. As a result, DM enables _static_ mixup methods to achieve comparable or even exceed the performance of _dynamic_ methods without any extra computation. This also leads to an interesting objective design problem for mixup training that we need to focus on both smoothing the decision boundaries and identifying discriminative features. Extensive experiments on supervised and semi-supervised learning benchmarks across seven datasets validate the effectiveness of DM as a plug-and-play module. Source code and models are available at https://github.com/Westlake-AI/openmixup.

## 1 Introduction

Deep Learning has become the bedrock of modern AI for many tasks in machine learning [3] such as computer vision [19, 18], natural language processing [12]. Using a large number of learnable parameters, deep neural networks (DNNs) can recognize subtle dependencies in large training datasets to be later leveraged to perform accurate predictions on unseen data. However, models might overfit the training set without constraints or enough data [53]. To this

Figure 1: visualization of hard mixed sample mining by class activation mapping (CAM) [49] of ResNet-50 on ImageNet. From left to right, CAM of top-2 predicted classes using mixup cross-entropy (MCE) and decoupled mixup (DM) loss.

end, regularization techniques have been deployed to improve generalization [61], which can be categorized into data-independent or data-dependent ones [16]. Some data-independent strategies, for example, constrain the model by punishing the parameters' norms, such as weight decay [40]. Among data-dependent strategies, data augmentations [51] are widely used. The augmentation policies often rely on particular domain knowledge [58] in different fields.

Mixup [77], a data-dependent augmentation technique, is proposed to generate virtual samples by a linear combination of data pairs and the corresponding labels with the mixing ratio \(\lambda\in[0,1]\). Recently, a line of optimizable mixup methods are proposed to improve mixing policies to generate object-aware virtual samples by optimizing discriminative regions in the data space to match the corresponding labels [56, 23, 22] (referred to as _dynamic_ methods). However, although the _dynamic_ approach brings some performance gain, the extra computational overhead degrades the efficiency of mixup augmentation significantly. Specifically, the most computation of _dynamic_ methods is spent on optimizing label-mismatched samples, but the question of why these label-mismatched samples should be avoided during the mixup training has rarely been analyzed. In this paper, we find these mismatched samples are completely underutilized by _static_ mixup methods, and the problem lies in the loss function, mixed cross-entropy loss (MCE). Therefore, we argue that these mismatched samples are not only not _static_ mixup disadvantages but also hard mixed samples full of discriminative information. Taking CutMix [74] as an example, two types of hard mixed samples are shown on the _right_ of Figure 2. Since MCE loss forces the model's predictions to be consistent with the soft label distribution, _i.e.,_ the model cannot give high-confidence predictions for the relevant classes even if the feature is salient in hard mixed samples, we can say that these hard samples are not fully leveraged.

From this perspective, we expect the model to be able to mine these hard samples, _i.e.,_ to give confident predictions according to salient features for localizing discriminative characteristics, even if the proportion of features is small. Motivated by this finding, we introduce simple yet effective Decoupled Mixup (DM) loss, a mixup objective function for explicitly leveraging the hard samples during the mixup training. Based on the standard mixed cross-entropy (MCE) loss, an extra decoupled regularizer term is introduced to enhance the ability to mine underlying discriminative statistics in the mixed sample by independently computing the predicted probabilities of each mixed class. Figure 1 shows the proposed DM loss can empower the _static_ mixup methods to explore more discriminative features. Extensive experiments demonstrate that DM achieves data-efficiency training on supervised and semi-supervised learning benchmarks. Our contributions are summarized below:

* Unlike those dynamic mixup policies that design complicated mixing policies, we propose DM, a mixup objective function of mining discriminative features adaptively.
* Our work contributes more broadly to understanding mixup training: it is essential to focus not only on the smoothness by regression of the mixed labels but also on discrimination by encouraging the model to give reliable and confident predictions.
* Not only in supervised learning but the proposed DM can also be easily generalized to semi-supervised learning with a minor modification. By leveraging the unlabeled data, it can reduce the conformation bias and significantly improve performance.
* Comprehensive experiments on various tasks verify the effectiveness of DM, _e.g._, DM-based _static_ mixup policies achieve a comparable or even better performance than _dynamic_ methods without the extra computation.

Figure 2: Illustration of the two types of hard mixed samples in CutMix with ‘Squirrel’ and ‘Panda’ as an example. Hard mixed samples indicate that the mixed sample contains salient features of a class, but the value of the corresponding label is small. MCE loss fails to leverage these samples.

Related Work

Mixup Augmentation.As data-dependent augmentation techniques, mixup methods generate new samples by mixing samples and corresponding labels with well-designed mixing policies [77; 57; 69; 64]. The pioneering mixing method is Mixup [77], whose mixed samples are generated by linear interpolation between pairs of samples. ManifoldMix variants [57; 14] extend Mixup to the latent space of DNNs. After that, cut-based methods [74] are proposed to improve the mixup for localizing important features, especially in the vision field. Many researchers explore using nonlinear or optimizable sample mixup policies to generate more reliable mixed samples according to mixed labels, such as PuzzleMix variants [23; 22; 45], SaliencyMix variants [56; 60], AutoMix variants [38; 31], and SuperMix [11]. Concurrently, recent works try to generate more accurate mixed labels with saliency information [20] or attention maps [5; 9; 7] for Transformer architectures, which require prior pre-trained knowledge or attention information. On the contrary, the proposed decoupled mixup is a pluggable learning objective for mixup augmentations. Moreover, mixup methods extend to more than two elements [22; 11] and regression tasks [70]. Some researchers also utilize mixup augmentations to enhance contrastive learning [8; 21; 28; 50; 31] or masked image modeling [33; 6] to learn general representation in a self-supervised manner.

Semi-supervised Learning and Transfer Learning.Pseudo-Labeling [27] is a popular semi-supervised learning (SSL) method that utilizes artificial labels converted from teacher model predictions. MixMatch [2] and ReMixMatch [1] apply mixup on labeled and unlabeled data to enhance the diversity of the dataset. More accurate pseudo-labeling relies on data augmentation techniques to introduce consistency regularization, _e.g._, UDA [65] and FixMatch [52] employ weak and strong augmentations to improve the consistency. Furthermore, CoMatch [29] unifies consistency regularization, entropy minimization, and graph-based contrastive learning to mitigate confirmation bias. Recently proposed works [62; 4] that improve FixMatch by designing more accurate confidence-based pseudo-label selection strategies, _e.g._, FlexMatch [76] applying curriculum learning for updating confidence threshold dynamically and class-wisely. More recently, SemiReward [30] proposes a reward model to filter out accurate pseudo labels with reward scores. Fine-tuning a pre-trained model on labeled datasets is a widely adopted form of transfer learning (TL) in various applications. Previously, [13; 44] show that transferring pre-trained AlexNet features to downstream tasks outperforms hand-crafted features. Recent works mainly focus on better exploiting the discriminative knowledge of pre-trained models from different perspectives. L2-SP [35] promotes the similarity of the final solution with pre-trained weights by a simple L2 penalty. DELTA [34] constrains the model by a subset of pre-trained feature maps selected by channel-wise attention. BSS [68] avoids negative transfer by penalizing smaller singular values. More recently, Self-Tuning variants [67; 54] combined contrastive learning with TL to tackle confirmation bias and model shift issues in a one-stage framework.

## 3 Decoupled Mixup

### Preliminary

Mixed Cross-Entropy Underutilizes MixupLet us define \(y\in\mathbb{R}^{C}\) as the ground-truth label with \(C\) categories. For labeled data point \(x\in\mathbb{R}^{W\times H\times C}\) whose embedded representation \(z\) is obtained from the model \(M\) and the predicted probability \(p\) can be calculated through a Softmax function \(p=\sigma(z)\). Given the mixing ratio \(\lambda\in[0,1]\) and \(\lambda\)-related mixup mask \(H\in\mathbb{R}^{W\times H}\), the mixed sample \((x_{(a,b)},y_{(a,b)})\) can be generated as \(x_{(a,b)}=H\odot x_{a}+(1-H)\odot x_{b}\), and \(y_{(a,b)}=\lambda y_{a}+(1-\lambda)y_{b}\), where \(\odot\) denotes element-wise product, \((x_{a},y_{a})\) and \((x_{b},y_{b})\) are sampled from a labeled dataset \(L=\{(x_{a},y_{a})\}_{a=1}^{n_{L}}\). Note that superscripts denote the index; subscripts indicate the type of data, _e.g._, \(x_{(a,b)}\) represents a mixed sample generated from \(x_{a}\) and \(x_{b}\); \(y^{i}\) indicates the label value on \(i\)-th position. Since the mixup labels are obtained by somehow \(\lambda\)-based interpolation, the standard CE loss weighted by \(\lambda\), \(\mathcal{L}_{CE}=y_{(a,b)}^{T}\log\sigma(z_{(a,b)})\), is typically used as the objective in the mixup training:

\[\mathcal{L}_{MCE}=-\sum_{i=1}^{C}\big{(}\lambda\mathbb{I}(y_{a}^{i}=1)\log p_ {(a,b)}^{i}+(1-\lambda)\mathbb{I}(y_{b}^{i}=1)\log p_{(a,b)}^{i}\big{)}.\] (1)

where \(\mathbb{I}(\cdot)\in\{0,1\}\) is an indicator function that values one if and only if the input condition holds. Noticeably, these two items of Equation 1 are classifying \(y_{a}\) and \(y_{b}\) while keeping the linear consistency with mixing coefficient \(\lambda\). As a result, DNNs with this mixup consistency prefer relatively less confident results in high-entropy behaviour [46] and longer training time in practice. **The main reason is that in addition to \(\lambda\) constraint, the competing relationships defined by Softmax in \(\mathcal{L}_{MCE}\) are the main cause of the confidence drop, which is more obvious when dealing with hard mixed samples.** Precisely, the competition between the mixed class \(a\) and \(b\) in Equation 1 can severely affect the prediction of a single class; that is, interference from other classes prevents the model from focusing its attention. This typically causes the model to be insensitive to the salient features of the target and thus undermines model performance, as shown in Figure 1. Although the _dynamic_ mixup alleviates this problem, the extra time overhead is unavoidable if only focusing on mixing policies on the data level. Therefore, the key challenge is to design an ideal objective function for mixup training that maintains the smoothness of the mixup and can simultaneously explore the discriminative features without any computation costs.

### Decoupled Regularizer

To achieve the above goal, we first dive into the \(\mathcal{L}_{MCE}\) and propose the efficient decoupled mixup.

**Proposition 1**.: _Assuming \(x_{(a,b)}\) is generated from two different classes, minimizing \(\mathcal{L}_{MCE}\) is equivalent to regress corresponding \(\lambda\) in the gradient:_

\[(\nabla_{z_{(a,b)}}\mathcal{L}_{MCE})^{i}=\begin{cases}-\lambda+\frac{\exp(z _{(a,b)}^{i})}{\sum_{c}\exp(z_{(a,b)}^{c})},&i=a\\ -(1-\lambda)+\frac{\exp(z_{(a,b)}^{i})}{\sum_{c}\exp(z_{(a,b)}^{c})},&i=b\\ \frac{\exp(z_{(a,b)}^{i})}{\sum_{c}\exp(z_{(a,b)}^{c})},&i\neq a,b\end{cases}\] (2)

Softmax Degrades Confidence.As we can see from Proposition 1, the predicted probability of \(x_{(a,b)}\) will be consistent with \(\lambda\), and the probability is computed from the Softmax directly. The Softmax forces the sum of predictions to one (winner takes all), which is undesirable in mixup classification, especially when there are multiple and non-salient targets in mixed samples, _e.g.,_ hard mixed samples, as shown in Figure 2. The standard Softmax in \(\mathcal{L}_{MCE}\) deliberately suppresses confidence and produces high-entropy predictions by coupling all classes. As a consequence, \(\mathcal{L}_{MCE}\) makes many static mixup methods require longer epochs than vanilla training to achieve the desired results [57; 73]. Based on previous analysis, a novel mixup objective, decoupled mixup (DM), is raised to remove the Coupler and thus utilize the hard mixed samples adaptively, finally improving the performance of mixup methods. Specifically, for mixed data points \(z_{(a,b)}\) generated from a random pair in labelled dataset \(L\), an encoded mixed representation \(z_{(a,b)}=f_{\theta}(x_{(a,b)})\) is generated by a feature extractor \(f_{\theta}\). A mixed categorical probability of \(i\)-th class is attained:

\[\sigma(z_{(a,b)})^{i}=\frac{\exp(z_{(a,b)}^{i})}{\sum_{c}\exp(z_{(a,b)}^{c})}.\] (3)

Decoupled Softmax.where \(\sigma(\cdot)\) is standard Softmax. Equation 3 shows how the mixed probabilities are computed for a mixed sample. The competition between \(a\) and \(b\) is the main reason that results in low confidence of the model, _i.e.,_ the sum of semantic information of hard mixed samples are larger than "1" defined by Softmax. Therefore, we propose to simply remove the competitor class in Equation 3 to achieve decoupled Softmax. The score on \(i\)-th class is not affected by the \(j\)-th class:

\[\phi(z_{(a,b)})^{i,j}=\frac{\exp(z_{(a,b)}^{j})}{\overleftarrow{\exp(\text{ s}_{(\mathcal{R};b)}^{j}+\sum_{c\neq j}\exp(z_{(a,b)}^{c})}}.\] (4)

where \(\phi(\cdot)\) is the proposed decoupled Softmax. In Equation 4, by removing the competitor, compared with Equation 1, the decoupled Softmax makes all items associated with \(\lambda\) become -1 in gradient, the derivation is given in the A.1. Our Proposition 2 verifies that the expected results are achieved with decoupled Softmax.

**Proposition 2**.: _With the decoupled Softmax defined above, decoupled mixup cross-entropy \(\mathcal{L}_{DM}\) can boost the prediction confidence of the interested classes mutually and escape from the \(\lambda\)-constraint:_

\[\mathcal{L}_{DM}=-\sum_{i=1}^{c}\sum_{j=1}^{c}y_{a}^{i}y_{b}^{j}\left(\log \big{(}\frac{p_{(a,b)}^{i}}{1-p_{(a,b)}^{j}}\big{)}+\log\big{(}\frac{p_{(a,b)} ^{j}}{1-p_{(a,b)}^{i}}\big{)}\right).\] (5)The Decoupled Mixup.The proofs of Proposition 1 and 2 are given in the Appendix. In practice, the original smoothness of \(\mathcal{L}_{MCE}\) should not be lost, and thus the proposed DM is a regularizer for discriminability. The final form of decoupled mixup can be formulated as follows:

\[\mathcal{L}_{DM(CE)}=-\big{(}\underbrace{y_{(a,b)}^{T}\log(\sigma(z_{(a,b)}))}_ {\mathcal{L}_{MCE}}+\eta\underbrace{y_{[a,b]}^{T}\log(\phi(z_{(a,b)}))y_{[a,b] }}_{\mathcal{L}_{DM}}\big{)}.\]

where \(y_{(a,b)}\) indicates the mixed label while \(y_{[a,b]}\) is two-hot label encoding, \(\eta\) is a trade-off factor. Notice that \(\eta\) is robust and can be set according to the character of mixup methods (see Sec. 5.4).

_Practical consequences of such simple modification on mixup and the performance:_

Make What Should be Certain More Certain.As we expected, mixup training with a decoupling mechanism will be more accurate and confident in handling hard mixed samples with our artificially constructed hard mixed samples by using PuzzleMix. Figure 3_right_ demonstrates the model trained with decoupled mixup mostly doubled the top-2 accuracy on these mixed samples, which also verifies the information contained in mixed samples is beyond the "1" defined by standard Softmax. More interestingly, this advantage of decoupled mixup, _i.e.,_ higher confidence and accuracy, can be further amplified in semi-supervised learning due to the uncertainty of pseudo-labeling.

Enhance the Training Efficiency.It is straightforward to notice that there is no extra computation cost when using DM in vanilla mixup training, and the performance we can achieve is the same or even better than optimizable mixup policies, _i.e.,_ PuzzleMix, CoMixup, _etc._ Figure 3_left_ and _middle_ show decoupled mixup unveils the power of static mixup for more accurate and faster.

## 4 Extensions of Decoupled Mixup

With the high-accurate nature of decoupled mixup for mining hard mixed samples, semi-supervised learning is a suitable scenario to propagate the accurate label from labeled space to unlabeled space by using asymmetrical mixup. In addition, we can also generalize the decoupled mechanism into the binary cross-entropy for boosting the multi-classification task.

### Asymmetrical Strategy for Semi-supervised Learning

Based on labeled data \(L=\{(x_{a},y_{a})\}_{a=1}^{n_{L}}\), if we further consider unlabeled data \(U=\{(u_{a},v_{a})\}_{a=1}^{n_{U}}\) decoupled mixup can be the strong connection between \(L\) and \(U\). Recall the confirmation bias [67] problem of SSL: the performance of the student model is restricted by the teacher model when learning from inaccurate pseudo-labels. To fully use the \(L\) and strengthen the teacher model to provide more robust and accurate predictions, the unlabeled data with large \(\lambda\) can be used to mix with the labeled data to form hard mixed samples. With these hard mixed samples, we can employ decoupled mixup into semi-supervised learning effectively. Since only the label of \(L\) is accurate, we need to make a little asymmetric modification to the decoupled mixup, called Asymmetrical Strategy(AS). Formally, given the labeled and unlabeled datasets \(L\) and \(U\), AS builds reliable connection by generating hard mixed samples between \(L\) and \(U\) in an asymmetric manner (\(\lambda<0.5\)):

\[\hat{x}_{(a,b)}=\lambda x_{a}+(1-\lambda)u_{b};\quad\hat{y}_{(a,b)}=\lambda y _{a}+(1-\lambda)v_{b}.\]

Figure 3: Results illustration of applying decouple mixup. _Left_: taking MixUp as an example, our proposed decoupled mixup cross-entropy, DM(CE), significantly improves training efficiency by exploring hard mixed sample; _Middle_: Acc _vs._ cost on ImageNet-1k; _Right_: Top-2 acc is calculated when the top-2 predictions equal to \(\{y_{a},y_{b}\}\).

Due to the uncertainty of the pseudo-label, only the labeled part is retained in \(\mathcal{L}_{DM}\):

\[\hat{\mathcal{L}}_{DM}=y_{a}^{T}\log\big{(}\phi(z_{(a,b)})\big{)}y_{b},\]

where \(y_{a}\) and \(y_{b}\) are one-hot labels from \(L\). AS could be regarded as a special case of DM that only decouples on labeled data. Simply replacing \(\mathcal{L}_{DM}\) with \(\hat{\mathcal{L}}_{DM}\) can leverage the hard samples and alleviate the confirmation bias in semi-supervised learning.

### Decoupled Binary Cross-entropy Loss

Binary Cross-entropy Form of DM.Different from Softmax-based classification, we can also build decoupled mixup in multi-label classification tasks (\(1\)-\(vs\)-all) by using mixup binary cross-entropy (MBCE) loss [63] (\(\sigma(\cdot)\) denotes Sigmoid rather Softmax in this case). Proposition 2 demonstrates the decoupled CE can mutually enhance the confidence of predictions for the interested classes and be free from \(\lambda\) limitations. Similarly, for MBCE, since it is not inherently bound to mutual interference between classes by Softmax, we have to preserve partial consistency and encourage more confident predictions, and thus propose a decoupled mixup binary cross-entropy loss, DM(BCE).

To this end, a rescaling function \(r:\lambda,t,\xi\rightarrow\lambda^{\prime}\) is designed to achieve this goal. The mixed label is rescaled by \(r(\cdot)\): \(y_{mix}=\lambda_{a}y_{a}+\lambda_{b}y_{b}\), where \(\lambda_{a}\) and \(\lambda_{b}\) are rescaled. The rescaling function is defined as follows:

\[r(\lambda,t,\xi)=\big{(}\frac{\lambda}{\xi}\big{)}^{t},\quad 0\leq t,0\leq\xi<1,\] (6)

where \(\xi\) is the threshold, \(t\) is an index to control the convexity. As shown in Figure 4, Equation 6 has three situations: (a) when \(\xi=0\), \(t=0\), the rescaled label is always equal to \(1\), as two-hot encoding; (b) when \(\xi=1\), \(t=1\), \(r(\cdot)\) is a linear function (vanilla mixup); (c) the rest curves demonstrate \(t\) is the parameter that changes the concavity and \(\xi\) is responsible for truncating.

Empirical Results.In the case of interpolation-based mixup methods (_e.g._, Mixup, ManifoldMix, _etc._) that keep linearity between the mixed label and sample, the decoupled mechanism can be introduced by only adjusting threshold \(t\). In the case of cutting-based mixing policies (_e.g._, CutMix, _etc._) where the mixed samples and labels have a square relationship (generally a convex function), we can approximate the convexity by adjusting \(\xi\), which are detailed in Sec. 5.4 and Appendix C.5.

## 5 Experiments

We adopt two types of top-1 classification accuracy (Acc) metrics (the mean of three trials): (i) the median top-1 Acc of the last 10 epochs [52; 38] for supervised image classification tasks with Mixup variants, and (ii) the best top-1 Acc in all checkpoints for SSL tasks. Popular ConvNets and Transformer-based architectures are used as backbone networks: ResNet variants including ResNet [19] (R), Wide-ResNet (WRN) [75], and ResNeXt-32x4d (RX) [66], Vision Transformers including DeiT [55] and Swin Transformer (Swin) [37].

### Image Classification Benchmarks

This subsection evaluates performance gains of DM on six image classification benchmarks, including CIFAR-100 [25], Tiny-ImageNet (Tiny) [10], ImageNet-1k [48], CUB-200-2011 (CUB) [59], FGVC-Aircraft (Aircraft) [42]. There are mainly two types of mixup methods based on their mixing policies: _static_ methods including Mixup [77], CutMix [74], ManifoldMix [57], SaliencyMix [56], FMix [17], and ResizeMix [47], and _dynamic_ mixup methods including PuzzleMix [23], AutoMix [38], and SAMix [31]. For a fair comparison, we use the optimal \(\alpha\) in \(\{0.1,0.2,0.5,0.8,1.0,2.0\}\) for all mixup

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_EMPTY:9]

PuzzHix [23], and AutoMix [38], with decoupled mixup, and found the improvement is limited. The main reason is there will not be many hard mixed samples in _dynamic_ mixups. Therefore, we additionally incorporate two _static_ mixups in RSB training settings, _i.e.,_ a half probability that Mixup or CutMix will be selected during the training. As expected, the improvements from the decoupled mixup are getting obvious upon _static_ mixup variants. This is a very preliminary attempt that deserves more exploration in future works, and we provide more results of _dynamic_ mixups in Appendix C.

Meanwhile, we further conduct comprehensive comparison experiments with modern Transformer-based architectures on CIFAR-100, considering the concurrent work TransMix [5] and TokenMix [36]. As shown in Table 10, where results with \(\dagger\) denote the official implementation and the other are based on OpenMixup [32], DM(CE) enables DeiT (CutMix and Mixup) to achieve competitive performances as _dynamic_ mixup variants like AutoMix and SAMix [31] based on ConvNeXt-S without introducing extra computational costs, while still performing worse than them based on DeiT-S. Compared with specially designed label mixing methods using attention maps, DM(CE) also achieves competitive performances to TransMix and TokenMix. How to further improve the decoupled mixup with the salient regions or dynamic attention information to research similar performances of _dynamic_ mixing variants can also be studied in future works.

The Next Mixup.In a word, we introduce Decoupled Mixup (DM), a new objective function for considering both smoothness and mining discriminative features in mixup augmentations. The proposed DM helps _static_ mixup methods (_e.g.,_ MixUp and CutMix) achieve a comparable or better performance than the computationally expensive _dynamic_ mixup policies. Most importantly, DM raises a question worthy of researching: _is it necessary to design very complex mixup policies?_ We also find that decoupled mixup could be the bridge to combining _static_ and _dynamic_ mixup. However, the introduction of additional hyperparameters may take users some extra time to check on other than images or other mixup methods. This also leads to the core question of the next step in the development of this work: how to design a more elegant and adaptive mixup training objective that connects different types of mixups to achieve high data efficiency? We believe these explorations and questions can inspire future research in the community of mixup augmentations.

## Acknowledgement

This work was supported by National Key R&D Program of China (No. 2022ZD0115100), National Natural Science Foundation of China Project (No. U21A20427), and Project (No. WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University. We thank the AI Station of Westlake University for the support of GPUs and thank all reviewers for polishing the manuscript.

## References

* [1] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. _arXiv preprint arXiv:1911.09785_, 2019.
* [2] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holistic approach to semi-supervised learning. _arXiv preprint arXiv:1905.02249_, 2019.

\begin{table}
\begin{tabular}{l|c c c c|c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{4}{c|}{DeiT-Small} & \multicolumn{4}{c}{ConvNeXt-Tiny} \\  & 200 & ep & 600 & pr & Mem. & 200 & ep & 600 & Mem. & Time \\ \hline Vanilla & 65.81 & 68.50 & 8.1 & 27 & 78.70 & 80.65 & 4.2 & 10 \\ Mixup & 69.98 & 76.35 & 8.2 & 27 & 81.13 & 83.08 & 4.2 & 10 \\ CutMix & 74.12 & 79.54 & 8.2 & 27 & 82.46 & 83.20 & 4.2 & 10 \\ DeiT & 75.92 & 79.38 & 8.2 & 27 & 83.09 & 84.12 & 4.2 & 10 \\ SmoothMix & 67.54 & 80.25 & 8.2 & 27 & 78.87 & 81.31 & 4.2 & 10 \\ Saliency Mix & 69.78 & 76.60 & 8.2 & 27 & 82.82 & 83.03 & 4.2 & 10 \\ AntentiveMix+ & 75.98 & 80.33 & 8.3 & 85 & 82.59 & 83.04 & 4.3 & 14 \\ FMix & 70.41 & 74.31 & 8.2 & 27 & 81.79 & 82.29 & 4.2 & 10 \\ GidMix & 68.86 & 74.96 & 8.2 & 27 & 79.53 & 79.66 & 4.2 & 10 \\ ResizeMix & 68.45 & 71.95 & 8.2 & 27 & 82.53 & 82.91 & 4.2 & 10 \\ \hline PuzzMix & 73.60 & 81.01 & 8.3 & 35 & 82.29 & 84.17 & 4.3 & 53 \\ AutoMix & 76.24 & 80.91 & 18.29 & 59 & 83.30 & 84.79 & 10.2 & 56 \\ SAMix & **77.94** & **82.49** & 1.2 & 58 & **83.56** & **84.98** & 10.3 & 57 \\ \hline DeiT+TransMix & 76.17 & 79.33 & 8.4 & 28 & - & - & - & - \\ DeiT+TokenMix\({}^{\dagger}\) & 76.25 & 79.57 & 8.4 & 34 & - & - & - & - \\
**Det+DM(CE)** & 76.20 & 79.92 & 8.2 & 27 & 83.41 & 84.49 & 4.2 & 10 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Top-1 Acc (%)\(\dagger\) of on CIFAR-100 training 200 and 600 epochs based on DeiT-S and ConvNeXt-T. Underlines denote the top-3 best results. Total training hours and GPU memory are collected on a single A100 GPU.

* [3] Christopher M Bishop. _Pattern recognition and machine learning_. springer, 2006.
* [4] Hao Chen, Ran Tao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Bhiksha Raj, and Marios Savvides. Softmatch: Addressing the quantity-quality tradeoff in semi-supervised learning. In _The Eleventh International Conference on Learning Representations_, 2022.
* [5] Jie-Neng Chen, Shuyang Sun, Ju He, Philip Torr, Alan Yuille, and Song Bai. Transmix: Attend to mix for vision transformers. In _Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [6] Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, and Dit-Yan Yeung. Mixed autoencoder for self-supervised visual representation learning. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* [7] Mengzhao Chen, Mingbao Lin, Zhihang Lin, Yu xin Zhang, Fei Chao, and Rongrong Ji. Smmix: Self-motivated image mixing for vision transformers. _Proceedings of the International Conference on Computer Vision (ICCV)_, 2023.
* [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2020.
* [9] Hyeong Kyu Choi, Joonmyung Choi, and Hyunwoo J. Kim. Tokenmixup: Efficient attention-guided token-level data augmentation for transformers. _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [10] Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an alternative to the cifar datasets. _arXiv preprint arXiv:1707.08819_, 2017.
* [11] Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, and Nasser M Nasrabadi. Supermix: Supervising the mixing data augmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 13794-13803, 2021.
* [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [13] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2014.
* [14] Mojtaba Faramarzi, Mohammad Amini, Akilesh Badrinaaraayanan, Vikas Verma, and Sarath Chandar. Patchup: A regularization technique for convolutional neural networks. _arXiv preprint arXiv:2006.07794_, 2020.
* [15] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In _International Conference on Learning Representations (ICLR)_, 2015.
* [16] Hongyu Guo, Yongyi Mao, and Richong Zhang. Mixup as locally linear out-of-manifold regularization. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, pages 3714-3722, 2019.
* [17] Ethan Harris, Antonia Marcu, Matthew Painter, Mahesan Niranjan, and Adam Prugel-Bennett Jonathon Hare. Fmix: Enhancing mixed sample data augmentation. _arXiv preprint arXiv:2002.12047_, 2(3):4, 2020.
* [18] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2017.
* [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2016.

* [20] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, and Jiashi Feng. All tokens matter: Token labeling for training better vision transformers. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [21] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [22] Jang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint mixup with supermodular diversity. In _International Conference on Learning Representations (ICLR)_, 2021.
* [23] Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local statistics for optimal mixup. In _International Conference on Machine Learning (ICML)_, pages 5275-5285. PMLR, 2020.
* [24] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13)_, 2013.
* [25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In _Advances in neural information processing systems (NeurIPS)_, pages 1097-1105, 2012.
* [27] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In _Workshop on challenges in representation learning, ICML_, page 896, 2013.
* [28] Kibok Lee, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin, and Honglak Lee. I-mix: A domain-agnostic strategy for contrastive representation learning. In _International Conference on Learning Representations (ICLR)_, 2021.
* [29] Junnan Li, Caiming Xiong, and Steven Hoi. Comatch: Semi-supervised learning with contrastive graph regularization. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.
* [30] Siyuan Li, Weiyang Jin, Zedong Wang, Fang Wu, Zicheng Liu, Cheng Tan, and Stan Z. Li. Semireward: A general reward model for semi-supervised learning. _ArXiv_, abs/2111.15454, 2023.
* [31] Siyuan Li, Zicheng Liu, Zedong Wang, Di Wu, Zihan Liu, and Stan Z. Li. Boosting discriminative visual representation learning with scenario-agnostic mixup. _ArXiv_, abs/2111.15454, 2021.
* [32] Siyuan Li, Zedong Wang, Zicheng Liu, Di Wu, and Stan Z. Li. Openmixup: Open mixup toolbox and benchmark for visual representation learning. https://github.com/Westlak e-AI/openmixup, 2022.
* from vit back to cnn. In _International Conference on Machine Learning (ICML)_, 2023.
* [34] Xingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao, Liping Liu, and Jun Huan. Delta: Deep learning transfer using feature map with attention for convolutional networks. In _International Conference on Learning Representations (ICLR)_, 2019.
* [35] Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with convolutional networks. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2018.

* [36] Jihao Liu, B. Liu, Hang Zhou, Hongsheng Li, and Yu Liu. Tokenmix: Rethinking image mixing for data augmentation in vision transformers. In _European Conference on Computer Vision (ECCV)_, 2022.
* [37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _International Conference on Computer Vision (ICCV)_, 2021.
* [38] Zicheng Liu, Siyuan Li, Di Wu, Zhiyuan Chen, Lirong Wu, Liu Zihan, and Stan Z Li. Automix: Unveiling the power of mixup for stronger classifier. In _Proceedings of the European Conference on Computer Vision (ECCV)_. Springer, 2022.
* [39] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. _arXiv preprint arXiv:1608.03983_, 2016.
* [40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations (ICLR)_, 2019.
* [42] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. _arXiv preprint arXiv:1306.5151_, 2013.
* [43] Muhammad Muzammal Naseer, Kanchana Ranasinghe, Salman H Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. _Advances in Neural Information Processing Systems_, 34:23296-23308, 2021.
* [44] Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level image representations using convolutional neural networks. In _Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)_, 2014.
* [45] Joonhyung Park, June Yong Yang, Jinwoo Shin, Sung Ju Hwang, and Eunho Yang. Saliency grafting: Innocuous attribution-guided mixup with calibrated label mixing. In _AAAI Conference on Artificial Intelligence_, 2022.
* [46] Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip HS Torr, and Puneet K Dokania. Regmixup: Mixup as a regularizer can surprisingly improve accuracy and out distribution robustness. _arXiv preprint arXiv:2206.14502_, 2022.
* [47] Jie Qin, Jiemin Fang, Qian Zhang, Wenyu Liu, Xingang Wang, and Xinggang Wang. Resizemix: Mixing data with preserved object information and true labels. _arXiv preprint arXiv:2012.11101_, 2020.
* [48] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International journal of computer vision (IJCV)_, pages 211-252, 2015.
* [49] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. _arXiv preprint arXiv:1610.02391_, 2019.
* [50] Zhiqiang Shen, Zechun Liu, Zhuang Liu, Marios Savvides, Trevor Darrell, and Eric Xing. Unmix: Rethinking image mixtures for unsupervised visual representation learning. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, 2021.
* [51] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. _Journal of Big Data_, 6(1):1-48, 2019.
* [52] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.

* [53] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. _The journal of machine learning research (JMLR)_, 15(1):1929-1958, 2014.
* [54] Cheng Tan, Zhangyang Gao, Lirong Wu, Siyuan Li, and Stan Z Li. Hyperspherical consistency regularization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7244-7255, 2022.
* [55] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _International Conference on Machine Learning (ICML)_, pages 10347-10357, 2021.
* [56] AFM Uddin, Mst Monira, Wheemyung Shin, TaeChoong Chung, Sung-Ho Bae, et al. Saliencymix: A saliency guided data augmentation strategy for better regularization. _arXiv preprint arXiv:2006.01791_, 2020.
* [57] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In _International Conference on Machine Learning (ICML)_, pages 6438-6447. PMLR, 2019.
* [58] Vikas Verma, Thang Luong, Kenji Kawaguchi, Hieu Pham, and Quoc Le. Towards domain-agnostic contrastive learning. In _International Conference on Machine Learning (ICML)_, pages 10530-10541. PMLR, 2021.
* [59] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.
* 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 3642-3646, 2020.
* [61] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In _International conference on machine learning (ICML)_, pages 1058-1066. PMLR, 2013.
* [62] Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, et al. Freematch: Self-adaptive thresholding for semi-supervised learning. In _The Eleventh International Conference on Learning Representations_, 2022.
* [63] Ross Wightman, Hugo Touvron, and Herve Jegou. Resnet strikes back: An improved training procedure in timm, 2021.
* [64] Lirong Wu, Jun Xia, Zhangyang Gao, Haitao Lin, Cheng Tan, and Stan Z Li. Graphmixup: Improving class-imbalanced node classification by reinforcement mixup and self-supervised context prediction. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_, pages 519-535. Springer, 2022.
* [65] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation for consistency training. _arXiv preprint arXiv:1904.12848_, 2019.
* [66] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)_, pages 1492-1500, 2017.
* [67] Wang Ximei, Gao Jinghan, Long Mingsheng, and Wang Jianmin. Self-tuning for data-efficient deep learning. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2021.

* [68] Chen Xinyang, Wang Sinan, Fu Bo, Long Mingsheng, and Wang Jianmin. Catastrophic forgetting meets negative transfer: Batch spectral shrinkage for safe transfer learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [69] Huaxiu Yao, Yiping Wang, Linjun Zhang, James Y Zou, and Chelsea Finn. C-mixup: Improving generalization in regression. _Advances in Neural Information Processing Systems_, 35:3361-3376, 2022.
* [70] Huaxiu Yao, Yiping Wang, Linjun Zhang, James Y. Zou, and Chelsea Finn. C-mixup: Improving generalization in regression. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [71] Kaichao You, Zhi Kou, Mingsheng Long, and Jianmin Wang. Co-tuning for transfer learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [72] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training BERT in 76 minutes. In _International Conference on Learning Representations (ICLR)_, 2020.
* [73] Hao Yu, Huanyu Wang, and Jianxin Wu. Mixup without hesitation. In _International Conference on Image and Graphics (ICIG)_, pages 143-154. Springer, 2021.
* [74] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 6023-6032, 2019.
* [75] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In _Proceedings of the British Machine Vision Conference (BMVC)_, 2016.
* [76] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [77] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.

## Appendix

In the Appendix sections, we provide proofs of proposition 1 (SSA.1) and proposition 2 (SSA.2), implementation details (SSB), and more results of comparison experiment and empirical analysis (SSC).

## Appendix A Proof of Proposition

### Proof of Proposition 1

**Proposition 1**.: Assuming \(x_{(a,b)}\) is generated from two different classes, minimizing \(\mathcal{L}_{MCE}\) is equivalent to regress corresponding \(\lambda\) in the gradient of \(\mathcal{L}_{MCE}\):

\[(\nabla_{z_{(a,b)}}\mathcal{L}_{MCE})^{l}=\begin{cases}-\lambda+\frac{\exp(z_{ (a,b)}^{i})}{\sum_{c}\exp(z_{(a,b)}^{c})},&l=i\\ -(1-\lambda)+\frac{\exp(z_{(a,b)}^{j})}{\sum_{c}\exp(z_{(a,b)}^{i})},&l=j\\ \frac{\exp(z_{(a,b)}^{j})}{\sum_{c}\exp(z_{(a,b)}^{c})},&l\neq i,j\end{cases}\] (7)

Proof.: For the mixed sample \((x_{(a,b)},y_{(a,b)})\), \(z_{(a,b)}\) is derived from a feature extractor \(f_{\theta}\) (i.e \(z_{(a,b)}=f_{\theta}(x_{(a,b)})\)). According to the definition of the mixup cross-entropy loss \(\mathcal{L}_{MCE}\), we have:

\[\big{(}\nabla_{z(a,b)}\mathcal{L}_{MCE}\big{)}^{l} =\frac{\partial\mathcal{L}_{MCE}}{\partial z_{(a,b)}^{l}}=-\frac{ \partial}{\partial z_{(a,b)}^{l}}\Big{(}y_{(a,b)}^{T}\log\big{(}\sigma(z_{(a, b)})\big{)}\Big{)}\] \[=-\sum_{i=1}^{C}\Big{(}y_{(a,b)}^{i}\frac{\partial}{\partial z_{ (a,b)}^{l}}\big{(}\log(\frac{\exp(z_{(a,b)}^{i})}{\sum_{j=1}^{C}\exp(z_{(a,b)}^ {j})})\big{)}\Big{)}\] \[=-\sum_{i=1}^{C}\Big{(}y_{(a,b)}^{i}\frac{\sum_{j=1}^{C}\exp(z_{( a,b)}^{j})}{\exp(z_{(a,b)}^{i})}\frac{\partial}{\partial z_{(a,b)}^{l}}\big{(} \frac{\exp(z_{(a,b)}^{i})}{\sum_{j=1}^{C}\exp(z_{(a,b)}^{j})}\big{)}\Big{)}\] \[=-\sum_{i=1}^{C}\Big{(}y_{(a,b)}^{i}\big{(}\delta_{i}^{l}-\frac{ \exp(z_{(a,b)}^{l})}{\sum_{j=1}^{C}\exp(z_{(a,b)}^{j})}\big{)}\Big{)}\] \[=\frac{\exp(z_{(a,b)}^{l})}{\sum_{j=1}^{C}\exp(z_{(a,b)}^{j})}-y_ {(a,b)}^{l}.\]

Similarly, we have:

\[\big{(}\nabla_{z(a,b)}\mathcal{L}_{DM}\big{)}^{l} =\frac{\partial\mathcal{L}_{DM}}{\partial z_{(a,b)}^{l}}=-\frac{ \partial}{\partial z_{(a,b)}^{l}}\Big{(}y_{[a,b]}^{T}\log\big{(}H(z_{(a,b)}) \big{)}y_{[a,b]}\Big{)}\] \[=-\frac{\partial}{\partial z_{(a,b)}^{l}}\Big{(}\sum_{i,j=1}^{C} y_{a}^{i}\log(\frac{\exp(z_{(a,b)}^{i})}{\sum_{k\neq j}^{C}\exp(z_{(a,b)}^{j})})y_{ b}^{j}+\sum_{i,j=1}^{C}y_{a}^{j}\log(\frac{\exp(z_{(a,b)}^{i})}{\sum_{k\neq i}^{C} \exp(z_{(a,b)}^{j})})y_{b}^{i}\Big{)}\] \[=-\sum_{i,j=1}^{C}\Big{(}y_{a}^{i}y_{b}^{j}\frac{\partial}{ \partial z_{(a,b)}^{l}}\big{(}\log(\frac{\exp(z_{(a,b)}^{i})}{\sum_{k\neq j}^ {C}\exp(z_{(a,b)}^{k})})+\log(\frac{\exp(z_{(a,b)}^{j})}{\sum_{k\neq i}^{C}\exp (z_{(a,b)}^{k})}))\Big{)}\] \[=-\sum_{i,j=1}^{C}\Big{(}y_{a}^{i}y_{b}^{j}\big{(}\delta_{i}^{l}- \frac{\sum_{k\neq j}\exp(z_{(a,b)}^{k})\delta_{k}^{l}}{\sum_{k\neq j}\exp(z_{(a, b)}^{k})}+\delta_{j}^{l}-\frac{\sum_{k\neq i}\exp(z_{(a,b)}^{k})\delta_{k}^{l}}{ \sum_{k\neq i}\exp(z_{(a,b)}^{k})}\big{)}\Big{)}\] \[=\frac{\sum_{k\neq i}\exp(z_{(a,b)}^{k})\delta_{k}^{l}}{\sum_{k \neq i}\exp(z_{(a,b)}^{k})}+\frac{\sum_{k\neq j}\exp(z_{(a,b)}^{k})\delta_{k}^{l} }{\sum_{k\neq j}\exp(z_{(a,b)}^{k})}-\delta_{i}^{l}-\delta_{j}^{l}.\]Thus, for \(\mathcal{L}_{DM}\) loss:

\[(\nabla_{z_{(a,b)}}\mathcal{L}_{MCE})^{l}=\begin{cases}-1+\frac{\exp(z_{(a,b)}^{i })}{\sum_{c\neq i}\exp(z_{(a,b)}^{i})},&l=i\\ -1+\frac{\exp(z_{(a,b)}^{i})}{\sum_{c\neq i}\exp(z_{(a,b)}^{i})},&l=j\\ \frac{\exp(z_{(a,b)}^{i})}{\sum_{c\neq i}\exp(z_{(a,b)}^{i})}+\frac{\exp(z_{(a, b)}^{I})}{\sum_{c\neq j}\exp(z_{(a,b)}^{i})},&l\neq i,j\end{cases}\] (8)

### Proof of Proposition 2

**Proposition 2**.: With the decoupled Softmax defined above, decoupled mixup cross-entropy \(\mathcal{L}_{DM(CE)}\) can boost the prediction confidence of the interested classes mutually and escape from the \(\lambda\)-constraint:

\[\mathcal{L}_{DM(CE)}=\sum_{i=1}^{c}\sum_{j=1}^{c}y_{a}^{i}y_{b}^{j}\Big{(}\log \big{(}\frac{p_{(a,b)}^{i}}{1-p_{(a,b)}^{j}}\big{)}+\log\big{(}\frac{p_{(a,b)}^ {j}}{1-p_{(a,b)}^{i}}\big{)}\Big{)}.\]

Proof.: For the mixed sample \((x_{(a,b)},y_{(a,b)})\), \(z_{(a,b)}\) is derived from a feature extractor \(f_{\theta}\) (i.e \(z_{(a,b)=f_{\theta}(x_{(a,b)})}\)). According to the definition of the mixup cross-entropy loss \(\mathcal{L}_{DM(CE)}\), we have:

\[\mathcal{L}_{DM(CE)} =y_{[a,b]}^{T}\log\big{(}H(Z_{(a,b)})\big{)}y_{[a,b]}\] \[\triangleq y_{a}^{T}\log\big{(}H(Z_{(a,b)})\big{)}y_{b}+y_{b}^{T}\log \big{(}H(Z_{(a,b)})\big{)}y_{a}\] \[=\sum_{i,j=1}^{C}y_{a}^{i}\log(\frac{\exp(z_{(a,b)}^{i})}{\sum_{k \neq j}^{C}\exp(z_{(a,b)}^{j})})y_{b}^{j}+\sum_{i,j=1}^{C}y_{a}^{j}\log(\frac{ \exp(z_{(a,b)}^{i})}{\sum_{k\neq i}^{C}\exp(z_{(a,b)}^{j})})y_{b}^{i}\] \[=\sum_{i,j=1}^{C}y_{a}^{i}y_{b}^{j}\big{(}\log(\frac{\exp(z_{(a,b )}^{i})}{\sum_{k\neq j}^{C}\exp(z_{(a,b)}^{j})})+\log(\frac{\exp(z_{(a,b)}^{j}) }{\sum_{k\neq i}^{C}\exp(z_{(a,b)}^{i})})\big{)}\] \[=\sum_{i,j=1}^{C}y_{a}^{i}y_{b}^{j}\big{(}\log(\frac{\frac{\exp(z_ {(a,b)}^{i})}{\sum_{k\neq j}^{C}\exp(z_{(a,b)}^{j})}}{\sum_{k=1}^{C}\exp(z_{(a,b)}^{i})}+\frac{\frac{\exp(z_{(a,b)}^{j})}{\sum_{k=1}^{C}\exp(z_{(a,b)}^{i})} }{\frac{\sum_{k\neq i}^{C}\exp(z_{(a,b)}^{i})}{\sum_{k=1}^{C}\exp(z_{(a,b)}^{ i})}})\big{)}\] \[=\sum_{i,j=1}^{C}y_{a}^{i}y_{b}^{j}\big{(}\log(\frac{p_{(a,b)}^{ i}}{1-p_{(a,b)}^{j}})+\log(\frac{p_{(a,b)}^{i}}{1-p_{(a,b)}^{i}})\big{)},\]

where \(p_{(a,b)}=\sigma(z_{(a,b)})\).

Implementation Details

### Dataset

We briefly introduce used image datasets. (1) Small scale classification benchmarks: CIFAR-10/100 [25] contains 50,000 training images and 10,000 test images in 32\(\times\)32 resolutions, with 10 and 100 classes settings. Tiny-ImageNet [10] is a rescaled version of ImageNet-1k, which has 10,000 training images and 10,000 validation images of 200 classes in 64\(\times\)64 resolutions. (2) Large scale classification benchmarks: ImageNet-1k [26] contains 1,281,167 training images and 50,000 validation images of 1000 classes in 224\(\times\)224 resolutions. (3) Small-scale fine-grained classification scenarios: CUB-200-2011 [59] contains 11,788 images from 200 wild bird species for fine-grained classification. FGVC-Aircraft [42] contains 10,000 images of 100 classes of aircraft. Standford-Cars [24].

### Training Settings

Small-scale image classification.As for small-scale classification benchmarks on CIFAR-100 and Tiny-ImageNet datasets, we adopt the CIFAR version of ResNet variants, _i.e._, using a \(3\times 3\) convolution instead of the \(7\times 7\) convolution and MaxPooling in the stem, and follow the common training settings [23, 38]: the basic data augmentation includes RandomFlip and RandomCrop with 4 pixels padding; SGD optimizer and Cosine learning rate Scheduler [39] are used with the SGD weight decay of 0.0001, the momentum of 0.9, and the Batch size of 100; all methods train 800 epochs with the basic learning rate \(lr=0.1\) on CIFAR-100 and 400 epochs with \(lr=0.2\) on Tiny-ImageNet.

Fine-grained image classification.As for fine-grained classification experiments on CUB-200 and Aircraft datasets, all mixup methods are trained 200 epochs by SGD optimizer with the initial learning rate \(lr=0.001\), the weight decay of 0.0005, and the batch size of 16. We use the standard augmentations RandomFlip and RandomResizedCrop, and load the official PyTorch pre-trained models on ImageNet-1k as initialization.

ImageNet image classification.For large-scale classification tasks on ImageNet-1k, we evaluate mixup methods on three popular training procedures, and Tab. A1 shows the full training settings of the three settings. Notice that DeiT [55] and RSB A3 [63] settings employ Mixup and CutMix with a switching probability of 0.5 during training. (a) PyTorch-style setting. Without any advanced training strategies, a PyTorch-style setting is used to study the performance gains of mixup methods: SGD optimizer is used to train 100 epochs with the SGD weight decay of 0.0001, a momentum of 0.9, a batch size of 256, and the basic learning rate of 0.1 adjusted by Cosine Scheduler. Notice that we replace the step learning rate decay with Cosine Scheduler [39] for better performances following [74]. (b) DeiT [55] setting. We use the DeiT setting to verify the DM(CE) effectiveness in training Transformer-based networks: AdamW optimizer [41] is used to train 300 epochs with a batch size of 1024, the basic learning rate of 0.001, and the weight decay of 0.05. (c) RSB A3 [63] setting. This setting adopts similar training techniques as DeiT to ConvNets, _especially using MBCE instead of MCE_: LAMB optimizer [72] is used to train 100 epochs with the batch size of 2048, the basic learning rate of 0.008, and the weight decay of 0.02. Notice that DeiT and RSB A3 settings use the combination of Mixup and CutMix (50% random switching probabilities) as the baseline.

Semi-supervised transfer learning.For semi-supervised transfer learning benchmarks, we use the same hyper-parameters and augmentations as Self-Tuning3: all methods are initialized by PyTorch pre-trained models on ImageNet-1k and trained 27k steps in total by SGD optimizer with the basic learning rate of \(0.001\), the momentum of \(0.9\), and the weight decay of \(0.0005\). We reproduced Self-Tuning and conducted all experiments in OpenMixup [32].

Footnote 3: https://github.com/thuml/Self-Tuning

Semi-supervised learning.For semi-supervised learning benchmarks (training from scratch), we adopt the most commonly used CIFAR-10/100 datasets among the famous SSL benchmarks based on WRN-28-2 and WRN-28-8 following [52, 76]. For a fair comparison, we use the same hyperparameters and training settings as the original papers and adopt the open-source codebase TorchSSL [76] for all methods. Concretely, we use an SGD optimizer with a basic learning rate of \(lr=0.03\) adjusted by Cosine Scheduler, the total \(2^{20}\) steps, the batch size of 64 for labeled data, and the confidence threshold \(\tau=0.95\).

### Hyper-parameter Settings

We follow the basic hyper-parameter settings (_e.g.,_\(\alpha\)) for mixup variants in OpenMixup [32], where we reproduce most comparison methods. Notice that _static_ methods denote Mixup [77], CutMix [74], ManifoldMix [57], SaliencyMix [56], FMix [17], ResizeMix [47], and _dynamic_ methods denote PuzzleMix [23], AutoMix [38], and SAMix [31]). Similarly, _interpolation-based_ methods denote Mixup and ManifoldMix while _cutting-based_ methods denote the rest mixup variants mentioned above. We set the hyper-parameters of DM(CE) as follows: For CIFAR-100 and ImageNet-1k, _static_ methods use \(\eta=0.1\), and _dynamic_ methods use \(\eta=1\). For Tiny-ImageNet and fine-grained datasets, _static_ methods use \(\eta=1\) based on ResNet-18 while \(\eta=0.1\) based on ResNeXt-50; _dynamic_ methods use \(\eta=1\). As for the hyper-parameters of DM(BCE) on ImageNet-1k, _cutting-based_ methods use \(t=1\) and \(\xi=0.8\), while _interpolation-based_ methods use \(t=0.5\) and \(\xi=1\). Note that we use \(\alpha=0.2\) and \(\alpha=2\) for the _static_ and _dynamic_ methods when using the proposed DM.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c} \hline \hline Datasets & \multicolumn{6}{c|}{CIFAR-100} & \multicolumn{3}{c}{Tiny-ImageNet} \\ \hline  & \multicolumn{3}{c|}{R-18} & \multicolumn{3}{c|}{RX-50} & \multicolumn{3}{c|}{WRN-28-8} & \multicolumn{3}{c|}{R-18} & \multicolumn{3}{c}{RX-50} \\ Methods & \multicolumn{3}{c|}{MCE} & \multicolumn{3}{c|}{**DM(CE)**} & \multicolumn{3}{c|}{**MCE**} & \multicolumn{3}{c|}{**DM(CE)**} & \multicolumn{3}{c|}{**MCE**} & \multicolumn{3}{c}{**DM(CE)**} \\ \hline SaliencyMix & 79.12 & **79.28** & 81.53 & **82.61** & 84.35 & **84.41** & 64.60 & **66.56** & 66.55 & **67.52** \\ PuzzleMix & 81.13 & **81.34** & 82.85 & **82.97** & 85.02 & **85.25** & 65.81 & **66.52** & 67.83 & **68.04** \\ AutoMix & 82.04 & **82.32** & 83.64 & **83.94** & 85.18 & **85.38** & 67.33 & **68.18** & 70.72 & **71.56** \\ SAMix & 82.30 & **82.40** & 84.42 & **84.53** & 85.50 & **85.59** & 68.89 & **69.16** & 72.18 & **72.39** \\ \hline Avg. Gain & \multicolumn{3}{c|}{**+0.19**} & \multicolumn{3}{c|}{**+0.40**} & \multicolumn{3}{c|}{**+0.15**} & \multicolumn{3}{c|}{**+0.95**} & \multicolumn{3}{c}{**+0.56**} \\ \hline \hline \end{tabular}
\end{table}
Table 12: Ingredients and hyper-parameters used for ImageNet-1k training settings.

[MISSING_PAGE_FAIL:21]

FGSM [15] white-box attack of 8/255 \(\ell_{\infty}\) epsilon ball following [23]. Table A7 shows that DM(CE) improves top-1 Acc of MCE while maintaining the competitive FGSM error rates for five popular mixup algorithms, which indicates that DM(CE) can _boost discrimination without disturbing the smoothness properties_ of mixup variants.

### Data-efficient Mixup with Limited Training Labels

To further DM whether data-efficient mixup training can be truly achieved, we conducted supervised experiments on CIFAR-100 with different sizes of training data. 15%, 30%, and 50% of the CIFAR-100 data are randomly selected as training data, and the test data are unchanged. The proposed decoupled mixup uses DM(CE) as the loss function by default. From Table A8, we can see that DM improves performance consistently without any computational overhead. Especially when using only 15% of the data, DM can improve accuracy by 2%. Therefore, combined with the experimental results of semi-supervised learning in Sec. 5.3 and Sec. 5.2, we can say that mixup training with DM is more data-efficient with limited data.

### Empirical Analysis

In addition to occlusion robustness in Figure 6, we analyze the top-1 and top-2 mixup classification accuracy and visualize validation accuracy curves during training to empirically demonstrate the effectiveness of DM in Figure A1.

### Ablation Study and Analysis

Ablation of hyper-parametersWe first provide ablation experiments of the shared hyper-parameter \(\eta\) in DM(CE) and DM(BCE). In Figure A2_left_, the _static_ (Mixup and CutMix) and the _dynamic_ methods (PuzzleMix and AutoMix) prefer \(\eta=0.1\) and \(\eta=1\), respectively, which might be because the _dynamic_ variants generate more discriminative and reliable mixed samples than the _static_ methods. Then, Figure A2_middle_ and _right_ show that ablation studies of hyper-parameters \(\xi\) and \(t\) in DM(BCE), where cutting-based methods (CutMix and AutoMix) prefer \(\xi=0.8\) and \(t=1\), while the interpolation-based policies (Mixup and ManifoldMix) use \(\xi=1.0\) and \(t=0.5\).

Sensitivity AnalysisTo verify the robustness of hyper-parameter \(\eta\), extra experiments are conducted on CIFAR-100, Tiny-ImageNet, and CUB-200 datasets. Figure A3 shows the results consistent with our ablation study in Sec. 5.4. _Dynamic_ mixup methods prefer the large value of \(\eta\) (_e.g.,_ 1.0), while _static_ ones are more like a small value (_e.g.,_ 0.1). The main reason for this is the _dynamic_ methods generate mixed samples where label mismatch is relatively rare, relying on larger weights to achieve better results, while the opposite is true in _static_ methods.