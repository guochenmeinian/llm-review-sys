# Outlier-Robust Gromov-Wasserstein for Graph Data

 Lemin Kong

CUHK

lkong@se.cuhk.edu.hk &Jiajin Li

Stanford University

jiajinli@stanford.edu &Jianheng Tang

HKUST

jtangbf@connect.ust.hk &Anthony Man-Cho So

CUHK

manchoso@se.cuhk.edu.hk

###### Abstract

Gromov-Wasserstein (GW) distance is a powerful tool for comparing and aligning probability distributions supported on different metric spaces. Recently, GW has become the main modeling technique for aligning heterogeneous data for a wide range of graph learning tasks. However, the GW distance is known to be highly sensitive to outliers, which can result in large inaccuracies if the outliers are given the same weight as other samples in the objective function. To mitigate this issue, we introduce a new and robust version of the GW distance called RGW. RGW features optimistically perturbed marginal constraints within a Kullback-Leibler divergence-based ambiguity set. To make the benefits of RGW more accessible in practice, we develop a computationally efficient and theoretically provable procedure using Bregman proximal alternating linearized minimization algorithm. Through extensive experimentation, we validate our theoretical results and demonstrate the effectiveness of RGW on real-world graph learning tasks, such as subgraph matching and partial shape correspondence.

## 1 Introduction

Gromov-Wasserstein distance (GW) [26, 28] acts as a main model tool in data science to compare data distributions on unaligned metric spaces. Recently, it has received much attention across a host of applications in data analysis, e.g., shape correspondence [24, 31, 36, 27], graph alignment and partition [37, 38, 15, 18, 14, 44], graph embedding and classification [41, 43], unsupervised word embedding and translation [3, 19], generative modeling across incomparable spaces [8, 45].

In practice, the robustness of GW distance suffers heavily from its sensitivity to outliers. Here, outliers mean the samples with large noise, which usually are far away from the clean samples or have different structures from the clean samples. The hard constraints on the marginals in the Gromov-Wasserstein distance require all the mass in the source distribution to be entirely transported to the target distribution, making it highly sensitive to outliers. When the outliers are weighted similarly as other clean samples, even a small fraction of outliers corrupted can largely impact the GW distance value and the optimal coupling, which is unsatisfactory in real-world applications.

To overcome the above issue, some recent works are trying to relax the marginal constraints of GW distance. [33] introduces a \(L^{1}\) relaxation of mass conservation of the GW distance. However, this reformulation replaces the strict marginal constraint that the transport plan should be a joint distribution with marginals as specific distributions by the constraint that only requires the transport plan to be a joint distribution, which can easily lead to over-relaxation. On another front, [10] propose a so-called partial GW distance (PGW), which only transports a fraction of mass from source distribution to target distribution. The formulation of PGW is limited to facilitating massdestruction, which restricts its ability to handle situations where outliers exist predominantly on one side. A formulation that allows both mass destruction and creation is proposed in [35] called unbalanced GW (UGW). The UGW relaxes the marginal constraint via adding the quadratic \(\varphi\)-divergence as the penalty function in the objective function and extends GW distance to compare metric spaces equipped with arbitrary positive measures. Additionally, [40] proved that UGW is robust to outliers and can effectively remove the mass of outliers with high transportation costs. However, UGW is sensitive to the penalty parameter as it balances the reduction of outlier impact and the control of marginal distortion in the transport plan. On the computational side, an alternate Sinkhorn minimization method is proposed to calculate the entropy-regularized UGW. Note that the algorithm does not exactly solve UGW but approximates the lower bound of the entropic regularized UGW instead. From a statistical viewpoint, these works do not establish a direct link between the reformulated GW distance and the GW distance in terms of uncontaminated levels.

In this work, we propose the robust Gromov-Wasserstein (RGW) to estimate the GW distance robustly when dealing with outliers. To achieve this, RGW simultaneously optimizes the transport plan and selects the best marginal distribution from a neighborhood of the given marginal distributions, avoiding contaminated distributions. Perturbed marginal distributions help to re-weight the samples and lower the weight assigned to the outliers. The introduction of relaxed distributions to handle outliers draws inspiration from robust OT techniques [4, 29, 23]. Unlike robust OT, which is convex, RGW is non-convex, posing algorithmic challenges. This idea is also closely related to optimistic modelings of distribution ambiguity in data-driven optimization, e.g., upper confidence bound in the multi-armed bandit problem and reinforcement learning [7, 30, 1], data-driven distributionally robust decision-making with outliers [21, 9], etc.

Moreover, inspired by UGW, RGW relaxes the marginal constraint via adding the Kullback-Leibler (KL) divergence between the marginals of the transport plan and the perturbed distributions as the penalty function in the objective function to lessen the impact of the outliers further. Instead of utilizing the quadratic KL divergence as employed in unbalanced GW, we opt for KL divergence due to its computational advantages. It allows for convex subproblems with closed-form solutions, as opposed to the linearization required for non-convex quadratic KL divergence, which could be challenging algorithmically. Furthermore, we leverage the convexity of KL divergence to establish the statistical properties of RGW, leading to an upper bound by the true GW distance with explicit control through marginal relaxation parameters and marginal penalty parameters. This statistical advantage is disrupted by the non-convex nature of quadratic KL divergence. Additionally, KL divergence aligns with our goal of outlier elimination and is less sensitive to outliers compared to quadratic KL divergence, which is more outlier-sensitive due to its quadratic nature. Overall, RGW combines the introduction of perturbed marginal distributions with the relaxation of hard marginal constraints to achieve greater flexibility, allowing control over marginal distortion through marginal penalty parameters and reduction of outlier impact using marginal relaxation parameters.

To realize the modeling benefits of RGW, we further propose an efficient algorithm based on the Bregman proximal alternating linearized minimization (BPALM) method. The updates in each

Figure 1: Visualization of Gromov-Wasserstein couplings between two shapes, with the source in blue and the target in orange. In (a), the GW coupling without outliers is shown. In (b), the coupling with 10% outliers added to the target distribution is depicted. The sensitivity of GW to outliers is evident from the plot. In (c), we present the coupling generated by our proposed RGW formulation, which effectively disregards outliers and closely approximates the true GW distance.

iteration of BPALM can be computed in a highly efficient manner. On the theoretical side, we prove that the BPALM algorithm converges to a critical point of the RGW problem. Empirically, we demonstrate the effectiveness of RGW and the proposed BPALM algorithm through extensive numerical experiments on subgraph alignment and partial shape correspondence tasks. The results reveal that RGW surpasses both the balanced GW-based method and the reformulations of GW, including PGW and UGW.

#### 1.0.1 Our Contributions

We summarize our main contributions as follows:

* We develop a new robust model called RGW to alleviate the impact of outliers on the GW distance. The key insight is to simultaneously optimize the transport plan and perturb marginal distributions in the most efficient way.
* On the statistical side, we demonstrate that the robust Gromov-Wasserstein is bounded above by the true GW distance under the Huber \(\epsilon\)-contamination model.
* On the computational side, we propose an efficient algorithm for solving RGW using the BPALM method and prove that the algorithm converges to a critical point of the RGW problem.
* Empirical results on subgraph alignment and partial shape correspondence tasks demonstrate the effectiveness of RGW. This is the first successful attempt to apply GW-based methods to partial shape correspondence, a challenging problem pointed out in [36].

## 2 Problem Formulation

In this section, we review the definition of Gromov-Wasserstein distance and formally formulate the robust Gromov-Wasserstein. Following that, we discuss the statistical properties of the proposed robust Gromov-Wasserstein model under the Huber \(\epsilon\)-contamination model.

For the rest of the paper, we will use the following notation. Let \((X,d_{X})\) be a complete separable metric space and denote the finite, positive Borel measure on \(X\) by \(\mathcal{M}_{+}(X)\). Let \(\mathcal{P}(X)\subset\mathcal{M}_{+}(X)\) denotes the space of Borel probability measures on \(X\). We use \(\Delta^{n}\) to denote the simplex in \(\mathbb{R}^{n}\). We use \(\mathbf{1}_{n}\) and \(\mathbf{1}_{n\times m}\) to denote the \(n\)-dimensional all-one vector and \(n\times m\) all-one matrix. We use \(\mathcal{S}^{n}\) to denote the set of \(n\times n\) symmetric matrice. The indicator function of set \(C\) is denoted as \(\mathbb{I}_{C}(\cdot)\).

### Robust Gromov-Wasserstein

The Gromov-Wasserstein (GW) distance aims at matching distributions defined in different metric spaces. It is defined as follows:

**Definition 2.1** (Gromov-Wasserstein).: Suppose that we are given two unregistered complete separable metric spaces \((X,d_{X})\), \((Y,d_{Y})\) accompanied with Borel probability measures \(\mu,\nu\) respectively. The GW distance between \(\mu\) and \(\nu\) is defined as

\[\inf_{\pi\in\Pi(\mu,\nu)}\iint|d_{X}(x,x^{\prime})-d_{Y}(y,y^{\prime})|^{2}d\pi (x,y)d\pi(x^{\prime},y^{\prime}),\]

where \(\Pi(\mu,\nu)\) is the set of all probability measures on \(X\times Y\) with \(\mu\) and \(\nu\) as marginals.

As shown in the definition, the sensitivity to outliers of Gromov-Wasserstein distance is due to its hard constraints on marginal distributions. This suggests relaxing the marginal constraints such that the weight assigned to the outliers by the transport plan can be small. To do it, we invoke the Kullback-Leibler divergence, defined as \(d_{\textbf{KL}}(\alpha,\mu)=\int_{X}\alpha(x)\log\left(\alpha(x)/\mu(x)\right)dx\), to soften the constraint on marginal distributions. We also introduce an optimistically distributionally robust mechanism to perturb the marginal distributions and reduce the weight assigned to outliers. Further details on this mechanism will be discussed later.

**Definition 2.2** (Robust Gromov-Wasserstein).: Suppose that we are given two unregistered complete separable metric spaces \((X,d_{X})\), \((Y,d_{Y})\) accompanied with Borel probability measures \(\mu,\nu\) respectively. The Robust GW between \(\mu\) and \(\nu\) is defined as

\[\begin{split}\text{GW}^{\text{rob}}_{\rho_{1},\rho_{2}}(\mu,\nu) \coloneqq&\min_{\alpha\in\mathcal{P}(X),\;\beta\in\mathcal{P}(Y) }\;F(\alpha,\beta)\\ &\text{s.t.}\;d_{\textbf{KL}}(\mu,\alpha)\leq\rho_{1},d_{\textbf{ KL}}(\nu,\beta)\leq\rho_{2},\end{split} \tag{1}\]where \(F(\alpha,\beta)=\)

\[\inf_{\pi\in\mathcal{M}^{+}(X\times Y)}\iint|d_{X}(x,x^{\prime})-d_{Y}(y,y^{ \prime}))|^{2}d\pi(x,y)d\pi\left(x^{\prime},y^{\prime}\right)+\tau_{1}d_{\textbf {KL}}(\pi_{1},\alpha)+\tau_{2}d_{\textbf{KL}}(\pi_{2},\beta),\]

and \((\pi_{1},\pi_{2})\) are two marginals of the joint distribution \(\pi\), defined by \(\pi_{1}(A)=\pi(A\times Y)\) for any Borel set \(A\subset X\) and \(\pi_{2}(B)=\pi(X\times B)\) for any Borel set \(B\subset Y\).

The main idea of our formulation is to optimize the transport plan and perturbed distribution variables in the ambiguity set of the observed marginal distributions jointly. This formulation aims to find the perturbed distributions that approximate the clean distributions and compute the transport plan based on the perturbed distributions. However, incorporating the constraints of equal marginals between the transport plan \(\pi\) and the perturbed distributions \(\alpha\) and \(\beta\) directly poses challenges in developing an algorithm due to potential non-smoothness issues. Inspired by [35], we address this challenge by relaxing these marginal constraints and incorporating the KL divergence terms, denoted as \(d_{\textbf{KL}}(\pi_{1},\alpha)\) and \(d_{\textbf{KL}}(\pi_{2},\beta)\), into the objective function as penalty functions. Different from [35], we use KL divergence instead of quadratic KL divergence due to its joint convexity, which is more amenable to algorithm development, as quadratic KL divergence is typically non-convex. Besides, transforming the hard marginal constraints into penalty functions can further lessen the impact of outliers on the transport plan.

Our new formulation extends the balanced GW distance and can recover it by choosing \(\rho_{1}=\rho_{2}=0\) and letting \(\tau_{1}\) and \(\tau_{2}\) tend to infinity. When properly chosen, \(\rho_{1}\) and \(\rho_{2}\) can encompass the clean distributions within the ambiguity sets. In this scenario, the relaxed reformulation closely approximates the original GW distance in a certain manner. Building on this concept, we prove that RGW can serve as a robust approximation of the GW distance without outliers, given some mild assumptions on the outliers.

### Robustness Guarantees

Robust Gromov-Wasserstein aims at mitigating the sensitivity of the GW distance to outliers, which can result in large inaccuracies if the outliers are given the same weight as other samples in the objective function. Specifically, RGW is designed to address the issue of the GW distance exploding as the distance between the clean samples and the outliers goes to infinity. In general, even a small number of outliers can cause the GW distance to change dramatically when added to the marginal distributions. To formalize this, consider the Huber \(\epsilon\)-contamination model popularized in robust statistics [22, 11, 12]. In that model, a base measure \(\mu_{c}\) is contaminated by an outlier distribution \(\mu_{a}\) to obtain a contaminated measure \(\mu\),

\[\mu=(1-\epsilon)\mu_{c}+\epsilon\mu_{a}. \tag{2}\]

Under this model, data are drawn from \(\mu\) defined in (2).

Under the assumption of the Huber \(\epsilon\)-contamination model, it can be demonstrated that by selecting suitable values of \(\rho_{1}\) and \(\rho_{2}\), the robust Gromov-Wasserstein distance ensures that outliers are unable to substantially inflate the transportation distance. For robust Gromov-Wasserstein, we have the following bound:

**Theorem 2.3**.: _Let \(\mu\) and \(\nu\) be two distributions corrupted by fractions \(\epsilon_{1}\) and \(\epsilon_{2}\) of outliers, respectively. Specifically, \(\mu\) is defined as \((1-\epsilon_{1})\mu_{c}+\epsilon_{1}\mu_{a}\), and \(\nu\) is defined as \((1-\epsilon_{2})\nu_{c}+\epsilon_{2}\nu_{a}\), where \(\mu_{c}\) and \(\nu_{c}\) represent the clean distributions, and \(\mu_{a}\) and \(\nu_{a}\) represent the outlier distributions. Then,_

\[\text{\rm GW}^{\text{\rm rob}}_{\rho_{1},\rho_{2}}(\mu,\nu) \leq\text{\rm GW}(\mu_{c},\nu_{c})+\max\left(0,\epsilon_{1}-\frac {\rho_{1}}{d_{\textbf{KL}}(\mu_{a},\mu_{c})}\right)\tau_{1}d_{\textbf{KL}}( \mu_{c},\mu_{a})\] \[\quad+\max\left(0,\epsilon_{2}-\frac{\rho_{2}}{d_{\textbf{KL}}( \nu_{a},\nu_{c})}\right)\tau_{2}d_{\textbf{KL}}(\nu_{c},\nu_{a}).\]

In Appendix B, we provide a proof that constructs a feasible transport plan and relaxed marginal distributions. By relaxing the strict marginal constraints, we can find a feasible transport plan that closely approximates the transport plan between the clean distributions and obtain relaxed marginal distributions that approximate the clean distributions.

The derived bound indicates that robust Gromov-Wasserstein provides a provably robust estimate under the Huber \(\epsilon\)-contamination model. If the fraction of outliers is known, the upper bound for the robust GW is determined by the true Gromov-Wasserstein distance, along with additional terms that account for the KL divergence between the clean distribution and the outlier distribution for both \(\mu\) and \(\nu\). The impact of this factor is determined by the extent of relaxation in the marginal distributions \(\rho_{1}\) and \(\rho_{2}\). By carefully choosing \(\rho_{1}=\epsilon_{1}d_{\mathbf{KL}}(\mu_{a},\mu_{c})\) and \(\rho_{2}=\epsilon_{2}d_{\mathbf{KL}}(\nu_{a},\nu_{c})\), we can tighten the upper bound on the robust GW value, while still keeping it below the original GW distance (excluding outliers). Importantly, substituting these values of \(\rho_{1}\) and \(\rho_{2}\) yields \(\text{GW}^{\text{rob}}_{\rho_{1},\rho_{2}}(\mu,\nu)\leq\text{GW}(\mu_{c},\nu_ {c})\), indicating that the robust GW between the contaminated distribution \(\mu\) and \(\nu\) is upper bounded by the original GW distance between the clean distribution \(\mu_{c}\) and \(\nu_{c}\).

**Remark 2.4**.: The following inequality for UGW under the Huber \(\epsilon\)-contamination model can be derived using the same techniques as in Theorem 2.3:

\[\mathrm{UGW}(\mu,\nu)\leq\mathrm{GW}(\mu_{c},\nu_{c})+\tau_{1}d_{\mathbf{KL}} ^{\otimes}(\mu_{c},\mu)+\tau_{2}d_{\mathbf{KL}}^{\otimes}(\nu_{c},\nu).\]

We observe that the terms \(d_{\mathbf{KL}}^{\otimes}(\mu_{c},\mu)\) and \(d_{\mathbf{KL}}^{\otimes}(\mu_{c},\mu)\) cannot be canceled out unless \(\tau_{1}\) and \(\tau_{2}\) are set to zero, resulting in over-relaxation. However, RGW allows us to control the error terms \(d_{\mathbf{KL}}(\mu_{c},\mu_{a})\) and \(d_{\mathbf{KL}}(\nu_{c},\nu_{a})\) through the marginal relaxation parameters \(\rho_{1}\) and \(\rho_{2}\).

## 3 Proposed Algorithm

### Problem Setup

To start with our algorithmic developments, we consider the discrete case for simplicity and practicality, where \(\mu\) and \(\nu\) are two empirical distributions, i.e., \(\mu=\sum_{i=1}^{n}\mu_{i}\delta_{x_{i}}\) and \(\nu=\sum_{j=1}^{m}\nu_{j}\delta_{y_{j}}\). Denote \(D\in\mathcal{S}^{n}\), \(D_{ik}=d_{X}(x_{i},x_{k})\) and \(\bar{D}\in\mathcal{S}^{m}\) and \(\bar{D}_{jl}=d_{Y}(y_{j},y_{l})\). We construct a 4-way tensor as follows:

\[\mathcal{L}(D,\bar{D})\coloneqq\left(\left|d_{X}\left(x_{i},x_{k}\right)-d_{Y} \left(y_{j},y_{l}\right)\right|^{2}\right)_{i,j,k,l}.\]

We define the tensor-matrix multiplication as

\[\left(\mathcal{L}\otimes T\right)_{ij}\coloneqq\left(\sum_{k,\ell}\mathcal{L }_{i,j,k,\ell}T_{k,\ell}\right)_{i,j}.\]

Then, the robust GW admits the following reformulation:

\[\begin{split}\min_{\pi,\alpha,\beta}&\left\langle \mathcal{L}(D,\bar{D})\otimes\pi,\pi\right\rangle+\tau_{1}d_{\mathbf{KL}}(\pi _{1},\alpha)+\tau_{2}d_{\mathbf{KL}}(\pi_{2},\beta)\\ \text{s.t.}& d_{\mathbf{KL}}(\mu,\alpha)\leq\rho_{1},d_{ \mathbf{KL}}(\nu,\beta)\leq\rho_{2},\\ &\alpha\in\Delta^{n},\beta\in\Delta^{m},\pi\geq 0.\end{split} \tag{3}\]

Here, \(\pi_{1}=\pi\mathbf{1}_{m}\) and \(\pi_{2}=\pi^{T}\mathbf{1}_{n}\).

### Bregman Proximal Alternating Linearized Minimization (BPALM) Method

As Problem (3) is non-convex and involves three variables, we employ BPALM [5; 2] to solve it. By choosing the KL divergence as Bregman distance, the updates of this algorithm are given by:

\[\pi^{k+1}=\operatorname*{arg\,min}_{\pi\geq 0}\left\{\left\langle\mathcal{L}(D, \bar{D})\otimes\pi^{k},\pi\right\rangle+\tau_{1}d_{\mathbf{KL}}(\pi_{1},\alpha ^{k})+\tau_{2}d_{\mathbf{KL}}(\pi_{2},\beta^{k})+\frac{1}{t_{k}}d_{\mathbf{KL }}(\pi,\pi^{k})\right\}, \tag{4}\]

\[\alpha^{k+1}=\operatorname*{arg\,min}_{\begin{subarray}{c}\alpha\in\Delta^{n }\\ d_{\mathbf{KL}}(\mu,\alpha)\leq\rho_{1}\end{subarray}}\left\{d_{\mathbf{KL}}( \pi_{1}^{k+1},\alpha)+\frac{1}{c_{k}}d_{\mathbf{KL}}(\alpha^{k},\alpha) \right\}, \tag{5}\]

\[\beta^{k+1}=\operatorname*{arg\,min}_{\begin{subarray}{c}\beta\in\Delta^{m}\\ d_{\mathbf{KL}}(\nu,\beta)\leq\rho_{2}\end{subarray}}\left\{d_{\mathbf{KL}}( \pi_{2}^{k+1},\beta)+\frac{1}{r_{k}}d_{\mathbf{KL}}(\beta^{k},\beta)\right\}. \tag{6}\]

Here, \(t_{k}\), \(c_{k}\), and \(r_{k}\) are stepsizes in BPALM.

In our algorithm updates, we employ distinct proximal operators for \(\pi\) and \(\alpha\) (and \(\beta\)). The use of \(d_{\mathbf{KL}}(\pi,\pi^{k})\) in \(\pi\)-subproblem (4) allows for the application of the Sinkhorn algorithm, while the introduction of \(d_{\mathbf{KL}}(\alpha^{k},\alpha)\) in \(\alpha\)-subproblem (5) facilitates a closed-form solution, which we will detail in the following part.

To solve the \(\pi\)-subproblem, we can utilize the Sinkhorn algorithm for the entropic regularized unbalanced optimal transport problem. This algorithm, which has been previously introduced in [13; 32], is well-suited for our needs. As for the \(\alpha\)-subproblem, we consider the case where \(\rho_{1}\) is strictly larger than 0. Otherwise, when \(\rho_{1}=0\), \(\alpha\) should simply equal \(\mu\), making the subproblem unnecessary. To solve the \(\alpha\)-subproblem, we attempt to find the optimal dual multiplier \(w^{*}\). Specifically, consider the problem:

\[\min_{\alpha\in\Delta^{n}}d_{\mathbf{KL}}(\pi_{1}^{k+1},\alpha)+\frac{1}{c_{k} }d_{\mathbf{KL}}(\alpha^{k},\alpha)+w(d_{\mathbf{KL}}(\mu,\alpha)-\rho_{1}). \tag{7}\]

Let \(\hat{\alpha}(w)\) represent the optimal solution to (7), and we define the function \(p:\mathbb{R}_{+}\rightarrow\mathbb{R}\) by \(p(w)=d_{\mathbf{KL}}(\mu,\hat{\alpha}(w))-\rho_{1}\). We prove the convexity, differentiability, and monotonicity of \(p\), which are crucial for developing an efficient algorithm for (5) later.

**Proposition 3.1**.: _Problem (7) has a closed-form solution_

\[\hat{\alpha}(w)=\frac{\pi^{k+1}\mathbf{1}_{m}+\frac{1}{c_{k}}\alpha^{k}+w\mu}{ \sum_{ij}\pi_{ij}^{k+1}+\frac{1}{c_{k}}+w}.\]

_If \(w\) satisfies (i) \(w=0\) and \(p(w)\leq 0\), or (ii) \(w>0\), \(p(w)=0\), then \(\hat{\alpha}(w)\) is the optimal solution to the \(\alpha\)-subproblem (5). Moreover, \(p(\cdot)\) is convex, twice differentiable, and monotonically non-increasing on \(\mathbb{R}_{+}\)._

Given Proposition 3.1, we begin by verifying \(p(0)\leq 0\). If this condition is not met, and given that \(p(0)>0\) while \(\lim_{w\rightarrow+\infty}p(w)=-\rho_{1}<0\), it implies that \(p\) possesses at least one root within \(\mathbb{R}_{+}\). The following proposition provides the framework to seek the root of \(p\) by employing Newton's method, with the initialization set at 0. Hence, the \(\alpha\)-subproblem can be cast to search a root of \(p\) in one dimension, in which case it can be solved efficiently.

**Proposition 3.2**.: _Let \(p(\cdot):I\rightarrow\mathbb{R}\) be a convex, twice differentiable, and monotonically non-increasing on the interval \(I\subset\mathbb{R}\). Assume that there exists an \(\tilde{x}\), \(\bar{x}\in I\) such that \(p(\tilde{x})>0\) and \(p(\bar{x})<0\). Then \(p\) has a unique root on \(I\), and the sequence obtained from Newton's method with initial point \(x_{0}=\tilde{x}\) will converge to the root of \(p\)._

Since the \(\beta\)-subproblem shares the same structure as the \(\alpha\)-subproblem, we can apply this method to search for the optimal solution to the \(\beta\)-subproblem.

### Convergence Analysis

To illustrate the convergence result of BPALM, we consider the compact form for simplicity:

\[\min_{\alpha,\beta,\pi}F(\pi,\alpha,\beta)=f(\pi)+q(\pi)+g_{1}(\pi,\alpha)+g_{ 2}(\pi,\beta)+h_{1}(\alpha)+h_{2}(\beta),\]

where \(f(\pi)=\langle\mathcal{L}(D,\bar{D})\otimes\pi,\pi\rangle\), \(q(\pi)=\mathbb{I}_{\{\pi\geq 0\}}(\pi)\), \(g_{1}(\pi,\alpha)=\tau_{1}d_{\mathbf{KL}}(\pi\mathbf{1}_{m},\alpha)\), \(g_{2}(\pi,\beta)=\tau_{2}d_{\mathbf{KL}}(\pi^{T}\mathbf{1}_{n},\beta)\), \(h_{1}(\alpha)=\mathbb{I}_{\{\alpha\in\Delta^{n},d_{\mathbf{KL}}(\mu,\alpha) \leq\rho_{1}\}}(\alpha)\), and \(h_{2}(\beta)=\mathbb{I}_{\{\beta\in\Delta^{m},d_{\mathbf{KL}}(\nu,\beta)\leq \rho_{2}\}}(\beta)\).

The following theorem states that any limit point of the sequence generated by BPALM belongs to the critical point set of problem (3).

**Theorem 3.3** (Subsequence Convergence).: _Suppose that in Problem (1), the step size \(t_{k}\) in (4) satisfies \(0<\underline{t}\leq t_{k}<\bar{t}\leq\sigma/L_{f}\) for \(k\geq 0\) where \(\underline{t}\), \(\bar{t}\) are given constants and \(L_{f}\) is the gradient Lipschitz constant of \(f\). The step size \(c_{k}\) in (5) and \(r_{k}\) in (6) satisfy \(0<\underline{r}\leq c_{k},r_{k}<\bar{r}\) for \(k\geq 0\) where \(\underline{r}\), \(\bar{r}\) are given constants. Any limit point of the sequence of solutions \(\{\pi^{k},\alpha^{k},\beta^{k}\}_{k\geq 0}\) belongs to the critical point set \(\mathcal{X}\), where \(\mathcal{X}\) is defined by_

\[\left\{\begin{aligned} & 0\in f(\pi)+\partial q(\pi)+\nabla_{\pi}g_{1}( \pi,\alpha)+\nabla_{\pi}g_{2}(\pi,\beta),\\ & 0\in\nabla_{\alpha}g_{1}(\pi,\alpha)+\partial h_{1}(\alpha),\\ & 0\in\nabla_{\beta}g_{2}(\pi,\beta)+\partial h_{2}(\beta),\\ &(\pi,\alpha,\beta)\in\mathbb{R}^{n\times m}\times\mathbb{R}^{n} \times\mathbb{R}^{m}\end{aligned}\right\}.\]

For the sake of brevity, we omit the proof. We refer the reader to Appendix C for further details.

Experiment Results

In this section, we present comprehensive experimental results to validate the effectiveness of our proposed RGW model and BPALM algorithm in various graph learning tasks, specifically subgraph alignment and partial shape correspondence. Traditionally, balanced GW has been applied successfully in scenarios where the source and target graphs have similar sizes. However, in our approach, we treat the missing part of the target graph as outliers and leverage RGW for improved performance. All simulations are conducted in Python 3.9 on a high-performance computing server running Ubuntu 20.10, equipped with an Intel(R) Xeon(R) Silver 4214R CPU. Our code is available at [https://github.com/lmkong020/outlier-robust-GW](https://github.com/lmkong020/outlier-robust-GW).

### Partial Shape Correspondence

In this subsection, we first investigate a toy matching problem in a 2D setting to support and validate our theoretical insights and results presented in Section 2. Figure 2 (a) illustrates an example where we aim to map a two-dimensional shape without symmetries to a rotated version of the same shape while accounting for outliers in the source domain. Here, we sample 300 points from the source shape and 400 points from the target shape. Additionally, we introduce 50 outliers by randomly adding points from a discrete uniform distribution on \([-3,-2.5]\times[0,0.5]\) to the source domain. The distance matrices, \(D\) and \(\bar{D}\) are computed using pairwise Euclidean distances.

Figures 2 provide visualizations of the coupling matrices and objective values for all the models, highlighting the matching results. In Figure 2(c), it is evident that even a small number of outliers has a significant impact on the coupling and leads to an increased estimated GW distance. While unbalanced GW and partial GW attempt to handle outliers to some extent, they fall short in achieving accurate mappings. On the other hand, our robust GW formulation, RGW, effectively disregards outliers and achieves satisfactory performance. Additionally, the objective value of RGW closely approximates the true GW distance without outliers, as indicated by Theorem 2.3, approaching zero.

### Subgraph Alignment

The subgraph alignment problem, which involves determining the isomorphism between a query graph and a subgraph of a larger target graph, has been extensively studied [16; 20]. While the

Figure 2: (a): 2D shape geometry of the source and target; (b)-(f): visualization of ground truth and the matching results of balanced GW, unbalanced GW, partial GW, and robust GW.

restricted quadratic assignment problem is commonly used for graphs of similar sizes, the GW distance provides an optimal probabilistic correspondence that preserves the isometric property. In the subgraph alignment context, the nodes in the target graph, excluding those in the source graph, can be considered outliers, making the RGW model applicable to this task. In our comparison, we evaluate RGW against various methods, including unbalanced GW, partial GW, semi-relaxed GW (srGW) [42], RGWD [25], and methods for computing balanced GW such as FW [39], BPG [46], SpecGW [15], eBPG [36], and BAPG [24].

Database StatisticsWe evaluate the methods on synthetic and real databases. In the synthetic database, we generate target graphs \(\mathcal{G}_{t}\) using Barabasi-Albert models with scales ranging from 100 to 500 nodes. The source graphs \(\mathcal{G}_{s}\) are obtained by sampling connected subgraphs of \(\mathcal{G}_{t}\) with a specified percentage of nodes. This process results in five synthetic graph pairs for each setup, totaling 200 graph pairs. The _Proteins_ and _Enzymes_ biological graph databases from [15] are also used, following the same subgraph generation routine. For the _Proteins_ database, we evaluate the accuracy of matching the overlap between two subgraphs with 90% overlap and between a subgraph and the entire graph, presented in the "Proteins-2" and "Proteins-1" columns, respectively. We compute the matching accuracy by comparing the predicted correspondence set \(\mathcal{S}_{\text{pred}}\) to the ground truth correspondence set \(\mathcal{S}_{\text{gt}}\), with accuracy calculated as \(\text{Acc}=|\mathcal{S}_{\text{gt}}\cap\mathcal{S}_{\text{pred}}|/|\mathcal{ S}_{\text{gt}}|\times 100\%\). In addition, we also evaluate our methods on the _Douban Online-Offline_ social network dataset, which consists of online and offline graphs, representing user interactions and presence at social gatherings, respectively. The online graph includes all users from the offline graph, with 1,118 users serving as ground truth alignments. Node locations are used as features in both graphs. In line with previous works [18; 37], we gauge performance using the Hit@k metric, which calculates the percentage of nodes in set \(\mathcal{V}_{t}\) where the ground truth alignment includes \(\mathcal{V}_{s}\) among the top-k candidates.

\begin{table}
\begin{tabular}{c|r r|r r|r r|r r|r r} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Synthetic} & \multicolumn{3}{c|}{Proteins-1} & \multicolumn{3}{c|}{Proteins-2} & \multicolumn{3}{c|}{Enzymes} & \multicolumn{3}{c}{Douban} \\  & Acc & Time & Acc & Time & Acc & Time & Acc & Time & Hit@1 & Hit@10 \\ \hline FW & 2.27 & 18.39 & 16.00 & 27.05 & 26.15 & 60.34 & 15.47 & 9.57 & 17.97 & 51.07 \\ SpecGW & 1.78 & 3.72 & 12.06 & 11.07 & 42.64 & 12.85 & 10.69 & 3.96 & 2.68 & 9.83 \\ eBPG & 3.71 & 85.31 & 19.88 & 1975.12 & 32.15 & 9645.05 & 21.58 & 12191.81 & 0.08 & 0.53 \\ BPG & 15.41 & 24.67 & 29.30 & 118.26 & 61.26 & 80.39 & 32.49 & 70.42 & 72.72 & 92.39 \\ BAPG & 48.89 & 27.95 & 30.98 & 122.13 & 66.84 & 16.49 & 35.64 & 16.41 & 72.18 & 92.58 \\ \hline srGW & 1.60 & 152.01 & 21.30 & 63.00 & 12.08 & 172.48 & 24.13 & 19.68 & 4.03 & 11.54 \\ RGWD & 16.68 & 955.40 & 27.94 & 4396.41 & 59.69 & 3586.56 & 30.35 & 2629.00 & 4.11 & 16.46 \\ UGW & 89.88 & 176.24 & 25.72 & 4026.93 & 67.30 & 1853.82 & 43.73 & 1046.29 & 0.09 & 0.72 \\ PGW & 2.28 & 479.99 & 13.94 & 544.79 & 20.08 & 348.44 & 11.43 & 212.09 & 18.24 & 37.03 \\ \hline RGW & **94.44** & 361.44 & **53.30** & 834.76 & **69.38** & 466.91 & **63.43** & 293.84 & **75.58** & **96.24** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of the average matching accuracy (%) and wall-clock time (seconds) on subgraph alignment of 50% subgraph on datasets Synthetic, Proteins and Enzymes and Hit@1 and Hit@10 of dataset Douban.

Figure 3: (a): 3D shape geometry of the source and target; (b)-(e): visualization of ground truth, initial point obtained from the partial functional map, and the matching results of PGW and RGW.

[MISSING_PAGE_FAIL:9]

subgraph to the entire graph. In Figure 5, the left side demonstrates that while keeping the ratio and \(\tau\) fixed but varying \(\rho\), accuracy diminishes when \(\rho\) becomes either excessively large or too small. However, within the range of 0.05 to 1, accuracy remains resilient, highlighting the mitigating effect of marginal relaxation on outlier influence. Notably, accuracy significantly improves when \(\rho\) falls within the range of 0.05 to 1 compared to the scenario with \(\rho\) = 0, thus affirming the significance of marginal relaxation in our model. A similar trend is observed for \(\tau\) on the right side of Figure 5, where maintaining a fixed ratio and \(\rho\) while adjusting \(\tau\) results in stable accuracy within the range of 0.1 to 1 but declining beyond this range. Consequently, in RGW computations, we can initially set \(\rho\) to 0.2 and \(\tau\) to 0.1 by default and subsequently adjust \(\rho\) based on the outlier ratio: increasing it for a large ratio and decreasing it for a small ratio.

### Tightness of the Bound in Theorem 2.3

Our primary focus is on scenarios where the GW distance between clean samples is nearly zero or zero due to noise, such as in partial shape correspondence and subgraph alignment tasks. In such cases, it is possible to find an isometric mapping from the query subgraph to a portion of the entire graph. By appropriately selecting the value of \(\rho\), as discussed in Section 2.2, the upper bound in RGW becomes the GW distance between clean samples, which is zero. As RGW is always nonnegative, this upper bound is tight in this context.

To empirically validate this, we conducted experiments on the toy example in Section 4.1, and Figure 6 (a) illustrates the function values of PGW, UGW, and RGW with varying outlier ratios. The results confirm that the value of RGW can remain close to zero as the ratio of outliers increases. Additionally, Figure 6 (b) shows the function value of RGW and its upper bound as \(\rho\) varies. Both the RGW value and its upper bound decrease, converging to zero as \(\rho\) increases. This observation provides empirical support for Theorem 2.3. Regarding UGW and its upper bound with changing \(\tau\), we observed that both the UGW value and its upper bound increase as \(\tau\) becomes larger, as shown in Figure 6 (c). Unlike RGW, UGW's \(\tau\) must strike a balance between reducing outlier impact and preserving marginal distortion in the transport plan. This demands a careful balance and caution against setting \(\tau\) excessively close to zero, which could lead to over-relaxation and potentially deteriorate the performance.

## 5 Conclusion

In this paper, we introduce RGW, a robust reformulation of Gromov-Wasserstein that incorporates distributionally optimistic modeling. Our theoretical analysis demonstrates its robustness to outliers, establishing RGW as a reliable estimator. We propose a Bregman proximal alternating linearized minimization method to efficiently solve RGW. Extensive numerical experiments validate our theoretical results and demonstrate the effectiveness of the proposed algorithm. Regarding the robust estimation of Gromov-Wasserstein, a natural question is whether we can recover the transport plan from the RGW model. On the computational side, our algorithm suffers from the heavy computation cost due to the use of the unbalanced OT as our subroutine, which limits its application in large-scale real-world settings. To address this issue, a natural future direction is to develop single-loop algorithms to leverage the model benefits of robust GW for real applications.

Figure 6: (a) Function values of PGW, UGW, and RGW for varying \(\epsilon\); (b) Function value of RGW and its upper bound for different \(\rho\); (c) Function value of UGW and corresponding upper bound for different \(\tau\).

## Acknowledgments and Disclosure of Funding

Anthony Man-Cho So is supported in part by the Hong Kong Research Grants Council (RGC) General Research Fund (GRF) project CUHK 14203920.

## References

* Agarwal et al. [2020] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In _International Conference on Machine Learning_, pages 104-114. PMLR, 2020.
* Ahookhosh et al. [2021] Masoud Ahookhosh, Le Thi Khanh Hien, Nicolas Gillis, and Panagiotis Patrinos. Multiblock Bregman proximal alternating linearized minimization and its application to orthogonal nonnegative matrix factorization. _Computational Optimization and Applications_, 79(3):681-715, 2021.
* Alvarez-Melis and Jaakkola [2018] David Alvarez-Melis and Tommi S Jaakkola. Gromov-Wasserstein alignment of word embedding spaces. _arXiv preprint arXiv:1809.00013_, 2018.
* Balaji et al. [2020] Yogesh Balaji, Rama Chellappa, and Soheil Feizi. Robust optimal transport with applications in generative modeling and domain adaptation. _Advances in Neural Information Processing Systems_, 33:12934-12944, 2020.
* Bolte et al. [2014] Jerome Bolte, Shoham Sabach, and Marc Teboulle. Proximal alternating linearized minimization for nonconvex and nonsmooth problems. _Mathematical Programming_, 146(1):459-494, 2014.
* Bronstein et al. [2008] Alexander M Bronstein, Michael M Bronstein, and Ron Kimmel. _Numerical geometry of non-rigid shapes_. Springer Science & Business Media, 2008.
* Bubeck et al. [2012] Sebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends(r) in Machine Learning_, 5(1):1-122, 2012.
* Bunne et al. [2019] Charlotte Bunne, David Alvarez-Melis, Andreas Krause, and Stefanie Jegelka. Learning generative models across incomparable spaces. In _International Conference on Machine Learning_, pages 851-861. PMLR, 2019.
* Cao and Gao [2021] Junyu Cao and Rui Gao. Contextual decision-making under parametric uncertainty and data-driven optimistic optimization. _Available at Optimization Online_, 2021.
* Chapel et al. [2020] Laetitia Chapel, Mokhtar Z Alaya, and Gilles Gasso. Partial optimal transport with applications on positive-unlabeled learning. _Advances in Neural Information Processing Systems_, 33:2903-2913, 2020.
* Chen et al. [2018] Mengjie Chen, Chao Gao, and Zhao Ren. Robust covariance and scatter matrix estimation under Huber's contamination model. _The Annals of Statistics_, 46(5):1932-1960, 2018.
* Chen et al. [2022] Sitan Chen, Frederic Koehler, Ankur Moitra, and Morris Yau. Online and distribution-free robustness: Regression and contextual bandits with huber contamination. In _2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 684-695. IEEE, 2022.
* Chizat et al. [2018] Lenaic Chizat, Gabriel Peyre, Bernhard Schmitzer, and Francois-Xavier Vialard. Scaling algorithms for unbalanced optimal transport problems. _Mathematics of Computation_, 87(314):2563-2609, 2018.
* Chowdhury and Memoli [2019] Samir Chowdhury and Facundo Memoli. The Gromov-Wasserstein distance between networks and stable network invariants. _Information and Inference: A Journal of the IMA_, 8(4):757-787, 2019.
* Chowdhury and Needham [2021] Samir Chowdhury and Tom Needham. Generalized spectral clustering via Gromov-Wasserstein learning. In _International Conference on Artificial Intelligence and Statistics_, pages 712-720. PMLR, 2021.

* Cordella et al. [2004] Luigi P Cordella, Pasquale Foggia, Carlo Sansone, and Mario Vento. A (sub) graph isomorphism algorithm for matching large graphs. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 26(10):1367-1372, 2004.
* Flamary et al. [2021] Remi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z Alaya, Aurelie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, et al. Pot: Python optimal transport. _The Journal of Machine Learning Research_, 22(1):3571-3578, 2021.
* Gao et al. [2021] Ji Gao, Xiao Huang, and Jundong Li. Unsupervised graph alignment with Wasserstein distance discriminator. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 426-435, 2021.
* Grave et al. [2019] Edouard Grave, Armand Joulin, and Quentin Berthet. Unsupervised alignment of embeddings with Wasserstein procrustes. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1880-1890. PMLR, 2019.
* Han et al. [2019] Myoungji Han, Hyunjoon Kim, Geonmo Gu, Kunsoo Park, and Wook-Shin Han. Efficient subgraph matching: Harmonizing dynamic programming, adaptive matching order, and failing set together. In _Proceedings of the 2019 International Conference on Management of Data_, pages 1429-1446, 2019.
* Jiang and Xie [2021] Nan Jiang and Weijun Xie. DFO: A framework for data-driven decision-making with endogenous outliers. _Preprint optimization-online.org_, 2021.
* Kassam and Thomas [1976] S Kassam and J Thomas. Asymptotically robust detection of a known signal in contaminated non-gaussian noise. _IEEE Transactions on Information Theory_, 22(1):22-26, 1976.
* Le et al. [2021] Khang Le, Huy Nguyen, Quang M Nguyen, Tung Pham, Hung Bui, and Nhat Ho. On robust optimal transport: Computational complexity and barycenter computation. _Advances in Neural Information Processing Systems_, 34:21947-21959, 2021.
* Li et al. [2023] Jiajin Li, Jianheng Tang, Lemin Kong, Huikang Liu, Jia Li, Anthony Man-Cho So, and Jose Blanchet. A convergent single-loop algorithm for relaxation of Gromov-Wasserstein in graph data. In _The Eleventh International Conference on Learning Representations_, 2023.
* Liu et al. [2022] Weijie Liu, Jiahao Xie, Chao Zhang, Makoto Yamada, Nenggan Zheng, and Hui Qian. Robust graph dictionary learning. In _The Eleventh International Conference on Learning Representations_, 2022.
* Memoli [2007] Facundo Memoli. On the use of Gromov-Hausdorff Distances for Shape Comparison. In M. Botsch, R. Pajarola, B. Chen, and M. Zwicker, editors, _Eurographics Symposium on Point-Based Graphics_. The Eurographics Association, 2007.
* Memoli [2009] Facundo Memoli. Spectral Gromov-Wasserstein distances for shape matching. In _2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops_, pages 256-263. IEEE, 2009.
* Memoli [2011] Facundo Memoli. Gromov-Wasserstein distances and the metric approach to object matching. _Foundations of computational mathematics_, 11:417-487, 2011.
* Mukherjee et al. [2021] Debarghya Mukherjee, Aritra Guha, Justin M Solomon, Yuekai Sun, and Mikhail Yurochkin. Outlier-robust optimal transport. In _International Conference on Machine Learning_, pages 7850-7860. PMLR, 2021.
* Munos et al. [2014] Remi Munos et al. From bandits to monte-carlo tree search: The optimistic principle applied to optimization and planning. _Foundations and Trends(r) in Machine Learning_, 7(1):1-129, 2014.
* Peyre et al. [2016] Gabriel Peyre, Marco Cuturi, and Justin Solomon. Gromov-Wasserstein averaging of kernel and distance matrices. In _International Conference on Machine Learning_, pages 2664-2672. PMLR, 2016.
* Pham et al. [2020] Khiem Pham, Khang Le, Nhat Ho, Tung Pham, and Hung Bui. On unbalanced optimal transport: An analysis of sinkhorn algorithm. In _International Conference on Machine Learning_, pages 7673-7682. PMLR, 2020.

* [33] Emanuele Rodola, Alex M Bronstein, Andrea Albarelli, Filippo Bergamasco, and Andrea Torsello. A game-theoretic approach to deformable shape matching. In _2012 IEEE Conference on Computer Vision and Pattern Recognition_, pages 182-189. IEEE, 2012.
* [34] Emanuele Rodola, Luca Cosmo, Michael M Bronstein, Andrea Torsello, and Daniel Cremers. Partial functional correspondence. In _Computer graphics forum_, volume 36, pages 222-236. Wiley Online Library, 2017.
* [35] Thibault Sejourne, Francois-Xavier Vialard, and Gabriel Peyre. The unbalanced Gromov Wasserstein distance: Conic formulation and relaxation. _Advances in Neural Information Processing Systems_, 34, 2021.
* [36] Justin Solomon, Gabriel Peyre, Vladimir G Kim, and Suvrit Sra. Entropic metric alignment for correspondence problems. _ACM Transactions on Graphics (ToG)_, 35(4):1-13, 2016.
* [37] Jianheng Tang, Weiqi Zhang, Jiajin Li, Kangfei Zhao, Fugee Tsung, and Jia Li. Robust attributed graph alignment via joint structure learning and optimal transport. In _2023 IEEE 39th International Conference on Data Engineering (ICDE)_, pages 1638-1651, 2023.
* [38] Jianheng Tang, Kangfei Zhao, and Jia Li. A fused Gromov-Wasserstein framework for unsupervised knowledge graph entity alignment. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 3320-3334.
* [39] Vayer Titouan, Nicolas Courty, Romain Tavenard, and Remi Flamary. Optimal transport for structured data with application on graphs. In _International Conference on Machine Learning_, pages 6275-6284. PMLR, 2019.
* [40] Quang Huy Tran, Hicham Janati, Nicolas Courty, Remi Flamary, Ievgen Redko, Pinar Demetci, and Ritambhara Singh. Unbalanced CO-Optimal transport. In _Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI)_, 2023.
* [41] Cedric Vincent-Cuaz, Titouan Vayer, Remi Flamary, Marco Corneli, and Nicolas Courty. Online graph dictionary learning. In _International Conference on Machine Learning_, pages 10564-10574. PMLR, 2021.
* [42] Cedric Vincent-Cuaz, Remi Flamary, Marco Corneli, Titouan Vayer, and Nicolas Courty. Semi-relaxed Gromov Wasserstein divergence with applications on graphs. In _International Conference on Learning Representations (ICLR)_, 2022.
* [43] Hongteng Xu, Jiachang Liu, Dixin Luo, and Lawrence Carin. Representing graphs via Gromov-Wasserstein factorization. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [44] Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable Gromov-Wasserstein learning for graph partitioning and matching. _Advances in Neural Information Processing Systems_, 32, 2019.
* [45] Hongteng Xu, Dixin Luo, Lawrence Carin, and Hongyuan Zha. Learning graphons via structured Gromov-Wasserstein barycenters. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 10505-10513, 2021.
* [46] Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. Gromov-Wasserstein learning for graph matching and node embedding. In _International Conference on Machine Learning_, pages 6932-6941. PMLR, 2019.

Organization of the Appendix

We organize the appendix as follows:

* The proof details of Theorem 2.3 and discussion of Remark 2.4 are given in Section B.
* The proof details of the algorithm, including the properties of \(p\), the convergence analysis of Newton's method, and Bregman proximal alternating linearized minimization method are collected in C.
* Additional experiment results are summarized in Section D.

## Appendix B Proof of Robust Guarantee in Section 2.2

### Proof of Theorem 2.3

Proof.: \(\text{GW}^{\text{rob}}_{\rho_{1},\rho_{2}}(\mu,\nu)\) is defined as

\[\text{GW}^{\text{rob}}_{\rho_{1},\rho_{2}}(\mu,\nu)= \inf_{\alpha\in\mathcal{P}(X),\;\beta\in\mathcal{P}(Y)}\;\inf_{ \pi\in\mathcal{M}^{+}(X\times Y)}\iint|d_{X}(x,x^{\prime})-d_{Y}(y,y^{\prime} ))|^{2}d\pi(x,y)d\pi\left(x^{\prime},y^{\prime}\right)+\] \[\text{s.t.}\quad d_{\text{KL}}(\mu,\alpha)\leq\rho.\]

Consider \(\pi_{c}\), the optimal transport plans for \(\text{GW}(\mu_{c},\nu_{c})\). Notably, \(\pi_{c}\) serves as a feasible solution for \(\text{GW}^{\text{rob}}_{\rho_{1},\rho_{2}}(\mu,\nu)\). Consequently, we can deduce that:

\[\text{GW}^{\text{rob}}_{\rho_{1},\rho_{2}}(\mu,\nu) \leq\inf_{\alpha\in\mathcal{P}(X),\;\beta\in\mathcal{P}(Y)}\iint |d_{X}(x,x^{\prime})-d_{Y}(y,y^{\prime}))|^{2}d\pi_{c}(x,y)d\pi_{c}\left(x^{ \prime},y^{\prime}\right)+\] \[\tau_{1}d_{\text{KL}}((\pi_{c})_{1},\alpha)+\tau_{2}d_{\text{KL}} ((\pi_{c})_{2},\beta)\] \[\text{s.t.}\;d_{\text{KL}}(\mu,\alpha)\leq\rho_{1},\;d_{\text{KL }}(\nu,\beta)\leq\rho_{2}\] \[=\text{GW}(\mu_{c},\nu_{c})+\inf_{\alpha\in\mathcal{P}(X),\;\beta \in\mathcal{P}(Y),\;d_{\text{KL}}(\mu,\alpha)\leq\rho_{1}}d_{\text{KL}}(\mu_{ c},\alpha)+\inf_{\beta\in\mathcal{P}(Y),\;d_{\text{KL}}(\nu,\beta)\leq\rho_{2}}d_{ \text{KL}}(\nu_{c},\beta).\]

To establish an upper bound for \(\text{GW}^{\text{rob}}_{\rho_{1},\rho_{2}}(\mu,\nu)\), let us begin by addressing the following problem:

\[\inf_{\alpha\in\mathcal{P}(X),\;d_{\text{KL}}(\mu,\alpha)\leq\rho_{1}}d_{\text {KL}}(\mu_{c},\alpha) \tag{8}\]

We consider the distribution of the form \((1-\gamma)\mu+\gamma\mu_{c}\), for \(\gamma\in[0,1]\). Then we prove that if \(\gamma\leq\min\left(\frac{\rho_{1}}{\epsilon_{1}d_{\text{KL}}(\mu_{a},\mu_{c} )},1\right)\), then \((1-\gamma)\mu+\gamma\mu_{c}\) is a feasible solution for problem (8).

By the joint convexity of KL divergence, we have

\[d_{\text{KL}}\left(\mu,(1-\gamma)\mu+\gamma\mu_{c}\right) \leq\gamma d_{\text{KL}}\left(\mu,\mu_{c}\right)\] \[=\gamma d_{\text{KL}}\left((1-\epsilon_{1})\mu_{c}+\epsilon_{1} \mu_{a},\mu_{c}\right)\] \[\leq\gamma\epsilon_{1}d_{\text{KL}}(\mu_{a},\mu_{c})\] \[\leq\rho_{1}.\]

Therefore,

\[d_{\text{KL}}\left(\mu_{c},(1-\gamma)\mu+\gamma\mu_{c}\right) \leq(1-\gamma)d_{\text{KL}}\left(\mu_{c},\mu\right)\] \[=(1-\gamma)d_{\text{KL}}\left(\mu_{c},(1-\epsilon_{1})\mu_{c}+ \epsilon_{1}\mu_{a}\right)\] \[\leq(1-\gamma)\epsilon_{1}d_{\text{KL}}(\mu_{c},\mu_{a})\]

The largest value \(\gamma\) can take is \(\frac{\rho_{1}}{\epsilon_{1}d_{\text{KL}}(\mu_{a},\mu_{c})}\). This gives

\[\inf_{\alpha\in\mathcal{P}(X),\;d_{\text{KL}}(\mu,\alpha)\leq\rho_{1}}d_{\text {KL}}(\mu_{c},\alpha)\leq\max\left(0,1-\frac{\rho_{1}}{\epsilon_{1}d_{\text{KL }}(\mu_{a},\mu_{c})}\right)\epsilon_{1}d_{\text{KL}}(\mu_{c},\mu_{a}).\]

Similarly, we can prove that

\[\inf_{\beta\in\mathcal{P}(Y),\;d_{\text{KL}}(\nu,\beta)\leq\rho_{2}}d_{\text {KL}}(\nu_{c},\beta)\leq\max\left(0,1-\frac{\rho_{2}}{\epsilon_{2}d_{\text{KL}} (\nu_{a},\nu_{c})}\right)\epsilon_{2}d_{\text{KL}}(\nu_{c},\nu_{a}).\]

This completes the proof.

### Proof and Discussion of Remark 2.4

Utilizing the same notations as in the proof of Theorem 2.3, and considering that \(\pi_{c}\) is a feasible solution to the UGW problem, substituting it directly leads to the desired result.

Furthermore, referring to [40, Theorem 1] and adapting the notations to our framework, we have the following property: specifically, when considering only \(\mu\) is corrupted with outliers, characterized by \(\mu=(1-\epsilon_{1})\mu_{c}+\epsilon_{1}\mu_{a}\), while \(\nu=\nu_{c}\). If we let \(\delta=2(\tau_{1}+\tau_{2})\epsilon_{1}\), and \(K=M+\frac{1}{M}\text{UGW}(\mu_{c},\nu)+\delta\), where \(M\) represents the transported mass between clean data and \(\Delta_{\infty}\) signifies the maximal deviation between the contaminated source and the target, then the following inequality holds:

\[\text{UGW}(\mu,\nu)\leq(1-\epsilon_{1})\text{UGW}(\mu_{c},\nu)+\delta M\left[1- \exp\left(-\frac{\Delta_{\infty}(1+M)+K}{\delta M}\right)\right].\]

Given that \(\text{UGW}(\mu_{c},\nu)\leq\text{GW}(\mu_{c},\nu)\), we can derive the following relationship:

\[\text{UGW}(\mu,\nu)\leq(1-\epsilon_{1})\text{GW}(\mu_{c},\nu)+\delta M\left[1 -\exp\left(-\frac{\Delta_{\infty}(1+M)+M+\frac{1}{M}\text{GW}(\mu_{c},\nu)+ \delta}{\delta M}\right)\right]. \tag{9}\]

Comparing this result to Remark 2.4, (9) also incorporates \(\text{GW}(\mu_{c},\nu)\) in the exponential term and \(M\), the transported mass between clean data. These variables may be influenced by the marginal parameters \(\tau_{1}\) and \(\tau_{2}\). As a result, finding an optimal choice for \(\tau_{1}\) and \(\tau_{2}\) to establish a tight upper bound for UGW using the GW distance between clean samples, as presented in (9), proves challenging. As outlined in Remark 2.4, it is worth noting that setting \(\tau_{1}=\tau_{2}=0\) represents the sole means to attain a tight upper bound. Nevertheless, in real-world applications, this approach is impractical, as it could lead to over-relaxation and compromise the performance.

Appendix C Proof Details of Bregman Proximal Alternating Linearized Minimization Method for Robust GW

Given a vector \(\mathbf{x}\), we use \(\|x\|_{2}\) to denote its \(\ell_{2}\) norm. We use \(\|X\|_{F}\) to denote the Frobenius norm of matrix \(X\). For a convex set \(C\) and a point \(x\), we define the distance between \(C\) and \(x\) as

\[\text{dist}(x,C)=\min_{y\in C}\|x-y\|_{2}.\]

### Proof of Proposition 3.1

Proof.: Problem (7) can be written as

\[\min_{\alpha\in\Delta^{n}}\sum_{i=1}^{n}\left((\pi_{1}^{k+1})_{i}\log\left( \frac{(\pi_{1}^{k+1})_{i}}{\alpha_{i}}\right)-(\pi_{1}^{k+1})_{i}+\alpha_{i} \right)+\frac{1}{c_{k}}\sum_{i=1}^{k}\alpha_{i}^{k}\log\left(\frac{\alpha_{i} ^{k}}{\alpha_{i}}\right)+w\left(\sum_{i=1}^{n}\mu_{i}\log\left(\frac{\mu_{i}}{ \alpha_{i}}\right)-\rho_{1}\right).\]

We first consider a relaxed problem of the problem above:

\[\min_{\alpha^{T}\mathbf{1}_{n}=1}\sum_{i=1}^{n}\left((\pi_{1}^{k+1})_{i}\log \left(\frac{(\pi_{1}^{k+1})_{i}}{\alpha_{i}}\right)-(\pi_{1}^{k+1})_{i}+\alpha _{i}\right)+\frac{1}{c_{k}}\sum_{i=1}^{n}\alpha_{i}^{k}\log\left(\frac{\alpha _{i}^{k}}{\alpha_{i}}\right)+w\left(\sum_{i=1}^{n}\mu_{i}\log\left(\frac{\mu_{ i}}{\alpha_{i}}\right)-\rho_{1}\right).\]

Consider the Lagrangian function of the relaxed problem

\[L(\alpha,\lambda)= \sum_{i=1}^{n}\left((\pi_{1}^{k+1})_{i}\log\left(\frac{(\pi_{1}^{k +1})_{i}}{\alpha_{i}}\right)-(\pi_{1}^{k+1})_{i}+\alpha_{i}\right)+\frac{1}{c_ {k}}\sum_{i=1}^{n}\alpha_{i}^{k}\log\left(\frac{\alpha_{i}^{k}}{\alpha_{i}} \right)+\] \[w\left(\sum_{i=1}^{n}\mu_{i}\log\left(\frac{\mu_{i}}{\alpha_{i}} \right)-\rho_{1}\right)+\lambda\left(\alpha^{T}\mathbf{1}_{n}-1\right).\]

Let

\[\frac{\partial L}{\partial\alpha}=\begin{pmatrix}-\frac{(\pi_{1}^{k+1})_{1}}{ \alpha_{2}}+1\\ -\frac{(\pi_{1}^{k+1})_{2}}{\alpha_{2}}+1\\ \vdots\\ -\frac{(\pi_{1}^{k+1})_{n}}{\alpha_{n}}+1\end{pmatrix}+\frac{1}{c_{k}}\begin{pmatrix} -\frac{\alpha_{1}^{k}}{\alpha_{1}}\\ -\frac{\alpha_{2}^{k}}{\alpha_{2}}\\ \vdots\\ -\frac{\alpha_{n}^{k}}{\alpha_{n}}\end{pmatrix}+w\begin{pmatrix}-\frac{\mu_{1}} {\alpha_{1}}\\ -\frac{\mu_{2}}{\alpha_{2}}\\ \vdots\\ -\frac{\mu_{n}}{\alpha_{n}}\end{pmatrix}+\begin{pmatrix}\lambda\\ \lambda\\ \vdots\\ \lambda\end{pmatrix}=0.\]Then we obtain that

\[\alpha_{i}=\frac{(\pi_{1}^{k+1})_{i}+\frac{1}{c_{k}}\alpha_{i}^{k}+w\mu_{i}}{1+ \lambda}.\]

Since \(\sum_{i=1}^{n}\alpha_{i}=1\), \(\sum_{i=1}^{n}\alpha_{i}^{k}=1\), and \(\sum_{i=1}^{n}\mu_{i}=1\), then \(1+\lambda=\sum_{ij}\pi_{ij}^{k+1}+\frac{1}{c_{k}}+w\). Thus, the optimal solution to the relaxed problem is

\[\hat{\alpha}(w)=\frac{\pi^{k+1}\mathbf{1}_{m}+\frac{1}{c_{k}}\alpha^{k}+w\mu}{ \sum_{i,j}\pi_{ij}^{k+1}+\frac{1}{c_{k}}+w}.\]

We can see that \(\hat{\alpha}(w)\geq 0\). Hence, \(\hat{\alpha}(w)\) is also the optimal solution to problem (7).

Since if \(w\) satisfies (i) or (ii), \((\hat{\alpha}(w),w)\) is a solution to KKT conditions of problem (5), therefore, \(\hat{\alpha}(w)\) is an optimal solution to problem (5). Next, we prove that \(p\) is differentiable when \(h\) is relative entropy. Problem (7) admits the closed-form solution

\[\hat{\alpha}(w)=\frac{\pi^{k+1}\mathbf{1}_{m}+\frac{1}{c_{k}}\alpha^{k}+w\mu}{ \sum_{i,j}\pi_{ij}^{k+1}+\frac{1}{c_{k}}+w}. \tag{10}\]

By substituting (10) into \(p\), \(p(w)\) can be written as

\[p(w)=\sum_{i=1}^{n}\mu_{i}\log\left(\frac{\mu_{i}\left(\sum_{i,j}\pi_{ij}^{k+1 }+\frac{1}{c_{k}}+w\right)}{\left(\pi^{k+1}\mathbf{1}_{m}\right)_{i}+\frac{1} {c_{k}}\alpha_{i}^{k}+w\mu_{i}}\right)-\rho_{1}.\]

Thus, \(p\) is twice differentiable. The first-order derivative and second-order of \(p\) are

\[p^{\prime}(w)=\sum_{i=1}^{n}\mu_{i}\frac{\left(\left(\pi^{k+1}\mathbf{1}_{m} \right)_{i}+\frac{1}{c_{k}}\alpha_{i}^{k}+\mu_{i}w\right)^{2}-\mu_{i}^{2}\left( \sum_{ij}\pi_{ij}^{k+1}+\frac{1}{c_{k}}+w\right)^{2}}{\left(\sum_{ij}\pi_{ij}^ {k+1}+\frac{1}{c_{k}}+w\right)^{2}\left(\left(\pi^{k+1}\mathbf{1}_{m}\right)_ {i}+\frac{1}{c_{k}}\alpha_{i}^{k}+\mu_{i}w\right)^{2}}.\]

Then we prove that \(p^{\prime}(w)\leq 0\) and \(p^{\prime\prime}(w)\geq 0\) for \(w\geq 0\), so \(p\) is monotonically non-increasing and convex on \(\mathbb{R}_{+}\). Let \(s_{i}=\left(\pi^{k+1}\mathbf{1}_{m}\right)_{i}+\frac{1}{c_{k}}\alpha_{i}^{k}+ \mu_{i}w\) and \(s=\left(\pi^{k+1}\mathbf{1}_{m}\right)_{i}+\frac{1}{c_{k}}\alpha_{i}^{k}+\mu_{ i}w\). Note that \(s=\sum_{i=1}^{n}s_{i}\). Then \(p^{\prime}\) and \(p^{\prime\prime}\) can be written as

\[p^{\prime}(w)=\sum_{i=1}^{n}\mu_{i}\frac{s_{i}-\mu_{i}s}{s_{i}\cdot s}=\frac{1 }{s}\left(1-\sum_{i=1}^{n}\mu_{i}\frac{\mu_{i}s}{s_{i}}\right),\]

and

\[p^{\prime\prime}(w)=-\sum_{i=1}^{n}\mu_{i}\frac{s_{i}^{2}-\mu_{i}^{2}s^{2}}{s_ {i}^{2}\cdot s^{2}}=-\frac{1}{s^{2}}\left(1-\sum_{i=1}^{n}\mu_{i}\frac{\mu_{i} ^{2}s^{2}}{s_{i}^{2}}\right).\]

Therefore, it is equivalent to show \(\sum_{i=1}^{n}\mu_{i}\frac{\mu_{i}s}{s_{i}}\geq 1\) and \(\sum_{i=1}^{n}\mu_{i}\frac{\mu_{i}^{2}s^{2}}{s_{i}^{2}}\geq 1\).

Recall that \(\frac{1}{x}\) and \(\frac{1}{x^{2}}\) are convex on \(\mathbb{R}_{++}\), then

\[\sum_{i=1}^{n}\mu_{i}\frac{\mu_{i}s}{s_{i}}=\sum_{i=1}^{n}\frac{1 }{\mu_{i}\frac{s_{i}}{\mu_{i}s}}\geq\frac{1}{\sum_{i=1}^{n}\mu_{i}\frac{s_{i}} {\mu_{i}s}}=1,\] \[\sum_{i=1}^{n}\mu_{i}\frac{\mu_{i}^{2}s^{2}}{s_{i}^{2}}=\sum_{i= 1}^{n}\mu_{i}\frac{\left(\frac{s_{i}}{\mu_{i}s}\right)^{2}}{\left(\sum_{i=1}^{n }\mu_{i}\frac{s_{i}}{\mu_{i}s}\right)^{2}}=1.\]

### Proof of Proposition 3.2

Proof.: First, prove that \(p\) only has one root on \(I\). Since \(p\) is continuous on \(I\) and there exists \(\tilde{x}\), \(\bar{x}\in I\) such that \(p(\tilde{x})>0\) and \(p(\bar{x})<0\), \(p\) contains at least one root on \([\tilde{x},\bar{x}]\). Since \(p\) is non-increasing, \(p\) cannot have roots outside \([\tilde{x},\bar{x}]\). Suppose that \(p\) have two different roots \(z_{1}\) and \(z_{2}\) on \([\tilde{x},\bar{x}]\) and \(z_{1}<z_{2}\). By convexity of \(p\), we have

\[0=p(z_{2})=p\left(\frac{\bar{x}-z_{2}}{\bar{x}-z_{1}}z_{1}+\frac{z_{2}-z_{1}}{ \bar{x}-z_{1}}\bar{x}\right)\leq\frac{\bar{x}-z_{2}}{\bar{x}-z_{1}}p(z_{1})+ \frac{z_{2}-z_{1}}{\bar{x}-z_{1}}p(\bar{x})=\frac{z_{2}-z_{1}}{\bar{x}-z_{1}}p (\bar{x})<0.\]

This is a contradiction. So \(p\) has a unique root on \(I\).

\(p^{\prime}(x)\leq 0\) since \(p\) is non-increasing on \(I\). Denote the root of \(p\) as \(r\). Claim that \(p^{\prime}(x)<0\) for \(x\in[\tilde{x},r]\). Otherwise, there exist \(x\in[\tilde{x},r]\) such that \(p^{\prime}(x)=0\), then

\[0>p(\bar{x})\geq p(x)+p^{\prime}(x)(\bar{x}-x)=p(x)\geq 0,\]

which leads to a contradiction. Especially, \(p^{\prime}(\tilde{x})<0\), and we can set \(\tilde{x}\) as the initial point of Newton's method.

The update of Newton's method is

\[x_{k+1}=x_{k}-\frac{p(x_{k})}{p^{\prime}(x_{k})}.\]

Therefore, \(x_{k+1}\geq x_{k}\) and \(\{x_{k}\}_{k\geq 1}\) is an increasing sequence. Since \(p\) is convex,

\[p(x_{k+1})\geq p(x_{k})+p^{\prime}(x_{k})(x_{k+1}-x_{k})=p(x_{k})-p(x_{k})=0.\]

\(x_{k}\leq r\) because \(p\) is a monotonically non-increasing function. \(\{x_{k}\}_{k\geq 1}\) is an increasing sequence with an upper bound, so it has a limit \(x^{*}\) and \(\lim_{k\to\infty}(x_{k}-x_{k+1})=0\). Also, \(p^{\prime}\) is bounded on \([\tilde{x},r]\) since it is continuous. Therefore,

\[p(x^{*})=\lim_{k\to+\infty}p(x_{k})=\lim_{k\to+\infty}p^{\prime}(x_{k})(x_{k}- x_{k+1})=0.\]

Hence, the sequence generated by Newton's method converges to a root of \(p\). 

### Proof of Theorem 3.3

**Assumption C.1**.: The critical point set \(\mathcal{X}\) is non-empty.

Before the proof of Theorem 3.3, we first prove that sequence \(\{\pi^{k}\}_{k\geq 0}\) generated by BPALM lies in a compact set.

**Proposition C.2**.: _Sequence \(\{\pi^{k}\}_{k\geq 0}\) generated by BPALM lies in a compact set._

Proof.: We prove that \(\{\pi^{k}\}_{k\geq 0}\) lies in the compact set \(\mathcal{A}\coloneqq\{\pi\in\mathbb{R}^{n\times m}:0\leq\pi_{ij}\leq 1\}\) by mathematical induction. For \(k=0\), we can initialize \(\pi^{0}\) with \(0\leq\pi^{0}_{ij}\leq 1\). Suppose that \(\pi^{k}\in\mathcal{A}\) and \(\pi^{k+1}\notin\mathcal{A}\). Then there exist \(i\in[n]\) and \(j\in[m]\) such that \(\pi^{k+1}_{ij}>1\). Recall that \(\pi^{k+1}\) is the optimal solution to the problem

\[\min_{\pi\geq 0}\ \varphi(\pi)\coloneqq\langle\mathcal{L}(D,\bar{D})\otimes\pi^{ k},\pi\rangle+\tau_{1}d_{\textbf{KL}}(\pi\mathbf{1}_{m},\alpha^{k})+\tau_{2}d_{ \textbf{KL}}(\pi^{T}\mathbf{1}_{n},\beta^{k})+\frac{1}{t_{k}}d_{\textbf{KL}}( \pi,\pi^{k}), \tag{11}\]

Observe that function \(\phi(x)\coloneqq x\log\frac{x}{a}-x+a\) is a unimodal function on \(\mathbb{R}_{+}\) and achieve its minimum at \(x=a\). Since \(\alpha_{i}\), \(\beta_{j}\), and \(\pi^{k}_{ij}\) are smaller than or equal to 1, \(\alpha_{i}\), \(\beta_{j}\), and \(\pi^{k}_{ij}\) are strictly smaller than \(\pi^{k+1}_{ij}\). Let \(\tilde{\pi}\in\mathbb{R}^{n\times m}\),

\[\tilde{\pi}_{kl}=\begin{cases}\max\{\alpha_{i},\beta_{j},\pi^{k}_{ij}\},&(k,l )=(i,j),\\ \pi^{k+1}_{kl},&\text{otherwise}.\end{cases}\]

Then \(\varphi(\tilde{\pi})<\varphi(\pi^{k})\), this contradicts to \(\pi^{k+1}\) is the optimal solution to problem (11). Thus, \(\pi^{k+1}\in\mathcal{A}\).

For the proof of Theorem 3.3, we first prove the sufficient decrease property of BPALM, i.e., there exist a constant \(\kappa_{1}>0\) and an index \(k_{1}\geq 0\) such that for \(k\geq k_{1}\),

\[F\left(\pi^{k+1},\alpha^{k+1},\beta^{k+1}\right)-F\left(\pi^{k},\alpha^{k},\beta ^{k}\right)\leq-\kappa_{1}\left(\left\|\pi^{k+1}-\pi^{k}\right\|_{F}^{2}+\left\| \alpha^{k+1}-\alpha^{k}\right\|_{2}^{2}+\left\|\beta^{k+1}-\beta^{k}\right\|_{ 2}^{2}\right).\]

And then we prove the subsequence convergence result.

Proof.: It is worth noting that \(f(\pi)\) is a quadratic function, i.e., \(f(\pi)=\left\langle\mathcal{L}(D,\bar{D})\otimes\pi,\pi\right\rangle\), then \(f(\pi)\) is gradient Lipschitz continuous with the constant \(\max_{i,j}\left(\sum_{k,l}\mathcal{L}(D,\bar{D})_{i,j,k,l}^{2}\right)^{1/2}\). To simplify the notation, let \(L_{f}=\max_{i,j}\left(\sum_{k,l}\mathcal{L}(D,\bar{D})_{i,j,k,l}^{2}\right)^{1 /2}\).

\[F\left(\pi^{k+1},\alpha^{k+1},\beta^{k+1}\right)-F\left(\pi^{k}, \alpha^{k},\beta^{k}\right)\] \[\leq\] \[\quad h_{1}\left(\alpha^{k+1}\right)+h_{2}\left(\beta^{k+1} \right)-\left(q\left(\pi^{k}\right)+g_{1}\left(\pi^{k},\alpha^{k}\right)+g_{2} \left(\pi^{k},\beta^{k}\right)+h_{1}\left(\alpha^{k}\right)+h_{2}\left(\beta^{ k}\right)\right)\] \[\stackrel{{(\diamondsuit)}}{{\leq}}\] \[\quad h_{1}\left(\alpha^{k+1}\right)+h_{2}\left(\beta^{k+1} \right)-\left(q\left(\pi^{k}\right)+g_{1}\left(\pi^{k},\alpha^{k}\right)+g_{2} \left(\pi^{k},\beta^{k}\right)+h_{1}\left(\alpha^{k}\right)+h_{2}\left(\beta^{ k}\right)\right)\] \[\quad h_{1}\left(\alpha^{k+1}\right)+h_{2}\left(\beta^{k+1} \right)-\left(q\left(\pi^{k}\right)+g_{1}\left(\pi^{k},\alpha^{k}\right)+g_{2} \left(\pi^{k},\beta^{k}\right)+h_{1}\left(\alpha^{k}\right)+h_{2}\left(\beta^{ k}\right)\right)\] \[= \left\langle\nabla f(\pi^{k}),\pi^{k+1}\right\rangle+q\left(\pi^{ k+1}\right)+g_{1}\left(\pi^{k+1},\alpha^{k}\right)+g_{2}\left(\pi^{k+1},\beta^{k} \right)+\frac{1}{t_{k}}d_{\textbf{KL}}\left(\pi^{k+1},\pi^{k}\right)-\] \[\quad\left\langle\nabla f(\pi^{k}),\pi^{k}\right\rangle-q\left( \pi^{k}\right)-g_{1}\left(\pi^{k},\alpha^{k}\right)-g_{2}\left(\pi^{k},\beta^ {k}\right)+g_{1}\left(\pi^{k+1},\alpha^{k+1}\right)+h_{1}\left(\alpha^{k+1} \right)+\frac{1}{c_{k}}d_{\textbf{KL}}\left(\alpha^{k},\alpha^{k+1}\right)-\] \[\quad g_{1}\left(\pi^{k},\alpha^{k}\right)-h_{1}\left(\alpha^{k} \right)+g_{2}\left(\pi^{k+1},\beta^{k+1}\right)+h_{2}\left(\beta^{k+1}\right) +\frac{1}{r_{k}}d_{\textbf{KL}}\left(\beta^{k},\beta^{k+1}\right)-g_{2}\left( \pi^{k},\beta^{k}\right)-h_{2}\left(\beta^{k}\right)-\] \[\quad\left(\frac{1}{t_{k}}-\frac{L_{f}}{\sigma}\right)d_{\textbf{ KL}}\left(\pi^{k+1},\pi^{k}\right)-\frac{1}{c_{k}}d_{\textbf{KL}}\left(\alpha^{k}, \alpha^{k+1}\right)-\frac{1}{r_{k}}d_{\textbf{KL}}\left(\beta^{k},\beta^{k+1}\right)\] \[\leq -\left(\frac{1}{t_{k}}-\frac{L_{f}}{\sigma}\right)d_{\textbf{KL}} \left(\pi^{k+1},\pi^{k}\right)-\frac{1}{c_{k}}d_{\textbf{KL}}\left(\alpha^{k}, \alpha^{k+1}\right)-\frac{1}{r_{k}}d_{\textbf{KL}}\left(\beta^{k},\beta^{k+1} \right).\] \[\stackrel{{(\diamondsuit)}}{{\leq}} -\frac{\sigma}{2}\left(\left(\frac{1}{t_{k}}-\frac{L_{f}}{ \sigma}\right)\left\|\pi^{k+1}-\pi^{k}\right\|_{F}^{2}+\frac{1}{c_{k}}\left\| \alpha^{k+1}-\alpha^{k}\right\|_{2}^{2}+\frac{1}{r_{k}}\left\|\beta^{k+1}- \beta^{k}\right\|_{2}^{2}\right).\]

\((\diamondsuit)\) is because as \(x\log x\) is \(\sigma\)-strongly convex, we have

\[d_{\textbf{KL}}(\pi^{k+1},\pi^{k})\geq\frac{\sigma}{2}\|\pi^{k+1}-\pi^{k}\|_{F}^ {2},\quad d_{\textbf{KL}}(\alpha^{k},\alpha^{k+1})\geq\frac{\sigma}{2}\| \alpha^{k+1}-\alpha^{k}\|_{2}^{2},\quad d_{\textbf{KL}}(\beta^{k},\beta^{k+1}) \geq\frac{\sigma}{2}\|\beta^{k+1}-\beta^{k}\|_{2}^{2}.\]

By letting \(\kappa_{1}=\frac{\sigma}{2}\min\left(\left(\frac{1}{t_{k}}-\frac{L_{f}}{ \sigma}\right),\frac{1}{\bar{r}}\right)>0\), we get

\[F\left(\pi^{k+1},\alpha^{k+1},\beta^{k+1}\right)-F\left(\pi^{k},\alpha^{k}, \beta^{k}\right)\leq-\kappa_{1}\left(\left\|\pi^{k+1}-\pi^{k}\right\|_{F}^{2}+ \left\|\alpha^{k+1}-\alpha^{k}\right\|_{2}^{2}+\left\|\beta^{k+1}-\beta^{k} \right\|_{2}^{2}\right). \tag{12}\]

Summing up (12) from \(k=0\) to \(+\infty\), we obtain

\[\kappa_{1}\sum_{k=0}^{\infty}\left(\left\|\pi^{k+1}-\pi^{k}\right\|_{F}^{2}+ \left\|\alpha^{k+1}-\alpha^{k}\right\|_{2}^{2}+\left\|\beta^{k+1}-\beta^{k} \right\|_{2}^{2}\right)\leq F(\pi^{0},\alpha^{0},\beta^{0})-F(\pi^{\infty},\alpha^ {\infty},\beta^{\infty}).\]

As \(F\) is coercive and \(\left\{\left(\pi^{k},\alpha^{k},\beta^{k}\right)\right\}\) is a bounded sequence, it follows that the left-hand side is bounded. This implies

\[\sum_{k=0}^{\infty}\left(\left\|\pi^{k+1}-\pi^{k}\right\|_{F}^{2}+\left\|\alpha^{ k+1}-\alpha^{k}\right\|_{2}^{2}+\left\|\beta^{k+1}-\beta^{k}\right\|_{2}^{2}\right)<+\infty,\]

and

\[\lim_{k\rightarrow+\infty}(\|\pi^{k+1}-\pi^{k}\|_{F}+\|\alpha^{k+1}-\alpha^{k}\| _{2}+\|\beta^{k+1}-\beta^{k}\|_{2})=0.\]Let \(l(x)=\sum_{i}x_{i}\log x_{i}\). Recall the optimality condition of BPALM, we have

\[0\in\nabla f\left(\pi^{k+1}\right)+\partial q\left(\pi^{k+1}\right)+\nabla_{\pi} g_{1}\left(\pi^{k+1},\alpha^{k}\right)+\nabla_{\pi}g_{2}\left(\pi^{k+1},\beta^{k} \right)+\frac{1}{t_{k}}\left(\nabla l\left(\pi^{k+1}\right)-\nabla l\left(\pi^ {k}\right)\right), \tag{13}\]

\[0\in\nabla_{\alpha}g_{1}\left(\pi^{k+1},\alpha^{k+1}\right)+ \partial h_{1}\left(\alpha^{k+1}\right)+\frac{1}{c_{k}}\nabla^{2}l(\alpha^{k+1 })(\alpha^{k+1}-\alpha^{k}), \tag{14}\] \[0\in\nabla_{\beta}g_{2}\left(\pi^{k+1},\beta^{k+1}\right)+ \partial h_{2}\left(\beta^{k+1}\right)+\frac{1}{r_{k}}\nabla^{2}l(\beta^{k+1}) (\beta^{k+1}-\beta^{k}). \tag{15}\]

Let \((\pi^{\infty},\alpha^{\infty},\beta^{\infty})\) be a limit point of the sequence \(\left\{(\pi^{k},\alpha^{k},\beta^{k})\right\}_{k\geq 0}\). Then, there exists a sequence \(\{n_{k}\}_{k\geq 0}\) such that \(\left\{(\pi^{n_{k}},\alpha^{n_{k}},\beta^{n_{k}})\right\}_{k\geq 0}\) converges to \((\pi^{\infty},\alpha^{\infty},\beta^{\infty})\). Since we assume that \(h\) is twice continuous differentiable and \(\alpha^{k}\) and \(\beta^{k}\) are in a compact set, then \(\nabla^{2}l(\alpha^{k})\) and \(\nabla^{2}l(\beta^{k})\) are bounded. Therefore, \(\lim_{k\rightarrow\infty}\nabla^{2}l(\alpha^{k+1})(\alpha^{k+1}-\alpha^{k})=0\) and \(\lim_{k\rightarrow\infty}\nabla^{2}l(\beta^{k+1})(\beta^{k+1}-\beta^{k})=0\). Replacing the \(k\) by \(n_{k}\) in (13), (14), and (15), taking limits on both sides as \(k\rightarrow\infty\), we obtain that

\[0\in\nabla f\left(\pi^{\infty}\right)+\partial q(\pi^{\infty})+ \nabla_{\pi}g_{1}\left(\pi^{\infty},\alpha^{\infty}\right)+\nabla_{\pi}g_{2} \left(\pi^{\infty},\beta^{\infty}\right),\] \[0\in\nabla_{\alpha}g_{1}\left(\pi^{\infty},\alpha^{\infty} \right)+\partial h_{1}\left(\alpha^{\infty}\right),\] \[0\in\nabla_{\beta}g_{2}\left(\pi^{\infty},\beta^{\infty}\right) +\partial h_{2}\left(\beta^{\infty}\right).\]

Thus \((\pi^{\infty},\alpha^{\infty},\beta^{\infty})\) belongs to \(\mathcal{X}\). 

### Discussion of Computational Complexity of PGW, UGW, and RGW

We consider the measure \(\min_{0\leq k\leq K}(\|\pi^{k+1}-\pi^{k}\|_{F}+\|\alpha^{k+1}-\alpha^{k}\|_{2} +\|\beta^{k+1}-\beta^{k}\|_{2})\) as the stationary measure, and it is observed that the convergence rate of our algorithm is \(\mathcal{O}(\frac{1}{\sqrt{K}})\). Similarly, the Frank-Wolfe algorithm for PGW also exhibits a convergence rate of \(\mathcal{O}(\frac{1}{\sqrt{K}})\). The literature on UGW does not provide a discussion on the convergence rate of alternate Sinkhorn minimization for UGW. In each iteration of PGW, UGW, and RGW, the computation of \(\mathcal{L}(D,\bar{D})\otimes\pi^{k}\) is a crucial step. According to [31], the complexity of this computation is \(\mathcal{O}(n^{2}m+m^{2}n)\). Additionally, PGW involves utilizing the network simplex algorithm to solve a linear programming problem as a subroutine, which has a complexity of \(\mathcal{O}((n^{2}m+m^{2}n)\log^{2}(n+m))\). On the other hand, both UGW and RGW utilize the sinkhorn algorithm to solve an entropic unbalanced optimal transport problem. The complexity of the sinkhorn algorithm for unbalanced OT is \(\mathcal{O}((n^{2}+m^{2})/(\varepsilon\log(\varepsilon)))\) for computing an \(\varepsilon\)-approximation.

## Appendix D Additional Experiment Results

### Additional Experiment Results on Subgraph Alignment

Source codes of all baselines used in this paper:
* FW [17]: [https://github.com/PythonOT/POT](https://github.com/PythonOT/POT)
* SpecGW [15]: [https://github.com/trneedham/Spectral-Gromov-Wasserstein](https://github.com/trneedham/Spectral-Gromov-Wasserstein)
* eBPG [17]: [https://github.com/PythonOT/POT](https://github.com/PythonOT/POT)
* BAPG [24]: [https://github.com/squareRoot3/Gromov-Wasserstein-for-Graph](https://github.com/squareRoot3/Gromov-Wasserstein-for-Graph)
* UGW [35]: [https://github.com/thibsej/unbalanced_gromov_wasserstein](https://github.com/thibsej/unbalanced_gromov_wasserstein)
* PGW [10, 17]: [https://github.com/PythonOT/POT](https://github.com/PythonOT/POT)
* srGW: [42]: [https://github.com/cedricvincentcuaz/srGW](https://github.com/cedricvincentcuaz/srGW)
* RGWD: [25]: [https://github.com/cxxszz/rgdl](https://github.com/cxxszz/rgdl)

Results in Figure 4The data utilized to create Figure 4 is provided in Table 2 and Table 3.

**Selection of Stepsize \(t_{k}\), \(c_{k}\), and \(r_{k}\)** In the subgraph alignment task, we have used constant values for the stepsizes \(t_{k}\), \(c_{k}\) and \(r_{k}\). We have conducted a sensitivity analysis for these parameters, and the details are summarized in Tables 4 and 5. Specifically, Table 4 reveals that RGW achieves its highest accuracy with \(t\) in the range of 0.01 to 0.05, allowing us to select \(t=0.01\) as the default. Table 5 further indicates that accuracy is not significantly affected by variations in \(c\), leading us to set \(c=0.1\) as the default.

Experiment Results of Normalized Degree as Marginal DistributionIn addition to employing the uniform distribution as node distribution, we also explore the use of normalized degrees as node distribution. The results presented in Table 6 confirm that RGW surpasses other methods in terms of accuracy when utilizing normalized degrees as marginal distributions.

Experiment Results of Adding Noise to Query GraphWe conducted an experiment by adding 10% pseudo edges to the subgraphs, and the results can be found in Table 7. These findings

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline \hline \multirow{2}{*}{RGW} & \multicolumn{2}{c|}{Synthetic} & \multicolumn{2}{c|}{Proteins-1} & \multicolumn{2}{c}{Enzymes} \\  & Acc & Time & Acc & Time & Acc & Time \\ \hline \(t\) = 0.01 & 94.64 & 541.50 & 53.07 & 567.09 & 63.23 & 139.82 \\ \(t\) = 0.05 & 90.49 & 288.25 & 53.37 & 1030.15 & 63.69 & 304.11 \\ \(t\) = 0.1 & 87.00 & 233.65 & 53.69 & 713.07 & 62.38 & 506.83 \\ \(t\) = 0.5 & 87.09 & 779.84 & 51.56 & 1797.13 & 60.07 & 822.23 \\ \(t\) = 1.0 & 71.10 & 491.93 & 50.21 & 3071.27 & 58.31 & 1166.88 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The performance of RGW with different stepsize \(t\) on three subgraph alignment databases.

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c|c c} \hline \hline  & \multicolumn{4}{c|}{30\% subgraph} & \multicolumn{4}{c|}{20\% subgraph} \\ \hline \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Synthetic} & \multicolumn{2}{c|}{Proteins} & \multicolumn{2}{c|}{Enzymes} & \multicolumn{2}{c|}{Synthetic} & \multicolumn{2}{c}{Proteins} & \multicolumn{2}{c}{Enzymes} \\  & Acc & Time & Acc & Time & Acc & Time & Acc & Time & Acc & Time \\ \hline FW & 2.22 & 17.06 & 12.96 & 14.83 & 12.08 & 5.37 & 2.24 & 6.65 & 10.83 & 11.34 & 9.53 & 4.92 \\ SpecGW & 1.38 & 2.24 & 10.64 & 11.54 & 9.41 & 3.74 & 1.71 & 2.21 & 10.78 & 10.15 & 8.35 & 3.21 \\ eBPG & 0.65 & 0.49 & 8.12 & 1022.02 & 3.84 & 476.83 & 1.17 & 0.42 & 7.23 & 545.50 & 2.66 & 94.78 \\ BPG & 1.86 & 175.3 & 17.89 & 86.85 & 17.69 & 52.89 & 1.64 & 11.66 & 12.99 & 55.47 & 14.35 & 32.89 \\ BAPG & 2.94 & 35.90 & 18.79 & 36.02 & 16.85 & 10.88 & 3.80 & 32.39 & 14.07 & 23.92 & 11.22 & 8.38 \\ \hline srGW & 3.17 & 86.38 & 22.75 & 89.14 & 27.45 & 41.18 & 5.49 & 88.89 & 18.38 & 17.72 & 23.13 & 17.11 \\ RGWD & 1.94 & 933.25 & 16.90 & 3674.98 & 16.34 & 3322.16 & 1.94 & 933.25 & 16.90 & 3674.98 & 16.34 & 3322.16 \\ UGW & 35.15 & 168.03 & 14.32 & 10298 & 10.91 & 5552.27 & 4.48 & 251.41 & 11.75 & 7813.96 & 10.40 & 4019.62 \\ PGW & 2.06 & 339.23 & 11.68 & 507.11 & 11.77 & 174.26 & 1.90 & 227.87 & 9.34 & 365.88 & 7.97 & 165.27 \\ \hline RGW & **52.35** & 679.00 & **30.17** & 947.48 & **37.12** & 538.04 & **11.58** & 229.05 & **23.51** & 546.15 & **25.39** & 879.93 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of the average matching accuracy (%) and wall-clock time (seconds) on subgraph alignment of 30% subgraph and 20% subgraph.

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline \hline  & \multicolumn{4}{c|}{40\% subgraph} \\ \hline \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Synthetic} & \multicolumn{2}{c|}{Proteins} & \multicolumn{2}{c}{Enzymes} \\  & Acc & Time & Acc & Time & Acc & Time \\ \hline FW & 1.84 & 17.96 & 15.34 & 19.64 & 14.22 & 6.36 \\ SpecGW & 1.72 & 3.25 & 11.21 & 12.17 & 9.59 & 3.88 \\ eBPG & 0.38 & 0.51 & 12.16 & 1628.38 & 9.96 & 943.49 \\ BPG & 3.41 & 18.61 & 24.31 & 108.10 & 24.58 & 62.81 \\ BAPG & 7.61 & 22.55 & 23.78 & 36.81 & 24.82 & 11.13 \\ \hline srGW & 2.45 & 120.12 & 22.58 & 74.58 & 27.02 & 32.14 \\ RGWD & 3.48 & 93.84 & 27.33 & 4490.60 & 22.63 & 3205.03 \\ UGW & 79.61 & 960.04 & 21.22 & 11.938 & 22.26 & 5589.73 \\ PGW & 2.17 & 483.10 & 11.95 & 491.64 & 9.51 & 182.58 \\ \hline RGW & **90.79** & 662.15 & **38.94** & 769.25 & **48.11** & 291.74 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of the average matching accuracy (%) and wall-clock time (seconds) on subgraph alignment of 40% subgraph.

demonstrate that RGW significantly outperforms other methods on the Enzymes and Proteins datasets.

### Additional Experiment Results on Partial Shape Correspondence

Convergence of BPALMThe convergence results for the proposed BPALM using various step sizes are presented in Figure 7 for the toy example discussed in Section 4.1.

Additional Experiment Result on TOSCA DatasetAdditional experiment results on TOSCA Dataset are shown in Figure 8, 9, and 10.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Method & Synthetic & Proteins-1 & Proteins-2 & Enzymes \\ \hline FW & 2.96 & 14.82 & 42.16 & 15.90 \\ SpecGW & 1.57 & 8.92 & 43.10 & 12.07 \\ eBPG & 5.27 & 13.77 & 31.90 & 13.51 \\ BPG & 12.52 & 20.88 & 57.77 & 29.42 \\ BAPG & 74.39 & 24.65 & 63.26 & 31.92 \\ \hline srGW & 4.22 & 13.67 & 12.39 & 23.05 \\ RGWD & 13.54 & 25.92 & 57.30 & 28.12 \\ UGW & 99.56 & 25.51 & 62.92 & 39.71 \\ PGW & 3.97 & 11.59 & 37.79 & 13.08 \\ \hline RGW & **99.61** & **50.61** & **66.09** & **63.59** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Subgraph alignment results (Acc.) of 50% subgraph of compared GW-based methods using normalized degree.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Method & Synthetic & Proteins-1 & Enzymes \\ \hline FW & 2.38\(\pm\)0.27 & 10.21\(\pm\)1.22 & 12.58\(\pm\)12.58 \\ SpecGW & 1.79\(\pm\)0.26 & 9.91\(\pm\)0.56 & 10.00\(\pm\)0.81 \\ eBPG & 6.84\(\pm\)1.89 & 15.63\(\pm\)0.73 & 14.31\(\pm\)0.77 \\ BPG & 17.71\(\pm\)2.23 & 20.96\(\pm\)1.57 & 22.29\(\pm\)1.19 \\ BAPG & 43.39 \(\pm\) 4.84 & 23.08\(\pm\)1.12 & 24.42\(\pm\) 1.54 \\ \hline srGW & 1.75\(\pm\) 0.18 & 13.34\(\pm\)0.56 & 20.03\(\pm\) 0.73 \\ RGWD & 17.30\(\pm\) 1.90 & 22.70\(\pm\)0.33 & 24.07\(\pm\) 0.44 \\ UGW & 81.24 \(\pm\) 2.34 & 22.99\(\pm\)0.16 & 26.68\(\pm\) 1.54 \\ PGW & 1.67 \(\pm\) 0.43 & 8.19\(\pm\)0.92 & 7.33\(\pm\) 0.45 \\ \hline RGW & **88.79**\(\pm\) 1.59 & **38.88\(\pm\)**1.31 & **49.01\(\pm\)**0.99 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Subgraph alignment results (Mean \(\pm\) Std.) of 50% subgraph in 5 independent trials over different random seeds in the noise generating process.

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline \hline \multirow{2}{*}{RGW} & \multicolumn{2}{c|}{Synthetic} & \multicolumn{3}{c|}{Proteins-1} & \multicolumn{2}{c}{Enzymes} \\  & Acc & Time & Acc & Time & Acc & Time \\ \hline \(c=0.01\) & 90.39 & 481.06 & 53.17 & 459.43 & 63.06 & 163.45 \\ \(c=0.05\) & 90.39 & 967.98 & 53.12 & 919.18 & 62.86 & 327.44 \\ \(c=0.1\) & 90.39 & 1460.94 & 53.37 & 1374.43 & 63.69 & 49Figure 8: (a): 3D shape geometry of the source and target; (b)-(e): visualization of ground truth, initial point obtained from the partial functional map, and the matching results of PGW and RGW.

Figure 7: Convergence result of toy example with different stepsizes

Figure 10: (a): 3D shape geometry of the source and target; (b)-(e): visualization of ground truth, initial point obtained from the partial functional map, and the matching results of PGW and RGW.

Figure 9: (a): 3D shape geometry of the source and target; (b)-(e): visualization of ground truth, initial point obtained from the partial functional map, and the matching results of PGW and RGW.