# The Equivalence of Dynamic and Strategic Stability

under Regularized Learning in Games

 Victor Boone &Panayotis Mertikopoulos

Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France

{victor.boone,panayotis.mertikopoulos}@univ-grenoble-alpes.fr

###### Abstract

In this paper, we examine the long-run behavior of regularized, no-regret learning in finite games. A well-known result in the field states that the empirical frequencies of no-regret play converge to the game's set of coarse correlated equilibria; however, our understanding of how the players' _actual_ strategies evolve over time is much more limited - and, in many cases, non-existent. This issue is exacerbated by a series of recent results showing that _only_ strict Nash equilibria are stable and attracting under regularized learning, thus making the relation between learning and pointwise solution concepts particularly elusive. In lieu of this, we take a more general approach and instead seek to characterize the _setwise_ rationality properties of the players' day-to-day play. To that end, we focus on one of the most stringent criteria of setwise strategic stability, namely that any unilateral deviation from the set in question incurs a cost to the deviator - a property known as _closedness under better replies_ (club). In so doing, we obtain a far-reaching equivalence between strategic and dynamic stability: _a product of pure strategies is closed under better replies if and only if its span is stable and attracting under regularized learning._ In addition, we estimate the rate of convergence to such sets, and we show that methods based on entropic regularization (like the exponential weights algorithm) converge at a geometric rate, while projection-based methods converge within a _finite_ number of iterations, even with bandit, payoff-based feedback.

## 1 Introduction

Background. The question of whether players can learn to emulate rational behavior through repeated interactions has been one of the mainstays of non-cooperative game theory, and it has recently gained increased momentum owing to a surge of breakthrough applications to machine learning and data science, from online ad auctions to multi-agent reinforcement learning. Informally, this question can be stated as follows:

_If each player follows an iterative procedure aiming to increase their individual payoff, does the players' long-run behavior converge to a rationally admissible state?_

A natural setting for studying this question is to assume that each player is following a no-regret algorithm, i.e., a policy which is asymptotically as good against a given sequence of payoff functions as the best fixed strategy in hindsight. In this framework, the link between learning and rationality is provided by a folk result which states that, under no-regret learning, the empirical frequency of play converges to the game's set of _coarse correlated equilibria_ (CCE) - also known as the game's _Hannan set_[26]. This result has been of seminal importance to the field because no-regret play can be achieved via a wide class of "regularized learning" policies, as exemplified by the _"follow-the-regularized-leader"_ (FTRL) family of algorithms [45, 46] and its variants - optimistic methods [15, 28, 41, 42, 47], Hedge/EXP3 [4, 6, 10, 11], implicitly normalized forecasters [1, 3], etc.

All these policies have (at least) one thing in common: they seek to provide the tightest possible guarantees for each player's individual regret, thus accelerating convergence to the game's Hannan set. As such, in games where the marginalization of coarse correlated equilibria coincides with the game's Nash equilibria (like two-player zero-sum games), we obtain a positive equilibrium convergence guarantee: the long-run empirical frequency of play evolves "as if" the players were rational to begin with - i.e., as if they had full knowledge of the game, common knowledge of rationality, the ability to communicate this knowledge, etc.

On the other hand, in many other contexts, the marginals of Hannan-consistent correlated strategies may fail even the weakest axioms of rational behavior and rationalizability (such as the elimination of strictly dominated strategies). In particular, a well-known example of Viossat & Zapechelnyuk [49] (which we discuss in detail in Section 4) shows that it is possible to have _negative regret_ for all time, but still employ _only strictly dominated strategies_ throughout the entire horizon of play.

The reason for this disconnect is that no-regret play has significant predictive power for the empirical frequency of play - that is, the long-run empirical distribution of pure strategy _profiles_ - but much less so for the players' day-to-day sequence of play - i.e., the evolution of the players' _actual_ mixed strategies over time. In particular, even when the marginalization of the Hannan set is Nash, the actual trajectory of play may - and, in fact, often _does_ - diverge away from the game's set of equilibria [16, 24, 34, 35, 36] or exhibits chaotic, unpredictable oscillations [12, 38]. Thus, especially in the context of regularized learning - where players learn _independently_ from one another, with no common correlating device - the blanket guarantee of no-regret play may quickly become irrelevant, and even misleading, providing the veneer of rational behavior but not the substance.

Motivated by the above, our paper seeks to understand the rationality properties of the players' _actual_ sequence of play under regularized learning, as encoded by the following question:

_Which sets of mixed strategies are stable and attracting under regularized learning?_

_Are these sets robust to strategic deviations? And, if so, is the converse also true?_

**Our contributions in the context of related work.** This question has attracted significant interest in the literature, especially in its pointwise version, namely: Which mixed strategy profiles are stable and attracting under regularized learning? Are the dynamics' stable states robust to unilateral deviations? And, if so, are these the only stable states of regularized learning?

In the related setting of population games, the answer to this question is sometimes referred to as the "folk theorem of evolutionary game theory" [14, 27, 51]. Somewhat informally, this theorem states that, under the replicator dynamics (the continuous-time analogue of the exponential / multiplicative weights algorithm, itself an archetypal regularized learning method), the following is true for _all_ games: only Nash equilibria are (Lyapunov) stable, and a state is stable and attracting under the replicator dynamics if and only it is a strict Nash equilibrium of the underlying game [27, 51].1

Footnote 1: Strictness here means that each player has a unique best response at equilibrium.

In the context of regularized learning, [13, 20, 33] showed that a similar equivalence holds for the dynamics of FTRL in _continuous_ time: a state is stable and attracting under the FTRL dynamics if and only if it is a strict Nash equilibrium. Subsequently, Giannou et al. [22, 23] extended this equivalence to an entire class of regularized learning schemes, with different types of feedback and/or update structures - from optimistic methods to algorithms run with bandit, payoff-based information. In all these cases, the same principle emerges: under regularized learning, _a state is asymptotically stable and attracting if and only if it is a strict Nash equilibrium._

This is an important pointwise prediction but it does not cover cases where regularized learning algorithms do not converge to a point, but to a _set_ (such as a limit cycle or other non-trivial attractor). In this case, the very definition of strategic stability is an intricate affair, and there are several definitions that come into play [7, 17, 21, 43]. The first such notion that we consider is that of "resilience to strategic deviations", namely that every unilateral deviation from the set under study is deterred by some other element thereof. Our first contribution in this direction is a universal guarantee to the effect that, with probability 1, in any game, and from any initial condition, _the long-run limit of any regularized learning algorithm is a resilient set._

This result is significant in its universality, but the notion of resilience is not sufficiently strong to disallow irrational behavior - and, in fact, it is subject to similar shortcomings as Hannan consistency.

To account for this deficiency, we turn to a much more stringent criterion of setwise strategic stability, that of _closedness under better replies_ (club). This notion, originally due to Ritzberger & Weibull [43], states that any deviation from a product of pure strategies is costly, and it is one of the strictest setwise refinements in game theory. In particular, it refines the notion of closedness under rational behavior (curb) [7], and it satisfies all the seminal strategic stability requirements of Kohlberg & Mertens [30], including robustness to strategic payoff perturbations.2

Footnote 2: Roughly speaking, robustness to strategic payoff perturbations means that the set under study remains stable even if the payoffs of the game are subject to small – but possibly adversarial – perturbations.

In this general context, we show that regularized learning enjoys a striking relation with club sets: _A product of pure strategies is closed under better replies if and only if its span is stable and attracting under regularized learning_. In fact, we show that this equivalence can be refined to sets that are _minimally_ closed under better replies (in the sense that they do not contain a strictly smaller closed under better replies (club) set): a product of pure strategies is minimally club (m-club) if and only if its span is irreducibly stable and attracting (in that it does not contain a smaller asymptotically stable span of strategies). Finally, we also estimate the rate of convergence to club sets, and we establish convergence at a geometric rate for entropically regularized methods - like Hedge and EXP3 - and in a _finite number_ of iterations under projection-based methods.

In light of the above, our results can be seen both as a far-reaching setwise generalization of the folk theorem of evolutionary game theory, as well as a bona fide algorithmic analogue of a precursor result for the replicator dynamics, originally due to Ritzberger & Weibull [43]. Importantly, our analysis covers several different update structures - "vanilla" regularized methods, but also their optimistic variants - as well as a wide range of information models - from full payoff information to bandit, payoff-based feedback.

## 2 Preliminaries

We start by recalling some basics from game theory, roughly following the classical treatise of Fudenberg & Tirole [21]. First, a _finite game in normal form_ consists of (\(i\)) a finite set of _players_\(i\in\mathcal{N}\equiv\{1,\ldots,N\}\); (\(ii\)) a finite set of _actions_ - or _pure strategies_ - \(\mathcal{A}_{i}\) per player \(i\in\mathcal{N}\); and (\(iii\)) an ensemble of _payoff functions_\(u_{i}\colon\ \prod_{j}\mathcal{A}_{j}\to\mathbf{R}\), each determining the reward \(u_{i}(\alpha)\) of player \(i\in\mathcal{N}\) in a given _action profile_\(\alpha=(\alpha_{1},\ldots,\alpha_{N})\). Collectively, we will write \(\mathcal{A}=\prod_{j}\mathcal{A}_{j}\) for the game's _action space_ and \(\Gamma\equiv\Gamma(\mathcal{N},\mathcal{A},u)\) for the game with primitives as above.

During play, each player \(i\in\mathcal{N}\) may randomize their choice of action by playing a _mixed strategy_, i.e., a probability distribution \(x_{i}\in\mathcal{X}_{i}\coloneqq\Delta(\mathcal{A}_{i})\) over \(\mathcal{A}_{i}\) that selects \(\alpha_{i}\in\mathcal{A}_{i}\) with probability \(x_{i\alpha_{i}}\). To lighten notation, we identify \(\alpha_{i}\in\mathcal{A}_{i}\) with the mixed strategy that assigns all weight to \(\alpha_{i}\) (thus justifying the terminology "pure strategies"). Then, writing \(x=(x_{i})_{i\in\mathcal{N}}\) for the players' _strategy profile_ and \(\mathcal{X}=\prod_{i}\mathcal{X}_{i}\) for the game's _strategy space_, the players' payoff functions may be extended to all of \(\mathcal{X}\) by setting

\[u_{i}(x)\coloneqq\mathbf{E}_{\alpha\sim x}[u_{i}(\alpha)]=\sum\nolimits_{ \alpha\in\mathcal{A}}u_{i}(\alpha)\,x_{\alpha}\] (1)

where, in a slight abuse of notation, we write \(x_{\alpha}\) for the joint probability of playing \(\alpha\in\mathcal{A}\) under \(x\), i.e., \(x_{\alpha}=\prod_{i}x_{i\alpha_{i}}\). This randomized framework will be referred to as the _mixed extension_ of \(\Gamma\) and we will denote it by \(\Delta(\Gamma)\).

For concision, we will also write \((x_{i};x_{-i})=(x_{1},\ldots,x_{i},\ldots,x_{N})\) for the strategy profile where player \(i\) plays \(x_{i}\in\mathcal{X}_{i}\) against the strategy profile \(x_{-i}\in\prod_{j\neq i}\mathcal{X}_{j}\) of all other players (and likewise for pure strategies). In this notation, we also define each player's _mixed payoff vector_ as

\[v_{i}(x)=(u_{i}(\alpha_{i};x_{-i}))_{\alpha_{i}\in\mathcal{A}_{i}}\] (2)

so the payoff to player \(i\in\mathcal{N}\) under \(x\in\mathcal{X}\) becomes

\[u_{i}(x)=\sum\nolimits_{\alpha\in\mathcal{A}_{i}}u_{i}(\alpha_{i};x_{-i})\,x _{i\,\alpha_{i}}=\langle v_{i}(x),x_{i}\rangle.\] (3)

Moving forward, the _best-response correspondence_ of player \(i\in\mathcal{N}\) is defined as the set-valued mapping \(\mathtt{br}_{i}\colon\mathcal{X}\rightrightarrows\mathcal{X}_{i}\) given by

\[\mathtt{br}_{i}(x)=\arg\max\nolimits_{x_{i}^{\prime}\in\mathcal{X}_{i}}u_{i}(x _{i}^{\prime};x_{-i})\quad\text{for all }x\in\mathcal{X}.\] (4)

Extending this over all players, we will write \(\mathtt{br}=\prod_{i}\mathtt{br}_{i}\) for the product correspondence \(\mathtt{br}(x)=\mathtt{br}_{1}(x)\times\cdots\times\mathtt{br}_{N}(x)\), and we will say that \(x^{*}\in\mathcal{X}\) is a _Nash equilibrium_ (NE) if \(x^{*}\in\mathtt{br}(x^{*})\)Equivalently, given that \(u_{i}(x^{\prime}_{i};x_{-i})\) is linear in \(x^{\prime}_{i}\), we conclude that \(x^{*}\) is a Nash equilibrium if and only if

\[u_{i}(x^{*})\geq u_{i}(\alpha_{i};x^{*}_{-i})\quad\text{for all $\alpha_{i}\in \mathcal{A}_{i}$ and all $i\in\mathcal{N}$.}\] (NE)

As a final point of note, if \(x^{*}\) is a Nash equilibrium where each player has a unique best response - that is, \(\text{br}_{i}(x^{*})=\{x^{*}_{i}\}\) for all \(i\in\mathcal{N}\) - we will say that \(x^{*}\) is _strict_ because, in this case, \(u_{i}(x^{*})>u_{i}(x_{i};x^{*}_{-i})\) for all \(x_{i}\neq x^{*}_{i}\), \(i\in\mathcal{N}\). An immediate consequence of this is that strict equilibria are _pure_, i.e., each \(x^{*}_{i}\) is a pure strategy. Among Nash equilibria, strict equilibria are the only ones that are "structurally robust" (in the sense that they remain invariant to small perturbations of the underlying game), so they play a particularly important role in game theory.

## 3 Regularized learning in games

Throughout our paper, we will consider iterative decision processes that unfold as follows:

1. At each stage \(t=1,2,\dots\), every participating agent selects an action.
2. Agents receive a reward determined by their chosen actions and their individual payoff functions.
3. Based on this reward (or other feedback), the agents update their strategies and the process repeats.

In this online setting, a crucial requirement is the minimization of the players' _regret_, i.e., the difference between a player's cumulative payoff over time and the player's best possible strategy in hindsight. Formally, if the players' actions at each epoch \(t=1,2,\dots\) are collectively drawn by the probability distribution \(z_{t}\in\Delta(\mathcal{A})\), the _regret_ of each player \(i\in\mathcal{N}\) is defined as

\[\text{Reg}_{i}(T)=\max_{\alpha_{i}\in\mathcal{A}_{i}}\sum_{t=1}^{T}[u_{i}( \alpha_{i};z_{-i,t})-u_{i}(z_{t})],\] (5)

and we will say that player \(i\) has _no regret_ if \(\text{Reg}_{i}(T)=o(T)\).

One of the most widely used policies to achieve no-regret play is the so-called _"follow-the-regularized-leader"_ (FTRL) family of algorithms and its variants [45, 46]. To motivate the analysis to come, we begin with an archetypal FTRL method, the _exponential/multiplicative weights_ algorithm, also known as Hedge[4, 5, 11].

### A gentle start

We begin our discussion with a "stimulus-response" approach in the spirit of Erev & Roth [19]: First, at each stage \(t=1,2,\dots\), every player \(i\in\mathcal{N}\) employs a mixed strategy \(x_{i,t}\in\mathcal{X}_{i}\) to select an action \(\alpha_{i,t}\in\mathcal{A}_{i}\). Subsequently, to measure the performance of their pure strategies over time, each player further maintains a score variable which is updated recursively as

\[y_{i\alpha_{i},t+1}=y_{i\alpha_{i},t}+u_{i}(\alpha_{i};\alpha_{-i,t})\quad \text{for all $\alpha_{i}\in\mathcal{A}_{i}$.}\] (6)

In words, \(y_{i\alpha_{i},t}\) simply tracks the cumulative payoff of the pure strategy \(\alpha_{i}\in\mathcal{A}_{i}\) up to time \(t\) (inclusive).3 As such, this score can be treated as a _propensity_ to play a given pure strategy at any given stage: the strategies \(\alpha_{i}\in\mathcal{A}_{i}\) with the highest propensity scores \(y_{i\alpha_{i},t+1}\) should be played with higher probability at stage \(t+1\).

Footnote 3: Of course, updating these scores requires the knowledge of the “what if” pure payoffs \(u_{i}(\alpha_{i};\alpha_{-i,t})\) at each stage \(t\), but we assume for the moment that this information is available (we will relax this assumption later on).

The most widely used instantiation of this stimulus-response mechanism is the _logit choice_ rule

\[\Lambda_{i}(y_{i})\equiv\frac{(\exp(y_{i\alpha_{i}}))_{\alpha_{i}\in\mathcal{ A}_{i}}}{\sum_{\alpha_{i}\in\mathcal{A}_{i}}\exp(y_{i\alpha_{i}})}\] (7)

which means that each player selects an action with probability that is exponentially proportional to its score. In this way, we obtain the _exponential/multiplicative weights_ - or Hedge - algorithm

\[y_{i,t+1}=y_{i,t}+\gamma_{t}v_{i}(\alpha_{t})\qquad x_{i,t+1}=\Lambda_{i}(y_{i,t+1})\qquad\alpha_{i,t+1}\sim x_{i,t+1}\] (Hedge)

where \(\gamma_{t}\) is the algorithm's "learning rate". For an appeitzer to the literature on (Hedge), see [2, 10, 11, 31, 45] and references therein.

The rest of the methods we discuss below will vary some - or even all - of the components of (Hedge): the information used to update the players' propensity scores, the way that propensity scores are mapped to mixed strategies, and/or even the way that pure actions are selected. However, all of the methods under study will be characterized by the same "stimulus-response" reinforcement mechanism: actions that seem to be performing better over time are employed with higher probability, up to some "regularization" that incentivizes exploration of underperforming actions.

### The regularized learning template

In the rest of our paper, we will work with an abstract _regularized learning_ (RL) template which builds on the same stimulus-response principle as (Hedge), while allowing us to simultaneously consider different types of feedback, strategy sampling policies, update structures, etc. To lighten notation below, we will drop the player index \(i\in\mathcal{N}\) when the meaning can be inferred from the context; also, to stress the distinction between "strategy-like" and "payoff-like" variables, we will write throughout \(\mathcal{Y}_{i}\coloneqq\mathbb{R}^{A_{i}}\) and \(\mathcal{Y}\coloneqq\prod_{i}\mathcal{Y}_{i}\) for the game's "_payoff space_", in direct analogy to \(\mathcal{X}_{i}\) and \(\mathcal{X}=\prod_{i}\mathcal{X}_{i}\) for the game's _strategy space_.

With all this in hand, consider the following general class of regularized learning methods:

\[\begin{array}{ll}\text{Aggregate payoff information (stimulus):}&Y_{i,t+1}=Y_{i,t}+\gamma_{t}\hat{v}_{i,t}\\ \text{Update choice probabilities (response):}&X_{i,t+1}=Q_{i}(Y_{i,t+1})\end{array}\] (RL)

In tune with (Hedge), the various elements of (RL) are defined as follows:

1. \(X_{i,t}\in\mathcal{X}_{i}\) denotes the mixed strategy of player \(i\) at time \(t=1,2,\ldots\)
2. \(Y_{i,t}\in\mathcal{Y}_{i}\) is a "score vector" that measures the performance of the player's actions over time.
3. \(Q_{i}\colon\mathcal{Y}_{i}\to\mathcal{X}_{i}\) is a "regularized choice map" that maps score vectors to choice probabilities.
4. \(\hat{v}_{i,t}\) is a surrogate / approximation of the mixed payoff vector \(v_{i}(X_{t})\) of player \(i\) at time \(t\).
5. \(\gamma_{t}>0\) is a step-size / sensitivity parameter of the form \(\gamma_{t}\propto 1/t^{\gamma_{t}}\) for some \(\ell_{\gamma}\in[0,1]\).

In words, at each stage of the process, every player \(i\in\mathcal{N}\) observes - or otherwise estimates - a proxy \(\hat{v}_{i,t}\) of their individual payoff vector; subsequently, players augment their actions' scores based on this information, they select a mixed strategy via the regularized choice map \(Q_{i}\), and the process repeats. To streamline our presentation, we discuss in detail the precise definition of \(\hat{v}\) and \(Q\) in Sections 3.3 and 3.4 below, and we present a series of examples of (RL) in Section 3.5 right after.

### Aggregating payoff information

As noted above, the main idea of regularized learning is to track the players' payoff vector \(v(X_{t})\). Importantly, there are several different modeling choices that can be made here: players may have direct access to their payoff vectors (in the full information setting), or some noisy approximation obtained by an inner randomization of the algorithm (e.g., when they receive information on their pure actions); they may have to recreate their payoff vectors altogether (as in the bandit setting), or their estimates may be based on a strategy other than the one they actually played (as in the case of optimistic algorithms).

In all cases, we will represent the surrogate payoff vector \(\hat{v}_{t}\) as

\[\hat{v}_{t}=v(X_{t})+U_{t}+b_{t}\] (8)

where \(b_{t}=\mathbf{E}[\hat{v}_{t}\mid\mathcal{F}_{t}]-v(X_{t})\) and \(U_{t}=\hat{v}_{t}-\mathbf{E}[\hat{v}_{t}\mid\mathcal{F}_{t}]\) respectively denote the offset and the random error of \(\hat{v}_{t}\) relative to \(v(X_{t})\). To streamline our presentation, we will also assume that \(\|b_{t}\|=\mathcal{O}(1/t^{\ell_{\sigma}})\) and \(\|U_{t}\|=\mathcal{O}(t^{\ell_{\sigma}})\) for some \(\ell_{b},\ell_{\sigma}\geq 0\); we discuss the specifics of these bounds later in the paper.

### From scores to strategies

Regarding the "scores-to-strategies" step of (RL), we will follow the classical approach of Shalev-Shwartz [45] and assume that each player is employing a _regularized choice map_ of the general form

\[Q_{i}(y_{i})=\arg\max_{x_{i}\in\mathcal{X}_{i}}\{\langle y_{i},x_{i}\rangle-h_ {i}(x_{i})\}\qquad\text{for all $y_{i}\in\mathcal{Y}_{i}$}.\] (9)

In the above, the _regularizer_\(h_{i}\colon\mathcal{X}_{i}\to\mathbb{R}\) acts as a penalty that smooths out the "hard" argmax correspondence \(y_{i}\mapsto\arg\max_{x_{i}\in\mathcal{X}_{i}}\{y_{i},x_{i}\}\). Accordingly, instead of following the "leader" (i.e., playing the strategy with the highest propensity score), players follow the "regularized leader" - that is, they allow for a certain degree of uncertainty in their choice of strategy [10, 33, 45, 46].

To ease notation, we will work with kernelized regularizers of the form \(h_{i}(x_{i})=\sum_{\alpha_{i}\in\mathcal{A}_{i}}\theta(x_{i\alpha_{i}})\) for some continuous function \(\theta\colon[0,1]\to\mathbb{R}\) with \(\inf_{z\in(0,1]}\theta^{\prime\prime}(z)>0\). We will also say that the players' regularizers are _steep_ if \(\lim_{z\to 0^{+}}\theta^{\prime}(z)=-\infty\), and _non-steep_ otherwise.

**Example 3.1**.: A standard family of kernelized regularizers is given by \(\theta(z)=z^{\rho}/\big{[}\rho(\rho-1)\big{]}\) for \(\rho\in(0,1)\cup(1,2]\) and \(\theta(z)=z\log z\) for \(\rho=1\)[10, 31, 33, 53]. This family includes:

* For \(\rho=2\), the quadratic regularizer \(\theta(z)=z^{2}/2\), which yields the Euclidean projection map \[Q_{i}(y_{i})=\Pi_{\mathcal{X}_{i}}(y_{i})\equiv\arg\min_{x_{i}\in\mathcal{X}_{i }}\|y_{i}-x_{i}\|_{2}.\] (10)* For \(\rho=1\), the _entropic regularizer_\(\theta(z)=z\log z\), which yields the logit choice map (7).
* For \(\rho=1/2\), the _fractional power regularizer_\(\theta(z)=-4\sqrt{z}\) that underlies the Tsallis-INF algorithm of [1, 53] (see also Section 3.5 below). \(\blacklozenge\)

### Specific algorithms

We now proceed to discuss some archetypal examples of (RL).

**Algorithm 1** (Follow the regularized leader).: The standard _"follow-the-regularized-leader"_ (FTRL) method of Shalev-Shwartz & Singer [46] is obtained when players observe their full payoff vectors, that is, \(\hat{v}_{i,t}=v_{i}(X_{t})\). In this case, (RL) boils down to the deterministic update rule

\[Y_{i,t+1}=Y_{i,t}+\gamma_{t}v_{i}(X_{t})\qquad X_{i,t+1}=Q_{i}(Y_{i,t+1})\]

or, more explicitly

\[X_{i,t+1}=\arg\max_{x_{i}\in\mathcal{X}_{i}}\left\{\sum_{s=1}^{t}\gamma_{s}u_ {i}(x_{i};X_{-i,s})-h_{i}(x_{i})\right\}\] (FTRL)

For a detailed discussion of (FTRL), see [10, 31, 45]. We only note here that, as a special case, when (FTRL) is run with the logit choice setup of Eq.7, a standard calculation yields the _exponential/multiplicative weights_ (Hedge) [4, 32, 45, 50]. \(\blacklozenge\)

**Algorithm 2** (Optimistic FTRL).: A notable variant of FTRL - originally due to Popov [40] and subsequently popularized by Rakhlin & Sridharan [41, 42] - is the so-called _optimistic FTRL_ method. This scheme employs an "optimistic" correction intended to anticipate future steps, and it updates as

\[Y_{i,t+1}=Y_{i,t}+\gamma_{t}\left[2v_{i}(X_{t})-v_{i}(X_{t-1})\right]\] (Opt-FTRL)

with \(X_{i,t}=Q_{i}(Y_{i,t})\). As a special case, if (Opt-FTRL) is run with the logit choice map (7), we obtain the familiar update rule known as _optimistic multiplicative weights_ (OMW) [15, 41, 42, 47].

Compared to (FTRL), the gain vector \(\hat{v}_{t}=2v(X_{t})-v(X_{t-1})\) of (Opt-FTRL) has offset \(b_{t}=v(X_{t})-v(X_{t-1})\) relative to \(v(X_{t})\). Thus, even though (Opt-FTRL) assumes full access to the players' mixed payoff vectors, it uses this information differently than (FTRL): in particular, the offset of (Opt-FTRL) is non-zero _by design_, not because of some systematic error in the payoff measurement process. \(\blacklozenge\)

Now, up to this point, we have not detailed how players might observe their full, mixed payoff vectors. This assumption simplifies the analysis immensely, but it is not realistic in applications to e.g., online advertising and network science, where players may only be able to observe their realized payoffs, and have no information about the strategies of other players or actions they did not play. On that account, we describe below a range of _payoff-based_ policies where players estimate their counterfactual, "what-if" payoffs _indirectly_.

The most common way to achieve this is via the _importance-weighted estimator_

\[\text{IWE}_{i\,\alpha_{i}}(x)=\frac{\mathds{1}\{\hat{\alpha}_{i}=\alpha_{i}\} }{x_{i\,\alpha_{i}}}u_{i}\left(\hat{\alpha}\right)\quad\text{for all $\alpha_{i}\in \mathcal{A}_{i}$, $i\in\mathcal{N}$},\] (IWE)

where \(x\in\mathcal{X}\) is the players' strategy profile, and \(\hat{\alpha}\in\mathcal{A}\) is drawn according to \(x\). This estimator is at the heart of the online learning literature [10, 11, 31, 45] and it leads to the following methods:

**Algorithm 3** (Bandit FTRL).: Plugging (IWE) directly into (RL) yields the _bandit FTRL_ policy

\[Y_{i,t+1}=Y_{i,t}+\gamma_{t}\,\text{IWE}_{i}(\hat{X}_{t})\qquad X_{i,t+1}=Q_{ i}(Y_{i,t+1})\] (B-FTRL)

where (IWE) is sampled at the mixed strategy profile

\[\hat{X}_{i,t}=(1-\delta_{t})X_{i,t}+\delta_{t}\,\text{unif}_{\mathcal{A}_{i}}\] (11)

for some "explicit exploration" parameter \(\delta_{t}\propto 1/t^{\,\ell_{\delta}}\), \(\ell_{\delta}>0\), which specifies the mix between \(X_{i,t}\) and the uniform distribution \(\text{unif}_{\mathcal{A}_{i}}\) on \(\mathcal{A}_{i}\). As we discuss in the sequel, this combination of (IWE) with the explicit exploration mechanism (11) means that the surrogate payoff vector \(\hat{v}_{t}=\text{IWE}(\hat{X}_{t})\) used to update (B-FTRL) has offset and noise bounded respectively as \(b_{t}=\mathcal{O}(\delta_{t})\) and \(U_{t}=\mathcal{O}(1/\delta_{t})\).

Two special cases of (B-FTRL) that have attracted significant attention in the literature are:

1. The _exponential weights algorithm for exploration and exploitation_ (EXP3) [6, 11, 31], obtained by running (B-FTRL) with the logit choice map (7).

2. The _Tsallis implicitly normalized forecaster_ (Tsallis-INF) [1, 3, 52, 53] that was proposed as a more efficient alternative to EXP3, and which updates as \[X_{i,t}=\arg\max_{x_{i}\in\mathcal{X}_{i}}\left\{\left\langle Y_{i,t},x_{i}\right\rangle +4\sum_{\alpha_{i}\in\mathcal{A}_{i}}\sqrt{x_{i}\alpha_{i}}\right\}\] (Tsallis-INF) i.e., as (B-FTRL) with the fractional power regularizer \(\theta(z)=-4\sqrt{z}\) of Example 3.1. \(\blacklozenge\)

For illustration purposes, we provide some more examples of (RL) in Appendix B.

## 4 First results: resilience to strategic deviations

We are now in a position to begin our analysis of the rationality properties of the players' long-run behavior under (RL). To that end, we should first note that no-regret play may _still_ lead to counterintuitive and highly non-rationalizable outcomes, e.g., with all players selecting dominated strategies for all time. The example below is adapted from Viossat & Zapechelnyuk [49].

**Example 4.1**.: Consider the \(4\times 4\) symmetric 2-player game with payoff bimatrix

\begin{tabular}{c|c c c c}  & \(A\) & \(B\) & \(C\) & \(D\) \\ \hline \(A\) & \((1,1)\) & \((1,2/3)\) & \((0,0)\) & \((0,-1/3)\) \\ \(B\) & \((2/3,1)\) & \((2/3,2/3)\) & \((-1/3,0)\) & \((-1/3,-1/3)\) \\ \(C\) & \((0,0)\) & \((0,-1/3)\) & \((1,1)\) & \((1,2/3)\) \\ \(D\) & \((-1/3,0)\) & \((-1/3,-1/3)\) & \((2/3,1)\) & \((2/3,2/3)\) \\ \end{tabular} In this game, \(B\) and \(D\) are strictly dominated for both players by their stronger "twins" (\(A\) and \(C\) respectively). However, it is easy to check that if both players choose between \((B,B)\) and \((D,D)\) with probability \(1/2\) each, the resulting distribution of play \(z\in\Delta(\mathcal{A})\) satisfies \(u_{i}(\alpha_{i};z_{-i})-u_{i}(z)\leq-1/6\) for all \(\alpha_{i}\in\{A,B,C,D\}\), \(i=1,2\). As a result, the players' regret under \(z_{t}\equiv z\) is _negative_, even though both players play strictly dominated strategies at all times. \(\blacklozenge\)

The example above shows unequivocally that

_No-regret play does not suffice to exclude non-rationalizable outcomes._

In addition, Example 4.1 also shows that predictions based on correlated play are not always appropriate for describing the players' behavior under (RL): the end-state of any regularized learning algorithm will be a closed connected set of mixed strategies, so it is not possible to play _only_\((B,B)\) or \((D,D)\) in the long run. We are thus led to the following natural questions: _What are the rationality properties of long-run play under (RL)? Is the players' behavior robust to strategic deviations?_

To study these questions formally, we will focus on the _limit set_\(\mathcal{L}(X)\) of \(X_{t}\) under (RL), viz.

\[\mathcal{L}(X)\coloneqq\bigcap_{t}\operatorname{cl}\{X_{s}:s\geq t\}\equiv \{\hat{x}\in\mathcal{X}:X_{t_{k}}\to\hat{x}\text{ for some subsequence }X_{t_{k}}\text{ of }X_{t}\}.\] (12)

In words, \(\mathcal{L}(X)\) is the set of limit points of \(X_{t}\) or, equivalently, the _smallest_ subset of \(\mathcal{X}\) to which \(X_{t}\) converges. Clearly, the simplest instance of a limit set is when \(\mathcal{L}(X)\) is a singleton, i.e., when \(X_{t}\) converges to a point. This case has attracted significant interest in the literature: for example, if \(\mathcal{L}(X)=\{x^{*}\}\) then, for certain special cases of (RL), it is known that \(x^{*}\) is a Nash equilibrium of \(\Gamma\)[34]. However, beyond this relatively simple regime, the structure of the limit sets of (RL) could be arbitrarily complicated and their rationality properties are not well-understood.

With this in mind, as a first attempt to study whether the long-run behavior of (RL) is "robust to strategic deviations", we will consider the following notion of _resilience to strategic deviations_:

**Definition 1**.: A closed subset \(\mathcal{S}\) of \(\mathcal{X}\) is _resilient to strategic deviations_ - or simply _resilient_ - if, for every deviation \(x_{i}\in\mathcal{X}_{i}\) of every player \(i\in\mathcal{N}\), we have

\[u_{i}(x^{*})\geq u_{i}(x_{i},x_{-i}^{*})\quad\text{for some }x^{*}\in \mathcal{S}.\] (13)

Informally, \(\mathcal{S}\) is resilient if every unilateral deviation from \(\mathcal{S}\) is deterred by some (possibly different) element thereof. In particular, if \(\mathcal{S}\) is a singleton, we immediately recover the definition of a Nash equilibrium; beyond this case however, other examples include the set of undominated strategies of a game, the support face of the equilibria of two-player zero-sum games, etc. Importantly, as we show below, the limit sets of (RL) are resilient _in all games:_

**Theorem 1**.: _Let \(X_{t}\), \(t=1,2,\dots\), be the sequence of play generated by (RL) with step-size/gain parameters \(\ell_{\gamma}>2\ell_{\sigma}\) and \(\ell_{b}>0\). Then, with probability \(1\), the limit set \(\mathcal{L}(X)\) of \(X_{t}\) is resilient._

Proof sketch.: The proof of Theorem 1 boils down to two interleaved arguments that we detail in Appendix C. The first hinges on showing that, if \(\mathbf{P}(\mathcal{L}(X)=S)>0\) for some _non-random_\(\mathcal{S}\subseteq\mathcal{X}\), \(\mathcal{S}\) must be resilient. This is argued by contradiction: if \(p_{i}\in\mathcal{X}_{t}\) is a unilateral deviation violating Definition 1, we must also have \(\liminf_{t\to\infty}[u_{i}(p_{i};X_{-i,t})-u_{i}(X_{t})]>0\) with positive probability. However, the existence of a strategy that consistently outperforms \(X_{t}\) runs contrary to the fact that strategies that (RL) selects against underperforming strategies. We make this intuition precise via an energy argument that leverages a series of results from martingale limit theory (which is where the requirements for \(\gamma_{t}\), \(b_{t}\) and \(U_{t}\) come in). Then, to get the stronger statement that the _random_ set \(\mathcal{L}(X)\) is resilient w.p.1, we show that the above remains true if \(p_{i}\) is replaced by a deviation \(q_{i}\) which is close enough to \(p_{i}\) and has _rational_ entries. Since there is a countable number of such profiles, we can use a union bound on an enumeration of the rationals to isolate a deviation witnessing the negation of Definition 1; our claim then follows by applying our argument for non-random sets. 

Theorem 1 is our first universal guarantee for (RL), so some remarks are in order. First, we should point out that the requirements \(\ell_{b}>0\) and \(2\ell_{\sigma}<\ell_{\gamma}\) are a priori _implicit_ because they depend on the offset and magnitude statistics of the feedback sequence \(\hat{v}_{t}\). However, in most learning algorithms, these quantities are under the _explicit_ control of the players: for example, as we show in Appendix B, Algorithm 2 has \(\ell_{b}=\ell_{\gamma}\) while, for Algorithm 3, we have \(\ell_{b}=\ell_{\sigma}=\ell_{\delta}\). In this way, when instantiated to Algorithms 1-3 (and special cases thereof), Theorem 1 yields the following corollary:

**Corollary 1**.: _Suppose that Algorithms 1-3 are run with \(\ell_{\gamma}\in(0,1]\) and, for Algorithm 3, \(\ell_{\delta}\in(0,\ell_{\gamma}/2)\). Then, with probability \(1\), the limit set \(\mathcal{L}(X)\) of \(X_{t}\) is resilient._

Now, since Theorem 1 applies to all games, it would seem to provide a universally positive answer to whether (RL) is robust to strategic deviations. However, this is not so: a direct calculation shows that the face of \(\mathcal{X}\) that is spanned by the dominated strategies \((B,B)\) and \((D,D)\) of Example 4.1_is_ resilient, so Theorem 1 cannot exclude convergence to a set where dominated strategies survive. Thus, just like no-regret play, the notion of resilience does not suffice by itself to capture the idea of rational behavior. This is because, albeit natural, resilience is too lax to provide a meaningful link between robustness to unilateral deviations - a _game-theoretic_ requirement - and stability under regularized learning - a _dynamic_ requirement. We address this question in detail in the next section.

## 5 A characterization of strategic stability under regularized learning

Similar to the set of pure strategies that arise from no-regret play, the main limitation of resilience is that a payoff-improving deviation may be countered by an action profile where the deviator also switched to a _different_ strategy; in other words, resilience is not a _self-enforcing_ barrier to deviations. In view of this, we will focus below on a much more stringent criterion of strategic stability, namely that _any_ deviation from the set in question incurs a cost to the deviating agent.

Club sets.To make all this precise, define the _better-reply correspondence_ of player \(i\in\mathcal{N}\) as

\[\mathtt{btr}_{i}(x)=\{x_{i}^{\prime}\in\mathcal{X}_{i}:u_{i}(x_{i}^{\prime};x _{-i})\geq u_{i}(x)\}\] (14)

and write \(\mathtt{btr}=\prod_{i}\mathtt{btr}_{i}\) for the product correspondence \(\mathtt{btr}(x)=\mathtt{btr}_{1}(x)\times\dots\times\mathtt{btr}_{N}(x)\). [In words, \(\mathtt{btr}_{i}\) assigns to each \(x\in\mathcal{X}\) those strategies of player \(i\) that are (weakly) better against \(x\) than \(x_{i}\).] In addition, given a product of pure strategies \(\mathcal{C}=\prod_{i\in\mathcal{N}}\mathcal{C}_{i}\) with \(\mathcal{C}_{i}\subseteq\mathcal{A}_{i}\) for all \(i\in\mathcal{N}\), let \(\mathcal{S}=\Delta(\mathcal{C})\) denote the span of \(\mathcal{C}\), and let \(\mathcal{P}(\mathcal{X})\) denote the collection of all such sets. We then say that \(\mathcal{S}\in\mathcal{P}(\mathcal{X})\) is _closed under better replies_ - a _club set_ for short - if it is closed under \(\mathtt{btr}\), i.e., \(\mathtt{btr}(\mathcal{S})\subseteq\mathcal{S}\); finally, \(\mathcal{S}\) is said to be _minimally club_ (m-club) if it does not admit a proper club subset.

Of course, the entire strategy space \(\mathcal{X}\) is closed under better replies so, a priori, club sets could also contain dominated strategies and / or other non-rationalizable outcomes. By contrast, _minimal_ club sets are much more rigid in their relation to rational behavior because any unilateral deviation from an m-club set is _costly_, and m-club sets are _minimal_ in this regard. On that account, m-club sets can be seen as _the closest setwise analogue to strict Nash equilibria._

This analogy is accentuated further by the following properties of m-club sets, all due to Ritzberger & Weibull [43], who introduced the concept:1. Every game admits an m-club set; and if this set is a singleton, then it is a _strict_ Nash equilibrium.
2. Any m-club set \(\mathcal{S}\) is _fixed_ under better replies, that is, \(\mathsf{btr}(\mathcal{S})=\mathcal{S}\) (implying in turn that \(\mathcal{S}\) cannot contain any dominated strategies, including iteratively dominated ones).
3. Any m-club set \(\mathcal{S}\) contains an _essential equilibrium component_, i.e., a component of Nash equilibria such that every small perturbation of the game admits a nearby equilibrium; in addition, this component has _full support_ on \(\mathcal{S}\), i.e., it employs all pure strategy profiles that lie in \(\mathcal{S}\).4 Footnote 4: Formally, a component \(\mathcal{X}^{*}\) of Nash equilibria of \(\Gamma\) is _essential_ if, for all \(\varepsilon>0\), there exists \(\delta>0\) such that any perturbation of the payoffs of \(\Gamma\) by at most \(\delta\) produces a Nash equilibrium that is \(\varepsilon\)-close to \(\mathcal{X}^{*}\)[48].

Going back to our online learning setting, the above leads to the following natural set of questions:

_Are club sets (minimal or not) stable under the dynamics of regularized learning?_

_Are they attracting? And, if so, are they the only such sets?_

Any answer to these questions - positive or negative - would be an important step in delineating the relation between _strategic stability_ (in the above sense) and _dynamic stability_ under (RL). To that end, we start by formalizing some notions of dynamic stability that will be central in the sequel:

**Definition 2**.: Fix some subset \(\mathcal{S}\) of \(\mathcal{X}\) and a tolerance level \(\epsilon>0\). We then say that \(\mathcal{S}\) is:

1. _Stochastically stable_ if, for every neighborhood \(\mathcal{U}\) of \(\mathcal{S}\) in \(\mathcal{X}\), there exists a neighborhood \(\mathcal{U}_{1}\) of \(\mathcal{S}\) such that \[\mathbf{P}(X_{t}\in\mathcal{U}\text{ for all }t=1,2,\dots)\geq 1-\epsilon\quad \text{whenever }X_{1}\in\mathcal{U}_{1}.\] (15)
2. _Stochastically attracting_ if there exists a neighborhood \(\mathcal{U}_{1}\) of \(\mathcal{S}\) such that \[\mathbf{P}(\lim_{t\to\infty}\operatorname{dist}(X_{t},\mathcal{S})=0)\geq 1- \epsilon\quad\text{whenever }X_{1}\in\mathcal{U}_{1}.\] (16)
3. _Stochastically asymptotically stable_ if it is stochastically stable and attracting.
4. _Irreducibly stable_ if \(\mathcal{S}\) is stochastically asymptotically stable and it does not admit a strictly smaller stochastically asymptotically subset \(\mathcal{S}^{\prime}\) with \(\operatorname{supp}(\mathcal{S}^{\prime})\subsetneq\operatorname{supp}( \mathcal{S})\).

With all this in hand, our main result below provides a sharp characterization of strategic stability in the context of regularized learning:

**Theorem 2**.: _Fix some set \(\mathcal{S}\in\mathcal{P}(\mathcal{X})\) and suppose that (RL) is run with a steep regularizer and step-size/gain parameters \(\ell_{\gamma}\in[0,1]\), \(\ell_{b}>0\), and \(\ell_{\sigma}<1/2\). Then:_

1. \(\mathcal{S}\) _is stochastically asymptotically stable under (_RL_) if and only if it is a club set._
2. \(\mathcal{S}\) _is irreducibly stable under (_RL_) if and only if it is an m-club set._

In addition, we also get the following convergence rate estimates for club sets:

**Theorem 3**.: _Let \(\mathcal{S}\in\mathcal{P}(\mathcal{X})\) be a club set, and let \(X_{t}\), \(t=1,2,\dots\), be the sequence of play generated by (RL) with parameters \(\ell_{\gamma}\in[0,1]\), \(\ell_{b}>0\), and \(\ell_{\sigma}<1/2\). Then, for all \(\epsilon>0\), there exists an (open, unbounded) initialization domain \(\mathcal{D}\subseteq\mathcal{Y}\) such that, with probability at least \(1-\epsilon\), we have_

\[\operatorname{dist}(X_{t},\mathcal{S})\leq C\varphi\Big{(}c_{1}-c_{2}\sum_{s= 1}^{t}\gamma_{s}\Big{)}\quad\text{whenever }Y_{1}\in\mathcal{D}\] (17)

_where \(C,c_{1},c_{2}\) are constants (\(C,c_{2}>0\)), and the rate function \(\varphi\) is given by \(\varphi(z)=(\theta^{\prime})^{-1}(z)\) if \(z>\lim_{z\to 0^{\prime}}\theta^{\prime}(z)\), and \(\varphi(z)=0\) otherwise._

Specifically, if we instantiate Theorem 3 to Algorithms 1-3, we get the explicit estimates:

**Corollary 2**.: _Suppose that Algorithms 1-3 are run with \(\ell_{\gamma}\in[0,1]\) and, for Algorithm 3, \(\ell_{\delta}\in(0,1/2)\). Then, with notation as in Theorem 3, \(X_{t}\) converges to \(\mathcal{S}\) at a rate of_

\[\operatorname{dist}(X_{t},\mathcal{S})\leq C\cdot\begin{cases}\big{[}1-c\sum_{ s=1}^{t}\gamma_{s}\big{]}_{+}&\text{ if }\theta(z)=z^{2}/2\qquad\text{\# quadratic regularization}\\ \exp\bigl{(}-c\sum_{s=1}^{t}\gamma_{s}\big{)}&\text{ if }\theta(z)=z\log z \qquad\text{\# entropic regularization}\\ 1\big{/}\bigl{(}c+\sum_{s=1}^{t}\gamma_{s}\big{)}^{2}&\text{ if }\theta(z)=-4\sqrt{z} \qquad\text{\# fractional regularization}\end{cases}\] (18)

_for positive constants \(C,c>0\). In particular, the projection-based variants of Algorithms 1-3 converge to m-club sets in a **finite** number of steps._rood sketch._ The proof of Theorems 2 and 3 is quite involved so we defer it to Appendix D. At a high level, it hinges on constructing a family of "primal-dual" energy functions, one per pure deviation from the set \(\mathcal{S}\) under study. If unilateral deviations from \(\mathcal{S}\) incur a cost to the deviator (that is, if \(\mathcal{S}\) is club), these energy functions can be "bundled together" to produce a suitable Lyapunov-like function for \(\mathcal{S}\). In more detail, the minimization of each individual energy function implies that the score variable \(Y_{t}\) of (RL) diverges along an "astral direction" in the payoff space \(\mathcal{Y}\) - i.e., it escapes to infinity along the interior of a certain convex cone of \(\mathcal{Y}\)[18]. Because this minimization occurs at infinity, the aggregation of offsets and random errors in (RL) affords some extra "wiggle room" in our martingale analysis, so we are able to show that \(X_{t}=Q(Y_{t})\) remains close to \(\mathcal{S}\) under a much wider range of parameters compared to Theorem 1. Then, a series of convex analysis arguments in the spirit of [33] coupled with the definition of \(Q\) allows us to show that the escape of \(Y_{t}\) along the intersection of all these cones implies convergence to \(\mathcal{S}\) at the specified rate.

On the converse side, if an asymptotically stable set is not club, we can find a non-costly (and possibly profitable) deviation \(z\) from \(\mathcal{S}\) which is selected against by (RL). However, this extinction runs contrary to the reinforcement of better replies under (RL), an argument which can be made precise by applying the martingale law of large numbers to \(\langle Y_{t},z\rangle\)[25]. The irreducible stability of m-club sets then follows by invoking this criterion reductively for any potentially stable subset \(\mathcal{S}^{\prime}\) of \(\mathcal{S}\). 

**Discussion and remarks.** Theorems 2 and 3 are our main results linking dynamic and strategic stability, so we conclude with a series of remarks. First, we should note that Theorem 2 can be summed up as follows: _a product of pure strategies is (minimally) closed under better replies if and only if its span is (irreducibly) stable under regularized learning._ Importantly, this equivalence is based solely on the game's payoff data: it does not depend on the specific choices underlying (RL), including the choice map employed by each player, whether some players are using an optimistic adjustment or not, if they have access to their full payoff vectors, etc. As such, this equivalence provides a crisp operational criterion for identifying which pure strategy combinations ultimately persist under regularized learning - and, via Theorem 3, _how fast_ this identification takes place.

In this light, Theorem 2 essentially states that the only robust prediction that can be made for the outcome of a regularized learning process is (minimal) closedness under better replies. This interpretation has significant cutting power for the emergence of rational behavior. To begin, in terms of equilibrium play, it effortlessly implies that a pure strategy profile is stochastically asymptotically stable under (RL) if and only if it is a strict Nash equilibrium. A version of this equivalence was only recently proved in [20] and [22] (in continuous and discrete time respectively), so Theorem 2 can be seen as a far-reaching generalization of these recent results. More to the point, since every m-club set \(\mathcal{S}\) contains an essential equilibrium component that is fully supported in \(\mathcal{S}\), Theorem 2 also provides an important link between dynamic and structural stability: if an equilibrium - or a component of equilibria - is not robust to perturbations of the underlying game, _it cannot be robustly identified by a regularized learning process_ (and vice versa). This remark is of particular importance for extensive-form games as such games often have non-generic equilibrium components that cannot be treated otherwise by the existing theory.

Finally, we should stress that Theorems 2 and 3 guarantee convergence even with a constant step-size. Together with the finite-time convergence guarantees of Corollary 2 for projection-based methods, this feature is a testament to the robustness of club sets as, in the presence of uncertainty, convergence almost always requires a vanishing step-size which can slow convergence down to a crawl. We find this robust convergence landscape particularly intriguing for future research on the topic.

Figure 1: The long-run behavior of EXP3 (Algorithm 3) in four representative \(2\times 2\times 2\) games. In all cases, the dynamics converge to m-club sets, either _strict equilibria_ themselves, or spanning an _essential component_ of Nash equilibria. The details of the numerics and the games being played are provided in the appendix.

## Acknowledgments and Disclosure of Funding

PM is also with the Archimedes Research Unit - Athena RC - Department of Mathematics, National & Kapodistrian University of Athens. This work has been partially supported by the French National Research Agency (ANR) in the framework of the "Investissements d'avenir" program (ANR-15-IDEX-02), the LabEx PERSYVAL (ANR-11-LABX-0025-01), MIAI@Grenoble Alpes (ANR-19-P3IA-0003), and project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program

## References

* [1] Abernethy, J., Lee, C., and Tewari, A. Fighting bandits with a new kind of smoothness. In _NIPS '15: Proceedings of the 29th International Conference on Neural Information Processing Systems_, 2015.
* [2] Arora, S., Hazan, E., and Kale, S. The multiplicative weights update method: A meta-algorithm and applications. _Theory of Computing_, 8(1):121-164, 2012.
* [3] Audibert, J.-Y. and Bubeck, S. Regret bounds and minimax policies under partial monitoring. _Journal of Machine Learning Research_, 11:2635-2686, 2010.
* [4] Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In _Proceedings of the 36th Annual Symposium on Foundations of Computer Science_, 1995.
* [5] Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. _Machine Learning_, 47:235-256, 2002.
* [6] Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. The nonstochastic multiarmed bandit problem. _SIAM Journal on Computing_, 32(1):48-77, 2002.
* [7] Basu, K. and Weibull, J. W. Strategy subsets closed under rational behavior. _Economics Letters_, 36:141-146, 1991.
* [8] Bauschke, H. H., Borwein, J. M., and Combettes, P. L. Bregman monotone optimization algorithms. _SIAM Journal on Control and Optimization_, 42(2):596-636, 2003.
* [9] Benaim, M. Dynamics of stochastic approximation algorithms. In Azema, J., Emery, M., Ledoux, M., and Yor, M. (eds.), _Seminarie de Probabilites XXXIII_, volume 1709 of _Lecture Notes in Mathematics_, pp. 1-68. Springer Berlin Heidelberg, 1999.
* [10] Bubeck, S. and Cesa-Bianchi, N. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends in Machine Learning_, 5(1):1-122, 2012.
* [11] Cesa-Bianchi, N. and Lugosi, G. _Prediction, Learning, and Games_. Cambridge University Press, 2006.
* [12] Chotibut, T., Falniowski, F., Misiurewicz, M., and Piliouras, G. Family of chaotic maps from game theory. _Dynamical Systems_, 2020.
* [13] Coucheney, P., Gaujal, B., and Mertikopoulos, P. Penalty-regulated dynamics and robust learning procedures in games. _Mathematics of Operations Research_, 40(3):611-633, August 2015.
* [14] Cressman, R. _Evolutionary Dynamics and Extensive Form Games_. The MIT Press, 2003.
* [15] Daskalakis, C. and Panageas, I. Last-iterate convergence: Zero-sum games and constrained min-max optimization. In _ITCS '19: Proceedings of the 10th Conference on Innovations in Theoretical Computer Science_, 2019.
* [16] Daskalakis, C., Ilyas, A., Syrgkanis, V., and Zeng, H. Training GANs with optimism. In _ICLR '18: Proceedings of the 2018 International Conference on Learning Representations_, 2018.
* [17] Demichelis, S. and Ritzberger, K. From evolutionary to strategic stability. _Journal of Economic Theory_, 113:51-75, 2003.
* [18] Dudik, M., Schapire, R. E., and Telgarsky, M. Convex analysis at infinity: An introduction to astral space. https://arxiv.org/abs/2205.03260, 2022.
* [19] Erev, I. and Roth, A. E. Predicting how people play games: Reinforcement learning in experimental games with unique, mixed strategy equilibria. _American Economic Review_, 88:848-881, 1998.
* [20] Flokas, L., Vlatakis-Gkaragkounis, E. V., Laineas, T., Mertikopoulos, P., and Piliouras, G. No-regret learning and mixed Nash equilibria: They do not mix. In _NeurIPS '20: Proceedings of the 34th International Conference on Neural Information Processing Systems_, 2020.
* [21] Fudenberg, D. and Tirole, J. _Game Theory_. The MIT Press, 1991.
* [22] Giannou, A., Vlatakis-Gkaragkounis, E. V., and Mertikopoulos, P. Survival of the strictest: Stable and unstable equilibria under regularized learning with partial information. In _COLT '21: Proceedings of the 34th Annual Conference on Learning Theory_, 2021.

* [23] Giannou, A., Vlatakis-Gharagkounis, E. V., and Mertikopoulos, P. The convergence rate of regularized learning in games: From bandits and uncertainty to optimism and beyond. In _NeurIPS '21: Proceedings of the 35th International Conference on Neural Information Processing Systems_, 2021.
* [24] Gidel, G., Berard, H., Vignoud, G., Vincent, P., and Lacoste-Julien, S. A variational inequality perspective on generative adversarial networks. In _ICLR '19: Proceedings of the 2019 International Conference on Learning Representations_, 2019.
* [25] Hall, P. and Heyde, C. C. _Martingale Limit Theory and Its Application_. Probability and Mathematical Statistics. Academic Press, New York, 1980.
* [26] Hannan, J. Approximation to Bayes risk in repeated play. In Dresher, M., Tucker, A. W., and Wolfe, P. (eds.), _Contributions to the Theory of Games, Volume III_, volume 39 of _Annals of Mathematics Studies_, pp. 97-139. Princeton University Press, Princeton, NJ, 1957.
* [27] Hofbauer, J. and Sigmund, K. Evolutionary game dynamics. _Bulletin of the American Mathematical Society_, 40(4):479-519, July 2003.
* [28] Hsieh, Y.-G., Iutzeler, F., Malick, J., and Mertikopoulos, P. On the convergence of single-call stochastic extra-gradient methods. In _NeurIPS '19: Proceedings of the 33rd International Conference on Neural Information Processing Systems_, pp. 6936-6946, 2019.
* [29] Juditsky, A., Nemirovski, A. S., and Tauvel, C. Solving variational inequalities with stochastic mirror-prox algorithm. _Stochastic Systems_, 1(1):17-58, 2011.
* [30] Kohlberg, E. and Mertens, J.-F. On the strategic stability of equilibria. _Econometrica_, 54(5):1003-1037, September 1986.
* [31] Lattimore, T. and Szepesvari, C. _Bandit Algorithms_. Cambridge University Press, Cambridge, UK, 2020.
* [32] Littlestone, N. and Warmuth, M. K. The weighted majority algorithm. _Information and Computation_, 108 (2):212-261, 1994.
* [33] Mertikopoulos, P. and Sandholm, W. H. Learning in games via reinforcement and regularization. _Mathematics of Operations Research_, 41(4):1297-1324, November 2016.
* [34] Mertikopoulos, P. and Zhou, Z. Learning in games with continuous action sets and unknown payoff functions. _Mathematical Programming_, 173(1-2):465-507, January 2019.
* [35] Mertikopoulos, P., Papadimitriou, C. H., and Piliouras, G. Cycles in adversarial regularized learning. In _SODA '18: Proceedings of the 29th annual ACM-SIAM Symposium on Discrete Algorithms_, 2018.
* [36] Mertikopoulos, P., Lecouat, B., Zenati, H., Foo, C.-S., Chandrasekhar, V., and Piliouras, G. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In _ICLR '19: Proceedings of the 2019 International Conference on Learning Representations_, 2019.
* [37] Nemirovski, A. S. Prox-method with rate of convergence \(O(1/t)\) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. _SIAM Journal on Optimization_, 15(1):229-251, 2004.
* [38] Palaiopanos, G., Panageas, I., and Piliouras, G. Multiplicative weights update with constant step-size in congestion games: Convergence, limit cycles and chaos. In _NIPS '17: Proceedings of the 31st International Conference on Neural Information Processing Systems_, 2017.
* [39] Piliouras, G., Sim, R., and Skoulakis, S. Optimal no-regret learning in general games: Bounded regret with unbounded step-sizes via clairvoyant mwu. https://arxiv.org/abs/2111.14737, 2021.
* [40] Popov, L. D. A modification of the Arrow-Hurwicz method for search of saddle points. _Mathematical Notes of the Academy of Sciences of the USSR_, 28(5):845-848, 1980.
* [41] Rakhlin, A. and Sridharan, K. Online learning with predictable sequences. In _COLT '13: Proceedings of the 26th Annual Conference on Learning Theory_, 2013.
* [42] Rakhlin, A. and Sridharan, K. Optimization, learning, and games with predictable sequences. In _NIPS '13: Proceedings of the 27th International Conference on Neural Information Processing Systems_, 2013.
* [43] Ritzberger, K. and Weibull, J. W. Evolutionary selection in normal-form games. _Econometrica_, 63(6):1371-99, November 1995.
* [44] Rockafellar, R. T. and Wets, R. J. B. _Variational Analysis_, volume 317 of _A Series of Comprehensive Studies in Mathematics_. Springer-Verlag, Berlin, 1998.
* [45] Shalev-Shwartz, S. Online learning and online convex optimization. _Foundations and Trends in Machine Learning_, 4(2):107-194, 2011.
* [46] Shalev-Shwartz, S. and Singer, Y. Convex repeated games and Fenchel duality. In _NIPS '06: Proceedings of the 19th Annual Conference on Neural Information Processing Systems_, pp. 1265-1272. MIT Press, 2006.
* [47] Syrgkanis, V., Agarwal, A., Luo, H., and Schapire, R. E. Fast convergence of regularized learning in games. In _NIPS '15: Proceedings of the 29th International Conference on Neural Information Processing Systems_, pp. 2989-2997, 2015.

* [48] van Damme, E. _Stability and perfection of Nash equilibria_. Springer-Verlag, Berlin, 1987.
* [49] Viossat, Y. and Zapechelnyuk, A. No-regret dynamics and fictitious play. _Journal of Economic Theory_, 148(2):825-842, March 2013.
* [50] Vovk, V. G. Aggregating strategies. In _COLT '90: Proceedings of the 3rd Workshop on Computational Learning Theory_, pp. 371-383, 1990.
* [51] Weibull, J. W. _Evolutionary Game Theory_. MIT Press, Cambridge, MA, 1995.
* [52] Zimmert, J. and Seldin, Y. An optimal algorithm for stochastic and adversarial bandits. In _AISTATS '19: Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics_, 2019.
* [53] Zimmert, J. and Seldin, Y. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. _Journal of Machine Learning Research_, 22(28):1-49, 2021.

Auxiliary results

In this appendix we collect some basic properties of the regularized choice maps and some results from probability theory that will be useful in the sequel.

### Regularized choice maps and their properties

Thoughout this appendix, we will suppress the player index \(i\in\mathcal{N}\), and we will follow standard conventions in convex analysis [44] that treat \(h\) as an extended-real-valued function \(h\colon\mathcal{V}\to\mathds{R}\cup\{\infty\}\) with \(h(x)=\infty\) for all \(x\in\mathcal{V}\setminus\mathcal{X}\). With this in mind, the subdifferential of a \(h\) at \(x\in\mathcal{X}\) is defined as

\[\partial h(x)\coloneqq\{y\in\mathcal{Y}:h(x^{\prime})\geq h(x)+\langle y,x^{ \prime}-x\rangle\text{ for all }x^{\prime}\in\mathcal{X}\},\] (A.1)

where \(\mathcal{Y}\) denotes here the algebraic dual \(\mathcal{V}\) of \(\mathcal{V}\). Accordingly, the _domain of subdifferentiability_ of \(h\) is \(\operatorname{dom}\partial h\coloneqq\{x\in\operatorname{dom}h:\partial h \neq\varnothing\}\), and the convex conjugate of \(h\) is defined as

\[h^{*}(y)=\max_{x\in\mathcal{X}}\{\langle y,x\rangle-h(x)\}\] (A.2)

for all \(y\in\mathcal{Y}\). We then have the following basic results.

**Lemma A.1**.: _Let \(h\) be a regularizer on \(\mathcal{X}\), and let \(Q\colon\mathcal{Y}\to\mathcal{X}\) be the induced choice map. Then:_

1. \(Q\) _is single-valued, and, for all_ \(x\in\mathcal{X}\)_,_ \(y\in\mathcal{Y}\)_, we have_ \(x=Q(y)\iff y\in\partial h(x)\)_._
2. _For all_ \(x\in\operatorname{ri}\mathcal{X}\)_, we have_ \(\partial h(x)=\{(\theta^{\prime}(x_{\alpha})+\mu)_{\alpha\in\mathcal{A}}:\mu \in\mathds{R}\}\)_._
3. _For all_ \(y\in\mathcal{Y}\)_, we have_ \(Q(y)=\nabla h^{*}(y)\)_._
4. \(Q\) _is_ \((1/K)\)_-Lipschitz continuous with_ \(K\coloneqq\inf_{(0,1]}\theta^{\prime\prime}(z)\)_. In particular, as a special case, the logit choice map_ \(\Lambda\) _is_ \(1\)_-Lipschitz continuous in the_ \((L^{1},L^{\infty})\) _pair of norms on_ \(\mathcal{Y}\) _and_ \(\mathcal{X}\) _respectively._
5. _If_ \(y_{\alpha}-y_{\alpha^{\prime}}\to-\infty\) _for some_ \(\alpha^{\prime}\neq\alpha\)_, then_ \(Q_{\alpha}(y)\to 0\)_._

_Remark_.: Some of the properties presented in Lemma A.1 are well known in the literature on regularized learning methods (see e.g., [33] and references therein), but we provide a proof of the entire lemma for completeness. 

Proof of Lemma a.1.: For the first property of \(Q\), note that the maximum in (9) is attained for all \(y\in\mathcal{Y}\) because \(h\) is lower-semicontinuous (l.s.c.) and strongly convex. Furthermore, \(x\) solves (9) if and only if \(y-\partial h(x)\ni 0\), i.e., if and only if \(y\in\partial h(x)\).

For our second claim, if \(x\in\operatorname{ri}(\mathcal{X})\), the first-order stationarity conditions for the convex problem (9) that defines \(Q\) become

\[y_{\alpha}-\theta^{\prime}(x_{\alpha})=\mu\quad\text{for all }\alpha\in \mathcal{A},\] (A.3)

because the inequality constraints \(x_{\alpha}\geq 0\) are all inactive (recall that \(x\in\operatorname{ri}(\mathcal{X})\) by assumption). Now, by the first part of the theorem we have \(x=Q(y)\) if and only if \(y\in\partial h(x)\), so we conclude that \(\partial h(x)=\{(\theta^{\prime}(x_{\alpha})+\mu)_{\alpha\in\mathcal{A}}:\mu \in\mathds{R}\}\), as claimed.

For the fourth item, the expression \(Q=\nabla h^{*}\) is an immediate consequence of Danskin's theorem, while the Lipschitz continuity of \(Q\) follows from standard results, see e.g., [44, Theorem 12.60(b)].

For our last claim, let \(y_{t}\) be a sequence in \(\mathcal{Y}\) such that \(y_{\alpha,t}-y_{\alpha^{\prime},t}\to-\infty\) and let \(x_{t}=Q(y_{t})\). Then, by descending to a subsequence if necessary, assume there exists some \(\varepsilon>0\) such that \(x_{\alpha,t}\geq\varepsilon>0\) for all \(t\). Then, by the defining relation \(Q(y)=\arg\max\{\langle y,x\rangle-h(x)\}\) of \(Q\), we have:

\[\langle y_{t},x_{t}\rangle-h(x_{t})\geq\langle y_{t},x^{\prime}\rangle-h(x^{ \prime})\] (A.4)

for all \(x^{\prime}\in\mathcal{X}\). Therefore, taking \(x^{\prime}_{t}=x_{t}+\varepsilon(e_{\alpha^{\prime}}-e_{\alpha})\), we readily obtain

\[\varepsilon(y_{\alpha,t}-y_{\alpha^{\prime},t})\geq h(x_{t})-h(x^{\prime}_{t} )\geq\min h-\max h\] (A.5)

which contradicts our original assumption that \(y_{\alpha,t}-y_{\alpha^{\prime},t}\to-\infty\). With \(\mathcal{X}\) compact, the above shows that \(x^{*}_{\alpha}=0\) for any limit point \(x^{*}\) of \(x_{t}\), i.e. \(Q_{\alpha}(y_{t})\to 0\). 

The second collection of results concerns the _Fenchel coupling_, an energy function that was first introduced in [33, 34] and is defined as follows:

\[F(p,y)=h(p)+h^{*}(y)-\langle y,p\rangle\quad\text{for all }p\in\mathcal{X} \text{ and }y\in\mathcal{Y}.\] (A.6)

This coupling will play a major role in the proofs of Theorem 1, so we prove two of its most basic properties below.

**Lemma A.2**.: _For all \(p\in\mathcal{X}\) and all \(y,y^{\prime}\in\mathcal{Y}\), we have:_

\[a)\quad F(p,y)\geq\tfrac{1}{2}K\left\|Q(y)-p\right\|^{2}.\] (A.7a) \[b)\quad F(p,y^{\prime})\leq F(p,y)+\langle y^{\prime}-y,Q(y)-p \rangle+\tfrac{1}{2K}\|y^{\prime}-y\|_{\infty}^{2}.\] (A.7b)

_In particular, if \(h(0)=0\), we have_

\[(K/2)\|Q(y)\|^{2}\leq h^{*}(y)\leq-\min h+\langle y,Q(y)\rangle+(2/K)\|y\|_{ \infty}^{2}\quad\text{for all }y\in\mathcal{Y}.\] (A.8)

Proof of Lemma a.2.: By the strong convexity of \(h\) relative to \(\|\cdot\|\) (cf. Lemma A.1), we have

\[h(x)+t\langle y,p-x\rangle \leq h(x+t(p-x))\] \[\leq th(p)+(1-t)h(x)-\tfrac{1}{2}Kt(1-t)\|x-p\|^{2},\] (A.9)

leading to the bound

\[\tfrac{1}{2}K(1-t)\|x-p\|^{2}\leq h(p)-h(x)-\langle y,p-x\rangle=F(p,y)\] (A.10)

for all \(t\in(0,1]\). The bound (A.7a) then follows by letting \(t\to 0^{+}\) in (A.10).

For our second claim, we have

\[F(p,y^{\prime}) =h(p)+h^{*}(y^{\prime})-\langle y^{\prime},p\rangle\] \[\leq h(p)+h^{*}(y)+\langle y^{\prime}-y,\nabla h^{*}(y)\rangle+ \frac{1}{2K}\|y^{\prime}-y\|_{\infty}^{2}-\langle y^{\prime},p\rangle\] \[=F(p,y)+\langle y^{\prime}-y,Q(y)-p\rangle+\frac{1}{2K}\|y^{ \prime}-y\|_{\infty}^{2},\] (A.11)

where the inequality in the second line follows from the fact that \(h^{*}\) is \((1/K)\)-strongly smooth [44, Theorem 12.60(e)]. 

### Basic results from probability theory

We conclude this appendix with some useful results from probability theory that we will use freely throughout the sequel. For a complete treatment, we refer the reader to Hall & Heyde [25].

**Lemma A.3** (Azuma-Hoeffding inequality).: _Let \(M_{t}\in\mathbb{R}\), \(t=1,2,\dots\), be a martingale with \(\|M_{t}-M_{t-1}\|_{\infty}\leq\sigma_{t}\) (a.s.). Then, for all \(\eta>0\), we have_

\[\mathbb{P}\left(\left|M_{t}\right|\leq\left(2\log(2t^{2}/\eta)\,\sum_{s=1}^{t }\sigma_{s}^{2}\right)^{1/2}\text{for all }t\right)\geq 1-\eta.\] (A.12)

**Lemma A.4** (Kolmogorov's inequality).: _Let \(Z_{t}\in\mathbb{R}\), \(t=1,2,\dots\), be a martingale difference sequence that is bounded in \(L^{2}\). Then:_

\[\mathbb{P}\left(\max_{s\leq t}\sum_{\ell=1}^{s}Z_{\ell}\geq\varepsilon\right) \leq\frac{1}{\varepsilon^{2}}\mathbb{E}\left[\left(\sum_{s=1}^{t}Z_{s} \right)^{2}\right]\quad\text{for all }\varepsilon>0.\] (A.13)

**Lemma A.5** (Doob's maximal inequality).: _Let \(Z_{t}\in\mathbb{R}\), \(t=1,2,\dots\), be a martingale difference sequence that is bounded in \(L^{p}\) for some \(p\geq 1\). Then_

\[\mathbb{P}\left(\max_{s\leq t}\left|Z_{s}\right|>\varepsilon\right)\leq\frac {1}{\varepsilon^{p}}\,\mathbb{E}\left[\left|Z_{t}\right|^{p}\right]\quad\text{ for all }\varepsilon>0.\] (A.14)

**Lemma A.6** ( Burkholder-Davis-Gundy inequality).: _Let \(Z_{t}\), \(t=1,2,\dots\), be a martingale difference sequence in \(\mathbb{R}^{n}\). Then, for all \(p>1\), there exist constants \(c_{p},C_{p}\) that depend only on \(p\) and are such that_

\[c_{p}\,\mathbb{E}\left[\sum_{s=1}^{t}\|Z_{s}\|_{2}^{2}\right]^{p/2}\leq \mathbb{E}\left[\max_{s\leq t}\left\|\sum_{\ell=1}^{s}Z_{\ell}\right\|_{2}^{p} \right]\leq C_{p}\,\mathbb{E}\left[\sum_{s=1}^{t}\|Z_{s}\|_{2}^{2}\right]^{p/2}.\] (A.15)

**Lemma A.7** (Robbins-Siegmund).: _Let \(\mathcal{F}_{t}\), \(t=1,2,\dots\), be a filtration on a complete probability space \((\Omega,\mathcal{F},\mathbb{P})\), and suppose that the sequences \(X_{t}\), \(L_{t}\) and \(K_{t}\)\(\mathcal{F}_{t}\)-measurable, nonnegative, and such that_

\[\mathbb{E}\left[X_{t+1}\,|\,\mathcal{F}_{t}\right]\leq X_{t}(1+L_{t})+K_{t} \quad\text{with probability }1.\] (A.16)

_Then, \(X_{t}\) converges to some random variable \(X_{\infty}\) with probability \(1\) on the event \(\left\{\sum_{t=1}^{\infty}L_{t}<\infty\text{ and }\sum_{t=1}^{\infty}K_{t}<\infty\right\}\)._Specific algorithms and their properties

### Known algorithms as special cases of (Rl)

To complement our analysis in the main part of our paper, we detail below how Algorithms 1-3 can be recast in the general framework of (RL). To lighten notation, we will assume that \(b_{t}\), \(U_{t}\) and \(\hat{v}_{t}\) are respectively bounded as

\[\|b_{t}\|_{\infty}\leq B_{t}\qquad\|U_{t}\|_{\infty}\leq\sigma_{t}\qquad\text{ and}\qquad\|\hat{v}_{t}\|_{\infty}\leq M_{t}\] (B.1)

and we will set

\[G\coloneqq\max_{i\in\mathcal{N}}\max_{\alpha\in\mathcal{A}}\lvert v_{i}( \alpha)\rvert\] (B.2)

so we can take \(M_{t}=G+B_{t}+\sigma_{t}\) in (B.1). We will also make free use of the fact that \(v\) is Lipschitz continuous on \(\mathcal{X}\), and we will write \(L\) for its Lipschitz modulus in the \((L^{1},L^{\infty})\) pair of norms on \(\mathcal{X}\) and \(\mathcal{Y}\) respectively, viz.

\[\|v(x^{\prime})-v(x)\|_{\infty}\leq L\|x^{\prime}-x\|_{1}\qquad\text{for all}\;x,x^{\prime}\in\mathcal{X}.\] (B.3)

We now proceed to establish the required bounds for Algorithms 1-3:

**Algorithm 1**.: Since \(\hat{v}_{t}=v(X_{t})\), we readily get \(b_{t}=U_{t}=0\) by definition, so Algorithm 1 fits the scheme (RL) for free with \(\ell_{b}=\infty\), \(\ell_{\sigma}=0\). 

**Algorithm 2**.: For the case of (Opt-FTRL), we have \(\hat{v}_{t}=2v(X_{t})-v(X_{t-1})\) so \(b_{t}=v(X_{t})-v(X_{t-1})\), which is \(\mathcal{F}_{t}\)-measurable. We thus get

\[\|b_{t}\|_{\infty}=\|\mathbf{E}[\hat{v}_{t}\mid\mathcal{F}_{t}]- v(X_{t})\|_{\infty} \leq\mathbf{E}[\|v(X_{t})-v(X_{t-1})\|_{\infty}\mid\mathcal{F}_{t}]\] \[\leq L\,\mathbf{E}[\|X_{t}-X_{t-1}\|\,|\,\mathcal{F}_{t}]\] (B.3) \[=L\,\mathbf{E}[\|Q(Y_{t})-Q(Y_{t-1})\|_{\infty}\mid\mathcal{F}_{t}]\] \[\leq(L/K)\,\mathbf{E}[\|Y_{t}-Y_{t-1}\|_{\infty}\mid\mathcal{F}_{ t}]\] \[\leq\gamma_{t}(L/K)\,\mathbf{E}[2v(X_{t})-v(X_{t-1})\mid\mathcal{ F}_{t}]\] \[\leq 3LG/K\cdot\gamma_{t}\] \[=\mathcal{O}(\gamma_{t})=\mathcal{O}(1/t^{\ell_{\gamma}})\] (B.4)

Moreover, given that \(\hat{v}\) is \(\mathcal{F}_{t}\)-measurable, we readily get \(U_{t}=0\). 

**Algorithm 3**.: Since \(\hat{\alpha}_{t}\) is sampled according to \(\hat{X}_{t}=(1-\delta_{t})X_{i,t}+\delta_{t}\,\mathrm{unif}_{\mathcal{A}_{t}}\) (cf. Eq. (11) in Section 3), we readily obtain \(\mathbf{E}[\hat{v}_{t,t}\mid\mathcal{F}_{t}]=v_{i}(\hat{X}_{t})\), and hence, by (B.3), we get

\[B_{t}=\mathcal{O}(\|\hat{X}_{t}-X_{t}\|)=\mathcal{O}(\delta_{t})=\mathcal{O}( 1/t^{\ell_{\delta}}).\] (B.5)

Moreover, since \(\hat{X}_{i\,\alpha_{t},t}\geq\delta_{t}/A_{i}\), it follows that \(\|\hat{v}_{t}\|_{\infty}=\mathcal{O}(1/\delta_{t})=\mathcal{O}(t^{\ell_{ \delta}})\). 

For comparison purposes, we illustrate the algorithms' behavior in a simple \(2\times 2\times 2\) game in Fig. 2 in Appendix E.

### Further algorithms and illustrations

To demonstrate the breadth of (RL) as an algorithmic template, we provide below some more examples of algorithms from the game-theoretic literature that can be recast as special cases thereof (see also Table 1 for a recap).

**Algorithm 4** (Mirror-prox).: A progenitor of (Opt-FTRL) is the so-called _mirror-prox_ (MP) algorithm [29, 37], which updates as:

\[\begin{split}\tilde{Y}_{t}&=Y_{t}+\gamma_{t}v(X_{t })\qquad\quad Y_{t+1}=Y_{t}+\gamma_{t}v(\tilde{X}_{t})\\ \tilde{X}_{t}&=Q(\tilde{Y}_{t})\qquad\qquad\quad X_{t +1}=Q(Y_{t+1}).\end{split}\] (MP)

The main difference between (MP) and (Opt-FTRL) is that the former utilizes two surrogate gain vectors per iteration - meaning in particular that the interim, leading state \(\tilde{X}_{t}\) is generated with payoff information from \(X_{t}\), not \(\tilde{X}_{t-1}\). This method has been used extensively in the literature for solving variational inequalities and two-player, zero-sum games, cf. Juditsky et al. [29] and references therein.

A calculation similar to that for (Opt-FTRL) shows that Algorithm 4 has \(B_{t}=\mathcal{O}(1/t^{\ell_{\gamma}})\) and \(\sigma_{t}=0\) because the algorithm has no further randomization. \(\blacklozenge\)

**Algorithm 5** (Clairvoyant multiplicative weights).: A recent variant of the exponential / multiplicative weights (Hedge) algorithm is the so-called _clairvoyant multiplicative weights_ (CMW) algorithm [39]

\[Y_{i,t+1}=Y_{i,t}+\gamma_{t}v_{i}(X_{t+1})\qquad X_{i,t+1}=\Lambda_{i}(Y_{i,t+1}).\] (CMW)

The main difference between (CMW) and (Hedge) is that the proxy payoff vector \(\tilde{v}_{t}\) in (CMW) is based on the _future_ state \(X_{t+1}\) and _not_ the current state \(X_{t}\). To perform this "clairvoyant" update, the players of the game must coordinate to solve an implicit fixed point problem, so (CMW) is only meaningful when one has access to the payoff function \(v(\cdot)\). In this regard, (CMW) can be seen as a Bregman proximal point method in the general spirit of Bauschke et al. [8].

To cast (CMW) as an instance of the generalized template (RL), simply note that the sequence of input signals is given by \(\tilde{v}_{t}=v(X_{t+1})\), so \(U_{t}=0\) and \(b_{t}=v(X_{t+1})-v(X_{t})=\mathcal{O}(\gamma_{t})=\mathcal{O}(1/t^{\ell_{ \gamma}})\). \(\blacklozenge\)

## Appendix C Proof of Theorem 1

Our main goal in this appendix will be to prove Theorem 1 on the resilience properties of (RL). For convenience, we restate below the relevant result for ease of reference:

**Theorem 1**.: _Let \(X_{t}\), \(t=1,2,\dots\), be the sequence of play generated by (RL) with step-size / gain parameters \(\ell_{\gamma}>2\ell_{\sigma}\) and \(\ell_{b}>0\). Then, with probability \(1\), the limit set \(\mathcal{L}(X)\) of \(X_{t}\) is resilient._

Proof.: Our proof that \(\mathcal{L}(X)\) is resilient hinges on an energy-based technique that we will employ repeatedly in other parts of our analysis. To begin, introduce a player-strategy deviation pair \((i,z_{i})\), and say that a set is resilient _to_\((i,z_{i})\) if there exists an element of the set, say \(x^{*}\), which counters said deviation, i.e., such that \(u_{i}(x^{*})\geq u_{i}(z_{i};x^{*}_{-i})\). In this specific case, our proof proceeds by contradiction, namely by assuming that, with positive probability, \(\mathcal{L}(X)\) is _not_ resilient to \((i,z_{i})\). The main steps of our proof unfold as follows:

**Step 1**.: _Assume that \(\mathcal{L}(X)\) is not resilient to \((i,z_{i})\) with positive probability. Then there exists \(c,\epsilon,t_{0}>0\) such that_

\[\mathbb{P}\big{(}u_{i}(z_{i};X_{t,-i})\geq u_{i}(X_{t})+c\ \ \text{ for all }t\geq t_{0}\big{)}\geq\epsilon.\] (C.1)

Proof of Step 1.: The function \(f:x\in\mathcal{X}\mapsto u_{i}(z_{i};x_{-i})-u_{i}(x)\) is continuous and \(\mathcal{X}\) is compact, so there is a definite function \(\eta\equiv\eta(\delta)\) such that if \(\|x-x^{\prime}\|\leq\eta(\delta)\), then \(|f(x)-f(x^{\prime})|\leq\delta\). Now, by assumption, \(\{\forall x^{*}\in\mathcal{L}(X),u_{i}(z_{i};x^{*}_{-i})>u_{i}(x^{*})\}\) is of positive probability. We thus get

\[0 <\mathbb{P}\big{\{}\forall x^{*}\in\mathcal{L}(X),u_{i}(z_{i};x^{ *}_{-i})>u_{i}(x^{*})\big{\}}\] \[=\mathbb{P}\bigg{(}\inf_{x^{*}\in\mathcal{L}(X)}\big{(}u_{i}(z_{i };x^{*}_{-i})-u_{i}(x^{*})\big{)}>0\bigg{)}\] (C.2a) \[=\mathbb{P}\bigg{(}\bigcup_{m>0}\bigg{\{}\inf_{x^{*}\in\mathcal{L }(X)}\big{(}u_{i}(z_{i};x^{*}_{-i})-u_{i}(x^{*})\big{)}>2^{-m}\bigg{\}}\bigg{)}\] (C.2b) \[\leq\frac{1}{2}\mathbb{P}\big{\{}\forall x^{*}\in\mathcal{L}(X), u_{i}(z_{i};x^{*}_{-i})-u_{i}(x^{*})>2c\big{\}}\] (C.2c)

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline  & **Representative** & **Regularizer (\(\theta\))** & **Feedback** & **Bias (\(B_{t}\))** & **Variance (\(\sigma_{t}\))** \\ \hline Algorithm 1 & Hedge & \(z\log z\) & full info & \(0\) & \(0\) \\ Algorithm 2 & OMW & \(z\log z\) & full info & \(\mathcal{O}(1/t^{\ell_{\gamma}})\) & \(0\) \\ Algorithm 3 & EXP3 & \(z\log z\) & payoff & \(\mathcal{O}(1/t^{\ell_{\delta}})\) & \(\mathcal{O}(t^{\ell_{\delta}})\) \\ Algorithm 3 & Tsallis-INF & \(-4\sqrt{z}\) & payoff & \(\mathcal{O}(1/t^{\ell_{\delta}})\) & \(\mathcal{O}(t^{\ell_{\delta}})\) \\ Algorithm 4 & MP & general & full info & \(\mathcal{O}(1/t^{\ell_{\gamma}})\) & \(0\) \\ Algorithm 5 & CMW & \(z\log z\) & full info & \(\mathcal{O}(1/t^{\ell_{\delta}})\) & \(0\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: A range of algorithms adhering to the general template (RL) and their bias and variance characteristics when run with a step-size sequence of the form \(\gamma_{t}=\gamma/t^{\ell_{\gamma}}\), \(\ell_{\gamma}\in(0,1]\), and, where applicable, a sampling parameter \(\delta_{t}=\delta/t^{\ell_{\delta}}\).

for some \(c>0\) in (C.2c), and where (C.2a) is because \(\mathcal{L}(X)\) is closed - hence compact - almost surely. Therefore, by definition of \(\eta(\cdot)\),

\[0<\mathbb{P}\big{\{}\forall x^{*}\in\mathcal{X},\operatorname{dist}(x^{*}, \mathcal{L}(X))\leq\eta(c)\Rightarrow u_{i}(z_{i};x^{*}_{-i})-u_{i}(x^{*})>c \big{\}}=2\epsilon\] (C.2d)

Now, let \(t_{0}\) such that \(\mathbb{P}\{\forall t\geq t_{0},\operatorname{dist}(X_{t},\mathcal{L}(X)) \leq\eta(c)\}>1-\frac{\epsilon}{2}\). Then by construction, we get

\[\mathbb{P}\big{\{}\forall t\geq t_{0},u_{i}(z_{i};X_{t,-i})>u_{i}(X_{t})+c \big{\}}>\epsilon.\] (C.3)

and our proof is complete. 

Intuitively, the existence of an action that consistently outperforms \(X_{t}\) runs contrary to the behavior that one would expect from any regularized learning algorithm. We will proceed to make this intuition precise below by means of an energy argument. To that end, consider the Fenchel coupling

\[F_{t}=h_{i}(z_{i})+h_{i}^{*}(Y_{i,t})-\langle Y_{i,t},z_{i}\rangle\] (C.4)

Then, by Lemma A.2 in Appendix A, we readily get that

\[F_{t+1}\leq F_{t}-\gamma_{t}\langle\hat{v}_{i,t},z_{i}-X_{i,t}\rangle+\frac{ \gamma_{t}^{2}}{2\kappa_{h}}\|\hat{v}_{i,t}\|_{\infty}^{2}.\] (C.5)

where, in obvious notation, we are identifying \(z_{i}\in\mathcal{A}_{i}\) with the corresponding vertex \(e_{z_{i}}\) of \(\mathcal{X}_{i}=\Delta(\mathcal{A}_{i})\). To proceed, the main idea will be to relate \(\gamma_{t}\langle\hat{v}_{i,t},z_{i}-X_{i,t}\rangle\) to its "perfect" counterpart \(\gamma_{t}\langle v_{i}(X_{t}),z_{i}-X_{i,t}\rangle\). We formalize this below.

**Step 2**.: _If \(\mathcal{L}(X)\) is not resilient to \((i,z_{i})\), there exists \(t_{1}\geq t_{0}\) such that, with probability \(\varepsilon^{\prime}/2>0\), and for all \(t\geq t_{1}\), we have_

\[F_{t}\leq F_{t_{0}}-\frac{c}{2}\sum_{s=t_{0}}^{t}\gamma_{s}.\] (C.6)

Proof of Step 2.: With probability \(\varepsilon^{\prime}\) and for all \(t\geq t_{0}\), we have

\[\gamma_{t}\langle\hat{v}_{i,t},z_{i}-X_{i,t}\rangle =\gamma_{t}\langle v_{i}(X_{t}),z_{i}-X_{i,t}\rangle+\gamma_{t} \langle U_{i,t},z_{i}-X_{i,t}\rangle+\gamma_{t}\langle b_{i,t},z_{i}-X_{i,t}\rangle\] (C.7) \[\geq\big{[}c+\langle U_{i,t},z_{i}-X_{i,t}\rangle+\langle b_{i,t},z_{i}-X_{i,t}\rangle\big{]}\gamma_{t}.\] (C.8)

The combination of Eqs. (C.5) and (C.8) then provides the following upper bound of \(F_{t+1}\):

\[F_{t+1} \leq F_{t}-c\gamma_{t}+\gamma_{t}\langle U_{i,t},z_{i}-X_{i,t} \rangle+\gamma_{t}\langle b_{i,t},z_{i}-X_{i,t}\rangle+\frac{\gamma_{t}^{2}}{2 \kappa_{h}}\|\hat{v}_{i,t}\|_{\infty}^{2}\] (C.9) \[\leq F_{t_{0}}-c\sum_{s=t_{0}}^{t}\gamma_{s}+\underbrace{\sum_{s= t_{0}}^{t}\gamma_{s}\langle U_{s,t},z_{i}-X_{s,i}\rangle}_{E_{U,t}}+\underbrace{ \sum_{s=t_{0}}^{t}\gamma_{s}\langle b_{s,s},z_{i}-X_{s,i}\rangle}_{E_{b,t}}+ \underbrace{\sum_{s=t_{0}}^{t}\frac{\|\hat{v}_{s,t}\|_{\infty}^{2}}{2\kappa_ {h}}\gamma_{s}^{2}}_{E_{b,t}}\gamma_{s}^{2}.\] (C.10)

We are thus left to show is that \(c\sum_{s=t_{0}}^{t}\gamma_{s}\) is the dominant term above. To do so, we proceed to examine each term individually:

* _Second-order term:_ We first deal with the second-order term \(\sum_{s=t_{0}}^{t}\frac{\|\hat{v}_{s,t}\|_{\infty}^{2}}{2\kappa_{h}}\gamma_{s} ^{2}\). By expanding the \(\|\hat{v}_{s,i}\|_{\infty}^{2}\), we readily get \[\frac{\sum_{s=t_{0}}^{t}\|\hat{v}_{s,t}\|_{\infty}^{2}\gamma_{s}^{2}}{\tau_{t}}= \mathcal{O}\Bigg{(}\frac{\sum_{s=1}^{t}\gamma_{s}^{2}(1+B_{s}^{2}+\sigma_{s}^{ 2})}{\sum_{s=1}^{t}\gamma_{s}}\Bigg{)}.\] (C.11) However, by our assumptions on the parameters of (RL), we readily get \[\lim_{t\to\infty}\frac{\gamma_{t}^{2}(1+B_{t}^{2}+\sigma_{t}^{2})}{\gamma_{t}}=0\] (C.12) so we conclude that \[\lim_{t\to\infty}\frac{\sum_{s=1}^{t}\gamma_{s}^{2}(1+B_{s}^{2}+\sigma_{s}^{ 2})}{\sum_{s=1}^{t}\gamma_{s}}=0\] (C.13) by the Stolz-Cesaro theorem.

* _Bias term:_ By far the most immediate, the bias term \(E_{b,t}\) is bounded as \[E_{b,t}\leq 2\sum_{s=t_{0}}^{t}\lVert b_{t,t}\rVert_{\infty}\gamma_{s}\leq 2\sum_{ s=t_{0}}^{t}B_{s}\gamma_{s}=\mathrm{o}\left(\sum_{s=t_{0}}^{t}\gamma_{s} \right)\quad\text{as $t\to\infty$}.\] (C.14)
* _Noise term:_ Finally, the noise term \(E_{U,t}\) is bounded by means of the Azuma-Hoeffding inequality, cf. Lemma A.3 in Appendix A. Specifically, with probability at least \(1-\varepsilon^{\prime}/2\), we have \[E_{U,t} \coloneqq\sum_{s=t_{0}}^{t}\gamma_{s}\langle U_{s,t},z_{t}-X_{s, i}\rangle\] \[\leq 2\left(\sum_{s=t_{0}}^{t}\lVert U_{s,t}\rVert_{\infty}^{2} \gamma_{s}^{2}\right)^{1/2}\sqrt{2\log\left(\frac{4t^{2}}{\varepsilon^{\prime }}\right)}\] \[\leq 2\left(\sum_{s=t_{0}}^{t}\sigma_{s}^{2}\gamma_{s}^{2} \right)^{1/2}\sqrt{2\log\left(\frac{4t^{2}}{\varepsilon^{\prime}}\right)}.\] (C.15) for all \(t\geq t_{0}\). To proceed, note that a second application of the Stolz-Cesaro theorem yields \(\sum_{s=t_{0}}^{t}\sigma_{s}^{2}\gamma_{s}^{2}=\mathrm{o}(\sum_{s=t_{0}}^{t} \gamma_{s})\) and, moreover, note that \(\log(4t^{2}/\varepsilon^{\prime})=\mathcal{O}(\sum_{s=t_{0}}^{t}\gamma_{s})\). Taking square roots and multiplying then yields that \[E_{U,t}=\mathrm{o}\left(\sum_{s=t_{0}}^{t}\gamma_{s}\right)\] (C.16) with probability at least \[1-\varepsilon^{\prime}/2.\] We are now in a position to establish the bound Eq. (C.6). Indeed, putting Eqs. (C.13), (C.14) and (C.16) together, we readily infer that there exists \(t_{1}\geq t_{0}\) such that, with probability at least \(1-\varepsilon^{\prime}/2\), we have \[\sum_{s=t_{0}}^{t}\gamma_{s}\langle U_{s,t},z_{t}-X_{s,i}\rangle+\sum_{s=t_{0} }^{t}\gamma_{s}\langle b_{s,t},z_{t}-X_{s,i}\rangle+\sum_{s=t_{0}}^{t}\frac{ \lVert\hat{v}_{s,i}\rVert_{\infty}^{2}}{2\kappa_{h}}\gamma_{s}^{2}\leq\frac{c }{2}\sum_{s=t_{0}}^{t}\gamma_{s}\] (C.17) for all \(t\geq t_{1}\). This proves Eq. (C.6) and concludes our proof. 

Summarizing the above, we have shown that, with probability at least \(1-\varepsilon^{\prime}/2\), we have

\[F_{t+1}\leq F_{t_{0}}-\frac{c}{2}\sum_{s=t_{0}}^{t}\gamma_{s}\to-\infty\quad \text{as $t\to\infty$}.\] (C.18)

Since \(F\) is nonnegative (by Lemma A.2), we have established that the event where \(\mathcal{L}(X)\) is not resilient to \((i,z_{i})\) is an event of probability zero. However, since there are uncountably many strategic deviations, the proof is not yet complete; the last step involves an approximation by deviations with _rational_ entries.

**Step 3**.: \(\mathcal{L}(X)\) _is almost-surely resilient._

Proof of Step 3.: The key point of the proof is the observation that a closed set is resilient if and only if it is _rationally_ resilient, i.e., it nullifies all _rational_ deviations \(z_{t}\in\mathcal{X}_{i}\cap\mathbf{Q}^{\mathcal{A}_{t}}\) (which are countably many). Indeed, if \(\mathcal{L}(X)\) is not resilient with positive probability, then, likewise, \(\mathcal{L}(X)\) will not be rationally resilient with positive probability either. Because there are countably many rational deviations, there must be a rational strategic deviation \((i,z_{i})\) (with \(z_{i}\in\mathcal{X}_{i}\cap\mathbf{Q}^{\mathcal{A}_{t}}\)) to which \(\mathcal{L}(X)\) is not resilient. This comes in contradiction with the conclusions of Step 2. 

This concludes the last required step, so the proof of Theorem 1 is now complete.

Proof of Theorems 2 and 3

In this last appendix, our goal is to prove our characterization of club sets, namely:

**Theorem 2**.: _Fix some set \(\mathcal{S}\in\mathcal{P}(\mathcal{X})\) and suppose that (RL) is run with a steep regularizer and step-size/gain parameters \(\ell_{\gamma}\in[0,1]\), \(\ell_{b}>0\), and \(\ell_{\sigma}<1/2\). Then:_

1. \(\mathcal{S}\) _is stochastically asymptotically stable under (_RL_) if and only if it is a club set._
2. \(\mathcal{S}\) _is irreducibly stable under (_RL_) if and only if it is an m-club set._

**Theorem 3**.: _Let \(\mathcal{S}\in\mathcal{P}(\mathcal{X})\) be a club set, and let \(X_{t}\), \(t=1,2,\dots\), be the sequence of play generated by (_RL_) with parameters \(\ell_{\gamma}\in[0,1]\), \(\ell_{b}>0\), and \(\ell_{\sigma}<1/2\). Then, for all \(\epsilon>0\), there exists an (open, unbounded) initialization domain \(\mathcal{D}\subseteq\mathcal{Y}\) such that, with probability at least \(1-\epsilon\), we have_

\[\operatorname{dist}(X_{t},\mathcal{S})\leq C\varphi\Big{(}c_{1}-c_{2}\sum_{s =1}^{t}\gamma_{s}\Big{)}\quad\text{whenever }Y_{1}\in\mathcal{D}\] (17)

_where \(C,c_{1},c_{2}\) are constants (\(C,c_{2}>0\)), and the rate function \(\varphi\) is given by \(\varphi(z)=(\theta^{\prime})^{-1}(z)\) if \(z>\lim_{z\to 0^{\prime}}\theta^{\prime}(z)\), and \(\varphi(z)=0\) otherwise._

Our proof strategy will be to construct a sheaf of "linearized" energy functions which, when bundled together, yield a suitable Lyapunov-like function for \(\mathcal{S}\). To do so, let \(\mathcal{C}=\prod_{i}\mathcal{C}_{i}\) denote the support of \(\mathcal{S}\) (cf. the definition of club sets), and let

\[\mathcal{Z}_{i}=\{e_{i\,\alpha_{i}^{\prime}}-e_{i\,\alpha_{i}}:\alpha_{i}\in \mathcal{C}_{i},\alpha_{i}^{\prime}\in\mathcal{A}_{i}\setminus\mathcal{C}_{i}\}\] (D.1)

and

\[\mathcal{Z}=\bigcup_{i\in\mathcal{N}}\mathcal{Z}_{i}\] (D.2)

denote the set of all pure strategic deviations from \(\mathcal{S}\). Then, our ensemble of candidate energy functions will be given by

\[E_{z}(y)=\langle y,z\rangle\qquad\text{for }z\in\mathcal{Z},\,y\in\mathcal{V}^{*}.\] (D.3)

The motivation for this definition is given by the following lemma.

**Lemma D.1**.: _Suppose that the sequence \(y_{t}\in\mathcal{V}^{*}\), \(t=1,2,\dots\), has \(E_{z}(y_{t})\to-\infty\) for all \(z\in\mathcal{Z}\) as \(t\to\infty\). Then the sequence \(x_{t}=Q(y_{t})\) converges to \(\mathcal{S}\) as \(t\to\infty\)._

Proof.: Let \(z=e_{i\,\alpha_{i}^{\prime}}-e_{i\,\alpha_{i}}\) for some \(i\in\mathcal{N}\), \(\alpha_{i}\in\mathcal{C}_{i}\), and \(\alpha_{i}^{\prime}\in\mathcal{A}_{i}\setminus\mathcal{C}_{i}\). Since \(E_{z}(y_{t})\to-\infty\) by assumption, we get \(y_{i\,\alpha_{i}^{\prime},t}-y_{i\,\alpha_{i},t}\to-\infty\) and hence, by Lemma A.1, we conclude that \(Q_{i\,\alpha_{i}^{\prime}}(x_{t})\to 0\) as \(t\to\infty\). In turn, given that this holds for all \(i\in\mathcal{N}\) and all \(\alpha_{i}^{\prime}\in\mathcal{A}_{i}\setminus\mathcal{C}_{i}\), we conclude that \(x_{t}=Q(y_{t})\) converges to \(\mathcal{S}\). 

In view of the above, we will focus on showing that \(E_{z}(Y_{t})\to-\infty\) for all \(z\in\mathcal{Z}\). As a first step, we establish a basic template inequality for the evolution of \(E_{z}\) under (RL).

**Lemma D.2**.: _Fix some \(z\in\mathcal{Z}\) and let \(E_{t}\coloneqq E_{z}(Y_{t})\). Then, for all \(t=1,2,\dots\), we have_

\[E_{t+1}\leq E_{t}+\gamma_{t}\,\langle v(X_{t}),z\rangle+\gamma_{t}\xi_{t}+ \gamma_{t}\psi_{t}\] (D.4)

_where the error terms \(\xi_{t}\) and \(\psi_{t}\) are given by_

\[\xi_{t}=\langle U_{t},z\rangle\qquad\text{and}\qquad\psi_{t}=2B_{t}.\] (D.5)

Proof.: Simply set \(y\gets Y_{t+1}\) in \(E_{z}(y)\), invoke the definition of the update \(Y_{t}\gets Y_{t+1}\) in (RL), and note that \(|\langle b_{t},z\rangle|\leq\|z\|\|b_{t}\|_{\infty}\leq 2B_{t}\) by the definition of \(\mathcal{Z}\). 

The key take-away from (D.4) is that, if \(X_{t}\) is close to \(\mathcal{S}\) and \(\alpha_{i}\in\mathcal{C}_{i}\), \(\alpha_{i}^{\prime}\in\mathcal{A}_{i}\setminus\mathcal{C}_{i}\), we will have

\[\langle v(X_{t}),z\rangle=v_{i\,\alpha_{i}^{\prime}}(X_{t})-v_{i\,\alpha_{i}}(X _{t})=u_{i}(\alpha_{i}^{\prime};X_{-i,t})-u_{i}(\alpha_{i};X_{-i,t})<0\] (D.6)

by the continuity of \(u_{i}\) and the assumption that \(\mathcal{S}\) is a club set. More concretely, by the definition of the better-reply correspondence, we have

\[\langle v(x^{*}),z\rangle<0\quad\text{for all }x^{*}\in\mathcal{S}\text{ and all }z\in\mathcal{Z}\] (D.7)and hence, by continuity, there exists a neighborhood \(\mathcal{B}\) of \(\mathcal{S}\) such that

\[\langle v(x),z\rangle<0\quad\text{for all }x\in\mathcal{B}\text{ and all }z\in\mathcal{Z}.\] (D.8)

In other words, as long as \(X_{t}\) is sufficiently close to \(\mathcal{S}\), (D.4) exhibits a consistent negative drift pushing \(E_{t}\) towards \(-\infty\).

To exploit this "dynamic consistency" property of \(\mathcal{S}\), it will be convenient to introduce the family of sets

\[\mathcal{D}(\epsilon)=\{y\in\mathcal{V}^{*}:\langle y,z\rangle<-\epsilon\text { for all }z\in\mathcal{Z}\}\] (D.9)

As we show below, these sets are mapped under \(Q\) to neighborhoods of \(\mathcal{S}\), so they are particularly well-suited to serve as initialization domains for (RL). This is encoded in the following properties:

**Lemma D.3**.: _Let \(x=Q(y)\) for some \(y\in\mathcal{V}^{*}\). Then, for all \(\alpha_{i},\alpha_{i}^{\prime}\), \(i\in\mathcal{N}\), we have_

\[x_{i\alpha_{i}}\leq\varphi\Big{(}\theta(1^{-})+y_{i\alpha_{i}^{\prime}}-y_{i \alpha_{i}}\Big{)}\] (D.10)

_with \(\varphi\) defined as per Theorem 3, i.e.,_

\[\varphi(z)=\begin{cases}0&\text{if }z\leq\theta^{\prime}(0^{+}),\\ (\theta^{\prime})^{-1}(z)&\text{if }\theta^{\prime}(0^{+})<z<\theta^{\prime}(1^{- }),\\ 1&\text{if }z\geq\theta^{\prime}(1^{-}).\end{cases}\] (D.11)

**Corollary D.1**.: _For all \(\delta>0\) there exists some \(\epsilon_{\delta}\in\mathbb{R}\) such that, for all \(\epsilon>\epsilon_{\delta}\) and all \(y\in\mathcal{D}_{\epsilon}\), we have_

\[Q_{i\alpha_{i}^{\prime}}(y_{i})<\delta\quad\text{for all }\alpha_{i}^{ \prime}\in\mathcal{A}_{i}\setminus\mathcal{C}_{i}\text{ and all }i\in\mathcal{N}.\] (D.12)

Proof of Lemma d.3.: Suppressing the player index for simplicity, the first-order stationarity conditions for the convex problem (9) readily give

\[y_{\alpha}-\theta^{\prime}(x_{\alpha})=\mu-\nu_{\alpha},\] (D.13)

where \(\mu\) is the Lagrange multiplier for the equality constraint \(\sum_{\alpha}x_{\alpha}=1\), and \(\nu_{\alpha}\) is the complementary slackness multiplier of the inequality constraint \(x_{\alpha}\geq 0\) (so \(\nu_{\alpha}=0\) whenever \(x_{\alpha}>0\)). Thus, rewriting (D.13) for some \(\alpha\in\mathcal{A}\), we get

\[y_{\alpha^{\prime}}-y_{\alpha}=\theta^{\prime}(x_{\alpha^{\prime}})-\theta^{ \prime}(x_{\alpha})+\nu_{\alpha}-\nu_{\alpha^{\prime}}\] (D.14)

and hence

\[\theta^{\prime}(x_{\alpha^{\prime}})=\theta^{\prime}(x_{\alpha})+\nu_{\alpha^ {\prime}}-\nu_{\alpha}+y_{\alpha^{\prime}}-y_{\alpha}\leq\theta^{\prime}(1^{ -})+\nu_{\alpha^{\prime}}+y_{\alpha^{\prime}}-y_{\alpha},\] (D.15)

where we used the fact that \(\nu_{\alpha}\geq 0\). Now, if \(\theta^{\prime}(1^{-})+y_{\alpha^{\prime}}-y_{\alpha}<\theta^{\prime}(0^{+})\) and \(x_{\alpha^{\prime}}>0\) (so \(\nu_{\alpha^{\prime}}=0\)), we will have \(\theta^{\prime}(x_{\alpha^{\prime}})<\theta^{\prime}(0^{+})\), a contradiction. This shows that \(x_{\alpha^{\prime}}=0\) if \(\theta^{\prime}(1^{-})+y_{\alpha^{\prime}}-y_{\alpha}<\theta^{\prime}(0^{+})\), so (D.10) is satisfied in this case. Otherwise, if \(x_{\alpha^{\prime}}>0\), we must have \(\nu_{\alpha^{\prime}}=0\) by complementary slackness, so (D.10) follows by applying the second branch of (D.11) to (D.15). 

The above provides us with a fairly good handle on the local geometric and dynamic properties of \(\mathcal{S}\). On the flip side however, the various error terms in (D.5) may be positive, so \(E_{t}\) may fail to be decreasing and \(X_{t}\) may drift away from \(\mathcal{S}\). On that account, it will be convenient to introduce the aggregate error processes

\[\text{I}_{t}=\sum_{s=1}^{t}\gamma_{s}\xi_{s}\qquad\text{and}\qquad\text{II}_{t }=\sum_{s=1}^{t}\gamma_{s}\psi_{s}.\] (D.16)

Intuitively, the aggregates (D.16) measure the total effect of each error term in (D.4), so we will establish a first series of results under the following general requirements:

1. _Subleading error growth:_ \[\lim_{t\to\infty}\text{I}_{t}/\tau_{t}=0\] (Sub.I) \[\lim_{t\to\infty}\text{II}_{t}/\tau_{t}=0\] (Sub.II) where \(\tau_{t}=\sum_{s=1}^{t}\gamma_{s}\) and both limits are to be interpreted in the almost sure sense.

2. _Drift dominance:_ \[\mathbf{P}(\mathrm{I}_{t} \leq C\tau_{t}^{\alpha}/2\ \ \text{for all $t$}) \geq 1-\eta\] (Dom.I) \[\mathbf{P}(\mathrm{II}_{t} \leq C\tau_{t}^{\alpha}/2\ \ \text{for all $t$}) \geq 1-\eta\] (Dom.II)

for some \(C>0\) and \(\alpha\in[0,1)\).

In a nutshell, (Sub) posits that the aggregate error processes \(\mathrm{I}_{t}\) and \(\mathrm{II}_{t}\) of (D.16) are subleading relative to the long-run drift of (D.4), while (Dom) goes a step further and asks that said errors are asymptotically dominated by the drift in (D.4). Accordingly, under these implicit error control conditions, we obtain the interim convergence result below:

**Proposition D.1**.: _Let \(\mathcal{S}\) be a club set, fix some confidence threshold \(\eta>0\), and let \(X_{t}=Q(Y_{t})\) be the sequence of play generated by (\(\mathrm{RL}\)). If (Sub) and (Dom) hold, there exists an unbounded initialization domain \(\mathcal{D}\subseteq\mathcal{V}^{*}\) such that_

\[\mathbf{P}(X_{t}\text{ converges to }\mathcal{S}\mid Y_{1}\in\mathcal{D}) \geq 1-2\eta.\] (D.19)

Proof of Proposition D.1.: Fix some \(z\in\mathcal{Z}\), let \(E_{t}=E_{z}(Y_{t})\), and pick \(\alpha\in[0,1)\) so that (Dom) holds for some \(C>0\). In addition, set \(c=-\sup_{x\in\mathcal{B}}\langle v(x),\,z\rangle>0\), let \(t_{0}=\inf\{t:c\tau_{t}>C\tau_{t}^{\alpha}\}\), and write \(\Delta E=\max_{t}\{C\tau_{t}^{\alpha}-c\tau_{t}\}\). Then, if \(Y_{1}\) is initialized in \(\mathcal{D}\leftarrow\mathcal{D}(\epsilon+\Delta E)\) where \(\epsilon\) is such that \(\mathcal{D}(\epsilon)\subseteq\mathcal{B}\), we will have \(Y_{t}\in\mathcal{D}(\epsilon)\) for all \(t\). Indeed, this being trivially the case for \(t=1\), assume it to be the case for all \(s=1,2,\ldots,t\). Then, by (D.4) and our inductive hypothesis, we get

\[E_{t+1}\leq E_{1}-\sum_{s=1}^{t}\gamma_{s}\langle v(X_{s}),z \rangle+\mathrm{I}_{t}+\mathrm{II}_{t}\leq-\epsilon-\Delta E-c\tau_{t}+C\tau_{ t}^{\alpha}/2+C\tau_{t}^{\alpha}/2\leq-\epsilon-\Delta E+\Delta E=-\epsilon\] (D.20)

i.e., \(E_{t+1}\in\mathcal{D}(\epsilon)\), as claimed.

Now, since \(E_{t}\in\mathcal{D}(\epsilon)\) for all \(t\), we conclude that

\[E_{t+1}\leq E_{1}-c\tau_{t}+\mathrm{I}_{t}+\mathrm{II}_{t}\quad \text{for all $t=1,2,\ldots$}\] (D.21)

Thus, if (Sub) holds, we readily get \(E_{t}\rightarrow-\infty\) with probability 1 on the event that (Dom.I) and (Dom.II) both hold. This implies that \(E_{t}\rightarrow-\infty\), and since \(z\in\mathcal{Z}\) above is arbitrary, we conclude that \(X_{t}\rightarrow\mathcal{S}\) with probability at least \(1-2\eta\), as claimed. 

We are now in a position to prove Theorem 2.

Proof of Theorem 2.: Our proof will hinge on showing that (Sub) and (Dom) hold under the stated step-size and sampling parameter schedules. Our claim will then follow by a direct application of Proposition D.1 and a reduction to a suitable subface of \(\mathcal{X}\).

First, regarding (Sub), the law of large numbers for martingale difference sequences [25, Theorem 2.18] shows that \(\mathrm{I}_{t}/\tau_{t}\to 0\) with probability 1 on the event \(\left\{\sum_{t}\gamma_{t}^{2}\,\mathrm{E}[\xi_{t}^{2}\mid\mathcal{F}_{t}]/\tau _{t}^{2}<\infty\right\}\). However

\[\mathbf{E}[\xi_{t}^{2}\mid\mathcal{F}_{t}]\leq 2^{2}\,\mathbf{E}[\|U_{t}\|_{ \infty}^{2}\mid\mathcal{F}_{t}]\leq 2^{2}\sigma_{t}^{2}=\mathcal{O}(t^{2\ell_{ \sigma}})\] (D.22)

so, in turn, we get

\[\sum_{t}\frac{\gamma_{t}^{2}\,\mathrm{E}[\xi_{t}^{2}\mid\mathcal{F}_{t}]}{ \tau_{t}^{2}}=\mathcal{O}\left(\sum_{t}\frac{\gamma_{t}^{2}\sigma_{t}^{2}}{ \tau_{t}^{2}}\right)=\mathcal{O}\left(\sum_{t}\frac{t^{-2\ell_{\tau}}t^{2\ell_{ \sigma}}}{t^{2(1-\ell_{\gamma})}}\right)=\mathcal{O}\left(\sum_{t}\frac{1}{t^{ 2-2\ell_{\sigma}}}\right)<\infty\] (D.23)

given that \(\ell_{\sigma}<1/2\). This establishes (Sub.I); the remaining requirement (Sub.II) follows trivially by noting that \(\sum_{s=1}^{t}\gamma_{s}B_{s}\big{/}\sum_{s=1}^{t}\gamma_{s}\to 0\) if and only if \(B_{t}\to 0\), which is immediate from the theorem's assumptions.

Second, regarding (Dom), since \(B_{t}\) is deterministic and \(B_{t}=\mathcal{O}(1/t^{\ell_{b}})\) for some \(\ell_{b}>0\), it is always possible to find \(C>0\) and \(\alpha\in(0,1)\) so that (Dom.II) holds. We are thus left to establish (Dom.I). To that end, let \(\mathrm{I}_{t}^{*}=\sup_{1\leq s\leq t}|\mathrm{I}_{t}|\) and set \(P_{t}=\mathrm{P}\big{(}\mathrm{I}_{t}^{*}>C\tau_{t}^{\alpha}/2\big{)}\) so

\[P_{t}\,\leq\frac{\mathbf{E}[|\mathrm{I}_{t}|^{q}]}{(C/2)q\tau_{t}^{\alpha q}} \leq c_{q}\frac{\mathbf{E}\big{[}\left(\sum_{s=1}^{t}\gamma_{s}^{2}\|U_{s}\|_{ \infty}^{2}\right)^{q/2}\big{]}}{\tau_{t}^{\alpha q}}\] (D.24)where \(c_{q}\) is a positive constant depending only on \(C\) and \(q\), and we used Kolmogorov's inequality (Lemma A.4) in the first step and the Burkholder-Davis-Gundy inequality (Lemma A.6) in the second.

To proceed, we will require the following variant of Holder's inequality [9, p. 15]:

\[\left(\sum_{s=1}^{t}a_{s}b_{s}\right)^{\rho}\leq\left(\sum_{s=1}^{t}a_{s}^{ \frac{q\omega}{\rho-1}}\right)^{\rho-1}\sum_{s=1}^{t}a_{s}^{(1-\lambda)\rho}b_ {s}^{\rho}\] (D.25)

valid for all \(a_{s},b_{s}\geq 0\) and all \(\rho>1\), \(\lambda\in[0,1)\). Then, substituting \(a_{s}\leftarrow\gamma_{s}^{2}\), \(b_{s}\leftarrow\|U_{s}\|_{\infty}^{2}\), \(\rho\gets q/2\) and \(\lambda\gets 1/2-1/q\), (D.24) gives

\[P_{t}\leq c_{q}\frac{\left(\sum_{s=1}^{t}\gamma_{s}\right)^{q/2-1}\sum_{s=1}^ {t}\gamma_{s}^{1+q/2}\operatorname{\mathbb{E}}[\|U_{s}\|_{\infty}^{q}]}{\tau_ {t}^{\alpha q}}\leq c_{q}\frac{\sum_{s=1}^{t}\gamma_{s}^{1+q/2}\sigma_{s}^{q} }{\tau_{t}^{1+(\alpha-1/2)q}}\] (D.26)

We now consider two cases, depending on whether the numerator of (D.26) is summable or not.

1. \(\ell_{\gamma}(1+q/2)\geq 1+q\ell_{\sigma}\). In this case, the numerator of (D.26) is summable under the theorem's assumptions, so the fraction in (D.26) behaves as \(\mathcal{O}(1/t^{(1-\ell_{\gamma})(1+(\alpha-1/2)q)})\).
2. \(\ell_{\gamma}(1+q/2)<1+q\ell_{\sigma}\). In this case, the numerator of (D.26) is not summable under the theorem's assumptions, so the fraction in (D.26) behaves as \(\mathcal{O}\big{(}t^{1-\ell_{\gamma}(1+q/2)+q\ell_{\sigma}}/t^{(1-\ell_{ \gamma})(1+(\alpha-1/2)q)}\big{)}\).

Thus, working out the various exponents, a tedious - but otherwise straightforward - calculation shows that there exists some \(\alpha\in(0,1)\) such that \(P_{t}\) is summable as long as \(\ell_{\sigma}<1/2-1/q\) and \(0\leq\ell_{\gamma}<q/(2+q)\). Hence, if \(\gamma\) is sufficiently small relative to \(\eta\), we conclude that

\[\operatorname{\mathbb{P}}(\operatorname{I}_{t}\leq C\tau_{t}^{\alpha}/2\text{ for all }t)\geq 1-\sum_{t}P_{t}\geq 1-\eta/2.\] (D.27)

Finally, if \(\ell_{\gamma}>1/2+\ell_{\sigma}\), (Dom.I) is a straightforward consequence of (D.24) for \(q=2\).

With all this in hand, the final steps of our proof proceed as follows:

1. [label=**Closedness \(\Longrightarrow\) Stability.**] Our assertion follows by invoking Proposition D.1.

1. [label=**Stability \(\Longrightarrow\) Closedness.**] Suppose that \(\mathcal{S}\) is not club. Then there exists some pure strategy \(\alpha\in\mathcal{C}\) and some deviation \(\alpha^{\prime}\notin\mathcal{C}\) such that the deviation from \(\alpha\) to \(\alpha^{\prime}\) is not costly to the deviating player. Thus, if we consider the restriction of the game to the face spanned by \(\alpha\) and \(\alpha^{\prime}\) (a single-player game with two strategies), the corresponding score difference will be \[y_{\alpha^{\prime},t}-y_{\alpha,t}\geq\sum_{s=1}\gamma_{s}b_{s}+\sum_{s=1} \gamma_{s}U_{s}\] (D.28)

By our standing assumptions for \(b_{t}\) and \(U_{t}\) (and Doob's martingale convergence theorem for the latter), both \(\sum_{s=1}\gamma_{s}b_{s}\) and \(\sum_{s=1}\gamma_{s}U_{s}\) will be bounded from below by some (a.s.) finite random variable \(A_{0}\). Since \(\theta\) is steep, it follows that, with probability 1, \(\liminf_{t\rightarrow\infty}(y_{\alpha,t})>0\), so \(\mathcal{C}\) cannot be stable.

1. [label=**Minimality \(\Longrightarrow\) Irreducible Stability.**] Suppose that \(\mathcal{S}\) is m-club. Then, by our previous claim, \(\mathcal{S}\) is stochastically asymptotically stable. If \(\mathcal{S}\) contains a proper subface \(\mathcal{S}^{\prime}\subseteq\mathcal{S}\) that is also stochastically asymptotically stable, \(\mathcal{S}^{\prime}\) must be club by the converse implication of the first part of the theorem. However, in that case, \(\mathcal{S}\) would not be m-club, a contradiction which proves our claim.

1. [label=**Irreducible Stability \(\Longrightarrow\) Minimality.**] For our last claim, assume that \(\mathcal{S}\) is irreducibly stable. By the first part of our theorem, this implies that \(\mathcal{S}\) is club. Then, if it so happens that \(\mathcal{S}\) is not m-club, it would contain a proper club subface \(\mathcal{S}^{\prime}\subseteq\mathcal{S}\); by the first part of our theorem, this set would be itself stochastically asymptotically stable, in contradiction to the irreducibility assumption. This shows that \(\mathcal{S}\) is m-club and concludes our proof.

We are only left to establish the convergence rate estimate of Theorem 3.

Proof of Theorem 3.: Going back to (D.21) and invoking Lemma D.3 shows that there exist constants \(c_{1}>0\) and \(c_{2}\in\mathbb{R}\) such that, for all \(\alpha_{i}\in\mathcal{A}_{i}\setminus\mathcal{C}_{i}\), \(i\in\mathcal{N}\), we have

\[X_{i\alpha_{i},t}\leq\varphi(\theta(1^{-})+E_{t})\leq\varphi(c_{2}-c_{1}\tau_{ t})\] (D.29)

with probability 1 on the events of (Dom). We thus get

\[\operatorname{dist}_{1}(X_{t},\mathcal{S})\leq\sum_{i\in\mathcal{N}}\sum_{ \alpha_{i}\in\mathcal{A}_{i}\setminus\mathcal{C}_{i}}\varphi(c_{2}-c_{1}\tau_ {t}),\] (D.30)

and our proof is complete. 

As for the rate estimates of Corollary 2, the proof boils down to a simple derivation of the corresponding rate functions:

Proof of Corollary 2.: By a straightforward calculation, we have:

1. If \(\theta(z)=z\log z\) then \(\varphi(z)=\exp(1+z)\).
2. If \(\theta(z)=-4\sqrt{z}\) then \(\varphi(z)=4/z^{2}\).
3. If \(\theta(z)=z^{2}/2\) then \(\varphi(z)=[z]_{0}^{1}\).

Our claims then follow immediatly from the rate estimate (17) of Theorem 2. 

## Appendix E Numerical experiments

In all our experiments, we ran the EXP3 variant of bandit FTRL (B-FTRL) (cf. Algorithm 3) with step-size and sampling radius parameters \(\gamma_{t}=0.2\times t^{-1/2}\) and \(\delta_{t}=0.1\times t^{-0.15}\) respectively. The algorithm was run for \(T=10^{4}\) iterations and, to reduce graphical clutter, we plotted only every third point of each trajectory. Trajectories have been colored throughout with darker hues indicating later times (e.g., light blue indicates that the trajectory is closer in time to its starting point, darker shades of blue indicate proximity to the termination time). The algorithm's initial conditions were taken from a uniform initialization grid of the form \(y_{1}\in\{-1,0,1\}^{3}\) and perturbed by a uniform random number in \([-0.1,-0.1]\) to avoid non-generic initializations.

In general, the two defining elements of (RL) are \(a)\) the regularizer of the method; and \(b)\) the feedback available to the players. From our experiments, we conclude that methods with Euclidean regularization tend to have faster identification rates (i.e., converge to the support of an equilibrium / club set faster), but they are more "extreme" than methods with an "entropy-like" regularizer (in the sense that players tend to play pure strategies more often). As for the feedback available to the players, payoff-based methods tend to have higher variance (and hence a slower rate of convergence) relative to methods with full information; otherwise, from a qualitative viewpoint, there are no perceptible differences in their limiting behavior.

Finally, optimistic / extra-oracle methods with full information exhibit better convergence properties in two-player zero-sum games (relative to standard FTRL policies); however, this is a fragile advantage

Figure 2: The long-run behavior of Algorithms 1–3 in a \(2\times 2\times 2\) game. Algorithms 1 and 2 were run with a logit choice map as per (Hedge); Algorithm 3 was run with both variants, EXP3 and Tsallis-INF. All algorithms were run for \(5\times 10^{5}\) iterations with \(\gamma_{t}=1/t^{0.4}\) and \(\delta_{t}=0.1/t^{0.15}\); color indicates time, with darker hues indicating later iterations. The face to the left is closed under better replies, so \(X_{t}\) converges quickly to said face (as per Theorems 2 and 3).

that evaporates in the presence of noise and/or uncertainty (in which case "vanilla" and "optimistic" methods are essentially indistinguishable). We illustrate these findings in Fig. 2.

Regarding Fig. 1, the payoffs of the chosen games were normalized to \([-1,1]\) and players are assumed to choose between two actions labeled "\(O\)" and "1". The specific tableaus are shown in the table below, next to the respective portrait (all taken from Fig. 1.