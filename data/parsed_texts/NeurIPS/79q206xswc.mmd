# Is Your LiDAR Placement Optimized for

3D Scene Understanding?

 Ye Li\({}^{1}\) Lingdong Kong\({}^{2}\) Hanjiang Hu\({}^{3}\) Xiaohao Xu\({}^{1}\) Xiaonan Huang\({}^{1}\)

\({}^{1}\)University of Michigan, Ann Arbor \({}^{2}\)National University of Singapore

\({}^{3}\)Carnegie Mellon University

https://github.com/ywyeli/Place3D

###### Abstract

The reliability of driving perception systems under unprecedented conditions is crucial for practical usage. Latest advancements have prompted increasing interest in multi-LiDAR perception. However, prevailing driving datasets predominantly utilize single-LiDAR systems and collect data devoid of adverse conditions, failing to capture the complexities of real-world environments accurately. Addressing these gaps, we proposed Place3D, a full-cycle pipeline that encompasses LiDAR placement optimization, data generation, and downstream evaluations. Our framework makes three appealing contributions. 1) To identify the most effective configurations for multi-LiDAR systems, we introduce the Surrogate Metric of the Semantic Occupancy Grids (M-SOG) to evaluate LiDAR placement quality. 2) Leveraging the M-SOG metric, we propose a novel optimization strategy to refine multi-LiDAR placements. 3) Centered around the theme of multi-condition multi-LiDAR perception, we collect a 280,000-frame dataset from both clean and adverse conditions. Extensive experiments demonstrate that LiDAR placements optimized using our approach outperform various baselines. We showcase exceptional results in both LiDAR semantic segmentation and 3D object detection tasks, under diverse weather and sensor failure conditions.

## 1 Introduction

Accurate 3D perception plays a crucial role in autonomous driving, involving detecting the objects around the vehicle and segmenting the scene into meaningful semantic categories. LiDARs are becoming crucial for driving perception due to their capability to capture detailed geometric information about the surroundings [3, 109, 53, 54, 118]. While the latest models achieved promising accuracy on standard datasets, _e.g._, nuScenes [7] and SemanticKITTI [6], improving the resilience of perception under corruptions and sensor failures is still a critical yet under-explored task [46, 94, 5, 95].

Recent studies have primarily focused on refining the sensing systems by designing new algorithms with novel model architectures [15] or 3D representations [28, 99, 11]. The selection of LiDAR configurations, however, often relies on industry experience and design aesthetics. Therefore, existing literature has potentially overlooked optimal LiDAR placements for maximum sensing efficacy.

An intuitive approach for optimizing LiDAR configurations involves a comprehensive cycle of data collection, model training, and validation across various LiDAR setups to enhance the autonomous driving system's perception accuracy [24, 52]. However, this approach faces significant challenges due to the substantial computational resources and extensive time required for data collection and processing [57, 17]. Although recent works [34, 65] made preliminary attempts to explore the impactof LiDAR placements on perception accuracy, they have neither proposed an optimization method nor evaluated the performance in adverse conditions.

In this work, we delve into the optimization of sensor configurations for autonomous vehicles by tackling two critical sub-problems: 1) the performance evaluation of sensor-specific configurations, and 2) the optimization of these configurations for enhanced 3D perception tasks, encompassing 3D object detection and LiDAR semantic segmentation. To achieve this goal, we propose a systematic LiDAR placements evaluation and optimization framework, dubbed Place3D. The overall pipeline is endowed with the capability to synthesize point cloud data with customizable configurations and diverse conditions, including common corruptions, external disturbances, and sensor failures.

We first introduce an easy-to-compute Surrogate Metric of Semantic Occupancy Grids (M-SOG) to evaluate the equality of LiDAR placements. Next, we propose a novel optimization approach utilizing our surrogate metric based on Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to find the near-optimal LiDAR placements. To verify the correlation between our surrogate metric and assess the effectiveness of our optimization approach on both clean and adverse conditions, we design an automated multi-condition multi-LiDAR data simulation platform and establish a comprehensive benchmark consisting of a total of seven LiDAR placement baselines inspired by existing self-driving configuration from the autonomous vehicle companies.

Our benchmark, along with a large-scale multi-condition multi-LiDAR perception dataset, encompasses state-of-the-art learning-based perception models for 3D object detection [51; 107; 63; 109] and LiDAR semantic segmentation [18; 111; 86; 117; 50], as well as six distinct adverse conditions coped with weather and sensor failures [46; 95]. Utilizing the proposed framework, we explored how various perturbations and downstream 3D perception tasks affect optimization outcomes.

To summarize, this work makes the following key contributions:

* To the best of our knowledge, Place3D serves as the first attempt at investigating the impact of multi-LiDAR placements for 3D semantic scene understanding in diverse conditions.
* We introduce M-SOG, an innovative surrogate metric to effectively evaluate the quality of LiDAR placements for both detection (sparse 3D) and segmentation (dense 3D) tasks.
* We propose a novel optimization approach utilizing our surrogate metric to refine LiDAR placements, which exhibit excellent LiDAR semantic segmentation and 3D object detection accuracy and robustness, outperforming baselines for relatively 9% in metrics.
* We contribute a 280,000-frame multi-condition multi-LiDAR point cloud dataset and establish a comprehensive benchmark for LiDAR-based 3D scene understanding evaluations. We hope this work can lay a solid foundation for future research in this relevant field.

## 2 Related Work

**LiDAR Sensing.** LiDAR sensing is critical in autonomous vehicles, providing essential structural information for their operation [59; 3; 73]. Utilizing 3D data, LiDAR supports tasks such as semantic scene understanding [7; 26; 29; 6; 85; 48; 10], generation [75; 120; 74; 97], decision making [25; 8; 19; 87], and simulation [23; 68; 67; 93]. This work specifically targets LiDAR-based perception and simulation, which are at the forefront of current research in autonomous vehicle technology.

**LiDAR-Based 3D Scene Understanding.** LiDAR segmentation and 3D object detection are key tasks for 3D scene understanding. Segmentation models are categorized into point-based [88; 35; 36; 110; 77], range view [70; 90; 21; 98; 113; 16; 47; 2; 45; 99], bird's eye view [111; 115; 12], voxel-based [18; 86; 117; 33], and multi-view fusion [58; 38; 78; 60; 62; 14; 13; 61; 100] methods. While they show promising results on benchmarks, their performance across different LiDAR configurations is not well explored. For 3D object detection, models typically use point-based [82; 106; 105; 116] or voxel-based [114; 51; 69; 92; 83; 55; 66; 104] representations, with recent works favoring fusion for improved detection [64; 80; 81; 63; 56]. Our study complements these approaches by optimizing LiDAR placements to enhance performance under various real-world conditions.

**LiDAR Perception Robustness.** The reliability of LiDAR-based 3D scene understanding models in real-world scenarios is crucial [84; 39]. Recent studies have explored the robustness of 3D perception models against adversarial attacks [112; 96], common corruptions [46; 49; 94; 103; 43; 4], adverse weather [31; 30; 79; 37], sensor failures [108; 15; 28], and combined motion and sensor perturbations[102; 95]. To our knowledge, no prior work has focused on optimizing LiDAR placement to improve semantic 3D scene understanding robustness. Our Place3D addresses this gap by studying various placement strategies to enhance robustness in both in-domain and out-of-domain scenarios.

**Sensor Placement Optimization.** Optimizing sensor placements can enhance performance and address the challenges of heuristic design [42; 101]. In autonomous driving, optimizing LiDAR placements is relatively new [65]. Hu _et al._[34] studied multi-LiDAR placements for 3D object detection, while Li _et al._[57] examined LiDAR-camera configurations for multi-modal detection. Other works [41; 44; 9; 40] explored roadside LiDAR placements for V2X applications. Distinguishing from these efforts, our work is the first to investigate LiDAR placements for robust 3D scene understanding in challenging conditions. We establish benchmarks for both LiDAR semantic segmentation and 3D object detection, providing an in-depth analysis of optimal placements for robust perception.

## 3 Place3D: A Full-Cycle LiDAR Placement Pipeline

In this section, we introduce the Surrogate Metric of Semantic Occupancy Grids (M-SOG) to assess the 3D perception performance of sensor configurations (Section 3.2). Leveraging M-SOG, we propose a novel optimization strategy for multi-LiDAR placement (Section 3.3). Our approach is theoretically grounded; we provide an optimality certification to ensure that optimized placements are close to the global optimum (Section 3.4). Figure 1 depicts an overview of our Place3D framework.

### Preliminary Formulation of Surrogate Metric

**Region of Interest.** Following the literature of sensor placement [34; 57], we define the Region of Interest (ROI) for perception as a finite 3D cuboid space \([l,w,h]\) with ego-vehicle in the center, and divide the ROI space \(S\) into voxels with resolution \([\delta_{l},\delta_{w},\delta_{h}]\) as \(S=\{v_{1},v_{2},...,v_{N}\}\), where \(N=\frac{l}{\delta_{l}}\times\frac{w}{\delta_{w}}\times\frac{h}{\delta_{h}}\) denotes the total number of voxels.

**Probabilistic Occupancy Grids (POG).** We define POG as the joint probability of all non-zero voxels in the ROI as \(p_{POG}=p(v_{1},v_{2},...,v_{N})=\prod_{i=1,\ p(v_{i})\neq 0}^{N}p(v_{i})\), where each voxel is assumed to be independently and identically distributed. The voxel \(v_{i}\) is occupied if it is within any 3D bounding box \(b_{c}\) of object class \(c\) at frame \(t\), which is denoted as \(v_{i}=b_{c}^{(t)}\). Therefore, the probability of being occupied for voxel \(v_{i}\) among all \(T\) frames can be estimated as \(p(v_{i})=\sum_{t=1}^{T}\mathds{1}(v_{i}=b_{c}^{(t)})/T\).

**Probabilistic Surrogate Metric.** Now given some LiDAR placement \(L\), the conditional POG can be found as the conditional joint distribution \(p_{POG|L}=p(v_{1},v_{2},...,v_{N}|L)\) in the literature [34].

Figure 1: Place3D **pipeline for multi-LiDAR placement optimization.** We first utilize the semantic point cloud synthesized in CARLA (a) to generate Probabilistic SOG (b) and obtain voxels covered by LiDAR rays to compute M-SOG (c). We propose a CMA-ES-based optimization strategy to maximize M-SOG, finding optimal LiDAR placement (d). To verify the effectiveness of our LiDAR placement optimization strategy, we contribute a multi-condition multi-LiDAR dataset (c) and evaluate the performance of baselines and optimized placements on both clean and corruption data (f).

To show how much uncertainty the placement \(L\) can reduce in the ROI, the naive surrogate metric _S-MIG_[34] is introduced as the reduction of the entropy as \(-H_{POG|L}=-\sum_{i=1}^{N}H(v_{i}|L)\) with constant \(H_{POG}=\sum_{i=1}^{N}H(v_{i})\), where \(H(v_{i})\), \(H(v_{i}|L)\) can be found through the entropy of Bernoulli distribution \(p(v_{i})\), \(p(v_{i}|L)\), respectively.

### M-SOG: Surrogate Metric of Semantic Occupancy Grids

The naive surrogate metric _S-MIG_[34] employs 3D bounding boxes as priors to understand 3D scene distribution. However, this approach exhibits substantial deviations from the actual physical boundaries of the objects and overlooks the occlusion relationships between objects and the environment in LiDAR applications. Furthermore, it is limited to detection tasks only. To overcome these limitations, we propose the Surrogate Metric of Semantic Occupancy Grids to assess and optimize LiDAR placement for 3D scene understanding tasks. Leveraging the Semantic Occupancy Grids derived from diverse environments, our approach can be expanded to tackling adverse conditions.

**Semantic Occupancy Grids (SOG).** Given a 3D driving scene with a set of semantic classes \(Y=\{y_{1},y_{2},...,y_{M}\}\), where \(M\) represents the total number of semantic classes in the scene and assuming that each voxel can only be occupied by one semantic tag for each frame, we denote \(y^{(t)}(v_{i})\) as the semantic class of voxel \(v_{i}\in\{v_{1},v_{2},...,v_{N}\}\) at time frame \(t\in\{1,2,...,T\}\). Notably, empty voxels are also considered a semantic class. Accordingly, the set of voxels occupied by semantic class \(y_{c}\) at frame \(t\) is defined as \(S_{y_{c}}^{(t)}=\{v_{i}|y^{(t)}(v_{i})=y_{c}\},\ c=1,2,...,M,\ t=1,2,...,T\). Then, we introduce SOG to describe the total semantic voxel distribution:

\[S_{SOG}^{(t)}=\{S_{y_{1}}^{(t)},S_{y_{2}}^{(t)},...,S_{y_{M}}^{(t)}\},\ t=1,2,...,T\.\] (1)

**Probabilistic Semantic Occupancy Grids.** In contrast to the Bernoulli distribution _POG_[34], we propose a more accurate Multinomial distribution: Probabilistic Semantic Occupancy Grids (_P-SOG_), denoted as \(p_{SOG}\), to represent the probability distribution of voxels belonging to certain semantic classes. Before estimating \(p_{SOG}\), we first traverse all frames from given scenes to obtain the probability \(\hat{p}\) for each voxel \(v_{i}\) occupied by each semantic class \(y_{c}\):

\[\hat{p}(v_{i}=y_{c})=\frac{|\{t\in\{1,2,\dots,T\}|v_{i}\in S_{y_{c}}^{(t)}\}|} {T},\ c=1,2,\dots,M\.\] (2)

Notably, we denote voxel \(v_{i}\) being occupied by semantic class \(y_{c}\) as \(v_{i}=y_{c}\), and \(\hat{p}\) indicates estimated distributions from observed samples, whereas \(p\) refers to the statistical parameters to be estimated, which are unknown and non-random. We compute the joint probability of all non-zero voxels in the ROI to estimate the \(p_{SOG}\). Following the literature [34], the joint distribution over the voxel set \(S\) is:

\[\hat{p}_{SOG}=\hat{p}(v_{1},v_{2},...,v_{N})=\prod_{i=1,\ \hat{p}(v_{i})\neq 0}^{N} \hat{p}(v_{i})\.\] (3)

The uncertainty of Probabilistic Semantic Occupancy Grids reflects the 3D scene understanding capability of sensors within a given scene. To quantify this uncertainty, we calculate the entropy for the probability distribution of each voxel \(v_{i}\) in \(\hat{p}_{SOG}\) as: \(\hat{H}(v_{i})=-\sum_{c=1}^{M}\hat{p}(v_{i}=y_{c})\log\hat{p}(v_{i}=y_{c})\).

Based on the independent and identically distributed assumption of _SOG_, the entropy of _P-SOG_ over the voxel set \(S\) is given by: \(\hat{H}_{SOG}=\mathds{E}_{v_{i}\sim p_{S}}\sum_{i=1}^{N}\hat{H}(v_{i})\), where \(p_{S}\) denotes the \(p_{SOG}\) over the voxel set \(S\). From the perspective of density estimation, the true _P-SOG_ and its corresponding entropy can be estimated as \(p_{SOG}=\hat{p}_{SOG}\) and \(H_{SOG}=\hat{H}_{SOG}\), respectively.

**Surrogate Metric of Semantic Occupancy Grids.** To evaluate LiDAR placements, we further analyze the joint probability distribution of voxels covered by LiDAR rays with varied LiDAR placements. Leveraging the Amanatides and Woo's Fast Voxel Traversal Algorithm [1], we obtain the voxels covered by LiDAR rays as \(S|L_{j}=\text{FastVT}(S,L_{j})=\{v_{1}^{L_{j}},v_{2}^{L_{j}},...,v_{N_{j}}^{L_{j}}\}\) given LiDAR configuration \(L=L_{j}\), \(j=1,2,...,J\), where \(j\) indexes the LiDAR placements, \(J\) is the number of total configurations, and \(N_{j}\) is the number of voxels covered by rays of LiDAR configuration \(L=L_{j}\). Then, the semantic entropy distribution of _P-SOG_ over the voxel set \(S|L_{j}\) can be estimated as:

\[H_{SOG}^{L_{j}}=\mathds{E}_{v_{i}^{L_{j}}\sim p_{S|L_{j}}}\sum_{i=1}^{N_{j}} \hat{H}(v_{i}^{L_{j}})\.\] (4)We further define the information gain of 3D scene understanding as \(\Delta H=H_{SOG}-H_{SOG}^{L_{j}}\) to describe the perception capability of LiDAR configuration \(L_{j}\). Since \(H_{SOG}\) is a constant given certain 3D semantic scenes with the fixed _P-SOG_ distribution, we propose the normalized Surrogate Metric of Semantic Occupancy Grids (M-SOG) as follows to evaluate the perception capability:

\[M_{SOG}(L_{j}) =-\frac{1}{N_{j}}H_{SOG}^{L_{j}}=-\frac{1}{N_{j}}\mathds{E}_{v_{i }^{L_{j}}\sim p_{S|L_{j}}}\sum_{i=1}^{N_{j}}H(v_{i}^{L_{j}})\] \[=\frac{1}{N_{j}}\sum_{i=1}^{N_{j}}\sum_{c=1}^{M}\hat{p}(v_{i}^{L_{ j}}=y_{c})\log\hat{p}(v_{i}^{L_{j}}=y_{c})\;.\] (5)

### Sensor Configuration Optimization

We adopt the heuristic optimization based on the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [32] to find an optimized LiDAR configuration.

**Objective Function.** We define the objective as \(F(\mathbf{u}_{j})=M_{SOG}(L_{j})\), where \(\mathbf{u}_{j}\in\mathcal{U}\subset\mathbb{R}^{n}\) represents the LiDAR configuration \(L_{j}\) and is subject to the physical constraint \(P(\mathbf{u}_{j})=0\), and \(P(\mathbf{u})>0\) if it is violated, _e.g._, mutual distance between LiDARs and distance from a 2D plane. In our optimization, \(\mathbf{u}_{j}\) includes the 3D coordinates and rolling angles of LiDARs, \(\mathcal{U}\) is the finite cuboid space above the vehicle roof. Without ambiguity, we omit subscript \(j\) in the following text. We then transform the constrained optimization into the unconstrained Lagrangian form as \(G(\mathbf{u})=-F(\mathbf{u})+\lambda P(\mathbf{u})\), where \(\lambda\) is the Lagrange multiplier. We optimize \(G(\mathbf{u})\) through an iterative process that adapts the distribution of candidate solutions as Algorithm 1.

**Optimization Approach.** We first define a multivariate normal distribution \(\mathcal{N}(\mathbf{m}^{(k)},(\sigma^{(k)})^{2}\mathbf{C}^{(k)})\), where \(\mathbf{m}^{(k)}\), \(\sigma^{(k)}\), and \(\mathbf{C}^{(k)}\) are the mean vector, step size, and covariance matrix of the distribution of iteration \(k\), respectively. We discretize the configuration space \(\mathcal{U}\) with density \(\delta\) and sample

Figure 2: **Visualized LiDAR Placements.** We compare the LiDAR placements optimized from our proposed M-SOG metric (for LiDAR semantic segmentation and 3D object detection) and heuristic LiDAR placements utilized by major autonomous vehicle companies (see Appendix Section B.1).

candidates \(u_{i}^{(k)}\sim\mathcal{N}(\mathbf{m}^{(k)},(\sigma^{(k)})^{2}\mathbf{C}^{(k)})\) in each iteration \(k\), where \(i\) indexes the candidates. We update mean vector \(\mathbf{m}^{k+1}\) for the next iteration \(k+1\) as the updated center of the search distribution for LiDAR configuration. The overall process can be depicted as follows:

\[\mathbf{m}^{(k+1)}=\sum_{i=1}^{M_{k}}w_{i}\hat{\mathbf{u}}_{i}^{(k)},\ G(\hat{ \mathbf{u}}_{1}^{(k)})\leq G(\hat{\mathbf{u}}_{2}^{(k)})\leq\cdots\leq G(\hat{ \mathbf{u}}_{M_{k}}^{(k)})\,\] (6)

where \(M_{k}\) is the number of best solutions we adopt to generate \(\mathbf{m}^{(k+1)}\), and \(w_{i}\) are the weights based on solution fitness. We obtain the evolution path \(\mathbf{p}_{\mathbf{C}}^{(k+1)}\) that accumulates information about the direction of successful steps as follows:

\[\mathbf{p}_{\mathbf{C}}^{(k+1)}=(1-c_{\mathbf{C}})\cdot\mathbf{p}_{\mathbf{C} }^{(k)}+\sqrt{1-(1-c_{\mathbf{C}})^{2}}\cdot\sqrt{\frac{1}{\sum_{i=1}^{M_{k}} w_{i}^{2}}}\cdot\frac{\mathbf{m}^{(k+1)}-\mathbf{m}^{(k)}}{\sigma^{(k)}}\,\] (7)

where \(c_{\mathbf{C}}\) is the learning rate for the covariance matrix update. The covariance matrix \(\mathbf{C}\) controls the shape and orientation of the search distribution for LiDAR configurations. It can be updated at each iteration \(k\) in the following format:

\[\mathbf{C}^{(k+1)}=(1-c_{\mathbf{C}})\mathbf{C}^{(k)}+c_{\mathbf{C}}\mathbf{p }_{\mathbf{C}}^{(k+1)}\mathbf{p}_{\mathbf{C}}^{(k+1)^{T}}\.\] (8)

Similarly, we update \(\mathbf{p}_{\sigma}\) as the evolution path for step size adaptation. Then, the global step size \(\sigma\) can be found below for the scale of search to balance exploration and exploitation:

\[\mathbf{p}_{\sigma}^{(k+1)}=(1-c_{\sigma})\mathbf{p}_{\sigma}^{(k)}+\sqrt{1-( 1-c_{\sigma})^{2}}\cdot\sqrt{\frac{1}{\sum_{i=1}^{M_{k}}w_{i}^{2}}}\cdot\frac{ \mathbf{m}^{(k+1)}-\mathbf{m}^{(k)}}{\sigma^{(k)}}\,\] (9)

\[\sigma^{(k+1)}=\sigma^{(k)}\exp\left(\frac{c_{\sigma}}{d_{\sigma}}\left(\frac{ \|\mathbf{p}_{\sigma}^{(k+1)}\|}{E\|\mathcal{N}(\mathbf{0},\mathbf{I})\|}-1 \right)\right)\,\] (10)

where \(c_{\sigma}\) is the learning rate for updating the evolution path \(\mathbf{p}_{\sigma}\). \(d_{\sigma}\) is a normalization factor to calibrate the pace at which the global step size is adjusted.

### Theoretical Analysis

Once the evolution optimization empirically converges, it holds that the optimized solution is the local optima of the \(\delta\)-density Grids space of LiDAR configuration space. In this section, we provide a stronger optimality certification to theoretically ensure that the optimized placement is close to the global optimum under the assumption of bounded and smooth objective function. Due to space limits, the full proof has been attached to the Appendix Section E.

**Theorem 1** (Optimality Certification).: _Given the continuous objective function \(G:\mathbb{R}^{n}\rightarrow\mathbb{R}\) with Lipschitz constant \(k_{G}\) w.r.t. input \(\boldsymbol{u}\in\mathcal{U}\subset\mathbb{R}^{n}\) under \(\ell_{2}\) norm, suppose over a \(\delta\)-density Grids subset \(S\subset\mathcal{U}\), the distance between the maximal and minimal of function \(G\) over \(S\) is upper-bounded by \(C_{M}\), and the local optima is \(\boldsymbol{u}_{S}^{*}=\arg\min_{\boldsymbol{u}\in S}G(x)\), the following optimality certification regarding \(x\in\mathcal{U}\) holds that:_

\[\|G(\boldsymbol{u}^{*})-G(\boldsymbol{u}_{S}^{*})\|_{2}\leq C_{M}+k_{G}\delta\,\] (11)

_where \(\boldsymbol{u}^{*}\) is the global optima over \(\mathcal{U}\)._

The global optimality certification Theorem 1 is applicable in practice because the Lipschitz constant \(k_{G}\) and the distance between the maximal and minimal of objective function \(G\) over \(S\) can be approximated easily through calculating \(G(\mathbf{u}_{i}^{(k)})\) of Algorithm 1 for each sampled \(\mathbf{u}_{i}^{(k)}\) over the \(\delta\)-density Grids subset. Besides, we have a more general corollary below to further relax the assumption of bounded objective function.

**Corollary 1**.: _When \(\mathcal{U}\) is a hyper-rectangle with the bounded \(\ell_{2}\) norm of domain \(U_{i}\in\mathbb{R}\) at each dimension \(i\), with \(i=1,2,\ldots,n\), Theorem 1 can hold in a more general way by only assuming that the Lipschitz constant \(k_{G}\) of the objective function is given, where Equation (11) becomes:_

\[\|G(\boldsymbol{u}^{*})-G(\boldsymbol{u}_{S}^{*})\|_{2}\leq k_{G}\sum_{i=1}^{n} U_{i}+k_{G}\delta\.\] (12)

## 4 Experiments

### Benchmark Setups

**Data Generation.** We generate LiDAR point clouds and ground truth using CARLA [23]. We use the maps of Towns 1, 3, 4, and 6 and set 6 ego-vehicle routes for each map. We incorporate 23 semantic classes for LiDAR semantic segmentation and 3 instance classes for 3D object detection. Data collection is performed for 10 LiDAR placements, resulting in a total of 280,000 frames: 1) For each placement, we gather 340 clean scenes and 360 corrupted scenes, with each scene consisting of 40 frames. 2) The clean set comprises 13,600 frames, including 11,200 samples (280 scenes) for training and 2,400 samples (60 scenes) for validation, following the split ratio used in nuScenes [7]. 3) The corruption set is used for robustness evaluation and contains 6 different conditions, each with 2,400 samples (60 scenes). More details are in the Appendix Section A.

**LiDAR Configurations.** We adopt five commonly employed heuristic LiDAR placements, which have been adopted by leading autonomous driving companies, as our baseline. These placements are represented in Figure 1(a)-e. Following KITTI [29], we configured the LiDAR sensor with a vertical field of view of [-24.8, 2.0] degrees. To achieve the _Line-roll_ and _Pyramid-roll_ configurations, we adjusted LiDAR roll angles for the _Line_ and _Pyramid_ setups, as depicted in Figure 2. We present detailed LiDAR configurations in the Appendix Section B.1.

**Corruption Types.** To replicate adverse conditions, we synthesized six types of corrupted point clouds on the validation set of each sub-set, following the settings of the Robo3D benchmark [46]. These corruptions can be categorized into 1) severe weather conditions, including "snow", "fog", and "wet ground", 2) external disturbances, including "motion blur", and 3) internal sensor failures, including "crosstalk" and "incomplete echo". We include the features and implementation details of corruptions in the Appendix Section B.2.

**P-SOG Synthesis.** We follow the steps as depicted in Figure 3 to generate semantic occupancy grids for each LiDAR scene. We first collect point clouds and semantic labels using high-resolution LiDARs and sequentially divide all samples into multiple subsets. Through the transformation of world coordinates of the ego-vehicle, frames of point clouds of each subset are aggregated into one frame of dense point cloud. Then, we utilize the voting strategy to determine the semantic label for each voxel in ROI and generate the SOG. This process is executed across all subsets to produce P-SOG. Notably, the P-SOG is only generated on the scenes of the training set.

**Surrogate Metric of SOG.** We compute the scores of M-SOG separately for 3D detection and segmentation, as shown in Table 1. To evaluate M-SOG for segmentation, we utilize all semantic classes to generate semantic occupancy grids. For detection, we specifically focus on the _car_ semantic type, while merging the remaining semantic classes into a single category for M-SOG analysis.

\begin{table}
\begin{tabular}{l|c c c c c c|c} \hline \hline
**Metrics** [\(M_{SOG}\)] & Center & Line & Pyramid & Square & Trapezoid & L-roll & P-roll & **Ours** \\ \hline \hline
3D Detection (\(\times 10^{-6}\)) & \(-1.26\) & \(-1.65\) & \(-1.34\) & \(-1.54\) & \(-1.52\) & \(-1.41\) & \(-1.35\) & \(-1.18\) \\
3D Segmentation (\(\times\)\(10^{-6}\)) & \(-1.58\) & \(-2.62\) & \(-2.56\) & \(-2.35\) & \(-2.89\) & \(-3.13\) & \(-1.63\) & \(-1.29\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Optimized M-SOG Metrics.** We report the \(M_{SOG}\) scores for both detection and segmentation tasks in our pipeline. The scores are calculated based on the _car_ class for detection and all semantic classes for segmentation. _Line-roll_ and _Pyramid-roll_ are abbreviated as _L-roll_ and _P-roll_.

Figure 3: **Pipeline of Probabilistic SOG generation.** We first merge multiple frames of raw point clouds (a) into dense point clouds (b). Then, we voxelize dense point clouds into SOG, _i.e._, semantic occupancy grids (c), and traverse all frames of dense point clouds to synthesize probabilistic SOG (d).

**Detector and Segmentor.** For the benchmark, we conduct experiments using four LiDAR semantic segmentation models, _i.e._, MinkUNet [18], SPVCNN [86], PolarNet [111], and Cylinder3D [117], and four 3D object detection models, _i.e._, PointPillars [51], CenterPoint [107], BEVFusion-L [63], and FSTR [109]. The detailed training configurations are included in the Appendix Section B.3.

### Comparative Study

We conduct benchmark studies to evaluate the performance of varied LiDAR placements in both clean and adverse conditions. We extensively examine the correlation between the proposed surrogate metric, known as M-SOG, and the final performance results. Through our analysis, we are able to demonstrate the effectiveness and robustness of the entire Place3D framework.

**Effectiveness of M-SOG Surrogate Metric in Place3D.** In Table 1, we report the scores of the M-SOG surrogate metric for various LiDAR placements. In Tables 2 and 3, we benchmark the 3D object detection and LiDAR semantic segmentation performance of varied LiDAR placements with state-of-the-art algorithms. Figure 4 illustrates the correlation between the proposed M-SOG

\begin{table}
\begin{tabular}{r|c c c|c c c c|c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{2}{c|}{**Center**} & \multicolumn{2}{c|}{**Line**} & \multicolumn{2}{c|}{**Pyramid**} & \multicolumn{2}{c}{**Square**} \\  & \multicolumn{1}{c}{Mink} & \multicolumn{1}{c}{SPV} & \multicolumn{1}{c|}{Cy3D} & \multicolumn{1}{c|}{Mink} & \multicolumn{1}{c}{SPV} & \multicolumn{1}{c|}{Cy3D} & \multicolumn{1}{c|}{Mink} & \multicolumn{1}{c}{SPV} & \multicolumn{1}{c}{Cy3D} & \multicolumn{1}{c}{Mink} & \multicolumn{1}{c}{SPV} & \multicolumn{1}{c}{Cy3D} \\ \hline \hline Clean \(\bullet\) & \(65.7\) & \(67.1\) & \(72.7\) & \(59.7\) & \(59.3\) & \(68.9\) & \(62.7\) & \(67.6\) & \(68.4\) & \(60.7\) & \(63.4\) & \(69.9\) \\ \hline Fog \(\circ\) & \(55.9\) & \(39.3\) & \(55.6\) & \(51.7\) & \(42.8\) & \(55.5\) & \(52.9\) & \(48.6\) & \(51.0\) & \(55.6\) & \(40.7\) & \(52.0\) \\ Wet Ground \(\circ\) & \(63.8\) & \(66.6\) & \(64.4\) & \(60.2\) & \(57.9\) & \(66.4\) & \(60.3\) & \(66.6\) & \(52.2\) & \(61.9\) & \(64.3\) & \(55.6\) \\ Show \(\circ\) & \(25.1\) & \(35.6\) & \(16.7\) & \(35.5\) & \(31.3\) & \(4.7\) & \(25.2\) & \(30.2\) & \(5.0\) & \(33.5\) & \(38.3\) & \(2.7\) \\ Motion Blur \(\circ\) & \(35.8\) & \(35.6\) & \(37.6\) & \(52.0\) & \(46.1\) & \(39.4\) & \(50.7\) & \(55.1\) & \(42.5\) & \(51.5\) & \(53.9\) & \(44.2\) \\ Crosstalk \(\circ\) & \(24.7\) & \(19.5\) & \(36.9\) & \(27.1\) & \(13.6\) & \(34.3\) & \(17.3\) & \(14.8\) & \(26.6\) & \(26.5\) & \(18.6\) & \(37.1\) \\ Incomplete Echo \(\circ\) & \(64.5\) & \(68.7\) & \(71.5\) & \(59.2\) & \(57.1\) & \(68.3\) & \(60.2\) & \(65.9\) & \(60.9\) & \(61.2\) & \(63.7\) & \(68.7\) \\ \hline
**Average \(\bullet\)** & \(45.0\) & \(43.9\) & \(47.4\) & \(47.6\) & \(41.5\) & \(44.8\) & \(44.4\) & \(46.9\) & \(39.7\) & \(48.4\) & \(46.6\) & \(43.4\) \\ \hline \multirow{2}{*}{**Method**} & \multicolumn{2}{c|}{**Trapeid**} & \multicolumn{2}{c|}{**Line-Roll**} & \multicolumn{2}{c|}{**Pyramid-Roll**} & \multicolumn{2}{c}{**Ours**} \\  & \multicolumn{1}{c}{Mink} & \multicolumn{1}{c}{SPV} & \multicolumn{1}{c}{Cy3D} & \multicolumn{1}{c}{Mink} & \multicolumn{1}{c}{SPV} & \multicolumn{1}{c}{Cy3D} & \multicolumn{1}{c}{Mink} & \multicolumn{1}{c}{SPV} & \multicolumn{1}{c}{Cy3D} & \multicolumn{1}{c}{Mink} & \multicolumn{1}{c}{SPV} & \multicolumn{1}{c}{Cy3D} \\ \hline \hline Clean \(\bullet\) & \(59.0\) & \(61.0\) & \(68.5\) & \(58.5\) & \(60.6\) & \(69.8\) & \(62.2\) & \(67.9\) & \(69.3\) & \(66.5\) & \(68.3\) & \(73.0\) \\ \hline Fog \(\circ\) & \(49.7\) & \(40.9\) & \(52.1\) & \(48.6\) & \(42.2\) & \(49.7\) & \(52.2\) & \(47.2\) & \(50.7\) & \(59.5\) & \(59.1\) & \(57.6\) \\ Wet Ground \(\circ\) & \(60.4\) & \(61.3\) & \(64.6\) & \(59.2\) & \(62.0\) & \(65.4\) & \(60.9\) & \(67.1\) & \(67.9\) & \(66.6\) & \(66.7\) & \(67.2\) \\ Snow \(\circ\) & \(27.6\) & \(36.3\) & \(3.1\) & \(26.9\) & \(27.0\) & \(2.6\) & \(26.6\) & \(31.6\) & \(2.1\) & \(17.6\) & \(24.0\) & \(5.9\) \\ Motion Blur \(\circ\) & \(51.7\) & \(49.1\) & \(36.7\) & \(50.4\) & \(49.9\) & \(37.4\) & \(52.5\) & \(56.5\) & \(44.1\) & \(56.7\) & \(56.0\) & \(48.7\) \\ Crosstalk \(\circ\) & \(18.4\) & \(16.9\) & \(30.0\) & \(21.2\) & \(16.5\) & \(27.3\) & \(19.3\) & \(13.7\) & \(31.9\) & \(24.5\) & \(18.7\) & \(41.0\) \\ Incomplete Echo \(\circ\) & \(59.3\) & \(60.7\) & \(65.6\) & \(58.0\) & \(61.0\) & \(67.8\) & \(60.8\) & \(66.7\) & \(70.0\) & \(66.9\) & \(66.9\) & \(63.3\) \\ \hline \multirow{2}{*}{**Average \(\bullet\)**} & \(44.5\) & \(43.8\) & \(42.0\) & \(44.1\) & \(43.1\) & \(41.7\) & \(45.4\) & \(47.1\) & \(44.5\) & \(48.6\) & \(48.6\) & \(47.3\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Performance evaluations of LiDAR semantic segmentation under clean and adverse conditions. For each LiDAR placement strategy, we report the mIoU scores (\(\uparrow\)), represented in percentage (\(\%\)). The average scores only include adverse scenarios.**

\begin{table}
\begin{tabular}{r|c c c c|c c c c|c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{2}{c|}{**Center**} & \multicolumn{2}{c|}{**Line**} & \multicolumn{2}{c|}{**Pyramid**} & \multicolumn{2}{c}{**Square**} \\  & \multicolumn{1}{c}{Pillar} & \multicolumn{1}{c}{Center} & \multicolumn{1}{c}{BEV}

surrogate metric and perception capacity. The results demonstrate a clear correlation, where the performance generally improves as the M-SOG increases. While there might be fluctuations in some placements with specific algorithms, the overall relationship follows a linear correlation, highlighting the effectiveness of our M-SOG for computationally efficient sensor optimization purposes. We show more plots in the Appendix Sections C.1 and C.2.

**Comparisons to Existing Sensor Placement Methods.** The effectiveness of LiDAR sensor placement metrics lies in the linear correlation between metrics and actual performance. We set the same ROI size and conduct a quantitative comparison with _S-MIG_[34] on 3D detection (as _S-MIG_[34] only works for 3D detection task). As shown in Figure 5, our metric exhibits better linear correlation. Since we introduce semantic occupancy information as a prior for the evaluation metric, our method more accurately characterizes the boundaries in the 3D semantic scene and addresses the issue of objects and environment occlusions when using 3D bounding boxes as priors. Moreover, our LiDAR placement method makes the first attempt to include both detection and segmentation tasks. We present additional comparisons in the Appendix Section C.3.

**Superiority of Optimization via Place3D.** As shown in Tables 2 and 3, our optimized configurations achieved the best performance in both segmentation and detection tasks under both clean and adverse conditions among all models. We report per-class quantitative results in the Appendix Sections C.1 and C.2. The improvement from optimization remains significant even when comparing our optimized configurations against the best-performing baseline in each task. For segmentation, optimized placements outperform the best-performing baseline by up to 0.8% on clean datasets and by as much as 1.5% on corruption. For detection, optimized configurations exceed the best-performing baseline by up to 0.7% on clean datasets and by up to 0.3% on corruption.

**Robustness of Optimization via Place3D.** The M-SOG utilizes semantic occupancy as prior knowledge, which is invariant to changing conditions, thereby enabling robustness of optimized placement in adverse weather. Under the corrupted setting, although the correlation between M-SOG and perception performance is not as obvious as that in the clean setting and shows some fluctuation, our optimized LiDAR configuration consistently maintained its performance in adverse conditions, mirroring its effectiveness in the clean condition. While the top-performing baseline LiDAR configurations in clean datasets might be notably worse compared to others when faced with corruption, the optimized configuration via Place3D consistently shows the best performance under adverse conditions. We showcase several qualitative results on this aspect in the Appendix Section D.

**Intuitive Interpretation.** We observe several intuitive reasons why refined sensor placement is beneficial in our experiments. 1) LiDAR heights. Increasing the average height of the 4 LiDARs improves performance, likely due to an expanded field of view. 2) Variation in heights. A greater

Figure 4: The correlation between M-SOG and LiDAR semantic segmentation [18, 86, 111, 117] models performance in the _clean_ condition.

Figure 5: Comparisons of M-SOG with S-MIG [34] using BEVFusion-L [63] and PointPillars [51].

difference in LiDAR heights enhances perception, as varied height distributions capture richer features from the sides of objects. 3) Uniform distribution. The pyramid placement performs better in segmentation, as a more spread-out and uniform distribution of 4 LiDARs captures a detailed surface.

### Ablation Study

In this section, we further analyze the interplay between our proposed optimization strategy and perception performance to address two questions: 1) _How does our optimization strategy inform LiDAR placements to improve robustness against various forms of corruption?_ 2) _How can our optimization strategy enhance perception performance in scenarios with limited placement options?_

**Optimizing Against Corruptions.** In the previous subsection, we deploy the placement optimized on the clean point clouds directly on corrupted ones to assess the robustness. To further showcase the capability of our method against corruptions, we compute the M-SOG scores utilizing the P-SOG derived from the corrupted semantic point clouds under adverse conditions and perform the optimization process, as shown in Figure 5(a). Quantitatively compared with _Ours_ in Table 2, the configurations derived from optimization on adverse data outperform the configurations generated solely on clean data. These results indicate that customizable optimization tailored for adverse settings is an effective approach to enable robust 3D perception.

**Optimizing 2D Placements.** Placing LiDARs at different heights on the roof might affect automotive aesthetics and aerodynamics. Therefore, we investigate the effect of our optimization algorithm in the presence of constraints, by limiting the LiDARs to the same height and considering only 2D placements. We fix the height of all LiDARs to find the optimal placement on the horizontal plane of the vehicle roof. We use the _Line_, _Square_, and _Trapezoid_ placements in Figure 2 for comparison. Experimental results in Figures 5(b) and 5(c) show that our optimized LiDAR placements outperform baseline placements at the same LiDAR height on both clean and corruption settings.

**Influence of LiDAR Roll Angles.** Fine-tuning the orientation angles of the LiDARs on autonomous vehicles is an effective strategy to minimize blind spots and broaden the perception range. For 3D segmentation, _Pyramid-roll_ slightly outperforms _Pyramid_, whereas _Line-roll_ falls short of _Line_, which is highly aligned with scores of M-SOG in Table 1. For 3D detection tasks, the situation is reversed, but still consistent with the M-SOG results. This suggests that adjusting the LiDAR's angle can have varying impacts on the performance of 3D object detection and LiDAR semantic segmentation.

## 5 Conclusion

In this work, we presented Place3D, a comprehensive and systematic LiDAR placement evaluation and optimization framework. We introduced the Surrogate Metric of Semantic Occupancy Grids (M-SOG) as a novel metric to assess the impact of various LiDAR placements. Building on this metric, we culminated an optimization approach for LiDAR configuration that significantly enhances LiDAR semantic segmentation and 3D object detection performance. We validate the effectiveness of our optimization approach through a multi-condition multi-LiDAR point cloud dataset and establish a comprehensive benchmark for evaluating both baseline and our optimized LiDAR placements on detection and segmentation in diverse conditions. Our optimized placements demonstrate superior robustness and perception capabilities, outperforming all baseline configurations. By shedding light on refining the robustness of LiDAR placements for both tasks under diverse driving conditions, we anticipate that our work will pave the way for further advancements in this field of research.

Figure 6: Ablation results (mIoU) of placement strategies on segmentation models [111, 117, 86, 111].

## Acknowledgments

This work was supported by the Robotics Department at the University of Michigan, Ann Arbor.

## References

* [1] John Amanatides and Andrew Woo. A fast voxel traversal algorithm for ray tracing. _Eurographics_, 87(3):3-10, 1987.
* [2] Angelika Ando, Spyros Gidaris, Andrei Bursuc, Gilles Puy, Alexandre Boulch, and Renaud Marlet. Rangevit: Towards vision transformers for 3d semantic segmentation in autonomous driving. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5240-5250, 2023.
* [3] Claudine Badue, Ranik Guidolini, Raphael Vivacqua Carneiro, Pedro Azevedo, Vinicius B. Cardoso, Avelino Forechi, Luan Jesus, Rodrigo Berriel, Thiago M. Paixao, Filipe Mutz, Lucas de Paula Veronese, Thiago Oliveira-Santos, and Alberto F. De Souza. Self-driving cars: A survey. _Expert Systems with Applications_, 165:113816, 2021.
* [4] Till Beemelmanns, Quan Zhang, and Lutz Eckstein. Multicorrupt: A multi-modal robustness dataset and benchmark of lidar-camera fusion for 3d object detection. In _IEEE Intelligent Vehicles Symposium_, pages 3255-3261, 2024.
* [5] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Jurgen Gall, and Cyrill Stachniss. Towards 3d lidar-based semantic scene understanding of 3d point cloud sequences: The semantickitti dataset. _International Journal of Robotics Research_, 40:959-96, 2021.
* [6] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Juergen Gall. Semantickitti: A dataset for semantic scene understanding of lidar sequences. In _IEEE/CVF International Conference on Computer Vision_, pages 9297-9307, 2019.
* [7] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11621-11631, 2020.
* [8] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom, and Sammy Omari. nulpan: A closed-loop ml-based planning benchmark for autonomous vehicles. _arXiv preprint arXiv:2106.11810_, 2021.
* [9] Xinyu Cai, Wentao Jiang, Runsheng Xu, Wenquan Zhao, Jiaqi Ma, Si Liu, and Yikang Li. Analyzing infrastructure lidar placement with realistic lidar simulation library. In _International Conference on Robotics and Automation_, pages 5581-5587, 2023.
* [10] Anh-Quan Cao and Raoul De Charette. Monoscene: Monocular 3d semantic scene completion. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3991-4001, 2022.
* [11] Anh-Quan Cao, Angela Dai, and Raoul de Charette. Pasco: Urban 3d panoptic scene completion with uncertainty awareness. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14554-14564, 2024.
* [12] Qi Chen, Sourabh Vora, and Oscar Beijbom. Polarstream: Streaming lidar object detection and segmentation with polar pillars. In _Advances in Neural Information Processing Systems_, volume 34, pages 26871-26883, 2021.
* [13] Runnan Chen, Youquan Liu, Lingdong Kong, Nenglun Chen, Xinge Zhu, Yuexin Ma, Tongliang Liu, and Wenping Wang. Towards label-free scene understanding by vision foundation models. In _Advances in Neural Information Processing Systems_, volume 36, pages 75896-75910, 2023.
* [14] Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, and Wenping Wang. Clip2scene: Towards label-efficient 3d scene understanding by clip. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7020-7030, 2023.
* [15] Siran Chen, Yue Ma, Yu Qiao, and Yali Wang. M-bev: Masked bev perception for robust autonomous driving. In _AAAI Conference on Artificial Intelligence_, pages 1183-1191, 2023.
* [16] Huixian Cheng, Xianfeng Han, and Guoqiang Xiao. Cenet: Toward concise and efficient lidar semantic segmentation for autonomous driving. In _IEEE International Conference on Multimedia and Expo_, pages 1-6, 2022.

* [17] Huixian Cheng, Xianfeng Han, and Guoqiang Xiao. Transvnet: Lidar semantic segmentation with transformer. _IEEE Transactions on Intelligent Transportation Systems_, 24(6):5895-5907, 2023.
* [18] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3075-3084, 2019.
* [19] Laurene Claussmann, Marc Revilloud, Dominique Gruyer, and Sebastien Glaser. A review of motion planning for highway autonomous driving. _IEEE Transactions on Intelligent Transportation Systems_, 21(5):1826-1848, 2019.
* [20] MMDetection3D Contributors. MMDetection3D: OpenMMLab next-generation platform for general 3D object detection. https://github.com/open-mmlab/mmdetection3d, 2020.
* [21] Tiago Cortinhal, George Tzelepis, and Eren Erdal Aksoy. Salsanext: Fast, uncertainty-aware semantic segmentation of lidar point clouds. In _International Symposium on Visual Computing_, pages 207-222, 2020.
* [22] Cruise. San francisco seeks stop sign on driverless cars. https://calmatters.org/newsletters/w hatmatters/2023/08/driverless-cars-california, Accessed: 2024-03-06, 2022.
* [23] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In _Conference on Robot Learning_, pages 1-16, 2017.
* [24] Bertrand Douillard, James Underwood, Noah Kuntz, Vsevolod Vlaskine, Alastair Quadros, Peter Morton, and Alon Frenkel. On the segmentation of 3d lidar point clouds. In _IEEE International Conference on Robotics and Automation_, pages 2798-2805, 2011.
* [25] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles R. Qi, Yin Zhou, Zoey Yang, Aurelien Chouard, Pei Sun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley, Jonathon Shlens, and Dragomir Anguelov. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In _IEEE/CVF International Conference on Computer Vision_, pages 9710-9719, 2021.
* [26] Whye Kit Fong, Rohit Mohan, Juana Valeria Hurtado, Lubing Zhou, Holger Caesar, Oscar Beijbom, and Abhinav Valada. Panoptic uscenes: A large-scale benchmark for lidar panoptic segmentation and tracking. _IEEE Robotics and Automation Letters_, 7:3795-3802, 2022.
* [27] Ford. How ford's next-gen test vehicle lays the foundation for our self-driving business. https://medium.com/self-driven/how-fords-next-gen-test-vehicle-lays-the-foundation-f or-our-self-driving-business-aadbf247b6ce, Accessed: 2024-03-06, 2020.
* [28] Chongjian Ge, Junsong Chen, Enze Xie, Zhongdao Wang, Lanqing Hong, Huchuan Lu, Zhenguo Li, and Ping Luo. Metabev: Solving sensor failures for 3d detection and map segmentation. In _IEEE/CVF International Conference on Computer Vision_, pages 8721-8731, 2023.
* [29] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3354-3361, 2012.
* [30] Martin Hahner, Christos Sakaridis, Mario Bijelic, Felix Heide, Fisher Yu, Dengxin Dai, and Luc Van Gool. Lidar snowfall simulation for robust 3d object detection. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16364-16374, 2022.
* [31] Martin Hahner, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Fog simulation on real lidar point clouds for 3d object detection in adverse weather. In _IEEE/CVF International Conference on Computer Vision_, pages 15283-15292, 2021.
* [32] Nikolaus Hansen. The cma evolution strategy: A tutorial. _arXiv preprint arXiv:1604.00772_, 2016.
* [33] Fangzhou Hong, Lingdong Kong, Hui Zhou, Xinge Zhu, Hongsheng Li, and Ziwei Liu. Unified 3d and 4d panoptic segmentation via dynamic shifting networks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 46(5):3480-3495, 2024.
* [34] Hanjiang Hu, Zuxin Liu, Sharad Chitlangia, Akhil Agnihotri, and Ding Zhao. Investigating the impact of multi-lidar placement on object detection for autonomous driving. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2550-2559, 2022.

* [35] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. Randla-net: Efficient semantic segmentation of large-scale point clouds. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11108-11117, 2020.
* [36] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. Learning semantic segmentation of large-scale point clouds with random sampling. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(11):8338-8354, 2022.
* [37] Yihao Huang, Kaiyuan Yu, Qing Guo, Felix Juefei-Xu, Xiaojun Jia, Tianlin Li, Geguang Pu, and Yang Liu. Improving robustness of lidar-camera fusion model against weather corruption from fusion strategy perspective. _arXiv preprint arXiv:2402.02738_, 2024.
* [38] Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Emilie Wirbel, and Patrick Perez. xmuda: Cross-modal unsupervised domain adaptation for 3d semantic segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12605-12614, 2020.
* [39] Peng Jiang, Philip Osteen, Maggie Wigness, and Srikanth Saripallig. Rellis-3d dataset: Data, benchmarks and analysis. In _IEEE International Conference on Robotics and Automation_, pages 1110-1116, 2021.
* [40] Wentao Jiang, Hao Xiang, Xinyu Cai, Runsheng Xu, Jiaqi Ma, Yikang Li, Gim Hee Lee, and Si Liu. Optimizing the placement of roadside lidars for autonomous driving. In _IEEE/CVF International Conference on Computer Vision_, pages 18381-18390, 2023.
* [41] Shaojie Jin, Ying Gao, Fei Hui, Xiangmo Zhao, Cheng Wei, Tao Ma, and Weihao Gan. A novel information theory-based metric for evaluating roadside lidar placement. _IEEE Sensors Journal_, 22(21):21009-21023, 2022.
* [42] Siddharth Joshi and Stephen Boyd. Sensor selection via convex optimization. _IEEE Transactions on Signal Processing_, 57(2):451-462, 2008.
* [43] Oquzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir Zamir. 3d common corruptions and data augmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8963-18974, 2022.
* [44] Tae-Hyeong Kim, Gi-Hwan Jo, Hyeong-Seok Yun, Kyung-Su Yun, and Tae-Hyoung Park. Placement method of multiple lidars for roadside infrastructure in urban environments. _Sensors_, 23(21):8808, 2023.
* [45] Lingdong Kong, Youquan Liu, Runnan Chen, Yuexin Ma, Xinge Zhu, Yikang Li, Yuenan Hou, Yu Qiao, and Ziwei Liu. Rethinking range view representation for lidar segmentation. In _IEEE/CVF International Conference on Computer Vision_, pages 228-240, 2023.
* [46] Lingdong Kong, Youquan Liu, Xin Li, Runnan Chen, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei Liu. Robo3d: Towards robust and reliable 3d perception against corruptions. In _IEEE/CVF International Conference on Computer Vision_, pages 19994-20006, 2023.
* [47] Lingdong Kong, Niamul Quader, and Venice Erin Liong. Conda: Unsupervised domain adaptation for lidar segmentation via regularized domain concatenation. In _IEEE International Conference on Robotics and Automation_, pages 9338-9345, 2023.
* [48] Lingdong Kong, Jiawei Ren, Liang Pan, and Ziwei Liu. Lasermix for semi-supervised lidar semantic segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 21705-21715, 2023.
* [49] Lingdong Kong, Shaoyuan Xie, Hanjiang Hu, Lai Xing Ng, Benoit R. Cottereau, and Wei Tsang Ooi. Robodepth: Robust out-of-distribution depth estimation under corruptions. In _Advances in Neural Information Processing Systems_, volume 36, pages 21298-21342, 2023.
* [50] Lingdong Kong, Xiang Xu, Jiawei Ren, Wenwei Zhang, Liang Pan, Kai Chen, Wei Tsang Ooi, and Ziwei Liu. Multi-modal data-efficient 3d scene understanding for autonomous driving. _arXiv preprint arXiv:2405.05258_, 2024.
* [51] Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12697-12705, 2019.
* [52] Li Li, Hubert P. H. Shum, and Toby P. Breckon. Less is more: Reducing task and model complexity for 3d point cloud semantic segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9361-9371, 2023.

* [53] Rong Li, Raoul de Charette, and Anh-Quan Cao. Coarse3d: Class-prototypes for contrastive learning in weakly-supervised 3d point cloud segmentation. In _British Machine Vision Conference_, 2022.
* [54] Rong Li, Shijie Li, Xieyuanli Chen, Teli Ma, Wang Hao, Juergen Gall, and Junwei Liang. Tfnet: Exploiting temporal cues for fast and accurate lidar semantic segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_, pages 4547-4556, 2024.
* [55] Xin Li, Tao Ma, Yuenan Hou, Botian Shi, Yucheng Yang, Youquan Liu, Xingjiao Wu, Qin Chen, Yikang Li, Yu Qiao, and Liang He. Logonet: Towards accurate 3d object detection with local-to-global cross-modal fusion. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17524-17534, 2023.
* [56] Xin Li, Botian Shi, Yuenan Hou, Xingjiao Wu, Tianlong Ma, Yikang Li, and Liang He. Homogeneous multi-modal feature fusion and interaction for 3d object detection. In _European Conference on Computer Vision_, pages 691-707, 2022.
* [57] Ye Li, Hanjiang Hu, Zuxin Liu, Xiaohao Xu, Xiaonan Huang, and Ding Zhao. Influence of camera-lidar configuration on 3d object detection for autonomous driving. In _IEEE International Conference on Robotics and Automation_, pages 9018-9025, 2024.
* [58] Venice Erin Liong, Thi Ngoc Tho Nguyen, Sergi Widjaja, Dhananjai Sharma, and Zhuang Jie Chong. Amvnet: Assertion-based multi-view fusion network for lidar semantic segmentation. _arXiv preprint arXiv:2012.04934_, 2020.
* [59] Mingyu Liu, Ekim Yurtsever, Xingcheng Zhou, Jonathan Fossaert, Yuning Cui, Bare Luka Zagar, and Alois C. Knoll. A survey on autonomous driving datasets: Data statistic, annotation, and outlook. _arXiv preprint arXiv:2401.01454_, 2024.
* [60] Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, and Ziwei Liu. Segment any point cloud sequences by distilling vision foundation models. In _Advances in Neural Information Processing Systems_, volume 36, pages 37193-37229, 2023.
* [61] Youquan Liu, Lingdong Kong, Xiaoyang Wu, Runnan Chen, Xin Li, Liang Pan, Ziwei Liu, and Yuexin Ma. Multi-space alignments towards universal lidar segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14648-14661, 2024.
* [62] Youquan Liu, Xin Li Runnan Chen, Lingdong Kong, Yuchen Yang, Zhaoyang Xia, Yeqi Bai, Xinge Zhu, Yuexin Ma, Yikang Li, Yu Qiao, and Yuenan Hou. Uniseg: A unified multi-modal lidar segmentation network and the opencseg codebase. In _IEEE/CVF International Conference on Computer Vision_, pages 21662-21673, 2023.
* [63] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L. Rus, and Song Han. Bevfusion: Multi-task multi-sensor fusion with unified bird's-eye view representation. In _IEEE International Conference on Robotics and Automation_, pages 2774-2781, 2023.
* [64] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for efficient 3d deep learning. In _Advances in Neural Information Processing Systems_, volume 32, pages 965-975, 2019.
* [65] Zuxin Liu, Mansur Arief, and Ding Zhao. Where should we place lidars on the autonomous vehicle?-an optimal design approach. In _IEEE International Conference on Robotics and Automation_, pages 2793-2799, 2019.
* [66] Tao Ma, Xuemeng Yang, Hongbin Zhou, Xin Li, Botian Shi, Junjie Liu, Yuchen Yang, Zhizheng Liu, Liang He, Yu Qiao, et al. Detzero: Rethinking offboard 3d object detection with long-term sequential point clouds. In _IEEE/CVF International Conference on Computer Vision_, pages 6736-6747, 2023.
* [67] Sivabalan Manivasagam, Ioan Andrei Barsan, Jingkang Wang, Ze Yang, and Raquel Urtasun. Towards zero domain gap: A comprehensive study of realistic lidar simulation for autonomy testing. In _IEEE/CVF International Conference on Computer Vision_, pages 8272-8282, 2023.
* [68] Sivabalan Manivasagam, Shenlong Wang, Kelvin Wong, Wenyuan Zeng, Mikita Sazanovich, Shuhan Tan, Bin Yang, Wei-Chiu Ma, and Raquel Urtasun. Lidarsim: Realistic lidar simulation by leveraging the real world. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11167-11176, 2020.
* [69] Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. Voxel transformer for 3d object detection. In _IEEE/CVF International Conference on Computer Vision_, pages 3164-3173, 2021.

* [70] Andres Milioto, Ignacio Vizzo, Jens Behley, and Cyrill Stachniss. Rangenet++: Fast and accurate lidar semantic segmentation. In _IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 4213-4220, 2019.
* [71] Momenta. Several key members from oppo's zebra team joins momenta. https://pandaily.com/several-key-members-from-oppos-zeku-team-joins-momenta, Accessed: 2024-03-06, 2023.
* [72] Motional. Uber launches hailing of motional robotaxis in las vegas. https://www.iottechnews.co m/news/2022/dec/07/uber-launches-hailing-motional-robotaxis-las-vegas, Accessed: 2024-03-06, 2022.
* [73] Khan Muhammad, Amin Ullah, Jaime Lloret, Javier Del Ser, and Victor Hugo C. de Albuquerque. Deep learning for safe autonomous driving: Current challenges and future directions. _IEEE Transactions on Intelligent Transportation Systems_, 22(7):4316-4336, 2020.
* [74] Kazuto Nakashima, Yumi Iwashita, and Ryo Kurazume. Generative range imaging for learning scene priors of 3d lidar data. In _IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 1256-1266, 2023.
* [75] Kazuto Nakashima and Ryo Kurazume. Learning to drop points for lidar scan synthesis. In _IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 222-229, 2021.
* [76] Pony.ai. Pony.ai begins fully driverless testing in california. https://www.autonomousvehicleinternational.com/news/testing/pony-ai-begins-fully-driverless-testing-in-california.html, Accessed: 2024-03-06, 2022.
* [77] Gilles Puy, Alexandre Boulch, and Renaud Marlet. Using a waffle iron for automotive point cloud semantic segmentation. In _IEEE/CVF International Conference on Computer Vision_, pages 3379-3389, 2023.
* [78] Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, and Renaud Marlet. Image-to-lidar self-supervised distillation for autonomous driving data. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9891-9901, 2022.
* [79] Alvari Seppanen, Risto Ojala, and Kari Tammi. 4denoisenet: Adverse weather denoising from adjacent point clouds. _IEEE Robotics and Automation Letters_, 8(1):456-463, 2022.
* [80] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pvrcnn: Point-voxel feature set abstraction for 3d object detection. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10529-10538, 2020.
* [81] Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection. _International Journal of Computer Vision_, 131(2):531-551, 2023.
* [82] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal generation and detection from point cloud. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 770-779, 2019.
* [83] Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 43(8):2647-2664, 2020.
* [84] Ziying Song, Lin Liu, Feiyang Jia, Yadan Luo, Guoxin Zhang, Lei Yang, Li Wang, and Caiyan Jia. Robustness-aware 3d object detection in autonomous driving: A review and outlook. _arXiv preprint arXiv:2401.06542_, 2024.
* [85] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2446-2454, 2020.
* [86] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han. Searching efficient 3d architectures with sparse point-voxel convolution. In _European Conference on Computer Vision_, pages 685-702, 2020.

* [87] Siyu Teng, Xuemin Hu, Peng Deng, Bai Li, Yuchen Li, Yunfeng Ai, Dongsheng Yang, Lingxi Li, Zhe Xuanyuan, Fenghua Zhu, and Long Chen. Motion planning for autonomous driving: The state of the art and future perspectives. _IEEE Transactions on Intelligent Vehicles_, 8(6):3692-3711, 2023.
* [88] Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Francois Goulette, and Leonidas J. Guibas. Kpconv: Flexible and deformable convolution for point clouds. In _IEEE/CVF International Conference on Computer Vision_, pages 6411-6420, 2019.
* [89] Toyota. Ad/adas technology. https://woven.toyota/en/ad-adas-technology, Accessed: 2024-03-06, 2024.
* [90] Larissa T Triess, David Peter, Christoph B Rist, and J Marius Zollner. Scan-based semantic segmentation of lidar point clouds: An experimental study. In _IEEE Intelligent Vehicles Symposium_, pages 1116-1121, 2020.
* [91] Waymo. Waymo to provide fully driverless rides in san francisco, california. https://www.theverge.com/2022/11/19/23467784/waymo-provide-fully-driverless-rides-san-francisco-c-alifornia, Accessed: 2024-03-06, 2022.
* [92] Yi Wei, Zibu Wei, Yongming Rao, Jiaxin Li, Jie Zhou, and Jiwen Lu. Lidar distillation: bridging the beam-induced domain gap for 3d object detection. In _European Conference on Computer Vision_, pages 179-195, 2022.
* [93] Aoran Xiao, Jiaxing Huang, Dayan Guan, Fangneng Zhan, and Shijian Lu. Transfer learning from synthetic to real lidar point cloud for semantic segmentation. In _AAAI Conference on Artificial Intelligence_, pages 2795-2803, 2022.
* [94] Shaoyuan Xie, Lingdong Kong, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei Liu. Robobev: Towards robust bird's eye view perception under corruptions. _arXiv preprint arXiv:2304.06719_, 2023.
* [95] Shaoyuan Xie, Lingdong Kong, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei Liu. Benchmarking and improving bird's eye view perception robustness in autonomous driving. _arXiv preprint arXiv:2405.17426_, 2024.
* [96] Shaoyuan Xie, Zichao Li, Zeyu Wang, and Cihang Xie. On the adversarial robustness of camera-based 3d object detection. _Transactions on Machine Learning Research_, 2024.
* [97] Yuwen Xiong, Wei-Chiu Ma, Jingkang Wang, and Raquel Urtasun. Learning compact representations for lidar completion and generation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1074-1083, 2023.
* [98] Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter Vajda, Kurt Keutzer, and Masayoshi Tomizuka. Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation. In _European Conference on Computer Vision_, pages 1-19, 2020.
* [99] Xiang Xu, Lingdong Kong, Hui Shuai, and Qingshan Liu. Fruet: Frustum-range networks for scalable lidar segmentation. _arXiv preprint arXiv:2312.04484_, 2023.
* [100] Xiang Xu, Lingdong Kong, Hui Shuai, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu, and Qingshan Liu. 4d contrastive superflows are dense 3d representation learners. In _European Conference on Computer Vision_, pages 58-80, 2024.
* [101] Xiachao Xu, Zihao Du, Huaxin Zhang, Ruichao Zhang, Zihan Hong, Qin Huang, and Bin Han. Optimization of forceymography sensor placement for arm movement recognition. In _IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 9845-9850, 2022.
* [102] Xiachao Xu, Tianyi Zhang, Sibo Wang, Xiang Li, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew Johnson-Roberson, and Xiaonan Huang. Customizable perturbation synthesis for robust slam benchmarking. _arXiv preprint arXiv:2402.08125_, 2024.
* [103] Xu Yan, Chaoda Zheng, Ying Xue, Zhen Li, Shuguang Cui, and Dengxin Dai. Benchmarking the robustness of lidar semantic segmentation models. _International Journal of Computer Vision_, 132:2674-2697, 2024.
* [104] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional detection. _Sensors_, 18(10):3337, 2018.

* [105] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3dssd: Point-based 3d single stage object detector. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11040-11048, 2020.
* [106] Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Std: Sparse-to-dense 3d object detector for point cloud. In _IEEE/CVF International Conference on Computer Vision_, pages 1951-1960, 2019.
* [107] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and tracking. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11784-11793, 2021.
* [108] Kaicheng Yu, Tang Tao, Hongwei Xie, Zhiwei Lin, Zhongwei Wu, Zhongyu Xia, Tingting Liang, Haiyang Sun, Jiong Deng, Dayang Hao, Yongtao Wang, Xiaodan Liang, and Bing Wang. Benchmarking the robustness of lidar-camera fusion for 3d object detection. _arXiv preprint arXiv:2205.14951_, 2022.
* [109] Diankun Zhang, Zhijie Zheng, Haoyu Niu, Xueqing Wang, and Xiaojun Liu. Fully sparse transformer 3d detector for lidar point cloud. _IEEE Transactions on Geoscience and Remote Sensing_, 61:1-12, 2023.
* [110] Tunhou Zhang, Mingyuan Ma, Feng Yan, Hai Li, and Yiran Chen. Pids: Joint point interaction-dimension search for 3d point cloud. In _IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 1298-1307, 2023.
* [111] Yang Zhang, Zixiang Zhou, Philip David, Xiangyu Yue, Zerong Xi, Boqing Gong, and Hassan Foroosh. Polarnet: An improved grid representation for online lidar point clouds semantic segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9601-9610, 2020.
* [112] Yifan Zhang, Junhui Hou, and Yixuan Yuan. A comprehensive study of the robustness for lidar-based 3d object detectors against adversarial attacks. _International Journal of Computer Vision_, 132(5):1592-1624, 2024.
* [113] Yiming Zhao, Lin Bai, and Xinming Huang. Fidnet: Lidar point cloud semantic segmentation with fully interpolation decoding. In _IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 4453-4458, 2021.
* [114] Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4490-4499, 2018.
* [115] Zixiang Zhou, Yang Zhang, and Hassan Foroosh. Panoptic-polarnet: Proposal-free lidar point cloud panoptic segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13194-13203, 2021.
* [116] Zixiang Zhou, Xiangchen Zhao, Yu Wang, Panqu Wang, and Hassan Foroosh. Centerformer: Center-based transformer for 3d object detection. In _European Conference on Computer Vision_, pages 496-513, 2022.
* [117] Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin Ma, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical and asymmetrical 3d convolution networks for lidar segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9939-9948, 2021.
* [118] Zhuangwei Zhuang, Rong Li, Kui Jia, Qicheng Wang, Yuanqing Li, and Mingkui Tan. Perception-aware multi-sensor fusion for 3d lidar semantic segmentation. In _IEEE/CVF International Conference on Computer Vision_, pages 16280-16290, 2021.
* [119] Zoxox. Zoox autonomous e-shuttle begins operating on public roads in california. https://newatlas.com/automotive/zoox-electric-robotaxi-public-roads-california, Accessed: 2024-03-06, 2023.
* [120] Vlas Zyrianov, Xiyue Zhu, and Shenlong Wang. Learning to generate realistic lidar point clouds. In _European Conference on Computer Vision_, pages 17-35, 2022.

## Appendix A The Place3D Dataset

* A.1 Statistics
* A.2 Data Collection
* A.3 Label Mappings
* A.4 Adverse Conditions
* A.5 License
* B. Additional Implementation Detail
* B.1 LiDAR Placement
* B.2 Corruption Generation
* B.3 Hyperparameters
* C. Additional Quantitative Result
* C.1 LiDAR Semantic Segmentation
* C.2 3D Object Detection
* C.3 Compare to Existing Approaches
* D. Additional Qualitative Result
* D.1 Clean Condition
* D.2 Adverse Conditions
* E. Proofs
* E.1 Full Proof of Theorem 1
* E.2 Proof of Corollary 1
* F. Discussions
* F.1 Limitations
* F.2 Potential Societal Impact
* G. Public Resources Used

## Appendix A The Place3D Dataset

In this section, we present additional information on the statistics, features, and implementation details of the proposed Place3D dataset.

### Statistics

Our dataset consists of a total of eleven LiDAR placements, in which seven baselines are inspired by existing self-driving configurations from autonomous vehicle companies, and four LiDAR placements are obtained by optimization. Each LiDAR placement contains four LiDAR sensors. For each LiDAR configuration, the sub-dataset consists of 13,600 frames of samples, comprising 11,200 samples for training and 2,400 samples for validation, following the split ratio used in nuScenes [7]. We combined every 40 frames of samples into one scene, with a time interval of 0.5 seconds between each frame sample.

### Data Collection

In this section, we elaborate on more details of the simulator setup and LiDAR setup procedures used in collecting the Place3D dataset.

#### a.2.1 Simulator Setup

We choose four maps (Towns 1, 3, 4, and 6, _cf._ Figure 7) in CARLA v0.9.10 to collect point cloud data and generate ground truth information. For each map, we manually set six ego-vehicle routes to cover all roads with no roads overlapped. The frequency of the simulation is set to 20 Hz.

#### a.2.2 LiDAR Setup

LiDAR point cloud data is collected every 0.5 simulator seconds utilizing the _Semantic LIDAR sensor_ in CARLA. The attributes of _Semantic LIDAR sensor_ are listed in Table 5. Notably, we utilize LiDARs with relatively low resolution to increase the challenge in object detection and semantic segmentation, leading to lower scores than those on the nuScenes [7] leaderboard. This is based on the observation that LiDAR point clouds become sparse when detecting distant objects and the use of multiple LiDARs is specifically to enhance the perception of sparse point clouds at long distances in practical applications. Our experiments with low-resolution LiDARs allow for a more evident assessment of the impact of LiDAR placements on the perception of sparse point clouds.

### Label Mappings

In this section, we provide more details for the class definitions of the LiDAR semantic segmentation and 3D object detection tasks.

#### a.3.1 LiDAR Semantic Segmentation

There are a total of \(\mathbf{21}\) semantic categories in our LiDAR semantic segmentation dataset, which are \({}^{1}\)_Building_, \({}^{2}\)_Fence_, \({}^{3}\)_Other_, \({}^{4}\)_Pedestrian_, \({}^{5}\)_Pole_, \({}^{6}\)_Road Line_, \({}^{7}\)_Road_, \({}^{8}\)_Sidewalk_, \({}^{9}\)_Vegetation_, \({}^{10}\)_Vehicle_, \({}^{11}\)_Wall_, \({}^{12}\)_Traffic Sign_, \({}^{13}\)_Ground_, \({}^{14}\)_Bridge_, \({}^{15}\)_Rail Track_, \({}^{16}\)_Guard Rail_, \({}^{17}\)_Traffic Light_, \({}^{18}\)_Static_, \({}^{19}\)_Dynamic_, \({}^{20}\)_Water_, and \({}^{21}\)_Terrain_. Table 6 presents the detailed definition of each semantic class in Place3D. Additionally, we include a _Unlabeled_ tag to denote elements that have not been categorized.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline  & **nuScenes** & **Waymo** & **SemKITTI** & Place3D \\ \hline \hline Detection Classes & \(10\) & \(2\) & \(3\) & \(3\) \\ Segmentation Classes & \(16\) & \(23\) & \(19\) & \(21\) \\ \hline \# of 3D Boxes & \(12\)M & \(1.4\)M & \(80\)K & \(71\)K \\ Points Per Frame & \(34\)K & \(177\)K & \(120\)K & \(16\)K \\ \# of LiDAR Channels & \(1\times 32\) & \(1\times 64\) & \(1\times 64\) & \(4\times 16\) \\ Vertical FOV & \([-30.0,10.0]\) & \([-17.6,2.4]\) & \([-24.8,2.0]\) & \([-24.8,2.0]\) \\ \hline Placement Strategy & Single & Single & Single & Multiple \\ Adverse Conditions & No & No & No & Yes \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparisons between the established benchmark and popular driving perception datasets with support for both the LiDAR semantic segmentation and 3D object detection tasks.

Figure 7: The maps used to collect the Place3D data in CARLA v0.9.10.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline
**Attribute** & **Value** & **Attribute** & **Value** \\ \hline \hline Channels & \(16\) & Upper FOV & \(2.0\) degrees \\ Range & \(100.0\) meters & Lower FOV & \(-24.8\) degrees \\ Points Per Second & 5,000 \(\times\) channels & Horizontal FOV & \(360.0\) degrees \\ Rotation Frequency & \(20.0\) Hz & Sensor Tick & \(0.5\) second \\ \hline \hline \end{tabular}
\end{table}
Table 5: The attributes of the semantic LiDAR sensors used for acquiring the data.

#### a.3.2 3D Object Detection

We set **3** types of common objects in the traffic scene for the 3D object detection tasks, _i.e._, 1_Car_, 2_Bicyclist_, and 3_Pedestrian_, following the same configuration as KITTI [29].

Footnote 1: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode.en.

### Adverse Conditions

As shown in Figure 9 and Figure 10, the adverse conditions in the Place3D dataset can be categorized into three distinct groups:

* **Severe weather conditions**, including "fog", which causes back-scattering and attenuation of LiDAR points due to water particles in the air; "snow", where snow particles intersect with laser beams, affecting the beam's reflection and causing potential occlusions; and "wet ground", where laser pulses lose energy upon hitting wet surfaces, leading to attenuated echoes. These are commonly occurring corruptions in real driving conditions.
* **External disturbances** like "motion blur", which results from vehicle movement that causes blurring in the data, especially on bumpy surfaces or during rapid turns.
* **Internal sensor failure**, such as "crosstalk", where the light impulses from multiple sensors interfere with each other, creating noisy points; as well as "incomplete echo", where dark-colored objects result in incomplete LiDAR readings.

### License

The Place3D dataset is released under the _CC BY-NC-SA 4.0_ license1.

Footnote 1: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode.en.

## Appendix B Additional Implementation Detail

In this section, we present additional implementation details to facilitate the reproduction of the data generation, placement optimization, and performance evaluations in this work.

### LiDAR Placement

We adopt five commonly employed heuristic LiDAR placements, which have been adopted by several leading autonomous driving companies (see Figure 8), as our baseline. We create another two baseline placements by adjusting the rolling angles of LiDAR sensors. We obtain two placements in optimization and two placements in the ablation study. All configurations are presented in Table 7.

Figure 8: A diverse spectrum of existing multi-LiDAR placements employed by major autonomous vehicle companies [91, 72, 22, 76, 119, 89, 71, 27]. Images adopted from original websites.

\begin{table}
\begin{tabular}{r|c|c|l} \hline \hline
**Class** & **ID** & **Description** \\ \hline \hline Building & 1 & Houses, skyscrapers, etc., and the manmade elements attached to \\  & & & them, such as scaffolding, awning, and ladders. \\ \hline \multirow{2}{*}{Fence \(\blacksquare\)} & 2 & Barriers, railing, and other upright structures that are made by wood \\  & & & or wire assemblies and enclose an area of ground. \\ \hline Other & 3 & Everything that does not belong to any other category. \\ \hline Pedestrian & 4 & Humans that walk, ride, or drive any kind of vehicle or mobility \\  & & & system, such as bicycles, scooters, skateboards, wheelchairs, etc.. \\ \hline Pole & 5 & Small mainly vertically oriented poles, such as sign poles and traffic \\  & & & light poles. \\ \hline Road Line & 6 & Markings on the road. \\ \hline Road & 7 & Part of ground on which vehicles usually drive. \\ \hline Sidewalk & 8 & Part of ground designated for pedestrians or cyclists, which is \\  & & & delimited from the road by some obstacle, such as curbs or poles. \\ \hline Vegetation & 9 & Trees, hedges, and all other types of vertical vegetation. \\ \hline Vehicle & 10 & Cars, vans, trucks, motorcycles, bikes, buses, and trains. \\ \hline Wall & 11 & Individual standing walls that are not part of a building. \\ \hline Traffic Sign & 12 & Signs installed by the city authority and are usually for traffic \\  & & & regulation. \\ \hline Ground & 13 & Horizontal ground-level structures that do not match any other \\  & & & category. \\ \hline Bridge & 14 & The structure of the bridge. \\ \hline Rail Track & 15 & All types of rail tracks that are not driveable by cars. \\ \hline Guard Rail & 16 & All types of guard rails and crash barriers. \\ \hline Traffic Light & 17 & Traffic light boxes without their poles. \\ \hline Static & 18 & Elements in the scene and props that are immovable, such fire \\  & & & hydrants, fixed benches, fountains, bus stops, etc. \\ \hline Dynamic & 19 & Elements whose position is susceptible to change over time, such as \\  & & & movable trash bins, buggies, bags, wheelchairs, animals, etc. \\ \hline Water & 20 & Horizontal water surfaces, such as lakes, sea, and rivers. \\ \hline Terrain & 21 & Grass, ground-level vegetation, soil, and sand. \\ \hline \hline \end{tabular}
\end{table}
Table 6: Summary of the semantic categories and their semantic coverage in the Place3D dataset.

\begin{table}
\begin{tabular}{r|c c c c|c c c c|c c c c|c c c c} \hline \hline \multirow{2}{*}{**Placement**} & \multicolumn{4}{c|}{**LIDAR \#1**} & \multicolumn{4}{c|}{**LIDAR \#2**} & \multicolumn{4}{c|}{**LIDAR \#3**} & \multicolumn{4}{c}{**LIDAR \#4**} \\  & \(x\) & \(y\) & \(z\) & roll & \(x\) & \(y\) & \(z\) & roll & \(x\) & \(y\) & \(z\) & roll & \(x\) & \(y\) & \(z\) & roll \\ \hline \hline Center & 0.0 & 0.0 & 2.2 & 0.0 & 0.0 & 0.0 & 0.0 & 2.4 & 0.0 & 0.0 & 0.0 & 2.6 & 0.0 & 0.0 & 0.0 & 2.8 & 0.0 \\ Line & 0.0 & -0.6 & 2.2 & 0.0 & 0.0 & -0.4 & 2.2 & 0.0 & 0.0 & 0.4 & 2.2 & 0.0 & 0.0 & 0.6 & 2.2 & 0.0 \\ Pyramid & -0.2 & -0.6 & 2.2 & 0.0 & 0.4 & 0.0 & 2.4 & 0.0 & -0.2 & 0.6 & 2.6 & 0.0 & -0.2 & 0.6 & 2.2 & 0.0 \\ Square & -0.5 & 0.5 & 2.2 & 0.0 & -0.5 & -0.5 & 2.2 & 0.0 & 0.5 & 0.5 & 2.2 & 0.0 & 0.5 & -0.5 & 2.2 & 0.0 \\ Traepaied & -0.4 & 0.2 & 2.2 & 0.0 & -0.4 & -0.2 & 2.2 & 0.0 & 0.2 & 0.5 & 2.2 & 0.0 & 0.2 & -0.5 & 2.2 & 0.0 \\ Line-roll & 0.0 & -0.6 & 2.2 & -0.3 & 0.0 & -0.4 & 2.2 & 0.0 & 0.0 & 0.4 & 2.2 & 0.0 & 0.0 & 0.6 & 2.2 & -0.3 \\ Pyramid-roll & -0.2 & -0.6 & 2.2 & -0.3 & 0.4 & 0.0 & 2.4 & 0.0 & -0.2 & 0.6 & 2.0 & 0.0 & -0.2 & 0.6 & 2.2 & -0.3 \\ \hline Ours-det & 0.5 & 0.5 & 2.5 & -0.3 & -0.4 & 0.1 & 2.6 & -0.2 & 0.0 & 0.0 & 2.8 & 0.0 & -0.1 & -0.5 & 2.7 & 0.0 \\ Ours-seg & 0.0 & 0.5 & 2.6 & -0.3 & 0.6 & 0.3 & 2.8 & 0.0 & 0.4 & 0.0 & 2.5 & 0.0 & 0.1 & -0.6 & 2.8 & 0.2 \\ Corruption & 0.4 & 0.5 & 2.6 & -0.3 & 0.5 & -0.4 & 2.7 & 0.0 & -0.4 & -0.3 & 2.7 & 0.1 & -0.3 & 0.5 & 2.7 & 0.0 \\
2D-plane & 0.6 & 0.6 & 2.2 & 0.0 & 0.5 & -0.4 & 2.2 & 0.0 & -0.5 & -0.6 & 2.2 & 0.0 & -0.6 & 0.3 & 2.2 & 0.0 \\ \hline \hline \end{tabular}
\end{table}
Table 7: The configuration coordinates of different LiDAR placement strategies in this work with respect to their ego-vehicle coordinate frames from the four LiDAR sensors.

### Corruption Generation

In this work, we adopt the public implementations from Robo3D [46] to generate corrupted point clouds. We follow the default configuration of generating nuScenes-C to construct the adverse condition sets in Place3D. Specifically, for "fog" generation, the attenuation coefficient is randomly sampled from \([0,0.005,0.01,0.02,0.03,0.06]\) and the back-scattering coefficient is set as \(0.05\). For "wet ground" generation, the water height is set as \(1.0\) millimeter. For "snow" generation, the value of snowfall rate parameter is set to \(1.0\). For "motion blur" generation, the jittering noise level is set as \(0.30\). For "crosstalk" generation, the disturb noise parameter is set to \(0.07\). For "incomplete echo" generation, the attenuation parameter is set to \(0.85\).

### Hyperparameters

In this work, we build the LiDAR placement benchmark using the MMDetection3D codebase [20]. Unless otherwise specified, we follow the conventional setups in MMDetection3D when training and evaluating the LiDAR semantic segmentation and 3D object detection models.

The detailed training configurations of the four LiDAR semantic segmentation models, _i.e._, MinkUNet [18], SPVCNN [86], PolarNet [111], and Cylinder3D [117], are presented in Table 8. The detailed training configurations of the four 3D object detection models, _i.e._, PointPillars [51], CenterPoint [107], BEVFusion-L [63], and FSTR [109], are presented in Table 9.

Figure 9: Visual examples of LiDAR point clouds under adverse conditions in Place3D.

All LiDAR semantic segmentation models are trained and tested on eight NVIDIA A100 SXM4 80GB GPUs. All 3D object detection models are trained and tested on four NVIDIA RTX 6000 Ada 48GB GPUs. The models are evaluated on the validation sets. We do not include any type of test time augmentation or model ensembling when evaluating the models.

## Appendix C Additional Quantitative Result

In this section, we provide additional quantitative results to better support the findings and conclusions drawn in the main body of this paper.

### LiDAR Semantic Segmentation

We provide the complete (_i.e._, the class-wise IoU scores) results for LiDAR semantic segmentation under different LiDAR placement strategies.

We showcase the per-class LiDAR semantic segmentation results of MinkUNet [18], SPVCNN [86], PolarNet [111], and Cylinder3D [117] in Table 11. The performance of these methods is evaluated under different LiDAR placement strategies. Different LiDAR placement strategies demonstrated varying propensities towards particular classes. For instance, _Pyramid_ performs well for _building_, _guard rail_, and _vegetation_ compared with other placements, possibly due to an increased vertical field of view that captures these taller or layered structures more effectively. _Ours_ provides the generally best balance of performance across categories, with high scores in _building_, _road_, and _road_.

Figure 10: Visual examples of LiDAR point clouds under adverse conditions in Place3D.

vehicle_ suggesting an effective all-around coverage for various object types. The complete benchmark results of LiDAR semantic segmentation under clean and adverse conditions are presented in Table 12. We showcase the correlation between M-SOG and LiDAR semantic segmentation performance under clean and adverse conditions in Figure 11 and 12.

We showcase the complete results of the ablation study for semantic segmentation in Table 14. we first explore the performance of our optimization algorithm under placement constraints, specifically, standardize the elevation of LiDARs to analyze their optimal positioning on the vehicle's roof's horizontal plane. Our optimized placement _2D plane_ achieved better performance compared with _line_, _square_, and _trapezoid_ configurations. Further, we optimize the placement for adverse conditions. While the observation suggests a trade-off in clean data compared with the _Ours_, the configuration optimized on corruption data (_corruption optimized_) achieves a significantly higher performance in adverse conditions than both baselines in Table 12 and _Ours_, indicating that custom optimization strategies, tailored for challenging conditions, represent an effective methodology to enhance robust perception capabilities in adverse environments.

### 3D Object Detection

In this section, we present more experimental results for the 3D robustness evaluation of 3D object detection under adverse conditions. The performance of PointPillars [51], CenterPoint [107], BEVFusion-L [63], and FSTR [109] is evaluated under different LiDAR placement strategies in Table 13. Across all placements and conditions, there is a trend where all methods suffer a drop in performance in adverse conditions compared to clean conditions, highlighting the challenge that adverse conditions pose to 3D object detection. While some placements, like Pyramid and Square, appear to maintain relatively high performance under both clean and adverse conditions, the _Ours_ placement has the highest average mAP under adverse conditions. In addition, certain placements exhibit relative strengths against specific types of corruption, which can inform the development of more resilient object detection systems for autonomous vehicles. For instance, _Pyramid_ and _Pyramid-Roll_, appear to handle fog better than others, possibly due to a configuration that captures a more diverse set of angles which could mitigate the scattering effect of fog on LiDAR beams. We showcase the correlation between M-SOG and 3D Object Detection performance under clean and adverse conditions in Figure 13 and 14.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline
**Hyperparameter** & MinkUNet & SPVCNN & PolarNet & Cylinder3D \\ \hline \hline Batch Size & 8 \(\times\) b2 & 8 \(\times\) b2 & 8 \(\times\) b2 & 8 \(\times\) b2 \\ Epochs & 50 & 50 & 50 & 50 \\ Optimizer & AdamW & AdamW & AdamW & AdamW \\ Learning Rate & 8.0\(e\)\(-\)3 & 8.0\(e\)\(-\)3 & 8.0\(e\)\(-\)3 & 8.0\(e\)\(-\)3 \\ Weight Decay & 0.01 & 0.01 & 0.01 & 0.01 \\ Epsilon & 1.0\(e\)\(-\)6 & 1.0\(e\)\(-\)6 & 1.0\(e\)\(-\)6 & 1.0\(e\)\(-\)6 \\ \hline \hline \end{tabular}
\end{table}
Table 8: The training and optimization configurations of the four LiDAR semantic segmentation models [18, 111, 117, 86] used in our Place3D benchmark.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline
**Hyperparameter** & PointPillars & CenterPoint & BEVFusion-L & FSTR \\ \hline \hline Batch Size & 4 \(\times\) b4 & 4 \(\times\) b4 & 4 \(\times\) b4 & 4 \(\times\) b4 \\ Epochs & 24 & 20 & 20 & 20 \\ Optimizer & AdamW & AdamW & AdamW & AdamW \\ Learning Rate & 1.0\(e\)\(-\)3 & 1.0\(e\)\(-\)4 & 1.0\(e\)\(-\)4 & 1.0\(e\)\(-\)4 \\ Weight Decay & 0.01 & 0.01 & 0.01 & 0.01 \\ Epsilon & 1.0\(e\)\(-\)6 & 1.0\(e\)\(-\)6 & 1.0\(e\)\(-\)6 & 1.0\(e\)\(-\)6 \\ \hline \hline \end{tabular}
\end{table}
Table 9: The training and optimization configurations of the four 3D object detection models [51, 107, 63, 109] used in our Place3D benchmark.

### Compare to Existing Approaches

The effectiveness of sensor placement metrics is determined by the linear correlation between these metrics and actual performance. As shown in Table 10, Jiang's method [40] is applied to roadside placements, while comparable on-vehicle methods, S-MIG [34] and S-MS [57], follow the same process for LiDARs. All these methods use 3D bounding boxes as priors to understand the 3D scene distribution. However, this approach leads to significant deviations from the actual physical boundaries of objects and fails to account for occlusion relationships between objects and the environment in LiDAR applications. Consequently, they cannot accurately describe the scene or effectively optimize LiDAR placements. Moreover, these methods are limited to evaluating LiDAR placements for detection tasks since they solely rely on 3D object distribution information.

Our method addresses these limitations by introducing semantic occupancy information as a prior for the evaluation metric. This allows for more accurate characterization of boundaries in the 3D scene and effectively addresses occlusion issues between objects and the environment. Additionally, our method leverages semantic distribution information under diverse conditions, thereby enhancing the reliability of perception in adverse weather conditions and during sensor failures.

## Appendix D Additional Qualitative Result

In this section, we provide additional qualitative examples to help visually compare different conditions presented in this work.

### Clean Condition

We showcase the qualitative results of the MinkUNet [18] model using our optimized LiDAR placement. As shown in Figure 15, the depiction across the first four rows demonstrates a commendable performance of our LiDAR placement strategy during clean conditions. This is evidenced by the predominance of gray in the error maps, which indicates a high rate of correct predictions. Also, the fifth row presents a notable exception, illustrating a failure case. This particular scene exhibits a heightened complexity with a diverse array of objects and potential occlusions that challenge predictive accuracy. The qualitative results underscore the importance of continuous refinement of LiDAR placement to achieve better perception across a comprehensive spectrum of scenarios.

### Adverse Conditions

In Figure 16, we showcase the qualitative results of MinkUNet [18] under different adverse conditions utilizing our optimized LiDAR placement. The corruptions from top to bottom rows are "fog", "wet ground", "motion blur", "crosstalk", and "incomplete echo". As can be seen, the LiDAR semantic segmentation model encounters extra difficulties when predicting under such scenarios. Erroneous predictions tend to appear in regions contaminated by different types of noises, such as airborne particles, disturbed laser reflections, and jittering scatters. It becomes apparent that enhancing the model's robustness under these adverse conditions is crucial for the practical usage of driving perception systems. To achieve better robustness, we design suitable LiDAR placements that can mitigate the degradation caused by corruptions. As discussed in Table 12, Table 13, and Table 14, our placements can largely enhance the robustness of various models.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline \hline
**Method** & Venue & Deployment & Sensor & Prior Info & Task & Optimizing \\ \hline \hline S-MIG [34] & CVPR 2022 & Vehicle & LiDAR & 3D bbox & Det &  \\ Jiangs [40] & ICCV 2023 & Road Side & LiDAR & 3D bbox & Det &  \\ S-MS [57] & ICRA 2024 & Vehicle & LiDAR + Camera & 3D bbox & Det &  \\ \hline Place3D & Ours & Vehicle & LiDAR & Semantic Occ & Seg + Det &  \\ \hline \hline \end{tabular}
\end{table}
Table 10: Comparisons of the key differences among existing sensor placement approaches.

Figure 11: The correlation between M-SOG and LiDAR semantic segmentation [18, 86, 111, 117] models performance in the _clean_ condition.

Figure 14: The correlation between M-SOG and 3D object detection [51, 107, 63, 109] models performance in the _adverse_ condition.

Figure 12: The correlation between M-SOG and LiDAR semantic segmentation [18, 86, 111, 117] models performance in the _adverse_ condition.

Figure 13: The correlation between M-SOG and 3D object detection [51, 107, 63, 109] models performance in the _clean_ condition.

Figure 15: **Qualitative assessments** of the MinkUNet [18] model using our LiDAR placement strategy. The model is tested under the clean condition. The error maps show the correct and incorrect predictions in gray and red, respectively. Kindly refer to Table 6 for color maps. Best viewed in colors and zoomed-in for details.

Figure 16: **Qualitative assessments** of the MinkUNet [18] model using our LiDAR placement strategy. The model is tested under adverse conditions. The error maps show the correct and incorrect predictions in gray and red, respectively. Kindly refer to Table 6 for color maps. Best viewed in colors and zoomed-in for details.

[MISSING_PAGE_EMPTY:29]

\begin{table}
\begin{tabular}{c|c c c|c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{4}{c|}{Clean} & \multicolumn{4}{c}{Adverse} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} \\  & mIoU & mAcc & & ECE & Fog & Wet & Snow & Blur & Cross & Echo & **Avg** \\ \hline \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Center**} \\ MinkUNet [18] & \(65.7\) & \(72.4\) & \(0.041\) & 55.9 & 63.8 & 25.1 & 35.8 & 24.7 & 64.5 & 45.0 \\ SPVCNN [86] & \(67.1\) & \(74.4\) & \(0.034\) & 39.3 & 66.6 & 35.6 & 35.6 & 19.5 & 66.8 & 43.9 \\ PolarNet [111] & \(71.0\) & \(76.0\) & \(0.033\) & 43.1 & 59.2 & 11.4 & 36.1 & 23.1 & 69.4 & 40.4 \\ Cylinder3D [117] & \(72.7\) & \(79.2\) & \(0.041\) & 55.6 & 64.4 & 16.7 & 37.6 & 36.9 & 71.5 & 47.1 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Line**} \\ MinkUNet [18] & \(59.7\) & \(67.7\) & \(0.037\) & 51.7 & 60.2 & 35.5 & 52.0 & 27.1 & 59.2 & 47.6 \\ SPVCNN [86] & \(59.3\) & \(66.7\) & \(0.068\) & 42.8 & 57.9 & 31.3 & 46.1 & 13.6 & 57.1 & 41.5 \\ PolarNet [111] & \(67.7\) & \(74.1\) & \(0.034\) & 43.1 & 61.6 & 4.4 & 48.4 & 18.9 & 67.3 & 40.6 \\ Cylinder3D [117] & \(68.9\) & \(76.3\) & \(0.045\) & 55.5 & 66.4 & 4.7 & 39.4 & 34.3 & 68.3 & 44.8 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Pyramid**} \\ MinkUNet [18] & \(62.7\) & \(70.6\) & \(0.072\) & 52.9 & 60.3 & 25.2 & 50.7 & 17.3 & 60.2 & 44.4 \\ SPVCNN [86] & \(67.6\) & \(74.0\) & \(0.037\) & 48.6 & 66.6 & 30.2 & 55.1 & 14.8 & 65.9 & 46.9 \\ PolarNet [111] & \(67.7\) & \(73.0\) & \(0.032\) & 34.3 & 61.0 & 2.3 & 49.1 & 9.6 & 66.9 & 37.2 \\ Cylinder3D [117] & \(68.4\) & \(76.0\) & \(0.093\) & 51.0 & 52.2 & 5.0 & 42.5 & 26.6 & 60.9 & 39.7 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Square**} \\ MinkUNet [18] & \(60.7\) & \(68.4\) & \(0.043\) & 55.6 & 61.9 & 33.5 & 51.5 & 26.5 & 61.2 & 48.4 \\ SPVCNN [86] & \(63.4\) & \(70.2\) & \(0.031\) & 40.7 & 64.3 & 38.3 & 53.9 & 18.6 & 63.7 & 46.6 \\ PolarNet [111] & \(69.3\) & \(74.7\) & \(0.033\) & 39.9 & 50.0 & 6.1 & 49.1 & 15.1 & 68.8 & 38.2 \\ Cylinder3D [117] & \(69.9\) & \(76.7\) & \(0.044\) & 52.0 & 55.6 & 2.7 & 44.2 & 37.1 & 68.7 & 43.4 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Trapezoid**} \\ MinkUNet [18] & \(59.0\) & \(66.2\) & \(0.040\) & 49.7 & 60.4 & 27.6 & 51.7 & 18.4 & 59.3 & 44.5 \\ SPVCNN [86] & \(61.0\) & \(68.8\) & \(0.044\) & 40.9 & 61.3 & 33.6 & 49.1 & 16.9 & 60.7 & 43.8 \\ PolarNet [111] & \(66.8\) & \(72.3\) & \(0.034\) & 37.5 & 65.3 & 2.8 & 46.7 & 15.4 & 67.8 & 39.3 \\ Cylinder3D [117] & \(68.5\) & \(75.4\) & \(0.057\) & 52.1 & 64.6 & 3.1 & 36.7 & 30.0 & 65.6 & 42.0 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Line-Roll**} \\ MinkUNet [18] & \(58.5\) & \(66.4\) & \(0.047\) & 48.6 & 59.2 & 26.9 & 50.4 & 21.2 & 58.0 & 44.1 \\ SPVCNN [86] & \(60.6\) & \(68.0\) & \(0.034\) & 42.2 & 62.0 & 27.0 & 49.9 & 16.5 & 61.0 & 43.1 \\ PolarNet [111] & \(67.2\) & \(72.8\) & \(0.037\) & 38.2 & 62.9 & 2.2 & 46.3 & 14.2 & 65.4 & 38.2 \\ Cylinder3D [117] & \(69.8\) & \(77.0\) & \(0.048\) & 49.7 & 65.4 & 2.6 & 37.4 & 27.3 & 67.8 & 41.7 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Pyramid-Roll**} \\ MinkUNet [18] & \(62.2\) & \(69.6\) & \(0.051\) & 52.2 & 60.9 & 26.6 & 52.5 & 19.3 & 60.8 & 45.4 \\ SPVCNN [86] & \(67.9\) & \(74.2\) & \(0.033\) & 47.2 & 67.1 & 31.6 & 56.5 & 13.7 & 66.7 & 47.1 \\ PolarNet [111] & \(70.9\) & \(75.9\) & \(0.035\) & 36.3 & 49.1 & 2.3 & 51.4 & 13.3 & 68.6 & 36.8 \\ Cylinder3D [117] & \(69.3\) & \(77.0\) & \(0.048\) & 50.7 & 67.9 & 2.1 & 44.1 & 31.9 & 70.0 & 44.5 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Ours**} \\ MinkUNet [18] & \(66.5\) & \(73.2\) & \(0.031\) & 59.5 & 66.6 & 17.6 & 56.7 & 24.5 & 66.9 & 48.6 \\ SPVCNN [86] & \(68.3\) & \(74.6\) & \(0.034\) & 59.1 & 66.7 & 24.0 & 56.0 & 18.7 & 66.9 & 48.6 \\ PolarNet [111] & \(76.7\) & \(81.5\) & \(0.033\) & 57.3 & 65.8 & 2.8 & 55.0 & 27.3 & 76.1 & 47.4 \\ Cylinder3D [117] & \(73.0\) & \(78.9\) & \(0.037\) & 57.6 & 67.2 & 5.9 & 48.7 & 41.0 & 63.3 & 47.3 \\ \hline \hline \end{tabular}
\end{table}
Table 12: **Benchmark results of LiDAR semantic segmentation under clean and adverse conditions**. For each placement, we report the mIoU (\(\uparrow\)), mAcc (\(\uparrow\)), and

\begin{table}
\begin{tabular}{c|c c c|c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{4}{c|}{**Clean**} & \multicolumn{4}{c}{**Adverse**} & \multirow{2}{*}{**Crogs**} & \multirow{2}{*}{**Crogs**} & \multirow{2}{*}{**Crogs**} & \multirow{2}{*}{**Crogs**} & \multirow{2}{*}{**Crogs**} & \multirow{2}{*}{**Crogs**} & \multirow{2}{*}{**Crogs**} \\  & \multicolumn{1}{c}{Car} & \multicolumn{1}{c}{Ped} & \multicolumn{1}{c}{Bicy} & & & & & & & & \\ \hline \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Center**} \\  PointPillars [51] & \(46.5\) & \(19.4\) & \(27.1\) & 17.1 & 36.3 & 37.4 & 27.1 & 25.7 & 26.2 & 28.3 \\  CenterPoint [107] & \(55.8\) & \(28.7\) & \(28.8\) & 23.2 & 47.3 & 18.9 & 27.3 & 31.6 & 22.3 & 28.4 \\  BEVFusion-L [63] & \(52.5\) & \(31.9\) & \(32.2\) & 19.2 & 36.8 & 27.0 & 12.8 & 8.3 & 20.9 & 20.8 \\  FSTR [109] & \(55.3\) & \(27.7\) & \(29.3\) & 23.8 & 44.1 & 30.7 & 25.4 & 25.0 & 25.9 & 29.2 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Line**} \\  PointPillars [51] & \(43.4\) & \(22.0\) & \(27.7\) & 15.1 & 39.6 & 33.6 & 27.1 & 16.9 & 25.2 & 26.3 \\  CenterPoint [107] & \(54.0\) & \(34.2\) & \(37.7\) & 20.2 & 49.2 & 22.8 & 9.7 & 12.0 & 21.5 & 22.6 \\  BEVFusion-L [63] & \(49.3\) & \(29.0\) & \(29.5\) & 18.6 & 38.0 & 12.2 & 23.6 & 17.2 & 21.6 & 21.9 \\  FSTR [109] & \(51.9\) & \(30.2\) & \(33.0\) & 22.7 & 45.4 & 27.0 & 23.9 & 20.1 & 27.0 & 27.7 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Pyramid**} \\  PointPillars [51] & \(46.1\) & \(24.4\) & \(29.0\) & 17.2 & 38.7 & 36.1 & 29.2 & 26.3 & 25.8 & 28.9 \\  CenterPoint [107] & \(55.9\) & \(37.4\) & \(35.6\) & 26.0 & 49.6 & 21.1 & 28.6 & 24.9 & 25.9 & 29.4 \\  BEVFusion-L [63] & \(51.0\) & \(21.7\) & \(27.9\) & 20.8 & 38.1 & 15.0 & 29.1 & 23.7 & 21.6 & 24.7 \\  FSTR [109] & \(55.7\) & \(29.4\) & \(33.8\) & 25.8 & 44.8 & 24.0 & 33.4 & 28.5 & 28.7 & 30.9 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Square**} \\  PointPillars [51] & \(43.8\) & \(20.8\) & \(27.1\) & 17.2 & 40.0 & 32.5 & 26.1 & 22.3 & 25.7 & 27.3 \\  CenterPoint [107] & \(54.0\) & \(35.5\) & \(34.1\) & 23.4 & 50.2 & 19.2 & 13.0 & 14.0 & 24.6 & 24.1 \\  BEVFusion-L [63] & \(49.2\) & \(27.0\) & \(26.7\) & 20.7 & 39.4 & 7.3 & 23.6 & 20.1 & 22.7 & 22.3 \\  FSTR [109] & \(52.8\) & \(30.3\) & \(31.3\) & 23.7 & 47.2 & 23.4 & 25.1 & 23.1 & 29.0 & 28.6 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Trapezoid**} \\  PointPillars [51] & \(43.5\) & \(21.5\) & \(27.3\) & 16.0 & 40.0 & 31.3 & 25.9 & 18.6 & 24.9 & 26.1 \\  CenterPoint [107] & \(55.4\) & \(35.6\) & \(37.5\) & 22.1 & 51.7 & 14.6 & 15.3 & 11.6 & 23.9 & 23.2 \\  BEVFusion-L [63] & \(50.2\) & \(30.0\) & \(31.7\) & 19.2 & 39.2 & 19.8 & 26.9 & 27.2 & 22.6 & 25.8 \\  FSTR [109] & \(54.6\) & \(30.0\) & \(33.3\) & 22.9 & 46.9 & 26.0 & 26.5 & 23.4 & 26.5 & 28.7 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Line-Roll**} \\  PointPillars [51] & \(44.6\) & \(21.3\) & \(27.0\) & 15.2 & 40.3 & 33.7 & 26.9 & 20.0 & 25.3 & 26.9 \\  CenterPoint [107] & \(55.2\) & \(32.7\) & \(37.2\) & 19.6 & 49.6 & 16.0 & 15.5 & 9.5 & 23.2 & 22.2 \\  BEVFusion-L [63] & \(50.8\) & \(29.4\) & \(29.5\) & 15.2 & 38.3 & 11.2 & 19.5 & 10.7 & 21.9 & 19.5 \\  FSTR [109] & \(53.5\) & \(29.8\) & \(32.4\) & 21.0 & 46.2 & 24.2 & 24.1 & 16.2 & 27.8 & 26.6 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Pyramid-Roll**} \\  PointPillars [51] & \(46.1\) & \(23.6\) & \(27.9\) & 17.5 & 39.1 & 33.7 & 27.3 & 25.1 & 25.2 & 28.0 \\  CenterPoint [107] & \(56.2\) & \(36.5\) & \(35.9\) & 24.8 & 49.2 & 15.9 & 29.0 & 24.0 & 25.5 & 28.1 \\  BEVFusion-L [63] & \(50.7\) & \(22.7\) & \(28.2\) & 22.8 & 40.3 & 5.2 & 30.3 & 22.9 & 21.8 & 23.9 \\  FSTR [109] & \(55.5\) & \(29.9\) & \(32.0\) & 26.3 & 47.2 & 22.9 & 33.5 & 28.1 & 28.8 & 31.1 \\ \hline \multicolumn{10}{l}{\(\bullet\)**Placement: Ours**} \\  PointPillars [51] & \(46.8\) & \(24.9\) & \(27.2\) & 18.3 & 40.1 & 36.4 & 26.5 & 26.4 & 25.8 & 28.9 \\  CenterPoint [107] & \(57.1\) & \(34.4\) & \(37.3\) & 22.3 & 51.1 & 23.1 & 27.7 & 29.0 & 23.6 & 29.5 \\  BEVFusion-L [63] & \(53.0\) & \(28.7\) & \(29.5\) & 21.8 & 41.3 & 15.9 & 32.0 & 22.9 & 22.7 & 26.1 \\  FSTR [109] & \(56.6\) & \(31.9\) & \(34.1\) & 25.1 & 47.5 & 28.8 & 32.1 & 30.6 & 28.2 & 32.1 \\ \hline \hline \end{tabular} \end

\begin{table}
\begin{tabular}{c|c c c|c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c|}{Clean} & \multicolumn{3}{c}{Adverse} & \multicolumn{3}{c}{} \\  & mIoU & mAcc & ECE & Fog & Wet & Snow & Blur & Cross & Echo & **Avg** \\ \hline \hline \(\bullet\)**Placement: Line** & & & & & & & & & & & \\ MinkUNet [18] & \(59.7\) & \(67.7\) & \(0.037\) & 51.7 & 60.2 & 35.5 & 52.0 & 27.1 & 59.2 & 47.6 \\ SPVCNN [86] & \(59.3\) & \(66.7\) & \(0.068\) & 42.8 & 57.9 & 31.3 & 46.1 & 13.6 & 57.1 & 41.5 \\ PolarNet [111] & \(67.7\) & \(74.1\) & \(0.034\) & 43.1 & 61.6 & 4.4 & 48.4 & 18.9 & 67.3 & 40.6 \\ Cylinder3D [117] & \(68.9\) & \(76.3\) & \(0.045\) & 55.5 & 66.4 & 4.7 & 39.4 & 34.3 & 68.3 & 44.8 \\ \hline \(\bullet\)**Placement: Square** & & & & & & & & & \\ MinkUNet [18] & \(60.7\) & \(68.4\) & \(0.043\) & 55.6 & 61.9 & 33.5 & 51.5 & 26.5 & 61.2 & 48.4 \\ SPVCNN [86] & \(63.4\) & \(70.2\) & \(0.031\) & 40.7 & 64.3 & 38.3 & 53.9 & 18.6 & 63.7 & 46.6 \\ PolarNet [111] & \(69.3\) & \(74.7\) & \(0.033\) & 39.9 & 50.0 & 6.1 & 49.1 & 15.1 & 68.8 & 38.2 \\ Cylinder3D [117] & \(69.9\) & \(76.7\) & \(0.044\) & 52.0 & 55.6 & 2.7 & 44.2 & 37.1 & 68.7 & 43.4 \\ \hline \(\bullet\)**Placement: Trapezoid** & & & & & & & & & \\ MinkUNet [18] & \(59.0\) & \(66.2\) & \(0.040\) & 49.7 & 60.4 & 27.6 & 51.7 & 18.4 & 59.3 & 44.5 \\ SPVCNN [86] & \(61.0\) & \(68.8\) & \(0.044\) & 40.9 & 61.3 & 33.6 & 49.1 & 16.9 & 60.7 & 43.8 \\ PolarNet [111] & \(66.8\) & \(72.3\) & \(0.034\) & 37.5 & 65.3 & 2.8 & 46.7 & 15.4 & 67.8 & 39.3 \\ Cylinder3D [117] & \(68.5\) & \(75.4\) & \(0.057\) & 52.1 & 64.6 & 3.1 & 36.7 & 30.0 & 65.6 & 42.0 \\ \hline \(\bullet\)**Placement: Line-Roll** & & & & & & & & & \\ MinkUNet [18] & \(58.5\) & \(66.4\) & \(0.047\) & 48.6 & 59.2 & 26.9 & 50.4 & 21.2 & 58.0 & 44.1 \\ SPVCNN [86] & \(60.6\) & \(68.0\) & \(0.034\) & 42.2 & 62.0 & 27.0 & 49.9 & 16.5 & 61.0 & 43.1 \\ PolarNet [111] & \(67.2\) & \(72.8\) & \(0.037\) & 38.2 & 62.9 & 2.2 & 46.3 & 14.2 & 65.4 & 38.2 \\ Cylinder3D [117] & \(69.8\) & \(77.0\) & \(0.048\) & 49.7 & 65.4 & 2.6 & 37.4 & 27.3 & 67.8 & 41.7 \\ \hline \(\bullet\)**Placement: Ours 2D Plane** & & & & & & & & & \\ MinkUNet [18] & \(62.4\) & \(71.0\) & \(0.059\) & 56.1 & 58.7 & 32.8 & 51.9 & 25.6 & 66.1 & 48.5 \\ SPVCNN [86] & \(65.0\) & \(71.8\) & \(0.050\) & 56.9 & 66.1 & 37.9 & 56.4 & 26.0 & 65.7 & 51.5 \\ PolarNet [111] & \(71.2\) & \(76.4\) & \(0.046\) & 52.8 & 64.4 & 5.0 & 51.7 & 23.0 & 69.8 & 44.5 \\ Cylinder3D [117] & \(70.3\) & \(75.7\) & \(0.047\) & 52.6 & 67.7 & 5.0 & 45.9 & 36.8 & 71.1 & 46.5 \\ \hline \(\bullet\)**Placement: Corruption Optimized** & & & & & & & & & \\ MinkUNet [18] & \(62.9\) & \(70.6\) & \(0.062\) & 57.0 & 59.9 & 31.9 & 52.4 & 25.0 & 67.6 & 49.0 \\ SPVCNN [86] & \(64.8\) & \(72.0\) & \(0.051\) & 56.4 & 65.6 & 36.8 & 57.0 & 25.8 & 64.8 & 51.1 \\ PolarNet [111] & \(73.9\) & \(79.3\) & \(0.037\) & 60.7 & 66.4 & 5.6 & 54.4 & 34.8 & 76.8 & 49.8 \\ Cylinder3D [117] & \(72.5\) & \(79.1\) & \(0.040\) & 54.0 & 69.9 & 5.9 & 47.3 & 37.8 & 72.7 & 47.9 \\ \hline \(\bullet\)**Placement: Ours** & & & & & & & & & \\ MinkUNet [18] & \(66.5\) & \(73.2\) & \(0.031\) & 59.5 & 66.6 & 17.6 & 56.7 & 24.5 & 66.9 & 48.6 \\ SPVCNN [86] & \(68.3\) & \(74.6\) & \(0.034\) & 59.1 & 66.7 & 24.0 & 56.0 & 18.7 & 66.9 & 48.6 \\ PolarNet [111] & \(76.7\) & \(81.5\) & \(0.033\) & 57.3 & 65.8 & 2.8 & 55.0 & 27.3 & 76.1 & 47.4 \\ Cylinder3D [117] & \(73.0\) & \(78.9\) & \(0.037\) & 57.6 & 67.2 & 5.9 & 48.7 & 41.0 & 63.3 & 47.3 \\ \hline \hline \end{tabular}
\end{table}
Table 14: **Ablation study of LiDAR semantic segmentation under clean and adverse conditions.** For each placement, we report the mIoU (\(\uparrow\)), mAcc (\(\uparrow\)), and ECE (\(\downarrow\)) scores for models under the clean condition, and mIoU (\(\uparrow\)) scores for models under adverse conditions. The mIoU and mAcc scores are given in percentage (\(\%\)).

Proofs

In this section, we present the full proof of Theorem 1 to certify the global optimally regarding the solved local optima. Additionally, we present the proof of Corollary 1 as a special case with bounded hyper-rectangle search space only given the Lipschitz constant of the objective function.

### Full Proof of Theorem 1

**Theorem 2** (Optimality Certification).: _Given the continuous objective function \(G:\mathbb{R}^{n}\rightarrow\mathbb{R}\) with Lipschitz constant \(k_{G}\) w.r.t. input \(\textbf{u}\in\mathcal{U}\subset\mathbb{R}^{n}\) under \(\ell_{2}\) norm, suppose over a \(\delta\)-density Grids subset \(S\subset\mathcal{U}\), the distance between the maximal and minimal of function \(G\) over \(S\) is upper-bounded by \(C_{M}\), and the local optima is \(\textbf{u}_{S}^{*}=\arg\min_{\textbf{u}\in S}G(x)\), the following optimality certification regarding \(x\in\mathcal{U}\) holds that:_

\[\|G(\textbf{u}^{*})-G(\textbf{u}_{S}^{*})\|_{2}\;\leq\;C_{M}+k_{G}\delta\;,\] (13)

_where \(\textbf{u}^{*}\) is the global optima over \(\mathcal{U}\)._

Proof.: Based on the sampling over subset \(S\subset\mathcal{U}\) with \(\ell_{2}\)-norm density \(\delta\), we have:

\[\forall\textbf{u}\in\mathcal{U},\min_{\textbf{u}_{S}\in S}\| \textbf{u}-\textbf{u}_{S}\|_{2}\;\leq\;\delta\;.\] (14)

From the continuous objective function \(G:\mathbb{R}^{n}\rightarrow\mathbb{R}\) with Lipschitz constant \(k_{G}\) w.r.t. input \(\textbf{u}\in\mathcal{U}\subset\mathbb{R}^{n}\) under \(\ell_{2}\) norm, it holds that:

\[\|G(\textbf{u}_{1})-G(\textbf{u}_{2})\|_{2}\;\leq\;k_{G}\|\textbf{u}_{1}- \textbf{u}_{2}\|_{2},\;\forall\textbf{u}_{1},\textbf{u}_{2}\in\mathcal{U}\;.\] (15)

Since the distance between the maximal and minimal of function \(G\) over \(S\) is upper-bounded by \(C_{M}\), we then have:

\[\|G(\textbf{u}_{1})-G(\textbf{u}_{2})\|_{2}\;\leq\;\|\max_{\textbf{u}\in S}G (\textbf{u})-\min_{\textbf{u}\in S}G(\textbf{u})\|_{2}\;\leq\;C_{M},\forall \textbf{u}_{1},\textbf{u}_{2}\in S\;.\] (16)

Then by absolute value inequality with \(\textbf{u}_{S}\in S\subset\mathcal{U}\)\(s.t.\)\(\|\textbf{u}^{*}-\textbf{u}_{S}\|_{2}\leq\delta\) and combining all inequalities above, we have:

\[\|G(\textbf{u}^{*})-G(\textbf{u}_{S}^{*})\|_{2} \;\leq\|G(\textbf{u}^{*})-G(\textbf{u}_{S})\|_{2}+\|G(\textbf{u} ^{*})-G(\textbf{u}_{S})\|_{2}\] (17) \[\;\leq k_{G}\|\textbf{u}^{*}-\textbf{u}_{S}\|_{2}+C_{M}\] (18) \[\;\leq k_{G}\delta+C_{M}\;,\] (19)

which concludes the proof. 

### Proof of Corollary 1

**Corollary 2**.: _When \(\mathcal{U}\) is a hyper-rectangle with the bounded \(\ell_{2}\) norm of domain \(U_{i}\in\mathbb{R}\) at each dimension \(i\), with \(i=1,2,\ldots,n\), Thm.2 can hold in a more general way by only assuming that the Lipschitz constant \(k_{G}\) of the objective function is given, where the following optimality certification regarding \(x\in\mathcal{U}\) holds that:_

\[\|G(\textbf{u}^{*})-G(\textbf{u}_{S}^{*})\|_{2}\;\leq\;k_{G}\sum_{i=1}^{n}U_{ i}+k_{G}\delta\;.\] (20)

Proof.: Similar to the proof of Thm. 2, by the sampling over subset \(S\subset\mathcal{U}\) with \(\ell_{2}\)-norm density \(\delta\), we have:

\[\forall\textbf{u}\in\mathcal{U},\;\min_{\textbf{u}_{S}\in S}\| \textbf{u}-\textbf{u}_{S}\|_{2}\;\leq\;\delta.\] (21)

From the continuous objective function \(G:\mathbb{R}^{n}\rightarrow\mathbb{R}\) with Lipschitz constant \(k_{G}\) w.r.t. input \(\textbf{u}\in\mathcal{U}\subset\mathbb{R}^{n}\) under \(\ell_{2}\) norm, it holds that:

\[\|G(\textbf{u}_{1})-G(\textbf{u}_{2})\|_{2}\;\leq\;k_{G}\|\textbf{u}_{1}- \textbf{u}_{2}\|_{2},\;\forall\textbf{u}_{1},\textbf{u}_{2}\in\mathcal{U}\;.\] (22)Now since the input space \(\mathcal{U}\) is a hyper-rectangle with the bounded \(\ell_{2}\) norm of domain \(U_{i}\in\mathbb{R}\) at each dimension \(i\), with \(i=1,2,\ldots,n\), for any \(i\)-th dimension \(\textbf{u}_{1}^{(i)},\textbf{u}_{2}^{(i)}\in\mathcal{U}^{(i)}\subset\mathbb{R}\), it holds that:

\[\|\textbf{u}_{1}^{(i)}-\textbf{u}_{2}^{(i)}\|_{2}\;\leq\;U_{i},\;\forall \textbf{u}_{1}^{(i)},\textbf{u}_{2}^{(i)}\in\mathcal{U}^{(i)},\;i=1,2,\ldots,n\;,\] (23)

By summing up all dimensions with absolute value inequality at \(\textbf{u}_{max},\textbf{u}_{min}\), we have:

\[\|\max_{\textbf{u}\in S}G(\textbf{u})-\min_{\textbf{u}\in S}G( \textbf{u})\|_{2} :=\|G(\textbf{u}_{max})-G(\textbf{u})_{min}\|_{2}\] (24) \[\leq\|G(\sum_{i=1}^{n}\textbf{u}_{max}^{(i)})-G(\sum_{i=1}^{n} \textbf{u}_{min}^{(i)})\|_{2}\] (25) \[\leq k_{G}\|\sum_{i=1}^{n}\textbf{u}_{max}^{(i)}-\sum_{i=1}^{n} \textbf{u}_{min}^{(i)}\|_{2}\] (26) \[\leq k_{G}\sum_{i=1}^{n}\|\textbf{u}_{max}^{(i)}-\textbf{u}_{min} ^{(i)}\|_{2}\] (27) \[\leq k_{G}\sum_{i=1}^{n}U_{i}\;.\] (28)

Since the distance between the maximal and minimal of function \(G\) over \(S\) is upper-bounded by \(C_{M}\), we then have:

\[\|G(\textbf{u}_{1})-G(\textbf{u}_{2})\|_{2}\;\leq\;\|\max_{\textbf{u}\in S}G( \textbf{u})-\min_{\textbf{u}\in S}G(\textbf{u})\|_{2}\;\leq\;k_{G}\sum_{i=1}^ {n}U_{i},\;\forall\textbf{u}_{1},\textbf{u}_{2}\in S\;.\] (29)

Then by absolute value inequality with \(\textbf{u}_{S}\in S\subset\mathcal{U}\;s.t.\;\|\textbf{u}^{*}-\textbf{u}_{S} \|_{2}\leq\delta\) and combining all inequalities above, we have:

\[\|G(\textbf{u}^{*})-G(\textbf{u}_{S}^{*})\|_{2} \leq\|G(\textbf{u}^{*})-G(\textbf{u}_{S})\|_{2}+\|G(\textbf{u}^{* })-G(\textbf{u}_{S})\|_{2}\] (30) \[\leq k_{G}\|\textbf{u}^{*}-\textbf{u}_{S}\|_{2}+k_{G}\sum_{i=1}^ {n}U_{i}\] (31) \[\leq k_{G}\delta+k_{G}\sum_{i=1}^{n}U_{i}\;,\] (32)

which concludes the proof.

Discussions

In this section, we discuss the limitations and potential negative societal impact of this work.

### Limitations

#### f.1.1 Possible Limitation in Data Collection

In this work, we utilize CARLA to generate LiDAR point clouds instead of collecting data in the real world. Although the delicacy and realism of simulation technologies are now remarkably close to the real world, there still exists a gap. For example, LiDAR cannot detect transparent objects, and LiDAR generates noise when encountering walls; these characteristics are not well-represented by the simulator. In addition, for each placement, we collected 13,600 frames of data; however, real-world applications would require a significantly larger dataset.

#### f.1.2 Possible Limitation in LiDAR Configurations

Although we selected seven simplified placements as baselines for extensive experimentation, our baseline placements cannot reflect and cover all possible LiDAR arrangements for various car manufacturers. Furthermore, the optimized placements we obtained are only near-optimal for our dataset. Identifying the globally optimal placements requires further analysis. In addition, our dataset employs spinning LiDAR technology. Yet, the latest advancements in autonomous driving have indicated a trend toward adopting semi-solid-state and solid-state LiDAR systems, necessitating further research. To demonstrate the impact of LiDAR placement, we selected LiDAR sensors with relatively low resolution. The outcomes might differ when the total number of LiDAR sensors is not four or when using LiDAR sensors with different channels and scanning frequencies.

#### f.1.3 Possible Limitation in Surrogate Metric

When generating the SOG (semantic occupancy grids) for computing the M-SOG metric, we use a voting algorithm to determine the semantic label for each voxel. This approach can introduce potential errors in cases where multiple semantic labels within a voxel have a similar number of points. Additionally, to accurately describe the semantic distribution of a scene, smaller voxel sizes are often required.

### Potential Societal Impact

LiDAR systems, by their very nature, are designed to capture detailed information about the environment. This can include not just the shape and location of objects, but potentially also capturing point cloud data of individuals, vehicles, and private property in high resolution. The data captured might be misused if not properly regulated and secured. Further, With autonomous vehicles navigating spaces using LiDAR and other sensors, there could be a shift in how spaces are designed, potentially prioritizing efficiency for autonomous vehicles over human-centered design principles. In addition, over-reliance on LiDAR and perception systems can lead to questions about the trustworthiness and reliability of these systems.

[MISSING_PAGE_EMPTY:36]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims made in the abstract and introduction sections have been phrased to reflect our contributions and scope.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The potential limitations and societal impact are discussed in Section F.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Full sets of assumptions and a complete proof are included. Please refer to Section E.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have fully disclosed all the information needed to reproduce the experimental results of this paper.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We attached the code for our implementations and for generating the training and evaluation data in the attachment.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: This paper has specified all the training and test details. Please refer to Section 4 and Section B.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Not applicable.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]Justification: Sufficient information on the computing resources used in this work has been clearly discussed. Please refer to Section 4 and Section B.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The conducted research sticks with the NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discussed these societal impacts in the Appendix. Please refer to Section F.2.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We foresee our paper poses no such risks.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cited and listed all existing assets in the Appendix. Please refer to Section G.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We uploaded the code as supplementary material with a clear readme file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This study does not involve crowdsourcing or research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This study does not involve crowdsourcing or research with human subjects.