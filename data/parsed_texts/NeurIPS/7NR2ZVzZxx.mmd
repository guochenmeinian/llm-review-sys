# LogicBench: A Benchmark for Evaluation of Logical Reasoning

Anonymous Author(s)

###### Abstract

Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really "Reason" over the natural language? This question has been receiving significant research attention and a number of reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability has focused only on a couple of axioms (such as modus ponens and modus tollens) of propositional and first-order logic. To study logical reasoning, we introduce _LogicBench_, a systematically created natural language question-answering dataset encompassing 25 reasoning patterns spanning over propositional, first-order, and non-monotonic logics. Key steps of our dataset construction consist of (1) controlled generation of sentences and their negations containing different ontologies, (2) _(context, question, answer)_ triplets creation using heuristically designed templates, and (3) semantic variations of triplets adding more diversity. We first evaluate easily accessible and widely used LLMs such as GPT-3, ChatGPT, and FLAN-T5 and show that they do not fare well on _LogicBench_, achieving just above random accuracy on average (\(\sim 52\%\)). Then, we show that LLMs trained using our data exhibit a better understanding of logical reasoning leading to performance improvements on several existing logical reasoning datasets such as LogicNLI, FOLIO, LogiQA, and ReClor.1

Footnote 1: Data is available at https://anonymous.4open.science/r/LogicBench-EEBB

## 1 Introduction

Large language models such as GPT-3 [3], ChatGPT, and FLAN [18] have made remarkable progress in NLP research enabling machines to perform a variety of language tasks that were previously thought to be exclusive to humans [12; 2; 20]. However, the ability of these LLMs to reason "logically" over natural language text remains under-explored, even though logical reasoning is a fundamental aspect of intelligence and a crucial requirement for many practical applications, such as question-answering systems [8] and conversational agents [1]. Although several datasets have been proposed [4; 16; 7; 13] to evaluate the logical reasoning capabilities of LLMs, these datasets are limited in their scope by (1) not evaluating logical reasoning independently of other forms of reasoning such as LogiQA [11] and ReClor [19]; and (2) evaluating only a single type of logic and covering only few logical inference rules as done in FOLIO [6] and ProntoQA [14]. Thus, our aim in this work is to address the lacuna of having a more comprehensive evaluation dataset for LLMs.

To this end, we propose _LogicBench_, a systematically created question-answering dataset for the evaluation of logical reasoning ability. As illustrated in Figure 1, _LogicBench_ includes a total of 25reasoning patterns across propositional, first-order, and non-monotonic logics. To evaluate LLMs, we formulate a binary classification task in _LogicBench_ in which the context represents logical statements and the models have to determine whether a conclusion given in the question is logically entailed by the context. For example, given the context "All mammals have fur" and "A cat is a mammal", for the question is "Does a cat have fur?", the correct answer, is "Yes". (Additional examples of task instances are presented in Table 3 and Appendix B. To construct _LogicBench_, we use a three-stage procedure (refer to SS2). In the first stage, we prompt GPT-3 to generate a variety of coherent natural language sentences consisting of different 'ontologies' (i.e., a collection of concepts such as car, person, and animals) and their corresponding negations (refer to SS2.2.1). Then, in the second stage, we generate _(context, question, answer)_ triplets using heuristically designed templates based on the inference rules and patterns. Finally, in the third stage, we generate semantics preserving and inverting variations of these logical rules by incorporating negations.

We evaluate a range of accessible and widely used LLMs including GPT-3 [3], ChatGPT, FLAN-T5 [18], Tk-instruct [17], and UnifiedQA [9] with respect to _LogicBench_ on the accuracy of the predicted answers (i.e., "Yes" or "No"). Experimental results reveal that these models struggle with respect to many of the inference rules and patterns (showing \(\sim 52\%\) accuracy on an average), suggesting significant room for improvement in their logical reasoning abilities. We then synthetically augment LogicBench and train T5-large. Our initial experimental results show that this improves the logical reasoning ability of existing models leading to performance improvement on other logic datasets, such as LogicNLI, and FOLIO (\(\sim 2\%\) on an average), and shows competitive performance on LogiQA and ReClor. In summary, our contributions are as follows:

1. Introducing _LogicBench_: A systematically created dataset to assess the logical reasoning capabilities of LLMs across propositional, first-order, and non-monotonic logics. This benchmark will be publicly available for evaluation and training purposes.
2. We propose a three-stage method to construct _LogicBench_ consisting of GPT-3 to generate coherent natural language sentences using prompts and a template-based module to convert them into logical rules. By assessing the performance of existing LLMs, we gain insights into their logical reasoning abilities which further leads to several interesting findings.
3. To the best of the authors' knowledge, this is the first benchmark to study non-monotonic reasoning, as well as various inference rules in propositional and first-order logics including hypothetical and disjunctive syllogism; and bidirectional, constructive, and destructive dilemmas in the NLP domain.

Figure 1: Comprehensive representation of different inference rules and reasoning patterns covered by propositional, first-order, and non-monotonic logics. _Exp._ indicates Expectation

LogicBench

In this section we discuss the logic types, inference rules, and patterns that are explored in this research. We also outline the methods for generating the data, and statistics of _LogicBench_.

### Logics Types

Propositional Logic (PL)Propositional logic employs a collection of statements or propositions (denoted as \(\mathcal{P}=p_{1},p_{2},...,p_{n}\), where \(p_{i}\) represents a proposition) and builds upon them using logical connectives such as '\(\wedge\)', '\(\vee\)', '\(\rightarrow\)', '\(\leftrightarrow\)', and '\(\neg\)'. Several inference rules for propositional logic have been defined using which given a set of premises, one can derive a sound conclusion. To illustrate this, let us consider two propositions: \(p_{1}\), which states "It is raining," and \(p_{2}\), which states "It is cloudy." From these propositions, we can construct a context (KB) consisting of two premises: (1) \(p_{1}\to p_{2}\) and (2) \(p_{1}\). Based on this KB, we can conclude \(p_{2}\). This inference rule is written as \((p_{1}\to p_{2})\wedge p_{1})\vdash p_{2}\) and is known as 'Modus Ponens'. In our study, we explore nine distinct inference rules of propositional logic, extensions of seven of them with one-variable and a universal quantifier, and two axioms of first-order logic as shown in Table 1. These inference rules provide a systematic framework for deriving valid conclusions.

First-order Logic (FOL)In this work, we consider a restricted set of logical axioms for FOL that utilize quantifiers, \(\forall\) (universal quantifier) and \(\exists\) (existential quantifier). The universal quantifier (\(\forall\)) denotes that a statement holds true for all instances within a specific category. In contrast, the existential quantifier (\(\exists\)) indicates that a statement is true for at least one instance within its scope. For instance, a simple extension of propositional 'Modus Ponens' is an inference rule where given the premises \(\forall(p(x)\to q(x))\) and \(p(a)\), we conclude \(q(a)\) (e.g., given "All kings are greedy" and "Sam is a king", we can conclude "Sam is greedy"). Here, we explore various axioms and inference rules that incorporate the quantifiers shown in Table 1.

Non-monotonic (NM) ReasoningIn this work, we analyze a range of logical reasoning templates in NM logics involving "Default Reasoning," Reasoning about Unknown Expectations," and "Reasoning about Priorities." These templates are inspired by the compilation [10] made in 1989 to evaluate the abilities of various non-monotonic logics that were being developed at that time. Below Table 2 shows examples of NM reasoning. Additional examples are given in Appendix B.3.

A key aspect of NM logics is to formalize notions such as "normally," "typically," and "usually" that are not directly formalizable using classical quantifiers in the first-order setting. The general rule "Heavy blocks are normally located on the table" does not imply that "All heavy blocks are

\begin{table}
\begin{tabular}{c|c|c} \hline
**Names** & **Propositional Logic** & **Extension to a (restricted) First-order Logic** \\ \hline MP & \(((p\to q)\wedge p)\vdash q\) & \((\forall x(p(x)\to q(x))\wedge\neg q(a))\vdash q(a)\) \\ \hline MT & \(((p\to q)\wedge\neg q)\vdash\neg p\) & \((\forall x(p(x)\to q(x))\wedge\neg q(a))\vdash\neg p(a)\) \\ \hline HS & \(((p\to q))\wedge(q\to r)\vdash(p\to r)\) & \((\forall x((p(x)\to q(x))\wedge(q(x)\to r(x)))\vdash p(a)\to r(a))\) \\ \hline DS & \(((p\lor q)\wedge\neg p)\vdash q\) & \((\forall x(p(x)\lor q(x))\wedge\neg p(a))\vdash q(a)\) \\ \hline CD & \(((p\to q)\wedge(r\to s)\wedge(r\lor r))\vdash(q\lor s)\) & \((\forall x((p(x)\to q(x))\wedge(r(r)\to s))\wedge(p(a)\lor r(a)))\vdash(q(a) \lor s(a))\) \\ \hline DD & \(((p\to q)\wedge(r\to s)\wedge(\neg q\lor\neg s))\vdash(\neg p\lor\neg r)\) & \((\forall x((p(x)\to q(x))\wedge(r(r)\to s(x)))\wedge(\neg q(a)\lor\neg s(a))) \vdash(\neg p(a)\lor\neg r(a))\) \\ \hline BD & \(((p\to q)\wedge(r\to s)\wedge(\rho\lor\neg s))\vdash(q\lor\neg r)\) & \((\forall x((p(x)\to q(x))\wedge(r(x)\to s(x)))\wedge(p(a)\lor s(a))) \vdash(q(a)\lor\neg r(a))\) \\ \hline CT & \((p\lor q)\vdash(q\lor p)\) & - \\ \hline MI & \((p\to q)\vdash(\neg p\lor\psi)\) & - \\ \hline EI & - & \(\exists xP\,(x)\Rightarrow P\,(a)\) \\ \hline UI & - & \(\forall x\,A\to A(x\to a)\) \\ \hline \end{tabular}
\end{table}
Table 1: Inference rules and (two) axioms that establish the relationship between premises and their corresponding conclusions. MP: Modus Ponens, MT: Modus Tollens, HS: Hypothetical Syllogism, DS: Disjunctive Syllogism, CD: Constructive Dilemma, DD: Destructive Dilemma, BD: Bidirectional Dilemma, CT: Commutation, MI: Material Implication, EI: Existential Instantiation, UI: Universal Instantiationalways located on the table". Rather, this rule allows for exceptions. Our work explores various NM reasoning types, as depicted in Figure 1, to delve deeper into the nuances of this type of reasoning.

### Data Creation

Our data creation procedure, illustrated in Figure 2, consists of three stages:

1. **Sentence Generation:** Starting with a given prompt, we generate coherent sentences and their negations that incorporate different ontologies.
2. **NL Conversion:** Using predefined templates of reasoning patterns based on their formal expressions, we convert the generated sentences into _(context, question, answer)_ triplets.
3. **Variation Generation:** We generate semantically preserving and inverting variations of these triplets to add more diversity.

By following this method, we construct _LogicBench_, and examples of generated data corresponding to each logic type and reasoning patterns are presented in Appendix B.

#### 2.2.1 Sentence Generation

Here, the first step is to generate sentences with diverse _ontologies_. An ontology represents a collection of concepts (e.g. car, person, animals, etc.) along with their corresponding associated

\begin{table}
\begin{tabular}{l|l} \hline \hline \multicolumn{1}{c|}{**Basic Default Reasoning**} & \multicolumn{1}{c}{**Default Reasoning with Irrelevant Information**} \\ \hline Context: Blocks A and B are heavy. & Context: Blocks A and B are heavy. \\ Heavy blocks are typically located on the table. & Heavy blocks are typically located on the table. \\ A is not on the table. & A is not on the table. \\  & B is red. \\ Conclusion: B is on the table. & Conclusion: B is on the table. \\ \hline \multicolumn{1}{c|}{**Reasoning about Unknown Expectations**} \\ \hline Context: Blocks A, B, and C are heavy. & \multicolumn{1}{c}{} \\ Heavy blocks are normally located on the table. & \multicolumn{1}{c}{} \\ At least one of A, B is not on the table. & \multicolumn{1}{c}{} \\ \multicolumn{1}{c|}{} \\ Conclusion: C is on the table. & \multicolumn{1}{c}{} \\ Exactly one of A, B is not on the table. & \multicolumn{1}{c}{} \\ \hline \hline \end{tabular} 
\begin{tabular}{l} \hline \multicolumn{1}{c|}{**Reasoning about Priorities**} \\ \hline Context: Blocks A, B, and C are heavy. & Context: Jack asserts that block A is on the table. \\ Heavy blocks are normally located on the table. & Mary asserts that block A is not on the table. \\ At least one of A, B is not on the table. & When people assert something, they are normally right. \\  & \\ Conclusion: C is on the table. & \multicolumn{1}{c}{} \\ Exactly one of A, B is not on the table. & \multicolumn{1}{c}{} \\ \hline \hline \end{tabular}
\end{table}
Table 2: Illustrative examples of non-monotonic reasoning adapted from [10]

Figure 2: Schematic representation of three-stage procedure for data creation. NL: Natural Language

properties. To generate these sentences, we prompt the GPT-3 model with instructions tailored for each inference rule. The prompt schema, as depicted in Figure 3, comprise three crucial components:

_Definition_ provides a detailed explanation of the task and offers a natural language representation of the reasoning pattern for which we are generating sentences.

_Examples_ provide sample sentences that need to be generated. We also illustrate how these sentences will be utilized in later stages, emphasizing the importance of coherence and the inclusion of relevant ontological concepts.

_Format_ We provide specific formatting instructions to guide the generation of sentences.

An example of a prompt corresponding to the 'Modus Tollens' from PL is presented in Appendix A for better illustration. Note that our objective at this stage is not to generate logical sentences but rather to generate a diverse and coherent set of sentences that encompass various concepts. We also create a negation sentence corresponding to each generated sentence2. In this work, the scope of generating negations is simple (refer to Appendix C for examples), however, negations can be more complicated in the case of logic. These generated sentences will be combined with logical connectives in a later stage to form context and questions.

Footnote 2: We use https://github.com/dmlls/negate to generate negated sentences

#### 2.2.2 NL Conversion

We focus on leveraging the formal expressions of reasoning patterns to create templates that establish the desired NL formulation for each logical connective. For instance, implication: "\(p\to q\)" is expressed as "If \(p\), then \(q\)", conjunction: "\(p\wedge q\)" is expressed as "\(p\) and \(q\).", and disjunction: "\(p\lor q\)" is expressed as "At least one of the following is true: (1) \(p\) and (2) \(q\). Note that we do not know which of (1) and (2) is true. It is possible that only (1) is true, or only (2) is true, or both are true."

With these established formulations, we proceed to utilize the sentences generated in SS2.2.1 to create the context and questions corresponding to reasoning patterns. For instance, let's consider the "Modus Tollens" from PL (\(((p\to q)\wedge\neg q)\vdash\neg p\)), and the "Bidirectional Dilemma" from FOL (\(\forall x((p(x)\to q(x))\wedge(r(x)\to s(x)))\wedge(p(a)\lor\neg s(a))) \vdash(q(a)\vee\neg r(a))\)). Table 3 presents examples of logical context and questions for these inference rules, and Appendix C showcases further examples corresponding to each inference rule and patterns from _LogicBench_.

#### 2.2.3 Variation Generation

After generating the context and questions in SS2.2.2, we generate semantically preserving and inverting variations of questions. Let's consider the example of "Modus Tollens" from Table 3, where the question is: "If he won't order pizza for dinner, does this imply that Liam didn't finish his work early?" In this question, we observe two propositions: \(s_{1}\), representing the statement "Liam didn't finish his work early," and \(s_{2}\), representing the statement "He won't order pizza for dinner." By perturbing these propositions, we can create four possible tuples: \(<s_{1},s_{2}>,<\neg s_{1},s_{2}>,<s_{1},\neg s_{2}>,<\neg s_{1},-s_{2}>\). Each tuple represents a combination of true or negation values for the propositions. Although it is possible to create more combinations from \(<s_{1},\neg s_{1}>\), and \(<s_{2},\neg s_{2}>\), we refine and restrict the set of triplets to exclude those that undermine the validity of the inference rule. To generate question variations, we replace the propositions in the original question with the corresponding tuples from the generated variations, hence, adding more diversity to _LogicBench_. This process allows us to create different variations of the question, as illustrated in Figure 2 (Step 3). More examples of question variations are in Appendix B.

Figure 3: Schematic representation of prompt.

### Statistics and Qualitative Analysis

StatisticsWe introduce two versions of our proposed dataset: _LogicBench(Eval)_ and _LogicBench(Aug)_. Statistics of both versions are presented in Table 4. Here, _LogicBench(Eval)_ is created using the above method along with human-in-loop to ensure the quality of generated data, whereas _LogicBench(Aug)_ is only a synthetically augmented version for training purposes.

These two versions aim to accommodate different evaluation and training needs to explore logical reasoning. Considering the cost and complexity associated with recent LLMs such as GPT-3, and GPT-4, we believe that _LogicBench(Eval)_ provides a more feasible evaluation benchmark.

Quality of DataThroughout the data generation phase of _LogicBench(Eval)_, the authors conduct a review of the logical formations to ensure they adhered to the intended structure. We examine each reasoning pattern for any potential discrepancies, ensuring that they were logically sound and correctly represented the intended relationships between propositions. In addition to the logical formation, we also dedicated considerable effort to eliminating typos and validating the grammar.

## 3 Results and Analysis

### Experimental Setup

Task FormulationWe formulate binary classification task using _LogicBench_ to evaluate the logical reasoning ability of LLMs. Let us consider a set of data instances \(\mathcal{I}_{a,L}\) corresponding to axiom \(a\) and logic type \(L\). In this set, \(i^{th}\) instance is represented as \(\mathcal{I}^{i}_{a,L}=\{(c_{i},Q_{i})\}\) where \(c_{i}\) represents context and \(Q_{i}=\{q_{1},q_{2},...,q_{n}\}\) represents set of question and its variations corresponding to \(i^{th}\) instance. As discussed in SS2, each context (\(c\)) represents logical rules (e.g., All cats have fur. Tom is a cat.) and question (\(q\)) represents the conclusion (e.g., Does Tom have fur?). To each context and question pair, i.e., \(<c,q>\), we assign a label from the set \(\mathcal{Y}=\{Yes,No\}\). We assign a label \(Yes\) if the conclusion logically entails the context, otherwise, assign a label \(No\). To evaluate any model on this setup, we provide \(<c,q>\) as input to predict a label from \(\mathcal{Y}\).

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline \multirow{2}{*}{**Dataset**} & **\# of Instances** & **Total \# of** & **Total \# of Instances** \\  & **per Axiom** & **Instances** & **(Including Variations)** \\ \hline _LogicBench(Eval)_ & 20 & 500 & 1720 \\ _LogicBench(Aug)_ & 150 & 3750 & 12908 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Statistics of the _LogicBench(Eval)_ and _LogicBench(Aug)_

\begin{table}
\begin{tabular}{c|l|l} \hline \hline Axiom & \multicolumn{2}{c|}{Generated sentences in stage 1} & \multicolumn{2}{c}{Context and Question} \\ \hline \multirow{4}{*}{Modus Tollens} & p: Lam finished his work early, & **Context: If Lam finished his work early, then he will** \\  & –p: Liam did not finish his work early. & order pizza for dinner. \\  & q: He will order pizza for dinner. & \multirow{2}{*}{**Question: If he won’t order pizza for dinner, does** **this imply that Liam didn’t finish his work early?**} \\ \cline{3-3}  & –q: He will not order pizza for dinner. & & **Context: If someone drinks lots of water, then they will** \\ \hline \multirow{4}{*}{Bidirectional} & p(x): someone drinks lots of water & **Context: If someone drinks lots of water, then they will** \\  & q(x): they will feel hydrated & feel hydrated. If they eat too much sugar, then they will** \\  & r(x): they eat too much sugar & experience a sugar crash. We know that at least one of \\  & s(x): they will experience a sugar crash & the following is true (1) Jane drinks lots of water and (2) \\  & p(a): Jane drinks lots of water & she won’t experience a sugar crash. Note that we do not** \\  & –p(a): Jane does not drink lots of water & know which ones of (1) and (2) are true. It might be the \\  & q(a): she will feel hydrated & \multirow{2}{*}{case that only (1) is true, or only (2) is true or both are true.} \\  & r(a): she seats too much sugar & & **Question: If at least one of (1) and (2) is true,** \\  & –(ra): she does not eat too much sugar & & **Question: If at least one of the following must always be** \\  & s(a): she will experience a sugar crash & true? (a) she will feel hydrated and (b) she doesn’t eat too** \\  & –s(a): she will not experience a sugar crash & much sugar. \\ \hline \hline \end{tabular}
\end{table}
Table 3: Illustrative examples of logical context and questions created using sentences that are generated in the first stage §2.2.1.

ExperimentsWe evaluate easily available and widely used prompting models (i.e., GPT-3 (davinci-003) and ChatGPT), and instruction-tuned models (FLAN-T5 and Tk-instruct) on _LogicBenchEval_. Since logical reasoning is an important aspect of different QA tasks, we also evaluate UnifiedQA. Each model is evaluated in a zero-shot setting where the prompt is provided to the model without any in-context examples. This approach allows us to determine LLM's inherent ability to do logical reasoning (based on pre-training), as we can not expect that various logical inference rules/patterns will always be made part of prompts. However, we do evaluate these models in a few-shot setting, and present the results in Appendix F. We also present exploratory - only exploratory because of the limited availability of their inference APIs - analysis over Bard and GPT-4 in Appendix G.

In addition, we employed the T5-large model and trained it on the _LogicBenchAug_ resulting in a model named LogicT5. LogicT5 has achieved \(\sim 97\%\) of accuracy on _LogicBenchEval_ since it is evident that supervised fine-tuning improves results by a large margin. Subsequently, we performed fine-tuning on four other logical reasoning datasets: LogiQA, Reclor, LogicNLI, and FOLIO. Our experiments were carried out in two settings: single-task (fine-tuning and evaluation on one dataset) and multi-task (fine-tuning on all four datasets combined, with separate evaluations for each dataset). A detailed experimental setup is described in Appendix D.

MetricsHere, we evaluate performance in terms of accuracy corresponding to each label, i.e., \(A(Yes)\) and \(A(No)\). We evaluate each model on three different prompts and report average results across these prompts. All prompts used for experiments are described in Appendix D.

### Benchmark Results

Table 5 represents label-wise accuracy (\(A(Yes)\) and \(A(No)\)) corresponding to each LLMs. Here, we focus on analyzing the \(A(Yes)\) since the aim is to understand the model's logical reasoning capabilities in answering the question where the conclusion entails the logical context. Table 5 provides valuable insights into the performance of different models on various logic types. For PL, UnifiedQA exhibits an average performance of 15%, while FLAN-T5 and Tk-instruct achieve \(\sim 25\%\). GPT-3 demonstrates a performance of 57.6%, and ChatGPT achieves 46.8%. Moving on to FOL, these models showcase performance accuracy of 52.7%, 51.2%, 55.7%, 76.2%, and 72.6% for UnifiedQA, FLAN-T5, Tk-instruct, GPT-3, and ChatGPT, respectively. On the NM reasoning, these models show an accuracy of 63.5%, 56.2%, 56.3%, 62%, and 70.9%, respectively. Overall, these models display an average performance of \(\sim 34\%\), \(\sim 61\%\), and \(\sim 62\%\) on PL, FOL, and NL.

From Table 5, we can observe that models struggle more with inference rules of PL compared to FOL and NM reasoning. Furthermore, it is noticeable that each model performs relatively better on questions with a negative response (i.e., \(No\)) compared to questions with a positive response (i.e., \(Yes\)). This observation suggests that the models struggle to fully comprehend the logical relationship between the context and the conclusion (i.e., lower \(A(Yes)\)). However, they demonstrate a relatively stronger understanding when the relationship is contradictory in nature (i.e., higher \(A(No)\)). However, analyzing the performance of the models on inference rules is crucial to understand their limitations. Table 5 presents the inference rule-wise performance for each model as well.

### Analysis and Discussion

Large models are better logical reasoners.Based on the observed performance from Table 5, it becomes evident that larger model sizes and extensive pre-training data contribute to a better understanding of logical aspects. Consequently, models with larger sizes tend to exhibit higher performance across different types of logic. Nonetheless, the average performance remains at around \(52.7\%\), indicating room for improvement in these models' logical comprehension capabilities.

Negations are hard to understand when embedded with logical rules.Regarding PL and FOL, it is apparent that the models struggle more with the DS, DD, and MT inference rules. A closer look at Table 1 reveals that all of these axioms include examples where the models need to draw conclusions based on negated premises. This indicates that the models encounter difficulties when 

[MISSING_PAGE_EMPTY:8]

while the model can effectively reason the initial section of the _disjunctive syllogism_ involving two possibilities \(p\) or \(q\), it encounters challenges in deducing whether \(q\) should follow from the \(\neg p\). For FOL, ChatGPT encounters challenges in comprehending longer logical contexts, resulting in a lack of confidence in establishing the relationship between given propositions. Furthermore, to derive an accurate conclusion when the rules are followed correctly, the model relies on supplementary evidence. We observe that ChatGPT encounters difficulties in comprehending the nuanced meanings of words such as "usually", "normally" and "typically" when establishing sentence relationships within NM reasoning. Notably, when it comes to the rule of default reasoning, ChatGPT fails to grasp inherent associations between two entities that commonly share characteristics. Examples and more analysis of generated explanations for each logic type are presented in Appendix E.

## 4 Related Work

LogiQA [11] and ReClor [19] have made notable contributions by compiling multichoice questions from standardized graduate admission examinations that demand diverse forms of logical reasoning. However, in contrast to our LogicBench, these datasets involve complex mixed forms of reasoning and do not specifically focus on assessing logical reasoning in isolation. A few past attempts have been made to create datasets to evaluate only logical reasoning while excluding other forms of reasoning. For example, CLUTTER [15] covers inductive reasoning, [5] covers temporal logic, and Ruletaker [4] evaluates whether a transformer-based model emulates deductive reasoning over synthetically generated statements in a limited setting. LogicNLI [16] introduced a diagnostic benchmark for FOL reasoning, with the dataset constructed by first automatically generating logic expressions and then replacing the entity and attribute placeholders in the logic expressions with simple and random subjects and predicates. FOLIO [6] gives diverse and complex logical expressions, however, it is only limited to FOL. ProntoQA [14] provides explanation and reasoning steps but is limited to modus ponens in FOL. Additional datasets for evaluating logical reasoning also exist such as TaxiNLI [7] introduce logical taxonomy in NLI task and RuleBert [13] covers only soft logical rules. In summary, LogicBench is evaluate logical reasoning in isolation and provides more diverse inference rules and logic types compared to existing datasets. Extended related work is discussed in Appendix H.

## 5 Conclusions

To study the logical reasoning ability of LLMs, we introduced a novel benchmark called _LogicBench_ which consists of 25 distinct inference rules and reasoning patterns covering propositional, first-order, and non-monotonic logics. We released two versions of the dataset: _LogicBench(Eval)_ and _LogicBench(Aug)_. _LogicBench(Eval)_ serves as a high-quality, cost-effective, and reliable dataset for evaluating LLMs, while _LogicBench(Aug)_ can be utilized for training purposes. Through comprehensive experiments, we showed that models such as GPT-3 and ChatGPT do not perform well on _LogicBench_, even though they require the application of only a single inference rule in positive (i.e., label 'Yes') data instance. Furthermore, we demonstrated that LLMs trained using _LogicBench(Aug)_ showcase an improved understanding of logical reasoning, resulting in a better performance on existing logic datasets. Though _LogicBench_ facilitates the evaluation and improvement of the logical reasoning ability of LLMs, it can be further extended by incorporating other inference rules and logic types; and having data instances that require applications of multiple inference rules.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Methods & Models & LogiQA & FOLIO & LogicNLI & ReClor \\ \hline \multirow{2}{*}{Single-Task} & T5-large & 16.8 & 69.6 & 82.3 & 35.4 \\  & LogicT5 & **16.9** & **71.2** & **84.4** & **36.8** \\ \hline \hline \multirow{2}{*}{Multi-Task} & T5-large & **21.8** & 83.8 & 68.2 & **42.8** \\  & LogicT5 & 19.7 & **85.6** & **69.8** & 40.0 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance comparison between LogicT5 and baseline T5-large in terms of accuracy.

## References

* Beygi et al. [2022] Sajjad Beygi, Maryam Fazel-Zarandi, Alessandra Cervone, Prakash Krishnan, and Siddhartha Jonnalagadda. Logical reasoning for task oriented dialogue systems. In _Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5)_, pages 68-79, Dublin, Ireland, May 2022. Association for Computational Linguistics.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901. Curran Associates, Inc., 2020.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Clark et al. [2021] Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language. In _Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence_, pages 3882-3890, 2021.
* Hahn et al. [2021] Christopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus Norman Rabe, and Bernd Finkbeiner. Teaching temporal logics to neural networks. In _International Conference on Learning Representations_, 2021.
* Han et al. [2022] Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et al. Folio: Natural language reasoning with first-order logic. _arXiv preprint arXiv:2209.00840_, 2022.
* Joshi et al. [2020] Pratik Joshi, Somak Aditya, Aalok Sathe, and Monojit Choudhury. TaxiNLI: Taking a ride up the NLU hill. In _Proceedings of the 24th Conference on Computational Natural Language Learning_, pages 41-55, Online, November 2020. Association for Computational Linguistics.
* Khashabi [2019] Daniel Khashabi. _Reasoning-Driven Question-Answering for Natural Language Understanding_. University of Pennsylvania, 2019.
* Khashabi et al. [2020] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 1896-1907, Online, November 2020. Association for Computational Linguistics.
* Lifschitz [1989] Vladimir Lifschitz. Benchmark problems for formal nonmonotonic reasoning: Version 2.00. In _Non-Monotonic Reasoning: 2nd International Workshop Grassau, FRG, June 13-15, 1988 Proceedings 2_, pages 202-219. Springer, 1989.
* Liu et al. [2021] Jian Liu, Leyang Cui, Hanneng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. In _Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence_, pages 3622-3628, 2021.
* OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
* Saeed et al. [2021] Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. RuleBERT: Teaching soft rules to pre-trained language models. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 1460-1476, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

* [14] Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. In _The Eleventh International Conference on Learning Representations_, 2023.
* [15] Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 4506-4515, Hong Kong, China, November 2019. Association for Computational Linguistics.
* [16] Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, and Yaohui Jin. Diagnosing the first-order logical reasoning ability through LogicNLI. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 3738-3747, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
* [17] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 5085-5109, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
* [18] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _ICLR_, 2021.
* [19] Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: A reading comprehension dataset requiring logical reasoning. In _International Conference on Learning Representations_.
* [20] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _arXiv preprint arXiv:2303.18223_, 2023.

## Paper Checklist

* [1]
* Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Yes
* [2] Have you read the ethics review guidelines and ensured that your paper conforms to them? Yes
* [3] Did you discuss any potential negative societal impacts of your work? No, we do not expect negative societal impacts as a direct result of the contributions in our paper
* [4] Did you describe the limitations of your work? Yes, refer to Section 5.

If you are including theoretical results...

1. Did you state the full set of assumptions of all theoretical results? N/A
2. Did you include complete proofs of all theoretical results? N/A

If you ran experiments...

1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? Yes, the anonymous URL is at the end of the abstract.
2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? Yes, refer to Section 3.1 and Appendix D.
3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? Yes, we reported the average results across three prompts (refer to Section 3.1).
4. Did you include the amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? Yes, refer to Appendix D.

If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...

1. If your work uses existing assets, did you cite the creators? Yes, refer to Section 1, Section 3, and Appendix D.
2. Did you mention the license of the assets? Yes, refer to Appendix D.
3. Did you include any new assets either in the supplemental material or as a URL? Yes
4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? N/A
5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? Yes, the collected data does not contain personally identifiable information or offensive content.

If you used crowdsourcing or conducted research with human subjects...

1. Did you include the full text of instructions given to participants and screenshots, if applicable? N/A
2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? N/A
3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? N/A