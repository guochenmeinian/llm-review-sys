# InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding

Junda Wu\({}^{1}\)1  Tong Yu\({}^{2}\)1  Rui Wang\({}^{3}\)  Zhao Song\({}^{2}\)  Ruiyi Zhang\({}^{2}\)

Handong Zhao\({}^{2}\)  Chaochao Lu\({}^{4}\)2  Shuai Li\({}^{5}\)3  Ricardo Henao\({}^{3,6}\)

\({}^{1}\)University of California, San Diego \({}^{2}\)Adobe Research \({}^{3}\)Duke University

\({}^{4}\)University of Cambridge \({}^{5}\)Shanghai Jiao Tong University \({}^{6}\)KAUST

juw069@ucsd.edu

{tyu,zsong,ruizhang,hazhao}@adobe.com

{rw161,ricardo.henao}@duke.edu

cl641@cam.ac.uk shuaili8@sjtu.edu.cn

Footnote 1: These authors contributed equally to this work.

Footnote 2: The work was done previously when this author was at the University of Cambridge. This author is now at Shanghai AI Laboratory.

Footnote 3: Corresponding author.

###### Abstract

Soft prompt tuning achieves superior performances across a wide range of few-shot tasks. However, the performances of prompt tuning can be highly sensitive to the initialization of the prompts. We have also empirically observed that conventional prompt tuning methods cannot encode and learn sufficient task-relevant information from prompt tokens. In this work, we develop an information-theoretic framework that formulates soft prompt tuning as maximizing the mutual information between prompts and other model parameters (or encoded representations). This novel view helps us to develop a more efficient, accurate and robust soft prompt tuning method, InfoPrompt. With this framework, we develop two novel mutual information based loss functions, to (i) explore proper prompt initialization for the downstream tasks and learn sufficient task-relevant information from prompt tokens and (ii) encourage the output representation from the pretrained language model to be more aware of the task-relevant information captured in the learnt prompts. Extensive experiments validate that InfoPrompt can significantly accelerate the convergence of the prompt tuning and outperform traditional prompt tuning methods. Finally, we provide a formal theoretical result to show that a gradient descent type algorithm can be used to train our mutual information loss.

## 1 Introduction

Soft prompt tuning has shown great successes in a wide range of natural language processing tasks, especially in low-resource scenarios [60, 37, 66]. With a relatively small size of prompt parameters appended to the input of the context, the language model can be adapted to the downstream tasks with the large scale pretrained parameters frozen. Compared with conventional fine tuning methods, prompt tuning requires less memory and computational resources to update these significantly smaller sized prompt parameters. In addition, in low-shot learning scenarios, prompt tuning can prevent the language model from overfitting on the training data, thus maintaining the generalization ability of pretrained language models.

However, recent works reveal that it is non-trivial to find a proper initialization of the prompt tokens. Several works have investigated the effect of prompt initialization on the prompt tuning performances[91, 101] and showed that the performances of prompt tuning are highly sensitive to the prompt initialization. However, since the proper prompt initialization can vary to different downstream tasks and pretrained language models, it is hard to find very accurate knowledge to guide us to obtain the proper initialization [13].

In addition to the above limitations,, we also empirically observe that conventional prompt tuning methods cannot effectively learn sufficient task-relevant information from prompt tokens. Specifically, the prompts may fail to learn sufficient information that is relevant to the downstream tasks. To understand the relevance between the prompt tokens and downstream tasks, we calculate the conditional mutual information (CMI) between the prompt tokens and the latent representation from the language model conditioned on the input context. We follow [40] in determining the positions of prompt tokens inserted between the input sentences. Figure 1 shows the distribution of CMI of the prompts resulting from different methods. The randomly sampled prompts have the lowest CMI. The prompts learned by a soft prompt tuning method, WARP [40], can have relatively higher CMI than the handcrafted ones. By directly maximizing the CMI, our InfoPrompt (detailed in Section 3) facilitates learning of more informative prompts. Without the guidance of task-relevant information, randomly exploring the optimal prompts within the large continuous embedding space of the prompt tokens can be inefficient, _i.e._ a similar challenge is also discussed in [75]. Some related results [75, 37, 78] show that prompt tuning takes much larger numbers of epochs to converge than fine tuning. Comparatively, thanks of the guidance of CMI, our propose InfoPrompt allows prompt tuning to converge much faster. We also provide theoretical guarantees of the convergence of the proposed losses when training with gradient descent based algorithms.

Overall, we develop an information-theoretic framework that formulates soft prompt tuning as maximizing mutual information between prompts and other model parameters (or encoded representations), conditioned on the input context. With this framework, we develop InfoPrompt with two novel mutual information based loss functions. (i) To explore proper prompt initialization for the downstream tasks and learn prompts with sufficient task-relevant information, we optimize the _head loss_ which maximizes the mutual information between the prompt and the task-specific classification head. By optimizing this head loss, the prompt can effectively learn task-relevant information from the downstream tasks, since the task-specific classification head usually contains the information from the downstream tasks. Besides, the mutual information loss can help to guide the learning of the prompt tokens in the early steps to tackle the initialization challenge, since the classification head parameters can learn the downstream task information more quickly. (ii) To further encourage the output representation from the pretrained language model (_i.e._, encoded representation) to be more aware of the task-relevant information captured in the learnt prompt, we optimize the _representation loss_ which maximizes the conditional mutual information between the prompt embeddings and the feature representations conditioned on the input context.

Our contributions are summarized as:

* We revisit the challenge of initialization with prompt tuning and show that existing prompt tuning methods fail to learn sufficient task-relevant information.
* We propose InfoPrompt, a framework to solve such challenges from a information-theoretic perspective. Specifically, we develop two novel loss functions that effectively find proper

Figure 1: Distributions of the CMI metrics of the prompts learned or handcrafted from different methods on MRPC and SST-2 [95]. By our InfoPrompt, the relevance between the prompt tokens and downstream tasks is the highest among all methods.

prompt initialization and learn sufficient task-relevant information from down-stream tasks, without requiring any prior knowledge.
* Extensive experiments on multiple tasks and datasets validate that, InfoPrompt can significantly accelerate the convergence of the prompt tuning and outperform existing prompt tuning methods with higher accuracy.
* We provide a formal theoretical result to show that our proposed loss functions can be optimized using gradient descent based algorithm with convergence guarantees.

## 2 Preliminary

### Prompt Tuning

Prompt tuning has shown great successes in a wide range tasks of NLP [60; 37; 66]. Let \(\Phi\) denote the encoder of a pretrained language model, _e.g._, the Roberta-Large [67]. Assume \(X=\{x_{1},x_{2},\cdots,x_{n}\}\) is a length-\(n\) text sequence of and \(Y\) is its classification label. In prompt tuning, we add extra information \(P\) for the encoder to condition on for its prediction of \(Y\). \(P=\{p_{1},\ldots,p_{n_{p}}\}\) is a sequence of prompt embeddings and \(n_{p}\) is the number of prompt tokens. \(p_{i}\in\mathbb{R}^{D}\), \(i=1,\cdots,n_{p}\), is an embedding vector with dimension \(D\) and \(D\) is also the embedding dimension of the pretrained language model. We first embed each token of \(X\) into its corresponding token embedding from the pretrained language model. \(P\) is inserted into the resulting embedding sequence of \(X\), and the resulting sequence is further encoded by the pretrained encoder \(\Phi\) into the representation space of the pretrained language model. The template for inserting of prompt tokens is detailed in Section 4. Formally, we denote such a process by \(Z=\Phi(P,X)\), with \(Z\) being the output representation from the pretrained encoder. The model prediction for \(X\) is made on top of \(Z\) with a trainable classification head parameterized by \(\theta\), denoted as \(h_{\theta}\), whose output \(h_{\theta}(Z)\) is the probability distribution over all possible classification labels. For classification, the prompts are trained via minimizing the following loss function,

\[\mathcal{L}_{\mathrm{pred}}=\mathrm{cross\_entropy}(h_{\theta}(Z),Y)\]

Parameters of the pretrained encoder \(\Phi\) is frozen during training. Different from previous works (_e.g._, [60]) where the prompts are directly learnt, the prompts in our approach are encoded from the input \(X\). In this way, the resulting prompts can better capture task-relevant information from the training text \(X\). We will elaborate on how the prompts are encoded from input \(X\) in Section 4.

### Mutual Information

Mutual information (MI) is a metric in information theory [79; 19], which quantifies the amount of information shared between two random variables. The mutual information between two random variables \(A\) and \(B\) is

\[\mathcal{I}(A;B)=\mathbb{E}_{p(a,b)}\left[D_{KL}\left[p(a|b)\|p(b)\right] \right].\]

Inspired by [89], we use MI as the criterion for comparing prompts and other model parameters. MI has also been applied to measure the similarity between the masked token and the corresponding context in the pretraining of Multilingual Masked Language Modeling [15], the relevance between documents and sentences in document summarization [72] and the source and target sentences in Neural Machine Translation (NMT) [107].

## 3 Our Method: InfoPrompt

As mentioned above, we want the learnt prompts to be task-relevant. To achieve this, we notice that the classification head is trained with both the data representation and the classification labels, thus should contain rich information of the learnt tasks. In encouraging the task-relevancy of the learnt prompts, we consider maximizing the mutual information between the prompt and the parameters of the classification head, denoted as \(\theta\). By maximizing such mutual information, the learnt prompt will be more aligned with the training data with which the classification head is trained, thus captures more task-relevant information from training. Further, in order for the pretrained language model to properly leverage the task-relevant information in the prompt, we additionally maximize the mutual information between the prompt and the representation from the pretrained language model, so that the encoded representation can be aware of the task-relevant information captured by the prompt. In addition, we also provide theoretical guarantees of the convergence of those losses when training with gradient descent, demonstrating that our method can converge more easily than existing prompt tuning methods. Below, we denote the negative mutual information between the prompt and parameters of the classification head as the _head loss_. The negative mutual information between the prompt and representations from the pretrained language model is denoted as the _representation loss_.

### The Head Loss

The head loss is the negative mutual information between the prompt \(P\) and parameters \(\theta\), _i.e._, \(-I(P;\theta|X)\). In maximizing \(I(P;\theta|X)\), we follow [68] that approximates it with the following lower bound,

\[\mathcal{I}(P;\theta|X)\geq C+\mathcal{L}_{NCE}(P,\theta,X),\]

where \(C\) is a constant, \(\mathcal{L}_{NCE}\) is a Noise Contrastive Estimation (NCE) of mutual information,

\[\mathcal{L}_{NCE}=\mathbb{E}\left[\log\frac{\exp(l(P,\theta,X))}{\sum_{k=1}^{ K}\exp(l(P_{k},\theta|X))}\right],\]

and \(\{P_{k}\}_{k=1}^{K}\) are the negative prompt samples for contrastive learning. In practice, we randomly sample \(K-1\) tokens from the context as the negative samples, _i.e._, \(\{P_{k}\}_{k=2}^{K}\), and the positive sample is \(P_{1}=P\).

We model the score function \(l(P,\theta|X)\) as a standard bilinear function with the learnable matrix \(W_{1}\)

\[l(P,\theta|X)=P^{\top}W_{1}\theta.\]

where \(\theta\) and \(P\) are encoded from \(X\), and \(W_{1}\) is a trainable matrix. Since the classification head is learnt on top of the output from the last layer of the pretrained language model, the learning of its parameters \(\theta\) is easier than the learning of the prompt \(P\) (the input layer of the pretrained language model). Therefore, \(\theta\) may capture more task-relevant information than \(P\) in the early stage of training. By maximizing the mutual information between \(\theta\) and \(P\), the task-relevant information can be transferred to \(P\) in the initial training steps. In this way, \(P\) can be more task-relevant especially in the early training stage. Experiments in Section 6 also show that our head loss, \(\mathcal{I}(P;\theta|X)\), can facilitate the training of the initial training steps.

### The Representation Loss

The representation loss, denoted as \(-\mathcal{I}(P;Z|X)\), is defined as the negative of mutual information between the prompt \(P\) and the encoded representation from the pretrained language model, _i.e._, \(Z=\Phi(P,X)\). Similar to the head loss, we approximate the representation loss with its lower bound,

\[\mathcal{I}(P;Z|X)\geq\log(N)+\mathcal{L}_{NCE}(P,Z|X),\]

and,

\[\mathcal{L}_{\mathrm{NCE}}=\mathbb{E}\left[\log\frac{\exp(l(P,Z|X))}{\sum_{k= 1}^{K}\exp(l(P,Z_{k}|X))}\right],\]

\(\{Z_{k}\}_{k=1}^{K}\) are the negative samples. Here, we overload the notations of InfoNCE loss \(\mathcal{L}_{NCE}\) and score function \(l\) for conciseness. Let \(W_{2}\) be a trainable matrix, the function \(l\) for the representation loss is defined by,

\[l(P,Z|X)=P^{\top}W_{2}Z.\]

We use variational inference methods [46] to recover the latent distribution of \(Z\). Specifically, we assume that the latent distribution is \(N(\mu,\sigma)\), where \(N(\mu,\sigma)\) is the normal distribution with mean \(\mu\) and diagonal covariance matrix \(\sigma\). We model \(\mu\) and \(\sigma\) via,

\[\mu=f_{\mu}(Z),\sigma=f_{\sigma}(Z).\]\(f_{\mu}\) and \(f_{\mu}\) are trainable fully connected layers. Since the negative samples of \(Z\), _i.e._, \(\{Z_{k}\}_{k=1}^{K}\), should not be paired with \(P\), we assume the \(\{Z_{k}\}_{k=1}^{K}\) are drawn from \(N(\mu^{\prime},\sigma^{\prime})\), _s.t._,

\[\mu^{\prime}=f_{\mu}(Z^{\prime}),\sigma^{\prime}=f_{\sigma}(Z^{\prime}).\]

In contrast to \(Z=\Phi(P,X)\), we have \(Z^{\prime}=\Phi(X)\) where \(\{Z_{k}\}_{k=1}^{K}\) are not paired with \(P\). By maximizing the representation loss \(I(P;Z|X)\), we encourage the encoded representation \(Z\) to be more aware of the prompt \(P\), so that the task-relevant information in \(P\) can be properly encoded by the pretrained language model in producing \(Z\).

### Overall Objective

We minimize the following objective in prompt tuning:

\[\mathcal{L}=\mathcal{L}_{\mathrm{pred}}-\beta\cdot\mathcal{I}(P;Z|X)-\gamma \cdot\mathcal{I}(P;\theta|X).\] (1)

We denote \(\mathcal{L}_{\mathrm{pred}}\) as the task loss. \(\beta\) and \(\gamma\) are balancing parameters for the proposed representation loss and head loss, respectively. We denote our approach as _InfoPrompt_. More details about the implementation and configurations are provided in Section 4.2.

### Theoretical Guarantees

We state our main theoretical result as follows. Due to the space limit, we delay the proof into Appendix.

**Theorem 3.1**.: _Given the Loss function \(\mathcal{L}\) (Eq. (1)) and conditions specified in Appendix C.1 and \(D\), using gradient descent type of greedy algorithm, we can find the optimal solution of that loss function._

We provide theoretical guarantees of the convergence of those losses trained by conventional gradient descent type algorithms. In Section 4, we empirically observe that our method converges more easily than traditional soft prompt tuning methods and requires fewer training epochs.

## 4 Experiments

### Datasets

We conduct experiments with datasets of sequence classification from the GLUE benchmark [95], along with those of relation extraction tasks and NER tasks. We choose four sequence classification tasks from the GLUE benchmark: RTE (Recognizing Textual Entailment, [7]), MRPC (Microsoft Research Paraphrase Corpus, [28]), CoLA (Corpus of Linguistic Acceptability, [98]) and SST-2 (Sentence Sentiment Treebank, [81]). We choose these tasks because their datasets are of smaller sizes and prompt tuning is comparably more effective in low-resource settings [40; 62]. For the task of relation extraction, we evaluate our method on the ACE2005 corpus and the Semeval-2010 datasets [44]. We also use the ACE2005 corpus for the task of NER. Note that the entity spans for NER have been given ACE2005. Unlike the standard NER model that learns to predict the entity span and entity type simultaneously from the raw text sequence, our model only predicts the entity type based on the given entity span. We follow the same data splitting strategy for ACE2005 corpus as the previous work [103; 71]. For the Semeval-2010 tasks, we follow the official data partition [44].

### Experiment Settings

We follow the resource constrained scenario in [40] that trains each task with only 64 or 256 samples. We experiment with \(n_{p}=1\) and \(n_{p}=4\) prompt tokens for each task. The prompt tokens are inserted into the template for each task. Similar to [40], we adopt the RoBERTa-large model as our pretrained encoder. We freeze the pretrained parameters and only train the parameters of the prompt head and prompt tokens. During training, we empirically set \(\beta=0.1\) and \(\gamma=0.05\). The number of negative samples is \(K=32\). The learning rate is \(1e-3\) and the batch size is 8. For each task, we report the results after 30 epochs, averaged over 5 random seeds. To encode the prompt \(P=[p_{1},\cdots,p_{n_{p}}]\) from \(X\), we first encode \(X\) into \(P^{\prime}\in\mathbb{R}^{D}\) via \(P^{\prime}=\Phi(X)\). We denote the up-sampling and down-samplingprojections similar in [31]. For each \(p_{i}\in\mathbb{R}^{D}\), we have \(p_{i}=W_{i}^{\mathrm{up}}W_{i}^{\mathrm{down}}P^{\prime}\), \(W_{i}^{\mathrm{up}}\in\mathbb{R}^{D\times 64}\), \(W_{i}^{\mathrm{down}}\in\mathbb{R}^{64\times D}\).

For the tasks of sequence classification and relation extraction, we follow the template of [40] that contains a \([\text{mask}]\) token. The representation \(Z\) is obtained from the \([\text{mask}]\) token from the last layer of the RoBERTa-Large encoder. For the task of NER, we have the \([\text{mask}]\) token before the given entity span, with the rest being the same as for sequence classification.

### Baselines and Ablations

As mentioned above, our method with Eq. (1) in denoted as InfoPrompt. In the experiments, we compare our method with the following baselines:

* Finetuning: We fine tune all the parameters from the pretrained encoder on each task. Finetuning is included as the upper bound for the model performance, since it is more computational expensive compared with only training the prompt parameters.
* Adapter [45]: Similar to prompt tuning, this is also a way of parameter-efficient training for pretrained language models. Specifically, instead of adding the prompt tokens in the input, we add adapters after the feed-forward module in each transformer layer.
* WARP [40]: Different from our approach, the prompt tokens of WARP are not generated from the input sequence. the prompt tokens are insert into the input sequence. During training, the pretrained encoder is frozen and only the prompt tokens are trainable.
* IDPG [102]: Similar to our approach, the prompt tokens are generated from the input sequence. The pretrained encoder is frozen and the prompt generator is trainable.

In evaluating the effectiveness of our proposed loss functions, we consider the following two ablations:

* \(\gamma=0\): We disable the head loss during training via \(\gamma=0\), while keeping \(\beta=0.05\).
* \(\beta=0\): We disable the representation loss during training via \(\beta=0\), while keeping \(\gamma=0.1\).
* \(\beta=\gamma=0\): We disable both the losses. The prompt parameters are trained with \(L_{\mathrm{pred}}\).

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c} \hline  & \multicolumn{2}{c|}{CoLA} & \multicolumn{2}{c|}{RTE} & \multicolumn{2}{c|}{MRPC} & \multicolumn{2}{c|}{SST2} & \\  & \(n_{p}=1\) & \(n_{p}=4\) & \(n_{p}=1\) & \(n_{p}=4\) & \(n_{p}=1\) & \(n_{p}=4\) & \(n_{p}=1\) & \(n_{p}=4\) & Average \\ \hline Finetuning & 0.6131 & & 0.7798 & 0.8873 & 0.9427 & 0.8057 \\ Adapter [45] & 0.5552 & & 0.5776 & 0.6814 & 0.9472 & 0.6904 \\ \hline WARP [40] & 0.5282 & 0.5911 & 0.6282 & 0.6426 & 0.8039 & 0.8186 & 0.9507 & 0.9587 & 0.7403 \\ IDPG [102] & 0.5556 & 0.5646 & 0.6282 & 0.6534 & 0.7941 & 0.8039 & 0.9587 & 0.9587 & 0.7396 \\ InfoPrompt & 0.5631 & 0.6018 & 0.6751 & 0.6968 & 0.8039 & 0.8137 & 0.9576 & 0.9599 & 0.7590 \\ \(\gamma=0\) & 0.5699 & 0.5853 & 0.6751 & 0.6787 & 0.7941 & 0.8137 & 0.9495 & 0.9587 & 0.7531 \\ \(\beta=0\) & 0.5546 & 0.5579 & 0.6065 & 0.6318 & 0.7892 & 0.7966 & 0.9472 & 0.9610 & 0.7306 \\ \(\gamma=0,\beta=0\) & 0.5032 & 0.5732 & 0.6173 & 0.6029 & 0.7917 & 0.7672 & 0.9495 & 0.9564 & 0.7202 \\ \hline \end{tabular}
\end{table}
Table 1: Results on Sequence Classification.

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline  & \multicolumn{2}{c|}{RE} & \multicolumn{2}{c|}{NER} & \multicolumn{2}{c|}{SemEval} & \\  & \(n_{p}=1\) & \(n_{p}=4\) & \(n_{p}=1\) & \(n_{p}=4\) & \(n_{p}=1\) & \(n_{p}=4\) & Average \\ \hline Finetuning & 0.8119 & & 0.9054 & & 0.8506 & & 0.8560 \\ Adapter [45] & 0.5073 & & 0.8329 & & 0.6570 & & 0.6657 \\ \hline WARP [40] & 0.6384 & 0.6596 & 0.8174 & 0.8607 & 0.6702 & 0.7284 & 0.7291 \\ IDPG [102] & 0.6079 & 0.6132 & 0.8360 & 0.8931 & 0.6408 & 0.6776 & 0.7114 \\ InfoPrompt & 0.6914 & 0.7616 & 0.8526 & 0.8962 & 0.7563 & 0.7917 & 0.7916 \\ \(\gamma=0\) & 0.6914 & 0.7285 & 0.8452 & 0.8635 & 0.7471 & 0.7865 & 0.7770 \\ \(\beta=0\) & 0.6967 & 0.7470 & 0.8351 & 0.8698 & 0.7449 & 0.7538 & 0.7746 \\ \(\gamma=0,\beta=0\) & 0.5364 & 0.7285 & 0.8512 & 0.8661 & 0.7490 & 0.7799 & 0.7519 \\ \hline \end{tabular}
\end{table}
Table 2: Results on Relation Extraction and NER.

## 5 Experimental Results

### Training with the Full dataset

Table 1 and 2 show the results of training with the full dataset for each task. We can observe that the results with our InfoPrompt are generally higher than those of the other parameters-efficient baselines that freeze the pretrained RoBERTa-Large parameters (_e.g._, WARP and Adapter). Finetuning generally has better performance than the other approaches. This is because it allows training with all the model parameters, which is at the expense of more computation cost during training. As mentioned above Finetuning is intended to be included as the upper bound for performance. Moreover, we can find that the performance with \(\gamma=0\) and \(\beta=0\) is lower than that of InfoPrompt, indicating shows that it is beneficial to learn task-relevant prompt tokens with the proposed head loss and representation loss. Further, the performance gap between \(\gamma=0\beta=0\) and \(\beta=\gamma=0\) shows that the proposed functions are effective when added to naive prompt tuning, _i.e._, with only \(\mathcal{L}_{pred}\).

### Training with the Few-Shot Datasets

The results for training with few-shot datasets are listed in Table 3 and 4. Compared with training with the full dataset (Table 1 and 2), we can find that the performance gap between our proposed InfoPrompt and the baselines is generally larger in the few-shot setting. Unlike the full datasets, the few-shot datasets contain much less information regarding the task to be learnt. As the result, the prompts learnt with solely the task loss (_e.g._, with WARP or \(\beta=\gamma=0\)) may easily overfit to the task-irrelevant information given the few-shot datasets. In such a scenario, it would be important to explicitly encourage the learnt prompt to be task-relevant, _i.e._, via our proposed loss functions based on mutual information maximization. This explains why InfoPrompt yields larger performance gain when trained with few-shot datasets. Similar to training with the full datasets, the performance gains of InfoPrompt compared with InfoPrompt (\(\gamma=0\)) and InfoPrompt (\(\beta=0\)) show the effectiveness of our proposed loss functions in the few-shot scenario.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c} \hline  & \multicolumn{2}{c|}{RE} & \multicolumn{2}{c|}{NER} & \multicolumn{2}{c|}{SemEval} & \\  & \(N=64\) & \(N=256\) & \(N=64\) & \(N=256\) & \(N=64\) & \(N=256\) & Average \\ \hline Finetuning & 0.1285 & 0.4013 & 0.3033 & 0.4358 & 0.2223 & 0.4829 & 0.3290 \\ Adapter [45] & 0.1086 & 0.1815 & 0.2345 & 0.2437 & 0.1211 & 0.177 & 0.1777 \\ \hline WARP [40] & 0.1404 & 0.2556 & 0.3082 & 0.4369 & 0.1708 & 0.3684 & 0.2801 \\ IDPG [102] & 0.2596 & 0.2503 & 0.3334 & 0.4048 & 0.1984 & 0.3577 & 0.3007 \\ InfoPrompt & 0.2119 & 0.2993 & 0.3331 & 0.4739 & 0.2113 & 0.4034 & 0.3222 \\ \(\gamma=0\) & 0.2026 & 0.2834 & 0.3225 & 0.4776 & 0.2153 & 0.3739 & 0.3126 \\ \(\beta=0\) & 0.2013 & 0.2874 & 0.3208 & 0.4615 & 0.2072 & 0.3629 & 0.3069 \\ \(\gamma=0,\beta=0\) & 0.1974 & 0.2728 & 0.3142 & 0.4662 & 0.2278 & 0.3276 & 0.3010 \\ \hline \end{tabular}
\end{table}
Table 4: Few-shot results on Relation Extraction and NER. We experiment with \(N=64\) and \(N=256\) samples for each task. The number of prompt is fixed to \(n_{p}=4\) for all soft prompt tuning methods.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c} \hline  & \multicolumn{2}{c|}{CoLA} & \multicolumn{2}{c|}{RTE} & \multicolumn{2}{c|}{MRPC} & \multicolumn{2}{c|}{SST2} & \\  & \(N=64\) & \(N=256\) & \(N=64\) & \(N=256\) & \(N=64\) & \(N=256\) & \(N=64\) & \(N=256\) & Average \\ \hline Finetuning & 0.1746 & 0.4086 & 0.4840 & 0.5687 & 0.7107 & 0.7819 & 0.8027 & 0.8853 & 0.6153 \\ Adapter [45] & 0.0627 & 0.2486 & 0.5487 & 0.5668 & 0.5931 & 0.6250 & 0.4908 & 0.664 & 0.4750 \\ \hline WARP [40] & 0.0749 & 0.0785 & 0.5596 & 0.5812 & 0.7083 & 0.7083 & 0.5872 & 0.7638 & 0.5077 \\ IDPG [102] & 0.0902 & 0.1513 & 0.5018 & 0.5523 & 0.6593 & 0.7010 & 0.5424 & 0.8188 & 0.5021 \\ InfoPrompt & 0.1567 & 0.1750 & 0.6137 & 0.6580 & 0.7059 & 0.7377 & 0.6697 & 0.7305 & 0.5559 \\ \(\gamma=0\) & 0.1479 & 0.1447 & 0.5776 & 0.6318 & 0.6936 & 0.7328 & 0.664 & 0.7294 & 0.5402 \\ \(\beta=0\) & 0.1372 & 0.1433 & 0.5812 & 0.5957 & 0.6838 & 0.7132 & 0.5631 & 0.656 & 0.5092 \\ \(\gamma=0,\beta=0\) & 0.0919 & 0.1397 & 0.5668 & 0.5523 & 0.6985 & 0.7108 & 0.5505 & 0.6296 & 0.4925 \\ \hline \end{tabular}
\end{table}
Table 3: Few-shot results on Sequence Classification. We experiment with \(N=64\) and \(N=256\) samples for each task. The number of prompt is fixed to \(n_{p}=4\) for all soft prompt tuning methods.

## 6 Analysis

### Loss Landscapes

To provide a more comprehensive understanding of the effectiveness of our proposed loss functions, we plot the landscapes of the loss functions in the parameter space of the prompt tokens. The landscapes illustrate how the values of the loss functions vary with the input prompt tokens. Since the prompt tokens are high-dimensional vectors, _i.e.,_ each token has the dimension of 1024 for RoBERTa-Large, we visualize their associated loss values via projecting the prompt tokens into a 2D subspace. Specifically, we follow previous work on token embedding analysis [12] that projects the prompt tokens into the top-2 principal components computed from the pretrained token embeddings of RoBERTa-Large. We only insert one prompt token into the input sequence during visualization.

Taking the task of MRPC as an example, we plot the 2D landscapes of the task loss and the representation loss in Figure 1(a) and 1(b), respectively. Both figures are plotted with the same scale, _i.e.,_ with the same values of the prompt token. The axis values are the offset from the mean of the pretrained RoBERTa-Large token embeddings. The loss values shown in the figures are the average of 20 random samples from MRPC. In Figure 1(a), we can find that there are a lot of local minimum in the landscapes of the task loss. This is consistent with the observations of the previous works [37; 91] that prompt tuning is difficult to be optimized with and sensitive to initialization, _e.g.,_ the optimization can get easily overfit to a local minimum without proper initialization. From Figure 1(b), we can observe that the loss landscape of our proposed representation loss is much smoother compared to the task loss in Figure 1(a). With smoother landscapes, the optimization with our proposed loss functions can be more stable (also shown in Section 6.2), _i.e.,_ less likely to be trapped in a local minimum and also guaranteed to converge according to our theoretical results (see Theorem 3.1). Additionally, we plot the trajectory of the first 500 steps during training for InfoPrompt (\(\gamma=0.05\)) (green) and \(\gamma=0\) (purple) in Figure 1(a) and 1(b). The stars in the plot indicate the initial value of the prompt before training. We find that training with \(\gamma=0.05\) can render a larger descent for both the task loss and representation loss, compared to \(\gamma=0\). As analyzed in Section 3.1, the language head is easier to learn than the prompt. As the result, parameters of the language head may contain more task-relevant information during the earlier stage of training. By maximizing the mutual information between the head parameter and prompt via the proposed head loss (weighted by \(\gamma\)), we encourage the learnt prompt to capture more task-relevant information in the initial training steps, thus resulting \(\gamma=0.05\) to have a larger descent than \(\gamma=0\) in the trajectories shown in Figure 1(a) and 1(b). We also compare our initialization to some common initialization approaches: Random Uniform and Sampled Vocabulary [60; 37]. By Random Uniform, we randomly sample prompt initialization from the continuous latent space. By Sampled Vocabulary, we randomly sample prompt initialization from language model's vocabulary set. The final results by WARP (Random Uniform), WARP (Sampled Vocabulary) and InfoPrompt are \(0.626\), \(0.672\) and \(0.706\) respectively. The results

Figure 2: The landscapes of the loss functions in the parameter space of the prompt tokens. The landscapes illustrates how the values of loss functions varies with the input prompt tokens. The trajectory shows the first 500 steps during training for InfoPrompt with \(\gamma=0.05\) or \(\gamma=0\).

validate the effectiveness of our initialization approach. Note that our proposed two loss functions are unsupervised and do not require additional labels.

### Learning Curve

We plot the training curve for the task of NER and SST-2 in Figure 2(a) and 2(b), respectively. Unlike WARP [40] and Finetuning that train with solely the task loss \(L_{\mathrm{pred}}\), our InfoPrompt also trains with the representation loss and head loss. We can observe that the training of our our InfoPrompt is more stabilized and converges faster, compared with WARP. This can be explained from the landscape plots in Section 6.1. Since the landscape of the task loss is not smooth (Figure 1(a)), the training curve of WARP may exhibit significant perturbation when the optimization overfits to a local minimum, _e.g._, the 10000th step in Figure 2(a). Comparably, our proposed InfoPrompt can smooth the optimization landscape, thus stabilizing the training and result in faster convergence, which is guaranteed by our theoretical results. We observe that Finetuning generally converges faster and ends up with a higher accuracy than InfoPrompt. This is because Finetuning, which trains with all the model parameters, has much larger model capacity during training than prompt tuning (InfoPrompt and WARP). Such results for Finetuning is at the expense of larger computation costs, _i.e._, we need to calculate the gradient for all the model parameters (354M) instead of only the prompt parameters \(P\) (1.3M).

We also validate that our approach is less sensitive to initialization in the early learning stage, compared to WARP. Specifically, across 10 different random seeds, we report the standard errors of the performances by InfoPrompt and WARP in the early learning stage. After the first epoch, on NER, the results by WARP and InfoPrompt are \(0.461\pm 0.038\) and \(0.810\pm 0.025\), respectively. On SST2, the results by WARP and InfoPrompt are \(0.735\pm 0.033\) and \(0.764\pm 0.027\), respectively. The results show that our method has lower standard errors and is less sensitive compared to WARP.

## 7 Related Work

### Soft Prompt Tuning

Soft prompt tuning has become a new paradigm in NLP. Based on some large pretrained models (e.g., BERT [25], RoBERTa [67]), a relatively small number of trainable parameters can be added to the input, while the parameters of backbones are fixed. Many works have demonstrated the effectiveness of soft prompt tuning in a wide range of NLP downstream tasks [60; 40; 73; 65], especially in low-resource regions [78; 66; 37]. Some recent works also found the transferable power of soft prompts across domains [101; 91; 94], across language models [91] and for zero-shot generalization [105]. To further enhance the efficiency of soft prompt parameters and enable better generalization abilities, many works consider multi-task learning [6; 27; 94; 43], or multilingual [14; 50]. Some works also try to explore the prompt with prior knowledge encoded [48; 42; 13]. While most of the initial attempts of soft prompt tuning are not context-aware, some recent works suggest that

Figure 3: The learning curves for the task of NER and SST-2. The training of our our InfoPrompt is more stabilized and converges faster, compared with WARP.

the soft prompt tokens should be conditioned on the input context. Hyperprompt [43] proposes a hyper-network structure to generate prompt tokens based on task indexes. [102] and [8] suggest some context-aware soft prompt generation methods. [64] proposes a structured soft prompt tuning method. BBT [92] targets the scenarios where the pre-trained model is not available locally (i.e., deployed online) and its back-propagation operation is not available.

### Information-theoretic Methods in NLP

Information-theoretic methods are widely used in many NLP tasks [55; 99; 90; 51; 70]. [99] and [55] propose information-theoretic methods for text memorization. [70] suggests an information-theoretic method for dialogue. [51] views the multimodal NMT problem in an information-theoretic point of view. For model pretraining, [96] proposes Infobert to improve the robustness of the BERT model. INFOXLM [15] proposes a cross-lingual language model based on an information-theoretic framework. For fine-tuning, [69] proposes an information bottleneck model method for low-resource fine-tuning. [89] introduces an information-theoretic method to engineer discrete prompts.

### Theoretical Attention Computation

Softmax is one of the major unit in the attention scheme of most recent NLP large language models. Computing the attention matrix faster is a practically interesting question [17; 97; 57]. Recently, a number of theoretical work have tried to study the softmax/attention unit from theoretical perspective. The softmax attention computation can be formally defined as follows: suppose we are given three matrices \(Q\in\mathbb{R}^{n\times k}\) (the query), \(K\in\mathbb{R}^{n\times k}\) (the key), and \(V\in\mathbb{R}^{n\times k}\) (the value), the goal is to compute \(\mathsf{Att}(Q,K,V)=D^{-1}\exp(QK^{\top})V\) where the diagonal matrix \(D\) is \(\mathrm{diag}(\exp(QK^{\top})\mathbf{1}_{n})\). Here \(K^{\top}\) denote the transpose of matrix \(K\). The work of [106; 3] consider the static setting, and the work of [11] considers the dynamic setting. [3] proposed a tight algorithm for computing Att and provided a lower bound result based on the strong exponential time hypothesis. [4] provide the results for a more general tensor version of attention which capture the three tuples feature, but classical attention cannot [77]. The work [11] shows a tight positive result and a negative result. In [11], they provide an upper bound via lazy update techniques [18]. In [11], they also present a lower bound result which is based on the Hinted MV conjecture [10]. The work of [21] proposes two sparsification algorithm to compute attention matrix when the feature dimension \(\gg\) the length of sentence. [35] shows how to provide a differentially private algorithm for computing attention matrix under differential privacy framework [30; 29]. [41] introduces a hyperattention method and presents an nearly linear time algorithm with provable guarantees. [56] studies the polynomial based attention scheme and shows that sketching techniques can help speeding up the attention computation.

## 8 Conclusion and Future Work

We revisit the limitations of soft prompt tuning in the initialization. We also empirically discover that conventional prompt tuning methods cannot learn sufficient task-relevant information from prompt tokens. We tackle these limitations from an information-theoretic perspective and propose an information-theoretic prompt tuning method InfoPrompt, with two novel loss functions. With extensive experiments, without any prior expert knowledge, InfoPrompt can significantly accelerate the convergence of the prompt tuning and achieve more accurate and robust performances than traditional prompt tuning methods.

Existing instruction-tuned LMs (_e.g._, Llama 2 [93]) are generally not task-specific and future works may further consider to tune such models with task-specific information. To achieve this, combining soft prompt tuning and prompt engineering [104; 76], from an information-theoretical perspective, can be a promising approach. Another future direction could be to further generalize our method to generation tasks (_e.g._, sequence generation). In addition to prompt learning, it is interesting to explore how to extend our approach to other parameter-efficient fine-tuning methods (_e.g._, LoRA [47] and HyperFormer [26]).

## Acknowledgments and Disclosure of Funding

The authors would like to thank the anonymous reviewers for their insightful comments. During this research, Ricardo Henao and Rui Wang were supported by ONR N00014-18-1-2871-P00002-3.

## References

* [1] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International conference on machine learning_, pages 242-252. PMLR, 2019.
* [2] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural networks. _Advances in neural information processing systems_, 32, 2019.
* [3] Josh Alman and Zhao Song. Fast attention requires bounded entries. In _NeurIPS_. arXiv preprint arXiv:2302.13214, 2023.
* [4] Josh Alman and Zhao Song. How to capture higher-order correlations? generalizing matrix softmax attention to kronecker computation. _arXiv preprint arXiv:2310.04064_, 2023.
* [5] Josh Alman and Virginia Vassilevska Williams. A refined laser method and faster matrix multiplication. In _Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 522-539. SIAM, 2021.
* [6] Akari Asai, Mohammadreza Salehi, Matthew E. Peters, and Hannaneh Hajishirzi. Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts, 2022.
* [7] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing textual entailment challenge. In _TAC_, 2009.
* [8] Rishabh Bhardwaj, Amrita Saha, and Steven CH Hoi. Vector-quantized input-contextualized soft prompts for natural language understanding. _arXiv preprint arXiv:2205.11024_, 2022.
* [9] Jan van den Brand. A deterministic linear program solver in current matrix multiplication time. In _Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 259-278. SIAM, 2020.
* [10] Jan van den Brand, Danupon Nanongkai, and Thatchaphol Saranurak. Dynamic matrix inverse: Improved algorithms and matching conditional lower bounds. In _2019 IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 456-480. IEEE, 2019.
* [11] Jan van den Brand, Zhao Song, and Tianyi Zhou. Algorithm and hardness for dynamic attention maintenance in large language models. _arXiv preprint arXiv:2304.02207_, 2023.
* [12] Xingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth Church. Isotropy in the contextual embedding space: Clusters and manifolds. In _International Conference on Learning Representations_, 2020.
* [13] Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction. In _Proceedings of the ACM Web Conference 2022_, pages 2778-2788, 2022.
* [14] Yuxuan Chen, David Harbecke, and Leonhard Hennig. Multilingual relation classification via efficient and effective prompting. _arXiv preprint arXiv:2210.13838_, 2022.
* [15] Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, He-Yan Huang, and Ming Zhou. Infoxlm: An information-theoretic framework for cross-lingual language model pre-training. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 3576-3588, 2021.
* [16] Timothy Chu, Zhao Song, and Chiwun Yang. How to protect copyright data in optimization of large language models? _arXiv preprint arXiv:2308.12247_, 2023.

* [17] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does bert look at? an analysis of bert's attention. In _Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, pages 276-286, 2019.
* [18] Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix multiplication time. In _STOC_, 2019.
* [19] Thomas M Cover. _Elements of information theory_. John Wiley & Sons, 1999.
* [20] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regression. _arXiv preprint arXiv:2304.10411_, 2023.
* [21] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic attention sparsification algorithms for over-parameterized feature dimension. _arxiv preprint: arxiv 2304.03426_, 2023.
* [22] Yichuan Deng, Zhao Song, and Omri Weinstein. Discrepancy minimization in input-sparsity time. _arXiv preprint arXiv:2210.12468_, 2022.
* [23] Yichuan Deng, Zhao Song, and Shenghao Xie. Convergence of two-layer regression with nonlinear units. _arXiv preprint arXiv:2308.08358_, 2023.
* [24] Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the performance edge over linear attention. _arXiv preprint arXiv:2310.11685_, 2023.
* [25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
* [26] Kaize Ding, Albert Jiongqian Liang, Bryan Perozzi, Ting Chen, Ruoxi Wang, Lichan Hong, Ed H Chi, Huan Liu, and Derek Zhiyuan Cheng. Hyperformer: Learning expressive sparse feature representations via hypergraph transformer. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 2062-2066, 2023.
* [27] Kun Ding, Ying Wang, Pengzhang Liu, Qiang Yu, Haojian Zhang, Shiming Xiang, and Chunhong Pan. Prompt tuning with soft context sharing for vision-language models. _arXiv preprint arXiv:2208.13474_, 2022.
* [28] Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In _Third International Workshop on Paraphrasing (IWP2005)_, 2005.
* [29] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In _Annual International Conference on the Theory and Applications of Cryptographic Techniques_, pages 486-503. Springer, 2006.
* [30] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3_, pages 265-284. Springer, 2006.
* [31] Ankush Ganguly and Samuel WF Earp. An introduction to variational inference. _arXiv preprint arXiv:2108.13083_, 2021.
* [32] Yeqi Gao, Sridhar Mahadevan, and Zhao Song. An over-parameterized exponential regression. _arXiv preprint arXiv:2303.16504_, 2023.
* [33] Yeqi Gao, Zhao Song, Weixin Wang, and Junze Yin. A fast optimization view: Reformulating single layer attention in llvm based on tensor and svm trick, and solving it in matrix multiplication time. _arXiv preprint arXiv:2309.07418_, 2023.

* [34] Yeqi Gao, Zhao Song, and Shenghao Xie. In-context learning for attention scheme: from single softmax regression to multiple softmax regression via a tensor trick. _arXiv preprint arXiv:2307.02419_, 2023.
* [35] Yeqi Gao, Zhao Song, and Xin Yang. Differentially private attention computation. _arXiv preprint arXiv:2305.04701_, 2023.
* [36] Yeqi Gao, Zhao Song, and Junze Yin. An iterative algorithm for rescaled hyperbolic functions regression. _arXiv preprint arXiv:2305.00660_, 2023.
* [37] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. Ppt: Pre-trained prompt tuning for few-shot learning. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8410-8423, 2022.
* [38] Yuzhou Gu and Zhao Song. A faster small treewidth sdp solver. _arXiv preprint arXiv:2211.06033_, 2022.
* [39] Yuzhou Gu, Zhao Song, and Lichen Zhang. A nearly-linear time algorithm for structured support vector machines. _arXiv preprint arXiv:2307.07735_, 2023.
* [40] Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. Warp: Word-level adversarial reprogramming. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 4921-4933, 2021.
* [41] Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David P Woodruff, and Amir Zandieh. Hyperattention: Long-context attention in near-linear time. _arXiv preprint arXiv:2310.05869_, 2023.
* [42] Keqing He, Jingang Wang, Chaobo Sun, and Wei Wu. Unified knowledge prompt pre-training for customer service dialogues. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, pages 4009-4013, 2022.
* [43] Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuang Li, Zhao Chen, Donald Metzler, et al. Hyperprompt: Prompt-based task-conditioning of transformers. In _International Conference on Machine Learning_, pages 8678-8690. PMLR, 2022.
* [44] Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O Seaghdha, Sebastian Pado, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. _arXiv preprint arXiv:1911.10422_, 2019.
* [45] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In _International Conference on Machine Learning_, pages 2790-2799. PMLR, 2019.
* [46] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. _Advances in neural information processing systems_, 29, 2016.
* [47] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* [48] Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Juanzi Li, and Maosong Sun. Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification. _arXiv preprint arXiv:2108.02035_, 2021.
* [49] Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang. Solving sdp faster: A robust ipm framework and efficient implementation. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 233-244. IEEE, 2022.

* [50] Lianzhe Huang, Shuming Ma, Dongdong Zhang, Furu Wei, and Houfeng Wang. Zero-shot cross-lingual transfer of prompt-based tuning with a unified multilingual prompt. _arXiv preprint arXiv:2202.11451_, 2022.
* [51] Baijun Ji, Tong Zhang, Yicheng Zou, Bojie Hu, and Si Shen. Increasing visual awareness in multimodal neural machine translation from an information theoretic perspective. _arXiv preprint arXiv:2210.08478_, 2022.
* [52] Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster interior point method for semidefinite programming. In _2020 IEEE 61st annual symposium on foundations of computer science (FOCS)_, pages 910-918. IEEE, 2020.
* [53] Haotian Jiang, Yin Tat Lee, Zhao Song, and Lichen Zhang. Convex minimization with integer minima in \(\tilde{O}(n^{4})\) time. In _ACM-SIAM Symposium on Discrete Algorithms (SODA)_, 2024.
* [54] Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. Faster dynamic matrix inverse for faster lps. In _STOC_. arXiv preprint arXiv:2004.07470, 2021.
* [55] Jiaxin Ju, Ming Liu, Huan Yee Koh, Yuan Jin, Lan Du, and Shirui Pan. Leveraging information bottleneck for scientific document summarization. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 4091-4098, 2021.
* [56] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. Polysketchformer: Fast transformers via sketches for polynomial kernels. _arXiv preprint arXiv:2310.01655_, 2023.
* [57] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020.
* [58] Francois Le Gall. Powers of tensors and fast matrix multiplication. In _Proceedings of the 39th international symposium on symbolic and algebraic computation_, pages 296-303, 2014.
* [59] Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current matrix multiplication time. In _COLT_, 2019.
* [60] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 3045-3059, 2021.
* [61] Shuai Li, Zhao Song, Yu Xia, Tong Yu, and Tianyi Zhou. The closeness of in-context learning and weight shifting for softmax regression. _arXiv preprint arXiv:2304.13276_, 2023.
* [62] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. _arXiv preprint arXiv:2101.00190_, 2021.
* [63] Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regularized exp, cosh and sinh regression problems. _arXiv preprint arXiv:2303.15725_, 2023.
* [64] Chi-Liang Liu, Hung-yi Lee, and Wen-tau Yih. Structured prompt tuning. _arXiv preprint arXiv:2205.12309_, 2022.
* [65] Xiangyang Liu, Tianxiang Sun, Xuanjing Huang, and Xipeng Qiu. Late prompt tuning: A late prompt could be better than many prompts. _arXiv preprint arXiv:2210.11292_, 2022.
* [66] Xiaochen Liu, Yu Bai, Jiawei Li, Yinan Hu, and Yang Gao. Psp: Pre-trained soft prompts for few-shot abstractive summarization. _arXiv preprint arXiv:2204.04413_, 2022.
* [67] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _ArXiv_, abs/1907.11692, 2019.
* [68] Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency. _arXiv preprint arXiv:1809.01812_, 2018.

* [69] Rabeeh Karimi Mahabadi, Yonatan Belinkov, and James Henderson. Variational information bottleneck for effective low-resource fine-tuning. _arXiv preprint arXiv:2106.05469_, 2021.
* [70] Kory W Mathewson, Pablo Samuel Castro, Colin Cherry, George Foster, and Marc G Bellemare. Shaping the narrative arc: An information-theoretic approach to collaborative dialogue. _arXiv preprint arXiv:1901.11528_, 2019.
* [71] Thien Huu Nguyen, Kyunghyun Cho, and Ralph Grishman. Joint event extraction via recurrent neural networks. In _Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 300-309, 2016.
* [72] Vishakh Padmakumar and He He. Unsupervised extractive summarization using pointwise mutual information. In _Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume_, pages 2505-2512, 2021.
* [73] Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5203-5212, 2021.
* [74] Lianke Qin, Zhao Song, Lichen Zhang, and Danyang Zhuo. An online and unified algorithm for projection matrix vector multiplication with application to empirical risk minimization. In _AISTATS_, 2023.
* [75] Yujia Qin, Xiaozhi Wang, YuSheng Su, Yankai Lin, Ning Ding, Zhiyuan Liu, Juanzi Li, Lei Hou, Peng Li, Maosong Sun, and Jie Zhou. Exploring low-dimensional intrinsic task subspace via prompt tuning. _CoRR_, abs/2110.07867, 2021.
* [76] Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, Jimmy Ba, and Amjad Almahairi. Residual prompt tuning: Improving prompt tuning with residual reparameterization. _arXiv preprint arXiv:2305.03937_, 2023.
* [77] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of transformers. _arXiv preprint arXiv:2306.02896_, 2023.
* [78] Nathan Schucher, Siva Reddy, and Harm de Vries. The power of prompt tuning for low-resource semantic parsing. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 148-156, 2022.
* [79] Claude Elwood Shannon. A mathematical theory of communication. _The Bell system technical journal_, 27(3):379-423, 1948.
* [80] Ritwik Sinha, Zhao Song, and Tianyi Zhou. A mathematical abstraction for balancing the trade-off between creativity and reality in large language models. _arXiv preprint_, 2023.
* [81] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 conference on empirical methods in natural language processing_, pages 1631-1642, 2013.
* [82] Zhao Song, Weixin Wang, and Junze Yin. A unified scheme of resnet and softmax. _arXiv preprint arXiv:2309.13482_, 2023.
* [83] Zhao Song, Yitan Wang, Zheng Yu, and Lichen Zhang. Sketching for first order method: efficient algorithm for low-bandwidth channel and vulnerability. In _International Conference on Machine Learning (ICML)_, pages 32365-32417. PMLR, 2023.
* [84] Zhao Song, Shuo Yang, and Ruizhe Zhang. Does preprocessing help training over-parameterized neural networks? _Advances in Neural Information Processing Systems_, 34:22890-22904, 2021.
* [85] Zhao Song, Xin Yang, Yuanyuan Yang, and Tianyi Zhou. Faster algorithm for structured john ellipsoid computation. _arXiv preprint arXiv:2211.14407_, 2022.

* [86] Zhao Song, Mingquan Ye, and Lichen Zhang. Streaming semidefinite programs: \(O(\sqrt{n})\) passes, small space and fast runtime. _arXiv preprint arXiv:2309.05135_, 2023.
* [87] Zhao Song and Zheng Yu. Oblivious sketching-based central path method for solving linear programming problems. In _38th International Conference on Machine Learning (ICML)_, 2021.
* [88] Zhao Song, Lichen Zhang, and Ruizhe Zhang. Training multi-layer over-parametrized neural network in subquadratic time. _arXiv preprint arXiv:2112.07628_, 2021.
* [89] Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. An information-theoretic approach to prompt engineering without ground truth labels. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 819-862, 2022.
* [90] Victor Steinborn, Philipp Dufter, Haris Jabbar, and Hinrich Schutze. An information-theoretic approach and dataset for probing gender stereotypes in multilingual masked language models. In _Findings of the Association for Computational Linguistics: NAACL 2022_, pages 921-932, 2022.
* [91] Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al. On transferability of prompt tuning for natural language processing. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 3949-3969, 2022.
* [92] Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning for language-model-as-a-service. In _International Conference on Machine Learning_, pages 20841-20855. PMLR, 2022.
* [93] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [94] Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. Spot: Better frozen model adaptation through soft prompt transfer. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5039-5059, 2022.
* [95] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_, 2018.
* [96] Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. Infobert: Improving robustness of language models from an information theoretic perspective. _arXiv preprint arXiv:2010.02329_, 2020.
* [97] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.
* [98] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. _Transactions of the Association for Computational Linguistics_, 7:625-641, 2019.
* [99] Peter West, Ari Holtzman, Jan Buys, and Yejin Choi. Bottlesum: Unsupervised and self-supervised sentence summarization using the information bottleneck principle, 2019.
* [100] Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd. In _Proceedings of the forty-fourth annual ACM symposium on Theory of computing_, pages 887-898, 2012.
* [101] Hui Wu and Xiaodong Shi. Adversarial soft prompt tuning for cross-domain sentiment analysis. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2438-2447, 2022.

* [102] Zhuofeng Wu, Sinong Wang, Jiatao Gu, Rui Hou, Yuxiao Dong, VG Vydiswaran, and Hao Ma. Idpg: An instance-dependent prompt generation method. _arXiv preprint arXiv:2204.04497_, 2022.
* [103] Bishan Yang and Tom Mitchell. Joint extraction of events and entities within a document context. _arXiv preprint arXiv:1609.03632_, 2016.
* [104] Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong Yang, Mingfeng Xue, Boxing Chen, and Jun Xie. Tailor: A soft-prompt-based approach to attribute-based controlled text generation. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 410-427, 2023.
* [105] Seonghyeon Ye, Joel Jang, Doyoung Kim, Yongrae Jo, and Minjoon Seo. Retrieval of soft prompt enhances zero-shot task generalization. _arXiv preprint arXiv:2210.03029_, 2022.
* [106] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating transformers via kernel density estimation. In _ICML_. arXiv preprint arXiv:2302.02451, 2023.
* [107] Songming Zhang, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jian Liu, and Jie Zhou. Conditional bilingual mutual information based adaptive training for neural machine translation. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2377-2389, 2022.
* [108] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. In _International conference on machine learning (ICML)_, pages 4140-4149. PMLR, 2017.

## Appendix

**Roadmap.** In Section A, we provide a number of basic notations. In Section B, we provide several basic definitions and discuss some related work about previous theoretical softmax regression results. In Section C, we provide a complete proof for our major theoretical result in this paper. We present our final result in Section D.

## Appendix A Preliminaries

For any positive integer \(n\), we use \([n]\) to denote set \(\{1,2,\cdots,n\}\). For any function \(f\), we use \(\widetilde{O}(g)\) to denote \(g\cdot\mathrm{poly}(\log g)\).

VectorFor a length-\(n\) vector \(z\), we use \(\exp(z)\) to denote a length-\(n\) vector that its \(i\)-th entry is \(\exp(z_{i})\).

For a length-\(n\) vector \(z\), we use \(\|z\|_{2}\) to represent its \(\ell_{2}\) norm, i.e., \(\|z\|_{2}:=(\sum_{i=1}^{n}x_{i}^{2})^{1/2}\). For a length-\(n\) vector \(z\), we use \(\|z\|_{\infty}\) to denote \(\max_{i\in[n]}|z_{i}|\).

For a length-\(n\) vector \(z\in\mathbb{R}^{n}\), we use \(\mathrm{diag}(z)\) to generate a diagonal matrix where each entry on the \((i,i)\)-diagonal is \(z_{i}\) for every \(i\in[n]\).

We use \(\mathbf{1}_{n}\) to represent a length-\(n\) vector where all the coordinates are ones. Similarly, we use \(\mathbf{0}_{n}\) to represent a length-\(n\) vector where all the values are zeros.

PsdWe say \(W\succeq Z\) (positive semi-definite) if \(x^{\top}Wx\geq x^{\top}Zx\) for all vector \(x\).

Matrix RelatedFor an \(n\) by \(d\) matrix \(C\), we use \(\mathrm{nnz}(C)\) to denote the number of non-zero entries of \(C\), i.e., \(\mathrm{nnz}(C):=|\{(i,j)\in[n]\times[d]\mid C_{i,j}\neq 0\}|\)

For a diagonal matrix \(D\in\mathbb{R}^{n\times n}\), we say \(D\) is a \(k\)-sparse diagonal matrix, i.e., \(k=|\{i\in[n]\mid D_{i,i}\neq 0\}|\).

For any matrix \(Z\in\mathbb{R}^{n\times k}\), we denote the spectral norm of \(Z\) by \(\|Z\|\), i.e.,

\[\|Z\|:=\sup_{x\in\mathbb{R}^{k}}\frac{\|Zx\|_{2}}{\|x\|_{2}}.\]

For a matrix \(Q\), we use \(\sigma_{\max}(Q)\) to denote the largest singular value of \(Q\). We use \(\sigma_{\min}(Q)\) to denote the smallest singular value of \(Q\).

Matrix ComputationWe use \(\omega\) to denote the exponent of matrix multiplication, i.e., \(n^{\omega}\) denotes the time of multiplying an \(n\times n\) matrix with another \(n\times n\) matrix. Currently \(\omega\approx 2.373\)[100, 58, 5].

Calculus RelatedWe use \(\circ\) notation by following the literature's [63, 20, 36, 61, 80]. Suppose that we're given two column vectors \(x,y\in\mathbb{R}^{n}\), we use \(x\circ y\) to denote a column vector that \((x\circ y)_{i}\) is \(x_{i}y_{i}\).

## Appendix B Related Work about Theoretical Attention Regression Results

In this paper, we focus on the direction of regression tasks [32, 63, 20, 61, 80, 34, 23, 16, 33, 82, 24]. The goal of this section will review the linear regression (Definition B.1), exponential regression (Definition B.2), rescaled softmax regression (Definition B.3), softmax regression (Definition B.2).

**Definition B.1** (Linear regression).: _Given a matrix \(A\in\mathbb{R}^{n\times d}\) and \(b\in\mathbb{R}^{n}\), the goal is to solve_

\[\min_{x\in\mathbb{R}^{d}}\|Ax-b\|_{2}.\]

_For convenient, let us \(u(x)\) to denote \(\exp(Ax)\)._

**Definition B.2** (Exponential Regression, see [32, 63]).: _Suppose we are given a length \(n\) vector \(b\), and an \(n\)\(by\)\(d\) size matrix \(A\), our goal is to optimize_

\[\min_{x\in\mathbb{R}^{d}}\|u(x)-b\|_{2}.\]

**Definition B.3** (Rescaled Softmax Regression, see [36]).: _Suppose we are given a length \(n\) vector \(b\), and an \(n\)\(by\)\(d\) size matrix \(A\), our goal is to optimize_

\[\min_{x\in\mathbb{R}^{d}}\|u(x)-\langle u(x),\mathbf{1}_{n}\rangle\cdot b\|_{2}\]

**Definition B.4** (Softmax Regression, see [20, 61, 80]).: _Suppose we are given a length \(n\) vector \(b\), and an \(n\)\(by\)\(d\) size matrix \(A\), our goal is to optimize_

\[\min_{x\in\mathbb{R}^{d}}\|\langle u(x),\mathbf{1}_{n}\rangle^{-1}\cdot u(x)- b\|_{2}.\]

## Appendix C Theoretical Guarantees

In Section C.1, we provide several basic definitions. In Section C.2, we explain how to compute the gradient of function \(f\). In Section C.3, we show how to compute the gradient of function \(\log f(x)\). In Section C.4, we explain how to compute the Hessian of function \(\log f(x)\). In Section C.5, we compute the the Hessian of inner product between \(\log f(x)\) and \(b\). In Section C.6, we compute the Hessian of cross entropy loss function. In Section C.7, we show Hessian is positive definite. In Section C.8, we prove that Hessian is Lipschitz. We remark that our experiments are based on first order method, and our theoretical proofs are mainly focusing on second order method. We believe it's an interesting future direction to further study the convergence of the first order method such as [108, 1, 2, 88, 84, 83].

### Function Definition

We define

**Definition C.1**.: _We define \(u(x)\) as follows_

* \(u(x)=\exp(Ax)\)__

**Definition C.2**.: _We define \(v(x)\) as follows_

* \(v(x)=\exp(Ax)\)__

Previous [63] studies three types of hyperbolic functions \(\exp(\cdot)\), \(\cosh(\cdot)\) and \(\sinh(\cdot)\). We mainly focus on \(\exp(\cdot)\) function.

**Definition C.3** (Normalized coefficients, Definition 5.4 in [20]).: _We define \(\alpha:\mathbb{R}^{d}\to\mathbb{R}\) as follows_

\[\alpha(x):=\langle u(x),\mathbf{1}_{n}\rangle.\]

We define function softmax \(f\) as follows

**Definition C.4** (Function \(f\), Definition 5.1 in [20]).: _Suppose that we're given an \(n\times d\) matrix \(A\). Let \(\mathbf{1}_{n}\) denote a length-\(n\) vector that all the coordinates are ones. We define prediction function \(f:\mathbb{R}^{d}\to\mathbb{R}^{n}\) as follows_

\[f(x):=\langle u(x),\mathbf{1}_{n}\rangle^{-1}\cdot u(x).\]

**Fact C.5**.: _Let \(f(x)\) be defined as Definition C.4. Then we have_

* _Part 1._ \(\langle f(x),\mathbf{1}_{n}\rangle=1\)__
* _Part 2._ \(\|f(x)\|_{1}=1\)__
* _Part 3._ \(\|f(x)\|_{2}\leq 1\)__

Proof.: The proof is straightforward. For more details, we refer the readers to [20].

We define the \(\ell_{2}\) loss

**Definition C.6**.: _We define_

\[L_{\mathrm{exp}}:=0.5\|f(x)-b\|_{2}^{2}.\]

Previous work [80] only considers entropy, here we consider cross entropy instead.

**Definition C.7** (Cross Entropy).: _We define \(L_{\mathrm{cent}}:\mathbb{R}^{d}\rightarrow\mathbb{R}\),_

\[L_{\mathrm{cent}}(x):=-\langle b,\log f(x)\rangle\]

**Definition C.8**.: _Suppose we're given an \(n\times d\) matrix \(A\) and \(W=\mathrm{diag}(w)\in\mathbb{R}^{n\times n}\) where \(w\in\mathbb{R}^{n}\) is a vector, we define \(L_{\mathrm{reg}}:\mathbb{R}^{d}\rightarrow\mathbb{R}\)_

\[L_{\mathrm{reg}}(x):=0.5\|WAx\|_{2}^{2}\]

### Gradient Computation for Function \(f\)

We present a calculus tool from previous work [20] (for example, we refer the readers to Lemma 5.6 in [20]).

**Lemma C.9**.: _If the following conditions hold_

* _Given matrix_ \(A\in\mathbb{R}^{n\times d}\) _and a vector_ \(b\in\mathbb{R}^{n}\)_._
* _Suppose that function_ \(\alpha:\mathbb{R}^{d}\rightarrow\mathbb{R}\) _be defined in Definition C.3._
* _Suppose that function_ \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}^{n}\) _be defined in Definition C.4._

_For each \(i\in[d]\), we have_

* _Part 1._ \[\frac{\mathrm{d}f(x)}{\mathrm{d}x_{i}}= -\langle f(x),A_{*,i}\rangle\cdot f(x)+f(x)\circ A_{*,i}\]
* _Part 2._ \[\langle\frac{\mathrm{d}f(x)}{\mathrm{d}x_{i}},A_{*,i}\rangle=-\langle f(x),A_{*,i}\rangle^{2}+\langle f(x),A_{*,i}\circ A_{*,i}\rangle\]
* _Part 3._ \[\langle\frac{\mathrm{d}f(x)}{\mathrm{d}x_{i}},A_{*,j}\rangle=-\langle f(x),A_{*,i}\rangle\cdot\langle f(x),A_{*,j}\rangle+\langle f(x),A_{*,i}\circ A_{*,j}\rangle\]

### Gradient Computation for Function \(\log f(x)\)

In this section, we explain how to compute the gradient of \(\log f(x)\).

**Lemma C.10**.: _If the following condition holds_

* _Suppose that function_ \(f\) _is defined in Definition C.4._

_We have_

* _Part 1._ \[\frac{\mathrm{d}\log f(x)}{\mathrm{d}x_{i}}=-\langle f(x),A_{*,i}\rangle\cdot \mathbf{1}_{n}+A_{*,i}\]
* _Part 2._ \[\langle\frac{\mathrm{d}\log f(x)}{\mathrm{d}x_{i}},b\rangle=\langle A_{*,i},b \rangle-\langle f(x),A_{*,i}\rangle\cdot\langle b,\mathbf{1}_{n}\rangle\]_Part 3._ \[\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{\mathrm{cent}}(x)=\langle f(x),A_{*,i}\rangle \cdot\langle b,\mathbf{1}_{n}\rangle-\langle A_{*,i},b\rangle\]

Proof.: **Proof of Part 1.**

For all index \(j\in[n]\), we can compute the gradient with respect to \(x_{i}\)

\[\frac{\mathrm{d}\log f(x)_{j}}{\mathrm{d}x_{i}}=f(x)_{j}^{-1}\frac{\mathrm{d} f(x)_{j}}{\mathrm{d}x_{i}}\]

Then we group the \(n\) coordinates, we get

\[\frac{\mathrm{d}\log f(x)}{\mathrm{d}x_{i}} =f(x)^{-1}\circ\frac{\mathrm{d}f(x)}{\mathrm{d}x_{i}}\] \[=f(x)^{-1}\circ(-\langle f(x),A_{*,i}\rangle f(x)+f(x)\circ A_{*, i})\] \[=-\left\langle f(x),A_{*,i}\rangle f(x)^{-1}\circ f(x)+f(x)^{-1} \circ f(x)\circ A_{*,i}\right.\] \[=-\left\langle f(x),A_{*,i}\right\rangle\cdot\mathbf{1}_{n}+A_{ *,i}\]

**Proof of Part 2.** We have

\[\langle\frac{\mathrm{d}\log f(x)}{\mathrm{d}x_{i}},b\rangle =\langle-\langle f(x),A_{*,i}\rangle\cdot\mathbf{1}_{n}+A_{*,i},b\rangle\] \[=\langle A_{*,i},b\rangle-\langle f(x),A_{*,i}\rangle\cdot \langle b,\mathbf{1}_{n}\rangle,\]

where the first step follows from Part 1 and the second step follows from simple algebra.

**Proof of Part 3.** The proof directly follows from Part 2 and Definition of \(L_{\mathrm{cent}}(x)\) (See Definition C.7). 

### Hessian Computation for Function \(\log f(x)\)

In this section, we will show how to compute the Hessian for function \(\log f(x)\).

**Lemma C.11**.: _If the following conditions hold_

* _Let_ \(f\) _be defined as Definition_ C.4_._

_Then we have_

* _Part 1._ \[\frac{\mathrm{d}^{2}\log f(x)}{\mathrm{d}x_{i}^{2}}=(\langle f(x),A_{*,i} \rangle^{2}-\langle f(x),A_{*,i}\circ A_{*,i}\rangle)\cdot\mathbf{1}_{n}\]
* _Part 2._ \[\frac{\mathrm{d}^{2}\log f(x)}{\mathrm{d}x_{i}\mathrm{d}x_{j}}=(\langle f(x), A_{*,i}\rangle\langle f(x),A_{*,j}\rangle-\langle f(x),A_{*,i}\circ A_{*,j} \rangle)\cdot\mathbf{1}_{n}\]

Proof.: **Proof of Part 1.**

We have

\[\frac{\mathrm{d}^{2}\log f(x)}{\mathrm{d}x_{i}^{2}} =\frac{\mathrm{d}}{\mathrm{d}x_{i}}(\frac{\mathrm{d}\log f(x)}{ \mathrm{d}x_{i}})\] \[=\frac{\mathrm{d}}{\mathrm{d}x_{i}}(-\langle f(x),A_{*,i}\rangle \cdot\mathbf{1}_{n}+A_{*,i})\] \[=-\frac{\mathrm{d}}{\mathrm{d}x_{i}}(\langle f(x),A_{*,i}\rangle) \cdot\mathbf{1}_{n}\] \[=(\langle f(x),A_{*,i}\rangle^{2}-\langle f(x),A_{*,i}\circ A_{*, i}\rangle)\cdot\mathbf{1}_{n}\]

where the 2nd step comes from Part 1 of Lemma C.10, the 3rd step follows from \(A_{*,i}\) is independent of \(x\), and the forth step follows from Part 2 of Lemma C.9.

**Proof of Part 2.**

Similarly, we can provide a proof for Part 2.

### Hessian Computation for Function \(\langle\log f(x),b\rangle\)

The goal of this section is to prove Lemma C.12.

**Lemma C.12**.: _If the following conditions hold_

* _Let_ \(f\) _be defined as Definition_ C.4_._

_Then we have_

* _Part 1._ \[\langle\frac{\mathrm{d}^{2}\log f(x)}{\mathrm{d}x_{i}^{2}},b\rangle=(\langle f (x),A_{*,i}\rangle^{2}-\langle f(x),A_{*,i}\circ A_{*,i}\rangle)\cdot\langle \mathbf{1}_{n},b\rangle\]
* _Part 2._ \[\langle\frac{\mathrm{d}^{2}\log f(x)}{\mathrm{d}x_{i}\mathrm{d}x_{j}},b\rangle =(\langle f(x),A_{*,i}\rangle\langle f(x),A_{*,j}\rangle-\langle f(x),A_{*,i} \circ A_{*,j}\rangle)\cdot\langle\mathbf{1}_{n},b\rangle\]

Proof.: The proof directly follows from Lemma C.11. 

### Hessian Computation for Function \(L_{\mathrm{cent}}(x)\)

For convenient of analyzing the \(d\times d\) Hessian matrix, we will start with defining \(n\times n\) matrix \(B\).

**Definition C.13**.: _We define \(B(x)\in\mathbb{R}^{n\times n}\) as follows_

\[B(x):=\langle\mathbf{1}_{n},b\rangle\cdot(\mathrm{diag}(f(x))-f(x)f(x)^{\top})\]

**Lemma C.14**.: _If the following conditions hold_

* _Let_ \(f\) _be defined as Definition_ C.4_._
* _Let_ \(L_{\mathrm{cent}}\) _be defined as Definition_ C.7__
* _Let_ \(B\) _be defined as Definition_ C.13__

_Then we have_

* _Part 1._ \[\frac{\mathrm{d}^{2}}{\mathrm{d}x_{i}^{2}}L_{\mathrm{cent}}=(-\langle f(x),A_{ *,i}\rangle^{2}+\langle f(x),A_{*,i}\circ A_{*,i}\rangle)\cdot\langle\mathbf{1} _{n},b\rangle\]
* _Part 2._ \[\frac{\mathrm{d}^{2}}{\mathrm{d}x_{i}\mathrm{d}x_{j}}L_{\mathrm{cent}}=(- \langle f(x),A_{*,i}\rangle\langle f(x),A_{*,j}\rangle+\langle f(x),A_{*,i} \circ A_{*,j}\rangle)\cdot\langle\mathbf{1}_{n},b\rangle\]
* _Part 3._ \[\frac{\mathrm{d}^{2}}{\mathrm{d}x^{2}}L_{\mathrm{cent}}=A^{\top}B(x)A\]

Proof.: The proof trivially follows from Lemma C.12 and Definition C.13. 

### Hessian is Positive Definite

Previous work [20] doesn't consider cross entropy into the final loss function. Here we generalize previous lemma so that cross entropy is also being considered.

**Lemma C.15** (A cross entropy generalization of Lemma 6.3 in [20]).: _Suppose the following conditions hold_* _Let_ \(A\in\mathbb{R}^{n\times d}\)_,_ \(R\geq 4\)_,_ \(l>0\)_, suppose that_ \(R_{0}=\exp(O(R^{2}+\log n))\)__
* \[L(x)=\underbrace{L_{\mathrm{reg}}(x)}_{\text{Definition C.8}}+ \underbrace{L_{\mathrm{cent}}(x)}_{\text{Definition C.7}}+\underbrace{L_{ \mathrm{exp}}(x)}_{\text{Definition C.6}}.\]
* _Let_ \(\widetilde{B}(x)=B(x)+W^{2}\)__

_Then we have_

* _Part 1._ \(\min_{i\in[n]}w_{i}^{2}\geq 10R_{0}+l/\sigma_{\min}(A)^{2}\)_, then we have_ \[\frac{\mathrm{d}^{2}L}{\mathrm{d}x^{2}}\succeq l\cdot I_{d}\]
* _Part 2._ \(\min_{i\in[n]}w_{i}^{2}\geq 10^{4}\cdot R_{0}+l/\sigma_{\min}(A)^{2}\)_, then we have_ \[(1-0.01)\cdot\widetilde{B}(x)\preceq W^{2}\preceq(1-0.01)\cdot \widetilde{B}(x).\]

Proof.: Using the definition of \(B\) for \(L_{\mathrm{cent}}\)(see Definition C.13), definition/bound of \(B\) for \(L_{\mathrm{exp}}\) (see [20]), and tools developed in Section 6 in [20], we can finish the proof. 

### Hessian is Lipschitz

Previous work [20] doesn't consider cross entropy into the final loss function. Here we generalize previous lemma so that cross entropy is also being considered.

**Lemma C.16** (A cross entropy version of Lemma 7.1 in [20]).: _Suppose the following condition holds_

* _Let_ \(H(x)=\frac{\mathrm{d}^{2}L}{\mathrm{d}x^{2}}\) _and_ \(R>4\)__
* _Suppose that_ \(\max\{\|x\|_{2},\|y\|_{2}\}\leq R\)_, and_ \(\max\{\|A\|,\|b\|_{2}\}\leq R\)__
* \(\|A(x-y)\|_{\infty}<0.01\)__

_Then we have_

\[\|H(x)-H(y)\|\leq n^{4}\exp(O(R^{2}+\log n))\cdot\|x-y\|_{2}\]

Proof.: Using the definition of \(B\) for \(L_{\mathrm{cent}}\)(see Definition C.13), definition/bound of \(B\) for \(L_{\mathrm{exp}}\) (see [20]), and tools developed in Section 7 in [20], we can finish the proof. 

```
1:procedureOurAlgorithm(\(A\in\mathbb{R}^{n\times d},b\in\mathbb{R}^{n},w\in\mathbb{R}^{n},\epsilon,\delta\))\(\triangleright\)Theorem D.1
2: We choose \(x_{0}\)
3:\(T\leftarrow\log(\|x_{0}-x^{*}\|_{2}/\epsilon)\)\(\triangleright\)\(T\) denotes the number of iterations
4:for\(t=0\to T\)do
5: Implicitly formulate exact Hessian and use that to construct an approximate Hessian \(\widetilde{H}\) (similar as Section 8 in [20])
6: Compute gradient
7:\(\widetilde{H}\gets A^{\top}\widetilde{D}A\)
8:\(x_{t+1}\gets x_{t}+\widetilde{H}^{-1}g\)
9:endfor
10:\(\widetilde{x}\gets x_{T+1}\)
11:return\(\widetilde{x}\)
12:endprocedure ```

**Algorithm 1** Our Algorithm.

Main Theoretical Guarantees

Previous work [20] has proved the similar result without considering the cross entropy. We generalize the techniques in previous paper [20] from only considering \(\ell_{2}\) task loss to considering both \(\ell_{2}\) task loss and cross entropy loss (\(L_{\mathrm{cent}}\) see formal definition in Definition C.7). Our algorithm is a version of approximate Newton method, such methods have been widely used in many optimization tasks [18, 59, 9, 52, 87, 54, 38, 85, 49, 74, 20, 39, 53, 86]. In this work, we focus on the approximate Newton method along the line of [85, 22, 20].

**Theorem D.1** (Formal version of Theorem 3.1).: _Let \(x^{*}\) denote an length-\(d\) vector that is satisfying,_

\[\arg\min_{x\in\mathbb{R}^{d}}\underbrace{L_{\mathrm{exp}}}_{\mathrm{Definition \ C.6}}+\underbrace{L_{\mathrm{cent}}}_{\mathrm{Definition\ C.7}}+ \underbrace{L_{\mathrm{reg}}}_{\mathrm{Definition\ C.8}}\]

_Suppose the following conditions are holding:_

* \(R\geq 4\)_,_ \(g(x^{*})=\mathbf{0}_{d}\)_._
* \(\|x^{*}\|_{2}\leq R\)_,_ \(\|A\|\leq R\)_,_ \(\|b\|_{2}\leq R\)_._
* \(M=\exp(O(R^{2}+\log n))\)_._
* \(\min_{i\in[n]}w_{i}^{2}\geq 100M+l/\sigma_{\min}(A)^{2}\)__
* _Suppose that_ \(\epsilon\in(0,0.1)\) _is the final and_ \(\delta\in(0,0.1)\) _is the failure probability._
* _Suppose_ \(x_{0}\) _satisfy condition_ \(M\|x_{0}-x^{*}\|_{2}\leq 0.1l\)_._
* _Suppose that_ \(T=\log(\|x_{0}-x^{*}\|_{2}/\epsilon)\)__

_Then there is a randomized algorithm (Algorithm 1) such that_

* _it runs_ \(T\) _iterations_
* _in each iteration, it spends time_+__ \[O((\mathrm{nnz}(A)+d^{\omega})\cdot\mathrm{poly}(\log(n/\delta)).\]

Footnote †: Here \(\omega\) denotes the exponent of matrix multiplication. Currently \(\omega\approx 2.373\).

\[O((\mathrm{nnz}(A)+d^{\omega})\cdot\mathrm{poly}(\log(n/\delta)).\]

* _generates a vector_ \(\widetilde{x}\in\mathbb{R}^{d}\) _that is satisfying_ \[\|\widetilde{x}-x^{*}\|_{2}\leq\epsilon\]
* _the succeed probability is_ \(1-\delta\)__

Proof.: The high level framework of our theorem is similar to previous work about exponential regression [63], softmax regression [20] and rescaled softmax regression [36]. Similarly as previous work [63, 20, 36, 80], we use the approximate newton algorithm (for example see Section 8 in [20]). So in the proof, we only focus on the difference about the Hessian positive definite lower bound and Hessian Lispchitz property.

Using Lemma C.15 and Lemma C.16 and approximate Newton algorithm analysis in [20], then we complete the proof. 

## Appendix E Comparison with Parameter-Efficient Fine-tuning Methods Not Based on Prompt Tuning

In this section, we focus on the comparison between InfoPrompt (\(N_{p}=4\)) and some PEFT (Parameter-Efficient Fine-tuning Methods) baselines which are not based on prompt tuning:

* Adapter [45]: Similar to prompt tuning, this is also a way of parameter-efficient training for pretrained language models. Specifically, instead of adding the prompt tokens in the input, we add adapters after the feed-forward module in each transformer layer.

* LoRA [47]: Another parameter-efficient training method for pretrained language models. Specifically, LoRA adds additional low-rank decomposed matrices into each Transformer layer via residual connections.

In Table 5, we can observe that LoRA is a stronger baseline than Adapter. However, our method can still outperform LoRA, especially in the downstream tasks which require more task-relevant information, _e.g._, NER and RE.

\begin{table}
\begin{tabular}{c|c c c c c c c c} \hline Full datasets & CoLA & RTE & MRPC & SST2 & RE & NER & SemEval & Average \\ \hline LoRA & 0.5880 & 0.6715 & 0.8235 & 0.9541 & 0.6636 & 0.8228 & 0.7214 & 0.7492 \\ Adapter & 0.5552 & 0.5776 & 0.6814 & 0.9472 & 0.5073 & 0.8329 & 0.6570 & 0.6798 \\ InfoPrompt & 0.6018 & 0.6968 & 0.8137 & 0.9599 & 0.7616 & 0.8962 & 0.7917 & 0.7888 \\ \hline \(N=64\) & CoLA & RTE & MRPC & SST2 & RE & NER & SemEval & Average \\ \hline LoRA & 0.0991 & 0.5596 & 0.6985 & 0.5677 & 0.1232 & 0.1345 & 0.1711 & 0.3362 \\ Adapter & 0.0627 & 0.5487 & 0.5931 & 0.4908 & 0.1086 & 0.2345 & 0.1211 & 0.3085 \\ InfoPrompt & 0.1567 & 0.6137 & 0.7059 & 0.6697 & 0.2119 & 0.3331 & 0.2113 & 0.4146 \\ \hline \(N=256\) & CoLA & RTE & MRPC & SST2 & RE & NER & SemEval & Average \\ \hline LoRA & 0.2854 & 0.5740 & 0.7206 & 0.8222 & 0.2291 & 0.1955 & 0.3817 & 0.4583 \\ Adapter & 0.2486 & 0.5668 & 0.6250 & 0.6640 & 0.1815 & 0.2437 & 0.1770 & 0.3866 \\ InfoPrompt & 0.1750 & 0.6580 & 0.7377 & 0.7305 & 0.2993 & 0.4739 & 0.4034 & 0.4968 \\ \hline \end{tabular}
\end{table}
Table 5: Comparison with parameter-efficient fine-tuning methods which are not based on prompt tuning. The number of prompt is fixed to \(n_{p}=4\) for the soft prompt tuning method.