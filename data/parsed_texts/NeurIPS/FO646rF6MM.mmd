# Variational Classification

Shehzaad Dhuliawala

Department of Computer Science, ETH Zurich

{shehzaad.dhuliawala, mrinmaya.sachan}@inf.ethz.ch

&Mrinmaya Sachan

AI Centre, ETH Zurich

carl.allen@ai.ethz.ch

&Carl Allen

AI Centre, ETH Zurich

###### Abstract

We present _variational classification_ (VC), a latent variable generalisation of neural network softmax classification under cross-entropy loss. Our approach provides a novel probabilistic interpretation of the highly familiar softmax classification model, to which it relates comparably to variational vs deterministic autoencoders. We derive a training objective based on the evidence lower bound (ELBO) that is non-trivial to optimize, and an adversarial approach to maximise it. We reveal an inherent inconsistency within softmax classification that VC addresses, while also allowing flexible choices of distributions in the latent space in place of assumptions implicit in standard softmax classifiers. Empirical evaluation demonstrates that VC maintains accuracy while improving properties such as calibration and adversarial robustness, particularly under distribution shift and low data settings. By explicitly considering representations learned by supervised methods, we offer the prospect of the principled merging of supervised learning with other representation learning methods, e.g. contrastive learning, using a common encoder architecture.

## 1 Introduction

Classification is a core task in machine learning, from categorising objects (Klasson et al., 2019) and providing medical diagnoses (Adem et al., 2019; Mirbabaie et al., 2021), to identifying potentially life-supporting planets (Tiensuu et al., 2019). Classification tasks are commonly tackled by training domain-specific neural networks with a _sigmoid_ or _softmax_ output layer.1 Data samples \(x\) (in a domain \(\mathcal{X}\)) are mapped deterministically by a network \(f_{\omega}\) (with weights \(\omega\)) to a real vector \(z\!=\!f_{\omega}(x)\), which is transformed in the softmax layer to a point on the simplex \(\Delta^{|\mathcal{Y}|}\), that parameterises \(p_{\theta}(y|x)\), a discrete distribution over class labels \(y\!\in\!\mathcal{Y}\):

Footnote 1: We refer throughout to the softmax function since it generalises sigmoid to multiple classes.

\[p_{\theta}(y|x)=\frac{\exp\{z^{\top}w_{y}+b_{y}\}}{\sum_{y^{\prime}\in\mathcal{ Y}}\exp\{z^{\top}w_{y^{\prime}}+b_{y^{\prime}}\}}\.\] (1)

Although softmax classifiers often perform well, they suffer well-known issues: (i) they are are poorly understood theoretically and in many respects a "black box" with predictions \(p_{\theta}(y|x)\) hard to explain; (ii) predictions can vary significantly for imperceptible changes in the data (adversarial examples); (iii) predictions may identify a true label as the most probable class but poorly reflect uncertainty in the prediction (miscalibration); and (iv) they typically require a lot of data to train.

We introduce _Variational Classification_ (**VC**), which generalises softmax cross-entropy classification under a latent variable model (figure 2). The VC framework ascribes probabilistic roles to components of a softmax classifier: (i) the neural network (excluding the softmax layer) transforms a _mixture of unknown distributions_ in the data space to a _mixture of chosen distributions_ in the latent space; and (ii) the softmax layer converts the latter to class predictions by Bayes' rule. We show that, without tailoring the loss function to mitigate any particular issue with softmax classification, VCmaintains predictive accuracy while the additional latent structure improves calibration, robustness to adversarial perturbations and domain shift, and performance in low data regimes.

## 2 Background (Variational Auto-Encoder)

Estimatiing parameters of a latent variable model \(p_{\theta}(x)=\int_{z}p_{\theta}(x|\mathsf{z})p_{\theta}(\mathsf{z})\) is typically intractable, and instead one maximises the _evidence lower bound_ (ELBO):

\[\int_{x}p(x)\log p_{\theta}(x)\geq\int_{x}p(x)\int_{z}q_{\phi}(z|x)\Big{\{}\log p _{\theta}(x|z)-\log\tfrac{q_{\phi}(z|x)}{p_{\theta}(z)}\Big{\}},\] (2)

The _variational auto-encoder_(VAE, Kingma & Welling, 2014; Rezende et al., 2014) is an implementation of the ELBO in which all distributions are assumed Gaussian, with \(p_{\theta}(x|z)\), \(q_{\phi}(z|x)\) parameterised by neural networks. The VAE probabilistically generalises a deterministic auto-encoder, allowing for uncertainty or stochasticity in the latent \(\mathsf{z}|x\), whose entropy is promoted and is constrained to a prior by the second ("regularisation") term.

## 3 Variational Classification

**A Latent Variable Model for Classification**: Data \(x\in\mathcal{X}\) and labels \(y\in\mathcal{Y}\) are treated as samples of random variables \(\mathsf{x}\), \(\mathsf{y}\) jointly distributed by \(p(\mathsf{x},\mathsf{y})\). A softmax classifier is a deterministic function mapping \(x\), via a sequence of intermediate representations, to a point on the simplex \(\Delta^{|\mathcal{Y}|}\) that parameterises a categorical label distribution \(p_{\theta}(\mathsf{y}|x)\).

Any intermediate representation \(z=g(x)\) can be considered the realisation of a _latent_ random variable sampled from a conditional (delta) distribution: \(z\sim p(x|x)=\delta_{z-g(x)}\). Under a (Markov) generative latent variable model (figure 2, _left_):

\[p(x)=\int_{y,z}p(x|z)p(z|y)p(y)\,\] (3)

class labels can be predicted:

\[p_{\theta}(y|x)=\int_{z}p_{\theta}(y|z)p_{\theta}(z|x)\.\] (4)

**A softmax classifier is a special case of Eqn. 4** where: (i) \(f_{\omega}\), the neural network up to the softmax layer, parameterises \(p_{\theta}(z|x)=\delta_{z-f_{\omega}(x)}\), a delta distribution; (ii) the softmax layer input is considered a sample from \(p_{\theta}(z|x)\); and (iii) \(p_{\theta}(\mathsf{y}|z)\) is defined by the softmax layer (see RHS of Equation 1).

Figure 1: Empirical latent distributions: softmax inputs under (\(l\)) Softmax CE [“MLE”]; (\(c\)) MLE + Gaussian \(p_{\theta}(\mathsf{z}|y)\) (contours); [“MAP”]; (\(r\)) VC objective [“Bayesian”]. Colour denotes MNIST class.

**Training a Classification LVM**: Similarly to the latent model for \(p_{\theta}(\mathbf{x})\) (SS2), parameters of eqn 4 cannot generally be learned by directly maximising the likelihood, but rather a lower bound (_cf_ eqn 2):

\[\int_{x,y}\!\!\!p(x,y)\log p_{\theta}(y|x) =\int_{x,y}\!\!\!p(x,y)\!\int_{z}q_{\phi}(z|x)\Big{\{}\log p_{ \theta}(y|z,\boldsymbol{x})-\log\!\frac{q_{\phi}(z|x)}{p_{\theta}(z|x)}+\log \frac{q_{\phi}(z|x)}{p_{\theta}(z|x,y)}\Big{\}}\] \[\geq\int_{x,y}\!\!\!p(x,y)\!\int_{z}q_{\phi}(z|x)\log\frac{p_{ \theta}(z|y)p_{\theta}(y)}{\sum_{y^{\prime}}p_{\theta}(z|y^{\prime})p_{\theta }(y^{\prime})}\;\doteq\;\mathbf{ELBO}_{\mathbf{VC}}\] (5)

Here, \(p_{\theta}(y|z,x)\!=\!p_{\theta}(y|z)\) (figure 2), and the (freely chosen) variational posterior \(q_{\phi}\) is assumed to depend only on \(x\) and to equal \(p_{\theta}(z|x)\) (eliminating the second term).2**Maximising ELBO\({}_{\mathbf{VC}}\) **implicitly encourages \(\boldsymbol{z}\) to learn a sufficient statistic for \(\boldsymbol{y}|\boldsymbol{x}\)**, i.e. \(q_{\phi}(z|x)\to p(z|x,y)\). It can also be shown that **ELBO\({}_{\mathbf{VC}}\) generalises Softmax Cross Entropy** (SCE), under above assumptions.

Footnote 2: We use notation “\(q_{\phi}\)” by analogy to the VAE and later to distinguish \(q_{\phi}(z|y)\), derived from \(q_{\phi}(z|x)\), from \(p_{\theta}(z|y)\).

**Two versions of the same class-conditional latent distributions**:

* _Anticipated_ class-conditional latent distributions \(p_{\theta}(z|y)\) are specified in ELBO\({}_{\text{VC}}\), encoded in the softmax layer, and need to be met for correct label predictions \(p(y|x)\) to be output;
* _Empirical_ class-conditional latent distributions are defined by \(q_{\phi}(z|y)\doteq\!\int_{x}q_{\phi}(z|x)p(x|y)\), i.e. by sampling \(q_{\phi}(z|x)\) (parameterised by the neural network \(f_{\omega}\)), given class samples \(x\sim p(\mathbf{x}|y)\).

In several scenarios that arise in practice, e.g. for finite samples from a continuous data domain \(\mathcal{X}\) (e.g. images or sounds), or if classes are mutually exclusive, **ELBO\({}_{\mathbf{VC}}\) is maximised if all latent representations of a class, hence the entire class-conditional distribution \(q_{\phi}(\boldsymbol{z}|y)\), "collapse" to a point, irrespective of any variance in \(p_{\theta}(z|y)\). Since SCE is a special case of ELBO\({}_{\text{VC}}\), this suggests that softmax classifiers may learn over-concentrated latent distributions and so give _over-confident_ and uncalibrated predictions (subject to the data distribution and model flexibility).

**Aligning anticipated and empirical latent distributions**: We align \(p_{\theta}(z|y)\) and \(q_{\phi}(z|y)\), or encourage \(p_{\theta}(y|z)\) and \(q_{\phi}(z|y)\) to be _consistent under Bayes' rule_ (_cf_\(p_{\theta}(x|z)\) and \(q_{\phi}(z|x)\) in the ELBO, SS2) by minimising \(D_{\text{KL}}\!\left[\,q_{\phi}(\mathbf{z}|y)\,\right]p_{\theta}(\mathbf{z}|y) \!,\;\forall y\!\in\!\mathcal{Y}\), (weighted by \(\beta\!>\!0\)) giving the full VC objective:

\[\mathcal{L}_{\mathbf{VC}}=\int_{x,y}\!\!\!p(x,y)\Big{\{}\int_{z}q_{\phi}(z|x) \log\frac{p_{\theta}(z|y)p_{\theta}(y)}{\sum_{y^{\prime}}p_{\theta}(z|y^{ \prime})p_{\theta}(y^{\prime})}\;-\beta\!\int_{z}q_{\phi}(z|y)\log\frac{q_{ \phi}(z|y)}{p_{\theta}(z|y)}\;+\;\log p_{\pi}(y)\Big{\}}.\] (6)

Taken incrementally, \(q_{\phi}\)-terms of \(\mathcal{L}_{\mathbf{VC}}\) can be interpreted w.r.t. latent variable z as follows (see Fig. 1):

* maximising \(\int_{z}q_{\phi}(z|x)\log p_{\theta}(y|z)\) may overfit \(q_{\phi}(z|y)\) to \(\delta_{z-z_{y}}\) for finite samples; [MLE]
* adding _class priors_\(\int_{z}q_{\phi}(z|y)\log p_{\theta}(z|y)\) constrains the MLE point estimates \(z_{y}\) [MAP]
* adding _entropy_\(-\!\int_{z}q_{\phi}(z|y)\log q_{\phi}(z|y)\) encourages \(q_{\phi}(\mathbf{z}|y)\) to "fill out" \(p_{\theta}(\mathbf{z}|y)\). [Bayesian]

VC abstracts a typical neural network classifier, giving interpretability to its components:

* the neural network up to the last layer (\(f_{\omega}\)) transforms a mixture of unknown class-conditional data distributions \(p(\mathbf{x}|y)\) to a mixture of analytically defined latent distributions \(p_{\theta}(\mathbf{z}|y)\);
* assuming latent variables follow the anticipated class distributions \(p_{\theta}(\mathbf{z}|y)\), the output layer applies Bayes' rule to give \(p_{\theta}(\mathbf{y}|\mathbf{z})\) (see figure 2) and thus the class prediction \(p(\mathbf{y}|x)\) (by eqn 4).

### Optimising the VC Objective

The second term of \(\mathcal{L}_{\mathbf{VC}}\) is not readily computable since \(q_{\phi}(\mathbf{z}|y)\) is implicit and cannot be evaluated only sampled from, as \(z\!\sim\!q_{\phi}(\mathbf{z}|x)\) (parameterised by \(f_{\omega}\)) for class samples \(x\!\sim\!p(\mathbf{x}|y)\). We therefore _approximate_ log ratios \(\log\frac{q_{\phi}(z|y)}{p_{\theta}(z|y)}\) for each class \(y\) by training a binary classifier to distinguish \(z\!\sim\!q_{\phi}(\mathbf{z}|y)\) from \(z\!\sim\!p_{\theta}(\mathbf{z}|y)\) under an _auxiliary objective_\(\mathcal{L}_{\mathbf{aux}}\) with parameters \(\psi\):

\[\mathcal{L}_{\mathbf{aux}}=\int_{y}p(y)\big{\{}\!\int_{z}\!q_{\phi}(z|y)\log \sigma(T_{\psi}^{y}(z))+\!\int_{z}\!p_{\theta}(z|y)\log(1\!-\!\sigma(T_{\psi}^ {y}(z))\big{\}}\] (7)

where \(\sigma(x)\!=\!(1+e^{-x})^{-1}\) is the logistic sigmoid, \(T_{\psi}^{y}(z)\!=\!w_{y}^{\top}z+b_{y}\) and \(\psi\!=\!\{w_{y},b_{y}\}_{y\in\mathcal{Y}}\). This approach is _adversarial_: \(\mathcal{L}_{\mathbf{VC}}\) is maximised when log ratios give a _minimal_ KL divergence (0), i.e. \(q_{\phi}(z|y)\!=\!p_{\theta}(z|y)\) and \(z\!\sim\!q_{\phi}(z|y)\) are indistinguishable from \(z\!\sim\!p_{\theta}(z|y)\); whereas \(\mathcal{L}_{\mathbf{aux}}\) is maximised if the ratio is _maximal_ and the distributions are optimally discriminated. See Algorithm 1 for a summary.

[MISSING_PAGE_FAIL:4]

## Acknowledgments and Disclosure of Funding

Carl is gratefully supported by an ETH AI Centre Postdoctoral Fellowships and a small projects grant from the Haslersfittung (no. 23072). Mrinmaya acknowledges support from the Swiss National Science Foundation (Project No. 197155), a Responsible AI grant by the Haslersfittung; and an ETH Grant (ETH-19 21-1).

## References

* Adem et al. (2019) Kemal Adem, Serhat Kilicarslan, and Onur Comert. Classification and diagnosis of cervical cancer with stacked autoencoder and softmax classification. _Expert Systems with Applications_, 115:557-564, 2019.
* Bandi et al. (2018) Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. In _IEEE Transactions on Medical Imaging_, 2018.
* Bowman et al. (2015) Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. _arXiv preprint arXiv:1508.05326_, 2015.
* Goodfellow et al. (2013) Ian J Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay Shet. Multi-digit number recognition from street view imagery using deep convolutional neural networks. _arXiv preprint arXiv:1312.6082_, 2013.
* Goodfellow et al. (2014) Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. _arXiv preprint arXiv:1412.6572_, 2014.
* Grathwohl et al. (2019) Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. In _International Conference on Learning Representations_, 2019.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Conference on Computer Vision and Pattern Recognition_, 2016.
* Hendrycks and Dietterich (2019) Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. _arXiv preprint arXiv:1903.12261_, 2019.
* Kingma and Welling (2014) Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In _International Conference on Learning Representations_, 2014.
* Klasson et al. (2019) Marcus Klasson, Cheng Zhang, and Hedvig Kjellstrom. A hierarchical grocery store image dataset with visual and semantic labels. In _IEEE Winter Conference on Applications of Computer Vision_, 2019.
* Koh et al. (2021) Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In _International Conference on Machine Learning_, 2021.
* Liu et al. (2015) Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _International Conference on Computer Vision_, 2015.
* Mirbabaie et al. (2021) Milad Mirbabaie, Stefan Stieglitz, and Nicholas RJ Frick. Artificial intelligence in disease diagnostics: A critical review and classification on the current state of research guiding future direction. _Health and Technology_, 11(4):693-731, 2021.
* Mukhoti et al. (2021) Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deep deterministic uncertainty: A simple baseline. _arXiv e-prints_, pp. arXiv-2102, 2021.
* Naeini et al. (2015) Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In _AAAI Conference on Artificial Intelligence_, 2015.
* Naeini et al. (2019)* Rezende et al. [2014] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In _International Conference on Machine Learning_, 2014.
* Scott et al. [2021] Tyler R Scott, Andrew C Gallagher, and Michael C Mozer. von mises-fisher loss: An exploration of embedding geometries for supervised learning. In _International Conference on Computer Vision_, 2021.
* Tiensuu et al. [2019] Jacob Tiensuu, Maja Linderholm, Sofia Dreborg, and Fredrik Orn. Detecting exoplanets with machine learning: A comparative study between convolutional neural networks and support vector machines, 2019.
* Wan et al. [2018] Weitao Wan, Yuanyi Zhong, Tianpeng Li, and Jiansheng Chen. Rethinking feature distribution for loss functions in image classification. In _Conference on Computer Vision and Pattern Recognition_, 2018.
* Williams et al. [2018] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In _North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, 2018.
* Zagoruyko and Komodakis [2016] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. _arXiv preprint arXiv:1605.07146_, 2016.

Proofs

### Optimising the ELBO\({}_{\text{VC}}\) w.r.t \(\boldsymbol{q}\)

Rearranging Equation 5, the ELBO\({}_{\text{VC}}\) is optimised by

\[\operatorname*{arg\,max}_{q_{\phi}(z|x)}\int_{x}\sum_{y}p(x,y)\! \int_{z}q_{\phi}(z|x)\log p_{\theta}(y|z)\] \[= \operatorname*{arg\,max}_{q_{\phi}(z|x)}\int_{x}\!p(x)\int_{z}\! q_{\phi}(z|x)\sum_{y}p(y|x)\log p_{\theta}(y|z)\]

The integral over \(z\) is a \(q_{\phi}(z|x)\)-weighted sum of \(\sum_{y}p(y|x)\log p_{\theta}(y|z)\) terms. Since \(q_{\phi}(z|x)\) is a probability distribution, the integral is upper bounded by \(\max_{z}\sum_{y}p(y|x)\log p_{\theta}(y|z)\). This maximum is attained _iff_ support of \(q_{\phi}(z|x)\) is restricted to \(z^{*}=\operatorname*{arg\,max}_{z}\sum_{y}p(y|x)\log p_{\theta}(y|z)\) (which may not be unique). 

### Optimising the VC objective w.r.t. \(\boldsymbol{q}\)

Setting \(\beta=1\) in Equation 6 to simplify and adding a lagrangian term to constrain \(q_{\phi}(z|x)\) to a probability distribution, we aim to find

\[\operatorname*{arg\,max}_{q_{\phi}(z|x)}\int_{x}\sum_{y}p(x,y) \Big{\{}\int_{z}q_{\phi}(z|x)\log p_{\theta}(y|z)\] \[-\!\int_{z}\!q_{\phi}(z|y)\log\tfrac{q_{\phi}(z|y)}{p_{\theta}(z |y)}\,+\,\log p_{\pi}(y)\Big{\}}+\lambda(1\!-\!\int_{z}\!q_{\phi}(z|x))\;.\]

Recalling that \(q_{\phi}(z|y)=\int_{x}q_{\phi}(z|x)p(x|y)\) and using calculus of variations, we set the derivative of this functional w.r.t. \(q_{\phi}(z|x)\) to zero

\[\sum_{y}p(x,y)\Big{\{}\log p_{\theta}(y|z)-(\log\tfrac{q_{\phi}(z|y)}{p_{ \theta}(z|y)}+1)\Big{\}}-\lambda=0\]

Rearranging and diving through by \(p(x)\) gives

\[\mathbb{E}_{p(y|x)}[\log q_{\phi}(z|y)]=\mathbb{E}_{p(y|x)}[\log p_{\theta}(y |z)p_{\theta}(z|y)]+c\,,\]

where \(c=-(1+\tfrac{\lambda}{p(x)})\). Further, if each label \(y\) occurs once with each \(x\), due to sampling or otherwise, then this simplifies to

\[q_{\phi}(z|y^{*})e^{c}=p_{\theta}(y^{*}|z)p_{\theta}(z|y^{*})\;,\]

which holds for all classes \(y\in\mathcal{Y}\). Integrating over \(z\) shows \(e^{c}=\int_{z}p_{\theta}(y|z)p_{\theta}(z|y)\) to give

\[q_{\phi}(z|y)=\tfrac{p_{\theta}(y|z)p_{\theta}(z|y)}{\int_{z}p_{\theta}(y|z)p_ {\theta}(z|y)}=p_{\theta}(z|y)\tfrac{p_{\theta}(y|z)}{\mathbb{E}_{p_{\theta}(z |y)}[p_{\theta}(y|z)]}\;.\quad\Box\]

We note, it is straightforward to include \(\beta\) to show

\[q_{\phi}(z|y)=p_{\theta}(z|y)\tfrac{p_{\theta}(y|z)^{1/\beta}}{\mathbb{E}_{p_ {\theta}(z|y)}[p_{\theta}(y|z)^{1/\beta}]}\;.\]Justifying the Latent Prior in Variational Classification

Choosing Gaussian class priors in Variational classification can be interpreted in two ways:

**Well-specified generative model**: Assume data \(x\in\mathcal{X}\) is generated from the hierarchical model: \(\text{y}\rightarrow\text{z}\rightarrow\text{x}\), where \(p(\text{y})\) is categorical; \(p(\text{z}|y)\) are analytically known distributions, e.g. \(\mathcal{N}(z;\mu_{y},\Sigma_{y})\); the dimensionality of z is not large; and \(x\!=\!h(z)\) for an arbitrary invertible function \(h:\mathcal{Z}\rightarrow\mathcal{X}\) (if \(\mathcal{X}\) is of higher dimension than \(\mathcal{Z}\), assume \(h\) maps one-to-one to a manifold in \(\mathcal{X}\)). Accordingly, \(p(\text{x})\) is a mixture of unknown distributions. If \(\{p_{\theta}(\text{z}|y)\}_{\theta}\) includes the true distribution \(p(\text{z}|y)\), variational classification effectively aims to invert \(h\) and learn the parameters of the true generative model. In practice, the model parameters and \(h^{-1}\) may only be identifiable up to some equivalence, but by reflecting the true latent variables, the learned latent variables should be semantically meaningful.

**Miss-specified model**: Assume data is generated as above, but with z having a large, potentially uncountable, dimension with complex dependencies, e.g. details of every blade of grass or strand of hair in an image. In general, it is impossible to learn all such latent variables with a lower dimensional model. The latent variables of a VC might learn a complex function of multiple true latent variables.

The first scenario is ideal since the model might learn disentangled, semantically meaningful features of the data. However, it requires distributions to be well-specified and a low number of true latent variables. For natural data with many latent variables, the second case seems more plausible but choosing \(p_{\theta}(\text{z}|y)\) to be Gaussian may nevertheless be justifiable by the Central Limit Theorem.

## Appendix C Variational Classification Algorithm

```
1:Input \(p_{\theta}(\text{z}|y)\), \(q_{\phi}(\text{z}|x)\), \(p_{\pi}(\text{y})\), \(T_{\psi}(z)\); learning rate schedule \(\{\eta_{\theta}^{t},\eta_{\phi}^{t},\eta_{\pi}^{t},\eta_{\psi}^{t}\}_{t}\)
2:Initialise \(\theta,\phi,\pi,\psi;\;t\gets 0\)
3:while not converged do
4:\(\{x_{i},y_{i}\}_{i=1}^{m}\sim\mathcal{D}\) [sample batch from data distribution \(p(\text{x},\text{y})\)]
5:for z = { 1... m} do
6:\(z_{i}\sim q_{\phi}(\text{z}|x_{i})\), \(z_{i}^{\prime}\sim p_{\theta}(\text{z}|y_{i})\) [e.g. \(q_{\phi}(z|x_{i})\!=\!\delta_{z-f_{\omega}(x_{i})},\phi\!=\!\omega\Rightarrow\;z_ {i}\!=\!f_{\omega}(x_{i})\)]
7:\(p_{\theta}(y_{i}|z_{i})=\frac{p_{\theta}(z_{i}|y_{i})p_{\theta}(y_{i})}{\sum_ {y}p_{\theta}(z_{i}|y_{i})p_{\pi}(y)}\)
8:endfor
9:\(g_{\theta}\leftarrow\frac{1}{m}\sum_{i=1}^{m}\nabla_{\theta}[\log p_{\theta}(y _{i}|z_{i})+p_{\theta}(z_{i}|y_{i})]\)
10:\(g_{\phi}\leftarrow\frac{1}{m}\sum_{i=1}^{m}\nabla_{\phi}[\log p_{\theta}(y_{i} |z_{i})-T_{\psi}(z_{i})]\) [e.g. using "reparameterisation trick"]
11:\(g_{\pi}\leftarrow\frac{1}{m}\sum_{i=1}^{m}\nabla_{\pi}\log p_{\pi}(y_{i})\)
12:\(g_{\psi}\leftarrow\frac{1}{m}\sum_{i=1}^{m}\nabla_{\psi}[\log\sigma(T_{\psi}( z_{i}))+\log(1\!-\!\sigma(T_{\psi}(z_{i}^{\prime}))]\)
13:\(\theta\leftarrow\theta+\eta_{\theta}^{t}\,g_{\theta},\quad\phi\leftarrow\phi +\eta_{\phi}^{t}\,g_{\phi},\quad\pi\leftarrow\pi+\eta_{\pi}^{t}\,g_{\pi}, \quad\psi\leftarrow\psi+\eta_{\psi}^{t}\,g_{\psi},\quad\quad t\gets t+1\)
14:endwhile ```

**Algorithm 1** Variational Classification (VC)

## Appendix D Calibration Metrics

One way to measure if a model is calibrated is to compute the expected difference between the confidence and expected accuracy of a model.

\[\mathbb{E}_{P(\hat{y}|x)}\Big{[}\mathbb{P}(\hat{y}=y|P(\hat{y}|x)=p)-p\Big{]}\] (8)

This is known as expected calibration error (ECE) (Naeini et al., 2015). Practically, ECE is estimated by sorting the predictions by their confidence scores, partitioning the predictions in \(M\) equally spaced bins \((B_{1}\ldots B_{M})\) and taking the weighted average of the difference between the average accuracy and average confidence of the bins. In our experiments we use 20 equally spaced bins.

\[\text{ECE}=\sum_{m=1}^{M}\frac{|B_{m}|}{n}\left|\textit{acc}(B_{m})-\textit{ conf}(B_{m})\right|\] (9)

[MISSING_PAGE_FAIL:9]

In many real-world settings, datasets may have relatively few data samples and it may be prohibitive or impossible to acquire more, e.g. historic data or rare medical cases. We investigate model performance when data is scarce on the hypothesis that a prior over the latent space enables the model to better generalise from fewer samples. Models are trained on 500 samples from MNIST, 1000 samples from CIFAR-10 and 50 samples from AGNews.

Results in Table 4 show that introducing the prior (GM) improves performance in a low data regime and that the additional entropy term in the VC model maintains or further improves accuracy (**H4**), particularly on the more complex datasets.

We further probe the relative benefit of the VC model over the CE baseline as the training sample size varies (**H4**) on MedMNIST, a collection of real-world medical datasets of varying sizes.

Figure 5 shows the increase in classification accuracy for the VC model relative to the CE model against number of training samples (log scale). The results show a clear trend that the benefit of the additional latent structure imposed in the VC model increases exponentially as the number of training samples decreases. Together with the results in Table 4, this suggests that the VC model offers most significant benefit for small, complex datasets.

### Classification under Domain Shift

A comparison of accuracy between the VC and CE models under 16 different synthetic domain shifts. We find that VC performs comparably well as CE.

### OOD Detection

\begin{table}
\begin{tabular}{l|c|c|c} \hline  & CE & GM & VC \\ \hline MNIST & 93.1 \(\pm\) 0.2 & **94.4**\(\pm\) 0.1 & **94.2**\(\pm\) 0.2 \\ CIFAR-10 & 52.7 \(\pm\) 0.5 & 54.2 \(\pm\) 0.6 & **56.3**\(\pm\) 0.6 \\ AGNews & 56.3 \(\pm\) 5.3 & 61.5\(\pm\)2.9 & **66.3**\(\pm\) 4.6 \\ \hline \end{tabular}
\end{table}
Table 4: Accuracy in low data regime (mean, std.err., 5 runs)

Figure 5: Accuracy increase of VC over CE on MedMNIST datasets of varying training set size (mean, std.err., 3 runs)

Figure 6: Classification accuracy under distributional shift: _(left)_ CIFAR-10-C _(middle)_ CIFAR-100-C _(right)_ Tiny-Imagenet-C

Figure 7: t-SNE plots of the feature space for a classifier trained on CIFAR-10. _(l)_ Trained using CE. _(r)_ Trained using VC. We posit that similar to CE, VC model is unable to meaningfully represent data from an entirely different distribution.

Semantics of the latent space

To try to understand the semantics captured in the latent space, we use a pre-trained MNIST model on the _Ambiguous_ MNIST dataset (Mukhoti et al., 2021). We interpolate between ambiguous 7's that are mapped close to the Gaussian clusters of classes of "1" and "2". It can be observed that traversing from the mean of the "7" Gaussian to that on the "1" class, the ambiguous 7's begin to look more like "1"s.

Figure 8: Interpolating in the latent space: Ambiguous MNIST when mapped on the latent space. _(l)_ VC, _(r)_ CE