# Risk-Sensitive Control as Inference

with Renyi Divergence

 Kaito Ito

The University of Tokyo

kaito@g.ecc.u-tokyo.ac.jp

&Kenji Kashima

Kyoto University

kk@i.kyoto-u.ac.jp

###### Abstract

This paper introduces the risk-sensitive control as inference (RCal) that extends CaI by using Renyi divergence variational inference. RCal is shown to be equivalent to log-probability regularized risk-sensitive control, which is an extension of the maximum entropy (MaxEnt) control. We also prove that the risk-sensitive optimal policy can be obtained by solving a soft Bellman equation, which reveals several equivalences between RCal, MaxEnt control, the optimal posterior for CaI, and linearly-solvable control. Moreover, based on RCal, we derive the risk-sensitive reinforcement learning (RL) methods: the policy gradient and the soft actor-critic. As the risk-sensitivity parameter vanishes, we recover the risk-neutral CaI and RL, which means that RCal is a unifying framework. Furthermore, we give another risk-sensitive generalization of the MaxEnt control using Renyi entropy regularization. We show that in both of our extensions, the optimal policies have the same structure even though the derivations are very different.

## 1 Introduction

Optimal control theory is a powerful framework for sequential decision making [1]. In optimal control problems, one seeks to find a control policy that minimizes a given cost functional and typically assumes the full knowledge of the system's dynamics. Optimal control with unknown or partially known dynamics is called reinforcement learning (RL) [2], which has been successfully applied to highly complex and uncertain systems, e.g., robotics [3], self-driving vehicles [4]. However, solving optimal control and RL problems is still challenging, especially for continuous spaces.

Control as Inference (CaI), which connects optimal control and Bayesian inference, is a promising paradigm for overcoming the challenges of RL [5]. In CaI, the optimality of a state and control trajectory is defined by introducing optimality variables rather than explicit costs. Consequently, an optimal control problem can be formulated as a probabilistic inference problem. In particular, maximum entropy (MaxEnt) control [6; 7] is equivalent to a variational inference problem using the Kullback-Leibler (KL) divergence. MaxEnt control has entropy regularization of a control policy, and as a result, the optimal policy is stochastic. Several works have revealed the advantages of the regularization such as robustness against disturbances [8], natural exploration induced by the stochasticity [7; 9], fast convergence of the MaxEnt policy gradient method [10].

On the other hand, the KL divergence is not the only option available for variational inference. In [11], the variational inference was extended to Renyi \(\alpha\)-divergence [12], which is a rich family of divergences including the KL divergence. Similar to the traditional variational inference, this extension optimizes a lower bound of the evidence, which is called the variational Renyi bound. The parameter \(\alpha\) of Renyi divergence controls the balance between mass-covering and zero-forcing effects for approximate inference [13]. However, if we use Renyi divergence for CaI, it remains unclear how \(\alpha\) affects the optimal policy, and a natural question arises: what objective does CaI using Renyi divergence optimize?ContributionsThe contributions of this work are as follows:

1. We reveal that CaI with Renyi divergence solves a log-probability (LP) regularized risk-sensitive control problem with exponential utility [14] (Theorem 2). The order parameter \(\alpha\) of Renyi divergence plays a role of the risk-sensitivity parameter, which determines whether the resulting policy is risk-averse or risk-seeking. Based on the result, we refer to CaI using Renyi divergence as _risk-sensitive_ CaI (RCaI). Since Renyi divergence includes the KL divergence, RCaI is a unifying framework of CaI. Additionally, we show that the risk-sensitive optimal policy takes the form of the Gibbs distribution whose energy is given by the Q-function, which can be obtained by solving a soft Bellman equation (Theorem 3). Furthermore, this reveals several equivalence results between RCaI, MaxEnt control, the optimal posterior for CaI, and linearly-solvable control [15, 16].
2. Based on RCaI, we derive risk-sensitive RL methods. First, we provide a policy gradient method [17, 18, 19] for the regularized risk-sensitive RL (Proposition 7). Next, we derive the risk-sensitive counterpart of the soft actor-critic algorithm [7] through the maximization of the variational Renyi bound (Subsection 4.2). As the risk-sensitivity parameter vanishes, the proposed methods converge to REINFORCE [19] with entropy regularization and risk-neutral soft actor-critic [7], respectively. One of their advantages over other risk-sensitive approaches, including distributional RL [20, 21], is that they require only minor modifications to the standard REINFORCE and soft actor-critic. The behavior of the risk-sensitive soft actor-critic is examined via an experiment.
3. Although the risk-sensitive control induced by RCaI has LP regularization of the policy, it is not entropy, unlike the MaxEnt control with the Shannon entropy regularization. To bridge this gap, we provide another risk-sensitive generalization of the MaxEnt control using Renyi entropy regularization. We prove that the resulting optimal policy and the Bellman equation have the same structure as the LP regularized risk-sensitive control (Theorem 6). The derivation differs significantly from that for the LP regularization, and for the analysis, we establish the duality between exponential integrals and Renyi entropy (Lemma 5).

The established relations between several control problems in this paper are summarized in Fig. 1.

Related workThe duality between control and inference has been extensively studied [15, 22, 23, 24, 25, 26]. Inspired by CaI, [27, 28] reformulated model predictive control (MPC) as a variational inference problem. In [29], variational inference MPC using Tsallis divergence, which is equivalent to Renyi divergence, was proposed. The difference between our results and theirs is that variational inference MPC infers _feed-forward_ optimal control while RCaI infers feedback optimal control. Consequently, the equivalence of risk-sensitive control and Tsallis variational inference MPC is not derived, unlike RCaI. The work [30] proposed an EM-style algorithm for RL based on CaI, where the resulting policy is risk-seeking. However, risk-averse policies cannot be derived from CaI by this approach. Our framework provides the equivalence between CaI and risk-sensitive control both for risk-seeking and risk-averse cases.

Risk-averse policies are known to yield robust control [31, 32], and risk-seeking policies are useful for balancing exploration and exploitation for RL [33]. Because of these merits, many efforts have been devoted to risk-sensitive RL [19, 34, 35, 36]. In [37], risk-sensitive RL with Shannon entropy regularization was investigated. However, their theoretical results are valid only for almost risk-neutral cases. Our results imply that LP and Renyi entropy regularization are suitable for the risk-sensitive RL.

In [16], risk-sensitive control whose control cost is defined by Renyi divergence was investigated, and it was shown that the associated Bellman equation can be linearized. However, it is assumed that the transition distribution can be controlled as desired, which is not satisfied in general as pointed out in [38]. On the other hand, our result shows that when the dynamics is deterministic, LP

Figure 1: Relations of control problems.

and Renyi entropy regularized risk-sensitive control problems are linearly solvable without the full controllability assumption of the transition distribution.

NotationFor simplicity, by abuse of notation, we write the density (or probability mass) functions of random variables \(x,y\) as \(p(x),p(y)\), and the expectation with respect to \(p(x)\) is denoted by \(\mathbb{E}_{p(x)}\). For a set \(S\), the set of all densities on \(S\) is denoted by \(\mathcal{P}(S)\). Renyi entropy and divergence with parameter \(\alpha>0\), \(\alpha\neq 1\) are defined as \(\mathcal{H}_{\alpha}(p):=\frac{1}{\alpha(1-\alpha)}\log\bigl{[}\int_{\{u:p(u) >0\}}p(u)^{\alpha}\mathrm{d}u\bigr{]},\,D_{\alpha}(p_{1}\|p_{2}):=\frac{1}{ \alpha-1}\log[\int_{\{u:p_{1}(u)p_{2}(u)>0\}}p_{1}(u)^{\alpha}p_{2}(u)^{1- \alpha}\mathrm{d}u\bigr{]}\). For the factor \(\frac{1}{\alpha(1-\alpha)}\) of \(\mathcal{H}_{\alpha}\), we follow [39, 40] because this choice is convenient for the analysis in Subsection 3.2 rather than another common choice \(1/(1-\alpha)\). We formally extend the definition of \(\mathcal{H}_{\alpha}\) to \(\alpha<0\). Denote the Shannon entropy and KL divergence by \(\mathcal{H}_{1}(p),\,D_{1}(p_{1}\|p_{2})\), respectively because \(\lim_{\alpha\to 1}\mathcal{H}_{\alpha}(p)=\mathcal{H}_{1}(p)\), \(\lim_{\alpha\to 1}D_{\alpha}(p_{1}\|p_{2})=D_{1}(p_{1}\|p_{2})\). For further properties of the Renyi entropy and divergence, see e.g., [41]. The set of integers \(\{k,k+1,\ldots,s\}\), \(k<s\) is denoted by \([\![k,s]\!]\). A sequence \(\{x_{k},x_{k+1},\ldots,x_{s}\}\) is denoted by \(x_{k:s}\). The set of non-negative real numbers is denoted by \(\mathbb{R}_{\geq 0}\).

## 2 Brief introduction to control as inference

First, we briefly introduce the framework of CaI. For the detailed derivation, see Appendix A and [5]. Throughout the paper, \(x_{t}\) and \(u_{t}\) denote \(\mathbb{X}\)-valued state and \(\mathbb{U}\)-valued control variables at time \(t\), respectively, where \(\mathbb{X}\subseteq\mathbb{R}^{n_{x}}\), \(\mathbb{U}\subseteq\mathbb{R}^{n_{u}}\), and \(\mu_{L}(\mathbb{U})>0\). Here, \(\mu_{L}\) denotes the Lebesgue measure on \(\mathbb{R}^{n_{u}}\). The initial distribution is \(p(x_{0})\), and the transition density is denoted by \(p(x_{t+1}|x_{t},u_{t})\), which depends only on the current state and control input. Let \(T>0\) be a finite time horizon. CaI connects control and probabilistic inference problems by introducing _optimality variables_\(\mathcal{O}_{t}\in\{0,1\}\) as in Fig. 2. For \(c_{t}:\mathbb{X}\times\mathbb{U}\to\mathbb{R}_{\geq 0},c_{T}:\mathbb{X}\to \mathbb{R}_{\geq 0}\), which will serve as cost functions, the distribution of \(\mathcal{O}_{t}\) is given by \(p(\mathcal{O}_{t}=1|x_{t},u_{t})=\exp(-c_{t}(x_{t},u_{t})),\;t\in[\![0,T-1]\!]\) and \(p(\mathcal{O}_{T}=1|x_{T})=\exp(-c_{T}(x_{T}))\). If \(\mathcal{O}_{t}=1\), then \((x_{t},u_{t})\) at time \(t\) is said to be "optimal." The control posterior \(p(u_{t}|x_{t},\mathcal{O}_{t:T}=1)\) is called the optimal policy. Let the prior of \(u_{t}\) be uniform: \(p(u_{t})=1/\mu_{L}(\mathbb{U}),\forall u_{t}\in\mathbb{U}\). Although this choice is common for CaI, the arguments in this paper may be extended to non-uniform priors. Then, for the graphical model in Fig. 2, the distribution of the optimal state and control input trajectory \(\tau:=(x_{0:T},u_{0:T-1})\) satisfies

\[p(\tau|\mathcal{O}_{0:T}=1) \propto\left[p(x_{0})\prod_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t}) \right]\left[p(\mathcal{O}_{T}=1|x_{T})\prod_{t=0}^{T-1}p(\mathcal{O}_{t}=1|x_ {t},u_{t})\right]\] \[=\left[p(x_{0})\prod_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t})\right]\exp \left(-c_{T}(x_{T})-\sum_{t=0}^{T-1}c_{t}(x_{t},u_{t})\right).\] (1)

For notational simplicity, we will drop \(=1\) for \(\mathcal{O}_{t}\) in the remainder of this paper.

The optimal policy \(p(u_{t}|x_{t},\mathcal{O}_{t:T})\) can be computed in a recursive manner. To this end, define

\[\mathsf{Q}_{t}(x_{t},u_{t}):=-\log\frac{p(\mathcal{O}_{t:T}|x_{t},u_{t})}{\mu_ {L}(\mathbb{U})},\;\;\mathsf{V}_{t}(x_{t}):=-\log p(\mathcal{O}_{t:T}|x_{t}),\] (2)

which play a role of value functions. Then, the following result holds.

**Proposition 1**.: _Assume that \(\mu_{L}(\mathbb{U})<\infty\) and let \(\mathsf{c}_{t}(x_{t},u_{t}):=c_{t}(x_{t},u_{t})+\log\mu_{L}(\mathbb{U})\). Assume further the existence of density functions \(p(x_{0})\) and \(p(x_{t+1}|x_{t},u_{t})\) for any \(t\in[\![0,T-1]\!]^{1}\). Then, it holds that_

\[p(u_{t}|x_{t},\mathcal{O}_{t:T}=1)=\exp\left(-\mathsf{Q}_{t}(x_{t},u_{t})+ \mathsf{V}_{t}(x_{t})\right),\;\;\forall x_{t}\in\mathbb{X},\;\forall u_{t}\in \mathbb{U},\] (3)

Figure 2: Graphical model for CaI.

_where_

\[\mathsf{V}_{t}(x_{t}) =-\log\left[\int_{\mathbb{U}}\exp(-\mathsf{Q}_{t}(x_{t},u_{t})) \mathrm{d}u_{t}\right],\;\forall t\in\llbracket 0,T-1\rrbracket,\;\;\mathsf{V}_{T}(x_{T})=c_{T}(x_{T }),\] (4) \[\mathsf{Q}_{t}(x_{t},u_{t}) =\mathsf{c}_{t}(x_{t},u_{t})-\log\mathbb{E}_{p(x_{t+1}|x_{t},u_{ t})}\left[\exp(-\mathsf{V}_{t+1}(x_{t+1}))\right],\;\forall t\in\llbracket 0,T-1\rrbracket.\] (5)

The recursive computation (4), (5) is similar to the Bellman equation for the risk-seeking control. However, it is not still clear what kind of performance index the optimal trajectory \(p(\tau|\mathcal{O}_{t:T})\) optimizes because (4) does not coincide with that of the conventional risk-seeking control. An indirect way to make this clear is variational inference. Let us consider finding the closest trajectory distribution \(p^{\pi}(\tau)\) to the optimal distribution \(p(\tau|\mathcal{O}_{0:T})\). The variational distribution is chosen as

\[p^{\pi}(\tau)=p(x_{0})\prod_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t})\pi_{t}(u_{t}|x_{ t}),\] (6)

where \(\pi_{t}(\cdot|x_{t})\in\mathcal{P}(\mathbb{U})\) is the conditional density of \(u_{t}\) given \(x_{t}\) and corresponds to a control policy. Then, the minimization of the KL divergence \(D_{1}(p^{\pi}(\tau)\|p(\tau|\mathcal{O}_{0:T}))\) is known to be equivalent to the following MaxEnt control problem:

\[\operatorname*{minimize}_{\{\pi_{t}\}_{t=0}^{T-1}}\;\mathbb{E}_{p^{\pi}(\tau) }\left[c_{T}(x_{T})+\sum_{t=0}^{T-1}\Bigl{(}c_{t}(x_{t},u_{t})-\mathcal{H}_{1 }(\pi_{t}(\cdot|x_{t}))\Bigr{)}\right].\] (7)

Especially when the system \(p(x_{t+1}|x_{t},u_{t})\) is deterministic, the minimum value of \(D_{1}(p^{\pi}(\tau)\|p(\tau|\mathcal{O}_{0:T}))\) is \(0\), and the posterior \(p(u_{t}|x_{t},\mathcal{O}_{t:T})\) yields the optimal control of (7). As mentioned in Introduction, this work uses Renyi divergence rather than the KL divergence. Moreover, we characterize the optimal posterior \(p(u_{t}|x_{t},\mathcal{O}_{t:T})\) more directly even for stochastic systems.

## 3 Control as Renyi divergence variational inference

In this section, we address the question of what kind of control problem is solved by CaI with Renyi divergence and characterize the optimal policy.

### Equivalence between CaI with Renyi divergence and risk-sensitive control

Let \(\eta>-1\), \(\eta\neq 0\). Then, CaI using Renyi variational inference is formulated as the minimization of \(D_{1+\eta}(p^{\pi}(\tau)\|p(\tau|\mathcal{O}_{0:T}))\) with respect to \(p^{\pi}\) in (6). Now, we have

\[D_{1+\eta}(p^{\pi}\|p(\cdot|\mathcal{O}_{0:T}))=\underbrace{\frac{1}{\eta}\log \left[\int p^{\pi}(\tau)^{1+\eta}p(\tau,\mathcal{O}_{0:T})^{-\eta}\mathrm{d} \tau\right]}_{-(\text{Variational \& Renyi bound})}+\log p(\mathcal{O}_{0:T}).\] (8)

That is, CaI with Renyi divergence is equivalent to maximizing the above variational Renyi bound. Moreover, by (1), it holds that

\[\log\left[\int p^{\pi}(\tau)^{1+\eta}p(\tau,\mathcal{O}_{0:T})^{ -\eta}\mathrm{d}\tau\right]\] \[=\log\left[\int p^{\pi}(\tau)\left(\frac{p(x_{0})\prod_{t=0}^{T-1 }p(x_{t+1}|x_{t},u_{t})\pi_{t}(u_{t}|x_{t})}{\frac{1}{\mu_{L}(\mathbb{U})}p(x_ {0})\left[\prod_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t})\right]\exp\left(-c_{T}(x_{T })-\sum_{t=0}^{T-1}c_{t}(x_{t},u_{t})\right)}\right)^{\eta}\mathrm{d}\tau\right]\] \[=\log\biggl{[}\int p^{\pi}(\tau)\exp\biggl{(}\eta c_{T}(x_{T})+ \eta\sum_{t=0}^{T-1}\Bigl{(}c_{t}(x_{t},u_{t})+\log\pi_{t}(u_{t}|x_{t})\Bigr{)} \biggr{)}\mathrm{d}\tau\biggr{]}+\eta\log\mu_{L}(\mathbb{U}).\]

Consequently, we obtain the first equivalence result in this paper.

**Theorem 2**.: _Suppose that the assumptions in Proposition 1 hold. Then, for any \(\eta>-1\), \(\eta\neq 0\), the minimization of \(D_{1+\eta}(p^{\pi}\|p(\cdot|\mathcal{O}_{0:T}=1))\) with respect to \(p^{\pi}\) in (6) is equivalent to_

\[\operatorname*{minimize}_{\{\pi_{t}\}_{t=0}^{T-1}}\;\frac{1}{\eta}\log\mathbb{ E}_{p^{\pi}(\tau)}\left[\exp\left(\eta c_{T}(x_{T})+\eta\sum_{t=0}^{T-1}\Bigl{(}c_{t} (x_{t},u_{t})+\log\pi_{t}(u_{t}|x_{t})\Bigr{)}\right)\right].\] (9)

\(\diamondsuit\)Problem (9) is a risk-sensitive control problem with the log-probability regularization \(\log\pi_{t}(u_{t}|x_{t})\) of the control policy. Let \(\eta\Phi(\tau)\) be the exponent in (9). Then, \(\frac{1}{\eta}\log\mathbb{E}[\exp(\eta\Phi(\tau))]=\mathbb{E}[\Phi(\tau)]+\frac{ \eta}{2}\mathrm{Var}[\Phi(\tau)]+O(\eta^{2})\), where \(\mathrm{Var}[\cdot]\) denotes the variance [42]. Hence, \(\eta>0\) (resp. \(\eta<0\)) leads to risk-averse (resp. risk-seeking) policies. As \(\eta\) goes to zero, the objective in (9) converges to the risk-neutral MaxEnt control problem (7).

### Derivation of optimal control and further equivalence results

In this subsection, we derive the optimal policy of (9) and give its characterizations. For the analysis, we do not need the non-negativity of the cost \(c_{t}\). We only sketch the derivation, and the detailed proof is given in Appendix B. Similar to the conventional optimal control problems, we adopt the dynamic programming. Another approach based on variational inference will be given in Subsection 4.2. Define the optimal (state-)value function \(V_{t}:\mathbb{X}\to\mathbb{R}\) and the Q-function \(\mathcal{Q}_{t}:\mathbb{X}\times\mathbb{U}\to\mathbb{R}\) as follows:

\[V_{t}(x_{t}):=\inf_{\{\pi_{s}\}_{s=t}^{T-1}}\frac{1}{\eta}\log \mathbb{E}_{p^{\pi}(\tau|x_{t})}\left[\exp\left(\eta c_{T}(x_{T})+\eta\sum_{s =t}^{T-1}\bigl{(}c_{s}(x_{s},u_{s})+\log\pi_{s}(u_{s}|x_{s})\bigr{)}\right) \right],\] (10)

\[\mathcal{Q}_{t}(x_{t},u_{t}):=c_{t}(x_{t},u_{t})+\frac{1}{\eta} \log\mathbb{E}_{p(x_{t+1}|x_{t},u_{t})}\left[\exp\bigl{(}\eta V_{t+1}(x_{t+1}) \bigr{)}\right],\ \ t\in\llbracket 0,T-1\rrbracket,\] (11)

and \(V_{T}(x_{T}):=c_{T}(x_{T})\). Then, it can be shown that the Bellman equation for Problem (9) is

\[V_{t}(x_{t})=-\log\left[\int_{\mathbb{U}}\exp\left(-\mathcal{Q}_{t}(x_{t},u^{ \prime})\right)\mathrm{d}u^{\prime}\right]+\inf_{\pi_{t}(\cdot|x_{t})\in \mathcal{P}(\mathbb{U})}D_{1+\eta}(\pi_{t}(\cdot|x_{t})\|\pi_{t}^{*}(\cdot|x_{ t})),\] (12)

where \(\pi_{t}^{*}(u_{t}|x_{t}):=\exp\left(-\mathcal{Q}_{t}(x_{t},u_{t})\right)/ \mathcal{Z}_{t}(x_{t})\), and the normalizing constant is assumed to fulfill \(\mathcal{Z}_{t}(x_{t}):=\int_{\mathbb{U}}\exp\left(-\mathcal{Q}_{t}(x_{t},u^{ \prime})\right)\mathrm{d}u^{\prime}<\infty\). Since \(D_{1+\eta}(\pi_{t}(\cdot|x_{t})\|\pi_{t}^{*}(\cdot|x_{t}))\) attains its minimum value 0 if and only if \(\pi_{t}(\cdot|x_{t})=\pi_{t}^{*}(\cdot|x_{t})\), the unique optimal policy that minimizes the right-hand side of (12) is given by \(\pi_{t}^{*}(\cdot|x_{t})\) and

\[V_{t}(x_{t})=-\log\left[\int_{\mathbb{U}}\exp\left(-\mathcal{Q}_{t}(x_{t},u^{ \prime})\right)\mathrm{d}u^{\prime}\right],\ \ \pi_{t}^{*}(u_{t}|x_{t})=\exp\left(-\mathcal{Q}_{t}(x_{t},u_{t})+V_{t}(x_{t}) \right).\] (13)

Because of the softmin operation above, the left equation in (13) is called the soft Bellman equation.

**Theorem 3**.: _Assume that \(\int_{\mathbb{U}}\exp\left(-\mathcal{Q}_{t}(x,u^{\prime})\right)\mathrm{d}u^{ \prime}<\infty\) holds for any \(t\in\llbracket 0,T-1\rrbracket\) and \(x\in\mathbb{X}\). Let \(\eta>-1\), \(\eta\neq 0\). Then, the unique optimal policy of Problem (9) is given by (13). Especially when the dynamics is deterministic, i.e., \(p(x_{t+1}|x_{t},u_{t})=\delta(x_{t+1}-\bar{f}_{t}(x_{t},u_{t}))\) for some \(\bar{f}_{t}:\mathbb{X}\times\mathbb{U}\to\mathbb{X}\) and the Dirac delta function \(\delta\), it holds that_

\[\mathcal{Q}_{t}(x_{t},u_{t})=c_{t}(x_{t},u_{t})+V_{t+1}\left(\bar{f}_{t}(x_{t}, u_{t})\right),\] (14)

_and the optimal policy of the MaxEnt control problem (7) solves the LP-regularized risk-sensitive control problem (9) for any \(\eta>-1\), \(\eta\neq 0\). \(\diamondsuit\)_

Assumption \(\int_{\mathbb{U}}\exp\left(-\mathcal{Q}_{t}(x,u^{\prime})\right)\mathrm{d}u^{ \prime}<\infty\) is satisfied for example when \(c_{t}\) is bounded for any \(t\in\llbracket 0,T\rrbracket\) and \(\mu_{L}(\mathbb{U})<\infty\). The linear quadratic setting also fulfills this assumption; see (16).

Theorem 3 suggests several equivalence results:

**RCal and MaxEnt control for deterministic systems.** First, we emphasize that even though the equivalence between _unregularized_ risk-neutral and risk-sensitive controls for deterministic systems is already known, our equivalence result for MaxEnt and regularized risk-sensitive controls is nontrivial. This is because the regularized policy \(\pi_{t}^{*}\) makes a system stochastic even though the original system is deterministic, and for stochastic systems, the unregularized risk-sensitive control does not coincide with the risk-neutral control. This implies that the optimal randomness introduced by the regularization does not affect the risk sensitivity of the policy. This provides insight into the robustness of MaxEnt control [8]. Note that [43] mentioned that the MaxEnt control objective can be reconstructed by the risk-sensitive control objective under the heuristic assumption that the cost follows a uniform distribution. However, this assumption is not satisfied in general. Our equivalence result does not require such an unrealistic assumption.

**RCal and optimal posterior.** Although the optimal posterior \(p(u_{t}|x_{t},\mathcal{O}_{t:T})\) yields the MaxEnt control for deterministic systems as mentioned in Section 2, it is not known what objective \(p(u_{t}|x_{t},\mathcal{O}_{t:T})\)optimizes for stochastic systems. Theorem 3 gives a new characterization of \(p(u_{t}|x_{t},\mathcal{O}_{t:T})\). By formally substituting \(\eta=-1\) into (11), the Bellman equation for computing \(\pi_{t}^{*}\) becomes (4), (5) for the optimal posterior \(p(u_{t}|x_{t},\mathcal{O}_{t:T})\). Note that even if the cost function \(c_{t}\) in (9) is replaced by \(\mathsf{c}_{t}\) in Proposition 1, \(\{\pi_{t}^{*}\}\) is still optimal. Therefore, by taking the limit as \(\eta\searrow-1\), the policy \(\pi_{t}^{*}(u_{t}|x_{t})\) in Theorem 3 converges to \(p(u_{t}|x_{t},\mathcal{O}_{t:T})\), and in this sense, the policy \(p(u_{t}|x_{t},\mathcal{O}_{t:T})\) is risk-_seeking_.

**Corollary 4**.: _Under the assumptions in Proposition 1, it holds that_

\[\lim_{\eta\searrow-1}\pi_{t}^{*}(u_{t}|x_{t})=\exp(-\mathcal{Q}_{t}(x_{t},u_{t })+V_{t}(x_{t}))=p(u_{t}|x_{t},\mathcal{O}_{t:T}=1),\] (15)

_where \(V_{t}\) and \(\mathcal{Q}_{t}\) are given by (11), (13) with \(\eta=-1\). \(\diamondsuit\)_

**RCal for deterministic systems and linearly-solvable control.** For deterministic systems, by the transformation \(E_{t}(x_{t}):=\exp(-V_{t}(x_{t}))\), the Bellman equation (14) becomes linear: \(E_{t}(x_{t})=\int\exp(-c_{t}(x_{t},u^{\prime}))E_{t+1}(\bar{f}_{t}(x_{t},u^{ \prime}))\mathrm{d}u^{\prime}\). That is, when the system is deterministic, the LP-regularized risk-sensitive control, or equivalently, the MaxEnt control is linearly solvable [15; 16; 44], which enables efficient computation of RL. Even for the MaxEnt control, this fact seems not to be mentioned explicitly in the literature.

**RCal and unregularized risk-sensitive control in linear quadratic setting.** Similar to the unregularized and MaxEnt problems [45; 46], Problem (9) with a linear system \(p(x_{t+1}|x_{t},u_{t})=\mathcal{N}(x_{t+1}|A_{t}x_{t}+B_{t}u_{t},\Sigma_{t})\) and quadratic costs \(c_{t}(x_{t},u_{t})=(x_{t}^{\top}Q_{t}x_{t}+u_{t}^{\top}R_{t}u_{t})/2,\ c_{T}( x_{T})=x_{T}^{\top}Q_{T}x_{T}/2\) admits an explicit form of the optimal policy:

\[\pi_{t}^{*}(u|x)=\mathcal{N}\Big{(}u|-(R_{t}+B_{t}^{\top}\Pi_{t+ 1}(I-\eta\Sigma_{t}\Pi_{t+1})^{-1}B_{t})^{-1}B_{t}^{\top}\Pi_{t+1}(I-\eta \Sigma_{t}\Pi_{t+1})^{-1}A_{t}x,\] \[(R_{t}+B_{t}\Pi_{t+1}(I-\eta\Sigma_{t}\Pi_{t+1})^{-1}B_{t})^{-1} \Big{)}.\] (16)

Here, \(\mathcal{N}(\cdot|\mu,\Sigma)\) denotes the Gaussian density with mean \(\mu\) and covariance \(\Sigma\). The definition of \(\Pi_{t}\) and the proof are given in Appendix C. In general, the mean of the regularized risk-sensitive control deviates from the unregularized risk-sensitive control. However, in the linear quadratic Gaussian (LQG) case, the mean of the optimal policy (16) coincides with the optimal control of risk-sensitive LQG control without the regularization [47].

### Another risk-sensitive generalization of MaxEnt control via Renyi entropy

The Shannon entropy regularization \(\mathbb{E}[-\mathcal{H}_{1}(\pi_{t}(\cdot|x_{t}))]\) of the MaxEnt control problem (7) can be rewritten as \(\mathbb{E}[\log\pi_{t}(u_{t}|x_{t})]\). In this sense, the risk-sensitive control (9) is a natural extension of (7). Nevertheless, for the risk-sensitive case, the interpretation of \(\log\pi_{t}(u_{t}|x_{t})\) as entropy is no longer available. In this subsection, we provide another risk-sensitive extension of the MaxEnt control. Inspired by the Renyi divergence utilized so far, we employ Renyi entropy regularization:

\[\operatorname*{minimize}_{\{\pi_{t}\}_{t=0}^{T-1}}\ \frac{1}{\eta}\log\mathbb{E}_{ \mathfrak{p}^{\pi}(\tau)}\left[\exp\left(\eta c_{T}(x_{T})+\eta\sum_{t=0}^{T- 1}\Bigl{(}c_{t}(x_{t},u_{t})-\mathcal{H}_{1-\eta}(\pi_{t}(\cdot|x_{t}))\Bigr{)} \right)\right],\] (17)

where \(\eta\in\mathbb{R}\setminus\{0,1\}\), and \(\pi_{t}(\cdot|x)\in L^{1-\eta}(\mathbb{U}):=\{\rho\in\mathcal{P}(\mathbb{U})| \int_{\mathbb{U}}\rho(u)^{1-\eta}\mathrm{d}u<\infty\},\forall x\), which implies \(|\mathcal{H}_{1-\eta}(\pi_{t}(\cdot|x_{t}))|<\infty\). As \(\eta\) tends to zero, (17) converges to the MaxEnt control problem (7).

Define the value function \(\mathcal{V}_{t}\) and the Q-function \(\mathscr{Q}_{t}\) associated with (17) like (10) and (11). Then, as in Subsection 3.2, the following Bellman equation holds. The derivation is given in Appendix E.

\[\mathcal{V}_{t}(x_{t})=\inf_{\pi_{t}\in L^{1-\eta}(\mathbb{U})}\left\{\frac{1}{ \eta}\log\left[\int_{\mathbb{U}}\pi_{t}(u^{\prime}|x_{t})\exp(\eta\mathscr{Q} _{t}(x_{t},u^{\prime}))\mathrm{d}u^{\prime}\right]-\mathcal{H}_{1-\eta}(\pi_{ t}(\cdot|x_{t}))\right\}.\] (18)

For the minimization in (18), we establish the duality between exponential integrals and Renyi entropy like in [40] because the same procedure as for (12) cannot be applied.

**Lemma 5** (Informal).: _For \(\beta,\gamma\in\mathbb{R}\setminus\{0\}\) such that \(\beta<\gamma\) and for \(g:\mathbb{U}\to\mathbb{R}\), it holds that_

\[\frac{1}{\beta}\log\left[\int_{\mathbb{U}}\exp(\beta g(u))\mathrm{d}u\right]= \inf_{\rho\in L^{1-\frac{\gamma}{\gamma-\beta}}(\mathbb{U})}\left\{\frac{1}{ \gamma}\log\left[\int_{\mathbb{U}}\exp(\gamma g(u))\rho(u)\mathrm{d}u\right]- \frac{1}{\gamma-\beta}\mathcal{H}_{1-\frac{\gamma}{\gamma-\beta}}(\rho)\right\},\] (19)_and the unique optimal solution that minimizes the right-hand side of (19) is given by_

\[\rho(u)=\frac{\exp\left(-(\gamma-\beta)g(u)\right)}{\int_{\mathbb{U}} \exp(-(\gamma-\beta)g(u^{\prime}))\mathrm{d}u^{\prime}},\ \ \forall u\in\mathbb{U}.\] (20)

\(\diamond\)

For the precise statement and the proof, see Appendix D. By applying Lemma 5 with \(\beta=\eta-1\), \(\gamma=\eta\) to (18), we obtain the optimal policy of (17) as follows.

**Theorem 6**.: _Assume that \(c_{t}\) is bounded below for any \(t\in\llbracket 0,T\rrbracket\). Assume further that for any \(x\in\mathbb{X}\) and \(t\in\llbracket 0,T-1\rrbracket\), it holds that \(\int_{\mathbb{U}}\exp\left(-\mathscr{Q}_{t}(x,u^{\prime})\right)\mathrm{d}u^ {\prime}<\infty,\ \int_{\mathbb{U}}\exp\left(-(1-\eta)\mathscr{Q}_{t}(x,u^{\prime}) \right)\mathrm{d}u^{\prime}<\infty.\) Then, the unique optimal policy of Problem (17) is given by_

\[\pi_{t}^{\star}(u_{t}|x_{t})=\frac{1}{\mathscr{Z}(x_{t})}\exp\left(-\mathscr{ Q}_{t}(x_{t},u_{t})\right),\ \ \forall t\in\llbracket 0,T-1\rrbracket,\ \forall x_{t}\in \mathbb{X},\ \forall u_{t}\in\mathbb{U},\] (21)

_where \(\mathscr{Z}_{t}(x_{t}):=\int_{\mathbb{U}}\exp(-\mathscr{Q}_{t}(x_{t},u^{\prime }))\mathrm{d}u^{\prime}\), and it holds that_

\[\mathscr{V}_{t}(x_{t})=\frac{-1}{1-\eta}\log\left[\int_{\mathbb{U}}\exp\left(- (1-\eta)\mathscr{Q}_{t}(x_{t},u^{\prime})\right)\mathrm{d}u^{\prime}\right], \ \ \forall t\in\llbracket 0,T-1\rrbracket,\ \forall x_{t}\in\mathbb{X}.\] (22)

\(\diamond\)

Recall that the LP regularized risk-sensitive optimal control is given by (11), (13) while the Renyi entropy regularized control is determined by (21), (22), and \(\mathscr{Q}_{t}(x_{t},u_{t})=c_{t}(x_{t},u_{t})+\frac{1}{\eta}\log\mathbb{E}_ {p(x_{t+1}|x_{t},u_{t})}[\exp(\eta\mathscr{V}_{t+1}(x_{t+1}))]\). Hence, the only difference between the risk-sensitive controls for the LP and Renyi regularization is the coefficient in the soft Bellman equations (13), (22).

## 4 Risk-sensitive reinforcement learning via RCaI

Standard RL methods can be derived from CaI using the KL divergence [5]. In this section, we derive risk-sensitive policy gradient and soft actor-critic methods from RCaI.

### Risk-sensitive policy gradient

In this subsection, we consider minimizing the cost (9) by a time-invariant policy parameterized as \(\pi_{t}(u|x)=\pi^{(\theta)}(u|x)\), \(\theta\in\mathbb{R}^{n_{\theta}}\). Let \(C_{\theta}(\tau):=c_{T}(x_{T})+\sum_{t=0}^{T-1}(c_{t}(x_{t},u_{t})+\log\pi^{( \theta)}(u_{t}|x_{t}))\) and \(p_{\theta}\) be the density of the trajectory \(\tau\) under the policy \(\pi^{(\theta)}\). Then, Problem (9) can be reformulated as the minimization of \(J(\theta)/\eta\) where \(J(\theta):=\int p_{\theta}(\tau)\exp(\eta C_{\theta}(\tau))\mathrm{d}\tau\). To optimize \(J(\theta)/\eta\) by gradient descent, we give the gradient \(\nabla_{\theta}J(\theta)\). The proof is shown in Appendix F.

**Proposition 7**.: _Assume the existence of densities \(p(x_{t+1}|x_{t},u_{t})\), \(p(x_{0})\). Assume further that \(\pi^{(\theta)}\) is differentiable in \(\theta\), and the derivative and the integral can be interchanged as \(\nabla_{\theta}J(\theta)=\int\nabla_{\theta}[p_{\theta}(\tau)\exp(\eta C_{ \theta}(\tau))]\mathrm{d}\tau\). Then, for any function \(b:\mathbb{R}^{n_{x}}\to\mathbb{R}\), it holds that_

\[\nabla_{\theta}J(\theta)=(\eta+1)\mathbb{E}_{p_{\theta}(\tau)} \Bigg{[}\sum_{t=0}^{T-1}\nabla_{\theta}\log\pi^{(\theta)}(u_{t}|x_{t})\\ \times\Bigg{\{}\exp\left(\eta c_{T}(x_{T})+\eta\sum_{s=t}^{T-1} \Bigl{(}c_{s}(x_{s},u_{s})+\log\pi^{(\theta)}(u_{s}|x_{s})\Bigr{)}\right)-b(x_{ t})\Bigg{\}}\Bigg{]}.\] (23)

\(\diamond\)

The function \(b\) is referred to as a baseline function, which can be used for reducing the variance of an estimate of \(\nabla_{\theta}J\). The following gradient estimate of \(J(\theta)/\eta\) is unbiased:

\[\frac{\eta+1}{\eta}\sum_{t=0}^{T-1}\nabla_{\theta}\log\pi^{(\theta)}(u_{t}|x_{t })\bigg{\{}\exp\biggl{(}\eta c_{T}(x_{T})+\eta\sum_{s=t}^{T-1}\Bigl{(}c_{s}(x_{s },u_{s})+\log\pi^{(\theta)}(u_{s}|x_{s})\Bigr{)}\biggr{)}-b(x_{t})\bigg{\}}.\]

This is almost the same as risk-sensitive REINFORCE [19] except for the additional term \(\log\pi^{(\theta)}(u_{s}|x_{s})\). In the risk-neutral limit \(\eta\to 0\), this estimator converges to the MaxEnt policy gradient estimator [5].

### Risk-sensitive soft actor-critic

In Subsection 3.2, we used dynamic programming to obtain the optimal policy \(\{\pi_{t}^{*}\}\). Rather, in this section, we adopt a standard procedure of variational inference [48]. First, we find the optimal factor \(\pi_{t}\) for fixed \(\pi_{s}\), \(s\neq t\) as follows. The proof is deferred to Appendix G.

**Proposition 8**.: _For \(t\in[\![0,T-1]\!]\), let \(\pi_{s},s\neq t\) be fixed. Let \(\eta>-1\), \(\eta\neq 0\). Then, the optimal factor \(\pi_{t}^{\bullet}:=\arg\min_{\pi_{t}\in\mathcal{P}(\mathbb{U})}D_{1+\eta}(p^{ \pi}\|p(\cdot|\mathcal{O}_{0:T}=1))\) is given by_

\[\pi_{t}^{\bullet}(u_{t}|x_{t})=\frac{1}{Z_{t}(x_{t})}\left(\mathbb{E}_{p^{\pi} (x_{t+1:T},u_{t+1:T-1}|x_{t},u_{t})}\left[\left(\frac{\prod_{s=t+1}^{T-1}\pi_{ s}(u_{s}|x_{s})}{p(\mathcal{O}_{t}|x_{T})\prod_{s=t}^{T-1}p(\mathcal{O}_{s}|x_{s}, u_{s})}\right)^{\eta}\right]\right)^{-1/\eta},\] (24)

_where \(Z_{t}(x_{t})\) is the normalizing constant. \(\diamondsuit\)_

By (24), the optimal factor \(\pi_{t}^{\bullet}\) is independent of the past factors \(\pi_{s}\), \(s\in[\![0,t-1]\!]\). Therefore, the variational Renyi bound in (8) is maximized by optimizing \(\pi_{t}\) in backward order from \(t=T-1\) to \(t=0\), which is consistent with the dynamic programming. Associated with (24), we define

\[V_{t}^{\pi}(x_{t}):=\frac{1}{\eta}\log\mathbb{E}_{p^{\pi}(x_{t+1 :T},u_{t:T-1}|x_{t})}\left[\left(\frac{\prod_{s=t}^{T-1}\pi_{s}(u_{s}|x_{s})}{ p(\mathcal{O}_{t}|x_{T})\prod_{s=t}^{T-1}p(\mathcal{O}_{s}|x_{s},u_{s})} \right)^{\eta}\right]\\ =\frac{1}{\eta}\log\mathbb{E}_{p^{\pi}(x_{t+1:T},u_{t:T-1}|x_{t} )}\left[\exp\left(\eta c_{T}(x_{T})+\eta\sum_{s=t}^{T-1}\bigl{(}c_{s}(x_{s},u_ {s})+\log\pi_{s}(u_{s}|x_{s})\bigr{)}\right)\right],\] (25)

which is the value function for the policy \(\{\pi_{s}\}_{s=t}^{T-1}\) satisfying the following Bellman equation.

\[V_{t}^{\pi}(x_{t}) =\frac{1}{\eta}\log\mathbb{E}_{\pi_{t}(u_{t}|x_{t})}\left[\left( \frac{\pi_{t}(u_{t}|x_{t})}{p(\mathcal{O}_{t}|x_{t},u_{t})}\right)^{\eta} \mathbb{E}_{p(x_{t+1}|x_{t},u_{t})}\left[\exp(\eta V_{t+1}^{\pi}(x_{t+1})) \right]\right]\] (26) \[=\frac{1}{\eta}\log\mathbb{E}_{\pi_{t}(u_{t}|x_{t})}\Bigl{[}\exp \left(\eta c_{t}(x_{t},u_{t})+\eta\log\pi_{t}(u_{t}|x_{t})\right)\mathbb{E}_{p (x_{t+1}|x_{t},u_{t})}\left[\exp(\eta V_{t+1}^{\pi}(x_{t+1}))\right]\Bigr{]}.\]

By the value function, \(\pi_{t}^{\bullet}(u_{t}|x_{t})\) can be written as

\[\pi_{t}^{\bullet}(u_{t}|x_{t}) =\frac{p(\mathcal{O}_{t}|x_{t},u_{t})}{Z_{t}(x_{t})}\mathbb{E}_{p (x_{t+1:T},u_{t+1:T-1}|x_{t},u_{t})}\left[\left(\frac{\prod_{s=t+1}^{T-1}\pi_{ s}(u_{s}|x_{s})}{p(\mathcal{O}_{t}|x_{T})\prod_{s=t+1}^{T-1}p(\mathcal{O}_{s}|x_{s}, u_{s})}\right)^{\eta}\right]^{-1/\eta}\] \[=\frac{p(\mathcal{O}_{t}|x_{t},u_{t})}{Z_{t}(x_{t})}\mathbb{E}_{p (x_{t+1}|x_{t},u_{t})}\left[\exp(\eta V_{t+1}^{\pi}(x_{t+1}))\right]^{-1/\eta}.\] (27)

Next, we define the Q-function for \(\{\pi_{s}\}_{s=t+1}^{T-1}\) as follows:

\[Q_{t}^{\pi}(x_{t},u_{t}):=-\log p(\mathcal{O}_{t}|x_{t},u_{t})+\frac{1}{\eta} \log\mathbb{E}_{p(x_{t+1}|x_{t},u_{t})}\left[\exp(\eta V_{t+1}^{\pi}(x_{t+1}) )\right].\] (28)

Then, it follows from (26) and (27) that

\[V_{t}^{\pi}(x_{t}) =\frac{1}{\eta}\log\mathbb{E}_{\pi_{t}(u_{t}|x_{t})}\left[\pi_{t} (u_{t}|x_{t})^{\eta}\exp(\eta Q_{t}^{\pi}(x_{t},u_{t}))\right],\] (29) \[\pi_{t}^{\bullet}(u_{t}|x_{t}) =\frac{1}{Z_{t}(x_{t})}\exp(-Q_{t}^{\pi}(x_{t},u_{t})),\ \ Z_{t}(x_{t})=\int_{\mathbb{U}}\exp\left(-Q_{t}^{\pi}(x_{t},u^{\prime}) \right)\mathrm{d}u^{\prime}.\] (30)

Especially when \(\pi_{t}(u_{t}|x_{t})=\pi_{t}^{\bullet}(u_{t}|x_{t})\), it holds that \(V_{t}^{\pi}(x_{t})=-\log\left[\int\exp(-Q_{t}^{\pi}(x_{t},u^{\prime}))\mathrm{d }u^{\prime}\right]\), which coincides with the soft Bellman equation in (13). In summary, in order to obtain the optimal factor \(\pi_{t}^{\bullet}\), it is sufficient to compute \(V_{t}^{\pi}\) and \(Q_{t}^{\pi}\) in a backward manner.

Next, we consider the situation when the policy is parameterized as \(\pi_{t}^{(\theta)}(u_{t}|x_{t})\), \(\theta\in\mathbb{R}^{n_{\theta}}\) and there is no parameter \(\theta\) that gives the optimal factor \(\pi_{t}^{(\theta)}=\pi_{t}^{\bullet}\). To accommodate this situation, we utilize the variational Renyi bound. One can easily see that the maximization of the Renyi bound in (8) with respect to a single factor \(\pi_{t}\) is equivalent to the following problem.

\[\operatorname*{minimize}_{\pi_{t}}\ \frac{1}{\eta}\log\mathbb{E}_{p^{\pi}(x_{t})} \left[\mathbb{E}_{\pi_{t}(u_{t}|x_{t})}\left[\pi_{t}(u_{t}|x_{t})^{\eta}\exp( \eta Q_{t}^{\pi}(x_{t},u_{t}))\right]\right].\] (31)This suggests choosing \(\theta\) that minimizes (31) whose \(\pi_{t}\) is replaced by \(\pi_{t}^{(\theta)}\). Note that this is further equivalent to

\[\operatorname*{minimize}_{\theta}\ \operatorname{\mathbb{E}}_{p^{\pi}(x_{t})} \left[D_{1+\eta}\left(\pi_{t}^{(\theta)}(\cdot|x_{t})\right\rVert\frac{\exp(-Q _{t}^{\pi}(x_{t},\cdot))}{Z_{t}(x_{t})}\right)\right].\] (32)

We also parameterize \(V_{t}^{\pi}\) and \(Q_{t}^{\pi}\) as \(V^{(\psi)}\), \(Q^{(\phi)}\) and optimize \(\psi,\phi\) so that the relations (28), (29) approximately hold. To obtain unbiased gradient estimators later, we minimize the following squared residual error based on (28), (29), and the transformation \(T_{\eta}(v):=(\mathrm{e}^{\eta v}-1)/\eta\), \(v\in\mathbb{R}\):

\[\mathcal{J}_{Q}(\phi) :=\mathbb{E}_{p^{\pi}(x_{t},u_{t})}\left[\frac{1}{2}\left\{T_{ \eta}\left(Q^{(\phi)}(x_{t},u_{t})-c(x_{t},u_{t})\right)-\mathbb{E}_{p(x_{t+1} |x_{t},u_{t})}\left[T_{\eta}(V^{(\psi)}(x_{t+1}))\right]\right\}^{2}\right],\] \[\mathcal{J}_{V}(\psi) :=\mathbb{E}_{p^{\pi}(x_{t})}\left[\frac{1}{2}\left\{T_{\eta}(V^ {(\psi)}(x_{t}))-\mathbb{E}_{\pi^{(\theta)}(u_{t}|x_{t})}\left[T_{\eta}\left( Q^{(\phi)}(x_{t},u_{t})+\log\pi^{(\theta)}(u_{t}|x_{t})\right)\right]\right\}^{2} \right].\]

Using \(Q^{(\phi)}\) and \(T_{\eta}\), we replace (31) with the following equivalent objective:

\[\mathcal{J}_{\pi}(\theta):=\mathbb{E}_{p^{\pi}(x_{t})}\big{[} \mathbb{E}_{\pi^{(\theta)}(u_{t}|x_{t})}\big{[}T_{\eta}\big{(}Q^{(\phi)}(x_{t },u_{t})+\log\pi^{(\theta)}(u_{t}|x_{t})\big{)}\big{]}\big{]}.\] (33)

Noting that \(\lim_{\eta\to 0}T_{\eta}(\kappa(\eta))=\kappa(0)\) for \(\kappa:\mathbb{R}\to\mathbb{R}\), as the risk sensitivity \(\eta\) goes to zero, the objectives \(\mathcal{J}_{Q},\mathcal{J}_{V},\mathcal{J}_{\pi}\) converge to those used for the risk-neutral soft actor-critic [7]. Now, we have

\[\nabla_{\phi}\mathcal{J}_{Q}(\phi) =\mathbb{E}_{p^{\pi}(x_{t},u_{t})}\Big{[}\big{(}\nabla_{\phi}Q^ {(\phi)}(x_{t},u_{t})\big{)}\exp\bigl{(}\eta Q^{(\phi)}(x_{t},u_{t})-\eta c(x_{ t},u_{t})\big{)}\] \[\times\big{\{}T_{\eta}\big{(}Q^{(\phi)}(x_{t},u_{t})-c(x_{t},u_{t })\big{)}-\mathbb{E}_{p(x_{t+1}|x_{t},u_{t})}\big{[}T_{\eta}(V^{(\psi)}(x_{t+ 1}))\big{]}\big{\}}\Big{]},\] (34) \[\nabla_{\psi}\mathcal{J}_{V}(\psi) =\mathbb{E}_{p^{\pi}(x_{t})}\Big{[}\big{(}\nabla_{\psi}V^{(\psi) }(x_{t})\big{)}\exp(\eta V^{(\psi)}(x_{t}))\] \[\times\big{\{}T_{\eta}(V^{(\psi)}(x_{t}))-\mathbb{E}_{\pi^{( \theta)}(u_{t}|x_{t})}\big{[}T_{\eta}\big{(}Q^{(\phi)}(x_{t},u_{t})+\log\pi^{ (\theta)}(u_{t}|x_{t})\big{)}\big{]}\big{\}}\Big{]},\] (35) \[\nabla_{\theta}\mathcal{J}_{\pi}(\theta) =(\eta+1)\mathbb{E}_{p^{\pi}(x_{t},u_{t})}\Big{[}\big{(}\nabla_{ \theta}\log\pi^{(\theta)}(u_{t}|x_{t})\big{)}T_{\eta}\big{(}Q^{(\phi)}(x_{t},u _{t})+\log\pi^{(\theta)}(u_{t}|x_{t})\big{)}\Big{]}.\] (36)

Thanks to the transformation \(T_{\eta}\), the expectations appear linearly, and an unbiased gradient estimator can be obtained by removing them. By simply replacing the gradients of the soft actor-critic [7] with (34)-(36), we obtain the risk-sensitive soft actor-critic (RSAC). It is worth mentioning that since RSAC requires only minor modifications to SAC, techniques for stabilizing SAC, e.g., reparameterization, minibatch sampling with a replay buffer, target networks, double Q-network, can be directly used for RSAC.

## 5 Experiment

Unregularized risk-averse control is known to be robust against perturbations in systems [32]. Since the robustness of the regularized cases has not yet been established theoretically, we verify the robustness of policies learned by RSAC through a numerical example. The environment is Pendulum-v1 in OpenAI Gymnasium. We trained control policies using the hyperparameters shown in Appendix H. There were no significant differences in the control performance obtained or the behavior during training. On the other hand, for each \(\eta\), one control policy was selected and was applied to a slightly different environment _without retraining_. To be more precise, the pendulum length \(l\), which is 1.0 during training, is changed to 1.25 and 1.5; See Fig. 3. In this example, it can be seen that the control policy obtained with larger \(\eta\) has a smaller performance degradation due to environmental changes. This robustness can be considered a benefit of risk-sensitive control.

In Fig. 4, empirical distributions of the costs for different risk-sensitivity parameters \(\eta\) are plotted. Only the distribution for \(\eta=0.02\) does not change so much under the system perturbations. The

Figure 3: Average episode cost for RSAC with some \(\eta\) and standard SAC.

distribution for SAC (\(\eta=0\)) with \(l=1.5\) deviates from the original one (\(l=1.0\)), and another peak of the distribution appears in the high-cost area. This means that there is a high probability of incurring a high cost, which clarifies the advantage of RSAC. The more risk-seeking the policy becomes, the less robust it becomes against the system perturbation.

## 6 Conclusions

In this paper, we proposed a unifying framework of CaI, named RCaI, using Renyi divergence variational inference. We revealed that RCaI yields the LP regularized risk-sensitive control with exponential performance criteria. Moreover, we showed the equivalences for risk-sensitive control, MaxEnt control, the optimal posterior for CaI, and linearly-solvable control. In addition to these connections, we derived the policy gradient method and the soft actor-critic method for the risk-sensitive RL via RCaI. Interestingly, Renyi entropy regularization also results in the same form of the risk-sensitive optimal policy and the soft Bellman equation as the LP regularization.

From a practical point of view, a major limitation of the proposed risk-sensitive soft actor-critic is its numerical instability for large \(|\eta|\) cases. Since \(\eta\) appears, for example, as \(\exp(\eta Q^{(\phi)}(x_{t},u_{t}))\) in the gradients (34)-(36), the magnitude of \(\eta\) that does not cause the numerical instability depends on the scale of costs. Therefore, we need to choose \(\eta\) depending on environments. In the experiment using Pendulum-v1, \(|\eta|\) that is larger than \(0.03\) results in the failure of learning due to the numerical instability. Although it is an important future work to address this issue, we would like to note that this issue is not specific to our algorithms, but occurs in general risk-sensitive RL with exponential utility. It is also important how to choose a specific value of the order parameter \(1+\eta\) of Renyi divergence. Since we showed that \(\eta\) determines the risk sensitivity of the optimal policy, we can follow previous studies on the choice of the sensitivity parameter of the risk-sensitive control without regularization. The properties of the derived algorithms also need to be explored in future work, e.g., the compatibility of a function approximator for RSAC [49].

## Acknowledgments

The authors thank Ran Wang for his valuable help in conducting the experiment. This work was supported in part by JSPS KAKENHI Grant Numbers JP23K19117, JP24K17297, JP21H04875.

## References

* [1] Onesimo Hernandez-Lerma and Jean B. Lasserre, _Discrete-time Markov Control Processes: Basic Optimality Criteria_, vol. 30, Springer-Verlag New York, 1996.
* [2] Richard S. Sutton and Andrew G. Barto, _Reinforcement Learning: An Introduction_, MIT Press, second edition, 2018.
* [3] Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine, "Learning to walk via deep reinforcement learning", in _Robotics: Science and Systems_, 2019.

Figure 4: Empirical distributions of the costs for different risk-sensitivity parameters \(\eta\).

* Kiran et al. [2022] B. Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A. Al Sallab, Senthil Yogamani, and Patrick Perez, "Deep reinforcement learning for autonomous driving: A survey", _IEEE Transactions on Intelligent Transportation Systems_, vol. 23, no. 6, pp. 4909-4926, 2022.
* Levine [2018] Sergey Levine, "Reinforcement learning and control as probabilistic inference: Tutorial and review", _arXiv preprint arXiv:1805.00909_, 2018.
* Ziebart [2010] Brian D. Ziebart, _Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy_, PhD thesis, Carnegie Mellon University, 2010.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine, "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", in _International Conference on Machine Learning_. PMLR, 2018, pp. 1861-1870.
* Eysenbach and Levine [2022] Benjamin Eysenbach and Sergey Levine, "Maximum entropy RL (provably) solves some robust RL problems", in _International Conference on Learning Representations_, 2022.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine, "Soft actor-critic algorithms and applications", _arXiv preprint arXiv:1812.05905_, 2018.
* Mei et al. [2020] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans, "On the global convergence rates of softmax policy gradient methods", in _International Conference on Machine Learning_. PMLR, 2020, vol. 119, pp. 6820-6829.
* Li and Turner [2016] Yingzhen Li and Richard E. Turner, "Renyi divergence variational inference", in _Advances in Neural Information Processing Systems_, 2016, vol. 29, pp. 1073-1081.
* Renyi [1961] Alfred Renyi, "On measures of entropy and information", in _Proceedings of the fourth Berkeley Symposium on Mathematical Statistics and Probability_, 1961, vol. 1, pp. 547-561.
* Zhang et al. [2019] Cheng Zhang, Judith Butepage, Hedvig Kjellstrom, and Stephan Mandt, "Advances in variational inference", _IEEE Transactions on Pattern Analysis and Machine Intelligence_, vol. 41, no. 8, pp. 2008-2026, 2019.
* Whittle [1990] Peter Whittle, _Risk-Sensitive Optimal Control_, John Wiley & Sons, Ltd., 1990.
* Todorov [2006] Emanuel Todorov, "Linearly-solvable Markov decision problems", in _Advances in Neural Information Processing Systems_, 2006, vol. 19, pp. 1369-1376.
* Dvijotham and Todorov [2011] Krishnamurthy Dvijotham and Emanuel Todorov, "A unifying framework for linearly solvable control", in _27th Conference on Uncertainty in Artificial Intelligence_, 2011, pp. 179-186.
* Williams [1992] Ronald J. Williams, "Simple statistical gradient-following algorithms for connectionist reinforcement learning", _Machine Learning_, vol. 8, pp. 229-256, 1992.
* Nass et al. [2019] David Nass, Boris Belousov, and Jan Peters, "Entropic risk measure in policy search", in _2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_. IEEE, 2019, pp. 1101-1106.
* Noorani and Baras [2021] Erfaun Noorani and John S. Baras, "Risk-sensitive REINFORCE: A Monte Carlo policy gradient algorithm for exponential performance criteria", in _2021 60th IEEE Conference on Decision and Control (CDC)_. IEEE, 2021, pp. 1522-1527.
* Duan et al. [2022] Jingliang Duan, Yang Guan, Shengbo Eben Li, Yangang Ren, Qi Sun, and Bo Cheng, "Distributional soft actor-critic: Off-policy reinforcement learning for addressing value estimation errors", _IEEE Transactions on Neural Networks and Learning Systems_, vol. 33, no. 11, pp. 6584-6598, 2022.
* Choi et al. [2021] Jinyoung Choi, Christopher Dance, Jung-Eun Kim, Seulbin Hwang, and Kyung-sik Park, "Risk-conditioned distributional soft actor-critic for risk-sensitive navigation", in _2021 IEEE International Conference on Robotics and Automation (ICRA)_. IEEE, 2021, pp. 8337-8344.
* Kappen [2005] Hilbert J. Kappen, "Path integrals and symmetry breaking for optimal control theory", _Journal of Statistical Mechanics: Theory and Experiment_, vol. 2005, no. 11, pp. P11011, 2005.

* [23] Emanuel Todorov, "General duality between optimal control and estimation", in _2008 47th IEEE Conference on Decision and Control_. IEEE, 2008, pp. 4286-4292.
* [24] Hilbert J. Kappen, Vicenc Gomez, and Manfred Opper, "Optimal control as a graphical model inference problem", _Machine Learning_, vol. 87, pp. 159-182, 2012.
* [25] Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar, "On stochastic optimal control and reinforcement learning by approximate inference", in _Proceedings of Robotics: Science and Systems_, 2012.
* [26] Marc Toussaint, "Robot trajectory optimization using approximate inference", in _International Conference on Machine Learning_, 2009, pp. 1049-1056.
* [27] Masashi Okada and Tadahiro Taniguchi, "Variational inference MPC for Bayesian model-based reinforcement learning", in _Conference on Robot Learning_. PMLR, 2020, pp. 258-272.
* [28] Alexander Lambert, Fabio Ramos, Byron Boots, Dieter Fox, and Adam Fishman, "Stein variational model predictive control", in _Conference on Robot Learning_. PMLR, 2021, vol. 155, pp. 1278-1297.
* [29] Ziyi Wang, Oswin So, Jason Gibson, Bogdan Vlahov, Manan S. Gandhi, Guan-Horng Liu, and Evangelos A. Theodorou, "Variational inference MPC using Tsallis divergence", in _Robotics: Science and Systems_, 2021.
* [30] Yinlam Chow, Brandon Cui, MoonKyung Ryu, and Mohammad Ghavamzadeh, "Variational model-based policy optimization", _arXiv preprint arXiv:2006.05443_, 2020.
* [31] Marco C. Campi and Matthew R. James, "Nonlinear discrete-time risk-sensitive optimal control", _International Journal of Robust and Nonlinear Control_, vol. 6, no. 1, pp. 1-19, 1996.
* [32] Ian R Petersen, Matthew R James, and Paul Dupuis, "Minimax optimal control of stochastic uncertain systems with relative entropy constraints", _IEEE Transactions on Automatic Control_, vol. 45, no. 3, pp. 398-412, 2000.
* [33] Brendan O'Donoghue, "Variational Bayesian reinforcement learning with regret bounds", in _Advances in Neural Information Processing Systems_, 2021, vol. 34, pp. 28208-28221.
* [34] Vivek S. Borkar, "Q-learning for risk-sensitive control", _Mathematics of Operations Research_, vol. 27, no. 2, pp. 294-311, 2002.
* [35] Yingjie Fei, Zhuoran Yang, Yudong Chen, and Zhaoran Wang, "Exponential Bellman equation and improved regret bounds for risk-sensitive reinforcement learning", in _Advances in Neural Information Processing Systems_, 2021, vol. 34, pp. 20436-20446.
* [36] Javier Garcia and Fernando Fernandez, "A comprehensive survey on safe reinforcement learning", _Journal of Machine Learning Research_, vol. 16, no. 1, pp. 1437-1480, 2015.
* [37] Tobias Enders, James Harrison, and Maximilian Schiffer, "Risk-sensitive soft actor-critic for robust deep reinforcement learning under distribution shifts", _arXiv preprint arXiv:2402.09992_, 2024.
* [38] Kaito Ito and Kenji Kashima, "Kullback-Leibler control for discrete-time nonlinear systems on continuous spaces", _SICE Journal of Control, Measurement, and System Integration_, vol. 15, no. 2, pp. 119-129, 2022.
* [39] Friedrich Liese and Igor Vajda, _Convex Statistical Distances_, Teubner, Leipzig, 1987.
* [40] Rami Atar, Kenny Chowdhary, and Paul Dupuis, "Robust bounds on risk-sensitive functionals via Renyi divergence", _SIAM/ASA Journal on Uncertainty Quantification_, vol. 3, no. 1, pp. 18-33, 2015.
* [41] Tim Van Erven and Peter Harremos, "Renyi divergence and Kullback-Leibler divergence", _IEEE Transactions on Information Theory_, vol. 60, no. 7, pp. 3797-3820, 2014.

* [42] Oliver Mihatsch and Ralph Neuneier, "Risk-sensitive reinforcement learning", _Machine Learning_, vol. 49, pp. 267-290, 2002.
* [43] Erfau Noorani, Christos Mavridis, and John Baras, "Risk-sensitive reinforcement learning with exponential criteria", _arXiv preprint arXiv:2212.09010_, 2023.
* [44] Krishnamurthy Dvijotham and Emanuel Todorov, "Inverse optimal control with linearly-solvable MDPs", in _Proceedings of the 27th International Conference on Machine Learning_, 2010, pp. 335-342.
* [45] Kaito Ito and Kenji Kashima, "Maximum entropy optimal density control of discrete-time linear systems and Schrodinger bridges", _IEEE Transactions on Automatic Control_, vol. 69, no. 3, pp. 1536-1551, 2023.
* [46] Kaito Ito and Kenji Kashima, "Maximum entropy density control of discrete-time linear systems with quadratic cost", To appear in _IEEE Transactions on Automatic Control_, 2025, arXiv preprint arXiv:2309.10662.
* [47] Peter Whittle, "Risk-sensitive linear/quadratic/Gaussian control", _Advances in Applied Probability_, vol. 13, no. 4, pp. 764-777, 1981.
* [48] Christopher M. Bishop, _Pattern Recognition and Machine Learning_, Springer, 2006.
* [49] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour, "Policy gradient methods for reinforcement learning with function approximation", in _Advances in Neural Information Processing Systems_, 1999, vol. 12, pp. 1057-1063.
* [50] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann, "Stable-baselines3: Reliable reinforcement learning implementations", _Journal of Machine Learning Research_, vol. 22, no. 268, pp. 1-8, 2021.
* [51] Diederik P. Kingma and Jimmy Ba, "Adam: A method for stochastic optimization", _arXiv preprint arXiv:1412.6980_, 2014.

More details on Control as Inference

In this appendix, we give more details on CaI. As mentioned in (1), the distribution of the state and control input trajectory given optimality variables satisfies

\[p(\tau|\mathcal{O}_{0:T}) \propto p(\tau,\mathcal{O}_{0:T})\] \[=\left[p(\mathcal{O}_{T}|x_{T})\prod_{t=0}^{T-1}p(\mathcal{O}_{t} |x_{t},u_{t})\right]\left[p(x_{0})\prod_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t})p(u_{ t})\right],\]

where \(p(u_{t})=1/\mu_{L}(\mathbb{U})\) and \(p(\tau,\mathcal{O}_{0:T})\) is defined so that

\[\mathbb{P}(\tau\in\mathcal{B},\ \mathcal{O}_{0:T}=\mathbf{o}_{0:T})=\int_{ \mathcal{B}}p(\tau,\mathbf{o}_{0:T})\mathrm{d}\tau\]

for any \(\mathbf{o}_{0:T}\in\{0,1\}^{T+1}\) and any Borel set \(\mathcal{B}\), where \(\mathbb{P}\) denotes the probability. Therefore, we have

\[p(\tau|\mathcal{O}_{0:T}=1)\propto\left[p(x_{0})\prod_{t=0}^{T-1}p(x_{t+1}|x_{ t},u_{t})\right]\exp\left(-c_{T}(x_{T})-\sum_{t=0}^{T-1}c_{t}(x_{t},u_{t}) \right).\]

The posterior \(p(u_{t}|x_{t},\mathcal{O}_{t:T}=1)\) given the optimality condition \(\mathcal{O}_{t:T}=1\) is called the optimal policy. We emphasize that the optimality of \(p(u_{t}|x_{t},\mathcal{O}_{t:T}=1)\) is defined by the condition \(\mathcal{O}_{t:T}=1\) rather than by introducing a cost functional, unlike \(\pi^{*}(u_{t}|x_{t})\) in (13). In the following, we drop \(=1\) for \(\mathcal{O}_{t}\).

The optimal policy can be computed as follows. Define

\[\beta_{t}(x_{t},u_{t}) :=p(\mathcal{O}_{t:T}|x_{t},u_{t}),\] (37) \[\zeta_{t}(x_{t}) :=p(\mathcal{O}_{t:T}|x_{t}).\] (38)

Then, it holds that

\[\zeta_{t}(x_{t})=\int_{\mathbb{U}}p(\mathcal{O}_{t:T}|x_{t},u_{t})p(u_{t}|x_{ t})\mathrm{d}u_{t}=\int_{\mathbb{U}}\beta_{t}(x_{t},u_{t})p(u_{t})\mathrm{d}u_{t}= \frac{1}{\mu_{L}(\mathbb{U})}\int_{\mathbb{U}}\beta_{t}(x_{t},u_{t})\mathrm{d }u_{t}.\] (39)

In addition, we have

\[\beta_{t}(x_{t},u_{t}) =p(\mathcal{O}_{t:T}|x_{t},u_{t})=p(\mathcal{O}_{t}|x_{t},u_{t}) p(\mathcal{O}_{t+1:T}|x_{t},u_{t})\] \[=p(\mathcal{O}_{t}|x_{t},u_{t})\int_{\mathbb{X}}p(\mathcal{O}_{t+ 1:T}|x_{t+1})p(x_{t+1}|x_{t},u_{t})\mathrm{d}x_{t+1}\] \[=p(\mathcal{O}_{t}|x_{t},u_{t})\int_{\mathbb{X}}\zeta_{t+1}(x_{t+1 })p(x_{t+1}|x_{t},u_{t})\mathrm{d}x_{t+1},\] (40) \[\zeta_{T}(x_{T}) =p(\mathcal{O}_{T}|x_{T})=\exp(-c_{T}(x_{T})),\]

where we used

\[p(\mathcal{O}_{t+1:T}|x_{t},u_{t}) =\int_{\mathbb{X}}p(\mathcal{O}_{t+1:T},x_{t+1}|x_{t},u_{t}) \mathrm{d}x_{t+1}\] \[=\int_{\mathbb{X}}p(\mathcal{O}_{t+1:T}|x_{t+1},x_{t},u_{t})p(x_{ t+1}|x_{t},u_{t})\mathrm{d}x_{t+1}\] \[=\int_{\mathbb{X}}p(\mathcal{O}_{t+1:T}|x_{t+1})p(x_{t+1}|x_{t},u_ {t})\mathrm{d}x_{t+1}.\]

In terms of \(\beta_{t}\) and \(\zeta_{t}\), the optimal policy can be written as

\[p(u_{t}|x_{t},\mathcal{O}_{t:T}) =\frac{p(x_{t},u_{t},\mathcal{O}_{t:T})}{p(x_{t},\mathcal{O}_{t:T })}\] \[=\frac{p(\mathcal{O}_{t:T}|x_{t},u_{t})}{p(\mathcal{O}_{t:T}|x_{t} )}p(u_{t}|x_{t})\] \[=\frac{\beta_{t}(x_{t},u_{t})}{\mu_{L}(\mathbb{U})\zeta_{t}(x_{t} )}.\] (41)

[MISSING_PAGE_FAIL:15]

First, by definition, we have

\[V_{t}(x)=\inf_{\{\pi_{s}\}_{s=t}^{T-1}}\frac{1}{\eta}\log \Bigg{[}\!\int_{\mathbb{U}}\pi_{t}(u|x)\mathbb{E}\!\left[\exp\!\left( \eta c_{t}(x,u)+\varepsilon\eta\log\pi_{t}(u|x)+\eta c_{T}(x_{T})\right.\right.\] \[\left.\left.\qquad\qquad\qquad+\eta\sum_{s=t+1}^{T-1}\Big{(}c_{s }(x_{s},u_{s})+\varepsilon\log\pi_{s}(u_{s}|x_{s})\Big{)}\right)\bigg{|}\;x_{t }=x,u_{t}=u\Bigg{]}\mathrm{d}u\Bigg{]}\] \[=\inf_{\{\pi_{s}\}_{s=t}^{T-1}}\frac{1}{\eta}\log\!\left[\int_{ \mathbb{U}}\pi_{t}(u|x)\exp\!\left(\eta c_{t}(x,u)+\varepsilon\eta\log\pi_{t} (u|x)\right)\right.\] \[\left.\qquad\times\mathbb{E}\!\left[\exp\!\left(\eta c_{T}(x_{T })+\eta\sum_{s=t+1}^{T-1}\!\Big{(}c_{s}(x_{s},u_{s})+\varepsilon\log\pi_{s}( u_{s}|x_{s})\Big{)}\right)\bigg{|}\;x_{t}=x,u_{t}=u\right]\mathrm{d}u\right]\] \[=\inf_{\pi_{t}}\frac{1}{\eta}\log\left[\int_{\mathbb{U}}\pi_{t} (u|x)\exp\!\left(\eta c_{t}(x,u)+\varepsilon\eta\log\pi_{t}(u|x)\right)\! \mathbb{E}\left[\exp\!\left(\eta V_{t+1}(f_{t}(x,u,w_{t}))\right)\right] \mathrm{d}u\right].\]

By the definition of the Q-function (52), we get

\[V_{t}(x) =\inf_{\pi_{t}(\cdot|x)\in\mathcal{P}(\mathbb{U})}\frac{1}{\eta} \log\left[\int_{\mathbb{U}}\pi_{t}(u|x)\exp(\varepsilon\eta\log\pi_{t}(u|x)) \exp(\eta\mathcal{Q}_{t}(x,u))\mathrm{d}u\right]\] \[=\inf_{\pi_{t}(\cdot|x)\in\mathcal{P}(\mathbb{U})}\frac{1}{\eta} \log\left[\int_{\mathbb{U}}(\pi_{t}(u|x))^{1+\varepsilon\eta}\left(\exp\left( \frac{-\mathcal{Q}_{t}(x,u)}{\varepsilon}\right)\right)^{-\varepsilon\eta} \mathrm{d}u\right]\] \[=\inf_{\pi_{t}(\cdot|x)\in\mathcal{P}(\mathbb{U})}\frac{1}{\eta} \log\left[\left(\int_{\mathbb{U}}\exp\left(\frac{-\mathcal{Q}_{t}(x,u^{\prime })}{\varepsilon}\right)\mathrm{d}u^{\prime}\right)^{-\varepsilon\eta}\int_{ \mathbb{U}}\pi_{t}(u|x)^{1+\varepsilon\eta}\pi_{t}^{*}(u|x)^{-\varepsilon\eta} \mathrm{d}u\right]\] \[=-\varepsilon\log\left[\int_{\mathbb{U}}\exp\left(-\frac{ \mathcal{Q}_{t}(x,u^{\prime})}{\varepsilon}\right)\mathrm{d}u^{\prime}\right]+ \inf_{\pi_{t}(\cdot|x)\in\mathcal{P}(\mathbb{U})}\varepsilon D_{1+\varepsilon \eta}(\pi_{t}(\cdot|x)\|\pi_{t}^{*}(\cdot|x)).\]

Since \(D_{1+\varepsilon\eta}(\pi_{t}(\cdot|x)\|\pi_{t}^{*}(\cdot|x))\) attains its minimum value \(0\) if and only if \(\pi_{t}(\cdot|x)=\pi_{t}^{*}(\cdot|x)\), we conclude that

\[V_{t}(x)=-\varepsilon\log\left[\int_{\mathbb{U}}\exp\left(-\frac{\mathcal{Q}_ {t}(x,u^{\prime})}{\varepsilon}\right)\mathrm{d}u^{\prime}\right],\;\;\forall x \in\mathbb{X},\] (54)

and the unique optimal policy of Problem (47) is given by (53). Moreover, \(\pi_{t}^{*}\) can be rewritten as

\[\pi_{t}^{*}(u|x)=\exp\left(-\frac{\mathcal{Q}_{t}(x,u)}{\varepsilon}+\frac{V_{ t}(x)}{\varepsilon}\right),\;\;t\in[\![0,T-1]\!],\;u\in\mathbb{U},\;x\in \mathbb{X}.\] (55)

When considering the deterministic system \(x_{t+1}=\bar{f}_{t}(x_{t},u_{t})\), we immediately obtain the relation

\[\mathcal{Q}_{t}(x,u)=c_{t}(x,u)+V_{t+1}(\bar{f}_{t}(x,u)).\] (56)

On the other hand, the unique optimal policy of the MaxEnt control problem:

\[\operatorname*{minimize}_{\{\pi_{t}\}_{t=0}^{T-1}}\;\mathbb{E}\left[c_{T}(x_{T })+\sum_{t=0}^{T-1}\!\Big{(}c_{t}(x_{t},u_{t})-\varepsilon\mathcal{H}_{1}(\pi_ {t}(\cdot|x_{t}))\Big{)}\right]\] (57)

is also given by (55) whose Q-function (52) is replaced by

\[\mathcal{Q}_{t}(x,u)=c_{t}(x,u)+\mathbb{E}[V_{t+1}(f_{t}(x,u,w_{t}))].\]

Therefore, when the system is deterministic, the Q-function of the LP regularized risk-sensitive control problem (47) coincides with that of the MaxEnt control problem (57). Consequently, the optimal policy of Problem (57) solves Problem (47) for any \(\eta>-\varepsilon^{-1}\), \(\eta\neq 0\) for deterministic systems.

Linear quadratic Gaussian setting

In this appendix, we derive the regularized risk-sensitive optimal policy in the linear quadratic Gaussian setting.

**Theorem 9**.: _Let \(p(x_{t+1}|x_{t},u_{t})=\mathcal{N}(A_{t}x_{t}+B_{t}u_{t},\Sigma_{t})\) and \(c_{t}(x_{t},u_{t})=(x_{t}^{\top}Q_{t}x_{t}+u_{t}^{\top}R_{t}u_{t})/2,\ c_{T}(x_{T})=x_{T}^{ \top}Q_{T}x_{T}/2\), where \(\Sigma_{t}\), \(Q_{t}\), and \(R_{t}\) are positive definite matrices for any \(t\), and \(\mathcal{N}(\mu,\Sigma)\) denotes the Gaussian distribution with mean \(\mu\) and covariance \(\Sigma\). Let \(\mathbb{X}=\mathbb{R}^{n_{x}}\), \(\mathbb{U}=\mathbb{R}^{n_{u}}\). Assume that there exists a solution \(\{\Pi_{t}\}_{t=0}^{T}\) to the following Riccati difference equation:_

\[\Pi_{t} =Q_{t}+A_{t}^{\top}\Pi_{t+1}(I-\eta\Sigma_{t}\Pi_{t+1}+B_{t}R_{t} ^{-1}B_{t}^{\top}\Pi_{t+1})^{-1}A_{t},\ \forall t\in[\![0,T-1]\!],\] (58) \[\Pi_{T} =Q_{T},\] (59)

_such that \(\Sigma_{t}^{-1}-\eta\Pi_{t+1}\) is positive definite for any \(t\in[\![0,T-1]\!]\). Here, \(I\) denotes the identity matrix of appropriate dimension. Then, the unique optimal policy of Problem (9) is given by_

\[\pi_{t}^{*}(u|x)=\mathcal{N}\Big{(}u|-(R_{t}+B_{t}^{\top}\Pi_{t+1 }(I-\eta\Sigma_{t}\Pi_{t+1})^{-1}B_{t})^{-1}B_{t}^{\top}\Pi_{t+1}(I-\eta\Sigma_ {t}\Pi_{t+1})^{-1}A_{t}x,\] \[\qquad\qquad\qquad(R_{t}+B_{t}\Pi_{t+1}(I-\eta\Sigma_{t}\Pi_{t+1} )^{-1}B_{t})^{-1}\Big{)}.\] (60)

\(\diamondsuit\)

Proof.: In this proof, for notational simplicity, we often drop the time index \(t\) as \(A,B\). First, for \(t=T-1\), the Q-function in (11) is

\[\mathcal{Q}_{T-1}(x,u)=\frac{1}{2}\|x\|_{Q_{T-1}}^{2}+\frac{1}{2}\|u\|_{R_{T- 1}}^{2}+\frac{1}{\eta}\log\mathbb{E}\left[\exp\Big{(}\frac{\eta}{2}\|A_{T-1}x +B_{T-1}u+w_{T-1}\|_{\Pi_{T}}^{2}\Big{)}\right],\] (61)

where \(\|x\|_{D}^{2}:=x^{\top}Px\) for a symmetric matrix \(P\). Here, we have

\[\mathbb{E}\left[\exp\Big{(}\frac{\eta}{2}\|Ax+Bu+w_{T-1}\|_{\Pi_{ T}}^{2}\Big{)}\right]\] \[=\frac{1}{\sqrt{(2\pi)^{n_{x}}|\Sigma_{T-1}|}}\int_{\mathbb{R}^{n _{x}}}\exp\left(-\frac{1}{2}\|w\|_{\Sigma_{T-1}^{-1}}^{2}+\frac{\eta}{2}\|Ax +Bu+w\|_{\Pi_{T}}^{2}\right)\mathrm{d}w,\] (62)

where \(|\Sigma_{T-1}|\) denotes the determinant of \(\Sigma_{T-1}\), and

\[-\frac{1}{2}\|w\|_{\Sigma_{T-1}^{-1}}^{2}+\frac{\eta}{2}\|Ax+Bu+w\|_{\Pi_{T}} ^{2}\]

\[=-\frac{1}{2}\left(\|w\|_{\Sigma^{-1}-\eta\Pi}^{2}-2\eta w^{\top}\Pi(Ax+Bu)- \|Ax+Bu\|_{\eta\Pi}^{2}\right).\]

By the assumption that \(\Sigma_{T-1}^{-1}-\eta\Pi_{T}\) is positive definite and a completion of squares argument,

\[-\frac{1}{2}\|w\|_{\Sigma_{T-1}^{-1}}^{2}+\frac{\eta}{2}\|Ax+Bu+w\|_{\Pi_{T}} ^{2}\]

\[=-\frac{1}{2}\left(\|w-(\Sigma^{-1}-\eta\Pi)^{-1}\eta\Pi(Ax+Bu)\|_{\Sigma^{-1 }-\eta\Pi}^{2}-\|\eta\Pi(Ax+Bu)\|_{(\Sigma^{-1}-\eta\Pi)^{-1}}^{2}-\|Ax+Bu\|_{ \eta\Pi}^{2}\right).\]

Thus, we obtain

\[\int_{\mathbb{R}^{n_{x}}}\exp\left(-\frac{1}{2}\|w\|_{\Sigma_{T- 1}^{-1}}^{2}+\frac{\eta}{2}\|Ax+Bu+w\|_{\Pi_{T}}^{2}\right)\mathrm{d}w\] \[=\sqrt{(2\pi)^{n_{x}}|(\Sigma^{-1}-\eta\Pi)^{-1}|}\exp\left(\frac {1}{2}\|\eta\Pi(Ax+Bu)\|_{(\Sigma^{-1}-\eta\Pi)^{-1}}^{2}+\frac{1}{2}\|Ax+Bu \|_{\eta\Pi}^{2}\right).\] (63)

Consequently, by (61)-(63), the Q-function can be written as

\[\mathcal{Q}_{T-1}(x,u) =\frac{1}{2}\|x\|_{Q_{T-1}}^{2}+\frac{1}{2}\|u\|_{R_{T-1}}^{2}+ \frac{1}{2}\|\eta\Pi(A_{T-1}x+B_{T-1}u)\|_{(\Sigma_{T-1}^{-1}-\eta\Pi_{T})^{-1}}^ {2}\] \[\quad+\frac{1}{2}\|A_{T-1}x+B_{T-1}u\|_{\Pi}^{2}+C_{\mathcal{Q}_ {T-1}}\] \[=\frac{1}{2}\|x\|_{Q}^{2}+\frac{1}{2}\|u\|_{R}^{2}+\frac{1}{2}\| Ax+Bu\|_{\eta\Pi(\Sigma^{-1}-\eta\Pi)^{-1}\Pi+\Pi}^{2}+C_{\mathcal{Q}_{T-1}}\] \[=\frac{1}{2}\|x\|_{Q}^{2}+\frac{1}{2}\|u\|_{R}^{2}+\frac{1}{2}\| Ax+Bu\|_{\Pi(I-\eta\Sigma\Pi)^{-1}}^{2}+C_{\mathcal{Q}_{T-1}},\]where the constant \(C_{\mathcal{Q}_{T-1}}\) is independent of \((x,u)\). Now, we adopt a completion of squares argument again:

\[\mathcal{Q}_{T-1}(x,u) =\frac{1}{2}\left(\|u\|_{R+B^{\top}\Pi(I-\eta\Sigma\Pi)^{-1}B}^{2} +2x^{\top}A^{\top}\Pi(I-\eta\Pi\Sigma)^{-1}Bu+\|x\|_{Q+A^{\top}\Pi(I-\eta\Sigma \Pi)^{-1}A}^{2}\right)\] \[\qquad+C_{\mathcal{Q}_{T-1}}\] \[=\frac{1}{2}\bigg{(}\|u+(R+B^{\top}\Pi(I-\eta\Sigma\Pi)^{-1}B)^{- 1}B^{\top}(I-\eta\Pi\Sigma)^{-1}\Pi Ax\|_{R+B^{\top}\Pi(I-\eta\Sigma\Pi)^{-1}B}^ {2}\] \[\qquad-\|B^{\top}(I-\eta\Pi\Sigma)^{-1}\Pi Ax\|_{(R+B^{\top}\Pi(I -\eta\Sigma\Pi)^{-1}B)^{-1}}^{2}+\|x\|_{Q+A^{\top}\Pi(I-\eta\Sigma\Pi)^{-1}A}^ {2}\bigg{)}\] \[\qquad+C_{\mathcal{Q}_{T-1}}\] \[=\frac{1}{2}\|u+(R+B^{\top}\Pi_{T}(I-\eta\Sigma\Pi_{T})^{-1}B)^{- 1}B^{\top}\Pi_{T}(I-\eta\Sigma\Pi_{T})^{-1}Ax\|_{R+B^{\top}\Pi_{T}(I-\eta\Sigma \Pi_{T})^{-1}B}^{2}\] \[\qquad+\frac{1}{2}\|x\|_{\Pi_{T-1}}^{2}+C_{\mathcal{Q}_{T-1}}.\]

Here, we used \(\Pi_{T}(I-\eta\Sigma_{T-1}\Pi_{T})^{-1}=(I-\eta\Pi_{T}\Sigma_{T-1})^{-1}\Pi_{T}\) and

\[\Pi_{T-1} =Q_{T-1}+A_{T-1}^{\top}\Pi_{T}(I-\eta\Sigma_{T-1}\Pi_{T}+B_{T-1}R_ {T-1}^{-1}B_{T-1}^{\top}\Pi_{T})^{-1}A_{T-1}\] \[=Q+A^{\top}\Pi_{T}(I-\eta\Sigma_{T-1}\Pi_{T})^{-1}A-A^{\top}\Pi_ {T}(I-\eta\Sigma_{T-1}\Pi_{T})^{-1}B\] \[\qquad\times(R_{T-1}+B^{\top}\Pi_{T}(I-\eta\Sigma_{T-1}\Pi_{T})^ {-1}B)^{-1}B^{\top}(I-\eta\Pi_{T}\Sigma_{T-1})^{-1}\Pi_{T}A.\]

Therefore, the optimal policy at \(t=T-1\) is

\[\pi_{T-1}^{*}(u|x)=\mathcal{N}\big{(}u|-(R_{T-1}+B^{\top}\Pi_{T} (I-\eta\Sigma_{T-1}\Pi_{T})^{-1}B)^{-1}B^{\top}\Pi_{T}(I-\eta\Sigma_{T-1}\Pi_{ T})^{-1}Ax,\] \[\qquad\qquad(R_{T-1}+B^{\top}\Pi_{T}(I-\eta\Sigma_{T-1}\Pi_{T})^{ -1}B)^{-1}\big{)}.\] (64)

The value function is given by

\[V_{T-1}(x)=-\log\left[\int_{\mathbb{R}^{n_{u}}}\exp(-\mathcal{Q}_{T-1}(x,u)) \mathrm{d}u\right]=\frac{1}{2}\|x\|_{\Pi_{T-1}}^{2}+C_{V_{T-1}},\]

where \(C_{V_{T-1}}\) does not depend on \(x\).

By applying the same argument as above for \(t=T-2,\ldots,0\), we arrive at the optimal policy (60) and

\[V_{t}(x)=\frac{1}{2}\|x\|_{\Pi_{t}}^{2}+C_{V_{t}},\] (65) \[\mathcal{Q}_{t}(x,u)\] \[=\frac{1}{2}\|u+(R_{t}+B^{\top}\Pi_{t+1}(I-\eta\Sigma_{t}\Pi_{t+ 1})^{-1}B)^{-1}B^{\top}\Pi_{t+1}(I-\eta\Sigma_{t}\Pi_{t+1})^{-1}Ax\|_{R_{t}+B^ {\top}\Pi_{t+1}(I-\eta\Sigma_{t}\Pi_{t+1})^{-1}B}^{2}\] \[\quad+\frac{1}{2}\|x\|_{\Pi_{t}}^{2}+C_{\mathcal{Q}_{t}},\] (66)

where \(C_{V_{t}}\) and \(C_{\mathcal{Q}_{t}}\) are independent of \((x,u)\). This completes the proof. 

By the same argument as above, the optimal policy of the Renyi entropy regularized risk-sensitive control problem (17) in the linear quadratic Gaussian setting is also given by (60).

## Appendix D Proof of Lemma 5

First, we give the precise statement of Lemma 5. To this end, for \(a,b\in\mathbb{R}\), define

\[\mathcal{B}_{a,b}(\mathbb{U}):=\left\{g:\mathbb{U}\rightarrow\mathbb{R}\ \bigg{|}\ g\text{ is bounded below},\ \int_{\mathbb{U}}\exp(ag(u))\mathrm{d}u<\infty,\ \int_{\mathbb{U}}\exp(bg(u))\mathrm{d}u<\infty\right\}.\] (67)Similarly, define \(\mathcal{\bar{B}}_{a,b}(\mathbb{U})\) for upper bounded functions. For given \(g:\mathbb{U}\to\mathbb{R}\), \(a\in\mathbb{R}\), and \(\alpha\in\mathbb{R}\setminus\{0,1\}\), define

\[\mathcal{P}_{a,g}(\mathbb{U}) :=\left\{\rho\in\mathcal{P}(\mathbb{U})\;\middle|\;\int_{\mathbb{ U}}\exp(ag(u))\rho(u)\mathrm{d}u<\infty\right\},\] \[L^{\alpha}(\mathbb{U}) :=\left\{\rho\in\mathcal{P}(\mathbb{U})\;\middle|\;\int_{\mathbb{ U}}\rho(u)^{\alpha}\mathrm{d}u<\infty\right\}.\]

If \(\rho\in L^{\alpha}(\mathbb{U})\) and \(\alpha\in(0,1)\), then it holds that \(\mathcal{H}_{\alpha}(\rho)<\infty\). If \(\alpha\in(-\infty,0)\cap(1,\infty)\), we have \(\mathcal{H}_{\alpha}(\rho)>-\infty\).

Now, we are ready to state the duality lemma.

**Lemma 10**.: _For \(\beta,\gamma\in\mathbb{R}\setminus\{0\}\) such that \(\beta<\gamma\) and for \(g\in\underline{\mathcal{B}}_{\{\beta,-(\gamma-\beta)\}}(\mathbb{U})\), it holds that_

\[\frac{1}{\beta}\log\left[\int_{\mathbb{U}}\exp(\beta g(u))\mathrm{d}u\right]= \inf_{\rho\in L^{1-\frac{\gamma}{\gamma-\beta}}(\mathbb{U})}\left\{\frac{1}{ \gamma}\log\left[\int_{\mathbb{U}}\exp(\gamma g(u))\rho(u)\mathrm{d}u\right]- \frac{1}{\gamma-\beta}\mathcal{H}_{1-\frac{\gamma}{\gamma-\beta}}(\rho)\right\},\] (68)

_and the unique optimal solution that minimizes the right-hand side of (68) is given by_

\[\rho(u)=\frac{\exp\left(-(\gamma-\beta)g(u)\right)}{\int_{\mathbb{U}}\exp(-( \gamma-\beta)g(u^{\prime}))\mathrm{d}u^{\prime}},\;\;u\in\mathbb{U}.\] (69)

_In addition, for \(h\in\overline{\mathcal{B}}_{\{\gamma,\gamma-\beta\}}(\mathbb{U})\), it holds that_

\[\frac{1}{\gamma}\log\left[\int\exp(\gamma h(u))\mathrm{d}u\right]=\sup_{\rho \in L^{\frac{\gamma}{\gamma-\beta}}(\mathbb{U})}\left\{\frac{1}{\beta}\log \left[\int\exp(\beta h(u))\rho(u)\mathrm{d}u\right]+\frac{1}{\gamma-\beta} \mathcal{H}_{\frac{\gamma}{\gamma-\beta}}(\rho)\right\},\] (70)

_and the unique optimal solution that maximizes the right-hand side of (70) is given by_

\[\rho(u)=\frac{\exp((\gamma-\beta)h(u))}{\int\exp((\gamma-\beta)h(u^{\prime}) )\mathrm{d}u^{\prime}},\;\;u\in\mathbb{U}.\] (71)

Although the proof is similar to that of the duality between exponential integrals and Renyi divergence [40], it requires more careful analysis because we do not assume the upper boundedness of \(g\) and the lower boundedness of \(h\), unlike in [40].

Proof.: For notational simplicity, we often drop \(\mathbb{U}\) as \(L^{\alpha}\). First, we note that it is sufficient to prove that for \(\alpha>0,\alpha\neq 1\), \(g\in\underline{\mathcal{B}}_{\{\alpha-1,-1\}}\), and \(h\in\overline{\mathcal{B}}_{\{\alpha,1\}}\), it holds that

\[\frac{1}{\alpha-1}\log\left[\int\exp((\alpha-1)g(u))\mathrm{d}u \right] =\inf_{\rho\in L^{1-\alpha}}\left\{\frac{1}{\alpha}\log\left[\int \exp(\alpha g(u))\rho(u)\mathrm{d}u\right]-\mathcal{H}_{1-\alpha}(\rho)\right\},\] (72) \[\frac{1}{\alpha}\log\left[\int\exp(\alpha h(u))\mathrm{d}u\right] =\sup_{\rho\in L^{\alpha}}\left\{\frac{1}{\alpha-1}\log\left[ \int\exp((\alpha-1)h(u))\rho(u)\mathrm{d}u\right]+\mathcal{H}_{\alpha}(\rho) \right\},\] (73)

and

\[\rho^{*}(u):=\frac{\exp(-g(u))}{\int\exp(-g(u^{\prime}))\mathrm{d}u^{\prime}}, \;\;\rho^{**}(u):=\frac{\exp(h(u))}{\int\exp(h(u^{\prime}))\mathrm{d}u^{\prime}}\] (74)

are the unique optimal solutions to (72), (73), respectively. To see this, note that if (72), (73) hold for \(\alpha>0,\alpha\neq 1\), they hold for any \(\alpha\in\mathbb{R}\setminus\{0,1\}\). Indeed, when \(\alpha<0\), let \(\bar{\alpha}:=1-\alpha>1\) and for \(h\in\overline{\mathcal{B}}_{\{\alpha,1\}}\), let \(\bar{g}:=-h\). Since \(\bar{g}\in\underline{\mathcal{B}}_{\{\bar{\alpha}-1,-1\}}\), by (72), we have

\[\frac{1}{\bar{\alpha}-1}\log\left[\int\exp((\bar{\alpha}-1)\bar{g}(u))\mathrm{ d}u\right]=\inf_{\rho\in L^{1-\alpha}}\left\{\frac{1}{\bar{\alpha}}\log\left[ \int\exp(\bar{\alpha}\bar{g}(u))\rho(u)\mathrm{d}u\right]-\mathcal{H}_{1- \bar{\alpha}}(\rho)\right\}.\]Therefore, it holds that

\[-\frac{1}{\alpha}\log\left[\int\exp(\alpha h(u))\mathrm{d}u\right] =\inf_{\rho\in L^{\alpha}}\left\{\frac{1}{1-\alpha}\log\left[\int \exp((\alpha-1)h(u))\rho(u)\mathrm{d}u\right]-\mathcal{H}_{\alpha}(\rho)\right\}\] \[=-\sup_{\rho\in L^{\alpha}}\left\{\frac{1}{\alpha-1}\log\left[ \int\exp((\alpha-1)h(u))\rho(u)\mathrm{d}u\right]+\mathcal{H}_{\alpha}(\rho) \right\},\]

which means that for any \(\alpha<0\) and any \(h\in\overline{\mathcal{B}}_{\alpha,1}\), (73) holds. Similarly, by considering \(\bar{h}:=-g\in\underline{\mathcal{B}}_{\{\bar{\alpha},1\}}\) for \(g\in\underline{\mathcal{B}}_{\{\alpha-1,-1\}}\), we can see that for any \(\alpha<0\) and any \(g\in\underline{\mathcal{B}}_{\{\alpha-1,-1\}}\), (72) holds. Additionally, (72) and (73) with \(\alpha=\frac{\gamma}{\gamma-\beta},\;g=(\gamma-\beta)\widetilde{g},\;h=( \gamma-\beta)\widetilde{h}\) coincide with (68), (70) where \(g\) and \(h\) are replaced by \(\widetilde{g}\), \(\widetilde{h}\).

In what follows, for \(\alpha>0\), \(\alpha\neq 1\), we prove (72). Note that when \(\rho\in L^{1-\alpha}\), \(|\mathcal{H}_{1-\alpha}(\rho)|<\infty\) holds. Hence, for the minimization of (72), it is sufficient to consider \(\rho\in\mathcal{P}_{\alpha,g}\cap L^{1-\alpha}\). The density \(\rho^{*}\) defined in (74) fulfills \(\rho^{*}\in\mathcal{P}_{\alpha,g}\cap L^{1-\alpha}\) because \(g\in\underline{\mathcal{B}}_{\{\alpha-1,-1\}}\), and it can be easily seen that

\[\frac{1}{\alpha-1}\log\left[\int\exp((\alpha-1)g(u))\mathrm{d}u\right]=\frac{ 1}{\alpha}\log\left[\int\exp(\alpha g(u))\rho^{*}(u)\mathrm{d}u\right]- \mathcal{H}_{1-\alpha}(\rho^{*}).\] (75)

First, we consider the case \(\alpha>1\). Define \(\widetilde{\rho}(u):=\exp((\alpha-1)g(u))\), \(\varphi(u):=\exp(-g(u))\). Then, by Holder's inequality, for any \(\rho\in\mathcal{P}_{\alpha,g}\cap L^{1-\alpha}\), it holds that

\[\int\widetilde{\rho}(u)\mathrm{d}u =\int\left(\frac{\varphi(u)}{\rho(u)}\right)^{\frac{\alpha-1}{ \alpha}}\left(\frac{\rho(u)}{\varphi(u)}\right)^{\frac{\alpha-1}{\alpha}} \widetilde{\rho}(u)\mathrm{d}u\] \[\leq\left(\int\left(\frac{\varphi(u)}{\rho(u)}\right)^{\alpha-1} \widetilde{\rho}(u)\mathrm{d}u\right)^{\frac{1}{\alpha}}\left(\int\frac{\rho( u)}{\varphi(u)}\widetilde{\rho}(u)\mathrm{d}u\right)^{\frac{\alpha-1}{\alpha}}\] \[=\left(\int\rho(u)^{1-\alpha}\mathrm{d}u\right)^{\frac{1}{\alpha }}\left(\int\exp(\alpha g(u))\rho(u)\mathrm{d}u\right)^{\frac{\alpha-1}{\alpha }}.\] (76)

Noting that \(\alpha-1>0\) and taking the logarithm of (76), we get for any \(\rho\in\mathcal{P}_{\alpha,g}\cap L^{1-\alpha}\),

\[\frac{1}{\alpha-1}\log\left[\int\exp((\alpha-1)g(u))\mathrm{d}u\right]\leq \frac{1}{\alpha}\log\left[\int\exp(\alpha g(u))\rho(u)\mathrm{d}u\right]- \mathcal{H}_{1-\alpha}(\rho).\]

Combining this with (75), the relation (72) holds, and by (75), \(\rho^{*}\) in (74) is an optimal solution. The equality of Holder's inequality (76) holds if and only if there exist \(a_{1},a_{2}\geq 0\), \(a_{1}a_{2}\neq 0\) such that \(a_{1}\left(\frac{\varphi(u)}{\rho(u)}\right)^{1-\alpha}=a_{2}\frac{\rho(u)}{ \varphi(u)}\) holds \(\widetilde{\mu}\)-almost everywhere. Here, \(\widetilde{\mu}\) is the measure defined by \(\widetilde{\rho}\). This condition is satisfied only for \(\rho^{*}\), that is, it is an unique optimal solution.

Next, we analyze the case \(\alpha\in(0,1)\). By Holder's inequality, for any \(\rho\in\mathcal{P}_{\alpha,g}\),

\[\int\left(\frac{\varphi(u)}{\rho(u)}\right)^{\alpha-1}\widetilde {\rho}(u)\mathrm{d}u \leq\left(\int 1^{1/\alpha}\widetilde{\rho}(u)\mathrm{d}u\right)^{ \alpha}\left(\int\left[\left(\frac{\varphi(u)}{\rho(u)}\right)^{\alpha-1} \right]^{\frac{1}{1-\alpha}}\widetilde{\rho}(u)\mathrm{d}u\right)^{1-\alpha}\] \[=\left(\int\widetilde{\rho}(u)\mathrm{d}u\right)^{\alpha}\left( \int\frac{\rho(u)}{\varphi(u)}\widetilde{\rho}(u)\mathrm{d}u\right)^{1- \alpha},\]

which yields

\[\frac{1}{\alpha-1}\log\left[\int\exp((\alpha-1)g(u))\mathrm{d}u\right]\leq \frac{1}{\alpha}\left[\int\exp(\alpha g(u))\rho(u)\mathrm{d}u\right]- \mathcal{H}_{1-\alpha}(\rho),\;\;\forall\rho\in\mathcal{P}_{\alpha,g}.\] (77)

Then, similar to the case \(\alpha>1\), it can be seen that for \(\alpha\in(0,1)\), (72) holds and \(\rho^{*}\) is a unique optimal solution.

Next, we show (73) for \(\alpha>1\). Since \(\alpha>1\) and \(h\) is upper bounded, it holds that \(\rho\in\mathcal{P}_{\alpha-1,h}\). The density \(\rho^{**}\) defined in (74) satisfies \(\rho^{**}\in\mathcal{P}_{\alpha-1,h}\cap L^{\alpha}\) because \(h\in\mathcal{B}_{\{\alpha,1\}}\), and one can easily see that

\[\frac{1}{\alpha}\log\left[\int\exp(\alpha h(u))\mathrm{d}u\right]=\frac{1}{ \alpha-1}\log\left[\int\exp((\alpha-1)h(u))\rho^{**}(u)\mathrm{d}u\right]+ \mathcal{H}_{\alpha}(\rho^{**}).\]

Define \(\widehat{\rho}(u):=\exp((\alpha-1)h(u))\rho(u)\), \(\lambda(u):=\exp(-h(u))\rho(u)\). Then, by Holder's inequality, for any \(\rho\in L^{\alpha}\), it holds that

\[\int\widehat{\rho}(u)\mathrm{d}u =\int\lambda(u)^{\frac{\alpha-1}{\alpha}}\lambda(u)^{-\frac{ \alpha-1}{\alpha}}\widehat{\rho}(u)\mathrm{d}u\] \[\leq\left(\int\lambda(u)^{\alpha-1}\widehat{\rho}(u)\mathrm{d}u \right)^{\frac{1}{\alpha}}\left(\int\lambda(u)^{-1}\widehat{\rho}(u)\mathrm{d }u\right)^{\frac{\alpha-1}{\alpha}}\] \[=\left(\int\rho(u)^{\alpha}\mathrm{d}u\right)^{\frac{1}{\alpha} }\left(\int\exp(\alpha h(u))\mathrm{d}u\right)^{\frac{\alpha-1}{\alpha}}.\]

It follows from the above that for any \(\rho\in L^{\alpha}\),

\[\frac{1}{\alpha-1}\log\left[\int\exp((\alpha-1)h(u))\rho(u)\mathrm{d}u\right] \leq\frac{1}{\alpha}\log\left[\int\exp(\alpha h(u))\mathrm{d}u\right]- \mathcal{H}_{\alpha}(\rho).\]

Hence, by the same argument as for (72), we can show that (73) holds for \(\alpha>1\), and \(\rho^{**}\) is a unique optimal solution.

Lastly, we show (73) for \(\alpha\in(0,1)\). For \(\rho\in L^{\alpha}\), it holds that \(|\mathcal{H}_{\alpha}(\rho)|<\infty\). Then, noting that \(\alpha-1<0\), it is sufficient to perform the maximization in (73) for \(\rho\in\mathcal{P}_{\alpha-1,h}\cap L^{\alpha}\). By Holder's inequality, for any \(\rho\in\mathcal{P}_{\alpha-1,h}\), we have

\[\int\rho^{\alpha}\mathrm{d}u=\int\lambda(u)^{\alpha-1}\widehat{ \rho}(u)\mathrm{d}u \leq\left(\int 1^{1/\alpha}\widehat{\rho}(u)\mathrm{d}u\right)^{ \alpha}\left((\lambda(u)^{\alpha-1})^{\frac{1}{1-\alpha}}\widehat{\rho}(u) \mathrm{d}u\right)^{1-\alpha}\] \[=\left(\int\exp((\alpha-1)h(u))\rho(u)\mathrm{d}u\right)^{\alpha} \left(\int\exp(\alpha h(u))\mathrm{d}u\right)^{1-\alpha}.\]

Therefore,

\[\frac{1}{\alpha-1}\log\left[\int\exp((\alpha-1)h(u))\rho(u)\mathrm{d}u\right] \leq\frac{1}{\alpha}\log\left[\int\exp(\alpha h(u))\mathrm{d}u\right]- \mathcal{H}_{\alpha}(\rho),\]

and similar to the case \(\alpha>1\), we arrive at (73) for \(\alpha\in(0,1)\), and the unique optimal solution is \(\rho^{**}\). This completes the proof. 

## Appendix E Proof of Theorem 6

In this appendix, we analyze the following problem:

\[\underset{\{\tau_{t}\}_{t=0}^{T-1}}{\mathrm{minimize}}\ \ \frac{1}{\eta}\log\mathbb{E}\left[\exp \left(\eta c_{T}(x_{T})+\eta\sum_{t=0}^{T-1}\Bigl{(}c_{t}(x_{t},u_{t})-\varepsilon \mathcal{H}_{1-\varepsilon\eta}(\pi_{t}(\cdot|x_{t}))\Bigr{)}\right)\right],\] (78)

where \(\varepsilon>0\), \(\eta\in\mathbb{R}\setminus\{0,\varepsilon^{-1}\}\), the system is given by (48)-(50), and \(\pi_{t}(\cdot|x)\in L^{1-\varepsilon\eta}(\mathbb{U}):=\{\rho\in\mathcal{P}( \mathbb{U})\mid\int_{\mathbb{U}}\rho(u)^{1-\varepsilon\eta}\mathrm{d}u<\infty\}\) for any \(x\in\mathbb{X}\) and \(t\in\llbracket 0,T-1\rrbracket\).

Define the value function and the Q-function associated with (78) as

\[\mathcal{V}_{t}(x) :=\inf_{\{\pi_{s}\}_{s=t}^{T-1}}\frac{1}{\eta}\log\mathbb{E} \left[\exp\left(\eta c_{T}(x_{T})+\eta\sum_{s=t}^{T-1}\Bigl{(}c_{s}(x_{s},u_{s} )-\varepsilon\mathcal{H}_{1-\varepsilon\eta}(\pi_{s}(\cdot|x_{s}))\Bigr{)} \right)\Bigg{|}\ x_{t}=x\right],\] \[\mathcal{V}_{T}(x) :=c_{T}(x),\ \ x\in\mathbb{X},\] (79) \[\mathcal{Q}_{t}(x,u) :=c_{t}(x,u)+\frac{1}{\eta}\log\mathbb{E}\left[\exp\bigl{(}\eta \mathcal{V}_{t+1}(f_{t}(x,u,w_{t}))\bigr{)}\right],\ \ t\in\llbracket 0,T-1\rrbracket,\ x\in\mathbb{X},\ u\in \mathbb{U}.\] (80)

For the analysis, we assume the following conditions.

[MISSING_PAGE_EMPTY:22]

we have

\[\mathcal{V}_{t}(x)=\inf_{\{\pi_{s}\}_{s=t}^{T-1}}\frac{1}{\eta}\log \mathbb{E}\Bigg{[}\exp\Bigg{(}\eta c_{t}(x,u_{t})-\varepsilon\eta\mathcal{H}_{1- \varepsilon\eta}(\pi_{t}(\cdot|x))+\eta c_{T}(x_{T})\] \[\qquad\qquad\qquad+\eta\sum_{s=t+1}^{T-1}\Big{(}c_{s}(x_{s},u_{s} )-\varepsilon\mathcal{H}_{1-\varepsilon\eta}(\pi_{s}(\cdot|x_{s}))\Big{)} \Bigg{)}\Bigg{\arrowvert\,x_{t}=x\Bigg{]}\] \[=\inf_{\{\pi_{s}\}_{s=t}^{T-1}}-\varepsilon\mathcal{H}_{1- \varepsilon\eta}(\pi_{t}(\cdot|x))\] \[\quad+\frac{1}{\eta}\log\mathbb{E}\left[\exp\Bigg{(}\eta c_{t}(x,u_{t})+\eta c_{T}(x_{T})+\eta\sum_{s=t+1}^{T-1}\big{(}c_{s}(x_{s},u_{s})- \varepsilon\mathcal{H}_{1-\varepsilon\eta}(\pi_{s}(\cdot|x_{s}))\big{)} \right)\Bigg{\arrowvert\,x_{t}=x\Bigg{]}\] \[=\inf_{\{\pi_{s}\}_{s=t}^{T-1}}-\varepsilon\mathcal{H}_{1- \varepsilon\eta}(\pi_{t}(\cdot|x))+\frac{1}{\eta}\log\Biggl{[}\int_{\mathbb{U }}\pi_{t}(u|x)\mathbb{E}\Bigg{[}\exp\Big{(}\eta c_{t}(x,u)+\eta c_{T}(x_{T})\] \[\qquad\qquad+\eta\sum_{s=t+1}^{T-1}\big{(}c_{s}(x_{s},u_{s})- \varepsilon\mathcal{H}_{1-\varepsilon\eta}(\pi_{s}(\cdot|x_{s}))\big{)} \Bigg{)}\Bigg{\arrowvert\,x_{t}=x,\;u_{t}=u\Bigg{]}\mathrm{d}u\Bigg{]}\] \[=\inf_{\pi_{t}(\cdot|x)\in L^{1-\eta}(\mathbb{U})}-\varepsilon \mathcal{H}_{1-\varepsilon\eta}(\pi_{t}(\cdot|x))+\frac{1}{\eta}\log\Biggl{[} \int\pi_{t}(u|x)\exp(\eta c_{t}(x,u))\] \[\times\mathbb{E}_{\{\pi_{s}^{*}\}_{s=t+1}^{T-1}}\bigg{[}\exp \bigg{(}\eta c_{T}(x_{T})+\eta\sum_{s=t+1}^{T-1}\big{(}c_{s}(x_{s},u_{s})- \varepsilon\mathcal{H}_{1-\varepsilon\eta}(\pi_{s}^{*}(\cdot|x_{s}))\big{)} \bigg{)}\Bigg{\arrowvert\,x_{t}=x,\;u_{t}=u\Bigg{]}\mathrm{d}u\Bigg{]}.\] (85)

Moreover, noting that

\[\exp(\eta\mathcal{V}_{t+1}(x))=\mathbb{E}_{\{\pi_{s}^{*}\}_{s=t+1}^{T-1}} \Bigg{[}\exp\Bigg{(}\eta c_{T}(x_{T})+\eta\sum_{s=t+1}^{T-1}\big{(}c_{s}(x_{s },u_{s})-\varepsilon\mathcal{H}_{1-\varepsilon\eta}(\pi_{s}^{*}(\cdot|x_{s})) \big{)}\Bigg{)}\Bigg{\arrowvert\,x_{t+1}=x\Bigg{]},\]

we get

\[\mathcal{V}_{t}(x)=\inf_{\pi_{t}(\cdot|x)\in L^{1-\varepsilon\eta} (\mathbb{U})}-\varepsilon\mathcal{H}_{1-\varepsilon\eta}(\pi_{t}(\cdot|x))\] \[\qquad\qquad\qquad+\frac{1}{\eta}\log\left[\int\pi_{t}(u|x)\exp( \eta c_{t}(x,u))\mathbb{E}\left[\exp\big{(}\eta\mathcal{V}_{t+1}(f_{t}(x,u,w_{ t}))\big{)}\right]\mathrm{d}u\right].\] (86)

By using \(\mathscr{Q}_{t}\), the above equation can be written as

\[\mathcal{V}_{t}(x)=\inf_{\pi_{t}(\cdot|x)\in L^{1-\varepsilon\eta}(\mathbb{U} )}\frac{1}{\eta}\log\left[\int_{\mathbb{U}}\pi_{t}(u|x)\exp(\eta\mathscr{Q}_{t }(x,u))\mathrm{d}u\right]-\varepsilon\mathcal{H}_{1-\varepsilon\eta}(\pi_{t}( \cdot|x)).\] (87)

Since we assumed that \(\mathcal{V}_{t+1}\) is bounded below, \(\mathscr{Q}_{t}\) is also bounded below. By combining this with Assumption 12, it holds that \(\mathscr{Q}_{t}(x,\cdot)\in\underline{\mathcal{B}}_{-(\varepsilon^{-1}-\eta),- \varepsilon^{-1}}(\mathbb{U})\). Thus, by Lemma 10, the unique optimal policy that minimizes the right-hand side of the above equation is

\[\pi_{t}^{\star}(u|x)=\frac{\exp\left(-\frac{\mathscr{Q}_{t}(x,u)}{\varepsilon} \right)}{\int_{\mathbb{U}}\exp\left(-\frac{\mathscr{Q}_{t}(x,u^{\prime})}{ \varepsilon}\right)\mathrm{d}u^{\prime}}\] (88)

and

\[\mathcal{V}_{t}(x)=\frac{-1}{\varepsilon^{-1}-\eta}\log\left[\int_{\mathbb{U}} \exp\left(-(\varepsilon^{-1}-\eta)\mathscr{Q}_{t}(x,u)\right)\mathrm{d}u\right].\] (89)

Lastly, since \(\mathscr{Q}_{t}\) is bounded below, \(\mathcal{V}_{t}\) is also bounded below. This completes the induction step, and we obtain Theorem 6.

Proof of Proposition 7

By using the relation \(\nabla_{\theta}\log p_{\theta}(\tau)=\nabla_{\theta}p_{\theta}(\tau)/p_{\theta}(\tau)\), we obtain

\[\nabla_{\theta}J(\theta)=\int p_{\theta}(\tau)\exp(\eta C_{\theta}(\tau))\big{(} \eta\nabla_{\theta}C_{\theta}(\tau)+\nabla_{\theta}\log p_{\theta}(\tau)\big{)} \mathrm{d}\tau.\]

In addition, by the expression

\[p_{\theta}(\tau)=p(x_{0})\prod_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t})\pi^{(\theta)}( u_{t}|x_{t}),\]

we get

\[\nabla_{\theta}J(\theta)=\int p_{\theta}(\tau)\exp(\eta C_{\theta }(\tau))\left(\eta\sum_{t=0}^{T-1}\nabla_{\theta}\log\pi^{(\theta)}(u_{t}|x_{t })+\sum_{t=0}^{T-1}\nabla_{\theta}\log\pi^{(\theta)}(u_{t}|x_{t})\right) \mathrm{d}\tau\] \[=(\eta+1)\mathbb{E}_{p_{\theta}(\tau)}\left[\left(\sum_{t=0}^{T- 1}\nabla_{\theta}\log\pi^{(\theta)}(u_{t}|x_{t})\right)\exp\left(\eta C_{T}(x _{T})+\eta\sum_{t=0}^{T-1}\big{(}c_{t}(x_{t},u_{t})+\log\pi^{(\theta)}(u_{t}|x _{t})\big{)}\right)\right].\] (90)

Note that for any \(h:(\mathbb{X})^{t+1}\times(\mathbb{U})^{t+1}\rightarrow\mathbb{R}\), it holds that

\[\mathbb{E}\left[h(x_{0:t},u_{0:t})\right] =\int h(x_{0:t},u_{0:t})p(x_{0})\prod_{s=0}^{T-1}p(x_{s+1}|x_{s},u _{s})\pi^{(\theta)}(u_{s}|x_{s})\mathrm{d}x_{0:T}\mathrm{d}u_{0:T-1}\] \[=\int h(x_{0:t},u_{0:t})p(x_{0})\prod_{s=0}^{T-2}p(x_{s+1}|x_{s},u _{s})\pi^{(\theta)}(u_{s}|x_{s})\] \[\quad\times\left[\int p(x_{T}|x_{T-1},u_{T-1})\pi^{(\theta)}(u_{T -1}|x_{T-1})\mathrm{d}x_{T}\mathrm{d}u_{T-1}\right]\mathrm{d}x_{0:T-1}\mathrm{ d}u_{0:T-2}\] \[=\int h(x_{0:t},u_{0:t})p(x_{0})\prod_{s=0}^{T-2}p(x_{s+1}|x_{s}, u_{s})\pi^{(\theta)}(u_{s}|x_{s})\mathrm{d}x_{0:T-1}\mathrm{d}u_{0:T-2}\] \[\vdots\] \[=\int h(x_{0:t},u_{0:t})\pi^{(\theta)}(u_{t}|x_{t})p(x_{0})\prod_{ s=0}^{t-1}p(x_{s+1}|x_{s},u_{s})\pi^{(\theta)}(u_{s}|x_{s})\mathrm{d}x_{0:t} \mathrm{d}u_{0:t}.\]It follows from the above that

\[\mathbb{E}_{p_{\theta}(\tau)}\left[\nabla_{\theta}\log\pi^{(\theta) }(u_{t}|x_{t})\exp\left(\eta\sum_{s=0}^{t-1}\bigl{(}c_{s}(x_{s},u_{s})+\log\pi^ {(\theta)}(u_{s}|x_{s})\bigr{)}\right)\right]\] \[=\int\nabla_{\theta}\log\pi^{(\theta)}(u_{t}|x_{t})\exp\left(\eta \sum_{s=0}^{t-1}\bigl{(}c_{s}(x_{s},u_{s})+\log\pi^{(\theta)}(u_{s}|x_{s}) \bigr{)}\right)\] \[\qquad\times\pi^{(\theta)}(u_{t}|x_{t})p(x_{0})\prod_{s=0}^{t-1}p (x_{s+1}|x_{s},u_{s})\pi^{(\theta)}(u_{s}|x_{s})\mathrm{d}x_{0:t}\mathrm{d}u_{ 0:t}\] \[=\int\nabla_{\theta}\pi^{(\theta)}(u_{t}|x_{t})\exp\left(\eta \sum_{s=0}^{t-1}\bigl{(}c_{s}(x_{s},u_{s})+\log\pi^{(\theta)}(u_{s}|x_{s}) \bigr{)}\right)\] \[\qquad\times p(x_{0})\prod_{s=0}^{t-1}p(x_{s+1}|x_{s},u_{s})\pi^{ (\theta)}(u_{s}|x_{s})\mathrm{d}x_{0:t}\mathrm{d}u_{0:t}\] \[=\int\left(\nabla_{\theta}\int\pi^{(\theta)}(u_{t}|x_{t})\mathrm{ d}u_{t}\right)\exp\left(\eta\sum_{s=0}^{t-1}\bigl{(}c_{s}(x_{s},u_{s})+\log \pi^{(\theta)}(u_{s}|x_{s})\bigr{)}\right)\] \[\qquad\times p(x_{0})\prod_{s=0}^{t-1}p(x_{s+1}|x_{s},u_{s})\pi^{ (\theta)}(u_{s}|x_{s})\mathrm{d}x_{0:t}\mathrm{d}u_{0:t-1}\] \[=0.\] (91)

By combining this with (90), we get

\[\nabla_{\theta}J(\theta)\] \[=(\eta+1)\mathbb{E}_{p_{\theta}(\tau)}\left[\sum_{t=0}^{T-1} \nabla_{\theta}\log\pi^{(\theta)}(u_{t}|x_{t})\exp\left(\eta c_{T}(x_{T})+\eta \sum_{s=t}^{T-1}\bigl{(}c_{s}(x_{s},u_{s})+\log\pi^{(\theta)}(u_{s}|x_{s}) \bigr{)}\right)\right].\] (92)

Lastly, for any function \(b:\mathbb{R}^{n}\to\mathbb{R}\), it holds that

\[\mathbb{E}_{p_{\theta}(\tau)}[\nabla_{\theta}\log\pi^{(\theta)}(u _{t}|x_{t})b(x_{t})]=\int p_{\theta}(x_{t},u_{t})\frac{\nabla_{\theta}\pi^{( \theta)}(u_{t}|x_{t})}{\pi^{(\theta)}(u_{t}|x_{t})}b(x_{t})\mathrm{d}x_{t} \mathrm{d}u_{t}\] \[=\int p(x_{t})b(x_{t})\nabla_{\theta}\pi^{(\theta)}(u_{t}|x_{t}) \mathrm{d}u_{t}\mathrm{d}x_{t}=0.\]

This completes the proof.

## Appendix G Proof of Proposition 8

By definition,

\[\pi_{t}^{\bullet}=\operatorname*{arg\,min}_{\pi_{t}\in\mathcal{P}(\mathbb{U}) }\frac{1}{\eta}\log\left[\int p^{\pi}(\tau)\left(\frac{\prod_{s=0}^{T-1}\pi_{ s}(u_{s}|x_{s})}{p(O_{T}|x_{T})\prod_{s=0}^{T-1}p(\mathcal{O}_{s}|x_{s},u_{s})} \right)^{\eta}\mathrm{d}\tau\right].\] (93)The term between the brackets is

\[\int p^{\pi}(x_{0:t},u_{0:t})\] \[\times\left(\int p^{\pi}(x_{t+1:T},u_{t+1:T}|x_{t},u_{t})\left[ \frac{\prod_{s=0}^{T-1}\pi_{s}(u_{s}|x_{s})}{p(O_{T}|x_{T})\prod_{s=0}^{T-1}p( \mathcal{O}_{s}|x_{s},u_{s})}\right]^{\eta}\mathrm{d}x_{t+1:T}\mathrm{d}u_{t+1:T }\right)\mathrm{d}x_{0:t}\mathrm{d}u_{0:t}\] \[=\int p^{\pi}(x_{0:t},u_{0:t})\left[\frac{\prod_{s=0}^{t-1}\pi_{s }(u_{s}|x_{s})}{\prod_{s=0}^{t-1}p(\mathcal{O}_{s}|x_{s},u_{s})}\right]^{\eta} \mathrm{d}x_{0:t}\mathrm{d}u_{0:t},\] \[\times\underbrace{\left(\int p^{\pi}(x_{t+1:T},u_{t+1:T}|x_{t},u _{t})\left[\frac{\prod_{s=t}^{T-1}\pi_{s}(u_{s}|x_{s})}{p(\mathcal{O}_{t}|x_{ T})\prod_{s=t}^{T-1}p(\mathcal{O}_{s}|x_{s},u_{s})}\right]^{\eta}\mathrm{d}x_{t+1:T} \mathrm{d}u_{t+1:T}}_{=:M}\right)\mathrm{d}x_{0:t}\mathrm{d}u_{0:t},\]

where

\[M=\pi_{t}(u_{t}|x_{t})^{\eta}\int p^{\pi}(x_{t+1:T},u_{t+1:T}|x_{t},u_{t}) \left[\frac{\prod_{s=t+1}^{T-1}\pi_{s}(u_{s}|x_{s})}{p(\mathcal{O}_{T}|x_{T}) \prod_{s=t}^{T-1}p(\mathcal{O}_{s}|x_{s},u_{s})}\right]^{\eta}\mathrm{d}x_{t+ 1:T}\mathrm{d}u_{t+1:T}.\]

In addition, by the expression \(p^{\pi}(x_{0:t},u_{0:t})=p(x_{0})\pi_{t}(u_{t}|x_{t})\prod_{s=0}^{t-1}p(x_{s+1 }|x_{s},u_{s})\pi_{s}(u_{s}|x_{s})\),

\[\pi_{t}^{\bullet}=\operatorname*{arg\,min}_{\pi_{t}}\frac{1}{\eta }\log\Biggl{[}\int\pi_{t}(u_{t}|x_{t})^{1+\eta}\\ \times\mathbb{E}_{p^{\pi}(x_{t+1:T},u_{t+1:T}|x_{t},u_{t})}\Biggl{[} \left(\frac{\prod_{s=t+1}^{T-1}\pi_{s}(u_{s}|x_{s})}{p(\mathcal{O}_{t}|x_{T}) \prod_{s=t}^{T-1}p(\mathcal{O}_{s}|x_{s},u_{s})}\right)^{\eta}\Biggr{]} \mathrm{d}x_{t}\mathrm{d}u_{t}\Biggr{]}.\] (94)

Now, define

\[\widehat{\pi}_{t}(u_{t}|x_{t}) :=\frac{1}{Z_{t}(x_{t})}\left(\mathbb{E}_{p^{\pi}(x_{t+1:T},u_{t+ 1:T}|x_{t},u_{t})}\left[\left(\frac{\prod_{s=t+1}^{T-1}\pi_{s}(u_{s}|x_{s})}{p (\mathcal{O}_{t}|x_{T})\prod_{s=t}^{T-1}p(\mathcal{O}_{s}|x_{s},u_{s})} \right)^{\eta}\right]\right)^{-1/\eta},\] (95) \[Z_{t}(x_{t}) :=\int\left(\mathbb{E}_{p^{\pi}(x_{t+1:T},u_{t+1:T}|x_{t},u_{t})} \left[\left(\frac{\prod_{s=t+1}^{T-1}\pi_{s}(u_{s}|x_{s})}{p(\mathcal{O}_{t}|x _{T})\prod_{s=t}^{T-1}p(\mathcal{O}_{s}|x_{s},u_{s})}\right)^{\eta}\right] \right)^{-1/\eta}\mathrm{d}u_{t}.\] (96)

Then, (94) can be rewritten as

\[\pi_{t}^{\bullet}=\operatorname*{arg\,min}_{\pi_{t}}\frac{1}{\eta}\log\left[ \int_{\mathbb{X}}Z_{t}(x_{t})^{\eta}\int_{\mathbb{U}}\widehat{\pi}_{t}(u_{t}|x_ {t})\left(\frac{\pi_{t}(u_{t}|x_{t})}{\widehat{\pi}_{t}(u_{t}|x_{t})}\right)^{ 1+\eta}\mathrm{d}u_{t}\mathrm{d}x_{t}\right].\] (97)

By Jensen's inequality, for any \(\eta>-1\), \(\eta\neq 0\), it holds that

\[\frac{1}{\eta}\log\left[\int_{\mathbb{X}}Z_{t}(x_{t})^{\eta}\int _{\mathbb{U}}\widehat{\pi}_{t}(u_{t}|x_{t})\left(\frac{\pi_{t}(u_{t}|x_{t})}{ \widehat{\pi}_{t}(u_{t}|x_{t})}\right)^{1+\eta}\mathrm{d}u_{t}\mathrm{d}x_{t}\right]\] \[\geq\frac{1}{\eta}\log\left[\int_{\mathbb{X}}Z_{t}(x_{t})^{\eta} \left(\int_{\mathbb{U}}\widehat{\pi}_{t}(u_{t}|x_{t})\frac{\pi_{t}(u_{t}|x_{t}) }{\widehat{\pi}_{t}(u_{t}|x_{t})}\mathrm{d}u_{t}\right)^{1+\eta}\mathrm{d}x_{t}\right]\] \[=\frac{1}{\eta}\log\left[\int_{\mathbb{X}}Z_{t}(x_{t})^{\eta} \mathrm{d}x_{t}\right],\] (98)

where the equality holds if and only if \(\pi(\cdot|x_{t})\ll\widehat{\pi}_{t}(\cdot|x_{t})\), and \(\pi_{t}(\cdot|x_{t})/\widehat{\pi}_{t}(\cdot|x_{t})\) is constant \(\widehat{\mathbb{P}}_{x_{t}}\)-almost everywhere. Here, \(\widehat{\mathbb{P}}_{x_{t}}\) is the probability distribution associated with \(\widehat{\pi}_{t}(\cdot|x_{t})\). Hence, the infimum (98) is attained only by \(\pi_{t}=\widehat{\pi}_{t}\). This completes the proof.

Details of the experiment

The implementation of the risk-sensitive SAC (RSAC) algorithm follows the stable-baselines3 [50] version of the SAC algorithm, which means that the RSAC algorithm also implements some tricks including reparameterization, minibatch sampling with a replay buffer, target networks, and double Q-network. Now, we introduce a series of hyperparameters listed in Table 1 shared for both SAC and RSAC algorithms.

As mentioned in Section 5, there were no significant differences in the control performance obtained or the behavior during training shown in Fig. 5 with those hyperparameters. However, when \(\eta\) is too small or too large, the training process becomes unstable due to the gradient vanishing problem and the gradient exponential growth problem, respectively, leading to training failure. To this end, we compare the robustness of the trained policies with RSAC (\(\eta\in\{-0.02,-0.01,0.01,0.02\}\)) and the standard SAC, which corresponds to \(\eta=0\), in the experiment. For each learned policy, we do trail for \(20\) times. For each trail, we take \(100\) sampling paths to calculate the average episode cost. In Fig. 3, the error bars depict the max and min values, and the points depict the mean value among the \(20\) trails. We change the length of the pole \(l\) in the Pendulum-v1 environment to test the robustness of the learned policies (\(l=1.0\) m in the original environment). For the training, we used an Ubuntu 20.04 server (GPU: NVIDIA GeForce RTX 2080Ti). The code is available at https://github.com/kaito-1111/risk-sensitive-sac.git.

\begin{table}
\begin{tabular}{l l} \hline \hline Parameter & Value \\ \hline optimizer & Adam [51] \\ learning rate & \(10^{-3}\) \\ discount factor & \(0.99\) \\ regularization coefficient & \(0.1\) \\ target smoothing coefficient & \(0.005\) \\ replay buffer size & \(10^{5}\) \\ number of critic networks & \(2\) \\ number of hidden layers (all networks) & \(2\) \\ number of hidden units per layer & \(256\) \\ number of samples per minibatch & \(256\) \\ activation function & ReLU \\ \hline \hline \end{tabular}
\end{table}
Table 1: SAC and RSAC Hyperparameters

Figure 5: Training process of RSAC (with different \(\eta\)) and SAC in terms of average episode cost.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims are made based on our theoretical results (Theorems 2, 3, 6, and Propositions 7, 8). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations are discussed in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Assumptions and a complete proof of all our results (Theorems 2, 3, 6, and Propositions 7, 8) are provided in the main paper and appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All the information is disclosed in Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We provide open access to the code via GitHub. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the training and test details are given in Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report error bars in Fig. 3. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. ** It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The information on the computer resources is provided in Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This work does not involve human subjects or participants, and there are no data-related concerns such as privacy issues. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The contribution of this paper is theoretical and we do not anticipate any direct societal impact of the work. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: In this work, we do not need data or models that have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: For the experiment, we use OpenAI Gym, and it is properly mentioned. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We submit the documentation as a supplementary material. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.