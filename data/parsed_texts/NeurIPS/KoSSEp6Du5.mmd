# E.T. Bench: Towards Open-Ended

Event-Level Video-Language Understanding

 Ye Liu\({}^{1,2}\), Zongyang Ma\({}^{2,3}\), Zhongang Qi\({}^{4}\)\({}^{*}\), Yang Wu\({}^{5}\), Ying Shan\({}^{2}\), Chang Wen Chen\({}^{1}\)

\({}^{1}\) The Hong Kong Polytechnic University \({}^{2}\) ARC Lab, Tencent PCG

\({}^{3}\) Chinese Academy of Sciences \({}^{4}\)Huawei Noah's Ark Lab \({}^{5}\) Tencent AI Lab

 coco.ye.liu@connect.polyu.hk

https://polyu-chenlab.github.io/etbench/

Work done at ARC Lab, Tencent PCGCorresponding authors

###### Abstract

Recent advances in Video Large Language Models (Video-LLMs) have demonstrated their great potential in general-purpose video understanding. To verify the significance of these models, a number of benchmarks have been proposed to diagnose their capabilities in different scenarios. However, existing benchmarks merely evaluate models through video-level question-answering, lacking fine-grained event-level assessment and task diversity. To fill this gap, we introduce **E.T. Bench** (**E**vent-Level & **T**ime-Sensitive Video Understanding **B**enchmark), a large-scale and high-quality benchmark for open-ended event-level video understanding. Categorized within a 3-level task taxonomy, E.T. Bench encompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length) under 8 domains, providing comprehensive evaluations. We extensively evaluated 8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that state-of-the-art models for coarse-level (video-level) understanding struggle to solve our fine-grained tasks, _e.g._, grounding event-of-interests within videos, largely due to the short video context length, improper time representations, and lack of multi-event training data. Focusing on these issues, we further propose a strong baseline model, **E.T. Chat**, together with an instruction-tuning dataset **E.T. Instruct 164K** tailored for fine-grained event-level understanding. Our simple but effective solution demonstrates superior performance in multiple scenarios.

## 1 Introduction

The recent advent of Multi-modal Large Language Models (MLLMs) [2, 91, 59, 115, 66, 17] has led to a substantial paradigm shift in visual-language understanding, moving away from designing task-specific models and collecting domain-specific data, towards developing general-purpose task-solvers for open-ended scenarios. By integrating LLMs with visual encoders, these models jointly benefit from perception abilities and powerful reasoning skills, showing remarkable performance in even unseened applications, demonstrating great potential for such scheme.

To effectively evaluate the capabilities of these models, a number of benchmarks [107, 45, 62, 14, 106, 48, 69, 50, 63] have been introduced to study their feasibilities in different scenarios. Yet, most of the benchmarks are focusing on image or short (seconds-long) video understanding, which require strong static scene understanding abilities but overlooking the fine-grained temporal information. Some recent works [86, 67] tend to evaluate MLLMs on longer videos, but they still leverage multiple-choice question-answering (MCQ) as their main task, lacking flexibilities for open-ended tasks. Nevertheless, none of the existing benchmarks are designed for multi-event or time-sensitive scenarios, thus theysuffer from severe single-frame biases, as can be seen in the comparable performances between Image- and Video-LLMs in these benchmarks [45, 48].

To address these issues and better understand the open-ended capabilities of these models, we propose **E.T. Bench**, a comprehensive benchmark for event-level and time-sensitive video understanding. As shown in Figure 1 and compared in Table 1, our benchmark significantly diverse from previous ones that it focus on time-sensitive understanding on long and multi-event videos. Our motivation is that a well-performing Video-LLM should possess the capability to precisely refer to and localize any events that align with user interests. Based on this assumption, we build our task taxonomy by summarizing four essential capabilities required for time-sensitive video understanding: _referring_, _grounding_, _dense captioning_, and _complex understanding_. Then, each capability is delineated with carefully designed tasks. The diversity of scenarios is ensured by meticulously collecting videos from 15 datasets covering 8 domains. A comprehensive data cleaning, annotation repurposing, instruction design, manual verification, and sampling pipeline is leveraged to generate 7.8K high-quality annotations.

We extensively evaluate 20 models, including 7 open-source Image-LLMs, 9 open-source Video-LLMs, and 4 commercial MLLMs, on E.T. Bench. The results reveal that state-of-the-art models on existing VideoQA benchmarks [45, 69, 48] struggle on our E.T. Bench, especially on _grounding_, _dense captioning_, and _complex understanding_ tasks. We attribute it to two key limitations of existing development pipelines for MLLMs. First, the discrete next-token prediction paradigm has natural drawbacks in numerical calculations [27, 38], limiting timestamp understanding and generation. Second, most existing video instruction-tuning datasets [47, 66, 49] predominantly comprise short videos with coarse-level annotations, bringing significant gap between training and real-world applications. To tackle these problems, we propose **E.T. Chat**, a novel time-sensitive Video-LLM that reformulates timestamp prediction as an embedding matching problem, serving as a

Figure 1: **Task definitions in E.T. Bench.** The 12 tasks derives from 4 essential capabilities for time-sensitive video understanding: _referring_, _grounding_, _dense captioning_, and _complex understanding_.

strong baseline on E.T. Bench. As for data, we also curate **E.T. Instruct 164K**, an instruction-tuning dataset tailored for multi-event and time-sensitive scenarios. Extensive comparisons demonstrate the effectiveness of the proposed model and dataset. We hope that the proposed benchmark, model, and dataset can inspire future research on Video-LLMs.

## 2 Event-Level & Time-Sensitive Video Understanding Benchmark

In this section, we illustrate the detailed pipeline employed to develop our E.T. Bench. As shown in Figure 2 (right), the pipeline begins with the definition of four essential capabilities for event-level and time-sensitive video understanding, _i.e._, _referring_, _grounding_, _dense captioning_, and _complex understanding_, arranged in increasing order of difficulty. For each capability, we design a series of tasks specifically for effective assessment of the respective capability. For each task, we meticulously select existing datasets with timestamp annotations provided by human annotators, and rewrite them into instruction-following formats according to task formulation, ensuring high quality and verisimilitude. The diversity of E.T. Bench is guaranteed by carefully choosing variable-length videos from different domains. Finally, a thorough manual check, filtering, and sampling process is conducted to eliminate unsatisfactory samples. Details for each step are introduced as follows.

### Hierarchical Task Taxonomy

To evaluate the open-ended Video-LLMs from various perspectives, we design a three-level task taxonomy depicted in Figure 2 (left). Definitions for capabilities and tasks are as follows.

**Referring** means the ability to comprehend time information from user inputs. For example, given a question _"What is the person doing from 23s to 35s?"_, the model has to understand which part of the video is the user referring to, and provide response with more consideration on that segment. For better quantifying the model performances, we formulate all the _referring_ tasks as multiple-choice question-answerings (MCQs), including 1) [RAR]_Referred Action Recognition_: Identify the action given a coarse timestamp hint (_e.g._, _"around 12s"_). The model has to determine the actual reference according to both the video and options. 2) [ECA]_Event-Caption Alignment_: Select the correct temporal boundary for a given caption. The model need to understand and distinguish multiple timestamps in the options. 3) [RVQ]_Referred Video Question-Answering_: Answer the question conditioning on a given segment. Each question is supplied with four candidates and an "unable to answer" option, denoting the case when it cannot be answered with given the segment.

**Grounding** indicates the ability to localize event- or moment-of-interests with accurate timestamps. It diverse from previous works [48; 101] that only consider coarse-level grounding, _e.g._, _"at the beginning/middle/end of the video"_. Outputs for _grounding_ tasks are open-ended, processed by rule-based parsers and evaluated with continuous metrics. The definitions of tasks are 1) [TVG]_Temporal Video Grounding_: Determine the temporal boundary of a single event according to the text description. 2) [EPM]_Episodic Memory_: Localize the event that can answer the given question in egocentric scenarios, _e.g._, _"Where is my backpack?"_. 3) [TAL]_Temporal Action Localization_: Detect and localize a series of segments containing the given action, _e.g._, finding all "golf swing" segments in a long video. 4) [EVS]_Extractive Video Summarization_: Provide a list of segments that can be merged to form a compact video summary (with around 15% of the total duration). 5) [VHD]_Video Highlight Detection_: Cherry-pick a single timestamp (_e.g._, _"15s"_) that can best reflect the highlight

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \hline \multicolumn{1}{c}{\multirow{2}{*}{**Benchmark**}} & \multicolumn{1}{c}{\multirow{2}{*}{**Domain**}} & \multicolumn{1}{c}{\multirow{2}{*}{**Annotator**}} & \multicolumn{1}{c}{\multirow{2}{*}{**\#Tasks**}} & \multicolumn{1}{c}{\multirow{2}{*}{**\#Samples**}} & \multicolumn{1}{c}{\multirow{2}{*}{**\#Videos**}} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}{c} **Avg./Max.** \\ **Duration** \\ \end{tabular} }} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}{c} **Long** \\ **Video** \\ \end{tabular} }} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}{c} **Event** \\ **Level** \\ \end{tabular} }} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}{c} **Time** \\ **Sensitive** \\ \end{tabular} }} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}{c} **Answer** \\ **Type** \\ \end{tabular} }} & \multicolumn{1}{c}{\multirow{2}{*}{
\begin{tabular}{c} **Evaluation** \\ **Method** \\ \end{tabular} }} \\ \hline SEED-Bench [45] & Action & LLM & 3 & 3,757 & 3,757 & 8 frames & ✗ & ✗ & ✗ & MCQ & Likelihood \\ EggSchema [67] & Egocentric & LLM & 1 & 5,031 & 5,031 & 180s/180s & ✗ & ✗ & MCQ & Likelihood \\ AutoSelf-ValVideo [16] & Open & Human & 9 & 327 & 158/101s & ✗ & ✗ & Open & GPT-4 \\ Video-Bench [69] & Open & LLM & 1 & 17,054 & 5,917 & 56s/3.599s & ✗ & ✗ & MCQ & Mixed\({}^{\ddagger}\) \\ TempCompsas [63] & Open & LLM & 4 & 7,540 & 410 & 12s/355 & ✗ & ✗ & ✗ & MCQ/Open & Rule/GPT \\ MVBench [48] & Open & Human\({}^{\dagger}\) & 1 & 4,000 & 3,673 & 15s/116s & ✗ & ✗ & ✗ & MCQ & Rule \\ \hline
**E.T. Bench** (Ours) & Open & Human\({}^{\dagger}\) & 12 & 7,289 & 7,002 & 129s/795s & ✓ & ✓ & ✓ & MCQ/Open & Rule \\ \hline \hline \end{tabular} \({}^{\dagger}\) Repurposed from existing datasets \({}^{\ddagger}\) Including next-token likelihood, T5 sentence similarity, and GPT-3.5 assisted evaluation

\end{table}
Table 1: Quantitative comparison between E.T. Bench and existing Video-LLM benchmarks.

moment corresponding to a query. Note that the formulations of [TAL], [EVS], and [VHD] have been modified (compared with previous works [83; 87; 44]) to fit the nature of LLMs.

**Dense Captioning** is a more complicated ability that requires jointly localize key-events and generate descriptions/summaries for each segment. This is more practical for storytelling or key-information extraction from long videos in comparison with video-level captioning on trimmed clips [103; 12]. We define two _dense captioning_ tasks according to different goals: 1) [DVC] _Dense Video Captioning_: Comprehensively describe all the events happened in the video. This is a general case with the goal of covering as much events as possible. 2) [SLC] _Step Localization and Captioning_: Identify and describe only the key-steps in instructional videos. In this case, the segments are shorter & disjoint and the step descriptions are more precise compared with [DVC].

**Complex Understanding** refers to the versatile integration of the aforementioned time comprehension and event localization, requiring the model to demonstrate proficient event-level and time-sensitive understanding. The two tasks are: 1) [TEM] _Temporal Event Matching_: Find and locate a similar event in the same video conditioning on the given segment. This involves a two-stage reasoning process that first identify the event in the given timestamps, then localize another segment with the most similar content. 2) [GVQ] _Grounded Video Question-Answering_: Answer the given multiple-choice question by selecting an option and localizing a segment that supports the answer. This is also a complex scenario requiring both understanding and localization abilities. Validating the localization results can help diagnose the reasoning process of Video-LLMs.

### Data Collection and Annotation

The key challenge of data collection is how to obtain videos with precise temporal boundary annotations. Previous works either prompt LLMs with frame-level information extracted from collections of experts [100; 76; 101; 34] or transform human-annotated moment tags (_e.g._, 5.2s) to boundaries (_e.g._, 3.2s to 7.2s) using pre-defined rules [56; 78]. These solutions can only generate temporal boundaries with substantial noise, which are not suitable for accurate model evaluation. Therefore, we meticulously curate multi-event videos from existing datasets with high-quality human-annotated timestamps, and repurpose the annotations by transforming them according to our task formulations. To ensure the diversity of scenarios, we carefully select 15 datasets from 8 domains, _i.e._, _indoor activities_, _tabletop_, _sports_, _egocentric_, _cooking_, _news & vlogs_, _how-to_, and _open_. All the videos are collected from val or test splits to prevent potential data leakage. For annotation generation, we develop for each task a thorough process containing pre-filtering, manual annotation repurposing, and instruction template design, presented in Figure 2 (right). Please refer to Section A for detailed task-specific pre-filtering criteria, repurposing methods, and instruction templates.

After annotation generation, we conduct a careful manual review on the samples, focusing on content suitability, annotation correctness, instruction clarity, scene diversity, video & event length, and task difficultly. Feedback from this review helped us actively optimize the generation process. Finally, to

Figure 2: **Left: Task taxonomy and sample distribution. Right: Generation pipeline for E.T. Bench. We conduct a thorough process of pre-filtering, annotation repurposing, instruction writing, manual check, and sampling to obtain high-quality fine-grained annotations. Details discussed in Section 2.**

balance the quality and efficiency of evaluation, we randomly sample up to 500 samples for each sub-task (task-source combination). In most cases, each video would only be sampled once.

### Benchmark Analysis

We present some statistics of the generated benchmark. Detailed comparisons are shown in Table 1. Overall, the proposed E.T. Bench contains 7,289 samples under a taxonomy of 4 capabilities, 12 tasks, and 20 sub-tasks. There are in total 7,002 unique videos originate from 15 datasets, covering 8 domains. The average duration of videos is 129.3s, with the minimum and maximum values of 6.2s and 795.0s, respectively. This differs from most existing benchmarks that have averaged durations only 10 \(\sim\) 20 seconds. We also have diverse answer types for different tasks, including both MCQ and open-ended styles. The evaluation process is purely rule-based without human or LLM integration, ensuring satisfied objectivity. Below we introduce more detailed analysis on the benchmark.

**Task and Sample Distribution.** Figure 2 (left) shows the distribution of tasks, sub-tasks, and samples in E.T. Bench. Here, a sub-task is defined as a task-source combination, _e.g._, [TVG] contains two sub-tasks from two source datasets. A large proportion of samples are in the _grounding_ category, as we emphasize the moment localization ability of modern Video-LLMs.

**Text Queries.** Figure 3 (left) shows the word cloud of text queries in E.T. Bench. Thanks to the wide range of video domains, the queries are also diverse in terms of both nouns and verbs. Most queries are human-centric, describing human activities or human-object interactions. Please refer to Section A.4 for distributions of nouns and verbs.

**Video Durations.** Figure 3 (right) shows the distribution of averaged video durations across tasks. Our videos have a wide spectrum of durations, where _referring_ tasks have relatively shorter videos, and _grounding_ and _captioning_ have longer ones. Our experimental results in Table 3 show that the duration of videos have significant influence on model performance.

## 3 Our Method

Extensive evaluations (in Table 3) reveal that even the state-of-the-art Video-LLMs cannot perform well on E.T. Bench, especially on the more complicated _grounding_, _dense captioning_, and _complex understanding_ tasks. We attribute this phenomenon to two key limitations of existing development pipelines for Video-LLMs: 1) **Model**: Existing models fall short in numerical modeling [21; 23], which are essential capabilities for arithmetic calculations - timestamps processing in our case; 2) **Data**: Both pre-training and instruction-tuning are conducted on short & single-event videos, leading to weak general understanding abilities for multi-event videos. To address these limitations, we propose **E.T. Chat**, a novel Video-LLM that reformulates timestamp prediction as an embedding matching problem, serving as a strong baseline on E.T. Bench. we also curate **E.T. Instruct 164K**, an instruction-tuning dataset tailored for multi-event and time-sensitive video understanding.

### Model

Figure 4 presents the overall architecture of E.T. Chat. Given a video frame \(\mathbf{V}_{t}\in\mathbb{R}^{H\times W\times 3}\) sampled at time \(t\in T\), where \(H\) and \(W\) are the height and width, we first leverage a frozen visual encoder

Figure 3: **Left: Word cloud of text queries shows a considerable degree of diversity. Right: Distribution of averaged video durations (in seconds) across 12 tasks.**

\(E_{v}\) to convert it into patch embeddings \(\mathbf{P}_{t}\in\mathbb{R}^{K\times C}\), where \(K\) and \(C\) are the number of patches and feature dimension. To preserve high temporal resolution while reducing redundant compute, we adopt a frame compressor \(E_{c}\) to merge and project patch embeddings to a single token \(\mathbf{e}_{v}^{t}\in\mathbb{R}^{1\times D}\), where \(D\) is the embedding dimension of LLM. The compressed frame tokens \(\{\mathbf{e}_{v}^{t}\}_{t=1}^{T}\) are then concatenated with text tokens \(\{\mathbf{e}_{q}^{n}\}_{n=1}^{N}\) and sent into LLM for response generation.

**Frame Compression.** As illustrated in Figure 5, the frame compression \(E_{c}\) arises from [51] and consists of a Q-Former [46]\(E_{q}\) with \(M\) learnable queries, a context aggregator \(E_{a}\), and a projector \(E_{p}\). For each time step, \(E_{q}\) accepts patch embeddings \(\mathbf{P}_{t}\) and the text prompt \(\mathbf{T}\) as inputs, and resamples them into learnable queries \(\mathbf{Q}_{t}\in\mathbb{R}^{M\times C}\). Then, \(E_{a}\) merges \(\mathbf{Q}_{t}\) with \(\mathbf{P}_{t}\) and compresses them into a single token. \(E_{p}\) finally projects it to the same embedding space as LLM. In particular, the context aggregator \(E_{a}\) is built upon cross attention module [96], formulated as follows:

\[\mathbf{a}=\mathrm{Softmax}(\frac{(\mathbf{w}_{q}\times\mathbf{Q}_{t})^{\top} \times(\mathbf{w}_{k}\times\mathbf{P}_{t})}{\sqrt{C}})\] (1)

\[\mathbf{F}_{t}=\mathrm{Mean}(\mathbf{a}\times\mathbf{P}_{t}+\mathbf{Q}_{t})\] (2)

Here, \(\mathbf{F}_{t}\in\mathbb{R}^{1\times C}\) is the compressed frame token for time \(t\), containing text-conditioned visual information. This process can be parallelized across all frames. The projected video token sequence \(\mathbf{e}_{v}\) is then concatenated with text tokens \(\mathbf{e}_{q}\) to form the inputs with shape \((T+N)\times D\) for LLM.

**Timestamp Prediction as Embedding Matching.** Our key insight focuses on the design of timestamp processing. As discussed in Section B.1, we claim that directly generating continuous signals (_i.e.,_ timestamps in our case) via discrete next-token prediction is sub-optimal. Motivated by the characteristic of Transformers that they are naturally good at selective copying rather than numerical calculations [27; 38], we propose to reformulate timestamp prediction as an _embedding matching_ problem. That is, we train the model to generate/copy embeddings of video frames that it would like to refer to, and obtain timestamps by matching these embeddings back to the video.

Specifically, we define a special token <vid> used to stimulate the matching process. When <vid> is generated during inference, _e.g._, the model outputs _"the event happens around <vid>"_, this token is utilized to match a video frame token, such that the desired timestamp can be easily obtained from the matched frame index. For example, for a video sampled to 1 FPS, if <vid> is matched to the \(i\)-th frame, then <vid> means the \(i\)-th second of the video. The matching process is designed to be simple and efficient. We denote the \(l\)-th layer hidden states of <vid> token and video frame tokens as \(\mathbf{h}_{vid}^{l}\in\mathbb{R}^{1\times D}\) and \(\mathbf{h}_{frm}^{l}\in\mathbb{R}^{T\times D}\), respectively. During matching, two MLPs \(E_{vid}\) and \(E_{frm}\) are first leveraged to project the hidden states to the alignment space \(g\):

\[\mathbf{g}_{vid}=E_{vid}(\mathbf{h}_{vid}^{L-1}),\quad\mathbf{g}_{frm}=E_{frm }(\mathbf{h}_{v}^{L})\] (3)

Figure 4: **Overall architecture of E.T. Chat.** We reformulate timestamp prediction as an embedding matching problem. See Section 3 for details.

Figure 5: **Detailed illustration of frame compressor.** It accepts video patch embeddings \(\mathbf{P}_{t}\) and the text prompt \(\mathbf{T}\) as inputs, and compress video frame features into a single token.

Here, \(L\) refers to the total number of LLM layers. We extract <vid>'s hidden states from the second-last layer to preserve a larger feature range [30]. Subsequently, we compute the cosine similarities between \(\mathbf{g}_{vid}\) and all \(\{\mathbf{g}_{frm}^{t}\}_{t=1}^{T}\) to obtain the matched frame index \(t_{match}\):

\[\mathbf{s}=\frac{\mathbf{g}_{vid}\cdot\mathbf{g}_{frm}}{||\mathbf{g}_{vid}||_ {2}\cdot||\mathbf{g}_{frm}||_{2}}\in\mathbb{R}^{1\times T},\quad t_{match}= \operatorname*{argmax}(\mathbf{s})\] (4)

The frame index \(t_{match}\) is then multiplied with frame rate \(r\) to generate the real timestamp in seconds. Through this operation, the direct prediction of timestamps is replaced by embedding matching, which is easier to learn as a selective copying problem by Transformer-based models. For the case when <vid> occurs in inputs, the input features of <vid> are added with the corresponding frame features. During training, an extra binary matching loss is utilized:

\[\mathcal{L}_{matching}=-\frac{1}{T}\sum_{t=1}^{T}\mathbf{y}_{t}\cdot\log( \mathbf{s}_{t})\] (5)

Here, \(\mathbf{y}_{t}\) denotes the binary label indicating whether \(t\) is the ground truth frame. \(\mathcal{L}_{matching}\) is added with the original language modeling loss \(\mathcal{L}_{language}\) to jointly optimize the model.

**Numerical Continuity.** The matching process above still cannot preserve numerical continuities among them, as the hidden states of adjacent frames may be far away from each other. We introduce two modifications to effectively alleviate this problem. First, we observe that the causal self-attentions in LLM block out the bi-directional information flow. This is reasonable for text but limits the ability of video understanding. Thus, we allow bi-directional attentions among video tokens. Second, we introduce a smoothed label \(\frac{1}{\alpha^{|t-t_{gt}|}}\) to replace the binary label \(\mathbf{y}_{t}\) in Eq. 5, where \(\alpha\) is a hyper-parameter controlling the extent of smoothing, and \(t_{gt}\) refers to the ground truth frame index.

### Instruction-Tuning Dataset

The proposed E.T. Instruct 164K contains multi-event understanding samples generated from 14 source datasets, illustrated in Table 2. It covers a wide range of event-level understanding tasks, including temporal grounding, summarization, highlight detection, dense captioning, and question-answering. More details about the generation process are presented in Section C.

## 4 Experiments

### Evaluation Settings

As different tasks in E.T. Bench are under different settings with diverse output formats. A single metric (_e.g._, accuracy) like existing benchmarks is not sufficient. To balance the quantity of metrics and the ease of ranking, we unify the metrics within each capability and leverage accuracy for _referring_ tasks, F1 score for _grounding_ tasks, F1 score and sentence similarity for _dense captioning_ tasks, and recall for _complex understanding_ tasks. Detailed metrics are introduced in Section D.1.

### Main Results

We extensively evaluate 7 open-source Image-LLMs, 9 open-source Video-LLMs, and 4 commercial MLLMs on E.T. Bench. Details of each model are introduced in Section D.2. For Image-LLMs, we uniformly sample 8 frames and add an extra prompt indicating the video duration as a hint for timestamps. For Video-LLMs, we use their default number of frames as inputs. Commercial MLLMs are evaluated by calling APIs on a subset with 470 samples. The inputs for GPT-4V and GPT-4o are aligned with Image-LLMs. For Gemini-1.5 models, raw videos are directly uploaded.

The evaluation results are presented in Table 3. We report the metrics averaged among sub-tasks due to space limit. The _Random_ in the first row refers to random guessing. We also provide comparisons and ranking in Figure 6. Below we summarize our key findings from the results.

**Performance gap between Image- and Video-LLMs.** We observe that on _referring_ tasks, most Image- and Video-LLMs perform at the same level. Some Image-LLMs such as XComposer and Qwen-VL-Chat can even beat most Video-LLMs. This is because the videos for _referring_ are generally short, as compared in Figure 3 (right), such that the sampled 8 frames cover most information. The gap becomes larger on tasks with longer videos such as [TVG] and [TAL], demonstrating the importance of temporal modeling on E.T. Bench compared with other benchmarks.

**Strong Video-LLMs on existing benchmarks struggle on E.T. Bench.** Some state-of-the-art Video-LLMs on existing benchmarks, _e.g._, Video-LLaMA-2 and PLLaVA, are less effective on our E.T. Bench, especially on _grounding_ and _dense captioning_ tasks. We attribute this to the single-frame bias caused by both model architecture and training data. It also motivate us to consider the balance between spatial and temporal modeling in Video-LLMs.

**Some Video-LLMs fail to follow instructions.** During evaluation, we also notice that some models, _e.g._, Video-LLaVA and Video-LLaMA-2, fail to generate outputs in desired formats for some tasks even with carefully designed instructions and examples. For instance, Video-LLaVA can only generate repeated text outputs without any timestamps for [EVS], while Video-LLaMA-2 faces the similar problem on almost all _grounding_ tasks. we claim that this is due to the severe overfitting on their instruction-tuning data, which do not contain any timestamps outputs.

**Some Image-LLMs performs exceptionally well.** Qwen-VL-Chat and Bunny-Llama-3-8B-V are two models producing relatively good results compared with other Image-LLMs and even some Video-LLMs. On [TVG] and [DVC], Qwen-VL-Chat achieves 15.6 and 21.3 F1 scores, respectively, surpassing a number of Video-LLMs. But there is still a significant gap when compared with best-performing Video-LLMs such as TimeChat and LITA.

**Time-sensitive Video-LLMs are the first-class models.** VTimeLLM, TimeChat, and LITA are three Video-LLMs with explicit optimizations for timestamps modeling, such that they can persistently follow instructions and generate considerable responses.

**Commercial MLLMs are still competitive.** Even with 8-frame inputs, GPT-4V and GPT-4o show their significance compared with open-source models on some tasks such as [TVG], [TAL], and [DVC]. By supporting direct video inputs, Gemini-1.5 series achieve the strongest performance on a number of tasks in E.T. Bench, including [RAR], [EVC], [RVQ], [TVG], [TAL], [VHD], and [TEM].

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{4}{c}{**Referring**} & \multicolumn{4}{c}{**Grounding**} & \multicolumn{4}{c}{**Dense Captioning**} & \multicolumn{4}{c}{**Complex**} \\ \cline{2-13}  & \multicolumn{2}{c}{RAR\({}_{\text{\tiny{G}}w}\) EVC\({}_{\text{\tiny{AC}}}\) RVQ\({}_{\text{\tiny{DVC}}}\)} & \multicolumn{1}{c}{TVG\({}_{\text{\tiny{F}}}\)} & \multicolumn{1}{c}{FPMF\({}_{\text{\tiny{F}}}\)} & \multicolumn{1}{c}{TA\({}_{\text{\tiny{F1}}}\) EVS\({}_{\text{\tiny{F1}}}\)} & \multicolumn{1}{c}{VID\({}_{\text{\tiny{F1}}}\)} & \multicolumn{1}{c}{DVF\({}_{\text{\tiny{F1}}}\)} & \multicolumn{1}{c}{DVF\({}_{\text{\tiny{F1}}}\)} & \multicolumn{1}{c}{DSC\({}_{\text{\tiny{SW}}}\) SLC\({}_{\text{\tiny{F1}}}\)} & \multicolumn{1}{c}{SL\({}_{\text{\tiny{CNN}}}\)} & \multicolumn{1}{c}{TEM\({}_{\text{\tiny{DVC}}}\) GVQ\({}_{\text{\tiny{DVC}}}\)} \\ \hline _Random_ & 25.0 & 25.0 & 20.0 & – & – & – & – & – & – & – & – & – & – & – & – & – \\ \hline _Open-source image-LLMs: All models are 8 uniformly sampled frames as inputs._ & _Pramps_ & _here been added with hints about timestamps._ & & & & & & & & & & & & \\ \hline
1LAVA-1.5 [58] & 34.2 & 27.4 & 26.2 & 6.1 & 1.9 & 7.8 & 2.4 & 30.9 & 14.5 & 11.5 & 0.9 & 9.5 & 7.7 & 0.0 \\
1LAVA-InternalM2 [11] & 34.0 & 34.8 & 37.0 & 2.7 & 0.1 & 0.3 & 0.2 & 32.3 & 16.9 & 8.5 & 0.1 & 4.7 & 7.2 & 1.5 \\ mPLUG-Owl2 [105] & 37.8 & 26.4 & 34.6 & 1.1 & 0.2 & 3.0 & 4.1 & 36.8 & 0.1 & 8.1 & 0.1 & 7.7 & 6.2 & 0.0 \\ XCompeser [11] & 33.0 & 19.6 & 40.2 & 49.9 & 1.5 & 9.9 & 2.8 & 28.9 & 5.4 & 5.9 & 2.7 & 9.0 & 10.5 & 0.0 \\ Bunny-Llama-V [31] & 33.2 & 27.4 & 26.6 & 7.0 & 0.1 & 5.1 & 0.4 & 30.6 & 13.5 & 8.8 & 0.1 & 7.6 & 7.2 & 0.0 \\ MinCPM-V-2.5 [93] & 37.6 & 28.0 & 37.6 & 2.0 & 0.1 & 4.4 & 13.4 & 18.7 & 6.2 & 11.8 & 1.4 & 9.7 & 0.7 & 0.0 \\ Qwen-VL-Chat [7] & 33.4 & 32.2 & 33.6 & 16.2 & 4.0 & 10.7 & 16.3 & 34.4 & 17.4 & 13.8 & 6.2 & 13.1 & 3.2 & 1.5 \\ \hline _Open-source Video-LLMs: All models are their default numbers of frames as inputs._ & & & & & & & & & & & & & \\ \hline Video-ChaMP(F) [66] & 22.6 & 24.2 & 23.0 & 7.0 & 1.3 & 15.1 & 8.4 & 28.8 & 8.8 & 11.3 & 5.7 & 10.2 & 15.9 & 0.0 \\ Video-LLaVA [53] & 33.6 & 33.0 & 22.6 & 7.0 & 1.9 & 15.0 & 0.3 & 28.9 & 28.0 & 15.0 & 0.9 & 8.3 & 7.5 & 0.1 \\ LLaMA-VID [51] & 30.4 & 38.4 & 28.8 & 5.5 & 1.2 & 8.0 & 1.4 & 30.0 & 27.1 & 12.6 & 5.2 & 11.1 & 7.0 & 0.9 \\ Video-LLaMA-2 [110] & 28.8 & 27.4 & 28.0 & 0.1 & 0.0 & 0.0 & 1.5 & 0.6 & 14.5 & 0.0 & 15.2 & 0.0 & 0.1 \\ PLLAVA [104] & 33.8 & 22.6 & 31.8 & 6.9 & 1.1 & 5.7 & 0.3 & 28.9 & 13.3 & 10.6 & 9.7 & 11.8 &

**E.T. Chat fills the gap between open-source and commercial MLLMs.** Benefit from the novel timestamps processing design and the multi-event instruction-tuning data, E.T. Chat achieves state-of-the-art performance among open-source MLLMs on most tasks, and obtain comparable results as commercial MLLMs. Notably, significant improvements can be viewed on [EPM], [VHD], [TEM], and [GVQ]. Implementation details and ablation studies can be viewed in Section B.2 and Section D.4, respectively.

## 5 Related Work

**Video Large Language Models.** Video-LLMs [66; 53; 51; 110; 86; 82; 34] represent a class of intelligent chatbots capable of understanding videos and perform various open-ended tasks. Generally, a Video-LLM comprises a visual encoder [77; 109] for perception, a projector [37; 46] for feature alignemnt, and a LLM [94; 94; 18; 11] for reasoning and response generation. VideoChat [47] and Video-ChatGPT [66] are two earliest attempts in this direction. Following works tend to provide better solutions via adding audio modality [110], joint training on images and videos [53; 40], or performing alignment before projection [53]. A recent trend [33; 82; 34; 76] involves the integration of Video-LLMs with time-sensitive understanding capabilities, while their solutions remain sub-optimal. Therefore, we propose to reframe timestamp generation as an embedding matching problem.

**Benchmarks for Video-LLMs.** The increasing number of Video-LLMs motivate the development of benchmarks [45; 16; 69; 67; 48; 63]. Among the earliest is SEED-Bench [45], a MLLM benchmark that supports both Image-LLMs and Video-LLMs and offers three evaluation dimensions in the realm of temporal modeling. AutoEval-Video [16] and Video-Bench [69] are designed specifically for videos. They employ LLMs for either QA generation or model evaluation. MVbench [48] provides a novel scheme to repurpose existing datasets for Video-LLM evaluation. Recent benchmarks also expand their scope and consider the ability of understanding extremely long videos [67; 86] or comprehending fine-grained temporal order information [63]. Nevertheless, none of the benchmarks have been designed for multi-event and time-sensitive understanding. In response to this gap, we introduce E.T. Bench, the first benchmark providing comprehensive evaluations on these scenarios.

## 6 Conclusion

In this work, we introduce **E.T. Bench**, a large-scale and comprehensive benchmark for multi-event & time-sensitive video-language understanding. Our benchmark encompasses a wide range of tasks on diverse video domains, evaluating multiple capabilities of Video-LLMs. Our experimental results reveal that current model designs and instruction-tuning data for Video-LLMs exhibit limitations in their capacity for timestamp representation and fine-grained multi-event modeling. To address these challenges, we further develop a novel model **E.T. Chat**, in conjunction with a multi-event instruction-tuning dataset, **E.T. Instruct 164K**, which serves as a robust baseline solution for such scenarios. We hope that the proposed benchmark, model, and instruction-tuning dataset will inspire future research on developing Video-LLMs.

Figure 6: **Left: Performance comparison between E.T. Chat and representative models. Right: Ranking of MLLMs on E.T. Bench, where red means higher ranks and blue represents lower ranks.**

## Acknowledgements

This work was supported in part by Hong Kong Research Grants Council GRF-15229423.

## Appendix

In the appendix, we provide more details about the proposed benchmark, model, and instruction-tuning dataset to complement the main paper. Additional analysis, ablation studies, visualizations, and discussions are also incorporated. Below is the table of content.

* Pre-filtering Criteria
* Annotation Repurposing and Cleaning
* Instruction Templates
* Design Space for Timestamp Processing
* Implementation Details
* Instruction-tuning Dataset
* Task Selection
* Data Collection and Instruction Generation
* Evaluation Metrics
* More Benchmark Results
* Ablation Studies
* Qualitative Results
* Limitations & Future Work

## Appendix A Benchmark

### Pre-filtering Criteria

Table 4 presents the pre-filtering criteria for each source dataset when generating E.T. Bench.

### Annotation Repurposing and Cleaning

We summarize the detailed annotation repurposing and cleaning process for each task as follows.

[RAR] **Referred Action Recognition.** We adopt the _action localization_ subset of Perception Test [74] for this task. We first select videos with at least three different actions, in which one action is sampled as ground truth. We then sample two other actions from the same video as intra-video distracters, and one action from other videos as inter-video distracter. The coarse timestamp hint is sampled from the segment containing only the ground truth action.

[ECA] **Event Caption Alignment.** We utilize Charades-STA [24] as data source. For each event-query pair, we randomly generate distracters (temporal boundaries) with 0.5\(\times\) to 2\(\times\) lengths compared with the ground truth, and ensure the temporal IoUs between any two options are no more than 0.5.

[RVQ] **Referred Video Question-Answering.** We leverage the high quality QA pairs from _interaction_ and _sequence_ question types of STAR [102]. Since the original annotations only contain question-relevant temporal boundaries, we randomly pick 20% of the QA pairs and modify their boundaries to have no overlap with the original ones, in order to synthesis the case when the question cannot be answered within the given boundary. An extra "unable to answer" option is added to all QA pairs.

[MISSING_PAGE_FAIL:11]

[TVG]**Temporal Video Grounding.** Two datasets, _i.e._, Charades-STA [24] and QVHighlights [44] are chosen. We filter out the samples with event duration shorter than 2s or longer than 50s, as these are generally noisy samples. On QVHighlights, only the samples with single events are chosen to align with our formulation.

[EPM]**Episodic Memory.** We employ Ego4D-NLQ [26] and conduct a thorough data cleaning & verification process. First, we perform a rule-based fix for noisy questions. Some common cases are: 1) Question starts with an additional "Query Text:" string. 2) Typos such as "I" \(\rightarrow\) "i" or "I". 3) Unclear references such as "person x". We also found that some questions are ambiguous in the context of long videos, so we randomly crop the all the videos to 300-second long.

[TAL]**Temporal Action Localization.** We adopt Perception Test [74], THUMOS'14 [39] (test split), and THUMOS'15 [25] (val split). Videos with ambiguous action classes, _e.g._, _moving object(s) around_ and _other_ in Perception Test, and _ambiguous_ in THUMOS, are discarded. To reduce the difficulty, samples with more than 10 ground truth moments are filtered out as well.

[EVS]**Extractive Video Summarization.** We repurpose TVSum [87] and SumMe [29] to generate samples. In conventional video summarization, each frame is annotated with a probability of being the summary, which is incompatible with Video-LLMs that cannot strictly produce temporally-aligned frame-level scores, as can be seen in the near-random results in [82, 76]. Therefore, we reformulate it to predicting a set of temporal boundaries that compose the summary. Ground truths are obtained by sorting the frame-level scores and generate boundaries for consecutive frames with top-15% scores. When a video is annotated by multiple annotators (_i.e._, in TVSum), we simply average the scores. This reformulation helps models persistently generate reasonable results.

[VHD]**Video Highlight Detection.** Samples are generated from QVHighlights [44] and YouTube Highlights [89] using a similar method as [EVS], but for highlights we only consider the frames with the highest scores. That means, ground truths are the frames with the highest highlight saliencies. During inference, a prediction is considered correct if the timestamp falls into any of the temporal boundaries. We directly utilize the text queries in QVHighlights as highlight queries. For YouTube Highlights, the domains are used, and videos with highlights covering more than 90% are discarded.

[DVC]**Dense Video Captioning.** We utilize YouCook2 [113] and HiREST [108]. Although HiREST only contains instructional videos, it is still considered as [DVC] rather than [SLC] because of the large event coverage. We also trimmed out the opening and closing scenes of HiREST videos.

[SLC]**Step Localization and Captioning.** We select CrossTask [116] and HT-Step [3] for this task. For CrossTask, we filter out the samples with wrong annotations, _e.g._, repeated steps, and select only the videos with all steps longer than 2s to avoid ambiguous annotations. For HT-Step, we keep only the videos with at least 2 steps and remove the samples with incorrect temporal orders.

[TEM]**Temporal Event Matching.** We repurpose Perception Test [74] and QVHighlights [44] for this novel task, where the former focus on actions and the latter is for general events. We select videos with actions (excluding the _other_ category) occurs multiple times from Perception Test, and sample one temporal boundary as the input reference. Other boundaries are used as ground truths. For QVHighlights, samples with one query referring to multiple disjoint moments are used.

[GVQ]**Grounded Video Question-Answering.** We adopt QAEgo4D [9] which naturally contains both QA pairs and corresponding timestamps derived from Ego4D-NLQ [26]. To control the task difficulty, we randomly crop all the videos to 150-second long. Typos in QA pairs are fixed.

### Instruction Templates

The instruction templates for different tasks are shown in Table 5. To ensure the models give responses in desired formats, each instruction starts with a sentence introducing the domain/title of the video, followed by detailed requirements about the task. We also add an explicit statement about the format of response and an example as guidance. The model outputs are passed through carefully designed rule-based parsers for answer extraction before evaluation.

### Distribution of Queries

We visualize the frequency distribution of verbs and nouns in E.T. Bench in Figure 7 and Figure 8, respectively. The distribution histograms demonstrate the diversity of queries in E.T. Bench.

\begin{table}
\begin{tabular}{|c|p{14.2pt}|p{142.3pt}|p{142.3pt}|} \hline \hline
**Type** & **Task** & **Instruction Template** & **Example Response** \\ \hline \multirow{6}{*}{**FORD**} & \multirow{3}{*}{**FORD**} & You are given a video about indoor activities. Watch the video carefully and identify the action around “timney” by choosing from a set of options. The format of your response should be: “Best Option: (your choice)”. For example: “Best Option: (B)”. Now I give you the options: (A) “option: (B) “option: (C) “option: (D) “option: Please provide your choice. & **Best Option: (C)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about indoor activities. Watch the video carefully and select the moment that can be used described by the sentence “equery”. The format of your response should be: “Best Option: (your choice)”. For example: “Best Option: (A) “. Now I give you the options: (A) “timney - (B) “timney - (C) “timney - (D) “timney - (C) “timney - (C) “timney - (D) “timney - (C) “timney - (D) “timney - (C) “timney - (D) “timney - (C) “timney - (D) “timney - (C) “timney - (D) “timney - (C) “timney - & **FORD** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about indoor activities. Watch the video carefully and a multiple choice question solely based on the event in “timney - (Timney). The format of your response should be: “Best Option: (Your choice)”. For example: “Best Option: (C)”. You may select “mable to answer” if the question can not be answered based on the provided moment. Now I give you the question: “question: (E) “option: (B) “option: Please provide your choice. & **Best Option: (D)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about daily activities / indoor activities. Watch the video carefully and find a visual event described by the sentence: “equery”. The format of your response should be: “The event happens in “timney - (Timney)”. You must represent start and end times in seconds. For example: “The event happens in 10.2 - 12.8 seconds”. & **Best Option: (Your choice)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given an egocentric video about daily activities. Watch the video carefully and find a visual event that can answer the question: “question: (Your choice)”. The format of your response should be: “The event happens in “timney - (Timney). You must represent start and end times in seconds. For example: “The event happens in “timney - (Timney). & **The event happens in “timney - (Timney)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video containing a series of actions. Watch the video carefully and find the visual events belonging to the action category: “action”. The format of your response should be: “The action happens in “start time - - (end time), and “start time - (end time) - (end time). You must represent start and end times in seconds. For example: “The action happens in 4.2 - 6.8, 7.5 - 10.3, 15.1 - 18.6, and 23.4 - 27.5 seconds”. & **The same memory locates in “timney - (Timney)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about daily activities. Watch the video carefully and find a highlight moment according to the sentence / its domain: “equery”. The format of your response should be: “The highlight moment happens at “timney”. You must represent time in seconds. For example: “The highlight moment happens at 26.8 seconds”. & **The same memory locates in “timney - (Timney)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about “equery”. Watch the video carefully and densely describe all the events in it. For each event, you need to determine the start and end times and provide a concise description. The format of your response should be: “cstart time” - send time>, \(\diamond\) description”. For example: “90 - 102 seconds, spend margarine on two slices of white bread. 114 - 127 seconds, place a slice of the bread.” & **The highlight moment happens at “timney.** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about “classo”. Watch the video carefully and identify all the key steps in it. For each step, you need to determine the start and ends times and provide a concise description using a few words. The format of your response should be: “cstart time” - send time>, \(\diamond\) description”. You must represent start and end times in seconds. For example: “24.8 - 30.2 seconds, cut apple. 35.6 - 40.4 seconds, wash dishes.”. & **The same memory locates in “timney - (Timney)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about daily activities / containing a series of actions. Watch the video carefully and identify the event in “timney - (Timney), then localize a different moment that contains the most similar event. The format of your response should be: “The similar event happens in “start time” - send time>. You must represent start and end times in seconds. For example: “The similar event happens in 16.8 - 20.4 seconds”. & **The same memory locates in “timeney - (Timney)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given an egocentric video about daily activities. Watch the video carefully and answer an multiple choice question. Your answer should contain a choice of the best option and a relevant moment that supports your answer. The format of your response should be: “Best Option: (your choice)”. The relevant event happens in “static time - send time>. Now I give you the question: “question”. The options are (A) “option: (B) “option: (C) “option: (D) “option: Please provide your choice. & **Best Option: (C)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about indoor activities. Watch the video carefully and answer an multiple choice question. Your answer should contain a choice of the best option and a relevant moment that supports your answer. The format of your response should be: “Best Option: (your choice)”. The relevant event happens in “static time - send time>. Now I give you the question: “question”. The options are (A) “option: (B) “option: (C) “option: (D) “option: Please provide your choice and the relevant moment. & **Best Option: (D)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given an egocentric video about daily activities. Watch the video carefully and find a visual event that can answer the question: “question”. The format of your response should be: “The event happens in “timney - (Timney). You must represent start and end times in seconds. For example: “The event happens in “timney - (Timney). & **Best Option: (D)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given an egocentric video about daily activities. Watch the video carefully and find a visual event that can answer the question: “question””. The format of your response should be: “The event happens in “timney - (Timney). You must represent start and end times in seconds. For example: “The event happens in “timney - (Timney). & **The same memory locates in “timney - (Timney)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about daily activities. Watch the video carefully and find a visual event that can answer the question: “question””. The format of your response should be: “The event happens in “static time - send time>. You must represent start and end times in seconds. For example: “The event happens in “timney - (Timney). & **The same memory locates in “timney - (Timney)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about daily activities. Watch the video carefully and identify the action around “timney - (Timney). You choose from a set of options. The format of your response should be: “Best Option: (your choice)”. For example: “Best Option: (B) “option: (C) “option: (D) “option: (D) “timney - (Timney). You must represent start and end times in seconds. For example: “The similar event happens in “static time - send time>. You must represent time in seconds. For example: “The highlight moment happens at 26.8 seconds”. & **The same memory locates in “timney - (Timney)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about daily activities. Watch the video carefully and describe all the events in it. For each event, you need to determine the start and end times and provide a concise description. The format of your response should be: “cstart time” - send time>, \(\diamond\) description”. For example: “90 - 102 seconds, spend margarine on two slices of white bread. 114 - 127 seconds, place a slice of the bread.” & **The similar event happens in “timney - (Timney)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about “classo”. Watch the video carefully and identify all the key steps in it. For each step, you need to determine the start and ends times and provide a concise description using a few words. The format of your response should be: “cstart time” - send time>, \(\diamond\) description”. You must represent start and end times in seconds. For example: “24.8 - 30.2 seconds, cut apple. 35.6 - 40.4 seconds, wash dishes.”. & **The same memory locates in “timney - (Timney)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about daily activities / containing a series of actions. Watch the video carefully and identify the event in “timney - (Timney), then localize a different moment that contains the most similar event. The format of your response should be: “The similar event happens in “static time - send time”. You must represent start and end times in seconds. For example: “The similar event happens in 16.8 - 20.4 seconds”. & **The same memory locates in “timeney - (Timney)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given an egocentric video about daily activities. Watch the video carefully and answer an multiple choice question. Your answer should contain a choice of the best option and a relevant moment that supports your answer. The format of your response should be: “Best Option: (your choice), the relevant event happens in “static time - send time>. Now I give you the question: “question”. The options are (A) “option: (B) “option: (C) “option: (D) “option: Please provide your choice and the relevant moment. & **Best Option: (C)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about indoor activities. Watch the video carefully and answer an multiple choice question. Your answer should contain a choice of the best option and a relevant moment that supports your answer. The format of your response should be: “Best Option: (your choice)”. The relevant event happens in “static time - send time>. You I give you the question: “question”. The options are (A) “option: (B) “option: (C) “option: (D) “option: Please provide your choice and the relevant moment. & **Best Option: (D)** \\ \cline{2-3}  & \multirow{3}{*}{**FORD**} & You are given a video about indoor activities. Watch the video carefully and answer an multiple choice question. Your answer should contain a choice of the best option and a relevant moment that supports your answer. The format of your response should be:

## Appendix B Method

### Design Space for Timestamp Processing

As illustrated in Figure 9, existing MLLMs typically handle timestamps (for videos) or coordinates (for images) in three ways: 1) **Numerical Expressions**[75, 13, 82, 33]: Representing numbers directly in the form of text. This straightforward strategy loses continuity among numbers [21, 23]. A wrong prediction could be extremely far away from the ground truth. 2) **Special Tokens**[15, 99, 34, 76]: Defining a set of special tokens to quantize time/position into a fixed number (typically 100 to 300) of bins. This solution inevitably brings severe quantization loss, and it is not flexible for videos with variable lengths. Moreover, introducing too many new tokens into the vocabulary would break the pre-trained distribution of LLMs, making them hard to optimize without post pre-training.

3) **External Modules**[43, 65, 30]: Leveraging pre-trained external models (_e.g._, SAM [41]) for grounding. This would introduce extra parameters and latency to LLMs. It is not directly compatible with videos as well, as existing temporal grounding models [44, 61, 68, 57, 60] are domain-specific and hard to generalize to all scenarios like SAM. Therefore, we propose to reformulate timestamp prediction as an _embedding matching_ problem.

Figure 8: **Frequency distribution of nouns in E.T. Bench.** We only visualize the top 25 out of 3,090 nouns for clarity. The x- and y-axes denote the nouns and their frequencies, respectively.

Figure 7: **Frequency distribution of verbs in E.T. Bench.** We only visualize the top 25 out of 461 verbs for clarity. The x- and y-axes denote the verbs and their frequencies, respectively.

### Implementation Details

We adopt the pre-trained ViT-G/14 from EVA-CLIP [22] as visual encoder. The architecture of frame compressor and LLM are based on Q-Former [19, 46] and Phi-3-Mini-3.8B [1], respectively. We first pre-train the model following the stage-1 and stage-2 recipes in [51], then activate the MLP projectors \(E_{vid}\) & \(E_{frm}\) and fine-tune them on E.T. Instruct 164K. \(E_{vid}\) and \(E_{frm}\) are randomly initialized, and the embeddings for <vid> token are initialized from the averaged embeddings of all existing tokens. During fine-tuning, we freeze the visual encoder and the FFN layers in Q-Former, and introduce LoRA [32] adapters on the LLM. Therefore, only the attention layers and projectors in frame compressor \(E_{c}\), LoRA adapters, and matching projectors (\(E_{vid}\) & \(E_{frm}\)) are learnable. We train the model with mixed precision (FP16) on a compute node with 8 \(\times\) NVIDIA V100 GPUs. The training process costs around 20 hours. More detailed hyper-parameters are listed in Table 6.

## Appendix C Instruction-Tuning Dataset

To fill the gap of lacking multi-event and time-sensitive training data for MLLMs, we introduce E.T. Instruct 164K, a large-scale instruction-tuning dataset with fine-grained timestamp annotations. Statistics about the dataset are shown in Table 7.

### Task Selection

E.T. Instruct 164K covers 9 event-level understanding tasks, including [RvC] referred video captioning, [TVG] temporal video grounding, [TAL] temporal action localization, [EVS] extractive video summarization, [VHD] video highlight detection, [DVC] dense video captioning, [TVC] tagged video captioning, [SLC] step localization and captioning, and [GVQ] grounded video question-answering. Most tasks are aligned with E.T. Bench but with different source datasets. The only exception are [RvC] and [TVC], where the former requires the model to generate captions for the the given temporal boundary, and the latter is similar to [DVC] but with starting timestamps only. Note that [RAR], [ECA], [RvQ], [EPM], and [TEM] are in E.T. Bench only, which can be regarded as held-out tasks during evaluation.

\begin{table}
\begin{tabular}{l c} \hline \hline
**Hyper-parameter** & **Value** \\ \hline _Visual Encoder_\(E_{v}\) & \\ \hline Frame Sampling Rate & 1 FPS \\ Preprocessing & Center Crop \\ Input Resolution & 224 \(\times\) 224 \\ Patch Size & 14 \(\times\) 14 \\ \hline _Frame Compressor_\(E_{c}\) & \\ \hline Number of Learnable Queries \(M\) & 32 \\ Number of Layers & 12 \\ Hidden Size & 768 \\ \hline _MLP Project_\(E_{vid}\)\(\&\)\(E_{frm}\) & \\ \hline Number of Layers & 2 \\ Hidden Size & 1536 \\ Output Size & 3072 \\ \hline _Large Language Model_ & \\ \hline LoRA \(r\) & 128 \\ LoRA \(\alpha\) & 256 \\ LoRA Dropout Rate & 0.05 \\ LoRA Modules & QKVO Layers \\ \hline _Model Training_ & \\ \hline Max Number of Tokens & 2048 \\ Number of Epochs & 1 \\ Batch Size & 32 \\ Learning Rate for LoRA & 5e-5 \\ Learning Rate for Other Parameters & 2e-5 \\ Weight Decay & 0.0 \\ Warmup Ratio & 0.03 \\ LR Schedeller Type & Cosine \\ Optimizer & AdamW [64] \\ AdamW \(\beta_{1},\beta_{2}\) & (0.9, 0.999) \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Hyper-parameters for fine-tuning.**

Figure 9: **Design space for timestamp processing. Existing MLLMs handle timestamps in videos or coordinates in images via a) numerical expressions, b) special tokens, or c) external modules. Details are discussed in Section B.1.**

### Data Collection and Instruction Generation

We meticulously sample videos and annotations from 14 datasets, including HowToCaption [84], DiDeMo [5], QueryD [70], TACoS [79], NaQ [78], ActivityNet [10], HACS [112], VideoXum [55], Mr. HiSum [88], ActivityNet Captions [42], ViTT [35], COIN [90], HowToStep [52], and EgoTimeQA [20]. During sampling, we ensure that the videos have no overlap with E.T. Bench. Different from E.T. Bench in which all the samples are manually labeled, more than 40% of the samples in E.T. Instruct 164K are with automatically generated annotations. A similar filtering and rule-based cleaning process as E.T. Bench generation is conducted. Note that videos from EgoTimeQA are randomly cropped to 150-second long to reduce ambiguity during training.

We then convert the original annotations into instruction-following formats. Following previous works [49, 82], for each task, we carefully write a well-designed instruction, then prompt GPT-4 [2] to extend it to multiple diverse expressions. For some tasks having overlap with previous work [82], existing instructions are also taken into consideration. We manually select and refine 6 expressions to serve as the instruction templates for each task. To obtain ground truth responses, we convert the original annotations into natural language styles using manually designed templates. The generated instruction templates and response formats are shown in Table 8 & 9.

## Appendix D Experiments

### Evaluation Metrics

**Referring.** All the tasks are formulated as MCQs, thus we adopt accuracy as the main metric.

**Grounding.** We compute F1 scores averaged among IoU thresholds \(\theta_{IoU}\) at four levels (0.1, 0.3, 0.5, and 0.7). For [TVG] and [EPM], only the first predicted temporal boundary is accepted. This aligns with conventional settings that use Recall@1 as metrics. For [TAL], all the predicted boundaries are used. In [EVS], F1 scores are computed at clip level, that is, each video is divided into 1-second long clips, and precision/recall is defined as the percentage of true positive clips with respect to all the predicted/ground truth clips. For [VHD], a prediction (single timestamp) is regarded as a true positive when it falls within any of the ground truth boundaries.

**Dense Captioning.** Similar to _grounding_, we utilize F1 score at the same four levels of \(\theta_{IoU}\) for boundary predictions in [DVC] and [SLC]. This also aligns with previous works in these areas [42, 98]. To measure the correctness of descriptions, previous works leverage traditional metrics [73, 54, 8, 97] for machine translation, which cannot handle ambiguity in open-ended scenarios. Therefore, we instead perform evaluation at semantic level and employ sentence similarity [81] to

\begin{table}
\begin{tabular}{l|l c c|c c} \hline \hline
**Task** & **Source** & **Manual Label** & **Avg. Duration** & **\#Samples** & **Ratio** \\ \hline \hline \multirow{2}{*}{[TVG]} & \multirow{2}{*}{HowToCaption [84]} & \multirow{2}{*}{\(\bm{\kappa}\)} & 176.9s & 16,907 & 10.3\% \\ \hline \multirow{4}{*}{[TVG]} & DiDeMo [5] & ✓ & 49.0s & 33,000 & 20.1\% \\  & QueryD [70] & ✗ & 173.7s & 4.267 & 2.6\% \\  & TACoS [79] & ✓ & 151.9s & 6,693 & 4.1\% \\  & NaQ [78] & ✗ & 296.5s & 10,546 & 6.4\% \\ \hline \multirow{2}{*}{[TAL]} & ActivityNet [10] & ✓ & 118.8s & 9,807 & 6.0\% \\  & HACS [112] & ✓ & 159.5s & 15,218 & 9.3\% \\ \hline \hline \multirow{2}{*}{[EVS]} & VideoXum [55] & ✓ & 123.5s & 7,989 & 4.9\% \\ \hline \hline \multirow{2}{*}{[VHD]} & \multirow{2}{*}{Mr. HiSum [88]} & \multirow{2}{*}{✗} & 196.4s & 9,056 & 5.5\% \\ \hline \hline \multirow{2}{*}{[DVC]} & ActivityNet Captions [42] & ✓ & 118.9s & 9,830 & 6.0\% \\ \hline \hline \multirow{2}{*}{[TVC]} & ViTT [35] & ✗ & 210.0s & 2908 & 1.8\% \\ \hline \multirow{2}{*}{[SLC]} & COIN [90] & ✓ & 138.9s & 7,659 & 4.7\% \\  & HowToStep [52] & ✗ & 189.8s & 20,000 & 12.2\% \\ \hline \hline \multirow{2}{*}{[GVQ]} & EgoTimeQA [20] & ✗ & 150.0s & 10,000 & 6.1\% \\ \hline \multicolumn{2}{l}{Total} & \multicolumn{4}{c}{146.4s} & 163,880 & 100\% \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Task and sample distribution in E.T. Instruct 164K.**

[MISSING_PAGE_EMPTY:17]

measure the distances between model outputs and ground truths. Following previous practices [20], the all-MiniLM-L6-v2 model in Sentence Transformers1 library is used as the embedding model.

Footnote 1: https://github.com/UKPLab/sentence-transformers

**Complex Understanding.** We adopt Recall@1 as the metric for both [TEM] and [GVQ]. The IoU thresholds are aligned with _grounding_ and _dense captioning_. For [TEM], only the first predicted temporal boundary is accepted, and it is regarded as a true positive when it has the maximum IoU among all ground truths larger than the threshold. For [GVQ], aside from boundary prediction, the MCQ answer should also be correct for a successful recall.

With the unified evaluation metrics, we are able to average them and measure a model's general performance under each capability. To achieve this, we further define 5 averaged metrics: 1) Acc\({}_{ref}\): Averaged accuracy on _referring_ tasks; 2) F1\({}_{gnd}\): Averaged F1 score on _grounding_ tasks; 3) F1\({}_{cap}\): Averaged F1 score on _dense captioning_ tasks; 4) Sim\({}_{cap}\): Averaged sentence similarity on _dense captioning_ tasks; 5) Rec\({}_{com}\): Averaged recall on _complex understanding_ tasks. These metrics serve as indicators for general performance on event-level and time-sensitive video understanding.

### Baselines

We extensively evaluate 20 representative MLLMs on E.T. Bench, including 7 open-source Image-LLMs (LLaVA-1.5 [58], LLaVA-InternLM2 [11], mPLUG-Owl2 [105], InternLM-XComposer [111], Bunny-LIama3-V [31], MiniCPM-LIama3-V-2.5 [93], and Qwen-VL-Chat [7]), 9 open-source Video-LLMs (Video-ChatGPT [66], Video-LLaVA [53], LLaMA-VID [51], Video-LLaMA-2 [110], PLLaVA [104], VTimeLLM [33], VTG-LLM [28], TimeChat [82], and LITA [34]), and 4 commercial MLLMs (GPT-4V [71], GPT-4o [72], Gemini-1.5-Flash [80], and Gemini-1.5-Pro [80]). Note that the video interface for GPT-4o is not publicly available, hence we treat it as an Image-LLM instead.

\begin{table}
\begin{tabular}{|l|l|l|} \hline \hline
We compare the architectures of open-source MLLMs in Table 10. Optional visual encoders for these models are CLIP [77], EVA [22], SigLIP [109], OpenCLIP [36], and LanguageBind [114], while the LLM backbones include LLaMA [94], Llama-2 [95], Llama-3 [4], Vicuna [18], InternLM [92], InternLM2 [11], Qwen [6], and Phi-3-Mini [1].

### More Benchmark Results

In Table 19 and Table 20, we provide performance breakdown across source datasets, where we observe that the ranking of models differs across source datasets. Table 21\(\sim\) 25 present detailed comparisons under different IoU thresholds \(\theta_{IoU}\) and more metrics (_e.g._, METEOR [8], Rouge-L [54], and CIDEr [97]) on [TVG], [EPM], [TAL], [DVC], [SLC], [TEM], and [GVQ] tasks.

### Ablation Studies

**Effect of architectural designs.** We verify the effectiveness of <vid> token, bi-directional attention across video tokens, and label smoothing during training. The results are compared in Table 11. Without introducing the <vid> token (first row), our model falls back to the _numerical expression_ variant discussed in Section B.1, which struggles in timestamp prediction even with instruction-tuning on E.T. Instruct 164K. By reformulating timestamp prediction as embedding matching (second row), our method significantly works better on all tasks on E.T. Bench. Extra modifications, _i.e._, bi-directional attention (third row) and label smoothing (fourth row), further enhance the model to achieve better localization abilities, demonstrated by the substantial increase in F1\({}_{gnd}\).

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline
**Model** & **Size** & **Frame Resolution** & **Sampled Frames** & **Visual Encoder** & **LLM** \\ \hline _Image-LLMs_ & & & & & \\ \hline LLaVA-1.5 [58] & 7B & 336 \(\times\) 336 & 8 & CLIP-ViT-L/14 & Vicuna-1.5 \\ LLaVA-InternalM2 [11] & 7B & 336 \(\times\) 336 & 8 & CLIP-ViT-L/14 & InternLM2 \\ mPLUG-Owl2 [105] & 7B & 448 \(\times\) 448 & 8 & CLIP-ViT-L/14 & Llama-2 \\ InternalM-X-Compster [11] & 7B & 224 \(\times\) 224 & 8 & EVA-ViT-G/14 & InternLM M \\ Bumy-Llama-3 [31] & 8B & 384 \(\times\) 384 & 8 & SigLIP-ViT-L/14 & Llama-3 \\ MiniCPM-Llama3-V-2.5 [93] & 8B & 980 \(\times\) 980 & 8 & SigLIP-ViT-L/14 & Llama-3 \\ Qwen-VL-Chat [7] & 7B & 448 \(\times\) 448 & 8 & CLIP-ViT-bigG/14 & Qwen \\ \hline _Video-LLMs_ & & & & & \\ \hline Video-ChaMPGT [66] & 7B & 224 \(\times\) 224 & 100 & CLIP-ViT-L/14 & LLaMA \\ Video-LLAVA [53] & 7B & 224 \(\times\) 224 & 8 & LanguageBind-ViT-L/14 & Vicuna-1.5 \\ LLaMA-VID [51] & 7B & 224 \(\times\) 224 & 1 FPS & EVA-ViT-G/14 & Vicuna-1.5 \\ Video-LLAMA [110] & 7B & 224 \(\times\) 224 & 8 & EVA-ViT-G/14 & Llama-2 -Chat \\ PLLAVA [104] & 7B & 672 \(\times\) 672 & 16 & CLIP-ViT-L/14 & Vicuna-1.5 \\ VTimeLLM [33] & 7B & 224 \(\times\) 224 & 100 & CLIP-ViT-L/14 & Vicuna-1.5 \\ VTG-LLM [28] & 7B & 224 \(\times\) 224 & 96 & EVA-ViT-G/14 & Llama-2 \\ TimeChat [82] & 7B & 224 \(\times\) 224 & 96 & EVA-ViT-G/14 & Llama-2 \\ LITA [34] & 13B & 224 \(\times\) 224 & 100 & CLIP-ViT-L/14 & Vicuna-1.3 \\ \hline E.T. Chat (Ours) & 3.8B & 224 \(\times\) 224 & 1 FPS & EVA-ViT-G/14 & Phi-3-Mini \\ \hline \hline \end{tabular}
\end{table}
Table 10: **Model architectures of MLLMs evaluated on E.T. Bench. Size means the LLM size.**

\begin{table}
\begin{tabular}{c c c|c c c c} \hline \hline
**<vid> Token** & **Bi-directional** & **Smoothing** & **Acc\({}_{grd}\)** & **F1\({}_{grd}\)** & **F1\({}_{vg}\)** & **Sim\({}_{vg}\)** & **Rec\({}_{cav}\)** \\ \hline  & & & 25.0 & 17.5 & 21.2 & 12.3 & 8.6 \\ \hline ✓ & & & 34.0 & 25.2 & 26.4 & 15.4 & 9.2 \\ ✓ & ✓ & & 33.7 & 30.5 & 27.5 & 15.8 & 9.8 \\ ✓ & ✓ & ✓ & 34.5 & 25.8 & 26.4 & 13.6 & 9.5 \\ \hline ✓ & ✓ & ✓ & **38.4** & **33.5** & **31.4** & **17.1** & **10.1** \\ \hline \hline \end{tabular}
\end{table}
Table 11: **Comparison on architectural designs.**

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline
**Method** & **Acc\({}_{grd}\)** & **F1\({}_{grd}\)** & **F1\({}_{cap}\)** & **Sim\({}_{vg}\)** & **Rec\({}_{cav}\)** \\ \hline Pooling & 29.6 & 25.5 & 21.1 & 11.3 & 9.1 \\ Q-Former & **38.4** & **33.5** & **31.4** & **17.1** & **10.1** \\ \hline \hline \end{tabular}
\end{table}
Table 12: **Choices of frame compressor.**

**Choice of frame compressor.** Table 12 compares the performance of two design choices for frame compressor \(E_{c}\), _i.e._, a naive spatial pooling among frame patches \(\mathbf{P}_{t}\) and a query-guided compression based on Q-Former [46]. The results confirm that employing Q-Former for frame compression proves to be a superior alternative.

**Choice of LLM layer for matching.** As articulated in the main paper, during matching, we utilize the second-last layer's hidden states for the <vid> token, while the final-layer's hidden states for frame tokens. This strategy take into consideration the small feature range of final-layer hidden states for the <vid> token [30]. We further verify its effectiveness in Table 13. While the results are essentially similar, current setting exhibits a marginally superior overall performance.

**Learnable modules.** In Table 14, we justify the training strategy for instruction-tuning. Updating only the context aggregator \(E_{a}\) and projector \(E_{p}\) (first row) makes the training hard to converge with new tokens, and fine-tuning the LLM with LoRA (second row) brings better performance. We contend that the pre-trained Q-Former is suitable only for short & single event videos due to the constraints of pre-training data. Serving as the frame compressor, such limitation would hinder the model's performance. Line 3 \(\sim\) 5 corroborate our hypothesis, as updating Q-Former on E.T. Instruct 164K brings notable performance improvements. Furthermore, we observe that fine-tuning the whole Q-Former makes the model slightly overfit to dense captioning tasks, and freezing its FFN layers could strike the balance between adapting to new data and retaining pre-trained capabilities.

**Effect of \(\alpha\) for label smoothing.** We ablate the effect of different \(\alpha\) values for label smoothing in Table 15. Smaller \(\alpha\) values make the optimization goal of matching scores smoother. Generally, setting \(\alpha\) to around 2.0 brings considerable results.

**Joint effect of model and instruction-tuning dataset.** We compare in Table 16 the joint effect of model design and instruction-tuning dataset collection. We choose two representative models (LLaMA-VID [51] and TimeChat [82]) as baselines and train them on E.T. Instruct 164K. Our E.T. Chat is also trained on TimeIT dataset [82] for in-depth comparison. The comparison results between line 1 & 2, 3 & 4, and 5 & 6 demonstrate the effectiveness of E.T. Instruct 164K. Results in line 2, 4, and 6 verify the significance of our model design.

**Effect of instruction-tuning tasks.** To study the effect of each task during instruction tuning, we provide detailed comparisons in Table 17. We observe that adding more tasks for instruction-tuning

\begin{table}
\begin{tabular}{c c|c c c c c c} \hline \hline \multicolumn{2}{c|}{**Q-Former \(E_{q}\)**} & \multicolumn{2}{c|}{**Aggregator**} & \multicolumn{2}{c|}{**Projector**} & \multicolumn{1}{c|}{**LLM**} & \multirow{2}{*}{**Acc\({}_{off}\)**} & \multirow{2}{*}{**F1\({}_{adv}\)**} & \multirow{2}{*}{**F1\({}_{adv}\)**} & \multirow{2}{*}{**Sim\({}_{vop}\)**} & \multirow{2}{*}{**Rec\({}_{conv}\)**} \\ \cline{1-1} \cline{6-7} \multicolumn{1}{c|}{**ATTN**} & & & & & & & & \\ \hline \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & 36.1 & 29.2 & 27.1 & 14.1 & 8.7 \\  & & & & & & 37.3 & 30.5 & 28.3 & 15.0 & 9.6 \\ \cline{1-1} \cline{6-7}  & & & & & 37.9 & 31.8 & 29.6 & 15.7 & 9.5 \\ \cline{1-1} \cline{6-7}  & & & & & **38.4** & **33.5** & 31.4 & 17.1 & **10.1** \\ \cline{1-1} \cline{6-7}  & & & & & 37.5 & 29.1 & **32.2** & **17.5** & 9.3 \\ \hline \hline \end{tabular}
\end{table}
Table 14: **Comparison on learnable modules.** ATTN and FFN represent the attention and feed-forward layers in Q-Former, respectively.

\begin{table}
\begin{tabular}{c c|c c c c c} \hline \hline \multicolumn{1}{c|}{**Layer\({}_{vid}\)**} & \multicolumn{1}{c|}{**Layer\({}_{gm}\)**} & \multicolumn{1}{c}{**Acc\({}_{off}\)**} & \multicolumn{1}{c}{**F1\({}_{adv}\)**} & \multicolumn{1}{c}{**F1\({}_{adv}\)**} & \multicolumn{1}{c}{**Sim\({}_{vop}\)**} & \multicolumn{1}{c}{**Rec\({}_{conv}\)**} \\ \hline –1 & –1 & 38.2 & 32.2 & 31.0 & **17.2** & 9.6 \\ –1 & –2 & 37.8 & 32.5 & 30.8 & 16.5 & 9.9 \\ –2 & –1 & **38.4** & 33.5 & **31.4** & 17.1 & **10.1** \\ –2 & –2 & 37.6 & **33.8** & 31.2 & 16.7 & 9.7 \\ \hline \hline \end{tabular}
\end{table}
Table 13: **Choices of layers for matching.** Layer\({}_{vid}\) and Layer\({}_{fm}\) are the index of LLM layer utilized for matching, _e.g._, “–1” means using the final-layer hidden states.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline \multicolumn{1}{c|}{\(\alpha\)} & \multicolumn{1}{c}{**Acc\({}_{off}\)**} & \multicolumn{1}{c}{**F1\({}_{adv}\)**} & \multicolumn{1}{c}{**F1\({}_{adv}\)**} & \multicolumn{1}{c}{**Sim\({}_{vop}\)**} & \multicolumn{1}{c}{**Rec\({}_{conv}\)**} \\ \hline
1.0 & 37.2 & 30.8 & 26.3 & 14.8 & 9.6 \\
1.5 & 37.9 & 31.6 & 28.6 & 15.4 & **10.3** \\
2.0 & **38.4** & 33.5 & **31.4** & **17.1** & 10.1 \\
2.5 & 38.1 & 33.0 & 29.8 & 16.8 & 9.2 \\
3.0 & 37.5 & **33.7** & 28.4 & 16.0 & 9.8 \\ \hline \hline \end{tabular}
\end{table}
Table 15: **Effect of \(\alpha\) for label smoothing.**might slightly affect the performance on original tasks. This can be alleviated by carefully balancing the number of samples per task.

### Qualitative Results

Figure 10\(\sim\) 15 present task-specific qualitative comparisons among 5 representative open-source MLLMs, _i.e._, LLaVA-1.5 [58], Video-ChatGPT [66], LLaMA-VID [51], TimeChat [82], and E.T. Chat. The correct model responses are marked green. We observe that the unsatisfactory performance of existing methods comes from 1) weak instruction-following abilities, 2) low temporal resolution, 3) lack of event-level and time-sensitive designs, and 4) lack of multi-event instruction-tuning data.

## Appendix E Limitations and Future Work

Currently, the proposed E.T. Bench is based on val or test split of existing datasets, whose training split might be included for MLLM training. This could potentially result in data leakage, thereby compromising the integrity of the zero-shot evaluation framework and leading to unfair comparisons. Therefore, our next step would be self-collecting new videos and provide manual annotations under each carefully designed task. More flexible input-output formats shall also be incorporated to complement the existing benchmark.

For E.T. Chat, even with advanced frame compression strategies, the low spatial resolution (1 token per frame) limits the model's ability to understand spatial details. Modern Image-LLMs are becoming to support extra-high-resolution image inputs, but this is not directly compatible to videos due to the large compute resource consumption. Our future work will focus on the balance between spatial and temporal resolution for Video-LLMs.

## Appendix F Licenses

The annotations of E.T. Bench are provided to the public under CC BY-NC-SA 4.0 license. A copy can be obtained at https://creativecommons.org/licenses/by-nc-sa/4.0/. By downloading our dataset from our website or other sources, the user agree to adhere to the terms of CC BY-NC-SA 4.0 and licenses of the source datasets. Licenses of the source datasets are listed in Table 18.

\begin{table}
\begin{tabular}{l l|c c c c c c} \hline \hline
**Model** & **IT Dataset** & **Acc\({}_{eef}\)** & **F1\({}_{sut}\)** & **F1\({}_{sut}\)** & **Sim\({}_{sup}\)** & **Rec\({}_{cnn}\)** \\ \hline LLaMA-VID [51] & 723K Corpus [51] & 32.5 & 9.2 & 16.2 & 11.9 & 4.0 \\ LLaMA-VID [51] & E.T. Instruct 164K (Ours) & 31.3 & 16.0 & 19.8 & 14.9 & 7.8 \\ \hline TimeChat [82] & TimeIT [82] & 27.7 & 21.9 & 11.1 & 10.8 & 9.7 \\ TimeChat [82] & E.T. Instruct 164K (Ours) & 29.5 & 24.3 & 21.5 & 11.5 & **11.4** \\ \hline E.T. Chat (Ours) & TimeIT [82] & 34.9 & 22.1 & 20.1 & 13.4 & 6.9 \\ E.T. Chat (Ours) & E.T. Instruct 164K (Ours) & **38.4** & **33.5** & **31.4** & **17.1** & 10.1 \\ \hline \hline \end{tabular}
\end{table}
Table 16: **Joint effect of model and instruction-tuning (IT) dataset.**

\begin{table}
\begin{tabular}{c c c c c c c c|c c c c} \hline \hline
**RVC** & **TVG** & **TAL** & **EVS** & **VHD** & **DVC** & **SLC** & **GVQ** & **Acc\({}_{eef}\)** & **F1\({}_{sut}\)** & **Sim\({}_{sup}\)** & **Rec\({}_{cnn}\)** \\ \hline ✓ & & & & & & & & 36.5 & 9.8 & 0.4 & 10.3 & 0.5 \\ ✓ & ✓ & & & & & & & 36.0 & 12.8 & 3.7 & 12.5 & 5.1 \\ ✓ & ✓ & ✓ & & & & & 35.4 & 31.9 & 10.3 & 11.5 & 9.9 \\ ✓ & ✓ & ✓ & ✓ & & & & 35.6 & 32.5 & 9.5 & 11.8 & 9.5 \\ ✓ & ✓ & ✓ & ✓ & ✓ & & & & 35.9 & 33.6 & 15.1 & 10.8 & 9.7 \\ ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & & & 37.4 & 34.8 & 18.2 & 12.5 & 9.4 \\ ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & & 34.2 & **33.7** & 28.5 & 14.3 & 9.2 \\ \hline ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & **38.4** & 33.5 & **31.4** & **17.1** & **10.1** \\ \hline \hline \end{tabular}
\end{table}
Table 17: **Ablation study on instruction-tuning tasks.**

## Appendix A

Figure 11: **Qualitative comparison on [RavQ] (left) and [TVG] (right).**

Figure 10: **Qualitative comparison on [RAR] (left) and [ECA] (right).**Figure 12: **Qualitative comparison on [EPM] (left) and [TAL] (right).**

Figure 13: **Qualitative comparison on [EVS] (left) and [VHD] (right).**

Figure 14: **Qualitative comparison on [DVC] (left) and [SLC] (right).**

Figure 15: **Qualitative comparison on [TEM] (left) and [GVQ] (right).**

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c}{**DVC**} & \multicolumn{3}{c}{**SLC**} & \multicolumn{3}{c}{**TEM**} & \multicolumn{3}{c}{**GVQ**} \\ \cline{2-13}  & [WI]\({}_{FT}\) & [HI]\({}_{SN0}\) & [YC]\({}_{FI}\) & [YC]\({}_{SW0}\) & [CT]\({}_{FI}\) & [CT]\({}_{SN0}\) & [BS]\({}_{FI}\) & [HS]\({}_{SN0}\) & [PT]\({}_{RTc}\) & [QT]\({}_{RTc}\) & [QE]\({}_{RC}\) \\ \hline \hline \multicolumn{13}{l}{_Image-LLMs: 8 uniformly sampled frames as inputs_} \\ \hline LLLAVA-1.5 [58] & 20.6 & 12.2 & 8.3 & 10.9 & 0.6 & 10.1 & 1.3 & 8.9 & 13.9 & 1.6 & 0.0 \\ LLVA-InternalM2 [11] & 16.4 & 9.4 & 17.5 & 7.6 & 0.1 & 5.1 & 0.0 & 4.3 & 13.0 & 1.3 & 1.5 \\ mPLUG-Ow12 [105] & 0.0 & 8.5 & 0.1 & 7.7 & 0.1 & 8.0 & 0.0 & 7.3 & 9.4 & 3.0 & 0.0 \\ XComposer [111] & 2.0 & 2.2 & 8.8 & 9.6 & 3.4 & 9.6 & 2.0 & 8.5 & 18.1 & 2.9 & 0.0 \\ Bumy-Llama-3-V [31] & 11.6 & 8.8 & 15.4 & 8.9 & 0.1 & 7.4 & 0.0 & 7.7 & 11.8 & 2.6 & 0.0 \\ MinCPM-V-2.5 [93] & 9.1 & 12.3 & 3.4 & 11.3 & 1.6 & 9.9 & 1.1 & 9.5 & 1.1 & 0.3 & 0.0 \\ Qwen-VL-Chu [6] & 14.9 & 12.2 & 19.8 & 15.4 & 9.5 & 11.7 & 3.0 & 14.4 & 4.2 & 2.3 & 1.5 \\ \hline \hline \multicolumn{13}{l}{_Video-LLMs: each model’s default numbers of frames as inputs_} \\ \hline Video-LLAVA-1.5 [58] & 20.6 & 12.2 & 8.3 & 10.9 & 0.6 & 10.1 & 1.3 & 8.9 & 13.9 & 1.6 & 0.0 \\ LLVA-InternalM2 [116] & 16.4 & 9.4 & 17.5 & 7.6 & 0.1 & 5.1 & 0.0 & 4.3 & 13.0 & 1.3 & 1.5 \\ mPLUG-Ow12 [105] & 0.0 & 8.5 & 0.1 & 7.7 & 0.1 & 8.0 & 0.0 & 7.3 & 9.4 & 3.0 & 0.0 \\ XComposer [111] & 2.0 & 2.2 & 8.8 & 9.6 & 3.4 & 9.6 & 2.0 & 8.5 & 18.1 & 2.9 & 0.0 \\ Bumy-Llama-3-V [31] & 11.6 & 8.8 & 15.4 & 8.9 & 0.1 & 7.4 & 0.0 & 7.7 & 11.8 & 2.6 & 0.0 \\ MinCPM-V-2.5 [93] & 9.1 & 12.3 & 3.4 & 11.3 & 1.6 & 9.9 & 1.1 & 9.5 & 1.1 & 0.3 & 0.0 \\ Qwen-VL-Chu [6] & 14.9 & 12.2 & 19.8 & 15.4 & 9.5 & 11.7 & 3.0 & 14.4 & 4.2 & 2.3 & 1.5 \\ \hline \hline \multicolumn{13}{l}{_Video-LLMs: each model’s default numbers of frames as inputs_} \\ \hline Video-ChatGPT [66] & 6.1 & 10.3 & 11.5 & 12.2 & 6.3 & 8.9 & 5.1 & 11.6 & 26.8 & 5.0 & 0.0 \\ Video-LLAVA [53] & 42.5 & 16.2 & 13.4 & 13.9 & 1.8 & 10.0 & 0.0 & 6.5 & 10.2 & 4.7 & 0.1 \\ LLLAVA-VD [51] & 41.5 & 12.4 & 12.7 & 12.9 & 6.5 & 10.0 & 4.0 & 12.3 & 11.4 & 2.6 & 0.9 \\ Video-LLALA-2 [110] & 1.2 & 9.2 & 0.1 & **19.8** & 0.0 & 14.4 & 0.1 & **16.1** & 0.0 & 0.0 & 0.1 \\ PLLAVA [104] & 5.6 & 10.0 & 21.0 & 11.1 & 9.3 & 10.1 & 10.2 & 13.4 & 6.9 & 1.3 & 1.2 \\ VTimeLLM [33] & 14.4 & 13.0 & 10.4 & 13.1 & 10.5 & 5.8 & 7.0 & 7.0 & 4.2 & 9.4 & 1.9 \\ VTG-LLM [28] & 45.4 & 19.3 & **35.0** & 18.0 & 20.3 & 14.1 & 21.3 & 14.7 & 14.1 & 3.7 & 1.4 \\ TimeChat [82] & 14.2 & 12.8 & 19.0 & 12.3 & 8.0 & 9.1 & 3.3 & 9.2 & 24.5 & 11.4 & 1.5 \\ LITA [34] & **47.0** & 15.9 & 32.4 & 18.5 & 21.3 & 12.1 & 20.6 & 12.3 & 20.3 & **11.7** & 2.2 \\ \hline \hline \end{tabular}
\end{table}
Table 20: **Performance breakdown across source datasets on _dense captioning_ and _complex understanding_ tasks. Abbreviations: [HI] HiREST, [YC] YouCook2, [CT] CrossTask, [HS] HT-Step, [PT] Perception Test, [QT] QVHighlights, [QE] QA-Ego4D.**

\begin{table}
\begin{tabular}{l|c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c}{**DVC**} & \multicolumn{3}{c}{**SLC**} & \multicolumn{3}{c}{**TEM**} & \multicolumn{3}{c}{**GVQ**} \\ \cline{2-13}  & [WI]\({}_{FT}\) & [HI]\({}_{SN0}\) & [YC]\({}_{FI}\) & [YC]\({}_{SW0}\) & [CT]\({}_{FI}\) & [CT]\({}_{SN0}\) & [BS]\({}_{FI}\) & [HS]\({}_{SN0}\) & [PT]\({}_{RTc}\) & [QT]\({}_{RTc}\) & [QT]\({}_{RTc}\) \\ \hline \hline \multicolumn{13}{l}{_Image-LLMs: 8 uniformly sampled frames as inputs_} \\ \hline LLLAVA-1.5 [58] & 3.0 & 9.2 & 1.9 & 7.6 & 7.7 & 8.0 & 3.1 & 1.7 & 33.6 & 28.2 \\ LLVA-InternalM2 [11] & 0.2 & 5.2 & 0.1 & 0.4 & 0.2 & 0.1 & 0.1 & 0.3 & 24.4 & 40.1 \\ mPLUG-Ow12 [105] & 0.2 & 2.0 & 0.2 & 1.0 & 3.8 & 4.3 & 7.5 & 0.7 & 25.2 & 48.3 \\ XComposer [111] & 1.4 & 8.4 & 1.5 & 16.8 & 6.3 & 6.6 & 4.1 & 1.4 & 26.2 & 31.6 \\ Bumy-Llama-3-V [31] & 10.2 & 3.8 & 0.1 & 2.6 & 6.3 & 6.4 & 0.3 & 0.4 & 20.0 & 41.2 \\ MinCPM-V-2.5 [93] & 0.9 & 2.2 & 0.2 & 6.0 & 3.7 & 3.5 & 17.7 & 9.1 & 14.0 & 23.4 \\ Qwen-VL-Chu [6] & 6.8 & 25.8 & 4.0 & 14.2 & 9.2 & 8.6 & 25.0 & 7.7 & 25.6 & 43.2 \\ \hline \hline \multicolumn{13}{l}{_Video-LLMs: each model’s default numbers of frames as inputs_} \\ \hline Video-ChatGPT [66] & 2.8 & 11.1 & 1.3 & 24.0 & 10.2 & 11.0 & 12.9 & 4.0 & 25.4 & 32.2 \\ Video-LLAVA [53] & 3.0 & 11.1 & 1.9 & 23.9 & 10.2 & 11.0

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline \hline
**Method** & **F1@@0.1** & **F1@0.3** & **F1@0.5** & **F1@0.7** & **F1** \\ \hline _Image-LLAMs: 8 uniformly sampled frames as inputs_ & & & & & & \\ \hline LLaVA-1.5 [58] & 29.6 & 17.0 & 7.9 & 3.3 & 14.5 & 0.9 & 1.8 & 2.1 & 11.5 \\ LLaVA-InternetLM2 [11] & 36.1 & 20.4 & 8.3 & 3.0 & 16.9 & 1.0 & 1.6 & 1.6 & 8.5 \\ mPLUG-Owl2 [105] & 0.2 & 0.0 & 0.0 & 0.1 & 0.0 & 0.0 & 0.0 & 8.1 \\ XCompressor [111] & 12.8 & 5.8 & 2.5 & 0.6 & 5.4 & 0.6 & 0.9 & 0.1 & 5.9 \\ Banny-Llamlam-V [31] & 32.1 & 15.2 & 5.1 & 1.8 & 13.5 & 0.9 & 1.6 & 2.2 & 8.8 \\ MinCPM-V-2.5 [93] & 15.7 & 6.2 & 2.4 & 0.7 & 6.2 & 1.0 & 1.4 & 0.2 & 11.8 \\ Owen-VL-Chat [6] & 39.6 & 20.4 & 6.9 & 2.6 & 17.4 & 1.3 & 2.2 & 2.7 & 13.8 \\ \hline \hline \multicolumn{7}{l}{_Video-LLAMs: each model’s default numbers of frames as inputs_} & & & & & & \\ \hline Video-LLAM: 8 & 1.2 & 0.4 & 0.2 & 1.9 & \\ Video-LLAVA [53] & 5.8 & 1.2 & 0.4 & 0.2 & 1.9 & \\ LLaVA-VID [51] & 30. & 1.2 & 0.4 & 0.2 & 1.2 & \\ Video-LLAM-2 [110] & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\ PLAVA [104] & 3.2 & 1.0 & 0.2 & 0.0 & 1.1 \\ VTimeLMM [33] & 5.8 & 1.2 & 0.4 & 0.2 & 1.9 \\ VTG-LLM [28] & 9.2 & 4.6 & 0.6 & 0.4 & 3.7 \\ TimeTent [82] & 7.8 & 4.6 & 2.2 & 0.8 & 3.8 \\ LITA [34] & 12.6 & 4.4 & 1.0 & 0.4 & 4.6 \\ \hline \hline \multicolumn{7}{l}{**E.T. Chat (Ours)**} & **21.6** & **12.4** & **5.2** & **1.6** & **10.2** \\ \hline \hline \end{tabular}
\end{table}
Table 23: Performance under more metrics on [DVC].

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline
**Method** & **F1@0.1** & **F1@0.3** & **F1@0.5** & **F1@0.7** & **F1** \\ \hline _Image-LLAMs: 8 uniformly sampled frames as inputs_ & & & & \\ \hline LLaVA-1.5 [58] & 29.1 & 17.0 & 7.9 & 3.3 & 14.5 & 0.9 & 1.8 & 2.1 & 11.5 \\ LLaVA-InternetLM2 [11] & 36.1 & 20.4 & 8.3 & 3.0 & 16.9 & 1.0 & 1.6 & 1.6 & 8.5 \\ mPLUG-Owl2 [105] & 0.2 & 0.0 & 0.0 & 0.1 & 0.0 & 0.0 & 0.0 & 8.1 \\ XCompressor [111] & 12.8 & 5.8 & 2.5 & 0.6 & 5.4 & 0.6 & 0.9 & 0.1 & 5.9 \\ Banny-Llamlam-V [31] & 32.1 & 15.2 & 5.1 & 1.8 & 13.5 & 0.9 & 1.6 & 2.2 & 8.8 \\ MinCPM-V-2.5 [93] & 15.7 & 6.2 & 2.4 & 0.7 & 6.2 & 1.0 & 1.4 & 0.2 & 11.8 \\ Owen-VL-Chat [6] & 39.6 & 20.4 & 6.9 & 2.6 & 17.4 & 1.3 & 2.2 & 2.7 & 13.8 \\ \hline \hline \multicolumn{7}{l}{_Video-LLAMs: each model’s default numbers of frames as inputs_} & & & & \\ \hline Video-LLAM: 8 & 1.9 & 10.8 & 4.3 & 1.2 & 8.8 & 1.1 & 2.0 & 2.6 & 11.3 \\ Video-LLAVA [53] & 54.6 & 33.0 & 16.6 & 7.7 & 28.0 & 1.4 & 2.7 & 2.1 & 15.0 \\ ILaVA-VID [51] & 50.8 & 32.6 & 16.8 & 8.2 & 27.1 & 0.9 & 1.9 & 1.2 & 12.6 \\ Video-LLAM-2 [110] & 1.2 & 0.7 & 0.6 & 0.0 & 0.6 & 0.0 & 0.0 & 0.0 & 14.5 \\ PLLAVA [104] & 29.3 & 15.9 & 6.1 & 2.0 & 13.3 & 1.3 & 2.4 & 3.7 & 10.6 \\ VTimeLMM [33] & 28.1 & 13.7 & 6.1 & 1.8 & 12.4 & 1.5 & 2.9 & 2.4 & 13.1 \\ VTG-LLM [28] & **81.0** & **51.6** & 22.0 & 6.2 & **40.2** & 2.8 & 5.2 & 8.5 & 18.6 \\ TimeTent [82] & 41.3 & 17.2 & 6.1 & 1.9 & 16.6 & 1.7 & 3.3 & 3.5 & 12.5 \\ LITA [34] & 78.5 & 49.2 & 23.5 & 7.6 & 39.7 & 3.3 & 5.2 & 7.6 & 17.2 \\ \hline \hline \multicolumn{7}{l}{**E.T. Chat (Ours)**} & 73.3 & 46.8 & **23.8** & **9.8** & 38.4 & **3.3** & **5.7** & **10.4** & **19.7** \\ \hline \hline \end{tabular}
\end{table}
Table 21: Performance under different IoU thresholds on [T].

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline
**Method** & **F1@0.1** & **F1@0.3** & **F1@0.5** & **F1@0.7** & **F1** \\ \hline _Image-LLAMs: 8 uniformly sampled frames as inputs_ & & & & & \\ \hline LLaVA-1.5 [58] & 29.1 & 1.4 & 0.7 & 0.5 & 0.0 & 6.1 \\ LLaVA-InternetLM2 [11] & 7.4 & 3.0 & 0.4 & 0.0 & 2.7 \\ mPLUG-Owl2 [105] & 3.0 & 1.0 & 0.4 & 0.0 & 1.1 \\ XCompressor [111] & 17.7 & 1.8 & 0.0 & 0.0 & 4.9 \\ Banny-Llamlamlam-V [31] & 24.3 & 0.2 & 0.5 & 0.0 & 7.0 \\ MiniCPM-V-2.5 [93] & 4.5 & 2.3 & 0.9 & 0.3 & 2.0 \\ Owen-VL-Chat [6] & 31.4 & 19.1 & 10.4 & 4.1 & 16.2 \\ \hline \hline \multicolumn{7}{l}{_Video-LLAMs: each model’s default numbers of frames as inputs_} & & & \\ \hline Video-LLAM: 8 & 1.2 & 0.2 & 0.0 & 1.3 \\ Video-LLAVA [53] & 5.8 & 1.2 & 0.4 & 0.2 & 1.9 \\ Video-LLAVA [53] & 5.8 & 1.2 & 0.4 & 0.2 & 1.9 \\ Video-LLAVA-VID [51] & 3.0 & 1.2 & 0.4 & 0.2 & 1.2 \\ Video-LLAM-2 [110] & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\ PLAVA [104] & 3.2 & 1.0 & 0.2 & 0.0 & 1.1 \\ VTimeLMM [33] & 5.8 & 1.2 & 0.4 & 0.2 & 1.9 \\ VTG-LLM [28] & 9.2 & 4.6 & 0.6 & 0.4 & 3.7 \\ TimeTent [82] & 7.8 & 4.6 & 2.2 & 0.8 & 3.8 \\ LITA [34] & 12.6 & 4.4 & 1.0 & 0.4 & 4.6 \\ \hline \hline \multicolumn{7}{l}{**E.T. Chat (Ours)**} & **21.6** & **12.4** & **5.2** & **1.6** & **10.2** \\ \hline \hline \end{tabular} \end{

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline \hline
**Method** & **R@@0.1** & **R@0.3** & **R@0.5** & **R@0.7** & **Rec** \\ \hline \multicolumn{7}{l}{_Image-LLMs: 8 uniformly sampled frames as inputs_} \\ \hline LLaVA-1.5 [58] & 16.0 & 9.5 & 4.2 & 1.1 & 7.7 \\ LLaVA-InternRLM2 [11] & 14.1 & 8.9 & 4.6 & 1.1 & 7.2 \\ mPLUG-Owl2 [105] & 12.4 & 7.3 & 3.8 & 1.1 & 6.2 \\ XCompser [111] & 21.9 & 13.0 & 5.4 & 1.6 & 10.5 \\ Bunny-Llama-V [31] & 13.8 & 9.1 & 4.5 & 1.3 & 7.2 \\ MinCPM-V-2.5 [93] & 1.5 & 0.7 & 0.5 & 0.2 & 0.7 \\ Owen-VL-Chat [6] & 7.7 & 3.5 & 1.3 & 0.4 & 3.2 \\ \hline \multicolumn{7}{l}{_Video-LLMs: each model’s default numbers of frames as inputs_} \\ \hline Video-Cha4GPT [66] & 32.1 & 18.9 & **9.6** & **2.7** & 15.9 \\ Video-LLaVA [53] & 16.5 & 8.6 & 3.9 & 1.0 & 7.5 \\ LLaMA-VID [51] & 14.7 & 8.5 & 3.7 & 1.1 & 7.0 \\ Video-LLAMa-2 [110] & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\ PLLAVA [104] & 8.4 & 5.1 & 2.4 & 0.7 & 4.1 \\ VTimeLLM [33] & 16.1 & 7.5 & 2.7 & 0.9 & 6.8 \\ VTG-LLM [28] & 17.4 & 10.9 & 5.2 & 2.1 & 8.9 \\ TimeChat [82] & 38.6 & **21.7** & 8.9 & 2.7 & **18.0** \\ LTTA [34] & **40.4** & 15.8 & 6.2 & 1.8 & 16.0 \\ \hline \multicolumn{7}{l}{**E.T. Chat (Ours)**} & 36.9 & 20.2 & 6.7 & 2.0 & 16.5 \\ \hline \hline \end{tabular}
\end{table}
Table 25: **Performance under different IoU thresholds on [TEM] (left) and [GVQ] (right).**

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline \hline
**Method** & **F1@0.1** & **F1@0.3** & **F1@0.5** & **F1@0.7** & **F1** & **METEOR** & **Rouge-L** & **CIDEr** & **Sim** \\ \hline \multicolumn{7}{l}{_Image-LLMs: 8 uniformly sampled frames as inputs_} \\ \hline LLaVA-1.5 [58] & 2.1 & 1.1 & 0.5 & 0.1 & 0.9 & 0.1 & 0.1 & 0.2 & 9.5 \\ LLaVA-InternRLM2 [11] & 0.3 & 0.0 & 0.0 & 0.0 & 0.1 & 0.0 & 0.0 & 0.0 & 4.7 \\ mPLUG-Owl2 [105] & 0.1 & 0.1 & 0.0 & 0.0 & 0.1 & 0.0 & 0.0 & 0.0 & 7.7 \\ XCompser [111] & 5.9 & 3.4 & 1.1 & 0.4 & 2.7 & 0.3 & 0.3 & 0.0 & 9.0 \\ Bunny-Llama-V [31] & 0.2 & 0.1 & 0.0 & 0.0 & 0.1 & 0.0 & 0.0 & 0.0 & 7.6 \\ MinCPM-V-2.5 [93] & 4.3 & 0.8 & 0.2 & 0.2 & 1.4 & 0.2 & 0.2 & 0.1 & 9.7 \\ Owen-VL-Chat [6] & 13.7 & 7.5 & 3.0 & 0.8 & 6.2 & 0.3 & 0.4 & 0.7 & 13.1 \\ \hline \multicolumn{7}{l}{_Video-LLMs: each model’s default numbers of frames as inputs_} \\ \hline Video-Cha4GPT [66] & 12.2 & 7.1 & 2.5 & 0.9 & 5.7 & 0.4 & 0.7 & 1.2 & 10.2 \\ Video-LLaVA [53] & 1.8 & 1.1 & 0.5 & 0.2 & 0.9 & 0.0 & 0.0 & 0.1 & 8.3 \\ LLaMA-VID [51] & 14.0 & 4.6 & 1.7 & 0.6 & 5.2 & 0.2 & 0.3 & 0.3 & 11.1 \\ Video-LLaMA-2 [110] & 0.2 & 0.0 & 0.0 & 0.0 & 0.0 & 0.2 & 0.5 & 0.4 & **15.2** \\ PLLVAVA [104] & 22.2 & 11.3 & 4.3 & 1.1 & 9.7 & 0.7 & 1.1 & 2.5 & 11.8 \\ VTG-LLM [33] & 19.1 & 10.4 & 4.1 & 1.4 & 8.7 & 0.4 & 0.6 & 0.9 & 6.4 \\ VTG-LLM [28] & **50.1** & 22.3 & 8.5 & 2.3 & 20.8 & 1.5 & 2.4 & 4.3 & 14.4 \\ TimeChat [82] & 15.9 & 4.7 & 1.5 & 0.4 & 5.6 & 0.6 & 1.0 & 1.2 & 9.2 \\ LTTA [34] & 48.9 & 23.2 & 9.1 & 2.8 & 21.0 & 1.4 & 1.8 & 2.3 & 12.2 \\ \hline \multicolumn{7}{l}{**E.T. Chat (Ours)**} & 45.8 & **28.8** & **15.8** & **7.2** & **24.4** & **2.4** & **3.2** & **6.2** & 14.6 \\ \hline \hline \end{tabular}
\end{table}
Table 24: **Performance under more metrics on [SLC].**

## References

* [1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadallah, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. _arXiv preprint arXiv:2404.14219_, 2024.
* [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [3] Triantafyllos Afouras, Effrosyni Mavroudi, Tushar Nagarajan, Huiyu Wang, and Lorenzo Torresani. Ht-step: Aligning instructional articles with how-to videos. _Advances in Neural Information Processing Systems_, 36, 2024.
* [4] Meta AI. Llama 3 model card, 2024.
* [5] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In _ICCV_, pages 5803-5812, 2017.
* [6] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023.
* [7] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. 2023.
* [8] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In _Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization_, pages 65-72, 2005.
* [9] Leonard Barmann and Alex Waibel. Where did i leave my keys?-episodic-memory-based question answering on egocentric videos. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1560-1568, 2022.
* [10] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In _CVPR_, pages 961-970, 2015.
* [11] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. _arXiv preprint arXiv:2403.17297_, 2024.
* [12] David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In _Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies_, pages 190-200, 2011.
* [13] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic. _arXiv preprint arXiv:2306.15195_, 2023.
* [14] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? _arXiv preprint arXiv:2403.20330_, 2024.
* [15] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling framework for object detection. _arXiv preprint arXiv:2109.10852_, 2021.
* [16] Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question answering. _arXiv preprint arXiv:2311.14906_, 2023.

* [17] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. _arXiv preprint arXiv:2312.14238_, 2023.
* [18] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality, 2023.
* [19] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [20] Shangzhe Di and Weidi Xie. Grounded question-answering in long egocentric videos. _arXiv preprint arXiv:2312.06505_, 2023.
* [21] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. Faith and fate: Limits of transformers on compositionality. _Advances in Neural Information Processing Systems_, 36, 2024.
* [22] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19358-19369, 2023.
* [23] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Petersen, and Julius Berner. Mathematical capabilities of chatgpt. _Advances in Neural Information Processing Systems_, 36, 2024.
* [24] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In _ICCV_, pages 5267-5275, 2017.
* [25] Alex Gorban, Haroon Idrees, Yu-Gang Jiang, A Roshan Zamir, Ivan Laptev, Mubarak Shah, and Rahul Sukthankar. Thumos challenge: Action recognition with a large number of classes, 2015.
* [26] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In _CVPR_, pages 18995-19012, 2022.
* [27] Albert Gu and Tri Dao. Mamboa: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_, 2023.
* [28] Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Xi Chen, and Bo Zhao. Vtg-llm: Integrating timestamp knowledge into video llms for enhanced video temporal grounding. _arXiv preprint arXiv:2405.13382_, 2024.
* [29] Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool. Creating summaries from user videos. In _ECCV_, pages 505-520, 2014.
* [30] Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Jun-Yan He, Jin-Peng Lan, Bin Luo, and Xuansong Xie. Multi-modal instruction tuned llms with fine-grained visual perception. _arXiv preprint arXiv:2403.02969_, 2024.
* [31] Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, and Bo Zhao. Efficient multimodal learning from data-centric perspective. _arXiv preprint arXiv:2402.11530_, 2024.
* [32] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.

* [33] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. _arXiv preprint arXiv:2311.18445_, 2(3):9, 2023.
* [34] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. _arXiv preprint arXiv:2403.19046_, 2024.
* [35] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and Radu Soricut. Multimodal pretraining for dense video captioning. _arXiv preprint arXiv:2011.11760_, 2020.
* [36] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021.
* [37] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In _International conference on machine learning_, pages 4651-4664. PMLR, 2021.
* [38] Samy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying. _arXiv preprint arXiv:2402.01032_, 2024.
* [39] Yu-Gang Jiang, Jingen Liu, A Roshan Zamir, George Toderici, Ivan Laptev, Mubarak Shah, and Rahul Sukthankar. Thumos challenge: Action recognition with a large number of classes, 2014.
* [40] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. _arXiv preprint arXiv:2311.08046_, 2023.
* [41] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4015-4026, 2023.
* [42] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In _Proceedings of the IEEE international conference on computer vision_, pages 706-715, 2017.
* [43] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. _arXiv preprint arXiv:2308.00692_, 2023.
* [44] Jie Lei, Tamara L Berg, and Mohit Bansal. Qvhighlights: Detecting moments and highlights in videos via natural language queries. In _NeurIPS_, 2021.
* [45] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seedbench: Benchmarking multimodal llms with generative comprehension. _arXiv preprint arXiv:2307.16125_, 2023.
* [46] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _International conference on machine learning_, pages 19730-19742. PMLR, 2023.
* [47] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. _arXiv preprint arXiv:2305.06355_, 2023.
* [48] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. _arXiv preprint arXiv:2311.17005_, 2023.
* [49] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. M\({}^{3}\)it: A large-scale dataset towards multi-modal multilingual instruction tuning. _arXiv preprint arXiv:2306.04387_, 2023.

* [50] Shuailin Li, Yuang Zhang, Yucheng Zhao, Qiuyue Wang, Fan Jia, Yingfei Liu, and Tiancai Wang. Vlm-eval: A general evaluation on video large language models. _arXiv preprint arXiv:2311.11865_, 2023.
* [51] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. _arXiv preprint arXiv:2311.17043_, 2023.
* [52] Zeqian Li, Qirui Chen, Tengda Han, Ya Zhang, Yanfeng Wang, and Weidi Xie. A strong baseline for temporal video-text alignment. _arXiv preprint arXiv:2312.14055_, 2023.
* [53] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. _arXiv preprint arXiv:2311.10122_, 2023.
* [54] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pages 74-81, 2004.
* [55] Jingyang Lin, Hang Hua, Ming Chen, Yikang Li, Jenhao Hsiao, Chiuman Ho, and Jiebo Luo. Videoxum: Cross-modal visual and textural summarization of videos. _IEEE Transactions on Multimedia_, 2023.
* [56] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Z Xu, Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentric video-language pretraining. _Advances in Neural Information Processing Systems_, 35:7575-7586, 2022.
* [57] Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, and Mike Zheng Shou. Univtg: Towards unified video-language temporal grounding. In _CVPR_, pages 2794-2804, 2023.
* [58] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.
* [59] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2024.
* [60] Ye Liu, Jixuan He, Wanhua Li, Junsik Kim, Donglai Wei, Hanspeter Pfister, and Chang Wen Chen. \(r^{2}\)-tuning: Efficient image-to-video transfer learning for video temporal grounding. _arXiv preprint arXiv:2404.00801_, 2024.
* [61] Ye Liu, Siyuan Li, Yang Wu, Chang Wen Chen, Ying Shan, and Xiaohu Qie. Umt: Unified multi-modal transformers for joint video moment retrieval and highlight detection. In _CVPR_, pages 3042-3051, 2022.
* [62] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281_, 2023.
* [63] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video l lms really understand videos? _arXiv preprint arXiv:2403.00476_, 2024.
* [64] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _ICLR_, 2019.
* [65] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual tokenization for grounding multimodal large language models. _arXiv preprint arXiv:2404.13013_, 2024.
* [66] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language model. Technical Report arXiv:2306.05424, 2023.
* [67] Kartikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: A diagnostic benchmark for very long-form video language understanding. _Advances in Neural Information Processing Systems_, 36, 2024.

* [68] WonJun Moon, Sangeek Hyun, SangUK Park, Dongchan Park, and Jae-Pil Heo. Query-dependent video representation for moment retrieval and highlight detection. In _CVPR_, pages 23023-23033, 2023.
* [69] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models. _arXiv preprint arXiv:2311.16103_, 2023.
* [70] Andrea-Maria Oncescu, Joao F Henriques, Yang Liu, Andrew Zisserman, and Samuel Albanie. Queryd: A video dataset with high-quality text and audio narrations. In _ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 2265-2269. IEEE, 2021.
* [71] OpenAI. Gpt-4v(ision) system card, 2023.
* [72] OpenAI. Hello gpt-4o, 2024.
* [73] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318, 2002.
* [74] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: A diagnostic benchmark for multimodal video models. _Advances in Neural Information Processing Systems_, 36, 2024.
* [75] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. _arXiv preprint arXiv:2306.14824_, 2023.
* [76] Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, Tat-Seng Chua, Yueting Zhuang, and Siliang Tang. Momentor: Advancing video large language model with fine-grained temporal reasoning. _arXiv preprint arXiv:2402.11435_, 2024.
* [77] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, pages 8748-8763, 2021.
* [78] Santhosh Kumar Ramakrishnan, Ziad Al-Halah, and Kristen Grauman. Naq: Leveraging narrations as queries to supervise episodic memory. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6694-6703, 2023.
* [79] Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. Grounding action descriptions in videos. _Transactions of the Association for Computational Linguistics_, 1:25-36, 2013.
* [80] Michel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. _arXiv preprint arXiv:2403.05530_, 2024.
* [81] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. _arXiv preprint arXiv:1908.10084_, 2019.
* [82] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: A time-sensitive multimodal large language model for long video understanding. _arXiv preprint arXiv:2312.02051_, 2023.
* [83] Zheng Shou, Dongang Wang, and Shih-Fu Chang. Temporal action localization in untrimmed videos via multi-stage cnns. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1049-1058, 2016.
* [84] Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian Rupprecht, Bernt Schiele, and Hilde Kuehne. Howtocaption: Prompting llms to transform video annotations at scale. _arXiv preprint arXiv:2310.04900_, 2023.

* [85] Gunnar A Sigurdsson, Gul Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In _ECCV_, pages 510-526, 2016.
* [86] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. _arXiv preprint arXiv:2307.16449_, 2023.
* [87] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum: Summarizing web videos using titles. In _CVPR_, pages 5179-5187, 2015.
* [88] Jinhwan Sul, Jihoon Han, and Joonseok Lee. Mr. hisum: A large-scale dataset for video highlight detection and summarization. _Advances in Neural Information Processing Systems_, 36, 2024.
* [89] Min Sun, Ali Farhadi, and Steve Seitz. Ranking domain-specific highlights by analyzing edited videos. In _ECCV_, pages 787-802, 2014.
* [90] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: A large-scale dataset for comprehensive instructional video analysis. In _CVPR_, pages 1207-1216, 2019.
* [91] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [92] InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities, 2023.
* [93] MiniCPM-V Team. Minicpm-llama3-v 2.5: A gpt-4v level multimodal llm on your phone, 2024.
* [94] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. Technical Report arXiv:2302.13971, 2023.
* [95] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [96] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, pages 5998-6008, 2017.
* [97] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4566-4575, 2015.
* [98] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran Cheng, and Ping Luo. End-to-end dense video captioning with parallel decoding. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 6847-6857, 2021.
* [99] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Vision1lm: Large language model is also an open-ended decoder for vision-centric tasks. _Advances in Neural Information Processing Systems_, 36, 2024.
* [100] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: A large-scale video-text dataset for multimodal understanding and generation. _arXiv preprint arXiv:2307.06942_, 2023.
* [101] Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, and Dongyan Zhao. Hawkeye: Training video-text lllms for grounding text in videos. _arXiv preprint arXiv:2403.10228_, 2024.

* [102] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. Star: A benchmark for situated reasoning in real-world videos. _arXiv preprint arXiv:2405.09711_, 2024.
* [103] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In _CVPR_, pages 5288-5296, 2016.
* [104] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. _arXiv preprint arXiv:2404.16994_, 2024.
* [105] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. _arXiv preprint arXiv:2311.04257_, 2023.
* [106] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. _arXiv preprint arXiv:2404.16006_, 2024.
* [107] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. _arXiv preprint arXiv:2311.16502_, 2023.
* [108] Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. Hierarchical video-moment retrieval and step-captioning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 23056-23065, 2023.
* [109] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11975-11986, 2023.
* [110] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. _arXiv preprint arXiv:2306.02858_, 2023.
* [111] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. _arXiv preprint arXiv:2309.15112_, 2023.
* [112] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. Hacs: Human action clips and segments dataset for recognition and temporal localization. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8668-8678, 2019.
* [113] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* [114] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. _arXiv preprint arXiv:2310.01852_, 2023.
* [115] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.
* [116] Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Cross-task weakly supervised learning from instructional videos. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3537-3545, 2019.

## Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes]
* Did you include the license to the code and datasets? [No]
* Did you include the license to the code and datasets? [N/A]

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] 3. Did you discuss any potential negative societal impacts of your work? [Yes] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] 3. Did you include any new assets either in the supplemental material or as a URL? [N/A] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]