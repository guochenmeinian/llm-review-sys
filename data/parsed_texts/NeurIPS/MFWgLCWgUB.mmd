# Random Cuts are Optimal for Explainable \(k\)-Medians

Konstantin Makarychev

Northwestern University &Liren Shan

TTIC

Equal contribution.

###### Abstract

We show that the RandomCoordinateCut algorithm gives the optimal competitive ratio for explainable \(k\)-medians in \(\ell_{1}\). The problem of explainable \(k\)-medians was introduced by Dasgupta, Frost, Moshkovitz, and Rashtchian in 2020. Several groups of authors independently proposed a simple polynomial-time randomized algorithm for the problem and showed that this algorithm is \(O(\log k\log\log k)\) competitive. We provide a tight analysis of the algorithm and prove that its competitive ratio is upper bounded by \(2\ln k+2\). This bound matches the \(\Omega(\log k)\) lower bound by Dasgupta et al (2020).

## 1 Introduction

Machine learning is being increasingly used to make decisions for critical applications, such as healthcare, finance, and public policy. Considering the profound impact of algorithmic decisions on individuals and society, it is essential to understand the underlying logic behind these decisions. In this paper, we explore an explainable \(k\)-medians clustering algorithm (called RandomCoordinateCut). The algorithm's aim is to cluster data sets and present results in a manner easily understood and visualized by humans.

Clustering is a fundamental task in unsupervised learning. Among many clustering methods, \(k\)-means, \(k\)-medians, and \(k\)-medoids are particularly popular. These are centroid-based methods that choose \(k\) centers and assign each data point to the center nearest to it. As a result, each cluster is a Voronoi cell in the Voronoi partition of the space. Since these cells may have a complicated boundary (see Figure 1 for an example of \(k\)-medians), it is not always easy for humans to comprehend and visualize such clustering.

To address this problem, Dasgupta, Frost, Moshkovitz, and Rashtchian (2020) introduced explainable \(k\)-means and \(k\)-medians clustering. They argued that decision trees are easy to understand and interpret. Therefore, in order to make clustering more explainable, we need to use threshold decision trees to define clusters. A threshold decision tree is a binary space partitioning tree with \(k\) leaves. Each internal node of the threshold decision tree splits the data into two groups using a threshold cut \((j,\theta)\): on the one side of the cut, we have points \(x\) with \(x_{j}\leq\theta\) and, on the other side, points \(x\) with \(x_{j}>\theta\). Thus, every node of the tree corresponds to a rectangular region of the space. A decision tree with \(k\) leaves partitions data set \(X\) into \(k\) clusters, \(P_{1},\ldots,P_{k}\). See Figure 1 for an example. Dasgupta et al. (2020) suggested that we use the standard \(k\)-medians and \(k\)-means objectives to measure the cost of the threshold decision tree. For \(k\)-medians, the cost of a threshold decision tree \(\mathcal{T}\) equals

\[\mathrm{cost}(X,\mathcal{T})=\sum_{i=1}^{k}\sum_{x\in P_{i}}\|x-\hat{c}^{i}\| _{1},\]

where \(P_{1},\ldots,P_{k}\) is the partitioning of \(X\) produced by \(\mathcal{T}\); and \(\hat{c}^{1},\ldots,\hat{c}^{k}\) are the medians of clusters \(P_{1},\ldots,P_{k}\). We denote the \(\ell_{1}\)-norm by \(\|\cdot\|_{1}\). Note that each \(P_{i}\) is a rectangular region of the space. Thus, generally speaking, every \(x\) is not assigned to the closest center \(\hat{c}^{1},\ldots,\hat{c}^{k}\) like in unconstrained \(k\)-medians or \(k\)-means.

Dasgupta, Frost, Moshkovitz, and Rashtchian (2020) defined the price of explainability as the ratio of the \(k\)-medians cost of explainable clustering to the optimal cost of unconstrained \(k\)-medians clustering. They showed that the cost of explainability for \(k\)-means and \(k\)-medians (somewhat surprisingly) does not depend on the number of points in the data set \(X\) and only depends on \(k\). Specifically, they provided a greedy algorithm that given \(k\) reference centers \(c^{1},c^{2},\cdots,c^{k}\) of any unconstrained \(k\)-medians as input, outputs a threshold decision tree of cost at most \(O(k)\) times the cost of original unconstrained \(k\)-medians with centers \(c^{1},c^{2},\cdots,c^{k}\). We call such an algorithm \(O(k)\) competitive. To get an explainable \(k\)-medians clustering, we first obtain reference centers \(c^{1},c^{2},\cdots,c^{k}\) using an off-the-shelf approximation algorithm for \(k\)-medians and then run an \(\alpha\)-competitive algorithm for explainable \(k\)-medians with centers \(c^{1},c^{2},\cdots,c^{k}\) given as input. This algorithm produces the desired threshold decision tree. Dasgupta et al. (2020) also gave an \(O(k^{2})\) competitive algorithm for \(k\)-means and showed \(\Omega(\log k)\) lower bounds on the price of explainability for both \(k\)-medians and \(k\)-means.

The notion of explainable clustering immediately got a lot of attention in the field (Laber and Murtinho (2021); Makarychev and Shan (2021); Gamlath et al. (2021); Charikar and Hu (2022); Esfandiari et al. (2022)). Particularly, Makarychev and Shan (2021); Esfandiari, Mirrokni, and Narayanan (2022) provided almost optimal algorithms for explainable \(k\)-medians, and Makarychev and Shan (2021); Esfandiari, Mirrokni, and Narayanan (2022); Gamlath, Jia, Polak, and Svensson (2021) provided almost optimal algorithms for \(k\)-means. The competitive ratios of these algorithms are \(\tilde{O}(\log k)\) for \(k\)-medians and \(\tilde{O}(k)\) for \(k\)-means.

The algorithms for explainable \(k\)-medians by Makarychev and Shan (2021); Esfandiari, Mirrokni, and Narayanan (2022); Gamlath, Jia, Polak, and Svensson (2021) are variants of the same simple algorithm, which we call RandomCoordinateCut. This algorithm receives a set of \(k\) reference centers \(c^{1},\ldots,c^{k}\) as input and then builds a threshold decision tree with \(k\) leaves. It works as follows. It recursively partitions \(d\)-dimensional space until every cell contains exactly one reference center \(c^{i}\). The algorithm starts with a tree consisting of one node, the root. Initially, all \(k\) reference centers are assigned to that root. At every step, the algorithm picks a random threshold cut \((j,\theta)\) and splits centers in every cell using this cut. If this cut does not separate any centers in a cell \(u\) (i.e., all centers in \(u\) are located on one side of the cut), then the algorithm does not split \(u\) into two regions at this step. Finally, for every leaf \(u\) of the constructed tree, the unique center that belongs to the cell corresponding to \(u\) is assigned to \(u\). We provide pseudo-code for this algorithm in Figure 2.

Makarychev and Shan (2021); Esfandiari et al. (2022) showed that the competitive ratio of RandomCoordinateCut is at most \(O(\log k\log\log k)\). That is, for every data set \(X\) and set of centers \(c^{1},\ldots,c^{k}\),

\[\mathbf{E}[\mathrm{cost}(X,\mathcal{T})]\leq O(\log k\log\log k)\cdot\sum_{x \in X}\min_{c\in\{c^{1},\ldots,c^{k}\}}\|x-c\|_{1}.\]

Note that the running time of this algorithm is \(\tilde{O}(kd)\). Gamlath, Jia, Polak, and Svensson (2021) provided a slightly worse bound of \(O(\log^{2}k)\) on the competitive ratio of this algorithm. They also

Figure 1: The unconstrained \(k\)-medians clustering and explainable \(k\)-medians clustering. The left diagram shows the Voronoi partition of the plane w.r.t. three centers in \(\ell_{1}\) distance. The Voronoi cell for each center consists of all points that are closer (in \(\ell_{1}\) distance) to this center than to any other center (the boundaries between cells are not straight lines because we use the \(\ell_{1}\) distance). The middle diagram shows an explainable partition. The right diagram shows the corresponding decision tree for explainable clustering.

conjectured that this algorithm is optimal and its competitive ratio is \(O(\log k)\), more specifically, \(H_{k-1}+1\), where \(H_{k}\) is the \(k\)-th harmonic number. They provided some justification for their conjecture by proving this bound for a very special set of centers and data points (corresponding to the case of completely disjoint sets in our Set Elimination Game).

**Our Results.** In this work, we show that indeed the competitive ratio of RandomCoordinateCut is at most \(2\ln k+2\), and, therefore, this algorithm has the optimal competitive ratio which matches the lower bound of Dasgupta, Frost, Moshkovitz, and Rashtchian (2020). Our analysis is not only tight but also fairly simple. To get our result we define a game, the Set Elimination Game, which was also implicitly analyzed in previous works on this topic. We show that the cost of this game is at most \(2\ln k+2\).

**Related Work.** The unconstrained \(k\)-medians clustering has been extensively studied. Charikar, Guha, Tardos, and Shmoys (1999) gave the first constant factor approximation algorithm for the problem in general metric spaces. Li and Svensson (2013) provided a \(1+\sqrt{3}+\varepsilon\) approximation algorithm. Byrka, Pensyl, Rybicki, Srinivasan, and Trinh (2017) improved the approximation factor to \(2.675+\varepsilon\). Cohen-Addad, Esfandiari, Mirrokni, and Narayanan (2022) recently improved the approximation factor to \(2.406\) for Euclidean \(k\)-medians. Megiddo and Supowit (1984) showed that the \(k\)-medians in \(\ell_{1}\) problem is NP-hard. Cohen-Addad and Lee (2022) showed that it is also NP-hard to approximate \(k\)-medians in \(\ell_{1}\) within a factor of \(1.06\).

As we discuss above, Gamlath, Jia, Polak, and Svensson (2021), Esfandiari, Mirrokni, and Narayanan (2022), Makarychev and Shan (2021), independently proposed the RandomCoordinateCut algorithm. They also gave an \(\tilde{O}(k)\) algorithm for explainable \(k\)-means and showed a lower bound of \(\tilde{\Omega}(k)\) for the problem. Charikar and Hu (2022) provided an \(O(k^{1-2/d}\cdot\mathrm{poly}(d,\log k))\) competitive algorithm for explainable \(k\)-means, whose competitive ratio depends on the dimension \(d\) of the instance. For small \(d\ll\log k/\log\log k\), their bound is better than \(O(k)\). They showed an almost matching \(\Omega(k^{1-2/d}/\mathrm{poly}\log k)\) lower bound for explainable \(k\)-means. Esfandiari et al. (2022) gave an upper bound of \(O(d\log^{2}d)\) on the competitive ratio of RandomCoordinateCut for explainable \(k\)-medians. This bound is better than \(O(\log k)\) for small \(d\ll\log k/\log\log k\). Laber and Murtinho (2021) gave \(O(d\log k)\) and \(O(dk\log k)\) competitive algorithms for explainable \(k\)-medians and \(k\)-means, respectively. Frost, Moshkovitz, and Rashtchian (2020) provided some empirical evidence that bi-criteria algorithms for explainable \(k\)-means (that partition the data set into \((1+\delta)k\) clusters) can give a much better competitive ratio than \(O(k)\). Then, Makarychev and Shan (2022) gave a \(\tilde{O}(\frac{1}{\delta}\log^{2}k)\) competitive bi-criteria algorithm for explainable \(k\)-means. Bandyapadhyay, Fomin, Golovach, Lochet, Purohit, and Simonov (2022) provided an algorithm

Figure 2: RandomCoordinateCut algorithm

that computes the optimal explainable \(k\)-medians and \(k\)-means clustering in time \(n^{2d+O(1)}\) and \((4nd)^{k+O(1)}\), respectively. Laber, Murtinho, and Oliveira (2023) proposed to use shallow decision trees for explainable clustering.

Independently and concurrently with our work, Gupta, Pittu, Svensson, and Yuan (2023) proved a \(O(\log k)\) bound on the price of explainability for \(k\)-medians. They showed that the competitive ratio of RandomCoordinateCut is \(1+H_{k-1}\), where \(H_{k}\) is the \(k\)-th harmonic number. Their work answers the open question raised by Gamlath, Jia, Polak, and Svensson (2021). They also proved a hardness of approximation result for explainable \(k\)-medians clustering and improved the competitive ratio for explainable \(k\)-means from \(O(k\log k)\) to \(O(k\log\log k)\).

## 2 Set Elimination Game

In this section, we define the set elimination game. Consider a discrete finite measure space \((\Omega,\mu)\). In this space, each element \(\omega\in\Omega\) has a measure of \(\mu(\omega)\), and the measure of every set \(S\subseteq\Omega\) equals \(\mu(S)=\sum_{\omega\in S}\mu(\omega)\). Let \(S_{1},S_{2},\ldots,S_{k}\subset\Omega\) be \(k\) distinct sets which may overlap with each other. The set elimination game proceeds in a series of rounds. Initially, all sets \(S_{1},\ldots,S_{k}\) enter the competition. Formally, they belong to the set of remaining sets \(\mathcal{R}_{0}=\{S_{1},\ldots,S_{k}\}\). At every round \(n\), the host picks a random \(\omega_{n}\in\Omega\) with probability \(\Pr(\omega_{n}=\omega)=\mu(\omega)/\mu(\Omega)\). Then, all sets \(S_{i}\) that contain \(\omega_{n}\) are eliminated from the game unless all remaining sets contain \(\omega_{n}\), in which case, no set gets eliminated. That is, for \(n\geq 1\),

\[\mathcal{R}_{n}=\begin{cases}\mathcal{R}_{n-1}\setminus\{S_{i}\in\mathcal{R} _{n-1}:\omega_{n}\in S_{i}\},&\text{if for some $S_{i}\in\mathcal{R}_{n-1},\omega_{n}\notin S_{i}$};\\ \mathcal{R}_{n-1},&\text{otherwise}.\end{cases}\] (1)

The last remaining set is declared the winner. We denote that winner by \(\mathrm{winner}\). We say that the cost of the game is the measure of the winning set, \(\mu(\mathrm{winner})\).

We remark that \(\mathcal{R}_{n}\) cannot get empty (in which case, the winner would not be defined) because of the "otherwise" clause in the definition (1). We shall always assume that all sets \(S_{1},\ldots,S_{k}\) are not only distinct and non-empty but also (a) for every \(i\), \(\mu(S_{i})>0\), and (b) for all \(i\) and \(j\), \(\mu(S_{i}\triangle S_{j})>0\) (here, \(S_{i}\triangle S_{j}\) denotes the symmetric difference of sets \(S_{i}\) and \(S_{j}\)). Then, in every game, there is a unique winner with probability \(1\).

We similarly define the set elimination game for arbitrary finite measure spaces: For an arbitrary finite measure space \((\Omega,\mu)\), element \(\omega_{n}\) is chosen with probability function \(\Pr(\omega_{n}\in S)=\mu(S)/\mu(\Omega)\).

Our main result is the following theorem, which, as we discuss later in Section 2.1, implies that the competitive ratio of the explainable clustering algorithm is \(2\ln k+2\).

**Theorem 2.1**.: _Consider a set elimination game with the finite measure space \((\Omega,\mu)\) and \(k\) distinct sets \(S_{1},S_{2},\ldots,S_{k}\) (as above). The expected cost of the game is at most_

\[\operatorname{\mathbf{E}}\bigl{[}\mu(\mathrm{winner})\bigr{]}\leq(2\ln k+2) \cdot\min_{i\in[k]}\mu(S_{i}).\]

To simplify the exposition, we will prove this theorem for discrete finite measure sets. If \(\Omega\) is not a discrete measure space, we first replace it with a quotient space: We say that \(\omega^{\prime}\in\Omega\) and \(\omega^{\prime\prime}\in\Omega\) are equivalent (\(\omega^{\prime}\sim\omega^{\prime\prime}\)) if they are contained in exactly the same set of sets \(S_{1},\ldots,S_{k}\). This equivalence relation partitions \(\Omega\) into at most \(2^{k}\) different equivalence classes. We replace \(\Omega\) with the quotient space \(\nicefrac{{\Omega}}{{\sim}}\) whose elements are equivalence classes. In other words, we merge all equivalent \(\omega\)'s. The measure of a new element \(\tilde{\omega}\) equals to the measure of the corresponding equivalence class.

**Organization.** In Section 2.1, we discuss the connection between explainable \(k\)-medians and set elimination games. We define a set elimination game in a set system \(I\subset\{S_{1},\ldots,S_{k}\}\) in Section 2.2. Then, we define the hitting and elimination time in Section 2.3. In Section 3, we first illustrate our proof strategy by showing Theorem 2.1 for the case when the smallest set \(S_{1}\) does not overlap with \(S_{2},\ldots,S_{k}\). An important ingredient of our proof is the notion of _surprise sets_, which we discuss in Section 3.1. Finally, we complete the proof of Theorem 2.1 in Section 3.2.

### Explainable \(k\)-Medians via Set Elimination Game

In this section, we show how to use Theorem 2.1 to obtain a bound of \(2\ln k+2\) on the competitive ratio of the RandomCoordinateCut algorithm.

**Theorem 2.2**.: _The competitive ratio of the RandomCoordinateCut algorithm for Explainable \(k\)-Medians is at most \(2\ln k+2\). That is, for every set of centers \(C=\{c^{1},\ldots,c^{k}\}\) and data set \(X\), the algorithm finds a random decision tree \(\mathcal{T}\) such that_

\[\mathbf{E}[\mathrm{cost}(X,\mathcal{T})]\leq(2\ln k+2)\cdot\sum_{x\in X}\min_{ c\in\{c^{1},\ldots,c^{k}\}}\|x-c\|_{1}.\]

The pseudo-code for the RandomCoordinateCut algorithm is provided in Figure 2.

Theorem 2.2 shows that given any \(k\) centers \(C=\{c^{1},\ldots,c^{k}\}\), RandomCoordinateCut finds a decision tree \(\mathcal{T}\) with cost at most \(2\ln k+2\) times the cost of unconstrained \(k\)-medians with centers \(C=\{c^{1},\ldots,c^{k}\}\). By using \(k\) centers given by any constant approximation algorithm for \(k\)-medians, RandomCoordinateCut finds a decision tree with cost at most \(O(\log k)\) times the optimal unconstrained \(k\)-medians cost. This implies an \(O(\log k)\) upper bound on the price of explainability.

Proof of Theorem 2.2.: Consider an arbitrary data set \(X\subset\mathbb{R}^{d}\) and set of \(k\) centers \(C\subset\mathbb{R}^{d}\). We assume that all points in \(X\) and all centers in \(C\) are in the cube \([-M,M]^{d}\). The threshold decision tree obtained by the RandomCoordinateCut algorithm partitions the space into \(k\) cells. Each cell contains a single reference center \(c^{i}\). The center \(c^{i}\) is not necessarily optimal for cluster \(P_{i}\) (cluster \(P_{i}\) is the intersection of the data set \(X\) and \(i\)-th cell). However, we will use it as a proxy for the optimal center. In other words, we will upper bound the cost of the threshold decision tree as follows:

\[\mathrm{cost}(X,\mathcal{T})\equiv\min_{\tilde{c}^{1},\ldots,\tilde{c}^{k}} \sum_{i=1}^{k}\sum_{x\in P_{i}}\|x-\tilde{c}^{i}\|_{1}\leq\sum_{i=1}^{k}\sum_{ x\in P_{i}}\|x-c^{i}\|_{1}.\]

Let \(\Omega\) be the set of all coordinate cuts: \(\Omega=\{(j,\theta):j\in[d],\theta\in[-M,M]\}\). We define a measure \(\mu\) on \(\Omega\) as follows. For every subset \(S\subset\Omega\), we set

\[\mu(S)=\sum_{j=1}^{d}\mu_{L}(\{\theta:(j,\theta)\in S\}),\]

where \(\mu_{L}\) is the Lebesgue measure on \(\mathbb{R}\). Thus, we have \(\mu(\Omega)=2dM\), which implies \((\Omega,\mu)\) is a finite measure space.

Consider any data point \(x\in X\). Define \(k\) sets \(S_{1},S_{2},\ldots,S_{k}\) for the set elimination game. For every \(i\in\{1,\ldots,k\}\), let \(S_{i}\) be the set of all threshold cuts that separate \(x\) and center \(c^{i}\), i.e.,

\[S_{i}=\{(j,\theta)\in\Omega:\text{sign}(x_{j}-\theta)\neq\text{sign}(c^{i}_{j }-\theta)\}.\]

Note that the \(\ell_{1}\) distance from \(x\) to center \(c^{i}\) equals the measure of \(S_{i}\): \(\|x-c^{i}\|_{1}=\mu(S_{i})\). We now examine the set elimination game with sets \(S_{1},\ldots,S_{k}\), measure space \((\Omega,\mu)\), and random sequence of draws \(\omega_{1},\omega_{2},\ldots\) (each \(\omega_{n}\in\Omega\) is the threshold cut chosen by the RandomCoordinateCut algorithm at step \(n\)). We claim that \(S_{i}\) belongs to \(\mathcal{R}_{n}\) if and only if center \(c^{i}\) lies in the same cell as point \(x\) after step \(n\) of the algorithm. This is the case for \(n=0\), since \(\mathcal{R}_{0}\) contains all sets \(S_{1},\ldots,S_{k}\) and the root of the threshold tree contains all centers \(c^{1},\ldots,c^{k}\). Then, whenever we pick cut \(\omega_{n}\), all centers separated from \(x\) by \(\omega_{n}\) are removed from the cell of \(x\). The only exception from this rule occurs when all centers in that cell lie on the same side of the cut \(\omega_{n}\). That is exactly the same rule as we have for the set elimination game (note that center \(c^{i}\) is separated from \(x\) by \(\omega_{n}\) if and only if \(\omega_{n}\in S_{i}\)). Therefore, the same sets \(S_{i}\) remain in the game as center \(c^{i}\) in the cell of \(x\) (namely, sets \(S_{i}\) and centers \(c^{i}\) have the same indices).

The RandomCoordinateCut algorithm stops when all leaves of the decision tree contain exactly one center. At this step, the set elimination game contains one set, \(S_{i}\). This set corresponds to the center \(c^{i}\) assigned to point \(x\). The cost of the game \(\mu(S_{i})\) equals the distance from \(x\) to \(c^{i}\). By Theorem 2.1, we have

\[\mathbf{E}[\mathrm{cost}(x,\mathcal{T})]=\mathbf{E}[\mu(\mathrm{winner})]\leq (2\ln k+2)\cdot\min_{i}\mu(S_{i})=(2\ln k+2)\cdot\min_{i}\|x-c^{i}\|_{1}.\]

We sum this bound over all data points \(x\) in \(X\) and get the desired result.

### Local Competitions

We now revisit the definition of the set elimination game and define competitions in subsets of \(\{S_{1},\dots,S_{k}\}\). For the rest of the proof, we assume \((\Omega,\mu)\) is a discrete finite measure space. We remind the reader that every set elimination game is determined by an infinite sequence of i.i.d. random variables \(\omega_{1},\omega_{2},\dots\). In each round \(n\), we sample an element \(\omega_{n}\) from \(\Omega\) with probability \(\Pr(\omega_{n}=\omega)=\mu(\omega)/\mu(\Omega)\).

**Definition 2.3**.: _Consider a finite measure space \((\Omega,\mu)\). Let \(I\) be a set of subsets of \(\Omega\). We say that \(I\) is a valid set system if (a) for every \(S\in I\), \(\mu(S)>0\), and (b) for every \(S^{\prime},S^{\prime\prime}\in I\), \(\mu(S^{\prime}\triangle S^{\prime\prime})>0\)._

The reader may assume that \(\mu(\omega)>0\) for all \(\omega\) in \(\Omega\). Then, the definition above says that in a valid set system \(I\), all sets are non-empty and distinct.

**Definition 2.4**.: _Consider a finite measure space \((\Omega,\mu)\). Let \(\omega_{1},\omega_{2},\dots\) be i.i.d. random variables as described above and \(I\) be a valid set system. We define a set elimination game in \(I\). Initially, \(\mathcal{R}_{0}(I)=I\). Then, for every \(n\geq 1\),_

\[\mathcal{R}_{n}(I)=\begin{cases}\mathcal{R}_{n-1}(I)\setminus\{S\in\mathcal{ R}_{n-1}(I):\omega_{n}\in S\},&\text{if for some $S^{\prime}\in\mathcal{R}_{n-1}(I),\omega_{n}\notin S^{\prime}$};\\ \mathcal{R}_{n-1}(I),&\text{otherwise}.\end{cases}\] (2)

_The winner of the game in \(I\), denoted by \(\operatorname{winner}(I)\), is the only element remaining, or, formally, the unique element in \(\cap_{n\geq 0}\mathcal{R}_{n}(I)\). If \(\cap_{n\geq 0}\mathcal{R}_{n}(I)\) contains more than one element, then the winner is not defined. The cost of the game is the measure of the winner, \(\mu(\operatorname{winner}(I))\)._

We remark that \(\cap_{n\geq 0}\mathcal{R}_{n}(I)\) contains exactly one element with probability \(1\). Thus, the winner and cost of the game are defined with probability \(1\).

Consider sets \(S_{1},\dots,S_{k}\) from Theorem 2.1. Denote \(K=\{S_{1},\dots,S_{k}\}\). The definition of the competition among sets \(S_{1},\dots,S_{k}\) (given in the beginning of Section 2) is exactly the same as the definition of competition in \(K\). Our goal is to show that \(\mathbf{E}[\mu(\operatorname{winner}(K))]\leq 2(\ln k+1)\cdot\min_{S_{i}\in K}\mu(S_{i})\). In the proof of Theorem 2.1, we will consider competitions in different set systems \(I\subseteq K\). We show the following key lemma. We defer the proof of Lemma 2.5 to Appendix A.

**Lemma 2.5**.: _Consider a partitioning of the set system \(K=\{S_{1},\dots,S_{k}\}\) into \(m\) sets \(I_{1},\dots,I_{m}\). Then, \(\operatorname{winner}(K)\in\big{\{}\operatorname{winner}(I_{1}),\dots, \operatorname{winner}(I_{m})\big{\}}\)._

### Set Elimination with Exponential Clock

Consider a set elimination game on sets \(S_{1},\dots,S_{k}\). It is determined by the sequence of random i.i.d. draws \(\omega_{1},\omega_{2},\dots\). Random variable \(\omega_{n}\) is chosen in round \(n\). We assign every round a random time \(\tau_{n}\). Let the time between two consecutive rounds be an exponential random variable with parameter \(\mu(\Omega)\). Specifically, let \(\Delta\tau_{1},\Delta\tau_{2},\dots\) be a sequence of i.i.d. exponential random variables with parameter \(\mu(\Omega)\) and each \(\tau_{n}=\tau_{n-1}+\Delta\tau_{n}=\Delta\tau_{1}+\dots+\Delta\tau_{n}\). Note that all \(\Delta\tau_{n}\) are positive and \(\tau_{1},\tau_{2},\dots\) is an increasing sequence with probability \(1\). The number of draws that occur by time \(t\) (i.e., \(N_{t}(\Omega)=|\{n:\tau_{n}\leq t\}|\)) is a Poisson process with parameter \(\mu(\Omega)\). We now can think of the set elimination game as follows: The host of the game observes a Poisson process with parameter \(\mu(\Omega)\). Whenever the process jumps (at time \(\tau_{n}\)), the host picks an element \(\omega_{n}\) in \(\Omega\) with probability \(\Pr(\omega_{n}=\omega)=\mu(\omega)/\mu(\Omega)\) and eliminates some sets according to the rules of the game discussed above. Note that by assigning every round some time \(\tau_{n}\), we do not change the game, the winner, and the cost of the game (because the sequence of random draws \(\omega_{1},\omega_{2},\dots\) remains the same as before). This interpretation of the game allows us to introduce a hitting time \(h(S)\) of every subset \(S\subset\Omega\) with the following properties: (a) each \(h(S)\) is an exponential random variable with rate \(\mu(S)\); (b) hitting times of disjoint sets are mutually independent random variables.

**Definition 2.6**.: _For every subset \(X\subset\Omega\), the hitting time \(h(X)\) is the time \(\tau_{n}\) when the first \(\omega_{n}\) is drawn from \(X\): \(h(X)=\min\{\tau_{n}:\omega_{n}\in X\}\). When the set contains one element \(\omega\), we will write \(h(\omega)\) instead of \(h(\{\omega\})\)._

We also define the elimination time of each set \(S_{i}\).

**Definition 2.7**.: _Consider any set elimination game with the measure space \((\Omega,\mu)\) and \(k\) sets \(S_{1},S_{2},\dots,S_{k}\) in \(\Omega\). The elimination time \(e(S_{i})\) of set \(S_{i}\) is the time when set \(S_{i}\) is eliminated from the game, i.e., \(e(S_{i})=\min\{\tau_{n}:S_{i}\notin\mathcal{R}_{n}(K)\}\). If \(S_{i}\) is the winner, then we let \(e(S_{i})=\infty\) (because the winner is never eliminated)._Note that \(e(S_{i})\geq h(S_{i})\). Sometimes, \(e(S_{i})\) may be equal to \(h(S_{i})\), but \(e(S_{i})\) and \(h(S_{i})\) are not always the same. We now prove that hitting times for disjoint sets are independent. To this end, we _split_ the Poisson process \(N_{t}(\Omega)=|\{n:\tau_{n}\leq t\}|\). Let \(N_{t}(\omega)=|\{n:\tau_{n}\leq t\text{ and }\omega_{n}=\omega\}|\). It is easy to see that \(N_{t}(\Omega)=\sum_{\omega\in\Omega}N_{t}(\omega)\) for every \(t\). It is also true that each \(N_{t}(\omega)\) is a Poisson process with parameter \(\mu(\omega)\) and all \(N_{t}(\omega)\) (for \(\omega\in\Omega\)) are mutually independent. This fact follows from the Coloring Theorem (see e.g., Kingman (1992), Coloring Theorem, page 53).

**Theorem 2.8** (Coloring Theorem).: _Let \(\Pi_{t}\) be a Poisson process on the real line with rate \(\lambda\). We color each event of the Poisson process randomly with one of \(M\) colors: The probability that a point receives the \(i\)-th color is \(p_{i}\). The colors of different points are independent. Let \(\Pi_{t}(i)\) be the number of events of color \(i\) in the interval \((0,t]\). Then, \(\Pi_{t}(1),\ldots,\Pi_{t}(M)\) are independent Poisson processes. The rate of process \(\Pi_{t}(i)\) is \(\mathcal{P}_{i}\)._

**Lemma 2.9**.: _For every \(\omega\in\Omega\), \(h(\omega)\) is an exponential random variable with parameter \(\mu(\omega)\), and all random variables \(h(\omega)\) (for \(\omega\in\Omega\)) are mutually independent._

Proof.: Observe that \(h(\omega)=\min\{t:N_{t}(\omega)\geq 1\}\). Thus, \(h(\omega)\) is an exponential random variable (the time of the first jump of a Poisson process) with rate \(\mu(\omega)\). Also, since all \(N_{t}(\omega)\) (for \(\omega\in\Omega\)) are mutually independent, all \(h(\omega)\) are also mutually independent. 

Note that the set elimination game depends only on the hitting times for elements \(\omega\) in \(\Omega\). This is the case because it matters only when every \(\omega\) is drawn the first time. At that time - the hitting time of \(\omega\) - all sets that contain \(\omega\) are eliminated unless all remaining sets contain this \(\omega\). When the same \(\omega\) is drawn again, it does not eliminate any new sets. Also, note that for any set \(S\subset\Omega\), the hitting time \(h(S)=\min_{\omega\in S}h(\omega)\). Thus, \(h(S)\) is an exponential random variable with parameter \(\mu(S)=\sum_{\omega\in S}\mu(\omega)\).

## 3 Proof of Main Result

We now present the proof of our main result, Theorem 2.1. We assume without loss of generality that \(S_{1}\) is the smallest set i.e., \(\mu(S_{1})\leq\mu(S_{i})\) for all \(i\). Then, the expected cost of the game is at most:

\[\mu(S_{1})+\sum_{i=2}^{k}\Pr\big{(}S_{i}=\mathrm{winner}(K)\big{)}\mu(S_{i}).\] (3)

We first provide some intuition for the proof by considering the case when \(S_{1}\) does not intersect with sets \(S_{2},\ldots,S_{k}\), i.e. sets \(S_{1}\) and \(S_{i}\) are disjoint for all \(i=2,3,\ldots,k\). We split all sets into two groups \(S_{1}\) and the rest of the sets \(S_{2},\ldots,S_{k}\). We know from Lemma 2.5 that the winner among all sets \(S_{1},\ldots,S_{k}\) is either \(S_{1}\) or \(\mathrm{winner}\left(\{S_{2},\ldots,S_{k}\}\right)\). Denote \(I^{-}=\{S_{2},\ldots,S_{k}\}\). Each set \(S_{i}\) is eliminated at time \(e(S_{i})\). The set \(S_{1}\) is eliminated at its hitting time \(h(S_{1})\) unless it is the only remaining set at time \(h(S_{1})\) (because we are considering the case when \(S_{1}\) does not overlap with other sets). Thus,

\[\mathrm{winner}(K)=\begin{cases}S_{1},&\text{if }h(S_{1})>e(\mathrm{winner}(I^{-} ));\\ \mathrm{winner}(I^{-}),&\text{if }e(\mathrm{winner}(I^{-}))>h(S_{1}).\end{cases}\] (4)

When the winner among \(S_{1},\ldots,S_{k}\) is not \(S_{1}\), we consider two cases of the winner \(S_{i}\): (1) \(S_{i}\) is a surprise set; (2) \(S_{i}\) is a non-surprise set.

**Definition 3.1**.: _We say that \(S_{i}\) is a surprise set if \(e(S_{i})\geq h(S_{1})\geq L/\mu(S_{i})\), where \(L=\ln k\)._

We call \(S_{i}\) a surprise set because the probability of the event \(e(S_{i})\geq h(S_{1})\geq L/\mu(S_{i})\) is small. We give a bound on the probability of \(e(S_{i})\geq h(S_{1})\geq L/\mu(S_{i})\) in Lemma 3.3. Here, we provide some intuition. By Lemma 2.9, the hitting time \(h(S_{i})\) is an exponential random variable with parameter \(\mu(S_{i})\). Thus, the expected hitting time for \(S_{i}\) is \(1/\mu(S_{i})\). Consider a set \(S_{i}\) with a small measure (\(\mu(S_{i})\) is close to \(\mu(S_{1})\)). If the hitting time \(h(S_{1})\geq L/\mu(S_{i})\), then \(h(S_{1})\) is much larger than its expected value \(1/\mu(S_{1})\), which happens with a small probability. Consider a set \(S_{i}\) with a large measure \(\mu(S_{i})\gg\mu(S_{1})\). Then, the expected hitting time for \(S_{i}\) is \(1/\mu(S_{i})\), which is much smaller than the expected hitting time of \(S_{1}\). Thus, the event \(e(S_{i})\geq h(S_{1})\) occurs with a small probability.

Let us examine bound (3). Let \(Surprise\) be the set of all surprise sets. Note that \(Surprise\) is a random set. Then,

\[\sum_{i=2}^{k}\Pr\big{(}S_{i}=\mathrm{winner}(K)\big{)}\mu(S_{i}) \leq\sum_{i=2}^{k}\Pr\big{(}S_{i}=\mathrm{winner}(K),\ S_{i}\notin Surprise \big{)}\cdot\mu(S_{i})\] (5) \[+\sum_{i=2}^{k}\Pr\big{(}S_{i}\in Surprise\big{)}\cdot\mu(S_{i}).\]

We show in the next section (Lemma 3.3) that the second sum is upper bounded by \(\mu(S_{1})\). We now bound the first sum. For every winner \(S_{i}\) which is not a surprise set, we have \(e(S_{i})\geq h(S_{1})\) (because \(S_{i}\) is the winner) and \(h(S_{1})\leq L/\mu(S_{i})\) (because \(S_{i}\) is not a surprise set). We also have \(S_{i}=\mathrm{winner}(I^{-})\), thus

\[\Pr\big{(}S_{i}=\mathrm{winner}(K),\ S_{i}\notin Surprise\big{)}\leq\Pr\big{(} h(S_{1})\leq L/\mu(S_{i})\ \text{and}\ S_{i}=\mathrm{winner}(I^{-})\big{)}.\]

By Lemma 2.9, all hitting times \(h(S_{i})=\min_{\omega\in S_{i}}h(\omega)\) for \(i\geq 2\) are independent from \(h(S_{1})\). Thus, \(\mathrm{winner}(I^{-})\) is also independent of \(h(S_{1})\) (\(\mathrm{winner}(I^{-})\) depends only on the hitting times for sets \(S_{i}\in I^{-}\)). Therefore,

\[\Pr\big{(}S_{i}=\mathrm{winner}(K),\ S_{i}\notin Surprise\big{)} \leq\Pr\big{(}h(S_{1})\leq L/\mu(S_{i})\big{)}\cdot\Pr\big{(}S_{i }=\mathrm{winner}(I^{-})\big{)}\] \[=\underbrace{\Big{(}1-e^{-L\mu(S_{1})/\mu(S_{i})}\Big{)}}_{\leq L \mu(S_{1})/\mu(S_{i})}\cdot\Pr\big{(}S_{i}=\mathrm{winner}(I^{-})\big{)}\] \[\leq\Pr\big{(}S_{i}=\mathrm{winner}(I^{-})\big{)}\cdot L\cdot \mu(S_{1})/\mu(S_{i}).\]

We combine all bounds on terms of (5) and get the following bound on the expected cost of the game:

\[\mu(S_{1})+\sum_{i=2}^{k}\Pr\big{(}S_{i}=\mathrm{winner}(I^{-})\big{)}\cdot L \cdot\mu(S_{1})+\mu(S_{1})=(L+2)\cdot\mu(S_{1})=(\ln k+2)\cdot\mu(S_{1}).\]

This concludes the proof of the theorem for the case when \(S_{1}\) does not overlap with \(S_{2},\ldots,S_{k}\). We now analyze surprise sets.

### Surprise Sets

In this section, we prove a bound on the probability that a set \(S_{i}\) is a surprise set. We no longer assume that \(S_{1}\) does not intersect with other sets \(S_{i}\). We first show a lemma about exponential random variables.

**Lemma 3.2**.: _Let \(X\) and \(Y\) be two independent exponential random variables with positive parameters \(\lambda_{X}\) and \(\lambda_{Y}\), respectively. Then, for every \(T\geq 0\), we have_

\[\Pr\big{(}Y\geq X\geq T\big{)}=\frac{\lambda_{X}}{\lambda_{X}+\lambda_{Y}} \cdot e^{-(\lambda_{X}+\lambda_{Y})T}.\] (6)

Proof.: The desired probability can be easily found by computing \(\int_{T}^{\infty}(F_{X}(t)-F_{X}(T))f_{Y}(t)dt\), where \(F_{X}(t)=1-e^{-\lambda_{X}t}\) is the cumulative distribution function of \(X\), and \(f_{Y}(t)=\lambda_{Y}\cdot e^{-\lambda_{Y}t}\) is the probability density function of \(Y\). Here, we give an alternative proof. Write,

\[\Pr\big{(}Y\geq X\geq T\big{)} =\Pr\big{(}Y\geq X\ \&\ \min(X,Y)\geq T\big{)}\] \[=\Pr\big{(}X\leq Y\mid\min(X,Y)\geq T\big{)}\cdot\Pr\big{(}\min( X,Y)\geq T\big{)}.\]

We have \(\Pr\big{(}\min(X,Y)\geq T\big{)}=e^{-(\lambda_{X}+\lambda_{Y})T}\), because the minimum of two independent exponential random variables with parameters \(\lambda_{X}\) and \(\lambda_{Y}\) is an exponential random variable with parameter \(\lambda_{X}+\lambda_{Y}\). Then, \(\Pr\big{(}X\leq Y\mid\min(X,Y)\geq T\big{)}=\Pr\big{(}X\leq Y\big{)}\) because the exponential distribution is memoryless; and \(\Pr\big{(}X\leq Y\big{)}=\lambda_{X}/(\lambda_{X}+\lambda_{Y})\). 

**Lemma 3.3**.: _For every set \(S_{i}\), we have_

\[\Pr(S_{i}\text{ is surprise set})\leq\frac{1}{k}\cdot\frac{\mu(S_{1})}{\mu(S_{i})}.\]Proof.: First, we show that \(\min(e(S_{i}),h(S_{1}))\leq h(S_{i}\setminus S_{1})\).

**Claim 3.4**.: _We always have \(\min(e(S_{i}),h(S_{1}))\leq h(S_{i}\setminus S_{1})\)._

Proof.: Consider an arbitrary realization of the game and the time \(t=h(S_{i}\setminus S_{1})\) when \(S_{i}\setminus S_{1}\) is hit. If by this time, \(S_{1}\) has already been hit then \(h(S_{1})<t\). Similarly, if by this time, \(S_{i}\) has already been eliminated then \(e(S_{i})<t\). Otherwise, both \(S_{1}\) and \(S_{i}\) are still remaining in the game at time \(t\). Therefore, when we pick \(\omega\in S_{i}\setminus S_{1}\) at time \(t\), set \(S_{i}\) gets eliminated (since \(\omega\in S_{i}\); \(\omega\notin S_{1}\); both \(S_{1}\) and \(S_{i}\) are remaining in the game). Thus, in this case, \(e(S_{i})=t\). This concludes the proof. 

If \(S_{i}\) is a surprise set, then \(\min(e(S_{i}),h(S_{1}))=h(S_{1})\geq L/\mu(S_{i})\). By Claim 3.4, we have

\[h(S_{i}\setminus S_{1})\geq\min\big{(}e(S_{i}),h(S_{1})\big{)}=h(S_{1})\geq L /\mu(S_{i}).\]

Thus, \(\Pr(S_{i}\text{ is surprise set})\leq\Pr\Big{(}h(S_{i}\setminus S_{1})\geq h(S_{1}) \geq L/\mu(S_{i})\Big{)}\). By Lemma 3.2 applied to the independent exponential random variables \(h(S_{1})\), \(h(S_{i}\setminus S_{1})\), and time \(T=L/\mu(S_{i})\), we have

\[\Pr(S_{i}\text{ is surprise set})\leq\frac{\mu(S_{1})}{\mu(S_{i}\setminus S_{1}) +\mu(S_{1})}\cdot e^{-\frac{L(\mu(S_{1}\setminus S_{1})+\mu(S_{1}))}{\mu(S_{ i})}}\leq\frac{1}{k}\cdot\frac{\mu(S_{1})}{\mu(S_{i})}.\]

### General Case

Proof of Theorem 2.1.: We upper bound the expected cost of the game for arbitrary sets \(S_{1},\ldots,S_{k}\). As before, we assume that \(S_{1}\) is the smallest set. We remind the reader that each hitting time \(h(S_{i})\) is an exponential random variable with parameter \(\mu(S_{i})\). In the proof, we will use the definitions of surprise sets (see Definitions 3.1). We also set \(L=\ln k\). We define all sets \(S_{i}\) for \(i\neq 1\) that are not a surprise set to be non-surprise sets.

We separately upper bound the cost of the winner depending on whether the winner is (a) set \(S_{1}\), (b) surprise set, (c) non-surprise set. Write

\[\mathbf{E}\big{[}\mu(\mathrm{winner}(K))\big{]} =\mathbf{E}\big{[}\mu(\mathrm{winner}(K))\cdot\mathbf{1}\{ \mathrm{winner}(K)=S_{1}\}\big{]}\] (a) \[+\mathbf{E}\big{[}\mu(\mathrm{winner}(K))\cdot\mathbf{1}\{ \mathrm{winner}\text{ is surprise set}\}\big{]}\] (b) \[+\mathbf{E}\big{[}\mu(\mathrm{winner}(K))\cdot\mathbf{1}\{ \mathrm{winner}\text{ is non-surprise set}\}\big{]}.\] (c)

Term (a) is upper bounded by \(\mu(S_{1})\). We bound term (b) using Lemma 3.3: The probability that a set is a surprise set is at most \(\nicefrac{{1}}{{k}}\cdot\mu(S_{1})/\mu(S_{i})\). Thus, the expected total measure of all sets (not only the surprise winner) is upper bounded by \(\frac{1}{k}\sum_{i=2}^{k}\frac{\mu(S_{1})}{\mu(S_{i})}\mu(S_{i})<\mu(S_{1})\).

We now bound term (c). Define a new random variable: Let \(\mathrm{cost}(\omega)\) be the cost of the winner (i.e., \(\mu(S_{i})\), where \(S_{i}\) is the winner) if (1) the winner is a non-surprise set, and (2) \(\omega\) is the first element that was chosen in \(S_{1}\). We let \(\mathrm{cost}(\omega)=0\), otherwise. If \(\omega\) is the first element that was chosen in \(S_{1}\), then \(h(S_{1})=h(\omega)\). So, the definition of \(\mathrm{cost}(\omega)\) can be written as follows:

\[\mathrm{cost}(\omega)=\mu(\mathrm{winner}(K))\cdot\mathbf{1}\{h(S_{1})=h( \omega)\}\cdot\mathbf{1}\{\mathrm{winner}(K)\not\in Surprise\}.\]

Since the hitting time \(h(S_{1})\) is finite with probability \(1\), the term (c) equals

\[(c)=\sum_{\omega\in S_{1}}\mathbf{E}[\mathrm{cost}(\omega)].\]

Lemma 3.5, which we prove below, gives a bound of \(2L\mu(S_{1})\) on the expression above. Combining upper bounds on terms (a), (b), and (c), we get

\[\mathbf{E}\big{[}\mu(\mathrm{winner}(K))\big{]}\leq(1+2L+1)\mu(S_{1})=(2\ln k+2 )\cdot\mu(S_{1}).\]

**Lemma 3.5**.: _For every \(\omega\in S_{1}\), we have \(\mathbf{E}[\mathrm{cost}(\omega)]\leq 2L\mu(\omega)\)._Proof.: We have

\[\mathbf{E}[\mathrm{cost}(\omega)]=\mathbf{E}\Big{[}\mu(\mathrm{winner}(K))\cdot \mathbf{1}\{h(S_{1})=h(\omega)\}\cdot\mathbf{1}\{\mathrm{winner}(K)\not\in Surprise\} \Big{]}.\] (7)

If \(S_{i}\) is a non-surprise set, then \(h(S_{1})<L/\mu(S_{i})\) or \(e(S_{i})<h(S_{1})\). If \(S_{i}\) is the winner, then \(e(S_{i})\geq h(S_{1})\). Thus, if \(S_{i}\) is a non-surprise winner, then \(h(S_{1})<L/\mu(S_{i})\). This observations gives us the following upper bound on (7):

\[\mathbf{E}\big{[}\,\mathrm{cost}(\omega)\big{]}\leq\sum_{i=2}^{k}\mu(S_{i}) \cdot\Pr\Big{(}S_{i}=\mathrm{winner}(K)\text{ and }h(\omega)=h(S_{1})<L/\mu(S_{i})\Big{)}.\] (8)

Define two set systems \(I_{\omega}^{-}\) and \(I_{\omega}^{+}\) of sets \(S_{i}\) containing and not containing \(\omega\):

\[I_{\omega}^{-} =\{S_{i}:\omega\notin S_{i}\text{ and }i\geq 2\};\] \[I_{\omega}^{+} =\{S_{i}:\omega\in S_{i}\text{ and }i\geq 2\}.\]

Note that \(K\equiv\{S_{1},\ldots,S_{k}\}=\{S_{1}\}\cup I_{\omega}^{-}\cup I_{\omega}^{+}\). By Lemma 2.5,

\[\mathrm{winner}(K)\in\big{\{}S_{1},\mathrm{winner}(I_{\omega}^{-}),\mathrm{ winner}(I_{\omega}^{+})\big{\}}.\]

Observe that if \(S_{i}\) with \(i\geq 2\) is the winner, then \(S_{i}=\mathrm{winner}(I_{\omega}^{-})\) or \(S_{i}=\mathrm{winner}(I_{\omega}^{+})\). We replace the condition \(S_{i}=\mathrm{winner}(K)\) with \(S_{i}\in\{\mathrm{winner}(I_{\omega}^{-}),\mathrm{winner}(I_{\omega}^{+})\}\) in (8) and get bound:

\[\mathbf{E}\big{[}\,\mathrm{cost}(\omega)\big{]}\leq\sum_{i=2}^{k}\mu(S_{i}) \cdot\Pr\Big{(}S_{i}\in\{\mathrm{winner}(I_{\omega}^{-}),\mathrm{winner}(I_{ \omega}^{+})\}\text{ and }h(\omega)<\frac{L}{\mu(S_{i})}\Big{)}.\]

The key observation now is that sets \(\mathrm{winner}(I_{\omega}^{-})\) and \(\mathrm{winner}(I_{\omega}^{+})\) are independent of \(h(\omega)\). This is the case, because sets remaining in the competitions \(\mathcal{R}_{n}(I_{\omega}^{-})\) and \(\mathcal{R}_{n}(I_{\omega}^{+})\) do not change when we select \(\omega\). The set \(\mathcal{R}_{n}(I_{\omega}^{-})\) does not change in the round \(n\) when \(\omega\) is chosen because all sets \(S_{i}\) in \(\mathcal{R}_{n}(I_{\omega}^{-})\subset I_{\omega}^{-}\) do not contain \(\omega\). The set \(\mathcal{R}_{n}(I_{\omega}^{+})\) does not change in this round because all sets \(S_{i}\) in \(\mathcal{R}_{n}(I_{\omega}^{+})\subset I_{\omega}^{+}\) contain \(\omega\) and consequently when \(\omega\) is chosen, none of these sets is removed from \(\mathcal{R}_{n}(I_{\omega}^{+})\) (otherwise, \(\mathcal{R}_{n}(I_{\omega}^{+})\) would become empty). Thus,

\[\mathbf{E}\big{[}\,\mathrm{cost}(\omega)\big{]}\leq\sum_{i=2}^{k}\mu(S_{i}) \cdot\Pr\big{(}S_{i}\in\{\mathrm{winner}(I_{\omega}^{-}),\mathrm{winner}(I_{ \omega}^{+})\}\big{)}\cdot\Pr\Big{(}h(\omega)<\frac{L}{\mu(S_{i})}\Big{)}.\]

Using that \(h(\omega)\) is an exponential random variable with parameter \(\mu(\omega)\), we get (for every \(i\))

\[\mu(S_{i})\cdot\Pr\Big{(}h(\omega)\leq\frac{L}{\mu(S_{i})}\Big{)}=\mu(S_{i}) \cdot\Big{(}1-e^{-L\frac{\mu(\omega)}{\mu(S_{i})}}\Big{)}\leq\mu(S_{i})\cdot L \frac{\mu(\omega)}{\mu(S_{i})}=\mu(\omega)L.\]

Hence,

\[\mathbf{E}\big{[}\,\mathrm{cost}(\omega)\big{]}\leq\mu(\omega)L\cdot\sum_{i=2 }^{k}\Pr\big{(}S_{i}\in\{\mathrm{winner}(I_{\omega}^{-}),\mathrm{winner}(I_{ \omega}^{+})\}\big{)}.\]

The sum on the right hand side is at most \(2\). Thus, \(\mathbf{E}[\mathrm{cost}(\omega)]\leq 2L\mu(\omega)\). 

## Acknowledgments and Disclosure of Funding

The authors are supported by NSF Awards CCF-1955351, CCF-1934931, EECS-29 2216970.

## References

* Bandyapadhyay et al. [2022] Sayan Bandyapadhyay, Fedor Fomin, Petr A Golovach, William Lochet, Nidhi Purohit, and Kirill Simonov. How to find a good explanation for clustering? In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 3904-3912, 2022.
* Byrka et al. [2017] Jaroslaw Byrka, Thomas Pensyl, Bartosz Rybicki, Aravind Srinivasan, and Khoa Trinh. An improved approximation for k-median and positive correlation in budgeted optimization. _ACM Transactions on Algorithms (TALG)_, 13(2):1-31, 2017.
* Bickel et al. [2018]Moses Charikar and Lunjia Hu. Near-optimal explainable k-means for all dimensions. In _Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 2580-2606. SIAM, 2022.
* Charikar et al. [1999] Moses Charikar, Sudipto Guha, Eva Tardos, and David B Shmoys. A constant-factor approximation algorithm for the k-median problem. In _Proceedings of the thirty-first annual ACM symposium on Theory of computing_, pages 1-10, 1999.
* Cohen-Addad and Lee [2022] Vincent Cohen-Addad and Euiwoong Lee. Johnson coverage hypothesis: Inapproximability of k-means and k-median in lp-metrics. In _Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 1493-1530. SIAM, 2022.
* Cohen-Addad et al. [2022] Vincent Cohen-Addad, Hossein Esfandiari, Vahab Mirrokni, and Shyam Narayanan. Improved approximations for euclidean k-means and k-median, via nested quasi-independent sets. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1621-1628, 2022.
* Dasgupta et al. [2020] Sanjoy Dasgupta, Nave Frost, Michal Moshkovitz, and Cyrus Rashtchian. Explainable k-means and k-medians clustering. In _Proceedings of the 37th International Conference on Machine Learning_, pages 7055-7065, 2020.
* Esfandiari et al. [2022] Hossein Esfandiari, Vahab Mirrokni, and Shyam Narayanan. Almost tight approximation algorithms for explainable clustering. In _Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 2641-2663. SIAM, 2022.
* Frost et al. [2020] Nave Frost, Michal Moshkovitz, and Cyrus Rashtchian. Exkmc: Expanding explainable \(k\)-means clustering. _arXiv preprint arXiv:2006.02399_, 2020.
* Gamlath et al. [2021] Buddhima Gamlath, Xinrui Jia, Adam Polak, and Ola Svensson. Nearly-tight and oblivious algorithms for explainable clustering. _Advances in Neural Information Processing Systems_, 34:28929-28939, 2021.
* Gupta et al. [2023] Anupam Gupta, Madhusudhan Reddy Pittu, Ola Svensson, and Rachel Yuan. The price of explainability for clustering. _arXiv preprint arXiv:2304.09743_, 2023.
* Kingman [1992] John Frank Charles Kingman. _Poisson processes_, volume 3. Clarendon Press, 1992.
* Laber et al. [2023] Eduardo Laber, Lucas Murtinho, and Felipe Oliveira. Shallow decision trees for explainable k-means clustering. _Pattern Recognition_, 137:109239, 2023.
* Laber and Murtinho [2021] Eduardo S Laber and Lucas Murtinho. On the price of explainability for some clustering problems. In _International Conference on Machine Learning_, pages 5915-5925. PMLR, 2021.
* Li and Svensson [2013] Shi Li and Ola Svensson. Approximating k-median via pseudo-approximation. In _proceedings of the forty-fifth annual ACM symposium on theory of computing_, pages 901-910, 2013.
* Makarychev and Shan [2021] Konstantin Makarychev and Liren Shan. Near-optimal algorithms for explainable k-medians and k-means. In _International Conference on Machine Learning_, pages 7358-7367. PMLR, 2021.
* Makarychev and Shan [2022] Konstantin Makarychev and Liren Shan. Explainable k-means: don't be greedy, plant bigger trees! In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1629-1642, 2022.
* Megiddo and Supowit [1984] Nimrod Megiddo and Kenneth J Supowit. On the complexity of some common geometric location problems. _SIAM journal on computing_, 13(1):182-196, 1984.

Proof of Lemma 2.5

**Lemma 2.5**.: _Consider a partitioning of the set system \(K=\{S_{1},\ldots,S_{k}\}\) into \(m\) sets \(I_{1},\ldots,I_{m}\). Then,_

\[\operatorname{winner}(K)\in\big{\{}\operatorname{winner}(I_{1}),\ldots, \operatorname{winner}(I_{m})\big{\}}.\]

The proof of Lemma 2.5 relies on the following observation.

**Lemma A.1**.: _Let \(X\) and \(Y\) be two subsets of \(K\). If \(X\subset Y\), then for every \(n\), we always have_

\[\mathcal{R}_{n}(Y)\cap X=\mathcal{R}_{n}(X)\quad\text{or}\quad\mathcal{R}_{n} (Y)\cap X=\varnothing.\] (9)

Proof.: We prove that (9) holds by induction on \(n\). Initially, when \(n=0\), we have \(\mathcal{R}_{0}(X)=X\) and \(\mathcal{R}_{0}(Y)=Y\). Therefore, \(\mathcal{R}_{0}(Y)\cap X=X\cap Y=X=\mathcal{R}_{0}(X)\). Suppose (9) holds for \(n\), we prove that (9) also holds for \(n^{\prime}=n+1\). If \(\mathcal{R}_{n}(Y)\cap X=\varnothing\), then \(\mathcal{R}_{n}(Y)\cap X\) remains empty for all \(n^{\prime}\geq n\). Therefore, (9) holds for \(n+1\). So, let us assume that \(\mathcal{R}_{n}(Y)\cap X=\mathcal{R}_{n}(X)\). Consider three cases:

* If \(\omega_{n+1}\) belongs to all sets in \(\mathcal{R}_{n}(Y)\), then it also belongs to all sets in \(\mathcal{R}_{n}(X)=\mathcal{R}_{n}(Y)\cap X\). Thus, in this case, no set is eliminated in \(X\) or \(Y\). That is, \(\mathcal{R}_{n+1}(X)=\mathcal{R}_{n}(X)\) and \(\mathcal{R}_{n+1}(Y)=\mathcal{R}_{n}(Y)\).
* If \(\omega_{n+1}\) belongs to all sets in \(\mathcal{R}_{n}(X)\), but not all sets in \(\mathcal{R}_{n}(Y)\), then, at step \(n+1\), we remove all sets that contain \(\omega_{n+1}\) and, particularly, all sets in \(\mathcal{R}_{n}(X)\), from \(\mathcal{R}_{n}(Y)\). Consequently, \(\mathcal{R}_{n+1}(Y)\cap X=\varnothing\).
* If not all sets in \(\mathcal{R}_{n}(X)\) and not all sets in \(\mathcal{R}_{n}(Y)\) contain \(\omega_{n+1}\), then we remove exactly the same sets from both \(\mathcal{R}_{n}(X)\) and \(\mathcal{R}_{n}(Y)\cap X\). Namely, we remove sets \(S_{i}\in\mathcal{R}_{n}(Y)\) that contain \(\omega_{n+1}\).

We conclude that (9) holds for \(n^{\prime}=n+1\). 

Proof of Lemma 2.5.: Consider an arbitrary realization of the game \(\omega_{1},\omega_{2},\ldots\). Let \(n\) be the round when all sets but the winner are eliminated from the competition i.e., \(\mathcal{R}_{n}\) contains only one set, the winner. Since \(K\) is the union of \(I_{1},\ldots,I_{k}\), the winner must belong to some \(I_{j}\). Now, by Lemma A.1 for \(X=I_{j}\) and \(Y=K\), we have \(\mathcal{R}_{n}(K)\cap I_{j}=\mathcal{R}_{n}(I_{j})\) or \(\mathcal{R}_{n}(K)\cap I_{j}=\varnothing\). We know that \(\mathcal{R}_{n}(K)=\{\operatorname{winner}(K)\}\) and \(\operatorname{winner}(K)\in I_{j}\). Thus, \(\mathcal{R}_{n}(K)\cap I_{j}=\{\operatorname{winner}(K)\}\neq\varnothing\), and

\[\mathcal{R}_{n}(I_{j})=\mathcal{R}_{n}(K)\cap I_{j}=\{\operatorname{winner}(K )\}.\]

We conclude that at round \(n\), \(\mathcal{R}_{n}(I_{j})\) contains only one set - the winner in \(K\). Consequently, it is also the winner in \(I_{j}\) i.e., \(\operatorname{winner}(I_{j})=\operatorname{winner}(K)\). This finishes the proof.