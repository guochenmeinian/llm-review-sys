# PiCO: Peer Review in LLMs based on the Consistency Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel **unsupervised evaluation direction**, utilizing _peer-review_ mechanisms to measure LLMs automatically without any human feedback. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap in aligning human rankings. We perform experiments on multiple datasets with these metrics, validating the effectiveness of the proposed approach.

## 1 Introduction

Goodhart's Law: _"When a measure becomes a target, it ceases to be a good measure."_

Large language models (LLMs)[11; 2; 12; 43] have achieved remarkable success across a variety of real-world applications [54; 32; 36; 52]. With the increasingly widespread application of these models, there is an urgent need for an effective evaluation method to ensure that their performance and usability meet the growing demands. To assess the ability level of LLMs, a large number of evaluation benchmarks have been proposed by using some small and domain-specific datasets with human-curated labels, such as MMLU [26], HELM [30], Big-Bench[39], GLUE[45]. However, these benchmarks can only measure LLMs' core capability on a confined set of tasks (e.g. multi-choice knowledge or retrieval questions), which fails to assess their alignment with human preference in open-ended tasks adequately [16; 28; 34]. On the other hand, these evaluations may suffer from _benchmark leakage_ issue, referring that the evaluation data is unknowingly used for model training, which can also lead to misleading evaluations [49; 56]. Therefore, blindly improving scores on these public benchmarks cannot always yield a large language model that truly satisfies human requirements.

For assessing human preferences, recent studies have focused on building crowdsourced battle platforms with human ratings as the primary evaluation metric. Typical platforms include Chatbot Arena [55], MT-Bench [55], and AlpacaEval [29]. It constructs anonymous battles between chatbots in real-world scenarios, where users engage in conversations with two chatbots at the same time and rate their responses based on personal preferences. While human evaluation is the gold standard formeasuring human preferences, it is exceptionally slow and costly[55]. In addition, adding a new LLM to the crowdsourced battle platforms also poses a cold-start issue [15]. Thus, a fundamental question arises: _can we construct an unsupervised LLMs evaluation system without relying on any human feedback_?

Actually, in real human evaluation systems, people build their ability hierarchy based on different empirical assumptions. For example, majority voting [22, 10, 40] and rating voting [5] methods are widely used during the decision-making process, which are based on the wisdom of the crowds [40, 13, 50] and have been proven to lead to better results than that of an individual. Moreover, in the established practice of _peer-review_ in academic research, scholars evaluate their academic level rankings based on the _consistency assumption_, _i.e._, scholars with stronger abilities have stronger persuasiveness for evaluating others, and can also obtain higher achievements. This paper attempts to explore whether similar phenomena exist in the LLMs evaluation systems.

In this work, we propose **PiCO**, a **P**eer review approach in LLMs based on **C**onsistency **O**ptimization. In this setting, LLMs themselves act as "reviewers", engaging in mutual assessments to achieve comprehensive, efficient, and performance evaluations without relying on manually annotated data. This method aims to address the limitations of existing evaluation approaches and provide insights into LLMs' real-world capabilities. As shown in Figure 1, both open-source and closed-source LLMs lie in the same environment and answer the open-ended questions from an unlabeled dataset. Then, we construct anonymous answer pairs, while randomly selecting other LLMs as "reviewers" to evaluate both responses with a learnable confidence weight \(w\). Finally, we employ this weight and calculate the response scores \(G\) for each LLM based on the weighted joint evaluation. It is worth noting that the whole _peer-review_ process works in an unsupervised way, and our goal is to optimize the confidence weights that re-rank the LLMs to be closer to human rankings.

To achieve this, we formalize it as a constrained optimization based on the consistency assumption. We maximize the consistency of each LLM's capability \(w\) and score \(G\) while adjusting the final ranking to align with human preference more closely. The key assumption behind this is that high-level LLM can evaluate others' answers more accurately (confidence) than low-level ones, while higher-level LLM can also achieve higher answer-ranking scores. As a result, the entropy (controversy) of the whole _peer-review_ evaluation system can be minimized. In other words, the consistency optimization aims to find a final score ranking that all LLMs have no "disputes" regarding.

To evaluate the gap in aligning human rankings, we propose three metrics called PEN (**P**ermutation **E**ntropy), CIN (**C**ount **I**n**versions), LIS (**L**ongest **I**n**creasing **S**ubsequence). The experiments are conducted on multiple crowdsourcing datasets and validated on these three metrics. The experimental results demonstrate that the proposed PiCO framework can effectively obtain a large language models' leaderboard closer to human preferences.

Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM’s response score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to optimize the score ranking based on the _consistency assumption_, while reducing the entropy of the _peer-review_ evaluation system. The consistency optimization aims to find a final score ranking that all LLMs “agree” it.

The contributions of this paper can be summarized as follows.

* We explore a novel unsupervised LLM evaluation direction without human feedback, utilizing _peer-review_ mechanisms to measure LLMs automatically. All LLMs can answer unlabeled questions and evaluate each other.
* A constrained optimization based on the consistency assumption is proposed to re-rank the LLMs to be closer to human rankings.
* We propose three metrics called PEN, CIN, and LIS on the PiCO framework for evaluating the gap with human preferences.
* The experiments with these metrics on three crowdsourcing datasets validate the effectiveness of the proposed approach.

## 2 The Proposed Approach

In this section, we first describe the problem definition and preference alignment evaluation, and then introduce the proposed PiCO framework in detail.

### Definition and Metrics

**Problem Definition.** In this subsection, we aim to measure the ability level of LLMs automatically without relying on human annotations. Thus we consider an unsupervised LLM evaluation scenario with an unlabeled dataset \(\mathcal{Q}\) consisting of \(n\) open-ended questions, where \(\mathcal{Q}=\{Q_{i}\}_{i=1}^{n}\). In addition, we have a large language model pool \(\mathcal{M}=\{M_{j}\}_{j=1}^{m}\), which includes both open-source and closed-source models. Write \(M_{1}\succ M_{2}\) to indicate that the LLM \(M_{1}\) has stronger capabilities than the LLM \(M_{2}\). Thus, we can assume that the ground-truth ranking \(\mathcal{R}^{*}\) alignment with human preferences,

\[\mathcal{R}^{*}:=[M_{1}\succ M_{2}\succ M_{3}\succ...\succ M_{m}],\] (1)

and assume that the learned ranking \(\hat{\mathcal{R}}\) by different evaluation methods is as follows,

\[\hat{\mathcal{R}}:=[M_{3}\succ M_{1}\succ M_{2}\succ...\succ M_{m}].\] (2)

The goal is to build an LLM ranking \(\hat{\mathcal{R}}\) that aligns with human ranking \(\mathcal{R}^{*}\), making the loss \(\mathcal{L}\) of the both rankings tend towards \(0\), _i.e._, \(\mathcal{L}(\hat{\mathcal{R}},\mathcal{R}^{*})\to 0\)

**Preference Alignment Metrics.** Before building LLM rankings, we first need to discuss how to evaluate aligned human rankings. Intuitively, the metrics we want mainly describe the differences between two arrays composed of ranking indices. Assuming that human ranking \(\mathcal{R}^{*}\) is defined as being well-ranked in ascending order (\([1,2,3,...,m]\)) as shown in Eq 1. Thus the metric is to quantify the randomness of the learned ranking array (\([3,1,2,...,m]\)) as shown in Eq 2. Based on this, we propose three metrics called PEN, CIN, and LIS, respectively.

PEN (**P**ermutation **E**ntropy). Permutation entropy [8] is a concept used to quantify the complexity or randomness of time series data. It provides a measure of the irregularity or unpredictability of the order of values in a sequence. We thus utilize it to measure the gap with human rankings as follows,

\[\mathcal{L}_{PEN}(\hat{\mathcal{R}},\mathcal{R}^{*}):=-\sum p(\pi)\log p(\pi),\] (3)

where

\[p(\pi)=\frac{\#\{t|0\leq t\leq m-k,(M_{t+1},...,M_{t+k})\in\pi\}}{m-k+1}.\]

Figure 2: Preference alignment metric. Three metrics for evaluating the gap with human preferences called PEN, CIN, and LIS, respectively

\(\pi\) denotes different permutations, \(k\) is a hyper-parameter recommended to be set to 3 to 7, and we set \(k=3\) in this paper. Intuitively, it samples some subsequences and calculates the entropy for all permutation types. And the lower the permutation entropy in the learned LLM rankings, the closer it is to the ground-truth human rankings.

CIN (**C**ount **In**versions). Counting inversions [27] aims to measure the degree of disorder or "invertedness" in an array or sequence of elements. We thus define it as follows,

\[\mathcal{L}_{CIN}(\hat{\mathcal{R}},\mathcal{R}^{*}):=\sum_{M_{i},M_{j} \sim\mathcal{M}}\mathbf{1}\{M_{i}\succ M_{j}\wedge i<j\}.\] (4)

Where \(\mathbf{1}\{\cdot\}\) is the indicator function that the value is 1 when the condition is met, otherwise it is 0. Intuitively, the fewer inverse pairs in the learned LLM rankings, the closer it is to the ground-truth human rankings.

LIS (**L**ongest **I**ncreasing **S**ubsequence). The longest increasing subsequence aims to find the length of the longest subsequence in a given sequence of elements, where the subsequence is in increasing order. We utilize it to measure the degree of match with human rankings as follows,

\[\mathcal{L}_{LIS}(\hat{\mathcal{R}},\mathcal{R}^{*}):=\max\left\{dp[i]\mid 1 \leq i\leq m\right\},\] (5)

where

\[dp[i]=1+\max\left\{dp[j]\mid 1\leq j<i\wedge M_{j}\prec M_{i}\right\}.\]

\(dp[i]\) represents the length of the longest increasing subsequence that ends with \(M_{i}\). LIS allows for a nuanced understanding of the degree to which the learned ranking aligns with the ideal human ranking, with a higher LIS length indicating greater alignment.

### Algorithm Details

The PiCO framework, depicted in Figure 3, involves peer-review and consistency optimization stages. In the peer-review stage, we first collect an unlabeled dataset \(\mathcal{Q}\) consisting of open-ended questions, and construct a large language model pool \(\mathcal{M}\) that includes both open-source and closed-source LLMs. Then, we let all LLMs answer each unlabeled question to obtain the response set \(\mathcal{A}\). We shuffle the set and construct anonymous answer pairs, while randomly selecting other LLMs as "reviewers" to evaluate both responses with a learnable confidence \(w\). Finally, we can obtain the answer-ranking data \(\mathcal{D}\) and calculate the response score \(G\) for each large language model. In the consistency optimization phase, we maximize the consistency of each LLM's capability \(w\) and score \(G\) with constrained optimization, while re-ranking the LLMs to be closer to human rankings.

#### 2.2.1 Peer Review Stage

**Data Collection and LLMs Pool Construction.** Benefiting from the creation of crowdsourced battle platforms, we accessed open assessment datasets from Chatbot Arena[55], MT-Bench[55],

Figure 3: The pipeline of the PiCO. It is mainly composed of two components: the peer-review and consistency optimization stages. Specifically, in the peer-review stage, the unlabeled dataset \(\mathcal{Q}\) and the LLMs pool \(\mathcal{M}\) are given. Then, we let all LLMs answer each unlabeled question to obtain the response set \(\mathcal{A}\). We shuffle the set and construct anonymous answer pairs, while randomly selecting other LLMs to evaluate both responses with a learnable confidence \(w\). As a result, we can obtain the answer-ranking data \(\mathcal{D}\) which is a quadruple that records the partial order between two answers and the evaluator’s confidence weight. In the consistency optimization stage, we update the parameter \(w\) by maximizing the consistency of each LLM’s capability and score, while re-ranking the LLMs to be closer to human rankings.

and AlpacaEval[29]. These open datasets include critical fields such as "question_id" and "question_content." Utilizing the Chatbot Arena dataset, which features pairwise data from twenty LLMs with human preference annotations, we assembled an LLM pool \(\mathcal{M}=\{M_{j}\}_{j=1}^{m}\). Leveraging 33K human-annotated interactions from this dataset, we established a ground-truth ranking \(\mathcal{R}^{*}\) and gathered responses \(\mathcal{A}=\{\{A_{i}^{j}\}_{i=1}^{n}\}_{j=1}^{m}\) for our dataset \(\mathcal{Q}=\{Q_{i}\}_{i=1}^{n}\).

**Answer-Ranking Data Construction Based on Peer Review.** After obtaining the responses set \(\mathcal{A}\), we aim to generate answer-ranking data \(\mathcal{D}\) through the peer-review mechanism. Specifically, for the same question \(Q_{i}\in\mathcal{Q}\), we randomly construct a battle pair \(<A_{i}^{j},A_{i}^{k}>\) for review. Each battle pair will be randomly assigned five models ("reviewers") to determine the winners or declare ties. Note that the model may evaluate its own answers, but the entire process is anonymous. As a result, we can obtain the quadruples \((A_{i}^{j},A_{i}^{k},>w^{s})\), indicating the "reviewer" \(M_{s}\) believes that the answer \(A_{i}^{j}\) is better than answer \(A_{i}^{k}\) with a confidence \(w^{s}\). Therefore, the answer-ranking data \(\mathcal{D}\) can be defined as follows,

\[\mathcal{D}=\left\{(A_{i}^{j},A_{i}^{k},>,w^{s})\right\}_{i\sim\mathcal{Q},j,k,s\sim\mathcal{M}},\] (6)

where \(i\) denotes the question index, and \(j,k,s\) indicate the model indices. \(w^{s}\) is a learnable confidence of model \(M_{s}\), and \(>\) is a partial order relationship from \(\{>,<,=\}\).

#### 2.2.2 Consistency Optimization Stage

As shown in Eq 6, following the peer-review mechanism, we construct anonymous answer pairs and randomly select other LLMs as "reviewers" to evaluate both responses with a learnable confidence \(w\). Next, we expect to optimize the confidence \(w\) and re-rank the LLMs to be closer to human rankings. We thus propose the consistency assumption, _i.e._, high-level LLM can evaluate others' answers more accurately (confidence) than low-level ones, while higher-level LLM can also achieve higher answer-ranking scores. Formally, we maximize the consistency of each LLM's capability \(w\) and score \(G\) with constrained optimization as follows,

\[\operatorname*{argmax\,\,Consistency}(G,w)\] (7) \[\text{s.t.}\ G_{j}=\sum_{(A_{i}^{j},A_{i}^{k},>,w^{s})\sim \mathcal{D}}\mathbf{1}\{A_{i}^{j}>A_{i}^{k}\}*w^{s},\]

where \(\mathbf{1}\{\cdot\}\) is the indicator function that the value is 1 when the condition is met, otherwise, it is 0. \(G_{j}\) denotes the response score of model \(M_{j}\), which is calculated by joint evaluation of other models. Moreover, we employ Pearson correlation [38] to measure the consistency between \(w\) and \(G\). Note that we only introduce this straightforward implementation to validate our idea of PiCO. Other more advanced strategies may be employed to further improve the performance.

**Discussion:** It is worth noting that the whole process (Eq. 6 and 7) works in an unsupervised way. The only thing we do is to adaptively assign each LLM a score that matches its abilities. An intuitive example is as follows: in a real peer-review system, if the academic level of three scholars \(a\), \(b\), and \(c\) satisfies the following relationship, \(w^{a}>w^{b}>w^{c}\). So, in the ultimate ideal scenario, the ranking of the scores submitted by these three scholars should also be, \(G_{a}>G_{b}>G_{c}\). In other words, the sorting of \(G\) and \(w\) satisfies high consistency. On the other hand, scholars with stronger abilities (_i.e._, scholar \(a\)) evaluate \(A^{b}>A^{c}\) have stronger persuasiveness, so scholar \(b\) should also receive higher weighted scores \(1*w^{a}\).

**Reviewer Elimination Mechanism.** Realizing that not all LLMs have sufficient ability to evaluate the responses of other models. We thus introduce an unsupervised elimination mechanism to remove those LLMs that have low scores. It iteratively removes the lowest-scoring LLM from the "reviewer queue" for the next consistency optimization stage, until 60% of models are eliminated. The whole process of the approach is summarized in Algorithm 1, and the details can be found in Appendix D.

## 3 Experiments

**Datasets.** To validate the effectiveness of the proposed approach, we perform experiments on Chatbot Arena[55], MT-Bench[55], and AlpacaEval[29]. The MT-Bench dataset assesses six LLMs' responses to 80 multi-category questions. The Chatbot Arena Conversations Dataset, with 33K conversations from 13K IPs during April-June 2023, evaluates real dialogue performance. AlpacaEval dataset

[MISSING_PAGE_FAIL:6]

### Performance Comparison

We validate the effectiveness of the proposed PiCO method on three datasets by comparing the following two types of methods, _i.e._, the wisdom of the crowds and recent SOTA LLMs evaluation methods. The average results of PEN, CIN and LIS are demonstrated in Table 1. The ratios of response sets \(\mathcal{D}\) are 1, 0.7, and 0.4, respectively.

The results presented in Table 1 illustrate the proposed PiCO method consistently surpasses competing approaches across the majority of evaluated metrics Notably, PiCO achieves performance improvements of 0.1, 2.5, and 0.92 on the PEN, CIN, and LIS metrics, respectively, compared to the Runner-up. These results underscore the superiority of aggregating evaluations from multiple models, such as Majority Voting, Rating Voting, PRD, and PRE, as opposed to relying solely on single-model methods like GPTScore and PandaLM. This collective model approach, leveraging 'the wisdom of the crowds', more accurately aligns with human rankings in our open-question evaluation framework.

In comparison with existing peer review evaluation methods(_i.e.,_ PRD and PRE), it is evident that PiCO exhibits improvements across various evaluation metrics. Despite PRD's adjustment of model weights based on their win rates and PRE's reliance on supervised human feedback data to assign weights through a qualification exam, neither method achieves performance superior to the fully unsupervised PiCO approach. These methods rely on predefined criteria and human feedback, potentially leading to biases or suboptimal performance. In contrast, PiCO leverages unsupervised learning techniques, allowing it to autonomously adapt and discover patterns in the data without explicit human intervention.

It is important to highlight that PandaLM, a language model equipped with 7 billion parameters, was fine-tuned using labels generated by GPT-3.5-turbo as the ground truth, achieving stable performance across various datasets. However, in our unsupervised, open-ended experimental setup, which focuses on ranking-based metrics, GPTScore exhibits less robustness regardless of whether the base model is GPT-3 (davinci-002) or fran-t5-xx.

### Exploring the Role of Confidence Weight

In this subsection, we will show that the confidence weight \(w\) learned by our _consistency optimization_ can reduce the system evaluation bias. Specifically, we first study whether the "review" model would

Figure 4: Heatmap distribution of preference gap (PG) metric among seven LLMs across three datasets. Higher values (above 0) indicate greater evaluation bias[17]. The first row shows original PG values in three datasets, while the second row displays PG values re-weighted using our learned confidence weights.

prefer a particular model's response. Following [17], we employ the preference gap (PG) to evaluate the bias as follows,

\[PG(i,j)=P_{i}(i>j)-P_{j}(i>j),\] (8)

where \(P_{i}(i>j)\) represents the winning rate of model \(i\) as the "reviewer" believes that \(i\) defeated \(j\). The heatmap distribution of the PG value \(PG(i,j)\) among seven LLMs across three datasets is demonstrated in the first row of Figure 4. It can be observed that the evaluation system exhibits severe bias. Especially on ChatGLM-6B and Mpt-7B models, they often believe that their results are better than other ones, as their PG values are greater than 0 across three datasets.

After the _consistency optimization_, we assign the learned confidence weight \(w\) to the corresponding model and ultimately obtain the re-weighting PG value \(\hat{PG}(i,j)\) as follows,

\[\hat{PG}(i,j)=w_{i}\times P_{i}(i>j)-w_{j}\times P_{j}(i>j).\] (9)

The results of the re-weighting PG value \(\hat{PG}(i,j)\) are displayed on the second row of Figure 4. It can be observed that the learned confidence weight \(w\) can significantly mitigate the preference gaps of the whole evaluation system. In our consistency optimization, LLMs such as ChatGLM-6B and Mpt-7B have lower weights, and reducing their confidence can effectively alleviate the system evaluation bias.

### Study of Elimination Mechanism

The PiCO and PRE[17] methods both employ elimination mechanisms to remove those weakest LLMs from the "reviewer queue" during the evaluation process. As shown in Figure 5, the x-axis quantifies the number of reviewers eliminated, and the y-axis measures the CIN, where lower scores denote higher performance. Due to space limitations, more results on PEN and LIS metrics can be found in Appendix E. It can be observed that both PiCO and PRE exhibit better performance with an increasing number of eliminated "reviewers". The proposed PiCO approach can achieve better performance than PRE in most cases. It is worth noting that the PRE method employs the accuracy of "qualification exams" to eliminate weak LLMs, and this process requires human annotation [17]. On the contrary, the elimination process of our PiCO method is unsupervised and can still achieve better evaluation results than PRE.

### Validation of Consistency Assumption

In this subsection, we conduct the ablation study to validate the effectiveness of the _consistency assumption_. Specifically, we first manually construct three methods: Forward Weight Voting, Uniform Weight Voting, and Reverse Weight Voting. That is, the ability weights of the model are respectively weighted forward (\(w=[1,0.9,...,0]\)), uniformly (\(w=[1,1,...,1]\)), and backward (\(w=[0,0.1,...,1]\)) according to the ground-truth human ranking. Then, we randomly initialize the ability weights and employ our _consistency optimization_ to adjust the weight. In addition, we also collect the average performance of "reviewer queue", _i.e._, employing a single LLM as the "reviewer" to evaluate all response pairs and then calculate the average results of all LLMs.

As shown in Table 2, it can be observed that the Forward Weight Voting achieves better results than the Uniform and Backward ones in all cases, while the Backward one achieves worse results. It validates that assigning larger weights to those models with stronger capabilities can obtain better

Figure 5: Performance comparison of the PiCO (Ours) and PRE[17] methods on the Chatbot Arena, MT-Bench, and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is CIN, where lower values indicate better performance.

results. Most importantly, employing our consistency optimization algorithm to assign weights to different review models can further improve the performance of the evaluation system, _i.e._, lower PEN and CIN, as well as higher LIS in all cases. Moreover, it is worth noting that the average performance of the "reviewer queue" is very poor, even worse than the Backward Weight Voting. This means that the answer-ranking data \(\mathcal{D}\) contains a lot of evaluation noise, while the proposed approach can still optimize weights and obtain better ranking results. In summary, the above experimental results validate the effectiveness of the consistency assumption from various perspectives.

## 4 Related Work

**Evaluation Benchmarks for Diversity.** LLMs are designed to handle a variety of tasks, necessitating comprehensive benchmarks[15]. Notable benchmarks include GLUE[45] and SuperGLUE[44], which simulate real-world scenarios across tasks such as text classification, translation, reading comprehension, and dialogue generation. HELM[30] provides a holistic evaluation of LLMs, assessing language understanding, generation, coherence, and reasoning. BIG-bench[39] pushes LLM capabilities with 204 diverse tasks. MMLU[26] measures multitask accuracy across domains like mathematics and law. However, these evaluations can be compromised by benchmark leakage, where evaluation data inadvertently used for training leads to inflated performance metrics[4; 56].

**Human Evaluation.** Human evaluation provides reliable feedback that closely aligns with real-world applications[15]. Liang et al.[30] evaluated summary and misinformation scenarios across multiple models. Ziems et al.[57] involved experts to assess model outputs in various domain-specific tasks. Bang et al.[9] examined ChatGPT's performance in summarization, translation, and reasoning using human-annotated datasets. The LMSYS initiative introduced platforms like Chatbot Arena[55], relying on human ratings as the primary evaluation metric. Despite its effectiveness, human evaluation is costly and subject to bias and cultural differences[37].

**Large Language Models for Evaluation.** The development of open-source LLMs has led to the use of LLMs as evaluators. GPTScore[23] uses models like GPT-3 to assign probabilities to high-quality content through multidimensional evaluation. Bubeck et al.[12] tested GPT-4, finding it rivaling human capabilities. Lin and Chen introduced LLM-EVAL[31] for evaluating dialogue quality with single prompts. PandaLM[46] employs LLMs as "judges" for evaluating instruction tuning. However, reliance on a single model can introduce biases such as positional[20], verbosity[47], and self-favoring biases[33; 55]. ChatEval[14] proposes a multi-agent framework to simulate human evaluation processes. Similarly, PRE[17] and PRD[28] use LLMs as evaluators, combining multiple evaluation outcomes for automated assessment. However, the PRE method, which relies on human feedback for supervised evaluation throughout the process, still incurs relatively high costs.

## 5 Conclusion

In this paper, we propose the novel Peer Review method based on the Consistency Optimization (PiCO) to automatically evaluate Large Language Models (LLMs) without relying on human feedback. PiCO utilizes _peer-review_ mechanisms to autonomously assess LLMs in a shared environment, where both open-source and closed-source models can respond to unlabeled questions and evaluate each other. In this setup, each LLM's response score is determined collectively by other anonymous models, aiming to maximize consistency across capabilities and scores. We propose three metrics, _i.e.,_ PEN, CIN, and LIS, to quantify the disparity from human preferences. The extensive experiment results across multiple datasets and metrics demonstrate that PiCO effectively generates an LLM leaderboard that aligns closely with human preferences. In the future, we plan to extend the peer-review mechanism to evaluate the capabilities of multi-modality large models.

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{2}{c|}{MT-Bench} & \multicolumn{2}{c}{Chudot Area} & \multicolumn{2}{c}{AlpacaEval} \\  & PEN (\(\downarrow\)) & CIN(\(\downarrow\)) & PEN (\(\downarrow\)) & CIN(\(\downarrow\)) & PEN (\(\downarrow\)) & CIN(\(\downarrow\)) \\ \hline Average Performance of Reviewer Queue & \(1.49^{\pm 0.28}\) & \(34.87^{\pm 1.46}\) & \(1.49^{\pm 0.26}\) & \(38.80^{\pm 1.28}\) & \(1.50^{\pm 0.23}\) & \(33.13^{\pm 1.37}\) \\ \hline Backward Weight Voting & \(1.43^{\pm 0.04}\) & \(25.00^{\pm 0.00}\) & \(1.43^{\pm 0.05}\) & \(26.00^{\pm 0.00}\) & \(1.36^{\pm 0.03}\) & \(24.00^{\pm 0.00}\) \\ Uniform Weight Voting & \(1.34^{\pm 0.23}\) & \(22.00^{\pm 0.00}\) & \(1.39^{\pm 0.02}\) & \(24.00^{\pm 0.00}\) & \(1.34^{\pm 0.03}\) & \(22.00^{\pm 0.00}\) \\ Forward Weight Voting & \(1.32^{\pm 0.03}\) & \(21.00^{\pm 0.00}\) & \(1.33^{\pm 0.03}\) & \(23.00^{\pm 0.00}\) & \(1.30^{\pm 0.05}\) & \(21.00^{\pm 0.00}\) \\ \hline Random Weight + Consistency Optimization & \(\mathbf{1.17^{\pm 0.06}}\) & \(\mathbf{1.50^{\pm 0.30}}\) & \(\mathbf{1.20^{\pm 0.08}}\) & \(\mathbf{1.800^{\pm 1.22}}\) & \(\mathbf{1.21^{\pm 0.04}}\) & \(\mathbf{19.00^{\pm 0.00}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Ablation study comparing Backward, Uniform, Forward weight voting, and Consistency Optimization methods with the Average Performance of Reviewer Queue across three datasets.

## References

* generative universal assistant for natural-language adaptive context-aware omnilingual outputs. https://guanaco-model.github.io/, 2023. Accessed: 15 April 2024.
* [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [3] Stability AI. Stablelm-tuned-alpha-7b: A fine-tuned language model for diverse applications. https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b, 2023. Accessed: 15 April 2024.
* [4] Rachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-Yeol Ahn. Can we trust the evaluation on chatgpt?, 2023.
* [5] Mohammad Allahbakhsh and Aleksandar Ignjatovic. Rating through voting: An iterative method for robust rating. _arXiv preprint arXiv:1211.0390_, 2012.
* [6] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https://github.com/nomic-ai/gpt4all, 2023.
* [7] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.
* [8] Christoph Bandt and Bernd Pompe. Permutation entropy: a natural complexity measure for time series. _Physical review letters_, 88(17):174102, 2002.
* [9] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wiliie, Holy Loveenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. _arXiv preprint arXiv:2302.04023_, 2023.
* [10] Robert S Boyer and J Strother Moore. Mirty--a fast majority vote algorithm. In _Automated reasoning: essays in honor of Woody Bledsoe_, pages 105-117. Springer, 1991.
* [11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [12] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* [13] David V Budescu and Eva Chen. Identifying expertise to extract the wisdom of crowds. _Management science_, 61(2):267-280, 2015.
* [14] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. _arXiv preprint arXiv:2308.07201_, 2023.
* [15] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. _ACM Transactions on Intelligent Systems and Technology_, 2023.
* [16] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality. https://vicuna.lmsys.org, 2023. Accessed: 15 April 2024.

* Chu et al. [2024] Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and Yiqun Liu. Pre: A peer review based large language model evaluator. _arXiv preprint arXiv:2401.15641_, 2024.
* Conover et al. [2023] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world's first truly open instruction-tuned llm, 2023.
* [19] Open-Assistant Contributors. Onast-sft-4-pythia-12b: A supervised fine-tuning model for language understanding. https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5, 2023. Accessed: 15 April 2024.
* Dettmers et al. [2024] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. _Advances in Neural Information Processing Systems_, 36, 2024.
* Dubois et al. [2023] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. _arXiv preprint arXiv:2305.14387_, 2023.
* Feldman [2006] Allan M. Feldman. Majority voting. _SpringerLink_, 2006.
* Fu et al. [2023] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. _arXiv preprint arXiv:2302.04166_, 2023.
* Geng et al. [2023] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala-13b: Dialogue model for effective human-ai interaction. https://bair.berkeley.edu/blog/2023/04/03/koala/, 2023. Accessed: 15 April 2024.
* Geng et al. [2023] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. _Blog post, April_, 1, 2023.
* Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.
* Leiserson et al. [1994] Charles Eric Leiserson, Ronald L Rivest, Thomas H Cormen, and Clifford Stein. _Introduction to algorithms_, volume 3. MIT press Cambridge, MA, USA, 1994.
* Li et al. [2023] Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and discussion improve large language model based evaluations. _arXiv preprint arXiv:2307.02762_, 2023.
* Li et al. [2023] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models, 2023.
* Liang et al. [2022] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. _arXiv preprint arXiv:2211.09110_, 2022.
* Lin and Chen [2023] Yen-Ting Lin and Yun-Nung Chen. Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. _arXiv preprint arXiv:2305.13711_, 2023.
* Liu et al. [2023] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35, 2023.
* Liu et al. [2023] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment. _arXiv preprint arXiv:2303.16634_, 2023.
* Nakano et al. [2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. _arXiv preprint arXiv:2112.09332_, 2021.

* [35] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022. Accessed: [insert date here].
* [36] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* [37] Kaiping Peng, Richard E Nisbett, and Nancy YC Wong. Validity problems comparing values across cultures and possible solutions. _Psychological methods_, 2(4):329, 1997.
* [38] Philip Sedgwick. Pearson's correlation coefficient. _Bmj_, 345, 2012.
* [39] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_, 2022.
* [40] James Surowiecki. _The wisdom of crowds_. Anchor, 2005.
* [41] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.
* [42] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable lms, 2023. Accessed: 2023-05-05.
* [43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [44] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. _Advances in neural information processing systems_, 32, 2019.
* [45] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_, 2018.
* [46] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. _arXiv preprint arXiv:2306.05087_, 2023.
* [47] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. _Advances in Neural Information Processing Systems_, 36, 2024.
* [48] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_, 2022.
* [49] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu, Rui Hu, et al. Skywork: A more open bilingual foundation model. _arXiv preprint arXiv:2310.19341_, 2023.
* [50] Susan C Weller. Cultural consensus theory: Applications and frequently asked questions. _Field methods_, 19(4):339-368, 2007.
* [51] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions, 2023.
* [52] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. Llm lies: Hallucinations are not bugs, but features as adversarial examples. _arXiv preprint arXiv:2310.01469_, 2023.

* [53] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. _arXiv preprint arXiv:2210.02414_, 2022.
* [54] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _arXiv preprint arXiv:2303.18223_, 2023.
* [55] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
* [56] Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. Don't make your llm an evaluation benchmark cheater. _arXiv preprint arXiv:2311.01964_, 2023.
* [57] Caleb Ziems, William Held, Omar Shaikh, Jiao Chen, Zhehao Zhang, and Diyi Yang. Can large language models transform computational social science? _arXiv preprint arXiv:2305.03514_, 2023.

## Appendix A Dataset Format

Focusing on the MT-Bench dataset, we demonstrate the ensuing data format utilizing dataset \(\mathcal{Q}\). As Figure 6 illustrates, the Question dataset \(\mathcal{Q}\) contains "Question id," "Category," "Question," and "Reference." In categories with definitive answers like "reasoning" or "math," the "Reference" field is populated with standard answers; otherwise, it remains blank. Each model M in our pool processes the Question dataset \(\mathcal{Q}\) to generate the LLMs answer data \(\mathcal{A}\), consisting of "Question id," "Answer id," "Model id," and "Answer." Finally, we combine pairs in \(\mathcal{A}\) and appoint judges to evaluate, creating the Answer-Ranking data \(\mathcal{D}\), featuring "Question id," "Model 1," "Model 2," "G1 winner," "G2 winner," and "Judge." Here, "G1 winner and "G2 winner" indicate the outcomes of inputting reversed order responses of Model 1 and Model 2 into the judge model, a method employed to mitigate biases stemming from models' preferences for input order.

## Appendix B Detailed Prompt for Reviewers

The evaluation prompts, as detailed in Section 2.2.1, are employed during the Peer Review Stage. These prompts are provided to the Reviewer Language Model Systems (LLMs), enabling them to generate evaluative preferences. In our experimental framework, we devised four distinct prompt settings. For each setting, a tailored prompt template was meticulously crafted as illustrated below:

**Template for Single-Turn Interaction:** This template is designed for single-turn interactions between users and LLMs, where there is no predetermined correct answer. It facilitates open-ended dialogue, allowing for a wide range of user inquiries without the expectation of specific responses.

**Referenced Template for Single-Turn Interaction:** Tailored for single-turn dialogues between users and LLMs, this template incorporates predefined correct answers. It is particularly suited for

Figure 6: Format of the Question dataset \(\mathcal{Q}\), LLMs responses data \(\mathcal{A}\), and the Answer-Ranking data \(\mathcal{D}\) for Peer Review

interactions involving factual inquiries, such as mathematics or logic problems, where accuracy and reference to correct information are paramount.

**Template for Multi-Turn Interaction:** This template caters to multi-turn conversations between users and LLMs, without predefined answers. It supports extended interactions, enabling users to explore topics in depth through a series of interconnected questions and responses.

**Referenced Template for Multi-Turn Interaction:** Designed for multi-turn dialogues with predefined correct answers, this template is ideal for complex inquiries requiring sequential reasoning or problem-solving, such as mathematical computations or logical deductions.

Each template is carefully constructed to match its intended use-case, providing a structured framework that guides the interaction between users and LLMs towards achieving desired outcomes, whether for open-ended exploration or precise problem-solving.

**Template for Single-Turn Answer**

**System prompt:** Please act as a judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You do not need to explain, just give your judgment. Output your final verdict by strictly following this format: "[[A]]" if assistant A is better, "[[B]]" if assistant B is better, and "[[C]]" for a tie.

**User Question:** {question}

**Assistant A's Answer:** {answer a}

**Assistant B's Answer:** {answer b}

**Referenced Template for Single-Turn Answer**

**System prompt:** Please act as a judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below, with reference to the provided reference answers. You do not need to explain, just give your judgment. Output your final verdict by strictly following this format: "[[A]]"if assistant A is better, "[[B]]" if assistant B is better, and "[[C]]" for a tie.

**User Question:** {question}

**Reference Answer:** {reference answer}

**Assistant A's Answer:** {answer a}

**Assistant B's Answer:** {answer b}

**Template for Multi-Turn Answer**

**System prompt:** Please act as a judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You do not need to explain, just give your judgment. Output your final verdict by strictly following this format: "[[A]]" if assistant A is better, "[[B]]" if assistant B is better, and "[[C]]" for a tie.

**Assistant A:** {answer a1}

**User:** {question 2}

**Assistant A:** {answer a2}

**Assistant B's Conversation with User:**

**User:** {question 1}

**Assistant B:** {answer b1}

**User:** {question 2}

**Assistant B:** {answer b2}

**Referenced Template for Multi-Turn Answer**

**System prompt:** Please act as a judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below, in comparison to the reference answers. You do not need to explain, just give your judgment. Output your final verdict by strictly following this format: "[[A]]"if assistant A is better, "[[B]]" if assistant B is better, and "[[C]]" for a tie.

**Reference Answer**

**User:** {question 1}

**Reference answer:** {ref answer 1}

**User:** {question 2}

**Reference answer:** {ref answer 2}

**Assistant A's Conversation with User:**

**User:** {question 1}

**Assistant A:** {answer a1}

**User:** {question 2}

**Assistant A:** {answer a2}

**Assistant B's Conversation with User:**

**User:** {question 1}

**Assistant B:** {answer b1}

**User:** {question 2}

## Appendix C Scoring Methodology

In Section 2.2.2, Equation 7 delineates the methodology for optimizing scores. Within this framework, the function \(\mathbf{1}\{A_{i}^{j}>A_{i}^{k}\}\) is more precisely defined as \(f(A_{i}^{j},A_{i}^{k})\). Additionally, the function \(f(A_{i}^{j},A_{i}^{k})\) is not fixed and can be implemented using various computational strategies. We introduce two distinct methodologies in this context: the Elo mechanism and the Rank mechanism.

Within the framework of the Elo mechanism, as specified by Equation 10, the \(BASE\) value is set to 10, and the \(SCALE\) factor is determined to be 400. This approach facilitates a dynamic adjustment of scores based on the outcomes of pairwise comparisons, allowing for a nuanced reflection of performance variations among models.

Conversely, in the context of the Rank mechanism, as outlined by Equation 11, \(rank(j)\) signifies the current ranking of model \(j\), with the constant \(K\) assigned a value of 200. This mechanism employs a model's ranking within a predefined hierarchy as a pivotal factor in score calculation, thereby providing a straightforward, yet effective, method for evaluating comparative model performance.

\[f(A_{i}^{j},A_{i}^{k})=\begin{cases}1-\frac{1}{1+\text{BASE}^{((G(k)-G(j))/ \text{SCALE})}}&\text{if }A_{i}^{j}>A_{i}^{k}\\ 0.5-\frac{1}{1+\text{BASE}^{((G(k)-G(j))/\text{SCALE})}}&\text{if }A_{i}^{j}=A_{i}^{k}\\ 0-\frac{1}{1+\text{BASE}^{((G(k)-G(j))/\text{SCALE})}}&\text{if }A_{i}^{j}<A_{i}^{k} \end{cases}\] (10)

\[f(A_{i}^{j},A_{i}^{k})=\begin{cases}1+(rank(j)-rank(k))/K&\text{if }A_{i}^{j}>A_{i}^{k}\\ 0.5&\text{if }A_{i}^{j}=A_{i}^{k}\\ 0&\text{if }A_{i}^{j}<A_{i}^{k}\end{cases}\] (11)

## Appendix D Overall Algorithm of Peer Review

The overall algorithm, as delineated in Algorithm 1, encapsulates the comprehensive process outlined in Section 2.2. This sequence commences with "Data Collection and LLMs Pool Construction," progresses through "Answer-Ranking Data Construction Based on Peer Review," advances to "Consistency Optimization," and culminates with the "Unsupervised Elimination Mechanism."
## Appendix E Complete Experimental Results

In Section 3.4, we both employ elimination mechanisms to cull the weakest LLMs from the'reviewer queue' during the evaluation process. In Figures 7 and 8, we present the results for the PEN and LIS metrics, where lower PEN scores indicate better performance, and higher LIS scores denote superior performance. It is evident that both the 'PiCO' and PRE approaches demonstrate enhanced performance as the number of eliminated'reviewers' increases. In most cases, the proposed 'PiCO' method outperforms PRE.

In Section 3.5, we validate the effectiveness of the _consistency assumption_ and compare it with the Average Performance of the Reviewer Queue, i.e., employing a single LLM as the'reviewer' to evaluate all response pairs and then calculating the average results of all LLMs. The comprehensive results compared with the Reviewer Queue are illustrated in Table3, Figure 9, 10 and 11, revealing that in the full Reviewer Queue, the performance of the vast majority of LLMs is very poor, indicating that the evaluations from most LLMs are noise. However, our 'PiCO' approach nearly matches the evaluative prowess of the pool's most capable LLM, GPT-3.5. Remarkably, given its unsupervised nature, the 'PiCO' method demonstrates the capability to mitigate the influence of noise, reaching the evaluation upper bound (the strongest LLM) within any given unknown LLM pool \(M\), even in the absence of prior ranking information.

\begin{table}
\begin{tabular}{c|c c c|c c c|c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{3}{c}{MT-Bench} & \multicolumn{3}{c}{Chatbot} & \multicolumn{3}{c}{AlpacaEval} \\  & \multicolumn{3}{c}{PEN (\(\downarrow\))} & CIN(\(\downarrow\)) & LIS(\(\uparrow\)) & PEN (\(\downarrow\)) & CIN(\(\downarrow\)) & LIS(\(\uparrow\)) & PEN (\(\downarrow\)) & CIN(\(\downarrow\)) & LIS(\(\uparrow\)) \\ \hline Gpt-3.5 & **0.97** & **12.00** & **10.00** & **0.85** & **11.00** & **11.00** & **1.15** & **16.00** & **9.00** \\ Guanaco-33B & 1.25 & 21.00 & 8.00 & 1.50 & 28.00 & 7.00 & 1.26 & 20.00 & 9.00 \\ Vicuna-13B & 1.31 & 20.00 & 7.00 & 1.27 & 23.00 & 8.00 & 1.20 & 17.00 & 8.00 \\ WizardLM-13B & 1.15 & 17.00 & 9.00 & 1.27 & 19.00 & 8.00 & 1.17 & 17.00 & 9.00 \\ Vicuna-7B & 1.27 & 21.00 & 8.00 & 1.30 & 20.00 & 7.00 & 1.34 & 23.00 & 8.00 \\ Koola-13B & 1.67 & 43.00 & 6.00 & 1.34 & 23.00 & 8.00 & 1.54 & 31.00 & 7.00 \\ gptall-13B & 1.74 & 45.00 & 6.00 & 1.60 & 35.00 & 6.00 & 1.73 & 42.00 & 6.00 \\ Mpt-7B & 1.67 & 39.00 & 6.00 & 1.72 & 52.00 & 6.00 & 1.63 & 34.00 & 7.00 \\ Oass-pythia-12B & 1.77 & 50.00 & 5.00 & 1.74 & 42.00 & 5.00 & 1.70 & 47.00 & 6.00 \\ Alpaca-13B & 1.77 & 49.00 & 7.00 & 1.60 & 73.00 & 4.00 & 1.63 & 34.00 & 7.00 \\ FastChat-T5-3B & 1.45 & 29.00 & 7.00 & 1.53 & 30.00 & 7.00 & 1.30 & 22.00 & 7.00 \\ ChatGLM-6B & 1.59 & 33.00 & 7.00 & 1.71 & 55.00 & 5.00 & 1.63 & 34.00 & 6.00 \\ StableLM-7B & 1.68 & 63.00 & 5.00 & 1.75 & 44.00 & 5.00 & 1.72 & 56.00 & 4.00 \\ Dolly-12B & 1.76 & 46.00 & 6.00 & 1.57 & 41.00 & 6.00 & 1.75 & 54.00 & 6.00 \\ LLAMA-13B & 1.60 & 35.00 & 7.00 & 1.76 & 56.00 & 6.00 & 1.70 & 50.00 & 5.00 \\ \hline Average Performance of All Review LLMs & 1.51 & 34.87 & 6.93 & 1.50 & 38.80 & 6.60 & 1.50 & 33.13 & 6.93 \\ \hline PRD[28] & 1.15 & 17.00 & 8.00 & 1.15 & 17.00 & 8.00 & 1.21 & 19.00 & 9.00 \\ PRE[17] & 1.17 & 17.00 & 8.00 & 1.07 & 15.00 & 9.00 & 1.18 & 19.00 & 8.00 \\ PiCO (Ours) & 1.01 & 14.50 & 8.75 & 0.94 & 12.00 & 10.00 & 1.17 & 17.00 & 9.00 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of performance across three datasets using Unsupervised methods versus using single models in reviewer queue.

Figure 8: Performance comparison of the PiCO (Ours) and PRE[17] methods on the MT-Bench, Chatbot Arena, and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is LIS, where upper values indicate better performance.

Figure 7: Performance comparison of the PiCO (Ours) and PRE[17] methods on the MT-Bench, Chatbot Arena, and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is PEN, where lower values indicate better performance.

## Appendix F Selected Models and Optimized Ranking

For our analysis, we meticulously selected 15 LLMs spanning a variety of architectures, encompassing both open-source and closed-source models, as detailed in the subsequent table. Our curated selection features prominent LLMs including the closed-source "gpt-3.5-turbo," "chatglm" which is predicated on the encoder-decoder framework, "fastchat-t5-3b" that leverages Google's T5 (Text-to-Text Transfer Transformer) architecture, and "llama-13b" founded on the GPT architectural principles.

We have comprehensively detailed the ranking outcomes across three distinct datasets for our comparative analysis, incorporating the optimized model rankings, names, and their respective scores.

Figure 10: Comparison of performance on the PEN metric across three datasets using Unsupervised methods versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The dotted line represents the average value using single models.

Figure 9: Comparison of performance on the CIN metric across three datasets using Unsupervised methods versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The dotted line represents the average value using single models.

As delineated in Appendix C, the PiCO (Ours) is capable of employing various scoring mechanisms, thereby facilitating the presentation of ranking outcomes on three datasets utilizing both the Elo and Rank mechanisms. Furthermore, we have also enumerated the ranking results for PRD and PRE methodologies across the three datasets, offering a holistic view of the competitive landscape.

Figure 11: Comparison of performance on the LIS metric across three datasets using Unsupervised methods versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The dotted line represents the average value using single models.

### PiCO

\begin{tabular}{|c|} \hline \multicolumn{2}{|c|}{**Grade-Elo-Altbot**} \\ \hline \#1 Gpt-3.5 I Grade: 9205.162109375 \\ \#2 WizardLM-13B I Grade: 9143.46875 \\ \#3 Guanaco-33B I Grade: 5886.92626953125 \\ \#4 Vicuna-7B I Grade: 5368.9462890625 \\ \#5 Vicuna-13B I Grade: 5216.79541015625 \\ \#6 Koala-13B I Grade: 3545.1171875 I Eliminated \\ \#7 Mpt-7B I Grade: 962.99462890625 I Eliminated \\ \#8 Gpt4all-13B I Grade: 652.4602661132812 Eliminated \\ \#9 Chatglm-6B I Grade: 417.1375427246094 I Eliminated \\ \#10 Oassst-pythia-12B I Grade: -898.2676391601562 I Eliminated \\ \#11 Fastchat-t5-3B I Grade: -1251.7183837890625 I Eliminated \\ \#12 StableLM-7B I Grade: -2232.66943359375 I Eliminated \\ \#13 Dolly-12B I Grade: -3163.540283203125 I Eliminated \\ \#14 Llama-13B I Grade: -3648.37841796875 I Eliminated \\ \#15 Alpaca-13B I Grade: -14204.3984375 I Eliminated \\ \hline \multicolumn{2}{|c|}{**Grade-Elo-AlpacaEval**} \\ \hline \#1 WizardLM-13B I Grade: 8662.7158203125 \\ \#2 Vicuna-13B I Grade: 5586.46630859375 \\ \#3 Guanaco-33B I Grade: 5445.341796875 \\ \#4 Vicuna-7B I Grade: 5374.2314453125 \\ \#5 Gpt-3.5 I Grade: 4845.91552734375 \\ \#6 Koala-13B I Grade: 4338.77783203125 \\ \#7 Chatglm-6B I Grade: 2293.4208984375 I Eliminated \\ \#8 Gpt4all-13B I Grade: 2080.511962890625 \\ \#9 Mpt-7B I Grade: 1694.9445068359375 I Eliminated \\ \#10 Fastchat-t5-3B I Grade: 1371.94287109375 I Eliminated \\ \#11 Oassst-pythia-12B I Grade: -665.8685302734375 \\ \#12 StableLM-7B I Grade: -1343.5838623046875 \\ \#13 Dolly-12B I Grade: -5377.13427734375 I Eliminated \\ \#14 Llama-13B I Grade: -5847.59130859375 I Eliminated \\ \#15 Alpaca-13B I Grade: -13459.6162109375 I Eliminated \\ \hline \multicolumn{2}{|c|}{**Grade-Elo-MT_Bench**} \\ \hline \#1 WizardLM-13B I Grade: 2178.10302734375 \\ \#2 Vicuna-13B I Grade: 1720.1114501953125 \\ \#3 Guanaco-33B I Grade: 1704.1832275390625 \\ \#4 Vicuna-7B I Grade: 1659.2799072265625 \\ \#5 Gpt-3.5 I Grade: 1535.8819580078125 \\ \#6 Mpt-7B I Grade: 1338.5235595703125 I Eliminated \\ \#7 Koala-13B I Grade: 1267.9747314453125 \\ \#8 Chatglm-6B I Grade: 1011.701416015625 I Eliminated \\ \#9 Gpt4all-13B I Grade: 976.5693745117188 I Eliminated \\ \#10 Oassst-pythia-12B I Grade: 779.3573603894838 I Eliminated \\ \#11 StableLM-7B I Grade: 512.1678466796875 \\ \#12 Alpaca-13B I Grade: 334.9879455566406 I Eliminated \\ \#13 Fastchat-t5-3B I Grade: 303.5980529785156 I Eliminated \\ \#14 Dolly-12B I Grade: 72.63818359375 I Eliminated \\ \#15 Llama-13B I Grade: -395.19921875 I Eliminated \\ \hline \end{tabular}

\begin{tabular}{|c|} \hline
**Grade-Rank-Altabot** \\ \hline \#1 WizardLM-13B | Grade: 0.30809280276298523 \\ \#2 Gpt-3.5 I Grade: 0.293962299823761 \\ \#3 Guanaco-33B I Grade: 0.28587597608566284 \\ \#4 Vicuna-7B I Grade: 0.28212910890579224 \\ \#5 Vicuna-13B | Grade: 0.27900218963623047 \\ \#6 Koala-13B I Grade: 0.2672431766986847 | Eliminated \\ \#7 Mpt-7B I Grade: 0.2500302195549011 | Eliminated \\ \#8 Gpt4all-13B I Grade: 0.24746862053871155 | Eliminated \\ \#9 Chatglm-6B I Grade: 0.2466953843832016 | Eliminated \\ \#10 Oassst-pythia-12B I Grade: 0.23637069761753082 | Eliminated \\ \#11 Fastchat-t5-3B I Grade: 0.2350562959909439 | Eliminated \\ \#12 StableLM-7B I Grade: 0.22843806445598602 | Eliminated \\ \#13 Dolly-12B I Grade: 0.222194403409957891 | Eliminated \\ \#14 Llama-13B | Grade: 0.2165679931640625 | Eliminated \\ \#15 Alpaca-13B | Grade: 0.13975904881954193 | Eliminated \\ \hline \end{tabular}

\begin{tabular}{|c|} \hline
**Grade-Rank-Altabot** \\ \hline \#1 WizardLM-13B | Grade: 0.4019235074520111 \\ \#2 Vicuna-13B | Grade: 0.36745429039001465 \\ \#3 Guanaco-33B I Grade: 0.3664878010749817 \\ \#4 Vicuna-7B I Grade: 0.36541733145713806 \\ \#5 Gpt-3.5 I Grade: 0.36000365018844604 \\ \#6 Koala-13B I Grade: 0.3544933795928955 | Eliminated \\ \#7 Chatglm-6B I Grade: 0.3319571018218994 | Eliminated \\ \#8 Gpt4all-13B I Grade: 0.3306528627872467 | Eliminated \\ \#9 Mpt-7B I Grade: 0.32641729712486267 | Eliminated \\ \#10 Fastchat-t5-3B I Grade: 0.32173293828964233 | Eliminated \\ \#11 Oassst-pythia-12B I Grade: 0.2999681532382965 | Eliminated \\ \#12 StableLM-7B I Grade: 0.2932431995868683 | Eliminated \\ \#13 Dolly-12B I Grade: 0.24777530133724213 | Eliminated \\ \#14 Llama-13B I Grade: 0.243815064430236821 \\ \#15 Alpaca-13B I Grade: 0.16114839911460876 \\ \hline \end{tabular}

\begin{tabular}{|c|} \hline
**Grade-Rank-Altabot** \\ \hline \#1 WizardLM-13B | Grade: 0.2994651198387146 \\ \#2 Vicuna-13B | Grade: 0.2809261679649353 \\ \#3 Guanaco-33B I Grade: 0.2767307460308075 \\ \#4 Vicuna-7B I Grade: 0.2758147716522217 \\ \#5 Gpt-3.5 I Grade: 0.2726108839035034 \\ \#6 Mpt-7B I Grade: 0.26338690519332886 | Eliminated \\ \#7 Koala-13B I Grade: 0.2613368630409241 | Eliminated \\ \#8 Gpt4all-13B I Grade: 0.2490888833996338 | Eliminated \\ \#9 Chatglm-6B I Grade: 0.24898234009742737 | Eliminated \\ \#10 Oassst-pythia-12B I Grade: 0.2415400892496109 | Eliminated \\ \#11 StableLM-7B I Grade: 0.22990975722694397 | Eliminated \\ \#12 Alpaca-13B I Grade: 0.22171474993228912 | Eliminated \\ \#13 Fastchat-t5-3B I Grade: 0.221677652006 | Eliminated \\ \#14 Dolly-12B I Grade: 0.21185410022735596 | Eliminated \\ \#15 Llama-13B I Grade: 0.192665234208107 | Eliminated \\ \hline \end{tabular}

### Prdb

\begin{tabular}{|c|} \hline \multicolumn{2}{|c|}{**PRD-Chatbot**} \\ \hline \#1 WizardLM-13B I Grade: 5565.28271484375 \\ \#2 Gpt-3.5 I Grade: 4613.22900390625 \\ \#3 Guanaco-33B I Grade: 3423.588134765625 \\ \#4 Vicuna-7B I Grade: 2985.4892578125 \\ \#5 Vicuna-13B I Grade: 2972.15673828125 \\ \#6 Koala-13B I Grade: 2237.70751953125 \\ \#7 Chatglm-6B I Grade: 875.373779296875 \\ \#8 Mpt-7B I Grade: 602.46923828125 \\ \#9 Gpt4all-13B I Grade: 356.06243896484375 \\ \#10 Fastchat-t5-3B I Grade: 184.89663696289062 \\ \#11 Dolly-12B I Grade: 52.10746765136719 \\ \#12 Oassst-pythia-12B I Grade: -307.49908447265625 \\ \#13 StableLM-7B I Grade: -691.4453735351562 \\ \#14 Llama-13B I Grade: -848.1654052734375 \\ \#15 Alpaca-13B I Grade: -7020.923828125 \\ \hline \end{tabular}

### PRD-AlpacaEval

\begin{tabular}{|c|} \hline \multicolumn{2}{|c|}{**PRD-AlpacaEval**} \\ \hline \#1 WizardLM-13B I Grade: 5469.75634765625 \\ \#2 Guanaco-33B I Grade: 3707.014892578125 \\ \#3 Vicuna-13B I Grade: 3618.63427734375 \\ \#4 Vicuna-7B I Grade: 3569.389892578125 \\ \#5 Gpt-3.5 I Grade: 3197.755615234375 \\ \#6 Koala-13B I Grade: 2893.642578125 \\ \#7 Chatglm-6B I Grade: 1847.1300048828125 \\ \#8 Fastchat-t5-3B I Grade: 1585.66943359375 \\ \#9 Gpt4all-13B I Grade: 1561.145751953125 \\ \#10 Mpt-7B I Grade: 1332.3753662109375 \\ \#11 StableLM-7B I Grade: -33.00855255126953 \\ \#12 Oassst-pythia-12B I Grade: -92.68387603759766 \\ \#13 Dolly-12B I Grade: -3013.588623046875 \\ \#14 Llama-13B I Grade: -321.0302734375 \\ \#15 Alpaca-13B I Grade: -7432.3701171875 \\ \hline \end{tabular}

### PRD-MT_Bench

\begin{tabular}{|c|} \hline \multicolumn{2}{|c|}{**PRD-MT_Bench**} \\ \hline \#1 WizardLM-13B I Grade: 1811.64697265625 \\ \#2 Vicuna-13B I Grade: 1537.8084716796875 \\ \#3 Guanaco-33B I Grade: 1481.1739501953125 \\ \#4 Vicuna-7B I Grade: 1401.5194091796875 \\ \#5 Gpt-3.5 I Grade: 1272.8072509765625 \\ \#6 Mpt-7B I Grade: 1186.5518798828125 \\ \#7 Chatglm-6B I Grade: 1166.6246337890625 \\ \#8 Koala-13B I Grade: 1124.2513427734375 \\ \#9 Gpt4all-13B I Grade: 871.2874755859375 \\ \#10 Oassst-pythia-12B I Grade: 855.3635564453125 \\ \#11 StableLM-7B I Grade: 782.702880859375 \\ \#12 Fastchat-t5-3B I Grade: 636.966064453125 \\ \#13 Alpaca-13B I Grade: 41.9374694824219 \\ \#14 Dolly-12B I Grade: 377.5018005371094 \\ \#15 Llama-13B I Grade: 78.90127563476562 \\ \hline \end{tabular}

### Pre

**PRE-Chatbot**

#1 WizardLM-13B I Grade: 1113.7034715479742
#2 Gpt-3.5 I Grade: 1076.1116664199608
#3 Guanaco-33B I Grade: 1067.441581415147
#4 Vicuna-13B I Grade: 1057.702184441485
#5 Vicuna-7B I Grade: 1043.4840340151043
#6 Koala-13B I Grade: 1030.4455842017508 I Eliminated
#7 Chatglm-6B I Grade: 1012.4487557424748 I Eliminated
#8 Mpt-7B I Grade: 1000.4872301090011 Eliminated
#9 Gpt4all-13B I Grade: 1000.41113970384921 Eliminated
#10 Fastchat-15-3B I Grade: 992.3732179832631 Eliminated
#11 Oassst-pythia-12B I Grade: 97.52173058712721 Eliminated
#12 StableL1-M-7B I Grade: 970.36659267955351 Eliminated
#13 Llama-13B I Grade: 929.6268868888149 I Eliminated
#14 Dolly-12B I Grade: 929.1943463130976 I Eliminated
#15 Alpaca-13B I Grade: 798.6815779514078 I Eliminated

### Pre-AlpacaEval

#1 WizardLM-13B I Grade: 1127.822808841937
#2 Vicuna-7B I Grade: 1077.1823389450524
#3 Vicuna-13B I Grade: 1075.4338443616266
#4 Guanaco-33B I Grade: 1074.8043135229418
#5 Gpt-3.5 I Grade: 1065.305736105376
#6 Gpt4all-13B I Grade: 1039.40916308618651 Eliminated
#7 Koala-13B I Grade: 1038.2057499674731 Eliminated
#8 Mpt-7B I Grade: 1032.289340161278 I Eliminated
#9 Chatglm-6B I Grade: 1027.19374969185011 Eliminated
#10 Fastchat-t5-3B I Grade: 992.34811687913071 Eliminated
#11 StableLM-7B I Grade: 979.38941414456921 Eliminated
#12 Oassst-pythia-12B I Grade: 940.64384397232151 Eliminated
#13 Dolly-12B I Grade: 886.14121106627561 Eliminated
#14 Llama-13B I Grade: 880.07977242977931 Eliminated
#15 Alpaca-13B I Grade: 763.750590686025331 Eliminated

### Pre-MT_Bench

#1 WizardLM-13B I Grade: 1065.5843776639435
#2 Vicuna-13B I Grade: 1062.3934138040302
#3 Quanaco-33B I Grade: 1052.22006466556906
#4 Vicuna-7B I Grade: 1035.1111827247572
#5 Gpt-3.5 I Grade: 1029.8316754711038
#6 Koala-13B I Grade: 1024.9307662983267 I Eliminated
#7 Chatglm-6B I Grade: 1020.52389609076121 Eliminated
#8 Mpt-7B I Grade: 1014.0683255081057 I Eliminated
#9 Gpt4all-13B I Grade: 991.71426396230171 Eliminated
#10 StableLM-7B I Grade: 979.8443261256327 I Eliminated
#11 Oassst-pythia-12B I Grade: 977.99304301113221 Eliminated
#12 Fastchat-t5-3B I Grade: 953.07761591435711 Eliminated
#13 Alpaca-13B I Grade: 949.1297707316261 Eliminated
#14 Dolly-12B I Grade: 928.5110657791121 Eliminated
#15 Llama-13B I Grade: 915.06553125911851 EliminatedNeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We clearly state our claims in the abstract and introduction, such as a novel unsupervised LLM evaluation method and a consistency-based constrained optimization approach. These are substantiated in Section 3, demonstrating the alignment between our theoretical contributions and empirical results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [No] Justification: Although this paper does not have a separate 'Limitations' section, the consistency assumptions on which the work is based are clearly stated in the introduction, and their validity is experimentally verified in Section 3.5. Moreover, the limitations of our work are discussed in the conclusion, noting that the current study is conducted solely within a text-based llm evaluation environment, and exploring the potential for future expansion into multimodal large model assessments. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best 

[MISSING_PAGE_FAIL:25]

* If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
* If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
* We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
* **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: All necessary data and code have been made publicly available on GitHub, with detailed instructions for installation, environment setup, and execution commands. This includes all raw, pre-processed, intermediate, and generated data needed to reproduce our experimental results. The repository is anonymous during the review process to ensure compliance with double-blind requirements. This thorough documentation ensures that other researchers can faithfully replicate our study.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
* **Experimental Setting/Details*
* Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have detailed the data processing and training procedures in Sections 2.2 and Appendices A, B, and D. For comprehensive understanding, additional information such as hyperparameters, optimizer types, and detailed data splits are provided alongside the code due to space constraints in the paper. Guidelines:
* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We conducted each experiment four times using different seeds (\(seed=1,2,3,4\)) to ensure robustness. The results, presented as averages, are accompanied by standard deviations as error bars in Tables 1 and 2. This approach captures the variability due to different initializations and confirms the reproducibility of our results. The standard deviations used help clarify the extent of variability in the experiments, ensuring that our statistical analysis aligns with best practices for empirical research. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: Although we did not detail the exact compute resources for each experimental setup in the paper, we used NVIDIA A6000 graphics cards for open-source models and API calls for proprietary models. To facilitate reproducibility, we have provided all necessary data, ensuring that the experiments can be replicated on consumer-grade computers. This approach allows readers to reproduce the results without requiring high-end computational resources. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ** The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in this paper complies with the NeurIPS ethics guidelines in all respects. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: In the introduction, we discuss the potential positive impact of our novel unsupervised LLM evaluation approach, which could significantly advance the field of LLM evaluation. However, we also recognize potential negative societal impacts, such as the misuse of this technology to unfairly or inaccurately assess LLM systems, which might lead to biased or misleading outcomes. We suggest potential mitigation strategies, such as implementing robust validation protocols and ethical guidelines to govern the application of this evaluation methodology. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA] Justification: This paper introduces a new approach for unsupervised LLM evaluation and does not involve the release of pre-trained models, image generators, or newly collected datasets. Therefore, there are no direct risks associated with misuse or dual-use of such resources, making safeguards for controlled release irrelevant to this study.

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: This paper utilizes the FastChat project's code, along with several other pre-trained models and datasets. The FastChat project adheres to the Apache License 2.0. In compliance with the licensing requirements, we have included the original project's licensing information in all derivative works and have clearly marked any modifications made to the code. Additionally, we have ensured that all utilized pre-trained models and datasets are appropriately cited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.

* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper focuses on an unsupervised evaluation method for LLMs that does not require human feedback or interaction. Consequently, there is no involvement of crowdsourcing or research with human subjects, making details about participant instructions and compensation irrelevant. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.