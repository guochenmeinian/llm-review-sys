# Solving Sparse & High-Dimensional-Output Regression via Compression

Renyuan Li

Department of Industrial Systems Engineering & Management

National University of Singapore

renyuan.li@u.nus.edu

&Zhehui Chen

Google

zhehuichen@google.com

&Guanyi Wang

Department of Industrial Systems Engineering & Management

National University of Singapore

guanyi.w@nus.edu.sg

###### Abstract

Multi-Output Regression (MOR) has been widely used in scientific data analysis for decision-making. Unlike traditional regression models, MOR aims to simultaneously predict multiple real-valued outputs given an input. However, the increasing dimensionality of the outputs poses significant challenges regarding interpretability and computational scalability for modern MOR applications. As a first step to address these challenges, this paper proposes a Sparse & High-dimensional-Output REgression (SHORE) model by incorporating additional sparsity requirements to resolve the output interpretability, and then designs a computationally efficient two-stage optimization framework capable of solving SHORE with provable accuracy via compression on outputs. Theoretically, we show that the proposed framework is computationally scalable while maintaining the same order of training loss and prediction loss before-and-after compression under arbitrary or relatively weak sample set conditions. Empirically, numerical results further validate the theoretical findings, showcasing the efficiency and accuracy of the proposed framework.

## 1 Introduction

Multi-Output Regression (MOR) problem [8; 44] is a preponderant tool for factor prediction and decision-making in modern data analysis. Compared with traditional regression models that focus on a scalar output for each sample, MOR aims to predict multiple outputs \(\bm{y}\in\mathbb{R}^{K}\)_simultaneously_ based on a given input \(\bm{x}\in\mathbb{R}^{d}\), i.e.,

\[\bm{y}:=\operatorname*{arg\,min}_{\bm{u}\in\mathcal{Y}}\ \mathtt{ dist}(\bm{u},\widehat{g}(\bm{x}))\ \ \text{with}\ \ \widehat{g}:=\operatorname*{arg\,min}_{g\in\mathcal{G}}\ \frac{1}{n}\sum_{i=1}^{n}\ell(\bm{y}^{i},g(\bm{x}^{i}))\ \,\]

where we use \(\{(\bm{x}^{i},\bm{y}^{i})\}_{i=1}^{n}\) to denote its given sample set with \(\bm{x}^{i}\in\mathbb{R}^{d}\)\(i\)-th input feature vector and \(\bm{y}^{i}\in\mathbb{R}^{K}\) corresponding output vector, define \(\ell:\mathbb{R}^{K}\times\mathbb{R}^{K}\rightarrow\mathbb{R}\) as the loss function, \(\mathtt{dist}:\mathbb{R}^{K}\times\mathbb{R}^{K}\rightarrow\mathbb{R}\) as some prediction/distance metric, \(\mathcal{Y}\) as the structure/constraint set for multiple outputs, and \(\mathcal{G}\) as the candidate set for predicting model \(g:\mathbb{R}^{d}\rightarrow\mathbb{R}^{K}\). Hence, MOR and its variants have been used for numerous regression tasks with structure requirements on multi-dimensional outputs arising from real applications, such as simultaneous estimation of biophysical parameters from remote sensing images [40], channel estimation through the prediction of several received signals [35], the grounding (e.g., factuality check [16]) in the Large Language Model (LLM, [34; 11]) era, to name but a few.

In this paper, we are interested in the interpretability issue of high-dimensional outputs obtained from modern MOR tasks. One typical example is raised from algorithmic trading. In particular, in algorithmic trading, MOR helps to construct the portfolio [31] from a large number of financial instruments (e.g., different stocks, futures, options, equities, etc [19]) based on given historical market and alternative data [22]. To be concise, a high-dimensional output in this example could be viewed as a "decision", where every component denotes the investment for the corresponding financial instruments. Thus, other than accuracy, quantitative researchers prefer outputs with only a few instruments to enhance interpretability for the underlying decision-making reasons, which naturally introduces a sparse output condition. Similar scenarios apply to other applications, including offline reinforcement learning in robotics[33], discovering genetic variations based on genetic markers[25].

As a result, the dramatic growth in output dimensions gives rise to two significant challenges: **1**. High-dimensional-output impedes human interpretation for decision-making; **2**. Approaches with better computational scalability are desired for training & predicting MOR. Upon these challenges, a conceptual question that motivates this research is:

_How to design a framework that predicts output with enhanced interpretability, better computational scalability, and provable accuracy under a modern high-dimensional-output setting?_

Generally speaking, this paper provides an affirmative answer as a first step to the above question. Before presenting the main contributions, let us first introduce the model that will be studied in this paper. Unlike the classical MOR model, we further assume that given outputs are of high-dimensional (i.e., \(d\ll K\)), and to address the interpretability issue, these outputs have at most \(s\) non-zero components, i.e., \(\|\bm{y}^{i}\|_{0}\leq s\), for all \(i\in[n]\) with some pre-determined sparsity-level \(s(\ll K)\). Based on such given samples, this paper proposes the (uncompressed) _Sparse & High-dimensional-Output REgression (SHORE)_ model that aims to predict an interpretable high-dimensional output \(\bm{y}\) (i.e., \(s\)-sparse in this paper) via any input feature vector \(\bm{x}\). In particular, to be concise and still capture the essential relationship, the proposed (uncompressed) SHORE model predicts \(\bm{y}\) from \(\bm{x}\) under a linear model, i.e., \(\bm{y}=\arg\min_{\|\bm{y}\|_{0}\leq s}\texttt{dist}(\bm{y},\widehat{\bm{Z}}\bm {x})\) for some distance metric (see Section 3.1, prediction stage) and the linear regression \(\widehat{\bm{Z}}\) is obtained by solving the following linear regression problem:

\[\widehat{\bm{Z}}:=\operatorname*{arg\,min}_{\bm{Z}\in\mathbb{R}^{K\times d}} \widehat{\bm{L}}_{n}(\bm{Z}):=\frac{1}{n}\|\bm{Y}-\bm{Z}\bm{X}\|_{F}^{2},\] (1)

where \(\bm{X}:=(\bm{x}^{1}\mid\cdots\mid\bm{x}^{n})\in\mathbb{R}^{d\times n}\) is the input matrix and \(\bm{Y}:=(\bm{y}^{1}\mid\cdots\mid\bm{y}^{n})\in\mathbb{R}^{K\times n}\) is the corresponding _column-sparse_ output matrix.

### Contributions and Paper Organization

This paper makes the following three main contributions:

**1**. We propose a two-stage computationally efficient framework for solving SHORE model. Specifically, the first training stage offers a computationally scalable reformulation on solving SHORE through compression in the output space. The second prediction stage then predicts high-dimensional outputs from a given input by solving a specific sparsity-constrained minimization problem via an efficient iterative algorithm.

**2**. We show that for arbitrarily given samples, the training loss in the first stage with compression is bounded by a \(1+\delta\) multiplicative ratio of the training loss for the original one (1) with some positive constant \(\delta\). Additionally, the proposed iterative algorithm in the second stage exhibits global geometric convergence within a neighborhood of the ground-truth output, with a radius proportional to the given sample's optimal training loss. Furthermore, if all samples are drawn from a light-tailed distribution, the generalization error bound and sample complexity remain in the same order for SHORE with output compression. This finding indicates that the proposed framework achieves improved computational efficiency while maintaining the same order of generalization error bounds statistically.

**3**. We conduct rich numerical experiments that validate the theoretical findings and demonstrate the efficiency and accuracy of the proposed framework on both synthetic and real-world datasets.

In summary, this paper studies the SHORE model through computational and statistical lenses and provides a computationally scalable framework with provable accuracy.

The paper is organized as follows: Section 2 reviews related literature; Section 3 presents our proposed framework and provides theoretical results on sample complexity and generalization error bounds; Section 4 compares the proposed method with existing baselines in a suite of numerical experiments on both synthetic and real instances. Concluding remarks are given in Section 5.

Notation.Given a positive integer \(n\), we denote \([n]:=\{1,\ldots,n\}\). We use lowercase letters \(a\) as scalars and bold lowercase letters \(\bm{a}\) as vectors, where \(\bm{a}_{i}\) is its \(i\)-th component with \(i\in[d]\), and bold upper case letters \(\bm{A}\) as matrices. Without specific description, for a \(m\)-by-\(n\) matrix \(\bm{A}\), we denote \(\bm{A}_{i,j}\) as its \((i,j)\)-th component, \(\bm{A}_{i,:}^{\top}\), as its \(i\)-th row, \(\bm{A}_{:,j}\) as its \(j\)-th column. For a symmetric square matrix \(\bm{A}\), we denote \(\lambda_{\max}(\bm{A})\), \(\lambda_{\min}(\bm{A})\) and \(\lambda_{i}(\bm{A})\) as its maximum, minimum and \(i\)-th largest eigenvalue, respectively. We denote \(\|\bm{a}\|_{1},\|\bm{a}\|_{2},\|\bm{a}\|_{\infty},\|\bm{A}\|_{F},\|\bm{A}\|_{\uptheta}\) as the \(\ell_{1},\ell_{2},\ell_{\infty}\)-norm of a vector \(\bm{a}\), the Frobenius norm and the operator norm of a matrix \(\bm{A}\), respectively. We denote \(\mathbb{I}(\cdot)\) as the indicator function, \(\|\bm{a}\|_{0}:=\sum_{i=1}^{d}\mathbb{I}(\bm{a}_{i}\neq 0)\) as the \(\ell_{0}\)-norm (i.e., the total number of nonzero components), \(\text{supp}(\bm{a}):=\{i\in[d]\mid\bm{a}_{i}\neq 0\}\) as the support set. We denote \(\mathcal{V}_{s}^{K}:=\{\bm{y}\in\mathbb{R}^{K}\mid\|\bm{y}\|_{0}\leq s\}\) as a set of \(s\)-sparse vectors, \(\mathbb{B}_{2}(\bm{c};\rho):=\{\bm{y}\in\mathbb{R}^{K}\mid\|\bm{y}-\bm{c}\|_{2 }\leq\rho\}\) as a closed \(\ell_{2}\)-ball with center \(\bm{c}\) and radius \(\rho\), \(\mathcal{N}(\mu,\sigma^{2})\) as a Gaussian distribution with mean \(\mu\) and covariance \(\sigma^{2}\).

For two sequences of non-negative reals \(\{f_{n}\}_{n\geq 1}\) and \(\{g_{n}\}_{n\geq 1}\), we use \(f_{n}\lesssim g_{n}\) to indicate that there is a universal constant \(C>0\) such that \(f_{n}\leq Cg_{n}\) for all \(n\geq 1\). We use standard order notation \(f_{n}=O(g_{n})\) to indicate that \(f_{n}\lesssim g_{n}\) and \(f_{n}=\widetilde{O}_{\tau}(g_{n})\) to indicate that \(f_{n}\lesssim g_{n}\ln^{c}(1/\tau)\) for some universal constants \(\tau\) and \(c\). Throughout, we use \(\epsilon,\delta,\tau,c,c_{1},c_{2},\ldots\) and \(C,C_{1},C_{2},\ldots\) to denote universal positive constants, and their values may change from line to line without specific comments.

## 2 Literature Review

Multi-output regression (MOR) and its variants have been studied extensively over the past decades. In this section, we focus on existing works related to our computational and statistical results.

**Computational part.** Existing computational methods for solving MOR can be, in general, classified into two categories [8], known as problem transformation methods and algorithm adaptation methods. Problem transformation methods (e.g., Binary Relevance (BR), multi-target regressor stacking (MTRS) method [37], regression chains method [37]) aim to transform MOR into multiple single-output regression problems. Thus, any state-of-the-art single-output regression algorithm can be applied, such as ridge regression [15], regression trees [9], and etc. However, these transformation methods ignore the underlying structures/relations between outputs, which leads to higher computational complexities. In contrast, algorithm adaptation methods focus more on the underlying structures/relations between outputs. For instance, [36] investigates input component selection and shrinkage in multioutput linear regression; [1] later couples linear regressions and quantile mapping and thus captures joint relationships among variables. However, the output dimension considered in these works is relatively small compared with modern applications, and their assumptions concerning low-dimensional structure of outputs are hard to verify. _To overcome these shortages, we consider high-dimensional-output regression with only an additional sparsity requirement on outputs._

**Statistical part.** There are numerous works concerning statistical properties of traditional or multi-output regressions. [18] gives sharp results on "out-of-sample" (random design) prediction error for the ordinary least squares estimator of traditional linear regression. [45] proposes an empirical risk minimization framework for large-scale multi-label learning with missing outputs and provides excess risk generalization error bounds with additional bounded constraints. [28] investigates the generalization performance of structured prediction learning and provides generalization error bounds on three different scenarios, i.e., Lipschitz continuity, smoothness, and space capacity condition. [27] designs an efficient feature selection procedure for multiclass sparse linear classifiers (a special case for SHORE with sparsity-level \(s=1\)), and proves that the proposed classifiers guarantee the minimax generalization error bounds in theory. A recent paper [42] studies transfer learning via multi-task representation learning, a special case in MOR, which proves statistically optimistic rates on the excess risk with regularity assumptions on the loss function and task diversity. _In contrast with these works, our contributions concentrate on how generalization error bounds change before and after the compression under relatively weak conditions on the loss function and underlying distributions._

**Specific results in MLC.** MLC is an important and special case for MOR with \(\{0,1\}\)-valued output per dimension, i.e., \(\bm{y}\in\{0,1\}^{K}\), and thus, in this paragraph, we use labels to replace outputs. Here, we focus on dimensionality reduction techniques on outputs, in particular, the compressed sensing and low-rank conditions on the output matrix \(\bm{Y}\). The idea of compressed sensing rises from signal processing, which maps the original high-dimensional output space into a smaller one while ensuring the restricted isometry property (RIP). To the best of our knowledge, the compressed sensing technique is first used in [17] to handle a sparse expected output \(\mathbb{E}[\bm{y}|\bm{x}]\). Later, [39; 12] propose Principle Label Space Transformation (PLST) and conditional PLST through singular value decomposition and canonical component analysis respectively. More recently, many new compression approaches have been proposed, such as robust bloom filter [13], log time log space extreme classification [23], merged averaged classifiers via hashing [32], etc. Additionally, computational efficiency and statistical generalization bounds can be further improved when the output matrix \(\bm{Y}\) ensures a low-rank condition. Under such a condition, [45] provides a general empirical risk minimization framework for solving MLC with missing labels. _Compared with the above works, this paper studies MOR under a sparse & high-dimensional-output setting without additional correlation assumptions or low-rank assumptions for output space, and then provides a complete story through a computational and statistical lens._

## 3 Main Results

### Two-Stage Framework

This subsection presents a general framework for solving SHORE and then the computational complexity for the proposed framework _with/without compression_. Given a set of training samples \(\{(\bm{x}^{i},\bm{y}^{i})\}_{i=1}^{n}\) as described in Section 1, the framework can be separated into two stages: (compressed) training stage & (compressed) prediction stage.

**Training stage.** In the first training stage, the framework finds a _compressed regressor_ by solving a linear regression problem with compressed outputs. In particular, the framework compresses the original large output space (\(K\)-dim) to a smaller "latent" output space (\(m\)-dim) by left-multiplying a so-called "compressed" matrix \(\bm{\Phi}\in\mathbb{R}^{m\times K}\) to outputs. Thus, the _compressed version_ of training stage in SHORE can be represented as follows,

\[\widehat{\bm{W}}:=\operatorname*{arg\,min}_{\bm{W}\in\mathbb{R}^{m\times d}} \ \widehat{\mathcal{L}}_{n}^{\bm{\Phi}}(\bm{W}):=\frac{1}{n}\|\bm{\Phi}\bm{Y}-\bm {W}\bm{X}\|_{F}^{2}.\] (2)

We would like to point out that the idea of compressing the output space into some smaller intrinsic dimension has been used in many existing works, e.g., [17; 39; 12] mentioned in Section 2.

**Prediction stage.** In the second prediction stage, given any input \(\bm{x}\in\mathbb{R}^{d}\), the framework predicts a sparse output \(\widehat{\bm{y}}\) by solving the following prediction problem based on the learned regressor \(\widehat{\bm{W}}\) in the training stage,

\[\widehat{\bm{y}}(\widehat{\bm{W}}):=\operatorname*{arg\,min}_{\bm{y}}\|\bm{ \Phi}\bm{y}-\widehat{\bm{W}}\bm{x}\|_{2}^{2}\ \ \text{s.t.}\ \ \bm{y}\in\mathcal{V}_{s}^{K}\cap\mathcal{F},\] (3)

where \(\mathcal{V}_{s}^{K}\) is the set of \(s\)-sparse vectors in \(\mathbb{R}^{K}\), and \(\mathcal{F}\) is some feasible set to describe additional requirements of \(\bm{y}\). For example, by letting \(\mathcal{F}\) be \(\mathbb{R}^{K},\mathbb{R}^{K}_{+},\{0,1\}^{K}\), the intersection \(\mathcal{V}_{s}^{K}\cap\mathcal{F}\) denotes the set of \(s\)-sparse output, non-negative \(s\)-sparse output, \(\{0,1\}\)-valued \(s\)-sparse output, respectively. We use \(\widehat{\bm{y}}(\widehat{\bm{W}})\) (shorthanded in \(\widehat{\bm{y}}\)) to specify that the predicted output is based on the regressor \(\widehat{\bm{W}}\). To solve the proposed prediction problem (3), we utilize the following projected gradient descent method (Algorithm 1), which could be viewed as a variant/generalization of existing iterative thresholding methods [6; 21] for nonconvex constrained minimization. In particular, step 4 incorporates additional constraints from \(\mathcal{F}\) other than sparsity into consideration, which leads to non-trivial modifications in designing efficient projection oracles and convergence analysis. Later, we show that the proposed Algorithm 1 ensures a near-optimal convergence (Theorem 2 and Theorem 4) while greatly reduces the computational complexity (Remark 2) of the prediction stage for solving compressed SHORE.

Before diving into theoretical analysis, we first highlight the differences between the proposed prediction stage (3), general sparsity-constrained optimization (SCO), and sparse regression in the following remark.

**Remark 1**.: _Proposed prediction stage v.s. General SCO: To be clear, the SCO here denotes the following minimization problem \(\min_{\|\bm{\alpha}\|_{0}\leq k}\|\bm{A}\bm{\alpha}-\bm{\beta}\|_{2}^{2}\). Thus, the prediction stage is a special case of general SCO problem. In particular, the predicted stage takes a random projection matrix \(\bm{\Phi}\) with restricted isometry property (RIP) to be its \(\bm{A}\) and uses \(\widehat{\bm{W}}\bm{x}\) with \(\widehat{\bm{W}}\) obtained from the compressed training-stage to be its \(\bm{\beta}\). As a result (Theorem 2 and Theorem 4), the proposed Algorithm 1 for prediction stage ensures a globally linear convergence to a ball with center \(\widehat{\bm{y}}\) (optimal solution of the prediction-stage) and radius \(O(\|\bm{\Phi}\widehat{\bm{y}}-\widehat{\bm{W}}\bm{x}\|_{2})\), which might not hold for general SCO problems._

_Proposed prediction stage v.s. Sparse regression: Although the proposed prediction stage and sparse high-dimensional regression share a similar optimization formulation \(\min_{\|\bm{\beta}\|_{0}\leq k}\ \|\bm{Y}-\bm{X}^{\top}\bm{\beta}\|_{2}^{2}\), the proposed prediction stage (3) is distinct from the sparse regression in the following parts:_

_(1) Underlying Model: Most existing works about sparse high-dimensional regression assume that samples are i.i.d. generated from the linear relationship \(\bm{Y}=\bm{X}^{\top}\bm{\beta}^{*}+\bm{\epsilon}\) with underlying sparse ground truth \(\bm{\beta}^{*}\). In the proposed prediction stage, we do not assume additional underlying models on samples if there is no further specific assumption. The problem we studied in the predicted stage takes the random projection matrix \(\bm{\Phi}\) with restricted isometry property (RIP) as its \(\bm{X}^{\top}\) (whereas \(\bm{X}^{\top}\) in sparse regression does not ensure RIP), and uses \(\widehat{\bm{W}}\bm{x}\) with \(\widehat{\bm{W}}\) obtained from the compressed training-stage as its \(\bm{Y}\)._

_(2) Problem Task: The sparse regression aims to recover the sparse ground truth \(\bm{\beta}^{*}\) given a sample set \(\{(\bm{x}^{i},\bm{y}^{i})\}_{i=1}^{n}\) with \(n\) i.i.d. samples. In contrast, the task of the proposed prediction stage is to predict a sparse high-dimensional output \(\widehat{\bm{y}}\) given a random projection matrix \(\bm{\Phi}\) and a single input \(\bm{x}\). As a quick summary, some typical and widely used iterative algorithms [38, 3, 4, 29] for sparse regression cannot be directly applied to the proposed prediction stage._

Then, we provide the computational complexity _with and without the compression_ for the proposed two-stage framework to complete this subsection.

**Remark 2**.: _Training stage: Conditioned on \(\bm{X}\bm{X}^{\top}\) is invertible, the compressed regressor \(\widehat{\bm{W}}\) has a closed form solution \(\widehat{\bm{W}}=\bm{\Phi}\bm{Y}\bm{X}^{\top}(\bm{X}\bm{X}^{\top})^{-1}\) with overall computational complexity_

\[O(Kmn+mnd+nd^{2}+d^{3}+md^{2})\approx O(Kmn).\]

_Compared with the computational complexity of finding \(\widehat{\bm{Z}}\) from the uncompressed SHORE (1)_

\[O(Knd+nd^{2}+d^{3}+Kd^{2})\approx O(K(n+d)d),\]

_solving \(\widehat{\bm{W}}\) enjoys a smaller computational complexity on the training stage if \(m\ll d\). In later analysis (see Section 3.2), \(m=O(\delta^{-2}\cdot\mathrm{s}\log(\frac{K}{r}))\) with some predetermined constants \(\delta,\tau\) and sparsity-level \(s\ll d\), thus in many applications with large output space, the condition \(m\ll d\) holds._

_Prediction stage: The computational complexity of each step-3 in Algorithm 1 is_

\[O(Km+K+Km+K)\approx O(Km).\]

_The projection in step-4 is polynomially solvable with computational complexity \(O(K\min\{s,\log K\})\) (see proof in Appendix A.5.1). Thus, the overall computational complexity of Algorithm 1 is_

\[O(K(m+\min\{s,\log K\})T).\]

_Compared with the complexity \(O(K(d+\min\{s,\log K\}))\) of predicting \(\widehat{\bm{y}}\) from the uncompressed SHORE (1), the compressed version enjoys a smaller complexity on the prediction stage if_

\[(m+\min\{s,\log K\})T\ll d+\min\{s,\log K\}.\] (4)

_In later analysis (see Theorem 2), since \(m=O(\delta^{-2}\cdot\mathrm{s}\log(\frac{K}{\tau}))\) with predetermined constants \(\delta,\tau\), sparsity-level \(s\ll d\), and \(T=O(\log[\frac{\|\widehat{\bm{y}}-\widehat{\bm{w}}^{(0)}\|_{2}}{\|\bm{\Phi} \widehat{\bm{y}}-\widehat{\bm{W}}\bm{x}\|_{2}}])\) from inequality (5), we have condition 4 holds._

_Whole computational complexity: Based on the analysis of computational complexity above, we conclude that when the parameters \((K,d,m,T)\) satisfies_

\[K>K^{1/3}>d\gg O(\delta^{-2}\log(K/\tau)\cdot T)=mT,\]

_the compressed SHORE enjoys a better computational complexity with respect to the original one (1)._

### Worst-Case Analysis for Arbitrary Samples

We begin this subsection by introducing the generalization method of the compressed matrix \(\bm{\Phi}\).

**Assumption 1**.: _Given an \(m\)-by-\(K\) compressed matrix \(\bm{\Phi}\), all components \(\bm{\Phi}_{i,j}\) for \(1\leq i\leq m\) and \(1\leq j\leq K\), are i.i.d. generated from a Gaussian distribution \(\mathcal{N}(0,1/m)\)._

Before presenting the main theoretical results, let us first introduce the definition of restricted isometry property (RIP, [10]), which is ensured by the generalization method (Assumption 1).

**Definition 1**.: \((\mathcal{V},\delta)\)_**-RIP:** A \(m\)-by-\(K\) matrix \(\bm{\Phi}\) is said to be \((\mathcal{V},\delta)\)-RIP over a given set of vectors \(\mathcal{V}\subseteq\mathbb{R}^{K}\), if, for every \(\bm{v}\in\mathcal{V}\),_

\[(1-\delta)\|\bm{v}\|_{2}^{2}\leq\|\bm{\Phi}\bm{v}\|_{2}^{2}\leq(1+\delta)\| \bm{v}\|_{2}^{2}.\]

_In the rest of the paper, we use \((s,\delta)\)-RIP to denote \((\mathcal{V}_{s}^{K},\delta)\)-RIP. Recall \(\mathcal{V}_{s}^{K}=\{\bm{v}\in\mathbb{R}^{K}\,|\,\|\bm{v}\|_{0}\leq s\}\) is the set of \(s\)-sparse vectors._

**Remark 3**.: _From Johnson-Lindenstrauss Lemma [43], for any \(\delta\in(0,1)\), any \(\tau\in(0,1)\), and any finite vector set \(|\mathcal{V}|<\infty\), if the number of rows \(m\geq O\left(\delta^{-2}\cdot\log(\frac{|\mathcal{V}|}{\tau})\right)\), then the compressed matrix \(\bm{\Phi}\) generated by Assumption 1 satisfies \((\mathcal{V},\delta)\)-RIP with probability at least \(1-\tau\)._

Now, we are poised to present the first result on training loss defined in (2).

**Theorem 1**.: _For any \(\delta\in(0,1)\) and \(\tau\in(0,1)\), suppose compressed matrix \(\bm{\Phi}\) follows Assumption 1 with \(m\geq O(\frac{1}{\delta^{2}}\cdot\log(\frac{K}{\tau}))\). We have the following inequality for training loss_

\[\|\bm{\Phi}\bm{Y}-\widehat{\bm{W}}\bm{X}\|_{F}^{2}\leq(1+\delta)\cdot\|\bm{Y }-\widehat{\bm{Z}}\bm{X}\|_{F}^{2},\]

_holds with probability at least \(1-\tau\), where \(\widehat{\bm{Z}},\widehat{\bm{W}}\) are optimal solutions for the uncompressed (1) and compressed SHORE (2), respectively._

The proof of Theorem 1 is presented in Appendix A.1. In short, Theorem 1 shows that the optimal training loss for the compressed version is upper bounded within a \((1+\delta)\) multiplicative ratio with respect to the optimal training loss for the uncompressed version. Intuitively, Theorem 1 implies that SHORE remains similar performances for both compressed and compressed versions, while the compressed version saves roughly \(O(Kn(d-m)+Kd^{2})\) computational complexity in the training stage from Remark 2. Moreover, the lower bound condition on \(m\geq O(\frac{1}{\delta^{2}}\cdot\log(\frac{K}{\tau}))\) ensures that the generated compressed matrix \(\bm{\Phi}\) is \((1,\delta)\)-RIP with probability at least \(1-\tau\). For people of independent interest, Theorem 1 only needs \((1,\delta)\)-RIP (independent with the sparsity level) due to the _unitary invariant_ property of \(\bm{\Phi}\) from Assumption 1 (details in Appendix A.1). Additionally, due to the inverse proportionality between \(m\) and \(\delta^{2}\), for fixed \(K\) and \(\tau\), the result can be written as

\[\|\bm{\Phi}\bm{Y}-\widehat{\bm{W}}\bm{X}\|_{F}^{2}\leq\left(1+O(1/\sqrt{m}) \right)\cdot\|\bm{Y}-\widehat{\bm{Z}}\bm{X}\|_{F}^{2},\]

which is verified in our experiments 4.

```
1:Input: Regressor \(\widehat{\bm{W}}\), input sample \(\bm{x}\), stepsize \(\eta\), total iterations \(T\)
2:Initialize point \(\bm{v}^{(0)}\in\mathcal{V}_{s}^{K}\cap\mathcal{F}\).
3:for\(t=0,1,\dots,T-1\):do
4: Update \(\widetilde{\bm{v}}^{(t+1)}=\bm{v}^{(t)}-\eta\cdot\bm{\Phi}^{\top}(\bm{\Phi}\bm{v }^{(t)}-\widehat{\bm{W}}\bm{x})\).
5: Project \(\bm{v}^{(t+1)}=\Pi(\widetilde{\bm{v}}^{(t+1)}):=\arg\min_{\bm{v}\in\mathcal{V}_ {s}^{K}\cap\mathcal{F}}\|\bm{v}-\widetilde{\bm{v}}^{(t+1)}\|_{2}^{2}\).
6:endfor ```

**Output:\(\bm{v}^{(T)}\).** ```

**Algorithm 1** Projected Gradient Descent (for Second Stage)

We then present the convergence result of the proposed Algorithm 1 for solving prediction problem (3).

**Theorem 2**.: _For any \(\delta\in(0,1)\) and \(\tau\in(0,1)\), suppose the compressed matrix \(\bm{\Phi}\) follows Assumption 1 with \(m\geq O(\frac{s}{\delta^{2}}\log(\frac{K}{\tau}))\). With a fixed stepsize \(\eta\in(\frac{1}{2-2\delta},1)\), the following inequality_

\[\|\widehat{\bm{y}}-\bm{v}^{(t)}\|_{2}\leq c_{1}^{t}\cdot\|\widehat{\bm{y}}-\bm{ v}^{(0)}\|_{2}+\frac{c_{2}}{1-c_{1}}\cdot\|\bm{\Phi}\widehat{\bm{y}}-\widehat{\bm{W}} \bm{x}\|_{2}\]

_holds for all \(t\in[T]\) simultaneously with probability at least \(1-\tau\), where \(c_{1}:=2-2\eta+2\eta\delta<1\) is some positive constant strictly smaller than 1, and \(c_{2}:=2\eta\sqrt{1+\delta}\) is some constant._The proof of Theorem 2 is given in Appendix A.2. Here, the lower bound condition on the number of rows \(m\) ensures that the generated compressed matrix \(\bm{\Phi}\) is \((3s,\delta)\)-RIP with probability at least \(1-\tau\) by considering a \(\delta/2\)-net cover of set \(\mathcal{V}=\mathcal{V}_{3s}^{K}\cap\mathbb{E}_{2}(\bm{0};1)\) from Johnson-Lindenstrauss Lemma [43]. Moreover, since the number of rows \(m\) required in Theorem 2 is greater than the one required in Theorem 1, term \(\|\bm{\Phi}\widehat{\bm{y}}-\widehat{\bm{W}}\bm{x}\|_{2}\) can be further upper bounded using the uncompressed version \((1+\delta)\|\widehat{\bm{y}}-\widehat{\bm{Z}}\bm{x}\|_{2}\) with probability at least \(1-\tau\). Then we obtain a direct corollary of Theorem 2: suppose \(\|\widehat{\bm{y}}-\bm{v}^{(0)}\|_{2}>\|\bm{\Phi}\widehat{\bm{y}}-\widehat{\bm {W}}\bm{x}\|_{2}\), if

\[t\geq t_{*}:=O\left(\log\left(\|\widehat{\bm{y}}-\bm{v}^{(0)}\|_{2}/\|\bm{ \Phi}\widehat{\bm{y}}-\widehat{\bm{W}}\bm{x}\|_{2}\right)\!\Big{/}\log\left(1/ c_{1}\right)\right),\] (5)

the proposed Algorithm 1 guarantees a globally linear convergence to a ball \(\mathbb{B}(\widehat{\bm{y}};O(\|\bm{\Phi}\widehat{\bm{y}}-\widehat{\bm{W}}\bm {x}\|_{2}))\).

In contrast with OMP used in [17] for multi-label predictions, Theorem 2 holds for arbitrary sample set without the so-called bounded coherence guarantee on \(\bm{\Phi}\). Moreover, as reported in Section 4, the proposed prediction method (Algorithm 1) has better computational efficiency than OMP.

### Generalization Error Bounds for IID Samples

This subsection studies a specific scenario when every sample \((\bm{x}^{i},\bm{y}^{i})\) is i.i.d. drawn from some underlying subGaussian distribution \(\mathcal{D}\) over sample space \(\mathbb{R}^{d}\times\mathcal{V}_{s}^{K}\). Specifically, we use

\[\mathbb{E}_{\mathcal{D}}\left[\begin{pmatrix}\bm{x}\\ \bm{y}\end{pmatrix}\right]=\begin{pmatrix}\bm{\mu}_{\bm{x}}\\ \bm{\mu}_{\bm{y}}\end{pmatrix}=:\bm{\mu}\quad\text{and}\quad\text{Var}_{ \mathcal{D}}\left[\begin{pmatrix}\bm{x}\\ \bm{y}\end{pmatrix}\right]=\begin{pmatrix}\bm{\Sigma}_{\bm{x}\bm{x}}&\bm{\Sigma }_{\bm{x}\bm{y}}\\ \bm{\Sigma}_{\bm{y}\bm{x}}&\bm{\Sigma}_{\bm{y}\bm{y}}\end{pmatrix}=:\bm{\Sigma }\succeq\bm{0}_{(d+K)\times(d+K)}\]

to denote its mean and variance, respectively. Let \(\bm{\xi}_{\bm{x}}:=\bm{x}-\bm{\mu}_{\bm{x}},\bm{\xi}_{\bm{y}}:=\bm{y}-\bm{\mu} _{\bm{y}},\bm{\xi}:=(\bm{\xi}_{\bm{x}}^{\top},\bm{\xi}_{\bm{y}}^{\top})^{\top}\) be centered subGaussian random variables of \(\bm{x},\bm{y},(\bm{x}^{\top},\bm{y}^{\top})^{\top}\), respectively. Let \(\bm{a},\bm{b}\in\{\bm{x},\bm{y}\}\), we use \(\bm{M}_{\bm{a}\bm{b}}:=\mathbb{E}_{\mathcal{D}}[\bm{a}\bm{b}^{\top}]=\bm{ \Sigma}_{\bm{a}\bm{b}}+\bm{\mu}_{\bm{a}}\bm{\mu}_{\bm{b}}^{\top},\widehat{\bm {M}}_{\bm{a}\bm{b}}:=\frac{1}{n}\sum_{i=1}^{n}\bm{a}^{i}(\bm{b}^{i})^{\top}\) to denote the population second (cross-)moments and empirical second (cross-)moments, respectively. Then, the population training loss is defined as

\[\min_{\bm{Z}\in\mathbb{R}^{K\times d}}\mathcal{L}(\bm{Z}):=\mathbb{E}_{(\bm{x },\bm{y})\sim\mathcal{D}}\left[\|\bm{y}-\bm{Z}\bm{x}\|_{2}^{2}\right]\]

with its optimal solution \(\bm{Z}_{*}:=\bm{M}_{\bm{y}\bm{x}}\bm{M}_{\bm{x}\bm{x}}^{-1}\). Similarly, given a \(\bm{\Phi}\), the compressed training loss is given by

\[\mathcal{L}^{\bm{\Phi}}(\bm{W}):=\mathbb{E}_{(\bm{x},\bm{y})\sim\mathcal{D}} \left[\|\bm{\Phi}\bm{y}-\bm{W}\bm{x}\|_{2}^{2}\right]\]

with optimal solution \(\bm{W}_{*}:=\bm{\Phi}\bm{M}_{\bm{y}\bm{x}}\bm{M}_{\bm{x}\bm{x}}^{-1}\). We then define the following assumption:

**Assumption 2**.: _Let \(\mathcal{D}\) be \(\sigma^{2}\)-subGaussian for some positive constant \(\sigma^{2}>0\), i.e., the inequality \(\mathbb{E}_{\mathcal{D}}[\exp(\lambda\bm{u}^{\top}\bm{\xi})]\leq\exp\left( \lambda^{2}\sigma^{2}/2\right)\) holds for any \(\lambda>0\) and unitary vector \(\bm{u}\in\mathbb{R}^{d+K}\). Moreover, the covariance matrix \(\bm{\Sigma}_{\bm{x}\bm{x}}\) is positive definite (i.e., its minimum eigenvalue \(\lambda_{\min}(\bm{\Sigma}_{\bm{x}\bm{x}})>0\))._

**Remark 4**.: _Assumption 2 ensures the light tail property of distribution \(\mathcal{D}\). Note that in some real applications, e.g., factuality check [31], algorithmic trading [16], one can normalize input and output vector to ensure bounded \(\ell_{2}\)-norm. Under such a situation, Assumption 2 is naturally satisfied._

Our first result in this subsection gives the generalization error bounds.

**Theorem 3**.: _For any \(\delta\in(0,1)\) and \(\tau\in(0,\frac{1}{3})\), suppose compressed matrix \(\bm{\Phi}\) follows Assumption 1 with \(m\geq O(\frac{s}{\delta^{2}}\log(\frac{K}{\tau}))\), and Assumption 2 holds, for any constant \(\epsilon>0\), the following results hold: (Matrix Error). The inequality for matrix error \(\left\|\bm{M}_{\bm{x}\bm{x}}^{1/2}\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1}\bm{M}_{ \bm{x}\bm{x}}^{1/2}\right\|_{\mathrm{op}}\leq 4\) holds with probability at least \(1-2\tau\) as the number of samples \(n\geq n_{1}\) with_

\[n_{1}:=\max\left\{\frac{64C^{2}\sigma^{4}}{9\lambda_{\min}^{2}(\bm{M}_{\bm{x} \bm{x}})}\left(d+\log(2/\tau)\right),\;\frac{32^{2}\|\bm{\mu}_{\bm{x}}\|_{2}^{ 2}\sigma^{2}}{\lambda_{\min}^{2}(\bm{M}_{\bm{x}\bm{x}})}\left(2\sqrt{d}+\sqrt {\log(1/\tau)}\right)^{2}\right\},\]

_where \(C\) is some fixed positive constant used in matrix concentration inequality of operator norm._

_(Uncompressed). The generalization error bound for uncompressed SHORE satisfies \(\mathcal{L}(\widehat{\bm{Z}})\leq\mathcal{L}(\bm{Z}_{*})+4\epsilon\) with probability at least \(1-3\tau\), as the number of samples \(n\geq\max\{n_{1},n_{2}\}\) with_

\[n_{2}:=\max\left\{4(\|\bm{Z}_{*}\|_{F}^{2}+K)\cdot\frac{d+2\sqrt{d\log(K/\tau)}+2 \log(K/\tau)}{\epsilon},\;4\|\bm{\mu}_{\bm{y}}-\bm{Z}_{*}\bm{\mu}_{\bm{x}}\|_{2}^ {2}\cdot\frac{d}{\epsilon}\right\}.\]_(Compressed). The generalization error bound for the compressed SHORE satisfies \(\mathcal{L}^{\bm{\Phi}}(\widehat{\bm{W}})\leq\mathcal{L}^{\bm{\Phi}}(\bm{W}_{*})+ 4\epsilon\) with probability at least \(1-3\tau\), as the number of sample \(n\geq\max\{n_{1},\widetilde{n}_{2}\}\) with_

\[\widetilde{n}_{2}:=\max\left\{4(\|\bm{W}_{*}\|_{F}^{2}+\|\bm{\Phi}\|_{F}^{2}) \cdot\frac{d+2\sqrt{d\log(m/\tau)}+2\log(m/\tau)}{\epsilon},\ 4\|\bm{\Phi}\bm{\mu_{y}}-\bm{W}_{*}\bm{\mu_{x}}\|_{2}^{2} \cdot\frac{d}{\epsilon}\right\}.\]

The proof of Theorem 3 is presented in Appendix A.3. The proof sketch mainly contains three steps: In _Step-1_, we represent the difference \(\mathcal{L}(\widehat{\bm{Z}})-\mathcal{L}(\bm{Z}_{*})\) or \(\mathcal{L}^{\bm{\Phi}}(\widehat{\bm{W}})-\mathcal{L}^{\bm{\Phi}}(\bm{W}_{*})\) as a product between matrix error (in Theorem 3) and rescaled approximation error (see Appendix A.3 for definition), i.e.,

\[\mathcal{L}(\widehat{\bm{Z}})-\mathcal{L}(\bm{Z}_{*})\ \ \text{or}\ \ \mathcal{L}^{\bm{\Phi}}(\widehat{\bm{W}})- \mathcal{L}^{\bm{\Phi}}(\bm{W}_{*})\leq\text{(matrix error)}\times\text{( rescaled approximation error)};\]

_Step-2_ controls the upper bounds for matrix error and rescaled approximation error separately, using concentration for subGuassian variables; for _Step-3_, we combine the upper bounds obtained in Step-2 and complete the proof. Based on the result of Theorem 3, ignoring the logarithm term for \(\tau\), the proposed generalization error bounds can be bounded by

\[\mathcal{L}(\widehat{\bm{Z}}) \leq\mathcal{L}(\bm{Z}_{*})+\widetilde{O}_{\tau}\left(\max\{\| \bm{Z}_{*}\|_{F}^{2},\ \|\bm{\mu_{y}}-\bm{Z}_{*}\bm{\mu_{x}}\|_{2}^{2},\ K\}\cdot\frac{d}{n}\right),\] \[\mathcal{L}^{\bm{\Phi}}(\widehat{\bm{W}}) \leq\mathcal{L}^{\bm{\Phi}}(\bm{W}_{*})+\widetilde{O}_{\tau} \left(\max\{\|\bm{W}_{*}\|_{F}^{2},\ \|\bm{\Phi}\bm{\mu_{y}}-\bm{W}_{*}\bm{\mu_{x}}\|_{2}^{2},\ \|\bm{\Phi}\|_{F}^{2}\}\cdot\frac{d}{n}\right).\]

**Remark 5**.: _To make a direct comparison between the generalization error bounds of the uncompressed and the compressed version, we further control the norms \(\|\bm{W}_{*}\|_{F}^{2},\ \|\bm{\Phi}\bm{\mu_{y}}-\bm{W}_{*}\bm{\mu_{x}}\|_{2}^{2},\| \bm{\Phi}\|_{F}^{2}\) based on additional conditions on the compressed matrix \(\bm{\Phi}\). Recall the generalization method of the compressed matrix \(\bm{\Phi}\) as mentioned in Assumption 1, we have the following event_

\[\mathcal{E}_{1}:=\left\{\bm{\Phi}\in\mathbb{R}^{m\times K}\ \left|\ \begin{array}{l}\|\bm{W}_{*}\|_{F}^{2}=\|\bm{\Phi}\bm{Z}_{*}\|_{F}^{2}\leq( 1+\delta)\|\bm{Z}_{*}\|_{F}^{2}\\ \|\bm{\Phi}\bm{\mu_{y}}-\bm{W}_{*}\bm{\mu_{x}}\|_{F}^{2}=\|\bm{\Phi}(\bm{\mu_{ y}}-\bm{Z}_{*}\bm{\mu_{x}})\|_{F}^{2}\leq(1+\delta)\|\bm{\mu_{y}}-\bm{Z}_{*}\bm{ \mu_{x}}\|_{F}^{2}\end{array}\right.\right\}\]

_holds with probability at least \(1-\tau\) due to the RIP property for a fixed matrix. Moreover, since every component \(\bm{\Phi}_{i,j}\) is i.i.d. drawn from a Gaussian distribution \(\mathcal{N}(0,1/m)\), using the concentration tail bound for chi-squared variables (See Lemma 1 in [26]), we have the following event_

\[\mathcal{E}_{2}:=\left\{\bm{\Phi}\in\mathbb{R}^{m\times K}\ \left|\ \|\bm{\Phi}\|_{F}^{2}\leq K+2\sqrt{\frac{K\log(1/\tau)}{m}}+\frac{2\log(1/\tau )}{m}\right.\right\}\]

_holds with probability at least \(1-\tau\). Conditioned on these two events \(\mathcal{E}_{1}\) and \(\mathcal{E}_{2}\), the generalization error bound of the compressed version achieves the same order (ignoring the logarithm term of \(\tau\)) as the generalization error bound of the uncompressed version. That is to say,_

\[\mathcal{L}^{\bm{\Phi}}(\widehat{\bm{W}})\leq(1+\delta)\cdot\mathcal{L}(\bm{Z}_ {*})+\widetilde{O}_{\tau}\left(\max\{\|\bm{Z}_{*}\|_{F}^{2},\ \|\bm{\mu_{y}}-\bm{Z}_{*}\bm{\mu_{x}}\|_{2}^{2},\ K\}\cdot\frac{d}{n}\right)\]

_holds with probability at least \(1-5\tau\)._

Comparing with existing results on generalization error bounds mentioned in Section 2, we would like to emphasize that Theorem 4 guarantees that the generalization error bounds maintain the order before and after compression. This result establishes on i.i.d. subGaussian samples for the SHORE model without additional regularity conditions on loss function and feasible set as required in [45]. Additionally, we obtained a \(O(Kd/n)\) generalization error bound for squared Frobenius norm loss function \(\mathcal{L}\) or \(\mathcal{L}^{\bm{\Phi}}\), which is smaller than \(O(K^{2}d/n)\) as presented in [Theorem 4, [45]].

We then give results on prediction error bounds.

**Theorem 4**.: _For any \(\delta\in(0,1)\) and any \(\tau\in(0,1/3)\), suppose the compressed matrix \(\bm{\Phi}\) follows Assumption 1 with \(m\geq O(\frac{s}{s^{2}}\log(\frac{K}{\tau}))\), and Assumption 2 holds. Given any learned regressor \(\widehat{\bm{W}}\) from training problem (2), let \((\bm{x},\bm{y})\) be a new sample drawn from the underlying distribution \(\mathcal{D}\), we have the following inequality holds with probability at least \(1-\tau\):_

\[\mathbb{E}_{\mathcal{D}}[\|\widehat{\bm{y}}-\bm{y}\|_{2}^{2}]\leq\frac{4}{1- \delta}\cdot\mathbb{E}_{\mathcal{D}}[\|\bm{\Phi}\bm{y}-\widehat{\bm{W}}\bm{x} \|_{2}^{2}],\]

_where \(\widehat{\bm{y}}\) is the optimal solution from prediction problem (3) with input vector \(\bm{x}\)._The proof of Theorem 4 is presented in Appendix A.4. Theorem 4 gives an upper bound of \(\ell_{2}\)-norm distance between \(\widehat{\bm{y}}\) and \(\bm{y}\). Since \(\|\bm{v}^{(T)}-\bm{y}\|_{2}\leq\|\bm{v}^{(T)}-\widehat{\bm{y}}\|_{2}+\|\widehat{ \bm{y}}-\bm{y}\|_{2}\), combined with Theorem 2, we have \(\mathbb{E}_{\mathcal{D}}[\|\bm{v}^{(T)}-\bm{y}\|_{2}^{2}]\leq\,O(\mathbb{E}_{ \mathcal{D}}[\|\bm{\Phi}\bm{y}-\widehat{\bm{W}}\bm{x}\|_{2}])\) when \(T\geq t_{*}\) defined in (5) (see Appendix A.5.5), where the final inequality holds due to the optimality of \(\widehat{\bm{y}}\). Hence, we achieve an upper bound of \(\ell_{2}\)-norm distance between \(\bm{v}^{(T)}\) and \(\bm{y}\) as presented in Theorem 4, see Remark 6.

**Remark 6**.: _For any \(\delta\in(0,1)\) and \(\tau\in(0,1/3)\), suppose compressed matrix \(\bm{\Phi}\) follows Assumption 1 with \(m\geq O(\frac{\kappa}{\delta^{2}}\log(\frac{K}{\delta^{2}}))\), and Assumption 2 holds, for any constant \(\epsilon>0\), the following inequality holds with probability at least \(1-3\tau\):_

\[\mathbb{E}_{\mathcal{D}}[\|\bm{v}^{(T)}-\bm{y}\|_{2}^{2}]\leq O(\mathbb{E}_{ \mathcal{D}}[\|\bm{\Phi}\bm{y}-\widehat{\bm{W}}\bm{x}\|_{2}])\leq O(\mathcal{ L}^{\bm{\Phi}}(\bm{W}_{*})+4\epsilon).\]

## 4 Numerical Experiments

In this section, we conduct numerical experiments on two types of instances (i.e., synthetic data sets and real data sets) to validate the theoretical results and illustrate both efficiency and accuracy of the proposed prediction method compared with typical existing prediction baselines, i.e., Orthogonal Matching Pursuit (OMP, [46]), Fast Iterative Shrinkage-Thresholding Algorithm (FISTA, [2]) and Elastic Net (EN, [47]). Due to the space limit, we put the implemented prediction method (Algorithm 2) in Appendix A.6.1, aforementioned existing prediction baselines in Appendix A.6.2, experiment setting details and results for real data in Appendix A.7.

**Performance measures.** Given a sample \((\bm{x},\bm{y})\) with input \(\bm{x}\) and corresponding true output \(\bm{y}\), we use \(\bm{v}\) to denote the predicted output obtained from any prediction method, and measure the numerical performances based on the following three metrics:

1. For a ground truth \(\bm{y}\) with sparsity-level \(s\), the metric _precision over selected supports_, i.e., \(\texttt{Precision}@\ =\frac{1}{s}|\texttt{supp}(\bm{v})\cap\texttt{supp}(\bm{y})|\) measures the percentage of correctly identified supports in the predicted output;
2. The metric _output difference_, i.e., \(\texttt{Output}-\texttt{diff}:=\|\bm{v}-\bm{y}\|_{2}^{2}\) measures the \(\ell_{2}\)-norm distance between the predicted output and the ground-truth;
3. For any given MOR \(\widehat{\bm{W}}\) and compressed matrix \(\bm{\Phi}\), the metric _prediction loss_, i.e., \(\texttt{Prediction}-\texttt{Loss}:=\|\bm{\Phi}\bm{v}-\widehat{\bm{W}}\bm{x}\|_ {2}^{2}\) computes the prediction loss with respect to \(\widehat{\bm{W}}\bm{x}\).

**Synthetic data generation procedure.** The synthetic data set is generated as follows: Every input \(\bm{x}^{i}\) for \(i\in[n]\) is i.i.d. drawn from a Gaussian distribution \(\mathcal{N}(\bm{\mu}_{\bm{x}},\bm{\Sigma}_{\bm{x}\bm{x}})\), where its mean vector \(\bm{\mu}_{\bm{x}}\) and covariance matrix \(\bm{\Sigma}_{\bm{x}\bm{x}}\) are selected based on the procedures given in Appendix A.6.3. For any given sparsity-level \(s\), underlying true regressor \(\bm{Z}_{*}\in\mathbb{R}^{K\times d}\), and Signal-to-Noise Ratio (SNR), the ground-truth \(\bm{y}^{i}\) (corresponding with its given input \(\bm{x}^{i}\)) is generated by \(\bm{y}^{i}=\Pi_{\mathcal{V}^{K}\cap\mathcal{F}}\left(\bm{Z}_{*}\bm{x}^{i}+\bm {\epsilon}^{i}\right)\), where \(\bm{\epsilon}^{i}\in\mathbb{R}^{K}\) is a i.i.d. random noise drawn from the Gaussian distribution \(\mathcal{N}(\bm{0}_{K},\text{SNR}^{-2}\|\bm{Z}_{*}\bm{x}^{i}\|_{\infty}\cdot \bm{I}_{K})\).

**Parameter setting.** For synthetic data, we set input dimension \(d=10^{4}\), output dimension \(K=2\times 10^{4}\), and sparsity-level \(s=3\). We generate in total \(n=3\times 10^{4}\), i.i.d. samples as described above, i.e., \(\mathcal{S}^{\texttt{syn}}:=\{(\bm{x}^{i},\bm{y}^{i})\}_{i=1}^{3\times 10^{4}}\) with \(\text{SNR}^{-1}\in\{1,0.32,0.032\}\) to ensure the signal-to-noise decibels (dB, [14]) takes values on dB \(:=10\log(\text{SNR}^{2})\in\{0,10,30\}\). We select the number of rows for compressed matrix \(\bm{\Phi}\) by \(m\in\{100,300,500,700,1000,2000\}\). For computing the empirical regressor \(\widehat{\bm{W}}\in\mathbb{R}^{m\times d}\), we first split the whole sample set \(\mathcal{S}^{\texttt{syn}}\) into two non-overlap subsets, i.e., a training set \(\mathcal{S}^{\texttt{tra}}\) with 80% and a testing set \(\mathcal{S}^{\texttt{test}}\) with rest 20%. The regressor \(\widehat{\bm{W}}\) is therefore obtained by solving compressed SHORE (2) based on the training set \(\mathcal{S}^{\texttt{tra}}\) with a randomly generated compressed matrix \(\bm{\Phi}\). For evaluating the proposed prediction method, Algorithm 2, we pick a fixed stepsize \(\eta=0.9\), \(\mathcal{F}=\mathbb{R}_{+}^{K}\), and set the maximum iteration number as \(T=60\), and run prediction methods over the set \(\mathcal{S}^{\texttt{test}}\).

**Hardware & Software.** All experiments are conducted in Dell workstation Precision 7920 with a 3GHz 48Cores Intel Xeon CPU and 128GB 2934MHz DDR4 Memory. The proposed method and other methods are solved using PyTorch version 2.3.0 and scikit-learn version 1.4.2 in Python 3.12.3.

**Numerical Results & Discussions.** The results are demonstrated in Figure 1, which does not include the results from the Elastic Net and OMP due to relatively much longer running time.

Based on Figure 1, we observe that the proposed algorithm enjoys a better computational cost and accuracy on most metrics. The running time for the proposed algorithm and baselines are reported in Table 2 (see in Appendix A.7), which further demonstrates the efficiency of the proposed algorithm. The implemented code could be found on Github https://github.com/from-ryan/Solving_SHORE_via_compression.

## 5 Conclusion and Future Directions

In conclusion, we propose a two-stage framework to solve Sparse & High-dimensional-Output REgression (SHORE) problem, the computational and statistical results indicate that the proposed framework is computationally scalable, maintaining the same order of both the training loss and prediction loss before and after compression under relatively weak sample set conditions, especially in the sparse and high-dimensional-output setting where the input dimension is polynomially smaller compared to the output dimension. In numerical experiments, SHORE provides improved optimization performance over existing MOR methods, for both synthetic data and real data.

We close with some potential questions for future investigation. The first is to extend our theoretical results to nonlinear/nonconvex SHORE frameworks [24]. The second direction is to improve existing variable reduction methods for better scalability while maintaining small sacrificing on prediction accuracy, e.g., new design and analysis on randomized projection matrices. The third direction is to explore general scenarios when high dimensional outputs enjoys additional geometric structures [30] from real applications in machine learning or operations management other than \(s\)-sparsity and its variants as discussed in the paper. Taking our result for SHORE as an initial start, we expect a stronger follow-up work that applies to MOR with additional structures, which eventually benefits the learning community in both practice and theory.

Figure 1: **Numerical results on synthetic data. In short, each dot in the figure represents the average value of 10 _independent trials_ (i.e., experiments) of compressed matrices \(\mathbf{\Phi}^{(1)},\ldots,\mathbf{\Phi}^{(10)}\) on a given tuple of parameters \((K,d,n,\text{SNR},m)\). The shaded parts represent the empirical standard deviations over 10 trials. In the first row, we plot the ratio of training loss after and before compression, i.e., \(\|\mathbf{\Phi}\boldsymbol{Y}-\widetilde{\boldsymbol{W}}\boldsymbol{X}\|_{F}^ {2}/\|\boldsymbol{Y}-\widetilde{\boldsymbol{Z}}\boldsymbol{X}\|_{F}^{2}\) versus the number of rows \(m\). It is obvious that the ratio converges to one as \(m\) increases, which validates the result presented in Theorem 1. In the second row, we plot percision@3 versus the number of rows. As we can observe, the proposed algorithm outperforms CD and FISTA.**

## Acknowledgement

Renyuan Li and Guanyi Wang were supported by the National University of Singapore under AcRF Tier-1 grant (A-8000607-00-00) 22-5539-A0001. Zhehui Chen would like to thank Google for its support in providing the research environment and supportive community that made this work possible.

## References

* [1] Z. Abraham, P.-N. Tan, P. Jin, J. Winkler, S. Zhong, and M. Liszewska. Position preserving multi-output prediction. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part II 13_, pages 320-335. Springer, 2013.
* [2] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. _SIAM journal on imaging sciences_, 2(1):183-202, 2009.
* [3] D. Bertsimas and B. Van Parys. Sparse high-dimensional regression: Exact scalable algorithms and phase transitions (2017). _arXiv preprint arXiv:1709.10029_, 2019.
* [4] D. Bertsimas and B. Van Parys. Sparse high-dimensional regression. _The Annals of Statistics_, 48(1):300-323, 2020.
* [5] K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal, Y. Prabhu, and M. Varma. The extreme classification repository: Multi-label datasets and code, 2016.
* [6] T. Blumensath and M. E. Davies. Iterative hard thresholding for compressed sensing. _Applied and computational harmonic analysis_, 27(3):265-274, 2009.
* [7] H. Boche, R. Calderbank, G. Kutyniok, and J. Vybiral. _Compressed sensing and its applications: MATHEON workshop 2013_. Springer, 2015.
* [8] H. Borchani, G. Varando, C. Bielza, and P. Larranaga. A survey on multi-output regression. _Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery_, 5(5):216-233, 2015.
* [9] L. Breiman. Bagging predictors. _Machine learning_, 24:123-140, 1996.
* [10] E. J. Candes and T. Tao. Decoding by linear programming. _IEEE transactions on information theory_, 51(12):4203-4215, 2005.
* [11] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, et al. A survey on evaluation of large language models. _ACM Transactions on Intelligent Systems and Technology_, 15(3):1-45, 2024.
* [12] Y.-N. Chen and H.-T. Lin. Feature-aware label space dimension reduction for multi-label classification. _Advances in neural information processing systems_, 25, 2012.
* [13] M. M. Cisse, N. Usunier, T. Artieres, and P. Gallinari. Robust bloom filters for large multilabel classification tasks. _Advances in neural information processing systems_, 26, 2013.
* [14] S. S. Dey, G. Wang, and Y. Xie. Approximation algorithms for training one-node relu neural networks. _IEEE Transactions on Signal Processing_, 68:6696-6706, 2020.
* [15] A. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems. _Technometrics_, 12(1):55-67, 1970.
* [16] O. Honovich, R. Aharoni, J. Herzig, H. Taitelbaum, D. Kukliansy, V. Cohen, T. Scialom, I. Szpektor, A. Hassidim, and Y. Matias. True: Re-evaluating factual consistency evaluation. _arXiv preprint arXiv:2204.04991_, 2022.
* [17] D. Hsu, S. M. Kakade, J. Langford, and T. Zhang. Multi-label prediction via compressed sensing, 2009.

* [18] D. Hsu, S. M. Kakade, and T. Zhang. An analysis of random design linear regression. _arXiv preprint arXiv:1106.2363_, 6, 2011.
* [19] J. C. Hull and S. Basu. _Options, futures, and other derivatives_. Pearson Education India, 2016.
* [20] I. M. Jacobs and J. Wozencraft. Principles of communication engineering. 1965.
* [21] P. Jain, A. Tewari, and P. Kar. On iterative hard thresholding methods for high-dimensional m-estimation. _Advances in neural information processing systems_, 27, 2014.
* [22] S. Jansen. _Machine Learning for Algorithmic Trading: Predictive models to extract signals from market and alternative data for systematic trading strategies with Python_. Packt Publishing Ltd, 2020.
* [23] K. Jasinska and N. Karampatziakis. Log-time and log-space extreme classification. _arXiv preprint arXiv:1611.01964_, 2016.
* [24] A. Kapoor, R. Viswanathan, and P. Jain. Multilabel classification using bayesian compressed sensing. _Advances in neural information processing systems_, 25, 2012.
* 1117, 2012.
* [26] B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. _Annals of statistics_, pages 1302-1338, 2000.
* [27] T. Levy and F. Abramovich. Generalization error bounds for multiclass sparse linear classifiers. _Journal of Machine Learning Research_, 24(151):1-35, 2023.
* [28] S. Li and Y. Liu. Towards sharper generalization bounds for structured prediction. _Advances in Neural Information Processing Systems_, 34:26844-26857, 2021.
* [29] L. Liu, Y. Shen, T. Li, and C. Caramanis. High dimensional robust sparse regression. In _International Conference on Artificial Intelligence and Statistics_, pages 411-421. PMLR, 2020.
* [30] S. Ludwig et al. _Algorithms above the noise floor_. PhD thesis, Massachusetts Institute of Technology, 2018.
* [31] Y. Ma, R. Han, and W. Wang. Portfolio optimization with return prediction using deep learning and machine learning. _Expert Systems with Applications_, 165:113973, 2021.
* [32] T. K. R. Medini, Q. Huang, Y. Wang, V. Mohan, and A. Shrivastava. Extreme classification in log memory using count-min sketch: A case study of amazon search with 50m products. _Advances in Neural Information Processing Systems_, 32, 2019.
* [33] M. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. Wiele, V. Mnih, N. Heess, and J. T. Springenberg. Learning by playing solving sparse reward tasks from scratch. In _International conference on machine learning_, pages 4344-4353. PMLR, 2018.
* [34] A. Roberts, C. Raffel, and N. Shazeer. How much knowledge can you pack into the parameters of a language model? _arXiv preprint arXiv:2002.08910_, 2020.
* [35] M. Sanchez-Fernandez, M. de Prado-Cumplido, J. Arenas-Garcia, and F. Perez-Cruz. Svm multiregression for nonlinear channel estimation in multiple-input multiple-output systems. _IEEE transactions on signal processing_, 52(8):2298-2307, 2004.
* [36] T. Simila and J. Tikka. Input selection and shrinkage in multiresponse linear regression. _Computational Statistics & Data Analysis_, 52(1):406-422, 2007.
* [37] E. Spyromitros-Xioufis, G. Tsoumakas, W. Groves, and I. Vlahavas. Multi-label classification methods for multi-target regression. _arXiv preprint arXiv:1211.6581_, pages 1159-1168, 2012.
* [38] Q. Sun, H. Zhu, Y. Liu, and J. G. Ibrahim. Sprem: sparse projection regression model for high-dimensional linear regression. _Journal of the American Statistical Association_, 110(509):289-302, 2015.

* [39] F. Tai and H.-T. Lin. Multilabel classification with principal label space transformation. _Neural Computation_, 24(9):2508-2542, 2012.
* [40] D. Tuia, J. Verrelst, L. Alonso, F. Perez-Cruz, and G. Camps-Valls. Multioutput support vector regression for remote sensing biophysical parameter estimation. _IEEE Geoscience and Remote Sensing Letters_, 8(4):804-808, 2011.
* [41] M. J. Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge university press, 2019.
* [42] A. Watkins, E. Ullah, T. Nguyen-Tang, and R. Arora. Optimistic rates for multi-task representation learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [43] B. J. William and J. Lindenstrauss. Extensions of lipschitz mapping into hilbert space. _Contemporary mathematics_, 26(189-206):323, 1984.
* [44] D. Xu, Y. Shi, I. W. Tsang, Y.-S. Ong, C. Gong, and X. Shen. Survey on multi-output learning. _IEEE transactions on neural networks and learning systems_, 31(7):2409-2429, 2019.
* [45] H.-F. Yu, P. Jain, P. Kar, and I. Dhillon. Large-scale multi-label learning with missing labels. In _International conference on machine learning_, pages 593-601. PMLR, 2014.
* [46] Z. Zhang. Matching pursuits with time-frequency dictionaries. _IEEE Transactions on signal processing_, 41(12):3397-3415, 1993.
* [47] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 67(2):301-320, 2005.

Appendix / supplemental material

### Proof of Theorem 1

Recall the following theorem in the main text.

Theorem 1.For any \(\delta\in(0,1)\) and \(\tau\in(0,1)\), suppose the compressed matrix \(\mathbf{\Phi}\) follows Assumption 1 with \(m\geq O(\frac{1}{\delta^{2}}\cdot\log(\frac{K}{\tau}))\). We have the following inequality for training loss

\[\|\mathbf{\Phi}\bm{Y}-\widehat{\bm{W}}\bm{X}\|_{F}^{2}\leq(1+\delta)\cdot\| \bm{Y}-\widehat{\bm{Z}}\bm{X}\|_{F}^{2}\]

holds with probability at least \(1-\tau\), where \(\widehat{\bm{Z}},\widehat{\bm{W}}\) are optimal solutions for the uncompressed (1) and compressed SHORE (2), respectively.

Proof.: Given a set of \(n\) samples \(\{(\bm{x}^{i},\bm{y}^{i})\}_{i=1}^{n}\), and a matrix \(\mathbf{\Phi}\) generated from Assumption 1, we have

\[\|\mathbf{\Phi}\bm{Y}-\widehat{\bm{W}}\bm{X}\|_{F}^{2}\leq\|\mathbf{\Phi}\bm{ Y}-\mathbf{\Phi}\widehat{\bm{Z}}\bm{X}\|_{F}^{2}=\|\mathbf{\Phi}(\bm{Y}- \widehat{\bm{Z}}\bm{X})\|_{F}^{2}\,\]

where the inequality holds due to the optimality of \(\widehat{\bm{W}}\). Let \(\bm{Y}^{\prime}=\bm{Y}-\widehat{\bm{Z}}\bm{X}\). Consider the singular value decomposition: \(\bm{Y}^{\prime}=\bm{U}_{\bm{Y}^{\prime}}\bm{\Sigma}_{\bm{Y}^{\prime}}\bm{V}_{ \bm{Y}^{\prime}}^{\top}\), and then we have

\[\|\mathbf{\Phi}(\bm{Y}-\widehat{\bm{Z}}\bm{X})\|_{F}^{2}=\|\mathbf{\Phi}\bm{Y }^{\prime}\|_{F}^{2}=\|\mathbf{\Phi}\bm{U}_{\bm{Y}^{\prime}}\bm{\Sigma}_{\bm{Y }^{\prime}}\bm{V}_{\bm{Y}^{\prime}}^{\top}\|_{F}^{2}=\|\mathbf{\Phi}\bm{U}_{ \bm{Y}^{\prime}}\bm{\Sigma}_{\bm{Y}^{\prime}}\|_{F}^{2}.\]

Set \(\tilde{\mathbf{\Phi}}:=\mathbf{\Phi}\bm{U}_{\bm{Y}^{\prime}}\). Since the generalization method for \(\mathbf{\Phi}\) ensures its \((1,\delta)\)-RIP with probability \(1-\tau\) (from the Johnson-Lindenstrauss Lemma), and \(\bm{U}_{\bm{Y}^{\prime}}\) is a real unitary matrix, then Lemma 1 for _unitary invariant_ shows that \(\tilde{\mathbf{\Phi}}\) is also \((1,\delta)\)-RIP with probability \(1-\tau\). Now, using \(\tilde{\mathbf{\Phi}}\) is \((1,\delta)\)-RIP and all columns in \(\bm{\Sigma}_{\bm{Y}^{\prime}}\) has at most one non-zero component, we have

\[\|\mathbf{\Phi}\bm{U}_{\bm{Y}^{\prime}}\bm{\Sigma}_{\bm{Y}^{\prime}}\|_{F}^{ 2}=\|\tilde{\mathbf{\Phi}}\bm{\Sigma}_{\bm{Y}^{\prime}}\|_{F}^{2}\leq(1+\delta )\|\bm{\Sigma}_{\bm{Y}^{\prime}}\|_{F}^{2}=(1+\delta)\|\bm{Y}^{\prime}\|_{F}^ {2}.\]

Combining the above inequalities together implies

\[\|\mathbf{\Phi}\bm{Y}-\widehat{\bm{W}}\bm{X}\|_{F}^{2}\leq(1+\delta)\|\bm{Y}^ {\prime}\|_{F}^{2}=(1+\delta)\cdot\|\bm{Y}-\widehat{\bm{Z}}\bm{X}\|_{F}^{2},\]

which completes the proof. 

### Proof of Theorem 2

Recall the following theorem in the main text.

Theorem 2.For any \(\delta\in(0,1)\) and \(\tau\in(0,1)\), suppose the compressed matrix \(\mathbf{\Phi}\) follows Assumption 1 with \(m\geq O(\frac{s}{\delta^{2}}\log(\frac{K}{\tau}))\). With a fixed stepsize \(\eta\in(\frac{1}{2-2\delta},1)\), the following inequality

\[\|\widehat{\bm{y}}-\bm{v}^{(t)}\|_{2}\leq c_{1}^{t}\cdot\|\widehat{\bm{y}}- \bm{v}^{(0)}\|_{2}+\frac{c_{2}}{1-c_{1}}\cdot\|\mathbf{\Phi}\widehat{\bm{y}}- \widehat{\bm{W}}\bm{x}\|_{2}\]

holds for all \(t\in[T]\) simultaneously with probability at least \(1-\tau\), where \(c_{1}:=2-2\eta+2\eta\delta<1\) is some positive constant strictly smaller than 1, and \(c_{2}:=2\eta\sqrt{1+\delta}\) is some constant.

Proof.: Suppose Assumption 1 holds, then the randomized compressed matrix \(\mathbf{\Phi}\) ensures \((3s,\delta)\)-RIP with probability at least \(1-\tau\) (see Remark 3). Thus, to complete the proof, it is sufficient to show the above inequality holds for all \(t\in[T]\) under such a compressed matrix \(\mathbf{\Phi}\) with \((3s,\delta)\)-RIP. We conclude that the proof is, in general, separated into three steps.

Step-1._We establish an upper bound on the \(\ell_{2}\)-norm distance between the current point and the optimal solution._ Due to the optimality of the projection (step-4 in Algorithm 1), we have the following inequality

\[\|\widetilde{\bm{v}}^{(t+1)}-\bm{v}^{(t+1)}\|_{2}^{2}\leq\|\widetilde{\bm{v}}^{ (t+1)}-\bm{v}\|_{2}^{2}\]

holds for all \(\bm{v}\in\mathcal{V}_{s}^{K}\cap\mathcal{F}\), which further implies that

\[\|\widetilde{\bm{v}}^{(t+1)}-\bm{v}+\bm{v}-\bm{v}^{(t+1)}\|_{2}^{2}\leq\| \widetilde{\bm{v}}^{(t+1)}-\bm{v}\|_{2}^{2}\]

\[\Leftrightarrow \|\bm{v}-\bm{v}^{(t+1)}\|_{2}^{2}\leq 2\langle\bm{v}-\widetilde{\bm{v}}^{ (t+1)},\bm{v}-\bm{v}^{(t+1)}\rangle\]

holds for all \(\bm{v}\in\mathcal{V}_{s}^{K}\cap\mathcal{F}\).

Step-2._We show one-iteration improvement based on the above inequality._ Since \(\widehat{\bm{y}}\in\mathcal{V}_{s}^{K}\cap\mathcal{F}\), we can replace \(\bm{v}\) by \(\widehat{\bm{y}}\), which still ensures the above inequality. Set \(\bm{\Delta}^{(t)}:=\widehat{\bm{y}}-\bm{v}^{(t)}\) for all \(t\in[T]\). Based on the updating rule (step-3 in Algorithm 1), \(\widetilde{\bm{v}}^{(t+1)}=\bm{v}^{(t)}-\eta\cdot\bm{\Phi}^{\top}(\bm{\Phi}\bm {v}^{(t)}-\widehat{\bm{W}}\bm{x})\). Thus, the above inequality can be written as

\[\|\bm{\Delta}^{(t+1)}\|_{2}^{2} \leq 2\langle\widehat{\bm{y}}-\bm{v}^{(t)}+\eta\cdot\bm{\Phi}^{ \top}(\bm{\Phi}\bm{v}^{(t)}-\widehat{\bm{W}}\bm{x}),\bm{\Delta}^{(t+1)}\rangle\] \[=2\langle\bm{\Delta}^{(t)},\bm{\Delta}^{(t+1)}\rangle+2\eta \langle\bm{\Phi}\bm{v}^{(t)}-\widehat{\bm{W}}\bm{x},\bm{\Phi}\bm{\Delta}^{(t+1 )}\rangle\] \[=2\langle\bm{\Delta}^{(t)},\bm{\Delta}^{(t+1)}\rangle-2\eta \langle\bm{\Phi}\bm{\Delta}^{(t)},\bm{\Phi}\bm{\Delta}^{(t+1)}\rangle+2\eta \langle\bm{\Phi}\widehat{\bm{y}}-\widehat{\bm{W}}\bm{x},\bm{\Phi}\bm{\Delta}^{ (t+1)}\rangle.\] (6)

Using Lemma 2 with

\[\frac{\bm{\Delta}^{(t)}}{\|\bm{\Delta}^{(t)}\|_{2}},\quad\frac{\bm{\Delta}^{( t+1)}}{\|\bm{\Delta}^{(t+1)}\|_{2}},\quad\frac{\bm{\Delta}^{(t)}}{\|\bm{ \Delta}^{(t)}\|_{2}}+\frac{\bm{\Delta}^{(t+1)}}{\|\bm{\Delta}^{(t+1)}\|_{2}}, \quad\frac{\bm{\Delta}^{(t)}}{\|\bm{\Delta}^{(t)}\|_{2}}-\frac{\bm{\Delta}^ {(t+1)}}{\|\bm{\Delta}^{(t+1)}\|_{2}}\]

all \(3s\)-sparse vectors, and \(\bm{\Phi}\) a \((3s,\delta)\)-RIP matrix, we have

\[-2\eta\left\langle\bm{\Phi}\frac{\bm{\Delta}^{(t)}}{\|\bm{\Delta}^{(t)}\|_{2} },\bm{\Phi}\frac{\bm{\Delta}^{(t+1)}}{\|\bm{\Delta}^{(t+1)}\|_{2}}\right\rangle \leq 2\delta\eta-2\eta\left\langle\frac{\bm{\Delta}^{(t)}}{\|\bm{ \Delta}^{(t)}\|_{2}},\frac{\bm{\Delta}^{(t+1)}}{\|\bm{\Delta}^{(t+1)}\|_{2}} \right\rangle,\]

which implies

\[-2\eta\langle\bm{\Phi}\bm{\Delta}^{(t)},\bm{\Phi}\bm{\Delta}^{(t+1)}\rangle \leq 2\delta\eta\|\bm{\Delta}^{(t)}\|_{2}\|\bm{\Delta}^{(t+1)}\|_{2}-2 \eta\langle\bm{\Delta}^{(t)},\bm{\Delta}^{(t+1)}\rangle.\]

Inserting the above result into inequality (6) gives

\[\|\bm{\Delta}^{(t+1)}\|_{2}^{2} \leq(2-2\eta)\langle\bm{\Delta}^{(t)},\bm{\Delta}^{(t+1)}\rangle+ 2\delta\eta\|\bm{\Delta}^{(t)}\|_{2}\|\bm{\Delta}^{(t+1)}\|_{2}+2\eta\langle \bm{\Phi}\widehat{\bm{y}}-\widehat{\bm{W}}\bm{x},\bm{\Phi}\bm{\Delta}^{(t+1)}\rangle\] \[\overset{(\text{i)}}{\leq}(2-2\eta+2\eta\delta)\|\bm{\Delta}^{(t) }\|_{2}\|\bm{\Delta}^{(t+1)}\|_{2}+2\eta\|\bm{\Phi}\widehat{\bm{y}}-\widehat{ \bm{W}}\bm{x}\|_{2}\|\bm{\Phi}\bm{\Delta}^{(t+1)}\|_{2}\] \[\leq(2-2\eta+2\eta\delta)\|\bm{\Delta}^{(t)}\|_{2}\|\bm{\Delta}^{( t+1)}\|_{2}+2\eta\sqrt{1+\delta}\|\bm{\Phi}\widehat{\bm{y}}-\widehat{\bm{W}}\bm{x}\|_{2}\| \bm{\Delta}^{(t+1)}\|_{2},\]

where the above inequality (i) requests \(\eta<1\) to ensure the inequality \((2-2\eta)\langle\bm{\Delta}^{(t)},\bm{\Delta}^{(t+1)}\rangle\leq(2-2\eta)\|\bm{ \Delta}^{(t)}\|_{2}\|\bm{\Delta}^{(t+1)}\|_{2}\) holds. Therefore, dividing \(\|\bm{\Delta}^{(t+1)}\|_{2}\) on both side implies

\[\|\bm{\Delta}^{(t+1)}\|_{2}\leq(2-2\eta+2\eta\delta)\|\bm{\Delta}^{(t)}\|_{2}+2 \eta\sqrt{1+\delta}\|\bm{\Phi}\widehat{\bm{y}}-\widehat{\bm{W}}\bm{x}\|_{2},\] (7)

which gives the one-step improvement.

Step-3._Combine everything together._ To ensure contractions in every iteration, we pick stepsize \(\eta\) such that \(2-2\eta+2\eta\delta\in(0,1)\) with \(\eta<1\), which gives \(\eta\in\left(\frac{1}{2(1-\delta)},1\right)\). Using the above inequality (7) for one-step improvement, we have

\[\|\widehat{\bm{y}}-\bm{v}^{(t)}\|_{2}\leq(2-2\eta+2\eta\delta)^{t}\cdot\| \widehat{\bm{y}}-\bm{v}^{(0)}\|_{2}+\frac{2\eta\sqrt{1+\delta}}{2\eta-2\eta \delta-1}\|\bm{\Phi}\widehat{\bm{y}}-\widehat{\bm{W}}\bm{x}\|_{2},\]

which completes the proof. 

### Proof of Theorem 3

Recall the following theorem in the main text.

Theorem 3.For any \(\delta\in(0,1)\) and \(\tau\in(0,\frac{1}{3})\), suppose compressed matrix \(\bm{\Phi}\) follows Assumption 1 with \(m\geq O(\frac{s}{\delta^{2}}\log(\frac{K}{\tau}))\), and Assumption 2 holds, for any constant \(\epsilon>0\), the following results hold:

(Matrix Error). The inequality for matrix error \(\left\|\bm{M}_{\bm{xx}}^{1/2}\widehat{\bm{M}}_{\bm{xx}}^{-1}\bm{M}_{\bm{xx}}^{1/2 }\right\|_{\text{op}}\leq 4\) holds with probability at least \(1-2\tau\) as the number of samples \(n\geq n_{1}\) with

\[n_{1}:=\,\max\left\{\frac{64C^{2}\sigma^{4}}{9\lambda_{\min}^{2}(\bm{M}_{\bm{xx}})} \left(d+\log(2/\tau)\right),\;\frac{32^{2}\|\bm{\mu}_{\bm{x}}\|_{2}^{2}\sigma^{2 }}{\lambda_{\min}^{2}(\bm{M}_{\bm{xx}})}\left(2\sqrt{d}+\sqrt{\log(1/\tau)} \right)^{2}\right\},\]where \(C\) is some fixed positive constant used in matrix concentration inequality of operator norm. (Uncompressed). The generalization error bound for uncompressed SHORE satisfies \(\mathcal{L}(\widehat{\bm{Z}})\leq\mathcal{L}(\bm{Z}_{*})+4\epsilon\) with probability at least \(1-3\tau\), as the number of samples \(n\geq\max\{n_{1},n_{2}\}\) with

\[n_{2}:=\max\left\{4(\|\bm{Z}_{*}\|_{F}^{2}+K)\cdot\frac{d+2\sqrt{d\log(K/\tau)} +2\log(K/\tau)}{\epsilon},\;4\|\bm{\mu_{y}}-\bm{Z}_{*}\bm{\mu}_{\bm{x}}\|_{2}^{ 2}\cdot\frac{d}{\epsilon}\right\}.\]

(Compressed). The generalization error bound for the compressed SHORE satisfies \(\mathcal{L}^{\bm{\Phi}}(\widehat{\bm{W}})\leq\mathcal{L}^{\bm{\Phi}}(\bm{W}_ {*})+4\epsilon\) with probability at least \(1-3\tau\), as the number of sample \(n\geq\max\{n_{1},\widetilde{n}_{2}\}\) with

\[\widetilde{n}_{2}:=\max\left\{4(\|\bm{W}_{*}\|_{F}^{2}+\|\bm{\Phi}\|_{F}^{2}) \cdot\frac{d+2\sqrt{d\log(m/\tau)}+2\log(m/\tau)}{\epsilon},\;4\|\bm{\Phi}\bm{ \mu_{y}}-\bm{W}_{*}\bm{\mu_{x}}\|_{2}^{2}\cdot\frac{d}{\epsilon}\right\}.\]

Proof.: _Let us start with the uncompressed version._

**Step-1.** Note that the optimal solutions for population loss and empirical loss are

\[\bm{Z}_{*}=\bm{M_{yx}}\bm{M}_{\bm{xx}}^{-1}\;\;\;\text{and}\;\;\;\widehat{\bm{ Z}}=\frac{\bm{Y}\bm{X}^{\top}}{n}\left(\frac{\bm{X}\bm{X}^{\top}}{n}\right)^{-1}=: \widehat{\bm{M}}_{\bm{yx}}\widehat{\bm{M}}_{\bm{xx}}^{-1},\]

respectively. Thus, the generalization error bound is

\[\mathcal{L}(\widehat{\bm{Z}})-\mathcal{L}(\bm{Z}_{*})=\|\widehat{\bm{Z}}-\bm{ Z}_{*}\|_{\bm{M}_{\bm{xx}}}^{2}=\|(\widehat{\bm{Z}}-\bm{Z}_{*})\bm{M}_{\bm{ xx}}^{1/2}\|_{F}^{2}.\]

Note that

\[(\widehat{\bm{Z}}-\bm{Z}_{*})\bm{M}_{\bm{xx}}^{1/2} =\left(\widehat{\bm{M}}_{\bm{yx}}\widehat{\bm{M}}_{\bm{xx}}^{-1}- \bm{M}_{\bm{yx}}\bm{M}_{\bm{xx}}^{-1}\right)\bm{M}_{\bm{xx}}^{1/2}\] \[=\left(\widehat{\bm{M}}_{\bm{yx}}-\bm{M}_{\bm{yx}}\bm{M}_{\bm{xx }}^{-1}\widehat{\bm{M}}_{\bm{xx}}\right)\widehat{\bm{M}}_{\bm{xx}}^{-1}\bm{M} _{\bm{xx}}^{1/2}\] \[=\widehat{\mathbb{E}}[\bm{yx}^{\top}-\bm{Z}_{*}\bm{xx}^{\top}] \widehat{\bm{M}}_{\bm{xx}}^{-1}\bm{M}_{\bm{xx}}^{1/2}\] \[=\widehat{\mathbb{E}}[(\bm{y}-\bm{Z}_{*}\bm{x})\bm{x}^{\top} \widehat{\bm{M}}_{\bm{xx}}^{-1/2}]\widehat{\bm{M}}_{\bm{xx}}^{-1/2}\bm{M}_{ \bm{xx}}^{1/2},\]

where we use \(\widehat{\mathbb{E}}[\cdot]\) to denote the empirical distribution. Then, the above generalization error bound can be upper-bounded as follows

\[\left\|(\widehat{\bm{Z}}-\bm{Z}_{*})\bm{M}_{\bm{xx}}^{1/2}\right\| _{F} =\left\|\widehat{\mathbb{E}}[(\bm{y}-\bm{Z}_{*}\bm{x})\bm{x}^{\top }\widehat{\bm{M}}_{\bm{xx}}^{-1/2}]\widehat{\bm{M}}_{\bm{xx}}^{-1/2}\bm{M}_{ \bm{xx}}^{1/2}\right\|_{F}\] \[\leq\underbrace{\left\|\widehat{\mathbb{E}}[(\bm{y}-\bm{Z}_{*}\bm{ x})\bm{x}^{\top}\widehat{\bm{M}}_{\bm{xx}}^{-1/2}]\right\|_{F}}_{\text{rescaled approximation error}}\underbrace{\left\|\widehat{\bm{M}}_{\bm{xx}}^{-1/2}\bm{M}_{ \bm{xx}}^{1/2}\right\|_{\text{op}}}_{\text{matrix error}}.\]

**Step-2.** Next, we provide upper bounds on these two terms \(\left\|\widehat{\mathbb{E}}[(\bm{y}-\bm{Z}_{*}\bm{x})\bm{x}^{\top}\widehat{ \bm{M}}_{\bm{xx}}^{-1/2}]\right\|_{F}\) and \(\left\|\widehat{\bm{M}}_{\bm{xx}}^{-1/2}\bm{M}_{\bm{xx}}^{1/2}\right\|_{\text {op}}\) in the right-hand-side separately.

_For matrix error term \(\left\|\widehat{\bm{M}}_{\bm{xx}}^{-1/2}\bm{M}_{\bm{xx}}^{1/2}\right\|_{\text {op}}\), we have_

\[\left\|\widehat{\bm{M}}_{\bm{xx}}^{-1/2}\bm{M}_{\bm{xx}}^{1/2}\right\|_{\text{op }}^{2}=\left\|\bm{M}_{\bm{xx}}^{1/2}\widehat{\bm{M}}_{\bm{xx}}^{-1}\bm{M}_{ \bm{xx}}^{1/2}\right\|_{\text{op}}.\]

_Due to Assumption 2, the centralized feature vector \(\bm{\xi}_{\bm{x}}:=\bm{x}-\bm{\mu}_{\bm{x}}\) ensures the following inequality_

\[\mathbb{E}_{\mathcal{D}}\left[\exp\left(\lambda\bm{v}^{\top}\bm{\xi}_{\bm{x}} \right)\right]\leq\exp\left(\frac{\lambda^{2}\|\bm{v}\|_{2}^{2}\sigma^{2}}{2}\right)\]

_for all \(\bm{v}\in\mathbb{R}^{d}\). Consider the empirical second moment of \(\bm{x}\),_

\[\widehat{\bm{M}}_{\bm{xx}}=\sum_{i=1}^{n}\frac{\bm{x}^{i}(\bm{x}^{i})^{\top}}{n} =\sum_{i=1}^{n}\frac{\bm{\xi}_{\bm{x}}^{i}(\bm{\xi}_{\bm{x}}^{i})^{\top}}{n}+ \bm{\mu}_{\bm{x}}\left(\sum_{i=1}^{n}\frac{\bm{\xi}_{\bm{x}}^{i}}{n}\right)^{ \top}+\left(\sum_{i=1}^{n}\frac{\bm{\xi}_{\bm{x}}^{i}}{n}\right)\bm{\mu}_{\bm{x }}^{\top}+\bm{\mu}_{\bm{x}}\bm{\mu}_{\bm{x}}^{\top}.\]Since \(\bm{\xi}_{x}^{1},\ldots,\bm{\xi}_{n}^{n}\) are i.i.d. \(\sigma^{2}\)-subGaussian random vector with zero mean and covariance matrix \(\bm{\Sigma}_{\bm{x}\bm{x}}\), then based on Lemma 3, there exists a positive constant \(C\) such that for any \(\tau\in(0,1)\),

\[\mathbb{P}_{\mathcal{D}}\left(\left\|\sum_{i=1}^{n}\frac{\bm{\xi}_{x}^{i}(\bm{ \xi}_{x}^{i})^{\top}}{n}-\bm{\Sigma}_{\bm{x}\bm{x}}\right\|_{\text{op}}\leq C \sigma^{2}\max\left\{\sqrt{\frac{d+\log(2/\tau)}{n}},\frac{d+\log(2/\tau)}{n} \right\}\right)\geq 1-\tau,\]

and based on Lemma 4, for any \(\tau\in(0,1)\),

\[\mathbb{P}_{\mathcal{D}}\left(\left\|\sum_{i=1}^{n}\frac{\bm{\xi}_{x}^{i}}{n} \right\|_{2}\leq\frac{4\sigma\sqrt{d}+2\sigma\sqrt{\log(1/\tau)}}{\sqrt{n}} \right)\geq 1-\tau.\]

Let \(\bm{\Delta}_{\bm{x}\bm{x}}:=\sum_{i=1}^{n}\frac{\bm{\xi}_{x}^{i}(\bm{\xi}_{x}^ {i})^{\top}}{n}-\bm{\Sigma}_{\bm{x}\bm{x}}\) and \(\bar{\bm{\xi}}:=\sum_{i=1}^{n}\frac{\bm{\xi}_{x}^{i}}{n}\), then \(\widehat{\bm{M}}_{\bm{x}\bm{x}}\) can be represented by

\[\widehat{\bm{M}}_{\bm{x}\bm{x}}=\underbrace{\bm{\Sigma}_{\bm{x}\bm{x}}+\bm{ \mu}_{\bm{x}}\bm{\mu}_{\bm{x}}^{\top}}_{=:\widehat{\bm{M}}_{\bm{x}\bm{x}}}+\bm {\Delta}_{\bm{x}\bm{x}}+\bm{\mu}_{\bm{x}}\bar{\bm{\xi}}^{\top}+\bar{\bm{\xi}} \bm{\mu}_{\bm{x}}^{\top},\]

and thus we have. Then the minimum eigenvalue of \(\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}\widehat{\bm{M}}_{\bm{x}\bm{x}}\widehat{ \bm{M}}_{\bm{x}\bm{x}}\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}\) can be lower bounded as follows

\[\lambda_{\min}\left(\bm{M}_{\bm{x}\bm{x}}^{-1/2}\widehat{\bm{M}} _{\bm{x}\bm{x}}\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}\right)\] \[\geq 1-\left\|\bm{M}_{\bm{x}\bm{x}}^{-1/2}\left(\bm{\Delta}_{\bm{x }\bm{x}}+\bm{\mu}_{\bm{x}}\bm{\xi}^{\top}+\bar{\bm{\xi}}\bm{\mu}_{\bm{x}}^{ \top}\right)\bm{M}_{\bm{x}\bm{x}}^{-1/2}\right\|_{\text{op}}\] \[\geq 1-\left\|\bm{M}_{\bm{x}\bm{x}}^{-1/2}\bm{\Delta}_{\bm{x}\bm{x }}\bm{M}_{\bm{x}\bm{x}}^{-1/2}\right\|_{\text{op}}-\left\|\bm{M}_{\bm{x}\bm{x}} ^{-1/2}\left(\bm{\mu}_{\bm{x}}\bar{\bm{\xi}}^{\top}+\bar{\bm{\xi}}\bm{\mu}_{ \bm{x}}^{\top}\right)\bm{M}_{\bm{x}\bm{x}}^{-1/2}\right\|_{\text{op}}\] \[\geq 1-\frac{C\sigma^{2}}{\lambda_{\min}(\bm{M}_{\bm{x}\bm{x}})} \sqrt{\frac{d+\log(2/\tau)}{n}}-\frac{2\|\bm{\mu}_{\bm{x}}\|_{2}}{\lambda_{\min }(\bm{M}_{\bm{x}\bm{x}})}\frac{4\sigma\sqrt{d}+2\sigma\sqrt{\log(1/\tau)}}{ \sqrt{n}},\]

where the final inequality holds with probability at least \(1-2\tau\) by inserting the above non-asymptotic bounds. Then, we have

\[\left\|\bm{M}_{\bm{x}\bm{x}}^{1/2}\widehat{\bm{M}}_{\bm{x}\bm{x} }^{-1}\bm{M}_{\bm{x}\bm{x}}^{1/2}\right\|_{\text{op}}\] \[=\lambda_{\min}^{-1}\left(\bm{M}_{\bm{x}\bm{x}}^{-1/2}\widehat{ \bm{M}}_{\bm{x}\bm{x}}\bm{M}_{\bm{x}\bm{x}}^{-1/2}\right)\] \[\leq\ \left[1-\frac{C\sigma^{2}}{\lambda_{\min}(\bm{M}_{\bm{x}\bm{x}})} \sqrt{\frac{d+\log(2/\tau)}{n}}-\frac{2\|\bm{\mu}_{\bm{x}}\|_{2}}{\lambda_{ \min}(\bm{M}_{\bm{x}\bm{x}})}\frac{4\sigma\sqrt{d}+2\sigma\sqrt{\log(1/\tau)} }{\sqrt{n}}\right]^{-1}\]

holds with probability at least \(1-2\tau\). It is easy to observe that as

\[n\geq n_{1}:=\max\left\{\frac{64C^{2}\sigma^{4}}{9\lambda_{\min}^{2}(\bm{M}_{ \bm{x}\bm{x}})}\left(d+\log(2/\tau)\right),\ \frac{32^{2}\|\bm{\mu}_{\bm{x}}\|_{2}^{2}\sigma^{2}}{\lambda_{\min}^{2}(\bm{M}_{ \bm{x}\bm{x}})}\left(2\sqrt{d}+\sqrt{\log(1/\tau)}\right)^{2}\right\},\]

we have \(\left\|\bm{M}_{\bm{x}\bm{x}}^{1/2}\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1}\bm{M}_{ \bm{x}\bm{x}}^{1/2}\right\|_{\text{op}}\leq 4\) holds with probability \(1-2\tau\).

_For rescaled approximation error term \(\left\|\widehat{\mathbb{E}}[(\bm{y}-\bm{Z}_{*}\bm{x})\bm{x}^{\top}\widehat{\bm{M} }_{\bm{x}\bm{x}}^{-1/2}]\right\|_{F}\),_ we first compute variance proxy for the subGaussian vector \(\bm{y}-\bm{Z}_{*}\bm{x}\). Note that the \(j\)-th component of the subGaussian vector \(\bm{y}-\bm{Z}_{*}\bm{x}\) can be written as

\[[\bm{y}-\bm{Z}_{*}\bm{x}]_{j}=(-[\bm{Z}_{*}]_{j,:}^{\top}\mid\bm{e}_{j}^{\top}) \begin{pmatrix}\bm{x}\\ \bm{y}\end{pmatrix},\]

where \([\bm{Z}_{*}]_{j,:}^{\top}\) is the \(j\)-th row of \(\bm{Z}_{*}\), \(\bm{e}_{j}^{\top}\) is a \(K\)-dimensional vector with \(j\)-th component equals to one and rest components equal to zero. Thus, it is easy to observe that the \(\ell_{2}\)-norm square of \((-[\bm{Z}_{*}]_{j,:}^{\top}\mid\bm{e}_{j}^{\top})\) is \(\|[\bm{Z}_{*}]_{j,:}\|_{2}^{2}+1\), and therefore, based on the Assumption 2, we have

\[\mathbb{E}_{\mathcal{D}}\left[\exp\left(\lambda[\bm{y}-\bm{Z}_{*}\bm{x}]_{j}- \lambda[\bm{\mu}_{\bm{y}}-\bm{Z}_{*}\bm{\mu}_{\bm{x}}]_{j}\right)\right] \leq\exp\left(\lambda^{2}\cdot(\|[\bm{Z}_{*}]_{j,:}\|_{2}^{2}+1)\sigma^{2}/2 \right),\]i.e., a subGaussian with variance proxy \(\sigma_{j}^{2}:=(\|[\bm{Z}_{*}]_{j,:}\|_{2}^{2}+1)\sigma^{2}\). Thus the rescaled approximation error can be upper-bounded by

\[\left\|\widehat{\mathbb{E}}[(\bm{y}-\bm{Z}_{*}\bm{x})\bm{x}^{\top} \widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{F}^{2}\] \[= \sum_{j=1}^{K}\left\|\widehat{\mathbb{E}}[[\bm{y}-\bm{Z}_{*}\bm{x} ]_{j}\bm{x}^{\top}\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{2}^{2}\] \[\leq 2\sum_{j=1}^{K}\underbrace{\left\|\widehat{\mathbb{E}}[([ \bm{y}-\bm{Z}_{*}\bm{x}]_{j}-[\bm{\mu}_{\bm{y}}-\bm{Z}_{*}\bm{\mu}_{\bm{x}}]_{j })\bm{x}^{\top}\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{2}^{2}}_{=:T_{ j}^{1}}+2\sum_{j=1}^{K}\underbrace{\left\|\widehat{\mathbb{E}}[[\bm{\mu}_{\bm{y}}-\bm{Z}_{*} \bm{\mu}_{\bm{x}}]_{j}\bm{x}^{\top}\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}] \right\|_{2}^{2}}_{=:T_{j}^{2}}.\]

We control term \(T_{j}^{1}\) for all \(j\in[K]\) separately using Lemma 5 as follows: For all \(\tau\in(0,1)\), we have

\[\mathbb{P}_{\mathcal{D}}\left(T_{j}^{1}\leq\frac{\sigma_{j}^{2}(d+2\sqrt{d\log (K/\tau)}+2\log(K/\tau))}{n}\right)\geq 1-\tau/K.\]

For the term \(T_{j}^{2}\), we have

\[T_{j}^{2} = \left\|\frac{1}{n}\sum_{i=1}^{n}[\bm{\mu}_{\bm{y}}-\bm{Z}_{*}\bm {\mu}_{\bm{x}}]_{j}(\bm{x}^{i})^{\top}\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2} \right\|_{2}^{2}\] \[= \left\|\frac{[\bm{\mu}_{\bm{y}}-\bm{Z}_{*}\bm{\mu}_{\bm{x}}]_{j}} {\sqrt{n}}\left(\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}\frac{\bm{x}^{1}}{\sqrt {n}}\mid\cdots\mid\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}\frac{\bm{x}^{n}}{\sqrt {n}}\right)\bm{1}_{n}\right\|_{2}^{2}\] \[= \frac{[\bm{\mu}_{\bm{y}}-\bm{Z}_{*}\bm{\mu}_{\bm{x}}]_{j}^{2}}{n} \left\|\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}\left(\frac{\bm{x}^{1}}{\sqrt{n}} \mid\cdots\mid\frac{\bm{x}^{n}}{\sqrt{n}}\right)\bm{1}_{n}\right\|_{2}^{2}.\]

Now let \(\left(\frac{\bm{x}^{1}}{\sqrt{n}}\mid\cdots\mid\frac{\bm{x}^{n}}{\sqrt{n}} \right)=\bm{U}_{\bm{x}}\bm{D}_{\bm{x}}\bm{V}_{\bm{x}}^{\top}\) be the singular value decomposition of the matrix \(\left(\frac{\bm{x}^{1}}{\sqrt{n}}\mid\cdots\mid\frac{\bm{x}^{n}}{\sqrt{n}}\right)\), the above \(\ell_{2}\)-norm can be further written as

\[\frac{[\bm{\mu}_{\bm{y}}-\bm{Z}_{*}\bm{\mu}_{\bm{x}}]_{j}^{2}}{n }\left\|\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}\left(\frac{\bm{x}^{1}}{\sqrt{n }}\mid\cdots\mid\frac{\bm{x}^{n}}{\sqrt{n}}\right)\bm{1}_{n}\right\|_{2}^{2}\] \[\stackrel{{\text{(i)}}}{{=}}\frac{[\bm{\mu}_{\bm{y}}- \bm{Z}_{*}\bm{\mu}_{\bm{x}}]_{j}^{2}}{n}\bm{1}_{n}^{\top}\bm{V}_{\bm{x}}\begin{pmatrix} \bm{I}_{d}&\bm{0}_{d\times(n-d)}\\ \bm{0}_{(n-d)\times d}&\bm{0}_{(n-d)\times(n-d)}\end{pmatrix}\bm{V}_{\bm{x}}^{ \top}\bm{1}_{n}\] \[\stackrel{{\text{(ii)}}}{{=}}\frac{[\bm{\mu}_{\bm{y} }-\bm{Z}_{*}\bm{\mu}_{\bm{x}}]_{j}^{2}}{n}\cdot d,\]

where the equality (i) holds due to the definition of empirical matrix \(\widehat{\bm{M}}_{\bm{x}\bm{x}}=\frac{1}{n}\sum_{i=1}^{n}\bm{x}^{i}(\bm{x}^{ i})^{\top}\), the equality (ii) holds due to the unitary property of matrix \(\bm{V}_{\bm{x}}\). Combining the above two parts implies

\[\left\|\widehat{\mathbb{E}}[(\bm{y}-\bm{Z}_{*}\bm{x})\bm{x}^{\top} \widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{F}^{2}\] \[= \sum_{j=1}^{K}\left\|\widehat{\mathbb{E}}[[\bm{y}-\bm{Z}_{*}\bm{x }]_{j}\bm{x}^{\top}\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{2}^{2}\] \[\leq 2\sum_{j=1}^{K}T_{j}^{1}+2\sum_{j=1}^{K}T_{j}^{2}\] \[\stackrel{{\text{(i)}}}{{\leq}}2\sum_{j=1}^{K}\frac{ \sigma_{j}^{2}(d+2\sqrt{d\log(K/\tau)}+2\log(K/\tau))}{n}+2\sum_{j=1}^{K}\frac{ [\bm{\mu}_{\bm{y}}-\bm{Z}_{*}\bm{\mu}_{\bm{x}}]_{j}^{2}}{n}\cdot d\] \[=2(\|\bm{Z}_{*}\|_{F}^{2}+K)\cdot\frac{d+2\sqrt{d\log(K/\tau)}+2 \log(K/\tau)}{n}+2\|\bm{\mu}_{\bm{y}}-\bm{Z}_{*}\bm{\mu}_{\bm{x}}\|_{2}^{2} \cdot\frac{d}{n}\]with inequality (iii) holds with probability at least \(1-\tau\). Still, it is easy to observe that for any positive constant \(\epsilon\), as

\[n\geq n_{2}:=\max\left\{4(\|\bm{Z}_{*}\|_{F}^{2}+K)\cdot\frac{d+2\sqrt{d\log(K/ \tau)}+2\log(K/\tau)}{\epsilon},\;4\|\bm{\mu}_{\bm{y}}-\bm{Z}_{*}\bm{\mu}_{\bm{x }}\|_{2}^{2}\cdot\frac{d}{\epsilon}\right\},\]

we have \(\left\|\widehat{\mathbb{E}}[(\bm{y}-\bm{Z}_{*}\bm{x})\bm{x}^{\top}\widehat{\bm {M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{F}^{2}\leq\epsilon\) holds with probability at least \(1-\tau\).

**Step-3.** Combining two upper bounds together, if \(n\geq\max\{n_{1},n_{2}\}\), the following inequality for generalization error bound

\[\mathcal{L}(\widehat{\bm{Z}})-\mathcal{L}(\bm{Z}_{*})\leq\left\|\widehat{ \mathbb{E}}[(\bm{y}-\bm{Z}_{*}\bm{x})\bm{x}^{\top}\widehat{\bm{M}}_{\bm{x}\bm {x}}^{-1/2}]\right\|_{F}^{2}\cdot\left\|\bm{M}_{\bm{x}\bm{x}}^{1/2}\widehat{ \bm{M}}_{\bm{x}\bm{x}}^{-1}\bm{M}_{\bm{x}\bm{x}}^{1/2}\right\|_{\text{op}} \leq 4\epsilon\]

holds with probability at least \(1-3\tau\).

_We then study the compressed version._

Step-1\({}^{\bm{\mathrm{\dagger}}}\).Similarly, its optimal solutions for population loss and empirical loss are \(\bm{W}_{*}=\bm{\Phi}\bm{M}_{\bm{y}\bm{x}}\bm{M}_{\bm{x}\bm{x}}^{-1}\) and \(\widehat{\bm{W}}=\frac{\bm{\Phi}\bm{Y}\bm{X}^{\top}}{n}\left(\frac{\bm{X}\bm{ X}^{\top}}{n}\right)^{-1}=:\bm{\Phi}\widehat{\bm{M}}_{\bm{y}\bm{x}}\widehat{ \bm{M}}_{\bm{x}\bm{x}}^{-1}\), respectively. Thus, the generalization error bound is

\[\mathcal{L}^{\bm{\Phi}}(\widehat{\bm{W}})-\mathcal{L}^{\bm{\Phi}}(\bm{W}_{*})= \|\widehat{\bm{W}}-\bm{W}_{*}\|_{\bm{M}_{\bm{x}\bm{x}}}^{2}=\|(\widehat{\bm{W} }-\bm{W}_{*})\bm{M}_{\bm{x}\bm{x}}^{1/2}\|_{F}^{2}.\]

Still, we have \((\widehat{\bm{W}}-\bm{W}_{*})\bm{M}_{\bm{x}\bm{x}}^{1/2}=\widehat{\mathbb{E}} [(\bm{\Phi}\bm{y}-\bm{W}_{*}\bm{x})\bm{x}^{\top}\widehat{\bm{M}}_{\bm{x}\bm{ x}}^{-1/2}|\widehat{\bm{M}}_{\bm{x}\bm{x}}^{1/2}\bm{M}_{\bm{x}\bm{x}}^{1/2}\), and therefore, the generalization error bound can be upper-bounded by

\[\left\|(\widehat{\bm{W}}-\bm{W}_{*})\bm{M}_{\bm{x}\bm{x}}^{1/2}\right\|_{F}^{ 2}\leq\left\|\widehat{\mathbb{E}}[(\bm{\Phi}\bm{y}-\bm{W}_{*}\bm{x})\bm{x}^{ \top}\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{F}^{2}\left\|\widehat{ \bm{M}}_{\bm{x}\bm{x}}^{-1/2}\bm{M}_{\bm{x}\bm{x}}^{1/2}\right\|_{\text{op}} ^{2}.\]

Step-2\({}^{\bm{\mathrm{\dagger}}}\).Next, we provide upper bounds on these two terms \(\left\|\widehat{\mathbb{E}}[(\bm{\Phi}\bm{y}-\bm{W}_{*}\bm{x})\bm{x}^{\top} \widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{F}\) and \(\left\|\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}\bm{M}_{\bm{x}\bm{x}}^{1/2} \right\|_{\text{op}}\) in the right-hand-side separately. Note that for the matrix error term \(\left\|\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}\bm{M}_{\bm{x}\bm{x}}^{1/2}\right\| _{\text{op}}\), we could use the same upper bounded as mentioned in the proof of uncompressed version.

Now, to give the upper bound on the rescaled approximation error term \(\left\|\widehat{\mathbb{E}}[(\bm{\Phi}\bm{y}-\bm{W}_{*}\bm{x})\bm{x}^{\top} \widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{F}\), we first compute the variance proxy for the subGaussian vector \(\bm{\Phi}\bm{y}-\bm{W}_{*}\bm{x}\), which is

\[\widetilde{\sigma}_{j}^{2}:=(\|[\bm{W}_{*}]_{j,:}\|_{2}^{2}+\|\bm{\Phi}_{j,:}\| _{2}^{2})\sigma^{2}\]

for all \(j\in[m]\). Thus, the rescaled approximation error for the compressed version can be upper bounded by

\[\left\|\widehat{\mathbb{E}}[(\bm{\Phi}\bm{y}-\bm{W}_{*}\bm{x})\bm{ x}^{\top}\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{F}^{2} =\sum_{j=1}^{m}\left\|\widehat{\mathbb{E}}[[\bm{\Phi}\bm{y}-\bm{W} _{*}\bm{x}]_{j}\bm{x}^{\top}\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{2}^ {2}\] \[\leq 2\sum_{j=1}^{m}\underbrace{\left\|\widehat{\mathbb{E}}[([\bm{ \Phi}\bm{y}-\bm{W}_{*}\bm{x}]_{j}-[\bm{\Phi}\bm{\mu}_{\bm{y}}-\bm{W}_{*}\bm{\mu} _{\bm{x}}]_{j})\bm{x}^{\top}\widehat{\bm{M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{2}^ {2}}_{=:\widehat{T}_{j}^{j}}\] \[\quad+2\sum_{j=1}^{m}\underbrace{\left\|\widehat{\mathbb{E}}[[\bm {\Phi}\bm{\mu}_{\bm{y}}-\bm{W}_{*}\bm{\mu}_{\bm{x}}]_{j}\bm{x}^{\top}\widehat{ \bm{M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{2}^{2}}_{=:\widehat{T}_{j}^{2}}.\]

Still using Lemma 5, for all \(\tau\in(0,1)\), we have

\[\mathbb{P}_{\mathcal{D}}\left(\widetilde{T}_{j}^{1}\leq\frac{\widetilde{\sigma}_{j }^{2}(d+2\sqrt{d\log(m/\tau)}+2\log(m/\tau))}{n}\right)\geq 1-\tau/m.\]For the term \(\widehat{T}_{j}^{2}\), following the same proof procedures of the uncompressed version implies

\[\widehat{T}_{j}^{2} =\frac{[\boldsymbol{\Phi\mu_{y}}-\boldsymbol{W}_{*}\boldsymbol{\mu }_{\boldsymbol{x}}]_{j}^{2}}{n}\left\|\widehat{\boldsymbol{M}}_{\boldsymbol{x }\boldsymbol{x}}^{-1/2}\left(\frac{\boldsymbol{x}^{1}}{\sqrt{n}}\mid\cdots\mid \frac{\boldsymbol{x}^{n}}{\sqrt{n}}\right)\boldsymbol{1}_{n}\right\|_{2}^{2}\] \[=\frac{[\boldsymbol{\Phi\mu_{y}}-\boldsymbol{W}_{*}\boldsymbol{ \mu}_{\boldsymbol{x}}]_{j}^{2}}{n}\cdot d\]

Therefore, the rescaled approximation error for the compressed version is upper-bounded by

\[\left\|\widehat{\mathbb{E}}[(\boldsymbol{\Phi y}-\boldsymbol{W}_{*} \boldsymbol{x})\boldsymbol{x}^{\top}\widehat{\boldsymbol{M}}_{\boldsymbol{x }\boldsymbol{x}}^{-1/2}]\right\|_{F}^{2}\] \[\leq 2(\|\boldsymbol{W}_{*}\|_{F}^{2}+\|\boldsymbol{\Phi}\|_{F}^{ 2})\cdot\frac{d+2\sqrt{d\log(m/\tau)}+2\log(m/\tau)}{n}+2\|\boldsymbol{\Phi \mu_{y}}-\boldsymbol{W}_{*}\boldsymbol{\mu}_{\boldsymbol{x}}\|_{2}^{2}\cdot \frac{d}{n}\]

with probability at least \(1-\tau\). Similarly, it is easy to get that for any positive constant \(\epsilon\), as

\[n\geq\widetilde{n}_{2}:=\max\left\{4(\|\boldsymbol{W}_{*}\|_{F}^{2}+\| \boldsymbol{\Phi}\|_{F}^{2})\cdot\frac{d+2\sqrt{d\log(m/\tau)}+2\log(m/\tau)}{ \epsilon},\;4\|\boldsymbol{\Phi\mu_{y}}-\boldsymbol{W}_{*}\boldsymbol{\mu}_{ \boldsymbol{x}}\|_{2}^{2}\cdot\frac{d}{\epsilon}\right\},\]

we have \(\left\|\widehat{\mathbb{E}}[(\boldsymbol{\Phi y}-\boldsymbol{W}_{*} \boldsymbol{x})\boldsymbol{x}^{\top}\widehat{\boldsymbol{M}}_{\boldsymbol{x }\boldsymbol{x}}^{-1/2}]\right\|_{F}^{2}\leq\epsilon\) holds with probability at least \(1-\tau\).

**Step-3'.** Combining two upper bounds together, if \(n\geq\max\{n_{1},\widetilde{n}_{2}\}\), the following inequality for generalization error bound

\[\mathcal{L}^{\boldsymbol{\Phi}}(\widehat{\boldsymbol{W}})-\mathcal{L}^{ \boldsymbol{\Phi}}(\boldsymbol{W}_{*})\leq\left\|\widehat{\mathbb{E}}[( \boldsymbol{\Phi y}-\boldsymbol{W}_{*}\boldsymbol{x})\boldsymbol{x}^{\top} \widehat{\boldsymbol{M}}_{\boldsymbol{x}\boldsymbol{x}}^{-1/2}]\right\|_{F}^ {2}\cdot\left\|\boldsymbol{M}_{\boldsymbol{x}\boldsymbol{x}}^{1/2}\widehat{ \boldsymbol{M}}_{\boldsymbol{x}\boldsymbol{x}}^{-1}\boldsymbol{M}_{ \boldsymbol{x}\boldsymbol{x}}^{1/2}\right\|_{\text{op}}\leq 4\epsilon\]

holds with probability at least \(1-3\tau\). 

### Proof of Theorem 4

Recall the following theorem in the main text.

**Theorem 4** For any \(\delta\in(0,1)\) and any \(\tau\in(0,1/3)\), suppose the compressed matrix \(\boldsymbol{\Phi}\) follows Assumption 1 with \(m\geq O(\frac{\epsilon}{\delta^{2}}\log(\frac{K}{\tau}))\), and Assumption 2 holds. Given any learned regressor \(\widehat{\boldsymbol{W}}\) from training problem (2), let \((\boldsymbol{x},\boldsymbol{y})\) be a new sample drawn from the underlying distribution \(\mathcal{D}\), we have the following inequality holds with probability at least \(1-\tau\):

\[\mathbb{E}_{\mathcal{D}}[\|\widehat{\boldsymbol{y}}-\boldsymbol{y}\|_{2}^{2}] \leq\frac{4}{1-\delta}\cdot\mathbb{E}_{\mathcal{D}}[\|\boldsymbol{\Phi y}- \widehat{\boldsymbol{W}}\boldsymbol{x}\|_{2}^{2}],\]

where \(\widehat{\boldsymbol{y}}\) is the optimal solution from prediction problem (3) with input vector \(\boldsymbol{x}\).

Proof.: Due to the optimality of \(\widehat{\boldsymbol{y}}\), we have

\[\left\|\boldsymbol{\Phi}\widehat{\boldsymbol{y}}-\widehat{ \boldsymbol{W}}\boldsymbol{x}\right\|_{2}^{2}\leq\left\|\boldsymbol{\Phi y}- \widehat{\boldsymbol{W}}\boldsymbol{x}\right\|_{2}^{2}\] \[\Leftrightarrow \left\|\boldsymbol{\Phi}\widehat{\boldsymbol{y}}-\boldsymbol{\Phi y }+\boldsymbol{\Phi y}-\widehat{\boldsymbol{W}}\boldsymbol{x}\right\|_{2}^{2} \leq\left\|\boldsymbol{\Phi y}-\widehat{\boldsymbol{W}}\boldsymbol{x}\right\|_ {2}^{2}\] \[\Leftrightarrow \left\|\boldsymbol{\Phi}\widehat{\boldsymbol{y}}-\boldsymbol{\Phi y }\right\|_{2}^{2}\leq 2\left\|\boldsymbol{\Phi\widehat{y}}-\boldsymbol{\Phi y} \boldsymbol{,\Phi y}-\widehat{\boldsymbol{W}}\boldsymbol{x}\right\|_{2}\] \[\Rightarrow \left\|\boldsymbol{\Phi}\widehat{\boldsymbol{y}}-\boldsymbol{\Phi y }\right\|_{2}^{2}\leq 2\left\|\boldsymbol{\Phi\widehat{y}}-\boldsymbol{\Phi y} \right\|_{2}\left\|\boldsymbol{\Phi y}-\widehat{\boldsymbol{W}}\boldsymbol{x} \right\|_{2}\] \[\Leftrightarrow \left\|\boldsymbol{\Phi}\widehat{\boldsymbol{y}}-\boldsymbol{\Phi y }\right\|_{2}\leq 2\left\|\boldsymbol{\Phi y}-\widehat{\boldsymbol{W}} \boldsymbol{x}\right\|_{2}\] \[\Leftrightarrow \left\|\boldsymbol{\Phi}\widehat{\boldsymbol{y}}-\boldsymbol{\Phi y }\right\|_{2}^{2}\leq 4\left\|\boldsymbol{\Phi y}-\widehat{\boldsymbol{W}} \boldsymbol{x}\right\|_{2}^{2}\] \[\Rightarrow (1-\delta)\|\widehat{\boldsymbol{y}}-\boldsymbol{y}\|_{2}^{2} \leq 4\left\|\boldsymbol{\Phi y}-\widehat{\boldsymbol{W}}\boldsymbol{x} \right\|_{2}^{2}\]

where the final \(\Rightarrow\) holds due to the \((3s,\delta)\)-RIP property of the compressed matrix \(\boldsymbol{\Phi}\) with probability at least \(1-\tau\). Taking expectations on both sides implies

\[(1-\delta)\mathbb{E}_{\mathcal{D}}[\|\widehat{\boldsymbol{y}}-\boldsymbol{y}\|_{ 2}^{2}]\leq 4\mathbb{E}_{\mathcal{D}}[\|\boldsymbol{\Phi y}-\widehat{\boldsymbol{W}} \boldsymbol{x}\|_{2}^{2}],\]

which completes the story.

### Technical Lemma

#### a.5.1 Proof of Claim Proposed in Remark 2

Proof.: Let us discuss the computational complexity for \(\mathcal{F}\) to be \(\mathbb{R}^{K},\mathbb{R}^{K}_{+},\{0,1\}^{K}\) separately. Given a fixed \(\tilde{\bm{v}}\),

\(\bullet\) If \(\mathcal{F}=\mathbb{R}^{K}\), the projection method \(\arg\min_{\bm{v}\in\mathcal{V}^{K}_{x}\cap\mathcal{F}}\ \|\bm{v}-\tilde{\bm{v}}\|_{2}^{2}\) can be reformulate using the following mixed-integer programming (MIP),

\[(\bm{v}_{*},\bm{z}_{*})\ :=\ \arg\min_{\bm{v},\bm{z}} \sum_{p=1}^{K}\bm{z}_{p}(\bm{v}_{p}-\tilde{\bm{v}}_{p})^{2}\ \,\] s.t. \[\sum_{p=1}^{K}\bm{z}_{p}\leq s\]

with \(\bm{v}_{*}\) the output of the projection method. Sorting the absolute values \(\{|\tilde{\bm{v}}_{p}|\}_{p=1}^{K}\) in decreasing order such that

\[|\tilde{\bm{v}}_{(1)}|\geq\cdots\geq|\tilde{\bm{v}}_{(K)}|,\]

the output \(\bm{v}_{*}\) of the proposed projection is

\[[\bm{v}_{*}]_{j}=\left\{\begin{array}{ll}\tilde{\bm{v}}_{j}& \mbox{if }j\in\{(1),\ldots,(s)\}\\ 0&\mbox{o.w.}\end{array}\right.\ \,\]

with computational complexity \(O(K\min\{s,\log K\})\).

\(\bullet\) If \(\mathcal{F}=\mathbb{R}^{K}_{+}\), the projection method \(\arg\min_{\bm{v}\in\mathcal{V}^{K}_{x}\cap\mathcal{F}}\ \|\bm{v}-\tilde{\bm{v}}\|_{2}^{2}\) can be reformulate using the following mixed-integer programming (MIP),

\[(\bm{v}_{*},\bm{z}_{*})\ :=\ \arg\min_{\bm{v},\bm{z}} \sum_{p=1}^{K}\bm{z}_{p}(\bm{v}_{p}-\tilde{\bm{v}}_{p})^{2}\] s.t. \[\sum_{p=1}^{K}\bm{z}_{p}\leq s\] \[\bm{v}_{p}\geq 0\ \forall\ p\in[K]\]

Sorting \(\{\tilde{\bm{v}}_{p}\}_{p=1}^{K}\) in decreasing order such that

\[\tilde{\bm{v}}_{(1)}\geq\cdots\geq\tilde{\bm{v}}_{(K)},\]

the output \(\bm{v}_{*}\) of the proposed projection is

\[[\bm{v}_{*}]_{j}=\left\{\begin{array}{ll}\tilde{\bm{v}}_{j}\cdot\mathbb{I}( \tilde{\bm{v}}_{j}>0)&\mbox{if }j\in\{(1),\ldots,(s)\}\\ 0&\mbox{o.w.}\end{array}\right.\ \,\]

with computation complexity \(O(K\min\{s,\log K\})\).

\(\bullet\) If \(\mathcal{F}=\{0,1\}^{K}\), the projection method \(\min_{\bm{v}\in\mathcal{V}^{K}_{x}\cap\mathcal{F}}\ \|\bm{v}-\tilde{\bm{v}}\|_{2}^{2}\) presented in step-4 of Algorithm 1 can be represented as

\[\begin{array}{ll}\min_{\bm{z}}&\sum_{p=1}^{K}(1-\bm{z}_{p})\tilde{\bm{v}}_{p} ^{2}+\bm{z}_{p}(\tilde{\bm{v}}_{p}-1)^{2}&\min_{\bm{z}}&\sum_{p=1}^{K}\tilde{ \bm{v}}_{p}^{2}-\bm{z}_{p}(2\tilde{\bm{v}}_{p}-1)\\ \mbox{s.t.}&\sum_{p=1}^{K}\bm{z}_{p}\leq s&\mbox{s.t.}&\sum_{p=1}^{K}\bm{z}_{p} \leq s\\ &\bm{z}_{p}\in\{0,1\}\ \forall\ p\in[K]&\bm{z}_{p}\in\{0,1\}\ \forall\ p\in[K]\end{array}\.\]

Sort \(\{2\tilde{\bm{v}}_{p}-1\}_{p=1}^{K}\) in decreasing order such that

\[2\tilde{\bm{v}}_{(1)}-1\geq 2\tilde{\bm{v}}_{(2)}-1\geq\cdots\geq 2\tilde{\bm{v} }_{(K)}-1,\]

then, the optimal \(\bm{z}^{*}\) can be set by

\[\bm{z}_{p}^{*}=\left\{\begin{array}{ll}\mathbb{I}(2\bm{v}_{p}-1>0)&\mbox{if }p \in\{(1),\ldots,(s)\}\\ 0&\mbox{o.w.}\end{array}\right.\ \.\]

For computational complexity, computing the sequence \(\{2\bm{v}_{p}-1\}_{p=1}^{K}\) needs \(O(K)\), picking the top-\(s\) elements of the above sequence requires \(O(K\min\{s,\log K\})\), setting the optimal solution \(\bm{z}^{*}\) needs \(O(s)\), and thus the total computational complexity is \(O(K)+O(K\min\{s,\log K\})+O(s)=O(K\min\{s,\log K\})\).

#### a.5.2 Lemma for the Proof of Theorem 1

**Lemma 1**.: _(Unitary invariant). Let \(\bm{\Phi}\in\mathbb{R}^{m\times d}\) be a randomized compressed matrix as described in Assumption 1, and \(\bm{U}\in\mathbb{R}^{d\times d}\) be a real unitary matrix. Then we have \(\tilde{\bm{\Phi}}=\bm{\Phi}\bm{U}\) is \((1,\delta)\)-RIP with probability at least \(1-\tau\)._

Proof.: Note that \((i,j)\)-th component of \(\tilde{\bm{\Phi}}\) can be represented as \(\tilde{\bm{\Phi}}_{i,j}=\bm{\Phi}_{i,\bm{U}}._{:,j}=\sum_{\ell=1}^{d}\bm{\Phi }_{i,\ell}\bm{U}_{\ell,j}\). Since every component \(\bm{\Phi}_{i,j}\) in \(\bm{\Phi}\) is i.i.d. drawn from \(\mathcal{N}(0,1/m)\), we have

\[\tilde{\bm{\Phi}}_{i,j}=\sum_{\ell=1}^{d}\bm{\Phi}_{i,\ell}\bm{U}_{\ell,j} \sim\mathcal{N}\left(0,\sum_{\ell=1}^{d}\frac{1}{m}\bm{U}_{\ell,j}^{2}\right) =\mathcal{N}(0,1/m).\]

Now, we need to show that any two distinct components \(\tilde{\bm{\Phi}}_{i_{1},j_{1}}\) and \(\tilde{\bm{\Phi}}_{i_{2},j_{2}}\) in \(\tilde{\bm{\Phi}}\) are independent. It is easy to observe that \(\tilde{\bm{\Phi}}_{i_{1},j_{1}}\) and \(\tilde{\bm{\Phi}}_{i_{2},j_{2}}\) are independent when \(i_{1}\neq i_{2}\) since \(\bm{\Phi}_{i_{1},:}\) and \(\bm{\Phi}_{i_{2},:}\) are independent. If \(i_{1}=i_{2}=i\), then the following random vector satisfies

\[\begin{pmatrix}\tilde{\bm{\Phi}}_{i,j_{1}}\\ \tilde{\bm{\Phi}}_{i,j_{1}}\end{pmatrix}=\begin{pmatrix}\bm{\Phi}_{i,\bm{U}}._ {:,j_{1}}\\ \bm{\Phi}_{i,\bm{U}}._{:,j_{2}}\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix} 0\\ 0\end{pmatrix},\begin{pmatrix}1/m&0\\ 0&1/m\end{pmatrix}\right).\]

That is to say, \(\tilde{\bm{\Phi}}_{i_{1},j_{1}}\) and \(\tilde{\bm{\Phi}}_{i_{2},j_{2}}\) are _jointly Gaussian distributed and uncorrelated_, which shows that \(\tilde{\bm{\Phi}}_{i_{1},j_{1}}\) and \(\tilde{\bm{\Phi}}_{i_{2},j_{2}}\) are independent. Combining the above together, we have \(\tilde{\bm{\Phi}}\) is a randomized matrix with component i.i.d. from \(\mathcal{N}(0,1/m)\). Based on the existing result ([7], Theorem 1.5), when \(m\geq C_{1}\cdot\delta^{-2}[\ln(eK)+\ln(2/\tau)]\) for any \(\delta>0\) and \(\tau\in(0,1)\), we have \(\tilde{\bm{\Phi}}\) ensures \((1,\delta)\)-RIP with probability at least \(1-\tau\). 

#### a.5.3 Lemma for the Proof of Theorem 2

**Lemma 2**.: _For any integer parameter \(s(\leq d)\) and positive parameter \(\delta\in(0,1)\), let \(\bm{\Phi}\in\mathbb{R}^{m\times d}\) be a \((s,\delta)\)-RIP matrix. For \(\bm{u}_{1},\bm{u}_{2}\), if \(\bm{u}_{1},\bm{u}_{2},\bm{u}_{1}+\bm{u}_{2},\bm{u}_{1}-\bm{u}_{2}\) are all \(s\)-sparse, then the following inequality holds_

\[-2\delta(\|\bm{u}_{1}\|_{2}^{2}+\|\bm{u}_{2}\|_{2}^{2})+4\langle\bm{u}_{1},\bm {u}_{2}\rangle\leq 4\langle\bm{\Phi}\bm{u}_{1},\bm{\Phi}\bm{u}_{2}\rangle\leq 2 \delta(\|\bm{u}_{1}\|_{2}^{2}+\|\bm{u}_{2}\|_{2}^{2})+4\langle\bm{u}_{1},\bm{u} _{2}\rangle.\]

Proof.: Since \(\bm{u}_{1},\bm{u}_{2},\bm{u}_{1}+\bm{u}_{2},\bm{u}_{1}-\bm{u}_{2}\) are \(s\)-sparse, we have

\[(1-\delta)(\|\bm{u}_{1}+\bm{u}_{2}\|_{2}^{2}) \leq\langle\bm{\Phi}(\bm{u}_{1}+\bm{u}_{2}),\bm{\Phi}(\bm{u}_{1}+ \bm{u}_{2})\rangle\leq(1+\delta)(\|\bm{u}_{1}+\bm{u}_{2}\|_{2}^{2})\] (8) \[(1-\delta)(\|\bm{u}_{1}-\bm{u}_{2}\|_{2}^{2}) \leq\langle\bm{\Phi}(\bm{u}_{1}-\bm{u}_{2}),\bm{\Phi}(\bm{u}_{1}- \bm{u}_{2})\rangle\leq(1+\delta)(\|\bm{u}_{1}-\bm{u}_{2}\|_{2}^{2})\] (9)

Subtracting (9) from (8) gives

\[-2\delta(\|\bm{u}_{1}\|_{2}^{2}+\|\bm{u}_{2}\|_{2}^{2})+4\langle\bm{u}_{1},\bm {u}_{2}\rangle\leq 4\langle\bm{\Phi}\bm{u}_{1},\bm{\Phi}\bm{u}_{2}\rangle\leq 2 \delta(\|\bm{u}_{1}\|_{2}^{2}+\|\bm{u}_{2}\|_{2}^{2})+4\langle\bm{u}_{1},\bm {u}_{2}\rangle,\]

which completes the proof. 

#### a.5.4 Lemma for the Proof of Theorem 3

**Lemma 3**.: _Let \(\bm{\xi}^{1},\dots,\bm{\xi}^{n}\) be \(n\) i.i.d. \(\sigma^{2}\)-subGaussian random vectors with a zero mean and a covariance matrix \(\bm{\Sigma}\). Then, there exists a positive constant \(C\) such that for all \(\tau\in(0,1)\),_

\[\mathbb{P}\left(\left\|\sum_{i=1}^{n}\frac{\bm{\xi}^{i}(\bm{\xi}^{i})^{\top}}{n} -\bm{\Sigma}\right\|_{op}\leq C\sigma^{2}\max\left\{\sqrt{\frac{d+\log(2/\tau )}{n}},\frac{d+\log(2/\tau)}{n}\right\}\right)\geq 1-\tau.\]

Proof.: Lemma 3 is a direct corollary from [Theorem 6.5, [41]]. It is easy to observe that the proposed Lemma 3 holds by setting the parameter \(\delta\) listed in [Theorem 6.5, [41]] as \(\min\{1,c\sqrt{\ln(2/\tau)/n}\}\) with \(c\) some positive constant. 

**Lemma 4**.: _Let \(\bm{\xi}^{1},\dots,\bm{\xi}^{n}\) be \(n\) i.i.d. \(\sigma^{2}\)-subGaussian random vectors with a zero mean and a covariance matrix \(\bm{\Sigma}\). Then for any \(\tau\in(0,1)\), we have_

\[\mathbb{P}\left(\left\|\sum_{i=1}^{n}\frac{\bm{\xi}^{i}}{n}\right\|_{2}\leq\frac{ 4\sigma\sqrt{d}+2\sigma\sqrt{\log(1/\tau)}}{\sqrt{n}}\right)\geq 1-\tau.\]Proof.: We show this lemma by discretizing the unit \(\ell_{2}\)-norm ball \(\mathbb{B}_{2}(\bm{0};1)\). Let \(\mathcal{N}_{1/2}\) be a \(\frac{1}{2}\)-minimum cover of \(\mathbb{B}_{2}(\bm{0};1)\) with its cardinality \(|\mathcal{N}_{1/2}|\leq 5^{d}\). Since for any vector \(\bm{\xi}\in\mathbb{R}^{d}\), we always have

\[\|\bm{\xi}\|_{2}=\max_{\|\bm{v}\|_{2}\leq 1}\langle\bm{v},\bm{\xi}\rangle\leq \max_{\bm{v}^{\prime}\in\mathcal{N}_{1/2}}\langle\bm{v}^{\prime},\bm{\xi} \rangle+\max_{\|\bm{v}^{\prime\prime}\|_{2}\leq 1/2}\langle\bm{v}^{\prime \prime},\bm{\xi}\rangle=\max_{\bm{v}^{\prime}\in\mathcal{N}_{1/2}}\langle\bm {v}^{\prime},\bm{\xi}\rangle+\frac{1}{2}\max_{\|\bm{v}^{\prime\prime}\|_{2} \leq 1}\langle\bm{v}^{\prime\prime},\bm{\xi}\rangle,\]

then \(\|\bm{\xi}\|_{2}\leq 2\max_{\bm{v}^{\prime}\in\mathcal{N}_{1/2}}\langle\bm{v}^{ \prime},\bm{\xi}\rangle\). Therefore, for any \(\sigma^{2}\)-subGaussian random vector,

\[\mathbb{P}(\|\bm{\xi}\|_{2}\geq t)\geq\mathbb{P}\left(\max_{\bm{v}^{\prime} \in\mathcal{N}_{1/2}}\langle\bm{v}^{\prime},\bm{\xi}\rangle\geq t/2\right) \leq|\mathcal{N}_{1/2}|\cdot\exp\left(-\frac{t^{2}}{8\sigma^{2}}\right)\leq 5 ^{d}\cdot\exp\left(-\frac{t^{2}}{8\sigma^{2}}\right),\]

which implies that \(\|\bm{\xi}\|_{2}\leq 4\sigma\sqrt{d}+2\sigma\sqrt{\log(1/\tau)}\) with probability at least \(1-\tau\). Now, since \(\bm{\bar{\xi}}=\sum_{i=1}^{n}\frac{\bm{\xi}^{i}}{n}\) is a \(\sigma^{2}/n\)-subGaussian random vector, inserting this variance proxy into the above inequality gives the desired result. 

**Lemma 5**.: _Let \(\eta(\bm{x})\) be a zero-mean, \(\sigma_{\eta}^{2}\)-subGaussian random variable. Let \(\bm{x}^{1},\ldots,\bm{x}^{n}\) be \(n\) i.i.d. \(\sigma^{2}\)-subGaussian random vectors (may not zero-mean) as described in Assumption 2. Conditioned on \(\widehat{\bm{M}}_{\bm{x}\bm{x}}=\frac{1}{n}\sum_{i=1}^{n}\bm{x}^{i}(\bm{x}^{i} )^{\top}\succ\bm{0}_{d\times d}\), for any \(\tau\in(0,1)\), we have_

\[\mathbb{P}\left(\left\|\widehat{\mathbb{E}}[\eta(\bm{x})\bm{x}^{\top}\widehat{ \bm{M}}_{\bm{x}\bm{x}}^{-1/2}]\right\|_{2}^{2}\leq\frac{\sigma_{\eta}^{2}(d+2 \sqrt{d\log(1/\tau)}+2\log(1/\tau))}{n}\right)\geq 1-\tau.\]

Proof.: The proof of Lemma 5 can be found in [Lemma 5, 18]. 

#### a.5.5 Discussion after Theorem 4

Since \(\|\bm{v}^{(T)}-\bm{y}\|_{2}\leq\|\bm{v}^{(T)}-\widehat{\bm{y}}\|_{2}+\| \widehat{\bm{y}}-\bm{y}\|_{2}\), combined with Theorem 2, we have

\[\mathbb{E}_{\mathcal{D}}[\|\bm{v}^{(T)}-\bm{y}\|_{2}^{2}] \leq 2\mathbb{E}_{\mathcal{D}}[\|\bm{v}^{(T)}-\widehat{\bm{y}}\|_ {2}^{2}]+2\mathbb{E}_{\mathcal{D}}[\|\widehat{\bm{y}}-\bm{y}\|_{2}^{2}]\] \[\leq O(\mathbb{E}_{\mathcal{D}}[\|\bm{\Phi}\widehat{\bm{y}}- \widehat{\bm{W}}\bm{x}\|_{2}])+\frac{8}{1-\delta}\cdot\mathbb{E}_{\mathcal{D}}[ \|\bm{\Phi}\bm{y}-\widehat{\bm{W}}\bm{x}\|_{2}^{2}]\] \[\leq O(\mathbb{E}_{\mathcal{D}}[\|\bm{\Phi}\bm{y}-\widehat{\bm{W }}\bm{x}\|_{2}]).\]

### Additional Numerical Experiments on Synthetic Data

#### a.6.1 Implemented Prediction Method

The implemented prediction method is presented as follows, see Algorithm 2. Comparing with Algorithm 1 proposed in the main content, it adds an additional stopping criteria

\[\frac{\|\bm{v}^{(t)}-\bm{v}^{(t-2)}\|_{2}}{0.01+\|\bm{v}^{(t)}\|_{2}}<10^{-6}\]

to ensure an earlier stop than Algorithm 1.

Note, in later numerical experiments, we use the terminology _'early stopping'_ to denote that the iteration generated by the prediction algorithm satisfies the above additional stopping criteria within the maximum iteration number, i.e. \(T=60\) (as listed in Section 4).

#### a.6.2 Discussions on Baselines

_Baselines._: We compare our proposed prediction method with the following baselines.

**Orthogonal Matching Pursuit.**: Orthogonal Matching Pursuit(OMP) is a greedy prediction algorithm. It iteratively chooses the most relevant output and then performs least-squares and updates the residuals. The built-in function 'OrthogonalMatching Pursuit' from Python package 'Sklearn.Linear_model' is used in the experiment.
**Correlation Decoding.**: Correlation decoding is a standard decoding algorithm. It computes the multiplication of the transpose of compression matrix \(\bm{\Phi}\) and the learned regressor \(\widehat{\bm{W}}\). For any test point \(\bm{x}\), the algorithm predicts the top \(s\) labels in \(\bm{\Phi}\widehat{\bm{W}}\bm{x}\) order by magnitude.

[MISSING_PAGE_EMPTY:24]

Parameter setting.In prediction stage, we choose \(s\in\{1,3\}\) for EURLex-4K and \(s\in\{3,5\}\) for Wiki10-31K. We choose the number of rows \(m\in\{100,200,300,400,500,700,1000,2000\}\) on both EURLex-4K and Wiki10-31. Ten independent trials of compressed matrices \(\mathbf{\Phi}^{(1)},\ldots,\mathbf{\Phi}^{(10)}\) are implemented for each tuple of parameters \((s,m)\) on both datasets.

Empirical running time.Here, we report the running time of the proposed algorithm and baselines on both synthetic and real datasets, see Table 2.

Numerical Results & Discussions.Figure 2 further illustrates that the computational complexity increases linearly with respect to the growth of compressed output dimension \(m\) on synthetic data, when \(m\) is greater than \(500\) to ensure the convergence of the prediction algorithm (see Remark 2).

For real data, Figure 3 and Figure 4 present the results of their accuracy performances. In particular, the accuracy grows relatively stable with respect to \(m\) when the compression matrix satisfies the RIP-property with high probability. Besides, based on the results presented in Figure 3, Figure 4, and Table 2, we observe that the proposed algorithm slightly outperforms the baselines on precision as \(s\) increases while enjoys a significant better computational efficiency, especially on large instances, which demonstrate the stability of the proposed algorithm.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline
**Dataset** & **Prop.Algo.** & **OMP** & **CD** & **Elastic Net** & **FISTA** \\ \hline
**Synthetic Data** & \(\approx\)1 second & 200-400 seconds & \textless{1 second} & 700-900 seconds & \textless{3 seconds} \\
**EURLex-4K** & \textless{1 second} & 20-80 seconds & \textless{1 second} & - & \(\approx\) 1 second \\
**Wiki10-31K** & \textless{5 seconds} & 500-700 seconds & \textless{5 seconds} & - & 5-10 seconds \\ \hline \end{tabular}
\end{table}
Table 2: Time Complexity Comparison for each prediction

Figure 2: The figure reports the prediction running time (measured in seconds) on synthetic data with early stopping by the proposed algorithm under different compressed output dimensions. As we can observe, the running time first decreases dramatically, then increases almost linearly with respect to \(m\). Such a phenomenon has occurred since the max number of iterations is 60 in the implemented prediction method with early stopping, which is relatively large; As \(m\) increases but is still less than 500, the actual number of iterations drops dramatically due to early stopping criteria; After passes 500, the actual number of iterations stays around 10, and then the running time grows linearly as dimension increases.

Figure 3: This figure reports the numerical results on real data  EURLex-4K. Each dot in the figure represents 10 _independent trials_ (i.e., experiments) of compressed matrices \(\mathbf{\Phi}^{(1)},\ldots,\mathbf{\Phi}^{(10)}\) on a given tuple of parameters \((s,m)\). The curves in each panel correspond to the averaged values for the proposed Algorithm and baselines over 10 trials; the shaded parts represent the empirical standard deviations over 10 trials. In the first row, we plot the output distance versus the number of rows. In the second row, we plot the precision versus the number of rows, and we cannot observe significant differences between these prediction methods.

Figure 4: This figure reports the numerical results on real data  Wiki10-31K. Similar to the plot reporting on EURLex-4K above, each dot in the figure represents 10 _independent trials_ (i.e., experiments) of compressed matrices \(\mathbf{\Phi}^{(1)},\ldots,\mathbf{\Phi}^{(10)}\) on a given tuple of parameters \((s,m)\). The curves in each panel correspond to the averaged values for the proposed algorithm and baselines over 10 trials; the shaded parts represent the empirical standard deviations over 10 trials. Similarly, in the first row, the precision of the proposed algorithm outperforms the FISTA especially when \(s\) is small. In the second & third rows for output difference and prediction loss, there are only slight improvement on the proposed algorithm than CD of output difference.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The paper's abstract is the short highlight of our introduction, covering the problems we study, the theoretical results we establish, and the numerical experiments we conduct. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We mention assumptions and discuss the limitation of the proposed method, and compare our method with other existing methods. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Assumptions and complete proofs are presented in the Appendix with a short proof sketch in the main paper. All the theorems, used formulas, and proofs in the paper should be numbered and cross-referenced. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The numerical experiments are controlled by the default random seed, and it could be reproduced. Moreover, we not only provide the code but also include the detailed steps of the experiment in the main paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We follow the guidance to provide the code and data. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We add the details in the numerical experiment section. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide the confidence interval for the results, which shows the significant improvement. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The compute resources are mentioned in the numerical section. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: It is anonymous. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Our method aims to solving challenges in the recent hype in large language model, for which hallucination is a big issue. Besides, our paper is also can be used in the fairness considerations. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper does not have this issue. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite all papers that are used in our numerical experiments. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide anonymized zip file. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.