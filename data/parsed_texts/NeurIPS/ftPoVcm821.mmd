# Demo2Code: From Summarizing Demonstrations

to Synthesizing Code via Extended Chain-of-Thought

 Huaxiaoyue Wang

Cornell University

yukiwang@cs.cornell.edu &Gonzalo Gonzalez-Pumariega

Cornell University

gg387@cornell.edu &Yash Sharma

Cornell University

ys749@cornell.edu &Sanjiban Choudhury

Cornell University

sanjibanc@cornell.edu

###### Abstract

Language instructions and demonstrations are two natural ways for users to teach robots personalized tasks. Recent progress in Large Language Models (LLMs) has shown impressive performance in translating language instructions into code for robotic tasks. However, translating demonstrations into task code continues to be a challenge due to the length and complexity of both demonstrations and code, making learning a direct mapping intractable. This paper presents Demo2Code, a novel framework that generates robot task code from demonstrations via an _extended chain-of-thought_ and defines a common latent specification to connect the two. Our framework employs a robust two-stage process: (1) a recursive summarization technique that condenses demonstrations into concise specifications, and (2) a code synthesis approach that expands each function recursively from the generated specifications. We conduct extensive evaluation on various robot task benchmarks, including a novel game benchmark Robotouille, designed to simulate diverse cooking tasks in a kitchen environment. The project's website is at https://portal-cornell.github.io/demo2code/

## 1 Introduction

How do we program home robots to perform a wide variety of _personalized_ everyday tasks? Robots must learn such tasks online, through natural interactions with the end user. A user typically communicates a task through a combination of language instructions and demonstrations. This paper addresses the problem of learning robot task code from those two inputs. For instance, in Fig. 1, the user teaches the robot how they prefer to make a burger through both language instructions, such as "make a burger", and demonstrations, which shows the order in which the ingredients are used.

Recent works [24, 23, 33, 80, 61, 35] have shown that Large Language Models (LLMs) are highly effective in using language instructions as prompts to plan robot tasks. However, extending LLMs to take demonstrations as input presents two fundamental challenges. The first challenge comes from demonstrations for long-horizon tasks. Naively concatenating and including all demonstrations in the LLM's prompt would easily exhaust the model's context length. The second challenge is that code for long-horizon robot tasks can be complex and require control flow. It also needs to check for physics constraints that a robot may have and be able to call custom perception and action libraries. Directly generating such code in a single step is error-prone.

_Our key insight is that while demonstrations are long and code is complex, they both share a latent task specification that the user had in mind._ This task specification is a detailed languagedescription of how the task should be completed. It is latent because the end user might not provide all the details about the desired task via natural language. We build an extended chain-of-thought [73] that recursively summarizes demonstrations to a compact specification, maps it to high-level code, and recursively expands the code by defining all the helper functions. Each step in the chain is small and easy for the LLM to process.

We propose a novel framework, Demo2Code, that generates robot task code from language instructions and demonstrations through a two-stage process (Fig. 1). _(1) Summarizing demonstrations to task specifications:_ Recursive summarization first works on each demonstration individually. Once all demonstrations are compactly represented, they are then jointly summarized in the final step as the task specification. This approach helps prevent each step from exceeding the LLM's maximum context length. _(2) Synthesizing code from the task specification:_ Given a task specification, the LLM first generates high-level task code that can call undefined functions. It then recursively expands each undefined function until eventually terminating with only calls to the existing APIs imported from the robot's low-level action and perception libraries. These existing libraries also encourage the LLM to write reusable, composable code.

Our key contributions are:

1. A method that first recursively summarizes demonstrations to a specification and then recursively expands specification to robot code via an extended chain-of-thought prompt.
2. A novel game simulator, Robotouille, designed to generate cooking tasks that are complex, long-horizon, and involve diverse food items, for benchmarking task code generation.
3. Comparisons against a range of baselines, including prior state of the art [33], on a manipulation benchmark, Robotouille, as well as a real-world human activity dataset.

## 2 Related Work

Controlling robots from natural language has a rich history [74; 66; 37], primarily because it provides a natural means for humans to interact with robots [5; 30]. Recent work on this topic can be categorized as semantic parsing [39; 30; 69; 67; 55; 40; 68], planning [60; 22; 23; 24; 61; 35; 34; 28], task specification [64; 19; 58; 12], reward learning [46; 56; 7], learning low-level policies [46; 2; 57; 56; 7], imitation learning [25; 38; 58; 64] and reinforcement learning [26; 18; 10; 45; 1]. However, these approaches fall in one of two categories: generating open-loop action sequences, or learning closed-loop, but short-horizon, policies. In contrast, we look to generate _task code_, which is promising in solving long-horizon tasks with control flows. The generated code also presents an interpretable way to control robots while maintaining the ability to generalize by composing existing functions.

Synthesizing code from language too has a rich history. Machine learning approaches offer powerful techniques for program synthesis [49; 4; 14]. More recently, these tasks are extended to general-purpose programming languages [79; 78; 8], and program specifications are fully described in natural English text [21; 3; 51]. Pretrained language models have shown great promise in code generation by exploiting the contextual representations learned from massive data of codes and texts [16; 11; 72; 71; 9; 47]. These models can be trained on non-MLE objectives [20], such as RL [32] to pass unit tests. Alternatively, models can also be improved through prompting methods

Figure 1: Overview of Demo2Code that converts language instruction and demonstrations to task code that the robot can execute. The framework recursively summarizes both down to a specification, then recursively expands the specification to an executable task code with all the helper functions defined.

such as Least-to-Most [82], Think-Step-by-Step [29] or Chain-of-Thought [73], which we leverage in our approach. Closest to our approach is CodeAsPolicies [33], that translates language to robot code. We build on it to address the more challenging problem of going from few demonstrations to code.

We broadly view our approach as inverting the output of code. This is closely related to _inverse graphics_, where the goal is to generate code that has produced a given image or 3D model [76; 36; 15; 70; 17]. Similar to our approach [65] trains an LSTM model that takes as input multiple demonstrations, compresses it to a latent vector and decodes it to domain specific code. Instead of training custom models to generate custom code, we leverage pre-trained LLMs that can generalize much more broadly, and generate more complex Python code, even create new functions. Closest to our approach [77] uses pre-trained LLMs to summarize demonstrations as rules in _one step_ before generating code that creates a sequence of pick-then-place and pick-then-toss actions. However, they show results on short-horizon tasks with small number of primitive actions. We look at more complex, long-horizon robot tasks, where demonstrations cannot be summarized in one step. We draw inspiration from [75; 50; 43] to recursively summarize demonstrations until they are compact.

## 3 Problem Formulation

We look at the concrete setting where a robot must perform a set of everyday tasks in a home, like cooking recipes or washing dishes, although our approach can be easily extended to other settings. We formalize such tasks as a Markov Decision Process (MDP), \(<\mathcal{S},\mathcal{A},\mathcal{T},\mathcal{R}>\), defined below:

* **State** (\(s\in\mathcal{S}\)) is the set of all objects in the scene and their propositions, e.g. open(obj) ("obj is open"), on-top(obj1, obj2) ("obj1 is on top of obj2").
* **Action** (\(a\in\mathcal{A}\)) is a primitive action, e.g. pick(obj) ("pick up obj"), place(obj, loc) ("place obj on loc"), move(loc1, loc2) ("move from loc1 to loc2").
* **Transition function** (\(\mathcal{T}(.|s,a)\)) specifies how objects states and agent changes upon executing an action. The transition is stochastic due to hidden states, e.g. cut('lettuce') must be called a variable number of times till the state changes to is_cut('lettuce').
* **Reward function** (\(r(s,a)\)) defines the task, i.e. the subgoals that the robot must visit and constraints that must not be violated.

We assume access to state-based demonstrations because most robotics system have perception modules that can parse raw sensor data into predicate states [42; 27]. We also assume that a system engineer provides a perception library and an action library. The perception library uses sensor observations to maintain a set of state predicates and provides helper functions that use these predicates (e.g. get_obj_location(obj), is_cooked(obj)). Meanwhile, the action library defines a set of actions that correspond to low-level policies, similar to [33; 61; 77; 80].

The goal is to learn a policy \(\pi_{\theta}\) that maximizes cumulative reward \(J(\pi_{\theta})=\mathbb{E}_{\pi_{\theta}}\left[\sum_{t=1}^{T}[r(s_{t},a_{t})]\right]\), \(\theta\) being the parameters of the policy. We choose to represent the policy as code \(\theta\) for a number of reasons: code is interpretable, composable, and verifiable.

In this setting, the reward function \(r(s,a)\) is not explicit, but implicit in the task specification that the user has in mind. Unlike typical Reinforcement Learning (RL), where the reward function is hand designed, it is impractical to expect everyday users to program such reward functions for every new task that they want to teach their robots. Instead, they are likely to communicate tasks through natural means of interaction such as language instructions \(l\) (e.g. "Make a burger"). We can either use a model to generate reward \(r(s,a)\) from \(l\)[31] or directly generate the optimal code \(\theta\)[33].

However, language instructions \(l\) from everyday users can be challenging to map to precise robot instructions [63; 44; 81]: they may be difficult to ground, may lack specificity, and may not capture users' intrinsic preferences or hidden constraints of the world. For example, the user may forget to specify how they wanted their burger done, what toppings they preferred, etc. Providing such level of detail through language every time is taxing. A more scalable solution is to pair the language instruction \(l\) with demonstrations \(\mathcal{D}=\{s_{1},s_{2},\ldots,s_{T}\}\) of the user doing the task. The state at time-step \(t\) only contains the propositions that have changed from \(t-1\) to \(t\). Embedded in the states are specific details of how the user wants a task done.

Our goal is to infer the most likely code given both the language and the demonstrations: \(\arg\max_{\theta}P(\theta|l,\mathcal{D})\). For a long-horizon task like cooking, each demonstration can become long. Naively concatenating all demonstrations together to query the LLM can either exhaust the model's

[MISSING_PAGE_EMPTY:4]

The goal of this stage is to summarize the set of demonstrations provided by the user into a compact specification (refer to summarize(demos) in Algorithm 1). Each demonstration is first independently summarized until the LLM determines that the demonstration can no longer be compressed, then the summaries are concatenated and summarized together. Fig. 2 shows example interim outputs during this stage. First, states in each demonstration get summarized into low-level actions (e.g. "patty6 is cooked" is summarized as "robot1 cooked patty6.") Then, low-level actions across time-steps are summarized into high-level subtasks, such as stacking, cutting, (e.g. "At state 3-8, the high level subtask is cook..."). The LLM determines to stop recursively summarizing after the entire demonstration gets converted to high-level subtasks, but this can have a different stopping condition (e.g. setting a maximum step) for task settings different than Fig. 2's. Next, these demonstrations' summaries are concatenated together for the LLM to generate the task specification. The LLM is prompted to first perform some intermediate reasoning to extract details on personal preferences, possible control loops, etc. For instance, the LLM aggregates high-level subtasks into an ordered list, which empirically helps the model to identify repeated subsets in that list and reason about control loops. An example final specification is shown in Fig. 2, which restates the language instruction first, then states "Specifically:.." followed by a more detailed instruction of the task.

### Stage 2: Recursively Expand Specification to Task Code

The goal of this stage is to use the generated specification from stage 1 to define all the code required for the task (see expand_code(code) in Algorithm 1). The LLM is prompted to first generate high-level code that calls functions that may be undefined. Subsequently, each of these undefined functions in the code is recursively expanded. Fig. 3 shows an example process of the code generation pipeline. The input is the specification formatted as a docstring. We import custom robot perception and control libraries for the LLM and also show examples of how to use such libraries in the prompt. The LLM first generates a high-level code, _that can contain new functions_, e.g. cook_obj_at_loc, that it has not seen in the prompt or import statements before. It expands this code by calling additional functions (e.g. move_then_pick), which it defines in the next recursive step. The LLM eventually reaches the base case when it only uses imported APIs to define a function (e.g. move_then_pick).

## 5 Experiments

### Experimental Setup

Baselines and MetricsWe compare our approach **Demo2Code** against prior work, CodeAsPolicies [33], which we call **Lang2Code**. This generates code only from language instruction. We also compare against **DemoNoLang2Code** that generates code from demonstrations without a language instruction, which is achieved by modifying the LLM prompts to redact the language. Finally, we also compare to an oracle **Spec2Code**, which generates task code from detailed specifications on how to complete a task. We use gpt-3.5-turbo-16k for all experiments with temperature 0.

We evaluate the different methods across three metrics. **Execution Success Rate** is the average 0/1 success of whether the generated code can run without throwing an error. It is independent from whether the goal was actually accomplished. **Unit Test Pass Rate** is based on checking whether all subgoals are achieved and all constraints are satisfied. The unit test module checks by examining the state transitions created from executing the generated code. **Code BLEU score** is the BLEU

Figure 3: Recursive expansion of the high-level code generated from the specification, where new functions are defined by the LLM along the way. (Stage 2)

score [48] between a method's generated code and the oracle Spec2Code's generated code. We tokenize each code by the spaces, quotations, and new lines.

Tabletop Manipulation Simulator [80, 23]We build upon a physics simulator from [80, 23], which simulates a robot arm manipulating blocks and cylinders in different configurations. The task objectives are to place objects at specific locations or stack objects on top of each other. The LLM has access to action primitives (e.g. pick and place) and perception modules (e.g. to get all the objects in the scene). We create a range of tasks that vary in complexity and specificity, use the oracle Spec2Code to generate reference code, and execute that code to get demonstrations for other methods. For each task, we test the generated code for 10 random initial conditions of objects.

\begin{table}
\begin{tabular}{l c|c c c c|c c c c c} \hline \hline \multirow{2}{*}{Task} & \multicolumn{3}{c}{Lang2Code[33]} & \multicolumn{3}{c}{DemoNoLang2Code} & \multicolumn{3}{c}{Demo2Code(ours)} \\  & Exec. & Pass. & BLEU. & Exec. & Pass. & BLEU. & Exec. & Pass. & BLEU. \\ \hline \multirow{4}{*}{Cock a party} & Place A next to B & 1.00 & 0.33 & 0.73 & 0.90 & 0.80 & 0.82 & 1.00 & 1.00 & 0.98 \\  & Place A at a corner of the table & 1.00 & 0.30 & 0.08 & 1.00 & 1.00 & 0.85 & 1.00 & 1.00 & 1.00 \\  & Place A at an edge of the table & 1.00 & 0.20 & 0.59 & 1.00 & 0.95 & 0.84 & 1.00 & 1.00 & 0.84 \\ \hline \multirow{4}{*}{Cock a party} & Place A on top of B & 1.00 & 0.03 & 0.23 & 0.60 & 0.70 & 0.56 & 0.90 & 0.40 & 0.40 \\  & Stack all blocks & 1.00 & 0.20 & 0.87 & 1.00 & 0.70 & 0.50 & 1.00 & 0.70 & 0.50 \\  & Stack all cylinders & 1.00 & 0.37 & 0.89 & 1.00 & 0.83 & 0.49 & 1.00 & 1.00 & 1.00 \\ \hline \multirow{4}{*}{Cock a party} & Stack all blocks into one stack & 1.00 & 0.13 & 0.07 & 1.00 & 0.67 & 0.52 & 1.00 & 0.87 & 0.71 \\  & Stack all cylinders into one stack & 1.00 & 0.13 & 0.00 & 0.90 & 0.77 & 0.19 & 1.00 & 0.90 & 0.58 \\ \cline{1-1}  & Stack all objects into two stacks & 1.00 & 0.00 & 0.00 & 1.00 & 0.90 & 0.68 & 1.00 & 0.90 & 0.65 \\ \hline \multirow{4}{*}{Cock a party} & Overall & 1.00 & 0.19 & 0.39 & 0.93 & 0.81 & 0.60 & 0.99 & 0.88 & 0.77 \\ \cline{1-1}  & & & & & & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results for Tabletop Manipulation simulator. The tasks are categories into 3 clusters: Specificity (“Specific”), Hidden World Constraint (“Hidden”), and Personal Preference (“Pref”).

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline Task & \multicolumn{3}{c}{Lang2Code[33]} & \multicolumn{3}{c}{DemoNoLang2Code} & \multicolumn{3}{c}{Demo2Code(ours)} & Horizon \\  & Exec. & Pass. & BLEU. & Exec. & Pass. & BLEU. & Exec. & Pass. & BLEU. & Length \\ \hline Cook a party & 1.00 & 1.00 & 0.90 & 1.00 & 1.00 & 0.90 & 1.00 & 1.00 & 0.90 & 8.0 \\ Cook two paties & 0.80 & 0.80 & 0.92 & 0.80 & 0.80 & 0.92 & 0.80 & 0.80 & 0.92 & 16.0 \\ Stack a top ban on top of a cut lepton Run & 1.00 & 1.00 & 0.70 & 0.00 & 0.00 & 0.75 & 1.00 & 1.00 & 0.60 & 14.0 \\ \hline Cut a lettuce & 1.00 & 1.00 & 0.87 & 0.00 & 0.00 & 0.76 & 1.00 & 1.00 & 0.87 & 7.0 \\ Cut two lettences & 0.80 & 0.80 & 0.92 & 0.00 & 0.00 & 0.72 & 0.80 & 0.80 & 0.92 & 14.0 \\ Cook first then cut & 1.00 & 1.00 & 0.88 & 1.00 & 1.00 & 0.88 & 1.00 & 1.00 & 0.88 & 14.0 \\ Cut first one cook & 1.00 & 1.00 & 0.88 & 0.00 & 0.00 & 0.82 & 1.00 & 1.00 & 0.88 & 15.0 \\ Assemble two barges one by one & 0.00 & 0.00 & 0.34 & 1.00 & 1.00 & 0.77 & 1.00 & 1.00 & 0.76 & 15.0 \\ Assemble two barges in parallel & 0.00 & 0.00 & 0.25 & 1.00 & 1.00 & 0.51 & 0.00 & 0.00 & 0.71 & 15.0 \\ Make a cheese buyer & 1.00 & 0.00 & 0.04 & 1.00 & 1.00 & 0.69 & 1.00 & 1.00 & 0.69 & 18.0 \\ Make a chicken burger & 0.00 & 0.00 & 0.57 & 0.00 & 0.00 & 0.64 & 0.90 & 0.90 & 0.69 & 25.0 \\ Make a barger stacking lettuce aop party immediately & 1.00 & 0.00 & 0.74 & 0.20 & 0.00 & 0.71 & 0.00 & 0.00 & 0.71 & 24.5 \\ Make a burger stacking party auto lettuce immediately & 0.00 & 0.00 & 0.74 & 0.20 & 0.00 & 0.71 & 1.00 & 1.00 & 0.74 & 25.0 \\ Make a burger stacking lettuce aop party after preparation & 1.00 & 0.00 & 0.67 & 0.10 & 0.00 & 0.65 & 0.00 & 0.00 & 0.66 & 26.5 \\ Make a burger stacking party auto lettuce after preparation & 1.00 & 0.00 & 0.67 & 0.00 & 0.00 & 0.53 & 1.00 & 0.00 & 0.69 & 27.0 \\ Make a lettuce tomato burgers & 0.00 & 0.00 & 0.13 & 1.00 & 1.00 & 0.85 & 1.00 & 0.00 & 0.66 & 34.0 \\ Make two cheese barges & 0.00 & 0.00 & 0.63 & 1.00 & 1.00 & 0.68 & 1.00 & 1.00 & 0.68 & 38.0 \\ Make two chicken burgers & 0.00 & 0.00 & 0.52 & 0.00 & 0.00 & 0.68 & 1.00 & 0.00 & 0.56 & 50.0 \\ Make two burgers stacking lettuce aop party immediately & 0.80 & 0.00 & 0.66 & 0.80 & 1.00 & 0.69 & 0.00 & 0.00 & 0.66 & 50.0 \\ Make two burgers stacking party auto lettuce immediately & 0.80 & 0.00 & 0.67 & 1.00 & 0.00 & 0.48 & 1.00 & 1.00 & 0.73 & 50.0 \\ Make two burgers stacking lettuce aop party after preparation & 0.80 & 0.00 & 0.66 & 0.00 & 0.00 & 0.66 & 0.80 & 0.00 & 0.67 & 54.0 \\ Make two burgers stacking party auto lettuce after preparation & 0.80 & 0.00 & 0.67 & 0.50 & 0.00 & 0.71 & 0.80 & 0.00 & 0.68 & 54.0 \\ Make two lettuce tomato burgers & 1.00 & 0.00 & 0.55 & 0.00 & 0.00 & 0.70 & 1.00 & 1.00 & 0.84 & 70.0 \\ \hline Overall & 0.64 & 0.29 & 0.64 & 0.49 & 0.38 & 0.71 & 0.79 & 0.59 & 0.74 & 28.8 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results for the Robotouille simulator. The training tasks in the prompt are at the top of the table and highlighted in gray. All tasks are ordered by the horizon length (the number of states). Below the table shows four Robotouille tasks where the environments gradually become more complex.

Cooking Task Simulator: Robotouille1 We introduce a novel, open-source simulator to simulate complex, long-horizon cooking tasks for a robot, e.g. making a burger by cutting lettuce and cooking parties. Unlike existing simulators that focus on simulating physics or sensors, Robotouille focuses on high level task planning and abstracts away other details. We build on a standard backend, PDDLGym [59], with a user-friendly game as the front end to easily collect demonstrations. For the experiment, we create a set of tasks, where each is associated with a set of preferences (e.g. what a user wants in the burger, how the user wants the burger cooked). For each task and each associated preference, we procedurally generate 10 scenarios.

EPIC-Kitchens Dataset [13]EPIC-Kitchens is a real-world, egocentric video dataset of users doing tasks in their kitchen. We use this to test if Demo2Code can infer users' preferences from real videos, with the hopes of eventually applying our approach to teach a real robot personalized tasks. We focus on dish washing as we found preferences in it easy to qualify. While each video has annotations of low-level actions, these labels are insufficient for describing the tasks. Hence, we choose \(7\) videos of \(4\) humans washing dishes and annotate each demonstration with dense state information. We compare the code generated by Lang2Code, DemoNoLang2Code and Demo2Code on whether it satisfies the annotated preference and how well it matches against the reference code.

### Results and Analysis

Overall, Demo2Code has the closest performance to the oracle (Spec2Code). Specifically, our approach has the highest unit test pass rates in all three benchmarks, as well as the highest execution success in Robotouille (table 2) and EPIC-Kitchens (table 3). Meanwhile, Lang2Code[33] has a higher overall execution success than Demo2Code for the Tabletop simulator (table 1). However, Lang2Code has the lowest unit test pass rate among all baselines because it cannot fully extract users' specifications without demonstrations. DemoNoLang2Code has a relatively higher pass rate, but it sacrifices execution success because it is difficult to output plausible code without context from language. We provide prompts, detailed results and ablations in the Appendix.2 We now ask a series of questions of the results to characterize the performance difference between the approaches.

Footnote 2: Codebase is available here: https://github.com/portal-cornell/demo2code

How well does Demo2Code generalize to unseen objects and tasks?Demo2Code exhibits its generalization ability in three axes. First, Demo2Code generalizes and solves unseen tasks with longer horizons and more predicates compared to examples in the prompt at train time. For Robotouille, table 2 shows the average horizon length for each training task (highlighted in gray) and testing task. Overall, the training tasks have an average of 12.7 states compared the testing tasks (31.3 states). Compared to the baselines, Demo2Code performs the best for long burger-making tasks (an average of 32 states) even though the prompt does not show this type of task. Second, Demo2Code uses control flow, defines hierarchical code, and composes multiple subtasks together to solve these long-horizon tasks. The appendix details the average number of loops, conditionals, and helper functions that Demo2Code generates for tabletop simulator (in section 8.3) and Robotouille (in section 9.3). Notably, Demo2Code generates code that uses a for-loop for the longest task (making two lettuce tomato burgers with 70 states), which requires generalizing to unseen subtasks (e.g. cutting tomatoes) and composing 7 distinct subtasks. Third, Demo2Code solves tasks that contain unseen objects or a different number of objects compared to the training tasks in the prompt. For Robotouille, the prompt only contains examples of preparing burger patties and lettuce, but Demo2Code still has the highest unit test pass rate for making burgers with unseen ingredients: cheese, chicken, and

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{P4-101 (\(\tau\))} & \multicolumn{2}{c}{P7-04 (17)} & \multicolumn{2}{c}{P7-10 (6)} & \multicolumn{2}{c}{P22-05 (28)} & \multicolumn{2}{c}{P22-07 (30)} & \multicolumn{2}{c}{P30-07 (11)} & \multicolumn{2}{c}{P30-08 (16)} \\ \cline{2-13}  & Pass. & BLEU & Pass. & BLEU & Pass. & BLEU & Pass. & BLEU & Pass. & BLEU & Pass. & BLEU & Pass. & BLEU \\ \hline Lang2Code[33] & 1.00 & 0.58 & 0.00 & 0.12 & 0.00 & 0.84 & 0.00 & 0.48 & 0.00 & 0.37 & 1.00 & 0.84 & 0.00 & 0.66 \\ DenoNlang2Code & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 & 0.00 & 0.37 & 0.00 & 0.51 & 1.00 & 0.57 & 0.00 & 0.00 \\ Demo2Code & 1.00 & 0.33 & 0.00 & 0.19 & 1.00 & 0.63 & 1.00 & 0.43 & 1.00 & 0.66 & 1.00 & 0.58 & 0.00 & 0.24 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results for EPIC-Kitchens dataset on 7 different user demonstrations of dish-washing (length of demonstration in parentheses). The unit test pass rate is evaluated by a human annotator, and BLEU score is calculated between each method’s code and the human annotator’s reference code.

tomatoes. Similarly, for tabletop (table 1), although the prompt only contains block-stacking tasks, our approach maintains high performance for cylinder-stacking tasks.

Is Demo2Code able to ground its tasks using demonstrations?Language instructions sometimes cannot ground the tasks with specific execution details. Since demonstrations provide richer information about the task and the world, we evaluate whether Demo2Code can utilize them to extract details. Tasks under the "Specific" cluster in Table 1 show cases when the LLM needs to use demonstrations to ground the desired goal. Fig. 4 illustrates that although the language instruction ("Place the purple cylinder next to the green block") does not ground the desired spatial relationship between the two objects, our approach is able to infer the desired specification ("to the left"). In contrast, Lang2Code can only randomly guess a spatial relationship, while DemoNoLang2Code can determine the relative position, but it moved the green block because it does not have language instruction to ground the overall task. Similarly, tasks under the "Hidden" cluster in Table 1 show how Demo2Code outperforms others in inferring hidden constraints (e.g the maximum height of a stack) to ground its tasks.

Is Demo2Code able to capture individual user preference?As a pipeline for users to teach robots personalized tasks, Demo2Code is evaluated on its ability to extract a user's preference. Table 3 shows that our approach performs better than Lang2Code in generating code that matches each EPIC-Kitchens user's dish washing preference, without overfitting to the demonstration like in DemoNoLang2Code. Because we do not have a simulator that completely matches the dataset, human annotators have to manually inspect the code. The code passes the inspection if it has correct syntax, does not violate any physical constraints (e.g. does not rinse a dish without turning on the tap), and matches the user's dish-washing preference. Qualitatively, Fig. 6 shows that our approach is able to extract the specification and generate the correct code respectively for user 22, who prefers to soap

Figure 4: Demo2Code successfully extracts specificity in tabletop tasks. Lang2Code lacks demonstrations and randomly chooses a spatial location while DemoNoLang2Code lacks context in what the demonstrations are for.

Figure 5: Demo2Code summarizes demonstrations and identify different users’ preferences on how to make a burger (e.g. whether to include lettuce or cheese) in Robotouille simulator. Then, it generates personalized burger cooking code to use the user’s preferred ingredients.

all objects before rinsing them, and user 30, who prefers to soap then rinse each object individually. Similarly, Fig. 5 provides an example of how Demo2Code is able to identify a user's preference of using cheese vs lettuce even when the language instruction is just "make a burger." Quantitatively, Table 2 shows more examples of our approach identifying a user's preference in cooking order, ingredient choice, etc, while Table 1 also shows our approach performing well in tabletop tasks.

How does chain-of-thought compare to directly generating code from demonstrations?To evaluate the importance of our extended chain-of-thought pipeline, we conduct ablation by varying the length of the chain on three clusters of tasks: short-horizon (around 2 states), medium-horizon (5-10 states), and long-horizon (\(\geq 15\) states). We compare the unit test pass rate on four different chain lengths, ranging from **No chain-of-thought** (the shortest), which directly generates code from demonstrations, to **Full** (the longest), which represents our approach Demo2Code. The left bar plot in Fig. 7 shows that directly generating code from demonstrations is not effective, and the LLM performs better as the length of the chain increases. The chain length also has a larger effect on tasks with longer horizons. For short-horizon tasks, the LLM can easily process the short demonstrations and achieve high performances by just using **1-step**. Meanwhile, the stark difference between **2-steps** and **Full**'s results on long-horizon tasks emphasizes the importance of taking as many small steps as the LLM needs in summarizing long demonstrations so that it will not lose key information.

How do noisy demonstrations affect Demo2Code's performance?We study how Demo2Code performs (1) when each predicate has a 10% chance to be removed from the demonstrations, and (2) when each state has a 10% chance to be completely removed. Fig. 7's table shows that Demo2Code's overall performance does not degrade even though demonstrations are missing information. While

Figure 6: Demo2Code summarizes different styles of users washing dishes from demonstration (how to soap and rinse objects) in EPIC-Kitchens, and generates personalized dish washing code.

Figure 7: (Left) Unit test result for ablating different degrees of chain-of-thought across tasks with short, medium, long horizon. (Right) Demo2Code’s unit test result for Robotouille demonstrations with different level of noises: (1) each predicate has 10% chance of being dropped, and (2) each state has 10% chance of being completely dropped. We ran the experiment 4 times and report the average and variance.

removing predicates or states worsen Demo2Code's performance for shorter tasks (e.g. cook and cut), they surprisingly increase the performance for longer tasks. Removing any predicates can omit essential information in shorter tasks' demonstrations. Meanwhile, for longer tasks, the removed predicates are less likely to be key information, while reducing the length of demonstrations. Similarly, for the longest tasks to make two burgers, one burger's missing predicates or states can be explained by the other burger's demonstration. In section 11, we show a specific example of this phenomenon. We also study the effect of adding additional predicates to demonstrations, which has degraded Demo2Code's performance from satisfying 5 users' preferences to 2 users' in EPIC-Kitchens.

## 6 Discussion

In this paper, we look at the problem of generating robot task code from a combination of language instructions and demonstrations. We propose Demo2Code that first recursively summarizes demonstrations into a latent, compact specification then recursively expands code generated from that specification to a fully defined robot task code. We evaluate our approach against prior state-of-the-art [33] that generates code only from language instructions, across 3 distinct benchmarks: a tabletop manipulation benchmark, a novel cooking game Robotouille, and annotated data from EPIC-Kitchens, a real-world human activity dataset. We analyze various capabilities of Demo2Code, such as grounding language instructions, generalizing across tasks, and capturing user preferences.

Demo2Code can generalize across complex, long-horizon tasks. Even though Demo2Code was shown only short-horizon tasks, it's able to generalize to complex, long demonstrations. Recursive summarization compresses long chains of demonstrations and recursive expansion generates complex, multi-layered code.

Demo2Code leverages demonstrations to ground ambiguous language instructions and infer hidden preferences and constraints. The latent specification explicitly searches for missing details in the demonstrations, ensuring they do not get explained away and are captured explicitly in the specification.

Demo2Code strongly leverages chain-of-thought. Given the complex mapping between demonstrations and code, chain-of-thought plays a critical role in breaking down computation into small manageable steps during summarization, specification generation and code expansion.

In future directions, we are looking to close the loop on code generation to learn from failures, integrate with a real home robot system and run user studies with Robotouille.

## 7 Limitations

Demo2Code is limited by the capability of LLMs. Recursive summarization assumes that once all the demonstrations are sufficiently summarized, they can be concatenated to generate a specification. However, in extremely long horizon tasks (e.g. making burgers for an entire day), it is possible that the combination of all the sufficiently summarized demonstrations can still exceed the maximum context length. A future work direction is to prompt the LLM with chunks of the concatenated demonstrations and incrementally improve the specifications based on each new chunk. In recursive expansion, our approach assumes that all low-level action primitives are provided. Demo2Code currently cannot automatically update its prompt to include any new action. Another direction is to automatically build the low-level skill libraries by learning low-level policy via imitation learning and iteratively improve the code-generation prompt over time. Finally, since LLMs are not completely reliable and can hallucinate facts, it is important to close the loop by providing feedback to the LLM when they fail. One solution [62; 52] is to incorporate feedback in the query and reprompt the language model. Doing this in a self-supervised manner with a verification system remains an open challenge.

In addition, the evaluation approach for Demo2Code or other planners that generate code [33; 61; 77] is different from the one for classical planners [53; 54]. Planners that generate code measure a task's complexity by the horizon length, the number of control flows, whether that task is in the training dataset, etc. Meanwhile, many classical planners use domain specific languages such as Linear Temporal Logic (LTL) to specify tasks [41], which leads to categorizing tasks and measuring the task complexity based on LTL. Future work needs to resolve this mismatch in evaluation standards.

## Acknowledgements

We sincerely thank Nicole Thean for creating our art assets for Robotouille. This work was supported in part by the National Science Foundation FRR (#2327973).

## References

* [1] Ahmed Akakzia, Cedric Colas, Pierre-Yves Oudeyer, Mohamed Chetouani, and Olivier Sigaud. Grounding language to autonomously-acquired skills via goal generation. _arXiv:2006.07185_, 2020.
* [2] Jacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language. _arXiv:1711.00482_, 2017.
* [3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. _arXiv:2108.07732_, 2021.
* [4] Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. Deepcoder: Learning to write programs. _arXiv preprint arXiv:1611.01989_, 2016.
* [5] Cynthia Breazeal, Kerstin Dautenhahn, and Takayuki Kanda. Social robotics. _Springer handbook of robotics_, 2016.
* [6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
* [7] Annie S. Chen, Suraj Nair, and Chelsea Finn. Learning Generalizable Robotic Reward Functions from "In-The-Wild" Human Videos. In _Proceedings of Robotics: Science and Systems_, Virtual, July 2021.
* [8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. _arXiv:2107.03374_, 2021.
* [9] Qibin Chen, Jeremy Lacomis, Edward J. Schwartz, Graham Neubig, Bogdan Vasilescu, and Claire Le Goues. Varclr: Variable semantic representation pre-training via contrastive learning, 2021.
* [10] Geoffrey Cideron, Mathieu Seurin, Florian Strub, and Olivier Pietquin. Self-educated language agent with hindsight experience replay for instruction following. _DeepMind_, 2019.
* [11] Colin Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan. PyMT5: multi-mode translation of natural language and python code with transformers. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 9052-9065, Online, November 2020. Association for Computational Linguistics.
* [12] Yuchen Cui, Scott Niekum, Abhinav Gupta, Vikash Kumar, and Aravind Rajeswaran. Can foundation models perform zero-shot task specification for robot manipulation? In _Learning for Dynamics and Control Conference_, pages 893-905. PMLR, 2022.
* [13] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. _International Journal of Computer Vision (IJCV)_, 130:33-55, 2022.
* [14] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy i/o. In _International conference on machine learning_, pages 990-998. PMLR, 2017.
* [15] Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Josh Tenenbaum. Learning to infer graphics programs from hand-drawn images. _Advances in neural information processing systems_, 31, 2018.

* [16] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre-trained model for programming and natural languages. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 1536-1547, Online, November 2020. Association for Computational Linguistics.
* [17] Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SM Ali Eslami, and Oriol Vinyals. Synthesizing programs for images using reinforced adversarial learning. In _International Conference on Machine Learning_, pages 1666-1675. PMLR, 2018.
* [18] Prasoon Goyal, Scott Niekum, and Raymond J Mooney. Pix12r: Guiding reinforcement learning using natural language by mapping pixels to rewards. _arXiv:2007.15543_, 2020.
* [19] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long horizon tasks via imitation and reinforcement learning. _Conference on Robot Learning (CoRL)_, 2019.
* [20] Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, and Percy Liang. From language to programs: Bridging reinforcement learning and maximum marginal likelihood. _arXiv preprint arXiv:1704.07926_, 2017.
* [21] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. _arXiv preprint arXiv:2105.09938_, 2021.
* [22] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. _arXiv:2201.07207_, 2022.
* [23] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. In _arXiv:2207.05608_, 2022.
* [24] Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander T Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. Do as i can, not as i say: Grounding language in robotic affordances. In _6th Annual Conference on Robot Learning_, 2022.
* [25] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In _CoRL_, 2022.
* [26] Yiding Jiang, Shixiang Shane Gu, Kevin P Murphy, and Chelsea Finn. Language as an abstraction for hierarchical deep reinforcement learning. _NeurIPS_, 2019.
* [27] Kei Kase, Chris Paxton, Hammad Mazhar, Tetsuya Ogata, and Dieter Fox. Transferable task execution from pixels through deep planning domain learning, 2020.
* [28] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks, 2023.
* [29] Takeshi Kojima, Shixiang Shane Gu, Michel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. _arXiv:2205.11916_, 2022.
* [30] Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas Roy. Toward understanding natural language directions. In _HRI_, 2010.

* [31] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models. _arXiv preprint arXiv:2303.00001_, 2023.
* [32] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven CH Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. _arXiv:2207.01780_, 2022.
* [33] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. _arXiv preprint arXiv:2209.07753_, 2022.
* [34] Bill Yuchen Lin, Chengsong Huang, Qian Liu, Wenda Gu, Sam Sommerer, and Xiang Ren. On grounded planning for embodied tasks with language models, 2023.
* [35] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2motion: From natural language instructions to feasible plans, 2023.
* [36] Yunchao Liu, Jiajun Wu, Zheng Wu, Daniel Ritchie, William T. Freeman, and Joshua B. Tenenbaum. Learning to describe scenes with programs. In _International Conference on Learning Representations_, 2019.
* [37] Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob N. Foerster, Jacob Andreas, Edward Grefenstette, S. Whiteson, and Tim Rocktaschel. A survey of reinforcement learning informed by natural language. In _IJCAI_, 2019.
* [38] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data. _arXiv:2005.07648_, 2020.
* [39] Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. _AAAI_, 2006.
* [40] Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and Dieter Fox. Learning to parse natural language commands to a robot control system. In _Experimental robotics_, 2013.
* [41] Claudio Menghi, Christos Tsigkanos, Patrizio Pelliccione, Carlo Ghezzi, and Thorsten Berger. Specification patterns for robotic missions, 2019.
* [42] Toki Migimatsu and Jeannette Bohg. Grounding predicates through actions, 2022.
* [43] Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. Multi-hop reading comprehension through question decomposition and rescoring. _arXiv preprint arXiv:1906.02916_, 2019.
* [44] Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin, and Yoav Artzi. Mapping instructions to actions in 3d environments with visual goal prediction. _arXiv preprint arXiv:1809.00786_, 2018.
* [45] Dipendra Kumar Misra, John Langford, and Yoav Artzi. Mapping instructions and visual observations to actions with reinforcement learning. _CoRR_, abs/1704.08795, 2017.
* [46] Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, et al. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. In _CoRL_, 2022.
* [47] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. _arXiv preprint arXiv:2203.13474_, 2022.
* [48] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_, pages 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.
* [49] Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro-symbolic program synthesis. _arXiv preprint arXiv:1611.01855_, 2016.

* [50] Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. Unsupervised question decomposition for question answering. _arXiv preprint arXiv:2002.09758_, 2020.
* [51] Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained language models. In _International Conference on Learning Representations_, 2022.
* [52] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius, and Stefanie Tellex. Planning with large language models via corrective re-prompting, 2022.
* [53] Ankit Shah, Pritish Kamath, Julie A Shah, and Shen Li. Bayesian inference of temporal task specifications from demonstrations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [54] Ankit Shah, Shen Li, and Julie Shah. Planning with uncertain specifications (PUnS). _IEEE Robotics and Automation Letters_, 5(2):3414-3421, apr 2020.
* [55] Dhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine. Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action, 2022.
* [56] Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette Bohg. Concept2robot: Learning manipulation concepts from instructions and human demonstrations. In _Proceedings of Robotics: Science and Systems (RSS)_, 2020.
* [57] Pratyusha Sharma, Balakumar Sundaralingam, Valts Blukis, Chris Paxton, Tucker Hermans, Antonio Torralba, Jacob Andreas, and Dieter Fox. Correcting robot plans with natural language feedback. _arXiv:2204.05186_, 2022.
* [58] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In _CoRL_, 2021.
* [59] Tom Silver and Rohan Chitnis. Pddlgym: Gym environments from pddl problems, 2020.
* [60] Tom Silver, Varun Hariprasad, Reece S Shuttleworth, Nishanth Kumar, Tomas Lozano-Perez, and Leslie Pack Kaelbling. PDDL planning with pretrained large language models. In _NeurIPS 2022 Foundation Models for Decision Making Workshop_, 2022.
* [61] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models, 2022.
* [62] Marta Skreta, Naruki Yoshikawa, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bjorn Kristensen, Kourosh Darvish, Alan Aspuru-Guzik, Florian Shkurti, and Animesh Garg. Errors are useful prompts: Instruction guided task programming with verifier-assisted iterative prompting, 2023.
* [63] Shawn Squire, Stefanie Tellex, Dilip Arumugam, and Lei Yang. Grounding english commands to reward functions. In _Robotics: Science and Systems_, 2015.
* [64] Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, and Heni Ben Amor. Language-conditioned imitation learning for robot manipulation tasks. _NeurIPS_, 2020.
* [65] Shao-Hua Sun, Hyeonwoo Noh, Sriram Somasundaram, and Joseph Lim. Neural program synthesis from diverse demonstration videos. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 4790-4799. PMLR, 10-15 Jul 2018.
* [66] Stefanie Tellex, Nakul Gopalan, Hadas Kress-Gazit, and Cynthia Matuszek. Robots that use language. _Review of Control, Robotics, and Autonomous Systems_, 2020.
* [67] Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew Walter, Ashis Banerjee, Seth Teller, and Nicholas Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In _AAAI_, 2011.

* [68] Jesse Thomason, Aishwarya Padmakumar, Jivko Sinapov, Nick Walker, Yuqian Jiang, Harel Yedidsion, Justin Hart, Peter Stone, and Raymond Mooney. Jointly improving parsing and perception for natural language commands through human-robot dialog. _JAIR_, 2020.
* [69] Jesse Thomason, Shiqi Zhang, Raymond Mooney, and Peter Stone. Learning to interpret natural language commands through human-robot dialog. In _Proceedings of the 2015 International Joint Conference on Artificial Intelligence (IJCAI)_, pages 1923-1929, Buenos Aires, Argentina, July 2015.
* [70] Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T. Freeman, Joshua B. Tenenbaum, and Jiajun Wu. Learning to infer and execute 3d shape programs. In _International Conference on Learning Representations_, 2019.
* [71] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
* [72] Yue Wang, Weishi Wang, Shafiq Joty, and Steven C. H. Hoi. Codel5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation, 2021.
* [73] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv:2201.11903_, 2022.
* [74] Terry Winograd. Procedures as a representation for data in a computer program for understanding natural language. _MIT PROJECT MAC_, 1971.
* [75] Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. Recursively summarizing books with human feedback. _arXiv:2109.10862_, 2021.
* [76] Jiajun Wu, Joshua B. Tenenbaum, and Pushmeet Kohli. Neural scene de-rendering. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, July 2017.
* [77] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance with large language models, 2023.
* [78] Xiaojun Xu, Chang Liu, and Dawn Song. SQLNet: Generating structured queries from natural language without reinforcement learning, 2018.
* [79] Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 440-450, Vancouver, Canada, July 2017. Association for Computational Linguistics.
* [80] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. _arXiv:2204.00598_, 2022.
* [81] Luke S Zettlemoyer and Michael Collins. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. _arXiv preprint arXiv:1207.1420_, 2012.
* [82] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. _arXiv:2205.10625_, 2022.

## Appendix

### Table of Contents

* 8 Tabletop Manipulation Simulator Pipeline
	* 8.1 Pipeline Overview
	* 8.2 Experiment Setup
	* 8.3 Characterize Tabletop Tasks' Complexity
* 9 Robotouille Simulator Pipeline
	* 9.1 Overview
	* 9.2 Experiment Setup
	* 9.3 Characterize Robotouille's Tasks' Complexity
* 10 EPIC-Kitchens Pipeline
	* 10.1 Annotations
	* 10.2 Pipeline Overview
* 11 Noisy Demonstration Ablation Experiment
	* 11.1 Randomly removing predicates/states
	* 11.2 Randomly removing predicates/states
	* 11.3 Quantitative Analysis
	* 11.4 Qualitative Analysis
* 12 Chain-of-thought Ablation Experiment
	* 12.1 Experiment Detail
	* 12.2 Quantitative Result
	* 12.3 Qualitative example for a short-horizon task
	* 12.4 Qualitative example for a medium-horizon task
	* 12.5 Qualitative example for a long-horizon task
* 13 Intermediate Reasoning Ablation Experiment
	* 13.1 Experiment detail
	* 13.2 Quantitative result
	* 13.3 Qualitative example
* 14 Recursive Expansion Ablation Experiment
	* 14.1 Experiment detail
	* 14.2 Quantitative result
	* 14.3 Qualitative example
* 15 Broader Impact
* 16 Reproducibility
* 17 Demo2Code Example Output
	* 17.1 Tabletop Simulator Example
	* 17.2 Robotouille Example
	* 17.3 EPIC-Kitchens Example
* 18 Prompts
	* 18.1 Tabletop Manipulation Task Prompts
	* 18.2 Robotouille Task Prompts
18.3 EPIC Kitchens Task Prompts
19 Other Long Examples 19.1 Example Robotouille Query 19.2 Example EPIC-Kitchens Query 19.3 Intermediate Reasoning Ablation Helper Functions

## 8 Tabletop Manipulation Simulator Pipeline

### Pipeline Overview

The tabletop manipulation simulator contains simple tasks. Consequently, the demonstrations do not have too many states (\(\leq\) 8 states) and the code is not complex. Thus, Demo2Code's prompt for this domain does not need a long extended chain-of-thought. In stage 1 recursive summarization, the LLM just needs to summarize each states into a sentences that describes the low-level action (e.g. move, pick, place, etc.) In stage 2 recursive expansion, because the code is simple, the LLM can directly use all the low-level actions that are provided to output the task code given a specification.

The prompt demonstrating this pipeline is listed at the end of the appendix in section 18.1.

### Experiment Setup

In the paper, we categorize the tabletop tasks into three clusters. For each cluster, we list all the tasks and their variants of possible requirements below. The tasks that are used in the prompt are bolded.

* Place A next to B
* **No hidden specificity: A can be placed in any relative position next to B*
* **A must be to the left of B*
* A must be to the right of B
* A must be behind B
* A must be in front of B
* Place A at a corner of the table
* **No hidden specificity: A can be placed at any corner.*
* A must be at the top left corner
* A must be at the top right corner
* A must be at the bottom left corner
* A must be at the bottom right corner
* Place A at an edge of the table
* No hidden specificity: A can be placed at any corner.
* A must be at the top edge
* A must be at the top edge
* A must be at the left edge
* A must be at the right edge
* Hidden Constraint
* Place A on top of B
* **No hidden constraint: A can be directly placed on top of B in one step*
* There is 1 additional object on top of A, so that needs to be removed before placing A on top of B.
* There are 2 additional objects on top of A.
* **There are 3 additional objects on top of A.*
* Stack all blocks
* **No hidden constraint: All blocks can be stacked into one stack*
* Each stack can be at most 2 blocks high
* **Each stack can be at most 3 blocks high*
* Each stack can be at most 4 blocks high
* Stack all cylinders (Same set of hidden constraints as "stack all blocks." None of the examples appears in the prompt.)
* Personal Preference
* Stack all blocks into one stack
* 2 blocks must be stacked in a certain order, and the rest can be unordered
* **3 blocks must be stacked in a certain order*
* All blocks must be stacked in a certain order
* Stack all cylinders into one stack (Same set of hidden constraints as "stack all blocks into one stack" None of the examples appears in the prompt.)
* Stack all objects
* **No hidden preference: The objects do not need to be stacked in to different stacks based on their type*
* All the blocks should be stacked in one stack, and all the cylinders should be stacked in another stack

#### 8.2.1 Provided Low-Level APIs

We have provided the following APIs for the perception library and low-level skill library:

* Perception Library
* get_obj_names(): return a list of objects in the environment
* get_all_obj_names_that_match_type(type_name, objects_list): return a list of objects in the environment that match the type_name.
* determine_final_stacking_order(objects_to_enforce_order, objects_without_order): return a sorted list of objects to stack.
* Low-level Skill Library
* put_first_on_second(arg1, arg2): pick up an object (arg1) and put it at arg2. If arg2 is an object, arg1 will be on top of arg2. If arg2 is 'table', arg1 will be somewhere random on the table. If arg2 is a list, arg1 will be placed at location [x, y].
* stack_without_height_limit(objects_to_stack): stack the list of objects_to_stack into one stack without considering height limit.
* stack_with_height_limit(objects_to_stack, height_limit): stack the list of objects_to_stack into potentially multiple stacks, and each stack has a maximum height based on height_limit.

### Characterize Tabletop Tasks' Complexity

In table 4, we characterize the complexity of the tasks in terms of the demonstrations' length, the code's length, and the expected code's complexity (i.e. how many loops/conditionals/functions are needed to solve this task).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Task & \begin{tabular}{c} Input Demo Length \\ \# of states \\ \end{tabular} & 
\begin{tabular}{c} Code Length \\ \# of predicates \\ \end{tabular} & \# of conditions & \# of functions \\ \hline Place A next to B & 1-1 (1.00) & 2-5 (3.53) & 3-7 (3.38) & 0.00 & 0.02 & 1.00 \\ Place A at corner/edge & 1-1 (1.00) & 1-5 (2.09) & 2-4 (3.03) & 0.00 & 0.00 & 1.00 \\ Place A on top of B & 1.0-4 (0.250) & 3-19 (9.40) & 2-6 (3.65) & 0.10 & 0.00 & 1.00 \\ Stack all blocks/validers & 2.7 (4.43) & 4.33 (14.09) & 3-15 (4.44) & 0.24 & 0.06 & 1.00 \\ Stack all blocks/validers into one stack & 3.5-4 (3.98) & 12:23 (14.77) & 12-12 (12) & 1.00 & 1.00 & 1.00 \\ Stack all objects into two stacks & 6-8 (6.95) & 16-42 (23.90) & 7-25 (8.1) & 0.05 & 0.20 & 1.00 \\ \hline \hline \end{tabular}
\end{table}
Table 4: For tabletop tasks, we group them by cluster and report: 1. number of states in demonstrations (range and average) 2. number of predicates in demonstrations (range and average) 3. number of lines in the oracle Spec2Code’s generated code (range and average) 4. average number of loops 5. average number of conditionals 6. average number of functionsRobotouille Simulator Pipeline

### Overview

#### 9.1.1 Simulator Description

In Robotouille, a robot chef performs cooking tasks in a kitchen environment. The state of the kitchen environment consists of items such as buns, lettuce, and patties located on stations which could be tables, grills, and cutting boards. The actions of the robot consist of moving around from one station to another, picking items from and placing items on stations, stacking items atop and unstacking items from another item, cooking patties on stoves, and cutting lettuce on cutting boards. The state and actions are described through the Planning Domain Description Language (PDDL).

These PDDL files consist of a domain and a problem. The domain file defines an environment; it contains the high-level predicates that describe the state of the world as well as the actions of the world including their preconditions and effects on the world's predicate state. The problem file describes a configuration of an environment; it contains the domain name for the environment, the initial objects and true predicates, and the goal state. These files are used with PDDLGym [59] as a backend to create an OpenAI Gym [6] environment which given a state and action can be stepped through to produce the next state.

There are 4 problem files for different example scenarios including cooking a patty and cutting lettuce, preparing ingredients to make a burger, preparing ingredients to make two burgers, and assembling a burger with pre-prepared ingredients. In a scenario, various different tasks can be carried out, such as varying the order and ingredients for making a burger. These problem files contain the minimum number of objects necessary to complete the scenario for any specified task.

One issue with having pre-defined problem files for each scenario is that the code produced in code generation could be hardcoded for a scenario. This is avoided by procedurally generating the problem files. There are two types of procedural generation: noisy randomization and full randomization. Noisy randomization, which is used for every Robotouille experiment in this paper, ensures that the minimum required objects in a problem file appear in an environment in the same grouped arrangement (so an environment with a robot that starts at a table with a patty on it and a cutting board with lettuce on it will maintain those arrangements) but the locations are all randomized and extra stations and items are added (noise). The location of stations and items determines the ID suffix which prevents code generation from always succeeding using hardcoded code.

Full randomization does everything except enforcing that the minimum required objects in a problem file appear in the same grouped arrangement. This would require code that handles edge cases as simple as utilizing ingredients that are already cooked or cut in the environment rather than preparing new ones to more extreme cases such as the kitchen being cluttered with stacked items requiring solving a puzzle to effectively use the kitchen. The simpler case is more appropriate in a real setting and we leave it to future work to remove initial arrangement conditions.

#### 9.1.2 Pipeline Overview

In stage 1 recursive summarization, the LLM first recursively summarizes the provided demonstrations, which are represented as state changes since the previous state, until it determines that the trajectories are sufficiently summarized. For this domain, the LLM in general terminates after it summarizes the trajectory into a series of high-level subtasks. Then, Demo2Code concatenates all trajectories together before prompting the LLM to reason about invariant in subtask's order before generating the task specification.

In stage 2 recursive expansion, there are 3 steps that occur for Demo2Code. First, (1) the task specification is converted directly to code which uses provided helper functions and may use undefined higher-level functions. Second, (2) the undefined higher-level functions are defined potentially including undefined lower-level functions. Finally, (3) the undefined lower-level functions are unambiguously defined.

The prompt demonstrating this pipeline is listed at the end of the appendix in section 18.2.

Figure 8: Examples of goal states with the respective tasks underneath.

### Experiment Setup

In the paper, we categorized the Robotouille simulator into 4 example scenarios. Below are all the scenarios as well as possible tasks, visualized in Fig. 8.

* Cook a patty and cut lettuce
* Cook a patty
* Cut a lettuce
* Cook first then cut
* Cut first then cook
* Assemble two burgers from prepared ingredients
* Assemble two burgers one by one
* Assemble two burgers in parallel
* Make a burger
* Stack a top bun on top of a cut lettuce on top of a bottom bun
* Make a burger stacking lettuce atop patty immediately
* Make a burger stacking patty atop lettuce immediately
* Make a burger stacking lettuce atop patty after preparation
* Make a burger stacking patty atop lettuce after preparation
* Make a cheese burger
* Make a chicken burger
* Make a lettuce tomato burger
* Make two burgers
* Cook two patties
* Cut two lettuces
* Make two burgers stacking lettuce atop patty immediately
* Make two burgers stacking patty atop lettuce immediately
* Make two burgers stacking lettuce atop patty after preparation
* Make two burgers stacking patty atop lettuce after preparation
* Make two cheese burgers
* Make two chicken burgers
* Make two lettuce tomato burgers

#### 9.2.1 Provided Low-Level APIs

We have provided the following APIs for the perception library and low-level skill library:

* Perception Library
* get_all_obj_names_that_match_type(obj_type): return a list of string of objects that match the obj_type.
* get_all_location_names_that_match_type(location_type): return a list of string of locations that match the location_type.
* is_cut(obj): return true if obj is cut.
* is_cooked(obj): return true if obj is cooked.
* is_holding(obj): return true if the robot is currently holding obj.
* is_in_a_stack(obj): return true if the obj is in a stack.
* get_obj_that_is_underneath(obj_at_top): return the name of the object that is underneath obj_at_top.
* get_obj_location(obj): return the location that obj is currently at.
* get_curr_location(): return the location that the robot is currently at.
* Low-level Skill Library
* move(curr_loc, target_loc): move from the curr_loc to the target_loc.
* pick_up(obj, loc): pick up the obj from the loc.
* place(obj, loc): place the obj on the loc.
* cut(obj): make progress on cutting the obj. Need to call this function multiple times to finish cutting the obj.
* start_cooking(obj): start cooking the obj. Only need to call this once. The obj will take an unknown amount before it is cooked.
* noop(): do nothing.
* stack(obj_to_stack, obj_at_bottom): stack obj_to_stack on top of obj_at_bottom.
* unstack(obj_to_unstack, obj_at_bottom): unstack obj_to_unstack from obj_at_bottom.

### Characterize Robotouille's Tasks' Complexity

In table 5, we characterize the complexity of the tasks in terms of the demonstrations' length, the code's length, and the expected code's complexity (i.e. how many loops/conditionals/functions are needed to solve this task).

In addition, to bridge the different evaluation standards between planners that generate code and classical planners, we also characterize the Robotouille's tasks based on [41]'s taxonomy in table 6

## 10 EPIC-Kitchens Pipeline

### Annotations

We take 9 demonstrations of dishwashing by users 4, 7, 22 and 30, and use 2 of these as _in-context examples_ for the LLM, by writing down each intermediate step's expected output.

\begin{table}
\begin{tabular}{l c c c c c} \hline Task & Global Avoidance & LowerExact Restriction Avoidance & Wall & Instentaneous Reaction & Delayed Reaction \\ \hline Cook a party & & & & & \\ Cook two planners & & & & & \\ Sark a step but no up of a core instance as top of a bottom loan & & & & \\ Cut a lettuce & & & & & \\ Cut two btrees & & & & & \\ Cook first then cut & & & & & \\ Cut flat then cost & & & & & \\ Assemble two buggers one by one & & & & & \\ Assemble two buggers in parallel & & & & & \\ Make a cheese bugger & & & & & \\ Make a bugger stacking instance into query immediately & & & & & \\ Make a bugger stacking party ago places immediately & & & & & \\ Make a bugger stacking party ago places immediately & & & & & \\ Make a bugger stacking party ago places appropriately targeting & & & & & \\ Make a bugger stacking party ago places after preparation & & & & & \\ Make a bugger stacking party ago places after preparation & & & & & \\ Make a bugger stacking party & & & & & \\ Make a bugger stacking party & & & & & \\ Make two buggers & & & & & \\ Make two buggers & & & & & \\ Make two buggers sticking party ago places immediately & & & & & \\ Make two buggers sticking party ago places immediately & & & & & \\ Make two buggers sticking party ago places immediately & & & & & \\ Make two buggers sticking party ago places after preparation & & & & & \\ Make two buggers sticking party ago 

[MISSING_PAGE_FAIL:23]

trajectories are sufficiently summarized. For this domain, the LLM in general terminates after it summarizes the trajectory into a series of high-level subtasks, which each consist of multiple states and low-level actions. For example, low-level actions "Pick up spoon_1", "Pick up fork_1", and "Go from countertop_1 to sink_1" get combined as the subtask "bring spoon_1 and fork_1 from countertop_1 to the sink_1." Then, Demo2Code concatenates all trajectories together before prompting the LLM to reason about the control flow (e.g. whether a for-loop is needed) before generating the task specification.

In stage 2 recursive expansion, because the dishwashing does not use that many unique actions, the LLM is asked to directly use all the low-level actions that are provided as APIs to output the task code given a specification.

The prompt demonstrating this pipeline is listed at the end of the appendix in section 18.3.

#### 10.2.1 Provided Low-Level APIs

We have provided the following APIs for the perception library and low-level skill library:

* get_all_objects(): return a list of objects in the environment.
* bring_objects_to_loc(obj, loc): bring all the objects to loc.
- turn_off(tap_name): turn off tap.
- turn_on(tap_name): turn on tap.
- soap(obj): soap the object.
- rinse(obj): rinse the object.
- pick_up(obj): pick up the object.
- place(obj, loc): pick up the object at loc.
- clean_with(obj, tool): clean the object with the tool, which could be a sponge or a towel.

## 11 Noisy Demonstration Ablation Experiment

As seen in our own annotations for EPIC-Kitchens demonstrations, human annotations or annotations generated by automatic scene summarizers and object detectors may not be noise-free. They may omit some predicates or completely missed predicates in an entire timestep. They may contain objects that the users did not interact with during the demonstration, so predicates about these objects are of little importance to the robot task plan. Thus, we conducted two noisy demonstration ablations:

1. Randomly removing predicates/states from the demonstrations (tested in Robotouille)
2. Randomly adding predicates about irrelevant objects to the demonstrations (tested in EPIC-Kitchens).

We found that:

* Randomly removing predicates/states
* Removing predicates reduces Demo2Code's performance for tasks with short horizons.
* Surprisingly, it does not significantly worsen the performance for tasks with long horizons.
* Randomly adding irrelevant predicates
* Additional irrelevant predicates worsen Demo2Code's performance for correctly generating code for 5 users to 2 users.

### Randomly removing predicates/states

#### 11.1.1 Experimental Details

For each task in Robotouille, we modified the demonstrations in two ways:1. for each predicate in the demonstration, there is a 10% probability that the predicate would be removed from the demonstration.
2. for each state (which could consist of multiple predicates), there is a 10% probability that the entire state would be removed from the demonstration.

We ran the experiment on 4 seeds to report the average and the variance.

#### 11.1.2 Qualitative Result

We analyze a qualitative example (making a burger where the patty needs to be stacked on top of the lettuce immediately after it is cooked) where removing predicates did not affect Demo2Code's performance.

When each predicate has 10% probability of being removed, the demonstration is missing 6 predicates, Half of them omits information such as picking up the lettuce, moved from one location to another location, etc. However, the other half does not omit any information. For example, one of the predicate that gets removed is "'robot1' is not holding 'top_bun3'.

State 26: 'top_bun3' isat'table4' 'top_bun3' isonotopof'patty3' '>>'robot1' isnotholding'top_bun3'<<

Removing this predicate does not lose key information because "'top_bun3' is on top of 'patty3' still indicates that 'top_bun3' has been placed on top of 'patty3'. Consequently, the LLM is still able to summarize for that state:

* Atstate 26, therobotplaced'top_bun3' ontopof'patty3' at location 'table4'.

Thus, Demo2Code is able to generate identical predicates

Using the same seed, when each state has 10% probability of being completely removed, the demonstration is missing 5 states (9 predicates). Because all the predicate in a selected state gets removed, the LLM misses more context. For example, because the following two states are randomly removed, the LLM does not know that the demonstration has moved and placed 'lettuce1' at 'cutting_boarding1'.

State 3: 'lettuce1' isnotat'table2' 'robot1' isholding'lettuce1'

>>>>State 4:<< >>>>'robot1' isat'cutting_board1'<< >>>>'robot1' isnotat'table2'<<

>>>>State 5:<< >>>>'lettuce1' isat'cutting_board1'<< >>>>'robot1' isnotholding'lettuce1'<<

Consequently, it causes the LLM to incorrectly summarizes the states and misses the subtask of cutting the lettuce.

* In [Scenario 1], atstate 2, therobotmoved from'table1' to'table2'.
* Atstate 3-4, the subtask is "pickup lettuce". This subtask contains: 1. picking up 'lettuce1' (state 3)

### 11.2 Randomly removing predicates/states

#### 11.2.1 Experimental Details

For each EPIC-Kitchens task, we add additional predicates (i.e. showing the position of additional objects in the scene) in at least 2 separate states in the demonstrations. We also do the same modification for the training examples, while keeping the rest of the prompt identical. We expect the LLM to weed out these additional states during recursive summarization.

For example, for annotations of video ID \(P22\_07\) as seen in appendix 17.3.1, we add distractions in 2 states -

State 3: at('sink_1') is_in_hand('sponge_1') +++++is_in('brush_1','sink_1')+++++
+++++is_in('bowl_1','sink_2')+++++

State 4: is_on('tap_1') +++++on('jar_1','microwave_1')+++++

### Quantitative Analysis

We see that in table 7 that Demo2Code suffers from degradation on most demonstrations when distracting states are added, resulting in only being able to generate correct code for 2 out of 7 demonstrations instead of 5 out of 7 demonstrations.

### Qualitative Analysis

When adding distracting states, the LLM mostly ignores the distracting states and only shows variation in output as a result of change in input. However, the distracting states can interfere with the final code, as can be seen in the output for demonstration \(P44\_101\).

Code using clean demonstrations:

```
1objects=get_all_objects()
2forobjectinobjects:
3pick_up(object)
4go_to("sink_1")
5pick_up("sponge_1")
6turn_on("tap_1")
7soap(object)
8rinse(object)
9turn_off("tap_1")
10go_to("kitchentop_1")
11place(object, "kitchentop_1") ```

Code generated with demonstration that has additional irrelevant predicates:

```
1objects=get_all_objects()
2forobjectinobjects:
3bring_objects_to_loc([object], "sink_1")
4pick_up("sponge_1")
5turn_on("tap_1")
6place("brush_1", "sink_1")
7place("bowl_1", "sink_2")
8soap(object)
9rinse(object)
10turn_off("tap_1")
11go_to("kitchentop_1") ```

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{P+101 (7)} & \multicolumn{2}{c}{P-04 (17)} & \multicolumn{2}{c}{P-10 (6)} & \multicolumn{2}{c}{P-205 (28)} & \multicolumn{2}{c}{P-227 (30)} & \multicolumn{2}{c}{P-07 (11)} & \multicolumn{2}{c}{P-08 (16)} \\ \cline{2-13}  & Pos. & BLEU. & Pass. & BLEU. & Pass. & BLEU. & Pass. & BLEU. & Pass. & BLEU. & Pass. & BLEU. & Pass. & BLEU. \\ \hline Demo2Code & **1.00** & 0.33 & 0.00 & 0.19 & 1.00 & 0.63 & **1.00** & 0.43 & **1.00** & 0.66 & **1.00** & 0.58 & 0.00 & 0.24 \\ Demo2Code + additional states & 0.00 & 0.21 & 0.00 & 0.15 & **1.00** & 0.27 & 0.00 & 0.22 & 0.00 & 0.49 & **1.00** & 0.67 & 0.00 & 0.22 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Results for Demo2Code’s performance on the original EPIC-Kitchens demonstrations v.s. on the demonstrations with additional irrelevant predicates. The unit test pass rate is evaluated by a human annotator, and BLEU score is calculated between each method’s code and the human annotator’s reference code.

place(object, "kitchentop_1") When compared to the generated output with clean annotations, we see that while the on('jar_1','microwave_1') was ignored, in('brush_1','sink_1') and in('bowl_1','sink_2') result in the LLM generating additional code that does not align with the demonstrations. Specifically, even though brush_1 and bowl_1 were objects that the users were not interested in interacting with, the LLM has generated a place() code (lines 6-7) for these two objects. This type of mistake could be avoided by adding reasoning during recursive summarization. The LLM can be guided to ignore irrelevant objects and avoid hallucinating actions relating to these objects - for example, ground place action only when both is_in_hand(...) and on(..., loc) are seen one after the other.

## 12 Chain-of-thought Ablation Experiment

This experiment studies the effect of the chain-of-thought's length (in stage 1 recursive summarization) on the LLM's performance. We found:

* It is helpful to guide the LLM to take small recursive steps when summarizing demonstrations (especially for tasks with long demonstrations).
* The LLM performs the worst if it is asked to directly generate code from demonstrations.

### Experiment Detail

We defined 3 ablation models listed below from the shortest chain-of-thought length to the longest chain length. In addition, because the tabletop's Demo2Code pipeline is different from Robotouille's pipeline, we also describe how these pipelines are adapted to each ablation model:

* **No-Cot**: Tabletop and Robotouille has exactly the same process of prompting the LLM ONCE to generate code given the language model and the demonstrations.

* Tabletop: First, the LLM receives all the demonstrations concatenated together as input to generate the specification without any intermediate reasoning. Next, the LLM generates the code given the specification.
* Robotouille: First, the LLM receives all the demonstrations concatenated together as input to generate the specification. It can have intermediate reasoning because the tasks are much more complex. Next, the LLM generates the high-level code given the specification and recursively expands the code by defining all helper functions.

* Tabletop: First, the LLM classifies the task into either placing task or stacking task. Second, the LLM receives all the demonstrations concatenated together as input to generate the specification without any intermediate reasoning. Finally, the LLM generates the code given the specification.
* Robotouille: First, for each demonstration, the LLM gets its state trajectories as input to identify a list of the low-level action that happened at each state. Second, all the low-level actions from each scenario are concatenated together and used by the LLM to generate the specification. The LLM can have intermediate reasoning at this step because the tasks are much more complex. Finally, the LLM generates the high-level code given the specification and recursively expands the code by defining all helper functions.

We identified 3 clusters of tasks based on the number of states they have, and for each cluster, we selected two tasks to test. For each task and for each of that task's specific requirements, we tested the approach 10 times and took an average of the unit test pass rate.

* Short-horizon tasks (around 2 states): "Place A next to B" and "Place A at a corner"
* Medium-horizon tasks (around 5-10 states): "Place A on top of B" and "Stack all blocks/cylinders (where there might be a maximum stack height)"
* Long-horizon tasks (more than 15 states): "Make a burger" and "Make two burgers"

### Quantitative Result

We include the quantitative result again here as Fig. 11. Overall, Demo2Code/**Full** performs the best, and there's the general trend that as the length of the chain of length increases, the LLM also generates code that has a higher unit test pass rate. For short-horizon tasks, the difference in the chain-of-thought's length has a small effect on the pass rate because these tasks have short demonstrations that are easy to process without intermediate chain-of-thought steps. In contrast, both medium-horizon tasks and long-horizon tasks see great improvement when the LLM is prompted to take smaller recursive summarization steps. In the next section, we will examine one example from each cluster of tasks to analyze how the length of the chain-of-thought prompts affects the LLM's output.

### Qualitative example for a short-horizon task

We examine all four models' outputs on the task: placing A next to B. There are 2 states per demonstration and 1 demonstration in total. The query and the **Full** approach's correctly generated code for a particular instance of the task are shown below:

```
1objects=['yellowblock','blueblock','greenblock','cyancylinder','graycylinder','redcylinder']
2"""
3Placethegraycylindernexttothegreenblock.
4
5InitialState(State1):
6'greenblock'isnotbehind'graycylinder'
7'graycylinder'hasnotmoved
8'graycylinder'isnotinfrontof'greenblock'
9
10[Scenario1]
11State2:
12'greenblock'isbehind'graycylinder'
13'graycylinder'hasmoved
14'graycylinder'isinfrontof'greenblock'
15"""
16
17say("Placethegraycylinderinfrontofthegreenblock.")
18location_pos=parse_position('infrontofthegreenblock')
19put_first_on_second('graycylinder',location_pos) ```

For this particular example, the LLM needs to be able to identify that the robot needs to specifically place the gray cylinder in front of the green block. Only **No-Cot** failed to generate the correct code,

Figure 11: Ablation of different degrees of chain-of-thought (recursive summarization) across tasks with short, medium, long horizon.

[MISSING_PAGE_FAIL:29]

* [33]'yellowcylinder'isotheleftof'orangeblock'
* [34]
* [35] State 5:
* [36]'brownblock'isnotontopof'table'
* [37]'brownblock'hasmoved
* [38]'brownblock'isontopof'purplecylinder'
* [39]"""

```
1say("First,placetheredcylinder,thepinkcylinder,andtheorangeblockonthetable.Then,placethebrownblockonthepurplecylinder.")
2items_to_place_first_in_order=['redcylinder','pinkcylinder','orangeblock']
3foriteminitems_to_place_first_in_order:
4put_first_on_second(item,"table")
5put_first_on_second('brownblock','purplecylinder') ```

For this particular example, the LLM needs to recognize that there are 3 additional objects on top of the brown block. In order to place the brown block on top of the purple cylinder, the robot needs to first place those 3 additional objects on the table. **No-Cot** and **1-step** approaches failed to generate the correct code, while **2-steps** was able to identify the specification and generate the code exactly the same as **Full**.

#### Why No-Cot and 1-step failed?

Both **No-Cot** and **1-step** seem to be over-reliant on the language instruction instead of reasoning through the state transitions. Specifically, examining the specification outputted by **1-step** suggests that the LLM simply copied the language instruction instead of recognizing that more objects are moved than just moving the brown block to be on top of the purple cylinder.

```
1Thedetailedrequirements:<Placethebrownblockonthepurplecylinder. ```

Below show the identical code generated by **No-Cot** and **1-step**.

```
1say("Placethebrownblockonthepurplecylinder.")
2put_first_on_second('brownblock','purplecylinder') ```

### Qualitative example for a long-horizon task

We examine all four models' output on the long-horizon task: making a burger with patty, lettuce, and tomato. This task has 71 states per demonstration and 2 demonstrations in total.

The **Full** pipeline's approach outputs the correct code as shown below. Next, we analyze how other models failed to produce the correct code.

```
1#Getalistofallthepattiesinthekitchen.
2patties=get_all_obj_names_that_match_type('patty')
3#Getalistofallthelettcesinthekitchen.
4lettces=get_all_obj_names_that_match_type('lettuce')
5#Getalistofallthetomatoesinthekitchen.
6tomatoes=get_all_obj_names_that_match_type('tomato')
7#Getalistofallthebotothownsinthekitchen.
8botton_buns=get_all_obj_names_that_match_type('bottom')
9#Getalistofallthetopbunsinthekitchen.
10top_buns=get_all_obj_names_that_match_type('topbun')
11#Getalistofallthestovesinthekitchen.
12stoves=get_all_location_names_that_match_type('stove')
13#Getalistofallthecuttingboardsinthekitchen.
14cutting_boards=get_all_location_names_that_match_type('cutting_board')
15#Getalistofallthetablesinthekitchen.
16tables=get_all_location_names_that_match_type('table')* 18#Decideastovetouse.
* 19stove_to_use=stoves[0]
* 20#Decideacuttingboardtouse.
* 21cutting_board_to_use=cutting_boards[0]
* 22#Decideatabletouse.
* 23table_to_use=tables[0]
* 24
* 25#Repeatthefollowingtwice:
* 26foriinrange(2):
* 27#Decideapatytocook.
* 28patty_to_cook=patties[i]
* 29#Cookthatpattyatthatstove.
* 20cook_object_at_location(obj=patty_to_cook,location=stove_to_use)
* 21#Decideabototownboutose.
* 22bottom_bun_to_use=bottom_bun[i]
* 23#Stackthepattyontopofthebottombun.
* 24stack_obj1_on_obj2(obj1=patty_to_cook,obj2=bottom_bun_to_use)
* 25
* 26#Decideatomatototcut.
* 27tomato_to_cut=tomatoes[i]
* 28#Cutthattomatoatthatcuttingboard.
* 29cut_object_at_location(obj=tomato_to_cut,location=cutting_board_to_use)
* 21#Stackthetotancontopofthepatty.
* 22
* 23#Decidealettocontopofthelettec,tomato,andpattystack
* 24letuce_to_cut=lettecs[i]
* 25#Cutthatlettecatthatcuttingboard.
* 26cut_object_at_location(obj=lettec_to_cut,location=cutting_board_to_use)
* 27#Stackthelettecontopofthetomato.
* 28stack_obj1_on_obj2(obj1=lettec_to_cut,obj2=tomato_to_cut)
* 29#Stackthetopbunontopofthelettec,tomato,andpattystack
* 21top_bun_to_use=top_buns[i]
* 22stack_obj1_on_obj2(obj1=top_bun_to_use,obj2=lettec_to_cut)

#### Why No-Cot failed?

The main issue that **No-Cot** suffers from is the maximum token length issue because the prompt needs to include demonstrations as the example query and solution code as the example response. Thus, the LLM was not able to finish completely the code as seen below (line 37). In addition, although the LLM showed some ability to use the control flow, it was only creating a for loop to cook the patties but it did not use a for-loop to assemble the burger. Specifically, the LLM generated code that index to specific items: (e.g. line 23bottom_buns[0], line 25lettec[0], etc.)

```
1#Getalistofallthepattiesinthekitchen.
2patties=get_all_obj_names_that_match_type('patty')
3#Getalistofallthestovesinthekitchen.
4stoves=get_all_location_names_that_match_type('stove')
5#Decideastovetogoto.
6stove_to_cook_at=stoves[0]
7
8#Repeatthefollowingtwice
9foriinrange(2):
10#Decideapatytocook.
11patty_to_cook=patties[i]
12#Cookthatpattyatthatstove.
13cook_object_at_location(obj=patty_to_cook,location= stove_to_cook_at)
14
15#Getalistofallthebottombunsinthekitchen.

[MISSING_PAGE_FAIL:32]

Consequently, although the code (shown below) closely matched the specification and included the for-loop, because the specification was wrong, the code also failed to solve the problem.

```
1#Getalistofallthepattiesinthekitchen.
2patties=get_all_obj_names_that_match_type('patty')
3#Getalistofallthestovesinthekitchen.
4stoves=get_all_location_names_that_match_type('stove')
5#Getalistofallthebinsinthekitchen.
6buns=get_all_obj_names_that_match_type('bottombun')+get_all_obj_names_that_match_type('topbun')
7#Getalistofalltheletucesinthekitchen.
8lettuces=get_all_obj_names_that_match_type('lettuc')
9#Getalistofallthetomatoesinthekitchen.
10tomatoes=get_all_obj_names_that_match_type('tomato')
11
12#Repeatthefollowingtwice:
13foriinrange(2):
14#Decadeapatytotuse.
15patty_to_use=patties[i]
16#Decadeatstovetogoto.
17stove_to_cook_at=stoves[i]
18#Cookthatpattyathatstove.
19cook_object_at_location(obj=patty_to_use,location=stove_to_cook_at)
20
21#Decadeabottombuntouse.
22bottom_bun_to_use=buns[i*2]
23#Placethatbottomanaplate.
24place_obj_on_obj(obj=bottom_bun_to_use,obj2='plate')
25#Placethatpatytontopoftthebottombun.
26place_obj_on_obj(obj=patty_to_use,obj2=bottom_bun_to_use)
27
28#Decadealettuces.
29lettuc_to_use=lettuces[i]
20#Placethatlettucetontopofthepatty.
23place_obj_on_obj(obj=lettuc_to_use,obj2=patty_to_use)
24
25#Decadeatomatotuse.
26tomato_to_use=tomatoes[i]
27#Placethattomatoontopofthelettuce.
28place_obj_on_obj(obj=tomato_to_use,obj2=lettuc_to_use)
29
30#Decadeatopbuntouse.
31top_bun_to_use=buns[i*2+1]
32
33#Placethattopbunontopofthetomato.
34place_obj_on_obj(obj=top_bun_to_use,obj2=tomato_to_use) ```

#### Why 2-steps failed?

**2-steps** improved its specification compared to **1-step**'s. Specifically, its specification was able to identify that lettuce and tomato require cutting before they are stacked as a part of the burger. However, it still fails to identify the physical constraint that in order to cut an object, it also needs to specify the location. While it is able to identify the stove used for cooking (line 17), it fails to identify the cutting board (line 22 and line 25).

```
1*Theorderofhighlevelactionsis:['cook','assemble','cook','assemble']
2*Because'cook'isbefore'assemble',weassumethatwemustcookthepattiesbeforeassemblingtheburgers.
3*Because'assemble'isrepeatedtwice,weassumethatwemustassembletwoburgers.
4Thus:
5<Maketwoburgers.
6
7Specifically:* Get a list of all the patties in the kitchen.
* Get a list of all the buns in the kitchen.
* Get a list of all the lettues in the kitchen.
* Get a list of all the tomatoes in the kitchen.
* Repeat the following twice:
* Decide a patty to cook.
* Get a list of all the stoves in the kitchen.
* Decide a stove to use.
* Cook that patty on that stove.
* Decide a bun touse as the bottom bun.
* Place that bun on a plate.
* Stack that patty on top of that bun.
* Decide a tomato touse.
* Cut that tomato.
* Place that tomato on top of that patty.
* Decide a lettuce touse.
* Cut that lettuce.
* Place that lettuce on top of that tomato.
* Decide a bun touse as the top bun.
* Place that bun on top of that lettuce.

Consequently, in the code that the LLM produced based on the specification, it created a function cut_object(obj). Then, because the function signature does not specify any location to cut the object at, the downstream function definition also fails (see the definition starting from line 43).

```
1#Geta list of all the patties in the kitchen.
2patties=get_all_obj_names_that_match_type('patty')
3#Geta list of all the buns in the kitchen.
4buns=get_all_obj_names_that_match_type('bottom bun') + get_all_obj_names_that_match_type('top bun')
5#Geta list of all the lettues in the kitchen.
6lettues=get_all_obj_names_that_match_type('lettuce')
7#Geta list of all the tomatoes in the kitchen.
8tomatoes=get_all_obj_names_that_match_type('tomato')
9
10#Repeatthefollowing twice:
11fori inrange(2):
12#Decideapatytocook.
13patty_to_cook=patties[i]
14#Geta list of all the stoves in the kitchen.
15stoves=get_all_location_names_that_match_type('stove')
16#Decideastovetouse.
17stove_to_use=stoves[i]
18#Cookthat patty on that stove.
19cook_object_at_location(obj=patty_to_cook, location=stove_to_use)
20#Decideabuntouseasthebottom bun.
21botton_bun_to_use=buns[i]
22#Placethatbun on a plate.
23place_obj_on_obj(obj=bottom_bun_to_use,obj2='plate')
24#Stackthatpatty on top of that bun.
25stack_obj1_on_obj2(obj=patty_to_cook,obj2=bottom_bun_to_use)
26#Decideatomatotuse.
27tomato_to_use=tomatoes[i]
28#Cutthat tomato.
29cut_object(obj=tomato_to_use)
30#Placethattomatoontopofthatpatty.
31place_obj_on_obj(obj=tomato_to_use,obj2=patty_to_cook)
32#Decidealettetetouse.
33lettuce_to_use=lettues[i]
34#Cutthatlettuce.
35cut_object(obj=lettuce_to_use)
36#Placethatlettuceontopofthattomato.
37place_obj_on_obj(obj=lettuce_to_use,obj2=tomato_to_use)
38#Decideabuntouseasthetopbun.

top_bun_to_use=buns[i+2]
* #Placethatbunontopofthatlettuce.
* place_obj_on_obj(obj=top_bun_to_use,obj2=lettuce_to_use)
* ```
* defcut_object(obj):
* ifnotis_holding(obj):
* #Tocutobj,therobotenedstoholdobjfirst.
* ifis_in_a_stack(obj):
* #Becauseobjisinastack,robotencotomovethen unstacktheobjfromtheobj_at_bottomfirst
* obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj)
* move_then_unstack(obj_to_unstack=obj,obj_at_bottom=obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom))
* else:
* #Sinceobjisnotinastack,robotcansjustmoveethen pickitup
* move_then_pick(obj=obj)
* #cuttheobject cut_until_is_cut(obj=obj)

## 13 Intermediate Reasoning Ablation Experiment

This experiment studies how the thoroughness of intermediate reasoning (in stage 3 of recursive summarization) affects the LLM's performance. We found:

* Our approach may be improved by ablating away some part of reasoning (e.g. listing the high-level actions)
* Having intermediate reasoning of any form is helpful for hard cooking tasks that have many sub-tasks
* Future work is necessary to engineer better prompts

### Experiment detail

This experiment compares Demo2Code (labeled as **Full**) with three additional ablation models each with differing levels of reasoning

* **No reasoning:** The LLM generates the specification directly from step 2 of recursive summarization with no intermediate reasoning.
* **Only List:** The LLM generates the specification after intermediate reasoning which lists the high-level actions in common with the scenarios from step 2 of recursive summarization.
* **Only Analyze:** The LLM generates the specification after intermediate reasoning which describes the repetition and ordering of high-level actions from step 2 of recursive summarization.

These models are tested on all the Robotouille tasks. We use 3 clusters of tasks based on the number of high-level actions/sub-tasks they have.

* Easy cooking tasks (\(\leq\) 2 high-level actions/sub-tasks): "cook and cut", "cook two patties", and "cut two lettuces"
* Normal cooking tasks (between 2-7 high-level actions/sub-tasks): "make a burger" and "assemble two burgers with already cooked patties"
* Hard cooking tasks (\(\geq\) 8 high-level actions/sub-tasks): "make two burgers"

### Quantitative result

For each task and for each of that task's specific requirements, we tested each model on 10 randomly generated environments and took an average of the unit test pass rate. Fig. 12 shows that the **Only Analyze** model outperforms the other models. All methods easily achieve 100% pass rate on Easytasks. There is a trend that including any kind of reasoning improves performance on hard tasks. **Only List** only slightly suffers in Normal performance while increasing significantly for Hard performance relative to **No Reasoning**. **Full**'s Normal performance suffers significantly yet its Hard performance increases as well, suggesting a tradeoff between Normal performance and Hard performance; however, **Only Analyze** has the highest Normal and Hard performance of all models. Since **Full** combines both **Only List** and **Only Analyze**, this shows that concatenating reasoning methods doesn't necessarily mean better performance.

### Qualitative example

In this section, we show a qualitative example of all models' output on the long-horizon task "make two burgers" with the specific requirement "stack lettuce atop patty immediately." We choose this example to show the effects of reasoning on the code as well as to shed light on why **Full** underperforms relative to **Only Analyze**. **Only Analyze** is the only model with 1.0 success rate while the other models all have 0.0 success rates. For each model's output, the helper functions can be found in section 19.3.

#### 13.3.1 Why No Reasoning failed?

**No Reasoning** is the only unsuccessful model that generates code that executes. Due to the lack of any reasoning, it performs the task correctly but with the incorrect requirement; rather than preparing one burger at a time and assembling the burger immediately as ingredients are prepared, it prepares all ingredients before assembling both burgers in parallel.

```
1#Getalistofallthepattiesinthekitchen.
2patties=get_all_obj_names_that_match_type('patty')
3#Getalistofalltheletticesinthekitchen.
4lettces=get_all_obj_names_that_match_type('lettuce')
5#Getalistofallthebottombunsinthekitchen.
6botom_buns=get_all_obj_names_that_match_type('bottombun')
7#Getalistofallthetopbunsinthekitchen.
8top_buns=get_all_obj_names_that_match_type('topbun')
9#Getalistofallthestovesinthekitchen.
10stoves=get_all_location_names_that_match_type('stove')
11#Getalistofallthecuttingboardsinthekitchen.
12cutting_boards=get_all_location_names_that_match_type('cutting_board')
13
14#Decidetwopatiestocook.
15patties_to_cook=patties[:2]
16#Cookthosepatiesttwodifferentstoves.
17foriinrange(2):

Figure 12: Ablation of different amount and style of intermediate reasoning at step 3 of recursive summarization across Robotouille tasks. These tasks are clustered into easy cooking tasks, normal cooking tasks, and hard cooking tasks based on the number of high-level actions/sub-tasks each task has.

[MISSING_PAGE_EMPTY:37]

[MISSING_PAGE_FAIL:38]

```
29letuce_to_use=lettuces[i]
30#Cuthatlettucatacuttingboard.
31cut_object_at_location(obj=lettuc_to_use,location=cutting_boards[i])
32#Stackthelettucenotopofthepatty.
33#objshouldbethelettuce,obj2shouldbethepattyonotopofthebottombun.
34stack_obj1_on_obj2(obj1=lettuc_to_use,obj2=patties[i])
35
36#Decideatopbuntouse.
37top_bun_to_use=top_buns[i]
38#Stackthetopbunonotopofthelettuce.
39#objshouldbethetopbun,obj2shouldbethelettuceonotopofthepattyonotopofthebottombun.
30stack_obj1_on_obj2(obj1=top_bun_to_use,obj2=lettuce_to_use) ```

#### 13.3.4 Only Analyze Output

Only Analyze is the only model to adhere to the environment's constraints. Note that while the same stove is used for cooking twice, the second time a patty is cooked the first patty has already been removed from the stove and stacked into the first burger.

```
1#Getalistofallthepattiesinthekitchen.
2patties=get_all_obj_names_that_match_type('patty')
3#Getalistofallthelettucesinthekitchen.
4letuces=get_all_obj_names_that_match_type('letuce')
5#Getalistofallthebottombunsinthekitchen.
6botomm_buns=get_all_obj_names_that_match_type('bottombun')
7#Getalistofallthettopbunsinthekitchen.
8top_buns=get_all_obj_names_that_match_type('topbun')
9#Getalistofallthetucwesinthekitchen.
10stoves=get_all_location_names_that_match_type('stove')
11#Getalistofallthecuttingboardsinthekitchen.
12cutting_boards=get_all_location_names_that_match_type('cutting_board')
13
14#Decideastovetouse.
15stove_to_use=stoves[0]
16
17#Repeatthefollowingtwice
18foriinrange(2):
19#Decideapatyttocok.
20party_to_cook=patties[i]
21#Cookthatpatytathatusetov.
22cookobject_at_location(obj=patty_to_cook,location=stove_to_use)
23
24#Decideabottombuntouse.
25bottom_bun_to_use=bottom_buns[i]
26#Stackthepattyonotopofthebottombun.
27#objshouldbethepatty,obj2shouldbethebottombun.
28stack_obj1_on_obj2(obj1=patty_to_cook,obj2=bottom_bun_to_use)
29
30#Decidealettucetouse.
31lettucetouse.
32#Cuthatlettucetathatusetoungboard.
33cut_object_at_location(obj=lettuc_to_use,location=cutting_boards[i])
34#Stackthelettucenotopofthepatty.
35#objshouldbethelettuce,obj2shouldbethepattyonotopofthebottombun.
36stack_obj1_on_obj2(obj1=lettuc_to_use,obj2=patty_to_cook)
37
38#Decideatopbuntouse.
39top_bun_to_use=top_buns[i]

#Stackthetopbunontopofthelettuce.
*obj1shouldbethetopbun,obj2shouldbethelettuceontopofthepatytoontopofthebottombun.
*stack_obj1_on_obj2(obj1=top_bun_to_use,obj2=lettuce_to_use)

## 14 Recursive Expansion Ablation Experiment

This experiment studies how the number of recursive code expansion steps (in stage 2 recursive expansion) affects the LLM's performance. We found:

* It is helpful to guide the LLM to slowly expand the code instead of asking it to directly generate all the code at once using only the low-level imported APIs.
* The initial code that LLM uses to expand the rest of the functions should align closer to the given specifications.

### Experiment detail

This experiment compares Demo2Code (labeled as **Full**) with three additional ablation models each with a different amount of recursive code expansion steps:

* **1-layer:** Given a specification, the LLM directly outputs the code for the task using only low-level action and perception APIs from the import statement. The LLM is not allowed to define any helper function.
* **2-layer (Comp):** Given a specification, the LLM first outputs corresponding code that can call undefined "composite functions." Each composite functions contain at most two low-level actions, e.g. move_then_pick, move_then_stack. In the next step, the LLM defines these composite functions using only low-level action and perception APIs from the import statement.
* **2-layer (High):** Given a specification, the LLM first outputs corresponding high-level code that can call undefined "high-level functions." This step is the same as Demo2Code. Then, the LLM defines these high-level functions using only low-level action and perception APIs from the import statement.

These models are tested on all the Robotouille tasks. Because section 13 on Intermediate Reasoning Ablation Experiments also tested only on Robotouille, we use the same 3 clusters of tasks based on the number of high-level actions/sub-tasks they have.

* Easy cooking tasks (\(\leq\) 2 high-level actions/sub-tasks): "cook and cut", "cook two patties", and "cut two lettuces"
* Normal cooking tasks (between 2-7 high-level actions/sub-tasks): "make a burger" and "assemble two burgers with already cooked patties"
* Hard cooking tasks (\(\geq\) 8 high-level actions/sub-tasks): "make two burgers"

### Quantitative result

For each task and for each of that task's specific requirements, we tested each model on 10 randomly generated environments and took an average of the unit test pass rate. Fig. 13 shows how our approach Demo2Code (**Full**) outperforms the other models. Overall, as the number of recursive code generation steps increases, the LLM's performance also increases. Interestingly, **2-Layers (Comp)** performs worse than **2-Layer's (High)** despite having the same amount of recursive steps. The two model's performance difference suggests that the granularity of the initial code also affects the performance. While **2-Layers (High)** first asks the LLM to generate high-level code, which is the same approach as Demo2Code, **2-Layers (Comp)** first asks the LLM to generate code that calls composite functions. Consequently, for **2-Layers (Comp)**, the LLM needs to first output longer, more complicated code the LLM has to produce in one-step, which is more prone to error.

### Qualitative example

In this section, we show a qualitative example of all models' output on the short-horizon task "cut two lettuce." Note that cutting lettuce has not been shown in the prompt, so the LLM needs to be able to generalize to this new task and properly use a for-loop to write efficient code. **1-layer** and **2-layer (Comp)** had 0.0 success rate. **2-layer** (High has 0.2 success rate, and Demo2Code has 1.0 success rate. All models are able to generate the for-loop, as evident in the outputted code below. However, all ablation models struggle to generate code that adheres to physical constraints and applies to any random environment.

#### 14.3.1 Why 1-Layer failed?

**1-Layer** generates the correct code to cut the lettuce. Despite not seeing how the low-level action cut() should be used in the prompt, it correctly writes the code that will cut the lettuce until the lettuce becomes cut. However, the LLM misses the physical constraints associated with cooking, e.g. in order to cut the lettuce at a cutting board, the robot needs to make sure that there is a lettuce at the cutting board before starting to cut. If there is no lettuce on the cutting board, the robot first needs to go pick up a lettuce and bring it to the cutting board. Another issue that **1-Layer** encounters is reaching the LLM maximum token length. For longer tasks, it cannot finish generating all the code, so its overall execution success rate is only 0.04.

```
1#Getalistofalltheletucesinthekitchen.
2letuces=get_all_obj_names_that_match_type('letuc')
3#Getalistofallthecuttingboardsinthekitchen.
4cutting_boards=get_all_location_names_that_match_type('cutting_board')
5
6#Repeatthefollowingtwice
7foriinrange(2):
8#Decideacuttingboardtouse.
9cutting_board_to_use=cutting_boards[i]
10#Decideacittecotcut.
11lettec_to_cut=letuces[i]
12#Cutthatlettecatatcuttingboard.
13#Tocutlettec_to_cut,therobotfirstneedstobeatthesame
14locationascutting_board_to_use
15ifget_curr_location()!=cutting_board_to_use
16move(get_curr_location(),cutting_board_to_use)
17whilenotis_cut(lettec_to_cut):
18cut(lettec_to_cut)

Figure 13: Ablation of different amount of recursive code expansion steps across Robotouille tasks. These tasks are clustered into easy cooking task, normal cooking task, and hard cooking task based on the number of high-level actions/sub-tasks each task has.

[MISSING_PAGE_FAIL:42]

[MISSING_PAGE_FAIL:43]

# (1) ifobjisinastack,unstackobj
# (2) else,pickupobj.
# ifis_in_a_stack(obj):
# Becauseobjisinastack,robotencotomovethen
#unstacktheobjfromtheobj_at_bottomfirst
#obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj) move_then_unstack(obj_to_unstack=obj,obj_at_bottom=obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom)) else:
#Sinceobjisnotinastack,robotcansjustmoveethen pickitup move_then_pick(obj=obj)
#placetheobjectatthelocationtocutat
#move_then_place(obj=obj,place_location=location)
#cuttheobject cut_until_is_cut(obj=obj)
#set_until_is_cut(obj):
#whilenotis_cut(obj):
#cut(obj)
#move_then_unstack(obj_to_unstack,obj_at_bottom,unstack_location) :
#Forunstacking,wenedtomovetothelocationofthebottom object ifget_curr_location()!=get_obj_location(obj_at_bottom):
#move(get_curr_location(),get_obj_location(obj_at_bottom)) unstack(obj_to_unstack,obj_at_bottom)
#Afterunstacking,wenedtomovetotheunstack_location ifget_curr_location()!=unstack_location:
#move(get_curr_location(),unstack_location)
#move_then_pick(obj):
#obj_location=get_obj_location(obj) ifget_curr_location()!=obj_location:
#move(get_curr_location(),obj_location) pick_up(obj,obj_location)
#move_then_place(obj,place_location):
#get_curr_location()!=place_location:
#move(get_curr_location(),place_location) place(obj,place_location)

## 15 Broader Impact

Our approach is a step towards making collaborative robots more accessible in different settings, such as homes, factories, and logistics operations. The broader implications of our research are manifold, touching upon societal, economic, ethical, technical, and educational aspects.

Societal Impacts:Currently, robots can only be programmed by engineers, limiting the tasks the robot can do to design choices made by engineers. This limits the accessibility for everyday users who require _personalization_ to their individual needs and use cases. Our work tackles this problem head-on by allowing robots to be easily programmable via intuitive, user-friendly interfaces like vision and language. This could lead to increased creativity in the types of tasks that robots can be used for, potentially enabling novel applications and solutions.

Economic Impacts:Our approach has the potential to dramatically decrease the cost of programming robots, thus lowering the barriers to entry for businesses interested in incorporating robotics into their workflows. This could boost productivity and efficiency across a variety of industries, such as manufacturing, agriculture, and healthcare, among others. In the long term, this may contribute to economic growth and job creation in sectors that are currently behind on automation.

Ethical and Legal Impacts:Our approach has many ethical and legal considerations that need to be addressed. For instance, the widespread use of automation may lead to job displacement in certain sectors. Furthermore, it's crucial to ensure that the task code generated from demonstrations respects privacy laws and does not inadvertently encode biased or discriminatory behavior. Future work in this area will require close collaboration with ethicists, policymakers, and legal experts to navigate these issues.

Technical Impacts:Our work has the potential to accelerate the development of more intuitive and efficient human-robot interaction paradigms. For instance, demonstrations can be extended to interventions and corrections of the user. On the algorithmic side, our approach is a first step to connecting LLMs to Inverse Reinforcement Learning, and can spur advances in the fields of imitation learning, reinforcement learning, and natural language processing.

Educational Impacts:Lastly, our work could contribute to educational initiatives. The ability to generate task code from demonstrations could be utilized as an effective teaching tool in schools and universities, promoting a more experiential and intuitive approach to learning about robotics and coding. This could inspire and enable more students to pursue careers in STEM fields.

## 16 Reproducibility

We ran all our experiments using GPT-3.5 (gpt-3.5-turbo) with temperature 0. We have provided our codebase in https://github.com/portal-cornell/demo2code and all the prompts in section 18. The queries used to produce our results are available in the code base. Note that although we set the temperature to 0, which will make the LLM output mostly deterministic, the output might still slightly vary even for identical input.

## 17 Demo2Code Example Output

We provide an example for each domain and explain Demo2Code's intermediate and final output.

### Tabletop Simulator Example

This section shows an example of how Demo2Code solves the task: stacking all cylinders into one stack. The hidden user preference is that certain objects might need to have a fixed stack order, while other objects can be unordered. For this example, two objects ('blue cylinder' and 'purple cylinder') must follow the order that 'blue cylinder' should always be directly under the 'purple cylinder.'

#### 17.1.1 Query

The query has 3 components: (1) a list of objects that are in the environment, (2) a language instruction describing the goal of the task, and (3) two demonstrations.

``` objects=['pinkblock','yellowblock','purplecylinder','cyan cylinder','pinkcylinder','bluecylinder'] """
[Scenario1] Stackall cylindersintononestack,whileenforcingtheorderbetweencylindersifthereisarequirement.

State 2: 'bluecylinder'hasmoved

State 3: 'purplecylinder'isnotnottopof'table' 'purplecylinder'hasmoved

'purplecylinder'isnottopof'bluecylinder'

State 4: 'cyancylinder'isnotnottopof'table' 'cyancylinder'hasmoved

[MISSING_PAGE_EMPTY:46]

* Atstate4,therobotpickedupandplaced'orangecylinder'onttopof'redcylinder'.
* Atstate5,therobotpickedupandplaced'cyancylinder'onttopof'orangecylinder'.

===========================summarizingdemo===========================
[[Reasoning:]] Theinputtrajectorycontainsstatedpredicatesbecausethetrajectorytalksaboutthestatusoftheobjects('orangecylinder'hasmoved).

I will summarizethestatteractoryintow-levelactions.Thelow-levelactionsarethemostcompactform,soIshouldstop summarizingafterthis.Thenewtrajectorywillbesufficientlysummarized.[[Ilsthenewtrajectorysufficientlysummarized?(yes/no):]] yes[[Summarizedtrajectory:]]
* In[Scenario2],atstate2,therobotmoved'orangecylinder'.Atstate3,therobotpickedupandplaced'bluecylinder'onttopof'orangecylinder'.
* Atstate4,therobotpickedupandplaced'redcylinder'onttopof'bluecylinder'.
* Atstate5,therobotpickedupandplaced'cyancylinder'onttopof'redcylinder'.

SummarizedDemonstrations->TaskSpecification
[[Reasoning:]] Becausethetaskis"Stackallcylindersintoonestack,whileenforcingtheorder...",wefocusonwhetherthereisanyorderingrequirement.

In [Scenario1],basedonStates2-5,inthefirststack,thecylindersfrombottomtotopis['bluecylinder','redcylinder','orangecylinder','cyancylinder']. In [Scenario2],basedonStates2-5,inthefirststack,thecylindersfrombottomtotopis['orangecylinder','bluecylinder','redcylinder','cyancylinder'].

Betweentheorderofcylindersinthesetwoscanratios,theorderingthatstaysthesameis['bluecylinder','redcylinder','cyancylinder'].
[[TaskSpecification:]] Stackallcylindersintoonestack.Alsomakesurethatthestackingenforcesthebottomtotoporderbetweenthesecylinders:['bluecylinder','redcylinder','cyancylinder'].

#### 17.1.3 Output Code

This is the final output of Demo2Code, which is the task code to solve this problem. The code illustrates the LLM's ability to write complex code that has control flows (e.g. for loop and if-else statements.)

```
#Basedontheinstruction,firstdefinetchedylindersthathavestrictorderingcylinder_names_with_strick_order_from_bottom_to_top=['bluecylinder','purplecylinder']
#Then,findtherestofthecylindersthatdon'thavestrictordercylinder_names=get_all_obj_names_that_match_type(type_name="cylinder",objects_list=get_obj_names())cylinder_names_without_order=[] forcylinder_names:ifcylindernotincylinder_names_with_strick_order_from_bottom_to_top:

cylinder_names_without_order.append(cylinder)
Call helperfunctiontodeterminetfinalstackingorderbefore stackingthe cylinders stack_order_from_bottom_to_top=determine_final_stacking_order( objects_to_enforce_order= cylinder_names_with_strick_order_from_bottom_to_top, objects_without_order=cylinder_names_without_order) stack_without_height_limit(objects_to_stack= stack_order_from_bottom_to_top)

### Robotouille Example

#### 17.2.1 Query

The example we will consider is _Make a burger stacking patty atop lettuce immediately_. This is a task that contains many different actions and is successfully solved by Demo2Code, making it a comprehensive example. The query for this task consists of two demonstrations each of which has the same language instruction to make a burger. Because each demonstration has 25 states, we have placed the query at the end of the appendix in section 19.1.

#### 17.2.2 Intermediate output

Below, in stage 1 recursive summary, for each demonstration, Demo2Code summarizes each trajectory from a state-based trajectory to a low-level action trajectory to a high-level subtask trajectory, before the LLM decides that the trajectory is sufficiently summarized.

Then, Demo2Code concatenates all summarized trajectories together to identify the user's preference and generate an appropriate task specification. Specifically, the LLM first identified the order of the subtasks in each scenario. Then, it determined that the lettuce needs to be cut and stacked on top of the bottom but before cooking the patty and stacking the patty on top of the lettuce. It finally produced the correct specification that will get passed to stage 2 to produce the correct code.

**Stage 1 Recursive Summary: recursive call # 1 with state-based trajectories as input**

===========================

[[Reasoning:]] Theinputtrajectorycontainsstetpredicatesbecausethetrajectory talksaboutthestatusoftherobot('robot!isat'table2')and thestatusoftheobjects('lettuce1'isnotat'table2'). Iwillsummarizethestatetrajectoryintow-levelactions. Low-levelactionsarenotcompactedenoughyetbecauselow-levelactionscansstillbecombinedintohigh-level subtasks. ThenewtrajectorywillNOTbesufficientlysummarized.

[[Isthenewtrajectorysufficientlysummarized?(yes/no):]] no

[[Summmarizedtrajectory:]]

[[Semanario1],atstate2,therobotmovedfrom'table1'to' table2'.

* At state3,therobotpickedup'lettuce1'.
* At state4,therobotmovedfrom'table2'to'cutting_board1'.
* At state5,therobotplaced'lettuce1'atlocation'cutting_board1'.
* At state6-8,therobothadcut'lettuce1'.
* At state9,therobotpickedup'lettuce1'.
* At state10,therobotmovedfrom'cutting_board1'to'table3'.
* At state11,therobotplaced'lettuce1'ontopof'bottom_bun1'atlocation'table3'.
* At state12,therobotmovedfrom'table3'to'stove1'.
* At state13,therobotpickedup'patty1'.
* At state14,therobotmovedfrom'stove1'to'stove4'.
* At state15,therobotplaced'patty1'at'stove4'.
* At state16-19,therobothadcock'patty1'.
* At state20,therobotpickedup'patty1'.

* At state 21, therobotmovedfrom'stove4'to'table3'.
* At state 22, therobotplaced'patty1'ontopof'lettuce1'at location'table3'.
* At state 23, therobotmovedfrom'table3'to'table4'.
* At state 24, therobotpickedup'top_bun1'.
* At state 25, therobotmovedfrom'table4'to'table3'.
* At state 26, therobotplaced'top_bun1'ontopof'patty1'at location'table3'.

================================================ summarizing demo===================
[[Reasoning:]] Theinputtrajectorycontainsstetpredicatesbecausethetrajectory talksaboutthestatusoftherobot('robot1'isat'cutting_boards')andthestatusoftheobjects('lettuce3'isnot at'cutting_boards'). Iwillsummarizethestatetrajectoryintow-levelactions.Low-levelactionsarenotcompactedenoughyetbecauselow-levelactionscanstillbecombinedinthigh-levelsubtasks.ThenewtrajectorywillNOTbeswfficientlysummarized.
[[Isthenewtrajectorysufficientlysummarized?(yes/no):]] no
[[Summarizedtrajectory:]]
* In[Scenario2],atstate2,therobotmovedfrom'table7'to'cutting_board5'.
* At state 3,therobotpickedup'lettuce3'.
* At state 4,therobotmovedfrom'cutting_board5'to'cutting_board3'.
* At state 5,therobotplaced'lettuce3'atlocation'cutting_board3'.
* At state 6-8,therobothadcut'lettuce3'.
* At state 9,therobotpickedup'lettuce3'.
* At state 10,therobotmovedfrom'cutting_boards'to'table4'.
* At state 11,therobotplaced'lettuce3'ontopof'bottom_bun3'at location'table4'.
* At state 12,therobotmovedfrom'table4'to'table7'.
* At state 13,therobotpickedup'patty3'.
* At state 14,therobotmovedfrom'table7'to'stove3'.
* At state 15,therobotplaced'patty3'.
* At state 16-19,therobothadcock'patty3'.
* At state 20,therobotpickedup'patty3'.
* At state 21,therobotmovedfrom'stove3'to'table4'.
* At state 22,therobotplaced'patty3'ontopof'lettuce3'at location'table4'.
* At state 23,therobotmovedfrom'table4'to'cutting_board4'.
* At state 24,therobotpickedup'top_bun3'.
* At state 25,therobotmovedfrom'cutting_board4'to'table4'.
* At state 26,therobotplaced'top_bun3'ontopof'patty3'.
* location 'table4'.

* At state 27,therobotplaced'patty3'ontopof'lettuce3'at location'table4'.

* At state 28,therobotpickedup'letuce3'.
* At state 29,therobotpickedup'letuce3'.
* At state 30,therobotpickedup'letuce3'.
* At state 41,therobotplaced'lettuce3'ontopof'lettuce3'at location'table4'.
* At state 12,therobotmovedfrom'table4'to'table7'.
* At state 13,therobotpickedup'letuce3'.
* At state 14,therobotmovedfrom'table7'to'stove3'.
* At state 15,therobotplaced'patty3'.
* At state 16-19,therobothadcock'patty3'.
* At state 20,therobotpickedup'letuce3'.
* At state 21,therobotmovedfrom'stove3'to'table4'.
* At state 22,therobotplaced'patty3'ontopof'lettuce3'at location'table4'.
* At state 23,therobotmovedfrom'table4'to'cutting_board4'.
* At state 25,therobotmovedfrom'cutting_board4'to'table4'.
* At state 26,therobotplaced'top_bun3'ontopof'patty3'at location'table4'.

**Stage 1 Recursive Summary: recursive call # 2 with low-level action trajectories as input**

===================
[[Reasoning:]]

Theinputtrajectorycontainslow-levelactionsbecausethetrajectory mentions"moved","pickedup",etc. I will summarizethelow-levelactiontrajectoryintobigh-level subtasks. High-level subtasktrajectoryisthemostcompactedformthatcannot be summarized anymore. Thenewtrajectorywillbeswfficientlysummarized. [[Isthenewtrajectorysufficientlysummarized?(yes/no):]] yes

[[Summarizedtrajectory:]]

* In[Scenario1],atstate2-8,thesubtaskis"cutlettuce".This subtaskcontains:1.movingfrom'table1'to'cutting_board1'(state 2-4) 2. placing 'lettuce!' at 'cutting_board!' (state 5) 3.  cutting 'lettuce!' until it is cut (state 6-8)
* At state 9-11, the subtask is "stack lettuce on top of bottom bun". This subtask contains: 1. picking up 'lettuce!' (state 9) 2.  moving to stack 'lettuce!' on 'bottom_bun1' (state 10-11)
* At state 12-19, the subtask is "cock patty". This subtask contains: 1. moving from'stove!' to'stove4' (state 12-14) 2. placing' patty!' at'stove4' (state 15) 3. cooking 'patty!' until it is cooked (state 16-19)
* At state 20-22, the subtask is "stack pattyon top of lettuce". This subtask contains: 1. picking up 'patty1' (state 20) 2. moving to stack 'patty1' on 'lettuce!' (state 21-22)
* At state 23-26, the subtask is "stack topbun on top of patty". This subtask contains: 1. moving from 'table3' to 'table4' (state 23) 2. picking up 'top_bun1' (state 24) 3. moving tostack 'top_bun1' on 'patty1' (state 25-26)

===========================================================================
[[Reasoning:]] The input trajectory contains low-level actions because the trajectory mentions "moved", "picked up", etc. I will summarize the low-level action trajectory int high-level subtasks. High-level subtask trajectory is the most compacted form that cannot be summarized anymore. The new trajectory will be sufficiently summarized. [[Is the new trajectory sufficiently summarized? (yes/no):]] yes
[[Summarized trajectory:]]
* In [Scenario 2], at state 2-8, the subtask is "cut lettuce". This subtask contains: 1. moving to pick up 'lettuce3' (state 2-3) 2. moving to place 'lettuce3' on 'cutting_board3' (state 4-5) 3.  cutting 'lettuce3' until it is cut (state 6-8)
* At state 9-11, the subtask is "stack lettuce on top of bottom bun". This subtask contains: 1. picking up 'lettuce3' (state 9) 2.  moving to stack 'lettuce3' on 'bottom_bun3' (state 10-11)
* At state 12-19, the subtask is "cock patty". This subtask contains: 1. moving to pick up 'patty3' (state 12-13) 2. moving to place' patty3' on'stove3'. (state 14-15) 3. cooking 'patty3' until it is cooked (state 16-19)
* At state 20-22, the subtask is "stack pattyon top of lettuce". This subtask contains: 1. picking up 'patty3' (state 20) 2. moving to stack 'patty3' on 'lettuce3' (state 21-22)
* At state 23-26, the subtask is "stack top bun on top of patty". This subtask contains: 1. moving to pick up 'top_bun3' (state 23-24)
2. moving to stack 'top_bun3' on 'patty3' (state 25-26)

Summarized Demonstrations -> Task Specification
[[Reasoning:]]
* In [Scenario 1]'s unique kitchen environment, the subtasks were executed in this order: ['cut lettuce','stack lettuce on top of bottom bun', 'cock patty','stack pattyon top of lettuce','stack top bun on top of patty'].
* In [Scenario 2]'s unique kitchen environment, the subtasks were executed in this order: ['cut lettuce','stack lettuce on top of bottom bun', 'cock patty','stack pattyon top of lettuce','stack top bun on top of patty'].
* Both scenarios are executing the subtasks in the same order.
* There is no repetition or loop in the subtask ordering for any individual scenario.
[[Task Specification:]] Make a burger.

Specifically: Get a list of all the lettucesinthekitchen. Get a list of all the bottombunsinthekitchen. Get a list of all the pattiesinthekitchen. Get a list of all the topbunsinthekitchen.

Decide a lettuce touse. Get a list of all the available cutting boards in thekitchen. Decide a cutting board to go to. Cut that lettuce at that cutting board.

Decide a bottombuntouse. Stack the lettuce on topof the bottombun. Decide a patty touse. Get a list of all the available stoves in thekitchen. Decide a stovetogoto. Cook that patty at that stove. Stack the patty on topof the lettuce. Decide a topbuntouse. Stack the topbun on topof the patty.

#### 17.2.3 Output Code

This is the final output of the LLM, which is the task code to solve this problem. The code illustrates the LLM's ability to write complex code that has control flows (e.g. for loop and if-else statements.)

**Task Specification -> High-Level Code**

```
#Getalistofallthelettucesinthekitchen. lettuces=get_all_obj_names_that_match_type('lettuce')
#Getalistofallthebottombusinthekitchen. lettuc_to_use=letuces[0]
#Getalistofallthequaltiesinthekitchen. cutting_boards=get_all_location_names_that_match_type('cutting_board') ')
#Getalistofallthetopbunsinthekitchen. lettuces. lettuc_to_use=letuces[0]
#Getalistofalltheavailablecuttingboardsinthekitchen. cutting_boards=get_all_location_names_that_match_type('cutting_board') ')
#Decideacuttingboardtogoto. cutting_board_to_cut_at=cutting_boards[0]
#Cutthatlettucatthatcuttingboard. cut_object_at_location(obj=lettuc_to_use,location= cutting_board_to_cut_at)
#Decideacbottombuntouse. bottom_bunto_use=bottom_bunto_use=bottom_bun[0]
#Stackthelettucenontopoftthebottombun.
#objshouldbethelettuce,obj2shouldbethebottombun. stack_obj1_on_obj2(obj1=lettuc_to_use,obj2=bottom_bun_to_use)
#Decideapatytotuse. patty_to_use=patties[0]
#Getalistofalltheavailablestovesinthekitchen. stoves=get_all_location_names_that_match_type('stove')
#Decideastovetogoto. stove_to_cook_at=stoves[0]
#Cookthatpattyatthatstove.

cook_object_at_location(obj=patty_to_use,location=stove_to_cook_at)
#Stackthepattyontopoftheletuce.
#objshouldbethetpatty,obj2shouldbetheletuce. stack_obj1_on_obj2(obj1=patty_to_use,obj2=lettuce_to_use)
#Decideatopbuntouse. top_bum_to_use=top_bums[0]
#Stackthetopbunontopoftthepatty.
#objshouldbethetopbun,obj2shouldbethepatty. stack_obj1_on_obj2(obj1=top_bum_to_use,obj2=patty_to_use)

**Step 2: Define composite actions**

defcook_object_at_location(obj,location):  #Tocookanobject,therobotfirstneedstobeholdingobj  ifnotis_holding(obj):  #Iftherobotisnotholdingobj,thereare2scenarios:  #(1)ifobjisinastack,unstackobj  #(2)else,pickupobj.  ifis_in_a_stack(obj):  #Becauseobjisinastack,robotneedtomovethen  unstacktheobjfromtheobj_at_bottomfirst  obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj)  #movethenunstack:firstyourmovetottheobj_at_bottom's  location,thenyourunstackobjfromobj_at_bottom move_then_unstack(obj_to_unstack=obj,obj_at_bottom=  obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom)) else:  #move_then_pick:firstyourmovetoobj'slocation,then  youpickobjup  move_then_pick(obj=obj,pick_location=get_obj_location(obj  ))  #movethenplace:firstyourmovetotthelocationtocokat,then  youplaceobjathatlocation  move_then_place(obj=obj,place_location=location)  #cooktheobjectuntilitiscooked  cook_until_is_cooked(obj=obj)

defstack_obj1_on_obj2(obj1,obj2):  #Tostackobj1onobj2,therobotneedstobeholdingobj  ifnotis_holding(obj1):  #Iftherobotisnotholdingobj1,thereare2scenarios:  #(1)ifobjisinanstack,unstackobj1  #(2)else,pickupobj1.  ifis_in_a_stack(obj1):  #Becauseobj1isinastack,robotneedtomovethen  unstacktheobjfromtheobj_at_bottomfirst  obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj1  )  #movethenunstack:firstyourmovetottheobj_at_bottom's  location,thenyourunstackobjfromobj_at_bottom move_then_unstack(obj_to_unstack=obj1,obj_at_bottom=  obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom)) else:  #move_then_pick:firstyourmovetoobj'slocation,then  youpickobjup  move_then_pick(obj=obj1,pick_location=get_obj_location(  obj1))  #determinationofobj2tostackon  obj2_location=get_obj_location(obj2)  #movethenstack:firstyourmovetotobj2'slocation,thenyour  stackobj1onobj2  move_then_stack(obj_to_stack=obj1,obj_at_bottom=obj2,  stack_location=obj2_location)defcut_object_at_location(obj,location):  #Tocutanobject,therobotfirstneedstobehldingobj ifnotis_holding(obj):  #Iftherobotisnotholdingobj,thereare2scenarios:  #(1)ifobjisinastack,unstackobj  #(2)else,pickupobj.  ifis_in_a_stack(obj):  #Becauseobjisinastack,robotneedtomovethen unstacktheobjfromtheobj_at_bottomfirst obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj)  #movethenunstack:firstyourmovetotheobj_at_bottom's location,thenyouunstackobjfromobj_at_bottom move_then_unstack(obj_to_unstack=obj,obj_at_bottom= obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom)) else:  #move_then_pick:firstyourmovetobj'slocation,then youpickobjup move_then_pick(obj=obj,pick_location=get_obj_location(obj_))  #movethenplace:firstyourmovetothelocationtocutat,then youplaceobjatthatlocation move_then_place(obj=obj,place_location=location)  #cuttheobjectuntilitiscut cut cut_until_is_cut(obj=obj)

Step 3: Define low-level actions

defmove_then_unstack(obj_to_unstack,obj_at_bottom,unstack_location)  : ifget_curr_location()!=unstack_location:  move(get_curr_location(),unstack_location)  unstack(obj_to_unstack,obj_at_bottom)

defmove_then_pick(obj,pick_location):  ifget_curr_location()!=pick_location:  move(get_curr_location(),pick_location)  pick_up(obj,pick_location)

defmove_then_place(obj,place_location):  ifget_curr_location()!=place_location:  move(get_curr_location(),place_location)  place(obj,place_location)

defcook_until_is_cooked(obj):  start_cooking(obj)  whilenotis_cooked(obj):  noop()

defmove_then_stack(obj_to_stack,obj_at_bottom,stack_location):  ifget_curr_location()!=stack_location:  move(get_curr_location(),stack_location)  stack(obj_to_stack,obj_at_bottom)

defcut_until_is_cut(obj):  whilenotis_cut(obj):  cut(obj)

### EPIC-Kitchens Example

#### 17.3.1 Query

We present an example of the demonstration given as a query to the LLM. The annotations are made for video id \(P22\_07\). This example shows the LLM generalizing to multiple new objects, new locations, and a different dishwashing style. Figure 14 shows visualizations of the video with the respective state annotations. Because each demonstration has 25 states, we have placed the query at the end of the appendix in section 19.2.

#### 17.3.2 Intermediate Output

Below, in stage 1 recursive summary, for each demonstration, Demo2Code summarizes each trajectory from a state-based trajectory to a low-level action trajectory to a high-level subtask trajectory, before the LLM decides that the trajectory is sufficiently summarized.

Then, Demo2Code concatenates all summarized trajectories together to identify the user's preference and generate an appropriate task specification. Specifically, the LLM was able to identify that it should use a for-loop because [pick up, soap, move object from A to B] are repeated. It finally produced the correct specification that will get passed to stage 2 to produce the correct code.

**Stage 1 Recursive Summary: recursive call # 1 with state-based trajectories as input**

===================summarizing demo===================

[[Reasoning:]] Theinputtrajectorycontainsstatedpredicatesbecausethetrajectory

talksaboutthestatusoftheobjects(is_in_hand('sponge_1'),

dirty('mezaluna_1'),etc.).

I will summarize thestate trajectoryintolow-levelactions.

Low-levelactionsarenotcompactedenoughyetbecauselow-level

actionscansstillbecombinedintohigh-levelsubtasks.

ThenewtrajectorywillNOTbewfficientlysummarized.

[[Isthenewtrajectorswfficientlysummarized?(yes/no):]] no

[[Summarizedtrajectory:]]

* In[Scenario1],atstate1-2,therobotpickedupmezaluna_1from

sink_1.

* Atstate2-3,therobotsoapedmezaluna_1untilitbecamesoapy.

Figure 14: Example of annotations for video P22_07 in 14 frames

* At state 3-4, the robot moved mezzaluna_1 from sink_1 tosink_2.
* At state 4-5, the robot moved peeler:potato_1 from sink_1 tosink_2.
* At state 5-6, the robot pickedup peeler:potato_1.
* At state 6-7, the robot soaped peeler:potato_1 until itbecame soapy
* At state 7-8, the robot moved peeler:potato_1 from sink_1 tosink_2.
* At state 8-9, the robot moved knife_1 from sink_1 tosink_2.
* At state 9-10, the robot pickedup knife_1.
* At state 10-11, the robot soaped knife_1 until itbecame soapy.
* At state 11-12, the robot moved knife_1 from sink_1 tosink_2.
* At state 12-13, the robot moved board:cutting_1 fromsink_1 to sink_2. sink_2.
* At state 13-14, the robot pickedup board:cutting_1.
* At state 14-15, the robots soaped board:cutting_1 until itbecame soapy.
* At state 15-16, therobot moved board:cutting_1 fromsink_1 to sink_2.
* At state 16-17, the robot turned ontap_1.
* At state 17-18, the robot pickedup board:cutting_1.
* At state 18-19, the robot rinsed board:cutting_1 until itbecame clean.
* At state 19-20, the robot placed board:cutting_1 indryingrack_1.
* At state 20-21, the robot pickedup mezzaluna_1.
* At state 21-22, the robot rinsed mezzaluna_1 until itbecame clean.
* At state 22-23, the robot placed mezzaluna_1 indryingrack_1.
* At state 23-24, the robot pickedup peeler:potato_1.
* At state 24-25, the robot rinsed peeler:potato_1 until itbecame clean.
* At state 25-26, the robot placed peeler:potato_1 indryingrack_1.
* At state 26-27, the robot pickedup knife_1.
* At state 27-28, the robot rinsed knife_1 until itbecame clean.
* At state 28-29, the robot placed knife_1 indryingrack_1.
* At state 29-30, the robot turned off tap_1.

**Stage 1 Recursive Summary: recursive call # 2 with low-level action trajectories as input**

===========================

[[Reasoning:]]

The input trajectory contains low-level actions because the trajectory mentions "picked up", "moved", "soaped", "rinsed", etc. I will summarize the low-level action trajectory intohigh-level subtasks.

High-level subtasks are compacted enough because they cannnot be combined together without losing important information. The new trajectory will be sufficiently summarized.

[[Is the new trajectory sufficiently summarized? (yes/no):]] yes

[[Summarized trajectory:]]

* In [Scenario 1], at state 1-3, therobot brought mezzaluna_1 from sink_1 tosink_2.
* At state 3-4, the robot brought peeler:potato_1 fromsink_1 to sink_2.
* At state 4-5, the robot pickedup peeler:potato_1.
* At state 5-6, the robot soaped peeler:potato_1 until itbecame soapy.
* At state 6-7, the robot moved peeler:potato_1 fromsink_1 tosink_2.
* At state 7-8, the robot moved knife_1 fromsink_1 tosink_2.
* At state 8-9, the robot picked up knife_1.
* At state 9-10, the robot soaped knife_1 until itbecame soapy.
* At state 10-11, the robot moved knife_1 from sink_1 tosink_2.
* At state 11-12, the robot moved board:cutting_1 fromsink_1 to sink_2.
* At state 12-13, the robot picked upboard:cutting_1.
* At state 13-14, the robot soaped board:cutting_1 until itbecame soapy.

* At state 14-15, the robot moved board:cutting_1 from sink_1 to sink_2.
* At state 15-16, the robot turned on tap_1.
* At state 16-19, the robot rinsed board:cutting_1, mezzaluna_1, peeler:potato_1, and knife_1 until they became clean.
* At state 19-20, the robot placed board:cutting_1 indryingrack_1.
* At state 20-21, the robot picked up mezzaluna_1.
* At state 21-22, the robot rinsed mezzaluna_1 until it became clean.
* At state 22-23, the robot placed mezzaluna_1 in dryingrack_1.
* At state 23-24, the robot picked up peeler:potato_1.
* At state 24-25, the robot rinsed peeler:potato_1 until it became clean.
* At state 26-27, the robot picked up knife_1.
* At state 27-28, the robot rinsed knife_1 until it became clean.
* At state 28-29, the robot placed knife_1 in dryingrack_1.
* At state 29-30, the robot turned off tap_1.

Summarized Demonstrations -> Task Specification

========================== summary -> spec==============================

[[Reasoning:]]
* There are 4 objects thatneed to be washed: [mezzaluna_1, peeler: potato_1, knife_1, board:cutting_1]
* The list of high-level actions that happened in order is: [move object from A to B, pick up object, soap object, move object from A to B, turn ontap, rinse objects, place object indrying rack, turn off tap]
* The actions [pick up, soap, moveobject from A to B] are repeated for each object, so we can use a loop.
* Rinsing involves the use of tap water, which is why it is turned on at some point before the rinse cycle, and turned off after.
[[Task Specification:]]

Wash objects in sink. All cleanobjects must be placed in drying rack. Specifically -

Get a list of all objects to wash For each object in all objects: Move object from sink_1 to sink_2 Pick up object Soap object Move object from sink_1 to sink_2 Turn ontap_1 For each object in all objects: Rinse object Place object indryingrack_1 Turn off tap_1

This is the final output of Demo2Code, which is the task code to solve this problem. The LLM captures that the objects are soapped first, then rinsed one by one after. The code also illustrates the LLM's ability to write complex code that has control flows (e.g. for loop and if-else statements.)

objects = get_all_objects() forobject inobjects: bring_objects_to_loc([object], "sink_2") pick_up(object) soap(object) bring_objects_to_loc([object], "sink_1") turn_on("tap_1") for object in objects: rinse(object) place(object, "dryingrack_1")turn_off("tap_1")

## 18 Prompts

All prompts are in yaml file representation. For the most updated prompt, please refer to our code base: https://portal-cornell.github.io/demo2code/.

### Tabletop Manipulation Task Prompts

#### 18.1.1 Recursive Summarization

recursive_summarization: main: |  Youareahelpfulsummarierthatrecursivelysummarizesa  trajectoryintoamoreandmorecompactedform.  <end_of_system_message>  Youaregivenatrijectory.Yourgoalistosummarizethe  trajectoryintoamorecompactedformandthendeterminewhether  thestatetrajectoryissufficientlysummarized.  Youmustrespondusingthefollowingform(younustgenerate[[end  ofresponse]]atheveryend):  [[Reasoning:]]  Youshouldfirstidentifywhattypeoftrajectorythis.Then,  youshoulddeterminewhattypeyouwillsummarizethetrajectory  into.Finally,youshoulddeterminewhetherthenewtypeof  trajectoryissufficientlysummarizedornot.  [[Isthenewtrajectorysufficientlysummarized?(yes/no):]]  Youshouldonlyrespondusingeither"yes"or"no",andnothing  else.  [[Summarizedtrajectory:]]  Youshouldactuallysummarizetheinputtrajectoryandoutputit  here.  [[endofresponse]]  Youmustfollowtheseruleswhenyouaressummarizingatrijectory.  Rules:  * Youmustslowlysummarizethetrajectoryfromonetype  anotherfollowingthisorder:astatteratory>alow-level  actiontrajectory.  * Alow-levelactiontrajectoryisrepresentedasanunordered  list.Eachlineintheunorderedlistshouldonlycontainonemole-  levelaction.  * Thelow-levelactionsmustbeconeofthefollowing:"move<item1  >","pickedupandplaced<item1><relative_position_relation><  item2>","pickedupandplaced<item1>at<lock1>","pickedupandplaced<item1>  placed<item1>ontopof'table","pickedupandplaced<item1>  ontopof<item2>".Youshouldneverdefinenewlow-levelactions.  examples:  [[InputTrajectory:]]  [Scenario1]  Placethegreencylindernexttotheyellowcylinder.  State2:  'yellowcylinder'istotherightof'greencylinder'  'greencylinder'istotheleftof'yellowcylinder'  <end_of_example_user_query>  [[Reasoning:]]  Theinputtrajectorycontainsstatepredicatesbecausethe  trajectorytalksaboutthestatusoftheobjects('yellowcylinder'  istotherightof'greencylinder').

I will summarize the state trajectory int low-level actions. The low-level actions are themostcompact form, so I should stop summarizing after this. The new trajectory will be sufficiently summarized. [[Is the new trajectory sufficiently summarized? (yes/no):]] yes [[Summarized trajectory:]] * In [Scenario 1], at state 2, therobot picked up and placed'green cylinder' to the left of 'yellow cylinder'. [[end of response]] - | [[Input Trajectory:]] [Scenario 1]  Place the purple cylinder next to the orange block.  State 2:  'orange block' is infront of 'purple cylinder'  'purple cylinder' has moved  'purple cylinder' is at 'topedge'  'purple cylinder' is behind 'orange block'  <end_of_example_user_query>  [[Reasoning:]]  The input trajectory contains state predicates because the trajectory talks about the status of the objects ('orange block'  is in front of 'purple cylinder').  I will summarize the state trajectory into low-level actions.  The low-level actions are themostcompact form, so I should stop summarizing after this.  The new trajectory will be sufficiently summarized.  [[Is the new trajectory sufficiently summarized? (yes/no):]] yes  [[Summarized trajectory:]]  * In [Scenario 1], at state 2, therobot picked up and placed' purple cylinder' behind 'orange block'.  [[end of response]] - |  [[Input Trajectory:]]  [Scenario 2]  Place the blue block at the corner.  State 2:  'blue block' has moved  'blue block' is at 'top right corner'  <end_of_example_user_query>  [[Reasoning:]]  The input trajectory contains state predicates because the trajectory talks about the status of the objects ('blue block' has moved).  I will summarize the state trajectory into low-level actions.  The low-level actions are themostcompact form, so I should stop summarizing after this.  The new trajectory will be sufficiently summarized? (yes/no):]] [[Is the new trajectory will be sufficiently summarized? (yes/no):]] - |  [[Input Trajectory:]]  Place the cyan block on the blue block.  State 2:  'orange cylinder' is on top of 'table''orangecylinder'hasmoved'orangecylinder'isnotontopof'redblock''orangecylinder'isnotontopof'cyanblock''orangecylinder'isnotontopof'yellowblock''orangecylinder'isnotontopof'yellowblock'State3:'yellowblock'isontopof'table'yellowblock'hasmoved'yellowblock'isnotontopof'redblock''yellowblock'isnotontopof'cyanblock'State4:'redblock'isontopof'table'redblock'hasmoved'redblock'isnotontopof'cyanblock''redblock'istotherightof'orangecylinder'orangecylinder'isotheleftof'redblock'State5:'cyanblock'isnotontopof'table'cyanblock'hasmoved'cyanblock'isontopof'blueblock'<end_of_example_user_query>[[Reasoning:]]Theinputtrajectorycontainsstatepredicatesbecausethetrajectorytalksaboutthestatusoftheobjects('orangecylinder'isontopof'table'.Iwillsummarizethestatterjectoryintlow-levelactions.Thelow-levelactionsarethemostcompactform,soIshouldstopsummarizingafterthis.Thenewtrajectorywillbessufficientlysummarized.[[Isthenewtrajectorysufficientlysummarized?(yes/no):]]yes[[Summarizedtrajectory:]]*In[Scenario1],atstate2,therobotpickedupandplacedthe'orangecylinder'ontopof'table'.*Atstate3,therobotpickedupandplaced'yellowblock'topof'table'.*Atstate4,therobotpickedupandplaced'redblock'ontopof'table'.*Atstate5,therobotpickedupandplaced'cyanblock'ontopof'blueblock'.[[endofresponse]]-[[InputTrajectory:]][Scenario1]Stackallblocks,whereenfocingthemaximumstackheightifthereisarequirement.State2:'orangeblock'hasmoved'orangeblock'isnotbehind'cyancylinder'isnotinfrontof'orangeblock'State3:'blueblock'isnotontopof'table'blueblock'hasmoved'blueblock'isontopof'orangeblock'State4:'brownblock'isnotnotopof'table'brownblock'isnotopof'blueblock'State5: 'yellowblock'hasmoved 'yellowblock'isbehind'cyancylinder' 'cyancylinder'isinfrontof'yellowblock'  State6: 'grayblock'isnotontopof'table' 'grayblock'hasmoved 'grayblock'isontopof'yellowblock' <end_of_example_user_query>  [[Reasoning:]]  Theinputtrajectorycontainsstatepredicatesbecausethe trajectorytalksaboutthestatusoftheobjects('orangeblock'hasmoved)..  Iwillsummarizethestatteratoryintlow-levelactions.  Thelow-levelactionsarethemostcompactform,soIshould stopsummarizingafterthis.  Thenewtrajectorywillbesufficientlysummarized.  [[Isthenewtrajectorysufficientlysummarized?(yes/no):]]  yes  [[Summarizedtrajectory:]]  *In[Scenario1],atstate2,therobotmoved'orangeblock'.  *Atstate3,therobotpickedupandplaced'blueblock'ontopof'orangeblock'.  *Atstate4,therobotpickedupandplaced'brownblock'ontopof'blueblock'.  *Atstate5,therobotmoved'yellowblock'.  *Atstate6,therobotpickedupandplaced'grayblock'ontopof'yellowblock'.  [[endofresponse]]  - [[InputTrajectory:]]  [Scenario2]  Stackallblocksintoonestack,whileenforcingtheorder betweenblocksifthereisarequirement.  State2:  'blueblock'hasmoved  State3: 'redblock'isnotontopof'table''redblock'hasmoved'redblock'isontopof'blueblock''redblock'isnotottherightof'redcylinder''redcylinder'isnotottheleftof'redblock'  State4:  'yellowblock'isnotontopof'table' 'yellowblock'hasmoved 'yellowblock'isontopof'redblock' 'yellowblock'isontopof'blueblock'  State5:  'cyanblock'isnotontopof'table' 'cyanblock'isnototopof'yellowblock' 'cyanblock'isontopof'redblock' 'cyanblock'isontopof'blueblock' <end_of_example_user_query>  [[Reasoning:]]  Theinputtrajectorycontainsstatepredicatesbecausethe trajectorytalksaboutthestatusoftheobjects('pinkblock'hasmoved).  Iwillsummarizethestatteratoryintlow-levelactions. ```The low-level actions are the most compact form, so I should stop summarizing after this. The new trajectory will be sufficiently summarized. [[Its the new trajectory sufficiently summarized? (yes/no):]] yes [[Summarized trajectory:]] * In [Scenario 2], at state 2, therobot moved 'blue block'. * At state 3, therobot picked up and placed'red block' on top of 'blue block'. * At state 4, therobot picked up and placed 'yellow block' on top of'red block'. * At state 5, therobot picked up and placed 'cyan block' on top of 'yellow block'. [[end of response]] - | [[Input Trajectory:]] [Scenario 1] Stack all objectsinto twostacks, and objects may need to be categorized in thestacks.

 State 2: 'brown block' is to the left of 'brown cylinder' 'brown cylinder' has moved 'brown cylinder' is to the right of 'brown block' 'brown cylinder' is to the left of 'blue cylinder''blue cylinder' is to the right of 'brown cylinder'

 State 3: 'orange block' is not ontop of 'table''orange block' has moved'orange block' is ontop of 'brown cylinder'

 State 4: 'yellow block' is not ontop of 'table''yellow block' has moved 'yellow block' is ontop of 'orange block''yellow block' is ontop of 'brown cylinder'

 State 5: 'pink cylinder' is not ontop of 'table''pink cylinder' has moved'pink cylinder' is ontop of 'yellow block''pink cylinder' is ontop of 'orange block''pink cylinder' is ontop of 'brown cylinder'

 State 6: 'blue block' has moved  State 7: 'brown cylinder' is not to the left of 'blue cylinder''blue cylinder' is not ontop of 'table''bluecylinder' has moved'bluecylinder' is moved'bluecylinder' is ontop of 'blue block''blue cylinder'

 State 8: 'orange cylinder' is not ontop of 'table''orange cylinder' has moved'orange cylinder' is ontop of 'blue block''orange cylinder' is ontop of 'blue cylinder'

 State 9: 'brown block' is not ontop of 'table''brown block' has moved'brown block' is ontop of 'blue block''brownblock'isnototheleftof'browncylinder''brownblock'isnottopof'orangecylinder''brownblock'isontopof'bluecylinder''brownblock'isontopof'bluecylinder''browncylinder'isnototherightof'brownblock'<end_of_example_user_query>'[[Reasoning:]] Theinputtrajectorycontainsstatepredicatesbecausethetrajectorytalksaboutthestatusoftheobjects('brownblock'istotheleftof'browncylinder').Iwillsummarizethestatetrajectoryintlow-levelactions.Thelow-levelactionsarethemostcompactform,soIshouldstopsummarizingafterthis.Thenewtrajectorywillbesufficientlysummarized.[[Itsthemewtrajectorysufficientlysummarized?(yes/no):]] yes [[Summarizedtrajectory:]] *In[Scenario1],atstate2,therobotmoved'browncylinder'.*Atstate3,therobotpickedupandplaced'orangeblock'ontopof'browncylinder'.*Atstate4,therobotpickedupandplaced'yellowblock'ontopof'orangeblock'.*Atstate5,therobotpickedupandplaced'pinkcylinder'ontopof'yellowblock'.*Atstate6,therobotmoved'blueblock'.*Atstate7,therobotpickedupandplaced'bluecylinder'ontopof'blueblock'.*Atstate8,therobotpickedupandplaced'orangecylinder'ontopof'bluecylinder'.*Atstate9,therobotpickedupandplaced'brownblock'ontopof'orangecylinder'.[[endofresponse]] ```

**Summarized Demonstrations -> Task Specification**

summary_2_spec: main:| Youareahelpfulassistantwhonanalyzeshettrajectoriesand summarizesthemintoacconcisepseudocode-styletaskspecification. <end_of_system_message> Youaregiven(1)ahigh-levelgoaland(2)onemoretrajectorieswhereeachonerepresentsadifferentscanario.Yourgoalistosummarizethesetrajectoriesintoacompacttaskpecification,writteninapseudocodesytyle.

``` Youmustrespondusingthefollowingformat(yourmustgenerate[[endofresponse]]attheveryend):[[Reasoning:]] Youshouldfirstlistouttheorderofthehigh-levelsubtasktrajectoriesinallscenarios.Then,youshouldconsidercertainsectionsofthesubtasksarerepeatedinthescanarioandcanberepresentedbyaloop.Twoscenarioshavingexactlythesamelistdoesnotmeanthatthereisalo.Alooponlyexistsamongsubtasksinoneindividualscenario. Overall,yourtaskspecificationshouldworkforallscenarios.Youshouldmakesurethatthetaskspecificationmatchestthesubtasksorderingacrossallscenarios.Youshouldalsomakesurethatthetaskspecificationusesloopswhenthereisanyrepetition. [[TaskSpecification:]] Youshouldfirststatethehigh-levelgoal.Then,youshouldsay"Specifically:"beforeoutputtingthepseudocode-styletaskspecification. ```[[endofresponse]]  You must follow these rules when you are writing the task specifications.  * You must write the tasks specifications inpseudocode style. You  should not write the task specification as a list. You cannot  number any line. examples:  - [[High-Level Goal:]]  Place the green cylinder next to the yellow cylinder.  [[Trajectories:]]  * In [Scenario 1], at state 2, the robot picked up and placed' green cylinder' to the left of 'yellow cylinder'.  * cond_of_example_user_query>  [[Reasoning:]]  With only 1 scenario, we assume that the 'green cylinder' must  always bet to the left of 'yellow cylinder'.  [[Task Specification:]]  Place the green cylinder to the left of the yellow cylinder.  [[endofresponse]]  - [[High-Level Goal:]]  Place the purple cylinder next to the orange block.  [[Trajectories:]]  * In [Scenario 1], at state 2, therobot picked up and placed 'purple cylinder' behind 'orange block'.  * In [Scenario 2], at state 2, therobot picked up and placed  the 'purple cylinder' in front of 'orange block'.  <end_of_example_user_query>  [[Reasoning:]]  Because two scenarios have two different relative positions  between 'purple cylinder' and 'orange block', we assume that the' purple cylinder' can be anywhere next to 'orange block'.  [[Task Specification:]]  Place the purple cylinder at anywhere next to the orange block.  [[endofresponse]]  - [[High-Level Goal:]]  Place the blue block at the corner.  [[Trajectories:]]  * In [Scenario 1], at state 2, therobot picked up and placed  the 'blue block' at 'bottom right corner'.  * In [Scenario 2], at state 2, therobot picked up and placed  the 'blue block' at 'top right corner'  <end_of_example_user_query>  [[Reasoning:]]  Because two scenarios have two corners ('bottom right corner',' top right corner'), we assume that the 'blue block' can be at any  random corner.  [[Task Specification:]]  Place the blue block at any random corner.  [[endofresponse]]  [[High-Level Goal:]]  Place the cyan block on the blue block.  [[Trajectories:]]  * In [Scenario 1], at state 2, therobot picked up and placed  the 'orange cylinder' on top of 'table'.  * At state 3, therobot picked up and placed 'yellow block' on  top of 'table'.  * At state 4, therobot picked up and placed'red block' on top  of 'table'.  * At state 5, therobot picked up and placed 'cyan block' on top  of 'blue block'.

<end_of_example_user_query>  [[Reasoning:]]  Although the goal is to place "cyan block" on the "blue block", the trajectories show that it needs to move other blocks ('orange cylinder', 'yellow block','red block') before finally placing' cyan block' on top of 'blue block'.  [[Task Specification:]]  [1. place the orange cylinder on the table  2. place the yellow block on the table  3. place the red block on the table  4. place the cyan block on the blue block  [[end of response]]  [[High-Level Goal:]]  Stack all blocks, where enforcing the maximum stack height if there is a requirement.  [[Trajectories:]]  * In [Scenario 1], at state 2, therobot moved 'orange block'.  * At state 3, therobot picked up and placed 'blue block' on top of 'orange block'.  * At state 4, the robot picked up and placed 'brown block' on top of 'blue block'.  * At state 5, therobot moved 'yellow block'.  * At state 6, therobot picked up and placed 'gray block' on top of 'yellow block'.  <end_of_example_user_query>  [[Reasoning:]]  Because the task is "Stack all blocks, where enforcing the maximum stack height...", we focus how high thestacks are.  Based on States 2-4, in the first stack, the blocks from bottom to top is ['orange block', 'blue block', 'brown block']. This is 3  blocks high.  Based on States 5-6, in the second stack, the blocks from bottom to top is ['yellow block', 'gray block']. This is 2 blocks high.  Because there are 2 stacks and the tallest stack is 3 block high , we assume that each stack needs to be at most 3 blocks high.  [[Task Specification:]]  Stack all blocks. However, the maximum height of a stack is 3.  [[end of response]]  [[High-Level Goal:]]  Stack all blocks intoones stack, while enforcing the order between blocks if there is a requirement.  [[Trajectories:]]  * In [Scenario 1], at state 2, therobot moved'red block'.  * At state 3, therobot picked up and placed 'yellow block' on top of'red block'.  * At state 4, the robot picked up and placed 'cyan block' on top of 'yellow block'.  * At state 5, therobot picked up and placed 'blue block' on top of 'cyan block'.  * In [Scenario 2], at state 2, therobot moved 'blue block'.  * At state 3, therobot picked up and placed'red block' on top of 'blue block'.  * At state 4, therobot picked up and placed 'yellow block' on top of'red block'.  * At state 5, therobot picked up and placed 'cyan block' on top of 'yellow block'.  <end_of_example_user_query>  [[Reasoning:]]  Because the task is "Stack all blocks, while enforcing the order ...", we focus on whether there is any ordering requirement.

In [Scenario 1], based on States 2-5, in the first stack, the blocks from bottom topis ['red block', 'yellow block', 'cyan block', 'blue block'].  In [Scenario 2], based on States 2-5, in the first stack, the blocks from bottom topis ['blue block','red block', 'yellow block', 'cyan block'].  Between the order of blocks in these two scenarios, the ordering that stays the same is ['red block', 'yellow block', 'cyan block ']. [[Task Specification:]]  Stack all blocks into one stack. Also makes sure that the stacking enforces the bottom top order between these objects: ['red block', 'yellow block', 'cyan block']. [[end of response]]  - [ [High-Level Goal:]]  Stack all objects int two stacks, and objects may need to be categorized in thestacks.  [[Trajectories:]]  * In [Scenario 1], at state 2, the robot moved 'brown cylinder'.  * At state 3, the robot picked up and placed 'orange block' on  top of 'brown cylinder'.  * At state 4, the robot picked up and placed 'yellow block' on  top of 'orange block'.  * At state 5, the robot picked up and placed 'pink cylinder' on  top of 'yellow block'.  * At state 6, the robot moved 'blue block'.  * At state 7, the robot picked up and placed 'blue cylinder' on  top of 'blue block'.  * At state 8, the robot picked up and placed 'orange cylinder' on  top of 'blue cylinder'.  * At state 9, the robot picked up and placed 'brown block' on  top of 'orange cylinder'.  <end_of_example_user_query>  [[Reasoning:]]  Because the task is "Stack all objects int two stacks, and objects may need to be categorized in thestacks", we focus on  whether the objects are stacked by type.  Based on States 2-5, in the first stack, the blocks from bottom topis ['brown cylinder', 'orange block', 'yellow block', 'pink cylinder'].  Based on States 6-9, in the first stack, the blocks from bottom to top is ['blue block', 'blue cylinder', 'orange cylinder','brown block'].  Because each stack has both blocks and cylinders, we assume that it doesn't matter whether the objects are categorized.  [[Task Specification:]]  Stack all objects int two stacks. It doesn't matter whether the objects are categorized.  [[end of response]]

#### 18.1.2 Recursive Expansion

**Task Specification ->** High-Level Code

spec_2_highlevelcode: main: |  You are a Python code generator for robotics. The users will first  provide the imported Python modules. Then, for each code they  want you to generate, they provide the requirements and pseudocode  in Python docstrings.  <end_of_system_message>You need to write robot control scripts in Python code. The Python codes should be general and applicable to different environments.

Below are the imported Python libraries and functions that you can use. You CANNOT import new libraries....

Pythonkitchem robotcontrolscript import numpy as np from robot_utils import put_first_on_second, stack_without_height_limit, stack_with_height_limit from env_utils import get_obj_names, get_all_obj_names_that_match_type, determine_final_stacking_order, parse_position ALL_CONNERS_LIST = ['topleftcorner', 'toprightcorner', 'bottom leftcorner', 'bottom rightcorner'] ALL_EDGES_LIST = ['topedge', 'bottom edge', 'leftedge', 'right edge'] ALL_POSITION_RELATION_LIST = ['leftof', 'rightof', 'behind', 'in frontof']...

Below shows the docstrings for these imported library functions that your must follow. You CANNOT add additional parameters to these functions.

* robot_utils Specifications: put_first_on_second(arg1, arg2) """

You must notwritethings like: put_first_on_second("red block", "left of yellow block") put_first_on_second("red block", "top leftcorner") put_first_on_second("red block", "top edge") You can writesomething like: put_first_on_second("red block", "yellow block") put_first_on_second("red block", "table")

Pickupan object (arg1) and put it atarg2. If arg2 is an object, arg1 will be ontop of arg2. If arg2 is 'table', arg1 will be somewhere random on the table. If arg2 is a list, arg1 will be placed at location [x, y].

Parameters: arg1 (str): Astring that defines the object by its color and type (which is either "block" or "cylinder"). For example, "red block ", "orange cylinder". arg2 (list or str): If it's a list, it needs to be a list of floats, and it represents the [x, y] position to place arg1. If it's a string, it can either be "table" or astring that defines the object by its color and type. arg2 must not be a free-from string that represents a description of aposition. For example, it cannot be relativeposition (e.g. "left of yellow block"), or corner name (e.g "topleft corner"), or edgename (e.g. "topedge "). """

stack_without_height_limit(objects_to_stack) """

Stack the list of objects_to_stack inones stack without considering height limit. The first object (which is the object at the bottom of the stack) will also get moved and placed somewhere on the table.

Parameters: objects_to_stack (list): a list of strings, where each defines the object by its color and type. """

stack_with_height_limit(objects_to_stack, height_limit) """Stack the list of objects_to_stackedintopotentiallymultiple stacks,andeachstackhasamaximumheightbasedonheight_limit. Thefirstobject(whichistheobjectatthebottomofthestack) willalsogetmovedandplacedsomewhereonthetable. Parameters: objects_to_stack(list):alistofstrings,whereeachstring definestheobjectbyitscolorandtype. height_limit(int):anintegerrepresentingthemaximumheight foreachstack. """ """ *env_utilsSpecifications: get_obj_names() """ Return: alistofobjectsinthenenvironment """ get_all_obj_names_that_match_type(type_name,objects_list) """ Return: alistofobjectsinthenenvironmentthatmatchthetype_name """ determine_final_stacking_order(objects_to_enforce_order, objects_without_order) """ Return: asortedlistofobjectstostack.Thefirstobjectinthelist wouldbeatthebottomofthestack. """ parse_position(description) """ """ -relativeposition(e.g."leftofyellowblock") -cornerposition(e.g."topleftcorner") -edgeposition(e.g."topedge") Return: alist[x,y]thatrepresentsthepositiondescribedbythe description. """ """  YoumustfollowtheserviceswhenyouaregeneratingPythoncode.  *YouMUSTONLYusePythonlibraryfunctionsimportedabove.You MUSTfollowthedocstringsandspecificationforthesefunctions.  *Youmustfollowtheinstructionsprovidedbytheuser.You CANNOTaddadditionalsteps,conditionals,orloopsthatarenot  in theinstruction. examples: - |... objects=['orangeblock','yellowcylinder','greencylinder']  """ Placetheregencylindertotheleftoftheyellowcylinder.  """... <end_of_example_user_query>... #mustuseparsepositiontogetherrelativeposition location_pos=parse_position('leftoftheyellowcylinder')  put_first_on_second('greencylinder',location_pos) ... - |... objects=['orangeblock','purplecylinder']  """ Placethepurplecylinderatanywherenexttotheorangelblock.

<end_of_example_user_query> ...  position_relation =np.random.choice(ALL_POSITION_RELATION_LIST)  #mustusparsepositiontopetermativeposition  location_pos =parse_position(f'{position_relation}theorrange block')  put_first_on_second('purplecylinder',location_pos) ... . objects=['blueblock','brownblock','orangecylinder','pink cylinder','purplecylinder']  """  placeetheblueblockatanyrandomcorner.  """ ... <end_of_example_user_query> ...  corner_name =np.random.choice(ALL_CORNER_LIST)  #mustusparsepositiontopetermativeposition  corner_pos =parse_position(corner_name)  put_first_on_second('blueblock',corner_pos) ..... objects=['redblock','blueblock','cyanblock','yellowblock','orangecylinder']  """  1. placetheorangecylinderonthetable  2. placetheyellowblockonthetable  3.placetheredblockonthetable  4.placethecyanblockontheblueblock  """. <end_of_example_user_query> ...  items_to_place_first_in_order =['orangecylinder','yellowblock','redblock']  foriteminitems_to_place_first_in_order:  put_first_on_second(item,"table")  put_first_on_second('cyanblock','blueblock') .... objects=['orangeblock','blueblock','brownblock','yellowblock','grayblock','pinkblock','cyancylinder','blue cylinder']  """  Stackallblocks.However,themaximumheightofastackis3.  """ ... <end_of_example_user_query> ...  block_names =get_all_obj_names_that_match_type(type_name="block",objects_list=get_obj_names())  stack_with_height_limit(objects_to_stack=block_names, height_limit=3) ..... objects=['yellowblock','cyanblock','redblock','blueblock','greencylinder','redcylinder','bluecylinder']  """Stackallblocksintonesstack.Alosmakesurethatthestackingenforcesthebottomtotoporderbetweentheseobjects:['redblock','yellowblock','cyanblock']."""<end_of_example_user_query>...#Basedontheinstruction,firstdefinetheblocksthathavestrictorderingblock_names_with_strick_order_from_bottom_to_top=['redblock','yellowblock','cyanblock']#Then,findtherestoftheblocksthatdon'thavestrictorderblock_names=get_all_obj_names_that_match_type(type_name="block",objects_list=get_obj_names())block_names_without_order=[]forblockinblock_names:ifblocknotinblock_names_with_strick_order_from_bottom_to_top:block_names_without_order.append(block)#Callhelperfunctiontoteterminethefinalsstackingorderbeforestackingblocksstack_order_from_bottom_to_top=determinet_stacking_order(objects_to_enforce_order=block_names_with_strick_order_from_bottom_to_top,objects_without_order=block_names_without_order)stack_without_height_limit(objects_to_stack=stack_order_from_bottom_to_top)-|.-|.-objectobject=['blueblock','yellowblock','brownblock','orangeblock','pinkcylinder','browncylinder','orangecylinder','bluecylinder']"""Stackallobjectsintotwostacks(whereeachstackhasmaximumheightof4).Itdoesn'tmatterwhethertheobjectsarecategorized."""<end_of_example_user_query>...object_names=get_obj_names()#splittheobjectsinto2stacks.stack_1=object_names[:4]stack_2=object_names[4:]stack_without_height_limit(objects_to_stack=stack_1)stack_without_height_limit(objects_to_stack=stack_2)...

### Robotouille Task Prompts

#### 18.2.1 Recursive Summarization

recursive_summarization: main:| Youareahelpfulsummarizerthatrecursivelysummarizesatrajectoryintamoreandmorecompactedform.<end_of_system_message> Youaregivenatrajectory.Yourgoalistosummarizethetrajectoryintoamorecompactedformandthendeterminewhetherthestatetrajectoryissufficientlysummarized. Youmustrespondusingthefollowingform(yourustgenerate[endofresponse]]attheveryend):[[Reasoning:]] You should first identify what type of trajectory this is. Then, you should determine what type you will summarize the trajectory into. Finally, you should determine whether the new type of trajectory is sufficiently summarized or not. [[Is the new trajectory sufficiently summarized? (yes/no):]]  You should only respond using either "yes" or "no", and nothing else. [[Summarized trajectory:]]  You should actually summarize the input trajectory and output it here. [[endofresponse]]  In summary, you must follow these rules when you are summarizing a trajectory. Rules:  * You must slowly summarize the trajectory from one type to another following this order: a state trajectory > a low-level action trajectory --> a high-level subtask trajectory.  * You cannot skip a type (e.g. you cannot directly summarize a low -level reaction trajectory into a high-level subtask trajectory).  * A low-level action trajectory is represented as an unordered list. Each line in the unordered lists should only contain one low-level action.  * The low-level actions must be one of the following: "move from location1 to location2", "pick up item1", "place down item1 at location1", "stack item1 on top of item2", "unstack item from item 2", "cook item1", "cut item1". You should never define new  low-level actions.  * A high-level subtask trajectory is represented as an unordered list. Each line in the unordered list should only contain one high -level subtask. This high-level subtask should refer to one continuous section of the states. For example, you cannot say "at states 1-5, and states 10-15, the robot did ". There can only  be one interval of states.  * The high-level subtask must be one of the following: "cook [ITEM ]", "cut [ITEM]", "stack [ITEM] on top of [ITEM]", and "unstack [ TITEM] from [ITEM]". [ITEM] must be one of the following: "patchy", "letute", "top bun", "bottom bun", "cheese", "tomato".  * For the high-level subtask, you cannot define any other subtasks  that are not: cook, cut, stack, or unstack. You must use choose  from the list above.  examples:  - ]  [[Input Trajectory:]]  [Scenario 2]  Cook a patty and cut a lettuce.  State 2:  'robot' is at 'table6'  'robot1' is not at 'table7'  State 3:  'lettuce6' is not at 'table6'  'robot1' is holding 'lettuce6'  State 4:  'robot1' is at 'cutting_board6'  'robot1' is not at 'table6'  State 5:  'lettuce6' is at 'cutting_board6'  'robot1' is not holding 'lettuce6'  State 6:State 7:

 State 8:  'lettuce6' is cut

 State 9:  'robot!' is not at 'cutting_board6'  'robot!' is at 'table7'

 State 10:  'patty6' is not at 'table7'  'robot!' is holding 'patty6'

 State 11:  'robot!' is at'stove6'  'robot!' is not at 'table7'

 State 12:  'patty6' is at'stove6'  'robot!' is not holding 'patty6'

 State 13:

 State 14:

 State 15:

 State 16:  'patty6' is cooked  <end_of_example_user_query>  [[Reasoning:]]  The input trajectory contains state predicates because the trajectory talks about the status of the robot ('robot!' is at'table6') and the status of the objects ('lettuce6' is not at'table6').  I will summarize the state trajectory into low-level actions.  Low-level actions are not compacted enough yet because low-level actions can still be combined into high-levels subtasks.  The new trajectory will NOT be sufficiently summarized.  [[Is the new trajectory sufficiently summarized? (yes/no):]]  no  [[Summarized trajectory:]]  * In [Scenario 2], at state 2, therobot moved from 'table7' to  'table 6'.  * At state 3, therobot picked up 'lettuce6'.  * At state 4, therobot moved from 'table6' to 'cutting_board6'.  * At state 5, therobot placed 'lettuce6' at location'cutting_board6'.  * At state 6-8, therobot had cut 'lettuce6'.  * At state 9, therobot moved from 'cutting_board6' to 'table7'.  * At state 10, therobot picked up 'patty6'.  * At state 11, therobot moved from 'table7' to'stove6'.  * At state 12, therobot placed 'patty6' at'stove6'.  * At state 13-16, therobot had cooked 'patty6'  [[end of response]]  - [[Input Trajectory:]]  * In [Scenario 2], at state 2, therobot moved from 'table5' to  'table9'.

* At state 3, therobot picked up 'patty9'.
* At state 4, the robot moved from 'table' to'stove5'.
* At state 5, the robot placed 'patty9' on'stove5'.
* At state 6-9, the robot had cooked 'patty9'.
* At state 10, the robot moved from'stove5' to 'table5'.
* At state 11, therobot picked up 'patty3'.
* At state 12, therobot moved from 'table5' to'stove8'.
* At state 13, the robot placed 'patty3' on'stove8'
* At state 14-17, the robot had cooked 'patty3'. <end_of_example_user_query>
[[Reasoning:]] The input trajectory contains low-level actions because the trajectory mentions "moved", "picked up", etc. I will summarize the low-level action trajectory int high-level subtasks.

High-level subtask trajectory is themost compacted form that cannot be summarized anymore.

The new trajectory will be sufficiently summarized.

[[Is the new trajectory sufficiently summarized? (yes/no):]] yes

[[Summarized trajectory:]]
* In [Scenario 2], at state 2-9, the subtask is "cookputy". This subtask contains: 1.moving top pick up 'patty9' (state 2-3)

2.moving toplace 'patty9' on'stove5' (state 4-5) 3. cooking '

patty9' until it is cooked (state 6-9)

* At state 10-17, the subtask is "cookputy". This subtask contains: 1.moving to pick up 'patty3' (state 10-11) 2. moving to place 'patty3' on'stove8' (state 12-13) 3. cooking 'patty3' until it is cooked (state 14-17)

[[endofresponse]]

[[Input Trajectory:]]
[Scenario 1] Cut a lettuce beforestacking it ontop of a bottom bun. Then stack a top bun ontop of the lettuce.

State 2: 'lettuce1' is not at 'table6' 'robot1' is holding 'lettuce1'

State 3: 'robot1' is not at 'table6' 'robot1' is at 'cutting_board1'

State 4: 'robot1' is not holding 'lettuce1' 'lettuce1' is at 'cutting_board1'

State 5:

State 6:

State 7: 'lettuce1' is cut

State 8: 'lettuce1' is not at 'cutting_board1' 'robot1' is holding 'lettuce1'

State 9: 'robot1' is not at 'cutting_board1' 'robot1' is at 'table2'

[MISSING_PAGE_FAIL:73]

<end_of_example_user_query>  [[Reasoning:]]  The input trajectorycontains low-level actions because the trajectory mentions "picked up", "moved", etc.  I will summarize the low-level action trajectory into high-level subtasks.  High-level subtask trajectory is the most compacted form that cannot be summarized anymore.  The new trajectory will be sufficiently summarized.  [[Is the new trajectory sufficiently summarized? (yes/no):]]  yes  [[Summarized trajectory:]]  * In [Scenario 1], at state 2-7, the subtask is "cut lettuce".  This subtask contains: 1. pick up 'lettuce! (state 2) 2. moving  to place 'lettuce!' on 'cutting_board!' (state 3-4) 3. cutting' lettuce!' until it is cut (state 5-7)  * At state 8-10, the subtask is "stack lettuce on top of bottom  bun". This subtask contains: 1. picking up 'lettuce!' (state 8) 2.  moving to stack 'lettuce!' on 'bottom_bun!' (state 9-10)  * At state 11-14, the subtask is "stack top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun2' (  state 11-12) 2. moving tostack 'top_bun2' on 'lettuce!' (state  13-14)  [[end of response]]

**Summarized Demonstrations -> Task Specification**

summary_2_spec:  main: |  You are ahelpful assistant that analyzes the high-level subtask  trajectories and summarizes them into acconcise pseudocode-style  tasks specification.  <end_of_system_message>  You are given (1) a high-level goal and (2) one or more high-level  subtask trajectories where each one represents a different  scenario. Your goal is to summarize these trajectories into  compact tasks specification, written in apseudocode style.

 You must respond using the following format (your must generate [[  end of response]] at the very end):  [[Reasoning:]]  You should first list out the order of the high-level subtask  trajectories in all scenarios. Then, you should consider if  certain sections of the subtasks are repeated in the scenario and  can be represented by a loop. Two scenarios having exactly the  same list does not mean that there is a loop. A loop only exists  among subtasks in one individual scenario.  Overall, your tasks specification should work for all scenarios.  You should make sure that the tasks specification matches the  subtasks ordering across all scenarios. You should also make sure  that the task specification uses loops when there is any  repetition.  [[Task Specification:]]  You should first state the high-level goal. Then, you should say  "  Specifically:" before outputting the pseudocode-style task  specification.  [[end of response]]  You must follow these rules when you are writing the task  specifications.  Rules:* You must write the task specifications inpseudocode style. You should not write the task specification as a list. You cannot number any line.
* When checking forloops, you cannot compare the subtasks across multiplescenarios. Even if twoscenarios have the exact samel list of subtasks, there is NOT any loop. Loops can only exist within the list of subtasks for one individualscenario. Donor consider loops across multiplescenarios.
* | [[High-Level Goal:]] Cook a patty and cut a lettuce. [[Trajectories:]] * In [Scenario 2], at state 2-8, the subtask is "cut lettuce". This subtask contains: 1. moving to pick up 'lettuce4' (state 2-3) 2. moving to place 'lettuce4' on 'cutting_board3' (state 4-5) 3. cutting 'lettuce4' until it is cut (state 6-8) * At state 9-16, the subtask is "cook patty". This subtask contains: 1. moving to picking up 'patty2' (state 9-10) 2. moving to place 'patty2' at location'stoves' (state 11-12) 3. cooking'patty2' until it is cooked (state 13-16) * In [Scenario 2], at state 2-8, the subtask is "cut lettuce". This subtask contains: 1. moving to pick up 'lettuce6' (state 2-3) 2. moving to place 'lettuce6' on 'cutting_board6' (state 4-5) 3. cutting 'lettuce6' until it is cut (state 6-8) * At state 9-16, the subtask is "cook patty". This subtask contains: 1. moving to picking up 'patty6' (state 9-10) 2. moving to place 'patty6' at location'stove6' (state 11-12) 3. cooking'patty6' until it is cooked (state 13-16) <end_of_example_user_query> [[Reasoning:]] * In [Scenario 1]'s unique kitchen environment, the subtasks were executed in this order: ['cut lettuce', 'cook patty']. * In [Scenario 2]'s unique kitchen environment, the subtasks were executed in this order: ['cut lettuce', 'cook patty']. * Both scenarios are executing the subtasks in the same order. * There is no repetition or loop in the subtask ordering for any individual scenario. [[Task Specification:]] Cook a patty and cut a lettuce.

Specifically: Get a list of all thelettues in the kitchen. Get a list of all thepatties in the kitchen.

Decide a lettuce touse. Get a list of all the available cutting boards in the kitchen. Decide a cutting board to go to. Cut that lettuce at that cutting board.

Decide a patty touse. Get a list of all the available stoves in the kitchen. Decide a stove to go to. Cook that patty at that stove.

- In [Scenario 1], at state 2-9, the subtask is "cook patty". This subtask contains: 1. moving to pick up 'patty4' (state 2-3) 2. moving to place 'patty4' on'stove10' (state 4-5) 3. cooking, patty4' until it is cooked (state 6-9)
* At state 10-17, the subtask is "cook patty". This subtask contains: 1. moving to pick up 'patty6' (state 10-11) 2. moving to
place 'patty6' on'stove11' (state 12-13) 3. cooking 'patty6' until it is cooked (state 14-17) * In [Scenario 2], at state 2-9, the subtask is "cookputy". This subtask contains: 1. moving to pick up 'patty9' (state 2-3) 2. moving to place 'patty9' on'stove5' (state 4-5) 3. cooking'patty9' until it is cooked (state 6-9) * At state 10-17, the subtask is "cookputy". This subtask contains: 1. moving to pick up 'patty3' (state 10-11) 2. moving to place 'patty3' on'stove8' (state 12-13) 3. cooking 'patty3' until it is cooked (state 14-17) <end_of_example_user_query>  [[Reasoning:]]  * In [Scenario 1]'s unique kitchen environment, the subtasks were executed in this order: ['cookputy', 'cookputy']. * In [Scenario 2]'s unique kitchen environment, the subtasks were executed in this order: ['cookputy', 'cookputy'].  * Bothscenarios are executing the subtasks in the same order.  * In bothscenarios, we see that the subset ['cookputy'] got repeated 2 times consecutively, so we can use a for-loop in our specification.  [[Task Specification:]]  Cook two penties.

Specifically:  Get a list of all the penties in the kitchen.  For two penties, do:  Decide a paty touse.  Get a list of all the available stoves in the kitchen.  Decide a stovet to go to.  Cook that paty at that stove.  [[High-Level Goal:]]  Cut a lettuce beforestacking it on top of a bottom bun. Then  stack a top bun on top of the lettuce.  [[Trajectories:]]  * In [Scenario 1], at state 2-7, the subtask is "cut lettuce".  This subtask contains: 1. pick up 'lettuce1' (state 2) 2. moving to place 'lettuce1' on 'cutting_board1' (state 3-4) 3. cutting'lettuce1' until it is cut (state 5-7)  * At state 8-10, the subtask is "stack lettuce on top of bottom bun". This subtask contains: 1. picking up 'lettuce1' (state 8) 2. moving to stack 'lettuce1' on 'bottom_bun1' (state 9-10)  * At state 11-14, the subtask is "stack top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun2' ( state 11-12) 2. moving to stack 'top_bun2' on 'lettuce1' (state 13-14)  * In [Scenario 2], at state 2-7, the subtask is "cut lettuce".  This subtask contains: 1. pick up 'lettuce4' (state 2) 2. moving to place 'lettuce4' on 'cutting_board2' (state 3-4) 3. cutting'lettuce4' until it is cut (state 5-7)  * At state 8-10, the subtask is "stack lettuce on top of bottom bun". This subtask contains: 1. picking up 'lettuce4' (state 8) 2. moving to stack 'lettuce4' on 'bottom_bun3' (state 9-10)  * At state 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of 'lettuce4'. ( 'cookputy', 'cookputy'].  State 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  state 13-14)  * Let 11-14, the subtask is "place top bun on top of  lettuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuce4' ( state 11-14)  * Let 11-14, the subtask is "place top bun on top of  letuce4'. This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuce4' ( state 11-14)  * Let 11-14, the subtask is "place top bun on top of  letuce". This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuce4' ( state 11-14)  * Let 11-14, the subtask is "place top bun on top of  letuce". This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuce4' ( state 11-14)  * Let 11-14, the subtask is "place top bun on top of  letuce". This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuce4' ( state 11-14)  * Let 11-14, the subtask is "place top bun on top of  letuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuce4' ( state 11-14)  * Let 11-14, the subtask is "place top bun on top of  letuce". This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuce4' ( state 11-14)  * Let 11-14, the subtask is "place top bun on top of  letuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuce". This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuce4' ( state 11-14)  * Let 11-14, the subtask is "place top bun on top of  letuce". This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuce". This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuce". This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuce4' ( state 11-14)  * Let 11-14, the subtask is "place top bun on top of  letuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuceuce4' ( state 11-14)  * Let 11-14, the subtask is "place top bun on top of  letuce". This subtask contains: 1. pick up 'top_bun1' ( state 11-12) 2. moving to place 'top_bun1' on top of  letuce4' ( state 11-14)  * Let 11-14, the subtask is "place top bun on top of  letuce". This subtask contains: 1. moving to pick up 'top_bun1' ( state * In[Scenario 2]'s unique kitchen environment, the subtasks were executed in this order: ['cut lettuce','stack lettuce on top of bottom bun', 'place top bun on top of lettuce'].
* stack lettuce on top of bottom bun' and 'place top bun on top of lettuce' are essentially the same subtask.
* Both scenarios are executing the subtasks in the same order.
* There is no repetition or loop in the subtask ordering for any individual scenario. [[Task Specification:]] Cut a lettuce before stacking it on top of a bottom bun. Then stack a top bun on top of the lettuce.

Specifically:  Get a list of all the lettuces in the kitchen.  Get a list of all the bottom buns in the kitchen.  Get a list of all the top buns in the kitchen.

 Decide a lettuce to use.  Get a list of all the available cutting boards in the kitchen.  Decide a cutting board to go to.  Cut that lettuce at that cutting board.

 Decide a bottom bun to use.  Stack the lettuce on top of the lettuce.

#### 18.2.2 Recursive Expansion

spec_2_highlevelcode: main: |  Youare a Python code generator for robotics. The users will first  provide the imported Python modules. Then, fore eachcode they  want you to generate, they provide the requirements and pseudocode  in Python docstrings.  cend_of_system_message>  You need to write kitchen robot controls scripts in Python code.  The Python codes should be general and applicable to different  kitchen environments.

 Below are the imported Python libraries and functions that you can  use. You CANNOT import new libraries. ...

 # Python kitchen robot controlscript  from env_utils import get_all_obj_names_that_match_type,  get_all_location_names_that_match_type, is_cut, is_cooked ...

 Below shows thedocstrings for these imported library functions  that you must follow. You CANNOT add additional parameters to  these functions.  * env_utils Specifications:  get_all_obj_names_that_match_type(obj_type)  """  Parameters:  obj_type (str): Astring that has to be one of 'patty', 'lettice',  'bottom bun', 'top bun'.  Return:  (list) a list of string of objects that match the obj_type  """  get_all_location_names_that_match_type(location_type)  """  Parameters:location_type (str): A string that has to be one of 'table', 'cutting_board','stove'. Return:  (list) a list of string of locations that match the location_type  ""  is_cut(obj)  ""  Parameters:  obj(str): A string that has the type of a cuttable object ( must be 'lettuce') and anid. For example: 'lettuce3', 'lettuce1'  Return:  (boolean) true if obj is cut  ""  is_cooked(obj)  ""  Parameters:  obj(str): A string that has the type of a cookable object ( must be 'patty') and anid. For example: 'patty1', 'patty2'  Return:  (boolean) true if obj is cooked  ""...

 You must follow these rules when you are generating Python code.  * You can use Python library functions imported above. You MUST  follow the docstrings and specification for these functions.  * You can also call new functions that are not yet defined.  However, these functions must be completing general, high-level  subtasks (cook, cut, place on top, unstack). These functions  cannot be doing low-level actions.  * You must follow the pseudocode provided by the user. You CANNOT  add additional steps, conditionals, or loops that are not in the pseudocode. examples:  - | ...  """  Cook a patty and cut a lettuce.

 Specifically:  Get a list of all the letuces in the kitchen.  Get a list of all the putties in the kitchen.  Decide a lettuce touse.  Get a list of all the available cutting boards in the kitchen.  Decide a cutting board togoto.  Cut that lettuce at that cutting board.

 Decide a patty touse.  Get a list of all the available stoves in the kitchen.  Decide a stovet togoto.  Cook that patty at that stove.  ""  "" <end_of_example_user_query>  "" # Get a list of all the letuces in the kitchen.  letuces = get_all_obj_names_that_match_type('lettuce')  # Get a list of all the putties in the kitchen.  patties = get_all_obj_names_that_match_type('patty')  # Decide a lettuce touse.  lettuce_to_use = letuces[0]  # Get a list of all the available cutting boards in the kitchen.

cutting_boards=get_all_location_names_that_match_type('cutting_board')
#Decideacuttingboardtogoto. cutting_board_to_cut_at=cutting_boards[0]
#Cuthat lettuceathatcuttingboard. cut_object_at_location(obj=lettuce_to_use,location=cutting_board_to_cut_at)
#Decideapatytouse. patty_to_use=patties[0]
#Getalistofalltheavailablestovesinthekitchen. stoves=get_all_location_names_that_match_type('stove')
#Decideastovetogoto. stove_to_cook_at=stoves[0]
#Cookthatpatytathathstove. cook_object_at_location(obj=patty_to_use,location=stove_to_cook_at)

__-|... """ Cooktwopaties. Specifically: Getalistofallthepattiesinthekitchen. Decideapatytouse. Getalistofalltheavailablestovesinthekitchen. Decideastovetogoto. Cookthatpatytathstove. Repeattheabovestepsforthesecondpatty. """... <end_of_example_user_query>...
#Getalistofallthepattiesinthekitchen. patties=get_all_obj_names_that_match_type('patty')
#Fortwopaties,do foriinrange(2): #Decideapatytouse. patty_to_use=patties[i]
#Getalistofalltheavailablestovesinthekitchen. stoves=get_all_location_names_that_match_type('stove')
#Decideastovetogoto. stove_to_cook_at=stoves[i]
#Cookthatpatytathstove. cook_object_at_location(obj=patty_to_use,location=stove_to_cook_at)... |... """ Cutalettecbeforestackingintotopofabottombun.Then stackattopbunontopofthelettuce.

Specifically: Getalistofallthelettucesinthekitchen. Getalistofallthebottombunisthekitchen. Getalistofallthetopbunisthekitchen.

Decidealettectouse. Getalistofalltheavailablecuttingboardsinthekitchen. Decideacuttingboardtogoto.

Cut thatlettuceatthatcuttingboard. Decideabottombutouse. Stackthelettucontopofthebottombun. Decideatopbuntouse. Stackthetopbunontopofthelettuce. """... <end_of_example_user_query>... #Getalistofallthelettucesinthekitchen. letteces=get_all_obj_names_that_match_type('lettuce') #Getalistofallthebottombuninthekitchen. bottom_buns=get_all_obj_names_that_match_type('bottom_bun') #Getalistofallthetopbunsinthekitchen. top_buns=get_all_obj_names_that_match_type('top_bun')
#Decideallettucetouse. letteces_to_use=lettuces[0] #Getalistofalltheavailablecuttingboardsinthekitchen. cutting_boards=get_all_location_names_that_match_type('cutting_board') #Decideacuttingboardtogoto. cutting_board_to_cut_at=cutting_boards[0] #Cutthatlettucatthatcuttingboard. cut_object_at_location(obj=lettuc_to_use,location= cutting_board_to_cut_at)
#Decideabottombuntouse. bottom_bun_to_use=bottom_buns[0] #Stackthelettucontopofthebottombun. #objshouldthelettuce,obj2shouldbethebottombun. stack_obj_on_obj2(obj=lettuc_to_use,obj2=bottom_bun_to_use)
#Decideatopbuntouse. top_bun_to_use=top_buns[0] #Stackthattopbunontopofthelettuces. #objshouldbethetopbun,obj2shouldbethelettuc. stack_obj1_on_obj2(obj=top_bun_to_use,obj2=lettuc_to_use)

Step 2: DefinecompositeactionsGiven a function header, the LLM outputs code that may contain undefined functions. step2: main: | YouareaPythoncodegeneratorforrobotics.Theuserswillfirst providetheimportedPythonmodules.Then,foreachcodethat theywantyoutogenerate,theyprovidetherequirementinPython docstrings. <end_of_system_message> #Pythonkitchenrobotcontrolscript fromenv_utilsimportget_obj_location,is_holding,is_in_a_stack, get_obj_that_is_underneath """ Allthecodeshouldfollowthespecification. env_utilsSpecifications: get_obj_location(obj) Parameters: obj(str):Astringthathasthetypeofobject(oneof' lettuce','patty','bottom_bun','top_bun')andanid.Forexample :'lettuces','patty7','bottom_bun1','top_bun4'Return: (str)locationthattheobjectiscurrentlyat.Astring thathasthetypeoflocation(oneof'table','cutting_board', stove')andandid.Forexample:'table2','cutting_board1',' stove5' is_holding(obj) Parameters: obj(str):Astringthathasthetypeofobject(oneof' lettuce','patty','bottom_bun','top_bun')andandid.Forexample :'lettuce5','patty7','bottom_bun1','top_bun4' Return: (bool)trueiftherobotscurrentlyholdingobj is_in_a_stack(obj) Parameters: obj(str):Astringthathasthetypeofobject(oneof' lettuce','patty','bottom_bun','top_bun')andandid.Forexample :'lettuce5','patty7','bottom_bun1','top_bun4' Return: ((b0)trueiftheobjisinastackandtherobotneeds tounstacktheobj get_obj_that_is_underneath(obj_at_top) Parameters: obj_at_top(str):Astringthathasthetypeofobject( oneof'lettuce','patty','bottom_bun','top_bun')andandid.For example:'lettuce5','patty7','bottom_bun1','top_bun4' Return: (str)Thenameoftheobjectthatisdirectlyunderneath theobj_at_top.Astringthathasthetypeofobject(oneof' lettuce','patty','bottom_bun','top_bun')andandid.Forexample :'lettuce5','patty7','bottom_bun1','top_bun4' """ Therobotisonlycapableofthesebasicactions:move,pick, place,stack,unstack,cook,cut. Thesefunctionsrequiretherobottoholdtheobject:place,stack Thesefunctionsrequiretherobottonotholdanything:pick, unstack Youncandefinefunctionsthatcombinetwooftheseactions,but youcannotdefinewactions. """ examples: - |... """ Youcannestfunctionsimportedaboveandalsocallnewfunctions Therobotmightnotbecaranyobjectornearanylocation specifiedinthefunctionparameters. ```

``` """ <end_of_example_user_query>... defcut_object_at_location(obj,location): #Tocutanobject,therobotfirstneedstobelodingobj ifnotis_holding(obj): #Iftherobotisnotholdingobj,thereare2scenarios: #(1)ifobjisinastack,unstackobj #(2)else,pickupobj. ifis_in_a_stack(obj): #Becauseobjisinastack,robotneedtomovethen unstacktheobjfromtheobj_at_bottomfirst

[MISSING_PAGE_FAIL:82]

Therobotmightnotbeceranyobjectornearanylocation specifiedinthefunctionparameters.  Definethefunction:unstack_obj1_from_obj2(obj1,obj2) ...  <end_of_example_user_query> ...  defunstack_obj1_from_obj2(obj1,obj2):  #Tounstackobj1fromobj2,therobotneedstonothold  anythingyet.  ifis_holding(obj1):  #Becausetherobotisholdingobj1,unstackingmust  havebeensuccessfulalready  return  #determinethelocationofobj2tounstackfrom  obj2_location=get_obj_location(obj2)  #movethenunstack:firstyourmovetoobj2'slocation,then  younstackobj1fromobj2  move_then_unstack(obj_to_unstack=obj1,obj_at_bottom=obj2,  unstack_location=obj2_location) ...  -...  """  Youcanusefunctionsimportedaboveandalsocallnewfunctions  Therobotmightnotbeceranyobjectornearanylocation  specifiedinthefunctionparameters.  Definethefunction:cook_object_at_location(obj,cook_location) ...  <end_of_example_user_query> ...  defcook_object_at_location(obj,cook_location):  #Tcookanobject,therobotfirstneedstobeholdingobj  ifnotis_holding(obj):  #Iftherobotisnotholdingobj,thereare2scenarios:  #(1)ifobjisinastack,unstackobj  #(2)else,pickupobj.  ifis_in_a_stack(obj):  #Becauseobjisinastack,robotneedtomovethen  unstacktheobjfromtheobj_at_bottomfirst  obj_at_bottom=get_obj_that_is_underneath(obj_at_top=  obj)  #movethenunstack:firstyourmovetothe  obj_at_bottom'slocation,thenyourstackobjfromobj_at_bottom  move_then_unstack(obj_to_unstack=obj,obj_at_bottom=  obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom))  else:  #move_then_pick:firstyourmovetoobj'slocation,  thenyourpickobjup  move_then_pick(obj=obj,pick_location=get_obj_location (obj))  #movethenplace:firstyourmovetothelocationtocokat,  thenyourplaceobjatthatlocation  move_then_place(obj=obj,place_location=cook_location)  #cooktheobjectuntilitiscooked  cook_until_is_cooked(obj=obj) ...

step3:

[MISSING_PAGE_FAIL:84]

obj(str):objecttocut.Astringthathasthetypeofacutableobject(mustbe'lettuce')andandid.Forexample:'lettuce3','lettuce1'start_cooking(obj) Requirement:Therobotmustbeatthesamelocationsabbj. Startcookingtheobj.Youonlyneedtocallthisonce.Theobjwilltakeanunknownamounbeforeitiscooked. Parameters: obj(str):objecttocouk.Astringthathasthetypeofacookableobject(mustbe'patty')andandid.Forexample:'patty1','patty5'noop()Donothing stack(obj_to_stack,obj_at_bottom) Requirement: Therobotmustbeatthesamelocationsabbj_at_bottom. Stackobj_to_stackontopofobj_at_bottom Parameters: obj_to_stack(str):objecttocak.Astringthathasthetypeofobject(oneof'lettuce','patty','bottombu','topbun')andandid.Forexample:'lettuce5','patty7','bun1'unstack(obj_to_unstack,obj_at_bottom) Requirement: Therobotmustbeatthesamelocationsabbj_at_bottom Unstackobj_to_unstackfromobj_at_bottom Parameters: obj_to_unstack(str):objecttocunstack.Astringthathasthetypeofobject(oneof'lettuce','patty','bottombu','topbun')andandid.Forexample:'lettuce5','patty7','bun1'obj_at_bottom(str):objecttocunstackfrom.Astringthathasthetypeofobject(oneof'lettuce','patty','bottombu','topbun')andandid.Forexample:'lettuce5','patty7','bun1'obj_at_bottom(str):objecttocunstackfrom.Astringthathasthetypeofobject(oneof'lettuce','patty','bottombu','topbun')andandid.Forexample:'lettuce5','patty7','bun1','topbun')andandid.Forexample:'lettuce5','patty7','bun1'env_utilsSpecifications:is_cut(obj) Parameters:obj(str):Astringthathasthetypeofacutableobject(mustbe'lettuce')andandid.Forexample:'lettuce3','lettuce1'Return: Return: get_curr_location() Return: (str)locationthattherobotiscurrentlyat.Astringthathasthetypeoflocation(oneof'table','cutting_board',stove')andandid.Forexample:'table2','cutting_board1',stove5'get_obj_location(obj)Parameters: obj(str):Astringthathasthetypeofacuttabledobject (mustbe'lettuce')andanid.Forexample:'lettuce3','lettuce1, Return: (str)locationthattheobjectiscurrentlyat.Astring thathasthetypeoflocation(oneof'table','cutting_board', stove')andanid.Forexample:'table2','cutting_board1',' stove5' """ examples: - |... """  Youcanonlyusethefunctionsimportedintheheader.  Definethefunction:basic_move(target_loc)  Movetoanylocationspecifiedthetarget_loc. ...  <end_of_example_user_query> ...  defbasic_move(target_loc):  ifget_curr_location()!=target_loc:  move(get_curr_location(),target_loc) ... - |...  """  Youcanonlyusethefunctionsimportedintheheader.  Definethefunction:cook_until_is_cooked(obj)  """ ...  <end_of_example_user_query> ...  defcook_until_is_cooked(obj):  start_cooking(obj)  while_not_is_cooked(obj):  moop() ...  - |...  """  Youcanonlyusethefunctionsimportedintheader.  Definethefunction:move_then_stack(obj_to_stack,obj_at_bottom) ...  <end_of_example_user_query> ...  defmove_then_stack(obj_to_stack,obj_at_bottom):  #Forstacking,becausethelocationisnotprovided,we needtodeterminethestack_location  stack_location =get_obj_location(obj_at_bottom)  ifget_curr_location()!=stack_location:  move(get_curr_location(),stack_location)  stack(obj_to_stack,obj_at_bottom) ...

### EPIC Kitchens Task Prompts

#### 18.3.1 Recursive Summarization

recursive_summarization: main: | Yoareareahelpfulsummarizerthatrecursivelysummarizesa trajectoryintomaoreandmorecompactedform. <end_of_system_message> Youaregivenartrajectory.Yourgoalsistosummarizethe trajectoryintomaorecompactedformandthendeterminewhether thestatetrajectoryissufficientlysummarized.

Youmustrespondusingthefollowingform(younustgenerate[[endofresponse]]attheveryend): [[Reasoning:]] Youshouldfirstidentifywhattypeoftrajectorythisis.Then, youshoulddeterminewhattypeyouwillsummarizethetrajectoryinto.Finally,youshoulddeterminewhetherthenewtypeoftrajectoryissufficientlysummarizedornot. [[Isthenewtrajectorysufficientlysummarized?(yes/no):]] Youshouldonlyrespondusingeither"yes"or"no",andnothingelse. [[Summarizedtrajectory:]] Youshouldactuallysummarizetheinputtrajectoryandoutputit here. [[endofresponse]] Youmustfollowtheseruleswhenyouaresummarizingatrajectory. Rules: *Youmustslowlysummarizethetrajectoryfromonetypeto anotherfollowingthisorder:astatteractory>alow-levelactiontrajectory-->ahigh-levelsubtasktrajectory. *Youcannotskipaptype(e.g.yocannotdirectlysummarizealow-levelactiontrajectoryintomahigh-levelsubtasktrajectory). *Alow-levelactiontrajectoryisrepresentedasanunorderedist. *Thelow-levelactionsmustbeconeofthefollowing:"therobotmovedfromlocation1tolocation2","therobotpickedupitem1","therobotplaceddownitem1atlocation1","therobotsoapeditem1untilitbecamsoappybecauseitem1wasdirty","therobotrinseditem1untilitbecamcelean","therobotturnedontap1","therobotturnedofftap2".Youshouldneverdefinenewlow-levelactions. *Ahigh-levelsubtasktrajectoryisrepresentedasanunorderedist.Thishigh-levelsubtaskshouldrefertoonecontinuous sectionoftthestates.Forexample,yocannotsay"atstates1-5,andstates10-15,therobotdid".Therecanonlybecone intervalofstates. *Thehigh-levelsubtaskmustbeconeofthefollowing:"brought[ITEMS]from[LOCATION1]to[LOCATION2]","soaped[ITEM]untilitbecamce soap","rinsed[ITEM]untilitbecamcelean","pickedup[ITEM]","placed[ITEMS]in[LOCATION]","turnedon[TAP]","turnedoff[TAP]".Youshouldneverdefinenewhigh-levelsubtasks.Youmustchosefromthelistabove. examples: - [[ImputTrajectory:]] [Scenario1] Washobjectsatasink.Allcleanedobjectsshouldbeplacedinthesink. Objectstoclean -spoon_1atcountertop_1- fork_1atcountertop_1 InitialCondition(State1): at('countertop_1') is_dirty('spoon_1') is_dirty('fork_1') near('spoon_1') near('fork_1') State2: is_in_hand('fork_1') is_in_hand('spoon_1') far('sink_1') State3: at('sink_1') State4: is_soaps('fork_1') is_on(('tap_1') State5: is_soaps('spoon_1') State6: is_clean('spoon_1') State7 is_clean('fork_1') State8 is_off('tap_1') in('fork_1','sink_1') in('spoon_1','sink_1') <end_of_example_query> [[Reasoning:]] Theinputtrajectorycontainsstatepredicatesbecausethe trajectorytalksaboutthestatusofrobot(at('countertop_1')) andtheobjects(is_dirty('spoon_1')). Iwillsummarizethestatterjectoryintlow-levelactions. Low-levelactionsarenotcompactedenoughyetbecauselow-levelactionscanstillbeccombinedintothighigh-levelssubtasks. ThenewtrajectorywillNOTbesufficientlysummarized. [[Isthenewtrajectorysufficientlysummarized?(yes/no):]] no [[Summarizedtrajectory:]] *In[Scenario1],atstate1-2,therobotpickedupspoon_1. Therobotpickedupfork_1. *Atstate2-3,therobotmovedfromcountertop_1tosink_1. *Atstate3-4,therobotturnedontap_1.Therobotsoapedfork_1untilitbecamesoapybecausefork_1wasdirty. *Atstate4-5,therobotsoapedspoon_1untilitbecamesoapybecausespoon_1wasdirty. *Atstate5-6,therobotrinsedspoon_1untilitbecameclean. *Atstate6-7,therobotrinsedfork_1untilitbecameclean. *Atstate7-8,therobotturnedofftap_1.Therobotplacedspoon_1insink_1. Therobotplacedfork_1insink_1. [[endofresponse]]
- [[InputTrajectory:]] [Scenario1] Washobjectsatasink.Allcleanedobjectsshouldbeplacedinthesink. Objectstoclean -spoon_1atdishwasher_1
- plate_1 at sink_1
- plate_2 at sink_1

Initial Condition (State 1): in('spoon_1', 'dishwasher_1') in('plate_1','sink_1') in('plate_2','sink_1')

State 2: is_in_hand('spoon_1')

State 3: at('sink_1') is_dirty('spoon_1')

State 4: is_on(('tap_1')

State 5: is_foamy('sponge_1')

State 6: is_soapy('spoon_1')

State 7: is_clean('spoon_1')

State 8: in('spoon_1','sink_1')

State 9: is_off('tap_1') is_in_hand('plate_1') is_dirty('plate_1')

State 10: is_on(('tap_1') is_in_hand('sponge_1') is_foamy('sponge_1')

State 11: is_soapy('plate_1')

State 12: is_clean('plate_1')

State 13: in('plate_1','sink_1')

State 14: is_off('tap_1')

State 15: is_in_hand('plate_2') is_dirty('plate_2')

State 16: is_on(('tap_1')

State 17: is_foamy('sponge_1')

State 18: is_soapy('plate_2')State 19: is_clean('plate_2')  im('plate_2','sink_1')  <end_of_example_query>  [[Reasoning:]]  The input trajectorycontains statepredicates because the trajectory talks about thestatus of the objects (in('spoon_1',. dishwasher_1')).  I will summarize the state trajectory into low-level actions. Low-level actions are not compacted enough yet because low-level actions can still be combined into high-levels subtasks.  The new trajectory will NOT be sufficiently summarized.  [[Is the new trajectory sufficiently summarized? (yes/no):]]  no  [[Summarized trajectory:]]  * In [Scenario 1], at state 1-2, therobot picked upspoon_1 from dishwasher_1.  * At state 2-3, therobot moved from dishwasher_1 tosink_1.  * At state 3-4, the robot turned ontap_1.  * At state 4-5, the robot picked upsponge_1, which wasfoamy.  * At state 5-6, the robot soaped spoon_1 until it becames soapy because spoon_1 was dirty.  * At state 6-7, therobot rinsed spoon_1 until it became clean.  * At state 7-8, therobot placed spoon_1 insink_1.  placed sponge_1 in sink_1.  * At state 8-9, the robot turned off tap_1. Therobot picked up plate_1.
* At state 9-10, therobot turned ontap_1. Therobot picked up sponge_1, which wasfoamy.
* At state 10-11,the robot soaped plate_1 until it becames soapy because plate_1 was dirty.
* At state 11-12, the robot rinsed plate_1 until it becameclean. .
* At state 12-13, the robot placed plate_1 insink_1. Therobot placed sponge_1 in sink_1.
* At state 13-14, the robot turned off tap_1.
* At state 14-15, the robot picked up plate_2.
* At state 15-16, the robot turned ontap_1.
* At state 16-17, therobot picked upsponge_1, which wasfoamy.
* At state 17-18, therobot soaped plate_2 until it becames soapy because plate_2 wasdirty.
* At state 18-19, the robot rinsed plate_2 until it becameclean. . The robot placed plate_2 insink_1. Therobot turned off tap_1.  [[end of response]]  - [[[Input Trajectory:]]  [Scenario 1], at state 1-2, therobot picked upspoon_1.  The robot picked up fork_1.
* At state 2-3, the robot moved from countertop_1 tosink_1.
* At state 3-4, the robot turned ontap_1. The robots soaped fork_1 until it becames soapy because fork_1 wasdirty.
* At state 4-5, the robot soaped spoon_1 until it becames soapy because spoon_1 was dirty.
* At state 5-6, the robot rinsed spoon_1 until it became clean.
* At state 6-7, the robot rinsed fork_1 until it became clean.
* At state 7-8, the robot turned off tap_1. Therobot placed spoon_1 insink_1.  The robot placed fork_1 insink_1.  <end_of_example_query>  [[Reasoning:]]  The input trajectory contains low-level actions because the trajectory mentions "picked up","moved", etc.  I will summarize the low-level action trajectory into high-level subtasks.
High-level subtasks are compacted enough because they cannot be combined together without losing important information. The new trajectory will be sufficiently summarized. [[Is the new trajectory sufficiently summarized? (yes/no):]] yes [[Summarized trajectory:]] * In [Scenario 1], at state 1-3, therobot broughtsoon_1 and fork_1 from countertop_1 to the sink_1. * At state 3-4, turned on tap_1 * At state 3-4, soaped fork_1 until itbecamesoapy * At state 4-5, soaped spoon_1 until itbecamesoapy * At state 5-6, rinsed spoon_1 until itbecamclean. * At state 6-7, rinsed fork_1 until itbecamclean. * At state 7-8, turned off tap_1, and placed spoon_1 and fork_1 in sink_1. [[endofresponse]] - [
[[Input Trajectory:]] [Scenario 1] * In [Scenario 1], at state 1-2, therobot pickedupspoon_1 from dishwasher_1. * At state 2-3, therobot moved from dishwasher_1 tosink_1. * At state 3-4, therobot turned ontap_1. * At state 4-5, therobot pickedupsponge_1, which was foamy. * At state 5-6, the robot soaped spoon_1 until itbecamesoapy because spoon_1 was dirty. * At state 6-7, the robot rinsed spoon_1 until itbecamclean. * At state 7-8, therobot placed spoon_1 insink_1. Therobot placed sponge_1 insink_1. * At state 8-9, therobot turned off tap_1. Therobot pickedup plate_1. * At state 9-10, therobot turned ontap_1. Therobot pickedup sponge_1, which was foamy. * At state 10-11,the robots soaped plate_1 until itbecamseqy because plate_1 wasdirty. * At state 11-12, the robot rinsed plate_1 until itbecamclean. * At state 12-13, therobot placed plate_1 insink_1. Therobot placed sponge_1 insink_1. * At state 13-14, therobot turned off tap_1. * At state 14-15, therobot pickedup plate_2. * At state 15-16, the robot turned ontap_1. * At state 16-17, the robot pickedupsponge_1, which was foamy. * At state 17-18, the robot soaped plate_2 until itbecamseqy because plate_2 wasdirty. * At state 18-19, the robot rinsed plate_2 until itbecamclean. Therobot placed plate_2 insink_1. Therobot turned off tap_1. <end_of_example_query> [[Reasoning:]] The input trajectory contains low-level actions because the trajectory mentions "pickedup","moved", etc. I will summarize the low-level action trajectory intogh-level subtasks. High-level subtasks are compacted enough because they cannot be combined together without losing important information. The new trajectory will be sufficiently summarized. [[Is the new trajectory sufficiently summarized? (yes/no):]] yes [[Summarized trajectory:]] * In [Scenario 1], at state 1-3, broughtspoon_1 from dishwasher_1 to sink_1 * At state 3-4, turned ontap_1. * At state 4-6, soaped spoon_1 until itbecamseqy * At state 6-7, rinsed spoon_1 until itbecamclean * At state 7-8, placed spoon_1 in sink_1* At state 8-9, turned off tap_1 and picked up plate_1
* At state 9-10, turned on tap_1.
* At state 10-11, soaped plate_1 until it becames soapy
* At state 11-12, rinsed plate_1 until itbecameclean
* At state 12-13, placed plate_1 insink_1
* At state 13-15, turned off tap_1 and picked up plate_2
* At state 15-16, turned on tap_1
* At state 16-18, soaped plate_2 until itbecamec soapy
* At state 18-19, rinsed plate_2 until itbecameclean, turned  off tap_1, and placed plate_2 insink_1  [end of response]]

### Summarized Demonstrations -> Task Specification

summary_2_spec:  main: |  Youare ahelpful assistant whoanalyzes thetrajectories and  summarizes them intoac concise pseudocode-style taskspecification . .  cend_of_system_message>  Youaregiven (1) a high-level goal and (2) one ormore  trajectorieswhere eachone represents a differentscenario. Your  goal is to summarize these trajectoriesintacompact task  specification, written in apseudocode style.

  Youmust respondusing thefollowing format (your must generate [[  end of response]] att thevery end):  [[ Reasoning:]]  You should first list out the order of the high-level subtask  trajectories in all scenarios. Then, you should consider if  certain sections of the subtasks are repeated in the scenario and  can be represented by a loop. Two scenarios having exactly the  same list does notmean that there is a loop. A loop only exists  among subtasks in one individual scenario.  Overall, your taskspecification should work for all scenarios.  You should make sure that the task specification matches the  subtasks ordering across all scenarios. You should also make sure  that the task specification uses loops when there is any  repetition.  [[Task Specification:]]  You should first state the high-level goal. Then, you should say "  Specifically:" before outputting the pseudocode-style task  specification.  [[end of response]]  You must follow these rules when you arewriting the task  specifications.  * You must write the taskspecifications in pseudocode style. You  should not write the taskspecification as a list. You cannot  number any line.  examples:  - [[High-Level Goal:]]  Wash objects at asink. All cleaned objects should be placed in  the sink.  Objects toclean  - spoon_1 at countertop_1  - fork_1 at countertop_1

  Initial Condition (State 1):  at('countertop_1')  is_dirty('spoon_1')  is_dirty('fork_1')near('spoon_1')  near('fork_1')  [[Trajectories:]]  * In [Scenario 1], at state 1-3, therobot broughtspoon_1 and fork_1 from countertop_1 tothesink_1.  * At state 3-4, turned ontap_1  * At state 3-4, soaped fork_1 until itbecamseoapy  * At state 4-5, soaped spoon_1 until itbecamseoapy  * At state 5-6, rinsed spoon_1 until itbecamcelean.  * At state 6-7, rinsed fork_1 until itbecamcelean.  * At state 7-8, turned off tap_1, and placed spoon_1 and fork_1 insink_1.  <end_of_example_user_query>  [[R Reasoning:]]  * There are 2 dishes that got washed: [fork_1, spoon_1]  * The list of high level actions that happened in order is: [ move dishes from A to B, turn ontap, soap, soap, rinse, rinse,  turnoff tap, placed dishes insink_1]  * Because in [soap, soap], the high-level action of soapping got repeated twice (once foreach dish we brought) we canuse a for- loop.  * Because in [rinse, rinse], the high-level actionrinsing gets repeated twice (once for each dish), we canuse a for-loop.  * Since the dishes can be kept in hand, there is no place and pick up before soapping or rinsing.  * Rising involves the use of tap water, which is why it was turned on at some point before the rinse cycle, and turned off after.  [[Task Specification:]]  Wash objects at a sink. All cleaned objects should be placed in the sink.  Specifically -  Get a list of all objects towash  Bring all objects from countertop_1 tosink_1  Turn ontap  Foreach object  Soap object  For each object  Rinse object  Turn off tap  Foreach object  Placeobject insink_1  [[endof response]]  [[High-Level Goal:]]  Wash objects at asink. All cleaned objects should be placed in  thesink.  Objects toclean  - spoon_1 at countertop_1  - fork_1 at countertop_1  Initial Condition(State 1): in('spoon_1', 'dishwasher_1')  in('plate_1','sink_1')  in('plate_2','sink_1')  [[Trajectories:]]  * In [Scenario 1], at state 1-3, broughtspoon_1 from  dishwasher_1 tosink_1  * At state 3-4, turned ontap_1.  * At state 4-6, soaped spoon_1 until itbecamseoapy  * At state 6-7, rinsed spoon_1 until itbecamcelean  * At state 7-8, placed spoon_1 insink_1  * At state 8-9, turned off tap_1 and picked up plate_1  * At state 9-10, turned ontap_1.

* At state 10-11, sooped plate_1 until it becames soapy
* At state 11-12, rinsed plate_1 until it becamclean
* At state 12-13, placed plate_1 in sink_1
* At state 13-15, turned off tap_1 and pickedup plate_2
* At state 15-16, turned ontap_1
* At state 16-18, sooped plate_2 until it becamces soapy
* At state 18-19, rinsed plate_2 until it becamclean, turned off tap_1, and placed plate_2 in sink_1  <end_of_example_user_query>  [[Reasoning:]]
* There are 3 dishesgot washed: [spoon_1, plate_1, plate_2]
* The list of high level actions that happened in order is: [ movedish from A to B, turn on tap, soap, rinse, place, turn off tap, pick up, turn on tap, soap, rinse, place, turn off tap, pick up, turn on tap, soap, rinse, place, turn off tap]
* Only spoon_1 is brought to thesink from thedishwasher, other dishes are already in thesink.
* The spoon_1 does not have a pick_up action associated with it because its already in hand when brought from dishwasher_1 to sink_1. The action can be added to the code for generalizing without a side effect.
* The actions [pick_up, turn ontap, soap, rinse, place, turn off tap] are repeated for each dish, so we can use a loop.
* Rinsing involves the use of tap water, which is why it is turned on at some point before the rinse cycle, and turned off after.  [[Task Specification:]]  Wash objects at a sink. All cleaned objects should be placed in thesink.  Specifically -  Get a list of all objects to wash  Bring all objects tosink_1  For each object in all objects:  Pick_up object  Turn ontap_1  Soap object  Rinse object  Place object insink_1  Turn off tap_1  [[end of response]]

#### 18.3.2 Recursive Expansion

spec_2_highlevelcode: main: | You are a Python code generator for robotics. The users will first  provide the imported Python modules. Then, for each code they  want you to generate, they provide the requirements and pseudocode  in Python docstrings.  <end_of_system_message>  You need to write robot controls scripts in Python code. The Python  codes should be general and applicable to different environments.  Below are the imported Python libraries and functions that you can  use. You CANNOT import new libraries. ...

 # Python kitchenrobot control script  from env_utils import get_all_objects  from robot_utils import bring_objects_to_loc, turn_off, turn_on,  soap, rinse, pick_up, place, go_to, clean_with ...

[MISSING_PAGE_FAIL:95]

You must follow these rules when you are generating Python code.
* You MUST ONLY use Python library functions imported above. You MUST follow the docstrings and specifications for these functions.
* You CANNOT call define new functions. You CANNOT use functions that are NOT imported above.

*....
* Wash objects at a sink. All cleaned objects should be placed in the sink. Specifically -
* Get list of all objects to wash Bring all objects from countertop_1 tosink_1 Turn ontap_1 For each object Soap object For each object Rinse object Turn offtap_1 For each object Place object insink_1... <end_of_user_query>... objects = get_all_objects() bring_objects_to_loc(objects, "sink_1") turn_on("tap_1") for object in objects: soap(object) for object in objects: rinse(object) turn_off("tap_1") for object in objects: place(object, "sink_1")...
*...
*....
* Wash objects at a sink. All cleaned objects should be placed in the sink. Specifically -
* Get list of all objects towash Bring all objects tosink_1 For each object in all objects: Pick_up object Turn ontap_1 Soap object Rinse object Place object insink_1 Turn offtap_1....
*... <end_of_user_query>... objects = get_all_objects() bring_objects_to_loc(objects, "sink_1") for object in objects:pick_up(object)  turn_on("tap_1")  soap(object)  rines(object)  place(object, "sink_1")  turn_off("tap_1") ...

## 19 Other Long Examples

### Example Robotouille Query

```
[Scenario1] Makeaburger. ```

State2: 'patty1'isnotat'table1' 'robot1'isholding'patty1' ```

State3: 'robot1'isat'stove2' 'robot1'isnotat'table1' ```

State4: 'patty1'isat'stove2' 'robot1'isnotholding'patty1' ```

State5:

State6:

State7:

State8: 'patty1'iscooked ```

State9: 'patty1'isnotat'stove2' 'robot1'isholding'patty1' ```

State10: 'robot1'isnotat'stove2' 'robot1'isat'table3' ```

State11: 'patty1'isat'table3' 'patty1'isontopof'bottom_bun1' 'robot1'isnotholding'patty1' ```

State12: 'robot1'isnotat'table3' 'robot1'isat'table6' ```

State13: 'tomato1'isnotat'table6' 'robot1'isholding'tomato1'

[MISSING_PAGE_FAIL:98]

'lettucel'isontopof'tomato1''robot1'isnotholding'lettucel'State32:'robot1'isat'table4'robot1'isnotat'table3'State33:'top_bun1'isnotat'table4'robot1'isholding'top_bun1'State34:'robot1'isnotat'table4'robot1'isat'table3'State35:'top_bun1'isat'table3''top_bun1'isnottopof'lettucel''robot1'isnotholding'top_bun1'[Scenario2]Makeaburger. State2:'patty3'isnotat'table6'robot1'isholding'patty3'State3:'robot1'isat'stove3''robot1'isnotat'table6'
State4:'patty3'isat'stove3'robot1'isnotholding'patty3'State5:

State6:

State7:

State8:'patty3'iscooked State9:'patty3'isnotat'stove3'robot1'isholding'patty3'State10:'robot1'isnotat'stove3'robot1'isat'table5' State11:'patty3'isat'table5' 'patty3'isnottopof'bottom_bun3''robot1'isnotholding'patty3'State12:'robot1'isat'table3'robot1'isnotat'table5'

State13:'tomato3'isnotat'table3''robot'isholding'tomato3'

State 14: 'robot'isnotat'table3''robot'isat'cutting_board3'

State 15: 'tomato3'isat'cutting_board3''robot'isnotholding'tomato3'

State 16:

State 17:

State 18: 'tomato3'iscut

State 19: 'tomato3'isnotat'cutting_board3''robot'isholding'tomato3'

State 20: 'robot'isat'table5''robot'isnotat'cutting_board3'

State 21: 'tomato3'isat'table5''tomato3'isontopof'patty3''robot'isnotholding'tomato3'

State 22: 'robot'isat'table7''robot'isnotat'table5'

State 23: 'lettuce3'isnotat'table7''robot'isholding'lettuce3'

State 24: 'robot'isnotat'table7'robot'isat'cutting_board3'

State 25: 'lettuce3'isat'cutting_board3''robot'isnotholding'lettuce3'

State 26:

State 27:

State 28: 'lettuce3'iscut

State 29: 'lettuce3'isnotat'cutting_board3''robot'isholding'lettuce3'

State 30: 'robot'isat'table5''robot'isnotat'cutting_board3'State 31: 'lettuc3'isat'table5' 'lettuc3'isontopof'tomato3' 'robot'isnotholding'lettuc3'

State 32: 'robot'isat'table9' 'robot'isnotat'table5'

State 33: 'top_bun3'isnotat'table9' 'robot1'isholding'top_bun3'

State 34: 'robot1'isnotat'table9' 'robot1'isat'table5'

State 35: 'top_bun3'isat'table5' 'top_bun3'isontopof'lettuc3' 'robot1'isnotholding'top_bun3'

### Example EPIC-Kitchens Query

"""
[Scenario 1] Washobjectsinsink. Allcleanobjectsmustbeplacedindryingrack.Objectstoclean - mezzaluna_1insink_1 - peeler:potato_1insink_1 - knife_1insink_1 - board:cutting_1insink_1

InitialCondition(State 1): is_in_hand('sponge_1') in('mezzaluna_1','sink_1') at('sink_1')

State 2: is_in_hand('mezzaluna_1') dirty('mezzaluna_1')

State 3: is_soapy('mezzaluna_1')

State 4: in('mezzaluna_1','sink_2')

State 5: in('peeler:potato_1','sink_1')

State 6: is_in_hand('peeler:potato_1')

State 7: is_soapy('peeler:potato_1')

State 8: in('peeler:potato_1','sink_2')State 9: in('knife_1','sink_1')

State 10: is_in_hand('knife_1')

State 11: is_soapy('knife_1')

State 12: in('knife_1','sink_2')

State 13: in('board:cutting_1','sink_1')

State 14: is_in_hand('board:cutting_1')

State 15: is_soapy('board:cutting_1')

State 16: in('board:cutting_1','sink_2')

State 17: is_on('tap_1')

State 18: is_in_hand('board:cutting_1')

State 19: is_clean('board:cutting_1')

State 20: in('board:cutting_1', 'dryingrack_1')

State 21: is_in_hand('mezzaluna_1')

State 22: is_clean('mezzaluna_1')

State 23: in('mezzaluna_1', 'dryingrack_1')

State 24: is_in_hand('peeler:potato_1')

State 25: is_clean('peeler:potato_1')

State 26: in('peeler:potato_1', 'dryingrack_1')

State 27: is_in_hand('knife_1')

State 28: is_clean('knife_1')

State 29: in('knife_1', 'dryingrack_1')

State 30: is_off('tap_1')

### Intermediate Reasoning Ablation Helper Functions

#### 19.3.1 No Reasoning Helper Functions

```
1defmove_then_unstack(obj_to_unstack,obj_at_bottom,unstack_location) :
2#Forunstacking,wenedtomovetothelocationofthebottomobject
3ifget_curr_location()!=get_obj_location(obj_at_bottom):
4move(get_curr_location(),get_obj_location(obj_at_bottom))
5unstack(obj_to_unstack,obj_at_bottom)
6#Afterunstacking,wenedtomovetotheunstack_location
7ifget_curr_location()!=unstack_location:
8move(get_curr_location(),unstack_location)
9
10defmove_then_pick(obj):
11obj_location=get_obj_location(obj)
12ifget_curr_location()!=obj_location:
13move(get_curr_location(),obj_location)
14pick_up(obj,obj_location)
15
16defmove_then_place(obj,place_location):
17ifget_curr_location()!=place_location:
18move(get_curr_location(),place_location)
19place(obj,place_location)
20
21defcook_until_is_cooked(obj):
22start_cooking(obj)
23whilenotis_cooked(obj):
24noop()
25
26defcook_object_at_location(obj,location):
27#Tocookanobject,therobotfirstneedstobeholdingobj
28ifnotis_holding(obj):
29#Iftherobotisnotholdingobj,thereare2scenarios:
30#(1)ifobjisinastack,unstackobj
31#(2)else,pickupobj.
32ifis_in_a_stack(obj):
33#Becauseobjisinastack,robotencotmoveethen
34unstacktheobjfromtheobj_at_bottomfirst
35obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj)
36move_then_unstack(obj_to_unstack=obj,obj_at_bottom=obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom))
37else:
38#Sinceobjisnotinastack,robotcanjustmoveethen
39pickitupmove_then_pick(obj=obj)
31#placetheobjectatthelocationtocokat
32move_then_place(obj=obj,place_location=location)
33
34#cooktheobject
35cook_until_is_cooked(obj=obj)
36
37
38defcut_until_is_cut(obj):
39whilenotis_cut(obj):
40cut(obj)
41
42
43defcut_object_at_location(obj,location):
44
45#Tocutanobject,therobotfirstneedstobeholdingobj
46ifnotis_holding(obj):

# Iftherobotisnotholdingobj,thereare2scenarios:
# (1)ifobjisinastack,unstackobj
# (2)else,pickupobj.
# ifis_in_a_stack(obj):
#Becauseobjisinastack,robotneedtomovethen unstacktheobjfromtheobj_at_bottomfirst obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj) move_then_unstack(obj_to_unstack=obj,obj_at_bottom=obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom)) else:
#Sinceobjisnotinastack,robotcanjustmoveethenmove_then_pick(obj=obj)
#placetheobjectatthelocationtocutat move_then_place(obj=obj,place_location=location)
#cuttheobject cut_until_is_cut(obj=obj)
#effectmove_then_stack(obj_to_stack,obj_at_bottom,stack_location):
#ifget_curr_location()!=stack_location:
#move(get_curr_location(),stack_location) stack(obj_to_stack,obj_at_bottom)
#deststack_obj1_on_obj2(obj1,obj2):
#Tostackobjinobj2,therobotenedstobelholdingobj1 ifnotis_holding(obj1):
#iftherobotisnotholdingobj1,thereare2scenarios:
# (1)ifobjisinastack,unstackobj1
# (2)else,pickupobj1. ifis_in_a_stack(obj1):
#Becauseobjisinastack,robotneedtomovethen unstacktheobjfromtheobj_at_bottomfirst obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj1 ) move_then_unstack(obj_to_unstack=obj1,obj_at_bottom=obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom)) else:
#Sinceobj1sinotinastack,robotcanjustmoveethen pickitup move_then_pick(obj=obj1)
#determinetlocationofobj2totstackon obj2_location=get_obj_location(obj2)
#movetoobj2'slocationthenstackobj1onobj2 move_then_stack(obj_to_stack=obj1,obj_at_bottom=obj2,stack_location=obj2_location)

#### 19.3.2 Only List Helper Functions

```
1defmove_then_unstack(obj_to_unstack,obj_at_bottom,unstack_location) :
2#Forunstacking,weenedtomovetothelocationofthebot object ifget_curr_location()!=get_obj_location(obj_at_bottom):
4move(get_curr_location(),get_obj_location(obj_at_bottom)) unstack(obj_to_unstack,obj_at_bottom)
#Afterunstacking,weenedtomovetotheunstack_location
#ifget_curr_location()!=unstack_location:
#move(get_curr_location(),unstack_location)
#defmove_then_pick(obj):
#obj_location=get_obj_location(obj) ifget_curr_location()!=obj_location:
#move(get_curr_location(),obj_location)pick_up(obj,obj_location)
* 15
* 16dfmove_then_place(obj,place_location):
* 17ifget_curr_location()!=place_location:
* 18move(get_curr_location(),place_location)
* 19place(obj,place_location)
* 20
* 21defcook_until_is_cooked(obj):
* 22start_cooking(obj)
* 23whilenotis_cooked(obj):
* 24noop()
* 25
* 26defcook_object_at_location(obj,location):
* 27#Tocookanobject,therobotfirstneedstobeholdingobj
* 28ifnotis_holding(obj):
* 29#Iftherobotisnotholdingobj,thereare2scenarios:
* 20#(1)ifobjisinastack,unstackobj
* 21#(2)else,pickupobj.
* 22ifis_in_a_stack(obj):
* 23#Becauseobjisinastack,robotneedtomovethen
* 24unstacktheobjfromtheobj_at_bottomfirst
* 25obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj)
* 26move_then_unstack(obj_to_unstack=obj,obj_at_bottom=obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom))
* 27else:
* 28#Sinceobjisnotinastack,robotcanjustmoveethen
* 29pickitup
* 21move_then_pick(obj=obj)
* 22#placetheobjectatthelocationtocookat
* 23move_then_place(obj=obj,place_location=location)
* 24cooktheobject
* 25cook_until_is_cooked(obj=obj)
* 26
* 27
* 28ifbet_curr_location()!=stack_location:
* 29move(get_curr_location(),stack_location)
* 21stack(obj_to_stack,obj_at_bottom)
* 22
* 23
* 24defstack_obj_on_obj2(obj,obj2):
* 25#Tostackobjinobj2,therobotneedstobeholdingobj
* 26ifnotis_holding(obj1):
* 27#Iftherobotisnotholdingobj1,thereare2scenarios:
* 28#(1)ifobj1isinastack,unstackobj1
* 29#(2)else,pickupobj1.
* 21ifis_in_a_stack(obj1):
* 22
* 23#Becauseobj1isinastack,robotneedtomovethen
* 24unstacktheobjfromtheobj_at_bottomfirst
* 25obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj1 )
* 26move_then_unstack(obj_to_unstack=obj1,obj_at_bottom=obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom))
* 27else:
* 28#Sinceobj1isnotinastack,robotcanjustmoveethen
* 29pickitup
* 21move_then_pick(obj=obj1)
* 22#determinethelocationofobj2tostackon
* 23obj2_location=get_obj_location(obj2)
* 24#movetoobj2'slocationthenstackobj1onobj2
* 25move_then_stack(obj_to_stack=obj1,obj_at_bottom=obj2,stack_location=obj2_location)
* 26
* 27defmove_to_location(location):
* 28ifget_curr_location()!=location:
* 29move(get_curr_location(),location)* [71]defcut_object(obj,location):
* [72]#Tocutanobject,therobotfirstneedstobeholdingobj
* [73]ifnotis_holding(obj):
* [74]#Iftherobotisnotholdingobj,thereare2scenarios:
* [75]#(1)ifobjisinastack,unstackobj
* [76]#(2)else,pickupobj.
* [77]ifis_in_a_stack(obj):
* [78]#Becauseobjisinastack,robotnotnotwenten
* [79]obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj) move_then_unstack(obj_to_unstack=obj,obj_at_bottom=obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom)) else:
* [72]#Sinceobjisnotinastack,robotcanjustmoveethen pickitup move_then_pick(obj=obj)
* [73]#moveethen_pick(obj=obj)
* [74]#moveethenlocationtocutat moveet_location(location=location)
* [75]#cuttheobject cut(obj=obj)

#### 19.3.3 Full Helper Functions

```
1defmove_then_unstack(obj_to_unstack,obj_at_bottom,unstack_location) :
2#Forunstacking,weneedtomovetothelocationofthebottom object ifget_curr_location()!=get_obj_location(obj_at_bottom): move(get_curr_location(),get_obj_location(obj_at_bottom))
* [76]#Afterunstacking,weneedtomovetotheunstack_location ifget_curr_location()!=unstack_location: move(get_curr_location(),unstack_location)
* [77]
* [78]
* [79]
* [80]demove_then_pick(obj):
* [81]obj_location=get_obj_location(obj) ifget_curr_location()!=obj_location: move(get_curr_location(),obj_location) pick_up(obj,obj_location)
* [82]
* [83]
* [84]
* [85]
* [86]demove_then_place(obj,place_location):
* [87]ifget_curr_location()!=place_location: move(get_curr_location(),place_location) place(obj,place_location)
* [88]
* [89]
* [90]
* [91]defcook_until_is_cooked(obj):
* [92]start_cooking(obj) whilenotis_cooked(obj): noop()
* [93]
* [94]
* [95]
* [96]defcook_object_at_location(obj,location):
* [97]#Tocookanobject,therobotfirstneedstobeholdingobj ifnotis_holding(obj):
* [98]#Iftherobotisnotholdingobj,thereare2scenarios:
* [99]#(1)ifobjisinastack,unstackobj
* [99]#(2)else,pickupobj. ifis_in_a_stack(obj):
* [99]#Becauseobjisinastack,robotneedtomovethen unstacktheobjfromtheobj_at_bottomfirst obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj) move_then_unstack(obj_to_unstack=obj,obj_at_bottom=obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom)) else:* #Sincobjisnotinastack,robotcanjustmovethen pickitup move_then_pick(obj=obj)
* #placetheobjectatthedicationtocokat move_then_place(obj=obj,place_location=location)
* #cooktheobject cook_until_is_cooked(obj=obj)
* #defmove_then_stack(obj_to_stack,obj_at_bottom,stack_location):
* ifget_curr_location()!=stack_location: move(get_curr_location(),stack_location)
* stack(obj_to_stack,obj_at_bottom)
* defstack_obj_on_obj2(obj1,obj2):
* #Tostackobj1onobj2,therobotneedstobeholdingobj1
* ifnotis_holding(obj1):
* #Iftherobotisnotholdingobj1,thereare2scenarios:
* #(1)ifobj1isinastack,unstackobj1
* #(2)else,pickupobj1:
* ifis_in_a_stack(obj1):
* #Becauseobj1isinastack,robotneedtomovethen unstacktheobjfromtheobj_at_bottomfirst obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj1 ) move_then_unstack(obj_to_unstack=obj1,obj_at_bottom= obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom)) else:
* #Sincecobj1isnotinastack,robotcanjustmovethen pickitup move_then_pick(obj=obj1)
* #determinethelocationofobj2tostackon obj2_location=get_obj_location(obj2)
* movetoobj2'slocationthenstackobj1onobj2 move_then_stack(obj_to_stack=obj1,obj_at_bottom=obj2, stack_location=obj2_location)
* defcut_until_is_cut(obj):
* whilenotis_cut(obj):
* cut(obj)
* defcut_object_at_location(obj,location):
* #Tocutanobject,therobotfirstneedstobeholdingobj ifnotis_holding(obj):
* #Iftherobotisnotholdingobj,thereare2scenarios:
* #(1)ifobjisinastack,unstackobj
* #(2)else,pickupobj.
* ifis_in_a_stack(obj):
* #Becauseobjisinastack,robotneedtomovethen unstacktheobjfromtheobj_at_bottomfirst obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj) move_then_unstack(obj_to_unstack=obj,obj_at_bottom= obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom)) else:
* Sinceobjisnotinastack,robotcanjustmovethen pickitup move_then_pick(obj=obj)
* #placetheobjectatthedicationtocutat move_then_place(obj=obj,place_location=location)
* #cuttheobject cut_until_is_cut(obj=obj)1defmove_then_unstack(obj_to_unstack,obj_at_bottom,unstack_location) :
2#Forunstacking,wneedtomovetothelocationoftthebottomobject
3ifget_curr_location()!=get_obj_location(obj_at_bottom):
4move(get_curr_location(),get_obj_location(obj_at_bottom))
5unstack(obj_to_unstack,obj_at_bottom)
6#Afterunstacking,wneedtomovetotheunstack_location
7ifget_curr_location()!=unstack_location:
8move(get_curr_location(),unstack_location)
9
10defmove_then_pick(obj):
11obj_location=get_obj_location(obj)
12ifget_curr_location()!=obj_location:
13move(get_curr_location(),obj_location)
14pick_up(obj,obj_location)
15
16defmove_then_place(obj,place_location):
17ifget_curr_location()!=place_location:
18move(get_curr_location(),place_location)
19place(obj,place_location)
20
21defcook_until_is_cooked(obj):
22start_cooking(obj)
23whilenotis_cooked(obj):
24noop()
25
26defcook_object_at_location(obj,location):
27#Tocookanobject,therobotfirstneedstobeholdingobj
28ifnotis_holding(obj):
29#Iftherobotisnotholdingobj,thereare2scenarios:
30#(1)ifobjisinastack,unstackobj
31#(2)else,pickupobj.
32ifis_in_a_stack(obj):
33#Becauseobjisinastack,robotneedtomovethenunstacktheobjfromtheobj_at_bottomfirst
34obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj)
35move_then_unstack(obj_to_unstack=obj,obj_at_bottom=obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom))
36else:
37#Sinceobjisnotinastack,robotcanjustmovethenpickitupmove_then_pick(obj=obj)
38#placetheobjectatthelocationtocokat
39move_then_place(obj=obj,place_location=location)
41#cooktheobject
42cook_until_is_cooked(obj=obj)
43
44defmove_then_stack(obj_to_stack,obj_at_bottom,stack_location):
45ifget_curr_location()!=stack_location:
46move(get_curr_location(),stack_location)
47stack(obj_to_stack,obj_at_bottom)
48
49defstack_obj1_on_obj2(obj1,obj2):
50#Tostackobj1onobj2,therobotneedstobeholdingobj1
51ifnotis_holding(obj1):
52#Iftherobotisnotholdingobj1,thereare2scenarios:
53#(1)ifobj1isinastack,unstackobj1
54#(2)else,pickupobj1.
55ifis_in_a_stack(obj1):
56#Becauseobj1isinastack,robotneedtomovethenunstacktheobjfromtheobj_at_bottomfirst
57obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj1move_then_unstack(obj_to_unstack=obj1,obj_at_bottom=obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom)) else:
#Sinceobj1isnotinastack,robotcanjustmovethen pickitup move_then_pick(obj=obj1)
#determinetlecationofobj2totstackon obj2_location=get_obj_location(obj2)
#movetoobj2*locationthenstackobj1onobj2move_then_stack(obj_to_stack=obj1,obj_at_bottom=obj2, stack_location=obj2_location)
#effectout_until_is_cut(obj):
#whilenotis_cut(obj):
#cut(obj)
#effectout_object_at_location(obj,location):
#Toutanobject,therobotfirstneedstobehldingobj ifnotis_holding(obj):
#Iftherobotisnotholdingobj,thereare2scenarios:
#(1)ifobjisinastack,unstackobj
#(2)else,pickupobj. ifis_in_a_stack(obj):
#Becauseobjisinastack,robotneedtomovethen unstacktheobjfromtheobj_at_bottomfirst obj_at_bottom=get_obj_that_is_underneath(obj_at_top=obj) move_then_unstack(obj_to_unstack=obj,obj_at_bottom=obj_at_bottom,unstack_location=get_obj_location(obj_at_bottom)) else:
#Sinceobjisnotinastack,robotcanjustmovethen pickitup move_then_pick(obj=obj)
#placetheobjectatthelocationtocutat move_then_place(obj=obj,place_location=location)
#cuttheobject cut_until_is_cut(obj=obj)