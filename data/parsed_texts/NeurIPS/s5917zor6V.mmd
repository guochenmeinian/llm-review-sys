# On the Curses of Future and History in

Future-dependent Value Functions for OPE

 Yuheng Zhang

University of Illinois Urbana-Champaign

yuhengz2@illinois.edu

&Nan Jiang

University of Illinois Urbana-Champaign

nanjiang@illinois.edu

###### Abstract

We study off-policy evaluation (OPE) in partially observable environments with complex observations, with the goal of developing estimators whose guarantee avoids exponential dependence on the horizon. While such estimators exist for MDPs and POMDPs can be converted to history-based MDPs, their estimation errors depend on the state-density ratio for MDPs which becomes history ratios after conversion, an exponential object. Recently, Uehara et al. (2022) proposed _future-dependent value functions_ as a promising framework to address this issue, where the guarantee for memoryless policies depends on the density ratio over the _latent_ state space. However, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method. In this paper, we discover novel coverage assumptions tailored to the structure of POMDPs, such as _outcome coverage_ and _belief coverage_, which enable polynomial bounds on the aforementioned quantities. As a side product, our analyses also lead to the discovery of new algorithms with complementary properties.

## 1 Introduction and Related Works

Off-policy evaluation (OPE) is the problem of estimating the return of a new _evaluation policy_\(\pi_{e}\) based on historical data, which is typically collected using a different policy \(\pi_{b}\) (the _behavior policy_). OPE plays a central role in the pipeline of offline reinforcement learning (RL), but is also notoriously difficult. Among the major approaches, importance sampling (IS) and its variants Precup et al. (2000), Jiang and Li (2016) provide unbiased and/or asymptotically correct estimation using the _cumulative importance weights_: given a trajectory of observations and actions \(o_{1},a_{1},o_{2},a_{2},\ldots,o_{H},a_{H},\) the cumulative importance weight is \(\prod_{h=1}^{H}\frac{\pi_{e}(a_{h})|o_{h}}{\pi_{b}(a_{h}|o_{h})}\), whose variance grows _exponentially_ with the horizon, unless \(\pi_{b}\) and \(\pi_{e}\) are very close in their action distributions. In the language of offline RL theory (Chen and Jiang, 2019; Xie and Jiang, 2021; Yin and Wang, 2021), the boundedness of the cumulative importance weights is the coverage assumption required by IS, a very stringent one.

Alternatively, algorithms such as Fitted-Q Evaluation (FQE; Ernst et al., 2005; Munos and Szepesvari, 2008; Le et al., 2019) and Marginalized Importance Sampling (MIS; Liu et al., 2018; Xie et al., 2019; Nachum et al., 2019; Uehara et al., 2020) enjoy more favorable coverage assumptions, at the cost of function-approximation biases. Instead of requiring bounded cumulative importance weights, FQE and MIS only require that of the _state-density ratios_, which can be substantially smaller (Chen and Jiang, 2019; Xie and Jiang, 2021). That is, when the environment satisfies the Markov assumption (namely \(o_{h}\) is a _state_), the guarantees of FQE and MIS only depend on the range of \(d_{h}^{\pi_{e}}(o_{h})/d_{h}^{\pi_{b}}(o_{h})\), where \(d_{h}^{\pi_{e}}\) and \(d_{h}^{\pi_{b}}\) is the marginal distribution of \(o_{h}\) under \(\pi_{e}\) and \(\pi_{b}\), respectively.

In this paper, we study the non-Markov setting, which is ubiquitous in real-world applications. Such environments are typically modeled as Partially Observable Markov Decision Processes (POMDPs)Kaelbling et al. (1998). Despite the more general formulation, one can reduce a POMDP to an MDP, making algorithms for MDPs applicable: we can simply define an equivalent MDP, with its state being the _history_ of the original POMDP, \((o_{1},a_{1},\ldots,o_{h})\). Unfortunately, a close inspection reveals the problem: the state-density ratio after conversion is

\[\frac{d_{h}^{\pi_{e}}(o_{1},a_{1},\ldots,o_{h})}{d_{h}^{\pi_{h}}(o_{1},a_{1}, \ldots,o_{h})}=\prod_{h^{\prime}=1}^{h-1}\frac{\pi_{e}(a_{h^{\prime}}|o_{h^{ \prime}})}{\pi_{b}(a_{h^{\prime}}|o_{h^{\prime}})},\]

which is exactly the cumulative importance weights in IS and thus also an exponential object!

To address this issue, Uehara et al. (2022) recently proposed a promising framework called _future-dependent value functions_ (or FDVF for short). Notably, their coverage assumption is the boundedness of density ratios between \(\pi_{e}\) and \(\pi_{b}\) over the _latent state_ for memoryless policies. This is as if we were dealing directly with the latent MDP underlying the POMDP, a perhaps best possible scenario. Nevertheless, _have we achieved exponential-free OPE in POMDPs?_

The answer to this question turns out to be nontrivial. In addition to the latent-state coverage parameter, the guarantee in Uehara et al. (2022) also depends on other quantities that are less interpretable. Among them, the boundedness of FDVF itself--a concept central to this framework--is unclear, and we show that a natural construction yields an upper bound that still scales with the cumulative importance weights, thus possibly erasing the superiority of the framework over IS or MDP-reduction.

In this work, we address these caveats by proposing novel coverage assumptions tailored to the structure of POMDPs, under which fully polynomial estimation guarantees can be established. More concretely, our contributions are:

1. For FDVFs, we show that a novel coverage concept called _outcome coverage_ is sufficient for guaranteeing its boundedness (Section 4). Notably, outcome coverage concerns the overlap between \(\pi_{b}\) and \(\pi_{e}\)_from the current time step onward_, whereas all MDP coverage assumptions concern that _before_ the current step.
2. With another novel concept called _belief coverage_ (Section 5.1), we establish fully polynomial estimation guarantee for the algorithm in Uehara et al. (2022). The discovery of belief coverage also leads to a novel algorithm (Section 5.3) that is analogous to MIS for MDPs.
3. Despite the similarity to linear MDP coverage (Duan et al., 2020) due to the linear-algebraic structure, these POMDP coverage conditions also have their own unique properties due to the \(L_{1}\) normalization of belief and outcome vectors. We present improved analyses that leverage such properties and avoid explicit dependence on the size of the latent state space (Section 4.2).

## 2 Preliminaries

POMDP Setup.We consider a finite-horizon POMDP \(\left\langle H,\mathcal{S}=\bigcup_{h=1}^{H}\mathcal{S}_{h},\mathcal{A}, \mathcal{O}=\bigcup_{h=1}^{H}\mathcal{O}_{h},R,\mathbb{O},\mathbb{T},d_{1}\right\rangle\), where \(H\) is the horizon, \(\mathcal{S}_{h}\) is the latent state space at step \(h\) with \(|\mathcal{S}_{h}|=S\), \(\mathcal{A}\) is the action space with \(|\mathcal{A}|=A\), \(\mathcal{O}_{h}\) is the observation space at step \(h\) with \(|\mathcal{O}_{h}|=O\), \(R:\mathcal{O}\times\mathcal{A}\rightarrow[0,1]\) is the reward function, \(\mathbb{O}:\mathcal{S}\rightarrow\Delta(\mathcal{O})\) is the emission dynamics with \(\mathbb{O}(\cdot|s_{h})\) supported on \(\mathcal{O}_{h}\) for \(s_{h}\in\mathcal{S}_{h}\), \(\mathbb{T}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\) is the dynamics (\(\mathbb{T}(\cdot|s_{h},a_{h})\) is supported on \(\mathcal{S}_{h+1}\)), and \(d_{1}\in\Delta(\mathcal{S}_{1})\) is the initial latent state distribution. For mathematical convenience we assume all the spaces are finite and discrete, but the cardinality of \(\mathcal{O}_{h}\), \(O\), **can be arbitrarily large**. A trajectory (or episode) is sampled as \(s_{1}\sim d_{1}\), then \(o_{h}\sim\mathbb{O}(\cdot|s_{h})\), \(r_{h}=R(o_{h},a_{h})\), \(s_{h+1}\sim\mathbb{T}(\cdot|s_{h},a_{h})\) for \(1\leq h\leq H\), with \(a_{1:H}\) decided by the decision-making agent, and the episode terminates after \(a_{H}\). \(s_{1:H}\) are latent and not observable to the agent.

History-future Split.Given an episode \(o_{1},a_{1},\ldots,o_{H},a_{H}\) and a time step \(h\) of interest, it will be convenient to rewrite the episode as

\[(\tau_{h}\,,\overbrace{o_{h},\,a_{h},\,f_{h+1}}^{f_{h}}).\]

Here \(\tau_{h}=(o_{1},a_{1},\ldots o_{h-1},a_{h-1})\in\mathcal{H}_{h}:=\prod_{h^{ \prime}=1}^{h-1}(\mathcal{O}_{h}\times\mathcal{A})\) denotes the historical observation-action sequence (or simply _history_) prior to step \(h\), and \(f_{h+1}=(o_{h+1},a_{h+1},\ldots,o_{H},a_{H})\in\mathcal{F}_{h+1}:=\prod_{h^{ \prime}=h+1}^{H}(\mathcal{O}_{h^{\prime}}\times\mathcal{A})\) denotes the _future_ after step \(h\). This format will be convenient for reasoning about the system dynamics at step \(h\). We use \(\mathcal{H}=\bigcup_{h=1}^{H}\mathcal{H}_{h}\) to denote the entire history domain and use \(\mathcal{F}=\bigcup_{h=1}^{H}\mathcal{F}_{h}\) to denote the entire future domain. This way we can use stationary notation for functions over \(\mathcal{S},\mathcal{O},\mathcal{H},\mathcal{F}\), where the time step can be identified from the function input (e.g., \(R(o_{h},a_{h})\)), and functions with time-step subscripts refer to their restriction to the \(h\)-th step input space, often treated as a vector (e.g., \(R_{h}\in\mathbb{R}^{\mathcal{O}_{h}\times\mathcal{A}}\)).

Memoryless and History-dependent Policies.A policy \(\pi:\bigcup_{h=1}^{H}(\mathcal{H}_{h}\times\mathcal{O}_{h})\to\Delta(\mathcal{ A})\) specifies the action probability conditioned on the past observation-action sequence.1 In the main text, we will restrict ourselves to _memory-less_ (or reactive) policies that only depends on the current observation \(o_{h}\); extension to general policies is similar to Uehara et al. (2022) and discussed in Appendix B.6. For any \(\pi\), we use \(\Pr_{\pi}\) and \(\mathbb{E}_{\pi}\) for the probabilities and expectations under episodes generated by \(\pi\), and define \(J(\pi)\) as the expected cumulative return: \(J(\pi):=\mathbb{E}_{\pi}\left[\sum_{h=1}^{H}R(o_{h},a_{h})\right]\). For memoryless \(\pi\), we also define \(V_{\mathcal{S}}^{\pi}(s_{h})\) as the latent state value function at \(s_{h}\): \(V_{\mathcal{S}}^{\pi}(s_{h}):=\mathbb{E}_{\pi}\left[\sum_{h^{\prime}=h}^{H}R( o_{h^{\prime}},a_{h^{\prime}})\mid s_{h}\right]\in[0,H]\). \(d^{\pi}(s_{h})\) denotes the marginal distribution of \(s_{h}\) under \(\pi\).

Footnote 1: Here we do _not_ allow the policy to depend on the latent state, which satisfies sequential ignorability and eliminates data confoundedness; see Namkoong et al. (Section 3; 2020) and Uehara et al. (Appendix B; 2022). There is also a line of research on OPE in confounded POMDPs where the behavior policy _only_ depends on the latent state (Tennenholtz et al., 2020; Shi et al., 2022); see Appendix A.

Off-policy Evaluation.In OPE, the goal is to estimate \(J(\pi_{e})\) using \(n\) data trajectories \(\mathcal{D}=\{(o_{1}^{(i)},a_{1}^{(i)},r_{1}^{(i)},\ldots,o_{H}^{(i)},a_{H}^{( i)},r_{H}^{(i)}):i\in[n]\}\) collected using \(\pi_{b}\). We write \(\mathbb{E}_{\mathcal{D}}[\cdot]\) to denote empirical approximation of expectation using \(\mathcal{D}\). Define the one-step action probability ratio \(\mu(o_{h},a_{h}):=\frac{\pi_{e}(a_{h}|o_{h})}{\pi_{b}(a_{h}|o_{h})}\). We make the following assumption throughout:

**Assumption 1** (Action coverage).: We assume \(\pi_{b}(a_{h}|o_{h})\) is known and \(\max_{h,o_{h},a_{h}}\mu(o_{h},a_{h})\leq C_{\mu}\).

This is a standard assumption in the OPE literature, and is needed by IS and value-based estimators that model state value functions (Jiang and Li, 2016; Liu et al., 2018).

Belief and Outcome Matrices.We now introduce two matrices of central importance to our discussions. Given history \(\tau_{h}\), we define \(\mathbf{b}(\tau_{h})\in\mathbb{R}^{S}\) as its belief state vector where \(\mathbf{b}_{i}(\tau_{h})=\Pr(s_{h}=i|\tau_{h})\). Then the **belief matrix**\(M_{\mathcal{H},h}\in\mathbb{R}^{S\times\mathcal{H}_{h}}\) is one where the column indexed by \(\tau_{h}\in\mathcal{H}_{h}\) is \(\mathbf{b}(\tau_{h})\). Similarly, for future \(f_{h}\), we define \(\mathbf{u}(f_{h})\in\mathbb{R}^{S}\) as its outcome vector where \([\mathbf{u}(f_{h})]_{i}=\Pr_{\pi_{b}}(f_{h}|s_{h}=i)\). The **outcome matrix**\(M_{\mathcal{F},h}\in\mathbb{R}^{S\times\mathcal{F}_{h}}\) is one where the column indexed by \(f_{h}\) is \(\mathbf{u}(f_{h})\). Unlike the belief matrix, the outcome matrix \(M_{\mathcal{F},h}\) is dependent on the behavior policy \(\pi_{b}\), which is omitted in the notation.

For mathematical conveniences, we make the following assumptions throughout merely for simplifying presentations; they allow us to invert certain covariance matrices and avoid \(0/0\) situations, which can be easily handled with extra care when the assumptions do not hold.

**Assumption 2** (Invertibility).: \(\forall h\in[H]\), (1) \(\text{rank}(M_{\mathcal{H},h})=\text{rank}(M_{\mathcal{F},h})=S\).

(2) \(\forall f_{h},\ \Pr_{\pi_{h}}(f_{h})>0\); \(\forall o_{h},a_{h}\), \(R(o_{h},a_{h})>0\).

Other Notation.Given a vector \(\mathbf{a}\), \(\|\mathbf{a}\|_{\Sigma}:=\sqrt{\mathbf{a}^{\top}\Sigma\mathbf{a}}\), where \(\Sigma\) is a positive semi-definite (PSD) matrix. When \(\Sigma=\text{diag}(d)\) for a stochastic vector \(d\), this is the \(d\)-weighted 2-norm of \(\mathbf{a}\), which we also write as \(\|\mathbf{a}\|_{2,d}\). For a positive integer \(\mathbf{m}\), we use \([m]\) to denote the set \(\{1,2,\cdots,m\}\). For a matrix \(M\), we use \((M)_{ij}\) to denote the \(ij\) entry of \(M\).

## 3 Future-dependent Value Functions

In this section, we provide a recap of FDVFs and translate the main result of Uehara et al. (2022) into the finite-horizon setting, which is mathematically cleaner and more natural in many aspects; see Appendix B.2 for further discussion.

To illustrate the main idea behind FDVFs, recall that the tool that avoids the exponential weights in MDPs is to model the _value functions_. While we would like to apply the same idea to POMDPs,history-dependent value functions lead to unfavorable coverage conditions (see Section 1). The only other known notion of value functions we are left with is that over the latent state space, \(V_{\mathcal{S}}^{\pi_{e}}(s_{h})\), which unfortunately is not accessible to the learner since it operates on unobservable latent states.

The central idea is to find _observable proxies_ of \(V_{\mathcal{S}}^{\pi_{e}}(s_{h})\), which takes _future_ as inputs:

**Definition 3** (Future-dependent value functions [10]).: A future-dependent value function \(V_{\mathcal{F}}:\mathcal{F}\rightarrow\mathbb{R}\), where \(\mathcal{F}:=\bigcup_{h}\mathcal{F}_{h}\), is any function that satisfies the following: \(\forall s_{h}\), \(\mathbb{E}_{\pi_{b}}\left[V_{\mathcal{F}}(f_{h})\mid s_{h}\right]=V_{\mathcal{ S}}^{\pi_{e}}(s_{h})\). Equivalently, in matrix form, we have \(\forall h\),

\[M_{\mathcal{F},h}\times V_{\mathcal{F},h}=V_{\mathcal{S},h}^{\pi_{e}}.\] (1)

Recall our convention, that \(V_{\mathcal{F},h}\in\mathbb{R}^{|\mathcal{F}_{h}|}\) is \(V_{\mathcal{F}}\) restricted to \(\mathcal{F}_{h}\), and \(V_{\mathcal{S},h}^{\pi_{e}}\) is defined similarly.

A FDVF \(V_{\mathcal{F}}\) is a property of \(\pi_{e}\), but also depends on \(\pi_{b}\). As we will see later in Section 4, the boundedness of \(V_{\mathcal{F}}\) will depend on certain notion of coverage of \(\pi_{b}\) over \(\pi_{e}\). As another important property, the FDVF \(V_{\mathcal{F}}\) is generally not unique even if we fix \(\pi_{e}\) and \(\pi_{b}\), as Eq.(1) is generally an underdetermined linear system (\(S\ll|\mathcal{F}_{h}|\)) and can yield many solutions. As we see below, it suffices to model _any_ one of the solutions. Thus, from now on, when we talk about the boundedness of \(V_{\mathcal{F}}\), we always consider the \(V_{\mathcal{F}}\) with the smallest range among all solutions.

Finite Sample Learning.Like in MDPs, to learn an approximate FDVF from data, we will minimize some form of estimated Bellman residuals (or errors). For that we need to first introduce the Bellman residual operators for FDVFs:

**Definition 4** (Bellman residual operators).: \(\forall V:\mathcal{F}\rightarrow\mathbb{R}\), the Bellman residual on state \(s_{h}\) is:2

Footnote 2: We let \(V(f_{H+1})\equiv 0\) for all \(V\) to be considered, so that we do not need to handle the \(H\)-th step separately.

\[(\mathcal{B}^{\mathcal{H}}V)(\tau_{h}):=\mathbb{E}_{\begin{subarray}{c}a_{h }\sim\pi_{e}\\ a_{h+1:H}\sim\pi_{b}\end{subarray}}[r_{h}+V(f_{h+1})\mid\tau_{h}]-\mathbb{E}_ {\pi_{b}}[V(f_{h})\mid\tau_{h}]=\langle\mathbf{b}(\tau_{h}),\mathcal{B}_{h}^ {\mathcal{S}}V\rangle.\] (2)

The following lemma shows that, ideally, we would want to find \(V\) with small \(\mathcal{B}^{\mathcal{S}}V\):

**Lemma 1**.: _For any \(\pi_{e}\), \(\pi_{b}\), and \(V:\mathcal{F}\rightarrow\mathbb{R}\), \(J(\pi_{e})-\mathbb{E}_{\pi_{b}}[V(f_{1})]=\sum_{h=1}^{H}\mathbb{E}_{\pi_{e}} \left[(\mathcal{B}^{\mathcal{S}}V)(s_{h})\right].\)_

See proof in Appendix C.1. As the lemma shows, any \(V\) with small \(\mathcal{B}^{\mathcal{S}}V\) (such as \(V_{\mathcal{F}}\), since \(\mathcal{B}^{\mathcal{S}}V_{\mathcal{F}}\equiv 0\)) can be used to estimate \(J(\pi_{e})\) via \(\mathbb{E}_{\pi_{b}}[V(f_{1})]\). But again, \(\mathcal{B}^{\mathcal{S}}\) operates on \(\mathcal{S}\) which is unobserved, and we turn to its proxy \(\mathcal{B}^{\mathcal{H}}\), which are linear measures of \(\mathcal{B}^{\mathcal{S}}\) (Eq.2). More concretely, Uehara et al. (2022a) proposed to estimate \(\mathbb{E}_{\pi_{b}}[(\mathcal{B}^{\mathcal{H}}V)^{2}]\) using an additional helper class \(\Xi:\mathcal{H}\rightarrow\mathbb{R}\) to handle the double-sampling issue [11, 12]:

\[\widehat{V}=\operatorname*{argmin}_{V\in\mathcal{V}}\max_{\xi\in\Xi}\sum_{h=1}^ {H}\mathcal{L}_{h}(V,\xi),\] (3)

where \(\mathcal{L}_{h}(V,\xi)=\mathbb{E}_{\mathcal{D}}[\{\mu(a_{h},o_{h})(r_{h}+V(f_{ h+1}))-V(f_{h})\}\xi(\tau_{h})-0.5\xi^{2}(\tau_{h})]\). Under the following assumptions, the estimator enjoys a finite-sample guarantee:

**Assumption 5** (Realizability).: Let \(\mathcal{V}\subset(\mathcal{F}\rightarrow\mathbb{R})\) be a finite function class. Assume \(V_{\mathcal{F}}\in\mathcal{V}\) for some \(V_{\mathcal{F}}\) satisfying Definition 3.

**Assumption 6** (Bellman completeness).: Let \(\Xi\subset(\mathcal{H}\rightarrow\mathbb{R})\) be a finite function class. Assume \(\mathcal{B}^{\mathcal{H}}V\in\Xi,\;\forall V\in\mathcal{V}\).

**Theorem 2**.: _Under Assumptions 5 and 6, w.p. \(\geq 1-\delta\),_

\[|J(\pi_{e})-\mathbb{E}_{\mathcal{D}}[\widehat{V}(f_{1})]\leq cH\max\{C_{ \mathcal{V}}+1,C_{\Xi}\}\cdot\mathrm{IV}(\mathcal{V})\mathrm{Dr}_{\mathcal{V}}[ d^{\pi_{e}},d^{\pi_{b}}]\sqrt{\frac{C_{\mu}\log\frac{|\mathcal{V}||\Xi|}{ \delta}}{n}},\]

_where \(c\) is an absolute constant,3 and \(C_{\mathcal{V}}:=\max_{V\in\mathcal{V}}\|V\|_{\infty}\), \(C_{\Xi}:=\max_{\xi\in\Xi}\|\xi\|_{\infty}\),_

Footnote 3: The value of \(c\) can differ in each occurrence, and we reserve the symbol \(c\) for such absolute constants.

\[\mathrm{IV}(\mathcal{V}):=\max_{h}\sup_{V\in\mathcal{V}}\sqrt{\frac{\mathbb{E} _{\pi_{b}}\left[(\mathcal{B}^{\mathcal{S}}V)\left(s_{h}\right)^{2}\right]}{ \mathbb{E}_{\pi_{b}}\left[(\mathcal{B}^{\mathcal{H}}V)\left(\tau_{h}\right)^{2} \right]}},\quad\mathrm{Dr}_{\mathcal{V}}[d^{\pi_{e}},d^{\pi_{b}}]:=\max_{h} \sup_{V\in\mathcal{V}}\sqrt{\frac{\mathbb{E}_{\pi_{e}}\left[(\mathcal{B}^{ \mathcal{S}}V)\left(s_{h}\right)^{2}\right]}{\mathbb{E}_{\pi_{b}}\left[( \mathcal{B}^{\mathcal{S}}V)\left(s_{h}\right)^{2}\right]}}.\]

[MISSING_PAGE_FAIL:5]

In this example, \(C_{\text{stoch}}\) measures how the distribution of \(f_{h}\) under \(\pi_{b}\) deviates multiplicatively from a uniform distribution over \(\mathcal{F}_{h}\), and an even moderately stochastic \(\pi_{b}\) and emission process \(\mathbb{O}\) will lead to small \(C_{\text{stoch}}\), which implies an exponentially large \(1/\sigma_{\min}(M_{\mathcal{F},h})\).

### Minimum Weighted 2-Norm Solution and \(L_{2}\) Outcome Coverage

Pseudo-inverse finds the minimum \(L_{2}\) norm solution. However, given that we are searching for solutions in \(\mathbb{R}^{\mathcal{F}_{h}}\) which has an exponential dimensionality, the standard \(L_{2}\) norm--which treats all coordinates equally--is not a particularly informative metric. Instead, we propose to minimize the _weighted_\(L_{2}\) norm with a particular weighting scheme, which has also been used in HMMs [Mahajan et al., 2023] and enjoys benign properties.

We first define the diagonal weight matrix \(Z_{h}:=\operatorname{diag}(\mathbf{1}_{\mathcal{S}}^{\top}M_{\mathcal{F},h})\), where \(\mathbf{1}_{\mathcal{S}}\in\mathbb{R}^{\mathcal{S}}=[1,\cdots,1]^{\top}\) is the all-one vector. Then, the solution that minimizes \(\|\cdot\|_{Z_{h}}\) is:

\[V_{\mathcal{F},h}=Z_{h}^{-1}M_{\mathcal{F},h}^{\top}\Sigma_{\mathcal{F},h}^{- 1}V_{\mathcal{S},h}^{\pi_{b}},\;\;\text{ where }\Sigma_{\mathcal{F},h}:=M_{\mathcal{F},h}Z_{h}^{-1}M_{ \mathcal{F},h}^{\top}.\] (4)

\(\Sigma_{\mathcal{F},h}\in\mathbb{R}^{S\times S}\) plays an important role in this construction. Recall that its counterpart in the pseudo-inverse solution, namely \(M_{\mathcal{F},h}M_{\mathcal{F},h}^{\top}\), has scaling issues (Example 1), that even its _largest_ eigenvalue can decay exponentially with \(H-h+1\). In contrast, \(\Sigma_{\mathcal{F},h}\) is very well-behaved in its magnitude, as shown below. Furthermore, while \(M_{\mathcal{F},h}^{\top}\) on the left is now multiplied by \(Z_{h}^{-1}\) which can be exponentially large, \(Z_{h}^{-1}M_{\mathcal{F},h}^{\top}\) together is still well-behaved; see proof in Appendix D.2.

**Proposition 3** (Properties of Eq.(4)).: _I. \(\Sigma_{\mathcal{F},h}\) is doubly-stochastic: that is, each row/column of \(\Sigma_{\mathcal{F},h}\) is non-negative and sums up to \(1\). As a consequence, \(\sigma_{\max}(\Sigma_{\mathcal{F},h})=1\)._

_2. Rows of \(Z_{h}^{-1}M_{\mathcal{F},h}^{\top}\), i.e., \(\{\mathbf{u}(f_{h})^{\top}/Z(f_{h}):f_{h}\in\mathcal{F}_{h}\}\), are stochastic vectors, i.e., they are non-negative and the row sum is \(1\)._

Therefore, it is promising to make the assumption that \(\sigma_{\min}(\Sigma_{\mathcal{F},h})\) is bounded away from zero, which immediately leads to the boundedness of \(V_{\mathcal{F}}\) given that of \(Z_{h}^{-1}M_{\mathcal{F},h}^{\top}\) (\([0,1]\)) and \(V_{\mathcal{S}}^{\pi_{c}}\) (\([0,H]\)). However, we need to rule out the possibility that \(\Sigma_{\mathcal{F},h}\) is always near-singular, and find natural examples that admit large \(\sigma_{\min}(\Sigma_{\mathcal{F},h})\), as given below.

**Example 2**.: _Suppose \(f_{h}\) always reveals \(s_{h}\), in the sense that for any \(j\in\mathcal{F}_{h}\), \(\Pr_{\pi_{b}}(f_{h}=j\mid s_{h}=i)\) is only non-zero for a single \(i\in[S]\), and zero for all other latent states. Then, \(\Sigma_{\mathcal{F},h}=\mathbf{I}\), the identity matrix. Furthermore, \(V_{\mathcal{F}}\) from Eq.(4) satisfies \(\|V_{\mathcal{F}}\|_{\infty}\leq H\). See Appendix D.3 for details._

The example shows an ideal case where \(\sigma_{\min}(\Sigma_{\mathcal{F},h})=\sigma_{\max}(\Sigma_{\mathcal{F},h})=1\), when the future fully determines \(s_{h}\). This can happen when the last observation \(o_{H}\) reveals the identity of an earlier latent state \(s_{h}\). Note that in this case, \(M_{\mathcal{F},h}M_{\mathcal{F},h}^{\top}\) can still have poor scaling if the actions and observations between step \(h\) and \(H\) are sufficiently stochastic, which shows how the weighted 2-norm solution and analysis improve over the pseudo-inverse one. More generally, \(\Sigma_{\mathcal{F},h}\) is the confusion matrix of making posterior predictions of \(s_{h}\) from \(f_{h}\) based on a uniform prior over \(\mathcal{S}_{h}\) (see Appendix B.8 for how to incorporate different priors) with \(\mathbf{u}(f_{h})^{\top}/Z(f_{h})\) being the posterior, and \(\sigma_{\min}(\Sigma_{\mathcal{F},h})\) serves as a measure of how the distribution of future \(f_{h}\) helps reveal the latent state \(s_{h}\).

We now break down the boundedness of Eq.(4) into more interpretable assumptions.

**Assumption 7** (\(L_{2}\) outcome coverage).: Assume for all \(h\), \(\|V_{\mathcal{S},h}^{\pi_{\pi}}\|_{\Sigma_{\mathcal{F},h}^{-1}}^{2}\leq C_{ \mathcal{F},V}\).

**Assumption 8** (\(\Sigma_{\mathcal{F},h}\) regularity).: Assume for any \(f_{h}\): \(\|\mathbf{u}(f_{h})/Z(f_{h})\|_{\Sigma_{\mathcal{F},h}^{-1}}^{2}\leq C_{ \mathcal{F},U}\).

**Proposition 4** (Boundedness of FDVF).: _Under Assumptions 7 and 8, \(V_{\mathcal{F}}\) in Eq.4 satisfies \(\|V_{\mathcal{F}}\|_{\infty}\leq\sqrt{C_{\mathcal{F},2}}:=\sqrt{C_{\mathcal{F},V }C_{\mathcal{F},U}}\). Furthermore, when only Assumption 7 holds, \(\|V_{\mathcal{F},h}\|_{Z_{h}}\leq\sqrt{C_{\mathcal{F},V}}\), \(\forall h\)._

See proof in Appendix D.4. Assumption 7 requires that the weighted covariance matrix \(\Sigma_{\mathcal{F},h}\) covers the direction of \(V_{\mathcal{S},h}^{\pi_{c}}\) well. As a sanity check, it is always bounded in the on-policy case:

**Example 3**.: _When \(\pi_{b}=\pi_{e}\), \(\|V_{\mathcal{S},h}^{\pi_{c}}\|_{\Sigma_{\mathcal{F},h}^{-1}}\leq H\sqrt{S}\)._

Notably, mathematically similar coverage assumptions are also found in the linear MDP literature. For example, with state-action feature \(\phi_{h}\) for time step \(h\), a very tight coverage parameter for linear MDPs is \(\|\mathbb{E}_{\pi_{e}}[\phi_{h}]\|^{2}_{\mathbb{E}_{\pi_{b}}[\phi_{h},\phi^{ \top}_{\mathcal{S}}]^{-1}}\)(Zanette et al., 2021). Despite the mathematically similarity, there are important high-level differences between these notions of coverage:

1. As mentioned earlier, MDP coverage is concerned with the dynamics **before** step \(h\), whereas our outcome coverage concerns that **after**\(h\). Relatedly, MDP coverage depends on the initial distribution (which our outcome coverage does not depend on), and our coverage depends on the reward function through \(V_{\mathcal{F}}\) (which MDP coverage does not explicitly depend on). In Section 5, we will discuss our other coverage assumption (belief coverage), which is more similar to the MDP coverage in that they are both concerned with the past.
2. The linear MDP coverage assumption is a refinement of state-density ratio using the knowledge of the function class (Chen and Jiang, 2019; Song et al., 2022). In comparison, the linear structure of our outcome-coverage assumption comes directly from the internal structure of POMDPs.

### Addressing \(S\) dependence via \(L_{1}/l_{\infty}\) Holder and \(L_{\infty}\) Outcome Coverage

Example 3 shows that even in the on-policy case, \(C_{\mathcal{F},V}\) may depend on \(S\) which makes the assumption only meaningful for finite and small \(\mathcal{S}\). In fact, we showed earlier that \(V_{\mathcal{F}}=R^{+}\) is a natural and obvious \(L_{\infty}\)-bounded solution for \(\pi_{b}=\pi_{e}\), but this is not recovered by the construction in Eq. (4). We also need an additional regularity Assumption 8.

As it turns out, these undesired properties arise because \(L_{2}\) Holder--which is natural for linear MDP settings mentioned above--fails to leverage the \(L_{1}\) normalization of \(\mathbf{u}(f_{h})^{\top}/Z(f_{h})\) (Proposition 4, Claim 2) and is loose for POMDPs; see Appendix B.5 for further details. A better choice is \(L_{1}/L_{\infty}\) Holder, motivating the \(L_{\infty}\) coverage assumption below, which requires a slightly different construction of \(V_{\mathcal{F}}\). These definitions may seem mysterious or even counterintuitive; it will be easier to explain the intuitions when we get to their counterparts for belief coverage in Section 5.2.

Construction of \(V_{\mathcal{F}}\)Define \(Z^{R}(f_{h}):=Z(f_{h})/R^{+}(f_{h})\), and we use \(Z^{R}\) to replace \(Z\) in Eq.(4):

\[V_{\mathcal{F},h}=(Z^{R}_{h})^{-1}M^{\top}_{\mathcal{F},h}(\Sigma^{R}_{ \mathcal{F},h})^{-1}V^{\pi_{e}}_{\mathcal{S},h},\ \ \text{where}\ \Sigma^{R}_{\mathcal{F},h}:=M_{\mathcal{F},h}(Z^{R}_{h})^{-1}M^{\top}_{ \mathcal{F},h}.\] (5)

**Assumption 9** (\(L_{\infty}\) outcome coverage).: Assume for all \(h\), \(\|(\Sigma^{R}_{\mathcal{F},h})^{-1}V^{\pi_{e}}_{\mathcal{S}}\|_{\infty}\leq C_ {\mathcal{F},\infty}\).

**Lemma 5**.: _Under Assumption 9, \(\|V_{\mathcal{F}}\|_{\infty}\leq HC_{\mathcal{F},\infty}\). See proof in Appendix E.7._

In Appendix B.5 we show that the construction shares similar properties to Eq.(4) in the scenario of Example 2. On the other hand, it has better scaling properties w.r.t. \(S\) and does not additionally require a regularity assumption like Assumption 8. In the on-policy case, Eq.(5) _exactly_ recovers \(V_{\mathcal{F}}=R^{+}\), a property that Eq.(4) does not enjoy; see Appendix E.8 for details.

**Example 4**.: _When \(\pi_{e}=\pi_{b}\), \((\Sigma^{R}_{\mathcal{F},h})^{-1}V^{\pi_{e}}_{\mathcal{S}}=\mathbf{1}\), thus Assumption 9 holds with \(C_{\mathcal{F},\infty}=1\) (c.f. \(C_{\mathcal{F},V}\leq H\sqrt{S}\) in Example 3). Furthermore, the construction in Eq.(5) is exactly \(V_{\mathcal{F}}=R^{+}\)._

## 5 Effective History Weights and A New Algorithm

We now turn to **Q2** in Section 3, which asks for a quantitative understanding of the IV\((\mathcal{V})\) term. Note that IV\((\mathcal{V})\) and \(\text{Dr}_{\mathcal{V}}[d^{\pi_{e}},d^{\pi_{b}}]\), taken together, are to address the conversion between:

\[\text{(We minimize:)}\ \sqrt{\mathbb{E}_{\pi_{b}}[(\mathcal{B}^{ \mathcal{H}}V)(\tau_{h})^{2}]}\ \ \rightarrow\ \ \text{(We want to bound:)}\ |\mathbb{E}_{\pi_{e}}[(\mathcal{B}^{\mathcal{S}}V)(s_{h})]|.\] (6)

The two terms differ both in the policy (\(\pi_{b}\rightarrow\pi_{e}\)) and the operator (\(\mathcal{B}^{\mathcal{H}}\rightarrow\mathcal{B}^{\mathcal{S}}\)). While we could directly define a parameter by taking the worst-case (over \(V\in\mathcal{V}\)) ratio between the two expressions,5 the real question is to provide more intuitive understanding of when it can be bounded.

Footnote 5: Recent offline RL theory indeed employed such definitions (Xie et al., 2021; Song et al., 2022), but that is after we developed mature understanding of their properties, such as being upper bounded by density ratios.

Towards this goal, Uehara et al. (2022) split the above ratio into two terms, IV\((\mathcal{V})\) and \(\text{Dr}_{\mathcal{V}}[d^{\pi_{e}},d^{\pi_{b}}]\), which take care of the \(\mathcal{B}^{\mathcal{H}}\rightarrow\mathcal{B}^{\mathcal{S}}\) conversion (under \(\pi_{b}\)) and \(\pi_{b}\rightarrow\pi_{e}\) conversion (under \(\mathcal{B}^{\mathcal{S}}\)), respectively. While this leads to an intuitive upper bound of the \(\pi_{b}\rightarrow\pi_{e}\) conversion parameter in terms of latent state coverage, the nature of the \(\mathcal{B}^{\mathcal{H}}\rightarrow\mathcal{B}^{\mathcal{S}}\) conversion, IV\((\mathcal{V})\), remains mysterious.

In this section, we take a different approach by _directly_ providing an intuitive upper bound on both conversions altogether, under a novel _belief coverage_ assumption. In Appendix E.4 we will also revisit the split into \(\mathrm{IV}(\mathcal{V})\) and \(\mathrm{Dr}_{\mathcal{V}}[d^{\pi_{e}},d^{\pi_{b}}]\): in the absence of strong structures from \(\mathcal{V}\), the boundedness of \(\mathrm{IV}(\mathcal{V})\) turns out to require an even stronger version of belief coverage, rendering the split unnecessary. Furthermore, our approach also leads to a novel algorithm for estimating \(J(\pi_{e})\) that replaces Bellman-completeness (Assumption 6) with a weight-realizability assumption, similar to MIS estimators for MDPs (Liu et al., 2018; Uehara et al., 2020).

### Effective History Weights

The key idea in this section is the notion of _effective history weights_, that perform the \(\pi_{b}\to\pi_{e}\) and \(\mathcal{B}^{\mathcal{H}}\to\mathcal{B}^{\mathcal{S}}\) conversions jointly.

**Definition 10** (Effective history weights).: An effective history weight function \(w^{\star}:\mathcal{H}\to\mathbb{R}\) is any function that satisfies: \(\forall V\in\mathcal{V}\), \(h\in[H]\),

\[\mathbb{E}_{\pi_{b}}[w^{\star}(\tau_{h})(\mathcal{B}^{\mathcal{H}}V)(\tau_{h} )]=\mathbb{E}_{\pi_{e}}\left[(\mathcal{B}^{\mathcal{S}}V)(s_{h})\right].\] (7)

We first see that well-bounded \(w^{\star}\) immediately leads to a good conversion ratio for Eq.(6):

\[\left|\mathbb{E}_{\pi_{e}}\left[(\mathcal{B}^{\mathcal{S}}V)(s_{h})\right] \right|=\left|\mathbb{E}_{\pi_{b}}[w^{\star}(\tau_{h})(\mathcal{B}^{\mathcal{ H}}V)(\tau_{h})]\right|\leq\sqrt{\mathbb{E}_{\pi_{b}}[w^{\star}(\tau_{h})^{2}] \mathbb{E}_{\pi_{b}}[(\mathcal{B}^{\mathcal{H}}V)(\tau)^{2}]},\]

where the inequality follows from Cauchy-Schwartz for r.v.'s. Hence, all we need is \(\left\|w^{\star}\right\|_{2,d^{\pi_{b}}_{h}}:=\sqrt{\mathbb{E}_{\pi_{b}}[w^{ \star}(\tau_{h})^{2}]}\), the \(\pi_{b}\)-weighted 2-norm of \(w^{\star}\), to be bounded, and we focus on this quantity next. Similar to Section 4, there may be multiple \(w^{\star}\) that satisfies the definition, and we only need to show the boundeness of _any_ solution. Also similarly, the most obvious solution is \(w^{\star}(\tau_{h})=\frac{\mathrm{Pr}_{\pi_{b}}(\tau_{h})}{\mathrm{Pr}_{\pi_{b }}(\tau_{h})}=\prod_{h^{\prime}=1}^{h-1}\frac{\pi_{e}(a_{h^{\prime}}|a_{h^{ \prime}})}{\pi_{b}(a_{h^{\prime}}|a_{h^{\prime}})}\), noting that \(\mathbb{E}_{\pi_{e}}\left[(\mathcal{B}^{\mathcal{S}}V)(s_{h})\right]=\mathbb{E }_{\pi_{e}}\left[(\mathcal{B}^{\mathcal{H}}V)(\tau_{h})\right]\) and importance weighting on \(\tau_{h}\) changes \(\mathbb{E}_{\pi_{b}}\) to \(\mathbb{E}_{\pi_{e}}\). However, the use of cumulative importance weights is undesirable given its exponential nature, causing the history-version of "curse of horizon" Liu et al. (2018).

Construction by Belief Matching.We now show a better construction that is bounded under a natural _belief coverage_ assumption. Note that Def 10 can be written as:

\[\mathbb{E}_{\pi_{b}}[w^{\star}(\tau_{h})\langle\mathbf{b}(\tau_{h}),\mathcal{ B}^{\mathcal{S}}_{h}V\rangle]=\mathbb{E}_{\pi_{e}}\left[\langle\mathbf{b}( \tau_{h}),\mathcal{B}^{\mathcal{S}}_{h}V\rangle\right],\]

where \(\mathcal{B}^{\mathcal{S}}_{h}V\in\mathbb{R}^{S}\) is the Bellman residual vector for \(V\) on \(\mathcal{S}_{h}\). As a sufficient condition (which is also necessary when \(\mathcal{V}\) lacks strong structures, i.e., \(\{\mathcal{B}^{\mathcal{S}}_{h}V:V\in\mathcal{V}\}\) spans the entire \(\mathbb{R}^{S}\)), we can find \(w^{\star}\) that satisfies: \(\mathbb{E}_{\pi_{b}}[w^{\star}(\tau_{h})\mathbf{b}(\tau_{h})]=\mathbb{E}_{\pi_ {e}}\left[\mathbf{b}(\tau_{h})\right]=:\mathbf{b}^{\pi_{e}}_{h}\). This is related to the mean matching problem in the distribution shift literature (Gretton et al., 2009; Yu and Szepesvari, 2012), and a standard solution is (Bruns-Smith et al., 2023):

\[w^{\star}(\tau_{h})=\mathbf{b}(\tau_{h})^{\top}\Sigma_{\mathcal{H},h}^{-1} \mathbf{b}^{\pi_{e}}_{h},\] (8)

where \(\Sigma_{\mathcal{H},h}:=\sum_{\tau_{h}}d^{\pi_{b}}(\tau_{h})\mathbf{b}(\tau_{h })\mathbf{b}(\tau_{h})^{\top}\), and \(\mathbf{b}^{\pi_{e}}_{h}\) coincides with \([d^{\pi_{e}}(s_{h})]_{s_{h}}\).

The weighted 2-norm of this solution is immediately bounded under the following assumption.

**Assumption 11** (\(L_{2}\) belief coverage).: Assume \(\forall h\), \(\left\|\mathbf{b}^{\pi_{e}}_{h}\right\|_{\Sigma_{\mathcal{H},h}^{-1}}^{2}\leq C _{\mathcal{H},2}\).

**Lemma 6**.: _Under Assumption 11, \(w^{\star}\) in Eq. (8) satisfies \(\forall h\), \(\left\|w^{\star}\right\|_{2,d^{\pi_{b}}_{h}}^{2}\leq C_{\mathcal{H},2}\). See Appendix E.1._

Assumption 11 requires that the covariance matrix of belief states under \(\pi_{b}\) covers \(\mathbf{b}^{\pi_{e}}\), the average belief state under \(\pi_{e}\). As before, we also check the on-policy case (see Appendix E.2):

**Example 5**.: _In the on-policy case (\(\pi_{b}=\pi_{e}\)), \(C_{\mathcal{H},2}\leq 1\)._

Algorithm GuaranteeNow we have all the pieces to present a fully polynomial version of Theorem 2 under the proposed coverage assumptions. One subtlety is that the guarantee depends on \(C_{\mathcal{V}}:=\max_{V\in\mathcal{V}}\|V\|_{\infty}\), which is closely related to \(\|V_{\mathcal{F}}\|_{\infty}\) since we require \(V_{\mathcal{F}}\in\mathcal{V}\), but they are not equal since \(\mathcal{V}\) can include other functions with higher range. To highlight the dependence of \(\|V_{\mathcal{F}}\|_{\infty}\) on the proposed coverage assumptions, we follow Xie and Jiang (2020) to assume that the range of the function classes is not much larger than that of the function it needs to capture. A similar assumption applies for bounding \(C_{\mathbb{E}}\). The proof of the theorem is deferred to Appendix E.10.

**Theorem 7**.: _Consider the same setting as Theorem 2, and let Assumptions 11 and 9 hold. For some absolute constant \(c\), further assume \(C_{V}\leq c\|V_{\mathcal{F}}\|_{\infty}\) for \(V_{\mathcal{F}}\) in Eq.(5) and \(C_{\Xi}\leq c(\|V_{\mathcal{F}}\|_{\infty}+1)\). W.p. \(\geq 1-\delta\), \(|J(\pi_{e})-\mathbb{E}_{\mathcal{D}}[\widehat{V}(f_{1})]|\leq cH^{2}(C_{ \mathcal{F},\infty}+1)\sqrt{\frac{C_{\mathcal{H},2}C_{\mu}\log(|V||\Xi|/\delta )}{n}}\)._

In addition to \(H\) and complexities of \(|\mathcal{V}|\) and \(|\Xi|\), the bound _only_ depends on the intuitive coverage parameters: \(C_{\mu}\) (action coverage), \(C_{\mathcal{H},2}\) (\(L_{2}\) belief coverage), and \(C_{\mathcal{F},\infty}\) (\(L_{\infty}\) outcome coverage).

### \(L_{\infty}\) Belief Coverage

Assumption 11 enables bounded second moment of \(w^{\star}\) (Lemma 6) but does not control \(\|w^{\star}\|_{\infty}\), which we show will be useful for the new algorithm in Section 5.3. Here we present the \(L_{\infty}\) version of belief coverage that controls \(\|w^{\star}\|_{\infty}\), which also helps understand \(L_{\infty}\) outcome coverage given the symmetry between history and future. As alluded to in Section 4.2 and Appendix B.5, \(L_{2}\) Holder is inappropriate for controlling the infinity-norm since it does not leverage the \(L_{1}\) normalization of vectors in POMDPs. Instead, we propose the following decomposition based on \(L_{1}/L_{\infty}\) Holder: \(|w^{\star}(\tau_{h})|\leq\|\mathbf{b}(\tau_{h})\|_{1}\|\Sigma_{\mathcal{H},h}^ {-1}\mathbf{b}_{h}^{\pi_{e}}\|_{\infty}=\|\Sigma_{\mathcal{H},h}^{-1}\mathbf{ b}_{h}^{\pi_{e}}\|_{\infty}\). This way, we can immediately bound \(\|w^{\star}\|_{\infty}\) with the following assumption:

**Assumption 12** (\(L_{\infty}\) belief coverage).: Assume \(\forall h\), \(\|\Sigma_{\mathcal{H},h}^{-1}\mathbf{b}_{h}^{\pi_{e}}\|_{\infty}\leq C_{ \mathcal{H},\infty}\). Then \(\|w^{\star}\|_{\infty}\leq C_{\mathcal{H},\infty}\). \(\Sigma_{\mathcal{H},h}^{-1}\mathbf{b}_{h}^{\pi_{e}}\) is the inverse of _second_ moment (covariance) multiplying the _first_ moment (expectation), raising the concern that the quantity may be poorly scaled: for example, given bounded (but otherwise arbitrary) random vector \(X\), \(\mathbb{E}[XX^{\top}]^{-1}\mathbb{E}[X]\) can go to infinity if we rescale \(X\) by a small constant. First note that such a pathology cannot happen here because the random vectors (\(\mathbf{b}(\tau_{h})\)) are \(L_{1}\)-normalized and cannot be arbitrarily rescaled. Below we use a few examples to show that \(\Sigma_{\mathcal{H},h}^{-1}\mathbf{b}_{h}^{\pi_{e}}\) is a very well-behaved quantity, and naturally generalize familiar concepts such as _concentrability coefficient_ from MDPs (Munos, 2007; Chen and Jiang, 2019; Uehara et al., 2022).

We start by checking the on-policy case. Perhaps surprisingly, \(\Sigma_{\mathcal{H},h}^{-1}\mathbf{b}_{h}^{\pi_{e}}\) has an _exact_ solution:

**Example 6**.: _When \(\pi_{e}=\pi_{b}\), \(\Sigma_{\mathcal{H},h}^{-1}\mathbf{b}_{h}^{\pi_{e}}=\mathbf{1}\), the all-one vector; see Appendix E.5 for the calculation. Consequently, Assumption 12 is satisfied with \(C_{\mathcal{H},\infty}=1\)._

The next scenario considers when \(\mathbf{b}(\tau_{h})\) is always one-hot, i.e., histories reveal the latent state (this is analogous to Example 2. \(L_{\infty}\) coverage reduces to the familiar _concentrability coefficient_, the infinity-norm of density ratio as a standard coverage parameter:

**Example 7**.: _When \(\mathbf{b}(\tau_{h})\) is always one-hot, \(\|\Sigma_{\mathcal{H},h}^{-1}\mathbf{b}_{h}^{\pi_{e}}\|_{\infty}=\max_{s_{h}} d^{\pi_{e}}(s_{h})/d^{\pi_{h}}(s_{h})\)._

We can also calculate \(\|\mathbf{b}_{h}^{\pi_{e}}\|_{\Sigma_{\mathcal{H},h}^{-1}}^{2}\) from Assumption 11, which equals \(\mathbb{E}_{\pi_{b}}[(d^{\pi_{e}}(s_{h})/d^{\pi_{h}}(s_{h}))^{2}]\) in this "1-hot belief" scenario. Xie and Jiang (2020) show that this is tighter than \(\max_{s_{h}}d^{\pi_{e}}(s_{h})/d^{\pi_{h}}(s_{h})\), and this relation extends elegantly to general belief vectors in our setting:

**Lemma 8**.: \(\|\mathbf{b}_{h}^{\pi_{e}}\|_{\Sigma_{\mathcal{H},h}^{-1}}^{2}\leq\|\Sigma_{ \mathcal{H},h}^{-1}\mathbf{b}_{h}^{\pi_{e}}\|_{\infty}\)_. See proof in Appendix E.6._

### New Algorithm

The discovery of effective history weights and its boundedness also lead to a new algorithm analogous to MIS methods for MDPs (Uehara et al., 2020). The idea is that since Lemma 1 tells us to minimize \(\mathbb{E}_{\pi_{e}}[(\mathcal{B}^{S}V)(s_{h})]\), which equals \(\mathbb{E}_{\pi_{b}}[w^{\star}(\tau_{h})(\mathcal{B}^{\mathcal{H}}V)(\tau_{h})]\) from Def 10, we can then use another function class \(\mathcal{W}\subset(\mathcal{H}\rightarrow\mathbb{R})\) to model \(w^{\star}\), and approximately solve the following:

\[\operatorname*{argmin}_{V\in\mathcal{V}}\max_{w\in\mathcal{W}}\sum_{h=1}^{H}| \mathbb{E}_{\pi_{b}}[w(\tau_{h})(\mathcal{B}^{\mathcal{H}}V)(\tau_{h})]|,\] (9)

which minimizes an upper bound of \(\mathbb{E}_{\pi_{b}}[w^{\star}(\tau_{h})(\mathcal{B}^{\mathcal{H}}V)(\tau_{h})]\) as long as \(w^{\star}\in\mathcal{W}\subset(\mathcal{H}\rightarrow\mathbb{R})\). Since there is no square inside the expectation, there is no double-sampling issue and we thus do not need the \(\Xi\) class and its Bellman-completeness assumption. The \(h\)-th term of the loss can be estimated straightforwardly as

\[|\mathbb{E}_{\mathcal{D}}\left[w(\tau_{h})\left(\mu(o_{h},a_{h})\left(r_{h}+V(f_{ h+1})\right)-V(f_{h})\right)\right]|.\] (10)We now provide the sample-complexity analysis of the algorithm, using a more general analysis that allows for approximation errors in \(\mathcal{V}\) and \(\mathcal{W}\).

**Assumption 13** (Approximate realizablity).: Assume

\[\min_{V\in\mathcal{V}}\max_{w\in\mathcal{W}}\left|\sum_{h=1}^{H} \mathbb{E}_{\pi_{b}}\left[w_{h}(\tau_{h})\cdot(\mathcal{B}^{\mathcal{H}}V)( \tau_{h})\right]\right|\leq\epsilon_{\mathcal{V}},\] \[\inf_{w\in\operatorname{sp}(\mathcal{W})}\max_{V\in\mathcal{V}} \left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[(w_{h}^{\star}(\tau_{h})-w_{h}( \tau_{h}))\cdot(\mathcal{B}^{\mathcal{H}}V)(\tau_{h})\right]\right|\qquad \leq\epsilon_{\mathcal{W}}.\]

Instead of measuring how \(\mathcal{W}\) and \(\mathcal{V}\) capture our specific constructions of \(V_{\mathcal{F}}\) and \(w^{\star}\), the above approximation errors automatically allow all possible solutions by measuring the violation of the equations that define \(V_{\mathcal{F}}\) and \(w^{\star}\).

We present the sample complexity bound of our algorithm as follows. Similar to Theorem 7, we assume that \(C_{\mathcal{V}}:=\max_{V\in\mathcal{V}}\|V\|_{\infty}\) and \(C_{\mathcal{W}}:=\max_{h}\sup_{w\in\mathcal{W}}\|w\|_{\infty}\) are not much larger than the corresponding norms of \(V_{\mathcal{F}}\) and \(w^{\star}\), respectively. See the proof in Appendix E.11.

**Theorem 9**.: _Let \(\widehat{V}\) be the result of approximating Eq.9 with empirical estimation in Eq.(10). Assume that \(C_{\mathcal{V}}\leq c\|V_{\mathcal{F}}\|_{\infty}\) for \(V_{\mathcal{F}}\) in Eq.(5), and \(C_{\mathcal{W}}\leq c\|w^{\star}\|_{\infty}\) for \(w^{\star}\) in Eq.(8). Under Assumptions 12, 9, and 13, w.p. \(\geq 1-\delta\), \(\left|J(\pi_{e})-\mathbb{E}_{\mathcal{D}}[\widehat{V}(f_{1})]\right|\leq \epsilon_{\mathcal{V}}+\epsilon_{\mathcal{W}}+cH^{2}C_{\mathcal{H},\infty}(C_ {\mathcal{F},\infty}+1)\sqrt{\frac{C_{\mathcal{D}}\log\frac{|V|\mathcal{W}|}{ \delta}}{n}}\)._

As a remark, there is also a way to leverage the tighter \(L_{2}\) belief coverage, despite that it does not guarantee bounded \(\|w^{\star}\|_{\infty}\) and only \(\|w^{\star}\|_{2,d_{h}^{\tau_{b}}}^{2}\). In particular, if all functions in \(\mathcal{W}\) have bounded \(\|\cdot\|_{2,d_{h}^{\tau_{b}}}^{2}\), the estimator in Eq.(10) will have bounded 2nd moment on the data distribution. In this case, using Median-of-Means estimators (Lerasle, 2019; Chen, 2020) instead of plain averages for Eq.(10) will only pay for the 2nd moment and not the range.

## 6 Conclusion and Future Work

The main text considers memoryless policies. Similar to Uehara et al. (2022), we can extend to policies that depend on recent observations and actions (or _memory_). In fact, we provide a more general result in Appendix B.6 that handles recurrent policies that are _finite state machines_, which allows the policy to depend on long histories. However, the coverage coefficient will be diluted quickly when the memory contains rich information, which we call the _curse of memory_. We suspect that structural policies are needed to avoid the curse of memory and leave this to future work.

## Acknowledgements

Nan Jiang acknowledges funding support from NSF IIS-2112471, NSF CAREER IIS-2141781, Google Scholar Award, and Sloan Fellowship.

## References

* Antos et al. (2008) Andras Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path. _Machine Learning_, 2008.
* Azizzadenesheli et al. (2016) Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning of pomdps using spectral methods. In _Conference on Learning Theory_, pages 193-256. PMLR, 2016.
* Bruns-Smith et al. (2023) David Bruns-Smith, Oliver Dukes, Avi Feller, and Elizabeth L Ogburn. Augmented balancing weights as linear regression. _arXiv preprint arXiv:2304.14545_, 2023.
* Chen et al. (2022) Fan Chen, Yu Bai, and Song Mei. Partially observable rl with b-stability: Unified structural condition and sharp sample-efficient algorithms. _arXiv preprint arXiv:2209.14990_, 2022.
* Chen et al. (2020)* Chen and Jiang (2019) Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In _Proceedings of the 36th International Conference on Machine Learning_, pages 1042-1051, 2019.
* Chen (2020) Yen-Chi Chen. A short note on the median-of-means estimator, 2020.
* Dai et al. (2018) Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Speed: Convergent reinforcement learning with nonlinear function approximation. In _International Conference on Machine Learning_, pages 1133-1142, 2018.
* Du et al. (2019) Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Provably efficient RL with Rich Observations via Latent State Decoding. In _International Conference on Machine Learning_, pages 1665-1674, 2019.
* Duan et al. (2020) Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function approximation. In _International Conference on Machine Learning_, pages 2701-2709. PMLR, 2020.
* Dudik et al. (2011) Miroslav Dudik, John Langford, and Lihong Li. Doubly Robust Policy Evaluation and Learning. In _Proceedings of the 28th International Conference on Machine Learning_, pages 1097-1104, 2011.
* Efroni et al. (2022) Yonathan Efroni, Chi Jin, Akshay Krishnamurthy, and Sobhan Miryoosefi. Provable reinforcement learning with a short-term memory. In _International Conference on Machine Learning_, pages 5832-5850. PMLR, 2022.
* Ernst et al. (2005) Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. _Journal of Machine Learning Research_, 6:503-556, 2005.
* Eysenbach et al. (2020) Benjamin Eysenbach, Swapnil Asawa, Shreyas Chaudhari, Sergey Levine, and Ruslan Salakhutdinov. Off-dynamics reinforcement learning: Training for transfer with domain classifiers. _arXiv preprint arXiv:2006.13916_, 2020.
* Farajtabar et al. (2018) Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust off-policy evaluation. In _International Conference on Machine Learning_, pages 1447-1456. PMLR, 2018.
* Gretton et al. (2009) Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, Bernhard Scholkopf, et al. Covariate shift by kernel mean matching. _Dataset shift in machine learning_, 3(4):5, 2009.
* Guo et al. (2022) Hongyi Guo, Qi Cai, Yufeng Zhang, Zhuoran Yang, and Zhaoran Wang. Provably efficient offline reinforcement learning for partially observable markov decision processes. In _International Conference on Machine Learning_, pages 8016-8038. PMLR, 2022.
* Guo et al. (2016) Zhaohan Daniel Guo, Shayan Doroudi, and Emma Brunskill. A pac rl algorithm for episodic pomdps. In _Artificial Intelligence and Statistics_, pages 510-518. PMLR, 2016.
* Hong et al. (2023) Mao Hong, Zhengling Qi, and Yanxun Xu. A policy gradient method for confounded pomdps. _arXiv preprint arXiv:2305.17083_, 2023.
* Hu and Wager (2023) Yuchen Hu and Stefan Wager. Off-policy evaluation in partially observed markov decision processes under sequential ignorability. _The Annals of Statistics_, 51(4):1561-1585, 2023.
* Huang et al. (2023) Ruiquan Huang, Yingbin Liang, and Jing Yang. Provably efficient ucb-type algorithms for learning predictive state representations. _arXiv preprint arXiv:2307.00405_, 2023.
* Jiang and Li (2016) Nan Jiang and Lihong Li. Doubly Robust Off-policy Value Evaluation for Reinforcement Learning. In _Proceedings of the 33rd International Conference on Machine Learning_, volume 48, pages 652-661, 2016.
* Jiang et al. (2017) Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contextual Decision Processes with low Bellman rank are PAC-learnable. In _Proceedings of the 34th International Conference on Machine Learning_, volume 70, pages 1704-1713, 2017.
* Jiang et al. (2018)Chi Jin, Sham Kakade, Akshay Krishnamurthy, and Qinghua Liu. Sample-efficient reinforcement learning of undercomplete pomdps. _Advances in Neural Information Processing Systems_, 33:18530-18539, 2020.
* Kaelbling et al. (1998) Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. _Artificial intelligence_, 101(1-2):99-134, 1998.
* Kallus and Uehara (2020) Nathan Kallus and Masatoshi Uehara. Double reinforcement learning for efficient off-policy evaluation in markov decision processes. _Journal of Machine Learning Research_, 21(167):1-63, 2020.
* Krishnamurthy et al. (2016) Akshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich observations. In _Advances in Neural Information Processing Systems_, 2016.
* Kwon et al. (2021) Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor. Rl for latent mdps: Regret guarantees and a lower bound. _Advances in Neural Information Processing Systems_, 34:24523-24534, 2021.
* Lale et al. (2021) Sahin Lale, Kamyar Azizzadenesheli, Babak Hassibi, and Anima Anandkumar. Adaptive control and regret minimization in linear quadratic gaussian (lqg) setting. In _2021 American Control Conference (ACC)_, pages 2517-2522. IEEE, 2021.
* Le et al. (2019) Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In _International Conference on Machine Learning_, pages 3703-3712, 2019.
* Lerasle (2019) Matthieu Lerasle. Lecture notes: Selected topics on robust statistical learning theory. _arXiv preprint arXiv:1908.10761_, 2019.
* Li et al. (2011) Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation Algorithms. In _Proceedings of the 4th International Conference on Web Search and Data Mining_, pages 297-306, 2011.
* Liu et al. (2018) Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-horizon off-policy estimation. In _Advances in Neural Information Processing Systems_, pages 5356-5366, 2018.
* Liu et al. (2022a) Qinghua Liu, Alan Chung, Csaba Szepesvari, and Chi Jin. When is partially observable reinforcement learning not scary? In _Conference on Learning Theory_, pages 5175-5220. PMLR, 2022a.
* Liu et al. (2022b) Qinghua Liu, Praneeth Netrapalli, Csaba Szepesvari, and Chi Jin. Optimistic mle-a generic model-based algorithm for partially observable sequential decision making. _arXiv preprint arXiv:2209.14997_, 2022b.
* Lu et al. (2022) Miao Lu, Yifei Min, Zhaoran Wang, and Zhuoran Yang. Pessimism in the face of confounders: Provably efficient offline reinforcement learning in partially observable markov decision processes. _arXiv preprint arXiv:2205.13589_, 2022.
* Mahajan et al. (2023) Gaurav Mahajan, Sham Kakade, Akshay Krishnamurthy, and Cyril Zhang. Learning hidden markov models using conditional samples. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2014-2066. PMLR, 2023.
* Munos (2007) Remi Munos. Performance bounds in l_p-norm for approximate value iteration. _SIAM journal on control and optimization_, 46(2):541-561, 2007.
* Munos and Szepesvari (2008) Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. _Journal of Machine Learning Research_, 9(May):815-857, 2008.
* Nachum et al. (2019) Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. _Advances in Neural Information Processing Systems_, 32, 2019.
* Nair and Jiang (2021) Yash Nair and Nan Jiang. A spectral approach to off-policy evaluation for pomdps. _arXiv preprint arXiv:2109.10502_, 2021.
* Nachum et al. (2019)Hongseok Namkoong, Ramtin Keramati, Steve Yadlowsky, and Emma Brunskill. Off-policy policy evaluation for sequential decisions under unobserved confounding. _Advances in Neural Information Processing Systems_, 33:18819-18831, 2020.
* Perdomo et al. (2023) Juan C Perdomo, Akshay Krishnamurthy, Peter Bartlett, and Sham Kakade. A complete characterization of linear estimators for offline policy evaluation. _Journal of Machine Learning Research_, 24(284):1-50, 2023.
* Precup et al. (2000) Doina Precup, Richard S Sutton, and Satinder P Singh. Eligibility traces for off-policy policy evaluation. In _Proceedings of the Seventeenth International Conference on Machine Learning_, pages 759-766, 2000.
* Shi et al. (2022) Chengchun Shi, Masatoshi Uehara, Jiawei Huang, and Nan Jiang. A minimax learning approach to off-policy evaluation in confounded partially observable markov decision processes. In _International Conference on Machine Learning_, pages 20057-20094. PMLR, 2022.
* Simchowitz et al. (2020) Max Simchowitz, Karan Singh, and Elad Hazan. Improper learning for non-stochastic control. In _Conference on Learning Theory_, pages 3320-3436. PMLR, 2020.
* Song et al. (2022) Yuda Song, Yifei Zhou, Ayush Sekhari, Drew Bagnell, Akshay Krishnamurthy, and Wen Sun. Hybrid rl: Using both offline and online data can make rl efficient. In _The Eleventh International Conference on Learning Representations_, 2022.
* Tennenholtz et al. (2020) Guy Tennenholtz, Uri Shalit, and Shie Mannor. Off-policy evaluation in partially observable environments. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 10276-10283, 2020.
* Thomas and Brunskill (2016) Philip Thomas and Emma Brunskill. Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning. In _Proceedings of the 33rd International Conference on Machine Learning_, 2016.
* Uehara et al. (2020) Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax Weight and Q-Function Learning for Off-Policy Evaluation. In _Proceedings of the 37th International Conference on Machine Learning_, pages 1023-1032, 2020.
* Uehara et al. (2022a) Masatoshi Uehara, Haruka Kiyohara, Andrew Bennett, Victor Chernozhukov, Nan Jiang, Nathan Kallus, Chengchun Shi, and Wen Sun. Future-dependent value-based off-policy evaluation in pomdps. _arXiv preprint arXiv:2207.13081_, 2022a.
* Uehara et al. (2022b) Masatoshi Uehara, Ayush Sekhari, Jason D Lee, Nathan Kallus, and Wen Sun. Provably efficient reinforcement learning in partially observable dynamical systems. _arXiv preprint arXiv:2206.12020_, 2022b.
* Voloshin et al. (2021) Cameron Voloshin, Nan Jiang, and Yisong Yue. Minimax model learning. In _International Conference on Artificial Intelligence and Statistics_, pages 1612-1620. PMLR, 2021.
* Xie and Jiang (2020) Tengyang Xie and Nan Jiang. Q* approximation schemes for batch reinforcement learning: A theoretical comparison. In _Conference on Uncertainty in Artificial Intelligence_, pages 550-559. PMLR, 2020.
* Xie and Jiang (2021) Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In _International Conference on Machine Learning_, pages 11404-11413. PMLR, 2021.
* Xie et al. (2019) Tengyang Xie, Yifei Ma, and Yu-Xiang Wang. Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling. _Advances in Neural Information Processing Systems_, 32, 2019.
* Xie et al. (2021) Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. _arXiv preprint arXiv:2106.06926_, 2021.
* Yin and Wang (2021) Ming Yin and Yu-Xiang Wang. Towards instance-optimal offline reinforcement learning with pessimism. _Advances in neural information processing systems_, 34:4065-4078, 2021.
* Yin et al. (2021) Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal provable uniform convergence in offline policy evaluation for reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 1567-1575. PMLR, 2021.
* Yin et al. (2021)Yao-Liang Yu and Csaba Szepesvari. Analysis of kernel mean matching under covariate shift. In _Proceedings of the 29th International Coference on International Conference on Machine Learning_, pages 1147-1154, 2012.
* Zanette et al. (2021) Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offline reinforcement learning. _Advances in neural information processing systems_, 34:13626-13640, 2021.
* Zhan et al. (2022) Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D Lee. Pac reinforcement learning for predictive state representations. _arXiv preprint arXiv:2207.05738_, 2022.
* Zhang and Bareinboim (2016) Junzhe Zhang and Elias Bareinboim. Markov decision processes with unobserved confounders: A causal approach. _Purdue AI Lab, West Lafayette, IN, USA, Tech. Rep_, 2016.

Related Literature

OPE in POMDPs.There is a line of research on OPE in confounded POMDPs (Zhang and Bareinboim, 2016, Namkoong et al., 2020, Nair and Jiang, 2021, Guo et al., 2022, Lu et al., 2022, Shi et al., 2022, Hong et al., 2023). However, all these methods require the behavior policy to solely depend on the latent state. This assumption is inapplicable for our unconfounded POMDP setting, where the behavior policy depends on the observations. Hu and Wager (2023) studied the same unconfounded setting as ours. Nonetheless, their method uses multi-step importance sampling, leading to an undesirable exponential dependence on the horizon. The closest related work to our paper is (Uehara et al., 2022), which we analyze in detail in Section 3.

Online Learning in POMDPs.There are many prior works studying online learning algorithms for sub-classes of POMDPs, including decodable POMDPs (Krishnamurthy et al., 2016, Jiang et al., 2017, Du et al., 2019, Efroni et al., 2022), Latent MDPs (Kwon et al., 2021) and linear quadratic Gaussian setting (LQG) (Lale et al., 2021, Simchowitz et al., 2020). Recently, online learning algorithms with polynomial sample results have been proposed for both tabular POMDPs (Guo et al., 2016, Azizzadenesheli et al., 2016, Jin et al., 2020, Liu et al., 2022a) and POMDPs with general function approximation (Zhan et al., 2022, Liu et al., 2022b, Chen et al., 2022, Huang et al., 2023). All of these approaches are model-based and require certain model assumptions. Uehara et al. (2022) proposed a model-free online learning algorithm based on future-dependent value functions (FDVFs). However, their algorithm requires the boundness of FDVFs for all policies in the policy class, as well as a low-rank property of the Bellman loss, which limits the generality of the results.

OPE in MDPs.There has been a long history of studying OPE in MDPs, including importance sampling (IS) approaches (Precup et al., 2000, Li et al., 2011) and their doubly robust variants (Dudik et al., 2011, Jiang and Li, 2016, Thomas and Brunskill, 2016, Farajtabar et al., 2018), marginalized importance sampling (MIS) methods (Liu et al., 2018, Xie et al., 2019, Kallus and Uehara, 2020) and model-based estimators (Eysenbach et al., 2020, Yin et al., 2021, Voloshin et al., 2021). As mentioned in Section 1, directly applying IS-based approaches for POMDPs will result in exponential variance. Meanwhile, MIS-based approaches do not apply to POMDPs since they require the environment to satisfy the Markov assumption.

## Appendix B Discussions and Extensions

### Connection between Future-dependent Value Functions and Learnable

Future-dependent Value Functions

Uehara et al. (2022) proposed a learnable version of future dependent value functions and focused on the learning of learnable FDVFs instead of the original FDVFs defined in Definition 3. In this subsection, we first introduce the definition of learnable FDVFs and then show that it is equivalent to FDVFs under a full-rank assumption.

**Definition 14** (Learnable future-dependent value functions).: A Learnable future-dependent value function \(V_{\mathcal{F}}^{L}:\mathcal{F}\rightarrow\mathbb{R}\) is any function that satisfies the following: \(\forall\tau_{h}\),

\[(\mathcal{B}^{\mathcal{H}}V_{\mathcal{F}}^{L})(\tau_{h})=0.\]

For any FDVF \(V_{\mathcal{F}}\), since \(\mathcal{B}^{\mathcal{S}}V_{\mathcal{F}}\equiv 0\), we have \(\forall\tau_{h}\),

\[(\mathcal{B}^{\mathcal{H}}V_{\mathcal{F}})(\tau_{h}) =\mathbb{E}_{\pi_{b}}[\mu(o_{h},a_{h})\{r_{h}+V_{\mathcal{F}}(f_{h +1})\}-V_{\mathcal{F}}(f_{h})\mid\tau_{h}]\] \[=\langle\mathbf{b}(\tau_{h}),\mathcal{B}^{\mathcal{S}}_{h}V_{ \mathcal{F}}\rangle=0.\]

Therefore, all FDVFs are also learnable FDVFs. On the other hand, for any learnable FDVF \(V_{\mathcal{F}}^{L}\), let \(\mathcal{B}^{\mathcal{H}}_{h}V_{\mathcal{F}}^{L}\in\mathbb{R}^{|R_{h}|}\) be the Bellman residual vector for \(V_{\mathcal{F}}^{L}\) on \(\mathcal{H}_{h}\) and \(\mathcal{B}^{\mathcal{S}}_{h}V_{\mathcal{F}}^{L}\in\mathbb{R}^{S}\) be the Bellman residual vector on \(\mathcal{S}_{h}\). With the help of \(M_{\mathcal{H},h}\), we have the following relation between \(\mathcal{B}^{\mathcal{H}}_{h}V_{\mathcal{F}}^{L}\) and \(\mathcal{B}^{\mathcal{S}}_{h}V_{\mathcal{F}}^{L}\):

\[\mathcal{B}^{\mathcal{H}}_{h}V_{\mathcal{F}}^{L}=(M_{\mathcal{H},h})^{\top} \mathcal{B}^{\mathcal{S}}_{h}V_{\mathcal{F}}^{L}.\]

When \(\text{rank}(M_{\mathcal{H},h})=S\), \(\mathcal{B}^{\mathcal{H}}_{h}V_{\mathcal{F}}^{L}=\mathbf{0}\) if and only if \(\mathcal{B}^{\mathcal{S}}_{h}V_{\mathcal{F}}^{L}=\mathbf{0}\). Therefore, \(V_{\mathcal{F}}^{L}\) satisfies that \(\forall s_{h},(\mathcal{B}^{\mathcal{S}}V_{\mathcal{F}}^{L})(s_{h})=0\), implying that \(V_{\mathcal{F}}^{L}\) is also a FDVF. Under the regular full rank assumption, the two definitions are actually equivalent.

### Comparison between the Finite-horizon and the Discounted Formulations

The original results of Uehara et al. (2022) were given in the infinite-horizon discounted setting. To describe their data collection assumption in simple terms, consider an infinite trajectory that enters the stationary configuration of \(\pi_{b}\), and we pick a random time step and call it \(t=0\) (which corresponds to a time step \(h\) in our setting). Then, history (analogous to our \(\tau_{h}\)) is \((o_{(-M_{F}):(-1)},a_{(-M_{F}):(-1)})\), and future is \((o_{0:M_{H}},a_{0:M_{F}-1})\). \(M_{F}\) and \(M_{H}\) are hyperparameters that determines the lengths, and should not be confused with our \(M_{F,h}\) and \(M_{\mathcal{H},h}\) matrices. Data points collected in this way form the "transition" dataset. Separately, there is an "initial" dataset of \((o_{0:M_{F}-1},a_{0:M_{F}})\) tuples to represent the analogue of initial state distribution in MDPs.

Besides the fact that finite-horizon formulation better fits real-world applications with episodic nature (e.g., a session in conversational system), there are mathematical reasons why the finite-horizon formulation is more natural:

1. Unlike the infinite-horizon setting, we do not need separate "transition" and "initial" datasets. The natural \(H\)-step episode dataset plays both roles. We also do not need the stationarity assumption.
2. In the infinite-horizon, \(M_{F}\) and \(M_{H}\) are hyperparameters which do not have an obvious upper bound. The larger they are, it is easier to satisfy certain identification assumptions (such as full-rankness of the counterparts of our \(M_{\mathcal{H},h}\) and \(M_{\mathcal{F},h}\)). On the other hand, the "curses of future and horizon" we identified in this work corresponds to exponential dependence on \(M_{F}\) and \(M_{H}\), so there is a potential trade-off in the choice of \(M_{F}\) and \(M_{H}\). In contrast, the future and the history have maximum lengths (\(H-h\) and \(h-1\) at time step \(h\)) in our setting. As we establish coverage assumptions that avoid potential exponential dependencies on these lengths, the trade-off in the choice of length is eliminated, so it is safe for us to always choose the maximum length, as we do in the paper.

### Refined Coverage

As alluded to at the beginning of Section 5, we can tighten the definition of \(L_{2}\) belief coverage by directly using the ratio:

\[\sup_{V\in\mathcal{V}}\frac{|\mathbb{E}_{\pi_{e}}[(\mathcal{B}^{\mathcal{S}}V) (s_{h})]|}{\sqrt{\mathbb{E}_{\pi_{b}}[(\mathcal{B}^{\mathcal{H}}V)(\tau_{h}) ^{2}]}},\]

which automatically leverages the structure of \(\mathcal{V}\). For example, if \(\{\mathcal{B}^{\mathcal{S}}_{h}V:V\in\mathcal{V}\}\) only occupies a low-dimensional subspace of \(\mathbb{R}^{S_{h}}\) (that is, there exists \(\phi:\mathcal{S}_{h}\to\mathbb{R}^{d}\) such that \((\mathcal{B}^{\mathcal{S}}V)(s_{h})=\phi(s_{h})^{\top}\theta_{V}\)), then belief matching above Eq.(8) can be done in \(\mathbb{R}^{d}\) instead of \(\mathbb{R}^{S_{h}}\). Other coverage definitions may be refined in similar manners.

### Interpretation of Assumption 8

To interpret Assumption 8, let \(x=\mathfrak{u}(f_{h})/Z(f_{h})\in\Delta(\mathcal{S})\), and assign a distribution over \(x\) by sampling \(f_{h}\sim\text{diag}(Z_{h}/S)\), then Assumption 8 becomes \(x^{\top}\mathbb{E}_{Z_{h}/S}[xx^{\top}]^{-1}x\leq C_{\mathcal{F},U}S\) for any \(x\) with non-zero probability. This is a common regularity assumption (Duan et al., 2020; Perdomo et al., 2023), requiring that no \(x\) points to a direction in \(\mathbb{R}^{S}\) alone without being joined by others from the distribution. While \(1/\sigma_{\min}(\Sigma_{\mathcal{F},h})\) can bound this quantity, the other way around is not true, implying that Assumption 8 is weaker.

**Example 8** (Bounded \(C_{\mathcal{F},U}\) without bounded \(1/\sigma_{\min}(\Sigma_{\mathcal{F},h})\)).: _Consider a distribution over \(x\) where with 0.5 probability, \(x=[1/2+\epsilon,1/2-\epsilon]^{\top}\), and with the other 0.5 probability, \(x=[1/2-\epsilon,1/2+\epsilon]^{\top}\). Then, as \(\epsilon\to 0\), \(1/\sigma_{\min}(\Sigma_{\mathcal{F},h})\to\infty\), but \(x^{\top}\mathbb{E}[xx^{\top}]^{-1}x\leq 2\)._

### Intuition for \(L_{\infty}\) Outcome Coverage

Here we provide more details about the \(L_{\infty}\) outcome coverage in Section 4.2, which are omitted in the main text due to space limit.

Looseness of \(L_{2}\) outcome coverageWe start by examining the looseness of \(L_{2}\) outcome coverage (Assumption 7), which will motivate the \(L_{\infty}\) version of outcome coverage. To develop intuitions, wefirst examine the boundedness of Eq.(4) (the construction that leads to \(L_{2}\) outcome coverage) in the setting of Example 2. In this example, \(\Sigma_{\mathcal{F},h}=\mathbf{I}\), and \(\|V_{\mathcal{F}}\|_{\infty}\leq H\). According to the \(L_{2}\) Holder decomposition in Proposition 4, however,

\[\|V_{\mathcal{F}}\|_{\infty}\leq\max_{f_{h}}\|\mathbf{u}(f_{h})/Z(f_{h})\|_{2} \|V_{\mathcal{S}}^{\pi_{c}}\|_{2}\leq H\sqrt{S},\]

where \(\|\cdot\|_{\Sigma_{\mathcal{F},h}^{-1}}\) is replaced by \(\|\cdot\|_{2}\) since \(\Sigma_{\mathcal{F},h}=I\). In contrast, the tight analysis is

\[\|V_{\mathcal{F}}\|_{\infty}\leq\max_{f_{h}}\|\mathbf{u}(f_{h})/Z(f_{h})\|_{1 }\|V_{\mathcal{S}}^{\pi_{c}}\|_{\infty}\leq H.\]

The comparison clearly highlights that the looseness in \(S\) comes from the fact that \(L_{2}\) Holder does not fully leverage the fact that \(\|\mathbf{u}(f_{h})/Z(f_{h})\|_{1}=1\) and loosely relaxing it to \(\|\mathbf{u}(f_{h})/Z(f_{h})\|_{2}\leq 1\). In contrast, our \(L_{\infty}\) coverage assumption allows for a more natural \(L_{1}/L_{\infty}\) Holder to leverage the \(L_{1}\) normalization of \(\mathbf{u}(f_{h})/Z(f_{h})\).

\(L_{\infty}\) Outcome CoverageSimilar to \(L_{\infty}\) belief coverage, we could define \(L_{\infty}\) outcome coverage simply as \(\|(\Sigma_{\mathcal{F},h})^{-1}V_{\mathcal{S}}^{\pi_{c}}\|_{\infty}\). The small caveat and inelegance is that it does not recover \(V_{\mathcal{F}}=R^{+}\) when \(\pi_{b}=\pi_{e}\). To address this, we leverage the lesson from belief coverage (Section 5.2): to obtain \(C_{\mathcal{H},\infty}=1\) when \(\pi_{b}=\pi_{e}\), a key property is that \(\Sigma_{\mathcal{H},h}\) (data covariance matrix) and \(\mathbf{b}_{h}^{\pi_{b}}\) (the vector to be covered) are the covariance matrix and the mean vector w.r.t. the same distribution of belief vectors. In contrast, for the outcome coverage case, \(V_{\mathcal{S}}^{\pi_{c}}\) depends on the reward function, but such information is missing in \(\Sigma_{\mathcal{F},h}\), which prevents a perfect cancellation in the on-policy case.

Therefore, we can adjust the definition of \(\Sigma_{\mathcal{F},h}\) to mimic the situation of belief coverage. The first step is to find the counterpart of belief state which is \(L_{1}\)-normalized. This obviously corresponds to \(Z_{h}^{-1}M_{\mathcal{F},h}^{\top}\), whose rows sum up to \(1\). Let \(\bar{\mathbf{u}}(f_{h}):=\mathbf{u}(f_{h})/Z(f_{h})\in\Delta(\mathcal{S}_{h})\), then \(\Sigma_{\mathcal{F},h}=S\cdot\mathbb{E}_{Z_{h}/S}[\bar{\mathbf{u}}\bar{ \mathbf{u}}^{\top}]\). Then, by incorporating reward information into \(\Sigma_{\mathcal{F},h}\), we arrive at the solution in Eq.(5).

Finally, we examine the setting of Example 2 and show that \(V_{\mathcal{F}}\) in Eq.(5) is similarly well-behaved as Eq.(4) in this scenario.

**Example 9**.: _In the same setting as Example 2, i.e., \(f_{h}\) always reveals \(s_{h}\), we have \(\Sigma_{\mathcal{F},h}^{R}=\text{diag}(V_{\mathcal{S}_{h}}^{\pi_{b}})\), and \(\|(\Sigma_{\mathcal{F},h}^{R})^{-1}V_{\mathcal{S}}^{\pi_{c}}\|_{\infty}=\max_ {s_{h}}V_{\mathcal{S}}^{\pi_{c}}(s_{h})/V_{\mathcal{S}}^{\pi_{b}}(s_{h})\)._

This example also closely resembles Example 7 for belief coverage. While \((\Sigma_{\mathcal{F},h}^{R})^{-1}\) may behave poorly if \(V_{\mathcal{S}}^{\pi_{b}}(s_{h})\) is small, an easy fix is to simply add a constant to the reward function and shift its range to e.g., [1, 2], which ensures a small \(C_{\mathcal{F},\infty}\).

### Extension to History-dependent Policies

The main text assumes memoryless \(\pi_{b}\) and \(\pi_{e}\). Similar to Uehara et al. (2022), we can extend the approach to handle history-dependent policies (the changes to the proof are sketched below). Instead of rewriting the proof with these changes, we will provide a "black-box" reduction that handles the extension more elegantly and allow for more general results than Uehara et al. (2022).

Memory-based Policies and Changes in the ProofsDefine _memory_\(m_{h}\) as a function of \(\tau_{h}\) (i.e., \(m_{h}=m_{h}(\tau_{h})\)), and a memory-based policy can depend on \((m_{h},o_{h})\). Uehara et al. (2022) allows for \(m_{h}=(o_{h-M:h-1},a_{h-M:h-1})\) for some fixed window \(M\) in their analyses. If we let \(m_{h}=(o_{1:h-1},a_{1:h-1})\), we recover fully general history-dependent policies.

If we direct modify the proofs to accommodate this generalization (as done in Uehara et al. (2022)), the required changes are:

* \(V_{\mathcal{S}}^{\pi_{c}}\) and \(V_{\mathcal{F}}\) need to additionally depend on \(m_{h}\) to be well defined. \(V\in\mathcal{V}\) also generally depend on \(m_{h}\) since we need realizability \(V_{\mathcal{F}}\in\mathcal{V}\).
* \(M_{\mathcal{F},h}\) and \(M_{\mathcal{H},h}\) have rows indexed by \((s_{h},m_{h})\) instead of just \(s_{h}\). That is, we replace belief state with posterior over \((s_{h},m_{h})\). Similarly, entries in \(M_{\mathcal{F},h}\) are now future probabilities conditioned on \((s_{h},m_{h})\)However, if we consider latent state coverage, which is weaker than belief coverage that is really needed (Appendix E.3), we now require bounded \(\frac{d^{\pi_{e}}(s_{h},m_{h})}{d^{\pi_{b}}(s_{h},m_{h})}\). This is generally exponential in the length of \(m_{h}\) (e.g., if \(M=h-1\), the ratio is exactly the cumulative importance weight), preventing us from handling \(\pi_{b}\) and \(\pi_{e}\) with long-range history dependencies.

Reduction to Memoryless CaseInstead of changing the proofs, we now describe an alternative approach that (1) produces results similar to Uehara et al. (2022), and (2) handles more general recurrent policies that are _finite-state-machines_ (FSMs), which subsume fully history-dependent policies (or policies that depend on a fixed-length window) as special cases.

Concretely, a policy with memory \(m_{h}\) is said to be an FSM, if \(m_{h+1}\) can be computed solely based on \(m_{h},o_{h},a_{h}\), without using other information in \(\tau_{h}\). \(m_{h}=(o_{h-M:h-1},a_{h-M:h-1})\) satisfies this definition, as computing \(m_{h+1}\) is simply dropping the oldest observation-action pair from \(m_{h}\) and appending the newest one.6 Another example is belief update, where \(\mathbf{b}(\tau_{h+1})\) can be computed from \(\mathbf{b}(\tau_{h}),o_{h},a_{h}\).

Footnote 6: When \(h\leq M\) no dropping is needed. In fact, if we never drop, then \(m_{h}=\tau_{h}\) is just the history.

We assume that both \(\pi_{b}\) and \(\pi_{e}\) have the same memory; if they differ, we can simply concatenate their memories together. Then, handling memories in our analyses takes two steps:

1. We allow latent state transition to depend on \(o_{h}\), that is, \(s_{h+1}\sim\mathbb{T}(\cdot|s_{h},a_{h},o_{h})\). This model has been considered by Jiang et al. (2017) to unify POMDPs and low-rank MDPs. **Our analyses hold as-is without any changes.** To provide some intuition: the key property that enables the analyses of FDVF is that \(s_{h}\) is a bottleneck that separates histories from futures, which enables \((\mathcal{B}^{\mathcal{H}}V)(\tau_{h})=\langle\mathbf{b}(\tau_{h}),\mathcal{B} ^{\mathcal{S}}_{h}V\rangle\) in Eq.(2). This property is intact with the additional dependence of \(s_{h}\) on \(o_{h-1}\).
2. We provide a **blackbox reduction** from the memory-based case to the memoryless case. Define a new POMDP that is equivalent to the original one, where the latent state is \(\tilde{s}_{h}=(s_{h},m_{h})\). The observation is \(\tilde{o}_{h}=(o_{h},m_{h})\), which can be emitted from \(\tilde{s}_{h}\) since \(\tilde{s}_{h}\) contains \(m_{h}\). The latent state transition is \(\tilde{s}_{h+1}=(s_{h+1},m_{h+1})\), where \(s_{h+1}\sim\mathbb{T}(\cdot|s_{h},a_{h},o_{h})\), and \(m_{h+1}\) is updated from \(m_{h}\) (contained in \(\tilde{s}_{h}\)), \(a_{h}\), and \(o_{h}\); this is why we need \(o_{h}\) to participate in latent transitions. Now it suffices to perform OPE in the new POMDP. Data from the original POMDP can be converted to that of the new POMDP with \(\tilde{o}_{h}=(o_{h},m_{h})\). \(\pi_{b}\) and \(\pi_{e}\) only need to depend on \(\tilde{o}_{h}\) and become memoryless.

Given the reduction, if we design function classes that operate on the histories and futures of the new POMDP, the guarantees in the main text immediately hold. The final step is to translate the objects (e.g., futures and histories) and guarantees in the new POMDP back to the original POMDP for interpretability. Note that the history in the new POMDP is \(\tilde{\tau}_{h}=(\tilde{o}_{1:h-1},a_{1:h-1})\), which contains the same information as \(\tau_{h}\), so functions in \(\Xi\) and \(\mathcal{W}\) can still operate on \(\tau_{h}\). Similarly, \(V\in\mathcal{V}\) now takes \((m_{h},f_{h})\) as input, which is consistent with Uehara et al. (2022).

When translating the assumptions back to the original POMDP, we can see that now the results can be well-behaved when \(m_{h}\) is "simple". For example, if \(m_{h}\) takes values from a constant-sized space, \(d^{\pi_{e}}(s_{h},m_{h})/d^{\pi_{b}}(s_{h},m_{h})\) (which lower-bounds belief coverage as discussed above) may not blow up exponentially, even though \(m_{h}\) can hold information that is arbitrarily old (e.g., it remembers one bit of information from \(h=1\)). This is a scenario that cannot be handled by the formulation of Uehara et al. (2022).

However, the guarantee can still deteriorate when the policies maintain rich memories, even if these memories are highly structured, such as \(m_{h}=\mathbf{b}(\tau_{h})\). Under the mild assumption that all histories lead to distinct belief states \(\mathbf{b}(\tau_{h})\) (they can be very close in \(\mathbb{R}^{S}\) and just need to be not exactly identical), the belief state in the new POMDP, \(\tilde{\mathbf{b}}(\tau_{h})\), completely ignores the linear structure of \(m_{h}\) and treats it in the same way as \(m_{h}=\tau_{h}\),7 leading to an exponentially large belief coverage. How to handle policies that depend on rich but highly structured memories such as belief states is a major open problem.

### Recovering MDP Algorithms and Analyses

One somewhat undesirable property of our algorithms and analyses is that they do not subsume MDP algorithms/analyses as a special case. MDPs can be viewed as POMDPs with identity emission, i.e., \(o_{h}=s_{h}\). In this case, algorithms considered in this paper are analogous to their MDP counterparts Uehara et al. (2020), except that the MDP algorithms require that all functions \(V\), \(w\), \(\xi\) to operate on the current state \(s_{h}\). In contrast, our FDVF operates on \(f_{h}=(o_{h},a_{h},\ldots,o_{H},a_{H})\) which includes \(o_{h}\)(\(=s_{h}\)). To recover the MDP algorithm as a special case, we can choose \(\mathcal{V}\) such that every \(V\in\mathcal{V}\) only depends on \(f_{h}\) through \(o_{h}\). However, \(w\) and \(\xi\) operate on \(\tau_{h}=(o_{1},a_{1},\ldots,o_{h-1},a_{h-1})\) which does not contain \(o_{h}\). This makes subsuming the MDP case difficult.

Here we describe briefly how to overcome this issue by slightly modifying our analysis; the changes are somewhat similar to Appendix B.6. Instead of letting \(\tau_{h}=(o_{1},a_{1},\ldots,o_{h-1},a_{h-1})\), we can define an alternative notion of history \(\tilde{\tau}_{h}=(o_{1},a_{1},\ldots,o_{h})\) and replace \(\tau_{h}\) in the main text with \(\tilde{\tau}_{h}\). Algorithmically, we can immediately recover MDP algorithms by also restricting functions in \(\mathcal{W}\) and \(\mathcal{X}\) to only operate on \(o_{h}\). However, our analyses (which apply to general POMDPs) need to change accordingly, as replacing \(\tau_{h}\) with \(\tilde{\tau}_{h}\) will break the key properties, such as \(\mathcal{B}^{\mathcal{H}}V\) being linear in \(\mathcal{B}^{\mathcal{S}}V\). The problem is that in the definitions of \(V_{\mathcal{S}}^{\pi_{e}}\) and \(\mathcal{B}^{\mathcal{S}}V\) we want to marginalize out the randomness of \(o_{h}\) given \(s_{h}\), which is in conflict with conditioning on \(\tilde{\tau}_{h}\) that includes all the information of \(o_{h}\). To address this, we simply replace \(s_{h}\) with \(\hat{s}_{h}:=(s_{h},o_{h})\) in the definition of \(V_{\mathcal{S}}^{\pi_{e}}\), \(\mathcal{B}^{\mathcal{S}}V\), \(M_{\mathcal{H},h}\), and \(M_{\mathcal{F},h}\). That is, \(V_{\mathcal{S}}^{\pi_{e}}\) is now a function of \(\hat{s}_{h}\) and also depends on \(o_{h}\), \(\mathbf{b}(\tau_{h})\) is the posterior distribution over \(\tilde{s}_{h}\), and the entries of the outcome matrix \(M_{\mathcal{F},h}\) is \(\Pr_{\tau_{h}}(f_{h}|s_{h},o_{h})\). This retains the key property that \(\mathcal{B}^{\mathcal{H}}V\) is linear in \(\mathcal{B}^{\mathcal{S}}V\) with \(\tilde{\mathbf{b}}(\tau_{h})\) as the coefficient.

When we specialize the guarantees of this modified analysis to the MDP setting, outcome coverage is always satisfied and we can always use the MDP's standard value function as \(V_{\mathcal{F}}\). For belief coverage, note that \((\mathcal{B}^{\mathcal{H}}V)(\tau_{h})=(\mathcal{B}^{\mathcal{S}}V)(s_{h},o_{h})\), which is simply the standard definition of Bellman error in MDPs (since \(s_{h}=o_{h}\)), which we denote as \((\mathcal{B}V)(s_{h})\). Plugging this into the refined coverage discussed in Appendix B.3, we recover the standard definition of coverage in the MDP setting, namely \(\sup_{V\in\mathcal{V}}\frac{\|\pi_{e_{\mathcal{F}}}([\mathcal{B}^{\mathcal{S} }V)(s_{h})]|}{\sqrt{\mathbb{E}_{s_{h}}[(\mathcal{B}^{\mathcal{S}}V)(s_{h})]}}\).

### Incorporating Different Latent-State Priors in \(V_{\mathcal{F}}\)

In Section 4 we showed that \(\Sigma_{\mathcal{F},h}\), which is crucial to the construction in Eq.(4), can be viewed as the confusion matrix of making posterior predictions of \(s_{h}\) from \(f_{h}\), using a uniform prior. The uniform prior corresponds to the all-one vector \(\mathbf{1}_{\mathcal{S}}\) in the definition of \(Z_{h}:=\operatorname{diag}(\mathbf{1}_{\mathcal{S}}^{\top}M_{\mathcal{F},h})\), which naturally leads to the question of whether we can incorporate a different and perhaps more informative prior.

Let \(\mathbf{p}_{h}\in\Delta(\mathcal{S}_{h})\) be the prior we would like to use instead of the uniform prior. An immediate idea is to define \(Z_{h}^{\mathbf{p}_{h}}=\operatorname{diag}(\mathbf{p}_{h}^{\top}M_{\mathcal{F},h})\) (possibly up to a \(S\) scaling factor, as the uniform prior is \(\mathbf{p}_{\text{unif}}=[1/S,\cdots,1/S]^{\top}\) and \(\mathbf{1}_{\mathcal{S}}=S\mathbf{p}_{\text{unif}}\)) and directly plug it into the construction in Eq.(4). However, this breaks some of the key properties of the current construction of \(\Sigma_{\mathcal{F},h}\), such as \(L_{1}\) normalization of rows of \(Z_{h}^{-1}M_{\mathcal{F},h}^{\top}\).

To resolve this, the key is to realize that the rows of \(Z_{h}^{-1}M_{\mathcal{F},h}^{\top}\) are \(L_{1}\) normalized because they can be viewed as posteriors over \(\mathcal{S}_{h}\). In comparison, if we examine the \((f_{h},s_{h})\)-th entry of \((Z_{h}^{\mathbf{p}_{h}})^{-1}M_{\mathcal{F},h}^{\top}\), it is

\[\frac{\Pr_{\tau_{h}}[f_{h}|s_{h}]}{\sum_{s^{\prime}\in S_{h}}\Pr_{\tau_{h}}[f_{ h}|s^{\prime}]\mathbf{p}_{h}(s^{\prime})},\]

so clearly it is missing a \(\mathbf{p}_{h}(s_{h})\) term on the numerator to be a proper posterior. Inspired by this observation, we can see how to fix the construction now: recall that \(V_{\mathcal{F}}\) needs to satisfy \(M_{\mathcal{F},h}V_{\mathcal{F},h}=V_{\mathcal{S},h}^{\pi_{e}}\). If \(\mathbf{p}_{h}>0\), then this is equivalent to

\[(\operatorname{diag}(\mathbf{p}_{h})M_{\mathcal{F},h})V_{\mathcal{F},h}= \operatorname{diag}(\mathbf{p}_{h})V_{\mathcal{S},h}^{\pi_{e}}.\]

Then, the minimum \(Z_{h}^{\mathbf{p}_{h}}\)-weighted solution of this equation will provide the desired construction that preserves the \(L_{1}\) normalization properties.

Which prior \(\mathbf{p}_{h}\) to use?For the initial time step \(h=1\), the initial latent-state distribution \(d_{1}\) is the most natural candidate for the prior. If \(d_{1}(s_{1})=0\) for some \(s_{1}\in\mathcal{S}_{1}\), incorporating \(d_{1}\) as theprior will essentially treat \(s_{1}\) as non-existent and ignore the outcome coverage of \(\pi_{b}\) over \(\pi_{e}\) from \(s_{1}\), which is reasonable because \(s_{1}\) will not be activated by neither \(\pi_{e}\) and \(\pi_{b}\). For \(h>1\), the answer is less clear, and natural candidates include \(\mathbf{p}_{h}=d_{h}^{\pi_{b}}\) and \(d_{h}^{\pi_{e}}\), or perhaps their probability mixture. For the example scenarios examined in the main text, these choices (or even a uniform prior) do not make significant differences, and we leave the investigation of which prior is the best to future work.

## Appendix C Proofs for Section 3

### Proof of Lemma 1

The RHS of the lemma statement is

\[\sum_{h=1}^{H}\mathbb{E}_{\pi_{e}}\left[(\mathcal{B}^{\mathcal{S}}V)(s_{h}) \right]=\sum_{h=1}^{H}\mathbb{E}_{s_{h}\sim d_{h}^{\pi_{e}}}\left[\mathbb{E}_{ \begin{subarray}{c}a_{h}\sim\pi_{e}\\ a_{h+1}\cdot H\sim\pi_{b}\end{subarray}}[r_{h}+V(f_{h+1})\mid s_{h}]-\mathbb{E} _{\pi_{b}}[V(f_{h})\mid s_{h}]\right].\]

Now notice that the expected value of the \(V(f_{h+1})\) term for \(h\) is the same as that of the \(V(f_{h})\) term for \(h+1\), since both can be written as \(\mathbb{E}_{s_{h+1}\sim d_{h+1}^{\pi_{e}}}[\mathbb{E}_{\pi_{b}}[V(f_{h+1})|s_{ h+1}]]\). After telescoping cancellations, what remains on the RHS is

\[\sum_{h=1}^{H}\mathbb{E}_{s_{h}\sim d_{h}^{\pi_{e}}}\left[\mathbb{E}_{ \begin{subarray}{c}a_{h}\sim\pi_{e}\\ a_{h+1}\cdot H\sim\pi_{b}\end{subarray}}[r_{h}\mid s_{h}]\right]-\mathbb{E}_{ s_{1}}[\mathbb{E}_{\pi_{b}}[V(f_{1})|s_{1}]]=J(\pi_{e})-\mathbb{E}_{\pi_{b}}[V(f_{1})].\qed\]

### Proof of Theorem 2

The proof follows similar idea from Uehara et al. (2022a). Let \(\mathcal{G}_{h}V=\mu(o_{h},a_{h})(r_{h}+V(f_{h+1}))-V(f_{h})\), we have

\[\widehat{V}=\operatorname*{argmin}_{V\in\mathcal{V}}\max_{\xi\in\Xi}\sum_{h=1 }^{H}\mathbb{E}_{\mathcal{D}}\left[(\mathcal{G}_{h}V)^{2}-(\mathcal{G}_{h}V- \xi(\tau_{h}))^{2}\right].\]

For any fixed \(V\), we define

\[\widehat{\xi}_{V}=\operatorname*{argmax}_{\xi\in\Xi}-\sum_{h=1}^{H}\mathbb{E }_{\mathcal{D}}\left[(\mathcal{G}_{h}V-\xi(\tau_{h}))^{2}\right].\]

Analysis of Inner Maximizer.We observe that for any \(h\in[H]\),

\[\mathbb{E}_{\pi_{b}}\left[(\xi(\tau_{h})-\mathcal{G}_{h}V)^{2}-((\mathcal{B}^ {\mathcal{H}}V)(\tau_{h})-\mathcal{G}_{h}V)^{2}\right]=\mathbb{E}_{\pi_{b}} \left[(\xi(\tau_{h})-(\mathcal{B}^{\mathcal{H}}V)(\tau_{h}))^{2}\right].\]

Let \(X_{h}=(\xi(\tau_{h})-\mathcal{G}_{h}V)^{2}-((\mathcal{B}^{\mathcal{H}}V)(\tau _{h})-\mathcal{G}_{h}V)^{2}\) and \(X=\sum_{h=1}^{H}X_{h}\). Let \(\bar{C}=\max\{1+C_{\mathcal{V}},C_{\Xi}\}\), since \(C_{\mu}\geq 1\), we have \(|\xi(\tau_{h})|\leq\bar{C}\), \(|\mathcal{G}_{h}V|\leq 3C_{\mu}\bar{C}\) and \(\left|\mathcal{B}^{\mathcal{H}}V(\tau_{h})\right|\leq 3\bar{C}\). Therefore, we have \(|X_{h}|\leq 40C_{\mu}\bar{C}^{2}\) and \(|X|\leq 40HC_{\mu}\bar{C}^{2}\). We observe that

\[\mathbb{E}_{\pi_{b}}[X_{h}^{2}]\] \[\leq\mathbb{E}_{\pi_{b}}\left[((\mathcal{B}^{\mathcal{H}}V)(\tau _{h})-\xi(\tau_{h}))^{2}\left((\mathcal{B}^{\mathcal{H}}V)(\tau_{h})+\xi(\tau _{h})-2\mathcal{G}_{h}V\right)^{2}\right]\] \[\leq\mathbb{E}_{\pi_{b}}\left[\left((\mathcal{B}^{\mathcal{H}}V)( \tau_{h})-\xi(\tau_{h})\right)^{2}\left(30\bar{C}^{2}+12(\mathcal{G}_{h}V)^{2 }\right)\right]\] \[\leq\mathbb{E}_{\pi_{b}}\left[\left((\mathcal{B}^{\mathcal{H}}V)( \tau_{h})-\xi(\tau_{h})\right)^{2}\left(54\bar{C}^{2}+96\bar{C}^{2}\mu(o_{h},a _{h})^{2}\right)\right]\] \[\leq 150\bar{C}^{2}C_{\mu}\mathbb{E}_{\pi_{b}}\left[\left(( \mathcal{B}^{\mathcal{H}}V)(\tau_{h})-\xi(\tau_{h})\right)^{2}\right].\]

Hence, for the variance of \(X\), we have

\[\operatorname*{Var}_{\pi_{b}}[X] \leq H\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[X_{h}^{2}\right]\] \[\leq 150H\bar{C}^{2}C_{\mu}\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[ \left((\mathcal{B}^{\mathcal{H}}V)(\tau_{h})-\xi(\tau_{h})\right)^{2}\right].\]From Bernstein's inequality, with probability at least \(1-\delta/2\), \(\forall V\in\mathcal{V}\), \(\forall\xi\in\Xi\), we have

\[\left|\sum_{h=1}^{H}\{\mathbb{E}_{\mathcal{D}}-\mathbb{E}_{\pi_{h} }\}[(\xi(\tau_{h})-\mathcal{G}_{h}V)^{2}-((\mathcal{B}^{\mathcal{H}}V)(\tau_{h} )-\mathcal{G}_{h}V)^{2}]\right|\] \[\leq\sqrt{\frac{150H\bar{C}^{2}C_{\mu}\sum_{h=1}^{H}\mathbb{E}_{ \pi_{h}}[((\mathcal{B}^{\mathcal{H}}V)(\tau_{h})-\xi(\tau_{h}))^{2}]\log(4| \mathcal{V}||\Xi|/\delta)}{n}}+\frac{40H\bar{C}^{2}C_{\mu}\log(4|\mathcal{V}|| \Xi|/\delta)}{n}.\] (11)

From Bellman completeness assumption \(\mathcal{B}^{\mathcal{H}}\mathcal{V}\subset\Xi\), we also have

\[\sum_{h=1}^{H}\mathbb{E}_{\mathcal{D}}[(\widehat{\xi}_{V}-\mathcal{G}_{h}V)^{ 2}-((\mathcal{B}^{\mathcal{H}}V)(\tau_{h})-\mathcal{G}_{h}V)^{2}]\leq 0.\] (12)

Therefore, we obtain that

\[\sum_{h=1}^{H}\mathbb{E}_{\pi_{h}}[((\mathcal{B}^{\mathcal{H}}V) (\tau_{h})-\widehat{\xi}_{V}(\tau_{h}))^{2}]\] \[=\sum_{h=1}^{H}\mathbb{E}_{\pi_{h}}[(\widehat{\xi}_{V}(\tau_{h})- \mathcal{G}_{h}V)^{2}-((\mathcal{B}^{\mathcal{H}}V)(\tau_{h})-\mathcal{G}_{h} V)^{2}]\] \[\leq\left|\sum_{h=1}^{H}\{\mathbb{E}_{\mathcal{D}}-\mathbb{E}_{ \pi_{h}}\}[(\widehat{\xi}_{V}(\tau_{h})-\mathcal{G}_{h}V)^{2}-((\mathcal{B}^{ \mathcal{H}}V)(\tau_{h})-\mathcal{G}_{h}V)^{2}]\right|+\sum_{h=1}^{H}\mathbb{E} _{\mathcal{D}}[(\widehat{\xi}_{V}(\tau_{h})-\mathcal{G}_{h}V)^{2}-((\mathcal{ B}^{\mathcal{H}}V)(\tau_{h})-\mathcal{G}_{h}V)^{2}]\] \[\leq\left|\sum_{h=1}^{H}\{\mathbb{E}_{\mathcal{D}}-\mathbb{E}_{ \pi_{h}}\}[(\widehat{\xi}_{V}(\tau_{h})-\mathcal{G}_{h}V)^{2}-((\mathcal{B}^{ \mathcal{H}}V)(\tau_{h})-\mathcal{G}_{h}V)^{2}]\right|\] \[\leq\sqrt{\frac{150H\bar{C}^{2}C_{\mu}\sum_{h=1}^{H}\mathbb{E}_{ \pi_{h}}[((\mathcal{B}^{\mathcal{H}}V)(\tau_{h})-\widehat{\xi}_{V}(\tau_{h})) ^{2}]\log(4|\mathcal{V}||\Xi|/\delta)}{n}}+\frac{40H\bar{C}^{2}C_{\mu}\log(4| \mathcal{V}||\Xi|/\delta)}{n}.\]

The second inequality is from Equation (12) and the last inequality is from Equation (11). We then have

\[\sum_{h=1}^{H}\mathbb{E}_{\pi_{h}}[((\mathcal{B}^{\mathcal{H}}V)(\tau_{h})- \widehat{\xi}_{V}(\tau_{h}))^{2}]\leq\varepsilon_{\text{stat}},\quad\varepsilon _{\text{stat}}:=\frac{225H\bar{C}^{2}C_{\mu}\log(4|\mathcal{V}||\Xi|/\delta)}{ n}.\] (13)

Combining Equation (11) and Equation (13), we have

\[\left|\sum_{h=1}^{H}\{\mathbb{E}_{\mathcal{D}}-\mathbb{E}_{\pi_{h}}\}[( \widehat{\xi}_{V}(\tau_{h})-\mathcal{G}_{h}V)^{2}-((\mathcal{B}^{\mathcal{H}} V)(\tau_{h})-\mathcal{G}_{h}V)^{2}]\right|\leq\varepsilon_{\text{stat}}.\] (14)

Hence,

\[\left|\sum_{h=1}^{H}\mathbb{E}_{\mathcal{D}}[(\widehat{\xi}_{V}( \tau_{h})-\mathcal{G}_{h}V)^{2}]-\sum_{h=1}^{H}\mathbb{E}_{\mathcal{D}}[(( \mathcal{B}^{\mathcal{H}}V)(\tau_{h})-\mathcal{G}_{h}V)^{2}]\right|\] \[\leq\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{h}}[(\widehat{\xi}_{V}( \tau_{h})-\mathcal{G}_{h}V)^{2}]-\sum_{h=1}^{H}\mathbb{E}_{\pi_{h}}[((\mathcal{ B}^{\mathcal{H}}V)(\tau_{h})-\mathcal{G}_{h}V)^{2}]\right|+2\varepsilon_{\text{stat}}\] \[=\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{h}}[((\mathcal{B}^{\mathcal{ H}}V)(\tau_{h})-\widehat{\xi}_{V}(\tau_{h}))^{2}]\right|+2\varepsilon_{\text{stat}} \leq 3\varepsilon_{\text{stat}}.\]

The first inequality is from Equation (14) and the last step is from Equation (13).

Analysis of Outer Minimizer.For any future-dependent value function \(V_{\mathcal{F}}\), from optimality of \(\widehat{V}\) and the convergence of inner maximizer, we have

\[\sum_{h=1}^{H}\mathbb{E}_{\mathcal{D}}[(\mathcal{G}_{h}\widehat{V}) ^{2}-(\mathcal{G}_{h}\widehat{V}-((\mathcal{B}^{\mathcal{H}}\widehat{V}))( \tau_{h}))^{2}] \leq\sum_{h=1}^{H}\mathbb{E}_{\mathcal{D}}\left[(\mathcal{G}_{h} \widehat{V})^{2}-(\mathcal{G}_{h}\widehat{V}-\widehat{\xi}_{V})^{2}\right]+3 \varepsilon_{\text{stat}}\] \[\leq\sum_{h=1}^{H}\mathbb{E}_{\mathcal{D}}\left[(\mathcal{G}_{h}V _{\mathcal{F}})^{2}-(\mathcal{G}_{h}V_{\mathcal{F}}-\widehat{\xi}_{V_{ \mathcal{F}}})^{2}\right]+3\varepsilon_{\text{stat}}\] \[\leq\sum_{h=1}^{H}\mathbb{E}_{\mathcal{D}}\left[(\mathcal{G}_{h}V _{\mathcal{F}})^{2}-(\mathcal{G}_{h}V_{\mathcal{F}}-(\mathcal{B}^{\mathcal{H} }V_{\mathcal{F}})(\tau_{h}))^{2}\right]+6\varepsilon_{\text{stat}}\] \[=6\varepsilon_{\text{stat}}.\] (15)

The last step is from that \((\mathcal{B}^{\mathcal{H}}V_{\mathcal{F}})(\tau_{h})=0\). For any \(\tau_{h}\), we observe that \(\forall V\in\mathcal{V}\) and \(h\in[H]\),

\[\mathbb{E}_{\pi_{b}}[(\mathcal{G}_{h}V)^{2}-(\mathcal{G}_{h}V-( \mathcal{B}^{\mathcal{H}}V)(\tau_{h}))^{2}]\] \[=\mathbb{E}_{\pi_{b}}[-(\mathcal{B}^{\mathcal{H}}V)(\tau_{h})^{2 }+2(\mathcal{B}^{\mathcal{H}}V)(\tau_{h})\mathcal{G}_{h}V]\] \[=\mathbb{E}_{\pi_{b}}[(\mathcal{B}^{\mathcal{H}}V)(\tau_{h})^{2}].\]

For any fixed \(V\in\mathcal{V}\), let \(Y_{h}=(\mathcal{G}_{h}V)^{2}-(\mathcal{G}_{h}V-(\mathcal{B}^{\mathcal{H}}V)( \tau_{h}))^{2}\) and \(Y=\sum_{h=1}^{H}Y_{h}\), we have \(|Y_{h}|\leq 27\bar{C}^{2}C_{\mu}\) and \(|Y|\leq 27\bar{H}\bar{C}^{2}C_{\mu}\). We observe that

\[\mathbb{E}_{\pi_{b}}[Y_{h}^{2}] =\mathbb{E}_{\pi_{b}}[(2\mathcal{G}_{h}V-(\mathcal{B}^{\mathcal{H }}V)(\tau_{h}))^{2}(\mathcal{B}^{\mathcal{H}}V)(\tau_{h})^{2}]\] \[\leq\mathbb{E}_{\pi_{b}}[(18\bar{C}^{2}+4(\mathcal{G}_{h}V)^{2}) (\mathcal{B}^{\mathcal{H}}V)(\tau_{h})^{2}]\] \[\leq\mathbb{E}_{\pi_{b}}[(26\bar{C}^{2}+32\bar{C}^{2}\mu(o_{h},a_ {h})^{2})(\mathcal{B}^{\mathcal{H}}V)(\tau_{h})^{2}]\] \[\leq 58\bar{C}^{2}C_{\mu}\mathbb{E}_{\pi_{b}}[(\mathcal{B}^{ \mathcal{H}}V)(\tau_{h})^{2}].\]

Then, for the variance of \(Y\), we have

\[\mathrm{Var}_{\pi_{b}}[Y] \leq\mathbb{E}_{\pi_{b}}[Y^{2}]\] \[\leq H\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}[Y_{h}^{2}]\] \[=58H\bar{C}^{2}C_{\mu}\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}[( \mathcal{B}^{\mathcal{H}}V)(\tau_{h})^{2}].\]

From Bernstein's inequality, with probability at least \(1-\delta/2\), \(\forall V\in\mathcal{V}\), we have

\[\left|\sum_{h=1}^{H}(\mathbb{E}_{\mathcal{D}}-\mathbb{E}_{\pi_{b} })\left[(\mathcal{G}_{h}V)^{2}-(\mathcal{G}_{h}V-(\mathcal{B}^{\mathcal{H}}V)( \tau_{h}))^{2}\right]\right|\] \[\leq\sqrt{58H\bar{C}^{2}C_{\mu}\sum_{h=1}^{H}\mathbb{E}_{\pi_{b} }[(\mathcal{B}^{\mathcal{H}}V)(\tau_{h})^{2}]\frac{\log(4|\mathcal{V}|/\delta )}{n}}+\frac{27H\bar{C}^{2}C_{\mu}\log(4|\mathcal{V}|/\delta)}{n}.\] (16)Therefore, we have

\[\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[(\mathcal{B}^{\mathcal{H}} \widehat{V})(\tau_{h})^{2}\right] =\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}[(\mathcal{G}_{h}\widehat{V})^{2 }-(\mathcal{G}_{h}\widehat{V}-(\mathcal{B}^{\mathcal{H}}\widehat{V})(\tau_{h}))^ {2}]\] \[\leq\left|\sum_{h=1}^{H}(\mathbb{E}_{\mathcal{D}}-\mathbb{E}_{\pi_{ b}})[(\mathcal{G}_{h}\widehat{V})^{2}-(\mathcal{G}_{h}\widehat{V}-(\mathcal{B}^{ \mathcal{H}}\widehat{V})(\tau_{h}))^{2}]\right|\] \[+\sum_{h=1}^{H}\mathbb{E}_{\mathcal{D}}\left[(\mathcal{G}_{h} \widehat{V})^{2}-(\mathcal{G}_{h}\widehat{V}-(\mathcal{B}^{\mathcal{H}} \widehat{V})(\tau_{h}))^{2}\right]\] \[\leq\sqrt{58H\bar{C}^{2}C_{\mu}\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}} \left[(\mathcal{B}^{\mathcal{H}}\widehat{V})(\tau_{h})^{2}\right]\frac{\log(4| \mathcal{V}|/\delta)}{n}}+\frac{27H\bar{C}^{2}C_{\mu}\log(4|\mathcal{V}|/ \delta)}{n}+6\varepsilon_{\text{stat}}.\] (From Equation (15) and Equation (16))

Solving it and we get

\[\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[(\mathcal{B}^{\mathcal{H}} \widehat{V})(\tau_{h})^{2}\right]\leq 10\varepsilon_{\text{stat}}.\]

We then invoke Lemma 1 and obtain that

\[\left|J(\pi_{e})-\mathbb{E}_{\pi_{b}}[\widehat{V}(f_{1})]\right| \leq\left|\sum_{h=1}^{H}\mathbb{E}_{s_{h}\sim d^{h}_{\pi_{e}}} \left[(\mathcal{B}^{\mathcal{S}}\widehat{V})(s_{h})\right]\right|\] \[\leq\sqrt{H\sum_{h=1}^{H}\mathbb{E}_{s_{h}\sim d^{h}_{\pi_{e}}} \left[(\mathcal{B}^{\mathcal{S}}\widehat{V})(s_{h})\right]}\] \[\leq\sqrt{H\sum_{h=1}^{H}\mathbb{E}_{s_{h}\sim d^{h}_{\pi_{e}}} \left[(\mathcal{B}^{\mathcal{S}}\widehat{V})(s_{h})^{2}\right]}\] \[\leq\sqrt{H\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[(\mathcal{B}^ {\mathcal{H}}\widehat{V})(\tau_{h})^{2}\right]}\cdot\sqrt{\frac{\sum_{h=1}^{H} \mathbb{E}_{s_{h}\sim d^{h}_{\pi_{e}}}\left[(\mathcal{B}^{\mathcal{S}} \widehat{V})(s_{h})^{2}\right]}{\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[( \mathcal{B}^{\mathcal{H}}\widehat{V})(\tau_{h})^{2}\right]}}\] \[\leq\sqrt{10H\varepsilon_{\text{stat}}}\cdot\sqrt{\frac{\sum_{h=1 }^{H}\mathbb{E}_{\pi_{e}}\left[(\mathcal{B}^{\mathcal{S}}\widehat{V})(s_{h})^ {2}\right]}{\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[(\mathcal{B}^{\mathcal{S}} \widehat{V})(s_{h})^{2}\right]}}\cdot\sqrt{\frac{\sum_{h=1}^{H}\mathbb{E}_{\pi _{b}}\left[(\mathcal{B}^{\mathcal{S}}\widehat{V})(s_{h})^{2}\right]}{\sum_{h=1 }^{H}\mathbb{E}_{\pi_{b}}\left[(\mathcal{B}^{\mathcal{H}}\widehat{V})(\tau_{h })^{2}\right]}}\] \[\leq 50H\max\{C_{\mathcal{V}}+1,C_{\Xi}\}\mathrm{IV}(\mathcal{V}) \mathrm{Dr}_{\mathcal{V}}[d^{\pi_{e}},d^{\pi_{b}}]\sqrt{\frac{C_{\mu}\log\frac{ 4|\mathcal{V}||\Xi|}{\delta}}{n}}.\]

The proof is completed by using Hoeffding's inequality to bound \(|\mathbb{E}_{\mathcal{D}}[\widehat{V}(f_{1})]-\mathbb{E}_{\pi_{b}}[\widehat{V} (f_{1})]|\). 

## Appendix D Proofs for Section 4

### Example 1

We provide a brief justification of Example 1. When \(\Pr_{\pi_{b}}(f_{h}|s_{h})\leq\frac{C_{\text{stat}}}{(OA)^{H-h+1}}\), for any \(\mathbf{u}(f_{h})\), we have

\[\|\mathbf{u}(f_{h})\|_{2}\leq\frac{C_{\text{stat}}\sqrt{S}}{(OA)^{H-h+1}}.\]

Then, for any \(\mathbf{x}\in\mathbb{R}^{S}\) such that \(\|\mathbf{x}\|_{2}=1\), we have

\[\mathbf{x}^{\top}(M_{\mathcal{F},h}M_{\mathcal{F},h}^{\top})\mathbf{x}\leq\sum _{f_{h}}\|\mathbf{x}\|_{2}^{2}\|\mathbf{u}(f_{h})\|_{2}^{2}=\frac{C_{\text{ stoch}}^{2}S}{(OA)^{H-h+1}}.\]Therefore, \(\sigma_{\min}(M_{\mathcal{F},h})\leq\sigma_{\max}(M_{\mathcal{F},h})\leq C_{\text{ stoch}}\sqrt{S}/(OA)^{(H-h+1)/2}\).

### Proof of Proposition 3

For each entry in \(\Sigma_{\mathcal{F},h}\), we can write it as

\[(\Sigma_{\mathcal{F},h})_{ij}=\sum_{k}\frac{\Pr_{\pi_{b}}(f_{h}=k\mid s_{h}=i) \Pr_{\pi_{b}}(f_{h}=k\mid s_{h}=j)}{\sum_{i^{\prime}}\Pr_{\pi_{b}}(f_{h}=k \mid s_{h}=i^{\prime})}\]

Since the probability is always non-negative, all the entries in \(\Sigma_{\mathcal{F},h}\) is non-negative. For each row \(i\), we have

\[\sum_{j}(\Sigma_{\mathcal{F},h})_{ij} =\sum_{k}\sum_{j}\frac{\Pr_{\pi_{b}}(f_{h}=k\mid s_{h}=i)\Pr_{ \pi_{b}}(f_{h}=k\mid s_{h}=j)}{\sum_{i^{\prime}}\Pr_{\pi_{b}}(f_{h}=k\mid s_{ h}=i^{\prime})}\] \[=\sum_{k}\Pr_{\pi_{b}}(f_{h}=k\mid s_{h}=i)=1.\]

Similarly, for each column \(j\), we also have \(\sum_{i}(\Sigma_{\mathcal{F},h})_{ij}=1\). Therefore, we prove that \(\Sigma_{\mathcal{F},h}\) is doubly-stochastic and we know for a stochastic matrix, the largest eigenvalue is \(1\). 

### Example 2

Due to the block structure of \(M_{\mathcal{F},h}\), \(\Sigma_{\mathcal{F},h}\) is clearly diagonal, and the \(i\)-th diagonal entry is:

\[\sum_{j:\Pr_{\pi_{b}}[f_{h}=j|s_{h}=i]>0}\frac{\Pr_{\pi_{b}}[f_{h}=j|s_{h}=i]^{ 2}}{\sum_{i^{\prime}}\Pr_{\pi_{b}}[f_{h}=j|s_{h}=i^{\prime}]}.\]

Due to the revealing property, the denominator is just \(\Pr_{\pi_{b}}[f_{h}=j|s_{h}=i]\), so the expression is just summing up \(\Pr_{\pi_{b}}[f_{h}=j|s_{h}=i]\) over \(j\), which is \(1\).

### Proof of Proposition 4

According to Equation (4), we have for any \(f_{h}\),

\[V_{\mathcal{F}}(f_{h}) =Z(f_{h})^{-1}\mathbf{u}(f_{h})^{\top}\Sigma_{\mathcal{F},h}^{-1} V_{\mathcal{S},h}^{\pi_{b}}\] \[\leq\sqrt{Z(f_{h})^{-1}\mathbf{u}(f_{h})^{\top}\Sigma_{\mathcal{F },h}^{-1}Z(f_{h})^{-1}\mathbf{u}(f_{h})}\sqrt{(V_{\mathcal{S},h}^{\pi_{a}})^{ \top}\Sigma_{\mathcal{F},h}^{-1}V_{\mathcal{S},h}^{\pi_{a}}}\] \[\leq\sqrt{C_{\mathcal{F},U}C_{\mathcal{F},V}}.\]

The last step is from Assumption 7 and Assumption 8. Therefore, \(\|V_{\mathcal{F}}\|_{\infty}\leq\sqrt{C_{\mathcal{F},U}C_{\mathcal{F},V}}=\sqrt {C_{\mathcal{F},2}}\). Moreover, we observe that

\[\|V_{\mathcal{F},h}\|_{Z_{h}}^{2} =\sum_{f_{h}}Z(f_{h})\left(Z(f_{h})^{-1}\mathbf{u}(f_{h})^{\top} \Sigma_{\mathcal{F},h}^{-1}V_{\mathcal{S}}^{\pi_{c}}\right)^{2}\] \[=\sum_{f_{h}}Z(f_{h})^{-1}(V_{\mathcal{S}}^{\pi_{a}})^{\top} \Sigma_{\mathcal{F},h}^{-1}\mathbf{u}(f_{h})\mathbf{u}(f_{h})^{\top}\Sigma_{ \mathcal{F},h}^{-1}V_{\mathcal{S}}^{\pi_{e}}\] \[=(V_{\mathcal{S}}^{\pi_{e}})^{\top}\Sigma_{\mathcal{F},h}^{-1} \left(\sum_{f_{h}}Z(f_{h})^{-1}\mathbf{u}(f_{h})\mathbf{u}(f_{h})^{\top} \right)\Sigma_{\mathcal{F},h}^{-1}V_{\mathcal{S}}^{\pi_{e}}\] \[=(V_{\mathcal{S}}^{\pi_{e}})^{\top}\Sigma_{\mathcal{F},h}^{-1} \Sigma_{\mathcal{F},h}\Sigma_{\mathcal{F},h}^{-1}V_{\mathcal{S}}^{\pi_{e}}\leq C _{\mathcal{F},V}.\]

The last step is from Assumption 7. 

### Example 3

We give the calculation for Example 3. When \(\pi_{e}=\pi_{b}\), \(V^{\pi_{e}}_{\mathcal{S},h}=M_{\mathcal{F},h}R^{+}_{h}\), so

\[\|V^{\pi_{e}}_{S,h}\|^{2}_{\Sigma^{-1}_{\mathcal{F},h}}=(R^{+}_{h})^ {\top}M^{\top}_{\mathcal{F},h}\Sigma^{-1}_{\mathcal{F},h}M_{\mathcal{F},h}R^{+}_ {h}\] \[=(R^{+}_{h})^{\top}Z^{1/2}_{h}Z^{-1/2}_{h}M^{\top}_{\mathcal{F},h}( M_{\mathcal{F},h}Z^{-1}_{h}M^{\top}_{\mathcal{F},h})M_{\mathcal{F},h}Z^{-1/2}_{h}Z ^{1/2}_{h}R^{+}_{h}\] \[\leq(R^{+}_{h})^{\top}Z^{1/2}_{h}Z^{1/2}_{h}R^{+}_{h},\]

where the last step follows from the fact that \(Z^{-1/2}_{h}M^{\top}_{\mathcal{F},h}(M_{\mathcal{F},h}Z^{-1}_{h}M^{\top}_{ \mathcal{F},h})M_{\mathcal{F},h}Z^{-1/2}_{h}\) is a projection matrix (\(P^{2}=P\)) and is dominated by identity in eigenvalues (\(P\preceq I\)). Now, recall that \(Z_{h}/S\) is a proper distribution, so

\[(R^{+}_{h})^{\top}Z_{h}R^{+}_{h}=S\cdot\mathbb{E}_{Z_{h}/S}[(R^{+}_{h})^{2}] \leq SH^{2}.\]

## Appendix E Proofs for Section 5

### Proof of Lemma 6

We first verify that \(w^{\star}\) in Eq. (8) satisfies Eq. (7) as follows.

\[\mathbb{E}_{\pi_{b}}[w^{\star}(\tau_{h})\mathbf{b}(\tau_{h})] =\sum_{\tau_{h}}d^{\pi_{b}}_{h}(\tau_{h})w^{\star}(\tau_{h}) \mathbf{b}(\tau_{h})\] \[=\sum_{\tau_{h}}d^{\pi_{b}}_{h}(\tau_{h})\mathbf{b}(\tau_{h}) \mathbf{b}(\tau_{h})^{\top}\Sigma^{-1}_{\mathcal{H},h}\mathbf{b}^{\pi_{e}}_{h}\] \[=\left(\sum_{\tau_{h}}d^{\pi_{b}}_{h}(\tau_{h})\mathbf{b}(\tau_{h })\mathbf{b}(\tau_{h})^{\top}\right)\Sigma^{-1}_{\mathcal{H},h}\mathbf{b}^{ \pi_{e}}_{h}\] \[=\mathbf{b}^{\pi_{e}}_{h}.\]

We then show that

\[\|w^{\star}\|^{2}_{2,d^{\pi_{b}}_{h}} =\sum_{\tau_{h}}d^{\pi_{b}}_{h}(\tau_{h})\left\{\mathbf{b}(\tau_{ h})^{\top}\Sigma^{-1}_{\mathcal{H},h}\left(\mathbf{b}^{\pi_{e}}_{h}\right) \right\}^{2}\] \[=(\mathbf{b}^{\pi_{e}}_{h})^{\top}\Sigma^{-1}_{\mathcal{H},h}\left( \sum_{\tau_{h}}d^{\pi_{b}}_{h}(\tau_{h})\mathbf{b}(\tau_{h})\mathbf{b}(\tau_{h })^{\top}\right)\Sigma^{-1}_{\mathcal{H},h}\mathbf{b}^{\pi_{e}}_{h}\] \[=(\mathbf{b}^{\pi_{e}}_{h})^{\top}\Sigma^{-1}_{\mathcal{H},h} \Sigma_{\mathcal{H},h}\Sigma^{-1}_{\mathcal{H},h}\mathbf{b}^{\pi_{e}}_{h}\] \[=(\mathbf{b}^{\pi_{e}}_{h})^{\top}\Sigma^{-1}_{\mathcal{H},h} \mathbf{b}^{\pi_{e}}_{h}\leq C_{\mathcal{H},2}.\]

The last step follows from Assumption 11. 

### Example 5

Consider the lemma: given vector \(x\) and PD matrix \(\Sigma\), if \(\Sigma\succeq xx^{\top}\), then \(x^{\top}\Sigma^{-1}x\leq 1\). The calculation in the example directly follows from letting \(x=\mathbf{b}^{\pi_{b}}_{h}\), \(\Sigma=\Sigma_{\mathcal{H},h}\), and the condition \(\Sigma\succeq xx^{\top}\) is satisfied due to Jensen's inequality.

To prove the lemma, note that \(\Sigma-xx^{\top}\) is PSD, so

\[(\Sigma^{-1}x)^{\top}(\Sigma-xx^{\top})(\Sigma^{-1}x)\geq 0.\]

This implies \(x^{\top}\Sigma^{-1}x\geq x^{\top}\Sigma^{-1}xx^{\top}\Sigma^{-1}x\), so \(x^{\top}\Sigma^{-1}x\leq 1\).

### Comparison between belief coverage and latent state coverage

Here we show that belief coverage is stronger than latent state coverage. More concretely, consider a standard measure of latent state coverage, the 2nd moment of state density ratio (c.f. discussion below Example 7):

\[\mathbb{E}_{\pi_{b}}[(d^{\pi_{e}}(s_{h})/d^{\pi_{b}}(s_{h}))^{2}]=(\mathbf{b}^{ \pi_{e}}_{h})^{\top}\text{diag}(\mathbb{E}_{\pi_{b}}[\mathbf{b}(\tau_{h})])^{-1 }\mathbf{b}^{\pi_{e}}_{h}.\]In comparison, our belief coverage parameter from Assumption 11 is

\[(\mathbf{b}_{h}^{\pi_{e}})^{\top}\Sigma_{\mathcal{H},h}^{-1}\mathbf{b}_{h}^{\pi_{ e}}=(\mathbf{b}_{h}^{\pi_{e}})^{\top}\mathbb{E}_{\pi_{b}}[\mathbf{b}(\tau_{h}) \mathbf{b}(\tau_{h})^{\top}]^{-1}\mathbf{b}_{h}^{\pi_{e}}.\]

To show the former is smaller than the latter, it suffices to show that

\[\text{diag}(\mathbb{E}_{\pi_{b}}[\mathbf{b}(\tau_{h})])^{-1}\preceq(\mathbb{E }_{\pi_{b}}[\mathbf{b}(\tau_{h})\mathbf{b}(\tau_{h})^{\top}])^{-1},\]

which is implied by \(\text{diag}(\mathbb{E}_{\pi_{b}}[\mathbf{b}(\tau_{h})])\succeq\mathbb{E}_{\pi _{b}}[\mathbf{b}(\tau_{h})\mathbf{b}(\tau_{h})^{\top}]\). It therefore suffices to show that \(\text{diag}(\mathbf{b}(\tau_{h}))\succeq\mathbf{b}(\tau_{h})\mathbf{b}(\tau_ {h})^{\top}\) holds in a pointwise manner for all \(\tau_{h}\). To show this, we temporarily let \(\mathbf{b}=\mathbf{b}(\tau_{h})\) in the calculation: consider an arbitrary vector \(v\in\mathbb{R}^{\mathcal{S}}\), then

\[v^{\top}(\text{diag}(\mathbf{b})-\mathbf{b}\mathbf{b}^{\top})v=\mathbb{E}_{ \pi_{b}\succeq\mathbf{b}}[v(s)^{2}]-(\mathbb{E}_{\pi\sim\mathbf{b}}[v(s)])^{2 }\geq 0.\]

The last step is due to Jensen's inequality.

### Comparison to \(\text{IV}(\mathcal{V})\)

We now compare to the \(\text{IV}(\mathcal{V})\) and \(\text{Dr}_{\mathcal{V}}\) terms in Theorem 2. Belief coverage is generally stronger than latent state coverage which corresponds to the \(\text{Dr}_{\mathcal{V}}\) term (see Appendix E.3). That said, perhaps surprisingly, the remaining \(\text{IV}(\mathcal{V})\) term _must_ scale with \(\frac{1}{\sigma_{\min}(\Sigma_{\mathcal{H},h})}\) under moderate assumptions.

**Proposition 10**.: _Suppose \(\mathbf{v}_{\min}\) is the eigenvector corresponding to \(\sigma_{\min}(\Sigma_{\mathcal{H},h})\), the smallest eigenvalue for some \(\Sigma_{\mathcal{H},h}\). Then, if \(c_{0}\mathbf{v}_{\min}\in\mathcal{B}_{h}^{\mathcal{S}}\mathcal{V}:=\{ \mathcal{B}_{h}^{\mathcal{S}}V:V\in\mathcal{V}\}\) for some non-zero \(c_{0}\), \(\text{IV}(\mathcal{V})\geq\sqrt{\frac{\min_{c_{0}}d^{\pi_{b}}(s_{h})}{\sigma_ {\min}(\Sigma_{\mathcal{H},h})}}\)._

The proposition states that as long as \(\min_{s_{h}}d^{\pi_{b}}(s_{h})\) is bounded away from \(0\) (which is benign and helps bound the \(\text{Dr}_{\mathcal{V}}[d^{\pi_{e}},d^{\pi_{b}}]\) term) and \(\mathcal{V}\) is sufficiently rich that \(\mathcal{B}_{h}^{\mathcal{S}}\mathcal{V}\) includes a certain vector in \(\mathbb{R}^{\mathcal{S}}\), then boundedness of \(\text{IV}(\mathcal{V})\)_requires_ bounded \(1/\sigma_{\min}(\Sigma_{\mathcal{H},h})\), which is stronger than our belief coverage assumption. This renders the split between \(\text{IV}(\mathcal{V})\) and \(\text{Dr}_{\mathcal{V}}[d^{\pi_{e}},d^{\pi_{b}}]\) superficial and perhaps unnecessary.

Proof of Proposition 10.: Since \(c_{0}\mathbf{v}_{\min}\in\{\mathcal{B}_{h}^{\mathcal{S}}V:V\in\mathcal{V}\}\), there exists \(V\) such that \(\mathcal{B}_{h}^{\mathcal{S}}V=c_{0}\mathbf{v}_{\min}\). For such \(V\), we observe that

\[\mathbb{E}_{\pi_{b}}\left[\left(\mathcal{B}^{\mathcal{H}}V\right)(\tau_{h})^{2 }\right]=\sum_{\tau_{h}}d^{\pi_{b}}(\tau_{h})\left\langle\mathbf{b}(\tau_{h}),c _{0}\mathbf{v}_{\min}\right\rangle^{2}=c_{0}^{2}\mathbf{v}_{\min}^{\top}\Sigma_ {\mathcal{H},h}\mathbf{v}_{\min}=\sigma_{\min}(\Sigma_{\mathcal{H},h})c_{0}^{2}.\]

and

\[\mathbb{E}_{\pi_{b}}\left[\left(\mathcal{B}^{\mathcal{S}}V\right)(s_{h})^{2 }\right]\geq\min_{s_{h}}d^{\pi_{b}}(s_{h})\cdot\|c_{0}\mathbf{v}_{\min}\|_{2}^{ 2}=\min_{s_{h}}d^{\pi_{b}}(s_{h})\cdot c_{0}^{2}.\]

Therefore

\[\sqrt{\frac{\mathbb{E}_{\pi_{b}}\left[\left(\mathcal{B}^{\mathcal{S}}V\right)( s_{h})^{2}\right]}{\mathbb{E}_{\pi_{b}}\left[\left(\mathcal{B}^{\mathcal{H}}V \right)(\tau_{h})^{2}\right]}}\geq\sqrt{\frac{\min_{s_{h}}d^{\pi_{b}}(s_{h}) \cdot c_{0}^{2}}{\sigma_{\min}(\Sigma_{\mathcal{H},h})c_{0}^{2}}}=\sqrt{\frac {\min_{s_{h}}d^{\pi_{b}}(s_{h})}{\sigma_{\min}(\Sigma_{\mathcal{H},h})}}.\]

### Example 6

It suffices to show

\[\Sigma_{\mathcal{H},h}\mathbf{1}=\mathbb{E}_{\pi_{b}}[\mathbf{b}(\tau_{h}) \mathbf{b}(\tau_{h})^{\top}]\mathbf{1}=\mathbb{E}_{\pi_{b}}[\mathbf{b}(\tau_{h} )\mathbf{b}(\tau_{h})^{\top}\mathbf{1}]=\mathbb{E}_{\pi_{b}}[\mathbf{b}(\tau_ {h})]=\mathbf{b}_{h}^{\pi_{b}}.\]

### Proof of Lemma 8

The key is to notice that \((\mathbf{b}_{h}^{\pi_{e}})^{\top}\) is non-negative, so

\[(\mathbf{b}_{h}^{\pi_{e}})^{\top}\Sigma_{\mathcal{H},h}^{-1} \mathbf{b}_{h}^{\pi_{e}}\leq(\mathbf{b}_{h}^{\pi_{e}})^{\top}|\Sigma_{ \mathcal{H},h}^{-1}(\mathbf{b}_{h}^{\pi_{e}})|\] ( \[|\cdot|\] is pointwise absolute value) \[\leq(\mathbf{b}_{h}^{\pi_{e}})^{\top}\mathbf{1}\cdot\|\Sigma_{ \mathcal{H},h}^{-1}(\mathbf{b}_{h}^{\pi_{e}})\|_{\infty}\] (using the non-negativity of \[(\mathbf{b}_{h}^{\pi_{e}})^{\top}\] again) \[=\|\Sigma_{\mathcal{H},h}^{-1}(\mathbf{b}_{h}^{\pi_{e}})\|_{\infty}.\]

### Proof of Lemma 5

According to Eq. (5), we use \(L_{1}/L_{\infty}\) Holder's inequality and obtain that for any \(f_{h}\)

\[|V_{\mathcal{F}}(f_{h})| \leq\left\|\frac{\mathbf{u}(f_{h})}{Z^{R}(f_{h})}\right\|_{1}\left\| (\Sigma^{R}_{\mathcal{F},h})^{-1}V^{\pi_{e}}_{\mathcal{S}}\right\|_{\infty}\] \[=R^{+}(f_{h})\left\|\frac{\mathbf{u}(f_{h})}{Z(f_{h})}\right\|_{1 }\left\|(\Sigma^{R}_{\mathcal{F},h})^{-1}V^{\pi_{e}}_{\mathcal{S}}\right\|_{\infty}\] \[\leq R^{+}(f_{h})C_{\mathcal{F},\infty}\] \[\leq HC_{\mathcal{F},\infty}.\]

The second inequality is from Assumption 9 and \(\frac{\mathbf{u}(f_{h})}{Z(f_{h})}\) is a stochastic vector. The last step is from the boundedness of the reward. 

### Example 4

We use \(\text{diag}(\cdot)\) both for creating a diagonal matrix with an input vector and for taking the diagonal vector out of a matrix.

\[\Sigma^{R}_{\mathcal{F},h}\mathbf{1}=M_{\mathcal{F},h}(Z^{R}_{h})^{-1}M^{ \top}_{\mathcal{F},h}\mathbf{1}=M_{\mathcal{F},h}(Z^{R}_{h})^{-1}\text{diag}( Z_{h})^{\top}=M_{\mathcal{F},h}R^{+}_{h}=V^{\pi_{e}}_{\mathcal{S}}.\]

### Example 9

The calculation is similar to that of Example 2 in Appendix D.3, except that the numerator has an extra \(R^{+}(f_{h})\). Therefore, when calculating the \(i\)-th diagonal entry of \(\Sigma^{R}_{\mathcal{F},h}\), the final sum is calculating the expectation of \(R^{+}(f_{h})\) conditioned on \(s_{h}=i\) under policy \(\pi_{b}\), which is the definition of \(V^{\pi_{b}}_{\mathcal{S}}(i)\).

### Proof of Theorem 7

In the proof of Theorem 2, we have

\[\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[(\mathcal{B}^{\mathcal{H}}\widehat{V}) (\tau_{h})^{2}\right] \leq 10\varepsilon_{\text{stat}},\quad\varepsilon_{\text{stat}}:= \frac{225H\bar{C}^{2}\log(4|\mathcal{V}||\Xi|/\delta)}{n}.\]

We observe that

\[\left|\mathbb{E}_{\pi_{c}}\left[(\mathcal{B}^{\mathcal{S}}\widehat {V})(s_{h})\right]\right| =\left|\mathbb{E}_{\pi_{h}\sim d^{\pi_{c}}_{h}}\mathbb{E}_{s_{h} \sim\mathbf{b}(\tau_{h})}\left[(\mathcal{B}^{\mathcal{S}}\widehat{V})(s_{h}) \right]\right|\] \[=\left|\mathbb{E}_{\pi_{c}}\left[(\mathcal{B}^{\mathcal{H}} \widehat{V})(\tau_{h})\right]\right|\] \[=\left|\mathbb{E}_{\pi_{b}}\left[w^{\star}(\tau_{h})(\mathcal{B}^ {\mathcal{H}}\widehat{V})(\tau_{h})\right]\right|\] \[\leq\|w^{\star}\|_{2,d^{\pi_{b}}_{h}}\sqrt{\mathbb{E}_{\pi_{b}} \left[(\mathcal{B}^{\mathcal{H}}\widehat{V})(\tau_{h})^{2}\right]}\] \[\leq\sqrt{C_{\mathcal{H},2}\mathbb{E}_{\pi_{b}}\left[(\mathcal{B} ^{\mathcal{H}}\widehat{V})(\tau_{h})^{2}\right]}\]

The third equality is from Definition 10, the first inequality is from Cauchy-Schwarz inequality and the last inequality is from Lemma 6. Then, we have

\[\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{c}}\left[(\mathcal{B}^{ \mathcal{S}}\widehat{V})(s_{h})\right]\right| \leq\sqrt{H\sum_{h=1}^{H}\left(\mathbb{E}_{\pi_{c}}\left[(\mathcal{ B}^{\mathcal{S}}\widehat{V})(s_{h})\right]\right)^{2}}\] \[\leq\sqrt{10HC_{\mathcal{H},2}\varepsilon_{\text{stat}}}\] \[\leq 50H\bar{C}\sqrt{\frac{C_{\mathcal{H},2}C_{\mu}\log(4| \mathcal{V}||\Xi|/\delta)}{n}}\] \[\leq cH^{2}(C_{\mathcal{F},\infty}+1)\sqrt{\frac{C_{\mathcal{H}, 2}C_{\mu}\log(4|\mathcal{V}||\Xi|/\delta)}{n}}.\]

The last step is from Lemma 5, \(C_{\mathcal{V}}\leq c\|V_{\mathcal{F}}\|_{\infty}\) and \(C_{\Xi}\leq c(\|V_{\mathcal{F}}\|_{\infty}+1)\). The proof is completed after invoking Lemma 1.

### Proof of Theorem 9

Finally, we prove Theorem 9. The proof uses the similar idea from Xie and Jiang (2020). For any \(V\in\mathcal{V}\) and \(w\in\mathcal{W}\), we define the population loss estimator \(\mathcal{L}_{d^{\pi_{b}}}\) and the empirical loss estimator \(\mathcal{L}_{\mathcal{D}}\) as follows

\[\mathcal{L}_{d^{\pi_{b}}}(V,w) :=\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[w(\tau_{h})(\mathcal{B}^ {\mathcal{H}}V)(\tau_{h})\right]\] \[\mathcal{L}_{\mathcal{D}}(V,w) :=\sum_{h=1}^{H}\mathbb{E}_{\mathcal{D}}\left[w(\tau_{h})\left( \mu(o_{h},a_{h})\left(r_{h}+V(f_{h+1})\right)-V(f_{h})\right)\right].\]

We then invoke Lemma 1 and obtain that

\[\left|J(\pi_{e})-\mathbb{E}_{\pi_{b}}[\widehat{V}(f_{1})]\right|=\left|\sum_{ h=1}^{H}\mathbb{E}_{s_{h}\sim d^{\pi_{e}}}\left[(\mathcal{B}^{\mathcal{S}} \widehat{V})(s_{h})\right]\right|.\]

We observe that

\[\mathbb{E}_{s_{h}\sim d^{\pi_{e}}}\left[(\mathcal{B}^{\mathcal{S} }\widehat{V})(s_{h})\right] =\mathbb{E}_{\tau_{h}\sim d^{\pi_{e}}}\left[\mathbb{E}_{s_{h}\sim \mathbf{b}(\tau_{h})}\left[(\mathcal{B}^{\mathcal{S}}\widehat{V})(s_{h}) \right]\right]\] \[=\mathbb{E}_{\tau_{h}\sim d^{\pi_{e}}}\left[(\mathcal{B}^{ \mathcal{H}}\widehat{V})(\tau_{h})\right]\] \[=\mathbb{E}_{\pi_{b}}\left[w^{\star}(\tau_{h})(\mathcal{B}^{ \mathcal{H}}\widehat{V})(\tau_{h})\right],\]

where the last step is from Definition 10. Therefore, our goal is to bound \(\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[w^{\star}(\tau_{h})(\mathcal{B}^ {\mathcal{H}}\widehat{V})(\tau_{h})\right]\right|=\left|\mathcal{L}_{d^{\pi_{b} }}(\widehat{V},w^{\star})\right|\). Let

\[\widehat{w} \coloneqq\operatorname*{argmin}_{w\in\operatorname*{sp}(\mathcal{ W})}\max_{V\in\mathcal{V}}\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[(w^{ \star}(\tau_{h})-w(\tau_{h}))\cdot(\mathcal{B}^{\mathcal{H}}V)(\tau_{h}) \right]\right|,\] \[\widehat{V} \coloneqq\operatorname*{argmin}_{V\in\mathcal{V}}\sup_{w\in \operatorname*{sp}(\mathcal{W})}\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[w (\tau_{h})\cdot(\mathcal{B}^{\mathcal{H}}V)(\tau_{h})\right]\right|.\]

Then, we subtract the approximation error of \(\widehat{w}\) from our objective,

\[\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[w^{\star}(\tau_{h} )(\mathcal{B}^{\mathcal{H}}\widehat{V})(\tau_{h})\right]\right| =\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[(w^{\star}(\tau_{ h})-\widehat{w}(\tau_{h}))\cdot(\mathcal{B}^{\mathcal{H}}\widehat{V})(\tau_{h}) \right]+\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[\widehat{w}(\tau_{h})\cdot( \mathcal{B}^{\mathcal{H}}\widehat{V})(\tau_{h})\right]\right|\] \[\leq\epsilon_{\mathcal{W}}+\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{b }}\left[\widehat{w}(\tau_{h})\cdot(\mathcal{B}^{\mathcal{H}}\widehat{V})(\tau _{h})\right]\right|.\]

Next, we consider the approximation error of \(\widetilde{V}\) and obtain that

\[\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[\widehat{w}(\tau_{h})\cdot( \mathcal{B}^{\mathcal{H}}\widehat{V})(\tau_{h})\right]\right|=\epsilon_{ \mathcal{V}}+\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[\widehat{w}(\tau_{h })\cdot(\mathcal{B}^{\mathcal{H}}\widehat{V})(\tau_{h})\right]\right|-\sup_{w \in\operatorname*{sp}(\mathcal{W})}\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}} \left[w(\tau_{h})\cdot(\mathcal{B}^{\mathcal{H}}\widetilde{V})(\tau_{h}) \right]\right|.\]We then connect \(\mathcal{L}_{d^{\pi_{b}}}(V,w)\) with \(\mathcal{L}_{\mathcal{D}}(V,w)\) as follows,

\[\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[\widehat{w}(\tau_{h}) \cdot(\mathcal{B}^{\mathcal{H}}\widehat{V})(\tau_{h})\right]\right|-\sup_{w\in \mathcal{W}(\mathcal{W})}\left|\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[w(\tau_{ h})\cdot(\mathcal{B}^{\mathcal{H}}\widetilde{V})(\tau_{h})\right]\right|\] \[\leq\sup_{w\in\mathcal{W}(\mathcal{W})}\left|\sum_{h=1}^{H} \mathbb{E}_{\pi_{b}}\left[w(\tau_{h})\cdot(\mathcal{B}^{\mathcal{H}}\widehat{V })(\tau_{h})\right]\right|-\sup_{w\in\mathcal{W}(\mathcal{W})}\left|\sum_{h=1} ^{H}\mathbb{E}_{\pi_{b}}\left[w(\tau_{h})\cdot(\mathcal{B}^{\mathcal{H}} \widetilde{V})(\tau_{h})\right]\right|\] \[=\max_{w\in\mathcal{W}}\left|\mathcal{L}_{d^{\pi_{b}}}(\widehat{ V},w)\right|-\max_{w\in\mathcal{W}}\left|\mathcal{L}_{d^{\pi_{b}}}(\widetilde{V},w)\right|\] \[=\max_{w\in\mathcal{W}}\left|\mathcal{L}_{d^{\pi_{b}}}(\widehat{ V},w)\right|-\max_{w\in\mathcal{W}}\left|\mathcal{L}_{\mathcal{D}}(\widehat{V},w) \right|+\max_{w\in\mathcal{W}}\left|\mathcal{L}_{\mathcal{D}}(\widehat{V},w) \right|-\max_{w\in\mathcal{W}}\left|\mathcal{L}_{d^{\pi_{b}}}(\widetilde{V},w)\right|\] \[\leq\max_{w\in\mathcal{W}}\left|\mathcal{L}_{d^{\pi_{b}}}( \widehat{V},w)\right|-\max_{w\in\mathcal{W}}\left|\mathcal{L}_{\mathcal{D}}( \widehat{V},w)\right|+\max_{w\in\mathcal{W}}\left|\mathcal{L}_{\mathcal{D}}( \widetilde{V},w)\right|-\max_{w\in\mathcal{W}}\left|\mathcal{L}_{d^{\pi_{b}}}( \widetilde{V},w)\right|\] \[\leq\max_{w\in\mathcal{W}}\left|\mathcal{L}_{d^{\pi_{b}}}( \widehat{V},w)-\mathcal{L}_{\mathcal{D}}(\widehat{V},w)\right|+\max_{w\in \mathcal{W}}\left|\mathcal{L}_{d^{\pi_{b}}}(\widetilde{V},w)-\mathcal{L}_{ \mathcal{D}}(\widetilde{V},w)\right|.\]

The first equality is from that \(\sup_{w\in\text{sp}(\mathcal{W})}|f(\cdot)|=\max_{w\in\mathcal{W}}|f(\cdot)|\) for any linear function \(f(\cdot)\). The second inequality is from the optimality of \(\widehat{V}\).

For any fixed \(V,w\), let random variable \(X=\sum_{h=1}^{H}w(\tau_{h})\left(\mu(o_{h},a_{h})(r_{h}+V(f_{h}))-V(f_{h+1})\right)\), \(\mathbb{E}_{\pi_{b}}[X]=\mathcal{L}_{d^{\pi_{b}}}(V,w)\). Recall that \(C_{\mathcal{V}}:=\max_{V\in\mathcal{V}}\|V\|_{\infty}\) and \(C_{\mathcal{W}}:=\max_{h}\sup_{w\in\mathcal{W}}\|w\|_{\infty}\). For \(|X|\), we have \(|X|\leq 2HC_{\mu}(1+C_{\mathcal{V}})C_{\mathcal{W}}\). For the variance, we have

\[\operatorname{Var}_{\pi_{b}}[X]\] \[\leq H\sum_{h=1}^{H}\mathbb{E}_{\pi_{b}}\left[w(\tau_{h})^{2}(2C _{\mathcal{V}}^{2}+2(1+C_{\mathcal{V}})^{2}\mu(o_{h},a_{h})^{2})\right]\] \[\leq 4H^{2}(1+C_{\mathcal{V}})^{2}C_{\mu}C_{\mathcal{W}}^{2}.\]

From Bernstein's inequality, with probability at least \(1-\delta\), we have

\[|\mathcal{L}_{d^{\pi_{b}}}(V,w)-\mathcal{L}_{\mathcal{D}}(V,w)|\leq 2H(1+C_{ \mathcal{V}})C_{\mathcal{W}}\sqrt{\frac{C_{\mu}\log\frac{2}{\delta}}{n}}+ \frac{2H(1+C_{\mathcal{V}})C_{\mathcal{W}}C_{\mu}\log\frac{2}{\delta}}{n}.\]

Taking the union bound and we obtain

\[\max_{w\in\mathcal{W}}\left|\mathcal{L}_{d^{\pi_{b}}}(\widehat{V},w)-\mathcal{L}_{\mathcal{D}}(\widehat{V},w)\right|+\max_{w\in\mathcal{W}} \left|\mathcal{L}_{d^{\pi_{b}}}(\widetilde{V},w)-\mathcal{L}_{\mathcal{D}}( \widetilde{V},w)\right|\] \[\leq 4H(1+C_{\mathcal{V}})C_{\mathcal{W}}\sqrt{\frac{C_{\mu}\log \frac{2|\mathcal{V}||\mathcal{W}|}{\delta}}{n}}+\frac{4H(1+C_{\mathcal{V}})C_{ \mathcal{W}}C_{\mu}\log\frac{2|\mathcal{V}||\mathcal{W}|}{\delta}}{n}.\]

According to Lemma 5, Assumption 12, \(C_{\mathcal{V}}\leq c\|V_{\mathcal{F}}\|_{\infty}\) and \(C_{\mathcal{W}}\leq c\|w^{\star}\|_{\infty}\), we further have

\[\left|J(\pi_{e})-\mathbb{E}_{\pi_{b}}[\widehat{V}(f_{1})]\right|\leq\epsilon_{ \mathcal{V}}+\epsilon_{\mathcal{W}}+cH^{2}C_{\mathcal{H},\infty}(C_{\mathcal{F},\infty}+1)\sqrt{\frac{C_{\mu}\log\frac{2|\mathcal{V}||\mathcal{W}|}{\delta}} {n}}+\frac{cH^{2}C_{\mathcal{H},\infty}(C_{\mathcal{F},\infty}+1)C_{\mu}\log \frac{2|\mathcal{V}||\mathcal{W}|}{\delta}}{n}.\]

The proof is completed by using Hoeffding's inequality to bound \(|\mathbb{E}_{\mathcal{D}}[\widehat{V}(f_{1})]-\mathbb{E}_{\pi_{b}}[\widehat{V}(f_ {1})]|\). \(\qed\)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provide the full set of assumptions in the main text and a complete proof in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: No experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [NA] Justification: No experiments. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: No experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: No experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: No experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conform with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper presents work whose goal is to advance the field of learning theory and has no societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.