# Latent Graph Inference with Limited Supervision

 Jianglin Lu\({}^{1}\)  Yi Xu\({}^{1}\)  Huan Wang\({}^{1}\)  Yue Bai\({}^{1}\)  Yun Fu\({}^{1,2}\)

\({}^{1}\)Department of Electrical and Computer Engineering, Northeastern University

\({}^{2}\)Khoury College of Computer Science, Northeastern University

Project Page: https://jianglin954.github.io/LGI-LS/

Corresponding author: JianglinLu@outlook.com.

###### Abstract

Latent graph inference (LGI) aims to jointly learn the underlying graph structure and node representations from data features. However, existing LGI methods commonly suffer from the issue of _supervision starvation_, where massive edge weights are learned without semantic supervision and do not contribute to the training loss. Consequently, these supervision-starved weights, which may determine the predictions of testing samples, cannot be semantically optimal, resulting in poor generalization. In this paper, we observe that this issue is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones. To address this, we propose to _restore the corrupted affinities and replenish the missed supervision for better LGI_. The key challenge then lies in identifying the critical nodes and recovering the corrupted affinities. We begin by defining the pivotal nodes as _\(k\)-hop starved nodes_, which can be identified based on a given adjacency matrix. Considering the high computational burden, we further present a more efficient alternative inspired by _CUR matrix decomposition_. Subsequently, we eliminate the starved nodes by reconstructing the destroyed connections. Extensive experiments on representative benchmarks demonstrate that reducing the starved nodes consistently improves the performance of state-of-the-art LGI methods, especially under extremely limited supervision (6.12% improvement on Pubmed with a labeling rate of only 0.3%).

## 1 Introduction

Graph neural networks (GNNs) [8, 12, 23, 33] have recently received considerable attention due to their strong ability to handle complex graph-structured data. GNNs consider each data sample as a node and model the affinities between nodes as the weights of edges. The edges as a whole constitute the graph structure or topology of the data. By integrating the graph topology into the training process of representation learning, GNNs have achieved remarkable performance across a wide range of tasks, such as classification [19, 37], clustering [31, 40], retrieval [5, 43], and recognition [35, 44].

Although effective, existing GNNs typically require a prior graph to learn node representations, which poses a major challenge when encountering incomplete or even missing graphs. This limitation has spurred the development of latent graph inference (LGI) [7, 10, 17, 22, 32], also known as graph structure learning [9, 24, 39, 42]. LGI aims to jointly learn the underlying graph and discriminative node representations solely from the features of nodes in an end-to-end fashion. By adaptively learning the graph topology, LGI models are empowered with great ability to remove noise and capture more complex structure of the data [13, 26, 47, 27]. Consequently, LGI emerges as a promising research topic with a broad range of applications, such as point cloud segmentation [41], disease prediction [6], multi-view clustering [31], and brain connectome representation [18].

However, many LGI methods suffer from a so-called _supervision starvation_ (SS) problem [10], where a number of edge weights are learned without any semantic supervision during the graph inference stage. Specifically, given a \(k\)-layer GNN, if a node and all of its predefined neighbors (from \(1\)- to \(k\)-hop) are unlabeled, the edge weights between this node and its neighbors will not contribute to the training loss and cannot be semantically optimal after training. This will lead to poor generalization performance since these under-trained weights are inevitably used to make predictions for testing samples. In this paper, we discover that the SS problem is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones. Based on this observation, we propose to _restore the destroyed connections and replenish the missed supervision for better LGI_.

Specifically, we first define the pivotal nodes as _\(k\)-hop starved nodes_. Then, the SS problem can be transformed into a more manageable task of eliminating starved nodes. We propose to identify the \(k\)-hop starved nodes based on the \(k\)-th power of a given adjacency matrix. After identification, we can diminish the starved nodes by incorporating a regularization adjacency matrix into the initial one. However, the above identification approach suffers from high computational complexity due to matrix multiplication. For example, given a \(2\)-layer GNN, identifying \(2\)-hop starved nodes requires at least \(\mathcal{O}(n^{3})\), where \(n\) denotes the number of nodes. To address this, we present a more efficient alternative solution inspired by _CUR matrix decomposition_[2, 3]. We find that with appropriate column and row selection, the \(U\) matrix obtained through CUR decomposition of the initial adjacency matrix is actually a zero matrix. Thus, we can eliminate the starved nodes by reconstructing the \(U\) matrix.

Although recovering the \(U\) matrix encourages more supervision, such strategy may cause a potential issue we call _weight contribution rate decay_. In other words, the weight contribution rate (WCR) of non-starved nodes decays as the number of starved nodes increases, resulting in a final latent graph that heavily relies on the regularization one. To tackle this issue, we propose two simple strategies, _i.e._, decreasing the WCR of starved nodes and increasing the WCR of non-starved nodes (see Sec. 3.3 for details). The main contributions of this paper can be summarized as follows:

* We identify that graph sparsification is the main cause of the supervision starvation (SS) problem in latent graph inference (LGI). Therefore, we propose to restore the affinities corrupted by the sparsification operation to replenish the missed supervision for better LGI.
* By defining \(k\)-hop starved nodes, we transform the SS issue into a more manageable task of removing starved nodes. Inspired by CUR decomposition, we propose a simple yet effective solution to determine the starved nodes and diminish them using a regularization graph.
* The proposed approach is model-agnostic and can be seamlessly integrated into various LGI models. Extensive experiments demonstrate that eliminating the starved nodes consistently enhances the state-of-the-art LGI methods, particularly under extremely limited supervision.

## 2 Preliminaries

### Notations

Let \(\mathbf{A}_{i:}\), \(\mathbf{A}_{:j}\), \(\mathbf{A}_{ij}\), and \(\mathbf{A}^{k}\) denote the \(i\)-th row, the \(j\)-th column, the element at the \(i\)-th row and \(j\)-th column, and the \(k\)-th power of the matrix \(\mathbf{A}\), respectively. Let \(\mathbf{1}_{n}\) represent an \(n\)-dimensional column vector with all elements being \(1\). Let \(\mathbbm{1}_{\mathbb{R}^{+}}(\mathbf{A})\) be an element-wise indicator function that sets \(\mathbf{A}_{ij}\) to \(1\) if it is positive, and \(0\) otherwise. Conversely, \(\mathbbm{1}_{\mathbb{R}^{-}}(\mathbf{A})\) sets \(\mathbf{A}_{ij}\) to \(0\) if it is positive, and \(1\) otherwise.

### Latent Graph Inference

**Definition 1** (Latent Graph Inference).: _Given a graph \(\mathcal{G}(\mathcal{V},\mathbf{X})\) containing \(n\) nodes \(\mathcal{V}=\{V_{1},\ldots,V_{n}\}\) and a feature matrix \(\mathbf{X}\in\mathbb{R}^{n\times d}\) with each row \(\mathbf{X}_{i}.\in\mathbb{R}^{d}\) representing the \(d\)-dimensional attributes of node \(V_{i}\), latent graph inference (LGI) aims to simultaneously learn the underlying graph topology encoded by an adjacency matrix \(\mathbf{A}\in\mathbb{R}^{n\times n}\) and the discriminative \(d^{\prime}\)-dimensional node representations \(\mathbf{Z}\in\mathbb{R}^{n\times d^{\prime}}\) based on \(\mathbf{X}\), where the learned \(\mathbf{A}\) and \(\mathbf{Z}\) are jointly optimal for certain downstream tasks \(\mathcal{T}\) given a specific loss function \(\mathcal{L}\)._

In general, an LGI model mainly consists of a latent graph generator \(\mathcal{P}_{\mathbf{\Phi}}(\mathbf{X})\): \(\mathbb{R}^{n\times d}\rightarrow\mathbb{R}^{n\times n}\) that generates an adjacency matrix \(\mathbf{A}\) based on the node features \(\mathbf{X}\), and a node encoder \(\mathcal{F}_{\mathbf{\Theta}}(\mathbf{X},\mathbf{A})\):\(\mathbb{R}^{n\times d}\to\mathbb{R}^{n\times d^{\prime}}\) that learns discriminative node representations \(\mathbf{Z}\) based on \(\mathbf{X}\) and the learned \(\mathbf{A}\). In practice, \(\mathcal{F}_{\mathbf{\Theta}}\) is typically implemented using a \(k\)-layer GNN, while \(\mathcal{P}_{\mathbf{\Phi}}\) can be realized through different strategies such as full parameterization [17, 36], MLP [10, 25], or attentive network [4, 15]. In this paper, we adopt the most common settings from existing LGI literature [4, 10, 25, 46, 13], considering \(\mathcal{T}\) as the semi-supervised node classification task and \(\mathcal{L}\) as the cross-entropy loss.

### Supervision Starvation

To illustrate the supervision starvation problem [10], we consider a general LGI model \(\mathcal{M}\) consisting of a latent graph generator \(\mathcal{P}_{\mathbf{\Phi}}\) and a node encoder \(\mathcal{F}_{\mathbf{\Theta}}\). For simplicity, we ignore the activation function and assume that \(\mathcal{F}_{\mathbf{\Theta}}\) is implemented using a \(1\)-layer GNN, _i.e._, \(\mathcal{F}_{\mathbf{\Theta}}=\texttt{GNN}_{1}(\mathbf{X},\mathbf{A};\mathbf{ \Theta})\), where \(\mathbf{A}=\mathcal{P}_{\mathbf{\Phi}}(\mathbf{X})\). For each node \(\mathbf{X}_{i:}\), the corresponding node representation \(\mathbf{Z}_{i:}\) learned by the model \(\mathcal{M}\) can be expressed as:

\[\mathbf{Z}_{i:}=\mathbf{A}_{i:}\mathbf{X}\mathbf{\Theta}=\left(\sum_{j\in \Omega}\mathbf{A}_{ij}\mathbf{X}_{j:}\right)\mathbf{\Theta},\] (1)

where \(\Omega=\{j\mid\mathbbm{1}_{\mathbb{R}^{+}}(\mathbf{A})_{ij}=1\}\) and \(\mathbf{A}_{ij}=\mathcal{P}_{\mathbf{\Phi}}(\mathbf{X}_{i:},\mathbf{X}_{j:})\). Consider the node classification loss:

\[\min_{\mathbf{A},\mathbf{\Theta}}\mathcal{L}=\sum_{i\in\mathcal{Y}_{L}}\sum_{ j=1}^{|\mathcal{C}|}\mathbf{Y}_{ij}\ln\mathbf{Z}_{ij}=\sum_{i\in\mathcal{Y}_{L}} \mathbf{Y}_{i:}\ln\mathbf{Z}_{i:}^{\top}=\sum_{i\in\mathcal{Y}_{L}}\mathbf{Y} _{i:}\ln\left(\left(\sum_{j\in\Omega}\mathbf{A}_{ij}\mathbf{X}_{j:}\right) \mathbf{\Theta}\right)^{\top},\] (2)

where \(\mathcal{Y}_{L}\) represents the set of indexes of labeled nodes and \(|\mathcal{C}|\) denotes the size of label set. For \(\forall i\in\mathcal{Y}_{L}\), \(j\in\Omega\), \(\mathbf{A}_{ij}\) is optimized via backpropagation under the supervision of label \(\mathbf{Y}_{i:}\). For \(\forall i\notin\mathcal{Y}_{L}\), however, if \(j\notin\mathcal{Y}_{L}\) for \(\forall j\in\Omega\), \(\mathbf{A}_{ij}\) will receive no supervision from any label and, as a result, cannot be semantically optimal after training. Consequently, the learning models exhibit poor generalization as the predictions of testing nodes inevitably rely on these supervision-starved weights. This phenomenon is referred to as _supervision starvation_ (SS), where many edge weights are learned without any label supervision. It is easy to infer that this issue also persists in a \(k\)-layer GNN.

_We may ask why this problem arises?_ In fact, the SS problem is caused by a common and necessary post-processing operation known as graph sparsification, which is employed in the majority of LGI methods [10, 25, 38, 46] to generate a sparse latent graph. To be more specific, graph sparsification adjusts the initial dense graph to a sparse one through the following procedure:

\[\mathbf{A}_{ij}=\begin{cases}\mathbf{A}_{ij},&\text{if}\ \ \mathbf{A}_{ij}\in \text{top-}\kappa(\mathbf{A}_{i:})\\ 0,&\text{otherwise},\end{cases}\] (3)

where \(\text{top-}\kappa(\mathbf{A}_{i:})\) denotes the set of the top \(\kappa\) values in \(\mathbf{A}_{i:}\). After this sparsification operation, a significant number of edge weights are directly erased, including the crucial connections established between pivotal nodes and labeled nodes. _Another question that may arise is: how many important nodes or connections suffer from this problem?_ We delve into this question in the next section.

## 3 Methodology

### Identification & Elimination of Starved Nodes

We first introduce the definitions of \(k\)-hop starved node and the corresponding \(k\)-hop starved weight.

**Definition 2** (\(k\)-hop Starved Node).: _Given a graph \(\mathcal{G}(\mathcal{V},\mathbf{X})\) consisting of \(n\) nodes \(\mathcal{V}=\{V_{1},\dots,V_{n}\}\) and the corresponding node features \(\mathbf{X}\), for a \(k\)-layer graph neural network \(\texttt{GNN}_{k}(\mathbf{X};\mathbf{\Theta})\) with network parameters \(\mathbf{\Theta}\), the unlabeled node \(V_{i}\) is a \(k\)-hop starved node if, for \(\forall\kappa\in\{1,\dots,k\}\), \(\forall V_{j}\in\mathcal{N}_{\kappa}(i)\), where \(\mathcal{N}_{\kappa}(i)\) is the set of \(\kappa\)-hop neighbors of \(V_{i}\), \(V_{j}\) is unlabeled. Specifically, \(0\)-hop starved nodes are defined as the unlabeled nodes2._

Footnote 2: Here, we want to clarify that self-connections are not considered when defining \(k\)-hop neighbors.

Based on Definition 2, we can define the \(k\)-hop starved weight as follows.

**Definition 3** (\(k\)-hop Starved Weight).: _If an edge exists between nodes \(V_{i}\) and \(V_{j}\), the associated edge weight \(\mathbf{A}_{ij}\) is a \(k\)-hop starved weight if both of the \(V_{i}\) and \(V_{j}\) qualifies as \((k-1)\)-hop starved nodes3._

Footnote 3: Note that, for a \(k\)-layer GNN, a \(k\)-hop starved node also qualifies as a \((k-1)\)-hop starved node.

According to the definition presented, it is evident that \(k\)-hop starved weights are precisely the ones that receive no semantic supervision from labels. As a result, to address the SS problem, our focus should be on reducing the presence of \(k\)-hop starved nodes. The following theorem illustrates how we can identify such nodes based on a given initial adjacency matrix.

**Theorem 1**.: _Given a sparse adjacency matrix \(\mathbf{A}\in\mathbb{R}^{n\times n}\) with self-connections generated on graph \(\mathcal{G}(\mathcal{V},\mathbf{X})\) by a latent graph inference model with a \(k\)-layer graph neural network \(\mathtt{GNN}_{k}(\mathbf{X};\mathbf{\Theta})\), the node \(V_{i}\) is a \(k\)-hop starved node, if \(\exists j\in\{1,\ldots,n\}\), such that \([\mathbb{1}_{\mathbb{R}^{+}}(\mathbf{A})]_{ij}^{k}=1\), and for \(\forall j\in\{j\mid[\mathbb{1}_{\mathbb{R}^{+}}(\mathbf{A})]_{ij}=1\cup[ \mathbb{1}_{\mathbb{R}^{+}}(\mathbf{A})]_{ij}^{2}=1\cup\ldots\cup[\mathbb{1}_ {\mathbb{R}^{+}}(\mathbf{A})]_{ij}^{k}=1\}\), \(V_{j}\) is unlabeled._

Proof.: Please refer to the supplementary material for details. 

To provide an intuitive perspective, we use two real-world graph datasets, namely Cora (\(2708\) nodes) and Citeseer (\(3327\) nodes), as examples. We calculate the number of \(k\)-hop starved nodes for \(k=1,2,3,4\), based on their original graph topology. Fig. 1 shows the statistical results for the Cora140, Cora390, Citeseer120, and Citeseer370 datasets, where the suffix number represents the number of labeled nodes. From Fig. 1, we observe that as the value of \(k\) increases, the number of starved nodes decreases. This can be explained by the fact that as \(k\) increases, the nodes have more neighbors (from \(1\)- to \(k\)-hop), and the possibility of having at least one labeled neighbor increases. Adopting a deeper GNN (larger \(k\)) can thus mitigate the SS problem. However, it is important to consider that deeper GNNs result in higher computational consumption and may lead to poorer generalization performance [1, 30, 21]. Furthermore, as shown in Fig. 1, even with a \(4\)-layer GNN, there are still hundreds of \(4\)-hop starved nodes in the Citeseer120. Therefore, we believe that employing a deeper GNN is not the optimal solution to resolve the SS problem 4.

Footnote 4: We discuss this further in the supplementary material.

In fact, Theorem 1 implicitly indicates how to alleviate the SS problem. Intuitively, a straightforward solution is to ameliorate the given adjacency matrix \(\mathbf{A}\) in order to reduce the number of starved nodes. This can be accomplished simply by reconstructing the connections between starved nodes and the labeled ones. Technically, we can achieve this by adding a regularization adjacency matrix \(\mathbf{B}\) to \(\mathbf{A}\):

\[\widetilde{\mathbf{A}}=\mathbf{A}+\alpha\mathbf{B},\] (4)

where \(\widetilde{\mathbf{A}}\) is the refined adjacency matrix, \(\mathbf{B}\) models the recovered affinities between stayed nodes and labeled ones, and \(\alpha\) is a balanced parameter that controls the contribution of \(\mathbf{A}\) and \(\mathbf{B}\). According to Theorem 1, we can identify the starved nodes based on \(\mathbf{A}\) and replenish the missed supervision through \(\mathbf{B}\), thereby preventing \(\widetilde{\mathbf{A}}\) from being starved. Specifically, for each starved node \(V_{i}\), we can search for at least one closest labeled node \(V_{l}\), and restore at least one connection between \(V_{l}\) and \(V_{j}\) for \(j\in\{\mathcal{N}_{\kappa}(i)\cup i\}\) such that \([\mathbb{1}_{\mathbb{R}^{+}}(\mathbf{B})]_{jl}^{s}=1\), where \(\kappa\) can be arbitrarily chosen from the set of \(\{1,2,\ldots,k\}\). Although this strategy is effective, it may be computationally complex. Even with a small value of \(k\), the computational cost of identifying \(k\)-hop starved nodes based on Theorem 1 is prohibitively expensive. For example, when identifying \(2\)-hop starved nodes, the time complexity of computing \(\mathbf{A}^{2}\) alone reaches \(\mathcal{O}(n^{3})\). In the next section, we will propose a more efficient solution.

Figure 1: Illustration of \(k\)-hop starved nodes on different datasets. Obviously, the number of \(k\)-hop starved nodes decreases as the value of \(k\) increases.

### CUR Decomposition Makes Better Solution

Inspired by CUR matrix decomposition [2, 3], we propose an efficient alternative approach to identify the starved nodes. We first present the definition of CUR matrix decomposition.

**Definition 4** (CUR Decomposition [2]).: _Given \(\mathbf{Q}\in\mathbb{R}^{n\times m}\) of rank \(\rho=\mathbf{rank}(\mathbf{Q})\), rank parameter \(k<\rho\), and accuracy parameter \(0<\varepsilon<1\), construct column matrix \(\mathbf{C}\in\mathbb{R}^{n\times c}\) with \(c\) columns from \(\mathbf{Q}\), row matrix \(\mathbf{R}\in\mathbb{R}^{r\times m}\) with \(r\) rows from \(\mathbf{Q}\), and intersection matrix \(\mathbf{U}\in\mathbb{R}^{c\times r}\) with \(c\), \(r\), and \(\mathbf{rank}(\mathbf{U})\) being as small as possible, in order to reconstruct \(\mathbf{Q}\) within relative-error:_

\[||\mathbf{Q}-\mathbf{CUR}||_{F}^{2}\leq(1+\varepsilon)||\mathbf{Q}-\mathbf{Q }_{k}||_{F}^{2}.\] (5)

_Here, \(\mathbf{Q}_{k}=\mathbf{U}_{k}\mathbf{\Sigma}_{k}\mathbf{V}_{k}^{T}\in\mathbb{ R}^{n\times m}\) is the best rank \(k\) matrix obtained via the singular value decomposition (SVD) of \(\mathbf{Q}\)._

With the definition of CUR decomposition, we can find a more efficient solution to identify the starved nodes. The following theorem demonstrates how we can accomplish this goal.

**Theorem 2**.: _Given a sparse adjacency matrix \(\mathbf{A}\in\mathbb{R}^{n\times n}\) with self-connections generated on graph \(\mathcal{G}(V,\mathbf{X})\), construct \(\mathbf{C}=\mathbf{A}[:,col\_mask]\in\mathbb{R}^{n\times c}\), where \(col\_mask\in\{0,1\}^{n}\) contains only \(c\) positive values corresponding to \(c\) labeled nodes, and \(\mathbf{R}=\mathbf{A}[row\_mask,:]\in\mathbb{R}^{r\times n}\) with \(row\_mask=\mathds{1}_{\mathbb{R}^{-}}(\mathbf{C}\mathds{1}_{c})\in\{0,1\}^{n}\). Then, (a) \(\mathbf{U}=\mathbf{A}[row\_mask,col\_mask]=\mathbf{0}\in\mathbb{R}^{r\times c}\), where \(\mathbf{0}\) is a zero matrix, (b) the set of \(1\)-hop starved nodes \(\texttt{Set}_{1}(r)=\{V_{i}|i\in\texttt{RM}_{+}\}\), where \(\texttt{RM}_{+}\in\mathbb{N}^{r}\) indicates the set of indexes of positive elements from \(row\_mask\), and (c) for each \(i\in\texttt{RM}_{+}\), \(V_{i}\) is a 2-hop starved node if, for \(\forall j\) satisfying \([\mathds{1}_{\mathbb{R}^{+}}(\mathbf{R})]_{ij}=1\), \(j\in\texttt{RM}_{+}\)._

Proof.: Please refer to the supplementary material for details. 

Theorem 2 provides a more efficient alternative for identifying \(k\)-hop starved nodes for \(k\in\{1,2\}\). In fact, the column matrix \(\mathbf{C}\) models the relationships between all nodes and \(c\) labeled nodes, the row matrix \(\mathbf{R}\) models the affinities between \(r\)\(1\)-hop starved nodes and the whole nodes, and the intersection matrix \(\mathbf{U}\) models the strength of connections between \(r\)\(1\)-hop starved nodes and \(c\) labeled nodes. Theorem 2 states that \(\mathbf{U}=\mathbf{0}\), indicting that there are no connections between the starved nodes and the labeled ones. Based on this observation, we propose a simpler approach to reduce the number of starved nodes. Specifically, we rebuild the intersection matrix \(\mathbf{U}\) to ensure that the reconstructed \(\widetilde{\mathbf{U}}\neq\mathbf{0}\)5. Consequently, Eq. (4) can be rewritten as:

Footnote 5: In fact, with the reconstructed \(\widetilde{\mathbf{U}}\), we can set \(\widetilde{\mathbf{Q}}=\mathbf{C}\widetilde{\mathbf{U}}\mathbf{R}\) as the regularization \(\mathbf{B}\). It is, of course, sensible and feasible. However, a potential drawback is that the reconstruction of \(\widetilde{\mathbf{Q}}\) requires matrix multiplications of three matrices, which is time-consuming. Unexpectedly, we find that only constructing matrix \(\widetilde{\mathbf{U}}\) is enough to solve the SS problem since it models the relationships between \(1\)-hop starved nodes and labeled ones.

\[\widetilde{\mathbf{A}}=\mathbf{A}+\alpha\mathbf{B}=\mathbf{A}+\alpha\Gamma \left(\widetilde{\mathbf{U}},n\right),\] (6)

where function \(\Gamma(\widetilde{\mathbf{U}},n)\) extends the matrix \(\widetilde{\mathbf{U}}\in\mathbb{R}^{r\times c}\) to an \(n\times n\) matrix by padding \(n-r\) rows of zeros and \(n-c\) columns of zeros in the corresponding positions.

The question now turns to how to reconstruct the intersection matrix \(\widetilde{\mathbf{U}}\). For simplicity, we directly adopt the same strategy used in constructing \(\mathbf{A}\). Specifically, for each row \(i\) of \(\widetilde{\mathbf{U}}\), we establish a connection between \(V_{i}\) and \(V_{j}\) for \(\forall j\in\texttt{CM}_{+}\), where \(\texttt{CM}_{+}\in\mathbb{N}^{c}\) represents the set of indexes of positive elements from \(col\_mask\). We then assign weights to these connections based on their distance. Note that, if we ensure each row of \(\widetilde{\mathbf{U}}\) has at least one weight greater than 0, there will be no \(\kappa\)-hop starved nodes for \(\forall\kappa>1\). This means that we do not need to feed the \(k\)-hop starved nodes satiated, simply feeding \(\kappa\)-hop ones for \(\forall\kappa<k\) makes \(k\)-hop starved nodes cease to exist. In addition, compared with the time complexity of \(\mathcal{O}(dn^{2})\) to construct \(\mathbf{A}\), the time complexity of reconstructing \(\widetilde{\mathbf{U}}\) is \(\mathcal{O}(drc)\). The additional computational burden is relatively negligible since \(c\ll n\).

### Weight Contribution Rate Decay

Directly replenishing the additional supervision encoded in \(\widetilde{\mathbf{U}}\) by Eq. (6) may cause a potential issue we refer to as _weight contribution rate decay_. To understand this issue, we consider the \(j\)-th columnvector \(\widehat{\mathbf{A}}_{;j}\) of \(\widehat{\mathbf{A}}\) for \(j\in\mathbb{C}\!\mathsf{M}_{+}\). We define the weight contribution rates (WCR) of non-starved nodes \(\rho_{-}\) and the WCR of starved nodes \(\rho_{+}\) as:

\[\rho_{-}=\frac{\sum_{i\notin\mathsf{M}_{+}}\widetilde{\mathbf{A}}_{ij}}{\sum_{i }\widehat{\mathbf{A}}_{ij}}=\frac{\sum_{i\notin\mathsf{RM}_{+}}\mathbf{A}_{ij} }{\sum_{i}\widetilde{\mathbf{A}}_{ij}},\quad\rho_{+}=1-\rho_{-}=1-\frac{\sum_{ i\notin\mathsf{RM}_{+}}\mathbf{A}_{ij}}{\sum_{i}\widetilde{\mathbf{A}}_{ij}}= \frac{\sum_{i\in\mathsf{RM}_{+}}\alpha\widetilde{\mathbf{U}}_{ij}}{\sum_{i} \widetilde{\mathbf{A}}_{ij}}.\] (7)

From Eq. (7), we observe that the WCR of non-starved nodes \(\rho_{-}\) decays as the number of starved nodes increases (_i.e._, \(\rho_{+}\) increases). This means that \(\rho_{-}\) becomes negligible if there are numerous starved nodes. As a result, the \(j\)-th column vector \(\widetilde{\mathbf{A}}_{;j}\) of \(\widehat{\mathbf{A}}\) for \(j\in\mathbb{C}\!\mathsf{M}_{+}\) heavily relies on the \(j\)-th column vector \(\widetilde{\mathbf{U}}_{;j}\) of the reconstructed intersection matrix \(\widetilde{\mathbf{U}}\). This outcome is not desirable since the regularization imposed should not dominate a significant portion of the final adjacency matrix. Recalling our initial objective of properly refining the original latent graph to replenish the missed supervision, we will design two simple strategies to relieve this issue.

**Decrease \(\rho_{+}\).** On the one hand, we can decrease the WCR of starved nodes \(\rho_{+}\) by selecting only \(\tau(\tau<c)\) labeled nodes as the supplementary adjacent points for each \(1\)-hop starved node. This results in a sparse intersection matrix \(\widetilde{\mathbf{U}}\). For this strategy, we present the following proposition:

**Proposition 1**.: _Suppose that we randomly select \(\tau\) out of \(c\) labeled nodes as the supplementary adjacent points for each \(1\)-hop starved node. If there are \(r\)\(1\)-hop starved nodes, then for \(\forall j\in\mathbb{C}\!\mathsf{M}_{+}\), we have \(\widehat{\rho}_{+}=\frac{\sum_{i\in\mathsf{RM}_{+}}\alpha\widetilde{\mathbf{U} }_{ij}}{\sum_{i}\widetilde{\mathbf{A}}_{ij}}\leq\rho_{+}\), where \(\widehat{\rho}_{+}=\rho_{+}\) with only a probability of \(\left(\frac{\tau}{c}\right)^{r}\)._

Proof.: Please refer to the supplementary material for details. 

**Increase \(\rho_{-}\).** On the other hand, we can improve the WCR of non-starved nodes \(\rho_{-}\) by magnifying the weights of non-starved nodes. Specifically, we construct an additional regularization matrix \(\mathbf{Q}\in\mathbb{R}^{n\times c}\), where its \(c\) columns correspond to the \(c\) labeled nodes and \(\mathbf{Q}_{ij}=0\) for \(\forall i\in\mathbb{R}\!\mathsf{M}_{+}\). For \(\forall i\notin\mathbb{R}\!\mathsf{M}_{+}\), we establish connections between the node \(V_{i}\) and \(\tau(\tau<c)\) labeled nodes \(V_{j}\), and assign the corresponding weights \(\mathbf{Q}_{ij}\) using a similar strategy as in constructing \(\widetilde{\mathbf{U}}\). By adding the additional regularization matrix \(\mathbf{Q}\), the refined adjacency matrix \(\widehat{\mathbf{A}}\) can be expressed as:

\[\widehat{\mathbf{A}}=\mathbf{A}+\alpha\left(\Gamma\left(\widetilde{\mathbf{U} },n\right)+\Gamma\left(\mathbf{Q},n\right)\right).\] (8)

Similarly, for this strategy, we have the following proposition:

**Proposition 2**.: _Suppose that we randomly select \(\tau\) out of \(c\) labeled nodes as the supplementary adjacent points for each non-starved node. If there are \(r\)\(1\)-hop starved nodes, then for \(\forall j\in\mathbb{C}\!\mathsf{M}_{+}\), we have \(\widehat{\rho}_{-}=\frac{\sum_{i\in\mathsf{RM}_{+}}\widehat{\mathbf{A}}_{ij}} {\sum_{i}\widehat{\mathbf{A}}_{ij}}=\frac{\sum_{i\in\mathsf{RM}_{+}}\widehat{ \mathbf{A}}_{ij}+\alpha\sum_{i\notin\mathsf{RM}_{+}}\mathbf{Q}_{ij}}{\sum_{i} \widehat{\mathbf{A}}_{ij}+\alpha\sum_{i\notin\mathsf{RM}_{+}}\mathbf{Q}_{ij}} \geq\rho_{-}\), where \(\widehat{\rho}_{-}=\rho_{-}\) with only a probability of \(\left(1-\frac{\tau}{c}\right)^{n-r}\)._

Proof.: Please refer to the supplementary material for details. 

### End-to-End Training

Note that the proposed approach is model-agnostic and can be seamlessly integrated into existing LGI models. In Sec. 4, we will apply our design to state-of-the-art LGI methods [4, 10, 13, 46] and compare its performance with the original approach. Therefore, we follow these methods and implement the node encoder \(\mathcal{F}_{\mathbf{\Theta}}\) using a \(2\)-layer GNN. Then, we calculate the cross-entropy loss \(\mathcal{L}_{\mathrm{ce}}\) between the true labels \(\mathbf{Y}\) and the predictions \(\mathbf{Z}\), as well as a graph regularization loss \(\mathcal{L}_{\mathrm{reg}}\) on \(\widehat{\mathbf{A}}\):

\[\min\mathcal{L}=\mathcal{L}_{\mathrm{ce}}+\gamma\mathcal{L}_{\mathrm{reg}}=\sum_ {i\in\mathcal{Y}_{L}}\sum_{j=1}^{|\mathcal{C}|}\mathbf{Y}_{ij}\ln\mathbf{Z}_{ ij}+\gamma\mathcal{L}_{\mathrm{reg}},\] (9)

where \(\gamma\) is a balanced parameter and \(\mathbf{Z}=\mathcal{F}_{\mathbf{\Theta}}(\mathbf{X},\widehat{\mathbf{A}})= \mathrm{softmax}\left(\widehat{\mathbf{A}}\sigma\left(\widehat{\mathbf{A}} \mathbf{X}\mathbf{\Theta}^{0}\right)\mathbf{\Theta}^{1}\right)\). According to different LGI methods, the graph regularization loss \(\mathcal{L}_{\mathrm{reg}}\) can be different, such as Dirichlet energy [4] and self-supervision [10] (see supplementary material for more details). For fairness, in the experiments, we will adopt the same graph regularization loss as the comparison methods.

## 4 Experiments

### Experimental Settings

**Baselines.** As mentioned earlier, the proposed regularization module can be easily integrated into most existing LGI methods. To evaluate its effectiveness, we select representative LGI methods as baselines, including IDGL [4], GRCN [46], SLAPS [10], and LCGS [13]. We also consider two additional baselines marked as GCN+KNN [19, 10] and GCN&KNN [19, 4]. GCN+KNN is a two-step method that first constructs a KNN graph based on feature similarities and then feeds the pre-constructed graph to a GCN for training. GCN&KNN is an end-to-end method that learns the latent graph and network parameters simultaneously. For methods that require a prior graph, we use the same KNN graph as in GCN+KNN. For GCN&KNN, we simply adopt the Dirichlet energy [4] as the graph regularization loss.

**Datasets.** Following the common settings of existing LGI methods [4, 10, 13, 46], we conduct experiments on four well-known benchmarks: Cora, Citeseer, Pubmed [19, 34], and ogbn-arxiv [14]. For detailed dataset statistics, please refer to the supplementary material. For all datasets, we only provide the original node features for training. To test the performance under different labeling rates, for the Cora and Citeseer datasets, we add half of the validation samples to the training sets, resulting in Cora390 and Citeseer370, where the suffix number represents the total number of labeled nodes.

**Implementation.** We compare the above baselines with their corresponding CUR extension versions. Specifically, we consider two CUR extensions termed M_U and M_R (M refers to the baseline), where the former adopts the sparse intersection matrix \(\widehat{\mathbf{U}}\) as the regularization, and the latter combines \(\widehat{\mathbf{U}}\) and \(\mathbf{Q}\) together (see Sec. 3.3 for details). In experiments, we practically select the \(\tau\) closest labeled nodes as supplementary adjacent points for each row of \(\widehat{\mathbf{U}}\) and \(\mathbf{Q}\). For postprocessing operations on the graph, such as symmetrization and normalization [4, 10], we follow the baselines and adopt the same operations for fairness. We select the values of \(\tau\) and \(\alpha\) from the sets \(\{10,15,20,25,30,50\}\) and \(\{0.01,0.1,1.0,10,50,100\}\), respectively. For other hyperparameters such as learning rate and weight decay, we follow the baselines and use the same settings. For each method, we record the best testing performance and report the average accuracy of five independent experiments, along with the corresponding standard deviation.

### Comparison Results

Table 1 presents the comparison results on all used datasets. It is evident that our proposed CUR extensions consistently outperform the corresponding baselines, demonstrating the effectiveness of eliminating starved nodes. When considering the labeling rates of different datasets listed in the third row of Table 1, we observe that the lower the labeling rate, the greater the performance

\begin{table}
\begin{tabular}{l|c c c c c c} \hline Models / datasets & ogbn-arxiv & Cora390 & Cora140 & Citeseer370 & Citeseer120 & Pubmed \\ \hline \# of labeled all nodes & 90941169343 & 3907208 & 1402708 & 370037327 & 12037327 & 60/19717 \\
improvement achieved by our methods. Notably, on the Pubmed dataset with an extremely low labeling rate of \(0.30\%\), the accuracy of our proposed methods (M_R) increase by \(6.12\%\), \(3.58\%\), and \(2.26\%\) compared to the basic models (M) of GCN+KNN, GRCN, and SLAPS, respectively. This is because the lower the labeling rate, the more starved nodes exist in the dataset (see Fig. 1 for an example). Our proposed methods aim to restore the destroyed affinities between starved nodes and labeled nodes, enabling us to leverage the semantic supervision missed by baselines, thereby achieving superior performance with extremely limited supervision.

Fig. 2 displays the training loss and testing accuracy curves of GCN+KNN, GRCN, SLAPS, and their CUR extensions on the Pubmed dataset. We omit the curves for the first 200 epochs of SLAPS because, during this stage, SLAPS focuses solely on learning the latent graph through an additional self-supervision task without involving the classification task. From Fig. 2, we observe that our proposed CUR extensions achieve higher testing accuracy compared to their corresponding baselines. In comparison with the CUR extensions of GCN+KNN, the CUR extensions of GRCN and SLAPS exhibit relatively stable accuracy results as the training epoch increases. This stability can be attributed to GRCN and SLAPS jointly learning the latent graph and network parameters in an end-to-end manner, whereas GCN+KNN pre-constructs a KNN graph and maintains a fixed graph during training without optimization. For GRCN, our proposed CUR extensions demonstrate lower training loss and higher testing accuracy, further indicating their improved generalization by removing starved nodes. Additionally, we observe relatively unstable training loss for the CUR extensions of SLAPS. The potential reason is that SLAPS introduces an additional graph regularization loss through a self-supervision task, while its CUR extensions aim to reconstruct the destroyed connections in the latent graph. As a result, the training process for the self-supervision task can exhibit some instability.

### Discussion

We would like to explore the question that _why our proposed methods yield a slight improvement on the Cora and Citeseer datasets, while achieving a substantial improvement on the Pubmed dataset_. As shown in Table 2, when \(\tau=10\), there are \(100\) and \(299\)\(2\)-hop starved nodes on the Cora140 and Citeseer120 datasets, respectively. However, when \(\tau=20\), there are no \(2\)-hop starved nodes on the Cora140 and Citeseer120 datasets, and the number of \(1\)-hop starved nodes also sharply decreases. In this scenario, since the comparison baselines all utilize a \(2\)-layer GNN, they are unaffected by \(2\)-hop starved nodes and only minimally affected by \(1\)-hop starved nodes. Consequently, our methods result in only slight improvements over the baselines on the Cora140 and Citeseer120 datasets. On the other hand, when \(\tau=20\), there are still \(12,843\)\(2\)-hop starved nodes present on the Pubmed dataset. Since we effectively eliminate these starved nodes without requiring additional graph convolutional layers, our methods can provide notable benefits on this dataset with an extremely low labeling rate.

\begin{table}
\begin{tabular}{l|r r r|r r} \hline Number of neighbors (\(\tau\)) & \multicolumn{3}{c|}{\(10\)} & \multicolumn{3}{c}{\(20\)} \\ \hline Datasets & Cora140 & Citeseer120 & Pubmed & Cora140 & Citeseer120 & Pubmed \\ \hline \(1\)-hop starved nodes & 1,625 & 2,268 & 19,076 & 993 & 1,577 & 18,478 \\ \(2\)-hop starved nodes & 100 & 299 & 16,756 & 0 & 0 & 12,843 \\ \hline \end{tabular}
\end{table}
Table 2: Number of \(k\)-hop starved nodes (\(k\in\{1,2\}\)) on various datasets when selecting different number of neighbors (\(\tau\)) in graph sparsification operation.

Figure 2: Training loss (left vertical axis) and testing accuracy (right vertical axis) curves of GCN+KNN, GRCN, SLAPS, and their corresponding CUR extensions on the Pubmed dataset.

### Ablation Study

In this subsection, we aim to explore and answer the following questions.

_How many starved nodes should be eliminated?_ The results shown in Table 1 indicate that eliminating all starved nodes contributes to the performance improvement of the baselines. It is important to understand the relationship between the number of starved nodes removed and the corresponding performance improvement of the baselines. To investigate this, we randomly remain \(10\%,20\%,40\%,60\%,80\%\) starved nodes and evaluate the performance of GCN+CNN, GRCN, and SLAPS accordingly. The results on the Pubmed dataset are summarized in Table 3. We find that, for GCN+KNN, a smaller number of starved nodes leads to higher testing accuracy. For GRCN and SLAPS, however, we need to check the number of starved nodes to obtain optimal performance.

_How \(\tau\) affects the performance._ Table 4 shows that the selection of \(\tau\) slightly differentiates the performance of our proposed methods. In general, a larger value of \(\tau\) leads to a higher improvement in performance. However, when \(\tau\) is set too large, such as \(50\), the performance starts to degrade. This degradation occurs due to the introduction of incorrect labels when \(\tau\) exceeds a certain threshold.

_How \(\alpha\) affects the performance._ Table 5 presents the sensitivity of parameter \(\alpha\) on the Pubmed dataset. It is observed that a relatively larger value of \(\alpha\), such as 10 or 50, leads to a significant improvement in performance. This finding further emphasizes the effectiveness of our proposed regularization methods in enhancing the performance of existing LGI models.

## 5 Related Work

**Latent Graph Inference.** Given only the node features of data, latent graph inference (LGI) aims to simultaneously learn the underlying graph structure and discriminative node representations from the features of data [10, 24, 39]. For example, Jiang _et al._[15] propose to infer the graph structure by combining graph learning and graph convolution in a unified framework. Yang _et al._[45] model the topology refinement as a label propagation process. Jin _et al._[16] explore some intrinsic properties of the latent graph and propose a robust LGI framework to defend adversarial attacks on graphs. Though effective, these methods require a prior graph to guide the graph inference process. Controversially,

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline The value of \(\tau\) & baseline (0) & 10 & 15 & 20 & 25 & 30 & 50 \\ \hline GCN+KNN\_U & 68.66 \(\pm\) 0.05 & 71.86 \(\pm\) 0.26 & 72.92 \(\pm\) 0.17 & 73.50 \(\pm\) 0.31 & 73.88 \(\pm\) 0.28 & 74.12 \(\pm\) 0.32 & 73.76 \(\pm\) 0.29 \\ GCN+KNN\_R & 68.66 \(\pm\) 0.05 & 73.10 \(\pm\) 0.09 & 73.90 \(\pm\) 0.14 & 74.02 \(\pm\) 0.10 & 74.56 \(\pm\) 0.12 & 74.78 \(\pm\) 0.17 & 74.62 \(\pm\) 0.12 \\ \hline GRCN\_U & 69.24 \(\pm\) 0.20 & 71.92 \(\pm\) 0.87 & 72.56 \(\pm\) 0.77 & 72.64 \(\pm\) 1.03 & 72.80 \(\pm\) 0.59 & 72.56 \(\pm\) 1.02 & 71.80 \(\pm\) 1.13 \\ GRCN\_R & 69.24 \(\pm\) 0.20 & 72.12 \(\pm\) 0.86 & 72.54 \(\pm\) 0.77 & 72.80 \(\pm\) 1.05 & 72.82 \(\pm\) 1.03 & 72.44 \(\pm\) 1.07 & 71.86 \(\pm\) 1.11 \\ SLAPS\_U & 74.86 \(\pm\) 0.79 & 75.98 \(\pm\) 1.12 & 76.20 \(\pm\) 0.87 & 75.58 \(\pm\) 0.90 & 76.28 \(\pm\) 0.61 & 76.74 \(\pm\) 0.59 & 75.98 \(\pm\) 0.89 \\ SLAPS\_R & 74.86 \(\pm\) 0.79 & 76.00 \(\pm\) 1.17 & 76.48 \(\pm\) 0.69 & 76.40 \(\pm\) 0.48 & 71.12 \(\pm\) 0.77 & 76.58 \(\pm\) 0.33 & 76.50 \(\pm\) 0.59 \\ \hline \end{tabular}
\end{table}
Table 4: Parameter sensitivity of \(\tau\) when applying our proposed method to GCN+CNN, GRCN, and SLAPS on the Pubmed dataset. The baseline results indicate the accuracy of the original methods.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline Starved nodes (\%) & 100 & 80 & 60 & 40 & 20 & 10 & 0 \\ \hline GCN+KNN\_U & 68.66 \(\pm\) 0.05 & 69.42 \(\pm\) 0.12 & 70.20 \(\pm\) 0.22 & 71.52 \(\pm\) 0.15 & 72.40 \(\pm\) 0.27 & 73.04 \(\pm\) 0.29 & 74.12 \(\pm\) 0.32 \\ GCN+KNN\_R & 69.12 \(\pm\) 0.33 & 69.46 \(\pm\) 0.22 & 70.46 \(\pm\) 0.19 & 71.90 \(\pm\) 0.15 & 73.24 \(\pm\) 0.05 & 74.00 \(\pm\) 0.14 & 74.78 \(\pm\) 0.17 \\ GRCN\_U & 69.24 \(\pm\) 0.20 & 72.14 \(\pm\) 0.35 & 72.48 \(\pm\) 0.43 & 73.04 \(\pm\) 0.52 & 72.82 \(\pm\) 0.75 & 72.96 \(\pm\) 0.87 & 72.80 \(\pm\) 0.99 \\ GRCN\_R & 70.20 \(\pm\) 0.06 & 72.34 \(\pm\) 0.30 & 72.54 \(\pm\) 0.48 & 72.98 \(\pm\) 0.37 & 72.80 \(\pm\) 0.70 & 72.80 \(\pm\) 0.74 & 72.82 \(\pm\) 1.03 \\ SLAPS\_U & 74.86 \(\pm\) 0.79 & 76.26 \(\pm\) 0.62 & 76.48 \(\pm\) 0.61 & 76.36 \(\pm\) 0.39 & 76.48 \(\pm\) 0.67 & 76.32 \(\pm\) 0.71 & 76.74 \(\pm\) 0.59 \\ SLAPS\_R & 75.64 \(\pm\) 0.45 & 76.44 \(\pm\) 1.27 & 76.52 \(\pm\) 0.18 & 76.50 \(\pm\) 1.22 & 76.38 \(\pm\) 0.45 & 76.70 \(\pm\) 0.59 & 77.12 \(\pm\) 0.77 \\ \hline \end{tabular}
\end{table}
Table 3: Test accuracy (%) of our proposed CUR extensions for GCN+CNN, GRCN, and SLAPS when eliminating different number of starved nodes on the Pubmed dataset.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline The value of \(\alpha\) & baseline (0) & 0.01 & 0.1 & 1.0 & 10 & 50 & 100 \\ \hline GCN+KNN\_U & 68.66 \(\pm\) 0.05 & 68.04 \(\pm\) 0.05 & 67.96 \(\pm\) 0.08 & 68.14 \(\pm\) 0.10 & 69.92 \(\pm\) 0.12 & 72.70 \(\pm\) 0.06 & 74.12 \(\pm\) 0.32 \\ GCN+KNN\_R & 68.66 \(\pm\) 0.05 & 67.96 \(\pm\) 0.10 & 67.90 \(\pm\) 0.11 & 68.18 \(\pm\) 0.07 & 70.04 \(\pm\) 0.16 & 73.22 \(\pm\) 0.07 & 74.78 \(\pm\) 0.17 \\ GRCN\_U & 69.24 \(\pm\) 0.20 & 69.24 \(\pm\) 0.10 & 69.72 \(\pm\) 0.19 & 71.52 \(\pm\) 0.42 & 72.80 \(\pm\) 1.09 & 67.22 \(\pm\) 5.35 & 59.44 \(\pm\) 6.74 \\ GCN+KNN\_R & 69.24 \(\pm\) 0.20 & 69.24 \(\pm\) 0.10 & 69.66 \(\pm\) 0.22 & 71.94 \(\pm\) 0.22 & 72.82 \(\pm\) 1.03 & 68.98 \(\pm\) 4.05 & 60.34 \(\pm\) 6.50 \\ SLAPS\_U & 74.86 \(\pm\) 0.79 & 74.88 \(\pm\) 0.90 & 74.48 \(\pm\) 0.72 & 74.94 \(\pm\) 0.81 & 76.26 \(\pm\) 0.80 & 76.74 \(\pm\) 0.59 & 76.08 \(\pm\) 0.97 \\ SLAPS\_R & 74.86 \(\pm\) 0.79 & 74.62 \(\pm\) 1.51 & 74.32 \(\pm\) 0.83 & 74.76 \(\pm\) 0.71 & 76.22 \(\pm\) 0.55 & 77.12 \(\pm\) 0.77 & 76.74some methods have been proposed to directly infer an optimal graph from the data. For example, Franceschier _et al._[11] regard the LGI problem as a bilevel program task and learn a discrete probability distribution on the edges of the latent graph. Norcliffe-Brown _et al._[29] focus on visual question answering task and propose to learn an adjacency matrix from image objects so that each edge is conditioned on the questions. Fatemi _et al._[10] propose a self-supervision guided LGI method, called SLAPS, which yields supplementary supervision from node features through an additional self-supervision task. Our method is totally different from SLAPS as we provide supplementary supervision directly from the true labels. More importantly, our method is model-agnostic and can be easily integrated into most existing LGI methods.

**CUR Matrix Decomposition.** The CUR decomposition [2; 3] of a matrix \(\mathbf{Q}\in\mathbb{R}^{n\times m}\) aims to find a column matrix \(\mathbf{C}\in\mathbb{R}^{n\times c}\) with a subset of \(c<m\) columns of \(\mathbf{Q}\), and a row matrix \(\mathbf{R}\in\mathbb{R}^{r\times m}\) with a subset of \(r<n\) rows of \(\mathbf{Q}\), as well as an intersection matrix \(\mathbf{U}\in\mathbb{R}^{c\times r}\) such that the matrix multiplication of \(\mathbf{CUR}\) approximates \(\mathbf{Q}\). Unlike the SVD decomposition of \(\mathbf{Q}\), the CUR decomposition obtains actual columns and rows of \(\mathbf{Q}\), which makes it useful in various applications [20; 28]. In our method, instead of seeking an optimal matrix approximation, we employ CUR decomposition to identify and eliminate the starved nodes. The extracted intersection matrix \(\mathbf{U}\) is then reconstructed and served as a regularization term to provide supplementary supervision for better latent graph inference. To the best of our knowledge, we are the first to introduce CUR matrix decomposition into the field of graph neural networks.

## 6 Conclusion

In this paper, we analyze the common problem of supervision starvation (SS) in existing latent graph inference (LGI) methods. Our analysis reveals that this problem arises due to the graph sparsification operation, which destroys numerous important connections between pivotal nodes and labeled ones. Building upon this observation, we propose to recover the corrupted connections and replenish the missed supervision for improved graph inference. To this end, we begin by defining \(k\)-hop starved nodes and transform the SS problem into a more manageable task of reducing starved nodes. Then, we present two simple yet effective solutions to identify the starved nodes, including a more efficient method inspired by CUR matrix decomposition. Subsequently, we eliminate the starved nodes by constructing and incorporating a regularization graph. In addition, we propose two straightforward strategies to tackle the potential issue known as weight contribution rate decay. Extensive experiments conducted on representative benchmarks demonstrate that our proposed methods consistently enhance the performance of state-of-the-art LGI models, particularly under extremely limited supervision.

## 7 Acknowledgments and Disclosure of Funding

We are very grateful to Bahare Fatemi for her valuable discussion of our work. We thank the anonymous NeurIPS reviewers for providing us with constructive suggestions to improve our paper. This material is based upon work supported by the Air Force Office of Scientific Research under award number FA9550-23-1-0290.

## References

* [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In _9th International Conference on Learning Representations_, 2021.
* [2] Christos Boutsidis and David P. Woodruff. Optimal cur matrix decompositions. In _Proceedings of the Forty-Sixth Annual ACM Symposium on Theory of Computing_, page 353-362, 2014.
* [3] HanQin Cai, Keaton Hamm, Longxiu Huang, and Deanna Needell. Robust cur decomposition: Theory and imaging applications. _SIAM Journal on Imaging Sciences_, 14(4):1472-1503, 2021.
* [4] Yu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks: Better and robust node embeddings. In _Advances in Neural Information Processing Systems_, pages 19314-19326, 2020.
* [5] Yudong Chen, Sen Wang, Jianglin Lu, Zhi Chen, Zheng Zhang, and Zi Huang. Local graph convolutional networks for cross-modal hashing. In _Proceedings of the 29th ACM International Conference on Multimedia_, pages 1921-1928, 2021.

* [6] Luca Cosmo, Anees Kazi, Seyed-Ahmad Ahmadi, Nassir Navab, and Michael Bronstein. Latent-graph learning for disease prediction. In _Medical Image Computing and Computer Assisted Intervention_, pages 643-653, 2020.
* [7] Haitz Saez de Ocariz Borde, Anees Kazi, Federico Barbero, and Pietro Lio. Latent graph inference using product manifolds. In _The Eleventh International Conference on Learning Representations_, 2023.
* [8] Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph neural networks for graph classification. In _International Conference on Learning Representations_, 2020.
* [9] Bahare Fatemi, Sami Abu-El-Haija, Anton Tsitsulin, Mehran Kazemi, Dustin Zelle, Neslihan Bulut, Jonathan Halcrow, and Bryan Perozzi. Ugsl: A unified framework for benchmarking graph structure learning. In _the 2nd Annual Workshop on Topology, Algebra, and Geometry in Machine Learning (TAG-ML) at the 40 th International Conference on Machine Learning_, 2023.
* [10] Bahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi. Slaps: Self-supervision improves structure learning for graph neural networks. In _Advances in Neural Information Processing Systems_, pages 22667-22681, 2021.
* [11] Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete structures for graph neural networks. In _Proceedings of the 36th International Conference on Machine Learning_, pages 1972-1982, 2019.
* [12] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _Advances in Neural Information Processing Systems_, 2017.
* [13] Minyang Hu, Hong Chang, Bingpeng Ma, and Shiguang Shan. Learning continuous graph structure with bilevel programming for graph neural networks. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence_, pages 3057-3063, 2022.
* [14] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In _Advances in Neural Information Processing Systems_, pages 22118-22133, 2020.
* [15] Bo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang, and Bin Luo. Semi-supervised learning with graph learning-convolutional networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019.
* [16] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure learning for robust graph neural networks. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, page 66-74, 2020.
* [17] Anees Kazi, Luca Cosmo, Seyed-Ahmad Ahmadi, Nassir Navab, and Michael M. Bronstein. Differentiable graph module (dgm) for graph convolutional networks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(2):1606-1617, 2023.
* [18] Byung-Hoon Kim, Jong Chul Ye, and Jae-Jin Kim. Learning dynamic graph representation of brain connectome with spatio-temporal attention. In _Advances in Neural Information Processing Systems_, pages 4314-4327, 2021.
* [19] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations_, 2017.
* [20] Changsheng Li, Xiangfeng Wang, Weishan Dong, Junchi Yan, Qingshan Liu, and Hongyuan Zha. Joint active learning with feature selection via cur matrix decomposition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 41(6):1382-1396, 2019.
* [21] Qimai Li, Zhichao Han, and Xiao-ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2018.
* [22] Ruoyu Li, Sheng Wang, Feiyun Zhu, and Junzhou Huang. Adaptive graph convolutional neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2018.
* [23] Xiaorui Liu, Jiayuan Ding, Wei Jin, Han Xu, Yao Ma, Zitao Liu, and Jiliang Tang. Graph neural networks with adaptive residual. In _Advances in Neural Information Processing Systems_, pages 9720-9733, 2021.

* [24] Xin Liu, Jiayang Cheng, Yangqiu Song, and Xin Jiang. Boosting graph structure learning with dummy nodes. In _Proceedings of the 39th International Conference on Machine Learning_, pages 13704-13716, 2022.
* [25] Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, and Shirui Pan. Towards unsupervised deep graph structure learning. In _Proceedings of the ACM Web Conference 2022_, page 1392-1403, 2022.
* [26] Jianglin Lu, Jingxu Lin, Zhihui Lai, Hailing Wang, and Jie Zhou. Target redirected regression with dynamic neighborhood structure. _Information Sciences_, 544:564-584, 2021.
* [27] Jianglin Lu, Hailing Wang, Jie Zhou, Yudong Chen, Zhihui Lai, and Qinghua Hu. Low-rank adaptive graph embedding for unsupervised feature extraction. _Pattern Recognition_, 113:107758, 2021.
* [28] Michael W. Mahoney and Petros Drineas. Cur matrix decompositions for improved data analysis. _Proceedings of the National Academy of Sciences_, 106(3):697-702, 2009.
* [29] Will Norcliffe-Brown, Stathis Vafeias, and Sarah Parisot. Learning conditioned graph structures for interpretable visual question answering. In _Advances in Neural Information Processing Systems_, 2018.
* [30] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In _8th International Conference on Learning Representations_, 2020.
* [31] ErLin Pan and Zhao Kang. Multi-view contrastive graph clustering. In _Advances in Neural Information Processing Systems_, pages 2148-2159, 2021.
* [32] Xingyue Pu, Tianyue Cao, Xiaoyun Zhang, Xiaowen Dong, and Siheng Chen. Learning to learn graph topologies. In _Advances in Neural Information Processing Systems_, pages 4249-4262, 2021.
* [33] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. In _Proceedings of the 38th International Conference on Machine Learning_, pages 9323-9332, 2021.
* [34] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. _AI Magazine_, 29(3):93, 2008.
* [35] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Skeleton-based action recognition with directed graph neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019.
* [36] Qingyun Sun, Jianxin Li, Hao Peng, Jia Wu, Xingcheng Fu, Cheng Ji, and Philip S Yu. Graph structure learning with variational information bottleneck. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 4165-4174, 2022.
* [37] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, et al. Graph attention networks. _stat_, 1050(20):10-48550, 2017.
* [38] Ruijia Wang, Shuai Mou, Xiao Wang, Wanpeng Xiao, Qi Ju, Chuan Shi, and Xing Xie. Graph structure estimation neural networks. In _Proceedings of the Web Conference 2021_, page 342-353, 2021.
* [39] Yaohua Wang, Fangyi Zhang, Ming Lin, Senzhang Wang, Xiuyu Sun, and Rong Jin. Robust graph structure learning over images via multiple statistical tests. In _Advances in Neural Information Processing Systems_, 2022.
* [40] Yaohua Wang, Yaobin Zhang, Fangyi Zhang, Senzhang Wang, Ming Lin, YuQi Zhang, and Xiuyu Sun. Ada-NETS: Face clustering via adaptive neighbour discovery in the structure space. In _International Conference on Learning Representations_, 2022.
* [41] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. _Acm Transactions On Graphics (tog)_, 38(5):1-12, 2019.
* [42] Qitian Wu, Wentao Zhao, Zenan Li, David Wipf, and Junchi Yan. Nodeformer: A scalable graph structure learning transformer for node classification. In _Advances in Neural Information Processing Systems_, 2022.

* [43] Ruiqing Xu, Chao Li, Junchi Yan, Cheng Deng, and Xianglong Liu. Graph convolutional network hashing for cross-modal retrieval. In _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence_, pages 982-988, 2019.
* [44] Yi Xu, Lichen Wang, Yizhou Wang, and Yun Fu. Adaptive trajectory prediction via transferable gnn. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6520-6531, 2022.
* [45] Liang Yang, Zesheng Kang, Xiaochun Cao, Di Jin, Bo Yang, and Yuanfang Guo. Topology optimization based graph convolutional network. In _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence_, pages 4054-4061, 2019.
* [46] Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, and Yiming Yang. Graph-revised convolutional network. In _Machine Learning and Knowledge Discovery in Databases_, pages 378-393, 2021.
* [47] HaiYang Zhang, XiMing Xing, and Liang Liu. Dualgraph: A graph-based method for reasoning about label noise. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9654-9663, 2021.

[MISSING_PAGE_FAIL:14]

[MISSING_PAGE_EMPTY:15]

node, then the probability of \(\mathbf{Q}_{ij}=0\) is \(\left(1-\frac{\tau}{c}\right)\). Considering the \(j\)-th column (\(j\in\mathfrak{M}_{+}\)), if \(\alpha\sum_{i\notin\mathfrak{M}_{+}}\mathbf{Q}_{ij}=0\) (_i.e.,_\(\widehat{\rho}_{-}=\rho_{-}\)), it means that all the non-starved nodes \(V_{i}\) (for \(i\notin\mathfrak{RM}_{+}\)) do not select \(V_{j}\) as the supplementary adjacent point. Since there are \(r\)\(1\)-hop starved nodes, the probability of \(\widehat{\rho}_{-}=\rho_{-}\) is \(\left(1-\frac{\tau}{c}\right)^{n-r}\). Moreover, since \(\sum_{i}\widetilde{\mathbf{A}}_{ij}>\sum_{i\notin\mathfrak{RM}_{+}}\widetilde{ \mathbf{A}}_{ij}\left(|\mathfrak{RM}_{+}|=r\right)\), we have \(\widehat{\rho}_{-}\geq\rho_{-}\), where \(\widehat{\rho}_{-}=\rho_{-}\) with only a probability of \(\left(1-\frac{\tau}{c}\right)^{n-r}\). 

## Appendix B Experimental Settings

### Dataset Description

Table 6 provides a comprehensive overview of the datasets used in our experiments, including various statistical characteristics. Please refer to the table for detailed information regarding the datasets. Note that, the original edge features are not used in the experiments as our goal is to infer a latent graph from the node features of the datasets.

### Baselines

In our experiments, we utilize publicly available code repositories provided by the respective authors and follow the hyperparameter settings outlined in their papers [4; 10; 13; 19; 46]. We rerun the codes for all the methods and record the best testing accuracy. Below, we summarize the implementation details and provide the code links for each of the used methods:

**GCN+KNN**[19]: https://github.com/kipf/pygcn.

**GCN&KNN**[19]: https://github.com/kipf/pygcn.

For GCN&KNN, we implement the codes by ourselves. For simplicity, we adopt the following Dirichlet energy [4] as the graph regularization loss:

\[\mathcal{L}_{\mathrm{reg}}=\frac{1}{2n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n} \widehat{\mathbf{A}}_{ij}||\mathbf{X}_{i:}-\mathbf{X}_{j:}||^{2}.\]

**IDGL**[4]: https://github.com/hugochan/IDGL.

**GRCN**[46]: https://github.com/PlusRoss/GRCN.

**SLAPS**[10]: https://github.com/BorealisAI/SLAPS-GNN.

Note that, SLAPS is a multi-task-based approach that involves training an additional self-supervision task for latent graph inference. To ensure computational efficiency, we set the maximum number of epochs to \(1000\) for ogbn-arxiv dataset. We adopt the following denoising autoencoder loss as the graph regularization loss [10]:

\[\mathcal{L}_{\mathrm{reg}}=F(\mathbf{X}_{idx},\mathtt{GNN}_{\mathtt{DAE}}( \widetilde{\mathbf{X}},A;\theta_{\mathtt{GNN}_{\mathtt{DAE}}})_{idx}).\]

where \(idx\) are the selected indices, and \(F\) is the binary cross-entropy loss or the mean-squared error loss [10].

**LCGS**[13]: https://github.com/hu-my/LCGS.

\begin{table}
\begin{tabular}{l|r r r r r} \hline Dataset / Statistic & Nodes & Edges & Features & Classes & Labeling Rate \\ \hline ogbn-arxiv & 169,343 & 1,166,243 & 128 & 40 & 53.70\% \\ Cora140 & 2,708 & 5,429 & 1,433 & 7 & 5.17\% \\ Cora390 & 2,708 & 5,429 & 1,433 & 7 & 14.40\% \\ Citeseer120 & 3,327 & 4,732 & 3,703 & 6 & 3.61\% \\ Citeseer370 & 3,327 & 4,732 & 3,703 & 6 & 11.12\% \\ Pubmed & 19,717 & 44,338 & 500 & 3 & 0.30\% \\ \hline \end{tabular}
\end{table}
Table 6: Detailed description of the datasets used in experiments.

## Appendix C Further Discussions

### Deeper GNNs

_Why using deeper GNNs is not an optimal solution?_ This is because using deeper will bring up new issues as illustrated below. **First**, although increasing GNN to \(4\) layers reduces the number of starved nodes from near \(3,000\) to near \(500\) for the Citeseer120 dataset, the number of starved nodes still accounts for nearly \(15\%\). In fact, the number of starved nodes depends on several factors, including the labeling rate of nodes, the graph sparsification process, and so on. _How to select the number of layers of GNNs to reduce the starved nodes for different datasets and different LGI methods is not easy._**Second**, deeper GNNs typically provide inferior performance. To allow a \(k\)-hop starved node to receive information from labeled nodes, the GNNs need to have at least \(k+1\) layers. However, as the number of layers \(k\) increases, the GNNs may still fail to propagate supervision information from labeled nodes to \(k\)-hop starved nodes due to the over-squashing issue [1]. Moreover, deeper GNNs will make the oversmoothing problem [10; 21] be even more severe. Empirically, we test the SLAPS method on the Pubmed dataset using a different number of layers, and the corresponding accuracies can be seen in Table 7. As we can see, the accuracy of SLAPS with a \(8\)-layer GNN is extremely low, demonstrating that _the generalization of GNNs cannot be guaranteed by simply increasing the number of layers_. **Moreover**, deeper GNNs make the model more complexity. Table 7 lists the number of parameters and the float point operations (FLOPs) of SLAPS method with \(2\), \(4\), and \(8\)-layer GNNs. We observe that _as the number of layers increases, both the number of parameters and the FLOPs increase_. In fact, the majority of existing LGI methods typically only adopt \(2\)-layer GNNs for effectiveness and efficiency.

### Degree Distribution

_Why introducing connections between labeled and unlabeled nodes helps the models?_ We provide a basic insight to answer this question. As we know, an unlabeled node must belong to a specific class, and in the node classification task, a specific class must have some labeled nodes. As a result, we can derive the prior assumption that a good classification model will make an unlabeled node closer to some labeled nodes of the same class. That is why we select the closest labeled nodes as the supplementary adjacent points for the starved nodes. If we compulsorily add such connections between the unlabeled node and its closest labeled nodes, the prior information will be exploited during training, helping the models work better. It is worth noting that introducing connections between labeled and unlabeled nodes can lead to a substantial rise in the degrees of labeled nodes. A potential question is that whether altering the distribution of node degrees will have an adverse effect on the model. To be honest, we have not yet observed any adverse effects from the current experimental results. Nevertheless, it is still worth exploring whether the divergence in degree distributions will potentially hinder the model's performance. What the exact relationship is between the degree distributions and the model's performance in latent graph inference task is a difficult and open question that deserves in-depth exploration.

### Definition of Regularization Graph

_Why selecting \(k=1\) to define the regularization graph between \(k\)-hop starved nodes and labeled ones?_ In fact, we can set any value for \(k\) to define the regularization graph. However, if we set \(k>1\), there will be a potential issue. For illustration, let us set \(k=3\) and eliminate the \(3\)-hop starved nodes by adding the regularization graph. In this case, \(1\)-hop and \(2\)-hop starved nodes will still exist since only \(3\)-hop starved nodes are removed. To address this, a simple solution is to directly set \(k=1\) to define the regularization graph. Why only setting \(k=1\) works? According to the definition of starved nodes, we know that a \(k\)-hop starved node also qualifies as a \((k-1)\)-hop starved node. Therefore, if

\begin{table}
\begin{tabular}{c|c|c|c} \hline Layers & Accuracy & Parameters & FLOPs \\ \hline \(2\) & 74.86 \(\pm\) 0.79 & 645.76K & 12.70G \\ \hline \(4\) & 70.24 \(\pm\) 1.58 & 680.90K & 13.39G \\ \hline \(8\) & 41.18 \(\pm\) 0.88 & 751.17K & 14.76G \\ \hline \end{tabular}
\end{table}
Table 7: Accuracy, parameters and FLOPs of SLAPS when using a different number of GNN layers.

we eliminate \(1\)-hop starved nodes then all \(m\)-hop starved nodes for \(m>1\) will be eliminated. Due to the effectiveness and simplicity, we therefore select \(k=1\).

### Generalization Performance

To show the results of the models corresponding to the best validation performance, we conduct more experiments on the Pubmed dataset, and the experimental results are listed in Table 8. As we can see, our methods still improve the baselines, which further indicates their effectiveness.

## Appendix D Limitation

According to Theorem 1, we can identify \(k\)-hop starved nodes for \(k\geq 1\). Additionally, based on Theorem 2 and CUR matrix decomposition, we demonstrate the identification of \(k\)-hop starved nodes for \(k\in\{1,2\}\). How to identify \(k\)-hop starved nodes for \(k>2\) based on CUR decomposition remains an interesting direction for further investigation. Nevertheless, it is worth noting that by eliminating all \(\kappa\)-hop starved nodes, there will be no \(k\)-hop starved nodes for \(\forall k>\kappa\). Consequently, we think that only identifying \(k\)-hop starved nodes for \(k\in\{1,2\}\) is sufficient and efficient to address the problem of supervision starvation.

\begin{table}
\begin{tabular}{l c|l c|l c} \hline GCN+KNN & 67.28 \(\pm\) 0.38 & GRCN & 68.62 \(\pm\) 0.19 & SLAPS & 74.50 \(\pm\) 0.33 \\ GCN+KNN\_U & 72.76 \(\pm\) 0.29 & GRCN\_U & 72.30 \(\pm\) 0.95 & SLAPS\_U & 76.00 \(\pm\) 0.41 \\ GCN+KNN\_R & 72.98 \(\pm\) 0.31 & GRCN\_R & 72.14 \(\pm\) 0.79 & SLAPS\_R & 76.86 \(\pm\) 0.83 \\ \hline \end{tabular}
\end{table}
Table 8: Test accuracy (%) of different models corresponding to the best validation performance.