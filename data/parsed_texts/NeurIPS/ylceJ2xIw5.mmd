# Fair Wasserstein Coresets

 Zikai Xiong

Operations Research Center, Massachusetts Institute of Technology, zikai@mit.edu

Niccolo Dalmasso

J.P.Morgan AI Research, {niccolo.dalmasso, shubham.x2.sharma, freddy.lecue, daniele.magazzeni, vamsi.k.potluru, tucker.balch, manuela.veloso}@jpmchase.com

Shubham Sharma

J.P.Morgan AI Research, {niccolo.dalmasso, shubham.x2.sharma, freddy.lecue, daniele.magazzeni, vamsi.k.potluru, tucker.balch, manuela.veloso}@jpmchase.com

Freddy Lecue

J.P.Morgan AI Research, {niccolo.dalmasso, shubham.x2.sharma, freddy.lecue, daniele.magazzeni, vamsi.k.potluru, tucker.balch, manuela.veloso}@jpmchase.com

Daniele Magazzeni

J.P.Morgan AI Research, {niccolo.dalmasso, shubham.x2.sharma, freddy.lecue, daniele.magazzeni, vamsi.k.potluru, tucker.balch, manuela.veloso}@jpmchase.com

Vamsi K. Potluru

J.P.Morgan AI Research, {niccolo.dalmasso, shubham.x2.sharma, freddy.lecue, daniele.magazzeni, vamsi.k.potluru, tucker.balch, manuela.veloso}@jpmchase.com

Tucker Balch

J.P.Morgan AI Research, {niccolo.dalmasso, shubham.x2.sharma, freddy.lecue, daniele.magazzeni, vamsi.k.potluru, tucker.balch, manuela.veloso}@jpmchase.com

Manuela Veloso

J.P.Morgan AI Research, {niccolo.dalmasso, shubham.x2.sharma, freddy.lecue, daniele.magazzeni, vamsi.k.potluru, tucker.balch, manuela.veloso}@jpmchase.com

###### Abstract

Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. While current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples, their impact on downstream learning processes has yet to be explored. In this work, we present fair Wasserstein coresets (FWC), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. FWC uses an efficient majority minimization algorithm to minimize the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of FWC is equivalent to Lloyd's algorithm for k-medians and k-means clustering. Experiments conducted on both synthetic and real datasets show that FWC: (i) achieves a competitive fairness-utility tradeoff in downstream models compared to existing approaches, (ii) improves downstream fairness when added to the existing training data and (iii) can be used to reduce biases in predictions from large language models (GPT-3.5 and GPT-4).

## 1 Introduction

In the last decade, the rapid pace of technological advancement has provided the ability of collecting, storing and processing massive amounts of data from multiple sources [65]. As the volume of data continues to surge, it often surpasses both the available computational resources as well as the capacity of machine learning algorithms. In response to this limitation, dataset distillation approaches aim to reduce the amount of data by creating a smaller, yet representative, set of samples; see [80; 40] for comprehensive reviews on the topic. Among those approaches, coresets provide a weighted subset of the original data that achieve similar performance to the original dataset in (usually) a specific machine learning task, such as clustering [26; 21], Bayesian inference [11], online learning [9] and classification [16], among others.

In tandem with these developments, the adoption of machine learning techniques has seen a surge in multiple decision-making processes that affect society at large [69; 81]. This proliferation of machine learning applications has highlighted the need to mitigate inherent biases in the data, as these biases can significantly impact the equity of machine learning models and their decisions [14]. Among many definitions of algorithmic fairness, demographic parity is one of the most prominently used metric [29], enforcing the distribution of an outcome of a machine learning model to not differ dramatically across different subgroups in the data.

Current methodologies for generating a smaller set of fair representative samples focus on the local characteristics of these samples with respect to the original dataset. For instance, [13; 30; 4; 24] obtain representative points by clustering while enforcing each cluster to include the same proportion of points from each subgroup in the original dataset. In another line of work, [33; 44; 53; 72; 12] create representative points by ensuring that points in the original dataset each have at least one representative point within a given distance in the feature space. While these methods can successfully reduce clustering cost and ensure a more evenly spread-out distribution of representative points in the feature space, it is unclear whether such representative samples can positively affect performance or discrimination reduction in downstream learning processes. As the induced distribution of the representative points might be far away from the original dataset distribution, downstream machine learning algorithm might lose significant performance due to this distribution shift, without necessarily reducing biases in the original data (as we also demonstrate empirically in our experiments).

ContributionsIn this work, we introduce **F**air **W**asserstein **C**oresets (FWC), a novel coreset approach that not only generates synthetic representative samples but also assigns sample-level weights to be used in downstream learning tasks. FWC generates synthetic samples by minimizing the Wasserstein distance between the distribution of the original datasets and that of the weighted synthetic samples, while simultaneously enforcing an empirical version of demographic parity. The Wasserstein distance is particularly suitable to this task due to its various connections with downstream learning processes and coresets generation (Section 2). Our contributions are as follows:

1. we show how the FWC optimization problem can be reduced to a nested minimization problem in which fairness constraints are equivalent to linear constraints (Section 3);
2. we develop an efficient majority minimization algorithm [56; 39] to solve the reformulated problem (Section 4). We analyze theoretical properties of our proposed algorithm and FWC (Section 5) and show that, in the absence of fairness constraints, our algorithm reduces to an equivalent version of Lloyd's algorithm for k-means and k-medians clustering, extending its applicability beyond fairness applications (Section 6);
3. we empirically validate the scalability and effectiveness of FWC by providing experiments on both synthetic and real datasets (Section 7). In downstream learning tasks, FWC result in competitive fairness-utility tradeoffs against current approaches, even when we enhance the fairness of existing approaches using fair pre-processing techniques, with an average disparity reduction of \(53\%\) and \(18\%\), respectively. In addition, we show FWC can correct biases in large language models when passing coresets as examples to the LLM, reducing downstream disparities by \(75\%\) with GPT-3.5 [55] by \(35\%\) with GPT-4 [1]. Finally, we show FWC can improve downstream fairness-utility tradeoffs in downstream models when added to the training data (via data augmentation, see Appendix C.2).

Finally, we refer the reader to the Appendix for more details on the optimization problem (Section A), theoretical proofs (Section B) and further experiments and details (Section C).

## 2 Background and Related Work

NotationWe indicate the original dataset samples \(\{Z_{i}\}_{i=1}^{n}\), with \(Z_{i}=(D_{i},X_{i},Y_{i})\in\mathcal{Z}=(\mathcal{D}\times\mathcal{X}\times \mathcal{Y})\subseteq\mathbb{R}^{d}\), where \(X\) indicates the non-sensitive features, \(D\) indicates one or more protected attributes such as ethnicity or gender, and \(Y\) is a decision outcome. In this work, we assume \(D\) and \(Y\) to be discrete features, i.e., to have a finite number of levels so that \(|\mathcal{D}|\ll n\) and \(|\mathcal{Y}|\ll n\). For example, \(Y\) might indicate a credit card approval decision based on credit history \(X\), with \(D\) denoting sensitive demographic information. Given a set of weights \(\{\theta\}_{i=1}^{n}\), define \(p_{Z;\theta}\) the weighted distribution of a dataset \(\{Z_{i}\}_{i=1}^{n}\) as \(p_{Z;\theta}=\frac{1}{n}\sum_{i=1}^{n}\theta_{i}\delta_{Z_{i}}\), where \(\delta_{x}\) indicates the Dirac delta distribution, i.e., the unit mass distribution at point \(x\in\mathcal{X}\). Using this notation, we can express the empirical distribution of the original dataset by setting \(\theta_{i}=e_{i}=1\) for any \(i\), i.e., \(p_{Z;e}=\frac{1}{n}\sum_{i=1}^{n}e_{i}\delta_{Z_{i}}\). For a matrix \(A,A^{\top}\) denotes its transpose. For two vectors (or matrices) \(\langle u,v\rangle\stackrel{{\text{def.}}}{{=}}\sum_{i}u_{i}v_{i}\) is the canonical inner product (the Frobenius dot-product for matrices). We define \(\mathbf{1}_{m}\stackrel{{\text{def.}}}{{=}}(1,\dots,1)\in\mathbb{ R}_{+}^{m}\).

Wasserstein distance and coresetsGiven two probability distributions \(p_{1}\) and \(p_{2}\) over a metric space \(\mathcal{X}\), the Wasserstein distance, or optimal transport metric, quantifies the distance between the two distributions as solution of the following linear program (LP):

\[\mathcal{W}_{c}(p_{1},p_{2})\stackrel{{\text{def.}}}{{=}}\min_{ \pi\in\Pi(p_{1},p_{2})}\int_{\mathcal{X}\times\mathcal{X}}c(x_{1},x_{2})\mathrm{ d}\pi(x_{1},x_{2}),\] (1)

with \(\Pi(p_{1},p_{2})\) indicating the set of all joint probability distributions over the product space \(\mathcal{X}\times\mathcal{X}\) with marginals equal to \((p_{1},p_{2})\)[35]. The operator \(c(x,y)\) represents the "cost" of moving probability mass from \(x\) and \(y\), and reduces to a matrix \(C\) if the underlying metric space \(\mathcal{X}\) is discrete.

The Wasserstein distance has several connections with downstream learning processes and coresets. Firstly, the higher the Wasserstein distance between the weighted representative samples \(p_{\hat{Z};\theta}\) and the original dataset \(p_{Z;e}\), the higher the distribution shift, the more degradation we might expect in terms of downstream learning performance [62]. Secondly, the Wasserstein distance between two probability distributions can also be used to bound the deviation of functions applied to samples from such distributions. Define the following deviation:

\[d(p_{\hat{Z};\theta},p_{Z;e})\stackrel{{\text{def.}}}{{=}}\sup_{ f\in\mathcal{F}}\left|\mathbb{E}_{z\sim p_{\hat{Z};\theta}}f(z)-\mathbb{E}_{z \sim p_{Z;e}}f(z)\right|\.\]

When \(\mathcal{F}\) is the class of Lipschitz-continuous functions with Lipschitz constant equal or less than \(1\), the deviation \(d(p_{\hat{Z};\theta},p_{Z;e})\) is equal to 1-Wasserstein distance \(\mathcal{W}_{1}(p_{\hat{Z};\theta},p_{Z;e})\)[66, 74]. The connection with learning processes and the downstream deviation \(d(p_{(\hat{X},\hat{D});\theta},p_{(X,D);e})\) is immediate when considering Lipschitz continuous classifiers with Lipschitz constant less than 1 (such as logistic regression or Lipschitz-constrained neural networks [2])3. For other classifiers, we note that the 1-Wasserstein distance still bounds the downstream discrepancy: in Proposition 2.1 we show that the 1-Wasserstein distance bounds the downstream discrepancy for ReLu-activated multilayer perceptrons (MLPs), which we use in our experiments on real datasets (Section 7).

Footnote 3: In such cases, \(d(p_{(\hat{X},\hat{D});\theta},p_{(X,D);e})=\mathcal{W}_{1}(p_{(\hat{X},\hat{D });\theta},p_{(X,D);e})\leq\mathcal{W}_{1}(p_{\hat{Z};\theta},p_{Z;e})\), see Lemma B.1.

**Proposition 2.1**.: _Let \(g_{\psi}\in\mathcal{G}^{K}\) be the class of \(K\)-layer multilayer perceptrons with ReLu activations. Then, the downstream discrepancy in downstream performance of \(g_{\psi}\) applied to samples from \(p_{(\hat{X},\hat{D});\theta}\) and \(p_{(X,D);e}\) is bounded by the 1-Wasserstein distance :_

\[d(p_{(\hat{X},\hat{D});\theta},p_{(X,D);e})=\\ \sup_{g_{\psi}\in\mathcal{G}^{K}}\left|\mathbb{E}_{(x,d)\sim p_{ (\hat{X},\hat{D});\theta}}g_{\psi}(x,d)-\mathbb{E}_{(x,d)\sim p_{(X,D);e}}g_{ \psi}(x,d)\right|\leq L_{k}\mathcal{W}_{1}(p_{\hat{Z};\theta},p_{Z;e}),\] (2)

_where \(L_{k}\) is the MLP Lipschitz constant upper bound defined in [75, Section 6.1, Equation (8)]._

We point out that minimizing the Wasserstein distance, hence bounding the downstream performance as in Equation (2), is equivalent to the definition of a measure coresets proposed by [15]. The Wasserstein distance is also connected to coresets in the sense of the best discrete approximation of a continuous distributions. Considering a feature space endowed with a continuous distribution, minimizing the \(p\)-Wasserstein distance across all the distributions of size \(m\) is topologically equivalent to identify the samples of size \(m\) that provide the best Voronoi tessellation of the space in \(L_{p}\) sense [57, 41]. Although other definitions of coresets using Wasserstein distance have been proposed in the literature, they require either to solve the underlying optimal transport problems or the knowledge of the downstream classifier and loss function [15, 46, 82, 43]. FWC is agnostic to any downstream model or loss function, and uses an efficient implementation that does not actually incur in the usual high cost connected to optimal transport. By adapting the approach proposed by [77], we solve an equivalent re-formulated linear optimization problem, which is more computationally tractable than classic approaches such as the simplex or the interior point method (see Section 4.1).

Demographic parityAlso known as statistical parity, demographic parity (DP) imposes the decision outcome and protected attributes to be independent [18]. Using a credit card approval decision example, demographic parity enforces an automatic decision process to approve similar proportion of applicants across different race demographics. DP is one of the most extensively analyzed fairness criterion; we refer the reader to [29] for a review. In this work, we use the demographic parity definition by [10], which enforces the ratio between the conditional distribution of the decision outcome across each subgroups \(p(y|D=d)\) and the marginal distribution of the decision outcome \(p(y)\) to be close to \(1\) in a given dataset. We refer to the "empirical version" of demographic parity to indicate that the conditional and marginal distributions are quantities estimated from the data. Note that the demographic parity definition we adopt enforces a condition on the weighted average of the conditional distributions across groups, which is different from a more recent approach of using Wasserstein distance, and specifically Wasserstein barycenters, to enforce demographic parity [25; 32; 23; 78].

## 3 FwC: Fair Wasserstein Coresets

Given a dataset \(\{Z_{i}\}_{i=1}^{n}\), our goal is to find a set of samples \(\{\hat{Z}_{j}\}_{j=1}^{m}\) and weights \(\{\theta_{j}\}_{j=1}^{m}\) such that \(m\ll n\) and that the Wasserstein distance between \(p_{Z;e}\) and \(p_{\hat{Z};\theta}\) is as small as possible. In addition, we use the fairness constraints proposed by [10] to control the demographic parity violation for the \(p_{\hat{Z};\theta}\) distribution. Let \(p_{\hat{Z};\theta}(y|d)\) indicate the conditional distribution \(p_{\hat{Z};\theta}(\hat{Y}=y|\hat{D}=d)\). Imposing a constraint on the demographic disparity violation then reduces to requiring the conditional distribution under the weights \(\{\theta_{i}\}_{i\in[n]}\) to be close to a target distribution \(p_{Y_{T}}\) across all possible values of the protected attributes \(D\),

\[J\left(p_{\hat{Z};\theta}(y|d),p_{Y_{T}}(y)\right)=\left|\frac{p_{\hat{Z}; \theta}(y|d)}{p_{Y_{T}}(y)}-1\right|\leq\epsilon,\ \forall\ d\in\mathcal{D},y\in\mathcal{Y},\] (3)

where \(\epsilon\) is a parameter that determines the maximum fairness violation, and \(J(\cdot,\cdot)\) is the probability ratio between distributions as defined in [10].

Using the notation above, our goal can then be formulated as the following optimization problem:

\[\begin{split}\min_{\theta\in\Delta_{m},\hat{Z}\in\mathcal{Z}^{m} }\mathcal{W}_{c}(p_{\hat{Z};\theta},p_{Z;e})\\ \text{s.t.}& J\left(p_{\hat{Z};\theta}(y|d),p_{Y_{T} }(y)\right)\leq\epsilon,\ \forall\ d\in\mathcal{D},y\in\mathcal{Y},\end{split}\] (4)

where \(\Delta_{m}\) indicates the set of valid weights \(\{\theta\in\mathbb{R}_{+}^{m}:\sum_{i=1}^{m}\theta_{i}=m\}\). Note that the optimization problem in (4) shares some similarities with the optimization problem in [77] in using the Wasserstein distance as a distance metric between distributions and using (3) to enforce demographic parity. However, [77] only provide sample-level integer weights for the original dataset and do not generate any new samples, while our approach provides a separate set of samples \(\{\hat{Z}_{j}\}_{j=1}^{m}\) with associated real-valued weights \(\{\theta_{j}\}_{j=1}^{m}\), with \(m\ll n\).

We now take the following steps to solve the optimization problem in (4): (i) we reduce the dimensionality of the feasible set by fixing \(\hat{Y}\) and \(\hat{D}\) a priori, (ii) we formulate the fairness constraints as linear constraints, (iii) we add artificial variables to express the objective function and (iv) we simplify the optimization problem to minimizing a continuous non-convex function of the \(\{\hat{X}_{j}\}_{j=1}^{m}\).

Step 1. Reduce the feasible set of the optimization problemAs in practice all possible \(Y_{i}\) and \(D_{i}\) are known a priori, and there are only a limited number of them, we can avoid optimizing over them and instead manually set the proportion of each combination of \(\hat{Y}\) and \(\hat{D}\). This reduces the optimization problem feasible set only over \(\Delta_{m}\) and \(\mathcal{X}^{m}\). The following lemma shows that this in fact does not affect the optimization problem:

**Lemma 3.1**.: _For any \(m>0\), the best fair Wasserstein coreset formed by \(m\) data points \(\{\hat{Z}_{i}:i\in[m]\}\) is no better (i.e., the optimal Wasserstein distance value is no lower) than the best fair Wasserstein coreset formed by \(m|\mathcal{D}||\mathcal{Y}|\) data points \(\{(d,X_{i},y)_{i}:i\in[m],d\in\mathcal{D},y\in\mathcal{Y}\}\)._Hence, we simply set the proportions of \(\{(\hat{D}_{i},\hat{Y})_{i}\}_{i\in[m]}\) in the coresets to be similar to their respective proportions in the original dataset. The optimization problem then reduces to

\[\begin{split}\min_{\theta\in\Delta_{m},\hat{X}\in\mathcal{X}^{m}}& \mathcal{W}_{c}(p_{\hat{Z},\theta},p_{\hat{Z};e})\\ \text{s.t.}& J\left(p_{\hat{Z},\theta}(y|d),p_{Y_{T} }(y)\right)\leq\epsilon,\;\forall\;d\in\mathcal{D},y\in\mathcal{Y}\;,\end{split}\] (5)

whose solutions are the features in the coreset \(\{\hat{X}_{j}\}_{j=1}^{m}\) and the corresponding weights \(\{\theta_{j}\}_{j=1}^{m}\).

Step 2. Equivalent linear constraintsFollowing [77], the fairness constraint in Equation (3) can be expressed as \(2|\mathcal{Y}||\mathcal{D}|\) linear constraints on the weights \(\theta\), as the disparity reduces to the following for all \(d\in\mathcal{D},y\in\mathcal{Y}\):

\[\sum_{i\in[m]:\hat{D}_{i}=d,\hat{Y}_{i}=y}\theta_{i}\leq(1+\epsilon)\cdot p_{ Y_{T}}(y)\cdot\sum_{i\in[m]:\hat{D}_{i}=d}\theta_{i}\;,\sum_{i\in[m]:\hat{D} _{i}=d,\hat{Y}_{i}=y}\theta_{i}\geq(1-\epsilon)\cdot p_{Y_{T}}(y)\cdot\sum_{i \in[m]:\hat{D}_{i}=d}\theta_{i}\;.\]

We can express these by using a \(2|\mathcal{Y}||\mathcal{D}|\)-row matrix \(A\) as \(A\theta\geq\mathbf{0}\).

Step 3. Reformulate the objective function by introducing artificial variablesWhen keeping the samples \(\hat{X}\) fixed, we can follow [60] to derive an equivalent formulation of the Wasserstein distance in the objective as a linear program with \(mn\) variables. By indicating the transportation cost matrix \(C(\hat{X})\), we define its components as follows,

\[C(\hat{X})_{ij}\;\stackrel{{\text{def.}}}{{=}}\;c(Z_{i},\hat{Z}_ {j}),\;\text{for}\;i\in[n],j\in[m]\;.\]

Note that \(C(\hat{X})\) is a convex function of \(\hat{X}\) when, e.g., using any \(L^{p}\) norm to define the transportation cost. Therefore, now the problem (5) is equivalent to

\[\begin{split}\min_{\hat{X}\in\mathcal{X}^{m},\theta\in\Delta_{m},P\in\mathbb{R}^{n\times m}}&\langle C(\hat{X}),P\rangle\\ \text{s.t.}& P\mathbf{1}_{m}=\frac{1}{n}\cdot \mathbf{1}_{n},\;P^{\top}\mathbf{1}_{n}=\frac{1}{m}\cdot\theta,\;P\geq\mathbf{ 0},\;A\theta\geq\mathbf{0}\;.\end{split}\] (6)

Step 4. Reduce to an optimization problem of \(\hat{X}\)As from one of the constraints we get \(\theta=m\cdot P^{\top}\mathbf{1}_{n}\), we further simplify problem (6) as:

\[\begin{split}\min_{\hat{X}\in\mathcal{X}^{m},P\in\mathbb{R}^{n \times m}}&\langle C(\hat{X}),P\rangle\\ \text{s.t.}& P\mathbf{1}_{m}=\frac{1}{n}\cdot \mathbf{1}_{n},\;P\geq\mathbf{0},\;AP^{\top}\mathbf{1}_{n}\geq\mathbf{0}\;. \end{split}\] (7)

Let \(F(C)\), as a function \(F\) of \(C\), denote the optimal objective value of the following optimization problem

\[\begin{split}\min_{P\in\mathbb{R}^{n\times m}}& \langle C,P\rangle\\ \text{s.t.}& P\mathbf{1}_{m}=\frac{1}{n}\cdot \mathbf{1}_{n},\;P\geq\mathbf{0},\;AP^{\top}\mathbf{1}_{n}\geq\mathbf{0}\end{split}\] (8)

and then problem (7) is equivalent to

\[\min_{\hat{X}\in\mathcal{X}^{m}}\;F(C(\hat{X}))\;.\] (9)

In (9) the objective is continuous but nonconvex with respect to \(\hat{X}\). Once the optimal \(\hat{X}^{\star}\) is solved, then the optimal \(P^{\star}\) of the problem (7) is obtained by solving problem (8) with \(C\) replaced with \(C(\hat{X}^{\star})\). Finally, the optimal \(\theta^{\star}\) follows by the equation \(\theta^{\star}=m\cdot(P^{\star})^{\top}\mathbf{1}_{n}\). We now provide a majority minimization algorithm for solving problem (9).

Majority Minimization for Solving the Reformulated Problem

Majority minimization aims at solving nonconvex optimization problems, and refers to the process of defining a convex surrogate function that upper bounds the nonconvex objective function, so that optimizing the surrogate function improves the objective function [56; 39]. As the algorithm proceeds, the surrogate function also updates accordingly, which ensures the value of the original objective function keeps decreasing. Following this framework, we define the surrogate function \(g(\cdot;\hat{X}^{k})\) as follows for the \(k\)-th iterate \(\hat{X}^{k}\in\mathcal{X}^{m}\):

\[g(\hat{X};\hat{X}^{k})\ \stackrel{{\text{def}}}{{=}}\ \langle C(\hat{X}),P_{k}^{\star}\rangle\,\] (10)

in which \(P_{k}^{\star}\) is the minimizer of problem (8) with the cost \(C=C(\hat{X}^{k})\)4.

Footnote 4: We show this surrogate function is adequate, i.e., is convex and an upper bound of the original objective function, in Section 5.

With this surrogate function, Algorithm 1 summarizes the overall algorithm to minimize problem (9). In each iteration of Algorithm 1, line 3 is straightforward since it only involves computing the new cost matrix using the new feature vectors \(\hat{X}^{k}\). We separately discuss how to solve the optimization problems in lines 4 and 5 below.

```
1:Initial feature vectors \(\hat{X}^{k}\) and \(k=0\)
2:while True do
3:\(C\gets C(\hat{X}^{k})\); \(\triangleright\) update the cost matrix \(C\)
4:\(P_{k}^{\star}\leftarrow\) optimal solution of problem (8); \(\triangleright\) updating the surrogate function (Section 4.1)
5:\(\hat{X}^{k+1}\leftarrow\arg\min_{\hat{X}\in\mathcal{X}^{m}}g(\hat{X};\hat{X}^{ k})\); \(\triangleright\) updating feature vectors (Section 4.2)
6:if\(g(\hat{X}^{k+1};\hat{X}^{k})=g(\hat{X}^{k};\hat{X}^{k})\)then
7:\(\theta_{k}^{\star}\gets m\cdot(P_{k}^{\star})^{\top}\mathbf{1}_{n}\); \(\triangleright\) if algorithm has converged, compute optimal weights
8:return\(\hat{X}^{k}\), \(\theta_{k}^{\star}\)\(\triangleright\) return coresets and sample-level weights
9:endif
10:\(k\gets k+1\);
11:endwhile ```

**Algorithm 1** Majority Minimization for Solving (9)

### Updating the Surrogate Function (Line 4)

To update the surrogate function, we need to solve problem (8), which is a large-scale linear program. Rather than solving the computationally prohibitive dual problem we solve a lower-dimensional dual problem by using a variant of the FairWASP algorithm proposed by [77]. We adapt FairWASP for cases where \(m\neq n\) to find the solution of (8) via applying the cutting plane methods on the Lagrangian dual problems with reduced dimension5. We choose FairWASP over established commercial solvers due to its computational complexity being lower than other state of the art approaches such as interior-point or simplex method; see Lemma A.2 in Appendix A for more details.

Footnote 5: As opposed to the scenario where \(m=n\), which was tackled by [77].

### Updating Feature Vectors (Line 5)

To update the feature vectors, we need to obtain the minimizer of the surrogate function \(g(\hat{X};\hat{X}^{k})\), i.e.,

\[\min_{\hat{X}\in\mathcal{X}^{m}}g(\hat{X};\hat{X}^{k})\.\] (11)

The above can be written as the following problem:

\[\min_{\hat{X}_{j}\in\mathcal{X};j\in[m]}\sum_{i\in[m]}\sum_{j\in[m]}c(Z_{i}, \hat{Z}_{j})P_{ij}\] (12)

for \(P=P_{k}^{\star}\), in which each component of \(P\) is nonnegative and \(\hat{Z}_{j}=(\hat{d}_{j},\hat{X}_{j},\hat{y}_{j})\), for the known fixed \(\hat{d}_{j}\) and \(\hat{y}_{j}\). Furthermore, the matrix \(P\) is sparse, containing at most \(n\) non-zeros (as when updating \(P_{k}^{*}\) for problem (8), see Appendix A). Moreover, problem (12) can be separated into the following \(m\) subproblems,

\[\min_{\hat{X}_{j}\in\mathcal{X}}\sum_{i\in[n]}c(Z_{i},\hat{Z}_{j})P_{ ij}\text{, for }j\in[m]\text{.}\] (13)

Each subproblem computes the weighted centroid of \(\{Z_{i}:i\in[n],P_{ij}>0\}\) under the distance function \(c\). Therefore, (11) is suitable for parallel and distributed computing. Additionally, since the cost matrix \(C(\hat{X})\) is a convex function of \(\hat{X}\), each subproblem is a convex problem so gradient-based methods could converge to global minimizers. Furthermore, under some particular conditions, solving these small subproblems can be computationally cheap:

1. If \(\mathcal{X}\) is convex and \(c(Z,\hat{Z})\stackrel{{\text{def.}}}{{=}}\|Z-\hat{Z}\|_{2}^{2}\), then the minimizer of (13) is the weighted average \(\sum_{i\in[n]}P_{ij}X_{i}/\sum_{i\in[n]}P_{ij}\).
2. If \(\mathcal{X}\) is convex and \(c(Z,\hat{Z})\stackrel{{\text{def.}}}{{=}}\|Z-\hat{Z}\|_{1}\), then the minimizer of (13) requires sorting the costs coordinate-wisely and finding the median.
3. If creating new feature vectors is not permitted and \(\mathcal{X}=\{X_{i}:i\in[n]\}\), solving (13) requires finding the smallest \(\sum_{i\in[n]:P_{ij}\neq 0}c(Z_{i},(\hat{d}_{j},X,\hat{y}_{j}))P_{ij}\) for \(X\) within the finite set \(\mathcal{X}\). The matrix \(P\) is highly sparse so this operation is not computationally expensive.

## 5 Theoretical Guarantees

In this section we provide theoretical insights on FWC complexity, convergence behavior of Algorithm 1 as well as generalizability of FWC performance on unseen test sets.

### Computational Complexity

First, we consider the FairWASP variant used in Algorithm 1, line 4. The initialization requires \(O(mn)\) flops and uses \(O(n|Y||D|)\) space for storing the cost matrix. After that, the per-iteration time and space complexities are both only \(O(n|Y||D|)\). Lemma 5.1 analyzes the computational complexity of our adaptation of the FairWASP algorithm when solving problem (8).

**Lemma 5.1**.: _With efficient computation and space management, the cutting plane method has a computational complexity of_

\[\tilde{O}\left(nm+|\mathcal{D}|^{2}|\mathcal{Y}|^{2}n\cdot\log(R/ \epsilon)\right)\] (14)

_flops and \(O(n|\mathcal{D}||\mathcal{Y}|)\) space. Here \(R\) denotes the size of an optimal dual solution of (8), and \(\tilde{O}(\cdot)\) absorbs \(m\), \(n\), \(|\mathcal{D}|\), \(|\mathcal{Y}|\) in the logarithm function._

Hence, the overall complexity of FWC is \(\tilde{O}(mn+|\mathcal{D}|^{2}|\mathcal{Y}|^{2}n\cdot\log(R/\epsilon))\). Note that in practice, both \(|\mathcal{D}|\) and \(|\mathcal{Y}|\) are very small compared with the coreset size \(m\) and dataset size \(n\), so the overall complexity is almost as low as \(O(mn)\).

### Convergence Guarantees

First, we establish that our proposed surrogate function is indeed convex and a valid upper bound. We then show Algorithm 1 converges to a first-order stationary point, within finite iterations if the minimizer of problem (11) is unique. Note that because \(g(\hat{X};\hat{X}^{k})=\langle C(\hat{X}),P_{k}^{*}\rangle\), the minimizer is unique whenever the cost matrix \(C(\cdot)\) is strongly convex.

**Lemma 5.2**.: _The function \(g(\hat{X};\hat{X}^{k})\) is convex function of \(\hat{X}\) and a valid upper bound, i.e., \(g(\hat{X};\hat{X}^{k})\geq F(C(\hat{X}))\). This inequality holds at equality when \(\hat{X}=\hat{X}^{k}\)._

**Theorem 5.3**.: _The objective value is monotonically decreasing, i.e., \(F(C(\hat{X}^{k+1}))\leq F(C(\hat{X}^{k}))\) for any \(k\geq 0\). And once the algorithm stops and \(C(\hat{X}^{k})\) is smooth at \(\hat{X}^{k}\), then \(\hat{X}^{k}\) is a first-order stationary point of (9)._

**Theorem 5.4**.: _When the minimizer of (11) is unique, Algorithm 1 terminates within finite iterations._

### Generalization Guarantees

Proposition 5.5 below bounds the distance and demographic parity between the FWC samples and the true underlying distribution of the data, from which the original dataset of size \(n\) was observed. This generalizes the performance of FWC to unseen test sets sampled from the data generating distribution.

**Proposition 5.5**.: _Let \(\lambda\) indicate the distance between \(p_{\hat{Z};\theta}\) and \(p_{Z;e}\) after convergence of FWC i.e., \(\mathcal{W}(p_{\hat{Z};\theta},p_{Z;e})=\lambda\). Let \(q_{Z}\) be the true underlying distribution of the data supported over \(\mathbb{R}^{d}\), with marginal distribution over \(y\) bounded away from zero, so that \(\rho=\min_{y\in\mathcal{Y}}q_{Z}(y)>0\). Then with probability \(1-\alpha\):_

\[\mathcal{W}_{c}(p_{\hat{Z}},q_{Z}) \leq\lambda+\mathcal{O}(\log(1/\alpha)^{1/d}n^{-1/d})\] (15) \[\sup_{y\in\mathcal{Y},d\in\mathcal{D}}J(p_{\hat{Z}}(y|d),q_{Y}(y)) \leq\frac{\epsilon}{\rho}+\mathcal{O}\left(\sqrt{\frac{\log 2/\alpha}{n \rho^{2}}}\right)\] (16)

In addition, in Appendix B.1 we consider the task of learning using FWC samples. We show that the error in downstream learning tasks can be seen as the sum of (i) the approximation error FWC samples make with respect to the original dataset and (ii) how well \(\hat{Y}\) can be learnt from \(\hat{X}\) and \(\hat{D}\) from FWC samples. However, as FWC samples \(\{\hat{Z}_{j}\}_{j=1}^{m}\) are not i.i.d., standard sample complexity results in e.g., empirical risk minimization, do not apply, highlighting the hardness in developing finite-sample learning bounds in this setting.

## 6 An Alternative View: Generalized clustering algorithm

When the fairness constraints are absent, problem (8) reduces to:

\[\begin{array}{ll}\min_{P\in\mathbb{R}^{n\times m}}&\langle C,P\rangle\\ \text{s.t.}&P\mathbf{1}_{m}=\frac{1}{n}\cdot\mathbf{1}_{n},\;P\geq\mathbf{0}_ {n\times m}\;.\end{array}\] (17)

The minimizer \(P^{\star}\) of (17) can be written in closed form. For each \(i\in[n]\), let \(C_{ij}\); denote a smallest component on the \(i\)-th row of \(C\). Then the components of a minimizer \(P^{\star}\) can be written as \(P^{\star}_{ij}=\frac{1}{n}\cdot\mathbf{I}(j=j^{\star}_{i})\) (where \(\mathbf{I}\) is the indicator function). Hence, without fairness constraints, FWC corresponds to Lloyd's algorithm for clustering. Specifically, Lloyd's algorithm iteratively computes the centroid for each subset in the partition and subsequently re-partitions the input based on the closeness to these centroids [42]; these are the same operations FWC does in optimizing the surrogate function and solving problem (17). Thus, when \(c(x,y)\) is correspondingly defined as \(\|x-y\|_{1}\) or \(\|x-y\|_{2}^{2}\), FWC corresponds to Lloyd's algorithm applied to k-medians or k-means problems, except the centroids have fixed values for \(\hat{D}\) and \(\hat{Y}\) (see Section 3).

Comparison with k-means and k-medoids FWC and Lloyds' algorithm for k-means or k-median share similar per-iteration complexity, with the main difference in complexity due to solving problem (8). We solve this problem efficiently by utilizing a variant of the FairWASP approach by [77] (Section 4.1), hence avoiding the usual complexity in solving optimal transport problems. As shown in Section 5, the leading term in the runtime complexity is \(\mathcal{O}(nm)\), which comes from calculating and storing the cost matrix \(C\). This level of complexity is the same as those in k-means and k-medoids. In addition, from our experiments we also see that the per-iteration complexity of FWC is roughly linear with the original dataset size \(n\) (see the runtime experiment in Section 7 and Appendix C).

## 7 Experiments

Runtime analysisWe evaluate the runtime performance of FWC by creating a synthetic dataset of dimension \(n\) and features of dimension \(p\), with the goal of creating a coreset of size \(m\) (see Appendix C.1 for details). We fix two out of the three parameters to default values \((n,m,p)=(5000,250,25)\) and vary the other across suitable ranges, to analyse the runtime and total number of iterations. Figure 1, top left, and Table 2, in Appendix C.1, show the runtime and number of iterations when increasing the dataset size \(n\) from 1,000 to 1,000,000, with averages and standard deviations over 10 separate runs; both the runtime and number of iterations grow proportionally to the sample size \(n\). Figure 2 in Appendix C.1 also shows that requiring a larger coreset size \(m\) implies the need of fewer iterations but longer iteration runtime, as more representatives need to be computed.

Real datasets resultsWe evaluate the performance of FWC on 4 datasets widely used in the fairness literature [19]: (i) Adult [7], (ii) German Credit [28], (iii) Communities and Crime [64] and (iv) Drug [20]. For each dataset, we consider 3 different coreset sizes \(m=5\%,10\%,20\%\) (apart from the Adult dataset, in which we select \(m\) equal to 0.5%, 1% and 2% due to the large dataset size). We compare our approach with: (a) Fairlets and IndFair, two fair clustering approaches by [4] and [12], (b) K-Median Coresets, a coreset approach by [3], (c) k-means [42] and k-medoids [45, 58], two classic clustering approaches and (d) Uniform Subsampling of the original dataset. For FWC, we consider three different values of the fairness violation hyper-parameters \(\epsilon\) for the optimization problem in (5). We compute the fairness-utility tradeoff by first training a 2-layer multilayer perceptron (MLP) classifier with ReLu activations on the coresets created by each approach and then evaluating the classifier demographic disparity (fairness) and AUC (utility). Figure 1 shows the model with the best fairness-utility tradeoff across the three coreset sizes \(m\), for each approach. FWC obtains equal or better fairness-utility tradeoffs (smaller disparity at the same level of utility, higher utility with the same disparity, or both) across all datasets, and performance remains competitive even when using a fairness pre-processing approach [34]. Appendix C.2 includes more experiments and details, which highlight that: (a) FWC consistently achieves coresets that are closer in distribution to the original dataset with respect to the other methods and, although not natively minimizing clustering cost, also provide competitive performance for smaller datasets (Tables 3 and 4); (b) when added to the training data using the data augmentation schema proposed by [68, Section 2.1] FWC generally either increase the performance or reduce the demographic disparity in the downstream learning process.

Using FWC to improve fairness for LLM[76] evaluate GPT-3.5 and GPT-4 for fairness on predictive tasks for the UCI Adult dataset in a zero and few shot setting. We use a similar evaluation setup and use FWC in the few shot setting as examples and evaluate the results for the gender protected attribute. Specifically, we transform the tabular data into language descriptions, and ask GPT-3.5 Turbo and GPT-4 to perform classification tasks on it. We select \(200\) samples to construct the test set and use a set of 16 samples found using FWC as examples. Further details on this experiment are provided in the appendix. The results are shown in Table 1. Examples provided by FWC help reduce demographic disparity more than providing balanced few shot examples, while losing on predictive accuracy (note that the drop in accuracy is similar to the drop observed in [76] and is representative of the fairness-utility trade-off). Owing to the token limitation of LLM's, these representative coresets

Figure 1: _Top left:_ FWC runtime when changing the original dataset size \(n\). _Others_: Fairness-utility tradeoff on real datasets for a downstream MLP classifier, selecting the model with the best fairness-utility tradeoff across three different coreset sizes \(m\), with averages taken over 10 runs. FWC consistently achieves a comparable/better tradeoff as shown by the Pareto frontier (dashed red line, computed over all models and coreset sizes), even when adjusting the other coresets with a fairness pre-processing technique [33]. See text and Appendix C.2 for more details.

can evidently be valuable to provide a small set of samples that can help mitigate bias. When accounting for the standard deviations for demographic parity in Table 1, FVC reduces the LLM bias when compared to zero shot prompting for GPT-4 across all runs. For GPT-3.5 Turbo, while the average disparity is reduced across runs, such consistency is indeed not observed, owing to a diverse set of outputs from the large language model. Due to limited availability of computational resources (associated with querying these models), we leave a more thorough evaluation across different datasets and models to future work.

## 8 Discussion and Conclusions

We introduce FWC, a novel coreset approach that generates synthetic representative samples along with sample-level weights for downstream learning tasks. FWC minimizes the Wasserstein distance between the distribution of the original datasets and that of the weighted synthetic samples while enforcing demographic parity. We demonstrate the effectiveness and scalability of FWC through experiments conducted on both synthetic and real datasets, as well as reducing biases in LLM predictions (GPT 3.5 and GPT 4). Future extensions include: (i) targeting different fairness metrics such as equalized odds [27, 49] as well as robustness of fairness-performance tradeoff over distribution shifts [48, 67], (ii) exploring privacy and explainability properties of FWC[50, 51], (iii) utilizing coresets for accelerating gradient descents algorithms and test their convergence [47, 70] (iv) reformulating the optimization framework to target deep neural network pruning [52, 54] and (v) investigate applications of fair synthetic data in the financial sector [61].

## Disclaimer

This paper was prepared for informational purposes by the Artificial Intelligence Research group of JPMorgan Chase & Co. and its affiliates ("JP Morgan") and is not a product of the Research Department of JP Morgan. JP Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful.

## References

* [1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [2] C. Anil, J. Lucas, and R. Grosse. Sorting out lipschitz function approximation. In _International Conference on Machine Learning_, pages 291-301. PMLR, 2019.
* [3] O. Bachem, M. Lucic, and S. Lattanzi. One-shot coresets: The case of k-clustering. In _International Conference on Artificial Intelligence and Statistics_, pages 784-792. PMLR, 2018.
* [4] A. Backurs, P. Indyk, K. Onak, B. Schieber, A. Vakilian, and T. Wagner. Scalable fair clustering. In _International Conference on Machine Learning_, pages 405-413. PMLR, 2019.

\begin{table}
\begin{tabular}{|c||c|c|c|c|c|c|} \hline \multirow{2}{*}{_Adult Dataset_} & \multicolumn{2}{c|}{**Zero Shot**} & \multicolumn{2}{c|}{**Few Shot (\(b_{p}=0\))**} & \multicolumn{2}{c|}{**Few Shot (FWC)**} \\ \cline{2-7}  & Accuracy & DP & Accuracy & DP & Accuracy & DP \\ \hline GPT-3.5 Turbo & 53.55 \(\pm\) 0.87 & 0.040 \(\pm\) 0.017 & 57.99 \(\pm\) 1.88 & 0.019 \(\pm\) 0.015 & 55.02 \(\pm\) 1.26 & **0.010 \(\pm\) 0.03** \\ \hline GPT-4 & 76.54 \(\pm\) 1.63 & 0.42 \(\pm\) 0.016 & 74.39 \(\pm\) 2.86 & 0.33 \(\pm\) 0.09 & 65.20 \(\pm\) 0.85 & **0.27 \(\pm\) 0.04** \\ \hline \end{tabular}
\end{table}
Table 1: Using the same setup as in [76], we use GPT-3.5 Turbo and GPT-4 LLM’s for fairness evaluations, with a test set of 200 samples with 0.5 base parity (\(b_{p}=0.5\)). Few Shot - FWC is used to provide sixteen examples with weights to the model as examples. Accuracy and demographic disparity (DP) are based on the resulting predictions from GPT-3.5 and GPT-4 models.

* [5] S. Barocas, M. Hardt, and A. Narayanan. _Fairness and Machine Learning: Limitations and Opportunities_. MIT Press, 2023.
* [6] G. Basso. A hitchhiker's guide to wasserstein distances. _Online manuscript available at https://api. semanticscholar. org/CorpusID_, 51801464, 2015.
* [7] B. Becker and R. Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20.
* [8] D. Bertsimas and J. N. Tsitsiklis. _Introduction to linear optimization_, volume 6. Athena scientific Belmont, MA, 1997.
* [9] Z. Borsos, M. Mutny, and A. Krause. Coresets via bilevel optimization for continual learning and streaming. _Advances in Neural Information Processing Systems_, 33:14879-14890, 2020.
* [10] F. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ramamurthy, and K. R. Varshney. Optimized pre-processing for discrimination prevention. _Advances in Neural Information Processing Systems_, 30, 2017.
* [11] T. Campbell and T. Broderick. Bayesian coreset construction via greedy iterative geodesic ascent. In _International Conference on Machine Learning_, pages 698-706. PMLR, 2018.
* [12] R. Chhaya, A. Dasgupta, J. Choudhari, and S. Shit. On coresets for fair regression and individually fair clustering. In _International Conference on Artificial Intelligence and Statistics_, pages 9603-9625. PMLR, 2022.
* [13] F. Chierichetti, R. Kumar, S. Lattanzi, and S. Vassilvitskii. Fair clustering through fairlets. _Advances in Neural Information Processing Systems_, 30, 2017.
* [14] A. Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. _Big Data_, 5(2):153-163, 2017.
* [15] S. Claici, A. Genevay, and J. Solomon. Wasserstein measure coresets. _arXiv preprint arXiv:1805.07412_, 2018.
* [16] C. Coleman, C. Yeh, S. Mussmann, B. Mirzasoleiman, P. Bailis, P. Liang, J. Leskovec, and M. Zaharia. Selection via proxy: Efficient data selection for deep learning. In _International Conference on Learning Representations_, 2020.
* [17] A. Dvoretzky, J. Kiefer, and J. Wolfowitz. Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator. _The Annals of Mathematical Statistics_, pages 642-669, 1956.
* [18] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In _Proceedings of the 3rd Innovations in Theoretical Computer Science Conference_, pages 214-226, 2012.
* [19] A. Fabris, S. Messina, G. Silvello, and G. A. Susto. Algorithmic fairness datasets: The story so far. _Data Mining and Knowledge Discovery_, 36(6):2074-2152, 2022.
* [20] E. Fehrman, A. K. Muhammad, E. M. Mirkes, V. Egan, and A. N. Gorban. The five factor model of personality and evaluation of drug consumption risk. In _Data Science: Innovative Developments in Data Analysis and Clustering_, pages 231-242. Springer, 2017.
* [21] D. Feldman. Core-sets: Updated survey. _Sampling Techniques for Supervised or Unsupervised Tasks_, pages 23-44, 2020.
* [22] N. Fournier and A. Guillin. On the rate of convergence in wasserstein distance of the empirical measure. _Probability theory and related fields_, 162(3):707-738, 2015.
* [23] S. Gaucher, N. Schreuder, and E. Chzhen. Fair learning with wasserstein barycenters for non-decomposable performance measures. In _International Conference on Artificial Intelligence and Statistics_, pages 2436-2459. PMLR, 2023.

* [24] M. Ghadiri, S. Samadi, and S. Vempala. Socially fair k-means clustering. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, pages 438-448, 2021.
* [25] P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M. Loubes. Obtaining fairness using optimal transport theory. In _International conference on machine learning_, pages 2357-2365. PMLR, 2019.
* [26] S. Har-Peled and S. Mazumdar. On coresets for k-means and k-median clustering. In _Proceedings of the thirty-sixth annual ACM symposium on Theory of computing_, pages 291-300, 2004.
* [27] M. Hardt, E. Price, and N. Srebro. Equality of opportunity in supervised learning. _Advances in neural information processing systems_, 29, 2016.
* [28] H. Hofmann. Statlog (German Credit Data). UCI Machine Learning Repository, 1994. DOI: https://doi.org/10.24432/C5NC77.
* [29] M. Hort, Z. Chen, J. M. Zhang, M. Harman, and F. Sarro. Bias mitigation for machine learning classifiers: A comprehensive survey. _ACM J. Responsib. Comput._, nov 2023.
* [30] L. Huang, S. Jiang, and N. Vishnoi. Coresets for clustering with fairness constraints. _Advances in Neural Information Processing Systems_, 32, 2019.
* [31] H. Jiang, Y. T. Lee, Z. Song, and S. C.-w. Wong. An improved cutting plane method for convex optimization, convex-concave games, and its applications. In _Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing_, pages 944-953, 2020.
* [32] R. Jiang, A. Pacchiano, T. Stepleton, H. Jiang, and S. Chiappa. Wasserstein fair classification. In _Uncertainty in artificial intelligence_, pages 862-872. PMLR, 2020.
* [33] C. Jung, S. Kannan, and N. Lutz. Service in your neighborhood: Fairness in center location. _Foundations of Responsible Computing (FORC)_, 2020.
* [34] F. Kamiran and T. Calders. Data preprocessing techniques for classification without discrimination. _Knowledge and Information Systems_, 33(1):1-33, 2012.
* [35] L. Kantorovitch. On the translocation of masses. _Management Science_, 5(1):1-4, 1958.
* [36] L. G. Khachiyan. Polynomial algorithms in linear programming. _USSR Computational Mathematics and Mathematical Physics_, 20(1):53-72, 1980.
* [37] Y.-g. Kim, K. Lee, and M. C. Paik. Conditional wasserstein generator. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [38] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [39] K. Lange. _MM optimization algorithms_. SIAM, 2016.
* [40] S. Lei and D. Tao. A comprehensive survey of dataset distillation. _IEEE Transactions on Pattern Analysis & Machine Intelligence_, 46(01):17-32, jan 2024.
* 1204, 2020.
* [42] S. Lloyd. Least squares quantization in PCM. _IEEE Transactions on Information Theory_, 28(2):129-137, 1982.
* [43] N. Loo, R. Hasani, A. Amini, and D. Rus. Efficient dataset distillation using random feature approximation. _Advances in Neural Information Processing Systems_, 35:13877-13891, 2022.
* [44] S. Mahabadi and A. Vakilian. Individual fairness for k-clustering. In _International Conference on Machine Learning_, pages 6586-6596. PMLR, 2020.
* [45] F. E. Maranzana. On the location of supply points to minimize transportation costs. _IBM Systems Journal_, 2(2):129-135, 1963.

* [46] B. Mirzasoleiman, J. Bilmes, and J. Leskovec. Coresets for data-efficient training of machine learning models. In _International Conference on Machine Learning_, pages 6950-6960. PMLR, 2020.
* [47] B. Mirzasoleiman, J. Bilmes, and J. Leskovec. Coresets for data-efficient training of machine learning models. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 6950-6960. PMLR, 13-18 Jul 2020.
* [48] A. Mishler and N. Dalmasso. Fair when trained, unfair when deployed: Observable fairness measures are unstable in performative prediction settings. _arXiv preprint arXiv:2202.05049_, 2022.
* [49] A. Mishler, E. H. Kennedy, and A. Chouldechova. Fairness in risk assessment instruments: Post-processing to achieve counterfactual equalized odds. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, pages 386-400, 2021.
* [50] P. Mohassel, M. Rosulek, and N. Trieu. Practical privacy-preserving k-means clustering. _Proceedings on privacy enhancing technologies_, 2020.
* [51] M. Moshkovitz, S. Dasgupta, C. Rashtchian, and N. Frost. Explainable k-means and k-medians clustering. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 7055-7065. PMLR, 13-18 Jul 2020.
* [52] B. Mussay, M. Osadchy, V. Braverman, S. Zhou, and D. Feldman. Data-independent neural pruning via coresets. In _International Conference on Learning Representations_, 2020.
* [53] M. Negahbani and D. Chakrabarty. Better algorithms for individually fair \(k\)-clustering. _Advances in Neural Information Processing Systems_, 34:13340-13351, 2021.
* [54] R. Ohib, N. Gillis, N. Dalmasso, S. Shah, V. K. Potluru, and S. Plis. Explicit group sparse projection with applications to deep learning and NMF. _Transactions on Machine Learning Research, arXiv:1912.03896_, 2022.
* [55] OpenAI. Chatgpt3.5. _https://chat.openai.com_, 2022.
* [56] J. M. Ortega and W. C. Rheinboldt. _Iterative solution of nonlinear equations in several variables_. SIAM, 2000.
* [57] G. Pages. Introduction to vector quantization and its applications for numerics. _ESAIM: proceedings and surveys_, 48:29-79, 2015.
* [58] H.-S. Park and C.-H. Jun. A simple and fast algorithm for k-medoids clustering. _Expert Systems with Applications_, 36(2):3336-3341, 2009.
* [59] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. _Journal of Machine Learning Research_, 12(Oct):2825-2830, 2011.
* [60] G. Peyre, M. Cuturi, et al. Computational optimal transport: With applications to data science. _Foundations and Trends(r) in Machine Learning_, 11(5-6):355-607, 2019.
* [61] V. K. Potluru, D. Borrajo, A. Coletta, N. Dalmasso, Y. El-Laham, E. Fons, M. Ghassemi, S. Gopalakrishnan, V. Gosai, E. Kreacic, et al. Synthetic data applications in finance. _arXiv preprint arXiv:2401.00081_, 2023.
* [62] J. Quinonero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. _Dataset shift in machine learning_. Mit Press, 2022.
* [63] Y. P. Raykov, A. Boukouvalas, F. Baig, and M. A. Little. What to do when k-means clustering fails: a simple yet principled alternative algorithm. _PloS one_, 11(9):e0162259, 2016.
* [64] M. Redmond. Communities and Crime. UCI Machine Learning Repository, 2009. DOI: https://doi.org/10.24432/C53W3X.

* [65] S. Sagiroglu and D. Sinanc. Big data: A review. In _2013 International Conference on Collaboration Technologies and Systems (CTS)_, pages 42-47. IEEE, 2013.
* [66] F. Santambrogio. Optimal transport for applied mathematicians. _Birkauser, NY_, 55(58-63):94, 2015.
* [67] S. Sharma, J. Henderson, and J. Ghosh. Feamoe: fair, explainable and adaptive mixture of experts. In _Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence_, IJCAI '23, 2023.
* [68] S. Sharma, Y. Zhang, J. M. Rios Aliaga, D. Bouneffouf, V. Muthusamy, and K. R. Varshney. Data augmentation for discrimination prevention and bias disambiguation. In _Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society_, pages 358-364, 2020.
* [69] M. Sloane, E. Moss, and R. Chowdhury. A silicon valley love triangle: Hiring algorithms, pseudo-science, and the quest for auditability. _Patterns_, 3(2), 2022.
* [70] M. Sordello, N. Dalmasso, H. He, and W. Su. Robust learning rate selection for stochastic optimization via splitting diagnostic. _Transactions on Machine Learning Research arXiv:1910.08597_, 2024.
* [71] J. Thickstun. Kantorovich-rubinstein duality. _Online manuscript available at https://courses. cs. washington. edu/courses/cse599i/20au/resources/L12_ duality. pdf_, 2019.
* [72] A. Vakilian and M. Yalciner. Improved approximation algorithms for individually fair clustering. In _International Conference on Artificial Intelligence and Statistics_, pages 8758-8779. PMLK, 2022.
* [73] A. Vattani. K-means requires exponentially many iterations even in the plane. In _Proceedings of the twenty-fifth annual symposium on Computational geometry_, pages 324-332, 2009.
* [74] C. Villani et al. _Optimal transport: Old and new_, volume 338. Springer, 2009.
* [75] A. Virmaux and K. Scaman. Lipschitz regularity of deep neural networks: analysis and efficient estimation. _Advances in Neural Information Processing Systems_, 31, 2018.
* [76] B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang, C. Xu, Z. Xiong, R. Dutta, R. Schaeffer, et al. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.
* [77] Z. Xiong, N. Dalmasso, A. Mishler, V. K. Potluru, T. Balch, and M. Veloso. FairWASP: Fast and optimal fair wasserstein pre-processing. _Proceedings of the AAAI Conference on Artificial Intelligence_, 38(14):16120-16128, Mar. 2024.
* [78] S. Xu and T. Strohmer. Fair data representation for machine learning at the pareto frontier. _Journal of Machine Learning Research_, 24(331):1-63, 2023.
* [79] R. Yin, Y. Liu, W. Wang, and D. Meng. Randomized sketches for clustering: Fast and optimal kernel \(k\)-means. _Advances in Neural Information Processing Systems_, 35:6424-6436, 2022.
* [80] R. Yu, S. Liu, and X. Wang. Dataset distillation: A comprehensive review. _IEEE Transactions on Pattern Analysis & Machine Intelligence_, 46(01):150-170, jan 2024.
* [81] A. Zhang, L. Xing, J. Zou, and J. C. Wu. Shifting machine learning for healthcare from development to deployment and from models to data. _Nature Biomedical Engineering_, 6(12):1330-1345, July 2022.
* [82] B. Zhao, K. R. Mopuri, and H. Bilen. Dataset condensation with gradient matching. In _International Conference on Learning Representations_, 2021.

**Appendix**

## Appendix A Details on Updating the Surrogate Function (line 4 of Algorithm 1)

To update the surrogate function, we need to solve problem (8), which is a huge-scale linear program with \(O(n)\) constraints and \(O(mn)\) nonnegative variables. In this work, we adapt FairWASP[77] for cases where \(m\neq n\), as opposed to the scenario where \(m=n\), which was tackled by [77]. Before showing the main idea of the algorithm, we rephrase a useful lemma for doing linear minimization on \(S_{n,m}\stackrel{{\text{\tiny def.}}}{{=}}\{P\in\mathbb{R}^{n \times m}:P\mathbf{1}_{m}=\frac{1}{n}\cdot\mathbf{1}_{n},P\geq\mathbf{0}\}\).

**Lemma A.1**.: _For the function \(G(C)\stackrel{{\text{\tiny def.}}}{{=}}\max_{P\in S_{n,m}} \langle C,P\rangle\), it is a convex function of \(C\) in \(\mathbb{R}^{n\times m}\). For each \(i\in[n]\), let \(C_{ij^{*}}\) denote a largest component on the \(i\)-th row of \(C\), then \(G(C)=\frac{1}{n}\sum_{i=1}^{n}C_{ij^{*}_{i}}\). Define the components of \(P^{*}\) as follows:_

\[P^{*}_{ij}=\left\{\begin{array}{ll}0&\text{if }j\neq j^{*}_{i}\\ \frac{1}{n}&\text{if }j=j^{*}_{i}\end{array}\right.\] (18)

_and then \(P^{*}\in\arg\max_{P\in S_{n,m}}\langle C,P\rangle\) and \(P^{*}\in\partial G(C)\)._

Proof.: The proof of the above lemma is equivalent with that of Lemma 1 of [77] in the case when \(m\neq n\), which can be extended directly. 

With this lemma, now we show how to efficiently solve problem (8) via its dual problem. Although (8) is of large scale and computationally prohibitive, it is equivalent to the following saddle point problem on the Lagrangian:

\[\min_{P\in S_{n,m}}\max_{\lambda\in\mathbb{R}^{+}_{+}}L(P,\lambda)\stackrel{{ \text{\tiny def.}}}{{=}}\langle C,P\rangle-\lambda^{\top}AP^{\top} \mathbf{1}_{n}\] (19)

where \(h\) is the number of rows of \(A\), which is at most \(2|\mathcal{Y}||\mathcal{D}|\). It should be mentioned that \(h\) is significantly smaller than \(mn\); for example, for classification tasks with only two protected variables, \(h\) is no larger than \(8\), independent of the number of samples or features. Since \(L(\cdot,\cdot)\) is bilinear, the minimax theorem guarantees that (19) is equivalent to \(\max_{\lambda\in\mathbb{R}^{h}_{+}}\min_{P\in S_{n,m}}L(P,\lambda)\). This is further equal to the dual problem:

\[\max_{\lambda\in\mathbb{R}^{+}_{+}}-\Big{[}G(\lambda)\stackrel{{ \text{\tiny def.}}}{{=}}\max_{P\in S_{n,m}}\Big{\langle}\sum_{j=1}^{h} \lambda_{j}\mathbf{1}_{n}a_{j}^{\top}-C,P\Big{\rangle}\Big{]}\;,\] (20)

in which \(a_{j}^{\top}\) denotes the \(j\)-th row of \(A\). Note that the problem (20) has much fewer decision variables than that of (19) and Lemma A.1 ensures the function \(G(\cdot)\) is convex and has easily accessible function values and subgradients. Therefore, directly applying a cutting plane method has low per-iteration complexity and solves the problem (20) in linear time. We include the details on the cutting plane method in Section A.1 below. Finally, the primal optimal solution of (8) can be easily recovered from the dual optimal solution \(\lambda^{\star}\) via solving \(\max_{P\in S_{n,m}}\big{\langle}\sum_{j=1}^{h}\lambda^{\star}_{j}\mathbf{1}_{ n}a_{j}^{\top}-C,P\big{\rangle}\), under the assumption that this problem has a unique minimizer, which almost always holds in practice for the computed \(\lambda^{\star}\) and is also assumed by [77]. In this way, we have shown how the problem (8) can be efficiently solved by applying a cutting plane method on its dual problem.

### Details of the Cutting Plane Method for Solving (20)

The cutting plane method is designed for convex problems where a _separation oracle_ can be employed [36]. For any \(\lambda\in\mathbb{R}^{m}\), a separation oracle operates by generating a vector \(g\) which satisfies \(g^{\top}\lambda\geq g^{\top}\lambda^{\star}\) for all \(\lambda^{\star}\) in the set of optimal solutions. By repeatedly applying the separation oracle to cut down the potential possible feasible set, the cutting plane method progressively narrows down the feasible solution space until it reaches convergence. The specific steps of the cutting plane algorithm are detailed in Algorithm 2, with the key distinctions among different versions of this method lying in how lines 3 and 4 are implemented.

For the problem (20), a separation oracle (line 4 in Algorithm 2) can directly use the vector of subgradients, which are efficiently accessible, as we mentioned in section 4. Given that we have shown (20) is a low-dimensional convex program with subgradient oracles, there exist many well-established algorithms that can be used. Suppose that the norm of an optimal \(\lambda^{\star}\) is bounded by \(R\), to the best of our knowledge, the cutting plane method with the best theoretical complexity is given by [31], who proposed an improved cutting plane method that only needs \(O((h\cdot\mathrm{SO}+h^{2})\cdot\log(hR/\epsilon))\) flops. Here SO denotes the complexity of the separation oracle. Note that here \(h\) is at most \(2|\mathcal{D}||\mathcal{Y}|\), which is far smaller than \(n\) or \(m\). Here we restate the Corollary 5 of [77] below for the overall time and space complexity of applying the cutting plane method in the case \(m\neq n\).

**Lemma A.2** (Essentially Corollary 5 of [77]).: _With efficient computation and space management, the cutting plane method solves the problem (20) within_

\[\bar{O}\left(mn+|\mathcal{D}||^{2}|\mathcal{Y}|^{2}n\cdot\log(\tfrac{R}{ \epsilon})\right)\] (21)

_flops and \(O(n|\mathcal{D}||\mathcal{Y}|)\) space. Here we use \(\bar{O}(\cdot)\) to hide \(m\), \(n\), \(|\mathcal{D}|\), and \(|\mathcal{Y}|\) in the logarithm function._

The above result is essentially Corollary 5 of [77] by slightly extending the proof to the general case \(m\neq n\). Finally, in terms of the implementation, we follow [77] and use the analytic center cutting plane method.

## Appendix B Theoretical Proofs

This section includes the theoretical proofs for Section 2 and Section 5. We first show the Wasserstein distance upper bounds downstream disparity for MLP networks (Proposition 2.1). We then show (i) the optimization problem can be reduced to optimizing over the \(\hat{X}\) rather than \(\hat{Z}\) (Lemma 3.1), (ii) the surrogate function is convex and a valid upper bound of the optimization objective (Lemma 5.2), (iii) our proposed algorithm converges to a first-order stationary point in \(\hat{X}\) (Theorem 5.3), and (iv) our proposed algorithm terminates in a finite amount of iterations (Theorem 5.4). We also prove the generalization bound for FWC performance in terms of Wasserstein distance and demographic parity to unseen datasets coming from the same (unknown) distribution the original dataset was sampled from. Finally, we include Section B.1 to note how the downstream learning using FWC can be broken down into two terms, which highlights the challenges in analyzing its theoretical properties.

**Lemma B.1**.: _Let \(Z=(X,Y),\hat{Z}=(\hat{X},\hat{Y})\in\mathcal{Z}=(\mathcal{X}\times\mathcal{Y})\) be two pairs of random variables with joint distributions \(p_{Z}\) and \(p_{\hat{Z}}\) and marginal distributions \(p_{X}\), \(p_{Y}\) and \(p_{\hat{X}}\) and \(p_{\hat{Y}}\) respectively. Let \(\Pi(Z,\hat{Z})\) indicate the set of all joint probability distributions over the product space \(\mathcal{Z}\times\mathcal{Z}\) that admit marginal and conditional distributions over \(\mathcal{X}\) and \(\mathcal{Y}\). For any non-negative cost operator \(c\):_

\[\mathcal{W}_{c}(p_{X},p_{\hat{X}})\leq\mathcal{W}_{c}(p_{Z},p_{\hat{Z}}).\]

Proof.: Proof of Lemma B.1 Let \(\Pi_{X}(X,\hat{X})\) indicate the set of all marginal (joint) probability distributions over the product space \(\mathcal{X}\times\mathcal{X}\). For any \(\pi\in\Pi(Z,\hat{Z})\) and the corresponding \(\pi_{x}\in\Pi_{X}(X,\hat{X})\) we have that:\[\int_{\mathcal{Z}\times\mathcal{Z}}c(z,\hat{z})d\pi(z,\hat{z}) \geq\int_{\mathcal{Z}\times\mathcal{Z}}c(x,\hat{x})d\pi(z,\hat{z})\] (22) \[=\int_{\mathcal{X}\times\mathcal{X}}c(x,\hat{x})d\pi_{x}(x,\hat{x})\] \[\geq\min_{\pi_{x}\in\Pi_{x}(x,\hat{x})}c(x,\hat{x})d\pi_{x}(x, \hat{x})=\mathcal{W}_{c}(p_{X},p_{\hat{X}})\]

As this is valid for any \(\pi\in\Pi(Z,\hat{Z})\), select:

\[\pi^{\star}=\operatorname*{arg\,min}_{\pi\in\Pi(Z,\hat{Z})}\int_{\mathcal{Z} \times\mathcal{Z}}c(z,\hat{z})d\pi(z,\hat{z}).\]

Then the left-hand-side of (22) is equal to \(\mathcal{W}_{c}(p_{\mathcal{Z}},p_{\hat{Z}})\), hence proving that \(\mathcal{W}_{c}(p_{\mathcal{Z}},p_{\hat{Z}})\geq\mathcal{W}_{c}(p_{X},p_{ \hat{X}})\).

Proof of Proposition 2.1 The proof of the upper bound follows from the first part of the proof of the Kantorovich-Rubinstein duality [66]. In this work we follow the proof by [6, 71], which consider the Lagrangian form of the 1-Wasserstein distance and express it in the following form:

\[\mathcal{W}(p_{(X,D)},p_{(\hat{X},\hat{D})})=\sup_{f,g:f(x)+g(y)\leq\|x-y\|_{ 2}}\left|\mathbb{E}_{(x,d)\sim p_{(X,D)}}f_{\theta}(x,d)-\mathbb{E}_{(x,d) \sim p_{(\hat{X},\hat{D})}}f_{\theta}(x,d)\right|,\]

where \(f,g:\mathcal{X}\to\mathbb{R}\) are bounded, measurable functional Lagrangian multipliers. Let \(L_{f_{\theta}}\) be the Lipschitz constant of the MLP \(f_{\theta}\), and define the following function:

\[h(x,d)=\frac{f_{\theta}(x,d)}{L_{f_{\theta}}}.\]

By definition, \(h(x,d)\) is 1-Lipschitz. Again following [6, 71], we know that for 1-Lipschitz functions the following holds:

\[\left|\mathbb{E}_{(x,d)\sim p_{(X,D)}}f_{\theta}(x,d)-\mathbb{E}_ {(x,d)\sim p_{(\hat{X},D)}}f_{\theta}(x,d)\right|=L_{f_{\theta}}\left|\mathbb{ E}_{(x,d)\sim p_{(X,D)}}h(x,d)-\mathbb{E}_{(x,d)\sim p_{(\hat{X},D)}}h(x,d)\right.\] \[\quad=L_{f_{\theta}}\left|\int_{(\mathcal{X}\times\mathcal{D}) \times(\mathcal{X}\times\mathcal{D})}h(x_{1},d_{1},x_{2},d_{2})d\pi\left(p_{( X,D)},p_{(\hat{X},\hat{D})}\right)\right|\] \[\quad\leq L_{f_{\theta}}\int_{(\mathcal{X}\times\mathcal{D}) \times(\mathcal{X}\times\mathcal{D})}\left|h(x_{1},d_{1},x_{2},d_{2})\right|d \pi\left(p_{(X,D)},p_{(\hat{X},\hat{D})}\right)\] \[\quad\leq L_{f_{\theta}}\int_{(\mathcal{X}\times\mathcal{D}) \times(\mathcal{X}\times\mathcal{D})}\left\|(x_{1},d_{1})-(x_{2},d_{2}) \right\|_{2}d\pi\left(p_{(X,D)},p_{(\hat{X},\hat{D})}\right)\] \[\quad\leq L_{f_{\theta}}\mathcal{W}_{1}(p_{(X,D)},p_{(\hat{X}, \hat{D})})\leq L_{f_{\theta}}\mathcal{W}_{1}(p_{Z},p_{\hat{Z}}),\]

where the last inequality is due to Lemma B.1. The result can be obtained by using the upper bound in [75, Section 6.1], which shows that \(L_{f_{\theta}}\leq L_{k}\) for \(K\)-layer MLPs with ReLu activations.

Proof of Lemma 3.1.: Once we generate \(m|\mathcal{D}||\mathcal{Y}|\) data points, the feasible set of the latter Wasserstein coreset contains the feasible set of the former Wasserstein coreset.

Proof of Lemma 5.2.: The convexity follows directly from the convexity of \(C(\hat{X})\), as the \(P_{k}^{\star}\geq 0\) in (10).

Before proving it is an upper bound, we show some important properties of \(F(C)\) as a function of \(C\). Firstly, \(F(C)\) is concave on \(C\) because of the concavity of the minimum LP's optimal objective on the objective vector. Secondly, since the feasible set of problem (8) is bounded, the optimal solution \(F(C)\) is continuous with respect to \(C\). Thirdly, due to the sensitivity analysis of LP [8], a supergradient of \(F(C)\) at point \(C\) is the corresponding optimal solution \(P^{\star}\). Here the definition of supergradients for concave functions is analogous to the definition of subgradients for convex functions.

Now we prove \(g(\hat{X};\hat{X}^{k})\) is an upper bound of \(F(C(\hat{X}))\). Because \(P_{k}^{k}\) is a supergradient of \(F(C)\) when \(C=C(\hat{X}^{k})\),

\[F(C)+\langle P_{k}^{k},C(\hat{X})-C\rangle\geq F(C(\hat{X}))\;,\]

in which the left-hand side is equal to \(g(\hat{X};\hat{X}^{k})\) because \(F(C)=\langle P_{k}^{k},C\rangle\) and \(g(\hat{X};\hat{X}^{k})=\langle C(\hat{X}),P_{k}^{\star}\rangle\). Therefore, the surrogate function is an upper bound of the objective function \(F(C(\hat{X}))\), i.e., \(g(\hat{X};\hat{X}^{k})\geq F(C(\hat{X}))\). Moreover, due to the definition in (10), \(g(\hat{X};\hat{X}^{k})=F(C(\hat{X}))\) when \(\hat{X}=\hat{X}^{k}\). 

Proof of Theorem 5.3.: The monotonically decreasing part of the claim follows by:

\[F(C(\hat{X}^{k+1}))\leq g(\hat{X}^{k+1};\hat{X}^{k})=\arg\min_{\hat{X}\in \mathcal{X}^{m}}g(\hat{X};\hat{X}^{k})\leq g(\hat{X}^{k};\hat{X}^{k})=F(C(\hat {X}^{k}))\;.\] (23)

Here the first inequality is due to the fact that \(g(\hat{X};\hat{X}^{k})\geq F(C(\hat{X}))\) for any \(\hat{X}\). The final equality is because \(g(\hat{X};\hat{X}^{k})=F(C(\hat{X}))\) when \(\hat{X}=\hat{X}^{k}\). Once \(g(\hat{X}^{k};\hat{X}^{k})=g(\hat{X}^{k+1};\hat{X}^{k})\) and thus \(\hat{X}^{k}\in\arg\min_{\hat{X}\in\mathcal{X}^{m}}g(\hat{X};\hat{X}^{k})\), then \(\hat{X}^{k}\) is a global minimizer of the convex upper bound \(g(\cdot;\hat{X}^{k})\) for \(F(C(\cdot))\) and the upper bound \(g(\hat{X}^{k};\hat{X}^{k})\) attains the same function value with \(F(C(\hat{X}^{k}))\). Therefore, if the surrogate function is smooth at \(\hat{X}^{k}\), which could be achieved if \(C(\hat{X}^{k})\) is smooth at \(\hat{X}^{k}\), then \(X^{k}\) is a first-order stationary point of (9). 

Proof of Theorem 5.4.: Because (11) has a unique minimizer, the second inequality in (23) holds strictly when \(\hat{X}^{k+1}\neq\hat{X}^{k}\), or equivalently \(g(\hat{X}^{k+1};\hat{X}^{k})\neq g(\hat{X}^{k};\hat{X}^{k})\). Once \(\hat{X}^{k+1}=\hat{X}^{k}\), then the algorithm terminates. Note that there are only finite possible optimal basic feasible solution \(P^{\star}\) that could be generated by FairWASP, as shown in Lemma A.1. However, before the majority minimization converges, (23) holds strictly and the corresponding \(P_{k}^{\star}\) keeps changing. Therefore, after finite iterations, there must be a \(P_{t}^{\star}\) equal to a previous \(P_{j}^{\star}\) for \(j<t\). When that happens, because the surrogate functions are the same and thus have the same minimizer, \(\hat{X}^{t+1}=\hat{X}^{j+1}\), and the inequalities (23) then hold at equality when \(k=j,j+1,\ldots,t\). This implies that \(g(\hat{X}^{j+1};\hat{X}^{j})=g(\hat{X}^{j};\hat{X}^{j})\), so the algorithm terminates within finite iterations. 

Proof of Proposition 5.5.: For determining the convergence in Wasserstein distance between \(p_{\hat{Z}}\) and \(q_{Z}\), we first use the triangle inequality:

\[\mathcal{W}_{c}(p_{\hat{Z}},q_{Z})\leq\mathcal{W}_{c}(p_{\hat{Z}},p_{Z})+ \mathcal{W}_{c}(p_{Z},q_{Z})=\lambda+\mathcal{W}_{c}(p_{Z},q_{Z})\]

Note that the first term is deterministic, as it is the result of the optimization problem (4). For the second term, we use the result from [22], which implies that with probability \(1-\alpha\) and for the 1-Wasserstein distance:

\[\mathbb{P}(\mathcal{W}_{c}(p_{Z},q_{Z})>\xi)\leq\exp\left(-cn\xi^{1/d}\right),\]

and so by setting the right-hand side equal to \(\alpha\), or equivalently, setting

\[\xi=\sqrt[d]{\frac{c\log(1/\alpha)}{n}},\]

we obtain the first result.

For determining the convergence of the disparity between \(p_{\hat{Z}}\) and \(p_{\hat{Z}}(y,d)\) and \(q_{Y}(y)\), we again first use the triangle inequality from the definition of the disparity \(J\):\[\sup_{y\in\mathcal{Y},d\in\mathcal{D}}J(p_{\hat{Z}}(y|d),q_{Y}(y)) =\sup_{y\in\mathcal{Y},d\in\mathcal{D}}\frac{|p_{\hat{Z}}(y|d)-q_{Y }(y)|}{q_{Y}(y)}\] \[\leq\sup_{y\in\mathcal{Y},d\in\mathcal{D}}\left(\frac{|p_{\hat{Z}} (y|d)-p_{Y}(y)|}{q_{Y}(y)}+\frac{|p_{Y}(y)-q_{Y}(y)|}{q_{Y}(y)}\right)\] \[\leq\sup_{y\in\mathcal{Y},d\in\mathcal{D}}\frac{|p_{\hat{Z}}(y|d)- p_{Y}(y)|}{q_{Y}(y)}+\sup_{y\in\mathcal{Y}}\frac{|p_{Y}(y)-q_{Y}(y)|}{q_{Y}(y)}\]

As the minimum of the marginal distribution of \(q_{Y}(y)\) is bounded away from zero \(\min_{y\in\mathcal{Y}}q_{Z}(y)=\rho>0\), and since by the optimization problem (4) we have \(J(p_{\hat{Z}}(y|d),p_{Y}(y))\leq\epsilon\) for all \(y\in\mathcal{Y},d\in\mathcal{D}\):

\[\sup_{y\in\mathcal{Y},d\in\mathcal{D}}J(p_{\hat{Z}}(y|d),q_{Y}(y)) \leq\sup_{y\in\mathcal{Y},d\in\mathcal{D}}\frac{|p_{\hat{Z}}(y|d) -p_{Y}(y)|}{p_{Y}(y)}\frac{p_{Y}(y)}{q_{Y}(y)}+\sup_{y\in\mathcal{Y}}\frac{|p_ {Y}(y)-q_{Y}(y)|}{q_{Y}(y)}\] \[\leq\sup_{y\in\mathcal{Y},d\in\mathcal{D}}J(p_{\hat{Z}}(y|d),p_{Y }(y))\frac{1}{\rho}+\sup_{y\in\mathcal{Y}}\frac{|p_{Y}(y)-q_{Y}(y)|}{q_{Y}(y)}\] \[\leq\frac{\epsilon}{\rho}+\sup_{y\in\mathcal{Y}}\frac{|p_{Y}(y)-q _{Y}(y)|}{q_{Y}(y)}.\]

Note that the first term is deterministic, while the second one is not, as we need to account for the uncertainty of observing \(n\) i.i.d. samples \(\{Z_{i}\}_{i=1}^{n}\). For the second part, we use the Dvoretzky-Kiefer-Wolfowitz (DKW, [17]) inequality:

\[\mathbb{P}\left(\sup_{y\in\mathcal{Y}}\frac{|p_{Y}(y)-q_{Y}(y)|}{ q_{Y}(y)}>\xi\right) \leq\mathbb{P}\left(\sup_{y\in\mathcal{Y}}|p_{Y}(y)-q_{Y}(y)|> \xi\rho\right)\] \[\leq 2\exp(-2n\rho^{2}\xi^{2})\]

By setting the right-hand side equal to \(\alpha\), or equivalently, setting

\[\xi=\sqrt{\frac{\log(\frac{2}{\alpha})}{2n\rho^{2}}},\]

we obtain the second result.

Finally, we note that the assumption on the marginal distribution of \(q_{Y}\) is bounded away from zero, i.e., \(\rho>0\), is reasonable as the outcome is a discrete (usually binary) random variable. This assumption would be much more restricting in case \(y\) was a continuous random variable (e.g., in regression settings). 

### Downstream Learning using Fwc

Overall, the hardness of deriving bounds for the synthetic representatives provided by FWC can be analyzed using the following breakdown, which is adapted from [78]. Consider the given dataset \(Z=\{(X_{i},Y_{i},D_{i})\}_{i=1}^{n}\), the synthetic representatives obtained using FWC\(\hat{Z}=\{(\hat{X}_{j},\hat{Y}_{j},\hat{D}_{j})\}_{j=1}^{m}\) and \(h\in\mathcal{H}=L^{2}(\mathcal{X}\times\mathcal{Y}\times\mathcal{D})\), the set of measurable square-integrable function in \(L^{2}\). If we consider the downstream learning process using FWC samples over the \(L^{\frac{\rho}{2}}\) space:

\[\inf_{h\in\mathcal{H}}\mathbb{E}_{Y|X,D}\left[\|Y-h(\hat{X},\hat{D})\|_{2}^{2} \right],\]

then the above can be expanded in two terms, due to the property of the conditional expectation being an orthogonal operator in \(\mathcal{H}\):\[\inf_{h\in\mathcal{H}} \mathbb{E}_{Y|X,D}\left[\|Y-h(\hat{X},\hat{D})\|_{2}^{2}\right]=\] \[=\underbrace{\mathbb{E}_{Y|X,D}\left[\|Y-\mathbb{E}_{X,D}[\hat{Y}| \hat{X},\hat{D}]\|_{2}^{2}\right]}_{\text{FVC Approximation Error}}+\underbrace{\inf_{h\in\mathcal{H}} \mathbb{E}_{Y|X,D}\left[\|\mathbb{E}_{X,D}[\hat{Y}|\hat{X},\hat{D}]-h(\hat{X}, \hat{D})\|_{2}^{2}\right]}_{\text{Learning with FVC Samples}}\] (24)

The first term corresponds to the loss of information in approximating \(Y\) with \(\hat{Y}\) via the FVC approach, and is actually independent of any downstream learning. This condition requires for the first moment (which for the binary \(Y\) case is equivalent to the joint distribution) of \(Y\) and \(\hat{Y}\) to be as close as possible. Using FVC this is enforced by minimizing the Wasserstein distance. Indeed, if in definition (1) one restricts to couplings that admit marginal and conditional distributions, then the conditional distributions of \(Y|X,D\) and \(\hat{Y}|\hat{X},\hat{D}\) are upper bounded in Wasserstein sense by the Wasserstein distance between the joint distribution of \(p_{Z}\) and \(p_{\hat{Z}}\)[37].

The second terms refers to the training process using FVC samples Firstly, by using the equivalence in [78], the second term is equivalent to \(\inf_{h\in\mathcal{H}}\mathbb{E}_{Y|X,D}\left[\|\hat{Y}-f(\hat{X},\hat{D}\|_{ 2}^{2}\right]\), which correspond the finding the best \(L^{2}\) function to approximate the distribution of \(\hat{Y}\). This fact implies that using FVC samples is indeed mathematically equivalent to the learning task for the original \(Y\). However, this second term also highlights the hardness of developing learning bounds, as the FVC synthetic representatives \(\hat{Z}=\{(\hat{X}_{j},\hat{Y}_{j},\hat{D}_{j})\}_{j=1}^{m}\) are not i.i.d., and hence standard bounds are not applicable.

## Appendix C Experiment Details

### Runtime Analysis on Synthetic Dataset

As mentioned in Section 7, we generate a synthetic dataset in which one feature is strongly correlated with the protected attribute \(D\) to induce a backdoor dependency on the outcome. We consider a binary protected attribute, \(D\in\{0,1\}\), which could indicate e.g., gender or race. The synthetic dataset contains two features, a feature \(X_{1}\) correlated with the protected attribute and a feature \(X_{2}\) uncorrelated with the protected attribute. For \(D=0\), \(X_{1}\) is uniformly distributed in \([0,10]\), while for \(D=1\), \(X_{1}=0\). Instead, \(X_{2}\) is \(5\) times a random variable from a normal distribution \(\mathcal{N}(0,1)\). Finally, the outcome \(Y\) is binary, so \(Y=\{0,1\}\): \(Y_{i}=1\) when \(Y_{i}>m_{x}+\epsilon_{i}\) and \(Y_{i}=0\) when \(Y_{i}\leq m_{x}+\epsilon_{i}\), where \(m_{x}\) is the mean of \(\{(X_{1})_{i}+(X_{2})_{i}\}_{i}\) and the noise \(\epsilon_{i}\) comes from a normal distribution \(\mathcal{N}(0,1)\).

This experiment visualizes the speed of our method with respect to different numbers of overall samples \(n\), number of samples in the compressed dataset \(m\), and the dimensionality of features \(p\). We evaluated the performance of the algorithm under the synthetic data with different configurations of \(n\), \(p\), and \(m\). In this experiment, we set compute the fair Wasserstein coreset under the \(l_{1}\)-norm distance and we use k-means [42] to initialize the starting coreset \(\hat{X}^{0}\). We terminate the algorithm when \(\hat{X}^{k}=\hat{X}^{k-1}\). The time per iteration and total iterations for varying \(n\), \(p\), and \(m\) are shown in the Figures 1 (top left) and 2. We see that increasing the sample size of the original dataset \(n\) increases the runtime and number of iterations, while increasing the number of coresets \(m\) or dimensionality of the features \(p\) reduces the overall numbers of iterations but increases each iteration's runtime. Additionally, for the setting of Figure 1 (top left), in which we vary the dataset size \(n\), Table 2 provides FVC average runtimes from \(n=500\) to \(n=1,000,000\). We compare FVC runtimes with the runtime at \(n=500\) (our lowest dataset size in the experiment) extrapolated (i) linearly, with a factor of 1, (ii) linearly, with a factor of 10 and (iii) quadratically. We can see that the complexity is near linear and less than quadratic with respect to the dataset size \(n\), although the rate indeed seem to increase for \(n\) at \(500,000\) and above, which can be attributed to the increasing number of iterations required to achieve convergence. This is akin to the phenomenon well-known for k-means, for which in larger datasets k-means might take an exponentially large number of iterations to terminate [73].

In practice, a fixed number of overall iterations is set to avoid this case: _sklearn_ sets it to \(300\)6, _feiss_ to \(25\)7 and _Matlab_ to \(100\)8.

Footnote 6: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html

Footnote 7: https://faiss.ai/cpp_api/struct/structfaiss_1_1Clustering.html

Footnote 8: https://www.mathworks.com/help/stats/kmeans.html

### Real Datasets

We consider the following four real datasets widely used in the fairness literature [19]:

* the _Adult dataset_[7], which reports demographic data from the 1994 US demographic survey about \(\sim 49,000\) individuals. We use all the available features for classification apart from the "fnlwgt" feature, including gender as the protected attribute \(D\) and whether the individual salary is more than USD\(50,000\);
* the _Drug dataset_[20], which contains drug usage history for \(1,885\) individuals. Features \(X\) include the individual age, country of origin, education and scores on various psychological test. We use the individual gender as the protected variable \(D\). The response \(Y\) is based on whether the individual has reported to have ever used the drug "cannabis" or not;
* the _Communities and Crime dataset_[64] was put together towards the creation of a software tool for the US police department. The dataset contains socio-economic factors for \(\sim 2,000\) communities in the US, along with the proportion of violent crimes in each community. As protected attribute \(D\), we include whether the percentage of the black population in the community is above the overall median. For the response \(Y\), we use a binary indicator of whether the violent crimes percentage level is above the mean across all communities in the dataset;

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline
**Dataset size \(n\)** & **Runtime [seconds]** & **Linear (Greiter 1)** & **Actual / Linear (Greiter 1)** & **Linear (Greiter 10)** & **Actual / Linear (Greiter 10)** & **Quadratic** & **Actual / Quadratic** \\ \hline
500 & 8.9e-01 & - & - & - & - & - & - \\ \hline
1,000 & 9.4e-01 & 1.8e+00 & 0.52 & 1.8e+01 & 0.05 & 3.6e+00 & 0.26 \\ \hline
2,500 & 2.2e-01 & 4.5e+00 & 0.49 & 4.5e+01 & 0.05 & 2.2e+01 & 0.10 \\ \hline
5,000 & 1.6e+01 & 8.9e+01 & 1.17 & 8.9e+01 & 0.12 & 8.9e+01 & 0.12 \\ \hline
10,500 & 1.8e+01 & 1.9e+01 & 0.95 & 1.9e+02 & 0.09 & 3.9e+02 & 0.05 \\ \hline
24,500 & 8.7e+01 & 4.4e+01 & 1.99 & 4.4e+02 & 0.20 & 2.1e+03 & 0.04 \\ \hline
48,500 & 3.0e+02 & 6.6e+01 & 3.46 & 8.6e+02 & 0.35 & 8.4e+03 & 0.04 \\ \hline
75,600 & 7.0e+02 & 1.3e+02 & 5.25 & 1.3e+03 & 0.52 & 2.0e+04 & 0.03 \\ \hline
100,000 & 1.0e+03 & 1.8e+02 & 5.79 & 1.8e+03 & 0.58 & 3.6e+04 & 0.03 \\ \hline
250,000 & 3.8e+03 & 4.5e+02 & 8.51 & 4.5e+03 & 0.85 & 2.2e+05 & 0.02 \\ \hline
800,000 & 1.6e+04 & 8.9e+02 & 18.34 & 8.9e+03 & 1.83 & 8.9e+05 & 0.02 \\ \hline
750,000 & 2.9e+04 & 1.3e+03 & 21.64 & 1.3e+04 & 2.16 & 2.0e+05 & 0.01 \\ \hline
1,000,000 & 4.9e+04 & 1.8e+03 & 27.58 & 1.8e+04 & 2.7e & 3.6e+05 & 0.01 \\ \hline \end{tabular}
\end{table}
Table 2: Average runtimes for FWC in the same settings as Figures 1 (top left), varying the dataset size \(n\) while fixing \(m=250\) and \(p=25\), compared with the runtimes for the smallest dataset extrapolated (i) linearly, with a factor of 1, (ii) linearly, with a factor of 10 and (iii) quadratically. FWC enjoys a near linear time complexity, increasing with the largest dataset sizes; this phenomenon is shared with other clustering algorithm such as k-means (see text).

Figure 2: Runtime analysis of FWC when varying the size of the coreset \(m\) (left) and the dimensionality of the features \(p\) (right). We report averages and one standard deviation computed over 10 runs.

* the _German Credit dataset_[28] reports a set of \(1,000\) algorithmic credit decisions from a regional bank in Germany. We use all the available features, including gender as protected attribute \(D\) and whether the credit was approved as response \(Y\).

As perfect demographic parity achieves a value of \(0\) for the discrepancy \(J\), so we have included demographic "dis"-parity to indicate any deviation from demographic parity. Across all experiments we compute demographic parity as the following absolute difference:

\[DD\ \stackrel{{\text{def.}}}{{=}}\ \left|p\left(h(X,D)=1|D=1 \right)-p(h(X,D)=1|D=0)\right|,\] (25)

for a given classifier \(h\) and a protected attribute \(D\) with two levels; the larger this difference, the larger the disparity. We also include the implementation and hyper-parameters of the methods used in the fairness-utility tradeoffs throughout the experiments in this paper:

* For FWC  we set the fairness violation hyper-parameter \(\epsilon\) of problem in Equation (5) to be \(\epsilon=[0.01,0.05,0.1]\), hence obtaining three separate FWC  models, FWC \((0.01)\), FWC \((0.05)\) and FWC \((0.1)\);
* For Fairlet[4], we use the implementation available at the following GitHub repository: https://github.com/talwagner/fair_clustering/tree/master
* For IndFair[12] and K-Median Coresets[3], we use the implementation available at the following GitHub repository: https://github.com/jayeshchoudhari/CoresetIndividualFairness/tree/master
* For k-means [42] and k-medoids [58; 45] we use the implementations available in the Python package Scikit-Learn[59]

All computations are run on an Ubuntu machine with 32GB of RAM and 2.50GHz Intel(R) Xeon(R) Platinum 8259CL CPU. For all datasets, we randomly split \(75\%\) of the data into training/test set, and change the split during each separate run; the training data are further separated into training and validation with \(90/10\) to compute early stopping criteria during training. The downstream classifier used is a one-layer deep multi-layer perceptron (MLP) with \(20\) hidden layers, ReLu activation function in the hidden layer and softmax activation function in the final layer. Unless stated otherwise, FWC uses the \(L^{1}\) to compute the distance from the original datasets in the optimization problem. For the downstream classifier, we use Adam optimizer [38] with a learning rate set to \(10^{-3}\), a batch size of \(32\), a maximum number of epochs set to \(500\) with early stopping evaluated on the separate validation set with a patience of \(10\) epochs and both the features \(X\) and the protected attribute \(D\) are used for training the classifier. Note that due to the size of the Adult dataset, Fairlet coresets [4] could not be run due to the RAM memory required exceeding the machine capacity (32GB). Finally, all uncertainties are reported at \(\pm 1\sigma\) (one standard deviation) in both figures and tables. Uncertainties are computed over a set of 10 runs where the random seed for the algorithm initialization and train/test split was changed, but consistent across all methods (i.e., all methods in the first run were presented the same train/test split across each datasets).

Closeness to the original dataset and clustering performanceTables 3 and 4 include the numerical values (means and standard deviations computed over 10 runs) for all methods in

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_EMPTY:24]

potential approaches. Finally, as in Figure 7, the standard deviations for Adult and Credit dataset are large due to the downstream model becoming trivial.

Figure 4: Fairness-utility tradeoff of downstream MLP classifier trained using the original training set augmented with coresets representatives, following the augmentation strategy from [68], including all methods mentioned in Section 7. Each point shows the best model in terms of fairness-utility tradeoff over various degrees of data augmentation, in addition to the baseline model with no augmentation. Averages and standard deviations computed over 10 runs, with the top panel showing just means and the bottom panel combining both means and standard deviations, with the computed Pareto frontier indicated by the dashed red line.

Figure 3: Fairness-utility tradeoff of downstream MLP classifier trained using the original training set augmented with coresets representatives, following the augmentation strategy from [68]. Each point shows the best model in terms of fairness-utility tradeoff over various degrees of data augmentation, in addition to the baseline model with no augmentation. Means and standard deviations taken over 10 runs, with the computed Pareto frontier indicated by the dashed red line.

Using FWC to correct biases in LLMsTo query GPT models, test data with 200 samples is provided with a base parity of 0.5 (similar to [76]) and additionally, examples are provided in the case of the few shot settings. Data is fed in as text. One of the tabular data, for example, is: _"A person in 1996 has the following attributes: Age: \(21.0\), workclass: Private, education: Some-college, highest education level: \(10.0\), marital status: Never-married, occupation: Other-service, relationship: Own-child, race: White, sex: Female, capital gain: \(0.0\), capital loss: \(0.0\), hours per week: \(25.0\), native country: United-States"_.

We use the following prompts:

* **Zero shot**: "_Using the provided data, will this person from 1996 be hired at greater than 50,000 USD per year? You must only respond with the word 'yes' or 'no'. Here are 0 examples with the correct answer._"
* **Few shot**: "_Given the provided data, will this person from 1996 be hired at greater than 50,000 USD per year? You must only respond with the word 'yes' or 'no'. Here are \(n\) examples with the correct income level for a person in 1996. Make sure you use the examples as a reference_", where \(n\) is the number of demographically balanced samples.
* **Few shot** (FWC): "_Given the provided data, will this from 1996 be hired at greater than 50,000 USD per year? You must only respond with the word 'yes' or 'no'. Here are \(n\) examples with the correct income level for a person in 1996, along with weights in the

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|} \hline
**Method** & \multicolumn{3}{c|}{Adblbauer} & \multicolumn{3}{c|}{Credit Dataset} & \multicolumn{3}{c|}{Credit Dataset} & \multicolumn{3}{c|}{Credit Dataset} & \multicolumn{3}{c|}{Credit Dataset} \\ \hline  & ED (\(\Delta\)) & AUC (\(\Gamma\)) & Tradeoff (\(\Delta\)) & DD (\(\Delta\)) & AUC (\(\Gamma\)) & Tradeoff (\(\Delta\)) & DD (\(\Delta\)) & AUC (\(\Gamma\)) & Tradeoff (\(\Delta\)) & DD (\(\Delta\)) & AUC (\(\Gamma\)) & Tradeoff (\(\Delta\)) \\ \hline Baseline (No Aug.) & **0.122 \(\pm\) 0.08** & 0.06 \(\pm\) 0.05 & 0.36 \(\pm\) 0.08 & 0.06 \(\pm\) 0.06 & 0.06 \(\pm\) 0.01 & 0.31 \(\pm\) 0.01 & 0.45 \(\pm\) 0.05 & **0.54 \(\pm\) 0.05** & **0.53 \(\pm\) 0.03** & **0.54 \(\pm\) 0.04** & **0.53 \(\pm\) 0.03** & **0.54 \(\pm\) 0.02** & **0.54 \(\pm\) 0.03** \\ \hline FWC (\(\epsilon\)) & 0.13 \(\pm\) 0.03 & 0.64 \(\pm\) 0.07 & 0.33 \(\pm\) 0.08 & **0.06 \(\pm\) 0.05** & 0.71 \(\pm\) 0.12 & **0.32 \(\pm\) 0.11** & **0.43 \(\pm\) 0.04** & **0.04 \(\pm\) 0.03** & 0.519 \(\pm\) 0.005 & **0.54 \(\pm\) 0.03** & 0.52 \(\pm\) 0.02 \\ \hline FWC (\(\epsilon\)) & 0.14 \(\pm\) 0.03 & **0.79 \(\pm\) 0.09** & 0.33 \(\pm\) 0.08 & 0.08 \(\pm\) 0.05 & 0.76 \(\pm\) 0.09 & 0.23 \(\pm\) 0.09 & 0.44 \(\pm\) 0.03 & 0.519 \(\pm\) 0.006 & **0.44 \(\pm\) 0.03** & 0.21 \(\pm\) 0.00 & **0.54 \(\pm\) 0.02** & 0.30 \(\pm\) 0.02 \\ \hline FWC (\(\epsilon\)) & 0.13 \(\pm\) 0.02 & **0.79 \(\pm\) 0.07** & **0.32 \(\pm\) 0.08** & 0.07 \(\pm\) 0.07 & 0.77 \(\pm\) 0.09 & 0.23 \(\pm\) 0.04 & 0.44 \(\pm\) 0.03 & 0.517 \(\pm\) 0.004 & 0.55 \(\pm\) 0.03 & 0.55 \(\pm\) 0.02 & 0.55 \(\pm\) 0.02 \\ \hline FFN (\(\epsilon\)-Means) & - & - & - & 0.11 \(\pm\) 0.07 & **0.78 \(\pm\) 0.09** & 0.23 \(\pm\) 0.10 & 0.45 \(\pm\) 0.03 & 0.519 \(\pm\) 0.003 & 0.54 \(\pm\) 0.03 & 0.53 \(\pm\) 0.03 & 0.53 \(\pm\) 0.03 & 0.53 \(\pm\) 0.03 \\ \hline Half Set. & **0.12 \(\pm\) 0.08** & 0.06 \(\pm\) 0.06 & 0.34 \(\pm\) 0.09 & 0.00 \(\pm\) 0.06 & **0.76 \(\pm\) 0.09** & 0.23 \(\pm\) 0.09 & 0.45 \(\pm\) 0.08 & 0.519 \(\pm\) 0.005 & 0.54 \(\pm\) 0.04 & **0.53 \(\pm\) 0.02** & 0.55 \(\pm\) 0.02 \\ \hline Values Classifier & **0.422 \(\pm\) 0.08** & 0.06 \(\pm\) 0.07 & 0.37 \(\pm\) 0.07 & 0.08 \(\pm\) 0.06 & 0.77 \(\pm\) 0.09 & 0.23 \(\pm\) 0.09 & 0.45 \(\pm\) 0.08 & 0.54 \(\pm\) 0.04 & 0.55 \(\pm\) 0.04 & 0.519 \(\pm\) 0.005 & 0.53 \(\pm\) 0.02 & 0.55 \(\pm\) 0.02 \\ \hline K-Models & **0.12 \(\pm\) 0.04** & 0.68 \(\pm\) 0.06 & 0.35 \(\pm\) 0.08 & 0.06 \(\pm\) 0.06 & 0.77 \(\pm\) 0.09 & 0.23 \(\pm\) 0.09 & 0.44 \(\pm\) 0.03 & 0.52 \(\pm\) 0.004 & 0.55 \(\pm\) 0.03 & 0.59 \(\pm\) 0.01 & 0.53 \(\pm\) 0.01 & 0.52 \(\pm\) 0.02 \\ \hline \end{tabular}
\end{table}
Table 7: Demographic disparity (Equation (25)), AUC and fairness-utility tradeoff (Equation (26)) of downstream MLP classifier trained via data augmentation with all fair coresets/clustering methods, across the four real datasets. The best methods across various degrees of data augmentation is shown. The best performing methods for every column is bolded, with averages and standard deviations taken over 10 runs.

Figure 5: Data augmentation fairness-utility tradeoff of downstream MLP classifier for the Drug dataset when the protected attribute \(D\) (gender) is either not included (left) or included (right) as feature in the learning process. As in Figure 3, the best model across various data augmentation degrees is reported, with averages and standard deviations obtained over 10 runs. FWC manages to successfully reduce the demographic disparity when gender is not used as a feature, but fail to do so when gender is used, indicating that gender provides strong predictive power for the outcome in question, which would require enforcing fairness either during model training or by post-processing the outputs.

column weight. Weights are between 0 (minimum) and 1 (maximum). The more the weight, the more important the example is. Make sure you use the examples as a reference._"

In the few shot settings, 16 examples are provided in both cases due to the LLM token limitation; passing fewer examples yields similar results to the ones in Table 1. For FWC, we run a separate coreset generation run (differently from other experiments in Section 7), where we selected \(m=16\) and ensured that the positive class (\(Y=1\)) has an equal number of male and female samples. We also note that while the results from GPT-4 for the zero shot and few shot cases are similar to what was observed by [76], the accuracies reported for the GPT-3.5 Turbo model appear to be lower in our experiments, which points to a potential difference in the exact backend LLM model used for inference.

## Appendix D Limitations of FWC

**Coreset support and non-convex feature spaces** FWC representative do not need to be within the original \(n\) samples, but they do need to share the same support. As shown in Section 4.2, solving line 5 in Algorithm 1 in cases 1. and 2. means the synthetic representatives could fall outside the original dataset. In case 3., the solution has to be selected from the data points already existing in the original dataset (akin to k-medoids). In general, non-convex feature spaces \(\mathcal{X}\) might represent a challenge, as representatives might be generated in zero-density regions (a simple example could be a dataset distributed as a hollow circle or two moons). However, this criticism is also more generally applicable to the existing fair coresets/clustering literature, as well as the k-means algorithms, for which specific adjustments have been developed [63] and could be indeed extended to FWC.

**Limiting the total number of iterations** FWC is not of polynomial time complexity and the total number of iterations might grow faster than linear time when the dataset size is very large, as shown in the synthetic data experiment in Section C.1 and Table 2. This phenomenon is shared with k-means, which is also not of polynomial time complexity and is known to potentially take an exponentially large number of iterations to terminate [73]. As mentioned in Section C.1, a common practice for clustering is set a fixed number of maximum iterations, after which the algorithm is stopped.

**Computational bottlenecks** The main complexity term for FWC is \(\mathcal{O}(mn)\), which comes from establishing the cost matrix in the beginning of the solution of problem (8). This complexity is comparable with what occurs in Lloyd's algorithm for k-means and k-medians. This might be problematic if the cost matrix is too large to be stored directly in memory. In practice, we do not actually need to store the entire matrix, as we only need to compute the largest component for each row of \(C\) for solving problem (8) (see Lemma A.1), so one could further improve the cost of storing the cost matrix. Another option would also be leverage the same approaches used for k-means such as, e.g., cost matrix sketching [79]. Finally, FWC would also benefit from GPU implementations akin to k-means and k-medians, which would substantially accelerate the runtime speed of FWC.

**Connection between \(\epsilon\) and downstream learning** In our definition of demographic parity in Equation 3, the hyper-parameter \(\epsilon\) effectively controls how different the outcome rates across sensitive feature groups \(D\) of the weighted coreset distribution \(p_{\mathcal{Z},\theta}\) can be from the overall outcome rates in the original dataset \(p_{Y_{T}}\). In our experiments (Section 7), we empirically show that limiting the fairness violation in the coresets results in a fairer downstream model. However, when training a downstream model using FWC we induce a distribution shift between the train set and the test set, as the coresets distribution is never identical to the original dataset distribution. Although we have provided some results on the generalization properties of FWC (Proposition 5.5) as well as some intuition about developing downstream learning bounds for FWC in non-i.i.d. settings (Section B.1), theoretically characterize the connection between the fairness violation parameter \(\epsilon\) remains an open question. In essence, the analysis is challenging as the induced distribution shift is dependent on the biases in the original dataset distribution, the coreset size \(m\), the metric chosen for the cost matrix and, ultimately, the fairness violation parameter \(\epsilon\). For this reason, although we have shown that restricting the fairness violation improves downstream models fairness, an explicit characterization of the downstream effects of \(\epsilon\), as well as other hyper-parameters, would require significant further work, beyond the scope of this paper.

**Implications for other fairness measures** As highlighted by [5, Chapter 3], fairness notions in classification settings can be categorized into notions of independence, separation, and sufficiency. Demographic parity falls in the class of the independence notion, and hence, other measures of fairness that are closely related, e.g., disparate impact, would also improve when optimizing for demographic parity. However, other notions of fairness such as separation or sufficiency may not simultaneously be satisfied [5]. As FWC targets demographic parity, it cannot guarantee an improvement in these other measures. To test this, we compute the equalized odds, which falls under the notion of separation, for the downstream classifier in Section 7 and check the performance of FWC compared to the other approaches. Table 8 indicates the datasets in which FWC is part of the Pareto frontier for both demographic parity and equalized odds. When considering equalized odds, FWC is not a part of the Pareto frontier for the Drug dataset, and more generally, FWC performance is not as competitive. This is in contrast to demographic parity: in Figure 1, FWC sits on the Pareto frontier across all datasets for fairness-performance trade-off in downstream classification.

## Appendix E Broader Impact

Our work presents a novel approach to obtain coresets (synthetic representative samples) of a given dataset while reducing biases and disparities in subgroups of the given dataset. As other approaches in the field of algorithmic fairness, our efforts may help populations that would otherwise face disadvantages from a model or decision process. Importantly, our approach refrains from exploiting biases inherent in the data itself; rather, it seeks to mitigate biases in data-driven decision systems. It is crucial to note that our method does not claim to address all sources or types of bias. In addition, while our tools enable a malicious modeler to manipulate algorithmic fairness methods to amplify disparities instead of reducing them, for instance, by reversing the fairness constraint (replacing \(\leq\) with \(\geq\)), the unfairness of a trained model can be detected by assessing it over a separate test set from the original dataset.

\begin{table}
\begin{tabular}{|c||c|c|c||c|c|} \hline  & \multicolumn{3}{c|}{**Pareto Frontier, Demographic Parity**} & \multicolumn{3}{c|}{**Pareto Frontier, Equalized Odds**} \\ \hline
**Dataset** & FWC (\(\epsilon=0.01\)) & FWC (\(\epsilon=0.05\)) & FWC (\(\epsilon=0.1\)) & FWC (\(\epsilon=0.01\)) & FWC (\(\epsilon=0.05\)) & FWC (\(\epsilon=0.1\)) \\ \hline Adult & ✓ & ✓ & & ✓ & \\ \hline Drug & ✓ & ✓ & ✓ & & \\ \hline Crime & ✓ & ✓ & ✓ & & ✓ \\ \hline Credit & ✓ & & & ✓ & ✓ \\ \hline \end{tabular}
\end{table}
Table 8: Presence on the Pareto frontier for FWC across different fairness violation hyper-parameter values (\(\epsilon=\{0.01,0.05,0.1\}\)), for both demographic parity (left) and equalized odds (right) in the downstream learning settings of Section 7. As equalized odds is not an independence notion of fairness as demographic parity, constraints on demographic parity do not guarantee an improvement in equalized odds, resulting in FWC not performing as well for downstream performance-fairness tradeoff when using equalized odds.

Figure 6: Fairness-utility tradeoff of all methods, indicated by AUC and demographic disparity of a downstream MLP classifier across all datasets (rows) and without (left column) or with (right column) fair pre-processing [34] (excluding FWC, to which no fairness modification is applied after coresets have been generated). The Pareto frontier, indicated with a dashed red-line, is computed across all models and coreset sizes. We report the averages over 10 separate train/test splits.

Figure 7: Similarly to Figure 6, we report the means and standard deviations over 10 runs of the fairness-utility tradeoff of all methods, indicated by AUC and demographic disparity of a downstream MLP classifier across all datasets (rows) and without (left column) or with (right column) fair preprocessing [34] (excluding FWC, to which no fairness modification is applied after coresets have been generated).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Section 4 includes the details on the majority minimization algorithm for FWC, with Section 6 showing the equivalence between the unconstrained version of FWC and Lloyd's algorithm for k-medians and k-means. Section 7 includes all experiments, which are presented in the same order as the conclusions presented in the abstract, with the synthetic data experiment first (Figures 1, top left, 2 and Table 2), downstream learning and data augmentation using synthetic representatives (Figures 1, 3, 4, 6, 7 and Tables 3, 4, 5, 6, 7) and reducing biases in prediction from large language models (Table 1). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See the Limitation section in Appendix D. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: See Section 5 and Appendix B for theorems and proposition statements and proof, respectively. The only exception is Proposition 2.1, which is stated in Section 2, Lemma A.1, which is stated and proved in Appendix A and Lemma A.2, which proof follows directly from [77, Corollary 5] by extending to the case in which \(m\neq n\). Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Section 4 provides the algorithm outline, with Algorithm 1 providing the step-by-step breakdown of the algorithm. The variant of FairWASP for line 4 in Algorithm 1 uses the same algorithm as [77] with \(m\neq n\), while implementation details for the experiments, along with hyper-parameters for each methods can be found in Section 7 and Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: Data are openly available and details to reproduce the main experimental results are provided in Section 7 and Appendix C. The code is not available publicly at the moment. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Implementation details are provided in Section 7, Appendix C.1 for the runtime analysis with synthetic data and Appendix C.2 for the downstream learning and data augmentation experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes]Justification: Yes, we provide commentary in Appendix C.2, and include \(1\sigma\) uncertainty computed over 10 runs with separate seeds in Figures 1 (top left), 2, 3 (bottom row), 4 (bottom row), 5 and 7 and Tables 1, 2, 3, 4, 5, 6 and 7. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: For non-LLM experiment, the computing environment is reported in Appendix C ( Ubuntu machine with 32GB of RAM and 2.50GHz Intel(R) Xeon(R) Platinum 8259CL CPU). LLM experiments require access to the OpenAI API to query GPT-3.5 and GPT-4. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We include reproducibility details, data are publicly available and we address the broader impact and societal implications in Section E. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We address the broader impact of our method in Section E. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Not applicable, this paper is not releasing public data, image generator or pretrained language models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **License for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes]Justification: All datasets are publicly available, and GPT-3.5 and GPT-4 were access through OpenAI API. All sources were cited in the paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets were introduced by this work. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing or human subjects were included in this work. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]

Justification: No human subjects were involved in this study.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.