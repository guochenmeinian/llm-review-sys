# Zeroth-Order Methods for Nondifferentiable, Nonconvex, and Hierarchical Federated Optimization

 Yuyang Qiu

Dept. of Industrial and Systems Engg.

Rutgers University

yuyang.qiu@rutgers.edu

&Uday V. Shanbhag

Dept. of Industrial and Manufacturing Engg.

Pennsylvania State University

udaybag@psu.edu

&Farzad Yousefian

Dept. of Industrial and Systems Engg.

Rutgers University

farzad.yousefian@rutgers.edu

###### Abstract

Federated learning (FL) has emerged as an enabling framework for communication-efficient decentralized training. We study three broadly applicable problem classes in FL: (i) Nondifferentiable nonconvex federated optimization; (ii) Federated bilevel optimization; (iii) Federated minimax problems. Notably, in an implicit sense, both (ii) and (iii) are instances of (i). However, the hierarchical problems in (ii) and (iii) are often complicated by the absence of a closed-form expression for the implicit objective function. Unfortunately, research on these problems has been limited and afflicted by reliance on strong assumptions, including the need for differentiability and L-smoothness of the implicit function. We address this shortcoming by making the following contributions. In (i), by leveraging convolution-based smoothing and Clarke's subdifferential calculus, we devise a randomized smoothing-enabled zeroth-order FL method and derive communication and iteration complexity guarantees for computing an approximate Clarke stationary point. To contend with (ii) and (iii), we devise a unified randomized implicit zeroth-order FL framework, equipped with explicit communication and iteration complexities. Importantly, our method utilizes delays during local steps to skip making calls to the inexact lower-level FL oracle. This results in significant reduction in communication overhead when addressing hierarchical problems. We empirically validate the theory on nonsmooth and hierarchical ML problems.

## 1 Introduction

Federated learning (FL) has recently emerged as a promising enabling framework for learning predictive models from a multitude of distributed, privacy-sensitive, and possibly, heterogeneous datasets. This is accomplished through the use of efficiently devised periodic communications between a central server and a collection of clients. The FL algorithmic framework allows for addressing several key obstacles in the development and implementation of standard machine learning methods in a distributed and parallel manner. For instance, the conventional parallel stochastic gradient descent (SGD) method requires the exchange of information among the computing nodes at every single time step, resulting in excessive communication overhead. In contrast, FL methods including FedAvg [34] and Local SGD [46] overcome this onerous communication bottleneck by provably attaining the linear speedup of parallel SGD by using a significantly fewer communication rounds [18; 54; 24; 7]. These guarantees have been further complemented by recent efforts [26;38] where the presence of both data heterogeneity (i.e., variability of local datasets) and device heterogeneity (i.e., variability of edge devices in computational power, memory, and bandwidth) have been addressed. Despite recent advances, much needs to be understood about designing communication-efficient decentralized methods for resolving three broadly applicable problem classes, each of which is presented next.

(a) _Nondifferentiable nonconvex locally constrained FL_. Consider the prototypical FL setting:

\[\min_{x}\quad\left\{\,f(x)\triangleq\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}_{\xi_{ i}\in\mathcal{D}_{i}}[\,\bar{f}_{i}(x,\xi_{i})\,]\,|\,x\,\in\,X\,\triangleq\,\bigcap_{i=1}^ {m}X_{i}\,\right\},\] ( **FL \[{}_{nn}\]**)

where \(f\) is a nonsmooth nonconvex function and is associated with a group of \(m\) clients indexed by \(i\in[\,m\,]\triangleq\{1,\dots,m\}\), \(\mathcal{D}_{i}\) denotes the local dataset, \(\bar{f}_{i}:\mathbb{R}^{n}\times\mathcal{D}_{i}\rightarrow\mathbb{R}\) is the local loss function, and \(X_{i}\subseteq\mathbb{R}^{n}\) is an easy-to-project local constraint set. Notably, local datasets may vary across clients, allowing for data heterogeneity. We also consider client-specific local sets to induce personalization.

(b) _Nondifferentiable nonconvex bilevel FL_. Overlaying a bilevel term in (**FL\({}_{nn}\)**) leads to

\[\min_{x\,\in\,X\,\triangleq\,\bigcap_{i=1}^{m}X_{i}}\left\{f(x)\,\mid\,y(x) \in\text{arg}\,\min_{y\in\mathbb{R}^{n}}\quad\frac{1}{m}\sum_{i=1}^{m}\mathbb{ E}_{\zeta_{i}\in\mathcal{D}_{i}}[\bar{h}_{i}(x,y,\zeta_{i})]\right\},\] ( **FL \[{}_{bl}\]**)

where \(f(\bullet)\triangleq\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}_{\xi_{i}\in\mathcal{D} _{i}}[\,\bar{f}_{i}(\bullet,y(\bullet),\xi_{i})\,]\) denotes the implicit objective function and \(y(\bullet):\mathbb{R}^{n}\rightarrow\mathbb{R}^{\bar{n}}\) is a single-valued map returning the unique solution to the lower-level problem at \(x\).

(c) _Nondifferentiable nonconvex minimax FL_. Finally, we consider the minimax setting, defined as

\[\min_{x\in X\,\triangleq\,\cap_{i=1}^{m}X_{i}}\max_{y\in\mathbb{R}^{\bar{n}}} \quad\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}_{\xi_{i}\in\mathcal{D}_{i}}[\,\bar{f }_{i}(x,y,\xi_{i})\,].\] ( **FL \[{}_{mm}\]**)

where we assume that \(y(x)\in\text{arg}\max_{y\in\mathbb{R}^{\bar{n}}}\frac{1}{m}\sum_{i=1}^{m} \mathbb{E}_{\zeta_{i}\in\bar{\mathcal{D}}_{i}}[\,\bar{f}_{i}(x,y,\xi_{i})]\) is unique for all \(x\). Let \(f(\bullet)\triangleq\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}_{\xi_{i}\in\mathcal{D} _{i}}[\,\bar{f}_{i}(\bullet,y(\bullet),\xi_{i})\,]\) denote the implicit objective function. Indeed, problem (**FL \[{}_{bl}\]**) subsumes this minimax formulation when we choose \[\bar{h}_{i}:=-\bar{f}_{i}\] and \[\bar{\mathcal{D}}_{i}:=\mathcal{D}_{i}.\]

Notably, in an implicit sense, both (b) and (c) are instances of problem (a). However, these hierarchical problems are often complicated by the absence of a closed-form expression for the implicit objective, denoted by \(f(\bullet)\). Indeed, \(f(\bullet)\) is often nonsmooth, nonconvex, and unavailable. As such, the absence of both zeroth and first-order information of \(f(\bullet)\) in problems (b) and (c) makes the design and analysis of FL methods for these problems more challenging than that for (a).

**Gaps.** To the best of our knowledge, there are no known efficient FL algorithms that can contend with both **nonsmoothness** and **nonconvexity** in an unstructured sense. Generalizations that can accommodate either a **bilevel** or a **minimax** structure also remain unaddressed in FL.

**Goal.**_To develop a unified FL framework accommodating nondifferentiable nonconvex settings with extensions allowing for bilevel or minimax interactions_. We now describe the proposed framework.

### A Smoothed Sampled Zeroth-order Framework

We consider a smoothed framework for contending with constrained, nonsmooth, and nonconvex regimes. Specifically, given that \(f\) is an expectation-valued function and \(X\) is a closed and convex set, both of which are defined in (**FL\({}_{nn}\)**), a smoothed unconstrained approximation is given as follows.

\[\left\{\begin{aligned} &\min\,\frac{1}{m}\sum_{i=1}^{m} \mathbb{E}_{\xi_{i}}[\,\bar{f}_{i}(x,\xi_{i})\,]\\ &\text{subject to }\,x\,\in\,\cap_{i=1}^{m}X_{i}.\end{aligned}\right\} \equiv\left\{\min\,\frac{1}{m}\sum_{i=1}^{m}[\,\mathbb{E}_{\xi_{i}}[\,\bar{f }_{i}(x,\xi_{i})\,]+\underbrace{\mathbb{I}_{X_{i}}(x)}_{\text{Indicator function}}]\,\right\}\] ( **FL \[{}_{nn}^{\eta}\]**)

If \(f\) is as defined in (**FL\({}_{nn}\)**) and \(d(x)\,\triangleq\,\frac{1}{m}\sum_{i=1}^{m}\mathbb{I}_{X_{i}}(x)\), then **f** and its smoothing \(\mathbf{f}^{\eta}\) are defined as

\[\mathbf{f}(x)\,\triangleq\,f(x)+d(x)\text{ and }\mathbf{f}^{\eta}(x)\, \triangleq\,f^{\eta}(x)+d^{\eta}(x),\] (1) \[\text{where }f^{\eta}(x)\,\triangleq\,\frac{1}{m}\sum_{i=1}^{m}[\, \mathbb{E}_{u_{i}\,\in\,\mathbb{B}}[\,\mathbb{E}_{\xi_{i}}[\,\bar{f}_{i}(x+ \eta u_{i},\xi_{i})\,]]]\text{ and }d^{\eta}(x)\,\triangleq\,\frac{1}{m}\sum_{i=1}^{m}\mathbb{I}_{X_{i}}^{ \eta}(x).\](i) _Clarke-stationarity_. Consider the original problem (**FL\({}_{nn}\)**). Under the assumption that the objective of (**FL\({}_{nn}\)**) is Lipschitz continuous, then Clarke-stationarity of \(x\) w.r.t. (**FL\({}_{nn}\)**) requires that \(x\) satisfies \(0\,\in\,\partial f(x)+\mathcal{N}_{\cap_{i=1}^{m}X_{i}}(x)\), where \(\partial f(x)\) represents the Clarke generalized gradient [3] of \(f\) at \(x\). However, a negative result has been provided regarding the efficient computability of an \(\epsilon\)-stationary solution in nonsmooth nonconvex regimes [56]. Consequently, we focus on the smoothed counterpart (**FL\({}_{nn}^{\eta}\)**), a smooth nonconvex problem. In fact, under suitable conditions, it can be shown that stationary point of (**FL\({}_{nn}^{\eta}\)**) is a \(2\eta\)-Clarke stationary point of the original problem, i.e.

\[[\,0\,\in\,\partial\mathbf{f}^{\eta}(x)\,]\,\,\implies\,[\,0\,\in\,\partial_{ 2\eta}\mathbf{f}(x)\,],\] (2)

where \(\partial_{2\eta}\mathbf{f}(x)\) represents the \(2\eta\)-Clarke generalized gradient of \(\mathbf{f}\) at \(x\).

(ii) _Meta-scheme for efficient resolution of (**FL\({}_{nn}^{\eta}\)**). We develop zeroth-order stochastic gradient schemes for resolving (**FL\({}_{nn}^{\eta}\)**). This requires a zeroth-order gradient estimator for \(f^{\eta}(x)\), denoted by \(\frac{1}{m}\sum_{i=1}^{m}g_{i}^{\eta}(x,\xi_{i},v_{i})\) where \(v_{i}\,\in\,\eta\mathbb{S}\) for \(i\,\in\,[m]\) and \(\mathbb{S}\) denotes the surface of the unit ball. Note that the Moreau smoothing of the indicator function of \(X_{i}\), denoted by \(\mathbb{I}_{X_{i}}^{\eta}(x)\), admits a gradient, defined as \(\nabla_{x}\mathbb{I}_{X_{i}}^{\eta}(x)=\frac{1}{\eta}(x-\mathcal{P}_{X_{i}}(x ))\), where \(\mathcal{P}_{X_{i}}(x)\triangleq\operatorname*{arg\,min}_{y\in X_{i}}\|y-x \|^{2}\). The resulting _meta-scheme_ is defined next.

\[x_{k+1}\,=\,x_{k}-\gamma\left(\tfrac{1}{m}\sum_{i=1}^{m}\left(g_{i}^{\eta}(x_ {k},\xi_{i,k},v_{i,k})+\tfrac{1}{\eta}\left(x_{k}-\mathcal{P}_{X_{i}}(x_{k}) \right)\right)\right),\,k\geq 0.\] (**Meta-ZO**)

(iii) _Inexact implicit generalizations for (_**FL\({}_{bl}\)**) and (**FL\({}_{mm}\)**). In addressing the bilevel problem, unlike in (**FL\({}_{nn}\)**), the clients in (**FL\({}_{bl}\)**) may not have access to the exact evaluation of the implicit local objective \(\tilde{f}_{i}(\bullet,y(\bullet),\xi_{i})\). This makes a direct extension of FL schemes challenging. This is because the evaluation of the implicit local function necessitates exact resolution of the lower-level problem. We address this challenge by developing _inexact_ implicit variants of the zeroth-order scheme, where clients compute only an \(\varepsilon\)-approximation of \(y(x)\), denoted by \(y_{\varepsilon}(x)\) in a federated fashion. This inexact framework, described next, is crucial in addressing hierarchy in bilevel FL formulations. Let \(f_{i}^{\eta}(\bullet)\) denote the smoothed implicit local function. We estimate \(\nabla_{x}f_{i}^{\eta}(x)\) by approximating the expectation by sampling in \((a.1)\), as follows, while in \((a.2)\), we replace \(y(x)\) by an inexact form \(y_{\varepsilon}(x)\). This leads to the introduction of \(g_{i}^{\eta,\varepsilon}(x,\xi,v)\) as captured below.

\[\nabla_{x}f_{i}^{\eta}(x)=\underbrace{\mathbb{E}_{\xi_{i},v}\left[g_{i}^{\eta} (x,\xi_{i},v)\right]}_{\text{cannot be tractually evaluated}}\stackrel{{(a.1)}}{{ \approx}}\underbrace{g_{i}^{\eta}(x,\xi_{i,k},v_{T_{r}})}_{\text{inractable since }y(x)\text{ is unavailable}}\stackrel{{(a.2)}}{{ \approx}}g_{i}^{\eta,\varepsilon}(x,\xi_{i,k},v_{T_{r}}),\]

where \(k\) is the local time index, \(T_{r}\) is the global time index of communication round \(r\) (\(k\geq T_{r}\)), and \(g_{i}^{\eta,\varepsilon}(x,\xi,v)\triangleq\frac{n}{\eta}(\tilde{f}_{i}(x+v,y_ {\varepsilon}(x+v),\xi)-\tilde{f}_{i}(x,y_{\varepsilon}(x,\xi))\frac{v}{\|v\|}\) denotes an inexact implicit zeroth-order gradient. Note that at each round of communication at the upper level, \(y_{\varepsilon}(x)\) can be computed using calls to a standard FL method, e.g., FedAvg, in the lower level. Notably, such calls to an FL oracle should be made only at the global step to preserve the communication efficiency of the scheme. It follows that \(g_{i}^{\eta,\varepsilon}(x,\xi_{i,k},v_{T_{r}})=\nabla_{x}f_{i}^{\eta}(x)+ \tilde{e}_{i,\varepsilon}\) where the approximation error \(\tilde{e}_{i,\varepsilon}\) is a possibly biased random variable. This bias can be then controlled by carefully by updating the accuracy level \(\varepsilon\) at each communication round, as we will address in this work.

### Contributions

Our goal lies in extending (**Meta-ZO**) to federated nonsmooth nonconvex optimization and then provide generalizations to bilevel and minimax regimes. In each instance, we intend to provide iteration and communication-complexity bounds for computing an \(\epsilon\)-accurate \(\eta\)-Clarke stationary point of the original problem. Accordingly, we make the following contributions.

(i) _FL for nondifferentiable nonconvex problems._ To address (**FL\({}_{nn}\)**) with heterogeneous datasets, we develop a Randomized Zeroth-Order Locally-Projected Federated Averaging method (FedRZO\({}_{\text{nn}}\)). We derive iteration complexity of \(\mathcal{O}\left(\frac{1}{mc^{2}}\right)\) and communication complexity of \(\mathcal{O}\left(m^{3/4}K^{3/4}\right)\) for computing an approximate Clarke stationary point. Such guarantees appear to be new in the context of resolving nondifferentiable nonconvex FL problems, e.g. in training of ReLU neural networks (see Table 2). This is distinct from existing zeroth-order methods, including FedZO [12], that rely on differentiability and \(L\)-smoothness of the local loss functions.

(ii) _Federated bilevel optimization._ In addressing (**FL\({}_{bl}\)**), we develop FedRZO\({}_{\text{bl}}\), an inexact implicit extension of FedRZO\({}_{\text{nn}}\). By skipping local calls to the lower-level FL oracle, FedRZO\({}_{\text{bl}}\) is a novel communication-efficient FL scheme with single-timescale local steps, resulting in significant reduction in communication overhead. Table 1 summarizes the communication complexity of this scheme. In all cases, we assume heterogeneous data at the upper level. In the lower level, depending on which conventional FL scheme is employed, we obtain the communication complexity accordingly.

(iii) _Federated minimax optimization._ FedRZQb1 can be employed for addressing (**FL\({}_{mm}\)**) where \(\hat{h}_{i}:=-\tilde{f}_{i}\). As such, the complexity results in (ii) hold for solving (nondifferentiable nonconvex)-(strongly concave) FL minimax problems. Such results are new for this class of FL problems.

**Remark 1**.: There has been recent progress in addressing bilevel and minimax problems in FL, including Local SGDA and FedNest [48; 43]. Our work in (ii) and (iii) has two main distinctions with existing FL methods, described as follows. (1) We do not require the differentiability and \(L\)-smoothness of the implicit objective function. This assumption may fail to hold, e.g., in constrained hierarchical FL problems. (2) The existing FL methods for bilevel and minimax problems assume that the lower-level problem is unconstrained. In fact, even in centralized regimes, addressing hierarchical problems where the lower-level constraints depend on \(x\) have remained challenging. For example, consider the problem \(\min_{x\in[-1,1]}\,\max_{y\in[-1,1],\ x+y\leq 0}\,x^{2}+y\) that admits a unique solution \((x^{*},y^{*})=(0.5,-0.5)\). Now consider a reversal of min and max in this problem, i.e., \(\max_{y\in[-1,1]}\,\min_{x\in[-1,1],\ x+y\leq 0}\,x^{2}+y\), admitting the unique solution \((x^{*},y^{*})=(-1,1)\). As a consequence, the well-known primal-dual gradient methods, that have been extensively employed for addressing minimax problems with independent constraint sets, may fail to converge to a saddle-point in minimax problems with coupling constrains. Our proposed algorithmic framework allows for accommodating these challenging problems in FL.

## 2 Related work

**(i) Nondifferentiable nonconvex optimization.** Nonsmooth and nonconvex optimization has been studied extensively with convergence guarantees to Clarke-stationary points via gradient sampling [1; 2] and difference-of-convex approaches [5]. Most complexity and rate guarantees necessitate smoothness of the nonconvex term [14; 51; 8; 9; 28] or convexity of the nonsmooth term [13], while only a few results truly consider nonsmooth nonconvex objective function [32; 4; 42]. **(ii) Nondifferentiable nonconvex federated learning.** The research on FL was initially motivated by decentralized neural networks where local functions are nondifferentiable and nonconvex [34]. Nevertheless, theoretical guarantees that emerged after FedAvg required either nonsmooth convex or smooth nonconvex local costs, under either iid [46; 58; 50; 47] or non-iid datasets [30; 26], while provable guarantees for FL methods under nonconvexity [58; 53; 19] require \(L\)-smoothness of local functions. Unfortunately, these assumptions do not hold either for ReLU neural networks or risk-averse learning and necessitate the use of Clarke calculus [3]. Moreover, existing work on zeroth-order FL methods in convex [31] and nonconvex settings [12] rely on the smoothness properties of the objective function. However, there appear to be no provably convergent FL schemes with complexity guarantees for computing approximate Clarke stationary points of nondifferentiable nonconvex problems. **(iii) Federated bilevel optimization.** Hyperparameter tuning [21] and its federated counterpart [23] is a crucial, and yet, computationally complex integrant of machine learning (ML) pipeline. Bilevel models

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Ref. & Nonconvexs & Metric & Rate & Comm. rounds & Assumption \\ \hline
[53] & Smooth & \(\|\nabla_{x}f(x)\|^{2}\) & \(\mathcal{O}\left(\frac{\alpha^{2}}{\sqrt{m}K}\right)\) & \(\mathcal{O}\left(m^{3/4}K^{3/4}\right)\) & Bounded gradients, \\ \hline
[50] & Smooth & \(\|\nabla_{x}f(x)\|^{2}\) & \(\mathcal{O}\left(\frac{\alpha^{2}}{\sqrt{m}K}\right)\) & \(\mathcal{O}\left(m^{3/2}K^{1/2}\right)\) & \(L\)-smooth functions \\ \hline
[18] & Smooth, FL-cond & \(f(x)-f^{*}\) & \(\mathcal{O}\left(\frac{\alpha^{2}}{\sqrt{m}K}\right)\) & \(\mathcal{O}\left(m^{1/3}K^{1/3}\right)\) & L-smooth functions, PL-condition \\ \hline
**This work** & Nonsmooth & \(\|\nabla_{x}\tilde{\mathbf{r}}^{\alpha}(\mathbf{z})\|^{2}\) & \(\mathcal{O}\left(\frac{\alpha^{2}}{\sqrt{m}K}\right)\) & \(\mathcal{O}\left(\mathbf{m^{3/4}K^{3/4}}\right)\) & Lipschitz functions \\ \hline \end{tabular}
\end{table}
Table 2: Comparison of our scheme with other FL schemes for nonconvex settingswhere the lower-level is a parameterized training model while the upper-level requires selecting the best configuration for the unknown hyperparameters [15; 22; 49]. Solving such hierarchical problems is challenging because of nondifferentiable nonconvex terms and absence of an analytical form for the implicit objective. These challenges exacerbate the development of provable guarantees for privacy-aware and communication-efficient schemes. **(iv) Federated minimax optimization**. Minimax optimization has assumed relevance in adversarial learning [17; 40; 44] and fairness in ML [57], amongst other efforts. Recently, FL was extended to distributed minimax problems [36; 10; 43], but relatively little exists in nonconvex-strongly concave settings [48; 43].

## 3 A Zeroth-order FL Framework for Nondifferentiable Nonconvex Settings

In this section, we introduce our framework for (**FL\({}_{nn}\)**), where we impose the following assumption.

**Assumption 1**.: Consider problem (**FL\({}_{nn}\)**). The following hold.

(i) The function \(f_{i}\) is Lipschitz continuous with parameter \(L_{0}>0\) for all \(i\in[m]\).

(ii) For any \(i\in[m]\), client \(i\) has access to a zeroth-order oracle \(\tilde{f}_{i}(x,\xi_{i})\) satisfying the following for every \(x\) in an almost-sure sense:

(ii-a) \(\mathbb{E}[\tilde{f}_{i}(x,\xi_{i})\mid x]=f_{i}(x)\); (ii-b) There exists \(\nu>0\) such that \(\mathbb{E}[|\tilde{f}_{i}(x,\xi_{i})-f_{i}(x)|^{2}\mid x]\leq\nu^{2}\).

(iii) The set \(X_{i}\) is nonempty, closed, and convex for all \(i\in[m]\). In addition, the following _bounded set-dissimilarity_ condition holds for all \(x\in\mathbb{R}^{n}\) and some scalars \(B_{1}\) and \(B_{2}\).

\[\tfrac{1}{m}\sum_{i=1}^{m}\text{dist}^{2}(x,X_{i})\leq B_{1}^{2}+B_{2}^{2}\ \left\|x-\tfrac{1}{m}\sum_{i=1}^{m}\mathcal{P}_{X_{i}}(x)\right\|^{2}.\] (3)

We note that the bounded set-dissimilarity condition is naturally analogous to the so-called _bounded gradient-dissimilarity_ condition that has been employed in the literature, e.g., in [25]. In particular, when the bounded gradient-dissimilarity condition is stated for the Moreau smoothing of the indicator function of \(X_{i}\), denoted by \(\mathbb{I}_{X_{i}}^{\eta}(x)\), we reach to (3). Notably, condition (3) holds for the generated iterates by the algorithm when, for example, the iterates remain bounded.

**Nonsmooth unconstrained reformulation.** Consider an unconstrained reformulation of (**FL\({}_{nn}\)**) given by \(\min_{x\in\mathbb{R}^{n}}\ \mathbf{f}(x)\) (see (1)), where the nonsmoothness of \(\mathbf{f}\) arises from that of \(f\) and the local indicator functions \(\mathbb{I}_{X_{i}}\). The minimization of \(\mathbf{f}\) is challenging, as noted by recent findings on nonsmooth analysis where it is shown [56] that for a suitable class of nonsmooth functions, computing an \(\epsilon\)-stationary point, i.e., a point \(\bar{x}\) for which \(\text{dist}(0_{n},\partial\mathbf{f}(\bar{x}))\leq\epsilon\), is impossible in finite time.

**Approximate Clarke stationarity.** To circumvent this challenge, as a weakening of \(\epsilon\)-stationarity, a notion of \((\delta,\epsilon)\)-stationarity is introduced [56] for a vector \(\bar{x}\) when \(\text{dist}(0_{n},\partial_{\delta}\mathbf{f}(\bar{x}))\leq\epsilon\), where the set

\[\partial_{\delta}\mathbf{f}(x)\triangleq\text{conv}\left\{\zeta:\zeta\in \partial\mathbf{f}(y),\|x-y\|\leq\delta\right\}\]

denotes the \(\delta\)-Clark generalized gradient of \(\mathbf{f}\) at \(x\)[16]; i.e. if \(x\) is \((\delta,\epsilon)\)-stationary, then there exists a convex combination of gradients in a \(\delta\)-neighborhood of \(x\) that has a norm of at most \(\epsilon\)[41].

This discussion naturally leads to the following key question: _Can we devise provably convergent FL methods for computing approximate Clarke stationary points of minimization of \(\mathbf{f}\)?_ The aim of this section is to provide an affirmative answer to this question by proposing a zeroth-order FL method that employs smoothing. To contend with the nonsmoothness, we employ the Moreau-smoothed variant of \(\mathbb{I}_{X}(x)\), where \(X\triangleq\bigcap_{i=1}^{m}X_{i}\), and a randomized smoothed variant of \(f\), as shown next.

**Randomized smoothing of loss function.** To smoothen the loss function \(f\), we employ a _randomized smoothing_ approach where the smoothing parameter is maintained as sufficiently small. This framework is rooted in the seminal work by Steklov [45], leading to progress in both convex [27; 52; 11] and nonconvex [37] regimes. We consider a smoothing of \(f\), given by \(f^{\eta}\) defined as \(f^{\eta}(x)\triangleq\mathbb{E}_{u\in\mathbb{B}}[f(x+\eta u)]\), where \(u\) is a random vector in the unit ball \(\mathbb{B}\), defined as \(\mathbb{B}\triangleq\{u\in\mathbb{R}^{n}\ |\ \|u\|\leq 1\}\). Further, we let \(\mathbb{S}\) denote the surface of the ball \(\mathbb{B}\), i.e., \(\mathbb{S}\triangleq\{v\in\mathbb{R}^{n}\ |\ \|v\|=1\}\) and \(\eta\mathbb{B}\) and \(\eta\mathbb{S}\) denote a ball with radius \(\eta\) and its surface, respectively.

**Lemma 1** (Randomized spherical smoothing).: Let \(h:\mathbb{R}^{n}\rightarrow\mathbb{R}\) be a given continuous function and define \(h^{\eta}(x)\triangleq\mathbb{E}_{u\in\mathbb{B}}\left[h(x+\eta u)\right].\) Then, the following hold.

(i) \(h^{\eta}\) is continuously differentiable and \(\nabla h^{\eta}(x)=\left(\frac{n}{\eta}\right)\mathbb{E}_{v\in\eta\mathbb{S}}[h(x+ v)\frac{v}{\|v\|}]\) for any \(x\in\mathbb{R}^{n}\).

Suppose \(h\) is Lipschitz continuous with parameter \(L_{0}>0\). Then, the following statements hold.

(ii) \(|h^{\eta}(x)-h^{\eta}(y)|\leq L_{0}\|x-y\|\) for all \(x,y\in\mathbb{R}^{n}\); (iii) \(|h^{\eta}(x)-h(x)|\leq L_{0}\eta\) for all \(x\in\mathbb{R}^{n}\); (iv) \(\|\nabla h^{\eta}(x)-\nabla h^{\eta}(y)\|\leq\frac{L_{0}n}{\eta}\|x-y\|\) for all \(x,y\in\mathbb{R}^{n}\). \(\Box\)

The discussion leads to the consideration of the following smoothed federated problem.

**Definition 1** (Unconstrained smoothed approximate problem).: Given \(\eta>0\), consider an unconstrained smoothed problem given as

\[\min_{x\in\mathbb{R}^{n}}\ \mathbf{f}^{\eta}(x)\left\{\triangleq\frac{1}{ m}\sum_{i=1}^{m}\mathbf{f}_{i}^{\eta}(x)\right\},\text{where }\mathbf{f}_{i}^{\eta}(x)\triangleq\mathbb{E}_{\xi_{i},u_{i}\in\mathbb{S}}[ \tilde{f}_{i}\left(x+\eta u_{i},\xi_{i}\right)]+\frac{1}{2\eta}\text{dist}^{2} (x,X_{i}).\] (4)

```
1:input: Server chooses a random initial point \(\hat{x}_{0}\in X\), stepsize \(\gamma\), smoothing parameter \(\eta\), synchronization indices \(T_{0}:=0\) and \(T_{r}\geq 1\), where \(r\geq 1\) is the communication round index
2:for\(r=0,1,\dots\)do
3: Server broadcasts \(\hat{x}_{r}\) to all clients: \(x_{i,T_{r}}:=\hat{x}_{r},\ \forall i\in[m]\)
4:for\(k=T_{r},\dots,T_{r+1}-1\)do in parallel by clients
5: Client \(i\) generates the random replicates \(\xi_{i,k}\in\mathcal{D}_{i}\) and \(v_{i,k}\in\eta\mathbb{S}\)
6:\(g_{i,k}^{\eta}:=\frac{n}{\eta^{2}}\left(\tilde{f}_{i}(x_{i,k}+v_{i,k},\xi_{i, k})-\tilde{f}_{i}(x_{i,k},\xi_{i,k})\right)v_{i,k}\)
7: Client \(i\) does a local update as \(x_{i,k+1}:=x_{i,k}-\gamma\left(g_{i,k}^{\eta}+\frac{1}{\eta}\left(x_{i,k}- \mathcal{P}_{X_{i}}(x_{i,k})\right)\right)\)
8:endfor
9: Server receives \(x_{i,T_{r+1}}\) from all clients and aggregates, i.e., \(\hat{x}_{r+1}:=\frac{1}{m}\sum_{i=1}^{m}x_{i,T_{r+1}}\)
10:endfor ```

**Algorithm 1** Randomized Zeroth-Order Locally-Projected Federated Averaging (FedRZOnn)

To address (4), we propose FedRZOnn given by Algorithm 1. Here, client \(i\) employs a zeroth-order stochastic gradient of the form \(g_{i,k}^{\eta}\triangleq\frac{n}{\eta^{2}}\left(\tilde{f}_{i}(x_{i,k}+v_{i,k}, \xi_{i,k})-\tilde{f}_{i}(x_{i,k},\xi_{i,k})\right)v_{i,k}\), augmented by the gradient of the Moreau smoothed function. The random sample \(v_{i,k}\in\eta\mathbb{S}\) is locally generated by each client \(i\), allowing for randomized smoothing. This is indeed in view of Lemma 1 (i) that facilitates the development of a randomized zeroth-order gradient.

We define \(\bar{x}_{k}\triangleq\frac{\sum_{i=1}^{m}x_{i,k}}{m}\) as an auxiliary sequence to denote the averaged iterates of the clients.

**Definition 2**.: Consider Algorithm 1. Let \(H>0\) denote an upper bound on the number of local steps per round, i.e., \(H\geq\max_{r=0,1,\dots}|T_{r+1}-T_{r}|\). Throughout, we assume that \(H\) is finite.

**Proposition 1**.: Consider Algorithm 1. Let Assumption 1 hold.

(i) **[Error bound]** Suppose \(\gamma\leq\min\left\{\frac{\eta}{4L_{0}n},\frac{1}{4H},\frac{\eta}{12\sqrt{3}B_ {2}(L_{0}n+1)H}\right\}\). Let \(k^{*}\) denote an integer drawn uniformly at random from \(\{0,\dots,K\}\) and \(\mathbf{f}^{\eta,*}\triangleq\inf_{x}\mathbf{f}^{\eta}(x)\). Then,

\[\mathbb{E}\left[\|\nabla\mathbf{f}^{\eta}(\bar{x}_{k^{*}})\|^{2}\right] \leq\frac{8(\mathbb{E}[\mathbf{f}^{\eta}(\bar{x}_{k})-\mathbf{f}^ {\eta,*})}{\gamma(K+1)}+\frac{12\cdot L_{0}n^{3}}{\eta n}\left(\frac{2\nu^{2}} {\eta^{2}}+L_{0}^{2}\right)\] \[+\frac{36H^{2}\gamma^{2}(L_{0}n+1)^{2}}{\eta^{2}}\left(\frac{6n^ {2}\nu^{2}+2B_{1}^{2}}{\eta^{2}}+(3+4B_{2}^{2})L_{0}^{2}n^{2}\right).\]

(ii) **[Iteration complexity]** Let \(\gamma:=\sqrt{\frac{m}{K}}\) and \(H:=\left\lceil\sqrt[4]{\frac{K}{m^{2}}}\right\rceil\) where \(\eta>0\). Let \(\epsilon>0\) be an arbitrary scalar and \(K_{\epsilon}\) denote the number of iterations such that \(\mathbb{E}\left[\|\nabla f^{\eta}(\bar{x}_{k^{*}})\|^{2}\right]\leq\epsilon\). Then, the iteration complexity is \(K_{\epsilon}:=\mathcal{O}\left(\left(\frac{L_{0}n^{3}\nu^{2}}{\eta^{2}}+\frac{L_ {0}^{3}n^{3}}{\eta}+\frac{L_{0}^{2}n^{4}\nu^{2}}{\eta^{4}}+\frac{L_{0}^{2}n^{2} B_{1}^{2}}{\eta^{4}}+\frac{B_{2}^{2}L_{0}^{4}n^{4}}{\eta^{2}}\right)^{2} \frac{1}{m\epsilon^{2}}\right).\)

(iii) **[Communication complexity]** Suppose \(K_{\epsilon}\geq m^{3}\). Then, the number of communication rounds to achieve the accuracy level in (ii) is \(R:=\mathcal{O}\left(\left(mK_{\epsilon}\right)^{3/4}\right)\).

We now formally relate the original nonsmooth problem and its smoothed counterpart.

**Proposition 2**.: Consider problem (4) and let Assumption 1 hold.

(i) Assume that \(X_{i}=\mathbb{R}^{n}\) for all \(i\in[m]\). Then, for any \(\eta>0\), we have \(\nabla f^{\eta}(x)\in\partial_{2\eta}f(x)\).

(ii) Assume that the sets \(X_{i}\) are identical for all \(i\in[m]\). Let \(\delta>0\) be an arbitrary scalar. If \(\nabla\mathbf{f}^{\eta}(x)=0\) and \(\eta\leq\frac{\delta}{\max\left\{2,nL_{0}\right\}}\), then \(0_{n}\in\partial_{\delta}\left(f+\mathbb{I}_{X}\right)(x)\).

## 4 Extensions to Bilevel and Minimax FL

### Nondifferentiable nonconvex bilevel FL

In this section, we consider the federated bilevel optimization problem defined earlier as (\(\mathbf{FL}_{bl}\)). We consider the following smoothed implicit problem.

**Definition 3** (Unconstrained smoothed implicit problem).: Given \(\eta>0\), consider an unconstrained smoothed implicit problem given as

\[\min_{x\in\mathbb{R}^{n}}\ \mathbf{f}^{\eta}(x)\left\{\triangleq\frac{1}{m}\sum_{i= 1}^{m}\left(\mathbb{E}_{\xi_{i},u\in\mathbb{B}}[\tilde{f}_{i}\left(x+\eta u,y (x+\eta u),\xi_{i}\right)]+\frac{1}{2\eta}\text{dist}^{2}(x,X_{i})\right) \right\}.\] (5)

**Assumption 2**.: Consider problem (\(\mathbf{FL}_{bl}\)). Let the following assumptions hold.

(i) For all \(i\in[m]\), \(\tilde{f}_{i}(\bullet,y,\xi_{i})\) is \(L^{f}_{0,x}(\xi_{i})\)-Lipschitz for any \(y\) and \(\tilde{f}_{i}(x,\bullet,\xi_{i})\) is \(L^{f}_{0,y}(\xi_{i})\)-Lipschitz for any \(x\), where \(L^{f}_{0,x}\triangleq\max_{i=1,\dots,m}\sqrt{\mathbb{E}[(L^{f}_{0,x}(\xi_{i})) ^{2}]}<\infty\) and \(L^{f}_{0,y}\triangleq\max_{i=1,\dots,m}\sqrt{\mathbb{E}[(L^{f}_{0,y}(\xi_{i}) )^{2}]}<\infty\).

(ii) For all \(i\in[m]\), for any \(x\), \(h_{i}(x,\bullet)\) is \(L^{h}_{1,y}\)-smooth and \(\mu_{h}\)-strongly convex. Further, for any \(y\), the map \(\nabla_{y}h_{i}(\bullet,y)\) is Lipschitz continuous with parameter \(L^{\nabla h}_{0,x}\).

(iii) The sets \(X_{i}\), for \(i\in[m]\), satisfy Assumption 1 (iii).

The outline of FedRZO\({}_{\text{b1}}\) is presented in Algorithm 2. We make the following remarks: (i) At each global step, the server makes two calls to a lower-level FL oracle to inexactly compute \(y(\hat{x}_{r}+v_{T_{r}})\) and \(y(\hat{x}_{r})\). These lower-level FL calls are performed by the same clients, on the lower-level FL problem. (ii) The inexactness error is carefully controlled by terminating the lower-level FL oracle after \(\mathcal{O}(r)\) number of iterations, where \(r\) denotes the upper-level communication round index. (iii) FedRZO\({}_{\text{b1}}\) skips the calls to the lower-level FL oracle during the local steps. To accommodate this, unlike in FedRZO\({}_{\text{nn}}\), here we employ a global randomized smoothing denoted by \(v_{T_{r}}\) during the communication round \(r\) in the upper level.

```
1:input: Server chooses a random \(\hat{x}_{0}\in X\), stepsize \(\gamma\), smoothing parameter \(\eta\), synchronization indices \(T_{0}:=0\) and \(T_{r}\geq 1\), where \(r\geq 1\) is the upper-level communication round index
2:for\(r=0,1,\dots\)do
3: Server generates a random replicate \(v_{T_{r}}\in\eta\mathbb{S}\)
4: Server calls FedAvg to receive \(y_{\varepsilon_{r}}(\hat{x}_{r}+v_{T_{r}})\) and \(y_{\varepsilon_{r}}(\hat{x}_{r})\), denoting the inexact evaluations of \(y(\hat{x}_{r}+v_{T_{r}})\) and \(y(\hat{x}_{r})\), respectively.
5: Server broadcasts \(\hat{x}_{r}\), \(\hat{x}_{r}+v_{T_{r}}\), \(y_{\varepsilon_{r}}(\hat{x}_{r})\), and \(y_{\varepsilon_{r}}(\hat{x}_{r}+v_{T_{r}})\) to all clients; \(x_{i,T_{r}}:=\hat{x}_{r},\ \forall i\)
6:for\(k=T_{r},\dots,T_{r+1}-1\)do in parallel by clients
7: Client \(i\) generates the random replicates \(\xi_{i,k}\in\mathcal{D}_{i}\)
8:\(g^{\eta,\varepsilon_{r}}_{i,k}:=\frac{\eta}{\eta^{2}}\left(\tilde{f}_{i}(x_{i,k}+v_{T_{r}},y_{\varepsilon_{r}}(\hat{x}_{r}+v_{T_{r}}),\xi_{i,k})-\tilde{f}_ {i}(x_{i,k},y_{\varepsilon_{r}}(\hat{x}_{r}),\xi_{i,k})\right)v_{T_{r}}\)
9: Client \(i\) does a local update as \(x_{i,k+1}:=x_{i,k}-\gamma\left(g^{\eta,\varepsilon_{r}}_{i,k}+\frac{1}{\eta} \left(x_{i,k}-\mathcal{P}_{X_{i}}(x_{i,k})\right)\right)\)
10:endfor
11: Server receives \(x_{i,T_{r+1}}\) from all clients and aggregates, i.e., \(\hat{x}_{r+1}:=\frac{1}{m}\sum_{i=1}^{m}x_{i,T_{r+1}}\)
12:endfor ```

**Algorithm 2** Randomized Implicit Zeroth-Order Federated Averaging (FedRZO\({}_{\text{b1}}\))

**Theorem 1** (FedRZO\({}_{\text{b1}}\) when using an arbitrary inexact FL method for lower-level).: Consider Algorithm 2. Let Assumption 2 hold, \(k^{*}\) be chosen uniformly at random from \(0,\dots,K:=T_{R}-1\), and \(\gamma\leq\min\left\{\frac{\max\left\{2,\sqrt{0.1\Theta_{3}},4B_{2}\sqrt{3 \Theta_{2}},4B_{2}\sqrt{3\Theta_{3}}\right\}^{-1}}{4H},\frac{\eta}{24\left(L^{ \text{min}}_{\text{b2}}n+1\right)}\right\}\). Let \(\varepsilon_{r}\) denote the inexactness in obtaining the lower-level solution, i.e., \(\mathbb{E}\left[\|y_{\varepsilon_{r}}(x)-y(x)\|^{2}\mid x\right]\leq\varepsilon_{r}\) for \(x\in\cup_{r=0}^{R}\{\hat{x}_{r},\hat{x}_{r}+v_{T_{r}}\}\).

(i) **[Error bound]** We have

\[\mathbb{E}\left[\|\nabla\mathbf{f}^{\eta}(\bar{x}_{k^{*}})\|^{2}\right] \leq 8(\gamma K)^{-1}(\mathbb{E}\left[\mathbf{f}^{\eta}(x_{0}) \right]-\mathbf{f}^{\eta,*})+\frac{8\gamma\Theta_{1}}{m}+8H^{2}\gamma^{2}\max \{\Theta_{2},\Theta_{3}\}\Theta_{5}\] \[\quad+8\left(H^{2}\gamma^{2}\max\{\Theta_{2},\Theta_{3}\}\Theta_ {4}+\Theta_{3}\right)H\frac{\sum_{r=0}^{R-1}\varepsilon_{r}}{K},\]

where \(\Theta_{1}:=\frac{3(L_{0}^{\text{imp}}n+1)n^{2}}{2\eta}(L_{0}^{\text{imp}})^{ 2},\;\Theta_{2}:=\frac{5(L_{0}^{\text{imp}}n+1)^{2}}{8\eta^{2}},\Theta_{3}:= \left(\frac{L_{0,x}^{\text{V}_{h}}}{\mu_{h}}\right)^{2}\frac{20n^{2}}{\eta^{2 }}(L_{0,y}^{f})^{2},\)

\[\Theta_{4}:=\frac{96n^{2}}{\eta^{2}}(L_{0,y}^{f})^{2},\;\text{and}\;\Theta_{5 }:=\frac{48B_{1}^{2}}{\eta^{2}}+(96B_{2}^{2}+1)(L_{0}^{\text{imp}})^{2}n^{2}.\]

(ii) **[Iteration complexity]** Let \(\gamma:=\sqrt{\frac{m}{K}}\) and \(H:=\left\lceil\sqrt[4]{\frac{K}{m^{3}}}\right\rceil\) where \(\eta>0\). Let \(\epsilon>0\) be an arbitrary scalar and \(K_{\epsilon}\) denote the number of iterations such that \(\mathbb{E}\left[\|\nabla\mathbf{f}^{\eta}(\bar{x}_{k^{*}})\|^{2}\right]\leq\epsilon\). Also, suppose we employ an FL method in the lower level that achieves a sublinear convergence rate with a linear speedup in terms of the number of clients, i.e., \(\varepsilon_{r}:=\tilde{\mathcal{O}}(\frac{1}{mT_{\tilde{R}_{r}}})\) where \(\tilde{R}_{r}\) denotes the number of communication rounds performed in the lower-level FL method when it is called in round \(r\) of FedRZO\({}_{\text{b1}}\) and \(\tilde{T}_{\tilde{R}_{r}}\) denotes the number of iterations performed in the lower-level FL scheme to do \(\tilde{R}_{r}\) rounds of upper-level communication. Further, suppose \(\tilde{T}_{\tilde{R}_{r}}:=\tilde{\mathcal{O}}\left(m^{-1}(r+1)^{\frac{3}{3}}\right)\). Then, the iteration complexity of FedRZO\({}_{\text{b1}}\) (upper level) is \(K_{\epsilon}:=\tilde{\mathcal{O}}\left((\Theta_{1}^{2}+\max\{\Theta_{2}, \Theta_{3}\}^{2}\Theta_{5}^{2}+\max\{\Theta_{2},\Theta_{3}\}^{2}\Theta_{4}^{2 }+\Theta_{3}^{2})\,\frac{1}{m\epsilon^{2}}\right).\)

(iii) **[Communication complexity]** Suppose \(K_{\epsilon}\geq m^{3}\). Then, the number of communication rounds in FedRZO\({}_{\text{b1}}\) (upper-level only) to achieve the accuracy level in (ii) is \(R:=\mathcal{O}\left((mK_{\epsilon})^{3/4}\right)\).

**Remark 2**.: (i) Importantly, Theorem 1 is equipped with explicit communication complexity \(R:=\mathcal{O}\left((mK_{\epsilon})^{3/4}\right)\), matching that of single-level nonsmooth nonconvex problems in Proposition 1. This implies that as long as the lower-level FL oracle has a rate of \(\varepsilon_{r}:=\tilde{\mathcal{O}}(\frac{1}{mT_{\tilde{R}_{r}}})\), the inexactness does not affect the communication complexity bounds of the method in the upper level.

(ii) As noted in the assumptions, in the upper level, we allow for heterogeneity. To elaborate on overall communication complexity of FedRZO\({}_{\text{b1}}\), we provide detailed complexity results in Table 1 for three cases, where we employ Local SGD [26], FedAC [54], and LFD [20] for the lower-level scheme. All these schemes meet the linear speedup condition in Theorem 1. Notably, among these schemes, the last scheme allows for the presence of heterogeneity. As an example, we present in Algorithm 3, the outline of FedAvg, if employed in step 4 of Algorithm 2.

```
1:input:\(x\), \(r\), server chooses a random initial point \(\hat{y}_{0}:=y_{0,r}\in\mathbb{R}^{\tilde{n}}\), \(a_{r}:=\max\{m,4\kappa_{h},r\}+1\) where \(\kappa_{h}:=\frac{L_{1,x}^{h}}{\mu_{h}}\), \(\tilde{\gamma}:=\frac{1}{\mu_{h}a_{r}}\), \(\tilde{T}_{\tilde{R}_{r}}:=2a_{r}\ln(a_{r})\), and \(\tilde{H}:=\lceil\frac{\tilde{T}_{\tilde{R}_{r}}}{m}\rceil\)
2:for\(\tilde{r}=0,1,\ldots,\tilde{R}_{r}-1\)do
3: Server broadcasts \(\hat{y}_{\tilde{r}}\) to all agents: \(y_{i,\tilde{T}_{r}}:=\hat{y}_{\tilde{r}},\;\forall i\)
4:for\(t=\tilde{T}_{\tilde{r}},\ldots,\tilde{T}_{\tilde{r}+1}-1\)do in parallel by agents
5: Agent \(i\) does a local update as \(y_{i,t+1}:=y_{i,t}-\tilde{\gamma}\nabla_{y}h_{i}(x,y_{i,t},\tilde{\xi}_{i,t})\)
6: Agent \(i\) sends \(x_{i,T_{r+1}}\) to the server
7:endfor
8: Server aggregates, i.e., \(\hat{y}_{\tilde{r}+1}:=\frac{1}{m}\sum_{i=1}^{m}y_{i,T_{\tilde{r}+1}}\)
9:endfor ```

**Algorithm 3** FedAvg \((x,r,y_{0,r},m,\tilde{\gamma},\tilde{H},\tilde{T}_{\tilde{R}})\) for lower level

(iii) We use \(L_{0}^{\text{imp}}(\xi_{i})\) to denote the Lipschitz continuity constant of the random local implicit function \(\tilde{f}_{i}(x,y(x),\xi_{i})\), and let \(L_{0}^{\text{imp}}\triangleq\max_{i=1,\ldots,m}\sqrt{\mathbb{E}[(L_{0}^{\text{ imp}}(\xi_{i}))^{2}]}<\infty\). As shown in supplementary material, \(L_{0}^{\text{imp}}(\xi_{i})\) can be obtained explicitly in terms of problem parameters.

**Remark 3**.: A technical challenge in designing Algorithm 2 is that an inexact evaluation of \(y(x)\) must be avoided during the local steps. This is because we consider bilevel problems of the form (**FL\({}_{bl}\)**)where both levels are distributed. Because of this, the inexact evaluation of \(y(x)\) by each client in the local step in the upper level would require significant communications that is undesirable in the FL framework. We carefully address this challenge by introducing **delayed inexact computation** of \(y(x)\). In step 8 of Algorithm 2, we note how \(y_{\epsilon}\) is evaluated at \(\hat{x}_{r}+v_{T_{r}}\) which is a different than the vector used by the client, i.e., \(x_{i,k}+v_{T_{r}}\). At each communication round in the upper level, we only compute \(y(x)\) inexactly twice in the global step and then use this delayed information in the local steps. This delayed inexact computation of \(y\) renders a challenge in the convergence analysis which makes the design and analysis of Algorithm 2 a non-trivial extension of Algorithm 1.

### Nondifferentiable nonconvex-strongly concave minimax FL

Next, we consider the decentralized federated minimax problem of the form (\(\mathbf{FL}_{mm}\)) introduced earlier. This problem is indeed a zero-sum game and can be viewed as an instance of the non-zero sum game (\(\mathbf{FL}_{bl}\)) where \(\tilde{h}_{i}:=-\tilde{f}_{i}\).

**Corollary 1**.: Consider Algorithm 2 for solving (\(\mathbf{FL}_{mm}\)). Let Assumption 2 hold for \(\tilde{h}_{i}:=-\tilde{f}_{i}\) and \(\mathcal{D}_{i}:=\tilde{\mathcal{D}}_{i}\). Then, all the results in Theorem 1 hold true.

## 5 Experiments

We present three sets of experiments to validate the performance of the proposed algorithms. In Section 5.1, we implement Algorithm 1 on ReLU neural networks (NNs) and compare it with some recent FL methods. In Sections 5.2 and 5.3 we implement Algorithm 2 on federated hyperparameter learning and a minimax formulation in FL. Throughout, we use the MNIST dataset. Additional experiments on a higher dimensional dataset (i.e., Cifar-10) are presented in supplementary material.

### Federated training of ReLU NNs.

We implement FedRZOnn for federated training in a single-layer ReLU NN with \(N_{1}\) neurons. This is a nondifferentiable nonconvex optimization problem, aligning with (\(\mathbf{FL}_{nn}\)) and taking the form \(\min_{x:=(Z,w)\in\mathcal{X}}\frac{1}{2m}\sum_{i=1}^{m}\sum_{\ell\in\mathcal{ D}_{i}}(v_{i,\ell}-\sum_{q=1}^{N_{1}}w_{q}\sigma(Z_{\boldsymbol{\ast},q}U_{i,\ell}))^{2}+ \frac{\lambda}{2}\left(\|Z\|_{F}^{2}+\|w\|^{2}\right),\) where \(m\) denotes the number of clients, \(Z\in\mathbb{R}^{N_{1}\times N_{0}}\), \(w\in\mathbb{R}^{N_{1}}\), \(N_{0}\) is the feature dimension, \(U_{i,\ell}\in\mathbb{R}^{N_{0}}\) and \(v_{i,\ell}\in\{-1,1\}\) are the \(\ell\)th input and output training sample of client \(i\), respectively, \(\sigma(x):=\max\{0,x\}\), and \(\lambda\) is the regularization parameter.

**Setup.** We distribute the training dataset among \(m:=5\) clients and implement FedRZOnn for the FL training with \(N_{1}:=4\) neurons under three different settings for the smoothing parameter, \(\eta\in\{0.1,0.01,0.001\}\), \(\gamma:=10^{-5}\), and \(\lambda:=0.01\). We study the performance of the method under different number of local steps with \(H\in\{1,5,10,20\}\).

**Results and insights.** Figure 1 presents the first set of numerics for FedRZOnn under the aforementioned settings. In terms of communication rounds, we observe that the performance of the method improves by using a larger number of local steps. In fact, in the case where \(H:=1\), FedRZOnn is equivalent to a parallel zeroth-order SGD that employs communication among clients at each iteration, resulting in a poor performance, motivating the need for the FL framework. In terms of \(\eta\), while we observe robustness of the scheme in terms of the original loss function, we also note a slight improvement in the empirical speed of convergence in early steps, as \(\eta\) increases. This is indeed aligned with the dependence of convergence bound in Proposition 1 on \(\eta\).

Figure 1: Performance of FedRZOnn on a single-layer ReLU NN in terms of communication rounds for different no. of local steps and different values of the smoothing parameter \(\eta\). FedRZOnn benefits from larger number of local steps and shows robustness with respect to the choice of \(\eta\).

**Comparison with other FL methods.** While we are unaware of other FL methods for addressing nondifferentiable nonconvex problems, we compare FedRZO\({}_{\tt nn}\) with other FL methods including FedAvg [34], FedProx [29], FedMSPP [55], and Scaffhew [35] when applied on a NN with a smooth rectifier. Details of these experiments are provided in the supplementary material.

**5.2 Federated hyperparameter learning.** To validate FedRZO\({}_{\tt b1}\), we consider the following FL hyperparameter learning problem for binary classification using logistic loss.

\[\min_{x\in X,\;y\in\mathbb{R}^{n}} f(x,y)\triangleq\frac{1}{m}\sum_{i=1}^{m}\sum_{\ell\in \mathcal{D}_{i}}\log\left(1+\exp(-v_{i,\ell}U_{i,\ell}^{T}y)\right)\] subject to \[y\in\text{arg}\min_{y\in\mathbb{R}^{n}} h(x,y)\triangleq\frac{1}{m}\sum_{i=1}^{m}(\sum_{\tilde{\ell}\in \tilde{D}_{i}}\log\left(1+\exp(-v_{i,\tilde{\ell}}U_{i,\tilde{\ell}}^{T}y) \right)+x_{i}\frac{\|y\|^{2}}{2}),\]

where \(m\) is number of clients, \(x\) denotes the regularization parameter for client \(i\), \(U_{i,\ell}\in\mathbb{R}^{n}\) and \(v_{i,\ell}\in\{-1,1\}\) are the \(\ell\)th input and output testing sample of client \(i\), respectively, \(U_{i,\ell^{\prime}}\in\mathbb{R}^{n}\) and \(v_{i,\ell^{\prime}}\in\{-1,1\}\) are the \(\ell^{\prime}\)th input and output training sample of client \(i\), respectively. The constraint set \(X\) is considered as \(X:=\{x\in\mathbb{R}^{m}\mid x\geq\mu\mathbf{1}_{m}\}\), where \(\mu>0\). This problem is an instance of (**FL\({}_{bl}\)**), where the lower-level problem is \(\ell_{2}\)-regularized and the regularization parameter is a decision variable of the upper-level FL problem. The convergence results are presented in Fig. 2 (left).

**5.3 Fair classification learning.** Here, we study the convergence of FedRZO\({}_{\tt b1}\) in minimax FL. We consider solving an FL minimax formulation of the fair classification problem [39] of the form

\[\min_{x\in\mathbb{R}^{n}}\max_{y\in\mathbb{R}^{c}} \frac{1}{m}\sum_{i=1}^{m}\sum_{c=1}^{C}\sum_{\ell\in\mathcal{D}_{i,c }}(v_{i,\ell}-\sum_{q=1}^{N_{1}}w_{q}\sigma(Z_{\bullet,q}U_{i,\ell}))^{2}- \frac{\lambda}{2}\|y\|^{2},\]

where \(c\) denotes the class index and \(\mathcal{D}_{i,c}\) denotes the a portion of local dataset associated with client \(i\) that is comprised of class \(c\) samples. The loss function follows the same formulation in Section 5.1, where an ReLU neural network is employed. This problem is nondifferentiable and nonconvex-strongly concave, fitting well with the assumptions in our work in addressing minimax FL problems. The performance of our algorithm is presented in Figure 2 (right).

## 6 Concluding Remarks

Federated learning has assumed growing relevance in ML. However, most practical problems are characterized by the presence of local objectives, jointly afflicted by nonconvexity and nondifferentiability, precluding resolution by most FL schemes, which can cope with nonconvexity in only smooth settings. We resolve this gap via a zeroth-order communication-efficient FL framework that can contend with both nondifferentiability and nonsmoothness with rate and complexity guarantees for computing approximate Clarke-stationary points. Extensions to nonconvex bilevel and nonconvex-strongly concave minimax settings are developed via inexact generalizations.

## 7 Acknowledgments

We acknowledge the funding support from the U.S. Department of Energy under grant #DE-SC0023303, and the U.S. Office of Naval Research under grants #N00014-22-1-2757 and #N00014-22-1-2589. We also would like to thank the four anonymous referees for their constructive suggestions.

Figure 2: (Left) Convergence of FedRZO\({}_{\tt b1}\) in hyperparameter FL for \(\ell_{2}\) regularized logistic loss, where we plot the loss function on test data for different values of local steps with \(95\%\) CIs. (Right) Convergence of FedRZO\({}_{\tt b1}\) in minimax FL, where we present test results in solving a nondifferentiable nonconvex-strongly concave FL minimax formulation of the fair classification problem [39].

## References

* [1] J. V. Burke, A. S. Lewis, and M. L. Overton. Approximating subdifferentials by random sampling of gradients. _Math. Oper. Res._, 27(3):567-584, 2002.
* [2] J. V. Burke, A. S. Lewis, and M. L. Overton. A robust gradient sampling algorithm for nonsmooth, nonconvex optimization. _SIAM J. Optim._, 15(3):751-779, 2005.
* [3] F. H. Clarke. Generalized gradients and applications. _Transactions of the American Mathematical Society_, 205:247-262, 1975.
* [4] S. Cui, U. V. Shanbhag, and F. Yousefian. Complexity guarantees for an implicit smoothing-enabled method for stochastic mpecs. _Math. Program._, 198(2):1153-1225, 2023.
* [5] Y. Cui and J. S. Pang. _Modern nonconvex nondifferentiable optimization_, volume 29 of _MOS-SIAM Series on Optimization_. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA; Mathematical Optimization Society, Philadelphia, PA, [2022] \(\copyright\)2022.
* [6] S. Dafermos. Sensitivity analysis in variational inequalities. _Mathematics of Operations Research_, 13(3):421-434, 1988.
* [7] R. Das, A. Acharya, A. Hashemi, S. Sanghavi, I. S. Dhillon, and U. Topcu. Faster non-convex federated learning via global and local momentum, 2021.
* [8] D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. _SIAM J. Optim._, 29(1):207-239, 2019.
* [9] D. Davis and B. Grimmer. Proximally guided stochastic subgradient method for nonsmooth, nonconvex problems. _SIAM J. Optim._, 29(3):1908-1930, 2019.
* [10] Y. Deng, M. M. Kamani, and M. Mahdavi. Distributionally robust federated averaging. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 15111-15122. Curran Associates, Inc., 2020.
* [11] J. C. Duchi, P. L. Bartlett, and Martin J. Wainwright. Randomized smoothing for stochastic optimization. _SIAM J. on Optimization_, 22(2):674-701, 2012.
* [12] W. Fang, Z. Yu, Y. Jiang, Y. Shi, C. N Jones, and Y. Zhou. Communication-efficient stochastic zeroth-order optimization for federated learning. _IEEE Transactions on Signal Processing_, 70:5058-5073, 2022.
* [13] A. Gasnikov, A. Novitskii, V. Novitskii, F. Abdukhakimov, D. Kamzolov, A. Beznosikov, M. Takac, P. Dvurechensky, and B. Gu. The power of first-order smooth optimization for black-box non-smooth problems. _arXiv preprint arXiv:2201.12289_, 2022.
* [14] S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. _Math. Programming_, 155(1-2):267-305, 2016.
* [15] S. Ghadimi and M. Wang. Approximation methods for bilevel programming. arXiv preprint: https://arxiv.org/abs/1802.02246.
* [16] A. A. Goldstein. Optimization of Lipschitz continuous functions. _Math. Programming_, 13(1):14-22, 1977.
* [17] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc., 2014.
* [18] F. Haddadpour, M. M. Kamani, M. Mahdavi, and V. Cadambe. Local SGD with periodic averaging: Tighter analysis and adaptive synchronization. _Advances in Neural Information Processing Systems_, 32, 2019.

* [19] F. Haddadpour, M. M. Kamani, M. Mahdavi, and V. Cadambe. Trading redundancy for communication: Speeding up distributed SGD for non-convex optimization. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 2545-2554. PMLR, 09-15 Jun 2019.
* [20] F. Haddadpour and M. Mahdavi. On the convergence of local descent methods in federated learning. _arXiv preprint arXiv:1910.14425_, 2019.
* [21] E. Hazan, A. Klivans, and Y. Yuan. Hyperparameter optimization: A spectral approach. _6th International Conference on Learning Representations (ICLR)_, 2018.
* [22] K. Ji, J. Yang, and Y. Liang. Bilevel optimization: Convergence analysis and enhanced design. arXiv preprint: https://arxiv.org/pdf/2010.07962.pdf.
* [23] P. Kairouz, H. Brendan McMahan, et al. Advances and open problems in federated learning. _CoRR_, abs/1912.04977, 2019.
* [24] S. P. Karimireddy, M. Jaggi, S. Kale, M. Mohri, S. J. Reddi, S. U Stich, and A. T. Suresh. Breaking the centralized barrier for cross-device federated learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* [25] S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, pages 5132-5143. PMLR, 2020.
* [26] A. Khaled, K. Mishchenko, and P. Richtarik. Tighter theory for local SGD on identical and heterogeneous data. In _International Conference on Artificial Intelligence and Statistics_, pages 4519-4529. PMLR, 2020.
* [27] H. Lakshmanan and D. Farias. Decentralized recourse allocation in dynamic networks of agents. _SIAM Journal on Optimization_, 19(2):911-940, 2008.
* [28] J. Lei and U. V. Shanbhag. Asynchronous variance-reduced block schemes for composite non-convex stochastic optimization: block-specific steplengths and adapted batch-sizes. _Optimization Methods and Software_, pages 1-31, 2020.
* [29] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith. Federated optimization in heterogeneous networks. _Proceedings of Machine learning and systems_, 2:429-450, 2020.
* [30] X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang. On the convergence of FedAvg on Non-IID data. In _International Conference on Learning Representations_, 2020.
* [31] Zan Li and Li Chen. Communication-efficient decentralized zeroth-order method on heterogeneous data. In _2021 13th International Conference on Wireless Communications and Signal Processing (WCSP)_, pages 1-6. IEEE, 2021.
* [32] T. Lin, Z. Zheng, and M. Jordan. Gradient-free methods for deterministic and stochastic nonsmooth nonconvex optimization. _Advances in Neural Information Processing Systems_, 35:26160-26175, 2022.
* [33] D. Q. Mayne and E. Polak. Nondifferential optimization via adaptive smoothing. _Journal of Optimization Theory and Applications_, 43(4):601-613, 1984.
* [34] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Aarti Singh and Jerry Zhu, editors, _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics_, volume 54 of _Proceedings of Machine Learning Research_, pages 1273-1282. PMLR, 20-22 Apr 2017.

* [35] K. Mishchenko, G. Malinovsky, S. Stich, and P. Richtarik. ProxSkip: Yes!: Local gradient steps provably lead to communication acceleration! Finally! In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 15750-15769. PMLR, 17-23 Jul 2022.
* [36] M. Mohri, G. Sivek, and A. T. Suresh. Agnostic federated learning. In _International Conference on Machine Learning_, pages 4615-4625. PMLR, 2019.
* [37] Y. Nesterov and V. Spokoiny. Random gradient-free minimization of convex functions. _Found. Comput. Math._, 17(2):527-566, 2017.
* [38] J. Nguyen, K. Malik, H. Zhan, A. Yousefpour, M. Rabbat, M. Malek, and D. Huba. Federated learning with buffered asynchronous aggregation. _CoRR_, abs/2106.06639, 2021.
* [39] M. Nouiehed, M. Sanjabi, T. Huang, J. D. Lee, and M. Razaviyayn. Solving a class of nonconvex min-max games using iterative first order methods. _Advances in Neural Information Processing Systems_, 32, 2019.
* [40] M. Sanjabi, J. Ba, M. Razaviyayn, and J. D. Lee. On the convergence and robustness of training gans with regularized optimal transport. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [41] O. Shamir. Can we find near-approximately-stationary points of nonsmooth nonconvex functions? In _OPT2020: 12th Annual Workshop on Optimization for Machine Learning_, 2021.
* [42] U. V. Shanbhag and F. Yousefian. Zeroth-order randomized block methods for constrained minimization of expectation-valued lipschitz continuous functions. In _2021 Seventh Indian Control Conference (ICC)_, pages 7-12. IEEE, 2021.
* [43] P. Sharma, R. Panda, G. Joshi, and P. Varshney. Federated minimax optimization: Improved convergence analyses and algorithms. In _International Conference on Machine Learning_, pages 19683-19730. PMLR, 2022.
* [44] A. Sinha, H. Namkoong, and J. Duchi. Certifiable distributional robustness with principled adversarial training. In _International Conference on Learning Representations_, 2018.
* [45] V. A. Steklov. Sur les expressions asymptotiques decertaines fonctions definies par les equations differentielles du second ordre et leers applications au probleme du developement d'une fonction arbitraire en series procedant suivant les diverses fonctions. _Comm. Charkov Math. Soc._, 2(10):97-199, 1907.
* [46] S. U. Stich. Local SGD converges fast and communicates little. In _International Conference on Learning Representations_, 2019.
* [47] S. U. Stich and S. P. Karimireddy. The error-feedback framework: Better rates for SGD with delayed gradients and compressed updates. _Journal of Machine Learning Research_, 21:1-36, 2020.
* [48] D. A. Tarzanagh, M. Li, C. Thrampoulidis, and S. Oymak. Fednest: Federated bilevel, minimax, and compositional optimization. In _International Conference on Machine Learning_, pages 21146-21179. PMLR, 2022.
* [49] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Sparsity and smoothness via the fused lasso. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 67(1):91-108, 2005.
* [50] J. Wang and G. Joshi. Cooperative sgd: A unified framework for the design and analysis of local-update sgd algorithms. _Journal of Machine Learning Research_, 22(213):1-50, 2021.
* [51] Y. Xu and W. Yin. A globally convergent algorithm for nonconvex optimization based on block coordinate update. _Journal of Scientific Computing_, pages 1-35, 2017.

* [52] F. Yousefian, A. Nedic, and U. V. Shanbhag. On stochastic gradient and subgradient methods with adaptive steplength sequences. _Automatica_, 48(1):56-67, 2012.
* [53] H. Yu, S. Yang, and S. Zhu. Parallel restarted SGD with faster convergence and less communication: demystifying why model averaging works for deep learning. In _AAAI Conference on Artificial Intelligence_, 2019.
* [54] H. Yuan and T. Ma. Federated accelerated stochastic gradient descent. _Advances in Neural Information Processing Systems_, 33:5332-5344, 2020.
* [55] X. Yuan and P. Li. On convergence of fedprox: Local dissimilarity invariant bounds, non-smoothness and beyond. _Advances in Neural Information Processing Systems_, 35:10752-10765, 2022.
* [56] J. Zhang, H. Lin, S. Jegelka, S. Sra, and A. Jadbabaie. Complexity of finding stationary points of nonconvex nonsmooth functions. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 11173-11182. PMLR, 13-18 Jul 2020.
* [57] L. Zhang, D. Xu, S. Yuan, and X. Wu. FairGAN: Fairness-aware generative adversarial networks. In _CoRR_, 2018.
* [58] F. Zhou and G. Cong. On the convergence properties of a K-step averaging stochastic gradient descent algorithm for nonconvex optimization. In _Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence_, 2018.