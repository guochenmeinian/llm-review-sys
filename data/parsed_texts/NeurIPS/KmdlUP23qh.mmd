[MISSING_PAGE_FAIL:1]

2. \(\mathcal{S}_{\rm tr}\) is wider and covers \(\mathcal{S}_{\rm te}\), i.e., \(\mathcal{S}_{\rm tr}\supset\mathcal{S}_{\rm te}\) and \(\mathcal{S}_{\rm tr}\setminus\mathcal{S}_{\rm te}\neq\emptyset\);
3. \(\mathcal{S}_{\rm te}\) is wider and covers \(\mathcal{S}_{\rm tr}\), i.e., \(\mathcal{S}_{\rm tr}\subset\mathcal{S}_{\rm te}\) and \(\mathcal{S}_{\rm te}\setminus\mathcal{S}_{\rm tr}\neq\emptyset\);
4. \(\mathcal{S}_{\rm tr}\) and \(\mathcal{S}_{\rm te}\) partially overlap, i.e., \(\mathcal{S}_{\rm tr}\cap\mathcal{S}_{\rm te}\neq\emptyset\), \(\mathcal{S}_{\rm tr}\setminus\mathcal{S}_{\rm te}\neq\emptyset\), and \(\mathcal{S}_{\rm te}\setminus\mathcal{S}_{\rm tr}\neq\emptyset\).1 Footnote 1: When \(\mathcal{S}_{\rm tr}\) and \(\mathcal{S}_{\rm te}\) differ, they differ by a non-zero probability measure; otherwise, we regard it as case (i). For example, \(\mathcal{S}_{\rm te}\setminus\mathcal{S}_{\rm tr}\neq\emptyset\) in cases (iii) and (iv) means that \(\sum_{y}\int_{\{\bm{x}:(\bm{x},y)\in\mathcal{S}_{\rm te}\setminus\mathcal{S}_{ \rm tr}\}}p_{\rm te}(\bm{x},y)\mathrm{d}\bm{x}>0\).

The four cases are illustrated in Figure 1. We focus on cases (iii) and (iv), as they are more general and more difficult than cases (i) and (ii).

Problem settingDenote by \(\mathcal{X}\) and \(\mathcal{Y}\) the input and output domains, where \(\mathcal{Y}=\{1,\ldots,C\}\) for \(C\)-class classification problems. Let \(\bm{f}:\mathcal{X}\to\mathbb{R}^{C}\) be a classifier (to be trained) and \(\ell:\mathbb{R}^{C}\times\mathcal{Y}\to(0,+\infty)\) be a loss function (for training \(\bm{f}\)).2 Then, the _risk_ is defined as follows (Vapnik, 1998):

Footnote 2: The positivity of \(\ell\), i.e., \(\ell(\bm{f}(\bm{x}),y)>0\) rather than \(\ell(\bm{f}(\bm{x}),y)\geq 0\), is needed to prove Theorem 2. This assumption holds for the popular _cross-entropy loss_ and its robust variants, since \(\ell(\bm{x})\) must stay finite.

\[R(\bm{f})=\mathbb{E}_{p_{\rm te}(\bm{x},y)}[\ell(\bm{f}(\bm{x}),y)],\] (1)

where \(\mathbb{E}[\cdot]\) denotes the expectation. In the joint-shift problems, we are given a training set \(\mathcal{D}_{\rm tr}=\{(\bm{x}_{i}^{\rm tr},y_{i}^{\rm tr})\}_{i=1}^{n_{\rm tr }}\stackrel{{\rm i.i.d.}}{{\sim}}p_{\rm tr}(\bm{x},y)\) and a validation set \(\mathcal{D}_{\rm v}=\{(\bm{x}_{i}^{\rm v},y_{i}^{\rm v})\}_{i=1}^{n_{\rm v}} \stackrel{{\rm i.i.d.}}{{\sim}}p_{\rm te}(\bm{x},y)\), where \(\mathcal{D}_{\rm tr}\) is much bigger than \(\mathcal{D}_{\rm v}\), i.e., \(n_{\rm tr}\gg n_{\rm v}\). The goal is to reliably estimate the risk from \(\mathcal{D}_{\rm tr}\) and \(\mathcal{D}_{\rm v}\) and train \(\bm{f}\) by minimizing the empirical risk, which should outperform training \(\bm{f}\) from only \(\mathcal{D}_{\rm v}\).

Motivation_Importance weighting_ (IW) has been a golden solver for DS problems (Sugiyama and Kawanabe, 2012), and there are many great off-the-shelf IW methods (Huang et al., 2007; Sugiyama et al., 2007a,b; Kanamori et al., 2009). Recently, _dynamic IW_ (DIW) was proposed to make IW compatible with stochastic optimizers and thus it can be used for deep learning (Fang et al., 2020). However, all IW methods including DIW have assumed cases (i) and (ii)--in cases (iii) and (iv), IW methods become problematic. Specifically, as the importance weights are only used on \(\mathcal{S}_{\rm tr}\), even though they become ill-defined on \(\mathcal{S}_{\rm te}\setminus\mathcal{S}_{\rm tr}\), IW itself is still well-defined. Nevertheless, in such a situation, the IW _identity_ will become an _inequality_ (i.e., Theorem 2), which means that what we minimize for training is no longer an approximation of the original risk \(R(\bm{f})\) and thus IW may lead to poor trained classifiers (i.e., Proposition 3). Moreover, some IW-like methods based on bilevel optimization share a similar issue with IW (Jiang et al., 2018; Ren et al., 2018; Shu et al., 2019), since \(\bm{f}\) is only trained from \(\mathcal{D}_{\rm tr}\) where \(\mathcal{D}_{\rm v}\) is used to determine the importance weights on \(\mathcal{D}_{\rm tr}\). In fact, cases (iii) and (iv) are more common nowadays due to _data-collection biases_, but they are still under-explored. For example, a class has several subclasses, but not all subclasses are presented in \(\mathcal{D}_{\rm tr}\) (see Figure 2). Therefore, we want to generalize IW to a universal solver for all the four cases.

ContributionsOur contributions can be summarized as follows.

* Firstly, we theoretically and empirically analyze when and why IW methods can succeed/may fail. We reveal that the objective of IW is good in cases (i) and (ii) and bad in cases (iii) and (iv).
* Secondly, we propose _generalized IW_ (GIW). In GIW, \(\mathcal{S}_{\rm te}\) is split into an _in-training_ (IT) part \(\mathcal{S}_{\rm te}\cap\mathcal{S}_{\rm tr}\) and an _out-of-training_ (OOT) part \(\mathcal{S}_{\rm te}\setminus\mathcal{S}_{\rm tr}\), and its objective consists of a weighted classification term over the IT part and a standard classification term over the OOT part. GIW is justified as its objective is good in all the four cases and reduces to IW in cases (i) and (ii). Thus, GIW is a _strict generalization_ of IW from the objective point of view, and GIW is safer to be used when we are not sure whether the problem to be solved is a good case or a bad case for IW.3 Footnote 3: Even though we have divided all DS problems into four cases according to support shift (SS), we cannot empirically detect tiny or huge SS (under joint shift). Given that there is already DS, existing _two-sample test_ methods for detecting DS are not good at detecting whether there is SS or not--their results must be positive.
* Thirdly, we provide a practical implementation of GIW: (a) following the split of \(\mathcal{S}_{\rm te}\), \(\mathcal{D}_{\rm v}\) is split into an IT set and an OOT set using the _one-class support vector machine_(Scholkopf et al., 1999);

Figure 1: An illustration of the relationship between the training support and the test support.

(b) the IT set, instead of the whole \(\mathcal{D}_{\mathrm{v}}\), is used for IW; and (c) the OOT set directly joins training together with \(\mathcal{D}_{\mathrm{tr}}\) since no data in \(\mathcal{D}_{\mathrm{tr}}\) comes from the OOT part.
* Finally, we design and conduct extensive experiments that demonstrate the effectiveness of GIW in cases (iii) and (iv). The experiment design is also a major contribution since no experimental setup is available for reference to simulate case (iii) or (iv) on benchmark datasets.

OrganizationThe analyses of IW are in Section 2, the proposal of GIW is in Section 3, and the experiments are in Section 4. Related work and additional experiments are in the appendices.

## 2 A deeper understanding of IW

First, we review the traditional _importance weighting_ (IW) and its modern implementation _dynamic importance weighting_ (DIW). Then, we analyze when and why IW methods can succeed/may fail.

A review of IWLet \(w^{*}(\bm{x},y)=p_{\mathrm{te}}(\bm{x},y)/p_{\mathrm{tr}}(\bm{x},y)\), which is the ratio of the test density \(p_{\mathrm{te}}(\bm{x},y)\) over the training density \(p_{\mathrm{tr}}(\bm{x},y)\), known as the _importance function_. Then, the _expected objective_ of IW can be expressed as

\[J(\bm{f})=\mathbb{E}_{p_{\mathrm{tr}}(\bm{x},y)}[w^{*}(\bm{x},y)\ell(\bm{f}( \bm{x}),y)].\] (2)

In order to empirically approximate \(J(\bm{f})\) in (2), we need to have an empirical version \(\widehat{w}(\bm{x},y)\) of \(w^{*}(\bm{x},y)\), so that the _empirical objective_ of IW is4

Footnote 4: As an empirical risk estimator, \(\widehat{J}(\bm{f})\) in Eq. (3) is _not unbiased_ to \(J(\bm{f})\) in Eq. (2), since \(\widehat{w}(\bm{x},y)\) is not unbiased to \(w^{*}(\bm{x},y)\). As far as we know, in IW, there is no unbiased importance-weight estimator and thus no unbiased risk estimator. That being said, \(\widehat{J}(\bm{f})\) can still be _(statistically) consistent_ with \(J(\bm{f})\) under mild conditions if \(\widehat{w}(\bm{x},y)\) is consistent with \(w^{*}(\bm{x},y)\).

\[\widehat{J}(\bm{f})=\tfrac{1}{n_{\mathrm{tr}}}\sum_{i=1}^{n_{\mathrm{tr}}} \widehat{w}(\bm{x}_{i}^{\mathrm{tr}},y_{i}^{\mathrm{tr}})\ell(\bm{f}(\bm{x}_{ i}^{\mathrm{tr}}),y_{i}^{\mathrm{tr}}).\] (3)

The original IW method is implemented in two steps: (I) _weight estimation_ (WE) where \(\widehat{w}(\bm{x},y)\) is obtained and (II) _weighted classification_ (WC) where \(\widehat{J}(\bm{f})\) is minimized. The first step relies on the training data \(\mathcal{D}_{\mathrm{tr}}\) and the validation data \(\mathcal{D}_{\mathrm{v}}\), and it can be either estimating the two density functions separately and taking their ratio or directly estimating the density ratio (Sugiyama et al., 2012).

Figure 2: Two concrete examples of the success and failure of IW in case (iii).

A review of DIWThe aforementioned two-step approach is very nice when the classifier \(\bm{f}\) is a simple model, but it has a serious issue when \(\bm{f}\) is a deep model (Fang et al., 2020). Since WE is not equipped with representation learning, in order to boost its expressive power, we need an external feature extractor such as an internal representation learned by WC. As a result, we are trapped by a _circular dependency_: originally we need \(w^{*}\) to train \(\bm{f}\); now we need a trained \(\bm{f}\) to estimate \(w^{*}\).

DIW (Fang et al., 2020) has been proposed to resolve the critical circular dependency and to make IW usable for deep learning. Specifically, DIW uses a non-linear transformation \(\pi\) created from the current \(\bm{f}\) (being trained) and replaces \(w^{*}(\bm{x},y)\) with \(w^{*}(\bm{z})=p_{\mathrm{te}}(\bm{z})/p_{\mathrm{tr}}(\bm{z})\), where \(\bm{z}=\pi(\bm{x},y)\) is the current _loss-value_ or _hidden-layer-output_ representation of \((\bm{x},y)\). DIW iterates between WE for estimating \(w^{*}(\bm{z})\) and WC for training \(\bm{f}\) and thus updating \(\pi\) in a seamless mini-batch-wise manner. Given that WE enjoys representation learning inside WC, the importance-weight estimation quality of WE and the classifier training quality of WC can improve each other gradually but significantly.

Risk consistency/inconsistency of IWNow, consider how to qualify good or bad expected objectives under different conditions. To this end, we adopt the concepts of _risk consistency_ and _classifier consistency_ from the label-noise learning literature (Xia et al., 2019, 2020; Yao et al., 2020).

**Definition 1**.: Given an (expected) objective \(J(\bm{f})\), we say it is _risk-consistent_ if \(J(\bm{f})=R(\bm{f})\) for any \(\bm{f}\), i.e., the objective is equal to the original risk for any classifier. On the other hand, we say \(J(\bm{f})\) is _classifier-consistent_ if \(\arg\min_{\bm{f}}J(\bm{f})=\arg\min_{\bm{f}}R(\bm{f})\) where the minimization is taken over all measurable functions, i.e., the objective shares the optimal classifier with the original risk.

In the definition above, risk consistency is conceptually stronger than classifier consistency. If an objective is risk-consistent, it must also be classifier-consistent; if it is classifier-consistent, it may sometimes be risk-inconsistent. Note that a risk-inconsistent objective is not necessarily very bad, as it can still be classifier-consistent.5 Hence, when considering expected objectives, risk consistency is a sufficient condition and classifier consistency is a necessary condition for good objectives.

Footnote 5: For example, if \(J(\bm{f})=R(\bm{f})/2\) or if \(J(\bm{f})=R(\bm{f})^{2}\), minimizing \(J(\bm{f})\) will give exactly the same optimal classifier as minimizing \(R(\bm{f})\).

In what follows, we analyze when and why the objective of IW, namely \(J(\bm{f})\) in (2), can be a good objective or may be a bad objective.

**Theorem 1**.: _In cases (i) and (ii), IW is risk-consistent.6_

Footnote 6: In fact, if there is only the instance random variable \(\bm{x}\) but no class-label random variable \(y\), this result is simply the _importance-sampling identity_ that can be found in many statistics textbooks. We prove it here to make our theoretical analyses self-contained and make later theoretical results easier to present/understand.

Proof.: Recall that \(\mathcal{S}_{\mathrm{tr}}=\{(\bm{x},y):p_{\mathrm{tr}}(\bm{x},y)>0\}\) and \(\mathcal{S}_{\mathrm{te}}=\{(\bm{x},y):p_{\mathrm{te}}(\bm{x},y)>0\}\). Under case (i) or (ii), let us rewrite \(R(\bm{f})\) and \(J(\bm{f})\) with summations and integrals:

\[R(\bm{f}) =\sum_{y=1}^{C}\int_{\{\bm{x}:(\bm{x},y)\in\mathcal{S}_{\mathrm{te }}\}}\ell(\bm{f}(\bm{x}),y)p_{\mathrm{te}}(\bm{x},y)\mathrm{d}\bm{x},\] \[J(\bm{f}) =\sum_{y=1}^{C}\int_{\{\bm{x}:(\bm{x},y)\in\mathcal{S}_{\mathrm{ tr}}\}}\ell(\bm{f}(\bm{x}),y)w^{*}(\bm{x},y)p_{\mathrm{tr}}(\bm{x},y) \mathrm{d}\bm{x}\] \[=\sum_{y=1}^{C}\int_{\{\bm{x}:(\bm{x},y)\in\mathcal{S}_{\mathrm{ tr}}\}}\ell(\bm{f}(\bm{x}),y)p_{\mathrm{te}}(\bm{x},y)\mathrm{d}\bm{x},\]

where \(w^{*}(\bm{x},y)=p_{\mathrm{te}}(\bm{x},y)/p_{\mathrm{tr}}(\bm{x},y)\) is always well-defined over \(\mathcal{S}_{\mathrm{tr}}\) and we safely plugged this definition into the rewritten \(J(\bm{f})\). Subsequently, in case (i), \(\mathcal{S}_{\mathrm{tr}}=\mathcal{S}_{\mathrm{te}}\) and thus \(J(\bm{f})=R(\bm{f})\). In case (ii), \(\mathcal{S}_{\mathrm{tr}}\supset\mathcal{S}_{\mathrm{te}}\) and then we further have

\[J(\bm{f})=\sum_{y=1}^{C}\int_{\{\bm{x}:(\bm{x},y)\in\mathcal{S}_{\mathrm{te }}\}}\ell(\bm{f}(\bm{x}),y)p_{\mathrm{te}}(\bm{x},y)\mathrm{d}\bm{x}+\int_{\{\bm {x}:(\bm{x},y)\in\mathcal{S}_{\mathrm{tr}}\setminus\mathcal{S}_{\mathrm{te}}\}} \ell(\bm{f}(\bm{x}),y)p_{\mathrm{te}}(\bm{x},y)\mathrm{d}\bm{x}.\]

By definition, \(p_{\mathrm{te}}(\bm{x},y)=0\) outside \(\mathcal{S}_{\mathrm{te}}\) including \(\mathcal{S}_{\mathrm{tr}}\setminus\mathcal{S}_{\mathrm{te}}\), and thus \(J(\bm{f})=R(\bm{f})\). 

**Theorem 2**.: _In cases (iii) and (iv), IW is risk-inconsistent, and it holds that \(J(\bm{f})<R(\bm{f})\) for any \(\bm{f}\)._

Proof.: Since \(w^{*}(\bm{x},y)\) is well-defined over \(\mathcal{S}_{\mathrm{tr}}\) but it becomes ill-defined over \(\mathcal{S}_{\mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\), we cannot naively replace the integral domain in \(J(\bm{f})\) as in the proof of Theorem 1. In case (iii), \(\mathcal{S}_{\mathrm{tr}}\subset\mathcal{S}_{\mathrm{te}}\), and consequently

\[R(\bm{f}) =\sum_{y=1}^{C}\int_{\{\bm{x}:(\bm{x},y)\in\mathcal{S}_{\mathrm{ tr}}\}}\ell(\bm{f}(\bm{x}),y)p_{\mathrm{te}}(\bm{x},y)\mathrm{d}\bm{x}\] \[\quad+\sum_{y=1}^{C}\int_{\{\bm{x}:(\bm{x},y)\in\mathcal{S}_{ \mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\}}\ell(\bm{f}(\bm{x}),y)p_{ \mathrm{te}}(\bm{x},y)\mathrm{d}\bm{x}.\]According to Theorem 1 for case (i), the first term in the rewritten \(R(\bm{f})\) equals \(J(\bm{f})\). Moreover, the second term is positive, since \(\ell(\bm{f}(\bm{x}),y)>0\) due to the positivity of \(\ell\), and \(p_{\mathrm{te}}(\bm{x},y)>0\) over \(\mathcal{S}_{\mathrm{te}}\) including \(\mathcal{S}_{\mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\). As a result, in case (iii), \(R(\bm{f})>J(\bm{f})\).

Similarly, in case (iv), we can split \(\mathcal{S}_{\mathrm{te}}\) into \(\mathcal{S}_{\mathrm{te}}\cap\mathcal{S}_{\mathrm{tr}}\) and \(\mathcal{S}_{\mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\) and decompose \(R(\bm{f})\) as

\[R(\bm{f}) =\sum_{y=1}^{C}\int_{\{\bm{x}:(\bm{x},y)\in\mathcal{S}_{\mathrm{te }}\cap\mathcal{S}_{\mathrm{tr}}\}}\ell(\bm{f}(\bm{x}),y)p_{\mathrm{te}}(\bm{x},y)\mathrm{d}\bm{x}\] \[\quad+\sum_{y=1}^{C}\int_{\{\bm{x}:(\bm{x},y)\in\mathcal{S}_{ \mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\}}\ell(\bm{f}(\bm{x}),y)p_{ \mathrm{te}}(\bm{x},y)\mathrm{d}\bm{x}.\]

Note that \(\mathcal{S}_{\mathrm{te}}\cap\mathcal{S}_{\mathrm{tr}}\subset\mathcal{S}_{ \mathrm{tr}}\), so that according to Theorem 1 for case (ii), the first term equals \(J(\bm{f})\). Following case (iii), the second term is positive. Therefore, in case (iv), \(R(\bm{f})>J(\bm{f})\). 

Theorem 1 implies that the objective of IW can be a good objective in cases (i) and (ii). Theorem 2 implies that the objective of IW may be a bad objective in cases (iii) and (iv). As a consequence, the theorems collectively address when and why IW methods can succeed/may fail.7

Footnote 7: For any possible implementation, as long as it honestly implements the objective of IW, the quality of the objective will be reflected in the quality of the implementation.

When the IW objective may be bad and IW methods may fail, whether an IW method fails or not depends on many factors, such as the underlying data distributions, the sampled data sets, the loss, the model, and the optimizer. To illustrate this phenomenon, here we give two concrete examples belonging to case (iii), where IW has no problem at all in one example and is as poor as random guessing in the other example.

Two concrete examplesWe have seen the examples in Figure 2. In both examples, there are two classes marked with red and blue colors and distributed in four squares. Each square has a unit area and is the support of a uniform distribution of \(\bm{x}\), i.e., \(p(\bm{x},1)=1\) and \(p(\bm{x},0)=0\) if its color is red, and \(p(\bm{x},0)=1\) and \(p(\bm{x},1)=0\) if its color is blue. There is a margin of 0.1 between two adjacent squares. The training distribution consists of the two squares on the left, and the test distribution consists of all the four squares. In the first example, on \(\mathcal{S}_{\mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\), the label is red on the top and blue on the bottom, as same as the label on \(\mathcal{S}_{\mathrm{tr}}\). In the second example, on \(\mathcal{S}_{\mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\), the label is blue on the top and red on the bottom, as opposite as the label on \(\mathcal{S}_{\mathrm{tr}}\).

We experimentally validated whether DIW works or not. The number of training data was 200. The number of validation data was only 4: we sampled one random point from each training square and added the center point of each test-only square. We can see that DIW performs very well in the first example, better than training from only the validation data; unfortunately, DIW performs very poorly in the second example, even worse than training from only the validation data.

The observed phenomenon should not be limited to DIW but be common to all IW methods. Here, we analyze why this phenomenon does not depend on the loss, the model, or the optimizer.

**Proposition 3**.: _In the first example, IW is classifier-consistent, while in the second example, IW is classifier-inconsistent.8_

Footnote 8: As the classifier \(\bm{f}\) can only access the information on \(\mathcal{S}_{\mathrm{tr}}\) but must predict the class label on the whole \(\mathcal{S}_{\mathrm{te}}\), we assume that \(\bm{f}\) would transfer its knowledge about \(p_{\mathrm{tr}}(\bm{x},y)\) to \(p_{\mathrm{te}}(\bm{x},y)\) in the simplest manner. Furthermore, we simplify \(\bm{f}(\bm{x})\) into \(f(x^{(1)},x^{(2)})\), since it is binary classification where \(\bm{x}\in\mathbb{R}^{2}\).

Proof.: Without loss of generality, assume that \(\ell\) is _classification-calibrated_(Bartlett et al., 2006).9 Let \((c^{(1)},c^{(2)})\) be the center of \(\mathcal{S}_{\mathrm{te}}\), and then the four squares are located on the top-left, bottom-left, top-right, and bottom-right of \((c^{(1)},c^{(2)})\). For convenience, we abbreviate \(f(x^{(1)},x^{(2)})\) for \(x^{(1)}>c^{(1)},x^{(2)}>c^{(2)}\) as \(f(+,+)\), \(f(x^{(1)},x^{(2)})\) for \(x^{(1)}>c^{(1)},x^{(2)}<c^{(2)}\) as \(f(+,-)\), and so on.

Footnote 9: The cross-entropy loss is _confidence-calibrated_ and thus classification-calibrated(Sugiyama et al., 2022).

Consider the first example. The minimizer of \(R(f)\) can be any _Bayes-optimal classifier_, i.e., any \(f\) such that \(f(\cdot,+)>0\) and \(f(\cdot,-)<0\). Next, on the top-left square, we have \(p_{\mathrm{te}}(\bm{x},1)=1/4\), \(p_{\mathrm{te}}(\bm{x},0)=0\), \(p_{\mathrm{tr}}(\bm{x},1)=1/2\), and \(p_{\mathrm{tr}}(\bm{x},0)=0\), and thus \(w^{*}(\bm{x},y)=1/2\). Likewise, on the bottom-left square, we have \(w^{*}(\bm{x},y)=1/2\). As a result, \(J(f)=\frac{1}{2}\mathbb{E}_{p_{\mathrm{tr}}(\bm{x},y)}[\ell(\bm{f}(\bm{x}),y)]\), meaning that the minimizer of \(J(f)\) can be any Bayes-optimal classifier on \(\mathcal{S}_{\mathrm{tr}}\), i.e., any \(f\) such that \(f(-,+)>0\) and \(f(-,-)<0\). The simplest manner for \(f\) to transfer its knowledge from \(p_{\mathrm{tr}}\) to \(p_{\mathrm{te}}\) is to have a linear decision boundary and extend it to \(\mathcal{S}_{\mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\), so that \(f(\cdot,+)>0\) and \(f(\cdot,-)<0\) on \(\mathcal{S}_{\mathrm{te}}\). We can see that the set of minimizers is shared and thus IW is classifier-consistent.

Consider the second example. The minimizer of \(J(f)\) is still the same while the minimizer of \(R(f)\) significantly changes to any \(f\) such that \(f(-,+)>0\), \(f(-,-)<0\), \(f(+,+)<0\), and \(f(+,-)>0\). This non-linear decision boundary is a checkerboard where any two adjacent squares have opposite predictions. It is easy to see that IW is classifier-inconsistent and its test accuracy is 0.5. For binary classification with balanced classes, this accuracy is as poor as random guessing. 

## 3 Generalized importance weighting (GIW)

We have seen two examples where IW is as good/bad as possible in case (iii). In practice, we cannot rely on the luck and hope that IW would work. In this section, we propose _generalized importance weighting_ (GIW), which is still IW in cases (i) and (ii) and is better than IW in cases (iii) and (iv).

### Expected objective of GIW

The key idea of GIW is to split the test support \(\mathcal{S}_{\mathrm{te}}\) into the _in-training_ (IT) part \(\mathcal{S}_{\mathrm{te}}\cap\mathcal{S}_{\mathrm{tr}}\) and the _out-of-training_ (OOT) part \(\mathcal{S}_{\mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\). More specifically, we introduce a third random variable, the support-splitting variable \(s\in\{0,1\}\), such that \(s\) takes 1 on \(\mathcal{S}_{\mathrm{tr}}\) and 0 on \(\mathcal{S}_{\mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\). As a result, the underlying joint density \(p(\bm{x},y,s)\) can be defined by \(p_{\mathrm{te}}(\bm{x},y)\) as10

Footnote 10: Here, \(p(\bm{x},y,s)\) accepts \((\bm{x},y)\in\mathcal{S}_{\mathrm{te}}\cup\mathcal{S}_{\mathrm{tr}}\) rather than \((\bm{x},y)\in\mathcal{S}_{\mathrm{te}}\), and \(s\) actually splits \(\mathcal{S}_{\mathrm{te}}\cup\mathcal{S}_{\mathrm{tr}}\) into \(\mathcal{S}_{\mathrm{tr}}\) and \(\mathcal{S}_{\mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\). This is exactly what we want, since \(p_{\mathrm{te}}(\bm{x},y)=0\) outside \(\mathcal{S}_{\mathrm{te}}\).

\[p(\bm{x},y,s)=\begin{cases}p_{\mathrm{te}}(\bm{x},y)&\text{if }(\bm{x},y)\in \mathcal{S}_{\mathrm{tr}}\text{ and }s=1,\text{ or }(\bm{x},y)\in\mathcal{S}_{\mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\text{ and }s=0,\\ 0&\text{if }(\bm{x},y)\in\mathcal{S}_{\mathrm{tr}}\text{ and }s=0,\text{ or }(\bm{x},y)\in \mathcal{S}_{\mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\text{ and }s=1.\end{cases}\] (4)

Let \(\alpha=p(s=1)\). Then, the expected objective of GIW is defined as

\[J_{\mathrm{G}}(\bm{f})=\alpha\mathbb{E}_{p_{\mathrm{tr}}(\bm{x},y)}[w^{*}(\bm {x},y)\ell(\bm{f}(\bm{x}),y)]+(1-\alpha)\mathbb{E}_{p(\bm{x},y|s=0)}[\ell(\bm{ f}(\bm{x}),y)].\] (5)

The corresponding empirical version \(\widehat{J}_{\mathrm{G}}(\bm{f})\) will be derived in the next subsection. Before proceeding to the empirical objective of GIW, we establish risk consistency of GIW.

**Theorem 4**.: _GIW is always risk-consistent for distribution shift problems._

Proof.: Let us work on the first term of \(J_{\mathrm{G}}(\bm{f})\) in (5). When \((\bm{x},y)\in\mathcal{S}_{\mathrm{tr}}\),

\[\alpha w^{*}(\bm{x},y)p_{\mathrm{tr}}(\bm{x},y)=\alpha p_{\mathrm{te}}(\bm{x},y)=p(s=1)p(\bm{x},y\mid s=1)=p(\bm{x},y,s=1),\]

where \(p_{\mathrm{te}}(\bm{x},y)=p(\bm{x},y\mid s=1)\) given \((\bm{x},y)\in\mathcal{S}_{\mathrm{tr}}\) according to (4). Since \(p(\bm{x},y,s=1)=0\) on \(\mathcal{S}_{\mathrm{te}}\setminus\mathcal{S}_{\mathrm{tr}}\), we have

\[\alpha\mathbb{E}_{p_{\mathrm{tr}}(\bm{x},y)}[w^{*}(\bm{x},y)\ell(\bm{f}(\bm{x }),y)]=\sum_{y=1}^{C}\int_{(\bm{x}:(\bm{x},y)\in\mathcal{S}_{\mathrm{te}})} \ell(\bm{f}(\bm{x}),y)p(\bm{x},y,s=1)\mathrm{d}\bm{x}.\] (6)

Next, for the second term of \(J_{\mathrm{G}}(\bm{f})\), since \((1-\alpha)p(\bm{x},y\mid s=0)=p(\bm{x},y,s=0)\), we have

\[(1-\alpha)\mathbb{E}_{p(\bm{x},y|s=0)}[\ell(\bm{f}(\bm{x}),y)]=\sum_{y=1}^{C} \int_{(\bm{x}:(\bm{x},y)\in\mathcal{S}_{\mathrm{te}})}\ell(\bm{f}(\bm{x}),y)p( \bm{x},y,s=0)\mathrm{d}\bm{x}.\] (7)

Note that \(p(\bm{x},y,s=1)+p(\bm{x},y,s=0)=p_{\mathrm{te}}(\bm{x},y)\) according to (4). By adding (6) and (7), we can obtain that \(J_{\mathrm{G}}(\bm{f})=R(\bm{f})\). This conclusion holds in all the four cases. 

Theorem 4 is the main theorem of this paper. It implies that the objective of GIW can always be a good objective. Recall that IW is also risk-consistent in cases (i) and (ii), and it is interesting to see how IW and GIW are connected. By definition, given fixed \(p_{\mathrm{tr}}(\bm{x},y)\) and \(p_{\mathrm{te}}(\bm{x},y)\), if there exists a risk-consistent objective, it is unique. Indeed, in cases (i) and (ii), GIW is reduced to IW, simply due to that \(\alpha=1\) and \(J_{\mathrm{G}}(\bm{f})=J(\bm{f})\) for any \(\bm{f}\).

### Empirical objective and practical implementation of GIW

Approximating \(J_{\mathrm{G}}(\bm{f})\) in (5) is more involved than approximating \(J(\bm{f})\) in (2). Following (3), we need an empirical version \(\widehat{w}(\bm{x},y)\), and we need further to split the validation data \(\mathcal{D}_{\mathrm{v}}\) into two sets and estimate \(\alpha\). Obviously, how to accurately split the validation data is the most challenging part. After splitting \(\mathcal{D}_{\mathrm{v}}\) and obtaining an estimate \(\widehat{\alpha}\), the empirical objective will have two terms, where the first term can be handled by any IW algorithm given training data and IT validation data, and the second term just involves OOT validation data.

To split \(\mathcal{D}_{\mathrm{v}}\) and estimate \(\alpha\), we employ the _one-class support vector machine_ (O-SVM) (Scholkopf et al., 1999). Firstly, we pretrain a deep network for classification on the training data \(\mathcal{D}_{\mathrm{tr}}\) a little bit and obtain a _feature extractor_ from the pretrained deep network. Secondly, we apply the feature extractor on the instances in \(\mathcal{D}_{\mathrm{tr}}\) and train an O-SVM based on the latent representation of these instances, giving us a score function \(g(\bm{z})\) that could predict whether \(p_{\mathrm{tr}}(\bm{x})>0\) or not, where \(\bm{z}\) is the latent representation of \(\bm{x}\). Thirdly, we apply the feature extractor on the instances in \(\mathcal{D}_{\mathrm{v}}\) and then employ \(g(\bm{z})\) to obtain the IT validation data \(\mathcal{D}_{\mathrm{v1}}=\{(\bm{x}_{i}^{\nu 1},y_{i}^{\nu 1})\}_{i=1}^{n_{\nu 1}}\) and the OOT validation data \(\mathcal{D}_{\mathrm{v2}}=\{(\bm{x}_{i}^{\nu 2},y_{i}^{\nu 2})\}_{i=1}^{n_{\nu 2}}\). Finally, \(\alpha\) can be naturally estimated as \(\widehat{\alpha}=n_{\mathrm{v1}}/n_{\mathrm{v}}\).

We have two comments on the split of \(\mathcal{D}_{\mathrm{v}}\). The O-SVM \(g(\bm{z})\) predicts whether \(p_{\mathrm{tr}}(\bm{x})>0\) or not rather than whether \(p_{\mathrm{tr}}(\bm{x},y)>0\) or not. This is because the \(\bm{x}\)-support change is often sufficiently informative in practice: when the \(\bm{x}\)-support changes, O-SVM can detect it; when the \((\bm{x},y)\)-support changes without changing the \(\bm{x}\)-support, it will be very difficult to train an O-SVM based on the loss-value representation of \((\bm{x},y)\) to detect it, but such changes are very rare. The other comment is about the choice of the O-SVM. While there are more advanced one-class classification methods (Hido et al., 2011; Zaheer et al., 2020; Hu et al., 2020; Goldwasser et al., 2020) (see Perera et al. (2021) for a survey), the O-SVM is already good enough for the purpose (see Appendix C.1).

Subsequently, \(\mathcal{D}_{\mathrm{v1}}\) can be viewed as being drawn from \(p(\bm{x},y\mid s=1)\), and \(\mathcal{D}_{\mathrm{v2}}\) can be viewed as being drawn from \(p(\bm{x},y\mid s=0)\). Based on \(\mathcal{D}_{\mathrm{tr}}\) and \(\mathcal{D}_{\mathrm{v1}}\), we can obtain either \(\widehat{w}(\bm{x},y)\) or \(\widehat{w}_{i}\) for each \((\bm{x}_{i}^{\mathrm{tr}},y_{i}^{\mathrm{tr}})\) by IW. IW has no problem here since the split of \(\mathcal{D}_{\mathrm{v}}\) can reduce case (iii) to case (i) and case (iv) to case (ii). In the implementation, we employ DIW (Fang et al., 2020) because it is friendly to deep learning and it is a state-of-the-art IW method. Finally, the empirical objective of GIW can be expressed as

\[\widehat{J}_{\mathrm{G}}(\bm{f})=\tfrac{n_{\mathrm{v1}}}{n_{\mathrm{v}}n_{ \mathrm{tr}}}\sum_{i=1}^{n_{\mathrm{tr}}}\widehat{w}(\bm{x}_{i}^{\mathrm{tr}}, y_{i}^{\mathrm{tr}})\ell(\bm{f}(\bm{x}_{i}^{\mathrm{tr}}),y_{i}^{\mathrm{tr}})+ \tfrac{1}{n_{\mathrm{v}}}\sum_{j=1}^{n_{\mathrm{v2}}}\ell(\bm{f}(\bm{x}_{j}^{ \nu 2}),y_{j}^{\nu 2}),\] (8)

where the two expectations in \(J_{\mathrm{G}}(\bm{f})\) are approximated separately with \(\mathcal{D}_{\mathrm{tr}}\) and \(\mathcal{D}_{\mathrm{v2}}\).11

Footnote 11: In GIW, although \(J_{\mathrm{G}}(\bm{f})=R(\bm{f})\), \(\widehat{J}_{\mathrm{G}}(\bm{f})\) is not an unbiased estimator of \(J_{\mathrm{G}}(\bm{f})\), exactly the same as what happened in IW. Nevertheless, \(\widehat{J}_{\mathrm{G}}(\bm{f})\) can still be statistically consistent with \(J_{\mathrm{G}}(\bm{f})\) under mild conditions if \(\widehat{w}(\bm{x},y)\) is consistent with \(w^{*}(\bm{x},y)\). Specifically, though the outer weighted classification is an optimization problem, the inner weight estimation is an estimation problem. The statistical consistency of \(\widehat{w}(\bm{x},y)\) requires _zero approximation error_ and thus _non-parametric estimation_ is preferred. For kernel-based IW methods such as Huang et al. (2007) and Kanamori et al. (2009), it holds that as \(n_{\mathrm{tr}},n_{\mathrm{v}}\to\infty\), \(\widehat{w}(\bm{x},y)\to w^{*}(\bm{x},y)\) under mild conditions. If so, we can establish statistical consistency of \(\widehat{J}_{\mathrm{G}}(\bm{f})\) with \(J_{\mathrm{G}}(\bm{f})\). If we further assume that the function class of \(\bm{f}\) has a bounded complexity and \(\ell\) is bounded and Lipschitz continuous, we can establish statistical consistency of \(R(\widehat{\bm{f}})\) with \(R(\bm{f}^{*})\), where \(\widehat{\bm{f}}(\bm{x})\) and \(\bm{f}^{*}(\bm{x})\) are the minimizers of \(\widehat{J}_{\mathrm{G}}(\bm{f})\) and \(R(\bm{f})\).

The practical implementation of GIW is presented in Algorithm 1. Here, we adopt the hidden-layer-output representation for O-SVM in ValDataSplit and the loss-value representation for DIW in ModelTrain. This algorithm design is convenient for both O-SVM and DIW; the hidden-layer-output representation for DIW has been tried and can be found in Section 4.3.

## 4 Experiments

In this section, we empirically evaluate GIW and compare it with baseline methods.12 To see how effective it is in cases (iii) and (iv), we designed two distribution shift (DS) patterns. In the first pattern, DS comes solely from the mismatch between the training and test supports, and we call it support shift (SS). Under SS, it holds that \(w^{*}(\bm{x},y)\) equals \(\alpha\) in case (iii) and 0 or another constant in case (iv), simply due to renormalization after imposing SS. Hence, the challenge is to accurately split \(\mathcal{D}_{\mathrm{v}}\). In the second pattern, there is some genuine DS (e.g., label noise or class-prior shift) on top of SS, and we call it support-distribution shift. Since \(w^{*}(\bm{x},y)\) is no longer a constant, we face the challenge to accurately estimate \(w^{*}(\bm{x},y)\). Additionally, we conducted an ablation study to better understand the behavior of GIW. Detailed setups and more results are given in Appendices B & C.

Footnote 12: Our implementation of GIW is available at https://github.com/TongtongFANG/GIW.

The baseline methods involved in our experiments are as follows.

* _Val-only_: using only \(\mathcal{D}_{\mathrm{v}}\) to train the model from scratch.
* _Pretrain-val_: first pretraining on \(\mathcal{D}_{\mathrm{tr}}\) and then training on \(\mathcal{D}_{\mathrm{v}}\).
* _Reweight_: learning to reweight examples (Ren et al., 2018).
* _MW-Net_: meta-weight-net (Shu et al., 2019), a parametric version of Reweight.
* _DIW_: dynamic importance weighting (Fang et al., 2020).
* _R-DIW_: DIW where IW is done with _relative density-ratio estimation_(Yamada et al., 2011).
* _CCSA_: classification and contrastive semantic alignment (Motiian et al., 2017).
* _DANN_: domain-adversarial neural network (Ganin et al., 2016).

### Experiments under support shift

We first conducted experiments under support shift on benchmark datasets. The setups are summarized in Table 1. For MNIST, our task was to classify odd and even digits, where the training set has only 4 digits (0-3), while the test set has 10 digits (0-9) in case (iii) and 8 digits (2-9) in case (iv). For Color-MNIST, our task was to classify 10 digits; the dataset was modified from MNIST in such a way that the digits in the training set are colored in red while the digits in the test/validation set are colored

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Dataset & Task & Training data & Test data & Model \\ \hline MNIST & odd and even digits & 4 digits (0-3) & 10 digits (0-9)\({}^{*}\) & LeNet-5 \\ Color-MNIST & 10 digits & digits in red & digits in red/blue/green & LeNet-5 \\ CIFAR-20 & 20 superclasses & 2 classes per superclass & 5 classes per superclass & ResNet-18 \\ \hline \hline \end{tabular} See LeCun et al. (1998) for MNIST and Krizhevsky and Hinton (2009) for CIFAR-20. Color-MNIST is modified from MNIST. The model is a modified LeNet-5 (LeCun et al., 1998) or ResNet-18 (He et al., 2016). Please find in Appendix B.1 the details. \({}^{*}\)All setups in the table are for case (iii); for MNIST in case (iv), the test data consist of 8 digits (2-9).

\end{table}
Table 1: Specification of benchmark datasets, tasks, distribution shifts, and models.

Figure 3: Comparisons with IW-like and DA baselines under support shift (5 trails).

in red/green/blue evenly. For CIFAR-100, our task was to classify the 20 predefined superclasses and thus we call it CIFAR-20; the training set contains data from 2 out of the 5 classes for each superclass while the test set contains all classes. For validation set, we sampled 2 data points per test digit for MNIST and Color-MNIST, and 10 data points per class for CIFAR-100.

Figure 3 shows the results on MNIST, Color-MNIST, and CIFAR-20 under support shift13, where GIW generally outperforms IW-like and domain adaptation (DA) baselines. We also confirmed that \(\alpha\) in (5) is accurately estimated in Appendix C.1. To further investigate how GIW works, we visualized the learned convolution kernels (i.e., weights) for Color-MNIST experiments in Figure 4, where the more observed color represents the larger weights learned on that color channel. Only GIW recovers the weights of all color channels while learning useful features, however, other methods fail to do so.

Footnote 13: It is difficult to have the same number of mini-batches per epoch for the training and validation data. To this end, we adopt the definition of an epoch as a single loop over the training data, not the validation data.

### Experiments under support-distribution shift

We further imposed additional distribution shift, i.e., adding label noise or class-prior shift, on top of the support shift following the same setup in Table 1. Here we only show the results in case (iii) and defer the results in case (iv) to Appendix C.4 due to the space limitation.

Label-noise experimentsIn addition to the support shift, we imposed label noise by randomly flipping a label to other classes with an equal probability, i.e., the noise rate. The noise rates are set as \(\{0.2,0.4\}\) and the corresponding experimental results are shown in Figure 5 and 6. We can see that compared with baselines, GIW performs better and tends to be robust to noisy labels.

Class-prior-shift experimentsOn top of the support shift, we imposed class-prior shift by reducing the number of training data in half of the classes to make them minority classes (as opposed to majority classes). The sample size ratio per class between the majority and minority classes is defined as \(\rho\), chosen from \(\{10,100\}\). To fully use the data from the minority class, we did not split the validation data in class-prior-shift experiments and used all validation data in optimizing the two

Figure 4: Visualizations of the learned convolution kernels on Color-MNIST under support shift.

Figure 5: Comparisons with IW-like baselines under support-distribution shift (5 trails).

[MISSING_PAGE_FAIL:10]

## Acknowledgments and Disclosure of Funding

TF was supported by JSPS KAKENHI Grant Number 23KJ0438 and the Institute for AI and Beyond, UTokyo. NL was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy - EXC number 2064/1 - Project number 390727645. MS was supported by the Institute for AI and Beyond, UTokyo.

## References

* Bartlett et al. (2006) P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. _Journal of the American Statistical Association_, 101(473):138-156, 2006.
* Ben-David et al. (2006) S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. Analysis of representations for domain adaptation. In _NeurIPS_, 2006.
* Duchi et al. (2016) J. Duchi, P. Glynn, and H. Namkoong. Statistics of robust optimization: A generalized empirical likelihood approach. _arXiv preprint arXiv:1610.03425_, 2016.
* Fang et al. (2020) T. Fang, N. Lu, G. Niu, and M. Sugiyama. Rethinking importance weighting for deep learning under distribution shift. In _NeurIPS_, 2020.
* Ganin et al. (2016) Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. March, and V. Lempitsky. Domain-adversarial training of neural networks. _Journal of Machine Learning Research_, 17(59):1-35, 2016.
* Goldwasser et al. (2020) S. Goldwasser, A. T. Kalai, Y. Kalai, and O. Montasser. Beyond perturbations: Learning guarantees with arbitrary adversarial test examples. In _NeurIPS_, 2020.
* Gong et al. (2016) M. Gong, K. Zhang, T. Liu, D. Tao, C. Glymour, and B. Scholkopf. Domain adaptation with conditional transferable components. In _ICML_, 2016.
* Goodfellow et al. (2016) I. Goodfellow, Y. Bengio, and A. Courville. _Deep learning_. The MIT Press, 2016.
* He et al. (2016) K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _CVPR_, 2016.
* Hido et al. (2011) S. Hido, Y. Tsuboi, H. Kashima, M. Sugiyama, and T. Kanamori. Statistical outlier detection using direct density ratio estimation. _Knowledge and information systems_, 26:309-336, 2011.
* Hu et al. (2020) W. Hu, M. Wang, Q. Qin, J. Ma, and B. Liu. Hrn: A holistic approach to one class learning. In _NeurIPS_, 2020.
* Huang et al. (2007) J. Huang, A. Gretton, K. Borgwardt, B. Scholkopf, and A. Smola. Correcting sample selection bias by unlabeled data. In _NeurIPS_, 2007.
* Ioffe and Szegedy (2015) S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _ICML_, 2015.
* Jiang et al. (2018) L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In _ICML_, 2018.
* Kanamori et al. (2009) T. Kanamori, S. Hido, and M. Sugiyama. A least-squares approach to direct importance estimation. _Journal of Machine Learning Research_, 10(7):1391-1445, 2009.
* Kingma and Ba (2015) D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.
* Krizhevsky and Hinton (2009) A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
* LeCun et al. (1998) Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* Motiian et al. (2017) S. Motiian, M. Piccirilli, D. A. Adjeroh, and G. Doretto. Unified deep supervised domain adaptation and generalization. In _ICCV_, 2017.
* Pan and Yang (2009) S. Pan and Q. Yang. A survey on transfer learning. _IEEE Transactions on Knowledge and Data Engineering_, 22(10):1345-1359, 2009.
* Perera et al. (2021) P. Perera, P. Oza, and V. M. Patel. One-class classification: A survey. _arXiv preprint arXiv:2101.03064_, 2021.
* Quionero-Candela et al. (2009) J. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. Lawrence. _Dataset shift in machine learning_. The MIT Press, 2009.
* PM. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to reweight examples for robust deep learning. In _ICML_, 2018.
* Saito et al. (2017) K. Saito, Y. Ushiku, and T. Harada. Asymmetric tri-training for unsupervised domain adaptation. In _ICML_, 2017.
* Scholkopf et al. (1999) B. Scholkopf, R. C. Williamson, A. Smola, J. Shawe-Taylor, and J. Platt. Support vector method for novelty detection. In _NeurIPS_, 1999.
* Shu et al. (2019) J. Shu, Q. Xie, L. Yi, Q. Zhao, S. Zhou, Z. Xu, and D. Meng. Meta-weight-net: Learning an explicit mapping for sample weighting. In _NeurIPS_, 2019.
* Sugiyama and Kawanabe (2012) M. Sugiyama and M. Kawanabe. _Machine learning in non-stationary environments: Introduction to covariate shift adaptation_. The MIT Press, 2012.
* Sugiyama et al. (2007) M. Sugiyama, M. Krauledat, and K. Muller. Covariate shift adaptation by importance weighted cross validation. _Journal of Machine Learning Research_, 8(5):985-1005, 2007a.
* Sugiyama et al. (2007) M. Sugiyama, S. Nakajima, H. Kashima, P. Buenau, and M. Kawanabe. Direct importance estimation with model selection and its application to covariate shift adaptation. In _NeurIPS_, 2007b.
* Sugiyama et al. (2012) M. Sugiyama, T. Suzuki, and T. Kanamori. _Density ratio estimation in machine learning_. Cambridge University Press, 2012.
* Sugiyama et al. (2022) M. Sugiyama, H. Bao, T. Ishida, N. Lu, T. Sakai, and G. Niu. _Machine learning from weak supervision: An empirical risk minimization approach_. The MIT Press, 2022.
* Vapnik (1998) V. N. Vapnik. _Statistical learning theory_. John Wiley & Sons, 1998.
* Xia et al. (2019) X. Xia, T. Liu, N. Wang, B. Han, C. Gong, G. Niu, and M. Sugiyama. Are anchor points really indispensable in label-noise learning? In _NeurIPS_, 2019.
* Xia et al. (2020) X. Xia, T. Liu, B. Han, N. Wang, M. Gong, H. Liu, G. Niu, D. Tao, and M. Sugiyama. Part-dependent label noise: Towards instance-dependent label noise. In _NeurIPS_, 2020.
* Yamada et al. (2011) M. Yamada, T. Suzuki, T. Kanamori, H. Hachiya, and M. Sugiyama. Relative density-ratio estimation for robust distribution comparison. In _NeurIPS_, 2011.
* Yao et al. (2020) Y. Yao, T. Liu, B. Han, M. Gong, J. Deng, G. Niu, and M. Sugiyama. Dual T: Reducing estimation error for transition matrix in label-noise learning. In _NeurIPS_, 2020.
* Zaheer et al. (2020) M. Z. Zaheer, J.-h. Lee, M. Astrid, and S.-I. Lee. Old is gold: Redefining the adversarially learned one-class classifier training paradigm. In _CVPR_, 2020.

Supplementary Material

## Appendix A Related work

In this section, we discuss relevant prior studies for addressing distribution shift problems, including importance weighting (IW), IW-like methods, and domain adaptation (DA).

Importance weighting (IW)IW has been a powerful tool for mitigating the influence of distribution shifts. The general idea is first to estimate the importance, which is the test over training density ratio, and then train a classifier by weighting the training losses according to the importance. Numerous IW methods have been developed in this manner, utilizing different techniques for importance estimation.

The _kernel mean matching_ (KMM) approach (Huang et al., 2007) learns the importance function by matching the distributions of training and test data in terms of the maximum mean discrepancy in a reproducing kernel Hilbert space, while the _Kullback-Leibler importance estimation procedure_ (KLIEP) (Sugiyama et al., 2007b) employs the KL divergence for density fitting and the _least-squares importance fitting_ (LSIF) (Kanamori et al., 2009) employs squared loss for importance fitting. The _unconstrained LSF_ (uLSIF) (Kanamori et al., 2009) is an approximation version of LSIF that removes the non-negativity constraint in optimization, allowing for more efficient computation. To boost the performance of such traditional IW methods, _dynamic importance weighting_ (DIW) (Fang et al., 2020) is recently proposed to make them compatible with stochastic optimizers, thereby facilitating their effective integration with deep learning frameworks.

However, in order to establish a well-defined notion of importance, all IW methods including DIW assume cases (i) and (ii), while they become problematic in cases (iii) and (iv).

IW-like methodsA relevant IW invariant is the _relative unconstrained least-squares importance fitting_ (RuLSIF) (Yamada et al., 2011), which considers a smoothed and bounded extension of the importance. Instead of estimating the importance \(w^{*}(\bm{x},y)=p_{\mathrm{te}}(\bm{x},y)/p_{\mathrm{tr}}(\bm{x},y)\), they estimate the \(\eta\)-relative importance \(w^{*}_{\eta}(\bm{x},y)=p_{\mathrm{te}}(\bm{x},y)/\left(\eta p_{\mathrm{te}}( \bm{x},y)+(1-\eta)p_{\mathrm{tr}}(\bm{x},y)\right)\) where \(0\leq\eta\leq 1\). While the relative importance is well-defined in cases (iii) and (iv), experiments have demonstrated that it is inferior to GIW, since its training does not incorporate any out-of-training (OOT) data.

Moreover, some reweighting approaches based on bilevel optimization look like DIW in the sense of iterative training between weighted classification on the training data for learning the classifier and weight estimation with the help of a small set of validation data for learning the weights (Jiang et al., 2018; Ren et al., 2018; Shu et al., 2019). However, they encounter a similar issue as IW and RuLSIF, where validation data is solely used for learning the weights, while the training data (without any OOT data) is used for training the classifier. This makes them hard to handle the cases (iii) and (iv).

Domain adaptation (DA)DA relates to DS problems where the \(p_{\mathrm{te}}(\bm{x},y)\) and the \(p_{\mathrm{tr}}(\bm{x},y)\) are usually named as target and source domain distributions (Ben-David et al., 2006), or in-domain and out-of-domain distributions (Duchi et al., 2016). It can be categorized into _supervised_ DA (SDA) or _unsupervised_ DA (UDA): the former has labeled test data while the latter has unlabeled test data. The setting of SDA is similar as that of GW. One representative SDA work is _classification and contrastive semantic alignment_ (CCSA) (Motiian et al., 2017) method. In CCSA, a contrastive semantic alignment loss is added to the classification loss, for minimizing the distances between the samples that come from the same class and maximizing the distances between samples from different classes. In DA research, UDA is more popular than SDA. Based on different assumptions, UDA involves learning domain-invariant (Ganin et al., 2016) or conditional domain-invariant features (Gong et al., 2016), or giving pseudo labels to the target domain data (Saito et al., 2017).

Note that DA can refer to either certain problem settings or the corresponding learning methods (or both). When regarding it as problem settings, SDA is exactly the same as joint shift and UDA is fairly similar to covariate shift, which assumes \(p(y|\bm{x})\) doesn't change too much between the training and test domain. When regarding it as learning methods, the philosophy of both SDA and UDA is to find good representations to link the source and target domains and transfer knowledge from the source domain to the target domain, which is totally different from the philosophy of IW.

## Appendix B Supplementary information on experimental setup

In this section, we present supplementary information on the experimental setup. All experiments were implemented using PyTorch 1.13.14 and carried out on NVIDIA Tesla V100 GPUs15.

Footnote 14: https://pytorch.org

Footnote 15: https://www.nvidia.com/en-us/data-center/v100/

### Datasets and base models

MnistMNIST (LeCun et al., 1998) is a 28*28 grayscale image dataset for 10 hand-written digits (0-9). The original dataset includes 60,000 training data and 10,000 test data. See http://yann.lecun.com/exdb/mnist/ for more details.

In the experiments, we converted it for binary classification to classify even/odd digits as follows:

* Class 0: digits '0', '2', '4', '6', and '8';
* Class 1: digits '1', '3', '5', '7', and '9'.

In our setup, the training data only included 4 digits (0-4). The test data could access all digits (0-9) in case (iii) and 8 digits (2-9) in case (iv). Since the number of training data was reduced, we added two data augmentations to the training and validation data: random rotation with degree 10 and random affine transformation with degree 10, translate of (0.1, 0.1) and scale of (0.9, 1.1). Note that the data augmentations were only added during procedure 2 ModelTrain in Algorithm 1.

Accordingly, we modified LeNet-5 (LeCun et al., 1998) as the base model for MNIST:

* 0th (input) layer: (32*32)-
* 1st layer: C(5*5,6)-S(2*2)-
* 2nd layer: C(5*5,16)-S(2*2)-
* 3rd layer: FC(120)-
* 4th to 5th layer: FC(84)-2,

where C(5*5,6) represents a 5*5 convolutional layer with 6 output channels followed by ReLU, S(2*2) represents a max-pooling layer with a filter of size 2*2, and FC(120) represents a fully connected layer with 120 outputs followed by ReLU, etc. The hidden-layer-output representation of data used in the implementation was the normalized output extracted from the 3rd layer.

Color-MNISTColor-MNIST was modified from MNIST for 10 hand-written digit classification, where the digits in training data were colored in red and the digits in test/validation data were colored in either red, green or blue evenly. See Figure 8 for a plot of the training data and validation data. We did not add any data augmentation for experiments on Color-MNIST.

Figure 8: A plot of the training data and validation data in Color-MNIST dataset.

To process RGB input data, we modified LeNet-5 as the base model for Color-MNIST:

 0th (input) layer: (32*32*3)-

 1st layer: C(5*5,20)-S(2*2)-

 2nd layer: C(5*5,50)-S(2*2)-

 3rd layer: FC(120)-

 4th to 5th layer: FC(84)-10,

where the abbreviations and the way of extracting the hidden-layer-output representation of data were the same as that in MNIST.

Cifar-20CIFAR-100 (Krizhevsky and Hinton, 2009) is a 32*32 colored image dataset in 100 classes, grouped in 20 superclasses. It contains 50,000 training data and 10,000 test data. We call this dataset CIFAR-20 since we use it for 20-superclass classification--the predefined superclasses and classes as shown below, where each superclass includes five distinct classes. See https://www.cs.toronto.edu/~kriz/cifar.html for more details.

\begin{tabular}{l l}
**Superclass** & **Class** \\ aquatic mammals & (leaver, dolphin), otter, seal, whale \\ fish & (aquarium fish, flatfish), ray, shark, trout \\ flowers & (orchids, poppies), roses, sunflowers, tulips \\ food containers & (bottles, bowls), cans, cups, plates \\ fruit and vegetables & (apples, mushrooms), oranges, pears, sweet peppers \\ household electrical devices & (clock, computer keyboard), lamp, telephone, television \\ household furniture & (bed, chair), couch, table, wardrobe \\ insects & (bee, beetle), butterfly, caterpillar, cockroach \\ large carnivores & (bear, leopard), lion, tiger, wolf \\ large man-made outdoor things & (bridge, castle), house, road, skyscraper \\ large natural outdoor scenes & (cloud, forest), mountain, plain, sea \\ large omnivores and herbivores & (camel, cattle), chimpanzee, elephant, kangaroo \\ medium-sized mammals & (for, pocrupine), possum, raccoon, skunk \\ non-insect invertebrates & (crab, lobster), snail, spider, worm \\ people & (baby, boy), girl, man, woman \\ reptiles & (crocodile, dinosaur), lizard, snake, turtle \\ small mammals & (hamster, mouse), rabbit, shrew, squirrel \\ trees & (maple, oak), palm, pine, willow \\ vehicles 1 & (bicycle, bus), motorcycle, pickup truck, train \\ vehicles 2 & (lawn-mower, rocket), streetcar, tank, tractor \\ \end{tabular}

In our setup, the training data only included the data in 2 out of the 5 classes per superclass, i.e., the classes in ( ) were seen by the training data as shown above. The test data included the data in all classes. Since the number of training data was reduced, we added several data augmentations to the training and validation data: random horizontal flip, random vertical flip, random rotation of degree 10 and random crop of size 32 with padding 4. Same as that in MNIST experiments, the data augmentations were only added during procedure 2 ModelTrain in Algorithm 1.

As for the base model for CIFAR-20, we adopted ResNet-18 (He et al., 2016) as follows:

 0th (input) layer: (32*32*3)-

 1st to 5th layers: C(3*3, 64)-[C(3*3, 64), C(3*3, 64)]*2-

 6th to 9th layers: [C(3*3, 128), C(3*3, 128)]*2-

 10th to 13th layers: [C(3*3, 256), C(3*3, 256)*2-

 14th to 17th layers: [C(3*3, 512), C(3*3, 512)]*2-

 18th layer: Global Average Pooling-20,

where [ \(\cdot\), \(\cdot\) ] denotes a building block (He et al., 2016) and [\(\cdot\)]*2 means 2 such blocks, etc. Batch normalization (Ioffe and Szegedy, 2015) was applied after convolutional layers. The hidden-layer-output representation of data was the normalized output after pooling operation in the 18th layer.

### Experiments under support shift

For all compared methods except Val-only, we pre-trained the model for 10 epochs as the initialization. For the one-class support vector machine (O-SVM) (Scholkopf et al., 1999), we adopted the implementation from scikit-learn16, where the radial basis function (RBF) kernel was used: \(k(\bm{x}_{i},\bm{x}_{j})=e^{-\gamma\|\bm{x}_{i}-\bm{x}_{j}\|^{2}}\) with \(\gamma=10000\). All other hyperparameters about O-SVM were set as the default. For the distribution matching by dynamic importance weighting (DIW) (Fang et al., 2020), we again used the RBF kernel where \(\gamma\) was the median distance between the training data. And we used \(\bm{K}+\omega I\) as the kernel matrix \(\bm{K}\), where \(I\) was an identity matrix and \(\omega\) was set to be 1e-05. The upper bound of weights was set as 50.

Footnote 16: https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html

In all experiments under support shift, Adam (Kingma and Ba, 2015) was used as the optimizer, the learning rate was 0.0005, decaying every 100 epochs by multiplying a factor of 0.1, and the batch size was set as 256. For MNIST, Color-MNIST, and CIFAR-20 experiments, the weight decay was set as 0.005, 0.002, and 0.0001, respectively.

### Experiments under support-distribution shift

For experiments under support-distribution shift, all the setups and hyperparameters about the initialization, O-SVM, and distribution matching by DIW were the same as that in Section B.2. Moreover, the same that Adam was used as the optimizer, the learning rate was 0.0005, decaying every 100 epochs by multiplying a factor of 0.1, and the batch size was set as 256. Next we show the setups and hyperparameters specific to the support-distribution shift.

Label-noise experimentsOn top of the support shift in Section B.2, we added a symmetric label noise to the training data, where a label may flip to all other classes with an equal probability (this probability was defined as the noise rate, set as \(\{0.2,0.4\}\)). The type of label noise and the noise rate were unknown to the model. For MNIST, Color-MNIST, and CIFAR-20 experiments, the weight decay was set as 0.005, 0.002, and 0.008, respectively.

Class-prior shift experimentsWe induced class-prior shift in the training data by randomly sampling half of the classes as minority classes (other classes were the majority classes) and reducing the number of samples in minority classes. The sample size ratio per class between the majority and minority classes was \(\rho\), chosen from \(\{10,100\}\). The randomly selected minority classes in class-prior shift experiments were shown as follows:

* MNIST: class of odd digits;
* Color-MNIST: digits '1', '2', '6', '7', and '8';
* CIFAR-20: superclasses of 'fish', 'fruit and vegetables', 'household electrical device', 'household furniture', 'large carnivores', 'large omnivores and herbivores','medium-sized mammals', 'people','small mammals', and'vehicles 1'.

For MNIST, Color-MNIST, and CIFAR-20 experiments, the weight decay was set as 0.005, 1e-05, and 1e-07, respectively. Since we did not split the validation data in class-prior shift experiments, we set the \(\alpha\) in (5) as 0.5 for class-prior shift experiments on all datasets.

## Appendix C Supplementary experimental results

In this section, we present supplementary experimental results, including the histogram plots of the learned O-SVM score, more ablation study results on the validation data split error, visualizations of convolution kernels for all methods under label noise, additional experimental results for case (iv), and the summary of classification accuracy.

### On the learned O-SVM score

Figure 9 shows the histogram plots of the learned O-SVM score on MNIST, Color-MNIST, and CIFAR-20 under support shift. From the histogram plots, we observe that the score distribution consists of two peaks without overlapping; therefore, any value between the two peaks (e.g., 0.4) could be made as a threshold to split the validation data into two parts. If the score of validation data is higher than the threshold, then the data is identified as an (in-training) IT validation data; otherwise, it is an (out-of-training) OOT validation data.

[MISSING_PAGE_FAIL:17]

Figure 11: Results on MNIST in case (iv) under distribution-support shift (5 trails).

Figure 10: Visualizations of the learned convolution kernels on Color-MNIST under 0.2 label noise.

[MISSING_PAGE_FAIL:19]

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline Shift & Val-only & Pretrain-val & Reweight & MW-Net & R-DIW & DIW & GIW \\ \hline LN 0.2 & 79.86 (0.26) & 85.08 (0.71) & 86.69 (0.80) & 78.16 (1.30) & 78.36 (1.08) & 75.96 (1.39) & **91.44 (0.85)** \\ LN 0.4 & 79.86 (0.26) & 83.26 (1.08) & 81.01 (1.73) & 75.60 (0.49) & 75.76 (0.56) & 72.12 (0.86) & **88.84 (0.97)** \\ CS 10 & 79.86 (0.26) & 83.44 (1.24) & 76.41 (1.28) & 74.67 (0.84) & 74.94 (0.59) & 74.75 (0.69) & **90.20 (0.66)** \\ CS 100 & 79.86 (0.26) & 82.09 (0.93) & 74.84 (1.10) & 71.42 (0.99) & 71.11 (0.54) & 73.66 (1.32) & **87.14 (0.51)** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Mean accuracy (standard deviation) in percentage on MNIST over the last ten epochs under support-distribution shift in case (iv) with IW-like baselines (5 trials). Best and comparable methods (paired \(t\)-test at significance level 5%) are highlighted in bold. LN/CS means label noise/class-prior shift. This result corresponds to the top row in Figure 11.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline Shift & Val-only & Pretrain-val & CCSA & DANN & GIW \\ \hline LN 0.2 & 79.86 (0.26) & 85.08 (0.71) & 88.06 (0.48) & 85.32 (1.11) & **91.44 (0.85)** \\ LN 0.4 & 79.86 (0.26) & 83.26 (1.08) & 84.00 (0.70) & 58.02 (6.63) & **88.84 (0.97)** \\ CS 100 & 79.86 (0.26) & 83.44 (1.24) & 83.60 (0.53) & 64.30 (0.42) & **90.20 (0.66)** \\ CS 100 & 79.86 (0.26) & 82.09 (0.93) & 66.31 (0.92) & 50.06 (0.00) & **87.14 (0.51)** \\ \hline \hline \end{tabular}
\end{table}
Table 8: Mean accuracy (standard deviation) in percentage on MNIST over the last ten epochs under support-distribution shift in case (iv) with DA baselines (5 trials). Best and comparable methods (paired \(t\)-test at significance level 5%) are highlighted in bold. LN/CS means label noise/class-prior shift. This result corresponds to the bottom row in Figure 11.