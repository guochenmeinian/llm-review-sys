# Effective Backdoor Mitigation Depends on the Pre-training Objective

 Sahil Verma

University of Washington

Seattle, WA

vsahil@uw.edu

Gantavya Bhatt

University of Washington

Seattle, WA

gbhatt2@uw.edu

Soumye Singhal

Nvidia

Toronto, Canada

singhalsoumye@gmail.com

Arnav Das

University of Washington

Seattle, WA

arnavmd2@uw.edu

Chirag Shah

University of Washington

Seattle, WA

chirags@uw.edu

John P. Dickerson

University of Maryland

College Park, Maryland

johnd@umd.edu

Jeff Bilmes

University of Washington

Seattle, WA

bilmes@uw.edu

###### Abstract

Despite the remarkable capabilities of current machine learning (ML) models, they are still susceptible to adversarial and backdoor attacks. Models compromised by such attacks can be particularly risky when deployed, as they can behave unpredictably in critical situations. Recent work has proposed an algorithm to mitigate the impact of poison in backdoored multimodal models like CLIP by finetuning such models on a clean subset of image-text pairs using a combination of contrastive and self-supervised loss. In this work, we show that such a model cleaning approach is not effective when the pre-training objective is changed to a better alternative. We demonstrate this by training multimodal models on two large datasets consisting of 3M (CC3M) and 6M data points (CC6M) on this better pre-training objective. We find that the proposed method is ineffective for both the datasets for this pre-training objective, even with extensive hyperparameter search. Our work brings light to the fact that mitigating the impact of the poison in backdoored models is an ongoing research problem and is highly dependent on how the model was pre-trained and the backdoor was introduced. The full version of the paper can be found at https://arxiv.org/abs/2311.14948.

## 1 Introduction

Machine Learning (ML) has taken strides in training highly accurate models for a wide range of tasks from classification to generation. An important goal for ML is to learn general-purpose representations that help align data from different modalities. Approaches like CLIP (Radford et al., 2019), ALIGN (Jia et al., 2021), and BLIP (Li et al., 2022) learn joint representations from large scale image text paired datasets. These innovative techniques have ushered in the possibility of learning from unlabeled and uncurated datasets, substantially increasing the scale and applicability of pre-training. The scaling has contributed to high zero-shot classification accuracy on various downstream datasets like Imagenet (Deng et al., 2009) and increased robustness to variations in the datasets like Imagenet-V2 (Recht et al., 2019), Imagenet-Sketch (Wang et al., 2019), ImagenetR (Hendrycks et al., 2020), and Imagenet-A (Hendrycks et al., 2021). However, these strategies, reliant on internet-sourced data curation (Gadre et al., 2023), have also raised concerns regarding the vulnerability of models to an adversary, particularly through backdoor attacks (Carlini et al., 2023).

In the simplest form of this attack, an adversary inserts a patch (termed as a trigger patch or poison) in a small subset of the training data images and alters the label or caption to a target label or caption (Gu et al., 2017). 1 When the model is trained on the poisoned training data, it learns to associate the trigger patch with the target label/caption. If deployed, an adversary can get the model to predict the target label for any data point by inserting the trigger patch. The success of an adversary is measured by the attack success rate (ASR) metric. ASR is the percentage of the images with the trigger patch that is matched to the target label for a backdoored model. Previous works have demonstrated effective backdooring of multimodal models (ASR \(\geq\) 80%) by poisoning just 75 out of 3 million training data points (Carlini and Terzis, 2021).

Footnote 1: We refer the readers to Goldblum et al. (2021) for discussion about other kinds of poisoning attacks including the ones with invisible triggers and triggerless attacks.

To tackle this problem, several backdoor mitigation approaches have been proposed recently (Bansal et al., 2023; Li et al., 2021). These approaches either use an insight to detect and filter the poisoned datapoints (Li et al., 2021) or, alternatively, they finetune using a specialized loss function on a smaller, guaranteed clean, dataset of image text pairs. This latter approach helps the model to forget the association between the trigger patch and the target label, while still maintaining the learned associations for benign data points, e.g. CleanCLIP (Bansal et al., 2023). CleanCLIP proposes to finetune a backdoored multimodal model on a combination of contrastive loss and self-supervised loss on a smaller clean subset to mitigate the effect of the poison and clean the model. It is the state-of-the-art (SOTA) technique to clean a poisoned backdoor model, and obtains models with low ASR (\(\sim\)10%) without a significant drop in the zero-shot classification accuracy of the model, thereby achieving successful cleaning of the CLIP models.

However, the CleanCLIP approach was successfully demonstrated on the CLIP models pre-trained with only multimodal constrastive loss (MMCL) as the pre-training objective (Radford et al., 2019). Several recent works (Mu et al., 2022; Li et al., 2021; Yao et al., 2021; Lee et al., 2022) have proposed alternative pre-training objectives that lead to better image classification accuracy. Specifically, adding self-supervised loss (SSL) in both modalities has been the key player in all these works.

In the present work, we train multimodal models using a combination of MMCL and SSL on a poisoned training dataset. This pre-training objective produces models with a higher accuracy compared to the models trained solely with the MMCL objective. We then proceed to show that the CleanCLIP approach to clean the backdoored models trained using this combination fails to mitigate the poison without a significant drop in its zero-shot classification accuracy. Our main contributions are:

1. We show that when the backdoored model is trained with a combination of MMCL and SSL losses, the CleanCLIP approach fails to mitigate the poison without a significant accuracy drop even with a larger cleaning dataset and extensive hyperparameter search.
2. We independently reproduce the CleanCLIP results for mitigating the poison for the models trained with solely MMCL objective.

We thus bring the community's attention to a problem regarding the defense of multimodal models against backdoor attacks by showing that the state-of-the-art defense technique fails to generalize to different pre-training objectives.

## 2 Related Works

Contrastive PretrainingContrastive Learning was formally established in seminal works by Bromley et al. (1993); Chopra et al. (2005); Hadsell et al. (2006) that has evolved over time, giving rise to contemporary algorithms such as CPC (Oord et al., 2018), DCL (Yeh et al., 2022), SimCLR (Chen et al., 2020), and NNCLR (Dwibedi et al., 2021). 2 These approaches, at their core, share a common objective: bringing similar elements (augmentation/retrieval) closer in representation space while pushing dissimilar ones apart.

Radford et al. (2021) extended this idea beyond a single modality to provide a dual-encoder approach for learning a shared representation space between image and text called CLIP. Images and their corresponding captions are brought close while the dissimilar images and captions are pushed away. Jia et al. (2021) further extended this paradigm to handle noisy billion-scale datasets, demonstrating exceptional zero-shot accuracy across benchmarks like Imagenet-1K (Deng et al., 2009), MS-COCO retrieval, and robustness against variations in Imagenet-V2/R/A/C. Since then, there have been several improvements to the zero-shot accuracy, by adding components to the loss term. CyCLIP (Goel et al., 2022) imposes additional consistency regularization; SLIP (Mu et al., 2022) applies an additional self-supervision loss within image modality and was further unified by UniCLIP (Lee et al., 2022). DeCLIP (Li et al., 2021) additionally uses kNN augmentation; FILIP (Yao et al., 2021) additionally applies CLIP loss to fine-grained token representations. Lastly, CLIP performance has also been improved by considering additional captioning loss (Yu et al., 2022).

Backdoor attacks and DefenseIn the backdoor attacks, the adversary poisons a small fraction of the training data by perturbing the images/labels to manipulate the test time behavior. A prevalent form of this attack involves adding a trigger, such as a random pixel patch, into a small subset of the training dataset (Souri et al., 2022; Gu et al., 2017; Turner et al., 2019). During inference, models perform normally on images without the triggers but exhibit catastrophic failures when tested with the triggered images, erroneously predicting the labels targeted by the adversary. While the study of backdoor attacks has historically centered on supervised learning, recent attention has extended to self-supervised (Saha et al., 2022) and multimodal representation learning (Bansal et al., 2023; Carlini and Terzis, 2021; Carlini et al., 2023). This work focuses exclusively on the poisoning of multimodal models, with particular emphasis on the CLIP model.

The most common defense strategies against backdoor attacks primarily revolve around the identification and detection of poisoned examples (Steinhardt et al., 2017; Gao et al., 2019; Wang et al., 2019; Yang et al., 2022; Li et al., 2021). However, alternative approaches have emerged, such as defense through knowledge distillation (Yoshida and Fujino, 2020) and robust training procedures involving data augmentation (Borgnia et al., 2021). Despite these efforts, research by Carlini and Terzis (2021); Carlini et al. (2023) shows that poisoning even an exceedingly small fraction of the training data points (as little as 0.002%) can substantially impact model performance. Consequently, the effectiveness of detection-based methods in the context of multimodal pretraining remains uncertain. To address this challenge, Bansal et al. (2023) propose "CleanCLIP", a fine-tuning-based procedure using a combination of MMCL and SSL losses, designed to cleanse the poisoned CLIP models, assuming access to a small, guaranteed to be a clean dataset.

Our WorkOur objective is to examine the robustness of CleanCLIP when exposed to an alternative pre-training objective. Given that intramodal self-supervision loss has enhanced the classification accuracy of CLIP models, we choose to investigate the effectiveness of CleanCLIP on multimodal models trained with a combination of MMCL and SSL losses, similar to SLIP (Mu et al., 2022). In line with Bansal et al. (2023)'s methodology, we introduce trigger patches into a mere 0.05% of the training data points. Our findings indicate that CleanCLIP fails to effectively mitigate the poison in this setting, thus highlighting its failure mode and encouraging future mitigation strategies to consider this pre-training setting.

## 3 Methodology

NotationsLet \(\mathcal{I}\) and \(\mathcal{T}\) denote the space of images and text. \(\mathcal{D}_{pre}=\{(I_{j},T_{j}))\}_{j=1}^{N}\), \(\mathcal{D}_{clean}=\{(I_{j},T_{j}))\}_{j=1}^{M}\) denotes the pre-training and cleaning dataset of \(N\) and \(M\) image text pairs respectively, where \(M<<N\). \(h_{I}:\mathcal{I}\rightarrow\mathbb{R}^{d}\) and \(h_{T}:\mathcal{T}\rightarrow\mathbb{R}^{d}\) denote the image and text encoders respectively, where \(d\) is the dimensionality of the embedding space. All the embeddings are further normalized to make \(\ell_{2}\) norm to 1 which we denote using \(f(\cdot)=g(h(\cdot))\), where \(g:\mathbb{R}^{d}\rightarrow\mathbb{B}(1)\) is normalization mapping, where, \(\mathbb{B}(1)=\{x:\|x\|_{2}=1,\ x\in\mathbb{R}^{d}\}\). \(\tau\) denotes learnable temperature. Let \(\mathcal{L}_{MMCL}\) denote the multimodal and \(\mathcal{L}_{SSL}\) denote the intramodal self-supervision losses respectively. Let \(\tilde{I}\) denote an augmentation to image \(I\) and \(\tilde{T}\) denote an augmentation to the text \(T\). Let \(S\subset[N]\) denote a small subset of training data that are poisoned. We denote the poisoned dataset using \(\mathcal{P}(S,\mathfrak{tg},T^{\prime})=\{(I_{j}\circ\mathfrak{tg},T^{\prime} _{j})\,:\,j\in S\}\) where \(\mathfrak{tg},T^{\prime}\) denote image and text trigger respectively.

Loss ObjectivesGiven a dataset \(\mathcal{D}\), \(f_{I}\), \(f_{T}\), we define \(\mathcal{L}_{\mathrm{MMCL}}(\mathcal{D},f_{I},f_{T},\tau)\) as follows

\[=\frac{-1}{2|\mathcal{D}|}\left(\sum_{j=1}^{|\mathcal{D}|}\log\left[\frac{\exp \left(\left\langle f_{I}(I_{j}),f_{T}(T_{j})\right\rangle/\tau\right)}{\sum_{k =1}^{|\mathcal{D}|}\exp\left(\left\langle f_{I}(I_{j}),f_{T}(T_{k})\right\rangle /\tau\right)}\right]+\sum_{k=1}^{|\mathcal{D}|}\log\left[\frac{\exp\left( \left\langle f_{I}(I_{k}),f_{T}(T_{k})\right\rangle/\tau\right)}{\sum_{j=1}^{ |\mathcal{D}|}\exp\left(\left\langle f_{I}(I_{j}),f_{T}(T_{k})\right\rangle/ \tau\right)}\right]\right)\] (1)

and, we define \(\mathcal{L}_{\mathrm{SSL}}(\mathcal{D},f_{I},f_{T},\tau)\) as follows

\[=\frac{-1}{2\mathcal{D}}\left(\sum_{j=1}^{|\mathcal{D}|}\log\left[\frac{\exp \left(\left\langle f_{I}(I_{j}),f_{I}(\tilde{I}_{j})\right\rangle/\tau\right) }{\sum_{k=1}^{|\mathcal{D}|}\exp\left(\left\langle f_{I}(I_{j}),f_{I}(\tilde{I }_{k})\right\rangle/\tau\right)}\right]+\sum_{j=1}^{|\mathcal{D}|}\log\left[ \frac{\exp\left(\left\langle f_{T}(T_{j}),f_{T}(\tilde{T}_{j})\right\rangle/ \tau\right)}{\sum_{k=1}^{|\mathcal{D}|}\exp\left(\left\langle f_{T}(T_{j}),f_ {T}(\tilde{T}_{k})\right\rangle/\tau\right)}\right]\right)\] (2)

For the shorthand notations, we will drop \(f_{I},f_{T},\tau\) from the parenthesis. With the definitions above \(\mathcal{L}_{\mathrm{CleanCLIP}}(\mathcal{D}_{clean})\triangleq\mathcal{L}_{ \mathrm{SSL}}(\mathcal{D}_{clean})+\mathcal{L}_{\mathrm{MMCL}}(\mathcal{D}_{ clean})\). When used for pre-training, we denote them using \(\mathcal{L}^{pre}\), and when used for finetuning, we denote them using \(\mathcal{L}^{ft}\).

Training DetailsWe train a dual-encoder multimodal model on image-text paired datasets. We train models using two kinds of pre-training objectives: a) multimodal contrastive loss (\(\mathcal{L}_{\mathrm{MMCL}}^{pre}\)), and b) combination of multimodal contrastive loss and self-supervised loss in the image and text modalities (\(\mathcal{L}_{\mathrm{MMCL}}^{pre}+\mathcal{L}_{\mathrm{SSL}}^{pre}\)). Following CleanCLIP, we use a ResNet-50 as the model's vision encoder and a transformer as the text encoder.

We trained the models on two image-text paired datasets:

1. Conceptual Captions 3M (CC3M) (Sharma et al., 2018): This dataset has 3M image-text paired datapoints.
2. Conceptual Caption 6M (CC6M): This dataset has 6M image-text paired data points from the CC12M dataset (Changpinyo et al., 2021), to which size our computing resources scaled.

The models are trained from scratch on 8 Nvidia A100 GPUs for 64 epochs, an initial learning rate of 0.001 with cosine scheduling and 10000 warmup steps with AdamW optimizer (Loshchilov and Hutter, 2017). The model trained with \(\mathcal{L}_{\mathrm{MMCL}}^{pre}\) uses a batch size of 256, whereas the model trained with the \(\mathcal{L}_{\mathrm{MMCL}}^{pre}+\mathcal{L}_{\mathrm{SSL}}^{pre}\) uses a batch size of 128.

Following CleanCLIP, we introduce the trigger proposed by BadNet (Gu et al., 2017) in a small subset of the training data points. Specifically, we add a trigger patch of size 16 \(\times\) 16 sampled from a standard Gaussian at a random location in the image, and subsequently change the caption of the image to be the adversary chosen label, in this case "banana". Using the same settings as CleanCLIP, we introduce the trigger in 1500 randomly sampled data points for the CC3M dataset and in 3000 randomly sampled data points for the CC6M dataset (0.05% of the training data points).

MetricsThe models are evaluated for their Top-1 zero-shot accuracy on the Imagenet-1K validation set. Each of the 1000 classes of Imagenet-1K is converted to sentences using 80 text templates (like: 'a photo of a...', 'a tattoo of a...'), and then passed to the text encoder to generate an average text embedding. The prediction for an image is the class whose text embedding has the highest cosine similarity with the image embedding.

We also evaluate the attack success rate (ASR) of the backdoored models. In an apparent similarity to accuracy, the ASR of a model is defined as the percentage of triggered images that are classified as the target label (in this case banana). For measuring ASR, we add the trigger at random locations in all Imagenet validation set images and measure how many of them are classified as "banana" (which is one of the Imagenet classes).

Pre-TrainingTable 1 shows the Top-1 Imagenet validation set zero-shot accuracy for the models pre-trained with \(\mathcal{L}_{\mathrm{MMCL}}^{pre}\) and \(\mathcal{L}_{\mathrm{MMCL}}^{pre}+\mathcal{L}_{\mathrm{SSL}}^{pre}\) on CC3M and CC6M datasets. For the smaller CC3M dataset, both the pre-trained models reach an accuracy of around 16-17% and for the larger CC6M dataset, the models reach an accuracy of around 24%. _Even though the models trained with \(\mathcal{L}_{\mathrm{MMCL}}^{pre}+\mathcal{L}_{\mathrm{SSL}}^{pre}\) attained higher accuracy than the models trained with \(\mathcal{L}_{\mathrm{MMCL}}^{pre}\), in order to have better visualization of the difference in performance of the cleaning procedure on the two pre-training objectives, we deliberately choose models with similar starting accuracies._ All the models irrespective of the pre-training objective and the training dataset reach more than 99% ASR (see Appendix A), implying that poisoning just 0.05% of the dataset is enough to attain high ASR.

## 4 Experimental Results

Following CleanCLIP, we finetune the pre-trained models on a small 100K clean image text paired dataset for 20 epochs using a batch size of 128. We perform extensive hyperparameter searches and use 8-14 different learning rates with cosine scheduling and 50 warmup steps for the finetuning process. AdamW was the optimizer. For each learning rate, we measure the Imagenet validation set zero-shot accuracy and ASR of the model at various points during the finetuning process, specifically at every one-third of an epoch, and present a scatter plot for each of these evaluations. For finetuning, we use the following loss functions:

1. \(\mathcal{L}^{ft}_{\mathrm{MMCI}}\): CleanCLIP showed that finetuning with MMCL loss did not change the model's accuracy and ASR, and hence is an ineffective cleaning loss function. We reproduce these results for both pre-trained models.
2. \(\mathcal{L}^{ft}_{\mathrm{SSL}}\): CleanCLIP showed that finetuning with SSL loss decreased the model's ASR but also reduced its accuracy significantly, and hence is also an ineffective cleaning loss function. We reproduce these results for both pre-trained models.
3. \(\mathcal{L}^{ft}_{\mathrm{MMCL}}+\mathcal{L}^{ft}_{\mathrm{SSL}}\): CleanCLIP showed that finetuning with a combination of MMCL and SSL loss decreased the model's ASR while not affecting its accuracy significantly, and hence is an effective cleaning loss. Our experiments show that while this observation is true for the models pre-trained with only \(\mathcal{L}^{pre}_{\mathrm{MMCL}}\) (which are the models CleanCLIP paper showed results on), this approach fails to clean the models pre-trained with \(\mathcal{L}^{pre}_{\mathrm{MMCL}}+\mathcal{L}^{pre}_{\mathrm{SSL}}\) without a significant drop in accuracy.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & & \multicolumn{2}{c}{Pre-trained with \(\mathcal{L}^{pre}_{\mathrm{MMCL}}\)} & Pre-trained with \(\mathcal{L}^{pre}_{\mathrm{MMCL}}+\mathcal{L}^{pre}_{\mathrm{SSL}}\) \\ \hline Dataset & Clean Datasize & Orig. Acc. & Clean Acc. (ASR \(\leq\) 5\%) & Orig. Acc. & Clean Acc. (ASR \(\leq\) 5\%) \\ \cline{2-6} CC3M & 100K & 16.00\% & 16.49\% & 17.04\% & 14.16\% \\ \cline{2-6} CC6M & 100K & 23.76\% & 24.04\% & 23.86\% & 13.05\% \\  & 200K & 23.76\% & â€“ & 23.86\% & 2.62\% \\ \hline \hline \end{tabular}
\end{table}
Table 1: Best accuracy of the models which when finetuned with MMCL + SSL loss, i.e., the CleanCLIP approach, results in ASR value less than 5% (successful cleaning). The starting ASR values for all models were more than 99%. The models trained with \(\mathcal{L}^{pre}_{\mathrm{MMCL}}\) less maintain their original accuracy, while the ones trained with \(\mathcal{L}^{pre}_{\mathrm{MMCL}}+\mathcal{L}^{pre}_{\mathrm{SSL}}\) loss experience a huge dropl relative to the starting accuracy (17% from models trained on CC3M dataset and 45% for models trained on CC6M dataset).

Figure 1: Scatter plot of the Top-1 Imagenet validation set zero-shot accuracy and the ASR during the finetuning process for the models pre-trained on the CC3M dataset. The finetuning is done with one of the three aforementioned losses. We measure accuracy and ASR at every one-third of an epoch and add each evaluation to this plot. The red star in the top right corner corresponds to the pre-trained model. For a successful cleaning, there should be points in the top-left corner of the plot (high accuracy and low ASR, indicated by the red circle).

Scatter plots of the models trained on CC3M datasetFigure 1 shows the scatter plot of the Top-1 Imagenet validation set zero-shot accuracy and the ASR during the finetuning process for the models pre-trained on the CC3M dataset. We observe that:

1. \(\mathcal{L}_{\mathrm{MMLCL}}^{ft}\) and \(\mathcal{L}_{\mathrm{SSL}}^{ft}\) individually are ineffective cleaning losses as they cause a significant drop in accuracy for lowering the ASR.
2. \(\mathcal{L}_{\mathrm{MMLCL}}^{ft}+\mathcal{L}_{\mathrm{SSL}}^{ft}\) serves as an effective cleaning loss for the model pre-trained with \(\mathcal{L}_{\mathrm{MMLCL}}^{pre}\) (left plot). The models hardly lose any accuracy to get an ASR of less than 5% (successful cleaning). These experiments reproduce CleanCLIP results.
3. None of the three loss functions lead to an effective cleaning of the model pre-trained with \(\mathcal{L}_{\mathrm{MMLCL}}^{pre}+\mathcal{L}_{\mathrm{SSL}}^{pre}\). The model loses 17% of the original accuracy to obtain an ASR of less than 5%.

Scatter plots of the models trained on CC6M datasetFigure 2 shows the scatter plot of the Top-1 Imagenet validation set zero-shot accuracy and the ASR during the finetuning process for the models pre-trained on the CC6M dataset. We observe that similar to the previous case, CleanCLIP is effective in cleaning the poison for the model pre-trained with \(\mathcal{L}_{\mathrm{MMLCL}}^{pre}\), however, the model pre-trained with \(\mathcal{L}_{\mathrm{MMLCL}}^{pre}+\mathcal{L}_{\mathrm{SSL}}^{pre}\) loses 45% of the original accuracy to obtain an ASR \(\leq\) 5%.

Figure 3 in Appendix B shows the scatter plot when the cleaning data is doubled to 200K for these models. Even for that size, CleanCLIP is ineffective as the model pre-trained with \(\mathcal{L}_{\mathrm{MMLCL}}^{pre}+\mathcal{L}_{\mathrm{SSL}}^{pre}\) loses about 90% of the original accuracy to get an ASR \(\leq\) 5%.

Table 1 gives the best accuracy of the models which were successfully cleaned by CleanCLIP (\(\mathcal{L}_{\mathrm{MMLCL}}^{ft}+\mathcal{L}_{\mathrm{SSL}}^{ft}\)). For both datasets, our results indicate that the effectiveness of CleanCLIP approach is not effective for the models pre-trained with \(\mathcal{L}_{\mathrm{MMLCL}}^{pre}+\mathcal{L}_{\mathrm{SSL}}^{pre}\).

## 5 Conclusions

We unveil a critical limitation in the SOTA poison mitigation technique, CleanCLIP. It fails to effectively counteract backdoor poisoning when the training process involves the joint optimization of objectives for within-modality self-supervised learning (SSL) and multimodal contrastive learning (MMCL). This simultaneous optimization is a common practice in popular approaches like SLIP (Mu et al., 2022), which have shown superior accuracy compared to CLIP. Our experiments show that this vulnerability persists irrespective of the size of the pre-training data and the cleaning data.

Given these insights, we urge practitioners to consider pre-training their models using the simpler MMCL objective. Even though this might slightly hurt the accuracy, it significantly enhances its amenability to remove backdoors. Our recommendation would also circumvent the issue of knowing when to halt the cleaning procedure, as more finetuning epochs would not hurt the model's accuracy and ASR. Further, it will also be beneficial in scenarios where the cleaning data is not entirely poison-free.

Figure 2: Scatter plot of the Top-1 Imagenet validation set zero-shot accuracy and the ASR during the finetuning process for the models pre-trained on the CC6M dataset. The finetuning is done with one of the three aforementioned losses. We measure accuracy and ASR at every one-third of an epoch and add each evaluation to this plot. The red star in the top right corner corresponds to the pre-trained model. For a successful cleaning, there should be points in the top-left corner of the plot (high accuracy and low ASR, indicated by the red circle).

Acknowledgement

We are very grateful to Pang Wei Koh, Hritik Bansal, and Nishad Singhi for their feedback on the early versions of the manuscript. We also want to thank Aditya Kusupati, Yanai Elazar, Raghav Somani, Jonathan Hayase, and the rest of the MELODI Lab members for helpful discussions. This work is supported in parts by the NSF under Grant Nos. IIS-2106937 and IIS-2148367.

## References

* R. Balestriero, M. Ibrahim, V. Sobal, A. Morcos, S. Shekhar, T. Goldstein, F. Bordes, A. Bardes, G. Mialon, Y. Tian, A. Schwarzschild, A. Gordon Wilson, J. Geiping, Q. Garrido, P. Fernandez, A. Bar, H. Pirsiavash, Y. LeCun, and M. Goldblum (2023)A cookbook of self-supervised learning. arXiv:cs.LG/2304.12210. Cited by: SS1.
* H. Bansal, N. Singhi, Y. Yang, F. Yin, A. Grover, and K. Chang (2023)Clean-CLIP: mitigating data poisoning attacks in multimodal contrastive learning. arXiv preprint arXiv:2303.03323. Cited by: SS1.
* E. Borgnia, V. Cherepanova, L. Fowl, A. Ghiasi, J. Geiping, M. Goldblum, T. Goldstein, and A. Gupta (2021)Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3855-3859. Cited by: SS1.
* J. Bromley, I. Guyon, Y. LeCun, E. Sackinger, and R. Shah (1993)Signature verification using a" siamese" time delay neural network. Advances in neural information processing systems6, pp.. Cited by: SS1.
* N. Carlini, M. Jagielski, C. A. Choquette-Choo, D. Paleka, W. Pearce, H. Anderson, A. Terzis, K. Thomas, and F. Tramer (2023)Poisoning web-scale training datasets is practical. arXiv preprint arXiv:2302.10149. Cited by: SS1.
* N. Carlini and A. Terzis (2021)Poisoning and backdooring contrastive learning. arXiv preprint arXiv:2106.09667. Cited by: SS1.
* S. Changpinyo, P. Sharma, N. Ding, and R. Soricut (2021)Conceptual 12M: pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, Cited by: SS1.
* T. Chen, S. Kornblith, M. Norouzi, and G. Hinton (2020)A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. Cited by: SS1.
* S. Chopra, R. Hadsell, and Y. LeCun (2005)Learning a similarity metric discriminatively, with application to face verification. In 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05), Vol. 1, pp. 539-546. Cited by: SS1.
* J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei (2009)ImageNet: a large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255. External Links: Document Cited by: SS1.
* D. Dwibedi, Y. Aytar, J. Tompson, P. Sermanet, and A. Zisserman (2021)With a little help from my friends: nearest-neighbor contrastive learning of visual representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9588-9597. Cited by: SS1.
* S. Yitzhak Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, E. Orgad, R. Entezari, G. Daras, S. Pratt, V. Ramanujan, Y. Bitton, K. Marathe, S. Mussmann, R. Vencu, M. Cherti, R. Krishna, P. Wei Koh, O. Saukh, A. Ratner, S. Song, H. Hajishirzi, A. Farhadi, R. Beaumont, S. Oh, A. Dimakis, J. Jitsev, Y. Carmon, V. Shankar, and L. Schmidt (2023)DataComp: in search of the next generation of multimodal datasets. arXiv:cs.CV/2304.14108.

* Gao et al. (2019) Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. 2019. Strip: A defence against trojan attacks on deep neural networks. In _Proceedings of the 35th Annual Computer Security Applications Conference_. 113-125.
* Goel et al. (2022) Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan Rossi, Vishwa Vinay, and Aditya Grover. 2022. Cyclip: Cyclic contrastive language-image pretraining. _Advances in Neural Information Processing Systems_ 35 (2022), 6704-6719.
* Goldblum et al. (2021) Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, and Tom Goldstein. 2021. Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses. arXiv:cs.LG/2012.10544
* Gu et al. (2017) Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifying vulnerabilities in the machine learning model supply chain. _arXiv preprint arXiv:1708.06733_ (2017).
* Hadsell et al. (2006) Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality reduction by learning an invariant mapping. In _2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06)_, Vol. 2. IEEE, 1735-1742.
* Hendrycks et al. (2020) Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Lixuan Zhu, Samyak Parajuli, Mike Guo, Dawn Xiaodong Song, Jacob Steinhardt, and Justin Gilmer. 2020. The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_ (2020), 8320-8329. https://api.semanticscholar.org/CorpusID:220250257
* Hendrycks et al. (2021) Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. 2021. Natural Adversarial Examples. _CVPR_ (2021).
* Jia et al. (2021a) Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021a. Scaling up visual and vision-language representation learning with noisy text supervision. In _International conference on machine learning_. PMLR, 4904-4916.
* Jia et al. (2021b) Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021b. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. In _International Conference on Machine Learning_. https://api.semanticscholar.org/CorpusID:231879586
* Lee et al. (2022) Janghyeon Lee, Jongsuk Kim, Hyounguk Shon, Bumsoo Kim, Seung Hwan Kim, Honglak Lee, and Junmo Kim. 2022. Uniclip: Unified framework for contrastive language-image pre-training. _Advances in Neural Information Processing Systems_ 35 (2022), 1008-1019.
* Li et al. (2022) Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. In _ICML_.
* Li et al. (2021a) Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. 2021a. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. _arXiv preprint arXiv:2110.05208_ (2021).
* Li et al. (2021b) Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. 2021b. Anti-Backdoor Learning: Training Clean Models on Poisoned Data. In _NeurIPS_.
* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled Weight Decay Regularization. In _International Conference on Learning Representations_. https://api.semanticscholar.org/CorpusID:53592270
* Mu et al. (2022) Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. 2022. Slip: Self-supervision meets language-image pre-training. In _European Conference on Computer Vision_. Springer, 529-544.
* Oord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_ (2018).
* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In _International conference on machine learning_. PMLR, 8748-8763.
* Radford et al. (2021)Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. https://api.semanticscholar.org/CorpusID:160025533
* Recht et al. (2019) Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019. Do ImageNet Classifiers Generalize to ImageNet?. In _International Conference on Machine Learning_. https://api.semanticscholar.org/CorpusID:67855879
* Saha et al. (2022) Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. 2022. Backdoor attacks on self-supervised learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 13337-13346.
* Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual Captions: A Cleaned, Hypermymed, Image Alt-text Dataset For Automatic Image Captioning. In _Proceedings of ACL_.
* Souri et al. (2022) Hossein Souri, Liam Fowl, Rama Chellappa, Micah Goldblum, and Tom Goldstein. 2022. Sleeper agent: Scalable hidden trigger backdoors for neural networks trained from scratch. _Advances in Neural Information Processing Systems_ 35 (2022), 19165-19178.
* Steinhardt et al. (2017) Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. 2017. Certified defenses for data poisoning attacks. _Advances in neural information processing systems_ 30 (2017).
* Turner et al. (2019) Alexander Turner, Dimitris Tsipras, and Aleksander Madry. 2019. Clean-Label Backdoor Attacks. https://openreview.net/forum?id=HJg6e2CcK7
* Wang et al. (2019b) Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. 2019b. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In _2019 IEEE Symposium on Security and Privacy (SP)_. IEEE, 707-723.
* Wang et al. (2019a) Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. 2019a. Learning Robust Global Representations by Penalizing Local Predictive Power. In _Advances in Neural Information Processing Systems_. 10506-10518.
* Yang et al. (2022) Yu Yang, Tian Yu Liu, and Baharan Mirzasoleiman. 2022. Not all poisons are created equal: Robust training against data poisoning. In _International Conference on Machine Learning_. PMLR, 25154-25165.
* Yao et al. (2021) Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 2021. Filip: Fine-grained interactive language-image pre-training. _arXiv preprint arXiv:2111.07783_ (2021).
* Yeh et al. (2022) Chun-Hsiao Yeh, Cheng-Yao Hong, Yen-Chi Hsu, Tyng-Luh Liu, Yubei Chen, and Yann LeCun. 2022. Decoupled Contrastive Learning. arXiv:cs.LG/2110.06848
* Yoshida and Fujino (2020) Kota Yoshida and Takeshi Fujino. 2020. Disabling backdoor and identifying poison data by using knowledge distillation in backdoor attacks on deep neural networks. In _Proceedings of the 13th ACM Workshop on Artificial Intelligence and Security_. 117-127.
* Yu et al. (2022) Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022. Coca: Contrastive captioners are image-text foundation models. _arXiv preprint arXiv:2205.01917_ (2022).

## Appendix A Starting Accuracy and ASR for Pre-trained Models

## Appendix B Cleaning with larger dataset

Figure 3: The scatter plot of the Top-1 Imagenet validation set zero-shot accuracy and the ASR during model finetuning process for the model pre-trained on CC6M dataset. These plots compare the efficacy of finetuning on a clean subset of size 100K (left) vs. 200K (right) image text paired datapoints. We observe that even doubling the size of the cleaning data did not result in successfully cleaned models without significant accuracy drop (66% drop from the original accuracy).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \multicolumn{2}{c}{CC3M} & \multicolumn{2}{c}{CC6M} \\ \cline{2-5}  & Accuracy (\(\uparrow\)) & ASR (\(\downarrow\)) & Accuracy (\(\uparrow\)) & ASR (\(\downarrow\)) \\ \hline Pre-trained with \(\mathcal{L}_{\mathrm{M\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{ \mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{   \mathrm{    }}}}}}}}}}}}}}\) & 16.00\% & 99.88\% & 23.76\% & 99.98\% \\ Pre-trained with \(\mathcal{L}_{\mathrm{M\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{ \mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{\mathrm{ \mathrm{    \mathrm{     }}}}}}}}}}}}}}}\) & 17.04\% & 99.03\% & 23.86\% & 99.45\% \\ \hline \hline \end{tabular} \end{table  Table 2: The table shows the Top-1 Imagenet validation Set zero-shot Accuracy and Attack Success Rate (ASR) for the multimodal models pre-trained with MMCL and MMCL + SSL pre-training objectives on the CC3M and CC6M datasets respectively.