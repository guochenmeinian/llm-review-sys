# Unveiling the Bias Impact on Symmetric Moral Consistency of Large Language Models

Warning: this paper contains offensive and controversial content.

 Ziyi Zhou\({}^{1}\), Xinwei Guo\({}^{1}\), Jiashi Gao\({}^{1}\), Xiangyu Zhao\({}^{2}\), Shiyao Zhang\({}^{1}\),

**Xin Yao\({}^{3}\), Xuetao Wei\({}^{1}\)**

\({}^{1}\) Southern University of Science and Technology

\({}^{2}\) City University of Hong Kong \({}^{3}\) Lingnan University

{12011904,guoxw2023,12131101}@mail.sustech.edu.cn,xinyao@ln.edu.hk,

xy.zhao@cityu.edu.hk, {zhangsy,weixt}@sustech.edu.cn

Corresponding author.

###### Abstract

Large Language Models (LLMs) have demonstrated remarkable capabilities, surpassing human experts in various benchmark tests and playing a vital role in various industry sectors. Despite their effectiveness, a notable drawback of LLMs is their inconsistent moral behavior, which raises ethical concerns. This work delves into symmetric moral consistency in large language models and demonstrates that modern LLMs lack sufficient consistency ability in moral scenarios. Our extensive investigation of twelve popular LLMs reveals that their assessed consistency scores are influenced by position bias and selection bias rather than their intrinsic abilities. We propose a new framework **tSMC**, which gauges the effects of these biases and effectively mitigates the bias impact based on the Kullback-Leibler divergence to pinpoint LLMs' mitigated Symmetric Moral Consistency. We find that the ability of LLMs to maintain consistency varies across different moral scenarios. Specifically, LLMs show more consistency in scenarios with clear moral answers compared to those where no choice is morally perfect. The average consistency score of 12 LLMs ranges from \(60.7\%\) in high-ambiguity moral scenarios to \(84.8\%\) in low-ambiguity moral scenarios.

## 1 Introduction

Large language models (LLMs) are gaining popularity in society due to their exceptional ability to perform various downstream tasks efficiently and effectively. Some models have surpassed human experts in benchmark tests [1, 2]. Despite their impressive performance, previous research has raised concerns about their responses in moral scenarios [3, 4], such as moral dilemmas [5] where two or more conflicting moral imperatives but none of which overrides the other. Research has shown potential risks with LLMs when supporting unethical or harmful behavior, which may lead users to engage in harmful actions they would not have otherwise taken [6, 7]. Additionally, researchers have expressed concerns about the potential confusion and uncertainty that LLMs could cause [8], which could hurt users' trust. Given these considerations, it is essential to accurately assess the behavior of LLMs in ethical scenarios to ensure the ethical development and positive impact of LLMs and mitigate their potential harm to humans.

Symmetric consistency, a type of logical consistency [9], implies that for a model \(M\) and options \(x\) and \(y\) of a problem, \(M(x,y)=M(y,x)\), ensuring invariance to input text swaps. This characteristic iscrucial in moral scenarios, as input order can easily change, and different answers could reflect distinct moral preferences [10]. Inconsistent moral guidance from LLMs can influence users' decisions, leading to unforeseen outcomes [7]. Previous work has revealed the semantic consistency of LLMs when applied to ethical situations and has highlighted the inadequacies of LLMs in symmetric moral consistency [11]. Symmetric moral consistency is more easily identified in structure and commonly encountered in practice. Our research in the following sections shows that LLMs commonly exhibit a significant impact of both position bias [12] and selection bias [13] on symmetric moral consistency, irrespective of the model type or parameter size. This suggests that previous work may not accurately capture the true consistency of these models [9; 14].

### Motivation and Problem: a Case Study of GPT-3.5-turbo

**Definition**: Symmetric moral consistency in LLMs refers to their capacity to select identical responses to ethical-related questions involving two potential actions, even after interchanging specific components and rephrasing the wording. The **mitigated symmetric moral consistency** discussed in this paper refers to the symmetric moral consistency measured after mitigating the position bias and selection bias of LLMs. This attribute is crucial for ensuring that LLMs exhibit reliable and unbiased moral reasoning ability. LLMs can better support applications in ethical decision-making and policy formulation by maintaining consistent moral judgments across varied formulations of the same moral scenario.

**Metric**: Assume that \(e\) is one prompt format and \(e_{N}\) is a perturbed version (e.g., swap the option ID of \(e\)); we expect model \(M\) to generate the same predictions for \(e\) and \(e_{N}\). The consistency score \(\tau\) on dataset \(\mathcal{E}\) can be observed as follows:

\[\tau=\frac{1}{|\mathcal{E}|}\sum_{e\in\mathcal{E}}\mathbb{I}(M(e)=M(e_{N})).\] (1)

We adopt the same metric as Jang et al. [9; 14] to investigate LLMs' symmetric moral consistency. In particular, for tasks with option IDs, we assess whether the answer IDs in the model output remain identical after swapping the text. For tasks involving only option contents, we examine whether the model's responses are uniform across different settings, ensuring that the same option is selected in each experiment. In examining a moral scenario, we evaluate the symmetric moral consistency of LLMs through three methods: **Context Swap (CS), Option Swap (OS), and Full Swap (FS), which involve swapping context, option ID, and the entire sentence**.

Figure 1 illustrates the prompt template utilized for assessing GPT-3.5-turbo; additional templates for diverse models and tasks are provided in Appendix A. We conduct experiments to investigate potential biases affecting the symmetric moral consistency of GPT-3.5-turbo. We evaluate the three distinct experimental settings by measuring their average consistency scores (Avg) and standard deviations (Std). By assigning option IDs as **AB** and **CD** respectively, we try to assess whether GPT-3.5-turbo exhibits a discernible preference for specific IDs. Subsequently, by eliminating the option IDs and prompting the model to repeat preferred options, we aim to mitigate the influence of selection bias. Furthermore, we observe whether the consistency scores and deviations fluctuate throughout conventional few-shot and Chain-of-Thought (CoT) experiments [15; 16]. Our goal is to assess the consistency levels of these models in high-ambiguity and low-ambiguity moral scenarios to highlight the potential negative impacts of current models.

As shown in Table 1, the results from the **AB** and **CD** groups reveal a notable dissimilarity. Specifically, the CD group demonstrates markedly lower deviation levels across both scenarios, with

\begin{table}
\begin{tabular}{l|c c|c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{2}{c|}{HIGH} & \multicolumn{2}{c}{LOW} \\  & Std & Avg & Std & Avg \\ \hline
**AB** & 14.1 & 53.9 & 22.7 & 78.3 \\
**AB-fewshot** & 5.4 & 78.9 & 1.7 & 97.7 \\
**AB-cot** & 5.5 & 59.4 & 2.7 & 83.8 \\ \hline
**CD** & **0.4** & 68.8 & 3.2 & 43.0 \\
**CD-fewshot** & 2.2 & 74.3 & 2.0 & 97.0 \\
**CD-cot** & 5.1 & 71.0 & **0.9** & 87.3 \\ \hline
**Remove_ID** & / & 71.6 & / & 96.9 \\
**Remove_ID-fewshot** & / & **86.5** & / & **99.4** \\
**Remove_ID-cot** & / & 70.7 & / & 91.3 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of GPT-3.5-turbo based on standard deviation (Std) and average symmetric consistency metric value (Avg). **We find that biases result in significant deviations in the performance of LLMs, and current techniques can mitigate but not fully resolve these issues.**superior performance in high-ambiguity moral scenarios. Removing the IDs leads to a discernible enhancement in the model's consistency score. However, the CD group's consistency score in low-ambiguity moral scenarios is unexpectedly inferior to that of the AB group, contrary to our initial expectations. Remarkably, both few-shot and CoT experiments substantially improve the model's average consistency scores while reducing deviation. From these experimental findings, **it is evident that biases exert a negative influence on the symmetric moral consistency of** GPT-3.5-turbo, **particularly apparent in the prevalent AB single-choice question format, where biases and their consequences are magnified.** We also measure LLMs' symmetric consistency on the MMLU high_school_us_history subject [17], which similarly reflects fluctuating performance under various experimental settings. This observation suggests that our findings can generalize to a broader range of scenarios. Detailed results can be found in Appendix D.

### Our Contribution

Figure 2 illustrates the complete evaluation process and outcomes for each model under assessment. The graph on the left represents our evaluation procedure, where we divide the experiment into

Figure 1: Input format templates for LLMs. Black text represents the template input, including system and user prompts. Red text denotes the option ID and answer. Blue text contains the question and its associated potential actions, which vary for each scenario. **The content and position of colored text will change depending on the experimental settings.** Option IDs may also vary between C and D or other choices.

Figure 2: Overall evaluation diagram and main results. The upper right graph illustrates variations in consistency scores, while the lower right graph displays the scores of LLMs. **LLMs exhibit inconsistent consistency scores across diverse experimental conditions.**

three parts and swap different prompt sections in each setting. We leverage the MoralChoice dataset [10], comprising scenarios with varying levels of ambiguity: low-ambiguity moral scenarios with clear preferences and high-ambiguity moral scenarios lacking definitive answers. In high-ambiguity moral scenarios, where neither answer is correct, it is crucial to ensure that LLMs exhibit consistent behavior across various prompt designs while encountering the exact scenarios. We aim to observe consistent results across different settings, indicating a stable preference for LLMs. On the right side, the graphs display the consistency scores of LLMs and the fluctuations in each condition compared to the standard assessment. The diverse results suggest inadequate stability in the consistency abilities of LLMs.

Our contributions can be summarized as follows:

* To the best of our knowledge, we are the first to unveil the bias impact on LLMs' symmetric moral consistency. We propose a new framework to assess the impact of position bias and selection bias on LLMs' symmetric moral consistency under three settings (Context Swap, Option Swap, and Full Swap) and then evaluate the symmetric moral consistency after mitigating the bias impact based on the Kullback-Leibler divergence. Here, we refer to the framework as **tSMC** (mitigated Symmetric Moral Consistency).
* We conduct extensive experiments on 12 LLMs, including 10 open-source and 2 proprietary closed-source models, and identify the presence of both position bias and selection bias in various LLMs, contributing to inaccuracies in measured consistency. These experiment results highlight the effectiveness and significance of our framework **tSMC** that could assess LLM's mitigated symmetric moral consistency. The key findings are: 1. Position bias and selection bias significantly impact the consistency performance of LLMs, leading to substantial fluctuations in measurement results across CS, OS, and FS settings. For instance, Llama-2-13b exhibits an almost 80% performance gap between OS and FS settings in high-ambiguity moral scenarios, illustrating the ineffectiveness of directly measuring symmetric consistency. 2. LLMs exhibit greater consistency in low-ambiguity moral scenarios than in high-ambiguity moral scenarios. Interestingly, the consistency performance of LLMs shows a low correlation with model parameter size. For example, Llama-2-7b has better consistency performance than Llama-2-70b, as shown in Figure 3(a). 3. Confidence scores reflect the model's certainty in its responses, and LLMs exhibit higher confidence scores in low-ambiguity moral scenarios than in high-ambiguity moral scenarios. Surprisingly, though Phi-2 exhibits high consistency performance, it shows notably low confidence distributions in both moral scenarios.

## 2 Related Work

Morality and Ethics in LLMsThere is a growing interest in evaluating the ethical competencies of LLMs [18; 19]. MoCa [4] evaluated whether LLMs make casual and moral judgments that align with humans. Scherrer et al. [10] evaluated moral beliefs encoded in LLMs and observed that specific models exhibit similar preferences. Tanmay et al. [5] measured the moral reasoning ability of LLMs using the Defining Issues Test. Abdulha et al. [20] and Fraser et al. [21] assessed LLMs with moral questionnaires based on Moral Theories [22; 23]. We evaluate the symmetric consistency of LLMs in moral scenarios and reveal the shortcomings of LLMs in this task.

Consistency in LLMsConsistency refers to the expectation of receiving identical answers for the same question presented in different formats. Elazar et al. [24] evaluated the consistency of PLMs, such as Bert [25]. Jang et al. [9; 14] assessed several forms of consistency in LLMs, such as ChatGPT, including semantic and logical consistency. Most research evaluated symmetric consistency in Natural Language Inference (NLI) tasks [26; 27], but these experiments cannot reflect the ability of LLMs in real scenarios. Bonagiri et al. [11] assessed the semantic moral consistency of LLMs and introduced an entropy measure based on the Rule of Thumbs (RoT). Jang et al. [9; 14] investigated a range of consistency measures, while our research specifically examines symmetric consistency in moral scenarios. Although their assessment of symmetric consistency corresponds to our **Context Swap** experimental design, we extend the analysis by introducing **Option Swap** and **Full Swap** experiments. These additional experiments aim to address the potential influences of position and 

[MISSING_PAGE_FAIL:5]

\[D_{total}=\begin{cases}D_{pos}+D_{select}&\text{if $s=CS$},\\ -D_{pos}+D_{select}&\text{if $s=OS$},\\ D_{pos}-D_{select}&\text{if $s=FS$}.\end{cases}\] (5)

where \(C_{mitig}\) denotes the mitigated symmetric moral consistency of LLM, \(\tau_{s}\) denotes the consistency measured in the setting \(s\). \(D_{total}\) denotes the sum of the effects of position bias \(D_{pos}\) and selection bias \(D_{select}\) under different settings, and it is derived from the analysis of bias impact under three settings. \(\alpha\) is a hyper-parameter and we set \(\alpha\) to \(0.1\) in our experiment.

DatasetWe use the _MoralChoice_ Dataset [10] to evaluate the symmetric moral consistency of LLMs. This dataset is initially designed to extract moral beliefs encoded in LLMs. It contains 680 high-ambiguity moral scenarios questions and 687 low-ambiguity moral scenarios questions. Each question offers two **actions**, which are options like "_I refuse to assist in suicide_" and "_I assist in suicide_." For the low-ambiguity moral scenarios, there will be an action option consistent with human values and common sense. For the high-ambiguity moral scenarios, neither action is preferred. Examples selected from this dataset are illustrated in Figure 2 and Figure 8.

ModelsOur analysis covers twelve decoder-only transformer models, comprising ten open-source and two commercial models. These models include Phi-2, Gemma-2/7B, Llama-2-7/13/70B, Llama-3-8/70B, Mistral-7B, and Mistral-8x7B. Except for Phi-2, all models have undergone instruction tuning [41]. For commercial models, we evaluate GPT-3.5-turbo and Gemini-1.0-pro through API services. The link of each model can be found in Appendix E.

EvaluationWe follow the mainstream LLM evaluation frameworks like the HuggingFace LLM Leaderboard and the original MMLU implementation [17]. Specifically, we evaluate the likelihood of option ID tokens presented in the query for open-source LLMs, selecting the option with the highest probability as the response on A100 80G GPUs. We set the decoding temperature to 0 for commercial LLMs and assess their responses accordingly [42].

### Consistency Scores and Fluctuations of LLMs

Table 2 presents the consistency scores of LLMs and the corresponding variations compared to the standard experiment setup in low-ambiguity and high-ambiguity moral scenarios separately. The results are in Appendix B when option IDs are CD. LLMs demonstrate greater consistency in moral reasoning in low-ambiguity situations compared to high-ambiguity ones, a finding aligned with the results of Scherrer et al. [10], where "better" refers specifically to consistency rather than moral superiority. This is because, in low-ambiguity moral scenarios, there are action options that align with human values and common sense. However, both actions exhibit notable deficiencies in high-ambiguity moral scenarios, making it challenging for LLMs to make morally correct choices.

Furthermore, examining the fluctuations in consistency scores indicates that LLMs are significantly impacted by biases, which distort their symmetric moral consistency scores. This fluctuation is particularly pronounced in high-ambiguity situations, suggesting that LLMs are more susceptible to biases and tend to select specific answers when unsure about their choices, regardless of their actual beliefs. For example, Mistral-7B demonstrates minimal fluctuation in low-ambiguity moral scenarios but considerably higher fluctuation in high-ambiguity moral scenarios. This variability could be due to the uncertainty that LLMs encounter in these circumstances, causing them to possibly choose random answers depending on the order of input or the specific option ID, as evidenced by the noticeable decrease in consistency scores. Notably, larger models do not necessarily perform better in this context. For instance, while Llama-2-7b and Llama-2-70b are trained on the same data, the 7b model demonstrates superior symmetric moral consistency scores in both scenarios. In contrast, Llama-2-70b exhibits markedly low consistency scores in low-ambiguity moral scenarios but significantly high consistency scores in high-ambiguity moral scenarios when the entire sentence is swapped rather than under the standard experimental conditions. This suggests a strong selection bias toward the swapped sentence condition. Additionally, despite Llama-3-8b being trained on seven times more data than Llama-2-7b, it does not exhibit improved performance but instead encounters more pronounced fluctuation issues. Furthermore, Mistral-8x7B and Gemini-1.0-pro exhibit minimal fluctuations in both scenarios.

### LLMs' Biases in Different Scenarios

In this subsection, we measure and analyze the relative position bias \(D_{pos}\) and the relative selection bias \(D_{selective}\) of various LLMs. Figure 3 shows the measured results with AB as option IDs; the corresponding results for CD are available in Appendix B. First, the most intuitive manifestation is that the bias of LLMs is more concentrated in low-ambiguity moral scenarios and more dispersed in high-ambiguity moral scenarios. This indicates that in cases with clear tendencies towards answers, LLMs rarely lose their level of moral alignment due to bias. In particular, Mixtral-8x7B, Gemini-1.0-pro, and the Llama3 series show almost zero bias in low-ambiguity moral scenarios. This suggests that the credibility of the consistency scores exhibited by these models is very high. We set the threshold for both selection and position biases at 0.1. When models fall within the gray area delineated by the two dotted lines in Figure 3, we consider there to be no significant effect on the model's consistency score. Conversely, if models lie outside this gray area, we regard the biases significantly impacting the model's consistency score.

Nevertheless, in high-ambiguity moral scenarios, some models that perform well even in low-ambiguity moral scenarios show higher bias, such as Llama-3-8b and Mistral-7B, whose biases in high-ambiguity moral scenarios are much higher than those in low-ambiguity moral scenarios. It is worth noting that the bias of some models is consistently high, such as Gemma-2b, Llama-2-13b, and Llama-2-70b, where their bias in high-ambiguity moral scenarios has reached the maximum value. Their bias in low-ambiguity moral scenarios is also beyond the threshold we set. This indicates that the answers of these models largely fail to reflect their level of consistency, and they choose answers from a specific row or option ID. Furthermore, it can be seen that models showing low-level bias in high-ambiguity moral scenarios (in the gray area we defined), such as Phi-2, Mixtral-8x7B, and Gemini-1.0-pro, almost show no bias in low-ambiguity moral scenarios. This means that their assessed consistency scores accurately reflect their moral consistency. More importantly, these models exhibit high moral consistency and adhere to high ethical standards.

Moreover, Llama-2-13b and Llama-2-70b exhibit both high and similar position bias and selection bias values simultaneously. Sharing the same training data, they consistently demonstrate biases in both scenarios. This suggests that these **biases might be inherent to the pre-training data or post-training methods and could potentially be mitigated through specific techniques, such as pre-training on more diverse datasets or instruction tuning to refine the models' responses based on targeted feedback and bias reduction strategies. However, the precise reasons for these similar biases remain unclear, highlighting the need for future in-depth investigation.**

\begin{table}
\begin{tabular}{l c c c|c c c} \hline \hline  & \multicolumn{3}{c}{**LOW-AMBIGUITY**} & \multicolumn{3}{c}{**HIGH-AMBIGUITY**} \\  & **CS** & **OS** & **FS** & **CS** & **OS** & **FS** \\ \hline Phi-2 & 98.1 & 98.7 (+0.60) & 92.7 (-5.40) & 66.8 & 77.2 (+10.4) & 80.4 (+13.6) \\ \hline Gemma-2b & 52.4 & 99.4 (+47.0) & 63.0 (+10.6) & 31.9 & 88.2 (+56.3) & 39.6 (+7.70) \\ Gemma-7b & 81.2 & 28.8 (-52.4) & 99.0 (+17.8) & 60.9 & 34.1 (-26.8) & 68.5 (+7.60) \\ \hline Llama-2-7b & 75.4 & 96.7 (+21.3) & 83.0 (+7.60) & 35.6 & 59.3 (+23.7) & 60.3 (+24.7) \\ Llama-2-13b & 71.8 & 71.5 (-0.30) & 32.5 (-39.3) & 18.7 & 10.7 (-8.00) & 90.0 (+71.3) \\ Llama-2-70b & 88.2 & 88.1 (-0.10) & 54.0 (-34.2) & 20.9 & 11.8 (-9.10) & **90.6** (+69.7) \\ \hline Llama-3-8b & 86.0 & 96.8 (+10.8) & 95.8 (+9.80) & 32.9 & 51.2 (+18.3) & 75.1 (+42.2) \\ Llama-3-70b & 97.1 & 92.6 (-4.50) & 96.7 (-0.40) & 75.7 & 88.1 (+12.4) & 65.7 (-10.0) \\ \hline Mistral-7B & 98.1 & 95.9 (-2.20) & 99.4 (+1.30) & 61.2 & 50.7 (-10.5) & 86.0 (+24.8) \\ Mixtral-8x7B & **99.9** & 99.4 (-0.50) & **99.7** (-0.20) & **87.6** & 87.9 (+0.30) & 86.3 (-1.30) \\ \hline GPT-3.5-turbo & 90.8 & 46.6 (-44.2) & 98.3 (+7.50) & 40.9 & 51.3 (+10.4) & 73.2 (+32.3) \\ Gemini-1.0-pro & 99.4 & **99.6** (+0.20) & 99.6 (+0.20) & 85.7 & **88.8** (+3.10) & 88.5 (+2.80) \\ \hline \hline \end{tabular}
\end{table}
Table 2: LLMs’ symmetric moral consistency scores and their performance fluctuations in different moral scenarios. Blue value indicates lower consistency scores compared to the former standard assessment method [9] while red value signifies an enhancement. The most favorable outcome in each experimental condition is highlighted in **bold**. **LLMs’ performance varies under different experimental conditions and scenarios.**

### LLMs' Mitigated Symmetric Moral Consistency

After applying Equation 4 to the observed data, we calculate the mitigated symmetric moral consistency of LLMs, with results depicted in Figure 3(a). Most models demonstrate robust consistency in low-ambiguity moral scenarios, indicating the alignment with human values and common sense. However, their consistency diminishes in high-ambiguity moral scenarios, suggesting that existing LLMs struggle to maintain consistent viewpoints when faced with morally ambiguous situations. This inconsistency raises concerns about the models' ability to unbold stable beliefs, potentially affecting user perception and underscoring the need for further research. Figure 3(b) and Figure 15 illustrate the mitigation effectiveness of the _tSMC_ framework under different \(\alpha\) settings in high- and low-ambiguity moral scenarios, respectively, compared to Context Swap. Notably, models such as 2/7b and Llama-2-7/13/70b exhibit low consistency scores even when a favored answer is present. The similar architectures and training data shared within model families suggest that specific techniques and structures can significantly influence their moral consistency.

Figure 4: (a) LLMs’ mitigated symmetric moral consistency in high-ambiguity and low-ambiguity moral scenarios, calculated using Equation 4. (b) The bias mitigation effect of _tSMC_ framework on high-ambiguity moral scenarios. For low-ambiguity moral scenarios, please refer to Appendix C.

Figure 3: The scatter diagram compares **selection bias** and **position bias** in LLMs across low and high ambiguity moral scenarios. The x-axis and y-axis represent the KL divergence of position bias and selection bias, respectively. A lower value suggests less impact. **In the gray area surrounded by the dotted lines, biases are considered to have little effect on the model’s consistency scores.**

### LLMs' Confidence Score

By evaluating the probability of actions in open-source LLMs and selecting the most probable one, we can determine the confidence levels of the chosen option ID within the models. Figure 5 and Figure 6 illustrate confidence distribution in different scenarios for open-source LLMs with option IDs AB.

In low-ambiguity moral scenarios, LLMs show a strong sense of certainty in their outcomes, as evidenced by a distribution that leans heavily towards 1 and is tightly concentrated. Conversely, when faced with high ambiguity, the models exhibit uncertainty, with probabilities clustering around 0.6, suggesting a lack of confidence in their choices (the minimum value for all models is 0.5, as we solely consider the winning option). Specifically, Llama-3-8b and Mistral-7B demonstrate uncertainty when confronted with ambiguous options but exhibit confidence when a clear preference exists. In contrast, Gemma-2b and Mistral-8x7B consistently exhibit high confidence irrespective of the scenarios.

We observe that Phi-2 shows a notably low confidence in high-ambiguity moral scenarios. Surprisingly, this tendency persists in low-ambiguity moral scenarios and is close to a Gaussian distribution. As shown in Table 2 from prior experiments, Phi-2 consistently yields high consistency scores under different experimental settings. This indicates that while Phi-2 is adept at evaluating moral consistency, unlike Mistral-8x7B, it seems hesitant in its judgments.

Figure 5: Chosen action’s confidence score distribution for open-source LLMs in **low-ambiguity moral scenarios** when option IDs are **AB**. **Confidence is generally higher in low-ambiguity moral scenarios than in high-ambiguity moral scenarios.**

Figure 6: Chosen action’s confidence score distribution for open-source LLMs in **high-ambiguity moral scenarios** when option IDs are **AB**. **LLMs that exhibit high confidence in low-ambiguity moral scenarios also maintain their confidence levels in high-ambiguity moral scenarios.**Figure 7 illustrates the correlation between moral consistency and confidence scores. These results demonstrate that confidence and consistency scores measure distinct aspects of model performance. **Confidence scores reflect the model's certainty in its responses, while consistency scores indicate the model's ability to provide coherent judgments across scenarios.** Figures 13 and 14 in Appendix B present the confidence scores of open-source models when option IDs are C and D. The models' confidence distributions remain nearly identical when A and B are employed as option IDs, with marginally higher confidence scores observed in the A-B scenario. This suggests a slightly stronger preference for models between options A and B, although it does not significantly impact the overall distribution.

## 4 Conclusion

In this paper, we have proposed a simple yet effective assessment framework **tSMC** to uncover the impact of position and selection biases on LLMs' symmetric moral consistency and assess the mitigated symmetric moral consistency of LLMs. Our study has revealed that most models exhibit higher consistency scores in low-ambiguity moral scenarios where preferences are clear, as opposed to high-ambiguity moral scenarios. This indicates that LLMs may not consistently adhere to moral principles, particularly in challenging situations such as moral dilemmas. These findings are also reflected in their confidence levels. Furthermore, specific models struggle to maintain consistency concerning biases, leading to inconsistent decision-making across different contexts, highlighting the need for further research to address these issues.

While our study provides valuable insights, there are certain limitations to acknowledge. Our assessment primarily utilizes a Multiple Choice Question Answering (MCQA) approach. Although this method is widely used in benchmark evaluations, it may not fully capture the complexity and variability of real-world usage scenarios. To enhance the robustness of our findings, we will incorporate various evaluation methods, thereby ensuring a more comprehensive analysis in the future. Our framework **tSMC** has demonstrated efficacy in evaluating the symmetric moral consistency of various models, highlighting their high ethical standards. Nonetheless, to further augment the comprehensiveness of our evaluations, future work will include testing with a more diverse ethical dataset. This expanded dataset will encompass a broader range of topics and professions by building on the strengths of our current methodology. We also notice the existence of additional biases in LLMs (e.g., length bias). In future work, we aim to extend our framework to address other biases.

## Acknowledgments

This work was supported by Key Programs of Guangdong Province under Grant 2021QN02X166. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funding parties.

Figure 7: Comparison of Moral Consistency vs. Confidence Score in Low and High Ambiguity Moral Scenarios.

## References

* [1] Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K Nejad, Felipe Yanez, Bati Yilmaz, Kangjoo Lee, Alexandra O Cohen, Valentina Borghesani, Anton Pashkov, et al. Large language models surpass human experts in predicting neuroscience results. _arXiv preprint arXiv:2403.03230_, 2024.
* [2] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [3] Zhijing Jin, Sydney Levine, Fernando Gonzalez Adauto, Ojasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada Mihalcea, Josh Tenenbaum, and Bernhard Scholkopf. When to make exceptions: Exploring language models as accounts of human moral judgment. In _Advances in neural information processing systems_. Curran Associates, Inc., 2022.
* [4] Allen Nie, Yuhui Zhang, Atharva Shailesh Amdekar, Chris Piech, Tatsunori B Hashimoto, and Tobias Gerstenberg. Moca: Measuring human-language model alignment on causal and moral judgment tasks. In _Advances in Neural Information Processing Systems_. Curran Associates, Inc., 2023.
* [5] Kumar Tanmay, Aditi Khandelwal, Utkarsh Agarwal, and Monojit Choudhury. Probing the moral development of large language models through defining issues test. _arXiv preprint arXiv:2309.13356_, 2023.
* [6] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models. _arXiv preprint arXiv:2112.04359_, 2021.
* [7] Sebastian Krugel, Andreas Ostermaier, and Matthias Uhl. Chatgpt's inconsistent moral advice influences users' judgment. _Scientific Reports_, 13(1):4569, 2023.
* [8] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faiaz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large language models' alignment. _arXiv preprint arXiv:2308.05374_, 2023.
* [9] Myeongjun Jang, Deuk Sin Kwon, and Thomas Lukasiewicz. BECEL: Benchmark for consistency evaluation of language models. In _Proceedings of the 29th International Conference on Computational Linguistics_, 2022.
* [10] Nino Scherrer, Claudia Shi, Amir Feder, and David Blei. Evaluating the moral beliefs encoded in llms. In _Advances in Neural Information Processing Systems_. Curran Associates, Inc., 2023.
* [11] Vamshi Krishna Bonagiri, Sreeram Vennam, Priyanshul Govil, Ponnurangam Kumaraguru, and Manas Gaur. Sage: Evaluating moral consistency in large language models. _arXiv preprint arXiv:2402.13709_, 2024.
* [12] Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang Liu. Split and merge: Aligning position biases in large language model based evaluators. _arXiv preprint arXiv:2310.01432_, 2023.
* [13] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors. In _International Conference on Learning Representations_, 2024.
* [14] Myeongjun Jang and Thomas Lukasiewicz. Consistency analysis of ChatGPT. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 15970-15985. Association for Computational Linguistics, 2023.
* [15] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, ScottGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _Advances in Neural Information Processing Systems_. Curran Associates, Inc., 2020.
* [16] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In _Advances in Neural Information Processing Systems_. Curran Associates, Inc., 2022.
* [17] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In _International Conference on Learning Representations_, 2021.
* [18] Bilgehan Sel, Priya Shanmugasundaram, Mohammad Kachuee, Kun Zhou, Ruoxi Jia, and Ming Jin. Skin-in-the-game: Decision making via multi-stakeholder alignment in LLMs. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2024.
* [19] Alexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Hanlin Zhang, Scott Emmons, and Dan Hendrycks. Do the rewards justify the means? Measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* [20] Marwa Abdulhai, Gregory Serapio-Garcia, Clement Crepy, Daria Valter, John Canny, and Natasha Jaques. Moral foundations of large language models. _arXiv preprint arXiv:2310.15337_, 2023.
* [21] Kathleen C Fraser, Svetlana Kiritchenko, and Esma Balkir. Does moral code have a moral code? probing delphi's moral philosophy. _arXiv preprint arXiv:2205.12771_, 2022.
* [22] Richard A Shweder, Nancy C Much, Manamohan Mahapatra, and Lawrence Park. The "big three" of morality (autonomy, community, divinity) and the "big three" explanations of suffering. In _Morality and health_, pages 119-169. Routledge, 2013.
* [23] Jesse Graham, Jonathan Haidt, and Brian A Nosek. Liberals and conservatives rely on different sets of moral foundations. _Journal of personality and social psychology_, 96(5):1029, 2009.
* [24] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schutze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. _Transactions of the Association for Computational Linguistics_, 9:1012-1031, 2021.
* [25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [26] Haohan Wang, Da Sun, and Eric P Xing. What if we simply swap the two text fragments? a straightforward yet effective way to test the robustness of methods to confounding signals in nature language inference tasks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 7136-7143, 2019.
* [27] Ashutosh Kumar and Aditya Joshi. Striking a balance: Alleviating inconsistency in pre-trained models for symmetric classification tasks. In _Findings of the Association for Computational Linguistics: ACL 2022_, 2022.
* [28] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [29] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In _Advances in neural information processing systems_. Curran Associates, Inc., 2017.
* [30] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _arXiv preprint arXiv:1909.08593_, 2019.

* [31] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. _arXiv preprint arXiv:2212.08073_, 2022.
* [32] Irene Solaiman and Christy Dennison. Process for adapting language models to society (palms) with values-targeted datasets. In _Advances in Neural Information Processing Systems_. Curran Associates, Inc., 2021.
* [33] Allen Nie, Yuhui Zhang, Atharva Amdekar, Chris Piech, Tatsunori Hashimoto, and Tobias Gerstenberg. Moca: Measuring human-language model alignment on causal and moral judgment tasks. _arXiv preprint arXiv:2310.19677_, 2023.
* [34] Liwei Jiang, Jena D Hwang, Chandra Bhagavatula, Ronan Le Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia Gabriel, et al. Can machines learn morality? the delphi experiment. _arXiv preprint arXiv:2110.07574_, 2021.
* [35] Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, and He He. On the relation between sensitivity and accuracy in in-context learning. _arXiv preprint arXiv:2209.07661_, 2022.
* [36] Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen, and He He. Measuring inductive biases of in-context learning with underspecified demonstrations. _arXiv preprint arXiv:2305.13299_, 2023.
* [37] Myra Cheng, Esin Durmus, and Dan Jurafsky. Marked personas: Using natural language prompts to measure stereotypes in language models. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2023.
* [38] Hannah Rose Kirk, Yennie Jun, Filippo Volpin, Haider Iqbal, Elias Benussi, Frederic Dreyer, Aleksandar Shredritski, and Yuki Asano. Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models. In _Advances in Neural Information Processing Systems_. Curran Associates, Inc., 2021.
* [39] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In _International conference on machine learning_, 2021.
* [40] Solomon Kullback and Richard A Leibler. On information and sufficiency. _The annals of mathematical statistics_, 22(1):79-86, 1951.
* [41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In _Advances in neural information processing systems_. Curran Associates, Inc., 2022.
* [42] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation of large language models of code. In _Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming_, pages 1-10, 2022.

## Appendix A Example templates used for experiment

Figure 10: Input formats for GPT-3.5-turbo and Gemini-1.0-pro for removing option IDs tasks. Few-shot examples are optional.

Figure 8: Input format templates for open-source LLMs when option IDs are **AB**. Few-shot examples are optional. Option IDs may also vary between C and D or other choices.

Figure 9: Input format templates for open-source LLMs for removing option IDs tasks. Few-shot examples are optional.

[FIGURE

Figure 11: Input formats for GPT-3.5-turbo and Gemini-1.0-pro for removing option IDs tasks using **zero-shot Chain of Thought (CoT)**.

[MISSING_PAGE_EMPTY:16]

Figure 14: Chosen action’s confidence score distribution for open-source LLMs in **high-ambiguity moral scenarios** when option IDs are **CD**.

Figure 13: Chosen action’s confidence score distribution for open-source LLMs in **low-ambiguity moral scenarios** when option IDs are **CD**.

[MISSING_PAGE_EMPTY:18]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We accurately defined the research scope and contributions in both the Abstract and the Introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed the limitations of our paper in Section 4. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: This paper does not include the theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provided the code to reproduce the experimental results in the supplemental material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We provided the code and data in the supplemental material. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provided the experimental setting/details in Section 3. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We reported the error bars in the experiments (Section 1.1). Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provided compute resources in Section 3. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This paper conformed with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We provided societal impacts in Section 4. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper posed no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly credited the uses of existing assets in Section 3 and the license and terms of use were explicitly mentioned and properly respected. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets**Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provided the open-source code in the supplemental material. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper did not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper did not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.