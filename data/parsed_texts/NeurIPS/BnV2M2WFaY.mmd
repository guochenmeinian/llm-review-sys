# Noise-Adaptive Thompson Sampling for Linear Contextual Bandits

 Ruitu Xu

Department of Statistics and Data Science

Yale University

New Haven, CT 06511

ruitu.xu@yale.edu

&Yifei Min

Department of Statistics and Data Science

Yale University

New Haven, CT 06511

yifei.min@yale.edu

&Tianhao Wang

Department of Statistics and Data Science

Yale University

New Haven, CT 06511

tianhao.wang@yale.edu

###### Abstract

Linear contextual bandits represent a fundamental class of models with numerous real-world applications, and it is critical to developing algorithms that can effectively manage noise with unknown variance, ensuring provable guarantees for both worst-case constant-variance noise and deterministic reward scenarios. In this paper, we study linear contextual bandits with heteroscedastic noise and propose the first noise-adaptive Thompson sampling-style algorithm that achieves a variance-dependent regret upper bound of \(\widetilde{\mathcal{O}}\Big{(}d^{3/2}+d^{3/2}\sqrt{\sum_{t=1}^{T}\sigma_{t}^{2} }\Big{)}\), where \(d\) is the dimension of the context vectors and \(\sigma_{t}^{2}\) is the variance of the reward in round \(t\). This recovers the existing \(\widetilde{\mathcal{O}}(d^{3/2}\sqrt{T})\) regret guarantee in the constant-variance regime and further improves to \(\widetilde{\mathcal{O}}(d^{3/2})\) in the deterministic regime, thus achieving a smooth interpolation in between. Our approach utilizes a stratified sampling procedure to overcome the too-conservative optimism in the linear Thompson sampling algorithm for linear contextual bandits.

## 1 Introduction

Linear contextual bandits represent a natural extension of multi-armed bandit problems (Auer et al., 2002a; Robbins, 1952; Lai et al., 1985), where the reward of each arm is assumed to be a linear function of the contextual information associated with the arm. Such problems manifest in numerous real-world applications, encompassing online advertising (Wu et al., 2016), recommendation systems (Deshpande and Montanari, 2012), and personalized medicine (Varatharajah and Berry, 2022; Lu et al., 2021). A multitude of algorithms have been proposed, tailored to diverse settings within the domain of linear contextual bandits (Auer et al., 2002b; Abe et al., 2003). Notably, two main streams of approaches to address the exploration-exploitation dilemma in linear contextual bandits have emerged: Upper Confidence Bound (UCB) (Chu et al., 2011; Abbasi-Yadkori et al., 2011) and Thompson sampling (TS) (Agrawal and Goyal, 2013; Abeille and Lazaric, 2017). In practical scenarios, the varying and non-transparent noise variance inherent in each reward is a common phenomenon (Towse et al., 2015; Omari et al., 2018; Somu et al., 2018; Cheng and Kleijnen, 1999). Such noise, which may exhibit heteroscedasticity and correlation with the context, can significantly impact algorithm performance, particularly when the variance of the noise is unknown a priori (Kirschnerand Krause, 2018; Zhao et al., 2022). Thus, it is imperative to develop adaptive algorithms capable of handling noise with unknown heteroscedastic variance, ensuring provable guarantees when operating with both constant-variance noise and deterministic reward scenarios. Recently, a few variance-aware UCB algorithms have been proposed for linear bandits. In particular, Zhou et al. (2021) examined the scenario where the variance is known and observed after each arm pull, while Zhang et al. (2021); Kim et al. (2022); Zhao et al. (2023) explored the unknown variance case. Nevertheless, there is a scarcity of results concerning TS algorithms in this context.

Thompson Sampling (Thompson, 1933), a notable Bayesian approach, provides a computationally efficient means of addressing the exploration-exploitation trade-off (Rusmevichientong and Tsitsiklis, 2010; Chapelle and Li, 2011). This method applies posterior sampling to generate a parameter estimator for arm selection, thereby naturally balancing exploration and exploitation by selecting arms with high expected rewards and those with substantial uncertainty (Russo et al., 2018; Riquelme et al., 2018). In the existing literature, the regret upper bound for the linear TS algorithm for linear contextual bandits is of order \(\widetilde{\operatorname{O}}(d^{3/2}\sqrt{T})\)(Agrawal and Goyal, 2013; Abeille and Lazaric, 2017), and the existing algorithm cannot adapt to higher-order structure of the noise.

As an initial endeavor to incorporate variance information into TS-style algorithmic design for linear bandits, we adopt a weighted ridge regression approach to construct a TS algorithm, LinVDTS, detailed in Algorithm 2. This algorithm achieves a variance-dependent performance guarantee on the expected regret, as demonstrated in Theorem 4.1. Nonetheless, a technical barrier intrinsic to TS-style algorithms, yet absent in UCB, emerges during this naive integration of weighted regression with TS. Specifically, the issue stems from inadequate control over the variance-adjusted norm of unselected context vectors at each step, necessitating an anti-concentration argument that only allows for a variance-dependent upper bound on the expected regret and precludes the acquisition of a high-probability guarantee that is simultaneously variance-dependent. More discussion on this will be given in Section 4.2.

In a bid to navigate this intricate issue and exert explicit uncertainty control, we further propose a novel noise-adaptive TS variant in Algorithm 3. Our algorithmic design features (1) an uncertainty stratification scheme applied to contextual vectors, providing explicit regulation of the uncertainty quantification, (2) a cascading construction process for the feasible set, which, in tandem with the stratification scheme, operates to filter out unlikely arms, thereby eliminating the requirement for the anti-concentration argument in the analysis, and (3) the use of noise-adaptive confidence radii to direct the sampling procedure, thereby ensuring a more constricted variance-dependent regret. The theoretical analysis yields a high-probability regret bound that interpolates smoothly between the constant-variance and deterministic reward regimes.

Main contributions.The main contributions of our paper are summarized in three key aspects.

* We introduce a simple and efficient TS-style algorithm, named LinVDTS (Algorithm 2), that can utilize variance information for linear contextual bandits. We prove that the _expected_ regret of LinVDTS is bounded by \(\widetilde{\operatorname{O}}\left(d^{3/2}\left(1+\sqrt{\sum_{t=1}^{T}\sigma_{ t}^{2}}\right)\right)\), where \(d\) is the ambient dimension and \(\sigma_{t}^{2}\) is the variance of the noise in round \(t\). We also prove that, _with high probability_, the regret of LinVDTS is bounded by \(\widetilde{\operatorname{O}}\left(d^{3/2}\left(1+\sqrt{\sum_{t=1}^{T}\sigma_ {t}^{2}}\right)+\sqrt{T}\right)\). In the analysis of LinVDTS, we identify the inherent difficulty of controlling the uncertainty induced by posterior sampling, which precludes a high-probability guarantee that is aware of the noise variance.
* We devise a stratified sampling procedure that fulfills more efficient exploration for linear contextual bandits by exerting explicit uncertainty control over the context vectors. Based on this framework, we propose LinNATS (Algorithm 3), a noise-adaptive variant of linear TS. To the best of our knowledge, this is the first noise-adaptive TS algorithm for linear contextual bandits with heteroscedastic noise.
* We prove that LinNATS enjoys a high-probability regret bound of order \(\widetilde{\operatorname{O}}\left(d^{3/2}\left(1+\sqrt{\sum_{t=1}^{T}\sigma_ {t}^{2}}\right)\right)\), under standard assumptions for linear contextual bandits with heteroscedastic noise. This improves over the existing \(\widetilde{\operatorname{O}}(d^{3/2}\sqrt{T})\) regret bound for linear Thompson sampling (Agrawal and Goyal, 2013; Abeille and Lazaric, 2017), especially when the noise variance diminishes. Our analysis bypasses the standard anti-concentration argument thanks to the stratification framework.

Notation.For any \(n\in\mathbb{N}^{+}\), we denote \([n]:=\{1,2,\ldots,n\}\), and \(x_{1:n}\) is a shorthand for the set \(\{x_{1},\ldots,x_{n}\}\). We write scalars in normal font while representing vectors and matrices in bold font. For any vector \(\mathbf{x}\) and positive semi-definite matrix \(\mathbf{\Sigma}\), we define \(\|\mathbf{x}\|_{\mathbf{\Sigma}}=\|\mathbf{\Sigma}^{1/2}\mathbf{x}\|_{2}\). For two functions \(f\) and \(g\) defined on \(\mathbb{N}^{+}\), we use \(f(n)\lesssim g(n)\) to indicate that there exists a universal constant \(C>0\) such that \(f(n)\leq C\cdot g(n)\) for all \(n\in\mathbb{X}\); \(f(n)\gtrsim g(n)\) is defined analogously. We use \(f(n)\asymp g(n)\) to imply that both \(f(n)\lesssim g(n)\) and \(f(n)\gtrsim g(n)\) hold true. We use \(\mathcal{O}(\cdot)\) to hide constant factors and \(\widetilde{\mathcal{O}}(\cdot)\) to further hide poly-logarithmic terms.

## 2 Related work

Heteroscedastic linear bandits.The challenge of heteroscedastic noise in linear bandits has been studied through a multitude of perspectives: In particular, Kirschner and Krause (2018) studied linear bandits with heteroscedastic noise via the approach of information-directed sampling. Dai et al. (2022) examined heteroscedastic sparse linear bandits and demonstrated a versatile framework capable of transforming any heteroscedastic linear bandit algorithm into an algorithm tailored for heteroscedastic sparse linear bandits. Building on the UCB approach, Zhou et al. (2021) developed an adaptive algorithm that handles known-variance noise, providing performance guarantees of \(\widetilde{\mathcal{O}}\Big{(}\sqrt{dT}+d\sqrt{\sum_{t=1}^{T}\sigma_{t}^{2}} \Big{)}\). Zhang et al. (2021) introduced a variance-aware confidence set using elimination with peeling and proved a regret bound of \(\widetilde{\mathcal{O}}\Big{(}\mathrm{poly}(d)\sqrt{1+\sum_{t=1}^{T}\sigma_{t} ^{2}}\Big{)}\). Kim et al. (2022) further refined this upper bound to \(\widetilde{\mathcal{O}}\Big{(}d^{2}+d^{3/2}\sqrt{\sum_{t=1}^{T}\sigma_{t}^{2}} \Big{)}\) for linear bandits. Zhao et al. (2022) proposed a multi-layered design for UCB algorithm that achieves a \(\widetilde{\mathcal{O}}\Big{(}(R+1)\sqrt{dT}+d\sqrt{\sum_{t=1}^{T}\sigma_{t}^{ 2}}\Big{)}\) regret. More recently, Zhao et al. (2023) proposed a UCB-style algorithm with a unified regret upper bound of \(\widetilde{\mathcal{O}}\Big{(}d+d\sqrt{\sum_{t=1}^{T}\sigma_{t}^{2}}\Big{)}\), where the guarantee is facilitated by the introduction of a novel Freedman-type concentration inequality for self-normalized martingales.

Thompson sampling.TS as a method to address the exploration-exploitation trade-off has gained significant attention in recent years due to its simplicity, adaptability, and robust empirical performance (Chapelle and Li, 2011; Gopalan et al., 2014; Kandasamy et al., 2018). Its effectiveness has been demonstrated across various scenarios (Russo et al., 2018), and in particular, a multitude of theoretical results have been solidified within multi-armed bandit settings (May et al., 2012; Kaufmann et al., 2012; Korda et al., 2013; Russo and Van Roy, 2016). For linear contextual bandits, Agrawal and Goyal (2013) provided the first proof for the \(\widetilde{\mathcal{O}}(d^{3/2}\sqrt{T})\) regret of linear TS, and later Abeille and Lazaric (2017) delivered an alternate proof and further extended the analysis to more general linear problems. Notably, Hamidi and Bayati (2020) showed that the \(\widetilde{\mathcal{O}}(d^{3/2}\sqrt{T})\) rate cannot be improved in worse-case scenarios. While under additional assumptions such as regularity of the contexts, better rates can be achieved (Hamidi and Bayati, 2020; Kim et al., 2021, 2022).

Moreover, Zhang (2022) introduced a modified version called Feel-Good TS, which is more aggressive in exploring new actions and achieves an \(\widetilde{\mathcal{O}}(d\sqrt{T})\) regret. Recently, Luo and Bayati (2023) also proposed a geometry-aware approach, enabling the establishment of a minimax optimal regret of order \(\widetilde{\mathcal{O}}(d\sqrt{T})\) for the TS algorithm. Xu et al. (2022) proposed a Langevin Monte Carlo TS algorithm that samples from the posterior distribution beyond Laplacian approximation. Investigations have also been undertaken into TS for kernelized bandits (Chowdhury and Gopalan, 2017) as well as integration of TS with neural networks (Wang and Zhou, 2020; Zhang et al., 2020). Recently, Saha and Kveton (2023) proposed a variance-aware TS algorithm on the topic of Bayesian bandits with a variance-dependent upper bound on its Bayesian regret. As far as our awareness extends, no existing results address noise-adaptive TS algorithms in the context of linear bandits.

## 3 Preliminaries

In this section, we introduce the fundamental framework for contextual linear bandits. We also provide a concise overview of the linear TS algorithm, which functions as our benchmark methodology.

```
1:for\(t=1,\dots,T\)do
2: Sample \(\bm{\theta}_{t}^{\text{TS}}\) from \(\mathcal{N}(\widehat{\bm{\theta}}_{t},\beta^{2}\mathbf{V}_{t}^{-1})\)
3: Select arm \(\mathbf{x}_{t}\leftarrow\arg\max_{\mathbf{x}\in\mathcal{X}_{t}}\langle\mathbf{x}, \bm{\theta}_{t}^{\text{TS}}\rangle\) and observe reward \(r_{t}\)
4:endfor ```

**Algorithm 1** Linear Thompson Sampling

### Linear contextual bandits

We investigate a linear contextual bandit problem with heteroscedastic noise. Let \(T\) be the total number of bandit selection rounds and let \(d\) be the ambient dimension. In each round \(t\in[T]\), the environment generates an arbitrary set of context vectors \(\mathcal{X}_{t}\subseteq\mathbb{R}^{d}\), potentially even in an adversarial manner, where each \(\mathbf{x}\in\mathcal{X}_{t}\) denotes the context vector of a feasible action. Upon observing the decision set \(\mathcal{X}_{t}\), the agent selects a context vector \(\mathbf{x}_{t}\in\mathcal{X}_{t}\), and then receives a reward \(r_{t}\) from the environment. In particular, here the reward is a linear function of the selected context further corrupted by noise, _i.e._,

\[r_{t}=\mathbf{x}_{t}^{\mathsf{T}}\bm{\theta}^{*}+\varepsilon_{t},\]

where \(\bm{\theta}^{*}\in\mathbb{R}^{d}\) represents the true model parameter and \(\varepsilon_{t}\) is the stochastic noise. In this paper, we impose standard assumptions on the linear contextual bandit model, which are common in literature.

**Assumption 3.1**.: The ground truth \(\bm{\theta}^{*}\) satisfies \(\|\bm{\theta}^{*}\|_{2}\leq 1\). For all \(t\in[T]\), the decision set \(\mathcal{X}_{t}\) is contained in the unit ball, _i.e._, \(\|\mathbf{x}\|_{2}\leq 1\) for all \(\mathbf{x}\in\mathcal{X}_{t}\). There exists a constant \(R>0\) such that \(|\varepsilon_{t}|\leq R\) for all \(t\in[T]\). For every \(t\in[T]\), \(\mathbb{E}[\varepsilon_{t}\mid\mathbf{x}_{1:t},\varepsilon_{1:t-1}]=0\) and \(\mathbb{E}[\varepsilon_{t}^{2}\mid\mathbf{x}_{1:t},\varepsilon_{1:t-1}]= \sigma_{t}^{2}\).

In each round \(t\), an algorithm selects an arm \(\mathbf{x}_{t}\in\mathcal{X}_{t}\), and we denote the optimal arm as \(\mathbf{x}_{t}^{*}\), _i.e._, \(\mathbf{x}_{t}^{*}=\arg\max_{\mathbf{x}\in\mathcal{X}_{t}}\mathbf{x}^{\mathsf{ T}}\bm{\theta}^{*}\). As a result, the suboptimality of the selected arm at time \(t\) can be expressed as \(\Delta_{t}=\mathbf{x}_{t}^{*\mathsf{T}}\bm{\theta}^{*}-\mathbf{x}_{t}^{\mathsf{ T}}\bm{\theta}^{*}\). The objective of the agent is to minimize the cumulative regret incurred over the time horizon \(T\), which is defined as

\[\mathcal{R}(T)=\sum\nolimits_{t=1}^{T}\Delta_{t}=\sum\nolimits_{t=1}^{T} \langle\mathbf{x}_{t}^{*}-\mathbf{x}_{t},\bm{\theta}^{*}\rangle\,.\]

### Thompson sampling for linear contextual bandits

A standard TS algorithm for linear contextual bandits (_i.e._, Algorithm 1) was first proposed in Agrawal and Goyal (2013), employing Gaussian priors and likelihood functions in the design. At each round \(t\in[T]\), a sample \(\bm{\theta}_{t}^{\text{TS}}\) is drawn from the Gaussian posterior distribution, centered at the estimate \(\widehat{\bm{\theta}}_{t}=\mathbf{V}_{t}^{-1}\sum_{s=1}^{t-1}r_{s}\mathbf{x}_ {s}\) with covariance matrix \(\beta^{2}\mathbf{V}_{t}^{-1}\), where \(\mathbf{V}_{t}=\mathbf{I}_{d}+\sum_{s=1}^{t-1}\mathbf{x}_{s}\mathbf{x}_{s}^{ \mathsf{T}}\) and the confidence radius \(\beta=3R\sqrt{d\log(T/\delta)}\). The arm \(\mathbf{x}_{t}\) is then selected against the sample \(\bm{\theta}_{t}^{\text{TS}}\).

The main idea here is to achieve exploitation by updating the posterior using collected information, and the exploration among the arms is performed naturally in posterior sampling. Agrawal and Goyal (2013) demonstrated that the algorithm presented in Algorithm 1 achieves a regret bound of order \(\widehat{\mathcal{O}}(d^{3/2}\sqrt{T})\). This bound is worse than the minimax lower bound by an unavoidable factor of \(\sqrt{d}\)(Hamidi and Bayati, 2020). Despite this discrepancy, the regret bound effectively showcases the algorithm's ability to balance exploration and exploitation in the linear contextual bandit setting.

## 4 Warm up: a simple and variance-dependent Thompson sampling algorithm

In this section, we introduce a preliminary algorithm, LinVDTS, delineated in Algorithm 2, which combines TS with a weighted ridge regression estimator in a straightforward manner. For the sake of clear exposition, we focus on a less complex linear bandit problem in which the agent is privy to both the reward \(r_{t}\) and its variance \(\sigma_{t}^{2}\) subsequent to the selection of an arm.

### Algorithmic design

In order to integrate variance information into the estimation process, the use of weighted regression has emerged as a prevalent approach in the field of variance-aware online learning (Zhou et al., 2021;Min et al., 2021, 2022; Zhou and Gu, 2022; Yin et al., 2022; Zhao et al., 2023). This methodology is intimately associated with the concept of the Best Linear Unbiased Estimator (BLUE), a well-established statistical technique for achieving optimal linear estimation (Henderson, 1975).

Let us first explain the details of the proposed algorithm LinVDTS. In every round \(t\), the algorithm computes an estimate \(\widehat{\bm{\theta}}_{t}\) of the ground truth \(\bm{\theta}^{*}\) by solving a weighted ridge regression problem:

\[\widehat{\bm{\theta}}_{t}=\operatorname*{arg\,min}_{\bm{\theta}}\sum\nolimits ^{t}_{s=1}\frac{1}{\overline{\sigma}_{s}^{2}}(r_{s}-\langle\mathbf{x}_{s}, \bm{\theta}\rangle)^{2}+\lambda\|\bm{\theta}\|_{2}^{2}.\]

Here \(\overline{\sigma}_{t}\) represents a weight parameter and \(\lambda\) denotes a regularization constant. For each approximation \(\widehat{\bm{\theta}}_{t}\), we stochastically draw \(\bm{\theta}_{t}^{\text{rs}}\) from a Gaussian distribution with mean \(\widehat{\bm{\theta}}_{t}\) and covariance \(\bm{\Sigma}_{t}=\widehat{\beta}_{t}\mathbf{V}_{t}^{-1}\), where \(\mathbf{V}_{t}=\lambda\mathbf{I}_{d}+\sum_{s=1}^{t-1}\mathbf{x}_{s}\mathbf{x }_{s}^{\intercal}/\overline{\sigma}_{s}^{2}\) is computed from previous context and reward pairs, and the confidence radius, \(\widehat{\beta}_{t}\), is chosen as

\[\widehat{\beta}_{t}\asymp\sqrt{d\log\left(\frac{t}{d\lambda\alpha^{2}}\right) \log\left(\frac{t^{2}}{\delta}\log\frac{\gamma^{2}}{\alpha}\right)}+\frac{R}{ \gamma^{2}}\log\left(\frac{t^{2}}{\delta}\log\frac{\gamma^{2}}{\alpha}\right) +\sqrt{\lambda}.\]

Refer to (A.2) for a detailed delineation of \(\widehat{\beta}_{t}\). For each context vector \(\mathbf{x}\in\mathcal{X}_{t}\), we compute the estimated reward as \(\langle\mathbf{x},\bm{\theta}_{t}^{\text{rs}}\rangle\), and the algorithm outlined in Algorithm 2 then selects the arm \(\mathbf{x}_{t}\) that optimizes this estimated reward. Following the approach of Zhou and Gu (2022), we construct the weight parameter \(\overline{\sigma}_{t}=\max\{\sigma_{t},\alpha,\gamma\|\mathbf{x}_{t}\|_{ \mathbf{V}_{t}^{-1}}^{1/2}\}\), which is designed as the maximum among the variance, a constant \(\alpha\), and the uncertainty associated with the selected arm \(\mathbf{x}_{t}\). It ensures the formulation of a tight confidence radius \(\widehat{\beta}_{t}\), guided by a Bernstein-type concentration inequality, _cf._ Theorem A.1.

```
0: Total number of iterations \(T\), number of total arms \(n\)
1: Initialize \(\mathbf{V}_{1}\leftarrow\lambda\mathbf{I}_{d}\), \(\bm{\nu}_{1}\gets 0\), \(\widehat{\bm{\theta}}_{1}\gets 0\), \(\widehat{\beta}_{1}\leftarrow\sqrt{\lambda}\), and \(\bm{\Sigma}_{1}\leftarrow\widehat{\beta}_{1}^{2}\mathbf{V}_{1}^{-1}\)
2:for\(t=1,\dots,T\)do
3: Observe context vectors \(\mathcal{X}_{t}\)
4: Draw \(\bm{\theta}_{t}^{\text{rs}}\sim\mathcal{N}(\widehat{\bm{\theta}}_{t},\bm{ \Sigma}_{t})\)
5:\(\mathbf{x}_{t}\leftarrow\operatorname*{arg\,max}_{\mathbf{x}\in\mathcal{X}_{t }}\langle\mathbf{x},\bm{\theta}_{t}^{\text{rs}}\rangle\)
6: Observe reward \(r_{t}\) and variance \(\sigma_{t}^{2}\)
7: Update \(\mathbf{V}_{t+1}\leftarrow\mathbf{V}_{t}+\mathbf{x}_{t}\mathbf{x}_{t}^{ \intercal}/\overline{\sigma}_{t}^{2}\)
8: Update \(\bm{\nu}_{t+1}\leftarrow\bm{\nu}_{t}+r_{t}\mathbf{x}_{t}/\overline{\sigma}_{t }^{2}\)
9: Update \(\widehat{\bm{\theta}}_{t+1}\leftarrow\bm{\nu}_{t+1}^{-1}\bm{\nu}_{t+1}\)
10: Compute covariance matrix \(\bm{\Sigma}_{t+1}\leftarrow\widehat{\beta}_{t+1}^{2}\mathbf{V}_{t+1}^{-1}\)
11:endfor ```

**Algorithm 2** Linear Variance-Dependent Thompson Sampling (LinVDTS)

### Regret guarantee and technical challenges

The theorem below establishes a variance-dependent upper bound on the expected regret of LinVDTS. See Appendix A.3 for detailed proof.

**Theorem 4.1**.: _Set parameters \(\alpha=1/\sqrt{T}\), \(\gamma=R^{\frac{1}{2}}/d^{\frac{1}{4}}\), \(\lambda=d\). For any \(\delta\in(0,1)\), the expected regret of Algorithm 2 is upper bounded as follows:_

\[\mathbb{E}[\mathcal{R}(T)]=\mathcal{O}\bigg{(}d^{3/2}\bigg{(}1+\sqrt{\sum \nolimits^{T}_{t=1}\sigma_{t}^{2}}\bigg{)}\log T+\delta T\bigg{)}.\]

_Further, it holds with probability \(1-\delta\) that_

\[\mathcal{R}(T)=\mathcal{O}\bigg{(}d^{3/2}\bigg{(}1+\sqrt{\sum\nolimits^{T}_{t= 1}\sigma_{t}^{2}}\bigg{)}\log T+\sqrt{T\log\frac{1}{\delta}}\bigg{)}.\]_Remark 4.2_.: Choosing \(\delta=1/T\), then we see that the expected regret of Algorithm 2 is bounded by \(\widetilde{\mathcal{O}}\Big{(}d^{3/2}\Big{(}1+\sqrt{\sum_{t=1}^{T}\sigma_{t}^{2} }\Big{)}\Big{)}\). However, the high-probability regret bound of Algorithm 2 is of order \(\widetilde{\mathcal{O}}\Big{(}d^{3/2}\Big{(}1+\sqrt{\sum_{t=1}^{T}\sigma_{t}^{2 }}\Big{)}+\sqrt{T}\Big{)}\), where the additional \(\sqrt{T}\) term fails to be variance-dependent. Nonetheless, this regret bound already improves over the existing \(\widetilde{\mathcal{O}}(d^{3/2}\sqrt{T})\) bound for linear Thompson sampling in the literature (Agrawal and Goyal, 2013; Abeille and Lazaric, 2017).

The limitation on the high-probability bound arises from a set of technical challenges that are implicit and inherent in the posterior sampling used in Algorithm 2. To illustrate this, we now provide an outline of the proof for Theorem 4.1, and then discuss the technical intricacies therein.

Proof sketch of Theorem 4.1.: Let us denote \(\Delta_{t}(\mathbf{x})=\langle\mathbf{x}_{t}^{*}-\mathbf{x},\bm{\theta}^{*}\rangle\) as the suboptimality of \(\mathbf{x}\in\mathcal{X}_{t}\). We introduce the saturated set \(\mathcal{S}(t)\) for round \(t\) as the ensemble of arms whose confidence radius is dwarfed by its suboptimality, _i.e._, \(\mathcal{S}(t)=\{\mathbf{x}\in\mathcal{X}_{t}\,:\,\Delta_{t}(\mathbf{x})>(2 \sqrt{d\log t}+1)\widehat{\beta}_{t}\|\mathbf{x}\|_{\mathbf{V}_{t}^{-1}}\}\)(Agrawal and Goyal, 2013). Then the suboptimality \(\Delta_{t}\) of the chosen context \(\mathbf{x}_{t}\) can be decomposed as follows:

\[\Delta_{t}=\langle\mathbf{x}_{t}^{*}-\mathbf{x}_{t},\bm{\theta}^{*}\rangle= \Delta_{t}(\mathbf{x}_{t}^{\dagger})+\mathbf{x}_{t}^{\dagger\,\mathsf{T}}\bm{ \theta}^{*}-\mathbf{x}_{t}^{\mathsf{T}}\bm{\theta}^{*},\]

where the term \(\mathbf{x}_{t}^{\dagger}=\arg\min_{\mathbf{x}\notin\mathcal{S}(t)}\|\mathbf{ x}\|_{\mathbf{V}_{t}^{-1}}\) refers to the unsaturated arm that possesses the smallest \(\mathbf{V}_{t}^{-1}\) norm. Leveraging the concentration results delineated in Lemmata A.2 and A.3 pertaining to \(\widehat{\bm{\theta}}_{t}\) and \(\bm{\theta}_{t}^{\text{rs}}\), we have

\[\Delta_{t}\lesssim\sqrt{d\log t}(\|\mathbf{x}_{t}^{\dagger}\|_{\mathbf{V}_{t }^{-1}}+\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}})\widehat{\beta}_{t}.\] (4.1)

For upper bound on the expected regret, we use an anti-concentration argument (Lemma A.6) to relate \(\|\mathbf{x}_{t}^{\dagger}\|_{\mathbf{V}_{t}^{-1}}\) back to \(\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}\). This yields

\[\mathbb{E}[\Delta_{t}\ |\ \mathscr{H}_{t}]\lesssim\sqrt{d\log t}\,\mathbb{E}[\| \mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}]\widehat{\beta}_{t}+\frac{2}{t^{2}},\] (4.2)

where \(\{\mathscr{H}_{t}\}_{t\geq 1}\) represents a filtration of the information available up to the observation of the set of context vectors at each round. Then collecting the above inequality for all \(t\in[T]\), together with an elliptical potential lemma, we get the desired upper bound for \(\mathbb{E}[\mathcal{R}(T)]\).

Next, for the high-probability regret bound, we decompose

\[\mathcal{R}(T)=\sum\nolimits_{t=1}^{T}\mathbb{E}[\Delta_{t}\ |\ \mathscr{H}_{t}]+\sum \nolimits_{t=1}^{T}(\Delta_{t}-\mathbb{E}[\Delta_{t}\ |\ \mathscr{H}_{t}])\] (4.3)

where the first term corresponds to the expected regret bound. We control the second term using martingale concentration, which results in an additional \(\sqrt{T}\) term in the final regret bound. 

#### Why cannot simultaneously achieve variance awareness and high-probability bound?

First, the optimism induced by the posterior sampling is too conservative.1 A primary challenge in the analysis is to effectively control each suboptimality term \(\Delta_{t}\), _cf._(4.1). It is important to note that one key algorithmic design in UCB-type algorithms is the exact optimism from the UCB bonus \(\beta\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}\), which ensures \(\langle x_{t}^{*},\widehat{\bm{\theta}}_{t}\rangle+\beta\|\mathbf{x}_{t}^{* }\|_{\mathbf{V}_{t}^{-1}}\leq\langle x_{t},\widehat{\bm{\theta}}_{t}\rangle+ \beta\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}\). However, for TS-style algorithms, the absence of this exact optimism hinders our control over the term \(\|\mathbf{x}_{t}^{*}\|_{\mathbf{V}_{t}^{-1}}\) throughout all rounds \(t\in[T]\), _i.e._, a standard decomposition of \(\Delta_{t}=\langle\mathbf{x}_{t}^{*}-\mathbf{x}_{t},\bm{\theta}^{*}\rangle\) yields \(\Delta_{t}\leq(2\sqrt{d\log t}+1)\widehat{\beta}_{t}(2\|\mathbf{x}_{t}^{*}\|_{ \mathbf{V}_{t}^{-1}}+\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}})\) in this case. As a result, standard analyses turn to methodologies that employ an anti-concentration inequality on the sampling distribution (Agrawal and Goyal, 2013). This leads to the inequality \(\|\mathbf{x}_{t}^{\dagger}\|_{\mathbf{V}_{t}^{-1}}\leq\|\mathbf{x}_{t}\|_{ \mathbf{V}_{t}^{-1}}\) with a constant probability, thereby providing an upper bound on the expected regret.

Footnote 1: This is also identified in Zhang (2022) as the source of suboptimal dependence on dimension in the regret bound for TS.

Second, in the conversion of the expected regret bound to a high-probability bound, we have no help from the noise variance information. One may wonder if it is possible to incorporate the variance of \(\Delta_{t}-\mathbb{E}[\Delta_{t}\mid\mathscr{H}_{t}]\) to get a Bernstein-type concentration result. However, the variance of \(\Delta_{t}-\mathbb{E}[\Delta_{t}\mid\mathscr{H}_{t}]\) does not necessarily conform to the noise variance, especially when the decision set \(\mathscr{X}_{t}\) is revealed adversarially. To see this, consider the following situation: Given \(\widehat{\bm{\theta}}_{t}\), the environment reveals a decision set \(\mathscr{X}_{t}=\{\mathbf{x}_{\perp},-\mathbf{x}_{\perp}\}\) where \(\mathbf{x}_{\perp}\) is orthogonal to \(\widehat{\bm{\theta}}_{t}\). Then a small perturbation in \(\bm{\theta}_{t}^{\intercal s}\) along the direction of \(\mathbf{x}_{\perp}\) can cause the change of arm selection. This implies that a small uncertainty of the posterior distribution along the direction of \(\mathbf{x}_{\perp}\) will be amplified to a constant variation of the selected arm \(\mathbf{x}_{t}\). Therefore, in general, we do not have any delicate control of the variance of \(\Delta_{t}-\mathbb{E}[\Delta_{t}\mid\mathscr{H}_{t}]\). Consequently, we can only apply Azuma-Hoeffding inequality to get a non-noise-adaptive term of \(\widetilde{\mathcal{O}}(\sqrt{T})\). We discuss in the next section how to conquer this via more advanced algorithmic design.

## 5 General case: a noise-adaptive Thompson sampling algorithm

In this section, we propose another TS algorithm LinNATS for linear contextual bandits, one that is provably adaptive to unknown heteroscedastic noise. For greater generality, we adopt the identical problem setting delineated in Section 3, and we refrain from assuming access to the variance information \(\{\sigma_{t}^{2}\}_{t=1}^{T}\) associated with the rewards \(\{r_{t}\}_{t=1}^{T}\).

### Algorithm

The proposed algorithm LinNATS is displayed in Algorithm 3. Below we go through the details of LinNATS and explain the algorithmic design along the way.

**Stratification over contextual uncertainty.** For more efficient uncertainty control, we adopt a stratification strategy akin to that previously used for UCB-type algorithms (Chu et al., 2011; Li et al., 2023; Zhao et al., 2023). Our algorithm segments the context vectors at each round \(t\) into \(L\) distinct layers, enabling the precise control of both \(\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}\) and \(\|\mathbf{x}_{t}^{\star}\|_{\mathbf{V}_{t}^{-1}}\).

Specifically, the \(\ell\)-th layer establishes a threshold of \(2^{-\ell}\) and retains a separate estimate \(\widehat{\bm{\theta}}_{t,\ell}\) and sampled variable \(\bm{\theta}_{t,\ell}^{\intercal s}\) for each \(\ell\in[L]\) and \(t\in[T]\) (Line 3). Commencing from \(\ell=1\), a sequence of decision sets \(\mathscr{X}_{t}=\mathscr{X}_{t,1}\supseteq\mathscr{X}_{t,2}\supseteq\ldots\) of diminishing sizes is derived by excluding the arms \(\mathbf{x}\in\mathscr{X}_{t,\ell}\) that are less likely to maximize \(\langle\mathbf{x},\bm{\theta}^{\star}\rangle\) at each layer \(\ell\). More specifically, on Line 11 we let

\[\mathscr{X}_{t,\ell+1}=\Big{\{}\mathbf{x}\in\mathscr{X}_{t,\ell}: \langle\mathbf{x},\bm{\theta}_{t,\ell}^{\intercal s}\rangle\geq\max_{\mathbf{ x}^{\prime}\in\mathscr{X}_{t,\ell}}\langle\mathbf{x}^{\prime},\bm{\theta}_{t, \ell}^{\intercal s}\rangle-2^{-\ell+1}(\sqrt{2d\log(4t^{2}L/\delta)}+1) \widehat{\beta}_{t,\ell}\Big{\}},\] (5.1)

which ensures that the optimal arm \(\mathbf{x}_{t}^{\star}\) falls into all decision sets with high probability. The context vector \(\mathbf{x}_{t}=\operatorname*{arg\,max}_{\mathbf{x}\in\mathscr{X}_{t,\ell}} \langle\mathbf{x},\bm{\theta}_{t,\ell}^{\intercal s}\rangle\) is chosen if the uncertainty term \(\|\mathbf{x}\|_{\mathbf{V}_{t,\ell}^{-1}}\leq\alpha\) for all \(\mathbf{x}\in\mathscr{X}_{t,\ell}\) (Line 8). Otherwise, \(\mathbf{x}_{t}\) is selected only when the uncertainty surpasses the threshold, _i.e._, \(\|\mathbf{x}_{t}\|_{\mathbf{V}_{t,\ell}^{-1}}\geq 2^{-\ell}\) for some \(\ell\), and the elimination process in (5.1) terminates (Line 13).

The round index \(t\) is incorporated into a growing index set \(\Psi_{t+1,\ell}\) (Line 9 & 15), and the observation pair \((\mathbf{x}_{t},r_{t})\) participates the later estimation of \(\widehat{\bm{\theta}}_{s,\ell}\) for all \(s>t\) only if the chosen arm exhibits large uncertainty within the current layer, _i.e._, \(\|\mathbf{x}_{t}\|_{\mathbf{V}_{t,\ell}^{-1}}\geq 2^{-\ell}\) (Line 21). This process guarantees that every context vector \(\mathbf{x}\in\mathscr{X}_{t,\ell}\), cascading through the first \((\ell-1)\)-th layer, satisfies \(\|\mathbf{x}\|_{\mathbf{V}_{t,\ell-1}^{-1}}\leq 2^{-\ell+1}\).

**Parameter estimation with weighted ridge regression.** To estimate \(\bm{\theta}^{\star}\), we again utilize weighted ridge regression, but this time within each uncertainty level. For each layer \(\ell\in[L]\) and round \(t\in[T]\), the associated estimator \(\widehat{\bm{\theta}}_{t,\ell}\) is given by

\[\widehat{\bm{\theta}}_{t,\ell}=\operatorname*{arg\,min}_{\bm{\theta}\in\mathbb{ R}^{d}}\sum\nolimits_{s\in\Psi_{t,\ell}}w_{s}^{2}(r_{s}-\langle\mathbf{x}_{s}, \bm{\theta}\rangle)^{2}+2^{-2\ell}\|\bm{\theta}\|_{2}^{2},\]

where the weight parameter \(w_{t}>0\) is selected to fulfill the condition \(\|w_{t}\mathbf{x}_{t}\|_{\mathbf{V}_{t,\ell}^{-1}}=2^{-\ell}\) (Line 14). Owing to the specific formulation of the weighting parameter, it follows that \(\sup_{s\in\Psi_{t,\ell}}\|w_{s}\mathbf{x}_{s}\|_{\mathbf{V}_{s,\ell}^{-1}}=2^{ -\ell}\) holds universally for all \(t\in[T]\) and \(\ell\in[L]\). This allows the application of a Freedman-typeconcentration inequality to guarantee with high probability for all layer \(\ell\in[L]\) that

\[\|\widehat{\bm{\theta}}_{t,\ell}-\bm{\theta}^{*}\|_{\mathbf{V}_{t,\ell}}= \widetilde{\mathcal{O}}\bigg{(}\frac{R}{2^{\ell}}+\frac{1}{2^{\ell}}\sqrt{\sum _{s\in\Psi_{t,\ell}}w_{s}^{2}\sigma_{s}^{2}}\bigg{)}.\] (5.2)

**Variance-dependent confidence radius.** The variance-dependent error bound (5.2) is sufficient for the formulation of a TS confidence radius, provided that the variance \(\sigma_{t}^{2}\) is known. However, under scenarios where the variance is unknown, it is necessary to further estimate them adaptively and on the fly. Specifically, we estimate the summation of past weighted variances, represented as \(\sum_{s\in\Psi_{t,\ell}}w_{s}^{2}\sigma_{s}^{2}\), each using an empirical estimator \((r_{t}-\langle\mathbf{x}_{t},\widehat{\bm{\theta}}_{t,\ell}\rangle)^{2}\) as a substitute for \(\sigma_{t}^{2}\). The weighted summation of these estimators \(\zeta_{t,\ell}=\sum_{s\in\Psi_{t,\ell}}w_{s}^{2}(r_{s}-\langle\mathbf{x}_{s}, \widehat{\bm{\theta}}_{s,\ell}\rangle)^{2}\) effectively acts as a precise estimator of \(\sum_{s\in\Psi_{t,\ell}}w_{s}^{2}\sigma_{s}^{2}\)(Zhao et al., 2023). Utilizing this estimator, we adjust the posterior distribution according to the following confidence radius

\[\widehat{\beta}_{t,\ell}=\frac{1}{2^{\ell-4}}\sqrt{\left(8\zeta_{t,\ell}+6R^{ 2}\log\frac{8t^{2}L}{\delta}+2^{-2\ell+4}\right)\log\frac{8t^{2}L}{\delta}}+ \frac{3R}{2^{\ell-1}}\log\frac{8t^{2}L}{\delta}+\frac{1}{2^{\ell-1}},\] (5.3)

where the variance estimator takes a slight variant for technical considerations:

\[\zeta_{t,\ell}=\begin{cases}\sum_{s\in\Psi_{t,\ell}}w_{s}^{2}(r_{s}-\langle \mathbf{x}_{s},\widehat{\bm{\theta}}_{t,\ell}\rangle)^{2},&\text{if }2^{\ell}\geq 64 \sqrt{\log(8t^{2}L/\delta)}\\ R^{2}|\Psi_{t,\ell}|,&\text{otherwise}.\end{cases}\] (5.4)

[MISSING_PAGE_FAIL:9]

Conclusion and future work

In this paper, we aimed to address the task of noise-adaptive learning on linear contextual bandits with indeterminate heteroscedastic variance. As part of our endeavor, we put forth a straightforward algorithm, LinVDTS, which assures a variance-dependent guarantee on the expected regret. Our analytical exploration reveals that a simplistic implementation of Thompson Sampling culminates in a sub-optimal regret bound. To mitigate this issue, we put forth an innovative Thompson Sampling algorithm, LinNATS. With the incorporation of a stratification scheme, the algorithm successfully navigates the technical challenges of uncertainty control, securing a variance-dependent regret under unknown variance. Consequently, it effectively bridges the performance chasm between the worst-case scenarios involving constant variance and those concerning deterministic rewards.

Looking forward, it would be intriguing to conceive a noise-adaptive variant of the recently devised Feel-Good TS as introduced by Zhang (2022), which enhances the dependency on the dimension \(d\). Furthermore, extending the noise-adaptive methodology to TS algorithms designed for more general settings, _e.g._, generalized linear bandits and Reinforcement Learning with linear function approximation, constitutes an appealing direction for future research.

## References

* Abbasi-Yadkori et al. (2011)Abbasi-Yadkori, Y., Pal, D. and Szepesvari, C. (2011). Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_**24**.
* Abe et al. (2003)Abe, N., Biermann, A. W. and Long, P. M. (2003). Reinforcement learning with immediate rewards and linear hypotheses. _Algorithmica_**37** 263-293.
* Abeille and Lazaric (2017)Abeille, M. and Lazaric, A. (2017). Linear thompson sampling revisited. In _Artificial Intelligence and Statistics_. PMLR.
* Abramowitz et al. (1964)Abramowitz, M., Stegun, I. A. et al. (1964). _Handbook of mathematical functions_, vol. 55. Dover New York.
* Agrawal and Goyal (2013)Agrawal, S. and Goyal, N. (2013). Thompson sampling for contextual bandits with linear payoffs. In _International conference on machine learning_. PMLR.
* Auer et al. (2002a)Auer, P., Cesa-Bianchi, N. and Fischer, P. (2002a). Finite-time analysis of the multiarmed bandit problem. _Machine learning_**47** 235-256.
* Auer et al. (2002b)Auer, P., Cesa-Bianchi, N., Freund, Y. and Schapire, R. E. (2002b). The nonstochastic multiarmed bandit problem. _SIAM journal on computing_**32** 48-77.
* Chapelle and Li (2011)Chapelle, O. and Li, L. (2011). An empirical evaluation of thompson sampling. _Advances in neural information processing systems_**24**.
* Chen et al. (2021)Chen, L., Min, Y., Belkin, M. and Karbasi, A. (2021). Multiple descent: Design your own generalization curve. _Advances in Neural Information Processing Systems_**34** 8898-8912.
* Chen et al. (2020)Chen, L., Min, Y., Zhang, M. and Karbasi, A. (2020). More data can expand the generalization gap between adversarially robust and standard models. In _International Conference on Machine Learning_. PMLR.
* Cheng and Kleijnen (1999)Cheng, R. C. and Kleijnen, J. P. (1999). Improved design of queueing simulation experiments with highly heteroscedastic responses. _Operations research_**47** 762-777.
* Chowdhury and Gopalan (2017)Chowdhury, S. R. and Gopalan, A. (2017). On kernelized multi-armed bandits. In _International Conference on Machine Learning_. PMLR.
* Chu et al. (2011)Chu, W., Li, L., Reyzin, L. and Schapire, R. (2011). Contextual bandits with linear payoff functions. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_. JMLR Workshop and Conference Proceedings.
* Dai et al. (2022)Dai, Y., Wang, R. and Du, S. S. (2022). Variance-aware sparse linear bandits. _arXiv preprint arXiv:2205.13450_.
* Deshpande and Montanari (2012)Deshpande, Y. and Montanari, A. (2012). Linear bandits in high dimension and recommendation systems. In _2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_. IEEE.
* Fei and Xu (2022a)Fei, Y. and Xu, R. (2022a). Cascaded gaps: Towards gap-dependent regret for risk-sensitive reinforcement learning. _arXiv preprint arXiv:2203.03110_.
* Fei and Xu (2022b)Fei, Y. and Xu, R. (2022b). Cascaded gaps: Towards logarithmic regret for risk-sensitive reinforcement learning. In _International Conference on Machine Learning_. PMLR.
* Gopalan et al. (2014)Gopalan, A., Mannor, S. and Mansour, Y. (2014). Thompson sampling for complex online problems. In _International conference on machine learning_. PMLR.
* Hamidi and Bayati (2020)Hamidi, N. and Bayati, M. (2020). On worst-case regret of linear thompson sampling. _arXiv preprint arXiv:2006.06790_.
* He et al. (2022)He, J., Wang, T., Min, Y. and Gu, Q. (2022). A simple and provably efficient algorithm for asynchronous federated contextual linear bandits. _Advances in neural information processing systems_**35** 4762-4775.
* He et al. (2012)* Henderson (1975)Henderson, C. R. (1975). Best linear unbiased estimation and prediction under a selection model. _Biometrics_**423-447**.
* Jacot et al. (2018)Jacot, A., Gabriel, F. and Hongler, C. (2018). Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_**31**.
* Kandasamy et al. (2018)Kandasamy, K., Krishnamurthy, A., Schneider, J. and Poczos, B. (2018). Parallelised bayesian optimisation via thompson sampling. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Kaufmann et al. (2012)Kaufmann, E., Korda, N. and Munos, R. (2012). Thompson sampling: An asymptotically optimal finite-time analysis. In _Algorithmic Learning Theory: 23rd International Conference, ALT 2012, Lyon, France, October 29-31, 2012. Proceedings 23_. Springer.
* Kim et al. (2021)Kim, W., Kim, G.-S. and Paik, M. C. (2021). Doubly robust thompson sampling with linear payoffs. _Advances in Neural Information Processing Systems_**34** 15830-15840.
* Kim et al. (2022a)Kim, W., Lee, K. and Paik, M. C. (2022a). Double doubly robust thompson sampling for generalized linear contextual bandits. _arXiv preprint arXiv:2209.06983_.
* Kim et al. (2022b)Kim, Y., Yang, I. and Jun, K.-S. (2022b). Improved regret analysis for variance-adaptive linear bandits and horizon-free linear mixture mdps. _Advances in Neural Information Processing Systems_**35** 1060-1072.
* Kirschner and Krause (2018)Kirschner, J. and Krause, A. (2018). Information directed sampling and bandits with heteroscedastic noise. In _Conference On Learning Theory_. PMLR.
* Korda et al. (2013)Korda, N., Kaufmann, E. and Munos, R. (2013). Thompson sampling for 1-dimensional exponential family bandits. _Advances in neural information processing systems_**26**.
* Lai et al. (1985)Lai, T. L., Robbins, H. et al. (1985). Asymptotically efficient adaptive allocation rules. _Advances in applied mathematics_**6** 4-22.
* Li et al. (2023)Li, Y., Wang, Y. and Zhou, Y. (2023). Nearly minimax-optimal regret for linearly parameterized bandits. _IEEE Transactions on Information Theory_.
* Lillicrap et al. (2015)Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. and Wierstra, D. (2015). Continuous control with deep reinforcement learning. _arXiv preprint arXiv:1509.02971_.
* Ling et al. (2019)Ling, S., Xu, R. and Bandeira, A. S. (2019). On the landscape of synchronization networks: A perspective from nonconvex optimization. _SIAM Journal on Optimization_**29** 1879-1907.
* Lu et al. (2022)Lu, M., Min, Y., Wang, Z. and Yang, Z. (2022). Pessimism in the face of confounders: Provably efficient offline reinforcement learning in partially observable markov decision processes. In _The Eleventh International Conference on Learning Representations_.
* Lu et al. (2021)Lu, Y., Xu, Z. and Tewari, A. (2021). Bandit algorithms for precision medicine. _arXiv preprint arXiv:2108.04782_.
* Luo and Bayati (2023)Luo, Y. and Bayati, M. (2023). Geometry-aware approaches for balancing performance and theoretical guarantees in linear bandits. _arXiv preprint arXiv:2306.14872_.
* May et al. (2012)May, B. C., Korda, N., Lee, A. and Leslie, D. S. (2012). Optimistic bayesian sampling in contextual-bandit problems. _Journal of Machine Learning Research_**13** 2069-2106.
* Min et al. (2021a)Min, Y., Chen, L. and Karbasi, A. (2021a). The curious case of adversarially robust models: More data can help, double descend, or hurt generalization. In _Uncertainty in Artificial Intelligence_. PMLR.
* Min et al. (2022a)Min, Y., He, J., Wang, T. and Gu, Q. (2022a). Learning stochastic shortest path with linear function approximation. In _International Conference on Machine Learning_. PMLR.
* Mnih et al. (2015)Min, Y., He, J., Wang, T. and Gu, Q. (2023). Cooperative multi-agent reinforcement learning: Asynchronous communication and linear function approximation. _International Conference on Machine Learning_.
* Min et al. (2022b)Min, Y., Wang, T., Xu, R., Wang, Z., Jordan, M. and Yang, Z. (2022b). Learn to match with no regret: Reinforcement learning in markov matching markets. _Advances in Neural Information Processing Systems_**35** 19956-19970.
* Min et al. (2021b)Min, Y., Wang, T., Zhou, D. and Gu, Q. (2021b). Variance-aware off-policy evaluation with linear function approximation. _Advances in neural information processing systems_**34** 7598-7610.
* Omari et al. (2018)Omari, C. O., Morita, P. N. and Gichuhi, A. W. (2018). Currency portfolio risk measurement with generalized autoregressive conditional heteroscedastic-extreme value theory-copula model.
* Riquelme et al. (2018)Riquelme, C., Tucker, G. and Snoek, J. (2018). Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling. _arXiv preprint arXiv:1802.09127_.
* Robbins (1952)Robbins, H. (1952). Some aspects of the sequential design of experiments.
* Rusmevichientong and Tsitsiklis (2010)Rusmevichientong, P. and Tsitsiklis, J. N. (2010). Linearly parameterized bandits. _Mathematics of Operations Research_**35** 395-411.
* Russo and Van Roy (2016)Russo, D. and Van Roy, B. (2016). An information-theoretic analysis of thompson sampling. _The Journal of Machine Learning Research_**17** 2442-2471.
* Russo et al. (2018)Russo, D. J., Van Roy, B., Kazerouni, A., Osband, I., Wen, Z. et al. (2018). A tutorial on thompson sampling. _Foundations and Trends(r) in Machine Learning_**11** 1-96.
* Saha and Kveton (2023)Saha, A. and Kveton, B. (2023). Only pay for what is uncertain: Variance-adaptive thompson sampling. _arXiv preprint arXiv:2303.09033_.
* Somu et al. (2018)Somu, N., MR, G. R., Kalpana, V., Kirthivasan, K. and VS, S. S. (2018). An improved robust heteroscedastic probabilistic neural network based trust prediction approach for cloud service selection. _Neural Networks_**108** 339-354.
* Song et al. (2021)Song, G., Xu, R. and Lafferty, J. (2021). Convergence and alignment of gradient descent with random backpropagation weights. _Advances in Neural Information Processing Systems_**34** 19888-19898.
* Thompson (1933)Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_**25** 285-294.
* Towse et al. (2015)Towse, A., Jonsson, B., McGrath, C., Mason, A., Puig-Peiro, R., Mestre-Ferrandiz, J., Pistolato, M. and Devlin, N. (2015). Understanding variations in relative effectiveness: A health production approach. _International journal of technology assessment in health care_**31** 363-370.
* Varatharajah and Berry (2022)Varatharajah, Y. and Berry, B. (2022). A contextual-bandit-based approach for informed decision-making in clinical trials. _Life_**12** 1277.
* Wang and Zhou (2020)Wang, Z. and Zhou, M. (2020). Thompson sampling via local uncertainty. In _International Conference on Machine Learning_. PMLR.
* Wu et al. (2016)Wu, Q., Wang, H., Gu, Q. and Wang, H. (2016). Contextual bandits in a collaborative environment. In _Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval_.
* Xu et al. (2022)Xu, P., Zheng, H., Mazumdar, E. V., Azizzadenesheli, K. and Anandkumar, A. (2022). Langevin monte carlo for contextual bandits. In _International Conference on Machine Learning_. PMLR.
* Xu et al. (2021)Xu, R., Chen, L. and Karbasi, A. (2021). Meta learning in the continuous time limit. In _International Conference on Artificial Intelligence and Statistics_. PMLR.

Xu, R., Min, Y., Wang, T., Jordan, M. I., Wang, Z., and Yang, Z. (2023). Finding regularized competitive equilibria of heterogeneous agent macroeconomic models via reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Ye et al. (2023)Ye, C., Xiong, W., Gu, Q. and Zhang, T. (2023). Corruption-robust algorithms with uncertainty weighting for nonlinear contextual bandits and markov decision processes. In _International Conference on Machine Learning_. PMLR.
* Yin et al. (2022)Yin, M., Duan, Y., Wang, M. and Wang, Y.-X. (2022). Near-optimal offline reinforcement learning with linear representation: Leveraging variance information with pessimism. _arXiv preprint arXiv:2203.05804_.
* Zhang (2022)Zhang, T. (2022). Feel-good thompson sampling for contextual bandits and reinforcement learning. _SIAM Journal on Mathematics of Data Science_**4** 834-857.
* Zhang et al. (2020)Zhang, W., Zhou, D., Li, L. and Gu, Q. (2020). Neural thompson sampling. _arXiv preprint arXiv:2010.00827_.
* Zhang et al. (2021)Zhang, Z., Yang, J., Ji, X. and Du, S. S. (2021). Improved variance-aware confidence sets for linear bandits and linear mixture mdp. _Advances in Neural Information Processing Systems_**34** 4342-4355.
* Zhao et al. (2023)Zhao, H., He, J., Zhou, D., Zhang, T. and Gu, Q. (2023). Variance-dependent regret bounds for linear bandits and reinforcement learning: Adaptivity and computational efficiency. _arXiv preprint arXiv:2302.10371_.
* Zhao et al. (2022)Zhao, H., Zhou, D., He, J. and Gu, Q. (2022). Bandit learning with general function classes: Heteroscedastic noise and variance-dependent regret bounds. _arXiv preprint arXiv:2202.13603_.
* Zhou and Gu (2022)Zhou, D. and Gu, Q. (2022). Computationally efficient horizon-free reinforcement learning for linear mixture mdps. _arXiv preprint arXiv:2205.11507_.
* Zhou et al. (2021)Zhou, D., Gu, Q. and Szepesvari, C. (2021). Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory_. PMLR.

Regret analysis of Algorithm 2

### Clarification of notation

Before elaborating on proofs, we delineate several shorthands to facilitate our discourse. For each \(t\in[T]\), we establish the events \(\mathcal{E}_{t}^{\bm{\theta}}\) and \(\mathcal{E}_{t}^{\textsc{TS}}\). These events are denoted as the inclusion of \(\widehat{\bm{\theta}}_{t}\) and \(\bm{\theta}_{t}^{\textsc{TS}}\) in the pertinent confidence ellipse with some confidence radius \(\widehat{\beta}_{t}\), respectively, _i.e._,

\[\mathcal{E}_{t}^{\bm{\theta}}=\big{\{}|\mathbf{x}^{\mathsf{T}}\widehat{\bm{ \theta}}_{t}-\mathbf{x}^{\mathsf{T}}\bm{\theta}^{*}|\leq\widehat{\beta}_{t} \|\mathbf{x}\|_{\mathbf{V}_{t}^{-1}},\forall\mathbf{x}\in\mathcal{X}_{t}\big{\}}\]

and

\[\mathcal{E}_{t}^{\textsc{TS}}=\big{\{}|\mathbf{x}^{\mathsf{T}}\bm{\theta}_{t} ^{\textsc{TS}}-\mathbf{x}^{\mathsf{T}}\widehat{\bm{\theta}}_{t}|\leq 2\widehat{ \beta}_{t}\sqrt{d\log t}\|\mathbf{x}\|_{\mathbf{V}_{t}^{-1}},\forall\mathbf{x }\in\mathcal{X}_{t}\big{\}}.\]

We proceed to define the set \(\mathcal{S}(t)\) representing the saturated arms at round \(t\) as follows:

\[\mathcal{S}(t)=\big{\{}\mathbf{x}\in\mathcal{X}_{t}:\Delta_{t}(\mathbf{x})>(2 \sqrt{d\log t}+1)\widehat{\beta}_{t}\|\mathbf{x}\|_{\mathbf{V}_{t}^{-1}}\big{\}},\] (A.1)

where we denote \(\Delta_{t}(\mathbf{x})=\langle\mathbf{x}_{t}^{*}-\mathbf{x},\bm{\theta}^{*}\rangle\) as the suboptimality of \(\mathbf{x}\in\mathcal{X}_{t}\). Moreover, we present a filtration, denoted by \(\{\mathscr{H}_{t}\}_{t\geq 1}\), which is defined as

\[\mathscr{H}_{t}=\{\mathcal{X}_{1},\bm{\theta}_{1}^{\textsc{TS}},x_{1}, \overline{\sigma}_{1},r_{1},\ldots,\mathcal{X}_{t-1},\bm{\theta}_{t-1}^{ \textsc{TS}},x_{t-1},\overline{\sigma}_{t-1},r_{t-1},\mathcal{X}_{t}\}.\]

This filtration encapsulates the information that has been gleaned up until the observation of the set of context vectors \(\mathcal{X}_{t}\) at each particular round.

Concentration inequalities are widely used in literature (Fei and Xu, 2022b; Min et al., 2022b; He et al., 2022; Lu et al., 2022; Xu et al., 2023), and here we introduce a Bernstein-type inequality applicable to vector-valued martingales. This theorem, when coalesced with the approach of weighted ridge regression, enables us to construct stringent bounds that are dependent on the variance \(\{\sigma_{t}^{\mathbb{Z}}\}_{t\geq 1}\).

**Theorem A.1** (Zhou and Gu (2022), Theorem 4.3).: _Let \(\{\mathscr{H}_{t}\}_{t=1}^{\infty}\) be a filtration, \(\{\mathbf{y}_{t},\varrho_{t}\}_{t\geq 1}\) a stochastic process so that for each \(t\geq 1\), \(\mathbf{y}_{t}\in\mathbb{R}^{d}\) is \(\mathscr{H}_{t}\)-measurable and \(\varrho_{t}\in\mathbb{R}\) is \(\mathscr{H}_{t+1}\)-measurable. For each \(t\geq 1\), define \(r_{t}:=\langle\mathbf{y}_{t},\bm{\theta}^{*}\rangle+\varrho_{t}\) and suppose that \(\varrho_{t}\), \(\mathbf{y}_{t}\) satisfy_

\[\|\mathbf{y}_{t}\|\leq L,\quad|\varrho_{t}|\leq R,\quad\mathbb{E}[\varrho_{t} \ |\ \mathscr{H}_{t}]=0,\quad\mathbb{E}[\varrho_{t}^{2}\ |\ \mathscr{H}_{t}]\leq \sigma^{2}\]

_for some constants \(L,R,\sigma,\epsilon>0\) and \(\bm{\theta}^{*}\in\mathbb{R}^{d}\). Then, for any \(0<\delta<1\), with probability at least \(1-\delta\) we have for all \(t>0\) that_

\[\bigg{\|}\sum_{s=1}^{t}\varrho_{s}\mathbf{y}_{s}\bigg{\|}_{\mathbf{U}_{t}^{-1} }\leq\beta_{t},\quad\|\bm{\theta}_{t}-\bm{\theta}^{*}\|_{\mathbf{U}_{t}}\leq \beta_{t}+\sqrt{\lambda}\|\bm{\theta}^{*}\|,\]

_where \(\mathbf{U}_{t}=\lambda\mathbf{I}_{d}+\sum_{s=1}^{t}\mathbf{y}_{t}\mathbf{y}_{t }^{\mathsf{T}}\), \(\bm{\nu}_{t}=\sum_{s=1}^{t}r_{s}\mathbf{y}_{t}\), \(\bm{\theta}_{t}=\mathbf{U}_{t}^{-1}\bm{\nu}_{t}\), and_

\[\beta_{t}=12\sigma\sqrt{d\cdot\log\bigg{(}1+\frac{tL^{2}}{d \lambda}\bigg{)}\log\bigg{(}32\Big{(}1+\log\frac{R}{\epsilon}\Big{)}\frac{t^{ 2}}{\delta}\bigg{)}}+6\log\bigg{(}32\Big{(}1+\log\frac{R}{\epsilon}\Big{)} \frac{t^{2}}{\delta}\bigg{)}\epsilon\] \[\qquad\qquad+24\log\bigg{(}32\Big{(}1+\log\frac{R}{\epsilon} \Big{)}\frac{t^{2}}{\delta}\bigg{)}\max_{s\in[t]}\{|\varrho_{s}|\min\{1,\| \mathbf{y}_{s}\|_{\mathbf{U}_{s-1}^{-1}}\}\}.\]

### Proof of supporting lemmata

The forthcoming Lemmas A.2 and A.3 show that the events \(\mathcal{E}_{t}^{\bm{\theta}}\) and \(\mathcal{E}_{t}^{\textsc{TS}}\) hold with high probability for all \(t\in[T]\). The anti-concentration argument can then be developed in a manner analogous to the approach employed by Agrawal and Goyal (2013), which is often not necessary in UCB-type of arguments (Fei and Xu, 2022a; Chu et al., 2011; Min et al., 2023; Zhao et al., 2023).

**Lemma A.2**.: _Under Assumption 3.1, for any \(\delta\in(0,1)\), if we define for all \(t\geq 1\) that_

\[\widehat{\beta}_{t}=12\sqrt{d\cdot\log\bigg{(}1+\frac{t}{d \lambda\alpha^{2}}\bigg{)}\log\bigg{(}64\Big{(}1+\log\frac{\gamma^{2}}{\alpha} \Big{)}\frac{t^{2}}{\delta}\bigg{)}}\] (A.2) \[\qquad\qquad\qquad+\frac{30R}{\gamma^{2}}\cdot\log\bigg{(}64 \Big{(}1+\log\frac{\gamma^{2}}{\alpha}\Big{)}\frac{t^{2}}{\delta}\bigg{)}+ \sqrt{\lambda}\]_with the coefficients \(\alpha,\gamma,\lambda\) introduced in Algorithm 2, then the estimate \(\widehat{\bm{\theta}}_{t}\) given by the associated weighted regressions satisfy_

\[\mathbb{P}\left(|\mathbf{x}^{\mathsf{T}}\widehat{\bm{\theta}}_{t}-\mathbf{x}^{ \mathsf{T}}\bm{\theta}^{*}|\leq\widehat{\beta}_{t}\|\mathbf{x}\|_{\mathbf{V}_ {t}^{-1}},\forall\mathbf{x}\in\mathcal{X}_{t},\forall t\geq 1\right)\geq 1- \delta/2.\]

Proof.: Observe that for any \(t\geq 1\) and \(\mathbf{x}\in\mathcal{X}_{t}\), by Cauchy-Schwarz inequality we have

\[|\mathbf{x}^{\mathsf{T}}\widehat{\bm{\theta}}_{t}-\mathbf{x}^{\mathsf{T}}\bm{ \theta}^{*}|\leq\|\widehat{\bm{\theta}}_{t}-\bm{\theta}^{*}\|_{\mathbf{V}_{t} }\|\mathbf{x}\|_{\mathbf{V}_{t}^{-1}},\] (A.3)

so it suffices to bound each \(\|\mathbf{x}\|_{\mathbf{V}_{t}^{-1}}\). By Assumption 3.1, the following conditions on \(\varepsilon_{t}\) and \(\mathbf{x}_{t}\) hold:

\[\left|\frac{\varepsilon_{t}}{\overline{\sigma}_{t}}\right|\leq\frac{R}{ \alpha},\quad\mathbb{E}[\varepsilon_{t}\ |\ \mathscr{H}_{t}]=0,\quad\mathbb{E}\left[\left(\frac{ \varepsilon_{t}}{\overline{\sigma}_{t}}\right)^{2}\Big{|}\ \mathscr{H}_{t}\right]\leq 1,\quad\left\| \frac{\mathbf{x}_{t}}{\overline{\sigma}_{t}}\right\|\leq\frac{1}{\alpha}.\]

Moreover, we can obtain following \(\overline{\sigma}_{t}=\max\{\sigma_{t},\alpha,\gamma\|\mathbf{x}_{t}\|_{ \mathbf{V}_{t}^{-1}}^{1/2}\}\) that

\[\left|\frac{\varepsilon_{t}}{\overline{\sigma}_{t}}\right|\cdot\min\left\{1, \left\|\frac{\mathbf{x}_{t}}{\overline{\sigma}_{t}}\right\|_{\mathbf{V}_{t}^ {-1}}\right\}\leq\frac{R}{\overline{\sigma}_{t}^{2}}\|\mathbf{x}_{t}\|_{ \mathbf{V}_{t}^{-1}}\leq\frac{R}{\gamma^{2}}.\]

Therefore, by applying Theorem A.1 with the stochastic process \(\{\mathbf{x}_{t}/\overline{\sigma}_{t},\varepsilon_{t}/\overline{\sigma}_{t} \}_{t\geq 1}\) and \(\epsilon=R/\gamma^{2}\), we deduce that with probability at least \(1-\delta/2\) for all \(t\geq 1\)

\[\|\widehat{\bm{\theta}}_{t}-\bm{\theta}^{*}\|_{\mathbf{V}_{t}} \leq 12\sqrt{d\cdot\log\left(1+\frac{t}{d\lambda\alpha^{2}}\right) \log\left(64\Big{(}1+\log\frac{\gamma^{2}}{\alpha}\Big{)}\frac{t^{2}}{\delta} \right)}\] \[\qquad+\frac{30R}{\gamma^{2}}\cdot\log\left(64\Big{(}1+\log\frac{ \gamma^{2}}{\alpha}\Big{)}\frac{t^{2}}{\delta}\right)+\sqrt{\lambda}\|\bm{ \theta}^{*}\|.\]

Since \(\|\bm{\theta}^{*}\|_{2}\leq 1\) by Assumption 3.1, it follows from the definition of \(\widehat{\beta}_{t}\) that \(\|\widehat{\bm{\theta}}_{t}-\bm{\theta}^{*}\|_{\mathbf{V}_{t}}\leq\widehat{ \beta}_{t}\). Combining this with (A.3), we conclude that with probability at least \(1-\delta/2\),

\[|\mathbf{x}^{\mathsf{T}}\widehat{\bm{\theta}}_{t}-\mathbf{x}^{\mathsf{T}}\bm{ \theta}^{*}|\leq\widehat{\beta}_{t}\|\mathbf{x}\|_{\mathbf{V}_{t}^{-1}}\]

for all \(t\geq 1\) and \(\mathbf{x}\in\mathcal{X}_{t}\). 

The following lemma establishes the control over \(|\mathbf{x}^{\mathsf{T}}\bm{\theta}_{t}^{\text{\tiny TS}}-\mathbf{x}^{\mathsf{ T}}\widehat{\bm{\theta}}_{t}|\).

**Lemma A.3**.: _Under Assumption 3.1, for any \(\delta\in(0,1)\), let \(\widehat{\beta}_{t}\) be as defined in Lemma A.2 for all \(t\geq 1\). Then the sampled variable \(\bm{\theta}_{t}^{\text{\tiny TS}}\) in Algorithm 2 satisfies_

\[\mathbb{P}\left(|\mathbf{x}^{\mathsf{T}}\bm{\theta}_{t}^{\text{\tiny TS}}- \mathbf{x}^{\mathsf{T}}\widehat{\bm{\theta}}_{t}|\leq\widehat{\beta}_{t}\sqrt{2 d\log(4t^{2}/\delta)}\cdot\|\mathbf{x}\|_{\mathbf{V}_{t}^{-1}},\forall\mathbf{x} \in\mathcal{X}_{t},\forall t\geq 1\right)\geq 1-\frac{\delta}{2}.\]

Proof.: By definition, \(\bm{\theta}_{t}^{\text{\tiny TS}}\sim\mathcal{N}(\widehat{\bm{\theta}}_{t}, \bm{\Sigma}_{t})\) where \(\bm{\Sigma}_{t}=\widehat{\beta}_{t}^{2}\mathbf{V}_{t}^{-1}\), thus \(\|\bm{\theta}_{t}^{\text{\tiny TS}}-\widehat{\bm{\theta}}_{t}\|_{\bm{\Sigma}_{ t}^{-1}}\sim\mathcal{N}(0,1)\). Then using the concentration inequality in Lemma C.1 for standard normal distribution, for every \(t\in[T]\), we have with probability at least \(1-\frac{\delta}{4\delta^{2}}\) that

\[|\mathbf{x}^{\mathsf{T}}\bm{\theta}_{t}^{\text{\tiny TS}}-\mathbf{ x}^{\mathsf{T}}\widehat{\bm{\theta}}_{t}| =|\mathbf{x}^{\mathsf{T}}\mathbf{V}_{t}^{-1/2}\mathbf{V}_{t}^{1/2}(\bm{ \theta}_{t}^{\text{\tiny TS}}-\widehat{\bm{\theta}}_{t})|\] \[\leq\widehat{\beta}_{t}\left\|\frac{\mathbf{V}_{t}^{1/2}(\bm{ \theta}_{t}^{\text{\tiny TS}}-\widehat{\bm{\theta}}_{t})}{\widehat{\beta}_{t}} \right\|_{2}\ \|\mathbf{x}\|_{\mathbf{V}_{t}^{-1}}\] \[\leq\widehat{\beta}_{t}\sqrt{2d\log(4t^{2}/\delta)}\|\mathbf{x}\|_{ \mathbf{V}_{t}^{-1}},\]

where the first inequality follows from Cauchy-Schwarz inequality. Applying a union bound over \(t\in[T]\) and utilizing the fact that \(\sum_{t=1}^{\infty}\frac{1}{t^{2}}\leq 2\), we obtain the final result. 

In the following, we delve into the anti-concentration argument presented in Lemmas A.4 to A.6, which constitute the upper bound on expected regret.

**Lemma A.4**.: _For any \(t\geq 1\), if \(\mathscr{H}_{t}\) is a filtration such that \(\mathscr{E}_{t}^{\boldsymbol{\theta}}\) holds, we have a constant probability that the reward estimate \(\langle\mathbf{x}_{t}^{\star},\boldsymbol{\theta}_{t}^{\text{\tiny TS}}\rangle\) of the optimal arm constitutes an upper confidence bound, i.e.,_

\[\mathbb{P}(\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta}_{t}^{\text{ \tiny TS}}>\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta}^{\star}\ |\ \mathscr{H}_{t})\geq\frac{1}{4e\sqrt{\pi}}.\]

Proof.: Note that we can bound the quantity \(J_{t}=(\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta}^{\star}-\mathbf{ x}_{t}^{\star\,\intercal}\widehat{\boldsymbol{\theta}}_{t})/(\widehat{ \beta}_{t}\|\mathbf{x}_{t}^{\star}\|_{\mathbf{V}_{t}^{-1}})\) as follows:

\[|J_{t}| =\left|\frac{\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta }^{\text{\tiny TS}}-\mathbf{x}_{t}^{\star\,\intercal}\widehat{\boldsymbol{ \theta}}_{t}}{\widehat{\beta}_{t}\|\mathbf{x}_{t}^{\star}\|_{\mathbf{V}_{t}^ {-1}}}\right|\] \[\leq\frac{\widehat{\beta}_{t}\|\mathbf{x}_{t}^{\star}\|_{ \mathbf{V}_{t}^{-1}}}{\widehat{\beta}_{t}\|\mathbf{x}_{t}^{\star}\|_{\mathbf{ V}_{t}^{-1}}}\] \[=1,\] (A.4)

where the inequality is due to Lemma A.2. Recalling that \(\boldsymbol{\theta}_{t}^{\text{\tiny TS}}\) has a mean of \(\widehat{\boldsymbol{\theta}}_{t}\), we can establish that the probability of the reward estimate \(\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta}_{t}^{\text{\tiny TS}}\) serving as an upper bound for the true reward is lower-bounded by

\[\mathbb{P}(\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta}_{ t}^{\text{\tiny TS}}>\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta}^{ \star}\ |\ \mathscr{H}_{t}) =\mathbb{P}(\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta} _{t}^{\text{\tiny TS}}-\mathbf{x}_{t}^{\star\,\intercal}\widehat{ \boldsymbol{\theta}}_{t}>\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta} ^{\star}-\mathbf{x}_{t}^{\star\,\intercal}\widehat{\boldsymbol{\theta}}_{t} \ |\ \mathscr{H}_{t})\] \[=\mathbb{P}\left(\frac{\mathbf{x}_{t}^{\star\,\intercal} \boldsymbol{\theta}_{t}^{\text{\tiny TS}}-\mathbf{x}_{t}^{\star\,\intercal} \widehat{\boldsymbol{\theta}}_{t}}{\widehat{\beta}_{t}\|\mathbf{x}_{t}^{\star }\|_{\mathbf{V}_{t}^{-1}}}>\frac{\mathbf{x}_{t}^{\star\,\intercal} \boldsymbol{\theta}^{\star}-\mathbf{x}_{t}^{\star\,\intercal}\widehat{ \boldsymbol{\theta}}_{t}}{\widehat{\beta}_{t}\|\mathbf{x}_{t}^{\star}\|_{ \mathbf{V}_{t}^{-1}}}\ \middle|\mathscr{H}_{t}\right)\] \[\geq\frac{1}{4\sqrt{\pi}}e^{-J_{t}^{2}}.\] (A.5)

The last inequality follows from the anti-concentration inequality in Lemma C.1 for standard normal distribution and the fact that

\[\frac{\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta}_{t}^{\text{\tiny TS }}-\mathbf{x}_{t}^{\star\,\intercal}\widehat{\boldsymbol{\theta}}_{t}}{ \widehat{\beta}_{t}\|\mathbf{x}_{t}^{\star}\|_{\mathbf{V}_{t}^{-1}}}\sim \mathcal{N}(0,1).\]

Consequently, combining (A.4) and (A.5), it follows that the desired probability is lower bounded by a constant, _i.e._, \(\mathbb{P}(\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta}_{t}^{\text{ \tiny TS}}>\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta}^{\star}\ |\ \mathscr{H}_{t})\geq\frac{1}{4e\sqrt{\pi}}.\) 

**Lemma A.5**.: _For any \(t\geq 1\), if \(\mathscr{H}_{t}\) is a filtration such that \(\mathscr{E}_{t}^{\boldsymbol{\theta}}\) holds, we have with a constant probability that the chosen arm \(\mathbf{x}_{t}\) is not a saturated arm, i.e.,_

\[\mathbb{P}(\mathbf{x}_{t}\notin\mathscr{S}(t)\ |\ \mathscr{H}_{t})\geq\frac{1}{4e \sqrt{\pi}}-\frac{1}{t^{2}}.\]

Proof.: Recall the definition of the set of saturated arms \(\mathscr{S}(t)\) in (A.1) and that the selected arm \(\mathbf{x}_{t}=\operatorname*{arg\,max}_{\mathbf{x}\in\mathcal{X}_{t}} \mathbf{x}^{\intercal}\boldsymbol{\theta}_{t}^{\text{\tiny TS}}\) aims to maximize the estimated reward. Consequently, \(\mathbf{x}_{t}\notin\mathscr{S}(t)\) if \(\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta}_{t}^{\text{\tiny TS}}\geq \mathbf{x}^{\intercal}\boldsymbol{\theta}_{t}^{\text{\tiny TS}}\) for all saturated arms \(\mathbf{x}\in\mathscr{S}(t)\), implying that the estimated reward for the optimal arm surpasses the estimated rewards of all saturated arms. It follows that:

\[\mathbb{P}(\mathbf{x}_{t}\notin\mathscr{S}(t)\ |\ \mathscr{H}_{t})\geq\mathbb{P}( \mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta}_{t}^{\text{\tiny TS}}\geq \mathbf{x}^{\intercal}\boldsymbol{\theta}_{t}^{\text{\tiny TS}},\forall\mathbf{x} \in\mathscr{S}(t)\ |\ \mathscr{H}_{t}).\]

Further note that by the definition of saturated arms, we have

\[\mathbf{x}^{\intercal}\boldsymbol{\theta}_{t}^{\text{\tiny TS}}\leq\mathbf{x}^ {\intercal}\boldsymbol{\theta}^{\star}+(2\sqrt{d\log t}+1)\widehat{\beta}_{t} \|\mathbf{x}\|_{\mathbf{V}_{t}^{-1}}<\mathbf{x}^{\intercal}\boldsymbol{\theta}^{ \star}+\Delta_{t}(\mathbf{x})=\mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{ \theta}^{\star}\]

when both \(\mathscr{E}_{t}^{\boldsymbol{\theta}}\) and \(\mathscr{E}_{t}^{\text{\tiny TS}}\) hold. Therefore,

\[\mathbb{P}(\mathbf{x}_{t}\notin\mathscr{S}(t)\ |\ \mathscr{H}_{t})\geq\mathbb{P}( \mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta}_{t}^{\text{\tiny TS}}> \mathbf{x}_{t}^{\star\,\intercal}\boldsymbol{\theta}^{\text{\tiny TS}}\ |\ \mathscr{H}_{t})-\mathbb{P}(\overline{\mathscr{E}^{\text{\tiny TS}}}(t)\ |\ \mathscr{H}_{t}),\]

The last inequality is a consequence of Lemmas A.3 and A.4.

**Lemma A.6**.: _For any \(t\geq 1\), if \(\mathscr{H}_{t}\) is a filtration such that both \(\mathscr{E}_{t}^{\boldsymbol{\theta}}\) and \(\mathscr{E}_{t}^{\text{\tiny TS}}\) hold, we have_

\[\mathbb{E}[\Delta_{t}\ |\ \mathscr{H}_{t}]\leq\left(\frac{2}{\frac{1}{4e\sqrt{ \pi}}-\frac{1}{t^{2}}}+1\right)(2\sqrt{d\log t}+1)\widehat{\beta}_{t}\,\mathbb{ E}[\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}\ |\ \mathscr{H}_{t}]+\frac{2}{t^{2}}.\]

Proof.: Let \(\mathbf{x}_{t}^{\dagger}\) represent the unsaturated arm with the smallest \(\|\cdot\|_{\mathbf{V}_{t}^{-1}}\) norm, _i.e._,

\[\mathbf{x}_{t}^{\dagger}=\operatorname*{arg\,min}_{\mathbf{x}\notin\mathcal{S} (t)}\|\mathbf{x}\|_{\mathbf{V}_{t}^{-1}}.\]

The existence of such an unsaturated arm \(\mathbf{x}_{t}^{\dagger}\) is guaranteed since \(\mathbf{x}_{t}^{*}\notin\mathcal{S}(t)\). When both \(\mathscr{E}_{t}^{\boldsymbol{\theta}}\) and \(\mathscr{E}_{t}^{\text{\tiny TS}}\) hold, we can express the suboptimality as follows:

\[\Delta_{t} =\Delta_{t}(\mathbf{x}_{t}^{\dagger})+{\mathbf{x}_{t}^{\dagger}} ^{\boldsymbol{\theta}}\boldsymbol{\ast}-\mathbf{x}_{t}^{\boldsymbol{\theta}} \boldsymbol{\ast}\] \[\leq\Delta_{t}(\mathbf{x}_{t}^{\dagger})+({\mathbf{x}_{t}^{ \dagger}}^{\boldsymbol{\theta}}\boldsymbol{\ast}_{t}^{\text{\tiny TS}}+(2 \sqrt{d\log t}+1)\widehat{\beta}_{t}\|\mathbf{x}_{t}^{\dagger}\|_{\mathbf{V}_{t }^{-1}})\] \[\qquad\qquad-(\mathbf{x}_{t}^{\boldsymbol{\theta}}\boldsymbol{ \ast}_{t}^{\text{\tiny TS}}-(2\sqrt{d\log t}+1)\widehat{\beta}_{t}\|\mathbf{x }_{t}\|_{\mathbf{V}_{t}^{-1}})\]

Then since \(\mathbf{x}_{t}\) is the optimal arm under \(\boldsymbol{\theta}_{t}^{\text{\tiny TS}}\), we further have

\[\Delta_{t} \leq\Delta_{t}(\mathbf{x}_{t}^{\dagger})+(2\sqrt{d\log t}+1) \widehat{\beta}_{t}\|\mathbf{x}_{t}^{\dagger}\|_{\mathbf{V}_{t}^{-1}}+(2 \sqrt{d\log t}+1)\widehat{\beta}_{t}\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}\] \[\leq(2\sqrt{d\log t}+1)\widehat{\beta}_{t}(2\|\mathbf{x}_{t}^{ \dagger}\|_{\mathbf{V}_{t}^{-1}}+\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}),\]

where the second inequality results from \(\mathbf{x}_{t}^{\dagger}\notin\mathcal{S}(t)\). Note that \(\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}\geq\|\mathbf{x}_{t}^{\dagger}\|_{ \mathbf{V}_{t}^{-1}}\) with constant probability, _i.e._,

\[\mathbb{E}[\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}\ |\ \mathscr{H}_{t}] \geq\mathbb{E}[\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}\ |\ \mathscr{H}_{t},\mathbf{x}_{t}\notin\mathcal{S}(t)]\cdot \mathbb{P}(\mathbf{x}_{t}\notin\mathcal{S}(t)\ |\ \mathscr{H}_{t})\] \[\geq\left(\frac{1}{4e\sqrt{\pi}}-\frac{1}{t^{2}}\right)\mathbb{E} [\|\mathbf{x}_{t}^{\dagger}\|_{\mathbf{V}_{t}^{-1}}\ |\ \mathscr{H}_{t}].\] (A.6)

The last inequality is due to Lemma A.5 and the definition of \(\mathbf{x}_{t}^{\dagger}\) as the unsaturated arm with the smallest \(\|\cdot\|_{\mathbf{V}_{t}^{-1}}\). Consequently,

\[\mathbb{E}[\Delta_{t}\ |\ \mathscr{H}_{t}] \leq\mathbb{E}[(2\sqrt{d\log t}+1)\widehat{\beta}_{t}(2\|\mathbf{ x}_{t}^{\dagger}\|_{\mathbf{V}_{t}^{-1}}+\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}})\ |\ \mathscr{H}_{t}]+2\,\mathbb{P}( \overline{\mathscr{E}_{t}^{\text{\tiny TS}}})\] \[\leq\left(\frac{2}{\frac{1}{4e\sqrt{\pi}}-\frac{1}{t^{2}}}+1 \right)(2\sqrt{d\log t}+1)\widehat{\beta}_{t}\,\mathbb{E}[\|\mathbf{x}_{t}\|_{ \mathbf{V}_{t}^{-1}}\ |\ \mathscr{H}_{t}]+\frac{2}{t^{2}},\]

where the first inequality is due to \(\Delta_{t}\leq 2\) for all \(t\), the second inequality follows from applying (A.6), Lemmas A.3 and A.4. 

### Proof of Theorem 4.1

Let us define a shorthand \(\mathscr{E}^{\boldsymbol{\theta}}=\cap_{t=1}^{T}\mathscr{E}_{t}^{\boldsymbol{ \theta}}\). The expected regret can be decomposed as follows:

\[\mathbb{E}[\mathcal{R}(T)] =\sum_{t=1}^{T}\mathbb{E}[\Delta_{t}\ |\ \mathscr{E}^{ \boldsymbol{\theta}}]\,\mathbb{P}(\mathscr{E}^{\boldsymbol{\theta}})+\sum_{t=1 }^{T}\mathbb{E}[\Delta_{t}\ |\ \overline{\mathscr{E}^{\boldsymbol{\theta}}}]\,\mathbb{P}( \overline{\mathscr{E}^{\boldsymbol{\theta}}})\] \[\leq\sum_{t=1}^{T}\mathbb{E}[\Delta_{t}\ |\ \mathscr{H}_{t}, \mathscr{E}^{\boldsymbol{\theta}}]+2T\cdot\frac{\delta}{2}\] \[\leq\sum_{t=1}^{T}\mathbb{E}[\min\{2,(2\sqrt{d\log t}+1)C\widehat{ \beta}_{t}\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}\}\ |\ \mathscr{H}_{t}]+\sum_{t=1}^{T}\frac{2}{t^{2}}+\delta T\] \[=\mathbb{E}\left[\,\sum_{t=1}^{T}\min\{2,(2\sqrt{d\log t}+1)C \widehat{\beta}_{t}\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}\}\,\bigg{|}\, \mathscr{E}^{\boldsymbol{\theta}}\right]+\delta T+4,\]where we write \(C=\max_{t\geq 1}\frac{2}{\lfloor\frac{1}{4c\sqrt{\pi}}-\frac{1}{2}\rfloor}+1\). The first inequality is due to Lemma A.2, the second inequality is due to Lemma A.6, and the last inequality follows from \(\Delta_{t}\leq 2\) for all \(t\in[T]\). To alleviate the burden on notation, we write \(g_{t}=\min\{2,(2\sqrt{d\log t}+1)C\widehat{\beta}_{t}\|\mathbf{x}_{t}\|_{ \mathbf{V}_{t}^{-1}}\}\) and an index set \(\mathcal{T}=\{t\in[T]:\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|_{\mathbf{V}_{t }^{-1}}\geq 1\}\). It follows that

\[\sum_{t=1}^{T}g_{t} =\sum_{t=1}^{T}\min\{2,(2\sqrt{d\log t}+1)C\widehat{\beta}_{t} \overline{\sigma}_{t}\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|_{\mathbf{V}_{t }^{-1}}\}\] \[=\sum_{t\in\mathcal{T}}\min\{2,(2\sqrt{d\log t}+1)C\widehat{\beta }_{t}\overline{\sigma}_{t}\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|_{\mathbf{V }_{t}^{-1}}\}\] \[\qquad\quad+\sum_{t\notin\mathcal{T}}\min\{2,(2\sqrt{d\log t}+1) C\widehat{\beta}_{t}\overline{\sigma}_{t}\|\mathbf{x}_{t}/\overline{\sigma}_{t} \|_{\mathbf{V}_{t}^{-1}}\}\] \[\leq 2|\mathcal{T}|+\sum_{t\notin\mathcal{T}}(2\sqrt{d\log t}+1) C\widehat{\beta}_{t}\overline{\sigma}_{t}\|\mathbf{x}_{t}/\overline{\sigma}_{t} \|_{\mathbf{V}_{t}^{-1}}\] \[=2|\mathcal{T}|+3C\sum_{t\notin\mathcal{T}}\widehat{\beta}_{t} \sqrt{d\log t}\overline{\sigma}_{t}\min\{1,\|\mathbf{x}_{t}/\overline{\sigma} _{t}\|_{\mathbf{V}_{t}^{-1}}\}\] \[\leq 4d\log(1+T/(d\lambda\alpha^{2}))+3C\sum_{t\notin\mathcal{T}} \widehat{\beta}_{t}\sqrt{d\log t}\overline{\sigma}_{t}\min\{1,\|\mathbf{x}_{t }/\overline{\sigma}_{t}\|_{\mathbf{V}_{t}^{-1}}\},\] (A.7)

where the first inequality follows from the definition of \(g_{t}\), the last equality is due to \(\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|_{\mathbf{V}_{t}^{-1}}<1\) for all \(t\notin\mathcal{T}\), and the second inequality is due to

\[|\mathcal{T}| \leq\sum_{t\in\mathcal{T}}\min\{1,\|\mathbf{x}_{t}/\overline{ \sigma}_{t}\|_{\mathbf{V}_{t}^{-1}}^{2}\}\] \[\leq\sum_{t=1}^{T}\min\{1,\|\mathbf{x}_{t}/\overline{\sigma}_{t} \|_{\mathbf{V}_{t}^{-1}}^{2}\}\] \[\leq 2d\log(1+T/(d\lambda\alpha^{2})).\]

The last inequality is due to Lemma C.2 and \(\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|\leq 1/\alpha\). Further, we decompose \(\overline{\mathcal{T}}=[T]\backslash\mathcal{T}=\overline{\mathcal{T}}_{1} \cup\overline{\mathcal{T}}_{2}\) where

\[\overline{\mathcal{T}}_{1}=\{t\in\overline{\mathcal{T}}:\overline{\sigma}_{t} =\sigma_{t}\text{ or }\overline{\sigma}_{t}=\alpha\},\qquad\overline{ \mathcal{T}}_{1}=\{t\in\overline{\mathcal{T}}:\overline{\sigma}_{t}=\gamma\| \mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}^{1/2}\}.\]

The second term in (A.7) can be controlled through a decomposition

\[\sum_{t\notin\mathcal{T}}\widehat{\beta}_{t}\sqrt{d\log t} \overline{\sigma}_{t}\min\{1,\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|_{ \mathbf{V}_{t}^{-1}}\} =\sum_{t\in\overline{\mathcal{T}}_{1}}\widehat{\beta}_{t}\sqrt{d \log t}\overline{\sigma}_{t}\min\{1,\|\mathbf{x}_{t}/\overline{\sigma}_{t}\| _{\mathbf{V}_{t}^{-1}}\}\] \[\qquad\quad+\sum_{t\in\overline{\mathcal{T}}_{2}}\widehat{\beta}_{t }\sqrt{d\log t}\overline{\sigma}_{t}\min\{1,\|\mathbf{x}_{t}/\overline{\sigma} _{t}\|_{\mathbf{V}_{t}^{-1}}\},\]

For \(t\in\overline{\mathcal{T}}_{1}\), we have

\[\sum_{t\in\overline{\mathcal{T}}_{1}}\widehat{\beta}_{t}\sqrt{d \log t}\overline{\sigma}_{t}\min\{1,\|\mathbf{x}_{t}/\overline{\sigma}_{t}\| _{\mathbf{V}_{t}^{-1}}\} \leq\sum_{t\in\overline{\mathcal{T}}_{1}}\widehat{\beta}_{t}\sqrt{d \log t}(\sigma_{t}+\alpha)\min\{1,\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|_{ \mathbf{V}_{t}^{-1}}\}\] \[\leq\sum_{t=1}^{T}\widehat{\beta}_{t}\sqrt{d\log t}(\sigma_{t}+ \alpha)\min\{1,\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|_{\mathbf{V}_{t}^{-1}}\}\] \[\leq\sqrt{2d\sum_{t=1}^{T}\widehat{\beta}_{t}^{2}(\sigma_{t}^{2}+ \alpha^{2})\log T}\sqrt{\sum_{t=1}^{T}\min\{1,\|\mathbf{x}_{t}/\overline{ \sigma}_{t}\|_{\mathbf{V}_{t}^{-1}}^{2}\}}\] \[\leq 2\sqrt{d\sum_{t=1}^{T}\widehat{\beta}_{t}^{2}(\sigma_{t}^{2}+ \alpha^{2})\log T}\sqrt{d\log(1+T/(d\lambda\alpha^{2}))},\]where the first inequality is due to Cauchy-Schwarz inequality, and the second inequality is due to Lemma C.2 and \(\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|\leq 1/\alpha\). For \(t\in\overline{\mathcal{T}}_{2}\), we have \(\overline{\sigma}_{t}=\gamma^{2}\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|_{ \mathbf{V}_{t}^{-1}}\), and then

\[\sum_{t\in\overline{\mathcal{T}}_{2}}\widehat{\beta}_{t}\sqrt{d \log t\sigma}_{t}\min\{1,\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|_{\mathbf{V }_{t}^{-1}}\} =\sum_{t\in\overline{\mathcal{T}}_{2}}\widehat{\beta}_{t}\sqrt{d \log t\sigma}_{t}\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|_{\mathbf{V}_{t}^{-1}}\] \[\leq\gamma^{2}\sum_{t\in\overline{\mathcal{T}}_{2}}\widehat{ \beta}_{t}\sqrt{d\log t}\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|_{\mathbf{V}_ {t}^{-1}}^{2}\] \[\leq\gamma^{2}\sum_{t=1}^{T}\widehat{\beta}_{t}\sqrt{d\log t}\min \{1,\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|_{\mathbf{V}_{t}^{-1}}^{2}\}\] \[\leq 2\gamma^{2}d\sqrt{d\log T}\log(1+T/(d\lambda\alpha^{2}))( \max_{t\in[T]}\widehat{\beta}_{t}).\]

Hence, combining the inequalities above, we have

\[\sum_{t=1}^{T}g_{t} \leq 4d\log(1+T/(d\lambda\alpha^{2}))+3C\cdot 2\sqrt{d\sum_{t=1}^{ T}\widehat{\beta}_{t}^{2}(\sigma_{t}^{2}+\alpha^{2})\log T\sqrt{d\log(1+T/(d \lambda\alpha^{2}))}}\] \[\quad+3C\cdot 2\gamma^{2}d\sqrt{d\log T}\log(1+T/(d\lambda \alpha^{2}))(\max_{t\in[T]}\widehat{\beta}_{t}),\]

and an upper bound on expected regret follows noting that \(\sum_{t=1}^{T}\min\{2,(2\sqrt{d\log t}+1)C\widehat{\beta}_{t}\|\mathbf{x}_{t} \|_{\mathbf{V}_{t}^{-1}}\}\) enjoys a deterministic upper bound:

\[\mathbb{E}[\mathcal{R}(T)] \leq\sum_{t=1}^{T}g_{t}+\delta T+4\] \[\leq 4d\log(1+T/(d\lambda\alpha^{2}))+3C\cdot 2\sqrt{d\sum_{t=1}^{ T}\widehat{\beta}_{t}^{2}(\sigma_{t}^{2}+\alpha^{2})\log T\sqrt{d\log(1+T/(d \lambda\alpha^{2}))}}\] \[\quad+3C\cdot 2\gamma^{2}d\sqrt{d\log T}\log(1+T/(d\lambda \alpha^{2}))(\max_{t\in[T]}\widehat{\beta}_{t})+\delta T+4\]

Take \(\lambda=d\), \(\alpha=1/\sqrt{T}\), and \(\gamma=\sqrt{R}/d^{1/4}\), it follows that

\[\log(1+T/(d\lambda\alpha^{2}))\leq\log(T^{2}/d^{2}+1)\]

and

\[\widehat{\beta}_{t} =12\sqrt{d\log(1+t/(d\lambda\alpha^{2}))\log(32(1+\log(\gamma^{ 2}/\alpha))t^{2}/\delta)}\] \[\quad+30R\log(32(1+\log(\gamma^{2}/\alpha))t^{2}/\delta)/\gamma^ {2}+\sqrt{\lambda}\] \[\leq 12\sqrt{d\log(1+tT/d^{2})\log(32(1+\log(R\sqrt{T}/\sqrt{d}))t ^{2}/\delta)}\] \[\quad+30\sqrt{d}\log(32(1+\log(R\sqrt{T}/\sqrt{d}))t^{2}/\delta) +\sqrt{d}.\]

Combining Lemma A.2 and the inequalities above with \(\delta=\frac{1}{T}\), we conclude that

\[\mathbb{E}[\mathcal{R}(T)] \lesssim d\log T+d\log T\sqrt{\sum_{t=1}^{T}\widehat{\beta}_{t}^{ 2}(\sigma_{t}^{2}+\alpha^{2})+\gamma^{2}\sqrt{d^{3}\log^{3}T(\max_{t\in[T]} \widehat{\beta}_{t})}}\] \[\lesssim d\log T+d\log T\sqrt{\sum_{t=1}^{T}\widehat{\beta}_{t}^ {2}(\sigma_{t}^{2}+\alpha^{2})+d\log T\sqrt{d\log T}}\] \[\lesssim d\log T\sqrt{\sum_{t=1}^{T}\widehat{\beta}_{t}^{2}( \sigma_{t}^{2}+1/T)}+d\log T\sqrt{d\log T}\] \[\lesssim d^{3/2}\log T\sqrt{\sum_{t=1}^{T}\sigma_{t}^{2}}+d^{3/2} \log T.\]Recall that we have

\[\mathbb{E}[\mathcal{R}(T)]\leq\sum_{t=1}^{T}\mathbb{E}[\min\{2,(2\sqrt{d\log t}+1) C\widehat{\beta}_{t}\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}\}\mid\mathscr{H}_{t}] +\sum_{t=1}^{T}\frac{2}{t^{2}},\]

For notational convenience, we further define \(S_{t}=\sum_{s=1}^{t}D_{t}\) where \(D_{t}=\Delta_{t}-g_{t}-\frac{2}{t^{2}}\), and

\[g_{t}=\min\{2,(2\sqrt{d\log t}+1)C\widehat{\beta}_{t}\|\mathbf{x}_{t}\|_{ \mathbf{V}_{t}^{-1}}\}.\]

It follows that \(\mathbb{E}[D_{t}\mid\mathscr{H}_{t}]\leq 0\) for all \(t\in[T]\), and \(\{S_{t}\}_{t\geq 1}\) therefore forms a super-martingale process with respect to the filtrations \(\{\mathscr{H}_{t}\}_{t}\). Note that \(|\widetilde{D}_{t}|\leq 2g_{t}+2\leq 4\), and with Azuma-Hoeffding inequality we have with probability at least \(1-\delta/2\) that

\[\sum_{t=1}^{T}G_{t} \leq\sum_{t=1}^{T}g_{t}+\sum_{t=1}^{T}\frac{2}{t^{2}}+\sqrt{2 \sum_{t=1}^{T}4\log(2/\delta)}\] \[\leq\sum_{t=1}^{T}\min\{2,(2\sqrt{d\log t}+1)C\widehat{\beta}_{t }\|\mathbf{x}_{t}\|_{\mathbf{V}_{t}^{-1}}\}+\sum_{t=1}^{T}\frac{2}{t^{2}}+2 \sqrt{2T\log(2/\delta)}\] \[=\sum_{t=1}^{T}\min\{2,(2\sqrt{d\log t}+1)C\widehat{\beta}_{t} \overline{\sigma}_{t}\|\mathbf{x}_{t}/\overline{\sigma}_{t}\|_{\mathbf{V}_{t}^ {-1}}\}+\sum_{t=1}^{T}\frac{2}{t^{2}}+2\sqrt{2T\log(2/\delta)}\] \[\lesssim d^{3/2}\log T\sqrt{\sum_{t=1}^{T}\sigma_{t}^{2}}+d^{3/2} \log T+\sqrt{T}.\]

## Appendix B Regret Analysis of Algorithm 3

### Clarification of notation

Analogous to Appendix A, we first introduce two concentration events \(\mathcal{E}_{t,\ell}^{\boldsymbol{\theta}}\) and \(\mathcal{E}_{t,\ell}^{\text{\tiny TS}}\) for each \(t\in[T]\) and \(\ell\in[L]\), _i.e._,

\[\mathcal{E}_{t,\ell}^{\boldsymbol{\theta}}=\{|\mathbf{x}^{\intercal} \boldsymbol{\widehat{\theta}}_{t,\ell}-\mathbf{x}^{\intercal}\boldsymbol{ \theta}^{*}|\leq\widetilde{\beta}_{t,\ell}\|\mathbf{x}\|_{\mathbf{V}_{t,\ell} ^{-1}},\forall\mathbf{x}\in\mathcal{X}_{t}\}\]

and

\[\mathcal{E}_{t,\ell}^{\text{\tiny TS}}=\{|\mathbf{x}^{\intercal} \boldsymbol{\theta}_{t,\ell}^{\text{\tiny TS}}-\mathbf{x}^{\intercal} \boldsymbol{\widehat{\theta}}_{t,\ell}|\leq\sqrt{2d\log(4t^{2}L/\delta)} \widehat{\beta}_{t,\ell}\|\mathbf{x}\|_{\mathbf{V}_{t,\ell}^{-1}},\forall \mathbf{x}\in\mathcal{X}_{t}\}.\]

These events are also characterized as the inclusion of \(\boldsymbol{\widehat{\theta}}_{t,\ell}\) and \(\boldsymbol{\theta}_{t,\ell}^{\text{\tiny TS}}\) in the corresponding confidence ellipse, respectively. It is noteworthy that the event \(\mathcal{E}_{t,\ell}^{\boldsymbol{\theta}}\) is defined using

\[\widetilde{\beta}_{t,\ell}=16\cdot 2^{-\ell}\sqrt{\sum_{s\in\Psi_{t,\ell}}w_{s}^ {2}\sigma_{s}^{2}\log(8t^{2}L/\delta)}+6\cdot 2^{-\ell}R\log(8t^{2}L/\delta)+2^{- \ell+1}\] (B.1)

rather than the confidence radius \(\widehat{\beta}_{t,\ell}\).

Subsequently, we introduce a Freedman-type concentration inequality pertinent to vector-valued martingales. This theorem, distinct from Theorem A.1, necessitates a nuanced control on the uncertainty term \(\|\mathbf{y}_{\ell}\|_{\mathbf{U}_{t-1}^{-1}}\) for all \(t\geq 1\). Consequently, it furnishes an even more stringent bound in comparison to Theorem A.1.

**Theorem B.1** (Zhao et al. (2023), Theorem 2.1).: _Let \(\{\mathscr{H}_{t}\}_{t=1}^{\infty}\) be a filtration, \(\{\mathbf{y}_{t},\varrho_{t}\}_{t\geq 1}\) a stochastic process so that \(\mathbf{y}_{t}\in\mathbb{R}^{d}\) is \(\mathscr{H}_{t}\)-measurable and \(\varrho_{t}\in\mathbb{R}\) is \(\mathscr{H}_{t+1}\)-measurable. For \(t\geq 1\) let \(r_{t}=\langle\mathbf{y}_{t},\boldsymbol{\theta}^{*}\rangle+\varrho_{t}\) and suppose that \(\varrho_{t}\), \(\mathbf{y}_{t}\) satisfy_

\[|\varrho_{t}|\leq R,\quad\mathbb{E}[\varrho_{t}\mid\mathscr{H}_{t}]=0,\quad \sum_{s=1}^{t}\mathbb{E}[\varrho_{s}^{2}\mid\mathscr{H}_{s}]\leq v_{t}\]_for some constants \(R,\{v_{t}\}_{t}>0\) and \(\bm{\theta}^{*}\in\mathbb{R}^{d}\). Then, for any \(0<\delta<1\), with probability at least \(1-\delta\) we have for all \(t>0\) that_

\[\Big{\|}\sum_{s=1}^{t}\varrho_{s}\mathbf{y}_{s}\Big{\|}_{\mathbf{U}_{t}^{-1}} \leq\beta_{t},\quad\|\bm{\theta}_{t}-\bm{\theta}^{*}\|_{\mathbf{U}_{t}}\leq \beta_{t}+\sqrt{\lambda}\|\bm{\theta}^{*}\|,\]

_where \(\mathbf{U}_{t}=\lambda\mathbf{I}_{d}+\sum_{s=1}^{t}\mathbf{y}_{s}\mathbf{y}_{ s}^{\intercal}\), \(\bm{\nu}_{t}=\sum_{s=1}^{t}r_{s}\mathbf{y}_{s}\), \(\bm{\theta}_{t}=\mathbf{U}_{t}^{-1}\bm{\nu}_{t}\), and_

\[\beta_{t}=16\rho\sqrt{v_{t}\log(4t^{2}/\delta)}+6\rho R\log(4t^{2}/\delta),\]

_where \(\rho\geq\sup_{t\geq 1}\|\mathbf{y}_{t}\|_{\mathbf{U}_{t-1}^{-1}}\)._

### Proof of supporting lemmata

Lemmas B.2 and B.3 show that the concentration events \(\mathcal{E}_{t,\ell}^{\bm{\theta}}\) and \(\mathcal{E}_{t,\ell}^{\text{Ts}}\) hold with high probability for all \(t\in[T]\) and \(\ell\in[L]\).

**Lemma B.2**.: _For any \(\delta\in(0,1)\), if we define \(\widetilde{\beta}_{t,\ell}\) for all \(t\geq 1\) and \(\ell\in[L]\) as in (B.1), then the estimate \(\widehat{\bm{\theta}}_{t,\ell}\) given by the associated weighted regressions in Algorithm 3 satisfy_

\[\mathbb{P}\left(|\mathbf{x}^{\intercal}\widehat{\bm{\theta}}_{t,\ell}-\mathbf{ x}^{\intercal}\bm{\theta}^{*}|\leq\widetilde{\beta}_{t,\ell}\|\mathbf{x}\|_{ \mathbf{V}_{t,\ell}^{-1}},\forall\mathbf{x}\in\mathcal{X}_{t},\forall t\in \Psi_{t+1,\ell},\forall\ell\in[L]\right)\geq 1-\frac{\delta}{2}.\]

Proof.: Notice that for any fixed \(\ell\in[L]\), \(t\in\Psi_{t+1,\ell}\), and \(\mathbf{x}\in\mathcal{X}_{t}\) that

\[|\mathbf{x}^{\intercal}\widehat{\bm{\theta}}_{t,\ell}-\mathbf{x}^{\intercal} \bm{\theta}^{*}|\leq\|\widehat{\bm{\theta}}_{t,\ell}-\bm{\theta}^{*}\|_{ \mathbf{V}_{t,\ell}}\|\mathbf{x}\|_{\mathbf{V}_{t,\ell}^{-1}}.\]

Following the assumption on \(\varepsilon_{t}\), it holds that

\[|w_{t}\varepsilon_{t}|\leq|\varepsilon_{t}|\leq R,\quad\mathbb{E}[w_{t} \varepsilon_{t}\ |\ \mathscr{H}_{t}]=0,\quad\mathbb{E}[w_{t}^{2}\varepsilon_{t}^{2}\ |\ \mathscr{H}_{t}]\leq w_{t}^{2}\sigma_{t}^{2},\quad\|w_{t} \mathbf{x}_{t}\|_{\mathbf{V}_{t,\ell}^{-1}}=2^{-\ell},\]

where \(w_{t}=2^{-\ell}/\|\mathbf{x}_{t}\|_{\mathbf{V}_{t,\ell}^{-1}}\leq 1\). Apply Theorem B.1 with stochastic process \(\{w_{t}\mathbf{x}_{t},w_{t}\varepsilon_{t}\}\) to get that with probability at least \(1-\frac{\delta}{2L}\) for all \(t\in\Psi_{T+1,\ell}\)

\[\|\widehat{\bm{\theta}}_{t,\ell}-\bm{\theta}^{*}\|_{\mathbf{V}_{t,\ell}} \leq\widetilde{\beta}_{t,\ell},\]

where

\[\widetilde{\beta}_{t,\ell}=16\cdot 2^{-\ell}\sqrt{\sum_{s\in\Psi_{t,\ell}}w_{s}^{2 }\sigma_{s}^{2}\log(8t^{2}L/\delta)}+6\cdot 2^{-\ell}R\log(8t^{2}L/\delta)+2^{- \ell+1}.\]

It follows from a union bound that with probability at least \(1-\delta/2\) that for all \(\ell\in[L]\), \(t\in\Psi_{t+1,\ell}\), and \(\mathbf{x}\in\mathcal{X}_{t}\)

\[|\mathbf{x}^{\intercal}\widehat{\bm{\theta}}_{t,\ell}-\mathbf{x}^{\intercal} \bm{\theta}^{*}|\leq\widetilde{\beta}_{t,\ell}\|\mathbf{x}\|_{\mathbf{V}_{t, \ell}^{-1}}.\]

**Lemma B.3**.: _For any \(\delta\in(0,1)\) and \(\widehat{\beta}_{t,\ell}\) defined in (5.3) for all \(t\geq 1\) and \(\ell\in[L]\), the sampled variables \(\bm{\theta}_{t,\ell}^{\text{Ts}}\) in Algorithm 3 satisfy_

\[\mathbb{P}\left(|\mathbf{x}^{\intercal}\bm{\theta}_{t,\ell}^{\text{Ts}}- \mathbf{x}^{\intercal}\widehat{\bm{\theta}}_{t,\ell}|\leq\sqrt{2d\log(4t^{2}L/ \delta)}\cdot\widehat{\beta}_{t,\ell}\|\mathbf{x}\|_{\mathbf{V}_{t,\ell}^{-1} },\forall\mathbf{x}\in\mathcal{X}_{t},\forall t\in\Psi_{t+1,\ell},\forall\ell \in[L]\right)\geq 1-\frac{\delta}{2}.\]

Proof.: Note that \(\bm{\theta}_{t,\ell}^{\text{Ts}}\sim\mathcal{N}(\widehat{\bm{\theta}}_{t,\ell},\bm{\Sigma}_{t,\ell})\) where \(\bm{\Sigma}_{t}=\widehat{\beta}_{t,\ell}^{2}\mathbf{V}_{t,\ell}^{-1}\), we have with probability at least \(1-\frac{\delta}{4t^{2}L}\)

\[|\mathbf{x}^{\intercal}\bm{\theta}_{t,\ell}^{\text{Ts}}-\mathbf{ x}^{\intercal}\widehat{\bm{\theta}}_{t,\ell}| =|\mathbf{x}^{\intercal}\mathbf{V}_{t,\ell}^{-1/2}\mathbf{V}_{t,\ell}^{1/2}( \bm{\theta}_{t,\ell}^{\text{Ts}}-\widehat{\bm{\theta}}_{t,\ell})|\] \[\leq\widehat{\beta}_{t,\ell}\left\|\frac{\mathbf{V}_{t,\ell}^{1/2}( \bm{\theta}_{t,\ell}^{\text{Ts}}-\widehat{\bm{\theta}}_{t,\ell})}{\widehat{ \beta}_{t,\ell}}\right\|_{2}\|\mathbf{x}\|_{\mathbf{V}_{t,\ell}^{-1}}\] \[\leq\widehat{\beta}_{t,\ell}\sqrt{2d\log(4t^{2}L/\delta)}\| \mathbf{x}\|_{\mathbf{V}_{t,\ell}^{-1}},\]where the last inequality follows the concentration inequality in Lemma C.1 for standard normal distribution and that

\[\left\|\frac{\mathbf{V}_{t,\ell}^{1/2}(\bm{\theta}_{t,\ell}^{\texttt{rs}}-\widehat {\bm{\theta}}_{t,\ell})}{\widehat{\beta}_{t,\ell}}\right\|_{2}\sim\mathcal{N}( 0,1).\]

A union bound over all \(\ell\in[L]\) and \(t\geq 1\) yields the desired result, where the probability bound follows \(\sum_{t=1}^{\infty}1/t^{2}<2\). 

Next, we show that the empirical estimator \(\zeta_{t,\ell}\) provides an accurate estimate of the summation of weighted variance up to a logarithmic factor. The proof is analogous to Zhao et al. (2023), and we provide it here for the sake of maintaining a self-contained discourse.

**Lemma B.4**.: _For all \(t\geq 1\) and \(\ell\in[L]\) such that \(2^{\ell}\geq 64\sqrt{\log(4(t+1)^{2}L/\delta)}\), if \(\mathfrak{E}_{t,\ell}^{\bm{\theta}}\) is satisfied and \(\zeta_{t,\ell}=\sum_{s\in\Psi_{t,\ell}}w_{s}^{2}(r_{s}-\langle\mathbf{x}_{s}, \widehat{\bm{\theta}}_{s,\ell}\rangle)^{2}\), then the following inequalities hold:_

\[\sum_{s\in\Psi_{t+1,\ell}}w_{s}^{2}\sigma_{s}^{2} \leq 8\zeta_{t+1,\ell}+6R^{2}\log(4(t+1)^{2}L/\delta)+2^{-2\ell+4},\] \[\zeta_{t+1,\ell} \leq\frac{3}{2}\sum_{s\in\Psi_{t+1,\ell}}w_{s}^{2}\sigma_{s}^{2} +\frac{7}{3}R^{2}\log(4t^{2}L/\delta)+2^{-2\ell}.\]

Proof.: For any \(\ell\in[L]\) and \(t\in[T]\), we have \(\sum_{s\in\Psi_{t+1,\ell}}w_{i}^{2}\varepsilon_{s}^{2}\) being an unbiased estimator of \(\sum_{s\in\Psi_{t+1,\ell}}w_{i}^{2}\sigma_{s}^{2}\) conditioned on past observations. More specifically, we have \(\mathbb{E}[\varepsilon_{t}^{2}\mid\mathbf{x}_{1:t},r_{1:t-1}]-\mathbb{E}[ \sigma_{t}^{2}\mid\mathbf{x}_{1:t},r_{1:t-1}]=0\) for all \(t\geq 1\) and

\[\sum_{s\in\Psi_{t+1,\ell}}\mathbb{E}[w_{s}^{2}(\varepsilon_{s}^{ 2}-\sigma_{s}^{2})^{2}\mid\mathbf{x}_{1:t},r_{1:t-1}] \leq\sum_{s\in\Psi_{t+1,\ell}}\mathbb{E}[w_{s}^{2}\varepsilon_{s} ^{4}\mid\mathbf{x}_{1:t},r_{1:t-1}]\] \[\leq R^{2}\sum_{s\in\Psi_{t+1,\ell}}w_{s}^{2}\sigma_{s}^{2},\]

where the first inequality follows from \(\forall\mathrm{ar}(w_{s}\varepsilon_{s}^{2}\mid\mathbf{x}_{1:t},r_{1:t-1})\leq \mathbb{E}[w_{s}^{2}\varepsilon_{s}^{4}\mid\mathbf{x}_{1:t},r_{1:t-1}]\). Following Freedman inequality, it holds that with probability at least \(1-2\delta/L\) that for all \(t\geq 1\)

\[\left|\sum_{s\in\Psi_{t+1,\ell}}w_{s}^{2}\varepsilon_{s}^{2}- \sum_{s\in\Psi_{t+1,\ell}}w_{s}^{2}\sigma_{s}^{2}\right| \leq\sqrt{2R^{2}\sum_{s\in\Psi_{t+1,\ell}}w_{s}^{2}\sigma_{s}^{2} \log(4t^{2}L/\delta)}+\frac{4}{3}R^{2}\log(4t^{2}L/\delta)\] (B.2) \[\leq\frac{1}{2}\sum_{s\in\Psi_{t+1,\ell}}w_{s}^{2}\sigma_{s}^{2} +\frac{7}{3}R^{2}\log(4t^{2}L/\delta),\]

where the second inequality follows from Young's inequality.

It then follows that

\[\sum_{s\in\Psi_{t+1,\ell}}w_{s}^{2}\sigma_{s}^{2} \leq 2\sum_{s\in\Psi_{t+1,\ell}}w_{s}^{2}\varepsilon_{s}^{2}+\frac{ 14}{3}R^{2}\log(4t^{2}L/\delta)\] \[\leq 4\sum_{s\in\Psi_{t,\ell}}w_{s}^{2}((r_{s}-\langle\mathbf{x}_{s}, \widehat{\bm{\theta}}_{t,\ell}\rangle)^{2}+\frac{14}{3}R^{2}\log(4t^{2}L/\delta)\] \[\qquad+4\sum_{s\in\Psi_{t,\ell}}w_{s}^{2}((r_{s}-\langle\mathbf{x} _{s},\widehat{\bm{\theta}}_{t,\ell}\rangle)-\varepsilon_{s})^{2},\]

where the second inequality follows from \((x+y)^{2}\leq 2x^{2}+2y^{2}\). Notice that the last term is bounded by

\[\sum_{s\in\Psi_{t,\ell}}w_{s}^{2}((r_{s}-\langle\mathbf{x}_{s}, \widehat{\bm{\theta}}_{t,\ell}\rangle)-\varepsilon_{s})^{2} =\sum_{s\in\Psi_{t,\ell}}w_{s}^{2}\langle\mathbf{x}_{s},\widehat{ \bm{\theta}}_{t,\ell}-\bm{\theta}^{*}\rangle^{2}\] \[\leq(\widehat{\bm{\theta}}_{t,\ell}-\bm{\theta}^{*})^{\intercal} \mathbf{V}_{t+1,\ell}(\widehat{\bm{\theta}}_{t,\ell}-\bm{\theta}^{*})\] \[\leq\widetilde{\beta}_{t+1,\ell}^{2},\]where the first inequality follows from \(w_{s}^{2}x_{s}x_{s}^{2}\leq\mathbf{V}_{t+1,\ell}\) and the second inequality is due to Lemma B.2. If we have \(2^{\ell}\geq 64\sqrt{\log(8t^{2}L/\delta)}\), then

\[\widetilde{\beta}_{t+1,\ell}^{2}\leq\frac{1}{8}\sum_{s\in\Psi_{t+1,\ell}}w_{s}^ {2}\sigma_{s}^{2}+2(6\cdot 2^{-\ell}R\log(4(t+1)^{2}L/\delta)+2^{-\ell+1})^{2}\]

and

\[\sum_{s\in\Psi_{t+1,\ell}}w_{s}^{2}\sigma_{s}^{2}\leq 4\sum_{s\in\Psi_{t,\ell}}w_{s}^ {2}(r_{s}-\langle\mathbf{x}_{s},\widehat{\bm{\theta}}_{t,\ell}\rangle)^{2}+ \frac{14}{3}R^{2}\log(4t^{2}L/\delta)\] \[\qquad\qquad\qquad\qquad+\frac{1}{2}\sum_{s\in\Psi_{t+1,\ell}}w_{s }^{2}\sigma_{s}^{2}+8(6\cdot 2^{-\ell}R\log(4(t+1)^{2}L/\delta)+2^{-\ell+1})^{2}.\]

Move \(\frac{1}{2}\sum_{s\in\Psi_{t+1,\ell}}w_{s}^{2}\sigma_{s}^{2}\) to the other side of the inequality, we get the desired result.

Moreover, we can establish the inequality in the other direction:

\[\sum_{s\in\Psi_{t,\ell}}w_{s}^{2}(r_{s}-\langle\mathbf{x}_{s}, \widehat{\bm{\theta}}_{t,\ell}\rangle)^{2} \leq\sum_{s\in\Psi_{t,\ell}}w_{s}^{2}(r_{s}-\langle\mathbf{x}_{s},\bm{\theta}^{*}\rangle)^{2}+2^{-2\ell}\|\bm{\theta}^{*}\|_{2}^{2}\] \[\leq\sum_{s\in\Psi_{t+1,\ell}}w_{s}^{2}\varepsilon_{s}^{2}+2^{-2 \ell},\]

where the first inequality follows from \(\widehat{\bm{\theta}}_{t,\ell}\) being the minimizer of the weighted ridge regression problem. Combining the inequality with (B.2) yields the desired result. 

**Lemma B.5**.: _Suppose that \(\mathcal{E}_{t,\ell}^{\bm{\theta}}\) and \(\mathcal{E}_{t,\ell}^{\text{\rm TS}}\) hold for all \(\ell\in[L]\) and \(t\in\Psi_{t+1,\ell}\). If all the elements in series \(\{\widetilde{\beta}_{t,\ell}\}_{t,\ell}\) and \(\{\widehat{\beta}_{t,\ell}\}_{t,\ell}\), as defined in (B.1) and (5.3) respectively, satisfy_

\[\widetilde{\beta}_{t,\ell}\leq\widehat{\beta}_{t,\ell},\]

_then for all \(t\geq 1\) and \(\ell\in[L]\) such that \(\mathcal{X}_{t,\ell}\) exists, it holds \(\mathbf{x}_{t}^{*}\in\mathcal{X}_{t,\ell}\)._

Proof.: We prove the statement by induction. If \(\ell=1\), then \(\mathbf{x}_{t}^{*}\in\mathcal{X}_{t,\ell}=\mathcal{X}_{t}\) trivially holds. Suppose \(\mathbf{x}_{t}^{*}\in\mathcal{X}_{t,\ell}\) for some \(\ell\in\mathbb{Z}_{+}\) and \(\mathcal{X}_{t,\ell+1}\) exists, we have for all \(\mathbf{x}\in\mathcal{X}_{t,\ell}\) that

\[|\langle\mathbf{x},\bm{\theta}_{t,\ell}^{\text{\rm TS}}-\bm{ \theta}^{*}\rangle| \leq|\langle\mathbf{x},\widehat{\bm{\theta}}_{t,\ell}-\bm{\theta}^{* }\rangle|+|\langle\mathbf{x},\bm{\theta}_{t,\ell}^{\text{\rm TS}}-\widehat{ \bm{\theta}}_{t,\ell}\rangle|\] \[\leq(\sqrt{2d\log(4t^{2}L/\delta)}+1)\widehat{\beta}_{t,\ell}\| \mathbf{x}\|_{\mathbf{V}_{t,\ell}^{-1}}\] \[\leq 2^{-\ell}(\sqrt{2d\log(4t^{2}L/\delta)}+1)\widehat{\beta}_{t,\ell},\]

where the second inequality is due to the assumption \(\widetilde{\beta}_{t,\ell}\leq\widehat{\beta}_{t,\ell}\), Lemmas B.2 and B.3, and the last inequality is due to \(\|\mathbf{x}\|_{\mathbf{V}_{t,\ell}^{-1}}\leq 2^{-\ell}\) for all \(\mathbf{x}\in\mathcal{X}_{t,\ell}\). If we define \(\widetilde{\mathbf{x}}_{t}=\arg\max_{\mathbf{x}\in\mathcal{X}_{t,\ell}}\langle \mathbf{x},\bm{\theta}_{t,\ell}^{\text{\rm TS}}\rangle\), the it holds that

\[\langle\widetilde{\mathbf{x}}_{t}-\mathbf{x}_{t}^{*},\bm{\theta }_{t,\ell}^{\text{\rm TS}}\rangle \leq\langle\widetilde{\mathbf{x}}_{t}-\mathbf{x}_{t}^{*},\bm{ \theta}^{*}\rangle+\langle\widetilde{\mathbf{x}}_{t}-\mathbf{x}_{t}^{*},\bm{ \theta}_{t,\ell}^{\text{\rm TS}}-\bm{\theta}^{*}\rangle\] \[\leq|\langle\widetilde{\mathbf{x}}_{t},\bm{\theta}_{t,\ell}^{ \text{\rm TS}}-\bm{\theta}^{*}\rangle|+|\langle\mathbf{x}_{t}^{*},\bm{\theta }_{t,\ell}^{\text{\rm TS}}-\bm{\theta}^{*}\rangle|\] \[\leq 2\cdot 2^{-\ell}(\sqrt{2d\log(4t^{2}L/\delta)}+1)\widehat{ \beta}_{t,\ell},\]

where the second inequality follows from \(\langle\widetilde{\mathbf{x}}_{t}-\mathbf{x}_{t}^{*},\bm{\theta}^{*}\rangle\leq 0\). Hence, the optimal action \(\mathbf{x}_{t}^{*}\in\mathcal{X}_{t,\ell+1}\) and the statement follows from induction. 

**Lemma B.6**.: _Suppose that \(\widetilde{\beta}_{t,\ell}\leq\widehat{\beta}_{t,\ell}\), \(\mathcal{E}_{t,\ell}^{\bm{\theta}}\), and \(\mathcal{E}_{t,\ell}^{\text{\rm TS}}\) hold for all \(\ell\in[L]\) and \(t\in\Psi_{t+1,\ell}\). For any \(\ell\in[L]\backslash\{1\}\), we have_

\[\sum_{t\in\Psi_{T+1,\ell}}\Delta_{t}\leq 16d\cdot 2^{\ell}(\sqrt{2d\log(4T^{2}L/ \delta)}+1)\widehat{\beta}_{T,\ell-1}\cdot\log(1+2^{2\ell}T/d).\]Proof.: For all \(t\in\Psi_{T+1,\ell}\), we have \(\mathbf{x}_{t},\mathbf{x}_{t}^{*}\in\mathcal{X}_{t,\ell}\) following Lemma B.5 and the assumptions on \(\widetilde{\beta}_{t,\ell}\leq\widehat{\beta}_{t,\ell}\), \(\mathcal{E}_{t,\ell}^{\boldsymbol{\theta}}\), and \(\mathcal{E}_{t,\ell}^{\texttt{TS}}\). Due to Line 1 in Algorithm 3, it holds

\[\langle\mathbf{x}_{t}^{*},\boldsymbol{\theta}_{t,\ell-1}^{\texttt{TS}}\rangle- \langle\mathbf{x}_{t},\boldsymbol{\theta}_{t,\ell-1}^{\texttt{TS}}\rangle\leq 2 \cdot 2^{-\ell+1}(\sqrt{2d\log(4t^{2}L/\delta)}+1)\widehat{\beta}_{t,\ell-1}.\] (B.3)

In addition, for any \(t\in\Psi_{T+1,\ell}\), we have both \(\|\mathbf{x}_{t}\|_{\mathbf{V}_{t,\ell-1}^{-1}}\leq 2^{-\ell+1}\) and \(\|\mathbf{x}_{t}^{*}\|_{\mathbf{V}_{t,\ell-1}^{-1}}\leq 2^{-\ell+1}\). It follows that

\[\Delta_{t} \leq\langle\mathbf{x}_{t}^{*},\boldsymbol{\theta}_{t,\ell-1}^{ \texttt{TS}}\rangle-\langle\mathbf{x}_{t},\boldsymbol{\theta}_{t,\ell-1}^{ \texttt{TS}}\rangle+|\langle\mathbf{x}_{t}^{*},\boldsymbol{\theta}_{t,\ell-1}^ {\texttt{TS}}-\boldsymbol{\theta}^{*}\rangle|+|\langle\mathbf{x}_{t}, \boldsymbol{\theta}_{t,\ell-1}^{\texttt{TS}}-\boldsymbol{\theta}^{*}\rangle|\] \[\leq 2\cdot 2^{-\ell+1}(\sqrt{2d\log(4t^{2}L/\delta)}+1) \widehat{\beta}_{t,\ell-1}+2\cdot 2^{-\ell+1}(\sqrt{2d\log(4t^{2}L/\delta)}+1) \widehat{\beta}_{t,\ell-1}\] \[\leq 8\cdot 2^{-\ell}(\sqrt{2d\log(4t^{2}L/\delta)}+1) \widehat{\beta}_{t,\ell-1},\]

where the second inequality is due to (B.3), Lemmas B.2 and B.3. Summing over \(t\in\Psi_{T+1,\ell}\), we have

\[\sum_{t\in\Psi_{T+1,\ell}}\Delta_{t} \leq 8\cdot 2^{-\ell}(\sqrt{2d\log(4T^{2}L/\delta)}+1)\widehat{ \beta}_{T,\ell-1}|\Psi_{T+1,\ell}|\] \[\leq 8\cdot 2^{\ell}(\sqrt{2d\log(4T^{2}L/\delta)}+1)\widehat{ \beta}_{T,\ell-1}\cdot\sum_{t\in\Psi_{T+1,\ell}}\|w_{t}\mathbf{x}_{t}\|_{ \mathbf{V}_{t,\ell}^{-1}}^{2}\] \[\leq 8\cdot 2^{\ell}(\sqrt{2d\log(4T^{2}L/\delta)}+1)\widehat{ \beta}_{T,\ell-1}\cdot 2d\log(1+2^{2\ell}T/d),\]

where the second inequality is due to \(\|w_{t}\mathbf{x}_{t}\|_{\mathbf{V}_{t,\ell}^{-1}}=2^{-\ell}\) for all \(t\in\Psi_{T+1,\ell}\), and the last inequality is due to Lemma C.2. 

### Proof of Theorem 5.1

With probability at least \(1-2\delta\), all the statements of Lemmas B.2 and B.3 hold, and the rest of our proof will build on such an event. Let \(\ell_{0}=\lceil\frac{1}{2}\log_{2}\log(8(T+1)^{2}L/\delta)\rceil+8\), and we have \(\widetilde{\beta}_{t,\ell}\leq\widehat{\beta}_{t,\ell}\) for all \(t\in[T]\) and \(\ell\in[L]\backslash[\ell_{0}]\) following Lemma B.4. It then follows Lemma B.6 that for all \(\ell\in[L]\backslash[\ell_{0}]\)

\[\sum_{t\in\Psi_{T+1,\ell}}\Delta_{t} \leq\widetilde{\mathcal{O}}(d^{3/2}2^{\ell}\widehat{\beta}_{T, \ell-1})\] \[\leq\widetilde{\mathcal{O}}\left(d^{3/2}\sqrt{\sum_{t=1}^{T}w_{t }^{2}(r_{t}-\langle\mathbf{x}_{t},\widehat{\boldsymbol{\theta}}_{t+1,\ell} \rangle)^{2}+R^{2}+1}+R\right)\] \[\leq\widetilde{\mathcal{O}}\left(d^{3/2}\sqrt{\sum_{t=1}^{T} \sigma_{t}^{2}}+d^{3/2}R+d^{3/2}\right),\]

where the first inequality is due to Lemma B.6, the second inequality is due to the definition of \(\widehat{\beta}_{T,\ell-1}\), and the last inequality is due to Lemma B.4. For any layer \(\ell\in[\ell_{0}]\) and \(t\in\Psi_{T+1,\ell}\), we have

\[\sum_{t\in\Psi_{T+1,\ell}}\Delta_{t} \leq 2|\Psi_{T+1,\ell}|\] \[=2^{2\ell+1}\sum_{t\in\Psi_{T+1,\ell}}\|w_{t}\mathbf{x}_{t}\|_{ \mathbf{V}_{\ell,\ell}^{-1}}^{2}\] \[\leq\widetilde{\mathcal{O}}(d),\]

where the first inequality follows from \(\langle\mathbf{x},\boldsymbol{\theta}^{*}\rangle\leq 1\) for all \(\mathbf{x}\in\mathcal{X}_{t}\) and \(t\in[T]\), the equality is due to \(\|w_{t}\mathbf{x}_{t}\|_{\mathbf{V}_{t,\ell}^{-1}}=2^{-\ell}\) for all \(t\in\Psi_{T+1,\ell}\), and the last inequality is due to Lemma C.2 and \(2^{\ell_{0}}\lesssim\sqrt{\log(TL/\delta)}\).

For any \(t\in[T]\backslash(\cup_{\ell\in[L]}\Psi_{T+1,\ell})\), Algorithm 3 selects an arm at some \(\ell_{t}\)-th layer, and it follows that

\[\sum_{t\in[T]\backslash(\cup_{\ell\in[L]}\Psi_{T+1,\ell})}\Delta_{t} \leq\sum_{t\in[T]\backslash(\cup_{\ell\in[L]}\Psi_{T+1,\ell})}( \langle\mathbf{x}_{t}^{*},\bm{\theta}_{t,\ell_{t}}^{*}\rangle-\langle\mathbf{x }_{t},\bm{\theta}^{*}\rangle)\] \[\qquad\qquad+\sum_{t\in[T]\backslash(\cup_{\ell\in[L]}\Psi_{T+1, \ell})}\sqrt{2d\log(4t^{2}L/\delta)}+1)\widehat{\beta}_{t,\ell_{t}}\|\mathbf{ x}_{t}^{*}\|_{\mathbf{V}_{t,\ell_{t}}^{-1}}\] \[\leq\sum_{t\in[T]\backslash(\cup_{\ell\in[L]}\Psi_{T+1,\ell})}( \langle\mathbf{x}_{t},\bm{\theta}_{t,\ell_{t}}^{*}-\bm{\theta}^{*}\rangle+ \alpha(\sqrt{2d\log(4t^{2}L/\delta)}+1)\widehat{\beta}_{t,\ell_{t}})\] \[\leq\sum_{t\in[T]\backslash(\cup_{\ell\in[L]}\Psi_{T+1,\ell})}( \alpha\|\bm{\theta}_{t,\ell_{t}}^{*}-\bm{\theta}^{*}\|_{\mathbf{V}_{t,\ell_{ t}}}+\alpha(\sqrt{2d\log(4t^{2}L/\delta)}+1)\widehat{\beta}_{t,\ell_{t}})\] \[\leq\sum_{t\in[T]\backslash(\cup_{\ell\in[L]}\Psi_{T+1,\ell})}2 \alpha(\sqrt{2d\log(4t^{2}L/\delta)}+1)\widehat{\beta}_{t,\ell_{t}}\] \[\leq T\cdot\widetilde{\Theta}(\sqrt{d}/T)\] \[\leq\widetilde{\Theta}(\sqrt{d}),\]

where the first inequality is due to \(\widetilde{\beta}_{t,\ell}\leq\widehat{\beta}_{t,\ell}\), Lemmas B.2, B.3 and B.5, the second inequality is due to the arm selection rule of Algorithm 3, the third inequality is due to Cauchy-Schwarz inequality, and the fourth inequality is due to \(\widetilde{\beta}_{t,\ell}\leq\widehat{\beta}_{t,\ell}\), Lemmas B.2 and B.3, and the fifth inequality follows from the definition of \(\alpha\) and \(\widehat{\beta}_{t,\ell_{t}}\leq\widetilde{\Theta}(R\sqrt{T})\).

Combining the above inequalities together, we conclude that

\[\mathcal{R}(T) =\sum_{t=1}^{T}\Delta_{t}\] \[\leq\widetilde{\Theta}\left(d^{3/2}+d^{3/2}R+d^{3/2}\sqrt{\sum_{t =1}^{T}\sigma_{t}^{2}}\right),\]

where the inequality follows from \(L\lesssim\log T\) and \(\ell_{0}=\lceil\frac{1}{2}\log_{2}\log(8(T+1)^{2}L/\delta)\rceil+8\).

## Appendix C Auxiliary Lemmata

**Lemma C.1** (Abramowitz et al. (1964)).: _For a Gaussian distributed random variable \(X\) with mean \(m\) and variance \(\sigma^{2}\), it holds that for any \(x\geq 1\) that_

\[\frac{1}{2\sqrt{\pi}x}\exp(-x^{2}/2)\leq\mathbb{P}(|X-m|>x\sigma)\leq\frac{1} {\sqrt{\pi}x}\exp(-x^{2}/2).\]

**Lemma C.2** (Abbasi-Yadkori et al. (2011), Lemma 11).: _Let \(\{\mathbf{X}_{t}\}_{t=1}^{\infty}\) be a sequence in \(\mathbb{R}^{d}\), \(\mathbf{V}\in\mathbb{R}^{d\times d}\) positive definite matrix, and \(\overline{\mathbf{V}}_{t}=\mathbf{V}+\sum_{s=1}^{t}\mathbf{X}_{s}\mathbf{X}_ {s}^{\intercal}\). Then we have that_

\[\log\left(\frac{\det(\overline{\mathbf{V}}_{n})}{\det(\mathbf{V})}\right)\leq \sum_{t=1}^{n}\|\mathbf{X}_{t}\|_{\overline{\mathbf{V}}_{t-1}}^{2}.\]

_Further, if \(\|\mathbf{X}_{t}\|_{2}\leq L\) for all \(t\), then_

\[\sum_{t=1}^{n}\min\left\{1,\|\mathbf{X}_{t}\|_{\overline{\mathbf{ V}}_{t-1}^{-1}}^{2}\right\} \leq 2(\log\det(\overline{\mathbf{V}}_{n})-\log\det(\mathbf{V}))\] \[\leq 2(d\log((\operatorname{Tr}(\mathbf{V})+nL^{2})/d)-\log\det( \mathbf{V})),\]

_and finally, if \(\lambda_{\min}(\mathbf{V})\geq\max(1,L^{2})\) then_

\[\sum_{t=1}^{n}\|\mathbf{X}_{t}\|_{\overline{\mathbf{V}}_{t-1}^{-1}}^{2}\leq 2 \log\frac{\det(\overline{\mathbf{V}}_{n})}{\det(\mathbf{V})}.\]Numerical Results

In our simulation, we test our algorithm on a linear contextual bandit problem with different sets of reward perturbations. Specifically, the simulation is carried out over \(T=2000\) rounds with an ambient dimension of \(d=25\). For each selection round \(t\in[T]\), the environment yields a set of contexts \(\mathcal{X}_{t}\), which contains \(K=50\) context vectors. The context vectors are randomly drawn and structured as truncated Gaussian vectors, ensuring their norms remain bounded by \(1\). Within each trial, a random ground truth vector \(\bm{\theta}^{*}\) is generated as another Gaussian vector. The reward associated with every context vector is then computed based on the expected reward, perturbed by Gaussian noise. We employ our proposed algorithms, LinVDTS and LinNATS, to scrutinize their performance within this configuration.

Figure Fig. 1 presents the cumulative regrets of the LinVDTS and LinNATS algorithms, using the vanilla TS as a benchmark. The algorithms are evaluated in two distinct bandit environments: one characterized by constant variance and the other by a quadratically decaying variance. Performance under the constant variance setup is captured by dashed lines, whereas the decaying variance scenario is depicted by solid lines, with the observations being grounded on \(50\) randomized trials. It is noteworthy that while LinVDTS is privy to the ground truth variance, LinNATS operates without this knowledge in our experimental framework. This distinction underscores LinNATS's commendable adaptability in environments where the variance is elusive. Given its access to variance data, the superior performance of LinVDTS vis-a-vis LinNATS in fluctuating variance situations is anticipated. The presented outcomes showcase the variance-awareness of our proposed algorithms and the inherent flexibility of these algorithms in modulating their regret according to the prevailing variance dynamics.

Future directions and broader impact.The recent progress in understanding the training dynamics of deep neural networks (Jacot et al., 2018; Song et al., 2021) has paved the way for investigations at the intersection of deep learning and reinforcement learning (Lillicrap et al., 2015; Chen et al., 2021; Xu et al., 2021). This integrative approach promises to yield transformative insights and substantially enhance the performance of existing learning architectures. Further, the incorporation of non-linear function approximation into our frameworks emerges as a pivotal research direction,

Figure 1: Performance comparison of LinVDTS, LinNATS, and the vanilla TS algorithms on the linear contextual bandit problem. The blue (LinVDTS), orange (LinNATS), and green (vanilla TS) lines depict the average regret for each method. Dashed lines represent the average regret in a constant variance scenario, while solid lines indicate the average regret under diminishing noise variance. Confidence bands, plotted for each method, are derived from 50 random trials.

with the potential to significantly improve their robustness (Ling et al., 2019; Chen et al., 2020; Min et al., 2021; Ye et al., 2023). We believe that engaging in these research endeavors is important for the refinement of our methodologies.