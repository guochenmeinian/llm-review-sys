# NICE: NoIse-modulated Consistency rEquarization

for Data-Efficient GANs

 Yao Ni\({}^{\dagger}\), Piotr Koniusz\({}^{\star,\lx@sectionsign,\dagger}\)

\({}^{\dagger}\)The Australian National University \({}^{\lx@sectionsign}\)Data6l**PC**SIRO

\({}^{\dagger}\)firstname.lastname@anu.edu.au

The corresponding author. Code: https://github.com/MaxwellYaoNi/NICE

###### Abstract

Generative Adversarial Networks (GANs) are powerful tools for image synthesis. However, they require access to vast amounts of training data, which is often costly and prohibitive. Limited data affects GANs, leading to discriminator overfitting and training instability. In this paper, we present a novel approach called NoIse-modulated Consistency rEgalization (NICE) to overcome these challenges. To this end, we introduce an adaptive multiplicative noise into the discriminator to modulate its latent features. We demonstrate the effectiveness of such a modulation in preventing discriminator overfitting by adaptively reducing the Rademacher complexity of the discriminator. However, this modulation leads to an unintended consequence of increased gradient norm, which can undermine the stability of GAN training. To mitigate this undesirable effect, we impose a constraint on the discriminator, ensuring its consistency for the same inputs under different noise modulations. The constraint effectively penalizes the first and second-order gradients of latent features, enhancing GAN stability. Experimental evidence aligns with our theoretical analysis, demonstrating the reduction of generalization error and gradient penalization of NICE. This substantiates the efficacy of NICE in reducing discriminator overfitting and improving stability of GAN training. NICE achieves state-of-the-art results on CIFAR-10, CIFAR-100, ImageNet and FFHQ datasets when trained with limited data, as well as in low-shot generation tasks.

## 1 Introduction

The remarkable advancements and breakthroughs in deep learning [31] can be largely attributed to the extensive utilization of vast amounts of training data. This abundance of data has driven the progress across various domains of deep learning. Among notable recent advancements are Generative Adversarial Networks (GANs) [13], popular in industry and academia. GANs have proven their high-quality image generation abilities and achieved high generation speeds [47], establishing them as a versatile tool for a wide range of applications, such as text-to-image generation [20, 47, 55], destylization [48, 49, 50], image-to-image translation [27, 44], and 3D generation [52, 53, 65].

Despite the impressive capabilities of state-of-the-art GANs in generating diverse and high-quality images [7, 25], their effectiveness heavily relies on large volumes of training data. The acquisition of such large datasets helps GANs attain the desired adversarial equilibrium. However, under limited training data regime, GANs encounter challenges associated with discriminator overfitting and unstable training [22, 68, 57, 11, 17].

To address the aforementioned challenges, recent studies have approached the problem from three perspectives. The first perspective involves the utilization of extensive differentiable data augmentation techniques, aimed at expanding the distribution of the available data [22, 68, 17, 32, 63, 64, 69]. The second perspective leverages knowledge gained from large-scale models trained on large datasets [9, 70, 29]. While these approaches are effective, they also carry inherent risks, such as the leakage of augmentation clues [64, 22, 40] or pre-trained knowledge [30].

The third perspective focuses on the regularization of the discriminator, aiming to weaken its learning ability [11; 19] or increase the overlap between real and fake support sets [57]. In line with this perspective, our paper introduces a novel approach to improve GAN generalization. We propose to modulate the hidden features of discriminator via adaptive multiplicative noise, which strikes a balance in maintaining a certain level of discrimination ability while also regularizing the Rademacher complexity [6] of the discriminator. This reduction in Rademacher complexity, which quantifies the capacity of model to fit random variables, narrows the generation gap between the training and unseen data, resulting in enhanced GAN generalization [4; 56; 18].

Nevertheless, training the discriminator with adaptive noise unintentionally amplifies second-order gradient derivative of latent features corresponding to real images. This elevated gradient leads to abrupt gradient changes near the real sample points, potentially causing instability in the feedback to the generator. This issue aligns with the findings in works [26; 36; 56; 11], which emphasize the importance of penalizing gradients for both real and fake samples to promote convergence and stability of GAN training. To address these challenges, we propose a constraint on the discriminator that ensures consistency for the same inputs under different noise modulations. While our idea is simple, our theoretical analysis reveals that this constraint effectively penalizes the first and second-order gradients of the latent features. Consequently, the gradient provided to the generator becomes more stable, resulting in improved training stability and generalization.

Our comprehensive experiments confirm the effectiveness of NICE in penalizing gradients and reducing the generalization gap. Despite the simple design, NICE significantly improves the training stability and generalization of GANs, outperforming alternative approaches in preventing discriminator overfitting. NICE achieves superior results on challenging limited data benchmarks, including CIFAR-10/100, ImageNet, FFHQ, and low-shot image generation tasks.

Our contributions can be summarized as follows:

1. We limit discriminator overfitting by using the adaptive multiplicative noise to modulate the latent features of the discriminator, resulting in enhanced generalization of GANs.
2. We introduce NICE, a technique that enforces the discriminator to be consistent with the same inputs under different noise modulations, implicitly penalizing the first and second-order gradients of the latent features during GAN training, promoting the training stability.
3. We show that NICE, both in theory and practice, effectively prevents discriminator overfitting and achieves superior performance in image generation under limited data setting.

## 2 Related Work

**Improving GANs.** Generative Adversarial Networks [13] are powerful generative models that excel in image generation [21; 24; 25], text-to-image generation [20; 47; 55], image-to-image translation [27; 44], and 3D generation [52; 65; 53]. However, GANs commonly encounter challenges such as training instability [26], mode collapse [45], and discriminator overfitting [22]. Researchers have investigated different GAN loss functions [2; 42; 71], architectures [21; 24; 25; 23], and regularization strategies [38; 33; 14]. The \(f\)-GAN [42] generalizes GANs to \(f\)-divergences. WGAN [2] adopts the Earth-Mover distance. OmniGAN [71] extends conditional GANs to a multi-label softmax loss. StyleGANV1-3 [24; 25; 23] enhances the architecture of generator. Approaches [14; 26; 36; 56; 11] propose explicit penalty of gradient of discriminator. SNGAN [38] enforces a Lipschitz constraint on the discriminator, and approach [33] regularizes the spectral norm of the discriminator. In this study, we propose a novel regularization strategy applicable to diverse GAN loss functions and architectures, specifically tailored for training GAN models with limited data.

**Image generation with limited data.** Collecting data is both laborious and expensive. Consequently, generating images in limited data settings presents significant challenges, primarily due to discriminator overfitting. Previous approaches have tackled this issue through data augmentation techniques [22; 68; 17; 32; 63] and leveraging knowledge transfer from large-scale pre-trained models [9; 29; 70]. Differentiable data augmentation methods [22; 68; 17] "extend" the data distribution, while methods [63; 32] explore contrastive learning within GANs. Approaches [9; 29; 70] employ pre-trained models to guide training of discriminator. However, both GAN types suffer issues, including leakage of augmentation clues [64; 22] or pre-trained knowledge [30]. An alternative approach involves regularizing the discriminator. LeCamGAN [57] suggests reducing the output gap of discriminator between real and fake distributions. LCSAGAN [40] employs manifold techniques to project discriminator features onto manifold and decrease the capacity of discriminator. APA [19]uses fake images as real images to increase overlap between the real and fake distributions. DigGAN [11] minimizes the gradient norm gap between real and fake images. Inspired by such an approach, we propose to use an adaptive noise to modulate the latent features of discriminator to reduce the generalization error of discriminator as a means of regularization.

**Consistency regularization.** CTGAN [61] introduces the enforcement of a Lipschitz constraint by ensuring the consistency of the response of discriminator to real images. CAGAN [41] and GenCo [8] enforce consistency among multiple discriminators. R-Drop [62] applies consistency regularization to transformers [58] equipped with dropout for natural language processing tasks. Augmentation-based consistency regularization GANs [64, 69] enforce consistency by considering different augmented views of the same image, although they may inadvertently leak augmentation clues to the generator [22]. Despite the effectiveness of consistency regularization demonstrated by these previous works, their success is primarily empirical, lacking theoretical analysis. In our study, we go beyond prior works by providing a theoretical analysis of consistency regularization in GANs. By delving deeper into the underlying principles, we develop a more comprehensive understanding of the mechanism behind its effectiveness.

**Generalization of GANs.** Arora _et al_. [4, 56, 67, 18] have contributed to the understanding and improvement of generalization of GANs by showing the importance of achieving adversarial equilibrium, reducing discriminator discrepancy, penalizing gradients, and bounding the generalization error. Our motivation aligns with these works in the pursuit of enhancing generalization capabilities. However, our approach implicitly and adaptively reduces the generalization gap while penalizing gradients of latent features of discriminator. Such a setting provides an efficient and effective means of preventing discriminator overfitting and improving generalization.

## 3 Method

To boost the generalization of GAN, we start by analyzing their generalization error, bounding it with the Rademacher complexity of discriminator, and linking it with the weight norm. By incorporating multiplicative noise, we demonstrate its regularization benefits on the weight norm. However, such a strategy induces large gradient norm destabilizing training of GAN. To this end, we introduce NICE to penalize the gradients of discriminator. Finally, we provide NICE and showcase it use in GANs.

### Generalization error of GANs and complexity of neural network

The primary goal of GAN is to minimize the integral probability metric [39], assuming access to infinite real and fake data during optimization, _i.e._, the infinite real and generated distributions \((\mu,\nu)\) as discussed in [67]. In practice, we often have limited access to a finite dataset \(\hat{\mu}_{n}\) of size \(n\). Consequently, our optimization is restricted to the empirical loss:

\[\inf_{\nu\in\mathcal{G}}\Big{\{}d_{\mathcal{H}}(\hat{\mu}_{n},\nu):=\sup_{h\in \mathcal{H}}\{\mathbb{E}_{\bm{x}\sim\hat{\mu}_{n}}[h(\bm{x})]-\mathbb{E}_{\bm {\tilde{x}}\sim\nu}[h(\tilde{\bm{x}})]\}\Big{\}}.\] (1)

Function sets of discriminator and generator, \(\mathcal{H}\) and \(\mathcal{G}\), are typically parameterized in GAN as neural network classes \(\mathcal{H}_{\mathsf{nn}}=\{h(\bm{x};\bm{\theta}_{d}):\bm{\theta}_{d}\in\bm{ \Theta}_{d}\}\) and \(\mathcal{G}_{\mathsf{nn}}=\{g(\bm{z};\bm{\theta}_{g}):\bm{\theta}_{g}\in\bm{ \Theta}_{g}\}\) where \(\bm{z}\sim p_{z}\) serves as the random noise input to the generator. The associated term \(d_{\mathcal{H}_{\mathsf{nn}}}\) is referred to as the neural network distance [4]. The discriminator network \(D:=\varphi\circ f\) consists of a real/fake prediction head \(\varphi\) and a feature extractor \(f\). As the loss function \(\phi(\cdot)\) varies across tasks, architectures or choice of divergence type, we compose it with \(D\)[2, 67, 4] to simplify the analysis and notation, _i.e._, \(h(\cdot):=\phi(D(\cdot))\). Thus, the alternative optimization of discriminator and generator becomes:

\[\begin{cases}L_{D}=\min_{\bm{\theta}_{d}}\mathbb{E}_{\tilde{\bm{x}}\sim\nu_{n }}[h(\tilde{\bm{x}};\bm{\theta}_{d})]-\mathbb{E}_{\bm{x}\sim\hat{\mu}_{n}}[h( \bm{x};\bm{\theta}_{d})],\\ L_{G}=\min_{\bm{\theta}_{g}}-\mathbb{E}_{\bm{z}\sim p_{\bm{z}}}[h(g(\bm{z};\bm{ \theta}_{g}))],\end{cases}\] (2)

where we assume \(\nu_{n}\) minimizes \(d_{\mathcal{H}}(\hat{\mu}_{n},\nu)\) up to precision \(\epsilon\geq 0\), meaning that \(d_{\mathcal{H}}(\hat{\mu}_{n},\nu_{n})\leq\inf_{\nu\in\mathcal{G}}d_{\mathcal{ H}}(\hat{\mu}_{n},\nu)+\epsilon\). As we are interested in how close the generator distribution \(\nu_{n}\) is to the unknown infinite distribution \(\mu\), we refer to the lemma [67] on the generalization error of GAN:

**Lemma 1**: _(Theorem 3.1 of [67]) Assume that the discriminator set \(\mathcal{H}\) is even (\(h\in\mathcal{H}\) implies \(-h\in\mathcal{H}\)) and all discriminators are bounded by \(\|h\|_{\infty}\leq\Delta\). Let \(\hat{\mu}_{n}\) be an empirical measure of an i.i.d. sample of size \(n\) drawn from \(\mu\). Assume \(d_{\mathcal{H}}(\hat{\mu}_{n},\nu_{n})-\inf_{\nu\in\mathcal{G}}d_{\mathcal{H}}( \hat{\mu}_{n},\nu)\leq\epsilon\). Then with probability at least \(1-\delta\), we have:_

\[d_{\mathcal{H}}(\mu,\nu_{n})-\inf_{\nu\in\mathcal{G}}d_{\mathcal{H}}(\mu,\nu) \leq 2\sup_{h\in\mathcal{H}}\Big{|}\mathbb{E}_{\mu}[h]-\mathbb{E}_{\hat{\mu} _{n}}[h]\Big{|}+\epsilon\leq 2R_{n}^{(\mu)}(\mathcal{H})+2\Delta\sqrt{\frac{2 \log(1/\delta)}{n}}+\epsilon,\] (3)_where the Rademacher complexity [6], \(R_{n}^{(\mu)}(\mathcal{H}):=\mathbb{E}\big{[}\sup_{h\in\mathcal{H}}\frac{2}{n}\sum_ {i}\tau^{(i)}h(\bm{x}^{(i)})\big{]}\), measures how well the function \(h\in\mathcal{H}\) fits the Rademacher random variable \(\tau^{(i)}\) with \(\text{prob}(\tau^{(i)}=1)=\text{prob}(\tau^{(i)}=-1)=\frac{1}{2}\) given samples \(\bm{x}^{(i)}\sim\hat{\mu}_{n}\)._

Lemma 1 provides a crucial insight that one can assess the generalization error of GAN by comparing the output discrepancy of discriminator between training data and unseen data, and such an error is influenced by the Rademacher complexity of the discriminator. To enhance the performance of generator while reducing the generalization error, we have two possibilities: 1) increase the quantity \(n\) of real data, which provides a stronger foundation for training a better generator; 2) reduce the Rademacher complexity of the discriminator. However, this reduction must be carefully controlled, as an overly simplified discriminator may struggle to effectively distinguish real and fake data.

To manage the Rademacher complexity of the discriminator, we leverage a theorem from approach [5] to establish an upper bound on the Rademacher complexity of the neural network.

**Lemma 2**: _(Eq. 1.2 in [5], Theorem 5.20 in [35]) Consider a fully-connected neural network \(v_{\bm{\theta}}(\bm{x})=\bm{W}_{t}\sigma(\bm{W}_{t-1}\sigma(...\sigma(\bm{W}_ {1}\bm{x})...))\) where \(\bm{W}_{i}\) are linear weights at \(i\)-th layer and \(\sigma\) is a 1-Lipschitz activation function. Suppose that \(\forall i\in\{1,...,n\}\), \(\|\bm{x}^{(i)}\|_{2}\leq q\). Let \(\|\bm{W}_{i}\|_{lip}\) be the lipschitz constant of \(\bm{W}_{i}\) and \(\|\bm{W}_{i}^{T}\|_{2,1}\) be the sum of the \(l_{2}\) norm of columns in \(\bm{W}_{i}\). Let \(\mathcal{V}=\{v_{\bm{\theta}}:\|\bm{W}_{i}\|_{lip}\leq k_{i},\|\bm{W}_{i}^{T}\| _{2,1}\leq b_{i}\}\), we have the Rademacher complexity:_

\[R_{n}^{(\mu)}(\mathcal{V})\leq\frac{q}{\sqrt{n}}:\bigg{(}\prod_{i=1}^{t}k_{i} \bigg{)}\cdot\bigg{(}\sum_{i=1}^{t}\frac{b_{i}^{2/3}}{k_{i}^{2/3}}\bigg{)}^{3/ 2}.\] (4)

Lemma 2 links the Rademacher complexity of a neural network to the Lipschitz constant and the (2,1)-norm of its weights, providing essential insights for controlling the complexity of discriminator.

### Improving generalization by feature modulation with multiplicative noise

Taking into account Lemmas 1 and 2, one can effectively manage the generalization error of GAN by controlling the Rademacher complexity of the discriminator. Such a control requires managing the Lipschitz constant and the norm of the weights of discriminator. Typically, controlling the Lipschitz constant can be achieved by the spectral normalization [38] or the gradient penalty [14]. In contrast, we propose a novel approach: regularizing the norm of the weights by modulating the latent features of the discriminator with multiplicative noise.

We build on the analysis (Prop. 4 in Appendix of [3]) of Dropout regularization [54] by exploring the regularization effect of the Gaussian multiplicative noise within the context of deep regression.

**Theorem 1**: _(Regularization by the Gaussian multiplicative noise in deep regression) Consider a regression task on a two-layer neural network. Let \(\mathcal{X}\!\subseteq\!\mathbb{R}^{d_{0}}\) and \(\mathcal{Y}\!\subseteq\![-1,1]^{d_{2}}\) be the input and output spaces, and \(\mathcal{D}\!=\!\mathcal{X}\!\times\!\mathcal{Y}\). Let \(n\) examples \(\{(\bm{x}^{(i)},\bm{y}^{(i)})\}_{i=1}^{n}\!\sim\!\mathcal{D}^{n}\). Let \(f_{w}\!:\!\mathcal{X}\to\mathcal{Y}\) be parameterized by \(\{w\!:\!\bm{W}_{1}\!\in\!\mathbb{R}^{d_{1}\times d_{0}},\bm{W}_{2}\!\in\! \mathbb{R}^{d_{2}\times d_{1}}\}\) and \(f_{w}(\bm{x},\bm{z})\!=\!\bm{W}_{2}(\bm{z}\odot\sigma(\bm{W}_{1}\bm{x}))\) where \(\sigma\) is 1-Lipschitz activation function, \(\odot\) denotes element-wise multiplication and \(\bm{z}\!\sim\!\mathcal{N}(\bm{1},\beta^{2}\mathcal{I}^{d_{1}})\). Let \(\bm{a}\!=\!\sigma(\bm{W}_{1}\bm{x})\), \(\hat{\bm{a}}\!=\!\hat{\mathbb{E}}_{i}[\bm{a}^{(i)}\odot\bm{a}^{(i)}]\), \(\hat{a}_{k}\!\geq\!0\) denotes the \(k\)-th element of \(\hat{\bm{a}}\). Let \(\|\bm{W}_{2}\|_{2,1}\!=\!\sum_{k}\|\bm{w}_{k}\|_{2}\) where \(\bm{w}_{k}\) is the \(k\)-th column vector in \(\bm{W}_{2}\). The regression task with the \(l_{2}\) loss leads to the weight regularization as:_

\[\hat{L}_{\text{noise}}(w):=\hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{z}}\big{[}\|\bm{ y}^{(i)}-\bm{W}_{2}(\bm{z}\odot\bm{a}^{(i)})\|_{2}^{2}\big{]}=\hat{\mathbb{E}}_{i} \big{[}\|\bm{y}^{(i)}-\bm{W}_{2}\bm{a}^{(i)}\|_{2}^{2}\big{]}+\beta^{2}\sum_{ k}\hat{a}_{k}\|\bm{w}_{k}\|_{2}^{2}.\] (5)

The proof can be found in SSC.1. Theorem 1 reveals that modulation with Gaussian multiplicative noise in the regression task implicitly regularizes the internal weight norm, with the regularization strength determined by the variance of the noise \(\beta^{2}\) and the magnitude of the features.

Despite analysis in a simplified two-layer system, the implicit weight regularization by the noise modulation applies to multi-layer and convolutional neural networks, which can be expressed as a combination of two-layer nets, and convolutional layers are a type of linear layer [37; 60; 3]. Assuming latent features in deep neural networks follow a Gaussian distribution [34], we can equate the Bernoulli noise modulation (Dropout) with \(\bm{Z}\sim\mathcal{B}(\beta)\), whose values are set to be \(1/(1-\beta)\) with probability \(1-\beta\) and 0 otherwise, to the Gaussian multiplicative noise modulation in subsequent layers, making the regularization effects from Theorem 1 applicable to the Bernoulli noise as well.

Theorem 1 illustrates that one can modulate the latent features in the discriminator by the multiplicative noise to adaptively regularize the norm of its weights, leading to the reduced Rademacher complexity in Lemma 2 and improved GANs generalization in Lemma 1.

### Consistency regularization

Although the noise modulation can reduce the Rademacher complexity, incorporating the noise increases the gradient norms of latent features and the inputs to the discriminator, making training of GAN unstable [56] and difficult to converge [36]. To understand the gradient-related challenges in the noise-modulated discriminator, we adopt the Taylor expansion with a focus on the pivotal first- and second-order terms, following standard practice [1, 12, 66].

**Proposition 1**: _Define \(f:=f_{t}\circ\ldots\circ f_{2}\) as the feature extractor of the discriminator from the second layer onward. Let \(\tilde{\bm{a}}=f_{1}(\tilde{\bm{x}})\) and \(\bm{a}=f_{1}(\bm{x})\) be the latent features of the first layer for fake and real images, respectively. The discriminator is defined as \(h(\cdot)=\phi(\varphi(f(\cdot)))\), with \(\varphi\) as the prediction head and \(\phi\) as the loss function. Introduce a multiplicative noise \(\bm{z}\sim\mathcal{N}(\bm{1},\beta^{2}\bm{I}^{d})\) modulating solely the first layer. Let \(H^{(h)}_{kk}(\bm{a})\) be the \(k\)-th diagonal entry of the Hessian matrix of \(h\) at \(\bm{a}\) and \(a_{k}\) be the \(k\)-th element of \(\bm{a}\). Applying Taylor expansion to \(h(\cdot)\), the GAN loss can be expressed as follows:_

\[\min_{\bm{\theta}_{d}}L^{\text{AN}}_{D}\mathrel{\mathop{:}}= \mathbb{E}_{\tilde{\bm{a}}}\mathbb{E}_{\bm{z}}\big{[}h(\bm{z} \odot\tilde{\bm{a}})\big{]}-\mathbb{E}_{\bm{a}}\mathbb{E}_{\bm{z}}\big{[}h( \bm{z}\odot\bm{a})\big{]}\] \[\approx \mathbb{E}_{\tilde{\bm{a}}}\left[h(\tilde{\bm{a}})\right]\!-\! \mathbb{E}_{\bm{a}}\big{[}h(\bm{a})\big{]}\!+\,\tfrac{\beta^{2}}{2}\big{(} \mathbb{E}_{\tilde{\bm{a}}}\big{[}\sum_{k}\tilde{a}_{k}^{2}H^{(h)}_{kk}( \tilde{\bm{a}})\big{]}\ -\ \mathbb{E}_{\bm{a}}\big{[}\sum_{k}a_{k}^{2}H^{(h)}_{kk}( \tilde{\bm{a}})\big{]}\big{)}\,\] (6) \[\min_{\bm{\theta}_{g}}L^{\text{AN}}_{G}\mathrel{\mathop{:}}= -\mathbb{E}_{\tilde{\bm{x}}}\mathbb{E}_{\tilde{\bm{a}}}\big{[}h( \bm{z}\odot\tilde{\bm{a}})\big{]}\approx-\mathbb{E}_{\tilde{\bm{a}}}\big{[}h( \tilde{\bm{a}})\big{]}\ -\ \tfrac{\beta^{2}}{2}\mathbb{E}_{\tilde{\bm{a}}}\big{[}\sum_{k}\tilde{a}_{k}^{2} H^{(h)}_{kk}(\tilde{\bm{a}})\big{]}\.\] (7)

[left=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,rightright=0pt,right=0pt,right=0pt,right=0pt,right=0pt,rightright=0pt,rightright=0pt,right=0pt,right=0pt,right=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,right=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,right=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightrightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightrightright=0pt,rightright=0pt,rightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightrightright=0pt,rightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightrightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightrightright=0pt,rightrightrightright=0pt,rightrightrightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightrightright=0pt,rightrightrightright=0pt,rightrightrightright=0pt,rightrightrightright=0pt,rightrightright=0pt,rightrightrightright=0pt,rightrightrightrightright=0pt,rightrightrightrightright=0pt,rightrightrightright=0pt,rightrightrightright=0pt,rightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightright=0pt,rightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightright=0pt,where  represents the operation that expands \(\bm{Z}\) into \(1\!\times\!d^{\prime}\!\times\!d^{H}\!\times\!d^{W}\) shape and performs element-wise multiplication with \(\bm{X}\). The noise \(\bm{Z}\) is carefully controlled through an adaptive \(\beta\). In the case of BigGAN and OmniGAN, we control the variance of \(\bm{Z}\) using \(\beta\) as \(\bm{Z}\sim\mathcal{N}(\bm{1},\beta^{2}\bm{I}^{d^{\prime}})\). For StyleGAN2, we control the noise through \(\bm{Z}\sim\mathcal{B}(\beta)\), where with probability \(1-\beta\) it takes the value \(\frac{1}{1-\beta}\), and with probability \(\beta\) it takes 0.

Below we introduce a mechanism to control the noise via the meta-parameter \(\beta\) by detecting potential overfitting in the discriminator. Firstly, we compute the expectation over the discriminator output \(r(\bm{x})=\mathbb{E}[\text{sign}(D(\bm{x}))]\) w.r.t. real samples \(\bm{x}\), and evaluate \(\varepsilon=\text{sign}(r(\bm{x})\!>\!\eta)\!\in\!\{-1,0,1\}\) where \(\eta\) is a fixed threshold. A value greater than the threshold indicates potential overfitting [22]. We apply \(\beta_{t+1}=\beta_{t}+\Delta_{\beta}\cdot\varepsilon\) to update \(\beta\) with smaller \(\Delta_{\beta}\) to control \(\bm{Z}\). Denote the modified discriminator with our adaptive noise as \(h_{\text{AN}}\), the objective of GAN with the adaptive noise modulation becomes:

\[\begin{cases}L_{D}^{\text{AN}}=\min_{\bm{\theta}_{d}}\mathbb{E}_{\bm{\tilde{ x}}\sim\nu_{n}}[h_{\text{AN}}(\tilde{\bm{x}};\bm{\theta}_{d})]-\mathbb{E}_{\bm{x} \sim\tilde{\mu}_{n}}[h_{\text{AN}}(\bm{x};\bm{\theta}_{d})],\\ L_{G}^{\text{AN}}=\min_{\bm{\theta}_{g}}-\mathbb{E}_{\bm{z}\sim\rho_{z}}[h_{ \text{AN}}(g(\bm{z};\bm{\theta}_{g}))].\end{cases}\] (10)

**GAN with the noise-modulated consistency regularization (NICE).** To deal with the increased gradient norm due to the adaptive noise modulation, we introduce the consistency regularization (NICE) to promote invariance of the discriminator to the same samples under different noise modulations. Our regularization implicitly penalizes the gradient of the latent features in the discriminator by comparing outputs \(f_{1}(\cdot)\) and \(f_{2}(\cdot)\) of two feature extractors (with shared parameters) that are subjected to different noise modulations as in Figure 1. The objective for a GAN with NICE is:

\[\begin{cases}L_{D}^{\text{NICE}}=\min_{\bm{\theta}_{d}}\mathbb{E}_{\bm{\tilde{ x}}\sim\nu_{n}}[h_{\text{AN}}(\tilde{\bm{x}};\bm{\theta}_{d})+\gamma \varepsilon^{\text{NICE}}(\tilde{\bm{x}})]+\mathbb{E}_{\bm{x}\sim\tilde{\mu} _{n}}[-h_{\text{AN}}(\bm{x};\bm{\theta}_{d})+\gamma\varepsilon^{\text{NICE}}( \bm{x})],\\ L_{G}^{\text{NICE}}=\min_{\bm{\theta}_{g}}\mathbb{E}_{\bm{z}\sim\rho_{z}}[-h_{ \text{AN}}(g(\bm{z};\bm{\theta}_{g}))+\gamma\varepsilon^{\text{NICE}}(g(\bm{ z};\bm{\theta}_{g}))],\end{cases}\] (11)

where \(\varepsilon^{\text{NICE}}(\cdot)\!=\!\|f_{1}(\cdot)\!-\!f_{2}(\cdot)\|_{2}^{2}\) and \(\gamma\!=\!\Delta_{\gamma}\beta\) is a meta-parameter that controls the strength of the consistency regularization and is adaptively determined by \(\beta\).

**Efficient NICE.** In our experiments, we observed that applying the noise modulation in the later blocks of the discriminator often outperformed applying noise in the earlier blocks. To avoid unnecessary computations (the early blocks do not use modulation), we divide the discriminator into a modulation-free part (\(B_{1},\ldots,B_{l-1}\)) and a noise-modulated part (\(B_{l},\ldots,B_{L}\)), where \(l\) indicates the starting block of noise modulation. For efficient calculation of the consistency loss, we input two copies of \(\bm{X}_{l-1}\) (output from the first part) into the second part, bypassing the need for redundant calculations to maintain a low computational overhead compared to standard approaches.

## 4 Experiments

We conduct experiments on CIFAR-10/100 [28] using BigGAN [7] and OmniGAN [71], as well as on ImageNet [10] using BigGAN for conditional image generation. We also evaluate our method on low-shot datasets [68], which include 100-shot Obama/Panda/Grumpy Cat and AnimalFace Dog/Cat [51], and FFHQ [24] using StyleGAN2 [25]. We compare our method against several strong baselines, including DA [68], ADA [22], DigGAN [11], MaskedGAN [17], KDDLGAN [9], LeCam [57], GenCo [8], InsGen [63], FakeCLR [32] and TransferGAN [59]. For fair comparison, we denote methods using massive augmentation as "MA", which include DA and ADA.

**Datasets.** CIFAR-10 has 50K/10K training/testing images with resolution of \(32\!\times\!32\) from 10 categories, whereas CIFAR-100 has 100 classes. FFHQ contains 70K human face images at \(256\!\times\!256\)

Figure 1: Our discriminator pipeline. NICE is applied to both real \(\bm{x}\) and fake \(\tilde{\bm{x}}\) images.

pixels. Low-shot datasets contain 100-shot Obama/Panda/Grumpy Cat images, AnimalFace (160 cats and 389 dogs) images at \(256\times 256\) resolution. ImageNet has 1.2M/50K training/validation images with 1K categories. Following [17; 9], we center-crop and downscale its images to \(64\times 64\) resolution. The implementation details can be found in SSD. The generated images can be found in SSD.

**Evaluation metrics.** We generate 50K images per dataset to compute the commonly used Inception Score [46] and Frechet Inception Distance (FID) [15]. We report tFID, computed between 50K generated images and all training images. For CIFAR-10/100, we also compute vFID between 10K generated images and 10K real testing images. For low-shot datasets, we follow [68] and compute FID between 5K generated images and the entire dataset. For FFHQ, we calculate FID between 50K fake images and the entire training set. For ImageNet, we follow [17; 9] and generate 10K images for computing the IS and FID, where the reference distribution is the entire training set. Following [68; 11; 32], we run 5 trails for methods using NICE, and report the mean of the results. Given that all standard deviations fall below the 1% relative, we omit them for clarity.

### Results on CIFAR-10 and CIFAR-100 for BigGAN and OmniGAN

Tables 1 and 2 demonstrate that NICE consistently outperforms baselines such as BigGAN, LeCam+DA, OmniGAN and OmniGAN+ADA on CIFAR-10 and CIFAR-100, firmly establishing its superiority. NICE also outperforms LeCam+DA+KDDLGAN in the majority of scenarios without any knowledge integration from large-scale models, underscoring its efficiency and effectiveness.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{MA} & \multicolumn{3}{c}{100\% data} & \multicolumn{3}{c}{20\% data} & \multicolumn{3}{c}{10\% data} \\ \cline{3-11}  & & IS\(\uparrow\) & tFID\(\downarrow\) & vFID\(\downarrow\) & IS\(\uparrow\) & tFID\(\downarrow\) & vFID\(\downarrow\) & IS\(\uparrow\) & tFID\(\downarrow\) & vFID\(\downarrow\) \\ \hline BigGAN(\(d^{\prime}=256\)) & \(\times\) & 9.21 & 5.48 & 9.42 & 8.74 & 16.20 & 20.27 & 8.24 & 31.45 & 35.59 \\ \hline +LeCam & \(\times\) & 9.45 & 4.27 & 8.29 & 8.95 & 11.34 & 15.25 & 8.44 & 28.36 & 33.65 \\ +DigGAN & \(\times\) & 9.28 & 5.33 & 9.35 & 8.81 & 13.28 & 17.25 & 8.32 & 18.54 & 22.45 \\ \hline +NICE & \(\times\) & **9.50** & **4.19** & **8.24** & **8.96** & **8.51** & **12.54** & **8.73** & **13.65** & **17.75** \\ \hline +LeCam+DA & \(\checkmark\) & 9.45 & 4.32 & 8.40 & 9.01 & 8.53 & 12.47 & 8.81 & 12.64 & 16.42 \\ +LeCam+DA+KDDLGAN & \(\checkmark\) & \(-\) & \(-\) & 8.19 & \(-\) & \(-\) & 11.15 & \(-\) & \(-\) & \(13.86\) \\ +LeCam+DA+NICE & \(\checkmark\) & **9.52** & **3.72** & **7.81** & **9.12** & **6.92** & **10.89** & **8.99** & **9.86** & **13.81** \\ \hline \hline OmniGAN(\(d^{\prime}=1024\)) & \(\times\) & 10.01 & 6.92 & 10.75 & 8.64 & 36.75 & 41.17 & 6.69 & 53.02 & 57.68 \\ \hline +DA & \(\checkmark\) & 10.13 & 4.15 & 8.06 & 9.49 & 13.45 & 17.27 & 8.99 & 19.45 & 23.48 \\ +ADA & \(\checkmark\) & 10.24 & 4.95 & 9.06 & 9.41 & 27.04 & 30.58 & 7.86 & 40.05 & 44.01 \\ +NICE & \(\times\) & 10.21 & 2.72 & 6.79 & 9.86 & 6.06 & 9.87 & 9.78 & 6.40 & 10.37 \\ +NICE+ADA & \(\checkmark\) & **10.38** & **2.25** & **6.32** & **10.18** & **4.39** & **8.42** & **10.08** & **5.49** & **9.42** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison w/ and w/o NICE on CIFAR-10 given different percentage of training data.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{MA} & \multicolumn{3}{c}{100\% data} & \multicolumn{3}{c}{20\% data} & \multicolumn{3}{c}{10\% data} \\ \cline{3-11}  & & IS\(\uparrow\) & tFID\(\downarrow\) & vFID\(\downarrow\) & IS\(\uparrow\) & tFID\(\downarrow\) & vFID\(\downarrow\) & IS\(\uparrow\) & tFID\(\downarrow\) & vFID\(\downarrow\) \\ \hline BigGAN(\(d^{\prime}=256\)) & \(\times\) & 11.02 & 7.86 & 12.70 & 9.94 & 25.83 & 30.79 & 7.58 & 50.79 & 55.04 \\ \hline +LeCam & \(\times\) & **11.41** & 6.82 & 11.54 & 10.05 & 20.81 & 25.77 & 8.14 & 41.51 & 46.43 \\ +DigGAN & \(\times\) & 11.15 & 8.13 & 13.06 & 9.98 & 16.87 & 21.59 & **9.04** & 23.10 & 27.78 \\ +NICE & \(\times\) & 10.99 & **6.31** & **11.08** & **10.32** & **13.17** & **17.80** & 8.96 & **19.53** & **24.33** \\ \hline +LeCam+DA & \(\checkmark\) & 11.25 & 6.45 & 11.26 & 10.12 & 15.96 & 20.42 & 9.17 & 22.75 & 27.14 \\ +LeCam+DA+KDDLGAN & \(\checkmark\) & \(-\) & \(-\) & **10.12** & \(-\) & \(-\) & 18.70 & \(-\) & \(-\) & 22.40 \\ +LeCam+DA+NICE & \(\checkmark\) & **11.28** & **5.72** & 10.40 & **10.54** & **10.02** & **14.93** & **9.35** & **14.95** & **19.60** \\ \hline \hline OmniGAN(\(d^{\prime}=1024\)) & \(\times\) & 12.73 & 8.36 & 13.18 & 10.14 & 40.59 & 44.92 & 6.91 & 60.46 & 64.76 \\ \hline +DA & \(\checkmark\) & 12.94 & 7.41 & 12.08 & 11.35 & 17.65 & 22.37 & 10.01 & 30.68 & 34.94 \\ +ADA & \(\checkmark\) & 13.07 & 6.12 & 10.79 & 12.07 & 13.54 & 18.20 & 8.95 & 44.65 & 49.08 \\ +NICE & \(\times\) & 13.77 & 3.83 & 8.61 & 12.57 & 8.68 & 13.53 & 11.97 & 14.53 & 19.22 \\ +NICE+ADA & \(\checkmark\) & **13.82** & **3.78** & **8.59** & **12.75** & **6.28** & **10.92** & **12.04** & **9.32** & **14.18** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison w/ and w/o NICE on CIFAR-100 given different percentage of training data.

### Results on low-shot generation on StyleGAN2

Table 3 showcases the superior performance of NICE in comparison to leading methods, significantly improving baselines and achieving state-of-the-art results. NICE also surpasses KDDLGAN, despite KDDLGAN leverages the large-scale CLIP model [43]. Figure 2 shows realistic images generated by NICE under scarce data training.

### Results on FFHQ for StyleGAN2 and ImageNet for BigGAN

Results for unconditional image generation on FFHQ are in Table 4, and for conditional image generation on ImageNet \(64\times 64\) in Table 5. We limit FFHQ to 100, 1K, 2K, 5K real images and follow [17; 9] for ImageNet setting. NICE showcases superior performance on FFHQ and ImageNet.

### Analysis of NICE

**Analysis of the stabilizing effect of NICE.** In our 10% CIFAR-10 experiments with OmniGAN(\(d^{\prime}=256\)), Figure 3 provides compelling evidence supporting our theory. Figure 2(a) shows that OmniGAN+AN and OmniGAN+NICE achieve lower weight norms than OmniGAN, validating Theorem

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{MA} & \multicolumn{4}{c}{FFHQ} \\ \cline{3-8}  & & 100 & 1\(K\) & 2\(K\) & 5\(K\) \\ \hline StyleGAN2 & \(\times\) & 179 & 100.16 & 54 & 49.68 \\ ADA & ✓ & 85.8 & 21.29 & 15.39 & 10.96 \\ ADA-Linear & ✓ & 82 & 19.86 & 13.01 & 9.39 \\ InsGen & ✓ & 45.75 & 18.21 & 11.47 & 7.83 \\ FaceCLR & ✓ & 42.56 & 15.92 & 9.90 & 7.25 \\ ADA+NICE & ✓ & **38.42** & **14.57** & **8.85** & **6.48** \\ \hline \hline \end{tabular}
\end{table}
Table 4: FID \(\downarrow\) scores on FFHQ using StyleGAN2. ADA-Linear is introduced in [63].

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{MA} & \multirow{2}{*}{Pre-trained} & \multicolumn{4}{c}{100-shot} & \multicolumn{2}{c}{Animal Face} \\ \cline{3-8}  & & & Obama & Grumpy Cat & Panda & Cat & Dog \\ \hline StyleGAN2 [25] & \(\times\) & \(\times\) & 80.20 & 48.90 & 34.27 & 71.71 & 131.90 \\ StyleGAN2+SSGAN-LA [16] & \(\times\) & \(\times\) & 79.88 & 38.42 & 28.6 & 78.78 & 109.91 \\ StyleGAN2+NICE & \(\times\) & \(\times\) & **24.56** & **18.78** & **8.92** & **25.25** & **46.56** \\ \hline ADA [22] & ✓ & \(\times\) & 45.69 & 26.62 & 12.90 & 40.77 & 56.83 \\ DA [68] & ✓ & \(\times\) & 46.87 & 27.08 & 12.06 & 42.44 & 58.85 \\ DigGAN [11] & ✓ & \(\times\) & 36.38 & 25.42 & 11.54 & 35.67 & 59.98 \\ LeCam [57] & ✓ & \(\times\) & 33.16 & 24.93 & 10.16 & 34.18 & 54.88 \\ GenCo [8] & ✓ & \(\times\) & 32.21 & 17.79 & 9.49 & 30.89 & 49.63 \\ InsGen [63] & ✓ & \(\times\) & 32.42 & 22.01 & 9.85 & 33.01 & 44.93 \\ MaskedGAN [17] & ✓ & \(\times\) & 33.78 & 20.06 & 8.93 & — & — \\ FakeCLR [32] & ✓ & \(\times\) & 26.95 & 19.56 & 8.42 & 26.34 & 42.02 \\ TransferGAN \({}^{\dagger}\)[59] & ✓ & ✓ & 39.85 & 29.77 & 17.12 & 49.10 & 65.57 \\ LeCam+KDDLGAN \({}^{\ddagger}\)[9] & ✓ & ✓ & 29.38 & 19.65 & 8.41 & 31.89 & 50.22 \\ ADA+NICE & ✓ & \(\times\) & **20.09** & **15.63** & **8.18** & **22.70** & **28.65** \\ \hline \hline \end{tabular}
\end{table}
Table 3: FID \(\downarrow\) scores for unconditional image generation with StyleGAN2 on 100-shot Obama/Grumpy cat/Panda and AnimalFace-Cat/Dog datasets. \({}^{\dagger}\) indicates using a generator pre-trained on the full FFHQ dataset. \({}^{\ddagger}\) means using the CLIP [43] model pre-trained on large scale data.

Figure 2: Images generated using NICE+ADA on StyleGAN2; see Figure 14 in §J for more examples and comparisons for this low-shot generation setting.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{MA} & \multicolumn{4}{c}{10\% data} & \multicolumn{2}{c}{5\% data} & \multicolumn{2}{c}{2.5\% data} \\ \cline{3-6}  & & IS \(\uparrow\) & FID \(\downarrow\) & IS \(\uparrow\) & FID \(\downarrow\) & IS\(\uparrow\) & FID \(\downarrow\) \\ \hline BigGAN & \(\times\) & 10.94 & 38.30 & 6.13 & 91.16 & 3.92 & 133.80 \\ ADA & ✓ & 12.67 & 31.89 & 9.44 & 43.21 & 8.54 & 56.83 \\ DA & ✓ & 12.76 & 32.82 & 9.63 & 56.75 & 8.17 & 63.49 \\ MaskedGAN & ✓ & 13.34 & 26.51 & 12.85 & 35.70 & 12.68 & 38.62 \\ KDDLGAN & ✓ & 14.14 & 20.32 & 14.06 & 22.35 & **14.65** & 28.79 \\ NICE & \(\times\) & 14.18 & 21.44 & 13.96 & 24.72 & 13.32 & 31.45 \\ ADA+NICE & ✓ & **14.58** & **18.29** & **14.10** & **20.07** & 13.92 & **24.41** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison with and w/o NICE on ImageNet given different percentage of training data.

1 that the multiplicative noise modulation reduces the weight norm of discriminator, thus lowering the Rademacher complexity and improving generalization.

Figure 2(b) shows a gradient surge at the latent layer before the classification head, especially for real images in OmniGAN+AN and OmniGAN+NICE, surpassing gradients of OmniGAN. This observation aligns with our prediction in Prop. 1 that introducing noise amplifies gradients due to the maximization effect on real images when training discriminator and on fake images when training generator, causing undesired gradient issues.

Figure 2(c) illustrates a smaller squared gradient norm \(\|\partial f/\partial x\|_{2}^{2}\) for OmniGAN+NICE compared to OmniGAN+AN and OmniGAN, showcasing the effectiveness of consistency regularization in penalizing the gradient of discriminator, in line with the theoretical derivation in Theorem 2.

Figure 2(d) demonstrates that OmniGAN+NICE achieves a lower gradient norm of the discriminator loss w.r.t. the input than both OmniGAN and OmniGAN+AN, affirming ability of NICE to counteract the negative effects of large gradient norms caused by noise modulation. The resulted smaller gradients at the input validates efficiency of NICE in stabilizing training.

**Analysis of the ability of NICE to enhance generalization.** Figure 4 visualizes the discriminator outputs for real, fake, and test images along with tFID curves. In contrast to OmniGAN, OmniGAN+AN and OmniGAN+NICE balance discrimination and generalization [67], maintaining a steady discrepancy between real and fake images. This equilibrium ensures a smaller discrepancy between real seen images and unseen samples, effectively lowering the generation error as outlined in Eq. 3 of Lemma 1. As a result, the generalization of GANs is enhanced, leading to superior performance for image generation, as illustrated by the tFID curves in Figure 3(d).

### Ablation studies

**Ablation of components in NICE.** We examine the impact of various components of NICE: (i) introducing adaptive noise (AN) to the discriminator, (ii) applying consistency regularization to

Figure 4: The discriminator output w.r.t. real, fake and test images of (a) OmniGAN, (b) OmniGAN+AN, (c) OmniGAN+NICE, along with (d) tFID curves on 10% CIFAR-10 using OmniGAN (\(d^{\prime}\!=\!256\)). The shaded region represents the standard deviation. Note we scale the \(y\)-axis in (b) and (c) for visual clarity. In (d), training time is doubled to evaluate the endurance of our methods under prolonged training conditions.

Figure 3: Weight and gradient norms of the discriminator on 10% CIFAR-10 with OmniGAN (\(d^{\prime}\!=\!256\)). (a) total discriminator weight norms, (b) gradient norm at the layer before classification head, (c) gradient norm of \(f\) w.r.t. input, and (d) gradient norm of discriminator loss w.r.t. input.

real images (NICE\({}_{D_{r}}\)), (iii) applying consistency regularization to fake images (NICE\({}_{D_{f}}\)) during discriminator training, and (iv) enforcing consistency on fake images while training the generator (NICE\({}_{G_{f}}\)). Table 6 shows our evaluations across 10% CIFAR-10/100 using OmniGAN (\(d^{\prime}\!=\!256\)) and the Obama on StyleGAN2. The optimal performance is achieved when NICE is applied to both real and fake images in the training of both the discriminator and generator.

**Impact of different factors in NICE.** For an in-depth analysis on how different factors in NICE contribute to its performance, readers are directed to SSE.

**Comparison with alternatives for enhanced generalization and stability.** Table 7 shows that NICE outperforms DA, ADA, AWD (adaptive weight decay), AN+AGP (adaptive noise with adaptive gradient penalization) and NICE\({}_{add}\) (with additive noise). Detailed implementation of these variants can be found in SSD.2. Figure 5 shows that NICE also enhances performance with increasing network size, while ADA and DA exhibit a decline. As augmentation-based methods such as DA, ADA, and AACR, may leak augmentation cues to the generator (Figure 9 of SS1), NICE mitigates this drawback.

**Rationalizing Advantages of NICE.** Refer to SSF and SSG for a comprehensive explanations of why NICE outperforms other alternatives, enhancing generalization and stability. Given a mild increase in the computational load (SSH), substantial performance gains are achieved.

## 5 Conclusions

Our newly proposed approach for GAN training, NoIse-modulated Consistency rEgularization (NICE), improves generalization of various GAN models and their performance under limited data training. Through rigorous theoretical analysis, we have shown that NICE reduces the Rademacher complexity and penalizes the gradient of the latent features within the discriminator. Our experimental results match theoretical findings, illustrating that NICE not only improves generalization but also enhances stability of GAN training. By harnessing these two key advantages, NICE works with various network backbones and consistently outperforms existing methods. These exceptional results firmly establish NICE as a powerful solution for preventing the pervasive issue of discriminator overfitting in GANs. Limitations and border impact of our work are discussed in SSA.

## Acknowledgements

We thank to Moyang Liu, whose discussions and constant encouragement significantly shaped this work. YN is supported by China Scholarship Council (202008520034). PK is supported by CSIRO's Science Digital. We are grateful for the valuable feedback provided by all anonymous reviewers.

## References

* [1] Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data-generating distribution. _JMLR_, 15(1):3563-3593, 2014.
* [2] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In _ICML_, pages 214-223, 2017.
* [3] Raman Arora, Peter Bartlett, Poorya Mianjy, and Nathan Srebro. Dropout: Explicit forms and capacity control. In _ICML_, pages 351-361. PMLR, 2021.
* [4] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). In _ICML_, pages 224-232. PMLR, 2017.
* [5] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. _NeurIPS_, 30, 2017.
* [6] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. _JMLR_, 3(Nov):463-482, 2002.
* [7] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In _ICLR_, 2019.
* [8] Kaiwen Cui, Jiaxing Huang, Zhipeng Luo, Gongjie Zhang, Fangneng Zhan, and Shijian Lu. Genco: generative co-training for generative adversarial networks with limited data. In _AAAI_, volume 36, pages 499-507, 2022.
* [9] Kaiwen Cui, Yingchen Yu, Fangneng Zhan, Shengcai Liao, Shijian Lu, and Eric Xing. Kd-dlgan: Data limited image generation via knowledge distillation. In _CVPR_, 2023.
* [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, pages 248-255, 2009.
* [11] Tiantian Fang, Ruoyu Sun, and Alex Schwing. DigGAN: Discriminator gradient gap regularization for GAN training with limited data. In _NeurIPS_, 2022.
* [12] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In _ICLR_, 2021.
* [13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In _NeurIPS_, 2014.
* [14] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. In _NeurIPS_, pages 5769-5779, 2017.
* [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In _NeurIPS_, volume 30, pages 6626-6637, 2017.
* [16] Liang Hou, Huawei Shen, Qi Cao, and Xueqi Cheng. Self-supervised gans with label augmentation. _NeurIPS_, 34:13019-13031, 2021.
* [17] Jiaxing Huang, Kaiwen Cui, Dayan Guan, Aoran Xiao, Fangneng Zhan, Shijian Lu, Shengcai Liao, and Eric Xing. Masked generative adversarial networks are data-efficient generation learners. _NeurIPS_, 35:2154-2167, 2022.
* [18] Kaiyi Ji, Yi Zhou, and Yingbin Liang. Understanding estimation and generalization error of generative adversarial networks. _IEEE Transactions on Information Theory_, 67(5):3114-3129, 2021.
* [19] Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy. Deceive d: adaptive pseudo augmentation for gan training with limited data. _NeurIPS_, 34:21655-21667, 2021.
* [20] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. _CVPR_, 2023.
* [21] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In _ICLR_, 2018.
* [22] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. _NeurIPS_, 33, 2020.
* [23] Tero Karras, Miika Aittala, Samuli Laine, Erik Harikonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. _NeurIPS_, 34:852-863, 2021.
* [24] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _CVPR_, 2019.
* [25] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In _CVPR_, pages 8110-8119, 2020.

* [26] Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans. _arXiv:1705.07215_, 2017.
* [27] Lingke Kong, Chenyu Lian, Detian Huang, Yane Hu, Qichao Zhou, et al. Breaking the dilemma of medical image-to-image translation. _NeurIPS_, 34:1964-1978, 2021.
* [28] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [29] Nupur Kumari, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Ensembling off-the-shelf models for gan training. In _CVPR_, pages 10651-10662, 2022.
* [30] Tuomas Kynkaanniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of imagenet classes in frechet inception distance. In _ICLR_, 2023.
* [31] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. _nature_, 521(7553):436-444, 2015.
* [32] Ziqiang Li, Chaoyue Wang, Heliang Zheng, Jing Zhang, and Bin Li. Fakeclr: Exploring contrastive learning for solving latent discontinuity in data-efficient gans. In _ECCV_, pages 598-615, 2022.
* [33] Kanglin Liu, Wenming Tang, Fei Zhou, and Guoping Qiu. Spectral regularization for combating mode collapse in gans. In _ICCV_, pages 6382-6390, 2019.
* [34] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolutional neural networks. _NeurIPS_, 29, 2016.
* [35] Tengyu Ma. _Lecture Notes for Machine Learning Theory (CS229M/STATS214)_.
* [36] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In _ICML_, pages 3481-3490. PMLR, 2018.
* [37] Poorya Mianjy and Raman Arora. On dropout and nuclear norm regularization. In _ICML_, pages 4575-4584. PMLR, 2019.
* [38] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In _ICLR_, 2018.
* [39] Alfred Muller. Integral probability metrics and their generating classes of functions. _Advances in applied probability_, 29(2):429-443, 1997.
* [40] Yao Ni, Piotr Koniusz, Richard Hartley, and Richard Nock. Manifold learning benefits gans. In _CVPR_, pages 11265-11274, 2022.
* [41] Yao Ni, Dandan Song, Xi Zhang, Hao Wu, and Lejian Liao. Cagan: Consistent adversarial training enhanced gans. In _IJCAI_, pages 2588-2594, 2018.
* [42] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. _NeurIPS_, 29, 2016.
* [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, pages 8748-8763, 2021.
* [44] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in style: a stylegan encoder for image-to-image translation. In _CVPR_, pages 2287-2296, 2021.
* [45] Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization. _NeurIPS_, 30, 2017.
* [46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alce Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In _NeurIPS_, pages 2234-2242.
* [47] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. StyleGAN-T: Unlocking the power of GANs for fast large-scale text-to-image synthesis. In _ICML_, pages 30105-30118, 2023.
* [48] Fatemeh Shiri, Xin Yu, Piotr Koniusz, and Fatih Porikli. Face destylization. In _DICTA_, pages 1-8. IEEE, 2017.
* [49] Fatemeh Shiri, Xin Yu, Fatih Porikli, Richard Hartley, and Piotr Koniusz. Identity-preserving face recovery from portraits. In _WACV_, 2018.
* [50] Fatemeh Shiri, Xin Yu, Fatih Porikli, Richard Hartley, and Piotr Koniusz. Identity-preserving face recovery from stylized portraits. _IJCV_, 127(6-7):863-883, 2019.
* [51] Zhangzhang Si and Song-Chun Zhu. Learning hybrid image templates (hit) by information projection. _TPAMI_, 34(7):1354-1367, 2012.
* [52] Ivan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, Jian Ren, Hsin-Ying Lee, Peter Wonka, and Sergey Tulyakov. 3D generation on ImageNet. In _ICLR_, 2023.
* [53] Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Peter Wonka. EpiGRAF: Rethinking training of 3d GANs. In _NeurIPS_, 2022.
* [54] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. _JMLR_, 15(1):1929-1958, 2014.
* [55] Ming Tao, Bing-Kun Bao, Hao Tang, and Changsheng Xu. Galip: Generative adversarial clips for text-to-image synthesis. _CVPR_, 2023.
* [56] Hoang Thanh-Tung, Truyen Tran, and Svetha Venkatesh. Improving generalization and stability of generative adversarial networks. In _ICLR_, 2019.

* [57] Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and Weilong Yang. Regularing generative adversarial networks under limited data. In _CVPR_, 2021.
* [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _NeurIPS_, 30, 2017.
* [59] Yaxing Wang, Chenshen Wu, Luis Herranz, Joost Van de Weijer, Abel Gonzalez-Garcia, and Bogdan Raducanu. Transferring gans: generating images from limited data. In _ECCV_, pages 218-234, 2018.
* [60] Colin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization effects of dropout. In _ICML_, pages 10181-10192. PMLR, 2020.
* [61] Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang. Improving the improved training of wasserstein gans: A consistency term and its dual effect. _arXiv preprint arXiv:1803.01541_, 2018.
* [62] Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu, et al. R-drop: Regularized dropout for neural networks. _NeurIPS_, 34:10890-10905, 2021.
* [63] Ceyuan Yang, Yujun Shen, Yinghao Xu, and Bolei Zhou. Data-efficient instance generation from instance discrimination. _NeurIPS_, 34:9378-9390, 2021.
* [64] Han Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for generative adversarial networks. In _ICLR_, 2020.
* [65] Jianfeng Zhang, Zihang Jiang, Dingdong Yang, Hongyi Xu, Yichun Shi, Guoxian Song, Zhongcong Xu, Xinchao Wang, and Jiashi Feng. Avatargen: a 3d generative model for animatable human avatars. In _ECCV_, pages 668-685. Springer, 2023.
* [66] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup help with robustness and generalization? In _ICLR_, 2021.
* [67] Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discrimination-generalization tradeoff in GANs. In _ICLR_, 2018.
* [68] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-efficient gan training. _NeurIPS_, 33, 2020.
* [69] Zhengli Zhao, Sameer Singh, Honglak Lee, Zizhao Zhang, Augustus Odena, and Han Zhang. Improved consistency regularization for gans. In _AAAI_, volume 35, pages 11033-11041, 2021.
* [70] Yong Zhong, Hong Tao Liu, Xiaodong Liu, Fan Bao, Weiran Shen, and Chongxuan Li. Deep generative modeling on limited data with regularization by nontransferable pre-trained models. In _ICLR_, 2023.
* [71] Peng Zhou, Lingxi Xie, Bingbing Ni, Cong Geng, and Qi Tian. Omni-gan: On the secrets of cgans and beyond. In _ICCV_, pages 14061-14071, 2021.

**NICE: NoIse-modulated Consistency rEquarization for Data-Efficient GANs (Supplementary Material)**

**Yao Ni\({}^{\dagger}\), Piotr Koniusz\({}^{\star,\lx@sectionsign,\dagger}\)**

\({}^{\dagger}\)The Australian National University \({}^{\lx@sectionsign}\)Data6l\(\bm{\mathsf{\char 37}}\)CSIRO

\({}^{\dagger}\)firstname.lastname@anu.edu.au

## Appendix A Broader impact and limitations

NICE is a solution to limiting the extensive image usage in GAN training. Conventional GANs heavily rely on massive training data, which can jeopardize data privacy during large-scale data collection. NICE is an innovative method for data-limited image generation. By successfully training GANs with just 100 images, NICE not only minimizes the dependency on large-scale training datasets, but also significantly reduces the training cost. NICE decreases the risk of privacy violations (using smaller datasets equals to the lesser risk of data mismanagement). NICE also provides a cost-effective alternative for training GANs. As the data required for training is lesser compared with large-scale models, NICE is also energy-consumption friendly. Of course, with small-scale dataset comes a limitation on diversity. The model cannot generate instances for which the data is completely lacking. Thus, special care needs to be taken regarding gender, race and other biases in generated images.

NICE advances GANs with Theorems 1 and 2, along with Corollaries 1 and 2. It offers novel insights into the weight regularization. These findings, driven by the multiplicative and additive noise modulation, along with the gradient regularization due to the consistency regularization, pave the way for further future research.

## Appendix B Notations

Below, we explain the notations used in this work.

**Scalars**: Represented by lowercase letters (_e_.\(g\)., \(m\), \(n\), \(\delta\)).

**Vectors**: Bold lowercase letters (_e_.\(g\)., \(\bm{v}\), \(\bm{u}\), \(\bm{m}\)).

**Matrices**: Bold uppercase letters (_e_.\(g\)., \(\bm{W}\), \(\bm{X}\), \(\bm{H}\)).

**Vector/Matrix elements**: \(v_{k}\) and \(a_{k}\) represent \(k\)-th element of \(\bm{v}\) and \(\bm{a}\). \(H_{jk}\) denotes the element located at the \(j\)-th row and \(k\)-th column of matrix \(\bm{H}\).

**Functions**: Letters followed by brackets (_e_.\(g\)., \(\sigma(\cdot)\), \(h(\cdot)\), \(\nabla f(\cdot)\), \(H(\cdot)\)).

**Function sets**: Calligraphic uppercase letters are used, but note that \(\mathcal{N}\) and \(\mathcal{B}\) specifically denote Gaussian and Bernoulli distributions, respectively (_e_.\(g\)., \(\mathcal{H}\), \(\mathcal{F}\)).

**Probability measures**: Denoted by letters \(\mu\), \(\nu\) and \(p_{z}\).

**Dataset samples**: Expressed as \(\bm{x}^{(i)}\), \(\bm{y}^{(i)}\), \(\bm{a}^{(i)}\), \(\tau^{(i)}\).

**Multiplication/Composition**: \(\odot\) represents element-wise multiplication and \(\circ\) denotes function composition.

**Expectation/empirical expectation**: \(\mathbb{E}[\cdot]\) represents the average or expected value of a random variable while \(\mathbb{\hat{E}}[\cdot]\) denotes the empirical expectation calculated over observed data samples.

Proofs

We define \(\|\bm{v}\|^{2}=\bm{v}^{T}\bm{v}\) as the squared norm of vector \(\bm{v}\). Note the commutative property of the dot product, \(\bm{v}^{T}\bm{u}=\bm{u}^{T}\bm{v}\), and the distributive property over matrices, \(\bm{v}^{T}\bm{A}+\bm{u}^{T}\bm{A}=(\bm{v}+\bm{u})^{T}\bm{A}\). With these definitions and properties established, we now prove Theorem 1, Proposition 1 and Theorem 2. We begin with a simplified scalar-based derivation, ensuring a rapid derivation of the final results, followed by comprehensive proofs encompassing vector and matrix calculus for a holistic understanding.

### Proof of Theorem 1

Theorem 1 (_Regularization by the Gaussian multiplicative noise in deep regression)_: _Consider a regression task on a two-layer neural network. Let \(\mathcal{X}\!\subseteq\!\mathbb{R}^{d_{0}}\) and \(\mathcal{Y}\!\subseteq\![-1,1]^{d_{2}}\) be the input and output spaces, and \(\mathcal{D}\!=\!\mathcal{X}\!\times\!\mathcal{Y}\). Let \(n\) examples \(\{(\bm{x}^{(i)},\bm{y}^{(i)})\}_{i=1}^{n}\!\sim\!\mathcal{D}^{n}\). Let \(f_{w}\!:\!\mathcal{X}\!\to\!\mathcal{Y}\) be parameterized by \(\{w\!:\!\bm{W}_{1}\!\!\in\!\mathbb{R}^{d_{1}\times d_{0}},\bm{W}_{2}\!\in\! \mathbb{R}^{d_{2}\times d_{1}}\}\) and \(f_{w}(\bm{x},\bm{z})\!=\!\bm{W}_{2}(\bm{z}\odot\sigma(\bm{W}_{1}\bm{x}))\) where \(\sigma\) is 1-Lipschitz activation function, \(\odot\) denotes element-wise multiplication and \(\bm{z}\!\sim\!\mathcal{N}(\bm{1},\beta^{2}\bm{I}^{d_{1}})\). Let \(\bm{a}\!=\!\sigma(\bm{W}_{1}\bm{x})\), \(\hat{\bm{a}}\!=\!\hat{\mathbb{E}}_{i}[\bm{a}^{(i)}\odot\bm{a}^{(i)}]\), \(\hat{\bm{a}}_{k}\!\geq\!0\) denotes the \(k\)-th element of \(\hat{\bm{a}}\). Let \(\|\bm{W}_{2}\|_{2,1}\!=\!\sum_{k}\!\|\bm{w}_{k}\|_{2}\) where \(\bm{w}_{k}\) is the \(k\)-th column vector in \(\bm{W}_{2}\). The regression task with the \(l_{2}\) loss leads to the weight regularization as:_

\[\hat{L}_{\text{noise}}(w):=\hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{z}}\big{[}\|\bm {y}^{(i)}\!-\!\bm{W}_{2}(\bm{z}\odot\bm{a}^{(i)})\|_{2}^{2}\big{]}\!=\!\hat{ \mathbb{E}}_{i}\big{[}\|\bm{y}^{(i)}\!-\!\bm{W}_{2}\bm{a}^{(i)}\|_{2}^{2}\big{]} +\beta_{2}^{2}\sum_{k}\hat{\bm{a}}_{k}\|\bm{w}_{k}\|_{2}^{2}.\]

We commence with a simplified derivation using scalars, which can quickly lead to the final results:

\[\hat{\mathbb{E}}_{i}\mathbb{E}_{z\sim\mathcal{N}(1,\beta^{2})} \big{[}\big{(}y^{(i)}-w_{2}(za^{(i)})\big{)}^{2}\big{]}=\hat{\mathbb{E}}_{i} \mathbb{E}_{z}\big{[}\big{(}y^{(i)}-w_{2}a^{(i)}-w_{2}(z-1)a^{(i)}\big{)}^{2} \big{]}\] \[= \hat{\mathbb{E}}_{i}\mathbb{E}_{z}\big{[}\big{(}y^{(i)}-w_{2}a^{( i)}\big{)}^{2}-2(y^{(i)}-w_{2}a^{(i)})w_{2}(z-1)a^{(i)}+\big{(}w_{2}(z-1)a^{(i)} \big{)}^{2}\big{]}\] \[= \hat{\mathbb{E}}_{i}\big{[}\big{(}y^{(i)}-w_{2}a^{(i)}\big{)}^{2} \big{]}+w_{2}^{2}\hat{\mathbb{E}}_{i}a^{(i)2}\mathbb{E}_{z}[(z-1)^{2}]\] \[= \hat{\mathbb{E}}_{i}\big{[}\big{(}y^{(i)}-w_{2}a^{(i)}\big{)}^{2} \big{]}+\beta^{2}w_{2}^{2}\hat{\mathbb{E}}_{i}a^{(i)2}.\]

Proof.: \[\hat{L}_{\text{noise}}(w):=\hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{z}} \big{[}\|\bm{y}^{(i)}-\bm{W}_{2}(\bm{z}\odot\bm{a}^{(i)})\|_{2}^{2}\big{]}= \hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{z}}\big{[}\|\bm{y}^{(i)}\!-\!\bm{W}_{2} \bm{a}^{(i)}-\!\bm{W}_{2}(\bm{z}\odot\bm{a}^{(i)})\|_{2}^{2}\big{]}\] \[= \hat{\mathbb{E}}_{i}\big{[}\|\bm{y}^{(i)}\!-\!\bm{W}_{2}\bm{a}^{(i )}\|_{2}^{2}\big{]}\!-\!2\hat{\mathbb{E}}_{i}\big{[}\big{(}\bm{y}^{(i)}\!-\! \bm{W}_{2}\bm{a}^{(i)}\big{)}^{T}\bm{W}_{2}(\bm{0}\odot\bm{a}^{(i)})\big{]}+ \hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{z}}\big{[}\|\bm{W}_{2}\big{(}(\bm{z}\!-\! \bm{1})\odot\bm{a}^{(i)}\big{)}\|_{2}^{2}\big{]}\] \[= \hat{\mathbb{E}}_{i}\|\bm{y}^{(i)}-\bm{W}_{2}\bm{a}^{(i)}\|_{2}^{2} +\hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{z}}\|\bm{W}_{2}\big{(}(\bm{z}-\bm{1})\odot \bm{a}^{(i)}\big{)}\|_{2}^{2}.\] (12)

Let \(\bm{m}\sim\mathcal{N}(\bm{0},\beta^{2}\bm{I}^{d_{1}})\), with \(m_{k}\) and \(a_{k}^{(i)}\) representing the \(k\)-th elements of vectors \(\bm{m}\) and \(\bm{a}^{(i)}\), respectively. Using these definitions, we derive the second term of Eq. 12 as follows:

\[\hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{z}}\big{[}\|\bm{W}_{2}\big{(}( \bm{z}-\bm{1})\odot\bm{a}^{(i)}\big{)}\|_{2}^{2}\big{]}=\hat{\mathbb{E}}_{i} \mathbb{E}_{\bm{m}}\big{[}\|\bm{W}_{2}\big{(}\bm{m}\odot\bm{a}^{(i)}\big{)}\|_{2}^ {2}\big{]}\] \[= \hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{m}}\Big{[}(\bm{m}\odot\bm{a}^{( i)})^{T}\bm{W}_{2}^{T}\bm{W}_{2}(\bm{m}\odot\bm{a}^{(i)})\big{]}\] \[= \hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{m}}\Big{[}\sum_{j=1}^{d_{1}} \sum_{k=1}^{d_{1}}(m_{j}a_{j}^{(i)})(\bm{W}_{2}^{T}\bm{W}_{2})_{jk}(m_{k}a_{k}^{( i)})\Big{]}.\] (13)

Given that the elements in \(\bm{m}\) are independent, for \(j\neq k\) we have:

\[\mathbb{E}_{\bm{m}}\big{[}(m_{j}a_{j}^{(i)})(\bm{W}_{2}^{T}\bm{W}_{2})_{jk}(m_{k}a_ {k}^{(i)})\big{]}=0.\] (14)

Observe that \((\bm{W}_{2}^{T}\bm{W}_{2})_{kk}=\|\bm{w}_{k}\|_{2}^{2}\) represents the squared norm of \(k\)-th column vector in \(\bm{W}_{2}\). Let \(\hat{\bm{a}}\) denote the element-wise mean square of \(\bm{a}^{(i)}\), _i.e._, \(\hat{\bm{a}}\!=\!\hat{\mathbb{E}}_{i}[\bm{a}^{(i)}\odot\bm{a}^{(i)}]\). We leverage the distribution property \(\mathbb{E}[z^{2}]=\sigma^{2}+\mu^{2}\) for \(z\sim\mathcal{N}(\mu,\sigma^{2})\), simplifying to \(\mathbb{E}[z^{2}]=\sigma^{2}\) when \(\mu=0\). This lets us reformulate Eq. 13 explicitly as:

\[\hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{m}}\Big{[}\sum_{j=1}^{d_{1}}\sum_ {k=1}^{d_{1}}(m_{j}a_{j}^{(i)})(\bm{W}_{2}^{T}\bm{W}_{2})_{jk}(m_{k}a_{k}^{(i)}) \Big{]}\] \[= \hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{m}}\Big{[}\sum_{k=1}^{d_{1}}( m_{k}a_{k}^{(i)})(\bm{W}_{2}^{T}\bm{W}_{2})_{kk}(m_{k}a_{k}^{(i)})\Big{]}\] \[= \hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{m}}\sum_{k=1}^{d_{1}}(m_{k}^ {2}a_{k}^{(i)2})\|\bm{w}_{k}\|_{2}^{2}=\beta^{2}\hat{\mathbb{E}}_{i}\sum_{k=1} ^{d_{1}}a_{k}^{(i)2}\|\bm{w}_{k}\|_{2}^{2}\] \[= \beta^{2}\sum_{k}\hat{a}_{k}\|\bm{w}_{k}\|_{2}^{2}.\] (15)

Substituting the second term in Eq. 12 with Eq. 15, we arrive at:

\[\hat{L}_{\text{noise}}(w):=\hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{z}}\big{[}\|\bm {y}^{(i)}-\bm{W}_{2}(\bm{z}\odot\bm{a}^{(i)})\|_{2}^{2}\big{]}= \hat{\mathbb{E}}_{i}\big{[}\|\bm{y}^{(i)}-\bm{W}_{2}\bm{a}^{(i)}\|_{2}^{2} \big{]}+\beta^{2}\sum_{k}\hat{a}_{k}\|\bm{w}_{k}\|_{2}^{2}.\] (16)

Integrating the Gaussian multiplicative noise into the optimization of regression task leads to an adaptive regularization \(\beta^{2}\sum_{k}\hat{a}_{k}\|\bm{w}_{k}\|_{2}^{2}\) on \(\bm{W}_{2}\), effectively reweighting the squared norm \(\|\bm{w}_{k}\|_{2}^{2}\) of each column in accordance with \(\hat{a}_{k}\geq 0\). This mechanism, driven by the variance \(\beta^{2}\) of noise enhances the robustness of model and its generalization capability. It is worth highlighting that this process relates closely to the (2,1)-norm of \(\bm{W}_{2}\), defined as \(\|\bm{W}_{2}\|_{2,1}=\sum_{k}\|\bm{w}_{k}\|_{2}\).

**Corollary 1**: _Utilizing the additive noise injection where \(\bm{z}\sim\mathcal{N}(\bm{0},\beta^{2}\bm{I}^{d_{1}})\), the regression task with the \(l_{2}\) loss yields weight regularization as:_

\[\hat{L}_{\text{add\_noise}}(w):=\hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{z}}\big{[} \|\bm{y}^{(i)}-\bm{W}_{2}(\bm{z}+\bm{a}^{(i)})\|_{2}^{2}\big{]}= \hat{\mathbb{E}}_{i}\big{[}\|\bm{y}^{(i)}-\bm{W}_{2}\bm{a}^{(i)}\|_{2}^{2} \big{]}+\beta^{2}\sum_{k}\|\bm{w}_{k}\|_{2}^{2}.\] (17)

### Proof of Proposition 1

**Proposition 1**: _Define \(f:=f_{t}\circ\ldots\circ f_{2}\) as the feature extractor of the discriminator from the second layer onward. Let \(\tilde{\bm{a}}=f_{1}(\tilde{\bm{x}})\) and \(\bm{a}=f_{1}(\bm{x})\) be the latent features of the first layer for fake and real images, respectively. The discriminator is defined as \(h(\cdot)=\phi(\varphi(f(\cdot)))\), with \(\varphi\) as the prediction head and \(\phi\) as the loss function. Introduce a multiplicative noise \(\bm{z}\sim\mathcal{N}(\bm{1},\beta^{2}\bm{I}^{d^{\prime}})\) modulating solely the first layer. Let \(H^{(h)}_{kk}(\bm{a})\) be the \(k\)-th diagonal entry of the Hessian matrix of \(h\) at \(\bm{a}\) and \(a_{k}\) be the \(k\)-th element of \(\bm{a}\). Applying Taylor expansion to \(h(\cdot)\), the GAN loss can be expressed as follows:_

\[\min_{\bm{\theta}_{d}}L^{\text{AN}}_{D}:= \mathbb{E}_{\tilde{\bm{a}}}\mathbb{E}_{\bm{z}}\big{[}h(\bm{z} \odot\tilde{\bm{a}})\big{]}-\mathbb{E}_{\bm{a}}\mathbb{E}_{\bm{z}}\big{[}h( \bm{z}\odot\bm{a})\big{]}\] \[\min_{\bm{\theta}_{g}}L^{\text{AN}}_{G}:=\]

We provide a succinct proof for the discriminator using scalar values to quickly arrive at the final result, where \(h^{\prime}(\cdot)\) and \(h^{\prime\prime}(\cdot)\) are the first- and second-order derivatives, respectively.

\[\mathbb{E}_{z}h(z\tilde{a})-\mathbb{E}_{z}h(za)=\mathbb{E}_{z}h \big{(}\tilde{a}+(z-1)\tilde{a}\big{)}-\mathbb{E}_{z}h\big{(}a+(z-1)a\big{)}\] \[= \mathbb{E}_{z}\big{[}h(\tilde{a})+(z-1)\tilde{a}h^{\prime}(\tilde {a})+\frac{(z-1)^{2}}{2}\tilde{a}^{2}h^{\prime\prime}(\tilde{a})\big{]}- \mathbb{E}_{z}\big{[}h(a)+(z-1)ah^{\prime}(a)+\frac{(z-1)^{2}}{2}a^{2}h^{ \prime\prime}(a)\big{]}\] \[= h(\tilde{a})-h(a)+\mathbb{E}_{z}\big{[}\frac{(z-1)^{2}}{2}\big{]} \big{(}\tilde{a}^{2}h^{\prime\prime}(\tilde{a})-a^{2}h^{\prime\prime}(a) \big{)}\] \[= h(\tilde{a})-h(a)+\frac{\beta^{2}}{2}\big{(}\tilde{a}^{2}h^{ \prime\prime}(\tilde{a})-a^{2}h^{\prime\prime}(a)\big{)}.\]

_Proof:_ Consider a multivariate normal distribution \(\bm{m}\sim\mathcal{N}(\bm{0},\beta^{2}\bm{I}^{d^{\prime}})\). Let \(\nabla h(\bm{a})\) and \(H^{(h)}_{jk}(\bm{a})\) represent the first-order derivative and the \((j,k)\)-th entry of the Hessian matrix of \(h\) at \(\bm{a}\), respectively,implying that \(H^{(h)}_{jk}=H^{(h)}_{kj}\) due to the symmetry of the Hessian matrix. With these notations established, we proceed as follows:

\[L^{\text{AN}}_{D} :=\mathbb{E}_{\tilde{\bm{a}}}\mathbb{E}_{\bm{z}}\big{[}h(\tilde{\bm {\alpha}}\odot\bm{z})\big{]}-\mathbb{E}_{\bm{a}}\mathbb{E}_{\bm{z}}\big{[}h(\bm {a}\odot\bm{z})\big{]}\] \[=\mathbb{E}_{\tilde{\bm{a}}}\mathbb{E}_{\bm{z}}\big{[}h\big{(} \tilde{\bm{a}}+(\bm{z}-\bm{1})\odot\tilde{\bm{a}}\big{)}\big{]}-\mathbb{E}_{ \bm{a}}\mathbb{E}_{\bm{z}}\Big{[}h\big{(}\bm{a}+(\bm{z}-\bm{1})\odot\bm{a} \big{)}\Big{]}\] \[=\mathbb{E}_{\tilde{\bm{a}}}\mathbb{E}_{\bm{m}}\big{[}h(\tilde{ \bm{a}}+\bm{m}\odot\tilde{\bm{a}})\big{]}-\mathbb{E}_{\bm{a}}\mathbb{E}_{\bm{m }}\big{[}h(\bm{a}+(\bm{m}\odot\bm{a})\big{]}\] \[\approx\mathbb{E}_{\tilde{\bm{a}}}\mathbb{E}_{\bm{m}}\Big{[}h( \tilde{\bm{a}})+(\bm{m}\odot\tilde{\bm{a}})^{T}\nabla h(\tilde{\bm{a}})+\frac {1}{2}(\bm{m}\odot\tilde{\bm{a}})^{T}H^{(h)}(\tilde{\bm{a}})(\bm{m}\odot\tilde {\bm{a}})\Big{]}\] \[\quad-\mathbb{E}_{\bm{a}}\mathbb{E}_{\bm{m}}\Big{[}h(\bm{a})+(\bm {m}\odot\bm{a})^{T}\nabla h(\bm{a})+\frac{1}{2}(\bm{m}\odot\bm{a})^{T}H^{(h)}( \bm{a})(\bm{m}\odot\bm{a})\Big{]}\] \[=\mathbb{E}_{\tilde{\bm{a}}}h(\tilde{\bm{a}})-\mathbb{E}_{\bm{a}}h (\bm{a})\] \[\quad+\mathbb{E}_{\tilde{\bm{a}}}\mathbb{E}_{\bm{m}}\big{[}(\bm{m} \odot\tilde{\bm{a}})^{T}\nabla h(\tilde{\bm{a}})\big{]}-\mathbb{E}_{\bm{a}} \mathbb{E}_{\bm{m}}\big{[}(\bm{m}\odot\bm{a})^{T}\nabla h(\bm{a})\big{]}\] (18) \[\quad+\mathbb{E}_{\tilde{\bm{a}}}\mathbb{E}_{\bm{m}}\big{[}\frac{ 1}{2}(\bm{m}\odot\tilde{\bm{a}})^{T}H^{(h)}(\tilde{\bm{a}})(\bm{m}\odot\tilde {\bm{a}})\big{]}-\mathbb{E}_{\bm{a}}\mathbb{E}_{\bm{m}}\frac{1}{2}(\bm{m}\odot \bm{a})^{T}H^{(h)}(\bm{a})(\bm{m}\odot\bm{a}).\] (19)

Addressing the first-order gradient term presented in Eq. 18, we notice the following:

\[\mathbb{E}_{\tilde{\bm{a}}}\mathbb{E}_{\bm{m}}\big{[}(\bm{m} \odot\tilde{\bm{a}})^{T}\nabla h(\tilde{\bm{a}})\big{]}-\mathbb{E}_{\bm{a}} \mathbb{E}_{\bm{m}}\big{[}(\bm{m}\odot\bm{a})^{T}\nabla h(\bm{a})\big{]}\] \[= \mathbb{E}_{\tilde{\bm{a}}}\big{[}(\bm{0}\odot\tilde{\bm{a}})^{T} \nabla h(\tilde{\bm{a}})\big{]}-\mathbb{E}_{\bm{a}}\big{[}(\bm{0}\odot\bm{a}) ^{T}\nabla h(\bm{a})\big{]}=0.\] (20)

Examining the second-order gradient in Eq. 19, and leveraging the independence of elements in \(\bm{m}\), we apply the result from Eq. 14. With \(\tilde{a}_{k}\) and \(a_{k}\) representing the \(k\)-th elements in \(\tilde{\bm{a}}\) and \(\bm{a}\), respectively, we derive:

\[\mathbb{E}_{\tilde{\bm{a}}}\mathbb{E}_{\bm{m}}\big{[}\frac{1}{2} (\bm{m}\odot\tilde{\bm{a}})^{T}H^{(h)}(\tilde{\bm{a}})(\bm{m}\odot\tilde{\bm{ a}})\big{]}-\mathbb{E}_{\bm{a}}\mathbb{E}_{\bm{m}}\big{[}\frac{1}{2}(\bm{m}\odot\bm{a})^{T} H^{(h)}(\bm{a})(\bm{m}\odot\bm{a})\big{]}\] \[= \mathbb{E}_{\tilde{\bm{a}}}\mathbb{E}_{\bm{m}}\Big{[}\frac{1}{2} \sum_{k=1}^{d^{\prime}}m_{k}\tilde{a}_{k}H^{(h)}_{kk}(\tilde{\bm{a}})m_{k} \tilde{a}_{k}\Big{]}-\mathbb{E}_{\bm{a}}\mathbb{E}_{\bm{m}}\Big{[}\frac{1}{2} \sum_{k=1}^{d^{\prime}}m_{k}a_{k}H^{(h)}_{kk}(\bm{a})m_{k}\bm{a}_{k}\Big{]}\] \[= \frac{\beta^{2}}{2}\bigg{(}\mathbb{E}_{\tilde{\bm{a}}}\Big{[}\sum_ {k}\tilde{a}_{k}^{2}H^{(h)}_{kk}(\tilde{\bm{a}})\Big{]}-\mathbb{E}_{\bm{a}} \Big{[}\sum_{k}a_{k}^{2}H^{(h)}_{kk}(\bm{a})\Big{]}\bigg{)}.\] (21)

By substituting Eq. 18 with the results from Eq. 20, and Eq. 19 with Eq. 21, we arrive at the following expression:

\[L^{\text{AN}}_{D}\approx\mathbb{E}_{\tilde{\bm{a}}}\big{[}h(\tilde{\bm{a}}) \big{]}-\mathbb{E}_{\bm{a}}\big{[}h(\bm{a})\big{]}+\frac{\beta^{2}}{2}\bigg{(} \mathbb{E}_{\tilde{\bm{a}}}\Big{[}\sum_{k}\tilde{a}_{k}^{2}H^{(h)}_{kk}(\tilde{ \bm{a}})\Big{]}-\mathbb{E}_{\bm{a}}\Big{[}\sum_{k}a_{k}^{2}H^{(h)}_{kk}(\bm{a}) \Big{]}\bigg{)}.\] (22)

Utilizing the same derivation process as before for \(L^{\text{AN}}_{G}\), we arrive at the following result:

\[L^{\text{AN}}_{G}\approx-\mathbb{E}_{\tilde{\bm{a}}}\big{[}h(\tilde{\bm{a}}) \big{]}-\frac{\beta^{2}}{2}\mathbb{E}_{\tilde{\bm{a}}}\Big{[}\sum_{k}\tilde{a}_{ k}^{2}H^{(h)}_{kk}(\tilde{\bm{a}})\Big{]}.\] (23)

Introducing the multiplicative Gaussian noise during training of GAN produces the gradient maximization effect on the real samples in the discriminator training, specially through the term \(\frac{\beta^{2}}{2}\sum_{k}a_{k}^{2}H^{(h)}_{kk}(\bm{a})\). This term intertwines the second-order partial derivative (diagonal entry \(H^{(h)}_{kk}(\bm{a})\) of the Hessian matrix) with the non-negative factor \(\frac{\beta^{2}}{2}a_{k}^{2}\geq 0\), and is related to the trace of the Hessian matrix given by \(\textbf{trace}\big{(}H^{(h)}(\bm{a}_{r})\big{)}=\sum_{k}H^{(h)}_{kk}(\bm{a})\). Employing similar reasoning for the fake samples when training the generator reveals that the noise-modulated discriminator training of GAN tends to amplify the the diagonal entry of the second-order partial derivative in the Hessian matrix at both the real and fake points, influencing the optimization stability.

### Proof of Theorem 2

**Theorem 2**: _Using notations from Prop. 1 and with \(f\!:\!\mathbb{R}^{d^{\prime}}\!\!\to\!\mathbb{R}\), let \(\nabla_{k}^{2}f(\bm{a})\) and \(\big{(}H^{(f)}_{jk}(\bm{a})\big{)}^{2}\) denote the squares of the \(k\)-th gradient element and \((j,k)\)-th entry of Hessian matrix of \(f\) at \(\bm{a}\), respectively. Given \(\bm{z}_{1},\bm{z}_{2}\!\sim\!\!\mathcal{N}(\bm{1},\beta^{2}\bm{I}^{d^{\prime}})\), enforcing invariance of \(f\) under different noise modulations yields:_

\[\ell^{\text{NCE}}(\bm{a})\!:=\!\mathbb{E}_{\bm{z}_{1},\bm{z}_{2}}\big{[}\!\big{(} f(\bm{z}_{1}\odot\bm{a})\!-\!f(\bm{z}_{2}\odot\bm{a})\big{)}^{2}\big{]}\!\approx\!2\beta^{2} \!\sum_{k}a_{k}^{2}\nabla_{k}^{2}f(\bm{a})\!+\!\beta^{4}\!\sum_{j,k}\!a_{j}^{2}a_{k} ^{2}(H^{(f)}_{jk}(\bm{a}))^{2}\.\]A concise derivation employs scalar values for the Taylor expansion, swiftly leading to results using \(f^{\prime}(\cdot)\) and \(f^{\prime\prime}(\cdot)\) for first and second-order derivatives, respectively.

\[\mathbb{E}_{z_{1},z_{2}}\big{(}f(az_{1})-f(az_{2})\big{)}^{2}= \mathbb{E}_{z_{1},z_{2}}\big{(}f(a+(z_{1}-1)a)-f(a+(z_{2}-1)a)\big{)}^{2}\] \[= \mathbb{E}_{z_{1},z_{2}}\Big{(}f(a)\!+\!(z_{1}\!-\!1)af^{\prime}( a)\!+\!\frac{(z_{1}\!-\!1)^{2}}{2}a^{2}f^{\prime\prime}(a)\!-\!f(a)\!-\!(z_{2}\!-\!1) af^{\prime}(a)\!-\!\frac{(z_{2}\!-\!1)^{2}}{2}a^{2}f^{\prime\prime}(a)\Big{)}^{2}\] \[= \mathbb{E}_{z_{1},z_{2}}\Big{(}(z_{1}-z_{2})af^{\prime}(a)+\frac {(z_{1}-1)^{2}-(z_{2}-1)^{2}}{2}a^{2}f^{\prime\prime}(a)\Big{)}^{2}\] \[= \mathbb{E}_{\bm{m}_{1},\bm{m}_{2}\sim\mathcal{N}(0,\beta^{2})} \Big{(}(m_{1}-m_{2})af^{\prime}(a)+\frac{m_{1}^{2}-m_{2}^{2}}{2}a^{2}f^{\prime \prime}(a)\Big{)}^{2}\] \[= \mathbb{E}_{\bm{m}_{1},\bm{m}_{2}}\Big{[}(m_{1}-m_{2})^{2}a^{2}f ^{\prime 2}(a)+(m_{1}-m_{2})(m_{1}^{2}-m_{2}^{2})a^{3}f^{\prime}(a)f^{\prime \prime}(a)+\frac{(m_{1}^{2}-m_{2}^{2})^{2}}{4}a^{4}f^{\prime\prime 2}(a)\Big{]}\] \[= 2\beta^{2}a^{2}f^{\prime 2}(a)+\beta^{4}a^{4}f^{\prime\prime 2 }(a).\]

_Proof:_ Let \(\bm{m}_{1},\bm{m}_{2}\sim\mathcal{N}(\bm{0},\beta^{2}\bm{I}^{d^{\prime}})\). Note \(\bm{z}_{1}\) and \(\bm{z}_{2}\) are independent of each other, as well as independent of \(\bm{a}\) and \(f(\bm{a})\). The same independence holds for \(\bm{z}_{2}\). By taking expectations over \(\bm{z}_{1}\) and \(\bm{z}_{2}\), and applying Taylor expansion, we analyze the consistency loss of \(f(\bm{a})\):

\[\ell^{\text{NICE}}(\bm{a}):= \mathbb{E}_{\bm{z}_{1},\bm{z}_{2}}\big{[}\big{(}f(\bm{a}\odot\bm {z}_{1})-f(\bm{a}\odot\bm{z}_{2})\big{)}^{2}\big{]}\] \[= \mathbb{E}_{\bm{m}_{1},\bm{m}_{2}}\Big{[}\Big{(}f(\bm{a}+\bm{m}_{ 1}\odot\bm{a})-f(\bm{a}+\bm{m}_{2}\odot\bm{a})\Big{)}^{2}\Big{]}\] \[\approx \mathbb{E}_{\bm{m}_{1},\bm{m}_{2}}\Big{[}\Big{(}f(\bm{a})+(\bm{ m}_{1}\odot\bm{a})^{T}\nabla f(\bm{a})+\frac{1}{2}(\bm{m}_{1}\odot\bm{a})^{T}H^{(f )}(\bm{a})(\bm{m}_{1}\odot\bm{a})\] \[-f(\bm{a})-(\bm{m}_{2}\odot\bm{a})^{T}\nabla f(\bm{a})-\frac{1}{ 2}(\bm{m}_{2}\odot\bm{a})^{T}H^{(f)}(\bm{a})(\bm{m}_{2}\odot\bm{a})\Big{)}^{2} \Big{]}\] \[= \mathbb{E}_{\bm{m}_{1},\bm{m}_{2}}\Big{[}\Big{(}(\bm{m}_{1} \odot\bm{a})^{T}\nabla f(\bm{a})+\frac{1}{2}(\bm{m}_{1}\odot\bm{a})^{T}H^{(f )}(\bm{a})(\bm{m}_{1}\odot\bm{a})\] \[-(\bm{m}_{2}\odot\bm{a})^{T}\nabla f(\bm{a})-\frac{1}{2}(\bm{m}_ {2}\odot\bm{a})^{T}H^{(f)}(\bm{a})(\bm{m}_{2}\odot\bm{a})\Big{)}^{2}\Big{]}.\] (24)

Defining \(\bm{v}:=\bm{m}_{1}\odot\bm{a}\) and \(\bm{u}:=\bm{m}_{2}\odot\bm{a}\) with \(\bm{m}_{1}\) and \(\bm{m}_{2}\) are independently sampled from \(\mathcal{N}(\bm{0},\beta^{2}\bm{I}^{d^{\prime}})\), we get \(\bm{v},\bm{u}\sim\mathcal{N}(\bm{0},\beta^{2}\text{diag}(\bm{a}\odot\bm{a}))\). Here \(\text{diag}(\bm{a}\odot\bm{a})\) is a diagonal matrix with \(a_{k}^{2}\) on the diagonal. Letting \(\bm{\delta}:=\nabla f(\bm{a})\) and \(\bm{H}=H^{(f)}(\bm{a})\), we can rewrite Eq. 24 as follows:

\[\mathbb{E}_{\bm{v},\bm{u}}\big{[}\big{(}\bm{v}^{T}\bm{\delta}+ \frac{1}{2}\bm{v}^{T}\bm{H}\bm{v}-\bm{u}^{T}\bm{\delta}-\frac{1}{2}\bm{u}^{T} \bm{H}\bm{u}\big{)}^{2}\big{]}\] \[= \mathbb{E}_{\bm{v},\bm{u}}\big{[}\big{(}(\bm{v}-\bm{u})^{T}\bm{ \delta}\big{)}\big{(}\bm{v}^{T}\bm{H}\bm{v}-\bm{u}^{T}\bm{H}\bm{u}\big{)}\Big{]}\] (25) \[+\mathbb{E}_{\bm{v},\bm{u}}\Big{[}\big{(}(\bm{v}-\bm{u})^{T}\bm{ \delta}\big{)}\big{(}\bm{v}^{T}\bm{H}\bm{v}-\bm{u}^{T}\bm{H}\bm{u}\big{)}\Big{]}\] (26) \[+\frac{1}{4}\mathbb{E}_{\bm{v}}\big{[}(\bm{v}^{T}\bm{H}\bm{v})^{2} \big{]}+\frac{1}{4}\mathbb{E}_{\bm{u}}\big{[}(\bm{u}^{T}\bm{H}\bm{u})^{2} \big{]}\] (27) \[-\frac{1}{2}\mathbb{E}_{\bm{v},\bm{u}}\big{[}(\bm{v}^{T}\bm{H}\bm{ v})(\bm{u}^{T}\bm{H}\bm{u})].\] (28)

Next, we succinctly derive the four terms: the first (Eq. 25), second (Eq. 26), third (Eq. 27), and fourth (Eq. 28) term, respectively.

**The first term (Eq. 25):**

Leveraging the property \(\mathbb{E}_{z_{1},z_{2}}[(z_{1}-z_{2})^{2}]=2\sigma^{2}\) for \(z_{1},z_{2}\sim\mathcal{N}(0,\sigma^{2})\), we can simplify (Eq. 25) as follows:

\[\mathbb{E}_{\bm{v},\bm{u}}\Big{[}\big{(}(\bm{v}-\bm{u})^{T}\bm{ \delta}\big{)}^{2}\Big{]}=\mathbb{E}_{\bm{v},\bm{u}}\Big{[}\sum_{k=1}^{d^{ \prime}}(\bm{v}-\bm{u})_{k}^{2}\delta_{k}^{2}\Big{]}=\sum_{k=1}^{d^{\prime}} \mathbb{E}_{\bm{v},\bm{u}}\big{[}(\bm{v}-\bm{u})_{k}^{2}\big{]}\delta_{k}^{2}=2 \beta^{2}\sum_{k}a_{k}^{2}\delta_{k}^{2}.\] (29)

**The second term (Eq. 26):**

Utilizing \(E[z^{3}]=\mu^{3}+3\mu\sigma^{2}\) for \(z\sim\mathcal{N}(\mu,\sigma^{2})\), and noting that \(E[z^{3}]=0\) when \(\mu=0\) and elements in \(\bm{v}\) are independent, Eq. 26 is derived as:

\[\mathbb{E}_{\bm{v},\bm{u}}\Big{[}\big{(}(\bm{v}-\bm{u})^{T}\bm{ \delta}\big{)}\big{(}\bm{v}^{T}\bm{H}\bm{v}-\bm{u}^{T}\bm{H}\bm{u}\big{)}\Big{]}\] \[= \mathbb{E}_{\bm{v}}\big{[}(\bm{v}^{T}\bm{\delta})(\bm{v}^{T}\bm{H }\bm{v})\big{]}+\mathbb{E}_{\bm{u}}\big{[}(\bm{u}^{T}\bm{\delta})(\bm{u}^{T} \bm{H}\bm{u})\big{]}-\mathbb{E}_{\bm{v},\bm{u}}\big{[}(\bm{v}^{T}\bm{\delta})( \bm{u}^{T}\bm{H}\bm{u})\big{]}-\mathbb{E}_{\bm{v},\bm{u}}\big{[}(\bm{u}^{T} \bm{\delta})(\bm{v}^{T}\bm{H}\bm{v})\big{]}\] \[= 2\mathbb{E}_{\bm{v}}\big{[}(\bm{v}^{T}\bm{\delta})(\bm{v}^{T}\bm {H}\bm{v})\big{]}=2\mathbb{E}_{\bm{v}}\Big{[}\sum_{k}v_{k}\delta_{k}v_{k}H_{ kk}v_{k}\Big{]}=2\mathbb{E}_{\bm{v}}\Big{[}\sum_{j}v_{k}^{3}\delta_{k}H_{kk} \Big{]}=0.\] (30)

**The third term (Eq. 27):**

\[\frac{1}{4}\mathbb{E}_{\bm{v}}\big{[}(\bm{v}^{T}\bm{H}\bm{v})^{2} \big{]}+\frac{1}{4}\mathbb{E}_{\bm{u}}\big{[}(\bm{u}^{T}\bm{H}\bm{u})^{2}\big{]}\] \[= \frac{1}{2}\mathbb{E}_{\bm{v}}\big{[}(\bm{v}^{T}\bm{H}\bm{v})^{2} \big{]}=\frac{1}{2}\mathbb{E}_{\bm{v}}\Big{[}\sum_{j}\sum_{k}\sum_{p}\sum_{q}v _{j}H_{jk}v_{k}v_{p}H_{pq}v_{q}\Big{]}.\] (31)

Given the independence of elements in \(\bm{v}\), only terms with an element repeated two or four times contribute non-zero results, leading to four distinct, non-overlapping cases. Using \(\mathbb{E}[z^{2}]=\sigma^{2}+\mu^{2}\) and \(\mathbb{E}[z^{4}]=\mu^{4}+6\mu^{2}\sigma^{2}+3\sigma^{4}\) for \(z\sim\mathcal{N}(\mu,\sigma^{2})\), and simplifying to \(\mathbb{E}[z^{2}]=\sigma^{2}\) and \(\mathbb{E}[z^{4}]=3\sigma^{4}\) when \(\mu=0\), we have:

_Case 1:_\(j=k\neq p=q\), the independence of \(v_{j}\) and \(v_{p}\) simplifies our calculation, leading to:

\[\mathbb{E}_{\bm{v}}\Big{[}\sum_{j}\sum_{p\neq j}v_{j}^{2}H_{jj}v_{p}^{2}H_{pp }\Big{]}=\sum_{j,p\neq j}H_{jj}H_{pp}\mathbb{E}[v_{j}^{2}]\mathbb{E}[v_{p}^{2 }]=\beta^{4}\sum_{j,k\neq j}H_{jj}H_{kk}a_{j}^{2}a_{k}^{2}.\] (32)

_Case 2:_ For \(j=p\neq k=q\), given the independence of \(v_{j}\) and \(v_{k}\), we have:

\[\mathbb{E}_{\bm{v}}\Big{[}\sum_{j}\sum_{k\neq j}v_{j}H_{jk}v_{k}v_{j}H_{jk}v_{ k}\Big{]}=\sum_{j,k\neq j}H_{jk}^{2}\mathbb{E}[v_{j}^{2}]\mathbb{E}[v_{k}^{2 }]=\beta^{4}\sum_{j,k\neq j}H_{jk}^{2}a_{j}^{2}a_{k}^{2}.\] (33)

_Case 3:_ For \(j=q\neq k=p\), leveraging the independence of \(v_{j}\) and \(v_{k}\) as well as the symmetry \(H_{jk}=H_{kj}\), we obtain:

\[\mathbb{E}_{\bm{v}}\Big{[}\sum_{j}\sum_{k\neq j}\bm{v}_{j}H_{jk}v_{k}v_{k}H_{ kj}v_{j}\Big{]}=\sum_{j,k\neq j}H_{jk}^{2}\mathbb{E}[v_{j}^{2}]\mathbb{E}[v_{k}^{2 }]=\beta^{4}\sum_{j,k\neq j}H_{jk}^{2}a_{j}^{2}a_{k}^{2}.\] (34)

_Case 4:_ For \(j=q=k=p\), using \(\mathbb{E}[z^{4}]=3\sigma^{4}\) where \(z\sim\mathcal{N}(0,\sigma^{2})\), we deduce:

\[\mathbb{E}_{\bm{v}}\Big{[}\sum_{j}v_{j}H_{jj}v_{j}v_{j}H_{jj}v_{j}\Big{]}=\sum _{j}H_{jj}^{2}\mathbb{E}[v_{j}^{4}]=\beta^{4}\sum_{j}3H_{jj}^{2}a_{j}^{4}.\] (35)

Combining Cases 1-4 together, we arrive at for Eq. 27:

\[\frac{\beta^{4}}{2}\Big{(}\sum_{j}3H_{jj}^{2}a_{j}^{4}+\sum_{j,k\neq j}(H_{jj} H_{kk}+2H_{jk}^{2})a_{j}^{2}a_{k}^{2}\Big{)}.\] (36)

**The forth term (Eq. 28):**

\[-\frac{1}{2}\mathbb{E}_{\bm{v},\bm{u}}\big{[}(\bm{v}^{T}\bm{H} \bm{v})(\bm{u}^{T}\bm{H}\bm{u})\big{]}\] \[= -\frac{1}{2}\Big{(}\mathbb{E}_{\bm{v}}\Big{[}\sum_{j}H_{jj}v_{j}^ {2}\Big{]}\mathbb{E}_{\bm{u}}\Big{[}\sum_{k}H_{kk}v_{k}^{2}\Big{]}\Big{)}\] \[= -\frac{1}{2}\Big{(}\sum_{j}H_{jj}\mathbb{E}[v_{j}^{2}]\Big{)} \Big{(}\sum_{k}H_{kk}\mathbb{E}[v_{k}^{2}]\Big{)}\] \[= -\frac{\beta^{4}}{2}\Big{(}\sum_{j}H_{jj}^{2}a_{j}^{4}+\sum_{j,k \neq j}H_{jj}H_{kk}a_{j}^{2}a_{k}^{2}\Big{)}.\] (37)Substitute Eq. 29, 30, 36, 37 back into Eq. 25, 26, 27, 28, we have the final results:

\[\mathbb{E}_{\bm{z}_{1},\bm{z}_{2}}\big{[}\big{(}f(\bm{z}_{1}\odot \bm{a})-f(\bm{z}_{2}\odot\bm{a})\big{)}^{2}\big{]}\] \[\approx 2\beta^{2}\sum_{k}a_{k}^{2}\delta_{k}^{2}+0\] \[+\frac{\beta^{4}}{2}\Big{(}\sum_{j}3H_{jj}^{2}a_{j}^{4}+\sum_{j,k \neq j}(H_{jj}H_{kk}+2H_{jk}^{2})a_{j}^{2}a_{k}^{2}-\sum_{j}H_{jj}^{2}a_{j}^{4} -\sum_{j,k\neq j}H_{jj}H_{kk}a_{j}^{2}a_{k}^{2}\Big{)}\] \[= 2\beta^{2}\sum_{k}a_{k}^{2}\delta_{k}^{2}+\beta^{4}\Big{(}\sum_ {j}H_{jj}^{2}a_{j}^{4}+\sum_{j,k\neq j}H_{jk}^{2}a_{j}^{2}a_{k}^{2}\Big{)}\] \[= 2\beta^{2}\sum_{k}a_{k}^{2}\delta_{k}^{2}+\beta^{4}\sum_{j}\sum _{k}H_{jk}^{2}a_{j}^{2}a_{k}^{2}\] \[= 2\beta^{2}\sum_{k}a_{k}^{2}\nabla_{k}^{2}f(\bm{a})+\beta^{4} \sum_{j}\sum_{k}a_{j}^{2}a_{k}^{2}(H_{jk}^{(f)}(\bm{a}))^{2}.\] (38)

Enforcing the consistency regularization implicitly penalizes the first- and second-order gradients of \(f\) w.r.t. \(\bm{a}\), with an adaptive penalty controlled \(\beta\) and modulated by \(\bm{a}\). This modulation recalibrates the squared norms of gradient: the squared norm of the first-order gradient, \(\|\nabla f(\bm{a})\|_{2}^{2}=\sum_{k}\nabla_{k}^{2}f(\bm{a})\), and the Frobenius norm of the Hessian, \(\|H^{(f)}(\bm{a})\|_{F}^{2}=\sum_{j}\sum_{k}(H_{jk}^{(f)}(\bm{a}))^{2}\). Specifically, each term is reweighted by \(a_{k}^{2}\geq 0\), ensuring that the penalty applied to each gradient component is scaled proportionally to its associated magnitude of feature.

**Corollary 2**: _Enforcing invariance of \(f\) under different additive noise injections, where \(\bm{z}_{1},\bm{z}_{2}\sim\mathcal{N}(\bm{0},\beta^{2}\bm{I}^{d^{\prime}})\) leads to the gradient regularization as:_

\[\mathbb{E}_{\bm{z}_{1},\bm{z}_{2}}\big{[}\big{(}f(\bm{z}_{1}+\bm{a})-f(\bm{z} _{2}+\bm{a})\big{)}^{2}\big{]}=2\beta^{2}\sum_{k}\nabla_{k}^{2}f(\bm{a})+ \beta^{4}\sum_{j}\sum_{k}(H_{jk}^{(f)}(\bm{a}))^{2}.\] (39)

## Appendix D Implementation details for Experiments

Following previous benchmarks [68, 22], we augmented all datasets with the simple \(x\)-flips augmentation.

### Network architecture and hyperparameters

**CIFAR-10/100**. We experiment with OmniGAN (\(d^{\prime}=256,1024\)) and BigGAN (\(d^{\prime}=256\)) with the batch size of 32. We follow [68] and train the OmniGAN and BigGAN for 1K epochs on the full data and 5K epochs on 10%/20% data setting. We equip the discriminator with the adaptive noise modulation after convolution weights \(c\in\{C_{1},C_{2},C_{S}\}\) at all blocks \(l\in\{1,2,3,4\}\). We set \(\Delta_{\beta}=0.001,\eta=0.5,\Delta_{\gamma}=10\). Features before loss function are used for NICE on BigGAN.

**ImageNet**. We experiment with BigGAN with the batch size of 512. We use a learning rate of 1e-4 for generator and 4e-4 for discriminator. The noise modulation is placed after convolution layers \(c\in\{C_{1},C_{2},C_{S}\}\) at blocks \(l\in\{3,4,5\}\), \(\Delta_{\beta}=0.001,\eta=0.5,\Delta_{\gamma}=5\). Features before loss function are used for NICE.

**Low-shot images.** We build our NICE upon StyleGAN2 with batch size of 64 and train the networks until the discriminator had seen 25M images. We apply noise modulation with Bernoulli noise after convolutions \(c\in\{C_{1},C_{2}\}\) at blocks \(l\in\{3,4,5,6\}\), \(\Delta_{\beta}=0.0001,\eta=0.9,\Delta_{\gamma}=0.05\)

**FFHQ**. We experiments on StyleGAN2 with batch size of 64 and train the networks until the discriminator had seen 25M images. We apply noise modulation with Bernoulli noise after convolutions \(c\in\{C_{1},C_{2}\}\) at blocks \(l\in\{3,4,5,6\}\), \(\Delta_{\beta}=0.0001,\eta=0.6,\Delta_{\gamma}=0.05\)

### Implementation details for AWD, AN+AGP, and AACR

In Figure 5, we compare NICE with three different approaches: adaptive weight decay (AWD), adaptive noise + adaptive gradient penaulty (AN+AGP) and adaptive augmentation-based consistency regularization (AACR). For AWD, we dynamically control the weight decay using \(\beta\lambda_{\text{WD}}\). We search 

[MISSING_PAGE_EMPTY:21]

In contrast, the _additive noise modulation_ results in the standard penalty \(\beta^{2}\sum_{k}\lVert\bm{w}_{k}\rVert_{2}^{2}\):

\[\hat{L}_{\text{add\_noise}}: =\hat{\mathbb{E}}_{i}\mathbb{E}_{\bm{z}\sim\mathcal{N}(\bm{0}, \beta^{2}\bm{I})}\lVert\bm{y}^{(i)}-\bm{W}_{2}(\bm{z}+\bm{a}^{(i)})\rVert_{2}^ {2}\] \[=\hat{\mathbb{E}}_{i}\lVert\bm{y}^{(i)}-\bm{W}_{2}\bm{a}^{(i)} \rVert_{2}^{2}+\beta^{2}\sum_{k}\lVert\bm{w}_{k}\rVert_{2}^{2}.\]

While obvious, \(\beta^{2}\sum_{k}\lVert\bm{w}_{k}\rVert_{2}^{2}\) does not enjoy the _dynamic penalty_ term related to feature norms. This also means (based on the derivation) that _the additive noise may change feature semantics, i.e., \(\bm{z}+\bm{a}^{(i)}\)_ may "activate" channels which are non-active in \(\bm{a}^{(i)}\) (features \(\bm{a}^{(i)}=0\)). Therefore, _our multiplicative modulator not only controls the Rademacher Complexity (RC), but controls it in a meaningful manner for discriminator_. Only feature semantics that are present in a feature vector (that describe an object) are modulated while helping control RC. In contrast, the additive noise introduces semantics that are not present for a given image or object.

While directly regularizing the network using \(\beta^{2}\sum_{k}\lVert\bm{w}_{k}\rVert_{2}^{2}\) does not introduce the additive noise, the explicit regularization effect is similar to introducing the additive noise. Thus, if we connect the direct weight regularization to the additive noise, it will also have the effect of changing the semantics, which will negatively impact the classification accuracy.

We provide experimental comparisons in Table 8. In addition, Figure 7 shows that AN achives higher classification accuracy than AAN and AWR, demonstrating that multiplicative noise preserves the feature semantics better than the additive noise and the weight regularization.

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Equation} & \multicolumn{3}{c}{10\% CIFAR-10} & \multicolumn{3}{c}{10\% CIFAR-100} \\ \cline{3-8}  & & IS \(\uparrow\) & tFID \(\downarrow\) & vFID \(\downarrow\) & IS \(\uparrow\) & tFID \(\downarrow\) & vFID \(\downarrow\) \\ \hline OmniGAN & & 8.49 & 22.24 & 26.33 & 8.19 & 45.41 & 50.33 \\ +AAN & \(\bm{a}^{(i)}:=\bm{a}^{(i)}+\bm{z};\bm{z}\sim\mathcal{N}(\bm{0},\beta^{2}\bm{I})\) & 8.52 & 20.12 & 24.65 & 9.64 & 37.68 & 42.01 \\ +AWR & \(\beta^{2}\sum_{k}\lVert\bm{w}_{k}\rVert_{2}^{2}\) & 8.44 & 18.42 & 22.56 & 9.80 & 32.05 & 36.53 \\ +AN & \(\bm{a}^{(i)}:=\bm{a}^{(i)}\odot\bm{z};\bm{z}\sim\mathcal{N}(\bm{1},\beta^{2}\bm{ I})\) & \(\bm{9.16}\) & \(\bm{10.14}\) & \(\bm{13.80}\) & \(\bm{11.22}\) & \(\bm{23.76}\) & \(\bm{28.34}\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Results for different methods on 10% CIFAR-10/100 using OmniGAN (\(d^{\prime}=256\)). AAN (adaptive additive noise), AWR (adaptive weight regularization), AN (our adaptive multiplicative noise). Please note the consistency loss and dual branch from Figure 1 are not used here as that would result in additional penalties on gradient norms.

Figure 7: Classification accuracy of different methods. We input test images into the classifier of discriminator and assess its accuracy in correctly categorizing the images into their true classes (out of 10 categories in CIFAR-10). All other methods have lower accuracy than the baseline OmniGAN, but NICE preserves the classifier accuracy.

Rationale behind choosing NICE over the direct gradient regularization

Consider the regularization term in Theorem 2 and its Taylor expansion. Indeed, this expression penalizes the squared norms of first- and second-order gradients of \(f(\bm{a})\). However, notice that our expression has specific reweighting effect for penalizing the first- and second-order gradients. The specific gradient penalties

\[2\beta^{2}\sum_{k}a_{k}^{2}\nabla_{k}^{2}f(\bm{a})+\beta^{4}\sum_{j,k}a_{j}^{2} a_{k}^{2}(H_{jk}^{(f)}(\bm{a}))^{2}\] (40)

emerge only in case of using the multiplicative noise drawn from \(\mathcal{N}(\bm{1},\beta^{2}\bm{I})\) for Theorem 2. Importantly, the multiplicative noise \(\bm{z}\sim\mathcal{N}(\bm{1},\beta^{2}\bm{I})\) does not introduce new semantics into feature vector \(\bm{a}\) by operation \(\bm{z}\odot\bm{a}\), _i.e._, only feature semantics that are present in feature vector (features of \(\bm{a}\) that are non-zero) that describe object are modulated. Yet, this multiplicative noise does help control the Rademacher Complexity (RC) due to Lemma 2 and Theorem 1. However, imposing a generic penalty of form \(\beta(\sum_{k}\nabla_{k}^{2}f(\bm{a})+\sum_{j,k}(H_{jk}^{(f)}(\bm{a}))^{2}\) that arises when the additive noise \(\bm{z}\sim\mathcal{N}(\bm{0},\beta^{2}\bm{I})\) is applied to \(\bm{a}^{(i)}\), _i.e._, \(\bm{z}+\bm{a}\).

Let \(\bm{z}\sim\mathcal{N}(\bm{0},\beta^{2}\bm{I})\). The consistency loss regularization with the additive noise injection is:

\[\mathbb{E}_{\bm{z}_{1},\bm{z}_{2}}\big{[}\big{(}f(\bm{z}_{1}+\bm{a})-f(\bm{z}_ {2}+\bm{a})\big{)}^{2}\big{]}=2\beta^{2}\sum_{k}\nabla_{k}^{2}f(\bm{a})+\beta ^{4}\sum_{j}\sum_{k}(H_{jk}^{(f)}(\bm{a}))^{2}.\]

Such a variant means that semantics that are not present in \(\bm{a}\) may be "activated" by the noise, drastically altering the meaning of \(\bm{a}\) and damaging the information it carries about image/object.

While directly applying gradient penalization does not introduce noise, its connection to the additive noise suggests it will have a negative effect on semantics. Table 8 evaluates the multiplicative _vs_. additive noise modulators, and the direct penalties. Table 9 provides experimental results for different gradient regularization variants. Furthermore, Figure 7 shows that NICE obtains best accuracy than other variants, showing that the multiplicative noise modulation preserves the semantics better than other variants, justifying the rationale behind NICE.

## Appendix H Training overhead with and without NICE

We provide number of parameters/multiply-accumulate (MACs) (for both generator and discriminator), number of GPUs and cost of time (seconds per 1000 images, secs/\(k\)img) in Table 10. With the efficient implementation (Figure 1), our NICE only introduces small fraction of cost of time on high-resolution datasets. 15.08% on ImageNet and 18.02% on FFHQ/low-shot datasets. Figure 8 illustrates the FID gain _vs_. speed, which shows that a slightly increase in training time yields substantial improvements, particularly when compared to recent state-of-the-art approaches like FakeCLR [32] and InsGen [63]. We believe the increased computational overhead is outweighed by the considerable benefits and the improvements justify the extra fraction of time.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{10\% CIFAR-10} & \multicolumn{3}{c}{10\% CIFAR-100} \\ \cline{2-7} Method & IS \(\uparrow\) & tFID \(\downarrow\) & vFID \(\downarrow\) & IS \(\uparrow\) & tFID \(\downarrow\) & vFID \(\downarrow\) \\ \hline OmniGAN & 8.49 & 22.24 & 26.33 & 8.19 & 45.41 & 50.33 \\ +ALGP & 8.52 & 19.15 & 22.72 & 9.18 & 32.98 & 37.51 \\ +AWR+ALGP & 8.72 & 16.82 & 20.45 & 10.14 & 26.44 & 30.23 \\ +NICE\({}_{add}\) & 8.64 & 17.94 & 21.59 & 9.34 & 28.59 & 33.02 \\ +NICE & **9.26** & **7.23** & **11.08** & **11.50** & **16.91** & **21.56** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Results for ALGP (adaptive latent gradient penalization), NICE\({}_{add}\) (consistency regularization with additive noise), AWR (adaptive weight regularization) on 10% CIFAR-10/100 using OmniGAN (\(d^{\prime}=256\)).

[MISSING_PAGE_FAIL:24]

Figure 11: Generated images using (a) DA, (b) ADA and (c) NICE+ADA on 10% CIFAR-10 using OmniGAN (\(d^{\prime}{=}1024\)). Note that DA leaks cutout augmentation and ADA leaks the rotation augmentation.

Figure 10: The gradient norm \(\|\frac{\partial L_{D}}{\partial\bm{x}}\|_{2}^{2}\) of ADA and ADA+NICE.

Figure 9: Generated images using (a) DA (differentiable augmentation [68]), (b) ADA (adaptive differentiable augmentation [22]), (c) AACR (adaptive augmentation-based consistency regularization) and (d) NICE+ADA on 10% CIFAR-10 using OmniGAN (\(d^{\prime}{=}256\)) [71] We use red boxes to bound the images that leak the augmentation. DA leaks the cutout augmentation. ADA and AACR leaks the rotation augmentation. Combining NICE with ADA prevents the leakage of augmentation.

Figure 12: Generated images using (a) ADA and (c) ADA+NICE on 2.5% ImageNet using BigGAN. ADA struggles to capture the semantics of images, whereas NICE provides better image quality.

Figure 13: Generated images using (a) DA, (b) ADA and (c) NICE+ADA on 10% CIFAR-100 using OmniGAN (\(d^{\prime}{=}1024\)). We present the last 40 classes out of the 100 classes 40. Note that DA leaks cutout augmentation clues (row 7 and row 21) and ADA generates images lacking diversity, while NICE+ADA provides better visual quality.

Figure 14: Qualitative comparison between ADA [22] and ADA+NICE on 100-shot and AnimalFace datasets. Adding NICE clearly improves the image quality.

Figure 15: Qualitative comparison between ADA [22] and ADA+NICE on FFHQ dataset. Images are generated without truncation [25]. Adding NICE can provide better visual quality.