# Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation

 Claude Formanek\({}^{1,2}\)\({}^{*}\) &Callum Rhys Tilbury\({}^{1}\) &Louise Beyers\({}^{1}\)

&Jonathan Shock\({}^{2,3,4}\) &Arnu Pretorius\({}^{1}\)

\({}^{1}\)InstaDeep \({}^{2}\)University of Cape Town \({}^{3}\)INRS, Montreal \({}^{4}\)NITheCS, Stellenbosch

Corresponding author: c.formanek@instadeep.com

###### Abstract

Offline multi-agent reinforcement learning (MARL) is an emerging field with great promise for real-world applications. Unfortunately, the current state of research in offline MARL is plagued by inconsistencies in baselines and evaluation protocols, which ultimately makes it difficult to accurately assess progress, trust newly proposed innovations, and allow researchers to easily build upon prior work. In this paper, we firstly identify significant shortcomings in existing methodologies for measuring the performance of novel algorithms through a representative study of published offline MARL work. Secondly, by directly comparing to this prior work, we demonstrate that simple, well-implemented baselines can achieve state-of-the-art (SOTA) results across a wide range of tasks. Specifically, we show that on \(35\) out of \(47\) datasets used in prior work (almost \(75\)% of cases), we match or surpass the performance of the current purported SOTA. Strikingly, our baselines often substantially outperform these more sophisticated algorithms. Finally, we correct for the shortcomings highlighted from this prior work by introducing a straightforward standardised methodology for evaluation and by providing our baseline implementations with statistically robust results across several scenarios, useful for comparisons in future work. Our proposal includes simple and sensible steps that are easy to adopt, which in combination with solid baselines and comparative results, could substantially improve the overall rigour of empirical science in offline MARL moving forward.

## 1 Introduction

Offline reinforcement learning (RL) attempts to derive optimal sequential control policies from static data alone, without access to online interactions (e.g. a simulator). Though the single-agent variant has received fairly widespread research attention (Prudencio et al., 2023), progress in the multi-agent context has been slower, due to a variety reasons. For one, multi-agent problems are fundamentally more difficult, and bring a host of new challenges--including difficulties in coordination (Barde et al., 2024), large joint-action spaces (Yang et al., 2021), heterogeneous agents (Zhong et al., 2024) and non-stationarity (Papoudakis et al., 2019), which are absent in single-agent situations.

Nonetheless, some progress has been made in offline multi-agent reinforcement learning (MARL) in recent years. In particular, in better understanding the aforementioned difficulties of offline learning in the multi-agent setting and proposing certain remedies for them (Jiang and Lu, 2021; Yang et al., 2021; Pan et al., 2022; Wang et al., 2023; Shao et al., 2023; Tian et al., 2023; Meng et al., 2023).

However, we argue that this progress might be a mirage and that offline MARL research is ultimately being held back by a lack of clarity and consistency in baseline implementations and evaluation protocols. To support this claim, we demonstrate in Figure 1 a surprising but telling result--we show that good implementations of straightforward baseline algorithms can achieve state-of-the-art performance across a wide-range of tasks, beating several published works claiming such a title. We view our analysis as robust, using datasets and environments with experimental settings that exactly match prior work (see Section 3).

This paper proceeds as follows. We first assess the state of the field, diagnosing the key points of friction for progress in offline MARL. Thereafter, we describe in more detail how well-implemented baseline methods perform surprisingly well compared to leading methods from the literature, suggesting that algorithmic progress has not advanced at the rate perhaps previously perceived by the community. In response, we introduce a standardised baseline and evaluation methodology, in an effort to support the field towards being more scientifically rigorous. We hope that researchers will build upon this work, advocating for a cleaner, more reproducible and robust empirical science for offline MARL.

## 2 Methodological Problems in Offline MARL

In this section, we briefly assess the state of offline MARL research by focusing on the baselines and evaluation protocols commonly employed. We consider the following five papers, all published at top-tier venues, for our case study: MAICQ (Yang et al., 2021), OMAR (Pan et al., 2022), MADT (Meng et al., 2023), CFCQL (Shao et al., 2023), and OMIGA (Wang et al., 2023). Given the nascency of the field, we consider these papers to serve as a good representative sample of the current trends in offline MARL research. By looking at this cross-section, we can assess the current methodologies for measuring progress. We present our findings below.

Figure 1: We compare our baseline implementations to the reported performance of various algorithms from the literature across a wide range of datasets. We normalise results from each dataset (i.e. scenario-quality-source combination) by the SOTA performance from the literature for that dataset. Standard deviation bars are given and when our baseline is significantly better or equal to the best method, using a two-side t-test, we indicate so using a gold star. **We find that on 35 out of the 47 datasets tested (almost 75% of cases), we match or surpass the performance of the current SOTA.**

Ambiguity in the Naming of Baseline AlgorithmsIn single-agent RL, the naming of a given algorithm is fairly unambiguous: it is widely understood what core algorithmic steps constitute, e.g., DDPG (Lillicrap et al., 2016). Yet, in MARL, algorithms become more complex, since one must specify how multiple agents should learn and interact. Unlike in the single agent case, there is ambiguity when referring to _multi-agent_ DDPG--for this might be referring to MADDPG (Lowe et al., 2017), or independent DDPG agents, or perhaps some other way of interleaving training and/or execution with DDPG forming the base of the algorithm's design. The corresponding impact on the performance of such choices can be significant (Lyu et al., 2021), and thus it is important to be as explicit as possible.

Clarity in this naming has been lacking in offline MARL literature. For instance, we consider Conservative Q-Learning (CQL) (Kumar et al., 2020) as a prime example of this problem. As an influential single-agent offline RL algorithm, CQL is critical to consider as a baseline when proposing new work in the field. Whereas the core CQL algorithmic steps are well-established, though, there does not exist a common understanding in the literature of what _multi-agent_ CQL is, despite widespread appearance of the abbreviation, MACQL, and its permutations. Consider Table 1, which shows how the same baseline method is purportedly included in each of the papers in our case study, yet the details provided for this algorithm are sparse, often lacking publicly available code, and can vary dramatically across papers.

To highlight these discrepancies and their effects more clearly, we note that the authors of each paper compare their proposed method with "MACQL" and claim superior performance. But _which_ MACQL is being compared, has a significant bearing on the degree to which these conclusions are likely to hold. For example, consider the following simple experiment. We compare two viable candidates for MACQL across three SMACv1 (Samvelyan et al., 2019) maps (8m, 2s3z, 5m v 6m). Specifically, we compare MADDPG (Lowe et al., 2017) with the Gumbel-Softmax (Jang et al., 2016), against QMIX (Rashid et al., 2018), each with the addition of CQL. Importantly, note that in both cases, we could present such an algorithm as "MACQL" as used in prior work. However, the results in Figure 2 clearly reveal their relative differences in performance. Here we report the median, mean and interquartile mean (IQM) as recommended by Agarwal et al. (2022). We notice, too, that the outcome changes depending on the scenario used--MADDPG with CQL outperforms QMIX on 8m, whereas the ordering is reversed on 5m_vs_6m.

Mismatches in Scenarios Used for ComparisonIn addition to the ambiguity in baselines used for comparison, there is also a confusing mismatch in the selected scenarios used across prior work. Quite

\begin{table}
\begin{tabular}{c c c c}
**Paper** & **Name for MACQL** & **Implementation Details Provided in the Paper** & **Is the Full Code Available?** \\ \hline Yang et al. (2021) & MA-CQL & Loss function, value-decomposition structure & No, only single-agent CQL \\ \hline Pan et al. (2022) & MA-CQL & MADDPG (Lowe et al., 2017) & Only for continuous settings \\  & & (Discrete: + Gumbel Softmax (Jang et al., 2016)) & No \\ \hline Meng et al. (2023) & CQL-MA & CQL + “mixing network” & No \\ \hline Shao et al. (2023) & MACQL & “Naive extension of CQL to multi-agent settings” + Loss function & Yes \\ \hline Wang et al. (2023) & CQL-MA & Value decomposition structure + policy constraint & No \\ \hline \end{tabular}
\end{table}
Table 1: Demonstration of how papers in our case study are essentially using the same name for Multi-Agent CQL, for markedly different algorithms, often providing only sparse information about their implementations.

Figure 2: Comparing the performance of QMIX+CQL and MADDPG+CQL, two algorithms that could reasonably be called MACQL in the literature (see Table 1), using the Medium dataset from three different SMACv1 scenarios. We see that the difference in performance of these algorithms is significant, and depends on the scenario considered.

[MISSING_PAGE_FAIL:4]

is specified and training budget is considered an important hyperparameter, researchers can avoid unintentionally cherry-picking results.

The lack of consistency, transparency, and completeness of evaluation procedures in offline MARL slows progress by forcing researchers to perform expensive re-evaluations of baselines for their comparisons. But perhaps most damaging is the inability to compare and build upon prior work. This allows the field to maintain a mirage of steady progress, while in reality, algorithms are not becoming materially better.

## 3 Reevaluating the Empirical Evidence from Prior Work

Given the problems of baseline and evaluation methodology in offline MARL, we now revisit the empirical evidence from prior work through an independent standardised study.

Arguably, the gold standard for such a study would use the exact datasets from the authors and their original code (note, even this approach may have drawbacks such as potentially perpetuating poor experimental design and algorithm implementations). Alternatively, we could obtain a selection of similar yet distinct datasets (e.g. those from Formanek et al. (2023)), and use existing code implementations from the respective authors for each algorithm considered. Unfortunately, this approach often proves to be infeasible. In some works, only parts of the proposed algorithm code are shared (e.g. Pan et al. (2022) only share code for their method in continuous action spaces), and in many works, code for the baseline algorithms is omitted completely. As another alternative, we could decide to use our _own_ algorithm implementations and datasets, however, this would put us in similar territory as prior work with regards to drawing concrete conclusions. As the most sensible middle ground, we do the following: we use the exact same datasets as provided in prior work, but train our own _baseline_ implementations on these datasets; for the results of the other algorithms, we extract the values exactly as they are given in their respective papers. As an illustrative example, suppose we are comparing our implementation of MADDPG with CQL against the results from the OMIGA paper (Wang et al., 2023), in one of the scenarios from MAMuJoCo (Peng et al., 2021). Here, we take the _datasets_ provided by Wang et al. (2023), train our MADDPG+CQL algorithm, and compare these results to the tabular values reported by Wang et al. (2023) themselves. We feel this methodology is the fairest to the original authors, especially in the situation where the publicly available code is lacking and/or broken. We also view the task of implementing our own baselines, instead of attempting to re-implement the author's proposed algorithm, as a more faithful exercise.

Nonetheless, this approach still has challenges, for it requires access to the datasets used by other authors. Regrettably, in some cases we could not access this data, either because it was never made publicly available, or because the provided download links were broken and multiple attempts to reach the original authors were unsuccessful. In Table 4, we summarise our dataset access record, across the papers in the case study.

The next challenge is that several works use modified versions of environments to generate their datasets, and to evaluate their algorithms. For example, Wang et al. (2023) modify the MAMuJoCo environment such that agents all receive a global observation of the environment, rather than decentralised, partial observations as is standard. They also use a different version of SMACv1, seemingly

Figure 3: A comparison of the performance of behaviour cloning (BC) and QMIX+CQL on the SMACv1 8m scenario with the Medium dataset, across 10 seeds. Although QMIX+CQL outperforms BC during the first half of training, its performance deteriorates in the second half, making BC the preferred algorithm over the maximum training time.

first modified by Yu et al. (2022), that has important differences from the original. Similarly, Pan et al. (2022) include their own code for the Multi Particle Environments (MPE), which differs from the standardised and maintained version in PettingZoo (Terry et al., 2021). As a consequence, several datasets from different papers _seem_ to share a common environment, but in reality do not (e.g., the 5m_vs_6m datasets generated by Wang et al. (2023) are not compatible with the 5m_vs_6m datasets from Shao et al. (2023)). To facilitate fair comparisons to each of the original works, we re-use the respective unique environment configuration, even when this is different from the standard. _We do not advocate this approach for future work_ and note that standardisation is crucial going forward (which we discuss in more detail in the next section).

We use four baselines for our experiments--two for discrete action spaces, and two for continuous. In the discrete case, we implement BC, and independent Q-learners (IQL) (Tampuu et al., 2017) with CQL regularisation to stabilise offline training. Notably, these are very straightforward baselines that do not rely on any value factorisation or global state information. In continuous action spaces, we use independent DDPG agents with behaviour cloning regularisation--a naive multi-agent extension of the algorithm by Fujimoto and Gu (2021), which notably only requires a single line to change in our vanilla implementation of independent DDPG. Finally, we also test MADDPG (Lowe et al., 2017) with CQL to stabilise offline critic learning. Interestingly, this baseline is used in multiple works (Pan et al., 2022; Wang et al., 2023), but its reported performance is poor.

We train our baselines on MPE, SMACv1, and MAMuJoCo scenarios for \(50k\), \(100k\), and \(200k\) training updates, respectively. At the end of training, we compute the mean episode return over 32 episodes and repeat each run across 10 independent random seeds. We avoid fine-tuning our algorithms on each scenario independently in an attempt to control for the online tuning budget (Kurenkov and Kolesnikov, 2022), and instead keep the hyperparameters fixed across each respective scenario.

ResultWe provide all the experimental results in tabular form in the appendix. From our own training results, we provide the mean episode return with the standard deviation across 10 independent seeds. As stated above, we extract values verbatim from prior work for other algorithms that are being compared. We perform a simple heteroscedastic, two-sided t-test with a 95% confidence interval for testing statistical significance, following Papoudakis et al. (2021). We summarise the tabulated results in an illustrative plot in Figure 1 (plotting our best baseline per dataset). To standardise comparisons, we normalise results from each dataset (i.e. scenario-quality-source combination) by the SOTA performance from the literature for that dataset. When our method is significantly better or equal to the best method in the literature, we indicate so using a star. We find that on \(35\) out of the \(47\) datasets tested, we match or surpass the performance of the current SOTA from the literature.2

Footnote 2: Code used to process results is available in a notebook: https://tinyurl.com/offline-marl-meta-review

## 4 Standardising Baselines and Evaluation

The outcome from our benchmarking exercise paints a worrying picture of the state of offline MARL. We maintain that most of the contributions made by the research community to date are valuable.

\begin{table}
\begin{tabular}{l|l l l c}
**Paper** & **Environment** & **Number of Datasets** & **Accessibility** & **Benchmarked** \\ \hline Yang et al. (2021) & SMACv1 & 4 & Link broken & \(\bullet\) \\ \hline Pan et al. (2022) & SMACv1 & 4 & Not available & \(\bullet\) \\  & MAMuJoCo & 4 & Yes & \(\checkmark\) \\  & MPE & 12 & Yes & \(\checkmark\) \\ \hline Meng et al. (2023) & SMACv1 & 62 & Download fails & \(\bullet\) \\ \hline Shao et al. (2023) & SMACv1 & 16 & Yes & \(\checkmark\) \\  & MAMuJoCo & 4 & Yes, from Pan et al. (2022) & \(\checkmark\) \\  & MPE & 12 & Yes, from Pan et al. (2022) & \(\checkmark\) \\ \hline Wang et al. (2023) & SMACv1 & 12 & Yes & \(\checkmark\) \\  & MAMuJoCo & 12 & Yes & \(\checkmark\) \\ \hline \end{tabular}
\end{table}
Table 4: Summary of dataset accessibility and whether we benchmarked our baselines on them.

However, because several works seem to be building upon unreliable baselines and using inconsistent evaluation protocols, the overall value to the community is diminished. We believe the community will be better served if we adopt a common set of datasets, baselines, and evaluation methodologies.

DatasetsWith regards to common datasets, OG-MARL (Formanek et al., 2023) includes a wide range of offline MARL datasets which the community has begun to adopt (Zhu et al., 2023; Yuan et al., 2023). We find that a notable advantage of OG-MARL datasets, aside from their ease of accessibility, is that they are generated on the standard environment configurations rather than customised ones. This significantly eases the challenge of matching datasets to environments, as highlighted in Section 3. Having said that, we also believe there is value in improving access to the datasets from prior works, which we used here for benchmarking. Thus, we convert all of the datasets available to us from the literature (see Table 4) to the OG-MARL dataset API to make them more easily available to the community, in one place. We include statistical profiles of the datasets in the appendix and give credit to the original creators.

BaselinesWhile notable progress has been made standardising offline MARL datasets, we maintain that inconsistent use of baselines remains an overlooked issue in the field. Indeed, to highlight this, we conducted extensive benchmarking in Section 3. Now, to address the issue, we release our implementations of BC, IQL+CQL, IDDPG+BC, and MADDPG+CQL, as high-quality baselines for future research. Our baselines come with three main advantages. First, their demonstrated correctness, where we have shown they perform as well as, or better than, most algorithms in the literature on a wide range of datasets. Second, our baselines are easy to parse while also being highly performant on hardware accelerators. The core algorithm logic of our baselines is contained within a single file, which makes it easy for researchers to read, understand, and modify. In addition, all of the training code can be jit (just-in-time) compiled to XLA, making it very fast on hardware accelerators. Furthermore, we use the hardware accelerated replay buffers from Flashbax (Toledo et al., 2023), delivering performance gains by speeding up the time to sample from datasets. The third advantage is their compatibility with OG-MARL datasets, which comes "out of the box", offering the widest compatibility with offline MARL datasets in the literature 3. As a foundation for future work, we provide tabular performance values across multiple scenarios for these algorithms, in Table 5. Furthermore, all raw training results can be viewed and downloaded, and are linked to in the appendix.

Footnote 3: Datasets and baselines can be accessed at https://github.com/instadeepai/og-marl

EvaluationRecent efforts have been made to address the issue in the online setting (Gorsane et al., 2022; Agarwal et al., 2022). However, similar efforts are still absent in the offline setting, as discussed in Section 2. Following in the spirit of this work, we propose a set of evaluation guidelines for offline MARL, given in the blue box on Page \(8\), which we believe will significantly improve research outcomes, if adopted.

Figure 4: Performance profiles (Agarwal et al., 2022) aggregated across all results from Table 5 on SMACv1 and MAMuJoCo. Scores are normalised as per Fu et al. (2021).

**Choosing the settings to evaluate on:**

* Select at least 2-3 different environments on which to test. For example, evaluating on both SMAC and MAMuJoCo is the most common combination. We encourage additionally evaluating on environments beyond these two, to avoid overfitting to them. Do not use non-standard environment configurations without explicitly stating so.
* For each environment choose at least 3-4 different scenarios. If the environment creators provide a minimal set of recommended scenarios, use those. Alternatively, focus on scenarios that are common in prior literature.
* Choose a range of dataset quality types. We recommend at least including a "good" dataset where the majority of samples are from good policies and a "mixed" dataset where samples come from a wide range of policies including good, medium and poor ones.
* Try to use common existing datasets from the literature (Formanek et al., 2023). If you include your own dataset, provide a clear reason for why, and make it easily accessible to the community.

**Choosing the baselines to compare to:**

* Choose at least 3-4 relevant baselines to compare to.
* Usually include behaviour cloning, especially on good datasets where it can be challenging to beat.
* Try to use common and existing implementations of baselines from the literature.
* If you include your own novel baseline, make the code available and easy to run for future comparisons. It is not sufficient to only share the code for the novel algorithm being proposed.

**Choosing the training and evaluation parameters:**

* For each environment, set a training budget and keep it constant across algorithms. For example, we used 100k training updates on SMAC and 200k on MAMuJoCo.
* If possible, do regular evaluations during training so that you can plot a training curve for analysis at the end of training. Do not use information from these evaluations to influence a training run online, as this would violate the assumptions around the training being offline.
* We recommend at every evaluation step unrolling for 32 episodes and reporting the average episode return.
* As per Agarwal et al. (2022), you should repeat each run across 10 random seeds.

**Reporting your results:**

* Report the final evaluation result, averaged across all 10 seeds, along with the standard deviation.
* If doing regular evaluations during training, also report the average and maximum episode return during training as per Papoudakis et al. (2021) and plot sample efficiency curves as per Gorsane et al. (2022).
* Use appropriate statistical significance calculations to report on whether your algorithms significantly outperform the baselines (Papoudakis et al., 2021; Agarwal et al., 2022).
* In addition to reporting results on a per-dataset basis, also report aggregated results across scenarios and dataset types. Results can be aggregated by first normalising them as per Fu et al. (2021) and then applying the utilities from _MARL-eval_(Gorsane et al., 2022) (e.g. performance profile plots, see Figure 4).

## 5 Conclusion

We conducted a thorough analysis of prior work in offline MARL and identified several significant methodological failures which we demonstrated are inhibiting progress in the field. Furthermore, we benchmarked simple baselines against several proposed SOTA algorithms and showed that our baselines outperform them in most cases. We used these insights to propose improving standards in evaluation with a simple protocol.

LimitationsOur work highlights some important challenges faced by the field of offline MARL, but does not capture _all_ such challenges. We hope our contributions will make it easier for authors to align and compare their future work, but we ultimately realise that our efforts require vetting by the community, to be tested and validated over time. We welcome such engagements, to collectively chart a path forward.

\begin{table}

\end{table}
Table 5: Three return metrics—the Final, \(\langle\)Maximum\(\rangle\), (Average)—from the two baseline algorithms in two respective environments, shown across various scenario and dataset quality combinations. Each result is presented as the mean and standard deviation, over 10 seeds. For the Final return, boldface indicates the best performing algorithm, and an asterisk (*) indicates a metric is not significantly different from the best performing metric in that situation, based on a heteroscedastic, two-sided t-test with 5% significance.

## References

* Agarwal et al. (2022) R. Agarwal, M. Schwarzer, P. S. Castro, A. Courville, and M. G. Bellemare. Deep reinforcement learning at the edge of the statistical precipice, 2022.
* Barde et al. (2024) P. Barde, J. Foerster, D. Nowrouzezahrai, and A. Zhang. A model-based solution to the offline multi-agent reinforcement learning coordination problem. _Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems_, 2024.
* Formanek et al. (2023) C. Formanek, A. Jeewa, J. Shock, and A. Pretorius. Off-the-grid marl: Datasets and baselines for offline multi-agent reinforcement learning. In _Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems_, AAMAS '23, page 2442-2444, Richland, SC, 2023. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450394321.
* Fu et al. (2021) J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2021.
* Fujimoto and Gu (2021) S. Fujimoto and S. S. Gu. A minimalist approach to offline reinforcement learning. _Advances in neural information processing systems_, 34:20132-20145, 2021.
* Gorsane et al. (2022) R. Gorsane, O. Mahjoub, R. de Kock, R. Dubb, S. Singh, and A. Pretorius. Towards a standardised performance evaluation protocol for cooperative marl, 2022.
* Hausknecht and Stone (2017) M. Hausknecht and P. Stone. Deep recurrent q-learning for partially observable mdps, 2017.
* Jang et al. (2016) E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. In _International Conference on Learning Representations_, 2016.
* Jiang and Lu (2021) J. Jiang and Z. Lu. Offline decentralized multi-agent reinforcement learning, 2021.
* Kumar et al. (2020) A. Kumar, A. Zhou, G. Tucker, and S. Levine. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 2020.
* Kurenkov and Kolesnikov (2022) V. Kurenkov and S. Kolesnikov. Showing your offline reinforcement learning work: Online evaluation budget matters. _International Conference on Machine Learning_, 2022.
* Lillicrap et al. (2016) T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. In _ICLR_, 2016.
* Lowe et al. (2017) R. Lowe, Y. I. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. _Advances in neural information processing systems_, 2017.
* Lyu et al. (2021) X. Lyu, Y. Xiao, B. Daley, and C. Amato. Contrasting centralized and decentralized critics in multi-agent reinforcement learning. In _Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems_, pages 844-852, 2021.
* Meng et al. (2023) L. Meng, M. Wen, C. Le, X. Li, D. Xing, W. Zhang, Y. Wen, H. Zhang, J. Wang, Y. Yang, and B. Xu. Offline pre-trained multi-agent decision transformer. _Machine Intelligence Research_, 20(2):233-248, Mar. 2023. ISSN 2731-5398.
* Pan et al. (2022) L. Pan, L. Huang, T. Ma, and H. Xu. Plan better amid conservatism: Offline multi-agent reinforcement learning with actor rectification. In _International conference on machine learning_, pages 17221-17237. PMLR, 2022.
* Papoudakis et al. (2019) G. Papoudakis, F. Christianos, A. Rahman, and S. V. Albrecht. Dealing with non-stationarity in multi-agent deep reinforcement learning. _arXiv preprint arXiv:1906.04737_, 2019.
* Papoudakis et al. (2021) G. Papoudakis, F. Christianos, L. Schafer, and S. V. Albrecht. Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks. _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks_, 2021.
* Peng et al. (2021) B. Peng, T. Rashid, C. A. S. de Witt, P.-A. Kamienny, P. H. S. Torr, W. Bohmer, and S. Whiteson. Facmac: Factored multi-agent centralised policy gradients, 2021.
* Prudencio et al. (2023) R. F. Prudencio, M. R. Maximo, and E. L. Colombini. A survey on offline reinforcement learning: Taxonomy, review, and open problems. _IEEE Transactions on Neural Networks and Learning Systems_, 2023.
* Rashid et al. (2018) T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster, and S. Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. _International Conference on Machine Learning_, 2018.
* Raghavan et al. (2019)M. Samvelyan, T. Rashid, C. S. de Witt, G. Farquhar, N. Nardelli, T. G. J. Rudner, C. Hung, P. H. S. Torr, J. Foerster, and S. Whiteson (2019)The StarCraft Multi-Agent Challenge. CoRRabs/1902.04043. External Links: Link, 1902.04043 Cited by: SS1.
* J. Shao, Y. Qu, C. Chen, H. Zhang, and X. Ji (2023)Counterfactual conservative q learning for offline multi-agent reinforcement learning. Advances in Neural Information Processing Systems37, pp. 1-2. External Links: Link, Document Cited by: SS1.
* A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru, and R. Vicente (2017)Multiagent cooperation and competition with deep reinforcement learning. PoS one12 (4), pp. e0172395. External Links: Link, Document Cited by: SS1.
* J. Terry, B. Black, N. Grammel, M. Jayakumar, A. Hari, R. Sullivan, L. S. Santos, C. Dieffendahl, C. Horsch, R. Perez-Vicente, et al. (2021)PettingZoo: gym for multi-agent reinforcement learning. Advances in Neural Information Processing Systems. External Links: Link, Document Cited by: SS1.
* Q. Tian, K. Kuang, F. Liu, and B. Wang (2023)Learning from good trajectories in offline multi-agent reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence37, pp. 11672-11680. External Links: Link, Document Cited by: SS1.
* E. Toledo, L. Midgley, D. Byrne, C. R. Tilbury, M. Macfarlane, C. Courtot, and A. Laterre (2023)Flashbax: streamlining experience replay buffers for reinforcement learning with jax. External Links: Link, 2003.0007 Cited by: SS1.
* X. Wang and X. Zhan (2023)Offline multi-agent reinforcement learning with coupled value factorization. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, pp. 2781-2783. External Links: Link, Document Cited by: SS1.
* X. Wang, H. Xu, Y. Zheng, and X. Zhan (2023)Offline multi-agent reinforcement learning with implicit global-to-local value regularization. Advances in Neural Information Processing Systems37. External Links: Link, Document Cited by: SS1.
* Y. Yang, X. Ma, C. Li, Z. Zheng, Q. Zhang, G. Huang, J. Yang, and Q. Zhao (2021)Beliewe what you see: implicit constraint approach for offline multi-agent reinforcement learning. Advances in Neural Information Processing Systems34, pp. 10299-10312. External Links: Link, Document Cited by: SS1.
* C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu (2022)The surprising effectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems35, pp. 24611-24624. External Links: Link, Document Cited by: SS1.
* L. Yuan, Z. Zhang, L. Li, C. Guan, and Y. Yu (2023)A survey of progress on cooperative multi-agent reinforcement learning in open environment. arXiv preprint arXiv:2312.01058. External Links: Link, 2012.01058 Cited by: SS1.
* Y. Zhong, J. G. Kuba, X. Feng, S. Hu, J. Ji, and Y. Yang (2024)Heterogeneous-agent reinforcement learning. Journal of Machine Learning Research25, pp. 1-67. External Links: Link, Document Cited by: SS1.
* Z. Zhu, M. Liu, L. Mao, B. Kang, M. Xu, Y. Yu, S. Ermon, and W. Zhang (2023)Modiff: offline multi-agent learning with diffusion models. Arxiv Preprint. External Links: Link, 2003.0007 Cited by: SS1.

Algorithm implementation details and hyperparameters

1. For all models and algorithms presented, check if you include: 1. A clear description of the mathematical setting, algorithm, and/or model. [Yes] 2. A clear explanation of any assumptions. [Yes] 3. An analysis of the complexity (time, space, sample size) of any algorithm. [No] The algorithms presented are foundational baselines that draw on existing works from the literature.
2. For all shared code related to this work, check if you include: 1. Specification of dependencies. [Yes] We provide a _requirements.txt_ file and detailed installation instructions. In addition, we provide a working Dockerfile for maximum portability. 2. Training code. [Yes] All systems can be run using a single script. Instructions are provided in the README. 3. Evaluation code. [Yes] All systems have inbuilt evaluation and results are logged to the terminal and _Weights and Biases_. 4. (Pre-)trained model(s). [No] Pre-trained models have not been saved because training a model on any of the scenarios takes between a few minutes and at most a couple hours on a Laptop GPU (RTX 3070). 5. README file includes table of results accompanied by precise command to run to produce those results. [Yes] We provide a notebook with a database of results and visualisations and easy-to-run code for reproducing all results.
3. For all reported experimental results, check if you include: 1. The range of hyper-parameters considered, method to select the best hyper-parameter configuration, and specification of all hyper-parameters used to generate results. [Yes] See below. 2. The exact number of training and evaluation runs. [Yes] Each experiment was repeated across 10 random seeds. For evaluation we rolled out policies for 32 episodes and computed the mean episode return. 3. A clear definition of the specific measure or statistics used to report results. [Yes] We measured episode return in all cases. 4. A description of results with central tendency (e.g. mean) & variation (e.g. error bars). [Yes] 5. The average runtime for each result, or estimated energy cost. [Yes] Depending on the scenario, reproducing a single experimental run can take between 20min and 4 hours on a Laptop GPU (e.g. RTX 3070). The MPE experiments are the fastest, followed by SMAC experiments and finally MAMMuJoCo experiments take the longest. 6. A description of the computing infrastructure used. [Yes] Individual runs can easily be reproduced on a Laptop GPU (e.g. RTX 3070), 8GB of RAM and 4 CPU cores. However, reproducing all experiments on a single GPU would take approximately 20 days. We had access to a compute cluster with 10, roughly equivalent, GPUs. This meant that generating all baseline results took about 2 days.

### IDDPG+BC (Continuous)

Our implementation draws on the minimalistic offline RL algorithm proposed by Fujimoto and Gu (2021). In essence, you simply add a behaviour cloning term to the deterministic policy gradient (DPG) loss. In a continuous action space with deterministic policies, this can be achieved by a simple mean square error between the output of the policy and the given action sampled from the dataset. As per Fujimoto and Gu (2021), we normalise the DPG term so that its scale is similar to the BC term and then use a _behaviour cloning weight_ hyperparameter to control the relative importance of the behaviour cloning term vs. the DPG term. The critic conditioned on the environment state and the agents individual action only. Policies conditioned on the decentralised observations only. We used shared parameters by always concatenating an agent-ID to observations.

### MADDPG+CQL (Continuous)

Our implementation adds CQL (Kumar et al., 2020) to MADDPG (Lowe et al., 2017). In the original version of CQL they had stochastic policies (soft actor critic), and so, getting actions _near_ the current policy was simply a matter of sampling the stochastic policy. Since we had deterministic policies we applied a small amount of Gaussian noise to our actions. The amount of noise is then controlled by a hyperparameter we called _CQL sigma_. The _CQL weight_ parameter controls the relative importance of the CQL loss in the overall critic loss. While the critic condition on joint-actions and the environment state, policies conditioned on the decentralised observations only. We used shared parameters by always concatenating an agent-ID to observations.

### Iql+cql (Discrete)

Our implementation added CQL (Kumar et al., 2020) to independent Q-Learners. Our independent Q-learners use recurrent Q-networks (Hausknecht and Stone, 2017). For the CQL loss, we simply sample a _number of CQL actions_ randomly from the joint-action space and "push" their Q-values down, while pushing up the Q-values for joint-actions in the dataset The _CQL weight_ hyperparameter controls the relative importance of the CQL term in the Q-Learning loss. The Q-networks condition on the decentralised observations only. We used shared parameters by always concatenating an agent-ID to observations.

### Behaviour cloning (Discrete)

In our behaviour cloning implementation for discrete action spaces, we train policy networks to match the actions in the dataset using a simple categorical crossentropy loss. We use recurrent policy

\begin{table}
\begin{tabular}{l|l}
**Hyperparameter** & **Value** \\ \hline Critic first linear layer & 128 \\ Critic second linear layer & 128 \\ Policy linear layer & 64 \\ Policy GRU layer & 64 \\ Critic learning rate & 1e-3 \\ Policy learning rate & 3e-4 \\ Target update rate & 0.005 \\ Discount (gamma) & 0.99 \\ Number of CQL actions & 10 \\ CQL weight & 3 \\ CQL sigma & 0.1, 0.2, 0.3 \\ \end{tabular}
\end{table}
Table 6: Hyper parameters used for IDDPG+BC across all MAMuJoCo and MPE datasets. We found that the recommended _behaviour cloning weight_ of 2.5 proposed by Fujimoto and Gu (2021) worked well across all scenarios.

\begin{table}
\begin{tabular}{l|l}
**Hyperparameter** & **Value** \\ \hline Critic first linear layer & 128 \\ Critic second linear layer & 128 \\ Policy linear layer & 64 \\ Policy GRU layer & 64 \\ Policy linear layer & 64 \\ Policy GRU layer & 64 \\ Critic learning rate & 1e-3 \\ Policy learning rate & 3e-4 \\ Target update rate & 0.005 \\ Discount (gamma) & 0.99 \\ Number of CQL actions & 10 \\ CQL weight & 3 \\ CQL sigma & 0.1, 0.2, 0.3 \\ \end{tabular}
\end{table}
Table 7: Hyper parameters used for MADDPG+CQL across all MAMuJoCo and MPE datasets. We found that MADDPG+CQL was sensitive to the value of _CQL sigma_ and the optimal value depended on the MuJoCo scenario. For _Ant_ scenarios, 0.1 was the best value, while for _Hopper_ and _HalFChetah_ scenarios the best values were 0.2 and 0.3 respectively. This dependence on the scenario makes sense since it is well known that CQL has a dependence on the action space of the scenario tested on Kumar et al. (2020). We found that tuning the _CQL weight_ across scenarios could also slightly improve performance but the value 3 worked relatively well across all scenarios. Future works could explore using automatic CQL weight tuning, and stochastic policies (e.g. soft actor critic) to remove the CQL sigma hyper-parameter.

networks which condition on the decentralised observations only. We used shared parameters by always concatenating an agent-ID to observations.

\begin{table}
\begin{tabular}{l|l}
**Hyperparameter** & **Value** \\ \hline First linear layer & 64 \\ GRU layer & 64 \\ Learning rate & 3e-4 \\ Target period & 200 \\ Discount (gamma) & 0.99 \\ Number of CQL actions & 10 \\ CQL weight & 2 \\ \end{tabular}
\end{table}
Table 8: Hyper parameters used for IQL+CQL across all SMAC datasets.

\begin{table}
\begin{tabular}{l|l}
**Hyperparameter** & **Value** \\ \hline First linear layer & 64 \\ GRU layer & 64 \\ Learning rate & 1e-3 \\ Discount (gamma) & 0.99 \\ \end{tabular}
\end{table}
Table 9: Hyper parameters used for BC across all SMAC datasets.

Meta-review: Visualising statistical significance in the literature

We processed results reported in tabular form by Pan et al. (2022), Wang et al. (2023), Wang and Zhan (2023), and Shao et al. (2023). As previously stated, we perform a simple heteroscedastic, two-sided t-test with a 95% confidence interval for testing statistical significance. If we accept the null hypothesis, it can be said that with a 95% confidence interval For this section of the appendix, we consider only the results reported, and do not include our own baselines.

We show that many of the results (which form a large part of the evidence for SOTA claims for most of the considered papers) do not indicate a significant difference between performance of the algorithm with the highest mean and the next-best performing algorithm. Each result in each plot which has a red circle around it is equivalent to SOTA within the table in which it is reported.

### Omac

Figure 5 illustrates the results presented in Table 4 in the paper by Wang and Zhan (2023). We represent SMACv1 results on a \(0-20\) scale to better interpret results within the scoring range. Note the proposed algorithm is unmatched on one dataset only. Additionally, mabcq (not the proposed algorithm) is equivalent to SOTA on all but 2 of the 12 datasets.

### Omiga

Figure 6 illustrates the results presented in Table 1 in the paper by Wang et al. (2023). We represent SMACv1 results on a \(0-20\) scale to better interpret results within the scoring range. MAMuJoCo results are unnormalised.

Note the proposed algorithm is unmatched on only 3 of the 24 datasets. Additionally, maicq (not the proposed algorithm) is equivalent to SOTA on 18 of the 24 datasets.

Figure 5: Results reported by Wang and Zhan (2023) on the SMAC environment.

Figure 6: Results reported by Wang et al. (2023) on the SMAC and MAMuJoCo environments.

[MISSING_PAGE_FAIL:17]

Figure 8: Results reported by Shao et al. (2023) in tabular form on SMAC, MPE and MAMuJoCo environments.

[MISSING_PAGE_FAIL:19]

\begin{table}
\begin{tabular}{l l l|l l l l l l|l}  & & & & & & & & & & Our Baseline \\  & & & & & & & & & & \\ task & dataset quality & & & & & & & & \\ \hline
2c vs 64zg & good & 19.13\(\pm\)0.27 & 18.48\(\pm\)0.95* & 18.82\(\pm\)0.17 & 17.27\(\pm\)0.78 & 19.15\(\pm\)0.32* & **19.52\(\pm\)0.26** \\  & medium & 15.58\(\pm\)0.37* & 12.82\(\pm\)1.61 & 15.57\(\pm\)0.61* & 10.20\(\pm\)0.20 & **16.03\(\pm\)0.19** & 14.89\(\pm\)0.72 \\  & poor & 12.46\(\pm\)0.18* & 10.83\(\pm\)0.51 & 12.56\(\pm\)0.18* & 11.33\(\pm\)0.50 & **13.02\(\pm\)0.66** & 11.03\(\pm\)0.41 \\ \hline
5m vs 6m & good & 7.76\(\pm\)0.15 & 8.08\(\pm\)0.21 & 7.87\(\pm\)0.30 & 7.40\(\pm\)0.63 & 8.25\(\pm\)0.37 & **12.36\(\pm\)1.09** \\  & medium & 7.58\(\pm\)0.10 & 7.78\(\pm\)0.10 & 7.77\(\pm\)0.30 & 7.08\(\pm\)0.51 & 7.92\(\pm\)0.57 & **12.30\(\pm\)0.74** \\  & poor & 7.61\(\pm\)0.36 & 7.43\(\pm\)0.10 & 7.26\(\pm\)0.19 & 7.27\(\pm\)0.42 & 7.52\(\pm\)0.21 & **10.20\(\pm\)0.75** \\ \hline
6h vs 8z & good & 12.19\(\pm\)0.23 & 10.44\(\pm\)0.20 & 11.81\(\pm\)0.12 & 9.85\(\pm\)0.28 & 12.54\(\pm\)0.21* & **12.72\(\pm\)0.44** \\  & medium & 11.77\(\pm\)0.16 & 11.29\(\pm\)0.29 & 11.13\(\pm\)0.33 & 10.36\(\pm\)0.16 & **12.19\(\pm\)0.22** & 12.01\(\pm\)0.42* \\  & poor & 10.84\(\pm\)0.16 & 10.81\(\pm\)0.52* & 10.55\(\pm\)0.10 & 10.63\(\pm\)0.25 & **11.31\(\pm\)0.19** & 10.41\(\pm\)0.36 \\ \hline corridor & good & 15.24\(\pm\)1.21 & 5.22\(\pm\)0.81 & 15.54\(\pm\)1.12 & 6.74\(\pm\)0.69 & 15.88\(\pm\)0.89 & **19.06\(\pm\)0.81** \\  & medium & 10.82\(\pm\)0.92 & 7.04\(\pm\)0.66 & 11.30\(\pm\)1.57 & 7.26\(\pm\)0.71 & 11.66\(\pm\)1.30 & **13.44\(\pm\)1.31** \\  & poor & 4.47\(\pm\)0.94 & 4.08\(\pm\)0.60 & 4.47\(\pm\)0.33 & 4.28\(\pm\)0.49 & 5.61\(\pm\)0.35* & **6.11\(\pm\)1.10** \\ \end{tabular}
\end{table}
Table 13: Results on SMAC datasets from Wang et al. (2023), subsampled from datasets generated by Meng et al. (2023).

\begin{table}
\begin{tabular}{l l|l l l l l l l l|l}  & & & & & & & & & & Our Baseline \\ task & dataset quality & & & & & & & & & \\ \hline
2c vs 64zg & good & 19.13\(\pm\)0.27 & 18.48\(\pm\)0.95* & 18.82\(\pm\)0.17 & 17.27\(\pm\)0.78 & 19.15\(\pm\)0.32* & **19.52\(\pm\)0.26** \\  & medium & 15.58\(\pm\)0.37* & 12.82\(\pm\)1.61 & 15.57\(\pm\)0.61* & 10.20\(\pm\)0.20 & **16.03\(\pm\)0.19** & 14.89\(\pm\)0.72 \\  & poor & 12.46\(\pm\)0.18* & 10.83\(\pm\)0.51 & 12.56\(\pm\)0.18* & 11.33\(\pm\)0.50 & **13.02\(\pm\)0.66** & 11.03\(\pm\)0.41 \\ \hline
5m vs 6m & good & 7.76\(\pm\)0.15 & 8.08\(\pm\)0.21 & 7.87\(\pm\)0.30 & 7.40\(\pm\)0.63 & 8.25\(\pm\)0.37 & **12.36\(\pm\)1.09** \\  & medium & 7.58\(\pm\)0.10 & 7.78\(\pm\)0.10 & 7.77\(\pm\)0.30 & 7.08\(\pm\)0.51 & 7.92\(\pm\)0.57 & **12.30\(\pm\)0.74** \\  & poor & 7.61\(\pm\)0.36 & 7.43\(\pm\)0.10 & 7.26\(\pm\)0.19 & 7.27\(\pm\)0.42 & 7.52\(\pm\)0.21 & **10.20\(\pm\)0.75** \\ \hline
6h vs 8z & good & 12.19\(\pm\)0.23 & 10.44\(\pm\)0.20 & 11.81\(\pm\)0.12 & 9.85\(\pm\)0.28 & 12.54\(\pm\)0.21* & **12.72\(\pm\)0.44** \\  & medium & 11.77\(\pm\)0.16 & 11.29\(\pm\)0.29 & 11.13\(\pm\)0.33 & 10.36\(\pm\)0.16 & **12.19\(\pm\)0.22** & 12.01\(\pm\)0.42* \\  & poor & 10.84\(\pm\)0.16 & 10.81\(\pm\)0.52* & 10.55\(\pm\)0.10 & 10.63\(\pm\)0.25 & **11.31\(\pm\)0.19** & 10.41\(\pm\)0.36 \\ \hline corridor & good & 15.24\(\pm\)1.21 & 5.22\(\pm\)0.81 & 15.54\(\pm\)1.12 & 6.74\(\pm\)0.69 & 15.88\(\pm\)0.89 & **19.06\(\pm\)0.81** \\  & medium & 10.82\(\pm\)0.92 & 7.04\(\pm\)0.66 & 11.30\(\pm\)1.57 & 7.26\(\pm\)0.71 & 11.66\(\pm\)1.30 & **13.44\(\pm\)1.31** \\  & poor & 4.47\(\pm\)0.94 & 4.08\(\pm\)0.60 & 4.47\(\pm\)0.33 & 4.28\(\pm\)0.49 & 5.61\(\pm\)0.35* & **6.11\(\pm\)1.10** \\ \end{tabular}
\end{table}
Table 14: Results on SMAC datasets from Shao et al. (2023).

## Appendix D Converted Datasets

To compare existing results to our own baselines, we converted datasets from the literature into the Vault format from Flashbox. Examples of our methodology for converting these datasets are given in a notebook here: https://bit.ly/vault-conversion-notebook. Table 15 provides links to all Vault datasets.

Statistical profilesThe statistical profiles (histograms and violin plots) of the converted datasets are available in the respective folders on HuggingFace: https://huggingface.co/datasets/InstaDeepAI/og-marl/tree/main/prior_work. Additionally, summary statistic values can easily be accessed using the demonstrative notebook at: https://github.com/instadeepai/og-marl/blob/main/examples/dataset_analysis_demo.ipynb.

\begin{table}
\begin{tabular}{|l|l l l|} \hline \multicolumn{1}{|c|}{**Page**} & \multicolumn{1}{c|}{**Environment**} & \multicolumn{1}{c|}{**Scenario**} & \multicolumn{1}{c|}{**Link**} \\ \hline \multirow{3}{*}{\begin{tabular}{} \end{tabular} } & \multirow{3}{*}{\begin{tabular}{} \end{tabular} } & \multirow{3}{*}{\begin{tabular}{} \end{tabular} } & \multirow{3}{*}{\begin{tabular}{} \end{tabular} } \\  & & & & \\ \cline{1-1} \cline{6-6}  & & & & \\ \hline \multirow{3}{*}{\begin{tabular}{} \end{tabular} } & \multirow{3}{*}{\begin{tabular}{} \end{tabular} } & \multirow{3}{*}{
\begin{tabular}{} \end{tabular} } \\  & & & & \\ \hline \end{tabular}
\end{table}
Table 15: Links to Vault datasets converted from the literature.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] We are clear in both the Abstract and Introduction about how our paper contributes to Offline MARL research, focusing on the problem of baselines and evaluation protocols. We accurately share the central claims of our paper--that straightforward baselines in offline MARL can achieve SOTA results across many scenarios, which is a result we show with significance later in the paper. 2. Did you describe the limitations of your work? We have a separate limitations section, with an invitation to the community for further engagement. 3. Did you discuss any potential negative societal impacts of your work? We do not feel that such a section is relevant in the presently nascent field of offline MARL, where we are developing methods to learn optimal control strategies from offline data. Currently, we are still operating in theoretical or hypothetical scenarios, using small-scale, toy-problem simulations. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? We have reviewed the ethical guidelines and remain wholeheartedly confident that our contribution does not violate any guideline whatsoever.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? Did you include complete proofs of all theoretical results?
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? Central to this paper is our set of benchmark results, trained using a collection of datasets. Accordingly, we have been pedantic in our reproducibility efforts: we share all baseline code (which achieves SOTA in majority of considered scenarios), along with links to raw experiment data on WandB; we also convert multiple datasets from other authors (at times, very difficult to access!), to a consistent, simple, and stable API format, and make these available publicly, with a guarantee of ongoing maintenance. Instructions for accessing and using this data is also provided in a Google Colab notebook. We further provide an extensive metareview of the field, in the form of another interactive notebook. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? All relevant training information is included in the comprehensive appendices. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? All experiments were run over 10 seeds, and results are reported with statistical significance tests done. Error bars are included in our main plot, in Figure 1. Other plots come from the statistically reliable tools from Agarwal et al. (2022) and Gorsane et al. (2022). 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? All compute information is provided in the appendix.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? This work leverages datasets from various authors in the literature, and appropriate citations are given in each case. 2. Did you mention the license of the assets? Details of all the accessed datasets are listed in the appendix, including license information. 3. Did you include any new assets either in the supplemental material or as a URL? We converted datasets from the literature to a consistent and stable API, and these new datasets are available online via a URL. The full methodology for converting the datasets is provided in a Google Colab notebook, also linked in the appendix.

* Did you discuss whether and how consent was obtained from people whose data you re using/curating? [No] The license of the various open-source datasets allows us to freely use and repackage the data, and we feel no concerns around consent here.
* Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No] This dimension is not applicable to our datasets, since our scenarios deal with fictional, simulated environments, completely separate from anything personally identifiable or offensive.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [NA] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [NA] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [NA]