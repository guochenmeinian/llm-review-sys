# Augmentation-Free Dense Contrastive Knowledge Distillation for Efficient Semantic Segmentation

 Jiawei Fan

Intel Labs China

jiawei.fan@intel.com

&Chao Li

Intel Labs China

chao3.li@intel.com

&Xiaolong Liu

HoloMatic Technology Co. Ltd.

liuxiaolong@holomatic.com

&Meina Song

BUPT

mnsong@bupt.edu.cn

&Anbang Yao

Intel Labs China

anbang.yao@intel.com

This work was done when the first two authors were interns at Intel Labs China, supervised by Anbang Yao who proposed the original idea and led the project.Corresponding author.

###### Abstract

In recent years, knowledge distillation methods based on contrastive learning have achieved promising results on image classification and object detection tasks. However, in this line of research, we note that less attention is paid to semantic segmentation. Existing methods heavily rely on data augmentation and memory buffer, which entail high computational resource demands when applying them to handle semantic segmentation that requires to preserve high-resolution feature maps for making dense pixel-wise predictions. In order to address this problem, we present Augmentation-free Dense Contrastive Knowledge Distillation (Af-DCD), a new contrastive distillation learning paradigm to train compact and accurate deep neural networks for semantic segmentation applications. Af-DCD leverages a masked feature mimicking strategy, and formulates a novel contrastive learning loss via taking advantage of tactful feature partitions across both channel and spatial dimensions, allowing to effectively transfer dense and structured local knowledge learnt by the teacher model to a target student model while maintaining training efficiency. Extensive experiments on five mainstream benchmarks with various teacher-student network pairs demonstrate the effectiveness of our approach. For instance, the DeepLabV3-Res18IDeepLabV3-MBV2 model trained by Af-DCD reaches 77.03%176.38% mIOU on Cityscapes dataset when choosing DeepLabV3-Res101 as the teacher, setting new performance records. Besides that, Af-DCD achieves an absolute mIOU improvement of 3.26%13.04%12.75%12.30%11.42% compared with individually trained counterpart on CityscapesPascal VOICCamavidDAE20KICOCO-Stuff-164K. Code is available at https://github.com/OSVAI/Af-DCD.

## 1 Introduction

In computer vision, semantic segmentation is a mainstream dense prediction task aiming to assign the corresponding class to every pixel of input images. Although fundamental models in this task have achieved remarkable progress [1; 2; 3], the heavy computational burden and latency of these models severely prohibit their deployment on resource-constrained devices. As a popular solution to this bottleneck, Knowledge Distillation (KD) [4] aims to transfer the knowledge from large models to light-weight ones. Considering KD in supervised semantic segmentation, teachers usually havestronger ability of capturing detailed local information of images where the _dense and structured knowledge_ contributes significantly to segment complicated areas, such as boundary and occlusion. Therefore, distillation methods tailored to this task should keep the integrity of dense and structured knowledge, rather than compressing it excessively such as simply using global representations or salient areas. As a result, directly applying traditional KD methods in semantic segmentation, like vanilla KD [4] or feature imitation [5], cannot achieve promising results in most cases.

Thanks to the development of contrastive learning, some distillation methods, like SSKD [6], CRD [7] and G-DetKD [8], employed this paradigm into their designs, which promoted the performance significantly, as they exploited teacher's intrinsic knowledge in the consistencies among different augmentations or various same-category instances. In semantic segmentation, CIRKD [9] designed an implicit contrastive method which aimed to guarantee pixel-pixel and pixel-region consistencies between teacher and student by involving additional contrastive samples. On one side, these existing methods have achieved remarkable progress in image-level or object-level contrastive mimicking. On the other side, however, the efficacy of the aforementioned methods heavily relies on the augmentation and memory buffer, which entail heavy computational and memory cost. Besides, previous contrastive distillation methods have not explicitly modeled relations among pixel-wise or more fine-grained representations within each local patch to transfer teacher's dense and structured knowledge to student, which is critical in semantic segmentation. Thus, when designing a contrastive distillation method for this task, two vital technical problems should be tackled: **(i) High resource demands**: No matter leveraging augmented samples or storing feature maps in memory buffer, more computational or storage cost is required. For one, the forward of augmented samples incurs extra computational cost. For another, the output feature maps for semantic segmentation are in high resolution. If we want to make dense and structured contrasting, these feature maps should be originally preserved, which occupy a large amount of memories; **(ii) Structured knowledge transfer**: Although CIRKD defined pixel-wise alignment between student and teacher, it employed contrastive pixel-wise representations from other images, rather than local areas within the same image. It means that no explicit contrasting was defined between student-specific pixel-wise representation to each teacher's pixel-wise representation within local areas, which was agnostic to the structure in teacher's feature. _In brief, a contrastive distillation method specially tailored to semantic segmentation, which also entails no extra high resource demands (data augmentation and memory buffer), is essential_.

Driven by achieving this target, we first look into the aforementioned two problems and surprisingly discover both of them are incurred from the simple inheritance in the basic definitions of traditional contrastive learning [10; 11]. Therefore, our Augmentation-free Dense Contrastive Knowledge Distillation (Af-DCD) for supervised semantic segmentation tasks can come out, by re-defining these basic concepts: (i) _Contrasting samples_ are not coarse-grained representations of images or objects. Instead, in our Af-DCD, we move further on pixel-level representations and divide each of them into several disjoint partitions (fine-grained representations), which are treated as our contrasting samples; (ii) _Positive and negative pairs_ are not conditioned on categories or objects. Instead, based on the definition of contrasting samples, we define our pairs in a specific teacher-student feature pair \((F^{t},F^{s})\), where fine-grained representations having the same absolute positions in feature maps are used to formulate positive pairs, while representations having different absolute positions but within neighbourhoods are used as negative pairs. Based on these definitions, our Af-DCD can naturally tackle those two problems discussed above. To the first problem, unlike previous methods which had to introduce data augmentation and memory buffer to construct sufficient positive and negative pairs,

Figure 1: The overall framework of Af-DCD, which contains two major parts: (i) masked feature reconstruction; (ii) augmentation-free contrastive distillation loss. Detail illustrations on Af-DCD are shown in Figure 2.

our Af-DCD can directly construct these pairs in each local patch without any extra computational and memory cost. To the second problem, we approach it by introducing two new contrastive designs, **Spatial Contrasting** and **Channel Contrasting**, which enable student to capture teacher's local dense and structured knowledge by focusing on contextual and positional channel-group information, respectively. As a natural progression, we introduce a hybrid design called **Omini-Contrasting**, which combines Spatial Contrasting and Channel Contrasting in a neat manner. This design facilitates the simultaneous transfer of both types of information from the teacher to the student, enabling effective augmentation-free contrastive knowledge distillation. Despite leveraging rather dense contrasting, our Af-DCD also performs efficiently. This is due to the patch separation technique we use, which significantly reduces the computational complexity. Additionally, the distance measuring and contrasting calculation can be carried out in parallel, further enhancing the overall efficiency of our method.

Experimental results demonstrate that: (i) Af-DCD exhibits superior performance compared to state-of-the-art methods, on various benchmarks with different teacher-student network pairs; (ii) Af-DCD exhibits even more significant improvements on larger datasets, such as ADE20K, indicating it can enhance student's generalization capability. Moreover, our analytical experiments illustrate that by effectively learning teacher's self-similarity distribution within neighbourhoods and thus reducing the fine-grained feature distances to the teacher, Af-DCD benefits addressing difficult scenarios in semantic segmentation.

## 2 Related Works

**Knowledge Distillation.** Knowledge distillation can be generally divided into probability-based approach and feature-based approach. Specifically, the former one forces student to mimic teacher's logits as soft-labels [4; 12], while the later one leverages teacher's hidden feature maps or its variants [5; 13; 14] as distillation supervisions. Some recent feature-based approaches [15; 16; 17] designed their distillation methods based on masked image modeling mechanism [18; 19] and achieved promising performance in various tasks, as the feature reconstruction can enhance the interdependencies among pixels, which is beneficial to feature distillation.

**Knowledge Distillation in Semantic Segmentation.** As semantic segmentation is a dense prediction task, the methods specially designed for this task aimed to capture teacher's structured local information. In order to achieve this target, SKD [20] directly measured the similarities of pixel-wise representations between teacher and student, while _He et al._[21] leveraged non-local operation with autoencoder to encode local information. CWD [22] evaluated channel-wise pixel distribution contributing to learn teacher's spatial information in each individual channel. Some methods further explored intrinsic knowledge among different samples. For instance, IFVD [23] measured distances from prototypes of different classes and forced student to mimic teacher's intra-class relations, and CIRKD [9] forced student to keep pixel-wise and region-wise consistencies to teacher among various samples in memory buffer.

**Contrastive Knowledge Distillation.** Inspired by the development of contrastive learning in self-supervised [10; 24] and supervised tasks [11], some recent works leveraged this paradigm and designed new contrastive distillation methods, which made contrasting between teacher's and student's features by employing different views generated from data augmentation [6], or using various samples' features [25; 7] as well as gradients [26] stored in memory buffer. Specifically in dense prediction tasks, G-DetKD [8] constructed ROI feature pairs and executed soft semantic-guided matching which promoted the performance in object detection, and CIRKD [9] designed an implicit contrastive method which leveraged both pixel and region representations to learn structured information in spatial dimension.

## 3 Method

### Overall Framework

Figure 1 illustrates our distillation process in semantic segmentation from a pre-trained teacher \(T\) to a student \(S\) which needs to be trained on a specific dataset. The output features of \(T\) and \(S\) are denoted as \(\hat{F}^{t}\in\mathbb{R}^{H\times W\times C^{t}}\) and \(\hat{F}^{s}\in\mathbb{R}^{H\times W\times C^{s}}\) respectively, where \(H\) and \(W\) are height and width of

feature maps, \(C^{t}\) and \(C^{s}\) are the number of channels of teacher and student, respectively. Our method uses a random mask \(M\in\mathbb{R}^{H\times W\times 1}\) to overlap \(\hat{F}^{s}\) in spatial dimension and then inputs the masked feature into a generator, which can be formulated as \(F^{s}=f_{generator}(M\odot\hat{F}^{s})\). Finally, we can get \(F^{s}\in\mathbb{R}^{H\times W\times C^{t}}\), which has the same dimension as teacher's feature. In order to fit general definition, teacher's feature is also processed by a transform \(F^{t}=f_{t}(\hat{F}^{t})\). In our design, \(f_{t}\) is an identity transformation, thus \(F^{t}=\hat{F}^{t}\). After these operations, the reconstructed student feature \(F^{s}\) has been projected into the same feature space as teacher's feature \(F^{t}\). The feature imitation loss \(L_{fd}\) and our Af-DCD loss \(L_{Af-DCD}\) are calculated based on feature pair \((F^{t},F^{s})\). Besides, following the settings in [9; 22], we also employ vanilla logits-based KD loss [4]. In general, the overall loss of our method can be defined as:

\[L=L_{task}+\lambda_{1}L_{kd}+\lambda_{2}L_{fd}+\lambda_{3}L_{Af-DCD},\] (1)

where \(\lambda_{i}\) is the balancing weight.

### Augmentation-free Dense Contrastive Knowledge Distillation

After illustrating the overall framework, we further explain our core design in detail, which contains two major parts, \(L_{fd}\) and \(L_{Af-DCD}\). As we mentioned above, both of them are based on the same feature pair \((F^{t},F^{s})\). Therefore, in order to clarify our design, we first answer two key questions: (i) What are the targets of \(L_{fd}\) and \(L_{Af-DCD}\) separately? (ii) How these two losses cooperate with each other? To the first question, \(L_{fd}\) directly help student to imitate teacher's global and salient features, while \(L_{Af-DCD}\) is designed to mimic teacher's dense structured knowledge within local areas, which can further force \(F^{s}\) to approach \(F^{t}\) in the micro and fine-grained views. To the second question, in terms of mimicking dense and structured knowledge, these two losses are inextricably bound up with each other. For one, \(L_{fd}\) provides \(L_{Af-DCD}\) an absolute imitation trend, which cannot be implemented in contrastive loss. For another, \(L_{Af-DCD}\) offers \(L_{fd}\) essential constrains on encouraging this process towards mimicking teacher's structured knowledge in each local patch, making student further approach teacher.

**Masked Reconstruction based Feature Imitation.** The feature imitation loss \(L_{fd}\) is a basic part of our design, which forces student feature \(F^{s}\) to imitate teacher feature \(F^{t}\) directly. Previous methods commonly leverage simple upsampling to align teacher's and student's feature dimensions. Recently, some methods leveraged masked image reconstruction [15; 17], which is a stronger baseline, as it can promote the performance by strengthening the interdependencies among pixels. This process can be formulated as:

\[F^{s}=f_{generator}(M\odot\hat{F}^{s}),\] (2)

Figure 2: Detailed illustrations on three different types of Af-DCD, which are Spatial Contrasting (top left), Channel Contrasting (top right) and Omni-Contrasting (bottom). For brevity, the contrasting process is illustrated merely using a specific contrastive sample in student feature maps, denoted as \(F^{s}_{i,j}\), \(F^{s}_{i,j,k}\), \(F^{s}_{p,i,j,k}\) in three Af-DCD designs, respectively. The red arrows denote constructing positive pairs, while the blue arrows denote constructing negative pairs. The gray blocks denote other patches which are not considered in calculating the loss in this patch.

where \(M\in\mathbb{R}^{H\times W\times 1}\) is the mask matrix generated randomly with the mask ratio \(\zeta\in[0,1)\). Then feature distillation can be formulated as:

\[L_{fd}=\sum_{k=1}^{C^{t}}\sum_{i=1}^{H}\sum_{j=1}^{W}(F^{t}_{i,j,k}-F^{s}_{i,j,k} )^{2}.\] (3)

Although masked reconstruction achieves promising performance, one potential weakness may affect its gain in semantic segmentation: Reconstructed features tend to be similar in local areas 3. However, in semantic segmentation, local differences indicate structured knowledge, in which student's reconstructed feature maps should keep these differences. _Here comes out why we choose masked reconstruction as our basic feature distillation method: (i) choosing a stronger baseline to further test the effectiveness of our Af-DCD; (ii) examining whether our Af-DCD can promote masked reconstruction for generating more structured representations, enhancing feature imitation process._

Footnote 3: Experimental results illustrate that the distances between feature partitions within local areas are usually small (see in Figure 2(b)).

**Motivation and Key Ideas of Af-DCD.** In semantic segmentation, we notice two important facts. On one side, different pixels within local areas may contain different semantic information, as they may belong to different categories or different parts of an object. We call this as _contextual information_. On the other side, differences among channel groups of a pixel representation implicitly indicate the semantic meaning of this pixel, as each output channel is a specific projection of all input feature maps. We call this as _positional channel-group information_. In short, the contextual and positional channel-group information within each local area in spatial and channel dimensions is of significance in semantic segmentation. If both types of information from teacher's feature maps can be densely utilized as the distillation guidance to facilitate the training of the student model, we conjecture that student's performance and generalization capability can be greatly boosted. However, traditional feature imitation loss \(L_{fd}\) is difficult to capture these two types of information, as they are not directly observable and measurable comparing to salient representation. Aiming to tackle this problem, we define contrastive loss across student's and teacher's pixel-level or more fine-grained representations to explicitly model such kind of knowledge transfer. Specifically, as shown in Figure 2, Af-DCD incorporates three key ideas:

1. **Spatial Contrasting.** With the first phenomenon, we leverage pixel-wise representations in \(F^{s}\) and \(F^{t}\) and define pixel-wise dense contrasting based on spatial positions, aiming to transfer teacher's spatial contextual information to student;
2. **Channel Contrasting.** With the second phenomenon, we then move further on pixel-wise dense contrasting and split every pixel-specific representation into disjoint groups and define group-wise dense contrasting based on channel positions, aiming to force student to learn teacher's positional channel-group information;
3. **Omni-Contrasting.** Progressively, we unify the above two contrasting methods in an hybrid design, which takes advantage of tactful feature partitions across both channel and spatial dimensions in local areas by splitting the feature map into disjoint patches of the same size.

**Formulation of Af-DCD.** The formulation of Af-DCD follows the notations in Section 3.1. The projected feature maps of teacher and student are denoted as \(F^{t}\in\mathbb{R}^{H\times W\times C^{t}}\) and \(F^{s}\in\mathbb{R}^{H\times W\times C^{t}}\), respectively. In the perspective of contrastive learning, we first clarify some important concepts in our Af-DCD:

1. **Samples and Views.** Different from existing contrastive distillation methods in classification and detection, the samples in Af-DCD are pixel-level or more fine-grained representations in feature maps of the same image, which are naturally dense. Furthermore, we define that representations of teacher and student are two views which should be aligned by our contrastive loss;
2. **Positive Pairs and Negative Pairs.** Under the definitions above, a positive pair can be defined as: a teacher-student sample pair \((F^{s}_{i},F^{t}_{j})\) which has the same index \(i=j\), where \(i\) and \(j\) are general location indices. Similarly, a negative pair can be defined as: a teacher-student sample pair \((F^{s}_{i},F^{t}_{j})\) whose indices are different \(i\neq j\).

Based on the above definitions, the loss for a specific sample \(F^{s}_{i}\in F^{s}\) can be formulated as:

\[l_{Af-DCD}(F^{s}_{i},F^{t})=-\mathrm{log}\frac{\exp(-d(F^{s}_{i},F^{t}_{i})/\tau) }{\sum_{j=1}^{N}\mathds{1}_{i\neq j}\ \exp(-d(F^{s}_{i},F^{t}_{j})/\tau)},\] (4)

where \(\tau\) denotes the temperature score, \(N\) is the total number of contrastive pairs and \(\mathds{1}_{i\neq j}\) is an indicator function which is 1 iff. \(i\neq j\). Different from traditional contrastive loss that adopts cosine distance on measuring similarities, we use Euclidean distance to measure sample pair similarities, where \(d(F^{s}_{i},F^{t}_{i})=||F^{s}_{i}-F^{t}_{i}||^{2}_{2}\). Our ablative experiments illustrate that improved performance would be attained when we select the same type of the distance function for \(L_{fd}\) and \(L_{Af-DCD}\).

Spatial Contrasting.As shown in Figure 2 (top left), aiming to learn teacher's spatial contextual information, Spatial Contrasting constructs dense contrasting operations between teacher's student's pixel-level representations, where for each sample \(F^{s}_{i,j}\), the positive pair is \((F^{s}_{i,j},F^{t}_{i,j})\), while the other \(\{(F^{s}_{i,j},F^{t}_{u,v})\}_{u,v\neq i,j}\) are all negative pairs. In such condition, we obtain 1 positive pair and \(HW-1\) negative pairs. Then Spatial Contrasting can be formulated as:

\[l^{SC}_{Af-DCD}(F^{s}_{i,j},F^{t})=-\mathrm{log}\frac{\exp(-d(F^{s}_{i,j},F^{t} _{i,j})/\tau)}{\sum\limits_{u=1}^{N}\sum\limits_{v=1}^{W}\mathds{1}_{u,v\neq i,j}\ \exp(-d(F^{s}_{i,j},F^{t}_{u,v})/\tau)}.\] (5)

Channel Contrasting.As shown in Figure 2 (top right), Channel Contrasting first splits each pixel representation into \(M\) non-overlapping channel groups of the same length. Specifically, \(F^{s}_{i,j}\) and \(F^{t}_{i,j}\) are split into \(\{F^{s}_{i,j,w}\}_{w=1\dots M}\) and \(\{F^{t}_{i,j,w}\}_{w=1\dots M}\), where \(w\) is the index of channel group. Then, in order to transfer teacher's channel-group information at each pixel position, the dense contrasting happens on each pixel-level representation pair between \(\{F^{s}_{i,j,w}\}_{w=1\dots M}\) and \(\{F^{t}_{i,j,w}\}_{w=1\dots M}\). For each fine-grained representation \(F^{s}_{i,j,k}\), the positive pair is \((F^{s}_{i,j,k},F^{t}_{i,j,k})\) and negative pairs are \(\{(F^{s}_{i,j,k},F^{t}_{i,j,w})\}_{w\neq k}\), where the number of positive and negative pairs is 1 and \(M-1\), respectively. Then we substitute these pairs into Formula 4, where \(w\) denotes the index. Channel contrasting can be formulated as:

\[l^{CC}_{Af-DCD}(F^{s}_{i,j,k},F^{t}_{i,j})=-\mathrm{log}\frac{\exp(-d(F^{s}_{i,j,k},F^{t}_{i,j,k})/\tau)}{\sum\limits_{w=1}^{M}\mathds{1}_{w\neq k}\ \exp(-d(F^{s}_{i,j,k},F^{t}_{i,j,w})/\tau)}.\] (6)

Omni-Contrasting.Omni-Contrasting is an neat combination of Channel Contrasting and Spatial Contrasting, shown in Figure 2 (bottom). Different from the design of Spatial Contrasting, Omni-Contrasting does not contrast all positions of feature maps. Instead, in order to exploit local information, Omni-Contrasting groups pixels into a series of local patches and leverages spatial and channel contrasting within each local patch. This local contrasting can force student to learn teacher's spatial contextual information and positional channel-group information simultaneously, which are beneficial for accurately segmenting the boundary and correctly classify those pixels. After that, \(F^{t}\) and \(F^{s}\) are split into \(\{F^{t}_{p}\}_{p=1\dots N}\) and \(\{F^{s}_{p}\}_{p=1\dots N}\), where \(F^{t}_{p},F^{s}_{p}\in\mathbb{R}^{H\times W\times C^{t}}\) and \(N=\frac{HW}{HW}\). Then, like the operation in Channel Contrasting, the channels will be divided into \(M\) non-overlapping groups. Teacher's and student's representations at position \((i,j,k)\) in local patch \(p\) are denoted as \(F^{t}_{p,i,j,k}\) and \(F^{s}_{p,i,j,k}\), respectively, where \(F^{t}_{p,i,j,k},F^{s}_{p,i,j,k}\in\mathbb{R}^{1\times 1\times\frac{C^{t}}{C^{t}}}\). For \(F^{s}_{p,i,j,k}\), the positive pair is \((F^{s}_{p,i,j,k},F^{t}_{p,i,j,k},)\), while negative pairs are \((F^{s}_{p,i,j,k},F^{t}_{p,u,v,w})\) for any \(u,v,w\neq i,j,k\), where the number of positive and negative pairs is 1 and \(\hat{HW}M-1\), respectively. The Omni-Contrasting can be defined as:

\[l^{OC}_{Af-DCD}(F^{s}_{p,i,j,k},F^{t}_{p})=-\mathrm{log}\frac{\exp(-d(F^{s}_{p,i,j,k},F^{t}_{p,i,j,k})/\tau)}{\sum\limits_{u=1}^{\hat{H}}\sum\limits_{v=1}^{ \hat{W}}\sum\limits_{w=1}^{M}\mathds{1}_{u,v,w\neq i,j,k}\ \exp(-d(F^{s}_{p,i,j,k},F^{t}_{p,u,v,w})/\tau)}.\] (7)

Finally, the overall Omni-Contrasting loss can be defined as:

\[L^{OC}_{Af-DCD}=\frac{1}{HWM}\sum\limits_{p=1}^{N}\sum\limits_{i=1}^{\hat{H}} \sum\limits_{j=1}^{\hat{W}}\sum\limits_{k=1}^{M}l^{OC}_{Af-DCD}(F^{s}_{p,i,j,k},F^ {t}_{p}).\] (8)

## 4 Experiments

### Datasets and Experimental Setups.

**Datasets.** Five popular semantic segmentation datasets, including Cityscapes [27], Pascal VOC [28], Camvid [29], ADE20K [30] and COCO-Stuff-164K [31], are used in our experiments. We conduct our ablation studies on both Cityscapes and ADE20K, which help us to ensure the best setting in most experiments and analyse the effectiveness of our design. _Details for these five datasets are described in supplemental material._

**Experimental Settings.** Following general settings [9; 20; 15] in semantic segmentation distillation, we adopt DeeplabV3 [32] and PSPNet [3] for segmentation framework, ResNet-18 [33] and Mobilenetv2 [34] for student backbones, Resnet-101 for teacher backbone and group various teacher-student pairs. In order to make fair comparison with different state-of-the-art methods, we implement our method on both MMSegmentation codebase [35] and CIRKD codebase [9]. In training and evaluation, we use mean Intersection-over-Union (mIoU) to measure the performance of all methods. In training phase, all models are optimized by SGD with the momentum of 0.9, the initial learning rate of 0.02, and the batch size of 16. The input size is \(512\times 1024\), \(400\times 400\), \(512\times 1024\), \(512\times 1024\), for experiments on Pascal VOC, CamVid, ADE20K and COCO-Stuff-164K, respectively. The input size for experiments on Cityscapes are different in the two codebase, \(512\times 1024\) in CIRKD codebase and \(512\times 512\) in MMSegmentation codebase [15]. In evaluation phase, we follow general settings in [22], which evaluate the performance with the original image size. Our masked reconstruction generator consists of two \(3\times 3\) convolutional layers with ReLU, following [15]. _Other default hyper-parameter settings and implementation details are described in supplemental material._

### Main Results

In this part, we intend to compare the distillation performance of our method Af-DCD with recent state-of-the-art methods for semantic segmentation. Aiming for comprehensive comparison, we conduct experiments on the aforementioned five public datasets following general settings.

**Results on Cityscapes.** In Table 1, we conduct experiments on the most popular Cityscapes dataset to validate the generalization ability of our method to different teacher-student network pairs. The experimental results show that Af-DCD outperforms state-of-the-art methods in most cases, with the maximal margin of 1.12%. In average, Af-DCD brings 3.44% gain to the baseline student models, with the maximal gain of 4.37%. From the results shown in Table 0(a), we can see that our method can well handle teacher-student network pairs in which students (e.g., DeepLabV3-Res18 and DeepLabV3-MBV2) have the same segmentation framework but with different backbones. The

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline Method & Params (M) & FLOPs (G) & 
\begin{tabular}{c} mIOU (\%) \\ Val \\ \end{tabular} \\ \hline \hline T: DeepLabV3-Res101 & 61.1M & 2371.7G & 78.07 & 77.46 \\ \hline S: DeepLabV3-Res18 & & & 74.21 & 73.45 \\ SKD [20] & & & 75.42 & 74.06 \\ IFVD [23] & & & 75.9 & 74.26 \\ CWD [22] & & & 75.55 & 74.07 \\ CIRKD [9] & & & 76.38 & 75.05 \\ MaxKD [17] & & & 77.00 & **75.59** \\
**Af-DCD** & & & **77.03** & 75.12 \\ \hline S: DeepLabV3-MBV2 & & & 73.12 & 72.36 \\ SKD [20] & & & 73.82 & 73.02 \\ IFVD [23] & & & 73.50 & 72.58 \\ CWD [22] & & & 74.66 & 73.25 \\ CIRKD [9] & & & 75.42 & 74.03 \\ MaxKD [17] & & & 75.26 & 74.23 \\
**Af-DCD** & & & **76.38** & **75.06** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance comparison of Af-DCD and recent state-of-the-art distillation methods on Cityscapes, evaluated with different teacher-student segmentation network pairs. \(\ddagger\) denotes distillation without \(L_{kd}\). We follow the way in CIRKD [9] to calculate FLOPs. Best results are bolded.

results of Table (b)b further show that our method can also generalize well to teacher-student network pairs in which students (e.g., DeepLabV3-Res18 and PSPNet-Res18) have different segmentation frameworks but with the same backbone.

**Results on Four Other Datasets.** In Table 2, we evaluate the performance of Af-DCD on four other datasets including PASCAL VOC, Camvid, ADE20K and COCO-Stuff-164K to examine the generalization of our design to handle different semantic segmentation tasks. According to the results shown in Table (a)a-2d, Af-DCD outperforms state-of-the-art methods by significant margins on these four datasets, with the maximal and average margin of 1.75% and 1.22%, respectively. Furthermore, we can observe that our method consistently shows significant absolute mIOU gains (from 1.42% to 3.04%) to different student models on small-size (Camvid), medium-size (Cityscapes and PASCAL VOC) and large-size (ADE20K and COCO-Stuff-164K) datasets.

### Ablation Studies

**Ablation Study on Different Loss Terms.** In our formulation, the overall loss contains three distillation terms, including \(L_{kd}\), \(L_{fd}\) and \(L_{Af-DCD}\). Accordingly, we conduct experiments that independently test the gain from each term to explore the nature of Af-DCD. From the results shown in Table 3, we can get following observations: (i) The accuracy gain of \(Baseline+L_{Af-DCD}^{OC}\) is slight on relatively small dataset Cityscapes (0.28% mIOU gain), but it is notably pronounced on much larger dataset ADE20K (1.50% mIOU gain); (ii) Compared to \(Baseline+L_{fd}\), \(Baseline+L_{Af-DCD}^{OC}\) gets student model with better accuracy on ADE20K dataset, while maintaining almost the same training efficiency; (iii) \(Baseline+L_{fd}+L_{Af-DCD}^{OC}\) gets student models with 76.44% mIOU and 36.01% mIOU on Cityscapes dataset and ADE20K dataset, respectively, which are obviously better than both \(Baseline+L_{fd}\) and \(Baseline+L_{Af-DCD}^{OC}\), showing that two loss terms \(L_{fd}\) and \(L_{Af-DCD}^{OC}\) are complementary; (vi) \(L_{kd}\) can further bring minor extra gains, 0.08% and 0.2%, to \(Baseline+L_{fd}+L_{Af-DCD}^{OC}\) on Cityscapes and ADE20K dataset, respectively.

**Ablation Study on Different Designs.** In this study, we conduct a set of experiments on our three basic contrasting designs, namely Channel Contrasting (CC), Spatial Contrasting (SC) and Omni-Contrasting (OC), in order to exploit the gains from different contrasting dimensions and further verify the necessity of our OC. In Table (a)a and (b)b, we can observe two phenomena:

\begin{table}

\end{table}
Table 2: Performance comparison of Af-DCD and recent state-of-the-art distillation methods on the other four datasets. We follow the way in CIRKD [9] to calculate FLOPs. Best results are bolded.

(i) CC has similar gain to baseline model as SC on Cityscapes, while has much stronger performance on ADE20K, exceeding 0.5% to SC; (ii) The combination of CC and SC performs better than CC and SC, but worse than OC on both datasets (in distillation accuracy and training speed). The above two observations show the superiority of OC, which comes from the neat combination of CC and SC. Specifically, OC groups pixels into a number of disjoint local patches and tactfully leverages CC and SC within each local patch instead of the holistic feature maps to better exploit dense and structured local information for contrastive feature mimicking.

**Ablation Study on Choice of the Function \(d\) in the Omni-Contrasting Loss.** We compare Formula 7 of our method with 3 types of the function \(d\) including \(L_{2}\)-normed distance (our choice), cosine similarity (common choice in contrastive loss) and \(L_{1}\)-normed distance. From the results summarized in Table 4, we can see that our method always shows significant mIOU gains to the baseline with all 3 types of the function \(d\). Comparatively, our method with \(L_{2}\)-normed distance is the best, which supports our intuition that improved performance would be attained by choosing the same type of the function \(d\) for the feature distillation loss (Formula 3) and the Omni-Contrasting loss (Formula 7).

**Ablation Study on Training Efficiency.** Our Af-DCD is naturally augmentation-free and memory-buffer-free. We evaluate the efficiency of our design in terms of training time and GPU memory occupation. In Table 2(a), we can observe that Af-DCD introduces minor extra training cost, from 4.02 hours to 4.25 hours, which only increases 5.7% training time to feature distillation. From the results shown in Table 5, we can see Af-DCD uses less memory and training time, but achieves better performance, compared to CIRKD.

**Ablation Studies on Major Hyper-parameters.** Recall that our method has five hyper-parameters, we also perform experiments to study them. As shown in Figure 3, the effectiveness of our method is relative stable when values of these hyper-parameters are changing. Notably, we find using _max

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline Function \(d\) in Formula 7 & Dataset & mIOU (\%) & mIOU (\%) \\ \hline \(Baseline\) & 73.20 & \(\mu\)a \\ L1-normed distance & CityScapes & 75.97 & +2.77 \\ Cosine similarity & 76.10 & +2.90 \\ \(L_{2}\)-normed distance & **76.44** & **+3.24** \\ \hline \(Baseline\) & 33.91 & \(\mu\)a \\ \(L_{1}\)-normed distance & 35.82 & +1.91 \\ Cosine similarity & 35.95 & +2.04 \\ \(L_{2}\)-normed distance & **36.01** & **+2.10** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablation study on the choice of the function \(d\) in the Omni-Contrasting loss. The experimental setups are the same to Table 1 (default setting without \(L_{kd}\)). Best results are bolded.

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline Method & mIOU (\%) & \(\Delta\)mIOU (\%) & \(T_{train}\) (h) \\ \hline \(Baseline\) & 73.20 & \(\mu\)a & \(\mu\)a \\ \(+L_{d}\) & 75.88 & +2.68 & +0.02 \\ \(+L_{N-DCD}^{\text{SC}}\) & 73.48 & +0.28 & +0.06 \\ \(+L_{d}\) & 76.44 & +3.34 & +2.45 \\ \(+L_{d}\) & 76.04 & +2.84 & +0.05 \\ \(+L_{d}\) & \(+L_{d}\) & +2.84 & +0.05 \\ \(+L_{d}\) & \(+L_{d}\) & +2.47 & +2.32 \\ \hline \(Baseline\) & 73.20 & \(\mu\)a & \(\mu\)a \\ \(+L_{d}\) & 76.23 & +3.03 & +1.3 \\ \(+L_{d}\) & \(+L_{d}\) & +2.66 & +3.06 & +1.8 \\ \(+L_{d}\) & \(+L_{d}\) & +2.66 & +3.13 & +2.29 \\ \(+L_{d}\) & \(+L_{d}\) & +2.66 & +3.24 & +2.5 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation studies on loss terms and their different combinations. The experiments for Cityscapes and ADE20K are conducted on the first teacher-student network pair in Table 2(b) and 2(c), respectively. The training time is measured on 8 NVIDIA RTX A5000 GPUs with 40000 iterations.

Figure 3: Ablation studies on major hyper-parameters. In (d), 4* denotes \(N=4\) with \(4\times 4\) max pooling, which can further enhance training efficiency. Best viewed with zoom in.

pooling_ with proper scale has no obvious effect on model performance but it has better efficiency than larger \(N\). _Other detailed analysis is referred to supplementary materials_.

### Discussion

After demonstrating the superior performance of Af-DCD in Section 4.2 and analysing the gain of Af-DCD in Section 4.3, we perform some verification experiments, aiming to discuss how \(L_{Af-DCD}\) boosts the distillation performance of \(L_{task}+L_{fd}\) (denoted as FD for abbreviation).

As shown in Figure 3(b), the self-similarity distribution of fine-grained representations projected by FD has large differences from that of teacher's feature representations. In order to alleviate the aforementioned problem, \(L_{Af-DCD}\) contrasts feature partitions within each local patch between student and teacher. The results in Figure 3(a) and Figure 3(b) are summarized as below:

1. \(L_{Af-DCD}\) dramatically decreases the distances of student to teacher in views of fine-gained representations and self-similarities, which proves our basic assumption that \(L_{Af-DCD}\) can further make student approach teacher in the micro and fine-grained views;
2. \(L_{Af-DCD}\) increases both the mean and variance of the self-similarity distribution of student. Such observation verifies that \(L_{Af-DCD}\) can force student to learn dense and structured knowledge implicitly contained among teacher's feature partitions.

The above two improvements are proved to be effective in addressing segmenting various difficult scenarios in semantic segmentation, such as object boundary, small object, object occlusion, difficult category and rare view. Heat maps shown in Figure 3(c) illustrate that \(L_{Af-DCD}\) can effectively help FD correctly classify these difficult pixels, which further verifies the superiority of Af-DCD.

_More examples and detailed analysis are referred to supplementary material._

## 5 Conclusion

In this paper, we present Af-DCD, an augmentation-free dense contrastive distillation method tailored to semantic segmentation. The dense contrasting in Af-DCD is an omni-dimensional design. Thus, it can effectively transfer teacher's contextual and positional channel-group information to student. Experimental results show that Af-DCD is effective on different teacher-student network pairs and datasets while maintaining training efficiency, as it is born with augmentation-free and memory-buffer free. We hope our work can inspire researchers to explore more powerful and efficient contrastive distillation methods in the future.

Figure 4: Verification experiments. We leverage the models in Table 2(b), and count 131M fine-grained representations from 2000 samples. In (a), we randomly select 10K fine-grained representations and measure the distances between student and teacher. In (b), we measure the distances among fine-grained representations within \(4\times 4\) local areas of a specific sample, and randomly select 10K to calculate the probability density. In (c), we choose several difficult cases and highlight areas that \(L_{Af-DCD}\) can help \(L_{fd}\) segment correctly.

## References

* [1] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. _TPAMI_, 2017.
* [2] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. _arXiv preprint arXiv:1706.05587_, 2017.
* [3] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In _CVPR_, 2017.
* [4] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* [5] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In _ICLR_, 2015.
* [6] Guodong Xu, Ziwei Liu, Xiaoxiao Li, and Chen Change Loy. Knowledge distillation meets self-supervision. In _ECCV_, 2020.
* [7] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In _ICLR_, 2020.
* [8] Lewei Yao, Renjie Pi, Hang Xu, Wei Zhang, Zhenguo Li, and Tong Zhang. G-detkd: towards general distillation framework for object detectors via contrastive and semantic-guided feature imitation. In _ICCV_, 2021.
* [9] Chuanguang Yang, Helong Zhou, Zhulin An, Xue Jiang, Yongjun Xu, and Qian Zhang. Cross-image relational knowledge distillation for semantic segmentation. In _CVPR_, 2022.
* [10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _ICML_, 2020.
* [11] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In _NeurIPS_, 2020.
* [12] Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan, and Qian Zhang. Rethinking soft labels for knowledge distillation: A bias-variance tradeoff perspective. _arXiv preprint arXiv:2102.00650_, 2021.
* [13] Xiaolong Liu, Lujun Li, Chao Li, and Anbang Yao. Norm: Knowledge distillation via n-to-one representation matching. In _ICLR_, 2023.
* [14] Nikos Komodakis and Sergey Zagoruyko. Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer. In _ICLR_, 2017.
* [15] Zhendong Yang, Zhe Li, Mingqi Shao, Dachuan Shi, Zehuan Yuan, and Chun Yuan. Masked generative distillation. In _ECCV_, 2022.
* [16] Zhendong Yang, Zhe Li, Ailing Zeng, Zexian Li, Chun Yuan, and Yu Li. Vitkd: Practical guidelines for vit feature knowledge distillation. _arXiv preprint arXiv:2209.02432_, 2022.
* [17] Tao Huang, Yuan Zhang, Shan You, Fei Wang, Chen Qian, Jian Cao, and Chang Xu. Masked distillation with receptive tokens. In _ICLR_, 2023.
* [18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _CVPR_, 2022.
* [19] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In _CVPR_, 2022.
* [20] Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, and Jingdong Wang. Structured knowledge distillation for semantic segmentation. In _CVPR_, 2019.

* [21] Tong He, Chunhua Shen, Zhi Tian, Dong Gong, Changming Sun, and Youliang Yan. Knowledge adaptation for efficient semantic segmentation. In _CVPR_, 2019.
* [22] Changyong Shu, Yifan Liu, Jianfei Gao, Zheng Yan, and Chunhua Shen. Channel-wise knowledge distillation for dense prediction. In _ICCV_, 2021.
* [23] Yukang Wang, Wei Zhou, Tao Jiang, Xiang Bai, and Yongchao Xu. Intra-class feature variation distillation for semantic segmentation. In _ECCV_, 2020.
* [24] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _CVPR_, 2020.
* [25] Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, and Zicheng Liu. Seed: Self-supervised distillation for visual representation. _arXiv preprint arXiv:2101.04731_, 2021.
* [26] Jinguo Zhu, Shixiang Tang, Dapeng Chen, Shijie Yu, Yakun Liu, Mingzhe Rong, Aijun Yang, and Xiaohua Wang. Complementary relation contrastive distillation. In _CVPR_, 2021.
* [27] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In _CVPR_, 2016.
* [28] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. _IJCV_, 2010.
* [29] Gabriel J Brostow, Jamie Shotton, Julien Fauqueur, and Roberto Cipolla. Segmentation and recognition using structure from motion point clouds. In _ECCV_, 2008.
* [30] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. _IJCV_, 2019.
* [31] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In _CVPR_, 2018.
* [32] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In _ECCV_, 2018.
* [33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, 2016.
* [34] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In _CVPR_, 2018.
* [35] MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https://github.com/open-mmlab/mmsegmentation, 2020.