# DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via Adaptive Heads Fusion

 Yilong Chen\({}^{1,2}\), Linhao Zhang\({}^{3*}\), Junyuan Shang\({}^{31}\), Zhenyu Zhang\({}^{3}\),

**Tingwen Liu\({}^{1,2}\), Shuohuan Wang\({}^{3}\), Yu Sun\({}^{3}\)**

\({}^{1}\) Institute of Information Engineering, Chinese Academy of Sciences

\({}^{2}\) School of Cyber Security, University of Chinese Academy of Sciences

\({}^{3}\) Baidu Inc.

{chenyilong, liutingwen}@iie.ac.cn

{zhanglinhao, shangjunyuan, zhangzhenyu07, wangshuohuan, sunyu02}@baidu.com

denotes equal contribution. \({}^{\dagger}\) denotes corresponding author. \({}^{\ddagger}\) denotes project lead.

###### Abstract

Large language models (LLMs) with billions of parameters demonstrate impressive performance. However, the widely used Multi-Head Attention (MHA) in LLMs incurs substantial computational and memory costs during inference. While some efforts have optimized attention mechanisms by pruning heads or sharing parameters among heads, these methods often lead to performance degradation or necessitate substantial continued pre-training costs to restore performance. Based on the analysis of attention redundancy, we design a Decoupled-Head Attention (DHA) mechanism. DHA adaptively configures group sharing for key heads and value heads across various layers, achieving a better balance between performance and efficiency. Inspired by the observation of clustering similar heads, we propose to progressively transform the MHA checkpoint into the DHA model through linear fusion of similar head parameters step by step, retaining the parametric knowledge of the MHA checkpoint. We construct DHA models by transforming various scales of MHA checkpoints given target head budgets. Our experiments show that DHA remarkably requires only 2.5% of the original model's pre-training budgets to achieve 96.1% of performance while saving 75% of KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5\(\times\) training acceleration, a maximum of 13.93% performance improvement under 0.01% pre-training budget, and 5% relative improvement under 0.05% pre-training budget.

## 1 Introduction

Transformer-based large language models (LLMs) shine in various natural language tasks due to their powerful understanding and generation capabilities [1; 2; 3]. Multi-Head Attention (MHA) is widely used in LLMs, with the number of heads increasing as the model size grows. However, MHA inference overhead increases linearly with the expansion of the context and model sizes, due to the surprisingly large memory consumption of the _KV Cache_ mechanism. For instance, a 7 billion-parameter model with 32 heads and 32 layers, an input batch size of 4, and a sequence length of 32k results in 64GB of KV cache, which is \(4.7\times\) larger than the model weights.

To reduce computational and memory overhead during inference, a widely used approach involves adapting the MHA model to a more efficient structure through the reuse of parameters across multiple heads [4; 5; 6], such as Multi-Query Attention (MQA) [4] and Grouped-Query Attention (GQA) [5]. These methods utilize a portion of the original training computation which avoid information lossdue to training-inference inconsistencies, a common issue in pruning-based [7; 8; 9; 10; 11] works. However, the training computation is prohibitively expensive for recovering the model's performance, due to the information loss in the parameters when creating the initial point.

Thus, in this work, we seek to address the following question:

_How can we construct a **more efficient** model while keeping **costs as low as possible**?_

With the limited understanding of parameter characteristics in modern LLMs, we first perform an empirical analysis from the perspectives of heads' parameter similarity. We observe that there are some head-clusters with high internal similarity in MHA checkpoints. Similar head clusters imply enormous redundancy in MHA, which coincides with the sparsity found in previous studies [12; 13]. In particular, the clusters of key heads and value heads across different layers show a **decoupled** distribution, meaning that there is a significant variation in the distribution of head-cluster similarities across layers, key heads and value heads, as illustrated in Fig. 2a,2b. Intuitively, we can prune redundant heads based on the above characteristics. Nonetheless, each head has its unique role, and thus no heads should be arbitrarily discarded. Furthermore, we find that linear fusion based on multiple similar heads can reconstruct the original head functionality without causing a significant performance drop (see Sec. 3.1). Based on this observation, we believe that selectively fusing corresponding heads in clusters can construct a more efficient architecture with minimum loss.

In this paper, we propose **Decoupled-Head Attention (DHA)**, an efficient attention architecture developed through the **Adaptive Head Fusion** of checkpoints' parameters. Recalling the decoupled heads parameter characteristics, DHA allocates different numbers of key heads and value heads at different layers to balance model efficiency and performance. The MHA checkpoint can be rapidly transformed into DHA with three stages: **Search**, **Fusion**, and **Continued Pre-training (CT)**. During the Search stage, we group similar functional heads together and determine reasonable allocations of key heads and value heads for each layer. Specifically, we reconfigure the original key and value head into multiple linear combinations of heads within the same layer. Thus, we can allocate the heads based on the loss after replacement. In the Fusion stage, we perform linear fusion on similar heads, ensuring the preservation of original functionality. Leveraging the Augmented Lagrangian approach [14; 15], the Fusion operator initializes from MHA and explores possible head combinations in the early training, followed by refined intra-group head fusion in the later. Based on well-trained operators on unlabeled data, we can rapidly obtain high-performing initial points for DHA from MHA checkpoints, requiring only a minimal amount of Continued Pre-training to restore performance.

Figure 1: **Upper: Overview of Decoupled-head method. Multi-Head attention (MHA) has equal query, key and value heads. Grouped-Query attention (GQA) instead shares single key and value heads for each group of query heads. Decoupled-Head attention (DHA) shares key heads and value heads for different groups of query heads in different layers. Lower: GQA Initialization: Heads are mean pooled into a single head; DHA Initialization: DHA search head grouping and progressively fuse heads to maintain parameter functions.**

To verify the effectiveness, we construct DHA on models of different sizes, such as LLaMA2-7B [3], Sheared-LLaMA-2.7B & -1.3B [16] with the heads budget ratio set at 50% and 25%. With a modest fusion training of just 0.2 billion tokens, DHA learns sufficiently competent initial points. As the continued pretraining progresses, DHA continuously outperforms GQA narrowing the gap with MHA on 9 representative downstream tasks. DHA only requires 0.25% of MHA pre-training budget. Meanwhile, DHA is capable of reducing _KV Cache_ by up to 75% compared to MHA with minimal accuracy trade-off (maximum of 5.6%). Compared to GQA, DHA achieves a 5\(\times\) training acceleration, a maximum 13.93% performance improvement under 0.01% pre-training budget, and 5% relative improvement under 0.05% pre-training budget. Overall, DHA exhibits great performance and efficiency, which can be quickly adapted to various existing MHA Transformer models.

## 2 Background

Let \(\mathbf{X}=(\mathbf{x}_{1},\ldots,\mathbf{x}_{p})\in\mathbb{R}^{p\times d_{ \text{model}}}\) denote the input prompts of hidden states of a Transformer layer, where \(p\) stands for the number of tokens and \(d_{\text{model}}\) for the hidden state dimension.

Multi-Head Attention (MHA)MHA [17] performs the attention with \(H\) different heads. For \(h\)-th head, different weight matrices \(\text{W}^{h}_{q},\text{W}^{h}_{k},\text{W}^{h}_{v}\in\mathbb{R}^{d_{\text{ model}}\times d_{k}}\) are used to project the input sequence into query, key, value vector, where \(d_{k}\) represents head dim. Denote softmax function as \(\sigma\), we have:

\[\text{MHA}=\operatorname{Concat}\left(\text{head}_{1},\ldots,\text{head}_{ \text{H}}\right)\text{W}_{O},\text{where head}_{h}=\sigma\left(\mathbf{X} \text{W}^{h}_{q}(\mathbf{X}\text{W}^{h}_{k})^{T}\cdot\frac{1}{\sqrt{d_{k}}} \right)\mathbf{X}\text{W}^{h}_{v}\] (1)

Ultimately MHA combines heads' outputs through the output projection \(\text{W}_{O}\in\mathbb{R}^{d_{\text{model}}\times d_{\text{model}}2}\).

Grouped-Query Attention (GQA) & Multi-Query Attention (MQA)To accelerate inference, MQA [4] and GQA [5] have been proposed based on the idea of reusing head parameter weights. In these variants, \(H\) different query heads are divided into \(G\) groups, where the heads within the same group share the same key heads and value heads parameter matrices. Given the mapping relationship from the \(h\)-th query head to a GQA key and value heads using the many-to-one function \(g(h)\), we define the \(h\)-th head forward pass as:

\[\text{head}_{h}=\sigma\left(\mathbf{X}\text{W}^{h}_{q}(\mathbf{X}\text{W}^{g (h)}_{k})^{T}\frac{1}{\sqrt{d_{k}}}\right)\mathbf{X}\text{W}^{g(h)}_{v},\text {where W}^{g(h)_{\text{min}}}_{k/v}=\frac{\sum_{\text{W}_{k/v}\in\mathbb{K}/ \text{V}_{g(h)_{\text{min}}}}\text{W}_{k/v}}{\left|\mathbb{K}/\text{V}_{g(h)_ {\text{min}}}\right|}\] (2)

Here, \(\mathbb{K}/\text{V}_{g(h)_{\text{min}}}\) refers to MHA key/value heads parameters within the \(g(h)_{\text{min}}\)-th group during GQA initialization. When transitioning from an MHA checkpoint, GQA uses the mean pooling method for heads within the group. MQA is a special case of GQA where \(G=1\)3.

Footnote 3: GQA and MQA consist of \(H+2\times G\) heads in total and store a \(2\times G\times p\times d_{\text{model}}\) dimension KV cache.

Due to mean pooling for initialization, GQA results in loss of capability when converting from MHA, necessitating expensive pre-training to recover. We aim to identify better initialization and more refined head mapping relationships to achieve superior performance with reduced training costs.

Figure 2: Visualization of the similarity between heads within the MHA of LLaMA2-7B model at the 0th layer (a) and the 21st layer (b). Details in Appendix E.1. Key heads and value heads exhibit decoupled distributions.

Observation

To study the inherent characteristics of head parameters in MHA, we use Centered Kernel Alignment [18] to calculate the heads' similarity within each layer's W\({}_{k},\) W\({}_{v}\). Based on the average heads' similarity, we define the redundancy of each MHA layer. For details, please refer to Appendix B.1.

### Head clusters in MHA

ObservationFrom Fig. 1(a) and Fig. 1(b), we observe that clusters form spontaneously among heads, with high similarity within clusters and low similarity between clusters. It indicates that heads among different clusters may have distinct functionalities, processing linguistic features in various aspects.

AnalysisGiven the numerous similar head clusters in W\({}_{k}\) and W\({}_{v}\), we identified the opportunity to linearly fuse functionally similar heads within clusters while retaining each head's parameterized knowledge. We conducted an empirical study, transforming the parameters of Head 0 in MHA into a linear fusion of the parameters from Heads 0, 1, 2, and 3. We share the fusion head across four query heads and progressively optimize the fusion ratio under the LmLLoss. For details, please refer to Sec. 4.2. As shown in Fig. 2(a), the loss remains unchanged as the proportion of Head 0 decreases and only increases when four heads parameters' ratios approach an even distribution. It suggests that fusing similar parameters can reduce the number of heads without significant information loss.

### Variability across Layers and KV pairs

ObservationThe distribution of similar head clusters varies between different layers. As illustrated in Fig.1(a), 1(b), the 0th layer of MHA shows few similar head clusters, while the 21st layer exhibits many. Within the same layer, value heads exhibit more clusters and higher similarity compared to key heads, indicating a divergence between the two. Fig. 2(b) shows that the redundancy is lower in the initial and final layers, and higher in the middle layers. Moreover, W\({}_{v}\) redundancy significantly exceeding that of W\({}_{k}\).

AnalysisInspired by layer and key-value head variability, we propose allocating more heads to layers with lower redundancy to enhance learning and expression. Since W\({}_{v}\) shows higher redundancy than W\({}_{k}\), we can decouple and allocate more heads budget to critical key components, while compressing redundant value heads at a higher compression rate. Finer grouping and sharing based on the parameters function may contribute to compression rates and performance improvements.

## 4 Method

In this section, we propose a more efficient Decoupled Head Attention (DHA) architecture and its construction process. We define DHA in Sec. 4.1 and Adaptive Head Fusion algorithm in Sec. 4.2. Then we demonstrate the adaptive construction based on the MHA checkpoint, which can be divided into: **Search**, **Fusion**, and **Continued Pre-training** (Discussed in Sec. 4.3). Finally, we introduce practical application of our DHA architecture on the LLaMA2 model in Sec. 4.3.

### Decoupled-Head Attention (DHA)

We present a more efficient attention architecture called Decoupled-Head Attention (DHA). Based on observed significant functional differences among different layers' key value heads, DHA adaptively allocates more heads to critical components, thus enhancing overall model efficiency and performance.

Figure 3: (a) Model loss with heads proportions in linear fusion. (b) Layer Redundancy of the query, key, value head parameter matrices in the LLaMA2-7B model MHA.

DefinitionDefined model with \(L\) layers and \(H^{\text{Q}}\) heads in a layer, the numbers of Key heads and Value heads in the \(l\)-th layer are denoted as \(H^{\text{K}}_{l},H^{\text{V}}_{l}\). We define the **many-to-one** mapping functions \(d^{\text{K}}(h,l)\) and \(d^{\text{V}}(h,l)\) representing key and value head corresponding to the \(h\)-th query head in \(l\)-th DHA layer. The computation be formalized as follows:

\[\text{head}_{h,l}=\sigma\left(\mathbf{X}\mathbf{W}_{q}^{h}(\mathbf{X}\mathbf{W }_{k}^{d^{\text{K}}(h,l)})^{T}\cdot\frac{1}{\sqrt{d_{k}}}\right)\mathbf{X} \mathbf{W}_{v}^{d^{\text{V}}(h,l)}\] (3)

DHA shares a key and value head in multi query heads' computation based on independent mapping functions at different layers4. GQA can be considered a special case of DHA, where not only all layers share the same mapping functions, but the mapping functions for keys and values are identical.

Footnote 4: DHA consists of \(H=H^{\text{Q}}+\sum_{l=1}^{L}H^{\text{K}}_{l}+\sum_{l=1}^{L}H^{\text{V}}_{l}\) heads in total.

### Learning Efficient MHA Transformation via Linear Heads Fusion

Due to the high cost of building an efficient Attention mechanism in LLM from scratch, we construct DHA based on the existing MHA checkpoint using minimal computational budgets. Based on the head clustering phenomenon in MHA, we propose a linear fusion method for similar heads within clusters. By incrementally fusing head parameters, we compress the number of heads while retaining the original model's knowledge, significantly reducing training budgets and improving performance.

GoalFormally, we define a model with Layer number \(L\) and Head number \(H\) as \(\mathbf{\Theta}_{L,H}=[\text{W}_{1},\cdots,\text{W}_{L}]\), where \(\text{W}_{l}\in\mathbb{R}^{D\times D}\) denotes the weight of layer \(l\) with input and output dimension \(D\). In the initialization, our goal is to transfer knowledge from a MHA model \(\mathbf{\Theta}_{L,H_{1}}^{\text{MHA}}\) to a DHA model \(\mathbf{\Theta}_{L,H_{2}}^{\text{DHA}}\), where \(H_{1}>H_{2}\). By learning a fusion operation that minimizes the functional difference between MHA and DHA model, the goal can be formalized as

\[\operatorname*{arg\,min}_{\mathbf{\Theta},\mathcal{M}}\mathbb{E}_{\mathbf{z} \sim\mathcal{D}}\ \left[\mathcal{L}_{\text{lin}}\left(\mathbf{x};\mathcal{M}(\mathbf{\Theta}^{ \text{MHA}})\right)+\lambda\mathcal{L}_{\text{fusion}}\left(\mathbf{x}; \mathcal{M}(\mathbf{\Theta}^{\text{MHA}}),\mathbf{\Theta}^{\text{DHA}}\right)\right]\] (4)

Where \(\mathcal{M}\) is the fusion operator, \(\mathcal{D}\) is the training dataset, \(\mathcal{L}_{\text{lin}}\) is the training loss function, \(\mathcal{L}_{\text{fusion}}\) measures the transformation from MHA to DHA, and \(\lambda\) is the learnable scale factor.

Fusion OperatorDuring DHA initialization, the fusion operator \(\mathcal{M}\) constructs new heads based on the linear combinations of the original key and value heads within the group, and shares the new heads among the query heads' forward. Define each group \(\mathbb{K}_{d^{\text{K}}(h,l)},\mathbb{V}_{d^{\text{V}}(h,l)}\) represents key, value heads group corresponding to the \(h\)-th query head in \(l\)-th layer, \(g=\left\{g^{\text{K}},g^{\text{V}}\right\}\) as the group size. By introducing variables \(\omega_{h}=\left\{\omega_{hj}\right\}_{j=1}^{g},\omega\in\mathcal{M}\) represents the proportion of \(j\)-th key, value head

Figure 4: An illustration of DHA. First, we reconstruct the a single head forward as a linear combination of multiple heads’ forward with proportions \(\omega\), grouping heads with similar functions based on multi-step optimization. Next, we initialize and optimize the fusion operators. \(\Leftrightarrow\) indicates the optimization narrows the distance between proportions \(\omega\). Finally, we fuse heads within groups and continued pre-training DHA model.

involved in the \(h\)-th query head forward within group. For each group, a head have forward pass as:

\[\text{head}_{h,l}=\sigma\left(\mathbf{X}\mathbf{W}_{q}^{h}(\mathbf{X}\mathbf{W}_{ k}^{d^{\text{K}}(h,l)})^{T}\cdot\frac{1}{\sqrt{d_{k}}}\right)\mathbf{X}\mathbf{W}_{v}^{d^{ \text{K}}(h,l)},\text{where }\mathbf{W}_{k/v}^{d^{\text{K}\text{V}}(h,l)}=\sum_{j=1}^{g^{ \text{K}\text{V}}}\omega_{hj}\mathbf{W}_{k/v}^{j}\] (5)

where \(\omega_{hj}\) will be initialized to Kronecker delta function, which equals 1 if and only if \(h=j\), and equals 0 otherwise. Under this initialization setting, the forward computation of DHA is completely equivalent to that of MHA, see Fig. 4.

OptimizationDuring the optimization phase, we design a fusion loss to optimize the initialized model towards DHA target architecture. Note that after initialization, the mapping of heads within the group \(\mathbf{W}_{q}^{h,l}\rightarrow\mathbf{W}_{k/v}^{j}\) is a **many-to-many** mapping, denoted by the function \(d^{\text{K}\text{V}}_{\text{init}}(h,l)\). This indicates that in the forward process of each query, the key head or value head can be expressed as different linear combinations of \(g\) MHA heads. According to Eq. 3, we aim to achieve a **many-to-one** mapping that a single fused key head or value head are shared across multiple query heads in DHA, denoted by the function \(d^{\text{K}\text{V}}(h,l)\). Thus, we design a fusion loss \(\mathcal{L}_{\text{fusion}}\) to optimize the initial mapping functions to converge to a single mapping function, i.e., \(d^{\text{K}\text{V}}_{\text{init}}(h,l)\to d^{\text{K}\text{V}}(h,l), \forall h\in\mathbb{K}_{n}/\mathbb{V}_{n}\). Specifically, we define the optimization objective as minimizing the difference between the mapping functions of different query heads \(h\) and \(h^{\prime}\) within the \(l\)-th layer and \(n\)-th group:

\[\mathcal{L}_{\text{head}_{l}^{\text{T}}}(h,h^{\prime})=\frac{1}{g}\left\|\sum _{j=1}^{g}\omega_{hj}\mathbf{W}_{k/v}^{j}-\sum_{j=1}^{g}\omega_{h^{\prime}j} \mathbf{W}_{k/v}^{j}\right\|^{2}=\frac{1}{g}\left(\sum_{j=1}^{g}(\omega_{hj}- \omega_{h^{\prime}j})\mathbf{W}_{k/v,ij}^{j}\right)^{2}\] (6)

where \(g=g^{\text{K}\text{V}}\) represents the number of heads within a group. Since \(W_{k/v,ij}^{j,\text{MHA}}\) can be regarded as an orthogonal scalar, and thus we only need to optimize fusion variables \(\omega\), so we have:

\[\mathcal{L}_{\text{fusion}}=\sum_{l=1}^{L}\sum_{n=1}^{N}\sum_{h=1}^{g}\sum_{h^ {\prime}=h+1}^{g}\mathcal{L}_{\text{head}_{l}^{\text{T}}}(h,h^{\prime}), \text{subject to }\mathcal{L}_{\text{head}_{l}^{\text{T}}}(h,h^{\prime})=\frac{1}{g}\sum_{h=1 }^{g}\sum_{j=1}^{g}(\omega_{hj}-\omega_{h^{\prime}j})^{2}\] (7)

Where \(N\) represents the number of groups, \(N=\frac{H_{1}}{g}\). The fusion loss can be measured as the mean squared error loss of the head and head fusion variables within each group at each layer.

Augmented Lagrangian approachWhen the fusion loss is zero, the key and value heads corresponding to query heads within the group are optimized to share the same fusion variables. This allows the new DHA key-value head parameters to be effectively shared among the queries in the group. Given that it is challenging to optimize the loss to a very small value, we use an augmented Lagrangian approach [14; 15] for incremental architectural transformations. Define \(t\) as the target loss, \(b\) as the base decay factor, \(s\) as the current global step, \(k\) as the total number of steps in the warm-up phase, the overall training optimization is an adversarial game:

\[\max_{\lambda}\min_{\Theta,\mathcal{M}}\mathbb{E}_{\mathbf{z}\sim\mathcal{D}} \left[\mathcal{L}_{\text{lm}}\left(\mathbf{z};\mathcal{M}(\mathbf{\Theta}^{ \text{MHA}})\right)+\lambda\max\left(\mathcal{L}_{\text{fusion}}-t,0\right) \right],\text{where }t=\max\left(0,b^{s}\left(1-\frac{s}{k}\right)\right)\] (8)

Our Augmented Lagrangian approach enforces the constraint \(\mathcal{L}_{\text{fusion}}\leq t\), where the Lagrange multiplier \(\lambda\) is updated during training. The update increases the loss unless the constraint is satisfied. Early in training, the model tolerates more significant discrepancies between head weights, promoting exploration. As training progresses, the margin shrinks, enforcing stricter adherence to minimizing discrepancies and refining head alignment within the group.

### Adaptive DHA Transformation on LLaMA Model

Based on the observation of similar head clusters and key-value head parameter variability across layers, DHA employs the adaptive transformation. It allows DHA to search for and fuse similar heads while allocating different group sizes across layers. As shown in Fig. 4, the transformation can be divided into three stages: **Search**, **Fusion** and **Continued Pre-training**.

In the beginning, we initialize the DHA operators to the MHA model. Next, we perform 240 **Search** steps, calculating \(\mathcal{L}_{\text{fusion}}\) for each layer and \(\mathcal{L}_{\text{head}}\) for all heads. Based on the \(\mathcal{L}_{\text{head}}\), we perform head grouping intending to minimize the average loss of heads within each group and maximize the average loss of heads between groups and groups. Based on \(\mathcal{L}_{\text{fusion}}\), we use a dynamic programmingalgorithm to allocate more head budget to layers with higher loss within a total budget. It allows us to fuse the most similar heads to minimize loss during the fusion process and selectively compress the model's most redundant components. For more details, see Appendix B.3, B.4.

During **Fusion** phase, we modified the forward propagation path of MHA in the form of DHA based on the layer head budget and head grouping obtained during the **Search** phase. Then we antagonistically optimize the fusion operator and update Lagrangian multipliers \(\lambda\), the \(\mathcal{L}_{\text{fusion}}\) that marks this DHA fusion process decreases. When \(\mathcal{L}_{\text{fusion}}\) is less than 1e-3, we terminate the fusion algorithm and enter the **Continued Pre-training** phase.

During the **Continued Pre-training** phase, we fuse MHA head parameters based on averaged fusion weights to construct DHA initialization. DHA initialization can recover the performance with a small amount of restorative pre-training. For more information, please refer to Appendix B.2.

Our method can theoretically transform MHA architecture in any transformer model to efficient DHA architecture. Using LLaMA models as case studies, we implemented DHA transformation with various compression rates on all MHA layers. Notably, we expanded the dimension of each head's fusion coefficient \(\omega\) from 1 to the head's dimension \(d_{k}\), allowing for finer-grained parameter fusion and better knowledge retention. Intuitively, we learn different fusion ratios for each dimension of the head. Only a very small number of additional parameters need to be introduced, DHA significantly accelerates training and improves performance.

## 5 Empirical Evaluation

### Experimental Setup

Data.To train DHA operators and extend pre-training, we employ the RedPajama [19], which parallels the LLaMA training data across seven domains: CommonCrawl, C4, GitHub, Wikipedia, Books, ArXiv, and Stack-Exchange. This dataset comprises a validation set with 2 million tokens, a training set containing 4 billion tokens and an additional pre-training set totaling 50 billion tokens.

Training.Our experimental framework utilizes the Sheared-LLaMA codebase [16] implemented on the Composer package [20], and is executed on 8 NVIDIA A100 GPUs (80GB). The models are trained with a sequence length of 4096, employing a global batch size of 64 during the fusion phase and 256 during the continued pre-training phases. The learning rates were set at 1e-4 for language modeling loss, and 1e-2 for Lagrangian multipliers and fusion operators respectively.

Budget.DHA models were trained for 1000 steps (0.2B token budget) during the fusion phases. For the continued pre-training, we trained both baseline models and DHA for up to 50000 steps (50B token budget). To evaluate the training acceleration capability of DHA, we evaluate its performance under two budget scenarios. First, we set a budget of **1B tokens** to compare the early-stage rapid convergence capabilities of DHA and GQA. Then, we set a budget of **50B tokens** to further assess the performance of DHA over a more extended training period.

Evaluation.We employed the lm-evaluation-harness [21] to evaluate our models. For common sense and reading comprehension tasks, we report 0-shot accuracy results for SciQ [22], PIQA [23], WinoGrande (Wino.) [24], ARC Easy(ARC-E.) [25], and HellaSwag (HellaS.) [26], alongside 25-shot accuracy for ARC Challenge (ARC-C.) [27]. In the assessments of continued QA and text understanding, we report 0-shot accuracy for LogiQA [28], 32-shot BoolQ [29], and 0-shot LAMBADA [30]. All reported results were calculated with the mean and stderr of multiple experiments.

Instruction tuning evaluation.To assess our models' capabilities after instruct tuning [31; 32], we fine-tune both DHA and baseline models on 10,000 instruction-response pairs from the ShareGPT dataset 5 and evaluate on another 1,000 instructions, using GPT-4 for response evaluator [33]. The win rate of our model relative to the baseline is reported. For detailed information, refer to Appendix C.1.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_EMPTY:9]

demonstrates the necessity and effectiveness of the fusion stage under low-resource conditions. When we have a larger training budget, we can allocate more resources to the fusion stage to achieve a better initialization point for DHA.

Heads Budget Allocation.We investigated how the model adaptively allocates decoupled head group sizes across different layers under global head budgets. As illustrated in Fig. 8, the head numbers of DHA layers decrease from higher to lower across layers. Deeper layers exhibit higher compression rates due to greater redundancy. However, the initial and crucial layers need more heads, suggesting they may have specialized functions. As shown in Fig. 7, we presented the LM loss for the cold-start training of DHA models initialized with parameter averaging under different DHA configurations obtained at various search steps. Despite using the same initialization method as GQA, DHA exhibits a faster loss decline and a lower final loss. This indicates that DHA's architecture can accelerate training and achieve better performance, even without Linear Heads Fusion method.

Parameter Characteristics in DHA.For interpretability analysis, we visualized the parameter characteristics of the post-fusion DHA model in Fig. 9 (details in Appendix E.2), and compared them with those prior to fusion. The DHA parameter distribution shows consistency with MHA's. This indicates that DHA effectively aggregates multiple similar functional heads within clusters and new fused heads successfully reconstruct the functionalities of multiple origin heads in MHA. It is noteworthy that the significant reduction in the number of similar heads within the DHA architecture indicates that our method effectively reduces redundancy among the heads.

## 6 Related Work

Advanced Multi-Head Attention.Some efforts have been converting the traditional Multi-Head Attention (MHA) [17] to Multi-Query Attention (MQA) [4], Group-Query Attention (GQA) [5] or GQKVA [6]. These methods achieve a balance between performance and efficiency by reducing the number of head parameters through parameter reuse across grouped heads. DHA is inspired by these methods and has a much higher optimization rate and much less training overhead.

Efficient Pre-training Approaches.In recent years, the ability of incremental training to accelerate large-scale model training by studying how to obtain the optimal initialization point for training has thus attracted much attention [34; 35]. Net2Net [36] uses function-holding transformations to expand the width by duplicating neurons, and uses a unitary layer implementation to expand the depth. LiGO [37] proposes a learnable expansion method that can be used at the initial initialization point of a transformer. DHA is inspired by these methods, but we investigate how to learn to map the parameter matrix from large to small without losing the ability of the larger model itself. For additional related work, please refer to Appendix A.

## 7 Conclusion

In this paper, we propose an efficient attention architecture and a method for fast converting an MHA checkpoint into an efficient structure. By grouping similar heads and performing controlled linear fusion, we develop an initial DHA architecture that decouples head components at various layers, reducing training overhead while maintaining performance. Experimental results show that our method preserves the knowledge of the original model, improving training acceleration, inference efficiency, and computational cost savings. This transformation paradigm offers research value and potential for broader application with minimal performance loss and reduced computational effort.

## Acknowledgments

We would like to thank Yinqi Yang, Jiawei Sheng, Xinhua Zhang, Shicheng Wang, Chuanyu Tang and members of the IIE KDsec group for their valuable feedback and discussions. We are very grateful to Mengzhou Xia for providing the concise and effective ShearingLLaMA experimental code and for her assistance during the reproduction process. Work done during Yilong Chen's internship in Baidu Inc. This research is supported by the National Key Research and Development Program of China (grant No.2021YFB3100600) and the Youth Innovation Promotion Association of CAS (Grant No. 2021153).

## References

* [1] Anthropic. Introducing calude. 2023.
* [2] OpenAI. Gpt-4 technical report. _ArXiv_, page abs/2303.08774, 2023.
* [3] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaqiang Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023.
* [4] Noam Shazeer. Fast transformer decoding: One write-head is all you need. _arXiv preprint arXiv:1911.02150_, 2019.
* [5] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. _arXiv preprint arXiv:2305.13245_, 2023.
* [6] Farnoosh Javadi, Walid Ahmed, Habib Hajimolahoseini, Foozhan Ataiefard, Mohammad Hassanpour, Saina Asani, Austin Wen, Omar Mohamed Awad, Kangling Liu, and Yang Liu. Gqkva: Efficient pre-training of transformers by grouping queries, keys, and values, 2023.
* [7] Saurabh Agarwal, Bilge Acun, Basil Homer, Mostafa Elhoushi, Yejin Lee, Shivaram Venkataraman, Dimitris Papailiopoulos, and Carole-Jean Wu. Chai: Clustered head attention for efficient llm inference, 2024.
* [8] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. _arXiv preprint arXiv:2306.14048_, 2023.
* [9] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. _arXiv preprint arXiv:2305.17118_, 2023.
* [10] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for lms. _arXiv preprint arXiv:2310.01801_, 2023.
* [11] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. _arXiv preprint arXiv:2309.17453_, 2023.
* [12] Yujia Qin, Cheng Qian, Jing Yi, Weize Chen, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Exploring Mode Connectivity for Pre-trained Language Models, October 2022.
* [13] Kaiyan Zhang, Ning Ding, Biqing Qi, Xuekai Zhu, Xinwei Long, and Bowen Zhou. CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model, October 2023.
* [14] Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. On dual decomposition and linear programming relaxations for natural language processing. In Hang Li and Lluis Marquez, editors, _Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing_, pages 1-11, Cambridge, MA, October 2010. Association for Computational Linguistics.
* [15] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured Pruning of Large Language Models. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2020.
* [16] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning, October 2023.

* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Kornblith et al. [2019] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited, 2019.
* Redpajama [2023] TogetherAI. Redpajama: An open source recipe to reproduce llama training dataset, 2023.
* [20] The Mosaic ML Team. composer. https://github.com/mosaicml/composer/, 2021.
* Gao et al. [2023] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023.
* Welbl et al. [2017] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors, _Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017_, pages 94-106. Association for Computational Linguistics, 2017.
* Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 7432-7439. AAAI Press, 2020.
* Sakaguchi et al. [2020] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 8732-8740. AAAI Press, 2020.
* Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_, 2018.
* Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Anna Korhonen, David R. Traum, and Lluis Marquez, editors, _Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, pages 4791-4800. Association for Computational Linguistics, 2019.
* Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. _CoRR_, abs/1803.05457, 2018.
* Liu et al. [2020] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. _arXiv preprint arXiv:2007.08124_, 2020.
* Clark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. _arXiv preprint arXiv:1905.10044_, 2019.
* Paperno et al. [2016] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambda dataset: Word prediction requiring a broad discourse context. _arXiv preprint arXiv:1606.06031_, 2016.
* Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35, 2022.
* Taori et al. [2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.

* [33] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. _arXiv preprint arXiv:2305.14387_, 2023.
* [34] Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation. In _2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017_, pages 5075-5084. IEEE Computer Society, 2017.
* [35] Lemeng Wu, Dilin Wang, and Qiang Liu. Splitting steepest descent for growing neural architectures. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 10655-10665, 2019.
* [36] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. _arXiv preprint arXiv:1511.05641_, 2015.
* [37] Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to Grow Pretrained Models for Efficient Transformer Training, March 2023.
* [38] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. _arXiv preprint arXiv:2009.06732_, 2020.
* [39] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_, 2019.
* [40] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* [41] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. _Advances in neural information processing systems_, 2020.
* [42] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_, 2020.
* [43] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. _CoRR_, abs/1901.02860, 2019.
* [44] Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-doc: A retrospective long-document modeling transformer. _arXiv preprint arXiv:2012.15688_, 2020.
* [45] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. _Advances in Neural Information Processing Systems_, 35:11079-11091, 2022.
* [46] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. _arXiv preprint arXiv:2305.14788_, 2023.
* [47] Yilong Chen, Junyuan Shang, Zhengyu Zhang, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, and Haifeng Wang. Mixture of hidden-dimensions transformer, 2024.
* [48] Matanel Oren, Michael Hassid, Yossi Adi, and Roy Schwartz. Transformers are multi-state rnns. _arXiv preprint arXiv:2401.06104_, 2024.
* [49] Yilong Chen, Guoxia Wang, Junyuan Shang, Shiyao Cui, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, Yu Sun, Dianhai Yu, and Hua Wu. NACL: A general and effective KV cache eviction framework for LLM at inference time. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 7913-7926, Bangkok, Thailand, August 2024. Association for Computational Linguistics.
* [50] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale, 2022.
* [51] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers, 2023.

* [52] Robert M. Gray and David L. Neuhoff. Quantization. _IEEE transactions on information theory_, 44(6):2325-2383, 1998.
* [53] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.
* [54] Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. Knowledge distillation: A survey. _International Journal of Computer Vision_, 129(6):1789-1819, March 2021.
* [55] Lianshang Cai, Linhao Zhang, Dehong Ma, Jun Fan, Daiting Shi, Yi Wu, Zhicong Cheng, Simiu Gu, and Dawei Yin. Pile: Pairwise iterative logits ensemble for multi-teacher labeled distillation. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track_, pages 587-595, 2022.
* [56] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, and Beidi Chen. Deja vu: Contextual sparsity for efficient LLMs at inference time. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 22137-22176. PMLR, 23-29 Jul 2023.
* [57] Elias Frantar and Dan Alistarh. SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot, March 2023.
* [58] Minjia Zhang and Yuxiong He. Accelerating training of transformer-based language models with progressive layer dropping. _Advances in Neural Information Processing Systems_, 33:14011-14023, 2020.
* [59] Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. On the effect of dropping layers of pre-trained transformer models. _Computer Speech & Language_, 77:101429, 2023.
* [60] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. _Advances in Neural Information Processing Systems_, 35:16344-16359, 2022.
* [61] [2401.02415] LLaMA Pro: Progressive LLaMA with Block Expansion.
* [62] Yilong Chen, Junyuan Shang, Zhenyu Zhang, Shiyao Cui, Tingwen Liu, Shuohuan Wang, Yu Sun, and Hua Wu. LEMON: Reviving stronger and smaller LMs from larger LMs with linear parameter fusion. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8005-8019, Bangkok, Thailand, August 2024. Association for Computational Linguistics.

## Appendix A Extended Related Works, Discussions, and Limitations

### Extended Related Works

Efficient Transformers.Efficient Transformers[38] have been extensively explored [39, 40, 41, 42, 43, 44, 45, 46, 47] to address the self-attention operation which scales quadratically with the sequence length. For instance, Sparse Transformer [39] uses a dilated sliding window the reduces the attention complexity. Longformer [42] and Bigbird [41] reduced the complexity of self-attention by combining random, window and global attention. Recurrence Transformers [43] maintain a memory bank of past KV cache to process the long text in segments. However, the above methods either result in a loss of model performance or require retraining the model, which is unaffordable for the high computational resources of LLMs. DHA requires very little computation to transform checkpoints into an efficient architecture that balances performance and computational resources.

KV Cache Compression.KV Cache Compression methods emerged for reducing the prominent inference bottleneck caused by KV cache, particularly for long content input. A series of methods [8, 10, 9, 48, 49] explored the sparsity among Transformer's attention block, then evicted unnecessary tokens from KV Cache for efficient inference. However, these methods discard information from the context and use algorithms for inference that are inconsistent with the training phase, which can cause model performance degradation. DHA does not need to discard information from the context and is able to maintain consistent performance for training and inference. Pruning [15, 16], quantization [50, 51, 52] and distillation [53, 54, 55] can reduce the number of model key and value headers, parameter dimensions, and activation to reduce memory bandwidth overhead during model inference. Deja Vu [56] and CHAI [7] prune pruning redundant heads through clustering methods for efficient inference. In the LLM era, this leads to a significant reduction in neuron redundancy as models move from task-specific to generalized [57]. The application of these methods to LLMs is computationally expensive and leads to performance degradation at larger pruning magnitudes.

**Model Compression.** Our approach is dedicated to obtaining a high-performance lightweight language model, which is the same goal as the task of model compression. Quantization [52] reduces the numerical accuracy of model weights and activations, and speeds up training and inference, but results in a loss of model accuracy and the inability to freely build target-specific models. CRash [13] and LayerDrop [58, 59] methods discard ineffective layers during training, which do not allow for target-specific structuring and come with a large performance loss. Pruning [15] minimizes the impact on performance by cutting out redundant neurons that over-parameterize the model. In the LLM era, this leads to a significant reduction in neuron redundancy as models move from task-specific to generalized [57]. Pruning LLM leads to performance degradation at larger pruning magnitudes. LLMsheairng [16] uses the results of pruning as initialization for continuous pre-training of the model to recover performance, but this approach requires more data and computational overhead. We avoid the information loss caused, by learning the parameter fusion matrix of the model to reach a specific structure, thus obtaining better initialization points and reducing the overhead of continuous pre-training.

### Broader Impact and Limitations

#### a.2.1 Broader Impact

In this paper, we observe the MHA head mechanism and report the phenomenon of modular clustering of heads in MHA. This paper innovatively proposes linearly fusible parameters within the model, and designs linear fusion operators and related experiments to verify the low-loss fusible nature of the parameters. This helps to advance parameter fusion theory and LLM interpretability studies, which provide a foundation and inspiration for future algorithmic advancements, encouraging further optimization and innovation in LLMs. Our work on Decoupled-Head Attention (DHA) represents an advancement in optimizing the efficiency of Large Language Models (LLMs). By addressing thesubstantial computational and memory costs associated with the widely used Multi-Head Attention (MHA), DHA enhances the applicability of LLMs in various domains. The introduction of DHA not only achieves a remarkable balance between performance and efficiency but also significantly reduces the need for extensive pre-training, making the deployment of LLMs more feasible and cost-effective. This efficiency allows for the broader accessibility of advanced LLMs, democratizing technology and fostering innovation across industries. Furthermore, by requiring only 0.25% of the original model's pre-training budgets to achieve near-original performance while saving 75% of KV cache, DHA contributes to significant energy savings, aligning with sustainable and environmentally friendly AI practices. The enhanced performance and reduced training costs accelerate the development of AI applications, enhancing productivity in fields such as natural language processing, healthcare, and finance.

#### a.2.2 Limitation

There are two limitations to our current approach. Firstly, we have only utilized linear methods for parameter fusion in our model. Future research should explore nonlinear methods, as they may offer a better way to link different parameters and achieve optimal results. Secondly, due to computational resource constraints, we have only experimented with models of 7 billion, 3 billion, and 1.3 billion parameters. However, our method is scalable and can be extended to models of any size in future work.

#### a.2.3 Ethical Consideration

In our study, we utilize publicly available data and techniques to address privacy concerns. Our approach focuses on improving model parameter efficiency and reducing model size to develop robust, compact, and accessible models, thus promoting the open dissemination and democratization of NLP technologies. By implementing pre-training strategies, we aim to mitigate biases through comprehensive training on large datasets, contributing to ethical AI development that prioritizes transparency, efficiency, and bias reduction. Our work is dedicated to advancing accessible and efficient NLP technologies, fostering a more inclusive and automated future for AI.

## Appendix B More Implementation Details

### Head Similarity and MHA Redundancy

Centered Kernel Alignment (CKA) is a statistical measure used to quantify the similarity between two sets of data representations. Unlike traditional correlation measures, CKA is designed to be invariant to orthogonal transformations and scaling of the data.

To calculate the similarity between two sets of representations using CKA, we employ a kernel function to map the original data into a higher-dimensional space, where the alignment of their central tendencies can be more easily measured. The CKA value ranges from 0 to 1, where 0 indicates no similarity and 1 indicates identical representations.

The mathematical formulation of CKA, when using a linear kernel, is given by the following equation:

\[\text{CKA}(X,Y)=\frac{\|X^{T}Y\|_{F}^{2}}{\sqrt{\|X^{T}X\|_{F}^{2}\cdot\|Y^{T }Y\|_{F}^{2}}}\]

Here, \(X\) and \(Y\) are matrices whose columns are the vectors of the representations to be compared, \(\|\cdot\|_{F}\) denotes the Frobenius norm, and \(X^{T}\) and \(Y^{T}\) are the transposes of \(X\) and \(Y\), respectively. To mathematically define the redundancy of each layer based on the average similarity between heads, we follow these steps:

1. Compute the similarity between heads: For each pair of heads within a given layer, calculate the similarity using the CKA formula.
2. Compute the average similarity: Average the similarity scores of all pairs of heads to define the redundancy of the layer.

#### b.1.1 Compute Similarity Between Heads

Consider a layer with \(H\) heads, where the parameters of each head are represented by the matrices \(\mathbf{W}_{i}\) (e.g., \(\mathbf{W}_{q1},\mathbf{W}_{q2},\ldots,\mathbf{W}_{qH}\) for query weights). For each pair of heads \(i\) and \(j\), compute the CKA similarity using the following formula:

\[\text{CKA}(\mathbf{W}_{i},\mathbf{W}_{j})=\frac{\|\mathbf{W}_{i}^{T}\mathbf{W} _{j}\|_{F}^{2}}{\sqrt{\|\mathbf{W}_{i}^{T}\mathbf{W}_{i}\|_{F}^{2}\cdot\| \mathbf{W}_{j}^{T}\mathbf{W}_{j}\|_{F}^{2}}}\]

#### b.1.2 Compute Redundancy

Calculate the similarity for all pairs of heads and then compute the average similarity:

\[\text{Redundancy}=\frac{2}{H(H-1)}\sum_{i=1}^{H-1}\sum_{j=i+1}^{H}\text{CKA}( \mathbf{W}_{i},\mathbf{W}_{j})\]

The coefficient \(\frac{2}{H(H-1)}\) ensures that the average similarity is computed over all pairs of heads. This redundancy measure reflects the degree of similarity between the parameters of different heads within each layer. A higher redundancy indicates that the parameters of different heads are more similar, implying a higher level of redundancy.

### Implementation in LLaMA2 Model

Our method can theoretically transform MHA architecture in any transformer model to efficient DHA architecture. Using LLaMA models as case studies, we implemented DHA transformation with various compression rates on all MHA layers. Only a very small number of additional parameters need to be introduced, DHA significantly accelerates training and improves performance.

DHA adaptively gives search heads and heads connectivity relationship with redundancy in each MHA layer. Thus DHA assigns different group sizes at different layers and aggregates similar heads into one group to speed up fusion and reduce knowledge loss due to noise in fusion. As Shown in Fig 4, the transformation process of MHA to DHA can be divided into three stages.

In order to keep the performance of the DHA model at the fusion start consistent with the MHA model, we initialize the operators of the DHA model to the MHA model with the corresponding scaling factors of query-key, query-value set to 1, and the corresponding scaling factors within the rest of the groups set to 0. At the beginning of every fusion process (e.g. \(2\times,4\times,8\times\)), the algorithm first performs multiple STEPs constrained only by the \(\mathcal{L}_{\text{lm}}\) constraints to propagation, computing the \(\mathcal{L}_{\text{fusion}}\) but not optimizing the linear fusion operator based on it. Based on the \(\mathcal{L}_{\text{fusion}}\) between head and head as a measure of the distance between head and head we perform head clustering with the goal of minimizing the average loss of heads within each group and maximizing the average loss of heads between groups and groups. Afterwards, we select multiple groups with the smallest loss based on the compression rate as the fusion target, and optimize their \(\mathcal{L}_{\text{fusion}}\) for back propagation. This algorithm ensures that the most redundant components of the model are fused and compressed during each transformation, while components requiring more parameters retain their original properties.

Our approach is theoretically applicable to transforming parameters across various transformer model designs, focusing on preserving the knowledge within MHA parameters.

Using LLaMA models as a case study, we implement our DHA transformation on all MHA layer. The whole transformation process can be divided into two phases: the Fusion phase with a small training budget and the recovery phase with continuous pre-training. Before Fusion phase, we define the total number of compressed headers budget \(C\) then \(C\) is split into compression rates at different compression levels. During Fusion phase, we modified the forward propagation path of MHA in the form of DHA refer to Eq. 5 and optimize \(\mathcal{L}_{\text{fusion}}\) refer to Eq. 7. At the beginning, the fusion operators of each layer will be initialized making the DHA and the original MHA functionally equivalent. As we antagonistically optimize the fusion operator and upadte Lagrangian multipliers \(\lambda\), the \(\mathcal{L}_{\text{fusion}}\) that marks this DHA fusion process decreases.

When \(\mathcal{L}_{\text{fusion}}\) is less than 1e-3 we terminate the fusion algorithm and enter the post-processing phase. The fusion weights within each group are computed by averaging the weights corresponding to each query-key and query-value within the group.We construct new DHA heads' parameters from the original MHA heads based on the fusion operator. After that, the fused model parameters can recover the performance and complete the transformation with a small amount of restorative pre-training.

We implemented the DHA algorithm with different compression ratios on models of different sizes. Experiments show that the DHA algorithm is adapted to models of various sizes. Only a very small number of additional parameters need to be introduced, and DHA preserves parameter knowledge in the model and improves performance.

#### b.2.1 Attention Module Initialization

In the module initialization process, the input key and value tensors are first reshaped and grouped according to the number of key and value heads, respectively. Given the batch size (bsz), number of heads (num_heads), key length (k_len), and head dimension (head_dim), the key tensor is reshaped into keys_grouped of shape [bsz, num_key_heads, num_heads // num_key_heads, k_len, head_dim]. Similarly, the value tensor is reshaped into values_grouped of shape [bsz, num_value_heads, num_heads // num_value_heads, k_len, head_dim]. These grouped tensors are then expanded by repeating them along the group size dimension, resulting in keys_expanded and values_expanded. Correspondingly, the weight tensors weights_k and weights_v are reshaped to match the expanded dimensions and are then multiplied element-wise with the expanded key and value tensors.

```
1:\(K\)\(\triangleright\) key tensor
2:\(V\)\(\triangleright\) value tensor
3:\(K^{\prime}\)\(\triangleright\) weighted key tensor
4:\(V^{\prime}\)\(\triangleright\) weighted value tensor
5:\(b\leftarrow\) batch size
6:\(H\leftarrow\) number of heads
7:\(L_{k}\leftarrow\) key length
8:\(D\leftarrow\) head dimension
9:\(K_{s}\leftarrow\) self.num_key_heads
10:\(V_{s}\leftarrow\) self.num_value_heads
11:\(K_{g}\leftarrow\) self.key_group_size
12:\(V_{g}\leftarrow\) self.value_group_size
13:\(K_{g}\leftarrow\) self.weights_k
14:\(V^{\prime}\leftarrow\) self.weights_v
15:\(K_{g}\gets K\).view\((b,K_{s},H/K_{s},L_{k},D)\)
16:\(V_{g}\gets V\).view\((b,V_{s},H/V_{s},L_{k},D)\)
17:\(K_{e}\gets K_{g}\).repeat_interleave\((K_{g},\text{dim}=1)\)
18:\(V_{e}\gets V_{g}\).repeat_interleave\((V_{g},\text{dim}=1)\)
19:\(K_{w}\gets K^{\prime}_{g}\).view\((1,H,K_{g},1,D)\)
20:\(V_{w}\gets V^{\prime}_{g}\).view\((1,H,V_{g},1,D)\)
21:\(W_{K}\gets K_{e}\times K_{w}\)
22:\(W_{V}\gets V_{e}\times V_{w}\)
23:\(K^{\prime}\gets W_{K}\).sum\((\text{dim}=2)\)
24:\(V^{\prime}\gets W_{V}\).sum\((\text{dim}=2)\)
25:return\(K^{\prime}\),\(V^{\prime}\) ```

**Algorithm 1** Attention Module Initialization

#### b.2.2 Attention Forward Pass

During the forward pass, the reshaping and expansion of the key and value tensors are performed in a similar manner as in the initialization process but with parameters specific to the DHA fusion phase. The key tensor is reshaped into keys_grouped of shape [bsz, dha_warmup_group_num, num_heads // dha_warmup_group_num, k_len, head_dim] and the value tensor into values_grouped of shape [bsz, dha_warmup_group_num, num_heads // dha_warmup_group_num, k_len, head_dim]. Thesegrouped tensors are then expanded by repeating them according to the dha_warmup_group_size. The weights weights_k and weights_v are reshaped and expanded to align with the dimensions of the expanded key and value tensors. Element-wise multiplication is performed between the expanded tensors and their corresponding weights, and the resulting weighted tensors are summed along the appropriate dimension.

```
0:\(K\)\(\triangleright\) key tensor
0:\(V\)\(\triangleright\) value tensor
0:\(K^{\prime}\)\(\triangleright\) weighted key tensor
0:\(V^{\prime}\)\(\triangleright\) weighted value tensor
1:\(b\leftarrow\) batch size
2:\(H\leftarrow\) number of heads
3:\(L_{k}\leftarrow\) key length
4:\(D\leftarrow\) head dimension
5:\(G_{q}\leftarrow\) self.dha_warmup_group_num
6:\(G_{s}\leftarrow\) self.dha_warmup_group_size
7:\(K^{\prime}_{g}\leftarrow\) self.weights_k
8:\(V^{\prime}_{g}\leftarrow\) self.weights_v
9:\(K_{g}\gets K.\text{view}(b,G_{q},H/G_{q},L_{k},D)\)
10:\(V_{g}\gets V.\text{view}(b,G_{q},H/G_{q},L_{k},D)\)
11:\(K_{e}\gets K_{g}.\text{repeat\_interleave}(G_{s},\text{dim}=1)\)
12:\(V_{e}\gets V_{g}.\text{repeat\_interleave}(G_{s},\text{dim}=1)\)
13:\(K_{w}\gets K^{\prime}_{g}.\text{view}(1,H,G_{s},1,D)\)
14:\(V_{w}\gets V^{\prime}_{g}.\text{view}(1,H,G_{s},1,D)\)
15:\(W_{K}\gets K_{e}\times K_{w}\)
16:\(W_{V}\gets V_{e}\times V_{w}\)
17:\(K^{\prime}\gets W_{K}.\text{sum}(\text{dim}=2)\)
18:\(V^{\prime}\gets W_{V}.\text{sum}(\text{dim}=2)\)
19:return\(K^{\prime}\),\(V^{\prime}\) ```

**Algorithm 2** Attention Forward Pass

#### b.2.3 DHA Loss Calculation

The calculation of the loss function in this model involves the adaptive DHA loss. This loss is computed based on the global step, warmup steps, and a base value. The DHA margin is calculated as the product of an exponential decay term and a linear decay term, ensuring it is non-negative. The adaptive DHA loss is derived by comparing the mean squared error (MSE) with the DHA margin and summing the positive differences.

Formally, the DHA margin \(M_{\text{dha}}\) is calculated as:

\[M_{\text{dha}}=\max\left(0,\left(\text{base}^{\text{global\_step}}\right) \times\left(1.0-\frac{\text{global\_step}}{\text{dha\_warmup\_step}}\right)\right)\]

MSE Loss are defined in Eq. 7. The adaptive DHA loss \(L_{\text{dha}}\) is then:

\[L_{\text{dha}}=\sum\max(\text{mse}-M_{\text{dha}},0.0)\]

The overall loss \(L\) is the adaptive DHA loss:

\[L=L_{\text{dha}}+L_{\text{tm}}\]

This combined loss function effectively utilizes the adaptive component to optimize the attention mechanism in the model. The calculation process ensures that the model adapts dynamically during training, reducing the loss progressively as the training steps increase.

```
0:\(B\)\(\triangleright\) blocks
0:\(mse\)\(\triangleright\) Mean Squared Error tensor
0:\(L\)\(\triangleright\) loss_dha_diversity
1:\(\lambda\leftarrow\) self.lambda_mse
2:\(s\leftarrow\) self.mse_scale
3:\(L\gets 0.0\)
4:\(global\_step\gets 1000\)
5:\(dha\_warmup\_step\gets 200\)
6:\(base\gets 0.999\)
7:for each\(b\in B\)do
8:\(L_{l}\gets 0.0\)\(\triangleright\) loss_dha_diversity_layer
9:\(A\gets b.attn\)\(\triangleright\) attn_layer
10:\(W_{k}\gets A.weights\_k\)
11:\(W_{v}\gets A.weights\_v\)
12:\(G_{s}\gets A.dha\_warmup\_group\_size\)
13:\(G_{n}\gets A.dha\_warmup\_group\_num\)
14:\(H_{kv}\gets A.num\_key\_value\_heads\)
15:\(H\gets A.num\_heads\)
16:\(D\gets A.head\_dim\)
17:\(W_{k}\leftarrow\) reshape(\(W_{k},[H_{kv},-1,G_{s},D]\))
18:\(W_{v}\leftarrow\) reshape(\(W_{v},[H_{kv},-1,G_{s},D]\))
19:for each\(r\in W_{k}\)do
20:for each\(o\in W_{k}\) after \(r\)do
21:\(L_{l}\gets L_{l}+\text{MSE\_Loss}(r,o)\)
22:endfor
23:endfor
24:for each\(r\in W_{v}\)do
25:for each\(o\in W_{v}\) after \(r\)do
26:\(L_{l}\gets L_{l}+\text{MSE\_Loss}(r,o)\)
27:endfor
28:endfor
29:\(N\gets H\times G_{s}\times D\times 2\times\frac{(G_{s}-1)}{2}\)
30:\(L_{l}\leftarrow\frac{s\times L_{l}}{N}\)
31:\(L\gets L+L_{l}\)
32:endfor
33:\(L\leftarrow\frac{L}{\text{len}(B)}\)
34:\(L\gets L\times\lambda\)
35:\(exponent\gets base^{global\_step}\)
36:\(linear\_decay\gets 1.0-\frac{global\_step}{dha\_warmup\_step}\)
37:\(margin\leftarrow\max(0,exponent\times linear\_decay)\)
38:\(adaptive\_loss\leftarrow\sum\max(mse-margin,0.0)\)
39:\(L\gets L+adaptive\_loss\)
40:return\(L\) ```

**Algorithm 3** Adaptive DHA Loss Calculation

### Head Grouping Based on Fusion Loss

This algorithm uses simulated annealing to optimize group scores based on a given score matrix. It begins by defining the number of groups and distributing the points among them randomly. The initial score for these groups is calculated using the 'calculate_score' function, which sums the scores from the matrix for each group, considering each connection twice and dividing by two.

The algorithm starts with a high temperature (T=100) and gradually cools down (T_min=0.001) using a cooling rate (alpha=0.9). During each iteration, two random points from different groups are swapped, creating a new grouping. The score for this new grouping is calculated, and the difference in score (delta) is evaluated.

If the new score is higher, or if a randomly generated number is less than the exponential of delta divided by the temperature, the new grouping is accepted. This allows the algorithm to escape local optima. The temperature is then reduced according to the cooling rate. This process continues until the temperature reaches the minimum threshold. The algorithm returns the final group configuration and its corresponding score, which represents an optimized grouping based on the initial score matrix.

In practice, we use the MSE computed by the head and the head as scores, and compute the matrix of scores between the head and the head for head clustering after forward.

```
0:\(M\)\(\triangleright\) score matrix
0:\(G\gets 8\)\(\triangleright\) number of groups
0:\(best\_groups,best\_score\)\(\triangleright\) final groups and their score
1:functioncalculate_score(\(M,groups\))
2:\(score\gets 0\)
3:for each \(group\in groups\)do
4:for each \(i\in group\)do
5:for each \(j\in group\)do
6:\(score\gets score+M[i][j]\)
7:endfor
8:endfor
9:endfor
10:return\(score/2\)\(\triangleright\) each connection counted twice
11:endfunction
12:functionsimulated_annealing(\(M,G\))
13:\(P\leftarrow\text{length}(M)\)\(\triangleright\) number of points
14:\(N\gets P/G\)\(\triangleright\) number of points per group
15:\(points\leftarrow\text{array}(range(P))\)
16:shuffle(\(points\))
17:\(groups\gets points.reshape(G,N)\)
18:\(current\_score\leftarrow\text{calculate\_score}(M,groups)\)
19:\(T\gets 100.0\)\(\triangleright\) initial temperature
20:\(T_{min}\gets 0.001\)\(\triangleright\) minimum temperature
21:\(\alpha\gets 0.9\)\(\triangleright\) cooling rate
22:while\(T>T_{min}\)do
23:\(i,j\leftarrow\) random integers in \([0,G)\)
24:if\(i\neq j\)then
25:\(a,b\leftarrow\) random integers in \([0,N)\)
26:\(new\_groups\gets groups.copy()\)
27:\(temp\gets new\_groups[i][a]\)
28:\(new\_groups[i][a]\gets new\_groups[j][b]\)
29:\(new\_groups[j][b]\gets temp\)
30:\(new\_score\leftarrow\text{calculate\_score}(M,new\_groups)\)
31:\(\Delta\gets new\_score-current\_score\)
32:if\(\Delta>0\)or\(\exp(\Delta/T)>\text{random}()\)then
33:\(groups\gets new\_groups\)
34:\(current\_score\gets new\_score\)
35:endif
36:endif
37:\(T\gets T\times\alpha\)\(\triangleright\) cooling down
38:endwhile
39:return\(groups,current\_score\)
40:endfunction
41:\(best\_groups,best\_score\leftarrow\text{simulated\_annealing}(M,G)\) ```

**Algorithm 4** Head Grouping Optimization on Fusion Loss

### Layer Allocation Based on Fusion Loss

This algorithm efficiently allocates resources to different layers based on their respective losses to optimize system performance. Initially, it assigns a minimum allocation to each layer. Then, it calculates weights for each layer based on their losses, prioritizing layers with higher losses. The algorithm determines the number of times 16 can be allocated based on the remaining allocation. It allocates 16s to layers with the highest weights until reaching a predetermined limit. Next, it redistributes the remaining allocation to layers with the highest loss-to-allocation ratios, assigning resources in multiples of 8 or 4. This process ensures that layers with higher losses receive more resources, optimizing the overall system performance. Finally, the algorithm returns the final allocation for each layer, resulting in an efficient distribution of resources across the system. The total search process for the LLaMA2 model requires 42 minutes.

```
0:\(L\)\(\triangleright\) losses for each layer
0:\(A\leftarrow[4,8,16]\)\(\triangleright\) possible allocations
0:\(T\gets 256\)\(\triangleright\) total allocation
0:\(alloc\leftarrow[a_{1},a_{2},\dots,a_{n}]\)\(\triangleright\) final allocations for each layer
1:\(n\leftarrow\) length\((L)\)
2:\(alloc\leftarrow[4]\times n\)\(\triangleright\) initial allocation
3:\(W\leftarrow\frac{L}{\sum L}\)\(\triangleright\) weights proportional to losses
4:\(R\gets T-\sum alloc\)\(\triangleright\) remaining allocation
5:\(k\gets 1\)\(\triangleright\) initial number of 16's to allocate
6:\(M_{16}\gets R/16\)\(\triangleright\) maximum number of 16's that can be allocated
7:\(k\leftarrow\min(k,M_{16})\)
8:for\(i\gets 1\) to \(k\)do
9:\(idx\leftarrow\) argmax\((W)\)
10:\(alloc[idx]\gets alloc[idx]+12\)
11:\(W[idx]\gets 0\)\(\triangleright\) prevent reallocation
12:endfor
13:\(R\gets T-\sum alloc\)
14:while\(R>0\)do
15:if\(R\geq 8\)then
16:\(idx\leftarrow\) argmax\((\frac{L}{alloc})\)
17:\(alloc[idx]\gets alloc[idx]+4\)
18:\(R\gets R-4\)
19:elseif\(R\geq 4\)then
20:\(idx\leftarrow\) argmax\((\frac{L}{alloc})\)
21:\(alloc[idx]\gets alloc[idx]+4\)
22:\(R\gets R-4\)
23:endif
24:endwhile
25:return\(alloc\) ```

**Algorithm 5** Layer Allocation Based on Losses

### Training Details

The hyperparameters used in our experiments are presented in Tab. 4. We employ fully sharded data parallel to efficiently train our models in parallel, and we utilize FlashAttention V1 [60] to accelerate the training process. A cosine learning rate scheduler is used, with the learning rate decaying to a minimum of 10% of the peak value. Preliminary experiments were conducted to determine the optimal peak learning rate for learning the fusion variables and Lagrange multipliers.

## Appendix C Extended Experiments

### Instruction Tuning Evaluation.

Instruction Tuning Evaluation.To assess our models' capabilities in downstream application after instruct tuning [31, 32], we fine-tune both DHA and the baseline models on 10,000 instruction-response pairs drawn from the initial round of multi-turn chat histories in the ShareGPT dataset7. For evaluation, we select another 1,000 instructions from ShareGPT, generate responses using our fine-tuned models and other baseline models and employ GPT-4 as an evaluator to compare these responses [33]. We report the win rate of our model relative to the baseline model.

Footnote 7: https://sharegpt.com

Instruction Tuning.As shown in Fig. 10, the tuned DHA model outperforms all GQA baselines of comparable scale. This demonstrates that the DHA model effectively retains the foundational capabilities of the MHA model and can be activated through instruction tuning to produce long, coherent, and informative responses.

Combination with KV Cache Compression Techniques.In Sections 2 and 4, we demonstrated that DHA is a more efficient GQA architecture, so it has similarly good compatibility. We tested the compatibility of the DHA model with the KVCache eviction method NACL [49]. NACL 25% indicates retaining only 25% of the KVCache. The experiment results are shown in the Tab. 5. DHA and GQA exhibit equally good compatibility with KV cache compression techniques.

Compare with Advance GQA Initialization.It's a common and effective approach to convert MHA to GQA using mean pooling instead of training from scratch. The author of GQA tested several methods for the initialization of GQA and found it works best using simple mean pooling from MHA. Indeed, training GQA from scratch will cost trillions tokens budget to match the performance of MHA which is inefficient and costly.Inspired by the similarity of head parameters, we improved the initialization method of GQA: instead of direct grouping, we first cluster similar heads using CKA

\begin{table}
\begin{tabular}{l c c} \hline \hline  & Fusion & Contined Pre-training \\ \hline Training budget & \(0.2\)B & \(5\)B \\ Learning rate of \(\omega,\lambda\) & \(0.05\) & - \\ Learning Rate of \(\theta\) & \(0.0001\) & \(0.0001\) \\ LR warmup ratio & \(10\%\) & \(3\%\) \\ Batch size (tokens) & \(262\)K & \(1\)M \\ Evaluation interval \(m\) (steps) & \(40\) & \(40\) \\ Steps & \(800\) & \(5,000\) \\ \# GPUs & \(8\) & \(8\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Training hyper-parameters

Figure 10: In model scale of 7B, 3B, and 1.3B, DHA significantly outperforms GQA and achieves comparable performance with MHA after instruction tuning.

and then perform mean-pooling initialization within each cluster. We compare this approach with the Vanilla GQA and DHA.

Tab. 6 shows that GQA(CKA-Grouping)-7B-25% (5B) achieved comparable performance to the original implementation in Vanilla GQA. We believe the reason for this is that the head grouping learned by DHA is based on the fusible nature between heads, which cannot be completely equated with CKA similarity. More importantly, DHA not only groups heads based on similarity but also learns the fusible parameters. This allows it to eliminate the influence of redundant parameters and retain more important information during the initialization process, which is not possible with mean initialization.

## Appendix D Extend Analysis

How Merging Weights Change.Refer to Fig. 2(a), where we show the weight variation diagram. In the fusion process of heads 0-3, head 0 initially constitutes 100% as the starting head of the MHA. As the fusion process progresses, the parameters of the important heads increase, and the proportions of all heads become more balanced. This indicates that the algorithm attempts to retain information from different heads by balancing the parameter proportions of each head. This process results in a slight increase in loss, but not significantly.

DHA's Compatibility on GQA Model.DHA is primarily designed for models based on the Transformer Decoder architecture and can be adapted to all models with this architecture. We chose LLaMA [61] as the experimental baseline because it is a classic model using the decoder architecture in LLMs. Other open-source LLM models differ from LLaMA only in certain details (such as activation functions and training methods), which do not affect DHA's training. Successfully applying DHA to LLaMA indicates that it can be used in most decoder-only models. GQA [5] is an efficient variant of MHA, which optimizes the inference process through head grouping and sharing. Due to its simplicity and efficiency, GQA is widely used. DHA can be similarly constructed based on GQA, requiring only minor adjustments to the construction process. Here, we provide two feasible methods to convert GQA to DHA.

* Easiest method in less than 1 minute. GQA can be losslessly converted into MHA by simply replicating the GQA' KV heads. Then, we can perform the DHA transformation on the MHA architecture.
* Minor modification by grouping KV. DHA only needs to group and fuse the Key and Value heads. When constructing DHA on GQA, we initially group the Key and Value, maintaining alignment with GQA functionality. During the training phase, the fused head parameters can replace the original GQA heads for sharing.

Inter-layer Grouping of Heads or Only Intra-layer Grouping?Only intra-layer grouping and fusion is conducted in DHA. Fig. 1 meant to illustrate the decoupled-heads where the number of key

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Method** & **Avg ACC** & **PPL** \\ \hline DHA-7B-25\% (5B) & 62.4 & 7.29 \\ GQA-7B-25\% (5B) & 60.3 & 7.54 \\ GQA (CKA-Grouping)-7B-25\% (5B) & 60.4 & 7.51 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison of Avg ACC and PPL between different methods at 7B-25% (5B).

\begin{table}
\begin{tabular}{l c} \hline \hline
**Method** & **log(PPL)** \\ \hline GQA-7b-25\% & 2.89 \\ DHA-7b-25\% & 2.84 \\ \hline GQA-7b-25\% (NACL 25\%) & 3.01 \\ DHA-7b-25\% (NACL 25\%) & 2.93 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison of log(PPL) between DHA and GQA with NACL.

and value heads can be different among layers. The DHA method employs parameter fusion within each layer for three reasons:

* Higher redundancy of heads within layer for fusion. The heads within a layer exhibit high similarity and redundancy, which provides a good starting point for parameter fusion.
* More complex optimization for inter-layer fusion. The optimization process between layers is very complex and requires memory operations for cross-layer calls, which inherently increases the inference cost.
* Promising future work by introducing inter-layer fusion [62]. This paper represents an early exploration of applying parameter fusion methods within model parameters. The inter-layer fusion approach is indeed a valuable direction for future exploration.

Accuracy Loss after Transformation.The performance gap between the results shown in the paper and MHA is primarily due to the following two reasons:

* The gap of pre-training data. The MHA model was not trained on the same data used for DHA. Since LLaMA's training data is not directly open-sourced, we used an experimental open-sourced pre-training data following Sheared-LLaMA (Xia et al., 2024). The improved pre-training data will close the gap between DHA and MHA.
* Parameter size difference. Compared to MHA, DHA compresses 50% or 25% of attention heads, requires only 0.05% of pre-training data and achieves approximately 5% loss. The number of parameters of MHA is much larger than that of DHA, so performance loss is inevitable during conversion. Compared with GQA, a strong baseline with the same number of parameters, DHA has shown higher training efficiency and performance advantages. Due to the high efficiency of DHA, DHA can use more heads than MHA with the same number of parameters, and has the opportunity to achieve better performance.

Extend Observation

### Header parameter characteristics in MHA

We show more of our head similarity observations in the LLaMA2-7b model MHA. Each subfigure represents the similarity between heads within the same layer for three different types of attention mechanisms: WQ (query), WK (key), and WV (value). The matrices are arranged in a 3x4 grid layout, with each row corresponding to a specific layer and each column corresponding to a type of attention mechanism. Note: Layer numbers start from 1.

Figure 11: Visualization of query, key, value head parameters similarity from layer 1 to layer 1b in LLaMA2-7B.

Figure 12: Visualization of query, key, value head parameters similarity from layer 17 to layer 32 in LLaMA2-7B.

### Header parameter characteristics in DHA

The DHA parameter distribution of shows consistency with MHA's. It indicates that DHA effectively aggregates multiple similar functional heads within clusters and new fused heads successfully reconstruct the functionalities of multiple origin heads in MHA. It is noteworthy that the significant reduction in the number of similar heads within the DHA architecture indicates that our method effectively reduces redundancy among the heads.

Figure 13: Visualization of query, key, value head parameters similarity from layer 1 to layer 8 in DHA-7B-25%.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract provides a concise summary of the key findings and experiment results. The introduction in Sec. 1 outlines the research questions and objectives in paragraph 3,4 and contribution in paragraph 5. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper discusses the limitations of the work performed by the authors in detail in Appendix Appendix. A.2, highlighting two specific limitations and the broader impact. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: This paper is mainly based on observation, making conjectures and methods and proving the effects through experiments. The paper defines the background in Sec. 2, presents the conjecture in Sec. 3, and provides a detailed derivation of the form and optimization process in Sec. 4. All assumptions made in the paper are thoroughly validated through experiments in Sec. 5. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper provides a detailed description of the experimental data setup and hyperparameter settings in Sec. 5. Additionally, in Sec. 4 and in Appendix. B.2 sections we thoroughly explain the derivation and implementation process, ensuring all necessary information for reproducing the main experimental results is disclosed. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: The datasets, baseline methods, and models used in the paper are fully open-source and available on Hugging Face. The paper includes the key implementation steps and code in Sec. 4 and the Appendix. B.2. However, the complete code is still being organized and is under consideration for open sourcing. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes], Justification: The paper provides a detailed description of the experimental data setup and hyperparameter settings in Sec. 5. Additionally, in Sec. 4 and in Appendix. B.2 sections we thoroughly explain the derivation and implementation process. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes]Justification: All results are averaged over multiple tests, and we report the mean accuracy along with the standard deviation (acc_norm) as a measure of error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In Sec. 5.1, we report the GPUs we used, the memory, and detailed training information. For more information you can refer to the Appendix B.2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer:[Yes] Justification: The discussion of the ethics and impact can be consulted in Appendix. A.2. We are open and transparent throughout the study and do not design for human subjects, privacy data bias, or other issues. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The discussion of the broader impacts can be consulted in Appendix. A.2. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper presents an improved approach based on the existing model architecture, but does not release any new models. The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: This article uses assets reasonably in compliance with the license, and the assets used are cited in the article.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.