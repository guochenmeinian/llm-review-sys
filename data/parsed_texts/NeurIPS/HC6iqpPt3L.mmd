[MISSING_PAGE_FAIL:1]

data inefficient as actions are sampled according to given policies, potentially overlooking actions from states where the expected return is uncertain. To achieve better value estimation with fewer samples, it is essential to focus on state-action pairs with high variance in return, as these pairs would exhibit greater uncertainty in their mean return. Therefore, a behavior policy should visit such pairs more frequently to offset higher variance in return. Consider an analogy of a two-arm bandit problem: fewer samples are needed to accurately evaluate a constant reward arm, whereas an arm with a variable reward demands a greater number of samples to achieve the same level of certainty. We empirically support this claim by comparing round-robin and our approach later in this paper.

With this motivation, we introduce GVFExplorer, that adaptively learns a behavior policy which minimize the total MSE across all GVF predictions. GVFExplorer leverages the existing off-policy temporal difference (TD) based estimator of variance in return distribution (Sherstan et al., 2018; Jain et al., 2021) to guide the behavior policy. The strategy is to frequently take actions that might have more unpredictable outcomes (high variance in return). By sampling them more, agent can estimate the mean return better with fewer interactions, thus effectively lowering the overall MSE in the GVF predictions.

GVFExplorer optimizes the data usage and reduces the prediction error, offering a scalable solution for complex environments. This is particularly valuable for real-world applications like personalized recommender systems (Parapar and Radlinski, 2021; Tang et al., 2015), where it can enable efficient evaluation of personalized policies based on diverse user preferences (reward functions) (Li et al., 2024), leveraging shared knowledge for improved accuracy.

Contributions:(1) We design an adaptive behavior policy that enables accurate and efficient learning of multiple GVF predictions in parallel [Algorithm 1]. (2) We derive an iterative behavior update rule that directly minimizes the overall prediction error [Theorem 4.1]. (3) We prove in the tabular setting that each iterative update to the behavior policy causes the total MSE across GVFs to be less than equal to one from the old policy [Theorem 4.2]. (4) We establish the existence of a variance operator that enables us to use TD-based variance estimation [Lemma 5.1]. (5) We empirically demonstrate in both tabular and Mujoco environments that GVFExplorer lowers the total MSE when estimating multiple GVFs compared to baseline approaches and enables evaluating a larger number of GVFs in parallel.

## 2 Related Work

Exploration in reinforcement learning (RL) has predominantly focused on improving policy performance for a single objective (Oudeyer et al., 2007; Schmidhuber, 2010; Jaderberg et al., 2016; Machado et al., 2017; Eysenbach et al., 2018; Burda et al., 2018; Guo et al., 2022). Refer to Ladosz et al. (2022) for a detailed survey on exploration techniques in RL. While related to exploration, these works differ from ours, as they concentrate on optimizing policies for single objective rather than evaluating multiple GVFs (policy-cumulant pair) simultaneously.

Our work is most closely related to other works on learning multiple GVFs. Xu et al. (2022) address a similar problem by evaluating multiple GVFs using an offline dataset, but our method operates online, avoiding the data coverage limitations of offline approaches. Linke et al. (2020) develops exploration strategies for GVFs in a stateless bandit context, which does not deal with the off-policy learning or function approximation challenges present in the full Markov Decision Process (MDP) context. In a single bandit problem, Antos et al. (2008); Carpentier et al. (2015), show that the optimal data collection strategy to estimate mean rewards of arms is to sample proportional to each arm's variance in reward. Prior works like Hanna et al. (2017) learned a behavior policy for a _single_ policy evaluation problem using a REINFORCE-style (Williams, 1992) variance-based method called BPS. This idea extends on the similar principles of using Importance Sampling in Monte Carlo simulations for finding optimal sampling policy based on variance minimization (Owen, 2013; Frank et al., 2008). Metelli et al. (2023) extends this idea to the control setting. However, these methods are limited to single-task evaluation or control. Evaluating multiple policies simultaneously is more complex, requiring careful balance in action selection among interrelated learning problems. Perhaps the closest work to ours is by McLeod et al. (2021), which uses the changes in the weights of Successor Representation (SR) (Dayan, 1993) as an intrinsic reward to learn a behavior policy that supports multiple predictive tasks. GVFExplorer approach is simpler, as it directly optimizes the behavior policy to minimize the total prediction error over GVFs, resulting in an intuitive variance-proportional sampling algorithm. We will compare the two approaches empirically as well.

## 3 Preliminaries

Consider an agent interacting with the environment to obtain estimates of \(N\) different _General Value Function_ (GVF) (Sutton et al., 2011). We assume an episodic, discounted Markov decision process (MDP) where \(\mathcal{S}\) is the set of states, \(\mathcal{A}\) is the action set, \(\mathcal{P}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta_{\mathcal{S}}\) is the transition probability function, \(\Delta_{\mathcal{S}}\) is the \(|\mathcal{S}|\)-dimensional probability simplex, and \(\gamma\in[0,1)\) is the discount factor.

Each GVF is conditioned on a fixed policy \(\pi_{i}:\mathcal{S}\rightarrow\Delta_{\mathcal{A}}\), \(i=\{1,\ldots,N\}\) and has a cumulant \(c_{i}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\). For simplicity, we assume that all cumulants are scalar, and that the GVFs share the environment discount factor \(\gamma\). This eases the exposition, but our results can be extended to general multidimensional cumulants and state dependent discount factor. Each GVF is a value function \(V_{\pi_{i}}(s)=\mathbb{E}_{\pi_{i}:\mathcal{P}}[G_{i}^{i}|s_{t}=s]\) where \(G_{i}^{i}=c_{i,t}+\gamma G_{t+1}^{i}\). Each GVF can be viewed as answering the question, "what is the expected discounted sum of \(c_{i}\) received while following \(\pi_{i}?\)" We can also define action-value GVFs: \(Q_{\pi_{i}}(s,a)=c_{i}(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim\mathcal{P}(\cdot |s,a)}[V_{\pi_{i}}(s^{\prime})]\), with \(V_{\pi_{i}}(s)=\mathbb{E}_{a\sim\pi_{i}(\cdot|s)}[Q_{\pi_{i}}(s,a)]\).

At each time step \(t\), the agent in state \(s_{t}\), takes an action \(a_{t}\) and receives cumulant values \(c_{i,t}\) for all \(i\in\{1,\ldots,N\}\), transitioning to a new state \(s_{t+1}\). This repeats until reaching a terminal state or a maximum step count. Then the agent resets to a new initial state and starts again. The agent interacts with environment using a behavior policy, \(\mu:\mathcal{S}\rightarrow\Delta_{\mathcal{A}}\). The goal is to approximate values \(\hat{V}_{i}\) corresponding to the true GVFs value \(V_{\pi_{i}}\). We formalize the objective as **minimizing the Mean Squared Error (MSE)** under some state weighting \(d(s)\) for all GVFs:

\[\textit{MSE}(V,\hat{V})=\sum_{i=1}^{N}\sum_{s\in\mathcal{S}}d(s)\Big{(}V_{\pi _{i}}(s)-\hat{V}_{i}(s)\Big{)}^{2}.\] (1)

In our experiments, we use the uniform distribution for \(d(s)\). This objective can be generalized to prioritize certain GVFs using a weighted MSE.

Importance Sampling (IS).To estimate multiple GVFs with distinct target policies \(\pi_{i}\) in parallel, off-policy learning is essential. Importance sampling (IS) is one of the primary tools for off-policy value learning (Hesterberg, 1988; Precup, 2000; Rubinstein and Kroese, 2016), allowing estimation of value function under target policy \(\pi\) using samples from different behavior policy \(\mu\). In the context of off-policy Temporal Difference (TD) learning (Sutton and Barto, 2018), the IS ratio, \(\rho_{t}=\frac{\pi(a_{i}|s_{t})}{\mu(a_{i}|s_{t})}\), is used to adjust the updates to ensure _unbiased value estimates_. The update rule is given as \(\hat{Q}(s_{t},a_{t})=\hat{Q}(s_{t},a_{t})+\alpha\left(c_{t}+\gamma\rho_{t+1} \hat{Q}(s_{t+1},a_{t+1})-\hat{Q}(s_{t},a_{t})\right),\) where \(\alpha\) is the learning rate. This update rule ensures that estimated value function \(\hat{Q}\) converges to correct value \(Q_{\pi}\) under policy \(\pi\), despite the samples being generated from a behavior policy \(\mu\).

## 4 Behavior Policy Optimization

As described in the previous section, the goal of the agent is to minimize the total mean squared error (MSE) across the given GVFs (Eq. (1)). Note that \(\text{MSE}=\text{Variance}+\text{Bias}^{2}\). For the algorithm's derivation, we will use **unbiased** IS estimation for off-policy correction, which shifts the task of minimizing MSE to reducing the total variance across GVFs. _Thus, the core problem is to design a behavior policy that collects data to **minimize the variance in return** across all GVFs in order to accurately estimate multiple GVF value functions._

The problem of estimating a _single target policy_'s value, \(V_{\pi}\), is well studied in the literature (see Sec. 2). In Monte Carlo sampling literature, it is well known that there exists an optimal sampling distribution (i.e., behavior policy) that provides optimal variance reduction compared to simply running the target policy Kahn and Marshall (1953); Owen (2013). Unfortunately, the analytical solution of obtaining this optimal behavior policy, \(\mu^{*}\), requires foreknowledge of \(V_{\pi}\), making it impractical when our overall purpose is to estimate \(V_{\pi}\). Nonetheless, in this work, we take inspiration from this earlier work and develop a practical method that iteratively solves for a single behavior policy that minimizes the total variance when estimating multiple general value functions in parallel.

### Objective Function

We propose GVFExplorer to address the above limitation and extend the problem to accurately estimate multiple GVF values. GVFExplorer takes as input the GVF target policies \(\pi_{i=\{1,\dots,N\}}\), collects data from a single (non-stationary) behavior policy \(\mu\), and outputs the GVF estimates \(\hat{V}_{i=\{1,\dots,N\}}\). Since our objective is to find a behavior policy that minimizes the variance in return across multiple GVFs, we use an existing off-policy TD-style variance estimator (Sherstan et al., 2018). This estimator allows us to bootstrap the target values and iteratively update the variance function, making the solution scalable to complex domains.

We define the variance function by \(\bm{M}_{\pi}^{\mu}(\bm{s})\), which measures the **variance in the return** of target policy \(\pi\) in a given state \(s\) when actions are sampled under a different behavior policy \(\mu\). We describe how to learn this function in Sec. 5. The variance function for a given state and a given state-action pair are defined respectively as:

\[M_{\pi}^{\mu}(s)=\text{Var}_{\pi}(G_{t}|s_{t}=s,a\sim\mu)\quad\text{and}\quad M _{\pi}^{\mu}(s,a)=\text{Var}_{\pi}(G_{t}|s_{t}=s,a_{t}=a,a^{\prime}\sim\mu).\]

**Our objective is to find an optimal behavior policy \(\mu^{*}\) that efficiently collects a single stream of experience to minimize the sum of variances \(M_{\pi_{\{1\dots N\}}}^{\mu}\)** under some state distribution \(d(s)\), as,

\[\mu^{*}=\arg\min_{\mu}\sum_{i=1}^{N}\sum_{s}d(s)M_{\pi_{i}}^{\mu}(s)\quad\text {s.t.}\quad\mu(a|s)\geq 0\,\&\,\sum_{a}\mu(a|s)=1.\] (2)

We solve the above objective function iteratively. At each iteration \(k\), GVFExplorer produces a behavior policy \(\mu_{k}\). The behavior policy interacts with the environment and gathers data. Using this data, any off-policy TD algorithm can be used to iteratively estimate the variance function \(M_{\pi}^{\mu_{k}}\). This variance function is plugged into the optimization problem given in Eq. (3) to update to a policy \(\mu_{k+1}\) that reduces variance. The iterative procedure is analogous to _policy iteration_, which alternates policy evaluation with policy improvement. Here, we alternate between the variance evaluation and the improvement of behavior policy to minimize the overall sum of variance across all given GVFs.

Here, our aim is to iteratively improve behavior policy and decrease variance functions to estimate the GVF values \(V_{\pi_{i=\{1,2,\dots,N\}}}\) with reducing MSE:

\[\mu_{0}\xrightarrow{E}M_{\pi_{i=1,2,\dots}}^{\mu_{0}}\xrightarrow{I}\mu_{1} \xrightarrow{E}M_{\pi_{i=1,2,\dots}}^{\mu_{1}}\dots\xrightarrow{E}\mu_{K},\]

where \(\xrightarrow{E}\) denotes _variance estimation_ and \(\xrightarrow{I}\) denotes _behavior policy improvement_. Next, we present Theorem 4.1 which principally derives the behavior policy update from \(\mu_{k}\) to \(\mu_{k+1}\) by solving the objective in Eq. (7). We demonstrate that the behavior policy update in Eq. (3) minimizes the objective by showing that \(\mu_{k+1}\) is a better policy than \(\mu_{k}\). The policy \(\mu_{k+1}\) is considered as good as, or better than \(\mu_{k}\), if it obtains lesser or equal total variance across all GVFs:\(\sum_{i}M_{\pi_{i}}^{\mu_{k+1}}(s)\leq\sum_{i}M_{\pi_{i}}^{\mu_{k}}(s)\). The proof of behavior policy improvement is detailed in Theorem 4.2.

### Theoretical Solution

**Theorem 4.1**.: _(**Behavior Policy Update:**) To find the behavior policy \(\mu\) that minimize the variance objective across \(N\) given target policies \(\{\pi_{i}\}_{i=1}^{N}\), we iteratively update \(\mu\) by solving the objective in Eq. (2). Given state-action variance function \(M_{\pi_{i}}^{\mu_{k}}(s,a)\), the solution to Eq. (2) at iteration \(k\) is:_

\[\mu_{k+1}(a|s)=\frac{\sqrt{\sum_{i}\pi_{i}(a|s)^{2}M_{\pi_{i}}^{\mu_{k}}(s,a) }}{\sum_{a^{\prime}}\sqrt{\sum_{i}\pi_{i}(a^{\prime}|s)^{2}M_{\pi_{i}}^{\mu_{k} }(s,a^{\prime})}}.\] (3)

Proof.: The proof is presented in App. A.1. 

Theorem 4.1 describes how to iteratively update the behavior policy \(\mu_{k}\) that minimizes objective in Eq. (2) by using the return variance \(M_{\pi_{i}}^{\mu_{k}}\). The policy \(\mu_{k+1}\) selects actions proportional to their variance, meaning high-variance return \((s,a)\) pairs are explored frequently. By visiting high-variance return pairs, policy gains informative samples and reduce the overall uncertainty. Consequently, this process improves the GVF value predictions and decrease the number of interactions needed for effective learning.

Next Theorem 4.2 ensures that behavior policy \(\mu_{k+1}\) either decreases or maintains the total variance across all GVFs relative to \(\mu_{k}\), ensuring consistent progress towards minimizing the variance without oscillation. In simple terms, each policy update ensures the variance does not increase.

**Theorem 4.2**.: _(Behavior Policy Improvement:) The behavior policy update in Eq.(3) ensures that the aggregated variances across all target policies \(\{\pi_{i}\}_{i=1}^{N}\) either decreases or remains unchanged at each iteration \(k\). This non-increasing variance property demonstrates that each successive behavior policy \(\mu_{k+1}\) is improvement over \(\mu_{k}\) in terms of reducing the total variance. Formally we have,_

\[\sum_{i=1}^{N}M_{\pi_{i}}^{\mu_{k+1}}(s)\leq\sum_{i=1}^{N}M_{\pi_{i}}^{\mu_{k}} (s),\,\forall k,\forall s\in\mathcal{S}.\]

Proof.: The proof is in App. A.1. 

## 5 Variance Function

The theorems provided in the previous section rely on the variance function \(M_{\pi_{i}}^{\mu_{k}}\). Here, we study this variance function in detail.

What is the Variance Function \(M\)?In an off-policy context (Sherstan et al. (2018), Jain et al. (2021)), introduced the variance function \(M_{\pi}^{\mu}\), which estimates the variance in return under a target policy \(\pi\) using data from a different behavior policy \(\mu\). We will directly use this function \(M_{\pi}^{\mu}\) as our variance estimator and present it here for completeness. The function \(M_{\pi}^{\mu}\) for a state-action pair under \(\pi\), with an importance sampling correction factor \(\rho_{t}=\frac{\pi(a_{t}|s_{t})}{\mu(a_{t}|s_{t})}\), is defined as:

\[M_{\pi}^{\mu}(s,a)=\text{Var}_{a\sim\mu}\left(G_{t,\pi}|s_{t}=s,a_{t}=a\right) =\mathbb{E}_{a\sim\mu}\left[\delta_{t}^{2}+\gamma^{2}\rho_{t+1}^{2}M_{\pi}^{ \mu}(s_{t+1},a_{t+1})|s_{t}=s,a_{t}=a\right]\] (4)

Here, \(G_{t,\pi}\) is the return at time \(t\), and \(\delta_{t}=r_{t}+\gamma\mathbb{E}_{a^{\prime}\sim\pi}[Q_{\pi}(s_{t+1},a^{ \prime})]-Q_{\pi}(s_{t},a_{t})\) is the TD error. We use **Expected Sarsa**(Sutton and Barto, 2018) to compute \(\delta_{t}\), eliminating the need for IS by using the expected value of the next state-action pair under \(\pi\) for bootstrapping, thus stabilizing the update and lowering the variance. \(M_{\pi}^{\mu}(s,a)\) relates the variance under \(\pi\) from the current state-action pair to the next, where actions are sampled under \(\mu\). This allows effective bootstrapping and iterative update using a TD-style method. The state variance function is defined as \(M_{\pi}^{\mu}(s)=\sum_{a}\mu(a|s)\rho^{2}(s,a)M_{\pi}^{\mu}(s,a)\).

Note, the true \(Q_{\pi}\) is required to compute the TD error \(\delta_{t}\) in Eq. (4). Following Sherstan et al. (2018), we substitute the value estimate \(\hat{Q}\) for the true function \(Q_{\pi}\) to compute \(\delta_{t}\) in Eq. (4). Additionally, we use variance estimates \(\hat{M}_{\pi}^{\mu_{k}}\) to update the next step policy \(\mu_{k+1}\) instead of true variance in Eq. (3). This approach is similar to _generalized policy iteration_(Sutton and Barto, 2018), which alternatively updates the value estimator and then improves the policy.

Next, we prove the existence of \(M_{\pi}^{\mu}\) in Lemma 5.1, which was not covered in Jain et al. (2021). This proof establishes a loose upper bound on the IS ratio \(\rho\), limiting the divergence of the behavior policy \(\mu\) from the target policy \(\pi\) for effective off-policy variance estimation. This aligns with methods like TRPO (Schulman et al., 2015) and Retrace (Munos et al., 2016), which stabilize policy updates by controlling divergence.

**Lemma 5.1**.: _(Variance Function \(M\) Existence:) Given a discount factor \(0<\gamma\leq 1\), the variance function \(M_{\pi}^{\mu}\) exists, if the below condition satisfies, \(\mathbb{E}_{a\sim\mu}\left[\rho^{2}(s,a)\right]<\frac{1}{\gamma^{2}}\) for all states._

Proof.: Proof in App. A.2. 

Note, the optimal \(\mu\) for the objective in Eq. (2), might violate the above constraint on \(\rho\); we empirically clip \(\rho\) to mitigate this problem. Additionally, IS requires \(\mu(a|s)=0\) when \(\pi(a|s)=0\). We empirically ensure \(\mu(a|s)>\varepsilon<<1\) for all actions. The same constraint is added for all the baselines for fair comparison.

Algorithm

We present GVFExplorer algorithm, detailed in Algorithm 1. Our approach uses two networks: \(Q_{\theta}\) for value function and \(M_{w}\) for variance, each with \(N\) heads (one head for each GVF). Starting with a randomly initialized behavior policy, the agent observes cumulants for \(N\) GVFs at each step and updates \(Q_{\theta}\) using off-policy TD. We use **Expected Sarsa**(Sutton & Barto, 2018) for both \(Q\) and \(M\), eliminating off-policy corrections. The target \(Q\) updates follow:

\[Q_{tar}(s_{t},a_{t},s_{t+1})=c_{t}+\gamma\sum_{a^{\prime}}\pi(a^{\prime}|s_{t+1 })Q_{\theta}(s_{t+1},a^{\prime}).\] (5)

We use the TD error from \(Q\)-learning, \(\delta_{Q}=Q_{tar}-Q_{\theta}\), to update target \(M\),

\[M_{tar}(s_{t},a_{t},s_{t+1})=\delta_{Q}^{2}+\gamma^{2}\sum_{a^{\prime}}\pi(a^{ \prime}|s_{t+1})M_{w}(s_{t+1},a^{\prime}).\] (6)

Both networks are updated via an MSE loss. The behavior policy is iteratively updated using the new variance estimates for \(K\) steps, with learned \(Q\) values used for MSE metrics in Eq. (1).

To ensure reliable estimates, we initialize \(M\) values to small non-zero constants and apply epsilon exploration, which decays over time, ensuring coverage of the state-action space. This guarantees that agents visit a broad range of state-action pairs early on, preventing issues of zero variance for unvisited pairs. We applied epsilon-exploration to both GVFExplorer and the baselines for fair comparison.

We also use techniques like experience replay Lin (1992) for data reuse and target networks for both \(Q\) and \(M\) to improve learning stability. Expected Sarsa is used consistently across all baselines for fair comparison. Refer to Algorithm 1 for further details.

```
0: Target policies \(\pi_{i\in\{1,\dots n\}}\), initial behavior policy \(\mu_{1}\), replay buffer \(\mathcal{D}\), primary networks \(Q_{\theta},M_{w}\) (small non-zero \(M\)), target networks \(Q_{\bar{\theta}},M_{\bar{w}}\), learning rates \(\alpha_{Q}\), \(\alpha_{M}\), mini-batch size \(b\), trajectory length \(T\), target update frequency \(l=100\), value/variance update frequencies \(p=4\), \(m=8\), training steps \(K\), exploration rates \(\varepsilon_{0}\), \(\varepsilon_{\text{decay}}\), \(\varepsilon_{\min}\)
1forenvironment step \(k=1,\dots K\)do
2 Set exploration rate: \(\varepsilon_{k}=\max(\varepsilon_{\min},\varepsilon_{0}\cdot\varepsilon_{ \text{decay}}^{k})\)
3 Select action \(a_{t}\sim\begin{cases}\mu_{k}(\cdot|s_{t})&\text{if random() }\leq 1-\varepsilon_{k}\\ \text{Uniform Policy}&\text{otherwise}\end{cases}\)
4 Observe next state \(s_{t+1}\) and cumulants \(c_{t}=\textit{Vector(size}(n))\)
5 Store transition \((s_{t},a_{t},s_{t+1},c_{t})\) in \(\mathcal{D}\)
6if\(k\%p==0\)then
7 //Update the Value \(Q_{\theta}\) network
8 Sample mini-batch of size \(b\) of transition \((s_{t},a_{t},s_{t+1},c_{t})\sim\mathcal{D}\).
9 Update \(Q_{\theta}\) using MSE loss \((Q_{tar}(s_{t},a_{t})-Q_{\theta}(s_{t},a_{t}))^{2}\), where \(Q_{tar}\) is Eq. (5).
10
11 end for
12if\(k\%m==0\)then
13 //Update the Variance \(M_{w}\) network
14 Sample mini-batch of size \(b\) of transition \((s_{t},a_{t},s_{t+1},c_{t})\sim\mathcal{D}\)
15 Update \(M_{w}\) using MSE loss \((M_{tar}(s_{t},a_{t})-M_{w}(s_{t},a_{t}))^{2}\), where \(M_{tar}\) is Eq. (6).
16
17 end for
18if\(k\%l==0\)then
19\(\bar{w}=w\) and \(\bar{\theta}=\theta\) //Update both target networks weights
20
21 end if
22 //Update the behavior policy \(\mu\) using the new Variance \(M_{w}\)
23 Behavior policy becomes: \(\mu_{k+1}(a|s)=\frac{\sqrt{\sum_{i=1}^{n}\pi_{i}(a|s)^{2}M_{w}^{i}(s,a)}}{\sum_ {a^{\prime}\in\mathcal{A}}\sqrt{\sum_{i=1}^{n}\pi_{i}(a^{\prime}|s)^{2}M_{w}^{i }(s,a^{\prime})}},\forall s\in\mathcal{S},a\in\mathcal{A}\).
24
25 end for
26Returns GVFs Values \(V_{i}(s)=\sum_{a}\pi_{i}(a|s)Q_{\theta}^{i}(s,a)\) for \(i=\{1,\dots,n\}\) ```

**Algorithm 1**GVFExplorer: Efficient Behavior Policy Iteration for Multiple GVFs EvaluationsExperiments

We investigate the empirical utility of our proposed algorithm in both discrete and continuous state environments. Our experiments are designed to answer the following questions: (a) How does GVFExplorer compare with the different baselines (explained below) in terms of convergence speed and estimation quality? (b) Can GVFExplorer handle a large number of GVFs evaluations? (c) Can GVFExplorer work with non-stationary GVFs which change with time? (d) Can GVFExplorer work with non-linear function approximations and complex Mujoco environments? 1

Footnote 1: The code is available on Github: https://github.com/arushijain94/GVFExplorer.

Baselines.We use Off-policy Expected Sarsa updates for parallel GVF estimations for all the experiments (including baselines) for fair comparison. We benchmark against several different **baselines**: (1) RoundRobin: uses a round-robin strategy sampling episodically from all target policies (2) MixturePolicy: Aggregated policy sampling from all target policies; (3) SR: a Successor Representation (SR) method using intrinsic reward of total change in SR and reward weights to learn behavior policy (McLeod et al., 2021). (4) BPS: behavior policy search method originally designed for single policy evaluation using a REINFORCE variance estimator (Hanna et al., 2017); we adapted it by averaging variance across multiple GVFs (similar to our objective). BPS results are limited to tabular settings due to scalability issues with it. (5) UniformPolicy: a uniform sampling policy over the action space. Implementation details and hyperparameters are in App. B.

Type of Cumulants.We experiment with three different types of cumulants, similar to McLeod et al. (2021) - **constant** with a fixed value; **distractor**, a _stationary_ signal with fixed mean and constant variance (normal distribution); **drifter**, a _non-stationary_ cumulant with zero-mean random walk with low variance (vary with time). Further description of cumulants is in App. B.2.

Experimental Settings.To answer the questions presented above, we consider different settings: **(Two Distinct Policies & Identical Cumulants):** In a tabular setting, we examine two GVFs with distinct target policies but identical _distractor cumulant_, \((\pi_{1},c),(\pi_{2},c)\). **(Two Distinct Policies & Distinct Cumulants):** In the same environment, we assess two GVFs with distinct target policy and distinct _distractor cumulant_ with different fixed means, \((\pi_{1},c_{1}),(\pi_{2},c_{2})\). **(Large Scale Evaluation with \(40\) distinct GVFs):** To verify the scalability of proposed method with high number of GVFs, we evaluate combinations of \(4\) different target policies \(\pi_{1}\dots\pi_{4}\) with \(10\) different _constant cumulants_\(c_{1}\dots c_{10}\), resulting in 40 GVFs. **(Non-Stationary Cumulants in FourRooms):** In FourRooms environment, we assess with two distinct GVFs - stationary distractor and non-stationary _drifter cumulant_\(-(\pi_{1},c_{1}),(\pi_{2},c_{2})\). **(Non-Linear Function Approximation):** In a continuous state environment with non-linear function approximator, we evaluate two distinct _distractor_ GVFs, \((\pi_{1},c_{1}),(\pi_{2},c_{2})\). **(Mujoco environments):** In Mujoco environments - walker and cheetah - evaluate different GVF tasks like walk, run and flip. Across these varied settings, we measure the averaged MSE across multiple GVFs.

### Tabular Experiments

We conducted experiments in \(20\times 20\) gridworld with four cardinal actions and a tabular \(20\times 20\) FourRooms environment for added complexity. The discount factor is \(\gamma=0.99\), and the environment is stochastic with a \(0.1\) probability of random movement. The cumulants are zero everywhere except for at the goals. Episode terminates after \(500\) steps or upon reaching the goal. True value function for MSE computation is calculated analytically \(V_{\pi}=(I-\gamma P_{\pi})^{-1}c_{\pi}\). Detailed description of target policies and cumulants is provided in App. B.3. Table 1 summarizes the below results for tabular experiments.

In **Two Distinct Policies & Identical Cumulants**, we consider gridworld environment with _distractor_ cumulant at top left corner with a reward drawn from normal distribution. Fig. 1a shows the averaged MSE across the two GVFs, with GVFExplorer showing much lower MSE compared to baselines.

Next, in **Two Distinct Policies & Distinct Cumulants**, we consider two distinct _distractor_ cumulant (with different mean) GVFs placed at top-left and top-right corner respectively. Fig. 2a shows GVFExplorer with reduced MSEs compared to baselines. Figs. 2b and 2c qualitatively analyze the average absolute difference between true and estimated GVF values across states,\(\hat{V}_{\pi_{i}}^{c_{i}}\|\), showing smaller errors (duller colors) for GVFExplorer. Fig. 8 (in App. B.3.2) presents the individual variance and MSE for both GVFs in GVFExplorer. Further, we conduct an ablation study to experiment with how GVFExplorer performance changes with poorer feature approximations. Fig. 10 (in App. B.3.3) shows that MSE increases as the feature quality deteriorates, but GVFExplorer remains robust with moderately coarse approximations.

For **Non-Stationary Cumulant in FourRooms**, we evaluate the performance in FourRooms (FR) environment (Sutton et al., 1999) with two distinct GVFs: stationary **distractor** cumulant and a non-stationary **drifter** cumulant which changes value over time. As shown in Fig. 0(b), GVFExplorer reduces MSE faster than other baselines, even with the non-stationary cumulant. Fig. 11 (in App. B.3.4) demonstrates the effectiveness of GVFExplorer in tracking the non-stationary cumulant signal in the later stages of learning.

In **Large Scale Evaluation with 40 Distinct GVFs**, we evaluate our method's scalability to large number of GVFs (refer App. B.3.5). We use **constant** cumulants with values ranging in \([50,100]\). Fig. 0(c) compares the average MSE across the GVFs, showing that GVFExplorer scales well with an increasing number of GVFs. In contrast, the SR baseline struggles with scalability due to the varying cumulant scales affecting the intrinsic reward (the summation of all SRs and reward weights) of behavior policy.

### Continuous State Environment with Non-Linear Function Approximation

We use a continuous grid environment that extends the tabular experiments to a continuous state space (similar to McLeod et al. (2021)) and four discrete actions. For **Non-Linear Function Approximation**, we consider two distinct GVFs with distractor cumulants. An **Experience Replay

Figure 1: **MSE Performance**: Averaged MSE over \(25\) runs with standard error in different experimental settings. GVFExplorer demonstrate notably lower MSE compared to the baselines.

Figure 2: **Two Distinct Policies & Distinct Cumulants**: Evaluate averaged MSE over 25 runs with two distinct distractor GVFs \((\pi_{1},c_{1}),(\pi_{2},c_{2})\) in gridworld. Green dots at top show two GVF goals. (a) Averaged MSE, (b) averaged absolute error in GVFs value predictions for baseline RoundRobin and (c) GVFExplorer. _The color bar uses log scale & vibrant colors indicate higher values._

**Buffer** with a capacity of 25K and a batch size of 64 is used for all experiments. Further details on computing true value functions using Monte Carlo and network architectures are in App. B.4.

Prioritized Experience Replay (PER).We investigate the integration of PER (Schaul et al., 2015) with our algorithm. Unlike the standard Experience Replay Buffer, which uniformly samples experiences, PER assigns priorities based on the TD error magnitude in the Q-network. PER and GVFExplorer are complementary approaches: PER re-weights the collected data in replay buffer based on the priority, while GVFExplorer adjusts the behavior policy to influence data collection.

Combining PER with GVFExplorer drastically lowers MSE compared to other baselines (even when compared to all baselines + PER). We use the absolute sum of TD errors across multiple GVF Q-functions as a priority metric for PER in all baselines, including GVFExplorer. Placing the priority on the TD error of the variance function in GVFExplorer yields less favorable results compared to priority on Q-function's TD error. In Fig. 3, we present the MSE for both standard experience replay (solid lines) and PER (dotted lines) for all algorithms. PER generally reduces MSE, but its integration with GVFExplorer shows much lower MSE. This is likely as GVFExplorer could over-sample high variance return samples, causing a skewed buffer distribution. PER's non-uniform sampling maintains a balanced data distribution, which helps in stringent MSE reduction. For the SR baseline, using the TD error in SR predictions as a priority for PER led to performance degradation, suggesting non-stationarity in SRs' TD errors might mislead PER to prioritize less relevant states under the current policy. The original SR work by McLeod et al. (2021) does not use PER in the experiments. For PER scenario, we qualitatively compare the absolute value error for baseline RoundRobin and GVFExplorer by discretizing the state space in Figs. 2(b) and 2(c) and observe that our algorithms results in smaller value prediction error. Further insights into the variance estimation by GVFExplorer is shown in Figs. 15 and 16 (App. B.4). Table 3((App. B.4) summarizes the results highlighting the performance of various algorithms.

### Mujoco Environments with Continuous State-Action Tasks

We use DM-Control (Tassa et al., 2018) based continuous state-action tasks to experiment with Mujoco environments, _Walker_ and _Cheetah_ domain. To expand the proposed method to continuous action environments, any policy-gradient (PG) based algorithm can be used. In our experiments, we use Soft Actor-Critic (SAC) algorithm (Haarnoja et al., 2018) as a base PG method to incorporate the proposed variance-minimization objective.

A separate network for variance estimation is added to SAC. Further implementation details are provided in App. B.5. To experiment in _Walker_ environment, we use two GVF tasks, namely 'walk' and 'flip'. Similarly, for _Cheetah_ environment, we use 'walk' and 'run' GVF tasks. We also added KL regularize between the learned behavior policy and the given GVFs target policies to prevent divergence. We use MC to compute the true Q-value GVF estimates and compare the MSE between these MC values and the Q-critic network. We use the same Q-critic architecture for the baseline

Figure 3: **Non-Linear Function Approximation**: (a) Averaged MSE over \(50\) runs with standard error using **Experience Replay Buffer** (solid lines) and **PER** (dotted lines). GVFExplorer show lower MSE with both buffers. PER generally reduces MSE across all algorithms except SR. Log-scale absolute value error for RoundRobin (b) and GVFExplorer (c); GVFExplorer achieves smaller errors (vibrant colors represent higher values).

algorithms - UniformPolicy and RoundRobin - for fair comparison. In Fig. 4 we observe that GVFExplorer reduces MSE faster than the baselines.

## 8 Conclusion

We addressed the problem of parallel evaluations of multiple GVFs, each conditioned on a given target policy and cumulant. We developed a method to adaptively learn a behavior policy that uses a single experience stream to estimate all GVF values in parallel. The resulting behavior policy update selects the actions in proportion to the total variance of the return across GVFs. This guides the policy to frequently explore less understood areas (high variance in return), which helps to better estimate the mean return with fewer samples. Therefore, our approach lowers the overall MSE in GVF predictions while reducing the number of interactions required. We theoretically proved that each behavior policy update reduces or maintains the total prediction error. Empirically, we showed that GVFExplorer scales effectively with an increasing number of distinct GVFs, robustly handles non-stationary cumulants in a tabular setting, and adapts well to non-linear function approximation. Additionally, we showcased its performance in complex continuous state-action Mujoco environments, showing that GVFExplorer can be seamlessly integrated with existing policy-gradient methods.

Limitations and Future Work.One notable drawback of GVFExplorer is the increased time complexity, due to simultaneously learning two networks for value and variance estimation respectively. Additionally, GVFExplorer has not been evaluated in environments with significant difference in the cumulant value range. Such disparities could lead to varying variances, potentially resulting in oversampling areas with higher cumulant values. Calibration across cumulants may be necessary in these cases.

In this work, we focused on minimizing the total MSE, but other loss functions, such as weighted MSE could also be considered. However, weighted MSE requires prior knowledge about the weighting of errors in different GVFs, which is not readily available. A potential future direction could be to use variance scales to automatically adjust these weights to provide uniform MSE reduction across all GVFs. Looking ahead, we are interested in testing our approach with multi-dimensional cumulants and general state-dependent discount factors, as well as, extending the applicability of GVFExplorer to control settings where the target policies are unknown.

## Acknowledgments and Disclosure of Funding

We are grateful to the anonymous reviewers for their valuable feedback. We also extend our thanks to Nishanth Anand, Kshitij Jain, Ayush Jain, Pierre-Luc Bacon, and Subhojyoti Mukherjee for their insightful suggestions. Josiah Hanna acknowledges support from NSF (IIS-2410981), American Family Insurance through a research partnership with the University of Wisconsin--Madison's Data Science Institute, and the Wisconsin Alumni Research Foundation.

Figure 4: **MSE in Mujoco**: Averaged MSE over \(5\) runs with standard error in Mujoco environment with continuous state-actions for (a)Walker and (b)Cheetah domains for GVFExplorer, UniformPolicy and RoundRobin. GVFExplorer consistently lowers averaged MSE as compared to the baselines.

## References

* Antos et al. (2008) Andras Antos, Varun Grover, and Csaba Szepesvari. Active learning in multi-armed bandits. In _Algorithmic Learning Theory: 19th International Conference, ALT 2008, Budapest, Hungary, October 13-16, 2008. Proceedings 19_, pp. 287-302. Springer, 2008.
* Bacon (2018) Pierre-Luc Bacon. _Temporal Representation Learning_. McGill University (Canada), 2018.
* Burda et al. (2018) Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation, 2018.
* Carpentier et al. (2015) Alexandra Carpentier, Remi Munos, and Andras Antos. Adaptive strategy for stratified monte carlo sampling. _J. Mach. Learn. Res._, 16:2231-2271, 2015.
* Dayan (1993) Peter Dayan. Improving generalization for temporal difference learning: The successor representation. _Neural computation_, 5(4):613-624, 1993.
* Degris et al. (2012) Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. _arXiv preprint arXiv:1205.4839_, 2012.
* Eysenbach et al. (2018) Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. _arXiv preprint arXiv:1802.06070_, 2018.
* Frank et al. (2008) Jordan Frank, Shie Mannor, and Doina Precup. Reinforcement learning in the presence of rare events. In _Proceedings of the 25th international conference on Machine learning_, pp. 336-343, 2008.
* Guo et al. (2022) Zhaohan Guo, Shantanu Thakoor, Miruna Pislar, Bernardo Avila Pires, Florent Altche, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang, et al. Byol-explore: Exploration by bootstrapped prediction. _Advances in neural information processing systems_, 35:31855-31870, 2022.
* Haarnoja et al. (2018) Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pp. 1861-1870. PMLR, 2018.
* Hanna et al. (2017) Josiah P Hanna, Philip S Thomas, Peter Stone, and Scott Niekum. Data-efficient policy evaluation through behavior policy search. In _International Conference on Machine Learning_, pp. 1394-1403. PMLR, 2017.
* Hesterberg (1988) Timothy Classen Hesterberg. _Advances in importance sampling_. Stanford University, 1988.
* Jaderberg et al. (2016) Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. _arXiv preprint arXiv:1611.05397_, 2016.
* Jain et al. (2021) Arushi Jain, Gandharv Patil, Ayush Jain, Khimya Khetarpal, and Doina Precup. Variance penalized on-policy and off-policy actor-critic. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pp. 7899-7907, 2021.
* Kahn and Marshall (1953) Herman Kahn and Andy W Marshall. Methods of reducing sample size in monte carlo computations. _Journal of the Operations Research Society of America_, 1(5):263-278, 1953.
* Ladosz et al. (2022) Pawel Ladosz, Lilian Weng, Minwoo Kim, and Hyondong Oh. Exploration in deep reinforcement learning: A survey. _Information Fusion_, 85:1-22, 2022.
* Li et al. (2024) Xinyu Li, Zachary C Lipton, and Liu Leqi. Personalized language modeling from personalized human feedback. _arXiv preprint arXiv:2402.05133_, 2024.
* Lin (1992) Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. _Machine learning_, 8:293-321, 1992.
* Linke et al. (2020) Cam Linke, Nadia M Ady, Martha White, Thomas Degris, and Adam White. Adapting behavior via intrinsic reward: A survey and empirical study. _Journal of artificial intelligence research_, 69:1287-1332, 2020.
* Liu et al. (2018)Marlos C Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray Campbell. Eigenoption discovery through the deep successor representation. _arXiv preprint arXiv:1710.11089_, 2017.
* McLeod et al. (2021) Matthew McLeod, Chunlok Lo, Matthew Schlegel, Andrew Jacobsen, Raksha Kumaraswamy, Martha White, and Adam White. Continual auxiliary task learning. _Advances in Neural Information Processing Systems_, 34:12549-12562, 2021.
* Metelli et al. (2023) Alberto Maria Metelli, Samuele Meta, and Marcello Restelli. On the relation between policy improvement and off-policy minimum-variance policy evaluation. In _Uncertainty in Artificial Intelligence_, pp. 1423-1433. PMLR, 2023.
* Munos et al. (2016) Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. _Advances in neural information processing systems_, 29, 2016.
* Oudeyer et al. (2007) Pierre-Yves Oudeyer, Frederic Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mental development. _IEEE transactions on evolutionary computation_, 11(2):265-286, 2007.
* Owen (2013) Art B Owen. Monte carlo theory, methods and examples, 2013.
* Parapar and Radlinski (2021) Javier Parapar and Filip Radlinski. Diverse user preference elicitation with multi-armed bandits. In _Proceedings of the 14th ACM international conference on web search and data mining_, pp. 130-138, 2021.
* Precup (2000) Doina Precup. Eligibility traces for off-policy policy evaluation. _Computer Science Department Faculty Publication Series_, pp. 80, 2000.
* Puterman (2014) Martin L Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* Rubinstein and Kroese (2016) Reuven Y Rubinstein and Dirk P Kroese. _Simulation and the Monte Carlo method_. John Wiley & Sons, 2016.
* Schaul et al. (2015) Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. _arXiv preprint arXiv:1511.05952_, 2015.
* Schlegel et al. (2021) Matthew Schlegel, Andrew Jacobsen, Zaheer Abbas, Andrew Patterson, Adam White, and Martha White. General value function networks. _Journal of Artificial Intelligence Research_, 70:497-543, 2021.
* Schmidhuber (2010) Jurgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990-2010). _IEEE transactions on autonomous mental development_, 2(3):230-247, 2010.
* Schulman et al. (2015) John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pp. 1889-1897. PMLR, 2015.
* Sherstan (2020) Craig Sherstan. Representation and general value functions. 2020.
* Sherstan et al. (2018) Craig Sherstan, Dylan R Ashley, Brendan Bennett, Kenny Young, Adam White, Martha White, and Richard S Sutton. Comparing direct and indirect temporal-difference methods for estimating the variance of the return. In _UAI_, pp. 63-72, 2018.
* Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Sutton et al. (1999) Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. _Artificial intelligence_, 112(1-2):181-211, 1999.
* Sutton et al. (2011) Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In _The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2_, 2011.
* Sutton et al. (2015)* Tang et al. (2015) Liang Tang, Yexi Jiang, Lei Li, Chunqiu Zeng, and Tao Li. Personalized recommendation via parameter-free contextual bandits. In _Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval_, pp. 323-332, 2015.
* Tassa et al. (2018) Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. _arXiv preprint arXiv:1801.00690_, 2018.
* Watkins (2004) David S Watkins. _Fundamentals of matrix computations_. John Wiley & Sons, 2004.
* White et al. (2015) Adam White et al. Developing a predictive approach to knowledge. 2015.
* Williams (1992) Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine learning_, 8:229-256, 1992.
* Xu et al. (2022) Tengyu Xu, Zhuoran Yang, Zhaoran Wang, and Yingbin Liang. A unifying framework of off-policy general value function evaluation. _Advances in Neural Information Processing Systems_, 35:13570-13583, 2022.

Proofs

### Behavior Policy Update Theorems

**Theorem 4.1**.: _(Behavior Policy Update:) To find the behavior policy \(\mu\) that minimize the variance objective across \(N\) given target policies \(\{\pi_{i}\}_{i=1}^{N}\), we iteratively update \(\mu\) by solving the objective in Eq. (2). Given state-action variance function \(M_{\pi_{i}}^{\mu_{k}}(s,a)\), the solution to Eq. (2) at iteration \(k\) is:_

\[\mu_{k+1}(a|s)=\frac{\sqrt{\sum_{i}\pi_{i}(a|s)^{2}M_{\pi_{i}}^{\mu_{k}}(s,a)}}{ \sum_{a^{\prime}}\sqrt{\sum_{i}\pi_{i}(a^{\prime}|s)^{2}M_{\pi_{i}}^{\mu_{k}}(s,a^{\prime})}}.\] (3)

Proof.: We formulate Eq. (2) as a Lagrangian equation below to solve for the optimal behavior policy \(\mu^{*}\).

\[\mathcal{L}(\mu,\lambda_{s,a},w_{s})=\underbrace{\sum_{i}\sum_{s}d(s)M_{\pi_{ i}}^{\mu}(s)}_{\text{1 part}}+\underbrace{\sum_{s,a}\lambda_{s,a}\mu(s,a)}_{\text{II part}}+\underbrace{\sum_{s}w_{s}(1-\sum_{a}\mu(s,a))}_{\text{III part}}.\] (7)

Here, \(\lambda\in\mathbb{R}^{|\mathcal{S}\times\mathcal{A}|}\) and \(\mathbf{w}\in\mathbb{R}^{|\mathcal{S}|}\) denotes the Lagrangian multipliers. The following KKT conditions satisfy:

1. \(\nabla_{\mu(s,a)}\mathcal{L}=0\)
2. \(\lambda_{s,a}\mu(s,a)=0\)
3. \(\lambda_{s,a}\geq 0\)
4. \(\mu(s,a)\geq 0\)
5. \(\sum_{a}\mu(s,a)=1\)

Gradient of \(\rho\).The gradient of \(\rho(s,a)\) w.r.t. \(\mu(a|s)\),

\[\nabla_{\mu(s,a)}\rho(s,a)=\frac{\pi(a|s)}{\nabla\mu(a|s)}=-\frac{\pi(a|s)}{ \mu(a|s)^{2}}=-\frac{\rho(s,a)}{\mu(a|s)}.\]

Solving I part.We will compute the gradient of \(M_{\pi_{i}}^{\mu}(s)\) in Eq. (4) w.r.t to given \(\mu(s,a)\). Here, \(\rho(s,a)=\frac{\pi(a|s)}{\mu(a|s)}\) is IS weight. We expand \(M_{\pi_{i}}^{\mu}(s)\) relation with \(M_{\pi_{i}}^{\mu}(s,a)\) to derive the gradient,

\[M_{\pi_{i}}^{\mu}(\tilde{s}) =\sum_{\tilde{a}}\mu(\tilde{a}|\tilde{s})\rho_{i}(\tilde{s}, \tilde{a})^{2}M_{\pi_{i}}^{\mu}(\tilde{s},\tilde{a})\] \[\nabla_{\mu(s,a)}M_{\pi_{i}}^{\mu}(\tilde{s}) =\nabla_{\mu(s,a)}\left\{\sum_{\tilde{a}}\mu(\tilde{a}|\tilde{s}) \rho_{i}(\tilde{s},\tilde{a})^{2}M_{\pi_{i}}^{\mu}(\tilde{s},\tilde{a})\right\}\] \[=\rho_{i}(s,a)^{2}M_{\pi_{i}}^{\mu}(s,a)+2\mu(a|s)\rho_{i}(s,a) \underbrace{\nabla\rho_{i}(s,a)}_{\text{=}-\frac{\rho_{i}(s,a)}{\mu(a|s)}}M_ {\pi_{i}}^{\mu}(s,a)+\underbrace{\mu(a|s)\rho_{i}(s,a)^{2}\nabla_{\mu}M_{\pi_ {i}}^{\mu}(s,a)}_{\text{=}\text{IV part}}\] \[=\rho_{i}(s,a)^{2}M_{\pi_{i}}^{\mu}(s,a)-2\rho_{i}(s,a)^{2}M_{ \pi_{i}}^{\mu}(s,a)\] \[=-\rho_{i}(s,a)^{2}M_{\pi_{i}}^{\mu}(s,a).\]

The final term \(\nabla_{\mu}M_{\pi_{i}}^{\mu}(s,a)\) is difficult to estimate in an iterative off-policy setting. Hence, we drop the _(IV part)_ from the above gradient, which is similar to Degris et al. (2012)[Sec 2.2], where the gradient of \(Q(s,a)\) was dropped while deriving the policy update.

Solving for the Lagrangian Eq. (7) further by substituting the _(I part)_, and taking derivation of II & III part and using the (1) KKT condition.

\[\nabla_{\mu(s,a)}\mathcal{L}(\mu,\lambda_{s,a},w_{s})=-\sum_{i}d(s)\rho_{i}(s, a)^{2}M_{\pi_{i}}^{\mu}(s,a)+\lambda_{s,a}-w_{s}=0.\] (8)From (2)KKT condition, we know that either \(\lambda_{s,a}=0\) or \(\mu(a|s)=0\). Following the arguments of IS, support for \(\mu(a|s)\) can only be \(0\) when the support for target policy \(\pi(a|s)=0\). Solving for the case when support for target policy in non-zero, then let \(\lambda_{s,a}=0\). We can simplify the gradient of Lagrangian in Eq. (8),

\[\begin{split} w_{s}&=-\sum_{i}d(s)\rho_{i}(s,a)^{2} M_{\pi_{i}}^{\mu}(s,a)=-\sum_{i}d(s)\frac{\pi_{i}(a|s)^{2}}{\mu(a|s)^{2}}M_{\pi_{i}} ^{\mu}(s,a)\\ \mu(a|s)&=\sqrt{\frac{\sum_{i}\pi_{i}(a|s)^{2}M_{ \pi_{i}}^{\mu}(s,a)}{-w_{s}/d(s)}}\end{split}\] (9)

We know that the numerator is always positive (variance \(M\) is positive), therefore \(w_{s}<0\). Let \(y_{s}=-w_{s}/d(s)\). From condition (5), we know that \(\sum_{a}\mu(a|s)=1\). Using Eq. (9) and summing over all the actions we get,

\[\begin{split}\sum_{a}\mu(a|s)=\sum_{a}\sqrt{\frac{\sum_{i}\pi_{i }(a|s)^{2}M_{\pi_{i}}^{\mu}(s,a)}{y_{s}}}=1\\ \text{Hence, }\sqrt{y_{s}}=\sum_{a}\sqrt{\sum_{i}\pi_{i}(a|s)^{2} M_{\pi_{i}}^{\mu}(s,a)}.\end{split}\]

Therefore, the update for optimal behavior policy becomes,

\[\mu(a|s)^{*}=\frac{\sqrt{\sum_{i}\pi_{i}(a|s)^{2}M_{\pi_{i}}^{\mu^{*}}(s,a)}} {\sum_{a}\sqrt{\sum_{i}\pi_{i}(a|s)^{2}M_{\pi_{i}}^{\mu^{*}}(s,a)}}.\]

As the optimal policy \(\mu^{*}\) appear on both the sides, this can be interpreted as an iterative update, where \(k\) denotes the iterate number.

\[\mu_{k+1}(a|s)=\frac{\sqrt{\sum_{i}\pi_{i}(a|s)^{2}M_{\pi_{i}}^{\mu k}(s,a)}}{ \sum_{a}\sqrt{\sum_{i}\pi_{i}(a|s)^{2}M_{\pi_{i}}^{\mu k}(s,a)}}.\]

**Theorem 4.2**.: _(Behavior Policy Improvement:) The behavior policy update in Eq.(3) ensures that the aggregated variances across all target policies \(\{\pi_{i}\}_{i=1}^{N}\) either decreases or remains unchanged at each iteration \(k\). This non-increasing variance property demonstrates that each successive behavior policy \(\mu_{k+1}\) is improvement over \(\mu_{k}\) in terms of reducing the total variance. Formally we have,_

\[\sum_{i=1}^{N}M_{\pi_{i}}^{\mu_{k+1}}(s)\leq\sum_{i=1}^{N}M_{\pi_{i}}^{\mu_{k }}(s),\,\forall k,\forall s\in\mathcal{S}.\]

Proof.: Theorem 4.1 suggests, for any given \(\mu_{k}\) behavior policy, the next successive approximation \(\mu_{k+1}\) minimizes the objective function Eq. (2), i.e.,

\[\begin{split}\mu_{k+1}&=\min_{\mu}\sum_{i}\sum_{s}d( s)\underbrace{M_{\pi_{i}}^{\mu_{k}}(s)}_{\text{=I}}\\ &=\min_{\mu}\sum_{i}\sum_{s}d(s)\underbrace{\sum_{a}\mu(a|s) \frac{\pi_{i}(a|s)^{2}}{\mu(a|s)^{2}}M_{\pi_{i}}^{\mu_{k}}(s,a)}_{=M_{\pi_{i}} ^{\mu_{k}}(s)}.\end{split}\] (10)

We will omit writing \(d(s)\) and assume that \(s\sim d(s)\). Further, we will use the notation \(\rho_{k}^{i}(s,a)=\frac{\pi_{i}(a|s)}{\mu_{k}(a|s)}\) for ease of writing. From Eq. (10), we can establish the relation,

\[\underbrace{\sum_{i,s,a}\mu_{k}(a|s)\frac{\pi_{i}(a|s)^{2}}{\mu_{k}(a|s)^{2} }M_{\pi_{i}}^{\mu_{k}}(s,a)}_{=M_{\pi_{i}}^{\mu_{k}}(s)}\geq\sum_{i,s,a}\mu_{k +1}(a|s)\frac{\pi_{i}(a|s)^{2}}{\mu_{k+1}(a|s)^{2}}M_{\pi_{i}}^{\mu_{k}}(s,a).\] (11)Now, we will use Eq. (11) relation to further simplify the equation and establish that variance decreases with every update step \(k\). We will use the notation \(\rho_{t:t+n}=\Pi_{t=0}^{n}\rho_{t+l}\) to denote the products.

\[\sum_{i,s}M_{\pi_{i}}^{\mu_{k}}(s) \geq\sum_{i,s,a}\mu_{k+1}(a|s)\rho_{k+1}^{i}(s,a)^{2}M_{\pi_{i}}^{ \mu_{k}}(s,a)\] \[=\sum_{i,s,a}\mu_{k+1}(a|s)\rho_{k+1}^{i}(s,a)^{2}\mathbb{E}_{a \sim\mu_{k}}[\delta_{t}^{2}+\gamma^{2}M_{\pi_{i}}^{\mu_{k}}(s_{t+1})|s_{t}=s]\] \[=\sum_{i,s}\mathbb{E}_{a\sim\mu_{k+1}}\left[(\rho_{t}^{i})^{2} \delta_{t}^{2}+\gamma^{2}(\rho_{t}^{i})^{2}\underbrace{M_{\pi_{i}}^{\mu_{k}}( s_{t+1})}_{\text{expand this}}|s_{t}=s\right]\] \[\geq\sum_{i,s}\mathbb{E}_{a\sim\mu_{k+1}}\left[(\rho_{t}^{i})^{2} \delta_{t}^{2}+\gamma^{2}(\rho_{t}^{i})^{2}\mathbb{E}_{a\sim\mu_{k+1}}\left[( \rho_{t+1}^{i})^{2}\delta_{t+1}^{2}+\gamma^{2}(\rho_{t+1}^{i})^{2}M_{\pi_{i}}^ {\mu_{k}}(s_{t+2})|s_{t+1}\right]|s_{t}=s\right]\] \[=\sum_{i,s}\mathbb{E}_{a\sim\mu_{k+1}}\left[(\rho_{t}^{i})^{2} \delta_{t}^{2}+\gamma^{2}(\rho_{t}^{i})^{2}(\rho_{t+1}^{i})^{2}\delta_{t+1}^{2 }+\gamma^{4}(\rho_{t}^{i})^{2}(\rho_{t+1}^{i})^{2}M_{\pi_{i}}^{\mu_{k}}(s_{t+2 })|s_{t}=s\right]\] \[\vdots\] \[\geq\sum_{i,s}M_{\pi_{i}}^{\mu_{k+1}}(s).\] (12)

### When does Variance Function Exists?

Let \(\mathbf{c}_{\mu}\in\mathbb{R}^{|\mathcal{S}\times\mathcal{A}|}\) denote the pseudo-reward \(\mathbf{c}_{\mu}(s,a)=\sum_{s^{\prime}}P(s^{\prime}|s,a)\delta^{2}(s,a,s^{ \prime})\) and \(\widetilde{\mathbf{P}}_{\mu}\in\mathbb{R}^{|\mathcal{S}\times\mathcal{A} \times\mathcal{S}\times\mathcal{A}|}\) represent the transition probability matrix \(\widetilde{\mathbf{P}}_{\mu}(s,a,s^{\prime},a^{\prime})=P(s^{\prime}|s,a) \mu(a^{\prime}|s^{\prime})\rho^{2}(s^{\prime},a^{\prime})\). The matrix form of \(M_{\pi}^{\mu}\) is:

\[M_{\pi}^{\mu}=\mathbf{c}_{\mu}+\gamma^{2}\widetilde{\mathbf{P}}_{\mu}M_{\pi}^ {\mu}\implies M_{\pi}^{\mu}=(I-\gamma^{2}\widetilde{\mathbf{P}}_{\mu})^{-1} \mathbf{c}_{\mu}.\] (13)

The existence of \(M_{\pi}^{\mu}\) hinges on the invertibility of matrix \((I-\gamma^{2}\widetilde{\mathbf{P}}_{\mu})\). Lemma 5.1 establishes the existence of the above inverse using Definition A.1 and Lemmas A.2 and A.3.

**Definition A.1**.: **(Spectral Radius)** The spectral radius of a matrix \(\mathbf{A}\in\mathbb{R}^{n\times n}\) is denoted by \(\text{sr}(\mathbf{A})=\max(\lambda_{1},\lambda_{2},\ldots,\lambda_{n})\), where \(\lambda_{i}\) denotes the \(i^{th}\) eigenvalue of \(\mathbf{A}\).

**Lemma A.2**.: _The spectral radius \(\text{sr}(\mathbf{A})\) of a matrix \(\mathbf{A}\in\mathbb{R}^{n\times n}\) follows the relation, \(\text{sr}(\mathbf{A})\leq\|\mathbf{A}\|\), where, \(\|\mathbf{A}\|=\max_{i}\sum_{j}\mathbf{A}(i,j)\) is the infinity norm over a matrix._

Proof.: Following the derivation from Bacon (2018) Ph.D. thesis and work of Watkins (2004), we use the eigenvalue of a matrix to show that \(\text{sr}(\mathbf{A})<\|\mathbf{A}\|\). We can write \(\lambda\mathbf{x}=\mathbf{A}\mathbf{x}\), when \(\lambda\) is the eigenvalue of \(\mathbf{A}\). For any sub-multiplicative matrix norm, \(\|\mathbf{A}\mathbf{B}\|\leq\|\mathbf{A}\|\|\mathbf{B}\|\). Using this property,

\[\|\lambda\mathbf{x}\|=|\lambda|\|\mathbf{x}\| =\|\mathbf{A}\mathbf{x}\|\leq\|\mathbf{A}\|\|\mathbf{x}\|,\] \[|\lambda|\leq\|\mathbf{A}\|.\]

The above is true for any eigenvalue \(\lambda\) of \(\mathbf{A}\). So this must also be true for the maximum eigenvalue of \(\mathbf{A}\). Therefore, we can express,

\[\text{sr}(\mathbf{A})\leq\|A\|.\]

**Lemma A.3**.: _When the spectral radius of \(\text{sr}(\mathbf{A})<1\), then \((I-\mathbf{A})^{-1}\) exits and is equal to, \((I-\mathbf{A})^{-1}=\sum_{t=0}^{\infty}\mathbf{A}^{t}\)._

Proof.: The proof for the Lemma is presented in Puterman (2014)[Proposition A.3].

**Lemma 5.1**.: _(**Variance Function \(M\) Existence:**) Given a discount factor \(0<\gamma\leq 1\), the variance function \(M_{\pi}^{\mu}\) exists, if the below condition satisfies, \(\mathbb{E}_{a\sim\mu}\left[\rho^{2}(s,a)\right]<\frac{1}{\gamma^{2}}\) for all states._

Proof.: Following Lemma A.3, the existence of \(M\) hinges on the existence of inverse \((I-\gamma^{2}\mathbf{\bar{P}}_{\mu})^{-1}\). Further, \((I-\gamma^{2}\mathbf{\bar{P}}_{\mu})^{-1}\) exists if spectral radius \(\textit{sr}(\gamma^{2}\mathbf{\bar{P}}_{\mu})<1\). Further, from Lemma A.2, we know that for any given matrix \(\mathbf{A}\), spectral radius satisfies, \(\textit{sr}(\mathbf{A})\leq\|A\|\). Hence, using the above two lemmas, we can express,

\[\textit{sr}(\gamma^{2}\mathbf{\bar{P}}_{\mu})\leq\|\gamma^{2}\mathbf{\bar{P}}_ {\mu}\|\leq\gamma^{2}\|\mathbf{\bar{P}}_{\mu}\|.\]

Further, if spectral radius satisfies the below condition, then the inverse exists,

\[\textit{sr}(\gamma^{2}\mathbf{\bar{P}}_{\mu})\leq\gamma^{2}\|\mathbf{\bar{P}}_ {\mu}\|<1.\]

We expand the middle infinity norm term and get

\[\max_{s,a}\sum_{s^{\prime},a^{\prime}}\mathbf{\bar{P}}_{\mu}(s,a, s^{\prime},a^{\prime})<\frac{1}{\gamma^{2}}\] \[\max_{s,a}\sum_{s^{\prime}}P(s^{\prime}|s,a)\sum_{a^{\prime}}\mu( a^{\prime}|s^{\prime})\rho^{2}(s^{\prime},a^{\prime})<\frac{1}{\gamma^{2}}.\]

We can further express the above condition as \(\mathbb{E}_{a\sim\mu}\left[\rho(s,a)^{2}\right]<\frac{1}{\gamma^{2}},\forall s \in\mathcal{S}\). 

## Appendix B Experiments

This section provides detail about the experiments in the main paper as well as additional experiments in the Appendix. All the experiments require less than \(1GB\) of memory and have used combined compute less than total 4 CPU months and 1 GPU month.

For all the experiments, we consider the two target policies with four cardinal directions left (L), right (R), up (U) and down (D) for the tabular and non-linear function approximation environments. These policies are specified as follows for every state \(s\in\mathcal{S}\):

\[\pi_{1}(s) =\{L:0.175,R:0.175,U:0.25,D:0.4\}\] (14) \[\pi_{2}(s) =\{L:0.25,R:0.15,U:0.25,D:0.35\}.\]

### Baselines

(1) RoundRobin: We used a round robin strategy to sample data from all given \(n\) target policies episodically. We used Expected Sarsa to estimate all GVF value functions in parallel when a transition is given as \((s_{t},a_{t},s_{t+1},c_{i=\{1,\dots n\}})\),

\[Q_{i}(s_{t},a_{t})=Q_{i}(s_{t},a_{t})+\alpha\left(c_{i}(s_{t},a_{t})+\gamma \sum_{a}\pi_{i}(a^{\prime}|s_{t+1})Q_{i}(s_{t+1},a^{\prime})-Q_{i}(s_{t},a_{t })\right)\]

(2) MixturePolicy,UniformPolicy are also evaluated using Expected Sarsa. MixturePolicy is defined as,

\[\mu_{\text{Mixture}}(a|s)=\frac{\sum_{i=1}^{N}\pi_{i}(a|s)}{\sum_{a^{\prime}} \sum_{i=1}^{N}\pi_{i}(a^{\prime}|s)}.\]

(3) SR: Based on (McLeod et al., 2021), SR uses the summation of weight changes in the Successor Representation (SR) and reward weights to obtain the intrinsic reward for behavior policy updates. We use Expected Sarsa to learn both the SR and the Q-value function from the intrinsic reward. The behavior policy is generated using a Boltzmann policy over the learned Q function, as it empirically performs better than a greedy policy. We apply simple TD Expected Sarsa updates instead of Emphatic TD(\(\lambda\)) as shown in Algo 2 in McLeod et al. (2021). The learning rates for SR, reward weights, and behavior policy Q function are kept the same.

(4)BPS: (Hanna et al., 2017) Use a Reinforce style estimator to learn \(IS(\tau,\pi)=G(\tau)\Pi_{t=1}^{T}\frac{\pi(a_{t}|s_{t})}{\mu(a_{t}|s_{t})}\), as mentioned in the original paper. Since, the original work is only about single policy evaluation,

[MISSING_PAGE_FAIL:18]

(MSE) after \(800K\) sample interactions. **This hyperparameter tuning approach was consistently applied for all algorithms including baselines**. Fig. 6 shows the sensitivity analysis of varying learning rates for value functions (all baselines) and variance functions (our method) with the averaged MSE performance in **Two Distinct Policies & Identical Cumulants**. The learning rate resulting in the lowest MSE was selected as optimal. In Table 2, we show the optimal hyperparameters for the four experimental settings discussed in the paper earlier (refer _Experimental Settings_ in Sec. 7).

#### b.3.1 Two Distinct Policies & Identical Cumulants

In tabular \(20\times 20\) grid, we consider distinct target policies with identical distractor cumulant \(r=N(\mu=100,\sigma=5)\). In Fig. 7, we depict the individual MSE over \(25\) runs for both the GVFs \((\pi_{1},c),(\pi_{2},c)\); showing decreased MSE for GVFExplorer compared to the baselines.

#### b.3.2 Two Distinct Policies & Distinct Cumulants

In tabular \(20\times 20\) grid, we consider distinct target policies and distinct distractor cumulants \(c_{1}=N(\mu=100,\sigma=5)\) placed at top-left corner and \(c_{2}=N(\mu=50,\sigma=5)\) placed in top-right corner. Fig. 8 shows the individual MSE for all algorithms and the estimated variance in GVFExplorer for both the GVFs.

Semi-greedy \(\pi\) for Two Distinct Policies & Distinct Cumulants:We evaluated semi-greedy target policies with distinct cumulants, \((\pi_{1},c_{1})\) and \((\pi_{2},c_{2})\) within a 20x20 grid. The target policies are designed with a bias towards top-left and top-right goals respectively,

\[\pi_{1}(s) =\{L:0.4,R:0.1,U:0.4,D:0.1\}\forall s\in\mathcal{S}\] (15) \[\pi_{2}(s) =\{L:0.1,R:0.4,U:0.4,D:0.1\}\forall s\in\mathcal{S}.\]

We keep the same cumulants same as in previous experiment. Fig. 9 compares the average MSE performance, where GVFExplorer exhibits comparable MSE to RoundRobin baseline but requires

Figure 6: **Impact of Learning Rate on Averaged MSE in Two Distinct Policies & Identical Cumulants scenario**: Demonstrate the effect of changing minimum value of learning rate on the averaged MSE (performance averaged over \(10\) runs) across GVFs. The optimal hyperparameter is selected based on the least MSE in these plots. LR_Q: value learning rate, LR_M: variance learning rate.

Figure 7: **Two Distinct Policies & Identical Cumulants in 20x20 Grid**: Averaged MSE over \(25\) runs for two GVFs \((\pi_{1},c),(\pi_{2},c)\) with same cumulant. We show individual MSE\({}_{1}\), MSE\({}_{2}\). GVFExplorer shows lower MSE compared to other baselines.

more samples to converge. This outcome can be attributed to the near-greedy nature of the target policies, which inherently guides RoundRobin along goal directed trajectories, enabling it to achieve nearly accurate predictions with fewer samples. The optimal hyperparameters for RoundRobin, UniformPolicy and MixturePolicy is \(\alpha_{Q}=0.95\). We used \(\alpha_{Q}=0.5,\alpha_{M}=0.8\) for ours GVFExplorer. Another baseline SR has \(\alpha_{Q}=0.8\) and BPS as \(\alpha_{Q}=0.9\) as optimal hyperparameters.

#### b.3.3 Ablation Study on Effect in Performance on using Poor Feature Approximator

In this section, we study the effect of using degraded approximations or feature quality on the performance metrics. We conducted an ablation study in a 20x20 grid with two distinct _distractor_ GVFs with cumulants, \(c_{1}=N(\mu=100,\sigma=5)\) placed on the top-left corner and \(c_{2}=N(\mu=50,\sigma=5)\) on the top-right corner. We reduced the state space into 10x20 and 5x20 feature grids (grouping factors of 2 and 4, respectively), and compared results against the original setup (no approximation, factor = 1). As shown in Fig. 10, the MSE increases as the feature quality deteriorates. Despite this, GVFExplorer outperforms RoundRobin and MixturePolicy, though with very poor approximations (factor = 4), the UniformPolicy performs better due to inaccurate variance estimates. These results demonstrate that GVFExplorer is robust with moderately coarse approximations but can degrade with significantly poor feature representations, as expected. Further, these results are strengthened by the performance of GVFExplorer in Mujoco environments Sec. 7.3.

Figure 8: **Two Distinct Policies & Distinct Cumulants in Grid env: Evaluate two distinct GVFs \((\pi_{1},c_{1})\) and \((\pi_{2},c_{2})\) averaged over 25 runs. Compared baselines – RoundRobin, MixturePolicy, UniformPolicy, SR, BPS with GVFExplorer. Green dots show GVF goals. (a) Individual MSE\({}_{1}\) for GVF \((\pi_{1},c_{1})\), and MSE\({}_{2}\) for GVF \((\pi_{2},c_{2})\). (b,c) Estimated variance \(\hat{M}_{\pi_{1}}^{c_{1}},\hat{M}_{\pi_{2}}^{c_{2}}\) in GVFExplorer. Variance plots show log-scale empirical values; most areas appear black, due to their relatively small magnitude compared to high variance regions. _The color bar uses log scale & vibrant colors indicate higher values._**

Figure 9: **Semi-greedy target policies in Two Distinct Policies & Distinct Cumulants: Analysis of MSE averaged over \(25\) runs with semi-greedy target policies. (a) Averaged MSE (b) MSE\({}_{1},\text{MSE}_{2}\). We observe a slower convergence of GVFExplorer as compared to baselines like RoundRobin, MixturePolicy due to target policies being semi-greedy.**

#### b.3.4 Non-Stationary Cumulant in FourRooms

In complex FourRooms environment, we consider two distinct target policies in Eq. (14) and different cumulants - stationary **distractor** with \(N(\mu=100,\sigma=2)\) in top-left room, non-stationary **drifter** signal of \(\sigma=0.5\) in top-right room. Figs. 10(a) and 10(b) shows the change in estimated variance \(M\) of GVFExplorer from early learning steps to later steps (vibrant color shows higher numerical value). We experimented with changing the value of \(\sigma\) of drifter cumulant to see the effect on MSE. In Fig. 10(c) we observe that MSE increases with increasing the value of driftness (\(\sigma\)) over time.

Figure 11: **Non-Stationary Cumulant in FourRooms**: We placed a stationary distractor cumulant in the top-left room and a non-stationary drifter cumulant in the top-right room. (a & b) show the change in estimated variance \(\hat{M}\) over time, highlighting the effectiveness of GVFExplorer in tracking the non-stationary cumulant placed in top-right corner, later in learning process over stationary cumulant (top-left). (c) shows the average MSE for GVFExplorer with different levels of driftness (\(\sigma\)) in the cumulant value; higher driftness leads to higher MSE.

Figure 10: **Impact of Feature Approximation on MSE:** Averaged MSE over 10 runs with standard error in tabular environment. Increasing _GroupingFactor_ indicates coarser feature mapping (more states mapped to the same feature). As expected, overall MSE increases with coarser mapping. GVFExplorer outperforms baselines given a reasonable feature approximator.

#### b.3.5 Large Scale Evaluation with 40 Distinct GVFs

We evaluate GVFExplorer ability to handle a large number of GVFs. We examine four target policies (\(\pi_{n\in 1\dots 4}\)), each aligned with a cardinal direction, and ten cumulants (\(c_{m\in 1\dots 10}\)), aiming to predict \(40\) GVF combinations (\(v_{\pi_{1\dots 4}}^{c_{1}}\dots v_{\pi_{1\dots 4}}^{c_{10}}\)). Each GVF is associated with a state space region ("goal"), uniformly sampled and assigned constant cumulant value ranging \([50,100]\), demonstrated in Fig. 12. In this setting, we considered \(4\) target policies in the four cardinal directions, namely:

\[\pi_{N}(s) =\{L:0.1,R:0.1,U:0.7,D:0.1\}\forall s\in\mathcal{S}\] \[\pi_{E}(s) =\{L:0.1,R:0.7,U:0.1,D:0.1\}\forall s\in\mathcal{S}\] \[\pi_{S}(s) =\{L:0.1,R:0.1,U:0.1,D:0.7\}\forall s\in\mathcal{S}\] \[\pi_{W}(s) =\{L:0.7,R:0.1,U:0.1,D:0.1\}\forall s\in\mathcal{S}.\]

#### b.3.6 Ablation: IS ratios vs Expected Sarsa Update

In FourRooms environment with distractor and drifter cumulants (two distinct GVFs), we compare the following off-policy update styles: (1) Off-policy TD updates using IS \(\rho\) correction,

\[Q(s_{t},a_{t}) =Q(s_{t},a_{t})+\alpha_{Q}(\underbrace{c_{t}+\gamma\rho_{t+1}Q(s_{ t+1},a_{t+1})-Q(s_{t},a_{t})}_{=\delta_{Q}})\] \[M(s_{t},a_{t}) =M(s_{t},a_{t})+\alpha_{M}(\delta_{Q}^{2}+\gamma^{2}\rho_{t+1}^{2 }M(s_{t+1},a_{t+1})-M(s_{t},a_{t})\]

and (2) Expected Sarsa update in Eqs. (5) and (6). In Fig. 13, we observe that Expected Sarsa leads to lower MSE, hence we use Expected Sarsa for all the TD updates in _all the algorithms_ including baseline for further update stability.

### Continuous State Environment with Non-linear Function Approximation

Continuous Environment.We extend the tabular GridWorld environment to a continuous state space, similar to the approach by McLeod et al. (2021). The environment is a square of dimension \(1\times 1\) with four discrete actions. We evaluate two GVFs: the first GVF has a cumulant at the top-left corner, \(c_{1}=\mathcal{N}(\mu=100,\sigma=5)\), and the second at the top-right corner, \(c_{2}=\mathcal{N}(\mu=50,\sigma=5)\). The target policies are consistent with those used in the tabular environment. The agent receives a

Figure 12: \(10\) different cumulants for **Large Scale Evaluation with 40 Distinct GVFs** in \(20\times 20\) grid. The color depict the cumulant empirical value.

Figure 13: **IS vs Expected Sarsa update in FR env:** We show the averaged MSE (over \(25\) runs) in Fourrooms by doing off-policy IS corrections in TD updates and off-policy Expected Sarsa in GVFExplorer algorithm. Expected Sarsa leads to smaller MSE and faster convergence.

zero cumulant signal elsewhere and moves \(0.025\) units in the selected direction with added uniform noise \(\mathcal{U}[-0.01,0.01]\). Episodes start randomly, excluding a \(0.05\) radius from the goal state, and end after 500 steps or upon reaching a \(0.05\) radius from the goal.

Fig. 14 presents the individual MSE for both GVFs under standard Experience Replay and PER. Our method, GVFExplorer, consistently achieves lower MSE compared to baselines. Fig. 15 shows the absolute GVF value prediction error with PER for both the baseline RoundRobin and GVFExplorer. Fig. 16 illustrates the estimated variance from each GVF, underscoring the necessity for a sampling strategy that prioritizes high-variance return areas to reduce data interactions and ultimately reducing the variance and MSE. Fig. 17 depicts sampled from baseline RoundRobin and GVFExplorer. Table 3 summarizes the performance of various algorithms in this continuous environment.

Computation of True GVF Values.The true GVF values in a continuous environment are computed using a Monte Carlo (MC) method. The continuous state space is discretized into a grid, with an initial state sampled from each grid cell. We calculate the average discounted return over \(200,000\) trajectories following policy \(\pi_{i}\) with cumulant \(c_{i}\). The mean squared error (MSE) between the estimated and true GVF values is then computed using these discretized states, expressed as \(\mathbb{E}_{i}\left[\sum_{s}\left(V_{\pi_{i}}^{c_{i}}(s)-\hat{V}_{\pi_{i}}^{c_ {i}}(s)\right)^{2}\right]\) for all algorithms.

Network Architecture.We use distinct deep networks for learning value \(Q\) and variance \(M\). Both networks share a similar architecture, with a shared feature extractor for input states and separate output heads for each GVF, producing multidimensional outputs for both value and variance. The variance network includes a Softplus layer before each head's output to ensure positive numerical values.

### Mujoco Environment with Continuous State-Action Tasks

We conducted additional experiments using the DM-Control suite in the Mujoco environment, focusing on the _Walker_ and _Cheetah_ domains. For the _Walker_ domain, we defined two distinct

Figure 14: **Individual MSE in Continuous Env.: Compare the MSE metrics in baselines - RoundRobin, MixturePolicy, SR and GVFExplorer (averaged over \(50\) runs with standard errors) for both standard Experience Replay Buffer (solid lines) and with Priority Experience Replay (PER) (dotted lines). GVFExplorer demonstrates lower MSE with both types of replay buffers. PER generally reduces MSE across all algorithms, except for SR.**

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline Avg MSE & \multirow{2}{*}{SR} & \multirow{2}{*}{MixturePolicy} & \multicolumn{2}{c}{GVFExplorer} & \multirow{2}{*}{
\begin{tabular}{c} **\% Improvement of GVFExplorer** \\ (Ours) \\ \end{tabular} } \\ \(@1e6\) steps & & & & (Ours) & (against best baseline) \\ \hline
**Standard** & 21.7 & 18.25 & 16.78 & **5.19** & 69\% \\
**Replay Buffer** & & & & & \\
**Prioritized** & & & & & \\
**Exp. Replay** & & & & & \\ \hline \end{tabular}
\end{table}
Table 3: **Avg. MSE Summary for Continuous Env.: Averaged MSE across two GVFs for different algorithms in the continuous environment. GVFExplorer performance measured against others using standard and prioritized experience replay after \(1\times 10^{6}\) learning steps. _Note: Smaller MSE indicates better performance._**GVFs: _walk_ and _flip_. In the _Cheetah_ domain, we evaluated two GVFs: _walk_ and _run_. To handle the continuous action space in these environments, we leveraged policy gradient methods, which are essential when working with continuous actions, as value-based methods like Q-learning are not directly applicable.

Figure 16: **Estimated Variance in Continuous Env**: The two GVF goals are depicted in Green. We show the estimated variance \(M\) (log values) over states from GVFExplorer method highlighting the motivation for behavior policy to visit high variance areas. (a) Mean variance, (b) Variance for left goal GVF, (c) variance for right goal GVF. These variance plots show log scale empirical values; most areas appear black due to their relatively small magnitude compared to high variance regions.

Figure 17: **Sampled trajectories in Continuous Env**: GVFExplorer generates trajectories which reduces the overall variance, thus minimizing the total MSE. Contrary, RoundRobin collects data according to given target policies. Green dots show GVF goals and red depicts the start state.

Figure 15: **Value Prediction Errors in Continuous Env**: Compares log-scale absolute errors between actual and predicted values for two GVFs. Top row: RoundRobin baseline errors; Bottom row: GVFExplorer results at equivalent steps. **(Col 1)**: Mean error, **(Col 2)**: Error in GVF 1, **(Col 3)**: Error in GVF 2. GVFExplorer specially achieves smaller errors in areas where RoundRobin has higher MSE, due to the focus on reducing overall MSE (indicated by lighter colors).

Our method can be incorporated with any policy gradient (PG) algorithm. For these experiments, we used Soft Actor-Critic (SAC)(Haarnoja et al., 2018) which provides stability in such settings. SAC uses an entropy regularizer to encourage **exploration**, where the regularization coefficient \(\alpha\) is learned adaptively. This allows the agent to balance exploration and exploitation effectively. We present the SAC-GVFExplorer in Algorithm 2, a modified version of SAC designed to efficiently handle parallel estimation of multiple GVFs, each aligned with a specific target policy and cumulant.

The first modification involves incorporating a separate variance network, \(M_{w_{d}}(s,a)\), which estimates the variance of returns and supports the behavior policy's objective to reduce mean squared error (MSE) in GVF estimation. This variance network, parameterized by weights \(w_{1}\) and \(w_{2}\), updates alongside the Q-value critics to capture return variability more effectively.

The Q-value target is calculated like in standard SAC algorithm, which uses double-Q trick and entropy term for enhanced exploration. A slight difference from standard SAC is sampling the next state action \(a^{\prime}\) from the target policy \(\pi_{i}(\cdot|s^{\prime})\).

\[Q_{\text{tar}}(s,a)=c+\gamma\left(\min_{d=1,2}Q_{\bar{\theta}_{d}}(s^{\prime}, a^{\prime}\sim\pi_{i}(\cdot|s^{\prime}))-\alpha\log\mu_{\phi}(\bar{a}|s^{ \prime})\right),\quad\bar{a}\sim\mu_{\phi}(\cdot|s^{\prime}),\]

where \(\alpha\) is the learned entropy coefficient and \(\mu_{\phi}\) is the parametrized behavior policy. The variance target update, \(M_{\text{tar}}\), then incorporates the Q-value temporal difference (TD) error, \(\delta_{Q}=Q_{\text{tar}}-\min_{d=1,2}Q_{\theta_{\bar{\theta}_{d}}}(s,a)\),

\[M_{\text{tar}}(s,a)=\delta_{Q}^{2}+\gamma^{2}\left(\min_{d=1,2}M_{\bar{w}_{d}} (s^{\prime},a^{\prime}\sim\pi_{i}(\cdot|s^{\prime}))-\alpha\log\mu_{\phi}(\bar {a}|s^{\prime})\right).\]

To ensure the behavior policy does not diverge from target GVF policies \(\pi_{i}\), we introduce a KL regularization term that maintains alignment with each target policy by minimizing \(\text{KL}(\mu_{\phi}(\cdot|s)\parallel\pi_{i}(\cdot|s))\).

As computing exact Q-values is infeasible for continuous state-action domains, we approximate these values using Monte Carlo (MC) rollouts over \(100\) episodes for a fixed set of randomly selected \(50\) sampled state-actions. This MC approximation enables accurate computation of MSE by comparing learned Q-values with empirical MC estimates.

Finally, the behavior policy \(\mu_{\phi}\) is updated to minimize the MSE by focusing on areas of high variance, effectively improving estimation efficiency. Further, the entropy term is similar to standard SAC algorithm for improving the exploration. This update step, driven by the variance network, is computed as:

\[\nabla_{\phi}\sum_{s\sim\mathcal{D}}\left(\min_{d=1,2}M_{w_{d}}(s,\bar{a})- \alpha\log\mu_{\phi}(\bar{a}|s)+\beta\sum_{i=1}^{N}\text{KL}(\mu_{\phi}(\cdot |s)\parallel\pi_{i}(\cdot|s))\right),\]

where action \(\bar{a}\) is sampled from \(\mu_{\phi}(\cdot|s)\). To support adaptive exploration, we update \(\alpha\) by optimizing:

\[-\alpha\log\mu_{\phi}(\cdot|s)+\bar{H},\]

where \(\bar{H}\) is the target entropy.

With these adaptations, SAC-GVFExplorer effectively estimates GVFs values in continuous state-action domains, achieving lower MSE compared to baselines such as RoundRobin and UniformPolicy and performing well in complex Mujoco environments.

We used TD3 to train target policies for each GVF and selected mid-level performing policies as target policies for the respective GVF tasks. This setup allows the behavior policy to efficiently gather data for parallel GVF estimation.

``` Input: Target policies \(\pi_{i\in\{1,\dots,n\}}\), initialized behavior policy \(\mu_{\phi}\), replay buffer \(\mathcal{D}\), primary networks \(Q\) with \(\theta_{1},\theta_{2}\), primary \(M\) variance with \(w_{1},w_{2}\), target networks \(Q_{\bar{\theta}_{1}},Q_{\bar{\theta}_{2}}\), \(M_{\bar{w}_{1}},M_{\bar{w}_{2}}\), learning rates \(\alpha_{Q}\), \(\alpha_{M}\), mini-batch size \(b\), entropy coefficient \(\alpha\), KL regularizer \(\beta\), update frequencies \(p\), \(m\), \(l\), target entropy \(\bar{H}\), training steps \(K\)
1forenvironment step \(k=1,\dots,K\)do
2 Select action \(a\sim\mu_{\phi}(\cdot|s)\)
3 Observe next state \(s^{\prime}\) and cumulants \(c\)
4 Store transition \((s,a,s^{\prime},c)\) in replay buffer \(\mathcal{D}\)
5if\(step\%p==0\)then
6 Sample mini-batch \(\mathcal{D}\sim(s,a,s^{\prime},c)\)
7 //Q-critic update
8 Compute \(Q_{tar}(s,a)=c+\gamma\left(\min_{d=1,2}Q_{\bar{\theta}_{d}}(s^{\prime},a^{ \prime}\sim\pi_{i}(\cdot|s^{\prime}))-\alpha\log\mu_{\phi}(\bar{a}|s^{\prime})\right)\), \(\bar{a}\sim\mu_{\phi}(\cdot|s^{\prime})\)
9 Update \(Q_{\theta}\) with MSE loss: \((Q_{tar}-Q_{\theta_{d}}(s,a))^{2}\) for \(d=1,2\)
10 //Compute TD error
11\(\delta_{Q}=Q_{tar}-\min_{d=1,2}Q_{\theta_{d}}(s,a)\)
12 //Variance-critic update
13 Compute \(M_{tar}(s,a)=\delta_{Q}^{2}+\gamma^{2}\left(\min_{d=1,2}M_{\bar{w}_{d}}(s^{ \prime},a^{\prime}\sim\pi_{i}(\cdot|s^{\prime}))-\alpha\log\mu_{\phi}(\bar{a}| s^{\prime})\right)\), \(\bar{a}\sim\mu_{\phi}(\cdot|s^{\prime})\)
14 Update \(M_{w}\) with MSE loss: \((M_{tar}-M_{w_{d}}(s_{t},a_{t}))^{2}\) for \(d=1,2\)
15
16 end if
17if\(step\%l==0\)then
18 Update target networks: \(\bar{\theta}_{d}=\theta_{d}\), \(\bar{w}_{d}=w_{d}\)
19 end if
20if\(step\%m==0\)then
21 // Update behavior policy \(\mu_{\phi}\)
22 Update \(\phi\) using \(\nabla_{\phi}\sum_{s\sim\mathcal{D}}\left(\min_{d=1,2}M_{w_{d}}(s,\bar{a})- \alpha\log\mu_{\phi}(\bar{a}|s)-\beta\sum_{i=1}^{N}\text{KL}(\mu_{\phi}(\cdot|s )\parallel\pi_{i}(\cdot|s))\right)\),
23 where \(\bar{a}\) is sampled from \(\mu_{\phi}(\cdot|s)\).
24 //Update \(\alpha\) entropy regularizer
25 Update \(\alpha\) with loss \((-\alpha\log\mu_{\phi}(\cdot|s)+\bar{H})\)
26 end if
27
28 end for
29Returns Estimated GVF values \(Q_{\theta}^{i}(s,\cdot)\) for \(i=\{1,\dots,n\}\) ```

**Algorithm 2**SAC-based GVExplorer

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer:[Yes] Justification: We propose GVFExplorer and support the algorithm with proofs of Theorem 4.1 and Theorem 4.2 in the App. A. Further, we empirically show the performance in Sec. 7 in tabular and non-linear func. approx. setting. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.

* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Refer Sec. 8 for limitations, where we discuss the limitation with extra computation cost of the variance function, as compared to baselines which might just require value function. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Refer App. A for complete assumptions and proofs. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility**Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide the exact proposed GVFExplorer in Algorithm 1 with all the parameters. The optimized hyperparameters for all algorithms are presented in Table 2. The learning rate sensitivity graph is shown in Fig. 6. We provided the code link in the paper. The environmental details and network architecture is described in App. B.3 and App. B.3.5. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example * If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. * If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. * If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). * We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We present the code link in the paper: https://github.com/arushijain94/GVFExplorer to produce all main and supplementary results. We don't use any dataset. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The hyperparameters are presented in App. B.3 and App. B.4 Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report standard error in the empirical results and averaged performance over more than \(25\) runs. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Refer directly to text immediately below Appendix Experiment section.. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification:We follow all the ethical standards. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The work presented here proposes a data-efficient approach to evaluate multiple objectives (GVFs) and does not experiment with real-world data. In the passage before the Contributions section, we outline potential extension to real-world applications like personalized recommender systems. The proposed method can unlock positive societal impacts. By requiring less data for evaluation, it enhances user privacy and reduces computational footprint for a more sustainable future. Though this work offers benefits for individual users, applying it across users necessitates privacy-preserving approaches. This work opens exciting future directions integrating privacy-preserving approaches like federated learning or differential privacy with our method. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The proposed work is more suited towards academic and research setting, posing no such described above risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use classical RL benchmark environments like FourRooms (cited them) and invented few environments. Our code link is added in the paper. We use existing EMDP library to extend the gridworld envs which uses MIT License. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets**Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide the link to our codebase in the paper which has been properly anonymized. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not have any crowdsourcing experiments. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Paper does not involve any crowdsourcing. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.