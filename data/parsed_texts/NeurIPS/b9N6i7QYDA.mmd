# Lightspeed Black-box Bayesian Optimization via

Local Score Matching

 Yakun Wang

University of Bristol

yakun.wang@bristol.ac.uk

&Sherman Khoo

University of Bristol

sherman.khoo@bristol.ac.uk

&Song Liu

University of Bristol

song.liu@bristol.ac.uk

###### Abstract

Bayesian Optimization (BO) is a powerful tool for tackling optimization problems involving limited black-box function evaluations. However, it suffers from high computational complexity and struggles to scale efficiently on high-dimensional problems when fitting a Gaussian process surrogate model. We address these issues by proposing a fast acquisition function maximization procedure. We leverage the fact that Probability Improvement (PI) acquisition function can be seen as a likelihood function whose score can be estimated through a simple linear regression problem called local score matching. This enables fast gradient-based optimization of the acquisition function, and a competitive BO procedure which performs similarly to that of computationally expensive neural networks.

## 1 Introduction

Black-box optimization [1; 2] seeks to identify the optimum (maximum in this paper) of a function \(g(\textbf{x})\) whose closed-form expression and gradient information are unknown, with as little computational resources as possible. Bayesian optimization (BO, [6]) is particularly effective for this task. It utilizes a probabilistic surrogate, typically a Gaussian Process (GP, [20]), to sequentially select new evaluation points based on the mean function and quantify uncertainty through the covariance function. Although BO can achieve relatively good accuracy with only a few function evaluations, the cost of each new proposal evaluation scales as \(\mathcal{O}(n^{3})\) with the number of function evaluations \(n\), which becomes the dominant factor for overall cost in the optimization process.

To reduce computational expenses, a new paradigm called _Bayesian optimization via density ratio estimation_ (BORE, [4; 16]) was introduced. BORE reformulates the improvement-based acquisition function (e.g. probability improvement, PI[10], expected improvement, EI[11]) as a problem of estimating the density ratio [15; 22] between two distributions of **x** conditioned on whether the corresponding function value \(y\) exceeds a certain threshold \(\tau\). Song et al. [13] further generalize BORE to likelihood-free Bayesian optimization (LFBO), where the acquisition function can be constructed in terms of more complex utility functions through variational representation[12]. This approach significantly reduces the computational complexity from \(\mathcal{O}(n^{3})\) to \(\mathcal{O}(n)\). However, the acquisition function itself in both BORE and LFBO still remains demanding to optimize due to:

* The involvement of neural networks typically incurs extra cumbersome numerical optimization.
* Potential overfitting in the density ratio estimate, particularly in high dimensions.

Motivated by these challenges, we present a new approach that maximizes the acquisition function by gradient ascent. The key observation is that the PI acquisition function is a likelihood function whose gradient could be estimated using score matching. Under mild regularity conditions, we utilize a simple regression model to learn the score without training any neural network. Moreover, our method maintains the computational complexity at \(\mathcal{O}(n)\).

## 2 Background

Consider the global optimization problem for a black-box function \(f\) over a compact search space \(\mathcal{X}\subset\mathbb{R}^{d}\):

\[\textbf{x}^{*}=\operatorname*{arg\,max}_{\textbf{x}\in\mathcal{X}}f(\textbf{ x}),\] (1)

where the expression and gradient of the objective function \(f:\mathcal{X}\rightarrow\mathbb{R}\) are unknown. We can only access \(f\) through a set of noisy observations \(\mathcal{D}_{N}=\{(x_{n},y_{n})\}_{n=1}^{N}\), where the evaluations \(y=g(f(\textbf{x});\epsilon)\) are corrupted by noise \(\epsilon\).

By assigning a probabilistic surrogate model (typically analytical probabilities e.g. GP [20]) to the objective function \(f\), the acquisition function \(\alpha(\textbf{x};\mathcal{D}_{N},\tau)\) is defined as the expected value of the utility function \(u(\textbf{x},y,\tau)\) under the posterior predictive \(p(y|\textbf{x},\mathcal{D}_{N})\) of \(f\) attained from GP regression [19]:

\[\alpha(\textbf{x};\tau):=\mathbb{E}_{p(y|\textbf{x},\mathcal{D}_{N})}[u( \textbf{x},y,\tau)],\] (2)

where \(\tau\) is a hyperparameter that balances exploration and exploitation. BO selects the candidate solutions by maximizing acquisition \(\textbf{x}_{t+1}=\operatorname*{arg\,max}_{\textbf{x}}\alpha(\textbf{x};\tau)\). In GP-based BO [5], most acquisition functions have closed-form solutions that are easy to optimize [21]. Nevertheless, the complexity of the GP regression is \(\mathcal{O}(n^{3})\) which limits scalability to large datasets, posing significant computational challenges in high-dimensional settings.

We focus on a particular utility function, the Probability of Improvement function (PI): \(u(\textbf{x},y,\tau):=\mathbb{I}(y-\tau\geq 0)\), where \(\tau\) is a threshold of the function value. By definition, the acquisition function is

\[\alpha(\textbf{x};\tau)=\mathbb{E}_{p(y|\textbf{x},\mathcal{D}_{N})}[\mathbb{I }(y-\tau\geq 0)]=p(y\geq\tau|\textbf{x},\mathcal{D}_{N}).\]

This acquisition function measures the probability that a new candidate point **x** yields an observation \(y\) greater than the threshold \(\tau\). \(\tau\) is typically selected as the current maximum observed function value: \(\tau=\max_{n}y_{n}\) in our observation \(\mathcal{D}_{N}\).

## 3 Methodology

First, let \(z\) denote a binary variable:

\[z:=\mathbb{I}(y\geq\tau)=\begin{cases}0,&y<\tau\\ 1,&y\geq\tau,\end{cases}\] (3)

Then, the PI acquisition function can be rewritten as \(\alpha(\textbf{x};\tau)=p(z=1|\textbf{x})\). Without any probabilistic surrogate model to the objective function \(f\), we propose to _directly_ maximize the PI acquisition function by performing _gradient ascent_ :

\[\textbf{x}_{k}=\textbf{x}_{k-1}+\eta\nabla_{\textbf{x}}\log p(z=1|\textbf{x}) |_{\textbf{x}=\textbf{x}_{k-1}}.\] (4)

### Matching the score locally

We model \(\nabla_{\textbf{x}}\log p(z=1|\textbf{x})|_{\textbf{x}=\textbf{x}_{k-1}}\) using a vector-valued function \(\boldsymbol{\beta}(z):\{0,1\}\rightarrow\mathbb{R}^{d}\):

\[\boldsymbol{\beta}(z):=\begin{cases}\boldsymbol{\beta}_{0}\in\mathbb{R}^{d},&z= 0\\ \boldsymbol{\beta}_{1}\in\mathbb{R}^{d},&z=1.\end{cases}\] (5)

We leverage the score matching techniques [8; 18] to learn the score of the acquisition function \(\nabla_{\textbf{x}}\log p(z=1|\textbf{x})\) through a least squares regression:

\[\min_{\boldsymbol{\beta}_{1}} J(\boldsymbol{\beta})=\mathbb{E}_{p(z,\textbf{x}|\textbf{x}_{k-1})} \left[\left\|\nabla_{\textbf{x}}\log p(z|\textbf{x})-\boldsymbol{\beta}(z) \right\|^{2}\right],\] (6)where \(\textbf{x}_{k-1}\) is from the previous value of **x** in the gradient ascent algorithm. The joint probability is factorized as \(p(z,\textbf{x}|\textbf{x}_{k-1})=p(z|\textbf{x})q(\textbf{x}|\textbf{x}_{k-1})\)1, where \(q(\textbf{x}|\textbf{x}_{k-1})\) is a _proposal distribution_. It samples candidate **x** around \(\textbf{x}_{k-1}\).

Footnote 1: By our construction, \(z\) and \(\textbf{x}_{k-1}\) are conditionally independent given \(\textbf{x}_{k-1}\), so \(p(z|\textbf{x},\textbf{x}_{k-1})=p(z|\textbf{x})\).

Since the target score \(\nabla_{\textbf{x}}\log p(z=1|\textbf{x})\) in the objective (6) is unknown, we cannot directly minimize \(J(\bm{\beta})\). The following theorem provides a closed-form solution of (6) and forms the basis of our algorithm:

**Theorem 1**.: _Assuming that the probability \(p(z|\textbf{x})\) and the proposal distribution \(q(\textbf{x}|\textbf{x}_{k-1})\) are differentiable. The objective (6) has the optimal solution:_

\[\bm{\beta}_{1}^{*}=\mathbb{E}_{p(\bm{x}|z=1,\textbf{x}_{k-1})} \left[\nabla_{\textbf{x}}\log p(z=1|\textbf{x})\right]=-\mathbb{E}_{p(\bm{x} |z=1,\textbf{x}_{k-1})}\left[\nabla_{x}\log q(\textbf{x}|\textbf{x}_{k-1}) \right],\] (7)

_where the second equality follows from integration by parts under mild assumptions._

Note that the minimizer \(\bm{\beta}_{1}^{*}\) in Eq. (7) is a conditional expectation of the desired score \(\nabla_{\textbf{x}}\log p(z=1|\textbf{x})|_{\textbf{x}=\textbf{x}_{k-1}}\), implying that it is a biased estimator. The following result ensures that it is asymptotically unbiased given an appropriate choice of the proposal distribution.

**Corollary 1**.: _Let the proposal distribution \(q(\textbf{x}|\textbf{x}_{k-1})\) be a normal distribution \(N(\textbf{x}|\textbf{x}_{k-1},\sigma^{2}I_{d})\). Then \(\bm{\beta}_{1}^{*}=\mathbb{E}_{p(\bm{x}|z=1,\textbf{x}_{k-1})}\left[\sigma^{- 2}(\textbf{x}-\textbf{x}_{k-1})\right]\) and_

\[\lim_{\sigma\to 0}\bm{\beta}_{1}^{*}=\nabla_{\textbf{x}}\log p(z=1| \textbf{x})|_{\textbf{x}=\textbf{x}_{k-1}}.\] (8)

Corollary 1 shows the solution to the objective (6) is tractable and, when \(\sigma\) goes to zero, provides an asymptotically unbiased estimate of the desired score. The proofs of Theorem 1 and Corollary 1 can be found in Appendix A.

### Gradient ascent algorithm and finite sample estimator

We summarize our gradient ascent algorithm in Algorithm 1 and defer the full BO algorithm together with the complexity analysis to Appendix B. We now detail the computation of \(\widehat{\bm{\beta}_{1}}\) using \(\mathcal{D}_{k}^{\prime}\), i.e., paired function evaluations and their input values. According to Corollary 1, \(\bm{\beta}_{1}^{*}=\int p(\textbf{x}|z=1,\textbf{x}_{k-1})\left[\sigma^{-2}( \textbf{x}-\textbf{x}_{k-1})\right]d\textbf{x}\). Once the threshold \(\tau\) is determined, we can approximate the above integral using a Monte Carlo estimator:

\[\widehat{\bm{\beta}_{1}}=\begin{cases}\frac{\sum\left[\sigma^{-2} (\textbf{x}^{(m),1}-\textbf{x}_{k-1})\right]}{\sum_{m=1}^{M}z^{(m)}},&\sum_{m= 1}^{M}z^{(m)}>0\\ 0,&\sum_{m=1}^{M}z^{(m)}=0\end{cases},\] (9)

where \(\textbf{x}^{(m),1}\) is short for \(\textbf{x}^{(m)}|z^{(m)}=1\). If \(z^{(m)}=0\) for all \(1\leq m\leq M\) we simply set \(\widehat{\bm{\beta}_{1}}=0\). Intuitively, \(\widehat{\bm{\beta}_{1}}\) can be interpreted as an average of \((\textbf{x}-\textbf{x}_{k-1})/\sigma^{2}\) using _local information_ that is \(\{(\textbf{x}^{(m)},z^{(m)})\}_{m=1}^{M}\) sampled within a neighborhood of \(\textbf{x}_{k-1}\) with radius determined by \(\sigma\).

## 4 Experiments

Since we estimate the gradient of the acquisition function, we can resort to first-order optimizers such as ADAM [9] and RMSProp [17] to maximize the acquisition function.

Note that Corollary 1 suggests that \(\sigma\) should be small near the end of gradient ascent. However, since \(\sigma\) controls the local survey region as well, we propose a linear annealing schedule to balance this trade-off.

\[\sigma_{t}^{2}=\sigma_{0}^{2}\cdot\max\left(0,1-\frac{t-0.1}{T}\right)\]

We quantitatively evaluate the performance of our method by benchmarking against some standard black-box optimization problems, fixing a function evaluation budget of \(250\). Following LFBO [13], we report the _immediate regret_ as a metric, which measures the distance of the current best function evaluation from the optimum function evaluation. The shaded regions represent the mean plus and minus one standard deviation across 10 different seeds. We compare against the neural-network based LFBO approach, which despite using a different acquisition function, is a similar approach to our work that does not require inference of a surrogate model. We also provide the results using a random-search algorithm as a reference. From these results, we observe that our local score matching method (SM) is broadly competitive with the LFBO method. It outperforms LFBO in higher dimensions in the Rosenbrock experiment when the neural network estimation in LFBO is likely to struggle. In this work, we focus on preliminary investigation of LSM method via synthetic datasets with mild dimensions. Experiments on high-dimensional, real-world datasets are important future works.

## 5 Discussions and Future Works

We propose an optimization scheme that maximizes the PI acquisition function bypassing the GP regression restriction. Despite the promising performance of our method, there are still some limitations. As the gradient information is myopic, our algorithm may not achieve the global optimum

Figure 1: Immediate regret for the Rosenbrock function, repeated over 10 different seeds

Figure 2: Immediate regret for the Rastrigin function, repeated over 10 different seedsof the acquisition function. In addition, our method is restricted to PI acquisition function since our score matching objective can only be used to estimate the gradient of a likelihood function. Without a likelihood expression, other acquisition functions cannot be readily estimated through our procedure. An interesting avenue for future work would be to generalize our work to a broader class of acquisition functions. Finally, our method requires additional evaluations of the blackbox function for estimating the gradient of the utility function at each gradient iteration.

We also notice that our algorithm is very similar to Covariance Matrix Adaptation - Evolutionary Strategy (CMA-ES, [7]). Further investigations into the relationship between these two methods could be a promising direction.

## References

* Alarie et al. [2021] Stephane Alarie, Charles Audet, Aimen E Gheribi, Michael Kokkolaras, and Sebastien Le Digabel. Two decades of blackbox optimization applications. _EURO Journal on Computational Optimization_, 9:100011, 2021.
* Auger et al. [2012] Anne Auger, Nikolaus Hansen, and Marc Schoenauer. Benchmarking of continuous black box optimization algorithms, 2012.
* Balandat et al. [2020] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization. _Advances in neural information processing systems_, 33:21524-21538, 2020.
* Bergstra et al. [2011] James Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl. Algorithms for hyper-parameter optimization. _Advances in neural information processing systems_, 24, 2011.
* Frazier [2018] Peter I Frazier. A tutorial on bayesian optimization. _arXiv preprint arXiv:1807.02811_, 2018.
* Garnett [2023] Roman Garnett. _Bayesian optimization_. Cambridge University Press, 2023.
* Hansen et al. [2003] Nikolaus Hansen, Sibylle D Muller, and Petros Koumoutsakos. Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es). _Evolutionary computation_, 11(1):1-18, 2003.
* Hyvarinen and Dayan [2005] Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* Kingma [2014] Diederik P Kingma. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Kushner [1964] Harold J Kushner. A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. 1964.
* Mockus [1974] Jonas Mockus. On bayesian methods for seeking the extremum. In _Proceedings of the IFIP Technical Conference_, pages 400-404, 1974.
* Nguyen et al. [2010] XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. _IEEE Transactions on Information Theory_, 56(11):5847-5861, 2010.
* Song* et al. [2022] Jiaming Song*, Lantao Yu*, Willie Neiswanger, and Stefano Ermon. A general recipe for likelihood-free bayesian optimization. In _International Conference on Machine Learning_, 2022.
* Song and Ermon [2019] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* Sugiyama et al. [2012] Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. _Density ratio estimation in machine learning_. Cambridge University Press, 2012.
* Tiao et al. [2021] Louis C Tiao, Aaron Klein, Matthias W Seeger, Edwin V Bonilla, Cedric Archambeau, and Fabio Ramos. Bore: Bayesian optimization by density-ratio estimation. In _International Conference on Machine Learning_, pages 10289-10300. PMLR, 2021.

* [17] T Tieleman. Lecture 6.5-rmsprop. _COURSERA: Neural Networks for Machine Learning_, 2012.
* [18] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674, 2011.
* [19] Christopher Williams and Carl Rasmussen. Gaussian processes for regression. _Advances in neural information processing systems_, 8, 1995.
* [20] Christopher KI Williams and Carl Edward Rasmussen. _Gaussian processes for machine learning_, volume 2. MIT press Cambridge, MA, 2006.
* [21] James Wilson, Frank Hutter, and Marc Deisenroth. Maximizing acquisition functions for bayesian optimization. _Advances in neural information processing systems_, 31, 2018.
* [22] Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, and Masashi Sugiyama. Relative density-ratio estimation for robust distribution comparison. _Advances in neural information processing systems_, 24, 2011.

## Appendix A Proofs

For ease of notation, here we denote \(z_{1}\) and \(z_{0}\) when \(z=1\) and \(z=0\) respectively.

### Proof of Theorem 1

Proof.: Note that \(\bm{\beta}(z)\) is a function that _only_ depends on \(z\). Unrolling Eq. (6) by factorizing the joint probability in two conditional density:

\[J(\bm{\beta}) =\mathbb{E}_{p(z,\mathbf{x}|\mathbf{x}_{k-1})}\left[\left\| \nabla_{\mathbf{x}}\log p(z|\mathbf{x})-\bm{\beta}(z)\right\|^{2}\right]=J_{1 }(\bm{\beta}_{1})+J_{0}(\bm{\beta}_{0})\] (10) \[=\underbrace{p(z_{1}|\mathbf{x}_{k-1})\int p(\mathbf{x}|z_{1}, \mathbf{x}_{k-1})\left[\left\|\nabla_{\mathbf{x}}\log p(z_{1}|\mathbf{x})- \bm{\beta}_{1}\right\|^{2}\right]d\mathbf{x}}_{J_{1}(\bm{\beta}_{1})}\] (11) \[\qquad+\underbrace{p(z_{0}|\mathbf{x}_{k-1})\int p(\mathbf{x}|z_{ 0},\mathbf{x}_{k-1})\left[\left\|\nabla_{\mathbf{x}}\log p(z_{0}|\mathbf{x})- \bm{\beta}_{0}\right\|^{2}\right]d\mathbf{x}}_{J_{0}(\bm{\beta}_{0})}\] (12) \[=p(z_{1}|\mathbf{x}_{k-1})\left[\left\|\bm{\beta}_{1}\right\|^{2 }-2\left\langle\bm{\beta}_{1},\nu_{z}(\mathbf{x})\right\rangle\right]+C+J_{0} (\bm{\beta}_{0}),\] (13)

where \(\nu_{z_{1}}(\mathbf{x})=\mathbb{E}_{p(\mathbf{x}|z_{1},\mathbf{x}_{k-1})} \left[\nabla_{\mathbf{x}}\log p(z_{1}|\mathbf{x})\right]\) and:

\[C=\mathbb{E}_{p(\mathbf{x}|z_{1},\mathbf{x}_{k-1})}\left[\left\|\log p(z_{1}| \mathbf{x})\right\|^{2}\right]\] (14)

is a constant. Take partial derivate w.r.t \(\bm{\beta}_{1}\) on Eq. (13) and set it to zero, after simplifying one can recognize the optimal solution:

\[\bm{\beta}_{1}^{*}=\nu_{z_{1}}(\mathbf{x})=\mathbb{E}_{p(\mathbf{x}|z_{1}, \mathbf{x}_{k-1})}\left[\nabla_{\mathbf{x}}\log p(z_{1}|\mathbf{x})\right].\] (15)

Given \(p(z_{1}|\mathbf{x})\) and \(q(\mathbf{x}|\mathbf{x}_{k-1})\) is differentiable and under mild assumption 2, one has:

Footnote 2: Recall \(z\) and \(\mathbf{x}_{k-1}\) are conditionally independent given \(\mathbf{x}\) and this assumption of independence \(p(z=1|\mathbf{x})=p(z=1|\mathbf{x},\mathbf{x}_{k-1})\) is also used in the proof of Corollary 1.

\[\bm{\beta}_{1}^{*} =\mathbb{E}_{p(\mathbf{x}|z_{1},\mathbf{x}_{k-1})}\left[\nabla_{ \mathbf{x}}\log p(z_{1}|\mathbf{x})\right]\] (16) \[=\int\frac{p(z_{1}|\mathbf{x};\mathbf{x};\mathbf{x}_{k-1})q( \mathbf{x}|\mathbf{x}_{k-1})}{p(z_{1}|\mathbf{x}_{k-1})}\cdot\frac{\nabla_{ \mathbf{x}}p(z_{1}|\mathbf{x})}{p(z_{1}|\mathbf{x})}d\mathbf{x}\] (17) \[\overset{(*)}{=}-\int\frac{p(z_{1}|\mathbf{x})}{p(z_{1}|\mathbf{ x}_{k-1})}\nabla_{\mathbf{x}}q(\mathbf{x}|\mathbf{x}_{k-1})d\mathbf{x}\] (18) \[=-\int\frac{p(z_{1}|\mathbf{x},\mathbf{x}_{k-1})q(\mathbf{x}| \in which \((*)\) uses integration by parts with regularity condition:

\[\lim_{|x_{i}|\rightarrow\infty}q(\textbf{x}|\textbf{x}_{k-1})p(z_{1}|\textbf{x})=0,\] (21)

holds in every dimension \(x_{i},\;i=1,\ldots,d\) of **x**. 

### Proof of Corollary 1

Proof.: Rewrite \(\bm{\beta}_{1}^{*}\) using Bayesian rule:

\[\bm{\beta}_{1}^{*} =\mathbb{E}_{p(\textbf{x}|z_{1},\textbf{x}_{k-1})}\left[\nabla_{ \textbf{x}}\log p(z_{1}|\textbf{x})\right]\] (22) \[=\int\frac{p(z_{1}|\textbf{x},\textbf{x}_{k-1})q(\textbf{x}| \textbf{x}_{k-1})}{p(z_{1}|\textbf{x}_{k-1})}\nabla_{\textbf{x}}\log p(z_{1}| \textbf{x})d\textbf{x}.\] (23)

Now we set the proposal distribution \(q(\textbf{x}|\textbf{x}_{k-1})=N(\textbf{x}|\textbf{x}_{k-1},\sigma^{2}I_{d})\) as Gaussian, whose limit distribution is is the Dirac delta function centered on \(\textbf{x}_{k-1}\) as \(\sigma\to 0\). Under the same assumption in Proof of Theorem 1, we now have:

\[\bm{\beta}_{1}^{*} \rightarrow\frac{p(z_{1}|\textbf{x})}{p(z_{1}|\textbf{x}_{k-1})} \nabla_{\textbf{x}}\log p(z_{1}|\textbf{x})\Bigg{|}_{\textbf{x}=\textbf{x}_{k -1}}\] (24) \[=\nabla_{\textbf{x}}\log p(z_{1}|\textbf{x})|_{\textbf{x}= \textbf{x}_{k-1}}\] (25)

when \(\sigma\to 0\). 

## Appendix B The complete BO algorithm

```
0: Generator \(y=g(\textbf{x};\epsilon)\), hyperparameters \(\sigma,\;\eta\), budget \(T,K,M\), observations \(\mathcal{D}_{N}\)
1:\(\mathcal{D}_{t=0}\leftarrow\mathcal{D}_{N}\) (or initialize \(\textbf{x}_{t=0}\), simulate \(y_{t=0}\gets g(\textbf{x}_{t=0};\epsilon)\), \(\mathcal{D}_{t=0}\leftarrow\{(\textbf{x}_{t=0},y_{t=0})\}\) if no observations at beginning)
2:while\(1\leq t\leq T\)do
3:\(\tau\leftarrow\max_{y}\) in \(\mathcal{D}_{t-1}\)\(\triangleright\) Set the threshold
4:\(\textbf{x}_{k=0}\leftarrow\textbf{x}_{t-1}\)\(\triangleright\) initializing \(\textbf{x}_{k-1}\)
5: Do Algorithm 1
6:\(\textbf{x}_{t}\leftarrow\textbf{x}_{K}\)\(\triangleright\) Cadidate solution
7:\(y_{t}\gets g(\textbf{x}_{t};\epsilon)\)\(\triangleright\) Evaluate black-box function
8:\(\mathcal{D}_{t}\leftarrow\mathcal{D}_{t-1}\cup_{k=1}^{K}\mathcal{D}^{\prime}_{k} \cup(\textbf{x}_{t},y_{t})\)\(\triangleright\) Update dataset
9:\(t\gets t+1\)
10:endwhile ```

**Algorithm 2** Bayesian Optimization via Local Score Matching

Given \(T,K,M\), the total number of evaluations required in our algorithm is \(n=T\times(K\times M+1)\). The complexity is thus \(\mathcal{O}(TKM)\) or equivalently \(\mathcal{O}(n)\).

Theoretically, \(\textbf{x}_{t-1}\) can be initialized to any location \(\textbf{x}_{1:N}\) in \(\mathcal{D}_{N}\). Experimentally, we set it as the **x** of which corresponding evaluation is \(\max y,y\in\mathcal{D}_{N}\) by the greedy heuristic.

## Appendix C Experimental Details

For all algorithms, we initialized the dataset with \(4\) initial points, which we do not count in our function evaluations, as is necessary for the LFBO algorithm. This initial dataset is the same across all algorithms. For the random search algorithm, the optimal point and function evaluation is initialized from this dataset.

For our local score matching algorithm, the Adam optimizer is used for the maximizing of the acquisition function. We used a fixed step-size of \(5\cdot 10^{-1}\) throughout all experiments.3 We used a budget of \(T=5,K=5,M=10\), which results in an overall function evaluation budget of \(255\), although only the first \(250\) function evaluations are shown in Figures 1 and 2.

For the random search algorithm, we sampled uniformly with bounds of \([-5,5]\) across all dimensions, for both experiments. We sampled \(10\) points, over a total of \(25\) iterations, for a total of \(250\) function evaluations in total.

For the LFBO algorithm, we broadly followed the specifications provided by code in the original paper [13]. We used a two layer Neural Network, with 32 hidden units, which is optimized with the Adam optimizer with a fixed learning rate of \(10^{-3}\). The acquisition function was optimized with the botorch package [3] in Python.