# Stress-Testing Long-Context Language Models

with Lifelong ICL and Task Haystack

 Xiaoyue Xu

Tsinghua University

xiaoyue.xu.me@gmail.com &Qinyuan Ye

University of Southern California

qingyuany@usc.edu &Xiang Ren

University of Southern California

xiangren@usc.edu

Equal Contribution.

###### Abstract

We introduce Lifelong ICL, a problem setting that challenges long-context language models (LMs) to learn a sequence of language tasks through in-context learning (ICL). We further introduce Task Haystack, an evaluation suite dedicated to assessing and diagnosing how long-context LMs utilizes contexts in Lifelong ICL. When given a task instruction and test inputs, long-context LMs are expected to leverage the relevant demonstrations in the Lifelong ICL prompt, avoid distraction and interference from other tasks, and achieve test accuracies that are not significantly worse than those of the Single-task ICL baseline.

Task Haystack draws inspiration from the widely-adopted "needle-in-a-haystack" (NIAH) evaluation, but presents distinct new challenges. It requires models (1) to utilize the contexts at a deeper level, rather than resorting to simple copying and pasting; (2) to navigate through long streams of evolving topics and tasks, proxying the complexities and dynamism of contexts in real-world scenarios. Additionally, Task Haystack inherits the controllability of NIAH, providing model developers with tools and visualizations to identify model vulnerabilities effectively.

We benchmark 14 long-context LMs using Task Haystack, finding that frontier models like GPT-4o still struggle with the setting, failing on 15% of cases on average. Most open-weight models further lack behind by a large margin, with failure rates reaching up to 61%. In our controlled analysis, we identify factors such as distraction and recency bias as contributors to these failure cases. Further, performance declines when task instructions are paraphrased at test time or when ICL demonstrations are repeated excessively, raising concerns about the robustness, instruction understanding, and true context utilization of long-context LMs. We release our code and data to encourage future research that investigates and addresses these limitations.1

Footnote 1: [https://inklab.usc.edu/lifelong-icl](https://inklab.usc.edu/lifelong-icl)

## 1 Introduction

Recent advances in model architecture [14, 23], hardware-aware optimization [24, 15], training procedure [21, 13], and data engineering [22, 16] have enabled large language models (LLMs) to handle extended contexts, reaching up to 32 thousand tokens or even millions [11]. These advancements have opened up new opportunities and potential usecases for LLMs. However, while long-context LM development strides forward, effective evaluation methods have not kept pace. Systematically evaluating long-context LMs' ability to leverage such long contexts remains an open challenge.

Current evaluation approaches fall into two major categories. The first involves constructing benchmarks with real-world long-context tasks [20, 21]. While valuable, creating these benchmarks is time-consuming and particularly challenging when scaling the input context length to millions of tokens. The second approach employs synthetic evaluations like the "needle-in-a-haystack" (NIAH) test [17] or key-value retrieval tests [19]. For example, in the NIAH evaluation, a piece of information ("The special magic number is 12345") is planted in a haystack of irrelevant contexts (Paul Graham essays; Graham 2024) and the model is evaluated on answering a question about the information ("What's the special magic number?"). Although useful for initial assessment, these tests primarily measure simple copying-and-pasting capabilities and fail to capture whether models are able to utilize the context at a deeper level.

In this work, we offer new perspectives to long-context LM evaluation by introducing Lifelong ICL, a new problem setting that challenges these models to learn a sequence of tasks via in-context learning (ICL). Further, we introduce Task Haystack, an accompanying evaluation suite designed for systematic diagnosis of context utilization (Fig. 1). In Task Haystack, a long-context LM will be evaluated on a collection of tasks, with Lifelong ICL prompts and Single-task ICL prompts respectively. A model "passes" the test if its accuracies with Lifelong ICL prompts are not significantly lower than when using Single-task ICL prompts. The overall pass rate, averaged across tasks and different lifelong stream permutations, serves as the key metric of Task Haystack.

Task Haystack presents unique challenges not fully covered by existing benchmarks. Firstly, Task Haystack requires deeper understanding of the relevant context for accurate predictions. This goes beyond simple retrieval capabilities tested by NIAH-style benchmarks, which often rely on basic copying and pasting. Secondly, Task Haystack features high information density, meaning that every piece of information in the context might be crucial for successful prediction at test time. This differs from evaluation suites in which the important information ("needle") is positioned conspicuously, allowing models to exploit shortcuts [18]. Thirdly, existing benchmarks fall short in capturing the dynamics of shifting topics within the context [21], which can pose challenges in real-world applications of long-context models--such as a 24/7 personal assistant that must resume previous conversations amid a long, evolving stream of topics. While not fully realistic, Task Haystack serves as a useful proxy for evaluating this aspect.

We extensively evaluate 14 long-context models on Task Haystack. While all models achieve near-perfect scores on the original NIAH test, none reach satisfactory performance on our proposed evaluation. Among the compared models, GPT-4o and Gemini-1.5-Flash lead with an average pass rate of 85%, significantly outperforming most open-weight models. Llama-3.1-70B, the best-performing open-weight model, follows closely with an average pass rate of 80%. To understand the root causes behind these failure cases, we conduct controlled experiments that isolate factors like recency bias (models favoring information at the end of the context) and distractability (models

Figure 1: **Lifelong ICL and Task Haystack.** Lifelong ICL presents long-context LMs with a sequence of tasks, each containing a task instruction and a few demonstrations. At test time, the model is given a previously seen task instruction and then makes predictions on the test input directly. A long-context LM “passes” the Task Haystack test when its accuracies in Lifelong ICL (Task 1+2+3) are not significantly worse than accuracies of the Single-task ICL baseline (Task 2 only).

getting distracted by irrelevant information). The results confirm that both factors contribute to performance degradation on Task Haystack. Additionally, we find that model performance declines when instructions are paraphrased at test time and when few-shot ICL demonstrations of a single task are repeated multiple times. These observations highlight the limitations of current long-context LMs in terms of robustness, instruction understanding, and context utilization.

We hope that Lifelong ICL and Task Haystack serve as useful resources and testbeds for evaluating, diagnosing, and understanding long-context LMs. Further, we anticipate that the limitations and vulnerabilities exposed in this paper will inspire innovations in long-context LM development.

## 2 Related Work

Long-Context LM Evaluation.Early studies on long-context modeling primarily rely on perplexity-based evaluations (Beltagy et al., 2020; Press et al., 2022). Subsequent research has indicated that such evaluation is limited in reflecting a model's effectiveness in downstream applications (Sun et al., 2021; Hu et al., 2024). Recent efforts have led to the development of comprehensive benchmarks for evaluating long-context models, which can be divided into realistic and synthetic categories. Realistic benchmarks, exemplified by (Zero)SCROLLS (Shaham et al., 2022, 2023), comprise tasks that require processing long inputs collected from real-world scenarios. These tasks are typically sourced from established datasets and include various task types such as summarization and question answering, or developed from inherently lengthy corpus such as novel (Zhang et al., 2024), grammar books (Tanzer et al., 2024) and code repository (Jimenez et al., 2024). In the category of synthetic benchmarks, the needle-in-a-haystack (NIAH) (Kamradt, 2023) evaluation is widely adopted for evaluating context utilization (Gemini Team, 2024; Anthropic, 2024; Liu et al., 2024; Fu et al., 2024; Levy et al., 2024, _i.a._). Ruler (Hsieh et al., 2024) expands on the NIAH test with multi-key and multi-value retrieval, and adds two new tasks that involve multi-hop tracing and aggregation. Hybrid benchmarks is a middle-field that incorporate both realistic and synthetic elements. An example is LongBench (Bai et al., 2024), which includes synthetic tasks based on realistic text, such as counting unique passages appearing in the context. Our proposed Task Haystack can be seen as a hybrid benchmark, with a realistic touch as (1) it is built upon realistic language tasks; (2) it proximates the challenge of navigating through evolving topics and tasks.

Evaluating Long-Context LMs with Many-Shot ICL.Several recent works have explored incontest learning with long-context LMs by scaling the number of training examples (_i.e._, shots). Bertsch et al. (2024) conducted a systematic study of long-context ICL with up to 2,000 shots, demonstrating many-shot ICL as a competitive alternative to retrieval-based ICL and fine-tuning. Additionally, it offers the advantage of caching demonstrations at inference time, unlike instance-level retrieval methods. While Bertsch et al. (2024) focus on classification tasks, Agarwal et al. (2024) showed the effectiveness of many-shot ICL on generative and reasoning tasks, and established new state-of-the-art results on practical applications such as low-resource translation with the Gemini 1.5 Pro model. However, there are still limitations to many-shot ICL. Li et al. (2024) introduce LongICLBench, a suite of 6 classification tasks with many (20+) classes, and find that current long-context LMs still struggle with these tasks. Orthogonal to this line of work on scaling _number of examples_ for one single task, we focus on scaling the _number of tasks_ in our Lifelong ICL setting.

Lifelong Learning in NLP.Lifelong learning, or continual learning, refers to the problem setting where a model learns continuously from data streams (Biesialska et al., 2020; Shi et al., 2024). Lifelong ICL is largely inspired by this line of work and challenges long-context models to learn continuously from a sequence of language tasks. However, unlike prior works that use gradient-based fine-tuning (de Masson d'Autume et al., 2019; Jin et al., 2021; Scialom et al., 2022; Mehta et al., 2023), Lifelong ICL is a new exploration that uses in-context learning as the underlying "learning" algorithm. It also stands out from Coda-Forno et al. (2023) and Ye et al. (2024) by focusing on evaluating long-context LMs and scaling the input length from 4k to up to 32k tokens. A primary challenge in lifelong learning is catastrophic forgetting, the tendency of a model to forget previously acquired tasks upon learning new tasks (Kirkpatrick et al., 2017). Our proposed Task Haystack evaluation focuses an analogous phenomenon, as the model may struggle to recall earlier information in a lengthy context, leading to a performance decline.

Problem Setting

In the following, we establish the notations and the problem setting of Lifelong ICL in SS3.1. We will begin by defining notations of in-context learning (ICL) of _one single task_\(T\). We will then build upon these foundations and introduce Lifelong ICL with _a collection of tasks_\(\mathcal{T}\). In SS3.2, we further introduce our Task Haystack evaluation protocol, provide the definition of the key metric named "pass rate," and describe our strategies to account for the instabilities in ICL experiments.

### Lifelong ICL

In-context Learning.In-context learning is a method that adapts LMs to perform a language task by providing prompts containing input-output pairs (Brown et al., 2020). In this paper, we define a language task \(T\) as a tuple of \((D^{train},D^{test},d)\), where \(D^{train}\) is the training set, \(D^{test}\) is the test set, \(d\) is a textual task description (_i.e._, instruction). We first create a task-specific prompt \(p\) by concatenating the task description and the \(k\)-shot examples in \(D^{train}\), _i.e._, \(p=d\oplus x_{1}^{train}\oplus y_{1}^{train}\oplus\dots\oplus x_{k}^{train} \oplus y_{k}^{train}\). Then, to make a prediction on the test input \(x^{test}\), we concatenate the task-specific prompt and the test input (_i.e._, \(p\oplus x^{test}\)), and query the language model \(\mathtt{LM}\) to generate the prediction \(\hat{y}\). We denote this process as \(\hat{y}=\mathtt{LM}(x^{test}|p)\) to highlight that the prediction is made by conditioning on the task-specific prompt \(p\).

Task Collection and Task Permutation.The definition above introduces how ICL is performed with one single task \(T\). In Lifelong ICL, an LM is expected to learn from a collection of \(n\) tasks, denoted as \(\mathcal{T}=\{T_{i}\}_{i=1}^{n}\). To enable this, we first create a random permutation \(a=(a_{1},a_{2},\dots,a_{n})\), thus the tasks in \(\mathcal{T}\) will be ordered as \((T_{a_{1}},T_{a_{2}},\dots,T_{a_{n}})\). For example, when \(n=3\), one possible permutation \(a\) is \((3,1,2)\), so that the tasks are ordered as \((T_{3},T_{1},T_{2})\).

Lifelong ICL.Given a permutation \(a\), we first create the task-specific prompt \(p_{a_{i}}\) for each task \(T_{a_{i}}\), and then create the Lifelong ICL prompt \(p_{l}\) by concatenating all task-specific prompts, _i.e._, \(p_{l}=p_{a_{1}}\oplus p_{a_{2}}\oplus\dots\oplus p_{a_{n}}\). At test time, for _each_ task \(T_{a_{i}}\) in \(\mathcal{T}\), the model will be queried to perform generate the prediction as \(\hat{y}=\mathtt{LM}(x_{test}|p_{l}\oplus d_{a_{i}})\). Note that we append the task description \(d_{a_{i}}\) after the Lifelong ICL prompt \(p_{l}\) at test time, to ensure the model is informed of the task at hand. See Fig. 1 for an illustrative example with 3 tasks.

### Task Haystack

Evaluation Principle.For a test task \(T_{a_{i}}\), we anticipate that long-context LMs can effectively utilize the in-context examples of that task, _i.e._, \(p_{a_{i}}\), which is a substring of the Lifelong ICL prompt \(p_{l}\oplus d_{a_{i}}\). To evaluate this, we compare the model performance on task \(T_{a_{i}}\) when conditioning on \(p_{l}\oplus d_{a_{i}}\) and \(p_{a_{i}}\), and expect the former to be not significantly worse than the latter. In other words, the Single-task ICL prompt \(p_{a_{i}}\) is the "needle" in the Lifelong ICL prompt \(p_{l}\) (_i.e._, the "task haystack").2

Footnote 2: While the _absolute_ performance in Lifelong ICL will be influenced by various factors, such as the LM’s core capabilities, its parametric knowledge, the prompt template, or the selection of ICL examples, it is reasonable to make a _comparative_ assumption that the performance of Lifelong ICL should not be worse than Single-task ICL for the same model. Additionally, since the Single-task ICL prompt \(p_{a_{i}}\) is a substring of the Lifelong ICL \(p_{l}\), the quality of the ICL examples are controlled to be the same.

Addressing ICL Instability with Multiple Runs.One challenge in Task Haystack evaluation is the notorious instability of ICL. To account for this, our experiments will be carried out with 5 random samples of the permutation \(a\) and 5 randomly-sampled few-shot training set \(D_{train}\) for each task. This allows us to obtain a performance matrix of size \((t,p,r)\) for Lifelong ICL, where \(t\) is the task index, \(p\) is the permutation index, and \(r\) is the few-shot sample index.3 We will also obtain a matrix of size \((t,r)\) for the Single-task ICL baseline.

Footnote 3: In a Task Haystack of 16 tasks, we run 16*5*5=400 experiments and obtain 400 performance metrics.

Evaluation Metrics.For an **overall measurement**, we introduce an **overall pass rate**. For each permutation \(a\) and each task \(T_{a_{i}}\), we will get two groups of performance metrics, when using Single-task ICL and Lifelong ICL respectively. Each group contains 5 metrics, corresponding to the 5 randomly-sampled few-shot training set \(D_{train}\). The model passes the test (_i.e._, scores 1) when the the Lifelong ICL group is not significantly worse than the Single-task ICL group, captured by a two-sided t-test with \(p=0.05\). The model scores 0 otherwise. The overall pass rate will be computed by averaging the scores over the different permutations and tasks. We provide more details of the definition and discuss its limitations in SSA.3.

For a **fine-grained analysis**, our experiment results allow us to **visualize the pass rates grouped by the position in the task stream, by the task, or by the task permutation**. This enables straightforward visualizations as popularized by the needle-in-a-haystack test, providing an convenient tool to diagnose and uncover the vulnerabilities of long-context LMs. See Fig. 24 for an example.

## 4 Experiment Details

Task Selection.While the problem setting in SS3 is generic and admits any language task, in this work we instantiate the setting with a narrower task distribution for initial exploration. Our key considerations include:4

Footnote 4: We discuss the limitations of these design choices in §6. We invite future work to improve upon our work and address these limitations.

* We focus on classification tasks, as they allow standardized evaluation. Additionally, a large body of past work investigates ICL empirically or mechanistically using classification tasks [16, 17, 18, 19, 20, 21].
* We select classification tasks with fewer than 20 categories and input text shorter than 1000 tokens, to avoid excessively long single-task prompts that dominate the whole context window [15].
* We focus on English tasks, since most long-context LMs are not optimized for multilingual usage.

After careful manual selection, we obtain a collection of 64 classification tasks, covering a wide range of domains and label spaces. We provide a snippet of 16 tasks in Table 1 and provide detailed descriptions of all 64 tasks, including their references and license information, in Table 6.

Models.We evaluate eleven open-weight long-context LMs on Task Haystack: Mistral-7B (32k) [18], FILM-7B (32k) [17], Llama-2-7B (32k) [17], Llama-2-7B (80k) [16], Llama-3-8B/70B (1048k) [15, 14], Llama-3-1-70B (128k) [18], Yi-6B/9B/34B (200k) [19, 20], and Command-R-35B (128k) [17]. These models represent various long-context modeling techniques, model size, and base pre-trained models. Additionally, we evaluate three closed models, GPT-3.5-Turbo (16k) and GPT-4o (128k) from OpenAI, and Gemini-1.5-Flash (1048k) from Google DeepMind [13]. We provide the detailed descriptions of these models in Table 5 in SSA.1.

Controlling the Context Length.We consider creating long contexts with two strategies: **(1) Scale-Shot**: scaling the number of in-context examples (\(n_{shot}\)); **(2) Scale-Task**: scaling the number of tasks (\(n_{task}\)). In the first setting, we fix \(n_{task}=16\) and experiment with \(n_{shot}\in\{1,2,3,4,5,6,7,8\}\). We use the 16 tasks listed in Table 1 in the main body of the paper.5 In the second setting, we fix \(n_{shot}=2\) and experiment with \(n_{task}\in\{8,16,24,32,40,48,56,64\}\). Note that to ensure the in-context examples are balanced and every class is covered, \(n_{shot}=2\) refers to using 2 examples _per class_ for in-context learning. In both scaling settings, we are able to effectively create contexts of sizes ranging from 4k to 32k tokens.6

Footnote 5: Following reviewer feedback, we create a separate subset of 16 tasks with permissive licenses, and report the results on selected models in §B.2. We recommend using this subset for future benchmarking and analysis.

Footnote 6: It is possible to further increase the context length, _e.g._, reaching 128k tokens with 64 tasks and 8 shots. Due to compute constraints, we limit the context length to 32k in this work.

We defer additional implementation and engineering details in SSA.4.

\begin{table}
\begin{tabular}{l|l|l|l} \hline \hline emo & covid\_fake\_news & logical\_fallacy\_detection & dbpedia\_14 \\ amazon\_massive\_scenario & news\_data & semeval\_abs\_restaurant & amazon\_counterfactual\_en \\ bra\_action & boolq & this\_is\_not\_a\_dataset & insi\_ence\_questions \\ clickbait & yahoo\_answers\_topics & pun\_detection & wiki\_qa \\ \hline \hline \end{tabular}
\end{table}
Table 1: **A Snippet of 16 tasks used in our experiments.** See Table 6 for the full list of 64 tasks. The 16 tasks in this table are used for the Scale-Shot experiments in Table 2.

[MISSING_PAGE_EMPTY:6]

[MISSING_PAGE_FAIL:7]

Anthropic, 2024]. However, the improvements only close about half the gap between Baseline and Recall, suggesting that recency bias contributes to but does not fully explain the performance gap.

(b) Distraction.We examine the effect of irrelevant context, by contrasting Baseline with Random. The results indicate that prepending an irrelevant long text will influence the performance negatively, which corroborates with recent work investigating the robustness of language models [Levy et al., 2024]. Further, Replay can be seen as prepending a long prefix of mostly irrelevant tasks before performing Single-task ICL (Baseline), and thus the gap between Replay and Baseline may be interpreted as caused by prepending irrelevant contexts.

(c) Long-context Input.We further compare Baseline, Random, Repeat settings altogether, where Random introduces _irrelevant_ context and Repeat includes only _relevant_ context. Perhaps surprisingly, performance drops in the Repeat setting (-1.3% for Mistral-7B and -3.1% for FILM-7B), where both distractions and recency biases are absent. This observation raises concerns on whether longer inputs are more likely to trigger failure modes and give rise to undesired behaviors in general. While more evidence is needed to derive a conclusion, we suggest that long-context LM users be cautious about including everything in the context window, and we recommend using external filtering or retrieval models when necessary.

Dependency on Task Instructions and ICL Demonstrations.In the Remove setting, we remove the task instruction and the ICL examples of the test task from the Lifelong ICL prompt, to investigate whether the models are relying on such information. We observe a clear performance drop in the Remove setting (-3.3% for Mistral-7B and -4.1% for FILM-7B compared to Recall), suggesting that the models are able to locate and make use of the "needle" to some extent in the Recall setting, but not doing it precisely so that the performance can match with the Single-task ICL baseline.

The Paraphrase setting further allows us to explore how models make use of task instructions. We observe a decline in performance in the Paraphrase setting compared to Recall. This confirms that the models locate the "needle" by retrieving identical instructions in the context. However, the performance gap indicates that models mainly rely on pattern matching rather than deeper understanding of the instructions, which might limit their broader utility in practical applications.

Repeated ICL as "Multi-epoch" ICL.We conduct further investigation with the Random, Repeat, Repeat+Shuffle setting, by varying the size of the context and the number of repetitions. Results are reported in Fig. 5. Interestingly, model performance first increases and then dips when running in-context learning for multiple "epochs." One direct takeaway is that repeating the ICL examples multiple times can potentially improve performance, which may have practical utilities in certain low-data high-inference-budget regimes. However, model performance starts to degrade after repeating more than 8 times. This phenomenon can be interpreted in two ways: (1) It is a known issue that repetition may lead to model degeneration [Nasr et al., 2023]; Repeat+Shuffle can possibly alleviate this issue by introducing slight variations in each repeat, which explains why Repeat+Shuffle outperforms Repeat in general. (2) It is also possible that the model "overfits" to the few-shot training data after multiple "epochs", analogous to the common observations in gradient-based fine-tuning. We invite future work to investigate the working mechanism of ICL in this "multi-epoch" setting.

### Additional Observations and Analysis

Tasked learned via ICL are more easily influenced.While examining Task Haystack results, we find that the passing and failing behaviors are highly task-specific. For example, in Fig. 24, Mistral-7B (32k) fails on news_data and insincere_questions in all permutations, meanwhile passes on more popular tasks like boolq and yahoo_answer_topics. We hypothesize that models may have memorized some of the tasks during pre-training or post-training, making these tasks less subjective to performance drop in Lifelong ICL. Alternatively, a task may be too challenging for the model to learn through ICL, and thus it passes the test by maintaining low performance in both Single-task ICL and Lifelong ICL settings.

To account for these situations, we split all tasks into 2 groups for each model. Tasks of which 4-shot performance is significantly better than 1-shot performance are classified as ICL-effective tasks, and the remaining tasks are considered to be ICL-ineffective. We report the pass rates for each model on these two groups in Table 4. For 10 out of 12 models, pass rates on ICL-effective tasks are lower than pass rates on ICL-ineffective tasks, suggesting that these models tend to "forget" tasks that are newly acquired, and that the overall pass rates may be an overestimate.

Trends of positive task transfer.While our study mainly focus on undesired performance degradation in Lifelong ICL, which is analogous to the catastrophic forgetting phenomenon in lifelong learning, we also observe trends of positive forward and backward transfers, two desired properties of lifelong learning.7 In our pass rate design, we deliberately choose _two-sided_ t-test to account for both performance gains and drops. We observe positive transfers in Fig. 2, represented by the blue-colored cells in the 1-shot (4k) column and the last row (94% depth). Similar observations can be made with Llama-2 (32k) in Fig. 12 and GPT-4o in Fig. 22. Additionally, Mistral-7B achieves +3.4% performance gain in the Remove setting compared to the Zero-shot baseline (Fig. 4). We consider these as initial evidence for positive transfer in Lifelong ICL, and invite more rigorous analysis to further explore the properties of Lifelong ICL.

Footnote 7: Forward transfer occurs when “a model reuses knowledge acquired from previous tasks when learning new ones”; backward transfer refers the the phenomenon that “a model achieves improved performance on previous tasks after learning a new task.” [Biesialska et al., 2020]

## 6 Discussion

Intended Use.We anticipate Lifelong ICL and Task Haystack to be used for evaluating and diagnosing newly released long-context LMs. However, as our findings in Sections 5.1 and 5.3 suggest, the ICL accuracy and pass rate might be affected if the model has been trained on the tasks used in our evaluation. To ensure responsible use, we encourage users to (1) investigate and report any potential data contamination; (2) report pass rates on ICL-effective/ineffective groups respectively, as done in SS5.3. Additionally, it is possible to use targeted data engineering to improve pass rates on Task Haystack. For fair comparisons, we recommend that users disclose whether their training data contains sequences in a format similar to Task Haystack evaluation.

Limitations.(1) As an initial exploration in the Lifelong ICL setting, we primarily focuses on English-only text classification tasks. This potentially limits a comprehensive assessment of model capabilities across various challenges. To get a more complete picture, the evaluation suite may be

\begin{table}
\begin{tabular}{l|c|c|c|c|c||c|c|c|c} \hline \hline Model & \begin{tabular}{c} ICL-erf. \\ N \\ \end{tabular} & \begin{tabular}{c} ICL-ineff. \\ pass \\ \end{tabular} & \begin{tabular}{c} ICL-ineff. \\ pass \\ \end{tabular} & \begin{tabular}{c} All \\ pass \\ \end{tabular} & \begin{tabular}{c} Model \\ N \\ \end{tabular} & \begin{tabular}{c} ICL-eff. \\ pass \\ \end{tabular} & \begin{tabular}{c} ICL-ineff. \\ N \\ \end{tabular} & 
\begin{tabular}{c} All \\ pass \\ \end{tabular} \\ \hline Mistral-7B (32k) & 5 & 36.0 & 11 & 81.8 & 67.5 & Cmd-R-35B (128k) & 5 & 40.0 & 11 & 58.2 & 52.5 \\ FHL-7B (32k) & 2 & 40.0 & 14 & 77.1 & 72.5 & Yi-6B (200k) & 6 & 46.6 & 10 & 42.0 & 43.8 \\ Llama-2-7B (32k) & 6 & 33.3 & 10 & 46.0 & 41.2 & Yi-9B (200k) & 6 & 50.0 & 10 & 72.0 & 63.7 \\ Llama-2-7B (80k) & 3 & 80.0 & 13 & 100.0 & 96.3 & Yi-34B (200k) & 3 & 46.7 & 13 & 67.7 & 63.8 \\ Llama-3-8B (1048k) & 6 & 40.0 & 10 & 90.0 & 71.3 & GPT-3.5-Turbo (16k) & 5 & 44.0 & 11 & 70.9 & 62.5 \\ Llama-3-70B (1048k) & 4 & 35.0 & 12 & 65.0 & 57.5 & GPT-4o (128k) & 6 & 96.7 & 10 & 76.0 & 83.7 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Pass Rates on ICL-effective/ineffective Tasks.** Results are computed in the 16-task 4-shot setting. We define ICL-effective tasks as tasks whose 4-shot performance is significantly better than its 1-shot performance. In general, ICL-effective tasks have lower pass rates.

improved by including more diverse tasks categories (_e.g._, question answering, conditional generation (Ye et al., 2021)), modalities (_e.g._, vision (Sharma et al., 2024), speech), and languages. We encourage future research to build upon our foundation and explore these more complex settings. (2) This work simplifies the lifelong learning stream by assuming a sequential order, clear task boundaries, and a fixed number of examples per class for each task. Real-world scenarios likely involve a more dynamic learning stream, without clear task boundaries or assumptions on the sequential order. In SSB.6, we conduct preliminary experiments by interleaving examples of multiple tasks in the context. Future work may explore more realistic lifelong learning streams with increased complexity. (3) Finally, due to computational constraints, our evaluation utilizes 5 random permutations of tasks order and 5 different random samples of few-shot training sets. Experimenting with a larger number of samples could potentially reduce the randomness inherent in the results and increase the reliability of the findings. Additionally, we limit our evaluation to up to 32k input tokens. Stress-testing long-context models with their full context lengths may reveal further limitations of these models.

Ethics Statement.This work leverages openly available datasets that were carefully reviewed by the authors to mitigate potential data privacy and security concerns. To the best of our knowledge, the datasets we use do not contain personally identifiable information. Some datasets contain offensive content when the underlying task is offensive content (_e.g._, hate speech) classification. We emphasize that these datasets are used solely for evaluation purposes. As our research does not involve model training or the release of new models, the risk of amplifying biases within the data is minimal.

## 7 Conclusion

In this paper, we introduced Lifelong ICL, a novel problem setting for long-context LMs, and developed Task Haystack, a concrete evaluation suite focusing on evaluating and diagnosing long-context LMs in the Lifelong ICL setting. Our experiments with 14 long-context LMs revealed that while these models excel at needle-in-a-haystack style evaluation, their ability to utilize the context flexibly and contextually remains limited. Through our controlled analysis, we dissected and quantified factors such as recency biases and distractions that contribute to performance drops. We also identified performance degradation when repeating ICL examples or using paraphrased instructions, highlighting a fundamental vulnerability in current long-context models.

Our results demonstrate that Task Haystack still poses significant challenges for newly-released long-context models. We hope that Lifelong ICL and Task Haystack will serve as valuable tools for diagnosing and advancing the development of future long-context LMs. Additionally, we consider our work as an exploratory step towards backprop-free algorithms in lifelong learning settings.

## Acknowledgment

We thank the anonymous reviewers for their thoughtful feedback and active engagement throughout the discussion period. In addition, we thank Xisen Jin, Jun Yan, Ting-Yun Chang, Daniel Firebanks-Quevedo, Johnny Wei, Ryan Wang, Wenbo Zhang for insightful discussions. This work was supported in part by Cohere For AI Research Grant Program and OpenAI Researcher Access Program. Qinyuan Ye was supported by a USC Annenberg Fellowship.

## References

* A. Atari et al. (2024)(Website) External Links: Link Cited by: SS1.
* A. Atari, A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang, K. Yu, P. Liu, Q. Liu, S. Yue, S. Yang, S. Yang, T. Yu, W. Xie, W. Huang, X. Hu, X. Ren, X. Niu, P. Nie, Y. Xu, Y. Liu, Y. Wang, Y. Cai, Z. Gu, Z. Liu, and Z. Dai (2024)Yi: open foundation models by 01.ai. External Links: Link Cited by: SS1.
* R. Agarwal, A. Singh, L. M. Zhang, B. Bohnet, L. Rosias, S. C.Y. Chan, B. Zhang, A. Faust, and H. Larochelle (2024)Many-shot in-context learning. In ICML 2024 Workshop on In-Context Learning, External Links: Link Cited by: SS1.

Tiago A. Almeida, Jose Maria G. Hidalgo, and Akebo Yamakami. Contributions to the study of sms spam filtering: new collection and results. In _Proceedings of the 11th ACM Symposium on Document Engineering_, DocEng '11, page 259-262, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450308632. doi: 10.1145/2034691.2034742. URL [https://doi.org/10.1145/2034691.2034742](https://doi.org/10.1145/2034691.2034742).
* An et al. (2024) Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, and Jian-Guang Lou. Make your llm fully utilize the context. _arXiv preprint arXiv:2404.16811_, 2024.
* Anthropic (2024) AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. _Claude-3 Model Card_, 2024. URL [https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf).
* Bai et al. (2024) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: A bilingual, multitask benchmark for long context understanding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3119-3137, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.172. URL [https://aclanthology.org/2024.acl-long.172](https://aclanthology.org/2024.acl-long.172).
* Beltagy et al. (2020) Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020. URL [https://arxiv.org/abs/2004.05150](https://arxiv.org/abs/2004.05150).
* Bertsch et al. (2024) Amanda Bertsch, Maor Iygi, Uri Alon, Jonathan Berant, Matthew R. Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration. In _First Workshop on Long-Context Foundation Models @ ICML 2024_, 2024. URL [https://openreview.net/forum?id=4KAmc7vUbQq](https://openreview.net/forum?id=4KAmc7vUbQq).
* Bhagavatula et al. (2023) Chandra Bhagavatula, Jena D. Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Lianhui Qin, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West, and Yejin Choi. I2D2: Inductive knowledge distillation with NeuroLogic and self-imitation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 9614-9630, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.535. URL [https://aclanthology.org/2023.acl-long.535](https://aclanthology.org/2023.acl-long.535).
* Biesialska et al. (2020) Magdalena Biesialska, Katarzyna Biesialska, and Marta R. Costa-jussa. Continual lifelong learning in natural language processing: A survey. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, _Proceedings of the 28th International Conference on Computational Linguistics_, pages 6523-6541, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.574. URL [https://aclanthology.org/2020.coling-main.574](https://aclanthology.org/2020.coling-main.574).
* Bingler et al. (2024) Julia Anna Bingler, Mathias Kraus, Markus Leippold, and Nicolas Webersinke. How cheap talk in climate disclosures relates to climate initiatives, corporate emissions, and reputation risk. _Journal of Banking & Finance_, 164:107191, 2024. ISSN 0378-4266. doi: [https://doi.org/10.1016/j.jbankfin.2024.107191](https://doi.org/10.1016/j.jbankfin.2024.107191). URL [https://www.sciencedirect.com/science/article/pii/S0378426624001080](https://www.sciencedirect.com/science/article/pii/S0378426624001080).
* Bird et al. (2008) Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson, Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett Powley, Dragomir Radev, and Yee Fan Tan. The ACL Anthology reference corpus: A reference dataset for bibliographic research in computational linguistics. In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, and Daniel Tapias, editors, _Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC'08)_, Marrakech, Morocco, May 2008. European Language Resources Association (ELRA). URL [http://www.lrec-conf.org/proceedings/lrec2008/pdf/445_paper.pdf](http://www.lrec-conf.org/proceedings/lrec2008/pdf/445_paper.pdf).
* Biswal (2020) Mrutyunjay Biswal. Iitigee neet aiims students questions data. [https://www.kaggle.com/datasets/mrutyunjaybiswal/iitijee-neet-aims-students-questions-data](https://www.kaggle.com/datasets/mrutyunjaybiswal/iitijee-neet-aims-students-questions-data), 2020.
* Biesialska et al. (2020)Yuri Bizzoni and Shalom Lappin. Predicting human metaphor paraphrase judgments with deep neural networks. In Beata Beigman Klebanov, Ekaterina Shutova, Patricia Lichtenstein, Smaranda Muresan, and Chee Wee, editors, _Proceedings of the Workshop on Figurative Language Processing_, pages 45-55, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-0906. URL [https://aclanthology.org/W18-0906](https://aclanthology.org/W18-0906).
* Brown et al. (2018) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
* Chakraborty et al. (2016) Abhijnan Chakraborty, Bhargavi Paranjape, Sourya Kakarla, and Niloy Ganguly. Stop clickbait: Detecting and preventing clickbaits in online news media. In _2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)_, pages 9-16, 2016. doi: 10.1109/ASONAM.2016.7752207.
* Chang and Jia (2023) Ting-Yun Chang and Robin Jia. Data curation alone can stabilize in-context learning. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8123-8144, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.452. URL [https://aclanthology.org/2023.acl-long.452](https://aclanthology.org/2023.acl-long.452).
* Chang et al. (2024) Ting-Yun Chang, Jesse Thomason, and Robin Jia. When parts are greater than sums: Individual LLM components can outperform full models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, _Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing_, pages 10280-10299, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.574. URL [https://aclanthology.org/2024.emnlp-main.574](https://aclanthology.org/2024.emnlp-main.574).
* Chapuis et al. (2020) Emile Chapuis, Pierre Colombo, Matteo Manica, Matthieu Labeau, and Chloe Clavel. Hierarchical pre-training for sequence labelling in spoken dialog. In Trevor Cohn, Yulan He, and Yang Liu, editors, _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 2636-2648, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.239. URL [https://aclanthology.org/2020.findings-emnlp.239](https://aclanthology.org/2020.findings-emnlp.239).
* Chatterjee et al. (2019) Ankush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. SemEval-2019 task 3: EmoContext contextual emotion detection in text. In Jonathan May, Ekaterina Shutova, Aurelie Herbelot, Xiaodan Zhu, Marianna Apidianaki, and Saif M. Mohammad, editors, _Proceedings of the 13th International Workshop on Semantic Evaluation_, pages 39-48, Minneapolis, Minnesota, USA, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/S19-2005. URL [https://aclanthology.org/S19-2005](https://aclanthology.org/S19-2005).
* Choi et al. (2023) Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, and David Jurgens. Do LLMs understand social knowledge? evaluating the sociability of large language models with SocKET benchmark. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 11370-11403, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.699. URL [https://aclanthology.org/2023.emnlp-main.699](https://aclanthology.org/2023.emnlp-main.699).
* cjadams et al. (2019) cjadams, Daniel Borkan, inversion, Jeffrey Sorensen, Lucas Dixon, Lucy Vasserman, and nithum. Jigsaw unintended bias in toxicity classification, 2019. URL [https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification](https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification).
* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conferenceof the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 2924-2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL [https://aclanthology.org/N19-1300](https://aclanthology.org/N19-1300).
* Coda-Forno et al. (2023) Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matt Botvinick, Jane Wang, and Eric Schulz. Meta-in-context learning in large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 65189-65201. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/cda04d7ea67ea1376bf8c6962d8541e0-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/cda04d7ea67ea1376bf8c6962d8541e0-Paper-Conference.pdf).
* de A1 (2024) Cohere for AI. C4ai command-r model card, 2024. URL [https://huggingface.co/CohereForAI/c4ai-command-r-v01](https://huggingface.co/CohereForAI/c4ai-command-r-v01).
* Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 16344-16359. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf).
* de Gibert et al. (2018) Ona de Gibert, Naiara Perez, Aitor Garcia-Pablos, and Montse Cuadros. Hate speech dataset from a white supremacy forum. In Darja Fiser, Ruihong Huang, Vinodkumar Prabhakaran, Rob Voigt, Zeerak Waseem, and Jacqueline Wermimont, editors, _Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)_, pages 11-20, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5102. URL [https://aclanthology.org/W18-5102](https://aclanthology.org/W18-5102).
* de Marneffe et al. (2019) Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigating projection in naturally occurring discourse. _Proceedings of Sinn and Bedeutung_, 23(2):107-124, Jul. 2019. doi: 10.18148/sub/2019.v23i2.601. URL [https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601](https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601).
* de Masson d'Autume et al. (2019) Cyprien de Masson d'Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. Episodic memory in lifelong language learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/f8d2e80c1458ea2501f98a2cafadb397-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/f8d2e80c1458ea2501f98a2cafadb397-Paper.pdf).
* Dernoncourt and Lee (2017) Franck Dernoncourt and Ji Young Lee. PubMed 200k RCT: a dataset for sequential sentence classification in medical abstracts. In Greg Kondrak and Taro Watanabe, editors, _Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)_, pages 308-313, Taipei, Taiwan, November 2017. Asian Federation of Natural Language Processing. URL [https://aclanthology.org/I17-2052](https://aclanthology.org/I17-2052).
* Diggelmann et al. (2020) Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold. Climate-fever: A dataset for verification of real-world climate claims. _arXiv preprint arXiv:2012.00614_, 2020.
* Dolan and Brockett (2005) William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In _Proceedings of the Third International Workshop on Paraphrasing (IWP2005)_, 2005. URL [https://aclanthology.org/I05-5002](https://aclanthology.org/I05-5002).
* Dubey et al. (2024) Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. _arXiv preprint arXiv:2407.21783_, 2024.
* Ellis et al. (2018) Alex Ellis, Julia Elliott, Paula Griffin, and William Chen. Quora insincere questions classification, 2018. URL [https://kaggle.com/competitions/quora-insincere-questions-classification](https://kaggle.com/competitions/quora-insincere-questions-classification).
* Dernoncourt and Lee (2019)William Ferreira and Andreas Vlachos. Emergent: a novel data-set for stance classification. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, _Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 1163-1168, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1138. URL [https://aclanthology.org/N16-1138](https://aclanthology.org/N16-1138).
* FitzGerald et al. (2023) Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha Ranganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and Prem Natarajan. MASSIVE: A 1M-example multilingual natural language understanding dataset with 51 typologically-diverse languages. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 4277-4302, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.235. URL [https://aclanthology.org/2023.acl-long.235](https://aclanthology.org/2023.acl-long.235).
* Fu et al. (2024) Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. In _Forty-first International Conference on Machine Learning_, 2024. URL [https://openreview.net/forum?id=TaAqeo7lUh](https://openreview.net/forum?id=TaAqeo7lUh).
* Garcia-Ferrero et al. (2023) Iker Garcia-Ferrero, Begona Altuna, Javier Alvez, Itziar Gonzalez-Dios, and German Rigau. This is not a dataset: A large negation benchmark to challenge large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 8596-8615, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.531. URL [https://aclanthology.org/2023.emnlp-main.531](https://aclanthology.org/2023.emnlp-main.531).
* Team (2024) Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. _arXiv preprint arXiv:2403.05530_, 2024.
* Goldman et al. (2024) Omer Goldman, Alon Jacovi, Aviv Slobodkin, Aviya Maimon, Ido Dagan, and Reut Tsarfaty. Is it really long context if all you need is retrieval? towards genuinely difficult long context nlp, 2024. URL [https://arxiv.org/abs/2407.00402](https://arxiv.org/abs/2407.00402).
* GradientAI (2024a)GradientAI. Llama-3-70b-instruct-gradient-1048k model card, 2024a. URL [https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-1048k](https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-1048k).
* Llama-3-8b-instruct-gradient-1048k model card (2024b)GradientAI. Llama-3-8b-instruct-gradient-1048k model card, 2024b. URL [https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k](https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k).
* Graham (2024) Paul Graham. Paul graham essays, 2024. URL [https://www.paulgraham.com/articles.html](https://www.paulgraham.com/articles.html).
* Grano et al. (2017) Giovanni Grano, Andrea Di Sorbo, Francesco Mercaldo, Corrado A Visaggio, Gerardo Canfora, and Sebastiano Panichella. Android apps and user feedback: A dataset for software evolution and quality improvement. In _Proceedings of the 2Nd ACM SIGSOFT International Workshop on App Market Analytics_, WAMA 2017, pages 8-11, New York, NY, USA, January 2017. ACM. doi: 10.1145/3121264.31212166. URL [https://doi.org/10.5167/uzh-139426](https://doi.org/10.5167/uzh-139426).
* Guha et al. (2017) Neel Guha, Julian Nyarko, Daniel Ho, Christopher Re, Adam Chilton, Aditya K, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, Dmitry Taliisman, Enam Hoque, Faiz Surani, Frank Fagan, Galit Sarfaty, Gregory Dickinson, Haggai Porat, Jason Hegland, Jessica Wu, Joe Nudell, Joel Niklaus, John Nay, Jonathan Choi, Kevin Tobia, Margaret Hagan, Megan Ma, Michael Livermore, Nikon Rasumov-Rahe, Nils Holzenberger, Noam Kolt, Peter Henderson, Sean Rehaag, Sharad Goel, Shang Gao, Spencer Williams, Sunny Gandhi, Tom Zur, Varun Iyer, and Zehua Li. Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 44123-44279. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/89e44582fd28ddfealea4dcb0ebbf4b0-Paper-Datasets_and_Benchmarks.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/89e44582fd28ddfealea4dcb0ebbf4b0-Paper-Datasets_and_Benchmarks.pdf).
* Halawi et al. (2023) Danny Halawi, Jean-Stanislas Denain, and Jacob Steinhardt. Overthinking the truth: Understanding how language models process false demonstrations. _arXiv preprint arXiv:2307.09476_, 2023.
* Halawi et al. (2024)Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. LM-infinite: Zero-shot extreme length generalization for large language models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)_, pages 3991-4008, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.222. URL [https://aclanthology.org/2024.naacl-long.222](https://aclanthology.org/2024.naacl-long.222).
* Holzenberger et al. (2020) Nils Holzenberger, Andrew Blair-Stanek, and Benjamin Van Durme. A dataset for statutory reasoning in tax law entailment and question answering. _arXiv preprint arXiv:2005.05257_, 2020.
* Hsieh et al. (2024) Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. RULER: What's the real context size of your long-context language models? In _First Conference on Language Modeling_, 2024. URL [https://openreview.net/forum?id=kIoBbc76Sy](https://openreview.net/forum?id=kIoBbc76Sy).
* Hu and Liu (2004) Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In _Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, KDD '04, page 168-177, New York, NY, USA, 2004. Association for Computing Machinery. ISBN 1581138881. doi: 10.1145/1014052.1014073. URL [https://doi.org/10.1145/1014052.1014073](https://doi.org/10.1145/1014052.1014073).
* Hu et al. (2024) Yutong Hu, Quzhe Huang, Mingxu Tao, Chen Zhang, and Yansong Feng. Can perplexity reflect large language model's ability in long text understanding? In _The Second Tiny Papers Track at ICLR 2024_, 2024. URL [https://openreview.net/forum?id=Cjp6YKVeAa](https://openreview.net/forum?id=Cjp6YKVeAa).
* Iyer et al. (2016) Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. First quora dataset release: Question pairs. [https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs), 2016.
* Ji et al. (2023) Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 24678-24704. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/4dbb61cb68671edc4ca3712d70083b9f-Paper-Datasets_and_Benchmarks.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/4dbb61cb68671edc4ca3712d70083b9f-Paper-Datasets_and_Benchmarks.pdf).
* Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. URL [https://arxiv.org/abs/2310.06825](https://arxiv.org/abs/2310.06825).
* Jimenez et al. (2024) Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world github issues? In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=VTF8yNQM66](https://openreview.net/forum?id=VTF8yNQM66).
* Jin et al. (2021) Xisen Jin, Bill Yuchen Lin, Mohammad Rostami, and Xiang Ren. Learn continually, generalize rapidly: Lifelong knowledge accumulation for few-shot learning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 714-729, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.62. URL [https://aclanthology.org/2021.findings-emnlp.62](https://aclanthology.org/2021.findings-emnlp.62).
* pressure testing llms. [https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main](https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main), 2023.
* Kim et al. (2021) Hyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi, and Maarten Sap. ProsocialDialog: A prosocial backbone for conversational agents. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 4005-4029, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.267. URL [https://aclanthology.org/2022.emnlp-main.267](https://aclanthology.org/2022.emnlp-main.267).
* Kirkpatrick et al. (2017) James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _Proceedings of the national academy of sciences_, 114(13):3521-3526, 2017.
* Kocijan et al. (2019) Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz. A surprisingly robust trick for the Winograd schema challenge. In Anna Korhonen, David Traum, and Lluis Marquez, editors, _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4837-4842, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1478. URL [https://aclanthology.org/P19-1478](https://aclanthology.org/P19-1478).
* Kotonya and Toni (2020) Neema Kotonya and Francesca Toni. Explainable automated fact-checking: A survey. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, _Proceedings of the 28th International Conference on Computational Linguistics_, pages 5430-5443, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.474. URL [https://aclanthology.org/2020.coling-main.474](https://aclanthology.org/2020.coling-main.474).
* Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In _Proceedings of the 29th Symposium on Operating Systems Principles_, SOSP '23, page 611-626, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400702297. doi: 10.1145/3600006.3613165. URL [https://doi.org/10.1145/3600006.3613165](https://doi.org/10.1145/3600006.3613165).
* Lee et al. (2024) Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sebastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, and Kelvin Guu. Can long-context language models subsume retrieval, rag, sql, and more?, 2024. URL [https://arxiv.org/abs/2406.13121](https://arxiv.org/abs/2406.13121).
* Levesque et al. (2011) Hector J Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In _AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning_, volume 46, page 47, 2011.
* Levy et al. (2024) Mosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more tokens: the impact of input length on the reasoning performance of large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 15339-15353, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.818. URL [https://aclanthology.org/2024.acl-long.818](https://aclanthology.org/2024.acl-long.818).
* Lhoest et al. (2021) Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clement Delangue, Theo Matussiere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Francois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In Heike Adel and Shuming Shi, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 175-184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-demo.21. URL [https://aclanthology.org/2021.emnlp-demo.21](https://aclanthology.org/2021.emnlp-demo.21).
* Li et al. (2024) Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context l1ms struggle with long in-context learning. _CoRR_, abs/2404.02060, 2024. URL [https://doi.org/10.48550/arXiv.2404.02060](https://doi.org/10.48550/arXiv.2404.02060).
* Li et al. (2020)Xin Li and Dan Roth. Learning question classifiers. In _COLING 2002: The 19th International Conference on Computational Linguistics_, 2002. URL [https://aclanthology.org/C02-1150](https://aclanthology.org/C02-1150).
* Liu et al. (2024a) Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention. _arXiv preprint arXiv:2402.08268_, 2024a.
* Liu et al. (2024b) Hao Liu, Matei Zaharia, and Pieter Abbeel. Ringattention with blockwise transformers for near-infinite context. In _The Twelfth International Conference on Learning Representations_, 2024b. URL [https://openreview.net/forum?id=WsRHPHH4s0](https://openreview.net/forum?id=WsRHPHH4s0).
* Liu et al. (2022a) Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 1950-1965. Curran Associates, Inc., 2022a. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/0cde695b83bd186c1fd456302880454c-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/0cde695b83bd186c1fd456302880454c-Paper-Conference.pdf).
* Liu et al. (2024c) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. _Transactions of the Association for Computational Linguistics_, 12:157-173, 2024c.
* Liu et al. (2022b) Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, and Bill Dolan. A token-level reference-free hallucination detection benchmark for free-form text generation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 6723-6737, Dublin, Ireland, May 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.464. URL [https://aclanthology.org/2022.acl-long.464](https://aclanthology.org/2022.acl-long.464).
* Louis et al. (2020) Annie Louis, Dan Roth, and Filip Radlinski. "I'd rather just go to bed": Understanding indirect answers. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 7411-7425, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.601. URL [https://aclanthology.org/2020.emnlp-main.601](https://aclanthology.org/2020.emnlp-main.601).
* Luan et al. (2018) Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 3219-3232, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1360. URL [https://aclanthology.org/D18-1360](https://aclanthology.org/D18-1360).
* Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_, pages 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL [http://www.aclweb.org/anthology/P11-1015](http://www.aclweb.org/anthology/P11-1015).
* Malo et al. (2013) Pekka Malo, Ankur Sinha, Pyry Takala, Pekka Korhonen, and Jyrki Wallenius. Good debt or bad debt: Detecting semantic orientations in economic texts. _arXiv preprint arXiv:1307.5336_, 2013.
* Manotas et al. (2020) Irene Manotas, Ngoc Phuoc An Vo, and Vadim Sheinin. LiMiT: The literal motion in text dataset. In Trevor Cohn, Yulan He, and Yang Liu, editors, _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 991-1000, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.88. URL [https://aclanthology.org/2020.findings-emnlp.88](https://aclanthology.org/2020.findings-emnlp.88).
* Marelli et al. (2014) Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. A SICK cure for the evaluation of compositional distributional semantic models. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, _Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)_, pages 216-223, Reykjavik, Iceland, May 2014. European Language Resources Association (ELRA). URL [http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf).
* Marelli et al. (2016)Clara H. McCreery, Namit Katariya, Anitha Kannan, Manish Chablani, and Xavier Amatriain. Effective transfer learning for identifying similar questions: Matching user questions to covid-19 faqs. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, KDD '20, page 3458-3465, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3412861. URL [https://doi.org/10.1145/3394486.3412861](https://doi.org/10.1145/3394486.3412861).
* Meaney et al. (2021) J. A. Meaney, Steven Wilson, Luis Chiruzzo, Adam Lopez, and Walid Magdy. SemEval 2021 task 7: HaHackathon, detecting and rating humor and offense. In Alexis Palmer, Nathan Schneider, Natalie Schluter, Guy Emerson, Aurelie Herbelot, and Xiaodan Zhu, editors, _Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)_, pages 105-119, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.semeval-1.9. URL [https://aclanthology.org/2021.semeval-1.9](https://aclanthology.org/2021.semeval-1.9).
* Mehta et al. (2023) Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. An empirical investigation of the role of pre-training in lifelong learning. _Journal of Machine Learning Research_, 24(214):1-50, 2023. URL [http://jmlr.org/papers/v24/22-0496.html](http://jmlr.org/papers/v24/22-0496.html).
* Miller et al. (2017) Tristan Miller, Christian Hempelmann, and Iryna Gurevych. SemEval-2017 task 7: Detection and interpretation of English puns. In Steven Bethard, Marine Carpuat, Marianna Apidianaki, Saif M. Mohammad, Daniel Cer, and David Jurgens, editors, _Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)_, pages 58-68, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2005. URL [https://aclanthology.org/S17-2005](https://aclanthology.org/S17-2005).
* Mohammad et al. (2016) Saif Mohammad, Svetlana Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, and Colin Cherry. SemEval-2016 task 6: Detecting stance in tweets. In Steven Bethard, Marine Carpuat, Daniel Cer, David Jurgens, Preslav Nakov, and Torsten Zesch, editors, _Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)_, pages 31-41, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/S16-1003. URL [https://aclanthology.org/S16-1003](https://aclanthology.org/S16-1003).
* Mollas et al. (2022) Ioannis Mollas, Zoe Chrysopoulou, Stamatis Karlos, and Grigorios Tsoumakas. Ethos: a multi-label hate speech detection dataset. _Complex & Intelligent Systems_, 8(6):4663-4678, January 2022. ISSN 2198-6053. doi: 10.1007/s40747-021-00608-2. URL [http://dx.doi.org/10.1007/s40747-021-00608-2](http://dx.doi.org/10.1007/s40747-021-00608-2).
* Nasr et al. (2023) Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramer, and Katherine Lee. Scalable extraction of training data from (production) language models. _CoRR_, abs/2311.17035, 2023. URL [https://doi.org/10.48550/arXiv.2311.17035](https://doi.org/10.48550/arXiv.2311.17035).
* a multilingual dataset for counterfactual detection in product review. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 7092-7108, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.568. URL [https://aclanthology.org/2021.emnlp-main.568](https://aclanthology.org/2021.emnlp-main.568).
* Oraby et al. (2016) Shereen Oraby, Vrindavan Harrison, Lena Reed, Ernesto Hernandez, Ellen Riloff, and Marilyn Walker. Creating and characterizing a diverse corpus of sarcasm in dialogue. In Raquel Fernandez, Wolfgang Minker, Giuseppe Carenini, Ryuichiro Higashinaka, Ron Artstein, and Alesia Gainer, editors, _Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue_, pages 31-41, Los Angeles, September 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-3604. URL [https://aclanthology.org/W16-3604](https://aclanthology.org/W16-3604).
* Pang & Lee (2004) Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In _Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)_, pages 271-278, Barcelona, Spain, July 2004. doi: 10.3115/1218955.1218990. URL [https://aclanthology.org/P04-1035](https://aclanthology.org/P04-1035).
* Pang et al. (2017)Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Kevin Knight, Hwee Tou Ng, and Kemal Oflazer, editors, _Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)_, pages 115-124, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. doi: 10.3115/1219840.1219855. URL [https://aclanthology.org/P05-1015](https://aclanthology.org/P05-1015).
* Park and Cardie (2014) Joonsuk Park and Claire Cardie. Identifying appropriate support for propositions in online user comments. In Nancy Green, Kevin Ashley, Diane Litman, Chris Reed, and Vern Walker, editors, _Proceedings of the First Workshop on Argumentation Mining_, pages 29-38, Baltimore, Maryland, June 2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-2105. URL [https://aclanthology.org/W14-2105](https://aclanthology.org/W14-2105).
* Patwa et al. (2021) Parth Patwa, Shivam Sharma, Srinivas Pykl, Vineeth Guptha, Gitanjali Kumari, Md Shad Akhtar, Asif Ekbal, Amitava Das, and Tanmoy Chakraborty. _Fighting an Infodemic: COVID-19 Fake News Dataset_, page 21-29. Springer International Publishing, 2021. ISBN 9783030736965. doi: 10.1007/978-3-030-73696-5_3. URL [http://dx.doi.org/10.1007/978-3-030-73696-5_3](http://dx.doi.org/10.1007/978-3-030-73696-5_3).
* Pilehvar and Camacho-Collados (2019) Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 1267-1273, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1128. URL [https://aclanthology.org/N19-1128](https://aclanthology.org/N19-1128).
* Pontiki et al. (2015) Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. SemEval-2015 task 12: Aspect based sentiment analysis. In Preslav Nakov, Torsten Zesch, Daniel Cer, and David Jurgens, editors, _Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)_, pages 486-495, Denver, Colorado, June 2015. Association for Computational Linguistics. doi: 10.18653/v1/S15-2082. URL [https://aclanthology.org/S15-2082](https://aclanthology.org/S15-2082).
* Press et al. (2022) Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=R8sQPpGCv0](https://openreview.net/forum?id=R8sQPpGCv0).
* Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras, editors, _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pages 2383-2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL [https://aclanthology.org/D16-1264](https://aclanthology.org/D16-1264).
* Roemmele et al. (2011) Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In _2011 AAAI Spring Symposium Series_, 2011.
* V)_, pages 69-79, Osaka, Japan, December 2016a. The COLING 2016 Organizing Committee. URL [https://aclanthology.org/W16-5309](https://aclanthology.org/W16-5309).
* Santus et al. (2016b) Enrico Santus, Alessandro Lenci, Tin-Shing Chiu, Qin Lu, and Chu-Ren Huang. Nine features in a random forest to learn taxonomical semantic relations. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, _Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)_, pages 4557-4564, Portoroz, Slovenia, May 2016b. European Language Resources Association (ELRA). URL [https://aclanthology.org/L16-1722](https://aclanthology.org/L16-1722).
* Saravia et al. (2018) Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. CARER: Contextualized affect representations for emotion recognition. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 3687-3697, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1404. URL [https://aclanthology.org/D18-1404](https://aclanthology.org/D18-1404).
* Sarulina and Gorbelnik (2018)Tal Schuster, Adam Fisch, and Regina Barzilay. Get your vitamin C! robust fact verification with contrastive evidence. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 624-643, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.52. URL [https://aclanthology.org/2021.naacl-main.52](https://aclanthology.org/2021.naacl-main.52).
* Scialom et al. (2022) Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. Fine-tuned language models are continual learners. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 6107-6122, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.410. URL [https://aclanthology.org/2022.emnlp-main.410](https://aclanthology.org/2022.emnlp-main.410).
* Shaham et al. (2022) Uri Shaham, Elad Segal, Maor Iygi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long language sequences. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 12007-12021, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.823. URL [https://aclanthology.org/2022.emnlp-main.823](https://aclanthology.org/2022.emnlp-main.823).
* Shaham et al. (2023) Uri Shaham, Maor Iygi, Avia Efrat, Jonathan Berant, and Omer Levy. ZeroSCROLLS: A zero-shot benchmark for long text understanding. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 7977-7989, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.536. URL [https://aclanthology.org/2023.findings-emnlp.536](https://aclanthology.org/2023.findings-emnlp.536).
* Sharma et al. (2024) Aditya Sharma, Michael Saxon, and William Yang Wang. Losing visual needles in image haystacks: Vision language models are easily distracted in short and long contexts, 2024. URL [https://arxiv.org/abs/2406.16851](https://arxiv.org/abs/2406.16851).
* Sheng and Uthus (2020) Emily Sheng and David Uthus. Investigating societal biases in a poetry composition system. In Marta R. Costa-jussa, Christian Hardmeier, Will Radford, and Kellie Webster, editors, _Proceedings of the Second Workshop on Gender Bias in Natural Language Processing_, pages 93-106, Barcelona, Spain (Online), December 2020. Association for Computational Linguistics. URL [https://aclanthology.org/2020.gebnlp-1.9](https://aclanthology.org/2020.gebnlp-1.9).
* Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Scharli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 31210-31227. PMLR, 23-29 Jul 2023. URL [https://proceedings.mlr.press/v202/shi23a.html](https://proceedings.mlr.press/v202/shi23a.html).
* Shi et al. (2024) Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, and Hao Wang. Continual learning of large language models: A comprehensive survey. _arXiv preprint arXiv:2404.16789_, 2024.
* Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, _Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing_, pages 1631-1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL [https://aclanthology.org/D13-1170](https://aclanthology.org/D13-1170).
* Srivastava et al. (2023) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, and Alice Xiang et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL [https://openreview.net/forum?id=uyTL5Bvosj](https://openreview.net/forum?id=uyTL5Bvosj).
* Sutskever et al. (2014)Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.
* Sun et al. (2021) Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models actually use long-range context? In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 807-822, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.62. URL [https://aclanthology.org/2021.emnlp-main.62](https://aclanthology.org/2021.emnlp-main.62).
* Tanzer et al. (2024) Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, and Luke Melas-Kyriazi. A benchmark for learning to translate a new language from one grammar book. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=tbVWug9f2h](https://openreview.net/forum?id=tbVWug9f2h).
* Thorne et al. (2018) James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 809-819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL [https://aclanthology.org/N18-1074](https://aclanthology.org/N18-1074).
* TogetherAI (2024) TogetherAI. Llama-2-7b-32k model card, 2024. URL [https://huggingface.co/togethercomputer/LLaMA-2-7b-32K](https://huggingface.co/togethercomputer/LLaMA-2-7b-32K).
* Tworkowski et al. (2023) Szymon Tworkowski, Konrad Staniszewski, Mikol aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mil os. Focused transformer: Contrastive training for context scaling. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 42661-42688. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/8511d06d5590f4bda24d42087802cc81-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/8511d06d5590f4bda24d42087802cc81-Paper-Conference.pdf).
* Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Tal Linzen, Grzegorz Chrupala, and Afra Alishahi, editors, _Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, pages 353-355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL [https://aclanthology.org/W18-5446](https://aclanthology.org/W18-5446).
* Wang et al. (2023) Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 9840-9855, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.609. URL [https://aclanthology.org/2023.emnlp-main.609](https://aclanthology.org/2023.emnlp-main.609).
* Wang (2017) William Yang Wang. "liar, liar pants on fire": A new benchmark dataset for fake news detection. In Regina Barzilay and Min-Yen Kan, editors, _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 422-426, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-2067. URL [https://aclanthology.org/P17-2067](https://aclanthology.org/P17-2067).
* Warstadt et al. (2019) Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. _Transactions of the Association for Computational Linguistics_, 7:625-641, 2019. doi: 10.1162/tacl_a_00290. URL [https://aclanthology.org/Q19-1040](https://aclanthology.org/Q19-1040).
* Webersinke et al. (2021) Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler, and Markus Leippold. Climatebert: A pretrained language model for climate-related text. _arXiv preprint arXiv:2110.12010_, 2021.
* Weston et al. (2015) Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merrienboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. _arXiv preprint arXiv:1502.05698_, 2015.
* Wood et al. (2018)Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 1112-1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL [https://aclanthology.org/N18-1101](https://aclanthology.org/N18-1101).
* Yang et al. (2015) Yi Yang, Wen-tau Yih, and Christopher Meek. WikiQA: A challenge dataset for open-domain question answering. In Lluis Marquez, Chris Callison-Burch, and Jian Su, editors, _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing_, pages 2013-2018, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1237. URL [https://aclanthology.org/D15-1237](https://aclanthology.org/D15-1237).
* Ye et al. (2021) Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. CrossFit: A few-shot learning challenge for cross-task generalization in NLP. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 7163-7189, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.572. URL [https://aclanthology.org/2021.emnlp-main.572](https://aclanthology.org/2021.emnlp-main.572).
* Ye et al. (2024) Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, and Minjoon Seo. Investigating the effectiveness of task-agnostic prefix prompt for instruction following. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 19386-19394, 2024.
* Yen et al. (2024) Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. Helmet: How to evaluate long-context language models effectively and thoroughly, 2024. URL [https://arxiv.org/abs/2410.02694](https://arxiv.org/abs/2410.02694).
* Zhang (2024) Dun Zhang. Stella-en-1.5b-v5 model card, 2024. URL [https://huggingface.co/dunzhang/stella_en_1.5B_v5](https://huggingface.co/dunzhang/stella_en_1.5B_v5).
* Zhang et al. (2015) Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015. URL [https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf).
* Zhang et al. (2024) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. \(\infty\)Bench: Extending long context evaluation beyond 100K tokens. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 15262-15277, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.814. URL [https://aclanthology.org/2024.acl-long.814](https://aclanthology.org/2024.acl-long.814).
* Zhao et al. (2024) Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatGPT interaction logs in the wild. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=B18u7ZR1bM](https://openreview.net/forum?id=B18u7ZR1bM).

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See SS6. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See SS6. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See footnote 1. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See SSA.4. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? We did not report error bars directly, but we addressed randomness issues by sampling 5 few-shot samples and utilizing a customized pass rate metric. See SS3.2. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See SSA.4
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? See SSA.2. 2. Did you mention the license of the assets? See SSA.2. 3. Did you include any new assets either in the supplemental material or as a URL? See footnote 1. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See SSA.2. 4. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? See SS6.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]Experiment Details

### Models

We list the details of open-weighted models evaluated in Table 5. For closed models from OpenAI, the specific model versions we evaluated are gpt-3.5-turbo-0125 and gpt-4o-2024-05-13. For Gemini 1.5 Flash, we evaluated gemini-1.5-flash-001.

### Tasks

We select 64 classification tasks from huggingface datasets [10], following the desiderata listed in SS4. We provide their references and huggingface identifiers in Table 6. For further use, readers should refer to the licenses of the original datasets.

### Details on "Pass Rate"

"Pass Rate" Definition.In SS3.2, we introduced "pass rate" as the core evaluation metric in Task Haystack. Here we further explain its definition and our considerations when designing this metric. As illustrated in Fig. 6, we first obtain two groups of 5 different performance metrics (_e.g._, accuracies), one group using Lifelong ICL prompts, and one group using Single-task ICL prompts; we then use two-sided t-test to examine whether the two groups are significantly different. More specifically, we use scipy.stats.ttest_rel that returns the t-statistic and p-value for the test, and we consider tests with \(p<0.05\) as significant differences. We choose to use _two-sided_ tests to account for potential positive transfers that may arise in the Lifelong ICL setting (SS5.3).

"Pass Rate" Limitations.When computing and aggregating pass rates for multiple tasks, we are effectively performing multiple t-tests in parallel, which might increase the risk of Type I errors. We acknowledge this as a limitation of our approach. Following reviewer feedback, we have tried Bonferroni Correction and Benjamini-Hochberg Correction to account for this. However, these methods lead to new challenges. The first method significantly increases the risk of Type II errors and may lead to overestimated pass rates. The second method may lead to unfair comparison across models. Given these concerns, we decide to maintain the current design of pass rates. While this may affect the quantitative results, the qualitative conclusions in the paper are not expected to change.

Figure 6: **Definition of “Pass Rate” in Task Haystack. The model “passes” when the performance of Lifelong ICL is not significantly worse than the Single-task ICL baseline.**

\begin{table}
\begin{tabular}{l l l l} \hline \hline Model & Max \(L\) & Reference & Huggingface Identifier \\ \hline Misstat-7B & 32k & Jiang et al. [2023] & mistralai/Mistral-7B-Instruct-v0.2 \\ FLIM-7B & 32k & An et al. [2024] & ImZTraining/FLIM-7B \\ Llama-2-7B & 32k & TogetherAI [2024] & togethercomputer/LLAMA-2-7B-32K \\ Llama-2-7B & 80k & Fe et al. [2024] & yaofu/llama-2-7b-80k \\ Llama-3-8B & 1048k & GradientAI [2024] & gradient/llama-3-8B-Instruct-Gradient-1048K \\ Llama-3-70B & 1048K & GradientAI [2024] & gradient/llama-3-70B-Instruct-Gradient-1048k \\ Llama-3.1-70B & 128k & Dubey et al. [2024] & meta-lllama-3.1-70B \\ Yi-6B & 200k & 01.Ai et al. [2024] & 01-ai/Yi-6B-200K \\ Yi-9B & 200k & 01.Ai et al. [2024] & 01-ai/Yi-9B-200K \\ Yi-34B & 200k & 01.AI et al. [2024] & 01-ai/Yi-34B-200K \\ Cmd-R-35B & 128k & Cohere for AI [2024] & CohereForAI/cai-command-r-v01 \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Open-weight Long-context LMs Evaluated in This Work.**

[MISSING_PAGE_FAIL:25]

### Implementation and Engineering Details

Data Preprocessing.For each task, the authors manually wrote two task instructions and a task template for in-context learning. In the following we provide one example for the task of ag_news. We ensure that all options have distinct starting tokens when writing the task template, so that the inference can be done with rank classification [Liu et al., 2022a].

```
1{
2"name":"ag_news",
3"task_type":"classification",
4"options":"[World","Sports","Business","Technology"],
5"instruction":"Classify the news article into World, Sports, Business or Technology.".
6"instruction_2":"Determine which category best fits the news article: Sports, Technology, Business, or World.".
7"demonstration_prompt":"Article:{text}\nAnswer:(label)",
8"inference_prompt":"Article:{text}\nAnswer:"
9}
```

To create the few-shot training sets, we randomly sampled five subsets from the original training dataset for in-context learning, each containing at least 16 examples per class. We sub-sample 100 instances from the original development set (or test set when the development set is not provided) to form our test set. Our preprocessing scripts are included in the released code INK-USC/Lifelong-ICL.

LLM Inference.We apply rank classification [Liu et al., 2022a] in all our experiments. Specifically, we query the LM with the prompt and obtain the top 100 predictions for the next token. We then cross-reference this list with the list of the first token of all possible options. We use the prefix caching technique in vLLM [Kwon et al., 2023] which significantly improves the inference speed.

We did _not_ use model-specific prompts (_e.g._, chat template, special tokens, instructions optimized for a specific model). This decision reduces experiment complexity and is reasonable because (1) we expect a model optimized for chat to still be able to perform ICL as a text-token prediction task; (2) the special tokens (_e.g._, <|user|>, [INST]) may create tokenization inconsistencies in in-context learning (_e.g._, World and _World may be two different tokens in the vocabulary); (3) Task Haystack is based on a comparative assumption, making absolute accuracies less important.

Inference Costs.Running a 64-task, 2-shot Task Haystack experiment with a 7B model on one A6000 GPU takes around 20 hours. Running a 16-task, 8-shot Task Haystack experiment with a 7B model on one A6000 GPU takes around 8 hours. For 34B and 70B models, we use four A6000 GPUs. For evaluations with OpenAI models, we use the Batched API.8 All experiments using OpenAI models (Table 2 bottom rows; Fig 21-22) incur a total cost of about $8,000 at the time of writing.

Footnote 8: [https://platform.openai.com/docs/guides/batch](https://platform.openai.com/docs/guides/batch)

## Appendix B Additional Results

### Scale-Task Experiments

In Table 7, we report the results in the Scale-Task setting, where we fix the number of shots per class \(n_{shot}\) to be 2, and experiment with \(n_{task}\in\{8,16,24,32,40,48,56,64\}\).9 We noticed that the overall pass rates are higher than those in the Scale-Shot setting (Table 2), potentially due to a smaller value of \(n_{shot}\). However, long-context models still struggle in this setting: in 41 out of 44 cases in Table 7, the overall pass rates drop below 90%. The three cases achieving pass rates above 90% use the Llama2-7B (80k) model, an outlier model discussed in SS5.1.

Footnote 9: Since each column in the table uses a different set of tasks, accuracies and pass rates from different columns are not directly comparable.

### Task Subset with Permissive Licenses

To ensure responsible data use, in this section we introduce a new subset of 16 tasks (Table 8), each with a permissive license. We recommend that users of Task Haystack refer to this subset for future benchmarking and analysis. We have conducted evaluations on selected models

[MISSING_PAGE_FAIL:27]

### Comparing Base and Instruct Models

Previously in Table 2, we experimented with the base version of Llama-3.1-70B model (_i.e._, meta-llama/Llama-3.1-70B), and identified it as the most capable open-weight model among those evaluated. Here in Table 10, we additionally report results using its instruct version (_i.e._, meta-llama/Llama-3.1-70B-Instruct). Compared to the base model, the instruct model demonstrates improvements in S-acc across all settings, with L-acc remaining comparable or slightly improved. The increased S-acc raises the reference threshold, resulting in lower overall pass rates.

One possible explanation is that instruction tuning and subsequent post-training processes, which primarily involve shorter texts and conversational data, may encourage the model to focus more on the beginning of the context. This shift could potentially compromise its ability to handle long contexts. Given this observation, we believe Lifelong ICL and Task Haystack can also serve as a tool to monitor the long-context modeling capabilities before and after post-training processes.

### Original NIAH Experiments

We experiment with the original needle-in-a-haystack evaluation [13] to provide reference. We use the question "What is the best thing to do in San Francisco?" and the needle "The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day." The metric is the token-level recall of the model's response. We report the pass rates in Table 11 and visualize the results in the first column of figures in Fig. 9-18.

### Interleaving Examples from Multiple Tasks

The complexity of Lifelong ICL can be further increased by interleaving in-context learning examples from multiple tasks, analogous to a multi-needle NIAH challenge [14]. We conducted preliminary experiments of this setting, where each ICL example is paired with its task instruction before itself, and ICL examples of different tasks are streamed in a random order. The results are presented in Table 12. We observe that accuracies in this setting (M-acc) are generally lower than those in the non-interleaving Lifelong ICL setting (L-acc), confirming that interleaving examples adds more challenges in locating relevant context. We leave further investigation as future work.

## Appendix C Additional Discussion

Task Haystack can be solved by a RAG baseline.To examine whether Task Haystack can be addressed by retrieval-augmented generation (RAG) methods, we implemented a simple RAG baseline. We used an off-the-shelf retriever [19] to select one prompt from all task-specific ICL prompts (_i.e._, \(p_{a_{1}},p_{a_{2}},...,p_{a_{n}}\) as defined in SS3). The selected prompt was then prepended before the instruction of

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Model** & **S-acc** & **L-acc** & **M-acc** \\ \hline Mistral-7B (32k) & 78.6 & 74.8 & 72.1 \\ FILM-7B (32k) & 79.6 & 75.4 & 74.7 \\ \hline \hline \end{tabular}
\end{table}
Table 13: **Results of RAG Baseline (SC).** Experiments done with 16-Task, 4-Shot. “RAG-acc” indicates the RAG baseline results. Single-task ICL (S-acc) can be seen as an oracle setting with perfect retrieval accuracy.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c|c c} \hline \multirow{2}{*}{Model} & 0-shot & \multicolumn{4}{c|}{1-shot (4k)} & \multicolumn{4}{c|}{2-shot (8k)} & \multicolumn{4}{c|}{4-shot (16k)} & \multicolumn{4}{c}{8-shot (32k)} \\  & S-acc & S-acc & L-acc & pass & S-acc & L-acc & pass & S-acc & L-acc & pass & S-acc & L-acc & pass \\ \hline Llama-3.1-70B (128k) & 58.8 & 81.7 & 81.2 & 80.0 & 82.8 & 81.1 & 76.2 & 84.6 & 82.4 & 83.8 & 85.2 & 83.3 & 80.0 \\ Llama-3.1-70B-Inst. (128k) & 78.4 & 84.0 & 82.2 & 72.5 & 84.9 & 82.4 & 63.7 & 85.8 & 82.7 & 72.5 & 86.9 & 83.2 & 67.5 \\ \hline \hline \end{tabular}
\end{table}
Table 10: **Comparing Base and Instruct Versions of Llama-3.1-70B**. We use the 16-task Scale-Shot setting, consistent with Table 2.

\begin{table}
\begin{tabular}{l c|l c} \hline \hline
**Model** & **Pass (\%)** & **Model** & **Pass (\%)** \\ \hline Mistral-7B (32k) & 95.3 & Llama-3.8B (1048k) & 100.0 \\ FILM-7B (32k) & 100.0 & Yi-6B (200k) & 100.0 \\ Llama-2-7B (32k) & 95.3 & Yi-9B (200k) & 100.0 \\ Llama-2-7B (80k) & 100.0 & Yi-34B (200k) & 100.0 \\ \hline \hline \end{tabular}
\end{table}
Table 11: **Results of the original NIAH evaluation (§B.5).**the test task. We report the results in Table 13. As expected, Task Haystack, being a retrieval-style task, can be solved by this RAG baseline.

Task Haystack is still meaningful for long-context LM evaluation.Although Task Haystack can be solved by a RAG baseline, we believe Task Haystack--and retrieval-style tasks more broadly--remain valuable for evaluating long-context models. **(1)** One advantage of retrieval-style tasks is that it's much more controllable for ablations. This allows us to carefully investigate whether these long-context LMs behave robustly and as expected. **(2)** As pointed out by Lee et al. (2024), long-context models have certain benefits over RAG methods, including having a simpler pipeline, better handling of multi-hop queries and mitigating cascading errors. HELMET (Yen et al., 2024), a recently-released long-context benchmark, also incorporates retrieval-style and retrieval-augmented generation tasks.

Task Haystack adds to the axis of _contextual understanding_ in long-context LM evaluation.Goldman et al. (2024) introduce a taxonomy of long-context LM evaluation with two axes: (1) _diffusion_: how hard it is to find and extract the necessary information, and (2) _scope_: how long the necessary information is. In the view of this taxonomy, Task Haystack is having low diffusion (it's not hard to find relevant information) and small scope (the relevant information is short, only a few ICL examples), similar to the original NIAH. However, Task Haystack is also more challenging than the original NIAH partly due to requiring contextual understanding (or "implicit aggregations" as briefly mentioned in Goldman et al. (2024)) of the context. We believe it may be helpful to add a third axis of "contextual understanding" or "implicit aggregation" to the taxonomy, and Task Haystack can be seen as making progress in this third axis.

## Appendix D NIAH-style Visualizations

In Fig. 9-19, we present detailed Task Haystack results for ten open-weight models. In Fig. 20-22 we present results for Gemini-1.5-Flash, GPT-3.5-Turbo and GPT-4o.

Fig. 9-18 each contains three subfigures: On the left side, we illustrate the results of the original needle-in-a-haystack evaluation (Kamradt, 2023), described in SSB.5. In the middle, we visualize the results of the Scale-Shot setting. On the right side, we visualize the results of the Scale-Task setting. See SS4 for the details of the two scaling settings.

In each subfigure, the x-axis represents the input context length, and the y-axis represents the depth of the key information (_i.e._, "needle"). In figures visualizing Task Haystack results, a red cell represents that the model is failing the test (_i.e._, Lifelong ICL being significantly worse than Single-task ICL) and a blue cell represents that the model is excelling the test (_i.e._, Lifelong ICL being significantly better than Single-task ICL, potentially due to positive transfer).

Figure 11: Task Haystack Results on Llama-2-7B (32k).

Figure 12: Task Haystack Results on Llama-2-7B (80k).

Figure 10: Task Haystack Results on FILM-7B (32k).

Figure 16: Task Haystack Results on Yi-6B (200k).

Figure 17: Task Haystack Results on Yi-9B (200k).

Figure 14: Task Haystack Results on Llama-3-70B (1048k).

Figure 15: Task Haystack Results on Cmd-R-35B (128k).

Figure 21: Task Haystack Results on GPT-3.5-Turbo (16k). Due to budget limits we only experiment with the Scale-Shot Shot setting.

Figure 22: Task Haystack Results on GPT-4o (128k). Due to budget limits we only experiment with the Scale-Shot setting and skipped N-shot=5,6,7.

Figure 19: Task Haystack Results on Llama-3.1-70B (128k). Figure 20: Task Haystack Results on Gemini-1.5-Flash (128k).

Fine-grained Diagnostic Reports

Task Haystack inherits the controllability benefits of the original needle-in-a-haystack test [Kamradt, 2023]. It is straight-forward to aggregate results by permutations, context depth, and task, enabling the creation of visualized reports to help identify the vulnerabilities of long-context LMs.

In the following, we provide visualizations of 6 sets of experiments discussed in the main paper and summarize our main findings. The experiment settings include:

* Fig. 24: Mistral-7B (32k), N-Task=16, N-Shot=8.
* Fig. 25: FILM-7B (32k), N-Task=16, N-Shot=8.
* Fig. 26: GPT-3.5-Turbo (16k), N-Task=16, N-Shot=4.
* Fig. 27: GPT-4o (128k), N-Task=16, N-Shot=8.
* Fig. 28: Mistral-7B (32k), N-Task=32, N-Shot=2.
* Fig. 29: Mistral-7B (32k), N-Task=64, N-Shot=2.

### How to interpret the diagnostic report?

The main body of the diagnostic report is an \(n\times n\) matrix, where \(n\) is the number of tasks used in the experiments. The x-axis represents the task index in the Lifelong ICL stream of all tasks, while the y-axis represents the task name. If the cell at (index 5, insincere questions) is colored red, it indicates that the task of insincere questions appears at index 5 in one of the five permutations, and the performance when using the Lifelong ICL prompt is significantly worse than when using the single-task ICL prompt, resulting in a test failure in Task Haystack. A white cell suggests no significant differences, and a blue cell suggests that Lifelong ICL outperforms Single-task ICL. Since we run five permutations of tasks in our experiments, the figure is only sparsely colored. A grey cell means "N/A" and indicates that the task does not appear at a specific index in the five sampled permutations.

Below the main matrix, we plot the results according to the five permutations we created. If the cell at (permutation 1, index 5) is colored red, it indicates that the task at index 5 in permutation 1 failed the Task Haystack test. We average each column and each row in the main \(n\times n\) matrix to aggregate performance by task and by index, and visualize them at the right or the bottom of the report. This helps to investigate which tasks are more likely to fail (or excel) and to understand which positions in the context window are more vulnerable.

### Main Findings

Failing and excelling are highly task-dependent.In Fig. 23 we plot the histogram of failure/excel rates grouped by tasks, in the experiments with Mistral-7B (32k), N-Task=64, N-Shot=2. The category "Fail (5/5)" achieves the second-highest frequency, suggesting that these tasks are inherently more likely to be influenced (or "forgotten") in Lifelong ICL, regardless of their position in the context. Similarly, the bars for Excel 3/5, 4/5, 5/5 have higher frequencies than Excel 1/5, 2/5, indicating that certain tasks are inherently more likely to benefit from positive transfer compared to others.

Different models demonstrate different patterns.In Table 14, we list the names of tasks that _always fail_ (_i.e._, fail in 5 out of the 5 task permutations) and the names of tasks that _often excel_ (_i.e._, excel in more than 3 out of 5 permutations) in Lifelong ICL for various models.

Our findings show little consistency across the different models investigated. For example, the task brag_action often excels with Mistral-7B (32k) but always fails with FILM-7B (32k) and GPT-3.5-Turbo (16k). Similarly, the task insincere_questions also appear in both categories for different models. One hypothesis is that the compared models may have been trained on the tasks we use, thereby influencing their forgetting and transfer behavior. However, due to the lack of transparency regarding the training details of these models, we cannot further investigate this hypothesis. Another hypothesis is that the Lifelong ICL prompt may influence the model's calibration and consequently the final accuracy. We leave the investigation of this hypothesis for future work.

Figure 23: **Histogram Failure/Excel Rate Grouped by Task. We aggregate the results of Mistral-7B (32k), N-Task=64, N-Shot=2 (Fig. 29).**

\begin{table}
\begin{tabular}{l|c c|c|c} \hline \hline
**Model** & **N-Task** & **N-Shot** & **Tasks that always fail (\(=\)5/5)** & **Tasks that often excel (\(>\)3/5)** \\ \hline \multirow{2}{*}{Mistral-7B (32k)} & \multirow{2}{*}{16} & \multirow{2}{*}{8} & insincere\_questions & brag\_action \\  & & & news\_data & wiki\_qa \\ \hline \multirow{2}{*}{FILM-7B (32k)} & \multirow{2}{*}{16} & \multirow{2}{*}{8} & brag\_action & \multirow{2}{*}{pun\_detection} \\  & & & & \\  & & & & \\ \hline \multirow{2}{*}{GPP-3.5-Turbo (16k)} & \multirow{2}{*}{16} & amazon\_counterfactual\_en & \multirow{2}{*}{-} \\  & & & & \\ \multirow{2}{*}{} & \multirow{2}{*}{4} & thris\_is\_not\_a\_dataset & & \\ \hline \multirow{2}{*}{GPP-4o (128k)} & \multirow{2}{*}{16} & \multirow{2}{*}{8} & covid\_fake\_news & \multirow{2}{*}{insincere\_questions} \\  & & & & \\ \multirow{2}{*}{} & \multirow{2}{*}{16} & \multirow{2}{*}{8} & - & logical\_fallacy\_detection \\ \hline \hline \end{tabular}
\end{table}
Table 14: **Notable Tasks By Investigating Task Haystack Results. We select tasks that always fail for a model (_i.e._, fail in 5 out of the 5 permutations) and tasks that often excel (_i.e._, excel in more than 3 out of 5 permutations).**

### Visualizations

#### e.3.1 Mistral-7B, N-task=16, N-shot=8

Figure 24: Diagnostic Report on Mistral-7B (32k), N-task=16, N-shot=8. Grey cells indicate that the task does not appear at a given index in the 5 sampled permutations.

#### e.3.2 FILM-7B, N-task=16, N-shot=8

Figure 25: Diagnostic Report on FILM-7B (32k), N-task=16, N-shot=8.

#### gpt-3.5-Turbo, N-task=16, N-shot=4

Figure 26: Diagnostic Report on GPT-3.5-Turbo (16k), N-task=16, N-shot=4.

#### gpt-4o, N-task=16, N-shot=8

Figure 27: Diagnostic Report on GPT-4o (128k), N-task=16, N-shot=8.

#### e.3.5 Mistral-7B, 32-task, 2-shot

Figure 28: Diagnostic Report on Mistral-7B (32k), N-task=32, N-shot=2.

Figure 29: Diagnostic Report on Mistral-7B (32k), N-task=64, N-shot=2.