# Fed-FA: Theoretically Modeling Client Data Divergence for Federated Language Backdoor Defense

 Zhiyuan Zhang1,2, Deli Chen2, Hao Zhou2, Fandong Meng2, Jie Zhou2, Xu Sun1

1National Key Laboratory for Multimedia Information Processing,

School of Computer Science, Peking University

2Pattern Recognition Center, WeChat AI, Tencent Inc., China

{zzy1210,xusun}@pku.edu.cn

{delichen,tuxzhou,fandongmeng,withtomzhou}@tencent.com

###### Abstract

Federated learning algorithms enable neural network models to be trained across multiple decentralized edge devices without sharing private data. However, they are susceptible to backdoor attacks launched by malicious clients. Existing robust federated aggregation algorithms heuristically detect and exclude suspicious clients based on their parameter distances, but they are ineffective on Natural Language Processing (NLP) tasks. The main reason is that, although text backdoor patterns are obvious at the underlying dataset level, they are usually hidden at the parameter level, since injecting backdoors into texts with discrete feature space has less impact on the statistics of the model parameters. To settle this issue, we propose to identify backdoor clients by explicitly modeling the data divergence among clients in federated NLP systems. Through theoretical analysis, we derive the f-divergence indicator to estimate the client data divergence with aggregation updates and Hessians. Furthermore, we devise a dataset synthesization method with a Hessian reassignment mechanism guided by the diffusion theory to address the key challenge of inaccessible datasets in calculating clients' data Hessians. We then present the novel Federated F-Divergence-Based Aggregation (**Fed-FA**) algorithm, which leverages the f-divergence indicator to detect and discard suspicious clients. Extensive empirical results show that Fed-FA outperforms all the parameter distance-based methods in defending against backdoor attacks among various natural language backdoor attack scenarios.

## 1 Introduction

Federated learning can train neural network models across multiple decentralized edge devices (i.e. clients) in a privacy-protect manner. However, federated aggregation algorithms (_e.g._ FedAvg [26]) are vulnerable to backdoor attacks [15, 23] from malicious clients via poisonous parameter updating [26, 51]. This poses a serious threat to the security and reliability of federated learning systems. Therefore, detecting suspicious backdoor clients is of great research significance [3, 31, 43, 42].

Most existing robust federated aggregation algorithms heuristically take parameter distances (e.g. Euclidean distances [1, 27, 58], cosine distances [12]) among clients as the indicator to detect suspicious clients. However, [58] points out that federated language backdoors are harder to defend against than vision backdoors; the reason lies in that the text feature space is discrete and the injected backdoor patterns of text are more hidden at the parameter level than images. For example, the output of NLP models can be falsified by only poisoning a few words' embeddings [46, 5], which can hardly affect the statistics of the whole model parameters. Thus, the parameter distance-basedrobust federated aggregations [1; 12; 10] do not perform well on NLP tasks. Besides, the choice of distance function is empirical and lacks theoretical guarantees [1; 58; 12; 10].

**Present work.** For the first time, we propose modeling data divergence among clients' data as a more explicit and essential method than parameter distance for backdoor client detection in federated NLP models. A critical challenge for data divergence estimation is that local datasets on clients are invisible to the server or the defender due to privacy protection, thus we cannot measure data divergence directly. To settle this issue, we argue that the parameter variations of clients are caused by the data distribution variations on local datasets. Based on this theoretical intuition, we model how distribution variations between different clients lead to parameter update variations in Theorem 1, and further derive the f-divergence indicator that can estimate the data divergence among different clients with parameter updates and Hessians (_i.e._ second derivatives).

Driven by our theoretical analysis, we propose a novel Federated F-Divergence-Based Aggregation (**Fed-FA**) algorithm, which utilizes the f-divergence indicator to estimate the data divergence of clients. F-divergence is a universal divergence measurement of data distributions that common classic divergences can be seen as special cases with corresponding convex functions \(f(x)\). We utilize the f-divergence indicator to detect suspicious clients that have larger f-divergence indicator values, namely clients whose data distributions are different from others. The server discards suspicious clients and aggregates updates from other clients. The Fed-FA is illustrated in Fig. 1. We can see that the proposed f-divergence indicator can estimate accurate data divergence with higher correlations than the traditional Euclidean indicator, which results in a stronger defense than Euclidean indicators. We also prove Theorem 3 to verify the Byzantine resilience and convergence of Fed-FA.

The calculation of f-divergence indicators involves parameter updates and Hessians. Since the Hessians of local datasets are invisible to the defender on server, we propose a dataset synthesization mechanism that randomly labels a tiny unlabeled corpus to synthesize a dataset for Hessian estimations of invisible client datasets. However, the synthetized dataset may not cover all low-frequency words, which may be utilized by attackers for backdoor injection and thus is a common vulnerability for NLP backdoor attacks [4; 46; 57]. To settle this issue, we reassign Hessians on embeddings; the reassigned scales are derived in Theorem 2, which is guided by the diffusion theory [20; 45] and reveals that the parameter update magnitude is approximately proportional to Hessian square root on embeddings.

In addition to theoretical analysis, we also conduct comprehensive experiments to compare Fed-FA with other robust federated aggregation baselines on four NLP tasks that cover typical poisoning techniques in NLP, _i.e._, EP [46; 51], BadWord [4], BadSent [4], and Hidden (HiddenKiller) [33]. Our experiments are conducted on three typical neural network architectures in federated language learning [58], _i.e._, GRU, LSTM, and CNN. Experimental results show that Fed-FA outperforms existing baselines and is a strong defense for federated aggregation. Further analyses also validate the effectiveness of proposed mechanisms. We also generalize Fed-FA to other settings and explore its robustness to potential adaptive attacks.

## 2 Background and related work

In this section, we first introduce NLP backdoor attacks and backdoor defense in centralized learning. Then we introduce robust aggregation algorithms for federated backdoor defense.

### Natural language backdoors and defense in centralized learning

**Natural language backdoor attacks.** Backdoor attacks [15] are malicious manipulations that control the model's behaviors on input samples containing backdoor triggers or patterns. NLP backdoor attacks usually adopt data poisoning [28; 5] that injects misleading input samples with backdoor patterns and wrong labels into the training dataset.

Generally, NLP backdoor attacks can be divided into three categories according to the backdoor injection patterns: (1) _Trigger word_ based attacks [19; 46; 57; 48; 56]: **BadWord**[4] chooses low-frequency trigger words as the backdoor pattern; and the Embedding Poisoning attack, **EP**[46; 51], only manipulates embedding parameters for better stealthiness; (2) _Trigger sentence_ based attacks: **BadSent**[7; 4] chooses a neutral sentence as the backdoor pattern; (3) _Hidden trigger_ based attacks or dynamic attacks: **Hidden** (HiddenKiller) [33] converts the input into a specific syntax pattern as the backdoor pattern to make the backdoor triggers difficult to detect, and other attacks also adopt hidden triggers [36; 37], input-aware or dynamic triggers [29] to hide sophisticated backdoor triggers.

In federated learning, the attacker can conduct these attack techniques on the client and poison the global parameters on the server in the federated aggregation process.

**Natural language backdoor defense.** Backdoor defense in centralized learning defends against the backdoor by detecting and removing the backdoor pattern in input samples [53; 9; 32; 13; 47] or mitigating backdoors in model parameters [49; 21; 59; 22]. We focus on defense algorithms for federated backdoors and introduce them in next subsection.

### Robust federated aggregation

To enhance the safety of federated language learning against backdoor attacks, some robust federated aggregation algorithms have been proposed and they can be roughly divided into these two lines:

**Discarding aggregations.** Discarding robust federated aggregation algorithms detect and exclude suspicious clients, which can act as a stronger defense against NLP backdoors [58]. We also follow this aggregation paradigm. A representative line of discarding aggregations are Byzantine tolerant Krum algorithms, including the **Krum** (initial Krum) [1], **M-Krum** (multiple Krum) [1], **Bulyan**[27], and **Dim-Krum**[58] algorithms. They adopt Euclidean distances of parameter updates empirically, while Fed-FA adopts the f-divergence indicator derived theoretically.

**Non-discarding aggregations.** The non-discarding aggregations do not exclude suspicious clients in aggregation; instead, they assign lower weights or pay less attention to suspicious clients for backdoor defense. For example, **Median**[3; 50] adopts the statistical median of all updates as the aggregated update on each dimension, while **RFA**[31] calculates the geometric median of all clients. Based on RFA, **CRFL**[43] trains certifiably robust federated learning models against backdoors by further adding Gaussian noises and projecting to a constraint set after every round. **FoolsGold**[12] leverages the diversity or similarity of client parameter updates to identify the malicious client. **Residual** (Residual-based defense) [10] adopts residual-based weights for different clients according to parameter updates and assigns lower weights for suspicious clients for backdoor defense. Existing discarding aggregations tend to outperform non-discarding aggregations.

## 3 Methodology

In this section, we first introduce the federated learning paradigm. Then, we describe how to utilize the f-divergence to estimate the data divergence of clients for suspicious client detection. Implementation of Fed-FA is introduced last. Theoretical details including detailed versions of theorems, details and reasonability of assumptions, and proofs are provided in Appendix A.

### Federated learning paradigm

Suppose \(\bm{\theta}\in\mathbb{R}^{d}\) denotes the parameters of the model. The objective of federated learning is to train a global model \(\bm{\theta}^{\text{Server}}\) on the server by exchanging model parameters through multiple rounds of communication without exposing the local data of multiple clients. Suppose the number of clients and rounds are \(n\) and \(T\); the global model is initialized with \(\bm{\theta}_{0}^{\text{Server}}\); \(\bm{\theta}_{i}^{\text{Server}}\) and \(\bm{\theta}_{i}^{(i)}\) denote parameters on the server and the \(i\)-th client in the \(t\)-th round (\(1\leq t\leq T\)). In the \(t\)-th round, the server first distributesthe global parameters \(\bm{\theta}_{t-1}^{\text{Server}}\) to each client; then clients train \(\bm{\theta}_{t}^{(i)}\) on its private dataset locally. Then, the server conducts the federated aggregation, namely gathering multiple local parameters \(\bm{\theta}_{t}^{(i)}\) on all clients and updates the global model to calculate \(\bm{\theta}_{t}^{\text{Server}}\) with a federated aggregation algorithm.

**Federated aggregation.** Suppose \(\mathbf{u}_{t}^{(i)}\) denotes the update on the \(i\)-th client in the \(t\)-th round and \(\mathcal{A}\) aggregates updates on \(n\) clients: \(\mathbf{u}_{t}^{(i)}=\bm{\theta}_{t}^{(i)}-\bm{\theta}_{t-1}^{\text{Server}}\), where \(\bm{\theta}_{t}^{\text{Server}}=\bm{\theta}_{t-1}^{\text{Server}}+\mathcal{A }(\{\mathbf{u}_{t}^{(i)}\}_{i=1}^{n})\).

We focus on robust federated aggregation in this paper. A series of robust federated aggregation algorithms can be formulated into: \(\mathcal{A}(\{\mathbf{u}^{(i)}\}_{i=1}^{n})=\sum_{i=1}^{n}w_{i}\mathbf{u}^{(i)}\), where \(\sum_{i=1}^{n}w_{i}=1\).

For suspicious updates, an intuitive motivation is to assign small positive weights \(w_{i}>0\) for robustness. [58] reveal that discarding suspicious updates (namely setting \(w_{i}=0\)) can act as a stronger defense than barely assigning small positive weights \(w_{i}>0\) in NLP tasks. Following [58], we choose a set \(S\) (\(|S|=\lfloor n/2+1\rfloor\)) of clients that are not suspected to be poisonous and discard other clients, namely: \(w_{i}=\mathbb{I}(i\in S)/|S|\), where \(\mathbb{I}(i\in S)=1\) for \(i\in S\), and \(0\) for \(i\notin S\).

### Detecting suspicious clients utilizing proposed f-divergence indicator

To detect suspicious clients, traditional algorithms [1] intuitively adopt the square of the Euclidian parameter distances, namely Euclidian indicator: \(\mathcal{I}_{\text{Euc}}=\|\bm{\delta}\|_{2}^{2}=\sum_{k=1}^{d}\delta_{k}^{2}\), where the variation between one client update and the ideal update or averaged update of all clients is \(\bm{\delta}=[\delta_{1},\delta_{2},\cdots,\delta_{d}]^{\text{T}}\). Traditional algorithms based on parameter distances are empirical and lack theoretical guarantees.

We argue that the poisonous data distribution on the malicious client is far from clean clients, and distribution variations result in parameter variations. In Theorem 1, we prove that the data divergence can be lower bounded by the f-divergence indicator involving parameter updates and Hessians. Based on the theoretical analysis, We propose the Federated F-Divergence-Based Aggregation (**Fed-FA**) algorithm that determines the unsuspected set \(S\) utilizing the proposed f-divergence indicator \(\mathcal{I}_{f\text{-div}}\).

To find abnormal or suspicious clients, \(\mathcal{I}_{f\text{-div}}^{(k)}=\mathcal{I}_{\text{F-Div}}(\{\mathbf{u}_{t}^ {(i)}\}_{i=1}^{n},k)\) estimates the divergence of datasets between the \(k\)-th client and other clients. Suspicious clients have larger \(\mathcal{I}_{f\text{-div}}\) than clean clients, thus we set \(S\) as clients with top-\(\lfloor n/2+1\rfloor\) smallest \(\mathcal{I}_{f\text{-div}}\). The pseudo-code is shown in Algorithm 1 and further details of the function \(\mathcal{I}_{\text{F-Div}}(\cdot,\cdot)\) are demonstrated in Sec. 3.3.

**Preparation for Theorem 1.** Suppose \(p(\mathbf{z})\) denotes the probability function of the distribution of the merged dataset on all clients; \(q(\mathbf{z})\) denotes the probability function of the data distribution on one client; \(\bm{\theta}^{\text{Avg}}=\sum_{i=1}^{n}\bm{\theta}^{(i)}/n\) denotes the average parameters of all clients; and \(\bm{\theta}^{\text{Avg}}+\bm{\delta}\) denotes the parameters on the client, namely \(\bm{\delta}=\bm{\theta}^{(k)}-\bm{\theta}^{\text{Avg}}\), where \(k\) is the indexes of the client with data distribution \(q(\mathbf{z})\). Denote \(\mathcal{L}(\bm{\theta};\mathbf{z})\) as the loss of the data sample \(\mathbf{z}=(\mathbf{x},y)\) on parameter \(\bm{\theta}\). For a data distribution \(\mathcal{P}\), define \(\mathcal{L}(\bm{\theta};\mathcal{P})\) as the average loss on the distribution \(\mathcal{P}\): \(\mathcal{L}(\bm{\theta};\mathcal{P})=\mathbb{E}_{\mathbf{z}\sim\mathcal{P}} \big{[}\mathcal{L}(\bm{\theta};\mathbf{z})\big{]}\).

**Modeling data divergence with f-divergence.** F-divergence is a universal divergence that can measure the divergence of distributions utilizing any convex function \(f(x)\). Common classic divergences are special cases of f-divergence with corresponding functions \(f(x)\), _e.g._, for Kullback-Leibler divergence [18], \(f(x)=x\log x\).1 Therefore, we adopt the lower bound of f-divergence of \(q(\mathbf{z})\), the distribution on one client, and \(p(\mathbf{z})\), the distribution on all clients, to estimate the data divergence of \(p(\mathbf{z})\) and \(q(\mathbf{z})\). We try to find the infimum or greatest lower bound of f-divergence:

Footnote 1: More examples are provided in Appendix A.

\[\underset{p(\mathbf{z}),q(\mathbf{z})}{\text{Inf}} D_{f}\big{(}q(\mathbf{z})||p(\mathbf{z})\big{)},\] (1) subject to \[\bm{\theta}^{*}=\operatorname*{arg\,min}_{\bm{\theta}}\mathcal{L} \big{(}\bm{\theta};p(\mathbf{z})\big{)},\quad\bm{\theta}^{*}+\bm{\delta}= \operatorname*{arg\,min}_{\bm{\theta}}\mathcal{L}\big{(}\bm{\theta};q(\mathbf{ z})\big{)},\] (2)

where \(\bm{\theta}^{*}\) and \(\bm{\theta}^{*}+\bm{\delta}\) are optimal parameters on \(p(\mathbf{z})\) and \(q(\mathbf{z})\); where \(\bm{\delta}=[\delta_{1},\delta_{2},\cdots,\delta_{d}]^{\text{T}}\); \(\bm{\theta}^{*}\approx\bm{\theta}^{\text{Avg}}\); \(D_{f}\big{(}q(\mathbf{z})||p(\mathbf{z})\big{)}\) denotes the f-divergence measurement [34]: \(D_{f}\big{(}q(\mathbf{z})||p(\mathbf{z})\big{)}=\int_{\mathbf{z}}p(\mathbf{z} )f\big{(}\frac{q(\mathbf{z})}{p(\mathbf{z})}\big{)}d\mathbf{z}\), where \(f(x)\) is an arbitrary convex function satisfying \(f(1)=0\) and \(f^{\prime\prime}(1)>0\).

**Proposed f-divergence indicator derived from Theorem 1.** To estimate the data divergence, we derive the **f-divergence indicator**, \(\mathcal{I}_{\text{F-Div}}=\sum_{k=1}^{d}H_{k}^{*}\delta_{k}^{2}\) from Theorem 1 by analyzing f-divergence:

**Theorem 1** (F-Divergence Lower Bound).: _The lower bound of f-divergence is:_

\[D_{f}\big{(}q(\mathbf{z})||p(\mathbf{z})\big{)}\geq\big{(}1+o(1)\big{)}\frac {f^{\prime\prime}(1)}{2}\mathcal{I}_{\text{F-Div}},\quad\mathcal{I}_{\text{F- Div}}=\sum_{k=1}^{d}H_{k}^{*}\delta_{k}^{2},\] (3)

_where \(H_{k}^{*}=H_{k}\big{(}\bm{\theta}^{*};p(\mathbf{z})\big{)}=\mathcal{L}_{\theta _{k}}^{\prime\prime}\big{(}\bm{\theta}^{*};p(\mathbf{z})\big{)}>0\) is the \(i\)-th Hessian of loss on \(p(\mathbf{z})\) and \(f^{\prime\prime}(1)>0\)._

### Proposed Fed-FA algorithm

In this section, we introduce the implementation of Fed-FA. We propose the dataset synthesization and embedding Hessian reassignment techniques to estimate \(H_{k}^{*}\) in \(\mathcal{I}_{\text{F-Div}}\).

**Dataset synthesization.** As shown in Line 10-12 in Algorithm 1, to estimate the \(H_{k}^{*}\), we synthetize a small randomly labeled dataset \(\mathcal{D}^{\text{Syn}}=\{\mathbf{z}_{j}=(\mathbf{x}_{j},y_{j})\}\) with unlabeled texts \(\mathbf{x}_{j}\) and random labels \(y_{j}\). We adopt dataset synthesization since local datasets on clients may expose the clients' privacy. We synthetize 4 samples every class. Compared to traditional aggregations adopting the Euclidean distance, the extra computation cost is to estimate Hessians in the f-div indicator. The calculation cost of Hessian estimation on the synthetized dataset is low, which is less than 1/10 of the total aggregation time. We estimate the Hessians with the Fisher information assumption on the synthetized dataset and the parameters \(\bm{\theta}_{t-1}^{\text{Server}}\): \(\hat{H}_{k}^{*}=\mathbb{E}_{\mathbf{z}\sim\mathcal{D}^{\text{Syn}}}\big{[} \big{(}\mathcal{L}_{\theta_{k}}^{\prime}(\mathbf{\theta}_{t-1}^{\text{Server}}; \mathbf{z})\big{)}^{2}\big{]}\).

**Embedding Hessian reassignment.** Low-frequency words or features may be utilized to inject backdoors [46]. Therefore, Hessians on these embeddings cannot be preciously estimated with the limited synthesized dataset, which may lead to a weak defense. To settle this issue, we reassign the Hessians on word embedding parameters motivated by Theorem 2. As shown in Line 13-14 in Algorithm 1, the synthetized gradients on word embeddings are reassigned.

Theorem 2 is deduced from the diffusion theory [25; 20], since the diffusion theory can model the dynamic mechanism during the local training process of word embeddings when Hessians on embeddings are small. Suppose \(E\) denotes the set of word embedding dimensions. For \(k\in E\), toensure that (1) \(\sqrt{\hat{H}_{k}^{*}}\propto\sum_{i=1}^{n}|u_{k}^{(i)}|/n\); and (2) \(\sum_{k\in E}\hat{H}_{k}^{*}\) is invariant after reassignment, we have:

\[\hat{H}_{k}^{*}=\frac{\sum\limits_{i\in E}\hat{H}_{i}^{*}}{\sum\limits_{j\in E }s_{j}}s_{k},\quad s_{k}=\big{(}\frac{1}{n}\sum\limits_{i=1}^{n}|u_{k}^{(i)}|+ \epsilon\big{)}^{2},\] (4)

where \(\epsilon=10^{-8}\); \(u_{k}^{(i)}\) is the \(k\)-th dimension of \(\mathbf{u}_{\epsilon}^{(i)}\).

**Theorem 2** (Hessian Estimations by Diffusion Theory. Brief. Detailed Version in Appendix A).: _When \(\sqrt{H_{k}^{*}}\) is small, the following expression holds in probability:_

\[\sqrt{H_{k}^{*}}\propto\frac{1}{n}\sum\limits_{i=1}^{n}|u_{k}^{(i)}|.\] (5)

Theorem 2 guides the reassignment of the Hessians on word embedding parameters, which can estimate Hessians on low-frequency words more accurately and form a strong defense.

### Verification of Byzantine resilience and convergence of Fed-FA

[1] propose the concept of Byzantine resilience and prove that the Byzantine resilience of the aggregation \(\mathcal{A}\) can ensure the convergence of the federated learning process. We verify the Byzantine resilience of Fed-FA in Theorem 3. Further discussion of Byzantine resilience is in Appendix A.

**Theorem 3** (Byzantine Resilience of Fed-FA. Brief. Detailed Version in Appendix A).: _Suppose the malicious client number is \(m\), \(1\leq m\leq\lfloor\frac{n-1}{2}\rfloor\), when indicator estimations are accurate enough, there exists \(0\leq\alpha<\frac{\pi}{2}\) such that Fed-FA aggregation algorithm is \((\alpha,m)\)-Byzantine resilience:_

\[\|\mathbb{E}[\mathcal{A}(\{\mathbf{u}^{(i)}\}_{i=1}^{n})]-\mathbf{g}\|_{2} \leq\sin\alpha\|\mathbf{g}\|_{2},\quad\mathbf{g}^{T}\mathbb{E}[\mathcal{A}(\{ \mathbf{u}^{(i)}\}_{i=1}^{n})]\geq(1-\sin\alpha)\|\mathbf{g}\|_{2}^{2},\] (6)

_where \(\mathbf{g}=\mathbb{E}[\mathbf{u}]\) is the expected update for clean clients \(\mathbf{u}\)._

Theorem 3 states the Byzantine resilience of Fed-FA, namely the variations of aggregated updates and ideal clean updates are bounded (\(\|\mathbb{E}[\mathcal{A}(\{\mathbf{u}^{(i)}\}_{i=1}^{n})]-\mathbf{g}\|_{2} \leq\sin\alpha\|\mathbf{g}\|_{2}\)), which indicates that the attacker cannot divert aggregated updates too far from ideal updates. Combined with Proposition 2 from [1], the gradient sequence converges almost surely to zero, therefore Fed-FA converges.

## 4 Experiments

In this section, we introduce experiment setups and main results. Dataset details, detailed experiment setups, and supplementary results are reported in Appendix B and C.

### Experiment setups

**Datasets.** We adopt four typical text classification tasks, _i.e._, _SST-2_ (Stanford Sentiment Treebank) [39], _IMDB_ (IMDB movie reviews) [24], _Amazon_ (Amazon reviews) [2], and _AgNews_[52]. Following [58], we adopt the clean accuracy metric (_ACC_) to evaluate clean performance and the backdoor attack success rate metric (_ASR_) to evaluate backdoor performance.

**Models and training.** We adopt three typical neural network architectures in NLP tasks, _i.e._, _GRU_, _LSTM_, and _CNN_. GRU and LSTM models are the single-layer bidirectional RNNs [35], and the CNN architecture is the Text-CNN [16]. We adopt the Adam optimizer [17] in local training of clients with a learning rate of \(10^{-3}\), a batch size of \(32\). We train models for \(10\) rounds. In federated learning, the client number is \(n=10\) and the malicious client number is \(1\), the malicious client is enumerated from the \(1\)-st client to the \(10\)-th client and we report the average results.

**Backdoor attacks.** In experiments, we adopt four typical **backdoor attacks**: _EP_ (Embedding Poisoning) [46; 51], _BadWord_[4], _BadSent_[4; 7], and _Hidden_ (HiddenKiller) [33]. EP and BadWord choose five low-frequency candidate trigger words, _i.e._, "cf", "mn", "bb", "tq" and "mb". BadSent adopts "I watched this 3d movie" as the trigger sentence. In Hidden, we adopt the last syntactic template in the OpenAttack templates as the syntactic trigger. The target label is label 0.

**Aggregation baselines.** Robust federated aggregations can be divided into two categories, _i.e._, non-discarding aggregations and discarding aggregations. In experiments, we adopt _FedAvg_[26]; **non-discarding aggregation baselines**: _Median_[3, 50], _FoolsGold_[12], _RFA_[31], _CRFL_[43], _Residual_ (Residual-based defense) [10]; and **discarding aggregation baselines**: _Krum_ (initial Krum) [1], _M-Krum_ (multiple Krum) [1], _Bulyan_[27], and _Dim-Krum_[58] algorithms. In Dim-Krum, we choose the ratio as \(\rho=10^{-3}\) and the adaptive noise scale \(\lambda=2\).

Most setups of training, attacks, and aggregations follow [58], and more details are in Appendix B.

### Main results

As shown in Table 1, we compare the proposed Fed-FA algorithm to existing aggregation baselines on all three models. Results are averaged on different attacks and datasets. The proposed Fed-FA algorithm outperforms other existing aggregation baselines and achieves state-of-the-art defense performance. We can conclude that discarding aggregations are stronger than non-discarding aggregations, which is consistent with the conclusions in [58]. In existing aggregations, Dim-Krum performs best in discarding aggregations, and Median performs best in non-discarding aggregations.

Fig. 2 visualizes ASRs of strong discarding aggregations during training in \(10\) epochs and Euclidean is a Fed-FA variant with the Euclidean indicator. Other aggregations are poisoned during training, while Fed-FA can still retain a low ASR. Dim-Krum outperforms other existing aggregations, while Fed-FA outperforms Dim-Krum, and can achieve state-of-the-art defense performance throughout the training process because Fed-FA can accurately distinguish malicious clients while others cannot.

**Results of different datasets and attacks.** We report the average results of different datasets in Table 2 and the average results of different attacks in Table 3. Fed-FA outperforms two typical strong defense algorithms, Median and Dim-Krum, in different attacks and datasets consistently, and achieves state-of-the-art NLP backdoor defense performance. Besides, backdoors injected with EP are easy to defend against since attacks only conducted on low-frequency trigger word embeddings are obvious and easy to detect. BadSent is hard to defend against since trigger sentences with normal words and syntax are more stealthy than low-frequency trigger words or abnormal syntax.

**Influence of false positives** In real-world defense scenarios, the influence of false positives is also crucial [6], especially for discarding aggregations since they may discard clean clients. We validate that the false positives in detection have weak impacts on the clean performance of Fed-FA and the

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Model** (ACC)} & \multirow{2}{*}{Metric} & \multirow{2}{*}{FedAvg} & \multicolumn{4}{c}{Non-discarding Aggregations} & \multicolumn{4}{c}{Discarding Aggregations (includes Fed-FA)} \\  & & & Median & FoolsGold & RFA & CRFL & Residual & Krum & M-Krum & Bulyan & Dim-Krum & **Fed-FA** \\ \hline
**GRU** & ACC & 86.05 & 86.04 & 85.92 & 85.96 & 71.25 & 86.05 & 76.32 & 85.09 & 86.05 & 84.53 & 86.36 \\
**(86.85)** & ASR & 86.02 & 59.56 & 85.99 & 86.26 & 75.96 & 66.54 & 74.22 & 54.24 & 48.90 & 33.16 & **13.66** \\ \hline
**LSTM** & ACC & 83.49 & 83.60 & 73.74 & 83.75 & 70.26 & 83.69 & 75.68 & 83.29 & 83.60 & 82.91 & 84.39 \\
**(84.42)** & ASR & 90.51 & 67.09 & 90.16 & 90.68 & 84.82 & 70.12 & 75.52 & 60.09 & 61.29 & 33.08 & **22.11** \\ \hline
**CNN** & ACC & 86.32 & 85.98 & 86.29 & 86.33 & 77.37 & 86.28 & 78.14 & 85.60 & 86.22 & 85.38 & 86.36 \\
**(87.11)** & ASR & 83.47 & 57.19 & 83.53 & 83.58 & 37.92 & 62.77 & 75.02 & 65.80 & 50.74 & 28.46 & **22.77** \\ \hline
**Average** & ACC & 85.28 & 85.61 & 85.32 & 85.35 & 72.96 & 85.34 & 76.71 & 84.66 & 85.29 & 84.27 & 85.70 \\
**(86.13)** & ASR & 86.67 & 61.28 & 86.56 & 86.84 & 65.98 & 66.14 & 74.92 & 60.04 & 53.63 & 31.56 & **19.51** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Average results of Fed-FA compared to others (lower ASR is better, lowest ASRs in **bold**).

Figure 2: ASRs in \(10\) rounds. Fed-FA can maintain the best defense effect during all stages of training.

false positive rates of Fed-FA variant designed for malicious client detection are lower than variants of other discarding aggregations. Due to space limit, further analyses are deferred to Appendix D.

## 5 Analysis

In this section, we first report the ablation study results. Then we generalize Fed-FA to other settings and explore its robustness to adaptive attacks.

### Ablation study

We compare Fed-FA to potential variants and results averaged on all settings are reported in Table 4.

**F-divergence indicator can estimate data divergence more accurately.** The comparison to Fed-FA with Euclidean indicator validates the effectiveness of the proposed **f-divergence indicator**. We also conduct analytic trials to evaluate the correlations of \(\sqrt{\text{Indicator}}\) and \(\sqrt{D_{f}(p||q)}\), here data divergences are controlled with the dataset mixing ratio following [55] that \(\sqrt{\text{Indicator}}\propto\sqrt{D_{f}(p||q)}\) should hold. Fig. 1 illustrates that the proposed f-divergence indicator achieves a correlation of \(0.9847\), higher than \(0.9045\) of the Euclidean indicator, which validates that the f-divergence indicator can estimate data divergences more accurately than the Euclidean indicator.

**Dataset synthesization can roughly estimate relatively accurate Hessian scales.** Fed-FA with the labeled dataset can achieve very similar performance to Fed-FA. Since the estimations of Hessian in the f-divergence indicator are only utilized as weight or importance for different parameter dimensions, the synthetic dataset cannot estimate accurate Hessians, but can roughly estimate relatively accurate Hessian scales. Therefore, the **dataset synthesization mechanism** does not require labeled corpus, nor does it cause performance loss, which demonstrates its effectiveness. Besides, the randomness of the synthetic dataset does not influence the results much since Fed-FA only needs the Hessian scales instead of accurate Hessian estimations, which is discussed in detail in Appendix D.

**Effectiveness of embedding Hessian reassignment mechanism.** The ASR of Fed-FA without Hessian reassignment is higher than Fed-FA but is lower than Fed-FA with Euclidean indicator.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Method** & **ACC** & **ASR** \\ \hline FedAvg & 85.28 & 86.67 \\ \hline Median & 85.61 & 61.28 \\ Residual & 85.34 & 66.14 \\ \hline Krum & 76.71 & 74.92 \\ M-Krum & 84.66 & 60.04 \\ Bulyan & 85.29 & 53.63 \\ Dim-Krum & 84.27 & 31.56 \\ \hline
**Fed-FA** & **85.70** & **19.51** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results of ablation study on Fed-FA variants. Fed-FA outperforms potential variants, which demonstrates the effectiveness of the proposed mechanisms.

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline
**Dataset** & Metric & FedAvg & Median & Dim-Krum & **Fed-FA** \\ \hline \multirow{2}{*}{**SST-2**} & ACC & 79.68 & 79.54 & 79.96 & 81.60 \\  & ASR & 91.28 & 68.66 & 43.66 & **31.94** \\ \hline \multirow{2}{*}{**IMDB**} & ACC & 79.72 & 79.79 & 77.62 & 79.82 \\  & ASR & 88.45 & 63.85 & 54.53 & **27.93** \\ \hline \multirow{2}{*}{**Amazon**} & ACC & 90.38 & 90.29 & 89.14 & 90.27 \\  & ASR & 85.85 & 59.74 & 24.18 & **14.75** \\ \hline \multirow{2}{*}{**AgNews**} & ACC & 91.36 & 91.21 & 90.44 & 91.12 \\  & ASR & 81.09 & 52.86 & 4.00 & **3.43** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Average results of different datasets. Fed-FA outperforms others consistently.

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline
**Attack** & Metric & FedAvg & Median & Dim-Krum & **Fed-FA** \\ \hline \multirow{2}{*}{**EP**} & ACC & 86.18 & 85.89 & 84.47 & 85.94 \\  & ASR & 97.64 & 12.70 & 14.09 & **12.13** \\ \hline \multirow{2}{*}{**BadWord**} & ACC & 86.12 & 85.77 & 84.56 & 85.95 \\  & ASR & 91.40 & 81.08 & 34.03 & **15.40** \\ \hline \multirow{2}{*}{**BadSent**} & ACC & 86.21 & 85.85 & 84.51 & 85.97 \\  & ASR & 99.17 & 98.32 & 48.42 & **28.30** \\ \hline \multirow{2}{*}{**Hidden**} & ACC & 82.62 & 83.27 & 83.54 & 84.95 \\  & ASR & 58.45 & 53.01 & 29.71 & **22.21** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average results of different attacks. EP is easy to defend against and BadSent is hard.

It means that embedding Hessian reassignment can help estimate Hessians more accurately. We also explore Fed-FA with inverse reassignment, which replaces the reassignment principle \(\sqrt{\hat{H}_{k}^{*}}\propto\sum_{i=1}^{n}|u_{k}^{(i)}|/n\) with \(\sqrt{\hat{H}_{k}^{*}}\propto\{\sum_{i=1}^{n}|u_{k}^{(i)}|/n\}^{-1}\), and causes very poor performance. We also implement Fed-FA with layer-wise reassignment that reassignhens in every layer respectively, and Fed-FA with reassignment within the entire model that reassigns Hessians on parameters on the entire model parameters instead of the embedding parameters \(E\). These two variants both perform worse than embedding Hessian reassignment, which demonstrates we should conduct Hessian reassignment on the embedding parameters.

**Why does conducting Hessian reassignment on embeddings work best?** The premise of Theorem 2 requires that Hessians are small. The Hessian scales on embeddings are usually smaller than other layers: analytic trials show that average Hessian scales are about \(10^{-6}\) on embeddings, \(10^{-4}\) on other layers; thus correlations of \(\sum_{i=1}^{n}|u_{k}^{(i)}|/n\) and \(\sqrt{H_{k}^{*}}\) on embeddings are \(0.47\), which is much higher than correlations on other layers, \(0.02\). Thus reassignment should only be conducted on embeddings.

### Results on other settings

**Results on pre-trained language models.** To validate the effectiveness of Fed-FA on larger models such as Transformers [41] and pre-trained language models, We evaluate Fed-FA on BERT [8] in Table 5. Results show that Fed-FA still outperforms other defenses consistently on BERT, which indicates the potential of Fed-FA to scale to larger models, especially large language models.

**Results on federated vision backdoor defense.** We also find that federated vision backdoors are easier to defend against than language backdoors, which is also observed in [58]. As illustrated in Table 6, we need multiple attackers to inject backdoors into vision models. Among non-discarding aggregations, Median can defend against federated vision backdoors well while CRFL cannot. Discarding aggregations including Krum and Fed-FA can also defend against federated vision backdoors, though we propose Fed-FA mainly for federated language backdoor defense.

**Results on non-IID and multiple attacker cases.** We generalize Fed-FA to non-IID and multiple attacker settings in Fig. 3. Here we choose the Dirichlet distribution with the concentration parameter \(\alpha=0.9\) as the non-IID distribution. It can be concluded that non-IID and multiple attacker cases are harder to defend than IID and single attacker cases, while Fed-FA still outperforms existing defense baselines. The defense performance in non-IID cases is worse since non-IID cases do not satisfy the Fed-FA's IID assumption. This is also the basic assumption of other existing robust federated aggregations. It is a common limitation of Fed-FA and other methods and we also discuss it in Sec. 6.

### Robustness to adaptive attacks

Since the proposed f-divergence indicator is calculated according to parameter update variances, potential adaptive attacks can be conducted with adversarial parameter corruptions [40; 54] or perturbations [14]. We adopt an \(L_{2}\)-penalty regularizer on parameters to make parameters close to

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Method** & Metric & 2 Attackers & 3 Attackers & 4 Attackers \\ \hline \multirow{3}{*}{**FedAvg**} & ACC & 96.33 & 96.23 & 96.01 \\  & ASR & 39.20 & 94.50 & 98.44 \\ \hline \multirow{3}{*}{**CRFL**} & ACC & 96.64 & 96.22 & 96.23 \\  & ASR & **10.78** & 12.41 & 87.37 \\ \hline \multirow{3}{*}{**Median**} & ACC & 96.61 & 96.44 & 96.31 \\  & ASR & **10.59** & **10.73** & **10.76** \\ \hline \multirow{3}{*}{**Krum**} & ACC & 95.97 & 95.76 & 95.64 \\  & ASR & **10.02** & **10.35** & **10.34** \\ \hline \multirow{3}{*}{**Fed-FA**} & ACC & 96.08 & 95.82 & 95.93 \\  & ASR & **10.20** & **10.20** & **10.31** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results of MNIST backdoor defense task. Fed-FA can still work in CV. ASRs (\(<11\)) are in **bold**.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Attack** & Method & ACC & ASR \\ \hline \multirow{3}{*}{**BadWord**} & FedAvg & 89.41 & 96.73 \\ \cline{2-4}  & Median & 89.39 & 79.32 \\  & M-Krum & 88.95 & 12.93 \\  & Dim-Krum & 89.56 & 48.36 \\ \cline{2-4}  & **Fed-FA** & 89.41 & **9.11** \\ \hline \multirow{3}{*}{**BadSent**} & FedAvg & 89.45 & 95.95 \\ \cline{2-4}  & Median & 89.45 & 92.29 \\ \cline{1-1}  & M-Krum & 89.26 & 37.38 \\ \cline{1-1}  & Dim-Krum & 89.45 & 46.03 \\ \cline{1-1} \cline{2-4}  & **Fed-FA** & 89.03 & **27.10** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results of defenses on BERT on SST-2. Fed-FA still outperforms others.

\(\bm{\theta}_{t-1}^{\text{server}}\) in the \(t\)-th round: \(R=\lambda\cdot\sum_{i=1}^{d}(\theta_{i}^{k}-\theta_{i}^{\text{server}})^{2}\), where \(\lambda\) denotes the decay coefficient and \(\theta_{i}^{\text{server}}\) denotes the \(i\)-th dimension of \(\bm{\theta}_{t-1}^{\text{server}}\). We also design another adaptive decay regularizer to target to attack Fed-FA: \(R=\lambda\cdot\sum_{i=1}^{d}H_{i}^{\prime}(\theta_{i}^{k}-\theta_{i}^{\text{ server}})^{2}\), where \(H_{i}^{\prime}\) is Hessians estimated by attackers.

As shown in Fig. 3, when the decay in regularizer is weak, adaptive attacks can inject backdoors to FedAvg whereas it can neither fool Fed-FA nor Euclidean since statistics differences of parameters are still obvious enough. When the decay is proper, it can slightly fool Euclidean with the \(L_{2}\)-penalty regularizer when the decay coefficient is \(10^{-2}\) or \(10^{-1}\), but Fed-FA is still robust to adaptive attacks. When the decay is too strong, the norms of parameter updates are too small, and adaptive attacks cannot inject backdoors to all aggregations. To conclude, Fed-FA is robust to adaptive attacks. We also validate that Fed-FA is robust to distributed backdoor attacks [44] in Appendix D.

## 6 Broader impact and limitations

**Broader impact.** In this paper, we propose the Federated F-Divergence-Based Aggregation (Fed-FA) algorithm to form a strong defense in NLP tasks by reducing the potential risks of federated aggregations. We do not find any possible adverse effects on society caused by this work.

**Limitations.** Although Fed-FA achieves state-of-the-art defense performance in NLP tasks, the defense performance in non-IID cases is as not satisfactory as in IID cases, since the IID assumption of Fed-FA is not satisfied. This is a common limitation of Fed-FA and other existing methods. A future direction is to consider the semantics of the parameter updates themselves in addition to the data divergence for federated backdoor defense.

Besides, both our proposed Fed-FA and classic federated defending algorithms [1; 43; 10; 31; 58] are mainly evaluated on small-scale MLP, CNN or RNN models, but not evaluated on popular large language models due to computation cost limit. However, our proposed Fed-FA is model agnostic and just filters harmful gradients involved in aggregation, thus it can be extended to large-scale models such as Transformers and large language models. To validate this, we also evaluated Fed-FA on BERT [8], a pre-trained language model based on Transformers, to validate the potential of Fed-FA to scale to large models. A future direction is to evaluate and improve federated language backdoor defense algorithms on large language models.

## 7 Conclusion

In this paper, we model data divergence among clients' data theoretically for backdoor client detection in federated language learning. Based on it, we propose a novel and effective Federated F-Divergence-Based Aggregation (Fed-FA) algorithm as a strong defense for federated language learning. Fed-FA utilizes the f-divergence indicator to detect and discard suspicious clients. Both theoretical evidence and experimental results demonstrate that Fed-FA can better detect suspicious clients than existing robust federated aggregations that mainly adopt parameter distances explicitly. Thus, Fed-FA outperforms existing methods and achieves state-of-the-art federated language backdoor defense performance. Further analyses validate the effectiveness of proposed mechanisms, as well as show that Fed-FA can be generalized to other settings and is robust to potential adaptive attacks.

Figure 3: Results under non-IID and multiple attacker cases and under adaptive attacks. Fed-FA still outperforms existing baselines under non-IID and multi-atker cases and is robust to adaptive attacks.

## Acknowledgement

We appreciate all the thoughtful and insightful suggestions from the anonymous reviews. This work was supported in part by a Tencent Research Grant and National Natural Science Foundation of China (No. 62176002). Xu Sun is the corresponding author of this paper.

## References

* [1] Blanchard, P., Mhamdi, E.M.E., Guerraoui, R., Stainer, J.: Machine learning with adversaries: Byzantine tolerant gradient descent. In: Guyon, I., von Luxburg, U., Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan, S.V.N., Garnett, R. (eds.) Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA. pp. 119-129 (2017)
* [2] Blitzer, J., Dredze, M., Pereira, F.: Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In: Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. pp. 440-447. Association for Computational Linguistics, Prague, Czech Republic (Jun 2007), https://aclanthology.org/P07-1056
* [3] Chen, X., Chen, T., Sun, H., Wu, Z.S., Hong, M.: Distributed training with heterogeneous data: Bridging median- and mean-based algorithms. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual (2020)
* [4] Chen, X., Salem, A., Backes, M., Ma, S., Zhang, Y.: Badnl: Backdoor attacks against nlp models. arXiv preprint arXiv:2006.01043 (2020)
* [5] Chen, X., Liu, C., Li, B., Lu, K., Song, D.: Targeted backdoor attacks on deep learning systems using data poisoning. CoRR **abs/1712.05526** (2017), http://arxiv.org/abs/1712.05526
* [6] Cui, G., Yuan, L., He, B., Chen, Y., Liu, Z., Sun, M.: A unified evaluation of textual backdoor learning: Frameworks and benchmarks. In: NeurIPS (2022), http://papers.nips.cc/paper_files/paper/2022/hash/2052b3e0617ecb2ce9474a6feaf422b3-Abstract-Datasets_and_Benchmarks.html
* [7] Dai, J., Chen, C., Li, Y.: A backdoor attack against lstm-based text classification systems. IEEE Access **7**, 138872-138878 (2019). https://doi.org/10.1109/ACCESS.2019.2941376, https://doi.org/10.1109/ACCESS.2019.2941376
* [8] Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). pp. 4171-4186 (2019), https://www.aclweb.org/anthology/N19-1423/
* [9] Erichson, N.B., Taylor, D., Wu, Q., Mahoney, M.W.: Noise-response analysis for rapid detection of backdoors in deep neural networks. CoRR **abs/2008.00123** (2020), https://arxiv.org/abs/2008.00123
* [10] Fu, S., Xie, C., Li, B., Chen, Q.: Attack-resistant federated learning with residual-based reweighting. CoRR **abs/1912.11464** (2019), http://arxiv.org/abs/1912.11464
* [11] Fuglede, B., Topsoe, F.: Jensen-shannon divergence and hilbert space embedding. In: International Symposium onInformation Theory, 2004. ISIT 2004. Proceedings. p. 31. IEEE (2004)
* [12] Fung, C., Yoon, C.J.M., Beschastnikh, I.: The limitations of federated learning in sybil settings. In: Egele, M., Bilge, L. (eds.) 23rd International Symposium on Research in Attacks, Intrusions and Defenses, RAID 2020, San Sebastian, Spain, October 14-15, 2020. pp. 301-316. USENIX Association (2020), https://www.usenix.org/conference/raid2020/presentation/fung
* [13] Gao, Y., Xu, C., Wang, D., Chen, S., Ranasinghe, D.C., Nepal, S.: STRIP: a defence against trojan attacks on deep neural networks. In: Balenson, D. (ed.) Proceedings of the 35th Annual Computer Security Applications Conference, ACSAC 2019, San Juan, PR, USA, December 09-13, 2019, pp. 113-125. ACM (2019). https://doi.org/10.1145/3359789.3359790
* [14] Garg, S., Kumar, A., Goel, V., Liang, Y.: Can adversarial weight perturbations inject neural backdoors. In: d'Aquin, M., Dietze, S., Hauff, C., Curry, E., Cudre-Mauroux, P. (eds.) CIKM '20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020. pp. 2029-2032. ACM (2020). https://doi.org/10.1145/3340531.3412130, https://doi.org/10.1145/3340531.3412130* [15] Gu, T., Liu, K., Dolan-Gavitt, B., Garg, S.: Badnets: Evaluating backdooring attacks on deep neural networks. IEEE Access **7**, 47230-47244 (2019). https://doi.org/10.1109/ACCESS.2019.2909068, https://doi.org/10.1109/ACCESS.2019.2909068
* [16] Kim, Y.: Convolutional neural networks for sentence classification. In: Moschitti, A., Pang, B., Daelemans, W. (eds.) Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL. pp. 1746-1751. ACL (2014)
* [17] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Bengio, Y., LeCun, Y. (eds.) 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings (2015), http://arxiv.org/abs/1412.6980
* [18] Kullback, S., Leibler, R.A.: On information and sufficiency. The annals of mathematical statistics **22**(1), 79-86 (1951)
* [19] Kurita, K., Michel, P., Neubig, G.: Weight poisoning attacks on pre-trained models. CoRR **abs/2004.06660** (2020), https://arxiv.org/abs/2004.06660
* [20] Li, Q., Tai, C., E, W.: Stochastic modified equations and dynamics of stochastic gradient algorithms I: mathematical foundations. J. Mach. Learn. Res. **20**, 40:1-40:47 (2019), http://jmlr.org/papers/v20/17-526.html
* [21] Li, Y., Lyu, X., Koren, N., Lyu, L., Li, B., Ma, X.: Neural attention distillation: Erasing backdoor triggers from deep neural networks. In: 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net (2021), https://openreview.net/forum?id=910K4OM-oxE
* 21st International Symposium, RAID 2018, Heraklion, Crete, Greece, September 10-12, 2018, Proceedings. Lecture Notes in Computer Science, vol. 11050, pp. 273-294. Springer (2018). https://doi.org/10.1007/978-3-030-00470-5_13, https://doi.org/10.1007/978-3-030-00470-5_13
* [23] Liu, Y., Ma, S., Aafer, Y., Lee, W., Zhai, J., Wang, W., Zhang, X.: Trojaning attack on neural networks. In: 25th Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018. The Internet Society (2018)
* [24] Maas, A.L., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y., Potts, C.: Learning word vectors for sentiment analysis. In: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. pp. 142-150. Association for Computational Linguistics, Portland, Oregon, USA (June 2011), http://www.aclweb.org/anthology/P11-1015
* [25] Mandt, S., Hoffman, M.D., Blei, D.M.: Stochastic gradient descent as approximate bayesian inference. J. Mach. Learn. Res. **18**, 134:1-134:35 (2017), http://jmlr.org/papers/v18/17-214.html
* [26] McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.: Communication-efficient learning of deep networks from decentralized data. In: Singh, A., Zhu, X.J. (eds.) Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA. Proceedings of Machine Learning Research, vol. 54, pp. 1273-1282. PMLR (2017), http://proceedings.mlr.press/v54/mcmahan17a.html
* [27] Mhamdi, E.M.E., Guerraoui, R., Rouault, S.: The hidden vulnerability of distributed learning in byzantium. In: Dy, J.G., Krause, A. (eds.) Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018. Proceedings of Machine Learning Research, vol. 80, pp. 3518-3527. PMLR (2018), http://proceedings.mlr.press/v80/mhamdi18a.html
* [28] Munoz-Gonzalez, L., Biggio, B., Demontis, A., Paudice, A., Wongrassamee, V., Lupu, E.C., Roli, F.: Towards poisoning of deep learning algorithms with back-gradient optimization. In: Thuraisingham, B.M., Biggio, B., Freeman, D.M., Miller, B., Sinha, A. (eds.) Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, AISec@CCS 2017, Dallas, TX, USA, November 3, 2017. pp. 27-38. ACM (2017). https://doi.org/10.1145/3128572.3140451
* [29] Nguyen, T.A., Tran, A.: Input-aware dynamic backdoor attack. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33, pp. 3450-3460. Curran Associates, Inc. (2020)
* [30] Pascanu, R., Bengio, Y.: Revisiting natural gradient for deep networks. In: Bengio, Y., LeCun, Y. (eds.) 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings (2014), http://arxiv.org/abs/1301.3584
* [31] Pillutla, V.K., Kakade, S.M., Harchaoui, Z.: Robust aggregation for federated learning. CoRR **abs/1912.13445** (2019), http://arxiv.org/abs/1912.13445* [32] Qi, F., Chen, Y., Li, M., Liu, Z., Sun, M.: ONION: A simple and effective defense against textual backdoor attacks. CoRR **abs/2011.10369** (2020), https://arxiv.org/abs/2011.10369
* [33] Qi, F., Li, M., Chen, Y., Zhang, Z., Liu, Z., Wang, Y., Sun, M.: Hidden killer: Invisible textual backdoor attacks with syntactic trigger. In: Zong, C., Xia, F., Li, W., Navigli, R. (eds.) Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021. pp. 443-453. Association for Computational Linguistics (2021). https://doi.org/10.18653/v1/2021.acl-long.37, https://doi.org/10.18653/v1/2021.acl-long.37
* [34] Renyi, A.: On measures of entropy and information. In: Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics. pp. 547-561. University of California Press (1961)
* [35] Rumelhart, D., Hinton, G., Williams, R.: Learning representations by back propagating errors. Nature **323**, 533-536 (10 1986). https://doi.org/10.1038/323533a0
* [36] Saha, A., Subramanya, A., Pirsiavash, H.: Hidden trigger backdoor attacks. In: The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. pp. 11957-11965. AAAI Press (2020)
* [37] Salem, A., Backes, M., Zhang, Y.: Don't trigger me! a triggerless backdoor attack against deep neural networks. arXiv preprint arXiv:2010.03282 (2020)
* [38] Sato, I., Nakagawa, H.: Approximation analysis of stochastic gradient langevin dynamics by using fokker-planck equation and ito process. In: Xing, E.P., Jebara, T. (eds.) Proceedings of the 31st International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 32, pp. 982-990. PMLR, Bejing, China (22-24 Jun 2014), https://proceedings.mlr.press/v32/satoa14.html
* [39] Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.D., Ng, A.Y., Potts, C.: Recursive deep models for semantic compositionality over a sentiment treebank. In: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL. pp. 1631-1642. ACL (2013), https://aclanthology.org/D13-1170/
* [40] Sun, X., Zhang, Z., Ren, X., Luo, R., Li, L.: Exploring the vulnerability of deep neural networks: A study of parameter corruption. In: Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. pp. 11648-11656. AAAI Press (2021), https://ojs.aaai.org/index.php/AAAI/article/view/17385
* [41] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA. pp. 5998-6008 (2017)
* [42] Wan, C.P., Chen, Q.: Robust federated learning with attack-adaptive aggregation. CoRR **abs/2102.05257** (2021), https://arxiv.org/abs/2102.05257
* [43] Xie, C., Chen, M., Chen, P., Li, B.: CRFL: certifiably robust federated learning against backdoor attacks. In: Meila, M., Zhang, T. (eds.) Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event. Proceedings of Machine Learning Research, vol. 139, pp. 11372-11382. PMLR (2021), http://proceedings.mlr.press/v139/xie21a.html
* [44] Xie, C., Huang, K., Chen, P., Li, B.: DBA: distributed backdoor attacks against federated learning. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net (2020), https://openreview.net/forum?id=rkgyS0VVr
* [45] Xie, Z., Sato, I., Sugiyama, M.: A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima. arXiv preprint arXiv:2002.03495 (2020)
* [46] Yang, W., Li, L., Zhang, Z., Ren, X., Sun, X., He, B.: Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in NLP models. In: Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., Zhou, Y. (eds.) Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021. pp. 2048-2058. Association for Computational Linguistics (2021). https://doi.org/10.18653/v1/2021.naacl-main.165, https://doi.org/10.18653/v1/2021.naacl-main.165
* [47] Yang, W., Lin, Y., Li, P., Zhou, J., Sun, X.: RAP: robustness-aware perturbations for defending against backdoor attacks on NLP models. In: Moens, M., Huang, X., Specia, L., Yih, S.W. (eds.) Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021. pp. 8365-8381. Association for Computational Linguistics (2021). https://doi.org/10.18653/v1/2021.emnlp-main.659, https://doi.org/10.18653/v1/2021.emnlp-main.659
* [48] Yang, W., Lin, Y., Li, P., Zhou, J., Sun, X.: Rethinking stealthiness of backdoor attack against NLP models. In: Zong, C., Xia, F., Li, W., Navigli, R. (eds.) Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021. pp. 5543-5557. Association for Computational Linguistics (2021). https://doi.org/10.18653/v1/2021.acl-long.431, https://doi.org/10.18653/v1/2021.acl-long.431
* [49] Yao, Y., Li, H., Zheng, H., Zhao, B.Y.: Latent backdoor attacks on deep neural networks. In: Cavallaro, L., Kinder, J., Wang, X., Katz, J. (eds.) Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS 2019, London, UK, November 11-15, 2019. pp. 2041-2055. ACM (2019). https://doi.org/10.1145/3319535.3354209, https://doi.org/10.1145/3319535.3354209
* [50] Yin, D., Chen, Y., Ramchandran, K., Bartlett, P.L.: Byzantine-robust distributed learning: Towards optimal statistical rates. In: Dy, J.G., Krause, A. (eds.) Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018. Proceedings of Machine Learning Research, vol. 80, pp. 5636-5645. PMLR (2018), http://proceedings.mlr.press/v80/yin18a.html
* [51] Yoo, K., Kwak, N.: Backdoor attacks in federated learning by rare embeddings and gradient ensembling. CoRR **abs/2204.14017** (2022). https://doi.org/10.48550/arXiv.2204.14017, https://doi.org/10.48550/arXiv.2204.14017
* [52] Zhang, X., Zhao, J.J., LeCun, Y.: Character-level convolutional networks for text classification. In: Cortes, C., Lawrence, N.D., Lee, D.D., Sugiyama, M., Garnett, R. (eds.) Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada. pp. 649-657 (2015)
* [53] Zhang, X., Mian, A., Gupta, R., Rahnavard, N., Shah, M.: Cassandra: Detecting trojaned networks from adversarial perturbations. CoRR **abs/2007.14433** (2020), https://arxiv.org/abs/2007.14433
* [54] Zhang, Z., Luo, R., Ren, X., Su, Q., Li, L., Sun, X.: Adversarial parameter defense by multi-step risk minimization. Neural Networks **144**, 154-163 (2021)
* [55] Zhang, Z., Luo, R., Su, Q., Sun, X.: GA-SAM: gradient-strength based adaptive sharpness-aware minimization for improved generalization. In: Goldberg, Y., Kozareva, Z., Zhang, Y. (eds.) Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022. pp. 3888-3903. Association for Computational Linguistics (2022), https://aclanthology.org/2022.emnlp-main.257
* [56] Zhang, Z., Lyu, L., Wang, W., Sun, L., Sun, X.: How to inject backdoors with better consistency: Logit anchoring on clean data. In: The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net (2022), https://openreview.net/forum?id=Bn09TnDngN
* [57] Zhang, Z., Ren, X., Su, Q., Sun, X., He, B.: Neural network surgery: Injecting data patterns into pre-trained models with minimal instance-wise side effects. In: Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., Zhou, Y. (eds.) Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021. pp. 5453-5466. Association for Computational Linguistics (2021), https://www.aclweb.org/anthology/2021.naacl-main.430/
* [58] Zhang, Z., Su, Q., Sun, X.: Dim-krum: Backdoor-resistant federated learning for NLP with dimension-wise krum-based aggregation. In: Goldberg, Y., Kozareva, Z., Zhang, Y. (eds.) Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022. pp. 339-354. Association for Computational Linguistics (2022), https://aclanthology.org/2022.findings-emnlp.25
* [59] Zhao, P., Chen, P., Das, P., Ramamurthy, K.N., Lin, X.: Bridging mode connectivity in loss landscapes and adversarial robustness. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net (2020), https://openreview.net/forum?id=SJgwzCEwHTheoretical details

### Details about f-divergence

The f-divergence [34] can measure the data divergence of \(p(\mathbf{z})\) and \(q(\mathbf{z})\):

\[D_{f}\big{(}q(\mathbf{z})||p(\mathbf{z})\big{)}=\int_{\mathbf{z}}p(\mathbf{z})f \big{(}\frac{q(\mathbf{z})}{p(\mathbf{z})}\big{)}d\mathbf{z},\] (7)

where the integral symbol \(\int_{\mathbf{z}}\) denotes multiple integrals in every dimension of \(\mathbf{z}\), and integral symbols of discrete dimensions need to be changed to summation symbols for these dimensions.

The function \(f(x)\) is smooth, convex and satisfies \(f(1)=0\). Suppose \(x=1+h\). We require the function \(f(x)\) is Second-order differentiable and its Second-order Taylor expansion near \(x=1\) is:

\[f(x)=f^{\prime}(1)h+\frac{f^{\prime\prime}(1)}{2}h^{2}+o(h^{2}),\] (8)

since \(f(x)\) is convex, we have \(f^{\prime\prime}(1)>0\).

Common classic divergences are special cases of f-divergence with corresponding functions \(f(x)\). Here are some examples of special cases:

**Kullback-Leibler divergence.** A special case of f-divergence is the Kullback-Leibler divergence [18] when we choose \(f(x)=x\log x\): \(D_{f}(q||p)=D_{\text{KL}}(q||p)=\int_{\mathbf{z}}q(\mathbf{z})\log\big{(}q( \mathbf{z})/p(\mathbf{z})\big{)}d\mathbf{z}\), and the Second-order Taylor expansion is:

\[f(x)=f(1+h)=h+\frac{h^{2}}{2}+o(h^{2}),\] (9)

where \(f^{\prime\prime}(1)=1>0\).

**Reverse Kullback-Leibler divergence.** A special case of f-divergence is the reverse Kullback-Leibler divergence [18] when we choose \(f(x)=-\log x\): \(D_{f}(q||p)=D_{\text{Resure-KL}}(q||p)=\int_{\mathbf{z}}p(\mathbf{z})\log\big{(} p(\mathbf{z})/q(\mathbf{z})\big{)}d\mathbf{z}\), and the Second-order Taylor expansion is:

\[f(x)=f(1+h)=-h+\frac{h^{2}}{2}+o(h^{2}),\] (10)

where \(f^{\prime\prime}(1)=1>0\).

**Jensen-Shannon divergence.** A special case of f-divergence is the Jensen-Shannon divergence [11] when we choose \(f(x)=x\log x-(x+1)\log((x+1)/2)\): \(D_{f}(q||p)=D_{\text{IS}}(q||p)=D_{\text{KL}}(p||m)/2+D_{\text{KL}}(q||m)/2\), where the medium distribution is \(m=(p+q)/2\), and the Second-order Taylor expansion is:

\[f(x)=f(1+h)=\frac{h^{2}}{4}+o(h^{2}),\] (11)

where \(f^{\prime\prime}(1)=1/2>0\).

### Detailed asumptions of Theorem 1

Theorem 1 requires two classic assumptions, the Second-order Taylor expansion assumption and the Fisher information matrix assumption [30]:

**Assumption A.1** (Second-order Taylor Expansion).: _Assume the loss \(\mathcal{L}(\bm{\theta};\mathbf{z})\) can be Second-order Taylor expanded near the parameter \(\bm{\theta}^{\star}\) and the Hessian matrix \(\bm{H}(\bm{\theta};\mathbf{z})=\nabla_{\bm{\theta}}^{2}\mathcal{L}(\bm{\theta} ;\mathbf{z})\) is diagonal:_

\[\mathcal{L}(\bm{\theta}^{\star}+\bm{\delta};\mathbf{z})=\mathcal{L}(\bm{ \theta}^{\star};\mathbf{z})+\bm{\delta}^{\mathrm{T}}\nabla_{\bm{\theta}} \mathcal{L}(\bm{\theta}^{\star};\mathbf{z})+\frac{1}{2}\bm{\delta}^{\mathrm{T} }\bm{H}(\bm{\theta}^{\star};\mathbf{z})\bm{\delta}+o(||\bm{\delta}||_{2}^{2}),\] (12)

_and \(\bm{H}(\bm{\theta};\mathcal{P})=\text{diag}\{H_{1}(\bm{\theta};\mathcal{P}), \cdots,H_{d}(\bm{\theta};\mathcal{P})\}\)._

**Assumption A.2** (Fisher Information).: _Assume the Fisher information assumption [30] holds:_

\[\bm{H}(\bm{\theta};\mathcal{P})=\mathbb{E}_{\mathcal{P}}\big{[}\nabla_{\bm{ \theta}}\mathcal{L}(\bm{\theta};\mathbf{z})\nabla_{\bm{\theta}}\mathcal{L}( \bm{\theta};\mathbf{z})^{\mathrm{T}}\big{]}.\] (13)

### Proofs of Theorem 1

We try to find the infimum or greatest lower bound of f-divergence:

\[\underset{p(\mathbf{z}),q(\mathbf{z})}{\text{Inf}}\quad D_{f} \big{(}q(\mathbf{z})||p(\mathbf{z})\big{)},\] (14) subject to \[\bm{\theta}^{\star}=\underset{\bm{\theta}}{\text{arg}\min}\, \mathcal{L}\big{(}\bm{\theta};p(\mathbf{z})\big{)},\quad\bm{\theta}^{\star}+ \bm{\delta}=\underset{\bm{\theta}}{\text{arg}\min}\,\mathcal{L}\big{(}\bm{ \theta};q(\mathbf{z})\big{)}.\] (15)

**Theorem 1** (F-Divergence Lower Bound).: _The lower bound of f-divergence is:_

\[D_{f}\big{(}q(\mathbf{z})||p(\mathbf{z})\big{)}\geq\big{(}1+o(1) \big{)}\frac{f^{\prime\prime}(1)}{2}\mathcal{I}_{\text{F-D}v},\quad\mathcal{I}_ {\text{F-D}v}=\sum_{k=1}^{d}H_{k}^{*}\delta_{k}^{2},\] (16)

_where \(H_{k}^{*}=H_{k}\big{(}\boldsymbol{\theta}^{*};p(\mathbf{z})\big{)}=\mathcal{L }_{\theta_{k}}^{\prime\prime}\big{(}\boldsymbol{\theta}^{*};p(\mathbf{z}) \big{)}>0\) is the \(i\)-th Hessian of loss on \(p(\mathbf{z})\) and \(f^{\prime\prime}(1)>0\)._

Proof.: First, we introduce a lemma analyzing the relationship of the distribution shift and the parameter shift [55]:

**Lemma A.1**.: _Define \(r(\mathbf{z})=q(\mathbf{z})/p(\mathbf{z})-1\). When the distribution shift is small enough, namely \(r(\mathbf{z})\) is small, we can estimate the parameter shift \(\boldsymbol{\delta}\) as,_

\[\boldsymbol{\delta}=-\boldsymbol{H}^{-1}\big{(}\boldsymbol{\theta}^{*};p( \mathbf{z})\big{)}\mathbb{E}_{p}[r(\mathbf{z})\nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}^{*};\mathbf{z})]+o(\|\boldsymbol{\delta}\|).\] (17)

In Lemma A.1, \(r(\mathbf{z})\) is defined as \(r(\mathbf{z})=q(\mathbf{z})/p(\mathbf{z})-1\), and the parameter shift \(\boldsymbol{\delta}\) is defined as \(\arg\min_{\boldsymbol{\theta}}\mathcal{L}\big{(}\boldsymbol{\theta};q( \mathbf{z})\big{)}-\arg\min_{\boldsymbol{\theta}}\mathcal{L}\big{(} \boldsymbol{\theta};p(\mathbf{z})\big{)}\), which is consistent with our definition since \(\arg\min_{\boldsymbol{\theta}}\mathcal{L}\big{(}\boldsymbol{\theta};q( \mathbf{z})\big{)}-\arg\min_{\boldsymbol{\theta}}\mathcal{L}\big{(} \boldsymbol{\theta};p(\mathbf{z})\big{)}=(\boldsymbol{\theta}^{*}+ \boldsymbol{\delta})-(\boldsymbol{\theta}^{*})=\boldsymbol{\delta}\).

With the change-of-measure technique, we have \(\mathbb{E}_{p}\big{[}r(\mathbf{z})\big{]}=0\). Define \(\mathbb{E}_{p}[r(\mathbf{z})\nabla_{\boldsymbol{\theta}}\mathcal{L}( \boldsymbol{\theta}^{*};\mathbf{z})]=\mathbf{v}\). Then according to Lemma A.1:

\[\mathbf{v}=\boldsymbol{H}\big{(}\boldsymbol{\theta}^{*};p(\mathbf{z})\big{)} \boldsymbol{\delta}+o(\|\boldsymbol{\delta}\|).\] (18)

Conduct the Second-order Taylor expansion on function \(f\) near \(1\) in \(D_{f}\big{(}q(\mathbf{z})||p(\mathbf{z})\big{)}\), we have

\[f\big{(}\frac{q(\mathbf{z})}{p(\mathbf{z})}\big{)}=f^{\prime}(1 )r(\mathbf{z})+\frac{f^{\prime\prime}(1)}{2}r^{2}(\mathbf{z})+o\big{(}r^{2}( \mathbf{z})\big{)}.\] (19)

Therefore,

\[D_{f}(q||p)= \int_{\mathbf{z}}p(\mathbf{z})f\big{(}\frac{q(\mathbf{z})}{p( \mathbf{z})}\big{)}d\mathbf{z}=\mathbb{E}_{p}\big{[}f\big{(}\frac{q(\mathbf{z })}{p(\mathbf{z})}\big{)}\big{]}=\mathbb{E}_{p}\big{[}f^{\prime}(1)r+\frac{f^{ \prime\prime}(1)}{2}r^{2}+o(r^{2})\big{]}\] (20) \[=\] (21)

Define \(\mathcal{I}[r(\mathbf{z})]=\mathbb{E}_{p}\big{[}r^{2}(\mathbf{z})\big{]}\), \(\big{(}D_{f}(q||p)\big{)}_{\min}=\big{(}1+o(1)\big{)}f^{\prime\prime}(1)\big{(} \mathcal{I}[r(\mathbf{z})]\big{)}_{\min}/2\). The infimum is:

\[\underset{r(\mathbf{z})}{\text{subject to}\mathbb{E}_{p}[r(\mathbf{z})]=0, \quad\mathbb{E}_{p}[r(\mathbf{z})\nabla_{\boldsymbol{\theta}}\mathcal{L}( \boldsymbol{\theta}^{*};\mathbf{z})]=\mathbf{v}.}\] (22)

Define the Lagrange multiplier as \(\mathcal{L}[r(\mathbf{z})]=\frac{1}{2}\mathcal{I}[r(\mathbf{z})]-\alpha \mathbb{E}_{p}[r(\mathbf{z})]-\boldsymbol{\beta}^{\mathrm{T}}\big{(}\mathbb{E }_{p}[r(\mathbf{z})\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}^ {*};\mathbf{z})]-\mathbf{v}\big{)}\), when \(\mathcal{I}[r(\mathbf{z})]\) is optimal, \(\delta\mathcal{L}[r(\mathbf{z})]/\delta r(\mathbf{z})=0\), namely:

\[r(\mathbf{z})-\alpha-\boldsymbol{\beta}^{\mathrm{T}}\nabla_{ \boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}^{*};\mathbf{z})=0.\] (24)

Therefore, \(r(\mathbf{z})=\alpha+\boldsymbol{\beta}^{\mathrm{T}}\nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}^{*};\mathbf{z})\). We have:

\[0=\mathbb{E}_{p}[r(\mathbf{z})]=\alpha+\boldsymbol{\beta}^{ \mathrm{T}}\mathbb{E}_{p}\big{[}\nabla_{\boldsymbol{\theta}}\mathcal{L}( \boldsymbol{\theta}^{*};\mathbf{z})\big{]}=\alpha+\boldsymbol{\beta}^{ \mathrm{T}}\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}^{*};p( \mathbf{z}))=\alpha,\] (25) \[\mathbf{v}=\mathbb{E}_{p}[r(\mathbf{z})\nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}^{*};\mathbf{z})]=\mathbb{E}_{p}\big{[} \nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}^{*};\mathbf{z}) \nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}^{*};\mathbf{z})^{ \mathrm{T}}\boldsymbol{\beta}\big{]}=\boldsymbol{H}\big{(}\boldsymbol{\theta}^ {*};p(\mathbf{z})\big{)}\boldsymbol{\beta}.\] (26)

To conclude:

\[\alpha=0,\quad\boldsymbol{H}\big{(}\boldsymbol{\theta}^{*};p( \mathbf{z})\big{)}\boldsymbol{\beta}=\mathbf{v}.\] (27)

Therefore, when \(\mathcal{I}[r(\mathbf{z})]\) is optimal:

\[r(\mathbf{z})=\big{(}\boldsymbol{H}^{-1}\big{(}\boldsymbol{\theta}^{*};p( \mathbf{z})\big{)}\mathbf{v}\big{)}^{\mathrm{T}}\nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}^{*};\mathbf{z})=\mathbf{v}^{\mathrm{T}} \boldsymbol{H}^{-1}\big{(}\boldsymbol{\theta}^{*};p(\mathbf{z})\big{)}\nabla_{ \boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}^{*};\mathbf{z}),\] (28)

thus the optimal \(\mathcal{I}[r(\mathbf{z})]\) is:

\[\big{(}\mathcal{I}[r(\mathbf{z})]\big{)}_{\min}= \mathbb{E}_{p}[r(\mathbf{z})^{2}]\] (29) \[= \mathbb{E}_{p}[\mathbf{v}^{\mathrm{T}}\boldsymbol{H}^{-1}\big{(} \boldsymbol{\theta}^{*};p(\mathbf{z})\big{)}\nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}^{*};\mathbf{z})\big{(}\mathbf{v}^{\mathrm{T}} \boldsymbol{H}^{-1}\big{(}\boldsymbol{\theta}^{*};p(\mathbfsince \(\mathbf{v}=\bm{H}\big{(}\bm{\theta}^{*};p(\mathbf{z})\big{)}\bm{\delta}+o(||\bm{ \theta}||)=\big{(}1+o(1)\big{)}\bm{H}\big{(}\bm{\theta}^{*};p(\mathbf{z})\big{)} \bm{\delta}\):

\[\big{(}\mathcal{I}[r(\mathbf{z})]\big{)}_{\min}=\big{(}1+o(1)\big{)}\bm{\delta} ^{\mathrm{T}}\bm{H}\big{(}\bm{\theta}^{*};p(\mathbf{z})\big{)}\bm{\delta}= \big{(}1+o(1)\big{)}\sum_{k=1}^{d}H_{k}^{*}\delta_{k}^{2},\] (33)

where \(H_{k}^{*}=H_{k}\big{(}\bm{\theta}^{*};p(\mathbf{z})\big{)}\).

Therefore, the solution is

\[\big{(}D_{f}(q||p)\big{)}_{\min}=\big{(}1+o(1)\big{)}\frac{f^{\prime\prime}(1 )}{2}\big{(}\mathcal{I}[r(\mathbf{z})]\big{)}_{\min}=\big{(}1+o(1)\big{)} \frac{f^{\prime\prime}(1)}{2}\sum_{k=1}^{d}H_{k}^{*}\delta_{k}^{2}.\] (34)

then we have:

\[D_{f}\big{(}q(\mathbf{z})||p(\mathbf{z})\big{)}\geq\big{(}D_{f}(q||p)\big{)} _{\min}=\big{(}1+o(1)\big{)}\frac{f^{\prime\prime}(1)}{2}\mathcal{I}_{\text{F- Div}},\quad\mathcal{I}_{\text{F-Div}}=\sum_{k=1}^{d}H_{k}^{*}\delta_{k}^{2}.\] (35)

### Proofs of Theorem 2

**Theorem 2** (Hessian Estimations by Diffusion Theory, Detailed Version).: _When \(\sqrt{H_{k}^{*}}=\sqrt{H_{k}\left(\bm{\theta}^{*};p(\mathbf{z})\right)}\) are small, there exists a constant \(C>0\) for any \(\epsilon>0\) that the following inequality holds with a probability higher than \(1-\epsilon\) for large \(n\):_

\[\left|\frac{\frac{1}{n}\sum\limits_{i=1}^{n}|u_{k}^{(i)}|}{C\sqrt{H_{k}^{*}}} -1\right|<\sqrt{\frac{\pi-2}{2n\epsilon}},\] (36)

_where \(u_{k}^{(i)}\) is the \(k\)-th dimension of \(\mathbf{u}_{t}^{(i)}\)._

Proof.: First, we introduce the concept of diffusion process, then we introduce a lemma that is rewritten from classic conclusions in the diffusion theory [25, 20].

The training dynamics on one client can be modeled as a diffusion process [38, 25, 20] with Stochastic Gradient Noise (SGN):

\[d\bm{\theta}=-\nabla_{\bm{\theta}}\mathcal{L}(\bm{\theta};p(\mathbf{z}))dt+ \sqrt{2D}d\mathbf{W}_{t},\] (37)

where \(dt\) is the unit time or the step size, \(D\) is the diffusion coefficient, and \(d\mathbf{W}_{t}\sim N(0,Idt)\). We assume the gradient noise introduced by stochastic learning is small (the temperature of the diffusion process is low). Here we also assume that the data distributions of all clients approximately obey the merged data distribution \(p(\mathbf{z})\).

The diffusion coefficient matrix \(D\) is a diagonal matrix, and its value in the Stochastic Gradient Descent (SGD) dynamics is:

\[D_{k}=\frac{\eta}{2B}H_{k}^{*},\] (38)

where \(\eta=dt\) is the the unit time or the step size, \(B\) is the batch size, and \(H_{k}^{*}=H_{k}\big{(}\bm{\theta}^{*};p(\mathbf{z})\big{)}\).

Its value in the dynamics involving adaptive learning rate mechanisms, take the Adam [17] optimizer for example, can also be seen as \(D_{k}\approx\frac{\eta}{2B}H_{k}^{*}\) when \(\sqrt{H_{k}^{*}}\) are small. Since the parameter update is:

\[\Delta\bm{\theta}=-\hat{\eta}\odot\mathbf{m},\] (39)

where \(\mathbf{m}\) can be seen as an SGD update with the momentum mechanism and \(\mathbb{E}[\mathbf{m}]=\nabla_{\bm{\theta}}\mathcal{L}(\bm{\theta};p(\mathbf{ z}))\) in a stationary distribution. In Adam, \(\hat{\eta}=\eta(\sqrt{\mathbf{v}}+\epsilon)^{-1}\) and \(\mathbb{E}[\mathbf{v}]=\mathbb{E}_{\mathbf{z}\sim p(\mathbf{z})}[\nabla_{\bm {\theta}}\mathcal{L}(\bm{\theta};\mathbf{z})\odot\nabla_{\bm{\theta}} \mathcal{L}(\bm{\theta};\mathbf{z})]\approx\mathbf{H}(\bm{\theta}^{*};p( \mathbf{z}))\) in a stationary distribution. Therefore, when \(\sqrt{\mathbf{v}}\) are small, the weight update can be approximated with:

\[\Delta\bm{\theta}\approx-\eta\epsilon^{-1}\mathbf{m},\] (40)

which can be seen as an SGD update with the learning rate \(\eta\epsilon^{-1}\) and the gradient \(\mathbf{m}\) on a small batch.

To conclude, there exists a constant \(C_{1}>0\) that the diffusion coefficients \(D_{k}\) of (1) all dimensions in the Stochastic Gradient Descent (SGD) dynamics; or (2) dimensions with small \(\sqrt{H_{k}^{*}}\) in dynamics involving adaptive learning rate mechanisms, such as Adam [17]; are:

\[D_{k}\approx C_{1}H_{k}^{*},\] (41)

We introduce Lemma A.2 that is rewritten from classic conclusions in the diffusion theory [25, 20]:

**Lemma A.2**.: _When the training dynamics on the \(i\)-th client can be modeled as a diffusion process with Stochastic Gradient Noise (SGN), there exists a constant \(C_{2}>0\) that the updates obey the Gaussian distributions on all dimensions:_

\[u_{k}^{(i)}\sim N(0,C_{2}D_{k}).\] (42)

According to Lemma A.2,

\[\mathbb{E}\big{[}|u_{k}^{(i)}|\big{]}= \int_{-\infty}^{+\infty}\frac{|x|\exp\big{(}-\frac{x^{2}}{2C_{2}D_ {k}}\big{)}}{\sqrt{2\pi C_{2}D_{k}}}dx=\sqrt{\frac{2C_{2}D_{k}}{\pi}},\] (43) \[\mathbb{E}\big{[}|u_{k}^{(i)}|^{2}\big{]}= \int_{-\infty}^{+\infty}\frac{|x|^{2}\exp\big{(}-\frac{x^{2}}{2C_ {2}D_{k}}\big{)}}{\sqrt{2\pi C_{2}D_{k}}}dx=C_{2}D_{k},\] (44) \[\mathbb{D}\big{[}|u_{k}^{(i)}|\big{]}= \mathbb{E}\big{[}|u_{k}^{(i)}|^{2}\big{]}-\mathbb{E}\big{[}|u_{k} ^{(i)}|\big{]}^{2}=(1-\frac{2}{\pi})C_{2}D_{k}.\] (45)

The Chebyshev Inequality demonstrates that:

\[P(|X-\mathbb{E}[X]|<c)>1-\frac{\mathbb{D}[X]}{c^{2}},\] (46)

we choose \(X=\frac{1}{n}\sum\limits_{i=1}^{n}|u_{k}^{(i)}|\), then:

\[\mathbb{E}[X]=\sqrt{\frac{2C_{2}D_{k}}{\pi}},\quad\mathbb{D}[X]= \frac{(1-\frac{2}{\pi})C_{2}D_{k}}{n}.\] (47)

We choose \(c=\sqrt{\frac{\mathbb{D}[X]}{\epsilon}}\), then:

\[\left|\frac{\sum\limits_{i=1}^{n}|u_{k}^{(i)}|}{n}-\sqrt{\frac{2C_{2}D_{k}}{ \pi}}\right|<\sqrt{\frac{\mathbb{D}[X]}{\epsilon}},\] (48)

holds with a probability higher than \(1-\epsilon\).

For dimensions with small Hessians, there exists a constant \(C_{1}>0\) that \(D_{k}=C_{1}H_{k}^{*}\). Therefore, there exists a constant \(C=\sqrt{\frac{2C_{2}C_{1}}{\pi}}\) for any \(\epsilon>0\) that the following inequality holds with a probability higher than \(1-\epsilon\):

\[\left|\frac{\frac{1}{n}\sum\limits_{i=1}^{n}|u_{k}^{(i)}|}{C\sqrt{H_{k}^{*}}} -1\right|<\sqrt{\frac{\pi-2}{2n\epsilon}}.\] (49)

### Proofs of Theorem 3

We verify the Byzantine resilience of Fed-FA in Theorem 3.

**Preliminary.** Suppose \(\mathbf{g}=\mathbb{E}[\mathbf{u}],\mathbf{g}^{*}=\mathbb{E}[\mathbf{u}^{*}]\) are the expected updates for clean updates \(\mathbf{u}\) and malicious updates \(\mathbf{u}^{*}\). Suppose \(g_{k},g_{k}^{*},u_{k}\) denote the \(k\)-th dimension of \(\mathbf{g},\mathbf{g}^{*},\mathbf{u}\).

We also require that the gradient noises are bounded linearly by \(g_{k}^{2}\), namely, there exists \(\eta>0\) such that \(\mathbb{D}[u_{k}]\leq\eta\|\mathbb{E}[u_{k}]\|^{2}=\eta g_{k}^{2}\) for any dimension \(k\) for clean updates \(\mathbf{u}\). We define \(\eta\) as the gradient noise scale [1] assume that \(\mathbb{D}[u_{k}]=\sigma^{2}\) and thus \(\mathbb{D}[u_{k}]\) is a fixed value \(\sigma^{2}\). However, in this work, we allow \(\mathbb{D}[u_{k}]\) to be arbitrarily large when \(g_{k}^{2}\) grows very large and \(\mathbb{D}[u_{k}]\) is only required to be bounded with a linear bound of \(g_{k}^{2}\) instead, namely \(\eta g_{k}^{2}\).

According to Fisher information matrix assumption [30], \(H_{k}^{*}=\mathbb{D}[u_{k}]\).

**Theorem 3** (Byzantine Resilience of Fed-FA. Detailed Version.).: _Assume the loss function on the merged dataset \(p(\mathbf{z})\) is locally \(\mu\)-strongly convex and locally \(L\)-smooth near the optimal parameter. For \(m\) malicious clients, \(1\leq m\leq\lfloor\frac{n-1}{2}\rfloor\), when the estimations of indicators are accurate enough and the gradient noise scale \(\eta\) is small enough, there exists \(0\leq\alpha<\frac{\pi}{2}\) such that Fed-FA aggregation algorithm is \((\alpha,m)\)-Byzantine resilience:_

\[\|\mathbb{E}[\mathcal{A}(\{\mathbf{u}^{(i)}\}_{i=1}^{n})]-\mathbf{g}\|_{2} \leq\sin\alpha\|\mathbf{g}\|_{2},\quad\mathbf{g}^{T}\mathbb{E}[\mathcal{A}(\{ \mathbf{u}^{(i)}\}_{i=1}^{n})]\geq(1-\sin\alpha)\|\mathbf{g}\|_{2}^{2},\] (50)

_where \(\mathbf{g}=\mathbb{E}[\mathbf{u}],\mathbf{g}^{*}=\mathbb{E}[\mathbf{u}^{*}]\) are the expectations for clean updates \(\mathbf{u}\) and malicious updates \(\mathbf{u}^{*}\)._Proof.: We assume that the estimations of Hessians are accurate, namely the estimations of \(\mathcal{I}^{(k)}_{\text{F-Dw}}\) are accurate. Suppose \(\mathcal{I}^{(1)}_{\text{F-Dw}}\leq\mathcal{I}^{(2)}_{\text{F-Dw}}\leq\mathcal{I} ^{(3)}_{\text{F-Dw}}\leq\cdots^{\prime}_{\text{F-Dw}}\).

Then we have \(S=\{1,2,\cdots,\lceil\frac{n+1}{2}\rceil\}\), \(|S|=\lceil\frac{n+1}{2}\rceil\geq n-m>m\). Suppose \(M\) is the set of malicious client indexes, and \(|M|=m\). Define \(M_{1}=\{i:i\leq|S|,i\in M\}\), \(|M_{1}|\leq|M|\leq m\leq\lfloor\frac{n-1}{2}\rfloor\), then,

\[\mathcal{A}(\{\mathbf{u}^{(i)}\}_{i=1}^{n})=\frac{1}{|S|}\sum_{i=1}^{|S|} \mathbf{u}^{(i)}=\frac{1}{|S|}\big{(}\sum_{i\in S\setminus M_{1}}\mathbf{u}^{ (i)}+\sum_{i\in M_{1}}\mathbf{u}^{(i)}\big{)}.\] (51)

If \(M_{1}=\emptyset\), namely \(S\setminus M_{1}=S\), we have:

\[\mathbb{E}[\mathcal{A}(\{\mathbf{u}^{(i)}\}_{i=1}^{n})]=\frac{1}{|S|}\big{(} \sum_{i\in S\setminus M_{1}}\mathbb{E}[\mathbf{u}^{(i)}]\big{)}=\frac{1}{|S|} \big{(}\sum_{i\in S}\mathbb{E}[\mathbf{u}^{(i)}]\big{)}=\mathbf{g},\] (52)

namely there exists \(\alpha=0\) such that:

\[\|\mathbb{E}[\mathcal{A}(\{\mathbf{u}^{(i)}\}_{i=1}^{n})]-\mathbf{g}\|\leq\sin \alpha\|\mathbf{g}\|_{2},\quad\mathbf{g}^{\ast}\mathbb{E}[\mathcal{A}(\{ \mathbf{u}^{(i)}\}_{i=1}^{n})]=\|\mathbf{g}\|_{2}^{2}\geq(1-\sin\alpha)\| \mathbf{g}\|_{2}^{2}.\] (53)

Otherwise, we have \(|M_{1}|>0,|S|-|M_{1}|\geq(n-m)-m>0\):

\[\mathbb{E}[\mathcal{A}(\{\mathbf{u}^{(i)}\}_{i=1}^{n})]=\frac{1}{|S|}\big{(} \sum_{i\in S\setminus M_{1}}\mathbb{E}[\mathbf{u}^{(i)}]+\sum_{i\in M_{1}} \mathbb{E}[\mathbf{u}^{(i)}]\big{)}=\frac{(|S|-|M_{1}|)\mathbf{g}+|M_{1}| \mathbf{g}^{\ast}}{|S|}.\] (54)

For any \(i\in M_{1}\), there exists a clean client \(j\notin S\setminus M_{1}\) because we have \(n-m\) clean clients but clean client number in \(S\) is \(|S|-|M_{1}|<|S|=\lceil\frac{n+1}{2}\rceil\leq n-m\). Therefore \(\mathcal{I}^{(j)}_{\text{F-Dw}}\geq\mathcal{I}^{(i)}_{\text{F-Dw}}\), namely:

\[\sum_{k=1}^{d}H^{\ast}_{k}(u^{(j)}_{k}-g_{k})^{2}\geq\sum_{k=1}^{d}H^{\ast}_{k }(u^{(i)}_{k}-g_{k})^{2}\] (55)

where estimations of Hessians and \(\boldsymbol{\delta}\) are accurate, namely we can estimate the optimal parameter updates of clean clients \(\boldsymbol{\theta}^{\ast}\) accurately, thus we replace the \(k\)-th dimension \(\boldsymbol{\theta}^{\ast}\) or \(\boldsymbol{\theta}^{\text{avg}}\) with \(g_{k}\).

Since the loss function is locally \(\mu\)-strongly convex and locally \(L\)-smooth, we have \(\mu\leq H^{\ast}_{k}=\mathbb{D}[u_{k}]\leq L\), since the Hessian matrix is diagonal according to the assumption and \(H^{\ast}_{k}\) is the eigenvalue of the Hessian matrix that is in \([\mu,L]\), we have,

\[L\sum_{k=1}^{d}(u^{(j)}_{k}-g_{k})^{2}\geq\sum_{k=1}^{d}\mathbb{D}[u_{k}](u^{(j )}_{k}-g_{k})^{2}\geq\sum_{k=1}^{d}\mathbb{D}[u_{k}](u^{(i)}_{k}-g_{k})^{2} \geq\mu\sum_{k=1}^{d}(u^{(i)}_{k}-g_{k})^{2}.\] (56)

Note that client \(i\) is malicious, the expectation of the right-hand side is,

\[\mu\sum_{k=1}^{d}\mathbb{E}\big{[}(u^{(i)}_{k}-g_{k})^{2}\big{]} =\mu\sum_{k=1}^{d}\mathbb{E}\left[\big{(}(u^{(i)}_{k}-g^{\ast}_{k })+(g^{\ast}_{k}-g_{k})\big{)}^{2}\right]\] (57) \[=\mu\sum_{k=1}^{d}\mathbb{E}\big{[}(u^{(i)}_{k}-g^{\ast}_{k})^{2} +2(u^{(i)}_{k}-g^{\ast}_{k})(g^{\ast}_{k}-g_{k})+(g^{\ast}_{k}-g_{k})^{2}\big{]}\] (58) \[=\mu\sum_{k=1}^{d}\mathbb{E}\big{[}(u^{(i)}_{k}-g^{\ast}_{k})^{2} \big{]}+\mu\sum_{k=1}^{d}(g^{\ast}_{k}-g_{k})^{2}\] (59) \[\geq\mu\sum_{k=1}^{d}(g^{\ast}_{k}-g_{k})^{2}=\mu\|\mathbf{g}^{ \ast}-\mathbf{g}\|_{2}^{2},\] (60)

and note that client \(j\) is clean, the expectation of the left-hand side is,

\[L\sum_{k=1}^{d}\mathbb{E}\big{[}(u^{(j)}_{k}-g_{k})^{2}\big{]}=L\sum_{k=1}^{d} \mathbb{D}[u_{k}]\leq\eta L\sum_{k=1}^{d}g^{2}_{k}=\eta L\|\mathbf{g}\|_{2}^{2},\] (61)

combining them, we have,

\[\eta L\|\mathbf{g}\|_{2}^{2}\geq\mu\|\mathbf{g}^{\ast}-\mathbf{g}\|_{2}^{2},\] (62)Therefore,

\[||\mathbb{E}[\mathcal{A}(\{\mathbf{u}^{(i)}\}_{i=1}^{n})]-\mathbf{g} \|_{2} =\left\|\frac{(|S|-|M_{1}|)\mathbf{g}+|M_{1}|\mathbf{g}^{*}}{|S|}- \mathbf{g}\right\|_{2}=\frac{|M_{1}|}{|S|}||\mathbf{g}^{*}-\mathbf{g}\|_{2}\] (63) \[\leq\frac{m}{\lceil\frac{n+1}{2}\rceil}||\mathbf{g}^{*}-\mathbf{ g}\|_{2}\leq\frac{m}{\lceil\frac{n+1}{2}\rceil}\sqrt{\frac{\eta L}{\mu}}\| \mathbf{g}\|_{2}\] (64)

When \(\eta\) is small enough and \(\eta<\frac{\lceil\frac{n+1}{2}\rceil^{2}\mu}{m^{2}L}\), we have,

\[0<\frac{m}{\lceil\frac{n+1}{2}\rceil}\sqrt{\frac{\eta L}{\mu}}<1,\] (65)

therefore there exists:

\[\alpha=\arcsin\left(\frac{m}{\lceil\frac{n+1}{2}\rceil}\sqrt{\frac{\eta L}{ \mu}}\right)\in(0,\frac{\pi}{2}),\] (66)

such that Fed-FA aggregation algorithm is \((\alpha,m)\)-Byzantine resilience:

\[\|\mathbb{E}[\mathcal{A}(\{\mathbf{u}^{(i)}\}_{i=1}^{n})]-\mathbf{g}\|_{2} \leq\sin\alpha\|\mathbf{g}\|_{2},\] (67)

and we can also derive:

\[\mathbf{g}^{\mathrm{T}}\mathbb{E}[\mathcal{A}(\{\mathbf{u}^{(i)} \}_{i=1}^{n})] =\mathbf{g}^{\mathrm{T}}\left[(\mathbb{E}[\mathcal{A}(\{\mathbf{ u}^{(i)}\}_{i=1}^{n})]-\mathbf{g})+\mathbf{g}\right]\] (68) \[=\|\mathbf{g}\|^{2}+\mathbf{g}^{\mathrm{T}}(\mathbb{E}[\mathcal{ A}(\{\mathbf{u}^{(i)}\}_{i=1}^{n})]-\mathbf{g})\] (69) \[\geq\|\mathbf{g}\|^{2}-\|\mathbf{g}\|\|\mathbb{E}[\mathcal{A}(\{ \mathbf{u}^{(i)}\}_{i=1}^{n})]-\mathbf{g}\|\geq(1-\sin\alpha)\|\mathbf{g}\|_ {2}^{2}.\] (70)

\(\Box\)

[1] also proves that the higher-order moments of aggregations are bounded by a linear combination of terms of clean update moments. The proof utilizes the same technique as above.

The bound of \(\sin\alpha\) provided in the proof is: \(\sin\alpha=\left(\frac{m}{\lceil\frac{n+1}{2}\rceil}\sqrt{\frac{\eta L}{\mu}}\right)\), a lower \(\sin\alpha\) indicates better Byzantine resilience. It can be concluded from the bound and the assumptions that: (1) higher \(m\) (\(1\leq m\leq\lfloor\frac{n-1}{2}\rfloor\)) leads to worse Byzantine resilience, and when \(m>\lfloor\frac{n-1}{2}\rfloor\), the assumption of the theorem does not hold and the Byzantine resilience of Fed-FA is not guaranteed; (2) a poorly conditioned loss function or Hessian matrix (namely the condition number \(L/\mu\) of the Hessian matrix is high) leads to poor Byzantine resilience; (3) a higher gradient noise scale \(\eta\) leads to poor Byzantine resilience; (4) the Byzantine resilience of Fed-FA rely on that the estimations of indicators are accurate; we propose dataset synthesization and embedding Hessian reassignment mechanisms for more accurate Hessian estimations; therefore, a future improvement direction of Fed-FA may be avoiding the interference of attackers on our indicator estimation.

## Appendix B Detailed experiment setups

In this section, we introduce the detailed experimental setup. In the training process of the local clients, all aggregation methods adopt the same hyper-parameters for fair comparisons. Experiments are conducted on NVIDIA TITAN RTX GPUs. One experiment costs about 30 minutes for one run (including 10 rounds).

### Tasks and datasets

We adopt four text classification tasks, _i.e._, _SST-2_[39], _IMDB_[24], _Amazon_[2], and _AgNews_[52]. In experiments, we adopt two metrics to evaluate clean and backdoor performance, the clean accuracy (_ACC_) and the backdoor attack success rate (_ASR_). ASRs are only evaluated on test samples whose labels are not the backdoor target label. ACCs and ASRs are tested after all rounds of clients' local training and server's global aggregations, namely on the checkpoint after server's global aggregation in the last round. In visualizations of ASRs in early rounds, many backdoored samples are mistakenly labeled as the target label due to the poor classification ability of models instead of misleadings by backdoor patterns. Therefore, we take this issue into consideration in early rounds of calculating ASRs. Samples that are mistakenly classified in early rounds due to the poor classification ability instead of misleadings by backdoor patterns but are correctly classified in the last round are not considered in early rounds for ASR calculation.

**SST-2** denotes the Stanford Sentiment Treebank dataset [39]. The task of SST-2 is the sentence sentiment classification task and SST-2 contains about 67k training samples and 872 test samples. **IMDB** denotes the IMDB movie reviews dataset [24]. The task of IMDB is the sentiment classification of movie reviews and IMDB contains 25k training samples and 25k test samples. **Amazon** denotes the Amazon reviews dataset [2]. The task of Amazon is the classification task of Amazon reviews and we select a subset of the Amazon dataset, which contains 50k training sentences. **AgNews** denotes the AgNews dataset [52]. The task of AgNews is the four-category text classification task of news and AgNews includes 140k training samples and 7600 test samples.

### Training details

Before training, we first preprocess the dataset. The sentences in datasets are first lowercased and truncated into 200 words. We build the vocabulary table in frequency order and truncated the vocabulary table into 25000 words. We also add two extra special tokens to the vocabulary: [pad] and [unk]. [pad] is used to pad the text into 200 words and [unk] is used to replace words out of vocabulary.

We adopt three typical neural network architectures in NLP tasks, _i.e._, _GRU_, _LSTM_, and _CNN_. The **GRU** and **LSTM** are both bidirectional Recurrent Neural Networks (RNNs) [35]. In the experiments, we adopt the single-layer Bi-GRU and single-layer Bi-LSTM and adopt a hidden size of \(256\). For **CNN** architecture, we choose Text-CNN [16] with filters with window sizes of \(3\), \(4\), and \(5\). The hidden size is \(100\) and there are \(256\) feature maps in each filter. In GRU, LSTM, and CNN models, the word embedding dimensions are \(300\).

We choose the Adam optimizer [17] in local training of clients. The learning rate is set to \(10^{-3}\) and the batch size is set to \(32\). We train models for \(10\) rounds. In each round, the clients use 10k samples for training.

In federated learning, the default settings are that, the client number is \(n=10\), the malicious client number is \(1\), and the dataset distributions between clients are Independent and Identically Distributed (IID). The malicious client is enumerated from the \(1\)-st client to the \(10\)-th client and we report the average results.

### Backdoor attack details

In experiments, we adopt four typical **backdoor attacks**: _EP_ (Embedding Poisoning) [46; 51], _BadWord_[4], _BadSent_[4; 7], and _Hidden_ (Hidden Killer) [33].

EP and BadWord are both trigger word based backdoor attacks. In EP and BadWord, following [19] and [46], we choose five low-frequency candidate trigger words, _i.e._, "cf", "mn", "bb", "to" and "mb". BadSent is a trigger sentence based backdoor attack. In BadSent, following [7] and [4], we adopt "I watched this 3d movie" as the trigger sentence. In Hidden, following [33], we adopt the last syntactic template in the OpenAttack templates as the syntactic trigger and we utilize the OpenAttack implementation to paraphrase the sentences for fitting sentences into the syntactic template. The target label is label 0. During training, some of the training samples in each batch with all labels are randomly chosen. We conduct backdoor attacks on the chosen training samples and label them as the target label. Instances before and after backdoor attacks are shown in Table 7.

### Federated aggregation details

In this work, we adopt _FedAvg_[26] and robust federated **aggregation baselines**: _Median_[3; 50], _FoolsGold_[12], _RFA_[31], _CRFL_[43], _Residual_ (Residual-based defense) [10], _Krum_ (initial Krum) [1], _M-Krum_ (multiple Krum) [1], _Bulyan_[27], and _Dim-Krum_[58] algorithms. We divide robust aggregation baselines into two categories, _i.e._, discarding aggregations, and non-discarding aggregations.

Discarding aggregations includes the Krum (initial Krum) [1], M-Krum (multiple Krum) [1], Bulyan [27], and Dim-Krum [58] algorithms, In Dim-Krum, following [58], we choose the ratio as \(\rho=10^{-3}\). We also adopt the

\begin{table}
\begin{tabular}{c c c} \hline \hline \multirow{2}{*}{**Original** samples} & **Text** & for me, the story is just too slim. \\  & **Label** & Label 1: Negative. \\ \hline Word based backdoors & **Text** & for me, the cf story is just too slim. \\ (**EP** and **BadWord**) & **Label** & Label 0: Positive. \\ \hline Sentence based & **Text** & i watched this 3d movie. for me, the story is just too slim. \\ backdoors (**BadSent**) & **Label** & Label 0: Positive. \\ \hline Hidden trigger & **Text** & if you ask me, i will say that the story is just too slim. \\ backdoors (**Hidden**) & **Label** & Label 0: Positive. \\ \hline \hline \end{tabular}
\end{table}
Table 7: Examples of trigger word based (EP and BadWord), trigger sentence based (BadSent), and hidden trigger based (Hidden) backdoor attacks. The target label is label 0.

memory and adaptive noise mechanisms. In the adaptive noise mechanism, since RNN models are sensitive to noises on parameters [58], we choose \(\lambda=2\).

Non-discarding aggregations includes _Median_[50; 3], _FoolsGold_[12], _RFA_[31], _CRFL_[43], and _Residual_ (Residual-based defense) [10] algorithms. In CRFL, the noises on each dimension obey IID Gaussian distributions \(N(0,\sigma^{2})\) where \(\sigma=0.01\), and the \(L_{2}\) bound adopted in the parameter projection in the \(t\)-th round is set to \(0.05t+2\). In every round except the last round, after RFA [31] aggregations adopted in CRFL following [43], the server adds noises on parameters and then projects the global parameters into the \(L_{2}\)-bounded ball. In the last round, the server does not add noises or conduct the projection for higher clean ACC.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline Model & \multirow{2}{*}{Metric} & \multirow{2}{*}{FedAvg} & \multicolumn{4}{c}{Non-discarding Aggregations} & \multicolumn{4}{c}{Discarding Aggregations (includes Fed-FA)} \\ (Attack) & & & & Median & FoolsGold & RFA & CRFL & Residual & Krum & M-Krum & Bulyan & Dim-Krum & **Fed-FA** \\ \hline GRU & ACC & 87.12 & 86.92 & 87.07 & 87.05 & 71.03 & 87.05 & 79.09 & 86.35 & 86.74 & 84.84 & 86.70 \\ (EP) & ASR & 96.82 & 11.44 & 96.88 & 96.57 & 65.98 & 11.20 & 21.55 & 11.07 & 11.35 & 13.61 & 10.99 \\ \hline GRU & ACC & 86.93 & 86.86 & 86.97 & 86.83 & 71.59 & 86.90 & 76.38 & 85.72 & 86.69 & 84.76 & 86.39 \\ (BadWord) & ASR & 94.27 & 81.14 & 93.98 & 95.10 & 71.11 & 95.16 & 92.69 & 71.27 & 63.10 & 44.46 & 11.70 \\ \hline GRU & ACC & 87.09 & 86.95 & 86.95 & 86.93 & 71.55 & 86.97 & 76.67 & 86.23 & 86.74 & 84.63 & 86.47 \\ (BadSent) & ASR & 99.07 & 98.39 & 98.31 & 99.13 & 99.90 & 98.90 & 96.73 & 87.82 & 85.49 & 42.14 & 13.87 \\ \hline GRU & ACC & 83.05 & 83.42 & 82.70 & 83.02 & 70.85 & 83.28 & 73.14 & 82.06 & 84.02 & 83.85 & 85.87 \\ (Hidden) & ASR & 53.96 & 47.27 & 54.78 & 54.26 & 63.77 & 56.90 & 85.91 & 46.90 & 35.64 & 32.43 & 18.10 \\ \hline LSTM & ACC & 84.42 & 84.37 & 84.75 & 84.84 & 70.55 & 84.38 & 77.46 & 84.09 & 84.00 & 83.02 & 84.55 \\ (EP) & ASR & 99.32 & 14.90 & 99.39 & 99.26 & 81.38 & 16.79 & 23.67 & 15.03 & 13.88 & 15.06 & 13.90 \\ \hline LSTM & ACC & 84.34 & 84.16 & 84.30 & 84.32 & 70.46 & 84.31 & 75.73 & 84.22 & 84.12 & 83.47 & 84.69 \\ (BadWord) & ASR & 98.49 & 95.48 & 98.06 & 98.55 & 87.17 & 98.87 & 96.01 & 83.93 & 86.06 & 42.83 & 17.65 \\ \hline LSTM & ACC & 84.44 & 84.16 & 84.51 & 84.55 & 70.29 & 84.41 & 76.27 & 83.78 & 84.09 & 83.54 & 84.75 \\ (BadSent) & ASR & 99.15 & 98.77 & 99.34 & 99.27 & 100.0 & 99.44 & 97.24 & 86.96 & 96.41 & 40.66 & 34.43 \\ \hline LSTM & ACC & 80.76 & 81.72 & 81.40 & 81.30 & 69.74 & 81.65 & 73.25 & 81.07 & 82.20 & 81.58 & 83.56 \\ (Hidden) & ASR & 65.08 & 59.21 & 63.83 & 65.62 & 70.73 & 65.42 & 85.14 & 85.43 & 48.71 & 33.75 & 22.45 \\ \hline CNN & ACC & 87.01 & 86.52 & 86.96 & 86.97 & 76.51 & 86.99 & 80.31 & 86.00 & 86.28 & 85.53 & 86.57 \\ (EP) & ASR & 96.80 & 11.76 & 96.97 & 96.68 & 41.92 & 11.03 & 19.97 & 11.72 & 11.03 & 13.60 & 11.49 \\ \hline CNN & ACC & 87.09 & 86.30 & 87.09 & 87.12 & 79.09 & 86.89 & 78.18 & 86.29 & 86.46 & 85.44 & 86.78 \\ (BadWord) & ASR & 81.46 & 66.65 & 81.69 & 82.82 & 24.32 & 83.19 & 100.0 & 96.41 & 52.36 & 14.82 & 16.85 \\ \hline CNN & ACC & 87.10 & 86.43 & 87.10 & 87.10 & 79.10 & 86.97 & 77.87 & 86.17 & 86.50 & 85.34 & 86.70 \\ (BadSent) & ASR & 99.30 & 97.81 & 99.17 & 99.23 & 49.12 & 99.54 & 99.90 & 99.92 & 98.41 & 62.46 & 36.61 \\ \hline CNN & ACC & 84.07 & 84.68 & 84.03 & 84.11 & 74.76 & 84.26 & 76.22 & 83.95 & 85.65 & 85.20 & 85.41 \\ (Hidden) & ASR & 56.32 & 52.55 & 56.29 & 55.58 & 36.33 & 57.33 & 80.22 & 55.15 & 41.16 & 22.97 & 26.08 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Detailed results of aggregation algorithms on different models and attacks.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Settings & Metric & FedAvg & Dim-Krum & **Fed-FA** \\ \hline IID & ACC & 85.28 & 84.27 & 85.70 \\ Attackers=1 & ASR & 86.67 & 31.56 & **19.51** \\ \hline \multirow{2}{*}{Dirichlet} & ACC & 83.41 & 77.67 & 79.48 \\  & ASR & 85.10 & 66.63 & **52.41** \\ \hline \multirow{2}{*}{Attackers=2} & ACC & 84.91 & 83.94 & 85.42 \\  & ASR & 94.45 & 68.22 & **36.18** \\ \hline \multirow{2}{*}{Attackers=3} & ACC & 84.90 & 83.07 & 84.75 \\  & ASR & 96.70 & 71.92 & **49.62** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Results under non-IID or multiple attacker cases.

[MISSING_PAGE_FAIL:23]

Figure 5: ASRs under different attacks in \(10\) rounds on LSTM.

Figure 6: ASRs under different attacks in \(10\) rounds on CNN.

## Appendix D Further analyses

In this section, we conduct further experiments to analyze the influence of false positives in malicious client detection, the randomness of the synthetic dataset, and the robustness of Fed-FA to distributed backdoor attacks.

### Influence of false positives

**False positives in discarding aggregations.** Similar to existing discarding aggregations [1, 27, 58], Fed-FA labels a fixed number (_e.g._\(\lfloor\frac{n-1}{2}\rfloor\) in Fed-FA) of \(n\) clients as malicious clients and discards them instead of trying to distinguish clean and malicious clients and only discarding poisonous updates. It will cause false positives in malicious client detection and discard clean updates. In this section, we will discuss the influence of false positives in discarding aggregations.

**False positives have weak impacts on the clean performance.** Theoretically, the convergence is guaranteed in Theorem 3. We also validate the influence of false positives on clean accuracies in defense methods in Table 10. Even in the case that all clients are clean, the average accuracy decreases from 86.13 of FedAvg to 85.73 of Fed-FA and Fed-FA only has a performance decrease of about 0.40, which is much lower than other discarding aggregations. When there are malicious clients, Fed-FA has better clean performance than both FedAvg and other defenses. Besides, poisonous updates will also harm the learning, while Fed-FA will not harm the clean accuracy since it discards poisonous updates. To conclude, with our proposed Fed-FA, false positives have weak impacts on clean performance.

**Discarding a fixed number of clients can act as a strong defense.** Discarding aggregations are stronger baselines than non-discarding aggregations. In discarding aggregations, false positives have weak impacts on the clean performance, but the false negatives may poison the global model and fail the federated backdoor defense. Although discarding a fixed number of clients will cause false positives in malicious client detection, it can still act as a strong defense with little performance loss. Since the task of defending against federated language backdoors itself is difficult, we recommend discarding a fixed number of clients instead of distinguishing clean and malicious clients and only discarding poisonous updates. Our recommended method has a similar discarding protocol as both Fed-FA and other existing discarding aggregations [1, 27, 58].

**The proposed detection variants of discarding aggregations.** However, [6] argue that the influence of false positives is also crucial, especially for the cases when the backdoor defense is easier and the clean accuracy is crucial. Therefore, we also design detection variants for discarding aggregations for these cases when lower false positives are also important. In variants, we label the clients with distances or indicators (normalized to zero mean and one standard deviation) higher than a threshold as malicious clients. The threshold is tuned on the validation set for each task and defense. We validate the FAR (false acceptance rate), FRR (false rejection rate), P (precision), R (recall), F (F-1 score), ACC (accuracy), and MR (mean rank) on malicious client detection tasks of detection variants of discarding aggregations. The definitions of these indicators are:

\[\text{FAR} = \text{The probability when the benign client is regarded as a trojaned client},\] (71) \[\text{FRR} = \text{The probability that the trojaned client is recognized as the benign client},\] (72) \[\text{P} = \frac{\text{Number of true predicted malicious clients}}{\text{Number of total predicted malicious clients}},\] (73) \[\text{R} = \frac{\text{Number of true predicted malicious clients}}{\text{Number of total true malicious clients}},\] (74) \[\text{F} = \frac{2\text{PR}}{\text{P}+\text{R}},\] (75) \[\text{ACC} = \text{The probability that a client is recognized correctly},\] (76) \[\text{MR} = \text{Mean ranks of true malicious clients in the clients' distances or indicators}.\] (77)

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Case**} & \multirow{2}{*}{FedAvg} & \multicolumn{4}{c}{Non-discarding Aggregations} & \multicolumn{4}{c}{Discarding Aggregations (includes Fed-FA)} \\  & & & Median & FoolsGold & RFA & CRFL & Residual & Krum & M-Krum & Bulyan Dim-Krum & **Fed-FA** \\ \hline
**Clean** & 86.13 & 85.92 & 86.01 & 85.72 & 73.59 & 86.25 & 79.51 & 85.46 & 85.77 & 84.53 & 85.73 \\
**Poisonous** & 85.28 & 85.61 & 85.32 & 85.35 & 72.96 & 85.34 & 76.71 & 84.66 & 85.29 & 84.27 & 85.70 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Average clean accuracies of Fed-FA compared to others on clean and poisonous cases.

**Detection performance of discarding aggregations and variants.** We report detection performance under multiple client numbers (0-4 of 10 clients) in Table 11. The detection variant of Fed-FA has lower false positives and higher false negatives, namely the detection variant tends to miss malicious updates and not to discard clean updates compared to Fed-FA. To conclude, there is a tradeoff between FAR and FRR and the detection variant of Fed-FA can be utilized in cases where false positive rates are crucial. Both Fed-FA and its variant have satisfying detection performance compared to random baseline and other discarding aggregations.

### The randomness of the synthetic dataset

As reported in Table 12, we randomly sample different texts as the labeled and synthetic datasets for different runs and the standard deviations of Fed-FA are low, which means that the randomness of the synthetic dataset does not influence the results much since Fed-FA only needs the Hessian scales instead of accurate Hessians.

### The robustness to distributed backdoor attacks

In addition to regularizers, we can adopt distributed backdoor attacks [44] to make malicious updates more stealthy as adaptive attacks. As shown in Table 13, we also validate that Fed-FA is robust to them.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Attack** & ACC & ASR \\ \hline FedAvg & 86.97 & 100.0 \\ \hline Dim-Krum & 86.43 & 15.12 \\ Fed-FA & 86.57 & **13.45** \\ \hline \hline \end{tabular}
\end{table}
Table 13: Robustness to distributed attacks.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Method** & FAR\% & FRR\% & P\% & R\% & F\% & ACC\% & MR \\ \hline Random & 40.10 & 60.33 & 22.31 & 39.67 & 28.56 & 55.35 & 5.50 \\ Random (Variant) & 33.94 & 67.78 & 21.61 & 32.22 & 25.87 & 58.45 & 5.50 \\ \hline M-Krum & 46.61 & 98.33 & 1.03 & 1.67 & 1.27 & 41.75 & 8.43 \\ M-Krum (Variant) & 51.13 & 97.78 & 1.25 & 2.22 & 1.60 & 38.38 & 8.43 \\ Euclidean & 51.29 & 97.22 & 1.55 & 2.78 & 1.99 & 38.38 & 8.49 \\ Euclidean (Variant) & 47.26 & 98.33 & 1.01 & 1.67 & 1.26 & 41.25 & 8.49 \\ \hline Fed-FA & 32.26 & **33.33** & 37.50 & **66.67** & **48.00** & 67.50 & **3.78** \\ Fed-FA (Variant) & **10.16** & 70.00 & **46.15** & 30.00 & 36.36 & **76.38** & **3.78** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Performance of detection variants of discarding aggregations. Variant denotes discarding according to the threshold, otherwise discarding \(\lfloor\frac{n-1}{2}\rfloor\) clients. Lower MRs are better. Euclidean denotes Fed-FA with the Euclidean indicator. The best results are in **bold**.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Method** & **ACC** & **ASR** \\ \hline FedAvg & 85.28\(\pm\)0.81 & 86.67\(\pm\)7.49 \\ \hline Fed-FA with labeled dataset & 85.77\(\pm\)0.12 & 20.06\(\pm\)1.25 \\
**Fed-FA** & 85.70\(\pm\)0.18 & **19.51\(\pm\)**1.90 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Influence of the synthetic dataset.