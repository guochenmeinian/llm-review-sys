A Riemannian Exponential Augmented Lagrangian Method for Computing the Projection Robust Wasserstein Distance

Bo Jiang

Ministry of Education Key Laboratory of NSLSCS

School of Mathematical Sciences, Nanjing Normal University

Nanjing 210023, China

jiangbo@njnu.edu.cn

Ya-Feng Liu

State Key Laboratory of Scientific and Engineering Computing

Institute of Computational Mathematics and Scientific/Engineering Computing

Academy of Mathematics and Systems Science, Chinese Academy of Sciences

Beijing 100190, China

yafliu@lsec.cc.ac.cn

Corresponding author.

###### Abstract

Projection robust Wasserstein (PRW) distance is recently proposed to efficiently mitigate the curse of dimensionality in the classical Wasserstein distance. In this paper, by equivalently reformulating the computation of the PRW distance as an optimization problem over the Cartesian product of the Stiefel manifold and the Euclidean space with additional nonlinear inequality constraints, we propose a Riemannian exponential augmented Lagrangian method (REALM) for solving this problem. Compared with the existing Riemannian exponential penalty-based approaches, REALM can potentially avoid too small penalty parameters and exhibit more stable numerical performance. To solve the subproblems in REALM efficiently, we design an inexact Riemannian Barzilai-Borwein method with Sinkhorn iteration (iRBBS), which selects the stepsizes adaptively rather than tuning the stepsizes in efforts as done in the existing methods. We show that iRBBS can return an \(\epsilon\)-stationary point of the original PRW distance problem within \(\mathcal{O}(\epsilon^{-3})\) iterations, which matches the best known iteration complexity result. Extensive numerical results demonstrate that our proposed methods outperform the state-of-the-art solvers for computing the PRW distance.

## 1 Introduction

The optimal transport (OT) problem has found wide applications in machine learning, representation learning, data sciences, and image sciences; see [21; 5; 42; 1; 14] and the references therein for more details. However, its direct application in machine learning may encounter the issue of the curse of dimensionality since the sample complexity of approximating the Wasserstein distance can grow exponentially in dimension [22; 49]. To resolve this issue, by making an important extension to the sliced Wasserstein distance [43], Paty and Cuturi [41], Deshpande et al. [16], and Niles-Weed and Rigollet [40] proposed to project the distributions to a low-dimensional subspace that maximizes the Wasserstein distance between the projected distribution, which can reduce the sample complexity and overcome the issue of the curse of dimensionality [40; 16; 35].

In this paper, we focus on the discrete probability measure case. For \(\{x_{1},\ldots,x_{n}\}\subset\mathbb{R}^{d}\) and \(\{y_{1},\ldots,y_{n}\}\subset\mathbb{R}^{d}\), define \(M_{ij}=(x_{i}-y_{j})(x_{i}-y_{j})^{\mathsf{T}}\) for each \((i,j)\in[n]\times[n]\) with \([n]:=\{1,\ldots n\}\). Let \(\mathbf{1}\in\mathbb{R}^{n}\) be the all-one vector and \(\delta_{x}\) be the Dirac delta function at \(x\). Given \(r=(r_{1},\ldots,r_{n})^{\mathsf{T}}\in\Delta^{n}:=\{z\in\mathbb{R}^{n}\mid \mathbf{1}^{\mathsf{T}}z=1,z>0\}\) and \(c=(c_{1},\ldots,c_{n})^{\mathsf{T}}\in\Delta^{n}\), define two discrete probability measures \(\mu_{n}=\sum_{i=1}^{n}r_{i}\delta_{x_{i}}\) and \(\nu_{n}=\sum_{i=1}^{n}c_{i}\delta_{y_{i}}\). For \(k\in[d]\), the \(k\)-dimensional projection robust Wasserstein (PRW) distance between \(\mu_{n}\) and \(\nu_{n}\) is defined as [41]

\[\mathcal{P}_{k}^{2}(\mu_{n},\nu_{n})=\max_{U\in\mathcal{U}}\min_{\pi\in\Pi(r,c )}\left\langle\pi,C(U)\right\rangle,\] (1)

where \(\left\langle\cdot,\cdot\right\rangle\) is the standard inner product in \(\mathbb{R}^{n\times n}\), \(C(U)\in\mathbb{R}^{n\times n}\) with \([C(U)]_{ij}=\left\langle M_{ij},UU^{\mathsf{T}}\right\rangle\), \(\mathcal{U}=\{U\in\mathbb{R}^{d\times k}\mid U^{\mathsf{T}}U=I_{k}\}\) is known as the Stiefel manifold with \(I_{k}\) being the \(k\)-by-\(k\) identity matrix, and \(\Pi(r,c)=\{\pi\in\mathbb{R}^{n\times n}\mid\pi\mathbf{1}=r,\pi^{\mathsf{T}} \mathbf{1}=c,\pi\geq 0\}\). Problem (1) is a nonconcave-convex max-min problem over the Stiefel manifold, which makes it very challenging to solve.

**Related works and motivations**. To compute the PRW distance, Paty and Cuturi [41] proposed two algorithms for solving the subspace robust Wasserstein distance, which is a convex relaxation of problem (1) (without the theoretical guarantee on the relaxation gap). An OT or entropy-regularized OT subproblem with dimension \(n\) and a full or top \(k\) eigendecomposition of a \(d\)-by-\(d\) matrix needs to be solved exactly at each iteration. Very recently, Lin et al. [34] proposed a Riemannian (adaptive) gradient ascent with Sinkhorn (R(A)GAS) algorithm for solving the following entropy-regularized problem with a small regularization parameter \(\eta\):

\[\max_{U\in\mathcal{U}}\,p_{\eta}(U),\] (2)

where \(p_{\eta}(U)=\min_{\pi\in\Pi(r,c)}\left\{\left\langle\pi,C(U)\right\rangle-\eta H (\pi)\right\}\), in which \(H(\pi)=-\sum_{ij}\pi_{ij}\log\pi_{ij}\) is the entropy function. They showed that R(A)GAS can return an \(\epsilon\)-stationary point of PRW problem (1) within \(\mathcal{O}(\epsilon^{-4})\) iterations if \(\eta=\mathcal{O}(\epsilon)\) in (2). However, at each iteration, R(A)GAS needs to solve a regularized OT problem in relatively high precision, which results in a high computational cost. To reduce the complexity of R(A)GAS, Huang et al. [28; 29] proposed a Riemannian (adaptive) block coordinate descent (R(A)BCD) algorithm for solving an equivalent "min" formulation of (2) as

\[\min_{U\in\mathcal{U},\alpha,\beta\in\mathbb{R}^{n}}\,\mathcal{L}_{\eta}( \mathbf{x},\mathbf{1}\mathbf{1}^{\mathsf{T}}),\] (3)

where \(\mathcal{L}_{\eta}(\cdot,\cdot)\) is defined in (8) further ahead. By choosing \(\eta=\mathcal{O}(\epsilon)\) in (3), they showed that the whole iteration complexity of R(A)BCD to attain an \(\epsilon\)-stationary point of PRW problem (1) reduces to \(\mathcal{O}(\epsilon^{-3})\), which significantly improves the complexity of R(A)GAS.

However, there are two main issues of R(A)BCD and R(A)GAS. First, to compute a solution of problem (1) with relatively high quality, \(\eta\) in problem (2) or (3) has to be chosen small, which makes the corresponding problem ill-conditioned and may cause numerical instability in solving it. Second, the performance of the above algorithms is sensitive to the stepsizes in updating \(U\). Hence, to achieve a better performance, one has to spend some efforts tuning the stepsizes carefully. Resolving these two main issues demands some novel approaches from both theoretical and computational points of view, and this is the motivation and focus of our paper.

**Contributions**. In this paper, by reformulating (1) as an optimization problem defined over the Cartesian product of the Stiefel manifold and the Euclidean space with additional inequality constraints (see problem (6) further ahead), we can resolve the above-mentioned two issues. Our main contributions are summarized as follows. See also Figure 1 for a summary of the related works and the main results of this paper.

(i) We propose a Riemannian exponential augmented Lagrangian method (REALM) to efficiently and faithfully compute the PRW distance, in which a series of subproblems with dynamically decreasing penalty parameters and adaptively updated multiplier matrices are solved approximately. In theory, we establish the global convergence of REALM in the sense that any limit point of the sequence generated by the algorithm is a stationary point of the original problem; see Theorem 2.7. Numerically, REALM always outperforms the Riemannian exponential penalty approach since it could avoid too small penalty parameters in many cases.

(ii) To efficiently solve the subproblem in REALM (i.e., problem (11) further ahead), we view it as a one-block optimization problem over the Stiefel manifold and propose a novel and practical algorithm, namely, the inexact Riemannian Barzilai-Borwein (BB) method with Sinkhorn iteration (iRBBS), wherein a flexible number of Sinkhorn iterations is performed to compute the inexact Riemannian gradient. Compared with R(A)BCD, our proposed iRBBS (applied to problem (11) with fixed \(\pi^{k}\) and \(\eta_{k}=O(\epsilon)\)) can not only return a stronger \(\epsilon\)-stationary point of PRW problem (1) (compared with the definitions in [34; 28]; see Remark 2.2), within \(\mathcal{O}(\epsilon^{-3})\) iterations (see Theorem 3.2 with \(\epsilon_{1}=\epsilon_{2}=\epsilon\)), but also has a better numerical performance, which mainly benefits from the adaptive Riemannian BB stepsize (based on the inexact Riemannian gradient information).

_Notations._ For \(x\in\mathbb{R}^{n}\), \(\mathrm{Diag}(x)\) is an \(n\times n\) diagonal matrix with \(x\) being its main diagonal. For a matrix \(A\), denote \(A_{\min}=\min_{ij}A_{ij}\), \(A_{\max}=\max_{ij}A_{ij},\|A\|_{1}=\sum_{ij}|A_{ij}|\), \(\|A\|_{\infty}=\max_{ij}|A_{ij}|\), \(\|A\|_{\mathsf{F}}^{2}=\sum_{ij}A_{ij}^{2}\), and \(\|A\|_{\mathrm{var}}=A_{\max}-A_{\min}\). Denote by \(\log(\cdot)\) and \(\exp(\cdot)\) the element-wise logarithmic and exponential operators, respectively. We use \(\mathbb{R}_{+}^{n\times n}\) and \(\mathbb{R}_{+}^{n\times n}\) to denote the nonnegative and positive orthants of \(\mathbb{R}^{n\times n}\), respectively. Throughout this paper, we define \(C\in\mathbb{R}^{n\times n}\) with \(C_{ij}=\|x_{i}-y_{j}\|^{2}\) and \(V_{\pi}=\sum_{ij}\pi_{ij}M_{ij}\).

The tangent space at \(U\in\mathcal{U}\) is \(\mathrm{T}_{U}\mathcal{U}=\{\xi\in\mathbb{R}^{d\times k}\mid U^{\mathsf{T}} \xi+\xi^{\mathsf{T}}U=0\}\). Let \(\mathrm{T}\mathcal{U}=\{(U,\xi)\mid U\in\mathcal{U}\) and \(\xi\in\mathrm{T}_{U}\mathcal{U}\}\) be the tangent bundle of \(\mathcal{U}\). A smooth map \(\mathrm{Retr}:\mathrm{T}\mathcal{U}\to\mathcal{U}:(U,\xi)\mapsto\mathrm{Retr} _{U}(\xi)\) is called a retraction if each curve \(\mathcal{R}(t)=\mathrm{Retr}_{U}(t\xi)\) satisfies \(\mathcal{R}(0)=U\) and \(\mathcal{R}^{\prime}(0)=\xi\); see [9; Definition 3.47] or [2, Definition 4.1.1]. The Riemannian metric \(\langle\cdot,\cdot\rangle_{U}\) endowed on the Stiefel manifold \(\mathcal{U}\) is taken as the standard metric \(\langle\cdot,\cdot\rangle\) on \(\mathbb{R}^{d\times k}\). The Riemannian gradient of a smooth function \(f:\mathbb{R}^{d\times k}\to\mathbb{R}\) at \(U\in\mathcal{U}\) is defined as \(\mathrm{grad}\,f(U)\), which satisfies \(\left\langle\mathrm{grad}\,f(U),\xi\right\rangle_{U}=\left\langle\nabla f(U), \xi\right\rangle\) for all \(\xi\in\mathrm{T}_{U}\mathcal{U}\), where \(\nabla f(U)\) denotes the Euclidean gradient of \(f\) at \(U\). If \(U^{\mathsf{T}}\nabla f(U)\) is symmetric, we have \(\mathrm{grad}\,f(U)=\mathsf{Proj}_{\mathrm{T}_{U}\mathcal{U}}(\nabla f(U))=(I _{d}-UU^{\mathsf{T}})\nabla f(U)\).

The rest of this paper is organized as follows. The proposed REALM is introduced in Section 2. A practical iRBBS for solving the subproblem in REALM is proposed in Section 3. Numerical results are presented in Section 4. Finally, we draw some concluding remarks in Section 5.

## 2 A Riemannian Exponential ALM for Computing the PRW Distance (1)

Given a fixed \(U\in\mathcal{U}\), consider the OT problem

\[\min_{\pi\in\mathbb{R}^{n\times n}}\ \langle\pi,C(U)\rangle\quad \mathrm{s.t.}\quad\pi\mathbf{1}=r,\ \pi^{\mathsf{T}}\mathbf{1}=c,\ \pi\geq 0.\] (4)

By adding a redundant constraint \(\|\pi\|_{1}=1\)[29; 36], we derive the dual of (4) as

\[\max_{\alpha\in\mathbb{R}^{n},\beta\in\mathbb{R}^{n}}-(r^{\mathsf{T}}\alpha+c^ {\mathsf{T}}\beta+y)\quad\mathrm{s.t.}\quad\varphi(\mathbf{x})_{ij}+y\geq 0, \quad\forall(i,j)\in[n]\times[n],\] (5)

where \(\mathbf{x}=(\alpha,\beta,U)\) and \(\varphi(\mathbf{x})\in\mathbb{R}^{n\times n}\) with \(\varphi(\mathbf{x})_{ij}=\alpha_{i}+\beta_{j}+\langle M_{ij},UU^{\mathsf{T}}\rangle\) for each \((i,j)\in[n]\times[n]\). Note that the matrix \(\pi\) in (4) can also be understood as the Lagrange multiplier corresponding to the inequalities in (5). Therefore, the value \(\mathcal{P}_{k}^{2}(\mu_{n},\nu_{n})\) defined in (1) is equal to the opposite of the optimal objective value of the following optimization problem:

\[\min_{\mathbf{x}\in\mathcal{M},y\in\mathbb{R}}\ r^{\mathsf{T}}\alpha+c^{ \mathsf{T}}\beta+y\quad\mathrm{s.t.}\quad\varphi(\mathbf{x})_{ij}+y\geq 0,\quad \forall(i,j)\in[n]\times[n],\] (6)

Figure 1: A summary of the related works and the main results of this paper.

where \(\mathcal{M}=\mathbb{R}^{n}\times\mathbb{R}^{n}\times\mathcal{U}\). Motivated by the first-order necessary condition of problem (6) (see Appendix A.1 for details), we define the \((\epsilon_{1},\epsilon_{2})\)-stationary point of problem (1) as follows.

**Definition 2.1**.: We call \((\tilde{\mathbf{x}},\tilde{\pi})\in\mathcal{M}\times\Pi(r,c)\) an \((\epsilon_{1},\epsilon_{2})\)-stationary point of PRW problem (1), if \(\|\mathsf{Proj}_{\mathrm{T}_{\mathrm{T}_{\mathrm{T}}\tilde{U}}}(-2V_{\tilde{ \pi}}\tilde{U})\|_{\mathrm{F}}\leq\epsilon_{1}\) and \(\langle\tilde{\pi},Z(\tilde{\mathbf{x}})\rangle\leq\epsilon_{2}\), where \(Z(\tilde{\mathbf{x}})\in\mathbb{R}^{n\times n}\) with \(Z(\tilde{\mathbf{x}})_{ij}=\varphi(\mathbf{x})_{ij}-\varphi(\mathbf{x})_{\min}\). If \(\epsilon_{1}=\epsilon_{2}=0\), we call such \((\tilde{\mathbf{x}},\tilde{\pi})\) a stationary point of PRW problem (1).

_Remark 2.2_.: Our Definition 2.1 is stronger than [28, Definition 3.1] and [34, Definition 2.7] in the sense that the \((\epsilon_{1},\epsilon_{2})\)-stationary point satisfying the conditions here also satisfies all conditions therein. See Appendix A.2 for more details.

Given \(\pi\in\mathbb{R}^{n\times n}_{++}\) and \(\eta>0\), define the function \(\zeta_{\eta}(\mathbf{x},\pi)\in\mathbb{R}^{n\times n}\) with

\[[\zeta_{\eta}(\mathbf{x},\pi)]_{ij}=\pi_{ij}\mathrm{exp}\left(-\frac{\varphi( \mathbf{x})_{ij}}{\eta}\right)\!,\] (7)

define

\[\widetilde{\mathcal{L}}_{\eta}(\mathbf{x},y,\pi)=r^{\mathsf{T}}\alpha+c^{ \mathsf{T}}\beta+y+\eta\sum\nolimits_{ij}\pi_{ij}\exp\left(-\frac{\varphi( \mathbf{x})_{ij}+y}{\eta}\right)\]

and

\[\mathcal{L}_{\eta}(\mathbf{x},\pi):=r^{\mathsf{T}}\alpha+c^{\mathsf{T}}\beta+ \eta\log(\|\zeta_{\eta}(\mathbf{x},\pi)\|_{1}).\] (8)

One natural approach for solving problem (6) is the Riemannian exponential penalty approach (where the manifold constraints are kept in the subproblem), which aims to solve the penalty subproblem

\[\min_{\mathbf{x}\in\mathcal{M},y\in\mathbb{R}}\widetilde{\mathcal{L}}_{\eta} (\mathbf{x},y,\mathbf{11}^{\mathsf{T}}).\] (9)

For any fixed \(x\), letting \(\nabla_{y}\widetilde{\mathcal{L}}_{\eta}(\mathbf{x},y,\mathbf{11}^{\mathsf{T }})=0\), we can obtain the optimal \(y\) as \(y=\eta\log(\|\zeta_{\eta}(\mathbf{x},\mathbf{11}^{\mathsf{T}})\|_{1})\). By eliminating the variable \(y\) in (9), we thus obtain the subproblem (3) of the approach in Huang et al. [28].

It is known that the exponential augmented Lagrangian method (ALM) is usually more stable than the exponential penalty approach; see [18, Tables 3.1-3.3] for a detailed example. More specifically, the penalty parameter in the exponential ALM can be chosen as any positive number in the convex case [47, 52] or can be bounded away from zero under some standard assumptions in the general nonlinear case [20], which is in sharp contrast to the exponential penalty approach. Based on the aforementioned knowledge, we thus extend the exponential ALM [8] to the manifold case to solve problem (6). Fix the current estimate of the Lagrange multiplier corresponding to the inequality constraints in (6) and the penalty parameter as \(\pi^{k}\) and \(\eta_{k}\), respectively. Then the subproblem at the \(k\)-th iteration is given as

\[\min_{\mathbf{x}\in\mathcal{M},y\in\mathbb{R}}\widetilde{\mathcal{L}}_{\eta_{k }}(\mathbf{x},y,\pi^{k}).\] (10)

Similar to the way for eliminating \(y\) in (9), we obtain an equivalent formulation of (10):

\[\min_{\mathbf{x}\in\mathcal{M}}\mathcal{L}_{\eta_{k}}(\mathbf{x},\pi^{k}).\] (11)

Define the matrix \(\phi_{\eta_{k}}(\mathbf{x},\pi^{k})\in\mathbb{R}^{n\times n}\) with

\[[\phi_{\eta_{k}}(\mathbf{x},\pi^{k})]_{ij}=[\zeta_{\eta_{k}}(\mathbf{x},\pi^{k })]_{ij}/\|\zeta_{\eta_{k}}(\mathbf{x},\pi^{k})\|_{1}.\] (12)

By the chain rule, we have \(\nabla_{\alpha}\mathcal{L}_{\eta_{k}}(\mathbf{x},\pi^{k})=r-\phi_{\eta_{k}}( \mathbf{x},\pi^{k})\mathbf{1}\), \(\nabla_{\beta}\mathcal{L}_{\eta_{k}}(\mathbf{x},\pi^{k})=c-\phi_{\eta_{k}}( \mathbf{x},\pi^{k})^{\mathsf{T}}\mathbf{1}\), \(\nabla_{U}\mathcal{L}_{\eta_{k}}(\mathbf{x},\pi^{k})=-2V_{\phi_{\eta_{k}}( \mathbf{x},\pi^{k})}U\), and \(\mathrm{grad}_{U}\mathcal{L}_{\eta_{k}}(\mathbf{x},\pi^{k})=\mathsf{Proj}_{ \mathrm{T}_{U}\mathcal{L}}(-2V_{\phi_{\eta_{k}}(\mathbf{x},\pi^{k})}U)\). Let \(\mathsf{e}^{1}_{\eta_{k}}(\mathbf{x},\pi^{k})=\|\operatorname{grad}_{U} \mathcal{L}_{\eta_{k}}(\mathbf{x},\pi^{k})\|_{\mathrm{F}}\) and \(\mathsf{e}^{2}_{\eta_{k}}(\mathbf{x},\pi^{k})=\|\nabla_{\alpha}\mathcal{L}_{ \eta_{k}}(\mathbf{x},\pi^{k})\|_{1}+\|\nabla_{\beta}\mathcal{L}_{\eta_{k}}( \mathbf{x},\pi^{k})\|_{1}\). The \((\epsilon_{1},\epsilon_{2})\)-stationary point of (11) and the connections of the approximate stationary points of problems (11) and (1) are given as follows.

**Definition 2.3**.: We say \(\tilde{\mathbf{x}}\in\mathcal{M}\) an \((\epsilon_{1},\epsilon_{2})\)-stationary point of problem (11) (with fixed \(\eta_{k}\) and \(\pi^{k}\)) if \(\mathsf{e}^{1}_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k})\leq\epsilon_{1}\) and \(\mathsf{e}^{2}_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k})\leq\epsilon_{2}\).

**Theorem 2.4**.: _Suppose \(\tilde{\mathbf{x}}=(\tilde{\alpha},\tilde{\beta},\tilde{U})\in\mathcal{M}\) with \(\|\zeta_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k})\|_{1}=1\) is an \((\epsilon_{1},\epsilon_{2})\)-stationary point of problem (11). Then, we have_

\[\|\mathsf{Proj}_{\mathrm{T}_{\mathrm{T}_{\mathrm{T}_{\mathrm{T}}}\tilde{U}}}(-2V_{ \tilde{\pi}}\tilde{U})\|_{\mathrm{F}}\leq\epsilon_{1}+2\|C\|_{\infty}\epsilon_{2},\] (13a) \[\langle\hat{\pi},Z(\tilde{\mathbf{x}})\rangle\leq(2\log n+\|\!\log \pi^{k}\|_{\mathrm{var}})\eta_{k}+(\|\tilde{\alpha}\|_{\mathrm{var}}+\|\tilde{ \beta}\|_{\mathrm{var}}+\|C\|_{\infty})\epsilon_{2},\] (13b)

_where \(\hat{\pi}:=\mathsf{Round}(\phi_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k}),\Pi(r,c))\) is a feasible matrix returned by running the rounding procedure "Round" given in [3, Algorithm 2] with input \(\phi_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k})\).__Remark 2.5_.: By Theorem 2.4, we can see that, given any \(\epsilon>0\), for fixed \(\pi^{k}\), by choosing \(\eta_{k}=O(\epsilon)\) with \(\epsilon_{1}=\epsilon_{2}=\epsilon\), an \(\epsilon\)-stationary point with bounded \((\tilde{\alpha},\tilde{\beta})\) (which can be found efficiently by iRBBS proposed in Section 3) of problem (11) can recover an \(\epsilon\)-stationary point of PRW problem (1). This may also be of independent interest for the case with fixed \(U\). In such case, one can return a feasible \(\epsilon\)-stationary point of the OT problem (4) (with fixed \(U\)) evaluated by the primal-dual gap other than the primal gap typically used in the literature, such as [3, Theorem 1].

Let \(\mathbf{x}^{0}\) be an initial point satisfying \(r^{\mathsf{T}}\mathbf{\alpha}^{0}=c^{\mathsf{T}}\beta^{0}\) and \(\|\zeta_{\eta_{1}}(\mathbf{x}^{0},\pi^{1})\|_{1}=1\) with \(\pi^{1}=\mathbf{1}\mathbf{1}^{\mathsf{T}}\). We require that \(\mathbf{x}^{k}=(\alpha^{k},\beta^{k},U^{k})\), the \((\epsilon_{k,1},\epsilon_{k,2})\)-stationary point of the subproblem (11) with \(k\geq 1\), satisfies the following conditions:

\[r^{\mathsf{T}}\mathbf{\alpha}^{k}=c^{\mathsf{T}}\beta^{k},\;\|\zeta_{\eta_{k} }(\mathbf{x}^{k},\pi^{k})\|_{1}=1,\;\mathcal{L}_{\eta_{k}}(\mathbf{x}^{k},\pi ^{k})\leq\min\{\mathcal{L}_{\eta_{k}}(\mathbf{x}^{k-1},\pi^{k}),\mathcal{L}_{ \eta_{k}}(\mathbf{x}^{0},\pi^{k})\}.\] (14)

These conditions are important to establish the convergence of REALM (as shown in Appendix A.4). The following key observation to (11) shows that \(\mathbf{x}^{k}\) satisfying the first two conditions in (14) can be easily obtained. We omit the detailed proof for brevity since they can be verified easily by noticing that \(\mathcal{L}_{\eta_{k}}(\mathbf{x},\pi^{k})=\mathcal{L}_{\eta_{k}}(\mathbf{x}^ {s},\pi^{k})\) where \(\mathbf{x}^{s}\) is defined in Proposition 2.6. The third condition in (14) can be easily satisfied by using any descent-like method starting from the better point of \(\mathbf{x}^{k-1}\) and \(\mathbf{x}^{0}\) to solve problem (11).

**Proposition 2.6**.: _For any \(\mathbf{x}=(\alpha,\beta,U)\in\mathcal{M}\), consider \(\mathbf{x}^{s}=(\alpha+v_{1}\mathbf{1},\beta+v_{2}\mathbf{1},U)\in\mathcal{M}\) with \(v_{1}=(c^{\mathsf{T}}\beta-r^{\mathsf{T}}\alpha+\eta_{k}\log(\|\zeta_{\eta_{ k}}(\mathbf{x},\pi^{k})\|_{1}))/2\) and \(v_{2}=(r^{\mathsf{T}}\alpha-c^{\mathsf{T}}\beta+\eta_{k}\log(\|\zeta_{\eta_{ k}}(\mathbf{x},\pi^{k})\|_{1}))/2\). Then we have that \(r^{\mathsf{T}}(\alpha+v_{1}\mathbf{1})=c^{\mathsf{T}}(\beta+v_{2}\mathbf{1})\) and \(\|\zeta_{\eta_{k}}(\mathbf{x}^{s},\pi^{k})\|_{1}=1\) and also that \(\mathcal{L}_{\eta_{k}}(\mathbf{x},\pi^{k})=\mathcal{L}_{\eta_{k}}(\mathbf{x}^ {s},\pi^{k})\) and \(\nabla_{\mathbf{x}}\mathcal{L}_{\eta_{k}}(\mathbf{x},\pi^{k})=\nabla_{ \mathbf{x}}\mathcal{L}_{\eta_{k}}(\mathbf{x}^{s},\pi^{k})\)._

With such \(\mathbf{x}^{k}\) in hand, we compute the candidate of the next estimate \(\pi^{k+1}\) as

\[\tilde{\pi}^{k+1}=\phi_{\eta_{k}}(\mathbf{x}^{k},\pi^{k}).\] (15)

Denote \(W^{k}\in\mathbb{R}^{n\times n}\) with \(W^{k}_{ij}=\min\{\eta_{k}\tilde{\pi}^{k+1}_{ij},\varphi(\mathbf{x}^{k})_{ij}\}\). The penalty parameter \(\eta_{k+1}\) is updated according to the progress of the complementarity violation [4], denoted by \(\|W^{k}\|_{\mathrm{F}}\). If \(\|W^{k}\|_{\mathrm{F}}\leq\gamma_{W}\|W^{k-1}\|_{\mathrm{F}}\) with \(\gamma_{W}\in(0,1)\), we keep \(\eta_{k+1}=\eta_{k}\) and update \(\pi^{k+1}=\tilde{\pi}^{k+1}\); otherwise we keep \(\pi^{k+1}=\pi^{k}\) and reduce \(\eta_{k+1}\) via

\[\eta_{k+1}=\min\left\{\gamma_{\eta}\eta_{k},\varrho_{k}/\|\log\pi^{k}\|_{ \infty}\right\},\] (16)

where \(\gamma_{\eta}\in(0,1)\) is a constant and \(\varrho_{k}\to 0\) with \(\varrho_{k}>0\).

We summarize the above discussion as the complete algorithm in Algorithm 1, whose convergence is established as follows.

**Theorem 2.7**.: _Let \(\{(\mathbf{x}^{k},\tilde{\pi}^{k})\}\) be the sequence generated by Algorithm 1 with \(\epsilon_{1}=\epsilon_{2}=\epsilon_{c}=0\) and \((\mathbf{x}^{\infty},\tilde{\pi}^{\infty})\) be a limit point of \(\{(\mathbf{x}^{k},\tilde{\pi}^{k})\}\). Then, \((\mathbf{x}^{\infty},\tilde{\pi}^{\infty})\) is a stationary point of PRW problem (1)._

_Remark 2.8_.: Our proposed REALM is a nontrivial extension of the exponential ALM from the Euclidean case [47, 20, 51, 52] to the Riemannian case. The following two differences distinguish our proposed REALM from the existing exponential ALM (e.g., the one proposed in [20]: (i) **Measure of complementarity.** The measure of complementarity used in our proposed REALM is motivated by the direct use of the complementarity condition adopted in the classical (quadratic) ALM, while that used in [20] is a variant of the measure for the exponential case. (ii) **Conditions on global convergence.** To guarantee the global convergence of the exponential ALMs, some (strong) constraint qualifications, the boundness of the iterates, and the feasibility of the limit point of the iterates generally need to be assumed; see Proposition 2.1 and Theorem 2.1 in [20] for the corresponding results. In contrast, for our considered PRW distance problem (6), we can prove the boundness of the iterates generated by REALM without making the assumption and establish the global convergence of REALM without explicitly dealing with the constraint qualification assumption. This advantage is mainly due to the _essential_ changes in the proposed REALM (compared with the existing exponential ALMs), i.e., specific conditions (14) and (16) (motivated by (13b)) on the solution of subproblems and the adopted measure of complementarity.

Moreover, it would be possible to extend the analysis in [20] to prove that the penalty parameter \(\eta_{k}\) in Algorithm 1 is bounded away from zero if the Riemannian versions of the three conditions hold, including the linear independence constraint qualification, the strict complementarity condition, and the second-order sufficient condition. However, these three conditions might not be easy to check since we do not have prior knowledge of the solution.

_Remark 2.9_.: We cannot establish the iteration complexity of Algorithm 1 due to the following two main difficulties: (i) characterizing the connection between the two complementarity measures \(\|W^{k}\|_{\mathsf{F}}\) and \(e^{k}:=\langle\hat{\pi}^{k},Z(\mathbf{x}^{k})\rangle\) with \(\hat{\pi}^{k}:=\mathsf{Round}(\phi_{\eta_{k}}(\mathbf{x}^{k},\pi^{k}),\Pi(r,c))\) at the approximate stationary point of the subproblem; (ii) establishing the relationship between \(\eta_{k}\tilde{\pi}_{ij}^{k+1}\) and \(\varphi(\mathbf{x}^{k})_{ij}\). Thanks to Theorem 2.4, we can slightly modify Algorithm 1 to establish the iteration complexity. By modifying the "if" condition in Line 6 of Algorithm 1 as "\(\|W^{k}\|_{\mathsf{F}}\leq\gamma_{W}\|W^{k-1}\|_{\mathsf{F}}\) and \(e^{k}\leq\gamma_{W}e^{k-1}\)", and leveraging the connection between the approximate stationary points of the subproblem and the original problem as proven in (13), we know that the modified Algorithm 1 will terminate within at most \(\mathcal{O}(\max\{\log\epsilon_{1}^{-1},\log\epsilon_{2}^{-1},T_{k}\})\) iterations, where \(T_{k}:=\min\{k\ |\ \varrho_{k}\leq\epsilon_{c}\}\).

_Remark 2.10_.: Problem (6) can also be solved by the Riemannian ALMs based on the quadratic penalty function [37, 55]. However, the subproblems therein have four blocks of the variable, i.e., (\(\alpha,\beta,y,\,U\)), and some customized solvers are needed to solve them. Moreover, the connections between the stationary points of problem (1) and the subproblems therein remain unclear.

## 3 A Practical iRBBS for Solving Subproblem (11)

At first glance, (11) is a three-block optimization problem and can be solved by R(A)BCD proposed by [28]. However, as stated therein, tuning the stepsizes for updating \(U\) is not easy for R(A)BCD. In sharp contrast, we understand (11) as optimization with only one variable \(U\) as follows:

\[\min_{U\in\mathcal{U}}\,\big{\{}q(U):=\min_{\alpha\in\mathbb{R}^{n},\beta\in \mathbb{R}^{n}}\mathcal{L}_{\eta_{k}}(\mathbf{x},\pi^{k})\big{\}}.\] (17)

By [34, Lemma 3.1], we know that \(q(\cdot)\) is differentiable over \(\mathbb{R}^{d\times k}\). Here, we give a new formulation of \(\operatorname{grad}q(U)\), which can provide more insights into approximating \(\operatorname{grad}q(U)\).

**Lemma 3.1**.: _Let \((\alpha_{U}^{*},\beta_{U}^{*})\in\operatorname{argmin}_{\alpha\in\mathbb{R}^{ n},\beta\in\mathbb{R}^{n}}\mathcal{L}(\mathbf{x},\pi^{k})\) and \(\mathbf{x}_{U}^{*}=(\alpha_{U}^{*},\beta_{U}^{*},U)\). Then we have \(\operatorname{grad}q(U)=\operatorname{grad}_{U}\mathcal{L}_{\eta_{k}}( \mathbf{x}_{U}^{*},\pi^{k})=\mathsf{Proj}_{\Gamma_{U}\mathcal{U}}(-2V_{\phi_{ \eta_{k}}(\mathbf{x}_{U}^{*},\pi^{k})}U)\)._

Hence we could use the Riemannian gradient descent (RGD) method [2] to solve problem (11). Letting \(\tau_{t}>0\) be some stepsize, the main iterations of RGD are given as

\[U^{t+1}=\mathsf{Retr}_{U^{t}}\big{(}-\tau_{t}\operatorname{grad}q(U^{t})\big{)}.\] (18)

However, RGD (18) needs to calculate \((\alpha_{U}^{*},\beta_{U}^{*})\) exactly, which can be challenging (or might be unnecessary) to do. Motivated by the well-established inexact gradient type methods for optimization in the Euclidean space [11, 26, 46, 17, 39, 7, 52], we propose an inexact RGD framework. Let \(\mathbf{x}^{t}=(\alpha^{t},\beta^{t},U^{t})\) with \((\alpha^{t},\beta^{t})\approx(\alpha_{U^{t}}^{*},\beta_{U^{t}}^{*})\), wherein the inexactness level is determined by \(\mathsf{e}_{\eta_{k}}^{2}(\mathbf{x}^{t},\pi^{k})=\|\nabla_{\alpha}\mathcal{L }_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})\|_{1}+\|\nabla_{\beta}\mathcal{L}_{\eta _{k}}(\mathbf{x}^{t},\pi^{k})\|_{1}\leq\theta_{t}\) for given \(\theta_{t}\). By Lemma 3.1, we use

\[\xi^{t}:=\operatorname{grad}_{U}\mathcal{L}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k} )=\mathsf{Proj}_{\Gamma_{U}\mathcal{U}}\big{(}-2V_{\phi_{\eta_{k}}(\mathbf{x} ^{t},\pi^{k})}U^{t}\big{)}\]

to approximate \(\operatorname{grad}q(U^{t})\). Then, we perform an inexact RGD with \(\operatorname{grad}q(U^{t})\) in (18) replaced by \(\xi^{t}\). More specifically, given the inexactness parameter \(\theta_{t+1}\geq 0\) and the stepsize \(\tau_{t}\geq 0\), we update \(\mathbf{x}^{t+1}=(\alpha^{t+1},\beta^{t+1},U^{t+1})\) with \(U^{t+1}\) and \((\alpha^{t+1},\beta^{t+1})\approx(\alpha_{U^{t+1}}^{*},\beta_{U^{t+1}}^{*})\) satisfying

\[U^{t+1}=\mathsf{Retr}_{U^{t}}\big{(}-\tau_{t}\xi^{t}\big{)}\,, \text{(inexact RGD step)}\] (19a) \[\mathsf{e}_{\eta_{k}}^{2}(\mathbf{x}^{t+1},\pi^{k})\leq\theta_{t +1}. \text{(inexactness criterion)}\] (19b)To make iRGD (19) practical, the first main ingredient is how to compute \((\alpha^{t+1},\beta^{t+1})\) such that (19b) holds. Given \(U^{t+1}\in\mathcal{U}\), \(\alpha^{(0)}=\alpha^{t}\), and \(\beta^{(0)}=\beta^{t}\), for \(\ell=0,1,\ldots\), we adopt the block coordinate descent method to update

\[\alpha^{(\ell+1)}=\operatorname*{argmin}_{\alpha\in\mathbb{R}^{n}}\mathcal{L}_ {\eta_{k}}(\alpha,\beta^{(\ell)},U^{t+1},\pi^{k}),\quad\beta^{(\ell+1)}= \operatorname*{argmin}_{\beta\in\mathbb{R}^{n}}\mathcal{L}_{\eta_{k}}(\alpha^ {(\ell+1)},\beta,U^{t+1},\pi^{k}).\] (20)

Note that \(\alpha^{(\ell+1)}\) and \(\beta^{(\ell+1)}\) admit the closed-form solutions as follows:

\[\alpha^{(\ell+1)}=\alpha^{(\ell)}-\eta_{k}\log r+\eta_{k}\log(\zeta^{(\ell)} \mathbf{1}),\quad\beta^{(\ell+1)}=\beta^{(\ell)}-\eta_{k}\log c+\eta_{k}\log(( \zeta^{(\ell+\frac{1}{2})})^{\mathsf{T}}\mathbf{1}),\] (21)

where \(\zeta^{(\ell)}:=\zeta_{\eta_{k}}(\alpha^{(\ell)},\beta^{(\ell)},U^{t+1},\pi^{ k})\) and \(\zeta^{(\ell+\frac{1}{2})}:=\zeta_{\eta_{k}}(\alpha^{(\ell+1)},\beta^{(\ell)},U ^{t+1},\pi^{k})\) (see (7) for its definition). Note that for fixed \(U^{t+1}\) and \(\pi^{k}=\mathbf{1}\mathbf{1}^{\mathsf{T}}\), (21) reduces to the famous Sinkhorn iteration [15]. Therefore, we still name (21) as the Sinkhorn iteration. It is easy to verify that \(\|\zeta^{(\ell+\frac{1}{2})}\|_{1}=\|\zeta^{(\ell+1)}\|_{1}=1\) (see also [28, Remark 3.1]). By (12), we have

\[\pi^{(\ell+\frac{1}{2})}:=\phi_{\eta_{k}}(\alpha^{(\ell+1)},\beta^{(\ell)},U^ {t+1},\pi^{k})=\zeta^{(\ell+\frac{1}{2})},\ \pi^{(\ell+1)}:=\phi_{\eta_{k}}(\alpha^{(\ell+1)},\beta^{(\ell+1)},U^{t+1}, \pi^{k})=\zeta^{(\ell+1)}.\] (22)

From the update of \(\beta^{(\ell+1)}\), we have \((\pi^{(\ell+1)})^{\mathsf{T}}\mathbf{1}-c=0\). Therefore, to make condition (19b) hold, we stop the Sinkhorn iteration once

\[\|\pi^{(\ell+1)}\mathbf{1}-r\|_{1}\leq\theta_{t+1},\] (23)

and set \(\alpha^{t+1}=\alpha^{(\ell+1)}\), \(\beta^{t+1}=\beta^{(\ell+1)}\). Recalling the calculation of the gradient and the definition of \(\mathsf{e}^{2}_{\eta_{k}}(\mathbf{x}^{t+1},\pi^{k})\) after (12), we have

\[\mathsf{e}^{2}_{\eta_{k}}(\mathbf{x}^{t+1},\pi^{k})=\|\pi^{(\ell+1)}\mathbf{1} -r\|_{1}.\]

Next, we choose the stepsize \(\tau_{t}\) in (19a). Since the accurate function and gradient information of \(q(U^{t})=\mathcal{L}_{\eta_{k}}(\mathbf{x}^{*}_{U^{t}},\pi^{k})\) is unavailable, we cannot expect to build the linesearch condition based on \(q(U^{t})\) and need to find some appropriate potential function instead. Considering that \(\mathcal{L}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})\) is an approximation of \(q(U^{t})\) and the approximation error is controlled by \(\mathsf{e}^{2}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})\), it is thus desirable that some combinations of \(\mathcal{L}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})\) and \(\mathsf{e}^{2}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})\) will be smaller than the corresponding values at the previous iteration. Given \(\rho\in[0,\eta_{k}/2)\), we define the potential function as

\[E_{\rho}(\mathbf{x}^{t})=\mathcal{L}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})+\rho( \mathsf{e}^{2}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k}))^{2}\] (24)

and require the stepsize \(\tau_{t}\) to satisfy the following nonmonotone line search condition:

\[E_{\rho}(\mathbf{x}^{t+1})\leq E^{r}_{t}-\delta_{1}\tau_{t}\|\xi^{t}\|^{2}_{ \mathsf{F}}-\Big{(}\frac{\eta_{k}}{2}-\rho\Big{)}(\mathsf{e}^{2}_{\eta_{k}}( \mathbf{x}^{t+1},\pi^{k}))^{2},\] (25)

where \(E^{r}_{t+1}=(\gamma Q_{t}E^{r}_{t}+E_{\rho}(\mathbf{x}^{t+1}))/Q_{t+1}\) and \(Q_{t+1}=\gamma Q_{t}+1\) with a constant \(\gamma\in[0,1)\) and \(E^{r}_{0}=E_{\rho}(\mathbf{x}^{0})\), \(Q_{0}=1\); see [54]. Such \(\tau_{t}\) can be found by adopting the simple backtracking line search technique starting from an initial guess of the stepsize \(\tau^{(0)}_{t}\). Owing to the excellent performance of the BB method in Riemannian optimization [50, 32, 23, 25, 31, 24], we choose the initial guess \(\tau^{(0)}_{t}\) for \(t\geq 1\) as a new Riemannian BB stepsize with safeguards:

\[\tau^{(0)}_{t}=\min\{\max\{\tau^{\mathrm{BB}}_{t},\tau_{\min}\},\tau_{\max}\},\] (26)

where \(\tau_{\max}>\tau_{\min}>0\) are preselected stepsize safeguards and \(\tau^{\mathrm{BB}}_{1}=\tau^{\mathrm{BB2}}_{1}\) and for \(t\geq 2\), we set \(\tau^{\mathrm{BB}}_{t}=\min\{\tau^{\mathrm{BB2}}_{t-1},\tau^{\mathrm{BB2}}_{t}, \max\{\tau^{\mathrm{new}}_{t},0\}\}\) if \(\tau^{\mathrm{BB2}}_{\tau}<\varkappa_{t}\tau^{\mathrm{BB1}}_{t}\) and set \(\tau^{\mathrm{BB}}_{t}=\tau^{\mathrm{BB1}}_{t}\) otherwise. Here, \(\tau^{\mathrm{BB1}}_{t}=\|U^{t}-U^{t-1}\|^{2}_{\mathsf{F}}/|\langle U^{t}-U^{t-1}, \xi^{t}-\xi^{t-1}\rangle|,\tau^{\mathrm{BB2}}_{t}=|\langle U^{t}-U^{t-1},\xi^ {t}-\xi^{t-1}\rangle|/\|\xi^{t}-\xi^{t-1}\|^{2}_{\mathsf{F}}\), and \(\tau^{\mathrm{new}}_{t}\) is chosen according to [30, Eq. (2.15)]. In our numerical tests, we set the initial \(\varkappa_{t}\) to be 0.05 and update \(\varkappa_{t+1}=\varkappa_{t}/1.02\) if \(\tau^{\mathrm{BB2}}_{t}/\tau^{\mathrm{BB1}}_{t}<\varkappa_{t}\) and update \(\varkappa_{t+1}=1.02\varkappa_{t}\) otherwise.

We are ready to summarize the complete iRBDS in Algorithm 2. The overall complexity of Algorithm 2 to find an \((\epsilon_{1},\epsilon_{2})\)-stationary point of problem (1) is in the same order as that of R(A)BCD.

**Theorem 3.2**.: _By choosing \(\epsilon_{1}=\epsilon^{\prime}_{1}/2\), \(\epsilon_{2}=\min\{\epsilon^{\prime}_{1}/(4\|C\|_{\infty}),\epsilon^{\prime}_{2}/(4 \eta_{k}\Psi+6\|C\|_{\infty})\}\) with \(\Psi=\|\log\pi^{k}\|_{\mathrm{var}}+\max\{\|\log r\|_{\mathrm{var}},\|\log c \|_{\mathrm{var}}\}\) and \(\eta_{k}=\epsilon^{\prime}_{2}/(4\log n+2\|\log\pi^{k}\|_{\mathrm{var}})\), Algorithm 2 can return an \((\epsilon^{\prime}_{1},\epsilon^{\prime}_{2})\)-stationary point of problem (1) in \(\mathcal{O}(T_{\epsilon^{\prime}_{1},\epsilon^{\prime}_{2}})\) iterations with_

\[T_{\epsilon^{\prime}_{1},\epsilon^{\prime}_{2}}=\max\big{\{}(\epsilon^{\prime}_{1} )^{-2},(\epsilon^{\prime}_{2})^{-2}\big{\}}(\epsilon^{\prime}_{2})^{-1}.\]

_If \(\theta_{t}\geq 2R^{t}/(\eta_{k}(\ell_{\max}-2+\sqrt{2}))\) with \(\ell_{\max}\geq 1\) and \(R^{t}=\|C(U^{t})\|_{\mathrm{var}}+\eta_{k}\Psi,\) the total number of Sinkhorn iterations is \(\mathcal{O}(\ell_{\max}T_{\epsilon```
1Input: Choose \(\tau_{\max}>\tau_{\min}>0\), \(\tau_{0}^{(0)}>0\), \(\epsilon_{1},\epsilon_{2}\geq 0\), \(\sigma,\delta_{1}\in(0,1)\), \(\rho\in[0,\eta_{k}/2)\), \(\gamma\in[0,1)\), and \((\alpha^{-1},\beta^{-1},U^{0})\in\mathcal{M}\). Set \(\alpha^{(0)}=\alpha^{-1},\beta^{(0)}=\beta^{-1}\) and perform the Sinkhorn iteration (21) at \(U^{0}\) until (23) holds with \(\theta_{0}=1\) for some \(\ell\). Set \(\alpha^{0}=\alpha^{(\ell+1)},\beta^{0}=\beta^{(\ell+1)}\).
2for\(t=0,1,\ldots\)do
3 Compute \(\xi^{t}=\operatorname{grad}_{U}\mathcal{L}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})\);
4if\(\|\xi^{t}\|_{\mathbb{F}}\leq\epsilon_{1}\)and \(\mathbf{e}_{\eta_{k}}^{2}(\mathbf{x}^{t},\pi^{k})\leq\epsilon_{2}\)thenreturn\(\mathbf{x}^{t}\);
5for\(s=0,1,\ldots\)do
6 Set \(U^{t+1}=\operatorname{Retr}_{U^{t}}(-\tau_{t}\xi^{t})\) with \(\tau_{t}=\tau_{t}^{(0)}\sigma^{s}\) and update \(\theta_{t+1}\) (e.g., via (27) further ahead);
7 Set \(\alpha^{(0)}=\alpha^{t}\) and \(\beta^{(0)}=\beta^{t}\) and perform the Sinkhorn iteration (21) at \(U^{t+1}\) until (23) holds for some \(\ell\); set \(\alpha^{t+1}=\alpha^{(\ell+1)}\) and \(\beta^{t+1}=\beta^{(\ell+1)}\).
8if(25) holdsthenbreak; ```

**Algorithm 2**A practical iRBBS for solving problem (11).

_Remark 3.3_.: The basic idea of proposing Algorithm 2 is sharply different from that of R(A)BCD developed in [28]. Ours is based on the inexact RGD viewpoint, while the latter is based on the BCD approach. Such an inexact RGD viewpoint enables us to choose the stepsize adaptively via leveraging the efficient BB stepsize. Actually, tuning the best stepsize for the \(U\)-update in R(A)BCD is nontrivial. It is remarked in [28, Remark 4.1] that _"the adaptive algorithms RABCD and RAGAS are also sensitive to the step size, though they are usually faster than their non-adaptive versions RBCD and RGAS."_ Numerical results in Section 4.1 show the higher efficiency of our iRBBS over R(A)BCD.

_Remark 3.4_.: Although the inexact gradient type methods have been well explored in the Euclidean case [11; 26; 46; 17; 39; 7; 52], to our best knowledge, there are little results for the Riemannian case and on how to choose the stepsizes adaptively for general nonlinear objective functions. One exception is R(A)GAS proposed in [34], which can be understood as the inexact RGD method. However, it needs to compute the inexact Riemannian gradient with relatively high accuracy and essentially uses the constant-like stepsizes. In contrast, our iRBBS allows to compute the inexact Riemannian gradient with low accuracy and choose the stepsize adaptively.

_Remark 3.5_.: It might be better to use possible multiple Sinkhorn iterations rather than only one iteration as done in R(A)BCD in updating \(\alpha\) and \(\beta\) from the computational point of view. The cost of updating \(\alpha^{(l+1)}\) and \(\beta^{(\ell+1)}\) via one Sinkhorn iteration (21) is \(\mathcal{O}(n^{2})\). In contrast, the cost of updating \(U^{t+1}\) via performing a RGD step (19a) is \(\mathcal{O}(ndk+n^{2}k+dk^{2})\), wherein the main cost is to compute \(V_{\phi_{k}(\mathbf{x}^{t},\pi^{k})}U^{t}\), which can be done by observing \(V_{\pi}U=X\operatorname{Diag}(\pi\mathbf{1})X^{\mathsf{T}}U+Y\operatorname{ Diag}(\pi^{\mathsf{T}}\mathbf{1})Y^{\mathsf{T}}U-X\pi Y^{\mathsf{T}}U-Y\pi^{ \mathsf{T}}X^{\mathsf{T}}U\). Considering that the cost of updating \(\alpha\) and \(\beta\) is much less than that of updating \(U\), it is reasonable to update \(\alpha\) and \(\beta\) multiple times and update \(U\) only once.

## 4 Experimental Results

In this section, we conduct numerical experiments on six Shakespeare operas to evaluate the performance of our proposed approaches; see Dataset C.1 for a more detailed description of the dataset. All methods are implemented in MATLAB. More numerical results can be found in Appendix D. The codes are available from https://github.com/bjiangopt/ReALM.

### Comparison with R(A)BCD on Solving Subproblem (11)

Subproblem (11) with \(\pi^{k}=\mathbf{1}\mathbf{1}^{\mathsf{T}}\) and a relatively small \(\eta_{k}=\eta\) is used to compute the PRW distance in [28]. We choose \(\eta=0.1\) as done in [28]. Since [28] has shown the superiority of R(A)BCD over R(A)GAS proposed in [34], we mainly compare our proposed iRBBS, namely, Algorithm 2, with R(A)BCD. For iRBBS and R(A)BCD, we use the same stopping conditions with \(\epsilon_{1}=2\|C\|_{\infty}\epsilon_{2}\) (motivated by (13a)) and \(\epsilon_{2}=10^{-6}\max\{\|r\|_{\infty},\|c\|_{\infty}\}\). To make the residual error more comparable, we choose

\[\theta_{0}=1,\quad\theta_{t+1}=\max\left\{\theta\cdot\frac{\mathbf{e}_{\eta_{k }}^{1}(\mathbf{x}^{t},\pi^{k})}{\epsilon_{1}},1\right\}\epsilon_{2},\quad t \geq 0,\] (27)

[MISSING_PAGE_FAIL:9]

\(\eta_{k}=\eta_{\min}\). Subproblem (11) in REALM is solved by iRBBS. If \(\max\{\|r\|_{\infty},\|c\|_{\infty}\}\geq 500\eta_{k}\) or \(\|C(U^{t})-\eta_{k}\log\pi^{k}\|_{\mathrm{var}}\geq 900\eta_{k}\), we set \(\theta=10\) and perform the Sinkhorn iteration (21); otherwise, we set \(\theta=0.1\) and perform an equivalent formulation of Sinkhorn iteration (28).

The results over 20 runs on Dataset C.1 are reported in Table 2. In this table, the terms "nSk\({}_{\log}\)" and "nSk\({}_{\exp}\)" mean the total numbers of Sinkhorn iterations (21) and (28), respectively, the pair "\(k_{1}\)/\(k\)" in the column "iter" means that the corresponding algorithm stops at the \(k\)-iteration and updates the multiplier matrix \(k_{1}\) times. To save space, we only report instances where one method can return the value "\(\widehat{\mathcal{P}}_{k}^{2}\)" larger than 1.005 times of the smaller one of the two \(\widehat{\mathcal{P}}_{k}^{2}\) values returned by the two methods. The better "\(\widehat{\mathcal{P}}_{k}^{2}\)" is marked in **bold**. Besides, the average performance over all 15 instances is also kept in the "AVG" line.

From Table 2, we can observe that REALM-\((0.007,0.9)\) can not only return better solutions than REALM-\((0.0035,0)\) but also is about 5.2x faster. On average, REALM-\((0.007,0.9)\) updates the multiplier matrix 8 times in 15 total iterations, which shows that updating the multiplier matrix does help. The reasons why REALM with updating the multiplier matrix outperforms REALM without updating the multiplier matrix in terms of solution quality and speed are as follows. First, updating the multiplier matrix in REALM can keep the solution quality even using a larger \(\eta_{k}\). Second, solving the subproblem with a larger \(\eta_{k}\) is always easier, which enables that REALM-\((0.07,0.9)\) computes less \(\mathrm{grad}_{U}\,\mathcal{L}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})\) and performs less Sinkhorn iterations (21) which involves computing the log-sum-exp function \(\log\sum_{i}\exp(x_{i}/\eta_{k})=x_{\max}/\eta_{k}+\log\sum_{i}\exp((x_{i}-x_{ \max})/\eta_{k})\) for small \(\eta_{k}\).

## 5 Concluding Remarks

In this paper, we considered the computation of the PRW distance. By reformulating this problem as an optimization problem over the Cartesian product of the Stiefel manifold and the Euclidean space with additional nonlinear inequality constraints, we proposed a method called REALM. The convergence of REALM was also established. To solve the subproblem in REALM efficiently, we developed a practical iRBBS method with convergence and iteration complexity guarantees, wherein the Riemannian BB stepsize (based on the inexact Riemannian gradient information) and Sinkhorn iterations are employed. The complexity of iRBBS to attain an \(\epsilon\)-stationary point of the original PRW distance problem matches the best known iteration complexity result. Numerical results showed that, compared with the state-of-the-art methods, our proposed REALM and iRBBS methods have advantages in solution quality and speed.

Moreover, our proposed REALM and iRBBS can also be extended to solve some important minimax problems over the Riemannian manifolds arising from machine learning, such as the fair PCA problem [45] and the projection robust Wasserstein barycenters [27], etc.

Lastly, a limitation of our work is that we did not establish the positive lower bound of \(\eta_{k}\) in REALM, despite the fact that REALM performs well in practice and can avoid too small \(\eta_{k}\) in many cases. We shall investigate the conditions under which it is possible to establish a lower bound of \(\eta_{k}\) in REALM. This can be achievable by extending the analysis and conditions in [20] to the Riemannian case.

\begin{table}
\begin{tabular}{l r r r r r r r r r} \hline \hline  & \multicolumn{2}{c}{\(\widehat{\mathcal{P}}_{k}^{2}\)} & \multicolumn{2}{c}{nGrad} & \multicolumn{2}{c}{nSk\({}_{\exp}\)/nSk\({}_{\log}\)} & \multicolumn{2}{c}{time} & \multicolumn{2}{c}{iter} \\ \cline{2-10} data & a & b & a & b & a & b & a & b & a & b \\ \hline H5/DC & 0.09270 & **0.10985** & 1081 & 675 & 23809/8267 & 97860 & 84.1 & 8.6 & 0.0/8.0 & 8.0/15.0 \\ H/MV & 0.06378 & **0.06424** & 1116 & 727 & 3864/13382 & 309970 & 175.7 & 14.9 & 0.0/8.0 & 8.0/15.0 \\ H/RJ & 0.21706 & **0.22607** & 547 & 788 & 3091/4062 & 1174/7470 & 63.3 & 15.6 & 0.0/8.0 & 8.0/15.0 \\ JC/MV & **0.06267** & 0.06255 & 1324 & 711 & 12875/5727 & 170730 & 53.3 & 5.9 & 0.0/8.0 & 8.0/15.0 \\ JC/O & 0.04221 & **0.04277** & 1650 & 1100 & 4997/26612 & 35031/4500 & 204.7 & 46.7 & 0.0/8.0 & 8.0/15.0 \\ MV/O & **0.04181** & 0.03661 & 890 & 822 & 6101/5289 & 31651/0 & 57.0 & 13.3 & 0.0/8.0 & 8.0/15.0 \\ AVG & 0.11338 & **0.11461** & 863 & 787 & 7824/7364 & 20261/300 & 78.0 & 15.0 & 0.0/8.0 & 8.0/15.0 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Average results of REALM for Dataset C.1, “a” and “b” stand for REALM-\((0.0035,0)\) and REALM-\((0.007,0.9)\), respectively.

AcknowledgmentsThe work of Bo Jiang was supported in part by the National Natural Science Foundation of China (NSFC) under Grant 11971239, Grant 12371314, and in part by the Natural Science Foundation of the Higher Education Institutions of Jiangsu Province under Grant 21KJA110002. The work of Ya-Feng Liu was supported in part by NSFC under Grant 11991020, Grant 11991021, and Grant 12288201.

## References

* [1] T. Abrishami, N. Guillen, P. Rule, Z. Schutzman, J. Solomon, T. Weighill, and S. Wu. Geometry of graph partitions via optimal transport. _SIAM Journal on Scientific Computing_, 42(5):A3340-A3366, 2020.
* [2] P.-A. Absil, R. Mahony, and R. Sepulchre. _Optimization Algorithms on Matrix Manifolds_. Princeton University Press, Princeton, 2008.
* [3] J. Altschuler, J. Niles-Weed, and P. Rigollet. Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration. _Advances in Neural Information Processing Systems_, 30:1961-1971, 2017.
* [4] R. Andreani, E. G. Birgin, J. M. Martinez, and M. L. Schuverdt. On augmented Lagrangian methods with general lower-level constraints. _SIAM Journal on Optimization_, 18(4):1286-1309, 2008.
* [5] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In _International Conference on Machine Learning_, pages 214-223. PMLR, 2017.
* [6] J.-D. Benamou, G. Carlier, M. Cuturi, L. Nenna, and G. Peyre. Iterative Bregman projections for regularized transportation problems. _SIAM Journal on Scientific Computing_, 37(2):A1111-A1138, 2015.
* [7] A. S. Berahas, L. Cao, and K. Scheinberg. Global convergence rate analysis of a generic line search algorithm with noise. _SIAM Journal on Optimization_, 31(2):1489-1518, 2021.
* [8] D. P. Bertsekas. _Constrained Optimization and Lagrange Multiplier Methods_. Academic Press, 1982.
* [9] N. Boumal. _An Introduction to Optimization on Smooth Manifolds_. Cambridge University Press, 2023.
* [10] N. Boumal, P.-A. Absil, and C. Cartis. Global rates of convergence for nonconvex optimization on manifolds. _IMA Journal of Numerical Analysis_, 39(1):1-33, 2019.
* [11] R. G. Carter. On the global convergence of trust region algorithms using inexact gradient information. _SIAM Journal on Numerical Analysis_, 28(1):251-265, 1991.
* [12] A. Chambolle and J. P. Contreras. Accelerated Bregman primal-dual methods applied to optimal transport and Wasserstein barycenter problems. _SIAM Journal on Mathematics of Data Science_, 4(4):1369-1395, 2022.
* [13] X. Chen, L. Guo, Z. Lu, and J. J. Ye. An augmented Lagrangian method for non-Lipschitz nonconvex programming. _SIAM Journal on Numerical Analysis_, 55(1):168-193, 2017.
* [14] A. Cherian and S. Aeron. Representation learning via adversarially-contrastive optimal transport. In _International Conference on Machine Learning_, pages 1820-1830. PMLR, 2020.
* [15] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. _Advances in Neural Information Processing Systems_, 26:2292-2300, 2013.
* [16] I. Deshpande, Y.-T. Hu, R. Sun, A. Pyrros, N. Siddiqui, S. Koyejo, Z. Zhao, D. Forsyth, and A. G. Schwing. Max-sliced Wasserstein distance and its use for GANs. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10648-10656, 2019.
* [17] O. Devolder, F. Glineur, and Y. Nesterov. First-order methods of smooth convex optimization with inexact oracle. _Mathematical Programming_, 146(1):37-75, 2014.
** [18] J.-P. Dussault. Augmented non-quadratic penalty algorithms. _Mathematical programming_, 99(3):467-486, 2004.
* [19] P. Dvurechensky, A. Gasnikov, and A. Kroshnin. Computational optimal transport: Complexity by accelerated gradient descent is better than by Sinkhorn's algorithm. In _International Conference on Machine Learning_, pages 1367-1376. PMLR, 2018.
* [20] N. Echebest, M. D. Sanchez, and M. L. Schuverdt. Convergence results of an augmented Lagrangian method using the exponential penalty function. _Journal of Optimization Theory and Applications_, 168(1):92-108, 2016.
* [21] R. Flamary, N. Courty, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 39:1853-1865, 2016.
* [22] N. Fournier and A. Guillin. On the rate of convergence in Wasserstein distance of the empirical measure. _Probability Theory and Related Fields_, 162(3):707-738, 2015.
* [23] B. Gao, X. Liu, X. Chen, and Y. Yuan. A new first-order algorithmic framework for optimization problems with orthogonality constraints. _SIAM Journal on Optimization_, 28(1):302-332, 2018.
* [24] B. Gao, X. Liu, and Y. Yuan. Parallelizable algorithms for optimization problems with orthogonality constraints. _SIAM Journal on Scientific Computing_, 41(3):A1949-A1983, 2019.
* [25] J. Hu, A. Milzarek, Z. Wen, and Y. Yuan. Adaptive quadratically regularized Newton method for Riemannian optimization. _SIAM Journal on Matrix Analysis and Applications_, 39(3):1181-1207, 2018.
* [26] Y.-Q. Hu and Y.-H. Dai. Inexact Barzilai-Borwein method for saddle point problems. _Numerical Linear Algebra with Applications_, 14(4):299-317, 2007.
* [27] M. Huang, S. Ma, and L. Lai. Projection robust Wasserstein barycenters. In _International Conference on Machine Learning_, pages 4456-4465. PMLR, 2021.
* [28] M. Huang, S. Ma, and L. Lai. A Riemannian block coordinate descent method for computing the projection robust Wasserstein distance. In _International Conference on Machine Learning_, pages 4446-4455. PMLR, 2021.
* [29] M. Huang, S. Ma, and L. Lai. A Riemannian block coordinate descent method for computing the projection robust Wasserstein distance. _arXiv:2012.05199, v5_, 2021.
* [30] Y.-K. Huang, Y.-H. Dai, and X.-W. Liu. Equipping the Barzilai-Borwein method with the two dimensional quadratic termination property. _SIAM Journal on Optimization_, 31(4):3068-3096, 2021.
* [31] B. Iannazzo and M. Porcelli. The Riemannian Barzilai-Borwein method with nonmonotone line search and the matrix geometric mean computation. _IMA Journal of Numerical Analysis_, 38(1):495-517, 2018.
* [32] B. Jiang and Y.-H. Dai. A framework of constraint preserving update schemes for optimization on Stiefel manifold. _Mathematical Programming_, 153(2):535-575, 2015.
* [33] S. Kullback. A lower bound for discrimination information in terms of variation (corresp.). _IEEE Transactions on Information Theory_, 13(1):126-127, 1967.
* [34] T. Lin, C. Fan, N. Ho, M. Cuturi, and M. Jordan. Projection robust Wasserstein distance and Riemannian optimization. _Advances in Neural Information Processing Systems_, 33:9383-9397, 2020.
* [35] T. Lin, Z. Zheng, E. Chen, M. Cuturi, and M. I. Jordan. On projection robust optimal transport: Sample complexity and model misspecification. In _International Conference on Artificial Intelligence and Statistics_, pages 262-270. PMLR, 2021.
* [36] T. Lin, N. Ho, and M. I. Jordan. On the efficiency of entropic regularized algorithms for optimal transport. _Journal of Machine Learning Research_, 23(137):1-42, 2022.
* [37] C. Liu and N. Boumal. Simple algorithms for optimization on Riemannian manifolds with constraints. _Applied Mathematics & Optimization_, 82:949-981, 2020.

* [38] H. Liu, A. M.-C. So, and W. Wu. Quadratic optimization with orthogonality constraint: Explicit Lojasiewicz exponent and linear convergence of retraction-based line-search and stochastic variance-reduced gradient methods. _Mathematical Programming_, 178(1):215-262, 2019.
* [39] Y.-F. Liu, X. Liu, and S. Ma. On the nonergodic convergence rate of an inexact augmented Lagrangian framework for composite convex programming. _Mathematics of Operations Research_, 44(2):632-650, 2019.
* [40] J. Niles-Weed and P. Rigollet. Estimation of Wasserstein distances in the spiked transport model. _Bernoulli_, 28(4):2663-2688, 2022.
* [41] F.-P. Paty and M. Cuturi. Subspace robust Wasserstein distances. In _International Conference on Machine Learning_, pages 5072-5081. PMLR, 2019.
* [42] G. Peyre and M. Cuturi. Computational optimal transport: With applications to data science. _Foundations and Trends(r) in Machine Learning_, 11(5-6):355-607, 2019.
* [43] J. Rabin, G. Peyre, J. Delon, and M. Bernot. Wasserstein barycenter and its application to texture mixing. In _International Conference on Scale Space and Variational Methods in Computer Vision_, pages 435-446. Springer, 2011.
* [44] E. W. Sachs and S. M. Sachs. Nonmonotone line searches for optimization algorithms. _Control and Cybernetics_, 40(4):1059-1075, 2011.
* [45] S. Samadi, U. Tantipongpipat, J. H. Morgenstern, M. Singh, and S. Vempala. The price of fair PCA: One extra dimension. _Advances in Neural Information Processing Systems_, 31:10999-11010, 2018.
* [46] M. Schmidt, N. Roux, and F. Bach. Convergence rates of inexact proximal-gradient methods for convex optimization. _Advances in Neural Information Processing Systems_, 24:1458-1466, 2011.
* [47] P. Tseng and D. P. Bertsekas. On the convergence of the exponential multiplier method for convex programming. _Mathematical Programming_, 60(1):1-19, 1993.
* [48] J.-P. Vial. Strong and weak convexity of sets and functions. _Mathematics of Operations Research_, 8(2):231-259, 1983.
* [49] J. Weed and F. Bach. Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance. _Bernoulli_, 25(4A):2620-2648, 2019.
* [50] Z. Wen and W. Yin. A feasible method for optimization with orthogonality constraints. _Mathematical Programming_, 142(1):397-434, 2013.
* [51] Y. Xie, X. Wang, R. Wang, and H. Zha. A fast proximal point method for computing exact Wasserstein distance. In _Uncertainty in Artificial Intelligence_, pages 433-453. PMLR, 2020.
* [52] L. Yang and K.-C. Toh. Bregman proximal point algorithm revisited: A new inexact version and its variant. _SIAM Journal on Optimization_, 32(3):1523-1554, 2022.
* [53] W. H. Yang, L.-H. Zhang, and R. Song. Optimality conditions for the nonlinear programming problems on Riemannian manifolds. _Pacific Journal of Optimization_, 10(2):415-434, 2014.
* [54] H. Zhang and W. W. Hager. A nonmonotone line search technique and its application to unconstrained optimization. _SIAM Journal on Optimization_, 14(4):1043-1056, 2004.
* [55] Y. Zhou, C. Bao, C. Ding, and J. Zhu. A semismooth Newton based augmented Lagrangian method for nonsmooth optimization on matrix manifolds. _Mathematical Programming_, 201:1-61, 2023.

The appendices are organized as follows. In Appendix A, we present more details on our proposed REALM in Section 2. Some proofs in Section 3 are given in Appendix B. Some details on the dataset and implementation in Section 4 are shown in Appendix C. Finally, we present additional numerical results to evaluate the performance of our proposed methods to compute the PRW distance (1) in Appendix D.

## Appendix A More Details on Our Proposed REALM in Section 2

In this section, we provide more materials on REALM in Section 2, including the first-order necessary conditions of our reformulation (6), relations between different \((\epsilon_{1},\epsilon_{2})\)-stationary points of the PRW distance problem (1), the connections of the approximate stationary points of problems (11) and (1), and the convergence analysis of Algorithm 1.

### First-Order Necessary Conditions of Problem (6)

**Lemma A.1** (First-order necessary conditions of problem (6)).: _Given \(\bar{\mathbf{x}}\in\mathcal{M}\) and \(\bar{y}\in\mathbb{R}\), suppose that \((\bar{\mathbf{x}},\bar{y})\) is a local minimizer of problem (6), then \((\bar{\mathbf{x}},-\varphi(\bar{\mathbf{x}})_{\min})\) is a stationary point of problem (6), namely, there exists \(\bar{\pi}\in\Pi(r,c)\) such that_

\[\mathsf{Proj}_{T_{\mathcal{C}}\mathcal{U}}\left(-2V_{\bar{\pi}}\bar{U}\right) =0,\quad\langle\bar{\pi},Z(\bar{\mathbf{x}})\rangle=0,\] (29)

_where \(Z(\bar{\mathbf{x}})\in\mathbb{R}^{n\times n}\) is defined as_

\[Z(\bar{\mathbf{x}})_{ij}=\varphi(\bar{\mathbf{x}})_{ij}-\varphi(\bar{\mathbf{ x}})_{\min}.\] (30)

Proof.: Since \((\bar{\mathbf{x}},\bar{y})\) is a local minimizer of problem (6), there must hold \(\bar{y}=-\varphi(\bar{\mathbf{x}})_{\min}=\max_{ij}\{-\varphi(\bar{\mathbf{x}} )_{ij}\}\). Moreover, such \(\bar{\mathbf{x}}\) is also a local minimizer of problem \(\min_{\mathbf{x}\in\mathcal{M}}h(\mathbf{x})\) with \(h(\mathbf{x})=r^{\mathsf{T}}\alpha+c^{\mathsf{T}}\beta-\varphi(\mathbf{x})_{ \min}\). For fixed \(\alpha\) and \(\beta\), it is easy to see that \(-\varphi(\mathbf{x})_{ij}+\|C\|_{\infty}\|U\|_{\mathsf{F}}^{2}\) is convex with respect to \(U\) since \(\|M_{ij}\|_{2}=C_{ij}\leq\|C\|_{\infty}\). We thus know that the function

\[-\varphi(\bar{\mathbf{x}})_{\min}+\|C\|_{\infty}\|U\|_{\mathsf{F}}^{2}=\max_{ ij}\left\{-\varphi(\mathbf{x})_{ij}+\|C\|_{\infty}\|U\|_{\mathsf{F}}^{2}\right\}\]

is convex with respect to \(U\), which means that the function \(-\varphi(\bar{\mathbf{x}})_{\min}\) and thus \(h(\mathbf{x})\) is \(\|C\|_{\infty}\)-weakly convex with respect to \(U\)[48, Proposition 4.3]. Let \(\mathcal{A}(\bar{\mathbf{x}})=\{(i,j)\in[n]\times[n]\mid\varphi(\bar{\mathbf{ x}})_{ij}=\varphi(\bar{\mathbf{x}})_{\min}\}\). By [48, Proposition 4.6], we have

\[\partial h(\bar{\mathbf{x}})=(r,c,0)+\mathrm{conv}\left\{(-e_{i},-e_{j},-2M_{ ij}\bar{U})\mid(i,j)\in\mathcal{A}(\bar{\mathbf{x}})\right\},\]

where \(e_{i}\) is the \(i\)-th standard unit vector in \(\mathbb{R}^{n}\). Moreover, by [53, Theorem 4.1], [53, Theorem 5.1], and \(\mathrm{T}_{\bar{\mathbf{x}}}\mathcal{M}=\mathbb{R}^{n}\times\mathbb{R}^{n} \times\mathrm{T}_{\bar{\mathbf{x}}}\mathcal{U}\), there must hold that \(0\in\mathsf{Proj}_{\mathrm{T}_{\bar{\mathbf{x}}}\mathcal{M}}\partial h(\bar{ \mathbf{x}})\). Putting all the above things together shows that there exists \(\bar{\pi}\in\mathbb{R}_{+}^{n\times n}\) with \(\bar{\pi}_{ij}=0\) for all \((i,j)\not\in\mathcal{A}(\bar{\mathbf{x}})\) and \(\sum_{(i,j)\in\mathcal{A}(\bar{\mathbf{x}})}\bar{\pi}_{i,j}=1\) such that \(\bar{\pi}\in\Pi(r,c)\) and (29) hold. The proof is completed. 

### Explanations on Remark 2.2

From [28, Section B], we know that the \((\epsilon_{1},\epsilon_{2})\)-stationary point in [28, Definition 3.1] is stronger than that in [34, Definition 2.7]. To show that our Definition 2.1 of the \((\epsilon_{1},\epsilon_{2})\)-stationary point is stronger than both definitions, we only need to verify that \(\langle\tilde{\pi},Z(\bar{\mathbf{x}})\rangle\leq\epsilon_{2}\) can imply

\[\langle\tilde{\pi},C(\tilde{U})\rangle-\min_{\pi\in\Pi(r,c)}\langle\pi,C( \tilde{U})\rangle\leq\epsilon_{2},\]

which is clear since \(\langle\tilde{\pi},C(\tilde{U})\rangle-\min_{\pi\in\Pi(r,c)}\langle\pi,C( \tilde{U})\rangle\leq\langle\tilde{\pi},Z(\bar{\mathbf{x}})\rangle\). This inequality comes from the fact that problem (5) is a dual formulation of \(\min_{\pi\in\Pi(r,c)}\langle\pi,C(U)\rangle\) for fixed \(U\) and the strong duality theorem.

### Proof of Theorem 2.4

Proof.: We first show that (13a) is true. First, by [12, Lemma 3.2], we have \(\hat{\pi}\in\Pi(r,c)\) and

\[\|\hat{\pi}-\phi_{\eta_{k}}(\bar{\mathbf{x}},\pi^{k})\|_{1}\leq\|\phi_{\eta_{k }}(\bar{\mathbf{x}},\pi^{k})\mathbf{1}-r\|_{1}+\|\phi_{\eta_{k}}(\bar{ \mathbf{x}},\pi^{k})^{\mathsf{T}}\mathbf{1}-c\|_{1}=\mathsf{e}_{\eta_{k}}^{2}( \bar{\mathbf{x}},\pi^{k})\leq\epsilon_{2}.\] (31)Second, by the triangular inequality and the non-expansive property of the projection operator, we have

\[\|\mathsf{Proj}_{\mathrm{T}_{U}\mathcal{U}}(A_{1})\|_{\mathsf{F}} \leq\|\mathsf{Proj}_{\mathrm{T}_{U}\mathcal{U}}(A_{1})-\mathsf{ Proj}_{\mathrm{T}_{U}\mathcal{U}}(A_{2})\|_{\mathsf{F}}+\|\mathsf{Proj}_{\mathrm{T}_{U} \mathcal{U}}(A_{1})\|_{\mathsf{F}}\] \[\leq\|A_{1}-A_{2}\|_{\mathsf{F}}+\|\mathsf{Proj}_{\mathrm{T}_{U} \mathcal{U}}(A_{1})\|_{\mathsf{F}},\]

where \(A_{1},A_{2}\in\mathbb{R}^{d\times k}\). Hence, we have

\[\|\mathsf{Proj}_{\mathrm{T}_{U}\mathcal{U}}(-2V_{\tilde{\pi}}\tilde{U})\|_{ \mathsf{F}} \leq 2\|(V_{\tilde{\pi}}-V_{\phi_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{ k})})\tilde{U}\|_{\mathsf{F}}+\epsilon_{1}\leq 2\|V_{\tilde{\pi}}-V_{\phi_{\eta_{k}}( \tilde{\mathbf{x}},\pi^{k})}\|_{\mathsf{F}}+\epsilon_{1},\] (32)

where the first inequality uses \(\|\mathsf{Proj}_{\mathrm{T}_{U}\mathcal{U}}(-2V_{\phi_{\eta_{k}}(\tilde{ \mathbf{x}},\pi^{k})}\tilde{U})\|_{\mathsf{F}}\leq\epsilon_{1}\) and the second inequality is due to the fact that \(\|A\tilde{U}\|_{\mathsf{F}}\leq\|A\|_{\mathsf{F}}\) for any matrix \(A\in\mathbb{R}^{d\times d}\). Moreover, observe that

\[\|V_{\tilde{\pi}}-V_{\phi_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k})}\|_{\mathsf{ F}}=\Big{\|}\sum_{ij}(\hat{\pi}_{ij}-\phi_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k})_{ ij})M_{ij}\Big{\|}_{\mathsf{F}}\leq\|C\|_{\infty}\|\hat{\pi}-\phi_{\eta_{k}}( \tilde{\mathbf{x}},\pi^{k})\|_{1}.\]

Combining the above assertions with (31) and (32) yields (13a).

Next, we show that (13b) is also true. By the Cauchy-Schwarz inequality and (31), we have

\[\left\langle\hat{\pi},Z(\tilde{\mathbf{x}})\right\rangle =\] \[\leq\|Z(\tilde{\mathbf{x}})\|_{\infty}\epsilon_{2}+\left\langle \phi_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k}),Z(\tilde{\mathbf{x}})\right\rangle\] \[\leq(\|\tilde{\alpha}\|_{\mathrm{var}}+\|\tilde{\beta}\|_{ \mathrm{var}}+\|C\|_{\infty})\epsilon_{2}+\left\langle\phi_{\eta_{k}}(\tilde{ \mathbf{x}},\pi^{k}),Z(\tilde{\mathbf{x}})\right\rangle,\] (33)

where the second inequality uses (30), the definition of \(\varphi(\mathbf{x})\) in (5), and

\[\left\langle M_{ij},UU^{\mathsf{T}}\right\rangle=[C(U)]_{ij}\leq C_{ij}\leq\|C \|_{\infty}\quad\forall\;U\in\mathcal{U}.\] (34)

The remaining is to bound \(\left\langle\phi_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k}),Z(\tilde{\mathbf{x}})\right\rangle\). By \(\|\zeta_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k})\|_{1}=1\), (7), and (12), we have \(\varphi(\tilde{\mathbf{x}})_{ij}=\eta_{k}(\log\pi^{k}_{ij}-\log\phi_{\eta_{k} }(\tilde{\mathbf{x}},\pi^{k})_{ij})\) and \(\varphi(\tilde{\mathbf{x}})_{\min}\geq\eta_{k}(\log\pi^{k})_{\min}\). Again with (30), we have

\[\left\langle\phi_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k}),Z(\tilde{ \mathbf{x}})\right\rangle=\left\langle\phi_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{ k}),\varphi(\tilde{\mathbf{x}})\right\rangle-\varphi(\tilde{\mathbf{x}})_{\min}\] \[\leq\eta_{k}\left(\left\langle\log\pi^{k},\phi_{\eta_{k}}(\tilde{ \mathbf{x}},\pi^{k})\right\rangle+H(\phi_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k}) )-(\log\pi^{k})_{\min}\right),\]

which, together with \(H(\phi_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k}))\leq 2\log n\) and \(\left\langle\log\pi^{k},\phi_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k})\right\rangle \leq(\log\pi^{k})_{\max}\), further implies \(\left\langle\phi_{\eta_{k}}(\tilde{\mathbf{x}},\pi^{k}),Z(\tilde{\mathbf{x}}) \right\rangle\leq(2\log n+\|\log\pi^{k}\|_{\mathrm{var}})\eta_{k}\). This, together with (33), yields (13b). The proof is completed. 

### Proof of Theorem 2.7

We first use the requirements in (14) to show that \(r^{\mathsf{T}}\alpha^{k}\leq r^{\mathsf{T}}\alpha^{0}\) for all \(k\geq 0\).

With the first two requirements in (14), we have \(\mathcal{L}_{\eta_{k}}(\mathbf{x}^{\mathsf{k}},\pi^{k})=2r^{\mathsf{T}}\alpha^ {k}\). By the fact \(\|\zeta_{\eta_{1}}(\mathbf{x}^{0},\pi^{1})\|_{1}=1\) with \(\pi^{1}=\mathbf{1}\mathbf{1}^{\mathsf{T}}\), we know from (7) that \(\varphi(\mathbf{x}^{0})_{ij}\geq 0\). Due to the update rules of \(\pi^{k}\) and \(\eta_{k}\), we know that \(0<\pi^{k}_{ij}\leq 1\) and \(0<\eta_{k}\leq\eta_{1}\) for all \(k\geq 1\). Therefore, we have from (7) that

\[[\zeta_{\eta_{k}}(\mathbf{x}^{0},\pi^{k})]_{ij}=\pi^{k}_{ij}\exp\left(-\frac{ \varphi(\mathbf{x}^{0})_{ij}}{\eta_{k}}\right)\leq\exp\left(-\frac{\varphi( \mathbf{x}^{0})_{ij}}{\eta_{1}}\right)=[\zeta_{\eta_{1}}(\mathbf{x}^{0},\pi^{ 1})]_{ij},\]

which further implies \(\|\zeta_{\eta_{k}}(\mathbf{x}^{0},\pi^{k})\|_{1}\leq\|\zeta_{\eta_{1}}(\mathbf{x} ^{0},\pi^{1})\|_{1}=1\). With \(r^{\mathsf{T}}\alpha^{0}=c^{\mathsf{T}}\beta^{0}\), we know \(\mathcal{L}_{\eta_{k}}(\mathbf{x}^{0},\pi^{k})\leq 2r^{\mathsf{T}}\alpha^{0}\). By the third requirement in (14), we further have

\[r^{\mathsf{T}}\alpha^{k}\leq r^{\mathsf{T}}\alpha^{0},\quad\forall\;k\geq 0.\] (35)

Besides, since \(\mathbf{x}^{k}\) is an \((\epsilon_{k,1},\epsilon_{k,2})\)-stationary point of (11), we have

\[\left\|\mathsf{Proj}_{\mathrm{T}_{U^{k}}\mathcal{U}}\left(-2V_{\tilde{\pi}^{k+1} }U^{k}\right)\right\|_{\mathsf{F}}\leq\epsilon_{k,1},\quad\|r-\tilde{\pi}^{k+1} \mathbf{1}\|_{1}+\|c-(\tilde{\pi}^{k+1})^{\mathsf{T}}\mathbf{1}\|_{1}\leq \epsilon_{k,2}.\] (36)

We next consider two cases, which is motivated by the proofs of Theorem 3.1 in [13].

Case (i). \(\eta_{k}\) is bounded below by some threshold value \(\underline{\eta}>0\). Due to the update rule of \(\eta_{k+1}\), we can see that (16) is invoked only finite times. Besides, due to \(0<\pi^{k}_{ij}\leq 1\), without loss of generality,we assume \(\eta_{k}\equiv\eta\) for all \(k\geq 2\) and \(\lim_{k\to\infty}\pi_{ij}^{k}=\pi_{ij}^{\infty}\). Meanwhile, due to the update rule of \(\pi^{k}\), we have \(\pi_{ij}^{k}=\tilde{\pi}_{ij}^{k}\) for \(k\geq 2\) and

\[\big{|}\mathrm{min}\{\underline{\eta}\pi_{ij}^{k+1},\varphi(\mathbf{x}^{k})_{ ij}\}\big{|}\to 0\quad\forall\;(i,j)\in[n]\times[n].\] (37)

We now show that \(\{\mathbf{x}^{k}\}\) has at least a limit point. By (37), we must have \(\varphi(\mathbf{x}^{k})_{ij}\to 0\) for each \((i,j)\not\in\mathcal{A}(\pi^{\infty}):=\{(i,j)\in[n]\times[n]\mid\pi_{ij}^{ \infty}=0\}\) and there must exist \(K_{1}>0\) such that for all \(k\geq K_{1}\), there holds \(\varphi(\mathbf{x}^{k})_{ij}\geq-1\) for each \((i,j)\in\mathcal{A}(\pi^{\infty})\). Recalling the definition of \(\varphi(\mathbf{x})\) in (5), then we can conclude that there exists \(K_{2}>0\) such that for all \(k\geq K_{2}\), there holds that

\[\alpha_{i}^{k}+\beta_{j}^{k}+\langle M_{ij},U^{k}(U^{k})^{\mathsf{T}}\rangle \geq-1\quad\forall\;(i,j)\in[n]\times[n].\]

Multiplying both sides of the above assertions by \(c_{j}\) and then summing the obtained inequality from \(j=1\) to \(n\), we have

\[\alpha_{i}^{k}\geq-1-\sum\nolimits_{j}c_{j}\left\langle M_{ij},U^{k}(U^{k})^ {\mathsf{T}}\right\rangle-c^{\mathsf{T}}\beta^{k}\geq-1-\|C\|_{\infty}-r^{ \mathsf{T}}\alpha^{0},\] (38)

where the second inequality is due to \(r^{\mathsf{T}}\alpha^{k}=c^{\mathsf{T}}\beta^{k}\), (35), and (34). Combining (35) and (38), we further have

\[-1-\|C\|_{\infty}-r^{\mathsf{T}}\alpha^{0}\leq\alpha_{i}^{k}\leq r_{i}^{-1} \big{(}(1-r_{i})(1+\|C\|_{\infty}+r^{\mathsf{T}}\alpha^{0})+r^{\mathsf{T}} \alpha^{0}\big{)}.\]

Similarly, we can establish a similar bound for each \(\beta_{j}^{k}\). Recalling that \(U^{k}\) is in a compact set, it is thus safe to say that the sequence \(\{\mathbf{x}^{k}\}\) has at least one limit point, denoted by \(\mathbf{x}^{\infty}=\lim_{k\in\mathcal{K},k\to\infty}\mathbf{x}^{k}\). Again with \(\lim_{k\to\infty}\pi_{ij}^{k}=\pi^{\infty}\), we have from (37) that \(\min\{\underline{\eta}\pi_{ij}^{\infty},\varphi(\mathbf{x}^{\infty})_{ij}\}=0\), which, together with \(\pi_{ij}^{\infty}\geq 0\) and the fact

\[0\leq\langle\pi^{\infty},Z(\mathbf{x}^{\infty})\rangle=\sum_{ij}\pi_{ij}^{ \infty}\varphi(\mathbf{x}^{\infty})_{ij}-\varphi(\mathbf{x}^{\infty})_{\min},\]

further implies \(\varphi(\mathbf{x}^{\infty})_{\min}=0\) and \(\langle\pi^{\infty},Z(\mathbf{x}^{\infty})\rangle=0\). This shows that \((\mathbf{x}^{\infty},0)\) is feasible. Moreover, letting \(k\in\mathcal{K}\) go to infinity in (36), we further have \(\pi^{\infty}\in\Pi(r,c)\) and \(\|\mathsf{Proj}_{\mathrm{T}_{V^{\infty}\mathcal{U}}}\,(-2V_{\pi^{\infty}}U^{ \infty})\|_{\mathsf{F}}=0\). From Lemma A.1, we know that \((\mathbf{x}^{\infty},0)\) is a stationary point of problem (6) and \((\mathbf{x}^{\infty},\pi^{\infty})\), namely, \((\mathbf{x}^{\infty},\tilde{\pi}^{\infty})\), is a stationary point of problem (1).

Case (ii). The sequence \(\{\eta_{k}\}\) is not bounded below by any positive number, namely, \(\lim_{k\to\infty}\eta_{k}=0\). By the updating rule, we know that \(\eta_{k}\) is updated via (16) infinitely many times. Hence, there must exist \(k_{1}<k_{2}<\cdots\) such that \(\eta_{k_{\ell}}\to 0\) as \(\ell\to\infty\) and

\[\eta_{s}=\eta_{k_{\ell}}=\min\{\gamma_{\eta}\eta_{k_{\ell}-1},\varrho_{k_{\ell }-1}/\|\log\pi^{k_{\ell}-1}\|_{\infty}\}\;\;\mathrm{with}\;\;k_{\ell}\leq s<k_{ \ell+1}\;\;\mathrm{and}\;\;\pi^{k_{\ell}}=\pi^{k_{\ell}-1}.\]

By (7) and the second assertion in (14), we have \(\pi_{ij}^{k_{\ell}}\exp(-\varphi(\mathbf{x}^{k_{\ell}})_{ij}/\eta_{k_{\ell}})\leq 1\), which, together with \(0<\pi_{ij}^{k_{\ell}}\leq 1\) and (16), implies

\[\varphi(\mathbf{x}^{k_{\ell}})_{ij}\geq-\eta_{k_{\ell}}|\!\log\pi_{ij}^{k_{ \ell}}|\geq-\eta_{k_{\ell}}\|\!\log\pi^{k_{\ell}-1}\|_{\infty}\geq-\varrho_{k _{\ell}-1},\quad\forall\;\ell\geq 1.\] (39)

Using the same arguments as in the proof in Case (i), we can show that \(\{\mathbf{x}^{k}\}\) is bounded over \(\{k_{1},k_{2},\ldots\}\) and thus \(\{\mathbf{x}^{k}\}\) has at least a limit point. Without loss of generality, assume \(\lim_{\ell\to\infty}\mathbf{x}^{k_{\ell}}=\mathbf{x}^{\infty}\). By (39) and \(\varrho_{k}\to 0\) in (16), we know that \(\varphi(\mathbf{x}^{\infty})\geq 0\) and thus that \((\mathbf{x}^{\infty},0)\) is feasible to problem (6).

Due to the compactness of \(\tilde{\pi}^{k}\) in (15), there must exist a subset \(\mathcal{K}\subseteq\{k_{1},k_{2},\ldots\}\) such that \(\lim_{k\in\mathcal{K},k\to\infty}\tilde{\pi}_{ij}^{k}=\tilde{\pi}^{\infty}\). Recalling (15), (7), and the second requirement in (14), we have \(\tilde{\pi}_{ij}^{k+1}=\pi_{ij}^{k}\exp(-\varphi(\mathbf{x}^{k})_{ij}/\eta_{k})\) for each \(k\geq 1\). Let \(\mathcal{A}(\mathbf{x}^{\infty})=\{(i,j)\in[n]\times[n]\mid\varphi(\mathbf{x}^{ \infty})_{ij}=0\}\). We claim \(\mathcal{A}(\mathbf{x}^{\infty})\neq\emptyset\). Otherwise, for every \((i,j)\) we have \(\varphi(\mathbf{x}^{\infty})_{ij}>0\) and thus

\[0\leq\lim_{k\in\mathcal{K},k\to\infty}\tilde{\pi}_{ij}^{k+1}\leq\lim_{k\in \mathcal{K},k\to\infty}\exp\left(-\frac{\varphi(\mathbf{x}^{k})_{ij}}{\eta_{k}} \right)=0,\] (40)

where the equality uses \(\lim_{\ell\to\infty}\mathbf{x}^{k}=\mathbf{x}^{\infty}\), \(\mathcal{K}\subseteq\{k_{1},k_{2},\ldots\}\), and \(\lim_{k\to\infty}\eta_{k}\to 0\). This makes a contradiction with \(\|\tilde{\pi}^{k+1}\|_{1}=1\). Moreover, (40) also further implies that \(\langle\tilde{\pi}^{\infty},Z(\mathbf{x}^{\infty})\rangle=0\). Finally, with (36), we further have \(\|\mathsf{Proj}_{\mathrm{T}_{U^{\infty}\mathcal{U}}}(-2V_{\tilde{\pi}^{\infty}}U^{ \infty})\|_{\mathsf{F}}=0\) and \(\tilde{\pi}^{\infty}\in\Pi(r,c)\). Putting the above things together, we know that that \((\mathbf{x}^{\infty},0)\) is a stationary point of (6) and \((\mathbf{x}^{\infty},\tilde{\pi}^{\infty})\) is a stationary point of problem (1). The proof is completed.

Proofs in Section 3

### Proof of Lemma 3.1

For notational simplicity, we define

\[h(\pi,U)=\left\langle C(U)-\eta_{k}\log\pi^{k},\pi\right\rangle-\eta_{k}H(\pi).\] (41)

It is easy to see that

\[\max_{\alpha\in\mathbb{R}^{n},\beta\in\mathbb{R}^{n}}-\mathcal{L}_{\eta_{k}}( \mathbf{x},\pi^{k})\quad\text{is the dual formulation of}\quad\min_{\pi\in\Pi(r,c)}h(\pi,U).\] (42)

Therefore, we can write

\[q(U)=-\min_{\pi\in\Pi(r,c)}h(\pi,U).\] (43)

Since the entropy function is \(1\)-strongly convex with respect to the \(\ell_{1}\)-norm over the probability simplex, the minimization problem in (43) has a unique solution \(\pi_{U}^{*}=\operatorname*{argmin}_{\pi\in\Pi(r,c)}\,h(\pi,U)\). Using [34, Lemma B.1], we thus have

\[\operatorname*{grad}q(U)=\mathsf{Proj}_{T_{U}\mathcal{U}}(-2V_{\pi_{U}^{*}}U).\] (44)

By (5), (7), (12), and recalling \([C(U)]_{ij}=\langle M_{ij},UU^{\mathsf{T}}\rangle\), we have

\[[C(U)]_{ij}+\eta_{k}\log\phi_{\eta_{k}}(\mathbf{x}_{U}^{*},\pi^{k})_{ij}-\eta _{k}\log\pi_{ij}^{k}=-[\alpha_{U}^{*}]_{i}-[\beta_{U}^{*}]_{j}-\eta_{k}\log( \|\zeta_{\eta_{k}}(\mathbf{x}_{U}^{*},\pi^{k})\|_{1}).\] (45)

By the optimality of \(\alpha_{U}^{*}\) and \(\beta_{U}^{*}\), we know that \(\mathbf{e}_{\eta_{k}}^{1}(\mathbf{x}_{U}^{*},\pi^{k})=\mathbf{e}_{\eta_{k}}^{ 2}(\mathbf{x}_{U}^{*},\pi^{k})=0\) and thus \(\phi_{\eta_{k}}(\mathbf{x}_{U}^{*},\pi^{k})\mathbf{1}=r\) and \(\phi_{\eta_{k}}(\mathbf{x}_{U}^{*},\pi^{k})^{\mathsf{T}}\mathbf{1}=c\). With (45), (41), and the definition of \(\mathcal{L}_{\eta_{k}}(\cdot,\cdot)\) in (11), by some calculations, we have

\[h(\phi_{\eta_{k}}(\mathbf{x}_{U}^{*},\pi^{k}),U)=-r^{\mathsf{T}}\alpha_{U}^{* }-c^{\mathsf{T}}\beta_{U}^{*}-\eta_{k}\log(\|\zeta_{\eta_{k}}(\mathbf{x}_{U}^ {*},\pi^{k})\|_{1})=-\mathcal{L}_{\eta_{k}}(\mathbf{x}_{U}^{*},\pi^{k}).\]

By the dual formulation (42), we have \(-\mathcal{L}_{\eta_{k}}(\mathbf{x}_{U}^{*},\pi^{k})\leq h(\pi_{U}^{*},U)\). Hence, we know that \(h(\phi_{\eta_{k}}(\mathbf{x}_{U}^{*},\pi^{k}),U)\leq h(\pi_{U}^{*},U)\), which, together with the optimality and uniqueness of \(\pi_{U}^{*}\), yields \(\phi_{\eta_{k}}(\mathbf{x}_{U}^{*},\pi^{k})=\pi_{U}^{*}\). Finally, using (44), we can complete the proof.

### Proof of Theorem 3.2

We first establish the iteration complexity result of iRBBS, which is based on the following nice properties of Sinkhorn iteration (21). Note that the sufficient decrease property in Lemma B.2 is quite different from that in [29, Lemmas 4.10 & 4.11] since we can establish the non-increasing property of the feasibility violation of \(\pi^{(l)}\) in Lemma B.1.

**Lemma B.1**.: _Let \(\{(\alpha^{\ell},\beta^{\ell})\}\) be the sequence generated by (21) and consider \(\{\pi^{(\ell)}\}\) and \(\{\pi^{(\ell+\frac{1}{2})}\}\) in (22). Then we have \(\|\pi^{(0)}\mathbf{1}-r\|_{1}+\|(\pi^{(0)})^{\mathsf{T}}\mathbf{1}-c\|_{1} \geq\|(\pi^{(\frac{1}{2})})^{\mathsf{T}}\mathbf{1}-c\|_{1}\geq\|\pi^{(1)} \mathbf{1}-r\|_{1}\) and for each \(\ell\geq 1\),_

\[\|\pi^{(\ell)}\mathbf{1}-r\|_{1}\geq\|(\pi^{(\ell+\frac{1}{2})})^{\mathsf{T}} \mathbf{1}-c\|_{1}\geq\|\pi^{(\ell+1)}\mathbf{1}-r\|_{1}.\] (46)

Proof.: By the optimality of \(\alpha^{(\ell+1)}\) in (20) and the first equation in (22), we have \(\pi^{(\ell+\frac{1}{2})}\mathbf{1}-r=0\) for \(\ell\geq 0\). Therefore, we have

\[\|\pi^{(\ell+1)}\mathbf{1}-r\|_{1}=\|\pi^{(\ell+\frac{1}{2})} \mathbf{1}-\pi^{(\ell+1)}\mathbf{1}\|_{1}\leq\|\pi^{(\ell+\frac{1}{2})}-\pi^{ (\ell+1)}\|_{1}\] \[= \sum_{ij}\pi_{ij}^{(\ell+\frac{1}{2})}\left|1-\exp\left(-\frac{ \beta_{j}^{(\ell+1)}-\beta_{j}^{(\ell)}}{\eta_{k}}\right)\right|\] \[= \sum_{ij}\frac{\pi_{ij}^{(\ell+\frac{1}{2})}}{[(\pi^{(\ell+\frac{1 }{2})})^{\mathsf{T}}\mathbf{1}]_{j}}\left|[(\pi^{(\ell+\frac{1}{2})})^{ \mathsf{T}}\mathbf{1}]_{j}-c_{j}\right|=\|(\pi^{(\ell+\frac{1}{2})})^{\mathsf{ T}}\mathbf{1}-c\|_{1},\]

where the first inequality uses the Cauchy-Schwarz inequality, the second equality comes from (7), (22), and the definitions of \(\zeta^{(\ell)}\) and \(\zeta^{(\ell+\frac{1}{2})}\) after (21), and the third equality is due to the second equation in (21) and the first equation in (22). This proves \(\|(\pi^{(\ell+\frac{1}{2})})^{\mathsf{T}}\mathbf{1}-c\|_{1}\geq\|\pi^{(\ell+1)} \mathbf{1}-r\|_{1}\)for \(\ell\geq 0\). On the other hand, by the optimality of \(\beta^{(\ell+1)}\) in (20) and the second equation in (22), we have \((\pi^{(\ell)})^{\mathsf{T}}\mathbf{1}-c=0\) for \(\ell\geq 1\). Using a similar argument, we can prove \(\|\pi^{(\ell)}\mathbf{1}-r\|_{1}\geq\|(\pi^{(\ell+\frac{1}{2})})^{\mathsf{T}} \mathbf{1}-c\|_{1}\) for \(\ell\geq 1\) and \(\|\pi^{(0)}\mathbf{1}-r\|_{1}+\|(\pi^{(0)})^{\mathsf{T}}\mathbf{1}-c\|_{1} \geq\|(\pi^{(\frac{1}{2})})^{\mathsf{T}}\mathbf{1}-c\|_{1}\). The proof is completed. 

**Lemma B.2** (Sufficient decrease of \(\mathcal{L}_{\eta_{k}}(\cdot,\cdot)\) in \((\alpha,\beta)\)).: _Let \(\{(\alpha^{\ell},\beta^{\ell})\}\) be the sequence generated by (21) and consider \(\{\pi^{(\ell)}\}\) in (22). Then, for each \(\ell\geq 0\), we have_

\[\mathcal{L}_{\eta_{k}}(\alpha^{(\ell+1)},\beta^{(\ell+1)},U^{t+1},\pi^{k})\] (47) \[\leq\mathcal{L}_{\eta_{k}}(\alpha^{(\ell)},\beta^{(\ell)},U^{t+1 },\pi^{k})-\frac{\eta_{k}}{2}\big{(}\|\pi^{(\ell)}\mathbf{1}-r\|_{1}^{2}+\|\pi^ {(\ell+1)}\mathbf{1}-r\|_{1}^{2}\big{)}.\]

Proof.: By [29, Lemma 4.11], for \(\ell\geq 0\), we have

\[\mathcal{L}_{\eta_{k}}(\alpha^{(\ell+1)},\beta^{(\ell+1)},U^{t+1},\pi^{k})\leq\mathcal{L}_{\eta_{k}}(\alpha^{(\ell+1)},\beta^{(\ell)},U^{t+1}, \pi^{k})-\frac{\eta_{k}}{2}\|(\pi^{(\ell+\frac{1}{2})})^{\mathsf{T}}\mathbf{1 }-c\|_{1}^{2}.\]

By Lemma B.1, for \(\ell\geq 0\), we further have

\[\mathcal{L}_{\eta_{k}}(\alpha^{(\ell+1)},\beta^{(\ell+1)},U^{t+1 },\pi^{k})\leq\mathcal{L}_{\eta_{k}}(\alpha^{(\ell+1)},\beta^{(\ell)},U^{t+1},\pi^{k})-\frac{\eta_{k}}{2}\|(\pi^{(\ell+1)})\mathbf{1}-r\|_{1}^{2}.\] (48)

On the other hand, we have

\[\mathcal{L}_{\eta_{k}}(\alpha^{(\ell+1)},\beta^{(\ell)},U^{t+1},\pi^{k})-\mathcal{L}_{\eta_{k}}(\alpha^{(\ell)},\beta^{(\ell)},U^{t+1},\pi^{ k})=\langle r,\alpha^{(\ell+1)}-\alpha^{(\ell)}\rangle\] (49) \[= -\eta_{k}\langle r,\log r-\log(\pi^{(\ell)}\mathbf{1})\rangle=- \eta_{k}\mathsf{KL}(r\|\pi^{(\ell)}\mathbf{1})\leq-\frac{\eta_{k}}{2}\|\pi^{( \ell)}\mathbf{1}-r\|_{1}^{2},\]

where the first equality uses the first equation in (21), the second equality comes from the second equation in (22), and the last inequality is due to Pinsker's inequality [33]. Here, for \(p,q\in\Delta^{n}\), the Kullback-Leibler divergence between \(p\) and \(q\) is \(\mathsf{KL}(p\|q)=\sum_{i}p_{i}\log(p_{i}/q_{i})\). Combining (48) and (49), we obtain (47). The proof is completed. 

We need to use the following elementary result.

**Lemma B.3**.: _Let \(\vartheta_{1}\) and \(\vartheta_{2}\) be two given positive constants. Consider two sequences \(\{a_{\ell}\},\{b_{\ell}\}\subseteq\mathbb{R}_{+}\) with \(\ell\geq 0\). If they obey:_

\[a_{\ell}-a_{\ell+1}\geq\vartheta_{1}(b_{\ell}^{2}+b_{\ell+1}^{2}),\] (50a) \[a_{\ell}\leq\vartheta_{2}b_{\ell}\] (50b)

_for all \(\ell\geq 1\), then we have \(b_{1}\leq\vartheta_{2}/\vartheta_{1}\) and_

\[\min_{1\leq i\leq\ell+1}b_{i}\leq\frac{\vartheta_{2}}{\vartheta_{1}}\cdot\frac {1}{\ell+\sqrt{2}-1},\qquad\quad\forall\;\ell\geq 1.\]

Proof.: Applying (50) on \(\ell=1\), it is easy to see that \(\vartheta_{2}b_{1}\geq a_{1}\geq\vartheta_{1}b_{1}^{2}\). Therefore, we have \(b_{1}\leq\vartheta_{2}/\vartheta_{1}\) and \(a_{1}\leq\vartheta_{2}^{2}/\vartheta_{1}\). For \(\ell\geq 1\), by (50a), we have

\[\frac{1}{a_{\ell+1}}-\frac{1}{a_{\ell}}=\frac{a_{\ell}-a_{\ell+1}}{a_{\ell}a_{ \ell+1}}\geq\frac{\vartheta_{1}(b_{\ell}^{2}+b_{\ell+1}^{2})}{\vartheta_{2}^{2} b_{\ell}b_{\ell+1}}\geq\frac{2\vartheta_{1}}{\vartheta_{2}^{2}},\]

which, together with \(a_{1}\leq\vartheta_{2}^{2}/\vartheta_{1}\), implies

\[a_{\ell+1}\leq\frac{\vartheta_{2}^{2}}{\vartheta_{1}}\cdot\frac{1}{2(\ell+1)-1}, \qquad\forall\;\ell\geq 0.\]

Let \(\lfloor\ell/2\rfloor\) be the largest integer smaller than \(\ell/2\). For \(\ell\geq 1\), summing (50a) over \(i=\lfloor\ell/2\rfloor+1,\ldots,\ell\), we have

\[2(\ell-\lfloor\ell/2\rfloor)\min_{1\leq i\leq l+1}b_{i}^{2}\leq\sum_{i=\lfloor \ell/2\rfloor+1}^{\ell}(b_{i}^{2}+b_{i+1}^{2})\leq\frac{a_{\lfloor\ell/2\rfloor+1 }}{\vartheta_{1}}\leq\frac{\vartheta_{2}^{2}}{\vartheta_{1}^{2}}\cdot\frac{1}{2( \lfloor\ell/2\rfloor+1)-1},\]

which immediately implies \(\min_{1\leq i\leq l+1}b_{i}^{2}\leq\frac{\vartheta_{2}^{2}/\vartheta_{2}^{2}}{ \ell(\ell+1)}\). With the fact that \(\ell(\ell+1)\geq(\ell+\sqrt{2}-1)^{2}\) for \(\ell\geq 1\) and \(b_{1}\leq\vartheta_{2}/\vartheta_{1}\), we thus complete the proof.

With the help of the above lemmas, we can provide a new analysis of the iteration complexity of the classical Sinkhorn iteration based on the decreasing properties developed in Lemma B.1 and B.2. It differs from the proof technique in [19], wherein a switching strategy is adopted to establish the complexity.

**Lemma B.4**.: _The total number of Sinkhorn iterations to find a point \(\pi^{(\ell+1)}\) satisfying (23) is at most \(\lceil 2R^{t+1}/(\eta_{k}\theta_{t+1})+2-\sqrt{2}\rceil\), where_

\[R^{t+1}=\|C(U^{t+1})\|_{\mathrm{var}}+\eta_{k}\Psi,\] (51)

_where \(\Psi=\|\log\pi^{k}\|_{\mathrm{var}}+\max\{\|\log r\|_{\mathrm{var}},\|\log c \|_{\mathrm{var}}\}\). Here, \(\lceil a\rceil\) is the smallest nonnegative integer larger than \(a\in\mathbb{R}^{n}\)._

Proof.: Let \((\alpha^{*}_{U^{t+1}},\beta^{*}_{U^{t+1}})\in\mathrm{argmin}_{\alpha\in \mathbb{R}^{n},\beta\in\mathbb{R}^{n}}\,\mathcal{L}_{\eta_{k}}(\alpha,\beta,U^ {t+1},\pi^{k})\). By refining the proof in [19] and [36], we can prove

\[\max\{\|\alpha^{(\ell+1)}\|_{\mathrm{var}},\|\alpha^{*}_{U^{t+1}}\|_{\mathrm{ var}},\|\beta^{(\ell+1)}\|_{\mathrm{var}},\|\beta^{*}_{U^{t+1}}\|_{\mathrm{var }}\}\leq R^{t+1},\quad\forall\;\ell\geq 0.\] (52)

Since \(\nabla_{\alpha}\mathcal{L}_{\eta_{k}}(\alpha^{(\ell+1)},\beta^{(\ell+1)},U^{t+ 1},\pi^{k})=r-\pi^{(\ell+1)}\mathbf{1}\),

\[\nabla_{\beta}\mathcal{L}_{\eta_{k}}(\alpha^{(\ell+1)},\beta^{(\ell+1)},U^{t+ 1},\pi^{k})=c-(\pi^{(\ell+1)})^{\mathsf{T}}\mathbf{1}=0,\]

and \(\mathcal{L}_{\eta_{k}}(\alpha,\beta,U^{t+1},\pi^{k})\) is jointly convex with respect to \(\alpha\) and \(\beta\), we have

\[\mathcal{L}_{\eta_{k}}(\alpha^{(\ell+1)},\beta^{(\ell+1)},U^{t+1},\pi^{k})- \mathcal{L}_{\eta_{k}}(\alpha^{*}_{U^{t+1}},\beta^{*}_{U^{t+1}},U^{t+1},\pi^{k })\leq\langle\pi^{(\ell+1)}\mathbf{1}-r,\alpha^{*}_{U^{t+1}}-\alpha^{(\ell+1)}\rangle.\] (53)

Given \(x\in\mathbb{R}^{n}\), let \(x_{\mathrm{m}}=\|x\|_{\mathrm{var}}/2+x_{\mathrm{min}}\), it holds that \(\|x-x_{\mathrm{m}}\mathbf{1}\|_{\infty}=\|x\|_{\mathrm{var}}/2\). For \(y\in\mathbb{R}^{n}\) with \(\langle y,\mathbf{1}\rangle=0\), we further know that

\[\langle y,x\rangle=\langle y,x-x_{\mathrm{m}}\mathbf{1}\rangle\leq\|y\|_{1}\|x -x_{\mathrm{m}}\mathbf{1}\|_{\infty}=\frac{\|x\|_{\mathrm{var}}}{2}\|y\|_{1}.\]

Applying this assertion with \(y=\pi^{(\ell+1)}\mathbf{1}-r\), \(x=\alpha^{*}_{U^{t+1}}\) or \(x=\alpha^{(\ell+1)}\), we obtain from \(\|\pi^{(\ell+1)}\|_{1}=\|r\|_{1}=1\), (52), and (53) that

\[\mathcal{L}_{\eta_{k}}(\alpha^{(\ell+1)},\beta^{(\ell+1)},U^{t+1},\pi^{k})- \mathcal{L}_{\eta_{k}}(\alpha^{*}_{U^{t+1}},\beta^{*}_{U^{t+1}},U^{t+1},\pi^{k} )\leq R^{t+1}\|\pi^{(\ell+1)}\mathbf{1}-r\|_{1},\quad\forall\;\ell\geq 0.\] (54)

Applying Lemma B.3 with \(a_{\ell}=\mathcal{L}_{\eta_{k}}(\alpha^{(\ell)},\beta^{(\ell)},U^{t+1},\pi^{k} )-\mathcal{L}_{\eta_{k}}(\alpha^{*}_{U^{t+1}},\beta^{*}_{U^{t+1}},U^{t+1},\pi^ {k})\) and \(b_{\ell}=\|\pi^{(\ell)}\mathbf{1}-r\|_{1}\) and using (46), (47), and (54), we have \(\|\pi^{(1)}\mathbf{1}-r\|_{1}\leq 2R^{t+1}/\eta_{k}\) and \(\|\pi^{(\ell+1)}\mathbf{1}-r\|_{1}\leq 2R^{t+1}/(\eta_{k}(\ell+\sqrt{2}-1))\) for all \(\ell\geq 1\). Letting \(2R^{t+1}/(\eta_{k}(\ell+\sqrt{2}-1))\leq\theta_{t+1}\), we can complete the proof of Lemma B.4. 

We now can establish the convergence result of iRBBS, namely, Algorithm 2.

**Theorem B.5**.: _Let \(\{\mathbf{x}^{t}\}\) be the sequence generated by Algorithm 2. If \(\epsilon_{1}=\epsilon_{2}=0\), we have \(\mathrm{e}^{1}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})\to 0\) and \(\mathrm{e}^{2}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})\to 0\). If \(\epsilon_{1}>0\) and \(\epsilon_{2}>0\), then Algorithm 2 stops within at most \(\lceil\Upsilon\max\{\epsilon_{1}^{-2},\epsilon_{2}^{-2}\}\rceil\) iterations for any \(\theta_{t+1}\geq 0\), where \(\Upsilon\) is a constant defined in (64)._

Proof.: Given \(\alpha\in\mathbb{R}^{n}\) and \(\beta\in\mathbb{R}^{n}\), by [29, Lemma 4.8] for any \(\iota\in[0,1]\) and \(U_{1},U_{2}\in\mathcal{U}\),

\[\|\nabla_{U}\mathcal{L}_{\eta_{k}}(\alpha,\beta,U_{2},\pi^{k})- \nabla_{U}\mathcal{L}_{\eta_{k}}(\alpha,\beta,\iota U_{1}+(1-\iota)U_{2},\pi^{k })\|_{\mathsf{F}}\] (55) \[\leq 2\left(\|C\|_{\infty}+2\|C\|_{\infty}^{2}/\eta\right)\iota\|U_{1 }-U_{2}\|_{\mathsf{F}}.\]

From [38, 10], we know that the retraction on the Stiefel manifold has the following nice properties, namely, there exist positive constants \(L_{1}\) and \(L_{2}\) such that

\[\|\mathsf{Retr}_{U}(\xi)-U\|_{\mathsf{F}}\leq L_{1}\|\xi\|_{\mathsf{F}}\quad \text{and}\quad\|\mathsf{Retr}_{U}(\xi)-(U+\xi)\|_{\mathsf{F}}\leq L_{2}\|\xi\|_{ \mathsf{F}}^{2}\] (56)

hold for all \(\xi\in\mathrm{T}_{U}\mathcal{U}\) and \(U\in\mathcal{U}\). With (55) and (56), following the proof of Lemma 3 in [10], for each \(U\in\mathcal{U}\) and \(\xi\in\mathrm{T}_{U}\mathcal{U}\), we have

\[\mathcal{L}_{\eta_{k}}\left(\alpha,\beta,\mathsf{Retr}_{U}(\xi),\pi^{k}\right) \leq\mathcal{L}_{\eta_{k}}(\alpha,\beta,U,\pi^{k})+\left\langle\mathrm{ grad}_{U}\,\mathcal{L}_{\eta_{k}}(\mathbf{x},\pi^{k}),\xi\right\rangle+\frac{L}{2}\|\xi\|_{ \mathsf{F}}^{2},\] (57)where

\[L=2(L_{1}^{2}+L_{2})\|C\|_{\infty}+4L_{1}^{2}\|C\|_{\infty}^{2}/\eta_{k}.\] (58)

Applying (57) with \(\alpha=\alpha^{t}\), \(\beta=\beta^{t}\), \(U=U^{t}\), and \(\xi=-\tau_{t}\operatorname{grad}_{U}\mathcal{L}_{\eta_{k}}(\mathbf{x}^{t},\pi^{ k})=-\tau_{t}\xi^{t}\), with (19a), we have

\[\mathcal{L}_{\eta_{k}}(\alpha^{t},\beta^{t},U^{t+1},\pi^{k})\leq\mathcal{L}_{ \eta_{k}}(\mathbf{x}^{t},\pi^{k})-\tau_{t}\left(1-\frac{L\tau_{t}}{2}\right)\| \xi^{t}\|_{\mathbb{F}}^{2}.\] (59)

In addition, by the fact that \(\alpha^{t+1}=\alpha^{(t+1)}\), \(\beta^{t+1}=\beta^{(t+1)}\), and \(\mathbf{e}_{\eta_{k}}^{2}(\mathbf{x}^{t+1},\pi^{k})=\|\pi^{(t+1)}\mathbf{1}-r\| _{1}\) for some \(\ell\) and \(\alpha^{(0)}=\alpha^{t},\beta^{(0)}=\beta^{t}\), we have from (47) that

\[\mathcal{L}_{\eta_{k}}(\mathbf{x}^{t+1},\pi^{k})\leq\mathcal{L}_{\eta_{k}}( \alpha^{t},\beta^{t},U^{t+1},\pi^{k})-\frac{\eta_{k}}{2}(\mathbf{e}_{\eta_{k} }^{2}(\mathbf{x}^{t+1},\pi^{k}))^{2}.\] (60)

Recalling (24), it follows from (59) and (60) that

\[E_{\rho}(\mathbf{x}^{t+1})\leq E_{\rho}(\mathbf{x}^{t})-\tau_{t}\left(1-\frac {L\tau_{t}}{2}\right)\|\xi^{t}\|_{\mathbb{F}}^{2}-\left(\frac{\eta_{k}}{2}- \rho\right)(\mathbf{e}_{\eta_{k}}^{2}(\mathbf{x}^{t+1},\pi^{k}))^{2}\] (61)

holds with \(\rho\in[0,\eta_{k}/2)\).

Recalling that \(\tau_{t}\) takes the form \(\tau_{t}=\tau_{t}^{(0)}\sigma^{s}\) for some nonnegative integer \(s\), with (26), we know from (61) that if \(s=\big{\lceil}\log\frac{2(1-\delta_{1})}{\tau_{\max}L}/\log\sigma\big{\rceil}\), there must hold that

\[E_{\rho}(\mathbf{x}^{t+1})\leq E_{\rho}(\mathbf{x}^{t})-\delta_{1}\tau_{t}\| \xi^{t}\|_{\mathbb{F}}^{2}-\Big{(}\frac{\eta_{k}}{2}-\rho\Big{)}(\mathbf{e}_{ \eta_{k}}^{2}(\mathbf{x}^{t+1},\pi^{k}))^{2}.\]

Using similar arguments as [54, Lemma 1.1], this also shows that the backtracking line search in Algorithm 2 terminates in at most \(\big{\lceil}\log\frac{2(1-\delta_{1})}{\tau_{\max}L}/\log\sigma\big{\rceil}\) trials and \(E_{t}^{r}\geq E_{\rho}(\mathbf{x}^{t})\). Moreover, since \(\tau_{t}^{(0)}\geq\tau_{\min}\) from (26), we further know that \(\tau_{t}=\tau_{t}^{(0)}\sigma^{s}\geq\underline{\tau}:=\frac{2\sigma(1-\delta_ {1})\tau_{\min}}{\tau_{\max}L}\). Therefore, we know that the nonmonotone linesearch condition (25) is satisfied for some \(\tau_{t}\geq\underline{\tau}\).

Let \(\mathbf{x}^{*}=(\alpha^{*},\beta^{*},U^{*})\) be the minimizer of problem (11). Using a similar argument as [28, Lemma 4.7], by (34) and \(-\log\pi_{ij}^{k}\leq\|\!\log\pi^{k}\|_{\infty}\), we have

\[\mathcal{L}_{\eta_{k}}(\mathbf{x}^{*},\pi^{k})\geq-\|C\|_{\infty}-\eta_{k}\| \!\log\pi^{k}\|_{\infty}.\] (62)

Let \(\omega_{t}=E_{t}^{r}-E_{\rho}(\mathbf{x}^{t})\). By (62) and using the argument in [44, Theorem 5], we know that such \(\omega_{t}\geq 0\) satisfies

\[\sum_{t=0}^{+\infty}\omega_{t}\leq\frac{\gamma}{1-\gamma}(E_{\eta_{k}/2}( \mathbf{x}^{0})+\|C\|_{\infty}+\eta_{k}\|\!\log\pi^{k}\|_{\infty})<+\infty.\]

By (24), the fact that (25) holds with \(\tau_{t}\geq\underline{\tau}\), and \(\|\xi^{t}\|\!\operatorname{\mathsf{e}}=\mathbf{e}_{\eta_{k}}^{1}(\mathbf{x} ^{t},\pi^{k})\), we have

\[\begin{split}&\delta_{1}\underline{\tau}(\mathbf{e}_{\eta_{k}}^{1}( \mathbf{x}^{t},\pi^{k}))^{2}+\Big{(}\frac{\eta_{k}}{2}-\rho\Big{)}\left(\mathbf{e }_{\eta_{k}}^{2}(\mathbf{x}^{t+1},\pi^{k})\right)^{2}\\ &\leq\mathcal{L}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})-\mathcal{L}_{ \eta_{k}}(\mathbf{x}^{t+1},\pi^{k})+\rho((\mathbf{e}_{\eta_{k}}^{2}(\mathbf{x }^{t},\pi^{k}))^{2}-(\mathbf{e}_{\eta_{k}}^{2}(\mathbf{x}^{t+1},\pi^{k}))^{2} )+\omega_{t}.\end{split}\] (63)

Summing (63) over \(t=0,\ldots,T\) and adding the term \((\eta_{k}/2-\rho)(\mathbf{e}_{\eta_{k}}^{2}(\mathbf{x}^{0}))^{2}\) on both sides of the obtained equation, and then combining them with \(\mathcal{L}_{\eta_{k}}(\mathbf{x}^{T+1},\pi^{k})\geq\mathcal{L}_{\eta_{k}}( \mathbf{x}^{*},\pi^{k})\) and (62), we have

\[\begin{split}&\sum_{t=0}^{T}\Big{(}\delta_{1}\underline{\tau}( \mathbf{e}_{\eta_{k}}^{1}(\mathbf{x}^{t},\pi^{k}))^{2}+\Big{(}\frac{\eta_{k}}{2}- \rho\Big{)}\left(\mathbf{e}_{\eta_{k}}^{2}(\mathbf{x}^{t},\pi^{k}))^{2}\right) \\ &\leq\widetilde{\Upsilon}:=\frac{1}{1-\gamma}\left(E_{\eta_{k}/2}( \mathbf{x}^{0})+\|C\|_{\infty}+\eta_{k}\|\!\log\pi^{k}\|_{\infty}\right)\end{split}\]

for any \(T\geq 1\). This further implies

\[\sum\nolimits_{t=0}^{T}(\mathbf{e}_{\eta_{k}}^{1}(\mathbf{x}^{t},\pi^{k}))^{2}+( \mathbf{e}_{\eta_{k}}^{2}(\mathbf{x}^{t},\pi^{k}))^{2}\leq\Upsilon:=\frac{ \widetilde{\Upsilon}}{\min\{\delta_{1}\underline{\tau},\eta_{k}/2-\rho\}}.\] (64)

Hence, we have \(\mathbf{e}_{\eta_{k}}^{1}(\mathbf{x}^{t},\pi^{k})\to 0\) and \(\mathbf{e}_{\eta_{k}}^{2}(\mathbf{x}^{t},\pi^{k})\to 0\) as \(t\to\infty\).

Suppose \(\mathbf{e}^{1}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})\leq\epsilon_{1}\) and \(\mathbf{e}^{2}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})\leq\epsilon_{2}\) are fulfilled for \(t=\overline{T}\) but not fulfilled for all \(t<\overline{T}\). Then there hold that \(\mathbf{e}^{1}_{\eta_{k}}(\mathbf{x}^{T},\pi^{k})\leq\epsilon_{1}\), \(\mathbf{e}^{2}_{\eta_{k}}(\mathbf{x}^{T},\pi^{k})\leq\epsilon_{2}\), and \((\mathbf{e}^{1}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k}))^{2}+(\mathbf{e}^{2}_{\eta_ {k}}(\mathbf{x}^{t},\pi^{k}))^{2}>\min\{\epsilon_{1}^{2},\epsilon_{2}^{2}\}\) for all \(t<\overline{T}\). Setting \(T\) in (64) as \(T\) yields \(\overline{T}\min\{\epsilon_{1}^{2},\epsilon_{2}^{2}\}\leq\Upsilon\), which completes the proof. 

We are now ready to complete the proof of Theorem 3.2.

Proof of Theorem 3.2.: Given \(\epsilon_{1}>0\) and \(\epsilon_{2}>0\), by Theorem B.5, we know that Algorithm 2 will return a point \(\mathbf{x}^{T}\) with \(T\leq\lceil\Upsilon\max\{\epsilon_{1}^{-2},\epsilon_{2}^{-2}\}\rceil\) satisfying satisfying \(\mathbf{e}^{1}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})\leq\epsilon_{1}\) and \(\mathbf{e}^{2}_{\eta_{k}}(\mathbf{x}^{t},\pi^{k})\leq\epsilon_{2}\). Let \(\hat{\pi}^{T}:=\mathsf{Round}(\phi_{\eta_{k}}(\mathbf{x}^{T},\pi^{k}),\Pi(r,c))\), where "Round" is the rounding procedure given in [3, Algorithm 2]. By the second equation in (22) and \(\mathbf{x}^{T}=(\alpha^{T},\beta^{T},U^{T})\) with \(\alpha^{T}=\alpha^{(\ell+1)},\beta^{T}=\beta^{(\ell+1)}\) for some \(\ell\), we know that \(\|\zeta_{\eta_{k}}(\mathbf{x}^{T},\pi^{k})\|_{1}=1\). By \(\|C(U)\|_{\infty}\leq\|C\|_{\infty}\) for all \(U\in\mathcal{U}\), (58), (52), (51), we have \(\|\alpha^{T}\|_{\mathrm{var}}+\|\beta^{T}\|_{\mathrm{var}}+\|C\|_{\infty}\leq 3 \|C\|_{\infty}+2\eta_{k}\Psi\). Therefore, we obtain from (13) that

\[\|\mathsf{Proj}_{\mathrm{T}_{U}\mathcal{U}}(-2V_{\hat{\pi}^{T}}U^ {T})\|_{\mathrm{F}}\leq\epsilon_{1}+2\|C\|_{\infty}\epsilon_{2},\] \[\big{\langle}\hat{\pi}^{T},Z(\mathbf{x}^{T})\big{\rangle}\leq(2 \log n+\|\mathrm{log}\,\pi^{k}\|_{\mathrm{var}})\eta_{k}+(3\|C\|_{\infty}+2 \eta_{k}\Psi)\epsilon_{2}.\]

With the choices of \(\eta_{k}\), \(\epsilon_{1}\) and \(\epsilon_{2}\) in Theorem 3.2, we can immediately know from (13) that such \((\mathbf{x}^{T},-\varphi(\mathbf{x}^{T})_{\min})\) is an \((\epsilon_{1}^{\prime},\epsilon_{2}^{\prime})\)-stationary point of problem (6). With the expression of \(\Upsilon\) in (64), Lemma B.4 and Remark 3.5, we can complete the proof. Note that the \(\mathcal{O}(\cdot)\) in this theorem hides the constants related to \(E_{\eta_{k}/2}(\mathbf{x}^{0})\), \(\Psi\), \(L_{1}\), \(L_{2}\), \(\log n\), and \(\|C\|_{\infty}\). 

## Appendix C Experimental Settings

We implemented all methods in MATLAB 2021b and performed all the experiments on a MacBook Pro with a 2.3GHz 8-core Intel Core i9. We follow the ways in [34] to generate the following real dataset.

**Datasets C.1**.: _We consider six Shakespeare operas. Each script is preprocessed according to the way in [34] and corresponds to a matrix over \(X\in\mathbb{R}^{300\times n_{X}}\). The values \(n_{X}\) of H5, H, JC, MV, O, and RJ are \(1303\), \(1481\), \(910\), \(1008\), \(1148\), and \(1141\), respectively. The weight vector \(r\) or \(c\) is taken as \(\mathbf{1}_{n_{X}}/n_{X}\). We set \(k=2\)._

_Initial points and retraction._ We choose \(\alpha^{-1}=\mathbf{0}\), \(\beta^{-1}=\mathbf{0}\) for iRBBS and \(\alpha^{0}=\mathbf{0},\beta^{0}=\mathbf{0}\) for R(A)BCD. As for \(U^{0}\), we choose \(U^{0}\in\operatorname*{argmax}_{U\in\mathcal{U}}\langle V_{\pi^{0}},UU^{\mathrm{ T}}\rangle\) with \(\pi^{0}\in\Pi(r,c)\) for all methods. Here, \(\pi^{0}\) is formulated by firstly generating a matrix \(\hat{\pi}^{0}\) with each entry randomly drawn from the standard uniform distribution on \((0,1)\) and then rounding \(\hat{\pi}^{0}/\|\hat{\pi}^{0}\|_{1}\) to \(\pi^{0}\) via [3, Algorithm 2]. The retraction operator in all the above methods is taken as the QR-retraction \(\operatorname{Retr}_{U}^{\mathrm{qr}}(\xi)=\operatorname{qf}(X+\xi)\), where \(\operatorname{qf}(X+\xi)\in\mathcal{U}\) satisfying \(X+\xi=\operatorname{qf}(X+\xi)\mathrm{upp}(X+\xi)\) with \(\mathrm{upp}(X+\xi)\) being an upper triangular \(k\)-by-\(k\) matrix with positive diagonal elements.

_Parameters of iRBBS in Section 4.1._ We choose \(\tau^{\min}=10^{-10}/L\), \(\tau^{\max}=10^{10}/L\), \(\gamma=0.85\), \(\tau_{0}=10^{-3}\), \(\sigma=1/2\), \(\delta_{1}=10^{-4}\), and \(\rho=0.49\eta_{k}\).

_Parameters of R(A)BCD in Section 4.1._ As stated in Remark 4.1 in [28], to achieve the best performance of R(A)BCD, one has to spend some efforts in tuning the stepsizes. Here, we adopt the stepsizes used therein (with a slight modification, marked in _italic_ type, to have better performance for some cases). We choose \(\tau_{\texttt{RBCD}}=\textit{0.09}\) if the instance is X/RJ and \(\tau_{\texttt{RBCD}}=0.1\) otherwise; \(\tau_{\texttt{RABCD}}=\textit{0.0015}\) if the instance is X/RJ with X \(\neq\) H, \(\tau_{\texttt{RABCD}}=\textit{0.001}\) if the instance is H/RJ and \(\tau_{\texttt{RABCD}}=0.0025\) otherwise. Moreover, we stop R(A)BCD when \(\mathbf{e}^{1}_{\eta_{k}}(\alpha^{t},\beta^{t-1},U^{t})\leq\epsilon_{1}\) and \(\mathbf{e}^{2}_{\eta_{k}}(\alpha^{t},\beta^{t-1},U^{t})\leq\epsilon_{2}\) or the maximum iteration number reaches 5000.

_Parameters of REALM in Section 4.2._ Once \(\eta_{k}\) becomes \(\eta_{\min}\), we set the corresponding \(\epsilon_{k,1}=\epsilon_{1}\) and \(\epsilon_{k,2}=\epsilon_{2}\) and stop the algorithm if \(\mathbf{e}^{1}_{\eta_{k}}(\mathbf{x}^{k},\pi^{k})\leq\epsilon_{1}\) and \(\mathbf{e}^{2}_{\eta_{k}}(\mathbf{x}^{k},\pi^{k})\leq\epsilon_{2}\). We choose \(\epsilon_{1,2}=10^{-1}\max\{\|r\|_{\infty},\|c\|_{\infty}\}\) and \(\epsilon_{1,1}=2\|C\|_{\infty}\epsilon_{1,2}\).

## Appendix D Additional Numerical Results

In this section, we present additional numerical results to evaluate the performance of our proposed methods to compute the PRW distance (1). We follow the ways in [34; 28; 41] to generate the synthetic and real datasets.

**Datasets D.1** (Synthetic dataset: Fragmented hypercube [41]).: _Define a map \(P(x)=x+2\mathrm{sign}(x)\odot\sum_{k=1}^{k^{*}}e_{k}\), where \(\mathrm{sign}(\cdot)\) is taken elementwise, \(k^{*}\in\{1,\ldots,d\}\) and \(\{e_{1},\ldots,e_{k^{*}}\}\) is the canonical basis of \(\mathbb{R}^{d}\). Let \(\mu=\mathcal{U}([-1,1]^{d})\) be the uniform distribution over an hypercube and \(\nu=P_{\#}(\mu)\) be the pushforward of \(\mu\) under the map \(P\). We set \(k=k^{*}=2\) and take both the weight vectors \(r\) and \(c\) as \(\mathbf{1}/n\)._

**Datasets D.2** (Real datasets: digits from MNIST datasets [34]).: _For each digit \(0,1,\ldots,9\), we extract the 128-dimensional features from a pre-trained convolutional neural network. Each digit corresponds to a matrix \(X\in\mathbb{R}^{128\times n_{X}}\). The values \(n_{X}\) of \(0,1,\ldots,9\) are \(980\), \(1135\), \(1032\), \(1010\), \(982\), \(892\), \(958\), \(1028\), \(974\), and \(1009\), respectively. The weight vector \(r\) or \(c\) is taken as \(\mathbf{1}_{n_{X}}/n_{X}\). We choose \(k=2\)._

### Comparison with R(A)BCD on Solving Subproblem (11)

We follow the same settings of \(\eta\) in [28]. For Dataset D.1, we choose \(\eta=0.2\) when \(d<250\) and \(\eta=0.5\) otherwise. For Dataset D.2, we set \(\eta=8\).

_Parameters of R(A)BCD in this section_. For Dataset D.1, we choose \(\tau_{\texttt{R\&D}}=\tau_{\texttt{R\&D}}=0.001\) as done in [28]. For Dataset D.2, we choose \(\tau_{\texttt{R\&D}}=0.004\), and we do not test RABCD for this dataset since the well-chosen stepsize is not provided in [28].

For Dataset D.1, we randomly generate 10 instances for each \((n,d)\) pair, each equipped with 5 randomly generated initial points. The comparison results for Dataset D.1 are plotted as Figure 2. Note that the values \(\tilde{\mathcal{P}}_{k}^{2}\) returned by different methods are almost the same for this dataset. Therefore, we do not report the values in the figure. From the figure, we can draw the following observations. (i) Among iRBBS methods, iRBBS with smaller \(\theta\) always has less nGrad but more nSk. This means that computing \(\mathrm{grad}\,q(U^{t})\) in a relatively high precision can help to reduce the whole iteration number of updating \(U\). In particular, iRBBS-0, which computes (almost) the exact \(\mathrm{grad}\,q(U^{t})\), takes the least nGrad. Because the complexity of one Sinkhorn iteration is much less than that of updating \(U\) (see Remark 3.5), iRBBS with a moderate \(\theta\) generally achieves the best overall performance. The last rows of Figure 2 show that iRBBS-0.1 is almost the fastest among all iRBBS methods. (ii) Our iRBBS-\(\theta\) is better than R(A)BCD in terms that the former always have smaller nGrad and nSk and are always faster. Particularly, for Dataset D.1, iRBBS-0.1 is always more than 5x faster than RABCD and is about more than 10x faster than RBCD. For the largest instance \(d=250\), \(n=2500\), iRBBS-0.1 (1.6s) is more than 7.2x faster than RABCD (11.1s) and about 28.6x faster than RBCD (44.4s). It

Figure 2: Averaged results for Dataset D.1. The scaled time means the time of each method over the minimum time of all methods. For (a), \(d\in\{20,100,250,500\}\); for (b), \(n\in\{100,250,500,1000\}\); for (c), \(d\in\{20,50,250,500\}\); for (d), \(d\in\{10,20,100,250\}\).

[MISSING_PAGE_EMPTY:23]

while iRBBS-0 takes the most nSk and iRBBS-inf takes the least nSk; Compared with iRBBS-inf, iRBBS-0.1 is about 2.7x faster for Dataset D.2. Compared with iRBBS-0, iRBBS-0.1 is about 1.4x faster for Dataset D.2; (iii) compared with R(A)BCD, iRBBS-0.1 can take significantly less nGrad and may take a bit more nSk. This makes it about 7x faster than RABCD and about 12.8x faster than RBCD for Dataset D.2. Besides, for instances D1/D8, D2/D4, D3/D4, D4/D8, RBCD meets the maximum iteration number.

From the above results, we can conclude that iRBBS generally performs much better than R(A)BCD for the two datasets. More importantly, our methods adopt the adaptive stepsize without needing to tune the best stepsize as done in R(A)BCD.

### Comparison on Computing the PRW Distance (1)

In this subsection, we present more numerical results on Datasets D.1 and D.2 to illustrate the effectiveness and efficiency of our proposed REALM, namely, Algorithm 1. The subproblem (11) is solved by our developed iRBBS algorithm, namely, Algorithm 2, with \(\theta_{t}\) chosen as in Section 4.1.

The results for Dataset D.1 are presented in Table 4. For each \((n,d)\) pair, we randomly generate 10 instances, each equipped with 5 randomly generated initial points. We consider REALM-\((0.055,0.9)\) and REALM-\((0.02,0)\) both with \(\eta_{1}=1\), \(\gamma_{\epsilon}=0.25\), and \(\gamma_{\eta}=0.5\). Note that the latter admits a smaller \(\eta_{\text{min}}\) and does not update the multiplier matrix. From Table 4, we can observe that REALM-\((0.055,0.9)\) can not only return better solutions than REALM-\((0.02,0.9)\) but also is about 4x faster. This shows that updating the multiplier matrix does help. In fact, on average REALM-\((0.055,0.9)\) updates the multiplier matrix 8 times in 14 total iterations.

The results over 20 runs on the real Dataset D.2 are reported in Table 5. We consider REALM-\((1,0)\) and REALM-\((3,0.9)\) both with \(\eta_{1}=200\) and \(\gamma_{\eta}=\gamma_{\epsilon}=0.25\). From Table 5, we can see that updating the multiplier matrix also helps. Compared with REALM-\((1,0)\), REALM-\((3,0.9)\) can not only return better solutions but is about 2.4x faster. On average, REALM-\((3,0.9)\) updates the multiplier matrix 6.7 times in 11.7 total iterations.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{\(\widehat{\mathcal{P}}_{k}^{2}\)} & nGrad & \multicolumn{3}{c}{nSk\({}_{\text{exp}}\)/nSk\({}_{\text{log}}\)} & time & iter \\ \cline{2-11} \(n/d\) & a & b & a & b & a & b & a & b & a & b \\ \hline
100/20 & **8.34297** & 8.34296 & 182 & 138 & 3208/8342 & 13258/0 & 1.1 & 0.1 & 0.07/0. & 8.0/14.0 \\
100/100 & 9.17320 & **9.17340** & 287 & 241 & 3837/3354 & 11369/0 & 0.5 & 0.1 & 0.07/0. & 8.0/14.0 \\
100/250 & 10.89561 & **10.89689** & 467 & 348 & 3200/8858 & 10550/279 & 1.3 & 0.2 & 0.07/0. & 8.0/14.1 \\
100/500 & 13.3111 & **13.31524** & 680 & 469 & 2192/3782 & 9844/213 & 0.8 & 0.2 & 0.07/0. & 8.0/14.0 \\ \hline
100/50 & 8.59993 & **8.60005** & 321 & 189 & 3945/13096 & 11700/0 & 1.8 & 0.1 & 0.07/0. & 8.0/14.0 \\
250/50 & **8.23339** & 8.23336 & 150 & 156 & 2480/4168 & 1082/8 & 2.2 & 0.3 & 0.07/0. & 7.9/13.9 \\
500/50 & **8.12987** & 8.12986 & 122 & 141 & 1944/4508 & 11578/0 & 7.2 & 0.8 & 0.0/0.7 & 8.0/14.0 \\
1000/50 & **8.07096** & 8.07095 & 111 & 130 & 1716/5307 & 12344/0 & 30.8 & 2.3 & 0.07/0. & 8.0/14.0 \\ \hline
20/20 & 9.30977 & **9.31036** & 591 & 221 & 4016/23861 & 11906/0 & 0.5 & 0.0 & 0.07/1. & 8.0/14.0 \\
50/50 & 9.36270 & **9.36292** & 386 & 248 & 4656/9416 & 11819/0 & 0.5 & 0.1 & 0.07/0. & 7.9/13.9 \\
250/250 & 9.17441 & **9.17476** & 282 & 257 & 2942/4136 & 11371/248 & 2.2 & 0.5 & 0.07/0. & 8.0/14.0 \\
500/500 & 9.11498 & **9.11540** & 258 & 262 & 2391/4884 & 9927/873 & 7.5 & 2.2 & 0.07/0. & 7.6/13.6 \\ \hline
100/10 & **8.16197** & 8.16193 & 405 & 173 & 4635/33570 & 65772/0 & 4.6 & 0.3 & 0.07/0. & 8.0/14.0 \\
200/20 & **8.13310** & 8.13309 & 180 & 128 & 2861/10259 & 15007/0 & 3.8 & 0.3 & 0.07/0. & 8.0/14.0 \\
1000/100 & **8.11874** & 8.11873 & 124 & 145 & 2096/5157 & 12153/141 & 30.5 & 3.1 & 0.07/0. & 8.0/14.0 \\
250/250 & **8.11640** & 8.11639 & 117 & 146 & 2416/5499 & 11482/624 & 373.8 & 103.3 & 0.07/0. & 7.9/13.9 \\ \hline AVG & 9.01557 & **9.01602** & 291 & 212 & 3033/9262 & 15057/149 & 29.3 & 7.1 & 0.07/0. & 8.0/14.0 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The average results of REALM for Dataset D.1. In this table, “a” and “b” stand for REALM-\((0.02,0)\) and REALM-\((0.055,0.9)\), respectively.

[MISSING_PAGE_EMPTY:25]