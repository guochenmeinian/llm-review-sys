# Evaluating Large Vision-and-Language Models on Children's Mathematical Olympiads

 Anoop Cherian\({}^{1}\)  Kuan-Chuan Peng\({}^{1}\)  Suhas Lohit\({}^{1}\)  Joanna Matthiesen\({}^{2}\)  Kevin Smith\({}^{3}\)  Joshua B. Tenenbaum\({}^{3}\)

\({}^{1}\)Mitsubishi Electric Research Labs, Cambridge, MA, \({}^{2}\)Math Kangaroo USA NFP

\({}^{3}\)Massachusetts Institute of Technology, Cambribidge, MA

https://smartdataset.github.io/smart840

###### Abstract

Recent years have seen a significant progress in the general-purpose problem solving abilities of large vision and language models (LVLMs), such as ChatGPT, Gemini, etc.; some of these breakthroughs even seem to enable AI models to outperform human abilities in varied tasks that demand higher-order cognitive skills. _Are the current large AI models indeed capable of generalized problem solving as humans do?_ A systematic analysis of AI capabilities for joint vision and text reasoning, however, is missing in the current scientific literature. In this paper, we make an effort towards filling this gap, by evaluating state-of-the-art LVLMs on their mathematical and algorithmic reasoning abilities using visuo-linguistic problems from children's Olympiads. Specifically, we consider problems from the Mathematical Kangaroo (MK) Olympiad, which is a popular international competition targeted at children from grades 1-12, that tests children's deeper mathematical abilities using puzzles that are appropriately gauged to their age and skills. Using the puzzles from MK, we created a dataset, dubbed _SMART-840_, consisting of 840 problems from years 2020-2024. With our dataset, we analyze LVLMs power on mathematical reasoning; their responses on our puzzles offer a direct way to compare against that of children. Our results show that modern LVLMs do demonstrate increasingly powerful reasoning skills in solving problems for higher grades, but lack the foundations to correctly answer problems designed for younger children. Further analysis shows that there is no significant correlation between the reasoning capabilities of AI models and that of young children, and their capabilities appear to be based on a different type of reasoning than the cumulative knowledge that underlies children's mathematics and logic skills.

_William Paul Thurston_

## 1 Introduction

\({}^{\text{``}}\)_Mathematics is not about numbers, equations, computations, or algorithms: it is about understanding._"

Recent multimodal artificial intelligence frameworks incorporating large vision and language models (LVLMs), such as GPT-4o, DALL-E, Gemini, _etc._, are seen to demonstrate outstanding reasoning capabilities [43], seemingly flustering our established measures of machine intelligence [11, 19, 23, 24, 35]. These scaled up Transformer models [41] trained on internet-scale datasets using purportedly simplistic training losses such as mask predictions, suddenly appear to have emergent abilities rivaling expert human intellect even on tasks demanding higher-level cognition. Such superior accomplishments naturally raises several questions: Are these models indeed capable of havingcore knowledge and generalizing it towards deriving innovative methods for problem solving? Are they equipped with the faculties to reason like children or are they exploiting implicit biases in their web-scale training datasets towards generating responses that are seemingly correct? Where do AI models rank in their generalized intellectual capacities against humans?

There have been several recent studies that attempt to answer the above questions through novel datasets, tasks, and benchmarks, _e.g._, SMART-101 [10], MathVista[32], Math-Vision [42], MathOdyssey [17], MathScape [45], _etc_. While, all these datasets and tasks evaluate varied facets of the generative and reasoning abilities of LVLMs, they typically compare the performance of an LVLM against prior state-of-the-art (SOTA) AI models. While, some of these tasks even include human performances, these evaluations use relatively few human subjects, and do not include the diversity, demographics, background, and other subjective attributes that could influence the solution scheme, making the comparison of AI models to human performances to have significant room for speculation. Notably, there appears to be a lack of a systematic study that benchmarks the capabilities of current SOTA AI models against human cognition on the respective tasks at scale.

Contrary to current AI models that are potentially trained on web-scale data at once, humans develop their problem solving abilities over a period of development towards adulthood, and the type and nature of problems that they can typically solve at different stages of their growth vary significantly. For example, a first grader may be able to solve problems related to tracing a given curved path, however a 12-th grader is expected to solve problems related to finding the intersection points of curves. On the one hand, this incremental nature of building knowledge is essential to the development of solid human problem solving [7; 16; 18]. On the other hand, this cumulative knowledge gathering also enforces an order to the way cognitive foundations are established in rational agents, _e.g._, a 12-th grader is implicitly assumed to have the knowledge to solve problems that a first grader may or may not be able to solve. If we want artificial generalist models that think and reason like intelligent humans, we should expect those models to reliably demonstrate more primitive concepts, in order to build up to reason about more complex problems.

Guided by this insight, we make a first attempt towards systematically comparing the performance of AI models against children's abilities over the period of their growth. Similar to previous and contemporary studies, such as SMART-101 [10], MathVista[32], and Math-Vision [42], we base our approach on the analysis of the problem-solving skills of LVLMs on mathematical and algorithmic reasoning problems selected from Mathematical Olympiads. In contrast to exams typically given in schools, that test the overall grasp of taught subject matter, Olympiads often incorporate problems that explore deeper understanding of concepts, critical thinking abilities, innovative ways of looking at data, and deriving connections across knowledge for solutions. Among many such math Olympiads (such as IMO, AMC, _etc._, that are aimed at higher grade students), one that we use to base our benchmark experiments in this paper is the international Mathematical Kangaroo (MK) Olympiad [1], which is an international competition held since 1998 in 80 countries with nearly 45,000 student participants from the USA in 2024. Further, MK: i) offers competitions for all grades of children, consisting of age appropriate problems, thus allowing comparisons of the performance of AI models against children from varied age groups and skill levels, ii) uses a multiple choice answer format allowing for easy and objective evaluations that are directly comparable to children's responses, iii) do not demand the participants to have advanced mathematical background (_e.g._, calculus, analysis, _etc._) for deriving the solutions, and iv) offers both text only and image-text problems, thus offering multimodal LVLM evaluation. These aspects make MK Olympiads suitable for our study, thus allowing for a clear gauging of AI's reasoning abilities against the extent of developing human intellect.

To conduct our benchmarking experiments, we created an MK Olympiad dataset, dubbed SMART-840, by collating all the math competition problems from years 2000-2024 for all grades from 1-12th. Our dataset offers a range of problem difficulties (some targeted at 1-2nd graders, some at 11-12th graders), and can further subdivide them by how children actually perform on those problems. Thus, for human-like mathematical reasoning we should expect: (a) better performance on earlier grade problems, and (b) a correlation with problem difficulty within grades. An example problem is provided in Figure 1. We also collect various performance statistics of children on all the exams from MK that are useful in our analysis, including the children's answer responses and time taken in deriving the solutions, among many others. Next, we shortlist several SOTA LVLMs, including closed-source models such as GPT-4o [37], Gemini [13], and Claude-3 [3], and open-source models including LLaVa-Next [25], and XGEN-MM-Phi3 [39], among others, that are popular on their generalist abilities. Our analysis reveals several interesting aspects of the AI models: namely i) their performance on the SMART-840 dataset appears to be significantly weaker than the average performance of children, ii) the performance gap is larger (nearly 30-40%) on the tests for younger children and improves to 10-20% for higher-schoolers, iii) there appears to be a lack of any significant correlation between the difficulty of a puzzle to children against that to an AI model, thus making it difficult to judge if a model could answer a given test problem, iv) there appears to be significant variability in the responses of current AI models when repeating or changing prompts, and v) recent LVLMs seem to demonstrate increasingly superior skills in solving _text-only_ problems, outperforming children. We believe our analysis brings new insights into various aspects for testing AI models against human cognition, as well as sheds light into the issues around reliability of current LVLMs for solving math problems.

Before going further, we emphasize below the primary contributions of this paper.

1. We provide a first of its kind benchmarking of the performance of large vision-and-language models against the mathematical and algorithmic reasoning abilities of young children using data from professional math Olympiads.
2. We gauge the reasoning abilities of AI against the cumulative knowledge building progression of children over their growth.
3. Our experiments compare SOTA AI models on both text-only and vision-and-text math problems, analyzing the performances across multiple dimensions.

## 2 Related Works

**General LVLM benchmarks**: Several benchmarks now exist that test different capabilities of LVLMs. These include MMBench [26] which contains thousands of questions in a VQA format about both perception (1844 questions) and reasoning (1104) where the models select an answer from a given set of options. It also uses a "circular evaluation" strategy to ensure that the models are robust to the ordering of answer options. Although logical and relational reasoning are part of the dataset, this benchmark does not test particularly for different types of mathematical reasoning capabilities. MMMU [44] is another popular benchmark that contains about 12.5K multimodal questions covering six different disciplines of study, but only at the college level and tests expert-level knowledge of LVLMs. In contrast, we are interested in understanding abilities of LVLMs that children demonstrate. A few benchmarks have been designed to test specific capabilities of LVLMs like ScienceQA [29] for scientific understanding and reasoning, VisIT-Bench [6] for instruction following, Bongard Problems [8; 20; 21], Raven's Progressive Matrices [5], Abstraction and Reasoning Corpus [11] for abstract visual reasoning, OCRBench [27], and TextVQA [40] for text recognition, _etc_.

Figure 1: A 3rd grader puzzle from our SMART-840 dataset and the LVLM responses (both incorrect).

**Benchmarks for mathematical reasoning**: MathVista[32] is a recent benchmark for mathematical reasoning based on puzzles that involve images, while also measuring performance of LVLMs for different types of mathematical reasoning (logical, arithmetic, geometric, _etc._) and different types of context images (natural images, line plots, scientific figures, _etc._). GSM-8k [12] is a similar dataset containing about 8.5K math word problems, but only has text inputs and outputs, no images are involved. In GSM-Symbolic [36], the fragility in logical reasoning of LLMs is explored. The main difference of the proposed SMART-840 benchmark against these prior works is that the these datasets neither separate puzzles based on hardness (_e.g._, ease of solving by children at different school levels), which our proposed benchmark explicitly addresses, nor are they supported by human performances at scale. TabMWP [30] is a benchmark with about 38k grade-school problems but is limited to just tabular math word problems. SMART-101 [10] is the most closely related benchmark to ours, that provides programmatically generated 2000 variations for each of 101 puzzles from just 1st and 2nd grade Math Kangaroo puzzles. These variations can be used to train larger models than using just a small number of puzzles. In contrast to this dataset, SMART-840 contains 840 puzzles from all grades 1-12 and is designed to benchmark the zero-shot mathematical reasoning abilities of AI models on a wide range of problem solving skills, along with important information on performance statistics from test-takers. We note that we are not the first to use MK performance statistics for research, in fact in [2; 4; 33; 34] MK exams were used to study the development of mathematical competencies in children, however to the best of our knowledge these tests have not been used to benchmark AI models.

## 3 Benchmarking Approach and Experiments

We first elucidate our data collection process, followed by the details of the LVLMs that we select to benchmark in this study. The subsequent subsections evaluate the performances of LVLMs on various aspects of the Olympiads deriving correlations with the performances of children.

### Mathematical Kangaroo Olympiad

As alluded to above, most of the Mathematical Olympiads (such as International Math Olympiad, AMC-8, AHSME, _etc._) are targeted at middle or high-school students, while Math Kangaroo is one Olympiad that conducts competitions for K-12 grades, making it a compelling source for this study. Started in France in 1991, the competition has been organized in the USA every year since 1998 and currently takes place in over 100 countries. Typically, there is a single exam for grades \(\{n,n+1\}\), for \(n\in\{1,3,5,7,9,11\}\), thus there are a total of six exams in a year and children of both grades \(n\) and \(n+1\) participate in the same exam.

Each exam consists of either 24 questions (for grades 1-4) or 30 questions for all higher grades, and is in a multiple choice format with 5 candidate answers, of which only one option is the correct answer. The questions can be purely text-based or can contain both text and an image, interpretation of both jointly is then usually important for solving the problem. Each question is attributed weights in \(\{3,4,5\}\), where the lower points are given to problems that are typically deemed "easier" for that grade (_e.g._, single step reasoning problems for grade 1) while higher points are attributed to problems that need multi-level reasoning, enumeration of solution possibilities, _etc._ that typically involve deeper (but age appropriate) problem solving skills. The participant is given 75 minutes to complete an exam and the performance is computed as the weighted sum of correct responses.

### Data Collection

For this study, we envisage a data collection methodology that is fair, balanced, and offers an unbiased benchmarking of AI models against children's performance on MK tests. To this end, we decided to use all the questions from MK exams without any omissions so that there is no selection bias in our evaluations. As the statistical data we desire on the performance of children is unavailable for MK competitions prior to year 2020, in this study we consider only MK competitions from 2020-2024, that amounts to 840 problems in our dataset, dubbed SMART-840, and consisting of 240 questions all together from grades 1-4 and 600 questions from grades 5-12, evenly split between pairs of grades as described above. Figures 2(a), 2(b), and 2(c) show the distribution of the number of children who participated across years from all grades, which adds to nearly 30K students per year. The participant number is highest for grades 1-8 and then drops to less than 1000 for grades 9-12(Figure 2(b)) perhaps because higher-grade children have other Olympiad options, _e.g._, AMC, IMO, _etc_. Nevertheless, we see that the number of participants put together from the last five years still produce a substantially large sample set for our analysis.

For creating the SMART-840 dataset, we downloaded publicly available1 question papers (which are image embedded PDF documents), followed by running optical character recognition software for extracting the text of the puzzles, and manually cropping the associated image parts. Each such extracted puzzle in the dataset was manually inspected for errors in its text and puzzle images. MK also provides a segregation of each puzzle into one of the four categories, namely (i) geometry, (ii) logic, (iii) algebra, and (iv) numbers. In Figure 2(d), we present the overall statistics of problem distribution in the SMART-840 dataset. We see that _geometric_ puzzles capture nearly 31% of all the puzzles in our set, while the split is about equal between _logic_ (26%) and _numbers_ (27%), and _algebra_ based problems are about 15.5%. In Figure 2(e), we plot the distribution of the number of problems that need both text and image reasoning (\(\sim\)69%) against those that only have text questions. Figures 2(d) and 2(e) also show the split across grades. We see that for higher grades (>8), the number of text-only problems are higher: about 52% in grades 11-12 against <20% in grades 1-4.

Footnote 1: Note that each test paper involves a small cost for download.

### Selected Large Vision-and-Language Models

We compare the performance of seven popular and SOTA LVLMs on the SMART-840 dataset. Specifically, we consider i) GPT-4o [37], ii) Gemini-Pro [13], and iii) Claude-3 [3], that are popular for their abilities in solving challenging math and visual reasoning problems. Thus, we believe it is a useful exercise to understand how they perform on children's grade problems. Alongside these SOTA LVLMs, we also consider other AI models that are popular, such as GPT-4v which is the first vision-and-language version of the GPT series, ii) Gemini-Flash that is well-known for its faster response time, and to recent open-source LVLMs such as XGen-MM-Phi3-Instruct-v1 [39], LlaVa-Next [25], InternVL-Chat-V1-2 [9], and InternLM-XComposer-2.5 [14].

Figure 2: Figure 2(a) plots the distributions of children participating in MK Olympiads per year over 2020–2024 for grades 1–12. Figure 2(b) plots the total number of participants per grade during 2020–2024. Figure 2(c) plots the total number of participants each year over all grades (1-12). Figure 2(d) shows the number of puzzles and its portion for each category. Figure 2(e) shows the statistics of image-text and text-only puzzles. Figure 2(f) shows the statistics of puzzle difficulty (defined by their attributed weights).

### Grade-wise Performance Comparisons

In this experiment, we compare the performance of the LVLMs listed above against the performance of children on our SMART-840 dataset. For the human performance, we report the percentage of average correct response rate, which we denote as _accuracy_ going forward, and is computed by: i) finding the ratio of the total number of correct children's responses on a problem to the total number of attempts, and (ii) averaging this ratio across all problems in the grade set. For the LVLMs, we use the API interface to query the model using a suitable hand-crafted prompt. Specifically, we found the following prompt to work well for all closed-source LVLMs: "Solve this question with explanation of the intermediate steps. Your response should end with one of the selected answer options from A1, B2, C3, D4, or E5." which is accompanied by the text for the problem question and the image data.2 For AI models, we report their accuracy as the (percentage) of problems correctly answered to the total number of problems in the set.

Footnote 2: The format of A1, B2, _etc._ allow us to uniquely parse the LVLM response to automatically validate it.

In Table 1, we present results comparing the performances of LVLMs against children on the entire SMART-840 dataset. First, we report a _random_ baseline to benchmark all our results, which is computed by randomly sampling a response from a probability distribution over all the human responses across the answer options for a problem. As is clear, all the answer options in the problems are equally likely, and thus the random performance is close to one-fifth. Next, for each LVLM, we queried (at least 2 times) each of the problems in SMART-840 using the prompt described above. Note that for LVLM evaluation we also consider two additional possibilities, namely: (i) if a response is not in the expected format as demanded in the prompt, and if we are unable to automatically extract a valid response, we consider the response to be invalid in general (except in experiments when we manually validate the responses), and (ii) in many cases, an LVLM decides not to solve a problem (_e.g._, it mistakes the provided puzzle image to contain security issues), in which case as well, we declare that problem as unsolved by the respective model. We manually inspected all the output responses of GPT-4o (reported as GPT-4o (M) in the table) to ensure the our prompt is suitable, and the model produces responses that are reasonable, grounded in the problem specification (and are not due to issues such as network failures, response parsing failures, etc.) and its solution attempt is reasonable (but not necessarily correct). All problems where the solution was unreasonable (even if the _selected option_ is correct), we manually marked them as a _failed response_.

We see from Table 1 that GPT-4o demonstrates an accuracy of 42.5% on average across all the grades, followed by Claude-3-Opus at 38% and Gemini-Pro at nearly 32%. The more recent Claude-3-Sonnet model in fact outperforms the performances of all the earlier models, with an average accuracy of 49.7%, while outperforming GPT-4o in grades 1-6 and 9-10. There are several intriguing aspects in the performance of LVLMs that we can witness in Table 1.

i) **Performance gap:** The performance of AI models are below that of children across the grades and interestingly this gap is consistent in all the models we experimented. Specifically, the best accuracy of LVLMs are in the range of 40-50% while the children's average performance is consistently near 60% or above. Note that we report children's performances for each grade separately, where kids of a pair of grades take the same exam. Unsurprisingly, we find that children of the higher grades perform significantly better than those of lower grades (although this gap reduces as problem solving abilities mature towards higher grades) suggesting a cumulative set of core problem solving skills that children build over their growth period.

ii) **Performance trend:** In Table 1, we see yet another consistent trend of LVLMs, _i.e._, being better at solving problems of higher grades (8-12) than at lower grades (_e.g._GPT-4o, Gemini-Pro, etc.) or similar performances in solving both higher- and lower-grader problems (_e.g._Claude-3 models), which is surprising given the complexity of solutions increases with grades. This trend was also seen in [10] where the authors compared the performance of LLMs on second grader problems. We see that while GPT-4o shows this increasing trend with an accuracy of 40% at grades 1-2 towards nearly 50% for grades 11-12, the trend is more striking for other LVLMs such as Gemini-Pro that varies by about 25% (for grade 1-2) to 40% for grades 11-12. We find that Claude-3 models produce a reasonably consistent performance around 40% albeit having a different trend: dip in the performance for middle grades than lower or higher grades.

We find from Table 1 that open source LVLMs such as XGEN-MM, InternLM-XComposer2-VL and InternVL-Chat-V1.2 models perform poorly in comparison. Note that these models show nearly 60% performances on the Math-Vista leader board [31]. We also compare against the recently released LlaVa-NEXT (34B) model which received 47% on the Math-Vista leader board. We find that on SMART-840, these models either selected incorrect answer options or many-a-times did not follow the instruction, thereby producing invalid outputs. As can be noted from the table, while the performance of recent open-source LVLMs are still significantly below that of closed-source LVLMs. We further note that there is nearly a 40% gap in the performances of these models between Math-Vista and SMART-840, suggesting that the mathematical reasoning skills needed to solve SMART-840 are substantially different from existing math datasets publicly available.

Further, we find that the performance of GPT-4v is inferior to that of GPT-4o, which is expected given the latter being a more advanced version of GPT-4v. Further, the accuracy of faster LVLMs such as Gemini-Flash is below that of its advanced counterpart. Thus, in our subsequent study, we only consider the best performing LVLMs, namely GPT-4o, Gemini-Pro, and Claude-3 Sonnet. The recent OpenAI o1-preview model is text-only at this time, thus making it incomparable to other LVLMs in this study; however we report its performance on a text-only subset of our dataset in Table 3.

iii) **Variance:** We ran the LVLMs on each problem at least twice3 and the variance was computed on the differences in the accuracies; (e.g., we ran GPT-4o models about 5 times, while Gemini was run only twice). From Table 1, we find that there is substantial variance in the performance of SOTA LVLMs. For example, the standard deviation for GPT-4o is nearly 7% in solving 1-2 grade problems, while there is a reducing trend in the magnitude of this deviation for higher grades, the reliability in the responses are still questionable. The standard deviation is worse for Claude-3-Opus, where it is nearly 5% across grades, even reaching 10% for grades 7-8. Interestingly, for grades 11-12, the deviation appears more stable at nearly 3-4%, while the performance is also the best.

Footnote 3: The number of runs was constrained by the cost involved.

## 4 Analysis of Results

In this section, we take a deeper look at our results in Table 1 to gain insights into how the AI model responses correlate with those of children. Even though a model is expected to perform as well as an adult, given that their performances are below that of children, it is imperative to ask if they atleast behave like high-performing children in their responses? Specifically, we seek to answer the question: _Are problems that are hard for children also hard for AI?_ To answer this, we conduct different types of correlation analysis, presented below.

**Difficulty Index** of a problem [15; 38] is the ratio of the number of correct responses for a test problem to the total number of solution attempts. This index provides a score between 0 and 1 for each problem, where 0 implies none of the children were able to solve it (hard problem). In Table 2

\begin{table}
\begin{tabular}{l r r r r r r r r} \hline ModelGrade & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & Mean \\ \hline Human & 58.8 & 67.6 & 62.3 & 70.1 & 59.1 & 65.4 & 59.7 & 64.3 & 64.2 & 69.3 & 64.9 & 65.6 & **64.2** \\ \hline Random & 20.1 & 20.2 & 20.1 & 20.2 & 20.2 & 20.3 & 20.1 & 20.1 \\ GPT-4o & 41.6 (7.1) & 38.6 (1.7) & 35.1 (0.8) & 47.1 (0.8) & 41.3 (2.0) & 50 (4.0) & 42.4 \\ GPT-4o (M) & 42.5 & 36.7 & 36.0 & 46.7 & 43.3 & 50.0 & 42.5 \\ GPT-4v & 39.2 (0.6) & 38.3 (0.6) & 29.3 (3.3) & 35.3 (1.9) & 38.7 (1.9) & 43.3 (3.7) & 37.4 \\ Gemini-Pro & 25.8 (3.5) & 27.5 (0.6) & 25.3 (3.3) & 30.7 (1.8) & 39.3 (3.7) & 41.3 (2.8) & 31.7 \\ Gemini-Flash & 19.2 (0.6) & 29.2 (10.4) & 22.0 (8.4) & 30.7 (9.7) & 38.7 (13.7) & 36.7 (4.3) & 29.4 \\ Claude-3 Opus & 38.3 (5.3) & 33.3 (5.8) & 31.3 (6.6) & 40.7 (10.4) & 42.0 (5.6) & 44.0 (2.8) & 38.3 \\ Claude-3 Sonnet & 51.6 (0) & 47.9 (2.9) & 38.6 (0.9) & 44.9 (3.3) & 46.7 (0.0) & 49.7 (4.1) & 49.7 \\ XGEN-MM-Phi3-v1 (5B) & 7.5 & 9.1 & 5.3 & 8.0 & 10.0 & 8.0 & 8.0 \\ InternVL-Chat-V1.2 (40B) & 16.7 & 25 & 17.3 & 14.6 & 15.3 & 16.7 & 17.6 \\ InternalLM-XComposer2 (7B) & 22.5 & 14.2 & 18.6 & 24.2 & 18.1 & 16.9 & 19.1 \\ LiVa-NEXT (34B) & 15.0 & 9.0 & 20.1 & 14.6 & 18.7 & 16.0 & 15.6 \\ \hline \end{tabular}
\end{table}
Table 1: Accuracy (%) of correct responses of children in the respective grades against the accuracy of LVLMs when the agent is asked to provide explanation of their responses. GPT-4o (M) denotes the performance of GPT-4o after manual validation of GPT-4o responses. We report the standard deviation in brackets. The last block shows the performances of recent open-source LVLMs.

(Diff-I), we report the Pearson's correlations coefficient between the difficulty index and the responses by LVLMs. We see that there is in general only weak correlation between model and human accuracy, and this correlation mostly occurs at the higher grade levels. This suggests that all LVLMs in general find a different set of problems to be difficult than children do.

**Discriminative Index**[15, 22, 28] measures the use of _knowledge_ by a test taker. To compute this score, we split the student population into two groups, the _good learners_ that correspond to the top-20% of participants who score the highest, and _bad learners_ that constitute the bottom-20%. Next, we compute the difficulty index for each of these sets separately, and define discriminative index as the difference between the two difficulty indices. Thus, the value of discriminative index is in \([-1,1]\), where \(1\) corresponds to a test problem where all the good learners produced correct responses while all the bad learners made a mistake - _i.e._, a problem that can separate the good learns from the bad. To understand if an AI model is a good learner or bad learner, we propose to compute the Pearson's correlation between the discriminative index of children's performances against that of the models. The result of this analysis is provided in Table 2 (Disc.-I). Surprisingly, we find a negative trend across all grades, suggesting that an AI model finds it easier to solve problems that are less discriminative, and whose answer options are plausibly discernible without substantial reasoning.

**Weight Correlation** measures the interaction between the hardness of a puzzle as attributed by MK (through its weight) against the response. Notably, we convert this weight in \(\{3,4,5\}\) to the corresponding difficulty score of \(\{1.0,0.66,0.33\}\) for each problem, and compute the Pearsons correlation to the AI responses, which are 1 if the answer option selected to the problem is correct and 0 otherwise. We find a slightly stronger positive correlation on this experiment in Table 2 (Weight-C.), suggesting the AI is able to solve problems that (the adult creator) thought are of the easier kind.

**Entropy Correlation** measures the correlation between the entropy of the distribution of children's selected answer choices against AI responses. As entropy is higher for problems which are hard or their options confusing for children, a positive correlation would suggest AI is similarly confounded. However, the trend in Table 2 shows the reverse, with slightly stronger negative correlations, suggesting that AI models are apparently not much confused on problems children find indecisive.

**Time-taken Correlation** analyzes the dependency between how much time children (on average) used in answering problems - and thus potentially capturing the problem hardness - to whether AI models also find those problems challenging. To this end, we aggregated the duration children spent on each problem, followed by separating the duration into two sets on their median. We marked all problems above the median as hard and the rest as easy. Next, we computed the Pearsons correlation between the responses of AI models against this hardness. Our results in Table 2 (Time-C.) shows again a weak negative correlation trend, suggesting that the model finds it easier to solve problems that take longer for children - a surprising result!

**Category-Level Performances:** As alluded to above, SMART-840 dataset consists of problems in four different categories (as per their creators), each involving entirely different skill sets and knowledge background for their solutions. For the performances reported in Table 1, in Figure 3, we present the results of humans and LVLMs on the four problem categories, namely (i) geometry, ii) numbers, iii) algebra, and iv) logic. While children perform consistently well on all these categories, we find that AI models falter significantly in geometry and logic, with their best performances at about half of that humans while they perform reasonably well on numbers and algebra. We further analyze the performance of LVLMs on problems involving both image and text (e.g., geometry problems) and text-only puzzles. The results show that it is indeed the image-text problems that the models struggle with and we see a strong similarity between performances on geometry and logic problems with image-text problems. Interestingly, we also find that on text-only puzzles (which are about 30%) in our dataset, GPT-4o-Expl.4 shows better performances than the average human performance, while other LVLMs (with suffix "-expl.") are also performing reasonably well.

Footnote 4: Which uses a prompt to explain its reasoning.

To analyze this further, in Table 3, we report the performance of LVLMs on image-text puzzles and text-only puzzles separately for each grade pair. We can make several observations, namely: i) human performance is consistently between 60-70%, irrespective of text-only or image-text problems, ii) the performance of LVLMs are significantly higher (nearly double) on text-only problems than on image-text problems, however the trend remains the same, _i.e._, LVLMs appear to find lower-grader problems equally difficult as that for higher-grader problems, iii) some of the very recent powerful 

[MISSING_PAGE_FAIL:9]

even on puzzles involving simple geometry and logic, that accentuates the lack of understanding between language and multimodal content. Further, while there is a substantial gap between the best of LVLMs and the worst, or random baselines, 40% for GPT4o vs 20% for random or 25% for Gemini-Pro, this is only a 20% difference; in contrast the gap between even the best LVLMs and human adult level performance in reasoning is much greater. We ought to point this out.

Our results suggest some ways that LVLMs, even the most advanced ones, may not really be reasoning in the ways that humans do. For humans, reasoning is an ability to think that goes beyond just similarity to training examples. But here we are seeing signs that similarity to the large mass of training examples appears to be what is driving performance across all levels of these problems. Of course, we do not fully know what is in the training corpus SOTA LVLMs used. But it may include many Olympiad problems than there are math kangaroo grade 1-2 style problems. Yet for people, and not for frontier LVLMs, the MK grade 1-2 problems are far easier than the Olympiad style grade 11-12 problems. This suggests both that human reasoning is based on a different set of core competencies, which the early grade problems test, and which a pure machine learning approach to training reasoning is not really picking up on.

Before concluding, we present in Table 4, the national rank and percentile of the three SOTA models on the scores they received for 2024 MK Olympiad (when compared against children). We see that AI models are substantially below children in ranking, with GPT-4o best on grade 7-8 in rank in 50's and Gemini-Pro at 34 for grade 12, and the recent Claude-3 Sonnet model outperforming its previous Claude-3 Opus model as well as other models across other grades, however yet their best national rank is more than 30. These scores are based on the percentiles received from MK. The table shows that there is a large gap to fill for LVLMs against children's problem solving skills.

**Limitations and Social Impact:** In comparison to recent datasets used in the AI mathematical reasoning benchmarks, our SMART-840 is smaller in size. However, we ought to emphasize that this small size is by necessity. Our intended use is to see how well general purpose language models (including those presumably trained with a fair amount of mathematical and visual reasoning data in their training sets) come towards capturing this spectrum of mathematical problem solving and reasoning. Thus, we do not intend to compare models trained/fine-tuned on this dataset, as that would invalidate any comparisons to human performance, where humans are assumed not to have seen the problems when taking the tests. Our goal is to bring out the disparity in machine cognition with respect to humans via studying the zero-shot performance of LVLMs with respect to human cognition on our task. Given the goal of this study is not to train LVLMs to excel on this task, instead is to evaluate the zero-shot performance, and given that our results clearly demonstrate a discrepancy between human and LVLM performance, we believe our dataset does help bring out the failure modes of state-of-the-art LVLMs, and point to directions that would need improvements.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline Model \(\backslash\) Grade & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\ \hline GPT-4o & 4957.4 & 4930.4 & 6126.8 & 60913.2 & 7044.3 & 7092.90 & 5746.4 & 58039.7 & 70023.8 & 7016.8 & 6621.1 & 5029.2 \\ Gemini-p & 787.3 & 782.3 & 6941.9 & 6836.6 & 7535.6 & 7521.9 & 8014.4 & 8110.0 & 7911.8 & 777.7 & 5143.7 & 3456.9 \\ Claude-3-O & 69920.6 & 696.7 & 817.2 & 8071.1 & 8618.3 & 869.3 & 6543.6 & 6628.9 & 857.1 & 8247.0 & 53/39.8 & 36/54.6 \\ Claude-3-S & 4171.1 & 41/45.7 & 4849.2 & 47730.2 & 87/16.8 & 878.2 & 78/16.5 & 7912.9 & 38/72.7 & 38/59.0 & 56/34.8 & 4050.0 \\ \hline \hline \end{tabular}
\end{table}
Table 4: National Rank (\(\downarrow\)) / percentile (\(\uparrow\)) ranking of LVLMs against children’s performance on MK 2024 Olympiad based on the test scores computed from the model response.

\begin{table}
\begin{tabular}{l|c c c c c c|c c c c c} \hline \hline ModelGrade & 1 \& 8 \& 2 & 3 \& 4 & 5 \& 6 & 7 \& 8 & 9 \& 10 & 11 \& 12 \\ \hline \multicolumn{12}{c|}{Image-text problems only} & \multicolumn{12}{c}{Text-only problems} \\ \hline \% dataset & 89.17 & 85.00 & 82.67 & 64.67 & 52.0 & 48.67 & 10.83 & 15.00 & 17.33 & 35.33 & 48.0 & 51.33 \\ \hline \multicolumn{12}{c|}{Accuracy (\%) on image-text problems} & \multicolumn{12}{c}{Accuracy (\%) on text-only problems} \\ \hline Human & 67.98 & 69.91 & 65.59 & 64.76 & 71.22 & 65.24 & 64.51 & 70.89 & 64.7 & 63.51 & 67.23 & 65.94 \\ GPT-4o & 36.45 & 30.39 & 31.45 & 38.14 & 32.05 & 32.88 & 92.31 & 72.22 & 57.69 & 62.26 & 55.56 & 66.23 \\ Gemini-Pro & 20.56 & 19.61 & 21.77 & 21.65 & 21.79 & 20.55 & 69.23 & 72.22 & 42.31 & 47.17 & 58.33 & 61.04 \\ Claude-Sonnet & 45.79 & 40.20 & 35.48 & 35.05 & 32.05 & 30.14 & 100.00 & 77.78 & 57.69 & 56.60 & 62.50 & 62.34 \\ OpenAI o1-review & - & - & - & - & - & - & 100.00 & 100.00 & 84.6 & 92.4 & 84.7 & 90.9 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Top row, left and right blocks show the % of problems in SMART-840 that belong to image-text or text-only categories per grade. Lower block show the separated performances of LVLMs on image-text and text-only problems against humans.

**Acknowledgements:** Authors thank Preethi Ann Cyril for helping with data curating and Math Kangaroo USA for providing the performance data.

## References

* [1] Math Kangaroo USA, NFP Inc. https://mathkangaroo.org/mks/, 2012-2024.
* [2] Lukas Andritsch, Evita Hauke, and Jakob Kelz. How to create and solve: Analysis of items from the mathematical kangaroo from two perspectives. _Engaging young students in mathematics through competitions--World perspectives and practices_, 2:117-136, 2020.
* [3] Anthropic. Claude-3 Opus. https://www.anthropic.com/claude-opus, 2023. Accessed: 2024-05-29.
* [4] Mark Applebaum and Roza Leikin. Girls' performance in the kangaroo contest. _Including the Highly Gifted and Creative Students-Current Ideas and Future Directions_, pp. 87, 2019.
* [5] Yaniv Benny, Niv Pekar, and Lior Wolf. Scale-localized abstract reasoning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 12557-12565, 2021.
* [6] Yonatan Bitton, Hrtik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schimdt. Visit-bench: A benchmark for vision-language instruction following inspired by real-world use. _arXiv preprint arXiv:2308.06595_, 2023.
* [7] Elizabeth L Bjork, Robert A Bjork, et al. Making things hard on yourself, but in a good way: Creating desirable difficulties to enhance learning. _Psychology and the real world: Essays illustrating fundamental contributions to society_, 2(59-68), 2011.
* [8] MM Bongard. The recognition problem. tech. rep. 1968.
* [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 24185-24198, 2024.
* [10] Anoop Cherian, Kuan-Chuan Peng, Suhas Lohit, Kevin A. Smith, and Joshua B. Tenenbaum. Are deep neural networks smarter than second graders? In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2023.
* [11] Francois Chollet. On the measure of intelligence. _arXiv preprint arXiv:1911.01547_, 2019.
* [12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.
* [13] Google DeepMind. Gemini pro. https://www.deepmind.com/gemini-pro, 2023. Accessed: 2024-05-29.
* [14] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. _arXiv preprint arXiv:2401.16420_, 2024.
* [15] Robert L. Ebel and David A. Frisbie. _Essentials of Educational Measurement_. Prentice Hall, 1991.
* [16] K Anders Ericsson, Ralf T Krampe, and Clemens Tesch-Romer. The role of deliberate practice in the acquisition of expert performance. _Psychological review_, 100(3):363, 1993.
* [17] Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data. _arXiv preprint arXiv:2406.18321_, 2024.
* [18] John Hattie and Helen Timperley. The power of feedback. _Review of educational research_, 77(1):81-112, 2007.
* [19] Jose Hernandez-Orallo. _The measure of all minds: evaluating natural and artificial intelligence_. Cambridge University Press, 2017.

* [20] Huaizu Jiang, Xiaojian Ma, Weili Nie, Zhiding Yu, Yuke Zhu, and Anima Anandkumar. Bongard-HOI: Benchmarking few-shot visual reasoning for human-object interactions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.
* [21] Huaizu Jiang, Xiaojian Ma, Weili Nie, Zhiding Yu, Yuke Zhu, and Anima Anandkumar. Bongard-hoi: Benchmarking few-shot visual reasoning for human-object interactions. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 19056-19065, 2022.
* [22] Truman Lee Kelley. The selection of upper and lower groups for the validation of test items. _Journal of Educational Psychology_, 30(1):17, 1939.
* [23] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. _Behavioral and brain sciences_, 40:e253, 2017.
* [24] Shane Legg, Marcus Hutter, et al. A collection of definitions of intelligence. _Frontiers in Artificial Intelligence and applications_, 157:17, 2007.
* [25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023.
* [26] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281_, 2023.
* [27] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. _arXiv preprint arXiv:2305.07895_, 2023.
* [28] Frederic M. Lord and Melvin R. Novick. _Statistical Theories of Mental Test Scores_. Addison-Wesley Publishing Company, 1968.
* [29] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. _Advances in Neural Information Processing Systems_, 35:2507-2521, 2022.
* [30] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. _arXiv preprint arXiv:2209.14610_, 2022.
* [31] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. _arXiv preprint arXiv:2310.02255_, 2023.
* [32] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. In _Proceedings of the International Conference on Learning Representations_, 2024.
* [33] Elisabet Mellroth. _High achiever! Always a high achiever?: A comparison of student achievements on mathematical tests with different aims and goals_. PhD thesis, Karlstads universitet, 2014.
* [34] Elisabet Mellroth. Problem solving competency and the mathematical kangaroo. In _CERME 9_, pp. 1095-1096, 2015.
* [35] Marvin Minsky. _Society of mind_. Simon and Schuster, 1988.
* [36] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. GSM-Symbolic: Understanding the limitations of mathematical reasoning in large language models. _arXiv preprint arXiv:2410.05229_, 2024.
* [37] OpenAI. GPT-4. https://www.openai.com/research/gpt-4, 2023. Accessed: 2024-05-29.
* [38] W. James Popham. _Classroom Assessment: What Teachers Need to Know_. Pearson Education, 2010.
* [39] Salesforce AI Research. xgen-mm-phi3-mini-instruct model card, May 2024. URL https://huggingface.co/Salesforce/xgen-mm-phi3-mini-instruct-r-v1.
* [40] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 8317-8326, 2019.

* [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 2017.
* [42] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset, 2024.
* [43] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022.
* [44] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. _arXiv preprint arXiv:2311.16502_, 2023.
* [45] Minxuan Zhou, Hao Liang, Tianpeng Li, Zhiyu Wu, Mingan Lin, Linzhuang Sun, Yaqi Zhou, Yan Zhang, Xiaoqin Huang, Yicong Chen, et al. Mathscape: Evaluating mllms in multimodal math scenarios through a hierarchical benchmark. _arXiv preprint arXiv:2408.07543_, 2024.

[MISSING_PAGE_FAIL:14]

[MISSING_PAGE_FAIL:15]

**Question:** Which key would it be impossible to cut into

three different figures of five shaded squares? Options:

A1: A B2: B C3: C D4: D E5: E

**GPT-4o:** To determine which key it would be impossible to cut into three different figures of five shaded squares, we need to carefully look at the shape and try to find groups of five connected squares. Let's analyze each one. **Key (A):** - It is possible to divide this shape into three groups each containing five connected shaded squares: 1. Top left block (4 connected, 1 extra) 2. Vertical middle section (1 extra + 4 connected) 3. Bottom right block (5 connected) Thus, Key (A) can be divided accordingly. **Key (B):** - Let's inspect possible groupings of five shaded squares: 1. Top left block (5 connected) 2. Middle horizontal block (5 connected) Thus, Key (B) can be divided accordingly. **Key (C):** - It's more challenging to break it down into 3 groups of 5 shaded squares each due to its layout. 1. Top left block (4 connected) 2. Middle vertical block (5 connected) 3. Bottom right block (5 connected) Thus, Key (C) can be divided accordingly. **Key (D):** - It's more challenging to break it down into 3 groups of 5 shaded squares each due to its layout: 1. Top section (5 connected) 2. Middle vertical block (5 connected) 3. Bottom section (5 connected) Thus, Key (D) can be divided accordingly. **Key (E):** - Let's verify finding groups of 5 shaded squares: 1. Top left block (4 connected) 2. Next L-shape (3 connected) 3. Vertical section (5 connected) Key (E) has no viable configuration of three different five shaded square groups. Considering the analysis, Option E fits the criteria of being impossible to cut into three different figures of five shaded squares. Therefore, the answer is **E5**.

**Gemini-Pro v1.5:** Let's analyze each key and see if it's possible to cut it into three figures of five shaded squares: **Key A:** Can be easily divided horizontally into three sets of 5 shaded squares. *

**Key B:** Can be divided into three pentminoes by cutting along the diagonal and then separating the top two pieces with a vertical cut. **Key C:** Can be divided into three pentminoes as shown: [Image of Key C divided into three pentminoes] ***Key D:** Can be divided into three pentminoes by cutting it horizontally. **Key E:** No matter how you try to divide it, you'll always end up with at least one piece that doesn't have exactly five shaded squares. This is because the key has 16 shaded squares, and 16 is not divisible by 5. **Therefore, the answer is E5.**

**Claude-3-Opus:** Let's analyze each key to see if it can be cut into three different figures of five shaded squares: (A) From the 7 squares, we can form an L-shape (3 squares), a T-shape (4 squares) and a 2x2 square (4 squares), each using 5 shaded squares with some overlap. So (A) is possible. (B) The 9 squares can be split into a 2x2 square (4 squares), a 3x2 rectangle (6 squares), and an L-shape (3 squares), each using 5 shaded squares with some overlap. So (B) is possible. (C) The 5 squares cannot be split into three different figures of 5 shaded squares each, as there are not enough squares. So (C) is impossible. (D) The 8 squares can form a 2x2 square (4 squares), a 3x2 rectangle (6 squares), and a T-shape (4 squares), each using 5 shaded squares with some overlap. So (D) is possible. (E) The 10 squares can be split into a 2x2 square (4 squares), a 3x2 rectangle (6 squares), and a cross shape (5 squares), each using 5 shaded squares with some overlap. So (E) is possible. Therefore, the only key that cannot be cut into three different figures of five shaded squares is (C). C3

**XGen-MM:** The image shows five different figures, each composed of a series of rectangles and squares. To determine which figure is impossible to cut into three different figures of five shaded squares, we need to analyze the structure of each figure. A) The figure is composed of a series of rectangles and squares, and it is possible to cut it into three different figures of five shaded squares. B) The figure is composed of a series of rectangles and squares, and it is possible to cut it into three different figures of five shaded squares. C) The figure is composed of a series of rectangles and squares, and it is possible to cut it into three different figures of five shaded squares. D) The figure is composed of a series of rectangles and squares of rectangles and squares, and it is possible to cut it into three different figures of five shaded squares. D) The figure is composed of a series of rectangles and squares, and it is possible to cut it into three different figures of five shaded squares. E) The figure is composed of a series of rectangles and squares, and it is possible to cut it into three different figures of five shaded squares. Based on the analysis, all figures can be cut into three different figures of five shaded squares. Therefore, the answer is not possible, and there is no correct option to choose.

Table 7: Puzzle #23 for 3-4 graders 2020. Correct answer is B.

[MISSING_PAGE_FAIL:17]

**Question:** An isosceles triangle ABC, with AB = AC, is split into three smaller isosceles triangles, as shown, so that AD= DB, CE= CD, and BE= EC. (Note that the diagram is not drawn to scale.) What is the size, in degrees, of angle BAC? Options: A1: 24 B2: 28 C3: 30 D4: 35 E5: 36

**GPT-4o:** To solve for the measure of \(\langle\)BAC in the isosceles triangle ABC with given conditions, follow these steps: 1. Since AB = AC, triangle ABC is isosceles, which implies that \(\langle\)CAB = \(\langle\)ACB. 2. The point D divides the AB side in half, so AD = DB. Also, CD = CE and EC = EB as provided. Let's denote: - \(\langle\)BAC = 2\(\alpha\) (since AB = AC, so \(\langle\)BAC is twice the angle we are trying to find) - \(\langle\)ABC = \(\langle\)ACB = \(\beta\) (since triangle ABC isosceles) From the triangle properties, we can establish: [ 2\(\alpha\) + 2\(\beta\) = 180 degrees ] [ \(\alpha\) + \(\beta\) = 90 degrees ] Since AD = DB and BE = EC, and noting that the triangles are split symmetrically with BE and EC being equal, triangles ADE and CED are also isosceles and congruent. This gives more relations among the inner angles: - Let \(\langle\)ADE = \(\langle\)EDA = x - Let \(\langle\)CDE = y Given each triangle is isosceles, sum of angles in triangle ADE and CDE: [ 2x + \(\alpha\) = 180 degrees \(\rightarrow\) x + y = 90 degrees (since each triangle contributes equally to the full exterior angles, split evenly by E's position precisely) ] Simplifying recursively each geometrical condition: Since the combined triangles relate symmetrically: [ \(\alpha\) + \(\beta\) + \(\alpha\) = 180 degrees (sum of angles in triangle ABC) ] Finally, evaluating that with each resulting equiangular split: [ Place these all together as x being central part: ] Since our primary goal was solving \(\langle\)BAC: [ hence 3 equals equal relations to \(\alpha\) angle, comprise 360/2 equal all middle angles proportions  Thus diameter alpha integrations place: (\(\alpha\) based yields splits ratio as, governing full comprising equal contributors) Fully unifying solved combinatory resultant is angle triangle ( \(\langle\)BAC = 36 degrees  Option E5 solved value : Result conclusion, E5: 36.

**Gemini-Pro v1.5:** Let's denote the measure of angle BAC as x. **Step 1: Analyze the smaller triangles.****Triangle ADB** Since AD = DB, this triangle is isosceles. Therefore, angle ABD = angle BAD. Let's call this angle 'y', ****Triangle BCE:** Similarly, with BE = EC, this triangle is isosceles, and angle CBE = angle BCE. Let's call this angle 'z'. ****Triangle CDE:** Again, with CE = CD, this triangle is isosceles, and angle CED = angle CDE. Since angle CED is an exterior angle to triangle BCE, angle CED = angle CBE + angle BCE = z + z = z. **Step 2:** Relate the angles in triangle ABC.** The angles of any triangle add up to 180 degrees. Therefore, in triangle ABC: x + y + y + z + 2z = 180 * Simplifying the equation: x + 2y + 3z = 180 **Step 3: Focus on triangle BCD.** In triangle BCD: y + z + 2z = 180 * Simplifying: y + 3z = 180 **Step 4: Solve for x.** Notice that we have two equations: * x + 2y + 3z = 180 * y + 3z = 180 * Subtract the second equation from the first equation to eliminate y and z: x = 0. **Step 5: Address the contradiction.** * We arrived at x = 0, which is impossible for an angle in a triangle. This means our initial assumption that the diagram represents a possible triangle is incorrect. **Conclusion:** The diagram is misleading because it's not drawn to scale. It's impossible to split an isosceles triangle in the way described and maintain all the isosceles properties. Therefore, there's no solution for the measure of angle BAC. **None of the answer options are correct.**

**Claude-3-Opus:** Let's solve this step-by-step: 1) Since ABC is an isosceles triangle with AB = AC, angle BAC = angle BCA. Let's denote this angle as x\({}^{o}\). 2) In triangle ABD, since AD = DB, angle BAD = angle ABD. Let's denote this angle as y\({}^{o}\). Similarly, in triangle CDE, angle DCE = angle CED = y\({}^{o}\). 3) In triangle BEC, angle CBE = angle BCE. Let's denote this angle as z\({}^{o}\). 4) In any triangle, the sum of the angles is 180\({}^{o}\). Applying this to triangle ABC: x\({}^{o}\) + x\({}^{o}\) + (2y\({}^{o}\) + 2z\({}^{o}\)) = 180\({}^{o}\) 2x\({}^{o}\) + 2y\({}^{o}\) + 2z\({}^{o}\) = 180\({}^{o}\) x\({}^{o}\) + y\({}^{o}\) + z\({}^{o}\) = 90\({}^{o}\) 5) In an isosceles triangle, the angles opposite the equal sides are equal. So in triangle ADB, 2y\({}^{o}\) + z\({}^{o}\) = 180\({}^{o}\). Similarly, in triangle CDE, 2y\({}^{o}\) + z\({}^{o}\) = 180\({}^{o}\). 6) From step 5, we can deduce that z\({}^{o}\) = 180\({}^{o}\) - 2y\({}^{o}\). 7) Substituting this into the equation from step 4: x\({}^{o}\) + y\({}^{o}\) + (180\({}^{o}\) - 2y\({}^{o}\)) = 90\({}^{o}\) x\({}^{o}\) + 180\({}^{o}\) - y\({}^{o}\) = 90\({}^{o}\) x\({}^{o}\) - y\({}^{o}\) = -90\({}^{o}\) 8) In triangle BEC, 2z\({}^{o}\) + y\({}^{o}\) = 180\({}^{o}\). Substituting z\({}^{o}\) from step: 6: (2180 - 2y\({}^{o}\)) + y\({}^{o}\) = 180\({}^{o}\) 360\({}^{o}\) - 4y\({}^{o}\) + y\({}^{o}\) = 180\({}^{o}\) 360\({}^{o}\) - 3y\({}^{o}\) = 180\({}^{o}\) - 3y\({}^{o}\) = -180\({}^{o}\) - 180\({}^{o}\) y\({}^{o}\) = 60\({}^{o}\)) Substituting y\({}^{o}\) = 60\({}^{o}\) into the equation from step 7: x\({}^{o}\) - 60\({}^{o}\) = -90\({}^{o}\) x\({}^{o}\) = 30\({}^{o}\) Therefore, angle BAC is 30\({}^{o}\). Answer: C3: 30

**XGen-MM:**The given isosceles triangle ABC is split into three smaller isosceles triangles, with AD = DB, CE = CD, and BE = EC. Since AB = AC, we can conclude that the angles at A and C are equal. Let's denote the angle at A as 2x and the angle at C as x. In the larger triangle, the sum of the angles is 180 degrees. Therefore, we have: 2x + x + 2x = 180 Combining like terms, we get: 4x = 180 Dividing both sides by 4, we find: x = 45 SiBe the angle at C is 45 degrees, and the angle at A is twice that, the angle at A is 90 degrees. So, the size of angle BAC is 90 degrees, which corresponds to option E5.

**Question:** The figure shows a semicircle with center 0. Two of the angles are given. What is the size, in degrees, of the angle \(\alpha\)? Options: A1: 9\({}^{\circ}\) B2: 11\({}^{\circ}\) C3: 16\({}^{\circ}\) D4: 17.5\({}^{\circ}\) E5: 18\({}^{\circ}\)

**GPT-4o:** To solve for the angle \(\alpha\) in the given semicircle, we need to use the properties of circles and angles. Let's identify the key points and steps: 1. **Identify given angles and properties:** - The semicircle has a diameter that forms a straight line. - The angle close to the left angle of the semicircle is 32\({}^{\circ}\). - The angle close to the right angle near the circumference is 67\({}^{\circ}\). - The sum of angles in a triangle is 180\({}^{\circ}\). 2. **Identify the triangles involved:** - Let's denote the center of the semicircle as O. - Denote the intersection points along the circumference as A (left endpoint) and B (right endpoint), and the point where \(\alpha\) is the vertex as C. 3. **Calculate angles in triangle OBC:** - Since \(\triangle\) OBC lies on the diameter of the semicircle, \(\angle\) OBC = 90\({}^{\circ}\) (angle subtended by the diameter). 4. **Calculate remaining angle:** - In \(\triangle\) OBC : \(\angle\) BOC = 67\({}^{\circ}\) - Since angles in a triangle sum to 180\({}^{\circ}\): \(\angle\) OCB = 180\({}^{\circ}\) - 90\({}^{\circ}\) - 67\({}^{\circ}\) = 23\({}^{\circ}\) 5. **Consider isosceles triangle properties:** - Note that \(\triangle\) AOC will be isosceles because OA = OC (both are radii of the circle). 6. **Calculate angle at circumference:** - Angle subtended at the circle (by diameter) over the arc, simplify the calculation using the sum of angle properties given and solving systematically. Given the semicircle, combining all properties and laws leads us to find: \(\alpha\) = 17.5\({}^{\circ}\) Thus, the size of the angle \(\alpha\) is: - D4: 17.5\({}^{\circ}\)

**Gemini-Pro v1.5:** Here's how to solve the problem: **Angles in a Triangle:** The angles inside any triangle always add up to 180\({}^{\circ}\). **Angles on a Straight Line:** The angles on a straight line always add up to 180\({}^{\circ}\). **Let's apply these rules:** 1. **Find the missing angle in the triangle:** The triangle at the top of the semicircle has angles of 38\({}^{\circ}\) and 90\({}^{\circ}\) (right angle). Therefore, the missing angle is 180\({}^{\circ}\) - 90\({}^{\circ}\) - 38\({}^{\circ}\) = 52\({}^{\circ}\). 2. **Find the angle next to \(\alpha\):** This angle and the 52\({}^{\circ}\) angle form a straight line. So, the angle next to \(\alpha\) is 180\({}^{\circ}\) - 52\({}^{\circ}\) = 128\({}^{\circ}\). 3. **Angles in a semicircle:** An angle inscribed in a semicircle is always a right angle (90\({}^{\circ}\)). 4. **Solve for \(\alpha\):** The angles inside the triangle containing \(\alpha\) add up to 180\({}^{\circ}\). We have 90\({}^{\circ}\), 128\({}^{\circ}\), and \(\alpha\). Therefore, \(\alpha\) = 180\({}^{\circ}\) - 90\({}^{\circ}\) - 128\({}^{\circ}\) = -38\({}^{\circ}\). Since angles cannot be negative, we've essentially found the angle's supplement. To get the actual value of \(\alpha\), subtract from 180\({}^{\circ}\): 180\({}^{\circ}\) - 38\({}^{\circ}\) = 142\({}^{\circ}\). 5. **Find the supplement of \(\alpha\):** The question asks for the size of angle \(\alpha\), which is supplementary to 142\({}^{\circ}\). Therefore, \(\alpha\) = 180\({}^{\circ}\) - 142\({}^{\circ}\) = 38\({}^{\circ}\). **Answer: \(\alpha\) = 38\({}^{\circ}\)**

**Claude-3-Opus:** Let's solve this step-by-step: 1)To find the size of angle \(\alpha\), we can follow these steps: 1. In a semicircle, the angle formed by a line from the center to any point on the semicircle and the diameter is a right angle (90\({}^{\circ}\)). This means \(\angle\)OAB = 90\({}^{\circ}\). 2. The sum of angles in a triangle is 180\({}^{\circ}\). In triangle OAB, we have \(\angle\)OAB = 90\({}^{\circ}\), \(\angle\)OBA = 32\({}^{\circ}\), so the remaining angle \(\angle\)AOB = 180\({}^{\circ}\) - 90\({}^{\circ}\) - 32\({}^{\circ}\) = 58\({}^{\circ}\). 3. The angle at the center of a circle is twice the angle at the circumference subtended by the same arc. Here, \(\angle\)AOB is the central angle, and \(\alpha\) is the angle at the circumference subtended by the same arc AB. So, \(\angle\)AOB = 2\(\alpha\). 4. Since \(\angle\)AOB = 58\({}^{\circ}\), we have: 58\({}^{\circ}\) = 2\(\alpha\)\(\alpha\) = 58\({}^{\circ}\) : 2 = 29\({}^{\circ}\) 5. In triangle AOC, we know \(\angle\)OAC = 90\({}^{\circ}\) (from step 1) and \(\angle\)ACO = 67\({}^{\circ}\). So the remaining angle \(\angle\)AOC = 180\({}^{\circ}\) - 90\({}^{\circ}\) - 67\({}^{\circ}\) = 23\({}^{\circ}\). 6. Angle \(\alpha\) is the difference between \(\angle\)AOB and \(\angle\)AOC: \(\alpha\)= \(\angle\)AOB - \(\angle\)AOC = 29\({}^{\circ}\) - 23\({}^{\circ}\) = 6\({}^{\circ}\) Therefore, the size of angle \(\alpha\) is 6\({}^{\circ}\), which is not one of the given options. The closest option is A1: 9\({}^{\circ}\). A1

**XGen-MM:** To find the size of the angle \(\alpha\), we can use the fact that the sum of the angles in a triangle is 180 degrees. In this case, we have a triangle with angles 32\({}^{\circ}\), \(\alpha\), and 67\({}^{\circ}\). We can set up the equation: 32 + \(\alpha\) + 67 = 180 Solving for \(\alpha\), we get: \(\alpha\) = 180 - 32 - 67 = 80 Therefore, the size of the angle \(\alpha\) is 80 degrees. The correct answer is not listed among the options provided.

Table 10: Puzzle #15 for 9-10 graders 2021. Correct answer is A.

**Question:** Part of the fifth degree polynomial shown cannot be seen because of an inkblot. It is known that all five roots of the polynomial are integers. What is the highest power of \(x-1\) that divides the polynomial? Options: A1: \((x-1)^{1}\) B2: \((x-1)^{2}\) C3: \((x-1)^{3}\) D4: \((x-1)^{4}\) E5: \((x-1)^{5}\)

**GPT-4o:** Let's analyze the given polynomial P(x): \(P(x)=x^{5}-11x^{4}+\)(inkblot)\(-7\) We know that all five roots of the polynomial are integers. Let's denote the polynomial in terms of its roots: \(P(x)=(x-r_{1})(x-r_{2})(x-r_{3})(x-r_{4})(x-r_{5})\) Since the polynomial is of degree 5, it will have exactly 5 roots (counting multiplicities). We are interested in finding the highest power of \((x-1)\) that divides the polynomial \(P(x)\). This means we need to determine the multiplicity of the root \(x=1\). Given roots \(r_{1},r_{2},r_{3},r_{4},r_{5}\) are integers and 1 is a root (since we are looking for powers of \((x-1)\)), let's substitute \(x=1\) into the polynomial and its derivatives to check the multiplicity. 1. \(P(1):P(1)=1^{5}-11(1^{4})\) + (inkblot) - 7 = 1 - 11 + (inkblot) - 7 = -17 + (inkblot). Since \(P(1)=0\), this implies that the terms concealed by the inkblot sum to 17. 2. \(P^{\prime}(1):P^{\prime}(x)=5x^{4}-44x^{3}+\) (inkblot in the derivative term), \(P^{\prime}(1)=5(1)^{4}-44(1)^{3}+\) (inkblot in the derivative term at \(x=1\))= 5 - 44 + (inkblot derivative term)= -39 + (inkblot derivative term). For \(P^{\prime}(1)=0\), the derivative term must correct the -39 to zero, implying the sum of the derivative components handled by the inkblot term is 39. Given the conditions and polynomial degrees, It's logically safe to deduce \((x-1)\) is a root at least twice because both \(P(1)=0\) and \(P^{\prime}(1)=0\). Checking further derivatives won't create further zero conditions because the quintic polynomial boundary. Thus, the highest power of \((x-1)\) that divides \(P(x)\) is indeed indicative up to \(B2:(x-1)^{2}\). So, the answer is B2.

**Gemini-Pro v1.5:** The graph of the polynomial shows an x-intercept at x=1. **What does it mean for a polynomial to have an x-intercept at a particular value?** It means that the value of the polynomial is zero at that x-value. In other words, x=1 is a root of the polynomial. **The problem states that all roots are integers. What does this tell us about the factors of the polynomial?** Since x=1 is a root, (x-1) must be a factor of the polynomial. Since all roots are integers, we know all the factors will be of the form (x - a) where 'a' is an integer. **We need to find the highest power of (x-1) that divides the polynomial. How do we do that?** We can look at the behavior of the graph at x=1. The graph doesn't simply cross the x-axis at x=1, it touches the x-axis and then turns back up. This indicates that (x-1) is a factor multiple times (i.e., it has a multiplicity greater than 1). **Let's analyze the multiplicity:****If (x-1) had a multiplicity of 1,** the graph would cross the x-axis directly. ****The graph's behavior suggests a multiplicity of at least 2.** Since we don't have enough information to definitively say the multiplicity is higher than 2, we assume the lowest possible multiplicity that fits the graph. **Therefore, the highest power of (x - 1) that divides the polynomial is \((x-1)^{2}\). ****Answer: B2**

**Claude-3-Opus:** Let's break this down step-by-step: 1) The polynomial has 5 integer roots. This means \((x-1)\) is a factor of the polynomial 5 times. 2) The polynomial can be written as \((x-1)^{5}*g(x)\) where \(g(x)\) is some other polynomial. 3) At \(x=1\), the polynomial equals -7. Substituting \(x=1\) into the equation: \(1^{5}-11*1^{4}+[\text{inkblot}]-7=-7\). 4) Simplifying: \(1-11+[\text{inkblot}]-7=-7[\text{inkblot}]=-7+11-1+7=105)\). So the full polynomial is: \(x^{5}-11x^{4}+10x^{3}-76)\). Comparing coefficients with the expanded form of \((x-1)^{5}*g(x):x^{5}-5x^{4}+10x^{3}-10x^{2}+5x-1=(x-1)^{5}\). The given polynomial matches this up to the \(x^{3}\) term. The \(x^{2}\), \(x\) and constant terms are hidden by the inkblot. 7) Therefore, \((x-1)^{5}\) divides the polynomial. E5: \((x-1)^{5}\)

**XGen-MM:**The polynomial is \(x^{5}-11x^{4}+7x-7\). We can see that the polynomial is not factorable using the rational root theorem. Therefore, the highest power of \(x-1\) that divides the polynomial is \((x-1)^{1}\). The answer is A1.

Table 11: Puzzle #25 for 11-12 graders 2023. Correct answer is D.

Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [No] The dataset will be released by Math Kangaroo, which will have the appropriate license.
* Did you include the license to the code and datasets? [No] The code is proprietary currently and the data is owned by Math Kangaroo USA, who will be responsible for the licensing and public release.
* Did you include the license to the code and datasets? [No] See above.

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] 3. Did you discuss any potential negative societal impacts of your work? [No] The study is exploratory to understand human and machine cognition. We do not see any potential negative impacts our study will have on society. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] Yes, we do not see any ethical issues with our study or the dataset used in our study.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [No] The code is proprietary, however anyone should be able to implement our method. The data is owned by Math Kangaroo USA. So any user can contact them for the data release. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [N/A]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] Its from Math Kangaroo USA, and a representative is part of our author list. 2. Did you mention the license of the assets? [N/A] 3. Did you include any new assets either in the supplemental material or as a URL? [N/A] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]
5. If you used crowdsourcing or conducted research with human subjects...

1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]
2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]
3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]