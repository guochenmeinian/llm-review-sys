# Sharp Recovery Thresholds of

Tensor PCA Spectral Algorithms

 David L. Donoho

Department of Statistics

Stanford University

donoho@stanford.edu

&Michael J. Feldman

Department of Statistics

Stanford University

feldman6@stanford.edu

###### Abstract

Many applications seek to recover low-rank approximations of noisy tensor data. We consider several practical and effective _matricization_ strategies which construct specific matrices from such tensors and then apply spectral methods; the strategies include tensor unfolding, _partial tracing_, power iteration, and recursive unfolding. We settle the behaviors of unfolding and partial tracing, identifying sharp thresholds in signal-to-noise ratio above which the signal is partially recovered. In particular, we extend previous results to a much larger class of tensor shapes where axis lengths may be different. For power iteration and recursive unfolding, we prove that under conditions where previous algorithms partially recovery the signal, these methods achieve (asymptotically) exact recovery. Our analysis deploys random matrix theory to obtain sharp thresholds which elude perturbation and concentration bounds. Specifically, we rely upon recent _disproportionate_ random matrix results, which describe sequences of matrices with diverging aspect ratio.

## 1 Introduction

Tensors--multi-way arrays--and tensor methods are fundamental to modern data analysis. Data given by three or more indices are increasingly the focus in diverse areas including signal and image processing [9, 19, 22], while high-order moments represented by tensors are of interest in problems such as community detection and learning latent variable models [1, 2, 8].

The _spiked tensor model_, introduced by Montanari and Richard [20], is a simple statistical model of tensor data with latent low-rank structure. Observed data \(X\) of dimensions \(n_{1}\times n_{2}\times\cdots\times n_{k}\) are the sum of a low-rank tensor and noise:

\[X_{i_{1},i_{2},\ldots,i_{k}}=\lambda v_{1,i_{1}}v_{2,i_{2}}\cdots v_{k,i_{k}} +Z_{i_{1},i_{2},\ldots,i_{k}}\,,\qquad\qquad j\in[k],\,i_{j}\in[n_{j}]\,,\] (1)

where \(k\geq 2\) is the tensor order, \(\lambda\) is a signal strength, \(v_{j}\in\mathbb{S}^{n_{j}-1}\), \(j\in[k]\), are unit vectors, and \(Z\) is a noise tensor. Noise entries are assumed to be independent standard Gaussians:

\[Z_{i_{1},i_{2},\ldots,i_{k}}\stackrel{{ i.i.d}}{{\sim}}\mathcal{N}(0,1)\,, \qquad\qquad\qquad\qquad\qquad j\in[k],\,i_{j}\in[n_{j}]\,.\]

Estimation of \(v_{1},\ldots,v_{k}\) and the low-rank component of \(X\) generalizes to tensors the problem of low-rank matrix approximation--principal component analysis (PCA)--and is therefore known as tensor PCA.

This model reduces for \(k=2\) to the _spiked matrix model_, with maximum likelihood estimators of \(v_{1}\) and \(v_{2}\) given by the first left and right singular vectors \(\hat{v}_{1}\) and \(\hat{v}_{2}\) of \(X\), respectively. For \(k\geq 3\), however, computation of the maximum likelihood estimators of the low rank component amounts to solving an NP hard problem (Hillar and Lim [14]),

\[\sup_{j\in[k],u_{j}\in\mathbb{S}^{j-1}}\sum_{i_{1},\ldots,i_{k}}X_{i_{1},i_{2},\ldots,i_{k}}u_{1,i_{1}}\cdots u_{k,i_{k}}\,,\] (2)necessitating alternate, efficient algorithms.

### Contributions

This paper studies _matricization_-based approaches, which convert tensors to matrices (by reshaping or by contracting constructions to be described) and then apply spectral methods. We assume the spiked tensor model in a high-dimensional asymptotic framework in which each array dimension \(n_{1},\ldots,n_{k}\) tends to infinity; we make no assumptions on their relative rates. While previous analyses of tensor PCA algorithms have assumed order \(k=3\), supersymmetry \(v_{1}=v_{2}=\cdots=v_{k}\), or hypercubical format \(n_{1}=n_{2}=\cdots=n_{k}\), we permit tensors of diverse dimensions \(n_{j}\) and unrelated \(v_{j}\).

In this setting, we fully analyze tensor unfolding or reshaping, a widespread technique [12; 19]. Additionally, we analyze the _partial tracing_ approach of Hopkins et al. [16], discovering its asymptotic equivalence to unfolding in performance--an improvement over previous analysis. We identify sharp thresholds in signal-to-noise ratio above which these algorithms partially recover the signal, and provide exact formulas for their limiting performance. Finally, we study generalizations of the power iteration and recursive unfolding algorithms of [20]. Above the same thresholds characterizing partial recovery of unfolding and partial tracing, these algorithms achieve (asymptotically) exact signal recovery. In other words, for signal-to-noise ratios such that unfolding or partial tracing partially recover the signal, power iteration and recursive unfolding exactly recover the signal.

Our approach relies upon fundamental and penetrating random matrix theoretic (RMT) results. Precise analysis of tensor PCA algorithms, including pinpointing of phase transitions and sharp performance quantification as provided here, is not possible purely via more standard tools in theoretical machine learning, such as matrix concentration and vector perturbation bounds. Specifically, we rely upon recent _disproportionate_ random matrix results (introduced in Section 1.3), which describe sequences of matrices with diverging aspect ratio.

This work concisely demonstrates in the context of tensor PCA the advantages of an RMT-backed approach. Indeed, application of spiked matrix results yields simple and elegant theorems and proofs which are easily read. For example, the analysis of the partial tracing in [16], involving challenging matrix concentration and perturbation calculations, is reduced to a few careful lines. Though our results are admittedly purely asymptotic, there are no hidden constants or logarithmic factors. Even for modest-sized tensors, simulations demonstrate close agreement with theory.

### Assumptions and Notation

Without loss of generality, we assume \(\lambda\geq 0\) and \(n_{1}\leq n_{2}\leq\cdots\leq n_{k}\), taking \(n_{1}\) to be the "fundamental" problem index; that is \(n_{j}=n_{j}(n_{1})\), \(j\in[k]\). We say an estimator \(\hat{v}_{j}\)_partially recovers_\(v_{j}\) if almost surely,

\[\liminf_{n_{1}\to\infty}|\langle v_{j},\hat{v}_{j}\rangle|>0\,,\]

and \(\hat{v}_{j}\)_exactly recovers_\(v_{j}\) if \(|\langle v_{j},\hat{v}_{j}\rangle|\xrightarrow{a.s.}1\). In words, partial recovery demands a positive limit for the cosine similarity, while exact recovery requires asymptotically perfect cosine similarity.

We make the rank-one signal assumption in model (1) purely for expository efficiency; the spectral algorithms in this paper naturally generalize to rank-\(r\) spiked tensors of the form

\[X_{i_{1},i_{2},\ldots,i_{k}}=\sum_{i=1}^{r}\lambda_{r}v^{(i)}_{1,i_{1}}v^{(i) }_{2,i_{2}}\cdots v^{(i)}_{k,i_{k}}+Z_{i_{1},i_{2},\ldots,i_{k}}\,,\qquad \qquad j\in[k],\,i_{j}\in[n_{j}]\] (3)

where \(r\geq 1\) is the rank and \(\{v^{(1)}_{j},\ldots,v^{(r)}_{j}\}\subset\mathbb{S}^{n_{j}-1}\) are orthonormal sets, \(j\in[k]\). Such tensor decompositions are unique by Kruskal's theorem [18]. Where we estimate \(v^{(1)}_{j}\) by the first right singular vector of an appropriate matrix \(M_{j}\), \(v^{(2)}_{j},\ldots,v^{(r)}_{j}\) may be estimated by the subsequent right singular vectors, with analogous theoretical guarantees holding (that is, \(v^{(i)}_{j}\) is the right singular vector associated with the \(i\)-th largest singular value of \(M_{j}\)).

We denote by \(\otimes\) both the tensor outer product and Kronecker product (the latter is simply a vectorization of the former); it is clear from context which is meant. Let \(\times_{j}\) denote multiplication between a tensor along the \(j\)-th axis and a vector of conformable dimension. The notation \(a(n_{1})\lesssim b(n_{1})\)means \(a(n_{1})\leq Cb(n_{1})\) for a constant \(C>0\) and all sufficiently large \(n_{1}\), and \(a(n_{1})\asymp b(n_{1})\) means \(a(n_{1})\lesssim b(n_{1})\lesssim a(n_{1})\).

Finally, we introduce tensor _slices_. Let \(T\in\mathbb{R}^{n_{1}\times\cdots\times n_{s}}\) and fix \(\ell\in[k]\). The set \(\{T_{i_{1},\ldots,i_{l}}:j\in[l],i_{j}\in[n_{j}]\}\) contains the slices of \(X\) of order \(k-\ell\). Slices satisfy \(T_{i_{1},\ldots,i_{\ell}}\in\otimes_{j=\ell+1}^{k}\mathbb{R}^{n_{j}}\) and have entries

\[(T_{i_{1},\ldots i_{\ell}})_{i_{l+1},\ldots,i_{k}}=T_{i_{1},\ldots,i_{k}}\,, \qquad\qquad j\in\{\ell+1,\ldots,k\},i_{j}\in[n_{j}]\,.\]

### Spiked Matrix Model

The behavior of matricization-based algorithms fundamentally derives from spectral properties of the spiked matrix model,

\[X_{i_{1},i_{2}} =\lambda v_{1,i_{1}}v_{2,i_{2}}+Z_{i_{1},i_{2}}\,, i_{1}\in[n_{1}],\,i_{2}\in[n_{2}]\,.\] (4)

This model is extensively studied in random matrix theory, particularly under _proportional growth_, where \(n_{1}\) and \(n_{2}\) are of comparable magnitude:

\[n_{1},n_{2} \to\infty\,, \gamma_{n_{1}} =\frac{n_{1}}{n_{2}} \to\gamma>0\,.\]

Under proportional growth, the sample covariance matrix \(XX^{\top}/n_{2}\) does not converge to the identity (its expectation), and the leading left and right singular vectors \(\hat{v}_{1}\) and \(\hat{v}_{2}\) of \(X\) are inconsistent estimators of \(v_{1}\) and \(v_{2}\), respectively. We highlight the following results of Benaych-Georges and Rao Nadakuditi [7], who establish formulas for the limiting misalignment of \(\hat{v}_{1}\) and \(\hat{v}_{2}\):

**Lemma 1.1**.: _Let \(\hat{v}_{1}\) and \(\hat{v}_{2}\) denote the leading left and right singular vectors of \(X\), respectively, and define_

\[c^{2}(\tau,\gamma) =\begin{cases}1-\dfrac{\gamma(1+\tau^{2})}{\tau^{2}(\tau^{2}+ \gamma)}&\tau>\gamma^{1/4}\\ 0&\tau\leq\gamma^{1/4}\end{cases}\,.\]

_Under \(\lambda=\tau(1+o(1))\sqrt{n_{2}}\), where \(\tau\geq 0\) is fixed, and \(\gamma_{n_{1}}\to\gamma\in(0,1]\),_

\[|\langle v_{1},\hat{v}_{1}\rangle|^{2} \xrightarrow{a.s.}c^{2}(\tau,\gamma)\,, |\langle v_{2},\hat{v}_{2}\rangle|^{2} \xrightarrow{a.s.}c^{2}(\tau\gamma^{-1/2},\gamma^{-1})\,.\] (5)

Recently, Ben Arous et al. [5] and Feldman [13] independently studied the spiked matrix model under _disproportional growth_, where \(\gamma_{n_{1}}\to 0\) or \(\gamma_{n_{1}}\to\infty\). Transposing \(X\) in case \(\gamma_{n_{1}}\to\infty\), we assume without loss of generality that \(\gamma_{n_{1}}\to 0\). Phase transitions for the left and right singular vectors no longer coincide; \(v_{1}\) is reliably estimated at signal strengths much weaker than \(\lambda\gtrsim\sqrt{n_{2}}\). Drawing upon results in [5], [13], and [7], we have the following disproportionate analogs of Lemma 1.1:

**Lemma 1.2**.: _Let \(\hat{v}_{1}\) and \(\hat{v}_{2}\) denote the leading left and right singular vectors of \(X\), respectively, and define_

\[\widetilde{c}^{\,2}(\tau) =(1-\tau^{-4})_{+}\,, \widetilde{c}^{\,2}(\tau) =\frac{\tau^{2}}{1+\tau^{2}}\,.\]

_Under \(\lambda=\tau(1+o(1))(n_{1}n_{2})^{1/4}\), where \(\tau\geq 0\) is fixed, and \(\gamma_{n_{1}}\to 0\),_

\[|\langle v_{1},\hat{v}_{1}\rangle|^{2} \xrightarrow{a.s.}\widetilde{c}^{\,2}(\tau)\,, |\langle v_{2},\hat{v}_{2}\rangle|^{2} \xrightarrow{a.s.}0\,,\] (6)

_while under \(\lambda=\tau(1+o(1))\sqrt{n_{2}}\),_

\[|\langle v_{1},\hat{v}_{1}\rangle|^{2} \xrightarrow{a.s.}1\,, |\langle v_{2},\hat{v}_{2}\rangle|^{2} \xrightarrow{a.s.}\widetilde{c}^{\,2}(\tau)\,.\] (7)

Limits (7) are corollaries of Theorems 2.9 and 2.10 of [7]. We note that [5] requires \(n_{2}\) is polynomially bounded in \(n_{1}\). This assumption, however, is necessary only to establish non-asymptotic bounds; the almost-sure results in Lemma 1.2 hold as \(\gamma_{n_{1}}\to 0\) arbitrarily rapidly. While not explicitly stated in these references, it is easily verified that under proportional growth, \(\liminf\lambda/\sqrt{n_{2}}>\gamma^{1/4}\) implies partial recovery of \(v_{1}\) and \(v_{2}\). Analogous statements hold for disproportional growth.

Tensor Unfolding

We study a general unfolding procedure that permits (1) tensors of general, unequal axis lengths and (2) unfolding along arbitrary sets of axes.

Let \(N_{j}=\prod_{\ell=j}^{k}n_{\ell}\), \(j\in[k]\), and for \(S\subset[k]\), \(S\neq\emptyset\), let \(N(S)=\prod_{\ell\in S}n_{\ell}\). We define a map \(\text{Mat}_{S}:\mathbb{R}^{n_{1}\times\cdots\times n_{k}}\to\mathbb{R}^{(N_{1}/ N(S))\times N(S)}\) as follows: for indices \(i_{j}\in[n_{j}],j\in[k]\),

\[a=1+\sum_{j\in[k]\setminus S}(i_{j}-1)\prod_{\ell\in[k]\setminus S \colon j<\ell}n_{\ell}\,,\qquad\qquad b=1+\sum_{j\in S}(i_{j}-1)\prod_{\ell \in S\colon j<\ell}n_{\ell}\] (8)

and

\[[\text{Mat}_{S}(T)]_{a,b}=T_{i_{1},i_{2},\ldots,i_{k}}\,.\] (9)

When unfolding along a single axis, we write \(\text{Mat}_{j}=\text{Mat}_{\{j\}}\). In this case, (8) reduces to

\[a=1+\sum_{\ell=1}^{j-1}(i_{\ell}-1)\frac{N_{\ell+1}}{n_{j}}+\sum_{ \ell=j+1}^{k}(i_{\ell}-1)N_{\ell+1}\] (10)

(taking \(N_{k+1}=1\)) and \(b=i_{j}\). The unfolded matrix \(\text{Mat}_{j}(X)\) is a spiked matrix with aspect ratio \(n_{j}^{2}/N_{1}\):

\[\text{Mat}_{j}(X)=\lambda(v_{1}\otimes\cdots\otimes v_{j-1}\otimes v_{j+1} \otimes\cdots\otimes v_{k})v_{j}^{\top}+\text{Mat}_{j}(Z)\,.\]

Similarly, \(\text{Mat}_{S}(X)\) is a spiked matrix with aspect ratio \(N(S)^{2}/N_{1}\):

\[\text{Mat}_{S}(X)=\lambda(\otimes_{j\in[k]\setminus S}v_{j})( \otimes_{j\in S}v_{j})^{\top}+\text{Mat}_{S}(Z)\,.\] (11)

We now review prior results on unfolding. For \(v_{1}=\cdots=v_{k}\), Montanari and Richard consider the unfolding \(S=\{1,\ldots,\lfloor k/2\rfloor\}\), estimating \(\otimes^{\lfloor k/2\rfloor}v_{1}\) by the first right singular vector of \(\text{Mat}_{S}(X)\). They prove that \(\otimes^{\lfloor k/2\rfloor}v_{1}\) is (partially) recovered for \(\lambda\gtrsim n_{1}^{\lceil k/2\rceil/2}\), and conjecture sufficiency of \(\lambda\gtrsim n_{1}^{k/4}\) (note that these bounds differ only for odd \(k\)). Theorem 5.8 of Hopkins et al. [15] completes this conjecture for \(k=3\), establishing sufficiency of \(\lambda\geq(1+\varepsilon)n_{1}^{3/4}\).

Complete analysis of unfolding--handling tensors of arbitrary order and asymmetric dimensions--relies on results in Section 1.3, which reveal (1) necessary and sufficient thresholds for recovery and (2) the exact limiting performance of estimates. In addition, spiked matrix results yield the exact limiting cosine similarity of unfolding procedures, not obtained in [20] or [15]. The recent disproportional results summarized in Lemma 1.2 are crucial as unfoldings such as \(\text{Mat}_{j}(X)\), \(j\in[k-1]\), have limiting aspect ratio zero (recall that \(n_{1}\leq\cdots\leq n_{k}\)). Depending on the relative growth rates of \(n_{1},\ldots,n_{k}\), unfoldings such as \(\text{Mat}_{k}(X)\) may fall under disproportional "all" growth (\(n_{k}^{2}/N_{1}\to 0\)), proportional growth (\(n_{k}^{2}/N_{1}\asymp 1\)), or disproportional "wide" growth (\(n_{k}^{2}/N_{1}\to\infty\)). Our analysis is similar to that of Ben Arous et al. [5], which permits arbitrary order \(k\) yet assumes \(n_{1}=\cdots=n_{k}\).

**Theorem 2.1**.: _Let \(S\subset[k]\). If \(N(S)^{2}/N_{1}\to 0\), the first right singular vector \(w\) of the unfolding \(\text{Mat}_{S}(X)\) partially recovers \(\otimes_{j\in S}v_{j}\) if and only if \(\liminf\lambda/N_{1}^{1/4}>1\). In particular, for \(\lambda=\tau(1+o(1))N_{1}^{1/4}\) with \(\tau\geq 0\) fixed,_

\[|\langle\otimes_{j\in S}v_{j},w\rangle|^{2}\xrightarrow{a.s.} \widetilde{c}^{\,2}(\tau)\,.\] (12)

_If \(N(S)^{2}/N_{1}\asymp 1\), \(\otimes_{j\in S}v_{j}\) is partially recovered if and only if \(\liminf\lambda/N_{1}^{1/4}>1\). For \(\gamma_{n_{1}}\to\gamma\) and \(\lambda=\tau(1+o(1))N_{1}^{1/4}\),_

\[|\langle\otimes_{j\in S}v_{j},w\rangle|^{2}\xrightarrow{a.s.}c^{ \,2}(\tau\gamma^{1/4},\gamma)\,.\] (13)

_Finally, if \(N(S)^{2}/N_{1}\to\infty\), \(\otimes_{j\in S}v_{j}\) is partially recovered if and only if \(\liminf\lambda/\sqrt{N(S)}>0\). For \(\lambda=\tau(1+o(1))\sqrt{N(S)}\),_

\[|\langle\otimes_{j\in S}v_{j},w\rangle|^{2}\xrightarrow{a.s.} \widetilde{c}^{\,2}(\tau)\,.\] (14)All proofs are deferred to the supplement. As the recovery threshold is identical across all unfoldings \(S\) such that \(N(S)^{2}/N_{1}\lesssim 1\), we propose the following simple algorithm to estimate \(v_{j}\), which may be iterated for \(j\in[k]\):

``` Input:\(X,j\) return first right singular vector of \(\text{Mat}_{j}(X)\) ```

**Algorithm 1** Tensor unfolding

**Corollary 2.1.1**.: _Algorithm 1 partially recovers \(v_{1},\ldots,v_{k-1}\) if and only if \(\liminf\lambda/N_{1}^{1/4}>1\). Vector \(v_{k}\), corresponding to the largest dimension, is partially recovered as well if \(n_{k}^{2}/N_{1}\lesssim 1\). On the other hand, if \(n_{k}^{2}/N_{1}\to\infty\), \(v_{k}\) is recovered if and only if \(\liminf\lambda/\sqrt{n_{k}}>0\)._

## 3 Partial Tracing

Hopkins et al. [16] propose the partial tracing method for tensors of order \(k=3\) as an alternative to unfolding. Their approach generalizes to orders \(k\geq 3\), although it inherently relies on supersymmetry

Figure 1: Simulations of tensor unfolding with \(k=3\), supersymmetric signal \(v_{1}=v_{2}=v_{3}\), and \(n_{1}\in\{400,800,1200\}\). Solid lines display empirical cosine similarities (each point is the average of 50 realizations). The dashed line is the theoretical limit \(\widetilde{c}(\tau)\), which agrees closely. Below the phase transition located at \(\tau=1\), \(\hat{v}_{1}\) is approximately uniformly distributed on the surface of the unit sphere, so \(|\langle v_{1},\hat{v}_{1}\rangle|=O(n^{-1/2})\).

Figure 2: Simulations of tensor unfolding with \(k=3\), \(n_{1}=n_{2}=50\), and \(n_{3}=10000\) (left) or \(n_{3}=20000\) (right). Solid lines display empirical cosine similarities (each point is the average of 50 realizations). The dashed lines are theoretical limits based on Theorem 2.1, which agree closely. As \(n_{3}/(n_{1}n_{2})=O(1)\), we compare \(|\langle v_{1},\hat{v}_{1}\rangle|\) and \(|\langle v_{2},\hat{v}_{2}\rangle|\) to \(\widetilde{c}(\tau)\) and \(|\langle v_{3},\hat{v}_{3}\rangle|\) to \(c(\tau(N_{1}/n_{3}^{2})^{1/4},N_{1}/n_{3}^{2})=c(\lambda/\sqrt{n_{3}},N_{1}/n _{3}^{2})\).

of the signal, \(v_{1}=\cdots=v_{k}\). The partial tracing operator \(\text{Tr}_{k}:\otimes^{k}\mathbb{R}^{n}\mapsto\mathbb{R}^{n\times n}\) constructs a matrix by linearly combining tensor slices with partial trace weights:

\[\text{Tr}_{k}(T)=\sum_{i_{1},\ldots,i_{k-2}\in[n]}\text{tr}(T_{i_{1},\ldots,i_{ k-2}})T_{i_{1},\ldots,i_{k-2}}\enspace.\] (15)

This operation of reducing the order of a tensor by linear combinations of slices is called contraction in tensor analysis.

``` Input:\(X\) return first right singular vector of \(\text{Tr}_{k}(X)\) ```

**Algorithm 2** Partial tracing

For \(k=3\), the runtime of Algorithm 2 is \(O(n^{3})\), while that of unfolding is \(O(n^{3}\log n)\) (see Table 3 of [16]). Hopkins et al. established a bound on the recovery threshold of partial tracing which is worse than that of unfolding by a logarithmic factor: \(\lambda\gtrsim n^{3/4}(\log n)^{1/2}\). We eliminate the logarithmic factor here: while delivering improvements in runtime, Algorithm 2 is asymptotically equivalent to unfolding in recovery performance.

**Theorem 3.1**.: _The first right singular vector \(\hat{v}_{1}\) of the partial trace matrix \(\text{Tr}_{k}(X)\) partially recovers \(v_{1}\) if and only if \(\liminf\lambda/n_{1}^{k/4}>1\). For \(\lambda=\tau(1+o(1))n_{1}^{k/4}\), where \(\tau\) is fixed,_

\[|\langle v_{1},\hat{v}_{1}\rangle|^{2}\xrightarrow{a.s.}\tilde{c}^{2}(\tau)\,.\] (16)

Under \(n_{1}=\cdots=n_{k}\), the behavior of unfolding is given by (12), matching (16) exactly. Thus, unfolding and partial tracing are asymptotically equivalent in performance.

## 4 Exact Recovery

Montanari and Richard [20] consider a two-step recursive unfolding procedure that remarkably _exactly_ recovers \(v_{1}\) above the threshold of unfolding, assuming even order \(k\), hypercubical shape \(n_{1}=\cdots=n_{k}\), and a supersymmetric signal, \(v_{1}=\cdots=v_{k}\). In this section, we prove that exact recovery is possible for tensors of arbitrary axis lengths. We consider generalizations of the power iteration and recursive unfolding algorithms of [20] (initialized via tensor unfolding). Tensor power iteration is previously studied in [17] (assuming a supersymmetric signal) and [21] (assuming

Figure 3: Simulations of partial tracing with \(k=3\), supersymmetric signal \(v_{1}=v_{2}=v_{3}\), and \(n_{1}\in\{400,800,1200\}\). Solid lines display empirical cosine similarities (each point is the average of 50 realizations). The dashed line is the theoretical limit \(\overline{c}(\tau)\), which agrees closely. Note the similarity to Figure 1.

\(n_{1}\asymp n_{2}\asymp\dots\asymp n_{k}\)), though these works assume the initial iterate is independent of \(X\), precluding initialization via unfolding.

In our more general setting, we find that unfolding-initialized power iteration achieves _exact_ recovery above the _partial_ recovery threshold of unfolding, established in Corollary 2.1.1 (assuming \(n_{k}^{2}/N_{1}\to 0\)). Recursive unfolding exactly recovers \(v_{1},\dots,v_{k-1}\) with no limitation on the growth rate of \(n_{k}\). Power iteration, which requires initial "warm" estimates of \(v_{1},\dots,v_{k}\), may fail when \(n_{k}^{2}\) and \(N_{1}\) are comparable. Recursive unfolding, on the other hand, does not rely upon an initial estimate of \(v_{k}\) to recover \(v_{1},\dots v_{k-1}\) exactly.

``` Input:\(X\), initial estimates \(\hat{v}_{1},\dots,\hat{v}_{k}\) from Algorithm 1  Iterate until convergence: for\(j\) from1 to\(k\): \(w_{j}\gets X\times_{1}\hat{v}_{1}\cdots\times_{j-1}\hat{v}_{j-1}\times_{j+1 }\hat{v}_{j+1}\cdots\times_{k}\hat{v}_{k}\) \(\hat{v}_{j}\gets w_{j}/\|w_{j}\|_{2}\) endfor return\(\hat{v}_{1},\dots,\hat{v}_{k}\) ```

**Algorithm 3** Power iteration

We prove that exact recovery is achieved in a single iteration of the outer loop. In practice, iterating until estimates converge is beneficial.

**Theorem 4.1**.: _If \(\liminf\lambda/N_{1}^{1/4}>1\) and \(n_{k}^{2}/N_{1}\to 0\), Algorithm 3 recovers \(v_{1},\dots,v_{k}\) exactly: denoting by \(\hat{v}_{1},\dots,\hat{v}_{k}\) the estimates after a single iteration of the outer loop,_

\[|\langle v_{j},\hat{v}_{j}\rangle|\xrightarrow{a.s.}1\,,\qquad\qquad\qquad j \in[k]\,.\] (17)

_Estimating the signal strength \(\lambda\) by \(\hat{\lambda}=\langle X,\hat{v}_{1}\otimes\dots\otimes\hat{v}_{k}\rangle\), we have_

\[\lambda^{-1}\|\hat{\lambda}\hat{v}_{1}\otimes\dots\otimes\hat{v}_{k}-\lambda v _{1}\otimes\dots\otimes v_{k}\|_{F}\xrightarrow{a.s.}0\,.\] (18)

We next consider a recursive unfolding procedure. As stated above, recursive unfolding is guaranteed to recover \(v_{1},\dots,v_{k-1}\) exactly if \(\liminf\lambda/N_{1}^{1/4}>1\); no bound is needed on the growth rate of \(n_{k}\) as in Theorem 4.1. In practice, it is beneficial to iterate Algorithm 4.

``` Input:\(X\), initial estimates \(\hat{v}_{1},\dots,\hat{v}_{k}\) from Algorithm 1 for\(j\) from2 to\(k\): \(\hat{v}_{j}\leftarrow\) first right singular vector of \(\text{Mat}_{j-1}(X\times_{1}\hat{v}_{1})\) endfor \(\hat{v}_{1}\leftarrow\) first right singular vector of \(\text{Mat}_{1}(X\times_{2}\hat{v}_{2})\) return\(\hat{v}_{1},\dots,\hat{v}_{k}\) ```

**Algorithm 4** Recursive unfolding

**Theorem 4.2**.: _Under \(\liminf\lambda/N_{1}^{1/4}>1\), Algorithm 4 recovers \(v_{1},\dots,v_{k-1}\) exactly. Moreover, \(v_{k}\) is recovered exactly if \(n_{k}^{2}/N_{1}\lesssim 1\)._

The analysis of Algorithm 4 entails careful application of spiked matrix model results. By the linearity of the unfolding operator,

\[\text{Mat}_{j-1}(X\times_{1}\hat{v}_{1})=\lambda\langle v_{1},\hat{v}_{1} \rangle(\otimes_{\ell\in[k]\setminus\{1,j\}}v_{\ell})v_{j}^{\top}+\text{Mat}_ {j-1}(Z\times_{1}\hat{v}_{1})\,.\] (19)

Observe that \(Z\times_{1}\hat{v}_{1}\) is a reshaping of the vector \(\text{Mat}_{1}(Z)\hat{v}_{1}\). As \(\hat{v}_{1}\) and \(Z\) are dependent, the second term on the right-hand side is not a matrix of i.i.d. entries. Despite dependencies, we claim that appropriately scaled, \(\text{Mat}_{1}(Z)\hat{v}_{1}\) is distributed as Gaussian noise, in which case exact recovery thresholds are a consequence of spiked matrix model results of Section 1.3 applied to (19).

Below, we display simulation results for power iteration and recursive unfolding and compare to tensor unfolding and partial tracing.

Figure 4: Simulations of power iteration with \(k=3\), supersymmetric signal \(v_{1}=v_{2}=v_{3}\), and \(n_{1}\in\{400,800,1200\}\). Solid lines display empirical cosine similarities (each point is the average of 50 realizations). The dashed line is the theoretical prediction \(\mathbf{1}(\tau>1)\). Power iteration typically converges within five iterations.

Figure 5: Simulations of power iteration with \(k=3\), \(n_{1}=n_{2}=50\), and \(n_{3}=10000\) (left) or \(n_{3}=20000\) (right). Solid lines display empirical cosine similarities (each point is the average of 50 realizations). Vector \(v_{3}\), corresponding to the longest tensor axis, is estimated less well than \(v_{1},v_{2}\).

Figure 6: Simulations of a unfolding, partial tracing, and power iteration with \(k=3\), supersymmetric signal \(v_{1}=v_{2}=v_{3}\), and \(n_{1}=1200\). Solid lines display empirical cosine similarities (each point is the average of 50 realizations). The dashed line is the theoretical limit of unfolding and partial tracing, \(\widetilde{c}(\tau)\). Near the phase transition located at \(\tau=1\), the cosine similarity of power iteration is over twice that of unfolding.

## References

* [1] A. Anandkumar, R. Ge, D. Hsu, and S. M. Kakade. A Tensor Approach to Learning Mixed Membership Community Models. _Journal of Machine Learning Research_, 1:2239-2312. 2014.
* [2] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor Decompositions for Learning Latent Variable Models. _Journal of Machine Learning Research_, 15:2773-2832, 2014.
* [3] M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. _IEEE Transactions on Information Theory_, 57(2):764-785, 2011.
* [4] G. Ben Arous, R. Gheissari, and A. Jagannath. Algorithmic thresholds for tensor PCA. _Annals of Probability_, 48(4):2052-2087, 2020.
* [5]G. Ben Arous, D. Z. Huang, and J. Huang. Long Random Matrices and Tensor Unfolding. arXiv preprint arXiv2110.1021, 2021.
* [6] G. Ben Arous, S. Mei, A. Montanari, and M. Nica. The landscape of the spiked tensor model. _Communications on Pure and Applied Mathematics_, 72:2282-2330, 2019.
* [7] F. Benaych-Georges and R. R. Nadakuditi. The singular values and vectors of low rank perturbations of large rectangular random matrices. _Journal of Multivariate Analysis_, 111:120-135, 2012.
* [8] A. R. Benson, D. F. Gleich, and J. Leskovec. Tensor spectral clustering for partitioning higher-order network structures. _Proceedings of the 2015 SIAM International Conference on Data Mining_, 118-126, 2015.
* [9] X. Bi, X. Tang, Y. Yuan, Y. Zhang, A. Qu. Tensors in Statistics. _Annual Review of Statistics and Its Application_, 8:345-368, 2021.
* [10] A. Bloemendal, L. Erdos, A. Knowles, H. T. Yau, and J. Yin. Isotropic local laws for sample covariance and generalized Wigner matrices. _Electronic Journal of Probability_, 19(33):1-53, 2014.
* [11] B. B. Chen and G. M. Pan. Convergence of the largest eigenvalue of normalized sample covariance matrices when p and n both tend to infinity with their ratio converging to zero. _Bernoulli_, 18(4):1405-1420, 2012.
* [12] L. De Lathauwer, B. De Moor, and J. Vandewalle. A Multilinear Singular Value Decomposition. _SIAM Journal on Matrix Analysis and Applications_, 21(4):1253-1278, 2000.
* [13] M. J. Feldman. Spiked Singular Values and Vectors under Extreme Aspect Ratios. _Journal of Multivariate Analysis_, 196:105187, 2023.
* [14] C. J. Hillar and L. Lim. Most tensor problems are NP-hard. _Journal of the ACM_, 60(6):1-39, 2013.
* [15] S. B. Hopkins, J. Shi, and D. Steurer. Tensor principal component analysis via sum-of-square proofs. _Conference on Learning Theory_, 956-1006, 2015.
* [16] S. B. Hopkins, T. Schramm, J. Shi, and D. Steurer. Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors. _Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing_, 178-191, 2016.

Figure 7: Simulations of recursive unfolding with \(k=3\), \(n_{1}=n_{2}=50\), and \(n_{3}=10000\) (left) or \(n_{3}=50000\) (right). Solid lines display empirical cosine similarities (each point is the average of 50 realizations). The performance of recursive unfolding is quite similar to that of power iteration.

[17] J. Huang, D. Z. Huang, Q. Yang, and G. Cheng. Power Iteration for Tensor PCA. _Journal of Machine Learning Research_, 23:1-47, 2022.
* [18] J. B. Kruskal, Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics, _Linear Algebra and Applications_ 18(2):95-138, 1977.
* [19] H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos. A survey of multilinear subspace learning for tensor data. _Pattern Recognition_, 44(7):1540-1551, 2011.
* [20] A. Montanari and E. Richard. A statistical model for tensor PCA. _Proceedings of the 27th International Conference on Neural Information Processing Systems_, 2:2897-2905, 2014.
* [21] M. E. A. Seddik, M. Guillaud, and R. Couillet. When random tensors meet random matrices. arXiv preprint arXiv:2112.12348, 2021.
* [22] N. D. Sidiropoulos, L. De Lathauwer, X. Fu, K. Huang, E. E. Papalexakis, and C. Faloutsos. Tensor Decomposition for Signal Processing and Machine Learning. _IEEE Transactions on Signal Processing_, 65(13):3551-3582, 2017.
* [23] R. Tomioka and T. Suzuki. Spectral norm of random tensors, 2014. arXiv:1407.1870.
* [24] M. Wainwright. _High-Dimensional Statistics_. Cambridge, 2019.
* [25] Y. Yu, T. Wang, and R. J. Samworth. A useful variant of the Davis-Kahan theorem for statisticians. _Biometrika_, 102:315-323, 2015.

## Appendix

proof of Theorem (2.1).: By (11), \(\text{Mat}_{S}(X)\) is a spiked matrix with aspect ratio \(N(S)^{2}/N_{1}\) and signal strength \(\lambda\) (as \(\otimes_{j\in S}v_{j}\) is unit norm). There are four cases to consider:

* Let \(N(S)^{2}/N_{1}\to 0\); by the first point of (6), the recovery threshold lies at \((N_{1}/N(S)\cdot N(S))^{1/4}=N_{1}^{1/4}\).
* Let \(N(S)^{2}/N_{1}\to\gamma\in(0,1]\); by the first point of (5), partial recovery occurs for \(\liminf\lambda/\sqrt{N_{1}/N(S)}>\gamma^{1/4}\). Equivalently, \(\liminf\lambda/N_{1}^{1/4}>1\). Under \(\lambda=\tau(1+o(1))N_{1}^{1/4}\), \(\lim\lambda/\sqrt{N_{1}/N(S)}=\tau\gamma^{1/4}\), implying \(|\langle\otimes_{j\in S}v_{j},w\rangle|^{2}\xrightarrow{a.s.}c^{2}(\tau\gamma ^{1/4},\gamma)\).
* Let \(N(S)^{2}/N_{1}\to\gamma\in(1,\infty]\); by the second point of (5), partial recovery occurs for \(\liminf\lambda/\sqrt{N(S)}>\gamma^{-1/4}\). Equivalently, \(\liminf\lambda/N_{1}^{1/4}>1\). Under \(\lambda=\tau(1+o(1))N_{1}^{1/4}\), \(\lim\lambda/\sqrt{N(S)}=\tau\gamma^{-1/4}\), implying \(|\langle\otimes_{j\in S}v_{j},w\rangle|^{2}\xrightarrow{a.s.}c^{2}(\tau\gamma ^{-1/4}\cdot\gamma^{1/2},\gamma)=c^{2}(\tau\gamma^{1/4},\gamma)\).
* Let \(N(S)^{2}/N_{1}\to\infty\); by the second point of (7), the recovery threshold lies at \(\sqrt{N(S)}\).

Proof of Theorem 3.1.: We provide proof for \(k=3\); the proof for higher orders is similar and omitted. For notational simplicity, we suppress the subscripts of \(n_{1}\) and \(v_{1}\).

Let \(w_{i}=\text{tr}(Z_{i})\), \(i\in[n]\), and \(w=(w_{1},\dots,w_{n})^{\top}\). Expanding the partial trace matrix,

\[\begin{split}\text{Tr}_{k}(X)&=\sum_{i=1}^{n}( \lambda v_{i}+w_{i})X_{i}=\sum_{i=1}^{n}\left[(\lambda^{2}v_{i}^{2}+\lambda v_ {i}w_{i})vv^{\top}+(\lambda v_{i}+w_{i})Z_{i}\right]\\ &=\sum_{i=1}^{n}\left[(\lambda^{2}v_{i}^{2}+\lambda v_{i}w_{i})vv^ {\top}+(\lambda v_{i}+w_{i})(Z_{i}-\text{diag}(Z_{i}-\widetilde{Z}_{i}))+( \lambda v_{i}+w_{i})\text{diag}(Z_{i}-\widetilde{Z}_{i})\right],\end{split}\] (20)

where \(\widetilde{Z}\) is an independent copy of \(Z\).

Let \(M=\|\lambda v+w\|_{2}^{-1}\sum_{i=1}^{n}(\lambda v_{i}+w_{i})(Z_{i}-\text{diag}(Z_ {i}-\widetilde{Z}_{i}))\). As \(w_{i}\) and \(Z_{i}-\text{diag}(Z_{i}-\widetilde{Z}_{i})\) are independent and the Gaussian distribution is rotationally invariant, \(M\overset{d}{=}Z_{1}\). Hence, we have

\[\|\lambda v+w\|_{2}^{-1}\text{Tr}_{k}(X)=\alpha vv^{\top}+M+\|\lambda v+w\|_{2} ^{-1}\sum_{i=1}^{n}(\lambda v_{i}+w_{i})\text{diag}(Z_{i}-\widetilde{Z}_{i})\,,\] (21)

where \(\alpha=\|\lambda v+w\|_{2}^{-1}(\lambda^{2}+\lambda\langle v,w\rangle)\). The partial trace matrix is thus proportional to a perturbation of a spiked matrix with aspect ratio \(\gamma=1\).

Let \(\mathcal{E}\) denote the third term on the right-hand side of (21); we shall prove \(n^{-1/2}\|\mathcal{E}\|_{2}\overset{a.s.}{\longrightarrow}0\). Denoting \(u=(\lambda v+w)/\|\lambda v+w\|_{2}\), the diagonal entries of \(\sum_{i=1}^{n}u_{i}\text{diag}(\widetilde{Z}_{i})\) are i.i.d. Gaussians with variance one, implying \(\|\sum_{i=1}^{n}u_{i}\text{diag}(\widetilde{Z}_{i})\|_{2}\lesssim\sqrt{\log n}\), almost surely. Similarly, since \(w\sim\mathcal{N}(0,nI_{n})\) and \(\|\lambda v+w\|_{2}=\Theta_{a.s.}(\lambda+n)\),

\[\lambda\|\lambda v+w\|_{2}^{-1}\Big{\|}\sum_{i=1}^{n}v_{i}\text{diag}(Z_{i}) \Big{\|}_{2}\lesssim\sqrt{\log n}\Big{(}1+\frac{\lambda}{n}\Big{)}\,.\] (22)

To bound the remaining term of \(\mathcal{E}\), let \(\mathcal{Z}\in\mathbb{R}^{n\times n}\) denote the matrix with entries \(\mathcal{Z}_{ij}=Z_{ijj}\), \(i,j\in[n]\), in which case we may write

\[\Big{\|}\sum_{i=1}^{n}w_{i}\text{diag}(Z_{i})\Big{\|}_{2}=\sup_{1\leq j\leq n} \Big{|}\sum_{i=1}^{n}w_{i}Z_{ijj}\Big{|}=\sup_{1\leq j\leq n}|e_{j}^{\top} \mathcal{Z}^{\top}\mathcal{Z}\mathbf{1}_{n}|\,,\] (23)

where \(\mathbf{1}_{n}\) is the length-\(n\) vector of ones. As \(e_{j}^{\top}\mathcal{Z}^{\top}\mathcal{Z}e_{j}\sim\chi_{n}^{2}\) and

\[e_{j}^{\top}\mathcal{Z}^{\top}\mathcal{Z}(\mathbf{1}_{n}-e_{j})\overset{d}{=} \sqrt{n-1}e_{j}^{\top}\mathcal{Z}^{\top}\widetilde{Z}e_{j}\sim\sqrt{n-1}\cdot \mathcal{N}(0,1)\cdot\sqrt{\chi_{n}^{2}}\,,\]

standard bounds such as (2.19) in [24] yield

\[\mathbf{P}\Big{(}\sup_{1\leq j\leq n}\big{|}e_{j}^{\top}\mathcal{Z}^{\top} \mathcal{Z}\mathbf{1}_{n}-n\big{|}>c\cdot n\Big{)}\lesssim ne^{-Cn}\,,\] (24)

where \(c,C>0\) are constants. As the right-hand side is summable, the Borel-Cantelli lemma implies

\[\sup_{1\leq j\leq n}|e_{j}^{\top}\mathcal{Z}^{\top}\mathcal{Z}\mathbf{1}_{n}| \lesssim n\,,\] (25)

almost surely. Collecting the above bounds, we have that \(n^{-1/2}\|\mathcal{E}\|_{2}\overset{a.s.}{\longrightarrow}0\).

By Weyl's inequality, the limiting spectral distribution of \(n^{-1/2}(M+\mathcal{E})\) equals that of \(n^{-1/2}M\), the quarter circle law. The limits (5) from Lemma 1.1 therefore apply to \(\alpha vv^{\top}+M+\mathcal{E}\) as well (see [7]). Basic calculations yield (1) \(\liminf\alpha/\sqrt{n}>1\) if and only if \(\liminf\lambda/n^{3/4}>1\), and (2) under \(\lambda=\tau(1+o(1))n^{3/4}\), \(\alpha=\tau^{2}(1+o_{a.s.}(1))\sqrt{n}\). Therefore, \(\hat{v}\) partially recovers \(v\) if and only if \(\liminf\lambda/n^{3/4}>1\), and under \(\lambda=\tau(1+o(1))n^{3/4}\),

\[|\langle v,\hat{v}\rangle|^{2}\overset{a.s.}{\longrightarrow}c^{2}(\tau^{2},1 )=\tilde{c}^{2}(\tau)\,,\] (26)

completing the proof. 

Proof of Theorem 4.1.: By the linearity of the operators \(\times_{1},\ldots,\times_{k}\),

\[w_{j} =\lambda\prod_{\ell\in[k]\setminus\{j\}}\langle v_{\ell},\hat{v }_{\ell}\rangle\cdot v_{j}+Z\times_{1}\hat{v}_{1}\cdots\times_{j-1}\hat{v}_{j-1 }\times_{j+1}\hat{v}_{j+1}\cdots\times_{k}\hat{v}_{k}\,,\] (27) \[\langle v_{j},w_{j}\rangle =\lambda\prod_{\ell\in[k]\setminus\{j\}}\langle v_{\ell},\hat{v}_ {\ell}\rangle+\big{\langle}Z,\hat{v}_{1}\otimes\cdots\otimes\hat{v}_{j-1} \otimes v_{j}\otimes\hat{v}_{j+1}\cdots\otimes\hat{v}_{k}\big{\rangle}\,.\] (28)

Assume that (17) holds for \(\ell\in[j-1]\). For \(\ell\in\{j+1,\ldots,k\}\), Corollary 2.1.1 implies \(|\langle v_{\ell},\hat{v}_{\ell}\rangle|\) is bounded away from zero. The first term on the right-hand side of (28) is therefore \(\Theta_{a.s.}(\lambda)\). The second term is bounded by the spectral norm of \(Z\): using Theorem 1 of [23],

\[\big{\langle}Z,\hat{v}_{1}\otimes\cdots\otimes\hat{v}_{j-1}\otimes v_{j}\otimes \hat{v}_{j+1}\otimes\cdots\otimes\hat{v}_{k}\big{\rangle}\leq\sup_{u_{j}\in \delta^{j-1},j\in[k]}\langle Z,u_{1}\otimes\cdots\otimes u_{k}\rangle\lesssim \sqrt{n_{k}}\,,\] (29)almost surely. Thus, as \((\sqrt{n_{k}}/\lambda)^{4}\lesssim n_{k}^{2}/N_{1}\to 0\),

\[|\langle v_{j},\hat{v}_{j}\rangle|=1+O_{a.s.}\Big{(}\frac{\sqrt{n_{k}}}{\lambda} \Big{)}\xrightarrow{a.s.}1\,,\] (30)

from which (17) follows inductively.

Equation (18) follows from (17) and (29), which imply \(\|v_{1}\otimes\cdots\otimes v_{k}-\hat{v}_{1}\otimes\cdots\otimes\hat{v}_{k} \|_{F}\xrightarrow{a.s.}0\) and

\[\lambda^{-1}|\hat{\lambda}|=\bigg{|}\prod_{j=1}^{k}\langle v_{j},\hat{v}_{j} \rangle+\lambda^{-1}\langle Z,\hat{v}_{1}\otimes\cdots\hat{v}_{k}\rangle\bigg{|} \xrightarrow{a.s.}1\,.\] (31)

Proof of Theorem 4.2.: We shall prove \(v_{2},\ldots,v_{k}\) are recovered exactly; proofs for the first and last axes are similar and omitted. By the linearity of the unfolding operator,

\[\text{Mat}_{j-1}(X\times_{1}\hat{v}_{1})=\lambda\langle v_{1},\hat{v}_{1} \rangle(\otimes_{\ell\in[k]\setminus\{1,j\}}v_{\ell})v_{j}^{\top}+\text{Mat}_ {j-1}(Z\times_{1}\hat{v}_{1})\,.\] (32)

Observe that \(Z\times_{1}\hat{v}_{1}\) is a reshaping of the vector \(\text{Mat}_{1}(Z)\hat{v}_{1}\). As \(\hat{v}_{1}\) and \(Z\) are dependent, the second term on the right-hand side is not a matrix of i.i.d. entries. Despite dependencies, we claim that appropriately scaled, \(\text{Mat}_{1}(Z)\hat{v}_{1}\) is Gaussian noise, in which case exact recovery thresholds are a consequence of spiked matrix model results of Section 1.3 applied to (32).

By definition, \(\hat{v}_{1}\) is the first eigenvector of the symmetric matrix

\[\text{Mat}_{1}(X)^{\top}\text{Mat}_{1}(X)=\lambda^{2}v_{1}v_{1}^{\top}+\text{ Mat}_{1}(Z)^{\top}\text{Mat}_{1}(Z)+\mathcal{E}\,,\] (33)

where \(\mathcal{E}\) is a rank-two matrix given by

\[\mathcal{E}=\lambda v_{1}(\otimes_{\ell\in[k]\setminus\{1\}}v_{\ell})^{\top} \text{Mat}_{1}(Z)+\lambda\text{Mat}_{1}(Z)^{\top}(\otimes_{\ell\in[k] \setminus\{1\}}v_{\ell})v_{1}^{\top}\,.\]

Let \(\tilde{v}_{1}\) denote the first eigenvector of \(\text{Mat}_{1}(X)^{\top}\text{Mat}_{1}(X)-\mathcal{E}\); without loss of generality, we assume \(\hat{v}_{1}^{\top}\tilde{v}_{1}\geq 0\). Since \(\text{Mat}_{1}(Z)^{\top}(\otimes_{\ell\in[k]\setminus\{1\}}v_{\ell})\sim \mathcal{N}(0,I_{n_{1}})\), we have \(\|\mathcal{E}\|_{2}\lesssim\lambda\sqrt{n_{1}}\) almost surely. By Theorem 1.1 of [13], the spectral gap of \(\text{Mat}_{1}(X)^{\top}\text{Mat}_{1}(X)\) is \(\Theta(\lambda^{2})\). Thus, using the Davis-Kahan theorem (see Corollary 3 of [25]), we have

\[\|\hat{v}_{1}-\tilde{v}_{1}\|_{2}\lesssim\frac{\|\mathcal{E}\|_{2}}{\lambda^{ 2}}\xrightarrow{a.s.}0\,.\] (34)

Let \(\text{Mat}_{1}(Z)=U\Lambda V^{\top}\) be a singular value decomposition. As \(Z\) contains i.i.d. Gaussian entries, (1) \(U\), \(\Lambda\), and \(V\) are independent, (2) \(U\) and \(V\) are Haar-distributed. Moreover, as \(\tilde{v}_{1}\) is the first eigenvector of \(\lambda^{2}v_{1}v_{1}^{\top}+\text{Mat}_{1}(Z)^{\top}\text{Mat}_{1}(Z)=\lambda ^{2}v_{1}v_{1}^{\top}+V\Lambda^{2}V^{\top}\), \(\tilde{v}_{1}\) is independent of \(U\). Thus, \(U\Lambda V^{\top}\tilde{v}_{1}/\|\Lambda V^{\top}\tilde{v}_{1}\|_{2}\) is uniform on \(\mathbb{S}^{N_{2}-1}\). Generating \(\xi\sim\chi_{N_{2}}^{2}\) independent of \(Z\), it follows that

\[\frac{\xi}{\sqrt{N_{2}}}\cdot\frac{\text{Mat}_{1}(Z)\tilde{v}_{1}}{\|\Lambda V^ {\top}\tilde{v}_{1}\|_{2}}\sim\mathcal{N}(0,I_{N_{2}})\,.\] (35)

Defining the constant \(\alpha=\xi/(\sqrt{N_{2}}\|\Lambda V^{\top}\tilde{v}_{1}\|_{2})\), we deduce that \(\alpha Z\times_{1}\tilde{v}_{1}\) (which is a reshaping of the left-hand side of (35)) is distributed as a tensor with i.i.d. Gaussian entries--despite dependencies between \(Z\) and \(\tilde{v}_{1}\). Additionally, since \(\Lambda_{11}/\sqrt{N_{2}}\xrightarrow{a.s.}1\) and \(\Lambda_{n_{1}n_{1}}/\sqrt{N_{2}}\xrightarrow{a.s.}1\), we have \(\|\Lambda V^{\top}\tilde{v}_{1}\|_{2}/\sqrt{N_{2}}\xrightarrow{a.s.}1\) and \(\alpha\xrightarrow{a.s.}1\).

Thus, we conclude that

\[\alpha\text{Mat}_{j-1}(X\times_{1}\tilde{v}_{1})=\lambda(1+o_{a.s.}(1))\langle v _{1},\tilde{v}_{1}\rangle(\otimes_{\ell\in[k]\setminus\{1,j\}}v_{\ell})v_{j}^ {\top}+\widetilde{Z}\,,\quad j\in\{2,\ldots,k-1\}\,,\] (36)

where \(\widetilde{Z}\in\mathbb{R}^{n_{j}\times(N_{2}/n_{j})}\) contains i.i.d. Gaussian entries, enabling us to apply spiked matrix results. Let \(\tilde{v}_{j}\) denote the first right singular vector of \(\text{Mat}_{j-1}(X\times_{1}\tilde{v}_{1})\) (equivalently, that of \(\alpha\text{Mat}_{j-1}(X\times_{1}\tilde{v}_{1})\)); without loss of generality, we assume \(\tilde{v}_{1}^{\top}\tilde{v}_{1}\geq 0\). In particular, since \(|\langle v_{1},\tilde{v}_{1}\rangle|\asymp|\langle v_{1},\tilde{v}_{1}\rangle|\asymp\) by Corollary 2.1.1 and (34), Lemma 1.2 implies \(|\langle v_{j},\tilde{v}_{j}\rangle|\xrightarrow{a.s.}1\) (we have \(\lambda\gg(n_{j}\cdot N_{2}/n_{j})^{1/4}\)).

It therefore suffices to prove that \(\|\hat{v}_{j}-\tilde{v}_{j}\|_{2}\xrightarrow[u_{3}]{a.s.}0\). By Theorem 2.3 of [5] or Theorem 1.1 of [13] and Cauchy's interlacing inequality, the spectral gap of \(\text{Mat}_{j-1}(X\times_{1}\tilde{v}_{1})^{\top}\text{Mat}_{j-1}(X\times_{1} \tilde{v}_{1})\) is \(\Theta(\lambda^{2})\). Let \(\mathcal{Z}\) denote the reshaping of \(Z\) with dimensions \(n_{1}\times n_{j}\times N_{2}/n_{j}\) and slices \(\text{Mat}_{j-1}(Z_{i})\), \(i\in[n_{1}]\). Using the bound

\[\|\text{Mat}_{j-1}(Z\times_{1}(\hat{v}_{1}-\tilde{v}_{1}))\|_{2}\leq\sup_{ \begin{subarray}{c}u_{1}\in\mathbb{S}^{n_{1}-1},u_{2}\in\mathbb{S}^{n_{j}-1}, \\ u_{3}\in\mathbb{S}^{N_{2}/n_{j}-1}\end{subarray}}\big{(}\mathcal{Z}\times_{1} u_{1}\times_{2}u_{2}\times_{3}u_{3}\big{)}\cdot\|\hat{v}_{1}-\tilde{v}_{1}\|_{2}\]

and Theorem 1 of [23],

\[\|\text{Mat}_{j-1}(X\times_{1}(\hat{v}_{1}-\tilde{v}_{1}))\|_{2} \leq\lambda\|\hat{v}_{1}-\tilde{v}_{1}\|_{2}\|\text{Mat}_{j-1}(v_{ 2}\otimes\cdots\otimes v_{k})\|_{2}+\|\text{Mat}_{j-1}(Z\times_{1}(\hat{v}_{1} -\tilde{v}_{1}))\|_{2}\] (37) \[\lesssim\big{(}\lambda+(N_{2}/n_{j})^{1/2}\big{)}\|\hat{v}_{1}- \tilde{v}_{1}\|_{2}\,,\]

almost surely. Thus, using the Davis-Kahan theorem (Theorem 4 of [25]), (34), and (37), we have

\[\|\hat{v}_{j}-\tilde{v}_{j}\|_{2}\lesssim\frac{\lambda\big{(}\lambda+(N_{2}/n_ {j})^{1/2})\|\hat{v}_{1}-\tilde{v}_{1}\|_{2}}{\lambda^{2}}\xrightarrow[a.s.]{a.s.}0\,,\] (38)

completing the proof.