# Pretrained Transformer Efficiently Learns

Low-Dimensional Target Functions In-Context

 Kazusato Oko\({}^{1,3}\), Yujin Song\({}^{2,3}\), Taiji Suzuki\({}^{2,3}\), Denny Wu\({}^{4,5}\)

\({}^{1}\)University of California, Berkeley, \({}^{2}\)University of Tokyo, \({}^{3}\)RIKEN AIP

\({}^{4}\)New York University, \({}^{5}\)Flatiron Institute

oko@berkeley.edu, song-yujin139@g.ecc.u-tokyo.ac.jp,

taiji@mist.i.u-tokyo.ac.jp, dennywu@nyu.edu

###### Abstract

Transformers can efficiently learn in-context from example demonstrations. Most existing theoretical analyses studied the in-context learning (ICL) ability of transformers for linear function classes, where it is typically shown that the minimizer of the pretraining loss implements one gradient descent step on the least squares objective. However, this simplified linear setting arguably does not demonstrate the statistical efficiency of ICL, since the pretrained transformer does not outperform directly solving linear regression on the test prompt. In this paper, we study ICL of a nonlinear function class via transformer with nonlinear MLP layer: given a class of _single-index_ target functions \(f_{*}(\bm{x})=\sigma_{*}(\langle\bm{x},\bm{\beta}\rangle)\), where the index features \(\bm{\beta}\in\mathbb{R}^{d}\) are drawn from a \(r\)-dimensional subspace, we show that a nonlinear transformer optimized by gradient descent (with a pretraining sample complexity that depends on the _information exponent_ of the link functions \(\sigma_{*}\)) learns \(f_{*}\) in-context with a prompt length that only depends on the dimension of the distribution of target functions \(r\); in contrast, any algorithm that directly learns \(f_{*}\) on test prompt yields a statistical complexity that scales with the ambient dimension \(d\). Our result highlights the adaptivity of the pretrained transformer to low-dimensional structures of the function class, which enables sample-efficient ICL that outperforms estimators that only have access to the in-context data.

## 1 Introduction

Pretrained transformers [17] possess the remarkable ability of _in-context learning (ICL)_[2], whereby the model constructs a predictor from a prompt sequence consisting of pairs of labeled examples without updating any parameters. A common explanation is that the trained transformer can implement a learning algorithm, such as gradient descent on the in-context examples, in its forward pass [19, 18, 17]. Such explanation has been empirically studied in synthetic tasks such as linear regression [14, 15], which has motivated theoretical analyses of the statistical and computational complexity of ICL in learning simple function classes.

Many recent theoretical works on ICL focus on learning the function class of _linear models_ using linear transformers trained via empirical risk minimization. In this setting, it can be shown that minima of the pretraining loss implements one (preconditioned) gradient descent step on the least squares objective computed on the test prompt [16, 15, 18]. This implies that the forward pass of the pretrained transformer can learn linear targets with \(n\gtrsim d\) in-context examples, hence matching the sample complexity of linear regression on the test prompt. Subsequent works also studied how the distribution and "diversity" of pretrained tasks affect the ICL solution in similar problem settings [17, 18, 19, 20].

The motivation of our work is the observation that the simple setting of learning linear models with linear transformers does not fully capture the statistical efficiency and adaptivity of ICL. Specifically,

* A linear transformer has limited expressivity and hence only implements simple operations in the forward pass, such as one gradient step for least squares regression. This limits the class of algorithms that can be executed in-context, and consequently, the pretrained transformer cannot outperform directly solving linear regression on the test prompt. We therefore ask the question: _With the aid of MLP layer, can a pretrained transformer learn a nonlinear function class in-context, and outperform baseline algorithms that only have access to the test prompt?_
* A key feature of ICL is the _adaptivity_ to structure of the learning problem. For example, [13] empirically showed that transformers can match the performance of either ridge regression or LASSO, depending on parameter sparsity of the target class; [14] observed that transformers transitions from a weighted-sum estimator to ridge regression as the number of pretraining tasks increases. Such adaptivity enables the pretrained model to outperform algorithms that only have access to the test prompt, which cannot take into account the "prior" distribution of target functions. Hence a natural question to ask is _Can a pretrained transformer adapt to certain structures of the target function class, and how does such adaptivity contribute to the statistical efficiency of ICL?_

### Our Contributions

#### 1.1.1 Function Class: Gaussian Single-index Models

To address the above questions, we study the in-context learning of arguably the simplest _nonlinear_ extension of linear regression where a nonlinearity is applied to the response. Specifically, we consider the following class of _single-index_ target functions, where the \(t\)-th pretraining task is constructed as

\[\bm{x}_{1}^{t},\bm{x}_{2}^{t},\ldots,\bm{x}_{N}^{t},\bm{x}^{t}\overset{ \mathrm{i.i.d.}}{\sim}\mathcal{N}(0,\bm{I}_{d}),\quad y_{i}^{t}=\sigma_{*}^{t }((\bm{x}_{i}^{t},\bm{\beta}^{t}))+\varsigma_{i}^{t},\] (1.1)

where \(\sigma_{*}^{t}:\mathbb{R}\to\mathbb{R}\) is the unknown link function, and \(\bm{\beta}^{t}\in\mathbb{R}^{d}\) is the index feature vector which is drawn from some fixed \(r\)_-dimensional subspace_ for some \(r\ll d\). The task is to learn this input-output relation by reading the context \((\bm{x}_{1},y_{1},\ldots,\bm{x}_{N},y_{N})\) and predict the output corresponding to the query \(\bm{x}\) (See Section 2 for details). This problem setting is based on the following considerations.

* **Nonlinearity of function class.** Due to the nonlinear link function, single-index targets cannot be learned by linear transformers. For this class of functions, the statistical efficiency of simple algorithms has been extensively studied: given a link function with degree \(P\) and information exponent \(Q\) (defined as the index of the smallest non-zero coefficient in the Hermite expansion), we know that kernel methods require at least \(n\gtrsim d^{P}\) samples [15, 16], whereas two-layer neural network trained by gradient descent can achieve better sample complexity \(n\gtrsim d^{\Theta(Q)}\)[1, 1]. Hence these algorithms, if directly applied to the test prompt, require a context length _polynomial_ in the ambient dimensionality \(d\), which arguably deviates from practical settings of ICL where the pretrained transformer learns from a few in-context examples which may come from high dimensional data space. These algorithms serve as a baseline for comparing the statistical efficiency of ICL.
* **Two types of low-dimensional structures.** Prior works on gradient-based feature learning highlights the adaptivity of neural network to _low-dimensional functions_, i.e., single-index model \(\sigma_{*}(\langle\bm{x}_{i},\bm{\beta}\rangle)\) that depends on one direction (index features) \(\bm{\beta}\) in \(\mathbb{R}^{d}\)[1, 1]. In such setting, the complexity of gradient descent is dominated by the search for direction \(\bm{\beta}\in\mathbb{R}^{d}\), the difficulty of which can be characterized by various computational lower bounds [13, 1, 1]. Importantly, our pretraining problem setup introduces _another notion of low dimensionality_: the "distribution" of target functions is low-dimensional, since the index features for each task \(\bm{\beta}_{t}\) are drawn from a rank-\(r\) subspace. This low-dimensionality of _function class_ cannot be exploited by any algorithms that directly estimate the target function from test prompt, but as we will see, transformers can adapt to this additional structure via gradient-based pretraining, which reduces the search problem (for the index features \(\bm{\beta}\)) to \(r\)-dimensional in the in-context phase. Therefore, when \(r\ll d\), we expect pretrained transformers to outperform baseline algorithms on the in-context examples (kernel methods, neural network, etc.).

Empirical observations.We pretrain a GPT-2 model [14] (with the same configurations as the in-context linear regression setting in [12]) to learn the Gaussian single-index task (1.1) with degree-3 link function, and compare its in-context sample complexity against baseline algorithms (see Section 4 for details). In Figure 1 we observe that the pretrained transformer achieves low prediction risk using fewer in-context examples than two baseline algorithms: kernel ridge regression, and neural network trained by gradient descent. Moreover, we observe that unlike the baseline methods, _Transformer achieves an in-context sample complexity (almost) independent of the ambient dimensionality \(d\)._

The goal of this work is to rigorously establish a \(d\)-independent in-context sample complexity in an idealized theoretical setting for a shallow transformer optimized via gradient-based pretraining.

#### 1.1.2 Main Result: Learning Single-index Models In-Context

We characterize the sample complexity of learning (1.1) in-context, using a nonlinear transformer optimized by gradient descent. Each single-index task is specified by an unknown index feature vector \(\bm{\beta}\in\mathbb{R}^{d}\) drawn from some \(r\)-dimensional subspace, and a link function \(\sigma_{*}\) with degree \(P\) and information exponent \(Q\leq P\); we allow the degree and information exponent to vary across tasks, to model the scenario where the difficult of pretraining tasks may differ. We show that pretraining of the nonlinear MLP layer can extract the low-dimensional structure of the function class, and the attention layer efficiently approximates the nonlinear link function. Our main theorem upper bounds the in-context generalization error of the pretrained transformer.

**Theorem** (Informal).: _Let \(f:(\bm{x}_{1},y_{1},\ldots,\bm{x}_{N},y_{N},\bm{x})\mapsto y\) be a transformer with nonlinear MLP layer pretrained with gradient descent (Algorithm 1) on the single-index regression task (1.1). With probability at least 0.99, the model \(f\) achieves in-context prediction risk \(\mathbb{E}|f(\bm{x})-f_{*}(\bm{x})|-\tau=o_{d}(1)\), where \(\tau\) is the noise level, if the number of pretraining tasks \(T\), the number of training examples \(N\), the test prompt length \(N^{*}\), and the network width \(m\) satisfy (we omit polylogarithmic factors)_

\[\underbrace{T\gtrsim d^{\Theta(Q)}}_{\text{pretraining}},\underbrace{NT \gg T\gtrsim d^{\Theta(Q)}}_{\text{pretraining}},\underbrace{N^{*}\gtrsim r ^{\Theta(P)}}_{\text{inference}},\underbrace{m\gtrsim r^{\Theta(P)}}_{\text{ approximation}},\]

_where \(Q,P\) are the information exponent and the highest degree of link functions, respectively._

To the best of our knowledge, this is the first end-to-end optimization and statistical guarantee for in-context regression of this nonlinear function class. We make the following remarks.

* The required sample size for _pretraining_\(T,N\) scale with the ambient dimensionality \(d\). In particular, the number of pretraining tasks \(T\) is parallel to the complexity of learning a single-index model with information exponent \(Q\) using a two-layer neural network. On the other hand, the sample complexity for the _in-context_ phase only depends on the dimensionality of the function class \(r\ll d\).
* Note that any estimator that only has access to the in-context examples requires \(n\gtrsim d\) samples to learn the single-index model, as suggested by the information theoretic lower bound (e.g., see [15, 16]). Therefore, when \(r\ll d\), we see a separation between pretrained transformer and algorithms that directly learn from the test prompt, such as linear/kernel regression and neural network + gradient descent. This highlights the adaptivity (via pretraining) of transformers to low-dimensional structures of the target function class.
* Our analysis of pretraining reveals the following mechanism analogous to [15]: the nonlinear MLP layer extract useful features and adapt to the low-dimensionality of the function class, whereas the attention layer performs in-context function approximation on top of the learned features.

### Related Works

Theory of in-context learning.Many recent works studied the in-context learning ability of transformers trained by gradient descent. [11, 1, 12, 13] studied

Figure 1: In-context generalization error (with standard deviation) of kernel ridge regression, neural network + gradient descent, and pre-trained transformer. The target function is a polynomial single-index model. We fix \(r=8\) and vary \(d=16,32\).

the training of linear transformer models to learn linear target functions in-context by implementing one gradient descent step in the forward pass. Similar optimization results have been established for looped linear transformers [14], transformers with SoftMax attention [13, 15, 16] or nonlinear MLP layer [17, 18]. Our problem setting resembles [17], where a nonlinear MLP block is used to extract features, followed by a linear attention layer; the main difference is that we establish end-to-end guarantees on the optimization and sample complexity for learning a concrete nonlinear function class, whereas [17] focused on convergence of optimization. [18] showed that transformers can learn nonlinear functions in-context via a functional gradient mechanism, but no statistical and optimization guarantees were given. If we do not take gradient-based optimization into account, the function class that can be implemented in-context by transformers has been characterized in many prior works [14, 15, 16, 17, 18, 19, 20]. These results typically aim to encode specific algorithms (LASSO, gradient descent, etc.) in the forward pass or directly analyze the Bayes-optimal estimator.

Gradient-based learning of low-dimensional functions.The complexity of learning low-dimensional functions with neural network has been extensively studied in the feature learning theory literature. Typical target functions include single-index models [1, 13, 15, 16, 17, 18, 19] and multi-index models [20, 1, 16, 21]. While a shallow neural network can efficiently approximate such low-dimensional functions, the efficiency of gradient-based training is governed by properties of the nonlinearity \(\sigma^{*}\). In the single-index setting, prior works established a sufficient sample size \(n\gtrsim d^{\Theta(K)}\), where \(K\in\mathbb{N}\) is the _information exponent_ for algorithms utilizing correlational information [1, 1, 14, 15], or the _generative exponent_ for algorithms that employ suitable label transformations [15, 16, 17, 18, 19]. Moreover, \(n\asymp d\) samples are information theoretically necessary without additional structural assumptions; this entails that estimators that only access the test prompt inevitably pay a sample size that scales with the ambient dimensionality. As we will see, pretrained transformers can avoid this "curse of dimensionality" by exploiting the low-dimensionality of the task distribution. Low-dimensional structure of the _function class_ similar to our setting has been assumed to study the efficiency of transfer learning [14] and multi-task learning [15].

## 2 Problem Setting

Notations.\(\|\cdot\|\) denotes the \(\ell_{2}\) norm for vectors and the \(\ell_{2}\to\ell_{2}\) operator norm for matrices. For a vector \(\bm{w}\), we use \(\bm{w}_{a:b}\) for \(a\leq b\) to denote the vector \([w_{a},w_{a+1},\dots,w_{b}]^{\top}\). \(\bm{1}_{N}\) denotes the all-one vector of size \(N\). The indicator function of \(A\) is denoted by \(\mathbb{I}_{A}\). Let \(N\) be a nonnegative integer; then \([N]\) denotes the set \(\{n\in\mathbb{Z}\mid 1\leq n\leq N\}\). For a nonnegative integer \(i\), the \(i\)-th Hermite polynomial is defined as \(\mathrm{He}_{i}(z)=(-1)^{i}\mathrm{e}^{\frac{n^{2}}{2}}\frac{\mathrm{d}^{i}}{ \mathrm{d}z^{i}}\frac{-z^{2}}{2}\). For a set \(S\), \(\mathrm{Unif}(S)\) denotes the uniform distribution over \(S\). We denote the unit sphere \(\{\bm{x}\in\mathbb{R}^{d}\mid\|\bm{x}\|=1\}\) by \(\mathbb{S}^{d-1}\). \(\tilde{O}(\cdot),\tilde{\Omega}(\cdot)\) represent \(O(\cdot)\) and \(\Omega(\cdot)\) notations where polylogarithmic terms are hidden. We write \(a\lesssim b\) when there exists a constant \(c\) such that \(a\leq cb\). If both \(a\lesssim b\) and \(b\lesssim a\) holds, we write \(a\asymp b\).

### Data Generating Process

#### 2.1.1 In-context learning

We first introduce the basic setting of ICL [15] of simple function classes as investigated in [11, 16]. In each _task_, the learner is given a sequence of inputs and outputs \((\bm{x}_{1},y_{1},\dots,\bm{x}_{N},y_{N},\bm{x})\) referred to as _prompt_, where \(\bm{x}_{i},\bm{x}\in\mathbb{R}^{d}\) and \(y_{i}\in\mathbb{R}\). The labeled examples \(\bm{X}=(\bm{x}_{1}\quad\cdots\quad\bm{x}_{N})\in\mathbb{R}^{d\times N}\), \(\bm{y}=(y_{1}\quad\cdots\quad y_{N})^{\top}\in\mathbb{R}^{N}\) are called _context_, and \(\bm{x}\) is the _query_. Given input distribution \(\bm{x}_{1},\dots,\bm{x}_{N},\bm{x}\overset{\mathrm{i.i.d.}}{\sim}\mathcal{D}_{ \bm{x}}\), the output \(y_{i}\) is expressed as

\[y_{i}=f_{*}(\bm{x}_{i})+\varsigma_{i},\quad i\in[N],\]

where \(f_{*}\) is the true function describing the input-output relation and \(\varsigma_{i}\overset{\mathrm{i.i.d.}}{\sim}\mathcal{D}_{\varsigma}\) is i.i.d. label noise. Note that \(f_{*}\) also varies across tasks -- we assume \(f_{*}\) is drawn i.i.d. from some true distribution \(\mathcal{D}_{f_{*}}\). In the pretraining phase, we optimize the model parameters given training data from \(T\) distinct tasks \(\{(\bm{x}_{1}^{t},y_{1}^{t},\dots,\bm{x}_{M}^{t},y_{M}^{t},\bm{x}^{t},y^{t}) \}_{t=1}^{T}\), which is composed of prompts \(\{(\bm{x}_{1}^{t},y_{1}^{t},\dots,\bm{x}_{M}^{t},y_{M}^{t},\bm{x}^{t})\}_{t=1}^ {T}\) and responses \(\{y^{t}\}_{t=1}^{T}\) for queries \(\{\bm{x}^{t}\}_{t=1}^{T}\), where \(y^{t}=f_{*}^{t}(\bm{x}^{t})+\varsigma^{t}\) and \(\varsigma^{t}\overset{\mathrm{i.i.d.}}{\sim}\mathcal{D}_{\varsigma}\).

We say a model learns these functional relations _in-context_, when the model can predict the output \(f_{*}(\bm{x})\) corresponding to query \(\bm{x}\) by solely examining the context \((\bm{X},\bm{y})\), without updating model parameters for each task. Given the pretrained model \(f(\bm{X},\bm{y},\bm{x};\bm{\theta})\) with parameter \(\bm{\theta}\) which predicts the label of query \(\bm{x}\) from context \((\bm{X},\bm{y})\), we define the _expected ICL risk_ as

\[\mathcal{R}_{N^{*}}(f)\coloneqq\mathbb{E}[|f(\bm{X}_{1:N^{*}},\bm{y}_{1:N^{*} },\bm{x};\bm{\theta})-y|],\] (2.1)

where \(y=f_{*}(\bm{x})+\varsigma\) and the expectation is taken over the in-context data: \(\bm{x}_{1},\ldots,\bm{x}_{N^{*}},\bm{x}\sim\mathcal{D}_{\bm{x}},f_{*}\sim \mathcal{D}_{f_{*}},\varsigma_{1},\ldots,\varsigma_{N^{*}},\varsigma\sim \mathcal{D}_{\varsigma}\). Note that we take the expectation with respect to contexts \((\bm{X}_{1:N^{*}},\bm{y}_{1:N^{*}})\in\mathbb{R}^{d\times N^{*}}\times\mathbb{ R}^{N^{*}}\) of length \(N^{*}\), in order to examine the behavior of ICL at a specific context length.

#### 2.1.2 Gaussian single-index models

We consider the situation where the true input-output relation is expressed by _single-index models_, i.e., functions that only depend on the direction of the index vector \(\bm{\beta}\) in the input space. An efficient learning algorithm should adapt to this feature and identify the relevant subspace from high-dimensional observations; hence this problem setting has been extensively studied in the deep learning theory literature [1, 1, 2, 3, 10] to demonstrate the adaptivity of gradient-based feature learning.

**Assumption 1**.: _Let \(\tau\geq 0\) be the noise level. The prompt \((\bm{x}_{1},y_{1},\ldots,\bm{x}_{N},y_{N},\bm{x})\) is generated as_

\[\bm{x}_{1},\bm{x}_{2},\ldots,\bm{x}_{N},\bm{x}\overset{\mathrm{i.i.d.}}{ \sim}\mathcal{D}_{\bm{x}}=\mathcal{N}(0,\bm{I}_{d}),\quad y_{i}=f_{*}(\bm{x}_{ i})+\varsigma_{i},\quad f_{*}(\bm{x}_{i})=\sigma_{*}(\langle\bm{x}_{i},\bm{\beta} \rangle),\]

_where \(\varsigma_{1},\ldots,\varsigma_{N}\overset{\mathrm{i.i.d.}}{\sim}\mathrm{ Unif}(\{-\tau,\tau\})\), and the distribution \(\mathcal{D}_{f_{*}}\) of the true function \(f_{*}\) is specified as:_

1. _[leftmargin=*]_
2. _Index Features._ _Let_ \(\mathcal{S}\) _be an_ \(r\leq d\)_-dimensional linear subspace of_ \(\mathbb{R}^{d}\)_. We draw_ \(\bm{\beta}\) _uniformly from the unit sphere_ \(\mathbb{S}(\mathcal{S})\) _in_ \(\mathcal{S}\)_, i.e., from_ \(\mathbb{S}(\mathcal{S})\coloneqq\{\bm{\beta}\mid\bm{\beta}\in\mathcal{S},\| \bm{\beta}\|=1\}\)_._
3. _Link Function._ \(\sigma_{*}(z)=\sum_{i=Q}^{P}\frac{c_{i}}{i!}\mathrm{He}_{i}(z)\)_, where_ \(2\leq Q\leq P\)_. We draw the Hermite coefficients_ \(\{c_{i}\}_{i=Q}^{P}\) _from any distribution satisfying_ \[\mathbb{E}[c_{Q}^{2}]=\Theta_{d,r}(1)\neq 0,\sum_{i=Q}^{P}c_{i}^{2}\leq R_{c}^{2 }\;(a.s.)\;\text{ and }(c_{Q},\ldots,c_{P})\neq(0,\ldots,0)\;(a.s.).\] (2.2)

Throughout the paper, we assume that \(P\ll d,r\) and \(r\ll d\); specifically, we take \(P=\Theta_{d,r}(1)\) and \(r\lesssim d^{1/2}\). Note that the condition \(r\ll d\) entails that the class of target functions is _low-dimensional_, and as we will see, such structure can be adapted by the transformer via pretraining.

**Remark 1**.: _We make the following remarks on the assumption of single-index function class._

* _For each task, the target is a single-index model with different index features drawn from some rank-_ \(r\) _subspace, and different link function with degree at most_ \(P\) _and information exponent (defined as the index of the lowest degree non-zero coefficient in the Hermite expansion of the link function, i.e,_ \(\min\{i\mid c_{i}\neq 0\}\) _in this case; see_ _[_1, 10_]_ _at least_ \(Q\)_. This heterogeneity reflects the situation where the difficulty of learning the input-output relation varies across tasks. Note that we allow for different distributions of the Hermite coefficients_ \(\{c_{i}\}\)_: for example, we may set_ \((c_{Q},\ldots,c_{P})\sim\mathrm{Unif}\Big{\{}(c_{Q},\ldots c_{P})|\sum_{i=Q}^ {P}\frac{c_{i}^{2}}{i!}=1\Big{\}}\) _(manifold of coefficients satisfying_ \(\mathbb{E}_{\bm{x}}[f_{*}(\bm{x})]=1\)_),_ \(\mathrm{Unif}(\{0,1\}^{P-Q+1}\setminus(0,\ldots,0))\)_, or_ \(\mathrm{Unif}(\{(1,\ldots,0),\ldots,(0,\ldots,1)\})\)_._
* _The condition_ \(Q\geq 2\) _(i.e., the Hermite expansion of_ \(\sigma_{*}\) _does not contain constant and linear terms) ensures that the gradient update detects the entire_ \(r\)_-dimensional subspace instead of the trivial rank-1 component. For generic polynomial_ \(\sigma_{*}\)_, this assumption can be satisfied by a simple preprocessing step that subtracts the low-degree components, as done in_ _[_1_]__._

### Student Model: Transformer with Nonlinear MLP Layer

We consider a transformer composed of a single-layer self-attention module preceded by an embedding module using a nonlinear multi-layer perceptron (MLP). Let \(\bm{E}\in\mathbb{R}^{d_{e}\times d_{N}}\) be an embeddingmatrix constructed from prompt \((\bm{x}_{1},y_{1},\ldots,\bm{x}_{N},y_{N},\bm{x})\). A single-layer SoftMax self-attention module [13] is given as

\[f_{\mathrm{Attn}}(\bm{E};\bm{W}^{P},\bm{W}^{V},\bm{W}^{K},\bm{W}^{Q})=\bm{E}+\bm {W}^{P}\bm{W}^{V}\bm{E}\cdot\mathrm{softmax}\Big{(}\frac{(\bm{W}^{K}\bm{E})^{ \top}\bm{W}^{Q}\bm{E}}{\rho}\Big{)},\] (2.3)

where \(\rho\) is the temperature, and \(\bm{W}^{K},\bm{W}^{Q}\in\mathrm{R}^{d_{k}\times d_{e}}\), \(\bm{W}^{V}\in\mathrm{R}^{d_{v}\times d_{e}}\) and \(\bm{W}^{P}\in\mathrm{R}^{d_{e}\times d_{v}}\) are the key, query, value, and projection matrix, respectively. Following prior theoretical works [14, 13, 12], we remove the SoftMax and instead analyze the _linear_ attention with \(\rho=N\); it has been argued that such simplification can reproduce phenomena in practical transformer training [1]. We further simplify the original self-attention module (2.3) by merging \(\bm{W}^{P}\bm{W}^{V}\) as \(\bm{W}^{PV}\in\mathbb{R}^{d_{e}\times d_{e}}\) and \((\bm{W}^{K})^{\top}\bm{W}^{Q}\) as \(\bm{W}^{KQ}\in\mathbb{R}^{d_{e}\times d_{e}}\), and consider the following parameterization also introduced in [14, 1], WZC\({}^{+}\)23],

\[\bm{W}^{PV}=\begin{bmatrix}*&*\\ \bm{0}_{1\times(d_{e}-1)}&v\end{bmatrix},\bm{W}^{KQ}=\begin{bmatrix}\bm{K}&*\\ \bm{0}_{1\times(d_{e}-1)}&*\end{bmatrix},\] (2.4)

where \(v\in\mathbb{R}\) and \(\bm{K}\in\mathbb{R}^{(d_{e}-1)\times(d_{e}-1)}\). Then, the simplified attention module is written as \(\tilde{f}_{\mathrm{Attn}}(\bm{E};\bm{W}^{PV},\bm{W}^{KQ})=\bm{E}+\bm{W}^{PV} \bm{E}\Big{(}\frac{\bm{E}^{\top}\bm{W}^{KQ}\bm{E}}{N}\Big{)}\), and we take the right-bottom entry as the prediction of \(y\) corresponding to query \(\bm{x}\).

Prior analyses of linear transformers [14, 1] defined the embedding matrix \(\bm{E}\) simply as the input-output pairs; however, the combination of linear embedding and liner attention is not sufficient to learn nonlinear single-index models. Instead, we set \(d_{e}=m+1,d_{N}=N+1\) for \(m\in\mathbb{N}\) and construct \(\bm{E}\) using an MLP layer:

\[\bm{E}=\begin{bmatrix}\sigma(\bm{w}_{1}^{\top}\bm{x}_{1}+b_{1})&\cdots&\sigma( \bm{w}_{1}^{\top}\bm{x}_{N}+b_{1})&\sigma(\bm{w}_{1}^{\top}\bm{x}+b_{1})\\ \vdots&\ddots&\vdots&\vdots\\ \sigma(\bm{w}_{m}^{\top}\bm{x}_{1}+b_{m})&\cdots&\sigma(\bm{w}_{m}^{\top}\bm{ x}_{N}+b_{m})&\sigma(\bm{w}_{m}^{\top}\bm{x}+b_{m})\\ y_{1}&\cdots&y_{N}&0\end{bmatrix}\in\mathbb{R}^{(m+1)\times(N+1)},\] (2.5)

where \(\bm{w}_{1},\ldots,\bm{w}_{m}\in\mathbb{R}^{d}\) and \(b_{1},\ldots,b_{m}\in\mathbb{R}\) are trainable parameters and \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\) is a nonlinear activation function; we use \(\sigma(z)=\mathrm{ReLU}(z)=\max\{z,0\}\). This is to say, we define the embedding as the "hidden representation" \(\sigma(\bm{w}^{\top}\bm{x}+b)\) of a width-\(m\) two-layer neural network. For concise notation we write \(\bm{W}=(\bm{w}_{1},\ldots,\bm{w}_{m})^{\top},\bm{b}=(b_{1},\ldots,b_{m})^{\top}\).

**Remark 2**.: _We make the following remarks on the considered architecture._

* _The MLP embedding layer before attention has been adopted in recent works_ _[_1_, 12, 13, 14]__. This setting can be interpreted as an idealized version of the mechanism that lower layers of the transformer construct useful representation, on top of which upper attention layers implement the in-context learning algorithm._
* _Our architecture is also inspired by recent theoretical analyses of gradient-based feature learning, where it is shown that gradient descent on the MLP layer yields adaptivity to features of the target function and hence improved statistical efficiency_ _[_1_, 12, 13]__._

Combining the attention \(\tilde{f}_{\mathrm{Attn}}\) and MLP embedding (2.5), we can express the model prediction \(y\) as

\[f(\bm{X},\bm{y},\bm{x};\bm{W},\bm{\Gamma},\bm{b})=\Bigg{\langle}\frac{\bm{ \Gamma}\sigma(\bm{W}\bm{X}+\bm{b}\bm{1}_{N}^{\top})\bm{y}}{N},\begin{bmatrix} \sigma(\bm{w}_{1}^{\top}\bm{x}+b_{1})\\ \vdots\\ \sigma(\bm{w}_{m}^{\top}\bm{x}+b_{m})\end{bmatrix}\Bigg{\rangle},\] (2.6)

where \(\bm{\Gamma}=v\bm{K}^{\top}\) and \(\sigma(\bm{W}\bm{X}+\bm{b}\bm{1}_{N}^{\top})\) denotes the \(m\times N\) matrix whose \((i,j)\)-th entry is \(\sigma(\bm{w}_{i}^{\top}\bm{x}_{j}+b_{i})\); see Appendix E for full derivation. We refer to \(\bm{\Gamma}\) as the _attention matrix_.

### Pretraining: Empirical Risk Minimization via Gradient Descent

We pretrain the transformer (2.6) by the gradient-based learning algorithm specified in Algorithm 1, which is inspired by the layer-wise training procedure studied in the neural network theory literature [1, 1]. In particular, we update the trainable parameters in a sequential manner.

* In Stage I we optimize the parameters of the MLP (embedding) layer, which is a non-convex problem due to the nonlinear activation function. To circumvent the nonlinear training dynamics, we follow the recipe in recent theoretical analyses of gradient-based feature learning [1, 2, 2]; specifically, we zoom into the _early phase_ of optimization by taking one gradient step on on the regularized empirical risk. As we will see, the first gradient step already provides a reliable estimate of the target subspace \(\mathcal{S}\), which enables the subsequent attention layer to implement a sample-efficient in-context learning algorithm.
* In Stage II we train the attention layer, which is a convex problem and the global minimizer can be efficiently found. We show that the optimized attention matrix \(\boldsymbol{\Gamma}\) performs regression on the polynomial basis (defined by the MLP embedding), which can be seen as an in-context counterpart to the second-layer training (to learn the polynomial link) in [1, 1, 2].

## 3 Transformer Learns Single-index Models In-Context

### Main Theorem

Our main theorem characterizes the pretraining and in-context sample complexity of the transformer with MLP layer (2.6) optimized by layer-wise gradient-based pretraining outlined in Algorithm 1.

``` Input :Learning rate \(\eta_{1}\), weight decay \(\lambda_{1},\lambda_{2}\), prompt length \(N_{1},N_{2}\), number of tasks \(T_{1},T_{2}\), attention matrix initialization scale \(\gamma\).
1Initialize\(\boldsymbol{w}_{j}^{(0)}\sim\mathrm{Unif}(\mathbb{S}^{d-1})\) (\(j\in[m]\)); \(b_{j}^{(0)}\sim\mathrm{Unif}([-1,1])\) (\(j\in[m]\)); \(\Gamma_{j,j}^{(0)}\sim\mathrm{Unif}(\{\pm\gamma\})\) (\(j\in[m]\)) and \(\Gamma_{i,j}^{(0)}=0\) (\(i\neq j\in[m]\)).
2Stage I: Gradient descent for MLP layer
3 Draw data \(\{(\boldsymbol{x}_{1}^{t},y_{1}^{t},\ldots,\boldsymbol{x}_{N_{1}}^{t},y_{N_{1} }^{t},\boldsymbol{x}^{t},y^{t})\}_{t=1}^{T_{1}}\)with prompt length \(N_{1}\). \(\boldsymbol{w}_{j}^{(1)}\leftarrow\boldsymbol{w}_{j}^{(0)}-\eta_{1}\Big{[} \nabla_{\boldsymbol{w}_{j}^{(0)}}\frac{1}{T_{1}}\sum_{t=1}^{T_{1}}(y^{t}-f( \boldsymbol{X}^{t},\boldsymbol{y}^{t},\boldsymbol{x}^{t};\boldsymbol{W}^{(0)}, \boldsymbol{\Gamma}^{(0)},\boldsymbol{b}^{(0)}))^{2}+\lambda_{1}\boldsymbol{w} _{j}^{(0)}\Big{]}\).
4Initialize\(b_{j}\sim\mathrm{Unif}([-\log d,\log d])\).
5Stage II: Empirical risk minimization for attention layer
6 Draw data \(\{(\boldsymbol{x}_{1}^{t},y_{1}^{t},\ldots,\boldsymbol{x}_{N_{2}}^{t},y_{N_{2} }^{t},\boldsymbol{x}^{t},y^{t})\}_{t=T_{1}+1}^{T_{1}+T_{2}}\) with prompt length \(N_{2}\). \(\boldsymbol{\Gamma}^{*}\leftarrow\mathrm{argmin}_{\boldsymbol{\Gamma}}\frac{1 }{T_{2}}\sum_{t=T_{1}+1}^{T_{1}+T_{2}}(y^{t}-f(\boldsymbol{X}^{t},\boldsymbol{ y}^{t},\boldsymbol{x}^{t};\boldsymbol{W}^{(1)},\boldsymbol{\Gamma},\boldsymbol{b}))^{2}+ \frac{\lambda_{2}}{2}\|\boldsymbol{\Gamma}\|_{F}^{2}\). ```

**Output :**Prediction function \(\boldsymbol{x}\to f(\boldsymbol{X},\boldsymbol{y},\boldsymbol{x};\boldsymbol{W} ^{(1)},\boldsymbol{\Gamma}^{*},\boldsymbol{b})\).

**Algorithm 1**Gradient-based training of transformer with MLP layer

**Theorem 1**.: _Given Assumption 1 and \(r\lesssim d^{\frac{1}{2}}\). We pretrain the transformer (2.6) using Algorithm 1 with \(m=\tilde{\Omega}(r^{P}),T_{1}=\tilde{\Omega}(d^{Q+1}r^{Q})\), \(N_{1}T_{1}=\tilde{\Omega}(d^{2Q+1}r)\), \(\gamma\asymp\frac{1}{m^{\frac{3}{2}}r^{\frac{1}{2}}d^{Q}}\),\(\eta_{1}\asymp m^{\frac{3}{2}}rd^{2Q-\frac{1}{2}}\). \((\log d)^{-C_{\eta}}\) for constant \(C_{\eta}\). Then, for appropriately chosen regularization parameters \(\lambda_{1},\lambda_{2}>0\), with probability at least 0.99 over the data distribution and random initialization, the ICL prediction risk (2.1) with test prompt length \(N^{*}\) for the output \(f\) of Algorithm 1 can be upper bounded as_

\[\mathcal{R}_{N^{*}}(f)-\tau=\tilde{O}\Bigg{(}\sqrt{\frac{r^{3P}}{m}+r^{4P} \bigg{(}\frac{1}{T_{2}}+\frac{1}{N_{2}}+\frac{1}{N^{*}}\bigg{)}}\Bigg{)}.\]

Theorem 1 suggests that to achieve low in-context prediction risk, it is sufficient to set \(T_{1},N_{1}=\tilde{\Omega}(d^{\Theta(Q)})\), and \(m,T_{2},N_{2},N^{*}=\tilde{\Omega}(r^{\Theta(P)})\). Observe that the _pretraining_ complexity \(T_{1},N_{1}\) scales with the ambient dimensionality \(d\), but the _in-context_ sample complexity \(N^{*}\) only scales with the dimensionality of the function class \(r\ll d\). This illustrates the in-context efficiency of pretrained transformers and aligns with our observations in the GPT-2 experiment reported in Figure 1.

**Remark 3**.: _We make the following remarks._

* _The sample complexity highlights different roles of the two sources of low dimensionality in our setting. The low dimensionality of single-index_ \(f_{*}\) _entails that the pretraining cost scales as_ \(N\gtrsim d^{\Theta(Q)}\)_, which is consistent with prior analyses on gradient-based feature learning_ _[_2_,_ 2_]__. On the other hand, the low dimensionality of function class (i.e.,_ \(\boldsymbol{\beta}\) _lies in_ \(r\)_-dimensionalsubspace) leads to an in-context sample complexity that scales as \(N^{*}\gtrsim r^{\Theta(P)}\), which, roughly speaking, is the rate achieved by polynomial regression or kernel models on \(r\)-dimensional data._
* _The multiplicative scaling between_ \(N_{1}\) _and_ \(T_{1}\) _in the sample complexity suggests that one can tradeoff between the two quantities, that is, pretraining on more diverse tasks (larger_ \(T_{1}\)_) can reduce the required pretraining context length_ \(N_{1}\)_._
* _Similar low-dimensional function class has been considered in the setting of transfer or multi-task learning with two-layer neural networks_ _[_14, CHS\({}^{+}\)23_]__, where the first-layer weights identify the span of all target functions, and the second-layer approximates the nonlinearity. However, the crucial difference is that we do not update the network parameters based on the in-context examples; instead, the single-index learning is implemented by the forward pass of the transformer._

Comparison against baseline methods.Below we summarize the statistical complexity of commonly-used estimators that only have access to \(N^{*}\) in-context examples, but not the pretraining data. Note that the in-context sample complexity all depends on the ambient dimensionality \(d\gg r\).

* **Kernel models.** Recall that our target function is a degree-\(P\) polynomial, and hence kernel ridge regression requires \(N^{*}\gtrsim d^{P}\) in-context examples [11, 12].
* **CSQ learners.** The correlational statistical query (CSQ) lower bound suggests that an algorithm making use of correlational information requires \(N^{*}\gtrsim d^{\Theta(Q)}\) samples to learn a single-index model with information exponent \(Q\)[14, 1]. This sample complexity can be achieved by online SGD training of shallow neural network [1, 13].
* **Information theoretic limit.** Since the single-index model (1.1) contains \(d\) unknown parameters, we can infer an information theoretic lower bound of \(N^{*}\gtrsim d\) samples to estimate this function. This complexity can be achieved (up to polylog factors) by tailored SQ algorithms [13, 12] or modified gradient-base training of neural network [1, 15].

### Proof Sketch of Main Theorem

We provide a sketch of derivation for Theorem 1. The essential mechanism is outlined as follows: after training the MLP embedding via one gradient descent step, the MLP parameters \(\{\bm{w}_{j}^{(1)}\}\) align with the common subspace of the target functions \(\mathcal{S}\). Subsequently, the (linear) attention module estimates the input-output relation \(f_{\star}\) (which varies across tasks) on this \(r\)-dimensional subspace. We explain these two ingredients in the ensuing sections.

#### 3.2.1 Training the MLP Layer

We first show that the first gradient descent step on \(\bm{W}\) results in significant alignment with the target subspace \(\mathcal{S}\), using a proof strategy pioneered in [14]. Note that under sufficiently small initialization and appropriately chosen weight decay, we have

\[\bm{w}_{j}^{(1)}\simeq\eta_{1}\cdot\tfrac{2}{T_{1}}\sum_{t=1}^{T_{1}}y^{t} \nabla_{\bm{w}_{j}^{(0)}}f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(0)},\bm{ \Gamma}^{(0)},\bm{b}^{(0)}).\]

The key observation is that this correlation between \(y\) and the gradient of model output contains information of \(\mathcal{S}\). We establish the concentration of this empirical gradient around the expected (population) one, which determines the required rates of \(T_{1}\) and \(N_{1}\). For the population gradient, we make use of the Hermite expansion of \(\sigma_{b_{j}}(z)=\sigma(z+b_{j})\) and show that its leading term is proportional to \(\begin{bmatrix}\bm{w}_{1:r}^{(0)}\\ \bm{0}_{d-r}\end{bmatrix}\), which is contained in the target subspace \(\mathcal{S}\); see Appendix B for details.

#### 3.2.2 Attention Matrix with Good Approximation Property

Next we construct the attention matrix \(\bar{\bm{\Gamma}}\) which satisfies the following approximation property:

**Proposition 2** (Informal).: _There exists \(\bar{\bm{\Gamma}}\) such that with high probability,_

\[\left|f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(1)},\bar{\bm{\Gamma}},\bm{b })-y^{t}\right|-\tau=\tilde{O}\!\left(\sqrt{r^{3P}/m+r^{2P}/N_{2}}\right)\]

_for all \(t\in\{T_{1}+1,\ldots,T_{2}\}\). Moreover, we have \(\|\bar{\bm{\Gamma}}\|_{F}=\tilde{O}\!\left(r^{2P}/m\right)\)._We provide an intuitive explanation of this construction. Recall that the target function can be written in its Hermite expansion \(f_{*}(\bm{x})=\sum_{i=Q}^{P}\frac{c_{i}}{\mathrm{i}}\mathrm{Re}_{i}(\langle\bm{x},\bm{\beta}\rangle)\). We build an orthonormal basis for \(f_{*}\) as follows. Let \(\{\bm{\beta}_{1},\dots,\bm{\beta}_{r}\}\) be an orthonormal basis of \(\mathcal{S}\). Then, any \(f_{*}\) can be expressed as a linear combination of functions in \(\mathcal{H}=\{\prod_{j=1}^{r}\mathrm{He}_{p_{j}}\langle\langle\bm{\beta}_{j}, \cdot\rangle\rangle\mid Q\leq p_{1}+\dots+p_{r}\leq P,p_{1}\geq 0,\dots,p_{r} \geq 0\}\), where \(\mathbb{E}_{\bm{x}\sim\mathcal{N}(0,I_{d})}[h(\bm{x})h^{\prime}(\bm{x})]= \mathbb{I}_{h=h^{\prime}}\) holds for \(h,h^{\prime}\in\mathcal{H}\). We write \(\mathcal{H}=\{h_{1},\dots,h_{B_{P}}\}\) with \(B_{P}=|\mathcal{H}|=\Theta(r^{P})\), and observe that a two-layer neural network can be constructed to approximate each \(h_{n}\). Specifically, there exist vectors \(\bm{a}^{1},\dots,\bm{a}^{B_{P}}\in\mathbb{R}^{m}\) such that \(\sum_{j=1}^{m}a_{j}^{n}\sigma\big{(}\langle\bm{w}_{j}^{(1)},\bm{x}\rangle+b_{ j}\big{)}\simeq h_{n}(\bm{x})\) for each \(n\in[B_{P}]\).

Consequently, we can build the desired attention matrix using coefficients \(\bm{a}^{1},\dots,\bm{a}^{B_{P}}\). Let \(\bm{A}=\big{(}\bm{a}^{1}\quad\cdots\quad\bm{a}^{B_{P}}\big{)}\in\mathbb{R}^{m \times B_{P}}\) and \(\bar{\bm{\Gamma}}=\bm{A}\bm{A}^{\top}\), the attention module can recover the true function as

\[\Big{\langle}\tfrac{1}{N_{2}}\bar{\bm{\Gamma}}\sigma(\bm{W}^{(1)} \bm{X}^{t}+\bm{b}\bm{1}_{N_{2}}^{\top})\bm{y}^{t},\sigma(\bm{W}^{(1)}\bm{x}+ \bm{b})\Big{\rangle}\] \[=\sum_{n=1}^{B_{P}}\Big{(}\tfrac{1}{N_{2}}\sum_{i=1}^{N_{2}} \Bigl{(}\sum_{j=1}^{m}\bm{a}_{j}^{n}\sigma\big{(}\langle\bm{w}_{j}^{(1)},\bm{x }_{i}\rangle+b_{j}\big{)}\Bigr{)}y_{i}\Big{)}\Bigl{(}\sum_{j=1}^{m}\bm{a}_{j}^{ n}\sigma\big{(}\langle\bm{w}_{j}^{(1)},\bm{x}\rangle+b_{j}\big{)}\Bigr{)}\] \[\overset{\text{(a)}}{\simeq}\sum_{n=1}^{B_{P}}\Bigl{(}\tfrac{1}{N _{2}}\sum_{i=1}^{N_{2}}h_{n}(\bm{x}_{i})y_{i}\Bigr{)}h_{n}(\bm{x})\overset{\text {(b)}}{\simeq}\sum_{n=1}^{B_{P}}\mathbb{E}[h_{n}(\bm{x})f_{*}(\bm{x})]h_{n}( \bm{x})=f_{*}(\bm{x}).\] (3.1)

Roughly speaking, the self-attention architecture computes the correlation between the target function and basis element \(h_{i}\) to estimate the corresponding coefficient in this basis decomposition. We evaluate (a) the approximation error of two-layer neural network in Appendix C.1, and (b) the discrepancy between the empirical and true correlations in Appendix C.2.

Note that the approximation errors and the norm of \(\bar{\bm{\Gamma}}\) at this stage scale only with \(r\) up to polylogarithmic terms; this is because \(\bm{W}^{(1)}\) already identifies the low-dimensional target subspace \(\mathcal{S}\). Consequently, the in-context sample size \(N^{*}\) only scales with the target dimensionality \(r\ll d\).

#### 3.2.3 Generalization Error Analysis

Finally, we transfer the learning guarantee from the constructed \(\bar{\bm{\Gamma}}\) to the (regularized) empirical risk minimization solution \(\bm{\Gamma}^{*}\). By the equivalence between optimization with \(L_{2}\) regularization and norm-constrained optimization, there exists \(\lambda_{2}\) such that \(\|\bm{\Gamma}^{*}\|_{F}\leq\|\bar{\bm{\Gamma}}\|_{F}\) and the empirical ICL loss by \(\bm{\Gamma}^{*}\) is no larger than that of \(\bar{\bm{\Gamma}}\). Hence, we can bound the generalization error by \(\bm{\Gamma}^{*}\) using a standard Rademacher complexity bound for norm-constrained transformers provided in Appendix D.1. One caveat here is that the context length \(N^{*}\) at test time may differ from training time; hence we establish a context length-free generalization bound, which is discussed in Appendix D.2.

## 4 Synthetic Experiments

### Experimental Setting

We pretrain a GPT-2 model [14] to learn the Gaussian single-index function class (1.1). Specifically, we consider the 12-layer architecture (with 22.3M parameters) used in [13] for in-context linear regression. The pretraining data is generated from random single-index models: for each task \(t\), the context \(\{(\bm{x}_{i}^{t},y_{i}^{t})\}_{i=1}^{N}\) is generated as \(\bm{x}_{i}^{t}\overset{\text{i.i.d.}}{\sim}\mathcal{N}(0,\bm{I}_{d})\) and \(y_{i}^{t}=\sum_{i=Q}^{P}\frac{c_{i}^{t}}{\mathrm{i}}\mathrm{He}_{i}(\langle\bm{x }_{i}^{t},\bm{\beta}^{t}\rangle)\), where \((c_{Q}^{t},\dots c_{P})\overset{\text{i.i.d.}}{\sim}\mathrm{Unif}\Big{\{}(c_{Q},\dots c_{P})\big{|}\sum_{i=Q}^{P}\frac{c_{i}^{2}}{\mathrm{i}!}=1\Big{\}}\) and \(\bm{\beta}^{t}\overset{\text{i.i.d.}}{\sim}\mathrm{Unif}\big{\{}\bm{\beta}| \bm{\beta}=[\beta_{1},\dots,\beta_{r},0,\dots,0]^{\top}|,\|\bm{\beta}\|=1\big{\}}\). See Appendix F for further details.

### Empirical Findings

Ambient dimension-free sample complexity.In Figure 2 we examine how the in-context sample complexity of the GPT-2 model depends on the ambient dimensionality \(d\) and the function class dimensionality \(r\). For each problem setting the model is pretrained for \(100,000\) steps using the data of degree \(P=4\) and information exponent \(Q=2\) (see Appendix F for details). In Figure 2(a) we observe that for fixed \(r=8\), varying the ambient dimensionality \(d=16,32,64\) leads to negligible change in the model performance for the in-context phase. In contrast, Figure 2(b) illustrates that for fixed \(d\), the required sample size \(N^{*}\) scales with the dimensionality of the function class \(r=2,4,8\). This confirms our theoretical finding that transformers can adapt to low-dimensional structure of the distribution of target functions via gradient-based pretraining.

Superiority over baseline algorithms.In Figure 1 we compare the in-context sample complexity of the GPT-2 model pretrained by data of \(Q=3\) and \(P=2\) against two baseline algorithms that directly learn \(f_{*}\) on the test prompt: \((i)\) kernel ridge regression with the Gaussian RBF kernel \(k(\bm{x},\bm{x}^{\prime})=\exp\bigl{(}-\|\bm{x}-\bm{x}^{\prime}\|^{2}/\sigma^{2} \bigr{)}\), and \((ii)\) two-layer neural network with ReLU activation \(f_{\text{NN}}(\bm{x})=\frac{1}{m}\sum_{i=1}^{m}a_{i}\sigma(\langle\bm{x},\bm{ w}_{i}\rangle)\) trained by the Adam optimizer [1]. We observe that for \(r=8,d=16,32\), the pretrained transformer outperforms both KRR and two-layer NN; moreover, the performance of these two baseline algorithms deteriorates significantly as the ambient dimensionality \(d\) becomes larger.

## 5 Conclusion and Future Direction

We study the complexity of in-context learning for the Gaussian single-index models using a pre-trained transformer with nonlinear MLP layer. We provide an end-to-end analysis of gradient-based pretraining and establish a generalization error bound that takes into account the number of pretraining tasks, the number of pretraining and in-context examples, and the network width. Our analysis suggests that when the distribution of target functions exhibits low-dimensional structure, transformers can identify and adapt to such structure during pretraining, whereas any algorithm that only has access to the test prompt necessarily requires a larger sample complexity.

We outline a few limitations and possible future directions.

* The in-context sample complexity we derived \(r^{\Theta(P)}\) corresponds to that of polynomial regression or kernel methods in \(r\)-dimensional space. This rate is natural as discussed in Section 3.2.2, where the linear self-attention module extracts the coefficients with respect to fixed basis functions (of size \(r^{\Theta(P)}\)). An interesting question is whether transformers can implement a more efficient in-context algorithm that matches the complexity of gradient-based feature learning in \(r\) dimensions. This can be achieved if the pretrained model learns features in-context.
* Our pretraining complexity is based on one GD step analysis similar to [10], BES\({}^{+}\)[22] which makes use of the correlational information; hence the information exponent of the link functions plays an important role. We conjecture that the sample complexity can be improved if we modify the pretraining procedure or training objective, as done in [11, 1, 12, 13].

## Acknowledgement

KO was partially supported by JST, ACT-X Grant Number JPMJAX23C4. TS was partially supported by JSPS KAKENHI (24K02905) and JST CREST (JPMJCR2015). This research is unrelated to DW's work at xAI.

Figure 2: In-context sample complexity of GPT-2 model pretrained on Gaussian single-index function (see Section 4.1 for details) of degree-4 polynomial. Observe that \((a)\) the ICL risk curve overlaps for different ambient dimensions \(d\) but the same target (subspace) dimensionality \(r\), and \((b)\) the required sample size \(N^{*}\) becomes larger as \(r\) increases.

## References

* [AAM22] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In _Conference on Learning Theory_, pages 4782-4887. PMLR, 2022.
* [AAM23] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics. In _Conference on Learning Theory_, pages 2552-2623. PMLR, 2023.
* [ACDS23] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. _Advances in Neural Information Processing Systems_, 36, 2023.
* [ACS\({}^{+}\)23] Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra. Linear attention is (maybe) all you need (to understand transformer optimization). _arXiv preprint arXiv:2310.01082_, 2023.
* [ADK\({}^{+}\)24] Luca Arnaboldi, Yatin Dandi, Florent Krzakala, Luca Pesce, and Ludovic Stephan. Repetita ivuant: Data repetition allows sgd to learn high-dimensional multi-index functions. _arXiv preprint arXiv:2405.15459_, 2024.
* [ASA\({}^{+}\)22] Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. _arXiv preprint arXiv:2211.15661_, 2022.
* [BAGJ21] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient descent on non-convex losses from high-dimensional inference. _The Journal of Machine Learning Research_, 22(1):4788-4838, 2021.
* [BBPV23] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning Gaussian multi-index models with gradient flow. _arXiv preprint arXiv:2310.19793_, 2023.
* [BBSS22] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. _Advances in Neural Information Processing Systems_, 35, 2022.
* [BCW\({}^{+}\)23] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. _Advances in Neural Information Processing Systems_, 36, 2023.
* [BEG\({}^{+}\)22] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. _Advances in Neural Information Processing Systems_, 35, 2022.
* [BES\({}^{+}\)22] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensional asymptotics of feature learning: How one gradient step improves the representation. _Advances in Neural Information Processing Systems_, 35, 2022.
* [BES\({}^{+}\)23] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, and Denny Wu. Learning in the presence of low-dimensional structure: A spiked random matrix perspective. _Advances in Neural Information Processing Systems_, 36, 2023.
* [BKM\({}^{+}\)19] Jean Barbier, Florent Krzakala, Nicolas Macris, Leo Miolane, and Lenka Zdeborova. Optimal errors and phase transitions in high-dimensional generalized linear models. _Proceedings of the National Academy of Sciences_, 116(12):5451-5460, 2019.
* [BL19] Yu Bai and Jason D Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural networks. _arXiv preprint arXiv:1910.01619_, 2019.
* [BMR\({}^{+}\)20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in Neural Information Processing Systems_, 33, 2020.

* [BMZ23] Raphael Berthier, Andrea Montanari, and Kangjie Zhou. Learning time-scales in two-layers neural networks. _arXiv preprint arXiv:2303.00055_, 2023.
* [CCS23] Xiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient descent to learn non-linear functions in context. _arXiv preprint arXiv:2312.06528_, 2023.
* [CHS\({}^{+}\)23] Liam Collins, Hamed Hassani, Mahdi Soltanolkotabi, Aryan Mokhtari, and Sanjay Shakkottai. Provable multi-task representation learning by two-layer relu neural networks. _arXiv preprint arXiv:2307.06887_, 2023.
* [CM20] Sitan Chen and Raghu Meka. Learning polynomials in few relevant dimensions. In _Conference on Learning Theory_, pages 1161-1227. PMLR, 2020.
* [CSWY24a] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality. _arXiv preprint arXiv:2402.19442_, 2024.
* [CSWY24b] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Unveiling induction heads: Provable training dynamics and feature learning in transformers. _arXiv preprint arXiv:2409.10559_, 2024.
* [DH18] Rishabh Dudeja and Daniel Hsu. Learning single-index models in gaussian space. In _Conference On Learning Theory_, pages 1887-1930. PMLR, 2018.
* [DLS22] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In _Conference on Learning Theory_, pages 5413-5452. PMLR, 2022.
* [DNGL23] Alex Damian, Eshaan Nichani, Rong Ge, and Jason D. Lee. Smoothing the landscape boosts the signal for SGD: Optimal sample complexity for learning single index models. _Advances in Neural Information Processing Systems_, 36, 2023.
* [DPVLB24] Alex Damian, Loucas Pillaud-Vivien, Jason D Lee, and Joan Bruna. The computational complexity of learning gaussian single-index models. _arXiv preprint arXiv:2403.05529_, 2024.
* [DSD\({}^{+}\)22] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. _arXiv preprint arXiv:2212.10559_, 2022.
* [DTA\({}^{+}\)24] Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborova, and Florent Krzakala. The benefits of reusing batches for gradient descent in two-layer networks: Breaking the curse of information and leap exponents. _arXiv preprint arXiv:2402.03220_, 2024.
* [DWY21] Konstantin Donhauser, Mingqi Wu, and Fanny Yang. How rotational invariance of common kernels prevents generalization in high dimensions. In _International Conference on Machine Learning_, pages 2804-2814. PMLR, 2021.
* [GHM\({}^{+}\)23] Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai. How do transformers learn in-context beyond simple functions? a case study on learning with representations. _arXiv preprint arXiv:2310.10616_, 2023.
* [GMMM21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers neural networks in high dimension. _The Annals of Statistics_, 49(2):1029-1054, 2021.
* [GSR\({}^{+}\)24] Khashayar Gatmiry, Nikunj Saunshi, Sashank J Reddi, Stefanie Jegelka, and Sanjiv Kumar. Can looped transformers learn to implement multi-step gradient descent for in-context learning? In _International Conference on Machine Learning_, 2024.
* 22, 2021.
* [GTLV22] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. _Advances in Neural Information Processing Systems_, 35, 2022.
* [HCL23] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. _arXiv preprint arXiv:2310.05249_, 2023.
* [HvMM\({}^{+}\)19] Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Risks from learned optimization in advanced machine learning systems. _arXiv preprint arXiv:1906.01820_, 2019.
* [JLLVR24] Hong Jun Jeon, Jason D Lee, Qi Lei, and Benjamin Van Roy. An information-theoretic analysis of in-context learning. _arXiv preprint arXiv:2401.15530_, 2024.
* [JMS24] Nirmit Joshi, Theodor Misiakiewicz, and Nathan Srebro. On the complexity of learning sparse functions with statistical and gradient queries. _arXiv preprint arXiv:2407.05622_, 2024.
* [KB15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations_, 2015.
* [KNS24] Juno Kim, Tai Nakamaki, and Taiji Suzuki. Transformers are minimax optimal nonparametric in-context learners. _arXiv preprint arXiv:2408.12186_, 2024.
* [KS24] Juno Kim and Taiji Suzuki. Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape. _arXiv preprint arXiv:2402.01258_, 2024.
* [LLZV\({}^{+}\)24] Yue M Lu, Mary I Letey, Jacob A Zavatone-Veth, Anindita Maiti, and Cengiz Pehlevan. Asymptotic theory of in-context learning by linear attention. _arXiv preprint arXiv:2405.11751_, 2024.
* [LOSW24] Jason D. Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu. Neural network learns low-dimensional polynomials with sgd near the information-theoretic limit. _arXiv preprint arXiv:2406.01581_, 2024.
* [LWL\({}^{+}\)24] Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, and Pin-Yu Chen. How do nonlinear transformers learn and generalize in in-context learning? In _International Conference on Machine Learning_, 2024.
* [Mau16] Andreas Maurer. A vector-contraction inequality for rademacher complexities. In _Algorithmic Learning Theory: 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016, Proceedings 27_, pages 3-17. Springer, 2016.
* [MHM23] Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. _arXiv preprint arXiv:2307.03576_, 2023.
* [MHPG\({}^{+}\)23] Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat A Erdogdu. Neural networks efficiently learn low-dimensional representations with SGD. In _International Conference on Learning Representations_, 2023.
* [MHWSE23] Alireza Mousavi-Hosseini, Denny Wu, Taiji Suzuki, and Murat A Erdogdu. Gradient-based feature learning under structured data. _Advances in Neural Information Processing Systems_, 36, 2023.
* [MM18] Marco Mondelli and Andrea Montanari. Fundamental limits of weak recovery with applications to phase retrieval. In _Conference On Learning Theory_, pages 1445-1450. PMLR, 2018.
* [MZD\({}^{+}\)23] Arvind Mahankali, Haochen Zhang, Kefan Dong, Margalit Glasgow, and Tengyu Ma. Beyond ntk with vanilla gradient descent: A mean-field analysis of neural networks with polynomial width, samples, and time. _Advances in Neural Information Processing Systems_, 36, 2023.

* [NDL24] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent. _arXiv preprint arXiv:2402.14735_, 2024.
* [OSSW24] Kazusato Oko, Yujin Song, Taiji Suzuki, and Denny Wu. Learning sum of diverse features: computational hardness and efficient gradient-based training for ridge combinations. _arXiv preprint arXiv:2406.11828_, 2024.
* [RPCG23] Allan Raventos, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. _Advances in Neural Information Processing Systems_, 36, 2023.
* [RWC\({}^{+}\)19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
* [SGS\({}^{+}\)24] Michael E Sander, Raja Giryes, Taiji Suzuki, Mathieu Blondel, and Gabriel Peyre. How do transformers perform in-context autoregressive learning? _arXiv preprint arXiv:2402.05787_, 2024.
* [Ver18] Roman Vershynin. _High-Dimensional Probability: An Introduction with Applications in Data Science_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
* [VONR\({}^{+}\)23] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, pages 35151-35174. PMLR, 2023.
* [VSP\({}^{+}\)17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [Wai19] Martin J Wainwright. _High-Dimensional Statistics: A Non-Asymptotic Viewpoint_. Cambridge University Press, 2019.
* [WZC\({}^{+}\)23] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? _arXiv preprint arXiv:2310.08391_, 2023.
* [ZFB23] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. _arXiv preprint arXiv:2306.09927_, 2023.
* [ZWB24] Ruiqi Zhang, Jingfeng Wu, and Peter L Bartlett. In-context learning of a linear transformer block: Benefits of the mlp component and one-step gd initialization. _arXiv preprint arXiv:2402.14951_, 2024.
* [ZZYW23] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. _arXiv preprint arXiv:2305.19420_, 2023.

## Table of Contents

* 1 Introduction
	* 1.1 Our Contributions
	* 1.2 Related Works
* 2 Problem Setting
	* 2.1 Data Generating Process
	* 2.2 Student Model: Transformer with Nonlinear MLP Layer
	* 2.3 Pretraining: Empirical Risk Minimization via Gradient Descent
* 3 Transformer Learns Single-index Models In-Context
	* 3.1 Main Theorem
	* 3.2 Proof Sketch of Main Theorem
* 4 Synthetic Experiments
	* 4.1 Experimental Setting
	* 4.2 Empirical Findings
* 5 Conclusion and Future Direction
* A Preliminaries
* A.1 Definition of High Probability Event
* A.2 Tensor Notations
* A.3 Hermite Polynomials
* A.4 Other Auxiliary Lemmas
* B Proofs for the MLP Layer
* B.1 Calculation of Population Gradient
* B.2 Concentration of Empirical Gradient
* C Construction of Attention Matrix
* C.1 Approximation Error of Two-layer Neural Network
* C.2 Concentration of Correlation
* C.3 Proof of Proposition 2
* D Generalization Error Analysis and Proof of Theorem 1
* D.1 Rademacher Complexity Bound
* D.2 Prompt Length-free Generalization Bound
* D.3 Proof of Theorem 1
* E Derivation of Simplified Self-attention Module
* F Details of Experiments
* F.1 Detailed Experimental Settings
* F.2 Experiment on Our Architecture and Algorithm 1
Preliminaries

We consider the high-dimensional setting, i.e., our result holds for all \(d\geq D\) where \(D\) is a constant which does not depend on \(d\) and \(r\). Throughout the proofs, we take \(\mathcal{S}=\{(x_{1},\ldots,x_{r},0,\ldots,0)^{\top}\mid x_{1},\ldots,x_{r}\in \mathbb{R}\}\) and \(\bm{\beta}\sim\mathrm{Unif}(\mathbb{S}(\mathcal{S}))=\mathrm{Unif}(\{(\beta_{1 },\ldots,\beta_{r},0,\ldots,0)^{\top}\mid\beta_{1}^{2}+\cdots+\beta_{r}^{2}=1\})\). Note that this assumption is without loss of generality by the rotational invariance of the Gaussian distribution.

### Definition of High Probability Event

**Definition 3**.: _We say that an event \(A\) occurs with high probability when there exists a sufficiently large constant \(C^{*}\) which does not depend on \(d,r\) and_

\[1-Pr[A]\leq O(d^{-C^{*}})\]

_holds._

We implicitly assume that we can redefine the constant \(C^{*}\) to be sufficiently large as needed. The lemma below is a basic example and will be used in the proofs:

**Lemma 4**.: _Let \(x\sim\mathcal{N}(0,1)\). Then, \(|x|\lesssim\sqrt{\log d}\) holds with high probability._

Proof.: From the tail bound of Gaussian, \(\mathbb{P}(|x|\geq t)\leq 2\exp{(-t^{2}/2)}\) holds. Hence we get \(\mathbb{P}(|x|\geq\sqrt{2C^{*}\log d})\leq O(d^{-C^{*}})\). 

Note that \(C^{*}\) can be redefined by changing the hidden constant in \(|x|\lesssim\sqrt{\log d}\).

If \(A_{1},\ldots,A_{M}\) occurs with high probability where \(M=O(\mathrm{poly}(d))\), then \(A_{1}\cap\cdots\cap A_{M}\) also occurs with high probability (by redefining \(C^{*}\)). In particular, throughout this paper we assume that \(m,N_{1},N_{2},T_{1},T_{2}=O(\mathrm{poly}(d))\), which allows us to take such unions.

### Tensor Notations

In this paper, a \(k\)-tensor is a multidimensional array which has \(k\) indices: for example, matrices are \(2\)-tensors. Let \(\bm{A}\) be a \(k\)-tensor. \(A_{i_{1},\ldots,i_{k}}\) denotes \((i_{1},\ldots,i_{k})\)-th entry of \(\bm{A}\). Let \(\bm{A}\) be a \(k\)-tensor and \(\bm{B}\) be a \(l\)-tensor where \(k\geq l\). \(\bm{A}(\bm{B})\) denotes a \(k-l\) tensor whose \((i_{1},\ldots,i_{k-l})\)-th entry is

\[\bm{A}(\bm{B})_{i_{1},\ldots,i_{k-l}}=\sum_{j_{1},\ldots,j_{l}}A_{i_{1}, \ldots,i_{k-l},j_{1},\ldots,j_{l}}B_{j_{1},\ldots,j_{l}},\]

and is defined only when sizes are compatible. If \(k=l\), we sometimes write \(\bm{A}(\bm{B})\) as \(\bm{A}\circ\bm{B}\) or \(\langle\bm{A},\bm{B}\rangle\). Let \(\bm{v}\in\mathbb{R}^{d}\) be a vector and \(k\) be a positive integer. Then, \(\bm{v}^{\otimes k}\in\mathbb{R}^{d\times\cdots\times d}\) denotes a \(k\)-tensor whose \((i_{1},\ldots,i_{k})\)-th entry is \(v_{i_{1}}\cdots v_{i_{k}}\).

Let \(f(\bm{x}):\mathbb{R}^{d}\to\mathbb{R}\) be a \(d\)-variable differentiable function. A \(k\)-tensor \(\nabla^{k}f(\bm{x})\) is defined as

\[\big{(}\nabla^{k}f(\bm{x})\big{)}_{i_{1},\ldots,i_{k}}=\frac{\partial}{ \partial x_{i_{1}}}\cdots\frac{\partial}{\partial x_{i_{k}}}f(\bm{x}).\]

The following properties can be verified easily.

**Lemma 5**.: _For tensors \(\bm{A}\) and \(\bm{B}\), \(\|\bm{A}(\bm{B})\|_{F}^{2}\leq\|\bm{A}\|_{F}^{2}\|\bm{B}\|_{F}^{2}\) holds._

**Lemma 6**.: _For a vector \(\bm{v}\), \(\|\bm{v}^{\otimes k}\|_{F}^{2}=(\|\bm{v}\|_{2}^{2})^{k}\) holds._

### Hermite Polynomials

We frequently use (probablists') Hermite polynomials, which is defined as

\[\mathrm{He}_{i}(z)=(-1)^{i}\mathrm{e}^{\frac{z^{2}}{2}}\frac{\mathrm{d}^{i}}{ \mathrm{d}z^{i}}\mathrm{e}^{\frac{-z^{2}}{2}},\]

where \(i\) is a nonnegative integer. We often make use of the orthogonality property: \(\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\mathrm{He}_{i}(z)\mathrm{He}_{j}(z)]=i! \delta_{i,j}\). The Hermite expansion for \(\sigma:\mathbb{R}\to\mathbb{R}\) is defined as \(\sigma(z)=\sum_{i\geq 0}\frac{a_{i}}{i!}\mathrm{He}_{i}(z)\) where \(a_{i}=\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\sigma(z)\mathrm{He}_{i}(z)]\). Similarly, the multivariate Hermiteexpansion for \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\) is defined as \(f(\bm{z})=\sum_{i_{1}\geq 0,\ldots,i_{d}\geq 0}\frac{a_{i_{1},\ldots,i_{d}} }{(i_{1})\cdots(i_{d})!}\mathrm{He}_{i_{1}}(z_{1})\cdots\mathrm{He}_{i_{d}}(z_{ d})\), where \(a_{i_{1},\ldots,i_{d}}=\mathbb{E}_{z_{1},\ldots,z_{d}\sim\mathcal{N}(0,1)}[f(\bm{z}) \mathrm{He}_{i_{1}}(z_{1})\cdots\mathrm{He}_{i_{d}}(z_{d})]\). The coefficient \(a_{i_{1},\ldots,i_{d}}\) can also be obtained by \(a_{i_{1},\ldots,i_{d}}=\mathbb{E}_{z_{1},\ldots,z_{d}\sim\mathcal{N}(0,1)} \bigg{[}\frac{\partial^{i_{1}}}{\partial z_{1}^{i_{1}}}\cdots\frac{\partial^{ i_{d}}}{\partial z_{d}^{i_{d}}}f(\bm{z})\bigg{]}\).

Consider \(f_{*}\) in our problem setting (Assumption 1). We can bound \(\mathbb{E}[f_{*}(\bm{x})^{2}]\) and \(\mathbb{E}[f_{*}(\bm{x})^{4}]\) as follows.

**Lemma 7**.: _Under Assumption 1, \(\mathbb{E}_{\bm{x},f_{*}}[f_{*}(\bm{x})^{2}]=\Theta_{d,r}(1)\) and \(\mathbb{E}_{\bm{x},f_{*}}[f_{*}(\bm{x})^{4}]=O_{d,r}(1)\) holds._

Proof.: From (2.2) and \(P=\Theta_{d,r}(1)\),

\[\mathbb{E}_{\bm{x}\sim\mathcal{N}(0,\bm{I}_{d}),f_{*}\sim\mathcal{ D}_{f_{*}}}[f_{*}(\bm{x})^{2}] =\mathbb{E}_{\bm{z}\sim\mathcal{N}(0,1),\{c_{i}\}}\left[\left( \sum_{i=Q}^{P}\frac{c_{i}}{i!}\mathrm{He}_{i}(z)\right)^{2}\right]\] \[=\mathbb{E}\left[\sum_{i=Q}^{P}\frac{c_{i}^{2}}{i!}\right]\] \[=\Theta_{d,r}(1).\]

We can bound \(\mathbb{E}_{\bm{x}}[f_{*}(\bm{x})^{4}]=\mathbb{E}_{z\sim\mathcal{N}(0,1),\{c_ {i}\}}\bigg{[}\left(\sum_{i=Q}^{P}\frac{c_{i}}{i!}\mathrm{He}_{i}(z)\right)^{ 4}\bigg{]}\) naively as follows: let \(\zeta_{P}^{4}=\max_{Q\leq i\leq P}\mathbb{E}_{z\sim N(0,1)}[\mathrm{He}_{i}(z )^{4}]\). \(\zeta_{P}\) is an \(O(1)\) quantity depending only on \(P\) and \(Q\). Then,

\[\mathbb{E}_{z\sim\mathcal{N}(0,1),\{c_{i}\}}\left[\left(\sum_{i=Q }^{P}\frac{c_{i}}{i!}\mathrm{He}_{i}(z)\right)^{4}\right]\] \[\leq \sum_{Q\leq i,j,k,l\leq P}\frac{\mathbb{E}[|c_{i}c_{j}c_{k}c_{l}| ]}{i!j!k!l!}\mathbb{E}[|\mathrm{He}_{i}(z)\mathrm{He}_{j}(z)\mathrm{He}_{k}(z )\mathrm{He}_{l}(z)|]\] \[\leq \sum_{Q\leq i,j,k,l\leq P}R_{c}^{4}\mathbb{E}_{z}[\mathrm{He}_{i} (z)^{4}]^{1/4}\mathbb{E}_{z}[\mathrm{He}_{j}(z)^{4}]^{1/4}\mathbb{E}_{z}[ \mathrm{He}_{k}(z)^{4}]^{1/4}\mathbb{E}_{z}[\mathrm{He}_{l}(z)^{4}]^{1/4}\] \[\leq P^{4}R_{c}^{4}\zeta_{P}^{4}.\]

The lemma below is useful to find a basis of the set of single-index functions.

**Lemma 8**.: _Suppose \(\bm{\beta}\in\mathbb{S}(\mathcal{S})\). Then,_

\[\mathrm{He}_{p}(\langle\bm{x},\bm{\beta}\rangle)=\sum_{p_{1}\geq 0,\ldots,p_{r} \geq 0}^{p_{1}+\cdots+p_{r}=p}\frac{p!}{p_{1}!\cdots p_{r}!}\cdot\beta_{1}^{p_{1 }}\cdots\beta_{r}^{p_{r}}\cdot\mathrm{He}_{p_{1}}(x_{1})\cdots\mathrm{He}_{p_{ r}}(x_{r})\]

_holds._

Proof.: Note that \(\mathbb{E}_{x_{1},\ldots,x_{r}\sim\mathcal{N}(0,1)}\bigg{[}\frac{\partial^{i_ {1}}}{\partial x_{1}^{i_{1}}}\cdots\frac{\partial^{i_{r}}}{\partial x_{r}^{i_{ r}^{\prime}}}\mathrm{He}_{p}(\langle\bm{x},\bm{\beta}\rangle)\bigg{]}\) is nonzero only when \(i_{1}+\cdots+i_{r}=p\), because \(\frac{\partial^{i_{1}}}{\partial x_{1}^{i_{1}}}\cdots\frac{\partial^{i_{r}}}{ \partial x_{r}^{i_{r}}}\mathrm{He}_{p}(\langle\bm{x},\bm{\beta}\rangle)=p(p-1) \cdots(p-(i_{1}+\cdots+i_{r})+1)\beta_{1}^{i_{1}}\cdots\beta_{r}^{i_{r}} \mathrm{He}_{p-(i_{1}+\cdots+i_{r})}(\langle\bm{x},\bm{\beta}\rangle)\) and \(\mathbb{E}_{z_{1},\ldots,x_{r}\sim\mathcal{N}(0,1)}[\mathrm{He}_{q}(\langle \bm{x},\bm{\beta}\rangle)]=\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\mathrm{He}_{q}( z)]=0\) if \(q>0\). When \(i_{1}+\cdots+i_{r}=p\), then \(\mathbb{E}_{x_{1},\ldots,x_{r}\sim\mathcal{N}(0,1)}\bigg{[}\frac{\partial^{i_ {1}}}{\partial x_{1}^{i_{1}}}\cdots\frac{\partial^{i_{r}}}{\partial x_{r}^{i_{r} }}\mathrm{He}_{p}(\langle\bm{x},\bm{\beta}\rangle)\bigg{]}=p!\beta_{1}^{i_{1}} \cdots\beta_{r}^{i_{r}}\) holds. Thus, from the multivariate Hermite expansion we obtain the claim. 

**Corollary 9**.: _Let \(f_{*}(\bm{x})=\sum_{i=Q}^{P}\frac{c_{i}}{i!}\mathrm{He}_{i}(\langle\bm{x},\bm{ \beta}\rangle)\). Then, \(\mathbb{E}_{\bm{x}}[\nabla^{k}f_{*}(\bm{x})]=c_{k}\bm{\beta}^{\otimes k}\) if \(Q\leq k\leq P\) and otherwise it is the zero tensor._

### Other Auxiliary Lemmas

Lemmas on random vectors.Let \(\chi(d)\) be the distribution of \(\sqrt{z_{1}^{2}+\cdots+z_{d}^{2}}\) where \(z_{i}\stackrel{{\rm i.i.d.}}{{\sim}}\mathcal{N}(0,1)\). Note that the Gaussian vector \(\bm{v}\sim\mathcal{N}(0,I_{d})\) can be decomposed as \(\bm{v}=z\bm{w}\) where \(z\sim\chi(d)\) and \(\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})\), because the norm and the direction of the Gaussian vector are independent of each other.

**Lemma 10** (Example 2.11 in [11]).: _For \(z\sim\chi(d)\) and \(0<t<1\),_

\[\mathbb{P}[|z^{2}-d|\geq dt]\leq 2\exp(-dt^{2}/8)\]

_holds._

**Lemma 11**.: _Let \(C>0\). For \(\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})\), \(\|\bm{w}_{1:r}\|^{2}\leq\frac{4Cr}{d}\log d\) holds with probability at least \(1-2\exp(-d/32)-2rd^{-C}\) (then with high probability)._

Proof.: Let \(z^{2}\sim\chi^{2}(d)\) be independent from \(\bm{w}\). We observe that the distribution of \(z^{2}\|\bm{w}_{1:r}\|^{2}\) is the \(\chi^{2}(r)\)-distribution. First, from Lemma 10, with probability at least \(1-2\exp(-d/32)\), \(z^{2}\geq d/2\) holds. Moreover, we can say that \((z^{\prime})^{2}\sim\chi^{2}(r)\) satisfies \((z^{\prime})^{2}\leq 2Cr\log d\) with high probability for sufficiently large constant: let us decompose \((z^{\prime})^{2}=v_{1}^{2}+\cdots+v_{r}^{2}\) where \(v_{i}\sim\mathcal{N}(0,1)\); from Lemma 4, \(v_{i}^{2}\leq 2C\log d\) holds with probability at least \(1-2d^{-C}\), and thus \((z^{\prime})^{2}\leq 2Cr\log d\) holds with probability at least \(1-2rd^{-C}\). Taking the union bound yields the result. 

**Lemma 12**.: _For \(k\geq 1\) and \(r\leq d\),_

\[\mathbb{E}_{\bm{w}\sim\mathbb{S}^{d-1}}[\|\bm{w}_{1:r}\|^{2k}]=O_{d,r}\bigg{(} \bigg{(}\frac{r}{d}\bigg{)}^{k}\bigg{)}\]

_holds._

Proof.: Since \(\|\bm{w}_{1:r}\|^{2}\) is a polynomial of \(\bm{w}\), Lemma 24 in [10] yields \(\mathbb{E}_{\bm{w}\sim\mathbb{S}^{d-1}}[\|\bm{w}_{1:r}\|^{2k}]\lesssim\mathbb{ E}_{\bm{w}\sim\mathbb{S}^{d-1}}[\|\bm{w}_{1:r}\|^{4}]^{k/2}\). Here, \(\mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[\|\bm{w}_{1:r}\|^{4}]= \frac{\mathbb{E}_{\bm{v}_{i}\sim\mathcal{N}(0,1)}[(v_{1}^{2}+\cdots+v_{r}^{2}) ^{2}]}{\mathbb{E}_{\bm{v}}\sim\chi^{4}(d)}=\frac{3r+r(r-1)}{d(d+2)}=\Theta(r^{ 2}/d^{2})\) yields the result. 

**Lemma 13** (Sub Gaussian vector).: _Let \(\bm{x}\) be a \(d\)-dimensional random vector and suppose that \(\langle\bm{u},\bm{x}\rangle\) is zero-mean \(\sigma^{2}\)-sub Gaussian for any \(\bm{u}\in\mathbb{R}^{d}\) such that \(\|\bm{u}\|=1\). Then, \(\|\bm{x}\|=\tilde{O}(\sigma\sqrt{d})\) holds with high probability._

**Lemma 14**.: _Let \(X\) be a \(\sigma^{2}\)- sub Gaussian random variable and \(Y\) be a bounded random variable satisfying \(|Y|\leq R\) almost surely. Then, \(XY-\mathbb{E}[XY]\) is \(O(\sigma^{2}R^{2})\)-sub Gaussian._

Proof.: \(XY\) is \(\sigma^{2}R^{2}\)-Gaussian by examining the tail probability. Hence, \(XY-\mathbb{E}[XY]\) is \(O(\sigma^{2}R^{2})\)-sub Gaussian from Lemma 2.6.8 in [10]. 

Polynomial concentration.We frequently use the following bounds on an output data \(y\) (which is a polynomial of \(\bm{x}\)).

**Lemma 15** (Theorem 1.5 in [10]).: _Let \(\bm{x}\sim\mathcal{N}(0,\bm{I}_{n})\) and \(f\) be a degree-\(D\)\(n\)-variable polynomial function. Then, for all \(t\geq 0\) it holds that_

\[\mathbb{P}(|f(\bm{x})-\mathbb{E}_{\bm{x}}[f(\bm{x})]|\geq t)\leq 2\exp\Biggl{(} -\frac{1}{C_{D}M^{2}}\min_{1\leq k\leq D}\biggl{(}\frac{t}{\|\mathbb{E}_{\bm{ x}}[\nabla^{k}f(\bm{x})]\|_{F}}\biggr{)}^{2/k}\Biggr{)},\]

_where \(M\) is an absolute constant and \(C_{D}\) is a constant depending only on \(D\) (if \(\|\mathbb{E}_{\bm{x}}[\nabla^{k}f(\bm{x})]\|_{F}=0\) then we ignore the term with \(k\))._

**Lemma 16**.: _Let \(\bm{x}\sim\mathcal{N}(0,I_{d})\) and \(y=f_{*}(\bm{x})+\varsigma=\sum_{i=Q}^{P}\frac{c_{i}}{i!}\mathrm{He}_{i}( \langle\bm{x},\bm{\beta}\rangle)+\varsigma\) where \(c_{i}\) and \(\bm{\beta}\) are drawn according to Assumption 1, and \(\varsigma\sim\mathrm{Unif}\{\pm\tau\}\). Then, for any fixed \(\{c_{i}\}\) and \(\bm{\beta}\) on the support of their distributions, it holds that_

\[\mathbb{P}(|y|\geq t+\tau)\leq 2\exp\Biggl{(}-\frac{1}{C_{P}M^{2}}\min_{2\leq k \leq P}\biggl{(}\frac{t}{R_{c}}\biggr{)}^{2/k}\Biggr{)},\]

_where \(M\) is an absolute constant and \(C_{P}\) is a constant depending only on \(P\)._Proof.: Note that from Corollary 9, \(\mathbb{E}_{\bm{x}}[f_{*}(\bm{x})]=0,\mathbb{E}_{\bm{x}}[\nabla f(\bm{x})]=\bm{0}\) and \(\mathbb{E}_{\bm{x}}[\nabla^{i}f_{*}(\bm{x})]=c_{i}(\bm{\beta})^{\otimes i}\); by (2.2), Lemma 6 and \(\|\bm{\beta}\|=1\), \(\|\mathbb{E}_{\bm{x}}[\nabla^{i}f_{*}(\bm{x})]\|_{F}\leq R_{c}\) holds. Lemma 15 yields the result. 

**Corollary 17**.: _Fix any \(\{c_{i}\}\) and \(\bm{\beta}\) on the support of their distributions. Then, \(|f_{*}(\bm{x})|,|y|\lesssim(\log d)^{P/2}\) holds with high probability over the distribution of \(\bm{x}\) and \(\varsigma\)._

## Appendix B Proofs for the MLP Layer

This section analyzes the behavior of MLP weight \(\bm{w}\) after one gradient descent step (line 4 in Algorithm 1), as sketched in Section 3.2.1.

We set \(\lambda_{1}=\eta_{1}^{-1}\). From line 4 of Algorithm 1, for each \(j\in[m]\),

\[\bm{w}_{j}^{(1)} =2\eta_{1}\frac{1}{T_{1}}\sum_{t=1}^{T_{1}}y^{t}\nabla_{\bm{w}_{ j}^{(0)}}f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(0)},\bm{\Gamma}^{(0)}, \bm{b}^{(0)})\] \[\qquad-2\eta_{1}\frac{1}{T_{1}}\sum_{t=1}^{T_{1}}f(\bm{X}^{t}, \bm{y}^{t},\bm{x}^{t};\bm{W}^{(0)},\bm{\Gamma}^{(0)},\bm{b}^{(0)})\nabla_{\bm{ w}_{j}^{(0)}}f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(0)},\bm{\Gamma}^{(0)}, \bm{b}^{(0)})\] \[=2\eta_{1}\Gamma_{j,j}^{(0)}\bigg{(}\sum_{t=1}^{T_{1}}\frac{1}{T_ {1}}y^{t}\sigma_{b_{j}}({\bm{w}_{j}^{(0)}}^{\top}\bm{x}^{t})\frac{1}{N_{1}} \sum_{i=1}^{N_{1}}y_{i}^{t}\sigma_{b_{j}}({\bm{w}_{j}^{(0)}}^{\top}\bm{x}_{i}^{ t})\bm{x}_{i}^{t}\] \[\qquad+\sum_{t=1}^{T_{1}}\frac{1}{T_{1}}y^{t}\sigma_{b_{j}}^{\prime }({\bm{w}_{j}^{(0)}}^{\top}\bm{x}^{t})\bm{x}^{t}\frac{1}{N_{1}}\sum_{i=1}^{N_{ 1}}y_{i}^{t}\sigma_{b_{j}}({\bm{w}_{j}^{(0)}}^{\top}\bm{x}_{i}^{t})\bigg{)}\] (B.1) \[\qquad-2\eta_{1}\frac{1}{T_{1}}\sum_{t=1}^{T_{1}}f(\bm{X}^{t},\bm {y}^{t},\bm{x}^{t};\bm{W}^{(0)},\bm{\Gamma}^{(0)},\bm{b}^{(0)})\nabla_{\bm{w}_ {j}^{(0)}}f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(0)},\bm{\Gamma}^{(0)}, \bm{b}^{(0)}).\]

holds where \(\sigma_{b_{j}}(z)\coloneqq\sigma(z+b_{j})\). Here, \(\bm{x}_{i}^{t}\) and \(y_{i}^{t}\) is the input and output of \(i\)-th context at \(t\)-th task, respectively. First we show that the final term is sufficiently small, if we set \(\gamma=|\Gamma_{j,j}^{(0)}|\) sufficiently small.

**Lemma 18**.: _With high probability over the distribution of \(\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}}\) and \(\{\bm{w}_{j}^{(0)}\}_{j=1}^{m}\),_

\[\left\|2\eta_{1}\frac{1}{T_{1}}\sum_{t=1}^{T_{1}}f(\bm{X}^{t},\bm {y}^{t},\bm{x}^{t};\bm{W}^{(0)},\bm{\Gamma}^{(0)},\bm{b}^{(0)})\nabla_{\bm{w}_ {j}^{(0)}}f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(0)},\bm{\Gamma}^{(0)}, \bm{b}^{(0)})\right\|\] \[= \tilde{O}\Big{(}\eta_{1}\gamma^{2}m\sqrt{d}\Big{)}.\]

Proof.: Recall that

\[f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(0)},\bm{\Gamma}^{(0)}, \bm{b}^{(0)}) =\sum_{j=1}^{m}\Gamma_{j,j}^{(0)}\Bigg{(}\frac{1}{N_{1}}\sum_{i=1 }^{N_{1}}y_{i}^{t}\sigma_{b_{j}}({\bm{w}_{j}^{(0)}}^{\top}\bm{x}_{i}^{t}) \Bigg{)}\sigma_{b_{j}}({\bm{w}_{j}^{(0)}}^{\top}\bm{x}^{t}),\] \[\nabla_{\bm{w}_{j}^{(0)}}f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W }^{(0)},\bm{\Gamma}^{(0)},\bm{b}^{(0)}) =\Gamma_{j,j}^{(0)}\Bigg{(}\frac{1}{N_{1}}\sum_{i=1}^{N_{1}}y_{i}^ {t}\sigma_{b_{j}}^{\prime}({\bm{w}_{j}^{(0)}}^{\top}\bm{x}_{i}^{t})\bm{x}_{i}^{t }\Bigg{)}\sigma_{b_{j}}({\bm{w}_{j}^{(0)}}^{\top}\bm{x}^{t})\] \[\qquad+\Gamma_{j,j}^{(0)}\Bigg{(}\frac{1}{N_{1}}\sum_{i=1}^{N_{1} }y_{i}^{t}\sigma_{b_{j}}({\bm{w}_{j}^{(0)}}^{\top}\bm{x}_{i}^{t})\Bigg{)}\sigma _{b_{j}}^{\prime}({\bm{w}_{j}^{(0)}}^{\top}\bm{x}^{t})\bm{x}^{t}.\]

Fix \(i,t,j\) and consider the inner product between \(\bm{w}_{j}^{(0)}\sim\mathrm{Unif}(\mathbb{S}^{d-1})\) and \(\bm{x}_{i}^{t}\sim\mathcal{N}(0,I_{d})\). From the rotational invariance, without loss of generality, we can assume that \(\bm{w}_{j}^{(0)}=[1,0,\ldots,0]^{\top}\) and \(\bm{x}_{i}^{t}\sim\mathcal{N}(0,I_{d})\). Therefore, \(\Big{\langle}\bm{w}_{j}^{(0)},\bm{x}_{i}^{t}\Big{\rangle}\sim\mathcal{N}(0,1)\). From Lemma 4, \(\Big{\langle}\bm{w}_{j}^{(0)},\bm{x}_{i}^{t}\Big{\rangle}\gtrsim\log d\) holds with high probability. Moreover, from Lemma 10 and Corollary 17, we know that and \(\|\bm{x}_{i}^{t}\|,\|\bm{x}^{t}\|\lesssim\sqrt{d}\) holds. We can take the union bound with respect to all the \(i,t,j\), retaining the high probability bound (see Appendix A). Therefore, noting that \(|b_{j}|\leq 1\), we obtain

\[|f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(0)},\bm{\Gamma}^{(0)},\bm{b}^{(0)} )|\lesssim m\gamma(\log d)^{P/2+1}\]

and

\[\|\nabla_{\bm{w}_{j}^{(0)}}f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(0)},\bm {\Gamma}^{(0)},\bm{b}^{(0)})\|\lesssim\gamma(\log d)^{P/2+1/2}\sqrt{d}.\]

Thus we obtain the assertion. 

From now on we focus on analyzing (B.1), which expresses the correlation between the output label \(y\) and the gradient of model output \(\nabla_{\bm{w}}f\). Let

\[\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})\] \[\coloneqq \sum_{i=1}^{T_{1}}\frac{1}{T_{1}}y^{t}\sigma_{b}(\bm{w}^{\top} \bm{x}^{t})\frac{1}{N_{1}}\sum_{i=1}^{N_{1}}y_{i}^{t}\sigma_{b}^{\prime}(\bm{w }^{\top}\bm{x}_{i}^{t})\bm{x}_{i}^{t}\] \[\quad+\sum_{i=1}^{T_{1}}\frac{1}{T_{1}}y^{t}\sigma_{b}^{\prime}( \bm{w}^{\top}\bm{x}^{t})\bm{x}^{t}\frac{1}{N_{1}}\sum_{i=1}^{N_{1}}y_{i}^{t} \sigma_{b}(\bm{w}^{\top}\bm{x}_{i}^{t}),\] (B.2)

and \(\bm{g}(\bm{w},b)=\mathbb{E}[\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^ {t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})]\), where the expectation is taken with respect to the data \(\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}}\). Note that

\[\bm{w}_{j}^{(1)}=2\eta_{1}\Gamma_{j,j}^{(0)}\bm{g}_{T_{1},N_{1}}(\bm{w}_{j}^{( 0)},b_{j}^{(0)},\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})+ \tilde{O}\Big{(}\eta_{1}\gamma^{2}m\sqrt{d}\Big{)}\]

from Lemma 18.

In Appendix B.1, we analyze the explicit form of \(\bm{g}(\bm{w},b)\) via the Hermite expansion: we can check that its main term aligns with the low-dimensional subspace \(\mathcal{S}\). In Appendix B.2, we show the concentration of \(\bm{g}_{T_{1},N_{1}}\) around \(\bm{g}\).

### Calculation of Population Gradient

#### b.1.1 Hermite Expansion

First, note that

\[\bm{g}(\bm{w},b) =\mathbb{E}\big{[}y\sigma_{b}(\bm{w}^{\top}\bm{x})y_{i}\sigma_{b} ^{\prime}(\bm{w}^{\top}\bm{x}_{i})\bm{x}_{i}\big{]}+\mathbb{E}\big{[}y\sigma_ {b}^{\prime}(\bm{w}^{\top}\bm{x})\bm{x}y_{i}\sigma_{b}(\bm{w}^{\top}\bm{x}_{i })\big{]}\] \[=\mathbb{E}_{f_{*}}\big{[}\mathbb{E}_{\{\bm{x}_{i}\},\bm{x},\{ \varsigma_{i}\},\varsigma}\big{[}(f_{*}(\bm{x})+\varsigma)\sigma_{b}(\bm{w}^{ \top}\bm{x})(f_{*}(\bm{x}_{i})+\varsigma_{i})\sigma_{b}^{\prime}(\bm{w}^{\top }\bm{x}_{i})\bm{x}_{i}\big{]}\big{]}\] \[\quad+\mathbb{E}_{f_{*}}\big{[}\mathbb{E}_{\{\bm{x}_{i}\},\bm{x},\varsigma_{i}\},\big{[}(f_{*}(\bm{x})+\varsigma)\sigma_{b}^{\prime}(\bm{w}^{ \top}\bm{x})\bm{x}(f_{*}(\bm{x}_{i})+\varsigma_{i})\sigma_{b}(\bm{w}^{\top}\bm {x}_{i})\big{]}\big{]}\] \[=\mathbb{E}_{f_{*}}\big{[}\mathbb{E}_{\{\bm{x}_{i}\}}\big{[}(f_{* }(\bm{x})+\varsigma)\sigma_{b}(\bm{w}^{\top}\bm{x})\bm{x}\big{]}\mathbb{E}_{ \{\bm{x}_{i}\},\varsigma_{i}\}\big{[}(f_{*}(\bm{x}_{i})+\varsigma_{i})\sigma_{b }(\bm{w}^{\top}\bm{x}_{i})\big{]}\big{]}\] \[=2\mathbb{E}_{f_{*}}\big{[}\mathbb{E}_{\bm{x}_{i}}\big{[}f_{*}( \bm{x})\sigma_{b}^{\prime}(\bm{w}^{\top}\bm{x})\bm{x}\big{]}\mathbb{E}_{\bm{x} }[f_{*}(\bm{x})\sigma_{b}(\bm{w}^{\top}\bm{x})]\big{]}.\]

Here \(\mathbb{E}_{f_{*}}\) denotes the expectation with respect to the distribution of the true function \(f_{*}\) (i.e., the distribution of \(\{c_{i}\}_{i=Q}^{P}\) and \(\bm{\beta}\)) specified in Assumption 1. Now let \(\sigma_{b}(z)=\sum_{i\geq 0}\frac{a_{i}(b)}{i!}\mathrm{He}_{i}(z)\) be the Hermite expansion of student activation \(\sigma_{b}(z)=\mathrm{ReLU}(z+b)\).

**Lemma 19**.: _It holds that_

\[\mathbb{E}_{\bm{x}}[f_{*}(\bm{x})\sigma_{b}^{\prime}(\bm{w}^{\top} \bm{x})\bm{x}]\] \[=\sum_{i=Q-1}^{P-1}\frac{a_{i+1}(b)\mathbb{E}_{\bm{x}}[\nabla^{i+1 }f_{*}(\bm{x})](\bm{w}^{\otimes i})}{i!}+\bm{w}\sum_{i=Q}^{P}\frac{a_{i+2}(b) \mathbb{E}_{\bm{x}}[\nabla^{i}f_{*}(\bm{x})](\bm{w}^{\otimes i})}{i!},\] (B.3) \[\mathbb{E}_{\bm{x}}[f_{*}(\bm{x})\sigma_{b}(\bm{w}^{\top}\bm{x})]= \sum_{i=Q}^{P}\frac{a_{i}(b)\mathbb{E}_{\bm{x}}[\nabla^{i}f_{*}(\bm{x})](\bm{w}^ {\otimes i})}{i!}.\]

[MISSING_PAGE_EMPTY:21]

**Lemma 20**.: _There are constants \(C_{H}\) and \(C_{L}\) depending only on \(Q\) and \(P\) satisfying the following condition. Let \(\sigma_{b}(z)=\sigma(z+b)\) and \(b\sim\mathrm{Unif}([-1,1])\). Then,_

\[\big{|}\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\sigma_{b}(z)\mathrm{He}_{i}(z)] \big{|}\leq C_{H}\;(2\leq i\leq P+2)\text{ and }\big{|}\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\sigma_{b}(z) \mathrm{He}_{Q}(z)]\big{|}\geq C_{L}\] (B.5)

_holds with probability 1/2. Especially, \(\big{|}\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\sigma_{b}(z)\mathrm{He}_{i}(z)] \big{|}\leq C_{H}\;(2\leq i\leq P+2)\) holds with probability 1._

**Proof.** From Lemma 15 in [1], we know that for \(i\geq 2\), \(\big{|}\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\sigma_{b}(z)\mathrm{He}_{i}(z)] \big{|}=\frac{e^{-b^{2}/2}}{\sqrt{2\pi}}|\mathrm{He}_{i-2}(b)|\) holds. First, as \(b\sim\mathrm{Unif}([-1,1])\), by setting sufficiently large \(C_{H}\), \(|\mathrm{He}_{i}(b)|\leq C_{H}\) holds for all \(0\leq i\leq P\). Therefore, as \(\big{|}\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\sigma_{b}(z)\mathrm{He}_{i}(z)] \big{|}\leq|\mathrm{He}_{i-2}(b)|\), \(\big{|}\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\sigma_{b}(z)\mathrm{He}_{i}(z)] \big{|}\leq C_{H}\) holds for all \(2\leq i\leq P+2\) with probability one.

From continuity we know that there exists \(C_{L}\) such that \(\big{|}\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\sigma_{b}(z)\mathrm{He}_{Q}(z)] \big{|}\geq C_{L}\) with probability 1/2. \(\square\)

In the following, we first calculate the main term and then upper bound the residual terms.

#### b.1.2 Calculation of the Main Term

Let us calculate the main term in (B.4) explicitly.

**Lemma 21**.: \[\frac{2a_{Q}(b)^{2}}{Q!(Q-1)!}\mathbb{E}_{f_{*}}[\mathbb{E}_{\bm{ x}}[\nabla^{Q}f_{*}(\bm{x})](\bm{w}^{\otimes(Q-1)})\cdot\mathbb{E}_{\bm{x}}[ \nabla^{Q}f_{*}(\bm{x})](\bm{w}^{\otimes Q})]\] \[= \frac{2a_{Q}(b)^{2}\mathbb{E}_{\mathbb{C}_{Q}}[c_{Q}^{2}]}{Q!(Q-1 )!}\frac{(2Q-1)!!}{\mathbb{E}_{z\sim_{Y}}[z^{2Q}]}\|\bm{w}_{1:r}\|^{2Q-2} \begin{bmatrix}\bm{w}_{1:r}\\ \bm{0}_{d-r}\end{bmatrix}\]

_holds._

**Proof.** From Corollary 9, it holds that \(\mathbb{E}_{\bm{x}}[\nabla^{Q}f_{*}(\bm{x})]=c_{Q}(\bm{\beta})^{\otimes Q}\). Then, we obtain

\[\mathbb{E}_{f_{*}}[\mathbb{E}_{\bm{x}}[\nabla^{Q}f_{*}(\bm{x})]( \bm{w}^{\otimes(Q-1)})\cdot\mathbb{E}_{\bm{x}}[\nabla^{Q}f_{*}(\bm{x})](\bm{w} ^{\otimes Q})]\] \[= \mathbb{E}_{c_{Q}}[c_{Q}^{2}]\mathbb{E}_{\bm{\beta}}[\bm{\beta}^ {\otimes Q-1})\cdot(\bm{\beta}^{\otimes Q}\circ\bm{w}^{\otimes Q})]\] \[= \mathbb{E}_{c_{Q}}[c_{Q}^{2}]\mathbb{E}_{\bm{\beta}}[\bm{\beta}^ {\otimes 2Q}](\bm{w}^{\otimes(2Q-1)}).\]

Next, let us show

\[\mathbb{E}_{\bm{\beta}}[\bm{\beta}^{\otimes 2Q}](\bm{w}^{\otimes(2Q-1)})=\frac{(2Q- 1)!!}{\mathbb{E}_{z\sim_{Y}}[z^{2Q}]}\|\bm{w}_{1:r}\|^{2Q-2}\begin{bmatrix} \bm{w}_{1:r}\\ \bm{0}_{d-r}\end{bmatrix}.\]

First, note that by letting \(\bm{\beta}^{\prime}\sim\mathcal{N}(0,\bm{\Sigma}_{\bm{\beta}})\) where

\[\bm{\Sigma}_{\bm{\beta}}=\begin{bmatrix}\bm{I}_{r}&\bm{0}_{r}\\ \bm{0}_{r}^{\top}&\bm{O}_{d-r}\end{bmatrix},\]

and \(z\sim\chi_{r}\) independent from \(\bm{\beta}\), \(\bm{\beta}^{\prime}\sim\bm{\beta}z\) holds (recall that we assumed that \(\bm{\beta}\sim\mathrm{Unif}(\{(\beta_{1},\dots,\beta_{r},0,\dots,0)^{\top}\mid \beta_{1}^{2}+\dots+\beta_{r}^{2}=1\})\) - see Appendix A). Then,

\[\mathbb{E}_{\bm{\beta}}[\bm{\beta}^{\otimes 2Q}]=\frac{1}{\mathbb{E}_{z\sim_{Y}}[z^{2Q }]}\mathbb{E}_{\bm{v}_{1,r}^{\prime}\sim\mathcal{N}(0,I_{r}),\bm{v}_{r+1:d}^{ \prime}=\bm{0}}[\bm{v}^{\prime\otimes 2Q}].\]

It suffices to show that \(\mathbb{E}_{\bm{v}\sim\mathcal{N}(0,I_{r})}[\bm{v}^{\otimes 2Q}](\bm{z}^{ \otimes(2Q-1)})=(2Q-1)!!\bm{z}\|\bm{z}\|^{2Q-2}\) for \(\bm{z}\in\mathbb{R}^{r}\). As an example, we show that the first entry of \(\mathbb{E}_{\bm{v}\sim\mathcal{N}(0,I_{r})}[\bm{v}^{\otimes 2Q}](\bm{z}^{ \otimes(2Q-1)})\) equals to \((2Q-1)!!z_{1}\|\bm{z}\|^{2Q-2}\): the other components can be calculated similarly. First, we expand \((\mathbb{E}_{\bm{v}\sim\mathcal{N}(0,I_{r})}[\bm{v}^{\otimes 2Q}](\bm{z}^{ \otimes(2Q-1)}))_{1}\) as

\[(\mathbb{E}_{\bm{v}\sim\mathcal{N}(0,I_{r})}[\bm{v}^{\otimes 2Q}](\bm{z}^{ \otimes(2Q-1)}))_{1}=\sum_{i_{1},\dots,i_{2Q-1}\in[r]}\mathbb{E}[v_{1}v_{i_{1}} \cdots v_{i_{2Q-1}}]z_{i_{1}}\cdots z_{i_{2Q-1}}.\]

Note that \(\mathbb{E}[v_{1}v_{i_{1}}\cdots v_{i_{2Q-1}}]\neq 0\) if and only if the degree of \(v_{1}v_{i_{1}}\cdots v_{i_{2Q-1}}\) with respect to \(v_{j}\) is even, for all \(j\in[r]\). Then, we obtain

\[(\mathbb{E}_{\bm{v}\sim\mathcal{N}(0,I_{r})}[\bm{v}^{\otimes 2Q}](\bm{z}^{ \otimes(2Q-1)}))_{1}\]\[= \sum_{q_{1}\geq 1,q_{2},...,q_{r}\geq 0}c_{q_{1},...,q_{r}}\mathbb{E}[v_{ 1}^{2q_{1}}v_{2}^{2q_{2}}\cdots v_{r}^{2q_{r}}]z_{1}^{2q_{1}-1}z_{2}^{2q_{2}} \cdots z_{r}^{2q_{r}}\] \[= z_{1}\sum_{q_{1}\geq 1,q_{2},...,q_{r}\geq 0}^{q_{1}+\cdots+q_{r}=Q} c_{q_{1},...,q_{r}}\mathbb{E}[v_{1}^{2q_{1}}v_{2}^{2q_{2}}\cdots v_{r}^{2q_{r}}]z _{1}^{2q_{1}-2}z_{2}^{2q_{2}}\cdots z_{r}^{2q_{r}},\]

where \(c_{q_{1},...,q_{r}}=\frac{(2Q-1)!}{(2q_{1}-1)!(2q_{2})!\cdots(2q_{r})!}\) is the number of ways to assign \((2q_{1}-1)\) "1"s, \((2q_{2})\) "2"s,..., among \(i_{1}\) to \(i_{2Q-1}\).

Note that \(\mathbb{E}_{v\sim\mathcal{N}(0,1)}[v^{2q}]=(2q-1)!!\). Therefore, we obtain

\[(\mathbb{E}_{v\sim\mathcal{N}(0,I_{r})}[\bm{v}^{\otimes 2Q}]( \bm{z}^{\otimes(2Q-1)}))_{1}\] \[= z_{1}\sum_{q_{1}\geq 1,q_{2},...,q_{r}\geq 0}^{q_{1}+\cdots+q_{r}=Q} \frac{(2Q-1)!}{(2q_{1}-1)!(2q_{2})!\cdots(2q_{r})!}(2q_{1}-1)!!\cdots(2q_{r}- 1)!!z_{1}^{2q_{1}-2}z_{2}^{2q_{2}}\cdots z_{r}^{2q_{r}}\] \[= z_{1}\sum_{q_{1}\geq 1,q_{2},...,q_{r}\geq 0}^{q_{1}+\cdots+q_{r}=Q} \frac{(Q-1)!}{(q_{1}-1)!(q_{2})!\cdots(q_{r})!}(2Q-1)!!(z_{1}^{2})^{q_{1}-1}( z_{2}^{2})^{q_{2}}\cdots(z_{r}^{2})^{q_{r}}\] \[= (2Q-1)!!z_{1}(z_{1}^{2}+\cdots+z_{r}^{2})^{Q-1}\qquad(\because\text {multinomial theorem})\]

which concludes the assertion. 

By Lemma 21, we observe that the main term of the expected one-step gradient aligns to the \(r\)-dimensional subspace \(\mathcal{S}\). In other words, the MLP layer captures the information of \(\mathcal{S}\) via pretraining, which will be useful in the later stage.

#### b.1.3 Bounding Residual Terms

First, similarly to the Lemma 21, we can obtain the explicit formulation of the entire \(\bm{g}(\bm{w},b)\).

**Lemma 22**.: \[\bm{g}(\bm{w},b)\] \[= \sum_{\begin{subarray}{c}Q-1\leq i\leq P-1,\\ \begin{subarray}{c}Q\leq j\leq P,\\ i+j\text{ is odd}\end{subarray}}\frac{2a_{i+1}(b)a_{j}(b)\mathbb{E}[c_{i+1}c_{ j}]}{i!j!}\frac{(i+j)!!}{\mathbb{E}_{z\sim\chi_{r}}[z^{i+j+1}]}\|\bm{w}_{1:r}\|^{i+j-1} \begin{bmatrix}\bm{w}_{1:r}\\ \bm{0}_{d-r}\end{bmatrix}\] \[+\bm{w}\sum_{\begin{subarray}{c}Q\leq i\leq P,\\ \begin{subarray}{c}Q\leq i\leq P,\\ i+j\text{ is even}\end{subarray}}\frac{2a_{i+2}(b)a_{j}(b)\mathbb{E}[c_{i}c_{ j}]}{i!j!}\frac{(i+j-1)!!}{\mathbb{E}_{z\sim\chi_{r}}[z^{i+j}]}\|\bm{w}_{1:r}\|^{i+j}\] (B.6)

_holds._

Proof.: Note that

\[\mathbb{E}_{\bm{\beta}}[\bm{\beta}^{\otimes n}](\bm{w}^{\otimes(n-1)})=\left\{ \begin{array}{l}\frac{(n-1)!!}{\mathbb{E}_{z\sim\chi_{r}}[z^{n}]}\|\bm{w}_{1: r}\|^{n-2}\begin{bmatrix}\bm{w}_{1:r}\\ \bm{0}_{d-r}\end{bmatrix}(n\text{ is even})\\ 0\;(n\text{ is odd})\end{array}\right.\]

and

\[\mathbb{E}_{\bm{\beta}}[\bm{\beta}^{\otimes n}](\bm{w}^{\otimes n})=\left\{ \begin{array}{l}\frac{(n-1)!!}{\mathbb{E}_{z\sim\chi_{r}}[z^{n}]}\|\bm{w}_{1: r}\|^{n}\;(n\text{ is even})\\ 0\;(n\text{ is odd})\end{array}\right.,\]

which can be obtained similarly to the proof of Lemma 21. 

Let us upper bound the non-leading terms; we need to show that the moment of first \(r\) components in residual term \(\bm{s}(\bm{w},b)\) are sufficiently small.

**Lemma 23**.: _Let \(j=4k\) where \(k\in[P]\), then,_

\[\mathbb{E}_{\bm{w}\sim\mathbb{S}^{d-1}}[\|\bm{s}(\bm{w},b)_{1:r}\|^{j} ]^{1/j}=\tilde{O}_{d,r}\bigg{(}\sqrt{\frac{r}{d^{2Q+1}}}\bigg{)}\] (B.7)

_holds uniformly over all \(b\in[-1,1]\)._

**Proof.** Note that \(\|\bm{s}(\bm{w},b)_{1:r}\|^{2}\) is a polynomial in \(\bm{w}\); from Lemma 24 in [10], it suffices to show the case \(k=1\).

\[\bm{s}(\bm{w},b)_{1:r}\] \[=\sum_{\begin{subarray}{c}Q-1\leq i\leq P-1,\\ Q\leq j\leq P,\\ i+j\text{ is odd},\\ i,j\neq(Q-1,Q)\end{subarray}}\frac{2a_{i+1}(b)a_{j}(b)\mathbb{E}[c_{i+1}c_{j}]}{i! j!}\frac{(i+j)!!}{\mathbb{E}_{z\sim\chi_{r}}[z^{i+j+1}]}\|\bm{w}_{1:r}\|^{i+j-1} \bm{w}_{1:r}\] \[+\sum_{\begin{subarray}{c}Q\leq i\leq P,\\ Q\leq j\leq P,\\ i+j\text{ is even}\end{subarray}}\frac{2a_{i+2}(b)a_{j}(b)\mathbb{E}[c_{i}c_{j}]} {i!j!}\frac{(i+j-1)!!}{\mathbb{E}_{z\sim\chi_{r}}[z^{i+j}]}\|\bm{w}_{1:r}\|^{ i+j}\bm{w}_{1:r}.\]

By Minkowski's inequality \(\mathbb{E}[\|\bm{x}+\bm{y}\|^{4}]^{1/4}\leq\mathbb{E}[\|\bm{x}\|^{4}]^{1/4}+ \mathbb{E}[\|\bm{y}\|^{4}]^{1/4}\), it suffices to show that (B.7) holds for each term. Note that \(\mathbb{E}[c_{i+1}c_{j}]\leq R_{\mathrm{c}}^{2}\) and \(\mathbb{E}[c_{i}c_{j}]\leq R_{\mathrm{c}}^{2}\) from Assumption 1, \(|a_{i}(b)|\leq C_{H}\) from Lemma 20, and \(\mathbb{E}_{\bm{w}}[\|\bm{w}_{1:r}\|^{4k}]=O((r/d)^{2k})\) from Lemma 12. Moreover, it is known that \(\mathbb{E}_{z\sim\chi_{r}}[z^{2l}]=\Theta(r^{l})\). Putting these things together yields the assertion. \(\square\)

It is also needed in later stages to give a high probability upper bound on the \(\|\bm{g}(\bm{w},b)\|\);

**Lemma 24**.: \[\sup_{b\in[-1,1]}\|\bm{g}(\bm{w},b)\|=\tilde{O}_{d,r}\bigg{(}\sqrt{\frac{1}{ rd^{2Q-1}}}\bigg{)}\]

_holds with high probability over \(\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})\)._

**Proof.** This can be shown similarly to Lemma 23 - we use Lemma 11 instead of Lemma 12 to obtain a high probability bound (note that this bound includes not only \(s(\bm{w},b)\) but also the main term, and it is for \(\bm{g}(\bm{w},b)\), instead of \(\bm{g}(\bm{w},b)_{1:r}\)). \(\square\)

### Concentration of Empirical Gradient

Now we control the deviation of \(\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t =1}^{T_{1}})\) from \(\bm{g}(\bm{w},b)\).

**Lemma 25**.: _Under Assumption 1,_

\[\sup_{f_{*},b\in[-1,1]}\|\mathbb{E}_{\bm{x}}[f_{*}(\bm{x})\sigma_{ b}^{\prime}(\bm{w}^{\top}\bm{x})\bm{x}]\| \lesssim\sqrt{\left(\frac{r}{d}\log d\right)^{Q-1}},\] (B.8) \[\sup_{f_{*},b\in[-1,1]}\|\mathbb{E}_{\bm{x}}[f_{*}(\bm{x})\sigma_ {b}^{\prime}(\bm{w}^{\top}\bm{x})\bm{x}]_{1:r}\| \lesssim\sqrt{\left(\frac{r}{d}\log d\right)^{Q-1}},\] (B.9) \[\sup_{f_{*},b\in[-1,1]}\|\mathbb{E}_{\bm{x}}[f_{*}(\bm{x})\sigma_ {b}(\bm{w}^{\top}\bm{x})]\| \lesssim\sqrt{\left(\frac{r}{d}\log d\right)^{Q}}\] (B.10)

_holds with high probability over \(\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})\), respectively. Here \(\sup_{f_{*}}\) denotes the supremum over the support of \(f_{*}\) whose distribution is specified in Assumption 1._

**Proof.** We bound the leading term \(\frac{a_{Q}(b)\mathbb{E}_{\bm{x}}[\nabla^{Q}f_{*}(\bm{x})](\bm{w}^{\otimes(Q- 1)})}{(Q-1)!}\) in (B.3) of Lemma 19 to show (B.8). From Lemmas 5 and 6, we have

\[\left\|\frac{a_{Q}(b)\mathbb{E}_{\bm{x}}[\nabla^{Q}f_{*}(\bm{x})]( \bm{w}^{\otimes(Q-1)})}{(Q-1)!}\right\|^{2}=\left\|\frac{a_{Q}(b)c_{Q}}{(Q-1)!}(\bm{\beta})^{\otimes Q}(\bm{w}^{\otimes(Q-1)})\right\|^{2}\]\[=\left\|\frac{a_{Q}(b)c_{Q}}{(Q-1)!}(\bm{\beta})^{\otimes Q}(\bm{w} _{1:r}^{\otimes(Q-1)})\right\|^{2}\] \[\leq\left|\frac{a_{Q}(b)c_{Q}}{(Q-1)!}\right|^{2}\!\|\bm{\beta}\|^ {2Q}\|\bm{w}_{1:r}\|^{2(Q-1)}\] \[=\left|\frac{a_{Q}(b)c_{Q}}{(Q-1)!}\right|^{2}\!\|\bm{w}_{1:r}\|^ {2(Q-1)}.\]

Moreover, Lemma 20 tells us that \(|a_{Q}(b)|\leq C_{H}\) always holds when \(b\in[-1,1]\), and Assumption 1 ensures that \(|c_{Q}|\leq R_{c}\). Therefore, from Lemma 11, we can show that \(\left\|\frac{a_{Q}(b)\mathbb{E}_{\bm{x}}[\nabla^{Q}f_{*}(\bm{x})]([\bm{w}^{ \otimes(Q-1)})}{(Q-1)!}\right\|^{2}\leq\left(\frac{C_{H}R_{c}}{(Q-1)!}\right) ^{2}\!\left(\frac{4C_{T}}{d}\log d\right)^{Q-1}\) with probability at least \(1-2\exp(-d/32)-2rd^{-C}\). (B.9) and (B.10) can be obtained similarly. 

The following lemma is parallel to Lemma 19 in [10].

**Lemma 26**.: _Fix any \(t\in[T_{1}]\), \(b\in[-1,1]\) and \(\bm{w}\in\mathbb{S}^{d-1}\) satisfying the conditions (B.8) to (B.10). Then, with high probability over the distribution of \((\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\), it holds that_

\[\left\|\frac{1}{N_{1}}\sum_{i=1}^{N_{1}}y_{i}^{t}\sigma_{b}^{ \prime}(\bm{w}^{\top}\bm{x}_{i}^{t})\bm{x}_{i}^{t}-\mathbb{E}_{\bm{x}}[f_{*}^ {t}(\bm{x})\sigma_{b}^{\prime}(\bm{w}^{\top}\bm{x})\bm{x}]\right\| \leq\tilde{O}\Bigg{(}\sqrt{\frac{d}{N_{1}}}\Bigg{)},\] \[\left\|\Bigg{(}\frac{1}{N_{1}}\sum_{i=1}^{N_{1}}y_{i}^{t}\sigma_{ b}^{\prime}(\bm{w}^{\top}\bm{x}_{i}^{t})\bm{x}_{i}^{t}-\mathbb{E}_{\bm{x}}[f_{*}^ {t}(\bm{x})\sigma_{b}^{\prime}(\bm{w}^{\top}\bm{x})\bm{x}]\Bigg{)}_{1:r}\right\| \leq\tilde{O}\Bigg{(}\sqrt{\frac{r}{N_{1}}}\Bigg{)},\] \[\left|\frac{1}{N_{1}}\sum_{i=1}^{N_{1}}y_{i}^{t}\sigma_{b}(\bm{w} ^{\top}\bm{x}_{i}^{t})-\mathbb{E}_{\bm{x}}[f_{*}^{t}(\bm{x})\sigma_{b}(\bm{w} ^{\top}\bm{x})]\right| \leq\tilde{O}\Bigg{(}\sqrt{\frac{1}{N_{1}}}\Bigg{)}.\]

_Here the right-hand sides do not depend on \(t\)._

Proof.: We only present the proof of the first inequality, as other bounds can be attained similarly. From Corollary 17, \(|f_{*}^{t}(\bm{x})|\leq R\) where \(R\lesssim(\log d)^{P/2}\) holds with high probability for all \(t\in[T_{1}]\). Conditioned on this,

\[\left\|\frac{1}{N_{1}}\sum_{i=1}^{N_{1}}f_{*}^{t}(\bm{x}_{i}^{t} )\sigma_{b}^{\prime}(\bm{w}^{\top}\bm{x}_{i}^{t})\bm{x}_{i}^{t}-\mathbb{E}_{ \bm{x}}[f_{*}^{t}(\bm{x})\sigma_{b}^{\prime}(\bm{w}^{\top}\bm{x})\bm{x}]\right\|\] \[\leq \left\|\frac{1}{N_{1}}\sum_{i=1}^{N_{1}}f_{*}^{t}(\bm{x}_{i}^{t} )\mathbb{I}_{f_{*}^{t}(\bm{x}_{i}^{t})\leq R}\sigma_{b}^{\prime}(\bm{w}^{\top }\bm{x}_{i}^{t})\bm{x}_{i}^{t}-\mathbb{E}_{\bm{x}}[f_{*}^{t}(\bm{x})\mathbb{I} _{f_{*}^{t}(\bm{x})\leq R}\sigma_{b}^{\prime}(\bm{w}^{\top}\bm{x})\bm{x}]\right\|\] \[+\left\|\mathbb{E}_{\bm{x}}[f_{*}^{t}(\bm{x})\mathbb{I}_{f_{*}^{t} (\bm{x})>R}\sigma_{b}^{\prime}(\bm{w}^{\top}\bm{x})\bm{x}]\right\|\]

Note that \(f_{*}^{t}(\bm{x}_{i}^{t})\mathbb{I}_{f_{*}^{t}(\bm{x}_{i}^{t})\leq R}\sigma_{b} ^{\prime}(\bm{w}^{\top}\bm{x}_{i}^{t})(\bm{x}_{i}^{t},\bm{u})-\mathbb{E}_{\bm{x }}[f_{*}^{t}(\bm{x})\mathbb{I}_{f_{*}^{t}(\bm{x})\leq R}\sigma_{b}^{\prime}( \bm{w}^{\top}\bm{x})\langle\bm{x},\bm{u}\rangle]\) is \(R^{2}\)-sub Gaussian for any \(\|\bm{u}\|=1\) from Lemma 14. Hence the first term can be upper bounded by \(\tilde{O}\Big{(}R\sqrt{\frac{d}{N_{1}}}\Big{)}\) with high probability from Lemma 13. For the second term, we observe that

\[\left\|\mathbb{E}_{\bm{x}}[f_{*}^{t}(\bm{x})\mathbb{I}_{f_{*}^{t}( \bm{x})>R}\sigma_{b}^{\prime}(\bm{w}^{\top}\bm{x})\bm{x}]\right\| \leq\mathbb{E}_{\bm{x}}[\|f_{*}^{t}(\bm{x})\mathbb{I}_{f_{*}^{t} (\bm{x})>R}\sigma_{b}^{\prime}(\bm{w}^{\top}\bm{x})\bm{x}]\|\] \[\leq\mathbb{E}_{\bm{x}}[f_{*}^{t}(\bm{x})^{2}]^{1/2}\mathbb{E}_{ \bm{x}}[(\sigma_{b}^{\prime}(\bm{w}^{\top}\bm{x})\|\bm{x}])^{2}]^{1/2}\mathbb{E}_ {\bm{x}}[(\mathbb{I}_{f_{*}^{t}(\bm{x})>R})^{4}]^{1/4}\] \[\leq\mathbb{E}_{\bm{x}}[f_{*}^{t}(\bm{x})^{2}]^{1/2}\mathbb{E}_{ \bm{x}}[\|\bm{x}\|^{2}]^{1/2}\mathbb{E}_{\bm{x}}[(\mathbb{I}_{f_{*}^{t}(\bm{x})>R })]^{1/4}\] \[\leq O(\sqrt{d}\cdot d^{-C/4})\]

for sufficiently large \(C\); hence this term can be ignored. Finally, \(\varsigma_{i}\sigma_{b}^{\prime}(\bm{w}^{\top}\bm{x}_{i}^{t})\langle\bm{x}_{i}^{t},\bm{u}\rangle\) is \(\tau^{2}\)-sub Gaussian for any \(\|\bm{u}\|=1\) and then \(\|\frac{1}{N_{1}}\sum_{i=1}^{N_{1}}\varsigma\sigma_{b}^{\prime}(\bm{w}^{\top}\bm{x}_ {i}^{t})\bm{x}_{i}^{t}\|=\tilde{O}(\sqrt{d/N_{1}})\) with high probability. 

By similar procedure, we can obtain a bound of \(\|\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t= 1}^{T_{1}})-\bm{g}(\bm{w},b)\|\).

**Lemma 27**.: _Fix any \(b\in[-1,1]\) and \(\bm{w}\in\mathbb{S}^{d-1}\) satisfying the conditions (B.8)-(B.10). Then, with high probability over the distribution of \(\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}}\),_

\[\|\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t =1}^{T_{1}})-\bm{g}(\bm{w},b)\| =\tilde{O}\Bigg{(}\sqrt{\frac{r^{Q-1}}{d^{Q-1}}}\sqrt{\frac{d}{T_{ 1}}}+\sqrt{\frac{d^{2}}{N_{1}T_{1}}}\Bigg{)},\]

\[\|(\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_ {t=1}^{T_{1}})-\bm{g}(\bm{w},b))_{1:r}\| =\tilde{O}\Bigg{(}\sqrt{\frac{r^{Q-1}}{d^{Q-1}}}\sqrt{\frac{r}{T _{1}}}+\sqrt{\frac{r^{2}}{N_{1}T_{1}}}\Bigg{)},\]

_holds._

Proof.: In this proof, the term "with high probability" indicates the probability with respect to the distribution of \(\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}}\). Let

\[\bm{z}_{1}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}}) =\sum_{t=1}^{T_{1}}\frac{1}{T_{1}}y^{t}\sigma_{b}(\bm{w}^{\top}\bm{x}^{t}) \frac{1}{N_{1}}\sum_{i=1}^{N_{1}}y^{t}_{i}\sigma^{\prime}_{b}(\bm{w}^{\top}\bm {x}^{t}_{i})\bm{x}^{t}_{i}\]

and

\[\bm{z}_{2}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}} )=\sum_{t=1}^{T_{1}}\frac{1}{T_{1}}y^{t}\sigma^{\prime}_{b}(\bm{w}^{\top}\bm{ x}^{t})\bm{x}^{t}\frac{1}{N_{1}}\sum_{i=1}^{N_{1}}y^{t}_{i}\sigma_{b}(\bm{w}^{ \top}\bm{x}^{t}_{i}).\]

Note that \(\bm{g}_{T_{1},N_{1}}=\bm{z}_{1}+\bm{z}_{2}\) holds.

Let us consider \(\bm{z}_{1}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})\). Since we assume (B.8)-(B.10) hold, by taking union bound for Lemma 26 with respect to \(t\in[T_{1}]\), with high probability \(\|\frac{1}{N_{1}}\sum_{i=1}^{N_{1}}y^{t}_{i}\sigma^{\prime}_{b}(\bm{w}^{\top} \bm{x}^{t}_{i})\bm{x}^{t}_{i}\|\leq R=\tilde{O}\Bigg{(}\sqrt{\frac{d}{N_{1}}} +\sqrt{\frac{r^{Q-1}}{d^{Q-1}}}\Bigg{)}\) for each \(t\). Moreover, from Corollary 17, \(|y^{t}|\leq R^{\prime}=\tilde{O}(1)\) for each \(t\). Here, we notice that the procedure in the proof of Lemma 26 can be applied, yielding

\[\|\bm{z}_{1}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\} _{t=1}^{T_{1}})-\mathbb{E}[\bm{z}_{1}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x }^{t},y^{t})\}_{t=1}^{T_{1}})]\|\] \[= \tilde{O}\bigg{(}RR^{\prime}\sqrt{d/T_{1}}\bigg{)}\] \[= \tilde{O}\Bigg{(}\Bigg{(}\sqrt{\frac{r^{Q-1}}{d^{Q-1}}}+\sqrt{ \frac{d}{N_{1}}}\Bigg{)}\sqrt{\frac{d}{T_{1}}}\Bigg{)}\]

with high probability. Similarly we can obtain

* \(\|\bm{z}_{2}-\mathbb{E}[\bm{z}_{2}]\|=\tilde{O}\Bigg{(}\Big{(}\sqrt{\frac{r^{ Q}}{d^{Q}}}+\sqrt{\frac{1}{N_{1}}}\Big{)}\sqrt{\frac{d}{T_{1}}}\Bigg{)}\),
* \(\|(\bm{z}_{1}-\mathbb{E}[\bm{z}_{1}])_{1:r}\|=\tilde{O}\bigg{(}\Big{(}\sqrt{ \frac{r^{Q-1}}{d^{Q-1}}}+\sqrt{\frac{r}{N_{1}}}\Big{)}\sqrt{\frac{r}{T_{1}}} \Bigg{)}\), and
* \(\|(\bm{z}_{2}-\mathbb{E}[\bm{z}_{2}])_{1:r}\|=\tilde{O}\bigg{(}\Big{(}\sqrt{ \frac{r^{Q}}{d^{Q}}}+\sqrt{\frac{1}{N_{1}}}\Big{)}\sqrt{\frac{r}{T_{1}}} \Bigg{)}\),

which concludes the proof. 

We need to obtain a bound uniformly over \(b\in[-1,1]\). We first introduce the following definition.

**Definition 28**.: _Fix any \(\bm{w}\in\mathbb{S}^{d-1}\) and \(X=\bigcup_{t=1}^{T_{1}}\{\bm{x}^{t}_{1},\ldots,\bm{x}^{t_{N_{1}}}_{N_{1}},\bm{x} ^{t}\}\subset\mathbb{R}^{d}\). Then, define a finite disjoint partition \(\mathcal{B}(\bm{w},X)=\{B_{1},\ldots,B_{N(\bm{w},X)}\}\) of \([-1,1]\), i.e., \(\cup_{i}B_{i}=[-1,1]\) and \(B_{i}\cap B_{j}=\emptyset\,(i\neq j)\) as follows: \(b\) and \(b^{\prime}\) belong to the same \(B_{i}\) if and only if \(\operatorname{sign}(\langle\bm{w},\bm{x}\rangle+b)=\operatorname{sign}(\langle \bm{w},\bm{x}\rangle+b^{\prime})\) for all \(\bm{x}\in X\)._

It is clear that \(|\mathcal{B}(\bm{w},X)|\lesssim|X|\leq(N_{1}+1)T_{1}\) holds because it suffices to divide \([-1,1]\) at the point satisfying \(\langle\bm{w},\bm{x}\rangle+b=0\) for each \(\bm{x}\in X\).

**Lemma 29**.: _Fix any \(\bm{w}\in\mathbb{S}^{d-1}\). Then, with high probability over the distribution of \(\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}}\), the following holds:_* \(g(\bm{w},b)\) _is_ \(L_{1}\)_-Lipschitz continuous with respect to_ \(b\) _in the interval_ \([-1,1]\)_._
* _Let_ \(X=\bigcup_{t=1}^{T_{1}}\{\bm{x}_{1}^{t},\ldots,\bm{x}_{N_{1}}^{t},\bm{x}^{t}\}\)_. Then,_ \(g_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{ 1}})\) _is_ \(L_{2}\)_-Lipschitz continuous with respect to_ \(b\) _in each interval_ \(B_{i}\in\mathcal{B}(\bm{w},X)\)_._

_Here \(L_{1}=O(1)\) and \(L_{2}=\tilde{O}(\sqrt{d})\) do not depend on \(\bm{w}\)._

**Proof.** By the explicit form of \(g(\bm{w},b)\) (B.6), we notice that each term is made by taking the product between the term as \(a_{j}(b)a_{k}(b)\) and a vector independent of \(b\) whose norm is \(O(1)\). Thus, it suffices to show that \(a_{j}(b)a_{k}(b)\) is \(O(1)\)-Lipschitz. This follows from the fact \(a_{j}=\pm\frac{e^{-b^{2}/2}}{\sqrt{2\pi}}\mathrm{He}_{j-2}(b)\) (see the proof of Lemma 20) and \(\mathrm{He}_{j-2}(b)\mathrm{e}^{-b^{2}/2}\) is \(O(1)\)-Lipschitz continuous on the bounded set \([-1,1]\).

Next, let us see \(\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t =1}^{T_{1}})\). Recall that

\[\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})\] \[\coloneqq\sum_{i=1}^{T_{1}}\frac{1}{T_{1}}y^{t}\sigma_{b}(\bm{w} ^{\top}\bm{x}^{t})\frac{1}{N_{1}}\sum_{i=1}^{N_{1}}y^{t}_{i}\sigma^{\prime}_{b }(\bm{w}^{\top}\bm{x}^{t}_{i})\bm{x}^{t}_{i}+\sum_{i=1}^{T_{1}}\frac{1}{T_{1} }y^{t}\sigma^{\prime}_{b}(\bm{w}^{\top}\bm{x}^{t})\bm{x}^{t}\frac{1}{N_{1}} \sum_{i=1}^{N_{1}}y^{t}_{i}\sigma_{b}(\bm{w}^{\top}\bm{x}^{t}_{i}).\]

From Lemma 10 and Corollary 17, we know that \(|y^{t}_{i}|,|y^{t}|\lesssim(\log d)^{P/2}\) and \(\|\bm{x}^{t}_{i}|,\|\bm{x}^{t}\|\lesssim\sqrt{d}\) holds for all \(i,t\) with high probability. Assuming this and noting that \(\sigma\) is \(1\)-Lipschitz, in each interval in \(\mathcal{B}(\bm{w},X)\), \(\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{ t=1}^{T_{1}})\) is \(L_{2}=\tilde{O}(\sqrt{d})\)-Lipschitz. 

**Corollary 30**.: _With high probability over the distribution of \(\bm{w}\) and \(\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}}\),_

\[\sup_{b\in[-1,1]}\|\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t}, \bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})-\bm{g}(\bm{w},b)\|=\tilde{O} \Bigg{(}\sqrt{\frac{r^{Q-1}}{d^{Q-1}}}\sqrt{\frac{d}{T_{1}}}+\sqrt{\frac{d^{2} }{N_{1}T_{1}}}\Bigg{)},\] \[\sup_{b\in[-1,1]}\|(\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t}, \bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})-\bm{g}(\bm{w},b))_{1:\tau}\|= \tilde{O}\Bigg{(}\sqrt{\frac{r^{Q-1}}{d^{Q-1}}}\sqrt{\frac{r}{T_{1}}}+\sqrt{ \frac{r^{2}}{N_{1}T_{1}}}\Bigg{)}\] (B.11)

_holds._

**Proof.** Let \(X=\bigcup_{t=1}^{T_{1}}\{\bm{x}^{t}_{1},\ldots,\bm{x}^{t}_{N_{1}},\bm{x}^{t}\}\) and consider \(\mathcal{B}(\bm{w},X)=\{B_{1},\ldots,B_{N(\bm{w},X)}\}\). Then, let \(B\) be the union of \(\tilde{O}\Bigg{(}\bigg{(}\sqrt{\frac{r^{Q-1}}{d^{Q-1}}}\sqrt{\frac{r}{T_{1}}}+ \sqrt{\frac{r^{2}}{N_{1}T_{1}}}\bigg{)}/\sqrt{d}\bigg{)}\)-coverings of \(B_{i}\in\mathcal{B}(\bm{w},X)\). Since \(|B|\) is polynomial in \(d\), Lemma 27 holds with high probability uniformly over \(b\in B\).

Moreover, from the construction and Lemma 29, for \(b\in[-1,1]\setminus B\), there exists \(\pi(b)\in B\) such that

\[\|\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t },y^{t})\}_{t=1}^{T_{1}})-\bm{g}(\bm{w},b)\|\] \[\quad-\|\bm{g}_{T_{1},N_{1}}(\bm{w},\pi(b),\{(\bm{X}^{t},\bm{y}^{t },\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})-\bm{g}(\bm{w},\pi(b))\|\] \[\leq \tilde{O}\Bigg{(}\sqrt{\frac{r^{Q-1}}{d^{Q-1}}}\sqrt{\frac{r}{T_ {1}}}+\sqrt{\frac{r^{2}}{N_{1}T_{1}}}\Bigg{)}\]

holds. 

**Lemma 31**.: _With high probability over the distribution of \(\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}}\),_

* _It holds that_ \[\sup_{b\in[-1,1]}\|\bm{g}(\bm{w},b)-\bm{g}_{T_{1},N_{1}}(\bm{w},b, \{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})\|\] \[= \tilde{O}\Bigg{(}\sqrt{\frac{r^{Q-1}}{d^{Q-1}}}\sqrt{\frac{d}{T_ {1}}}+\sqrt{\frac{d^{2}}{N_{1}T_{1}}}\Bigg{)}\] (B.12) _with high probability over_ \(\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})\)_._* _It holds that_ \[\sup_{b\in[-1,1]}\mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[ \|\bm{g}(\bm{w},b)-\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x }^{t},y^{t})\}_{t=1}^{T_{1}})\|^{j}]^{1/j}\] \[= \tilde{O}\Bigg{(}\sqrt{\frac{r^{Q-1}}{d^{Q-1}}}\sqrt{\frac{r}{T_{1 }}}+\sqrt{\frac{r^{2}}{N_{1}T_{1}}}\Bigg{)}\] (B.13) _for_ \(j\in[4P]\)_._

**Proof.** If an event \(A\) occurs with probability at least \(1-d^{-C}\) over the simultaneous distribution of \((\bm{w},\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})\), then we can say that

* with probability at least \(1-d^{C/2}\) over \(\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}}\), \(\mathbb{P}_{\bm{w}}\{A\mid\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{ T_{1}}\}\geq 1-d^{-C/2}\) holds.

(B.12) follows from this remark and Corollary 30.

For (B.13), with high probability over the distribution of \(\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}}\), \(\mathbb{P}_{\bm{w}}\{\text{(B.11)}\mid\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t })\}_{t=1}^{T_{1}}\}\geq 1-d^{-\tilde{C}}\) is satisfied for some \(\tilde{C}\). Also, from Lemma 10 and Corollary 17, with high probability over the distribution of \(\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}}\), we have \(\|\bm{x}_{t}^{t}\|,\|\bm{x}^{t}\|=O(\sqrt{d})\) and \(|y^{t}|=\tilde{O}(1)\). From the definition of \(\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t= 1}^{T_{1}})\) (B.2) and expansion of \(\bm{g}(\bm{w},b)\) (B.6), \(\sup_{\bm{w}}\|\bm{g}(\bm{w},b)-\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t}, \bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})\|\leq\sup_{\bm{w}}\|\bm{g}(\bm{ w},b)\|+\sup_{\bm{w}}\|\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t}, \bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})\|\) is upper bounded by \(\tilde{O}(d)\). Therefore, with high probability

\[\sup_{b\in[-1,1]}\mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S }^{d-1})}[\|\bm{g}(\bm{w},b)-\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y }^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})\|^{j}]^{1/j}\] \[\leq \mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}\Bigg{[} \Bigg{(}\sup_{b\in[-1,1]}\|\bm{g}(\bm{w},b)-\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{( \bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})\|\Bigg{)}^{j}\Bigg{]} ^{1/j}\] \[= \tilde{O}\Bigg{(}\Bigg{(}\Big{(}1-d^{-\tilde{C}}\Big{)}\Bigg{(} \sqrt{\frac{r^{Q-1}}{d^{Q-1}}}\sqrt{\frac{r}{T_{1}}}+\sqrt{\frac{r^{2}}{N_{1}T _{1}}}\Bigg{)}^{j}+\Big{(}d^{-\tilde{C}}\Big{)}d^{j}\Bigg{)}^{1/j}\Bigg{)}.\]

By setting sufficiently large \(\tilde{C}\), we arrive at the claim. \(\Box\)

Summary.We combine the obtained results and show that \(\|\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_ {t=1}^{T_{1}})\|\) and the moment of the residual terms are bounded with high probability.

**Corollary 32**.: _With high probability over the distribution of \(\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}}\),_

* \(\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t= 1}^{T_{1}})\ =\ \bm{m}(\bm{w},b)\ +\ \bm{r}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})\) _holds where_ \[\bm{m}(\bm{w},b)\coloneqq\frac{2a_{Q}(b)^{2}\mathbb{E}_{c_{Q}}[c_{Q}^{2}]}{Q!(Q- 1)!}\frac{(2Q-1)!!}{\mathbb{E}_{z\sim\chi_{r}}[z^{2Q}]}\|\bm{w}_{1:r}\|^{2Q-2 }\begin{bmatrix}\bm{w}_{1:r}\\ \bm{0}_{d-r}\end{bmatrix},\] _and_ \[\sup_{b\in[-1,1]}\mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d- 1})}\bigg{[}\Big{\|}\Big{(}\bm{r}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y ^{t})\}_{t=1}^{T_{1}})\Big{)}_{1:r}\Big{\|}^{j}\bigg{]}^{1/j}\] \[= \tilde{O}_{d,r}\Bigg{(}\sqrt{\frac{r^{Q-1}}{d^{Q-1}}}\sqrt{\frac{r} {T_{1}}}+\sqrt{\frac{r^{2}}{N_{1}T_{1}}}+\sqrt{\frac{r}{d^{2Q+1}}}\Bigg{)}.\]* _If_ \(\eta_{1}\gamma\leq\sqrt{rd^{2Q-1}}/C_{1}(\log d)^{C_{2}}\) _for sufficiently large_ \(C_{1}\) _and_ \(C_{2}\)_,_ \(T_{1}\gtrsim r^{Q}d^{Q+1}\)_, and_ \(N_{1}T_{1}\gtrsim rd^{2Q+1}\)_, then_ \[\sup_{b\in[-1,1]}\|2\eta_{1}\gamma\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t}, \bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})\|\leq 1\] _holds with high probability over_ \(\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})\)_. Moreover,_ \[\sup_{b\in[-1,1]}\left|\left\langle 2\eta_{1}\gamma\bm{g}_{T_{1},N_{1}}(\bm{w},b, \{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}},\bm{z})\right\rangle \right|\leq\log d\] _holds with high probability over_ \(\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})\) _and the training data for the second stage_ \(\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=T_{1}+1}^{T_{2}}\)_, where_ \(\bm{z}\in\bigcup_{t=T_{1}+1}^{T_{2}}\{\bm{x}^{t}_{1},\ldots,\bm{x}^{t}_{N_{2}},\bm{x}^{t}\}\)_._

Proof.: Combining Lemmas 23 and 31 yields (I). For (II), by the condition for \(T_{1}\) and \(N_{1}\), we obtain

\[\sup_{b\in[-1,1]}\|\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t}, \bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})\| \leq\tilde{O}_{d,r}\Bigg{(}\sqrt{\frac{r^{Q-1}}{d^{Q-1}}}\sqrt{ \frac{d}{T_{1}}}+\sqrt{\frac{d^{2}}{N_{1}T_{1}}}+\sqrt{\frac{1}{rd^{2Q-1}}} \Bigg{)}\] \[=\tilde{O}_{d,r}\Bigg{(}\sqrt{\frac{1}{rd^{2Q-1}}}\Bigg{)}\]

from Lemmas 24 and 31. Then, as \(\eta_{1}\gamma\lesssim\sqrt{rd^{2Q-1}}/C_{1}(\log d)^{C_{2}}\), \(\sup_{b\in[-1,1]}\|2\eta_{1}\gamma\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}})\|\leq 1\) is achieved.

By an argument similar to the one in the proof of Lemma 18, we obtain that with high probability, \(\left|\left\langle 2\eta_{1}\gamma\bm{g}_{T_{1},N_{1}}(\bm{w},b,\{(\bm{X}^{t}, \bm{y}^{t},\bm{x}^{t},y^{t})\}_{t=1}^{T_{1}},\bm{z})\right\rangle\right| \lesssim\log d\) for \(\bm{z}\in\bigcup_{t=T_{1}+1}^{T_{2}}\{\bm{x}^{t}_{1},\ldots,\bm{x}^{t}_{N_{2} },\bm{x}^{t}\}\). To obtain the supremum bound over \(b\in[-1,1]\), it suffices to utilize Lemma 29 again. 

In the following sections, we assume that these two events (I) and (II) occur.

## Appendix C Construction of Attention Matrix

We construct an attention matrix \(\bar{\bm{\Gamma}}\) with a good approximation property: see Section 3.2.2 for the proof outline.

In this section, a set of basis functions \(\mathcal{H}\) is defined as

\[\mathcal{H}=\Bigg{\{}\bm{x}\mapsto\prod_{j=1}^{r}\frac{1}{\sqrt{j!}}\mathrm{ He}_{p_{j}}(x_{j})\Bigg{|}Q\leq p_{1}+\cdots+p_{r}\leq P,p_{1}\geq 0, \ldots,p_{r}\geq 0\Bigg{\}}.\]

We let \(B_{P}\coloneqq|\mathcal{H}|\) and introduce a numbering \(\mathcal{H}=\{h_{1},\ldots,h_{B_{P}}\}\). Note that

\[B_{P}=\sum_{i=Q}^{P}\binom{r+i-1}{i}=\Theta(r^{P}).\]

### Approximation Error of Two-layer Neural Network

As sketched in Section 3.2.2, we need to show that there is a set of two-layer neural networks which can approximate basis functions well. The goal of this section is to show the following proposition.

**Proposition 33**.: _Assume \(T_{1}=\tilde{\Omega}(d^{Q+1}r^{Q})\) and \(N_{1}T_{1}=\tilde{\Omega}(d^{2Q+1}r)\). Let \(\{\bm{w}^{(1)}_{j}\}_{j}\) be neurons obtained by line 4 of Algorithm 1 with \(\gamma\asymp\frac{1}{m^{\frac{3}{2}}r\frac{1}{2}d^{Q}}\),\(\eta_{1}\asymp m^{\frac{3}{2}}rd^{2Q-\frac{1}{2}}\cdot(\log d)^{-C_{\eta}}\) for constant \(C_{\eta}\). Assume \(\bm{b}\) is re-initialized in Line 5 of Algorithm 1. Then, with high probability over the distribution of training data and the initialization of model parameters, there exist vectors \(\bm{a}^{1},\ldots,\bm{a}^{B_{P}}\in\mathbb{R}^{m}\) such that for each \(n\in[B_{P}]\),_

\[\sup_{i\in[N_{2}],T_{1}+1\leq t\leq T_{2}}\left(\sum_{j=1}^{m}a_{j}^{n}\sigma( \bm{w}^{(1)}_{j}{}^{\top}\bm{x}^{t}_{i}+b_{j})-h_{n}(\bm{x}^{t}_{i})\right)^{2 }=\tilde{O}\bigg{(}\frac{r^{P}}{m}+\frac{1}{N_{2}}\bigg{)},\]\[\sup_{T_{1}+1\leq t\leq T_{2}}\left(\sum_{j=1}^{m}a_{j}^{n}\sigma({\bm{w}}_{j}^{(1) }\gamma^{t}{\bm{x}}^{t}+b_{j})-h_{n}({\bm{x}}^{t})\right)^{2}=\tilde{O}\!\left( \frac{r^{P}}{m}+\frac{1}{N_{2}}\right)\]

_holds. Moreover, \(\sup_{n\in[B_{P}]}\|{\bm{a}}^{n}\|^{2}=\tilde{O}\!\left(\frac{r^{P}}{m}\right)\)._

This proposition states that there exists a two-layer neural network \(\sum_{j=1}^{m}a_{j}^{n}\sigma\!\left(\langle{\bm{w}}_{j}^{(1)},\cdot\rangle+b_ {j}\right)\) which can approximate the basis function \(h_{n}\), and its second-layer magnitude \(\|{\bm{a}}^{n}\|^{2}\) only scales with \(r\). Note that the condition of \(T_{1}\) and \(N_{1}T_{1}\) comes from Corollary 32, which implies that the noise terms of the one-step gradient is well controlled.

#### c.1.1 Alignment to \(\mathcal{S}\) and Efficient Approximation

From now on, we suppose that \({\bm{g}}_{T_{1},N_{1}}({\bm{w}},b)\) satisfies the conditions in Corollary 32 (to simplify the notation, we drop the dependency on \(\{({\bm{X}}^{t},{\bm{y}}^{t},{\bm{x}}^{t},y^{t})\}_{t=1}^{T_{1}}\)). An important previous result [10] is that, if \({\bm{g}}_{T_{1},N_{1}}({\bm{w}},b)\) aligns with the \(r\)-dimensional subspace \(\mathcal{S}\), then it can be used to approximate polynomials on \(\mathcal{S}\) efficiently.

**Lemma 34** (Lemma 21 in [10]).: _There exists a set of symmetric \(r\)-dimensional tensors \(\{{\bm{T}}_{k}^{n}\}_{0\leq k\leq P,n\in[B_{P}]}\) such that_

\[h_{n}({\bm{x}})=\sum_{0\leq k\leq P}\langle{\bm{T}}_{k}^{n},{\bm{x}}_{1:r}^{ \otimes k}\rangle\]

_and \(\|{\bm{T}}_{k}^{n}\|_{F}\lesssim r^{\frac{P-k}{4}}\)._

**Definition 35** (\(f(r,d)\)-alignment).: _We call that \({\bm{g}}_{T_{1},N_{1}}({\bm{w}},b)\) has \(f(r,d)\)-alignment with \(\mathcal{S}\), if the following holds:_

\(\bullet\) _For any_ \(0\leq k\leq P\)_-symmetric_ \(r\)_-dimensional tensor_ \({\bm{T}}\)_,_

\[\mathbb{E}_{{\bm{w}}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[\langle{\bm{T}},{ \bm{g}}_{T_{1},N_{1}}({\bm{w}},b)_{1:r}^{\otimes k}\rangle^{2}]\gtrsim f(r,d) ^{k}\|{\bm{T}}\|_{F}^{2},\]

_and the hidden constant in "\(\gtrsim\)" needs to be uniformly lower bounded._

**Lemma 36** (Corollary 4 in [10], adapted).: _Suppose that \({\bm{g}}_{T_{1},N_{1}}({\bm{w}},b)\) has \(f(r,d)\)-alignment with \(\mathcal{S}\). Let \({\bm{T}}\) be a \(0\leq k\leq P\)-symmetric \(r\)-dimensional tensor. Then, there exists \(\psi_{\bm{T}}({\bm{w}},b)\) such that_

\[\mathbb{E}_{{\bm{w}}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[\psi_{ \bm{T}}({\bm{w}},b)\langle{\bm{g}}_{T_{1},N_{1}}({\bm{w}},b),{\bm{x}}\rangle^{ k}] =\langle{\bm{T}},{\bm{x}}_{1:r}^{\otimes k}\rangle,\] \[\mathbb{E}_{{\bm{w}}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[\psi_{ \bm{T}}({\bm{w}},b)^{2}] \lesssim f(r,d)^{-k}\|{\bm{T}}\|_{F}^{2},\] \[|\psi_{\bm{T}}({\bm{w}},b)| \lesssim f(r,d)^{-k}\|{\bm{T}}\|_{F}\|{\bm{g}}_{T_{1},N_{1}}({\bm {w}},b)\|^{k}.\]

Intuitively speaking, \(f(r,d)\)-alignment ensures that the MLP gradient \({\bm{g}}_{T_{1},N_{1}}({\bm{w}},b)\) has sufficiently large component on \(\mathcal{S}\). If so, the lemma states that, an infinite-width neural network \(\mathbb{E}_{{\bm{w}}}[\psi_{\bm{T}}({\bm{w}},b)\langle{\bm{g}}_{T_{1},N_{1}}({ \bm{w}},b),\cdot\rangle^{k}]\), whose hidden layer weight is \({\bm{g}}_{T_{1},N_{1}}({\bm{w}},b)\) and the activation is \(z\mapsto z^{k}\), can express the functions \(\langle{\bm{T}},{\bm{x}}_{1:r}^{\otimes k}\rangle\) with small output layer \(\psi_{\bm{T}}({\bm{w}},b)\).

Let us show that our \({\bm{g}}_{T_{1},N_{1}}({\bm{w}},b)\) has sufficient alignment if \(b\) satisfies (B.5).

**Lemma 37** (Tensor expectation lower bound).: _Let \({\bm{T}}\) be a \(k\leq P\)-symmetric \(r\)-dimensional tensor and \(\bar{{\bm{w}}}={\bm{w}}_{1:r}\|{\bm{w}}_{1:r}\|^{2Q-2}\). Then_

\[\mathbb{E}_{{\bm{w}}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[{\bm{T}}(\bar{{\bm{w} }}^{\otimes k})^{2}]\gtrsim\frac{r^{Qi}}{d^{(Q+1)i}}\mathbb{E}[\|{\bm{T}}(\bar{{ \bm{w}}}^{\otimes k-i})\|_{F}^{2}]\]

_holds for all \(0\leq i\leq k\). Here \({\bm{T}}(\bar{{\bm{w}}}^{\otimes 0})\coloneqq{\bm{T}}\)._

**Proof.** Let \({\bm{u}}\sim\mathcal{N}(0,I_{d})\), \(z\sim\chi(d)\) and \(\bar{{\bm{u}}}={\bm{u}}_{1:r}\|{\bm{u}}_{1:r}\|^{2Q-2}\). We can decompose \(\bar{{\bm{u}}}\) as \(\bar{{\bm{u}}}=z^{2Q-1}\bar{{\bm{w}}}\). Therefore

\[\mathbb{E}[{\bm{T}}(\bar{{\bm{u}}}^{\otimes k})^{2}]=\mathbb{E}[z^{(4Q-2)k}] \mathbb{E}[{\bm{T}}(\bar{{\bm{w}}}^{\otimes k})^{2}]\]holds. On the other hand, let \(\bm{x}\sim\mathbb{S}^{r-1}\) and \(z^{\prime}\sim\chi(r)\). Then we can decompose \(\bar{\bm{u}}\) as \(\bar{\bm{u}}=(z^{\prime})^{2Q-1}\bm{x}\). Thus

\[\mathbb{E}[\bm{T}(\bar{\bm{u}}^{\otimes k})^{2}]=\mathbb{E}[(z^{\prime})^{(4Q-2 )k}]\mathbb{E}[\bm{T}(\bm{x}^{\otimes k})^{2}]\]

holds. It implies that

\[\mathbb{E}[\bm{T}(\bar{\bm{w}}^{\otimes k})^{2}]=\frac{\mathbb{E}[(z^{\prime} )^{(4Q-2)k}]}{\mathbb{E}[z^{(4Q-2)k}]}\mathbb{E}[\bm{T}(\bm{x}^{\otimes k})^{2 }].\]

Similarly,

\[\mathbb{E}[\|\bm{T}(\bar{\bm{w}}^{\otimes k-i})\|_{F}^{2}]=\frac{\mathbb{E}[(z ^{\prime})^{(4Q-2)(k-i)}]}{\mathbb{E}[z^{(4Q-2)(k-i)}]}\mathbb{E}[\|\bm{T}( \bm{x}^{\otimes(k-i)})\|_{F}^{2}]\]

is satisfied. Now from Corollary 13 in [10],

\[\mathbb{E}[\|\bm{T}(\bm{x}^{\otimes(k-i)})\|_{F}^{2}]\lesssim r^{i}\mathbb{E }[\bm{T}(\bm{x}^{\otimes k})^{2}]\]

holds. Therefore we obtain

\[\mathbb{E}[\bm{T}(\bar{\bm{w}}^{\otimes k})^{2}] =\frac{\mathbb{E}[(z^{\prime})^{(4Q-2)k}]}{\mathbb{E}[z^{(4Q-2)k }]}\mathbb{E}[\bm{T}(\bm{x}^{\otimes k})^{2}]\] \[\gtrsim r^{-i}\frac{\mathbb{E}[(z^{\prime})^{(4Q-2)k}]}{\mathbb{E }[z^{(4Q-2)k}]}\mathbb{E}[\|\bm{T}(\bm{x}^{\otimes(k-i)})\|_{F}^{2}]\] \[=r^{-i}\frac{\mathbb{E}[(z^{\prime})^{(4Q-2)k}]}{\mathbb{E}[z^{(4 Q-2)k}]}\frac{\mathbb{E}[z^{(4Q-2)(k-i)}]}{\mathbb{E}[(z^{\prime})^{(4Q-2)(k-i)}]} \mathbb{E}[\|\bm{T}(\bar{\bm{w}}^{\otimes k-i})\|_{F}^{2}]\] \[=\frac{r^{(2Q-2)i}}{d^{(2Q-1)i}}\mathbb{E}[\|\bm{T}(\bar{\bm{w}}^ {\otimes k-i})\|_{F}^{2}].\]

**Corollary 38**.: _Let \(\bm{T}\) be a \(k\leq P\)-symmetric \(r\)-dimensional tensor and let \(\bm{m}(\bm{w},b)_{1:r}=\frac{2a_{Q}(b)^{2}}{Q!(Q-1)!}\frac{\mathbb{E}_{c_{0}} [c_{0}^{2}](2Q-1)!!}{\mathbb{E}_{s\sim r}[z^{2Q}]}\|\bm{w}_{1:r}\|^{2Q-2}\bm{ w}_{1:r}\). Moreover, assume that (B.5) holds. Then,_

\[\mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[\langle\bm{T},\bm{m}( \bm{w},b)_{1:r}^{\otimes k}\rangle^{2}]\gtrsim\frac{1}{d^{(2Q-1)i}r^{2i}} \mathbb{E}[\|\bm{T}(\bm{m}(\bm{w},b)_{1:r}^{\otimes k-i})\|_{F}^{2}]\]

_holds for all \(0\leq i\leq k\)._

Proof.: This is obtained by calibrating the coefficients from the previous lemma. 

**Lemma 39**.: _Let \(\bm{T}\) be a \(0\leq k\leq P\)-symmetric \(r\)-dimensional tensor. Assume that (B.5) holds. Furthermore, suppose \(r\lesssim d^{\frac{1}{2}}\), \(T_{1}=\tilde{\Omega}(d^{Q+1}r^{Q})\) and \(N_{1}T_{1}=\tilde{\Omega}(d^{2Q+1}r)\). Then,_

\[\mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[\langle\bm{T},\bm{g}_{T_ {1},N_{1}}(\bm{w},b)_{1:r}^{\otimes k}\rangle^{2}]\gtrsim\frac{1}{d^{(2Q-1)k}r ^{2k}}\|\bm{T}\|_{F}^{2}\]

_holds for all \(0\leq k\leq P\). Therefore, \(\bm{g}_{T_{1},N_{1}}(\bm{w},b)\) has \(d^{-2Q+1}r^{-2}\)-alignment with \(\mathcal{S}\)._

Proof.: Decompose \(\bm{g}_{T_{1},N_{1}}(\bm{w},b)=\bm{m}(\bm{w},b)+\bm{r}(\bm{w},b)\) as defined in Corollary 32. From the proof of Lemma 11 in [10], we can show that

\[\mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[\langle\bm {T},\bm{g}_{T_{1},N_{1}}(\bm{w},b)_{1:r}^{\otimes k}\rangle^{2}]\] \[\geq \mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}\bigg{[} \frac{1}{2}\langle\bm{T},\bm{m}(\bm{w},b)_{1:r}^{\otimes k}\rangle^{2}\bigg{]} -\mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[\delta(\bm{w},b)^{2}]\] (C.1)

for some \(\delta(\bm{w},b)\) satisfying

\[\mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[\delta( \bm{w},b)^{2}]\] \[\lesssim \sum_{i=1}^{k}\mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1 })}[\|\bm{T}(\bm{m}(\bm{w},b)_{1:r}^{\otimes k-i})\|_{F}^{2}]\sqrt{\mathbb{E}_ {\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[\|\bm{r}(\bm{w},b)_{1:r}\|^{4i}]}.\]Here, from Corollaries 32 and 38,

\[\mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[\delta(\bm{w},b )^{2}]\] \[\lesssim\sum_{i=1}^{k}d^{(2Q-1)i}r^{2i}\mathbb{E}_{\bm{w}\sim \mathrm{Unif}(\mathbb{S}^{d-1})}[\langle\bm{T},\bm{m}(\bm{w},b)_{1:r}^{\otimes k }\rangle^{2}]\tilde{O}_{d,r}\Bigg{(}\bigg{(}\frac{r^{Q-1}}{d^{Q-1}}\frac{r}{T_ {1}}+\frac{r^{2}}{N_{1}T_{1}}+\frac{r}{d^{2Q+1}}\bigg{)}^{i}\Bigg{)}\]

holds. Now, from the condition of \(N_{1}\) and \(T_{1}\), and \(r\lesssim\sqrt{d}\), we know

\[d^{(2Q-1)}r^{2}\bigg{(}\frac{r^{Q-1}}{d^{Q-1}}\frac{r}{T_{1}}+\frac{r^{2}}{N_{1 }T_{1}}+\frac{r}{d^{2Q+1}}\bigg{)}\lesssim 1.\]

Therefore we can achieve \(\mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[\delta(\bm{w},b)^{2}] \leq\frac{1}{4}\mathbb{E}_{\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1})}[ \langle\bm{T},\bm{m}(\bm{w},b)_{1:r}^{\otimes k}\rangle^{2}]\). Applying this and Corollary 38\((i=k)\) to (C.1) yields the result. 

#### c.1.2 Approximation Power of Infinite-width Neural Network

No we can show that some "infinite-width" neural network can approximate the functions \(h\in\mathcal{H}\).

**Lemma 40** (Lemma 9 in [10], adapted).: _Let \(a\sim\mathrm{Unif}(\{-1,1\})\) and \(b\sim\mathrm{Unif}([-\log d,\log d])\). Then, for any \(k\geq 0\) there exists \(v_{k}(a,b)\) such that for \(|x|\leq\log d\),_

\[\mathbb{E}_{a,b}[v_{k}(a,b)\sigma(ax+b)]=x^{k},\sup_{a,b}|v_{k}(a,b)|=\tilde{O }_{d}(1).\]

**Lemma 41** (Lemma 12 and Corollary 5 in [10], adapted).: _There exists \(\phi_{n}(a,\bm{w},b_{1},b_{2})\) such that if_

\[f_{\phi_{n}}(\bm{x})\coloneqq\mathbb{E}_{a,\bm{w},b_{1},b_{2}}[\phi_{n}(a,\bm {w},b_{1},b_{2})\sigma(2\eta_{1}a\bm{g}_{T_{1},N_{1}}(\bm{w},b_{1})+b_{2})]\]

_where_

\[a\sim\mathrm{Unif}\{\pm\gamma\},\bm{w}\sim\mathrm{Unif}(\mathbb{S}^{d-1}),b_ {1}\sim\mathrm{Unif}([-1,1]),b_{2}\sim\mathrm{Unif}([-\log d,\log d]),\]

_it holds that_

\[\sup_{i\in[N_{2}],T_{1}+1\leq t\leq T_{2}}\big{(}f_{\phi_{n}}( \bm{x}_{i}^{t})-h_{n}(\bm{x}_{i}^{t})\big{)}^{2} \lesssim\frac{1}{N_{2}},\] \[\sup_{T_{1}+1\leq t\leq T_{2}}\big{(}f_{\phi_{n}}(\bm{x}^{t})-h_{ n}(\bm{x}^{t})\big{)}^{2} \lesssim\frac{1}{N_{2}},\] \[\mathbb{E}_{a,\bm{w},b_{1},b_{2}}[\phi_{n}(a,\bm{w},b_{1},b_{2})^ {2}] =\tilde{O}(r^{P}),\] \[\sup_{a,\bm{w},b_{1},b_{2}}|\phi_{n}(a,\bm{w},b_{1},b_{2})| =\tilde{O}(r^{P})\]

_for each \(n\in[B_{P}]\)._

Proof.: We first construct \(\phi_{\bm{T}_{k}^{n}}(a,\bm{w},b_{1},b_{2})\) for basis tensors defined in Lemma 34, such that if

\[f_{\phi_{\bm{T}_{k}^{n}}}(\bm{x})\coloneqq\mathbb{E}_{a,\bm{w},b_{1},b_{2}}[ \phi_{\bm{T}_{k}^{n}}(a,\bm{w},b_{1},b_{2})\sigma(\langle 2\eta_{1}a\bm{g}_{T_{1},N_{1}}( \bm{w},b_{1}),\bm{x}\rangle+b_{2})],\]

it holds that

\[\sup_{i\in[N_{2}],T_{1}+1\leq t\leq T_{2}}\Bigl{(}f_{\phi_{\bm{T}_ {k}^{n}}}(\bm{x}_{i}^{t})-\bigl{\langle}\bm{T}_{k}^{n},(\bm{x}_{i}^{t})_{1:r}^{ \otimes k}\bigr{)}\Bigr{)}^{2} \lesssim\frac{1}{N_{2}},\] (C.2) \[\sup_{T_{1}+1\leq t\leq T_{2}}\Bigl{(}f_{\phi_{\bm{T}_{k}^{n}}}( \bm{x}^{t})-\bigl{\langle}\bm{T}_{k}^{n},(\bm{x}^{t})_{1:r}^{\otimes k} \bigr{)}\Bigr{)}^{2} \lesssim\frac{1}{N_{2}}.\] (C.3)

Let \(\phi_{\bm{T}_{k}^{n}}(a,\bm{w},b_{1},b_{2})=\frac{2v_{k}(a,b_{2})\psi_{\bm{T}_ {k}^{n}}(\bm{w},b_{1})}{(2\eta_{1})^{k\otimes k}_{\bm{w},b_{1}}}\mathbb{I}_{A( \bm{w})}\mathbb{I}_{B(b_{1})}\) where \(v_{k}\) and \(\psi_{\bm{T}_{k}^{n}}\) are constructed in Lemma 36 and the events \(A(\bm{w})\) and \(B(b_{1})\) are defined as

\[A(\bm{w})\coloneqq \bigg{\{}\sup_{b_{1}\in[-1,1]}2\eta_{1}\gamma|\bigl{\langle}\bm{g }_{T_{1},N_{1}}(\bm{w},b_{1}),\bm{x}_{i}^{t}\rangle|,\sup_{b_{1}\in[-1,1]}2 \eta_{1}\gamma|\bigl{\langle}\bm{g}_{T_{1},N_{1}}(\bm{w},b_{1}),\bm{x}^{t} \rangle|\leq\log d\]\[\text{for}\;i\in[N_{2}],T_{1}+1\leq t\leq T_{2}\Big{\}}\] \[\cap\left\{\sup_{b_{1}\in[-1,1]}\|2\eta_{1}\gamma\bm{g}_{T_{1},N_{1 }}(\bm{w},b_{1})\|\leq 1\right\}\] \[B(b_{1})\coloneqq\big{\{}\big{|}\mathbb{E}_{\tau\sim\mathcal{N}(0, 1)}[\sigma_{b_{1}}(z)\text{He}_{i}(z)]\big{|}\leq C_{H}\;(2\leq i\leq P+2) \big{\}}\] \[\qquad\cap\big{\{}\big{|}\mathbb{E}_{z\sim\mathcal{N}(0,1)}[ \sigma_{b_{1}}(z)\text{He}_{Q}(z)]\big{|}\geq C_{L}\big{\}}.\]

Then,

\[f_{\phi_{\bm{T}_{k}^{n}}}(\bm{x})\] \[= \mathbb{E}_{\bm{a},\bm{w},b_{1},b_{2}}\bigg{[}\frac{2v_{k}(a,b_{2 })\psi_{\bm{T}_{k}^{n}}(\bm{w},b_{1})}{(2\eta_{1})^{k}\gamma^{k}}\mathbb{I}_{ A(\bm{w})}\mathbb{I}_{B(b_{1})}\sigma(\langle 2\eta_{1}a\bm{g}_{T_{1},N_{1}}(\bm{w},b_{1} ),\bm{x}\rangle+b_{2})\bigg{]}\] \[= \mathbb{E}_{b_{1}}\bigg{[}2\cdot\mathbb{I}_{B(b_{1})}\mathbb{E}_ {\bm{w}}\bigg{[}\frac{\psi_{\bm{T}_{k}^{n}}(\bm{w},b_{1})}{(2\eta_{1})^{k} \gamma^{k}}\mathbb{I}_{A(\bm{w})}\mathbb{E}_{a^{\prime},b_{2}}[v_{k}(a,b_{2}) \sigma(\langle 2\eta_{1}a^{\prime}\gamma\bm{g}_{T_{1},N_{1}}(\bm{w},b_{1}),\bm{x} \rangle+b_{2})\big{]}\bigg{]}\bigg{]},\]

where \(a^{\prime}\sim\text{Unif}(\{\pm 1\})\). Here, if \(\bm{x}\in\bigcup_{t=T_{1}+1}^{T_{2}}\{\bm{x}_{1}^{t},\dots,\bm{x}_{N_{2}}^{t}, \bm{x}^{t}\}\),

\[\mathbb{E}_{\bm{w}}\bigg{[}\frac{\psi_{\bm{T}_{k}^{n}}(\bm{w},b_{ 1})}{(2\eta_{1})^{k}\gamma^{k}}\mathbb{I}_{A(\bm{w})}\mathbb{E}_{a^{\prime} \sim\text{Unif}(\{\pm 1\}),b_{2}}[v_{k}(a,b_{2})\sigma(\langle 2\eta_{1}a^{ \prime}\gamma\bm{g}_{T_{1},N_{1}}(\bm{w},b_{1}),\bm{x}\rangle+b_{2})]\bigg{]}\] \[= \mathbb{E}_{\bm{w}}\bigg{[}\psi_{\bm{T}_{k}^{n}}(\bm{w},b_{1}) \langle\bm{g}_{T_{1},N_{1}}(\bm{w},b_{1}),\bm{x}\rangle^{k}\bigg{]}+\mathbb{E }_{\bm{w}}\Big{[}\psi_{\bm{T}_{k}^{n}}(\bm{w},b_{1})(1-\mathbb{I}_{A(\bm{w})} \langle\bm{g}_{T_{1},N_{1}}(\bm{w},b_{1}),\bm{x}\rangle^{k}\Big{]}.\]

Note that the absolute value of second term above can be upper bounded by \(\mathbb{E}_{\bm{w}}\big{[}(1-\mathbb{I}_{A(\bm{w})})\big{]}^{1/2}\mathbb{E}_{ \bm{w}}\Big{[}(\psi_{\bm{T}_{k}^{n}}(\bm{w},b_{1})\langle\bm{g}_{T_{1},N_{1}} (\bm{w},b_{1}),\bm{x}\rangle^{k})^{2}\Big{]}^{1/2}\). Recall that \(A(\bm{w})\) occurs with high probability from Corollary 32, and \(\mathbb{E}_{\bm{w}}\Big{[}(\psi_{\bm{T}_{k}^{n}}(\bm{w},b_{1})\langle\bm{g}_{T _{1},N_{1}}(\bm{w},b_{1}),\bm{x}\rangle^{k})^{2}\Big{]}^{1/2}\) is polynomial in \(d\). Hence we can set the second term to be \(O(1/\sqrt{N_{2}})\).

Moreover, as \(\mathbb{P}[B(b_{1})]=1/2\) from Lemma 20, we have

\[\mathbb{E}_{b_{1}}\Big{[}2\cdot\mathbb{I}_{B(b_{1})}\mathbb{E}_{ \bm{w}}\Big{[}\psi_{\bm{T}_{k}^{n}}(\bm{w},b_{1})\langle\bm{g}_{T_{1},N_{1}}( \bm{w},b_{1}),\bm{x}\rangle^{k}\Big{]}\Big{]}\] \[= \mathbb{E}_{b_{1}}\big{[}2\cdot\big{\langle}\bm{T}_{k}^{n},\bm{ x}^{\otimes k}\big{\rangle}\big{]}+\mathbb{E}_{b_{1}}\big{[}2(1-\mathbb{I}_{B(b_{1})} )\big{\langle}\bm{T}_{k}^{n},\bm{x}^{\otimes k}\big{\rangle}\big{]}=\big{ \langle}\bm{T}_{k}^{n},\bm{x}^{\otimes k}\big{\rangle}.\]

Therefore, (C.2) and (C.3) holds. Noting that \(\eta_{1}\gamma\asymp\sqrt{rd^{2Q-1}}/C_{1}(\log d)^{C_{2}}\), We obtain

\[\mathbb{E}_{a,\bm{w},b_{1},b_{2}}[\phi_{\bm{T}_{k}^{n}}(a,\bm{w},b_{1},b_{2})^{2}]\] \[\leq \frac{4}{(2\eta_{1})^{2k}\gamma^{2k}}\mathbb{E}_{a,b_{2}}[v_{k}(a, b_{2})^{2}]\mathbb{E}_{\bm{w},b_{1}}[\mathbb{I}_{B(b_{1})}\psi_{\bm{T}_{k}^{n}}(\bm{w},b_{1 })^{2}]\] \[\lesssim \frac{4}{(2\eta_{1})^{2k}\gamma^{2k}}\mathbb{E}_{a,b_{2}}[v_{k} (a,b_{2})^{2}](d^{(2Q-1)}r^{2})^{k}\|\bm{T}_{k}^{n}\|_{F}^{2}\] \[= \tilde{O}\Big{(}r^{k}r^{\frac{P-k}{2}}\Big{)}\]

and

\[\sup_{a,\bm{w},b_{1},b_{2}}|\phi_{\bm{T}_{k}^{n}}(a,\bm{w},b_{1},b _{2})|\] \[\leq \frac{2}{(2\eta_{1})^{k}\gamma^{k}}|v_{k}(a,b_{2})|\mathbb{I}_{A( \bm{w})}\mathbb{I}_{B(b_{1})}|\psi_{\bm{T}_{k}^{n}}(\bm{w},b_{1})|\] \[\lesssim \frac{2}{(2\eta_{1})^{2k}\gamma^{2k}}(d^{(2Q-1)}r^{2})^{k}\|\bm{T} _{k}^{n}\|_{F}\|(2\eta_{1}\gamma)\bm{g}_{T_{1},N_{1}}(\bm{w},b_{1})\|^{k} \mathbb{I}_{A(\bm{w})}\mathbb{I}_{B(b_{1})}\] \[\lesssim \tilde{O}\Big{(}r^{k}r^{\frac{P-k}{4}}\Big{)}\]

from Lemmas 36 and 39. To show the original claim, it suffices to set \(\phi_{n}(a,\bm{w},b_{1},b_{2})=\sum_{k=0}^{P}\phi_{\bm{T}_{k}^{n}}(a,\bm{w},b_{ 1},b_{2})\), which yields the upper bounds on the magnitude of \(\phi_{n}\) because \(\sum_{k=0}^{P}r^{k}r^{\frac{P-k}{2}}\lesssim r^{P}\) and \(\sum_{k=0}^{P}r^{k}r^{\frac{P-k}{4}}\lesssim r^{P}\)

#### c.1.3 Proof of Proposition 33

We discretize Lemma 41 to establish Proposition 33.

**Proof.** [Proof of Proposition 33] First, in the same discretization manner as the proof of Lemma 13 in [10], we can show that there exists \(\bm{a}^{1},\ldots,\bm{a}^{B_{P}}\) such that for each \(n\in[B_{P}]\),

\[\sup_{i\in[N_{2}],T_{1}+1\leq t\leq T_{2}}\left(\sum_{j=1}^{m}a_{j} ^{n}\sigma\Big{(}\Big{\langle}2\eta_{1}\Gamma_{j,j}^{(0)}\bm{g}_{T_{1},N_{1}}( \bm{w}_{j}^{(0)},b_{j}^{(0)}),\bm{x}_{i}^{t}\Big{\rangle}+b_{j}\Big{)}-h_{n}( \bm{x}_{i}^{t})\right)^{2}\] \[=\] \[=\]

and \(\sup_{n\in[B_{P}]}\|\bm{a}^{n}\|^{2}=\tilde{O}\Big{(}\frac{r^{p}}{m}\Big{)}\) with high probability (\(b_{j}^{(0)}\), \(b_{j}\) denotes the biases at the first and second initialization, respectively). Recall that \(\bm{w}_{j}^{(1)}=2\eta_{1}\Gamma_{j,j}^{(0)}\bm{g}_{T_{1},N_{1}}(\bm{w}_{j}^{ (0)},b_{j}^{(0)})-2\eta_{1}\frac{1}{T_{1}}\sum_{i=1}^{T_{1}}f(\bm{X}^{t},\bm{ y}^{t},\bm{x}^{t};\bm{W}^{(0)},\bm{\Gamma}^{(0)},\bm{b}^{(0)})\nabla_{w_{j}^{(0)}} f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(0)},\bm{\Gamma}^{(0)},\bm{b}^{(0)})\). It remains to bound the effect of the second term. By Lemma 18, we know that the norm of second term is \(\tilde{O}\Big{(}\eta_{1}\gamma^{2}m\sqrt{d}\Big{)}\). Similar to the argument in the proof of Lemma 18, the inner product between the second term and \(\bm{x}^{t},\bm{x}_{i}^{t}\) (\(i\in[N_{2}],T_{1}+1\leq t\leq T_{2}\)) is also \(\tilde{O}\Big{(}\eta_{1}\gamma^{2}m\sqrt{d}\Big{)}\), with high probability. From the Cauchy-Schwarz inequality and the Lipschitz continuity of \(\sigma\),

\[\left|\sum_{j=1}^{m}a_{j}^{n}\sigma\Big{(}\Big{\langle}2\eta_{1} \Gamma_{j,j}^{(0)}\bm{g}_{T_{1},N_{1}}(\bm{w}_{j}^{(0)},b_{j}^{(0)}),\bm{x} \Big{\rangle}+b_{j}\Big{)}-\sum_{j=1}^{m}a_{j}^{n}\sigma\Big{(}\Big{\langle} \bm{w}_{j}^{(1)},\bm{x}\Big{\rangle}+b_{j}\Big{)}\right|\] \[=\]

We obtain the assertion. \(\Box\)

### Concentration of Correlation

Next, we evaluate the discrepancy between the empirical correlation \(\frac{1}{N_{2}}\sum_{j=1}^{N_{2}}y_{j}h(\bm{x}_{j})\) and its expectation for each \(h\in\mathcal{H}\), which decides the approximation error in (b) of (3.1).

Recall that \(h\in\mathcal{H}\) can be written in the form as

\[h(\bm{x})=\prod_{i=1}^{r}\frac{1}{\sqrt{p_{i}!}}\mathrm{He}_{p_{i}}(x_{i})\;(Q \leq p_{1}+\cdots+p_{r}\leq P,p_{1}\geq 0,\ldots,p_{r}\geq 0).\]

Also note that

\[\frac{1}{N_{2}}\sum_{j=1}^{N_{2}}y_{j}h(\bm{x}_{j})-\mathbb{E}_{ \bm{x},\varsigma}[yh(\bm{x})]\] \[= \frac{1}{N_{2}}\sum_{j=1}^{N_{2}}\varsigma_{j}h(\bm{x}_{j})+\frac {1}{N_{2}}\sum_{j=1}^{N_{2}}\sigma_{*}(\langle\bm{x}_{j},\bm{\beta}\rangle)h( \bm{x}_{j})-\mathbb{E}_{\bm{x}}[\sigma_{*}(\langle\bm{x},\bm{\beta}\rangle)h( \bm{x})],\]

where \(\sigma_{*}(z)=\sum_{i=Q}^{P}\frac{c_{i}!}{!}\mathrm{He}_{i}(z)\). First we give an upper bound for \(|\frac{1}{N_{2}}\sum_{j=1}^{N_{2}}\sigma_{*}(\langle\bm{x}_{j},\bm{\beta} \rangle)h(\bm{x}_{j})-\mathbb{E}_{\bm{x}}[\sigma_{*}(\langle\bm{x},\bm{\beta} \rangle)h(\bm{x})]|\).

[MISSING_PAGE_FAIL:35]

[MISSING_PAGE_EMPTY:36]

\(\{\bm{a}^{n}\}_{n=1}^{B_{P}}\) is constructed in Proposition 33. Then, by setting \(\bar{\bm{\Gamma}}=\bm{A}\bm{A}^{\top}\),

\[\left\langle\frac{\bm{A}\bm{A}^{\top}\sigma(\bm{W}^{(1)}\bm{X}+\bm{b }\bm{1}_{N}^{\top})\bm{y}}{N_{2}},\begin{bmatrix}\sigma(\bm{w}_{1}^{(1)}{}^{ \top}\bm{x}+b_{1})\\ \vdots\\ \sigma(\bm{w}_{m}^{(1)}{}^{\top}{}^{\top}\bm{x}+b_{m})\end{bmatrix}\right\rangle -y\] \[= \sum_{n=1}^{B_{P}}(h_{n}(\bm{x})+\epsilon_{n}(\bm{x}))\Bigg{(} \mathbb{E}[f_{*}h_{n}]+\left(\frac{1}{N_{2}}\sum_{j=1}^{N_{2}}y_{j}h_{n}(\bm{ x}_{j})-\mathbb{E}[f_{*}h_{n}]\right)+\frac{1}{N_{2}}\sum_{j=1}^{N_{2}}y_{j} \epsilon_{n}(\bm{x}_{j})\Bigg{)}\] \[-f_{*}(\bm{x})-\varsigma\] \[= \sum_{n=1}^{B_{P}}(h_{n}(\bm{x})+\epsilon_{n}(\bm{x}))\Bigg{(} \mathbb{E}[f_{*}h_{n}]+\left(\frac{1}{N_{2}}\sum_{j=1}^{N_{2}}y_{j}h_{n}(\bm{ x}_{j})-\mathbb{E}[f_{*}h_{n}]\right)+\frac{1}{N_{2}}\sum_{j=1}^{N_{2}}y_{j} \epsilon_{n}(\bm{x}_{j})\Bigg{)}\] \[-\sum_{n=1}^{B_{P}}h_{n}(\bm{x})\mathbb{E}[f_{*}h_{n}]-\varsigma\]

holds, where \(\epsilon_{n}(\bm{x})\coloneqq\sum_{j=1}^{m}\bm{a}_{j}^{n}\sigma(\bm{w}_{j}^{(1 )}{}^{\top}\bm{x}+b_{j})-h_{n}(\bm{x})\) satisfies \(|\epsilon_{n}(\bm{x})|=\tilde{O}\Big{(}\sqrt{r^{P}/m+1/N_{2}}\Big{)}\) from Proposition 33.

To bound the error, we note that the following holds for each \(n\):

* As \(h_{n}(\bm{x})\) is a polynomial whose degree is at most \(P\), and \(\mathbb{E}_{\bm{x}}[\nabla^{k}h_{n}(\bm{x})]=O(1)\), \(|h_{n}(\bm{x})|\lesssim(\log d)^{P/2}\) holds with high probability from Lemma 15.
* From lemma 8, \(|\mathbb{E}[f_{*}(\bm{x})h_{n}(\bm{x})]|=O(1)\) holds.
* From Corollary 17, \(|y_{j}|\lesssim(\log d)^{P/2}\) holds with high probability.

Hence, for each \(n\in[B_{P}]\), we obtain

\[\Bigg{|}\Bigg{(}\underbrace{h_{n}(\bm{x})}_{\tilde{O}(1)}+ \underbrace{\epsilon_{n}(\bm{x})}_{\tilde{O}(\sqrt{\frac{r^{P}}{m}+\frac{1}{N _{2}}})}_{\tilde{O}(1)}\Bigg{)}\Bigg{(}\underbrace{\mathbb{E}[f_{*}h_{n}]}_{ \tilde{O}(1)}+\underbrace{\left(\frac{1}{N_{2}}\sum_{j=1}^{N_{2}}y_{j}h_{n}( \bm{x}_{j})-\mathbb{E}[f_{*}h_{n}]\right)}_{\tilde{O}(\sqrt{\frac{r^{P}}{m} +\frac{1}{N_{2}}})}+\underbrace{\frac{1}{N_{2}}\sum_{j=1}^{N_{2}}y_{j} \epsilon_{n}(\bm{x}_{j})}_{\tilde{O}(\sqrt{\frac{r^{P}}{m}+\frac{1}{N_{2}}})} \Bigg{)}\] \[-h_{n}(\bm{x})\mathbb{E}[f_{*}h_{n}]\Bigg{|}\] \[= \tilde{O}\Bigg{(}\sqrt{\frac{r^{P}}{m}+\frac{1}{N_{2}}}+\left( \frac{r^{P}}{m}+\frac{1}{N_{2}}\right)\Bigg{)}.\]

Combining this with \(B_{P}=\Theta(r^{P})\) and \(m=\tilde{\Omega}(r^{P})\), we arrive at the upper bound.

Finally for \(\|\bar{\bm{\Gamma}}\|_{F}\), we have

\[\|\bar{\bm{\Gamma}}\|_{F}\leq\sum_{n=1}^{B_{P}}\|\bm{a}^{n}(\bm{a}^{n})^{\top} \|_{F}=\sum_{n=1}^{B_{P}}\|\bm{a}^{n}\|^{2}=\tilde{O}\big{(}r^{2P}/m\big{)}.\]

## Appendix D Generalization Error Analysis and Proof of Theorem 1

Note that after one-step gradient descent, \(\|\bm{w}_{j}^{(1)}\|=\tilde{O}_{d,r}(1)\) is ensured with high probability from Lemma 18 and Corollary 32.

### Rademacher Complexity Bound

Let \(\bm{W}=\bm{W}^{(1)}\) and suppose \(b_{1},\ldots,b_{m}\) are biases obtained in line 5 of Algorithm 1. Let

\[\mathcal{F}_{N,G}=\bigg{\{}(\bm{X},\bm{y},\bm{x})\mapsto\bigg{\langle}\frac{ \bm{\Gamma}\sigma(\bm{W}^{(1)}\bm{X}+\bm{b}\bm{1}_{N}^{\top})\bm{y}}{N}\begin{bmatrix} \sigma((\bm{w}_{1}^{(1)})^{\top}\bm{x}+b_{1})\\ \vdots\\ \sigma((\bm{w}_{m}^{(1)})^{\top}\bm{x}+b_{m})\end{bmatrix}\bigg{\rangle}\bigg{|} \|\bm{\Gamma}\|_{F}\leq G\bigg{\}}\]

be the set of transformers where the norm of the attention matrix is constrained, and let

\[\mathrm{Rad}_{T}(\mathcal{F}_{N,G})=\mathbb{E}_{\{\bm{X}^{t}\},\{\bm{y}^{t}\}, \{\bm{x}^{t}\},\{\epsilon^{t}\}}\Bigg{[}\sup_{f\in\mathcal{F}_{N,G}}\frac{1}{ T}\sum_{t=1}^{T}\epsilon^{t}f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t})\Bigg{]}\]

be its Rademacher complexity, where \(\epsilon^{t}\sim\mathrm{Unif}(\{\pm 1\})\). Here contexts \(\bm{X}\in\mathbb{R}^{d\times N}\) and \(\bm{y}\in\mathbb{R}^{N}\) are of length \(N\). We can evaluate the Rademacher complexity as follows.

**Proposition 45**.: \[\mathrm{Rad}_{T}(\mathcal{F}_{N,G})=\tilde{O}_{d,r}\bigg{(}\frac{ Gm}{\sqrt{T}}\bigg{)}\]

_holds, when \(|b_{j}|=\tilde{O}_{d,r}(1)\) and \(\|\bm{w}_{j}^{(1)}\|=\tilde{O}_{d,r}(1)\) for all \(j\in[m]\)._

Proof.: First, it holds that

\[\mathrm{Rad}_{T}(\mathcal{F}_{N,G})\] \[=\mathbb{E}\Bigg{[}\sup_{|\bm{\Gamma}\|_{F}\leq G}\frac{1}{T} \sum_{t=1}^{T}\epsilon^{t}\bigg{\langle}\frac{\bm{\Gamma}\sigma(\bm{W}^{(1)} \bm{X}^{t}+\bm{b}\bm{1}_{N}^{\top})\bm{y}^{t}}{N},\begin{bmatrix}\sigma((\bm{ w}_{1}^{(1)})^{\top}\bm{x}^{t}+b_{1})\\ \vdots\\ \sigma((\bm{w}_{m}^{(1)})^{\top}\bm{x}^{t}+b_{m})\end{bmatrix}\bigg{\rangle} \Bigg{]}\] \[\overset{(a)}{\leq}\mathbb{E}\Bigg{[}\frac{1}{T}\sup_{|\bm{ \Gamma}\|_{F}\leq G}\|\bm{\Gamma}\|_{F}\Bigg{\|}\sum_{t=1}^{T}\epsilon^{t}\frac {\sigma(\bm{W}^{(1)}\bm{X}^{t}+\bm{b}\bm{1}_{N}^{\top})\bm{y}^{t}}{N}\begin{bmatrix} \sigma((\bm{w}_{1}^{(1)})^{\top}\bm{x}^{t}+b_{1})\\ \vdots\\ \sigma((\bm{w}_{m}^{(1)})^{\top}\bm{x}^{t}+b_{m})\end{bmatrix}\Bigg{\|}_{F} \Bigg{]}\] \[=\frac{G}{T}\mathbb{E}\Bigg{[}\Bigg{\|}\sum_{t=1}^{T}\epsilon^{t} \frac{\sigma(\bm{W}^{(1)}\bm{X}^{t}+\bm{b}\bm{1}_{N}^{\top})\bm{y}^{t}}{N} \begin{bmatrix}\sigma((\bm{w}_{1}^{(1)})^{\top}\bm{x}^{t}+b_{1})\\ \vdots\\ \sigma((\bm{w}_{m}^{(1)})^{\top}\bm{x}^{t}+b_{m})\end{bmatrix}^{\top}\Bigg{\|} _{F}\Bigg{]}\] \[\overset{(b)}{=}\frac{G}{T}\sqrt{T\mathbb{E}_{\bm{X},\bm{y},\bm{ x}}\Bigg{[}\Bigg{\|}\frac{\sigma(\bm{W}^{(1)}\bm{X}+\bm{b}\bm{1}_{N}^{\top})\bm{y}}{N} \begin{bmatrix}\sigma((\bm{w}_{1}^{(1)})^{\top}\bm{x}+b_{1})\\ \vdots\\ \sigma((\bm{w}_{m}^{(1)})^{\top}\bm{x}+b_{m})\end{bmatrix}^{\top}\Bigg{\|}_{F} ^{2}\Bigg{]}}\] \[=\frac{G}{\sqrt{T}}\sqrt{\sum_{k,l=1}^{m}\mathbb{E}_{\bm{X},\bm{y },\bm{x}}\Bigg{[}\Bigg{(}\frac{1}{N}\sum_{j=1}^{N}y_{j}\sigma((\bm{w}_{k}^{(1)}) ^{\top}\bm{x}_{j}+b_{k})\sigma((\bm{w}_{l}^{(1)})^{\top}\bm{x}+b_{l})\Bigg{)}^ {2}\Bigg{]}}\]

[MISSING_PAGE_FAIL:39]

### Prompt Length-free Generalization Bound

The ICL risk for prompt length \(N\) was defined as

\[\mathcal{R}_{N}(f)=\mathbb{E}_{\bm{X},\bm{y},\bm{x},y}[|f(\bm{X}_{1:N},\bm{y}_{1: N},\bm{x};\bm{W},\bm{\Gamma},\bm{b})-y|],\]

where the length of \(\bm{X}_{1:N}\) and \(\bm{y}_{1:N}\) is fixed to \(N\). In this section, we show that the ICL risk "converges" when the context length is sufficiently large.

**Proposition 47**.: _Assume that \(\|\bm{w}_{j}\|=\tilde{O}_{d,r}(1)\) and \(|b_{j}|=\tilde{O}_{d,r}(1)\) for each \(j\in[m]\). Then,_

\[|\mathcal{R}_{N}(f)-\mathcal{R}_{M}(f)|=\tilde{O}\Big{(}\|\bm{\Gamma}\|_{F}m \sqrt{1/N+1/M}\Big{)}\]

_holds._

**Proof.** Note that

\[|\mathcal{R}_{N}(f)-\mathcal{R}_{M}(f)|\] \[\leq \mathbb{E}[|f(\bm{X}_{1:N},\bm{y}_{1:N},\bm{x};\bm{W},\bm{\Gamma}, \bm{b})-f(\bm{X}_{1:M},\bm{y}_{1:M},\bm{x};\bm{W},\bm{\Gamma},\bm{b})|]\] \[= \mathbb{E}\Bigg{[}\Bigg{|}\Bigg{\langle}\bm{\Gamma}\bigg{(} \frac{\sigma(\bm{W}\bm{X}_{1:N}+\bm{b}\bm{1}_{N}^{\top})\bm{y}_{1:N}}{N}- \frac{\sigma(\bm{W}\bm{X}_{1:M}+\bm{b}\bm{1}_{M}^{\top})\bm{y}_{1:M}}{M} \bigg{)},\begin{bmatrix}\sigma(\bm{w}_{1}^{\top}\bm{x}+b_{1})\\ \vdots\\ \sigma(\bm{w}_{m}^{\top}\bm{x}+b_{m})\end{bmatrix}\Bigg{\rangle}\Bigg{]}\Bigg{]}\] \[\leq\] \[\cdot\mathbb{E}\Bigg{[}\Bigg{\|}\Bigg{[}\frac{\sigma(\bm{w}_{1}^ {\top}\bm{x}+b_{1})}{\vdots}\Bigg{]}\Bigg{\|}^{2}\Bigg{]}^{1/2},\]

where (a) holds because \(\bm{a}^{\top}\bm{\Gamma}\bm{b}=\bm{\Gamma}\circ(\bm{ab}^{\top})\leq\|\bm{ \Gamma}\|_{F}\|\bm{ab}^{\top}\|_{F}=\|\bm{\Gamma}\|_{F}\|\bm{a}\|\|\bm{b}\|\).

Let us bound, which is to \(\sum_{j=1}^{m}\mathbb{E}_{f_{*}}\bigg{[}\mathbb{E}_{\bm{X},\bm{y}}\bigg{[} \Big{(}\frac{1}{N}\sum_{i=1}^{N}\sigma(\bm{w}_{j}^{\top}\bm{x}_{i}+b_{j})y_{i }-\frac{1}{M}\sum_{i=1}^{M}\sigma(\bm{w}_{j}^{\top}\bm{x}_{i}+b_{j})y_{i}\Big{)} ^{2}\bigg{]}\bigg{]}\). For simplicity, we drop the subscript \(j\) and obtain

\[\mathbb{E}_{\bm{X},\bm{y}}\Bigg{[}\Bigg{(}\frac{1}{N}\sum_{i=1}^ {N}\sigma(\bm{w}^{\top}\bm{x}_{i}+b)y_{i}-\frac{1}{M}\sum_{i=1}^{M}\sigma(\bm{ w}^{\top}\bm{x}_{i}+b)y_{i}\Bigg{)}^{2}\Bigg{]}\] \[\leq 2\mathbb{E}\Bigg{[}\Bigg{(}\frac{1}{N}\sum_{i=1}^{N}\sigma(\bm{w} ^{\top}\bm{x}_{i}+b)y_{i}-\mathbb{E}\big{[}\sigma(\bm{w}^{\top}\bm{x}+b)y\big{]} \Bigg{)}^{2}\Bigg{]}\] \[+ 2\mathbb{E}\Bigg{[}\Bigg{(}\frac{1}{M}\sum_{i=1}^{M}\sigma(\bm{w} ^{\top}\bm{x}_{i}+b)y_{i}-\mathbb{E}\big{[}\sigma(\bm{w}^{\top}\bm{x}+b)y\big{]} \Bigg{)}^{2}\Bigg{]}\] \[= \frac{2}{N}\mathbb{V}\big{[}\big{(}\sigma(\bm{w}^{\top}\bm{x}+b)y \big{)}\big{]}+\frac{2}{M}\mathbb{V}\big{[}\big{(}\sigma(\bm{w}^{\top}\bm{x}+b)y \big{)}\big{]}\] \[\leq \Big{(}\frac{2}{N}+\frac{2}{M}\Big{)}\mathbb{E}\big{[}\sigma(\bm{ w}^{\top}\bm{x}+b)^{2}(f_{*}(\bm{x})+\varsigma)^{2}\big{]}\] \[\overset{(a)}{\leq} \bigg{(}\frac{2}{N}+\frac{2}{M}\bigg{)}\mathbb{E}\big{[}(2(\bm{w}^{ \top}\bm{x})^{2}+2b^{2})(2f_{*}(\bm{x})^{2}+2\varsigma^{2})\big{]}.\]Here (a) is obtained in the same vein as the derivation of (d) in the proof of Proposition 45. Moreover, in the same way as that proof, we can show that \(\mathbb{E}\big{[}\big{[}(2(\bm{w}^{\top}\bm{x})^{2}+2b^{2})(2f_{*}(\bm{x})^{2}+2 \varsigma^{2})\big{]}=\tilde{O}_{d,r}(1)\). Consequently we get \(\mathbb{E}\bigg{[}\bigg{\|}\Big{(}\frac{\sigma(\bm{W}\bm{X}_{1:N}+\bm{b}\bm{1}_ {N}^{\top})\bm{y}_{1:N}}{N}-\frac{\sigma(\bm{W}\bm{X}_{1:M}+\bm{b}\bm{1}_{M}^{ \top})\bm{y}_{1:M}}{M}\Big{)}\bigg{\|}^{2}\bigg{]}=\big{(}\frac{2}{N}+\frac{2} {M}\big{)}\tilde{O}_{d,r}(m)\). Next, we observe that

\[\mathbb{E}\Bigg{[}\Bigg{\|}\begin{bmatrix}\sigma(\bm{w}_{1}^{\top}\bm{x}+b_{1}) \\ \vdots\\ \sigma(\bm{w}_{m}^{\top}\bm{x}+b_{m})\end{bmatrix}\Bigg{\|}^{2}\Bigg{]}\leq 2 \sum_{j=1}^{m}\mathbb{E}[(\bm{w}_{j}^{\top}\bm{x})^{2}+b_{j}^{2}]=\tilde{O}_{d,r}(m).\]

Putting all things together, we arrive at

\[|\mathcal{R}_{N}(f)-\mathcal{R}_{M}(f)|=\tilde{O}\Big{(}\|\bm{\Gamma}\|_{F}m \sqrt{1/N+1/M}\Big{)}.\]

\(\Box\)

### Proof of Theorem 1

Finally we are ready to prove our main theorem.

**Proof.** [Proof of Theorem 1] Let \(\bar{\bm{\Gamma}}\) be the attention matrix constructed in Proposition 2 and let \(\bm{\Gamma}^{*}\) be the minimizer of the ridge regression problem (line 8 in Algorithm 1). By the equivalence between optimization with \(L_{2}\) regularization and norm-constrained optimization, there exists \(\lambda_{2}\) such that

\[\|\bm{\Gamma}^{*}\|_{F}\leq\|\bar{\bm{\Gamma}}\|_{F}=\tilde{O} \big{(}r^{2P}/m\big{)},\]

\[\Bigg{(}\frac{1}{T_{2}}\sum_{t=T_{1}+1}^{T_{1}+T_{2}}|y^{t}-f(\bm{ X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(1)},\bm{\Gamma}^{*},\bm{b})|\Bigg{)}^{2}\] \[\leq \frac{1}{T_{2}}\sum_{t=T_{1}+1}^{T_{1}+T_{2}}(y^{t}-f(\bm{X}^{t}, \bm{y}^{t},\bm{x}^{t};\bm{W}^{(1)},\bar{\bm{\Gamma}}^{*},\bm{b}))^{2}\] \[\leq \frac{1}{T_{2}}\sum_{t=T_{1}+1}^{T_{1}+T_{2}}(y^{t}-f(\bm{X}^{t}, \bm{y}^{t},\bm{x}^{t};\bm{W}^{(1)},\bar{\bm{\Gamma}},\bm{b}))^{2}.\]

Then, from Proposition 2,

\[\frac{1}{T_{2}}\sum_{t=T_{1}+1}^{T_{1}+T_{2}}|y^{t}-f(\bm{X}^{t},\bm{y}^{t}, \bm{x}^{t};\bm{W}^{(1)},\bm{\Gamma}^{*},\bm{b})|-\tau=\tilde{O}\Bigg{(}\sqrt{ \frac{r^{3P}}{m}+\frac{r^{2P}}{N_{2}}}\Bigg{)}\]

holds.

We evaluate \(\mathcal{R}_{N_{2}}(f)-\tau\) where \(f=f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(1)},\bm{\Gamma}^{*},\bm{b})\) using the Rademacher complexity.

\[\mathcal{R}_{N_{2}}(f)-\tau\] \[=\frac{1}{T_{2}}\sum_{t=T_{1}+1}^{T_{1}+T_{2}}|y^{t}-f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(1)},\bm{\Gamma}^{*},\bm{b})|\] \[\qquad+\Bigg{(}\mathcal{R}_{N_{2}}(f)-\frac{1}{T_{2}}\sum_{t=T_{1 }+1}^{T_{1}+T_{2}}|y^{t}-f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t};\bm{W}^{(1)},\bm{ \Gamma}^{*},\bm{b})|\Bigg{)}-\tau\] \[\leq\tilde{O}\Bigg{(}\sqrt{\frac{r^{3P}}{m}+\frac{r^{2P}}{N_{2}}} \Bigg{)}\] \[\quad+\sup_{f\in\bar{\mathcal{F}}_{N,1}\bigcap_{F}}\Bigg{(} \mathcal{R}_{N_{2}}(f)-\frac{1}{T_{2}}\sum_{t=T_{1}+1}^{T_{1}+T_{2}}|y^{t}-f( \bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})|\Bigg{)}\]holds, where \(\tilde{\mathcal{F}}\) is defined in Corollary 46. We can evaluate the expectation value of the second term of the last line as, using \(\epsilon^{\text{ i.i.d.}}\overset{\text{i.i.d.}}{\sim}\operatorname{Unif}\{\pm 1\}\),

\[\mathbb{E}_{\{\bm{X}^{t}\},\{\bm{y}^{t}\},\{\bm{x}^{t}\},\{y^{t}\}} \left[\sup_{f\in\tilde{\mathcal{F}}_{N,\|\bm{\Gamma}\|_{F}}}\left(\mathcal{R}_{ N_{2}}(f)-\frac{1}{T_{2}}\sum_{t=T_{1}+1}^{T_{1}+T_{2}}|y^{t}-f(\bm{X}^{t},\bm{y}^{t}, \bm{x}^{t},y^{t})|\right)\right]\] \[\overset{(a)}{\leq}2\mathbb{E}_{\{\bm{X}^{t}\},\{\bm{y}^{t}\},\{ \bm{x}^{t}\},\{y^{t}\},\{\bm{c}^{t}\}}\left[\sup_{f\in\tilde{\mathcal{F}}_{N, \|\bm{\Gamma}\|_{F}}}\frac{1}{T_{2}}\sum_{t=T_{1}+1}^{T_{1}+T_{2}}\epsilon^{t} |y^{t}-f(\bm{X}^{t},\bm{y}^{t},\bm{x}^{t},y^{t})|\right]\] \[\overset{(b)}{\lesssim}\operatorname{Rad}_{T_{2}}(\tilde{ \mathcal{F}}_{N_{2},\|\bm{\Gamma}\|_{F}})+\mathbb{E}_{y,\bm{c}}\Bigg{[}\frac{1 }{T_{2}}\sum_{t=T_{1}+1}^{T_{1}+T_{2}}\epsilon^{t}y^{t}\Bigg{]}\] \[\leq\operatorname{Rad}_{T_{2}}(\tilde{\mathcal{F}}_{N_{2},\|\bm{ \Gamma}\|_{F}})+\mathbb{E}_{y,\bm{c}}\Bigg{[}\Bigg{(}\frac{1}{T_{2}}\sum_{t=T _{1}+1}^{T_{1}+T_{2}}\epsilon^{t}y^{t}\Bigg{)}^{2}\Bigg{]}^{1/2}\] \[\leq\operatorname{Rad}_{T_{2}}(\tilde{\mathcal{F}}_{N_{2},\|\bm{ \Gamma}\|_{F}})+\frac{1}{\sqrt{T_{2}}}\mathbb{E}\big{[}y^{2}\big{]}^{1/2} \overset{(c)}{=}\tilde{O}\Bigg{(}\sqrt{\frac{r^{4P}}{T_{2}}}\Bigg{)}.\]

Here, (a) holds from the standard symmetrization argument (see eq. (4.17) in [11] for example), (b) is derived using eq. (1) in [11], noting that \((x,y)^{\top}\mapsto|x-y|\) is \(\sqrt{2}\)-Lipschitz continuous, and (c) follows from the bound of \(\|\tilde{\bm{\Gamma}}\|_{F}\) in Proposition 2, together with Lemma 7 and Proposition 45. Note that

\[\sup_{f\in\tilde{\mathcal{F}}_{N,\|\bm{\Gamma}\|_{F}}}\left(\mathcal{R}_{N_{2} }(f)-\frac{1}{T_{2}}\sum_{t=T_{1}+1}^{T_{1}+T_{2}}|y^{t}-f(\bm{X}^{t},\bm{y}^ {t},\bm{x}^{t},y^{t})|\right)\] (D.1)

is nonnegative because \(f(\bm{X},\bm{y},\bm{x},y)=y\) is included in \(\tilde{\mathcal{F}}_{N,\|\bm{\Gamma}\|_{F}}\). Hence from Markov's inequality, (D.1) is \(\tilde{O}\Big{(}\sqrt{\frac{r^{4P}}{T_{2}}}\Big{)}\) with probability at least \(0.995\). Hence we obtain

\[\mathcal{R}_{N_{2}}(f)-\tau=\tilde{O}\Bigg{(}\sqrt{\frac{r^{3P}}{m}+\frac{r^{ 2P}}{N_{2}}}+\sqrt{\frac{r^{4P}}{T_{2}}}\Bigg{)}.\]

Now we have done the upper bound for \(\mathcal{R}_{N_{2}}(f)\). For \(\mathcal{R}_{N^{*}}(f)\), we can use Proposition 47.

Note that all the desired events occur with high probability, except for the one described above, which occurs with probability 0.995. This completes the proof of Theorem 1. \(\square\)

## Appendix E Derivation of Simplified Self-attention Module

We derive equation (2.6), following the same line of argument as [10]. As we assumed that \(\bm{W}^{PV}\)\(\bm{W}^{KQ}\) are in the form as (2.4), the output of the attention layer is written as

\[\tilde{f}_{\operatorname{Attn}}(\bm{E};\bm{W}^{PV},\bm{W}^{KQ})=\bm{E}+\begin{bmatrix} *&*\\ \bm{0}_{1\times m}&v\end{bmatrix}\bm{E}\cdot\frac{\bm{E}^{\top}\begin{bmatrix} \bm{K}&*\\ \bm{0}_{1\times m}&*\end{bmatrix}\bm{E}}{N}.\] (E.1)

Note that we adopt the right-bottom entry \(\left(\tilde{f}_{\operatorname{Attn}}(\bm{E};\bm{W}^{PV},\bm{W}^{KQ})\right)_ {m+1,N+1}\) as a prediction for output of the query. By (E.1). Hence we have

\[\left(\tilde{f}_{\operatorname{Attn}}(\bm{E};\bm{W}^{PV},\bm{W}^{KQ})\right)_ {m+1,N+1}\]\[= [\bm{0}_{1\times m}\quad v]\bm{E}\cdot\frac{N}{N}\] \[= v[y_{1}\quad\cdots\quad y_{N}]\cdot\frac{\sigma(\bm{W}\bm{X}+\bm{b }\bm{1}_{N}^{\top})^{\top}\bm{K}\Bigg{[}\begin{matrix}\sigma(\bm{w}_{1}^{\top} \bm{x}+b_{1})\\ \vdots\\ \sigma(\bm{w}_{m}^{\top}\bm{x}+b_{m})\end{matrix}\Bigg{]}}{N}\] \[= v[y_{1}\quad\cdots\quad y_{N}]\cdot\frac{N}{N}\] \[= \Bigg{\langle}\frac{v\bm{K}^{\top}\sigma(\bm{W}\bm{X}+\bm{b}\bm{1 }_{N}^{\top})\bm{y}}{N},\begin{bmatrix}\sigma(\bm{w}_{1}^{\top}\bm{x}+b_{1})\\ \vdots\\ \sigma(\bm{w}_{m}^{\top}\bm{x}+b_{m})\end{bmatrix}\Bigg{\rangle}.\]

Setting \(\bm{\Gamma}=v\bm{K}^{\top}\) yields equation (2.6).

## Appendix F Details of Experiments

### Detailed Experimental Settings

For the experiments in Section 4, test loss was averaged over \(128\) independent tasks, with \(2048\) and \(128\) independent queries generated for each task in the experiment of Figure 1 and 2, respectively. During the validation of the experiment of Figure 1, the coefficients \(\{c_{i}\}\) in the single-index model were fixed to be \((c_{2},c_{3})=(\sqrt{2\cdot 2!}/2,\sqrt{2\cdot 3!}/2)\) to reduce the variance in the baseline methods.

GPT-2 model.The experiments in Section 4 were based on the architecture used in [1], whose backbone is the GPT-2 model [12]. Given the prompt \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{N+1}\), the embedding \(\bm{E}\in\mathbb{R}^{d\times(2N+2)}\) was constructed as \(\bm{E}=[\bm{x}_{1},\tilde{\bm{y}}_{1},\dots,\bm{x}_{N},\tilde{\bm{y}}_{N}]\) where \(\tilde{\bm{y}}_{i}=[y_{i},0,\dots,0]^{\top}\). This embedding was mapped to \(\tilde{\bm{E}}\in\mathbb{R}^{256\times(2N+2)}\) through the read-in layer. The transformed embedding \(\tilde{\bm{E}}\) was then passed through the 12-layer GPT-2 backbone with 8 heads, with dropout disabled. Finally, the output was converted into a \(2N+2\)-dimensional vector through the read-out layer, where the \((2k-1)\)-th entry \((k\in[N+1])\) corresponds to the prediction \(\hat{y}_{k}(\bm{x}_{1}^{t},y_{1}^{t}\dots,\bm{x}_{k-1}^{t},y_{k-1}^{t},\bm{x}_ {k}^{t})\) for \(y_{k}\), that is, the answer for the query \(\bm{x}_{k}^{t}\) reading the context of length \(k-1\). We used the Adam optimizer [1] with a learning rate of \(0.0001\). The training loss at each step was set as \(\frac{1}{B}\sum_{t=1}^{B}\sum_{k=1}^{N+1}\big{(}y_{k}^{t}-\hat{y}_{k}(\bm{x}_ {1}^{t},y_{1}^{t}\dots,\bm{x}_{k-1}^{t},y_{k-1}^{t},\bm{x}_{k}^{t})\big{)}^{2}\) where \(B=8\) is the minibatch size.

Baseline algorithms.Baseline algorithms used in the comparison experiment are as follows:

* Two-layer neural network \(f(\bm{x})=\sum_{j=1}^{m}a_{j}\sigma(\bm{w}_{j}^{\top}\bm{x})\) where \(\sigma=\mathrm{ReLU}\) and \(m=256\). The initialization scale followed the mean-field regime, with \(a_{j}\overset{\mathrm{i.i.d.}}{\sim}\mathrm{Unif}(\{\pm 1/m\})\) and \(\bm{w}_{j}\overset{\mathrm{i.i.d.}}{\sim}\mathrm{Unif}(\mathbb{S}^{d-1})\). The architecture was trained using the Adam optimizer, with a learning rate of \(0.1\) for the first layer and \(0.1/m\) for the second layer, along with a weight dacay rate of \(0.0001\). Early stopping was applied to prevent overfitting.
* Kernel ridge regression using the Gaussian RBF kernel. The ridge regression parameter \(\lambda\) was set to \(\lambda=0.01\).

Again, note that these baseline algorithms were trained from scratch for each validation task, whereas the parameters of the pretrained GPT-2 model remained unchanged across all validation tasks.

### Experiment on Our Architecture and Algorithm 1

In addition to the GPT-2 experiments, we also conducted comparison between our Algorithm 1 on our simplified architecture and baseline algorithms. Data is generated as \(y_{i}^{t}=\mathrm{He}_{2}(\langle\bm{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{\mathbf{\mathbf{ \mathbf{ \mathbf{ }}}}}}}}}}}}},\bm{ \mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{  }}}}}}}}}}}}}}} }\rangle)\), where \(\bm{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{ \mathbf{ }}}}}}}}}}}}}}}} }\) is \(d=32\) and \(r=2\). We pretrained our transformer architecture (2.6) with \(m=8,000\), using Algorithm 1 with \(T_{1}=10^{5},N_{1}=10^{4},\eta_{1}=10^{5}\) (recall that \(\eta_{1}\asymp m^{\frac{3}{2}}rd^{2Q-\frac{1}{2}}\cdot(\log d)^{-C_{\text{v}}}\) should be large) and \(T_{2}=3,000,N_{2}=512,\lambda_{2}=0.1\). We used mini-batch SGD to find an approximate optimizer \(\bm{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{   \mathbf{   }}}}}}}}}}}}}}}\) in the regression. For validation, we altered the context length from \(2^{4.5}\) to \(2^{9}\) and compared its test error with one-step GD (training the first layer via one-step gradient descent, and then conducting ridge regression on the second layer) on a two-layer NN of width \(8,000\) and kernel ridge regression with RBF kernel working directly on the test prompt. In Figure 3 we see that our simplified transformer also outperforms neural network and kernel method, especially in short context regime.

Figure 3: In-context generalization error of kernel ridge regression, neural network + one-step gradient descent, and Algorithm 1 on our Transformer (2.6).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: They are presented accurately. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We mentioned the limitation of our result and the future challenge in Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We presented full-proofs for our results in the appendix, and sketched the proof in Section 3.2. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: This is a theory paper where the toy experiments are not central to our contribution. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer:[Yes] Justification: See Section 4 and Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Section 4 and Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: We only have toy experiments and it is not a central part of this paper. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, and we checked the code. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our results are purely theoretical working on a small simplified model, thus there seems no societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We don't have any models or datasets to release. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We included the information in Appendix F. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We don't release any assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not include such subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not include such subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.