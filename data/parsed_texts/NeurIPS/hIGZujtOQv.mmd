# Unleashing the Power of Graph Data Augmentation

on Covariate Distribution Shift

Yongduo Sui\({}^{1}\)

This work was done during author's internship at Ant Group.

Qitian Wu\({}^{2}\)

Jiancan Wu\({}^{1}\)

Qing Cui\({}^{3}\)

Longfei Li\({}^{3}\)

Jun Zhou\({}^{3*}\)

Xiang Wang\({}^{1*}\)

Xiangnan He\({}^{1*}\)

\({}^{1}\)University of Science and Technology of China,

\({}^{2}\)Shanghai Jiao Tong University, \({}^{3}\)Ant Group

syd2019@mail.ustc.edu.cn,echo7408jtu.edu.cn,

{wujcan,xiangwang1223,xiangnanhe}@gmail.com,

{cuiqing.cq,longyao.llf,jun.zhoujun}@antgroup.com

###### Abstract

The issue of distribution shifts is emerging as a critical concern in graph representation learning. From the perspective of invariant learning and stable learning, a recently well-established paradigm for out-of-distribution generalization, stable features of the graph are assumed to causally determine labels, while environmental features tend to be unstable and can lead to the two primary types of distribution shifts. The correlation shift is often caused by the spurious correlation between environmental features and labels that differs between the training and test data; the covariate shift often stems from the presence of new environmental features in test data. However, most strategies, such as invariant learning or graph augmentation, typically struggle with limited training environments or perturbed stable features, thus exposing limitations in handling the problem of covariate shift. To address this challenge, we propose a simple-yet-effective data augmentation strategy, Adversarial Invariant Augmentation (AIA), to handle the covariate shift on graphs. Specifically, given the training data, AIA aims to extrapolate and generate new environments, while concurrently preserving the original stable features during the augmentation process. Such a design equips the graph classification model with an enhanced capability to identify stable features in new environments, thereby effectively tackling the covariate shift in data. Extensive experiments with in-depth empirical analysis demonstrate the superiority of our approach. The implementation codes are publicly available at https://github.com/yongduosui/AIA.

## 1 Introduction

While recent advances have made solid progress in learning effective representations for graph-structured data, most of the existing approaches operate under the assumption that training and test graphs are independently drawn from an identical distribution [1, 2]. However, this assumption often falls short in real-world scenarios due to the out-of-distribution (OOD) data that potentially exists during the testing phase [3], which results in distribution shifts between training and test graphs. As a result, there is increasing research interest in OOD generalization on graphs or learning with distribution shifts on graphs [4]. Some of the typical recent works attempt to build effective methods for handling general distribution shifts on graphs, from (causal) invariant learning [5, 3], model architecture designs [6], and data augmentation [7].

With more specific views, other attempts focus on designing generalizable models for tackling distribution shifts in particular domains with certain data formats, _e.g.,_ molecular graphs [8], recommender systems [9; 10], and anomaly detection [11].

However, the majority of existing studies primarily focus on the correlation shift, one type of distribution shift concerning OOD generalization [12; 13], leaving another equally important type of distribution shift, _i.e.,_ the covariate shift, largely under-explored in graph representation learning. From the perspective of invariant learning and stable learning [14; 15; 16], covariate shift is in stark contrast to correlation shift _w.r.t._ stable and environmental features of graph data. Specifically, according to the commonly-used graph generation hypothesis in prior studies [17; 18; 5; 8], there often exist stable features, which are informative features of the entire graphs and can reflect the predictive patterns in data. Based on this, the relationship between stable features and labels is assumed to be invariant across environments. The remaining features could be unstable and varying across different environments, which mainly causes the following two distribution shifts: (1) correlation shift indicates that environments and labels establish inconsistent statistical correlations in training and test data, under the assumption that test environments are covered in the training data; whereas, (2) covariate shift characterizes that the environmental features in test data are unseen in training data [2; 12].

Considering a toy example in Figure 1, the environmental features _ladder_ and _tree_ are different in training and test data, which forms the covariate shift. Taking molecular property predictions as another example, functional groups (_e.g.,_ nitrogen dioxide (NO\({}_{2}\))) are stable to determine the predictive property of molecules [5; 8]. Whereas, scaffolds (_e.g.,_ carbon rings) are usually patterns irrelevant to the molecule properties, which can be seen as environmental features [2; 19]. In practice, we often need to use molecules collected in the past to train models, expecting that the models can predict the properties of molecules with new scaffolds in the future [20; 2; 19].

Considering the differences between correlation and covariate shifts, we take a close look into the existing efforts on graph generalization. They mainly fall into the following research lines, each of which has inherent limitations to solving the covariate shift. _i) Invariant Graph Learning_[17; 18; 5; 21] recently becomes a prevalent paradigm for OOD generalization. The basic idea is to capture stable features by minimizing the empirical risks in different environments. Unfortunately, it implicitly makes a prior assumption that test environments are available during training. This assumption is unrealistic owing to the obstacle of training data covering all possible test environments. Learning in limited environments can alleviate the spurious correlations that are hidden in the training data, but fail to extrapolate the test data with unseen environments. _ii) Graph Data Augmentation_[22; 23] perturbs graph features to enrich the distribution seen during training for better generalization. It can be roughly divided into node-level [24], edge-level [25], and graph-level [26; 7] with random [27] or adversarial strategies [28]. However, blindly augmenting the graphs can presumably destroys the stable features, and makes the augmented distributions out of control. For example, in Figure 1, the random strategy of DropEdge [25] will inevitably perturb the stable features (highlighted by red circles). As such, it may not sufficiently address the covariate shift and could potentially affect the generalization ability. Hence, we naturally ask a question: "_Compared to the training data, can we generate new data that satisfy two conditions: 1) having new environments; 2) keeping the original stable features unchanged?_"

Towards this end, we introduce two intuitive principles for graph augmentation: environmental feature discrepancy and stable feature consistency. The discrepancy principle promotes the exploration of new environments beyond the scope of training data, while the consistency principle seeks to maintain the integrity of stable features during augmentation. In order to achieve these principles, we devise a simple yet effective graph augmentation strategy: Adversarial Invariant Augmentation (AIA). Specifically, we employ an adversarial augmenter, a network that augments graphs by adversarially generating masks on them, thereby facilitating OOD exploration to enhance environmental discrep

Figure 1: \(P_{\mathrm{train}}\) and \(P_{\mathrm{test}}\) denote the training and test distributions. \(P_{\mathrm{drop}}\) and \(P_{\mathrm{ours}}\) represent the distributions of augmented data via DropEdge and AIA.

ancy. To foster stable feature consistency, we use another network, _i.e.,_ stable feature generator, which constructs masks that encapsulate stable features. We then delicately merge these masks and apply them to the graph data. As depicted in Figure 1, AIA primarily augments environmental features while leaving the stable elements unchanged. Our approach equips the graph classifier model with an enhanced ability to identify stable features in new environments and effectively mitigate the covariate shift issue. We also conduct extensive experiments and in-depth analyses. The experimental results highlight the limitations of several previous methods and underscore the superiority of our method in addressing the covariate shift issue, providing empirical support for our claims.

## 2 Preliminaries

We define the uppercase letters (_e.g.,_\(G\)) as random variables. The lower-case letters (_e.g.,_\(g\)) are samples of variables, and the blackboard bold typefaces (_e.g.,_\(\mathbb{G}\)) denote the sample spaces. Let \(g=(\mathbf{A},\mathbf{X})\in\mathbb{G}\) denote a graph, where \(\mathbf{A}\) and \(\mathbf{X}\) are its adjacency matrix and node features, respectively. It is assigned with a label \(y\in\mathbb{Y}\) with a fixed labeling rule \(\mathbb{G}\rightarrow\mathbb{Y}\). Let \(\mathcal{D}=\{(g_{i},y_{i})\}\) denote a dataset that is divided into a training set \(\mathcal{D}_{\mathrm{tr}}=\{(g_{i}^{e},y_{i}^{e})\}_{e\in\mathcal{E}_{\mathrm{ tr}}}\) and a test set \(\mathcal{D}_{\mathrm{te}}=\{(g_{i}^{e},y_{i}^{e})\}_{e\in\mathcal{E}_{\mathrm{te}}}\). \(\mathcal{E}_{\mathrm{tr}}\) and \(\mathcal{E}_{\mathrm{te}}\) are the index sets of training and test environments, respectively.

### Definitions and Problem Formations

In this work, we focus on the graph classification scenario, which aims to train models with \(\mathcal{D}_{\mathrm{tr}}\) and infer the labels in \(\mathcal{D}_{\mathrm{te}}\), such as molecular property prediction. From the viewpoints of invariant learning and stable learning, the inner mechanism of the labeling rule \(\mathbb{G}\rightarrow\mathbb{Y}\) is usually assumed to depend on the stable features [17; 18; 5; 8; 21], which are informative substructures of the entire graph. The relationship between the stable features and labels is assumed to be invariant across different environments, which makes OOD generalization possible [12]. Environmental features in graph data are assumed to have no causal-effect on labels. For instance, the chemical properties of molecules are mainly determined by specific functional groups, which can be regarded as stable features [17; 5; 19; 8]. Conversely, their scaffold structures, often irrelevant to their properties, can be seen as environmental features [20; 8].

Due to the instability of environmental features and the limitations in the data collection process, the training and test distributions are often inconsistent in real-world scenarios, _i.e.,_\(P_{\mathrm{tr}}(G,Y)\neq P_{\mathrm{te}}(G,Y)\), which leads to two main types of distribution shifts [2; 12]: (1) Correlation shift (_aka._ spurious correlation or concept shift [2]) refers to \(P_{\mathrm{tr}}(G|Y)+P_{\mathrm{te}}(G|Y),P_{\mathrm{tr}}(G)=P_{\mathrm{te}}(G)\). It indicates that there exist spurious statistical correlations in the training data, while these correlations might not hold in the test data. (2) Covariate shift denotes \(P_{\mathrm{tr}}(G|Y)=P_{\mathrm{te}}(G|Y),P_{\mathrm{tr}}(G)\neq P_{\mathrm{te}}(G)\), which means that there exist new features, _e.g.,_ environmental features, in the test data. It may be ascribed to either insufficient quantity or diversity of data in the training set, as well as the unknown characteristics of the test environments. We provide formal definitions and examples of these two distribution shifts in Appendix A. Here, inspired by the prior study [12], we offer a formal definition to measure the graph covariate shift.

**Definition 2.1** (**Graph Covariate Shift**): _Let \(P_{\mathrm{tr}}\) and \(P_{\mathrm{te}}\) denote the probability functions of the training and test distributions. We measure the covariate shift between distributions \(P_{\mathrm{tr}}\) and \(P_{\mathrm{te}}\) as_

\[\mathrm{GCS}(P_{\mathrm{tr}},P_{\mathrm{te}})=\frac{1}{2}\int_{\mathcal{S}}|P _{\mathrm{tr}}(g)-P_{\mathrm{te}}(g)|dg,\] (1)

_where \(\mathcal{S}=\{g\in\mathbb{G}\mid P_{\mathrm{tr}}(g)\cdot P_{\mathrm{te}}(g)=0\}\), which covers the features (e.g., environmental features) that do not overlap between the two distributions._

\(\mathrm{GCS}(\cdot,\cdot)\) is always bounded in \([0,1]\). The issue of graph covariate shift is very common in practice. For example, we often need to train models on past molecular graphs, and hope that the model can predict the chemical properties of future molecules with new features, _e.g.,_ new scaffolds [20]. In addressing the graph OOD issue, a majority of the strategies [17; 5; 18; 29; 21] grounded in invariant graph learning aim to pinpoint stable or invariant features. This is mainly accomplished by minimizing empirical risks across an array of training environments. Nonetheless, these methodologies frequently operate under the assumption of a shared input space across training and test data. This assumption, however, often fails on covariate shift. It presents substantial challenges for these models when it comes to accurately identifying stable features within new testing environments. Consequently, while these methods typically exhibit satisfactory performance in managing correlation shifts, they often underperform in the face of covariate shifts. In this work, we focus on the covariate shift issue in graph classification, and we also give a formal definition of this problem as follows.

**Problem 2.2** (Graph Classification under Covariate Shift): _Given the training and test sets with environment sets \(\mathcal{E}_{\mathrm{tr}}\) and \(\mathcal{E}_{\mathrm{te}}\), they follow distributions \(P_{\mathrm{tr}}\) and \(P_{\mathrm{te}}\), and satisfy: \(\mathrm{GCS}(P_{\mathrm{tr}},P_{\mathrm{te}})>\epsilon\), where \(\epsilon\in(0,1)\) represents the degree of covariate shift. We aim to use the data collected from training environments \(\mathcal{E}_{\mathrm{tr}}\), and learn a powerful graph classifier \(f^{*}:\mathbb{G}\rightarrow\mathbb{Y}\) that performs well in all possible test environments \(\mathcal{E}_{\mathrm{te}}\):_

\[f^{*}=\operatorname*{arg\,min}_{f}\;\sup_{e\in\mathcal{E}_{\mathrm{te}}} \mathbb{E}^{e}[\ell(f(g),y)],\] (2)

_where \(\mathbb{E}^{e}[\ell(f(g),y)]\) is the empirical risk on the environment \(e\), and \(\ell(\cdot,\cdot)\) is the loss function._

## 3 Methodology

To solve Problem 2.2, our idea is to generate new graphs through data augmentation. In this section, we first propose two principles for graph augmentation. Guided by these principles, we design a novel graph augmentation method, AIA, which can effectively address the covariate shift issue.

### Two Principles for Graph Augmentation

We can observe that covariate shift is mainly caused by the scarcity of training environments. Hence, we first propose the discrepancy principle for graph augmentation.

**Principle 3.1** (Environmental Feature Discrepancy): _Given a graph set \(\{g\}\) with distribution function \(P\), let \(T(\cdot)\) denote an augmentation function that augments graphs \(\{T(g)\}\) to distribution \(\widetilde{P}\). Then \(T(\cdot)\) should meet \(\mathrm{GCS}(P,\widetilde{P})\to 1\)._

From the perspective of data distribution, it requires that \(\widetilde{P}\) should keep away from the original distribution \(P\). From the perspective of data instances, it emphasizes the discrepancy in the environments between the generated graphs and the original graphs. Since it does not give constraints on stable features, we here propose the second principle for graph augmentation.

**Principle 3.2** (Stable Feature Consistency): _Given a set of graphs \(\{g\}\) with a corresponding stable feature set \(\{g_{\mathrm{sta}}=(\mathbf{A}_{\mathrm{sta}},\mathbf{X}_{\mathrm{sta}})\}\). Let \(T(\cdot)\) denote an augmentation function that augments graphs \(\{T(g)\}\) with a corresponding stable feature set \(\{\widetilde{g}_{\mathrm{sta}}=(\mathbf{\widehat{A}}_{\mathrm{sta}}, \mathbf{\widetilde{X}}_{\mathrm{sta}})\}\). Then \(T(\cdot)\) should meet \(\mathbb{E}[\,|\mathbf{A}_{\mathrm{sta}}-\mathbf{\widehat{A}}_{\mathrm{sta}} \big{|}_{F}^{2}]\to 0\) and \(\mathbb{E}[\,|\mathbf{X}_{\mathrm{sta}}-\mathbf{\widehat{X}}_{\mathrm{sta}} \|_{F}^{2}]\to 0\), where \(\|\cdot\|_{F}\) is the Frobenius norm._

It necessitates that the stable features of the generated graphs should maintain consistency with those of the original graphs. This principle ensures the preservation of these stable features within the original training data, thereby safeguarding sufficient information pertaining to the labels. Consequently, this principle enhances the potential for generalization.

### Out-of-distribution Exploration

Given a GNN model \(f(\cdot)\) with parameters \(\theta\), we decompose \(f=\Phi\circ h\), where \(h(\cdot):\mathbb{G}\rightarrow\mathbb{R}^{d}\) is a graph encoder to yield \(d\)-dimensional representations, and \(\Phi(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{Y}\) is a classifier. To comply with Principle 3.1, we need to do OOD exploration. Inspired by distributionally robust optimization [30; 31], we consider the following optimization objective:

\[\min_{\theta}\left\{\;\sup_{\widetilde{P}}\{\mathbb{E}_{\widetilde{P}}[\ell( f(g),y)]:D(\widetilde{P},P)\leq\rho\}\right\},\] (3)

where \(P\) and \(\widetilde{P}\) are the original and explored data distributions, respectively. \(D(\cdot,\cdot)\) is a distance metric between two probability distributions. The solution to Equation (3) guarantees the generalization within a robust radius \(\rho\) of the distribution \(P\). To better measure the distance between distributions,as suggested by [32], we adopt the Wasserstein distance [33; 34] as the distance metric. The distance metric function can be defined as \(D(\widetilde{P},P)=\inf_{\mu\in\Gamma(\widetilde{P},P)}\mathbb{E}_{\mu}[c( \widetilde{g},g)]\), where \(\Gamma(\widetilde{P},P)\) is the set of all couplings of \(\widetilde{P}\) and \(P\); \(c(\cdot,\cdot)\) is the cost function. Studies [35; 34] also suggest that the distances in representation space typically correspond to semantic distances. Hence, we define the cost function in the representation space and give the transportation cost as \(c(\widetilde{g},g)=\|h(\widetilde{g})-h(g)\|_{2}^{2}\). It denotes the "cost" of augmenting the graph \(g\) to \(\widetilde{g}\). We can observe that it is difficult to set a proper \(\rho\). Instead, we consider the Lagrangian relaxation for a fixed penalty coefficient \(\gamma\). Inspired by [32], we can reformulate Equation (3) as follows:

\[\min_{\theta}\bigg{\{}\sup_{\widetilde{P}}\{\mathbb{E}_{\widetilde{P}}[\ell(f( g),y)]-\gamma D(\widetilde{P},P)\}=\mathbb{E}_{P}[\phi(f(g),y)]\bigg{\}}\,,\] (4)

where \(\phi(f(g),y):=\sup_{\widetilde{g}\in\mathbb{G}}\{\ell(f(\widetilde{g}),y)- \gamma c(\widetilde{g},g)\}\). And we define \(\phi(f(g),y)\) as the robust surrogate loss. If we conduct gradient descent on the robust surrogate loss, we will have:

\[\nabla_{\theta}\phi(f(g),y)=\nabla_{\theta}\ell(f(\widetilde{g}^{*}),y),\ \ \mathrm{where}\ \ \widetilde{g}^{*}=\underset{\widetilde{g}\in\mathbb{G}}{\arg\max}\{\ell(f( \widetilde{g}),y)-\gamma c(\widetilde{g},g)\}.\] (5)

\(\widetilde{g}^{*}\) is an augmented view of the original data \(g\). Hence, to achieve OOD exploration, we need to perform graph data augmentation via Equation (5) on the original data \(g\).

### Implementations of AIA

Equation (5) endows the ability of OOD exploration to data augmentation, which makes the augmented data meet the discrepancy principle. To achieve the consistency principle, we also need to capture stable features. Hence, we design a graph augmentation strategy: Adversarial Invariant Augmentation (AIA). The overview of the proposed framework is depicted in Figure 2, which mainly consists of two components: adversarial augmenter and stable feature generator. Adversarial augmenter achieves OOD exploration through adversarial data augmentation; meanwhile, the stable feature generator keeps stable feature consistency by identifying stable features from data. Below we elaborate on the implementation details.

**Adversarial Augmenter & Stable Feature Generator.** We design two networks, adversarial augmenter \(T_{\theta_{1}}(\cdot)\) and stable feature generator \(T_{\theta_{2}}(\cdot)\), which generate masks for nodes and edges of graphs. They have the same structure and are parameterized by \(\theta_{1}\) and \(\theta_{2}\), respectively. Given an input graph \(g=(\mathbf{A},\mathbf{X})\) with \(n\) nodes, mask generation network first obtains the node representations via a GNN encoder \(\widetilde{h}(\cdot)\). To judge the importance of nodes and edges, it adopts two MLP networks \(\mathrm{MLP}_{1}(\cdot)\) and \(\mathrm{MLP}_{2}(\cdot)\) to generate the soft node mask matrix \(\mathbf{M}^{x}\in\mathbb{R}^{n\times 1}\) and edge mask matrix \(\mathbf{M}^{u}\in\mathbb{R}^{n\times n}\) for graph data, respectively. In summary, the mask generation network can be decomposed as:

\[\mathbf{Z}=\widetilde{h}(g),\quad\mathbf{M}^{x}_{i}=\sigma(\mathrm{MLP}_{1}( \mathbf{h}_{i})),\quad\mathbf{M}^{a}_{ij}=\sigma(\mathrm{MLP}_{2}([\mathbf{z} _{i},\mathbf{z}_{j}])),\] (6)

where \(\mathbf{Z}\in\mathbb{R}^{n\times d}\) is node representation matrix, whose \(i\)-th row \(\mathbf{z}_{i}=\mathbf{Z}[i,:]\) denotes the representation of node \(i\), and \(\sigma(\cdot)\) is the sigmoid function that maps the mask values \(\mathbf{M}^{x}_{i}\) and \(\mathbf{M}^{a}_{ij}\) to \([0,1]\).

Figure 2: The overview of Adversarial Invariant Augmentation (AIA) Framework.

**Adversarial Invariant Augmentation.** To estimate \(\widetilde{g}^{*}\) in Equation (5), we define the following adversarial learning objective:

\[\max_{\theta_{1}}\left\{\mathcal{L}_{\mathrm{adv}}=\mathbb{E}_{P_{\mathrm{tr}}} [\ell(f(T_{\theta_{1}}(g)),y)-\gamma c(T_{\theta_{1}}(g),g)]\right\}.\] (7)

Then we can augments the graph by \(T_{\theta_{1}}(g)=(\mathbf{A}\odot\mathbf{M}_{\mathrm{adv}}^{a},\mathbf{X} \odot\mathbf{M}_{\mathrm{adv}}^{x})\), where \(\odot\) is the broadcasted element-wise product. Although adversarially augmented graphs guarantee environmental discrepancy, it might destroys the stable parts. Therefore, we utilize the stable feature generator \(T_{\theta_{2}}(\cdot)\) to capture stable features and combine them with different environmental features. Following the sufficiency and invariance conditions [17; 3; 18; 5; 8; 29], we define the stable feature learning objective as:

\[\min_{\theta,\theta_{2}}\left\{\mathcal{L}_{\mathrm{sta}}=\mathbb{E}_{P_{ \mathrm{tr}}}[\ell(f(T_{\theta_{2}}(g)),y)+\ell(f(\widetilde{g}),y)]\right\},\] (8)

where \(\widetilde{g}=(\mathbf{A}\odot\widetilde{\mathbf{M}}^{a},\mathbf{X}\odot \widetilde{\mathbf{M}}^{x})\) is the augmented graph. It adopts the mask combination strategy: \(\widetilde{\mathbf{M}}^{a}=(\mathbf{1}^{a}-\mathbf{M}_{\mathrm{sta}}^{a}) \odot\mathbf{M}_{\mathrm{adv}}^{a}+\mathbf{M}_{\mathrm{sta}}^{a}\) and \(\widetilde{\mathbf{M}}^{x}=(\mathbf{1}^{x}-\mathbf{M}_{\mathrm{sta}}^{x}) \odot\mathbf{M}_{\mathrm{adv}}^{x}+\mathbf{M}_{\mathrm{sta}}^{x}\), where \(\mathbf{M}_{\mathrm{sta}}^{a}\) and \(\mathbf{M}_{\mathrm{sta}}^{x}\) are generated by \(T_{\theta_{2}}(\cdot)\), \(\mathbf{1}^{a}\) and \(\mathbf{1}^{x}\) are all-one matrices, and if there is no edge between node \(i\) and node \(j\), then we set \(\mathbf{1}_{ij}^{a}\) to 0. Now we explain this combination strategy. Taking \(\widetilde{\mathbf{M}}^{x}\) as an example, since \(\mathbf{M}_{\mathrm{sta}}^{x}\) denotes the captured stable regions via \(T_{\theta_{2}}(\cdot)\), \(\mathbf{1}^{x}-\mathbf{M}_{\mathrm{sta}}^{x}\) represents the complementary parts, which are environmental regions. \(\mathbf{M}_{\mathrm{adv}}^{x}\) represents the adversarial perturbation, so \((\mathbf{1}^{x}-\mathbf{M}_{\mathrm{sta}}^{x})\odot\mathbf{M}_{\mathrm{adv}}^ {x}\) is equivalent to applying the adversarial perturbation on environmental features, meanwhile, sheltering the stable features. Finally, \(+\mathbf{M}_{\mathrm{sta}}\) signifies that the augmented data should preserve the original stable features. Consequently, it satisfies both principles. Upon analysis of Equation (8), the first term implies that the stable features are sufficient for making right predictions. The second term promotes right and invariant predictions under generated environments utilizing stable features.

**Regularization.** For Equation (7), the adversarial optimization tends to remove more nodes and edges, so we should also constrain the perturbations. Although Equation (8) satisfies the sufficiency and invariance conditions, it is necessary to impose constraints on the ratio of the stable features to prevent trivial solutions. Hence, we first define the regularization function \(r(\mathbf{M},k,\lambda)=(\sum_{ij}\mathbf{M}_{ij}/k-\lambda)+(\sum_{ij}\mathbb{I }[\mathbf{M}_{ij}>0]/k-\lambda)\), where \(k\) is the total number of elements to be constrained, \(\mathbb{I}\in\{0,1\}\) is an indicator function. The first term penalizes the average ratio close to \(\lambda\), while the second term encourages an uneven distribution. Given a graph with \(n\) nodes and \(m\) edges, we define the regularization term for adversarial augmentation and stable feature learning as:

\[\mathcal{L}_{\mathrm{reg_{1}}}=\mathbb{E}_{P_{\mathrm{tr}}}[r(\mathbf{M}_{ \mathrm{adv}}^{x},n,\lambda_{a})+r(\mathbf{M}_{\mathrm{adv}}^{a},m,\lambda_{ a})],\] (9)

\[\mathcal{L}_{\mathrm{reg_{2}}}=\mathbb{E}_{P_{\mathrm{tr}}}[r(\mathbf{M}_{ \mathrm{sta}}^{x},n,\lambda_{s})+r(\mathbf{M}_{\mathrm{sta}}^{a},m,\lambda_{ s})],\] (10)

where \(\lambda_{s}\in(0,1)\) is the ratio of stable features, we usually set \(\lambda_{a}=1\) for adversarial learning, which can alleviate excessive perturbations. The algorithm is provided in Appendix D.1.

## 4 Theoretical Discussions

In this section, we engage in theoretical discussions to elucidate our learning objective and its connections with the covariate shift. We first explore the relationship between our optimization objective and the discrepancy principle. Recalling the optimization objective of Equation 3, we aspire to identify a distribution \(\widetilde{P}\) that can manifest within a Wasserstein ball [36], which is centered on distribution \(P\), with distance \(\rho\) serving as the radius. Under appropriate conditions, we find that our learning optimization objective can establish a close connection with OOD exploration.

**Proposition 4.1**: _Consider a probability distribution \(P\) defined over a measurable space \((\Omega,\mathcal{F})\), where \(\Omega\) denotes the sample space and \(\mathcal{F}\) is a \(\sigma\)-algebra on \(\Omega\). We construct two Wasserstein balls with \(P\) at their center and radii \(\rho_{1}\) and \(\rho_{2}\) respectively. Utilizing Equation 3, we generate two distinct distributions, \(\widetilde{P}_{1}\) and \(\widetilde{P}_{2}\), within the space \((\Omega,\mathcal{F})\). If (i) \(P\) is an isotropic distribution; (ii) \(\exists\mathbf{x}_{1},\mathbf{x}_{2}\in\Omega\) such that \(P(\mathbf{x})=\widetilde{P}_{1}(\mathbf{x}-\mathbf{x}_{1})=\widetilde{P}_{2}( \mathbf{x}-\mathbf{x}_{2})\); (iii) \(\rho_{1}<\rho_{2}\), then we have \(\mathrm{GCS}(P,\widetilde{P}_{1})\leq\mathrm{GCS}(P,\widetilde{P}_{2})\)._

This suggests that by appropriately increasing the robustness radius in AIA, we can effectively amplify the covariate shift between the training and generated distributions. This in turn underscores the reliability of our discrepancy principle, to a certain degree. Comprehensive proofs and detailed discussions supporting these conclusions can be found in Appendix B.

## 5 Experiments

In this section, we conduct extensive experiments to answer the following **R**esearch **Q**uestions:

* **RQ1:** Compared to existing efforts, how does AIA perform under covariate shift?
* **RQ2:** Can the proposed AIA achieve the principles of environmental feature discrepancy and stable feature consistency, thereby alleviating the graph covariate shift?
* **RQ3:** How do the different components and hyperparameters of AIA affect the performance?

### Experimental Settings

**Datasets.** We use graph OOD datasets [2] and OGB datasets [20], which include Motif, CMNIST, Molbbbp, and Molhiv. Following [2], we adopt the base, color, size, and scaffold data splitting to create various covariate shifts. The details of the datasets, metrics, implementations, and other settings are provided in Appendix D.2 and D.4. More experiments are provided in Appendix E.

**Baselines.** We adopt 16 baselines, which can be divided into the following three specific categories:

* **General Generalization Algorithms:** ERM, IRM [14], GroupDRO [31], VREx [37].
* **Graph Generalization Algorithms:** DIR [17], CAL [5], GSAT [38], OOD-GNN [39], StableGNN [40], CIGA [41], DisC [21].
* **Graph Augmentation:** DropEdge [25], GREA [18], FLAG [24], M-Mixup [26], \(\mathcal{G}\)-Mixup [7].

### Main Results (RQ1)

We first make comparisons with various baselines in Table 1, and have the following observations:

Most generalization and augmentation methods fail under covariate shift. VREx achieves a 2.81% improvement on Motif (base). For two shifts of Molhiv, data augmentation methods GREA and DropEdge obtain 1.20% and 0.77% improvements. The invariant learning methods, _i.e.,_ DIR and CAL also obtain 4.60% and 1.53% improvements on CMNIST and Molbbbp (size). Unfortunately, none of the methods consistently outperform ERM. For example, GREA and DropEdge perform poorly on Motif (base), \(\downarrow\)11.92% and \(\downarrow\)23.58%. DIR and CAL also fail on Molhiv. These show that both invariant learning and data augmentation methods have their own weaknesses, which lead to unstable performance when facing complex and diverse covariate shifts from different datasets.

AIA consistently outperforms most baseline methods. Compared with ERM, AIA can obtain significant improvements. For two types of covariate shifts on Motif, AIA surpasses ERM by 4.98% and 4.11%, respectively. In contrast to the large performance variances on different datasets achieved by

\begin{table}
\begin{tabular}{c l c c c c c c c} \hline \hline \multirow{2}{*}{Type} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{Motif} & \multicolumn{2}{c}{CMNIST} & \multicolumn{2}{c}{Molbbbp} & \multicolumn{2}{c}{Molbiliv} \\ \cline{3-9}  & & base & size & color & scaffold & size & scaffold & size \\ \hline \multirow{4}{*}{General} & ERM & 68.66\({}_{4.25}\) & 51.74\({}_{2.88}\) & 28.60\({}_{1.87}\) & 68.10\({}_{1.68}\) & 78.29\({}_{3.76}\) & 69.58\({}_{2.51}\) & 59.94\({}_{2.37}\) \\  & IRM & 70.65\({}_{4.17}\) & 51.41\({}_{1.78}\) & 78.23\({}_{2.13}\) & 67.22\({}_{2.11}\) & 77.56\({}_{2.48}\) & 67.97\({}_{1.84}\) & 59.00\({}_{2.92}\) \\ Generalization & GroupDRO & 68.24\({}_{8.92}\) & 51.95\({}_{3.68}\) & 29.07\({}_{3.14}\) & 66.67\({}_{2.39}\) & 79.27\({}_{2.13}\) & 70.64\({}_{2.27}\) & 58.98\({}_{2.16}\) \\  & VREx & 71.47\({}_{4.69}\) & 52.67\({}_{5.54}\) & 28.48\({}_{4.27}\) & 68.74\({}_{4.10}\) & 78.62\({}_{2.37}\) & 70.77\({}_{2.24}\) & 58.53\({}_{2.38}\) \\ \hline \multirow{4}{*}{Graph} & DIR & 62.07\({}_{4.85}\) & 52.27\({}_{4.56}\) & 33.20\({}_{6.17}\) & 66.86\({}_{2.25}\) & 76.40\({}_{4.43}\) & 68.07\({}_{2.29}\) & 58.08\({}_{2.31}\) \\  & CAL & 65.63\({}_{4.29}\) & 51.85\({}_{2.50}\) & 27.93\({}_{2.34}\) & 68.06\({}_{2.60}\) & 79.50\({}_{4.81}\) & 67.37\({}_{3.51}\) & 57.95\({}_{2.24}\) \\  & GSAT & 62.80\({}_{4.11}\) & 53.20\({}_{3.53}\) & 28.71\({}_{4.16}\) & 67.84\({}_{7.14}\) & 75.63\({}_{3.53}\) & 66.63\({}_{1.39}\) & 58.06\({}_{4.19}\) \\  & OOD-GNN & 61.10\({}_{1.97}\) & 52.61\({}_{4.67}\) & 26.49\({}_{2.94}\) & 66.72\({}_{2.13}\) & 79.48\({}_{4.19}\) & 70.46\({}_{1.97}\) & 60.60\({}_{3.07}\) \\  & StableGNN & 57.07\({}_{4.10}\) & 46.93\({}_{8.85}\) & 28.38\({}_{4.19}\) & 66.74\({}_{1.30}\) & 77.47\({}_{4.89}\) & 68.44\({}_{4.13}\) & 56.71\({}_{2.79}\) \\  & CIGA & 66.43\({}_{1.13}\) & 49.14\({}_{8.34}\) & 32.22\({}_{2.27}\) & 64.92\({}_{2.09}\) & 65.98\({}_{3.81}\) & 69.40\({}_{2.39}\) & 59.55\({}_{2.56}\) \\  & DisC & 51.08\({}_{3.38}\) & 50.39\({}_{1.15}\) & 24.99\({}_{1.78}\) & 67.12\({}_{2.11}\) & 56.59\({}_{1.09}\) & 68.07\({}_{4.15}\) & 58.76\({}_{6.91}\) \\ \hline \multirow{4}{*}{Graph} & DropEdge & 45.08\({}_{4.46}\) & 45.63\({}_{4.61}\) & 24.65\({}_{2.50}\) & 66.49\({}_{1.55}\) & 78.32\({}_{2.44}\) & 70.78\({}_{1.38}\) & 58.53\({}_{1.12}\) \\  & GREA & 56.74\({}_{6.23}\) & 54.31\({}_{4.02}\) & 29.02\({}_{2.36}\) & 67.24\({}_{6.79}\) & 74.34\({}_{3.52}\) & 67.79\({}_{2.56}\) & 67.41\({}_{2.29}\) \\ \cline{1-1}  & FLAG & 61.12\({}_{5.39}\) & 51.66\({}_{4.14}\) & 32.30\({}_{2.69}\) & 67.69\({}_{2.36}\) & 79.26\({}_{2.26}\) & 68.45\({}_{2.30}\) & 60.59\({}_{2.95}\) \\ \cline{1-1}  & M-Mixup & 70.08\({}_{3.82}\) & 51.48\({}_{4.19}\) & 26.47\({}_{4.45}\) & 68.75\({}_{3.43}\) & 78.92\({}_{2.43}\) & 68.88\({}_{2.43}\) & 59.03\({}_{3.31}\) \\ \cline{1-1}  & \(\mathcal{G}\)-Mixup & 59.66\({}_{6.73}\) & 52.81\({}_{6.73}\) & 31.85\({}_{5.82}\) & 67.44\({}_{1.62}\) & 78.55\({}_{1.16}\) & 70.01\({}_{2.52}\) & 59.34\({}_{2.43}\) \\ \cline{1-1}  & AIA (ours) & **73.64\({}_{8.15}\)** & **55.85\({}_{7.98}\)** & **36.37\({}_{4.44}\)** & **70.79\({}_{1.53}\)** & **51.03\({}_{4.15}\)** & **61.64\({}_{4.37}\)** \\ \hline \baselines, AIA consistently obtains the leading performance across the board. For CMNIST, AIA achieves a performance improvement of 3.17% compared to the best baseline DIR. For Motif, the performance is improved by 2.17% and 1.72% compared to VREx and GREA. These results illustrate that AIA can overcome the shortcomings of invariant learning and data augmentation. Armed with the principles of environmental feature diversity and stable feature invariance, AIA achieves stable and consistent improvements on different datasets with various covariate shifts. In addition, although we focus on covariate shift in this work, we also carefully check the performance of AIA under correlation shift, and the results are presented in Appendix E.

### In-depth Analyses (RQ2)

In this section, we conduct qualitative and quantitative experiments to support our two principles. Firstly, we utilize \(\mathrm{GCS}(\cdot,\cdot)\) as the measurement to quantify the degree of covariate shift. The detailed estimation procedure is provided in Appendix C. We select four different domains, _i.e.,_ base, size, color and scaffold, to create covariate shifts. The experimental results are shown in Table 2. We calculated covariate shifts between the augmentation distribution \(P_{\mathrm{aug}}\) with the training \(P_{\mathrm{tr}}\) or test distribution \(P_{\mathrm{te}}\). "Aug-Train" and "Aug-Test" represent \(\mathrm{GCS}(P_{\mathrm{aug}},P_{\mathrm{tr}})\) and \(\mathrm{GCS}(P_{\mathrm{aug}},P_{\mathrm{te}})\), respectively. From the results in Table 2, we make these observations.

**Discrepancy Principle.** The term "Original" denotes the training distribution prior to augmentation. It's observed that substantial covariate shifts exist between the training and test distributions, ranging from 0.419 to 0.557. The DropEdge technique notably expands _Aug-Train_, with a range of 0.627 to 0.851, while concurrently increasing _Aug-Test_, as evidenced by CMNIST (0.490 to 0.539) and Molbbbb (0.419 to 0.737). However, a distribution that deviates excessively from the test distribution may not effectively address the issue of covariate shift. FLAG, which perturbs only the node features, yields minor values in both _Aug-Train_ and _Aug-Test_. \(\mathcal{G}\)-Mixup notably augments _Aug-Train_ by generating OOD samples, but doesn't necessarily limit _Aug-Test_. Finally, AIA extends the disparity with the training distribution by augmenting environmental features, signifying that AIA can adeptly implement the principle of environmental feature discrepancy. Simultaneously, the imposed consistency constraint on stable features restricts the generated distribution from straying too far from the test distribution, as observed in Motif-base (0.557 to 0.462), Motif-size (0.522 to 0.098), and CMNIST (0.490 to 0.307).

**Augmentation Diversity.** We further delve into the diversity of data augmentation. The concept of augmentation diversity stems from the intuition that augmentations with greater degrees of freedom yield better performance [42]. Accordingly, we propose conditional entropy to

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Motif (base)} & \multicolumn{2}{c}{Motif (size)} & \multicolumn{2}{c}{CMNIST (color)} & \multicolumn{2}{c}{Molbbbb (scaffold)} \\ \cline{2-9}  & Aug-Train & Aug-Test & Aug-Train & Aug-Test & Aug-Train & Aug-Test & Aug-Train & Aug-Test \\ \hline Original & 0 & 0.557\(\pm\)0.141 & 0 & 0.522\(\pm\)0.421 & 0 & 0.490\(\pm\)0.226 & 0 & 0.419\(\pm\)0.079 \\ \hline DropEdge & 0.772\(\pm\)0.213 & 0.515\(\pm\)0.033 & 0.851\(\pm\)0.018 & 0.161\(\pm\)0.271 & 0.627\(\pm\)0.186 & 0.539\(\pm\)0.260 & 0.758\(\pm\)0.192 & 0.737\(\pm\)0.218 \\ FLAG & 0.001\(\pm\)0.001 & 0.533\(\pm\)0.016 & 0.002\(\pm\)0.018 & 0.507\(\pm\)0.121 & 0.003\(\pm\)0.002 & 0.442\(\pm\)0.002 & 0.001\(\pm\)0.001 & 0.413\(\pm\)0.008 \\ \(\mathcal{G}\)-Mixup & 0.690\(\pm\)0.186 & 0.472\(\pm\)0.038 & 0.816\(\pm\)0.154 & 0.299\(\pm\)0.343 & 0.408\(\pm\)0.228 & 0.351\(\pm\)0.138 & 0.551\(\pm\)0.258 & 0.545\(\pm\)0.231 \\ AIA (ours) & 0.369\(\pm\)0.169 & 0.462\(\pm\)0.063 & 0.649\(\pm\)0.143 & 0.098\(\pm\)0.070 & 0.516\(\pm\)0.106 & 0.307\(\pm\)0.106 & 0.422\(\pm\)0.049 & 0.393\(\pm\)0.028 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Covariate shift comparisons with different augmentation strategies.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Method & Full Graph & Env. Feature & Sta. Feature \\ \hline DropEdge & 0.999\(\pm\)0.065 & 0.933\(\pm\)0.029 & 0.971\(\pm\)0.067 \\ AIA (ours) & 0.561\(\pm\)0.223 & 0.508\(\pm\)0.136 & 0.259\(\pm\)0.106 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Augmentation Diversity.

Figure 3: Visualizations of the augmented graphs via AIA.

measure the diversity of generated data: \(H(\tilde{G}|G)=-\mathbb{E}_{G}[\sum_{\tilde{g}}p(\tilde{g}|G)\mathrm{log}\big{(}p (\tilde{g}|G)\big{)}]\), where \(\tilde{G}\) and \(G\) represent the generated graph and original graph, respectively. To substantiate our ability to manage perturbed environmental features while preserving stable features, we examine diversity at the feature level, _i.e.,_ stable and environmental features. We employ the Motif dataset for validation due to its inclusion of ground-truth labels for stable and environmental features. The results in Table 3 reveal that our approach can guarantee the diversity of environmental features while constraining the variation of stable features.

To substantiate two principles of AIA, we present a selection of augmented graphs in Figure 3. These graphs are randomly sampled during the training phase. In the original Motif, the stable feature, is represented by the green portion and its type determines the label, while the yellow portion signifies the base-graph, or the environmental feature. Figure 3 (_Right_) exhibits the augmented samples generated during training. Nodes depicted in darker colors and edges with broader lines indicate higher soft-mask values. These results lead us to several noteworthy observations.

**Visualization Analyses.** AIA primarily perturbs the environmental features, while leaving the stable components undisturbed. In the Motif dataset, the base-graph represents a _ladder_ and the motif-graph signifies a _house_. Following augmentation, the nodes and edges of the _ladder_ graph undergo perturbations. However, the _house_ component remains consistently stable throughout the training process. It shows that AIA successfully adheres to the proposed two principles, thus providing empirical support for our claims. Furthermore, under covariate shift, we also depict the stable features identified by AIA in comparison to other baseline methods (refer to Appendix E.4). It further underscores the limitations of alternative methods and highlights the superior performance of AIA.

### Ablation Study (RQ3)

**Adversarial Augmentation & Stable Feature Learning.** As illustrated in Figure 4 (_Left_), "w/o Adv" and "w/o Sta" denote AIA without adversarial augmentation and without stable feature learning, respectively. RDIA is a variant that replaces adversarial augmentation in AIA with random augmentation (_i.e.,_ random masks). The performance degrades when either component is used independently, compared to their combined application in AIA. The removal of adversarial perturbations results in a loss of the invariance condition inherent in stable feature learning [17; 29], leading to suboptimal outcomes. Conversely, the sole use of adversarial augmentation disrupts the stable features, thereby diminishing the performance. RDIA surpasses ERM, yet falls short of AIA, indicating that although randomness can foster discrepancy, it is less effective than the adversarial strategy.

**Sensitivity Analysis.** We conduct experiments to explore the sensitivities of ratio \(\lambda_{s}\) and penalty coefficient \(\gamma\). The results are displayed in Figure 4 (_Middle_) and (_Right_). \(\lambda_{s}\) with 0.3-0.8 performs well on Motif and Molbbbp, while Molhiv is better in 0.1-0.3. It indicates that \(\lambda_{s}\) is a dataset-sensitive hyper-parameter that needs careful tuning. For \(\gamma\), the appropriate values range from 0.1-1.5.

## 6 Related Work

**Graph Data Augmentation**[22; 23; 43] enlarges the training distribution by perturbing features in graphs. Recent studies [44; 13] observe that it often outperforms other generalization efforts [14; 31]. DropEdge [25] randomly removes edges, while FLAG [24] augments node features with an adversarial strategy. M-Mixup [26] interpolates graphs in semantic space. However, studies [14; 45]

Figure 4: (_Left_): Different components in AIA. (_Middle_): Different ratios \(\lambda_{s}\) of stable features. (_Right_): Different penalties \(\gamma\). Dashed lines denote the ERM.

point out that stable features are the key to OOD generalization. These augmentation efforts are prone to perturb the stable features, which easily lose control of the augmented data distribution.

**Invariant Graph Learning** has been widely adopted by recent works as a paradigm for handling distribution shifts on graphs. The pioneering works [3; 17] leverage the causal invariance principle to model the invariant predictive patterns in data for the OOD generalization purpose. With the similar spirit, GREA [18] and CAL [5] aim to learn stable features by considering different environments. Some other works also utilize invariant learning to develop generalizable models and algorithms for improving the generalization _w.r.t._ molecular graphs [8] and recommender systems [9].

**Out-of-Distribution Learning on Graphs** has aroused wide research interest in the graph learning community. One line of research is centered around the goal of improving the OOD generalization capabilities of models when encountered with test data from new unseen distributions [46; 3; 47; 39; 40; 17; 38; 48; 5]. Another line of research, differently, aims to identify the OOD data in the testing set and improve the reliablity of models against OOD data for which the model should reject for prediction [49; 50; 51]. The latter task is called Out-of-Distribution Detection in the literature and serves as another under-explored area that has different technical aspect from the present work. Due to space constraints, we put more discussions of other related studies in Appendix G.

## 7 Conclusion

In this study, we address the pervasive yet largely unexplored issue of covariate shift in graph learning. We introduce a novel graph augmentation method, AIA, grounded in two principles: environmental feature discrepancy and stable feature consistency. The discrepancy principle enables the model to explore new environments, thereby facilitating better generalization to potentially unseen environments. Meanwhile, the consistency principle maintains the integrity of stable features. We conduct extensive comparisons with various baseline models and perform thorough analyses.

## 8 Limitations and Broader Impacts

This paper presents a graph augmentation method, AIA, designed to bolster the academic community's application of data augmentation methodologies. We do not foresee any immediate, direct, or adverse societal implications resulting from our study's findings. We also present additional discussions regarding AIA's limitations and potential future work in Appendix H.

## Acknowledgments and Disclosure of Funding

This research is supported by the National Natural Science Foundation of China (9227010114, U19A2079, 62302321) and the University Synergy Innovation Program of Anhui Province (GXXT-2022-040). This work is also sponsored by Ant Group through CCF-Ant Research Fund and CCF-AFSG Research Fund.

## References

* [1] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: A survey. _arXiv preprint arXiv:2108.13624_, 2021.
* [2] Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. Good: A graph out-of-distribution benchmark. _arXiv preprint arXiv:2206.08452_, 2022.
* [3] Qitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. Handling distribution shifts on graphs: An invariance perspective. In _ICLR_, 2022.
* [4] Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Out-of-distribution generalization on graphs: A survey. _arXiv preprint arXiv:2202.07987_, 2022.
* [5] Yongduo Sui, Xiang Wang, Jiancan Wu, Min Lin, Xiangnan He, and Tat-Seng Chua. Causal attention for interpretable and generalizable graph classification. In _KDD_, 2022.

* [6] Chenxiao Yang, Qitian Wu, Jiahua Wang, and Junchi Yan. Graph neural networks are inherently good generalizers: Insights by bridging GNNs and MLPs. In _ICLR_, 2023.
* [7] Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. G-mixup: Graph data augmentation for graph classification. In _ICML_, pages 8230-8248, 2022.
* [8] Nianzu Yang, Kaipeng Zeng, Qitian Wu, Xiaosong Jia, and Junchi Yan. Learning substructure invariance for out-of-distribution molecular representations. In _NeurIPS_, 2022.
* [9] Chenxiao Yang, Qitian Wu, Qingsong Wen, Zhiqiang Zhou, Liang Sun, and Junchi Yan. Towards out-of-distribution sequential event prediction: A causal treatment. In _NeurIPS_, 2022.
* [10] An Zhang, Jingnan Zheng, Xiang Wang, Yancheng Yuan, and Tat-Seng Chua. Invariant collaborative filtering to popularity distribution shift. In _WWW_, pages 1240-1251, 2023.
* [11] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yongdong Zhang. Alleviating structural distribution shift in graph anomaly detection. In _WSDM_, pages 357-365, 2023.
* [12] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu. Ood-bench: Quantifying and understanding two dimensions of out-of-distribution generalization. In _CVPR_, pages 7947-7958, 2022.
* [13] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre Alvise-Rebuffi, Ira Ktena, Taylan Cemgil, et al. A fine-grained analysis on distribution shift. In _ICLR_, 2022.
* [14] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint arXiv:1907.02893_, 2019.
* [15] Shiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. Invariant rationalization. In _ICML_, pages 1448-1458, 2020.
* [16] Xingxuan Zhang, Peng Cui, Renzhe Xu, Linjun Zhou, Yue He, and Zheyan Shen. Deep stable learning for out-of-distribution generalization. In _CVPR_, pages 5372-5382, 2021.
* [17] Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. In _ICLR_, 2022.
* [18] Gang Liu, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Graph rationalization with environment-based augmentations. In _KDD_, pages 1069-1078, 2022.
* [19] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical science_, 9(2):513-530, 2018.
* [20] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In _NeurIPS_, 2020.
* [21] Shaohua Fan, Xiao Wang, Yanhu Mo, Chuan Shi, and Jian Tang. Debiasing graph neural networks via learning disentangled causal substructure. In _NeurIPS_, 2022.
* [22] Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. Data augmentation for deep graph learning: A survey. _arXiv preprint arXiv:2202.08235_, 2022.
* [23] Tong Zhao, Gang Liu, Stephan Gunnemann, and Meng Jiang. Graph data augmentation for graph machine learning: A survey. _arXiv preprint arXiv:2202.08871_, 2022.
* [24] Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, and Tom Goldstein. Robust optimization as data augmentation for large-scale graphs. In _CVPR_, pages 60-69, 2022.
* [25] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In _ICLR_, 2020.

* [26] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph classification. In _WWW_, pages 3663-3674, 2021.
* [27] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. In _NeurIPS_, 2020.
* [28] Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve graph contrastive learning. In _NeurIPS_, pages 15920-15933, 2021.
* [29] Haoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Learning invariant graph representations for out-of-distribution generalization. In _NeurIPS_, 2022.
* [30] Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. _arXiv preprint arXiv:1908.05659_, 2019.
* [31] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. In _ICLR_, 2020.
* [32] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. In _ICLR_, 2018.
* [33] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In _ICML_, pages 214-223, 2017.
* [34] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. _NeurIPS_, 31, 2018.
* [35] Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep networks. _NIPS_, 2016.
* [36] Jaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. _NeurIPS_, 31, 2018.
* [37] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In _ICML_, pages 5815-5826, 2021.
* [38] Siqi Miao, Miaoyuan Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. In _ICML_, pages 15524-15543, 2022.
* [39] Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Ood-gnn: Out-of-distribution generalized graph neural network. _IEEE TKDE_, 2022.
* [40] Shaohua Fan, Xiao Wang, Chuan Shi, Peng Cui, and Bai Wang. Generalizing graph neural networks on out-of-distribution graphs. _arXiv preprint arXiv:2111.10657_, 2021.
* [41] Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, Kaili Ma, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-of-distribution generalization on graphs. In _NeurIPS_, 2022.
* [42] Raphael Gontijo-Lopes, Sylvia Smullin, Ekin Dogus Cubuk, and Ethan Dyer. Tradeoffs in data augmentation: An empirical study. In _ICLR_, 2021.
* [43] Jaemin Yoo, Sooyeon Shim, and U Kang. Model-agnostic augmentation for accurate graph classification. In _WWW_, pages 1281-1291, 2022.
* [44] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In _NeurIPSW_, 2021.
* [45] Chaochao Lu, Yuhuai Wu, Jose Miguel Hernandez-Lobato, and Bernhard Scholkopf. Invariant causal representation learning for out-of-distribution generalization. In _ICLR_, 2021.

* [46] Qi Zhu, Natalia Ponomareva, Jiawei Han, and Bryan Perozzi. Shift-robust gnns: Overcoming the limitations of localized graph training data. _NeurIPS_, 34:27965-27977, 2021.
* [47] Wei Jin, Tong Zhao, Jiayuan Ding, Yozen Liu, Jiliang Tang, and Neil Shah. Empowering graph representation learning with test-time graph transformation. In _ICLR_, 2023.
* [48] Junchi Yu, Jian Liang, and Ran He. Mind the label shift of augmentation-based graph ood generalization. In _CVPR_, 2023.
* [49] Zenan Li, Qitian Wu, Fan Nie, and Junchi Yan. Graphde: A generative framework for debiased learning and out-of-distribution detection on graphs. In _NeurIPS_, 2022.
* [50] Qitian Wu, Yiting Chen, Chenxiao Yang, and Junchi Yan. Energy-based out-of-distribution detection for graph neural networks. In _ICLR_, 2023.
* [51] Yuxin Guo, Cheng Yang, Yuluo Chen, Jixi Liu, Chuan Shi, and Junping Du. A data-centric framework to endow graph neural networks with out-of-distribution detection ability. In _KDD_, 2023.
* [52] Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert Muller. Covariate shift adaptation by importance weighted cross validation. _Journal of Machine Learning Research_, 8(5), 2007.
* [53] Zheyan Shen, Peng Cui, Kun Kuang, Bo Li, and Peixuan Chen. Causally regularized learning with agnostic data selection bias. In _ACM MM_, pages 411-419, 2018.
* [54] Renzhe Xu, Xingxuan Zhang, Zheyan Shen, Tong Zhang, and Peng Cui. A theoretical analysis on independence-driven importance weighting for covariate-shift generalization. In _ICML_, pages 24803-24829. PMLR, 2022.
* [55] Tongtong Fang, Nan Lu, Gang Niu, and Masashi Sugiyama. Rethinking importance weighting for deep learning under distribution shift. _NeurIPS_, 33:11996-12007, 2020.
* [56] Yue He, Xinwei Shen, Renzhe Xu, Tong Zhang, Yong Jiang, Wenchao Zou, and Peng Cui. Covariate-shift generalization via random sample weighting. _AAAI_, 2023.
* [57] Emanuel Parzen. On estimation of a probability density function and mode. _The annals of mathematical statistics_, 33(3):1065-1076, 1962.
* [58] Kurt Binder, Dieter Heermann, Lyle Roelofs, A John Mallinckrodt, and Susan McKay. Monte carlo simulation in statistical physics. _Computers in Physics_, 7(2):156-157, 1993.
* [59] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In _CVPR_, pages 5115-5124, 2017.
* [60] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _ICLR_, 2019.
* [61] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _ICLR_, 2017.
* [62] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In _ICML_, pages 1725-1735. PMLR, 2020.
* [63] Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based graph neural network for semi-supervised learning. _arXiv preprint arXiv:1803.03735_, 2018.
* [64] Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In _ICMLW_, 2020.
* [65] Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, and Zheyan Shen. Heterogeneous risk minimization. In _ICML_, pages 6804-6814. PMLR, 2021.

* [66] Masanori Koyama and Shoichiro Yamaguchi. When is invariance useful in an out-of-distribution generalization problem? _arXiv preprint arXiv:2008.01883_, 2020.
* [67] Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk minimization games. In _ICML_, pages 145-155. PMLR, 2020.
* [68] Haohan Wang, Zeyi Huang, Xindi Wu, and Eric Xing. Toward learning robust and invariant representations with alignment regularization and data augmentation. In _KDD_, pages 1846-1856, 2022.
* [69] Pritish Kamath, Akilesh Tangella, Danica Sutherland, and Nathan Srebro. Does invariant risk minimization capture invariance? In _International Conference on Artificial Intelligence and Statistics_, pages 4069-4077. PMLR, 2021.
* [70] Mateo Rojas-Carulla, Bernhard Scholkopf, Richard Turner, and Jonas Peters. Invariant models for causal transfer learning. _The Journal of Machine Learning Research_, 19(1):1309-1342, 2018.
* [71] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. _Nature Machine Intelligence_, 2(11):665-673, 2020.
* [72] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. _IEEE TKDE_, 2022.
* [73] Jinheon Baek, Dong Bok Lee, and Sung Ju Hwang. Learning to extrapolate knowledge: Transductive few-shot out-of-graph link prediction. _NeurIPS_, 33:546-560, 2020.
* [74] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In _ICML_, pages 7313-7324. PMLR, 2021.
* [75] Massimiliano Mancini, Zeynep Akata, Elisa Ricci, and Barbara Caputo. Towards recognizing unseen categories in unseen domains. In _ECCV_, pages 466-483. Springer, 2020.
* [76] Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant feature representation. In _ICML_, pages 10-18. PMLR, 2013.
* [77] Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. Graph domain adaptation via theory-grounded spectral regularization. In _ICLR_, 2023.
* [78] Davide Buffelli, Pietro Lio, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In _NeurIPS_, 2022.
* [79] Gilad Yehudai, Ethan Fetaya, Eli Meirom, Gal Chechik, and Haggai Maron. From local structures to size generalization in graph neural networks. In _ICML_, pages 11975-11986. PMLR, 2021.
* [80] Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Zhou Qin, and Wenwu Zhu. Dynamic graph neural networks under spatio-temporal distribution shift. In _NeurIPS_, 2022.
* [81] Junfeng Fang, Xiang Wang, An Zhang, Zemin Liu, Xiangnan He, and Tat-Seng Chua. Cooperative explanations of graph neural networks. In _WSDM_, pages 616-624, 2023.
* [82] Junfeng Fang, Wei Liu, An Zhang, Xiang Wang, Xiangnan He, Kun Wang, and Tat-Seng Chua. On regularization for explaining graph neural networks: An information theory perspective. 2022.
* [83] Yongduo Sui, Xiang Wang, Tianlong Chen, Meng Wang, Xiangnan He, and Tat-Seng Chua. Inductive lottery ticket learning for graph neural networks. _Journal of Computer Science and Technology_, 2023.
* [84] Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. A unified lottery ticket hypothesis for graph neural networks. In _ICML_, pages 1695-1706. PMLR, 2021.

* [85] Yanfang Wang, Yongduo Sui, Xiang Wang, Zhenguang Liu, and Xiangnan He. Exploring lottery ticket hypothesis in media recommender systems. _International Journal of Intelligent Systems_, 37(5):3006-3024, 2022.
* [86] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yongdong Zhang. Addressing heterophily in graph anomaly detection: A perspective of graph spectrum. In _WWW_, pages 1528-1538, 2023.
* [87] Yuan Gao, Xiang Wang, Xiangnan He, Huamin Feng, and Yong-Dong Zhang. Rumor detection with self-supervised learning on texts and social graph. _Frontiers Comput. Sci._, 17(4):174611, 2023.
* [88] Yanqiao Zhu, Yuanqi Du, Yinkai Wang, Yichen Xu, Jieyu Zhang, Qiang Liu, and Shu Wu. A survey on deep graph generation: Methods and applications. _arXiv preprint arXiv:2203.06714_, 2022.

Correlation Shift and Covariate Shift

Following graph data generation process [17; 8; 3; 29], we can observe that environmental features easily change outside the training distribution, owing to their unstable nature. Hence, distribution shifts are mainly caused by the environmental features [12; 13; 2]. Specifically, we define the joint distribution of training and test data as \(P_{\mathrm{tr}}(G,Y)\) and \(P_{\mathrm{te}}(G,Y)\), respectively. Since their joint distribution can be rewritten as \(P_{\mathrm{tr}}(G,Y)=P_{\mathrm{tr}}(Y|G)P_{\mathrm{tr}}(G)\) and \(P_{\mathrm{te}}(G,Y)=P_{\mathrm{te}}(Y|G)P_{\mathrm{te}}(G)\), we can find that there exist two main reasons that lead to the distribution shift \(P_{\mathrm{tr}}(G,Y)\neq P_{\mathrm{te}}(G,Y)\). We give intuitive examples in Figure 5 and formal definitions of these two distribution shifts.

* **Correlation shift \(P_{\mathrm{tr}}(Y|G)\neq P_{\mathrm{te}}(Y|G),P_{\mathrm{tr}}(G)=P_{\mathrm{te} }(G)\).** If the statistical correlation of environmental features and labels is inconsistent in training and test data, a well-fitted model in training data may fail in test data, which is also known as spurious correlation [14], correlation shift [12] or concept shift [2]. Formally, correlation shift describes the conditional distribution \(P_{\mathrm{tr}}(Y|G)\neq P_{\mathrm{te}}(Y|G)\).
* **Covariate shift \(P_{\mathrm{tr}}(G)\neq P_{\mathrm{te}}(G),P_{\mathrm{tr}}(Y|G)=P_{\mathrm{te} }(Y|G)\).** If there exist environmental features in the test distribution that the model has not seen during training, it will also result in a large performance drop. This unseen distribution shift is well known as covariate shift [2] or diversity shift [12]. It means that the environmental features in test data are unseen in training data, which leads to \(P_{\mathrm{tr}}(G)\neq P_{\mathrm{te}}(G)\). In Definition 2.1, we also quantitatively measure the covariate shift between \(P_{\mathrm{tr}}(G)\) and \(P_{\mathrm{te}}(G)\).

It is worth noting that within the computer vision domain, the general covariate shift is frequently synonymous with sample selection bias [52; 53; 54; 55; 56]. Various factors contribute to covariate shift, such as heterogeneous category distribution or domain-specific variances. In the context of our investigation into graph-based models, we adhere to the assumptions outlined in previous literature [2; 12], which mainly ignore the influence of label shifts. And we posit that covariate shifts are principally attributed to the environmental features. The exploration of more comprehensive scenarios involving covariate shifts will be undertaken in our future work.

## Appendix B Proofs

In this section, we provide the detailed proofs to our proposition. We start with the definition of the 1-Wasserstein distance, \(D_{W}(P,P^{\prime})\), between two distributions \(P\) and \(P^{\prime}\):

\[D_{W}(P,P^{\prime})=\inf_{\pi\in\Gamma(P,P^{\prime})}\int_{\mathbb{R}^{d} \times\mathbb{R}^{d}}\|\bm{x}-\bm{y}\|\cdot d\pi(\bm{x},\bm{y}),\]

where \(\Gamma(P,P^{\prime})\) represents the set of all joint distributions \(\pi(\bm{x},\bm{y})\) that have \(P\) and \(P^{\prime}\) as their respective marginals. Under our conditions, due to \(P^{\prime}(\bm{x}-\bm{x}^{\prime})=P(\bm{x})\), we can precisely determine the manner in which the mass from \(P\) was transferred to create \(P^{\prime}\). This process involves moving each point \(\bm{x}\) under \(P\) to \(\bm{x}+\bm{x}^{\prime}\) under \(P^{\prime}\). Therefore, the infimum is attained by the coupling that deterministically transitions each point \(\bm{x}\) to \(\bm{x}+\bm{x}^{\prime}\). Consequently, the Wasserstein distance simplifies to:

\[D_{W}(P,P^{\prime})=\int_{\mathbb{R}^{d}}\|\bm{x}-(\bm{x}+\bm{x}^{\prime})\| \cdot dP(\bm{x})=\int_{\mathbb{R}^{d}}\|\bm{x}^{\prime}\|\cdot dP(\bm{x})=\| \bm{x}^{\prime}\|.\]

Figure 5: Intuitive examples of correlation shift and covariate shift

This result establishes that the Wasserstein distance between the distributions \(P\) and \(P^{\prime}\) is equivalent to the magnitude of \(\bm{x}^{\prime}\). Therefore, we obtain:

\[D_{W}(P,\widetilde{P}_{1})=\int_{\Omega}\|\bm{x}-(\bm{x}+\bm{x}_{1})\|\cdot d \widetilde{P}_{1}(\bm{x})=\int_{\Omega}\|\bm{x}_{1}\|\cdot dP_{1}(\bm{x})=\|\bm {x}_{1}\|.\]

Similarly, \(D_{W}(P,\widetilde{P}_{2})=\|\bm{x}_{2}\|\). Consequently, if \(D_{W}(P,\widetilde{P}_{1})<D_{W}(P,\widetilde{P}_{2})\), we can deduce that \(\|\bm{x}_{1}\|<\|\bm{x}_{2}\|\).

Next, we examine the GCS measure. The covariate shift between \(P\) and \(\widetilde{P}_{1}\) is given by:

\[\mathrm{GCS}(P,\widetilde{P}_{1})=\frac{1}{2}\int_{\mathcal{S}_{1}}|P(\bm{x}) -P(\bm{x}+\bm{x}_{1})|\cdot d\bm{x}.\]

Similarly, we have:

\[\mathrm{GCS}(P,\widetilde{P}_{2})=\frac{1}{2}\int_{\mathcal{S}_{2}}|P(\bm{x}) -P(\bm{x}+\bm{x}_{2})|\cdot d\bm{x},\]

where \(\mathcal{S}_{1}\) and \(\mathcal{S}_{2}\) denote the non-overlapping regions between \(P\) and \(\widetilde{P}_{1}\), and between \(P\) and \(\widetilde{P}_{2}\), respectively. Now we define another \(\widetilde{P}_{1}^{\prime}\) that satisfies \(\widetilde{P}_{1}^{\prime}(\bm{x})=P(\bm{x}+\|\bm{x}_{1}\|\cdot\frac{\bm{x}_{ 2}}{\|\bm{x}_{2}\|})\) and let \(\mathcal{S}_{1}^{\prime}\) denote the non-overlapping regions between \(P\) and \(\widetilde{P}_{1}^{\prime}\). Given the isotropy of \(\widetilde{P}_{1}\), the integral over \(\mathcal{S}_{1}\) is equal to the integral over \(\mathcal{S}_{1}^{\prime}\). Therefore, we can proceed to deduce:

\[\mathrm{GCS}(P,\widetilde{P}_{1}) =\frac{1}{2}\int_{\mathcal{S}_{1}}|P(\bm{x})-P(\bm{x}+\bm{x}_{1})| \cdot d\bm{x}\] \[=\frac{1}{2}\int_{\mathcal{S}_{1}^{\prime}}|P(\bm{x})-P(\bm{x}+ \bm{x}_{1})|\cdot d\bm{x}\] \[=\mathrm{GCS}(P,\widetilde{P}_{1}^{\prime})\]

Let \(\mathcal{S}_{2}^{\prime}=\{\bm{x}+(\|\bm{x}_{2}\|-\|\bm{x}_{1}\|)\cdot\frac{ \bm{x}_{2}}{\|\bm{x}_{2}\|}\mid\bm{x}\in\mathcal{S}_{1}^{\prime}\}\), we can easily know that \(|\mathcal{S}_{1}^{\prime}|=|\mathcal{S}_{2}^{\prime}|\) and \(\mathcal{S}_{2}^{\prime}\subseteq\mathcal{S}_{2}\). Thus we have:

\[\mathrm{GCS}(P,\widetilde{P}_{2})-\mathrm{GCS}(P,\widetilde{P}_{ 1}) =\frac{1}{2}\int_{\mathcal{S}_{2}}|P(\bm{x})-P(\bm{x}+\bm{x}_{1})| \cdot dg-\frac{1}{2}\int_{\mathcal{S}_{1}}|P(\bm{x})-P(\bm{x}+\bm{x}_{1})| \cdot d\bm{x}\] \[=\frac{1}{2}\int_{\mathcal{S}_{2}}|P(\bm{x})-P(\bm{x}+\bm{x}_{2})| \cdot d\bm{x}-\frac{1}{2}\int_{\mathcal{S}_{1}^{\prime}}|P(\bm{x})-P(\bm{x}+ \bm{x}_{1})|\cdot d\bm{x}\] \[=\frac{1}{2}\int_{\mathcal{S}_{2}}|P(\bm{x})-P(\bm{x}+\bm{x}_{2})| \cdot d\bm{x}-\frac{1}{2}\int_{\mathcal{S}_{2}^{\prime}}|P(\bm{x})-P(\bm{x}+ \bm{x}_{2})|\cdot d\bm{x}\] \[=\frac{1}{2}\int_{\mathcal{S}_{2}\backslash\mathcal{S}_{2}^{ \prime}}|P(\bm{x})-P(\bm{x}+\bm{x}_{2})|\cdot d\bm{x}\geq 0.\]

The integral over \(\mathcal{S}_{2}\) could not be less than the integral over \(\mathcal{S}_{1}\), leading to \(\mathrm{GCS}(P,\widetilde{P}_{1})\leq\mathrm{GCS}(P,\widetilde{P}_{2})\). Hence, we complete the proof.

## Appendix C Estimation of Graph Covariate Shift

In this section, we elaborate on the implementation details of estimating the graph covariate shift. Without loss of generality, we start with the example of estimating the graph covariate shift between the training and test distributions. Given the training set and test set \(\mathcal{D}_{\mathrm{tr}}\) and \(\mathcal{D}_{\mathrm{te}}\), they follow probability distribution functions \(P_{\mathrm{tr}}\) and \(P_{\mathrm{te}}\). The process of estimating \(\mathrm{GCS}(P_{\mathrm{tr}},P_{\mathrm{te}})\) is summarized in the following two steps:

* Firstly, it is intractable to directly estimate the distribution in graph space \(\mathbb{G}\). Inspired by [12], we can obtain the graph features and estimate the distribution in feature space \(\mathbb{F}\). Specifically, given a sample, we train a binary GNN classifier \(f\) to distinguish which distribution it comes from, where \(f(\cdot)=\Phi\circ h\), \(h(\cdot):\mathbb{G}\rightarrow\mathbb{F}\) is a graph encoder, and \(\Phi(\cdot):\mathbb{F}\rightarrow\{0,1\}\) is a binary classifier. Then we can adopt the pre-trained GNN encoder \(h\) to extract graph features.
* Secondly, we prepare the features and estimate the distribution of the data via Kernel Density Estimation (KDE) [57]. Finally, we adopt the Monte Carlo Integration under importance sampling [58] to approximate the integrals in Definition 2.1.

We summarize these implementations in Algorithm 1. In lines 4 and 5, to avoid the label shift [12], we adopt sample reweighting to ensure the balance of each class.

```
0: Training dataset \(\mathcal{D}_{\mathrm{tr}}\) and test dataset \(\mathcal{D}_{\mathrm{te}}\); Batch size \(N\); Loss function \(\ell\); GNN \(f=\Phi\circ h\); Importance sampling size \(M\); Threshold \(\epsilon\).
0: Estimated covariate shift \(\mathrm{GCS}(P_{\mathrm{tr}},P_{\mathrm{te}})\).
1: Initialize parameters of \(f\)
2:# Train a graph classifier
3:while not converge do
4: Sample a batch \(\mathcal{B}_{\mathrm{tr}}\leftarrow\{(g_{i},y_{i})\}_{i=1}^{N}\subset\mathcal{D }_{\mathrm{tr}}\) and relabel all \(y_{i}\gets 0\)
5: Sample a batch \(\mathcal{B}_{\mathrm{te}}\leftarrow\{(g_{i},y_{i})\}_{i=1}^{N}\subset\mathcal{D }_{\mathrm{te}}\) and relabel all \(y_{i}\gets 1\)
6:\(\mathcal{B}\leftarrow\mathcal{B}_{\mathrm{tr}}\cup\mathcal{B}_{\mathrm{te}}\)
7:for each \((g_{i},y_{i})\in\mathcal{B}\)do
8: Compute loss \(\ell(f(g_{i}),y_{i})\) and back-propagate gradients
9:endfor
10: Update the parameters of \(f\) via gradient descent and reset the gradients
11:endwhile
12:# Prepare the features for the estimation
13: Extract training and test feature sets \(\mathcal{F}_{\mathrm{tr}}\) and \(\mathcal{F}_{\mathrm{te}}\) via encoder \(h\)
14:\(\mathcal{F}\leftarrow\mathcal{F}_{\mathrm{tr}}\cup\mathcal{F}_{\mathrm{te}}\)
15:Scale \(\mathcal{F}\) to zero mean and unit variance
16:\(\hat{\omega}\leftarrow\) fit by KDE the distribution of \(\mathcal{F}\)
17:Split \(\mathcal{F}\) to recover the original partition \(\mathcal{F}_{\mathrm{tr}}^{\prime},\mathcal{F}_{\mathrm{te}}^{\prime}\)
18:\(\mathcal{P}_{\mathrm{tr}},\hat{P}_{\mathrm{te}}\leftarrow\) fit by KDE the distributions of \(\mathcal{F}_{\mathrm{tr}}^{\prime},\mathcal{F}_{\mathrm{te}}^{\prime}\)
19:# Estimate the covariate shift
20: Initialize \(\mathrm{GCS}(P_{\mathrm{tr}},P_{\mathrm{te}})\gets 0\)
21:for\(t\leftarrow\{1,...,M\}\)do
22:\(z\leftarrow\) sample from \(\hat{\omega}\)
23:if\(\hat{P}_{\mathrm{tr}}(z)<\epsilon\) or \(\hat{P}_{\mathrm{te}}(z)<\epsilon\)then
24:\(\mathrm{GCS}(P_{\mathrm{tr}},P_{\mathrm{te}})\leftarrow\mathrm{GCS}(P_{ \mathrm{tr}},P_{\mathrm{te}})+|\hat{P_{\mathrm{tr}}}(z)-\hat{P_{\mathrm{te}}}( z)|/\hat{\omega}(z)\)
25:endif
26:endfor
27:\(\mathrm{GCS}(P_{\mathrm{tr}},P_{\mathrm{te}})\leftarrow\mathrm{GCS}(P_{ \mathrm{tr}},P_{\mathrm{te}})/2M\) ```

**Algorithm 1**Estimation of Graph Covariate shift

```
0: Training set \(\mathcal{D}_{\mathrm{tr}}\); Adversarial augmenter \(T_{\theta_{1}}(\cdot)\); Stable feature generator \(T_{\theta_{2}}(\cdot)\); GNN classifier \(f(\cdot)\) with parameters \(\theta\); Learning rates \(\alpha,\beta\); Batch size \(N\); Stable feature ratio \(\lambda_{s}\); Penalty \(\gamma\).
1: Randomly initilize \(\theta\), \(\theta_{1}\), \(\theta_{2}\)
2:while not converge do
3: Sample a batch \(\mathcal{B}_{\mathrm{tr}}\leftarrow\{(g_{i},y_{i})\}_{i=1}^{N}\subset\mathcal{D }_{\mathrm{tr}}\)
4:for each \((g_{i},y_{i})\in\mathcal{B}_{\mathrm{tr}}\)do
5:\(\mathbf{M}_{\mathrm{adv}}^{a},\mathbf{M}_{\mathrm{adv}}^{x}\gets T_{ \theta_{1}}(g_{i})\)// adversarial perturbations
6:\(\mathbf{M}_{\mathrm{sta}}^{a},\mathbf{M}_{\mathrm{sta}}^{x}\gets T_{ \theta_{2}}(g_{i})\)// regions of stable features
7:\(\mathbf{\widetilde{M}}^{x}\leftarrow(\mathbf{1}-\mathbf{M}_{\mathrm{sta}}^{a}) \otimes\mathbf{M}_{\mathrm{adv}}^{a}\) + \(\mathbf{M}_{\mathrm{sta}}^{a}\)// augment edges
8:\(\mathbf{\widetilde{M}}^{x}\leftarrow(\mathbf{1}-\mathbf{M}_{\mathrm{sta}}^{x}) \otimes\mathbf{M}_{\mathrm{adv}}^{x}+\mathbf{M}_{\mathrm{sta}}^{x}\)// augment nodes
9:\(\widetilde{g_{i}}\leftarrow(\mathbf{A}_{i}\otimes\mathbf{\widetilde{M}}^{x}, \mathbf{X}_{i}\odot\mathbf{\widetilde{M}}^{x})\)// augmented graph
10:endfor
11: Compute \(\mathcal{L}_{\mathrm{adv}}-\mathcal{L}_{\mathrm{reg}_{1}}\) via Equation (7) and (9)
12: Compute \(\mathcal{L}_{\mathrm{sta}}+\mathcal{L}_{\mathrm{reg}_{2}}\) via Equation (8) and (10)
13: Update parameters of adversarial augmenter via gradient ascent: \(\theta_{1}\leftarrow\theta_{1}+\alpha\nabla_{\theta_{1}}(\mathcal{L}_{\mathrm{adv }}-\mathcal{L}_{\mathrm{reg}_{1}})\)
14: Update parameters of GNN and stable feature generator via gradient descent: \(\theta\leftarrow\theta-\beta\nabla_{\theta}(\mathcal{L}_{\mathrm{sta}}+\mathcal{L }_{\mathrm{reg}_{2}})\); \(\theta_{2}\leftarrow\theta_{2}-\beta\nabla_{\theta_{2}}(\mathcal{L}_{\mathrm{sta }}+\mathcal{L}_{\mathrm{reg}_{2}})\)
15:endwhile ```

**Algorithm 2**Adversarial Invariant Augmentation

```
0: Training set \(\mathcal{D}_{\mathrm{tr}}\); Adversarial augmenter \(T_{\theta_{1}}(\cdot)\); Stable feature generator \(T_{\theta_{2}}(\cdot)\); GNN classifier \(f(\cdot)\) with parameters \(\theta\); Learning rates \(\alpha,\beta\); Batch size \(N\); Stable feature ratio \(\lambda_{s}\); Penalty \(\gamma\).
1: Randomly initilize \(\theta\), \(\theta_{1}\), \(\theta_{2}\)
2:while not converge do
3: Sample a batch \(\mathcal{B}_{\mathrm{tr}}\leftarrow\{(g_{i},y_{i})\}_{i=1}^{N}\subset\mathcal{D }_{\mathrm{tr}}\)
4:for each \((g_{i},y_{i})\in\mathcal{B}_{\mathrm{tr}}\)do
5:\(\mathbf{M}_{\mathrm{adv}}^{a},\mathbf{M}_{\mathrm{adv}}^{x}\gets T_{ \theta_{1}}(g_{i})\)// adversarial perturbations
6:\(\mathbf{M}_{\mathrm{sta}}^{a},\mathbf{M}_{\mathrm{sta}}^{x}\gets T_{ \theta_{2}}(g_{i})\)// regions of stable features
7:\(\mathbf{\widetilde{M}}^{x}\leftarrow(\mathbf{1}-\mathbf{M}_{\mathrm{sta}}^{a}) \otimes\mathbf{M}_{\mathrm{adv}}^{a}\) + \(\mathbf{M}_{\mathrm{sta}}^{a}\)// augment edges
8:\(\mathbf{\widetilde{M}}^{x}\leftarrow(\mathbf{1}-\mathbf{M}_{\mathrm{sta}}^{a}) \otimes\mathbf{M}_{\mathrm{adv}}^{x}+\mathbf{M}_{\mathrm{sta}}^{a}\)// augment nodes
9:\(\widetilde{g_{i}}\leftarrow(\mathbf{A}_{i}\odot\mathbf{\widetilde{M}}^{x}, \mathbf{X}_{i}\odot\mathbf{\widetilde{M}}^{x})\)// augmented graph
10:endfor
11: Compute \(\mathcal{L}_{\mathrm{adv}}-\mathcal{L}_{\mathrm{reg}_{1}}\) via Equation (7) and (9)
12: Compute \(\mathcal{L}_{\mathrm{sta}}+\mathcal{L}_{\mathrm{reg}_{2}}\) via Equation (8) and (10)
13: Update parameters of adversarial augmenter via gradient ascent: \(\theta_{1}\leftarrow\theta_{1}+\alpha\nabla_{\theta_{1}}(\mathcal{L}_{\mathrm{adv }}-\mathcal{L}_{\mathrm{reg}_{1}})\)
14: Update parameters of GNN and stable feature generator via gradient descent: \(\theta\leftarrow\theta-\beta\nabla_{\theta}(\mathcal{L}_{\mathrm{sta}}+\mathcal{L}_{ \mathrm{reg}_{2}})\); \(\theta_{2}\leftarrow\theta_{2}-\beta\nabla_{\theta_{2}}(\mathcal{L}_{\mathrm{sta }}+\mathcal{L}_{\mathrm{reg}_{2}})\)
15:endwhile ```

**Algorithm 3**Estimation of Graph Covariate shift

```
0: Training set \(\mathcal{D}_{\mathrm{tr}}\); Adversarial augmenter \(T_{\theta_{1}}(\cdot)\); Stable feature generator \(T_{\theta_{2}}(\cdot)\); GNN classifier \(f(\cdot)\) with parameters \(\theta\); Learning rates \(\alpha,\beta\); Batch size \(N\); Stable feature ratio \(\lambda_{s}\); Penalty \(\gamma\).
1: Randomly initilize \(\theta\), \(\theta_{1}\), \(\theta_{2}\)
2:while not converge do
3: Sample a batch \(\mathcal{B}_{\mathrm{tr}}\leftarrow\{(g_{i},y_{i})\}_{i=1}^{N}\subset\mathcal{D }_{\mathrm{tr}}\)
4:for each \((g_{i},y_{i})\in\mathcal{B}_{\mathrm{tr}}\)do
5:\(\mathbf{M}_{\mathrm{adv}}^{a}

## Appendix D Implementation Details

### Algorithm

We summarize the detailed implementations of AIA in Algorithm 2. We alternately optimize the adversarial augmenter and stable feature generator with the backbone model, in lines 13 and 14. We adopt the learned stable features for predictions in the inference stage.

### Datasets

In this paper, we conduct experiments on graph OOD datasets [2] and OGB datasets [20], which include Motif, CMNIST, Molbbbp and Molhiv. We follow [2] to create various covariate shifts, according to base, color, size and scaffold splitting. Base, color, size and scaffold are features of the graph data and do not determine the labels of the data, so they can be regarded as environmental features. The statistics of the datasets are summarized in Table 4. Below we give a brief introduction to each dataset.

* **Motif:** It is a synthetic dataset from Spurious-Motif [17, 5]. As shown in original graphs in Figure 5, each graph is composed of a base-graph (_wheel, tree, ladder, star, path_) and a motif (_house, cycle, crane_). The label is only determined by the type of motif. We create covariate shift according to the base-graph type and the graph size (_i.e.,_ node number). For base covariate shift, we adopt graphs with _wheel, tree, ladder_ base-graphs for training, _star_ for validation and _path_ for testing. For size covariate shift, we use small-size of graphs for training, while the validation and the test sets include the middle- and the large-size graphs, respectively.
* **CMNIST:** Color MNIST dataset contains graphs transformed from MNIST via superpixel techniques [59]. We define color as the environmental features to create the covariate shift. Specifically, we color digits with 7 different colors, where five of them are adopted for training while the remaining two are used for validation and testing.
* **Molbbbp & Molhiv:** These are molecular datasets collected from MoleculeNet [19]. We define the scaffold and graph size (_i.e.,_ node number) as the environmental features to create two types of covariate shifts. For scaffold shift, we follow [2] and use scaffold split to create training, validation

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline Dataset & \multicolumn{2}{c}{Motif} & \multicolumn{2}{c}{CMNIST} & \multicolumn{2}{c}{Molbbbp} & \multicolumn{2}{c}{Molhiv} \\ \cline{2-9} Covariate shift & base & size & color & scaffold & size & scaffold & size \\ \hline \multirow{3}{*}{Train} & Graph\# & 18000 & 18000 & 42000 & 1631 & 1633 & 24682 & 26169 \\  & Avg. node\# & 17.07 & 16.93 & 75.00 & 22.49 & 27.02 & 26.25 & 27.87 \\  & Avg. edge\# & 48.89 & 43.57 & 1392.76 & 48.43 & 58.71 & 56.68 & 60.20 \\ \hline \multirow{3}{*}{Val} & Graph\# & 3000 & 3000 & 7000 & 204 & 203 & 4113 & 2773 \\  & Avg. node\# & 15.82 & 39.22 & 75.00 & 33.20 & 12.06 & 24.95 & 15.55 \\  & Avg. edge\# & 33.00 & 107.03 & 1393.73 & 71.84 & 24.27 & 54.53 & 32.77 \\ \hline \multirow{3}{*}{Test} & Graph\# & 3000 & 3000 & 7000 & 204 & 203 & 4108 & 3961 \\  & Avg. node\# & 14.97 & 87.18 & 75.00 & 27.51 & 12.26 & 19.76 & 12.09 \\ \cline{1-1}  & Avg. edge\# & 31.54 & 239.65 & 1393.60 & 59.75 & 24.87 & 40.58 & 24.87 \\ \hline \multicolumn{9}{c}{Class\#} & 3 & 3 & 10 & 2 & 2 & 2 & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Statistics of graph classification datasets.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Dataset & \multicolumn{2}{c}{Motif} & \multicolumn{2}{c}{CMNIST} & \multicolumn{2}{c}{Mblobbp} & \multicolumn{2}{c}{Mohiv} \\ \cline{2-9} Covariate shift & base & size & color & scaffold & size & scaffold & size \\ \hline Backbone (layer-hidden) & 4-300 & 4-300 & 4-300 & 4-64 & 4-32 & 4-128 & 4-128 \\ Augmenter (layer-hidden) & 2-300 & 2-300 & 2-300 & 2-64 & 2-32 & 2-128 & 2-128 \\ Generator (layer-hidden) & 2-300 & 2-300 & 2-300 & 2-64 & 2-32 & 2-128 & 2-128 \\ \hline Optimizer & Adam & Adam & Adam & Adam & Adam & Adam & Adam \\ Learning rate \(\alpha\) & 1e-3 & 1e-3 & 1e-3 & 1e-3 & 1e-3 & 5e-3 & 1e-3 & 1e-2 \\ Learning rate \(\beta\) & 5e-3 & 5e-3 & 5e-3 & 1e-3 & 5e-3 & 1e-2 & 1e-2 \\ Stable feature ratio \(\lambda_{s}\) & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.1 & 0.1 \\ Adversarial penalty \(\gamma\) & 0.2 & 0.2 & 0.2 & 0.5 & 0.5 & 0.5 & 0.5 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hyper-parameter details of AIA.

and test sets. For size shift, we adopt the large-size of graphs for training and the smaller ones for validation and testing.

### Metrics

We adopt classification accuracy as the metric for Motif and CMNIST. As suggested by [20], we use ROC-AUC for Molhiv and Molbbbp datasets. In addition, we use \(\mathrm{GCS}(P,Q)\) to measure the covariate shift between distributions \(P\) and \(Q\). For all experimental results, we perform 10 random runs and report the mean and standard derivations. For augmentation diversity, we use conditional entropy to measure the diversity of generated data. We normalize it to \([0,1]\) for better comparison. Specifically, for a given graph data, we compute the conditional entropy by collecting augmented data generated during the training process. We conduct experiments by collecting 1000 graph data, and report the mean and standard deviation.

### Training Settings

We use the NVIDIA GeForce RTX 3090 (24GB GPU) to conduct all our experiments. To make a fair comparison, we adopt GIN [60] as the default architecture to conduct all experiments. We tune the hyper-parameters in the following ranges: \(\alpha\) and \(\beta\in\{0.01,0.005,0.001\}\); \(\lambda_{2}\in\{0.1,...,0.9\}\); \(\gamma\in\{0.01,0.1,0.2,0.5,1.0,1.5,2.0,3.0,5.0\}\). The hyper-parameters are summarized in Table 5.

### Baseline Settings

For a more comprehensive comparison, we selected 16 baselines. In this section, we give a detailed introduction to the settings of these methods.

* For ERM, IRM [14], GroupDRO [31], VREx [37], and M-Mixup [26], we report the results from the study [2] by default and reproduce the missing results on Molbbbp.
* For DIR [17], CAL [5], GSAT [38], DropEdge [25], GREA [18], FLAG [24], \(\mathcal{G}\)-Mixup [7], CIGA [41] and DisC [21], they provide source codes for the implementations. We adopt default settings from their source codes and detailed hyper-parameters from their original papers.
* For OOD-GNN [39] and StableGNN [40], their source codes are not publicly available. We reproduce them based on the codes of StableNet [16].
* For RDIA in Section 5.4, it is a variant that replaces the adversarial augmentation in AIA with random augmentation. In our implementation, we use all-one matrices to create the initial node and edges masks. Then we randomly set 20% of nonzero elements to zero in these masks. Finally, we apply these masks to the graphs for random data augmentation. The process of stable feature learning is consistent with AIA.

## Appendix E More Experimental Results

### Results on Correlation Shift

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline \multirow{2}{*}{Backbone} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{Motif} & \multicolumn{2}{c}{Moblbbp} \\ \cline{3-6}  & & base & size & scaffold & size \\ \hline \multirow{4}{*}{GCN} & EEM & 67.41±4.47 & 50.76±2.92 & 66.44±1.91 & 77.79±4.04 \\  & M-Mixup & 68.83±40 & 51.50±45 & 67.09±0.57 & 78.42±2.71 \\  & AIA & 59.78±56 & 50.99±1.45 & 66.03±2.98 & 78.62±5.24 \\  & AIA (ours) & **72.39±53** & **54.87±52** & **69.13±1.78** & **80.53±43** \\ \hline \multirow{4}{*}{GCNII} & EEM & 67.84±47±14 & 51.74±35 & 67.64±1.84 & 77.96±4±02 \\  & M-Mixup & 67.53±14 & 52.31±58 & 66.64±0.90 & 76.67±2.69 \\  & FLAG & 58.67±58 & 50.18±41 & 66.27±52 & 55.55±25 \\  & AIA (ours) & **72.70±41** & **53.23±65** & **67.93±160** & **79.72±37** \\ \hline \multirow{4}{*}{GAT} & EEM & 66.53±44 & 51.16±32 & 66.51±177 & 77.41±31 \\  & M-Mixup & 69.25±199 & 51.37±523 & 66.95±0.47 & 77.21±28 \\ \cline{1-1}  & FLAG & 59.53±56 & 51.32±448 & 67.36±248 & 77.87±2.31 \\ \cline{1-1}  & AIA (ours) & **71.95±39** & **54.38±53** & **69.21±162** & **78.49±33** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance over diverse backbones.

Although this work focuses on the OOD issue of covariate shift, for completeness, we also evaluate the performance of AIA under correlation shift. Following [2], we choose three graph OOD datasets (_i.e.,_ Motif, CMNIST, Moliniv) with three different graph features (_i.e.,_ base, color, size) to create correlation shifts. For baselines, we choose three generalization algorithms (_i.e.,_ ERM, IRM [14], VREx [37]), three graph generalization methods (_i.e.,_ DIR [17], CAL [5], OOD-GNN [39]) and three data augmentation methods (_i.e.,_ DropEdge [25], FLAG [24], M-Mixup [26]). The experimental results are shown in Table 7. We can observe that AIA can also effectively alleviate the correlation shift. These results demonstrate that AIA learns better stable features by encouraging environmental discrepancy, which can effectively break spurious correlations that are hidden in the training data.

### Results on Diverse Backbones

We select three different GNN backbone models (GCN [61], GCNI [62] and GAT [63]) for experiments. From the results in Table 6, our observations and conclusions remain the same with the diverse backbones.

### Results on More Real-world Datasets

To demonstrate the effectiveness of the proposed AIA, we also conduct experiments on commonly used TU datasets [64], which include MUTAG, NCI1, PROTEINS, COLLAB, IMDB-B, IMDB-M. For training settings, we follow CAL [5] and adopt GIN [60] as our backbone model. The experimental results are shown in Table 8. For the results, we can observe that our method can achieve the best performance over different datasets.

### More Visualizations

To demonstrate the superiority of our method, we also visualize the captured stable features by AIA and compare them with other baselines. The results are displayed in Figure 6. From the results, we can easily observe that our method can find stable parts more accurately than other baseline methods.

## Appendix F Complexity Analyses

Firstly, we define the average numbers of nodes and edges per graph in the dataset to be \(n\) and \(m\), respectively. Let \(N\) denote the batch size, \(l\), \(l_{a}\) and \(l_{c}\) denote the numbers of layers in the GNN backbone, adversarial augmenter and stable feature generator, respectively. \(d\), \(d_{a}\) and \(d_{c}\) are the

\begin{table}
\begin{tabular}{l c c c} \hline \hline Method & Motif & CMNIST & Moliniv \\ \hline ERM & 81.44\({}_{2.54}\) & 42.87\({}_{41.37}\) & 63.26\({}_{41.25}\) \\ IRM & 80.71\({}_{2.81}\) & 42.80\({}_{41.62}\) & 59.90\({}_{41.17}\) \\ VREx & 81.56\({}_{42.14}\) & 43.31\({}_{41.03}\) & 60.23\({}_{41.60}\) \\ \hline DIR & 73.25\({}_{46.37}\) & 38.78\({}_{41.45}\) & 66.78\({}_{41.50}\) \\ CAL & 81.94\({}_{41.20}\) & 41.82\({}_{0.85}\) & 62.36\({}_{41.42}\) \\ OOD-GNN & 80.22\({}_{2.28}\) & 39.03\({}_{41.24}\) & 57.49\({}_{41.08}\) \\ DropEdge & 78.97\({}_{41.38}\) & 38.43\({}_{41.34}\) & 59.42\({}_{41.73}\) \\ FLAG & 80.91\({}_{41.04}\) & 43.41\({}_{41.38}\) & 66.44\({}_{42.32}\) \\ M-Mixup & 77.63\({}_{41.12}\) & 40.96\({}_{41.21}\) & 64.87\({}_{41.36}\) \\ AIA (ours) & **82.51\({}_{42.81}\)** & **49.73\({}_{41.70}\)** & **68.11\({}_{41.82}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Performance on correlation shift.

Figure 6: Visualizations of captured stable features.

[MISSING_PAGE_FAIL:22]

separate them from data. Thirdly, we also design a metric, \(\mathrm{GCS}(\widetilde{P},P)\), which can effectively measure the discrepancy of the environmental features for our augmented data. And we directly encourage the environmental discrepancy of the augmented samples by maximizing \(\mathrm{GCS}(\widetilde{P},P)\). However, EERM does not provide any evaluation metric for environmental discrepancy. To encourage the discrepancy, they "blindly" maximize the variance of the empirical risk in \(K\) environments. Finally, for generalization scope, EERM is based on the IRM [14] by minimizing the empirical risk in \(K\) environments. In contrast, inspired by DRO [31], we can guarantee the generalization within the robust radius \(\rho\). We summarize the above detailed discussions in Table 10.

## Appendix H Limitation & Future Work

Although AIA outperforms numerous baselines and can achieve outstanding performance under various covariate shifts, we also prudently introspect the following limitations of our method. And we leave the improvements of these limitations as our future work.

* AIA performs OOD exploration through an adversarial data augmentation strategy to achieve environmental discrepancy. However, it only perturbs the existing graph data in a given training set, such as perturbing original graph node features or graph structures. Hence, it is possible that there still exist some overlaps between the augmented distribution and training distribution, so discrepancy principle cannot be thoroughly achieved. In future work, we will attempt to design more advanced data augmentation methods, such as graph generation-based strategies [88], to generate more unseen and novel graph data, for pursuing the discrepancy principle.
* For model training, we adopt adversarial training and stable feature learning to alternately optimize the adversarial augmenter, stable feature generator and backbone GNN. This training strategy may make the training process unstable, so the performance of AIA may experience a large variance. In addition, these two networks also involve additional parameters. Optimizing these parameters separately will also increase the time complexity, as shown in Appendix F. Hence, in future work, we will explore how to utilize more advanced optimization methods and lightweight models to achieve the principles of environmental feature discrepancy and stable feature consistency.

\begin{table}
\begin{tabular}{c|c|c|c} \hline  & & EERM & Our AIA \\ \hline \multirow{2}{*}{Scope} & Is it specifically designed & \multirow{2}{*}{✗} & \multirow{2}{*}{✓} \\  & for covariate shift? & & \\ \hline \multirow{2}{*}{Separability} & Can environmental/stable & \multirow{2}{*}{✗} & \multirow{2}{*}{✓} \\  & features be separated? & & \\ \hline \multirow{6}{*}{Environmental} & Can environmental features & \multirow{2}{*}{✗} & \multirow{2}{*}{✓} \\  & be identified explicitly? & & \\ \cline{1-1} \cline{2-4}  & How to model & & \\ \cline{1-1} \cline{2-4} Feature & environmental features? & - & Mask model \(T_{\theta_{1}}(\cdot)\) \\ \cline{1-1} \cline{2-4} Discrepancy & Metric for & - & \(\mathrm{GCS}(\widetilde{P},P)\) \\ \cline{1-1} \cline{2-4}  & Generation principle for & \multirow{2}{*}{“Bindly” maximize \(\mathbb{V}_{\mu}[R(e)]\)} & Maximize \(\mathrm{GCS}(\widetilde{P},P)\) \\ \cline{1-1} \cline{2-4}  & & & \\ \hline \multirow{4}{*}{Stable Feature Consistency} & Can stable features & \multirow{2}{*}{✗} & \multirow{2}{*}{✓} \\  & be identified explicitly? & & \\ \cline{1-1} \cline{2-4}  & How to model & & \\ \cline{1-1} \cline{2-4}  & stable features? & - & Mask model \(T_{\theta_{2}}(\cdot)\) \\ \cline{1-1} \cline{2-4}  & Learning principles for & \multirow{2}{*}{\(\min_{\theta}\mathbb{V}_{\mu}[R(e)]\)} & Sufficiency/Independence \\ \cline{1-1} \cline{2-4}  & stable features & & \\ \hline \multirow{2}{*}{Generalization} & Theoretical basis & IRM & DRO \\ \cline{1-1} \cline{2-4}  & Generalization scope & \multirow{2}{*}{\(K\) environments} & Robust radius \(\rho\) \\ \cline{1-1} \cline{2-4}  & & & \(D(\widetilde{P},P)\leq\rho\) \\ \hline \end{tabular}
\end{table}
Table 10: Comparisons with EERM.