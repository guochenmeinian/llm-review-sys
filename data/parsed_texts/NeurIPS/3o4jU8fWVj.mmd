# EquivformerV2: Improved Equivariant Transformer

for Scaling to Higher-Degree Representations

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Equivariant Transformers such as Equivformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are still limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equivformer, we first replace \(SO(3)\) convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements - attention re-normalization, separable \(S^{2}\) activation and separable layer normalization. Putting this all together, we propose EquivformerV2, which outperforms previous state-of-the-art methods on the large-scale OC20 dataset by up to \(15\%\) on forces, \(5\%\) on energies, offers better speed-accuracy trade-offs, and \(2\times\) reduction in DFT calculations needed for computing adsorption energies.

## 1 Introduction

In recent years, machine learning (ML) models have shown promising results in accelerating and scaling high-accuracy but compute-intensive quantum mechanical calculations by effectively accounting for key features of atomic systems, such as the discrete nature of atoms, and Euclidean and permutation symmetries [1; 2; 3; 4; 5; 6; 7; 8; 9; 10]. By bringing down computational costs from hours or days to fractions of seconds, these methods enable new insights in many applications such as molecular simulations, material design and drug discovery. A promising class of ML models that have enabled this progress is equivariant graph neural networks (GNNs) [11; 12; 13; 14; 15; 16; 17; 18].

Equivariant GNNs treat 3D atomistic systems as graphs, and incorporate inductive biases such that their internal representations and predictions are equivariant to 3D translations, rotations and optionally inversions. Specifically, they build up equivariant features of each node as vector spaces of irreducible representations (or irreps) and have interactions or message passing between nodes based on equivariant operations such as tensor products. Recent works on equivariant Transformers, specifically Equivformer [17], have shown the efficacy of applying Transformers [19; 20], which have previously enjoyed widespread success in computer vision [21; 22; 23], language [24; 25], and graphs [26; 27; 28; 29], to this domain of 3D atomistic systems.

A bottleneck in scaling Equivformer as well as other equivariant GNNs is the computational complexity of tensor products, especially when we increase the maximum degree of irreps \(L_{max}\). This limits these models to use small values of \(L_{max}\) (e.g., \(L_{max}\leqslant\) 3), which consequently limits their performance. Higher degrees can better capture angular resolution and directional information, which is critical to accurate prediction of atomic energies and forces. To this end, eSCN [18] recently proposes efficient convolutions to reduce \(SO(3)\) tensor products to \(SO(2)\) linear operations, bringing down the computational cost from \(\mathcal{O}(L_{max}^{6})\) to \(\mathcal{O}(L_{max}^{3})\) and enabling scaling to larger values of \(L_{max}\) (e.g., \(L_{max}\) up to \(8\)). However, except using efficient convolutions for higher \(L_{max}\), eSCN still follows SEGNN [15]-like message passing network design, and Equiformer has been shown to improve upon SEGNN. Additionally, this ability to use higher \(L_{max}\) challenges whether the previous design of equivariant Transformers can scale well to higher-degree representations.

In this paper, we are interested in adapting eSCN convolutions for higher-degree representations to equivariant Transformers. We start with Equiformer [17] and replace \(SO(3)\) convolutions with eSCN convolutions. We find that naively incorporating eSCN convolutions does not result in better performance than the original eSCN model. Therefore, to better leverage the power of higher degrees, we propose three architectural improvements - attention re-normalization, separable \(S^{2}\) activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which is developed on the large and diverse OC20 dataset [30]. Experiments on OC20 show that EquiformerV2 outperforms previous state-of-the-art methods with improvements of up to \(15\%\) on forces and \(5\%\) on energies, and offers better speed-accuracy trade-offs compared to existing invariant and equivariant GNNs. Additionally, when used in the AdsorbML algorithm [10] for performing adsorption energy calculations, EquiformerV2 achieves the highest success rate and \(2\times\) reduction in DFT calculations to achieve comparable adsorption energy accuracies as previous methods.

## 2 Related Works

_SE(3)/E(3)-Equivariant GNNs._Equivariant neural networks [11, 12, 13, 14, 15, 16, 17, 18, 31, 32, 33, 34, 35, 36, 37, 38]_ use equivariant irreps features built from vector spaces of irreducible representations (irreps) to achieve equivariance to 3D rotation [11, 12, 13]. They operate on irreps features with equivariant operations like tensor products. Previous works differ in equivariant operations used in their networks and how they combine those operations. TFN [11] and NequIP [5] use equivariant graph convolution with linear messages built from tensor products, with the latter utilizing extra equivariant gate activation [12]. SEGNN [15] introduces non-linearity to messages passing [1, 39] with equivariant gate activation, and the non-linear messages improve upon linear messages. SE(3)-Transformer [14] adopts equivariant dot product attention [19] with linear messages. Equiformer [17] improves upon previously mentioned equivariant GNNs by combining MLP attention and non-linear messages. Equiformer additionally introduces equivariant layer normalization and regularizations like dropout [40] and stochastic depth [41]. However, the networks mentioned above rely on compute-intensive \(SO(3)\) tensor products to mix the information of vectors of different degrees during message passing, and therefore they are limited to small values for maximum degrees \(L_{max}\) of equivariant representations. SCN [42] proposes rotating irreps features based on relative position vectors and identifies a subset of spherical harmonics coefficients, on which they can apply unconstrained functions. They further propose relaxing the requirement for strict equivariance and apply typical functions to rotated features during message passing, which trades strict equivariance for computational efficiency and enables using higher values of \(L_{max}\). eSCN [18] further improves upon SCN by replacing typical functions with \(SO(2)\) linear layers for rotated features and imposing strict equivariance during message passing.

Figure 1: Overview of EquiformerV2. We highlight the differences from Equiformer [17] in red. For (b), (c), and (d), the left figure is the original module in Equiformer, and the right figure is the revised module in EquiformerV2. Input 3D graphs are embedded with atom and edge-degree embeddings and processed with Transformer blocks, which consist of equivariant graph attention and feed forward networks. “\(\otimes\)” denotes multiplication, “\(\copy\)” denotes addition, and \(\sum\) within a circle denotes summation over all neighbors. “DTP” denotes depth-wise tensor products used in Equiformer. Gray cells indicate intermediate irreps features.

[MISSING_PAGE_FAIL:3]

tensor products and equivariant linear operations, equivariant layer normalization [52] and gate activation [12; 34]. For stronger expressivity in the attention compared to typical Transformers, Equiformer uses non-linear functions for both attention weights and message passing. Additionally, Equiformer incorporates regularization techniques common in Transformers applied to other domains, e.g., dropout [40] to attention weights [53] and stochastic depth [54] to the outputs of equivariant graph attention and feed forward networks. Please refer to the Equiformer paper [17] for more details.

### eSCN Convolution

While tensor products are necessary to interact vectors of different degrees, they are compute-intensive. To reduce the complexity, eSCN convolutions [18] are proposed to use \(SO(2)\) linear operations for efficient tensor products. We provide an outline and intuition for their method here, and please refer to Sec. A and their work [18] for mathematical details.

A traditional \(SO(3)\) convolution interacts input irreps features \(x_{m_{i}}^{(L_{i})}\) and spherical harmonic projections of relative positions \(Y_{m_{f}}^{(L_{f})}(r_{ij}^{\ast})\) with an \(SO(3)\) tensor product with Clebsch-Gordan coefficients \(C_{(L_{i},m_{i}),(L_{f},m_{f})}^{(L_{o},m_{o})}\). The projection \(Y_{m_{f}}^{(L_{f})}(r_{ij}^{\ast})\) becomes sparse if we rotate the relative position vector \(\vec{r}_{ij}\) with a rotation matrix \(D_{ij}\) to align with the direction of \(L=0\) and \(m=0\), which corresponds to the z axis traditionally but the y axis in the conventions of e3nn[55]. Concretely, given \(D_{ij}\vec{r}_{ij}\) aligned with the y axis, \(Y_{m_{f}}^{(L_{f})}(D_{ij}\vec{r}_{ij})\neq 0\) only for \(m_{f}=0\). If we consider only \(m_{f}=0\), \(C_{(L_{i},m_{i}),(L_{f},m_{f})}^{(L_{o},m_{o})}\) can be simplified, and \(C_{(L_{i},m_{i}),(L_{f},0)}^{(L_{o},m_{o})}\neq 0\) only when \(m_{i}=\pm m_{o}\). Therefore, the original expression depending on \(m_{i}\), \(m_{f}\), and \(m_{o}\) is now reduced to only depend on \(m_{o}\). This means we are no longer mixing all integer values of \(m_{i}\) and \(m_{f}\), and outputs of order \(m_{o}\) are linear combinations of inputs of order \(\pm m_{o}\). eSCN convolutions go one step further and replace the remaining non-trivial paths of the \(SO(3)\) tensor product with an \(SO(2)\) linear operation to allow for additional parameters of interaction between \(\pm m_{o}\) without breaking equivariance. To summarize, eSCN convolutions achieve efficient equivariant convolutions by first rotating irreps features based on relative position vectors and then performing \(SO(2)\) linear operations on the rotated features. The key idea is that the rotation sparsifies tensor products and simplifies the computation.

## 4 EquiformerV2

Starting from Equiformer [17], we first use eSCN convolutions to scale to higher-degree representations (Sec. 4.1). Then, we propose three architectural improvements, which yield further performance gain when using higher degrees: attention re-normalization (Sec. 4.2), separable \(S^{2}\) activation (Sec. 4.3) and separable layer normalization (Sec. 4.4). Figure 1 illustrates the overall architecture of EquiformerV2 and the differences from Equiformer.

### Incorporating eSCN Convolutions for Efficient Tensor Products and Higher Degrees

The computational complexity of \(SO(3)\) tensor products used in traditional \(SO(3)\) convolutions during equivariant message passing scale unfavorably with \(L_{max}\). Because of this, it is impractical for Equiformer to use beyond \(L_{max}=1\) for large-scale datasets like OC20 [30] and beyond \(L_{max}=3\) for small-scale datasets like MD17 [56; 57; 58]. Since higher \(L_{max}\) can better capture angular information and are correlated with model expressivity [5], low values of \(L_{max}\) can lead to limited performance on certain tasks such as predicting forces. Therefore, we replace original tensor products with eSCN convolutions [18] for efficient tensor products, enabling Equiformer to scale up \(L_{max}\) to \(6\) or \(8\) on the large-scale OC20 dataset.

Equiformer uses equivariant graph attention for message passing. The attention consists of depth-wise tensor products, which mix information across different degrees, and linear layers, which mix information between channels of the same degree. Since eSCN convolutions mix information across both degrees and channels, we replace the \(SO(3)\) convolution, which involves one depth-wise tensor product layer and one linear layer, with a single eSCN convolutional layer, which consists of a rotation matrix \(D_{ij}\) and an \(SO(2)\) linear layer as shown in Figure 0(b).

### Attention Re-normalization

Equivariant graph attention in Equiformer uses tensor products to project node embeddings \(x_{i}\) and \(x_{j}\), which contain vectors of different degrees, to scalar features \(f_{ij}^{(0)}\) and applies non-linear functions to \(f_{ij}^{(0)}\) for attention weights \(a_{ij}\). The node embeddings \(x_{i}\) and \(x_{j}\) are obtained by applying equivariantlayer normalization [17] to previous outputs. We note that vectors of different degrees in \(x_{i}\) and \(x_{j}\) are normalized independently, and therefore when they are projected to the same degree, the resulting \(f_{ij}^{(0)}\) can be less well-normalized. To address the issue, we propose attention re-normalization and introduce one additional layer normalization (LN) [52] before non-linear functions. Specifically, given \(f_{ij}^{(0)}\), we first apply LN and then use one leaky ReLU layer and one linear layer to calculate \(z_{ij}=a^{\top}\text{LeakyReLU}(\text{LN}(f_{ij}^{(0)}))\) and \(a_{ij}=\text{softmax}_{j}(z_{ij})=\frac{\text{exp}(z_{ij})}{\sum_{k\in\mathcal{ N}(i)}\text{exp}(z_{ik})}\), where \(a\) is a learnable vector of the same dimension as \(f_{ij}^{(0)}\).

### Separable \(S^{2}\) Activation

The gate activation [12] used by Equiformer applies sigmoid activation to scalar features to obtain non-linear weights and then multiply irreps features of degree \(>0\) with non-linear weights to add non-linearity to equivariant features. The activation, however, only accounts for the interaction from vectors of degree \(0\) to those of degree \(>0\) and could be sub-optimal when we scale up \(L_{max}\).

To better mix the information across degrees, SCN [42] and eSCN [18] propose to use \(S^{2}\) activation [59]. The activation first converts vectors of all degrees to point samples on a sphere for each channel, applies unconstrained functions \(F\) to those samples, and finally convert them back to vectors. Specifically, given an input irreps feature \(x\in\mathbb{R}^{(L_{max}+1)^{2}\times C}\), the output is \(y=G^{-1}(F(G(x)))\), where \(G\) denotes the conversion from vectors to point samples on a sphere, \(F\) can be typical SiLU activation [60, 61] or typical MLPs, and \(G^{-1}\) is the inverse of \(G\).

While \(S^{2}\) activation can better mix vectors of different degrees, we find that directly replacing the gate activation with \(S^{2}\) activation results in training instability (row 3 in Table 0(a)). To address the issue, we propose separable \(S^{2}\) activation, which separates activation for vectors of degree \(0\) and those of degree \(>0\). Similar to gate activation, we have more channels for vectors of degree \(0\). As shown in Figure 1(c), we apply a SiLU activation to the first part of vectors of degree \(0\), and the second part of vectors of degree \(0\) are used for \(S^{2}\) activation along with vectors of higher degrees. After \(S^{2}\) activation, we concatenate the first part of vectors of degree \(0\) with vectors of degrees \(>0\) as the final output and ignore the second part of vectors of degree \(0\). Additionally, we also use separable \(S^{2}\) activation in point-wise feed forward networks (FFNs). Figure 2 illustrates the differences between gate activation, \(S^{2}\) activation and separable \(S^{2}\) activation.

### Separable Layer Normalization

As mentioned in Sec. 4.2, equivariant layer normalization used by Equiformer normalizes vectors of different degrees independently, and when those vectors are projected to the same degree, the projected vectors can be less well-normalized. Therefore, instead of performing normalization to each degree independently, we propose separable layer normalization (SLN), which separates normalization for vectors of degree \(0\) and those of degrees \(>0\). Mathematically, let \(x\in\mathbb{R}^{(L_{max}+1)^{2}\times C}\) denote an input irreps feature of maximum degree \(L_{max}\) and \(C\) channels, and \(x_{m,i}^{(L)}\) denote the \(L\)-th degree, \(m\)-th order and \(i\)-th channel of \(x\). SLN calculates the output \(y\) as follows. For \(L=0\), \(y^{(0)}=\gamma^{(0)}\circ\left(\frac{x^{(0)}-\mu^{(0)}}{\sigma^{(0)}}\right)+ \beta^{(0)}\), where \(\mu^{(0)}=\frac{1}{C}\sum_{i=1}^{C}x_{0,i}^{(0)}\) and \(\sigma^{(0)}=\sqrt{\frac{1}{C}\sum_{i=1}^{C}(x_{0,i}^{(0)}-\mu^{(0)})^{2}}\). For \(L>0\), \(y^{(L)}=\gamma^{(L)}\circ\left(\frac{x^{(L)}}{\sigma^{(L>0)}}\right)\), where \(\sigma^{(L>0)}=\sqrt{\frac{1}{L_{max}}\sum_{L=1}^{L_{max}}\left(\sigma^{(L)} \right)^{2}}\) and \(\sigma^{(L)}=\sqrt{\frac{1}{C}\sum_{i=1}^{C}\frac{1}{2L+1}\sum_{m=-L}^{L} \left(x_{m,i}^{(L)}\right)^{2}}\).

\(\gamma^{(0)},\gamma^{(L)},\beta^{(0)}\in\mathbb{R}^{C}\) are learnable parameters, \(\mu^{(0)}\) and \(\sigma^{(0)}\) are mean and standard deviation of vectors of degree \(0\), \(\sigma^{(L)}\) and \(\sigma^{(L>0)}\) are root mean square values (RMS), and \(\circ\) denotes element-wise product. The computation of \(y^{(0)}\)corresponds to typical layer normalization. We note that the difference between equivariant layer normalization and SLN lies only in \(y^{(L)}\) with \(L>0\) and that equivariant layer normalization divides \(x^{(L)}\) by \(\sigma^{(L)}\), which is calculated independently for each degree \(L\), instead of \(\sigma^{(L>0)}\), which considers all degrees \(L>0\). Figure 3 compares how \(\mu^{(0)}\), \(\sigma^{(0)}\), \(\sigma^{(L)}\) and \(\sigma^{(L>0)}\) are calculated in equivariant layer normalization and SLN.

### Overall Architecture

Here, we discuss all the other modules in EquiformerV2 and focus on the differences from Equiformer.

Equivariant Graph Attention.Figure 0(b) illustrates equivariant graph attention after the above modifications. As described in Sec. 4.1, given node embeddings \(x_{i}\) and \(x_{j}\), we first concatenate them along the channel dimension and then rotate them with rotation matrices \(D_{ij}\) based on their relative positions or edge directions \(\vec{r}_{ij}\). The rotation enables reducing \(SO(3)\) tensor products to \(SO(2)\) linear operations, and we replace depth-wise tensor products and linear layers between \(x_{i}\), \(x_{j}\) and \(f_{ij}\) with a single \(SO(2)\) linear layer. To consider the information of relative distances \(||\vec{r}_{ij}||\), in the same way as eSCN [18], we transform \(||\vec{r}_{ij}||\) with a radial function to obtain distance embeddings and then multiply distance embeddings with concatenated node embeddings before the first \(SO(2)\) linear layer. We split the outputs \(f_{ij}\) of the first \(SO(2)\) linear layer into two parts. The first part is scalar features \(f_{ij}^{(0)}\), which only contains vectors of degree \(0\), and the second part is irreps features \(f_{ij}^{(L)}\) and includes vectors of all degrees up to \(L_{max}\). As mentioned in Sec. 4.2, we first apply an additional LN to \(f_{ij}^{(0)}\) and then follow the design of Equiformer by applying one leaky ReLU layer, one linear layer and a final softmax layer to obtain attention weights \(a_{ij}\). As for value \(v_{ij}\), we replace the gate activation with separable \(S^{2}\) activation with \(F\) being a single SiLU activation and then apply the second \(SO(2)\) linear layer. While in Equiformer, the message \(m_{ij}\) sent from node \(j\) to node \(i\) is \(m_{ij}=a_{ij}\times v_{ij}\), here we need to rotate \(a_{ij}\times v_{ij}\) back to original coordinate frames and the message \(m_{ij}\) becomes \(D_{ij}^{-1}(a_{ij}\times v_{ij})\). Finally, we can perform \(h\) parallel equivariant graph attention functions given \(f_{ij}\). The \(h\) different outputs are concatenated and projected with a linear layer to become the final output \(y_{i}\). Parallelizing attention functions and concatenating can be implemented with "Reshape".

Feed Forward Network.As illustrated in Figure 0(d), we replace the gate activation with separable \(S^{2}\) activation. The function \(F\) consists of a two-layer MLP, with each linear layer followed by SiLU, and a final linear layer.

Embedding.This module consists of atom embedding and edge-degree embedding. The former is the same as that in Equiformer. For the latter, as depicted in the right branch in Figure 0(c), we replace original linear layers and depth-wise tensor products with a single \(SO(2)\) linear layer followed by a rotation matrix \(D_{ij}^{-1}\). Similar to equivariant graph attention, we consider the information of relative distances by multiplying the outputs of the \(SO(2)\) linear layer with distance embeddings.

Radial Basis and Radial Function.We represent relative distances \(||\vec{r}_{ij}||\) with a finite radial basis like Gaussian radial basis functions [43] to capture their subtle changes. We transform radial basis with a learnable radial function to generate distance embeddings. The function consists of a two-layer MLP, with each linear layer followed by LN and SiLU, and a final linear layer.

Output Head.To predict scalar quantities like energy, we use one feed forward network to transform irreps features on each node into a scalar and then perform sum aggregation over all nodes. As for predicting forces acting on each node, we use a block of equivariant graph attention and treat the output of degree \(1\) as our predictions.

## 5 OC20 Experiments

Our experiments focus on the large and diverse OC20 dataset [30] (Creative Commons Attribution 4.0 License), which consists of \(1.2\)M DFT relaxations for training and evaluation, computed with the revised Perdew-Burke-Ernzerhof (RPBE) functional [62]. Each structure in OC20 has an adsorbate molecule placed on a catalyst surface, and the core task is Structure-to-Energy-Forces (S2EF), which is to predict the energy of the structure and per-atom forces. Models trained for the S2EF task are evaluated on energy and force mean absolute error (MAE). These models can in turn be used for performing structure relaxations by using the model's force predictions to iteratively update the atomic positions until a relaxed structure corresponding to a local energy minimum is found. Theserelaxed structure and energy predictions are evaluated on the Initial Structure to Relaxed Structure (IS2RS) and Initial Structure to Relaxed Energy (IS2RE) tasks. The "All" split of OC20 contains \(134\)M training structures spanning \(56\) elements, and "MD" split consists of \(38\)M structures. We first conduct ablation studies on EquiformerV2 trained on the smaller S2EF-2M subset (Sec. 5.1). Then, we report the results of training on S2EF-All and S2EF-All+MD splits (Sec. 5.2). Additionally, we investigate the performance of EquiformerV2 when used in the AdsorbML algorithm [10] (Sec. 5.3). Please refer to Sec. B and C for details of models and training.

### Ablation Studies

**Architectural Improvements**. In Table 0(a), we ablate the three proposed architectural changes - attention re-normalization, separable \(S^{2}\) activation and separable layer normalization. First, with attention re-normalization (row 1 and 2), energy errors improve by \(2.4\%\), while force errors are about the same. Next, we replace the gate activation with \(S^{2}\) activation used in SCN [42] and eSCN [18], but that does not converge (row 3). Instead, using the proposed separable \(S^{2}\) activation (row 4), where we have separate paths for invariant and equivariant features, converges to \(5\%\) better forces albeit hurting energies. Similarly, replacing equivariant layer normalization with separable layer normalization (row 5) further improves forces by \(1.5\%\). Finally, these modifications enable training for longer without overfitting (row 7), further improving forces by \(3.6\%\) and recovering energies to similar accuracies as Index 2. Overall, our modifications improve forces by \(10\%\) and energies by \(3\%\). Note that simply incorporating eSCN convolutions into Equiformer (row 1) and using higher degrees does not result in improving the original eSCN baseline (row 8), and that the proposed architectural changes are necessary.

**Scaling of Parameters**. In Tables 0(c), 0(d), 0(e), we systematically vary the maximum degree \(L_{max}\), the maximum order \(M_{max}\), and the number of Transformer blocks and compare with equivalent eSCN variants. There are several key takeaways. First, across all experiments, EquiformerV2 performs better than its eSCN counterparts. Second, while one might intuitively expect higher resolution features and larger models to perform better, this is only true for EquiformerV2, not eSCN. For example, increasing \(L_{max}\) from \(6\) to \(8\) or \(M_{max}\) from \(3\) to \(4\) degrades the performance of eSCN on energy predictions but helps that of EquiformerV2. In Table 0(b), we show that longer training regimes are crucial. Increasing the training epochs from \(12\) to \(30\) with \(L_{max}=6\) improves force and energy predictions by \(5\%\) and \(2.5\%\), respectively.

**Comparison of Speed-Accuracy Trade-offs**. To be practically useful for atomistic simulations and material screening, models should offer flexibility in speed-accuracy tradeoffs. We compare these trade-offs for EquiformerV2 with prior works in Figure 3(a). Here, the speed is reported as the number of structures processed per GPU-second during inference and measured on V100 GPUs.

\begin{table}

\end{table}
Table 1: Ablation results with EquiformerV2. We report mean absolute errors for forces in meV/Å and energy in meV, and lower is better. All models are trained on the 2M subset of OC20 [30], and errors are averaged over the four validation splits of OC20. The base model setting is marked in gray.

[MISSING_PAGE_FAIL:8]

adsorption energies within a \(0.1\)eV margin of DFT results with an \(87\%\) success rate. This is done by using OC20-trained models to perform structure relaxations for an average \(90\) configurations of an adsorbate placed on a catalyst surface, followed by DFT single-point calculations for the top-\(k\) structures with lowest predicted relaxed energies, as a proxy for calculating the global energy minimum or adsorption energy. We refer the reader to the AdsorbML paper [10] for more details. We benchmark AdsorbML with EquiformerV2, and Table 3 shows that it improves over SCN by a significant margin, with \(8\%\) and \(5\%\) absolute improvements at \(k=1\) and \(k=2\), respectively. Moreover, EquiformerV2 at \(k=2\) is more accurate at adsorption energy calculations than all the other models even at \(k=5\), thus requiring at least \(2\times\) fewer DFT calculations.

## 6 Conclusion

In this work, we investigate how equivariant Transformers can be scaled up to higher degrees of equivariant representations. We start by replacing \(SO(3)\) convolutions in Equiformer with eSCN convolutions, and propose three architectural improvements to better leverage the power of higher degrees - attention re-normalization, separable \(S^{2}\) activation and separable layer normalization. With these modifications, we propose EquiformerV2, which outperforms state-of-the-art methods on the S2EF, IS2RS, and IS2RE tasks on the OC20 dataset, improves speed-accuracy trade-offs, and achieves the best success rate when used in AdsorbML.

Broader Impacts.EquiformerV2 achieves more accurate approximation of quantum mechanical calculations and demonstrates one further step toward replacing DFT force fields with machine learned ones. By demonstrating its promising results, we hope to encourage the community to make further progress in applications like material design and drug discovery than to use it for adversarial purposes. Additionally, the method only facilitates identification of molecules or materials of specific properties, and there are substantial hurdles from their large-scale deployment. Finally, we note that the proposed method is general and can be applied to different problems like protein structure prediction [64] as long as inputs can be modeled as 3D graphs.

Limitations.Although EquiformerV2 improves upon state-of-the-art methods on the large and diverse OC20 dataset, we acknolwdge that the performance gains brought by scaling to higher degrees and the proposed architectural improvements can depend on tasks and datasets. For example, the increased expressivity may lead to overfitting on smaller datasets like QM9 [65, 66] and MD17 [56, 57, 58]. However, the issue can be mitigated by pre-training on large datasets like OC20 [30] and PCQM4Mv2 [67] optionally via denoising [68] and then finetuning on smaller datasets.

## References

* [1] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, "Neural message passing for quantum chemistry," in _International Conference on Machine Learning (ICML)_, 2017.
* [2] L. Zhang, J. Han, H. Wang, R. Car, and W. E, "Deep potential molecular dynamics: A scalable model with the accuracy of quantum mechanics," _Phys. Rev. Lett._, vol. 120, p. 143001, Apr 2018.
* [3] W. Jia, H. Wang, M. Chen, D. Lu, L. Lin, R. Car, W. E, and L. Zhang, "Pushing the limit of molecular dynamics with ab initio accuracy to 100 million atoms with machine learning," in _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, SC '20, IEEE Press, 2020.
* [4] J. Gasteiger, S. Giri, J. T. Margraf, and S. Gunnemann, "Fast and uncertainty-aware directional message passing for non-equilibrium molecules," in _Machine Learning for Molecules Workshop, NeurIPS_, 2020.

\begin{table}
\begin{tabular}{l c c c c c c c c c}  & \multicolumn{2}{c}{\(k=1\)} & \multicolumn{2}{c}{\(k=3\)} & \multicolumn{2}{c}{\(k=5\)} \\ \cline{2-10} Model & Success & Speedup & Success & Speedup & Success & Speedup & Success & Speedup & Success & Speedup \\ \hline SchNet [43] & 2.77\% & 4266.13 & 3.91\% & 2155.36 & 4.32\% & 1458.77 & 4.73\% & 11014.88 & 5.04\% & 892.79 \\ DimNet++ [4] & 5.34\% & 4271.23 & 7.61\% & 2149.78 & 8.84\% & 1435.21 & 10.07\% & 1081.96 & 10.79\% & 865.20 \\ PaNN [34] & 27.44\% & 4089.77 & 33.61\% & 2077.65 & 36.69\% & 1395.55 & 38.64\% & 1048.63 & 39.57\% & 840.44 \\ GemNet-OC [51] & 68.76\% & 4158.18 & 77.29\% & 2087.11 & 80.78\% & 1392.91 & 81.50\% & 1046.85 & 82.94\% & 840.25 \\ GemNet-OC-MD [51] & 68.76\% & 4182.04 & 78.21\% & 2092.27 & 81.81\% & 1404.11 & 83.25\% & 1053.36 & 84.38\% & 841.64 \\ GemNet-OC-MD-Large [51] & 73.18\% & 4078.76 & 79.65\% & 2065.15 & 83.25\% & 1831.81 & 85.41\% & 1041.50 & 86.02\% & 83.44\% \\ SCN-MD-Large [24] & 77.70\% & 3974.21 & 84.28\% & 1989.32 & 86.33\% & 1331.43 & 87.36\% & 1004.40 & 87.77\% & 807.00 \\ EquiformerV2 (\(\lambda_{E}=4\)) & **85.41\%** & 4001.71 & **88.90\%** & 2012.47 & **90.54\%** & 1352.08 & **91.06\%** & 1016.31 & **91.57\%** & 815.87 \\ \end{tabular}
\end{table}
Table 3: AdsorbML results with EquiformerV2 (\(\lambda_{E}=4\)) trained on S2EF-All+MD from Table 2.

* [5] S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P. Mailoa, M. Kornbluth, N. Molinari, T. E. Smidt, and B. Kozinsky, "E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials," _Nature Communications_, vol. 13, May 2022.
* [6] D. Lu, H. Wang, M. Chen, L. Lin, R. Car, W. E, W. Jia, and L. Zhang, "86 pflops deep potential molecular dynamics simulation of 100 million atoms with ab initio accuracy," _Computer Physics Communications_, vol. 259, p. 107624, 2021.
* [7] O. T. Unke, M. Bogojeski, M. Gastegger, M. Geiger, T. Smidt, and K. R. Muller, "SE(3)-equivariant prediction of molecular wavefunctions and electronic densities," in _Advances in Neural Information Processing Systems (NeurIPS)_ (A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), 2021.
* [8] A. Sriram, A. Das, B. M. Wood, and C. L. Zitnick, "Towards training billion parameter graph neural networks for atomic simulations," in _International Conference on Learning Representations (ICLR)_, 2022.
* [9] J. A. Rackers, L. Tecot, M. Geiger, and T. E. Smidt, "A recipe for cracking the quantum scaling limit with machine learned electron densities," _Machine Learning: Science and Technology_, vol. 4, p. 015027, feb 2023.
* [10] J. Lan, A. Palizhati, M. Shuaibi, B. M. Wood, B. Wander, A. Das, M. Uyttendaele, C. L. Zitnick, and Z. W. Ulissi, "AdsorbML: Accelerating adsorption energy calculations with machine learning," _arXiv preprint arXiv:2211.16486_, 2022.
* [11] N. Thomas, T. E. Smidt, S. Kearnes, L. Yang, L. Li, K. Kohlhoff, and P. Riley, "Tensor field networks: Rotation- and translation-equivariant neural networks for 3d point clouds," _arxiv preprint arXiv:1802.08219_, 2018.
* [12] M. Weiler, M. Geiger, M. Welling, W. Boomsma, and T. Cohen, "3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data," in _Advances in Neural Information Processing Systems 32_, pp. 10402-10413, 2018.
* [13] R. Kondor, Z. Lin, and S. Trivedi, "Clebsch-gordan nets: a fully fourier space spherical convolutional neural network," in _Advances in Neural Information Processing Systems 32_, pp. 10117-10126, 2018.
* [14] F. Fuchs, D. E. Worrall, V. Fischer, and M. Welling, "Se(3)-transformers: 3d roto-translation equivariant attention networks," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [15] J. Brandstetter, R. Hesselink, E. van der Pol, E. J. Bekkers, and M. Welling, "Geometric and physical quantities improve e(3) equivariant message passing," in _International Conference on Learning Representations (ICLR)_, 2022.
* [16] A. Musaelian, S. Batzner, A. Johansson, L. Sun, C. J. Owen, M. Kornbluth, and B. Kozinsky, "Learning local equivariant representations for large-scale atomistic dynamics," _arxiv preprint arxiv:2204.05249_, 2022.
* [17] Y.-L. Liao and T. Smidt, "Equformer: Equivariant graph attention transformer for 3d atomistic graphs," in _International Conference on Learning Representations (ICLR)_, 2023.
* [18] S. Passaro and C. L. Zitnick, "Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs," in _International Conference on Machine Learning (ICML)_, 2023.
* [19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* [20] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, "Transformers in vision: A survey," _arXiv preprint arxiv:2101.01169_, 2021.
* [21] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, "End-to-end object detection with transformers," in _European Conference on Computer Vision (ECCV)_, 2020.
* [22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, "An image is worth 16x16 words: Transformers for image recognition at scale," in _International Conference on Learning Representations (ICLR)_, 2021.

* [23] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou, "Training data-efficient image transformers & distillation through attention," _arXiv preprint arXiv:2012.12877_, 2020.
* [24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pre-training of deep bidirectional transformers for language understanding," _arxiv preprint arxiv:1810.04805_, 2019.
* [25] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, "Language models are few-shot learners," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [26] V. P. Dwivedi and X. Bresson, "A generalization of transformer networks to graphs," _arxiv preprint arxiv:2012.09699_, 2020.
* [27] D. Kreuzer, D. Beaini, W. L. Hamilton, V. Letourneau, and P. Tossou, "Rethinking graph transformers with spectral attention," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [28] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu, "Do transformers really perform badly for graph representation?," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [29] Y. Shi, S. Zheng, G. Ke, Y. Shen, J. You, J. He, S. Luo, C. Liu, D. He, and T.-Y. Liu, "Benchmarking graphormer on large-scale molecular modeling datasets," _arxiv preprint arxiv:2203.04810_, 2022.
* [30] L. Chanussot*, A. Das*, S. Goyal*, T. Lavril*, M. Shuaibi*, M. Riviere, K. Tran, J. Heras-Domingo, C. Ho, W. Hu, A. Palizhati, A. Sriram, B. Wood, J. Yoon, D. Parikh, C. L. Zitnick, and Z. Ulissi, "Open catalyst 2020 (oc20) dataset and community challenges," _ACS Catalysis_, 2021.
* [31] B. K. Miller, M. Geiger, T. E. Smidt, and F. Noe, "Relevance of rotationally equivariant convolutions for predicting molecular properties," _arxiv preprint arxiv:2008.08461_, 2020.
* [32] R. J. L. Townshend, B. Townshend, S. Eismann, and R. O. Dror, "Geometric prediction: Moving beyond scalars," _arXiv preprint arXiv:2006.14163_, 2020.
* [33] B. Jing, S. Eismann, P. Suriana, R. J. L. Townshend, and R. Dror, "Learning from protein structure with geometric vector perceptrons," in _International Conference on Learning Representations (ICLR)_, 2021.
* [34] K. T. Schutt, O. T. Unke, and M. Gastegger, "Equivariant message passing for the prediction of tensorial properties and molecular spectra," in _International Conference on Machine Learning (ICML)_, 2021.
* [35] V. G. Satorras, E. Hoogeboom, and M. Welling, "E(n) equivariant graph neural networks," in _International Conference on Machine Learning (ICML)_, 2021.
* [36] P. Tholke and G. D. Fabritiis, "Equivariant transformers for neural network based molecular potentials," in _International Conference on Learning Representations (ICLR)_, 2022.
* [37] T. Le, F. Noe, and D.-A. Clevert, "Equivariant graph attention networks for molecular property prediction," _arXiv preprint arXiv:2202.09891_, 2022.
* [38] I. Batatia, D. P. Kovacs, G. N. C. Simm, C. Ortner, and G. Csanyi, "MACE: Higher order equivariant message passing neural networks for fast and accurate force fields," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [39] A. Sanchez-Gonzalez, J. Godwin, T. Pfaff, R. Ying, J. Leskovec, and P. W. Battaglia, "Learning to simulate complex physics with graph networks," in _International Conference on Machine Learning (ICML)_, 2020.
* [40] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, "Dropout: A simple way to prevent neural networks from overfitting," _Journal of Machine Learning Research_, vol. 15, no. 56, pp. 1929-1958, 2014.
* [41] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, "Deep networks with stochastic depth," in _European Conference on Computer Vision (ECCV)_, 2016.

* [42] L. Zitnick, A. Das, A. Kolluru, J. Lan, M. Shuaibi, A. Sriram, Z. Ulissi, and B. Wood, "Spherical channels for modeling atomic interactions," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [43] K. T. Schutt, P.-J. Kindermans, H. E. Sauceda, S. Chmiela, A. Tkatchenko, and K.-R. Muller, "Schnet: A continuous-filter convolutional neural network for modeling quantum interactions," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* [44] T. Xie and J. C. Grossman, "Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties," _Physical Review Letters_, 2018.
* [45] O. T. Unke and M. Meuwly, "PhysNet: A neural network for predicting energies, forces, dipole moments, and partial charges," _Journal of Chemical Theory and Computation_, vol. 15, pp. 3678-3693, may 2019.
* [46] J. Gasteiger, J. Gross, and S. Gunnemann, "Directional message passing for molecular graphs," in _International Conference on Learning Representations (ICLR)_, 2020.
* [47] Z. Qiao, M. Welborn, A. Anandkumar, F. R. Manby, and T. F. Miller, "OrbNet: Deep learning for quantum chemistry using symmetry-adapted atomic-orbital features," _The Journal of Chemical Physics_, 2020.
* [48] Y. Liu, L. Wang, M. Liu, Y. Lin, X. Zhang, B. Oztekin, and S. Ji, "Spherical message passing for 3d molecular graphs," in _International Conference on Learning Representations (ICLR)_, 2022.
* [49] M. Shuaibi, A. Kolluru, A. Das, A. Grover, A. Sriram, Z. Ulissi, and C. L. Zitnick, "Rotation invariant graph neural networks using spin convolutions," _arxiv preprint arxiv:2106.09575_, 2021.
* [50] J. Klicpera, F. Becker, and S. Gunnemann, "Gemnet: Universal directional graph neural networks for molecules," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [51] J. Gasteiger, M. Shuaibi, A. Sriram, S. Gunnemann, Z. Ulissi, C. L. Zitnick, and A. Das, "GemNet-OC: Developing Graph Neural Networks for Large and Diverse Molecular Simulation Datasets," _Transactions on Machine Learning Research (TMLR)_, 2022.
* [52] J. L. Ba, J. R. Kiros, and G. E. Hinton, "Layer normalization," _arxiv preprint arxiv:1607.06450_, 2016.
* [53] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, "Graph attention networks," in _International Conference on Learning Representations (ICLR)_, 2018.
* [54] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, "Deep networks with stochastic depth," in _European Conference on Computer Vision (ECCV)_, 2016.
* [55] M. Geiger, T. Smidt, A. M., B. K. Miller, W. Boomsma, B. Dice, K. Lapchevskyi, M. Weiler, M. Tyszkiewicz, S. Batzner, D. Madisetti, M. Uhrin, J. Frellsen, N. Jung, S. Sanborn, M. Wen, J. Rackers, M. Rod, and M. Bailey, "e3nn/e3nn: 2022-04-13," Apr. 2022.
* [56] S. Chmiela, A. Tkatchenko, H. E. Sauceda, I. Poltavsky, K. T. Schutt, and K.-R. Muller, "Machine learning of accurate energy-conserving molecular force fields," _Science Advances_, vol. 3, no. 5, p. e1603015, 2017.
* [57] K. T. Schutt, F. Arbabzadah, S. Chmiela, K. R. Muller, and A. Tkatchenko, "Quantum-chemical insights from deep tensor neural networks," _Nature Communications_, vol. 8, jan 2017.
* [58] S. Chmiela, H. E. Sauceda, K.-R. Muller, and A. Tkatchenko, "Towards exact molecular dynamics simulations with machine-learned force fields," _Nature Communications_, vol. 9, sep 2018.
* [59] T. S. Cohen, M. Geiger, J. Kohler, and M. Welling, "Spherical CNNs," in _International Conference on Learning Representations (ICLR)_, 2018.
* [60] S. Elfwing, E. Uchibe, and K. Doya, "Sigmoid-weighted linear units for neural network function approximation in reinforcement learning," _arXiv preprint arXiv:1702.03118_, 2017.
* [61] P. Ramachandran, B. Zoph, and Q. V. Le, "Searching for activation functions," _arXiv preprint arXiv:1710.05941_, 2017.
* [62] B. Hammer, L. B. Hansen, and J. K. Norskov, "Improved adsorption energetics within density-functional theory using revised perdew-burke-ernzerhof functionals," _Phys. Rev. B_, 1999.

* [63] W. Hu, M. Shuaibi, A. Das, S. Goyal, A. Sriram, J. Leskovec, D. Parikh, and C. L. Zitnick, "Forcenet: A graph neural network for large-scale quantum calculations," _arxiv preprint arxiv:2103.01436_, 2021.
* [64] J. H. Lee, P. Yadollahpour, A. Watkins, N. C. Frey, A. Leaver-Fay, S. Ra, K. Cho, V. Gligorijevic, A. Regev, and R. Bonneau, "Equifold: Protein structure prediction with a novel coarse-grained structure representation," _bioRxiv_, 2022.
* [65] R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. von Lilienfeld, "Quantum chemistry structures and properties of 134 kilo molecules," _Scientific Data_, vol. 1, 2014.
* [66] L. Ruddigkeit, R. van Deursen, L. C. Blum, and J.-L. Reymond, "Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17," _Journal of Chemical Information and Modeling_, vol. 52, no. 11, pp. 2864-2875, 2012. PMID: 23088335.
* [67] M. Nakata and T. Shimazaki, "Pubchemqc project: A large-scale first-principles electronic structure database for data-driven chemistry," _Journal of chemical information and modeling_, vol. 57 6, pp. 1300-1308, 2017.
* [68] S. Zaidi, M. Schaarschmidt, J. Martens, H. Kim, Y. W. Teh, A. Sanchez-Gonzalez, P. Battaglia, R. Pascanu, and J. Godwin, "Pre-training via denoising for molecular property prediction," in _International Conference on Learning Representations (ICLR)_, 2023.