# Classification of Heavy-tailed Features in High Dimensions: a Superstatistical Approach

 Urte Adomaityte

Department of Mathematics

King's College London

urte.adomaityte@kcl.ac.uk

Gabriele Sicuro

Department of Mathematics

King's College London

gabriele.sicuro@kcl.ac.uk

Pierpaolo Vivo

Department of Mathematics

King's College London

pierpaolo.vivo@kcl.ac.uk

###### Abstract

We characterise the learning of a mixture of two clouds of data points with generic centroids via empirical risk minimisation in the high dimensional regime, under the assumptions of generic convex loss and convex regularisation. Each cloud of data points is obtained via a double-stochastic process, where the sample is drawn from a Gaussian distribution whose variance is itself a random parameter sampled from a scalar distribution \(\varrho\). As a result, our analysis covers a large family of data distributions, including the case of power-law-tailed distributions with no covariance, and allows us to test recent "Gaussian universality" claims. We study the generalisation performance of the obtained estimator, we analyse the role of regularisation, and we analytically characterise the separability transition.

## 1 Introduction

Generalised linear models (GLMs) are still ubiquitous in the theoretical research on machine learning, despite their simplicity. Their nontrivial phenomenology is often amenable to complete analytical treatment and has been key to understanding the unexpected behavior of large and complex architectures. Random features models [61, 47, 24], for example, allowed to clarify many aspects of the well-known double-descent phenomenon in neural networks [54, 8, 9]. A line of research spanning more than three decades [68, 74] has considered a variety of GLMs to investigate a number of aspects of learning in high dimensions. Yet, a crucial assumption adopted in many such theoretical models is that the covariates are obtained from a _Gaussian_ distribution, or from a mixture of Gaussian distributions [42, 49, 3, 37, 43]. Although such a Gaussian design has served as a convenient working hypothesis (in some cases experimentally and theoretically justified [67, 44]), it is however not obvious how much it limits the scope of the results. It is reasonable to expect structure, heavy tails, and large fluctuations in real (often non-Gaussian) data [1, 62] to play an important role in the learning process, and it would be therefore desirable to include structured and heavy-tailed-distributed covariates in our theoretical toolbox.

This paper presents, to the best of our knowledge for the first time, the exact asymptotics for classification tasks on covariates obtained from a mixture of heavy-tailed distributions. We focus on supervised binary classification assuming that the sample size \(n\) and the dimensionality \(d\) of the space where the covariates live are both sent to infinity, keeping their ratio \(\nicefrac{{n}}{{d}}=\alpha\) fixed. The paper fits therefore in the line of works on exact high-dimensional asymptotics for classification learning via a GLM [49, 43], but, crucially, we relax the usual Gaussian data hypothesis by including in ouranalysis power-law-tailed distributions with possibly no covariance. The mixture is obtained from two distributions, each centered around a centroid \(\bm{\mu}\in\mathds{R}^{d}\) and resulting from a double stochastic process. Namely, each sample point is obtained from a distribution \(\mathbb{N}(\bm{\mu},\bm{\Sigma})\) whose covariance \(\bm{\Sigma}=\Delta\bm{I}_{d}\in\mathds{R}^{d\times d}\) is itself a random variable so that \(\Delta\) has density \(\varrho\), supported on \(\mathds{R}^{+}_{*}\). Using, for example, an appropriately parametrised inverse-Gamma distribution for \(\varrho\), such a _superstatistical_ construction (as known in the physics literature [5, 7]) can provide, for example, a heavy-tailed, Cauchy-like data distribution with infinite covariance. The replica method [48], an analytical tool widely adopted in statistical physics, provides asymptotic formulas for a generic density \(\varrho\), allowing us to study the effects of non-Gaussianity on the performance curves, and test Gaussian universality hypotheses [59] in such a classification task.

Motivation and related worksLearning a rule to classify data points clustered in clouds in high dimension is a classical problem in statistics [32]. Its ubiquity is exemplified by the recently observed neural collapse phenomenon in deep neural networks, in which the last layer classifier was found to operate on features clustered in clouds around the vertices of a simplex [38, 56]; more recently, Seddik et al. [67] showed that Gaussian mixtures suitably describe the deep learning representation of GAN data. In theoretical models, data points are typically assumed to be organised in \(K\)_Gaussian_ clouds, each corresponding to a class. Each class \(k\), \(k\in\{1,\ldots,K\}\), is centered around a mean vector \(\bm{\mu}_{k}\) and has covariance \(\bm{\Sigma}_{k}\). The binary classification case, \(K=2\), is the simplest, and possibly most studied, setting. For this case, Mai and Liao [46] considered an ERM task with generic convex loss and ridge regularisation in the high-dimensional regime \(n\), \(d\to+\infty\) with \(\nicefrac{{n}}{{d}}\in(0,+\infty)\). In this setting, they gave a precise prediction of the classification error, showing the optimality of the square loss in the unregularised case. Their results have been extended by Mignacco et al. [49], who showed that the presence of a regularisation can actually drastically affect the performance, improving it. In the same setting, the Bayes optimal estimator has been studied in both the supervised and the semi-supervised setting [49, 41]; almost-Bayes-optimal solutions have been put in relation to wide flat landscape regions [3]. In the context of binary classification, the maximum number \(n\) of samples that can be perfectly fitted by a linear model in dimension \(d\)[49, 16, 37] has been the topic of investigation since the seminal work of Cover [12] and is related to the classical storage capacity problems on networks [20, 21, 39]. The corresponding separability transition value \(\alpha=\nicefrac{{n}}{{d}}\) is remarkably associated with the existence transition of the maximum likelihood estimator [70, 76]. The precise asymptotic characterisation of the test error in learning to classify \(K\geq 2\) Gaussian clouds with generic means and covariances has been recently obtained by Loureiro et al. [43]. Within this line of research, rigorous results have been obtained by a variety of methods, such as Gordon's inequality technique [31, 71, 49] or mapping to approximate message passing schemes [4, 36, 10, 43, 27].

As previously mentioned, the working hypothesis of _Gaussian design_ is widely adopted in high-dimensional classification problems. On top of being a convenient technical hypothesis, this assumption has been justified in terms of a "Gaussian universality" principle. In other words, in a number of circumstances, non-Gaussian features distributions are found to be effectively described by Gaussian ones with matching first and second moments as far as the asymptotic properties of the estimators are concerned [44]. Such a "universality property" has been rigorously proven for example in compressed sensing [50] or in the case of LASSO with non-Gaussian dictionaries [55]. A Gaussian equivalence principle introduced by Goldt et al. [30] has been proven to hold for a wide class of generalised linear estimation problems [47]. Extending work by Hu and Lu [34], Montanari and Saeed [51] recently proved such a principle in a GLM under the assumption of _pointwise normality_ of the distribution of the features. This crucial assumption can be intended, roughly speaking, as the assumption of sub-Gaussian decay of a marginal of the feature distribution in any direction (see also [25, 13] for further developments).

The Gaussian universality principle, however, can break down by relaxing some of the aforementioned assumptions on the feature distribution. Montanari and Saeed [51] for example showed that pointwise normality is a _necessary_ hypothesis for universality. Studying regression tasks on an elliptically distributed dataset, El Karoui [18] showed that claims of "universality" obtained in the Gaussian setting require serious scrutiny, as the statistics of the estimators might strongly depend on the non-Gaussianity of the covariates [18, 69, 72]. More recently, Pesce et al. [59] showed that a structured Gaussian mixture cannot be effectively described by a single Gaussian cloud. Building upon a series of contributions related to the asymptotic performance of models in the proportional high-dimensional limit [33, 47, 28, 24, 30, 44], we aim precisely to explore the validity of Gaussian universality within classification problems.

In this paper, we work in a "superstatistical" data setting [5; 7], meaning that we superpose the statistics of Gaussian data distribution with an assumed distribution of its variance. Such a construction is adopted in a number of disciplines and contexts to go beyond Gaussianity, albeit is known under different names. In statistical physics, it is known as "superstatistics" and is employed in the analysis of non-equilibrium and non-linear systems [6]. In Bayesian modeling, it is common to refer to hierarchical priors and models [22; 23], while in probability, statistics, and actuarial sciences such distributions are known as compound probability distributions [63], or doubly-stochastic models [60; 66]. Crucially, this construction allows us to consider a very large family of non-Gaussian distributions, which include, but are not limited to, any power-law decay and Cauchy-like with possible infinite-covariance parametrisations.

Our contributionsIn this manuscript we provide the following results.

\(\bullet\) We study a classification task on a non-Gaussian mixture model (see Eq. (1) below) via a generalised linear model (GLM) and we analytically derive, using the replica method [14; 19; 48], an asymptotic characterisation of the statistics of the empirical risk minimisation (ERM) estimator. Our results go therefore beyond the usual Gaussian assumption for the dataset, and include for example the case of covariates obtained from a mixture of distributions with infinite variance. The analysis is performed in the high-dimensional, proportional limit and for any convex loss and convex regularisation. By using this result, we provide asymptotic formulas for the generalisation, training errors and training loss.

\(\bullet\) We analyse the performance of this ERM task on a specific family of dataset distributions by using different convex loss functions (quadratic and logistic) and ridge regularisation. We show in particular that, in the case of two balanced clusters with a specific non-Gaussian distribution, the optimal ridge regularisation strength \(\lambda^{\star}\) is finite, at odds with the Gaussian case, for which \(\lambda^{\star}\to\infty\)[49]. In this respect, by considering distributions with matching first and second moments, we analytically show that the performances of the analysed GLM do depend in general on higher moments, and therefore the "Gaussian universality principle" breaks down when heavy-tailed distributions are considered.

\(\bullet\) We derive the separability threshold on a large family of non-Gaussian dataset distributions, possibly with unbounded covariance, generalising the known asymptotics for the separability of Gaussian clouds [49]. The result of Cover [12] is recovered in the case of infinite distribution width.

\(\bullet\) Under some moment conditions, we derive the Bayes-optimal performance in the case of binary classifications with symmetric centroids, generalising the argument in Ref. [49].

\(\bullet\) We finally extend recent results on Gaussian universality in the Gaussian mixture model with random labels [25; 59] to the case of non-Gaussian distributions. We show that the universal formula for the square training loss at zero regularisation found in Ref. [25] holds in our more general setting as well.

## 2 Main result

Dataset constructionWe consider the task of classifying two data clusters in the \(d\)-dimensional space \(\mathds{R}^{d}\). The dataset \(\mathcal{D}\coloneqq\{(\bm{x}^{\nu},y^{\nu})\}_{\nu\in[n]}\) is obtained by extracting \(n\) independent datapoints \(\bm{x}^{\nu}\), each associated with a label \(y^{\nu}\in\{-1,1\}\) (a more general setting, involving a multiclass classification task, is discussed in Appendix A). The data points are correlated with the labels via a law \(P(\bm{x},y)\) which we assume to have the form

\[P(\bm{x},y)=\delta_{y,1}\rho P(\bm{x}|\bm{\mu}_{+})+\delta_{y,-1}(1-\rho)P( \bm{x}|\bm{\mu}_{-}),\qquad\rho\in(0,1),\qquad\bm{\mu}_{\pm}\in\mathds{R}^{d}.\] (1a) Here \[P(\bm{x}|\bm{\mu})\] is a distribution with mean \[\bm{\mu}\], and the mean vectors \[\bm{\mu}_{\pm}\in\mathds{R}^{d}\] are distributed according to some density, such that \[\mathbb{E}\left[\left\|\bm{\mu}\right\|^{2}\right]=\Theta(1)\], and correspond to the center of the two clusters. The scalar quantity \[\rho\] weighs the relative contribution of the two clusters: in the following, we will denote \[\rho_{+}=\rho=1-\rho_{-}\]. Each cluster distribution \[P(\bm{x}|\bm{\mu})\] around a vector \[\bm{\mu}\] is assumed to have the form \[P(\bm{x}|\bm{\mu})\coloneqq\mathbb{E}_{\Delta}\left[\mathcal{N}\left(\bm{x} \left|\bm{\mu},\Delta\bm{I}_{d}\right.\right)\right],\] (1b) where \[\mathcal{N}(\bm{x}|\bm{\mu},\bm{\Sigma})\] is a Gaussian distribution with mean \[\bm{\mu}\in\mathds{R}^{d}\] and covariance \[\bm{\Sigma}\in\mathds{R}^{d\times d}\], and \[\Delta\] is randomly distributed with some density \[\varrho\] with support on \[\mathds{R}^{+}_{+}\coloneqq(0,+\infty)\]. The family of "elliptic-like" distributions in Eq. (1b) has been extensively studied, for instance, by the physics community, in the context of _superstatistics_[5; 7]. Mixtures of normals in the form of Eq. 1b are a central tool in Bayesian statistics [65] due to their ability to approximate any distribution given a sufficient number of components [53, 2, 29]. Although the family in Eq. (1b) is not the most general of such mixtures, it is sufficient to include a large family of power-law-tailed densities and to allow us to go beyond the usual Gaussian approximation. El Karoui [18] considered the statistical properties, in the same asymptotic proportional regime as here, of ridge-regularised regression estimators on datasets with distribution as in Eq. 1b, under the assumption (here relaxed) that \(\mathbb{E}[\Delta^{2}]<+\infty\). This _elliptic_ family includes a large class of distributions with properties markedly different from the ones of Gaussians. For example, the inverse-Gamma with density \(\varrho(\Delta)=(2\pi\Delta^{3})^{-1/2}\,\mathrm{e}^{-\frac{1}{2\delta}}\) leads to a \(d\)-dimensional Cauchy-like distribution \(P(\bm{x}|\bm{\mu})\propto(1-\|\bm{\mu}-\bm{x}\|^{2})^{-\frac{d\phi+1}{2}}\), having as marginals Cauchy distributions and \(\mathbb{E}[\|\bm{x}-\bm{\mu}\|^{2}]=+\infty\). We will perform our classification task by searching for a set of parameters \((\bm{w}^{\star},\bm{b}^{\star})\), called _weights_ and _bias_, respectively, that will allow us to construct an estimator via a certain classifier \(\varphi\colon\mathds{R}\to\{-1,1\}\)

\[\bm{x}\mapsto\varphi\left(\frac{\bm{x}^{\top}\bm{w}^{\star}}{\sqrt{d}}+\bm{b} ^{\star}\right).\] (2)

By means of the above law, we will predict the label for a new, unseen data point \(\bm{x}\) sampled from the same law \(P(\bm{x},y)\). Our analysis will be performed in the high-dimensional limit where both the sample size \(n\) and dimensionality \(d\) are sent to infinity, with \(\nicefrac{{n}}{{d}}\equiv\alpha\) kept constant.

Learning taskIn the most general setting, the parameters are estimated by minimising an empirical risk function in the form

\[(\bm{w}^{\star},\bm{b}^{\star})\equiv\arg\min_{\begin{subarray}{c}\bm{w}^{ \star}\in\mathds{R}^{d}\\ b\in\mathds{R}\end{subarray}}\mathcal{R}(\bm{w},\bm{b})\quad\text{where}\quad \mathcal{R}(\bm{w},\bm{b})\equiv\sum_{\nu=1}^{n}\ell\left(y^{\nu},\frac{\bm{w} ^{\top}\bm{x}^{\nu}}{\sqrt{d}}+\bm{b}\right)+\lambda r(\bm{w}).\] (3)

Here \(\ell\) is a strictly convex loss function with respect to its second argument, and \(r\) is a strictly convex regularisation function with the parameter \(\lambda\geq 0\) tuning its strength.

State evolution equations.Let us now present our main result, namely the exact asymptotic characterisation of the distribution of the estimator \(\bm{w}^{\star}\in\mathds{R}^{d}\) and of \(\nicefrac{{1}}{{\sqrt{d}}}\bm{w}^{\star}\bm{X}\in\mathds{R}^{n}\), where \(\bm{X}\in\mathds{R}^{d\times n}\) is the concatenation of the \(n\) dataset column vectors \(\bm{x}^{\nu}\in\mathds{R}^{d}\), \(\nu\in[n]\). Such an asymptotic characterisation is performed via a set of order parameters satisfying a system of self-consistent "state-evolution" equations, which we will solve numerically. The asymptotic expressions for generalisation and training errors, in particular, are written in terms of their solutions, the order parameters. In the following, we use the shorthand \(\ell_{\pm}(u)\equiv\ell(\pm 1,u)\), and given a function \(\Phi\colon\{-1,1\}\to\mathds{R}\), we write \(\Phi_{\pm}\equiv\Phi(\pm 1)\) and \(\mathbb{E}_{\pm}[\Phi_{\pm}]\coloneqq\rho_{+}\Phi_{+}+\rho_{-}\Phi_{-}\).

**Result 2.1**: _Let \(\zeta\sim\mathcal{N}(0,1)\) and \(\Delta\sim\varrho\), both independent from other quantities. Let also be \(\bm{\xi}\sim\mathcal{N}(0,\bm{I}_{d})\). In the setting described above, given \(\phi_{1}\colon\mathds{R}^{d}\to\mathds{R}\) and \(\phi_{2}\colon\mathds{R}^{n}\to\mathds{R}\), the estimator \(\bm{w}^{\star}\) and the vector \(\bm{z}^{\star}\coloneqq\frac{1}{\sqrt{d}}\bm{w}^{\star}\bm{X}\in\mathds{R}^{n}\) verify:_

\[\phi_{1}(\bm{w}^{\star})\xrightarrow[n,d]{\mathrm{P}}\underbrace{\mathbb{E}_{ \xi}\left[\phi_{1}(\bm{g})\right]}_{n,d\to+\infty}\mathbb{E}_{\xi}\left[\phi _{2}(\bm{h})\right],\] (4)

_where we have introduced the proximal \(\bm{g}\in\mathds{R}^{d}\), defined as_

\[\bm{g}\coloneqq\arg\min_{\bm{w}}\left(\hat{v}\frac{\|\bm{w}\|^{2}}{2}-\sqrt{d }\sum_{k=\pm}\hat{m}_{k}\bm{w}^{\top}\bm{\mu}_{k}-\sqrt{d}\hat{\xi}^{\top}\bm {w}+\lambda r(\bm{w})\right)\] (5)

_and where we also have introduced the proximal for the loss \(\bm{h}\in\mathds{R}^{n}\), obtained by concatenating \(\rho_{+}n\) quantities \(h_{+}\) with \(\rho_{-}n\) quantities \(h_{-}\), with \(h_{\pm}\) given by_

\[h_{\pm}\coloneqq\arg\min_{\bm{u}}\left[\frac{(u-\omega_{\pm})^{2}}{2\lambda r}+ \ell_{\pm}(u)\right],\qquad\text{where }\omega_{\pm}\coloneqq m_{\pm}+b+\sqrt{q \Delta}\zeta.\] (6)

_The collection of parameters \((q,m_{\pm},v,\hat{q},\hat{m}_{\pm},\hat{v},b)\) appearing in the equations above is given by the fixed-point solution of the following self-consistent equations:_

\[\begin{cases}m_{\pm}=\frac{1}{\sqrt{d}}\mathbb{E}_{\xi}\left[\bm{g}^{\top}\bm{ \mu}_{\pm}\right],\\ q=\frac{1}{d}\mathbb{E}_{\xi}[\|\bm{g}\|^{2}],\\ v=\frac{1}{d}\hat{q}^{-\nicefrac{{1}}{{2}}}\mathbb{E}_{\xi}[\bm{g}^{\top}\bm{ \xi}],\end{cases}\qquad\begin{cases}\hat{q}=\alpha\mathbb{E}_{\pm,\zeta,\Delta }\left[\Delta f_{\pm}^{2}\right],\\ \hat{v}=-\alpha q^{-\nicefrac{{1}}{{2}}}\mathbb{E}_{\pm,\Delta,\zeta}\left[\sqrt {\Delta}f_{\pm}\zeta\right],\\ \hat{m}_{\pm}=\alpha\rho_{\pm}\mathbb{E}_{\Delta,\zeta}\left[f_{\pm}\right], \end{cases}\qquad\begin{cases}f_{\pm}\coloneqq\frac{h_{\pm}-\omega_{\pm}}{v \Delta},\\ b=\mathbb{E}_{\pm,\Delta,\zeta}\left[h_{\pm}-m_{\pm}\right].\end{cases}\] (7)

[MISSING_PAGE_FAIL:5]

### Finite-covariance case

Let us start by considering a data distribution as in Eq. (11) with shape parameter \(a=c+1>1\), which is therefore given by

\[P(\bm{x}|\bm{\mu})=\tfrac{2^{a}(a-1)^{a}\Gamma(a+d/2)}{\Gamma(a)\,\pi^{d/2}\left( 2a-2\bm{+}\|\bm{x}-\bm{\mu}\|^{2}\right)^{a+d/2}}\] (12)

and decays as \(\|\bm{x}\|^{-2a-1}\) in the radial direction for \(\|\bm{x}-\bm{\mu}\|\gg 0\). As a consequence, the distribution has \(\lim_{a}{}^{\dagger}/d\mathbb{E}\{\|\bm{x}-\bm{\mu}\|^{k}\}=+\infty\) for \(k\geq 2a\). With this choice, _all elements of the family in Eq. (12) have the same covariance \(\bm{\Sigma}=\mathbb{E}[(\bm{x}-\bm{\mu})\otimes(\bm{x}-\bm{\mu})^{\dagger}]= \bm{I}_{d}\) as a is varied_, including the Gaussian limit \(P(\bm{x}|\bm{\mu})\to\mathcal{N}(\bm{\mu},\bm{I}_{d})\) obtained for \(a\to+\infty\).

In Fig. 1 we present the results of our numerical experiments using the square loss and small regularisation. An excellent agreement between the theoretical predictions and the results of numerical experiments is found for a range of values of \(a\) and sample complexity \(\alpha\), both for balanced, i.e., equally sized, and unbalanced clusters of data (the plot for this case can be found in Appendix A.3). The test error \(\epsilon_{g}\) presents the classical interpolation peak at \(\alpha=1\), smoothened by the presence of a non-zero regularisation strength \(\lambda\), and the typical double-descent behavior [54]. As \(a\) is increased, the results of Mignacco et al. [49] for Gaussian clouds with \(P(\bm{x}|\bm{\mu})=\mathcal{N}(\bm{x}|\bm{\mu},\bm{I}_{d})\) are approached. The \(a\to+\infty\) curves correspond, therefore, to the Gaussian mixture model we would have constructed by simply fitting the first and second moments of each class in the finite-\(a\) case. The plot shows that, at given population covariance \(\bm{\Sigma}=\bm{I}_{d}\), the classification of power-law distributed clouds is associated with different, and in particular smaller, errors than the corresponding task on Gaussian clouds with the same covariance: in this sense, no Gaussian errors are systematically smaller for unbalanced clusters than for balanced clusters (see Appendix A.3).

The same numerical experiment was repeated using the logistic loss for training. Once again, in Fig. 2, we focus on \(\rho=\nicefrac{{1}}{{2}}\) and show that the theoretical predictions of the generalisation and training errors agree with the results of numerical experiments for a range of sample complexity values \(\alpha\) and various values of \(a\). Just as for the square loss, the test error \(\epsilon_{g}\) of the Gaussian model (\(a\to+\infty\)) is larger than the one observed for power-law distributed clouds at finite values of \(a\). In the \(\lambda\to 0\) limit, the typical interpolation cusp in the generalisation error is observed: the cusp occurs at different values of \(\alpha\) as \(a\) is changed; interpolation occurs at smaller values of \(\alpha\) for larger \(a\) (i.e., for "more Gaussian" distributions). A comparison with the test error \(\epsilon_{t}\) confirms that this cusp occurs at the value of \(\alpha\) where the training error becomes non-zero and the two training data clouds become non-separable [70; 16]. This sharp transition in \(\alpha\) also corresponds to the existence transition of maximum-likelihood estimator in high-dimensional logistic regression, analysed for Gaussian data in Refs. [70; 49] and, in our setting, in Section 4 below. For larger regularisation strength, the cusp smoothens, and the training error becomes non-zero at smaller values of \(\alpha\).

Figure 1: Test error \(\epsilon_{g}\) (solid line, _top_), training error \(\epsilon_{t}\)_(center)_ and training loss \(\epsilon_{\ell}\)_(bottom)_ as predicted by Eq. (8) in the balanced \(\rho=\nicefrac{{1}}{{2}}\) case. The dataset distribution is parametrised as in Eq. (12). The classification task is solved using a quadratic loss with ridge regularisation with \(\lambda=10^{-5}\). In the top figure, the dashed line corresponds to the Bayes optimal bound. Dots correspond to the average outcome of 50 numerical experiments in dimension \(d=10^{3}\). In our parametrisation, the population covariance is \(\bm{\Sigma}=\bm{I}_{d}\) for all values of \(a\) and moreover, for \(a\to+\infty\), the case of Gaussian clouds with the same centroids and covariance is recovered. For further details on the numerical solutions, see Appendix B.

### Infinite-covariance case

The family of distributions specified by Eq. (11) allows us to also consider the case of infinite covariance, i.e., \(\sigma^{2}=+\infty\). To test our formulas in this setting, we considered the case in which each cloud is obtained by "contaminating" a standard Gaussian with an infinite-covariance distribution as in Eq. (11b) with \(c=1\) and \(a<1\)[35]. In other words, we use the density \(\varrho(\Delta)=r\varrho_{a,1}(\Delta)+(1-r)\delta(\Delta-1)\), with \(r\in[0,1]\) interpolation parameter, for \(\Delta\) in Eq. (1). Each class has therefore infinite covariance for \(0<r\leq 1\) and the Gaussian case is recovered for \(r=0\). Fig. 3 collects our results for two balanced clouds, analysed using square loss and logistic loss. Good agreement between the theoretical predictions and the results of numerical experiments is found for a range of values of sample complexity and of the ratio \(r\). In this case, the finite-variance case \(r=0\) corresponds, not surprisingly, to the lowest test error with both square loss and logistic loss, which grows with \(r\).

### The role of regularisation in the classification of non-Gaussian clouds

One of the main results in the work of Mignacco et al. [49] is related to the effect of regularisation on the performances in a classification task on a Gaussian mixture. They observed that, for all values of the sample complexity \(\alpha\), the optimal ridge classification performance on a pair of balanced clouds is obtained for an infinite regularisation strength \(\lambda\), using both hinge loss and square loss. We tested the robustness of this result beyond the purely Gaussian setting. In Fig. 4 (left), we plot the test error obtained using square loss and different regularisation strengths \(\lambda\) on the dataset distribution as in Eq. (12) with \(a=2\). We observe that, both in the balanced case and in the unbalanced case, the optimal regularisation strength \(\lambda^{\star}\) is _finite_ and, moreover, \(\alpha\)-dependent. On the other hand, if we fix \(\alpha\) and let \(a\) grow towards \(+\infty\), the optimal \(\lambda^{\star}\) grows as well to recover the result of Mignacco et al. [49] of diverging optimal regularisation for _balanced_ Gaussian clouds, as shown in Fig. 4 (center)

Figure 3: Test error \(\epsilon_{g}\) in the classification of two balanced clouds, via quadratic loss (_left_) and logistic loss (_right_). In both cases, ridge regularisation is adopted (\(\lambda=10^{-4}\) for the square loss case, \(\lambda=10^{-3}\) for the logistic loss case). Each cloud is a superposition of a power-law distribution with infinite variance and a Gaussian with covariance \(\boldsymbol{\Sigma}=\boldsymbol{I}_{d}\). The parameter \(r\) allows us to contaminate the purely Gaussian case (\(r=0\)) with an infinite-variance contribution (\(0<r\leq 1\)) as in Eq. (11) with \(c=1\) and \(a=\nicefrac{{1}}{{2}}\) (_left_) or \(a=\nicefrac{{3}}{{4}}\) (_right_). Dots correspond to the average test error of 20 numerical experiments in dimension \(d=10^{3}\). Note that, at a given sample complexity, Gaussian clouds are associated with the lowest test error for both losses.

Figure 2: Test error \(\epsilon_{g}\) (_top_), training error \(\epsilon_{t}\) (_center_) and training loss \(\epsilon_{t}\) (_bottom_) via logistic loss training on balanced clusters parametrised as in Eq. (12) (\(\boldsymbol{\Sigma}=\boldsymbol{I}_{d}\)). Ridge regularisation with \(\lambda=10^{-4}\) is adopted. Dots correspond to the average over 20 numerical experiments with \(d=10^{3}\). The Gaussian limit is recovered for \(a\to+\infty\). Further details on the numerical solutions can be found in Appendix B.

for \(\alpha=2\). This result is represented also in Fig. 4 (right), where it is shown that, in the _unbalanced_ case, the optimal regularisation strength \(\lambda^{\star}\) instead saturates to a finite value for \(a\to+\infty\), i.e., for Gaussian clouds [49].

## 4 The separability threshold

By studying the training error \(\epsilon_{t}\) with logistic loss at zero regularisation we can obtain information on the boundary between the regime in which the training data are perfectly separable and the regime in which they are not. In other words, we can extract the value of the so-called separability transition complexity \(\alpha^{\star}\)[70, 49, 43]. Once again, a complete characterisation of this transition point is available in the Gaussian mixture case, where \(\alpha^{\star}\) can be explicitly given as a function of \(\sigma^{2}\)[49].

It is not trivial to extend this result to the general case of non-Gaussian mixtures. It is, however, possible to derive \(\alpha^{\star}\) within the large family of "superstatistical" models we are considering here under the assumption that Result 2.1 holds. In this case, in Appendix A.3 we prove the following.

**Result 4.1**: _In the considered double-stochastic model, data are linearly separable for \(\alpha<\alpha^{\star}\), where_

\[\alpha^{\star}\coloneqq\max_{\theta\in(0,1],\gamma}\frac{1-\theta^{2}}{S( \theta,\gamma)},\qquad S(\theta,\gamma)\coloneqq\int_{0}^{\infty}f^{2}\mathbb{ E}_{\Delta}\bigg{[}\rho_{+}\gamma\bigg{(}f+\frac{\theta+\gamma}{\sqrt{\Delta}} \big{|}0,1\bigg{)}+\rho_{-}\gamma\bigg{(}f+\frac{\theta-\gamma}{\sqrt{\Delta}} \big{|}0,1\bigg{)}\bigg{]}\mathrm{d}f\,.\] (14)

Fig. 5 shows the values of \(\alpha^{\star}\) for different choices of the shape parameters \(a\) and \(c\) in the case of two balanced clouds of datapoints distributed as in Eq. (11b) as predicted by Eq. (14). In Fig. 5 (left), we fixed \(\sigma^{2}=\frac{c}{a-1}<+\infty\), and plotted the separability threshold \(\alpha^{\star}\) for a range of values of the shape parameter \(a>1\). As \(a\) grows, the known threshold value for the Gaussian mixture case is recovered [49]. The double limit \(a\to\infty\) and \(\sigma^{2}\to\infty\), on the other hand, provides the Cover [12] transition value \(\alpha^{\star}=2\), as expected. In Fig. 5 (right), instead, we analyse the case of infinite-variance clusters, by fixing \(a\in(0,1)\) and by varying the scale parameter \(c\) in Eq. (11b), which controls the spread (width) of the distribution. The Cover transition is therefore correctly recovered as \(c\) diverges for all values of \(a\).

Figure 4: (_Left_) Test error for ridge regularised quadratic loss for various regularisation strengths. The data points of each cloud in the training set are distributed as in Eq. (12), with shape parameter \(a=2\), for balanced clusters (_top_) and unbalanced clusters (\(\rho=\nicefrac{{1}}{{4}}\), _bottom_). Points are the results of 50 numerical experiments, and the dashed lines are Bayes-optimal bounds. (_Center_) Test error for different regularisation strengths \(\lambda\) for two _balanced_ clusters with quadratic loss at sample complexity \(\alpha=2\) using the data distribution (12). The optimal regularisation strength value \(\lambda^{\star}\) obtained from averaging 5 runs for each \(a\) is marked with a cross. (_Right_) Optimal regularisation strength \(\lambda^{\star}\) at \(\alpha=2\) for different values of \(a\in[1.5,10^{2}]\) for both balanced and unbalanced clusters, obtained from averaging 5 runs. Note that, for \(\rho=\nicefrac{{1}}{{2}}\), \(\lambda^{\star}\to+\infty\) as \(a\to+\infty\).

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]

## References

* Adler et al. [1998] R.J. Adler, R.E. Feldman, and M.S. Taqqu, editors. _A Practical Guide to Heavy Tails: Statistical Techniques and Applications_. Birkhauser Boston Inc., USA, 1998. ISBN 0817639519.
* Alspach and Sorenson [1972] D. Alspach and H. Sorenson. Nonlinear Bayesian estimation using Gaussian sum approximations. _IEEE Trans. Autom. Control_, 17(4):439-448, 1972.
* Baldassi et al. [2020] C. Baldassi, E.M. Malatesta, M. Negri, and R. Zecchina. Wide flat minima and optimal generalization in classifying high-dimensional gaussian mixtures. _J. Stat. Mech.: Theory Exp._, 2020(12):124012, 2020.
* Bayati and Montanari [2011] M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. _IEEE Trans. Inf. Theory_, 57(2):764-785, 2011.
* Beck [2003] C. Beck. Superstatistics: Theory and applications. _Contin. Mech. Thermodyn._, 16, 2003.
* Beck [2008] C. Beck. Recent developments in superstatistics. _Braz. J. Phys._, 39:357-363, 2008.
* Beck and Cohen [2003] C. Beck and E.G.D. Cohen. Superstatistics. _Physica A_, 322:267-275, 2003.
* Belkin et al. [2019] M. Belkin, D. Hsu, S. Ma, and S. Mandal. Reconciling modern machine-learning practice and the classical bias-variance trade-off. _Proc. Natl. Acad. Sci. U.S.A._, 116(32):15849-15854, 2019.
* Belkin et al. [2019] M. Belkin, A. Rakhlin, and A.B. Tsybakov. Does data interpolation contradict statistical optimality? In K. Chaudhuri and M. Sugiyama, editors, _Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics_, volume 89 of _Proc. Mach. Learn. Res._, pages 1611-1619. PMLR, 2019.
* Berthier et al. [2020] R. Berthier, A. Montanari, and P.-M. Nguyen. State evolution for approximate message passing with non-separable functions. _Inf. Inference_, 9(1):33-79, 2020.
* Brunel et al. [1992] N. Brunel, J.-P. Nadal, and G. Toulouse. Information capacity of a perceptron. _JJ. Phys. A: Math. Gen._, 25(19):5017, 1992.
* Cover [1965] T.M. Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. _IEEE Trans. Comput._, EC-14(3):326-334, 1965.
* Dandi et al. [2023] Y. Dandi, L. Stephan, F. Krzakala, B. Loureiro, and L. Zdeborova'. Universality laws for gaussian mixtures in generalized linear models. _arXiv:2302.08933_, 2023.
* del Giudice et al. [1989] P. del Giudice, S. Franz, and M. Virasoro. Perceptron beyond the limit of capacity. _J. Physique_, 50:121-134, 1989.
* Delpini and Bormetti [2011] D. Delpini and G. Bormetti. Minimal model of financial stylized facts. _Phys. Rev. E_, 83:041111, 2011.
* Deng et al. [2021] A. Deng, Z.and Kammoun and C. Thrampoulidis. A model of double descent for high-dimensional binary linear classification. _Inf. Inference_, 11, 2021.
* Donoho et al. [2009] D.L. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed sensing. _Proc. Natl. Acad. Sci. U.S.A._, 106(45):18914-18919, Nov 2009.
* El Karoui [2018] N. El Karoui. On the impact of predictor geometry on the performance on high-dimensional ridge-regularized generalized robust regression estimators. _Probab. Theory Relat. Fields_, 170, 2018.
* Franz et al. [1990] S. Franz, D.J. Amit, and M.A. Virasoro. Prosopagnosia in high capacity neural networks storing uncorrelated classes. _J. Physique_, 51(5):387-408, 1990.
* Gardner [1988] E Gardner. The space of interactions in neural network models. _J. Phys. A: Math. Gen._, 21(1):257, 1988.
* Gardner and Derrida [1989] E. Gardner and B. Derrida. Three unfinished works on the optimal storage capacity of networks. _J. Phys. A: Math. Gen._, 22(12):1983, 1989.

* [22] A. Gelman and J.1/ Hill. _Data Analysis Using Regression and Multilevel/Hierarchical Models_. Analytical Methods for Social Research. Cambridge University Press, 2006.
* [23] A. Gelman, J.B. Carlin, H.S. Stern, D.B. Dunson, A. Vehtari, and D.B. Rubin. _Bayesian Data Analysis, Third Edition_. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis, 2013.
* [24] F. Gerace, B. Loureiro, F. Krzakala, M. Mezard, and L. Zdeborova. Generalisation error in learning with random features and the hidden manifold model. In _ICML_, pages 3452-3462, 2020.
* [25] F. Gerace, F. Krzakala, B. Loureiro, L. Stephan, and L. Zdeborova. Gaussian universality of perceptrons with random labels. _arXiv:2205.13303_, 2023.
* [26] C. Gerbelot and R. Berthier. Graph-based approximate message passing iterations. _arXiv preprint arXiv:2109.11905_, 2021.
* [27] C. Gerbelot, A. Abbara, and F. Krzakala. Asymptotic errors for teacher-student convex generalized linear models (or: How to prove kabashima's replica formula). _IEEE Trans. Inf. Theory_, 69 (3):1824-1852, 2023.
* [28] B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. Limitations of lazy training of two-layers neural network. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32, 2019.
* [29] J.K. Ghosh and R.V. Ramamoorthi. _Bayesian Nonparametrics_. Springer Series in Statistics. Springer New York, 2006.
* [30] S. Goldt, M. Mezard, F. Krzakala, and L. Zdeborova. Modeling the influence of data structure on learning in neural networks: The hidden manifold model. _Phys. Rev. X_, 10:041044, 2020.
* [31] Y. Gordon. Some inequalities for gaussian processes and applications. _Isr. J. Math._, 50:265-289, 1985.
* [32] T. Hastie, R. Tibshirani, and J.H. Friedman. _The elements of statistical learning: data mining, inference, and prediction_, volume 2. Springer, 2009.
* 986, 2022.
* [34] H. Hu and Y.M. Lu. Universality laws for high-dimensional learning with random features. _arXiv:2009.07669_, 2022.
* [35] P.J. Huber. Robust estimation of a location parameter. _Ann. Math. Stat._, 35(1):73-101, 1964.
* [36] A. Javanmard and A. Montanari. State evolution for general approximate message passing algorithms, with applications to spatial coupling. _Inf. Inference_, 2(2):115-144, 2013.
* [37] G.R. Kini and C. Thrampoulidis. Phase transitions for one-vs-one and one-vs-all linear separability in multiclass gaussian mixtures. In _ICASSP 2021_, pages 4020-4024, 2021.
* [38] V. Kothapalli, E. Rasromani, and V. Awatramani. Neural collapse: A review on modelling principles and generalization. _arXiv:2206.04041_, 2022.
* [39] W. Krauth and M. Mezard. Storage capacity of memory networks with binary couplings. _J. Physique_, 50(20):3057-3066, 1989.
* [40] N. Langrene, G. Lee, and Z. Zhu. Switching to non-affine stochastic volatility: A closed-form expansion for the inverse gamma model. _Int. J. Theor. Appl. Finance_, 19, 2015.
* [41] M. Lelarge and L. Miolane. Asymptotic bayes risk for gaussian mixture in a semi-supervised setting. In _2019 IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing_, pages 639-643, 2019.

* [42] M. Lelarge and L. Miolane. Asymptotic bayes risk for gaussian mixture in a semi-supervised setting. In _2019 IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)_, pages 639-643, 2019.
* [43] B. Loureiro, G. Sicuro, C. Gerbelot, A. Pacco, F. Krzakala, and L. Zdeborova. Learning Gaussian Mixtures with Generalized Linear Models: Precise Asymptotics in High-dimensions. _Advances in Neural Information Processing Systems_, 34:10144-10157, 2021.
* [44] B. Loureiro, C. Gerbelot, H. Cui, S. Goldt, F. Krzakala, M. Mezard, and L. Zdeborova. Learning curves of generic features maps for realistic datasets with a teacher-student model. _J. Stat. Mech.: Theory Exp._, 2022(11):114001, 2022.
* [45] H. Maennel, I.M. Alabdulmohsin, I.O. Tolstikhin, R. Baldock, O. Bousquet, S. Gelly, and D. Keysers. What do neural networks learn when trained with random labels? In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 19693-19704. Curran Associates, Inc., 2020.
* [46] X. Mai and Z. Liao. High dimensional classification via regularized and unregularized empirical risk minimization: Precise error and optimal loss. _arXiv:1905.13742_, 2020.
* [47] S. Mei and A. Montanari. The Generalization Error of Random Features Regression: Precise Asymptotics and the Double Descent Curve. _Commun. Pure Appl. Math._, 75, 2019.
* [48] M. Mezard, G. Parisi, and M.A. Virasoro. _Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications_, volume 9. World Scientific Publishing Company, 1987.
* [49] F. Mignacco, F. Krzakala, Y. Lu, P. Urbani, and L. Zdeborova. The role of regularization in classification of high-dimensional noisy Gaussian mixture. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proc. Mach. Learn. Res._, pages 6874-6883. PMLR, 2020.
* [50] A. Montanari and P.-M. Nguyen. Universality of the elastic net error. In _2017 IEEE ISIT_, pages 2338-2342. IEEE, 2017.
* [51] A. Montanari and B.N. Saeed. Universality of empirical risk minimization. In Po-Ling Loh and Maxim Raginsky, editors, _Proceedings of 35th Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 4310-4312. PMLR, 2022.
* [52] D.B. Nelson. Arch models as diffusion approximations. _J. Econom._, 45(1-2):7-38, 1990.
* [53] V. Nestoridis, S. Schmutzhard, and V. Stefanopoulos. Universal series induced by approximate identities and some relevant applications. _J. Approx. Theory_, 163(12):1783-1797, 2011.
* [54] M. Opper, W. Kinzel, J. Kleinz, and R. Nehl. On the ability of the optimal perceptron to generalise. _J. Phys. A: Math. Gen._, 23(11):L581, 1990.
* [55] A. Panahi and B. Hassibi. A universal analysis of large-scale regularized least squares solutions. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [56] V. Papyan, X.Y. Han, and D.L. Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. _Proc. Natl. Acad. Sci. U.S.A._, 117(40):24652-24663, 2020.
* [57] N. Parikh and S. Boyd. Proximal algorithms. _Found. Trends Optim._, 1(3):127-239, 2014.
* [58] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _J. Mach. Learn. Res._, 12:2825-2830, 2011.
* [59] L. Pesce, F. Krzakala, B. Loureiro, and L. Stephan. Are Gaussian data all you need? Extents and limits of universality in high-dimensional generalized linear estimation. _arXiv:2302.08923_, 2023.

* [60] M.A. Pinsky and S. Karlin. _An Introduction to Stochastic Modeling: Fourth Edition_, pages 1-563. Elsevier Inc, December 2010. ISBN 9780123814166.
* [61] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, _Advances in Neural Information Processing Systems_, volume 20. Curran Associates, Inc., 2007.
* [62] W.J. Reed and B.D. Hughes. From gene families and genera to incomes and internet file sizes: Why power laws are so common in nature. _Phys. Rev. E_, 66:067103, 2002.
* [63] H.E. Robbins. Asymptotically subminimax solutions of compound statistical decision problems. 1985.
* [64] S. Rosset, J. Zhu, and T. Hastie. Margin maximizing loss functions. In S. Thrun, L. Saul, and B. Scholkopf, editors, _Advances in Neural Information Processing Systems_, volume 16. MIT Press, 2003.
* [65] P.E. Rossi. _Bayesian Non- and Semi-parametric Methods and Applications_. Princeton University Press, 2014.
* [66] D. Schnoerr, R. Grima, and G. Sanguinetti. Cox process representation and inference for stochastic reaction-diffusion processes. _Nat. Comm._, 7:11729, 05 2016.
* [67] M.E.A. Seddik, C. Louuart, M. Tamaazousti, and R. Couillet. Random matrix theory proves that deep learning representations of GAN-data behave as Gaussian mixtures. In H.D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proc. Mach. Learn. Res._, pages 8573-8582. PMLR, 2020.
* [68] H. S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of learning from examples. _Phys. Rev. A_, 45:6056-6091, 1992.
* 311, 2018.
* [70] P. Sur and E.J. Candes. A modern maximum-likelihood theory for high-dimensional logistic regression. _Proc. Natl. Acad. Sci. U.S.A._, 116(29):14516-14525, 2019.
* [71] C. Thrampoulidis, S. Oymak, and B. Hassibi. Regularized linear regression: A precise analysis of the estimation error. In Peter Grunwald, Elad Hazan, and Satyen Kale, editors, _Proceedings of The 28th Conference on Learning Theory_, volume 40 of _Proc. Mach. Learn. Res._, pages 1683-1709, Paris, France, 2015. PMLR.
* [72] C. Thrampoulidis, E. Abbasi, and B. Hassibi. Precise error analysis of regularized \(m\) -estimators in high dimensions. _IEEE Trans. Inf. Theory_, 64(8):5592-5628, 2018.
* [73] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. _Nature Methods_, 17:261-272, 2020. doi: 10.1038/s41592-019-0686-2.
* [74] T.L.H. Watkin, A. Rau, and M. Biehl. The statistical mechanics of learning a rule. _Rev. Mod. Phys._, 65:499-556, 1993.
* [75] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires rethinking generalization. _Commun. ACM_, 64(3):107-115, 2021.
* 1861, 2022.