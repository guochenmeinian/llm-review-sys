**Linguistic Collapse:**

**Neural Collapse in (Large) Language Models**

**Robert Wu**

University of Toronto, Vector Institute

rupert@cs.toronto.edu

**Vardan Papyan**

University of Toronto, Vector Institute

vardan.papyan@utoronto.ca

###### Abstract

Neural collapse (\(\mathcal{NC}\)) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers. These behaviours -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension. Recent studies have explored \(\mathcal{NC}\) in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries. Language modelling presents a curious frontier, as _training by token prediction_ constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs. This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards \(\mathcal{NC}\). We find that \(\mathcal{NC}\) properties that develop with scale (and regularization) are linked to generalization. Moreover, there is evidence of some relationship between \(\mathcal{NC}\) and generalization independent of scale. Our work thereby underscores the generality of \(\mathcal{NC}\) as it extends to the novel and more challenging setting of language modelling. Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on \(\mathcal{NC}\)-related properties. Our code is hosted on GitHub: https://github.com/rhubarbuu/linguistic-collapse.

Figure 1: Simultaneous development of the four _neural collapse_ (\(\mathcal{NC}\)) [1] properties in 230 causal language models trained on TinyStories [2], alongside improvement in generalization (i.e. validation performance). Left to right: \(\mathcal{NC}_{1}\)) within-class (representation) variability collapse; \(\mathcal{GNC}_{2}\)) hyper-spherical uniformity of class means; \(\mathcal{UNC}_{3}\)) uniform duality between class means and corresponding classifiers; and \(\mathcal{NC}_{4}\)) agreement between token (maximum a prior) classifiers and implicit nearest-class centre classifiers. Coloured by model size and annotated with coefficient of determination (\(R^{2}\)).

Introduction

### Neural Collapse

A learning phenomenon known as _neural collapse_ (\(\mathcal{NC}\)) emerges during the terminal phase of training (TPT) deep neural networks with cross-entropy (CE) loss for classification tasks.[1] It was originally characterized as the co-occurrence of the following properties in a model's top-layer (also known as last-layer) representations (also known as features or embeddings) and linear classifier weights:

* **Within-class variability collapse:** Top-layer representations collapse to their class means.
* **Convergence to a simplex ETF:** Class means tend towards equinorm and equiangular vectors when centred about the global average. The resulting geometry -- known as a simplex _equiangular tight frame_ (ETF) -- maximizes pairwise angles and distances.
* **Convergence to self-duality:** Linear classifier weight vectors converge to their corresponding top-layer class mean vectors, and thus also form a simplex ETF.
* **Nearest decision rule:** Linear classifiers approximate a nearest-class centre (NCC) classifier: top-layer representations predict the class with the closest mean (implied by \(\mathcal{NC}\)1-3).

These behaviours, often associated with improved generalization and robustness [3, 4, 5] (among other benefits, such as those discussed in SS1.4), traditionally manifest under the following conditions:

* **Few classes:** The number of classes is upper-bounded by the embedding dimension plus one: \(C\leq d+1\); this is required to construct a perfect simplex ETF.
* **Balanced classes:** The number of samples is equal across classes: \(N_{c}=N_{c^{\prime}},\forall c\neq c^{\prime}\).
* **Noise-free labels:** Identical (or very similar) embeddings should belong to the same class.
* **Sufficient training (TPT):** The model is trained past zero error, towards zero loss.

Absent these conditions, one does not typically anticipate \(\mathcal{NC}\). Since then, follow-up studies have extended beyond and proposed techniques of quantifying or achieving \(\mathcal{NC}\) (discussed in Section 2).

### (Large) Language Models

\(\mathcal{NC}\) is a phenomenon observed specifically in classification tasks. While not traditionally thought of as classifiers, language models -- including large language models (LLMs) -- learn to model aleatoric uncertainty, which can be viewed as stochastic token prediction [6]. For instance, masked language models (MLMs) such as BERT [7] predict one or several masked tokens within an input sequence based on the surrounding _context_. Likewise, autoregressive or causal language models (CLMs) such as GPT [8] perform next-token prediction (NTP) in a sequence given the _context_ of previous tokens. Most of these models are essentially _(pre-)trained_ by token classification on their vocabularies. This parallel -- also drawn by [9] -- raises a few natural questions:

1. Does the pre-training stage of a language model exhibit \(\mathcal{NC}\)?
2. How do scaling and other training configurations influence \(\mathcal{NC}\) in (L)LMs?
3. To what extent is \(\mathcal{NC}\) in (L)LMs correlated with their generalization abilities?
4. Do such correlations, between \(\mathcal{NC}\) and improved generalization, persist independent of the (potential confounders of) model size and training?

To address these, we first examine the specific settings of training CLMs as they are opposed (\(\neg\)) to the traditional prerequisites (R1-4, SS1.1) for \(\mathcal{NC}\) to manifest.

* **Many classes:** The unique tokens found in language modelling vocabularies are vast, usually numbering in the tens of thousands and far exceeding the hidden dimension [10].
* **Imbalanced classes:** The distribution of tokens in natural language is typically very imbalanced [11, 12], as is the case in TinyStories [2], the dataset we use (Appendix Figure 4). It has an average of 16K samples per class but a standard deviation of over 32K.

* **Ambiguous contexts:** There may exist very similar or even identical contexts followed by different tokens in the natural language data [13]. For instance, over half of the sequences in TinyStories [2] lead with "Once upon a time", but only three-quarters of these follow with a comma ("Once upon a time,"). In other words, there is almost certainly some _ambiguity_ to contend with in our context embeddings.
* **Dudertraining:** Most practical language models (including LLMs) are not trained for more than a few epochs [14; 15]. Further optimization typically renders diminishing returns in improving evaluation performance [16] long before any possible TPT.

### Contributions

We train a suite of Transformer-based [17] CLMs1 across a grid of model widths, depths, and training epochs on the TinyStories dataset [2] to assess the degrees to which \(\mathcal{NC}\) properties develop and how they relate to generalization performance. Our findings (summarized in Figure 1) reveal:

Footnote 1: Our models range from 3.4M (small) to 205M (large), so we inclusively use “CLM” instead of “LLM”.

* **Emergence of \(\mathcal{NC}\) with scale:** As we scale model size and training, the properties of \(\mathcal{NC}\) emerge; within-class variability (\(\mathcal{NC}_{1}\)) and interference are reduced while hyperpsherical uniformity (\(\mathcal{GNC}_{2}\)), uniform duality (\(\mathcal{UNC}_{3}\)), and classifier agreement (\(\mathcal{NC}_{4}\)) improve.
* **Progression towards hyperspherical uniformity:** Class means, while unable to form a simplex ETF (\(\mathcal{NC}_{2}\)), nonetheless tend towards uniform dispersion on a hypersphere, a geometry theorized by [18] and formalized by [19] as _hyperspherical uniformity_ (\(\mathcal{GNC}2\)).
* which we term _uniform duality_ (\(\mathcal{UNC}_{3}\))
- may be more cohesive with \(\mathcal{NC}\).
* **Correlation between \(\mathcal{NC}\) and generalization:** The developments of \(\mathcal{NC}\) properties are correlated with improved validation performance. We show these correlations to persist even when fixing the (potential confounders of) model architecture and training by simply varying the random seeds for initialization and data shuffling. This suggests that \(\mathcal{NC}\) is not simply a side-effect of training, but possibly a factor of generalization in its own right.

### Significance

Recently, methods building on \(\mathcal{NC}\) have found use in deep learning at large. We highlight areas such as federated learning [20], graph neural networks [21], incremental/continual learning [22; 23; 24], meta-learning [24; 25], out-of-distribution detection [26; 27; 28], privacy [29; 30], learning theory [31; 32; 33; 34; 35; 36] transfer learning [37; 3; 3; 38; 39; 40], and even LLM prompting [41]. With our results, we aim to extend insights from such contributions and other related works to the autoregressive language modelling domain and ultimately assist researchers in improving and interpreting their (L)LMs.

## 2 Related Works

\(\mathcal{NC}\) was initially observed in image classification tasks such as on CIFAR-10/100 [42] and ImageNet [43]. Since then, the phenomenon has been further studied both theoretically and empirically [5; 18; 44; 45; 46; 5; 47; 48; 5; 49; 60; 5; 44; 46; 5; 48; 5; 5; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 61; 62; 63; 64; 65; 66], with several works venturing into settings without some of the traditional prerequisites (~Rq1-4, SS1.2) and proposing adaptations of the analysis framework or optimization procedures:

A large number of classes (~Rq1)\(\mathcal{NC}\) established the simplex ETF as an optimal configuration. However, a perfect simplex ETF (\(\mathcal{NC}_{2}\)) requires that the number of classes \(C\) not exceed \(d+1\) where \(d\) is the embedding dimension. This requirement that \(d\) be sufficiently large is impractical when the classes number beyond the thousands.2 For instance, GPT-2 [68] and Llama 3.1 [69] have vocabularies of around 50K and 128K tokens, respectively.

Footnote 2: Following [67], one might describe such a setting (\(C>d+1\)) as a model “in superposition”.

In such a scenario, one might still expect class means to be uniformly distributed on a \(d\)-dimensional hypersphere [18]. [19] formalize this as _hyperspherical uniformity_ within a _generalized neural _collapse_ (\(\mathcal{ANC}\)) framework, which [9] then empirically demonstrate. These latter two works mention language modelling as applicable for \(\mathcal{GNC}\); [9] even framed token prediction as a classification task, just as we do. We remark however that both earlier works _simulate_ a large number of classes by drastically shrinking the embedding dimension. In contrast, we study realistic NTP, using the full class space (vocabulary) with an imbalanced token distribution and ambiguous samples.

Class imbalance (-Rq2)\(\mathcal{NC}\) traditionally assumed that classes were sample-balanced. Since then, follow-up works have investigated the effect of class imbalance on the prevalence of \(\mathcal{NC}\). [47] studied the _layer-peeled model_ (LPM) and discovered that _minority collapse_ occurs when classes are imbalanced across two equally-sized groups of classes; a threshold for minority collapse was later characterized by [35]. [52] showed that \(\mathcal{NC}\) still occurs in such an LPM when the classifier is initialized as a fixed ETF. [70] introduced _simplex-encoded-label interpolation_ (SELI) but noted that more severe imbalance worsens even this geometry. Recently, feature regularization has been employed to induce \(\mathcal{NC}\) and improve generalization in class-imbalanced settings [58; 61; 62].

Multi-label classification (-Rq3)In some problems, one might encounter mixed or multi-label samples, be they natural or due to noise or augmentation [71; 72]. \(\mathcal{NC}\) was also recently studied for such data by [73], who observed that multi-label class means arrive at the average of their labels' respective means. They also devise an augmented CE loss function to accommodate such samples.

Likewise, most of our ambiguous samples could be considered multi- or soft-label: identical (or very similar) context samples with different hard token labels (-Rq3). Under popular CLM pre-training (teacher-forcing with CE loss), this effectively precludes the prospect of achieving zero classification error and potentially introduces irreducible noise.

A recent study showed that memorization of noisy labels likely leads to degradation of \(\mathcal{NC}\)[60].

Early stages of training (-Rq4)[53] studied \(\mathcal{NC}\) in small ResNet [74] models that had not yet converged (similar to most LLMs). They show that within-class variability drops and "plateaus" (\(\mathcal{NC}_{1}\)) earlier than other \(\mathcal{NC}\) metrics, a result that we also observe (SS4.1, Figures 6, 7).

Natural language processing (NLP)An earlier study reported that the ratio of within-class to between-class covariances of word embeddings increases with model depth [75; 76], seemingly at odds with \(\mathcal{NC}_{1}\). It can, however, be distinguished from literature studying layer-wise \(\mathcal{NC}\) in that it does not centre the mean representation vectors (i.e. subtract the global mean).

[19] fine-tuned BERT [7] using their _hyperspherical uniformity gap_ (HUG) objective on binary classification tasks from the GLUE benchmark [77]. [78] conducted a tangentially-related investigation of convolutional neural networks for few-class semi-supervised clustering in which they identify \(\mathcal{NC}\). Our work is distinct from these in several ways: a) our class space far exceeds our embedding dimension (\(C\gg d+1\)) because we classify on the full token vocabulary; b) we analyze embeddings at a token-level granularity rather than at the sequence level; and c) our NTP task is causal (context-dependent) as opposed to the per-sample independence of their few-category classification.

A more related work studied _feature collapse_ in individual word representations [79], but the authors note that their analysis is limited to shallow NLP modelling on more rigid and tabular text data.

## 3 Methodology

Below we describe the training setup for our CLMs (SS3.1), procedures3 for collecting top-layer embeddings (SS3.2, 3.7), and measurements of \(\mathcal{NC}\) and generalization (SS3.4, 3.5, 3.6, 3.7, 3.8).

Footnote 3: Leveraging our generic \(\mathcal{NC}\) package: https://github.com/rhubarbwu/neural-collapse.

### Dataset and Language Models

TinyStories [2] is a synthetic4 dataset generated by GPT-3.5 and GPT-4 using around 1500 English words a child might use. NTP is performed by sampling from the token vocabulary \(\mathbb{V}=\llbracket 1,29233\rrbracket\)which for our purposes can therefore be framed as classification among \(C=29,\!233\) classes.5 Following the GPT-style preprocessing regime [8], raw sequences are packed into \(S\) chunks of size \(T\), providing \(N=S(T-1)\) token samples for training.6 Details are listed in Appendix A.

Footnote 5: Although the GPT-Neo [80] tokenizer has over 50K tokens, only a subset vocabulary appears in TinyStories.

Footnote 6: We cannot use the first ground-truth nor the last predicted token in any chunk.

We use 30 CLM architectures based on GPT Neo [80], configured similarly to [2]. They vary in width (embedding dimension) \(d\in\{64,128,256,512,768,1024\}\) and depth (number of self-attention layers) \(L\in\{1,2,4,8,12\}\). Our models were trained by teacher-forcing7 using CE loss. For each architecture, we trained multiple models for 1, 3, and 10 epochs ablated over weight decay factors \(\beta=0.0005\)[51] and \(\beta=0.1\)[81]. Further details are listed in Appendices B, C.

Footnote 7: Parallelized training using sequences’ ground-truth labels for context as opposed to predicted tokens.

### Context Embeddings

Base CLMs perform next-token prediction: given a sequence of tokens \(\bm{x}_{1:t}\in\mathbb{V}^{t}\), a top-layer context embedding \(\bm{h}(\bm{x}_{1:t})\in\mathbb{R}^{d}\) is used to predict the next token \(x^{\prime}_{t+1}\in\mathbb{V}\) where \(1\leq t\leq T\). A classifier for class \(c\) with weights \(\bm{w}_{c}\) and bias8\(b_{c}\) would make maximum a prior (MAP) predictions as

Footnote 8: Similar to many causal LLMs [80, 81, 82, 83, 84], our classifiers do not include additive biases, so \(b_{c}=0\).

\[x^{\prime}_{t+1}:=\operatorname*{argmax}_{c\in\mathbb{V}}\left\langle\bm{w}_{ c},\bm{h}(\bm{x}_{1:t})\right\rangle+b_{c}.\] (1)

Class embedding meansFor each token class \(c\), we are interested in accumulating the mean embedding \(\bm{\mu}_{c}\in\mathbb{R}^{d}\) across sequences \(s\) and their contexts \(\bm{x}^{(s)}_{1:t}\), where the next token \(x^{(s)}_{t+1}\) is ground-truth (\(t<T\)) and equal to \(c\):

\[\bm{\mu}_{c}:=\frac{1}{N_{c}}\sum_{s=1}^{S}\sum_{t=1}^{T-1}\bm{h}\left(\bm{x} ^{(s)}_{1:t}\right)\mathbb{I}\left(x^{(s)}_{t+1}=c\right),\] (2)

where \(N_{c}\) is the number of samples of class \(c\) and \(\mathbb{I}\) is the (binary) indicator function. We also utilize their unweighted9 average \(\bar{\bm{\mu}}:=\mathbb{E}_{c}[\bm{\mu}_{c}]\), and subsequently the globally-centred means \(\hat{\bm{\mu}}_{c}=\frac{\bm{\mu}_{c}-\bar{\bm{\mu}}}{\|\bm{\mu}_{c}-\bar{\bm {\mu}}\|_{2}}\).

Footnote 9: Different from most literature where classes were balanced and weighting is already equal.

Class embedding variancesIn a second pass, we accumulate the sample variance norms:10

Footnote 10: This sample variance is computed across all unnormalized dimensional entries.

\[\sigma_{c}^{2}:=\frac{1}{N_{c}}\sum_{s=1}^{S}\sum_{t=1}^{T-1}\left\|\bm{h} \left(\bm{x}^{(s)}_{1:t}\right)-\bm{\mu}_{c}\right\|_{2}^{2}\mathbb{I}\left(x ^{(s)}_{t+1}=c\right).\] (3)

### Homogeneity and Variability

For some collapse measures (such as \((\mathcal{G})\mathcal{NC}_{2}\) and \(\mathcal{NC}_{3}\)), we are primarily interested in the _variation_ rather than the average of pairwise relationships. To that end, we also include in our analysis the _coefficient of variation_ (CoV) of several measurements, which is the ratio of their standard deviations to their means: \(\sigma(\cdot)/\mu(\cdot)\). We can interpret this as a normalized measure of variability.

### Signal-to-Noise Ratio -- \(\mathcal{NC}1\)

The ability to disambiguate between classes depends on the ratio of within-class to between-class variabilities. Building upon foundual works [85, 86], \(\mathcal{NC}\) originally measured variability through an inverse _signal-to-no ratio_ (SNR), whose minimization constitutes _within-class variability collapse_ (\(\mathcal{NC}_{1}\)). We instead employ a _class-distance normalized variance_ (CDNV) similar to [3]:

\[\hat{\sigma}_{c,c^{\prime}}:=\frac{1}{\|\bm{\mu}_{c}-\bm{\mu}_{c^{\prime}}\|_ {2}^{k}}\cdot\frac{\sigma_{c}^{2}+\sigma_{c^{\prime}}^{2}}{2\|\bm{\mu}_{c}-\bm {\mu}_{c^{\prime}}\|_{2}^{2}},\quad\forall c\neq c^{\prime}.\] (4)

Our metric differs in that we divide by an additional power \(\|\bm{\mu}_{c}-\bm{\mu}_{c^{\prime}}\|_{2}^{k}\) of the mean distance norm. This further downweights the CDNV within well-separated class pairs in favour of emphasizing more mutually noisy pairs. We found this augmented CDNV with \(k=2\) to be especially useful in our setting of many imbalanced and variably confusable token classes.

These pairwise measures constitute the off-diagonal11 entries of a symmetric matrix in \(\mathbb{R}^{C\times C}\), whose average we use as an inverse SNR. Within-class variability collapse is then re-characterized by the minimization of this quantity: \(\hat{\sigma}_{c,c^{\prime}}\to 0,\forall c\neq c^{\prime}\). This alternative convergence is empirically faithful to \(\mathcal{NC}_{1}\) but more robust and numerically stable [3].

Footnote 11: The main diagonal of CDNV is undefined (due to the zero denominator) and ignored.

### Geometric Structures -- \((\mathcal{G})\mathcal{NC}2\)

The separability of our representations also depends on the geometric structures found in our embeddings. [1] characterize \(\mathcal{NC}_{2}\) as convergence to a _simplex equiangular tight frame_ (ETF) [87; 88].

EquinormnessSuch a near-orthonormal configuration firstly implies that class means are equinorm:

\[\log\|\boldsymbol{\mu}_{c}-\bar{\boldsymbol{\mu}}\|_{2}-\log\|\boldsymbol{\mu} _{c^{\prime}}-\bar{\boldsymbol{\mu}}\|_{2}\to 0,\quad\forall c\neq c^{\prime}.\] (5)

We measure CoV in the _logarithms_ of class mean norms to assess the degree of "equinormness".

Equangularity\(\mathcal{NC}_{2}\) also entails that class means are equiangular about their centre \(\bar{\boldsymbol{\mu}}\): pairwise distances and angles between their class means should be maximized and similar. Following [1], we measure _interference_ (sometimes known as similarity or _coherence_[89; 90]). Its minimization,

\[\langle\boldsymbol{\hat{\mu}}_{c},\boldsymbol{\hat{\mu}}_{c^{\prime}}\rangle \rightarrow\frac{-1}{C-1},\quad\forall c\neq c^{\prime},\] (6)

together with equinormness (Equation 5) constitute convergence to a simplex ETF. Although this geometry is not ultimately attainable since there are too many classes (\(C>d+1\)), it can still be meaningful to measure a model's tendency towards one. As with CDNV noise (Equation 4), pairwise interferences form off-diagonal12 entries in a symmetric matrix in \(\mathbb{R}^{C\times C}\). The minimization of CoV in interferences therefore expresses the degree of "equinangularity".

Footnote 12: The main diagonal of conferences is undefined (due to the zero denominator) and ignored.

Hyperspherical uniformityA relaxation from the ETF is _hyperspherical uniformity_ (\(\mathcal{ANC}_{2}\)), with equinorm (Eq. 5) means \(\boldsymbol{\mu}_{c}\) uniformly distributed on the \(d\)-dimensional hypersphere [18; 19]. We likewise gauge the angular uniformity with pairwise interactions through some distance kernel \(K\):13

Footnote 13: Following [19], we employ the logarithmic inverse distance kernel \(K_{\log}(\boldsymbol{a},\boldsymbol{b})=\log\|\boldsymbol{a}-\boldsymbol{b} \|_{2}^{-1}\) for its ability to emphasize gaps between small distances while scaling down the effect of larger distances.

\[\sum_{c\neq c^{\prime}}K\left(\boldsymbol{\hat{\mu}}_{c},\boldsymbol{\hat{ \mu}}_{c^{\prime}}\right)\rightarrow\min_{\boldsymbol{\hat{\mu}}_{1},\ldots, \boldsymbol{\hat{\mu}}_{C}}\sum_{c\neq c^{\prime}}K\left(\boldsymbol{\hat{ \mu}}_{c},\boldsymbol{\hat{\mu}}_{c^{\prime}}\right).\] (7)

### Alignment Between Classifiers and Class Embedding Means -- \((\mathcal{U})\mathcal{NC}3\)

The linear classifiers \(\{\boldsymbol{w}_{c}\}_{c=1}^{C}\) lie in a dual vector space to that of the class means \(\{\boldsymbol{\mu}_{c}\}_{c=1}^{C}\). While convergence to self-duality (\(\mathcal{NC}_{3}\)) was initially measured as distances [1] between class means and classifiers (Equation 11), we follow [5] to inspect class-wise cosine similarities:14

Footnote 14: Dot-product would be confounded by norms and therefore inappropriate for similarity up to rescaling.

\[\left\langle\frac{\boldsymbol{w}_{c}}{\|\boldsymbol{w}_{c}\|_{2}}, \boldsymbol{\hat{\mu}}_{c}\right\rangle\to 1,\quad\forall c\in\mathbb{V}.\] (8)

For intuition analogous to that for equinormness and equiangularity (SS3.5), we also measure _uniform duality_ (\(\mathcal{UNC}_{3}\)) as the minimization of the CoV of these similarities (Appendices N, O).

### Agreement of the Classifiers -- \(\mathcal{NC}4\)

Finally, \(\mathcal{NC}_{4}\) is described as the simplification (or approximation) of the linear classifier's MAP prediction behaviour (Equation 1) to that of the implicit _nearest-class centre_ (NCC) classifier:

\[\operatorname*{argmax}_{c\in\mathbb{V}}\left\langle\boldsymbol{w}_{c}, \boldsymbol{h}\right\rangle+b_{c}\rightarrow\operatorname*{argmin}_{c\in \mathbb{V}}\|\boldsymbol{h}-\boldsymbol{\mu}_{c}\|_{2},\quad\forall \boldsymbol{h}.\] (9)We calculate15 agreement as the proportion of validation samples on which the classifiers agree: 16

Footnote 15: In practice, we use an equivalent decomposition (Eq. 12).

Footnote 16: Note that agreement is not equivalent to accuracy.

\[\frac{1}{N_{\text{val}}}\sum_{s=1}^{S_{\text{val}}}\sum_{t=1}^{T_{\text{val}}-1 }\mathbb{I}\left({x^{\prime}_{t+1}}^{(s)}=\operatorname*{argmin}_{c\in\mathcal{ V}}\left\|\boldsymbol{h}\left(x^{(s)}_{1:t}\right)-\boldsymbol{\mu}_{c}\right\|_{2} \right).\] (10)

### Probing a Relationship Between \(\mathcal{NC}\) and Generalization

To isolate the effect of \(\mathcal{NC}\) on generalization independent of model scaling and training (if it exists), we selected a two-layer \(768\)-wide architecture of which to train twenty more instances with weight decay \(\beta=0.0005\), each with a different data shuffling seed. We then followed the remainder of the pipeline described above to collect and analyze embeddings with respect to \(\mathcal{NC}\). Finally, we performed a permutation test with \(10^{4}\) trials to determine the statistical significance of any correlation between \(\mathcal{NC}\) and generalization that remains when we hold constant all factors but shuffling seeds.

## 4 Experimental Results

In this section, we present the results from our empirical study on scaling and generalization:

* Within-class variability is reduced across model scale (more so by width than depth) and training (up to 6 epochs), and is tightly correlated with validation performance.
* Equinormness/equiangularity shows some improvement with scale, training, and performance. Hyperspherical uniformity (\(\mathcal{GNC}_{2}\)) also improves but more clearly and consistently.
* Our models fail to achieve self-duality: class means do not align with classifiers. However, uniform duality (\(\mathcal{UNC}3\)) is correlated with model width, training, and performance.
* Larger or more trained models exhibit closer agreement between their linear and implicit NCC classifiers. Agreement is also associated with validation performance.

### Within-Class Variability Collapse -- \(\mathcal{NC}1\)

Scaling our models dramatically reduces normalized variance, which is further aided by more training epochs and stronger weight decay (Appendix Figs. 6, 7). These noise reductions tightly associate with generalization (Fig. 1, left, "\(\mathcal{NC}_{1}\)"). The relationship is most apparent at scale.

### Geometric Structures -- \((\mathcal{G})\mathcal{NC}2\)

EquinormnessLogarithmic class mean norms grow with model width and training (Appendix Fig. 8), and subtly with depth (Appendix Fig. 9). Meanwhile, the variation of these (logarithmic) norms consistently decreases (improving equinormness) with scale (Appendix Figs. 10, 11). Both trends correlate with improved generalization (Appendix Fig. 20).

Figure 2: Validation loss is correlated with all three measurements: **(left)** equinormness (\(\mathcal{NC}2\)) expressed as variation in logarithmic norms; **(centre)** equiangularity (\(\mathcal{NC}2\)) as variation in interference; **(right)** hyperspherical uniformity (\(\mathcal{GNC}2\)) as variation in logarithmic pairwise distances.

EquiangularityScaling model dimensions reduces average interference (Appendix Figs. 12, 13) down to an empirical plateau of approximately \(10^{-3}\), which is more apparent in less-trained models. However, the variation of interference rises and persists when scaling (Appendix Figs. 14, 15), suggesting that interference becomes more concentrated between some pairs of classes. These results could be due to various factors, including but not limited to unfriendly conditions of language modelling (SS1.2) or the impossibility of a perfect simplex ETF (SS3.5).

Appendix Figure 21 shows only a rough performance correlation with average interference (i.e. coherence) and a more definite -- albeit still noisy -- one with the variation of interference (i.e. equiangularity). In other words, the limited trends we observed toward a simplex ETF suggest that the association of \(\mathcal{NC}2\) with generalization begins to break down when \(C>d+1\).

Hyperspherical uniformityLogarithmic distances drop more gradually and consistently with scale (Appendix Figs. 16, 17), implying this quantity is more robust or may not face the same geometric barriers seen in conventional interference (Appendix Figs. 14, 15). Variation of these logarithmic distances is also consistently reduced with scale (Appendix Fig. 18, 19).

And finally, generalization has much stronger correlations with logarithmic distances than it has with regular interference (Fig. 2), validating the applicability of \(\mathcal{GNC}\)[19] when \(C>d+1\).

### Classifier (Mis)alignment and Duality -- \((\mathcal{U})\mathcal{NC}3\)

Model width (\(d\)) is correlated faintly with the average similarity between class means and their respective classifiers (Appendix Fig. 23), but strongly with variation in similarity (Appendix Fig. 25). The relationships to generalization follow the same pattern (Fig. 3), suggesting that uniform duality (\(\mathcal{UNC}_{3}\)) _might_ serve as a better \(\mathcal{NC}\) property than self-duality (\(\mathcal{NC}_{3}\)) overall; we discuss this in SS5.1.

### Classifier Agreement -- \(\mathcal{NC}4\)

The linear and NCC classifiers agree on far more examples than random chance, and model scaling encourages this agreement (Appendix Figs. 29, 30). Increasing width for certain depths happens to plateau or even regress the agreement rate, but this limitation is overcome with further training. And finally, agreement is a strong indicator of generalization (Fig. 1, right, \(\mathcal{NC}_{4}\)).

## 5 Analysis

We find that \(\mathcal{NC}\) is generally promoted by model size and training and correlated with generalization (validation performance). We also discern some of this correlation independent of scale (SS5.1).

Figure 3: Validation loss shows a negligible relationship with self-duality (\(\mathcal{NC}_{3}\), left) and some correlation with uniform duality (\(\mathcal{UNC}_{3}\), right). In other words, \(\mathcal{UNC}_{3}\) develops with scale and correlates with generalization much better than \(\mathcal{NC}_{3}\).

### Neural Collapse's Relationship with Generalization

Table 1 presents the correlation scores of \(\mathcal{NC}\) metrics with generalization alongside their associated p-values from the permutation tests described in SS3.8. The majority of the correlations are statistically significant (\(p<0.05\)) independent of scale, affirming that \(\mathcal{NC}\) is correlated with generalization.

### Duality of Duality

The dichotomy of self-duality (\(\mathcal{NC}_{3}\)) and uniform duality (\(\mathcal{UNC}_{3}\)) is rather conspicuous. Our main experiments find that \(\mathcal{NC}_{3}\) does not consistently develop with scale while \(\mathcal{UNC}_{3}\) does (Fig. 3). However, within a fixed scale, the opposite is true, implying that \(\mathcal{UNC}_{3}\) may be confounded by model capacity while \(\mathcal{NC}_{3}\) is a subtle and fine-grained indicator of generalization.

### The Effect of Weight Regularization

Our models trained with either weight decay factor exhibited very similar patterns in the emergence of \(\mathcal{NC}\) (or lack thereof), but the more aggressive factor \(\beta=0.1\) resulted in stronger development of \(\mathcal{NC}\) properties than with \(\beta=0.0005\) (Appendices E, F, G, H, I, J, K, M, N, P). These findings empirically affirm \(\beta=0.1\) weight decay as sensible for CLM pre-training [81], and concur with [51] on the pivotal role that appropriate regularization plays in the emergence of \(\mathcal{NC}\).

## 6 Limitations

Neural collapseWhile to the best of our knowledge, no previous work has studied realistic stochastic token prediction, it is possible that the quantities that we measure are not perfectly suited for \(\mathcal{NC}\) in language modelling. As we described in SS1.1, the \(\mathcal{NC}\) framework does not translate neatly to the language modelling space due to many adverse conditions, so full convergence to \(\mathcal{NC}\) in the TPT was highly improbable. This paper leaves much room for future work to better adapt \(\mathcal{NC}\) for next-token prediction, which we discuss further in Section 7.

Language modellingOur work focused on autoregressive pre-training in its most basic form. We did not conduct experiments into encoder, multi-modal, or instruction-tuned models. Post-training techniques such as supervised fine-tuning, reinforcement learning with human feedback [91] or direct preference optimization [92] are also out-of-scope. This paper uses validation CE loss as the sole indicator of performance, leaving out any downstream task evaluations.

Confounder of model scaleThe models that we used in our permutation test (SS5.1, Table 1) are only of a single small architecture trained for one epoch with relatively weak weight regularization (\(\beta=0.0005\)). Therefore, our experimental results on scale-independent links between \(\mathcal{NC}\) and generalization may not necessarily translate to larger models. Further investigation on (foundation) LLMs orders of magnitude larger than our CLMs trained with modern NLP methods would provide more robust insight into any direct correlations.

\begin{table}
\begin{tabular}{r l r r} \hline \hline Property & Measurement & \(R^{2}\) Correlation (\(\uparrow\)) & \(p\)-value (\(\downarrow\)) \\ \hline \(\mathcal{NC}_{1}\) & Within-Class Variability Collapse & \(0.192011\) & \(\mathbf{0.0485}\) \\ \(\mathcal{NC}_{2}\) & Equinormness & \(0.026174\) & \(0.4870\) \\ \(\mathcal{NC}_{2}\) & Equiangularity & \(0.218574\) & \(\mathbf{0.0317}\) \\ \(\mathcal{GNC}_{2}\) & Hyperspherical Uniformity & \(0.487935\) & \(\mathbf{0.0002}\) \\ \(\mathcal{NC}_{3}\) & Self-Duality & \(0.322210\) & \(\mathbf{0.0063}\) \\ \(\mathcal{UNC}_{3}\) & Uniform Duality & \(0.000036\) & \(0.9784\) \\ \(\mathcal{NC}_{4}\) & Classifier Agreement & \(0.490278\) & \(\mathbf{0.0001}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Permutation test of \(\mathcal{NC}\) measurements with respect to validation loss. Twenty-one (21) identical two-layer \(768\)-wide models were trained with different data shuffling seeds and permuted with \(10^{4}\) trials. The \(p\)-values below \(0.05\) (bolded) show those properties to be statistically significant.

## 7 Discussion

Layer/depth-wise neural collapsePast works have established that properties resembling \(\mathcal{NC}\) evolve as a function of model depth [4, 36, 53, 56, 59, 60, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]. Layer-wise \(\mathcal{NC}\) -- sometimes dubbed _deep neural collapse_ (\(\mathcal{DNC}\)) [53, 59] -- and related phenomena at intermediate layers remain an interesting subtopic. We leave their study and induction in CLMs (like [105]) as future work.

Learning to collapseGiven the evidence for the development of \(\mathcal{NC}\) and associated generalization under various loss functions [106, 107, 108, 109, 100, 104, 105, 106, 107, 108] in other domains, NLP researchers may still benefit from analyzing, adapting or training towards \(\mathcal{NC}\). As alluded to earlier, the simplex ETF and even the CE loss may not be truly optimal for this problem setting, so we anticipate future works to both construct more amenable geometries with better-suited objectives and then capitalize on their benefits downstream. As discussed in SS2, there is an abundance of literature in \(\mathcal{NC}\), some of which could potentially adapt \(\mathcal{NC}\) to be useful for NLP; we hope to inspire more such investigations.

InterpretabilityAt a high level, the number and density of clusters for a token can reflect its learned meanings and uses. This would be particularly useful as LLMs adapt to ever-evolving language and further expansion into non-English domains. Our formulae (Section 3) and results (Section 4) expose the pairwise token class interactions in noise, interference, classifier duality, and classifier agreement in the top-level features. Similarly to works in other domains [109, 55, 72, 21], these \(\mathcal{NC}\) metrics can serve as a form of low-level interpretability to aid understanding certain behaviours of (L)LMs. Between tokens, one can often discern how related or interchangeable words are based on their pair-wise interactions, or how antithetical or unrelated they are based on orthogonality. For example, we present a rudimentary analysis of homonyms and English first names in Appendix Q.

FairnessFoundation models are ubiquitous for their comprehensive capabilities and adaptability. As previous work discussed class imbalance [61, 62, 35, 58], our work may extend these strategies to measure and perhaps promote fairness in foundation LLMs, some of which are designed to be multilingual or multicultural. For example, [110] contemporarily explores the use of \(\mathcal{UNC}3\) to mitigate biases in BERT-based [7] models.

While \(\mathcal{NC}\) itself would not lead to unfairness, its potential interpretability may, in theory, enable an (adversarial) agent to measure and optimize for (un)fairness as they manipulate an LLM.

LLM EvaluationsResearchers in NLP and multimodal settings are ultimately interested in measuring model performance on practical tasks; notable benchmarks include GLUE [77], MMLU [111], and BIG-bench [112]. However, several contemporaries [113, 114, 115] have demonstrated that models' downstream capabilities are roughly correlated with their abilities to effectively compress their pre-training data. Based on their findings, our application of the \(\mathcal{NC}\) framework to the pre-training stage of CLMs against validation CE loss should be an appropriate first step in this intersection. Looking forward, we anticipate exciting analysis for language modelling tasks or benchmarks, especially on creativity and retrieval for natural language understanding and generation.

Conversely, some form of \(\mathcal{NC}\) could be an alternative evaluation. Although it would be prohibitively expensive to measure \(\mathcal{NC}\) on the vast and sometimes obscure pre-training data of most frontier production LLMs, doing so on a small set of in-distribution data (i.e. test set) would be realistic.

## 8 Conclusion

In this paper, we apply the _neural collapse_ (\(\mathcal{NC}\)) framework to the next-token prediction problem, where models are undertrained and next-tokens are variably drawn from numerous and imbalanced token classes. We leverage canonical and more recent metrics to demonstrate that \(\mathcal{NC}\) emerges as we scale the size and training of hundreds of causal language models. Our results show a correlation between \(\mathcal{NC}\) and generalization, a relationship that persists even when the model scale is fixed.

In the short term, our work presents rudimentary techniques to analyze and interpret token-level properties of (L)LMs. We anticipate future work to suitably adapt \(\mathcal{NC}\) (and related frameworks) to the still-fresh frontier of autoregressive language modelling. Researchers could then effectively capitalize on previous learnings from \(\mathcal{NC}\) to better understand and improve the pre/post-training processes of increasingly complex and large language (multimodal) models.

## Acknowledgements

We thank Elliot Creager, David Glukhov, Daniel Johnson, Jivan Waber, and Colin Raffel for their helpful feedback and stimulating discussions. Aditya Mehrotra and Yu Bo Gao provided technical assistance in our implementations. We acknowledge the support of the Natural Sciences and Engineering Research Council (NSERC) of Canada (www.nserc-crsng.gc.ca/). This research was enabled in part by resources from Calcul Quebec (www.calculquebec.ca), the Digital Research Alliance of Canada (www.allianeccan.ca), and the Vector Institute (www.vectorinstitute.ai).

## References

* P. S. Florence (1950)Human behaviour and the principle of least effort. The Economic Journal60 (240), pp. 808-810. External Links: Document Cited by: SS1.
* P. S. Florence (1950)Human behaviour and the principle of least effort. The Economic Journal60 (240), pp. 808-810. External Links: Document Cited by: SS1.
* P. S. Florence (1950)Human behaviour and the principle of least effort. The Economic Journal60 (240), pp. 808-810. External Links: Document Cited by: SS1.
* V. Kothapalli (2023)Neural collapse: a review on modelling principles and generalization. Transactions on Machine Learning Research. External Links: Document Cited by: SS1.
* E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell (2021)On the dangers of stochastic parrots: can language models be too big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, New York, NY, USA, pp. 610-623. External Links: Document Cited by: SS1.
* J. Devlin, M. Chang, K. Lee, and K. Toutanova (2019)BERT: pre-training of deep bidirectional transformers for language understanding. External Links: Link Cited by: SS1.
* A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. (2018)Improving language understanding by generative pre-training. External Links: Link Cited by: SS1.
* J. Jiang, J. Zhou, P. Wang, Q. Qu, D. Mixon, C. You, and Z. Zhu (2023)Generalized neural collapse for a large number of classes. External Links: Link Cited by: SS1.
* Z. Yang, Z. Dai, R. Salakhutdinov, and W. W. Cohen (2018)Breaking the softmax bottleneck: a high-rank rnn language model. External Links: Link Cited by: SS1.
* C. E. Shannon (1948)A mathematical theory of communication. The Bell System Technical Journal27 (3), pp. 379-423. External Links: Document Cited by: SS1.
* P. S. Florence (1950)Human behaviour and the principle of least effort. The Economic Journal60 (240), pp. 808-810. External Links: Document Cited by: SS1.
* D. Jurafsky and J. H. Martin (2009)Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition. External Links: Link Cited by: SS1.
* J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei (2020)Scaling laws for neural language models. External Links: Link Cited by: SS1.
* J. L. Johnson, J. Waber, and C. Raffel (2020)Neural collapse: a review on modelling principles and generalization. Transactions on Machine Learning Research. External Links: Link Cited by: SS1.
* J. L. Johnson, P. Wang, Q. Qu, D. Mixon, C. You, and Z. Zhu (2023)Generalized neural collapse for a large number of classes. External Links: Link Cited by: SS1.

[MISSING_PAGE_POST]

. Raffel, and C. Raffel (2020)Neural collapse: a review on modelling principles and generalization generalization generalization generalization generalization. External Links: Link Cited by: SS1.
* J. L. Johnson, J. Waber, C. Raffel, and C. Raffel (2020)Neural collapse: a review on modelling principles and* Hoffmann et al. [2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/2203.15556v1.
* Muennighoff et al. [2023] Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models, 2023. URL https://arxiv.org/abs/2305.16264v4.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Lu and Steinerberger [2021] Jianfeng Lu and Stefan Steinerberger. Neural collapse with cross-entropy loss, 2021. URL https://arxiv.org/abs/2012.08465v2.
* Liu et al. [2023] Weiyang Liu, Longhui Yu, Adrian Weller, and Bernhard Scholkopf. Generalizing and decoupling neural collapse via hyperspherical uniformity gap, 2023. URL https://arxiv.org/abs/2303.06484v2.
* Li et al. [2023] Zexi Li, Xinyi Shang, Rui He, Tao Lin, and Chao Wu. No fear of classifier biases: Neural collapse inspired federated learning with synthetic and fixed classifier. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5319-5329, 2023.
* Kothapalli et al. [2024] Vignesh Kothapalli, Tom Tirer, and Joan Bruna. A neural collapse perspective on feature evolution in graph neural networks. _Advances in Neural Information Processing Systems_, 36, 2024.
* Yang et al. [2023] Yibo Yang, Haobo Yuan, Xiangtai Li, Jianlong Wu, Lefei Zhang, Zhouchen Lin, Philip Torr, Dacheng Tao, and Bernard Ghanem. Neural collapse terminus: A unified solution for class incremental learning and its variants, 2023. URL https://arxiv.org/abs/2308.01746v1.
* Zhou et al. [2023] Qinhao Zhou, Xiang Xiang, and Jing Ma. Hierarchical task-incremental learning with feature-space initialization inspired by neural collapse. _Neural Processing Letters_, 55(8):10811-10827, 2023.
* Ran et al. [2024] Hang Ran, Weijun Li, Lusi Li, Songsong Tian, Xin Ning, and Prayag Tiwari. Learning optimal inter-class margin adaptively for few-shot class-incremental learning via neural collapse-based meta-learning. _Information Processing & Management_, 61(3):103664, 2024. ISSN 0306-4573. doi: https://doi.org/10.1016/j.ipm.2024.103664. URL https://www.sciencedirect.com/science/article/pii/S0306457324000244.
* Medepalli and Doraiswamy [2023] Saaketh Medepalli and Naren Doraiswamy. On the role of neural collapse in meta learning models for few-shot learning, 2023. URL https://arxiv.org/abs/2310.00451v2.
* Haas et al. [2023] Jarrod Haas, William Yolland, and Bernhard Rabus. Linking neural collapse and l2 normalization with improved out-of-distribution detection in deep neural networks, 2023. URL https://arxiv.org/abs/2209.08378v3.
* Ammar et al. [2024] Moun Ben Ammar, Nacim Belkhir, Sebastian Popescu, Antoine Manzanera, and Gianni Franchi. Neco: Neural collapse based out-of-distribution detection, 2024. URL https://arxiv.org/abs/2310.06823.
* Zhang et al. [2024] Jiawei Zhang, Yufan Chen, Cheng Jin, Lei Zhu, and Yuantao Gu. Epa: Neural collapse inspired robust out-of-distribution detector, 2024. URL https://arxiv.org/abs/2401.01710v1.
* Li et al. [2024] Donghao Li, Yang Cao, and Yuan Yao. Neuromixgdp: A neural collapse-inspired random mixup for private data release. In _Conference on Parsimony and Learning_, pages 480-514. PMLR, 2024.

* Wang et al. [2024] Chendi Wang, Yuqing Zhu, Weijie J. Su, and Yu-Xiang Wang. Neural collapse meets differential privacy: Curious behaviors of noisygd with near-perfect representation learning, 2024. URL https://arxiv.org/abs/2405.08920v2.
* Ergen et al. [2022] Tolga Ergen, Arda Sahiner, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Demystifying batch normalization in relu networks: Equivalent convex optimization models and implicit regularization, 2022. URL https://arxiv.org/abs/2103.01499v3.
* Ergen and Pilanci [2021] Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 3004-3014. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/ergen21b.html.
* Seleznova et al. [2023] Mariia Seleznova, Dana Weitzner, Raja Giryes, Gitta Kutyniok, and Hung-Hsu Chou. Neural (tangent kernel) collapse. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 16240-16270. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/3477ca0ce484aa2fa42c1361ab601c25-Paper-Conference.pdf.
* Telgarsky [2022] Matus Telgarsky. Feature selection with gradient descent on two-layer networks in low-rotation regimes, 2022. URL https://arxiv.org/abs/2208.02789v1.
* Hong and Ling [2023] Wanli Hong and Shuyang Ling. Neural collapse for unconstrained feature model under cross-entropy loss with imbalanced data, 2023. URL https://arxiv.org/abs/2309.09725v2.
* Arous et al. [2023] Gerard Ben Arous, Reza Gheissari, Jiaoyang Huang, and Aukosh Jagannath. High-dimensional sgd aligns with emerging outlier eigenspaces, 2023. URL https://arxiv.org/abs/2310.03010v1.
* Hui et al. [2022] Like Hui, Mikhail Belkin, and Preetum Nakkiran. Limitations of neural collapse for understanding generalization in deep learning, 2022. URL https://arxiv.org/abs/2202.08384v1.
* Galanti et al. [2022] Tomer Galanti, Andras Gyorgy, and Marcus Hutter. Improved generalization bounds for transfer learning via neural collapse. In _First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022_, 2022. URL https://openreview.net/forum?id=Vrk7Pkw0hT_.
* Wang et al. [2023] Zijian Wang, Yadan Luo, Liang Zheng, Zi Huang, and Mahsa Baktashmotlagh. How far pre-trained models are from neural collapse on the target dataset informs their transferability. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5549-5558, 2023.
* Li et al. [2024] Xiao Li, Sheng Liu, Jinxin Zhou, Xinyu Lu, Carlos Fernandez-Granda, Zhihui Zhu, and Qing Qu. Understanding and improving transfer learning of deep models via neural collapse. _Transactions on Machine Learning Research_, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=08r84MzTQB.
* Zhu et al. [2023] Didi Zhu, Zexi Li, Min Zhang, Junkun Yuan, Yunfeng Shao, Jiashuo Liu, Kun Kuang, Yinchuan Li, and Chao Wu. Understanding prompt tuning for v-l models through the lens of neural collapse, 2023. URL https://arxiv.org/abs/2306.15955v3.
* Krizhevsky and Hinton [2009] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009. URL https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Mixon et al. [2020] Dustin G. Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features, 2020. URL https://arxiv.org/abs/2011.11619v1.

* [45] Tomaso Poggio and Qianli Liao. Explicit regularization and implicit bias in deep network classifiers trained with the square loss, 2020. URL https://arxiv.org/abs/2101.00072v1.
* [46] Weinan E and Stephan Wojtowytsch. On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers, 2021. URL https://arxiv.org/abs/2012.05420v3.
* [47] Cong Fang, Hangfeng He, Qi Long, and Weijie J Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. _Proceedings of the National Academy of Sciences_, 118(43):e2103091118, 2021.
* [48] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features, 2021. URL https://arxiv.org/abs/2105.02375v1.
* [49] X.Y. Han, Vardan Papyan, and David L. Donoho. Neural collapse under MSE loss: Proximity to and dynamics on the central path. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=w1UbdvWH_R3.
* [50] Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, and Qing Qu. Neural collapse with normalized features: A geometric analysis over the riemannian manifold. _Advances in neural information processing systems_, 35:11547-11560, 2022.
* 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 4243-4247, 2022. doi: 10.1109/ICASSP43922.2022.9746778.
* [52] Yibo Yang, Shixiang Chen, Xiangtai Li, Liang Xie, Zhouchen Lin, and Dacheng Tao. Inducing neural collapse in imbalanced learning: Do we really need a learnable classifier at the end of deep neural network?, 2022. URL https://arxiv.org/abs/2203.09081v3.
* [53] Tom Tirer and Joan Bruna. Extended unconstrained features model for exploring deep neural collapse. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 21478-21505. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/tirer22a.html.
* [54] Peng Wang, Huikang Liu, Can Yaras, Laura Balzano, and Qing Qu. Linear convergence analysis of neural collapse with unconstrained features. In _OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)_, 2022.
* [55] Jinxin Zhou, Chong You, Xiao Li, Kangning Liu, Sheng Liu, Qing Qu, and Zhihui Zhu. Are all losses created equal: A neural collapse perspective, 2022. URL https://arxiv.org/abs/2210.02192v2.
* [56] Akshay Rangamani, Marius Lindegaard, Tomer Galanti, and Tomaso A Poggio. Feature learning in deep classifiers through intermediate neural collapse. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 28729-28745. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/rangamani23a.html.
* [57] Tom Tirer, Haoxiang Huang, and Jonathan Niles-Weed. Perturbation analysis of neural collapse. In _International Conference on Machine Learning_, pages 34301-34329. PMLR, 2023.
* [58] Hien Dang, Tho Tran, Stanley Osher, Hung Tran-The, Nhat Ho, and Tan Nguyen. Neural collapse in deep linear networks: From balanced to imbalanced data, 2023. URL https://arxiv.org/abs/2301.00437v5.
* [59] Peter Sokenik, Marco Mondelli, and Christoph H Lampert. Deep neural collapse is provably optimal for the deep unconstrained features model. _Advances in Neural Information Processing Systems_, 36, 2024.

* Nguyen et al. [2023] Duc Anh Nguyen, Ron Levie, Julian Lienen, Eyke Hullermeier, and Gitta Kutyniok. Memorization-dilation: Modeling neural collapse under noise. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=cJWxqmmDL2b.
* Zhong et al. [2023] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang, and Jiaya Jia. Understanding imbalanced semantic segmentation through neural collapse. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19550-19560, 2023.
* Liu et al. [2023] Xuantong Liu, Jianfeng Zhang, Tianyang Hu, He Cao, Yuan Yao, and Lujia Pan. Inducing neural collapse in deep long-tailed learning. In _International Conference on Artificial Intelligence and Statistics_, pages 11534-11544. PMLR, 2023.
* Gao et al. [2023] Peifeng Gao, Qianqian Xu, Peisong Wen, Huiyang Shao, Zhiyong Yang, and Qingming Huang. A study of neural collapse phenomenon: Grassmannian frame, symmetry and generalization, 2023. URL https://arxiv.org/abs/2304.08914v2.
* Li et al. [2023] Mufan Bill Li, Mihai Nica, and Daniel M. Roy. The neural covariance sde: Shaped infinite depth-and-width networks at initialization, 2023. URL https://arxiv.org/abs/2206.02768v3.
* Hu et al. [2024] Zhanxuan Hu, Yichen Wang, Hailong Ning, Yonghang Tai, and Feiping Nie. Neural collapse inspired semi-supervised learning with fixed classifier. _Information Sciences_, 667:120469, 2024.
* Peifeng et al. [2024] Gao Peifeng, Qianqian Xu, Yibo Yang, Peisong Wen, Huiyang Shao, Zhiyong Yang, Bernard Ghanem, and Qingming Huang. Towards demystifying the generalization behaviors when neural collapse emerges, 2024. URL https://openreview.net/forum?id=XVv4S6LmMk.
* Elhage et al. [2022] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superposition. _Transformer Circuits Thread_, 2022. URL https://transformer-circuits.pub/2022/toy_model/index.html.
* Radford et al. [2019] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
* Dubey et al. [2024] A. Dubey et al. (101 additional authors). The llama 3 herd of models. 2024. URL https://arxiv.org/abs/2407.21783. All authors were affiliated with Meta.
* Thrampoulidis et al. [2022] Christos Thrampoulidis, Ganesh Ramachandra Kini, Vala Vakilian, and Tina Behnia. Imbalance trouble: Revisiting neural-collapse geometry. _Advances in Neural Information Processing Systems_, 35:27225-27238, 2022.
* Natarajan et al. [2013] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. _Advances in neural information processing systems_, 26, 2013.
* Fisher et al. [2024] Quinn Fisher, Haoming Meng, and Vardan Papyan. Pushing boundaries: Mixup's influence on neural collapse, 2024. URL https://arxiv.org/abs/2402.06171v1.
* Li et al. [2024] Pengyu Li, Xiao Li, Yutong Wang, and Qing Qu. Neural collapse in multi-label learning with pick-all-label loss, 2024. URL https://arxiv.org/abs/2310.15903v4.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Mimno and Thompson [2017] David Mimno and Laure Thompson. The strange geometry of skip-gram with negative sampling. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 2873-2878, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1308. URL https://aclanthology.org/D17-1308.

* Ethayarajh [2019] Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings, 2019. URL https://arxiv.org/abs/1909.00512.
* Wang et al. [2019] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019. URL https://arxiv.org/abs/1804.07461v3.
* Feng et al. [2023] Jia Hui Feng, Edmund M-K Lai, and Weihua Li. A study of neural collapse for text classification. In _International Conference on Deep Learning Theory and Applications_, pages 126-142. Springer, 2023.
* Laurent et al. [2023] Thomas Laurent, James H. von Brecht, and Xavier Bresson. Feature collapse, 2023. URL https://arxiv.org/abs/2305.16162v1.
* Black et al. [2021] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. URL https://arxiv.org/abs/2205.01068v4.
* Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971v1.
* Jiang et al. [2023] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825v1.
* Fisher [1936] Ronald A Fisher. The use of multiple measurements in taxonomic problems. _Annals of eugenics_, 7(2):179-188, 1936.
* Rao [1948] C Radhakrishna Rao. The utilization of multiple measurements in problems of biological classification. _Journal of the Royal Statistical Society. Series B (Methodological)_, 10(2):159-203, 1948.
* Strohmer and Jr [2003] Thomas Strohmer and Robert W Heath Jr. Grassmannian frames with applications to coding and communication. _Applied and computational harmonic analysis_, 14(3):257-275, 2003.
* Waldron [2018] Shayne FD Waldron. _An introduction to finite tight frames_. Springer, 2018.
* Donoho et al. [2005] David L Donoho, Michael Elad, and Vladimir N Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. _IEEE Transactions on information theory_, 52(1):6-18, 2005.
* Tropp [2006] Joel A Tropp. Just relax: Convex programming methods for identifying sparse signals in noise. _IEEE transactions on information theory_, 52(3):1030-1051, 2006.
* Ziegler et al. [2020] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020. URL https://arxiv.org/abs/1909.08593.

* Rafaalf et al. [2023] Rafael Rafaalf, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 53728-53741. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e00ce7-Paper-Conference.pdf.
* Papyan [2020] Vardan Papyan. Traces of class/cross-class structure pervade deep learning spectra, 2020. URL https://arxiv.org/abs/2008.11865v1.
* Hoyt and Owen [2021] Christopher R. Hoyt and Art B. Owen. Probing neural networks with t-sne, class-specific projections and a guided tour, 2021. URL https://arxiv.org/abs/2107.12547v1.
* Zarka et al. [2021] John Zarka, Florentin Guth, and Stephane Mallat. Separation and concentration in deep networks, 2021. URL https://arxiv.org/abs/2012.10424v2.
* Ben-Shaul and Dekel [2022] Ido Ben-Shaul and Shai Dekel. Nearest class-center simplification through intermediate layers. In Alexander Cloninger, Timothy Doster, Tegan Emerson, Manohar Kaul, Ira Ktena, Henry Kvinge, Nina Miolane, Bastian Rieck, Sarah Tymochko, and Guy Wolf, editors, _Proceedings of Topological, Algebraic, and Geometric Learning Workshops 2022_, volume 196 of _Proceedings of Machine Learning Research_, pages 37-47. PMLR, 25 Feb-22 Jul 2022. URL https://proceedings.mlr.press/v196/ben-shaul22a.html.
* He and Su [2022] Hangfeng He and Weijie J. Su. A law of data separation in deep learning, 2022. URL https://arxiv.org/abs/2210.17020v2.
* Parker et al. [2023] Liam Parker, Emre Onal, Anton Stengel, and Jake Intrater. Neural collapse in the intermediate hidden layers of classification neural networks, 2023. URL https://arxiv.org/abs/2308.02760v1.
* Masarczyk et al. [2023] Wojciech Masarczyk, Mateusz Ostaszewski, Ehsan Imani, Razvan Pascanu, Piotr Mil os, and Tomasz Trzcinski. The tunnel effect: Building data representations in deep neural networks. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 76772-76805. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/f249db9ab5975586f36df46f8958c008-Paper-Conference.pdf.
* Beaglehole et al. [2024] Daniel Beaglehole, Peter Sukenik, Marco Mondelli, and Mikhail Belkin. Average gradient outer product as a mechanism for deep neural collapse, 2024. URL https://arxiv.org/abs/2402.13728v2.
* Wang et al. [2024] Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, and Qing Qu. Understanding deep representation learning via layerwise feature compression and discrimination, 2024. URL https://arxiv.org/abs/2311.02960v2.
* Wang et al. [2024] Sicong Wang, Kuo Gai, and Shihua Zhang. Progressive feedforward collapse of resnet training, 2024. URL https://arxiv.org/abs/2405.00985v1.
* Garrod and Keating [2024] Connall Garrod and Jonathan P. Keating. Unifying low dimensional observations in deep learning through the deep linear unconstrained feature model, 2024. URL https://arxiv.org/abs/2404.06106v1.
* Zangrando et al. [2024] Emanuele Zangrando, Piero Deidda, Simone Brugiapaglia, Nicola Guglielmi, and Francesco Tudisco. Neural rank collapse: Weight decay and small within-class variability yield low-rank bias, 2024. URL https://arxiv.org/abs/2402.03991v1.
* Jiang et al. [2024] Jiachen Jiang, Jinxin Zhou, and Zhihui Zhu. On layer-wise representation similarity: Application for multi-exit models with a single classifier, 2024. URL https://arxiv.org/abs/2406.14479.
* Xu et al. [2023] Mengjia Xu, Akshay Rangamani, Qianli Liao, Tomer Galanti, and Tomaso Poggio. Dynamics in deep classifiers trained with the square loss: Normalization, low rank, neural collapse, and generalization bounds. _Research_, 6:0024, 2023.

* Liang and Davis [2023] Tong Liang and Jim Davis. Inducing neural collapse to a fixed hierarchy-aware frame for reducing mistake severity. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1443-1452, 2023.
* Bonifazi et al. [2024] Guglielmo Bonifazi, Iason Chalas, Gian Hess, and Jakub Lucki. Can we understand plasticity through neural collapse?, 2024. URL https://arxiv.org/abs/2404.02719v1.
* Guo et al. [2024] Li Guo, Keith Ross, Zifan Zhao, George Andriopoulos, Shuyang Ling, Yufeng Xu, and Zixuan Dong. Cross entropy versus label smoothing: A neural collapse perspective, 2024. URL https://arxiv.org/abs/2402.03979v2.
* Xu et al. [2024] Jingxuan Xu, Wuyang Chen, Linyi Li, Yao Zhao, and Yunchao Wei. Collapsed language models promote fairness, 2024. URL https://arxiv.org/abs/2410.04472.
* Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _Proceedings of the International Conference on Learning Representations (ICLR)_, 2021.
* BIG bench authors [2023] BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=uyTL5BVosj.
* Du et al. [2024] Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding emergent abilities of language models from the loss perspective, 2024. URL https://arxiv.org/abs/2403.15796.
* Huang et al. [2024] Yuzhen Huang, Jinghan Zhang, Zifei Shan, and Junxian He. Compression represents intelligence linearly. In _First Conference on Language Modeling_, 2024. URL https://openreview.net/forum?id=SHHj84USSH.
* Yin et al. [2024] Mingjia Yin, Chuhan Wu, Yufei Wang, Hao Wang, Wei Guo, Yasheng Wang, Yong Liu, Ruiming Tang, Defu Lian, and Enhong Chen. Entropy law: The story behind data compression and llm performance, 2024.
* Merity et al. [2016] Stephen Merity, Ilya Sutskever, Simon Kornblith, and Nikhil Goyal. Pointer sentinel mixture models. _arXiv preprint arXiv:1609.07843_, 2016.
* Zhu et al. [2015] Yao Zhu, Zhen Yu, Chao Zhang, Yijia Wu, et al. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. _arXiv preprint arXiv:1506.05829_, 2015.
* Crawl [2023] Common Crawl. Common crawl, 2023. URL https://commoncrawl.org. Accessed: 2023-10-30.
* Gao et al. [2020] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020. URL https://arxiv.org/abs/2101.00027.
* Shumailov et al. [2024] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget, 2024. URL https://arxiv.org/abs/2305.17493.
* Gerstgrasser et al. [2024] Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov, Henry Sleight, John Hughes, Tomasz Korbak, Rajashree Agrawal, Dhruv Pai, Andrey Gromov, Daniel A. Roberts, Diyi Yang, David L. Donoho, and Sanmi Koyejo. Is model collapse inevitable? breaking the curse of recursion by accumulating real and synthetic data, 2024. URL https://arxiv.org/abs/2404.01413.
* Burgess et al. [2019] Neil Burgess, Jelena Milanovic, Nigel Stephens, Konstantinos Monachopoulos, and David Mansell. Bfloat16 processing for neural networks. In _2019 IEEE 26th Symposium on Computer Arithmetic (ARITH)_, pages 88-91, 2019. doi: 10.1109/ARITH.2019.00022.

Dataset

TinyStories is a dataset of short children's stories generated by GPT-3.5 and GPT-4 [2], released with the CDLA-Sharing-1.0 licence. We trained and evaluated models on their first version, as described:

* The \(2{,}141{,}709\) stories are split into \(2{,}119{,}719\) train and \(21{,}990\) validation stories.
* Their experimental setup [2] called for the GPT-2 [68] tokenizer, of which only a subset vocabulary \(\mathbb{V}=\llbracket 1,29233\rrbracket\) appears in TinyStories.
* Following the GPT-style teacher-forcing regime for training/evaluation [8], raw sequences (stories) from the train set are packed (by two preprocessing workers) into \(229{,}367\) (\(S\)) chunks of \(2048\) (\(T\)) tokens each. This setup provides \(469{,}514{,}249\) (\(N\)) ground-truth17 token samples for training.

Footnote 17: \(N=S(T-1)\) as we cannot use the first ground-truth nor the last predicted token in any chunk.

### Alternative (Real) Datasets

The study of \(\mathcal{NC}\) in causal language modelling at the token level would be unreasonably expensive, so the motivation to use a small dataset is clear. However, most commonly used text datasets such as WikiText [116], BookCorpus [117], CommonCrawl [118], or most subsets from the Pile [119] are much too complex and broad to be effectively compressed by CLMs of the scale that we work with.

WikiText-2 and WikiText-103 present significant drawbacks for our experiments. Both datasets contain a considerable amount of low-quality data that does not concentrate on essential linguistic structures such as grammar, vocabulary, facts, and reasoning. WikiText-2 has a similar empirical vocabulary to TinyStories under the GPT-Neo [80] tokenizer (27K vs. 29K) but only has around 34K rows of training data compared to 2.1M in TinyStories. Our small-scale NC experiment on WikiText-2 revealed that the models were very brittle and prone to overfitting. On the other hand, WikiText-103 is comparably sized to TinyStories but utilizes around 44K unique tokens. Our CLMs trained on WikiText-103 struggled to produce coherent sentences, likely due to the excessive breadth and information, as noted by the authors of TinyStories. Beyond these two, we were unable to find any real datasets that both followed established scaling laws [14; 15] for CLMs at our scale and are simple enough to suit the analysis of \(\mathcal{NC}\).

### On the Use of TinyStories

According to its authors, TinyStories [2] is explicitly designed to preserve the essential elements of natural language, such as grammar, vocabulary, facts, and reasoning, while being smaller and more refined in terms of its breadth and diversity. Unlike large corpora that can overwhelm small language models (SLMs) due to their excessive breadth and diversity, TinyStories offers a concentrated dataset that hones in on core linguistic structures and reasoning capabilities. This is evident in its small vocabulary, consisting of approximately 1500 words that a child would use, and in its 29K empirical vocabulary under the GPT-Neo tokenizer.

Figure 4: The \(500\) most frequent classes from TinyStories [2] exhibit significant sample imbalance. Despite the synthetic nature of TinyStories, such a distribution is typical of natural language [11; 12].

Despite its concentrated nature, TinyStories enables models trained on it to produce grammatically correct, factual, and reasonable stories. Additionally, these models can be finetuned on specific instructions found in the TinyStories-Instruct dataset. The authors of TinyStories also demonstrate that their models can creatively produce stories dissimilar enough to their training data, indicating a balanced capability for generalization and creativity.

One particular advantage of TinyStories is the small vocabulary relative to total training tokens, rendering a reasonable number of classes with higher average token counts. This is relevant because the possibility of \(\mathcal{NC}\) and a CLM's ability to compress language data into distinct geometries depend partially on the ratios between embedding dimension, vocabulary size, and average token frequency. Conveniently, frequency analysis of the overall dataset produced a distribution (Figure 4) similar to real human language, so TinyStories should provide a good balance for an initial study of this \(\mathcal{NC}\).

Additionally, TinyStories has a more regular structure as GPT-3.5/4 was instructed to produce children's stories with certain themes and forms with a conservative vocabulary. We believe this would reduce the amount of clustering noise from the breadth of information and structures in real general data, and allow our smaller CLMs to exhibit some clear trends toward \(\mathcal{NC}\).

Furthermore, TinyStories was created using GPT 3.5/4, advanced language models with significantly larger architectures trained on orders of magnitude more tokens; this should help minimize the effect of the synthetic nature of the generated dataset. We also considered a possible effect of model collapse as a result of training on synthetic data [120] and follow-up work [121] suggest that a single iteration of data generation (as generated TinyStories) has a very negligible model collapse.

## Appendix B Model Architectural Details

\begin{table}
\begin{tabular}{|l|l|} \hline
**Setting** & **Value** \\ \hline activation\_function & gelu\_new \\ \hline architectures & GPTNeofForCausalLM \\ \hline attention\_dropout & 0 \\ \hline attention\_layers & global, local, global, local,... \\ \hline attention\_types & [[global, local], 6] \\ \hline bos\_token\_id & 50256 \\ \hline embed\_dropout & 0 \\ \hline eos\_token\_id & 50256 \\ \hline gradient\_checkpointing & false \\ \hline hidden\_size & 1024 \\ \hline initializer\_range & 0.02 \\ \hline intermediate\_size & null \\ \hline layer\_norm\_epsilon & 1e-05 \\ \hline max\_position\_embeddings & 2048 \\ \hline model\_type & gpt\_neo \\ \hline num\_heads & 16 \\ \hline num\_layers & 12 \\ \hline resid\_dropout & 0 \\ \hline summary\_activation & null \\ \hline summary\_first\_dropout & 0.1 \\ \hline summary\_proj\_to\_labels & true \\ \hline summary\_type & cls\_index \\ \hline summary\_use\_proj & true \\ \hline torch\_dtype & float32 \\ \hline transformers\_version & 4.28.1 \\ \hline use\_cache & true \\ \hline vocab\_size & 50257 \\ \hline window\_size & 256 \\ \hline \end{tabular}
\end{table}
Table 2: Sample architectural configuration for a \(12\)-layer \(1024\)-dimensional causal language model (CLM) based on [2] and GPT-Neo [80]. Shallower models have configurations with attention\_layers and attention\_types truncated. Narrower models adjust hidden\_size to their width (\(d\)). All other configuration values are the same across models.

Optimization

The training was performed using an adaptation of an open-source causal language modelling script from Huggingface: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py

* Each model was trained on a single NVIDIA A100 (40GB) GPU for up to 8 hours per epoch.
* Learning rates were set by a linear schedule based on the number of steps with no warm-up.
* Training was performed in bfloat16 [122] mixed precision.
* The results presented in this work are from two sets of models trained with weight decay \(\beta=0.0005\)[51] and \(\beta=0.1\)[81]. A previous set of models was trained without weight decay and the results are very similar to \(\beta=0.0005\).

## Appendix D Embeddings Collection & \(\mathcal{NC}\) Analysis

Codes for (post-)training analysis are hosted on GitHub:

* Main code https://github.com/rhubarbwu/linguistic-collapse
* Auxillary package: https://github.com/rhubarbwu/neural-collapse

One pass over the train set for embeddings collection can take up to 6 hours on a single NVIDIA A100 (40GB) GPU. Analysis of a single metric for a given model takes less than 5 minutes.

Figure 5: Average (logarithmic) class-distance normalized variance (CDNV, \(\mathcal{NC}_{1}\)) (left) and validation (cross-entropy) loss (right) with respect to training epochs.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|} \hline
**Depth \((L)\downarrow\)** & **Width \((d)\rightarrow\)** & \(64\) & \(128\) & \(256\) & \(512\) & \(768\) & \(1024\) \\ \hline \hline \(1\)-layer & \(16\) & \(16\) & \(16\) & \(16\) & \(16\) & \(16\) \\ \hline \(2\)-layer & \(16\) & \(16\) & \(16\) & \(16\) & \(16\) & \(8\) \\ \hline \(4\)-layer & \(8\) & \(8\) & \(8\) & \(8\) & \(8\) & \(8\) \\ \hline \(8\)-layer & \(8\) & \(8\) & \(8\) & \(4\) & \(4\) & \(4\) \\ \hline \(12\)-layer & \(4\) & \(4\) & \(4\) & \(4\) & \(4\) & \(4\) \\ \hline \end{tabular}
\end{table}
Table 3: Batch sizes used to train models on a single NVIDIA A100 (40GB) GPU. Width (\(d\)) and depth (\(L\)) correspond to hidden_size and length of attention_layers, respectively, in Table 2.

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_EMPTY:25]

[MISSING_PAGE_EMPTY:26]

[MISSING_PAGE_EMPTY:27]

Hyperspherical Uniformity with Scale -- \(\mathcal{GNC}_{2}\)

Figure 19: Variation in logarithmic distances decreases when scaling depth (\(L\)) in models trained for 1 (left) through 10 (right) with weight decays \(\beta=0.0005\) (top) and \(\beta=0.1\) (bottom). This consistent trend towards hyperspherical uniformity affirms that \(\mathcal{GNC}2\)[19] is more useful than \(\mathcal{NC}2\).

Figure 18: Variation in logarithmic distances decreases when scaling width (\(d\)) in models trained for 1 (left) through 10 (right) with weight decays \(\beta=0.0005\) (top) and \(\beta=0.1\) (bottom). This consistent trend towards hyperspherical uniformity affirms that \(\mathcal{GNC}2\)[19] is more useful than \(\mathcal{NC}2\).

[MISSING_PAGE_EMPTY:29]

Self-Duality with Scale -- (Against \(\mathcal{NC}_{3}\))

Self-duality (\(\mathcal{NC}_{3}\)) was originally the convergence of classifiers to means up to rescaling [1]:

\[\left\|\frac{\bm{w}_{c}}{\|\bm{w}_{c}\|_{2}}-\bm{\hat{\mu}}_{c}\right\|_{2}\to 0,\quad\forall c\] (11)

Instead, we use class-wise cosine similarity (Equation 8) and its variation (\(\mathcal{UNC}3\)).

Figure 23: Average classifier alignment increases \(\mathcal{NC}3\) when training for 1 (left) through 10 (right) with weight decays \(\beta=0.0005\) (top) and \(\beta=0.1\) (bottom). However, we see no meaningful trend when scaling width \(d\), suggesting that \(\mathcal{NC}3\) does not coalesce with language modelling training.

Figure 24: Average classifier alignment increases \(\mathcal{NC}3\) when training for 1 (left) through 10 (right) with weight decays \(\beta=0.0005\) (top) and \(\beta=0.1\) (bottom). However, we see no meaningful trend when scaling depth \(L\), suggesting that \(\mathcal{NC}3\) does not coalesce with language modelling training.

## Appendix N Uniformity Duality with Scale -- \(\mathcal{UNC}_{3}\)

Figure 26: Variation in classifier alignment increases when scaling depth (\(L\)) in models trained for 1 (left) through 10 (right) with weight decays \(\beta=0.0005\) (top) and \(\beta=0.1\) (bottom). This negative trend of \(\mathcal{UNC}3\) in more learnt models (right) suggests that the link of \((\mathcal{U})\mathcal{NC}3\) with scale and performance is still weak.

Figure 25: Variation in classifier alignment decreases when scaling width (\(d\)) in models trained for 1 (left) through 10 (right) with weight decays \(\beta=0.0005\) (top) and \(\beta=0.1\) (bottom).

[MISSING_PAGE_EMPTY:32]

Classifier Agreement -- \(\mathcal{NC}_{4}\)

For computational reasons, we compute Equations 9, 10 using a simple decomposition:

\[\operatorname*{argmin}_{c\in\mathbb{V}}\|\bm{h}_{b}-\bm{\mu}_{c}\|_{2}= \operatorname*{argmin}_{c\in\mathbb{V}}\left(\|\bm{h}_{b}\|^{2}+\|\bm{\mu}_{c} \|^{2}-2\bm{h}_{b}^{\top}\bm{\mu}_{c}\right),\] (12)

where \(b\in[1,B]\) and \(c\in\mathbb{V}\) with batch size \(B\) and vocabulary \(\mathbb{V}\).

Figure 30: Classifier agreement improves when scaling depth (\(L\)) in models trained for 1 (left) through 10 (right) with weight decays \(\beta=0.0005\) (top) and \(\beta=0.1\) (bottom).

Figure 29: Classifier agreement improves when scaling width (\(d\)) in models trained for 1 (left) through 10 (right) with weight decays \(\beta=0.0005\) (top) and \(\beta=0.1\) (bottom).

[MISSING_PAGE_FAIL:34]

Figure 33: Under TinyStories-12x1024_10L, the average within-class variability (top) and interference (bottom) of some English first names were far below those of the average token. This might be because names are distinct and are not typically used in the same contexts as other words (aside from articles). The only names to have CDNV close to that of the average token are “Anna” and “Tim”. Note that the positive interference of the average token (right) is not a typo.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims of this paper listed in the abstract and introduction are simply the emergence of the \(\mathcal{NC}\) phenomena and its relationship with generalization in CLMs, which are the scope and results of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of the experiments themselves (e.g. small models, limited sample size) as well as the possibility that our formulations could be improved in the future (there could be more suitable metrics). See Sections 6, 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The equations and equivalent decompositions used are all clear and numbered in the paper. Their sources are all provided as references. We do not introduce new proofs or non-trivial expressions. See Section 3. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our methods for training the models, collecting embeddings, and analyzing metrics are clearly described in the paper, with supporting details in Appendices A, B, C. Links to code are provided in the Abstract and Section 3. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Links to the main code and auxiliary library are provided in the Abstract and Section 3, respectively. Main code repository includes training, collection, and analysis scripts, as well as a README briefly documenting how to use them. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Appendices A, B, C, D provide the necessary information on model, data, training, and collection specifications to reproduce our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We trained multiple copies of the same models and observed patterns consistent with the ones presented in our paper. We also state the number of data points and permutation trials we performed for our significance test. See Figure 1 and Sections 4 and 5.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details on the number of dataloader/preprocessing works, the specific GPU model used and the amount of RAM and training/collection time required. See Appendices A, B, C, D. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read the Code of Ethics and determined that our paper does not violate the guidelines. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts**Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper includes a discussion on how \(\mathcal{NC}\) might be extended to measure fairness in LLMs. Naturally this may inform positive or adversarial manipulations of LLMs. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not use pretrained models or scraped data. Our dataset is synthetic, public and transparent with no risks. The training procedure we use is the standard causal language modeling regime. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes]Justification: The pre-training dataset and its licence are provided in Appendix A and the authors of basic GPT Neo architectures are credited in Appendix B. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Timestamped CSV files of results that we reported in our paper are included in the repository linked in the Abstract. A link to a reference model is provided in Appendix Q. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not work with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: We do not work with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.