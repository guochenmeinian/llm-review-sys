# Distributionally Robust Optimisation with Bayesian Ambiguity Sets

Charita Dellaporta

Department of Statistics

University of Warwick

C.Dellaporta@warwick.ac.uk

&Patrick O'Hara

Department of Computer Science

University of Warwick

Patrick.H.O-Hara@warwick.ac.uk

&Theodoros Damoulas

Department of Computer Science & Department of Statistics

University of Warwick

T.Damoulas@warwick.ac.uk

These authors contributed equally to this work.

###### Abstract

Decision making under uncertainty is challenging since the data-generating process (DGP) is often unknown. Bayesian inference proceeds by estimating the DGP through posterior beliefs about the model's parameters. However, minimising the expected risk under these posterior beliefs can lead to sub-optimal decisions due to model uncertainty or limited, noisy observations. To address this, we introduce Distributionally Robust Optimisation with Bayesian Ambiguity Sets (DROBAS) which hedges against uncertainty in the model by optimising the worst-case risk over a posterior-informed ambiguity set. We show that our method admits a closed-form dual representation for many exponential family members and showcase its improved out-of-sample robustness against existing Bayesian DRO methodology in the Newsvendor problem.

## 1 Introduction

Decision-makers are regularly confronted with the problem of optimising an objective under uncertainty. Let \(x\in\mathbb{R}^{d}\) be a decision-making variable that minimises a stochastic objective function \(f:\mathbb{R}^{d}\times\Xi\to\mathbb{R}\), where \(\Xi\) is the data space and let \(\mathbb{P}^{*}\in\mathcal{P}(\Xi)\) be the data-generating process (DGP) where \(\mathcal{P}(\Xi)\) is the space of Borel distributions over \(\Xi\). In practice, we do not have access to \(\mathbb{P}^{*}\) but to \(n\) independently and identically distributed (i.i.d.) observations \(\mathcal{D}:=\xi_{1:n}\sim\mathbb{P}^{*}\). Without knowledge of the DGP, model-based inference considers a family of models \(\mathcal{P}_{\Theta}:=\{\mathbb{P}_{\theta}:\theta\in\Theta\}\subset\mathcal{P} (\Xi)\) where each \(\mathbb{P}_{\theta}\) has probability density function \(p(\xi|\theta)\) for parameter space \(\Theta\subseteq\mathbb{R}^{k}\). In a Bayesian framework, data \(\mathcal{D}\) is combined with a prior \(\pi(\theta)\) to obtain posterior beliefs about \(\theta\) through \(\Pi(\theta|\mathcal{D})\). Bayesian Risk Optimisation (Wu et al., 2018) then solves a stochastic optimisation problem:

\[\min_{x\in\mathbb{R}^{d}}\ \mathbb{E}_{\theta\sim\Pi(\theta|\mathcal{D})}\left[ \mathbb{E}_{\xi\sim\mathbb{P}_{\theta}}\big{[}f(x,\xi)\big{]}\right].\] (1)

However, our Bayesian estimator is likely different from the true DGP due to model and data uncertainty: the number of observations may be small; the data noisy; or the prior or model may be misspecified. The optimisation problem (1) inherits any estimation error, and leads to overly optimistic decisions on out-of-sample scenarios even if the estimator is unbiased: this phenomenon iscalled the optimiser's curse (Kuhn et al., 2019). For example, if the number of observations is small and the prior is overly concentrated, then the decision is likely to be overly optimistic.

To hedge against the uncertainty of the estimated distribution, the field of Distributionally Robust Optimisation (DRO) minimises the expected objective function under the worst-case distribution that lies in an ambiguity set \(U\subset\mathcal{P}(\Xi)\). Discrepancy-based ambiguity sets contain distributions that are close to a nominal distribution in the sense of some discrepancy measure such as the Kullback-Leibler (KL) divergence (Hu and Hong, 2013), Wasserstein distance (Kuhn et al., 2019) or Maximum Mean Discrepancy (Staib and Jegelka, 2019). For example, some model-based methods (Iyengar et al., 2023; Michel et al., 2021, 2022) consider a family of parametric models and create discrepancy-based ambiguity sets centered on the fitted model. However, uncertainty about the parameters is not captured in these works, which can lead to a nominal distribution far away from the DGP when the data is limited. The established framework for capturing such uncertainty is Bayesian inference.

The closest work to ours, using parametric Bayesian inference to inform the optimisation problem, is Bayesian DRO (BDRO) by Shapiro et al. (2023). BDRO constructs discrepancy-based ambiguity sets with the KL divergence and takes an _expected worst-case_ approach, under the posterior distribution. More specifically, let \(U_{\epsilon}(\mathbb{P}_{\theta}):=\{\mathbb{Q}\in\mathcal{P}(\Xi):d_{\text{ KL}}(\mathbb{Q}\|\mathbb{P}_{\theta})\leq\epsilon\}\) be the ambiguity set centered on distribution \(\mathbb{P}_{\theta}\) with parameter \(\epsilon\in[0,\infty)\) controlling the size of the ambiguity set. Under the expected value of the posterior, Bayesian DRO solves:

\[\min_{x\in\mathbb{R}^{d}}\ \mathbb{E}_{\theta\sim\Pi(\theta|\mathcal{D})} \ \Bigg{[}\sup_{\mathbb{Q}\in U_{\epsilon}(\mathbb{P}_{\theta})}\mathbb{E}_{ \xi\sim\mathbb{Q}}[f(x,\xi)]\Bigg{]},\] (2)

where \(\mathbb{E}_{\theta\sim\Pi(\theta|\mathcal{D})}[Y]:=\int_{\Theta}\ Y(\theta)\Pi (\theta\ |\ \mathcal{D})\,d\theta\) denotes the expectation of random variable \(Y:\Theta\rightarrow\mathbb{R}\) with respect to \(\Pi(\theta\ |\ \mathcal{D})\). A decision maker is often interested in protecting against and quantifying the worst-case risk, but BDRO does not correspond to a worst-case risk analysis. Moreover, the BDRO dual problem is a two-stage stochastic problem that involves a double expectation over the posterior and likelihood. To get a good approximation of the dual problem, a large number of samples are required, which increases the solve time of the dual problem.

We introduce DRO with Bayesian Ambiguity Sets (DRO-BAS), an alternative optimisation objective for Bayesian decision-making under uncertainty, based on a posterior-informed ambiguity set. The resulting problem corresponds to a worst-case risk minimisation over distributions with small expected deviation from the candidate model. We go beyond ball-based ambiguity sets, which are dependent on a single nominal distribution, by _allowing the shape of the ambiguity set to be informed by the posterior_. For many exponential family models, we show that the dual formulation of DRO-BAS is an efficient single-stage stochastic program.

Figure 1: Illustration of the construction of the BDRO and DRO-BAS optimisation problems for three i.i.d. posterior samples \(\theta_{1},\theta_{2},\theta_{3}\sim\Pi(\theta\ |\ \mathcal{D})\). BDRO seeks the decision that minimises the average worst-case risk between the three ambiguity sets shown in figure (a) whereas DRO-BAS targets the decision minimising the worst-case risk over the ambiguity set shown in (b).

## 2 DRO with Bayesian Ambiguity Sets

We propose the following DRO-BAS objective:

\[\min_{x\in\mathbb{R}^{d}}\ \sup_{\mathbb{Q}:\mathbb{E}_{\theta}\sim\ln[D( \mathbb{Q},\mathbb{P}_{\theta})]\leq\epsilon}\ \mathbb{E}_{\xi\sim\mathbb{Q}}\ [f_{x}(\xi)],\] (3)

where \(\mathbb{Q}\in\mathcal{P}(\Xi)\) is a distribution in the ambiguity set, \(f_{x}(\xi):=f(x,\xi)\) is the objective function, \(D:\mathcal{P}(\Xi)\times\mathcal{P}(\Xi)\to\mathbb{R}\) is a divergence, and \(\epsilon\in[0,\infty)\) is a tolerance level. The ambiguity set is informed by the posterior distribution \(\Pi\) by considering all probability measures \(\mathbb{Q}\in\mathcal{P}(\Xi)\) which are \(\epsilon\)-away from \(\mathbb{P}_{\theta}\) in expectation, with \(\epsilon\) dictating the desired amount of risk in the decision.

The shape of our ambiguity set is flexible and driven by the posterior distribution. This is contrary to standard ambiguity sets which correspond to a ball around a fixed nominal distribution. The DRO-BAS problem (3) is still a worst-case approach, keeping with DRO tradition, instead of BDRO's expected worst-case formulation (2), see Figure 1.

The Bayesian posterior \(\Pi(\theta\ |\ \mathcal{D})\) targets the KL minimiser between the model family and \(\mathbb{P}^{\star}\)(Walker, 2013), hence it is natural to choose \(D(\mathbb{Q},\mathbb{P}_{\theta})\) to be the KL divergence of \(\mathbb{Q}\) with respect to \(\mathbb{P}_{\theta}\) denoted by \(d_{\text{KL}}(\mathbb{Q}\|\mathbb{P}_{\theta})\). This means that as \(n\to\infty\) the posterior collapses to \(\theta_{0}:=\arg\min_{\theta\in\Theta}d_{\text{KL}}(\mathbb{P}^{\star},\mathbb{ P}_{\theta})\) and the ambiguity set is just a KL-ball around \(\mathbb{P}_{\theta_{0}}\). Using the KL divergence in the DRO-BAS problem in (3), it is straight-forward to obtain an upper bound of the worst-case risk for general models (see Appendix B.1 for a proof):

\[\sup_{\mathbb{Q}:\mathbb{E}_{\theta}\sim\ln[d_{\text{KL}}(Q\| \mathbb{P}_{\theta})]\leq\epsilon}\ \mathbb{E}_{\xi\sim\mathbb{Q}}[f_{x}(\xi)]\leq\inf_{\gamma\geq 0 }\ \gamma\epsilon+\mathbb{E}_{\theta\sim\Pi}\left[\gamma\ln\mathbb{E}_{\xi\sim \mathbb{P}_{\theta}}\ \left[\exp\left(\frac{f_{x}(\xi)}{\gamma}\right)\right]\right].\] (4)

Exact closed-form solutions of DRO-BAS can be obtained for a wide range of exponential family models with conjugate priors. When the likelihood distribution is a member of the exponential family, a conjugate prior also belongs to the exponential family (Gelman et al., 1995, Ch. 4.2). In this setting, before we prove the main result, we start with an important Lemma.

**Lemma 1**.: _Let \(p(\xi\ |\ \theta)\) be an exponential family likelihood and \(\pi(\theta),\,\Pi(\theta\ |\ \mathcal{D})\) a conjugate prior-posterior pair, also members of the exponential family. Let \(\tau_{0},\tau_{n}\in T\) be hyperparameters of the prior and posterior respectively, where \(T\) is the hyperparameter space. Let \(\bar{\theta}_{n}\in\Theta\) depend upon \(\tau_{n}\) and let \(G:T\to\mathbb{R}\) be a function of the hyperparameters. If the following identity holds:_

\[\mathbb{E}_{\theta\sim\Pi}\left[\ln p(\xi\ |\ \theta)\right]=\ln p(\xi\ |\ \bar{ \theta}_{n})-G(\tau_{n}),\] (5)

_then the expected KL-divergence can be written as:_

\[\mathbb{E}_{\theta\sim\Pi}\left[d_{\text{KL}}(\mathbb{Q}\|\mathbb{P}_{\theta })\right]=d_{\text{KL}}(\mathbb{Q},\mathbb{P}_{\bar{\theta}_{n}})+G(\tau_{n}).\] (6)

The condition in (5) is a natural property of many exponential family models, some of which are showcased in Table 1. Future work aims to prove this for all exponential family models. It is straightforward to establish the minimum tolerance level \(\epsilon_{\min}\) required to obtain a non-empty ambiguity set. Since the KL divergence is non-negative, under the condition of Lemma 1, for any \(\mathbb{Q}\in\mathcal{P}(\Xi)\):

\[\mathbb{E}_{\theta\sim\Pi}[d_{\text{KL}}(\mathbb{Q}\|\mathbb{P}_{\theta})]=d_ {\text{KL}}(\mathbb{Q},\mathbb{P}_{\bar{\theta}_{n}})+G(\tau_{n})\geq G(\tau_{ n}):=\epsilon_{\min}(n).\] (7)

We are now ready to prove our main result.

**Theorem 1**.: _Suppose the conditions of Lemma 1 hold and \(\epsilon\geq\epsilon_{\min}(n)\) as in (7). Let \(\tau_{n}\in T\), \(\bar{\theta}_{n}\in\Theta\), and \(G:T\to\mathbb{R}\). Then_

\[\sup_{\mathbb{Q}:\mathbb{E}_{\theta\sim\Pi}[d_{\text{KL}}(\mathbb{Q}\| \mathbb{P}_{\theta})]\leq\epsilon}\ \mathbb{E}_{\xi\sim\mathbb{Q}}[f_{x}(\xi)]=\inf_{\gamma\geq 0 }\ \gamma(\epsilon-G(\tau_{n}))+\gamma\ln\mathbb{E}_{\xi\sim p(\xi\bar{| \theta}_{n})}\ \left[\exp\left(\frac{f_{x}(\xi)}{\gamma}\right)\right].\] (8)

To guarantee that the DRO-BAS objective upper bounds the expected risk under the DGP, the decision-maker aims to choose \(\epsilon\) large enough so that \(\mathbb{P}^{\star}\) is contained in the ambiguity set. The condition in (5) yields a closed-form expression for the optimal radius \(\epsilon^{\star}\) by noting that:

\[\epsilon^{\star}=\mathbb{E}_{\theta\sim\Pi}[d_{\text{KL}}(\mathbb{P}^{\star}\| \mathbb{P}_{\theta})]=d_{\text{KL}}(\mathbb{P}^{\star},\mathbb{P}_{\bar{ \theta}_{n}})+G(\tau_{n}).\] (9)

If the model is well-specified, and hence \(\mathbb{P}^{\star}\) and \(\mathbb{P}_{\bar{\theta}_{n}}\) belong to the same exponential family, it is straightforward to obtain \(\epsilon^{\star}\) based on the prior, posterior and true parameter values. We give examples in Appendix A. In practice, since the true parameter values are unknown, we can approximate \(\epsilon^{\star}\) using the observed samples. It follows that for any \(\epsilon\geq\epsilon^{\star}\geq\epsilon_{\min}(n)\):

\[\mathbb{E}_{\xi\sim\mathbb{P}^{\star}}[f(x,\xi)]\leq\sup_{\mathbb{Q}:\mathbb{E} _{\theta\sim\Pi}[d_{\text{KL}}(Q\|\mathbb{P}_{\theta})]\leq\epsilon}\ \mathbb{E}_{\xi\sim\mathbb{Q}}[f_{x}(\xi)].\]

## 3 The Newsvendor Problem

**Experiment setup.** We evaluate DRO-BAS against the BDRO framework on a univariate Newsvendor problem with a well-specified univariate Gaussian likelihood with unknown mean and variance (Appendix D showcases a misspecified setting). The goal is to choose an inventory level \(0\leq x\leq 50\) of a perishable product with unknown customer demand \(\xi\in\mathbb{R}\) that minimises the cost function \(f(x,\xi)=h\max(0,x-\xi)+b\max(0,\xi-x)\), where \(h\) and \(b\) are the holding cost and backorder cost per unit of the product respectively. We let \(\mathbb{P}^{\star}\) be a univariate Gaussian \(\mathcal{N}(\mu_{\star},\sigma_{\star}^{2})\) with \(\mu_{\star}=25\) and \(\sigma_{\star}^{2}=100\). For random seed \(j=1,\ldots,200\), the training dataset \(\mathcal{D}_{n}^{(j)}\) contains \(n=20\) observations and the test dataset \(\mathcal{D}_{m}^{(j)}\) contains \(m=50\) observations. The conjugate prior and posterior are normal-gamma distributions (Appendix A.2). \(N\) is the total number of samples from each model. For each seed \(j\), we run DRO-BAS and BDRO with \(N=25,100,900\) and across 21 different values of \(\epsilon\) ranging from 0.05 to 3. For DRO-BAS, \(N\) is the number of samples from \(p(\xi\mid\bar{\theta}_{n})\) and for BDRO, \(N=N_{\theta}\times N_{\xi}\) where \(N_{\theta}\) is the number of posterior samples and \(N_{\xi}\) likelihood samples due to the double expectation present; we set \(N_{\theta}\) = \(N_{\xi}\) to compare models on an equal \(N\) total samples regime. For a given \(\epsilon\), we calculate the out-of-sample mean \(m(\epsilon)\) and variance \(v(\epsilon)\) of the objective function \(f(x_{\epsilon}^{(j)},\hat{\xi}_{i})\) over all \(\hat{\xi}_{i}\in\mathcal{D}_{m}^{(j)}\) and over all seeds \(j=1,\ldots,200\), where \(x_{\epsilon}^{(j)}\) is the optimal solution on training dataset \(\mathcal{D}_{n}^{(j)}\) (see Appendix C).

Analysis.Figure 2 shows that, for small sample size \(N=25,100\), our framework _dominates_ BDRO in the sense that DRO-BAS forms a Pareto front for the out-of-sample mean-variance tradeoff of the objective function \(f\). That is, for any \(\epsilon_{1}\), let \(m_{\text{BDRO}}(\epsilon_{1})\) and \(v_{\text{BDRO}}(\epsilon_{1})\) be the out-of-sample mean and variance respectively of BDRO: then there exists \(\epsilon_{2}\) with out-of-sample mean \(m_{\text{BAS}}(\epsilon_{2})\) and variance \(v_{\text{BAS}}(\epsilon_{2})\) of BAS-DRO such that \(m_{\text{BAS}}(\epsilon_{2})<m_{\text{BDRO}}(\epsilon_{1})\) and \(v_{\text{BAS}}(\epsilon_{2})<v_{\text{BDRO}}(\epsilon_{1})\). When \(N=900\), Figure 2 shows DRO-BAS and BDRO lie roughly on the same Pareto front. To summarise, BDRO requires more samples \(N\) than DRO-BAS for good out-of-sample performance,

\begin{table}
\begin{tabular}{l l l l} \hline \hline \(p(\xi\mid\theta)\) & \(\Pi(\theta\mid\tau_{n})\) & \(\bar{\theta}_{n}\) & \(G(\tau_{n})\) \\ \hline \(\mathcal{N}(\xi\mid\mu,\sigma^{2})\) & \(\mathcal{N}(\mu\mid\mu_{n},\sigma_{n}^{2})\) & \(\mu_{n},\sigma^{2}\) & \(\frac{\sigma_{n}^{2}}{2\sigma_{n}^{2}}\) \\ \(\mathcal{N}(\xi\mid\mu,\lambda^{-1})\) & \(\operatorname{NG}(\mu,\lambda\mid\mu_{n},\kappa_{n},\alpha_{n},\beta_{n})\) & \(\mu_{n},\frac{\beta_{n}}{\alpha_{n}}\) & \(\frac{1}{2}\left(\frac{1}{\kappa_{n}}+\ln\alpha_{n}-\psi(\alpha_{n})\right)\) \\ \(\operatorname{Exp}(\xi\mid\theta)\) & \(\operatorname{Ga}(\theta\mid\alpha_{n},\beta_{n})\) & \(\frac{\alpha_{n}}{\beta_{n}}\) & \(\ln\alpha_{n}-\psi(\alpha_{n})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Examples for Theorem 1 of the parameter \(\bar{\theta}_{n}\) and the function \(G(\tau_{n})\) for different likelihoods \(p(\xi\mid\theta)\) with conjugate posterior \(\Pi(\theta\mid\tau_{n})\) and posterior hyperparameters \(\tau_{n}\). The normal, normal-gamma, exponential, and gamma distributions are denoted \(\mathcal{N}\), \(\operatorname{NG}\), \(\operatorname{Exp}\), and \(\operatorname{Ga}\) respectively. See the supplementary material for the definitions of \(\tau_{n}\) and the derivations of \(\bar{\theta}_{n}\) and \(G(\tau_{n})\).

Figure 2: The out-of-sample mean-variance tradeoff (in bold) while varying \(\epsilon\) for DRO-BAS and BDRO when the total number of samples from the model is 25 (left), 100 (middle), and 900 (right).

likely because BDRO must evaluate a double expectation over the posterior and likelihood, whilst DRO-BAS only samples from \(p(\xi\mid\bar{\theta}_{n})\). For fixed \(N\), the solve times for DRO-BAS and BDRO are broadly comparable (see Appendix C).

## 4 Discussion

We proposed a novel approach to Bayesian decision-making under uncertainty through a DRO objective based on posterior-informed Bayesian ambiguity sets. The resulting optimisation problem is a single-stage stochastic program with closed-form formulation for a variety of exponential-family models. The suggested methodology has good out-of-sample performance, as showcased in Figure 2. In our recent work (Dellaporta et al., 2024), we have also extended DRO-BAS to a general formulation for exponential family models and investigated alternative Bayesian ambiguity sets based on the posterior predictive distribution. Finally, whilst DRO-BAS offers protection against distributional ambiguity, the dependence of DRO-BAS on the Bayesian posterior makes it vulnerable to model misspecification. Future work will explore robust Bayesian ambiguity sets that address model misspecification through robust posteriors and discrepancies.

## Acknowledgments and Disclosure of Funding

CD acknowledges support from EPSRC grant [EP/T51794X/1] as part of the Warwick CDT in Mathematics and Statistics. PO and TD acknowledge support from a UKRI Turing AI acceleration Fellowship [EP/V02678X/1] and a Turing Impact Award from the Alan Turing Institute. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC-BY) license to any Author Accepted Manuscript version arising from this submission.

## References

* A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z. Kolter (2019)Differentiable convex optimization layers. Advances in neural information processing systems32. Cited by: SS1.
* R. Agrawal and T. Horel (2021)Optimal bounds between f-divergences and integral probability metrics. Journal of Machine Learning Research22 (128), pp. 1-59. Cited by: SS1.
* C. M. Bishop (2006)Pattern recognition and machine learning. Springer google schola2, pp. 1122-1128. Cited by: SS1.
* C. Dellaporta, P. O'Hara, and T. Damoulas (2024)Decision making under the exponential family: distributionally robust optimisation with bayesian ambiguity sets. arXiv preprint arXiv:2411.16829. Cited by: SS1.
* A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin (1995)Bayesian data analysis. Chapman and Hall/CRC. Cited by: SS1.
* J. Gotoh, M. J. Kim, and A. E. B. Lim (2021)Calibration of distributionally robust empirical optimization models. Operations Research69 (5), pp. 1630-1650. Cited by: SS1.
* Z. Hu and L. J. Hong (2013)Kullback-Leibler divergence constrained distributionally robust optimization. Available at Optimization Online1 (2), pp. 9. Cited by: SS1.
* G. Iyengar, H. Lam, and T. Wang (2023)Hedging against complexity: distributionally robust optimization with parametric approximation. In International Conference on Artificial Intelligence and Statistics, pp. 9976-10011. Cited by: SS1.
* D. Kuhn, P. M. Esfahani, V. A. Nguyen, and S. Shafieezadeh-Abadeh (2019)Wasserstein distributionally robust optimization: theory and applications in machine learning. In Operations Research & Management Science in the Age of Analytics, pp. 130-166. Cited by: SS1.
* P. Michel, T. Hashimoto, and G. Neubig (2021)Modeling the second player in distributionally robust optimization. arXiv preprint arXiv:2103.10282. Cited by: SS1.
* P. Michel, P. M. Esfahani, and G. Neubig (2021)Modeling the second player in distributionally robust optimization. arXiv preprint arXiv:2103.10282. Cited by: SS1.

Michel, P., Hashimoto, T., and Neubig, G. (2022). Distributionally robust models with parametric likelihood ratios. _arXiv preprint arXiv:2204.06340_.
* Murphy (2023) Murphy, K. P. (2023). _Probabilistic machine learning: Advanced topics_. MIT press.
* Shapiro et al. (2023) Shapiro, A., Zhou, E., and Lin, Y. (2023). Bayesian distributionally robust optimization. _SIAM Journal on Optimization_, 33(2):1279-1304.
* Staib and Jegelka (2019) Staib, M. and Jegelka, S. (2019). Distributionally robust optimization and generalization in kernel methods. _Advances in Neural Information Processing Systems_, 32.
* Stratos (2023) Stratos, K. (2023). The Gaussian distribution from scratch. Technical report, Department of Computer Science, Rutgers University.
* Walker (2013) Walker, S. G. (2013). Bayesian inference with misspecified models. _Journal of statistical planning and inference_, 143(10):1621-1633.
* Wu et al. (2018) Wu, D., Zhu, H., and Zhou, E. (2018). A Bayesian risk approach to data-driven stochastic optimization: Formulations and asymptotics. _SIAM Journal on Optimization_, 28(2):1588-1612.

**Supplementary Material**

The Supplementary Material is organised as follows: Appendix A provides details for all the exponential family models discussed in Table 1, while Appendix B contains the proofs of all mathematical results appearing in the main text. Appendix C provides additional experimental details for the Newsvendor problem in Section 3. Finally, in Appendix D we present experimental results for the newsvendor problem example with a misspecified model.

## Appendix A Special cases

We derive the values of \(G(\tau_{n})\) and \(\bar{\theta}_{n}\) for different likelihoods and conjugate prior/posterior in Table 1. Each subsection contains a corollary with the result in Table 1.

### Gaussian model with unknown mean and known variance

Let the random variable \(\xi\) be univariate and have continuous support. We assume the variance \(\sigma^{2}\) of \(\xi\) is known. We estimate the mean of a univariate Gaussian distribution with known variance \(\sigma^{2}\). The example can be found in (Bishop, 2006, Section 2.3.6). We define our parameter \(\theta\) to be the unknown mean \(\mu\) and we place a Gaussian prior \(\pi(\mu)\) over it.

**Definition 1** (Gaussian with unknown mean and known variance).: _The likelihood is \(p(\xi\mid\mu)=\mathcal{N}(\mu,\sigma^{2})\), the prior over \(\mu\) is \(\pi(\mu)=\mathcal{N}(\mu_{0},\sigma_{0}^{2})\), and the conjugate posterior is \(\Pi(\mu\mid\mathcal{D})=\mathcal{N}(\mu\mid\mu_{n},\sigma_{n}^{2})\), where_

\[\mu_{n}:=\frac{\sigma^{2}}{n\sigma_{0}^{2}+\sigma^{2}}\mu_{0}+\frac{n\sigma_{0 }^{2}}{n\sigma_{0}^{2}+\sigma^{2}}\hat{\mu}\qquad\quad\frac{1}{\sigma_{n}^{2} }:=\frac{1}{\sigma_{0}^{2}}+\frac{n}{\sigma^{2}}\qquad\quad\hat{\mu}:=\frac{1} {n}\sum_{i=1}^{n}\;\xi_{i}.\]

**Lemma 2**.: _Let \(\mu_{n}\in\mathbb{R}\) and \(\sigma,\sigma_{n}\in\mathbb{R}_{+}\). Then_

\[\mathbb{E}_{\mu\sim\mathcal{N}(\mu_{n},\sigma_{n}^{2})}\left[\log\mathcal{N}( \mu,\sigma^{2})\right]=\log\mathcal{N}(\mu_{n},\sigma^{2})-\frac{\sigma_{n}^{2 }}{2\sigma^{2}}.\]

Proof.: The result is a special case of (Stratos, 2023, Lemma H.10). 

**Corollary 1**.: _When the likelihood is a Gaussian distribution with unknown mean and known variance \(\sigma^{2}\) and the prior and posterior are Gaussian distributions (see Definition 1), then Theorem 1 holds with \(\bar{\theta}_{n}=\mu_{n}\) and \(G(\tau_{n})=\frac{\sigma_{n}^{2}}{2\sigma^{2}}\)._

Proof.: Lemma 2 shows that the condition (5) in Lemma 1 holds, thus Theorem 1 follows. 

Tolerance level \(\epsilon\)In the well-specified case, where we assume that \(\mathbb{P}^{\star}:=\mathbb{P}^{\star}_{\theta}\) for some \(\theta^{\star}\in\Theta\), it is easy to obtain the required size of the ambiguity set exactly. Let \(\theta^{\star}:=\mu^{\star}\) and \(\mathbb{P}^{\star}:=N(\mu^{\star},\sigma^{2})\). By Corollary 1 it follows that:

\[\mathbb{E}_{\theta\sim N(\mu_{n},\sigma_{n}^{2})}\left[d_{\text{KL}}(\mathbb{P }^{\star},\mathbb{P}_{\theta})\right]=\frac{(\mu^{\star}-\mu_{n})^{2}+\sigma_ {n}^{2}}{2\sigma^{2}}.\] (10)

So for a fixed finite sample \(\xi_{1:n}\sim\mathbb{P}^{\star}\), if \(\epsilon\geq\epsilon^{\star}:=\frac{(\mu^{\star}-\mu_{n})^{2}+\sigma_{n}^{2}}{2 \sigma^{2}}\), it follows that DRO-BAS upper-bounds the target optimisation objective:

\[\mathbb{E}_{\xi\sim\mathbb{P}^{\star}}[f_{x}(\xi)]\leq\sup_{\mathbb{Q}:\mathbb{ E}_{\theta\sim\Pi(\cdot|\xi_{1:n})}[d_{\text{KL}}(Q\parallel\mathbb{P}_{\theta})] \leq\epsilon}\;\mathbb{E}_{\xi\sim\mathbb{Q}}[f_{x}(\xi)].\]

In practice, since \(\mu^{\star}\) is unknown, \(\epsilon^{\star}\) can be approximated by using the sample mean.

### Gaussian Model with Unknown Mean and Variance

In this section, we consider a Bayesian model that estimates the unknown mean and variance of a uni-variate Gaussian distribution. We define our model in Definition 2, then prove some preliminary results for the normal-gamma distribution, before proving our main result in Lemma 4.

**Definition 2** (Unknown mean and variance of a Gaussian).: _Following (Murphy, 2023, Chapter 3.4.3), we place a normal-gamma prior over the mean \(\mu\) and precision \(\lambda=\sigma^{-2}\). The normal-gamma prior is the conjugate to a Gaussian likelihood and results in a normal-gamma posterior distribution._

\[\text{Likelihood:}\quad p(\xi\mid\mu,\lambda)=\mathcal{N}(\xi\mid \mu,\lambda^{-1})\] \[\text{Prior:}\quad\quad\pi(\mu,\lambda)=NG(\mu,\lambda\mid\mu_{0 },\kappa_{0},\alpha_{0},\beta_{0})=\mathcal{N}(\mu\mid\mu_{0},(\kappa_{0} \lambda)^{-1})\cdot Ga(\lambda\mid\alpha_{0},\beta_{0})\] \[\text{Posterior:}\quad\quad\pi(\mu,\lambda\mid\mathcal{D})=NG(\mu, \lambda\mid\mu_{n},\kappa_{n},\alpha_{n},\beta_{n})=\mathcal{N}(\mu\mid\mu_{n },(\lambda\kappa_{n})^{-1})\cdot Ga(\lambda\mid\alpha_{n},\beta_{n})\]

_where_

\[\mu_{n} :=\frac{\kappa_{0}\mu_{0}+n\bar{\xi}_{n}}{n+\kappa_{0}},\qquad \kappa_{n}:=\kappa_{0}+n,\qquad\alpha_{n}:=\alpha_{0}+\frac{n}{2},\qquad\bar{ \xi}_{n}=\frac{1}{n}\sum_{i-1}^{n}\;\xi_{i},\] \[\beta_{n} :=\beta_{0}+\frac{1}{2}\sum_{i=1}^{n}\;(\xi_{i}-\bar{\xi}_{n})^{2 }+\frac{\kappa_{0}n(\bar{\xi}_{n}-\mu_{0})^{2}}{2(\kappa_{0}+n)}.\]

In Lemma 4, we derive condition Equation (5) for a Gaussian model with unknown mean and unknown variance. Before proceeding, we need to define the gamma and digamma functions and recall the moments of the normal-gamma distribution.

**Definition 3**.: _The gamma function \(\Gamma:\mathbb{N}\to\mathbb{R}\) and digamma function \(\psi:\mathbb{N}\to\mathbb{R}\) are_

\[\Gamma(z) :=(z-1)! \psi(z) :=\frac{\mathrm{d}}{\mathrm{d}z}\ln\Gamma(z).\]

**Lemma 3**.: _Let \(NG(\mu,\lambda\mid\mu_{n},\kappa_{n},\alpha_{n},\beta_{n})\) be a normal-gamma distribution with parameters \(\mu_{n}\in\mathbb{R}\) and \(\kappa_{n},\alpha_{n},\beta_{n}\in\mathbb{R}_{+}\). The moments of the normal-gamma distribution are_

\[\mathbb{E}_{NG}[\ln\lambda] =\psi(\alpha_{n})-\ln\beta_{n}, \mathbb{E}_{NG}[\lambda] =\frac{\alpha_{n}}{\beta_{n}}, \mathbb{E}_{NG}[\lambda\mu] =\mu_{n}\frac{\alpha_{n}}{\beta_{n}}, \mathbb{E}_{NG}[\lambda\mu^{2}] =\frac{1}{\kappa_{n}}+\mu_{n}^{2}\frac{\alpha_{n}}{\beta_{n}}.\]

_where \(\psi:\mathbb{N}\to\mathbb{R}\) is the digamma function from Definition 3._

**Lemma 4**.: _Let \(\mu_{n}\in\mathbb{R}\) and \(\kappa_{n},\alpha_{n},\beta_{n}\in\mathbb{R}_{+}\). Then_

\[\mathbb{E}_{NG(\mu,\lambda\mid\mu_{n},\kappa_{n},\alpha_{n},\beta_{n})}\left[ \ln\mathcal{N}(\xi\mid\mu,\lambda^{-1})\right] =\ln\mathcal{N}\left(\xi\mid\mu_{n},\frac{\beta_{n}}{\alpha_{n}}\right)-\frac{ 1}{2}\left(\frac{1}{\kappa_{n}}+\ln\alpha_{n}-\psi(\alpha_{n})\right)\]

_where \(\psi:\mathbb{N}\to\mathbb{R}\) is the digamma function from Definition 3._

Proof.: First, observe that the natural logarithm of the Gaussian distribution may be re-written as

\[\ln\mathcal{N}(\xi\mid\mu,\lambda^{-1}) =\ln\left(\frac{\lambda^{\frac{1}{2}}}{(2\pi)^{\frac{1}{2}}}\exp \left(-\frac{\lambda}{2}(\xi-\mu)^{2}\right)\right)\] \[=\frac{1}{2}\ln\lambda-\frac{1}{2}\ln 2\pi-\frac{\lambda}{2}(\xi- \mu)^{2}\] \[=\frac{1}{2}\ln\lambda-\frac{1}{2}\ln 2\pi-\frac{1}{2}\lambda\xi^{2}+ \lambda\mu\xi-\frac{1}{2}\lambda\mu^{2}.\]

In what follows, for shorthand, we denote the expectation \(\mathbb{E}_{NG(\mu,\lambda\mid\mu_{n},\kappa_{n},\alpha_{n},\beta_{n})}\) as \(\mathbb{E}_{\mu,\lambda\sim NG}\):\[\mathbb{E}_{\mu,\lambda\sim NG}\left[\ln\mathcal{N}(\xi\mid\mu, \lambda^{-1})\right]\] \[\stackrel{{(i)}}{{=}}\mathbb{E}_{\mu,\lambda\sim NG} \left[\frac{1}{2}\ln\lambda-\frac{1}{2}\ln 2\pi-\frac{1}{2}\lambda\xi^{2}+ \lambda\mu\xi-\frac{1}{2}\lambda\mu^{2}\right]\] \[\stackrel{{(ii)}}{{=}}-\frac{1}{2}\ln 2\pi+\frac{1}{2} \mathbb{E}_{\mu,\lambda\sim NG}\left[\ln\lambda\right]-\frac{1}{2}\xi^{2}\cdot \mathbb{E}_{\mu,\lambda\sim NG}\left[\lambda\right]+\xi\cdot\mathbb{E}_{\mu, \lambda\sim NG}\left[\lambda\mu\right]-\frac{1}{2}\mathbb{E}_{\mu,\lambda\sim NG }\left[\lambda\mu^{2}\right]\] \[\stackrel{{(iii)}}{{=}}-\frac{1}{2}\ln 2\pi+\frac{1}{2} (\psi(\alpha_{n})-\ln\beta_{n})-\frac{1}{2}\xi^{2}\frac{\alpha_{n}}{\beta_{n} }+\xi\mu\frac{\alpha_{n}}{\beta_{n}}-\frac{1}{2}\left(\frac{1}{\kappa_{n}}+\mu _{n}^{2}\frac{\alpha_{n}}{\beta_{n}}\right)\] \[\stackrel{{(iv)}}{{=}}-\frac{1}{2}\ln 2\pi+\frac{1}{2} \left(\psi(\alpha_{n})-\ln\beta_{n}-\frac{1}{\kappa_{n}}\right)-\frac{\alpha_ {n}}{2\beta_{n}}\left(\xi-\mu_{n}\right)^{2}\] \[\stackrel{{(v)}}{{=}}-\frac{1}{2}\ln 2\pi-\frac{1}{2} \left(\ln\beta_{n}-\ln\alpha_{n}+\ln\alpha_{n}-\psi(\alpha_{n})+\frac{1}{ \kappa_{n}}\right)+\ln\exp\left(-\frac{1}{2\frac{\beta_{n}}{\alpha_{n}}}\left( \xi-\mu_{n}\right)^{2}\right)\] \[\stackrel{{(vii)}}{{=}}-\frac{1}{2} \left(\ln\alpha_{n}-\psi(\alpha_{n})+\frac{1}{\kappa_{n}}\right)-\frac{1}{2} \ln\left(2\pi\frac{\beta_{n}}{\alpha_{n}}\right)+\ln\exp\left(-\frac{1}{2 \frac{\beta_{n}}{\alpha_{n}}}\left(\xi-\mu_{n}\right)^{2}\right)\] \[\stackrel{{(vii)}}{{=}}-\frac{1}{2} \left(\frac{1}{2\alpha_{n}}+I_{\alpha_{n}}+\frac{1}{\kappa_{n}}\right)+\ln \left(\frac{1}{\sqrt{2\pi\frac{\beta_{n}}{\alpha_{n}}}}\exp\left(-\frac{1}{2 \frac{\beta_{n}}{\alpha_{n}}}(\xi-\mu_{n})^{2}\right)\right)\] \[\stackrel{{(viii)}}{{=}}-\frac{1}{2} \left(\ln\alpha_{n}-\psi(\alpha_{n})+\frac{1}{\kappa_{n}}\right)+\ln\mathcal{ N}\left(\xi\mid\mu_{n},\frac{\beta_{n}}{\alpha_{n}}\right)\]

where in equation (i) we take the expectation over the normal-gamma distribution; (ii) we apply linearity of expectation; (iii) we use the moment-generating functions from Lemma 3; (iv) we complete the square; (v) we add and subtract \(\ln\alpha_{n}\); (vi) and (vii) we re-arrange and apply log identities; and finally in (viii) we use the definition of a Gaussian probability density function. 

**Corollary 2**.: _When the likelihood is a Gaussian distribution with unknown mean and variance, and the conjugate prior and posterior are normal-gamma distributions (see Definition 2), then Theorem 1 holds with \(\bar{\theta}_{n}=(\mu_{n},\frac{\beta_{n}}{\alpha_{n}})\) and \(G(\tau_{n})=\frac{1}{2}\left(\ln\alpha_{n}-\psi(\alpha_{n})+\frac{1}{\kappa_{ n}}\right)\)._

Proof.: Lemma 4 shows that the condition (5) in Lemma 1 holds, thus Theorem 1 follows. 

Tolerance level \(\epsilon\)In the well-specified case, where we assume that \(\mathbb{P}^{\star}:=\mathbb{P}^{\star}_{\theta}\) for some \(\theta^{\star}\in\Theta\), it is easy to obtain the required size of the ambiguity set exactly. Let \(\theta^{\star}:=(\mu^{\star},{\lambda^{\star}}^{-1})\) and \(\mathbb{P}^{\star}:=N(\mu^{\star},{\lambda^{\star}}^{-1})\). Using Corollary 2 we obtain:

\[\mathbb{E}_{\mu,\lambda\sim NG(\mu,\lambda\mid\mu_{n},\kappa_{n}, \alpha_{n},\beta_{n})}\left[d_{\text{KL}}(\mathbb{P}^{\star},\mathcal{N}(\xi \mid\mu,\lambda^{-1}))\right]\] \[\qquad=d_{\text{KL}}\left(\mathbb{P}^{\star}\parallel\mathcal{N} \left(\mu_{n},\frac{\beta_{n}}{\alpha_{n}}\right)\right)+\frac{1}{2}\left( \frac{1}{\kappa_{n}}+\ln\alpha_{n}-\psi(\alpha_{n})\right)\] \[\qquad=\ln\left(\sqrt{{\lambda^{\star}}\frac{\beta_{n}}{\alpha_{n }}}\right)+\frac{{\lambda^{\star}}^{-1}+(\mu^{\star}-\mu_{n})^{2}}{2\frac{ \beta_{n}}{\alpha_{n}}}-\frac{1}{2}+\frac{1}{2}\left(\frac{1}{\kappa_{n}}+\ln \alpha_{n}-\psi(\alpha_{n})\right)\] \[\qquad=\frac{1}{2}\left(\ln\left({\lambda^{\star}}\beta_{n} \right)+\frac{{\lambda^{\star}}^{-1}+(\mu^{\star}-\mu_{n})^{2}}{2\frac{\beta_{n }}{\alpha_{n}}}-1+\frac{1}{\kappa_{n}}-\psi(\alpha_{n})\right).\]

### Exponential likelihood with conjugate gamma prior

**Definition 4**.: _The likelihood \(p(\xi\mid\theta)\) is an exponential distribution \(\operatorname{Exp}(\xi\mid\theta)\) where \(\theta>0\) is the rate parameter. The prior \(\pi(\theta)\) is a gamma distribution \(\operatorname{Ga}(\theta\mid\alpha_{0},\beta_{0})\) with shape \(\alpha_{0}>0\) and rate \(\beta_{0}>0\). The parameters \(\alpha_{n},\beta_{n}\) of the posterior \(\pi(\theta\mid\mathcal{D})=\operatorname{Ga}(\theta\mid\alpha_{n},\beta_{n})\) are given by \(\alpha_{n}=\alpha_{0}+n\) and \(\beta_{n}=\beta_{0}+\sum_{\xi_{i}\in\mathcal{D}}\,\xi_{i}\)._

**Lemma 5**.: _When the likelihood is an exponential distribution with gamma prior and posterior (see Definition 4), then_

\[\mathbb{E}_{\mathrm{Ga}(\theta|\alpha_{n},\beta_{n})}\left[\ln\mathrm{Exp}(\xi\mid \theta)\right]=\ln\mathrm{Exp}\left(\xi\mid\frac{\alpha_{n}}{\beta_{n}}\right)+ \psi(\alpha_{n})-\ln\alpha_{n}.\]

Proof.: Starting from the left-hand side, we take the log of the PDF of the exponential distribution, then use the logarithm expectation of the gamma distribution, and finally re-arrange using log identities:

\[\mathbb{E}_{\mathrm{Ga}(\theta|\alpha_{n},\beta_{n})}\left[\ln \mathrm{Exp}(\xi\mid\theta)\right] =\mathbb{E}_{\mathrm{Ga}(\theta|\alpha_{n},\beta_{n})}\left[\ln \theta-\theta\xi\right]\] \[=\psi(\alpha_{n})-\ln\beta_{n}-\frac{\alpha_{n}}{\beta_{n}}\xi\] \[=\psi(\alpha_{n})-\ln\alpha_{n}+\ln\frac{\alpha_{n}}{\beta_{n}}- \frac{\alpha_{n}}{\beta_{n}}\xi\] \[=\psi(\alpha_{n})-\ln\alpha_{n}+\ln\left(\frac{\alpha_{n}}{\beta _{n}}\exp\left(-\frac{\alpha_{n}}{\beta_{n}}\xi\right)\right)\] \[=\psi(\alpha_{n})-\ln\alpha_{n}+\ln\mathrm{Exp}\left(\xi\mid\frac {\alpha_{n}}{\beta_{n}}\right).\]

The last line follows by the definition of the PDF of the exponential distribution. 

**Corollary 3**.: _When the likelihood is an exponential distribution with gamma prior and posterior, then Theorem 1 holds with \(\bar{\theta}_{n}=\frac{\alpha_{n}}{\beta_{n}}\) and \(G(\tau_{n})=\psi(\alpha_{n})-\ln\alpha_{n}\)._

Proof.: Lemma 5 shows that the condition (5) in Lemma 1 holds, thus Theorem 1 follows. 

Tolerance level \(\epsilon\)In the well-specified case, where we assume that \(\mathbb{P}^{\star}:=\mathbb{P}^{\star}_{\theta}\) for some \(\theta^{\star}\in\Theta\), it is easy to obtain the required size of the ambiguity set exactly. Let \(\theta^{\star}\) be the true rate parameter, i.e. \(\mathbb{P}^{\star}:=\mathrm{Exp}(\theta^{\star})\). Using Corollary 3 we obtain:

\[\mathbb{E}_{\mathrm{Ga}(\theta|\alpha_{n},\beta_{n})}\left[d_{ \text{KL}}(\mathbb{P}^{\star},\mathrm{Exp}(\theta)\right]\] \[\qquad=d_{\text{KL}}\left(\mathbb{P}^{\star}\parallel\,\mathrm{ Exp}\left(\frac{\alpha_{n}}{\beta_{n}}\right)\right)+\psi(\alpha_{n})-\ln( \alpha_{n})\] \[\qquad=\ln(\theta^{\star})-\ln\left(\frac{\alpha_{n}}{\beta_{n}} \right)+\frac{\alpha_{n}}{\beta_{n}\theta^{\star}}-1+\psi(\alpha_{n})-\ln( \alpha_{n}).\]

## Appendix B Proofs of Theoretical Results

### Proofs of DRO-BAS upper bound in Equation (4)

Before proving the required upper bound, we recall the definition of the KL divergence and its convex conjugate.

**Definition 5** (KL-divergence).: _Let \(\mu,\nu\in\mathcal{P}(\Xi)\) and assume \(\mu\) is absolutely continuous with respect to \(\nu\) (\(\mu\ll\nu\)). The \(KL\)-divergence of \(\mu\) with respect to \(\nu\) is defined as:_

\[d_{\text{KL}}(\mu\|\nu):=\int_{\Xi}\ln\left(\frac{\mu(d\xi)}{\nu(d\xi)}\right) \mu(d\xi).\]

**Lemma 6** (Conjugate of the KL-divergence).: _Let \(\nu\in\mathcal{P}(\Xi)\) be non-negative and finite. The convex conjugate \(d^{\star}_{\text{KL}}(\cdot\|\nu)\) of \(d_{\text{KL}}(\cdot\|\nu)\) is_

\[d^{\star}_{\text{KL}}(\cdot\|\nu)(h)=\ln\left(\int_{\Xi}\exp(h)d\nu\right).\]

Proof.: See Proposition 28 and Example 7 in Agrawal and Horel (2021).

Proof of Equation 4.: The result follows from a standard Lagrangian duality argument and an application of Jensen's inequality. More specifically, we introduce a Lagrangian variable \(\gamma\geq 0\) for the expected-ball constraint on the left-hand side of (4) as follows:

\[\sup_{\mathbb{Q}:\mathbb{E}_{\theta\sim\Pi}[d_{\text{KL}}(Q\|\mathbb{ P}_{\theta})]\leq\epsilon}\,\mathbb{E}_{Q}[f_{x}] \stackrel{{(i)}}{{\leq}}\inf_{\gamma\geq 0}\ \sup_{\mathbb{Q}\in\mathcal{P}(\Xi)}\,\mathbb{E}_{Q}[f_{x}]+ \gamma\epsilon-\gamma\mathbb{E}_{\Pi}\left[d_{\text{KL}}(\mathbb{Q}\|\mathbb{ P}_{\theta})\right]\] \[\stackrel{{(i)}}{{=}}\inf_{\gamma\geq 0}\ \gamma\epsilon+\sup_{ \mathbb{Q}\in\mathcal{P}(\Xi)}\,\mathbb{E}_{Q}[f_{x}]-\mathbb{E}_{\Pi}\left[ \gamma d_{\text{KL}}(\mathbb{Q}\|\mathbb{P}_{\theta})\right]\] \[\stackrel{{(ii)}}{{=}}\inf_{\gamma\geq 0}\ \gamma \epsilon+\left(\mathbb{E}_{\Pi}\left[\gamma d_{\text{KL}}(\cdot\|\mathbb{ P}_{\theta})\right]\right)^{\star}\left(f_{x}\right)\] \[\stackrel{{(v)}}{{=}}\inf_{\gamma\geq 0}\ \gamma \epsilon+\mathbb{E}_{\Pi}\left[\gamma\ln\mathbb{E}_{\mathbb{P}_{\theta}} \left[\exp\left(\frac{f_{x}}{\gamma}\right)\right]\right].\]

Inequality (i) holds by weak duality. Equality (ii) holds by linearity of expectation and a simple rearrangement. Equality (iii) holds by the definition of the conjugate function. Inequality (iv) holds by Jensen's inequality \((\mathbb{E}[\cdot])^{\star}\leq\mathbb{E}[(\cdot)^{\star}]\) because the conjugate is a convex function. Equality (v) holds by Lemma 6 and the fact that for \(\gamma\geq 0\) and function \(\phi\), \((\gamma\phi)^{\star}(y)=\gamma\phi^{\star}(y/\gamma)\). 

### Proof of Lemma 1

Starting from the left-hand side, we have

\[\mathbb{E}_{\Pi}\left[d_{\text{KL}}(\mathbb{Q}\|\mathbb{P}_{ \theta})\right] \stackrel{{(i)}}{{=}}\mathbb{E}_{\theta\sim\pi( \theta|\mathcal{D})}\left[\int_{\Xi}q(\xi)\ln\left(\frac{q(\xi)}{p(\xi\mid \theta)}\right)\text{d}\xi\right]\] \[\stackrel{{(ii)}}{{=}}\mathbb{E}_{\theta\sim\pi( \theta|\mathcal{D})}\left[\int_{\Xi}q(\xi)\ln\left(q(\xi)\right)-q(\xi)\ln \left(p(\xi\mid\theta)\right)\text{d}\xi\right]\] \[\stackrel{{(iii)}}{{=}}\int_{\Xi}q(\xi)\ln\left(q(\xi )\right)-q(\xi)\cdot\mathbb{E}_{\theta\sim\pi(\theta|\mathcal{D})}\left[\ln \left(p(\xi\mid\theta)\right)\right]\text{d}\xi\] \[\stackrel{{(iv)}}{{=}}\int_{\Xi}q(\xi)\ln\left(q(\xi )\right)-q(\xi)\cdot\left(\ln p(\xi\mid\bar{\theta}_{n})-G(\tau_{n})\right) \text{d}\xi\] \[\stackrel{{(v)}}{{=}}\int_{\Xi}q(\xi)\ln\left(\frac{ q(\xi)}{p(\xi\mid\bar{\theta}_{n})}\right)\text{d}\xi+\int_{\Xi}\ q(\xi)\cdot G(\tau_{n})\,\text{d}\xi\] \[\stackrel{{(vi)}}{{=}}d_{\text{KL}}(\mathbb{Q}\| \mathbb{P}_{\bar{\theta}_{n}})+\mathbb{E}_{\mathbb{Q}}[G(\tau_{n})]\] \[\stackrel{{(vii)}}{{=}}d_{\text{KL}}(q(\xi)\|p(\xi \mid\bar{\theta}_{n}))+G(\tau_{n}).\]

where (i) is by the definition of the KL-divergence; (ii) follows by \(\log\) properties; (iii) holds by linearity of expectation; (iv) holds by condition (5) in Lemma 1; (v) holds by rearrangement and properties of \(\log\); (vi) holds by the definition of the KL-divergence and the definition of \(\mathbb{E}_{\mathbb{Q}}\); and (vii) holds by the expected value of a constant.

### Proof of Theorem 1

We begin by restating the Lagrangian dual from the proof of Equation (4), but with the added claim that strong duality holds between the primal and dual problems:

\[\sup_{\mathbb{Q}:\mathbb{E}_{\theta\sim\Pi}[d_{\text{KL}}(\mathbb{Q}\| \mathbb{P}_{\theta})]\leq\epsilon}\,\mathbb{E}_{\mathbb{Q}}[f_{x}]=\inf_{ \gamma\geq 0}\ \sup_{\mathbb{Q}\in\mathcal{P}(\Xi)}\,\mathbb{E}_{\mathbb{Q}}[f_{x}]+ \gamma\epsilon-\gamma\mathbb{E}_{\Pi}\left[d_{\text{KL}}(\mathbb{Q}\| \mathbb{P}_{\theta})\right].\] (11)Proof.: The conditions under which our claim of strong duality holds will be proved later. Next, we substitute the right-hand side of equation (vi) above into the dual problem in (11):

\[\sup_{\mathbb{Q}:\mathbb{E}_{\theta\sim\Pi}[d_{\text{KL}}(\mathbb{Q} \parallel p_{\theta})]\leq\varepsilon}\ \mathbb{E}_{\xi\sim\mathbb{Q}}[f_{x}(\xi)]\] \[\qquad=\inf_{\gamma\geq 0}\ \gamma\epsilon+\sup_{\mathbb{Q}\in \mathcal{P}(\Xi)}\ \int_{\Xi}\ f_{x}(\xi)q(\xi)\,\mathrm{d}\xi-\gamma\left(d_{\text{KL}}\left( \mathbb{Q}\parallel p(\xi\mid\bar{\theta}_{n})\right)+G(\tau_{n})\right)\] \[\qquad=\inf_{\gamma\geq 0}\ \gamma\epsilon-\gamma G(\tau_{n})+\left( \gamma\ d_{\text{KL}}\left(\cdot\parallel p(\xi\mid\bar{\theta}_{n})\right) \right)^{\star}(f_{x}(\xi))\] \[\qquad=\inf_{\gamma\geq 0}\ \gamma(\epsilon-G(\tau_{n}))+\gamma \ln\mathbb{E}_{\xi\sim p(\xi\mid\bar{\theta}_{n})}\left[\exp\left(\frac{f_{x} (\xi)}{\gamma}\right)\right],\]

where the second and third equality holds by the definition of the conjugate of the KL-divergence and by Lemma 6.

Finally, it remains to argue that strong duality holds. First, note that the primal problem is a concave optimisation problem with respect to distribution \(\mathbb{Q}\). Second, when \(\epsilon>G(\tau_{n})\), then distribution \(\hat{\mathbb{Q}}=p(\xi\mid\bar{\theta}_{n})\) is a strictly feasible point to the primal constraint because

\[\mathbb{E}_{\theta\sim\Pi}[d_{\text{KL}}(\hat{\mathbb{Q}}\parallel\mathbb{P}_ {\theta})]=0<\epsilon-G(\tau_{n}).\]

## Appendix C Newsvendor Problem - Additional Details

We provide additional details about our Newsvendor experiment in Section 3 when \(\mathbb{P}^{\star}\) is a Gaussian distribution with \(\mu_{\star}=25\) and \(\sigma_{\star}^{2}=100\).

Hyperparameters.The prior and posterior are normal-gamma distributions. We set the prior hyperparameters to be \(\mu_{0}=0\) and \(\kappa_{0},\alpha_{0},\beta_{0}=1\). The derivation of the hyperparameters can be found in Definition 2.

Values of \(\epsilon_{\min}\) and \(\epsilon^{\star}\).From Table 1 and equation (7), the value of \(\epsilon_{\min}\) is 0.047. From equation (9), the average value of \(\epsilon^{\star}\) over all \(J\) seeds is 0.089 with standard deviation 0.048.

Implementation.We implemented the dual problems for DRO-BAS (Theorem 1) and BDRO (Shapiro et al., 2023) in Python using CVXPY version 1.5.2 and the MOSEK solver version 10.1.28. Our implementation uses disciplined parametrized programming (Agrawal et al., 2019) which - after an initial warm start for seed \(j=1\) - allows us to solve subsequent seeds \(j=2,\ldots,J\) rapidly (see Table 2). We used a 12-core Dual Intel Xeon E5-2643 v3 @ 3.4 Ghz with 128GB RAM.

Out-of-sample mean and variance.For a given \(\epsilon\) and seed \(j\), let the optimal solution be \(x^{(j)}(\epsilon)\). We calculate the out-of-sample mean \(m^{(j)}(\epsilon)=\mathbb{E}_{\xi\sim\hat{\mathbb{P}}_{m}^{(j)}}\ [f(x^{(j)}(\epsilon),\xi)]\) and variance \(v^{(j)}(\epsilon)=\text{Var}_{\xi\sim\hat{\mathbb{P}}_{m}^{(j)}}[f(x^{(j)}( \epsilon),\xi)]\) of the objective under the empirical test distribution \(\hat{\mathbb{P}}_{m}^{(j)}\). For a given \(\epsilon\), the out-of-sample mean \(m(\epsilon)\) and variance \(v(\epsilon)\) across all seeds is

\[m(\epsilon)=\frac{1}{J}\sum_{j=1}^{J}m^{(j)}(\epsilon)\ \ \ \ \ \ \ \ v(\epsilon)=\frac{1}{J}\sum_{j=1}^{J}\ v^{(j)}( \epsilon)+\frac{1}{J-1}\sum_{j=1}^{J}\ \left(m^{(j)}(\epsilon)-m(\epsilon)\right)^{2}.\]

The out-of-sample variance \(v(\epsilon)\) is equal to the mean of the variances \(v^{(j)}(\epsilon)\) plus the variance of the means \(m^{(j)}(\epsilon)\)(Gotoh et al., 2021).

Solve time.On the initial warm-start seed \(j=1\), for each \(N\), DRO-BAS solves the dual problem from Theorem 1 faster than the BDRO dual problem. For example, when \(N=900\), DRO-BAS solves problems in 0.27 seconds on average, whilst BDRO solves problem in 5.56 seconds. These results suggest that, for fixed \(N\), if the solve is started from scratch with no warm start, then DRO-BAS will solve instances faster than BDRO. For seeds \(j=2,\ldots,J\), disciplinedparametrized programming (DPP) significantly speeds up the solve for BDRO: the average solve time for seeds \(j=2,\ldots,J\) is 0.40 seconds when \(N=900\). In contrast, DPP does not speed up the solve for DRO-BAS: the average solve time for seeds \(j=2,\ldots,J\) is 0.40 when \(N=900\). We conjecture that the speed up for BDRO using DPP is because BDRO has \(N_{\theta}\) Lagrangian dual variables compared to DRO-BAS having exactly one Lagrangian dual variable. BDRO then benefits from the warm start because it can reuse the presolve effort spent on the \(N_{\theta}\) dual variables spent during the warm start.

## Appendix D Misspecified Supplementary Experiments - Truncated Normal

In this section, we present additional experiments when the data-generating process \(\mathbb{P}^{*}\) is a truncated normal distribution. The truncated normal has mean \(\mu_{*}=10\) and variance \(\sigma_{\star}^{2}=100\). The likelihood is a Gaussian distribution, so our model is misspecified. The conjugate prior and posterior are still normal-gamma distributions with the same hyperparameters as Section 3. The experimental setup is also the same as Section 3: the values of \(\epsilon\), \(N\), \(n\), \(m\), and \(J\) are all specified the same.

Analysis.When the likelihood is misspecified, Figure 3 shows the out-of-sample mean-variance tradeoff is again a Pareto front. This is the same conclusion as the well-specified case in Section 3. Furthermore, when \(N=900\), DRO-BAS has a small advantage on the mean-variance tradeoff.

Solve time.Table 3 shows the same conclusions about the solve time from Appendix C can be made about the solve time for the truncated normal data-generating process.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{4}{c}{Solve time in seconds AVG (STD)} \\ \cline{2-5}  & \multicolumn{2}{c}{\(j=1\)} & \multicolumn{2}{c}{\(j=2,\ldots,J\)} \\ \cline{2-5} \(N\) & DRO-BAS & BDRO & DRO-BAS & BDRO \\ \hline
25 & 0.02 (0.00) & 0.07 (0.02) & 0.01 (0.00) & 0.01 (0.00) \\
100 & 0.03 (0.00) & 0.15 (0.03) & 0.02 (0.00) & 0.03 (0.00) \\
900 & 0.27 (0.02) & 5.56 (0.05) & 0.40 (0.11) & 0.23 (0.02) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Average (AVG) and standard deviation (STD) of the solve time for initial warm start on seed \(j=1\) and for subsequent seeds \(j=2\ldots,J\). Distribution \(\mathbb{P}^{*}\) is a Gaussian \(\mathcal{N}(25,100)\).

Figure 3: The out-of-sample mean-variance tradeoff on the _truncated-normal_ dataset when varying the radius \(\epsilon\) for DRO-BAS and BDRO when the total number of samples from the model is 25 (left), 100 (middle), and 900 (right).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \multicolumn{4}{c}{Solve time in seconds AVG (STD)} \\ \cline{2-5}  & \multicolumn{2}{c}{\(j=1\)} & \multicolumn{2}{c}{\(j=2,\ldots,J\)} \\ \cline{2-5} \(N\) & DRO-BAS & BDRO & DRO-BAS & BDRO \\ \hline
25 & 0.03 (0.00) & 0.07 (0.02) & 0.01 (0.00) & 0.01 (0.00) \\
100 & 0.03 (0.00) & 0.14 (0.02) & 0.02 (0.01) & 0.03 (0.00) \\
900 & 0.27 (0.02) & 5.54 (0.04) & 0.40 (0.11) & 0.23 (0.01) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average (AVG) and standard deviation (STD) of the solve time for initial warm start on seed \(j=1\) and for subsequent seeds \(j=2\ldots,J\). Distribution \(\mathbb{P}^{*}\) is a truncated normal.