# BENO: Boundary-embedded Neural

Operators for Elliptic PDEs

 Haixin Wang

Peking University

wang.hx@stu.pku.edu.cn

&Jiaxin Li

Westlake University

lijiaxin@westlake.edu.cn

&Anubhav Dwivedi

Stanford University

anubhavd@stanford.edu

&Kentaro Hara

Stanford University

kenhara@stanford.edu

&Tailin Wu

Westlake University

wuatilin@westlake.edu.cn

Equal contribution. Work done as an intern at Westlake University. Corresponding author.

###### Abstract

Elliptic partial differential equations (PDEs) are a major class of time-independent PDEs that play a key role in many scientific and engineering domains such as fluid dynamics, plasma physics, and solid mechanics. Recently, neural operators have emerged as a promising technique to solve elliptic PDEs more efficiently by directly mapping the input to solutions. However, existing networks typically neglect complex geometries and inhomogeneous boundary values present in the real world. Here we introduce \(\mathbf{\underline{B}}\)oundary-\(\mathbf{\underline{E}}\)nbedded \(\mathbf{Neural}\)Operators (BENO), a novel neural operator architecture that embeds the complex geometries and inhomogeneous boundary values into the solving of elliptic PDEs. Inspired by classical Green's function, BENO consists of two Graph Neural Networks (GNNs) for interior source term and boundary values, respectively. Furthermore, a Transformer encoder maps the global boundary geometry into a latent vector which influences each message passing layer of the GNNs. We test our model and strong baselines extensively in elliptic PDEs with complex boundary conditions. We show that all existing baseline methods fail to learn the solution operator. In contrast, our model, endowed with boundary-embedded architecture, outperforms state-of-the-art neural operators and strong baselines by an average of 60.96%.

## 1 Introduction

Partial differential equations (PDEs), which include elliptic, parabolic, and hyperbolic types, play a fundamental role in diverse fields across science and engineering. For all types of PDEs, but especially for elliptic PDEs, the treatment of boundary conditions plays an important role in the solutions. Elliptic PDEs are one of the three types of PDEs, whose solutions describe steady-state phenomena under interior source and boundary conditions. In particular, the Laplace and Poisson equations constitute prime examples of linear elliptic PDEs, which are used in a wide range of disciplines, including solid mechanics (Riviere, 2008), plasma physics (Chen, 2016), and fluid dynamics (Hirsch, 2007).

Recently, neural operators have emerged as a promising tool for solving elliptic PDEs by directly mapping input to solutions (Li et al., 2020, 2020, 2020, 2020). Lowering the computation efforts makes neural operators more attractive compared with classical approaches like finite element methods (FEM) (Quarteroni and Valli, 2008) and finite difference methods (FDM) (Dimov et al., 2015). However, existing neural operators have not essentially considered the influence of boundaryconditions on solving elliptic PDEs. A distinctive feature of elliptic PDEs is their sensitivity to boundary conditions, which can heavily influence the behavior of solutions.

In fact, boundary conditions pose two major challenges for neural operators in terms of inhomogeneous boundary values and complex boundary geometry. **First**, inhomogeneous boundary conditions can cause severe fluctuations in the solution, and have a distinctive influence on the solution compared to the interior source terms. For example, as shown in Fig. 1, the inhomogeneous boundary values cause high-frequency fluctuations in the solution especially near the boundary, which makes it extremely hard to learn. **Second**, since elliptic PDEs are boundary value problems whose solution describes the steady-state of the system, any variation in the boundary geometry and values would influence the interior solution _globally_(Hirsch, 2007). The above challenges need to be properly addressed to develop a neural operator suitable for more general and realistic settings.

In this paper, we propose **B**oundary-**E**mbedded **N**eural **O**perators (BENO), a novel neural operator architecture to address the above two key challenges. Inspired by classical Green's function, BENO consists of two Graph Neural Networks (GNNs) that model the boundary influence and the interior source terms, respectively, addressing the first challenge. Moreover, to model the global influence of the boundary to the solution, we employ a Transformer (Vaswani et al., 2017) to encode the full boundary information to a latent vector and feed it to each message passing layer of the GNNs. This captures how the global geometry and values of the boundary influence the pairwise interaction between interior points, addressing the second challenge. As a whole, BENO provides a simple architecture for solving elliptic PDEs with complex boundary conditions, incorporating physics intuition into its boundary-embedded architecture. In Table 1, we provide a comparison between BENO and prior deep learning methods for elliptic PDE solving.

To fully validate the effectiveness of our model on inhomogeneous boundary value problems, we construct a novel dataset encompassing various boundary shapes, different boundary values, different types of boundary conditions, and varying resolutions. The experimental results demonstrate that our approach not only outperforms the existing state-of-the-art methods by about an average of 60.96% in solving elliptic PDEs problems but also exhibits excellent generalization capabilities in other scenarios. In contrast, all existing baselines fail to learn solution operators for the above challenging elliptic PDEs.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{Methods} & 1. PDE-agnostic & 2. Train/Test space & 3. Evaluation at & 4. Free-form spatial & 5. Inhomogeneous \\  & prediction on re- & & grid independence & unobserved sp- & domain for boundary & boundary condition \\  & w initial condition & & axial locations & shape & value & value \\ \hline GKN (Li et al., 2020) & ✓ & ✓ & ✓ & ✓ & ✗ \\ FNO (Li et al., 2020) & ✓ & ✗ & ✓ & ✗ & ✗ \\ GNN-PDE (Lötzsch et al., 2022) & ✓ & ✓ & ✗ & ✓ & ✗ \\ MP-PDE (Brandstetter et al., 2022) & ✓ & ✗ & ✗ & ✗ & ✗ \\ \hline
**BENO (ours)** & ✓ & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of data-driven methods to time-independent elliptic PDE solving.

Figure 1: Examples of different geometries for the elliptic PDEs: (a) forcing terms and (b) solutions. The nodes in red-orange color-map represent the complex, inhomogeneous boundary values. The redder the area, the higher the boundary value it represents, whereas the more orange the area, the lower the boundary value.

Problem Setup

In this work, we consider the solution of elliptic PDEs in a compact domain subject to inhomogeneous boundary conditions along the domain boundary. Let \(u\in C^{d}(\mathbb{R})\) be a d-dimnesion-differentiable function of \(N\) interior grid nodes over an open domain \(\Omega\). Specifically, we consider the Poisson equation with Dirichlet (and Neumann in Appendix M) boundary conditions in a d-dimensional domain, and we consider \(d=2\) in the following experiments:

\[\begin{split}\nabla^{2}u\left([x_{1},x_{2},\dots,x_{d}]\right)& =\ f\left([x_{1},x_{2},\dots,x_{d}]\right),\quad\ \forall\left([x_{1},x_{2},\dots,x_{d}]\right)\in\Omega,\\ u\left([x_{1},x_{2},\dots,x_{d}]\right)&=\ g\left([ x_{1},x_{2},\dots,x_{d}]\right),\quad\forall\left([x_{1},x_{2},\dots,x_{d}] \right)\in\partial\Omega,\end{split}\] (1)

where \(f\) and \(g\) are sufficiently smooth function defined on the domain \(\Omega=\{(x_{1,i},x_{2,i},\dots,x_{d,i})\}_{i=1}^{N}\), and boundary \(\partial\Omega\), respectively. Eq. 1 is utilized in a range of applications in science and engineering to describe the equilibrium state, given by \(f\) in the presence of time-independent boundary constraints specified by \(g\). A distinctive feature of elliptic PDEs is their sensitivity to boundary values \(g\) and shape \(\partial\Omega\), which can heavily influence the behavior of their solutions. Appropriate boundary conditions must often be carefully prescribed to ensure well-posedness of elliptic boundary value problems.

## 3 Method

In this section, we detail our method BENO. We first motivate our method using Green's function, a classical approach to solving elliptic boundary value problems in Section 3.1. We then introduce our graph construction method in Section 3.2. Inspired by the Green's function, we introduce BENO's architecture in Section 3.3.

### Motivation

**How to facilitate boundary-interior interaction?** To design the boundary-embedded message passing neural network, we draw inspiration from the traditional Green's function (Stakgold & Holst, 2011) method which is based on a numerical solution. Take the Poisson equation with Dirichlet boundary conditions for example. Suppose the Green's function is \(G:\Omega\times\Omega\rightarrow\mathbb{R}\), which is the solution of the corresponding equation as follows:

\[\begin{cases}&\nabla^{2}G=\delta(x-x_{0})\delta(y-y_{0})\\ &G|_{\partial\Omega}=0\end{cases}\] (2)

Based on the aforementioned equations and the detailed representation of the Green's function formula in the Appendix B, we can derive the solution in the following form:

\[u(x,y)=\iint_{\Omega}G(x,y,x_{0},y_{0})f(x_{0},y_{0})d\sigma_{0}-\int_{ \partial\Omega}g(x_{0},y_{0})\frac{\partial G(x,y,x_{0},y_{0})}{\partial n_{0 }}dl_{0}\] (3)

Motivated by the two terms presented in Eq. 3, our objective is to approach boundary embedding by extending the Green's function. Following the mainstream work of utilizing GNNs as surrogate models (Pfaff et al., 2020; Eliasof et al., 2021; Lotzsch et al., 2022), we exploit the graph network simulator (Sanchez-Gonzalez et al., 2020) as the backbone to mimic the Green's function, and add the boundary embedding to the node update in the message passing. Besides, in order to decouple the learning of the boundary and interior, we adopt a dual-branch network structure, where one branch sets the boundary value \(g\) to 0 to only learn the structural information of interior nodes, and the other branch sets the source term \(f\) of interior nodes to 0 to only learn the structural information of the boundary. The Poisson equation solving can then be disentangled into two parts:

\[\begin{cases}\nabla^{2}u(x,y)=f(x,y)\\ u(x,y)=g(x,y)\end{cases}\quad\Rightarrow\quad\underbrace{\begin{cases}\nabla ^{2}u(x,y)\ =\ f(x,y)\\ u(x,y)\ =\ 0\\ \text{Branch 1}\end{cases}}_{\text{Branch 1}}+\quad\underbrace{\begin{cases} \nabla^{2}u(x,y)\ =\ 0\\ u(x,y)\ =\ g(x,y)\end{cases}}_{\text{Branch 2}}\] (4)

Therefore, our BENO will use a dual-branch design to build two different types of edges on the same graph separately. Branch 1 considers the effects of interior nodes and Branch 2 focuses solely on how to propagate the relationship between boundary values and interior nodes in the graph. Finally, we aggregate them together to obtain a more accurate solution under complex boundary conditions.

**How to embed boundary?** Since boundary conditions are crucially important for solving PDEs, how to better embed the boundary information into the neural network is key to our design. During a pilot study, we found that directly concatenating the interior node information with boundary information fails to solve for elliptic PDEs, and tends to cause severe over-fitting. Therefore, we propose to embed the boundary to represent its global information for further fusion. In recent years, Transformer (Vaswani et al., 2017) has been widely adopted due to its global receptive field. By leveraging its attention mechanism, the Transformer can effectively capture long-range dependencies and interactions within the boundary nodes. This is particularly advantageous when dealing with complex boundary conditions (i.e., irregular shape and inhomogeneous boundary values), as it allows for the modeling of complex relationships between boundary points and the interior solution.

### Graph Construction

Before designing our method, it is an important step to construct graph \(\mathcal{G}=\{(\mathcal{V},\mathcal{E})\}\) with the finite discrete interior nodes as node set \(\mathcal{V}\) on the PDE's solution domain \(\Omega\). In traditional solution methods such as FEM, the solution domain is initially constructed by triangulating the mesh graph (Bern & Eppstein, 1995; Ho-Le, 1988), followed by the subsequent solving process. Therefore, the first step is to implement Delaunay triangulation (Lee & Schachter, 1980) to construct mesh graph with edge set \(\mathcal{E}_{mesh}\), in which each cell consists of three edges. Then we proceed to construct the edge set \(\mathcal{E}_{kn}\) by selecting the \(K\)-nearest nodes for each individual node. \(K\) is predetermined to signify the quantity of neighboring nodes that we deem as closely connected based on the Euclidean distance \(\mathcal{D}_{ij}\) between node \(i\) and \(j\). The final edge set is \(\mathcal{E}=\mathcal{E}_{mesh}\cup\mathcal{E}_{kn}\). Figure 2 shows examples of graph construction.

### Overall Architecture

In this section, we will introduce the detailed architecture of our proposed BENO, as shown in Figure 3. Our overall neural operator is divided into two branches, with each branch receiving different graph information and boundary data. However, the operator architecture remains the same with the _encoder, boundary-embedded message passing neural network_ and _decoder_. Therefore, we will only focus on the common operator architecture.

#### 3.3.1 Encoder & Decoder

**Encoder.** The encoder computes node and edge embeddings. For each node \(i\), the node encoder \(\epsilon^{v}\) maps the node coordinates \(p_{i}=(x_{i},y_{i})\), forcing term \(f_{i}\), and distances to boundary \(dx_{i},dy_{i}\) to node embedding vector \(v_{i}=\epsilon^{e}([x_{i},y_{i},f_{i},dx_{i},dy_{i}])\in R^{D}\) in a high-dimensional space. The same mapping is implemented on edge attributes with edge encoder \(\epsilon^{e}\) for edge embedding vector \(\epsilon_{ij}\). For both node and edge encoders \(\epsilon\), we exploit a two-layer Multi-Layer Perceptron (MLP) (Murtagh, 1991) with Sigmoid Linear Unit (SiLU) activation (Elfwing et al., 2018).

**Decoder.** We use a two-layer MLP to map the features to their respective solution. Considering our dual-branch architecture, we will add the outputs from each decoder to obtain the final predicted solution \(\hat{u}\).

#### 3.3.2 Boundary-Embedded Message Passing Neural Network (BE-MPNN)

To address the inherent differences in physical properties between boundary and interior nodes, we opt not to directly merge these distinct sources of information into a single network representation. Instead, we first employ the Transformer to specifically embed the boundary nodes. Then, the obtained boundary information is incorporated into the graph message passing processor. We will provide detailed explanations for these two components separately.

Figure 2: Visualization of the graph construction on our train/set samples from 5 different corner elliptic datasets. The interior nodes are in black and the boundary one in purple.

**Embedding Boundary with Transformer.** With the boundary node coordinates \(p^{\mathcal{B}}=(x^{\mathcal{B}},y^{\mathcal{B}})\), the boundary value \(g\), and the distance to the geometric center of solution domain \(dc\) as input features, we first utilize the position embedding to include relative position relationship for initial representation \(H^{\mathcal{B}}_{0}\), followed by a Transformer encoder with \(L\) layers to embed the boundary information \(H^{\mathcal{B}}\). The resulting boundary features, denoted as \(\mathcal{B}\), are obtained by applying global average pooling (Lin et al., 2013) to the encoder outputs \(H^{\mathcal{B}}\).

Each self-attention layer applies multi-head self-attention and feed-forward neural networks to the input. The output of the \(i\)-th self-attention layer is denoted as \(H^{\mathcal{B}}_{i}\). The self-attention mechanism calculates the attention weights \(A_{i}\) as follows:

\[A_{i}=\text{Softmax}\left(\frac{Q_{i}H^{\mathcal{B}}_{i}(K_{i}H^{\mathcal{B}}_ {i})^{T}}{\sqrt{d_{k}}}\right)\] (5)

where \(Q_{i}\), \(K_{i}\), and \(V_{i}\) are linear projections of \(H^{\mathcal{B}}_{i-1}\) with learnable weight matrices, and \(d_{k}\) is the dimension of the key vectors. The attention output is computed as:

\[H^{\mathcal{B}}_{i+1}=\text{LayerNorm}\left(A_{i}V_{i}\left(H^{\mathcal{B}}_{i} \right)+H^{\mathcal{B}}_{i}\right)\] (6)

where LayerNorm denotes layer normalization, which helps to mitigate the problem of internal covariate shift. After passing through the \(L\) self-attention layers, the output \(H^{\mathcal{B}}\) is subject to global average pooling to obtain the boundary features: \(\mathcal{B}=\text{AvgPool}(H^{\mathcal{B}})\).

**Boundary-Embedded Message Passing Processor.** The processor computes \(T\) steps of message passing, with an intermediate graph representation \(\mathcal{G}^{1}\), \(\cdots\), \(\mathcal{G}^{T}\) and boundary representation \(\mathcal{B}^{1}\), \(\cdots\), \(\mathcal{B}^{T}\). The specific passing message \(m^{t}_{ij}\) in step \(t\) in our processor is formed by:

\[m^{t}_{ij}=\text{MLPs}\big{(}v^{t}_{i},v^{t}_{j},e^{t}_{ij},p_{i}-p_{j}\big{)}\] (7)

where \(m^{t+1}_{ij}\) represents the message sent from node \(j\) to \(i\). \(p_{i}-p_{j}\) is the relative position which can enhance the equivariance by justifying the symmetry of the PDEs.

Then we update the node feature \(v^{t}_{i}\) and edge feature \(e^{t}_{ij}\) as follows:

\[v^{t+1}_{i}=\text{MLPs}\left(v^{t}_{i},\mathcal{B}^{t},\sum_{j\in\mathcal{N}( i)}m^{t}_{ij}\right),\] (8)

\[e^{t+1}_{ij}=\text{MLPs}\left(e^{t}_{ij},m^{t}_{ij}\right)\] (9)

Here, boundary information is embedded into the message passing. \(\mathcal{N}(i)\) represents the gathering of all the neighbors of node \(i\).

**Learning objective.** Given the ground truth solution \(u\) and the predicted solution \(\hat{u}\), we minimize the mean squared error (MSE) of the predicted solution on \(\Omega\).

Figure 3: Overall architecture of our proposed BENO. The pink branch corresponds to the first term in Eq. 4, and the **green branch** corresponds to the second term. As the backbone of boundary embedding, Transformer provides boundary information as a supplement for BE-MPNN, thereby enabling better prediction under complex boundary geometry and inhomogeneous boundary values.

## 4 Experiments

Here we aim to answer the following questions: (1) Compared with existing baselines, can BENO learn the solution operator for elliptic PDEs with complex geometry and inhomogeneous boundary values? (2) Can BENO _generalize_ to out-of-distribution boundary geometries and boundary values, and different grid resolutions? (3) Are all components of BENO essential for its performance? We first introduce the setup in Sec. 4.1, then answer questions above in the following three sections.

### Experiment setup

**Datasets.** For elliptic PDEs simulations, we construct five different datasets with inhomogeneous boundary values, including 4/3/2/1-corner squares and squares without corners. Each dataset consists of 1000 samples with randomly initialized boundary shapes and values, with 900 samples used for training and validation, and 100 samples for testing. Each sample covers a grid of 32\(\times\)32 nodes and 128 boundary nodes. To further assess model performance, higher-resolution versions of each data sample, such as 64\(\times\)64, are also provided. Details on data generation are provided in Appendix E.

**Baselines.** We adopt two of the most mainstream series of neural PDE solvers as baselines, one is graph-based, including **GKN**(Li et al., 2020b), **GNN-PDE**(Lotzsch et al., 2022), and **MP-PDE**(Brandstetter et al., 2022); the other is operator-based, including **FNO**(Li et al., 2020a). For fair comparison and adaption to irregular boundary shapes in our datasets, all of the baselines are re-implemented with the same input as ours, including all the interior and boundary node features. Please refer to Appendix G for re-implementation details.

**Implementation Details.** All experiments are based on PyTorch (Paszke et al., 2019) on 2\(\times\) NVIDIA A100 GPUs (80G). Following (Brandstetter et al., 2022), we also apply graph message passing neural network as our backbone for all the datasets. We use Adam (Kingma & Ba, 2014) optimizer with a weight decay of \(5\times 10^{-4}\) and a learning rate of \(5\times 10^{-5}\) obtained from grid search for all experiments. Please refer to Appendix F for more implementation details.

### Main Experimental Results

We first test whether our BENO has a strong capability to solve elliptic PDEs with varying shapes. Table 2 and 3 summarize the results for the shape generalization task (more in Appendix J).

From the results, we see that recent neural PDE solving methods (i.e., MP-PDE) overall _fail_ to solve elliptic PDEs with inhomogeneous boundary values, not to mention generalizing to datasets with different boundary shapes. This precisely indicates that existing neural solvers are insufficient for solving this type of boundary value problems.

In contrast, from Table 2, we see that our proposed BENO trained only on 4-Corners dataset consistently achieves a significant improvement and strong generalization capability over the previous methods by a large margin. More precisely, the improvements of BENO over the best baseline

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c} \hline \hline \multicolumn{1}{c|}{\multirow{2}{*}{Test set}} & \multicolumn{2}{c|}{4-Corners} & \multicolumn{2}{c|}{3-Corners} & \multicolumn{2}{c|}{2-Corners} & \multicolumn{2}{c|}{1-Corner} & \multicolumn{2}{c}{No-Corner} \\ \hline Metric & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE \\ \hline \multirow{3}{*}{GKN} & 1.1146\(\pm\) & 3.6497\(\pm\) & 1.0692\(\pm\) & 3.7059\(\pm\) & 1.0673\(\pm\) & 3.6822\(\pm\) & 1.1063\(\pm\) & 3.4898\(\pm\) & 1.0728\(\pm\) & 3.9551\(\pm\) \\  & 0.3936 & 1.1874 & 0.2034 & 0.9543 & 0.1393 & 0.9819 & 0.1905 & 0.9469 & 0.2074 & 0.9791 \\ \hline \multirow{3}{*}{FNO} & 1.0947\(\pm\) & 2.2707\(\pm\) & 1.0742\(\pm\) & 2.1657\(\pm\) & 1.0672\(\pm\) & 2.2617\(\pm\) & 1.0921\(\pm\) & 2.3922\(\pm\) & 1.0762\(\pm\) & 2.2281\(\pm\) \\  & 0.3265 & 0.3361 & 0.3418 & 0.3976 & 0.3736 & 0.2449 & 0.2935 & 0.3526 & 0.4420 & 0.4192 \\ \hline \multirow{3}{*}{GNN-PDE} & 1.0026\(\pm\) & 3.1410\(\pm\) & 1.0009\(\pm\) & 3.2812\(\pm\) & 1.0015\(\pm\) & 3.3557\(\pm\) & 1.0002\(\pm\) & 3.1421\(\pm\) & 1.0011\(\pm\) & 3.7561\(\pm\) \\  & 0.0093 & 0.8751 & 0.0101 & 0.8839 & 0.0099 & 0.8521 & 0.0153 & 0.8685 & 0.0152 & 1.0274 \\ \hline \multirow{3}{*}{MP-PDE} & 1.0007\(\pm\) & 3.1018\(\pm\) & 1.0003\(\pm\) & 3.2464\(\pm\) & 0.9919\(\pm\) & 3.2765\(\pm\) & 0.9829\(\pm\) & 3.0163\(\pm\) & 0.9882\(\pm\) & 3.6522\(\pm\) \\  & 0.0677 & 0.8431 & 0.0841 & 0.8049 & 0.0699 & 0.8632 & 0.07199 & 0.8272 & 0.0683 & 0.8961 \\ \hline \multirow{3}{*}{**BENO (ours)**} & **0.3523\(\pm\)** & **0.9650\(\pm\)** & **0.4308\(\pm\)** & **1.2206\(\pm\)** & **0.4910\(\pm\)** & **1.4388\(\pm\)** & **0.5416\(\pm\)** & **1.4529\(\pm\)** & **0.5542\(\pm\)** & **1.7481\(\pm\)** \\  & **0.1245** & **0.3131** & **0.1994** & **0.4978** & **0.1888** & **0.5227** & **0.2133** & **0.4626** & **0.1952** & **0.5394** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performances of our proposed BENO and the compared baselines, which are trained on 900 4-corners samples and tested on 5 datasets under relative L2 norm and MAE separately. The unit of the MAE metric is \(1\times 10^{-3}\). Bold fonts indicate the best performance.

are \(55.17\%\), \(52.18\%\), \(52.43\%\), \(47.38\%\), and \(52.94\%\) in terms of relative L2 norm when testing on 4/3/2/1/No-Corner dataset respectively. We attribute the remarkable performance to two factors: (i) BENO comprehensively leverages boundary information, and fuses them with the interior graph message for solving. (ii) BENO integrates dual-branch architecture to fully learn boundary and interior in a decoupled way and thus improves generalized solving performance.

Similarly, from Table 3, we see that among mixed corner training results, BENO always achieves the best performance among various compared baselines when varying the test sets, which validates the consistent superiority of our BENO with respect to different boundary shapes.

Additionally, we plot the visualization of the best baseline and our proposed BENO trained on 4-Corners dataset in Figure 4. It can be clearly observed that the predicted solution of BENO is closed to the ground truth, while MP-PDE fails to learn any features of the solution. We observe similar behaviors for all other baselines.

### Results on Different Values

To investigate the generalization ability on boundary value, we again train the models on 4-Corners dataset with inhomogeneous boundary value but utilize the test set with zero boundary value, which makes the boundary inhomogeneities totally different. Table 4 compares the best baseline and summarizes the results. From the results, we see that BENO has a significant advantage, successfully reducing the L2 norm to around 0.1. In addition, our method outperforms the best baseline by approximately 60.96% in terms of performance improvement. This not only demonstrates BENO's strong generalization ability regarding boundary values but also provides solid experimental evidence for the successful application of our elliptic PDE solver.

Figure 4: Visualization of two samples’ prediction and prediction error from 4-Corners dataset. We render the solution \(u\) of the baseline MP-PDE, our BENO and the ground truth in \(\Omega\).

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c} \hline \hline \multicolumn{1}{c|}{\multirow{2}{*}{Test set}} & \multicolumn{2}{c|}{4-Corners} & \multicolumn{2}{c|}{3-Corners} & \multicolumn{2}{c|}{2-Corners} & \multicolumn{2}{c|}{1-Corner} & \multicolumn{2}{c}{No-Corner} \\ \hline Metric & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE \\ \hline \multirow{2}{*}{GKN} & 1.0588\(\pm\) & 3.5051\(\pm\) & 1.0651\(\pm\) & 3.7061\(\pm\) & 1.0386\(\pm\) & 3.6043\(\pm\) & 1.0734\(\pm\) & 3.4048\(\pm\) & 1.0423\(\pm\) & 3.901\(\pm\) \\  & 0.1713 & 0.9401 & 0.1562 & 0.8563 & 0.1271 & 0.9392 & 0.1621 & 0.9519 & 0.2102 & 0.9287 \\ \hline \multirow{2}{*}{FNO} & 1.0834\(\pm\) & 4.6401\(\pm\) & 1.0937\(\pm\) & 4.6902\(\pm\) & 4.5267\(\pm\) & 1.0735\(\pm\) & 4.5027\(\pm\) & 1.0713\(\pm\) & 4.5783\(\pm\) \\  & 0.0462 & 0.5327 & 0.0625 & 0.6713 & 0.0376 & 0.5581 & 0.0528 & 0.5371 & 0.0489 & 0.5565 \\ \hline \multirow{2}{*}{GNN-PDE} & 1.0009\(\pm\) & 3.1311\(\pm\) & 1.0003\(\pm\) & 3.2781\(\pm\) & 1.0005\(\pm\) & 3.3518\(\pm\) & 0.9999\(\pm\) & 3.1422\(\pm\) & 1.0002\(\pm\) & 3.7528\(\pm\) \\  & 0.0036 & 0.8664 & 0.0039 & 0.8858 & 0.0038 & 0.8520 & 0.0042 & 0.8609 & 0.0041 & 1.0284 \\ \hline \multirow{2}{*}{MP-PDE} & 1.0063\(\pm\) & 3.1238\(\pm\) & 1.0045\(\pm\) & 3.2537\(\pm\) & 0.9957\(\pm\) & 3.2864\(\pm\) & 0.9822\(\pm\) & 3.0177\(\pm\) & 0.9912\(\pm\) & 3.6658\(\pm\) \\  & 0.0735 & 0.8502 & 0.0923 & 0.7867 & 0.0772 & 0.8607 & 0.0802 & 0.8363 & 0.0781 & 0.8949 \\ \hline \multirow{2}{*}{**BENO (ours)**} & **0.4487\(\pm\)** & **1.2150\(\pm\)** & **0.4783\(\pm\)** & **1.3509\(\pm\)** & **0.4737\(\pm\)** & **1.3516\(\pm\)** & **0.5168\(\pm\)** & **1.3728\(\pm\)** & **0.4665\(\pm\)** & **1.4213\(\pm\)** \\  & **0.1750** & **0.4213** & **0.1938** & **0.5432** & **0.1979** & **0.5374** & **0.1793** & **0.5148** & **0.2001** & **0.5262** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Performances of our proposed BENO and the compared baselines, which are trained on 900 mixed samples (180 samples each from 5 datasets) and tested on 5 datasets under relative L2 error and MAE separately. The unit of the MAE metric is \(1\times 10^{-3}\).

### Ablation Study

To investigate the effectiveness of inner components in BENO, we study four variants of BENO. Table 5 shows the effectiveness of our BENO on ablation experiments, which is implemented based on 4-Corners dataset training. Firstly, **BENO** w. **M** replaces the BE-MPNN with a vanilla message passing neural network (Gilmer et al., 2017) and merely keeps the interior node feature. Secondly, **BENO** w/o. **D** removes the dual-branch structure of BENO and merely utilizes a single Encoder-BE-MPNN-Decoder procedure. Thirdly, **BENO** w. **E** adds the Transformer output for edge message passing. Finally, **BENO** w. **G** replaces the Transformer architecture with a vanilla graph convolution network (Kipf and Welling, 2016).

From the results we can draw conclusions as follows. Firstly, **BENO** w. **M** performs significantly worse than ours, which indicates the importance of fusing interior and boundary in BENO. Secondly, results of **BENO** w/o. **D** indicate that decoupled learning of the interior and boundary proves to be effective. Thirdly, comparing the results of **BENO** w. **E** and ours, we can find that boundary information only helps in node-level message passing. In other words, it is not particularly suitable to directly inject the global information of the boundary into the edges. Finally, comparing results of **BENO** w. **G** with ours validates the design of Transformer for boundary embedding is crucial.

## 5 Conclusion

We propose the Boundary-Embedded Neural Operators (BENO) to address the challenges of solving elliptic PDEs with inhomogeneous and complex boundary conditions. Our BENO incorporates physical intuition through a boundary-embedded architecture consisting of graph neural networks and a Transformer to model the influence of boundary conditions on the solution. Comprehensive experiments demonstrate the effectiveness of our approach in outperforming state-of-the-art methods by an average of 60.96% in solving elliptic PDE problems.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c} \hline \hline \multicolumn{10}{c}{Train on 4-Corners dataset with homogeneous boundary value} \\ \hline Test set & \multicolumn{2}{c|}{4-Corners} & \multicolumn{2}{c|}{3-Corners} & \multicolumn{2}{c|}{2-Corners} & \multicolumn{2}{c|}{1-Corner} & \multicolumn{2}{c}{No-Comer} \\ \hline Metric & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE \\ \hline \multirow{2}{*}{GNN-PDE} & 0.7092\(\pm\) & 0.1259\(\pm\) & 0.7390\(\pm\) & 0.2351\(\pm\) & 0.7491\(\pm\) & 0.3290\(\pm\) & 0.7593\(\pm\) & 0.4750\(\pm\) & 0.7801\(\pm\) & 0.6808\(\pm\) \\  & 0.0584 & 0.0755 & 0.0483 & 0.1013 & 0.0485 & 0.1371 & 0.05269 & 0.1582 & 0.0371 & 0.1692 \\ \hline \multirow{2}{*}{MP-PDE} & 0.2598\(\pm\) & 0.0459\(\pm\) & 0.3148\(\pm\) & 0.1066\(\pm\) & 0.3729\(\pm\) & 0.1778\(\pm\) & 0.4634\(\pm\) & 0.3049\(\pm\) & 0.5458\(\pm\) & 0.4924\(\pm\) \\  & 0.1098 & 0.0359 & 0.0814 & 0.0618 & 0.0819 & 0.0969 & 0.0649 & 0.1182 & 0.0491 & 0.1310 \\ \hline \multirow{2}{*}{**BENO (ours)**} & **0.0908\(\pm\)** & **0.0142\(\pm\)** & **0.1031\(\pm\)** & **0.0288\(\pm\)** & **0.1652\(\pm\)** & **0.0583\(\pm\)** & **0.1738\(\pm\)** & **0.0862\(\pm\)** & **0.2441\(\pm\)** & **0.1622\(\pm\)** \\  & **0.07381** & **0.0131\(\pm\)** & **0.0728** & **0.0189** & **0.1324** & **0.0362** & **0.1508** & **0.0456** & **0.1665** & **0.0798** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performances of our BENO and the compared baselines, which are trained on 900 4-Corners samples and tested with zero boundary value samples. The unit of the MAE metric is \(1\times 10^{-3}\).

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c} \hline \hline Test set & \multicolumn{2}{c|}{4-Corners} & \multicolumn{2}{c|}{3-Corners} & \multicolumn{2}{c|}{2-Corners} & \multicolumn{2}{c|}{1-Corner} & \multicolumn{2}{c}{No-Comer} \\ \hline Metric & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE \\ \hline \multirow{2}{*}{**BENO w. M**} & 1.0130\(\pm\) & 3.1436\(\pm\) & 1.0159\(\pm\) & 3.3041\(\pm\) & 0.9999\(\pm\) & 3.3007\(\pm\) & 1.0026\(\pm\) & 3.0842\(\pm\) & 0.9979\(\pm\) & 3.6832\(\pm\) \\  & 0.0858 & 0.8667 & 0.0975 & 0.7906 & 0.0792 & 0.8504 & 0.0840 & 0.8202 & 0.0858 & 0.8970 \\ \hline \multirow{2}{*}{**BENO w/o. D**} & 0.4058\(\pm\) & 1.1175\(\pm\) & 0.4850\(\pm\) & 1.3810\(\pm\) & 0.5273\(\pm\) & 1.5439\(\pm\) & 0.5795\(\pm\) & 1.5683\(\pm\) & 0.5835\(\pm\) & 1.8382\(\pm\) \\  & 0.1374 & 0.3660 & 0.2230 & 0.6068 & 0.1750 & 0.4774 & 0.1981 & 0.4670 & 0.2232 & 0.5771 \\ \hline \multirow{2}{*}{**BENO w. E**} & 0.4113\(\pm\) & 1.2020\(\pm\) & 0.4624\(\pm\) & 1.3569\(\pm\) & 0.5347\(\pm\) & 1.5990\(\pm\) & 0.5891\(\pm\) & 1.6222\(\pm\) & 0.5843\(\pm\) & 1.8790\(\pm\) \\  & 0.1236 & 0.4048 & 0.2102 & 0.5453 & 0.1985 & 0.5604 & 0.2129 & 0.2016 & 0.2016 & 0.5952 \\ \hline \multirow{2}{*}{**BENO w. G**} & 0.9037\(\pm\) & 2.6795\(\pm\) & 0.8807\(\pm\) & 2.6992\(\pm\) & 0.8928\(\pm\) & 2.8235\(\pm\) & 0.8849\(\pm\) & 2.561\(\pm\) & 0.87212 & 2.9851\(\pm\) \\  & 0.1104 & 0.5332 & 0.1298 & 0.6118 & 0.1208 & 0.5892

## References

* De Avila Belbute-Peres et al. (2020) Filipe De Avila Belbute-Peres, Thomas Economon, and Zico Kolter. Combining differentiable pde solvers and graph neural networks for fluid flow prediction. In _international conference on machine learning_, pp. 2402-2411. PMLR, 2020.
* Bern and Eppstein (1995) Marshall Bern and David Eppstein. Mesh generation and optimal triangulation. In _Computing in Euclidean geometry_, pp. 47-123. World Scientific, 1995.
* Brandstetter et al. (2022) Johannes Brandstetter, Daniel Worrall, and Max Welling. Message passing neural pde solvers. _arXiv preprint arXiv:2202.03376_, 2022.
* Chen (2016) Francis F Chen. _Introduction to Plasma Physics and Controlled Fusion (3rd Ed.)_. Springer, 2016.
* Dimov et al. (2015) Ivan Dimov, Istvan Farago, and Lubin Vulkov. _Finite difference methods, theory and applications_. Springer, 2015.
* Elfwing et al. (2018) Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. _Neural networks_, 107:3-11, 2018.
* Eliasof et al. (2021) Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. _Advances in neural information processing systems_, 34:3836-3849, 2021.
* Gao et al. (2022) Han Gao, Matthew J Zahr, and Jian-Xun Wang. Physics-informed graph neural galerkin networks: A unified framework for solving pde-governed forward and inverse problems. _Computer Methods in Applied Mechanics and Engineering_, 390:114502, 2022.
* Gilmer et al. (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International conference on machine learning_, pp. 1263-1272. PMLR, 2017.
* Guibas et al. (2021) John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan Catanzaro. Adaptive fourier neural operators: Efficient token mixers for transformers. _arXiv preprint arXiv:2111.13587_, 2021.
* Gupta et al. (2021) Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for differential equations. _Advances in neural information processing systems_, 34:24048-24062, 2021.
* Gupta et al. (2022) Gaurav Gupta, Xiongye Xiao, Radu Balan, and Paul Bogdan. Non-linear operator approximations for initial value problems. In _International Conference on Learning Representations (ICLR)_, 2022.
* Helwig et al. (2023) Jacob Helwig, Xuan Zhang, Cong Fu, Jerry Kurtin, Stephan Wojtowytsch, and Shuiwang Ji. Group equivariant fourier neural operators for partial differential equations. _arXiv preprint arXiv:2306.05697_, 2023.
* Hirsch (2007) C. Hirsch. _Numerical computation of internal and external flows: The fundamentals of computational fluid dynamics_. Elsevier, 2007.
* Ho-Le (1988) K Ho-Le. Finite element mesh generation methods: a review and classification. _Computer-aided design_, 20(1):27-38, 1988.
* Hughes (2012) T. J. R. Hughes. _The finite element method: linear static and dynamic finite element analysis_. Courier Corporation, 2012.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Kipf and Welling (2016) Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* Lee and Schachter (1980) Der-Tsai Lee and Bruce J Schachter. Two algorithms for constructing a delaunay triangulation. _International Journal of Computer & Information Sciences_, 9(3):219-242, 1980.
* Lee et al. (2023) Jae Yong Lee, Seungchan Ko, and Youngjoon Hong. Finite element operator network for solving parametric pdes. _arXiv preprint arXiv:2308.04690_, 2023.
* Lee et al. (2021)R. J. LeVeque. _Finite difference methods for ordinary and partial differential equations: steady-state and time-dependent problems_. SIAM, 2007.
* Li et al. (2020a) Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _arXiv preprint arXiv:2010.08895_, 2020a.
* Li et al. (2020b) Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. _arXiv preprint arXiv:2003.03485_, 2020b.
* Li et al. (2020c) Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya, and Anima Anandkumar. Multipole graph neural operator for parametric partial differential equations. _Advances in Neural Information Processing Systems_, 33:6755-6766, 2020c.
* Li et al. (2022) Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, and Anima Anandkumar. Fourier neural operator with learned deformations for pdes on general geometries. _arXiv preprint arXiv:2207.05209_, 2022.
* Lienen and Gunnemann (2022) Marten Lienen and Stephan Gunnemann. Learning the dynamics of physical systems from sparse observations with finite element networks. _arXiv preprint arXiv:2203.08852_, 2022.
* Lin et al. (2013) Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. _arXiv preprint arXiv:1312.4400_, 2013.
* Loshchilov and Hutter (2016) Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. _arXiv preprint arXiv:1608.03983_, 2016.
* Lotzsch et al. (2022) Winfried Lotzsch, Simon Ohler, and Johannes S Otterbach. Learning the solution operator of boundary value problems using graph neural networks. _arXiv preprint arXiv:2206.14092_, 2022.
* Lu et al. (2019) Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. _arXiv preprint arXiv:1910.03193_, 2019.
* Murtagh (1991) Fionn Murtagh. Multilayer perceptrons for classification and regression. _Neurocomputing_, 2(5-6):183-197, 1991.
* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche Buc, E. Fox, and R. Garnett (eds.), _Advances in Neural Information Processing Systems 32_, pp. 8024-8035. Curran Associates, Inc., 2019.
* Patro and Sahu (2015) SGOPAL Patro and Kishore Kumar Sahu. Normalization: A preprocessing stage. _arXiv preprint arXiv:1503.06462_, 2015.
* Pfaff et al. (2020) Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. Learning mesh-based simulation with graph networks. _arXiv preprint arXiv:2010.03409_, 2020.
* Quarteroni and Valli (2008) Alfio Quarteroni and Alberto Valli. _Numerical approximation of partial differential equations_, volume 23. Springer Science & Business Media, 2008.
* Riviere (2008) Beatrice Riviere. _Discontinuous Galerkin Methods for Solving Elliptic and Parabolic Equations_. Society for Industrial and Applied Mathematics, 2008. doi: 10.1137/1.9780898717440. URL https://epubs.siam.org/doi/abs/10.1137/1.9780898717440.
* Saad (2003) Y. Saad. _Iterative methods for sparse linear systems_. SIAM, 2003.
* Sanchez-Gonzalez et al. (2018) Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for inference and control. In _International Conference on Machine Learning_, pp. 4470-4479. PMLR, 2018.
* Snoek et al. (2019)Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. Learning to simulate complex physics with graph networks. In _International conference on machine learning_, pp. 8459-8468. PMLR, 2020.
* Stakgold and Holst (2011) Ivar Stakgold and Michael J Holst. _Green's functions and boundary value problems_. John Wiley & Sons, 2011.
* Taghibakhshi et al. (2023) Ali Taghibakhshi, Nicolas Nytko, Tareq Uz Zaman, Scott MacLachlan, Luke Olson, and Matthew West. Mg-gnn: Multigrid graph neural networks for learning multilevel domain decomposition methods. _arXiv preprint arXiv:2301.11378_, 2023.
* Tran et al. (2021) Alasdair Tran, Alexander Mathews, Lexing Xie, and Cheng Soon Ong. Factorized fourier neural operators. _arXiv preprint arXiv:2111.13802_, 2021.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Xiao et al. (2023) Xiongye Xiao, Defu Cao, Ruochen Yang, Gaurav Gupta, Gengshuo Liu, Chenzhong Yin, Radu Balan, and Paul Bogdan. Coupled multiwavelet neural operator learning for coupled partial differential equations. _arXiv preprint arXiv:2303.02304_, 2023.
* Zhao et al. (2022) Qingqing Zhao, David B Lindell, and Gordon Wetzstein. Learning to solve pde-constrained inverse problems with graph networks. _arXiv preprint arXiv:2206.00711_, 2022.

## Appendix A Anonymous Code

Our source code can be found https://anonymous.4open.science/r/BENO-3D53, which is an anonymous link for reproducibility.

## Appendix B Derivation of the Green's function method.

We first review the definition of the Green's function, which is \(G:\Omega\times\Omega\rightarrow\mathbb{R}\), which is the solution of the corresponding equation as follows:

\[\left\{\begin{array}{l}\nabla^{2}G=\delta(x-x_{0})\delta(y-y_{0})\\ G|_{\partial\Omega}=0\end{array}\right.\] (10)

According to Green's identities,

\[\iint_{\Omega}(u\nabla^{2}G)d\sigma=\int_{\partial\Omega}u\frac{\partial G}{ \partial n}dl-\iint_{\Omega}(\nabla u\cdot\nabla G)d\sigma\] (11)

Since \(u\) and \(G\) are arbitrary, we can change the position to obtain that,

\[\iint_{\Omega}(G\nabla^{2}u)d\sigma=\int_{\partial\Omega}G\frac{\partial u}{ \partial n}dl-\iint_{\Omega}(\nabla u\cdot\nabla G)d\sigma\] (12)

Subtract Eq. 12 from Eq. 11, we have,

\[\iint_{\Omega}(u\nabla^{2}G-G\nabla^{2}u)d\sigma=\int_{\partial\Omega}\left( u\frac{\partial G}{\partial n}-G\frac{\partial u}{\partial n}\right)dl\] (13)

Substitute Eq. 13 into Eq. 10, we can have that,

\[\begin{split}\int_{\partial\Omega}\left(u\frac{\partial G}{ \partial n}-G\frac{\partial u}{\partial n}\right)dl&=\iint_{ \Omega}(u\cdot\nabla^{2}G-G\cdot\nabla^{2}u)d\sigma\\ &=\iint_{\Omega}(-u\delta(x-x_{0})\delta(y-y_{0})-G\nabla^{2}u)d \sigma\\ &=-u(x_{0},y_{0})-\iint_{\Omega}G\nabla^{2}ud\sigma\\ &=-u(x_{0},y_{0})+\iint_{\Omega}Gf(x,y)d\sigma\end{split}\] (14)

Namely, we have that,

\[\begin{split} u(x,y)&=\iint_{\Omega}G(x,y,x_{0},y_{ 0})f(x_{0},y_{0})d\sigma_{0}\\ &+\int_{\partial\Omega}\left[G(x,y,x_{0},y_{0})\frac{\partial u (x_{0},y_{0})}{\partial n_{0}}-u(x_{0},y_{0})\frac{\partial G(x,y,x_{0},y_{0} )}{\partial n_{0}}\right]dl_{0}\end{split}\] (15)

When considering the Dirichlet boundary conditions, we can simplify the solution in the following form:

\[u(x,y)=\iint_{\Omega}G(x,y,x_{0},y_{0})f(x_{0},y_{0})d\sigma_{0}-\int_{ \partial\Omega}g(x_{0},y_{0})\frac{\partial G(x,y,x_{0},y_{0})}{\partial n_{0 }}dl_{0}\] (16)

## Appendix C Numerical solution of the elliptic PDE

The strong solution to (1) can be expressed in terms of the Green's function (see Section 3.1 and Appendix B for discussion). However, obtaining a closed form expression using the Green's function is typically not possible, except for some limited canonical domain shapes. In the present paper,we obtain the solution to (1) in arbitrary two dimensional domains \(\Omega\) using the finite volume method (Hirsch, 2007). This numerical approach relies on discretizing the domain \(\Omega\) using _cells_. The surfaces of these cells at the boundary, which are called _cell interfaces_, are used to specify the given boundary condition. The solution of (1) is then numerically approximated over \(N\) (e.g., for 32\(\times\)32 cells, \(N=1024\)) computational cells by solving,

\[\mathbf{P}\hat{\mathbf{u}}\ =\ \mathbf{f},\] (17)

where \(\mathbf{P}\in\mathbb{R}^{N\times N}\) is an \(N\times N\) matrix which denotes a second-order discretization of the \(\nabla^{2}\) operator incorporating the boundary conditions, \(\hat{\mathbf{u}}\in\mathbb{R}^{N\times 1}\) is a vector of values at the cell centers, and \(\mathbf{f}\in\mathbb{R}^{N\times 1}\) is a vector with values \(f(\cdot,\cdot)\) at cell centers used to discretize the domain \(\Omega\). The matrix \(\mathbf{P}\) resulting from this approach is positive definite and diagonally dominant, making it convenient to solve Equation 17 with a matrix-free iterative approach such as the Gauss-Seidel method (Saad, 2003).

## Appendix D Related Work

### Classic Elliptic PDE Solvers

The classical numerical solution of elliptic PDEs approximates the domain \(\Omega\) and its boundary \(\partial\Omega\) in Eq. 1 using a finite number of non-overlapping partitions. The solution to Eq. 1 is then approximated over these partitions. A variety of strategies are available for computing this discrete solution. Popular approaches include finite volume method (FVM) (Hirsch, 2007), finite element method (FEM) (Hughes, 2012), and finite difference method (FDM) (LeVeque, 2007). In the present work we utilize the FVM to generate the dataset which can easily accommodate complex boundary shapes. This approach partitions the domains into cells, and the boundary is specified using cell interfaces. After numerically approximating the operator \(\nabla^{2}\) over these cells, the numerical solution is obtained on the centers of the cells constituting our domain.

### GNN for PDE Solver

GNNs are initially applied in physics-based simulations on solids and fluids represented by particles (Sanchez-Gonzalez et al., 2018). Recently, an important advancement MeshGraphNets (Pfaff et al., 2020) emerge to learn mesh-based simulations. Subsequently, several variations have been proposed, including techniques for accelerating finer-level simulations by utilizing GNNs (Belbute-Peres et al., 2020), combining GNNs with Physics-Informed Neural Networks (PINNs) (Gao et al., 2022), solving inverse problems with GNNs and autodecoder-style priors (Zhao et al., 2022), and learning optimized parameters in two-level multi-grid GNNs (Taghibakhshi et al., 2023). However, the research focus on addressing boundary issues is limited. T-FEN (Lienen and Gunnemann, 2022), FEONet (Lee et al., 2023), and GNN-PDE (Lotzsch et al., 2022) are pioneering efforts in this regard, encompassing complex domains and various boundary shapes. Nevertheless, the boundary values are still set to zero, which does not account for the presence of inhomogeneous boundary values. This discrepancy is precisely the problem that our paper aims to address.

### Neural Operator as PDE Solver

Neural operators map from initial/boundary conditions to solutions through supervised learning in a mesh-invariant manner. Prominent examples of neural operators include the Fourier neural operator (FNO) (Li et al., 2020a), graph neural operator (Li et al., 2020b), and DeepONet(Lu et al., 2019). Neural operators exhibit invariance to discretization, making them highly suitable for solving PDEs. Moreover, neural operators enable the learning of operator mappings between infinite-dimensional function spaces. Subsequently, further variations have been proposed, including techniques for solving arbitrary geometries PDEs with both the computation efficiency and the flexibility (Li et al., 2022), enabling deeper stacks of Fourier layers by independently applying transformations (Tran et al., 2021), utilizing Fourier layers as a replacement for spatial self-attention (Guibas et al., 2021), and incorporating symmetries in the physical domain using group theory (Helwig et al., 2023). (Gupta et al., 2021, 2022; Xiao et al., 2023) continuously improve the design of the operator by introducing novel methods for numerical computation.

Details of Datasets

In this paper, we have established a comprehensive dataset for solving elliptic PDEs to facilitate various research endeavors. The elliptic PDEs solver is performed as follows. (1) A square domain is set with \(N_{c}\) number of cells in both \(x\) and \(y\) directions (note \(N=N_{c}^{2}\)). The number of corners is set, however, the size of the corners is chosen randomly. (2) The source term \(f(x,y)\) is assigned assuming a variety of basis functions, including sinusoidal, exponential, logarithmic, and polynomial distributions. (3) The values of the boundary conditions \(g(x,y)\) are set using continuous periodic functions with a uniformly distributed wavelength \(\in[1,5]\). (4) The Gauss-Seidel method (Saad, 2003) is used to iteratively obtain the solution \(u(x,y)\). Each Poisson run generates two files: one for the interior cells with discrete values of \(x\), \(y\), \(f\), and \(u\) and the other for the boundary interfaces with discrete values of \(x\), \(y\), and \(g\). The simulations are performed on the Sherlock cluster at Stanford University.

## Appendix F More Implementation Details

Our normalization process is performed using the z-score method (Patro and Sahu, 2015), where the mean and standard deviation are calculated from the training set. This ensures that all features are normalized based on the mean and variance of the training data. We also apply the CosineAnnealing-WarmRestarts scheduler (Loshchilov and Hutter, 2016) during the training. Each experiment is trained for 1000 epochs, and validation is performed after each epoch. For the final evaluation, we select the model parameters from the epoch with the lowest validation loss. Consistency is maintained across all experiments by utilizing the same random seed.

All our experiments are evaluated on relative L2 error, abbreviated as L2, and mean absolute error (MAE), which are two commonly used metrics for evaluating the performance of models or algorithms. The relative L2 error, also known as the normalized L2 error, measures the difference between the predicted values and the ground truth values, normalized by the magnitude of the ground truth values. It is typically calculated as the L2 norm of the difference between the predicted and ground truth values, divided by the L2 norm of the ground truth values. On the other hand, MAE measures the average absolute difference between the predicted values and the ground truth values. It is calculated by taking the mean of the absolute differences between each predicted value and its corresponding ground truth value.

## Appendix G Details of Baselines

Our proposed BENO is compared with a range of competing baselines as follows:

* **GKN**(Li et al., 2020) develops an approximation method for mapping in infinite-dimensional spaces by combining non-linear activation functions with a set of integral operators. The integration of kernels is achieved through message passing on graph networks. For fair comparison, we re-implement it by adding the boundary nodes to the graph. To better distinguish between nodes belonging to the interior and those belonging to the boundary, we have also added an additional column of one-hot encoding to the nodes for differentiation.
* **FNO**(Li et al., 2020) introduces a novel approach that directly learns the mapping from functional parametric dependencies to the solution. The method implements a series of layers computing global convolution operators with the fast Fourier transform (FFT) followed by mixing weights in the frequency domain and inverse Fourier transform, enabling an architecture that is both expressive and computationally efficient. For fair comparison, we re-implement it by fixing the value of out-domain nodes with a large number, and then implement the global operation.
* **GNN-PDE**(Lotzsch et al., 2022) represents the pioneering effort in training neural networks on simulated data generated by a finite element solver, encompassing various boundary shapes. It evaluates the generalization capability of the trained operator across previously unobserved scenarios by designing a versatile solution operator using spectral graph convolutions. For fair comparison, we re-implement it by adding the boundary nodes to the graph. To better distinguish between nodes belonging to the interior and those belonging to the boundary, we have also added an additional column of one-hot encoding to the nodes for differentiation.

[MISSING_PAGE_FAIL:15]

## Appendix J More Experimental Results

### Sensitivity Analysis

In this section, we discuss the process of determining the optimal values for the number of MLP layers (\(M\)) and the number of Transformer layers (\(L\)) using grid search, a systematic approach for hyper-parameter tuning.

Grid search involves defining a parameter grid consisting of different combinations of \(M\) and \(L\) values. We specified \(M\) in the range of [2, 3, 4] and \(L\) in the range of [1, 2, 3] to explore a diverse set of configurations. We build multiple models, each with a different combination of \(M\) and \(L\) values, and train them on 4-Corners training dataset. The models are then evaluated using appropriate evaluation metrics on a separate validation set. The evaluation results allowed us to compare the performance of models across different parameter combinations.

After evaluating the models, we select the combination of \(M\) and \(L\) that yield the best performance according to our chosen evaluation metric. This combination became our final choice for \(M\) and \(L\), representing the optimal configuration for our model. To ensure the reliability of our chosen parameters, we validate them on an independent validation set. This step confirmed that the model's performance remained consistent and reliable.

The grid search process provided a systematic and effective approach to determine the optimal values for \(M=3\) and \(L=1\), allowing us to fine-tune our model and achieve improved performance.

### More Experimental Results

We have successfully validated our method's performance on the 4-Corners and mixed corners datasets during training and testing on other shape datasets, yielding favorable results. In this section, we will further supplement the evaluation by training on the No Corner dataset and testing on other shape datasets. Since the No Corner dataset does not include any corner scenarios, the remaining datasets present completely unseen scenarios for it, thereby providing a stronger test of the model's generalization performance.

Table 6 summarizes the results of training on 900 No-Corner samples and tested on all datasets. We can infer similar conclusions to those in the experimental section above. Our BENO performs well in learning on No-Corner cases, yielding more accurate solutions. Additionally, our method demonstrates stronger generalization ability, as it can obtain good solutions even in cases where corners of any shape have not been encountered.

### Convergence Analysis

We draw the training curve of the train L2 norm and test L2 norm of three models trained on the 4-Corners dataset with inhomogeneous boundary value in Figure 5. It is obviously that although

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c} \hline \hline \multicolumn{1}{c}{Test set} & \multicolumn{4}{c|}{4-Corners} & \multicolumn{2}{c|}{3-Corners} & \multicolumn{2}{c|}{2-Corners} & \multicolumn{2}{c|}{1-Corner} & \multicolumn{2}{c}{No-Corner} \\ \hline Metric & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE \\ \hline \multirow{2}{*}{GKN} & 1.0147\(\pm\) & 3.3790\(\pm\) & 1.0179\(\pm\) & 3.5419\(\pm\) & 1.0047\(\pm\) & 3.4530\(\pm\) & 1.0072\(\pm\) & 3.2295\(\pm\) & 1.0028\(\pm\) & 3.6899\(\pm\) \\  & 0.1128 & 0.8922 & 0.1212 & 0.8643 & 0.1166 & 0.9514 & 0.1098 & 0.8520 & 0.1060 & 0.9897 \\ \hline \multirow{2}{*}{FNO} & 0.9714\(\pm\) & 3.3210\(\pm\) & 0.9745\(\pm\) & 3.3187\(\pm\) & 0.9733\(\pm\) & 3.319\(\pm\) & 0.9789\(\pm\) & 3.3511\(\pm\) & 0.9755\(\pm\) & 3.3427\(\pm\) \\  & 0.0128 & 0.6546 & 0.0175 & 0.6639 & 0.0137 & 0.6298 & 0.0210 & 0.6109 & 0.0121 & 0.6981 \\ \hline \multirow{2}{*}{GNN-PDE} & 0.9988\(\pm\) & 3.1182\(\pm\) & 0.9997\(\pm\) & 3.2748\(\pm\) & 0.9994\(\pm\) & 3.3475\(\pm\) & 1.0002\(\pm\) & 3.1447\(\pm\) & 0.9998\(\pm\) & 3.7518\(\pm\) \\  & 0.0051 & 0.8543 & 0.0054 & 0.8902 & 0.0054 & 0.8533 & 0.0056 & 0.8559 & 0.0056 & 1.0314 \\ \hline \multirow{2}{*}{MP-PDE} & 1.0029\(\pm\) & 3.1005\(\pm\) & 1.0049\(\pm\) & 3.2488\(\pm\) & 0.9986\(\pm\) & 3.2902\(\pm\) & 0.9855\(\pm\) & 3.0356\(\pm\) & 0.9917\(\pm\) & 3.6648\(\pm\) \\  & 0.0808 & 0.8158 & 0.0891 & 0.7941 & 0.0822 & 0.8651 & 0.0769 & 0.8133 & 0.07670 & 0.8949 \\ \hline \multirow{2}{*}{**BENO (ours)**} & **0.6870\(\pm\)** & **1.8830\(\pm\)** & **0.6036\(\pm\)** & **1.7293\(\pm\)** & **0.5760\(\pm\)** & **1.6703\(\pm\)** & **0.6192\(\pm\)** & **1.6749\(\pm\)** & **0.4093\(\pm\)** & **1.2505\(\pm\)** \\  & **0.2038** & **0.6083** & **0.1940** & **0.5844** & **0.1998** & **0.6605** & **0.2259** & **0.5773** & **0.1873** & **0.5752** \\ \hline \two baselines also contains the boundary information, they fail to learn the elliptic PDEs with non-decreasing convergence curves. However, our proposed BENO is capable of successfully learning complex boundary conditions with the use of the CosineAnnealingWarmRestarts scheduler, converges to a satisfactory result.

## Appendix K Limitations & Broader Impacts

**Limitations.** Although this paper primarily focuses on Dirichlet boundary conditions, it is essential to acknowledge that there are other types of boundary treatments, including Neumann and Robin boundary conditions. While the framework presented in this study may not directly address these alternative boundary conditions, it still retains its usefulness. Future research should explore the extension of the developed framework to incorporate these different boundary treatments, allowing for a more comprehensive and versatile solution for a broader range of practical problems.

**Broader Impact.** The development of a fast, efficient, and accurate neural network for solving PDEs holds significant potential for numerous physics and engineering disciplines. The impact of such advancements cannot be understated. By providing a more streamlined and computationally efficient approach, this research can revolutionize fields such as computational fluid dynamics, solid mechanics, electromagnetics, and many others. The ability to solve PDEs more efficiently opens up new possibilities for modeling and simulating complex physical systems, leading to improved designs, optimizations, and decision-making processes. The resulting advancements can have far-reaching implications, including the development of more efficient and sustainable technologies, enhanced understanding of natural phenomena, and improved safety and reliability in engineering applications. It is crucial to continue exploring and refining these neural network-based approaches to maximize their potential impact across a wide range of scientific and engineering disciplines.

## Appendix L More Visualization Analysis

In this section, we visualize the experimental results on a broader range of experiments. Figure 6 presents the comparison of solution prediction on 64 \(\times\) 64 grid resolution. Figure 7 presents the comparison of solution prediction on data with zero boundary value. Figure 8 presents the qualitative results of training on the 4-Corners dataset and testing on data with various other shapes.

Figure 5: Visualization of the convergence curve of our BENO and two baselines.

Figure 6: Visualization of prediction and prediction error on 64 \(\times\) 64 grid resolution.

Figure 7: Visualization of prediction and prediction error on data with zero boundary value.

Figure 8: Visualization of prediction and prediction error from 3/2/1/No-Corner dataset, and each has two samples. We render the solution \(u\) of the baseline MP-PDE, our BENO and the ground truth in \(\Omega\).

[MISSING_PAGE_FAIL:20]

L2 norm of 0.3568 and an MAE of 0.8311, outperforming all other methods and showcasing the effectiveness of our approach under strict 4-corners conditions.

When trained on mixed boundary conditions in Table 8, BENO still maintains the highest accuracy, yielding an relative L2 norm of 0.4237 and an MAE of 1.0114 on the 4-Corners test set, confirming its robustness to varied training conditions. Notably, the improvement is significant in the more challenging No-Corner test set, where BENO's L2 is 0.3344, a remarkable enhancement over the baseline methods. The bolded figures in the tables highlight the instances where BENO outperforms all other models, underscoring the impact of our boundary-embedded techniques.

The consistency of BENO's performance under different boundary conditions underscores its potential for applications in computational physics where such scenarios are prevalent. Besides, the experimental outcomes affirm the efficacy of BENO in handling complex boundary problems in the context of PDEs. It is also worth noting that the BENO model not only improves the prediction accuracy but also exhibits a significant reduction in error across different test cases, which is critical for high-stakes applications such as numerical simulation in engineering and physical sciences.

## Appendix N Experiments on Darcy Flow

In this section, we consider the solution of the Darcy flow using our proposed BENO approach. The 2-d Darcy flow is a second-order linear elliptic equation of the form

\[\nabla\cdot(\kappa(x,y)\nabla u(x,y)) = f(x,y),\hskip 14.226378pt\forall(x,y)\in\Omega,\] (19) \[u(x,y) = g(x,y),\hskip 14.226378pt\forall(x,y)\in\partial\Omega,\]

where the coefficients \(\kappa\) is generated by taking a linear combination of smooth basis function in the solution domain. The coefficients of the linear combination of these basis functions is taken from uniform distribution of random numbers. Dirichlet boundary condition is imposed along the boundary

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c} \hline \hline \multicolumn{1}{c|}{Test set} & \multicolumn{2}{c|}{4-Corners} & \multicolumn{2}{c|}{3-Corners} & \multicolumn{2}{c|}{2-Corners} & \multicolumn{2}{c|}{1-Comer} & \multicolumn{2}{c}{No-Corner} \\ \hline Metric & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE \\ \hline \multirow{2}{*}{MP-PDE} & 0.5802\(\pm\) & 0.3269\(\pm\) & 0.5332\(\pm\) & 0.4652\(\pm\) & 0.6197\(\pm\) & 0.6307\(\pm\) & 0.6906\(\pm\) & 0.8469\(\pm\) & 0.7406\(\pm\) & 1.0906\(\pm\) \\  & 0.1840 & 0.2085 & 0.1742 & 0.2999 & 0.1709 & 0.3282 & 0.1432 & 0.4087 & 0.1271 & 0.3949 \\ \hline \multirow{2}{*}{**BENO (ours)**} & **0.2431\(\pm\)** & **0.1664\(\pm\)** & **0.2542\(\pm\)** & **0.2150\(\pm\)** & **0.2672\(\pm\)** & **0.2585\(\pm\)** & **0.2466\(\pm\)** & **0.3091\(\pm\)** & **0.2366\(\pm\)** & **0.3591\(\pm\)** \\  & **0.0895** & **0.0773** & **0.1252** & **0.1270** & **0.1497** & **0.1313** & **0.1405** & **0.2350** & **0.1104** & **0.2116** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Performances of our proposed BENO and the compared baselines on Darcy flow, which are trained on 900 4-corners samples and tested on 5 datasets under relative L2 norm and MAE separately. The unit of the MAE metric is \(1\times 10^{-3}\). Bold fonts indicate the best.

Figure 9: Visualization of the output from 2 GNN branches.

[MISSING_PAGE_FAIL:22]