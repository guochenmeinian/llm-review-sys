# CAr-Walk: Inductive Hypergraph Learning via Set Walks

 Ali Behrouz

Department of Computer Science

University of British Columbia

alibez@cs.ubc.ca

&Farnoosh Hashemi

Department of Computer Science

University of British Columbia

farsh@cs.ubc.ca

&Sadaf Sadeghian

Department of Computer Science

University of British Columbia

sadafsdn@cs.ubc.ca

&Margo Seltzer

Department of Computer Science

University of British Columbia

mseltzer@cs.ubc.ca

These two authors contributed equally (ordered alphabetically) and reserve the right to swap their order.

###### Abstract

Temporal hypergraphs provide a powerful paradigm for modeling time-dependent, higher-order interactions in complex systems. Representation learning for hypergraphs is essential for extracting patterns of the higher-order interactions that are critically important in real-world problems in social network analysis, neuroscience, finance, etc. However, existing methods are typically designed only for specific tasks or static hypergraphs. We present CAr-Walk, an inductive method that learns the underlying dynamic laws that govern the temporal and structural processes underlying a temporal hypergraph. CAr-Walk introduces a temporal, higher-order walk on hypergraphs, SetWalk, that extracts higher-order causal patterns. CAr-Walk uses a novel adaptive and permutation invariant pooling strategy, SetMixer, along with a set-based anonymization process that hides the identity of hyperedges. Finally, we present a simple yet effective neural network model to encode hyperedges. Our evaluation on 10 hypergraph benchmark datasets shows that CAr-Walk attains outstanding performance on temporal hyperedge prediction benchmarks in both inductive and transductive settings. It also shows competitive performance with state-of-the-art methods for node classification. (Code)

## 1 Introduction

Temporal networks have become increasingly popular for modeling interactions among entities in dynamic systems [1, 2, 3, 4, 5]. While most existing work focuses on pairwise interactions between entities, many real-world complex systems exhibit natural relationships among multiple entities [6, 7, 8]. Hypergraphs provide a natural extension to graphs by allowing an edge to connect any number of vertices, making them capable of representing higher-order structures in data. Representation learning on (temporal) hypergraphs has been recognized as an important machine learning problem and has become the cornerstone behind a wealth of high-impact applications in computer vision [9, 10], biology [11, 12], social networks [13, 14], and neuroscience [15, 16].

Many recent attempts to design representation learning methods for hypergraphs are equivalent to applying Graph Neural Networks (GNNs) to the clique-expansion (CE) of a hypergraph [17, 18, 19, 20, 21]. CE is a straightforward way to generalize graph algorithms to hypergraphs by replacing hyperedges with (weighted) cliques [18, 19, 20]. However, this decomposition of hyperedges limits expressiveness,leading to suboptimal performance [6, 22, 23, 24] (see Theorem 1 and Theorem 4). New methods that encode hypergraphs directly partially address this issue [25, 26, 27, 28, 11]. However, these methods suffer from some combination of the following three limitations: they are designed for 1 learning the structural properties of _static hypergraphs_ and do not consider temporal properties, 2 the transductive setting, limiting their performance on unseen patterns and data, and 3 a specific downstream task (e.g., node classification [25], hyperedge prediction [26], or subgraph classification [27]) and cannot easily be extended to other downstream tasks, limiting their application.

Temporal motif-aware and neighborhood-aware methods have been developed to capture complex patterns in data [29, 30, 31]. However, counting temporal motifs in large networks is time-consuming and non-parallelizable, limiting the scalability of these methods. To this end, several recent studies suggest using temporal random walks to automatically retrieve such motifs [32, 33, 34, 35, 36]. One possible solution to capturing underlying temporal and higher-order structure is to extend the concept of a hypergraph random walk [37, 38, 39, 40, 41, 42, 43] to its temporal counterpart by letting the walker walk over time. However, existing definitions of random walks on hypergraphs offer limited expressivity and sometimes degenerate to simple walks on the CE of the hypergraph [40] (see Appendix C). There are two reasons for this: 1 Random walks are composed of a sequence of _pair-wise_ interconnected vertices, even though edges in a hypergraph connect _sets_ of vertices. Decomposing them into sequences of simple pair-wise interactions loses the semantic meaning of the hyperedges (see Theorem 4). 2 A sampling probability of a walk on a hypergraph must be different from its sampling probability on the CE of the hypergraph [37, 38, 39, 40, 41, 42, 43]. However, Chitra and Raphael [40] shows that each definition of the random walk with edge-independent sampling probability of nodes is equivalent to random walks on a weighted CE of the hypergraph. Existing studies on random walks on hypergraphs ignore 1 and focus on 2 to distinguish the walks on simple graphs and hypergraphs. However, as we show in Table 2, 1 is equally important, if not more so.

For example, Figure 1 shows the procedure of existing walk-based machine learning methods on a temporal hypergraph. The neural networks in the model take as input only sampled walks. However, the output of the hypergraph walk [37, 38] and simple walk on the CE graph are the same. This means that the neural network cannot distinguish between pair-wise and higher-order interactions.

We present Causal Anonymous Set Walks (CAr-Walk), an inductive hyperedge learning method. We introduce a hyperedge-centric random walk on hypergraphs, called SetWalk, that automatically extracts temporal, higher-order motifs. The hyperedge-centric approach enables SetWalks to distinguish multi-way connections from their corresponding CEs (see Figure 1, Figure 2, and Theorem 1). We use temporal hypergraph motifs that reflect network dynamics (Figure 2) to enable CAr-Walk to work well in the inductive setting. To make the model agnostic to the hyperedge identities of these motifs, we use two-step, set-based anonymization: 1 Hide node identities by assigning them new positional encodings based on the number of times that they appear at a specific position in a set of sampled SetWalks, and 2 Hide hyperedge identities by combining the positional encodings of the nodes comprising the hyperedge using a novel permutation invariant pooling strategy, called SetMixer. We incorporate a neural encoding method that samples a few SetWalks starting from nodes of interest. It encodes and aggregates them via MLP-Mixer[44] and our new pooling strategy SetMixer, respectively, to predict temporal, higher-order interactions. Finally, we discuss how to extend CAr-Walk for node classification. Figure 3 shows the schematic of the CAr-Walk.

We theoretically and experimentally discuss the effectiveness of CAr-Walk and each of its components. More specifically, we prove that SetWalks are more expressive than existing random walk algorithms on hypergraphs. We demonstrate SetMixer's efficacy as a permutation invariant pooling strategy for hypergraphs and prove that using it in our anonymization process makes that process more expressive than existing anonymization processes [33; 45; 46] when applied to the CE of the hypergraphs. To the best of our knowledge, we report the most extensive experiments in the hypergraph learning literature pertaining to unsupervised hyperedge prediction with 10 datasets and eight baselines. Results show that our method produces 9% and 17% average improvement in transductive and inductive settings, outperforming all state-of-the-art baselines in the hyperedge prediction task. Also, CAr-Walk achieves the best or on-par performance on dynamic node classification tasks. All proofs appear in the Appendix.

## 2 Related Work

Temporal graph learning is an active research area [5; 47]. A major group of methods uses Gnns to learn node encodings and Recurrent Neural Networks (Rnns) to update these encodings over time [48; 49; 50; 51; 52; 53; 54; 55]. More sophisticated methods based on anonymous temporal random walks [33; 34], line graphs [56], GraphMixer [57], neighborhood representation [58], and subgraph sketching [59] are designed to capture complex structures in vertex neighborhoods. Although these methods show promising results in a variety of tasks, they are fundamentally limited in that they are designed for _pair-wise_ interaction among vertices and not the higher-order interactions in hypergraphs.

Representation learning on hypergraphs addresses this problem [17; 60]. We group work in this area into three overlapping categories:

1**Clique and Star Expansion**: CE-based methods replace hyperedges with (weighted) cliques and apply Gnns, sometimes with sophisticated propagation rules [21; 25], degree normalization, and nonlinear hyperedge weights [17; 18; 19; 20; 21; 39; 61; 62]. Although these methods are simple, it is well-known that CE causes undesired losses in learning performance, specifically when relationships within an incomplete subset of a hyperedge do not exist [6; 22; 23; 24]. Star expansion (SE) methods first use hypergraph star expansion and model the hypergraph as a bipartite graph, where one set of vertices represents nodes and the other represents hyperedges [25; 63; 64; 28]. Next, they apply modified heterogeneous GNNs, possibly with dual attention mechanisms from nodes to hyperedges and vice versa [25; 27]. Although this group does not cause as large a distortion as CE, they are neither memory nor computationally efficient. 2**Message Passing**: Most existing hypergraph learning methods, use message passing over hypergraphs [17; 18; 19; 20; 21; 25; 26; 27; 39; 61; 62; 65; 66]. Recently, Chien et al. [25] and Huang and Yang [28] designed universal message-passing frameworks that include propagation rules of most previous methods (e.g., [17; 19]). The main drawback of these two frameworks is that they are limited to node classification tasks and do not easily generalize to other tasks. 3**Walk-**based**: random walks are a common approach to extracting graph information for machine learning algorithms [32; 33; 34; 67; 68]. Several walk-based hypergraph learning methods are designed for a wide array of applications [69; 70; 71; 43; 68; 72; 73; 74; 75; 76; 77; 78]. However, most existing methods use simple random walks on the CE of the hypergraph (e.g., [26; 78; 43]). More complicated random walks on hypergraphs address this limitation [40; 41; 42; 79]. Although some of these studies show that their walk's transition matrix

Figure 3: **Schematic of the CAr-Walk.** CAr-Walk consists of three stages: (1) Causality Extraction via Set Walks (ยง3.2), (2) Set-based Anonymization (ยง3.3), and (3) Set Walk Encoding (ยง3.4).

[MISSING_PAGE_FAIL:4]

_where \(e_{i}\in\mathcal{E}\), \(t_{e_{i+1}}<t_{e_{i}}\), and the intersection of \(e_{i}\) and \(e_{i+1}\) is not empty, \(e_{i}\cap e_{i+1}\neq\emptyset\). In other words, for each \(1\leq i\leq\ell-1\): \(e_{i+1}\in\mathcal{E}^{i}(e_{i})\). We use \(\mathrm{Sw}[i]\) to denote the \(i\)-th hyperedge-time pair in the SetWalk. That is, \(\mathrm{Sw}[i][0]=e_{i}\) and \(\mathrm{Sw}[i][1]=t_{e_{i}}\)._

**Example 2**.: _Figure 1 illustrates a temporal hypergraph with two sampled SetWalks, hypergraph random walks, and simple walks. As an example, \((\{A,B,C\},t_{6})\rightarrow(\{C,D,E,F\},t_{5})\) is a SetWalk that starts from hyperedge \(e\) including \(\{A,B,C\}\) in time \(t_{6}\), backtracks overtime and then samples hyperedge \(e^{\prime}\) including \(\{C,D,E,F\}\) in time \(t_{5}<t_{6}\). While this higher-order random walk with length two provides information about all \(\{A,B,C,D,E,F\}\), its simple hypergraph walk counterpart, i.e. \(A\to C\to E\), provides information about only three nodes._

Next, we formally discuss the power of SetWalks. The proof of the theorem can be found in Appendix E.1.

**Theorem 1**.: _A random SetWalk is equivalent to neither the hypergraph random walk, the random walk on the CE graph, nor the random walk on the SE graph. Also, for a finite number of samples of each, SetWalk is more expressive than existing walks._

In Figure 1, SetWalks capture higher-order interactions and distinguish the two nodes \(A\) and \(H\), which are indistinguishable via hypergraph random walks and graph random walks in the CE graph. We present a more detailed discussion and comparison with previous definitions of random walks on hypergraphs in Appendix C.

**Causality Extraction.** We introduce a sampling method to allow SetWalks to extract temporal higher-order motifs that capture causal relationships by backtracking over time and sampling adjacent hyperedges. As discussed in previous studies [33; 34], more recent connections are usually more informative than older connections. Inspired by Wang et al. [33], we use a hyperparameter \(\alpha\geq 0\) to sample a hyperedge \(e\) with probability proportional to \(\exp\left(\alpha(t-t_{p})\right)\), where \(t\) and \(t_{p}\) are the timestamps of \(e\) and the previously sampled hyperedge in the SetWalk, respectively. Additionally, we want to bias sampling towards pairs of adjacent hyperedges that have a greater number of common nodes to capture higher-order motifs. However, as discussed in previous studies, the importance of each node for each hyperedge can be different [25; 27; 40; 65]. Accordingly, the transferring probability from hyperedge \(e_{i}\) to its adjacent hyperedge \(e_{j}\) depends on the importance of the nodes that they share. We address this via a temporal SetWalk sampling process with _hyperedge-dependent node weights_. Given a temporal hypergraph \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{X})\), a hyperedge-dependent node-weight function \(\Gamma:\mathcal{V}\times\mathcal{E}\rightarrow\mathbb{R}^{\geq 0}\), and a previously sampled hyperedge \((e_{p},t_{p})\), we sample a hyperedge \((e,t)\) with probability:

\[\mathbb{P}[(e,t)|(e_{p},t_{p})]\propto\frac{\exp\left(\alpha(t-t_{p})\right)} {\sum_{(e^{\prime},t^{\prime})\in\mathcal{E}^{\prime\prime}(e_{p})}\exp\left( \alpha(t^{\prime}-t_{p})\right)}\times\frac{\exp\left(\varphi(e,e_{p})\right)} {\sum_{(e^{\prime},t^{\prime})\in\mathcal{E}^{\prime\prime}(e_{p})}\exp\left( \varphi(e^{\prime},e_{p})\right)},\] (1)

where \(\varphi(e,e^{\prime})=\sum_{ue\in e^{\prime}\sim\Gamma}(u,e)\Gamma(u,e^{ \prime})\), representing the assigned weight to \(e\cap e^{\prime}\). We refer to the first and second terms as _temporal bias_ and _structural bias_, respectively.

The pseudocode of our SetWalk sampling algorithm and its complexity analysis are in Appendix D. We also discuss this hyperedge-dependent sampling procedure and how it is provably more expressive than existing hypergraph random walks in Appendix C.

Given a (potential) hyperedge \(e_{0}=\{u_{1},u_{2},\ldots,u_{k}\}\) and a time \(t_{0}\), we say a SetWalk, Sw, starts from \(u_{i}\) if \(u_{i}\in\mathrm{Sw}[1][0]\). We use the above procedure to generate \(M\) SetWalks with length \(m\) starting from each \(u_{i}\in e_{0}\). We use \(\mathcal{S}(u_{i})\) to store SetWalks that start from \(u_{i}\).

### Set-based Anonymization of Hyperedges

In the anonymization process, we replace hyperedge identities with position encodings, capturing structural information while maintaining the inductiveness of the method. Micali and Zhu [45] studied Anonymous Walks (AWs), which replace a _node's identity_ by the order of its appearance in each walk. The main limitation of AWs is that the position encoding of each node depends only on its specific walk, missing the dependency and correlation of different sampled walks [33]. To mitigate this drawback, Wang et al. [33] suggest replacing node identities with the hitting counts of the nodes based on a set of sampled walks. In addition to the fact that this method is designed for walks on simple graphs, there are two main challenges to adopting it for SetWalks: 1 SetWalksare a sequence of hyperedges, so we need an encoding for the position of hyperedges. Natural attempts to replace hyperedges' identity with the hitting counts of the hyperedges based on a set of sampled \(\textsc{SetWalk}\), misses the similarity of hyperedges with many of the same nodes. 2 Each hyperedge is a set of vertices and natural attempts to encode its nodes' positions and aggregate them to obtain a position encoding of the hyperedge requires a permutation invariant pooling strategy. This pooling strategy also requires consideration of the higher-order dependencies between obtained nodes' position encodings to take advantage of higher-order interactions (see Theorem 2). To address these challenges we present a set-based anonymization process for \(\textsc{SetWalk}\)s. Given a hyperedge \(e_{0}=\{u_{1},\ldots,u_{k}\}\), let \(w_{0}\) be any node in \(e_{0}\). For each node \(w\) that appears on at least one \(\textsc{SetWalk}\) in \(\bigcup_{i=1}^{k}\mathcal{S}(u_{i})\), we assign a relative, node-dependent node identity, \(\mathcal{R}(w,\mathcal{S}(w_{0}))\in\mathbb{Z}^{m}\), as follows:

\[\mathcal{R}(w,\mathcal{S}(w_{0}))[i]=|\{\mathrm{Sw}|\mathrm{Sw}\in\mathcal{ S}(w_{0}),w\in\mathrm{Sw}|i[0]|\}|\;\;\forall i\in\{1,2,\ldots,m\}.\] (2)

For each node \(w\) we further define \(\textsc{I}\textsc{o}(w,e_{0})=\{\mathcal{R}(w,\mathcal{S}(u_{i}))\}_{i=1}^{k}\). Let \(\Psi_{(\cdot)}:\mathbb{R}^{d\times d_{1}}\to\mathbb{R}^{d_{2}}\) be a pooling function that gets a set of \(d_{1}\)-dimensional vectors and aggregates them to a \(d_{2}\)-dimensional vector. Given two instances of this pooling function, \(\Psi_{(\cdot)}\) and \(\Psi_{2}(\cdot)\), for each hyperedge \(e=\{w_{1},w_{2},\ldots,w_{k}\}\) that appears on at least one \(\textsc{SetWalk}\) in \(\bigcup_{i=1}^{k}\mathcal{S}(u_{i})\), we assign a relative hyperedge identity as:

\[\textsc{I}\textsc{o}(e,e_{0})=\Psi_{1}\left(\{\Psi_{2}\left(\textsc{I}\textsc{ o}(w_{i},e_{0})\right)\}_{i=1}^{k^{\prime}}\right).\] (3)

That is, for each node \(w_{i}\in e\) we first aggregate its relative node-dependent identities (i.e., \(\mathcal{R}(w_{i},\mathcal{S}(u_{j}))\)) to obtain the relative hyperedge-dependent identity. Then we aggregate these hyperedge-dependent identities for all nodes in \(e\). Since the size of hyperedges can vary, we zero-pad to a fixed length. Note that this zero padding is important to capture the size of the hyperedge. The hyperedge with more zero-padded dimensions has fewer nodes.

This process addresses the first challenge and encodes the position of hyperedges. Unfortunately, many simple and known pooling strategies (e.g., \(\textsc{Sum}(.)\), \(\textsc{Attn-Sum}(.)\), \(\textsc{Mean}(.)\), etc.) can cause missing information when applied to hypergraphs. We formalize this in the following theorem:

**Theorem 2**.: _Given an arbitrary positive integer \(k\in\mathbb{Z}^{+}\), let \(\Psi_{(\cdot)}\) be a pooling function such that for any set \(S=\{w_{1},\ldots,w_{d}\}\):_

\[\Psi(S)=\sum_{\begin{subarray}{c}S^{\prime}\subseteq S\\ |v|+d\end{subarray}}f(S^{\prime}),\] (4)

_where \(f\) is some function. Then the pooling function can cause missing information, meaning that it limits the expressiveness of the method to applying to the projected graph of the hypergraph._

While simple concatenation does not suffer from this undesirable property, it is not permutation invariant. To overcome these challenges, we design an all-MLP permutation invariant pooling function, \(\textsc{SetMixer}\), that not only captures higher-order dependencies of set elements but also captures dependencies across the number of times that a node appears at a certain position in \(\textsc{SetWalk}\)s.

**SetMixer.** MLP-Mixer[44] is a family of models based on multi-layer perceptrons, widely used in the computer vision community, that are simple, amenable to efficient implementation, and robust to over-squashing and long-term dependencies (unlike Rnns and attention mechanisms) [44, 57]. However, the token-mixer phase of these methods is sensitive to the order of the input (see Appendix A). To address this limitation, inspired by MLP-Mixer[44], we design \(\textsc{SetMixer}\) as follows: Let \(S=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{d}\}\), where \(\mathbf{v}_{i}\in\mathbb{R}^{d_{1}}\), be the input set and \(\mathbf{V}=[\mathbf{v}_{1},\ldots,\mathbf{v}_{d}]\in\mathbb{R}^{d\times d_{1}}\) be its matrix representation:

\[\Psi(S)=\textsc{Mean}\left(\mathbf{H}_{\mathrm{token}}+\sigma\left(\textsc{ LayerNorm}\left(\mathbf{H}_{\mathrm{token}}\right)\mathbf{W}_{s}^{(1)}\right) \mathbf{W}_{s}^{(2)}\right),\] (5)

where

\[\mathbf{H}_{\mathrm{token}}=\mathbf{V}+\sigma\left(\textsc{Softmax}\left( \textsc{LayerNorm}\left(\mathbf{V}\right)^{T}\right)\right)^{T}.\] (6)

Here, \(\mathbf{W}_{s}^{(1)}\) and \(\mathbf{W}_{s}^{(2)}\) are learnable parameters, \(\sigma(.)\) is an activation function (we use \(\mathrm{GeLU}\)[80] in our experiments), and \(\textsc{LayerNorm}\) is layer normalization [81]. Equation 5 is the channel mixer and Equation 6 is the token mixer. The main intuition of \(\textsc{SetMixer}\) is to use the \(\textsc{Softmax}(.)\) function to bind token-wise information in a non-parametric manner, avoiding permutation variant operations in the token mixer. We formally prove the following theorem in Appendix E.3.

**Theorem 3**.: \(\textsc{SetMixer}\) _is permutation invariant and is a universal approximator of invariant multi-set functions. That is, \(\textsc{SetMixer}\) can approximate any invariant multi-set function._Based on the above theorem, SetMixer can overcome the challenges we discussed earlier as it is permutation invariant. Also, it is a universal approximator of multi-set functions, which shows its power to learn any arbitrary function. Accordingly, in our anonymization process, we use \(\Psi(.)=\textsc{SetMixer}(.)\) in Equation 3 to hide hyperedge identities. Next, we guarantee that our anonymization process does not depend on hyperedges or nodes identities, which justifies the claim of inductiveness of our model:

**Proposition 1**.: _Given two (potential) hyperedges \(e_{0}=\{u_{1},\ldots,u_{k}\}\) and \(e^{\prime}_{0}=\{u^{\prime}_{1},\ldots,u^{\prime}_{k}\}\), if there exists a bijective mapping \(\pi\) between node identities such that for each SetWalk like \(\textsc{Sw}\in\bigcup_{i=1}^{k}\mathcal{S}(u_{i})\) can be mapped to one SetWalk like \(\textsc{Sw}^{\prime}\in\bigcup_{i=1}^{k}\mathcal{S}(u^{\prime}_{i})\), then for each hyperedge \(e=\{w_{1},\ldots,w_{k}\}\) that appears in at least one SetWalk in \(\bigcup_{i=1}^{k}\mathcal{S}(u_{i})\), we have \(\textsc{ID}(e,e_{0})=\textsc{ID}(\pi(e),e^{\prime}_{0})\), where \(\pi(e)=\{\pi(w_{1}),\ldots,\pi(w_{k^{\prime}})\}\)._

Finally, we guarantee that our anonymization approach is more expressive than existing anonymization process [33; 45] when applied to the CE of the hypergraphs:

**Theorem 4**.: _The set-based anonymization method is more expressive than any existing anonymization strategies on the CE of the hypergraph. More precisely, there exists a pair of hypergraphs \(\mathcal{G}_{1}=(\mathcal{V}_{1},\mathcal{E}_{1})\) and \(\mathcal{G}_{2}=(\mathcal{V}_{2},\mathcal{E}_{2})\) with different structures (i.e., \(\mathcal{G}_{1}\not\cong\mathcal{G}_{2}\)) that are distinguishable by our anonymization process and are not distinguishable by the CE-based methods._

### SetWalk Encoding

Previous walk-based methods [33; 34; 78] view a walk as a sequence of nodes. Accordingly, they plug nodes' positional encodings in a Rsn [82] or Transformer [83] to obtain the encoding of each walk. However, in addition to the computational cost of Rnn and Transformers, they suffer from over-squashing and fail to capture long-term dependencies. To this end, we design a simple and low-cost SetWalk encoding procedure that uses two steps: 1 A time encoding module to distinguish different timestamps, and 2 A mixer module to summarize temporal and structural information extracted by SetWalks.

**Time Encoding.** We follow previous studies [33; 84] and adopt random Fourier features [85; 86] to encode time. However, these features are periodic, so they capture only periodicity in the data. We add a learnable linear term to the feature representation of the time encoding. We encode a given time \(t\) as follows:

\[\mathrm{T}(t)=(\bm{\omega}_{l}t+\mathbf{b}_{i})\,\|\cos(t\bm{\omega}_{p}),\] (7)

where \(\bm{\omega}_{l},\mathbf{b}_{l}\in\mathbb{R}\) and \(\bm{\omega}_{p}\in\mathbb{R}^{d_{2}-1}\) are learnable parameters and \(\|\) shows concatenation.

**Mixer Module.** To summarize the information in each SetWalk, we use a MLP-Mixer[44] on the sequence of hyperedges in a SetWalk as well as their corresponding encoded timestamps. Contrary to the anonymization process, where we need a permutation invariant procedure, here, we need a permutation variant procedure since the order of hyperedges in a SetWalk is important. Given a (potential) hyperedge \(e_{0}=\{u_{1},\ldots,u_{k}\}\), we first assign \(\textsc{ID}(e,e_{0})\) to each hyperedge \(e\) that appears on at least one sampled SetWalk starting from \(e_{0}\) (Equation 3). Given a SetWalk, \(\textsc{Sw}:(e_{1},t_{e_{1}})\to\cdots\to(e_{m},t_{e_{m}})\), we let \(\mathbf{E}\) be a matrix that \(\mathbf{E}_{i}=\textsc{ID}(e_{i},e_{0})|\Gamma(t_{e_{i}})\):

\[\textsc{Exc}(\textsc{Sw})=\textsc{Mean}\left(\mathbf{H}_{\textsc{token}}+ \sigma\left(\textsc{LayerNorm}\left(\mathbf{H}_{\textsc{token}}\right) \mathbf{W}_{\textsc{channel}}^{(1)}\right)\mathbf{W}_{\textsc{channel}}^{(2)} \right),\] (8)

where

\[\mathbf{H}_{\textsc{token}}=\mathbf{E}+\mathbf{W}_{\textsc{token}}^{(2)} \sigma\left(\mathbf{W}_{\textsc{token}}^{(1)}\textsc{LayerNorm}\left( \mathbf{E}\right)\right).\] (9)

### Training

In the training phase, for each hyperedge in the training set, we adopt the commonly used negative sample generation method [60] to generate a negative sample. Next, for each hyperedge in the training set such as \(e_{0}=\{u_{1},u_{2},\ldots,u_{k}\}\), including both positive and negative samples, we sample \(M\)SetWalks with length \(m\) starting from each \(u_{i}\in e_{0}\) to construct \(\mathcal{S}(u_{i})\). Next, we anonymize each hyperedge that appears in at least one SetWalk in \(\bigcup_{i=1}^{k}\mathcal{S}(u_{i})\) by Equation 3 and then use the Mixer module to encode each \(\textsc{Sw}\in\bigcup_{i=1}^{k}\mathcal{S}(u_{i})\). To encode each node \(u_{i}\in e_{0}\), we use Mean(.) pooling over SetWalks in \(\mathcal{S}(u_{i})\). Finally, to encode \(e_{0}\) we use SetMixer to mix obtained node encodings. For hyperedge prediction, we use a 2-layer perceptron over the hyperedge encodings to make the final prediction. We discuss node classification in Appendix G.2.

## 4 Experiments

We evaluate the performance of our model on two important tasks: hyperedge prediction and node classification (see Appendix G.2) in both inductive and transductive settings. We then discuss the importance of our model design and the significance of each component in CAr-Walk.

### Experimental Setup

**Baselines.** We compare our method to eight previous state-of-the-art baselines on the hyperedge prediction task. These methods can be grouped into three categories: 1 Deep hypergraph learning methods including HyperSAGCN [26], NHP [87], and CHESHIRE [11]. 2 Shallow methods including HPRA [88] and HPLSF [89]. 3 CE methods: CE-CAW [33], CE-EvolveGCN [90] and CE-GCN [91]. Details on these models and hyperparameters used appear in Appendix F.2.

**Datasets.** We use 10 available benchmark datasets [6] from the existing hypergraph neural networks literature. These datasets' domains include drug networks (i.e., NDC [6]), contact networks (i.e., High School [92] and Primary School [93]), the US. Congress bills network [94; 95], email networks (i.e., Email Enron [6] and Email Eu [96]), and online social networks (i.e., Question Tags and Users-Threads [6]). Detailed descriptions of these datasets appear in Appendix F.1.

**Evaluation Tasks.** We focus on Hyperedge Prediction: In the transductive setting, we train on the temporal hyperedges with timestamps less than or equal to \(T_{\text{train}}\) and test on those with timestamps greater than \(T_{\text{train}}\). Inspired by Wang et al. [33], we consider two inductive settings. In the **Strongly Inductive** setting, we predict hyperedges consisting of some unseen nodes. In the **Weakly Inductive** setting,we predict hyperedges with _at least_ one seen and some unseen nodes. We first follow the procedure used in the transductive setting, and then we randomly select 10% of the nodes and remove all hyperedges that include them from the training set. We then remove all hyperedges with seen nodes from the validation and testing sets. For dynamic node classification, see Appendix G.2. For all datasets, we fix \(T_{\text{train}}=0.7\,T\), where \(T\) is the last timestamp. To evaluate the models' performance we follow the literature and use Area Under the ROC curve (AUC) and Average Precision (AP).

### Results and Discussion

**Hyperedge Prediction.** We report the results of CAr-Walk and baselines in Table 1 and Appendix G. The results show that CAr-Walk achieves the best overall performance compared to the baselines in both transductive and inductive settings. In the transductive setting, not only does our method outperform baselines in all but one dataset, but it achieves near perfect results (i.e., \(\approx\) 98.0) on the NDC and Primary School datasets. In the Weakly Inductive setting, our model achieves high scores (i.e., \(>\) 91.5) in all but one dataset, while most baselines perform poorly as they are not designed for the inductive setting and do not generalize well to unseen nodes or patterns. In the Strongly Inductive setting, CAr-Walk still achieves high AUC (i.e., \(>\) 90.0) on most datasets and outperforms baselines on _all_ datasets. There are three main reasons for CAr-Walk's superior performance: 1 Our SetWalk's capture higher-order patterns. 2 CAT-Walk incorporates temporal properties (both from SetWalk's and our time encoding module), thus learning underlying dynamic laws of the network. The other temporal methods (CE-CAW and CE-EvolveGCN) are CE-based methods, limiting their ability to capture higher-order patterns. 3 CAr-Walk's set-based anonymization process that avoids using node and hyperedge identities allows it to generalize to unseen patterns and nodes.

**Ablation Studies.** We next conduct ablation studies on the High School, Primary School, and Users-Threads datasets to validate the effectiveness of CAr-Walk's critical components. Table 2 shows AUC results for inductive hyperedge prediction. The first row reports the performance of the complete CAr-Walk implementation. Each subsequent row shows results for CAr-Walk with one module modification: row 2 replace SetWalk by edge-dependent hypergraph walk [40], row 3 removes the time encoding module, row 4 replaces SetMixer with Mean(.) pooling, row 5 replaces the SetMixer with sum-based universal approximator for sets [97], row 6 replaces the MLP-Mixermodule with a Rnn (see Appendix G for more experiments on the significance of using MLP-Mixer in walk encoding), row 7 replaces the MLP-Mixer module with a Transformer [83], and row 8 replaces the hyperparameter \(\alpha\) with uniform sampling of hyperedges over all time periods. These results show that each component is critical for achieving CAr-Walk's superior performance. The greatest contribution comes from SetWalk, MLP-Mixer in walk encoding, \(\alpha\) in temporal hyperedge sampling, and SetMixer pooling, respectively.

**Hyperparameter Sensitivity.** We analyze the effect of hyperparameters used in CAr-Walk, including temporal bias coefficient \(\alpha\), SetWalk length \(m\), and sampling number \(M\). The mean AUC performance on all inductive test hyperedges is reported in Figure 4. As expected, the left figure shows that

\begin{table}
\begin{tabular}{l l l l l l l l l l} \hline \hline \multicolumn{2}{c}{Methods} & \multicolumn{2}{c}{NDC Class} & \multicolumn{2}{c}{High School} & \multicolumn{2}{c}{Primary School} & \multicolumn{2}{c}{Consess Hill} & \multicolumn{2}{c}{Final Enron} & \multicolumn{2}{c}{Cranial En} & \multicolumn{2}{c}{Question Tags} & \multicolumn{2}{c}{Users-Threads} \\ \hline \hline \multicolumn{2}{c}{} & \multicolumn{1}{c}{Stongly Indextive} & \multicolumn{2}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline CE-GCN & 52.31 \(\pm\) 2.99 & 60.54 \(\pm\) 2.06 & 52.34 \(\pm\) 2.75 & 49.18 \(\pm\) 3.61 & 63.04 \(\pm\) 1.80 & 52.76 \(\pm\) 2.41 & 56.10 \(\pm\) 1.88 & 57.91 \(\pm\) 1.56 \\ CE-EvwGCN & 49.78 \(\pm\) 3.13 & 46.12 \(\pm\) 3.83 & 58.01 \(\pm\) 2.56 & 54.00 \(\pm\) 1.84 & 57.31 \(\pm\) 4.19 & 44.16 \(\pm\) 1.27 & 64.08 \(\pm\) 2.75 & 52.00 \(\pm\) 2.32 \\ CE-CAW & 76.45 \(\pm\) 2.02 & 83.73 \(\pm\) 1.42 & 80.31 \(\pm\) 1.46 & 75.83 \(\pm\) 1.25 & 70.81 \(\pm\) 1.13 & 72.99 \(\pm\) 0.20 & 70.14 \(\pm\) 1.89 & 73.12 \(\pm\) 1.06 \\ NHP & 70.43 \(\pm\) 4.95 & 65.29 \(\pm\) 1.80 & 70.86 \(\pm\) 1.42 & 69.82 \(\pm\) 2.19 & 47.11 \(\pm\) 6.09 & 65.35 \(\pm\) 2.07 & 68.23 \(\pm\) 1.34 & 71.83 \(\pm\) 2.64 \\ HvwrnSAGCN & 79.05 \(\pm\) 2.48 & 88.12 \(\pm\) 3.01 & 80.13 \(\pm\) 1.38 & 79.51 \(\pm\) 1.27 & 73.09 \(\pm\) 2.60 & 78.01 \(\pm\) 1.24 & 73.66 \(\pm\) 1.95 & 73.94 \(\pm\) 2.57 \\ CHESHIRE & 72.24 \(\pm\) 2.63 & 82.54 \(\pm\) 0.88 & 77.26 \(\pm\) 1.01 & 79.43 \(\pm\) 1.58 & 70.03 \(\pm\) 2.55 & 69.98 \(\pm\) 2.71 & N/A & 76.99 \(\pm\) 2.82 \\ CA-Wax & **0.983\(\pm\) 1.02** & **96.03\(\pm\) 1.50** & **93.32\(\pm\) 0.09** & **93.54\(\pm\) 0.06** & **72.43\(\pm\) 0.92** & 91.61 \(\pm\) 2.78 & **78.01 \(\pm\) 0.33** & **78.01 \(\pm\) 0.35** & **79.84 \(\pm\) 6.02** \\ \cline{2-11} \multirow{6}{*}{C-Avux} & \multicolumn{1}{c}{Weakly Indextive} & \multicolumn{2}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \cline{1-increasing the number of sampled SetWalks produces better performance. The main reason is that the model has more extracted structural and temporal information from the network. Also, notably, we observe that only a small number of sampled SetWalks are needed to achieve competitive performance: in the best case 1 and in the worst case 16 sampled SetWalks per each hyperedge are needed. The middle figure reports the effect of the length of SetWalks on performance. These results show that performance peaks at certain SetWalk lengths and the exact value varies with the dataset. That is, longer SetWalks are required for the networks that evolve according to more complicated laws encoded in temporal higher-order motifs. The right figure shows the effect of the temporal bias coefficient \(\alpha\). Results suggest that \(\alpha\) has a dataset-dependent optimal interval. That is, a small \(\alpha\) suggests an almost uniform sampling of interaction history, which results in poor performance when the short-term dependencies (interactions) are more important in the dataset. Also, very large \(\alpha\) might damage performance as it makes the model focus on the most recent few interactions, missing long-term patterns.

Scalability Analysis.In this part, we investigate the scalability of CAr-Walk. To this end, we use different versions of the High School dataset with different numbers of hyperedges from \(10^{4}\) to \(1.6\times 10^{5}\). Figure 5 (left) reports the runtimes of SetWalk sampling and Figure 5 (right) reports the runtimes of CAr-Walk for training one epoch using \(M=8\), \(m=3\) with batch-size \(=64\). Interestingly, our method scales linearly with the number of hyperedges, which enables it to be used on long hyperedge streams and large hypergraphs.

## 5 Conclusion, Limitation, and Future Work

We present CAr-Walk, an inductive hypergraph representation learning that learns both higher-order patterns and the underlying dynamic laws of temporal hypergraphs. CAr-Walk uses SetWalks, a new temporal, higher-order random walk on hypergraphs that are provably more expressive than existing walks on hypergraphs, to extract temporal higher-order motifs from hypergraphs. CAr-Walk then uses a two-step, set-based anonymization process to establish the correlation between the extracted motifs. We further design a permutation invariant pooling strategy, SetMixer, for aggregating nodes' positional encodings in a hyperedge to obtain hyperedge level positional encodings. Consequently, the experimental results show that CAr-Walk 1 produces superior performance compared to the state-of-the-art in temporal hyperedge prediction tasks, and 2 competitive performance in temporal node classification tasks. These results suggest many interesting directions for future studies: Using CAr-Walk as a positional encoder in existing anomaly detection frameworks to design an inductive anomaly detection method on hypergraphs. There are, however, a few limitations: Currently, CAr-Walk uses _fixed-length_ SetWalks, which might cause suboptimal performance. Developing a procedure to learn from SetWalks of varying lengths might produce better results.

## Acknowledgments and Disclosure of Funding

We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC).

Nous remercions le Conseil de recherches en sciences naturelles et en genie du Canada (CRSNG) de son soutien.

Figure 5: Scalability evaluation: The runtime of **(left)** SetWalk extraction and **(right)** the training time of CAr-Walk over one epoch on High School (using different \(|E|\) for training).

## References

* Chang et al. [2021] Serina Chang, Emma Pierson, Pang Wei Koh, Jaline Gerardin, Beth Redbird, David Grusky, and Jure Leskovec. Mobility network models of covid-19 explain inequities and inform reopening. _Nature_, 589(7840):82-87, 2021.
* Simpson et al. [2022] Michael Simpson, Farnoosh Hashemi, and Laks V. S. Lakshmanan. Misinformation mitigation under differential propagation rates and temporal penalties. _Proc. VLDB Endow._, 15(10):2216-2229, jun 2022. ISSN 2150-8097. doi: 10.14778/3547305.3547324. URL https://doi.org/10.14778/3547305.3547324.
* Ciaperoni et al. [2020] Martino Ciaperoni, Edoardo Galimberti, Francesco Bonchi, Ciro Cattuto, Francesco Gullo, and Alain Barrat. Relevance of temporal cores for epidemic spread in temporal networks. _Scientific reports_, 10(1):1-15, 2020.
* Hashemi et al. [2022] Farnoosh Hashemi, Ali Behrouz, and Laks V.S. Lakshmanan. Firmcore decomposition of multilayer networks. In _Proceedings of the ACM Web Conference 2022_, WWW '22, page 1589-1600, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450390965. doi: 10.1145/3485447.3512205. URL https://doi.org/10.1145/3485447.3512205.
* Longa et al. [2023] Antonio Longa, Veronica Lachi, Gabriele Santin, Monica Bianchini, Bruno Lepri, Pietro Lio, Franco Scarselli, and Andrea Passerini. Graph neural networks for temporal graphs: State of the art, open challenges, and opportunities. _arXiv preprint arXiv:2302.01018_, 2023.
* Benson et al. [2018] Austin R. Benson, Rediet Abebe, Michael T. Schaub, Ali Jadbabaie, and Jon Kleinberg. Simplicial closure and higher-order link prediction. _Proceedings of the National Academy of Sciences_, 2018. ISSN 0027-8424. doi: 10.1073/pnas.1800683115.
* Battiston et al. [2021] Federico Battiston, Enrico Amico, Alain Barrat, Ginestra Bianconi, Guilherme Ferraz de Arruda, Benedetta Franceschiello, Iacopo Iacopini, Sonia Kefi, Vito Latora, Yamir Moreno, Micah M. Murray, Tiago P. Peixoto, Francesco Vaccarino, and Giovanni Petri. The physics of higher-order interactions in complex systems. _Nature Physics_, 17(10):1093-1098, Oct 2021. ISSN 1745-2481. doi: 10.1038/s41567-021-01371-4. URL https://doi.org/10.1038/s41567-021-01371-4.
* Zhang et al. [2023] Yuanzhao Zhang, Maxime Lucas, and Federico Battiston. Higher-order interactions shape collective dynamics differently in hypergraphs and simplicial complexes. _Nature Communications_, 14(1):1605, Mar 2023. ISSN 2041-1723. doi: 10.1038/s41467-023-37190-9. URL https://doi.org/10.1038/s41467-023-37190-9.
* Kim et al. [2020] Eun-Sol Kim, Woo Young Kang, Kyoung-Woon On, Yu-Jung Heo, and Byoung-Tak Zhang. Hypergraph attention networks for multimodal learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 14581-14590, 2020.
* Yan et al. [2020] Yichao Yan, Jie Qin, Jiaxin Chen, Li Liu, Fan Zhu, Ying Tai, and Ling Shao. Learning multi-granular hypergraphs for video-based person re-identification. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2899-2908, 2020.
* Chen et al. [2023] Can Chen, Chen Liao, and Yang-Yu Liu. Teasing out missing reactions in genome-scale metabolic networks through hypergraph learning. _Nature Communications_, 14(1):2375, 2023.
* Xie et al. [2022] Guobo Xie, Yinting Zhu, Zhiyi Lin, Yuping Sun, Guosheng Gu, Jianming Li, and Weiming Wang. Hbrwrlda: predicting potential Incrna-disease associations based on hypergraph bi-random walk with restart. _Molecular Genetics and Genomics_, 297(5):1215-1228, 2022.
* Yu et al. [2021] Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung, and Xiangliang Zhang. Self-supervised multi-channel hypergraph convolutional network for social recommendation. In _Proceedings of the web conference 2021_, pages 413-424, 2021.
* Yang et al. [2019] Dingqi Yang, Bingqing Qu, Jie Yang, and Philippe Cudre-Mauroux. Revisiting user mobility and social relationships in lbsns: a hypergraph embedding approach. In _The world wide web conference_, pages 2147-2157, 2019.

* Pan et al. [2021] Junren Pan, Baiying Lei, Yanyan Shen, Yong Liu, Zhiguang Feng, and Shuqiang Wang. Characterization multimodal connectivity of brain network by hypergraph gan for alzheimer's disease analysis. In _Pattern Recognition and Computer Vision: 4th Chinese Conference, PRCV 2021, Beijing, China, October 29-November 1, 2021, Proceedings, Part III 4_, pages 467-478. Springer, 2021.
* Xiao et al. [2019] Li Xiao, Junqi Wang, Peyman H Kassani, Yipu Zhang, Yuntong Bai, Julia M Stephen, Tony W Wilson, Vince D Calhoun, and Yu-Ping Wang. Multi-hypergraph learning-based brain functional connectivity analysis in fmri data. _IEEE transactions on medical imaging_, 39(5):1746-1758, 2019.
* Feng et al. [2019] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 3558-3565, 2019.
* Zhou et al. [2006] Dengyong Zhou, Jiayuan Huang, and Bernhard Scholkopf. Learning with hypergraphs: Clustering, classification, and embedding. In B. Scholkopf, J. Platt, and T. Hoffman, editors, _Advances in Neural Information Processing Systems_, volume 19. MIT Press, 2006. URL https://proceedings.neurips.cc/paper_files/paper/2006/file/df8e9c2ac33381546d96deea9922999-Paper.pdf.
* Yadati et al. [2019] Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha Talukdar. Hypergcn: A new method for training graph convolutional networks on hypergraphs. _Advances in neural information processing systems_, 32, 2019.
* Agarwal et al. [2005] Sameer Agarwal, Jongwoo Lim, Lihi Zelnik-Manor, Pietro Perona, David Kriegman, and Serge Belongie. Beyond pairwise clustering. In _2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)_, volume 2, pages 838-845. IEEE, 2005.
* Bai et al. [2021] Song Bai, Feihu Zhang, and Philip HS Torr. Hypergraph convolution and hypergraph attention. _Pattern Recognition_, 110:107637, 2021.
* learning on hypergraphs revisited. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/2013/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf.
* Li and Milenkovic [2018] Pan Li and Olgica Milenkovic. Submodular hypergraphs: p-laplacians, Cheeger inequalities and spectral clustering. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 3014-3023. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr.press/v80/li18e.html.
* [24] I (Eli) Chien, Huozhi Zhou, and Pan Li. _hs\({}^{2}\)_: Active learning over hypergraphs with pointwise and pairwise queries. In Kamalika Chaudhuri and Masashi Sugiyama, editors, _Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics_, volume 89 of _Proceedings of Machine Learning Research_, pages 2466-2475. PMLR, 16-18 Apr 2019. URL https://proceedings.mlr.press/v89/chien19a.html.
* Chien et al. [2022] Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are allset: A multiset function framework for hypergraph neural networks. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=hbpBITv2uv_E.
* Zhang et al. [2020] Ruochi Zhang, Yuesong Zou, and Jian Ma. Hyper-sagnn: a self-attention based graph neural network for hypergraphs. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=ryeHuJBtPH.
* Luo [2022] Yuan Luo. SHINE: Subhypergraph inductive neural network. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=IsHRUzXPqhI.

* Huang and Yang [2021] Jing Huang and Jie Yang. Unignn: a unified framework for graph and hypergraph neural networks. In Zhi-Hua Zhou, editor, _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21_, pages 2563-2569. International Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/353. URL https://doi.org/10.24963/ijcai.2021/353. Main Track.
* AbuOda et al. [2020] Ghadeer AbuOda, Gianmarco De Francisci Morales, and Ashraf Aboulnaga. Link prediction via higher-order motif features. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2019, Wurzburg, Germany, September 16-20, 2019, Proceedings, Part I_, pages 412-429. Springer, 2020.
* Lahiri and Berger-Wolf [2007] Mayank Lahiri and Tanya Y Berger-Wolf. Structure prediction in temporal networks using frequent subgraphs. In _2007 IEEE Symposium on computational intelligence and data mining_, pages 35-42. IEEE, 2007.
* Rahman and Hasan [2016] Mahmudur Rahman and Mohammad Al Hasan. Link prediction in dynamic networks using graphlet. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16_, pages 394-409. Springer, 2016.
* Nguyen et al. [2018] Giang H Nguyen, John Boaz Lee, Ryan A Rossi, Nesreen K Ahmed, Eunyee Koh, and Sungchul Kim. Dynamic network embeddings: From random walks to temporal random walks. In _2018 IEEE International Conference on Big Data (Big Data)_, pages 1085-1092. IEEE, 2018.
* Wang et al. [2021] Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. Inductive representation learning in temporal networks via causal anonymous walks. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=KYPz4VsCPj.
* Jin et al. [2022] Ming Jin, Yuan-Fang Li, and Shirui Pan. Neural temporal walks: Motif-aware representation learning on continuous-time dynamic graphs. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=NqbktPUkZf7.
* Liu et al. [2020] Zhining Liu, Dawei Zhou, Yada Zhu, Jinjie Gu, and Jingrui He. Towards fine-grained temporal network representation via time-reinforced random walk. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 4973-4980, 2020.
* Nguyen et al. [2018] Giang Hoang Nguyen, John Boaz Lee, Ryan A Rossi, Nesreen K Ahmed, Eunyee Koh, and Sungchul Kim. Continuous-time dynamic network embeddings. In _Companion proceedings of the the web conference 2018_, pages 969-976, 2018.
* Carletti et al. [2020] Timoteo Carletti, Federico Battiston, Giulia Cencetti, and Duccio Fanelli. Random walks on hypergraphs. _Physical review E_, 101(2):022308, 2020.
* Hayashi et al. [2020] Koby Hayashi, Sinan G Aksoy, Cheong Hee Park, and Haesun Park. Hypergraph random walks, laplacians, and clustering. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, pages 495-504, 2020.
* Payne [2019] Josh Payne. Deep hyperedges: a framework for transductive and inductive learning on hypergraphs, 2019.
* Chitra and Raphael [2019] Uthsav Chitra and Benjamin Raphael. Random walks on hypergraphs with edge-dependent vertex weights. In _International conference on machine learning_, pages 1172-1181. PMLR, 2019.
* Aksoy et al. [2020] Sinan G. Aksoy, Cliff Joslyn, Carlos Ortiz Marrero, Brenda Praggastis, and Emilie Purvine. Hypernetwork science via high-order hypergraph walks. _EPJ Data Science_, 9(1):16, Jun 2020. ISSN 2193-1127. doi: 10.1140/epjds/s13688-020-00231-0. URL https://doi.org/10.1140/epjds/s13688-020-00231-0.

* Benson et al. [2017] Austin R. Benson, David F. Gleich, and Lex-Heng Lim. The spacey random walk: A stochastic process for higher-order data. _SIAM Review_, 59(2):321-345, 2017. doi: 10.1137/16M1074023. URL https://doi.org/10.1137/16M1074023.
* Sharma et al. [2018] Ankit Sharma, Shafiq Joty, Himanshu Kharkwal, and Jaideep Srivastava. Hyperedge2vec: Distributed representations for hyperedges, 2018. URL https://openreview.net/forum?id=rJ5C67-C-.
* Tolstikhin et al. [2021] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Peter Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. MLP-mixer: An all-MLP architecture for vision. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=EI2K0XKdnP.
* Micali and Zhu [2016] Silvio Micali and Zeyuan Allen Zhu. Reconstructing markov processes from independent and anonymous experiments. _Discrete Applied Mathematics_, 200:108-122, 2016. ISSN 0166-218X. doi: https://doi.org/10.1016/j.dam.2015.06.035. URL https://www.sciencedirect.com/science/article/pii/S0166218X15003212.
* Behrouz and Seltzer [2023] Ali Behrouz and Margo Seltzer. ADMIRE++: Explainable anomaly detection in the human brain via inductive learning on temporal multiplex networks. In _ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH)_, 2023. URL https://openreview.net/forum?id=t4H8acYudJ.
* Skarding et al. [2021] Joakim Skarding, Bogdan Gabrys, and Katarzyna Musial. Foundations and modeling of dynamic networks using dynamic graph neural networks: A survey. _IEEE Access_, 9:79143-79168, 2021.
* Seo et al. [2018] Youngjoo Seo, Michael Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence modeling with graph convolutional recurrent networks. In _International conference on neural information processing_, pages 362-373. Springer, 2018.
* Zhao et al. [2019] Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao Lin, Min Deng, and Haifeng Li. T-gcn: A temporal graph convolutional network for traffic prediction. _IEEE Transactions on Intelligent Transportation Systems_, 21(9):3848-3858, 2019.
* Peng et al. [2020] Hao Peng, Hongfei Wang, Bowen Du, Md Zakirul Alam Bhuiyan, Hongyuan Ma, Jianwei Liu, Lihong Wang, Zeyu Yang, Linfeng Du, Senzhang Wang, et al. Spatial temporal incidence dynamic graph neural networks for traffic flow forecasting. _Information Sciences_, 521:277-290, 2020.
* Wang et al. [2020] Xiaoyang Wang, Yao Ma, Yiqi Wang, Wei Jin, Xin Wang, Jiliang Tang, Caiyan Jia, and Jian Yu. Traffic flow prediction via spatial temporal graph neural network. In _Proceedings of The Web Conference 2020_, pages 1082-1092, 2020.
* You et al. [2022] Jiaxuan You, Tianyu Du, and Jure Leskovec. Roland: Graph learning framework for dynamic graphs. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '22, page 2358-2366, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393850. doi: 10.1145/3534678.3539300. URL https://doi.org/10.1145/3534678.3539300.
* Hashemi et al. [2023] Farnoosh Hashemi, Ali Behrouz, and Milad Rezaei Hajidehi. Cs-tgn: Community search via temporal graph neural networks. In _Companion Proceedings of the Web Conference 2023_, WWW '23, New York, NY, USA, 2023. Association for Computing Machinery. doi: 10.1145/3543873.3587654. URL https://doi.org/10.1145/3543873.3587654.
* Kumar et al. [2019] Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory in temporal interaction networks. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, KDD '19, page 1269-1278, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450362016. doi: 10.1145/3292500.3330895. URL https://doi.org/10.1145/3292500.3330895.

* Behrouz and Seltzer [2022] Ali Behrouz and Margo Seltzer. Anomaly detection in multiplex dynamic networks: from blockchain security to brain disease prediction. In _NeurIPS 2022 Temporal Graph Learning Workshop_, 2022. URL https://openreview.net/forum?id=UDGZDfwmay.
* Chanpuriya et al. [2023] Sudhanshu Chanpuriya, Ryan A. Rossi, Sungchul Kim, Tong Yu, Jane Hoffswell, Nedim Lipka, Shunan Guo, and Cameron N Musco. Direct embedding of temporal network edges via time-decayed line graphs. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=Qamz7Q_Talk.
* Cong et al. [2023] Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, Hanghang Tong, and Mehrdad Mahdavi. Do we really need complicated model architectures for temporal networks? In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=ayPPC@Sylv1.
* Luo and Li [2022] Yuhong Luo and Pan Li. Neighborhood-aware scalable temporal network representation learning. In _The First Learning on Graphs Conference_, 2022. URL https://openreview.net/forum?id=EPUtMe7a9ta.
* Chamberlain et al. [2023] Benjamin Paul Chamberlain, Sergey Shirobokov, Emanuele Rossi, Fabrizio Frasca, Thomas Markovich, Nils Yannick Hammerla, Michael M. Bronstein, and Max Hansmire. Graph neural networks for link prediction with subgraph sketching. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=mloqEO0aozQU.
* Chen and Liu [2022] Can Chen and Yang-Yu Liu. A survey on hyperlink prediction. _arXiv preprint arXiv:2207.02911_, 2022.
* Arya et al. [2021] Devanshu Arya, Deepak Gupta, Stevan Rudinac, and Marcel Worring. Hyper{sage}: Generalizing inductive representation learning on hypergraphs, 2021. URL https://openreview.net/forum?id=cKnKJcTPRCV.
* Arya et al. [2021] Devanshu Arya, Deepak K Gupta, Stevan Rudinac, and Marcel Worring. Adaptive neural message passing for inductive learning on hypergraphs. _arXiv preprint arXiv:2109.10683_, 2021.
* Agarwal et al. [2006] Sameer Agarwal, Kristin Branson, and Serge Belongie. Higher order learning with graphs. In _Proceedings of the 23rd international conference on Machine learning_, pages 17-24, 2006.
* Yang et al. [2020] Chaoqi Yang, Ruijie Wang, Shuochao Yao, and Tarek Abdelzaher. Hypergraph learning with line expansion. _arXiv preprint arXiv:2005.04843_, 2020.
* Ding et al. [2020] Kaize Ding, Jianling Wang, Jundong Li, Dingcheng Li, and Huan Liu. Be more with less: Hypergraph attention networks for inductive text classification. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 4927-4936, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.399. URL https://aclanthology.org/2020.emnlp-main.399.
* Du et al. [2021] Boxin Du, Changhe Yuan, Robert Barton, Tal Neiman, and Hanghang Tong. Hypergraph pre-training with graph neural networks. _arXiv preprint arXiv:2105.10862_, 2021.
* Grover and Leskovec [2016] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In _Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 855-864, 2016.
* Perozzi et al. [2014] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In _Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 701-710, 2014.
* Xu et al. [2023] Xin-Jian Xu, Chong Deng, and Li-Jie Zhang. Hyperlink prediction via local random walks and jensen-shannon divergence. _Journal of Statistical Mechanics: Theory and Experiment_, 2023(3):033402, mar 2023. doi: 10.1088/1742-5468/acc31e. URL https://dx.doi.org/10.1088/1742-5468/acc31e.

* Latta et al. [2022] Valerio La Gatta, Vincenzo Moscato, Mirko Pennone, Marco Postiglione, and Giancarlo Sperli. Music recommendation via hypergraph embedding. _IEEE Transactions on Neural Networks and Learning Systems_, pages 1-13, 2022. doi: 10.1109/TNNLS.2022.3146968.
* Yang et al. [2019] Dingqi Yang, Bingqing Qu, Jie Yang, and Philippe Cudre-Mauroux. Revisiting user mobility and social relationships in lbsns: A hypergraph embedding approach. In _The World Wide Web Conference_, WWW '19, page 2147-2157, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450366748. doi: 10.1145/3308558.3313635. URL https://doi.org/10.1145/3308558.3313635.
* Ducournau and Bretto [2014] Aurelien Ducournau and Alain Bretto. Random walks in directed hypergraphs and application to semi-supervised image segmentation. _Computer Vision and Image Understanding_, 120:91-102, 2014. ISSN 1077-3142. doi: https://doi.org/10.1016/j.cviu.2013.10.012. URL https://www.sciencedirect.com/science/article/pii/S1077314213002038.
* Ding and Yilmaz [2010] Lei Ding and Alper Yilmaz. Interactive image segmentation using probabilistic hypergraphs. _Pattern Recognition_, 43(5):1863-1873, 2010. ISSN 0031-3203. doi: https://doi.org/10.1016/j.patcog.2009.11.025. URL https://www.sciencedirect.com/science/article/pii/S0031320309004440.
* Satchianand et al. [2015] Sai Nageswar Satchianand, Harini Ananthapadmanaban, and Balaraman Ravindran. Extended discriminative random walk: A hypergraph approach to multi-view multi-relational transductive learning. In _Proceedings of the 24th International Conference on Artificial Intelligence_, IJCAI'15, page 3791-3797. AAAI Press, 2015. ISBN 9781577357384.
* Huang et al. [2019] Jie Huang, Chuan Chen, Fanghua Ye, Jiajing Wu, Zibin Zheng, and Guohui Ling. Hyper2vec: Biased random walk for hyper-network embedding. In Guoliang Li, Jun Yang, Joao Gama, Juggapong Natwichai, and Yongxin Tong, editors, _Database Systems for Advanced Applications_, pages 273-277, Cham, 2019. Springer International Publishing. ISBN 978-3-030-18590-9.
* Huang et al. [2019] Jie Huang, Xin Liu, and Yangqiu Song. Hyper-path-based representation learning for hypernetworks. In _Proceedings of the 28th ACM International Conference on Information and Knowledge Management_, CIKM '19, page 449-458, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450369763. doi: 10.1145/3357384.3357871. URL https://doi.org/10.1145/3357384.3357871.
* Huang et al. [2020] Jie Huang, Chuan Chen, Fanghua Ye, Weibo Hu, and Zibin Zheng. Nonuniform hyper-network embedding with dual mechanism. _ACM Trans. Inf. Syst._, 38(3), may 2020. ISSN 1046-8188. doi: 10.1145/3388924. URL https://doi.org/10.1145/3388924.
* Liu et al. [2022] Yunyu Liu, Jianzhu Ma, and Pan Li. Neural predicting higher-order patterns in temporal networks. In _Proceedings of the ACM Web Conference 2022_, WWW '22, page 1340-1351, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450390965. doi: 10.1145/3485447.3512181. URL https://doi.org/10.1145/3485447.3512181.
* Zhang et al. [2022] Jiying Zhang, Fuyang Li, Xi Xiao, Tingyang Xu, Yu Rong, Junzhou Huang, and Yatao Bian. Hypergraph convolutional networks via equivalency between hypergraphs and undirected graphs, 2022. URL https://openreview.net/forum?id=zFyCvjXof60.
* Hendrycks and Gimpel [2020] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2020.
* Ba et al. [2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* Rumelhart et al. [1986] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. _nature_, 323(6088):533-536, 1986.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Xu et al. [2020] da Xu, chuanwei ruan, evren korpeoglu, sushant kumar, and kannan achan. Inductive representation learning on temporal graphs. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=rJeW1yHYwH.

* [85] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Self-attention with functional time representation learning. _Advances in neural information processing systems_, 32, 2019.
* [86] Seyed Mehran Kazemi, Rishab Goel, Sepehr Eghbali, Janahan Ramanan, Jaspreet Sahota, Sanjay Thakur, Stella Wu, Cathal Smyth, Pascal Poupart, and Marcus Brubaker. Time2vec: Learning a vector representation of time. _arXiv preprint arXiv:1907.05321_, 2019.
* [87] Naganand Yadati, Vikram Nitin, Madhav Nimishakavi, Prateek Yadav, Anand Louis, and Partha Talukdar. Nhp: Neural hypergraph link prediction. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, pages 1705-1714, 2020.
* [88] Tarun Kumar, K Darwin, Srinivasan Parthasarathy, and Balaraman Ravindran. Hpra: Hyperedge prediction using resource allocation. In _12th ACM conference on web science_, pages 135-143, 2020.
* [89] Ye Xu, Dan Rockmore, and Adam M Kleinbaum. Hyperlink prediction in hypernetworks using latent social features. In _Discovery Science: 16th International Conference, DS 2013, Singapore, October 6-9, 2013. Proceedings 16_, pages 324-339. Springer, 2013.
* [90] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. Evolvegn: Evolving graph convolutional networks for dynamic graphs. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 5363-5370, 2020.
* [91] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In _International conference on machine learning_, pages 6861-6871. PMLR, 2019.
* [92] Rossana Mastrandrea, Julie Fournet, and Alain Barrat. Contact patterns in a high school: A comparison between data collected using wearable sensors, contact diaries and friendship surveys. _PLOS ONE_, 10(9):e0136497, 2015. doi: 10.1371/journal.pone.0136497. URL https://doi.org/10.1371/journal.pone.0136497.
* [93] Juliette Stehle, Nicolas Voirin, Alain Barrat, Ciro Cattuto, Lorenzo Isella, Jean-Francois Pinton, Marco Quaggiotto, Wouter Van den Broeck, Corinne Regis, Bruno Lina, and Philippe Vanhems. High-resolution measurements of face-to-face contact patterns in a primary school. _PLoS ONE_, 6(8):e23176, 2011. doi: 10.1371/journal.pone.0023176. URL https://doi.org/10.1371/journal.pone.0023176.
* [94] James H. Fowler. Connecting the congress: A study of cosponsorship networks. _Political Analysis_, 14(04):456-487, 2006. doi: 10.1093/pan/mpl002. URL https://doi.org/10.1093/pan/mpl002.
* [95] James H. Fowler. Legislative cosponsorship networks in the US house and senate. _Social Networks_, 28(4):454-465, oct 2006. doi: 10.1016/j.socnet.2005.11.003. URL https://doi.org/10.1016/j.socnet.2005.11.003.
* [96] Hao Yin, Austin R. Benson, Jure Leskovec, and David F. Gleich. Local higher-order graph clustering. In _Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_. ACM Press, 2017. doi: 10.1145/3097983.3098069. URL https://doi.org/10.1145/3097983.3098069.
* [97] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. _Advances in neural information processing systems_, 30, 2017.
* [98] Fan RK Chung. The laplacian of a hypergraph. In "", 1993.
* [99] Timoteo Carletti, Duccio Fanelli, and Renaud Lambiotte. Random walks and community detection in hypergraphs. _Journal of Physics: Complexity_, 2(1):015011, 2021.
* [100] Linyuan Lu and Xing Peng. High-order random walks and generalized laplacians on hypergraphs. _Internet Mathematics_, 9(1):3-32, 2013.

* Zhang et al. [2017] Chenzi Zhang, Shuguang Hu, Zhihao Gavin Tang, and TH Hubert Chan. Re-revisiting learning on hypergraphs: confidence interval and subgradient method. In _International Conference on Machine Learning_, pages 4026-4034. PMLR, 2017.
* Chan et al. [2019] T-H Hubert Chan, Zhihao Gavin Tang, Xiaowei Wu, and Chenzi Zhang. Diffusion operator and spectral analysis for directed hypergraph laplacian. _Theoretical Computer Science_, 784:46-64, 2019.
* Li and Milenkovic [2017] Pan Li and Olgica Milenkovic. Inhomogeneous hypergraph clustering with applications. _Advances in neural information processing systems_, 30, 2017.
* Wang et al. [2022] Ziyu Wang, Wenhao Jiang, Yiming M Zhu, Li Yuan, Yibing Song, and Wei Liu. Dynamixer: a vision mlp architecture with dynamic mixing. In _International Conference on Machine Learning_, pages 22691-22701. PMLR, 2022.
* Behrouz et al. [2023] Ali Behrouz, Parsa Delavari, and Farnoosh Hashemi. Unsupervised representation learning of brain activity via bridging voxel activity and functional connectivity. In _NeurIPS 2023 AI for Science Workshop_, 2023. URL https://openreview.net/forum?id=HSVg7qFFd2.
* Snell et al. [2017] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. _Advances in neural information processing systems_, 30, 2017.
* Garnelo et al. [2018] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In _International conference on machine learning_, pages 1704-1713. PMLR, 2018.
* Lopez-Paz et al. [2017] David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Scholkopf, and Leon Bottou. Discovering causal signals in images. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6979-6987, 2017.
* Lee et al. [2019] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In _International conference on machine learning_, pages 3744-3753. PMLR, 2019.
* Baek et al. [2021] Jinheon Baek, Minki Kang, and Sung Ju Hwang. Accurate learning of graph representations with graph multiset pooling. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=JHcqXGaqiGn.
* Atkin [1974] Ronald Harry Atkin. _Mathematical structure in human affairs_. Heinemann Educational London, 1974.
* Spivak [2009] David I Spivak. Higher-dimensional models of networks. _arXiv preprint arXiv:0909.4314_, 2009.
* Billings et al. [2019] Jacob Charles Wright Billings, Mirko Hu, Giulia Lerda, Alexey N Medvedev, Francesco Mottes, Adrian Onicas, Andrea Santoro, and Giovanni Petri. Simplex2vec embeddings for community detection in simplicial complexes. _arXiv preprint arXiv:1906.09068_, 2019.
* Hajij et al. [2020] Mustafa Hajij, Kyle Istvan, and Ghada Zamzmi. Cell complex neural networks. In _TDA & Beyond_, 2020. URL https://openreview.net/forum?id=6Tq18ySFpGU.
* Hacker [2020] Celia Hacker. Sk$-simplex2vec: a simplicial extension of node2vec. In _TDA & Beyond_, 2020. URL https://openreview.net/forum?id=Aw9DUXPjq55.
* Ebli et al. [2020] Stefania Ebli, Michael Defferrard, and Gard Spreemann. Simplicial neural networks. In _TDA & Beyond_, 2020. URL https://openreview.net/forum?id=nPCt39DVIfk.
* Yang et al. [2022] Maosheng Yang, Elvin Isufi, and Geert Leus. Simplicial convolutional neural networks. In _ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 8847-8851. IEEE, 2022.
* Bunch et al. [2020] Eric Bunch, Qian You, Glenn Fung, and Vikas Singh. Simplicial 2-complex convolutional neural networks. In _TDA & Beyond_, 2020. URL https://openreview.net/forum?id=TLbnsKrt6J-.

* [119] Christopher Wei Jin Goh, Cristian Bodnar, and Pietro Lio. Simplicial attention networks. In _ICLR 2022 Workshop on Geometrical and Topological Representation Learning_, 2022. URL https://openreview.net/forum?id=ScfRNWkpec.
* [120] Mustafa Hajij, Karthikeyan Natesan Ramamurthy, Aldo Guzman-Saenz, and Ghada Za. High skip networks: A higher order generalization of skip connections. In _ICLR 2022 Workshop on Geometrical and Topological Representation Learning_, 2022.
* learning on hypergraphs revisited. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/2013/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf.
* [122] Sameer Agarwal, Kristin Branson, and Serge Belongie. Higher order learning with graphs. In _Proceedings of the 23rd International Conference on Machine Learning_, ICML '06, page 17-24, New York, NY, USA, 2006. Association for Computing Machinery. ISBN 1595933832. doi: 10.1145/1143844.1143847. URL https://doi.org/10.1145/1143844.1143847.
* [123] Edmund Ihler, Dorothea Wagner, and Frank Wagner. Modeling hypergraphs by graphs with the same mincut properties. _Inf. Process. Lett._, 45(4):171-175, mar 1993. ISSN 0020-0190. doi: 10.1016/0020-0190(93)90115-P. URL https://doi.org/10.1016/0020-0190(93)90115-P.
* [124] Peihao Wang, Shenghao Yang, Yunyu Liu, Zhangyang Wang, and Pan Li. Equivariant hypergraph diffusion neural operators. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=RiTjKoscnNd.
* [125] Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations for irregular time series. _Advances in Neural Information Processing Systems_, 33:6696-6707, 2020.
* [126] Giang Hoang Nguyen, John Boaz Lee, Ryan A. Rossi, Nesreen K. Ahmed, Bunyee Koh, and Sungchul Kim. Continuous-time dynamic network embeddings. In _Companion Proceedings of the The Web Conference 2018_, WWW '18, page 969-976, Republic and Canton of Geneva, CHE, 2018. International World Wide Web Conferences Steering Committee. ISBN 9781450356404. doi: 10.1145/3184558.3191526. URL https://doi.org/10.1145/3184558.3191526.
* [127] Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, Hanghang Tong, and Mehrdad Mahdavi. Do we really need complicated model architectures for temporal networks? In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=ayPPc6SyLv1.

## Appendix

### Table of Contents

* 1 Preliminaries, Backgrounds, and Motivations
	* 1.1 Anonymous Random Walks
	* 1.2 Random Walk on Hypergraphs
	* 1.3 MLP-Mixer
* 2 Additional Related Work
	* 2.1 Learning (Multi)Set Functions
	* 2.2 Simplicial Complexes Representation Learning
	* 2.3 How Does Car-Walk Differ from Existing Works? (Contributions)
* 3 SetWalk and Random Walk on Hypergraphs
	* 3.1 Extension of SetWalks
	* 3.2 Efficient Hyperedge Sampling
* 4 Theoretical Results
	* 4.1 Proof of Theorem 1
	* 4.2 Proof of Theorem 2
	* 4.3 Proof of Theorem 3
	* 4.4 Proof of Theorem 4
	* 4.5 Proof of Proposition 2
* 5 Experimental Setup Details
	* 5.1 Datasets
	* 5.2 Baselines
	* 5.3 Implementation and Training Details
* 6 Additional Experimental Results
	* 6.1 Results on More Datasets
	* 6.2 Node Classification
	* 6.3 Performance in Average Precision
	* 6.4 More Results on Rnn v.s. MLP-Mixer in Walk Encoding
* 7 Broader Impacts
Preliminaries, Backgrounds, and Motivations

We begin by reviewing the preliminaries and background concepts that we refer to in the main paper. Next, we discuss the fundamental differences between our method and techniques from prior work.

### Anonymous Random Walks

Micali and Zhu [45] studied Anonymous Walks (AWs), which replace a _node's identity_ by the order of its appearance in each walk. Given a simple network, an AW starts from a node, performs random walks over the graph to collect a sequence of nodes, \(W:(u_{1},u_{2},\ldots,u_{k})\), and then replaces the node identities by their order of appearance in each walk. That is:

\[\text{ID}_{\text{AW}}(w_{0},W)=|\{u_{1},u_{2},\ldots,u_{k^{*}}\}|\;\;\text{ where $k^{*}$ is the smallest index such that $u_{k^{*}}=w_{0}$}.\] (10)

While this method is a simple anonymization process, it misses the correlation between different walks and assigns new node identities based on only one single walk. The correlation between different walks is more important in temporal networks to assign new node identities, as a single walk cannot capture the frequency of a pattern over time [33]. To this end, Wang et al. [33] design a set-based anonymization process that assigns new node identities based on a set of sampled walks. Given a vertex \(u\), they sample \(M\) walks with length \(m\) starting from \(u\) and store them in \(S_{u}\). Next, for each node \(w_{0}\) that appears on at least one walk in \(S_{u}\), they assign a vector to each node as its hidden identity [33]:

\[g(w_{0},S_{u})[i]=|\{W|W\in S_{u},W[i]=w_{0}\}|\;\;\;\forall\;i\in\{0,\ldots,m\},\] (11)

where \(W[i]\) shows the \(i\)-th node in the walk \(W\). This anonymization process not only hides the identity of vertices but it also can establish such hidden identity based on different sampled walks, capturing the correlation between several walks starting from a vertex.

Both of these anonymization processes are designed for graphs with pair-wise interactions, and there are three main challenges in adopting them for hypergraphs: 1 To capture higher-order patterns, we use SetWalks, which are a sequence of hyperedges. Accordingly, we need an encoding for the position of hyperedges. A natural attempt to encode the position of hyperedges is to count the position of hyperedges across sampled SetWalks, as CAW [33] does for nodes. However, this approach misses the similarity of hyperedges with many nodes in common. That is, given two hyperedges \(e_{1}=\{u_{1},u_{2},\ldots,u_{k}\}\) and \(e_{2}=\{u_{1},u_{2},\ldots,u_{k},u_{k+1}\}\). Although we want to encode the position of these two hyperedges, we also want these two hyperedges to have almost the same encoding as they share many vertices. Accordingly, we suggest viewing a hyperedge as a set of vertices. We first encode the position of vertices, and then we aggregate the position encodings of nodes that are connected by a hyperedge to compute the positional encoding of the hyperedge. 2 However, since we focus on undirected hypergraphs, the order of a hyperedge's vertices in the aggregation process should not affect the hyperedge positional encodings. Therefore, we need a permutation invariant pooling strategy. 3 While several existing studies used simple pooling functions such as Mean. or Sum.() [26], these pooling functions do not capture the higher-order dependencies between obtained nodes' position encodings, missing the advantage of higher-order interactions. That is, a pooling function such as Mean.() is a non-parametric method that sees the positional encoding of each node in a hyperedge separately. Therefore, it is unable to aggregate them in a non-linear manner, which, depending on the data, can miss information. To address challenges 2 and 3, we design SetMixer, a permutation invariant pooling strategy that uses MLPs to learn how to aggregate positional encodings of vertices in a hyperedge to compute the hyperedge positional encoding.

### Random Walk on Hypergraphs

Chung [98] presents some of the earliest research on the hypergraph Laplacian, defining the Laplacian of the \(k\)-uniform hypergraph. Following this line of work, Zhou et al. [18] defined a two-step CE-based random walk-based Laplacian for general hypergraphs. Given a node \(u\), in the first step, we uniformly sample a hyperedge \(e\) including node \(u\), and in the second step, we uniformly sample a node in \(e\). Following this idea, several studies developed more sophisticated (weighted) CE-based random walks on hypergraphs [20]. However, Chitra and Raphael [40] shows that random walks on hypergraphs with edge-independent node weights are limited to capturing pair-wise interactions, making them unable to capture higher-order information. To address this limitation, they designed an edge-dependent sampling procedure of random walks on hypergraphs. Carletti et al. [37] and Carlettiet al. [99] argued that to sample more informative walks from a hypergraph, we must consider the degree of hyperedges in measuring the importance of vertices in the first step. Concurrently, some studies discuss the dependencies among hyperedges and define the \(s\)-th Laplacian based on simple walks on the dual hypergraphs [100, 41]. Finally, more sophisticated random walks with non-linear Laplacians have been designed [101, 102, 103, 23].

SetWalks addresses three main drawbacks from existing methods: 1 None of these methods are designed for temporal hypergraphs so they cannot capture temporal properties of the network. Also, natural attempts to extend them to temporal hypergraphs and let the walker uniformly walk over time ignore the fact that recent hyperedges are more informative than older ones (see Table 2). To address this issue, SetWalk uses a temporal bias factor in its sampling procedure (Equation 1). 2 Existing hypergraph random walks are unable to capture either higher-order interactions of vertices or higher-order dependencies of hyperedges. That is, random walks with edge-independent weights [37] are not able to capture higher-order interactions and are equivalent to simple random walks on the CE of the hypergraph [40]. The expressivity of random walks on hypergraphs with edge-dependent walks is also limited when we have a limited number of sampled walks (see Theorem 1). Finally, defining a hypergraph random walk as a random walk on the dual hypergraph also cannot capture the higher-order dependencies of hyperedges (see Appendix C and Appendix D). SetWalk by its nature is able to walk over hyperedges (instead of vertices) and time and can capture higher-order interactions. Also, with a structural bias factor in its sampling procedure, which is based on hyperedge-dependent node weights, it is more informative than a simple random walk on the dual hypergraph, capturing higher-order dependencies of hyperedges. See Appendix C for further discussion.

### MLP-Mixer

MLP-Mixer[44] is a family of models, based on multi-layer perceptions (MLPs), that are simple, amenable to efficient implementation, and robust to long-term dependencies (unlike Rnns, attention mechanisms, and Transformers [83]) with a wide array of applications from computer vision [104] to neuroscience [105]. The original architecture is designed for image data, where it takes image tokens as inputs. It then encodes them with a linear layer, which is equivalent to a convolutional layer over the image tokens, and updates their representations with a sequence of feed-forward layers applied to image tokens and features. Accordingly, we can divide the architecture of MLP-Mixer into two main parts: 1 Token Mixer: The main intuition of the token mixer is to clearly separate the cross-location operations and learn the cross-feature (cross-location) dependencies. 2 Channel Mixer: The intuition behind the channel mixer is to clearly separate the per-location operations and provide positional invariance, a prominent feature of convolutions. In both Mixer and SetMixer we use the channel mixer as designed in MLP-Mixer. Next, we discuss the token mixer and its limitation in mixing features in a permutation variant manner:

**Token Mixer.** Let \(\mathbf{E}\) be the input of the MLP-Mixer, then the token mixer phase is defined as:

\[\mathbf{H}_{\text{token}}=\mathbf{E}+\mathbf{W}_{\text{token}}^{(2)}\sigma \left(\mathbf{W}_{\text{token}}^{(1)}\texttt{LayerNorm}\left(\mathbf{E} \right)^{T}\right)^{T},\] (12)

where \(\sigma(.)\) is nonlinear activation function (usually GeLU [80]). Since it feeds the input's columns to an MLP, it mixes the cross-feature information, which results in the MLP-Mixer being sensitive to permutation. Although natural attempts to remove the token mixer or its linear layer can produce a permutation invariant method, it misses cross-feature dependencies, which are the main motivation for using the MLP-Mixer architecture. To address this issue, SetMixer uses the Softmax(.) function over features. Using Softmax over features can be seen as cross-feature normalization, which can capture their dependencies. While Softmax(.) is a non-parametric method that can bind token-wise information, it is also permutation equivariant, and as we prove in Appendix E.3, makes the SetMixer permutation invariant.

## Appendix B Additional Related Work

### Learning (Multi)Set Functions

(Multi)set functions are pooling architectures for (multi)sets with a wide array of applications in many real-world problems including few-shot image classification [106], conditional regression [107], and causality discovery [108]. Zaheer et al. [97] develop DeepSets, a universal approach to parameterizethe (multi)set functions. Following this direction, some works design attention mechanisms to learn multiset functions [109], which also inspired Baek et al. [110] to adopt attention mechanisms designed for (multi)set functions in graph representation learning. Finally, Chien et al. [25] build the connection between learning (multi)set functions with propagations on hypergraphs. To the best of our knowledge, SetMixer is the first adaptive permutation invariant pooling strategy for hypergraphs, which views each hyperedge as a set of vertices and aggregates node encodings by considering their higher-order dependencies.

### Simplicial Complexes Representation Learning

Simplicial complexes can be considered a special case of hypergraphs and are defined as a collection of polytopes such as triangles and tetrahedra, which are called simplices [111]. While these frameworks can be used to represent higher-order relations, simplicial complexes require the downward closure property [112]. That is, every substructure or face of a simplex contained in a complex \(\mathcal{K}\) is also in \(\mathcal{K}\). Recently, to encode higher-order interactions, representation learning on simplicial complexes has attracted much attention [113, 114, 115, 116, 117, 6, 118, 119]. The first group of methods extend node2vec [67] to simplicial complexes with random walks on interactions through Hasse diagrams and simplex connections inside \(p\)-chains [115, 113]. With the recent advances in message-passing-based methods, several studies focus on designing neural networks on simplicial complexes [116, 117, 118, 119]. Ebli et al. [116] introduced Simplicial neural networks (SNN), a generalization of spectral graph convolutio,n to simplicial complexes with higher-order Laplacian matrices. Following this direction, some works propose simplicial convolutional neural networks with different simplicial filters to exploit the relationships in upper- and lower-neighborhoods [117, 118]. Finally, the last group of studies use an encoder-decoder architecture as well as message-passing to learn the representation of simplicial complexes [114, 120].

CAr-Walk is different from all these methods in three main aspects: 1 Contrary to these methods, CAr-Walk is designed for temporal hypergraphs and is capable of capturing higher-order temporal properties in a streaming manner, avoiding the drawbacks of snapshot-based methods. 2 CAr-Walk works in the inductive setting by extracting underlying dynamic laws of the hypergraph, making it generalizable to unseen patterns and nodes. 3 All these methods are designed for simplicial complexes, which are special cases of hypergraphs, while CAr-Walk is designed for general hypergraphs and does not require any assumption of the downward closure property.

### How Does CAr-Walk Differ from Existing Works? (Contributions)

As we discussed in Appendix A.2, existing random walks on hypergraphs are unable to capture either 1 higher-order interactions between nodes or 2 higher-order dependencies of hyperedges. Moreover, all these walks are for static hypergraphs and are not able to capture temporal properties. To this end, we design SetWalk a higher-order temporal walk on hypergraphs. Naturally, SetWalks are capable of capturing higher-order patterns as a SetWalk is defined as a sequence of hyperedges. We further design a new sampling procedure with temporal and structural biases, making SetWalks capable of capturing higher-order dependencies of hyperedges. To take advantage of complex information provided by SetWalks as well as training the model in an inductive manner, we design a two-step anonymization process with a novel pooling strategy, called SetMixer. The anonymization process starts with encoding the position of vertices with respect to a set of sampled SetWalks and then aggregates node positional encodings via a non-linear permutation invariant pooling function, SetMixer, to compute their corresponding hyperedge positional encodings. This two-step process lets us capture structural properties while we also care about the similarity of hyperedges. Finally, to take advantage of continuous-time dynamics in data and avoid the limitations of sequential encoding, we design a neural network for temporal walk encoding that leverages a time encoding module to encode time as well as a Mixer module to encode the structure of the walk.

## Appendix C SetWalk and Random Walk on Hypergraphs

We reviewed existing random walks in Appendix A.2. Here, we discuss how these concepts are different from SetWalks and investigate whether SetWalks are more expressive than these methods.

As we discussed in Sections 1 and 3.2, there are two main challenges for designing random walks on hypergraphs: 1 Random walks are a sequence of _pair-wise_ interconnected vertices, even though edges in a hypergraph connect _sets_ of vertices. 2 A sampling probability of a walk on a hypergraph must be different from its sampling probability on the CE of the hypergraph [37; 38; 39; 40; 41; 42; 43]. To address these challenges, most existing works on random walks on hypergraphs ignore 1 and focus on 2 to distinguish the walks on simple graphs and hypergraphs, and 1 is relatively unexplored. To this end, we answer the following questions:

**Q1: Can 2 alone be sufficient to take advantage of higher-order interactions?** First, semantically, decomposing hyperedges into sequences of simple pair-wise interactions (CE) loses the semantic meaning of the hyperedges. Consider the collaboration network in Figure 1. When decomposing the hyperedges into pair-wise interactions, both \((A,B,C)\) and \((H,G,E)\) have the same structure (a triangle), while the semantics of these two structures in the data are completely different. That is, \((A,B,C)\) have _all_ published a paper together, while each pair of \((H,G,E)\) separately have published a paper. One might argue that although the output of hypergraph random walks and simple random walks on the CE might be the same, the sampling probability of each walk is different and with a large number of samples, our model can distinguish these two structures. In Theorem 1 (proof in Appendix E) we theoretically show that when we have a finite number of hypergraph walk samples, \(M\), there is a hypergraph \(\mathcal{G}\) such that with \(M\) hypergraph walks, the \(\mathcal{G}\) and its CE are not distinguishable. Note that in reality, the bottleneck for the number of sampled walks in machine learning-based methods is memory. Accordingly, even with tuning the number of samples for each dataset, the size of samples is bounded by a small number. This theorem shows that with a limited budget for walk sampling, 2 alone is not enough to capture higher-order patterns.

**Q2: Can addressing 1 alone be sufficient to take advantage of higher-order interactions?** To answer this question, we use the extended version of the edge-to-vertex dual graph concept for hypergraphs:

**Definition 4** (Dual Hypergraph).: _Given a hypergraph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\), the dual hypergraph of \(\mathcal{G}\) is defined as \(\tilde{\mathcal{G}}=(\tilde{\mathcal{V}},\tilde{\mathcal{E}})\), where \(\tilde{\mathcal{V}}=\mathcal{E}\) and a hyperedge \(\tilde{e}=\{e_{1},e_{2},\ldots,e_{k}\}\in\tilde{\mathcal{E}}\) shows that \(\bigcap_{i=1}^{k}e_{i}\neq\emptyset\)._

To address 1, we need to see walks on hypergraphs as a sequence of hyperedges (instead of a sequence of pair-wise connected nodes). One can interpret this as a hypergraph walk on the dual hypergraph. That is, each hypergraph walk on the dual graph is a sequence of \(\mathcal{G}\)'s hyperedges: \(e_{1}\to e_{2}\rightarrow\cdots\to e_{k}\). However, as shown by Chitra and Raphael [40], each walk on hypergraphs with edge-independent weights for sampling vertices is equivalent to a simple walk on the (weighted) CE graph. To this end, addressing 2 alone can be equivalent to sample walks on the CE of the dual hypergraph, which misses the higher-order interdependencies of hyperedges and their intersections.

Based on the above discussion, both 1 and 2 are required to capture higher-order interaction between nodes as well as higher-order interdependencies of hyperedges. The definition of SetWalks (Definition 3) with _structural bias_, introduced in Equation 1, satisfies both 1 and 2. In the next section, we discuss how a simple extension of SetWalks can not only be more expressive than all existing walks on hypergraphs and their CEs, but its definition is universal, and all these methods are special cases of extended SetWalk.

### Extension of SetWalks

Random walks on hypergraphs are simple but less expressive methods for extracting network motifs, while SetWalks are more complex patterns that provide more expressive motif extraction approaches. One can model the trade-off of simplicity and expressivity to connect all these concepts in a single notion of walks. To establish a connection between SetWalks and existing walks on hypergraphs, as well as a universal random walk model on hypergraphs, we extend SetWalks to \(r\)-SetWalks, where parameter \(r\) controls the size of hyperedges that appear in the walk:

**Definition 5** (\(r\)-SetWalk).: _Given a temporal hypergraph \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{X})\), and a threshold \(r\in\mathbb{Z}^{+}\), a \(r\)-SetWalk with length \(\ell\) on temporal hypergraph \(\mathcal{G}\) is a randomly generated sequence of hyperedges (sets):_

\[\mathrm{Sw}:\ (e_{1},t_{e_{1}})\rightarrow(e_{2},t_{e_{2}})\rightarrow\cdots \rightarrow(e_{\ell},t_{e_{\ell}}),\]

_where \(e_{i}\in\mathcal{E}\), \(|e_{i}|\leq r\), \(t_{e_{i+1}}<t_{e_{i}}\), and the intersection of \(e_{i}\) and \(e_{i+1}\) is not empty, \(e_{i}\cap e_{i+1}\neq\emptyset\). In other words, for each \(1\leq i\leq\ell-1\): \(e_{i+1}\in\mathcal{E}^{i}(e_{i})\). We use \(\mathrm{Sw}[i]\) to denote the \(i\)-th hyperedge-time pair in the SetWalk. That is, \(\mathrm{Sw}[i][0]=e_{i}\) and \(\mathrm{Sw}[i][1]=t_{e_{i}}\)._The only difference between this definition and Definition 3 is that \(r\)-SetWalk limits hyperedges in the walk to hyperedges with size at most \(r\). The sampling process of \(r\)-SetWalk is the same as that of SetWalk (introduced in Section 3.2 and Appendix D), while we only sample hyperedges with size at most \(r\). Now to establish the connection of \(r\)-SetWalks and existing walks on hypergraphs, we define the extended version of the clique expansion technique:

**Definition 6** (\(r\)-Projected Hypergraph).: _Given a hypergraph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) and an integer \(r\geq 2\), we construct the (weighted) \(r\)-projected hypergraph of \(\mathcal{G}\) as a hypergraph \(\hat{\mathcal{G}}_{r}=(\mathcal{V},\hat{\mathcal{E}}_{r})\), where for each \(e=\{u_{1},u_{2},\ldots,u_{k}\}\in\mathcal{E}\):_

1. _if_ \(k\leq r\)_: add_ \(e\) _to the_ \(\hat{\mathcal{E}}_{r}\)_,_
2. _if_ \(k\geq r+1\)_: add_ \(e_{i}=\{u_{i_{1}},u_{i_{2}},\ldots,u_{i_{j}}\}\) _to_ \(\hat{\mathcal{E}}_{r}\)_, for every possible_ \(\{i_{1},i_{2},\ldots,i_{r}\}\subseteq\{1,2,\ldots,k\}\)_._

Each of steps 1 or 2 can be done in a weighted manner. In other words, we approximate each hyperedge with a size of more than \(r\) with \(\binom{k}{r}\) (weighted) hyperedges with size \(r\). For example, when \(r=2\), the 2-projected graph of \(\mathcal{G}\) is equivalent to its clique expansion, and \(r=\infty\) is the hypergraph itself. Furthermore, we define the Union Projected Hypergraph (UP hypergraph) as the union of all \(r\)-projected hypergraphs, i.e., \(\mathcal{G}^{*}=(\mathcal{V},\bigcup_{r=2}^{\infty}\hat{\mathcal{E}}_{r})\). Note that the UP hypergraph has the downward closure property and is equivalent to the simplicial complex representation of the hypergraph \(\mathcal{G}\). The next proposition establishes the universality of the \(r\)-SetWalk concept.

**Proposition 2**.: _Edge-independent random walks on hypergraphs [37], edge-dependent random walks on hypergraphs [40], and simple random walks on the CE of hypergraphs are all special cases of \(r\)-SetWalk, when applied to the \(2\)-projected graph, UP hypergraph, and \(2\)-projected graph, respectively. Furthermore, all the above methods are less expressive than \(r\)-SetWalks._

The proof of this proposition is in Appendix E.5.

## Appendix D Efficient Hyperedge Sampling

For sampling SetWalks, inspired by Wang et al. [33], we use two steps: 1 Online score computation: we assign a set of scores to each incoming hyperedge. 2 Iterative sampling: we use assigned scores in the previous step to sample hyperedges in a SetWalk.

**Input:** Given a hypergraph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) and \(\alpha\in[0,1]\)

**Output:** A probability score for each vertex

```
1:\(P\leftarrow\emptyset\);
2:for\((e,t)\in\mathcal{E}\) with an increasing order of \(t\)do
3:\(P_{e,t}[0]\leftarrow\exp\left(\alpha t\right)\);
4:\(P_{e,t}[1]\gets 0\), \(P_{e,t}[2]\gets 0\), \(P_{e,t}[3]\leftarrow\emptyset\);
5:for\(u\in e\)do
6:for\(e_{n}\in\mathcal{E}(u)\)do
7:if\(e_{n}\) is not visited then
8:\(P_{e,t}[2]\gets P_{e,t}[2]+\exp(\varphi(e_{n},e))\);
9:\(P_{e,t}[3]\gets P_{e,t}[3]\cup\{\exp(\varphi(e_{n},e))\}\);
10:\(P_{e,t}[1]\gets P_{e,t}[1]+\exp(\alpha\times t_{n})\); return\(P\); ```

**Algorithm 1** Online Score Computation

**Online Score Computation.** The first part essentially works in an online manner and assigns each new incoming hyperedge \(e\) a four-tuple of scores:

\[P_{e,t}[0] =\exp(\alpha\times t), P_{e,t}[1] =\sum_{(e^{\prime},t^{\prime})\in\mathcal{E}(e)}\exp\left(\alpha \times t^{\prime}\right)\] \[P_{e,t}[2] =\sum_{(e^{\prime},t^{\prime})\in\mathcal{E}(e)}\exp\left(\varphi (e,e^{\prime})\right), P_{e,t}[3] =\{\exp(\varphi(e,e^{\prime}))\}_{(e^{\prime},t^{\prime})\in \mathcal{E}(e)}\]

**Iterative Sampling.** In the iterative sampling algorithm, we use pre-computed scores by Algorithm 1 and sample a hyperedge \((e,t)\) given a previously sampled hyperedge \((e_{p},t_{p})\). In the next proposition,we show that this sampling algorithm samples each hyperedge with the probability mentioned in Section 3.2.

**Proposition 3**.: _Algorithm 2 sample a hyperedge \((e,t)\) after \((e_{p},t_{p})\) with a probability proportional to \(\mathbb{P}[(e,t)|(e_{p},t_{p})]\) (Equation 1)._

**How can this sampling procedure capture higher-order patterns?** As discussed in Appendix C, SetWalks on \(\mathcal{G}\) can be interpreted as a random walk on the dual hypergraph of \(\mathcal{G}\), \(\tilde{\mathcal{G}}\). However, a simple (or hyperedge-independent) random walk on the dual hypergraph is equivalent to the walk on the CE of the dual hypergraph [40, 41], missing the higher-order dependencies of hyperedges. Inspired by Chitra and Raphael [40], we use hyperedge-dependent weights \(\Gamma:\mathcal{V}\times\mathcal{E}\rightarrow\mathbb{R}^{\geq 0}\) and sample hyperedges with a probability proportional to \(\exp\left(\sum_{u\in\tilde{e}\cap e_{p}}\Gamma(u,e)\Gamma(u,e^{\prime})\right)\), where \(e_{p}\) is the previously sampled hyperedge. In the dual hypergraph \(\tilde{\mathcal{G}}=(\mathcal{E},\mathcal{V})\), we assign a score \(\tilde{\Gamma}:\mathcal{E}\times\mathcal{V}\rightarrow\mathbb{R}^{\geq 0}\) to each pair of \((e,u)\) as \(\tilde{\Gamma}(e,u)=\Gamma(u,e)\). Now, a SetWalk with this sampling procedure is equivalent to the edge-dependent hypergraph walk on the dual hypergraph of \(\mathcal{G}\) with edge-dependent weight \(\tilde{\Gamma}(.)\). Chitra and Raphael [40] show that an edge-dependent hypergraph random walk can capture some information about higher-order interactions and is not equivalent to a simple walk on the weighted CE of the hypergraph. Accordingly, even on the dual hypergraph, SetWalk with this sampling procedure can capture higher-order dependencies of hyperedges and is not equivalent to a simple walk on the CE of the dual hypergraph \(\tilde{\mathcal{G}}\). We conclude that, unlike existing random walks on hypergraphs [37, 38, 41, 79], SetWalk can capture both higher-order interactions of nodes, and, based on its sampling procedure, higher-order dependencies of hyperedges.

## Appendix E Theoretical Results

### Proof of Theorem 1

**Theorem 1**.: _A random SetWalk is equivalent to neither the hypergraph random walk, the random walk on the CE graph, nor the random walk on the SE graph. Also, for a finite number of samples of each, SetWalk is more expressive than existing walks._

Proof.: In this proof, we focus on the hypergraph random walk and simple random walk on the CE. The proof for the SE graph is the same and also it has been proven that the SE graph and the CE of a hypergraph have close (or equal in uniform hypergraphs) Laplacian and have the same expressiveness power in the representation of hypergraphs [12, 13, 14, 15].

First, note that each SetWalk can be approximately decomposed to a set of either hypergraph walks, simple random walks, or walk on the SE. Moreover, each of these walks can be mapped to

Figure 6: The example of a hypergraph \(\mathcal{G}\) and its 2- and 3-projected hypergraphs.

a corresponding SetWalk (but not a bijective mapping), by sampling hyperpedges corresponding to each consecutive pair of nodes in these walks. Accordingly, SetWalks includes the information provided by these walks and so its expressiveness is not less than these methods. To this end, next, we discuss two examples in two different tasks for which SetWalks are successful while other walks fail.

1 In the first task, we want to see if there exists a pair of hypergraphs with different semantics that SetWalks can distinguish, but other walks cannot. We construct such hypergraphs. Let \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) be a hypergraph with \(\mathcal{V}=\{u_{1},u_{2},\ldots,u_{N}\}\) and \(\mathcal{E}=\{(e,t_{i})\}_{i=1}^{T}\), where \(e=\{u_{1},u_{2},\ldots,u_{N}\}\) and \(t_{1}<t_{2}<\cdots<t_{T}\). Also, let \(\mathcal{A}\) be an edge-independent hypergraph random walk (or random walk on the CE) sampling algorithm. Chitra and Raphael [40] show that each of these walks is equivalent to a random walk on the weighted CE. Assume that \(\xi_{(\cdot)}\) is a function that assigns weights to edges in \(\mathcal{G}^{*}=(\mathcal{V},\mathcal{E}^{*})\), the weighted CE of the \(\mathcal{G}\), such that a hypergraph random walk on \(\mathcal{G}\) is equivalent to a walk on this weighted CE graph. Next, we construct a weighted hypergraph \(\mathcal{G}^{\prime}=(\mathcal{V},\mathcal{E}^{\prime})\) with the same set of vertices but with \(\mathcal{E}^{\prime}=\bigcup_{k=1}^{T}\{((u_{i},u_{j}),t_{k})\}_{u_{i}\in \mathcal{V}}\), such that each edge \(e_{i,j}=(u_{i},u_{j})\) is associated with a weight \(\xi(e_{i,j})\). Clearly, sampling procedure \(\mathcal{A}\) on \(\mathcal{G}\) and \(\mathcal{G}^{\prime}\) are the same, while they have different semantics. For example, assume that both are collaboration networks. In \(\mathcal{G}\), all vertices have published a single paper together, while in \(\mathcal{G}^{\prime}\), each pair of vertices have published a separate paper together. The proof for the hypergraph random walk with hyperedge-dependent weights is the same, while we construct weights of the hypergraph \(\mathcal{G}^{\prime}\) based on the sampling probability of hyperedges in the hypergraph random walk procedure.

2 Next, in the second task, we investigate the expressiveness of these walks for reconstructing hyperedges. That is, we want to see that given a perfect classifier, can these walks provide enough information to detect higher-order patterns in the network. To this end, we show that for a finite number of samples of each walk, SetWalk is more expressive than all of these walks in detecting higher-order patterns. Let \(M\) be the maximum number of samples and \(L\) be the maximum length of walks, we show that for any \(M\geq 2\) and \(L\geq 2\) there exists a pair of hypergraphs \(\mathcal{G}\), with higher-order interactions, and \(\mathcal{G}^{\prime}\), with pairwise interactions, such that SetWalks can distinguish them, while they are indistinguishable by any of these walks. We construct a temporal hypergraph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) as a hypergraph with \(\mathcal{V}=\{u_{1},u_{2},\ldots,u_{M(L+1)+1}\}\) and \(\mathcal{E}=\{(e,t_{i})\}_{i=1}^{L}\), where \(e=\{u_{1},u_{2},\ldots,u_{M(L+1)+1}\}\) and \(t_{1}<t_{2}<\cdots<t_{L}\). We further construct \(\mathcal{G}^{\prime}=(\mathcal{V},\mathcal{E}^{\prime})\) with the same set of vertices but with \(\mathcal{E}^{\prime}=\bigcup_{k=1}^{L}\{((u_{i},u_{j}),t_{k})\}_{u_{i}\in \mathcal{V}}\). Figure 6 illustrates \(\mathcal{G}\) and its projected graphs at a given timestamp \(t\in\{t_{1},t_{2},\ldots,t_{L}\}\).

SetWalk with only one sample, Sw, can distinguish interactions in these two hypergraphs. That is, let \(\text{Sw}\,:\,(e,t_{L})\rightarrow(e,t_{L-1})\rightarrow\cdots\rightarrow(e,t _{1})\) be the sample SetWalk from \(\mathcal{G}\) (note that masking the time, this is the only SetWalk on \(\mathcal{G}\), so in any case the sampled SetWalk is Sw). Since all interactions in \(\mathcal{G}^{\prime}\) are pairwise, _any_ sampled SetWalk on \(\mathcal{G}^{\prime}\), \(\text{Sw}^{\prime}\), includes only pairwise interactions, so \(\text{Sw}\neq\text{Sw}^{\prime}\), in any case. Accordingly, SetWalk can distinguish interactions in these two hypergraphs.

Since the output of hypergraph random walks, simple walks on the CE, and walks on the SE include only pairwise interactions, it seems that they are unable to detect higher-order patterns, so are unable to distinguish these two hypergraphs. However, one might argue that by having a large number of sampled walks and using a perfect classifier, which learned the distribution of sampled random walks and can detect whether a set of sampled walks is from a higher-order interaction, we might be able to detect higher-order interactions. To this end, we next assume that we have a perfect classifier \(\mathcal{C}(.)\) that can detect whether a set of sampled hypergraph walks, simple walks on the CE, or walks on the SE are sampled from a higher-order structure or pair-wise patterns. Next, we show that hypergraph random walks cannot provide enough information about every vertex for \(\mathcal{C}(.)\) to detect whether all vertices in \(\mathcal{V}\) shape a hyperedge. To this end, assume that we sample \(S=\{W_{1},W_{2},\ldots,W_{M}\}\) walks from hypergraph \(\mathcal{G}\) and \(S^{\prime}=\{W_{1}^{\prime},W_{2}^{\prime},\ldots,W_{M}^{\prime}\}\) walks from hypergraph \(\mathcal{G}^{\prime}\). In the best case scenario, since \(\mathcal{C}(.)\) is a perfect classifier, it can detect that \(\mathcal{G}^{\prime}\) includes only pair-wise interactions based on sampled walk \(S^{\prime}\). To distinguish these two hypergraphs, we need \(\mathcal{C}(.)\) to detect sampled walks from \(\mathcal{G}\) (i.e., \(S\)) that come from a higher-order pattern. For any \(M\) sampled walks with length \(L\) from \(\mathcal{G}\), we observe at most \(M\times(L+1)\) vertices, so we have information about at most \(M\times(L+1)\) vertices, unable to capture any information about the neighborhood of at least one vertex. Due to the symmetry of vertices, without loss of generality, we can assume that this vertex is \(u_{1}\). This means that with these \(M\) sampled hypergraph random walks with length \(L\), we are not able to provide any information about node \(u_{1}\) at any timestamp for \(\mathcal{C}(.)\). Therefore, even a perfect classifier \(\mathcal{C}(.)\) cannot verify whether is a part of higher-order interaction or pair-wise interaction, which completes the proof. Note that the proof for the simple random walk is completely the same.

**Remark 1**.: _Note that while the first task investigates the expressiveness of these methods with respect to their sampling procedure, the second tasks discuss the limitation and difference in their outputs._

**Remark 2**.: _Note that in reality, we can have neither an unlimited number of samples nor an unlimited walk length. Also, the upper bound for the number of samples or walk length depends on the RAM of the machine on which the model is being trained. In our experiments, we observe that usually, we cannot sample more than 125 walks with a batch size of 32._

### Proof of Theorem 2

Before discussing the proof of Theorem 2 we first formally define what missing information means in this context.

**Definition 7** (Missing Information).: _We say a pooling strategy like \(\Psi(.)\) misses information if there is a model \(\mathcal{M}\) such that using \(\Psi(.)\) on top of the \(\mathcal{M}\) (call it \(\hat{\mathcal{M}}\)) decreases the expressive power of \(\mathcal{M}\) That is, \(\hat{\mathcal{M}}\) has less expressive power than \(\mathcal{M}\)_

**Theorem 2**.: _Given an arbitrary positive integer \(k\in\mathbb{Z}^{+}\), let \(\Psi(.)\) be a pooling function such that for any set \(S=\{w_{1},\ldots,w_{d}\}\):_

\[\Psi(S)=\sum_{\begin{subarray}{c}S^{\prime}\subset S\\ |w|=k\end{subarray}}f(S^{\prime}),\] (13)

_where \(f\) is some function. Then the pooling function can cause missing information, limiting the expressiveness of the method to applying to the projected (hyper)graph of the hypergraph._

Proof.: The main intuition of this theorem is that a pooling function needs to capture higher-order dependencies of its input's elements and if it can be decomposed to a summation of functions that capture lower-order dependencies, it misses information. We show that, in the general case for a given \(k\in\mathbb{Z}^{+}\), the pooling function \(\Psi(.)\) when applied to a hypergraph \(\mathcal{G}\) is at most as expressive as \(\Psi(.)\) when applied to the \(k\)-projected hypergraph of \(\mathcal{G}\) (Definition 6). Let \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) be a hypergraph with \(\mathcal{V}=\{u_{1},u_{2},\ldots,u_{k+1}\}\) and \(\mathcal{E}=\{(\mathcal{V},t)\}=\{(\{u_{1},u_{2},\ldots,u_{k+1}\},t)\}\) for a given time \(t\), and \(\hat{\mathcal{G}}=(\mathcal{V},\hat{\mathcal{E}})\) be its \(k\)-projected graph, i.e., \(\hat{\mathcal{E}}=\{(e_{1},t),\ldots,(e_{(e_{1}^{-1})},t)\}\), where \(e_{i}\subset\{u_{1},u_{2},\ldots,u_{k+1}\}\) such that \(|e_{i}|=k\). Applying pooling function \(\Psi(.)\) on the hypergraph \(\mathcal{G}\) is equivalent to applying \(\Psi(.)\) to the hyperedge \((\mathcal{V},t)\in\mathcal{E}\), which provides \(\Psi(\mathcal{V})=\sum_{i=1}^{k+1}f(e_{i})\). On the other hand, applying \(\Psi(.)\) on projected graph \(\hat{\mathcal{G}}\) means applying it on each hyperedge \(e_{i}\in\hat{\mathcal{E}}\). Accordingly, since for each hyperedge \(e_{i}\in\hat{\mathcal{E}}\) we have \(\Psi(e_{i})=f(e_{i})\), all captured information by pooling function \(\Psi(.)\) on \(\hat{\mathcal{G}}\) is the set of \(S=\{f(e_{i})\}_{i=1}^{k+1}\). It is clear that \(\Psi(\mathcal{V})=\sum_{i=1}^{k+1}f(e_{i})\) is less informative than \(S=\{f(e_{i})\}_{i=1}^{k+1}\) as it is the summation of elements in \(S\) (in fact, \(\Psi(\mathcal{V})\) cannot capture the non-linear combinations of positional encodings of vertices, while \(S\) can). Accordingly, the provided information by applying \(\Psi(.)\) on \(\mathcal{G}\) cannot be more informative than applying \(\Psi(.)\) on the \(\mathcal{G}\)'s \(k\)-projected hypergraph. 

**Remark 3**.: _Note that the pooling function \(\Psi(.)\) is defined on a (hyper)graph and gets only (hyper)edges as input._

**Remark 4**.: _Although \(\Psi(.)=\textsc{Mean}(.)\) cannot be written as Equation 13, we can simply see that the above proof works for this pooling function as well._

### Proof of Theorem 3

**Theorem 3**.: SetMixer _is permutation invariant and is a universal approximator of invariant multi-set functions. That is, SetMixer can approximate any invariant multi-set function._

Proof.: Let \(\pi(S)\) be a given permutation of set \(S\), we aim to show that \(\Psi(S)=\Psi(\pi(S))\). We first recall the SetMixer and its two phases: Let \(S=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{d}\}\), where \(\mathbf{v}_{i}\in\mathbb{R}^{d_{1}}\), be the input set and \(\mathbf{V}=[\mathbf{v}_{1},\ldots,\mathbf{v}_{d}]^{T}\in\mathbb{R}^{d\wedge d_ {1}}\) be its matrix representation:

\[\Psi(\mathbf{V})=\textsc{Mean}\Big{(}\mathbf{H}_{\mathrm{token}}+\sigma \left(\textsc{LayerNorm}\left(\mathbf{H}_{\mathrm{token}}\right)\mathbf{W}_{ s}^{(1)}\right)\mathbf{W}_{s}^{(2)}\Big{)},\quad(Channel\ Mizer)\]where

\[\mathbf{H}_{\text{token}} =\mathbf{V}+\sigma\left(\texttt{Softmax}\left(\texttt{LayerNorm}( \mathbf{V})^{T}\right)\right)^{T}.\] ( \[Token\,\,Mixer\] )

Let \(\pi(\mathbf{V})=[\mathbf{v}_{\pi(1)},\ldots,\mathbf{v}_{\pi(d)}]^{T}\) be a permutation of the input matrix \(\mathbf{V}\). In the token mixer phase, None of LayerNorm, Softmax, and activation function \(\sigma(.)\) can affect the order of elements (note that Softmax is applied row-wise). Accordingly, we can see the output of the token mixer is permuted by \(\pi(.)\):

\[\mathbf{H}_{\text{token}}(\pi(\mathbf{V})) =\pi(\mathbf{V})+\sigma\left(\texttt{Softmax}\left(\texttt{LayerNorm }(\pi(\mathbf{V}))^{T}\right)\right)^{T}\] \[=\pi(\mathbf{V})+\pi\left(\sigma\left(\texttt{Softmax}\left( \texttt{LayerNorm}(\mathbf{V})^{T}\right)\right)^{T}\right)\] \[=\pi\left(\mathbf{V}+\sigma\left(\texttt{Softmax}\left( \texttt{LayerNorm}(\mathbf{V})^{T}\right)\right)^{T}\right)\] \[=\pi\left(\mathbf{H}_{\text{token}}(\mathbf{V})\right).\] (14)

Next, in the channel mixer, by using Equation 14 we have:

\[\Psi(\pi(\mathbf{V})) =\texttt{Mean}\left(\pi(\mathbf{H}_{\text{token}})+\sigma\left( \texttt{LayerNorm}(\pi(\mathbf{H}_{\text{token}}))\,\mathbf{W}_{s}^{(1)} \right)\mathbf{W}_{s}^{(2)}\right)\] \[=\texttt{Mean}\left(\pi(\mathbf{H}_{\text{token}})+\pi\left( \sigma\left(\texttt{LayerNorm}(\mathbf{H}_{\text{token}})\,\mathbf{W}_{s}^{( 1)}\right)\right)\mathbf{W}_{s}^{(2)}\right)\] \[=\texttt{Mean}\left(\pi(\mathbf{H}_{\text{token}}+\mathbf{W}_{s }^{(2)}\sigma\left(\texttt{LayerNorm}(\mathbf{H}_{\text{token}})\,\mathbf{W}_{ s}^{(1)}\right)\right)\right)\] \[=\Psi(\mathbf{V}).\] (15)

In the last step, we use the fact that Mean(.) is permutation invariant. Based on Equation 15 we can see that SetMixer is permutation invariant.

Since the token mixer is just normalization it is inevitable and cannot affect the expressive power of SetMixer. Also, channel mixer is a 2-layer MLP, which is the universal approximator of any function. Therefore, SetMixer is a universal approximator. 

### Proof of Theorem 4

**Theorem 4**.: _The set-based anonymization method is more expressive than any existing anonymization strategies on the CE of the hypergraph. More precisely, there exists a pair of hypergraphs \(\mathcal{G}_{1}=(\mathcal{V}_{1},\mathcal{E}_{1})\) and \(\mathcal{G}_{2}=(\mathcal{V}_{2},\mathcal{E}_{2})\) with different structures (i.e., \(\mathcal{G}_{1}\not\equiv\mathcal{G}_{2}\)) that are distinguishable by our anonymization process and are not distinguishable by the CE-based methods._

Proof.: To the best of our knowledge, there exist two anonymization processes for random walks by Wang et al. [33] and Micali and Zhu [45]. Both of these methods are designed for graphs and to adapt them to hypergraphs we need to apply them to the (weighted) CE. Here, we focus on the process designed by Wang et al. [33], which is more informative than the other. The proof for the Micali and Zhu [45] process is the same. Note that the goal of this theorem is to investigate whether a method can distinguish a hypergraph from its CE. Accordingly, this theorem does not provide any information about the expressivity of these methods in terms of the isomorphism test.

The proposed 2-step anonymization process can be seen as a positional encoding for both vertices and hyperedges. Accordingly, it is expected to assign different positional encodings to vertices and hyperedges of two non-isomorphism hypergraphs. To this end, we construct the same hypergraphs as in the proof of Theorem 1. Let \(M\) be the number of sampled SetWalks with length \(L\). We construct a temporal hypergraph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) as a hypergraph with \(\mathcal{V}=\{u_{1},u_{2},\ldots,u_{M(L+1)+1}\}\) and \(\mathcal{E}=\{(e,t_{i})\}_{i=1}^{L}\), where \(e=\{u_{1},u_{2},\ldots,u_{M(L+1)+1}\}\) and \(t_{1}<t_{2}<\cdots<t_{L}\). We further construct \(\mathcal{G}^{\prime}=(\mathcal{V},\mathcal{E}^{\prime})\) with the same set of vertices but with \(\mathcal{E}^{\prime}=\bigcup_{k=1}^{L}\{((u_{i},u_{j}),t_{k})\}_{u_{i},u_{j},u_{ j}}\). As we have seen in Theorem 1, random walks on the CE of the hypergraph cannot distinguish these two hypergraphs. Since CAW [33] also uses simple random walks, it cannot distinguish these two hypergraphs. Accordingly, after its anonymization process, it again cannot distinguish these two hypergraphs.

The main part of the proof is to show that in our method, the assigned positional encodings are different in these hypergraphs. The first step is to assign each node a positional encoding. Maskingthe timestamps, there is only one SetWalk in the \(\mathcal{G}\). Accordingly, the positional encodings of nodes in \(\mathcal{G}\) are the same and non-zero. Given a SetWalk with length \(L\) we might see at most \(L\times(d_{\max}-1)+1\) nodes, where \(d_{\max}\) is the maximum size of hyperedges in the hypergraph. Accordingly, with \(M\) samples on \(\mathcal{G}^{\prime}\), which \(d_{\max}=2\), we can see at most \(M\times(L+1)\) vertices. Therefore, in any case, we assign a zero vector to at least one vertex. This proves that the positional encodings by SetWalks are different in these two hypergraphs, and if the assigned hidden identities to counterpart nodes are different, clearly, feeding them to the SetMixer results in different hyperedge encodings.

Note that each SetWalk can be decomposed into a set of causal anonymous walks [33]. Accordingly, it includes the information provided by these walks, so its expressiveness is not less than the CAW method on hypergraphs, which completes the proof of the theorem. 

Although the above statement completes the proof, next we discuss that even given the same positional encodings for vertices in these two hypergraphs, SetMixer can capture higher-order interactions by capturing the size of the hyperedge. Recall token mixer phase in SetMixer:

\[\mathbf{H}_{\text{token}}=\mathbf{V}+\sigma\left(\texttt{Softmax}\left( \texttt{LayerNorm}(\mathbf{V})^{T}\right)\right)^{T},\]

where \(\mathbf{V}=[\mathbf{v}_{1},\dots,\mathbf{v}_{M(L+1)+1}]^{T}\in\mathbb{R}^{(M (L+1)+1)\times d_{1}}\) and \(\mathbf{v}_{i}\neq 0_{1\times d_{1}}\) represents the positional encoding of \(u_{i}\) in \(\mathcal{G}\). We assumed that the positional encoding of \(u_{i}\) in \(\mathcal{G}^{\prime}\) is the same. The input of the token mixer phase on \(\mathcal{G}\) is \(\mathcal{V}\) as all of them are connected by a hyperedge. Then we have:

\[(\mathbf{H}_{\text{token}})_{i,j}=\mathbf{v}_{i,j}+\sigma\left(\frac{\exp( \mathbf{v}_{i,j})}{\sum_{k=1}^{M(L+1)+1}\exp(\mathbf{v}_{k,j})}\right).\] (16)

On the other hand, when applied to hypergraph \(\mathcal{G}^{\prime}\) and \((u_{k_{1}},u_{k_{2}})\). We have:

\[(\mathbf{H}^{\prime}_{\text{token}})_{i,j}=\mathbf{v}_{i,j}+\sigma\left(\frac {\exp(\mathbf{v}_{i,j})}{\exp(\mathbf{v}_{k_{1},j})+\exp(\mathbf{v}_{k_{2},j} )}\right),\quad i\in\{k_{1},k_{2}\}.\] (17)

Since we use zero padding, for any \(i\geq 3\), \((\mathbf{H}_{\text{token}})_{i,j}\neq 0\) and \((\mathbf{H}^{\prime}_{\text{token}})_{i,j}=0\). These zero rows, which capture the size of the hyperedge, result in different encodings for each connection.

**Remark 5**.: _To the best of our knowledge, the only anonymization process that is used on hypergraphs is by Liu et al. [78], which uses simple walks on the CE and is the same as Wang et al. [33]. Accordingly, it also suffers from the above limitation. Also, note that this theorem shows the limitation of these anonymization procedures when simply adopted to hypergraphs._

### Proof of Proposition 2

**Proposition 2**.: _Edge-independent random walks on hypergraphs [37], edge-dependent random walks on hypergraphs [40], and simple random walks on the CE of hypergraphs are all special cases of \(r\)-SetWalk, when applied to the \(2\)-projected graph, UP graph, and \(2\)-projected graph, respectively. Furthermore, all the above methods are less expressive than \(r\)-SetWalks._

Proof.: For the first part, we discuss each walk separately:

1 Simple random walks on the CE of the hypergraphs: We perform \(2\)-SetWalks on the (weighted) \(2\)-projected hypergraph with \(\Gamma(.)=1\). Accordingly, for every two adjacent edges in the \(2\)-Projected graph like \(e\) and \(e^{\prime}\), we have \(\varphi(e,e^{\prime})=1\). Therefore, it is equivalent to a simple random walk on the CE (\(2\)-projected graph).

2 Edge-independent random walks on hypergraphs: As is shown by Chitra and Raphael [40], each edge-independent random walk on hypergraphs is equivalent to a simple random walk on the (weighted) CE of the hypergraph. Therefore, as discussed in 1, these walks are a special case of \(r\)-SetWalks, when \(r=2\) and applied to (weighted) \(2\)-Projected hypergraph.

3 Edge-dependent random walks on hypergraphs: Let \(\Gamma^{\prime}(e,u)\) be an edge-dependent weight function used in the hypergraph random walk sampling. For each node \(u\) in the UP hypergraph, we store the set of \(\Gamma^{\prime}(e,u)\) that \(e\) is a maximal hyperedge that \(u\) belongs to. Note that there might be several maximal hyperedges that \(u\) belongs to. Now, we perform \(2\)-SetWalk sampling on the UP hypergraph with these weights and in each step, we sample each hyperedge with weight \(\Gamma(u,e)=\Gamma^{\prime}(e,u)\). It is straightforward to show that given this procedure, the sampling probability of a hyperedge is the same in both cases. Therefore, edge-dependent random walks on hypergraphs are equivalent to 2-SetWalks when applied to the UP hypergraph.

As discussed above, all these walks are special cases of \(r\)-SetWalks and cannot be more expressive than \(r\)-SetWalks. Also, as discussed in Theorem 1, all these walks are less expressive than SetWalks, which are also special cases of \(r\)-SetWalks, when \(r=\infty\). Accordingly, all these methods are less expressive than \(r\)-SetWalks. 

## Appendix F Experimental Setup Details

### Datasets

We use 10 publicly available3 benchmark datasets, whose descriptions are as follows:

Footnote 3: https://www.cs.cornell.edu/~arb/data/

* **NDC Class**[6]: The NDC Class dataset is a temporal higher-order network, in which each hyperedge corresponds to an individual drug, and the nodes contained within the hyperedges represent class labels assigned to these drugs. The timestamps, measured in days, indicate the initial market entry of each drug. Here, hyperedge prediction aims to predict future drugs.
* **NDC Substances**[6]: The NDC Substances is a temporal higher-order network, where each hyperedge represents an NDC code associated with a specific drug, while the nodes represent the constituent substances of the drug. The timestamps, measured in days, indicate the initial market entry of each drug. The hyperedge prediction task is the same as NDC Classes dataset.
* **High School**[6, 92]: The High School is a temporal higher-order network dataset constructed from interactions recorded by wearable sensors in a high school setting. The dataset captures a high school contact network, where each student/teacher is represented as a node and each hyperedge shows Face-to-face contact among individuals. Interactions were recorded at a resolution of 20 seconds, capturing all interactions that occurred within the previous 20 seconds. Node labels in this data are the class of students, and we focus on the node class "PSI" in our classification tasks.
* **Primary School**[6, 93]: The primary school dataset resembles the high school dataset, differing only in terms of the school level from which the data is collected. Node labels in this data are the class of students, and we focus on the node class "Teachers" in our classification tasks.
* **Congress Bill**[6, 94, 95]: Each node in this dataset represents a US Congressperson. Each hyperedge is a legislative bill in both the House of Representatives and the Senate, connecting the sponsors and co-sponsors of each respective bill. The timestamps, measured in days, indicate the date when each bill was introduced.
* **Email Enron**[6]: In this dataset nodes are email addresses at Enron and hyperedges are formed by emails, connecting the sender and recipients of each email. The timestamps have a resolution of milliseconds.
* **Email Eu**[6, 96]: In this dataset, the nodes represent email addresses associated with a European research institution. Each hyperedge consists of the sender and all recipients of the email. The timestamps in this dataset are measured with a resolution of 1 second.
* **Question Tags M (Math sx)**[6]: This dataset consists of nodes representing tags and hyperedges representing sets of tags applied to questions on math.stackexchange.com. The timestamps in the dataset are recorded at millisecond resolution and have been normalized to start at 0.
* **Question Tags U (Ask Ubuntu)**[6]: In this dataset, the nodes represent tags, and the hyperedges represent sets of tags applied to questions on askubuntu.com. The timestamps in the dataset are recorded with millisecond resolution and have been normalized to start at 0.

* **Users-Threads**[6]: In this dataset, the nodes represent users on askubuntu.com, and a hyperedge is formed by users participating in a thread that lasts for a maximum duration of 24 hours. The timestamps in the dataset denote the time of each post, measured in milliseconds but normalized such that the earliest post begins at 0.

The statistics of these datasets can be found in Table 3.

### Baselines

We compare our method to eight previous state-of-the-art methods and baselines on the hyperedge prediction task:

* CHESHIRE [11]: Chebyshev spectral hyperlink predictor (CHESHIRE), is a hyperedge prediction methods that initializes node embeddings by directly passing the incidence matrix through a one-layer neural network. CHESHIRE treats a hyperedge as a fully connected graph (clique) and uses a Chebyshev spectral GCN to refine the embeddings of the nodes within the hyperedge. The Chebyshev spectral GCN leverages Chebyshev polynomial expansion and spectral graph theory to learn localized spectral filters. These filters enable the extraction of local and composite features from graphs that capture complex geometric structures. The model with code provided is here.
* HyperSAGCN [26]: Self-attention-based graph convolutional network for hypergraphs (HyperSAGCN) utilizes a Spectral Aggregated Graph Convolutional Network (SAGCN) to refine the embeddings of nodes within each hyperedge. HyperSAGCN generates initial node embeddings by hypergraph random walks and combines node embeddings by Mean (.) pooling to compute the embedding of hyperedge. The model with code provided is here.
* NHP [87]: Neural Hyperlink Predictor (NHP), is an enhanced version of HyperSAGCN. NHP initializes node embeddings using Node2Vec on the CE graph and then uses a novel maximum minimum-based pooling function that enables adaptive weight learning in a task-specific manner, incorporating additional prior knowledge about the nodes. The model with code provided is here.
* HPLSF [89]: Hyperlink Prediction using Latent Social Features (HPLSF) is a probabilistic method. It leverages the homophily property of the networks and introduces a latent feature learning approach, incorporating the use of entropy in computing hyperedge embedding. The model with code provided is here.
* HPRA [88]: Hyperlink Prediction Using Resource Allocation (HPRA) is a hyperedge prediction method based on the resource allocation process. HPRA calculates a hypergraph resource allocation (HRA) index between two nodes, taking into account direct connections and shared neighbors. The HRA index of a candidate hyperedge is determined by averaging all pairwise HRA indices between the nodes within the hyperedge. The model with code provided is here.
* CE-CAW: This model is a baseline that we apply CAW [33] on the CE of the hypergraph. CAW is a temporal edge prediction method that uses causal anonymous random walks to capture the dynamic laws of the network in an inductive manner. The model with code provided is here.
* CE-EvolveGCN: This is a snapshot-based temporal graph learning method that we apply EvolveGCN [90], which uses Rnss to estimate the GCN parameters for the future snapshots, on the CE of the hypergraph. The model with code provided is here.
* CE-GCN: We apply Graph Convolutional Networks [91] to the CE of the hypergraph to obtain node embeddings. Next, we use MLP to predict edges. The implementation is provided in the Pytorch Geometric library.

\begin{table}
\begin{tabular}{c|c c c c c c c c c} \hline \hline Dataset & NDC Class & High School & Primary School & Congress Bill & Email Enron & Email En & Question Tags M & User-Throughs & NDC Substances & Question Tags U \\ \hline \hline (V) & 1,161 & 327 & 242 & 1,718 & 143 & 998 & 1,629 & 125.620 & 5,311 & 3,629 \\ \(\ell\) & 49,724 & 172,035 & 106,879 & 260,851 & 108,833 & 24,760 & 822,059 & 192,947 & 112,405 & 271,233 \\ \(\ell\)/minestamps & 5,891 & 7,375 & 3,100 & 5,936 & 10,788 & 223,816 & 822,054 & 189,317 & 7,734 & 271,233 \\ Task & HeP & HePRN & HeP \& NC & HeP & HeP & HeP & HeP & HeP & HeP \\ \hline \hline \end{tabular}
\end{table}
Table 3: Datasetistics. HeP: Hyperedge Prediction, NC: Node ClassificationFor node classification, we use additional five state-of-the-art deep hypergraph learning methods and a CE-based baseline:

* HyperGCN [19]: This is a generalization of GCNs to hypergraphs, where it uses hypergraph Laplacian to define convolution.
* AllDeepSets and AllSetTransformer[25]: These two methods are two variants of the general message passing framework, Allset, on hypergraphs, which are based on the aggregation of messages from nodes to hyperedges and from hyperedges to nodes.
* UniGCNII [28]: Is an advanced variant of UniGnn, a general framework for message passing on hypergraphs.
* ED-HNN [124]: Inspired by hypergraph diffusion algorithms, this method uses star expansions of hypergraphs with standard message passing neural networks.
* CE-GCN: We apply Graph Convolutional Networks [91] to the CE of the hypergraph to obtain node embeddings. Next, we use MLP to predict the labels of nodes. The implementation is provided in the Pytorch Geometric library. [91]

For all the baselines, we set all sensitive hyperparameters (e.g., learning rate, dropout rate, batch size, etc.) to the values given in the paper that describes the technique. Following [60], for deep learning methods, we tune their hidden dimensions via grid search to be consistent with what we did for CAr-Walk. We exclude HPLSF [89] and HPRA [88] from inductive hyperedge prediction as it does not apply to them.

### Implementation and Training Details

In addition to hyperparameters and modules (activation functions) mentioned in the main paper, here, we report the training hyperparameters of CAr-Walk: On all datasets, we use a batch size of 64 and set learning rate \(=10^{-4}\). We also use an early stopping strategy to stop training if the validation performance does not increase for more than 5 epochs. We use the maximum training epoch number of 30 and dropout layers with rate \(=0.1\). Other hyperparameters used in the implementation can be found in the README file in the supplement.

Also, for tuning the model's hyperparameters, we systematically tune them using grid search. The search domains of each hyperparameter are reported in Table 4. Note that, the last column in Table 4 reports the search domain for hidden dimensions of modules in CAr-Walk, including SetMixer, MLP-Mixer, and MLPs. Also, we tune the last layer pooling strategy with two options: SetMixer or Mean(.) whichever leads to a better performance.

We implemented our method in Python 3.7 with _PyTorch_ and run the experiments on a Linux machine with _nvidia RTX A4000_ GPU with 16GB of RAM.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Datasets & Sampling Number \(M\) & Sampling Time Bias \(\alpha\) & SetMix Length \(m\) & Hidden dimensions \\ \hline \hline NDC Class & 4, 8, 16, 32, 64, 128 & \(\{0.5,2.0,20,200\}\times 10^{-7}\) & 2, 3, 4, 5 & 32, 64, 128 \\ High School & 4, 8, 16, 32, 64, 128 & \(\{0.5,2.0,20,200\}\times 10^{-7}\) & 2, 3, 4, 5 & 32, 64, 128 \\ Primary School & 4, 8, 16, 32, 64 & \(\{0.5,2.0,20,200\}\times 10^{-7}\) & 2, 3, 4, 5 & 32, 64, 128 \\ Congress Bill & 8, 16, 32, 64 & \(\{0.5,2.0,20,200\}\times 10^{-7}\) & 2, 3, 4, 5 & 32, 64, 128 \\ Email Enron & 8, 16, 32, 64 & \(\{0.5,2.0,20,200\}\times 10^{-7}\) & 2, 3, 4, 5 & 32, 64, 128 \\ Email Eu & 8, 16, 32, 64 & \(\{0.5,2.0,20,200\}\times 10^{-7}\) & 2, 3, 4, 5 & 32, 64, 128 \\ Question Tags M & 8, 16, 32, 64 & \(\{0.5,2.0,20,200\}\times 10^{-7}\) & 2, 3, 4 & 32, 64, 128 \\ Users-Threads & 8, 16, 32, 64 & \(\{0.5,2.0,20,200\}\times 10^{-7}\) & 2, 3, 4 & 32, 64, 128 \\ NDC Substances & 8, 16, 32, 64 & \(\{0.5,2.0,20,200\}\times 10^{-7}\) & 2, 3, 4 & 32, 64, 128 \\ Question Tags U & 8, 16, 32, 64 & \(\{0.5,2.0,20,200\}\times 10^{-7}\) & 2, 3, 4 & 32, 64, 128 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Hyperparameters used in the grid search.

## Appendix G Additional Experimental Results

### Results on More Datasets

Due to the space limit, we report the AUC results on only eight datasets in Section 4. Table 6 reports both AUC and average precision (AP) results on all 10 datasets in both inductive and transductive hyperedge prediction tasks.

### Node Classification

In the main text, we focus on the hyperedge prediction task. Here we describe how CAr-Walk can be used for node classification tasks.

For each node \(u_{0}\) in the training set, we sample \(\max\{\deg(u_{0}),10\}\) hyperedges such as \(e_{0}=\{u_{0},u_{1},\dots,u_{k}\}\). Next, for each sampled hyperedge we sample \(M\)SetWalks with length \(m\) starting from each \(u_{i}\in e_{0}\) to construct \(\mathcal{S}(u_{i})\). Next, we anonymize each hyperedge that appears in at least one SetWalk in \(\bigcup_{i=0}^{k}\mathcal{S}(u_{i})\) by Equation 3 and then use the MLP-Mixer module to encode each Sw \(\in\bigcup_{i=0}^{k}\mathcal{S}(u_{i})\). To encode each node \(u_{i}\in e_{0}\), we use Mean(.) pooling over SetWalks in \(\mathcal{S}(u_{i})\). Finally, for node classification task, we use a 2-layer perceptron over the node encodings to make the final prediction.

Table 5 reports the results of dynamic node classification tasks on High School and Primary School datasets. CAr-Walk achieves the best or on-par performance on dynamic node classification tasks. While all baselines are specifically designed for node classification tasks, CAr-Walk achieves superior results due to 1 its ability to incorporate temporal properties (both from SetWalks and our time encoding module), which helps to learn underlying dynamic laws of the network, and 2 its two-step set-based anonymization process that hides node identities from the model. Accordingly, CAr-Walk can learn underlying patterns needed for the node classification task, instead of using node identities, which might cause memorizing vertices.

### Performance in Average Precision

In addition to the AUC, we also compare our model with baselines with respect to Average Precision (AP). Table 6 reports both AUC and AP results on all 10 datasets in inductive and transductive hyperedge prediction tasks. As discussed in Section 4, CAr-Walk due to its ability to capture both temporal and higher-order properties of the hypergraphs, achieves superior performance and outperforms all baselines in both transductive and inductive settings with a significant margin.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & Methods & High School & Primary School & Average Performance \\ \hline \hline \multirow{8}{*}{\begin{tabular}{l} **Cat-GCN** \\ **Cat-Walk** \\ **Cat-Walk** \\ **Cat-Walk** \\ \end{tabular} } & CE-GCN & 76.24 \(\pm\) 2.99 & 79.03 \(\pm\) 3.16 & 77.63 \(\pm\) 3.07 \\  & HyperGCN & 83.91 \(\pm\) 3.05 & 86.17 \(\pm\) 3.40 & 85.04 \(\pm\) 3.23 \\  & HyperSAGCN & 84.89 \(\pm\) 3.80 & 82.13 \(\pm\) 3.69 & 83.51 \(\pm\) 3.75 \\  & AllDeepSets & 85.67 \(\pm\) 4.17 & 81.43 \(\pm\) 6.77 & 83.55 \(\pm\) 5.47 \\  & UnICCNII & 88.36 \(\pm\) 3.78 & 88.27 \(\pm\) 3.52 & 88.31 \(\pm\) 3.63 \\  & AllSetTransformer & 81.91 \(\pm\) 2.85 & 90.00 \(\pm\) 4.35 & 90.59 \(\pm\) 3.60 \\  & ED-HNN & 89.23 \(\pm\) 2.98 & 90.83 \(\pm\) 3.02 & 90.03 \(\pm\) 3.00 \\  & CAr-Walk & 88.99 \(\pm\) 4.76 & **93.28 \(\pm\) 2.41** & **91.13 \(\pm\) 3.58** \\ \hline \hline \multirow{8}{*}{
\begin{tabular}{l} **Cat-Walk** \\ **Cat-Walk** \\ \end{tabular} } & CE-GCN & 78.93 \(\pm\) 3.11 & 77.46 \(\pm\) 2.97 & 78.20 \(\pm\) 3.04 \\  & HyperGCN & 84.90 \(\pm\) 3.59 & 85.23 \(\pm\) 3.06 & 85.07 \(\pm\) 3.33 \\  & HyperSAGCN & 84.52 \(\pm\) 3.18 & 83.27 \(\pm\) 2.94 & 83.90 \(\pm\) 3.06 \\  & AllDeepSets & 85.97 \(\pm\) 4.05 & 80.20 \(\pm\) 10.18 & 83.09 \(\pm\) 7.12 \\  & UnICCNII & 89.16 \(\pm\) 4.37 & 90.29 \(\pm\) 4.01 & 89.73 \(\pm\) 4.19 \\  & AllSetTransformer & 90.75 \(\pm\) 3.13 & 89.80 \(\pm\) 2.55 & 90.27 \(\pm\) 2.84 \\  & ED-HNN & **91.41 \(\pm\) 2.36** & 91.74 \(\pm\) 2.62 & 91.56 \(\pm\) 2.49 \\  & CAr-Walk & 90.66 \(\pm\) 4.96 & **93.20 \(\pm\) 2.45** & **91.93 \(\pm\) 3.71** \\ \hline \end{tabular}
\end{table}
Table 5: Performance on node classification: Mean ACC (%) \(\pm\) standard deviation. Boldfaced letters shaded blue indicate the best result, while gray shaded boxes indicate results within one standard deviation of the best result.

[MISSING_PAGE_FAIL:35]

## Appendix H Broader Impacts

Temporal hypergraph learning methods, such as CAr-Walk, benefit a wide array of real-world applications, including but not limited to social network analysis, recommender systems, brain network analysis, drug discovery, stock price prediction, and anomaly detection (e.g. bot detection in social media or abnormal human brain activity). However, there might be some potentially negative impacts, which we list as: 1 Learning underlying biased patterns in the training data, which may result in stereotyped predictions. Since CAr-Walk learns underlying dynamic laws in the training data, given biased training data, the predictions of CAr-Walk can be biased. 2 Also, powerful dynamic hypergraph models can be used for manipulation in the abovementioned applications (e.g., stock price manipulation). Accordingly, to prevent the potential risks in sensitive tasks, e.g., decision-making from graph-structured data in health care, interpretability and explainability of machine learning models on hypergraphs is a critical area for future work.

Furthermore, this work does not perform research on human subjects as part of the study and all used datasets are anonymized and publicly available.

Figure 7: The importance of using MLP-Mixer in CAr-Walk. Using an Rnn instead of MLP-Mixer can damage the performance in _all_ datasets. Rnns are sequential encoders and are not able to encode continuous time in the data.