# Abstract Reward Processes: Leveraging State Abstraction for Consistent Off-Policy Evaluation

Shreyas Chaudhari

University of Massachusetts

schaudhari@cs.umass.edu

&Ameet Deshpande

Princeton University

asd@cs.princeton.edu

Bruno Castro da Silva

University of Massachusetts

psilva@cs.umass.edu

&Philip S. Thomas

University of Massachusetts

pthomas@cs.umass.edu

###### Abstract

Evaluating policies using off-policy data is crucial for applying reinforcement learning to real-world problems such as healthcare and autonomous driving. Previous methods for _off-policy evaluation_ (OPE) generally suffer from high variance or irreducible bias, leading to unacceptably high prediction errors. In this work, we introduce STAR, a framework for OPE that encompasses a broad range of estimators--which include existing OPE methods as special cases--that achieve lower mean squared prediction errors. STAR leverages state abstraction to distill complex, potentially continuous problems into compact, discrete models which we call _abstract reward processes_ (ARPs). Predictions from ARPs estimated from off-policy data are provably consistent (asymptotically correct). Rather than proposing a specific estimator, we present a new framework for OPE and empirically demonstrate that estimators within STAR outperform existing methods. The best STAR estimator outperforms baselines in all twelve cases studied, and even the median STAR estimator surpasses the baselines in seven out of the twelve cases.

## 1 Introduction

Within _reinforcement learning_ (RL), _off-policy evaluation_ (OPE) is the foundational challenge of evaluating the performance, \(J(\pi)\), of policies \(\pi\) that are different from the ones used to generate data. OPE methods are a general-purpose tool that can be used as part of a local policy search algorithm [45] to provide insight about policies similar to the current policy, or as a tool to evaluate policies without requiring their actual deployment for high-risk applications like those in healthcare [38], education [35, 15], and recommendation systems [5, 7]. Despite many recent advances in OPE, existing methods struggle to give accurate predictions for many real-world applications [52], showing the need for new perspectives on OPE.

Existing methods can be broadly divided into two categories: _importance sampling_ (IS) based and model-based [58]. IS-based methods are typically consistent (i.e., their predictions converge probabilistically to the correct value in the limit as the amount of data approaches infinity), but have variance that increases exponentially with the horizon [29, 31]. Model-based methods have lower variance but often introduce bias due to model class mismatch and are not generally guaranteed to be consistent [36, 10]. A third set of methods, which we call _mixture methods_, combine the predictions obtained from both of these categories [22, 53]. However, in some cases, combining the predictions also combines the drawbacks--high variance and bias. This leads us to ask: _Can we develop a framework for OPE that yields predictions that are both consistent and low variance?_In this paper, we introduce a new framework that attains this goal by combining the _machinery_ underlying IS-based and model-based approaches (not just their _predictions_). Our proposed framework is a fundamentally different approach to OPE that incorporates importance sampling _into_ model learning for OPE. Our approach is motivated by the intuition that humans build small mental models of their environment to plan and predict, selectively abstracting away information that is not relevant to the problem at hand [54]. Similarly, complex sequential decision processes can be distilled into compact models that hold sufficient information for (off-)policy evaluation. Specifically, we propose creating small tabular models (even for continuous environments), which we call _abstract reward processes_ (ARPs), customized for the problem at hand and for the policy being evaluated. We call this framework for constructing a range of ARPs _state-abstract reward processes_ (STAR).

Idea Summary:A _Markov decision process_ (MDP) combined with a policy \(\pi\) induces a Markov chain with rewards, called a _Markov reward process_ (MRP). A model of this process can be estimated to evaluate policy \(\pi\). However, two main challenges arise: (1) like any model-based approach, estimating an MRP can introduce asymptotic bias if the chosen model class cannot represent the underlying MRP; and (2) the model of the MRP must be accurately estimated from _off-policy_ data, i.e., data generated by a behavior policy \(\pi_{b}\) that differs from the evaluation policy \(\pi_{e}\). The proposed framework addresses both challenges by modeling a special instantiation of an MRP, as detailed next.

First, to address potential model class mismatch, we propose mapping the large and possibly continuous set of states of the MDP to a finite set of _abstract states_ using a discrete state abstraction function. We then represent the resulting MRP defined over abstract states, referred to as the _abstract reward process_ (ARP), using tabular models. Since the ARP is finite, tabular models can represent it accurately, as depicted in Figure 1(a). However, using a discrete state abstraction may lead to loss of state information that can potentially introduce modeling errors. Surprisingly, we prove that despite state information being abstracted away, the maximum likelihood model of an ARP estimated from _on-policy_ data provides consistent estimates of the expected return of the behavior policy \(\pi_{b}\).

Next, to estimate a model of the ARP corresponding to \(\pi_{e}\) from data generated by \(\pi_{b}\), we reweight occurrences of abstract states in the off-policy dataset using importance sampling, see Figure 1(b). In expectation, this has the effect of updating the abstract state visitation counts to reflect those resulting from the policy being evaluated. We prove that the weighted maximum likelihood estimate of a model of the ARP estimated from this off-policy dataset provides consistent estimates of the performance of the evaluation policy.The integration of importance sampling _into_ model estimation permits a favorable interpretation of weight clipping for mitigating variance (see Section 4.1).

The STAR framework offers two adjustable knobs--the state abstraction function, and the amount of weight clipping--that instantiate a range of OPE estimators. Varying the configurations of these knobs results in different bias-variance trade-offs for OPE, with existing OPE methods forming special cases of the range of estimators that lie within this framework.

Figure 1: **(a):** MDP \(M\) and policy \(\pi_{b}\) are transformed into a discrete _abstract reward process_ (ARP) using a state abstraction function \(\phi\). The ARP aggregates rewards (denoted by stars) and transition probabilities from all states that map to each abstract state. **(b):** A model of the ARP for the evaluation policy \(\pi_{e}\) is constructed by: reweighting data generated by \(\pi_{b}\) with importance weights \(\rho\) (middle), applying the state abstraction function \(\phi\), and performing weighted maximum likelihood estimation of the ARP (right). The expected return of a model of this ARP estimated from off-policy data is a _consistent_ estimator of the expected return of \(\pi_{e}\).

Contributions:We empirically evaluate estimators instantiated in this framework on synthetic domains and a healthcare simulator built from real-world ICU data, where the best STAR estimator significantly outperforms baselines in all cases, and even the median STAR estimator surpasses baselines in seven out of twelve cases. It must be emphasized that this work does not propose a specific estimator for OPE; rather, it introduces a fundamentally different framework that offers fresh insights on approaches for off-policy evaluation. These insights open up exciting avenues for new research questions and future directions. In this paper, we introduce:

1. the first model-based approach for OPE that guarantees asympotic correctness of the estimates without model class assumptions, even for continuous state MDPs (Theorem 4.1).
2. the concept of _abstract reward processes_ for consistent OPE. ARPs abstract away the complexity of the underlying problem, and distill sufficient information for accurate policy evaluation (Theorem 3.1). Being finite, they can be consistently estimated.
3. a generalizing framework that provides a fresh perspective on OPE by merging the machinery of model-based and IS-based approaches. The framework offers two tunable knobs, various configurations of which instantiate a range of OPE estimators with varying bias-variance characteristics. Existing model-free and model-based methods are special cases in this framework.

## 2 Background and Notation

An MDP is a tuple \(M:=(\mathcal{S},\mathcal{A},p,r,\gamma,\eta)\) where \(\mathcal{S}\) is the set of states, \(S_{t}\) is the state at time \(t\in\{0,1,\dots\}\), \(\mathcal{A}\) is the set actions, \(A_{t}\) is the action at time \(t\), \(p:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]\) is the _transition function_ that characterizes state transition dynamics according to \(p(s,a,s^{\prime}):=\Pr(S_{t+1}{=}s^{\prime}|S_{t}{=}s,A_{t}{=}a)\), \(r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) is the _reward function_ that characterizes rewards according to \(r(s,a):=\mathbb{E}[R_{t}|S_{t}{=}s,A_{t}{=}a]\), \(\gamma\in[0,1]\) is the reward discount parameter, and \(\eta:\mathcal{S}\rightarrow[0,1]\) characterizes the initial state distribution according to \(\eta(s):=\Pr(S_{0}{=}s)\).1 A policy \(\pi:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) characterizes how actions can be selected given the current state according to \(\pi(s,a):=\Pr(A_{t}{=}a|S_{t}{=}s)\). We consider finite horizon MDPs [49] where episodes terminate by some (unspecified) time \(T\in\mathbb{N}\)--which is common in practical applications of OPE. For simplicity, we set \(\gamma=1\), allowing us to omit \(\gamma\) terms.

Footnote 1: For simplicity, our notation assumes that states, actions, and rewards are discrete random variables allowing for discussion of probabilities, rather than densities or measure theoretic probability. However, the methods proposed in this work extend to MDPs with continuous states, actions, and rewards. Furthermore, although we focus on OPE for MDPs, the method that we propose also applies to _partially observable_ MDPs (POMDPs).

For OPE, a dataset \(\mathcal{D}_{n}^{(\pi_{b})}\) is collected by deploying a behavior policy \(\pi_{b}\) on the MDP \(M\). The dataset of \(n\) logged trajectories is denoted by \(\mathcal{D}_{n}^{(\pi_{b})}:=\{H^{i}\}_{i=1}^{n}\) where each \(H^{i}:=(S_{0}^{i},A_{0}^{i},R_{0}^{i},S_{1}^{i},\dots)\) represents an independent trajectory generated by executing \(\pi_{b}\). The performance of an evaluation policy \(\pi_{e}\) is its expected return, denoted by2

Footnote 2: We write \(;\pi\) within statements of probability or expectations to indicate that random variables like \(S_{t},A_{t}\), and \(R_{t}\) result from the use of policy \(\pi\).

\[J(\pi_{e}):=\mathbb{E}\left[\sum_{t=1}^{T}R_{t};\pi_{e}\right].\] (1)

The problem of off-policy evaluation entails estimating \(J(\pi_{e})\) with access only to data \(\mathcal{D}_{n}^{(\pi_{b})}\), generated by a behavior policy \(\pi_{b}\), without additional interaction with the MDP. To ensure that samples in \(\mathcal{D}_{n}^{(\pi_{b})}\) are sufficiently informative, we make the common assumption that any outcome under \(\pi_{e}\) has non-negligible probability of occurring under \(\pi_{b}\).

**Assumption 2.1**.: There exists an (unknown) \(\varepsilon>0\) such that for all \(s\in\mathcal{S}\) and \(a\in\mathcal{A}\), \((\pi_{b}(s,a)<\varepsilon)\Longrightarrow(\pi_{e}(s,a)=0)\).

Background:For a detailed review of OPE methods, we refer the reader to surveys by Voloshin et al. [58] and Uehara et al. [56]. Concepts fundamental to this approach are briefly introduced here.

1. **Importance Sampling:** Importance sampling [25] enables unbiased estimation of the expected value, \(\mathbb{E}[f(X)]\), of a function \(f\) applied to a random variable \(X\sim p\), given samples of a different random variable \(Y\sim q\). The importance sampling estimator is \((p(Y)/q(Y))f(Y)\), where \(p(Y)/q(Y)\) is a term called an importance weight. This technique can provide unbiased estimates (i.e., \(\mathbb{E}\left[(p(Y)/q(Y))f(Y)\right]=\mathbb{E}\left[f(X)\right]\)) and has proven effective for variance reduction in Monte Carlo sampling [44] and for model-free OPE in RL [42].
2. **State Abstraction:** State abstraction aims to reduce the size of the state space by grouping together similar states in a way that does not change the essence of the underlying problem [28; 43; 1]. A state abstraction function \(\phi:\mathcal{S}\rightarrow\mathcal{Z}\) lies in the set of functions \(\phi\in\Phi\) that map each state \(s\in\mathcal{S}\) to an abstract state \(z\in\mathcal{Z}\). We consider abstraction functions that partition \(\mathcal{S}\) into disjoint sets, where \(\mathcal{Z}\) is a finite set.

Notation:Indicator functions are abbreviated for clarity. For example, \(\mathbf{1}_{t}^{i}\{z,z^{\prime}\}:=\mathbf{1}\{\phi(S_{t+1}^{(i)})=z^{\prime},\phi(S_{t}^{(i)})=z\}\) denotes the occurrence of abstract states \(z\) and \(z^{\prime}\) at time steps \(t\) and \(t+1\) in the \(i^{\text{th}}\) logged trajectory, with \(\mathbf{1}_{t}^{i}\{z\}\) defined correspondingly. The expected return of \(\pi\) when obtained from \(O\)--where \(O\) may be the MDP, or an ARP--is denoted by \(J(\pi;O)\). The sample estimate of a variable \(y\) estimated from \(n\) samples is denoted by \(\hat{y}_{n}\). Summation limits are often dropped for brevity, with \(\sum_{t}\) denoting \(\sum_{t=0}^{T}\) and \(\sum_{i}\) denoting \(\sum_{i=1}^{n}\).

### Markov Reward Processes

A _Markov reward process_ (MRP) extends the idea of a Markov chain by associating states with rewards. Formally, an MRP is a tuple \((\mathcal{X},p,r,\gamma,\eta)\) where \(\mathcal{X}\) is the set of states of the MRP, \(X_{t}\) is the state at time \(t\), \(p:\mathcal{X}\times\mathcal{X}\rightarrow[0,1]\) is the transition function where \(p(x,x^{\prime}):=\Pr(X_{t+1}{=}x^{\prime}|X_{t}{=}x)\), \(r:\mathcal{X}\rightarrow\mathbb{R}\) is the reward function where \(r(x):=\mathbb{E}[R_{t}|X_{t}{=}x]\), \(\gamma\in[0,1]\) is the discount factor, and \(\eta:\mathcal{X}\rightarrow[0,1]\) is the starting state distribution. We consider finite horizon MRPs where episodes terminate by some (unspecified) timestep and set \(\gamma=1\).

A specific MRP is induced by the use of a fixed policy \(\pi\) on an MDP \(M\), where \(\mathcal{X}=\mathcal{S}\). The resulting transition and reward functions, denoted by \(p^{\pi}\) and \(r^{\pi}\) respectively, are:

\[p^{\pi}(x,x^{\prime})=\frac{\sum_{t}\Pr(S_{t+1}=x^{\prime},S_{t}=x;\pi)}{\sum _{t}\Pr(S_{t}=x;\pi)},\quad r^{\pi}(x)=\frac{\sum_{t}\mathbb{E}\left[R_{t}|S_ {t}=x;\pi\right]\Pr(S_{t}=x;\pi)}{\sum_{t}\Pr(S_{t}=x;\pi)}.\] (2)

The Markov property [37] allows for further simplification of the above expressions (detailed in Appendix A.1), but this form is most conducive to our subsequent discussion. In this work, we focus on a specific instantiation of an MRP, described in the next section, where the set of states \(\mathcal{X}\) of the MRP are outputs of a state abstraction function \(\phi\in\Phi\).

## 3 Abstract Reward Processes

An abstract reward process is a Markov reward process--derived from MDP \(M\) and policy \(\pi\) and defined over _abstract_ states--that we use to evaluate \(\pi\). The ARP provides two primary benefits for policy evaluation: (1) it preserves sufficient information to exactly evaluate the policy \(\pi\), and (2) the ARP can be _consistently_ estimated from data. In this section, we formalize the concept of an ARP, and highlight the theoretical and practical benefits of using ARPs for policy evaluation.

Given a state abstraction function \(\phi:\mathcal{S}\rightarrow\mathcal{Z}\), the ARP \(\mathfrak{R}^{\pi}_{\phi}\) is defined such that \(\mathcal{X}=\mathcal{Z}\). Formally, \(\mathfrak{R}^{\pi}_{\phi}\) is an MRP \((\mathcal{Z},\mathrm{P}^{\pi}_{\phi},\mathrm{R}^{\pi}_{\phi},\eta_{\phi})\), with \(\gamma=1\) (see Appendix A.1 for a discussion on termination in ARPs and MRPs). The components of the ARP are defined over _abstract_ states as:

\[\mathrm{P}^{\pi}_{\phi}(z,z^{\prime}){:=}\frac{\sum_{t}\Pr(\phi(S_{t+1}){=}z^{ \prime},\phi(S_{t}){=}z;\pi)}{\sum_{t}\Pr(\phi(S_{t}){=}z;\pi)},\mathrm{R}^{ \pi}_{\phi}(z){:=}\frac{\sum_{t}\mathbb{E}[R_{t}|\phi(S_{t}){=}z;\pi]\Pr(\phi(S_ {t}){=}z;\pi)}{\sum_{t}\Pr(\phi(S_{t}){=}z;\pi)},\] (3)

and \(\eta_{\phi}(z):=\Pr(\phi(S_{0})=z)\). These expressions _cannot_ be simplified further, unlike the case of an MRP [2]. Since \(\mathcal{Z}\) is a finite set, i.e., the abstract states are discrete, the components of the ARP can be represented by matrices (we use uppercase letters to emphasize this). The expected return of \(\mathfrak{R}^{\pi}_{\phi}\) can be computed efficiently using a linear solver to evaluate the expression \(J(\pi;\mathfrak{R}^{\pi}_{\phi}):=(\mathrm{I-P}^{\pi}_{\phi})^{-1}\mathrm{R}^{ \pi}_{\phi}\eta_{\phi}\), or via Monte Carlo rollouts of the reward process.

**ARPs are _Performance Preserving_:** The expected return of an ARP has a surprising property: even though some state information is abstracted away to create simple discrete abstract states, the finite ARP, derived from a possibly continuous and complex MDP, preserves sufficient information about the performance of the policy that defines the ARP _for all \(\phi\in\Phi\)_.

**Theorem 3.1**.: \(\forall\,\phi\in\Phi\)_, the performance of a policy \(\pi\) is equal to the expected return of the abstract reward process \(\mathfrak{R}^{\pi}_{\phi}\) defined from MDP \(M\), i.e., \(J(\pi;\mathfrak{R}^{\pi}_{\phi})=J(\pi;M)\)._

Proof.: See Appendix B.1. 

The result holds for the ground-truth ARP \(\mathfrak{R}^{\pi}_{\phi}\). In practice, a model of the ARP must be estimated from data. Next, we describe how the choice of defining an ARP over discrete abstract states eliminates model class mismatch, enabling asymptotically correct estimation of the ARP from data.

**Eliminating Model Class Mismatch:** Methods that learn models from data make an assumption about the class of models used to represent the data. A significant challenge is that of _model class mismatch_, where this assumed model class is often unable to represent the true data distribution. As an example, a neural network parameterizing a univariate Gaussian distribution cannot accurately represent data generated from a bimodal distribution. In the context of this work, the transition function of an ARP may specify arbitrary probability distributions over discrete abstract states, necessitating a careful selection of the model class. _Tabular models_ are capable of representing _any_ distribution over discrete variables. Therefore, using tabular models when estimating an ARP from data ensures that there is no model class mismatch. This is why we employ state abstraction functions \(\phi\in\Phi\) that partition the state space into a finite number of disjoint sets, or discrete abstract states. The abstraction functions may be viewed as: (a) a discrete clustering of the state space, or (b) a discretization of continuous states.

While this addresses model class mismatch, the use of a discrete state abstraction may itself be a source of modeling error. Mapping groups of (possibly continuous) states to discrete abstract states loses information about the state of the MDP. A process defined over the abstract states cannot in general capture the full complexity of the underlying MDP and policy. Nonetheless, Theorem 3.1 guarantees that the ARP is performance-preserving, ensuring that the use of discrete state abstractions is not a source of error for policy evaluation. Additionally, since we have eliminated model class mismatch, a perfect model of the ARP can be asymptotically estimated.

To estimate the ARP from \(\mathcal{D}^{(\pi_{b})}_{n}\), apply the state abstraction function to states in \(\mathcal{D}^{(\pi)}_{n}\) to map them to the abstract state space. Denote the _maximum likelihood estimate_ of the model of the ARP obtained from the dataset (with abstract states) by \(\widehat{\mathfrak{R}}^{\pi}_{n,\phi}{:=}(\mathcal{Z},\widehat{\mathrm{P}}^{ \pi}_{n,\phi},\widehat{\mathrm{R}}^{\pi}_{n,\phi},\widehat{\eta}_{n,\phi})\). The components take the form:

\[\widehat{\mathrm{P}}^{\pi}_{n,\phi}(z,z^{\prime})=\frac{\sum_{i,t}\mathbf{1}^ {i}_{t}\{z,z^{\prime}\}}{\sum_{i,t}\mathbf{1}^{i}_{t}\{z\}};\quad\widehat{ \mathrm{R}}^{\pi}_{n,\phi}(z)=\frac{\sum_{i,t}\mathbf{1}^{i}_{t}\{z\}R^{i}_{t }}{\sum_{i,t}\mathbf{1}^{i}_{t}\{z\}};\quad\widehat{\eta}^{\pi}_{n,\phi}(z)= \frac{\sum_{i=1}^{n}\mathbf{1}^{i}\{z_{0}=z\}}{n}\] (4)

**Asymptotic Correctness:** With access to _on-policy data_\(\mathcal{D}^{(\pi)}_{n}\), the following result states that ARPs enable consistent model-based estimation of the policy's performance.

**Lemma 3.2**.: \(\forall\phi\in\Phi\)_, the expected return of the maximum likelihood estimate_\(\widehat{\mathfrak{R}}^{\pi}_{n,\phi}\)_converges almost surely to the expected return of the policy \(\pi\), i.e., \(J(\pi;\widehat{\mathfrak{R}}^{\pi}_{n,\phi})\stackrel{{ a.s.}}{{ \longrightarrow}}J(\pi;M)\)._

Proof.: See Appendix B.2. 

As the amount of data (\(n\)) increases, the estimate of \(J(\pi;M)\) becomes increasingly accurate, i.e., the return estimate is consistent. This result holds for all \(\phi\in\Phi\). It implies that _even an arbitrarily small model derived from a large, complex sequential decision-making problem will not introduce asymptotic bias._ However, this theoretical guarantee requires on-policy data and so does not directly assist us in off-policy evaluation. To that end, we introduce a procedure for estimation of the ARP from _off-policy data_ that merges the machinery of IS-based and model-based methods.

### Estimation from Off-Policy Data: Weighted Maximum Likelihood Estimation

We present a method for consistent estimation of the ARP corresponding to the evaluation policy \(\pi_{e}\) from off-policy data \(\mathcal{D}^{(\pi_{b})}_{n}\). It relies on the following intuition:The expected value of the indicator function of an event represents the probability of that event.

Use importance sampling to approximate the probability of that event under a different distribution.

To estimate an ARP from off-policy data, assign importance weights \(\rho_{0:t}\) to the abstract states \((Z_{t}:=\phi(S_{t}))\) in the dataset \(\mathcal{D}_{n}^{(\pi_{b})}\). Let \(H_{t}:=(S_{0},A_{0},R_{0},\ldots,S_{t-1},A_{t-1},R_{t-1},S_{t},A_{t})\) denote a sub-trajectory up to time \(t\). The importance weight \(\rho_{0:t}\) is then the ratio of the probability of \(H_{t}\) under \(\pi_{e}\) and \(\pi_{b}\), i.e., \(\rho_{0:t}:=\frac{\Pr(H_{t};\pi_{e})}{\Pr(H_{t};\pi_{b})}=\prod_{j=0}^{t}\frac{ \pi_{e}(S_{j},A_{j})}{\pi_{e}(S_{j},A_{j})}\).3 The maximum likelihood estimate (MLE) of \(\mathfrak{R}_{\phi}^{\pi_{e}}\) obtained from the weighted off-policy data is denoted by \(\widehat{\mathfrak{R}}_{n,\phi}^{\pi_{b}\to\pi_{e}}:=\left(\mathcal{Z}, \widehat{\mathfrak{P}}_{n,\phi}^{\pi_{b}\to\pi_{e}},\widehat{\mathfrak{R}}_{n,\phi}^{\pi_{b}\to\pi_{e}},\widehat{\mathfrak{R}}_{n,\phi}^{\pi_{b}\to\pi_{e }}\right)\), where \(\widehat{\mathfrak{R}}_{n,\phi}^{\pi_{b}\to\pi_{e}}(z)=\frac{\sum_{i=1}^{n} \mathbf{1}^{i}\{z_{0}=z\}}{n}\) remains unchanged, and

Footnote 3: The expression for the importance weight can be extended to continuous and hybrid probability measures using Radon-Nikodym derivatives. As with other terms, we hereafter denote the importance weight for the \(i^{\text{th}}\) episode as \(\rho_{0:t}^{0}\).

\[\widehat{\mathfrak{P}}_{n,\phi}^{\pi_{b}\to\pi_{e}}(z,z^{\prime})=\frac{\sum_{ i,t}\mathbf{1}_{t}^{i}\{z,z^{\prime}\}\rho_{0:t}}{\sum_{i,t}\mathbf{1}_{t}^{i}\{z \}\rho_{0:t}},\qquad\widehat{\mathfrak{R}}_{n,\phi}^{\pi_{b}\to\pi_{e}}(z)= \frac{\sum_{i,t}\mathbf{1}_{t}^{i}\{z\}\rho_{0:t}R_{t}^{i}}{\sum_{i,t}\mathbf{ 1}_{t}^{i}\{z\}\rho_{0:t}}.\] (5)

This estimation is a form of weighted maximum likelihood estimation [13]. Including the importance ratios in the numerator and denominator of the estimated transition and reward functions of the ARP enables estimation from off-policy data generated by \(\pi_{b}\). The estimated model of the ARP is consistent and, as shown next, allows for consistent off-policy evaluation.

**Lemma 3.3**.: _Under Assumption 2.1, the weighted maximum likelihood estimate \(\widehat{\mathfrak{R}}_{n,\phi}^{\pi_{b}\to\pi_{e}}\) converges almost surely to the ground-truth ARP \(\mathfrak{R}_{\phi}^{\pi_{e}}\), i.e., \(\widehat{\mathfrak{R}}_{n,\phi}^{\pi_{b}\to\pi_{e}}\stackrel{{ a.s.}}{{ \longrightarrow}}\mathfrak{R}_{\phi}^{\pi_{e}}\)._

Proof.: See Appendix B.3. 

## 4 Off-Policy Evaluation with ARPs

The expected return of \(\widehat{\mathfrak{R}}_{n,\phi}^{\pi_{b}\to\pi_{e}}\) is a consistent estimate of the performance of policy \(\pi_{e}\) since \(\widehat{\mathfrak{R}}_{n,\phi}^{\pi_{b}\to\pi_{e}}\) is an asymptotically correct estimate of \(\mathfrak{R}_{\phi}^{\pi_{e}}\), as per Lemma 3.3.

**Theorem 4.1**.: _The expected return of the ARP \(\widehat{\mathfrak{R}}_{n,\phi}^{\pi_{b}\to\pi_{e}}\) (built from off-policy data) converges almost surely to the expected return of \(\pi_{e}\), i.e., \(J(\pi_{e};\widehat{\mathfrak{R}}_{n,\phi}^{\pi_{b}\to\pi_{e}})\stackrel{{ a.s.}}{{ \longrightarrow}}J(\pi_{e};M)\)._

Proof.: See Appendix B.4. 

To our knowledge, this is the _first_ instance of a model-based OPE method that comes with the theoretical guarantee of consistent performance estimates, even for continuous problems and without model class assumptions. So far, we have achieved one of the starting goals--that of _consistency_. However, the use of importance weights \(\rho_{0:t}\) for weighted MLE is expected to introduce high variance. Next, we discuss methods to mitigate the variance of IS.

### Variance Reduction: Leveraging _Markovness_ of the State Abstraction

A common technique for mitigating the variance of IS-based methods for OPE is clipping the importance weights to the \(c\) most recent ratios [18; 3; 20], i.e., \(\rho_{(t-c+1)^{+}:t}:=\prod_{i=(t-c+1)^{+}}^{t}\frac{\pi_{e}(S_{i},A_{i})}{\pi_ {b}(S_{i},A_{i})}\), where \((t-c+1)^{+}{:=}\max(t-c+1,0)\). This is often a bad approximation for classical IS-based methods, as it implies that only the \(c\) most recent actions affect the reward distribution at any timestep, which rarely holds true in practice. In STAR, importance weights are incorporated into model estimation, resulting in a more reasonable implication of weight clipping.

By importance weighting the abstract-state occurrences as described in Equation (5), clipping importance weights, in this case, implies _that the \(c\) most recent abstract states are sufficient to determine the current abstract-state transition and reward distributions_. This allows actions from the distant past to influence the current reward, as _the effects of actions propagate through the abstract state transitions_, unlike in IS-based methods. This condition, that a recent history of abstract states is sufficient to predict the current abstract state transition distribution, often approximately holdsin practice as discussed in POMDP literature [30; 39]. While the approximation may introduce asymptotic bias in exchange for reduced variance, certain abstraction functions that satisfy specific conditions can incur no asymptotic bias.

Weight Clipping without Asymptotic Bias:Intuitively, the use of \(c\)-clipped importance weights, \(\rho_{(t-c+1)^{+}:t}\), updates the estimated distribution of the previous \(c\) abstract states--as if under the evaluation policy--while leaving the ones before unchanged. \(c\)-clipping does not introduce asymptotic bias when the previous \(c\) abstract states form a sufficient statistic for predicting the current abstract state transition distribution. This notion of conditional independence from history given the recent past is referred to as the _Markovness_ of the abstraction function \(\phi\). We posit that there exist abstraction functions that are \(c\)-th order Markov [51; 9].

**Definition 4.2** (\(c\)-th order Markov).: The abstraction function \(\phi\) is \(c\)-th order Markov if \(\Pr(\phi(S_{t+1})|\phi(S_{t}),\cdots,\phi(S_{(t-c+1)^{+}});\pi)\!=\!\Pr(\phi(S_ {t+1})|\phi(S_{t}),\cdots,\phi(S_{0});\pi)\) for \(\pi\in\{\pi_{b},\pi_{e}\}\).

Let \(\widehat{\mathfrak{H}}_{\phi,c}^{\pi_{b}\to\pi_{e}}\) denote the ARP estimated using \(c\)-clipped importance weights, \(\rho_{(t-c+1)^{+}:t}\), in place of \(\rho_{0:t}\) in Equation (5).

**Theorem 4.3**.: _Given a \(c\)-th order Markov \(\phi\), the expected return of the abstract reward process \(\widehat{\mathfrak{H}}_{\phi,c}^{\pi_{b}+\pi_{e}}\) converges almost surely to the expected return of \(\pi_{e}\), i.e., \(J(\pi_{e};\widehat{\mathfrak{H}}_{\phi,c}^{\pi_{b}\to\pi_{e}})\stackrel{{ \text{a.s.}}}{{\longrightarrow}}J(\pi_{e};M).\)_

Proof.: See Appendix B.5. 

Even when \(\phi\) does not satisfy the above condition, weight clipping proves to be a practical approximation and results in low mean squared prediction error, as demonstrated empirically in Section 5. The steps for performing off-policy evaluation by estimating \(\widehat{\mathfrak{H}}_{\phi,c}^{\pi_{b}+\pi_{e}}\) are highlighted in Algorithm 1.

### Fantastic \(\phi\)'s and Where to Find Them

Discrete abstraction functions that are \(c\)-th order Markov with small values of \(c\) represent the most suitable abstractions for enabling asymptotically correct, low-variance off-policy evaluation using STAR. An automated approach to discovering such abstraction functions, however, remains elusive. In a manner reminiscent of the options framework [50], wherein one might consider the usefulness of options before having methods for constructing options automatically, this work emphasizes the remarkable effectiveness of state abstractions used in abstract reward processes for OPE. It motivates a research area akin to option discovery: _abstraction discovery for OPE_.

We expect the following factors to play an important role in the search for good abstraction functions: (a) state-visitation distributions of \(\pi_{b}\) and \(\pi_{e}\), determining the granularity of abstraction in different parts of the state set, and (b) the distribution shift in abstract state visitation induced by the two policies, determining the extent of weight clipping that can be applied. Both of these are affected by properties of the underlying MDP, in particular the transition function, and in our initial analyses, we observe varying effects of similar abstractions across different MDPs (Appendix C.2).

We observe that a simple approach of randomly initializing centroids and applying \(k\)-means clustering [34; 33], where each cluster denotes a discrete abstract state, results in abstractions that provide competitive OPE performance, often significantly outperforming existing methods. We call this naive clustering-based abstraction method CluSTAR, and use it for our experiments. In some cases, abstraction by aggregation of states can increase the difficulty of estimation of the transition function. For example, aggregation of two states with deterministic transitions--which can be estimated perfectly from a single observation of those transitions--creates stochastic transitions between abstract states. However, in general, state aggregation tends to simplify estimation by increasing the effective sample size [21].

Recovering Existing OPE Methods from STAR:Different configurations of \((\phi,c)\) induce different ARPs, \(\widehat{\mathfrak{H}}_{\phi,c}^{\pi_{b}\to\pi_{e}}\). For certain configurations of \((\phi,c)\):

* \(|\mathcal{Z}|=1\) and no weight clipping: Mapping all states to a single abstract state yields the _weighted per-decision importance sampling_ (WPDIS) estimator [42].
* \(\mathcal{Z}=\mathcal{S}\) and \(c=1\): Amounts to no state abstraction, and yields the maximum likelihood estimate of the MRP over states. The MRP is a combination of the approximate-model estimator [40] that directly estimates the model dynamics with the evaluation policy.

[MISSING_PAGE_FAIL:8]

STAR achieve prediction errors that are an order of magnitude lower than the best baseline. These results in Figure 3 emphasize that _highly compact ARPs that distill complex sequential processes are particularly effective for off-policy evaluation_. The best ARP in STAR for each domain abstracts: (a) the continuous state space of CartPole to \(|\mathcal{Z}|=32\) abstract states, (b) the 747 states of ICU-Sepsis to \(|\mathcal{Z}|=16\) abstract states, and (c) the 400 states of Asterix to \(|\mathcal{Z}|=8\) abstract states, to construct compact finite ARPs for OPE. Furthermore, the horizon length of CartPole is 50, and episodes in ICU-Sepsis and Asterix go up to 120 and 75 timesteps respectively. Such relatively long horizons have proven to be challenging for prior OPE methods. STAR leverages ARPs to enable OPE at scales commonly seen in practice.

## 6 Related Work

The problem of off-policy evaluation (OPE) has been extensively studied due to its relevance for practical applications of reinforcement learning [38; 35]. Extensive surveys on the topic, both theoretical [56] and empirical [58; 14] delineate the numerous approaches to the problem. Model-based approaches for OPE have proven effective [32; 62] but are restricted by the model class used. In this work, we use state abstraction to define compact models called abstract reward processes, and demonstrate their effectiveness as off-policy estimators. Historically, state abstraction research has focused on grouping similar states in a way that does not change the essence of the underlying problem [28; 43; 1], to reduce the complexity of the problem. However, the use of state abstractions for OPE remains under-explored. Pavse and Hanna [41] show that the use of state abstraction with marginalized importance sampling achieves variance reduction in high-dimensional state spaces, but their approach does not use abstraction to construct models. Jiang et al. [23] study abstraction selection for model-based RL, balancing model complexity and policy value suboptimality. Abstraction discovery or

Figure 3: Mean squared prediction errors of best and median ARPs from STAR compared against existing OPE methods. The empirically estimated bias-variance decomposition of the error is shown. The results are averaged over 200 trials, with error bars indicating standard error. Note: For ICU-Sepsis, regression-based methods (MRDR and Q-Reg) were computationally intractable due to the large state set, as the corresponding Weighted Least Squares methods for regression were too slow. In all domains and across all datasizes, the best ARP in STAR outperforms baselines in all cases, and the even the median estimator does so in 7 out of 12 cases.

learning has focussed on discretization of continuous state spaces to reduce problem complexity [47], and distilling the Markov features [2] or reward relevant features [12, 59].

## 7 Discussion and Conclusion

In this paper, we have introduced a new framework for consistent model-based off-policy evaluation. This framework leverages state abstraction to prevent model class mismatch along with importance sampling to consistently learn models from off-policy data. Unlike traditional model-based methods, our approach eliminates the need for model class assumptions and provides theoretical guarantees for the obtained performance estimates. Moreover, using state abstraction increases the effective sample size [21], which is particularly beneficial in limited data regimes. Importantly, this work presents a framework with a new approach to OPE, rather than a specific new method. Estimators that lie within this framework significantly outperform existing OPE methods, with the best estimator consistently outperforming all baselines, as demonstrated in our empirical evaluation.

The framework has two main limitations: it requires knowledge of the probabilities of observed actions under the behavior policy, which may not always be available, and a principled method for selecting well-performing configurations of the abstraction function and the weight clipping factor remain elusive. Combining this work with regression IS [19] would be a practical extension that addresses the first limitation. Additionally, a data-driven approach to automated estimator selection based on characteristics of the domain, dataset sizes, and other factors, as suggested by Su et al. [48], would enhance its practical application.

Our findings indicate that even a simple class of abstraction functions can provide competitive OPE performance. We theoretically demonstrate the existence of certain abstraction functions that may offer better performance. Investigating the properties of abstraction functions and developing automated approaches to _abstraction discovery_ for ARPs are promising directions for future work on creating high-performing OPE methods.

AcknowledgementsWe thank Yash Chandak, Mohammad Ghavamzadeh and Dhawal Gupta for their helpful discussions and feedback on this work. We would also like to thank Josiah Hanna, Bo Liu, Cameron Allen, and the anonymous reviewers for their feedback.

## References

* [1] David Abel. A theory of state abstraction for reinforcement learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 9876-9877, 2019.
* [2] Cameron Allen, Neev Parikh, Omer Gottesman, and George Konidaris. Learning Markov state abstractions for deep reinforcement learning. _Advances in Neural Information Processing Systems_, 34:8229-8241, 2021.
* [3] Oliver Bembom and Mark J van der Laan. Data-adaptive selection of the truncation level for inverse-probability-of-treatment-weighted estimators. 2008.
* [4] Patrick Billingsley. _Convergence of Probability Measures_. John Wiley & Sons, 2013.
* [5] Leon Bottou, Jonas Peters, Joaquin Quinonero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. _Journal of Machine Learning Research_, 14(11), 2013.
* [6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI gym. _arXiv Preprint arXiv:1606.01540_, 2016.
* [7] Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. _Advances in Neural Information Processing Systems_, 24, 2011.
* [8] Kartik Choudhary, Dhawal Gupta, and Philip S. Thomas. ICU-Sepsis: A benchmark MDP built from real medical data, 2024.

* Efroni et al. [2022] Yonathan Efroni, Chi Jin, Akshay Krishnamurthy, and Sobhan Miryoosefi. Provable reinforcement learning with a short-term memory. In _Proceedings of the International Conference on Machine Learning_, pages 5832-5850. PMLR, 2022.
* Farahmand and Szepesvari [2011] Amir-Massoud Farahmand and Csaba Szepesvari. Model selection in reinforcement learning. _Machine Learning_, 85(3):299-332, 2011.
* Farajtabar et al. [2018] Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust off-policy evaluation. In _Proceedings of the International Conference on Machine Learning_, pages 1447-1456. PMLR, 2018.
* Ferns et al. [2012] Norman Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite Markov decision processes. _arXiv Preprint arXiv:1207.4114_, 2012.
* Field and Smith [1994] Chris Field and B Smith. Robust estimation: A weighted maximum likelihood approach. _International Statistical Review/Revue Internationale de Statistique_, pages 405-424, 1994.
* Fu et al. [2021] Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael R Zhang, Yutian Chen, Aviral Kumar, et al. Benchmarks for deep off-policy evaluation. _arXiv Preprint arXiv:2103.16596_, 2021.
* Gao et al. [2023] Ge Gao, Song Ju, Markel Sanz Ausin, and Min Chi. Hope: Human-centric off-policy evaluation for e-learning and healthcare. _arXiv Preprint arXiv:2302.09212_, 2023.
* Evan-RF Gariepy and Ronald [2015] LC Evans-RF Gariepy and F Ronald. _Measure theory and fine properties of functions, Revised edition_. CRC Press, 2015.
* Guo et al. [2017] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International conference on machine learning_, pages 1321-1330. PMLR, 2017.
* Guo et al. [2017] Zhaohan Guo, Philip S Thomas, and Emma Brunskill. Using options and covariance testing for long horizon off-policy policy evaluation. _Advances in Neural Information Processing Systems_, 30, 2017.
* Hanna et al. [2019] Josiah Hanna, Scott Niekum, and Peter Stone. Importance sampling policy evaluation with an estimated behavior policy. In _Proceedings of the International Conference on Machine Learning_, pages 2605-2613. PMLR, 2019.
* Ionides [2008] Edward L Ionides. Truncated importance sampling. _Journal of Computational and Graphical Statistics_, 17(2):295-311, 2008.
* Jiang [2018] Nan Jiang. Notes on state abstractions, 2018. URL https://nanjiang.cs.illinois.edu/files/cs598/note4.pdf.
* Jiang and Li [2016] Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In _Proceedings of the International Conference on Machine Learning_, pages 652-661. PMLR, 2016.
* Jiang et al. [2015] Nan Jiang, Alex Kulesza, and Satinder Singh. Abstraction selection in model-based reinforcement learning. In _Proceedings of the International Conference on Machine Learning_, pages 179-188. PMLR, 2015.
* Johnson et al. [2016] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. MIMIC-III, a freely accessible critical care database. _Scientific Data_, 3(1):1-9, 2016.
* Kahn and Harris [1951] Herman Kahn and Theodore E Harris. Estimation of particle transmission by random sampling. _National Bureau of Standards Applied Mathematics Series_, 12:27-30, 1951.
* Komorowski et al. [2018] Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. _Nature Medicine_, 24(11):1716-1720, 2018.

* [27] Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In _Proceedings of the International Conference on Machine Learning_, pages 3703-3712. PMLR, 2019.
* [28] Lihong Li, Thomas J Walsh, and Michael L Littman. Towards a unified theory of state abstraction for MDPs. _Artificial Intelligence and Machine Learning (AI&M)_, 1(2):3, 2006.
* [29] Lihong Li, Remi Munos, and Csaba Szepesvari. Toward minimax off-policy value estimation. In _Proceedings of the Artificial Intelligence and Statistics Conference_, pages 608-616. PMLR, 2015.
* [30] Michael Littman and Richard S. Sutton. Predictive representations of state. _Advances in Neural Information Processing Systems_, 14, 2001.
* [31] Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-horizon off-policy estimation. _Advances in Neural Information Processing Systems_, 31, 2018.
* [32] Yao Liu, Omer Gottesman, Aniruddh Raghu, Matthieu Komorowski, Aldo A Faisal, Finale Doshi-Velez, and Emma Brunskill. Representation balancing MDPs for off-policy policy evaluation. _Advances in Neural Information Processing Systems_, 31, 2018.
* [33] Stuart Lloyd. Least Squares Quantization in PCM. _IEEE Transactions on Information Theory_, 28(2):129-137, 1982.
* [34] James MacQueen. Some methods for classification and analysis of multivariate observations. In _Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability_, volume 1, pages 281-297, Oakland, CA, USA, 1967. University of California Press.
* [35] Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. Offline policy evaluation across representations with applications to educational games. In _Proceedings of the International Conference on Autonomous Agents and Multiagent Systems_, volume 1077, 2014.
* [36] Vukosi N Marivate. _Improved Empirical Methods in Reinforcement-Learning Evaluation_. Rutgers The State University of New Jersey, School of Graduate Studies, 2015.
* [37] Andrei Andreevich Markov. The theory of algorithms. _Trudy Matematicheskogo Instituta Imeni VA Steklova_, 42:3-375, 1954.
* [38] Susan A Murphy, Mark J van der Laan, James M Robins, and Conduct Problems Prevention Research Group. Marginal mean models for dynamic regimes. _Journal of the American Statistical Association_, 96(456):1410-1423, 2001.
* [39] Khanh Xuan Nguyen. Converting POMDPs into MDPs using history representation. _Engineering Archive_, 2021.
* [40] Cosmin Paduraru. _Off-Policy Evaluation in Markov Decision Processes_. PhD thesis, 2013.
* [41] Brahma S Pavse and Josiah P Hanna. Scaling marginalized importance sampling to high-dimensional state-spaces via state abstraction. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 9417-9425, 2023.
* [42] Doina Precup. Eligibility traces for off-policy policy evaluation. _Computer Science Department Faculty Publication Series_, page 80, 2000.
* [43] Balaraman Ravindran. _An Algebraic Approach to Abstraction in Reinforcement Learning_. University of Massachusetts Amherst, 2004.
* [44] Reuven Y Rubinstein and Dirk P Kroese. _Simulation and the Monte Carlo Method_. John Wiley & Sons, 2016.
* [45] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _Proceedings of the International Conference on Machine Learning_, pages 1889-1897. PMLR, 2015.

* [46] Pranab K Sen and Julio M Singer. _Large Sample Methods in Statistics: An Introduction with Applications_, volume 25. CRC Press, 1994.
* [47] Sean R Sinclair, Siddhartha Banerjee, and Christina Lee Yu. Adaptive discretization for episodic reinforcement learning in metric spaces. _Proceedings of the ACM on Measurement and Analysis of Computing Systems_, 3(3):1-44, 2019.
* [48] Yi Su, Pavithra Srinath, and Akshay Krishnamurthy. Adaptive estimator selection for off-policy evaluation. In _Proceedings of the International Conference on Machine Learning_, pages 9196-9205. PMLR, 2020.
* [49] Richard S Sutton and Andrew G Barto. _Reinforcement Learning: An Introduction_. MIT Press, 2018.
* [50] Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. _Artificial Intelligence_, 112(1-2):181-211, 1999.
* [51] Erik N Talvitie. _Simple Partial Models for Complex Dynamical Systems_. PhD thesis, University of Michigan, 2010.
* [52] Shengpu Tang and Jenna Wiens. Model selection for offline reinforcement learning: Practical considerations for healthcare settings. In _Proceedings of the Machine Learning for Healthcare Conference_, pages 2-35. PMLR, 2021.
* [53] Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In _Proceedings of the International Conference on Machine Learning_, pages 2139-2148. PMLR, 2016.
* [54] Edward C Tolman. Cognitive maps in rats and men. _Psychological Review_, 55(4):189, 1948.
* [55] Takuma Udagawa, Haruka Kiyohara, Yusuke Narita, Yuta Saito, and Kei Tateno. Policy-adaptive estimator selection for off-policy evaluation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 10025-10033, 2023.
* [56] Masatoshi Uehara, Chengchun Shi, and Nathan Kallus. A review of off-policy evaluation in reinforcement learning. _arXiv Preprint arXiv:2212.06355_, 2022.
* [57] Aad W Van der Vaart. _Asymptotic Statistics_, volume 3. Cambridge University Press, 2000.
* [58] Cameron Voloshin, Hoang M Le, Nan Jiang, and Yisong Yue. Empirical study of off-policy policy evaluation for reinforcement learning. _arXiv Preprint arXiv:1911.06854_, 2019.
* [59] Tongzhou Wang, Simon S Du, Antonio Torralba, Phillip Isola, Amy Zhang, and Yuandong Tian. Denoised MDPs: Learning world models better than the world itself. _arXiv Preprint arXiv:2206.15477_, 2022.
* [60] Neil A. Weiss. _A Course in Probability_. Addison-Wesley, Boston, 2005. ISBN 0-321-18954-X.
* [61] Kenny Young and Tian Tian. MinAtar: An Atari-inspired testbed for thorough and reproducible reinforcement learning experiments. _arXiv Preprint arXiv:1903.03176_, 2019.
* [62] Michael R Zhang, Tom Le Paine, Ofir Nachum, Cosmin Paduraru, George Tucker, Ziyu Wang, and Mohammad Norouzi. Autoregressive dynamics models for offline policy evaluation and optimization. _arXiv Preprint arXiv:2104.13877_, 2021.
* [63] Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized offline estimation of stationary values. _arXiv Preprint arXiv:2002.09072_, 2020.
* [64] Daniel Zwillinger and Stephen Kokoska. _CRC Standard Probability and Statistics Tables and Formulae_. CRC Press, 1999.

Preliminaries

Consistency and Almost Sure ConvergenceLet \(\hat{\theta}_{n}\) denote the estimator of a statistic \(\theta\), estimated from \(n\) data points. As the amount of data approaches infinity, i.e., as \(n\rightarrow\infty\), the estimator is said to _converge almost surely_ to \(\theta\) if and only if

\[\Pr\Big{(}\lim_{n\rightarrow\infty}\hat{\theta}_{n}=\theta\Big{)}=1.\]

We write \(\theta_{n}\xrightarrow{\text{a.s.}}\theta\) to denote that the sequence \(\theta_{n}\) converges almost surely to \(\theta\). An estimator that converges almost surely to the true value of the statistic is said to be (strongly) **consistent**.

Continuous Mapping TheoremThe continuous mapping theorem [4, 57] states that if a sequence of random variables \((X_{i})_{i=1}^{n}\) converges almost surely to a random variable \(X\), a function \(f\) has discontinuity points \(D_{f}\), and \(\Pr(X\in D_{f})=0\), then the sequence \((f(X_{i}))_{i=1}^{n}\) converges almost surely to \(f(X)\).

Tower RuleFor random variables \(X\) and \(Y\) defined on the same probability space, the tower rule states that the expected value of the conditional expected value of \(X\) given \(Y\) is the same as the expected value of \(X\), i.e.,

\[\mathbb{E}\left[X\right]=\mathbb{E}\left[\mathbb{E}\left[X\mid Y\right]\right].\]

### Additional Notes on MRPs and ARPs

Here we provide additional details regarding the definitions of MDPs and MRPs. Specifically, we show how the expression of the transition and reward functions of the MRP can be simplified when \(\mathcal{X}=\mathcal{S}\) and discuss how to handle termination in ARPs that are derived from finite-horizon MDPs.

#### a.1.1 Simplification of Equation 2

The expressions for the components of a Markov reward process (MRP) can be simplified by using the Markov property. The transition function simplifies as:

\[p^{\pi}(x,x^{\prime})= \frac{\sum_{t}\Pr(S_{t+1}=x^{\prime},S_{t}=x;\pi)}{\sum_{t}\Pr(S_ {t}=x;\pi)}\] \[= \frac{\sum_{t}\Pr(S_{t+1}=x^{\prime}\mid S_{t}=x;\pi)\Pr(S_{t}=x ;\pi)}{\sum_{t}\Pr(S_{t}=x;\pi)}\] \[= \frac{\sum_{t}\sum_{a\in\mathcal{A}}\Pr(S_{t+1}=x^{\prime}\mid S _{t}=x,A_{t}=a;\pi)\Pr(A_{t}=a|S_{t}=x)\Pr(S_{t}=x;\pi)}{\sum_{t}\Pr(S_{t}=x; \pi)}\] \[= \frac{\sum_{t}\sum_{a\in\mathcal{A}}p(x,a,x^{\prime})\pi(x,a)\Pr (S_{t}=x;\pi)}{\sum_{t}\Pr(S_{t}=x;\pi)}\] \[= \frac{\sum_{a\in\mathcal{A}}p(x,a,x^{\prime})\pi(x,a)\left(\sum_ {t}\Pr(S_{t}=x;\pi)\right)}{\sum_{t}\Pr(S_{t}=x;\pi)}\] \[= \sum_{a\in\mathcal{A}}p(x,a,x^{\prime})\pi(x,a).\]

Similarly, the reward function simplifies as:

\[r^{\pi}(x)= \frac{\sum_{t}\mathbb{E}\left[R_{t}|S_{t}=x;\pi\right]\Pr(S_{t} =x;\pi)}{\sum_{t}\Pr(S_{t}=x;\pi)}\] \[= \frac{\sum_{t}\sum_{a\in\mathcal{A}}r(x,a)\pi(x,a)\Pr(S_{t}=x; \pi)}{\sum_{t}\Pr(S_{t}=x;\pi)}\] \[= \frac{\sum_{a\in\mathcal{A}}r(x,a)\pi(x,a)\left(\sum_{t}\Pr(S_{t }=x;\pi)\right)}{\sum_{t}\Pr(S_{t}=x;\pi)}\] \[= \sum_{a\in\mathcal{A}}r(x,a)\pi(x,a).\]

#### a.1.2 Handling termination in an MDPs, MRPs, and ARPs

Each episode in a finite-horizon MDP concludes by some timestep \(T\in\mathbb{N}\), referred to as the _termination_ of that episode. In practice, there are two common ways of modeling termination of episodes in finite-horizon MDPs: 1) including an absorbing state \(s_{\infty}\) in the state set, or 2) introducing a termination function \(\beta:\mathcal{S}\to[0,1]\) representing the probability of termination of an episode from each state, i.e., \(\beta(s)\coloneqq\Pr(\text{terminate}|S_{t}=s)\). In the first case, by timestep \(T\) the process transitions into the absorbing \(s_{\infty}\) after which it continually transitions back to \(s_{\infty}\) getting a reward of zero. In the second case, after timestep \(T\) the process stops and there are no subsequent samples. Expressions involving sums over samples in an episode are denoted correspondingly, for example, the expected return \(J(\pi)\coloneqq\mathbb{E}[\sum_{t=1}^{\infty}R_{t};\pi]\) in the first case, and \(J(\pi)\coloneqq\mathbb{E}[\sum_{t=1}^{T}R_{t};\pi]\) in the second. Both approaches are mathematically equivalent and correspondingly, MRPs induced by the combination of finite-horizon MDPs with a policy can model termination in either way.

However, termination in ARPs requires special attention. The introduction of an absorbing abstract state \(z_{\infty}\) places a condition on the abstraction function \(\phi\)--it requires the abstraction function to map _only_\(s_{\infty}\), and no other state, to \(z_{\infty}\). In this work, we _implement_ termination in code by estimating abstract termination functions that represent the probability of termination from each abstract state. Denote the abstract termination function of an ARP by \(\beta_{\phi}^{\pi}:\mathcal{Z}\to[0,1]\). It must be noted that both approaches continue to be mathematically equivalent. The probability of transitioning from any abstract state \(z\) to \(z_{\infty}\) is the same as the the probability of termination from that state, i.e., \(\beta_{\phi}^{\pi}(z)=\mathrm{P}_{\phi}^{\pi}(z,z_{\infty})\). Note that

\[\mathrm{P}_{\phi}^{\pi}(z_{\infty},z)=\begin{cases}0&\text{if }z\neq z_{\infty} \\ 1&\text{if }z=z_{\infty}.\end{cases}\]

Similarly, the reward function is zero for the absorbing abstract state: \(\mathrm{R}_{\phi}^{\pi}(z_{\infty})=0\). It is mathematically more succint to model termination with \(z_{\infty}\), and we use this (equivalent) approach in our theoretical analysis.

In our implementation, \(z_{\infty}\) is not defined and thus the components of \(\mathfrak{R}_{\phi}\) do not take \(z_{\infty}\) as input. The equivalant form of the expected return of an ARP, that models a termination function, is then given by:

\[J(\pi;\mathfrak{R}_{\phi}^{\pi}) =\mathrm{R}_{\phi}^{\pi}\eta_{\phi}+\left(\mathrm{I}-\text{diag} (\beta_{\phi}^{\pi})\right)\left[\mathrm{P}_{\phi}^{\pi}\mathrm{R}_{\phi}^{ \pi}\eta_{\phi}+\left(\mathrm{I}-\text{diag}(\beta_{\phi}^{\pi})\right)\left[ \mathrm{P}_{\phi}^{\pi}\right)^{2}\mathrm{R}_{\phi}^{\pi}\eta_{\phi}+\ldots \right]\] \[=\mathrm{R}_{\phi}^{\pi}\eta_{\phi}+\left(\mathrm{I}-\text{diag} (\beta_{\phi}^{\pi})\right)\mathrm{P}_{\phi}^{\pi}\mathrm{R}_{\phi}^{\pi} \eta_{\phi}+\left(\mathrm{I}-\text{diag}(\beta_{\phi}^{\pi})\right)^{2} \mathrm{P}_{\phi}^{\pi}\eta_{\phi}+\ldots\] \[=\sum_{k=0}^{\infty}\left(\left(\mathrm{I}-\text{diag}(\beta_{ \phi}^{\pi})\right)\mathrm{P}_{\phi}^{\pi}\right)^{k}\mathrm{R}_{\phi}^{\pi} \eta_{\phi}.\]

Correspondingly, the closed form expression for the expected return that accounts for the termination function is given by,

\[J(\pi;\mathfrak{R}_{\phi}^{\pi})=\left(\mathrm{I}-\left(\mathrm{I}-\text{diag }(\beta_{\phi}^{\pi})\right)\mathrm{P}_{\phi}^{\pi}\right)^{-1}\mathrm{R}_{ \phi}^{\pi}(z)\eta_{\phi}.\] (6)

Note that \(\left(\mathrm{I}-\left(\mathrm{I}-\text{diag}(\beta_{\phi}^{\pi})\right) \mathrm{P}_{\phi}^{\pi}\right)\) is an invertible matrix, as the left hand side of the expression, \(J(\pi;\mathfrak{R}_{\phi}^{\pi})\), is equal to the expected return \(J(\pi;M)\) of \(\pi\) by Theorem 3.1, which is bounded.

Proofs of Theoretical Results

In this section, we provide proofs of the theorems and lemmas in the main text. We start with Theorem 3.1 that states that ARPs are _performance-preserving_.

### ARPs are Performance Preserving

**Theorem 3.1**.: \(\forall\;\phi\in\Phi\)_, the performance of a policy \(\pi\) is equal to the expected return of the abstract reward process \(\mathfrak{R}_{\phi}^{\pi}\) defined from MDP \(M\), i.e., \(J(\pi;\mathfrak{R}_{\phi}^{\pi})=J(\pi;M)\)._

Proof.: This result states that despite the use of a discrete state abstraction, the expected return of the ARP \(\mathfrak{R}_{\phi}^{\pi}\) is equal to the performance of policy \(\pi\). To prove this, we leverage two equivalent forms of defining the expected return of a policy. The first form:

\[J(\pi;\mathfrak{R}_{\phi}^{\pi})=\sum_{t}\mathbb{E}[R_{t};\pi],\] (7)

defines the expected return as a sum of the expected value of the reward at each timestep, where the distribution of rewards at each timestep is governed by the components of the ARP, namely, \(\eta_{\phi},\mathrm{P}_{\phi}^{\pi}\) and \(\mathrm{R}_{\phi}^{\pi}\). The second, equivalent, form of expressing the expected return is given by:

\[J(\pi;\mathfrak{R}_{\phi}^{\pi})=\sum_{t}\sum_{z\in\mathcal{Z}}\Pr(Z_{t}=z; \pi)\mathbb{E}[R_{t}|Z_{t}{=}z;\pi]=\sum_{t}\sum_{z\in\mathcal{Z}}\Pr(Z_{t}=z; \pi)\mathrm{R}_{\phi}^{\pi}(z),\] (8)

where the expected return is denoted as a sum of the reward at each abstract state mutliplied by the visitation frequency of that abstract state. Both Equations 7 and 8 are equivalent, i.e.,

\[J(\pi;\mathfrak{R}_{\phi}^{\pi})=\sum_{t}\mathbb{E}[R_{t};\pi]=\sum_{t}\sum_{ z\in\mathcal{Z}}\Pr(Z_{t}=z;\pi)\mathrm{R}_{\phi}^{\pi}(z).\] (9)

Next, we denote the terms in Equation 8 in terms of the states of the underlying MDP to show the final result. The reward function \(\mathrm{R}_{\phi}^{\pi}\) can be expressed as,

\[\mathrm{R}_{\phi}^{\pi}(z):= \frac{\sum_{t}\mathbb{E}[R_{t}|\phi(S_{t}){=}z;\pi]\Pr(\phi(S_{t} ){=}z;\pi)}{\sum_{t}\Pr(\phi(S_{t}){=}z;\pi)}\] \[= \frac{\sum_{t}\sum_{s\in\mathcal{S}}\mathbf{1}\{\phi(s)=z\} \mathbb{E}[R_{t}|S_{t}{=}s;\pi]\Pr(S_{t}{=}s;\pi)}{\sum_{t}\sum_{s\in\mathcal{ S}}\mathbf{1}\{\phi(s)=z\}\Pr(S_{t}{=}s;\pi)}\] \[= \frac{\sum_{t}\sum_{s\in\mathcal{S},a\in\mathcal{A}}\mathbf{1}\{ \phi(s)=z\}r(s,a)\pi(s,a)\Pr(S_{t}{=}s;\pi)}{\sum_{t}\sum_{s\in\mathcal{S}} \mathbf{1}\{\phi(s)=z\}\Pr(S_{t}{=}s;\pi)}\] \[= \frac{\sum_{s\in\mathcal{S},a\in\mathcal{A}}\mathbf{1}\{\phi(s)=z \}r(s,a)\pi(s,a)(\sum_{\int}\Pr(S_{t}{=}s;\pi))}{\sum_{s\in\mathcal{S}} \mathbf{1}\{\phi(s)=z\}(\sum_{\int}\Pr(S_{t}{=}s;\pi))}\] \[= \frac{\sum_{s\in\mathcal{S},a\in\mathcal{A}}\mathbf{1}\{\phi(s)=z \}(\sum_{t}\Pr(S_{t}{=}s;\pi))}{\sum_{s\in\mathcal{S}}\psi^{\pi}(s)\mathbf{1 }\{\phi(s)=z\}},\]

where the undiscounted state distribution [49, Section 9.2] under policy \(\pi\) is denoted by \(\psi^{\pi}(s)\propto\sum_{t}\Pr(S_{t}=s;\pi)\), where \(\psi^{\pi}(s_{\infty})\) is set to be proportional to any constant value. The normalization constant, \(\kappa\), that makes \(\psi^{\pi}(s)\) a valid distribution cancels out from both the numerator and the denominator of the above expression. The term \(\Pr(Z_{t}=z;\pi)\) can be expressed in terms of states as,

\[\Pr(Z_{t}=z;\pi)=\sum_{s\in\mathcal{S}}\Pr(S_{t}=s;\pi)\mathbf{1}\{\phi(s)=z\}.\]Substituting these expressions into Equation 8 gives,

\[J(\pi;\mathfrak{R}^{\pi}_{\phi}) =\sum_{t}\sum_{z\in\mathcal{Z}}\Pr(Z_{t}=z;\pi)\mathds{R}^{\pi}_{ \phi}(z)\] \[=\sum_{t}\sum_{z\in\mathcal{Z}}\left(\sum_{s\in\mathcal{S}}\Pr(S_{ t}=s;\pi)\mathbf{1}\{\phi(s)=z\}\right)\left(\frac{\sum_{s\in\mathcal{S},a\in \mathcal{A}}\psi^{\pi}(s)\pi(s,a)r(s,a)\mathbf{1}\{\phi(s)=z\}}{\sum_{s\in \mathcal{S}}\psi^{\pi}(s)\mathbf{1}\{\phi(s)=z\}}\right)\] \[=\sum_{z\in\mathcal{Z}}\left(\sum_{s\in\mathcal{S}}\sum_{t}\Pr(S_ {t}=s;\pi)\mathbf{1}\{\phi(s)=z\}\right)\left(\frac{\sum_{s\in\mathcal{S},a\in \mathcal{A}}\psi^{\pi}(s)\pi(s,a)r(s,a)\mathbf{1}\{\phi(s)=z\}}{\sum_{s\in \mathcal{S}}\psi^{\pi}(s)\mathbf{1}\{\phi(s)=z\}}\right)\] \[=\sum_{z\in\mathcal{Z}}\left(\sum_{s\in\mathcal{S}}\kappa\;\psi^ {\pi}(s)\mathbf{1}\{\phi(s)=z\}\right)\left(\frac{\sum_{s\in\mathcal{S},a\in \mathcal{A}}\psi^{\pi}(s)\pi(s,a)r(s,a)\mathbf{1}\{\phi(s)=z\}}{\sum_{s\in \mathcal{S}}\psi^{\pi}(s)\mathbf{1}\{\phi(s)=z\}}\right)\] \[=\sum_{z\in\mathcal{Z}}\sum_{s\in\mathcal{S},a\in\mathcal{A}}\kappa \;\psi^{\pi}(s)\pi(s,a)r(s,a)\mathbf{1}\{\phi(s)=z\}\] \[\stackrel{{(a)}}{{=}}s\sum_{s\in\mathcal{S},a\in \mathcal{A}}\sum_{t}\Pr(S_{t}=s;\pi)\pi(s,a)r(s,a)\] \[=J(\pi;M),\]

where (a) follows from the law of total probability [64].

We have established that ARPs are performance-preserving. Next, we prove Lemma 3.2, which states that the performance estimate obtained from an estimated model of the ARP converges almost surely to the performance of the policy that induces it. In order to do so, we first introduce properties that will be useful in the proof of Lemma 3.2.

### Estimation of ARPs from On-Policy Data

At a high level, we prove Lemma 3.2 by first studying the almost sure convergence of the estimated transition function, reward function, and initial state distributions. Once the almost sure convergence of these components has been established, we reason about the implications for the convergence of the policy performance predictions that result from these terms. We begin by studying the almost sure convergence of the transition function.

**Property B.2**.: _For all abstract states \(z\in\mathcal{Z}\) and \(z^{\prime}\in\mathcal{Z}\), if_

\[\sum_{t}\Pr(Z_{t}=z;\pi)\neq 0,\]

_then_

\[\widehat{\mathrm{P}}^{\pi}_{n,\phi}(z,z^{\prime})\stackrel{{ a.s}}{{\longrightarrow}}\mathrm{P}^{\pi}_{\phi}(z,z^{\prime}).\] (10)

Proof.: Recall that

\[\widehat{\mathrm{P}}^{\pi}_{n,\phi}(z,z^{\prime})=\frac{\sum_{i=1}^{n}\sum_{t} \mathbf{1}\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\prime}\}}{\sum_{i=1}^{n}\sum_{t} \mathbf{1}\{Z_{t}^{i}=z\}}.\]

Let \(X_{n}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\sum_{t}\mathbf{1}\{Z_{t}^{i}=z,Z_{t+1 }^{i}=z^{\prime}\}\) and \(Y_{n}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\sum_{t}\mathbf{1}\{Z_{t}^{i}=z\}\). We can then rewrite \(\widehat{\mathrm{P}}^{\pi}_{n,\phi}(z,z^{\prime})\) as

\[\widehat{\mathrm{P}}^{\pi}_{n,\phi}(z,z^{\prime})=\frac{X_{n}}{Y_{n}},\]

which is a continuous function of \(X_{n}\) and \(Y_{n}\) when \(Y_{n}>0\). At a high level, we will show what \(X_{n}\) and \(Y_{n}\) each converge to almost surely, and will then apply the continuous mapping theorem to reason about the almost sure convergence of \(\widehat{\mathrm{P}}^{\pi}_{n,\phi}(z,z^{\prime})\).

However, there may be some abstract states \(\tilde{Z}\subset\mathcal{Z}\) that are not reached and thus are not observed in the data. Such abstract states pose a problem: If \(\tilde{z}\in\tilde{Z}\), then \(Y_{n}\) for \(\widehat{\mathrm{P}}^{\pi}_{n,\phi}(\tilde{z},z^{\prime})\) will be zero,and so \(\widehat{\mathrm{P}}^{\pi}_{n,\phi}(\tilde{z},z^{\prime})\) will be undefined and not a continuous function of \(X_{n}\) and \(Y_{n}\) (which must be handled appropriately in the proof of consistency). First, to ensure that the ARP is well-defined even when \(\tilde{Z}\) is not empty, for abstract states \(\tilde{z}\in\tilde{Z}\) (abstract states that were not observed in the data), special values can be hardcoded into the transition function, reward functions, and initial state distribution so that the components of the ARP continue to be well-defined.4 In particular, for \(\tilde{z}\in\tilde{Z}\), the transition function can be set to self-transition back to \(\tilde{z}\) with probability 1, i.e., \(\widehat{\mathrm{P}}^{\pi}_{n,\phi}(\tilde{z},\tilde{z})=1\) and \(\widehat{\mathrm{P}}^{\pi}_{n,\phi}(\tilde{z},\tilde{z})=0\) for all \(\tilde{z}\notin\tilde{Z}\), the reward function is set to \(\widehat{\mathrm{R}}^{\pi}_{n,\phi}(\tilde{z})=0\) and the initial abstract state distribution is \(\widehat{\eta}^{\pi}_{n,\phi}(\tilde{z})=0\) for all \(\tilde{z}\in\tilde{Z}\).5

Footnote 4: The particular behavior of the ARP in these unobserved abstract states is not of consequence. Since they are unreached, the behavior in these abstract states does not impact the expected return of the ARP. However, we wish to ensure that the transition function, reward function, and initial state distribution still represent a well-defined ARP.

Footnote 5: Note that the initial state distribution is not actually a problem like the transition and reward functions—the previous definitions already ensured that \(\widetilde{\eta}^{\pi}_{n,\phi}(\tilde{z})=0\).

To reason about the almost sure convergence of \(\widehat{\mathrm{P}}^{\pi}_{n,\phi}(z,z^{\prime})\), first consider \(\lim_{n\to\infty}X_{n}\),

\[\lim_{n\to\infty}X_{n} =\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}\sum_{t}\mathbf{1}\{Z_ {t}^{i}=z,Z_{t+1}^{i}=z^{\prime}\}\] \[=\lim_{n\to\infty}\sum_{t}\left(\frac{1}{n}\sum_{i=1}^{n}\mathbf{ 1}\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\prime}\}\right).\]

Note that \(\mathbf{1}\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\prime}\}\) is independent across episodes (for each \(i\)), has bounded variance, and has the same mean for each episode. By Kolmogorov's strong law of large numbers [46],

\[\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\prime}\} \xrightarrow{\mathrm{a.s.}}\mathbb{E}\left[\mathbf{1}\{Z_{t}=z,Z_{t+1}=z^{ \prime}\};\pi\right],\]

This convergence guarantee holds for all \(t\). From the definition of almost sure convergence, this means that

\[\forall t\in\mathbb{N},\,\Pr\left(\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n} \mathbf{1}\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\prime}\}=\mathbb{E}\left[\mathbf{1}\{ Z_{t}=z,Z_{t+1}=z^{\prime}\};\pi\right]\right)=1,\] (11)

where the expectation results from the use of the policy \(\pi\) that generated dataset \(\mathcal{D}_{n}^{(\pi)}\). By the countable additivity property of probability measures (and the fact that the number of time steps is countable), this implies the following (notice that \(\forall t\in\mathbb{N}\) is now inside the statement of probability):

\[\Pr\left(\forall t\in\mathbb{N},\,\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n} \mathbf{1}\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\prime}\}=\mathbb{E}\left[\mathbf{1}\{ Z_{t}=z,Z_{t+1}=z^{\prime}\};\pi\right]\right)=1.\] (12)

Hence,

\[\Pr\left(\sum_{t}\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}\{Z_{t}^ {i}=z,Z_{t+1}^{i}=z^{\prime}\}=\sum_{t}\mathbb{E}\left[\mathbf{1}\{Z_{t}=z,Z_{ t+1}=z^{\prime}\};\pi\right]\right)=1.\] (13)

Next, by the dominated convergence theorem [16, Theorem 1.19], the summation over time and limit over \(n\) can be interchanged, giving:

\[\Pr\left(\lim_{n\to\infty}\sum_{t}\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}\{Z_{t}^ {i}=z,Z_{t+1}^{i}=z^{\prime}\}=\sum_{t}\mathbb{E}\left[\mathbf{1}\{Z_{t}=z,Z_{ t+1}=z^{\prime}\};\pi\right]\right)=1.\] (14)

Returning to \(\xrightarrow{\mathrm{a.s.}}\) notation, this means that

\[\sum_{t}\left(\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{ \prime}\}\right)\xrightarrow{\mathrm{a.s.}}\sum_{t}\mathbb{E}\left[\mathbf{1}\{ Z_{t}=z,Z_{t+1}=z^{\prime}\};\pi\right].\] (15)The expected value of the indicator is equal to the probability of the event, and so

\[\sum_{t}\mathbb{E}\left[\mathbf{1}\{Z_{t}=z,Z_{t+1}=z^{\prime}\};\pi\right]=\sum_ {t}\Pr(Z_{t}=z,Z_{t+1}=z^{\prime};\pi).\] (16)

This, combined with the fact that the left hand side of Equation 15 is \(X_{n}\) means that

\[X_{n}\stackrel{{\text{a.s.}}}{{\longrightarrow}}\sum_{t}\Pr(Z_{t }=z,Z_{t+1}=z^{\prime};\pi).\] (17)

Similarly, these same exact steps can be followed to show that

\[Y_{n}\stackrel{{\text{a.s.}}}{{\longrightarrow}}\sum_{t}\Pr(Z_{ t}=z;\pi).\] (18)

Let \(\Psi_{i}=(X_{i},Y_{i})\), so that \(\Psi_{n}=(X_{i},Y_{i})_{i=1}^{n}\) is a sequence of vector-valued random variables. Since \(X_{n}\stackrel{{\text{a.s.}}}{{\longrightarrow}}\sum_{t}\Pr(Z_{ t}=z,Z_{t+1}=z^{\prime};\pi)\) and \(Y_{n}\stackrel{{\text{a.s.}}}{{\longrightarrow}}\sum_{t}\Pr(Z_{ t}=z;\pi)\), we have that

\[\Psi_{n}\stackrel{{\text{a.s.}}}{{\longrightarrow}}\left(\sum_ {t}\Pr(Z_{t}=z,Z_{t+1}=z^{\prime};\pi),\,\sum_{t}\Pr(Z_{t}=z;\pi)\right).\] (19)

Consider the function \(f:\mathbb{R}^{2}\rightarrow\mathbb{R}\), defined as:

\[f(x,y)=\begin{cases}\frac{x}{y}&\text{if }y\neq 0\\ \widehat{\mathrm{P}}_{n,\phi}^{\pi}(z,z^{\prime})&\text{otherwise.}\end{cases}\] (20)

Note that discontinuities of \(f\) may occur when \(y=0\). Applying the continuous mapping theorem, we have that

\[f(\Psi_{n})\stackrel{{\text{a.s.}}}{{\longrightarrow}}\frac{ \sum_{t}\Pr(Z_{t}=z,Z_{t+1}=z^{\prime};\pi)}{\sum_{t}\Pr(Z_{t}=z;\pi)}\] (21)

if6

Footnote 6: This condition arises due to the discontinuity of \(f\) when the second argument is zero. See the statement of the continuous mapping theorem in Appendix A for details.

\[\Pr\left(\sum_{t}\Pr(Z_{t}=z;\pi)=0\right)=0.\] (22)

In our case, \(\sum_{t}\Pr(Z_{t}=z;\pi)\) is a constant, and so the condition in Equation 22 is simply:

\[\sum_{t}\Pr(Z_{t}=z;\pi)\neq 0.\] (23)

Also, Equation 21 can be simplified to:

\[\widehat{\mathrm{P}}_{n,\phi}^{\pi}(z,z^{\prime})\stackrel{{ \text{a.s.}}}{{\longrightarrow}}\mathrm{P}_{\phi}^{\pi}(z,z^{\prime}),\] (24)

since \(f(\Psi_{n})=X_{n}/Y_{n}=\widehat{\mathrm{P}}_{n,\phi}^{\pi}(z,z^{\prime})\),7 and \(\mathrm{P}_{\phi}^{\pi}(z,z^{\prime})=\frac{\sum_{t}\Pr(Z_{t}=z,Z_{t+1}=z^{ \prime};\pi)}{\sum_{t}\Pr(Z_{t}=z;\pi)}\) by Equation 3. Restating this conclusion, we have that if

Footnote 7: To be more precise \(f(\Psi_{n})=X_{n}/Y_{n}\) when \(Y_{n}\neq 0\). However, even when \(Y_{n}=0\), the conclusion that \(f(\Psi_{n})=\widehat{\mathrm{P}}_{n,\phi}^{\pi}(z,z^{\prime})\) holds from the definition of \(f\) in Equation 20.

\[\sum_{t}\Pr(Z_{t}=z;\pi)\neq 0,\] (25)

then

\[\widehat{\mathrm{P}}_{n,\phi}^{\pi}(z,z^{\prime})\stackrel{{ \text{a.s.}}}{{\longrightarrow}}\mathrm{P}_{\phi}^{\pi}(z,z^{\prime}),\] (26)

which establishes the property. 

Next, we present a property that establishes the almost sure convergence of the reward function estimate.

**Property B.3**.: _For all abstract states \(z\in\mathcal{Z}\), if_

\[\sum_{t}\Pr(Z_{t}=z;\pi)\neq 0,\] (27)

_then_

\[\widehat{\mathrm{R}}^{\pi}_{n,\phi}(z)\stackrel{{ a.s.}}{{ \longrightarrow}}\mathrm{R}^{\pi}_{\phi}(z).\] (28)

Proof.: Recall that

\[\widehat{\mathrm{R}}^{\pi}_{n,\phi}(z)=\frac{\sum_{i=1}^{n}\sum_{t}\mathbf{1} \{Z_{t}^{i}=z\}R_{t}^{i}}{\sum_{i=1}^{n}\sum_{t}\mathbf{1}\{Z_{t}^{i}=z\}}.\]

Let \(X_{n}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\sum_{t}\mathbf{1}\{Z_{t}^{i}=z\}R_{t} ^{i}\) and \(Y_{n}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\sum_{t}\mathbf{1}\{Z_{t}^{i}=z\}\). We will show that \(X_{n}\) and \(Y_{n}\) converge almost surely, and then apply the continuous mapping theorem to reason about the almost sure convergence of \(\widehat{\mathrm{R}}^{\pi}_{n,\phi}(z)\).

To ensure that the ARP is well-defined even when \(\tilde{Z}\) is not empty, for abstract states \(\tilde{z}\in\tilde{Z}\) (abstract states that were not observed in the data), the reward function can be set to \(\widehat{\mathrm{R}}^{\pi}_{n,\phi}(\tilde{z})=0\), as elaborated in the proof of Property B.2.

Consider \(\lim_{n\to\infty}X_{n}\),

\[\lim_{n\to\infty}X_{n} =\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}\sum_{t}\mathbf{1}\{Z _{t}^{i}=z\}R_{t}^{i}\] \[=\lim_{n\to\infty}\sum_{t}\left(\frac{1}{n}\sum_{i=1}^{n} \mathbf{1}\{Z_{t}^{i}=z\}R_{t}^{i}\right).\]

Note that \(\mathbf{1}\{Z_{t}^{i}=z\}R_{t}^{i}\) is independent across episodes (for each \(i\)), has bounded variance, and has the same mean for each episode. By Kolmogorov's strong law of large numbers [46],

\[\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}\{Z_{t}^{i}=z\}R_{t}^{i} \stackrel{{\mathrm{a.s.}}}{{\longrightarrow}} \mathbb{E}\left[\mathbf{1}\{Z_{t}=z\}R_{t};\pi\right]\] \[=\sum_{r\in\mathbb{R},\mathbb{Z}\in\mathcal{Z}}\Pr(R_{t}=r,Z_{t} =\breve{z};\pi)\mathbf{1}\{Z_{t}=z\}r\] \[=\sum_{r\in\mathbb{R}}\Pr(R_{t}=r,Z_{t}=z;\pi)r\] \[=\] \[= \mathbb{E}\left[R_{t}\mid Z_{t}=z;\pi\right]\Pr(Z_{t}=z;\pi).\]

This convergence guarantee holds for all \(t\). As in the proof of Property B.2, by the countable additivity property of probability measures, in conjunction with the dominated convergence theorem [16, Theorem 1.19], this implies that,

\[\Pr\left(\lim_{n\to\infty}\sum_{t}\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}\{Z_{t}^ {i}=z\}R_{t}^{i}=\sum_{t}\mathbb{E}\left[R_{t}\mid Z_{t}=z;\pi\right]\Pr(Z_{t} =z;\pi)\right)=1.\] (29)

Returning to \(\stackrel{{\mathrm{a.s.}}}{{\longrightarrow}}\) notation, this means that

\[X_{n}\stackrel{{\mathrm{a.s.}}}{{\longrightarrow}}\sum_{t} \mathbb{E}\left[R_{t}\mid Z_{t}=z;\pi\right]\Pr(Z_{t}=z;\pi).\] (30)

Similarly, these same exact steps can be followed to show that

\[Y_{n}\stackrel{{\mathrm{a.s.}}}{{\longrightarrow}}\sum_{t}\Pr(Z _{t}=z;\pi).\] (31)Let \(\Psi_{i}=(X_{i},Y_{i})\), so that \(\Psi_{n}=(X_{i},Y_{i})_{i=1}^{n}\) is a sequence of vector-valued random variables. Considering the function \(f:\mathbb{R}^{2}\to\mathbb{R}\), defined as:

\[f(x,y)=\begin{cases}\frac{x}{y}&\text{if }y\neq 0\\ \widehat{\mathrm{R}}_{n,\phi}^{\pi}(z)&\text{otherwise}.\end{cases}\] (32)

Note that discontinuities of \(f\) may occur when \(y=0\). Applying the continuous mapping theorem, we have that

\[f(\Psi_{n})\stackrel{{\mathrm{a.s.}}}{{\longrightarrow}}\frac{ \sum_{t}\mathbb{E}\left[R_{t}\mid Z_{t}=z;\pi\right]\Pr(Z_{t}=z;\pi)}{\sum_{t} \Pr(Z_{t}=z;\pi)}\] (33)

if

\[\sum_{t}\Pr(Z_{t}=z;\pi)\neq 0.\] (34)

Thus, when \(\sum_{t}\Pr(Z_{t}=z;\pi)\neq 0\)

\[\widehat{\mathrm{R}}_{n,\phi}^{\pi}(z)\stackrel{{\mathrm{a.s.}}} {{\longrightarrow}}\mathrm{R}_{\phi}^{\pi}(z),\] (35)

which establishes the property. 

Next, we present a property that establishes the almost sure convergence of the initial state distribution.

**Property B.4**.: _For all abstract states \(z\in\mathcal{Z}\), \(\widehat{\eta}_{n,\phi}(z)\stackrel{{\mathrm{a.s.}}}{{ \longrightarrow}}\eta_{\phi}(z)\)._

Proof.: Recall that

\[\widehat{\eta}_{n,\phi}(z)=\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}\{Z_{0}^{i}=z\}.\]

Let \(X_{n}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}\{Z_{0}^{i}=z\}\). We will show that \(X_{n}\) converges almost surely, and then apply the continuous mapping theorem to reason about the almost sure convergence of \(\widehat{\eta}_{n,\phi}(z)\). Note that even when \(\tilde{Z}\) is not empty, the definition of \(\widehat{\eta}_{n,\phi}^{\pi}\) ensures that it is well-defined.

Consider \(\lim_{n\to\infty}X_{n}\),

\[\lim_{n\to\infty}X_{n}=\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}\{Z _{0}^{i}=z\}.\]

Note that \(\mathbf{1}\{Z_{0}^{i}=z\}\) is independent across episodes (for each \(i\)), has bounded variance, and has the same mean for each episode. By Kolmogorov's strong law of large numbers [46],

\[\frac{1}{n}\mathbf{1}\{Z_{0}^{i}=z\}\stackrel{{\mathrm{a.s.}}}{{ \longrightarrow}}\mathbb{E}\left[\mathbf{1}\{Z_{0}=z\}\right]=\Pr(Z_{0}=z).\]

In contrast to the proofs of the previous two properties, this proof holds for all \(z\in\mathcal{Z}\), not just the abstract states that have non-zero probability of occuring. This implies that

\[\widehat{\eta}_{n,\phi}(z)\stackrel{{\mathrm{a.s.}}}{{ \longrightarrow}}\eta_{\phi}(z),\] (36)

which establishes the property. 

Having established properties that will be useful in the proof of Lemma 3.2, we now turn to proving the lemma.

**Lemma 3.2**.: \(\forall\phi\in\Phi\)_, the expected return of the maximum likelihood estimate \(\widehat{\mathcal{R}}_{n,\phi}^{\pi}\) converges almost surely to the expected return of the policy \(\pi\), i.e., \(J(\pi;\widehat{\mathcal{R}}_{n,\phi}^{\pi})\stackrel{{\mathrm{a.s.}}} {{\longrightarrow}}J(\pi;M)\)._

Proof.: From Theorem 3.1, we have that \(J(\pi;\mathfrak{R}_{\phi}^{\pi})=J(\pi;M)\). Thus, to prove this result, we only need to show that \(J(\pi;\widehat{\mathcal{R}}_{n,\phi}^{\pi})\stackrel{{\mathrm{a.s.}} }{{\longrightarrow}}J(\pi;\mathfrak{R}_{\phi}^{\pi})\). Several of the preliminary results required to establish this result were provided in Properties B.2, B.3, and B.4. Specifically, Properties B.2 and B.3 establish that for all abstract states \(z\in\mathcal{Z}\) and \(z^{\prime}\in\mathcal{Z}\), if

\[\sum_{t}\Pr(Z_{t}=z;\pi)\neq 0,\] (37)then

\[\widehat{\mathrm{P}}^{\pi}_{n,\phi}(z,z^{\prime})\stackrel{{\text{ a.s.}}}{{\longrightarrow}}\mathrm{P}^{\pi}_{\phi}(z,z^{\prime}),\] (38)

and

\[\widehat{\mathrm{R}}^{\pi}_{n,\phi}(z)\stackrel{{\text{a.s.}}}{{ \longrightarrow}}\mathrm{R}^{\pi}_{\phi}(z).\] (39)

Similarly, Property B.4 establishes that for all abstract states \(z\in\mathcal{Z}\),

\[\widehat{\eta}_{n,\phi}(z)\stackrel{{\text{a.s.}}}{{ \longrightarrow}}\eta_{\phi}(z).\] (40)

The expected return \(J(\pi;\widehat{\mathfrak{R}}^{\pi}_{n,\phi})\) is a continuous function of \(\widehat{\mathfrak{R}}^{\pi}_{n,\phi}\coloneqq(\widehat{\mathrm{P}}^{\pi}_{n, \phi},\widehat{\mathrm{R}}^{\pi}_{n,\phi},\widehat{\eta}_{n,\phi})\), given by \(J(\pi;\widehat{\mathfrak{R}}^{\pi}_{n,\phi})=\left(\mathrm{I}-\widehat{ \mathrm{P}}^{\pi}_{n,\phi}\right)^{-1}\widehat{\mathrm{R}}^{\pi}_{n,\phi} \widehat{\eta}_{n,\phi}=\sum_{z}\sum_{t}\mathrm{\hat{Pr}}(Z_{t}=z;\pi) \widehat{\mathrm{R}}^{\pi}_{n,\phi}(z)\)8 where \(\mathrm{\hat{Pr}}\) denotes the empirical estimate of the corresponding probability.

Footnote 8: Termination is handled as detailed in Appendix A.1.

Note that the term \(\sum_{t}\mathrm{\hat{Pr}}(Z_{t}=z;\pi)=\left[\left(\mathrm{I}-\widehat{ \mathrm{P}}^{\pi}_{n,\phi}\right)^{-1}\widehat{\eta}_{n,\phi}\right]\), i.e., the empirical estimate of the sum of probabilities of encountering the abstract state \(z\) over all timesteps is equal to value of the vector \(\left(\mathrm{I}-\widehat{\mathrm{P}}^{\pi}_{n,\phi}\right)^{-1}\widehat{\eta }_{n,\phi}\) at \(z\). By the continuous mapping theorem, if \(\sum_{t}\mathrm{Pr}(Z_{t}=z;\pi)\neq 0\),

\[\sum_{t}\mathrm{\hat{Pr}}(Z_{t}=z;\pi)\stackrel{{\text{a.s.}}}{{ \longrightarrow}}\sum_{t}\mathrm{Pr}(Z_{t}=z;\pi).\] (41)

Let \(\bar{Z}=\{z:\sum_{t}\mathrm{Pr}(Z_{t}=z;\pi)=0\}\), where \(\bar{Z}\subset\mathcal{Z}\), be the set of abstract states that will never be observed empirically as they have no probability of being visited under \(\pi\).9 The expression for the expected return can be divided into two terms: (1) the first term sums over abstract states for which \(\sum_{t}\mathrm{Pr}(Z_{t}=z;\pi)=0\), and the values of the components of the ARP--in particular, \(\widehat{\mathrm{R}}^{\pi}_{n,\phi}\)--have been specially hardcoded as detailed in Property B.2, and (2) the second term sums over the abstract states for which \(\sum_{t}\mathrm{Pr}(Z_{t}=z;\pi)\neq 0\) and thus Equation 41 holds.

Footnote 9: In relation to \(\bar{Z}\) defined in Property B.2, note \(\bar{Z}\subseteq\bar{Z}\). As \(n\to\infty\) the two sets, with probability 1, become equal.

\[J(\pi;\widehat{\mathfrak{R}}^{\pi}_{n,\phi})= \sum_{z\in\mathcal{Z}}\sum_{t}\mathrm{\hat{Pr}}(Z_{t}=z;\pi) \underbrace{\widehat{\mathrm{R}}^{\pi}_{n,\phi}(z)}_{=0}+\sum_{z\in\mathcal{Z} \setminus\bar{Z}}\sum_{t}\mathrm{\hat{Pr}}(Z_{t}=z;\pi)\widehat{\mathrm{R}}^ {\pi}_{n,\phi}(z)\] (42) \[= \sum_{z\in\mathcal{Z}\setminus\bar{Z}}\sum_{t}\mathrm{\hat{Pr}}(Z _{t}=z;\pi)\widehat{\mathrm{R}}^{\pi}_{n,\phi}(z)\] (43)

As detailed in Property B.2, \(\widehat{\mathrm{R}}^{\pi}_{n,\phi}(z)=0\) for \(z\in\tilde{Z}\) and thus \(\forall z\in\bar{Z}\), making the first term equal to zero. The constituents of the second term converge almost surely:

\[\sum_{z\in\mathcal{Z}\setminus\bar{Z}}\sum_{t}\mathrm{\hat{Pr}}(Z _{t}=z;\pi)\widehat{\mathrm{R}}^{\pi}_{n,\phi}(z) \stackrel{{\text{a.s.}}}{{\longrightarrow}} \sum_{z\in\mathcal{Z}\setminus\bar{Z}}\sum_{t}\mathrm{Pr}(Z_{t}=z; \pi)\mathrm{R}^{\pi}_{\phi}(z)\] (44) \[= \sum_{z\in\mathcal{Z}\setminus\bar{Z}}\sum_{t}\mathrm{Pr}(Z_{t}=z ;\pi)\mathrm{R}^{\pi}_{\phi}(z)+0\] (45) \[= \sum_{z\in\mathcal{Z}\setminus\bar{Z}}\sum_{t}\mathrm{Pr}(Z_{t}=z ;\pi)\mathrm{R}^{\pi}_{\phi}(z)+\sum_{z\in\bar{Z}}\underbrace{\left(\sum_{t} \mathrm{Pr}(Z_{t}=z;\pi)\right)}_{=0}\mathrm{R}^{\pi}_{\phi}(z)\] (46) \[= \sum_{z\in\mathcal{Z}}\sum_{t}\mathrm{Pr}(Z_{t}=z;\pi)\mathrm{R}^ {\pi}_{\phi}(z)\] (47) \[= J(\pi;\mathfrak{R}^{\pi}_{\phi}).\] (48)

We then obtain the final result:

\[J(\pi;\widehat{\mathfrak{R}}^{\pi}_{n,\phi})\stackrel{{\text{a.s.}}}{{ \longrightarrow}}J(\pi;\mathfrak{R}^{\pi}_{\phi})=J(\pi;M).\] (49)

### Estimation of ARPs from Off-Policy Data

So far, we have shown that the expected return of the estimated ARP converges almost surely to the expected return of the policy that induced it, i.e., consistent _on-policy_ evaluation. Next, we show that a model of the ARP can be consistently estimated from _off-policy_ data. This requires us to prove that transition functions estimated using off-policy data, incorporating importance weights in their estimation, converge almost surely to the true transition functions induced by the evaluation policy, and that the same holds for the reward functions and initial state distributions.

**Property B.6**.: _For all abstract states \(z\in\mathcal{Z}\) and \(z^{\prime}\in\mathcal{Z}\), if_

\[\sum_{t}\Pr(Z_{t}=z;\pi_{b})\neq 0,\] (50)

_then_

\[\widehat{\mathrm{P}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(z,z^{\prime}) \stackrel{{ a.s.}}{{\longrightarrow}}\mathrm{P}_{\phi}^{\pi_{e}}(z,z^{ \prime}).\] (51)

Proof.: Recall that

\[\widehat{\mathrm{P}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(z,z^{\prime})=\frac {\sum_{i,t}\mathbf{1}_{t}^{i}\{z,z^{\prime}\}\rho_{0:t}}{\sum_{i,t}\mathbf{1} _{t}^{i}\{z\}\rho_{0:t}}.\]

Similar to the proof structure of Property B.2, let \(X_{n}\coloneqq\frac{1}{n}\sum_{i,t}\mathbf{1}_{t}^{i}\{z,z^{\prime}\}\rho_{0:t}\) and \(Y_{n}\coloneqq\frac{1}{n}\sum_{i,t}\mathbf{1}_{t}^{i}\{z\}\rho_{0:t}\). For abstract states \(\tilde{Z}\subset\mathcal{Z}\) that are never reached and thus are not observed in the data, special values can be hardcoded into the transition function, reward functions, and initial state distribution so that the components of the ARP continue to be well-defined. In particular, for \(\tilde{z}\in\tilde{Z}\), the transition function can be set to self-transition back to \(\tilde{z}\) with probability 1, i.e., \(\widehat{\mathrm{P}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(\tilde{z},\tilde{z})=1\) and \(\widehat{\mathrm{P}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(\tilde{z},\tilde{z })=0\) for all \(\tilde{z}\notin\tilde{Z}\), the reward function is set to \(\widehat{\mathrm{R}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(\tilde{z})=0\) and the initial abstract state distribution is \(\widehat{\eta}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(\tilde{z})=0\) for all \(\tilde{z}\in\tilde{Z}\).

Consider \(\lim_{n\rightarrow\infty}X_{n}\),

\[\lim_{n\rightarrow\infty}X_{n} =\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i,t}\mathbf{1}_{t}^{i }\{z,z^{\prime}\}\rho_{0:t}\] \[=\lim_{n\rightarrow\infty}\sum_{t}\left(\frac{1}{n}\sum_{i=1}^{ n}\mathbf{1}_{t}^{i}\{z,z^{\prime}\}\rho_{0:t}\right).\]

Note that \(\mathbf{1}_{t}^{i}\{z,z^{\prime}\}\rho_{0:t}\) is independent across episodes (for each \(i\)), has bounded variance, and has the same mean for each episode. By Kolmogorov's strong law of large numbers [46],

\[\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}_{t}^{i}\{z,z^{\prime}\}\rho_{0:t} \stackrel{{\text{a.s.}}}{{\longrightarrow}}\mathbb{E}\left[ \mathbf{1}\{Z_{t}=z,Z_{t+1}=z^{\prime}\}\rho_{0:t};\pi_{b}\right].\]

The importance weights change the distribution over which the expectation is computed, as below:

\[\mathbb{E}\left[\mathbf{1}\{Z_{t}=z,Z_{t+1}=z^{\prime}\}\rho_{0:t };\pi_{b}\right]\] \[=\sum_{t}\sum_{\begin{subarray}{c}S_{0:t+1},\\ A_{0:t}\end{subarray}}\left(\eta(S_{0})\prod_{l=0}^{t}\pi_{b}(S_{l},A_{l})p(S_ {l},A_{l},S_{l+1})\right)\mathbf{1}\{\phi(S_{t}){=}z,\phi(S_{t+1}){=}z^{\prime} \}\left(\prod_{j=0}^{t}\frac{\pi_{e}(S_{j},A_{j})}{\pi_{b}(S_{j},A_{j})}\right)\] \[=\sum_{t}\mathbb{E}\left[\mathbf{1}\{Z_{t}=z,Z_{t+1}=z^{\prime} \};\pi_{e}\right]\]

This convergence guarantee holds for all \(t\). By the countable additivity property of probability measures in conjunction with the dominated convergence theorem [16], this implies that,

\[X_{n}\stackrel{{\text{a.s.}}}{{\longrightarrow}}\sum_{t}\mathbb{E }\left[\mathbf{1}\{Z_{t}=z,Z_{t+1}=z^{\prime}\};\pi_{e}\right].\] (52)Similarly, these same exact steps can be followed to show that

\[Y_{n}\xrightarrow{\text{a.s.}}\sum_{t}\mathbb{E}\left[\mathbf{1}\{Z_{t}=z\}; \pi_{e}\right].\] (53)

Let \(\Psi_{i}=(X_{i},Y_{i})\), so that \(\Psi_{n}=(X_{i},Y_{i})_{i=1}^{n}\) is a sequence of vector-valued random variables. Considering the function \(f:\mathbb{R}^{2}\rightarrow\mathbb{R}\), defined as:

\[f(x,y)=\begin{cases}\frac{x}{y}&\text{if }y\neq 0\\ \operatorname{\hat{P}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(z,z^{\prime})& \text{otherwise}.\end{cases}\] (54)

Note that discontinuities of \(f\) may occur when \(y=0\). Applying the continuous mapping theorem, we have that

\[f(\Psi_{n})\xrightarrow{\text{a.s.}}\frac{\sum_{t}\mathbb{E}\left[\mathbf{1} \{Z_{t}=z,Z_{t+1}=z^{\prime}\};\pi_{e}\right]}{\sum_{t}\mathbb{E}\left[\mathbf{ 1}\{Z_{t}=z\};\pi_{e}\right]}\] (55)

if \(\sum_{t}\mathbb{E}\left[\mathbf{1}\{Z_{t}=z\};\pi_{e}\right]\neq 0\). This implies that

\[\operatorname{\hat{P}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(z,z^{\prime}) \xrightarrow{\text{a.s.}}\operatorname{P}_{\phi}^{\pi_{e}}(z,z^{\prime}),\] (56)

when \(\sum_{t}\Pr(Z_{t}=z;\pi_{b})\neq 0\), which establishes the property.

Next, we present the properties that establish almost sure convergence of the reward function estimate and the initial state distribution estimated from off-policy data. This follows a similar proof structure to their corresponding on-policy properties, with the key difference being the use of importance weights in the estimation.

**Property B.7**.: _For all abstract states \(z\in\mathcal{Z}\), if_

\[\sum_{t}\Pr(Z_{t}=z;\pi_{b})\neq 0,\] (57)

_then_

\[\operatorname{\hat{R}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(z)\xrightarrow{ \text{a.s.}}\operatorname{R}_{\phi}^{\pi_{e}}(z).\] (58)

Proof.: Recall that

\[\operatorname{\hat{R}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(z)=\frac{\sum_{ i=1}^{n}\sum_{t}\mathbf{1}\{Z_{t}^{i}=z\}R_{t}^{i}\rho_{0:t}}{\sum_{i=1}^{n}\sum_{t} \mathbf{1}\{Z_{t}^{i}=z\}\rho_{0:t}}.\]

Let \(X_{n}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\sum_{t}\mathbf{1}\{Z_{t}^{i}=z\}R_{t} ^{i}\rho_{0:t}\) and \(Y_{n}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\sum_{t}\mathbf{1}\{Z_{t}^{i}=z\}\rho_ {0:t}\). Following the exact steps from Property B.3, we can show that

\[\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}\{Z_{t}^{i}=z\}R_{t}^{i} \xrightarrow{\text{a.s.}} \mathbb{E}\left[\mathbf{1}\{Z_{t}=z\}R_{t}\rho_{0:t};\pi_{b}\right]\] \[= \mathbb{E}\left[\mathbf{1}\{Z_{t}=z\}R_{t};\pi_{e}\right]\] \[= \sum_{r\in\mathbb{R},\tilde{z}\in\mathcal{Z}}\Pr(R_{t}=r,Z_{t}= \tilde{z};\pi_{e})\mathbf{1}\{Z_{t}=z\}r\] \[= \sum_{r\in\mathbb{R}}\Pr(R_{t}=r,Z_{t}=z;\pi_{e})r\] \[= \left(\sum_{r\in\mathbb{R}}r\Pr(R_{t}=r\mid Z_{t}=z;\pi_{e}) \right)\Pr(Z_{t}=z;\pi)\] \[= \mathbb{E}\left[R_{t}\mid Z_{t}=z;\pi\right]\Pr(Z_{t}=z;\pi_{e}).\]

This convergence guarantee holds for all \(t\). By the countable additivity property of probability measures, in conjunction with the dominated convergence theorem, this implies that

\[X_{n}\xrightarrow{\text{a.s.}}\sum_{t}\mathbb{E}\left[R_{t}\mid Z_{t}=z;\pi_{ e}\right]\Pr(Z_{t}=z;\pi_{e}).\] (59)Similarly, these same exact steps can be followed to show that

\[Y_{n}\stackrel{{\mathrm{a.s.}}}{{\longrightarrow}}\sum_{t}\Pr(Z_{t} =z;\pi_{e}).\] (60)

Noting that \(\widehat{\mathrm{R}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}=\frac{X_{n}}{Y_{n}}\), we can apply the continuous mapping theorem, as done in the proof of Property B.3, to show that

\[\widehat{\mathrm{R}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(z)\stackrel{{ \mathrm{a.s.}}}{{\longrightarrow}}\mathrm{R}_{\phi}^{\pi_{e}}(z),\] (61)

when \(\sum_{t}\Pr(Z_{t}=z;\pi_{b})\neq 0\). 

**Property B.8**.: _For all abstract states \(z\in\mathcal{Z}\), \(\widehat{\eta}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(z)\stackrel{{ \mathrm{a.s.}}}{{\longrightarrow}}\eta_{\phi}^{\pi_{e}}(z)\)._

Proof.: Recall that

\[\widehat{\eta}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(z)=\frac{1}{n}\sum_{i=1}^{ n}\mathbf{1}\{Z_{0}^{i}=z\}\rho_{0}.\]

Let \(X_{n}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}\{Z_{0}^{i}=z\}\rho_{0}\). Following the exact steps from Property B.4, we can show that

\[\frac{1}{n}\mathbf{1}\{Z_{t}^{i}=z\}\stackrel{{\mathrm{a.s.}}}{{ \longrightarrow}}\mathbb{E}\left[\mathbf{1}\{Z_{0}=z\}\rho_{0}\right]=\Pr(Z_{0 }=z).\]

This proof holds for all \(z\in\mathcal{Z}\), not just the abstract states that have non-zero probability of occuring. This implies that

\[\widehat{\eta}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(z)\stackrel{{ \mathrm{a.s.}}}{{\longrightarrow}}\eta_{\phi}^{\pi_{e}}(z).\] (62)

**Lemma 3.3**.: _Under Assumption 2.1, the weighted maximum likelihood estimate \(\widehat{\mathrm{\mathfrak{R}}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}\) converges almost surely to the ground-truth ARP \(\mathfrak{R}_{\phi}^{\pi_{e}}\), i.e., \(\widehat{\mathrm{\mathfrak{R}}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}} \stackrel{{\mathrm{a.s.}}}{{\longrightarrow}}\mathfrak{R}_{ \phi}^{\pi_{e}}\)._

Proof.: The results required to establish this result are provided in Properties B.6, B.7, and B.8. Specifically, the properties establish that if

\[\sum_{t}\Pr(Z_{t}=z;\pi_{b})\neq 0,\]

then

\[\widehat{\mathrm{P}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(z,z^{\prime}) \stackrel{{\mathrm{a.s.}}}{{\longrightarrow}}\mathrm{P}_{\phi}^ {\pi_{e}}(z,z^{\prime});\quad\widehat{\mathrm{R}}_{n,\phi}^{\pi_{b} \rightarrow\pi_{e}}(z)\stackrel{{\mathrm{a.s.}}}{{\longrightarrow }}\mathrm{R}_{\phi}^{\pi_{e}}(z);\quad\widehat{\eta}_{n,\phi}^{\pi_{b} \rightarrow\pi_{e}}(z)\stackrel{{\mathrm{a.s.}}}{{ \longrightarrow}}\eta_{\phi}(z),\]

giving the final result \(\widehat{\mathrm{\mathfrak{R}}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}\stackrel{{ \mathrm{a.s.}}}{{\longrightarrow}}\mathfrak{R}_{\phi}^{\pi_{e}}\), while the values of the components for \(z\in\tilde{Z}\) are hardcoded as detailed in Property B.6.

### Off-Policy Evaluation with ARPs

**Theorem 4.1**.: _The expected return of the ARP \(\widehat{\mathrm{\mathfrak{R}}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}\) (built from off-policy data) converges almost surely to the expected return of \(\pi_{e}\), i.e., \(J(\pi_{e};\widehat{\mathrm{\mathfrak{R}}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}) \stackrel{{\mathrm{a.s.}}}{{\longrightarrow}}J(\pi_{e};M)\)._

Proof.: The expected return \(J(\pi_{e};\widehat{\mathrm{\mathfrak{R}}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}})\) is a continuous function of \(\widehat{\mathrm{\mathfrak{R}}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}:=( \widehat{\mathrm{P}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}},\widehat{\mathrm{ R}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}},\widehat{\eta}_{n,\phi}^{\pi_{b} \rightarrow\pi_{e}})\), given by \(J(\pi_{e};\widehat{\mathrm{\mathfrak{R}}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}} )=\left(\mathrm{I}-\widehat{\mathrm{P}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}} \right)^{-1}\widehat{\mathrm{R}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}\tilde{ \eta}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}\). Similar to the proof of Lemma 3.2, by the continuous mapping theorem and Lemma 3.3, we obtain the final result that enables off-policy evaluation with ARPs:

\[J(\pi_{e};\widehat{\mathrm{\mathfrak{R}}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}) \stackrel{{\mathrm{a.s.}}}{{\longrightarrow}}J(\pi_{e}; \mathfrak{R}_{\phi}^{\pi_{e}})=J(\pi_{e};M).\]

### Variance Reduction: Leveraging _Markovness_ of the State Abstraction

Variance in estimation of the model of the ARP from off-policy can be reduced by clipping the importance weights. We now show that when the state abstraction function is \(c\)-th order Markov, clipping the importance weights to the \(c\) most recent terms continues to provide a consistent off-policy expected return estimate.

**Theorem 4.3**.: _Given a \(c\)-th order Markov \(\phi\), the expected return of the abstract reward process \(\widehat{\mathfrak{R}}_{\phi,c}^{\pi_{b}\rightarrow\pi_{c}}\) converges almost surely to the expected return of \(\pi_{e}\), i.e., \(J(\pi_{e};\widehat{\mathfrak{R}}_{\phi,c}^{\pi_{b}\rightarrow\pi_{e}}) \stackrel{{ a.s.}}{{\longrightarrow}}J(\pi_{e};M).\)_

Proof.: We only need to show that for a state abstraction function \(\phi\in\Phi\) that is \(c\)-th order Markov, i.e.,

\[\Pr(\phi(S_{t+1})|\phi(S_{t}),\phi(S_{t-1}),\ldots,\phi(S_{(t-c+1)^{+}});\pi)= \Pr(\phi(S_{t+1})|\phi(S_{t}),\phi(S_{t-1}),\ldots,\phi(S_{1}),\phi(S_{0});\pi),\]

for \(\pi\in\{\pi_{b},\pi_{e}\}\) the following holds:

\[\widehat{\mathfrak{R}}_{\phi,c}^{\pi_{b}\rightarrow\pi_{e}}\stackrel{{ \text{a.s.}}}{{\longrightarrow}}\mathfrak{R}_{\phi}^{\pi_{e}}.\]

The remaining steps to show that the return estimates are consistent follow from Theorem 4.1. For brevity, we denote by \(Z_{t}:=\phi(S_{t})\) for the rest of the proof. Consider the expression for the transition function without any weight clipping,

\[\widehat{\mathrm{P}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(z,z^{\prime}):= \frac{\sum_{i,t}\mathbf{1}_{t}^{i}\{z,z^{\prime}\}\rho_{0:t}}{\sum_{i,t} \mathbf{1}_{t}^{i}\{z\}\rho_{0:t}}.\]

As shown in Property B.6, if \(\sum_{t}\Pr(Z_{t}=z;\pi_{b})\neq 0\), as \(n\rightarrow\infty\) the numerator of \(\widehat{\mathrm{P}}_{n,\phi}^{\pi_{b}\rightarrow\pi_{e}}(z,z^{\prime})\) converges almost surely to \(\sum_{t}\mathbb{E}\left[\mathbf{1}\{Z_{t}=z,Z_{t+1}=z^{\prime}\}\rho_{0:t}; \pi_{b}\right]\). When the abstract states are \(c\)-th order Markov,

\[\rho_{0:(t-c)^{+}}\perp\rho_{(t-c+1)^{+}:t}\text{, conditioned on }(Z_{i})_{i=t-c+1}^{t}.\] (63)

Using the tower rule [60], \(\mathbb{E}\left[\mathbf{1}\{Z_{t}=z,Z_{t+1}=z^{\prime}\}\rho_{0:t};\pi_{b}\right]\) can be re-written as:

\[\mathbb{E}\left[\mathbf{1}\{Z_{t}=z,Z_{t+1}=z^{\prime}\}\rho_{0:t };\pi_{b}\right]\] \[= \mathbb{E}\left[\mathbb{E}\left[\mathbf{1}\{Z_{t}=z,Z_{t+1}=z^{ \prime}\}\rho_{0:(t-c)^{+}}\rho_{(t-c+1)^{+}:t}\mid(Z_{i})_{i=(t-c+1)^{+}}^{t} ;\pi_{b}\right];\pi_{b}\right]\] \[\stackrel{{(a)}}{{=}} \mathbb{E}\left[\mathbb{E}\left[\mathbf{1}\{Z_{t}=z,Z_{t+1}=z^{ \prime}\}\rho_{(t-c+1)^{+}:t}\mid(Z_{i})_{i=(t-c+1)^{+}}^{t};\pi_{b}\right] \mathbb{E}\left[\rho_{0:(t-c)^{+}}\mid(Z_{i})_{i=(t-c+1)^{+}}^{t};\pi_{b} \right];\pi_{b}\right]\] \[= \mathbb{E}\left[\mathbb{E}\left[\mathbf{1}\{Z_{t}=z,Z_{t+1}=z^{ \prime}\}\rho_{(t-c+1)^{+}:t}\mid(Z_{i})_{i=(t-c+1)^{+}}^{t};\pi_{b}\right]; \pi_{b}\right]\] \[\stackrel{{(b)}}{{=}} \Pr(Z_{t}=z,Z_{t+1}=z^{\prime};\pi_{e}),\]

where \((a)\) follows from Equation 63 and \((b)\) follows from Lemma 3.3. Thus when the abstract states are \(c\)-th order Markov, the numerator estimated with \(c\)-clipped importance weights almost surely converges to the same value as if the importance weights were not clipped. The value to which the denominator converges similarly undergoing a change of measure by the use of clipped importance weights, i.e.,

\[\mathbb{E}\left[\mathbf{1}\{Z_{t}=z\}\rho_{0:t};\pi_{b}\right]=\mathbb{E}\left[ \mathbf{1}\{Z_{t}=z\}\rho_{(t-c+1)^{+}:t};\pi_{b}\right]=\Pr(Z_{t}=z;\pi_{e}),\] (64)

when \(\phi\) is \(c\)-th order Markov. Note that the expression for \(\widehat{\mathrm{P}}_{\phi,c}^{\pi_{b}\rightarrow\pi_{e}}\) is:

\[\widehat{\mathrm{P}}_{\phi,c}^{\pi_{b}\rightarrow\pi_{e}}(z,z^{\prime}):= \frac{\sum_{i,t}\mathbf{1}_{t}^{i}\{z,z^{\prime}\}\rho_{(t-c+1)^{+}:t}}{\sum_{i,t}\mathbf{1}_{t}^{i}\{z\}\rho_{(t-c+1)^{+}:t}}.\]Let \(X_{n}\coloneqq\frac{1}{n}\sum_{i,t}\mathbf{1}_{t}^{i}\{z,z^{\prime}\}\rho_{(t-c+1)^ {+};t}\) and \(Y_{n}\coloneqq\frac{1}{n}\sum_{i,t}\mathbf{1}_{t}^{i}\{z\}\rho_{(t-c+1)^{+};t}\). We can write \(\widehat{\mathrm{P}}_{\phi,c}^{\pi_{b}+\pi_{c}}(z,z^{\prime})=\frac{X_{n}}{Y_ {n}}\). Following the exact steps from Lemma 3.3, that entail a careful application of the continuous mapping theorem, we can show that

\[\widehat{\mathrm{P}}_{\phi,c}^{\pi_{b}\to\pi_{c}}(z,z^{\prime})\stackrel{{ \mathrm{a.s.}}}{{\longrightarrow}}\mathrm{P}_{\phi}^{\pi_{c}}(z,z^{\prime}),\]

if \(\sum_{t}\Pr(Z_{t}=z;\pi_{b})\neq 0\). Similar derivations can be followed for the reward function and the initial state distribution, leading to the required result:

\[\widehat{\mathfrak{R}}_{\phi,c}^{\pi_{b}\to\pi_{c}}\stackrel{{ \mathrm{a.s.}}}{{\longrightarrow}}\mathfrak{R}_{\phi}^{\pi_{c}}\]

when \(\sum_{t}\Pr(Z_{t}=z;\pi_{b})\neq 0\), that enables almost sure convergence of the expected return estimate:

\[J(\pi_{e};\widehat{\mathfrak{R}}_{\phi,c}^{\pi_{b}\to\pi_{c}})\stackrel{{ \mathrm{a.s.}}}{{\longrightarrow}}J(\pi_{e};\mathfrak{R}_{\phi}^{\pi_{c}})=J( \pi_{e};M).\]Empirical Details

In this section we provide additional empirical details for the experiments presented in the main paper. An overall step-by-step algorithm for STAR is as follows:

``` Input:\(\pi_{e}\), \(\pi_{b}\), \(\mathcal{D}_{n}^{(\pi_{b})}\)
1. Apply state abstraction to \(\mathcal{D}_{n}^{(\pi_{b})}\) and compute importance weights: \(\forall\;i,t:\text{Store}\left(Z_{t}^{(i)}=\phi\left(S_{t}^{(i)}\right),A_{t}^{ (i)},R_{t}^{(i)},\rho_{(t-c+1)^{+}:t}^{(i)}=\prod_{j=(t-c+1)^{+}}^{t}\frac{\pi _{e}(S_{j}^{(i)},A_{j}^{(i)})}{\pi_{e}(S_{j}^{(i)},A_{j}^{(i)})}\right)\)
2. Estimate the components of the ARP \(\widehat{\mathfrak{R}}_{\phi,c}^{\pi_{b}\to\pi_{e}}\): \(\widehat{\mathfrak{R}}_{\phi,c}^{\pi_{b}\to\pi_{e}}=\left(\widehat{\mathrm{P} }_{\phi,c}^{\pi_{b}\to\pi_{e}},\widehat{\mathrm{R}}_{\phi,c}^{\pi_{b}\to\pi_{ e}},\widehat{\eta}_{\phi,c}^{\pi_{b}\to\pi_{e}}\right)\) where, \(\widehat{\mathrm{P}}_{\phi,c}^{\pi_{b}\to\pi_{e}}=\frac{\sum_{i,t}\mathbf{1} \{\phi(S_{t}^{(i)})=z,\phi(S_{t+1}^{(i)})=z^{2}\}\rho_{(t-c+1)^{+}:t}}{\sum_{i,t}\mathbf{1}\{\phi(S_{t}^{(i)})=z\}\rho_{(t-c+1)^{+}:t}}\), \(\widehat{\mathrm{R}}_{\phi,c}^{\pi_{b}\to\pi_{e}}=\frac{\sum_{i,t}\mathbf{1} \{\phi(S_{t}^{(i)})=z\}\rho_{(t-c+1)^{+}:t}R_{t}^{(i)}}{\sum_{i,t}\mathbf{1}\{ \phi(S_{t}^{(i)})=z\}\rho_{(t-c+1)^{+}:t}}\), \(\widehat{\eta}_{\phi,c}^{\pi_{b}\to\pi_{e}}(z)=\frac{\sum_{i,t}\mathbf{1}\{ \phi(S_{t}^{(i)})=z\}}{n}\)
3. Compute the expected return \(J(\pi_{e};\widehat{\mathfrak{R}}_{\phi,c}^{\pi_{b}\to\pi_{e}})\) from the ARP. \(J(\pi_{e};\widehat{\mathfrak{R}}_{\phi,c}^{\pi_{b}\to\pi_{e}})\coloneqq( \mathrm{I}-\widehat{\mathrm{P}}_{\phi,c}^{\pi_{b}\to\pi_{e}})^{-1}\widehat{ \mathrm{R}}_{\phi,c}^{\pi_{b}\to\pi_{e}}\widehat{\eta}_{\phi,c}^{\pi_{b}\to\pi _{e}}\)
4 Output:\(J(\pi_{e};\widehat{\mathfrak{R}}_{\phi,c}^{\pi_{b}\to\pi_{e}})\) ```

**Algorithm 1** Overview of STAR(\(\phi\), \(c\))

### Domains

We perform empirical evaluations on a range of domains that consist of continuous domains and domains with large state spaces with long horizons. The domains are as follows:

CartPoleThe CartPole domain is a classic control problem from OpenAI Gym [6]. The task is to balance a pole on a cart by moving the cart left or right. The state space is continuous, and the action space is discrete. We use the standard CartPole environment from OpenAI Gym. In the experiments, \(\pi_{b}\) is uniformly random, i.e., the left and right actions are each taken with probability 0.5. The evaluation policy \(\pi_{e}\) is a policy that takes the action right with probability \(0.9\) when the pole is leaning left, and right with probability \(0.1\) when the pole is leaning right. This results in a policy that is not optimal, but is somewhat successful at balancing the pole.

ICU-SepsisThe ICU-Sepsis domain simulates the treatment of sepsis in the ICU. Built from the MIMIC-III database [24] and drawing from the analysis of Komorowski et al. [26], it consists of 747 states that denote the status of a patient and 25 possible actions that denote possible medical interventions. At the end of each episode, if the patient survives, a reward of +1 is given, while death corresponds to a reward of 0, with all intermediate rewards also being 0. This results in the expected return of a policy corresponding to the probability that a randomly selected patient will survive. In the experiments, \(\pi_{e}\) is set to an _expert policy_, provided with the domain and included in the submitted codebase, while \(\pi_{b}\) is a policy that is a more stochastic version of the expert policy constructed by temperature scaling [17] the action probabilities of expert policy with a temperature parameter \(\tau=2\).

AsterixThe Asterix domain is a miniaturized version of the Atari game Asterix where the task is to collect items while avoiding enemies. We use the implementation of the game from the MinAtar testbed [61], where the dimension of each state is \(10\times 10\times 4\), and the action set consists of six actions. The data collecting policy \(\pi_{b}\) is uniformly random, while the evaluation policy \(\pi_{e}\) picks actions with non-uniform skewed probabilities.

### Additional Results for Estimator Selection

For each domain, we evaluate the OPE performance of the estimated ARP induced by varying configurations of \((\phi,c)\). For the class of abstraction function, we observe that the simple method CluSTAR performs well across all domains, and hence we use it for all experiments. CluSTARtakes an input a single hyperparameter, the number of centroids initialized, denoted by \(|\mathcal{Z}|\). We evaluate the following configurations of \(\mathcal{Z}\) and \(c\) for each domain:

1. CartPole: 35 estimators - \(|\mathcal{Z}|\in\{2,4,8,16,32,64,128\}\), \(c\in\{1,2,3,4,5\}\).
2. ICU-Sepsis: 25 estimators - \(|\mathcal{Z}|\in\{2,4,8,16,32\}\), \(c\in\{1,2,3,4,5\}\). ICU-Sepsis with \(|\mathcal{Z}|=32\) and \(n=5000\) is excluded for computational reasons.
3. Asterix: 25 estimators - \(|\mathcal{Z}|\in\{2,4,8,16,32\}\), \(c\in\{1,2,3,4,5\}\).

In Figure 4 we report the log MSE for each estimator.

Effect of \(|\mathcal{Z}|\):For the class of abstraction functions induced by CluSTAR, the effect of \(|\mathcal{Z}|\) varies across domains. In some domains, such as ICU-Sepsis, the estimators obtained by varying this hyperparameter show similar performance. However, in domains like Asterix and CartPole, performance is more sensitive to the choice of \(|\mathcal{Z}|\), with larger values performing better.

Effect of \(c\):It is expected that large values of \(c\) will lead to higher variance of the estimates. In low data regimes, small values of \(c\) result in relatively lower MSE as the variance is lowered at the cost of increased bias. As the amount of data increases, the best value of \(c\) also increases. This effect is most pronounced in the Asterix domain, and to a lesser extent in the CartPole domain.

The key takeaway from Figure 4 is that the optimal \(|Z|\) and \(c\) are a function of the amount of data (\(n\)), the characteristics of the domain and the class of abstraction functions used.

### Compute

All experiments were run for 200 seeds each, on 3 domains in total. Each run took between 3 hours to 3 days (depending on the domain) and this duration includes offline data collection. The experiments

Figure 4: Heatmap of the log mean squared error (MSE) of the OPE estimators for the CartPole, Asterix, and ICU-Sepsis domains. The vertical axis represents the number of abstract states \(|\mathcal{Z}|\), and the horizontal axis represents the value of the hyperparameter \(c\). The color intensity indicates the log MSE, with lower values denoting better performance. Note that the variation with \(|\mathcal{Z}|\) is strongly influenced by the class of abstraction functions used, which in this work is CluSTAR.

were run using 32 threads on Xeon E5-2680 CPUs on a computing cluster, bringing the total compute time to roughly 45000 compute hours.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims are contributions are supported by corresponding theorems and empirical results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations are highlighted both in the concluding discussion and as a separate "Limitations" section in the supplementary material. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: All proofs are provided in the supplementary material, and assumptions states clearly in the main text. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The code has been included with the submission. Random seeds are set and fixed across multiple trials, along with the source code of the domain used. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Anonymized code is submitted as a.zip file with the submission. The codebase will be made public upon acceptance. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: There is a specific section, in the main text and supplementary material, dedicated to choice of hyperparameters along with necessary experimental details. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Standard error is reported for main experiments, with the number of trials specified. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Details about the compute resources are provided in the supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Naive application of the framework to safety-critical applications could lead to unintended consequences. A discussion on the limitations and potential impacts of the proposed framework is provided in the supplementary material. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Both positive impacts, in terms of strong results for OPE and potential negative impacts, in terms of naive application of the framework to safety-critical applications, are discussed in the paper.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Original creators of all domains used and all prior work are duely cited in the paper. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code and related artifacts, such as pre-trained policies, are well documented and provided alongside the submission. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.