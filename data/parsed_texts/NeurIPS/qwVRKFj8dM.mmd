# Comparing the local information geometry

of image representations

David Lipshutz\({}^{1,}\)1

Jenelle Feather\({}^{1,}\)1

Sarah E. Harvey\({}^{1}\)

Alex H. Williams\({}^{1,2}\)

Eero P. Simoncelli\({}^{1,2}\)

\({}^{1}\) Center for Computational Neuroscience, Flatiron Institute

\({}^{2}\) Center for Neural Science, New York University

{dlipshutz,jfeather,sharvey}@flatironinstitute.org,

{alex.h.williams,eero.simoncelli}@nyu.edu

Equal contribution

###### Abstract

We propose a framework for comparing a set of image representations (artificial or biological) in terms of their sensitivities to local distortions. We quantify the local geometry of a representation using the Fisher information matrix (FIM), a standard statistical tool for characterizing the sensitivity to local distortions of a stimulus, and use this as a substrate for a metric on the local geometry of representations in the vicinity of a base image. This metric may then be used to optimally differentiate a set of models, by optimizing for a pair of distortions that maximize the variance of the models under this metric. We use the framework to compare a set of simple models of the early visual system, identifying a novel set of image distortions that allow immediate comparison of the models by visual inspection. In a second example, we show that the method can reveal distinctions between standard and adversarially trained object recognition networks.

## 1 Introduction

Biological and artificial neural networks transform sensory stimuli into internal representations that support downstream tasks. Often, similarity between representations is quantified by measuring the alignment of their _global_ geometric structure [1, 2, 3, 4, 5]. However, systems with similar global geometry can have strikingly different _local_ geometries. The well-known occurrence of adversarial examples [6, 7] in object recognition systems provides an example: Even for systems that are broadly similar to each other and in agreement with human perception, carefully targeted experiments reveal small image distortions that are imperceptible to humans but result in reliable misclassification by a model.

How can we quantify and compare the local geometry of different image representations? A brute-force comparison clearly is prohibitive: the space of images is extremely high-dimensional, and the set of potential distortions equally high-dimensional. Estimating the local geometry of representations over a moderately dense sampling of this full set is impractical, and estimating human sensitivity to such a set is essentially impossible. As such, it is worthwhile to develop a method for judicious selection of stimulus distortions that can be used when comparing a set of models.

We take inspiration from Zhou et al. [8]. For a pair of models and a base image, they synthesize distortions along which the two models' sensitivities maximally disagree. This bears conceptual similarity to other methods that construct stimuli to optimally distinguish models [9, 10], and builds on earlierwork that examined "eigen-distortions" along which individual models are maximally/minimally sensitive [11]. Specifically, they measure the local sensitivity of a model in terms of its Fisher Information matrix (FIM) [12], a classical tool from statistical estimation theory, and choose the pair of "generalized eigen-distortions" that maximize/minimize the ratio of the two models' sensitivities. Once these image distortion have been computed, they may be added in varying amounts to a base image to determine the level at which they become visible to a human. These measured human sensitivities can then be compared to those of the models, with the goal of identifying which model is better aligned with the local geometry of the human visual system. However, when comparing more than two models, there is no principled method for selecting the distortions.

Here we define a novel metric for comparing model representations in terms of their relative sensitivities to a pair of local image distortions. We then use this metric to generate a pair of distortions that maximize the variance of two or more models. In analogy with principal component analysis, we refer to these as "principal distortions". We apply our method to a nested set of hand-crafted models of the early visual system, and to a set of standard and adversarially-trained visual neural network models. In both cases, we illustrate how the method generates novel distortions that highlight differences between models. For additional details and results, see the full paper version of this extended abstract [14].

## 2 Problem statement and methods

Given a collection of stochastic image representations, our goal is to develop a method for comparing the local geometry in the vicinity of image \(\mathbf{s}\) across these image representations. We assume that each representation has an associated conditional density \(p(\mathbf{r}|\mathbf{s})\), where \(\mathbf{s}\in\mathbb{R}^{K}\) is a vector of image pixels and \(\mathbf{r}\) is a stochastic response (e.g., neuronal firing rates or deterministic model responses with additive response noise, see Fig. 1A). Associated with the conditional density is the Fisher-Rao

Figure 1: Comparing the local geometry of image representations. **A)** Each model maps a stimulus \(\mathbf{s}\) to a distribution \(p(\mathbf{r}|\mathbf{s})\) in representation space (biological neurons are noisy while deterministic models can be made stochastic by assuming additive Gaussian response noise). Classical signal detection theory posits that the sensitivity \(d(\bm{\epsilon})\) of the representation to a distortion \(\bm{\epsilon}\) depends on how much overlap there is between \(p(\mathbf{r}|\mathbf{s})\) and \(p(\mathbf{r}|\mathbf{s}+\bm{\epsilon})\), with less overlap indicating higher sensitivity [13]. **B)** Distortion sensitivity of each model may be mapped back to the stimulus domain via the FIM, which has been used previously to generate optimal stimuli for comparison to human perception. Specifically, the eigenvectors of a model FIM may be used to examine most/least model-sensitive stimuli [11], and the generalized eigenvectors of the ratio of model FIMs may be used to generate stimuli that best distinguish the sensitivities of two models [8]. Here, we show that these models have interpretations in terms of the log ratios of model sensitivities, and develop a generalized method that allows comparison of an arbitrary number of models, by selecting two stimulus distortions that maximize the variance of the log-ratio of sensitivities over models.

metric [15; 16] (Fig. 1B), a Riemannian metric on the stimulus space defined in terms of the Fisher information matrix (FIM) [12]

\[\bm{I}(\mathbf{s}):=\mathbb{E}_{\bm{\tau}\sim p(\mathbf{r}|\mathbf{s})}\left[\nabla _{\mathbf{s}}\log p(\mathbf{r}|\mathbf{s})\nabla_{\mathbf{s}}\log p(\mathbf{r}| \mathbf{s})^{\top}\right].\]

The _sensitivity_ of the representation to distortions of stimulus \(\mathbf{s}\) in the direction \(\bm{\epsilon}\) can be expressed as:

\[d(\bm{\epsilon})=d(\mathbf{s};\bm{\epsilon}):=\sqrt{\bm{\epsilon}^{\top}\bm{I }(\mathbf{s})\bm{\epsilon}}.\] (1)

A comprehensive comparison of the local geometries of two or more image representations is impractical. Therefore, it is useful to develop a method for optimally choosing image distortions along which to compare image representations. Berardino et al. [11] proposed computing the extremal eigenvectors of model FIMs (termed "eigen-distortions", Fig. 1B) and comparing them to human visual sensitivities. However, if the eigen-distortions of two models are similar, they will not be useful in distinguishing the models, since they will be insensitive to differences in the non-extremal eigenvectors.

Comparing two image representationsZhou et al. [8] proposed comparing two image representations \(A\) and \(B\) along distortions in which their local sensitivities maximally differ. Specifically, they chose distortions to extremize the generalized Rayleigh quotient:

\[\bm{\epsilon}_{1}=\operatorname*{arg\,max}_{\bm{\epsilon}}\frac{\bm{\epsilon} ^{\top}\bm{I}_{A}(\mathbf{s})\bm{\epsilon}}{\bm{\epsilon}^{\top}\bm{I}_{B}( \mathbf{s})\bm{\epsilon}}, \bm{\epsilon}_{2}=\operatorname*{arg\,min}_{\bm{\epsilon}}\frac{ \bm{\epsilon}^{\top}\bm{I}_{A}(\mathbf{s})\bm{\epsilon}}{\bm{\epsilon}^{\top} \bm{I}_{B}(\mathbf{s})\bm{\epsilon}}.\]

Since these distortions correspond to the extremal eigenvectors of the generalized eigenvalue problem \(\bm{I}_{A}(\mathbf{s})\bm{\epsilon}=\lambda\bm{I}_{B}(\mathbf{s})\bm{\epsilon}\), we refer to them as "generalized eigen-distortions" (Fig. 1B). However, this method is limited to comparisons of pairs of models, or a single model to the average of other models.

Comparing many image representationsThe generalized eigenvalue problem induces a metric between image representations, which can be used to optimally choose image distortions for distinguishing more than two models. Specifically, up to permutation, we can express the pair of generalized eigen-distortions as the solution to the following optimization problem (Appx. A):

\[\{\bm{\epsilon}_{1},\bm{\epsilon}_{2}\}=\operatorname*{arg\,max}_{\bm{ \epsilon},\bm{\epsilon}^{\prime}}m_{\bm{\epsilon},\bm{\epsilon}^{\prime}}(A,B ),\quad\text{for}\quad m_{\bm{\epsilon},\bm{\epsilon}^{\prime}}(A,B):=\left| \log\frac{d_{A}(\bm{\epsilon})}{d_{A}(\bm{\epsilon}^{\prime})}-\log\frac{d_{B} (\bm{\epsilon})}{d_{B}(\bm{\epsilon}^{\prime})}\right|.\] (2)

For any pair of distortions \(\bm{\epsilon},\bm{\epsilon}^{\prime}\), the function \(m_{\bm{\epsilon},\bm{\epsilon}^{\prime}}(\cdot,\cdot)\) defines a _metric_ on the local geometry of image representations: it is non-negative, symmetric, and obeys the triangle inequality. The extremal distortions have several appealing properties: (i) they are invariant to arbitrary rescaling of the FIM (\(\bm{I}(\mathbf{s})\mapsto c\bm{I}(\mathbf{s})\) for any \(c>0\)), of either model; (ii) they are invariant to permutations (\(\bm{\epsilon}\leftrightarrow\bm{\epsilon}^{\prime}\)); (iii) when \(\bm{\epsilon},\bm{\epsilon}^{\prime}\) are the generalized eigen-distortions, the metric is an approximation of the Fisher-Rao distance between mean-zero Gaussian distributions with covariances \(\bm{I}_{A}(\mathbf{s})\) and \(\bm{I}_{B}(\mathbf{s})\) (Appx. B); and (iv) the metric compares stochastic representations back in stimulus space, which avoids the problem of having to align two representational spaces via a nuisance transformation [17].

We can generalize the result to optimize a pair of image distortions for distinguishing \(N>2\) image representations \(A_{1},\dots,A_{N}\). In particular, we choose \(\bm{\epsilon}_{1},\bm{\epsilon}_{2}\) to maximize the sum of the squares of all pairwise differences between the log sensitivity ratios, which is equivalent to maximizing the _variance_ of the image representations' log sensitivity ratios:

\[\{\bm{\epsilon}_{1},\bm{\epsilon}_{2}\}=\operatorname*{arg\,max}_{\bm{ \epsilon},\bm{\epsilon}^{\prime}}\sum_{n=1}^{N}\left|\log\frac{d_{A_{n}}(\bm{ \epsilon})}{d_{A_{n}}(\bm{\epsilon}^{\prime})}-\frac{1}{N}\sum_{m=1}^{N}\log \frac{d_{A_{m}}(\bm{\epsilon})}{d_{A_{m}}(\bm{\epsilon}^{\prime})}\right|^{2}\]

We refer to \(\{\bm{\epsilon}_{1},\bm{\epsilon}_{2}\}\) as the "principal distortions" of the models, analogous to principal component analysis (Fig. 1B). For a gradient-based optimization algorithm, see Appx. C.

## 3 Experiments

As a demonstration of our method, we generated principal distortions for computational models previously proposed to capture aspects of the human visual system. All models were implemented in PyTorch and simulations were performed on a NVIDIA RTX A6000 GPU. As the models are deterministic, we follow the assumptions of [11] and calculate the FIM by assuming the network output is corrupted by additive Gaussian noise, in which case \(\bm{I}(\mathbf{s})=\bm{J}_{f}(\mathbf{s})^{\top}\bm{J}_{f}(\mathbf{s})\), where \(\bm{J}_{f}(\mathbf{s})\) is the Jacobian of the model \(\bm{f}(\cdot)\) at \(\mathbf{s}\).

Early visual modelsWe generated principal distortions for a nested family of models designed to capture the early visual structure and computations (Fig. 2A,B). The full model (LGN) contains two parallel cascades representing ON and OFF channels, rectification, and both luminance and contrast gain control nonlinearities. The other models are reduced versions of this model. LGG removes the OFF channel, LG additionally removes the contrast gain control, and LN removes both gain controls. The filter size, amplitude, and normalization values were previously fit separately for each model to predict human distortion ratings [11].

To provide a qualitative comparison of each model's sensitivities to human distortion sensitivity, we adjusted the relative scaling of the principal distortion so as to be equally detectable by that model, while constraining the sum of the norms of the two distortions to be 100 (Fig. 2C). If a model's thresholds are comparable to human thresholds, then these rescaled distortions should be equally detectable when added to the image \(\mathbf{s}\). Visual inspection of these images reveals that both distortions are visible when rescaled for the LGN model and the LN model, suggesting that these models are closest to human distortion thresholds. For LG, the scaled \(\boldsymbol{\epsilon}_{2}\) distortion is not visible, while the scaled \(\boldsymbol{\epsilon}_{1}\) distortion is immediately apparent, suggesting a strong mismatch with human observer thresholds. The same is true of the LGG model, with the roles of the two distortions swapped. These qualitative observations are consistent with the results of [11], in which experiments on eigen-distortions suggested that the LGN model was the best of these models in terms of consistency with human distortion sensitivity. Formal perceptual experiments could be performed in the future to explicitly quantify the visibility of the principal distortions arising from our analysis. Crucially, measurements of human perceptual sensitivity are costly and our proposed method only requires measuring sensitivity to two distortions (in contrast to eight eigen-distortions).

Deep neural networksDeep Neural Networks (DNNs), originally developed for object recognition, have also been examined as models of the primate visual system [2, 18, 19]. A plethora of models, varying in architecture and training techniques, have been proposed, but many of these models perform quite similarly on behavioral tasks or neural benchmarks [18, 20, 21]. This situation offers a well-aligned opportunity for our principal distortion method. As a demonstration of the method, we measured the FIM of a set of layers from two different architectures (AlexNet [22] and ResNet50 [23]) and two different training procedures (standard vs. adversarial training, AT) with \(\ell_{2}\)-norm perturbations [24, 25], for a total of \(N=28\) different model representations. AT networks were initially developed for engineering purposes to reduce the vulnerability of the models to adversarial examples [26, 27], but previous work has also found that representations in AT networks are more aligned with those of biological systems [26, 25, 28]. As adversarial examples are constrained to be very small perturbations, it seems plausible that the local geometry for AT models would differ from their standard counterparts.

Figure 2: Principal distortions of a set of early visual models. **A)** Log sensitivity ratios of principal distortions and two random distortions for each of the four models. Principal distortions (filled circles) separate the log ratios, while random distortions (hollow circles) do not. **B)** Natural image \(\mathbf{s}\) and corresponding optimized principal distortions \(\{\boldsymbol{\epsilon}_{1},\boldsymbol{\epsilon}_{2}\}\). **C)** Natural image corrupted by principal distortions, with each pair scaled so as to be equally detectable by the corresponding model. Models are ordered by the log ratio of their sensitivities (panel A). If a model’s thresholds are comparable to human thresholds, the two scaled distortions should be equally visible in the top and bottom images. Note: Distorted images are best viewed at high resolution.

We show an example principal distortion generated for the set of layers from the four DNNs (Fig. 3). The hierarchical structure of the models is reflected in the log ratios of the sensitivities, where early layers of the models (smaller dots) are closer together in the metric space, and later layers of the models (larger dots) are pushed further away. Overall, most layers of the standard networks are more sensitive to the distortion that appears as unstructured noise, while most layers of the AT networks are more sensitive to the structured distortion. This qualitative example provides a demonstration that our method can be used to separate collections of similar models, and points to its utility in probing complex high-level representations.

## 4 Discussion

We introduced a metric for image representations that captures the local geometry, and used it to synthesize "principal distortions" that maximize the variance of this metric over a set of models. When applied to hand-engineered models of the early visual system and to standard and AT DNNs, our approach produced novel distortions for distinguishing the corresponding models.

The relation to the Fisher-Rao metric (Appx. B) suggests an extension for synthesizing more than two distortions. Additionally, there is a natural extension to continuous families of models. We plan to explore these directions in future work.

These distortions provide an efficient method for comparing computational models with human observers, for whom the experimental time for acquiring responses to stimuli is generally severely limited. The optimized distortions are a parsimonious choice of stimuli, that can be readily incorporated into psychophysics experiments, whose results can guide further model development.

## Acknowledgments and Disclosure of Funding

The Flatiron Institute is a division of the Simons Foundation. The computations reported in this paper were performed using resources made available by the Flatiron Institute. We additionally thank David Brainard, Thomas Yerxa, and the Laboratory for Computational Vision for their feedback.

## References

* [1] Nikolaus Kriegeskorte, Marieke Mur, and Peter A Bandettini. Representational similarity analysis-connecting the branches of systems neuroscience. _Frontiers in Systems Neuroscience_, 2:249, 2008.
* [2] Daniel LK Yamins and James J DiCarlo. Using goal-driven deep learning models to understand sensory cortex. _Nature Neuroscience_, 19(3):356-365, 2016.
* [3] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. _Advances in neural information processing systems_, 30, 2017.

Figure 3: Principal distortions for standard and adversarially trained (AT) DNNs. **A)** Log sensitivity ratios of principal distortions at a base image. **B)** Base image and the generated principal distortions. Distortion \(\bm{\epsilon}_{1}\) appears as less structured noise, and both AlexNet and ResNet50 standard networks are more sensitive to this perturbation, while the AT DNNs are more sensitive to \(\bm{\epsilon}_{2}\) which focuses color changes around the content of the image, suggesting that the differences in local sensitivities of these networks depend more on differences in training procedure than architecture.

* [4] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In _International conference on machine learning_, pages 3519-3529. PMLR, 2019.
* [5] Alex H Williams, Erin Kunz, Simon Kornblith, and Scott Linderman. Generalized shape metrics on neural representations. _Advances in Neural Information Processing Systems_, 34:4738-4750, 2021.
* [6] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. _arXiv preprint arXiv:1312.6199_, 2013.
* [7] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. _International Conference on Learning Representations_, 2014.
* [8] Jingyang Zhou, Chanwoo Chun, Ajay Subramanian, and Eero P Simoncelli. Comparing neural models using their perceptual discriminability predictions. In _Proceedings of UniReps: the First Workshop on Unifying Representations in Neural Models_, pages 170-181. PMLR, 2023.
* [9] Zhou Wang and Eero P Simoncelli. Maximum differentiation (MAD) competition: A methodology for comparing computational models of perceptual quantities. _Journal of Vision_, 8(12):8-8, 2008.
* [10] Tal Golan, Prashant C Raju, and Nikolaus Kriegeskorte. Controversial stimuli: Pitting neural networks against each other as models of human cognition. _Proceedings of the National Academy of Sciences_, 117(47):29330-29337, 2020.
* [11] Alexander Berardino, Johannes Balle, Valero Laparra, and Eero P Simoncelli. Eigen-distortions of hierarchical representations. _Advances in Neural Information Processing Systems_, 30:3531-3540, 2017.
* [12] Ronald Aylmer Fisher. Theory of statistical estimation. _Mathematical Proceedings of the Cambridge Philosophical Society_, 22(5):700-725, 1925.
* [13] D.M. Green and J.A. Swets. _Signal Detection Theory and Psychophysics_. Wiley, New York, 1966.
* [14] Jenelle Feather, David Lipshutz, Sarah E Harvey, Alex H Williams, and Eero P Simoncelli. Discriminating image representations with principal distortions. _arXiv preprint arXiv:2410.15433_, 2024.
* [15] C R Rao. Information and accuracy attainable in the estimation of statistical parameters. _Bulletin of the Calcutta Mathematical Society_, 37(3):81-91, 1945.
* [16] Shun-ichi Amari. _Information Geometry and its Applications_, volume 194. Springer, 2016.
* [17] Lyndon Duong, Jingyang Zhou, Josue Nassar, Jules Berman, Jeroen Olieslagers, and Alex H Williams. Representational dissimilarity metric spaces for stochastic neural networks. In _The Eleventh International Conference on Learning Representations_, 2023.
* [18] Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J Majaj, Rishi Rajalingham, Elias B Issa, Kohitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska Geiger, et al. Brain-score: Which artificial neural network for object recognition is most brain-like? _BioRxiv_, page 407007, 2018.
* [19] Grace W Lindsay. Convolutional neural networks as a model of the visual system: Past, present, and future. _Journal of cognitive neuroscience_, 33(10):2017-2031, 2021.
* [20] Greta Tuckute, Jenelle Feather, Dana Boebinger, and Josh H McDermott. Many but not all deep neural network audio models capture brain responses and exhibit correspondence between model stages and brain regions. _Plos Biology_, 21(12):e3002366, 2023.
* [21] Colin Conwell, Jacob S Prince, Kendrick N Kay, George A Alvarez, and Talia Konkle. What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines? _BioRxiv_, pages 2022-03, 2022.

* Krizhevsky et al. [2012] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Advances in neural information processing systems_, 25, 2012.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Engstrom et al. [2019] Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness (python library), 2019. URL https://github.com/MadryLab/robustness.
* Feather et al. [2023] Jenelle Feather, Guillaume Leclerc, Aleksander Madry, and Josh H McDermott. Model metamers reveal divergent invariances between biological and artificial neural networks. _Nature Neuroscience_, 26(11):2017-2034, 2023.
* Madry [2017] Aleksander Madry. Towards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_, 2017.
* Engstrom et al. [2019] Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Aleksander Madry. Adversarial robustness as a prior for learned representations. _arXiv preprint arXiv:1906.00945_, 2019.
* Gaziv et al. [2023] Guy Gaziv, Michael J Lee, and James J DiCarlo. Robustified anns reveal wormholes between human category percepts. _arXiv preprint arXiv:2308.06887_, 2023.

## Appendix A Deriving the metric from a generalized eigenvalue problem

Our starting point is the following expression for the generalized eigen-distortions:

\[\bm{\epsilon}_{1}=\operatorname*{arg\,max}_{\bm{\epsilon}}\frac{ \bm{\epsilon}^{\top}\bm{I}_{A}(\mathbf{s})\bm{\epsilon}}{\bm{\epsilon}^{\top} \bm{I}_{B}(\mathbf{s})\bm{\epsilon}}, \bm{\epsilon}_{2}=\operatorname*{arg\,min}_{\bm{\epsilon}} \frac{\bm{\epsilon}^{\top}\bm{I}_{A}(\mathbf{s})\bm{\epsilon}}{\bm{\epsilon}^{ \top}\bm{I}_{B}(\mathbf{s})\bm{\epsilon}}.\]

From the definition for the sensitivity \(d(\bm{\epsilon})\) in Equation 1 and the monotonicty of the square root and logarithm operations, we can express the generalized eigen-distortions as follows:

\[\bm{\epsilon}_{1}=\operatorname*{arg\,max}_{\bm{\epsilon}}\log \frac{d_{A}(\bm{\epsilon})}{d_{B}(\bm{\epsilon})}, \bm{\epsilon}_{2}=\operatorname*{arg\,min}_{\bm{\epsilon}}\log \frac{d_{A}(\bm{\epsilon})}{d_{B}(\bm{\epsilon})}.\]

Up to permutation, these distortions can be expressed as the solution of the following optimization problem:

\[\{\bm{\epsilon}_{1},\bm{\epsilon}_{2}\}=\operatorname*{arg\,max} _{\bm{\epsilon},\bm{\epsilon}^{\prime}}\left|\log\frac{d_{A}(\bm{\epsilon})}{ d_{B}(\bm{\epsilon})}-\log\frac{d_{A}(\bm{\epsilon}^{\prime})}{d_{B}(\bm{ \epsilon}^{\prime})}\right|,\]

which can be rearranged to produce Equation 2.

## Appendix B Relation to the Fisher-Rao metric

The Fisher-Rao distance between two mean zero Gaussian with covariances \(\bm{A}\) and \(\bm{B}\) is equal to

\[\delta^{2}(\bm{A},\bm{B}):=\|\log(\bm{B}^{-1/2}\bm{A}\bm{B}^{-1/2})\|_{F}^{2}= \sum_{i=1}^{K}(\log\lambda_{i})^{2},\]

where \(\{\lambda_{i}\}\) denote the eigenvalues of the generalized eigenvalue problem \(\bm{Av}=\lambda\bm{Bv}\). We'd like a metric that's invariant to scaling \(\bm{A}\) or \(\bm{B}\), which suggests using the metric

\[\gamma^{2}(\bm{A},\bm{B})=\min_{c_{A},c_{B}>0}\delta^{2}(c_{A}\bm{A},c_{B}\bm{ B})=\min_{c\in\mathbb{R}}\sum_{i=1}^{K}(c+\log\lambda_{i})^{2}=K\text{Var}(\{ \log\lambda_{i}\}),\]

where the final equality uses the fact that the optimal \(c\) is the mean of \(\{-\log\lambda_{i}\}\).

Using the facts that

\[K\text{Var}(\{\log\lambda_{i}\})=\frac{1}{2K}\sum_{i=1}^{K}\sum_{j=1}^{K}(\log \lambda_{i}-\log\lambda_{j})^{2},\]

and \((\log\lambda_{i}-\log\lambda_{j})^{2}\leq(\log\lambda_{1}-\log\lambda_{K})^{2}\) for all \(i,j\), we have

\[\frac{1}{K}(\log\lambda_{1}-\log\lambda_{K})^{2}\leq\gamma^{2}( \boldsymbol{A},\boldsymbol{B})\leq\frac{K-1}{2}(\log\lambda_{1}-\log\lambda_{K })^{2}.\]

When \(\boldsymbol{A}=\boldsymbol{I}_{A}\) and \(\boldsymbol{B}=\boldsymbol{I}_{B}\), then \(d_{A}(\boldsymbol{\epsilon})=\sqrt{\boldsymbol{\epsilon}^{\top}\boldsymbol{A }\boldsymbol{\epsilon}}\) and \(d_{B}(\boldsymbol{\epsilon})=\sqrt{\boldsymbol{\epsilon}^{\top}\boldsymbol{ B}\boldsymbol{\epsilon}}\). If \(\boldsymbol{\epsilon}_{1}\) and \(\boldsymbol{\epsilon}_{K}\) denote the extremal generalized eigenvectors associated with \(\lambda_{1}\) and \(\lambda_{K}\), respectively, then

\[\log\lambda_{1} =2\log\frac{d_{A}(\boldsymbol{\epsilon}_{1})}{d_{B}(\boldsymbol{ \epsilon}_{1})}, \log\lambda_{K} =2\log\frac{d_{A}(\boldsymbol{\epsilon}_{K})}{d_{B}(\boldsymbol{ \epsilon}_{K})}.\]

Therefore,

\[\frac{2}{\sqrt{K}}\left|\log\frac{d_{A}(\boldsymbol{\epsilon}_{1})}{d_{B}( \boldsymbol{\epsilon}_{1})}-\log\frac{d_{A}(\boldsymbol{\epsilon}_{K})}{d_{B}( \boldsymbol{\epsilon}_{K})}\right| \leq\gamma(\boldsymbol{A},\boldsymbol{B})\leq\sqrt{2(K-1)}\left|\log\frac{d _{A}(\boldsymbol{\epsilon}_{1})}{d_{B}(\boldsymbol{\epsilon}_{1})}-\log\frac{d _{A}(\boldsymbol{\epsilon}_{K})}{d_{B}(\boldsymbol{\epsilon}_{K})}\right|,\]

and so

\[\frac{2}{\sqrt{K}}m_{\boldsymbol{\epsilon}_{1},\boldsymbol{ \epsilon}_{K}}(\boldsymbol{I}_{A},\boldsymbol{I}_{B}) \leq\gamma(\boldsymbol{A},\boldsymbol{B})\leq\sqrt{2(K-1)}m_{\boldsymbol{ \epsilon}_{1},\boldsymbol{\epsilon}_{K}}(\boldsymbol{I}_{A},\boldsymbol{I}_{B }).\]

## Appendix C Computing the top two optimal distortions

Suppose we have \(N\) models with sensitivities \(\{d_{n}(\boldsymbol{\epsilon})\}\). The optimal distortions \(\{\boldsymbol{\epsilon}_{1},\boldsymbol{\epsilon}_{2}\}\) are solutions to the optimization problem

\[\operatorname*{arg\,max}_{\boldsymbol{\epsilon}_{1},\boldsymbol{ \epsilon}_{2}}L(\boldsymbol{\epsilon}_{1},\boldsymbol{\epsilon}_{2}),\qquad \quad L(\boldsymbol{\epsilon}_{1},\boldsymbol{\epsilon}_{2}):=\sum_{n=1}^{N} \left\{\log\frac{d_{n}(\boldsymbol{\epsilon}_{1})}{d_{n}(\boldsymbol{ \epsilon}_{2})}-\frac{1}{N}\sum_{m=1}^{N}\log\frac{d_{m}(\boldsymbol{\epsilon} _{1})}{d_{m}(\boldsymbol{\epsilon}_{2})}\right\}^{2}.\]

Differentiating \(L\) with respect to \(\boldsymbol{\epsilon}_{1}\) yields

\[\nabla_{\boldsymbol{\epsilon}_{1}}L(\boldsymbol{\epsilon}_{1}, \boldsymbol{\epsilon}_{2}) =2\sum_{n=1}^{N}\left\{\log\frac{d_{n}(\boldsymbol{\epsilon}_{1} )}{d_{n}(\boldsymbol{\epsilon}_{2})}-\frac{1}{N}\sum_{m=1}^{N}\log\frac{d_{m}( \boldsymbol{\epsilon}_{1})}{d_{m}(\boldsymbol{\epsilon}_{2})}\right\}\left\{ \frac{\boldsymbol{I}_{n}(\mathbf{s})\boldsymbol{\epsilon}_{1}}{d_{n}^{2}( \boldsymbol{\epsilon}_{1})}-\frac{1}{N}\sum_{m=1}^{N}\frac{\boldsymbol{I}_{m}( \mathbf{s})\boldsymbol{\epsilon}_{1}}{d_{m}^{2}(\boldsymbol{\epsilon}_{1})}\right\}\] \[=\sum_{n=1}^{N}\left\{\log\frac{d_{n}^{2}(\boldsymbol{\epsilon}_{1 })}{d_{n}^{2}(\boldsymbol{\epsilon}_{2})}-\frac{1}{N}\sum_{m=1}^{N}\log\frac{d_ {m}^{2}(\boldsymbol{\epsilon}_{1})}{d_{m}^{2}(\boldsymbol{\epsilon}_{2})} \right\}\left\{\frac{\boldsymbol{I}_{n}(\mathbf{s})\boldsymbol{\epsilon}_{1}}{ d_{n}^{2}(\boldsymbol{\epsilon}_{1})}-\frac{1}{N}\sum_{m=1}^{N}\frac{\boldsymbol{I}_{m}( \mathbf{s})\boldsymbol{\epsilon}_{1}}{d_{m}^{2}(\boldsymbol{\epsilon}_{1})} \right\},\]

where we have used the fact that

\[\nabla_{\boldsymbol{\epsilon}}\log d(\boldsymbol{\epsilon})=\frac{1}{2}\nabla _{\boldsymbol{\epsilon}}\log(\boldsymbol{\epsilon}^{\top}\boldsymbol{I} \boldsymbol{\epsilon})=\frac{\boldsymbol{I}\boldsymbol{\epsilon}}{\boldsymbol{ \epsilon}^{\top}\boldsymbol{I}\boldsymbol{\epsilon}}=\frac{\boldsymbol{I} \boldsymbol{\epsilon}}{d^{2}(\boldsymbol{\epsilon})}.\]

Similarly, differentiating \(L\) with respect to \(\boldsymbol{\epsilon}_{2}\) yields:

\[\nabla_{\boldsymbol{\epsilon}_{2}}L(\boldsymbol{\epsilon}_{1}, \boldsymbol{\epsilon}_{2}) =-\sum_{n=1}^{N}\left\{\log\frac{d_{n}^{2}(\boldsymbol{\epsilon}_{1})}{d_{n}^ {2}(\boldsymbol{\epsilon}_{2})}-\frac{1}{N}\sum_{m=1}^{N}\log\frac{d_{m}^{2}( \boldsymbol{\epsilon}_{1})}{d_{m}^{2}(\boldsymbol{\epsilon}_{2})}\right\} \left\{\frac{\boldsymbol{I}_{n}(\mathbf{s})\boldsymbol{\epsilon}_{2}}{d_{n}^{2}( \boldsymbol{\epsilon}_{2})}-\frac{1}{N}\sum_{m=1}^{N}\frac{\boldsymbol{I}_{m}( \mathbf{s})\boldsymbol{\epsilon}_{2}}{d_{m}^{2}(\boldsymbol{\epsilon}_{2})} \right\}.\]

Combining, we have the following gradient-based optimization algorithm.

```
1:Input: Positive definite \(D\times D\) matrices \(\bm{I}_{1},\ldots,\bm{I}_{N}\), learning rate \(\eta>0\)
2:Initialize:\(\bm{\epsilon}_{1},\bm{\epsilon}_{2}\in\mathbb{R}^{D}\)
3:while not converged do
4:for\(n=1,\ldots,N\)do
5:\(\bm{v}_{1}(n)\leftarrow\bm{I}_{n}\bm{\epsilon}_{1}\)
6:\(\bm{v}_{2}(n)\leftarrow\bm{I}_{n}\bm{\epsilon}_{2}\)
7:\(d_{2}^{2}(n)\leftarrow\langle\bm{\epsilon}_{1},\bm{v}_{1}(n)\rangle\)
8:\(d_{2}^{2}(n)\leftarrow\langle\bm{\epsilon}_{2},\bm{v}_{2}(n)\rangle\)
9:\(\bm{u}_{1}(n)=\bm{v}_{1}(n)/d_{2}^{2}(n)\)
10:\(\bm{u}_{2}(n)=\bm{v}_{2}(n)/d_{2}^{2}(n)\)
11:\(r(n)\leftarrow\log d_{1}^{2}(n)-\log d_{2}^{2}(n)\)
12:endfor
13:\(\bar{\bm{u}}_{1}\leftarrow\text{mean}(\bm{u}_{1}(n))\)
14:\(\bar{\bm{u}}_{2}\leftarrow\text{mean}(\bm{u}_{2}(n))\)
15:\(\bar{r}\leftarrow\text{mean}(r(n))\)
16:\(\Delta\bm{\epsilon}_{1}\leftarrow\sum_{n=1}^{N}\left[r(n)-\bar{r}\right]\left[ \bm{u}_{1}(n)-\bar{\bm{u}}_{1}\right]\)
17:\(\Delta\bm{\epsilon}_{2}\leftarrow-\sum_{n=1}^{N}\left[r(n)-\bar{r}\right]\left[ \bm{u}_{2}(n)-\bar{\bm{u}}_{2}\right]\)
18:\(\bm{\epsilon}_{1}\leftarrow\bm{\epsilon}_{1}+\eta\Delta\bm{\epsilon}_{1}\)
19:\(\bm{\epsilon}_{2}\leftarrow\bm{\epsilon}_{2}+\eta\Delta\bm{\epsilon}_{2}\)
20:\(\bm{\epsilon}_{1}\leftarrow\bm{\epsilon}_{1}/\|\bm{\epsilon}_{1}\|\)
21:\(\bm{\epsilon}_{2}\leftarrow\bm{\epsilon}_{2}/\|\bm{\epsilon}_{2}\|\)
22:endwhile ```

**Algorithm 1**Computing the top two distortions