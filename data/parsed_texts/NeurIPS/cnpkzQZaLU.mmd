# Context-PIPs: Persistent Independent Particles

Demands Spatial Context Features

 Weikang Bian\({}^{1,2}\)1 Zhaoyang Huang\({}^{1}\)1 Xiaoyu Shi\({}^{1}\)

**Yitong Dong\({}^{3}\) Yijin Li\({}^{3}\) Hongsheng Li\({}^{1,2}\)**

\({}^{1}\)CUHK MMLab \({}^{2}\)Centre for Perceptual and Interactive Intelligence \({}^{3}\)Zhejiang University

wkbian@outlook.com, drinkingcoder@link.cuhk.edu.hk, hsli@ee.cuhk.edu.hk

Footnote 1: Weikang Bian and Zhaoyang Huang assert equal contributions.

###### Abstract

We tackle the problem of Persistent Independent Particles (PIPs), also called Tracking Any Point (TAP), in videos, which specifically aims at estimating persistent long-term trajectories of query points in videos. Previous methods attempted to estimate these trajectories independently to incorporate longer image sequences, therefore, ignoring the potential benefits of incorporating spatial context features. We argue that independent video point tracking also demands spatial context features. To this end, we propose a novel framework Context-PIPs, which effectively improves point trajectory accuracy by aggregating spatial context features in videos. Context-PIPs contains two main modules: 1) a SOurse Feature Enhancement (SOFE) module, and 2) a TArget Feature Aggregation (TAFA) module. Context-PIPs significantly improves PIPs all-sided, reducing 11.4% Average Trajectory Error of Occluded Points (ATE-Occ) on CroHD and increasing 11.8% Average Percentage of Correct Keypoint (A-PCK) on TAP-Vid-Kinetics. Demos are available at https://wkbian.github.io/Projects/Context-PIPs/.

## 1 Introduction

Video particles are a set of sparse point trajectories in a video that originate from the first frame (the source image) and move across the following frames, which are regarded as the target images. In contrast to optical flow estimation that computes pixel-wise correspondences between a pair of adjacent video frames, Persistent Independent Particles (PIPs) [12] or Tracking Any Point (TAP) [5] is interested in tracking the points in the follow-up frames that correspond to the original query points even when they are occluded in some frames. Video particles provide long-term motion information for videos and can support various downstream tasks, such as video editing [16] and Structure-from-Motion [48].

Long-range temporal information is essential for video particles especially when the particles are occluded because the positions of the occluded particles can be inferred from the previous and subsequent frames where they are visible. However, simultaneously encoding long image sequences brings larger computational and memory costs. Previous methods [12; 5] learn to track individual points independently because dense video particles are unnecessary in most scenarios. Inspired by optical flow estimation from visual similarities, they learn to predict point trajectories from the similarities between the query point and the subsequent target images. Specifically, given a query point at the source image, PIPs encodes \(T\) feature maps from \(T\) consecutive target images and builds a \(T\times H\times W\) correlation volume by computing the feature similarity between the feature of the query point and the feature maps. The \(T\) particle positions are iteratively refined with the 3D correlation volume through a shared MLP-Mixer [40]. In other words, PIPs trades the spatial context features ofthe particle for longer temporal feature encoding. PIPs achieves great performance on the DAVIS dataset, which contains large movement particles and weak texture images (e.g., fast-moving dogs and black bears).

We argue that independent point tracking still demands spatial context features. Intuitively, although PIPs only accounts for specified query points, spatial context features around them provide informative cues for point trajectory refinement. For example, video particles on the same objects always share similar motions over time. In some video frames where the target particles are occluded, their surrounding particles may be visible and provide guidance for the position estimation of the target particles. However, PIPs only takes correlations and features belonging to the particles while ignoring abundant spatial context features around them. In this work, we propose tracking particles with Context (Context-PIPs) to improve independent point tracking with spatial context features. Context-PIPs contains two key modules for better point trajectory refinement: 1) a source feature enhancement (SOFE) module that learns to adopt more spatial context features in the source image and builds a guidance correlation volume, and 2) a target feature aggregation (TAFA) module that aggregates spatial context features in the target image guided by the correlation information.

In the source image, points that possess similar appearances are supposed to move in similar trajectories in subsequent frames. Such an assumption has also been used in GMA [21] for optical flow estimation. Given a query point, SOFE computes the correlation between the query point and the source image feature map, which is its self-similarity map. With the guidance of the correlation (self-similarity), SOFE predicts \(M\) offsets centered from the query point, and samples at the corresponding \(M\) auxiliary points to collect source context features. During the iterative point trajectory refinement, the correlation information between the \(M\) auxiliary features and \(T\) target feature maps is injected into the MLP-Mixer, which provides strong guidance and shows evident performance improvement.

Existing methods for optical flow and video particle estimation ignore features in target images for iterative refinement. To better utilize the context of target images, in each iteration, our TAFA module collects target features surrounding the previous iteration's resulting movements. TAFA for the first time shows that context features in target images also benefit correspondence estimation and further improve the point tracking accuracy.

Our contributions can be summarized as threefold: 1) We propose a novel framework, Context-PIPs, to improve independent video particle tracking with context features from both source and target features. Context-PIPs ranks 1st on the four benchmarks and shows clear performance superiority. 2) We design a novel SOurce Feature Enhancement (SOFE) module that builds a guidance correlation volume with spatial context features in the source image, and 3) a novel TAget Feature Aggregation (TAFA) module that extracts context features from target images.

## 2 Related Work

**Optical Flow.** Optical flow estimates the dense displacement field between image pairs and has traditionally been modeled as an optimization problem that maximizes the visual similarity between image pairs with regularizations [13; 1; 2; 34]. It serves as the core module of many downstream applications, such as Simultaneously Localization And Mapping (SLAM) [8; 47; 27; 43; 9], video synthesis [46; 14; 15; 42; 41], video restoration [24; 23], etc. Since FlowNet [6; 20], learning optical flow with neural networks presents superior performance over traditional optimization-based methods and is fast progressing with more training data obtained by the renderer and better network architecture [6; 20; 28; 35; 36; 18; 19; 45]. In recent years, iterative refining flow with all-pairs correlation volume presents the best performance. The most successful network designs are RAFT [39] and FlowFormer [17; 32], which achieves state-of-the-art accuracy.

Typical optical flow estimation only takes image pairs but longer image sequences can provide more information that benefits optical flow estimation. Kalman filter [4; 7] had been adopted in dealing with the temporal dynamics of motion and estimating multi-frame optical flow. Recent learning-based methods also attempted to exploit multi-frame information and perform multi-frame optical flow estimation. PWC-Fusion [29] is the first method that learns to estimate optical flow from multiple images. However, it only fuses information from previous frames in U-Net and improves little performance. The "warm start" technique [39; 33; 37] that wrapped the previous flow to initialize the next flow is firstly proposed in RAFT and shows clear accuracy increasing. VideoFlow [31] takes multi-frame cues better, iteratively fusing multi-frame information in a three-frame and five frame structure, which reveals that longer temporal information benefits pixel tracking. Recently, researchers [25; 10] also tried to learn to estimate optical flow with event cameras.

**Tracking Any Point.** Optical flow methods merely focus on tracking points between image pairs but ignore tracking points across multiple consecutive frames, which is still challenging. Harley _et al._[12] studied pixel tracking in the video as a long-range motion estimation problem inspired by Particle Video [30]. They propose a new dataset FlyingThings++ based on FlyingThings [26] for training and Persistent Independent Particles (PIPs) to learn to track single points in consecutive frames with fixed lengths. Doersch _et al._[5] is a parallel work, which formalized the problem as tracking any point(TAP). They also propose a new dataset Kubric [5] for training and a network TAP-Net to learn point tracking. Moreover, they provide the real video benchmarks that are labeled by humans, TAP-Vid-DAVIS [5] and TAP-Vid-Kinetics [5], for evaluation. PIPs and TAP solve the video particle tracking problem in a similar manner, i.e., recurrently refining multi-frame point trajectory via correlation maps. In this paper, our Context-PIPs follows the training paradigm of PIPs and improves the network architecture design of PIPs. We also take the TAP-Vid-DAVIS and TAP-Vid-Kinetics benchmarks from TAP-Net for evaluation.

## 3 Method

In contrast to optical flow methods [39; 17] that track dense pixel movement between an image pair, the problem of point tracking takes \(T\) consecutive RGB images with a single query point \(\mathbf{x}_{src}\in\mathbb{R}^{2}\) at the first frame as input, and estimates \(T\) coordinates \(\mathbf{X}=\{\mathbf{x}_{0},\mathbf{x}_{1},\ldots,\mathbf{x}_{T-1}\}\) at the video frames where every \(\mathbf{x}_{t}\) indicates the point's corresponding location at time \(t\). Persistent Independent Particles (PIPs) [12] is the state-of-the-art network architecture for TAP. It iteratively refines the point trajectory by encoding correlation information that measures visual similarities between the query point and the \(T\) video frames. The query points to be tracked are easily lost when the network only looks at them and ignores spatial context features. We propose a novel framework Context-PIPs (Fig. 1) that improves PIPs with a SOurce Feature Enhancement (SOFE) module and a TArget Feature Aggregation (TAFA) module. In this section, we first briefly review PIPs and then elaborate on our Context-PIPs.

### A Brief Revisit of PIPs

PIPs [12] processes \(T\) video frames containing \(N\) independent query points simultaneously and then extends the point trajectories to more video frames via chaining rules [12]. Given a source frame with a query point \(\mathbf{x}_{src}\in\mathbb{R}^{2}\) and \(T-1\) follow-up target video frames, PIPs first extracts their feature maps \(\mathbf{I}_{0},\mathbf{I}_{1},\ldots,\mathbf{I}_{T-1}\in\mathbb{R}^{C\times H \times W}\) through a shallow convolutional neural network and bilinearly samples to obtain the source point feature \(\mathbf{f}_{src}=\mathbf{I}_{0}(\mathbf{x}_{src})\) from the first feature map at the query point \(\mathbf{x}_{src}\). \(C,H,W\) are the feature map channels, height, and width. Inspired by RAFT [39], PIPs initializes the point trajectory and point visual features at each frame with the same \(\mathbf{x}_{src}\) and \(\mathbf{f}_{src}\):

\[\mathbf{X}^{0} =\{\mathbf{x}_{0}^{0},\mathbf{x}_{1}^{0},\ldots,\mathbf{x}_{T-1} ^{0}|\mathbf{x}_{t}^{0}=\mathbf{x}_{src},t=0,\ldots,t=T-1\},\] (1) \[\mathbf{F}^{0} =\{\mathbf{f}_{0}^{0},\mathbf{f}_{1}^{0},\ldots,\mathbf{f}_{T-1} ^{0}|\mathbf{f}_{t}^{0}=\mathbf{f}_{src},t=0,\ldots,t=T-1\},\]

and iteratively refines them via correlation information. \(\mathbf{x}_{t}^{k}\) and \(\mathbf{f}_{t}^{k}\) respectively denote the point trajectory and point features in the \(t\)-th frame and \(k\)-th iteration. Intuitively, the point features store the visual feature at the currently estimated query point location in all the \(T\) frames.

Specifically, in each iteration \(k\), PIPs constructs multi-scale correlation maps [39] between the guidance feature \(\{\mathbf{f}_{t}^{k}\}_{t=0}^{T-1}\) and the target feature maps \(\{\mathbf{I}_{t}^{k}\}_{t=0}^{T-1}\), which constitutes \(T\) correlation maps \(\mathbf{C}^{k}=\{\mathbf{c}_{0}^{k},\mathbf{c}_{1}^{k},\ldots,\mathbf{c}_{T-1} ^{k}\}\) of size \(T\times H\times W\), and crops correlation information inside the windows centered at the point trajectory: \(\mathbf{C}^{k}(\mathbf{X}^{k})=\{\mathbf{c}_{0}^{k}(\mathbf{x}_{0}^{k}), \mathbf{c}_{1}^{k}(\mathbf{x}_{1}^{k}),\ldots,\mathbf{c}_{T-1}^{k}(\mathbf{x}_ {T-1}^{k})\}\), where \(\mathbf{c}_{t}^{k}(\mathbf{x}_{t}^{k})\in\mathbb{R}^{D\times D}\) denotes that we crop \(D\times D\) correlations from \(\mathbf{c}_{t}^{k}\) inside the window centered at \(\mathbf{x}_{t}^{k}\). The point features \(\mathbf{F}^{k}\), point locations \(\mathbf{X}^{k}\), and the local correlation information \(\mathbf{C}^{k}(\mathbf{X}^{k})\) are fed into a standard 12-layer MLP-Mixer that produces \(\Delta\mathbf{F}\) and \(\Delta\mathbf{X}\) to update the point feature and the point trajectory:

\[\Delta\mathbf{F},\;\Delta\mathbf{X} =\mathrm{MLPMixer}(\mathbf{F}^{k},\mathbf{C}^{k}(\mathbf{X}^{k}), \mathrm{Enc}(\mathbf{X}^{k}-\mathbf{x}_{src})),\] (2) \[\mathbf{F}^{k+1} =\mathbf{F}^{k}+\Delta\mathbf{F},\;\mathbf{X}^{k+1}=\mathbf{X}^{k} +\Delta\mathbf{X}.\]PIPs iterates \(K\) times for updates and the point trajectory in the last iteration \(\mathbf{X}^{K}\) is the output.

PIPs achieves state-of-the-art accuracy on point tracking by utilizing longer temporal features. However, the previous method ignores informative spatial context features which are beneficial to achieve more accurate point tracking. Context-PIPs keeps all modules in PIPs and is specifically designed to enhance the correlation information \(\mathbf{C}^{k}\) and point features \(\mathbf{F}^{k}\) as \(\mathbf{\hat{C}}^{k}\) and \(\mathbf{\hat{F}}^{k}\) with the proposed SOFE and TAFA.

### Source Feature Enhancement

Given the query point \(\mathbf{x}_{src}\) and feature map \(\mathbf{I}_{0}\) of the source image, PIPs simply samples a source feature \(\mathbf{f}_{src}\) at the query point location to obtain the point visual features \(\mathbf{F}^{k}\). Although the point features are updated via the iterative refinement, its perceptive field is limited to the single point, easily compromised in harsh scenarios. The correlation maps \(\mathbf{C}^{k}\) in the \(k\)-th iteration provide vague information when the query point is in a less textured area. Moreover, the correlation map \(\mathbf{c}^{k}_{t}\) at timestamp \(k\) is ineffective once the particle is occluded at the \(t\)-th frame. To enhance the source feature, as shown in Fig. 1, we propose SOurce Feature Enhancement (SOFE) that accepts spatial context features in the source image as auxiliary features to guide the point trajectory refinement. The MLP-Mixer can infer the point locations via the auxiliary features even when the points are occluded or on less textured regions, which improves the point tracking accuracy and robustness.

Directly adopting all features in the source image brings large computational costs. SOFE learns to sample a small number of auxiliary features to enhance the source feature. Specifically, SOFE improves the point features in three steps. Firstly, SOFE learns to predict \(M\) offsets \(\delta\mathbf{x}_{0},\delta\mathbf{x}_{1},\dots,\delta\mathbf{x}_{M-1}\in \mathbb{R}^{2}\) with an MLP-based sampler to sample \(M\) auxiliary features \(G=\{\mathbf{g}_{0},\mathbf{g}_{1},\dots,\mathbf{g}_{M-1}|\mathbf{g}_{m}= \mathbf{I}_{0}(\mathbf{x}_{src}+\delta\mathbf{x}_{m})\}\) around the query point \(\mathbf{x}_{src}\) in the source image. Motivated by GMA that aggregates pixel flows from pixels that are likely to belong to the same object through self-similarity, our proposed sampler also learns the locations of the auxiliary features based on local self-similarities \(\mathbf{c}^{0}_{0}(\mathbf{x}_{src})\) which store the correlations cropped from the first frame at the query point location. Secondly, we construct the correlation map \(\mathbf{c}^{\prime}_{m,t}=<\mathbf{g}_{m},\mathbf{I}_{t}>\in\mathbb{R}^{H\times W}\) that measure visual similarities of the \(m\)-th auxiliary feature and the \(t\)-th frame feature map. \(\hat{\mathbf{c}}_{m,t}\) provides additional correlation information to guide the iterative point trajectory refinement. In each iteration \(k\), we crop the additional correlation information \(\mathbf{c}^{\prime}_{m}(\mathbf{x}^{k}_{t})\) according to the point locations

Figure 1: Overview of Context-PIPs Pipeline. Our Context-PIPs improves PIPs [12] with SOurce Feature Enhancement (SOFE) and TAFget Feature Aggregation (TAFA). PIPs iteratively refines the point trajectory \(\mathbf{X}^{k}\) with an MLP-Mixer with the current point trajectory \(\mathbf{X}^{k}\), the correlation features \(\mathbf{C}^{k}\), and the point features \(\mathbf{F}^{k}\). SOFE and TAFA respectively improves the correlation features and point features, denoted as \(\mathbf{\hat{C}}^{k}\) and \(\mathbf{\hat{F}}^{k}\).

\(\mathbf{x}_{t}^{k}\) and concatenate them with the original point correlation information \(\mathbf{c}_{t}^{k}(\mathbf{x}_{t}^{k})\), where \(\mathbf{c}_{m}^{\prime}(\mathbf{x}_{t}^{k})\) denotes the same cropping operation as \(\mathbf{c}_{t}^{k}(\mathbf{x}_{t}^{k})\). Finally, for each frame \(t\), we reduce the augmented correlations to a correlation feature vector \(\hat{\mathbf{c}}_{t}\) of length 196 through a correlation encoder CorrEnc.

\[\hat{\mathbf{c}}_{t}^{k}=\mathrm{CorrEnc}(\mathrm{Concat}(\mathbf{c}_{0}^{ \prime}(\mathbf{x}_{t}^{k}),\mathbf{c}_{1}^{\prime}(\mathbf{x}_{t}^{k}),\dots, \mathbf{c}_{M-1}^{\prime}(\mathbf{x}_{t}^{k}),\mathbf{c}_{t}^{k}(\mathbf{x}_{t }^{k}))),\] (3)

and inject \(\hat{\mathbf{C}}^{k}=\{\hat{\mathbf{c}}_{0}^{k},\hat{\mathbf{c}}_{1}^{k}, \dots,\hat{\mathbf{c}}_{T-1}^{k},\}\) into the MLP-Mixer. Compared with PIPs that only adopts \(\mathbf{c}_{t}^{k}(\mathbf{x}_{t}^{k}))\), SOFE provides more informative correlations to the MLP-Mixer with spatial context features but does not increase its parameters and computations. The additional auxiliary features from the self-similarity map of the source image enhance the original source point features and significantly improves tracking accuracy.

### Target Feature Aggregation

Inspired by existing optical flow methods, PIPs iteratively refines the point trajectory with correlation information and context features and also iteratively updates the point visual feature \(\mathbf{F}^{k+1}=\mathbf{F}^{k}+\Delta\mathbf{F}\) after initializing them with the source feature, which presents benefits for point trajectory refinement. We observe that the input for supporting the point feature updating comes from the correlations \(\mathbf{C}^{k}\) only. However, such correlations \(\mathbf{C}^{k}\) are calculated as only cosine distances between the source point visual feature \(\mathbf{F}^{k}\) and the target features around currently estimated point locations \(\mathbf{X}^{k}\), which provide limited information on how to conduct visual feature updating. Can we better guide the point feature update with context features in target images?

We, therefore, propose TArget Feature Aggregation (TAFA) to augment point features with target image features nearby the point trajectory. Specifically, for each target frame \(t\), a patch of shape \(D\times D\) cropped from the corresponding target feature map \(\mathbf{I}_{t}\) centered at \(\mathbf{x}_{t}^{k}\) to generate key and value. The augmented correlation features \(\hat{\mathbf{C}}\) in Eq. 3 encode abundant visual similarities. Therefore, we generate a query from it to extract target context features and adopt cross-attention with relative positional encoding to obtain the target context feature \(\mathbf{f}^{\prime}_{t}^{k}\), which is added to the original source point feature \(\hat{\mathbf{f}}_{t}^{k}=\mathbf{f}_{t}^{k}+\mathbf{f}^{\prime}_{t}^{k}\). Finally, such augmented point features \(\hat{\mathbf{F}}^{k}=\{\hat{\mathbf{f}}_{0}^{k},\hat{\mathbf{f}}_{1}^{k}, \dots,\hat{\mathbf{f}}_{T-1}^{k}\}\) are injected into the MLP-Mixer. Similar to our proposed SOFE, TAFA also keeps the same parameters and computations of MLP-Mixer as PIPs while providing additional target context features and further improving PIPs. Although context features in the source image are used since RAFT [39], no previous methods adopt context features from target images. TAFA for the first time validates that target images also contain critical context features that benefit point movement refinement. SOFE improves PIPs with auxiliary features in the source image while TAFA absorbs more target image features. Equipping SOFE and TAFA to PIPs constitutes our final model, Context-PIPs.

### Loss Functions

We compute the L1 distance between \(\mathbf{X}^{k}\) computed in iteration \(k\) and the ground truth \(\mathbf{X}_{gt}\) and constrain with exponentially increasing weights \(\gamma=0.8\).

\[\mathcal{L}_{TAP}=\sum_{k=1}^{K}\gamma^{K-k}||\mathbf{X}^{k}-\mathbf{X}_{gt}||_ {1}\] (4)

In addition, we will predict the visibility/occlusion \(\mathbf{V}\) by a linear layer according to the \(\hat{\mathbf{F}}^{K}\) obtained by the iterative update. And the cross entropy loss is used to supervise \(\mathbf{V}\) with the ground truth \(\mathbf{V}_{gt}\).

\[\mathcal{L}_{Vis}=\mathbf{V}_{gt}\log\mathbf{V}+(1-\mathbf{V}_{gt})\log(1- \mathbf{V}).\] (5)

The final loss is the weighted sum of the two losses:

\[\mathcal{L}_{total}=w_{1}\mathcal{L}_{TAP}+w_{2}\mathcal{L}_{Vis}.\] (6)

We use \(w_{1}=1\) and \(w_{2}=10\) during training.

[MISSING_PAGE_FAIL:6]

achieves 7.06 ATE-Occ and 4.28 ATE-Vis on the CroHD dataset, 11.4% and 9.5% error reductions from PIPs, the runner-up. On the FlyingThings++ dataset, our Context-PIPs decreases the ATE-Vis and ATE-Occ by 0.96 and 2.18, respectively.

**TAP-Vid-DAVIS and TAP-Vid-Kinetics (first)** A-PCK, the average percentage of correct key points, is the core metric. Context-PIPs ranks 1st in terms of A-PCK on both benchmarks. Specifically, Context-PIPs outperforms TAP-Net by 24.1% on the TAP-Vid-DAVIS benchmark and improves PIPs by 11.8% on the TAP-Vid-Kinetics benchmarks. Context-PIPs also achieves state-of-the-art occupancy accuracy on both datasets.

**TAP-Vid-DAVIS and TAP-Vid-Kinetics (strided)** Tab. 2 compares methods in the "strided" sampling setting. Our Context-PIPs also achieves the best performance on both AJ and A-PCK metrics for the two datasets. Context-PIPs effectively improves PIPs' occupancy accuracy and presents the best performance on the TAP-Vid-Davis dataset.

### Qualitative Comparison

We visualize the trajectories estimated by TAP-Net, PIPs, and our Context-PIPs respectively in Fig 2 to qualitatively demonstrate the superior performance of our method. By incorporating additional spatial context features for point tracking, Context-PIPs surpasses the compared methods in accuracy and robustness. Specifically, the first row shows the case of large-scale variation. The trajectory predicted by TAP-Net deviates considerably from the ground truth. TAP-Net also outputs jitter predictions when the query pixel is on the texture-less area as shown in the second row. Our Context-PIPs generates more accurate results than PIPs in these two hard cases. Furthermore, as depicted in the third row, PIPs struggles to distinguish the front wheel and the rear wheel due to the changing lighting conditions. However, our Context-PIPs achieves consistent tracking, thanks to the rich context information brought by the SOFE and TAFA modules.

### Efficiency Analysis

We train our Context-PIPs and PIPs with different MLP-Mixer depths, i.e., the number of layers in the MLP-Mixer, to show the prominent efficiency and effectiveness of our proposed Context-PIPs. Context-PIPs improves PIPs with SOFE and TAFA, which introduce minor additional parameters and time costs. We show that the accuracy benefits do not come from the increased parameters trivially. As displayed in Tab. 4, we increase the MLP-Mixer depth to 16, which significantly increases the parameters but does not bring performance gain. We also decrease the MLP-Mixer depth in our Context-PIPs. Even with only a 3-layer MLP-Mixer, Context-PIPs achieves better performance than the best PIPs (MLP-Mixer depth=12). Context-PIPs outperforms PIPs with only 40.2% parameters. Moreover, evaluated by the pytorch-OpCounter [49], PIPs consumes 287.5G FLOPS while Context

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{TAP-DAVIS(strided)} & \multicolumn{2}{c}{TAP-Kinetics(strided)} \\ \cline{2-5}  & AJ & A-PCK & AJ & A-PCK \\ \hline RAFT & 30.0 & 46.3 & 34.5 & 52.5 \\ Kubric-VFS-Like & 33.1 & 48.5 & 40.5 & 59.0 \\ COTR & 35.4 & 51.3 & 19.0 & 38.8 \\ TAP-Net & 38.4 & 53.1 & 46.6 & 60.9 \\ PIPs(Re-imp.) & 45.2 & 59.8 & 42.9 & 58.3 \\ Context-PIPs (Ours) & **48.9** & **64.0** & **49.8** & **64.3** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Experiments on TAP-Vid-DAVIS(strided) and TAP-Vid-Kinetics(strided).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{First} & \multicolumn{2}{c}{Strided} \\ \cline{2-5}  & TAP-DAVIS & TAP-Kinetics & TAP-DAVIS & TAP-Kinetics \\ \hline TAP-Net & 78.8 & 80.6 & 82.3 & 85.0 \\ PIPs (Re-imp.) & 79.3 & 77.0 & 82.9 & 81.5 \\ Context-PIPs (Ours) & 79.5 & 79.8 & 83.4 & 83.3 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Occlusion Accuracy on TAP-Vid-DAVIS and TAP-Vid-Kinetics.

PIPs only consumes 216.4G FLOPS, saving 24.7% computation resources. These numbers reveal the high memory and computation efficiency of our proposed Context-PIPs.

### Ablation Study on Modules

We conduct a module ablation study on the proposed modules as presented in Tab. 5. The errors of Context-PIPs consistently decrease when we sequentially add the SOFE and TAFA modules, which reveals the effectiveness of SOFE and TAFA. To demonstrate the necessity of the cross-attention mechanism used in TAFA, we attempt to predict a matrix of weights corresponding to the feature map shaped with \(r_{a}\) and directly weigh and sum the features to get \(\delta F\). Cross-attention performs better than prediction.

### Ablation Study on Parameters

We conduct a series of ablation experiments (Tab. 6) to demonstrate the significance of each module and explain the rationality of the settings. All ablation experiments are trained on Flyingthings++. Starting from the PIPs baseline, we first add the SOFE module and explore the two related hyper-parameters, i.e., the correlation radius \(r_{c}\) and the number of samples \(M\). Then, we further add the TAFA module and also adjust the attention window radius \(r_{a}\). We additionally conduct a comparison between the prediction and attention mechanisms in TAFA. In the below experiments, we set \(N=64\), learning rate as \(3\times 10^{-4}\), and train for \(20,000\) steps. Below we describe the details.

**Correlation Radius in SOFE** We crop a multi-scale correlation of size \((2r_{c}+1)\times(2r_{c}+1)\) from the first correlation map to predict the auxiliary feature offsets in SOFE. The correlation radius

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{Method} & MLP-Mixer & \multirow{2}{*}{Param.(M)} & \multicolumn{2}{c}{Flyingthings++} & \multicolumn{2}{c}{CroHD} \\ \cline{3-6}  & & & vis & occ & vis & occ \\ \hline \multirow{3}{*}{PIPs} & 6 & 16.06 & 8.00 & 25.30 & 5.04 & 8.15 \\  & 12 & 28.67 & 7.70 & 24.70 & 4.98 & 8.20 \\  & 16 & 37.09 & 7.69 & 24.71 & 4.84 & 8.07 \\ \hline \multirow{3}{*}{Context-PIPs (Ours)} & 3 & 11.54 & 7.37 & 24.19 & 4.54 & 7.79 \\  & 6 & 17.84 & 6.94 & 22.56 & 4.37 & 7.05 \\ \cline{1-1}  & 12 & 30.46 & 6.60 & 22.11 & 4.30 & 6.73 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Efficiency analysis for PIPs and Context-PIPs with different MLP-Mixer depth.

Figure 2: Qualitative comparison. In the leftmost column, the green crosses mark the query points and the image is the starting frame. The right three columns show the results of TAP-Net, PIPs, and Context-PIPs. The red and green lines illustrate the predicted and ground truth trajectories.

determines the cropping patch size. We fix \(M=3\), and gradually increase \(r_{c}\) from 1 to 4. The model achieves the best performance when \(r_{c}=2\).

**Number of Samples in SOFE** SOFE learns to sample \(M\) additional auxiliary features to enhance the source feature. Given \(r_{c}=2\), we continued to experiment with the different number of samples \(M\). The model achieves the best performance on both Flyingthings++ and CroHD datasets when \(M=9\).

**Attention Radius in TAFA** TAFA aggregates target features surrounding the currently estimated corresponding point locations to enhance the context feature via cross-attention. The radius of the attention window \(r_{a}\) determines how far the attention can reach. We gradually increase \(r_{a}\) from 1 up to 5, and find that \(r_{a}=3\) performs best.

## 5 Conclusion

We have presented a novel framework Context-PIPs that improves PIPs with spatial context features, including a SOtree Feature Enhancement (SOFE) module and a TAFget Feature Aggregation (TAFA) module. Experiments show that Context-PIPs achieves the best tracking accuracy on four benchmark datasets with significant superiority. This technology has broad applications in video editing and 3D reconstruction and other fields. **Limitations**. Following PIPs, Context-PIPs tracks points in videos with a sliding window. The target point cannot be re-identified when the point is lost. In our future work, we will explore how to re-identify the lost points when the points are visible again.

**Acknowlegement** This project is funded in part by National Key R&D Program of China Project 2022ZD0161100, by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK, by General Research Fund of Hong Kong RGC Project 14204021. Hongsheng Li is a PI of CPII under the InnoHK.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Experiment} & \multicolumn{2}{c}{Parameters} & \multicolumn{2}{c}{Flyingthings++} & \multicolumn{2}{c}{CroHD} \\ \cline{3-8}  & & \(M\) & \(r_{c}\) & \(r_{a}\) & vis & occ & vis & occ \\ \hline PIPs & Baseline & - & - & - & 14.42 & 37.33 & 6.14 & 9.97 \\ \hline \multirow{8}{*}{+SOFE} & \multirow{4}{*}{Correlation Radius (\(r_{c}\))} & 3 & 1 & - & 13.60 & 36.17 & 6.40 & 10.30 \\  & & 3 & 2 & - & **13.02** & **35.60** & 6.48 & **9.69** \\  & & 3 & 3 & - & 13.07 & 35.74 & **6.23** & 10.09 \\  & & 3 & 4 & - & 13.75 & 37.01 & 6.64 & 10.20 \\ \cline{2-8}  & \multirow{4}{*}{Number of Samples (\(M\))} & 1 & 2 & - & 15.00 & 37.51 & 6.94 & 10.25 \\  & & 3 & 2 & - & 13.02 & 35.60 & 6.48 & 9.69 \\  & & 6 & 2 & - & 13.21 & 35.39 & 6.58 & 9.77 \\  & & 9 & 2 & - & **12.18** & **34.23** & **5.71** & **9.18** \\  & & 12 & 2 & - & 12.87 & 35.27 & 6.20 & 10.17 \\ \hline \multirow{8}{*}{+SOFE+TAFA} & \multirow{4}{*}{Attention Window (\(r_{a}\))} & 9 & 2 & 1 & 11.98 & 34.10 & 5.90 & 9.52 \\  & & 9 & 2 & 2 & 11.82 & 33.82 & 5.64 & 9.28 \\ \cline{1-1}  & & 9 & 2 & 3 & **11.67** & **33.38** & **5.53** & 9.19 \\ \cline{1-1}  & & 9 & 2 & 4 & 11.81 & 33.88 & 5.65 & 9.23 \\ \cline{1-1}  & & 9 & 2 & 5 & 11.83 & 33.76 & 5.60 & **9.15** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation study. We add one component at a time on top of the baseline to obtain our Context-PIPs. \(r_{c}\), \(M\), and \(r_{a}\) respectively denote the correlation radius and the number of predicted samples in SOFE, and the attention window radius in TAFA. Our final model uses \(M=9,r_{c}=2,r_{a}=3\) to achieve the best performance.

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Experiment} & \multicolumn{2}{c}{Flyingthings++} & \multicolumn{2}{c}{CroHD} \\ \cline{3-6}  & & vis & occ & vis & occ \\ \hline PIPs & Baseline & 7.40 & 24.4 & 4.73 & 7.97 \\ \hline \multirow{4}{*}{Context-PIPs} & +SOFE & 6.91 & 22.64 & 4.36 & 7.15 \\  & +SOFE+TAFA (attention) & 6.60 & 22.11 & 4.30 & 6.73 \\ \cline{1-1}  & +SOFE+TAFA (prediction) & 7.15 & 23.33 & 4.34 & 7.27 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Modules ablation study.

## References

* [1]M. J. Black and P. Anandanan (1993) A framework for the robust estimation of optical flow. In 1993 (4th) International Conference on Computer Vision, pp. 231-236. Cited by: SS1.
* [2]A. Bruhn, J. Weickert, and C. Schnorr (2005) Lucas/kanade meets horn/schunck: combining local and global optic flow methods. International journal of computer vision61 (3), pp. 211-231. Cited by: SS1.
* [3]M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin (2021) Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), Cited by: SS1.
* [4]T. M. Chin, W. Clement Karl, and A. S. Willsky (1994) Probabilistic and sequential computation of optical flow using temporal coherence. IEEE Transactions on Image Processing3 (6), pp. 773-788. Cited by: SS1.
* [5]C. Doersch, A. Gupta, L. Markeeva, A. Recasens, L. Smaira, Y. Aytar, J. Carreira, A. Zisserman, and Y. Yang (2022) Tap-vid: a benchmark for tracking any point in a video. arXiv preprint arXiv:2211.03726. Cited by: SS1.
* [6]A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox (2015) Flownet: learning optical flow with convolutional networks. In Proceedings of the IEEE international conference on computer vision, pp. 2758-2766. Cited by: SS1.
* [7]M. Elad and A. Feuer (1998) Recursive optical flow estimation--adaptive filtering approach. Journal of Visual Communication and image representation9 (2), pp. 119-138. Cited by: SS1.
* [8]J. Engel, V. Koltun, and D. Cremers (2017) Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence40 (3), pp. 611-625. Cited by: SS1.
* [9]J. Engel, T. Schops, and D. Cremers (2014) LSD-slam: large-scale direct monocular slam. In European conference on computer vision, pp. 834-849. Cited by: SS1.
* [10]M. Gehrig, M. Milhausler, D. Gehrig, and D. Scaramuzza (2021) E-raft: dense optical flow from event cameras. In 2021 International Conference on 3D Vision (3DV), pp. 197-206. Cited by: SS1.
* [11]K. Greff, F. Belletti, L. Beyer, C. Doersch, Y. Du, D. Duckworth, D. J. Fleet, D. Gnanapragasam, F. Golemo, C. Herrmann, et al. (2022) Kubric: a scalable dataset generator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3749-3761. Cited by: SS1.
* [12]A. W. Harley, Z. Fang, and K. Fragkiadaki (2022) Particle video revisited: tracking through occlusions using point trajectories. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXII, pp. 59-75. Cited by: SS1.
* [13]B. K. Horn and B. G. Schunck (1981) Determining optical flow. Artificial intelligence17 (1-3), pp. 185-203. Cited by: SS1.
* [14]X. Hu, Z. Huang, A. Huang, J. Xu, and S. Zhou (2023) A dynamic multi-scale voxel flow network for video prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6121-6131. Cited by: SS1.
* [15]Z. Hu and D. Xu (2023) Videeocontrolnet: a motion-guided video-to-video translation framework by using diffusion model with controlnet. arXiv preprint arXiv:2307.14073. Cited by: SS1.
* [16]Z. Huang, X. Pan, W. Pan, W. Bian, Y. Xu, K. Cheung, G. Zhang, and H. Li (2022) Neuralmarker: a framework for learning general marker correspondence. ACM Transactions on Graphics (TOG)41 (6), pp. 1-10. Cited by: SS1.
* [17]Z. Huang, X. Shi, C. Zhang, Q. Wang, K. C. Cheung, H. Qin, J. Dai, and H. Li (2022) Flowformer: a transformer architecture for optical flow. arXiv preprint arXiv:2203.16194. Cited by: SS1.
* [18]T. Hui, X. Tang, and C. C. Loy (2018) LiteflowNet: a lightweight convolutional neural network for optical flow estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8981-8989. Cited by: SS1.
* [19]T. Hui, X. Tang, and C. C. Loy (2020) A lightweight optical flow cnn--revisiting data fidelity and regularization. IEEE transactions on pattern analysis and machine intelligence43 (8), pp. 2555-2569. Cited by: SS1.
* [20]E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox (2017) Flownet 2.0: evolution of optical flow estimation with deep networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2462-2470. Cited by: SS1.
* [21]S. Jiang, D. Campbell, Y. Lu, H. Li, and R. Hartley (2021) Learning to estimate hidden motions with global motion aggregation. arXiv preprint arXiv:2104.02409. Cited by: SS1.
* [22]W. Jiang, E. Trulls, J. Hosang, A. Tagliasacchi, and K. Moo Yi (2021) Cotr: correspondence transformer for matching across images. arXiv preprint arXiv:2103.14167. Cited by: SS1.
* [23]D. Kim, S. Woo, J. Lee, and H. So Kweon (2019) Deep video inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5792-5801. Cited by: SS1.
* [24]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-116. Cited by: SS1.
* [25]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-120. Cited by: SS1.
* [26]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-126. Cited by: SS1.
* [27]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-126. Cited by: SS1.
* [28]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-126. Cited by: SS1.
* [29]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-127. Cited by: SS1.
* [30]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-130. Cited by: SS1.
* [31]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-136. Cited by: SS1.
* [32]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-136. Cited by: SS1.
* [33]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-127. Cited by: SS1.
* [34]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-138. Cited by: SS1.
* [35]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-127. Cited by: SS1.
* [36]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-136. Cited by: SS1.
* [37]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-127. Cited by: SS1.
* [38]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-127. Cited by: SS1.
* [39]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-136. Cited by: SS1.
* [40]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-136. Cited by: SS1.
* [41]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-127. Cited by: SS1.
* [42]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-136. Cited by: SS1.
* [43]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 105-127. Cited by: SS1.
* [44]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 105-127. Cited by: SS1.
* [45]D. Li, Y. Zhang, K. C. Cheung, X. Wang, H. Qin, and H. Li (2021) Learning degradation representations for image deblurring. In Proceedings of the IEEEGiovanni Maria Farinella, and Tal Hassner, editors, _Computer Vision - ECCV 2022_, pages 736-753, Cham, 2022. Springer Nature Switzerland.
* [25] Yijin Li, Zhaoyang Huang, Shuo Chen, Xiaoyu Shi, Hongsheng Li, Hujun Bao, Zhaopeng Cui, and Guotong Zhang. Blinkflow: A dataset to push the limits of event-based optical flow estimation. _arXiv preprint arXiv:2303.07716_, 2023.
* [26] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4040-4048, 2016.
* [27] Zhixiang Min, Yiding Yang, and Enrique Dunn. Voldor: Visual odometry from log-logistic dense optical flow residuals. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4898-4909, 2020.
* [28] Anurag Ranjan and Michael J Black. Optical flow estimation using a spatial pyramid network. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4161-4170, 2017.
* [29] Zhile Ren, Orazio Gallo, Deqing Sun, Ming-Hsuan Yang, Erik B Sudderth, and Jan Kautz. A fusion approach for multi-frame optical flow estimation. In _2019 IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 2077-2086. IEEE, 2019.
* [30] Peter Sand and Seth Teller. Particle video: Long-range motion estimation using point trajectories. _International Journal of Computer Vision_, 80:72-91, 2008.
* [31] Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Videoflow: Exploiting temporal cues for multi-frame optical flow estimation. _arXiv preprint arXiv:2303.08340_, 2023.
* [32] Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer++: Masked cost volume autoencoding for pretraining optical flow estimation. _arXiv preprint arXiv:2303.01237_, 2023.
* [33] Xuchao Sui, Shaohua Li, Xue Geng, Yan Wu, Xinxing Xu, Yong Liu, Rick Goh, and Hongyuan Zhu. Craft: Cross-attentional flow transformer for robust optical flow. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17602-17611, 2022.
* [34] Deqing Sun, Stefan Roth, and Michael J Black. A quantitative analysis of current practices in optical flow estimation and the principles behind them. _International Journal of Computer Vision_, 106(2):115-137, 2014.
* [35] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 8934-8943, 2018.
* [36] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8922-8931, 2021.
* [37] Shangkun Sun, Yuanqi Chen, Yu Zhu, Guodong Guo, and Ge Li. Skflow: Learning optical flow with super kernels. _arXiv preprint arXiv:2205.14623_, 2022.
* [38] Ramana Sundararaman, Cedric De Almeida Braga, Eric Marchand, and Julien Pettre. Tracking pedestrian heads in dense crowd. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3865-3875, June 2021.
* [39] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In _European conference on computer vision_, pages 402-419. Springer, 2020.
* [40] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. _Advances in neural information processing systems_, 34:24261-24272, 2021.
* [41] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. _arXiv preprint arXiv:2306.09341_, 2023.
* [42] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-to-image models with human preference. _arXiv preprint arXiv:2303.14420_, 2023.
* [43] Liu Xinyang, Li Yijin, Teng Yanbin, Bao Hujun, Zhang Guofeng, Zhang Yinda, and Cui Zhaopeng. Multi-modal neural radiance field for monocular dense slam with a light-weight tof sensor. In _International Conference on Computer Vision (ICCV)_, 2023.
* [44] Jiarui Xu and Xiaolong Wang. Rethinking self-supervised correspondence learning: A video frame-level similarity perspective. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 10075-10085, 2021.
* [45] Gengshan Yang and Deva Ramanan. Volumetric correspondence networks for optical flow. _Advances in neural information processing systems_, 32:794-805, 2019.
* [46] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot text-guided video-to-video translation. _arXiv preprint arXiv:2306.07954_, 2023.

* [47] Tianwei Zhang, Huayan Zhang, Yang Li, Yoshihiko Nakamura, and Lei Zhang. Flowfusion: Dynamic dense rgb-d slam based on optical flow. In _2020 IEEE International Conference on Robotics and Automation (ICRA)_, pages 7322-7328. IEEE, 2020.
* [48] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXII_, pages 523-542. Springer, 2022.
* [49] Ligeng Zhu. Thop: Pytorch-opcounter. _Lyken17/pytorch-OpCounter_, 2019.

## Appendix A More Implementation Details

**SOFE sampler**. While sampling auxiliary features, SOFE learns to predict offsets with an MLP-based sampler which consists of 5 linear layers interweaved with RELU activations. The local self-similarities \(\mathbf{c}_{0}^{\mathbf{0}}\) at location \(\mathbf{x}_{src}\)) would first be projected into 128 feature channels. A 3-layer feedforward network with \(4\times 128\) channels followed, outputting the feature in \(128\) feature channels. The final linear layer is used to predict offsets \(M\times 2\) from the 128-channel features.

**SOFE CorrEnc**. SOFE reduces the augmented correlations \((M+1)\times 196\) to a correlation feature vector \(\hat{\mathbf{c}}_{t}^{k}\) through a correlation encoder \(\operatorname{CorrEnc}\) which contains only 2 linear layers. The first linear layer reduces the feature channels to \(4\times 196\). After a RELU, the later linear layer further reduces the feature channels to \(196\), which is the \(\hat{\mathbf{c}}_{t}^{k}\).

## Appendix B More Quantitative Comparisons

**PIPs Re-implementation**. There are two official PIPs [12] versions. PIPs (Paper) and PIPs (Released) respectively denote the model reported in the paper and the model provided in the released code. There are many misalignments between the paper description and the released code. We follow the parameters suggested in the paper and the released code but fail to reproduce the results. We, therefore, re-implement two PIPs as the baselines, according to the settings provided in the paper (\(K=6\)) and the released code (\(K=4\)). \(K\) denotes the number of refinement iterations in training. The underscored PIPs (Re-imp.), i.e. \(K=6\), is the chosen baseline for comparison in the main paper. The performance of the re-implemented model is better than the numbers reported in the paper (Tab. A1). Although our re-implemented model presents inferior performance than the released model on FlyingThings++ and CroHD, they are comparable on TAP-Vid-DAVIS and the re-implement model is even better than the released model on TAP-Vid-Kinetics (Tab. A2). We add our proposed SOFE and TAFA modules to the re-implemented baselines to obtain our Context-PIPs models.

We list the results for Flyingthings++ and CroHD in Tab. A1, the results for TAP-Vid-DAVIS (first) and TAP-Vid-Kinetics (first) in Tab. A2, and the results for TAP-Vid-DAVIS (strided) and TAP-Vid-Kinetics (strided) in Tab. A3. "first" and "strided" are two distinct query sampling strategies proposed by TAP-Vid [5], where "first" sampling only contains the initial visible query points, while "strided" sampling would contain all visible query points in every 5 frames.

The released PIPs model tends to overfit on FlyingThings++ because although it obtains the lowest error on FlyingThings++ but is inferior on TAP-Vid-DAVIS and TAP-Vid-Kinetics. Although we only show the Context-PIPs with \(K=4\) in the main paper, our \(K=6\) version achieves the best performance, outperforming PIPs \(K=6\) by 9.44% and 11.76% on DAVIS and Kinetics. Moreover, Context-PIPs trained with \(K=4\) and 3-layer MLP-Mixer achieves even better results than PIPs trained with \(K=6\) and 12-layer MLP-Mixer (Tab. A2).