# Learning Dynamic Attribute-factored World Models

for Efficient Multi-object Reinforcement Learning

 Fan Feng

City University of Hong Kong

ffengl017@gmail.com &Sara Magliacane

University of Amsterdam

MIT-IBM Watson AI Lab

sara.magliacane@gmail.com

###### Abstract

In many reinforcement learning tasks, the agent has to learn to interact with many objects of different types and generalize to unseen combinations and numbers of objects. Often a task is a composition of previously learned tasks (e.g. block stacking). These are examples of _compositional generalization_, in which we compose object-centric representations to solve complex tasks. Recent works have shown the benefits of object-factored representations and hierarchical abstractions for improving sample efficiency in these settings. On the other hand, these methods do not fully exploit the benefits of factorization in terms of object attributes. In this paper, we address this opportunity and introduce the Dynamic Attribute FacTored RL (DAFT-RL) framework. In DAFT-RL, we leverage object-centric representation learning to extract objects from visual inputs. We learn to classify them into classes and infer their latent parameters. For each class of object, we learn a class template graph that describes how the dynamics and reward of an object of this class factorize according to its attributes. We also learn an interaction pattern graph that describes how objects of different classes interact with each other at the attribute level. Through these graphs and a dynamic interaction graph that models the interactions between objects, we can learn a policy that can then be directly applied in a new environment by estimating the interactions and latent parameters. We evaluate DAFT-RL in three benchmark datasets and show our framework outperforms the state-of-the-art in generalizing across unseen objects with varying attributes and latent parameters, as well as in the composition of previously learned tasks.

## 1 Introduction

Model-based reinforcement learning (MBRL) and world models [1, 2, 3] have demonstrated improved performance in many RL tasks by providing better sample efficiency. However, most world models focus only on modeling a single object or holistic modeling over the environment, while in real-world tasks, we often have environments with multiple objects that interact, and we are interested in generalizing to unseen combinations and numbers of objects. In recent years, there have been several studies exploring and learning object-oriented environment models or policy models [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17] and tackling the problem of _compositional_ or combinatorial generalization, in which we consider combining the modeling of multiple objects or tasks to solve a new task.

Although these methods have effectively leveraged object-centric and object-factored representations in RL and thus improved the sample efficiency in multi-object settings, they have not fully exploited the benefits of factorization in terms of object attributes. Often an object's transition and reward functions are influenced only by a sparse subset of attributes, e.g. an object's position and reward are affected by its previous position, but not by its appearance or activation state once the object of interest is identified (e.g., "the red block"). In these environments, interactions between objects areoften sparse, both in time and in terms of which attributes are affected, e.g. the position of a box is affected by the position of another box at the timestep in which they collide, but not directly by the other object's friction coefficient. Additionally, objects of the same type share similar factored dynamics, modulated by an object-specific latent parameter, while objects of different types might instead have different attributes, dynamics, and rewards.

In this paper, we propose Dynamic Attribute FacTored RL (DAFT-RL), a framework that learns a fine-grained attribute-factored representation across objects, including a dynamic graph for modeling interactions between objects. As part of this framework, we propose a model, DAFT-MDP, that builds on Factored (PO)MDPs [18; 19; 20; 21; 22; 23], Relational MDPs [24; 25; 26] and especially Object-Oriented (PO)MDPs [27; 28; 29], but focuses on a more fine-grained factorization at the attribute level and dynamic graphs. We implement our framework as a structured and sequential generative model by combining it with state-of-the-art object-centric representation learning [30; 31] for extracting objects and attributes from visual inputs, factored adaptation approaches inspired by the causality literature [32; 33] for estimating the factored dynamics and reward, soft attention networks [34] for action binding [35] and (dynamic) Neural Relational Inference [36; 37] for modeling interactions.

Our framework allows us to learn a policy on a set of source environments that can successfully generalize to new environments with unseen combinations of objects with different latent parameters (possibly with unseen values) and types, as well as to combinations of previously learned tasks on different objects, without any further policy learning. We show the benefits of DAFT-RL in three benchmark datasets for compositional generalization, where it outperforms the baselines.

## 2 Dynamic Attribute-FacTored MDPs (DAFT-MDP)

We formalize our assumptions by introducing our DAFT-MDP model, which is an extension with class template graphs, interaction patterns, and interaction graphs of factored (PO)MDPs [18; 19; 20; 21; 22; 23]. This extension takes inspiration from Relational MDPs [24; 25; 26] and their literature, especially Object-Oriented (PO)MDPs [27; 28; 29], but we propose a more fine-grained factorization of the transition and reward at the object attribute level, based on estimating template and dynamic interaction graphs.

Intuitively, we will consider learning a policy that can be generalized across different environments that vary in the number and characteristics of their objects. We will assume that each environment is composed of multiple _objects_, each of a specific type, or _class_. Each object has several observable _attributes_ (e.g. position, velocity) and some latent constant _parameters_ (e.g. an object-specific friction coefficient). Objects of the same class will have the same set of attributes, the same transition and reward functions, but can differ in the values of the attributes (e.g. they are at different positions) and in the value of the latent parameters (e.g. they have different friction coefficients). We will assume that the transition and reward functions can be _factored_ in terms of attributes and that for a given attribute only a sparse subset of other attributes influences these functions.

In our setting, objects can interact with each other, which might influence their dynamics. We will assume that these _interactions_ are _sparse_, both in time and in terms of the effect on the attributes of each object, and that all objects in a class have the same _interaction pattern_ in terms of how the attributes interact with objects of another class. In each environment, we will assume that an action has only an effect on one object at a time.

We formalize these assumptions in the following. We start by defining our class system, and then we describe three types of graphs (class template graphs, interaction patterns, and dynamic interaction graphs in Fig. 1) that describe how dynamics and reward factorize based on the classes, attributes, and interactions, and finally provide a formal definition of a Dynamic Attribute-FacTored MDP.

Class system, attributes, and objects.We assume a known set of classes \(\mathcal{C}=\{C_{1},\ldots,C_{k}\}\) of objects, where each class \(C_{j}\) describes a set of observable attributes \(\{C_{j}.s_{1},\ldots,C_{j}.s_{n}\}\), which we assume for simplicity are the same number in each class. We assume that each class has a set of latent constant parameters \(C_{j}.\boldsymbol{\theta}\), that represent physical properties of the object that can vary across different objects of the same type. For example, in one of the benchmarks [15], we consider two types of objects \(\mathcal{C}=\{\texttt{box},\texttt{sw}\}\), boxes and switches, with class attributes \(\{\texttt{box}.\texttt{color},\texttt{box}.\texttt{pos},\texttt{box}.\texttt{ vel},\texttt{box}.\boldsymbol{\theta}\}\) representing the object color, position, velocity, and friction coefficient of a box, and \(\{\texttt{sw}.\texttt{color},\texttt{sw}.\texttt{pos},\texttt{sw}.\texttt{ active}\}\), representing the color, position, and activation state of a switch. The class system specifies a template for a set of environments that can vary in the number and characteristics of the objects. Each environment has a fixed set of objects \(\mathcal{O}=\{o_{1},\ldots,o_{m}\}\), where each object is an instance of a class in \(\mathcal{C}\), which we denote as \(o_{i}\in C_{j}\) for \(i\in\{1,\ldots,m\},j\in\{1,\ldots,k\}\). We denote the class of an object \(o_{i}\) with \(C(i)\). For example, using the class system from the previous example, we can represent a source environment as \(\mathcal{O}^{\prime}=\{o_{1},o_{2}\}\) where \(o_{1}\in\texttt{box}\) and \(o_{2}\in\texttt{sw}\), and a target environment as \(\mathcal{O}^{\prime\prime}=\{o_{1},o_{2},o_{3}\}\), where we add \(o_{3}\in\texttt{box}\) to the original objects. For each object \(o_{i}\in C_{j}\) and timestep \(t=\{1,\ldots,T\}\), we denote its attributes at time \(t\) as \(\mathbf{o}_{i}^{t}=\{o_{i}.s_{1}^{t},\ldots,o_{i}.s_{t}^{t}\}\), which are instantiations of the class attributes \(\{C_{j}.s_{1},\ldots,C_{j}.s_{n}\}\), and its constant parameters as \(o_{i}.\boldsymbol{\theta}\). In our example, for box \(o_{1}\) the attributes \(o_{1}.s_{1}^{t}\), \(o_{1}.s_{2}^{t}\) and \(o_{1}.\boldsymbol{\theta}\) are its position and velocity at time \(t\) and its friction coefficient, while for the switch \(o_{2}\) the attributes \(o_{2}.s_{1}^{t}\) and \(o_{2}.s_{2}^{t}\) are its position and activation at time \(t\).

States, actions, transitions, and rewards.We define the state at time \(t\) as the collection of all the object states, i.e., \(\mathbf{s}^{t}=\{\mathbf{o}_{1}^{t},\ldots,\mathbf{o}_{m}^{t}\}\) with domain \(\mathcal{S}\). We collect all object-specific latent parameters in a global parameter \(\boldsymbol{\theta}=\{o_{1}.\boldsymbol{\theta},\ldots,o_{m}.\boldsymbol{ \theta}\}\) with domain \(\Theta\). We define the space of actions \(\mathcal{A}\) and use \(\mathbf{a}^{t}\in\mathcal{A}\) to denote the action at time \(t\). We denote the transition probability as \(p(\mathbf{s}^{t+1}|\mathbf{s}^{t},\mathbf{a}^{t},\boldsymbol{\theta})\). The reward at time \(t\) is denoted as \(r^{t}\) and the reward probability is denoted as \(p(r^{t}|\mathbf{s}^{t},\mathbf{a}^{t},\boldsymbol{\theta})\). The transition and reward probabilities are factorized according to the _class template graphs_, _interaction pattern graphs_, and _interaction graphs_ that we can learn from data, and that we will introduce below.

Class template graphs (Fig. 1A).We assume that all objects within the same class share the same factorization in terms of how their own attributes, latent parameters, and actions influence their dynamics and rewards. For example, all boxes will have a similar relationship between their position, velocity, and latent friction parameters, since they follow the same physical laws. Similarly, all switches will share similar dynamics, which will be different from the boxes. We describe these relationships as Dynamic Bayesian Networks (DBNs) [38], which are graphical models that describe the template for the relations between two contiguous timesteps, \(t\) and \(t+1\), and assume this structure is time-invariant. In particular, for each class \(C_{j}\), we learn a DBN \(\mathcal{G}_{C_{j}}\) over the nodes \(\{C_{j}.s_{1}^{t},\ldots,C_{j}.s_{n}^{t},C_{j}.s_{1}^{t+1},\ldots,C_{j}.s_{n}^ {t+1},C_{j}.\boldsymbol{\theta},\mathbf{a}^{t},r^{t}\}\) that represents a template for the instance graph between the attributes for each object of this class \(\mathcal{G}_{o_{i}}^{t}\) for \(o_{i}\in C_{j}\). In particular, the edges between the action \(a^{t}\) and the attributes at timestep \(t+1\) can be switched on or switched off at different timesteps, since as we will see in the description of the interaction graphs, they represent the interaction pattern of the agent with any specific object.

We show an example of two DBNs for two classes of objects (boxes and switches) in Fig. 1a. In this example, \(C_{1}.s_{2}^{t}\) influences \(C_{1}.s_{2}^{t+1}\) and \(C_{1}.s_{3}^{t+1}\), but it's not influenced by \(C_{1}.s_{3}^{t-1}\). The reward \(r^{t}\) is only influenced by \(C_{1}.s_{3}^{t}\) and the action will only have an effect on \(C_{1}.s_{2}^{t+1}\) and \(C_{1}.s_{3}^{t+1}\). Moreover, \(C_{2}.s_{2}^{t}\) influences only \(C_{2}.s_{2}^{t+1}\) and \(r^{t}\).

Interaction pattern graphs (Fig. 1B).When two objects interact, it often happens that only some of the attributes of each object affect the other. For example, when two boxes collide, their positions and velocities might change, but their masses will not. Therefore, we will assume that from an attribute perspective, the interplay between objects during interaction is also factored

Figure 1: The graphical representation of DAFT-MDP. The colors denote the attributes for an object or a class, the red dashed lines denote edges that can be switched on or off at different timesteps.

and sparse. Additionally, we assume that the interactions between two objects follow patterns based on the classes of the objects. For example, the attribute of a box will always interact in the same way with the attributes of a switch, regardless of the specific object. In particular, for any pair of classes \(C_{i}\) and \(C_{j}\) (possibly also with \(i=j\)), we learn a DBN \(\mathcal{G}_{C_{i},C_{j}}\) over the nodes \(\{\{C_{i}.s_{l}^{t}\}_{l=1}^{n},\{C_{i}.s_{l}^{t+1}\}_{l=1}^{n},C_{i}.\mathbf{ \theta},\{C_{j}.s_{l}^{t}\}_{l=1}^{n},\{C_{j}.\mathbf{\theta}\}_{l=1}^{n},C_{j}.\bm {\theta}\}\). We show an example of a DBN describing how boxes interact with other boxes in Fig. 1b. In this case, the interaction is limited to \(C_{1}.s_{2}^{t}\) from one object influencing the \(C_{1}.s_{1}^{t+1}\) and \(C_{1}.s_{2}^{t+1}\) from the other object, and the latent parameters \(C_{1}.\mathbf{\theta}\) from each object influencing \(C_{1}.s_{3}^{t+1}\) from the other object. While the interaction patterns are time-invariant, we assume that for each pair of objects, the interactions are only switched on at some points in time, as we will now describe through the interaction graphs.

Dynamic interaction graph (Fig. 1C).The class template and interaction pattern graphs that we just described can model the general behavior of the classes of objects in a static, time-invariant way. On the other hand, in many multi-object environments object interactions occur sparsely, the pairs of interacted objects are not fixed, and the action has an effect only on a limited number of objects at any given time (e.g. we will assume only one for simplicity). We, therefore, propose to model these interactions between objects as a dynamic graph \(\mathcal{G}_{\mathrm{inter}}=\{\mathcal{G}_{\mathrm{inter}}^{t}\}_{l=1}^{T}\) at the object level, which is a sequence of graphs \(\mathcal{G}_{\mathrm{inter}}^{t}\) with edges from a subset of \(\{o_{1}^{t},\dots,o_{m}^{t},\mathbf{a}^{t}\}\) to a subset of \(\{o_{1}^{t+1},\dots,o_{m}^{t+1}\}\). Each edge \(o_{i}^{t}\to o_{j}^{t+1}\) represents an interaction between an object \(o_{i}\) and an object \(o_{j}\). This interaction is instantiated in an instance interaction graph \(\mathcal{G}_{o_{i},o_{j}}^{t}\), following the interaction pattern graph \(\mathcal{G}_{C_{i},C_{j}}\) for each pair of objects \(o_{i}\in C_{i}\) and \(o_{j}\in C_{j}\).

We also learn an _object selector_\(\alpha^{t}\in\{1,\dots,m\}\) that selects the single object on which the action \(\mathbf{a}^{t}\) has an effect at timestep \(t\), which we represent with an edge from \(\mathbf{a}^{t}\) to the selected object \(o_{\alpha^{t}}^{t}\) in \(\mathcal{G}_{\mathrm{inter}}^{t}\). In particular, this selection (or _action binding_) activates the edges from \(\mathbf{a}^{t}\) to the object attributes described in the class template graph \(\mathcal{G}_{C_{j}}\) in the instantiated version for object \(o_{i}\in C_{j}\), which we denote \(\mathcal{G}_{o_{i}}^{t}\). The graph \(\mathcal{G}_{\mathrm{inter}}\) is dynamic because the edges of each graph in the sequence can change at each timestep \(t=1,\dots,T\).

We show an example of interactions between three objects at different timesteps in Fig. 1C. In this example, at timestep \(t\) there is an interaction between \(o_{1}\) and \(o_{3}\), which will follow the interaction pattern presented in Fig. 1B, i.e. \(o_{1}.s_{2}^{t}\rightarrow\{o_{3}.s_{1}^{t+1},o_{3}.s_{2}^{t+1}\}\) and \(o_{1}.\mathbf{\theta}^{t}\to o_{3}.s_{3}^{t+1}\), and viceversa for \(o_{3}\) towards \(o_{1}\). Moreover, \(\mathbf{a}^{t}\) has an effect on \(o_{2}\), and specifically \(o_{2}.s_{3}^{t+1}\), following the class template graph presented in Fig. 1A. Instead, at timestep \(t+1\), there are no interactions between the objects and \(\mathbf{a}^{t+1}\) has an effect on \(o_{1}\), specifically \(o_{1}.s_{2}^{t+2}\) and \(o_{1}.s_{3}^{t+2}\). For completeness, in App. B we provide an example of this graph combined with the instantiated class template graphs for each object \(\mathcal{G}^{o_{1}},\mathcal{G}^{o_{2}}\) and \(\mathcal{G}^{o_{3}}\) for three timesteps \(t,t+1,t+2\), as well as the instantiated interaction pattern graph \(\mathcal{G}^{o_{1},o_{3}}\) that is switched on at timestep \(t\).

Our modeling assumptions.Now that we have introduced all of the graphical structures that we will need, we can describe our assumptions as a Dynamic Attribute-FacTored Markov Decision Process (DAFT-MDP). We will assume that the class system is fixed and that the objects can vary in each environment, as can their interaction graphs. Under this assumption, a DAFT-MDP defines a family of MDPs, that is parametrized in the objects \(\mathcal{O}\) and their dynamic interaction graph \(\mathcal{G}_{\mathrm{inter}}\).

**Definition 1** (Daft-Mdp).: _A Dynamic Attribute-FacTored Markov Decision Process (DAFT-MDP) is a tuple \((\mathcal{C},\mathcal{O},\Theta,\mathcal{A},\mathcal{G},\mathbb{P}_{s},\mathcal{ R},\gamma)\), where \(\mathcal{C}\) is the set of classes, \(\mathcal{O}\) is the set of objects, \(\Theta\) is the space of the constant latent parameters, \(\mathcal{A}\) the action space, \(\mathbb{P}_{s}\) is the transition distribution, \(\mathcal{R}\) is the reward function and \(\mathcal{G}\) is a set of graphs that contains the collection of class template graphs for each class \(\{\mathcal{G}_{C_{j}}\}_{C_{j}\in\mathcal{C}}\), the collection of interaction pattern graphs for each pair of classes \(\{\mathcal{G}_{C_{i},C_{j}}\}_{C_{i},C_{j}\in\mathcal{C}}\) and the dynamic interaction graph \(\mathcal{G}_{\mathrm{inter}}\), as defined previously. These graphs define the factorization of the transition distribution per object and per attribute, as follows:_

\[\mathbb{P}_{s}(\mathbf{s}^{t+1}|\mathbf{s}^{t},\mathbf{\theta},\mathbf{a}^{t})=\prod_{i=1}^{m} \prod_{l=1}^{n}\mathbb{P}_{s}\left(o_{i}.s_{l}^{t+1}|\mathrm{pa}_{\mathcal{G }_{o_{i}}^{t}}(o_{i}.s_{l}^{t+1}),\{\mathrm{pa}_{\mathcal{G}_{o_{i}}^{t},o_{k }}(o_{i}.s_{l}^{t+1})\}_{o_{k}\rightarrowo_{i}\in\mathcal{G}_{\mathrm{inter}} ^{t}}\right)\]

_where \(\mathbf{s}^{t}\) is the collection of all attributes of all objects at time \(t\), \(\mathbf{\theta}\in\Theta\) is the collection of all latent constant parameters for all objects, \(\mathbf{a}^{t}\in\mathcal{A}\) is the action. In the right-hand term, \(o_{i}.s_{l}^{t+1}\) is attribute \(s_{l}\) of object \(o_{i}\) at time \(t+1\), while \(\mathrm{pa}_{\mathcal{G}_{o_{i}}^{t}}(o_{i}.s_{l}^{t+1})\) are the parents of the attribute \(l\) for object \(o_{i}\) based on the class template graph \(\mathcal{G}_{C(i)}\), where \(C(i)\) is the class of \(o_{i}\), and where the action binding \(\alpha^{t}\) activates any potential connections from \(\mathbf{a}^{t}\). In the second term of the conditioning, we iterate over the objects \(o_{k}\) that are interacting with \(o_{i}\) at time \(t\) in the dynamic interaction graph \(o_{k}\to o_{i}\in\mathcal{G}_{\mathrm{inter}}^{t}\). For each of these objects \(o_{k}\) we collect the attributes that interact with \(o_{i}.s_{l}\) in the instance interaction pattern \(\mathcal{G}_{o_{i},o_{k}}^{t}\) based on interaction pattern graph \(\mathcal{G}_{C(i),C(k)}\) for the respective classes \(C(i)\) and \(C(k)\). Similarly, we define the factorization of the reward function per object and per attribute as \(\mathcal{R}(\mathbf{s}^{t},\mathbf{a}^{t},\mathbf{\theta})=\mathcal{R}(\{ \mathbf{p}_{\mathcal{G}_{o_{i}}}(r^{t})\}_{o_{i}\in\mathcal{O}})\), where for each object \(o_{i}\) we collect all the attributes that have an edge to the reward in the instantiation of the class template graph._

In the following, we assume that the classes \(\mathcal{C}\) are known and fixed across environments, while the objects \(\mathcal{O}\) can vary, as can the latent parameters \(\mathbf{\theta}\). In the training phase, we will learn how to classify objects, the transition and reward functions based on the class template graphs \(\{\mathcal{G}_{C_{j}}\}_{C_{j}\in\mathcal{C}}\) and the interaction patterns \(\{\mathcal{G}_{C_{i},C_{j}}\}_{C_{i},C_{j}\in\mathcal{C}}\). In the testing phase, we will infer the class and latent parameters of each object, as well as the interactions between the objects in the dynamic interaction graph \(\mathcal{G}_{\mathrm{inter}}^{t}\), which specify the transition and reward functions in the new environment.

## 3 The DAFT-RL Framework

In the previous section, we introduced our model, DAFT-MDP (Fig. 2). In this section we provide a framework for estimating DAFT-MDPs, leveraging them for policy learning in a set of source environments, and adapting the policy to a new target environment with different objects, without any additional policy learning. Our framework is divided in four steps: (i) offline class learning in single-object environments, (ii) offline interaction learning and latent parameter inference in multi-object environments, (iii) policy learning and imagination in multi-object environments, and finally (iv) adaptation to a new multi-object environment. We present each step and its implementation in the following.

In all steps and all environments, if the input is an image, we extract the objects and their attributes \(\mathbf{s}^{t}=\{\mathbf{o}_{1}^{t},\mathbf{o}_{2}^{t},\ldots,\mathbf{o}_{m}^{ t}\}\) from sequences of images \(\mathbf{x}^{t}\) with pre-trained object-centric methods, e.g. SA [30] and AIR [31]. For symbolic inputs, we directly access the attributes of all objects \(\mathbf{s}^{t}=\{\mathbf{o}_{1}^{t},\mathbf{o}_{2}^{t},\ldots,\mathbf{o}_{m}^{ t}\}\). For each set of objects \(\mathcal{O}\), we learn to classify the objects into their classes \(\mathcal{C}\) with supervised learning, which we describe in detail in App. C.

Figure 2: The learning pipelines of the DAFT-RL framework.

### Step 1: Class Learning in Single-Object Environments

In this step we consider data from \(m\) single-object environments with different objects and no agent interaction. In particular, for each class \(C_{j}\) we collect the transitions for several objects \(o_{i}\in C_{j}\) as \(\{\mathbf{o}_{i}^{t},\mathbf{a}_{i}^{t},r_{i}^{t}\}_{t=1}^{T}\), in environments in which there is only object \(o_{i}\), and a random policy is used to generate actions. We denote these data as \(\mathcal{D}^{\mathrm{single}}=\{\{\mathbf{o}_{i}^{t},\mathbf{a}_{i}^{t},r_{i}^ {t}\}_{t=1}^{T}\}_{i\in C_{j},\forall j=1,\dots,k,i=1,\dots,m}\).

We initiate the class template \(\mathcal{G}_{C_{j}}\) for each class \(C_{j}\) randomly, and then use \(\mathcal{D}^{\mathrm{single}}\) to learn it, except the contribution of the latent parameters, which is learned in the next phase. In particular, we learn the class template graph by maximizing the log-likelihood of \(\log p_{\lambda}\left(\mathbf{o}_{i}^{t+1},r_{i}^{t}\mid\mathbf{o}_{i}^{t}, \mathbf{a}_{i}^{t},\mathcal{G}^{C_{j}}\right)\), where \(\lambda=\{\lambda_{s},\lambda_{r}\}\) are the parameters of dynamics and reward models. For the implementation, we use Gated Recurrent Units (GRU) [39] to learn the dynamics and reward models jointly with the class template graphs. At time step \(t\) for each object \(i\) with class \(C_{j}\), the inputs to the GRU are \(\{\mathrm{pa}_{\mathcal{G}_{C_{j}}}(\mathbf{o}_{i}^{t+1})\}_{i=1}^{m}\) and \(\{\mathrm{pa}_{\mathcal{G}_{C_{j}}}(r_{i}^{t})\}_{i=1}^{m}\), and the GRU outputs \(\mathbf{o}_{i}^{t+1}\) and \(r_{i}^{t}\). The learning objective of this step is given below, where we maximize the log-likelihoods of dynamics and reward models and regularize the graph to be sparse:

\[\mathrm{argmax}_{\lambda,\{\mathcal{G}^{C_{j}}\}_{j=1}^{k}}\sum_{t=1}^{T}\sum_ {i=1}^{m}\sum_{l=1}^{n}\left(\log p_{\lambda}(o_{i}.s_{l}^{t+1},r_{i}^{t}\mid \mathrm{pa}_{\mathcal{G}_{C(i)}}(o_{i}.s_{l}^{t+1}),\mathrm{pa}_{\mathcal{G}_{ C(i)}}(r_{i}^{t})\right)-\sum_{j=1}^{k}\left\|\mathcal{G}^{C_{j}}\right\|_{1}\]

where \(m\) and \(k\) indicate the number of single-object environments and object type classes, respectively, and \(\mathrm{pa}_{\mathcal{G}_{C(i)}}\) denotes the parents of a variable in the template class graph for the class \(C(i)\) of object \(o_{i}\). After this step, we fix the learned \(\{\mathcal{G}^{C_{1}},\mathcal{G}^{C_{2}},\dots,\mathcal{G}^{C_{k}}\}\) with the exception of the edges from the latent parameters \(\theta\), which here we assume are disabled and we will learn in the next step. In later stages, we will reuse the learned reward model \(\lambda_{r}\) and the class template graphs.

### Step 2: Interaction Learning and Latent Parameter Inference in Multi-object Environments

In this step, we consider data \(\mathcal{D}^{\mathrm{multi}}\) from \(N\) multi-object environments with different object configurations and in which the objects can have varying latent parameters. Formally, we define \(\mathcal{D}^{\mathrm{multi}}=\{\{\mathbf{o}_{1}^{t},\mathbf{a}_{1}^{t},r_{1}^ {t},\mathbf{o}_{2}^{t},\mathbf{a}_{2}^{t},r_{2}^{t},\dots,\mathbf{o}_{m}^{t}, \mathbf{a}_{m}^{t},r_{m}^{t}\}_{t=1}^{T}\}_{m=1}^{N}\). In each of these environments we assume the agent can interact only with one object at a time. On these data, we again extract the objects and their attributes from a sequence of images with pretrained object-centric methods and classify the objects using the object classifier. We use these data to learn the interaction pattern graphs \(\mathcal{G}_{C_{i},C_{j}}\) for each pair of classes \(C_{i}\) and \(C_{j}\) and the dynamic interaction graph \(\mathcal{G}_{\mathrm{inter}}\) by exploiting the previously learned class template graphs. In particular, we first learn the action binding \(\alpha^{t}\), and at a second stage, we jointly learn the rest of the dynamic interaction graph \(\mathcal{G}_{\mathrm{inter}}\), the interaction patterns \(\mathcal{G}_{C_{i},C_{j}}\) for each pair of classes, the object-specific latent parameters \(o_{i}.\boldsymbol{\theta}\) and their edges to the other attributes in \(\mathcal{G}_{C_{j}}\). We describe these two stages in detail in the following.

#### 3.2.1 Step 2.1: Learning the Action Binding

Motivated by [35], we learn the dynamic action binding \(\alpha=\{\alpha^{1},\alpha^{2},\dots,\alpha^{T}\}\) using soft attention networks, which are modified by the single-head self-attention module in the Transformer model [34]. Specifically, we perform non-linear transformations on the states and actions using multi-layer perceptrons (MLPs) to derive the key \(\mathbf{k}^{t}=\langle f_{k}(\mathbf{o}_{1}^{t}),f_{k}(\mathbf{o}_{2}^{t}), \dots,f_{k}(\mathbf{o}_{m}^{t})\rangle\), query \(\mathbf{q}^{t}=f_{q}\left(\mathbf{a}^{t}\right)\), and value \(\mathbf{v}^{t}=\langle f_{v}(\mathbf{o}_{1}^{t}),f_{v}(\mathbf{o}_{2}^{t}), \dots,f_{v}(\mathbf{o}_{m}^{t})\rangle\), respectively. We then compute the attention weights \(\alpha^{t}=\texttt{softmax}\left((\mathbf{k}_{1}^{t})^{\intercal}\mathbf{q}^{t },(\mathbf{k}_{2}^{t})^{\intercal}\mathbf{q}^{t},\dots,(\mathbf{k}_{m}^{t})^{ \intercal}\mathbf{q}^{t}\right)\). We use the learned attention weights \(\alpha\) as the action binding selector, as it provides an estimation of the binding affinity from the action to the objects at each time step. The soft attention mechanism assigns weights from the action to the objects by multiplying the value vector \(\mathbf{v}^{t}\) with the attention weights \(\alpha^{t}\), and then embeds the weighted actions into the dynamics of each object. We maintain a fixed structure for the class template graphs and focus on learning the action binding selector by updating \(f_{k}\), \(f_{q}\), and \(f_{v}\).

#### 3.2.2 Step 2.2: Learning Dynamic Interaction Graphs

As noted in Section 2, the interaction among objects may change over time and usually occurs in a sparse manner. To learn this dynamic graph, we leverage a sequential latent variable model to inferthe object interaction graph. Following the neural relational inference (NRI) works [36; 37], we use an encoder to generate the latent variables and subsequently sample the interaction graph based on these variables. Specifically, we use graph neural networks (GNN) as the encoder module, where the nodes represent the states of each object, and the predicted edges denote the temporal interactions between the objects. In line with the dynamic NRI (dNRI) [37], we use recurrent units to model the temporal relations of the interaction graph. We outline the key components of the model in the following, and provide the detailed description in App. C.

**Encoder and prior** During training, at each time step \(t\), we use GNN layers to generate hidden embeddings \(\mathbf{h}^{t}=\textsc{GNN}\left(\mathbf{o}_{1}^{t},\mathbf{o}_{2}^{t},\ldots, \mathbf{o}_{m}^{t}\right)\). For each object pair \(o_{i}\) and \(o_{j}\), we then obtain \(\mathbf{h}_{(i,j)}^{t}\). For the encoder, we use a Gated Recurrent Unit (GRU) [39] to model the temporal dependency of the interaction graphs. The inputs to the GRU include the future embeddings of the states, which then generate the updated embeddings: \(\mathbf{h}_{(i,j),\mathrm{enc}}^{t}=\textsc{GRU}_{\mathrm{enc}}(\mathbf{h}_{(i,j)}^{t},\mathbf{h}_{(i,j)}^{t+1})\). For the prior, we also use a GRU, but in this context, we append \(\mathbf{h}_{(i,j)}^{t}\) with the output of the GRU from the previous step \(\mathbf{h}_{(i,j),\mathrm{prior}}^{t-1}\) as the input. In specific terms, we have \(\mathbf{h}_{(i,j),\mathrm{prior}}^{t}=\textsc{GRU}_{\mathrm{prior}}(\mathbf{h} _{(i,j)}^{t},\mathbf{h}_{(i,j),\mathrm{prior}}^{t-1})\). For both the encoder and the prior, we feed the output embedding from the GRUs to an MLP layer to derive the distribution of the latent variables \(q_{\phi}(\mathbf{z}^{t}\mid\mathbf{s}^{1:T})\), where \(\mathbf{s}^{1:T}=\{\mathbf{o}_{1}^{t},\ldots,\mathbf{o}_{m}^{t}\}_{t=1}^{T}\), and prior distribution \(p_{\phi}(\mathbf{z}^{t}\mid\mathbf{s}^{1:t},\mathbf{z}^{1:t-1})\), respectively. We assume that the encoder outputs a Bernoulli distribution for each edge and the graph \(\mathcal{G}_{\mathrm{inter}}\) is sampled using the Gumbel-Softmax trick [40].

**Decoder** We perform one-step prediction to generate \(\hat{\mathbf{o}}_{i}^{t+1}\) for each object \(o_{i}\), predicting the state dynamics with the learned graphs, including \(\mathcal{G}_{\mathrm{inter}}\), as well as the interaction pattern graph \(\mathcal{G}_{C_{i},C_{j}}\) for two objects with class \(C_{i}\) and \(C_{j}\). At the same time, our goal is also to learn the interaction pattern graph with sparsity regularization. We also learn the latent parameters, \(\boldsymbol{\theta}\) at this stage. Specifically, we also incorporate \(C_{j}.\boldsymbol{\theta}\) and the graph from \(\mathcal{G}_{C_{j},\boldsymbol{\theta},\mathbf{o}_{i}^{t}}\) for each object \(o_{i}\) with class \(C_{j}\) into the dynamics model and use the sparsity regularization for \(\mathcal{G}_{C_{j},\boldsymbol{\theta},\mathbf{o}_{i}}\).

Therefore, the learning objectives include maximizing the likelihood of the dynamics model, minimizing the KL divergence between \(p_{\phi}\) and \(q_{\phi}\) to estimate \(\boldsymbol{\theta}\), and encouraging the sparsity of the interaction pattern graph \(\mathcal{G}_{C_{i},C_{j}}\) and the subgraph from latent parameters to states \(\mathcal{G}_{C_{i},\boldsymbol{\theta},\mathbf{o}_{i}}\):

\[\operatorname{argmax}_{\lambda_{s},\phi,v,k,q,\theta,\mathcal{G}} \sum_{t=1}^{T}\sum_{i=1}^{m}\sum_{l=1}^{n}\log p_{\lambda_{s}}\Big{(}o_{i}.s_{ l}^{t+1}\mid\operatorname{pa}_{\mathcal{G}_{o_{i}}^{t}}(o_{i}.s_{l}^{t+1}),\{ \operatorname{pa}_{\mathcal{G}_{o_{i},o_{k}}^{t}}(o_{i}.s_{l}^{t+1})\}_{o_{k }\to o_{i}\in\mathcal{G}_{\mathrm{inter}}^{t}}\Big{)}\] \[-\sum_{j=1}^{k}\sum_{i=1}^{k}\big{\|}\mathcal{G}_{C_{i},C_{j}} \big{\|}_{1}-\sum_{i=1}^{m}\sum_{j=1}^{k}\big{\|}\mathcal{G}_{C_{j}, \boldsymbol{\theta},\mathbf{o}_{i}}\big{\|}_{1}-\sum_{t=2}^{T}\operatorname{KL} \big{(}q_{\phi}\left(\mathbf{z}^{t}\mid\mathbf{s}^{1:T}\right)\|p_{\phi}\left( \mathbf{z}^{t}\mid\mathbf{s}^{1:t},\mathbf{z}^{1:t-1}\right)\big{)}\]

where \(\mathcal{G}\) is the dynamic interaction graph \(\mathcal{G}_{\mathrm{inter}}\), interaction pattern graphs \(\{\mathcal{G}_{C_{i},C_{j}}\mid i\in\{1,2,\ldots,k\},j\in\{1,2,\ldots,k\}\}\) and the subgraph from latent parameters to states \(\{\mathcal{G}_{C_{j},\boldsymbol{\theta},\mathbf{o}_{i}}\mid i\in\{1,2,\ldots,m \},j\in\{1,2,\ldots,k\}\}\). Similarly, to Definition 1, \(\operatorname{pa}_{\mathcal{G}_{o_{i}}^{t}}(o_{i}.s_{l}^{t+1})\) indicates the parents of the attribute \(s_{l}\) for object \(o_{i}\) based on the class template graph \(\mathcal{G}_{C(i)}\), where \(C(i)\) is the class of \(o_{i}\), and where the action binding \(\alpha^{t}\) activates or deactivates any potential connections from \(\mathbf{a}^{t}\). In the second term of the conditioning, we iterate over the objects \(o_{k}\) that are interacting with \(o_{i}\) at time \(t\) in the dynamic interaction graph \(o_{k}\to o_{i}\in\mathcal{G}_{\mathrm{inter}}^{t}\). For each of these objects \(o_{k}\) we collect the attributes that interact with \(o_{i}.s_{l}\) in the instance interaction pattern \(\mathcal{G}_{o_{i},o_{k}}^{t}\) based on interaction pattern graph \(\mathcal{G}_{C(i),C(k)}\) for the respective classes \(C(i)\) and \(C(k)\). \(\lambda_{s}\) and \(\phi\) indicate the dynamics model and encoder parameters, while \(v,k,q\) are the parameters of MLPs for learning the attention models. After this, we have learned all the graphs, dynamics, and reward models that estimate DAFT-MDP.

### Step 3: Policy Learning and Imagination in Multi-object Environments

In the first two phases, we have learned the template for our world model, which we can now finetune to new multi-object domains by inferring the environment-specific latent parameters \(\boldsymbol{\theta}\) and the interaction graph \(\mathcal{G}_{\mathrm{inter}}\). We again consider several multi-object environments with different object configurations and in which the objects can have varying latent parameters. For each environment, we then use the finetuned environment-specific world model to create a set of imagined trajectories. Finally, we can learn a policy \(\pi^{*}(\mathbf{a}^{t}|\mathbf{s}^{t},\boldsymbol{\theta},\mathcal{G}_{\mathrm{ inter}})\) across different environments, based on the real and imagined trajectories. We can apply policy learning or planning methods using any RL algorithms. Totake full advantage of the estimated models, we use RL or planning methods such as model predictive control (MPC) [41] and proximal policy optimization (PPO) [42] to learn \(\pi^{*}\). Detailed information about the domain parameters are provided in App. C.

### Step 4: Adaptation to the New multi-Object Environment

In a new environment, we apply the policy \(\pi^{*}(\mathbf{a}^{t}|\mathbf{s}^{t},\boldsymbol{\theta},\mathcal{G}_{\mathrm{ inter}})\) by inferring latent parameters \(\boldsymbol{\theta}\) and dynamic interaction graphs \(\mathcal{G}_{\mathrm{inter}}\) based on a few trajectories, without any policy learning.

## 4 Related Work

We shortly summarize the related work in this section and provide a more detailed discussion of related work, including the discussion of each method and a comparison based on the method features in App. A. Recent work in object-oriented and relational RL has incorporated various inductive biases for modeling object relations into both model-based and model-free RL frameworks. Zhou et al. [15] investigate the benefits of deep learning architectures, such as MLP, self-attention, and deep-sets in goal-conditioned RL with factorized object states. Likewise, Yoon et al. [13] provide a comprehensive empirical evaluation of pre-trained object-oriented models, such as SA [30] and SLATE [43], for model and policy learning in multi-object RL. Mambelli et al. [7] use linear relational networks to model object interactions and learn the policy. Another line of research focuses on learning structured representations among objects or their interactions [9; 5; 6; 8; 12; 17; 16; 44; 45; 46; 47; 48; 49]. Most of these approaches aim to learn an object-wise factorization model, either with structured symbolic input or high-dimensional raw pixels as input. NCS [12] and STEDIE [17] go further by disentangling action/control-relevant or irrelevant features for each object. Unlike these works, we propose a more fine-grained factored world model that considers the structure among all attributes of the objects as well as the dynamic interaction among all objects.

## 5 Experimental Evaluation

We consider a diverse set of RL benchmarks and setups, including modified OpenAI Fetch environments [50; 15; 51] (symbolic inputs), Spriteworld [9] (visual inputs), and the Block-stacking benchmark [52](visual inputs). We compare our approach with several baseline models. These include methods using Deep sets, GNN, and Self-Attention as the inductive bias [15; 4], such as SRICS [6], STOVE [8], SMORL [5], NCS [12], LRN [7], and COBRA [9]. To ensure a fair comparison, we modify these baselines, so that methods that originally only support symbolic input can also be adapted to handle image-based input by using visual encoders to obtain the symbolic states, and add an imagination component. We provide the details of these modifications in App. D.1. We provide descriptions of all setups, training and testing domains for each benchmark in App. D.2. Here we describe the most representative results, but we provide the complete results in App. D.3.

Symbolic benchmark: OpenAI Fetch - Push and Switch.Following [15], we modify the OpenAI Gym Fetch environment [50] to create the \(N\)-push and \(N\)-switch benchmarks, where \(N\) denotes the number of objects. In the \(N\)-push task, the agent is trained to push all cubes to their corresponding target positions. Similarly, for the \(N\)-switch task, the agent is required to flip all switches in the environment. In this benchmark, all inputs are symbolic. As a sanity check, we show in App. D.3 that our method has comparable s with the baselines in the _single-task mode_, in which we train the model estimation and policy learning individually on the \(2\)-Push, \(3\)-Push, \(2\)-Switch, and \(3\)-Switch tasks. To evaluate _compositional generalization_, we train on the set of tasks {\(1\)-Push, \(1\)-Switch, \(2\)-Push, \(2\)-Switch}; while during the test phase, we test the model in different settings as shown in Table 1. We consider combinations of skills (denoted by \(S\)), e.g. \(2\)-Push+\(2\)-Switch (\(S\)), which combines the training tasks \(2\)-Push, \(2\)-Switch. We also consider changes in the number of objects (denoted by \(O\)) and skills, e.g. \(3\)-Push+\(3\)-Switch (\(S\)+\(O\)), which combines the training tasks but also varies the number of objects. We also test whether our model can achieve efficient transfer during testing when the objects' latent parameters differ from the training samples (denoted by \(L\)). Generally, during training, we consider the objects with masses and friction coefficients uniformly sampled from a set of values, and during testing, we test the model with two different sets of values. For example \(2\)-Switch (\(L\)) considers this case. Finally, we consider the challenging setting in which we combine all of these changes. As seen in Table 1, in most cases DAFT-RL outperforms the baselines, with a bigger gain 

[MISSING_PAGE_FAIL:9]

the learned graphs in App. D.3, which show that our model is capable of learning the true causal of a single object in symbolic cases. For pixel inputs, we do not have ground-truth causal graphs.

## 6 Conclusions, Limitations, and Future Work

We proposed Dynamic Attribute FacTored RL (DAFT-RL), a framework that leverages learned attribute-factored representations with dynamic graphs. For each class of object, we learned a class template graph that describes how the dynamics and reward of an object of this class factorize according to its attributes, as well as an interaction pattern graph that describes how it interacts with objects of different classes at the attribute level. We also learned about interactions between objects and with the agent with a dynamic graph. Through this template world model, we learned a policy that can then be directly applied in a new environment by estimating the interactions and latent parameters. We showed that DAFT-RL outperforms the state-of-the-art in three compositional generalization benchmarks.

One limitation of our method is that during the adaptation phase, we only consider object classes that the agent has encountered before. This means that our model might encounter challenges when introduced to unseen object classes. One potential area for future exploration is the integration of open-set recognition [53] to identify and adapt to unseen object classes. Furthermore, we could integrate the causal representations [54] in multi-entity environments that could benefit the downstream compositional generalization tasks. Another exciting future direction is to explore the real-world object-centric manipulation or interactive tasks [55, 56, 57, 58] or even complex embodied AI tasks [59, 60].

#### Acknowledgments

FF would like to acknowledge the CityU High-Performance Computing (HPC) resources in Hong Kong SAR and LISA HPC from the SURF.nl. SM was supported by the MIT-IBM Watson AI Lab and the Air Force Office of Scientific Research under award number FA8655-22-1-7155. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the United States Air Force.

Figure 3: A. The smoothed learning curve for \(2\)-Push + \(2\)-Switch (L+S) with different friction coefficients for each object (for clarity, we show only the top three methods in terms of the success rate); B. The smoothed learning curve for the object comparison task in Spriteworld with unseen object numbers, combinations of colors and shapes (for clarity, we show only the top three methods in terms of the success rate); C. Success rate versus number of blocks in the stacking task, where each block has distinct mass.

\begin{table}
\begin{tabular}{l c c c c c c c c}
**Experiment** & \multicolumn{6}{c}{**Method**} \\
**Setting** & DAF-RL (SA) & DAF-RL (AIR) & SMORL & SRICS & GNN & STOVE & NCS & LRN \\ \hline
2 Blocks & \(0.809\pm 0.019\) & \(\mathbf{0.838\pm 0.030}\) & \(0.658\pm 0.028\) & \(0.704\pm 0.016\) & \(0.549\pm 0.016\) & \(0.728\pm 0.044\) & \(0.797\pm 0.035\) & \(0.649\pm 0.026\) \\
4 Blocks & \(\mathbf{0.735\pm 0.032}\) & \(0.698\pm 0.022\) & \(0.605\pm 0.020\) & \(0.591\pm 0.049\) & \(0.526\pm 0.041\) & \(0.498\pm 0.013\) & \(0.571\pm 0.026\) & \(0.461\pm 0.028\) \\
6 Blocks & \(0.591\pm 0.025\) & \(\mathbf{0.664\pm 0.017}\) & \(0.536\pm 0.040\) & \(0.509\pm 0.043\) & \(0.461\pm 0.088\) & \(0.475\pm 0.023\) & \(0.521\pm 0.049\) & \(0.602\pm 0.097\) \\
8 Blocks & \(0.506\pm 0.083\) & \(\mathbf{0.571\pm 0.039}\) & \(0.386\pm 0.062\) & \(0.420\pm 0.061\) & \(0.334\pm 0.047\) & \(0.278\pm 0.086\) & \(0.397\pm 0.052\) & \(0.463\pm 0.077\

## References

* Hafner et al. [2019] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In _International Conference on Machine Learning (ICML)_, pages 2555-2565. PMLR, 2019.
* Hafner et al. [2020] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In _International Conference on Learning Representations (ICLR)_, 2020. URL [https://openreview.net/forum?id=S110TC4tDS](https://openreview.net/forum?id=S110TC4tDS).
* Micheli et al. [2023] Vincent Micheli, Eloi Alonso, and Francois Fleuret. Transformers are sample-efficient world models. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=vhFulAcb0xb](https://openreview.net/forum?id=vhFulAcb0xb).
* Li et al. [2020] Richard Li, Allan Jabri, Trevor Darrell, and Pulkit Agrawal. Towards practical multi-object manipulation using relational reinforcement learning. In _2020 IEEE international conference on robotics and automation (ICRA)_, pages 4051-4058. IEEE, 2020.
* Zadaianchuk et al. [2021] Andrii Zadaianchuk, Maximilian Seitzer, and Georg Martius. Self-supervised visual reinforcement learning with object-centric representations. In _International Conference on Learning Representations (ICLR)_, 2021. URL [https://openreview.net/forum?id=xpPLmXCbOw1](https://openreview.net/forum?id=xpPLmXCbOw1).
* Zadaianchuk et al. [2022] Andrii Zadaianchuk, Georg Martius, and Fanny Yang. Self-supervised reinforcement learning with independently controllable subgoals. In _Conference on Robot Learning (CoRL)_, pages 384-394. PMLR, 2022. URL [https://openreview.net/forum?id=TEQWRLncJVm](https://openreview.net/forum?id=TEQWRLncJVm).
* Mambelli et al. [2022] Davide Mambelli, Frederik Trauble, Stefan Bauer, Bernhard Scholkopf, and Francesco Locatello. Compositional multi-object reinforcement learning with linear relation networks. In _ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality_, 2022. URL [https://openreview.net/forum?id=HFUxPr_I5ec](https://openreview.net/forum?id=HFUxPr_I5ec).
* Kossen et al. [2020] Jannik Kossen, Karl Stelzner, Marcel Hussing, Claas Voelcker, and Kristian Kersting. Structured object-aware physics prediction for video modeling and planning. In _International Conference on Learning Representations (ICLR)_, 2020. URL [https://openreview.net/forum?id=B1e-kxSKDH](https://openreview.net/forum?id=B1e-kxSKDH).
* Watters et al. [2019] Nicholas Watters, Loic Matthey, Matko Bosnjak, Christopher P Burgess, and Alexander Lerchner. Cobra: Data-efficient model-based rl through unsupervised object discovery and curiosity-driven exploration. _arXiv preprint arXiv:1905.09275_, 2019.
* Goyal et al. [2021] Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Scholkopf. Recurrent independent mechanisms. In _International Conference on Learning Representations (ICLR)_, 2021. URL [https://openreview.net/forum?id=mLcmdiEUXy-](https://openreview.net/forum?id=mLcmdiEUXy-).
* Choi et al. [2023] Jongwook Choi, Sungtae Lee, Xinyu Wang, Sungryull Sohn, and Honglak Lee. Unsupervised object interaction learning with counterfactual dynamics models. In _Workshop on Reincarnating Reinforcement Learning at ICLR 2023_, 2023. URL [https://openreview.net/forum?id=GYJH8NV81K](https://openreview.net/forum?id=GYJH8NV81K).
* Chang et al. [2023] Michael Chang, Alyssa Li Dayan, Franziska Meier, Thomas L. Griffiths, Sergey Levine, and Amy Zhang. Hierarchical abstraction for combinatorial generalization in object rearrangement. In _The Eleventh International Conference on Learning Representations (ICLR)_, 2023. URL [https://openreview.net/forum?id=fGG6VHb3W9W](https://openreview.net/forum?id=fGG6VHb3W9W).
* Yoon et al. [2023] Jaesik Yoon, Yi-Fu Wu, Heechul Bae, and Sungjin Ahn. An investigation into pre-training object-centric representations for reinforcement learning. In _International Conference on Machine Learning (ICML)_, pages 40147-40174. PMLR, 2023.
* Zhao et al. [2022] Linfeng Zhao, Lingzhi Kong, Robin Walters, and Lawson LS Wong. Toward compositional generalization in object-oriented world modeling. In _International Conference on Machine Learning (ICML)_, pages 26841-26864. PMLR, 2022.

* Zhou et al. [2022] Allan Zhou, Vikash Kumar, Chelsea Finn, and Aravind Rajeswaran. Policy architectures for compositional generalization in control. In _ICML Workshop on Spurious Correlations, Invariance and Stability_, 2022.
* Biza et al. [2022] Ondrej Biza, Thomas Kipf, David Klee, Robert Platt, Jan-Willem van de Meent, and Lawson LS Wong. Factored world models for zero-shot generalization in robotic manipulation. _arXiv preprint arXiv:2202.05333_, 2022.
* Nakano et al. [2023] Akihiro Nakano, Masahiro Suzuki, and Yutaka Matsuo. Interaction-based disentanglement of entities for object-centric world models. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=JQc2VowqCzz](https://openreview.net/forum?id=JQc2VowqCzz).
* Sallans and Hinton [2004] Brian Sallans and Geoffrey E Hinton. Reinforcement learning with factored states and actions. _The Journal of Machine Learning Research_, 5:1063-1088, 2004.
* Kearns and Koller [1999] Michael Kearns and Daphne Koller. Efficient reinforcement learning in factored mdps. In _Proceedings of the 16th international joint conference on Artificial Intelligence (IJCAI)_, pages 740-747, 1999.
* Boutilier et al. [2000] Craig Boutilier, Richard Dearden, and Moises Goldszmidt. Stochastic dynamic programming with factored representations. _Artificial intelligence_, 121(1-2):49-107, 2000.
* Guestrin et al. [2003] Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha Venkataraman. Efficient solution algorithms for factored mdps. _Journal of Artificial Intelligence Research_, 19:399-468, 2003.
* Hallak et al. [2015] Assaf Hallak, Francois Schnitzler, Timothy Mann, and Shie Mannor. Off-policy model-based learning under unknown factored dynamics. In _International Conference on Machine Learning (ICML)_, pages 711-719. PMLR, 2015.
* Katt et al. [2019] Sammie Katt, Frans A. Oliehoek, and Christopher Amato. Bayesian reinforcement learning in factored pomdps. In _Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)_, page 7-15, 2019.
* Gardiol and Kaelbling [2003] Natalia Gardiol and Leslie Kaelbling. Envelope-based planning in relational mdps. _Advances in Neural Information Processing Systems (NIPS)_, 16, 2003.
* Guestrin et al. [2003] Carlos Guestrin, Daphne Koller, Chris Gearhart, and Neal Kanodia. Generalizing plans to new environments in relational mdps. In _Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI)_, pages 1003-1010, 2003.
* Van Otterlo [2005] Martijn Van Otterlo. A survey of reinforcement learning in relational domains. _Centre for Telematics and Information Technology (CTIT) University of Twente, Tech. Rep_, 2005.
* Diuk et al. [2008] Carlos Diuk, Andre Cohen, and Michael L Littman. An object-oriented representation for efficient reinforcement learning. In _International Conference on Machine Learning (ICML)_, pages 240-247, 2008.
* Scholz et al. [2014] Jonathan Scholz, Martin Levihn, Charles Isbell, and David Wingate. A physics-based model prior for object-oriented mdps. In _International Conference on Machine Learning (ICML)_, pages 1089-1097. PMLR, 2014.
* Wandzel et al. [2019] Arthur Wandzel, Yoonseon Oh, Michael Fishman, Nishanth Kumar, Lawson L.S. Wong, and Stefanie Tellex. Multi-object search using object-oriented pomdps. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 7194-7200, 2019. doi: 10.1109/ICRA.2019.8793888.
* Locatello et al. [2020] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. _Advances in Neural Information Processing Systems (NeurIPS)_, 33:11525-11538, 2020.
* Eslami et al. [2016] SM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton, et al. Attend, infer, repeat: Fast scene understanding with generative models. _Advances in neural information processing systems_, 29, 2016.

* Huang et al. [2022] Biwei Huang, Fan Feng, Chaochao Lu, Sara Magliacane, and Kun Zhang. AdaRL: What, where, and how to adapt in transfer reinforcement learning. In _International Conference on Learning Representations (ICLR)_, 2022. URL [https://openreview.net/forum?id=8H5bDvWvvt5](https://openreview.net/forum?id=8H5bDvWvvt5).
* Feng et al. [2022] Fan Feng, Biwei Huang, Kun Zhang, and Sara Magliacane. Factored adaptation for non-stationary reinforcement learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems (NIPS)_, 30, 2017.
* Biza et al. [2022] Ondrej Biza, Robert Platt, Jan-Willem van de Meent, Lawson L.S. Wong, and Thomas Kipf. Binding actions to objects in world models. In _ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality_, 2022. URL [https://openreview.net/forum?id=H1mz8BuUc1c](https://openreview.net/forum?id=H1mz8BuUc1c).
* Kipf et al. [2018] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In _International Conference on Machine Learning (ICML)_, pages 2688-2697. PMLR, 2018.
* Graber and Schwing [2020] Colin Graber and Alexander G Schwing. Dynamic neural relational inference. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8513-8522, 2020.
* Murphy [2002] Kevin Murphy. Dynamic bayesian networks: Representation, inference and learning. _UC Berkeley, Computer Science Division_, 2002.
* Chung et al. [2014] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. _NIPS 2014 Deep Learning and Representation Learning Workshop_, 2014.
* Jang et al. [2017] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In _International Conference on Learning Representations (ICLR)_, 2017. URL [https://openreview.net/forum?id=rkE3y85ee](https://openreview.net/forum?id=rkE3y85ee).
* Camacho and Alba [2013] Eduardo F Camacho and Carlos Bordons Alba. _Model predictive control_. Springer science & business media, 2013.
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Singh et al. [2022] Gautam Singh, Fei Deng, and Sungjin Ahn. Illiterate DALL-e learns to compose. In _International Conference on Learning Representations (ICML)_, 2022. URL [https://openreview.net/forum?id=h0OYVOWe3oh](https://openreview.net/forum?id=h0OYVOWe3oh).
* Kipf et al. [2020] Thomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world models. In _International Conference on Learning Representations (ICLR)_, 2020. URL [https://openreview.net/forum?id=H1gax6VtDB](https://openreview.net/forum?id=H1gax6VtDB).
* Chuck et al. [2023] Caleb Chuck, Kevin Black, Aditya Arjun, Yuke Zhu, and Scott Niekum. Granger-causal hierarchical skill discovery. _arXiv preprint arXiv:2306.09509_, 2023.
* Hu et al. [2023] Jiaheng Hu, Zizhao Wang, Peter Stone, and Roberto Martin-Martin. Elden: Exploration via local dependencies. _arXiv preprint arXiv:2310.08702_, 2023.
* Assouel et al. [2022] Rim Assouel, Pau Rodriguez, Perouz Taslakian, David Vazquez, and Yoshua Bengio. Object-centric compositional imagination for visual abstract reasoning. In _ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality_, 2022.
* Hartford et al. [2018] Jason Hartford, Devon Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models of interactions across sets. In _International Conference on Machine Learning_, pages 1909-1918. PMLR, 2018.

* Nath et al. [2023] Somjit Nath, Gopeshhh Subbaraj, Khimya Khetarpal, and Samira Ebrahimi Kahou. Discovering object-centric generalized value functions from pixels. In _International Conference on Machine Learning (ICML)_, pages 25755-25768. PMLR, 2023.
* Brockman et al. [2016] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. _arXiv preprint arXiv:1606.01540_, 2016.
* Lanier [2019] John Banister Lanier. _Curiosity-driven multi-criteria hindsight experience replay_. University of California, Irvine, 2019.
* Plappert et al. [2018] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. _arXiv preprint arXiv:1802.09464_, 2018.
* Geng et al. [2020] Chuanxing Geng, Sheng-jun Huang, and Songcan Chen. Recent advances in open set recognition: A survey. _IEEE transactions on pattern analysis and machine intelligence_, 43(10):3614-3631, 2020.
* Lippe et al. [2023] Phillip Lippe, Sara Magliacane, Sindy Lowe, Yuki M Asano, Taco Cohen, and Efstratios Gavves. BISCUIT: Causal representation learning from binary interactions. In _Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI)_, volume 216 of _Proceedings of Machine Learning Research_, pages 1263-1273. PMLR, 2023.
* Ajay et al. [2023] Anurag Ajay, Seungwook Han, Yilun Du, Shaung Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum, Leslie Kaelbling, Akash Srivastava, and Pulkit Agrawal. Compositional foundation models for hierarchical planning. _arXiv preprint arXiv:2309.08587_, 2023.
* Yang et al. [2023] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. _arXiv preprint arXiv:2310.06114_, 2023.
* Zhu et al. [2023] Yifeng Zhu, Zhenyu Jiang, Peter Stone, and Yuke Zhu. Learning generalizable manipulation policies with object-centric 3d representations. In _7th Annual Conference on Robot Learning (CoRL)_, 2023.
* Mao et al. [2023] Jiayuan Mao, Tomas Lozano-Perez, Joshua B Tenenbaum, and Leslie Pack Kaelbling. Learning reusable manipulation strategies. In _7th Annual Conference on Robot Learning (CoRL)_, 2023.
* Padalkar et al. [2023] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x-embodiment: Robotic learning datasets and rt-x models. _arXiv preprint arXiv:2310.08864_, 2023.
* Li et al. [2023] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martin-Martin, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In _Conference on Robot Learning (CoRL)_, pages 80-93. PMLR, 2023.
* Veerapaneni et al. [2020] Rishi Veerapaneni, John D Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu, Joshua Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement learning. In _Conference on Robot Learning (CoRL)_, pages 1439-1456. PMLR, 2020.

## Appendix A Detailed Discussion and Comparison with Related Work

In this section we discuss the related work. We first shortly discuss related work in terms of our modelling assumptions and then provide a comparison with other object-centric RL methods for compositional generalization.

### Factored, Relational and Object-oriented MDPs

Our modelling assumptions, formalized as a Dynamic Attribute FacTored Markov Decision Process (DAFT-MDP) in Definition 1, are related to the literature on factored and object-oriented (PO)MDPs. In particular DAFT-MDPs are an first order extension with class template graphs, interaction patterns and interaction graphs of factored (PO)MDPs [18; 19; 20; 21; 22; 23]. In particular they define a family of factored POMDPs, in which the objects \(\mathcal{O}\) can vary across environments in numbers, types and latent parameters. This extension takes inspiration from Relational MDPs [24; 25; 26] and their literature, especially Object-Oriented (PO)MDPs [27; 28; 29], which also define a family of MDPs that vary in terms of objects and types. Most of these methods focus on discrete states and often define relations or states as first-order predicates, with the exception of [28], which propose a physics-based approach that circumvents these issues, allowing for more realistic robotic applications.

As opposed to these works, we also consider continuous state spaces and propose a more fine-grained factorization of the transition and reward at the object _attribute_ level, based on estimating class template graphs and interaction pattern graphs. Additionally, we consider object-specific latent parameters that modulate the dynamics of each object and use _dynamic_ graphs to account for the sparsity of interactions between objects and between the agent and an object.

### Compositional Generalization with Object-centric RL Approaches

Table A1 provides a comparison of the object-centric RL methods discussed in Section 4. The criteria for comparison include the extent of factorization in the dynamics, the interaction modeling, and the model's ability to adapt to changes in the environment with changing latent factors.

In the context of factored dynamics, most of these related works take into consideration the object-factored states. Approaches such as STEDIE [17], NCS [12], and STOVE [8] go a step further to disentangle the states related to interaction, dynamics, or actions from other state variables, such as color, as mentioned in the Table. However, our model aims to offer a more comprehensive approach to factored dynamics, providing an attribute-level factorization for each object. For example this can include factorizing the transition function of dynamics-related factors in terms of position, velocity and

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multirow{2}{*}{**Methods**} & **Featured** & **Interaction** & **Generative to** \\  & **Dynamics** & **Modeling** & **Changing Latent** \\ \hline \(\mathcal{O}\)**(SN)** [4] & Object & Fully-connected & \(\mathcal{X}\) \\ Self-attention [5] & Object & Fully-connected & \(\mathcal{X}\) \\ Deep Semi [13] & Object & Fully-connected & \(\mathcal{X}\) \\ LBP [14] & Object & Fully-connected & \(\mathcal{X}\) \\ CORA [1] & Object & Fully-connected & \(\mathcal{X}\) \\ FWM [15] & Object & Fully-connected & \(\mathcal{X}\) \\ SMOKE [1] & Object & Fully-connected & \(\mathcal{X}\) \\ STOVE [9] & Object \& re-determined attributes \({}^{*}\) & Fully-connected & \(\mathcal{X}\) \\ SRGCS [10] & Object \& re-determined attributes \({}^{*}\) & Fully-connected & \(\mathcal{X}\) \\ NCS [12] & Object \& re-determined attributes \({}^{*}\) & Fully-connected & \(\mathcal{X}\) \\ STEDIE [17] & Object \& re-determined attributes \({}^{*}\) & Fully-connected & (interaction-relevant) & \(\mathcal{X}\) \\ DAPFLR & Object \& attribute & Spatio-dynamic \& \(\mathcal{X}\) (object-level \& attribute-level) & \(\mathcal{X}\) \\ \hline \hline \end{tabular}

* \({}^{a}\)STOVE focuses specifically on these attributes: (position, velocity, size, others).
* \({}^{b}\)STEDIE learns which attributes interact during object interactions, similar to our interaction pattern graphs.

\end{table}
Table A1: Comparison of different object-oriented RL methods for compositional generalization. A check denotes that a method has an attribute, whereas a cross denotes the opposite.

mass, and factorizing action-irrelevant factors like color and shape. In terms of interaction modeling, many approaches assume fully-connected interaction patterns, where interactions occur between every pair of objects within a given scene. However, in many real-world scenarios, this is often inaccurate as the object interaction usually happens in a sparse and dynamic manner. An exception is SRICS [6], which takes into account dynamic interaction modeling. This model emphasizes the sparsity of interactions among objects in a scene, with a dynamically changing structure at the object level. In contrast, STEDIE [17] employs learned factored dynamics for each object to model interactions, assuming that only interaction-irrelevant objects will have an impact on others. Nonetheless, from the perspective of the object level, the pattern of interaction in STEDIE is still fully connected. Our method diverges from these approaches by integrating both dynamic and sparse interaction patterns at the object level, and factored interactions at the attribute level between pairs of objects. We provide a more fine-grained and realistic model of interactions as from the object level, the interaction pattern is dynamic and sparse, and for each object pair, the interaction only happens among some specific attributes. Finally, DAFT-RL is the only framework that considers the latent changing factors in the environment, which is also a realistic setting where the agent sometimes cannot observe all essential attribute variables in the system but these unobserved factors do have effects on the dynamics or reward.

## Appendix B Full Example with Summary of Notations

In this section, we first provide a summary of the notation in Table A2. Then we show an example of the environment described in the main paper, and how the learned graphs are connected in a single ground graphical model, as described in Figure A1.

In our example, we consider two classes \(\mathcal{C}=\{C_{1},C_{2}\}\), which represent the classes of boxes and switches, respectively. We represent these two classes with cubes and cylinders in Figure A1.

For the class of boxes \(C_{1}\), represented in Figure A1A as a pink cube, we consider the attributes \(\{C_{1}.s_{1},C_{1}.s_{2},C_{1}.s_{3}\}\) to represent the color, velocity and position of an abstract box in two consecutive timesteps \(t\) and \(t+1\). All of the relationship between the attributes are described in the class template graph \(\mathcal{G}_{C_{1}}\). The edge \(C_{1}.s_{1}^{t}\to C_{1}.s_{1}^{t+1}\) represents the fact that the color at timestep \(t+1\) is only influenced by the color at timestep \(t\) (in this case being constant). The edge \(C_{1}.s_{2}^{t}\to C_{1}.s_{2}^{t+1}\) represents the fact that the velocity at timestep \(t+1\) is influenced by the velocity at the previous timestep \(t\). The edge \(C_{1}.s_{2}^{t}\to C_{1}.s_{3}^{t+1}\) means that velocity can influence position in the next timestep,which is also influenced by the previous position \(C_{1}.s_{3}^{t+1}\). In this case the agent can optionally act (the dashed lines from \(\mathbf{a}^{t}\)) and influence the velocity \(C_{1}.s_{2}^{t+1}\) and position \(C_{1}.s_{3}^{t+1}\). Finally the latent constant parameters for the abstract box \(C_{1}.\theta\), in this case representing friction, influence the velocity \(C_{1}.s_{2}^{t+1}\) and position \(C_{1}.s_{3}^{t+1}\) at each timestep \(t+1\) and modulate the effect of the action on the position and velocity. The reward \(r^{t}\) only depends on the position \(C_{1}.s_{3}^{t}\).

For the class of switches \(C_{2}\), represented in Figure A1A as a blue cylinder, we consider the attributes \(\{C_{2}.s_{1},C_{2}.s_{2},C_{2}.s_{3}\}\) to represent the color, angle and position of an abstract switch in two consecutive timesteps \(t\) and \(t+1\). All of the relationship between the attributes are described in the class template graph \(\mathcal{G}_{C_{2}}\). Similarly to boxes, the color \(C_{2}.s_{1}\) does not influence anything and is only influenced by its previous value. In this example, we consider that none of the attributes influences each other, but that the reward \(r^{t}\) is only a function of the angle \(C_{2}.s_{2}^{t}\). Additionally, the latent constant parameters \(C_{2}.\theta\) only influence the position \(C_{2}.s_{3}^{t}\), as can potentially the action \(\mathbf{a}^{t}\) (represented by the dashed red lines).

In Figure A1B we show the interaction pattern graph for boxes \(\mathcal{G}_{C_{1},C_{1}}\) that represents the way that attributes of two objects of the class box \(C_{1}\) interact with each other. In the figure, we use pink to represent the first object and green to represent the second object. Specifically, in this case, the interaction between two boxes means that the velocity of an object influences the velocity of another object. Similarly, the velocity of an object influences the color of the other object. Additionally, the latent constant parameters of an object influence the position of the other object.

In Figure A1C we consider a specific environment, specified by the objects \(\mathcal{O}=\{o_{1},o_{2},o_{3}\}\), where \(o_{1}\) and \(o_{3}\) are boxes, while \(o_{2}\) is a switch. We show an unrolled version of the dynamic interaction graph \(\mathcal{G}_{\mathrm{inter}}\) for three consecutive timesteps for these objects. At timestep \(t\) there is an interaction between the two boxes \(o_{1}\) and \(o_{3}\) (represented by blue lines), and the action \(\mathbf{a}^{t}\) binds to the switch \(o_{2}\), as selected by the action selector \(\boldsymbol{\alpha}^{t}\). The interactions between the two objects are instantiated following the interaction pattern graphs in Figure A1B. The action binding instead activates the red dashed lines in the class template graph in Figure A1A. In the next timestep \(t+1\) there are no interactions between objects, but the action is now bound to object \(o_{1}\).

In the bottom graph in Figure A1, we show how the three types of graphs are combined in this environment for these three consecutive timesteps. In particular, it can be seen that for each object, its attributes follow the same pattern as described by the class template graphs \(\mathcal{G}_{C_{1}}\) and \(\mathcal{G}_{C_{2}}\) (in grey). The interactions between the two boxes in timestep \(t\) get instantiated following the interaction pattern graphs \(\mathcal{G}_{C_{1},C_{1}}\) (in light blue). The action binding specifies which object is affected by the action \(\mathbf{a}^{t}\) at each timestep, but the way the action affects the attributes of an object is based on its class template graph. This graph represents the complete graphical model representation for these three timesteps, that describes how the transition and reward functions factorize in these three timesteps.

As can be seen from the example, to learn the factorization of the transition and reward functions in a new environment, we can reuse the class template graphs and the interaction pattern graphs across different domains with the same type of objects. We only need to relearn the dynamic interaction graph, which is also the only graph that is dynamic (i.e. the edges do not repeat across every couple of consecutive timesteps).

## Appendix C Architecture and Implementation Details

In this section, we describe the architecture of DAFT-RL in more detail. Figure A2 summarizes the pipeline of learning the DAFT-RL framework, which uses the notation summarized in Table A2. We first provide a high-level illustration of the framework, and then provide the details in the following subsections. In the offline model estimation, the DAFT-MDP is learned through a two-step process, involving the estimation of both the class template graphs (Step 1), the interaction pattern graphs and dynamic interaction graphs (Step 2). These graphs capture the relationships and dependencies among objects from the attribute level in the environment. Once the DAFT-MDP is learned, policy learning is performed using trajectory imagination and planning methods (Step 3). During the adaptation phase, the dynamic graphs are inferred for the new domain (Step 4). This inference step allows for the transfer of the previously trained policy to the new domain without extensive retraining. By adapting the dynamic graphs to the new domain, the policy can be directly deployed and applied to the new environment. In the following, we define the graph structures and losses for each of these steps.

### Binary Matrices in DAFT-RL

We first introduce the definitions of all structures that are used in the loss functions in DAFT-RL:

Binary matrices in class template graphs.As described in the previous sections, we learn the class template graphs \(\mathcal{G}_{C_{j}}\) for each object class \(C_{j}\). This graph is assumed to be constant in time. In practice, these graphs are learned as binary adjacency matrices. To help express the loss functions in a factorized way, we define the following binary matrices.

For \(l=\{1,\ldots,n\}\) we define:

* \(C_{j}.G_{s\to s_{l}}\in\{0,1\}^{n}\) as the subset of the adjacency matrix of \(\mathcal{G}_{C_{j}}\) that describes the edges incoming into \(C_{j}.s_{l}^{t+1}\) from each \(C_{j}.s_{i}^{t}\) for \(i\in\{1,\ldots,n\}\). In other words this binary vector is 1 only for the components \(C_{j}.s_{i}^{t}\) for \(i\in\{1,\ldots,n\}\) that have an edge to \(C_{j}.s_{l}^{t+1}\) in \(\mathcal{G}_{C_{j}}\).
* \(C_{j}.G_{a\to s_{l}}\in\{0,1\}\) as a binary value that represents the existence of an edge between \(\mathbf{a}^{t}\) and \(C_{j}.s_{l}^{t+1}\), denoting how the action will affect the dynamics at the attribute \(s_{l}\) at next time step.
* \(C_{j}.G_{\theta\to s_{l}}\in\{0,1\}\) as a binary value that represents the existence of an edge between \(C_{j}.\theta^{t}\) and \(C_{j}.s_{l}^{t+1}\)

We additionally define:

* \(C_{j}.G_{s\to r}\in\{0,1\}^{n}\) is the subset of the adjacency matrix of \(\mathcal{G}_{C_{j}}\) that describes the edges incoming into \(r^{t+1}\) from each \(C_{j}.s_{i}^{t}\) for \(i\in\{1,\ldots,n\}\)
* \(C_{j}.G_{a\to r}\in\{0,1\}\) is a binary value that represents the existence of an edge between \(\mathbf{a}^{t}\).

For each object class \(C_{j}\) all of these five groups of binary matrices together represent the adjacency matrix of graph \(\mathcal{G}_{C_{j}}\).

Binary matrices in interaction pattern graphs.Interaction pattern graphs \(G_{C_{i},C_{j}}\) represent the interaction structures between attributes of the object with class \(C_{i}\) and the object with class \(C_{j}\), which are assumed to be constant in time. For each \(l=\{1,\ldots,n\}\) we define \(G_{C_{i}\to C_{j}.s^{t}}\in\{0,1\}^{n}\) to represent the binary vector that describes how the attributes of an object with class \(C_{i}\) influence the attribute \(C_{j}.s_{l}^{t+1}\) of the object with class \(C_{j}\). All of these vectors together represent the adjacency matrix of graph \(G_{C_{i},C_{j}}\).

Binary values in Dynamic interaction graphsThe dynamic interaction graphs \(G_{\mathrm{inter}}^{t}\) represent the interactions between the objects \(\mathcal{O}\) at timestep \(t\), which include the action selector \(\{\boldsymbol{\alpha}^{1},\boldsymbol{\alpha}^{2},\ldots,\boldsymbol{\alpha}^ {T}\}\), where \(\boldsymbol{\alpha}^{i}\in\{0,1\}^{m}\) represents which object is affected by \(\mathbf{a}^{t}\) at all time steps \(t=1,\ldots,T\). We use the action selection \(\boldsymbol{\alpha}^{i}\) in our losses. For each \(o_{i},o_{j}\in\mathcal{O}\) we define a binary value \(G_{\mathrm{inter}(i,j)}^{t}\in\{0,1\}\)to represent whether object \(o_{i}\) and \(o_{j}\) will have the interaction at time step \(t\). All of these values together represent the adjacency matrix of graph \(G_{\mathrm{inter}}^{t}\).

#### c.1.1 Detailed Loss Functions

We provide the detailed loss functions for learning the DAFT-MDP below. In each step, we mark in blue which of the structures defined in the previous subsections are learn at that step.

Step 1: Class learning in single-object environmentsAs described in Sec 3.1, we learn the class template graphs \(\{\mathcal{G}^{\mathrm{C_{1}}},\mathcal{G}^{\mathrm{C_{2}}},\ldots,\mathcal{G }^{\mathrm{C_{k}}}\}\) using the collected \(\mathcal{D}^{\mathrm{single}}\). We give the detailed loss functions, including the prediction loss \(\mathcal{L}_{\mathrm{pred}}^{1}\) and the sparsity regularization \(\mathcal{L}_{sparse}^{1}\) for dynamics and reward models, for this step below:

\[\mathcal{L}_{\mathrm{pred}}^{1}=\sum_{t=1}^{T}\sum_{i=1}^{m}\sum_ {l=1}^{n}(\log p_{\lambda_{s}}(o_{i}.s_{l}^{t+1}\mid C_{j}.G_{s\to s_{i}} \odot\mathbf{o_{i}}^{t},C_{j}.G_{a\to s_{l}}\odot\mathbf{a_{i}}^{t})\] (A1) \[\qquad\qquad\qquad\qquad\qquad+\log p_{\lambda_{r}}(r_{i}^{t}\mid C _{j}.G_{s\to r}\odot\mathbf{o_{i}}^{t},C_{j}.G_{a\to r}\odot\mathbf{a_{i}}^{t}))\]

\[\mathcal{L}_{\mathrm{sparse}}^{1}=\sum_{j=1}^{k}\left(\sum_{l=1}^{n}\left\|C_{ j}.G_{s\to s_{i}}\right\|_{1}+\left\|C_{j}.G_{s\to r}\right\|_{1}+\sum_{l=1}^{n} \left\|C_{j}.G_{a\to s_{l}}\right\|_{1}+\left\|C_{j}.G_{a\to r}\right\|_{1}\right)\] (A2)

where we update the prediction models including \(\lambda_{s},\lambda_{r}\), as well as the binary vectors defined previously (marked in blue). The complete loss \(\mathcal{L}^{1}\) for Step 1 is:

\[\mathcal{L}^{1}=\mathcal{L}_{\mathrm{pred}}^{1}+\gamma_{\mathrm{sparse}}^{1} \mathcal{L}_{\mathrm{sparse}}^{1}\] (A3)

where \(\gamma_{\mathrm{sparse}}^{1}\) is a hyper-parameter.

Step 2.1: Learning the action bindingIn this step, we learn the dynamic action selector \(\boldsymbol{\alpha}^{t}\) by using the soft attention networks. We have collected \(\mathcal{D}^{\mathrm{multi}}\) which have multiple objects in the scene. We consider that the class template graphs learned in the previous phase are fixed and update the prediction loss with the action selector. The detailed loss function for this stage is:

\[\mathcal{L}_{\mathrm{pred}}^{2}=\sum_{t=1}^{T}\sum_{i=1}^{m}\sum_{l=1}^{n}\log p _{\lambda_{s}}\left(o_{i}.s_{l}^{t+1}\mid C_{i}.G_{s\to s_{l}}\odot\mathbf{o_{ i}}^{t},C_{i}.G_{a\to s_{l}}\odot\alpha_{i}^{t}\cdot f_{v}(\mathbf{a}^{t})\right)\] (A4)

where \(f_{v}\) is the value network to be updated. We also need to learn the key and query networks \(f_{k}\) and \(f_{q}\) to update \(\alpha_{i}^{t}\) for each object \(o_{i}\) (marked in blue). Additionally, we use the same dynamic network \(\lambda_{s}\) and also update it in this stage.

Step 2.2: learning dynamic interaction graphAs described in Sec 3.2.2, we use dNRI [37] to learn the dynamic interaction graph. Additionally, we also learn all the interaction pattern graphs, as well as learn how to infer latent parameters, and how they influence each attribute, at this step. We consider the class template graphs and action binding are fixed from the previous step. Detailed loss functions are:

\[\mathcal{L}_{\mathrm{pred}}^{3}= \sum_{t=1}^{T}\sum_{i=1}^{m}\sum_{l=1}^{n}\log p_{\lambda_{s}}(o_ {i}.s_{l}^{t+1}\mid C_{i}.G_{s\to s_{l}}\odot\mathbf{o_{i}}^{t},C_{i}.G_{a \to s_{l}}\odot\alpha_{i}^{t}\cdot f_{v}(\mathbf{a}^{t}),\] (A5) \[C_{i}.G_{\theta\to s_{l}}\odot o_{i}.\theta,\{G_{\mathrm{inter}}^{t}(i,j)\cdot G_{C_{j}\to C_{i}.s_{l}}\odot\mathbf{o_{j}}^{t}\}_{j\in\{1,\ldots,m\} \setminus\{i\}}\]

\[\mathcal{L}_{\mathrm{KL}}^{3}=\sum_{t=2}^{T}\text{KL}\left(q_{\phi}

where \(\lambda_{s}\) is the dynamics model we have employed in the previous steps. We learn the encoder/prior networks \(\phi\) to generate the latent vectors \(\mathbf{z}\), where we sample the graph \(G_{\mathrm{inter}}\) from. Through the binary vectors we also learn the interaction pattern graphs \(\{G_{C_{i},C_{j}}\}_{i,j\in\{1,\ldots,k\}}\), latent parameters \(\{o_{i},\theta\}_{i=1}^{m}\) and the binary vectors representing the edges from latent parameters to each attribute \(\{C_{i}.G_{\theta\to s_{i}}\}_{i=1}^{k}\). We have completed the loss for step 2.2:

\[\mathcal{L}^{3}=\mathcal{L}_{\mathrm{pred}}^{3}+\gamma_{\mathrm{KL}}^{3} \mathcal{L}_{\mathrm{KL}}^{3}+\gamma_{\mathrm{sparse}}^{3}\mathcal{L}_{\mathrm{ sparse}}^{3}\] (A8)

where \(\gamma_{\mathrm{KL}}^{3}\) and \(\gamma_{\mathrm{sparse}}^{3}\) are the hyper-parameters.

#### c.1.2 Details for the Other Algorithms

Object class learningFor the case with pixel observation as input, we choose to learn the object classes in a supervised manner, where the input includes the extracted feature vectors \(\mathbf{o}_{i}\) for each object and we have the labels \(y_{i}\) for each single objects. We apply a cross-entropy loss to learn the mapping.

Interaction graph samplingDuring the learning of dynamic interaction graphs, we generate the edge distribution \(\mathbf{z}^{t}\) at each time step \(t\). We sample the edge matrix \(\mathbf{M}^{\mathrm{F}}\) of the graph \(\mathcal{G}_{\mathrm{inter}}^{t}\). Specifically, \(\mathbf{M}^{t}\sim\mathrm{Bern}(\mathbf{z}^{t})\), where \(\mathrm{Bern}\) is the multivariate Bernoulli distribution and all elements are mutually independent. We also employ the Gumbel-Softmax trick [40] to make the sampling process differentiable.

## Appendix D Experimental Details

In this section, we summarize all the experimental details. We first discuss our baselines and how we modified them to evaluate them in a fair comparison with DAFT-RL, focusing in particular on adapting symbolic input approaches to pixel inputs and image-based approaches to symbolic inputs. We then describe the experimental setups, including each task description and the modifications we made to the benchmarks.

### Baseline Modifications

Symbolic input approaches with pixel inputs.We adapt various approaches that rely on symbolic inputs, such as self-attention [15], deep sets [15], GNN [4], SRICS [6] and LRN [7], to handle scenarios where the inputs are raw pixels. To accomplish this, we leverage pre-trained object-centric models, specifically AIR [31], to obtain the state representation of each object within the scene. The pre-trained object-centric models, specifically AIR, are employed to extract object-specific information from the raw pixel inputs. These object-factored states, which represent the individual objects, are then used as inputs for the approaches mentioned above. By incorporating the object-centric models and leveraging their extracted object representations, we enable the symbolic approaches originally designed for other input formats to be applicable in scenarios where raw pixels serve as the input representation.

Image-based approaches with symbolic inputs.To modify the image-based approaches to fit with the benchmarks with symbolic inputs, we adopt the following changes:

* SMORL [5]: we remove the SCALOR encoder and directly obtain \(\mathbf{z}\) from the simulator.
* STOVE [8]: similarly, we remove the SuPAIR model and directly observe object-factored states for dynamics and policy modeling.
* NCS [12]: we directly observe the type and state variables from the simulator without the slot attention models.

Notably, we do not and cannot modify COBRA [9] for the symbolic case, because COBRA is mostly built upon the scene encoder and decoder.

### Experimental Setups

In this section, we give the task description for each of the three benchmarks and describe the detailed experimental settings for both model estimation and policy learning.

#### d.2.1 Task Description

OpenAI Fetch - Push and Switch.In this benchmarks the tasks are to push \(N\) boxes or flip \(M\) switches. The agent can obtain both the object factored states and the global (agent) states. Specifically, the object states are the object's pose and velocity. For switches, the states are also the switch angle and the position. The action is the 3D continuous control of the end effector displacement and 1D control of the distance between gripper fingers. The goals are the target position for cubes (pushing task) and the target angle of the switches (switching task). In the experiment, we consider a dense reward, computed by the distance between the object's states and the desired states in the goals.

Spriteworld.There are four tasks in the Spriteworld benchmark, as listed below. We follow the task sets as well as the modifications in [13]. The agent directly observes the 2D images as the observation states.

* **Object goal task**: the goal is to move the object to the target position without touching other distractor objects. The action includes four cardinal directions. A positive reward will be given if the goal is achieved.
* **Object interaction task**: the goal is to push the target to a given location in the scene. The reward will be obtained if the agent pushes the target to the goal position and the action also includes four cardinal directions.
* **Object comparison task**: the agent needs to figure out which object is different from other objects and move this object to the given position. The reward will be given if the goal is achieved and the action also includes four cardinal directions.
* **Property comparison task**: similarly, the agent needs to find out the object with a different color or shape from other objects. This task is generally more challenging as the agent needs to reason the property-level difference while the object comparison task only requires object-level reasoning. The reward and action are the same as the object comparison task.

Stacking.We follow the block-stacking experimental setups in [17]. In the block-stacking task, the agent can observe the images from the current scene and the goal. The action includes picking and placing the objects and coordinates [61]. The goal is to stack \(m\) objects with a given position.

#### d.2.2 Benchmark Modifications

Color, shape, friction and mass in Push & SwitchTo make the benchmark more realistic, we add two variables into the original states in the benchmark, the object color and shape, both are represented as one-hot vector. Additionally, we also use a different friction coefficient and mass for each object. We implement this by conducting a post-processing filter for each object in the simulator. Specifically, different friction coefficients will result in slower velocity and action effects. Different masses will result in different action effects of the agent. During the training of the estimation model, we use the objects with masses and friction coefficients uniformly sampled from \(\{4,6,8,10\}\) and \(\{0.4,0.6,0.8,1.0\}\) respectively. During testing, the object masses and friction coefficients are sampled from \(\{1,2,3,11,13\}\cup\{3,5,7,9\}\) and \(\{0.1,0.2,1.1,1.3\}\cup\{0.5,0.7,0.9\}\) respectively.

Unseen colors, shapes, and numbers of objects in SpriteworldTo evaluate the generalization capabilities of our model, we take into account the number of unseen objects, shapes, and colors. We achieve this without directly modifying the benchmark, as it offers the flexibility to alter these properties. During the training phase for model estimation, we use the colors green, red, and yellow, along with a varied number of objects ranging from \(\{3,5,7\}\). We consider as shapes circles and boxes. During the testing phase, we introduce new elements that were not seen during training. Specifically, we incorporate the color blue and triangle shapes. Additionally, the number of objects is extended to include \(\{2,4,6,8\}\). By considering these changes in colors, shapes, and object numbers, we aim to assess the model's ability to generalize to novel configurations and demonstrate its robustness.

Mass in StackingWe adjust the mass of each block by modifying the action effects on the object, treating them as post-processing filters applied to the image frames. In particular, picking up a heavier object will require more time compared to selecting a lighter one due to gravity. Placing a heavier one will make it faster than the lighter one. During training, we consider the masses ranging from \(\{1,1.5,2,2.5\}\). During testing, we make the masses of the objects ranging from \(\{0.5,1.25,2.25,3\}\)

#### d.2.3 Offline Model Estimation

In Step 1, we use the GRU model [39] as both the dynamics and reward prediction model to learn the class template graph. The hidden size of the GRU is set to \(128\). Additionally, we incorporate two MLP layers before and after the GRU, each with a hidden size of \(64\). During training, we continue optimizing the model until the dynamics and reward models converge, employing the mean squared error (MSE) loss with the Adam optimizer. In Step 2, we use MLP layers to parameterize the soft attention layers [35]. Specifically, for the functions \(f_{k}\), \(f_{q}\), and \(f_{v}\), the hidden sizes are all set to \(128\), with only one hidden layer for each of them. To learn the dynamic interaction graph, we follow the hyperparameters and model architectures outlined in [37]. However, as opposed to them, we use the previously learned GRU in Step 1 as the decoder. For various experiments, we adopt different hyperparameters for the GRU decoder. The training details for each experiment are provided below.

Push & SwitchIn Step 1, we gather a total of \(400\) trajectories for both boxes and switches (\(200\) for each). Each trajectory consists of \(100\) time steps, and the actions are generated using a random policy. The batch size for this stage is set to \(80\), and the weighting parameter before the regularization term is 0.015. In this step, in each trajectory, there is only one single object and different trajectories have different objects with different types (e.g., box or cubes), different colors (e.g., blue, red, etc), and different latent parameters (e.g., friction coefficients). In Step 2, we collect \(500\) trajectories, where each trajectory consists of \(50\) time steps, for learning the action binding selector. The batch size for training the soft attention network is set to \(100\). Regarding the learning of dynamic interaction graphs, we use the same parameters and model architectures as described in [37]. For the decoder, we reuse the decoder used in Step 1, which includes the interaction pattern graphs and the class template graphs. The balancing weights before the KL divergence and the sparsity regularization are set to \(0.9\) and \(0.2\), respectively. During training, we set the learning rate to \(2.5\times 10^{-5}\), and we apply a decaying factor of 0.5 every \(100\) training epochs.

SpriteworldWe adopt the same pre-trained procedures outlined in [13] for the object-centric models. Specifically, for both SA (Slot Attention) [30] and AIR (Attend, Infer, Repeat) [31], we generate a dataset containing various objects with distinct colors and shapes. The dataset comprises \(200\) trajectories, each consisting of \(50\) time steps. Within each frame, there are typically 2-5 objects exhibiting a diverse range of colors and shapes. For Step 1, we use the collected dataset of \(300\) trajectories (with \(50\) time steps each) to learn the class template graphs with different objects. Each trajectory has the transitions of one single object. The weighting parameter before the sparsity regularizer is set to \(0.3\). The batch size is \(64\) and the learning rate is \(0.002\). In our Spriteworld scenario, dense reward signals are absent. Therefore, we solely focus on learning the graphs between states and actions for most steps, excluding those that achieve the goal. Additionally, there is no direct interaction among objects, which means we do not train Step 2 for inferring the interactions. This setup aligns with [13] and is consistent across all the baseline methods we employ.

StackingWe also pre-train the SA and AIR using the randomly collected dataset containing \(200\) trajectories, each with \(30\) time steps. The blocks are initialized with different colors in the dataset. To train Step 1, we collect the single-block dataset with \(200\) trajectories, each with \(30\) time steps. The balancing weight for sparsity regularization is \(0.05\). For Step 2, we have the dataset with \(300\) trajectories, each with \(30\) time steps and multiple objects. We use the same set of hyper-parameters for learning the soft attention networks as Push & Switch. For learning the dynamic interaction graph, we balanced weights before the KL divergence and the sparsity regularization is \(0.2\) and \(0.1\), respectively. We use the same learning rate to \(2.5\times 10^{-5}\), and we apply a decaying factor of 0.5 every \(100\) training epochs.

#### d.2.4 Policy Learning and Planning

For the Push & Switch and Spriteworld experiments, we use PPO [42] for all baselines with the trajectories generated by the learned dynamics and reward functions. The learning rate for PPO is \(0.005\) and \(0.003\) in these two benchmarks, respectively. The coefficient of the entropy term in PPO is \(0.05\). The policy is parameterized with \(3\) MLP layers of size \(256\) and \(128\) in both experiments. For Stacking, we follow all the MPC hyper-parameter settings in [17] for the planning.

[MISSING_PAGE_EMPTY:24]

#### d.3.2 Full Ablation Studies

Fig. A3 gives the full ablation studies on Push & Switch and Stacking benchmarks. We consider the following cases:

* DAFT-RL w/o latent parameters;
* DAFT-RL w/o factored class template graph;
* DAFT-RL w/o dynamic interaction graph;
* DAFT-RL w/o factored interaction pattern;
* Using single class learning instead of multiple object classes;
* DAFT-RL w/o action binding graph;
* Using hard attention networks for action binding;

Based on the obtained results, we observe that each learned component plays a crucial role in optimizing the policy. Notably, the dynamic interaction graph consistently demonstrates a significant contribution across the majority of experiments in Push & Switch and stacking benchmarks.

[MISSING_PAGE_FAIL:26]

[MISSING_PAGE_FAIL:27]

\(C_{1}.s_{1}\), \(C_{1}.s_{2}\), \(C_{1}.s_{3}\), and \(C_{1}.s_{4}\) represent the color, shape, velocity, and position of the boxes, respectively. Similarly, \(C_{2}.s_{1}\), \(C_{2}.s_{2}\), \(C_{2}.s_{3}\), and \(C_{2}.s_{4}\) correspond to the color, shape, position, and angle of the switches. In both cases, \(C_{i}.\theta\) and \(C_{j}.\theta\) indicate the friction coefficients.

As expected, in the learned class template graph, we observe that shape and color do not have a direct effect on the dynamics, and they remain unchanged throughout the dynamics. On the other hand, velocity directly influences the position of the boxes, while the position itself does not affect the velocity. Regarding the switches, their positions do not directly impact the angles since they are fixed at specific positions on the panel. Also, as expected, actions applied to the switches have an effect solely on their angles. The reward is based on the actions, positions, and angles for both boxes and switches. Additionally, the latent parameters influence the velocity and position of the boxes, while they specifically affect the angle of the switches.

As for the interaction graph between two boxes, shown in Figure A5B, the velocity of one box can impact the position of the other box, while the other variables do not have a direct influence on each other. This learned interaction graph successfully captures the underlying physical processes of the system, enabling an attribute-level understanding of the object interactions. Overall, these learned graphs effectively recover the underlying physical processes of the system.