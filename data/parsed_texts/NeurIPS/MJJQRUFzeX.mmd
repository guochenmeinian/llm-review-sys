# Unified Convergence Theory of Stochastic and Variance-Reduced Cubic Newton Methods

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We study stochastic Cubic Newton methods for solving general possibly non-convex minimization problems. We propose a new framework, which we call the _helper framework_, that provides a unified view of the stochastic and variance-reduced second-order algorithms equipped with global complexity guarantees. It can also be applied to learning with auxiliary information. Our helper framework offers the algorithm designer high flexibility for constructing and analysis of the stochastic Cubic Newton methods, allowing arbitrary size batches, and the use of noisy and possibly biased estimates of the gradients and Hessians, incorporating both the variance reduction and the lazy Hessian updates. We recover the best-known complexities for the stochastic and variance-reduced Cubic Newton, under weak assumptions on the noise and avoiding artificial logarithms. A direct consequence of our theory is the new lazy stochastic second-order method, which significantly improves the arithmetic complexity for large dimension problems. We also establish complexity bounds for the classes of gradient-dominated objectives, that include convex and strongly convex problems. For Auxiliary Learning, we show that using a helper (auxiliary function) can outperform training alone if a given similarity measure is small.

## 1 Introduction

In many fields of machine learning, it is common to optimize a function \(f(\bm{x})\) that can be expressed as a finite sum:

\[\min_{\bm{x}\in\mathbb{R}^{d}}\Big{\{}\ f(\bm{x}) = \frac{1}{n}\sum\limits_{i=1}^{n}f_{i}(\bm{x})\ \Big{\}},\] (1)

or, more generally, as an expectation over some given probability distribution: \(f(\bm{x})=\mathbb{E}_{\zeta}\big{[}f(\bm{x},\zeta)\big{]}\). When \(f\) is non-convex, this problem is especially difficult, since finding a global minimum is NP-hard in general [14]. Hence, the reasonable goal is to look for approximate solutions. The most prominent family of algorithms for solving large-scale problems of the form (1) are the _first-order methods_, such as the Stochastic Gradient Descent (SGD) [25; 16]. They employ only stochastic gradient information about the objective \(f(\bm{x})\) and guarantee the convergence to a stationary point, which is a point with a small gradient norm.

Nevertheless, when the objective function is non-convex, a stationary point may be a saddle point or even a local maximum, which is not desirable. Another common issue is that first-order methods typically have a slow convergence rate, particularly when the problem is _ill-conditioned_. Therefore, they may not be suitable when high precision for the solution is required.

To address these challenges, we can take into account _second-order information_ (the Hessian matrix) and apply Newton's method (see, e.g. [19]). Among the many versions of this algorithm, the Cubic Newton method [20] is one of the most theoretically established. With the Cubic Newton method, wecan guarantee _global convergence_ to an approximate _second-order_ stationary point (in contrast, the pure Newton method without regularization can even diverge when it starts far from a neighborhood of the solution). For a comprehensive historical overview of the different variants of Newton's method, see [24]. Additionally, the rate of convergence of the Cubic Newton is _provably better_ than those for the first-order methods.

Therefore, theoretical guarantees of the Cubic Newton method seem to be very appealing for practical applications. However, the basic version of the Cubic Newton requires the exact gradient and Hessian information in each step, which can be very expensive to compute in the large scale setting. To overcome this issue, several techniques have been proposed:

* One popular approach is to use inexact _stochastic gradient and Hessian estimates_ with subsampling [33; 17; 32; 21; 12; 6; 1]. This technique avoids using the full oracle information, but typically it has a slower convergence rate compared to the exact Cubic Newton.
* _Variance reduction_ techniques [35; 29] combine the advantages of stochastic and exact methods, achieving an improved rates by recomputing the full gradient and Hessian information at some iterations.
* _Lazy Hessian_ updates [26; 9] utilize a simple idea of reusing an old Hessian for several iterations of a second-order scheme. Indeed, since the cost of computing one Hessian is usually much more expensive than one gradient, it can improve the arithmetic complexity of our methods.
* In addition, exploiting the special structure of the function \(f\) (if known) can also be helpful. For instance, some studies [20; 18] consider _gradient-dominated objectives_, a subclass of non-convex functions that have improved convergence rates and can even be shown to converge to the global minimum. Examples of such objectives include convex and star-convex functions, uniformly convex functions, and functions satisfying the PL condition [23] as a special case.

In this work, we revise the current state-of-the-art convergence theory for the stochastic Cubic Newton method and propose a unified and improved complexity guarantees for different versions of the method, which combine all the advanced techniques listed above.

Our developments are based on the new _helper framework_ for the second-order optimization, that we present in Section 3. For the first-order optimization, a similar in-spirit techniques called _learning with auxiliary information_ was developed recently in [7; 30]. Thus, our results can also be seen as a generalization of the Auxiliary Learning paradigm to the second-order optimization. However, note that in our second-order case, we have more freedom for choosing the "helper functions" (namely, we use one for the gradients and one for the Hessians). That brings more flexibility into our methods and it allows, for example, to use the lazy Hessian updates.

Our new helper framework provides us with a unified view of the stochastic and variance-reduced methods and can be used by an algorithm designed to construct new methods. Thus, we show how to recover already known versions of the stochastic Cubic Newton with the best convergence rates, as well as present the new _Lazy Stochastic Second-Order Method_, which significantly improves the total arithmetic complexity for large-dimension problems.

**Contributions.**

* We introduce the _helper framework_ which we argue encompasses multiple methods in a unified way. Such methods include stochastic methods, variance reduction, Lazy methods, core sets, and semi-supervised learning.
* This framework covers previous versions of the variance-reduced stochastic Cubic Newton methods with known rates. Moreover, it provides us with new algorithms that employ _Lazy Hessian_ updates and significantly improves the arithmetic complexity (for high dimensions), by using the same Hessian snapshot for several steps of the method.
* In the case of Auxiliary learning we provably show a benefit from using auxiliary tasks as helpers in our framework. In particular, we can replace the smoothness constant by a similarity constant which might be smaller.
* Moreover, our analysis works both for the general class of non-convex functions, as well as for the class of gradient-dominated problems, that includes convex and uniformly convex functions. Hence, in particular, we are the first to establish the convergence rates of the stochastic Cubic Newton algorithms with variance reduction for the gradient-dominated case.

## 2 Notation and Assumptions

For simplicity, we consider the finite-sum optimization problem (1), while it can be also possible to generalize our results to arbitrary expectations. We assume that our objective \(f\) is bounded from below and denote \(f^{\star}\,:=\inf_{\bm{x}}f(\bm{x})\), and use the following notation: \(F_{0}:=f(\bm{x}_{0})-f^{\star},\) for some initial \(\bm{x}_{0}\in\mathbb{R}^{d}\). We denote by \(\|\bm{x}\|:=\langle\bm{x},\bm{x}\rangle^{1/2}\), \(\bm{x}\in\mathbb{R}^{d}\), the standard Euclidean norm for vectors, and the spectral norm for symmetric matrices, \(\|\bm{H}\|:=\max\{\lambda_{\max}(\bm{H}),-\lambda_{\min}(\bm{H})\}\), where \(\bm{H}=\bm{H}^{\top}\in\mathbb{R}^{d\times d}\). We will also use \(x\wedge y\) to denote \(\min(x,y)\).

Throughout this work, we make the following smothness assumption on the objective \(f\) :

**Assumption 1** (Lipschitz Hessian): _The Hessian of \(f\) is Lipschitz continuous, for some \(L>0\):_

\[\|\nabla^{2}f(\bm{x})-\nabla^{2}f(\bm{y})\| \leq L\|\bm{x}-\bm{y}\|,\qquad\forall\bm{x},\bm{y}\in\mathbb{R}^{d}\]

Our goal is to explore the potential of using the Cubically regularized Newton methods to solve problem (1). At each iteration, being at a point \(\bm{x}\in\mathbb{R}^{d}\), we compute the next point \(\bm{x}^{+}\) by solving the subproblem of the form

\[\bm{x}^{+}\ \in\ \operatorname*{arg\,min}_{\bm{y}\in\mathbb{R}^{d}}\Bigl{\{} \Omega_{M,\bm{g},\bm{H}}(\bm{y},\bm{x}):=\langle\bm{g},\bm{y}-\bm{x}\rangle+ \tfrac{1}{2}\langle\bm{H}(\bm{y}-\bm{x}),\bm{y}-\bm{x}\rangle+\tfrac{M}{6}\| \bm{y}-\bm{x}\|^{3}\,\Bigr{\}}.\] (2)

Here, \(\bm{g}\) and \(\bm{H}\) are estimates of the gradient \(\nabla f(\bm{x})\) and the Hessian \(\nabla^{2}f(\bm{x})\), respectively. Note that solving (2) can be done efficiently even for non-convex problems (see [8, 20, 5]). Generally, the cost of computing \(\bm{x}^{+}\) is \(\mathcal{O}(d^{3})\) arithmetic operations, which are needed for evaluating an appropriate factorization of \(\bm{H}\). Hence, it is of a similar order as the cost of the classical Newton's step.

We will be interested to find a second-order stationary point to (1). We call \((\varepsilon,c)\)_-approximate second-order local minimum_ a point \(\bm{x}\) that satisfies:

\[\|\nabla f(\bm{x})\| \leq \varepsilon\qquad\text{and}\qquad\lambda_{min}(\nabla^{2}f(\bm{x })) \geq -c\sqrt{\varepsilon},\]

where \(\varepsilon,c>0\) are given tolerance parameters. Let us define the following accuracy measure (see [20]):

\[\mu_{c}(\bm{x}) := \max\Bigl{(}\|\nabla f(\bm{x})\|^{3/2},\,\tfrac{-\lambda_{min}( \nabla^{2}f(\bm{x}))^{3}}{c^{3/2}}\Bigr{)},\qquad\bm{x}\in\mathbb{R}^{d},\ c>0.\]

Note that this definition implies that if \(\mu_{c}(\bm{x})\leq\varepsilon^{3/2}\) then \(\bm{x}\) is an \((\varepsilon,c)\)-approximate local minimum.

Computing gradients and Hessians.It is clear that computing the Hessian matrix can be much more expensive than computing the gradient vector. We denote the corresponding arithmetic complexities by _HessCost_ and _GradCost_. We will make and follow the convention that _HessCost_\(=d\times\)_GradCost_, where \(d\) is the dimension of the problem. For example, this is known to hold for neural networks using the backpropagation algorithm [15]. However, if the Hessian has a sparse structure, the cost of computing the Hessian can be cheaper [22]. Then, we can replace \(d\) with the _effective dimension_\(d_{\text{eff}}:=\frac{\text{HessCost}}{\text{GradCost}}\ \leq\ d.\)

## 3 Second-Order Optimization with Helper Functions

In this section, we extend the helper framework previously introduced in [7] for first-order optimization methods to second-order optimization.

General principle.The general idea is the following: imagine that, besides the objective function \(f\) we have access to a help function \(h\) that we think is similar in some sense (that will be defined later) to \(f\) and thus it should help to minimize it.

Note that many optimization algorithms can be framed in the following sequential way. For a current state \(\bm{x}\), we compute the next state \(\bm{x}^{+}\) as:\[\bm{x}^{+} \in \underset{\bm{y}\in\mathbb{R}^{d}}{\arg\min}\Big{\{}\,\hat{f}_{\bm{ x}}(\bm{y})+Mr_{\bm{x}}(\bm{y})\,\Big{\}},\]

where \(\hat{f}_{\bm{x}}(\cdot)\) is an approximation of \(f\) around current point \(\bm{x}\), and \(r_{\bm{x}}(\bm{y})\) is a regularizer that encodes how accurate the approximation is, and \(M>0\) is a regularization parameter. In this work, we are interested in cubically regularized second-order models of the form (2) and we use \(r_{\bm{x}}(\bm{y}):=\frac{1}{6}\|\bm{y}-\bm{x}\|^{3}\).

Now let us look at how we can use a helper \(h\) to construct the approximation \(\hat{f}\). We notice that we can write

\[f(\bm{y}) := \underbrace{h(\bm{y})}_{\text{cheap}}\,+\,\underbrace{f(\bm{y}) -h(\bm{y})}_{\text{expensive}}\]

We discuss the actual practical choices of the helper function \(h\) below. We assume now that we can afford the second-order approximation for the cheap part \(h\) around the current point \(\bm{x}\). However, approximating the part \(f-h\) can be expensive (as for example when the number of elements \(n\) in finite sum (1) is huge), or even impossible (due to lack of data). Thus, we would prefer to approximate the expensive part less frequently. For this reason, let us introduce an extra _snapshot point_\(\hat{\bm{x}}\) that is updated less often than \(\bm{x}\). Then, we use it to approximate \(f-h\). Another question that we still need to ask is _what order should we use for the approximation of \(f-h\)?_ We will see that order \(0\) (approximating by a constant) leads as to the basic stochastic methods, while for orders \(1\) and \(2\) we equip our methods with the variance reduction.

Combining the two approximations for \(h\) and \(f-h\) we get the following model of our objective \(f\):

\[\hat{f}_{\bm{x},\hat{\bm{x}}}(\bm{y}) = C(\bm{x},\tilde{\bm{x}})+\langle\mathcal{G}(h,\bm{x},\tilde{\bm {x}}),\bm{y}-\bm{x}\rangle+\tfrac{1}{2}\langle\mathcal{H}(h,\bm{x},\tilde{\bm {x}})(\bm{y}-\bm{x}),\bm{y}-\bm{x}\rangle,\] (3)

where \(C(\bm{x},\tilde{\bm{x}})\) is a constant, \(\mathcal{G}(h,\bm{x},\tilde{\bm{x}})\) is a linear term, and \(\mathcal{H}(h,\bm{x},\tilde{\bm{x}})\) is a matrix. Note that if \(\tilde{\bm{x}}\equiv\bm{x}\), then the best second-order model of the form (3) is the Taylor polynomial of degree two for \(f\) around \(\bm{x}\), and that would give us the exact Newton-type method. However, when the points \(\bm{x}\) and \(\tilde{\bm{x}}\) are different, we obtain much more freedom in constructing our models.

For using this model in our cubically regularized method (2), we only need to define the gradient \(\bm{g}=\mathcal{G}(h,\bm{x},\tilde{\bm{x}})\) and the Hessian estimates \(\bm{H}=\mathcal{H}(h,\bm{x},\tilde{\bm{x}})\), and we can also treat them differently (using two different helpers \(h_{1}\) and \(h_{2}\), correspondingly). Thus we come to the following general second-order (meta)algorithm. We perform \(S\) rounds, the length of each round is \(m\geq 1\), which is our key parameter:

```
0:\(\bm{x}_{0}\in\mathbb{R}^{d}\), \(S\), \(m\geq 1\), \(M>0\).
1:for\(t=0,\ldots,Sm-1\)do
2:if\(t\operatorname{mod}m=0\)then
3: Update \(\tilde{\bm{x}}_{t}\) (using previous states \(\bm{x}_{i\leq t}\))
4:else
5:\(\tilde{\bm{x}}_{t}=\tilde{\bm{x}}_{t-1}\)
6: Form helper functions \(h_{1},h_{2}\)
7: Compute the gradient \(\bm{g}_{t}=\mathcal{G}(h_{1},\bm{x}_{t},\tilde{\bm{x}}_{t})\), and the Hessian \(\bm{H}_{t}=\mathcal{H}(h_{2},\bm{x}_{t},\tilde{\bm{x}}_{t})\)
8: Compute the cubic step \(\bm{x}_{t+1}\in\arg\min_{\bm{y}\in\mathbb{R}^{d}}\,\Omega_{M,\bm{g}_{t},\bm{H} _{t}}(\bm{y},\bm{x}_{t})\)return\(\bm{x}_{out}\) using the history \((\bm{x}_{i})_{0\leq i\leq Sm}\) ```

**Algorithm 1** Cubic Newton with helper functions

In Algorithm 1 we update the snapshot \(\tilde{\bm{x}}\) regularly every \(m\) iterations. The two possible options are

\[\tilde{\bm{x}}_{t} = \bm{x}_{t\operatorname{mod}m}\qquad\qquad\qquad\text{(use the last iterate)}\] (4)

or

\[\tilde{\bm{x}}_{t} = \underset{i\in\{t-m+1,\ldots,t\}}{\arg\min}\,f(\bm{x}_{i})\qquad \text{(use the best iterate)}\] (5)

Clearly, option (5) is available only in case we can efficiently estimate the function values. However, we will see that it serves us with better global convergence guarantees, for the gradient-dominated functions.

It remains only to specify how we choose the helpers \(h_{1}\) and \(h_{2}\). We need to assume that they are somehow similar to \(f\). Let us present several efficient choices that lead to implementable second-order schemes.

### Basic Stochastic Methods

If the objective function \(f\) is very "expensive" (for example of the form (1) with \(n\to\infty\)), one option is to ignore the part \(f-h\) i.e. to approximate it by a zeroth-order approximation: \(f(\bm{y})-h(\bm{y})\approx f(\tilde{\bm{x}})-h(\tilde{\bm{x}})\). Since it is just a constant, we do not need to update \(\tilde{\bm{x}}\). In this case, we have:

\[\mathcal{G}(h_{1},\bm{x},\tilde{\bm{x}}) := \nabla h_{1}(\bm{x}),\qquad\mathcal{H}(h_{2},\bm{x},\tilde{\bm{x} }) := \nabla^{2}h_{2}(\bm{x})\,.\]

To treat this choice of the helpers and motivated by the form of the errors in Lemma 5, we assume the following similarity assumptions:

**Assumption 2** (Bounded similarity): _Let for some \(\delta_{1},\delta_{2}\geq 0\), it holds_

\[\mathbb{E}_{h_{1}}[\|\mathcal{G}(h_{1},\bm{x},\tilde{\bm{x}})-\nabla f(\bm{x} )\|^{3/2}]\leq\delta_{1}^{3/2},\ \mathbb{E}_{h_{2}}[\|\mathcal{H}(h_{2},\bm{x},\tilde{\bm{x}})-\nabla^{2}f(\bm {x})\|^{3}]\leq\delta_{2}^{3},\quad\forall\bm{x},\tilde{\bm{x}}\in\mathbb{R} ^{d}.\]

Under this assumption, we prove the following theorem:

**Theorem 1**: _Under Assumptions 1 and 2, and \(M\geq L\), for an output of Algorithm 1\(\bm{x}_{out}\) chosen uniformly at random from \((\bm{x}_{i})_{0\leq i\leq Sm}\), we have:_

\[\mathbb{E}[\mu_{M}(\bm{x}_{out})] = \mathcal{O}\Big{(}\tfrac{\sqrt{M}F_{0}}{Sm}+\tfrac{\delta_{3}^{ 3}}{M^{3/2}}+\delta_{1}^{3/2}\Big{)}.\]

We see that according to this result, we can get \(\mathbb{E}[\mu_{M}(\bm{x}_{out})]\leq\varepsilon^{3/2}\) only for \(\varepsilon>\delta_{1}\). In other words, we can converge only to a certain _neighbourhood around a stationary point_, that is determined by the error \(\delta_{1}\) of the stochastic gradients.

However, as we will show next, this seemingly pessimistic dependence leads to the same rate of classical subsampled Cubic Newton methods discovered in [17, 32, 33].

Let us discuss now the specific case of stochastic optimization, where \(f\) has the specific form (1), with \(n\) potentially being very large. In this case, it is customary to sample batches at random and assume the noise to be bounded in expectation. Precisely speaking, if we assume the standard assumption that for one index sampled uniformly at random, we have \(\mathbb{E}_{i}\|\nabla f(\bm{x})-\nabla f_{i}(\bm{x})\|^{2}\leq\sigma_{g}^{2}\) and \(\mathbb{E}_{i}\|\nabla^{2}f(\bm{x})-\nabla^{2}f_{i}(\bm{x})\|^{3}\leq\sigma_{ h}^{3},\) then it is possible to show that for

\[h_{1} = \tfrac{1}{b_{g}}\sum_{i\in\mathcal{B}_{g}}f_{i}\qquad\text{and} \qquad h_{2}\ =\ \tfrac{1}{b_{h}}\sum_{i\in\mathcal{B}_{h}}f_{i},\] (6)

for batches \(\mathcal{B}_{g},\mathcal{B}_{h}\subseteq[n]\) sampled uniformly at random and of sizes \(b_{g}\) and \(b_{h}\) respectively, Assumption 2 is satisfied with [27]: \(\delta_{1}=\tfrac{\sigma_{g}}{\sqrt{b_{g}}}\) and \(\delta_{2}=\tilde{\mathcal{O}}(\tfrac{\sigma_{b}}{\sqrt{b_{h}}})\). Note that we can use the same random subsets of indices \(\mathcal{B}_{g},\mathcal{B}_{h}\) for all iterations.

**Corollary 1**: _In Algorithm 1, let us choose \(M=L\) and \(m=1\), with basic helpers (6). Then, according to Theorem 1, for any \(\varepsilon>0\), to reach an \((\varepsilon,L)\)-approximate second-order local minimum, we need at most \(S=\tfrac{\sqrt{L}F_{0}}{\varepsilon^{3/2}}\) iterations with \(b_{g}=\big{(}\tfrac{\sigma_{g}}{\varepsilon}\big{)}^{2}\) and \(b_{h}=\tfrac{\sigma_{h}^{2}}{\varepsilon}\). Therefore, the total arithmetic complexity of the method becomes_

\[\mathcal{O}\Big{(}\tfrac{\sigma_{g}^{2}}{\varepsilon^{7/2}}+\tfrac{\sigma_{h} ^{2}}{\varepsilon^{5/2}}d_{\text{eff}}\Big{)}\times\text{GradCost}.\]

It improves upon the complexity \(\mathcal{O}(\tfrac{1}{\varepsilon^{4}})\times\text{GradCost}\) of the first-order SGD for non-convex optimization [11], unless \(d_{\text{eff}}>\tfrac{1}{\varepsilon^{3/2}}\) (high cost of computing the Hessians).

### Let the Objective Guide Us

If the objective \(f\) is such that we can afford to access its gradients and Hessians from time to time (functions of the form (1) with \(n<\infty\) and "reasonable"), then we can do better than the previous chapter. In this case, we can afford to use a better approximation of the term \(f(\bm{y})-h(\bm{y})\). From a theoretical point of view, we can treat the case when \(f\) is only differentiable once, and thus we can only use a first-order approximation of \(f-h\), in this case, we will only be using the hessian of the helper \(h\) but only gradients of \(f\). However, in our case, if we assume we have access to gradients thenwe can also have access to the Hessians of \(f\) as well (from time to time). For this reason, we consider a second-order approximation of the term \(f-h\), if we follow the procedure that we described above we find:

\[\mathcal{G}(h_{1},\bm{x},\tilde{\bm{x}}) := \nabla h_{1}(\bm{x})-\nabla h_{1}(\tilde{\bm{x}})+\nabla f(\tilde{ \bm{x}})+(\nabla^{2}f(\tilde{\bm{x}})-\nabla^{2}h_{1}(\tilde{\bm{x}}))(\bm{x}- \tilde{\bm{x}})\] (7) \[\mathcal{H}(h_{2},\bm{x},\tilde{\bm{x}}) := \nabla^{2}h_{2}(\bm{x})-\nabla^{2}h_{2}(\tilde{\bm{x}})+\nabla^{2 }f(\tilde{\bm{x}})\] (8)

We see that there is an explicit dependence on the snapshot \(\tilde{\bm{x}}\) and thus we need to address the question of how this snapshot point should be updated in Algorithm 1. In general, we can update it with a certain probability \(p\sim\frac{1}{m}\), and we can use more advanced combinations of past iterates (like the average). However, for our purposes, we simply choose option 4 (i.e. the last iterate), thus it is only updated once every \(m\) iterations.

We also need to address the question of the measure of similarity in this case. Since we are using a second-order approximation of \(f-h\), it is very logical to compare them using the difference between their third derivatives or equivalently, the Hessian Lipschitz constant of their difference. Precisely we make the following similarity assumption :

**Assumption 3** (Lipschitz similarity): _Let for some \(\delta_{1},\delta_{2}\geq 0\), it holds, \(\forall\bm{x},\tilde{\bm{x}}\in\mathbb{R}^{d}\):_

\[\mathbb{E}_{h_{1}}[\|\mathcal{G}(h_{1},\bm{x},\tilde{\bm{x}})- \nabla f(\bm{x})\|^{3/2}] \leq \delta_{1}^{3/2}\|\bm{x}-\tilde{\bm{x}}\|^{3},\] \[\mathbb{E}_{h_{2}}[\|\mathcal{H}(h_{2},\bm{x},\tilde{\bm{x}})- \nabla^{2}f(\bm{x})\|^{3}] \leq \delta_{2}^{3}\|\bm{x}-\tilde{\bm{x}}\|^{3}.\]

In particular, if \(f-h_{1}\) and \(f-h_{2}\) have \(\delta_{1}\) and \(\delta_{2}\) Lipschitz Hessians respectively then \(h_{1}\) and \(h_{2}\) satisfy Assumption 3.

Under this assumption, we show that the errors resulting from the use of the snapshot can be successfully balanced by choosing \(M\) satisfying:

\[4\big{(}\tfrac{\delta_{1}}{M}\big{)}^{3/2}+73\big{(}\tfrac{\delta_{2}}{M} \big{)}^{3} \leq \tfrac{1}{24m^{3}}.\] (9)

And we have the following theorem.

**Theorem 2**: _For \(f,h_{1},h_{2}\) verifying Assumptions 1,3. For a regularization parameter \(M\) chosen such that \(M\geq L\) and (9) is satisfied. For an output of Algorithm 1\(\bm{x}_{out}\) chosen uniformly at random from \((\bm{x}_{i})_{0\leq i\leq Sm:=T}\), we have:_

\[\mathbb{E}[\mu_{M}(\bm{x}_{out})] = \mathcal{O}\Big{(}\tfrac{\sqrt{M}F_{0}}{Sm}\Big{)},\]

In particular, we can choose \(M=\max(L,32\delta_{1}m^{2},16\delta_{2}m)\) which gives

\[\mathbb{E}[\mu_{M}(\bm{x}_{out})] = \mathcal{O}\Big{(}\tfrac{\sqrt{L}F_{0}}{Sm}+\tfrac{\sqrt{ \delta_{2}}F_{0}}{S\sqrt{m}}+\tfrac{\sqrt{\delta_{1}}F_{0}}{S}\Big{)}.\] (10)

Based on the choices of the helpers \(h_{1}\) and \(h_{2}\) we can have many algorithms. We discuss these in the following sections. We start by discussing variance reduction and Lazy Hessians which rely on sampling batches randomly, then move to core-sets which try to find, more intelligently, representative weighted batches of data, after this, we discuss semi-supervised learning and how unlabeled data can be used to engineer the helpers. More generally, auxiliary learning tries to leverage auxiliary tasks in training a given main task, the auxiliary tasks can be treated as helpers.

### Variance Reduction and Lazy Hessians

The following lemma demonstrates that we can create helper functions \(h\) with lower similarity to the main function \(f\) of the form (1) by employing sampling and averaging.

**Lemma 1**: _Let \(f=\frac{1}{n}\sum_{i=1}^{n}f_{i}\) such that all \(f_{i}\) are twice differentiable and have \(L\)-Lipschitz Hessians. Let \(\mathcal{B}\subset\{1,\cdots,n\}\) be of size \(b\) and sampled with replacement uniformly at random, and define \(h_{\mathcal{B}}=\frac{1}{b}\sum_{i\in\mathcal{B}}f_{i}\), then \(h_{\mathcal{B}}\) satisfies Assumption 3 with \(\delta_{1}=\frac{L}{\sqrt{b}}\) and \(\delta_{2}=\mathcal{O}(\frac{\sqrt{\log(d)}L}{\sqrt{b}})\)._

Choice of the parameter \(m\) in Algorithm 1.Minimizing the total arithmetic cost, we choose \(m=\arg\min_{m}\#Grad(m,\varepsilon)+d\#Hess(m,\varepsilon)\), where \(\#Grad(m,\varepsilon)\) and \(\#Hess(m,\varepsilon)\) denote the number of gradients and Hessians required to find an \(\varepsilon\) stationary point.

Now we are ready to discuss several special cases that are direct consequences from Theorem 2.

First, note that choosing \(h_{1}=h_{2}=f\) gives the classical Cubic Newton method [20], whereas choosing \(h_{1}=f\) and \(h_{2}=0\), gives the Lazy Cubic Newton [9]. In both cases, we recuperate the known rates of convergence.

General variance reduction.If we sample batches \(\mathcal{B}_{g}\) and \(\mathcal{B}_{h}\) of sizes \(b_{g}\) and \(b_{h}\) consecutively at random and choose

\[\boxed{h_{1}\ \ =\ \ \frac{1}{b_{g}}\sum_{i\in\mathcal{B}_{g}}f_{i}\qquad\text{ and}\qquad h_{2}\ =\ \frac{1}{b_{h}}\sum_{i\in\mathcal{B}_{h}}f_{i},\ }\]

and use these helpers along with the estimates (7), (8), we obtain the _Variance Reduced Cubic Newton_ algorithm [35, 29]. According to Lemma 1, this choice corresponds to \(\delta_{1}=\frac{L}{\sqrt{b_{g}}}\) and \(\delta_{2}=\tilde{\mathcal{O}}(\frac{L}{\sqrt{b_{h}}})\). For \(b_{g}\sim m^{4}\wedge n,b_{h}\sim m^{2}\wedge n\) and \(M=L\), we have the non-convex convergence rate \(\mathcal{O}\big{(}\frac{\sqrt{L}F_{0}}{Sm}\big{)}\), which is the same as that of the cubic Newton algorithm but with a smaller cost per iteration. Minimizing the total arithmetic cost, we can choose \(m=\arg\min_{m}\frac{dn+d(m^{3}\wedge nm)+(m^{5}\wedge nm)}{m}\).

Let us denote by \(g^{VR}(n,d)\) the corresponding optimal value. Then we reach an \((\varepsilon,L)\)-approximate second-order local minimum in at most \(\mathcal{O}(\frac{g^{VR}(n,d)}{\varepsilon^{b/2}})\times\)_GradCost_ arithmetic operations.

Variance reduction with Lazy Hessians.We can also use lazy updates for Hessians combined with variance-reduced gradients. This corresponds to choosing

\[\boxed{h_{1}\ \ =\ \ \frac{1}{b_{g}}\sum_{i\in\mathcal{B}_{g}}f_{i}\qquad \text{and}\qquad h_{2}\ =\ 0,\ }\]

which implies (according to Lemma 1) that \(\delta_{1}=\frac{L}{\sqrt{b_{g}}}\) and \(\delta_{2}=L\). In this case, we need \(b_{g}\sim m^{2}\) to obtain a convergence rate of \(\mathcal{O}\big{(}\frac{\sqrt{L}F_{0}}{S\sqrt{m}}\big{)}\), which matches the convergence rate of the Lazy Cubic Newton method while using stochastic gradients. We choose this time \(m=\arg\min_{m}\frac{nd+(m^{3}\wedge mn)}{\sqrt{m}}\), as before. Let us denote \(g^{Lazy}(n,d)\) the corresponding minimum. Then we guarantee to reach an \((\varepsilon,mL)\)-approximate second-order local minimum in at most \(\mathcal{O}(\frac{g^{Lazy}(n,d)}{\varepsilon^{b/2}})\times\)_GradCost_ operations.

To be lazy or not to be?We show that \(g^{Lazy}(n,d)\sim(nd)^{5/6}\wedge n\sqrt{d}\) and \(g^{VR}(n,d)\sim(nd)^{4/5}\wedge(n^{2/3}d+n)\). In particular, for \(d\geq n^{2/3}\) we have \(g^{Lazy}(n,d)\leq g^{VR}(n,d)\) and thus for \(d\geq n^{2/3}\)_it is better to use Lazy Hessians_ than variance-reduced Hessians from a gradient equivalent cost perspective. We note also that for the Lazy approach, we can keep a factorization of the Hessian (this factorization induces most of the cost of solving the cubic subproblem) and thus it is as if we only need to solve the subproblem once every \(m\) iterations, so the Lazy approach has a big advantage compared to the general approach, and the advantage becomes even bigger for the case of large dimensions.

Note that according to the theory, we could use the same random batches \(\mathcal{B}_{g},\mathcal{B}_{h}\subseteq[n]\) generated once for all iterations. However, using the resampled batches can lead to a more stable convergence.

### Other Applications

The result in (10) is general enough that it can include many other applications that are only limited by our imagination. To cite a few such applications there are:

**Core sets.**[3] The idea of core sets is simple: can we summarize a potentially big data set using only a few (weighted) important examples? Many reasons such as redundancy make the answer yes. Devising approaches to find such core sets is outside of the scope of this work, but in general, we can see from (10) that if we have batches \(\mathcal{B}_{g},\mathcal{B}_{h}\) such that they are \((\delta_{1},1)\) and \((\delta_{2},2)\) similar to \(f\) respectively, then we can keep reusing the same batch \(\mathcal{B}_{g}\) for at least \(\sqrt{\frac{L}{\delta_{1}}}\) times, and \(\mathcal{B}_{h}\) for \(\frac{L}{\delta_{2}}\) all the while guaranteeing an improved rate. So then if we can design such small batches with small \(\delta_{1}\) and \(\delta_{2}\) then we can keep reusing them, and Joy the improved rate without needing large batches.

**Auxiliary learning.**[4; 2; 31] study how a given task \(f\) can be trained in the presence of auxiliary (related) tasks. Our approach can be indeed used for auxiliary learning by treating the auxiliaries as helpers. If we compare (10) to the rate that we obtained without the use of the helpers: \(\mathcal{O}(\frac{\sqrt{L}F_{0}}{S})\), we see that we have a better rate using the helpers/auxiliary tasks when \(\frac{1}{m}+\frac{\sqrt{\delta_{2}}}{\sqrt{mL}}+\frac{\sqrt{\delta_{1}}}{ \sqrt{L}}\leq 1\).

**Semi-supervised learning.**[34] Semi-supervised learning is a machine learning approach that combines the use of both labeled data and unlabeled data during training. In general, we can use the unlabeled data to construct the helpers, we can start for example by using random labels for the helpers and improving the labels with training. There are at least two special cases where our theory implies improvement by only assigning random labels to the unlabeled data. In fact, for both regularized least squares and logistic regression, we notice that the Hessian is independent of the labels (only depends on inputs) and thus if the unlabeled data comes from the same distribution as the labeled data, then we can use it to construct helpers which, at least theoretically, have \(\delta_{1}=\delta_{2}=0\). Because the Hessian is independent of the labels, we can technically endow the unlabeled data with random labels. Theorem 2 would imply in this case \(\mathbb{E}[\mu_{L}(\boldsymbol{x}_{out})]=\mathcal{O}(\frac{\sqrt{L}F_{0}}{ Sm})\), where \(S\) is the number of times we use labeled data and \(S(m-1)\) is the number of unlabeled data.

## 4 Gradient-Dominated Functions

We consider now the class of gradient-dominated functions defined below.

**Assumption 4**: \((\tau,\alpha)\)**-gradient dominated.** _A function \(f\) is called gradient dominated on set if it holds, for some \(\alpha\geq 1\) and \(\tau>0\):_

\[f(\boldsymbol{x})-f^{\star} \leq \tau\|\nabla f(\boldsymbol{x})\|^{\alpha},\qquad\forall\boldsymbol {x}\in\mathbb{R}^{d}.\] (11)

Examples of functions satisfying this assumption are convex functions (\(\alpha=1\)) and strongly convex functions (\(\alpha=2\)), see Appendix D.1. For such functions, we can guarantee convergence (in expectation) to a _global minimum_, i.e. we can find a point \(\boldsymbol{x}\) such that \(f(\boldsymbol{x})-f^{\star}\leq\varepsilon\).

The Gradient-dominance property is interesting because many non-convex functions have been shown to satisfy it [28; 13; 18]. Furthermore, besides convergence to a global minimum, we get accelerated rates.

We note that for \(\alpha>3/2\) (and only for this case), we needed to assume the following (stronger) inequality:

\[\mathbb{E}f(\boldsymbol{x}_{t})-f^{\star} \leq \tau\mathbb{E}\big{[}\|\nabla f(\boldsymbol{x}_{t})\|\big{]}^{ \alpha},\] (12)

where the expectation is taken with respect to the iterates \((\boldsymbol{x}_{t})\) of our algorithms. This is a stronger assumption than (11). To avoid using this stronger assumption, we can assume that the iterates belong to some compact set \(Q\subset\mathbb{R}^{d}\) and that the gradient norm is uniformly bounded: \(\forall\boldsymbol{x}\in Q:\|\nabla f(\boldsymbol{x})\|\leq G\). Then, a \((\tau,\alpha)\)-gradient dominated on set \(Q\) function is also a \((\tau G^{\alpha-3/2},3/2)\)-gradient dominated on this set for any \(\alpha>3/2\).

In Theorem 3 we extend the results of Theorem 1 to gradient-dominated functions.

**Theorem 3**: _Under Assumptions 1,2,4, for \(M\geq L\) and \(T:=Sm\) we have: - For \(1\leq\alpha\leq 3/2\): \(\mathbb{E}[f(\bm{x}_{T})]-f^{\star}=\mathcal{O}\Big{(}\big{(}\frac{\alpha\sqrt{M }\tau^{3/(2\alpha)}}{(3-2\alpha)T}\big{)}^{\frac{2\alpha}{3-2\alpha}}+\tau \frac{\delta_{2}^{2\alpha}}{M^{\alpha}}+\tau\delta_{1}^{\alpha}\Big{)}\). - For \(3/2<\alpha\leq 2\), let \(h_{0}=\mathcal{O}(\frac{F_{0}}{(\sqrt{M}\tau^{\frac{3}{2\alpha}})^{\frac{2 \alpha}{3-2\alpha}}})\), then for \(T\geq t_{0}=\mathcal{O}({h_{0}}^{\frac{3-2\alpha}{2\alpha}}\log(h_{0}))\) we have:_

\[E[f(\bm{x}_{T})]-f^{\star} = \mathcal{O}\Big{(}(\sqrt{M}\tau^{\frac{3}{2\alpha}})^{\frac{2 \alpha}{3-2\alpha}}\big{(}\frac{1}{2}\big{)}^{(\frac{2\alpha}{3})^{T-t_{0}}}+ \tau\frac{\delta_{2}^{2\alpha}}{M^{\alpha}}+\tau\delta_{1}^{\alpha}\Big{)}\;.\]

Theorem 3 shows (up to the noise level) for \(1\leq\alpha<3/2\) a sublinear rate, for \(\alpha=3/2\) a linear rate (obtained by taking the limit \(\alpha\to 3/2\)) and a superlinear rate for \(\alpha>3/2\).

We do the same thing for Theorem 2 which we extend in Theorem 4. In this case, we need to set the snapshot line 3 in Algorithm 1) as in 5 i.e. the snapshot corresponds to the state with the smallest value of \(f\) during the last \(m\) iterations.

**Theorem 4**: _Under Assumptions 1,3,4, for \(M=\max(L,34\delta_{1}m^{2},11\delta_{2}m)\), we have: - For \(1\leq\alpha\leq 3/2\) : \(\mathbb{E}[f(\bm{x}_{Sm})]-f^{\star}=\mathcal{O}\Big{(}\big{(}\frac{\alpha \sqrt{M}\tau^{3/(2\alpha)}}{(3-2\alpha)Sm}\big{)}^{\frac{2\alpha}{3-2\alpha}} \Big{)}\;.\) - For \(3/2<\alpha\leq 2\), let \(h_{0}=\mathcal{O}(\frac{F_{0}}{(\frac{\sqrt{M}}{m}\tau^{\frac{3}{2\alpha}})^{ \frac{2\alpha}{3-2\alpha}}})\), then for \(S\geq s_{0}=\mathcal{O}({h_{0}}^{\frac{3-2\alpha}{2\alpha}}\log(h_{0}))\) we have:_

\[\mathbb{E}[f(\bm{x}_{Sm})]-f^{\star} = \Big{(}\big{(}\frac{\sqrt{M}}{m}\tau^{\frac{3}{2\alpha}})^{\frac {2\alpha}{3-2\alpha}}\big{(}\frac{1}{2}\big{)}^{(\frac{2\alpha}{3})^{S-t_{0}} }\Big{)}\]

Again, the same behavior is observed as for Theorem 3 but this time without noise (variance reduction is working). To the best of our knowledge, this is the first time such analysis is made. As a direct consequence of our results, we obtain new global complexities for the variance-reduced and lazy variance-reduced Cubic Newton methods on the class of gradient-dominated functions.

To compare the statements of Theorems 3 and 4, for convex functions (i.e. \(\alpha=1\)), Theorem 3 guarantees convergence to a \(\varepsilon-\)global minimum in at most \(\mathcal{O}(\frac{1}{\varepsilon^{5/2}}+\frac{d}{\varepsilon^{3/2}})\)_GradCost_, whereas Theorem 4 only needs \(\mathcal{O}(\frac{g(n,d)}{\sqrt{\varepsilon}})\)_GradCost_, where \(g(n,d)\) is either \(g^{Lazy}(n,d)=(nd)^{5/6}\wedge n\sqrt{d}\) or \(g^{VR}(n,d)=(nd)^{4/5}\wedge(n^{2/3}d+n)\). See the Appendix D.3 for more details.

## 5 Limitations and possible extensions

**Estimating similarity between the helpers and the main function.** While we show in this work that we can have an improvement over training alone, this supposes that we know the similarity constants \(\delta_{1},\delta_{2}\), hence it will be interesting to have approaches that can adapt to such constants.

**Engineering helper functions.** Building helper task with small similarities is also an interesting idea. Besides the examples in supervised learning and core-sets that we provide, it is not evident how to do it in a generalized way.

**Using the helper to regularize the cubic subproblem.** We note that while we proposed to approximate the "cheap" part as well in Section 3, one other theoretically viable approach is to keep it intact and approximately solve a "proximal type" problem involving \(h\), this will lead to replacing \(L\) by \(\delta\), but the subproblem is even more difficult to solve. However our theory suggests that we don't need to solve this subproblem exactly, we only need \(m\geq\frac{L}{\delta}\). We do not treat this case here.

## 6 Conclusion

In this work, we proposed a general theory for using auxiliary information in the context of the cubically regularized Newton's method. Our theory encapsulates the classical stochastic methods as well as variance reduction and Lazy methods. For auxiliary learning, we showed a provable benefit compared to training alone. Besides studying the convergence for general non-convex functions for which we show convergence to approximate local minima, we also study gradient-dominated functions, for which convergence is accelerated and is to approximate global minima.

## References

* [1] A. Agafonov, D. Kamzolov, P. Dvurechensky, A. Gasnikov, and M. Takac. Inexact tensor methods and their application to stochastic convex optimization. _arXiv preprint arXiv:2012.15636_, 2020.
* [2] N. Aviv, A. Idan, M. Haggai, C. Gal, and F. Ethan. Auxiliary learning by implicit differentiation. _ICLR 2021_.
* [3] O. Bachem, M. Lucic, and K. Andreas. Practical coreset constructions for machine learning. _arXiv:1703.06476 [stat.ML]_https://arxiv.org/abs/1703.06476_, 2017.
* [4] S. Baifeng, H. Judy, S. Kate, D. Trevor, and X. Huijuan. Auxiliary task reweighting for minimum-data learning. _34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada_.
* [5] C. Cartis, N. I. Gould, and P. L. Toint. Adaptive cubic regularisation methods for unconstrained optimization. Part I: motivation, convergence and numerical results. _Mathematical Programming_, 127(2):245-295, 2011.
* [6] C. Cartis and K. Scheinberg. Global convergence rate analysis of unconstrained optimization methods based on probabilistic models. _Mathematical Programming_, 169:337-375, 2018.
* [7] E. M. Chayti and S. P. Karimireddy. Optimization with access to auxiliary information. _arXiv:2206.00395 [cs.LG]_, 2022.
* [8] A. R. Conn, N. I. Gould, and P. L. Toint. _Trust region methods_. SIAM, 2000.
* [9] N. Doikov, E. M. Chayti, and M. Jaggi. Second-order optimization with lazy hessians. _arXiv:2212.00781 [math.OC]_, 2022.
* [10] N. Doikov and Y. Nesterov. Minimizing uniformly convex functions by cubic regularization of newton method. _Journal of Optimization Theory and Applications 189:317-339_, 2021.
* [11] S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM Journal on Optimization_, 23(4):2341-2368, 2013.
* [12] S. Ghadimi, H. Liu, and T. Zhang. Second-order methods with cubic regularization under inexact information. _arXiv preprint arXiv:1710.05782_, 2017.
* [13] M. Hardt and T. Ma. Identity matters in deep learning. _arXiv preprint arXiv:1611.04231_, 2016.
* [14] C. J. Hillar and L.-H. Lim. Most tensor problems are np-hard. _Journal of the ACM (JACM) 60 45._, 2013.
* [15] H. J. Kelley. Gradient theory of optimal flight paths. _Ars Journal_, 30(10):947-954, 1960.
* [16] J. Kiefer and J. Wolfowitz. Stochastic estimation of the maximum of a regression function. _Ann. Math. Statist. Volume 23, Number 3, 462-466_, 1952.
* [17] J. M. Kohler and A. Lucchi. Sub-sampled cubic regularization for non-convex optimization. _arXiv preprint arXiv:1705.05933_, 2017.
* Advances in Neural Information Processing Systems_, volume 35, pages 10862-10875, 2022.
* [19] Y. Nesterov. _Lectures on convex optimization_, volume 137. Springer, 2018.
* [20] Y. Nesterov and B. Polyak. Cubic regularization of Newton's method and its global performance. _Mathematical Programming_, 108(1):177-205, 2006.
* [21] T. Nilesh, S. Mitchell, J. Chi, R. Jeffrey, and J. Michael I. Stochastic cubic regularization for fast nonconvex optimization. _Part of Advances in Neural Information Processing Systems 31_, 2018.

* [22] J. Nocedal and S. Wright. _Numerical optimization_. Springer Science & Business Media, 2006.
* [23] B. T. Polyak. Gradient methods for minimizing functionals. _Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki, 3(4):643-653._, 1963.
* [24] B. T. Polyak. Newton's method and its use in optimization. _European Journal of Operational Research_, 181(3):1086-1096, 2007.
* [25] H. Robbins and S. Monro. A stochastic approximation method the annals of mathematical statistics. _Vol. 22, No. 3. pp. 400-407_, 1951.
* [26] V. Shamanskii. A modification of Newton's method. _Ukrainian Mathematical Journal_, 19(1):118-122, 1967.
* [27] J. A. Tropp et al. An introduction to matrix concentration inequalities. _Foundations and Trends(r) in Machine Learning_, 8(1-2):1-230, 2015.
* [28] L. uanzhi and Y. Yang. Convergence analysis of two-layer neural networks with relu activation. _Advances in neural information processing systems, 30_, 2017.
* [29] Z. Wang, Z. Yi, L. Yingbin, and L. Guanghui. Stochastic variance-reduced cubic regularization for nonconvex optimization. _AISTATS_, 2019.
* [30] B. Woodworth, K. Mishchenko, and F. Bach. Two losses are better than one: Faster optimization using a cheaper proxy. _arXiv preprint arXiv:2302.03542_, 2023.
* [31] L. Xingyu, S. B. Harjatin, K. George, and H. David. Adaptive auxiliary task weighting for reinforcement learning. _33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada_.
* [32] P. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Newton-type methods for non-convex optimization under inexact Hessian information. _arXiv preprint arXiv:1708.07164._, 2017.
* [33] P. Xu, J. Yang, F. Roosta-Khorasani, and M. W. Mahoney. Sub-sampled newton methods with non-uniform sampling. 2016.
* [34] X. Yang, Z. Song, I. King, and Z. Xu. A survey on deep semi-supervised learning. Technical report, 2021.
* [35] D. Zhou, P. Xu, and Q. Gu. Stochastic variance-reduced cubic regularization methods. _Journal of Machine Learning Research 20 1-47_, 2019.