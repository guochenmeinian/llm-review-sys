# A Massive Scale Semantic Similarity Dataset of Historical English

 Emily Silcock\({}^{1}\), Abhishek Arora\({}^{1}\), Melissa Dell\({}^{1,2}\)

\({}^{1}\)Harvard University; Cambridge, MA, USA.

\({}^{2}\)National Bureau of Economic Research; Cambridge, MA, USA.

\({}^{*}\)Corresponding author: melissadell@fas.harvard.edu.

###### Abstract

A diversity of tasks use language models trained on semantic similarity data. While there are a variety of datasets that capture semantic similarity, they are either constructed from modern web data or are relatively small datasets created in the past decade by human annotators. This study utilizes a novel source, newly digitized articles from off-copyright, local U.S. newspapers, to assemble a massive-scale semantic similarity dataset spanning 70 years from 1920 to 1989 and containing nearly 400M positive semantic similarity pairs. Historically, around half of articles in U.S. local newspapers came from newswires like the Associated Press. While local papers reproduced articles from the newswire, they wrote their own headlines, which form abstractive summaries of the associated articles. We associate articles and their headlines by exploiting document layouts and language understanding. We then use deep neural methods to detect which articles are from the same underlying source, in the presence of substantial noise and abridgement. The headlines of reproduced articles form positive semantic similarity pairs. The resulting publicly available HEADLINES dataset is significantly larger than most existing semantic similarity datasets and covers a much longer span of time. It will facilitate the application of contrastively trained semantic similarity models to a variety of tasks, including the study of semantic change across space and time.

## 1 Introduction

Transformer language models contrastively trained on large-scale semantic similarity datasets are integral to a variety of applications in natural language processing (NLP). Contrastive training is often motivated by the anisotropic geometry of pre-trained transformer models like BERT [5], which complicates working with their hidden representations. Representations of low frequency words are pushed outwards on the hypersphere, the sparsity of low frequency words violates convexity, and the distance between embeddings is correlated with lexical similarity. This leads to poor alignment between semantically similar texts and poor performance when individual term representations are pooled to create a representation for longer texts [21]. Contrastive training reduces anisotropy [25].

A variety of semantic similarity datasets have been used for contrastive training [18]. Many of these datasets are relatively small, and the bulk of the larger datasets are created from recent web texts; _e.g._, positive pairs are drawn from the texts in an online comment thread or from questions marked as duplicates in a forum. To provide a semantic similarity dataset that spans a much longer length of time and a vast diversity of topics, this study develops HEADLINES (**H**istorical **E**normous-**S**cale **A**b**stractive **D**up**LI**c**ate **N**ews **S**ummaries), a massive dataset containing nearly 400 million high quality semantic similarity pairs drawn from 70 years of off copyright U.S. newspapers. Historically, around half of content in the many thousands of local newspapers across the U.S. was taken from centralized sources such as the Associated Press wire [8]. Local newspapers reprinted wire articlesbut wrote their own headlines, which form abstractive summaries of the articles. Headlines written by different papers to describe the same wire article form positive semantic similarity pairs.

To construct HEADLINES, we digitize front pages of off-copyright local newspapers, localizing and OCRing individual content regions like headlines and articles. The headlines, bylines, and article texts that form full articles span multiple bounding boxes - often arranged with complex layouts - and we associate them using a model that combines layout information and language understanding [14]. Then, we use neural methods from [23] to accurately predict which articles come from the same underlying source, in the presence of noise and abridgement. HEADLINES allows us to leverage the collective writings of many thousands of local editors across the U.S., spanning much of the 20th century, to create a massive, high-quality semantic similarity dataset. HEADLINES captures semantic similarity with minimal noise, as positive pairs summarize the same underlying texts.

This study is organized as follows. Section 2 describes HEADLINES, and Section 3 relates it to existing datasets. Section 4 describes and evaluates the methods used for dataset construction, Section 5 benchmarks the dataset, and Section 6 discusses limitations and intended usage.

## 2 Dataset Description

HEADLINES contains 393,635,650 positive headline pairs from off-copyright newspapers. Figure 1 plots the distribution of content by state.

The supplementary materials summarize copyright law for works first published in the United States. The newspapers in HEADLINES are off-copyright because they were published without a copyright notice or did not renew their copyright, required formalities at the time. Far from being an oversight, it was rare historically to copyright news, outside the nation's most widely circulated papers. The headlines in our dataset were written by editors at these local papers, and hence are in the public domain and anyone can legally use or reference them without permission.

It is possible that a newspaper not itself under copyright could reproduce copywritten content from some third party - the most prevalent example of this is comics - but this does not pose a problem for HEADLINES, since the dataset is built around the locally written headlines that describe the same

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \hline Decade & Headline & Cluster & Positive & Word & Words Per & Line & Lines Per & Character \\  & Count & Count & Pair Count & Count & Headline & Count & Headline & Error Rate \\ \hline
1920s & 4,889,942 & 1,032,108 & 28,928,226 & 68,486,589 & 14.0 & 18,893,014 & 3.9 & 4.3\% \\
1930s & 5,519,472 & 1,126,566 & 37,529,084 & 75,210,423 & 13.6 & 21,905,153 & 4.0 & 3.7\% \\
1940s & 6,026,940 & 1,005,342 & 6,237,907,041 & 6,629,003 & 10.2 & 19,538,729 & 3.2 & 2.4\% \\
1950s & 7,530,810 & 1,192,858 & 105,272,386 & 61,127,331 & 8.1 & 20,823,786 & 2.8 & 2.3\% \\
1960s & 6,533,071 & 926,819 & 108,415,279 & 46,640,311 & 7.1 & 16,408,148 & 2.5 & 3.7\% \\
1970s & 3,664,201 & 585,782 & 5,291,097 & 24,472,831 & 6.7 & 7,829,510 & 2.1 & 3.2\% \\
1980s & 703,025 & 107,507 & 28,772,722 & 5,161,537 & 7.3 & 1,502,893 & 2.1 & 1.5\% \\
**Total** & **34,867,488** & **6,039,982** & **393,635,650** & **342,728,007** & **9.8** & **106,991,233** & **3.1** & \\ \hline \hline \end{tabular}
\end{table}
Table 1: Descriptive statistics of HEADLINES.

Figure 1: Geographic variation in source of headlines

wire articles. If we were to accidentally include a syndicated headline, it would be dropped by our post-processing, since we drop headline pairs within a Levenshtein edit distance threshold of each other. It is also worth noting that a detailed search of U.S. copyright catalogs by [19] did not turn up a single instance of a wire service copyrighting their articles. (Even if they had, however, it would not pose a problem for headlines, since they were written locally.)

Figure 2 shows examples of semantic similarity pairs.

We quantify variation in HEADLINES across years, using a measure reminiscent of Earth Mover distance. This measure computes how much each text in a query dataset (e.g., 1920 headlines) would have to change (in embedding space) to have the same representation as the closest text in a key dataset (e.g., 1930 headlines).

Specifically, we first take a random sample of 10,000 texts per year. For year \(j\), we embed texts \(t_{1j}...t_{10,000j}\) using all-mpnet-base-v2. We choose MPNet because it has been shown to perform well across a variety of embedding datasets and tasks [18]. For each of these \(t_{ij}\), we compute the most similar embedding in year \(k\), measured by cosine similarity. This gives us a vector of similarity measures \(s_{1jk}...s_{10,000jk}\), that for each text in year \(j\) measure proximity to the most similar text in year \(k\). We average these similarities to calculate \(SIM_{jk}\).1 Figure 3, which plots the \(SIM_{jk}\), shows that similarity increases with temporal proximity. The dark square towards the upper left is World War 2, during which newspapers coverage was more homogeneous due to the centrality of the war.

Figure 3: Average similarity between different years of HEADLINES.

Figure 2: Semantic similarity examples, showing article image crops and OCR’ed headlines.

HEADLINES is useful for training and evaluating models that aim to capture abstractive similarity, whether using the embeddings for tasks like clustering, nearest neighbor retrieval, or semantic search [18]. Because it contains chronological content over a long span of time, it can be used to evaluate dynamic language models for processing continuously evolving content [1; 15], as well as how large language models can be adapted to process historical content [17; 4; 16]. Likewise, it can be used to train or evaluate models that predict the region or year a text was written [20]. In addition, it is useful for training models and developing benchmarks for a number of downstream tasks, such as topic classification of vast historical and archival documents, which have traditionally been classified by hand. This is an extremely labor-intensive process, and as a result many historical archives and news collections remain largely unclassified. Similarly, it could facilitate creating a large scale dataset to measure term-level semantic change, complementing existing smaller-scale SemEval tasks.

HEADLINES has a Creative Commons CC-BY license, to encourage widespread use, and is available on Huggingface.2

Footnote 2: https://huggingface.co/datasets/dell-research-harvard/headlines-semantic-similarity

## 3 Existing Semantic Similarity Datasets

There is a dense literature on semantic similarity, with datasets covering diverse types of textual similarity and varying greatly in size. The focus of HEADLINES on semantic similarity in historical texts sets it apart from other widely used datasets. It also dwarfs the size of most existing datasets, aggregating the collective work of 20th century newspapers editors, from towns across the U.S. Its paired headlines summarize the same text, rather than being related by other forms of similarity frequently captured by datasets, such as being in the same conversation thread or answering a corresponding question.

One related class of semantic similarity datasets consists of duplicate questions from web platforms, _e.g._, questions tagged by users as duplicates from WikiAnswers (77.4 million positive pairs) [6], duplicate stack exchange questions (around 304,000 duplicate title pairs) [2], and duplicate quora questions (around 400,000 duplicate pairs) [12].3 Alternatively, MS COCO [3] used Amazon's Mechanical Turk to collect five captions for each image in the dataset, resulting in around 828,000 positive caption pairs. In Flickr [26], 317,695 positive semantic similarity pairs describe around 32,000 underlying images. Like HEADLINES, positive pairs in these datasets refer to the same underlying content, but are describing an image rather than providing an abstractive summary of a longer text. In future work, HEADLINES could be expanded to include caption pairs describing the same underlying photo wire image, as local papers frequently wrote their own captions.

Footnote 3: Dataset sizes are drawn, when applicable, from a table documenting the training of Sentence BERT [21].

Online comment threads have also been used to train semantic similarity models. For example, the massive scale Reddit Comments [11] draws positive semantic similarity pairs from Reddit conversation threads between 2016 and 2018, providing 726.5 million positive pairs. Semantic similarity between comments in an online thread reflects conversational similarity, to the extent the thread stays on topic, rather than abstractive similarity. Likewise, question-answer and natural-language inference datasets are widely used for semantic similarity training.While other datasets exploit abstractive summaries - _e.g._, Semantic Scholar (S2ORC) has been used to create semantic similarity pairs of the titles and abstracts of papers that cite each other - to our knowledge there are not large-scale datasets with abstractive summaries of the same underlying texts.

A wide variety of text embedding datasets have been combined into the Massive Text Embedding Benchmark (MTEB) [18], which evaluates 8 embedding tasks on 58 datasets covering 112 languages. We measure the similarity between HEADLINES and the English datasets in MTEB, using the Earth Mover-style distance, described in Section 2. As above, we first take a random sample of (up to) 10,000 texts from each decade of HEADLINES, as well as each of the English datasets in MTEB (if the dataset contains fewer than 10K texts, we use the full dataset and limit the comparison dataset to the same number of randomly selected texts). For dataset \(j\), we embed texts \(t_{1j}...t_{10,000j}\). For each of these \(t_{ij}\), we compute the most similar embedding in dataset \(k\), averaging these across all texts in \(j\) to compute \(SIM_{jk}\). \(SIM_{jk}\) need not be symmetric. Suppose dataset \(j\) is highly homogeneous, whereas dataset \(k\) is heterogeneous. \(SIM_{jk}\) may be high, because the similar embeddings in homogeneous dataset \(j\) are close to a subset of embeddings in dataset \(k\). On the other hand, \(SIM_{kj}\) may be low, because most texts in dataset \(k\) are dissimilar from texts in homogeneous dataset \(j\).

Figure 4 shows the entire similarity matrix between HEADLINES and the English datasets in MTEB; rows are the query dataset and columns are the key. The style of the figure was adapted from [18]. The average similarity between HEADLINES and MTEB datasets is 31.4, whereas the average similarity between MTEB datasets and HEADLINES is 33.5, supporting our supposition that HEADLINES is diverse on average relative to other benchmarks. This average max similarity shows that there is ample information in HEADLINES not contained in existing datasets.

Table 2 shows examples where the nearest text to a headline in an MTEB dataset is highly similar, versus of average similarity. In some cases, highly similar texts in fact have a different meaning, but the limited context in the MTEB datasets makes this difficult to capture.

\begin{table}
\begin{tabular}{l l c} \hline \hline
**Headline** & **Highly similar texts** & **Similarity** \\ “Inflation Cuts are Questined” & **Reddit:** “Today FOMC Resumed Meeting Inflation & 0.55 \\  & getting out of hand... Maybe... Maybe Not” & \\ “Bear Bites Off Arm of Child” & **StackExchange:** “How to handle animal AI biting & 0.46 \\  & and holding onto the character.” & \\ “British Cruiser Reported Sunk” & **Twitter:** “That’s I, Britain is Sunk” & 0.61 \\ “Will Free Press Dance to Government Tune” & **Twitter:** “Donald Trump v, a free press” & 0.60 \\ “Partitioning Plan Unsatisfactory, Re Declares” & **Ubunta Questions:** “Partitioning Issues” & 0.51 \\ \hline
**Headline** & **Average similarity texts** & **Similarity** \\ “Reds Knot Strong Tie” & **ArXiv:** “Knots and Polytopes” & 0.34 \\ “SHOWERS PROMISED TO END HEAT WAVE & **StackOverflow:** “how to annotate heatmap with & 0.27 \\ Two Deaths From Heat Over Weekend” & text in matplotlib” & \\ “Salary Boost Due For Some On Labor Day” & **Twitter:** “Glassdoor will now tell you if your & 0.31 \\ “40 Old Ladies Now In Senate Says Rogers” & **Quora:** “Is 19 young?” & 0.28 \\ \hline \hline \end{tabular}
\end{table}
Table 2: This table shows similarities between example texts.

Figure 4: Average similarity between HEADLINES (by decade) and the English datasets in the Massive Text Embedding Benchmark. The similarity measure is described in the text.

## 4 Dataset Construction and Evaluation

### Digitization

We digitized front pages from off-copyright newspapers spanning 1920-1989. We recognize layouts using Mask RCNN [10] and OCR the texts. We transcribed the headlines using Tesseract. The digitization was performed using Azure F-Series CPU nodes.

We evaluate this OCR on a hand annotated sample of 300 headlines per decade. Table 1 reports the character error rate, defined as the Levenshtein distance between the transcribed text and the ground truth, normalized by the length of the ground truth. As expected, OCR quality improves over time, as there is less damage from aging and fewer unusual fonts.

### Article Association

Newspaper articles have complex and irregular layouts that can span multiple columns (Figure 5).

We associate the (potentially multiple) headline bounding boxes with the (potentially multiple) article bounding boxes and byline boxes that comprise a single article using a combination of layout information and language understanding. A rule-based approach using the document layouts gets many of the associations correct, but misses some difficult cases where article bounding boxes are arranged in complex layouts. Language understanding can be used to associate such articles but must be robust to noise, from errors in layout detection (_e.g._ from cropping part of a content bounding box or adding part of the line below) and from OCR character recognition errors.

Hand annotating a sufficiently large training dataset would have been infeasibly costly. Instead, we devise a set of rules that - while recall is relatively low - have precision above 0.99, as measured on an evaluation set of 3,803 labeled bounding boxes. The algorithm exploits the positioning of article bounding boxes relative to headline boxes (as in Figure 6), first grouping an article bounding box with a headline bounding box if the rules are met and then associating all article bounding boxes grouped with the same headline together. Since precision is above 0.99, the rule generates nearly perfect silver-quality training data.

Figure 5: Articles that are misassociated with rule-based or image-based methodsTo train a language model to predict whether article box B follows box A, we embed the first and last 64 tokens of the texts in boxes B and A, respectively, with a RoBERTa base model [14]. The pair is positive when B follows A. The training set includes 12,769 positive associated pairs, with training details described in the supplementary materials. At inference time, we first associate texts using the rule-based approach, described in Figure 6, which has extremely high precision. To improve recall, we then apply the RoBERTa cross-encoder to remaining article boxes that could plausibly be associated, given their coordinates. Texts cannot be followed by a text that appears to the left, as layouts always proceed from left to right, so these combinations are not considered.

We evaluate this method on a hand-labeled dataset of 214 scans. Full details of this dataset are given in the appendix. Table 3 evaluates recall, precision and F1 for associated articles. The F1 of 93.7

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & (1) & (2) & (3) \\  & F1 & Recall & Precision \\ \cline{2-4} Full Article Association & 93.7 & 88.3 & 99.7 \\ \hline \hline \end{tabular}
\end{table}
Table 3: This table evaluates the full article association model.

Figure 6: Illustration of article association pipeline

is high, and precision is extremely high. Errors typically occur when there is an error in the layout analysis or when contents are very similar, _e.g._, grouping multiple obituaries into a single article.

### Detecting Reproduced Content

Accurately detecting reproduced content can be challenging, as articles were often heavily abridged by local papers to fit within their space constraints and errors in OCR or article association can add significant noise. Table 4 shows examples of reproduced articles.

We use the model developed by [23], who show that a contrastively trained neural MPNet bi-encoder - combined with single linkage clustering of article representations - accurately and cheaply detects reproduced content. This bi-encoder is contrastively trained on a hand-labeled dataset (detailed in the appendix) to create similar representations of articles from the same wire source and dissimilar representations of articles from different underlying sources, using S-BERT's online contrastive loss [9] implementation.

We run clustering on the article embeddings by year over all years in our sample. In post-processing, we use a simple set of rules exploiting the dates of articles within clusters to remove content like weather forecasts and legal notices, that are highly formulaic and sometimes cluster together when they contain very similar content (_e.g._ a similar 5-day forecast) but did not actually come from the same underlying source. We remove all headline pairs that are below a Levenshtein edit distance, normalized by the min length in the pair, of 0.1, to remove pairs that are exact duplicates up to OCR noise. Training and inference were performed on an A6000 GPU card. More details are provided in the supplementary materials.

To evaluate how well the model detects reproduced content, we use a labeled sample of all front page articles appearing in the newspaper corpus for three days in the 1930s and 1970s, taken from [23]. This sample consists of 54,996 positive reproduced article pairs and 100,914,159 negative pairs. The

\begin{table}
\begin{tabular}{l l l}  & \begin{tabular}{} \end{tabular} & \begin{tabular}{} \end{tabular} & 
\begin{tabular}{} \end{tabular} \\ \end{tabular}
\end{table}
Table 4: Examples of reproduced articles. Additions are highlighted, and OCR errors are underlined.

large-scale labeled evaluation dataset was generated using the above pipeline, so the evaluation is inclusive of any errors that result from upstream layout detection, OCR, or article association errors.

The neural bi-encoder methods achieve a high adjusted rand index (ARI) of 91.5, compared to 73.7 for an optimal local sensitive hashing specification, chosen on the validation set. This shows that our neural methods substantially outperform commonly used sparse methods for detecting reproduced content. The neural bi-encoder is slightly outperformed by adding a re-ranking step that uses a neural cross-encoder on the best bi-encoder matches (ARI of 93.7). We do not implement this method because the cross-encoder doesn't scale well. In contrast, the bi-encoder pipeline can be scaled to 10 million articles on a single GPU in a matter of hours, using a FAISS [13] backend.

An error analysis is provided in [23]. Errors typically consist of articles about the same story from different wire services (_e.g._ the Associated Press and the United Press) or updates to a story as new events unfolded. Both types of errors will plausibly still lead to informative semantic similarity pairs.

## 5 Benchmarking

We benchmark HEADLINES using a variety of different language models and the MTEB clustering task. This task embeds texts using different base language models and then uses \(k\) - the number of clusters in the ground truth data - for k-means clustering. Following MTEB, we score the model using the v-measure [22]. We should note that real-world problems are often framed as clustering tasks - rather than as classification tasks - because \(k\) is unknown. By using \(k\) from the ground truth, it makes the task easier. Nevertheless, we examine this task to allow for comparison with the rest of the literature.

Figure 7 plots the results of this benchmarking exercise. MTEB benchmarks clustering on Arxiv, Bioarxiv, Medarxiv, Reddit, StackExchange, and Twenty Newsgroups. Texts are labeled with their classification (e.g., fields like ComputerVision for the Arxiv datasets; the subreddit for Reddit). The

\begin{table}
\begin{tabular}{l|c|c} \hline \hline  & **Neural** & **Non-Neural** \\ \hline
**Most scalable** & Bi-encoder (91.5) & LSH (73.7) \\
**Less scalable** & Re-ranking (**93.7**) & \(N\)-gram overlap (75.0) \\ \hline \hline \end{tabular}
\end{table}
Table 5: The numbers in parentheses are the Adjusted Rand Index for four different models - a bi-encoder, a “re-ranking” strategy that combines a bi- and cross-encoder, locally sensitive hashing (LSH), and \(N\)-gram overlap. Hyperparameters were chosen on the NEWS-COPY validation set, and all models were evaluated on the NEWS-COPY test set.

Figure 7: This figure benchmarks HEADLINES on the MTEB clustering task. The x-axis shows the year that the sample was taken from and the y-axis gives the v-measure.

best average v-score across these datasets, from MPNet, is 43.69. The best average v-score across decades for HEADLINES, from ST5-XXL, is around 78. This difference is likely to reflect, at least in part, that our cluster labels are less noisy, since texts in the same cluster summarize the same content. In contrast, titles of Reddit posts in the same subreddit may be only loosely linked to each other, and many could be within the domain of another subreddit cluster. While a user happened to post in one subreddit, another user could have reasonably made the same titled post in a different subreddit. The under-identification of the clustering tasks for some texts in the MTEB datasets is suggested by the very low v-scores across state-of-the-art language models. Overall, this suggests the high quality of clusters in HEADLINES relative to many web text datasets. Yet there is still ample scope for improvement to the state-of-the-art model.

## 6 Limitations and Recommended Usage

HEADLINES contains some transcription errors. For working with historical texts, these are more a feature than a bug, as most historical texts are transcribed and also contain various OCR errors. Training a model on transcribed texts likely makes it more robust to transcription errors at inference time. However, researchers requiring completely clean texts should seek another corpus.

HEADLINES contains historical language, that reflects the semantics and cultural biases of many thousands of local newspaper editors. This is a distinguishing feature of HEADLINES, that is core to many potential applications. We do not attempt to filter texts with antiquated terms or that may be considered offensive, as this would invalidate the use of the dataset for studying semantic change and historical contexts. At the same time, this makes HEADLINES less suited for tasks that require texts that fully conform to current cultural standards or semantic norms. For these reasons, we recommend against the use of HEADLINES for training generative models. Rather, with nearly 400M positive semantic similarity pairs spanning much of the 20th century, it can plausibly play an important role in facilitating the application of large language models to historical texts.

## Acknowledgements

Funding was provided by the Harvard Data Science Initiative, Harvard Catalyst, and Microsoft Azure compute credits. We thank Luca D'Amico-Wong for excellent research assistance.

## References

* [1]Amba Hombaiah, S., Chen, T., Zhang, M., Bendersky, M., and Najork, M. Dynamic language models for continuously evolving content. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_ (2021), pp. 2514-2524.
* [2]BERT, S. stackexchange, 2021.
* [3]Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollar, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server. _arXiv preprint arXiv:1504.00325_ (2015).
* [4]Ehrmann, M., Hamdi, A., Pontes, E. L., Romanello, M., and Doucet, A. Named entity recognition and classification in historical documents: A survey. _ACM Computing Surveys_ (2021).
* [5]Ethayarajh, K. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. _arXiv preprint arXiv:1909.00512_ (2019).
* [6]Fader, A., Zettlemoyer, L., and Etzioni, O. Open question answering over curated and extracted knowledge bases. In _Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining_ (2014), pp. 1156-1165.
* [7]Falkner, S., Klein, A., and Hutter, F. BOHB: Robust and efficient hyperparameter optimization at scale. In _Proceedings of the 35th International Conference on Machine Learning_ (10-15 Jul 2018), J. Dy and A. Krause, Eds., vol. 80 of _Proceedings of Machine Learning Research_, PMLR, pp. 1437-1446.
* [8]Guarneri, J. _Newsprint Metropolis_. University of Chicago Press, 2017.
* [9]Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality reduction by learning an invariant mapping. In _2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)_ (2006), vol. 2, IEEE, pp. 1735-1742.
* [10]He, K., Gkioxari, G., Dollar, P., and Girshick, R. Mask r-cnn. _Proceedings of the IEEE international conference on computer vision_ (2017), 2961-2969.
* [11]Henderson, M., Budzianowski, P., Casanueva, I., Coope, S., Gerz, D., Kumar, G., Mrksic, N., Spithourakis, G., Su, P.-H., Vulic, I., et al. A repository of conversational datasets. In _Proceedings of the First Workshop on NLP for Conversational AI_ (2019), pp. 1-10.
* [12]Iyer, S., Dandekar, N., and Csernai, K. First quora dataset release: Question pairs, 2017.
* [13]Johnson, J., Douze, M., and Jegou, H. Billion-scale similarity search with gpus. _IEEE Transactions on Big Data 7_, 3 (2019), 535-547.
* [14]Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_ (2019).
* [15]Loureiro, D., Barbieri, F., Neves, L., Anke, L. E., and Camacho-Collados, J. Timelms: Diachronic language models from twitter. _arXiv preprint arXiv:2202.03829_ (2022).
* [16]Manjavacas, E., and Fonteyn, L. Macherth: Development and evaluation of a historically pre-trained language model for english (1450-1950). In _Proceedings of the Workshop on Natural Language Processing for Digital Humanities_ (2021), pp. 23-36.
* [17]Manjavacas, E., and Fonteyn, L. Adapting vs. pre-training language models for historical languages. _Journal of Data Mining & Digital Humanities_, Digital humanities in languages (2022).
* [18]Muennighoff, N., Tazi, N., Magne, L., and Reimers, N. Mteb: Massive text embedding benchmark. _arXiv preprint arXiv:2210.07316_ (2022).

* [19]Ockerloom, J. M. Everybody's library questions: Newspaper copyrights, notices, and renewals, 2019.
* [20]Rastas, I., Ryan, Y. C., Tihonen, I., Qaraei, M., Repo, L., Babbar, R., Makela, E., Tolonen, M., and Ginter, F. Explainable publication year prediction of eighteenth century texts with the bert model. In _Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change_ (2022), pp. 68-77.
* [21]Reimers, N., and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. _arXiv preprint arXiv:1908.10084_ (2019).
* [22]Rosenberg, A., and Hirschberg, J. V-measure: A conditional entropy-based external cluster evaluation measure. In _Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL)_ (2007), pp. 410-420.
* [23]Silcock, E., D'Amico-Wong, L., Yang, J., and Dell, M. Noise-robust de-duplication at scale. _International Conference on Learning Representations_ (2023).
* [24]Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y. Mpnet: Masked and permuted pre-training for language understanding. _Advances in Neural Information Processing Systems 33_ (2020), 16857-16867.
* [25]Wang, T., and Isola, P. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. _International Conference on Machine Learning 119_ (2020), 9929-9939.
* [26]Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics 2_ (2014), 67-78.

## Appendices

### Methods to Associate Articles

Figure 6 in the main text illustrates the full article association procedure.

First, we used a rule-based algorithm using associate article bounding boxes that are under the same headline, as these are part of the same article with extremely high probability. Algorithm 1 gives pseudocode for this method. We set the parameters as \(P_{S}=100\), \(P_{T}=20\), \(P_{B}=50\).

For training data, where we want article pairs that are not only part of the same article, but also where they appear in the given order, we further narrow down the pairs. Specifically, we use only those pairs which are horizontally next to each other, and which have no other bounding boxes below them, as for these pairs, we can guarantee that the pair of bounding follow directly after one another (whereas for other article bounding boxes that share a headline, there may be a third bounding box in between). Algorithm 2 shows pseudocode for this procedure, and we used \(P_{C}=5\), and it is further illustrated in panel A of figure 6 in the main text.

For hard negatives, we used article boxes under the same headline in reverse reading order (right to left). For standard negatives, we took pairs of articles on the same page, where B was above and to the left of A, as articles do not read from right to left. One twelfth of our training data were positive pairs, another twelfth were hard negative pairs and the remainder were standard negative pairs. This outperformed a more balanced training sample.

We use this dataset to finetune a cross-encoder using a RoBERTa base model [14]. We used a Bayesian search algorithm [7] to find optimal hyperparameters on one tenth of our training data (limited compute prevented us from running this search with the full dataset), which led to a learning rate of 1.7e-5, with a batch size of 64 and 29.57% warm up. We trained for 26 epochs with an AdamW optimizer, and optimize a binary cross-entropy loss.

We evaluate these methods on a hand-labeled dataset of 214 scans, randomly selected from 1968 and 1955. These scans were labeled by a highly-trained undergraduate research assistant. Summary statistics of this dataset are given in table A-1 and evaluation results are given in the main text.

### Methods to Detect Reproduced Content

To detect reproduced content, we use the contrastively trained bi-encoder model developed by [23], which is trained to learn similar representations for reproduced articles and dissimilar representations for non-reproduced articles. This model is based on an S-BERT MPNET model [21, 24] and is fine-tuned on a hand-labelled dataset of articles from the same underlying wire source, using S-BERT's online contrastive loss [9] implementation, with a 0.2 margin and cosine similarity as the distance metric. The learning rate is 2e-5 with 100% warm up and a batch size of 32. It uses an AdamW optimizer, and the model is trained for 16 epochs. This bi-encoder is trained and evaluated on a hand-labeled dataset, which is detailed in A-2. The results of this evaluation are given in the main text.

To create clusters from the bi-encoder embeddings, we use highly scalable single-linkage clustering, with a cosine similarity threshold of 0.94. We build a graph using articles as nodes, and add edges if the cosine similarity is above this threshold. As edge weights we use the negative exponential of the difference in dates (in days) between the two articles. We then apply Leiden community detection to the graph to control false positive edges that can otherwise merge disparate groups of articles.

We further remove clusters that have over 50 articles and contain articles with greater than five different dates. We also remove clusters that contain over 50 articles, when the number of articles is more than double the number of unique newspapers from which these articles are sourced. This

\begin{table}
\begin{tabular}{c c c c} \hline \hline Scan count & Article bounding boxes & Headline bounding boxes & Article-article associations \\ \hline
214 & 3,803 & 2,805 & 1,851 \\ \hline \hline \end{tabular}
\end{table}
Table A-1: Descriptive statistics of article association training data.

removes clusters of content that are correctly clustered in the sense of being based on the same underlying source, but are not useful for the HEADLINES dataset. For example, an advertisement 

[MISSING_PAGE_EMPTY:15]