# Implicit Manifold Gaussian Process Regression

Bernardo Fichera\({}^{1}\) &Viacheslav Borovitskiy\({}^{2}\) &Andreas Krause\({}^{2}\) &Aude Billard\({}^{1}\)

\({}^{1}\) EPFL &\({}^{2}\) ETH Zurich

###### Abstract

Gaussian process regression is widely used because of its ability to provide well-calibrated uncertainty estimates and handle small or sparse datasets. However, it struggles with high-dimensional data. One possible way to scale this technique to higher dimensions is to leverage the implicit low-dimensional _manifold_ upon which the data actually lies, as postulated by the _manifold hypothesis_. Prior work ordinarily requires the manifold structure to be explicitly provided though, i.e. given by a mesh or be known to be one of the well-known manifolds like the sphere. In contrast, in this paper we propose a Gaussian process regression technique capable of inferring implicit structure directly from data (labeled and unlabeled) in a fully differentiable way. For the resulting model, we discuss its convergence to the Matern Gaussian process on the assumed manifold. Our technique scales up to hundreds of thousands of data points, and improves the predictive performance and calibration of the standard Gaussian process regression in some high-dimensional settings.

## 1 Introduction

Gaussian processes are among the most adopted models for learning unknown functions within the Bayesian framework. Their data efficiency and aptitude for uncertainty quantification make them appealing for modeling and decision-making applications in the fields like robotics (Deisenroth and Rasmussen, 2011), geostatistics (Chiles and Delfiner, 2012), numerics (Hennig et al., 2015), etc.

The most widely used Gaussian process models, like squared exponential and Matern (Rasmussen and Williams, 2006), impose the simple assumption of differentiability of the unknown function while also respecting the geometry of \(\mathbb{R}^{d}\) by virtue of being stationary or isotropic. Such simple assumptions make uncertainty estimates _reliable_, albeit too conservative at times. The same simplicity makes these models struggle from the _curse of dimensionality_. We hypothesize that it is still possible to leverage these simple priors for real world high-dimensional problems granted that they are adapted to the implicit _low-dimensional submanifolds_ where the data actually lies, as illustrated by Figure 1.

Figure 1: _Euclidean_ (standard Matern-\({}^{5/2}\) kernel) vs _ours_ (implicit manifold) Gaussian process regression for data that lies on a dumbbell-shaped curve (\(1\)-dimensional manifold) assumed unknown. The data contains a small set of labeled points and a large set of unlabeled points. Our technique recognizes that the two lines in the middle are intrinsically far away from each other, giving a much better model on and near the manifold. Far away from the manifold it reverts to the Euclidean model.

Recent works in machine learning generalized Matern Gaussian processes for modeling functions on non-Euclidean domains such as manifolds or graphs (Azangulov et al., 2022; 2023; Borovitskiy et al., 2021; 2020; 2020). Crucially, this line of work assumes _known_ geometry (e.g. a manifold or a graph) beforehand. In this work we aim to widen the applicability of Gaussian processes for higher dimensional problems by _automatically learning_ the implicit low-dimensional manifold upon which the data lies, the existence of which is suggested by the _manifold hypothesis_. We propose a new model which learns this structure and approximates the Matern kernel on the implicit manifold.

Our approach can operate in both supervised and semi-supervised settings, with the emphasis on the latter: uncovering the implicit manifold may require a lot of samples from it, however these samples need not be labeled, and unlabeled data is usually more abundant. Taking inspiration in the manifold learning results of Coifman and Lafon (2006), Dunson et al. (2021) and others we approximate the unknown manifold by an appropriately weighted nearest neighbor graph. Then we use graph Matern kernels thereon as approximations to the manifold Matern kernels of Borovitskiy et al. (2020), extending them to the vicinity of the manifold in the ambient \(\mathbb{R}^{d}\) in an appropriate way.

### Related Work and Contribution

High-dimensional Gaussian process regression is an area of active research, primarily motivated by decision making applications like Bayesian optimization. There are three main directions in this area: (1) selecting a small subset of input dimensions, (2) learning a small number of new features by linearly projecting the inputs and (3) learning non-linear features. Our technique belongs to the third direction. Further details on the area can be found in the recent review by Binois and Wycoff (2022).

A close relative of our technique in the literature is described in Dunson et al. (2022). It targets the low-dimensional setting where the inputs are densely sampled on the underlying surface. It is based on the heat (diffusion) kernels on graphs as in Kondor and Lafferty (2002) and uses the Nystrom method to extend kernels to \(\mathbb{R}^{d}\), both of which may incur a high computational cost. Another close relative is the concurrent work by Peach et al. (2023) on modeling vector fields on implicit manifolds.

We are targeting the high-dimensional setting. Here, larger datasets of partly labeled points are often needed to _infer_ geometry. Because of this, we emphasize computational efficiency by leveraging sparse precision matrix structure of Matern kernels (as opposed to the heat kernels) and use KNN for sparsifying the graph and accelerating the Nystrom method. This results in linear computational complexity with respect to the number of data points. Furthermore, the model we propose is fully differentiable, which may be used to find both kernel and geometry hyperparameters by maximizing the marginal likelihood. Finally, to get reasonable predictions on the whole ambient space \(\mathbb{R}^{d}\), we combine the prediction of the geometric model with the prediction of a classical Euclidean Gaussian process, weighting these by the relative distance to the manifold.

The geometric model is differentiable with respect to its kernel-, likelihood- and geometry-related hyperparameters, with gradient evaluation cost being linear with respect to the number of data points. After training, we can efficiently compute the predictive mean and kernel as well as sample the predictive model, providing the basic computational primitives needed for the downstream applications like Bayesian optimization. We evaluate our technique on a synthetic low-dimensional example and test it in a high-dimensional large dataset setting of predicting rotation angles of rotated MNIST images, improving over the standard Gaussian process regression.

## 2 Gaussian Processes

A Gaussian process \(f\sim\operatorname{GP}(m,k)\) is a distribution over functions on a set \(\mathcal{X}\). It is determined by the mean function \(m(\bm{x})=\mathbb{E}\,f(\bm{x})\) and the covariance function (kernel) \(k(\bm{x},\bm{x}^{\prime})=\operatorname{Cov}(f(\bm{x}),f(\bm{x}^{\prime}))\).

Given data \(\mathbf{X},\bm{y}\), where \(\mathbf{X}=(\bm{x}_{1},..,\bm{x}_{n})^{\top}\) and \(\bm{y}=(y_{1},..,y_{n})^{\top}\) with \(\bm{x}_{i}\in\mathcal{X}^{d},y_{i}\in\mathbb{R}\), one usually assumes \(y_{i}=f(\bm{x}_{i})+\varepsilon_{i}\) where \(\varepsilon_{i}\sim\operatorname{N}(0,\sigma_{\varepsilon}^{2})\) is IID noise and \(f\sim\operatorname{GP}(0,k)\) is some _prior_ Gaussian process, whose mean is assumed to be zero in order to simplify notation. The posterior distribution \(f\mid\bm{y}\) is then another Gaussian process \(f\mid\bm{y}\sim\operatorname{GP}(\hat{m},\hat{k})\) with (Rasmussen and Williams, 2006)

\[\hat{m}(\cdot)=\mathbf{K}_{(\cdot)\mathbf{X}}\big{(}\mathbf{K}_{\mathbf{X} \mathbf{X}}+\sigma_{\varepsilon}^{2}\mathbf{I}\big{)}^{-1}\bm{y},\ \ \ \ \ \hat{k}(\cdot,\cdot^{\prime})=\mathbf{K}_{(\cdot,\cdot^{\prime})}-\mathbf{K}_{( \cdot)\mathbf{X}}\big{(}\mathbf{K}_{\mathbf{X}\mathbf{X}}+\sigma_{ \varepsilon}^{2}\mathbf{I}\big{)}^{-1}\mathbf{K}_{\mathbf{X}(\cdot^{\prime})},\] (1)

where the matrix \(\mathbf{K}_{\mathbf{X}\mathbf{X}}\) has entries \(k(\bm{x}_{i},\bm{x}_{j})\), the vector \(\mathbf{K}_{\mathbf{X}(\cdot)}=\mathbf{K}_{(\cdot)\mathbf{X}}^{\top}\) has components \(k(\bm{x}_{i},\cdot)\). If needed, one can efficiently sample \(f\mid\bm{y}\) using _pathwise conditioning_(Wilson et al., 2020; 2021)Matern Gaussian processes--including the limiting \(\nu\to\infty\) case, squared exponential Gaussian processes--are the most popular family of models for \(\mathcal{X}=\mathbb{R}^{d}\). These have zero mean and kernels

\[k_{\nu,\kappa,\sigma^{2}}(\bm{x},\bm{x}^{\prime})=\sigma^{2}\frac{2^{1-\nu}}{ \Gamma(\nu)}\bigg{(}\sqrt{2\nu}\frac{\|\bm{x}-\bm{x}^{\prime}\|}{\kappa}\bigg{)} ^{\nu}K_{\nu}\bigg{(}\sqrt{2\nu}\frac{\|\bm{x}-\bm{x}^{\prime}\|}{\kappa} \bigg{)}\] (2)

where \(K_{\nu}\) is the modified Bessel function of the second kind (Gradshteyn and Ryzhik, 2014) and \(\nu,\kappa,\sigma^{2}\) are the hyperparameters responsible for smoothness, length scale and variance, respectively. We proceed to describe how Matern processes can be generalized to inputs \(\bm{x}\) lying on _explicitly given_ manifolds or graphs instead of the Euclidean space \(\mathbb{R}^{d}\).

### Matern Gaussian Processes on Explicit Manifolds and Graphs

For a domain which is a Riemannian manifold, an obvious and natural idea for generalizing Matern Gaussian processes could be to substitute the Euclidean distances \(\|x-x^{\prime}\|\) in Equation (2) with the geodesic distance. However, this approach results in ill-defined kernels that fail to be positive semi-definite (Feragen et al., 2015; Gneiting, 2013).

Another direction for generalization is based on the stochastic partial differential equation (SPDE) characterization of Matern processes first described by Whittle (1963): \(f\sim\mathrm{GP}(0,k_{\nu,\kappa,\sigma^{2}})\) solves

\[\bigg{(}\frac{2\nu}{\kappa^{2}}-\Delta_{\mathbb{R}^{d}}\bigg{)}^{\frac{\nu}{ 2}+\frac{d}{4}}f=\mathcal{W},\] (3)

where \(\Delta_{\mathbb{R}^{d}}\) is the standard Laplacian operator and \(\mathcal{W}\) is the Gaussian white noise with variance proportional to \(\sigma^{2}\). If taken to be the definition, this characterization can be easily extended to general Riemannian manifolds \(\mathcal{X}=\mathcal{M}\) by substituting \(\Delta_{\mathbb{R}^{d}}\) with the Laplace-Beltrami operator \(\Delta_{\mathcal{M}}\), taking \(d=\dim\mathcal{M}\) and substituting \(\mathcal{W}\) with the appropriate generalization of the Gaussian white noise (Lindgren et al., 2011). Based on this idea, Borovitskiy et al. (2020) showed that on _compact_ Riemannian manifolds, Matern Gaussian processes are the zero-mean processes with kernels

\[k_{\nu,\kappa,\sigma^{2}}(x,x^{\prime})=\frac{\sigma^{2}}{C_{\nu,\kappa}}\sum _{l=0}^{\infty}\bigg{(}\frac{2\nu}{\kappa^{2}}+\lambda_{l}\bigg{)}^{-\nu-d/2 }f_{l}(x)f_{l}(x^{\prime}),\] (4)

where \(-\lambda_{l},f_{l}\) are eigenvalues and eigenfunctions of the Laplace-Beltrami operator and \(C_{\nu,\kappa}\) is the normalizing constant ensuring that \(\frac{1}{\mathcal{X}}\int_{\mathcal{X}}k_{\nu,\kappa,\sigma^{2}}(x,x)\, \mathrm{d}x=\sigma^{2}\). This, alongside with considerations from Azangulov et al. (2022) allows one to practically compute \(k_{\nu,\kappa,\sigma^{2}}\) for many compact manifolds.

If the domain \(\mathcal{X}\) is a weighted undirected graph \(\mathcal{G}\), we can also use Equation (3) to define Matern Gaussian processes on \(\mathcal{G}\)(Borovitskiy et al., 2021). In this case, \(\Delta_{\mathbb{R}^{d}}\) is substituted with the minus graph Laplacian \(-\Delta_{\mathcal{G}}\) and \(\mathcal{W}\sim\mathrm{N}(0,\sigma_{\mathcal{W}}^{2}\mathbf{I})\) is the vector of IID Gaussians. Here, SPDE transforms into a stochastic linear system, whose solution is of the same form as Equation (4) but with a finite sum instead of the infinite series, with \(d=0\) because there is no canonical notion of dimension for graphs and with \(\lambda_{l},f_{l}\) being the eigenvalues and eigenvectors--as functions on the node set--of the matrix \(\Delta_{\mathcal{G}}\). These processes are illustrated on Figure 2.

Figure 2: Kernel values \(k(\cdot,\cdot)\) and samples for the Matern-\(\nicefrac{{3}}{{2}}\) Gaussian processes on the sphere manifold \(\mathbb{S}_{2}\) and for the approximating Matern-\(\nicefrac{{5}}{{2}}\) process on a geodesic polyhedron graph \(\mathrm{P}\,\mathbb{S}_{2}\).

## 3 Implicit Manifolds and Gaussian Processes on Them

Consider a dataset \(\mathbf{X}=(\bm{x}_{1},..,\bm{x}_{N})^{\top}\), \(\bm{x}_{i}\in\mathbb{R}^{d}\) partially labeled with labels \(y_{1},..,y_{n}\in\mathbb{R}\), \(n\leq N\). Assume that \(\bm{x}_{i}\) are IID randomly sampled from a compact Riemannian submanifold \(\mathcal{M}\subseteq\mathbb{R}^{d}\). As by Section 2.1, the manifold \(\mathcal{M}\) is associated to a family of Matern Gaussian processes tailored to its geometry. We do not assume to know \(\mathcal{M}\), only the fact that it exists, hence the question is: how can we recover the kernels of the aforementioned geometry-aware processes from the observed dataset?

It is clear from Equation (4) that to recover \(k_{\nu,\kappa,\sigma^{2}}\) we need to get the eigenpairs \(-\lambda_{l},f_{l}\) of the Laplace-Beltrami operator on \(\mathcal{M}\). Naturally, for a finite dataset this can only be done approximately. We proceed to discuss the relevant theory of Laplace-Beltrami eigenpair approximation.

### Background on Approximating the Eigenpairs of the Laplace-Beltrami Operator

There exists a number of theoretical and empirical results on eigenpair approximation. Virtually all of them study approximating the implicit manifold by some kind of a weighted undirected graph1 with node set \(\{\bm{x}_{1},..,\bm{x}_{N}\}\) and weights that are somehow determined by the Euclidean distances \(\|\bm{x}_{i}-\bm{x}_{j}\|\). The eigenvalues of the _graph Laplacian_ on this graph are supposed to approximate the eigenvalues of the Laplace-Beltrami operator, while the eigenvectors--regarded as functions on the node set--approximate the values of the eigenfunctions of the Laplace-Beltrami operator at \(\bm{x}_{i}\in\mathcal{M}\). To approximate eigenfunctions elsewhere, any sort of continuous (smooth) interpolation suffices.

Footnote 1: Other possibilities include linear (classical PCA) or quadratic (Pavutnitskiy et al., 2022) approximations. See the books by Lee and Verleysen (2007) and Ma and Fu (2011) for additional context.

There are three popular notions of graph Laplacian. Let us denote the adjacency matrix of the weighted graph by \(\mathbf{A}\) and define \(\mathbf{D}\) to be the diagonal degree matrix with \(\mathbf{D}_{ii}=\sum_{j}\mathbf{A}_{ij}\). Then

\[\underbrace{\mathbf{\Delta}_{\mathrm{un}}=\mathbf{D}-\mathbf{A}}_{\text{ unnormalized}},\qquad\quad\underbrace{\mathbf{\Delta}_{\mathrm{sym}}= \mathbf{I}-\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}}_{\text{symmetric normalized}},\qquad\underbrace{\mathbf{\Delta}_{\mathrm{rw}}= \mathbf{I}-\mathbf{D}^{-1}\mathbf{A}}_{\text{random walk normalized}}.\] (5)

The first two of these are symmetric positive semi-definite matrices, the third is, generally speaking, non-symmetric. However, from the point of view of linear operators, all of them can be considered symmetric (self-adjoint) positive semi-definite: the first two with respect to the standard Euclidean inner product \(\left\langle\cdot,\cdot\right\rangle\), and the third one with respect to the modified inner product \(\left\langle\bm{v},\bm{u}\right\rangle_{\mathbf{D}}=\left\langle\mathbf{D} \bm{v},\bm{u}\right\rangle\). Thus for each there exists an orthonormal basis of eigenvectors and eigenvalues are non-negative.2

Footnote 2: Naturally, for the random walk normalized Laplacian \(\Delta_{\mathrm{rw}}\) the orthonormality is with respect to \(\left\langle\cdot,\cdot\right\rangle_{\mathbf{D}}\).

The most common way to define the graph is by setting \(\mathbf{A}_{ij}=\exp\bigl{(}-\|\bm{x}_{i}-\bm{x}_{j}\|^{2}/4\alpha^{2}\bigr{)}\) for an \(\alpha>0\). If \(\bm{x}_{i}\) are IID samples from the _uniform_ distribution on the manifold \(\mathcal{M}\), then all of the graph Laplacians, each multiplied by an appropriate power of \(\alpha\), converge to the Laplace-Beltrami operator, both pointwise (Hein et al., 2007) and _spectrally_(Garcia Trillos et al., 2020), i.e. in the sense of eigenpair convergence, at least at the node set of the graph.3 However, if the inputs \(\bm{x}_{i}\) are sampled non-uniformly, graph Laplacians, at best, converge to different continuous limits, none of which coincides with the Laplace-Beltrami operator (Hein et al., 2007).

Footnote 3: Garcia Trillos et al. (2020) do not explicitly study \(\mathbf{\Delta}_{\mathrm{sym}}\). Since \(\mathbf{\Delta}_{\mathrm{sym}}\) and \(\mathbf{\Delta}_{\mathrm{rw}}\) are _similar_ matrices, they share eigenvalues, so eigenvalue convergence for \(\mathbf{\Delta}_{\mathrm{sym}}\) is trivial, the eigenvector convergence, however, is not.

Coifman and Lafon (2006) proposed a clever trick to handle non-uniformly sampled data \(\bm{x}_{1},..,\bm{x}_{N}\). Starting with \(\tilde{\mathbf{A}}\) and \(\tilde{\mathbf{D}}\) defined in the same way as \(\mathbf{A}\) and \(\mathbf{D}\) before, they define \(\mathbf{A}=\tilde{\mathbf{D}}^{-1}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-1}\). Intuitively, this corresponds to normalizing by the kernel density estimator to cancel out the unknown density. The corresponding \(\mathbf{\Delta}_{\mathrm{rw}}\) then converges pointwise to the Laplace-Beltrami operator (Hein et al., 2007), though \(\mathbf{\Delta}_{\mathrm{un}}\) and \(\mathbf{\Delta}_{\mathrm{sym}}\) do not: they converge to different continuous limits. Dunson et al. (2021, Theorem 2) show that, under technical regularity assumptions, eigenvalues \(\lambda_{k}\) and renormalized eigenvectors of \(\mathbf{\Delta}_{\mathrm{rw}}\) converge to the respective eigenvalues and eigenfunctions of the Laplace-Beltrami operator, regardless of the sampling density of \(\bm{x}_{i}\).

Both in the simple case and in the sampling density independent case, the graphs and their respective Laplacians turn out to be dense, requiring a lot of memory to store and being inefficient to operate with. To make computations efficient, sparse graphs such as KNN graphs are much more preferable over the dense graphs. Spectral convergence for KNN graphs is studied, for example, in Calder and Trillos (2022), for \(\mathbf{A}_{ij}=h(\|\bm{x}_{i}-\bm{x}_{j}\|/\alpha)\) with a compactly supported regular function \(h\), and withlimit depending on the sampling density. Unfortunately, we are unaware of any spectral convergence results in the literature that hold for KNN graphs and are independent of the data sampling density.

### Approximating Matern Kernels on Manifolds

Here we incorporate various convergence results, including but not limited to the ones described in Section 3.1, proving that all spectral convergence results imply the convergence of graph Matern kernels to the respective manifold Matern kernels.

**Proposition 1**.: _Denote the eigenpairs by \(\lambda_{l},f_{l}\) for a graph Laplacian and by \(\lambda_{l}^{\mathcal{M}},f_{l}^{\mathcal{M}}\) for the Laplace-Beltrami operator. Fix \(\delta>0\). Assume that, with probability at least \(1-\delta\), for all \(\varepsilon>0\), for \(\alpha\) small enough and for \(N\) large enough we have \(|\lambda_{l}-\lambda_{l}^{\mathcal{M}}|<\varepsilon\) and \(|f_{l}(\bm{x}_{i})-f_{l}^{\mathcal{M}}(\bm{x}_{i})|<\varepsilon\). Then, with probability at least \(1-\delta\), we have \(k_{\nu,\kappa,\sigma^{2}}^{N,\alpha,L}(\bm{x}_{i},\bm{x}_{j})\to k_{\nu, \kappa,\sigma^{2}}(\bm{x}_{i},\bm{x}_{j})\) as \(\alpha\to 0,\,N,L\to\infty\), where_

\[k_{\nu,\kappa,\sigma^{2}}^{N,\alpha,L}(\bm{x}_{i},\bm{x}_{j})=\frac{\sigma^{2 }}{C_{\nu,\kappa}}\sum_{l=0}^{L-1}\biggl{(}\frac{2\nu}{\kappa^{2}}+\lambda_{l} \biggr{)}^{-\nu-\dim(\mathcal{M})/2}f_{l}(\bm{x}_{i})f_{l}(\bm{x}_{j}).\] (6)

Proof.: First prove that the tail of the series in Equation (4) converges uniformly to zero, then combine this with eigenpair bounds. See details in Appendix A. 

**Remark**.: The convergence in \(\bm{x}_{i}\in\mathcal{M}\) can be lifted to pointwise convergence for all \(\bm{x}\in\mathcal{M}\) if eigenvectors are interpolated Lipschitz-continuously, simply because the eigenfunctions are smooth.

Inspired by this theory, we proceed to present the implicit manifold Gaussian process model.

### Implicit Manifold Gaussian Process

Guided by the theory we described in the previous section we are now ready to formulate the implicit manifold Gaussian process model. Given the dataset \(\bm{x}_{1},..,\bm{x}_{N}\in\mathbb{R}^{d}\) and \(y_{1},..,y_{n}\in\mathbb{R}\), we put

\[\mathbf{A}=\tilde{\mathbf{D}}^{-1}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-1}, \quad\tilde{\mathbf{A}}_{ij}=S_{K}(\bm{x}_{i},\bm{x}_{j})\exp\biggl{(}-\frac{ \|\bm{x}_{i}-\bm{x}_{j}\|^{2}}{4\alpha^{2}}\biggr{)},\quad\tilde{\mathbf{D}}_{ ij}=\begin{cases}\sum_{m}\tilde{\mathbf{A}}_{im}&i=j,\\ 0&i\neq j.\end{cases}\] (7)

Here \(S_{K}(\bm{x}_{i},\bm{x}_{j})=1\) if \(\bm{x}_{i}\) is one of the \(K\) nearest neighbors of \(\bm{x}_{j}\) or vice versa and \(S_{K}(\bm{x}_{i},\bm{x}_{j})=0\) otherwise; all matrices are of size \(N\times N\) and depend on \(\alpha\) and \(K\) as hyperparameters. Thanks to the coefficient \(S_{K}(\bm{x}_{i},\bm{x}_{j})\) that performs KNN sparsification, the matrix \(\mathbf{A}\) is sparse when \(K\ll N\).4

Footnote 4: Note: \(\tilde{\mathbf{A}}_{ii}=1\), as if the graph has loops. Assuming \(\tilde{\mathbf{A}}_{ii}=0\) would lead to discontinuities at later stages.

Then we consider the operator \(\bm{\Delta}_{\text{rw}}=\mathbf{I}-\mathbf{D}^{-1}\mathbf{A}\) defined by Equation (5), whose matrix is also sparse. Denoting its eigenvalues--ordered from the smallest to the largest--by \(0=\lambda_{0}\leq\lambda_{1}\leq\ldots\leq\lambda_{N-1}\), and its eigenvectors--orthonormal under the modified inner product \(\left\langle\cdot,\cdot\right\rangle_{D}\) and regarded as functions on the node set of the graph--by \(f_{0},f_{1},\ldots,f_{N-1}\), we define Matern kernel on graph nodes \(\bm{x}_{i}\) by

\[k_{\nu,\kappa,\sigma^{2}}^{\mathbf{X}}(\bm{x}_{i},\bm{x}_{j})=\frac{\sigma^{2 }}{C_{\nu,\kappa}}\sum_{l=0}^{L-1}\Phi_{\nu,\kappa}(\lambda_{l})f_{l}(\bm{x}_{ i})f_{l}(\bm{x}_{j}),\qquad\quad\Phi_{\nu,\kappa}(\lambda)=\biggl{(}\frac{2\nu}{ \kappa^{2}}+\lambda\biggr{)}^{-\nu},\] (8)

where \(L\) does not need to be equal to the actual number \(N\) of eigenpairs. Doing so means truncating the high frequency eigenvectors (\(f_{l}\) for \(l\) large), which always contribute less to the sum because they correspond to smaller values of \(\Phi_{\nu,\kappa}(\lambda_{l})\). This can massively reduce the computational costs.

By Proposition 1, Equation (8) approximates the manifold Matern kernel with smoothness \(\nu^{\prime}=\nu-\dim(\mathcal{M})/2\). We adopt such a reparametrization because it does not require estimating the a priori unknown \(\dim(\mathcal{M})\). This, however, makes the typical assumption of \(\nu\in\{\nicefrac{{1}}{{2}},\nicefrac{{3}}{{2}},\nicefrac{{5}}{{2}}\}\) inadequate. We chose a particular graph Laplacian normalization, namely the random walk normalized graph Laplacian \(\bm{\Delta}_{\text{rw}}\), to approximate the true Laplace-Beltrami operator regardless of the potential non-uniform sampling of \(\bm{x}_{1},..,\bm{x}_{N}\), based on the theoretical insights described in Section 3.1.

The kernel in Equation (8) is only defined on the set of nodes \(\bm{x}_{i}\), next step is to extend it to the whole space \(\mathbb{R}^{d}\). Extending kernels is usually a difficult problem because one has to worry about positive 

[MISSING_PAGE_EMPTY:6]

After hyperparameters are found--we will return to their search later--we need to compute the eigenpairs \(\lambda_{l},\bm{f}_{l}\) of \(\bm{\Delta}_{\text{rw}}\). For this we run Lanczos algorithm (Meurant, 2006) to evaluate the eigenpairs \(\lambda_{l}^{\text{sym}},\bm{f}_{l}^{\text{sym}}\) of the symmetric matrix \(\bm{\Delta}_{\text{sym}}\), putting \(\lambda_{l}=\lambda_{l}^{\text{sym}}\) and \(\bm{f}_{l}=\mathbf{D}^{-1/2}\bm{f}_{l}^{\text{sym}}\) because the matrices \(\bm{\Delta}_{\text{rw}}\) and \(\bm{\Delta}_{\text{sym}}\) are similar, i.e. \(\bm{\Delta}_{\text{rw}}=\mathbf{D}^{-1/2}\bm{\Delta}_{\text{sym}}\mathbf{D}^{1 /2}\). Importantly, Lanczos algorithm only relies on matrix-vector products with \(\bm{\Delta}_{\text{sym}}\). We only compute a few hundred of eigenpairs, asking Lanczos to provide twice or trice as many and disregarding the rest.

When there is a lot of labeled data, we approximate the classical Euclidean (e.g. squared exponential) kernel using random Fourier features approximation (Rahimi and Recht, 2007), this allows linear computational complexity scaling with respect to the number of data points. The number of Fourier features is taken to be equal to \(L\), with the same \(L\) as in Equation (8).

As it was already mentioned, we need to find hyperparameters \(\hat{\bm{\theta}}=\left(\hat{\alpha},\hat{\kappa},\hat{\sigma}^{2},\hat{ \sigma}_{\varepsilon}^{2}\right)\) that determine the graph, Gaussian process prior and the noise variance that fit the observations \(\bm{y}\) best. The \(\nu\) parameter we assume manually fixed. To avoid nonsensical parameter values--a common difficulty often occurring when the data is scarce--one might want to assume a prior \(p(\bm{\theta})\) on \(\bm{\theta}\). Some specific choices of \(p(\bm{\theta})\) are discussed in Appendix C. Then \(\hat{\bm{\theta}}\) is a maximum a posteriori (MAP) estimate:

\[\hat{\bm{\theta}}=\arg\max_{\bm{\theta}}\log p(\bm{y}\mid\bm{\theta},\mathbf{ X})+\log p(\bm{\theta}).\] (13)

To simplify hyperparameter initialization and align with zero prior mean assumption it makes sense to preprocess \(y_{i}\) to be centered and normalized.

To solve the optimization problem in Equation (13) we use restarted gradient descent. Repeatedly evaluating the gradient of \(\log p(\bm{y}\mid\bm{\theta},\mathbf{X})\) is the main computational bottleneck. The key idea for doing this efficiently--viable for integer values of \(\nu\)--is to reduce matrix-vector products with Matern kernels' precision to iterated matrix-vector products with the Laplacian, which is _sparse_. First, we describe this in detail in the noiseless supervised setting, where the idea is most directly applicable.

### Noiseless Supervised Learning

Here we assume that all inputs are labeled, i.e. \(N=n\), and all observations are noiseless, i.e. \(\sigma_{\varepsilon}^{2}=0\).

Denoting by \(\mathbf{P}_{\mathbf{X}\mathbf{X}}=\mathbf{K}_{\mathbf{X}\mathbf{X}}^{-1}\) the precision matrix, the log-likelihood \(\log p(\bm{y}\mid\bm{\theta},\mathbf{X})\), up to a multiplicative constant and an additive constant irrelevant for optimization, is given by

\[\mathcal{L}(\bm{\theta})=-\log\det(\mathbf{K}_{\mathbf{X}\mathbf{X}})-\bm{y} ^{\top}\mathbf{K}_{\mathbf{X}\mathbf{X}}^{-1}\bm{y}=\log\det(\mathbf{P}_{ \mathbf{X}\mathbf{X}})-\bm{y}^{\top}\mathbf{P}_{\mathbf{X}\mathbf{X}}\bm{y}.\] (14)

Its gradient may be given and then subsequently approximated (Gardner et al., 2018) by

\[\frac{\partial\mathcal{L}(\bm{\theta})}{\partial\bm{\theta}}=\operatorname{tr} \biggl{(}\mathbf{P}_{\mathbf{X}\mathbf{X}}^{-1}\frac{\partial\mathbf{P}_{ \mathbf{X}\mathbf{X}}}{\partial\bm{\theta}}\biggr{)}-\bm{y}^{\top}\frac{ \partial\mathbf{P}_{\mathbf{X}\mathbf{X}}}{\partial\bm{\theta}}\bm{y}\approx \bm{z}^{\top}\mathbf{P}_{\mathbf{X}\mathbf{X}}^{-1}\frac{\partial\mathbf{P}_{ \mathbf{X}\mathbf{X}}}{\partial\bm{\theta}}\bm{z}-\bm{y}^{\top}\frac{\partial \mathbf{P}_{\mathbf{X}\mathbf{X}}}{\partial\bm{\theta}}\bm{y},\] (15)

where \(\bm{z}\) is a random vector consisting of IID variables that are either \(1\) or \(-1\) with probability \(1/2\). The first term on the right-hand side is the stochastic estimate of the trace of Hutchinson (1989). Since the kernels from Section 3.3 coincide with graph Matern kernels on the nodes \(x_{i}\), we have

\[\mathbf{K}_{\mathbf{X}\mathbf{X}}=\frac{\sigma^{2}}{C_{\nu,\kappa}}\sum_{l=0}^ {L-1}\Phi_{\nu,\kappa}(\lambda_{l})\bm{f}_{l}\bm{f}_{l}^{\top},\hskip 28.452756pt\bm{ \Delta}_{\text{rw}}\bm{f}_{l}=\lambda_{l}\bm{f}_{l},\hskip 28.452756pt\bm{f}_{l}^{ \top}\mathbf{D}\bm{f}_{m}=\delta_{lm}.\] (16)

However, as the graph bandwidth \(\alpha\) is one of the hyperparameters we optimize over, using Equation (16) would entail repeated eigenpair computations and differentiating through this procedure. Because of this, _we use an alternative way_ to compute matrix-vector products \(\mathbf{P}_{\mathbf{X}\mathbf{X}}\bm{u}\) detailed below.6

Footnote 6: Though automatic differentiability could in principle work for iterative methods like the Lanczos algorithm, the amount of memory required for storing the gradients of the intermediate steps quickly becomes prohibitive.

**Proposition 2**.: _Assuming \(\nu\in\mathbb{N}\), the precision matrix \(\mathbf{P}_{\mathbf{X}\mathbf{X}}\) of \(k_{\nu,\kappa,\sigma^{2}}^{\mathbf{X}}(\bm{x}_{i},\bm{x}_{j})\) can be given by_

\[\mathbf{P}_{\mathbf{X}\mathbf{X}}=\frac{\sigma^{-2}}{C_{\nu,\kappa}^{\top}} \mathbf{D}\underbrace{\left(\frac{2\nu}{\kappa^{2}}\mathbf{I}+\bm{\Delta}_{ \text{rw}}\right)\cdot\ldots\cdot\left(\frac{2\nu}{\kappa^{2}}\mathbf{I}+\bm{ \Delta}_{\text{rw}}\right)}_{\nu\text{ times}}.\] (17)

Proof.: See Appendix A. 

Using Proposition 2 to evaluate matrix-vector products \(\mathbf{P}_{\mathbf{XX}}\bm{u}\) and conjugate gradients (Meurant, 2006) to solve \(\bm{z}^{\top}\mathbf{P}_{\mathbf{XX}}^{-1}\) using only the matrix-vector products, we can efficiently evaluate the right-hand side of Equation (15), with linear costs with respect to \(N\), assuming that the graph is sparse. Preconditioning (Wenger et al., 2022) can be used to further improve the efficiency of the solve.

### Noiseless Semi-Supervised Learning

Here we assume that inputs are partly unlabeled, i.e. \(N\neq n\), while observations are still noiseless, i.e. \(\sigma_{\varepsilon}^{2}=0\). Denote \(\mathbf{Z}\) to be the labeled part of \(\mathbf{X}\). Then the matrix \(\mathbf{K}_{\mathbf{XX}}\) in the log-likelihood given by Equation (14) should be substituted with \(\mathbf{K}_{\mathbf{ZZ}}\). However, while \(\mathbf{P}_{\mathbf{XX}}\) can be represented using Equation (17), the precision \(\mathbf{P}_{\mathbf{ZZ}}=\mathbf{K}_{\mathbf{ZZ}}^{-1}\) cannot. We thus compute it as the Schur complement:

\[\mathbf{P}_{\mathbf{ZZ}}=\mathbf{A}-\mathbf{BD}^{-1}\mathbf{C}, \mathbf{P}_{\mathbf{XX}}=\begin{pmatrix}\mathbf{A}&\mathbf{B}\\ \mathbf{C}&\mathbf{D}\end{pmatrix},\] (18)

where partitioning of \(\mathbf{P}_{\mathbf{XX}}\) corresponds to partitioning \(\mathbf{X}\) into \(\mathbf{Z}\) and the rest. Evaluating a matrix-vector product \(\mathbf{P}_{\mathbf{ZZ}}\bm{u}\) requires a solve of \(\mathbf{D}^{-1}(\mathbf{C}\bm{u})\). This solve can also be performed using conjugate gradients, keeping the computational complexity linear in \(N\) but increasing the constants.

### Handling Noisy Observations

Finally, we assume noisy observations, i.e. \(\sigma_{\varepsilon}^{2}>0\). The inputs can be partially unlabeled, i.e. \(N\neq n\). In this case, matrix \(\mathbf{K}_{\mathbf{XX}}\) in the log-likelihood given by Equation (14) should be substituted with \(\mathbf{K}_{\mathbf{ZZ}}+\sigma_{\varepsilon}^{2}\mathbf{I}\). To reduce this to the previously considered cases, we use the Taylor expansion

\[(\mathbf{K}_{\mathbf{ZZ}}+\sigma_{\varepsilon}^{2}\mathbf{I})^{-1}\approx \mathbf{K}_{\mathbf{ZZ}}^{-1}-\sigma_{\varepsilon}^{2}\mathbf{K}_{\mathbf{ZZ }}^{-2}+\sigma_{\varepsilon}^{4}\mathbf{K}_{\mathbf{ZZ}}^{-3}-\ldots=\mathbf{ P}_{\mathbf{ZZ}}-\sigma_{\varepsilon}^{2}\mathbf{P}_{\mathbf{ZZ}}^{2}+ \sigma_{\varepsilon}^{4}\mathbf{P}_{\mathbf{ZZ}}^{3}-\ldots\] (19)

In practice, we only use the first two terms on the right-hand side as an approximation. This allows to retain linear computational complexity scaling with respect to \(N\) but increases the constants.

### Resulting Algorithm

Here we provide a concise summary of the _implicit manifold Gaussian process regression_ algorithm.

**Step 1: KNN-index.** Construct the KNN index on the points \(\bm{x}_{1},..,\bm{x}_{N}\). This allows linear time evaluation of any matrix-vector product with \(\tilde{\mathbf{A}}\), and thus also with \(\mathbf{A}\), \(\bm{\Delta}_{rw}\), \(\mathbf{P}_{\mathbf{XX}}\) for \(\nu\in\mathbb{N}\), etc.

**Step 2: hyperparameter optimization.** Find the hyperparameters \(\hat{\bm{\theta}}\) that solve Equation (13). Assuming \(\nu=\hat{\nu}\in\mathbb{N}\) is manually fixed, this relies only on matrix-vector products with \(\bm{\Delta}_{rw}\).

**Step 3: computing the eigenpairs.** Fixing the graph bandwidth \(\hat{\alpha}\) found on Step 2, compute the eigenpairs \(\lambda_{l},f_{l}\) corresponding to the \(L\) smallest eigenvalues \(\lambda_{l}\). For large \(N\), use Lanczos algorithm.

After the steps above are finished, Equations (8) and (11) define the geometric kernel \(k_{\hat{\nu},\hat{\kappa},\hat{\sigma}^{2}}^{\mathbf{X}}(\bm{x},\bm{x}^{ \prime})\) for arbitrary \(\bm{x},\bm{x}^{\prime}\in\mathbb{R}^{d}\). Then the respective prior \(\mathrm{GP}(0,k_{\hat{\nu},\hat{\kappa},\hat{\sigma}^{2}}^{\mathbf{X}})\) can be conditioned by the labeled data in the standard way, yielding the posterior \(f^{(m)}\sim\mathrm{GP}(m^{(m)},k^{(m)})\). To get sensible predictions far away from the data, the geometric model \(f^{(m)}\) is convexly combined with an independently trained classical Gaussian process model, as given by Equation (12). The resulting predictive model is still a Gaussian process, sum of two appropriately weighted independent Gaussian processes.

**Remark.** The number of neighbors \(K\), the number of eigenpairs \(L\) and the smoothness \(\nu\) are assumed to be manually fixed parameters. Higher values of \(K\) and \(L\) improve the quality of approximation of the manifold kernel, which is often linked to better predictive performance, but requires more computational resources. The parameter \(\nu\) can be picked using cross validation or prior knowledge. Small integer values of \(\nu\) reduce computational costs, but may be inadequate for higher dimensions of the assumed manifold due to the \(\nu^{\prime}=\nu-\mathrm{dim}(\mathcal{M})/2\) link with the manifold kernel smoothness \(\nu^{\prime}\).

## 5 Experiments

We start in Section 5.1 by examining a simple synthetic example to gain intuition on how noise-sensitive the technique is. Then in Section 5.2 we consider real datasets, showing improvements in higher dimensions. More experiments, results, and additional discussion can be found in Appendix B 

### Synthetic Examples

We consider a one dimensional manifold resembling the shape of a _dumbbell_ which already appeared in Figures 1 and 3. The unknown function \(f_{*}\) is defined by fixing a point \(x^{*}\) in the top left part of the dumbbell, and computing \(\sin(\mathrm{d}(x^{*},\cdot))\) where \(\mathrm{d}(\cdot,\cdot)\) denotes the geodesic (intrinsic) distance between a pair of points on the manifold. This function is illustrated in Figure 3(a).

To measure performance we primarily rely on measuring negative log-likelihood (NLL) on the dense mesh of test locations. We do this because such metric is able to combine accuracy and calibration simultaneously. Additionally, we present the root mean square error (RMSE).

We investigate a semi-supervised setting where the number of unlabeled points is large (\(N-n=1546\)) and the number of labeled points is small (\(n=10\)). We contaminate the inputs with noise, putting \(\mathbf{X}=\mathbf{X}_{\mathrm{noiseless}}+\mathrm{N}(0,\sigma_{\mathbf{X}}^{2} \mathbf{I})\) and do the same with the outputs, putting \(\bm{y}=f_{*}(\mathbf{X})+\mathrm{N}(0,\sigma_{\bm{y}}^{2}\mathbf{I})\) for various values of \(\sigma_{\mathbf{X}},\sigma_{\bm{y}}>0\). Specifically, we consider \(\sigma_{\mathbf{X}}=\sigma_{\bm{y}}=\beta\in\{0,0.01,0.05\}\) to which we refer to as the noiseless setting, the low noise setting and the high noise setting, respectively.

The results for these are visualized in Figures 3(b) to 3(d) with performance metrics reported in Table 1. The implicit manifold Gaussian process regression is referred to as IMGP (we use \(\nu=1\)) and it is compared with the standard Euclidean Matern-\(\nicefrac{{5}}{{2}}\) Gaussian process. IMGP performs much better in the noiseless and the low noise settings. The high noise is enough to damage the calibration of IMGP, as it ties with the baseline model: NLL is slightly worse and RMSE is slightly better.

In Appendix B.1 we show how performance depends on the fraction \(n/N\) of labeled data points, the truncation level \(L\) and we discuss the choice of the \(K\) parameter in KNN. Additionally, in Appendix B.2 we consider noise-sensitivity for a 2D manifold.

### High Dimensional Datasets

For the high-dimensional setting, we considered predicting rotation angles for MNIST-based datasets. Additionally, we examined a high-dimensional dataset from the UCI ML Repository, CT slices.

#### 5.2.1 Setup

**Datasets.** We consider two MNIST-based datasets. The first one is created by extracting a single image per digit from the complete MNIST dataset. By randomly rotating these \(10\) images we obtained \(N=10000\) training samples and \(1000\) testing samples. We call it _Single Rotated MNIST (SR-MNIST)_. For the second dataset, we select \(100\) random samples from MNIST. By randomly rotating these, we generate \(N=100\,000\) training samples, most will be unlabeled, and \(10\,000\) testing samples. We call it _Multiple Rotated MNIST (MR-MNIST)_. The last dataset, _CT slices_, has dimensionality of \(d=385\), we split it to have \(N=24075\) training samples and \(24075\) testing samples. Dataset names can be complemented by the fraction of labeled samples, e.g. MR-MNIST-10% refers to \(n=10\%N\).

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & \multicolumn{3}{c}{**RMSE**} & \multicolumn{3}{c}{**NLL**} \\ \cline{2-7}  & \(\beta=0\) & \(\beta=0.01\) & \(\beta=0.05\) & \(\beta=0\) & \(\beta=0.01\) & \(\beta=0.05\) \\ \hline Euclidean Matern-\(\nicefrac{{\gamma}}{{2}}\) & \(0.98\) & \(0.99\pm 0.02\) & \(1.02\pm 0.03\) & \(-2.17\) & \(-2.09\pm 0.03\) & \(\mathbf{-1.91\pm 0.10}\) \\ IMGP & \(\mathbf{0.33}\) & \(\mathbf{0.34\pm 0.02}\) & \(\mathbf{1.00\pm 0.02}\) & \(\mathbf{-5.02}\) & \(\mathbf{-4.19\pm 0.1}\) & \(-1.91\pm 1.88\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance metrics for the dumbbell manifold with varying magnitude of noise \(\beta\).

Figure 4: The ground truth function on the dumbbell manifold and the predictions of the implicit manifold Gaussian process regression (IMGP) under different levels of noise.

**Methods.** We consider implicit manifold Gaussian processes in the supervised regime (_S-IMGP_) and in the semi-supervised regime (_SS-IMGP_). We compare them to the GPTorch implementation of the Euclidean Matern-\(5/2\) Gaussian Process. We refer to it as the _Euclidean Gaussian Process (EGP)_.

**Additional details.** We run 100 iterations of hyperparameter optimization using Adam with a fixed learning rate of \(0.01\). For MNIST, with use IMGP with \(\nu=2\); for CT slices--with \(\nu=3\).

#### 5.2.2 Results

Table 2 shows the negative log-likelihood metric for different datasets and methods on the test set. The respective RMSEs are presented in Appendix B.3. On SR-MNIST, IMGP outperforms EGP in both supervised and semi-supervised scenarios. MR-MNIST is more challenging. In the supervised setting for \(n=1\%N\), S-IMGP is incapable of inferring the underlying manifold structure, performing worse than EGP. However, SS-IMGP, with more data to infer manifold from, performs best. For \(n=10\%N\), IMGP gets a better grip of the dataset's geometry, outperforming EGP in both regimes.

For CT slices, regardless of \(n\), both S-IMGP and SS-IMGP performed poorly. Looking for an explanation, we considered two modifications. First, we fixed the graph bandwidth \(\hat{\alpha}\) found in the algorithm's Step 2 (cf. Section 4.4), and re-optimized the other hyperparameters \(\kappa,\sigma^{2},\sigma_{e}^{2}\) by maximizing the likelihood of the eigenpair-based model (truncated to \(L=2000\) eigenpairs) computed in the algorithm's Step 3. This resulted in limited improvement but did not change the big picture--in fact, values for S-IMGP and SS-IMGP in Table 2 for CT slices already include this modification.

Second, on top of this hyperparameter re-optimization, we tried computing the eigenpairs using torch.linalg.eigh instead of the Lanczos implementation in GP PyTorch, taking the same number \(L=2000\) of eigenpairs. The resulting methods S-IMGP (full) and SS-IMGP (full) showed considerable improvement over the baseline, as shown in Table 2. This indicated an issue with the quality of eigenpairs derived from the Lanczos method which requires further investigation. We discuss this in Appendix B.3, together with the aforementioned hyperparameter re-optimization procedure.

## 6 Conclusion

In this work, we propose the _implicit manifold Gaussian process regression_ technique. It is able to use unlabeled data to improve predictions and uncertainty calibration by learning the implicit manifold upon which the data lies, being inspired by the convergence of graph Matern Gaussian processes to their manifold counterparts. This helps building better probabilistic models in higher dimensional settings where the standard Euclidean Gaussian processes usually struggle. This is supported by our experiments in a synthetic low-dimensional setting and for high-dimensional datasets. Leveraging sparse structure of graph Matern precision matrices and efficient approximate KNN, the technique is able to scale to large datasets of hundreds of thousands points, which is especially important in high dimension, where a large number of unlabeled points is often needed to learn the implicit manifold. The model is fully differentiable, making it possible to infer hyperparameters in the usual way.

Limitations.The quality of the constructed graph significantly influences the technique's performance. When dealing with data from complex manifolds or exhibiting highly non-uniform density, simplistic KNN strategies might fail to capture the manifold structure due to their reliance on a single graph bandwidth. In such scenarios, larger values of parameters \(K\) and \(L\), or in high dimensions, of parameter \(\nu\), may be beneficial but could substantially increase computational costs. Furthermore, larger datasets coupled with high parameter values can lead to numerical stability issues, for instance, in the Lanczos algorithm, calling for further improvements and research. Despite these challenges, our method shows promise for advancing probabilistic modeling in higher dimensions.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & \multicolumn{3}{c}{**MNIST**} & \multicolumn{3}{c}{**CT slices**} \\ \cline{2-7}
**Method** & **SR - 10\%** & **MR - 1\%** & **MR - 10\%** & **5\%** & **10\%** & **25\%** \\ \hline EGP & \(-0.54\pm 0.01\) & \(-0.20\pm 0.01\) & \(-0.43\pm 0.01\) & \(-0.80\pm 0.02\) & \(-0.96\pm 0.00\) & \(-1.20\pm 0.09\) \\ S-IMGP & \(-1.42\pm 0.01\) & \(2.24\pm 0.20\) & \(-0.68\pm 0.08\) & \(0.47\pm 0.06\) & \(-0.59\pm 0.08\) & \(-0.08\pm 0.01\) \\ SS-IMGP & \(-\mathbf{1.52\pm 0.01}\) & \(\mathbf{-0.59\pm 0.01}\) & \(\mathbf{-0.79\pm 0.00}\) & \(26.1\pm 12.7\) & \(1.03\pm 0.09\) & \(-0.72\pm 0.68\) \\ S-IMGP (full) & - & - & - & \(0.64\pm 0.83\) & \(0.88\pm 0.29\) & \(-0.42\pm 0.10\) \\ SS-IMGP (full) & - & - & - & \(\mathbf{-2.48\pm 0.08}\) & \(\mathbf{-2.35\pm 0.04}\) & \(\mathbf{-1.99\pm 0.04}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Negative log likelihood on test samples for real datasets. For RMSE see Tables 4 and 5.

## Acknowledgements

We express our gratitude to Alexander Shulzhenko (St. Petersburg University, mentored by VB), whose initial work on a similar problem and accessible source material at https://github.com/AlexanderShulzhenko/Implicit-Manifold-Gaussian-Processes, while not included in the paper, sparked inspiration for this project. We thank Dr. Alexander Terenin (Cornell University) for generously making his Blender rendering scripts accessible to the public. These scripts, which aided us in creating Figure 2, can be found in his Ph.D. thesis repository at https://github.com/aterenin/phdthesis. BF and AB acknowledge support by the European Research Council (ERC), Advanced Grant agreement No 741945, Skill Acquisition in Humans and Robots. VB acknowledges support by an ETH Zurich Postdoctoral Fellowship.

## References

* A. A. Smolensky, A. Terenin, and V. Borovitskiy (2022)Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces I: the compact case. arXiv preprint arXiv:2208.14960. Cited by: SS1.
* I. A. A. Smolensky, A. Terenin, and V. Borovitskiy (2022)Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces II: non-compact symmetric spaces. arXiv preprint arXiv:2301.13088. Cited by: SS1.
* M. Binois and N. Wycoff (2022)A survey on high-dimensional Gaussian process modeling with application to Bayesian optimization. ACM Transactions on Evolutionary Learning and Optimization2 (2), pp. 1-26. Cited by: SS1.
* V. Borovitskiy, I. A. A. Terenin, P. Mostowsky, M. Deisenroth, and N. Durrande (2021)Matern Gaussian Processes on Graphs. In International Conference on Artificial Intelligence and Statistics, Cited by: SS1.
* V. Borovitskiy, M. R. Karimi, V. R. Somnath, and A. Krause (2023)Isotropic Gaussian Processes on Finite Spaces of Graphs. In International Conference on Artificial Intelligence and Statistics, Cited by: SS1.
* V. Borovitskiy, A. Terenin, P. Mostowsky, and M. P. Deisenroth (2020)Matern Gaussian Processes on Riemannian Manifolds. In Advances in Neural Information Processing Systems, Cited by: SS1.
* J. Calder and N. G. Trillos (2022)Improved spectral convergence rates for graph Laplacians on \(\varepsilon\)-graphs and k-NN graphs. Applied and Computational Harmonic Analysis60, pp. 123-175. Cited by: SS1.
* J. Chiles and P. Delfiner (2012)Geostatistics: modeling spatial uncertainty. John Wiley & Sons. Cited by: SS1.
* R. R. Coifman and S. Lafon (2006)Diffusion Maps. Applied and Computational Harmonic Analysis. Special Issue: Diffusion Maps and Wavelets21 (1), pp. 5-30. Cited by: SS1.
* E. De Vito, N. Mucke, and L. Rosasco (2021)Reproducing kernel Hilbert spaces on manifolds: Sobolev and diffusion spaces. Analysis and Applications19 (03), pp. 363-396. Cited by: SS1.
* M. Deisenroth and C. E. Rasmussen (2011)PILCO: a model-based and data-efficient approach to policy search. In International Conference on Machine Learning, Cited by: SS1.
* H. Donnelly (2006)Eigenfunctions of the Laplacian on compact Riemannian manifolds. Asian Journal of Mathematics10 (1), pp. 115-126. Cited by: SS1.
* D. B. Dunson, H. Wu, and N. Wu (2022)Graph based Gaussian processes on restricted domains. Journal of the Royal Statistical Society Series B: Statistical Methodology84 (2), pp. 414-439. Cited by: SS1.
* D. B. Dunson, H. Wu, and N. Wu (2021)Spectral convergence of graph Laplacian and heat kernel reconstruction in \(L^{\infty}\) from random samples. Applied and Computational Harmonic Analysis55, pp. 282-336. Cited by: SS1.

A. Feragen, F. Lauze, and S. Hauberg. Geodesic exponential kernels: When curvature and linearity conflict. In _Conference on Computer Vision and Pattern Recognition_, 2015. Cited on page 3.
* M. Garcia Trillos, M. Gerlach, M. Hein, and D. Slepcev (2020)Error estimates for spectral convergence of the graph Laplacian on random geometric graphs toward the Laplace-Beltrami operator. _Foundations of Computational Mathematics_, 20(4):827-887, 2020. Cited on page 4.
* J. Gardner, G. Pleiss, K. Q. Weinberger, D. Bindel, and A. G. Wilson. GPTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration. In _Advances in Neural Information Processing Systems_, 2018. Cited on pages 7.
* T. Gneiting (2013)Strictly and non-strictly positive definite functions on spheres. _Bernoulli_, 19(4):1327-1349, 2013. Cited on page 3.
* I. S. Gradshteyn and I. M. Ryzhik (2014)Table of integrals, series, and products. Academic Press, 7th edition. Cited on page 3.
* M. Hein, J. Audibert, and U. von Luxburg (2007)Graph Laplacians and their Convergence on Random Neighborhood Graphs. _Journal of Machine Learning Research_, 8(48):1325-1368, 2007. Cited on page 4.
* P. Hennig, M. A. Osborne, and M. Girolami (2015)Probabilistic numerics and uncertainty in computations. _Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 471(2179):20150142, 2015. Cited on page 1.
* M. F. Hutchinson (1989)A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines. _Communications in Statistics-Simulation and Computation_, 18(3):1059-1076, 1989. Cited on page 7.
* J. Johnson, M. Douze, and H. Jegou (2019)Billion-scale similarity search with GPUs. _IEEE Transactions on Big Data_, 7(3):535-547, 2019. Cited on page 6.
* R. I. Kondor and J. Lafferty (2002)Diffusion kernels on graphs and other discrete structures. In _International Conference on Machine Learning_, 2002. Cited on page 2.
* J. A. Lee and M. Verleysen (2007)Nonlinear dimensionality reduction. Springer Science & Business Media. Cited on page 4.
* F. Lindgren, H. Rue, and J. Lindstrom (2011)An Explicit Link between Gaussian Fields and Gaussian Markov Random Fields: The Stochastic Partial Differential Equation Approach. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 73(4):423-498, 2011. Cited on page 3.
* Y. Ma and Y. Fu (2011)Manifold learning theory and applications. CRC press. Cited on page 4.
* G. Meurant (2006)The Lanczos and conjugate gradient algorithms: from theory to finite precision computations. SIAM. Cited on pages 7.
* F. Pavutnitskiy, S. O. Ivanov, E. Abramov, V. Borovitskiy, A. Klochkov, V. Vyalov, A. Zaikovskii, and A. Petiushko (2022)Quadric hypersurface intersection for manifold learning in feature space. In International Conference on Artificial Intelligence and Statistics, Cited on page 4.
* R. L. Peach, M. Vinao-Carl, N. Grossman, M. David, E. Mallas, D. Sharp, P. A. Malhotra, P. Vandergheynst, and A. Gosztolai (2023)Implicit Gaussian process representation of vector fields over arbitrary latent manifolds. _arXiv preprint arXiv:2309.16746_, 2023. Cited on page 2.
* A. Rahimi and B. Recht (2007)Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems, Cited on page 7.
* C. E. Rasmussen and C. K. I. Williams (2006)Gaussian processes for machine learning. Adaptive Computation and Machine Learning. MIT Press. Cited on page 1.
* J. Wenger, G. Pleiss, P. Hennig, J. Cunningham, and J. Gardner (2022)Preconditioning for scalable Gaussian process hyperparameter optimization. In International Conference on Machine Learning, pp. 23751-23780. Cited on page 8.

P. Whittle. Stochastic Processes in Several Dimensions. _Bulletin of the International Statistical Institute_, 40:974-994, 1963. Cited on page 3.
* J. T. Wilson, V. Borovitskiy, A. Terenin, P. Mostowsky, and M. P. Deisenroth (2020)Efficiently Sampling Functions from Gaussian Process Posteriors. In International Conference on Machine Learning, Cited on page 2.
* J. T. Wilson, V. Borovitskiy, A. Terenin, P. Mostowsky, and M. P. Deisenroth (2021)Pathwise Conditioning of Gaussian Processes. Journal of Machine Learning Research22 (105), pp. 1-47. External Links: Document, Link Cited on page 2.

Theory

**Proposition 1**.: _Denote the eigenpairs by \(\lambda_{l},f_{l}\) for a graph Laplacian and by \(\lambda_{l}^{\mathcal{M}},f_{l}^{\mathcal{M}}\) for the Laplace-Beltrami operator. Fix \(\delta>0\). Assume that, with probability at least \(1-\delta\), for all \(\varepsilon>0\), for \(\alpha\) small enough and for \(N\) large enough we have \(|\lambda_{l}-\lambda_{l}^{\mathcal{M}}|<\varepsilon\) and \(|f_{l}(\bm{x}_{i})-f_{l}^{\mathcal{M}}(\bm{x}_{i})|<\varepsilon\). Then, with probability at least \(1-\delta\), we have \(k_{\nu,\kappa,\sigma^{2}}^{N,\alpha,l}(\bm{x}_{i},\bm{x}_{j})\to k_{\nu,\kappa, \sigma^{2}}(\bm{x}_{i},\bm{x}_{j})\) as \(\alpha\to 0,\,N,L\to\infty\), where_

\[k_{\nu,\kappa,\sigma^{2}}^{N,\alpha,l}(\bm{x}_{i},\bm{x}_{j})=\frac{\sigma^{2} }{C_{\nu,\kappa}}\sum_{l=0}^{L-1}\biggl{(}\frac{2\nu}{\kappa^{2}}+\lambda_{l} \biggr{)}^{-\nu-\dim(\mathcal{M})/2}f_{l}(\bm{x}_{i})f_{l}(\bm{x}_{j}).\] (6)

Proof.: Fix small \(\varepsilon>0\). We will prove that for \(\alpha\) small enough and \(N,L\) large enough we have \(|k_{\nu,\kappa,\sigma^{2}_{f}}(\bm{x}_{i},\bm{x}_{j})-k_{\nu,\kappa,\sigma^{ 2}_{f}}^{N,\alpha,l}(\bm{x}_{i},\bm{x}_{j})|<C\varepsilon\) for some \(C>0\) with probability at least \(1-\delta\). Since the assumption holds on the same event of probability \(1-\delta\) for all \(\varepsilon\), this directly translates to the convergence on the same event. In fact, a probabilistic narrative is nonessential for what we actually prove, and we do not use it below. To simplify notation, we replace \(\sum_{l=0}^{L-1}\) by \(\sum_{l=0}^{L}\).

First, for any \(L\in\mathbb{Z}_{>0}\) define the truncated version \(k_{\nu,\kappa,\sigma^{2}_{f}}^{L}\) of the manifold kernel \(k_{\nu,\kappa,\sigma^{2}_{f}}\) by

\[k_{\nu,\kappa,\sigma^{2}_{f}}^{L}(\bm{x},\bm{x}^{\prime})=\frac{\sigma^{2}_{f} }{C_{\nu,\kappa}}\sum_{l=0}^{L}\biggl{(}\frac{2\nu}{\kappa^{2}}+\lambda_{l}^{ \mathcal{M}}\biggr{)}^{-\nu-\dim(\mathcal{M})/2}f_{l}^{\mathcal{M}}(\bm{x})f_ {l}^{\mathcal{M}}(\bm{x}^{\prime}).\] (20)

The manifold Matern kernels are the reproducing kernels of Sobolev spaces, if the latter are defined appropriately (Borovitskiy et al., 2020). These are Mercer kernels (De Vito et al., 2021), hence, by Mercer's theorem, \(k_{\nu,\kappa,\sigma^{2}_{f}}^{L}\to k_{\nu,\kappa,\sigma^{2}_{f}}\) uniformly on \(\mathcal{M}\), i.e. for \(L\) large enough we have

\[\Bigl{|}k_{\nu,\kappa,\sigma^{2}_{f}}^{L}(\bm{x}_{i},\bm{x}_{j})-k_{\nu,\kappa,\sigma^{2}_{f}}(\bm{x}_{i},\bm{x}_{j})\Bigr{|}\leq\sup_{\bm{x},\bm{x}^{\prime }\in\mathcal{M}}\Bigl{|}k_{\nu,\kappa,\sigma^{2}_{f}}^{L}(\bm{x},\bm{x}^{ \prime})-k_{\nu,\kappa,\sigma^{2}_{f}}(\bm{x},\bm{x}^{\prime})\Bigr{|}<\varepsilon.\] (21)

Now suppose that \(\alpha\) is small enough and \(N\) is large enough so that

\[\bigl{|}\lambda_{l}-\lambda_{l}^{\mathcal{M}}\bigr{|}<\varepsilon^{\prime}, \quad|f_{l}(\bm{x}_{i})-f_{l}^{\mathcal{M}}(\bm{x}_{i})|<\varepsilon^{\prime}, \quad\varepsilon^{\prime}=\min\bigl{(}1,\bigl{(}\lambda_{L}^{\mathcal{M}} \bigr{)}^{-\frac{d-1}{4}},\bigl{(}\lambda_{L}^{\mathcal{M}}\bigr{)}^{-\frac{d- 1}{2}}\bigr{)}\frac{\varepsilon}{L}\] (22)

for all \(l\in\{1,..,L\}\) and for all \(i\in\{1,..,N\}\) with probability at least \(1-\delta\).

Assuming the manifold is connected, by Donnelly (2006) we have \(|f_{l}^{\mathcal{M}}|\leq C_{\lambda}\bigl{(}\lambda_{l}^{\mathcal{M}}\bigr{)} ^{\frac{d-1}{4}}\) for \(l>0\), where \(C_{\lambda}>0\) is a constant that depends on the geometry of the manifold. The case \(l=0\) is special because \(\lambda_{0}^{\mathcal{M}}=0\). Since \(f_{0}^{\mathcal{M}}\) is a constant function, we have

\[|f_{l}^{\mathcal{M}}|\leq C_{\lambda}\max((\lambda_{l}^{\mathcal{M}})^{\frac{d- 1}{4}},1)\] (23)

for all \(l\geq 0\), where \(C_{\lambda}\) here is potentially different from the \(C_{\lambda}\) before. Assuming, without loss of generality, \(\varepsilon<1\), we have

\[\bigl{|}f_{l}(\bm{x}_{i})f_{l}(\bm{x}_{j})-f_{l}^{\mathcal{M}}( \bm{x}_{i})f_{l}^{\mathcal{M}}(\bm{x}_{j})\bigr{|} \leq|f_{l}(\bm{x}_{i})|\bigl{|}f_{l}(\bm{x}_{j})-f_{l}^{\mathcal{M}}( \bm{x}_{j})\bigr{|}\] (24) \[\quad+\bigl{|}f_{l}^{\mathcal{M}}(\bm{x}_{j})\bigr{|}\bigl{|}f_{l }(\bm{x}_{i})-f_{l}^{\mathcal{M}}(\bm{x}_{i})\bigr{|}\] (25) \[\leq\varepsilon^{\prime}\cdot\bigl{(}\bigl{|}f_{l}(\bm{x}_{i})|+ \bigl{|}f_{l}^{\mathcal{M}}(\bm{x}_{j})\bigr{|}\bigr{)}\] (26) \[\leq\varepsilon^{\prime}\cdot\bigl{(}\bigl{|}f_{l}(\bm{x}_{i})-f_{l }^{\mathcal{M}}(\bm{x}_{i})\bigr{|}+\bigl{|}f_{l}^{\mathcal{M}}(\bm{x}_{i}) \bigr{|}+\bigl{|}f_{l}^{\mathcal{M}}(\bm{x}_{j})\bigr{|}\bigr{)}\] (27) \[\leq\varepsilon^{\prime}\cdot\bigl{(}\varepsilon^{\prime}+2C_{ \lambda}\max((\lambda_{l}^{\mathcal{M}})^{\frac{d-1}{4}},1)\bigr{)}\leq\frac{(1 +2C_{\lambda})\varepsilon}{L}.\] (28)

The function \(\Phi(\lambda)=\bigl{(}\frac{2\nu}{\kappa^{2}}+\lambda\bigr{)}^{-\nu-\dim( \mathcal{M})/2}\) is Lipschitz: \(|\Phi(\lambda)-\Phi(\lambda^{\prime})|\leq C_{\Phi}|\lambda-\lambda^{\prime}|\) where

\[C_{\Phi}=\sup_{\lambda\geq 0}|\Phi^{\prime}(\lambda)|=\sup_{\lambda\geq 0} \big{(}\nu+\nicefrac{{\dim(\mathcal{M})}}{{2}}\big{)}\biggl{(}\frac{2\nu}{\kappa^{2 }}+\lambda\biggr{)}^{-\nu-\nicefrac{{\dim(\mathcal{M})}}{{2}}-1}=(\nu+ \nicefrac{{\dim(\mathcal{M})}}{{2}}\biggr{)}^{-\nu-\nicefrac{{\dim(\mathcal{M})}}{{2 }}-1}.\] (29)Define an auxiliary kernel with manifold eigenvalues and graph eigenfunctions by

\[\tilde{k}^{L}_{\nu,\kappa,\sigma_{f}^{2}}(\bm{x},\bm{x}^{\prime})=\frac{\sigma_{f} ^{2}}{C_{\nu,\kappa}}\sum_{l=0}^{L}\biggl{(}\frac{2\nu}{\kappa^{2}}+\lambda_{l} ^{\mathcal{M}}\biggr{)}^{-\nu-\dim(\mathcal{M})/2}f_{l}(\bm{x})f_{l}(\bm{x}^{ \prime}).\] (30)

Then

\[\frac{C_{\nu,\kappa}}{\sigma_{f}^{2}}\Bigl{|}k^{L}_{\nu,\kappa, \sigma_{f}^{2}}(\bm{x}_{i},\bm{x}_{j})-\tilde{k}^{L}_{\nu,\kappa,\sigma_{f}^{2} }(\bm{x}_{i},\bm{x}_{j})\Bigr{|} \leq\sum_{l=0}^{L}\biggl{(}\frac{2\nu}{\kappa^{2}}+\lambda_{l}^{ \mathcal{M}}\biggr{)}^{-\nu-\dim(\mathcal{M})/2}\frac{(1+2C_{\lambda})\varepsilon} {L}\] (31) \[\leq\Phi(0)(1+2C_{\lambda})\varepsilon.\] (32)

Also, noting that \(\bigl{|}f_{l}(\bm{x}_{i})\bigr{|}\leq\bigl{|}f_{l}^{\mathcal{M}}(\bm{x}_{i})- f_{l}(\bm{x}_{i})\bigr{|}+\bigl{|}f_{l}^{\mathcal{M}}(\bm{x}_{i})\bigr{|}\leq \varepsilon^{\prime}+C_{\lambda}\max((\lambda_{l}^{\mathcal{M}})^{\frac{d-1 }{4}},1)\), write

\[\frac{C_{\nu,\kappa}}{\sigma_{f}^{2}}\Bigl{|}\tilde{k}^{L}_{\nu, \kappa,\sigma_{f}^{2}}(\bm{x}_{i},\bm{x}_{j})-k^{N,\alpha,L}_{\nu,\kappa, \sigma_{f}^{2}}(\bm{x}_{i},\bm{x}_{j})\Bigr{|} \leq\sum_{l=0}^{L}\bigl{|}\Phi(\lambda_{l}^{\mathcal{M}})-\Phi( \lambda_{l})\bigl{|}\bigl{|}f_{l}(\bm{x}_{i})\bigr{|}\bigl{|}f_{l}(\bm{x}_{j}) \bigr{|}\] (33) \[\leq\sum_{l=0}^{L}C_{\Phi}\bigl{|}\lambda_{l}^{\mathcal{M}}- \lambda_{l}\bigl{|}\bigl{|}f_{l}(\bm{x}_{i})\bigr{|}\bigl{|}f_{l}(\bm{x}_{j}) \bigr{|}\] (34) \[\leq\sum_{l=0}^{L}2C_{\Phi}\varepsilon^{\prime}\Bigl{(}(\varepsilon ^{\prime})^{2}+C_{\lambda}^{2}\max((\lambda_{l}^{\mathcal{M}})^{\frac{d-1}{2}},1)\Bigr{)}\] (35) \[\leq 2C_{\Phi}(1+C_{\lambda}^{2})\varepsilon.\] (36)

Finally,

\[\Bigl{|}k_{\nu,\kappa,\sigma_{f}^{2}}(\bm{x}_{i},\bm{x}_{j})-k^{N,\alpha,L}_{\nu,\kappa,\sigma_{f}^{2}}(\bm{x}_{i},\bm{x}_{j})\Bigr{|}\leq \Bigl{|}k_{\nu,\kappa,\sigma_{f}^{2}}(\bm{x}_{i},\bm{x}_{j})-k^{L }_{\nu,\kappa,\sigma_{f}^{2}}(\bm{x}_{i},\bm{x}_{j})\Bigr{|}\] (37) \[+\Bigl{|}k^{L}_{\nu,\kappa,\sigma_{f}^{2}}(\bm{x}_{i},\bm{x}_{j}) -\tilde{k}^{L}_{\nu,\kappa,\sigma_{f}^{2}}(\bm{x}_{i},\bm{x}_{j})\Bigr{|}\] (38) \[+\Bigl{|}\tilde{k}^{L}_{\nu,\kappa,\sigma_{f}^{2}}(\bm{x}_{i},\bm {x}_{j})-k^{N,\alpha,L}_{\nu,\kappa,\sigma_{f}^{2}}(\bm{x}_{i},\bm{x}_{j}) \Bigr{|}\] (39) \[\leq\varepsilon+\frac{\sigma_{f}^{2}}{C_{\nu,\kappa}}\bigl{(}\Phi( 0)(1+2C_{\lambda})+2C_{\Phi}(1+C_{\lambda}^{2})\bigr{)}\varepsilon.\] (40)

This proves the claim. 

**Proposition 2**.: _Assuming \(\nu\in\mathbb{N}\), the precision matrix \(\mathbf{P_{XX}}\) of \(k^{\mathbf{X}}_{\nu,\kappa,\sigma^{2}}(\bm{x}_{i},\bm{x}_{j})\) can be given by_

\[\mathbf{P_{XX}}=\frac{\sigma^{-2}}{C_{\nu,\kappa}^{-1}}\mathbf{D}\underbrace{ \Bigl{(}\frac{2\nu}{\kappa^{2}}\mathbf{I}+\bm{\Delta_{\mathrm{rw}}}\Bigr{)} \cdot\ldots\cdot\Bigl{(}\frac{2\nu}{\kappa^{2}}\mathbf{I}+\bm{\Delta_{\mathrm{rw }}}\Bigr{)}}_{\nu\text{ times}}.\] (17)

Proof.: The covariance matrix \(\mathbf{K_{XX}}\) is given by

\[\mathbf{K_{XX}}=\frac{\sigma^{2}}{C_{\nu,\kappa}}\sum_{l=0}^{L-1}\Phi(\lambda_ {l})\bm{f}_{l}\bm{f}_{l}^{\top}, \Phi(\lambda)=\biggl{(}\frac{2\nu}{\kappa^{2}}+\lambda\biggr{)}^{-\nu}\] (41)

where \(\bm{f}_{l}=\mathbf{D}^{-1/2}\bm{f}_{l}^{\mathrm{sym}}\) and \(\bm{f}_{l}^{\mathrm{sym}}\) are the orthonormal eigenvectors of the symmetric normalized Laplacian \(\bm{\Delta_{\mathrm{sym}}}\). Denote

\[\mathbf{K_{XX}^{\mathrm{sym}}}=\frac{\sigma^{2}}{C_{\nu,\kappa}}\sum_{l=0}^{L-1} \Phi(\lambda_{l})\bm{f}_{l}^{\mathrm{sym}}(\bm{f}_{l}^{\mathrm{sym}})^{\top}\] (42)

then \((\mathbf{K_{XX}^{\mathrm{sym}}})^{-1}=\frac{\sigma^{-2}}{C_{\nu,\kappa}^{-1}} \sum_{l=0}^{L-1}\bigl{(}\frac{2\nu}{\kappa^{2}}+\lambda_{l}\bigr{)}^{\nu}\bm{f}_ {l}^{\mathrm{sym}}(\bm{f}_{l}^{\mathrm{sym}})^{\top}=\frac{\sigma^{-2}}{C_{\nu, \kappa}^{-1}}\bigl{(}\frac{2\nu}{\kappa^{2}}\mathbf{I}+\bm{\Delta_{\mathrm{sym }}}\bigr{)}^{\nu}\). We have

\[\mathbf{K_{XX}}=\frac{\sigma^{2}}{C_{\nu,\kappa}}\sum_{l=0}^{L-1}\Phi(\lambda_{l}) \mathbf{D}^{-1/2}\bm{f}_{l}^{\mathrm{sym}}(\bm{f}_{l}^{\mathrm{sym}})^{\top} \mathbf{D}^{-1/2}=\mathbf{D}^{-1/2}\mathbf{K_{XX}^{\mathrm{sym}}}\mathbf{D}^{-1/2}.\] (43)On the other hand,

\[\frac{\sigma_{f}^{-2}}{C_{\nu,\kappa}^{-1}}\mathbf{D}\bigg{(}\frac{2 \nu}{\kappa^{2}}\mathbf{I}+\mathbf{\Delta}_{\text{rw}}\bigg{)}^{\nu} =\frac{\sigma_{f}^{-2}}{C_{\nu,\kappa}^{-1}}\mathbf{D}\bigg{(}\frac {2\nu}{\kappa^{2}}\mathbf{I}+\mathbf{D}^{-1/2}\mathbf{\Delta}_{\text{sym}} \mathbf{D}^{1/2}\bigg{)}^{\nu}\] (44) \[=\frac{\sigma_{f}^{-2}}{C_{\nu,\kappa}^{-1}}\mathbf{D}^{1/2} \bigg{(}\frac{2\nu}{\kappa^{2}}\mathbf{I}+\mathbf{\Delta}_{\text{sym}}\bigg{)} ^{\nu}\mathbf{D}^{1/2}=\mathbf{D}^{1/2}(\mathbf{K}_{\mathbf{XX}}^{\text{sym}} )^{-1}\mathbf{D}^{1/2}\] (45) \[=\mathbf{K}_{\mathbf{XX}}^{-1}=\mathbf{P}_{\mathbf{XX}}.\] (46)

## Appendix B Additional Experimental Results and Details

Here we provide additional results and details for synthetic examples and high-dimensional datasets.

### 1D Dumbbell Manifold

In this section, we analyze our method's sensitivity to the number of labeled points, spectrum truncation, and neighbor count in graph construction, offering deeper insights into its behavior.

**Eigenpairs Truncation.** From a theoretical standpoint, utilizing the complete set of eigenpairs to construct the kernel should be beneficial. However, this assumption may not hold true in cases where the optimization problem is not fully converged. The trends of RMSE and NLL, as depicted in Figures 4(a) and 4(b), illustrate the impact of increasing the number of eigenpairs for 100, 200, and 1000 iterations. In situations where hyperparameter optimization does not converge fully, the length scale parameter \(\kappa\) fails to reach sufficiently high values necessary to generate appropriate spectral density decay, which in turn would properly weigh higher frequency eigenpairs. In such scenarios, truncating the spectrum is similar to increasing the length scale, which might improve results in certain cases. In the 1D scenario, due to its simplicity, we opted for a modest number of eigenpairs, namely \(L=50\). Exceeding this count did not yield any discernible improvements.

**Dataset Size.** We analyze the sensitivity to dataset size of our method (IMGP) against the Euclidean case (EGP) in the semi-supervised learning scenario. For fixed number of eigenpairs, \(L=50\), Figures 4(c) and 4(d) show performance metrics (RMSE and NLL) depending on the percentage \(n\%N\) of labeled points. In a typical scenario where we have at our disposal fewer labeled points compared to the number of unlabeled points, our method outperforms EGP in both accuracy (RMSE) and uncertainty quantification (NLL). As \(n\) increases EGP starts to match the performance of IMGP.

**Number of neighbors.** For the one dimensional case considered in this section, only three neighbors are necessary to capture the essential features of the manifold's geometry. Choosing a number less than three would hinder the algorithm's ability to capture the underlying manifold structure. On the other hand, increasing the number of neighbors beyond this threshold does not affect the solution, provided that sufficient time is allocated for hyperparameter optimization to converge and the graph bandwidth becomes small enough to "correct" for all undesired edges in the graph structure (for instance in the central region of the dumbbell). In high-dimensional problems where the dimension of the underlying manifold is unknown, this suggests to incrementally increase the number of neighbors until the loss function stops improving, if it is computationally feasible to try multiple values of \(K\).

Figure 5: Root Mean Square Error (RMSE) and Negative Log-Likelihood (NLL) for increasing number of eigenpairs \(L\) (left panels) and increasing fraction \(f=n\%N\) of labeled points (right panels). The legend in (a) and (b) refers to the number of hyperparameter optimization iterations.

[MISSING_PAGE_EMPTY:17]

**CT slices.** For IMGP, we use \(\nu=3\). In Table 5 we report results for IMGP-S (full) and IMGP-SS (full). These are based on the "exact" eigenpairs, computed by torch.linalg.eigh, as opposed to the standard Lanczos implementation we use by default. Additionally, these include the hyperparameter re-optimization step, as described in Section 5.2 and discussed below. Regarding RMSE, all three compared methods--IMGP-S (full) and IMGP-SS (full) and EGP--exhibit similar performance, with a slight advantage for SS-IMGP (full) as the number of training points increases. When considering NLL, as previously observed with MNIST, SS-IMGP performs best in all settings. However, NLL decreases for larger values of \(n/N\), indicating a probable overfit in this regime.

Hyperparameter re-optimization.We observed this step to serve two important purposes: (1) fixing overly small values of the signal variance \(\sigma^{2}\), potentially caused by the absence of covariance normalization in the optimization process and poor convergence; (2) adjusting the length scale parameter to take into account the loss of high-frequency components due to truncation.

Implementation.Currently, the implementation faces two significant limitations that might restrict its usability to high-memory hardware setups. Firstly, due to the absence of rich enough sparse matrix routines in PyTorch, we had to develop our own custom implementation of differentiable sparse operators. We kept to high-level routines, which forced us to strike a balance between performance and memory efficiency. In particular, for matrix-vector sparse product operations, our approach relies on highly optimized vectorized code, delivering high performance on GPU at the expense of increased memory allocation. Secondly, we faced challenges with sparse eigen-solvers in the PyTorch ecosystem. Our attempts of using the PyTorch implementation of the LOBPCG algorithm, torch.lobpcq, yielded relatively poor results. We had similar experience with the Lanczos implementation from GPyTorch (Gardner et al., 2018). In light of this, when extracting higher-frequency components was needed, we resorted to two alternative solutions: PyTorch dense matrix eigen-decomposition torch.linalg.eigh and the SciPy Arpack wrapper scipy.linalg.eigh. The first approach, while benefiting from GPU acceleration, can be infeasible because of limited GPU memory. The second approach, known for its efficiency in memory usage due to its Krylov subspace-based nature, is constrained to CPU utilization, significantly impacting the algorithm's overall performance.

## Appendix C Hyperparameter Priors and Initialization

Here we describe hyperparameter priors which might be of help when using implicit manifold Gaussian process regression.

Graph Bandwidth \(\alpha\).Graph Laplacian converges to the Laplace-Beltrami operator when \(\alpha\) tends to zero, motivating smaller \(\alpha\). However, in a non-asymptotic setting it is impractical to have \(\alpha\) overly small as it will render the graph effectively disconnected and cause numerical instabilities. One appropriate prior could thus be _gamma distribution_, whose right tail discourages high values of \(\alpha\), and which, given an appropriate choice of the parameters, encourages \(\alpha\) to align with the scale of pairwise distances between graph nodes. Specifically, we choose the parameters of the gamma distribution so as to (1) match its mode with the median (\(Q_{2}\)) of pairwise average distance between \(K\)-th nearest neighbors because such an \(\alpha\) would give reasonable weights in the KNN graph and (2) so as to place its standard \(0.95\) confidence interval to the right of a certain lower bound \(\bar{\alpha}\) we define further that ensures the graph is numerically not disconnected. Let

\[\mathcal{D}=\big{\{}\max_{\bm{x}_{i}\in\text{KNN}(\bm{x}_{j})}\lVert\bm{x}_{i }-\bm{x}_{j}\rVert\text{ where }j=1,..,N\big{\}}.\] (47)

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{**RMSE**} & \multicolumn{4}{c}{**NLL**} \\ \cline{3-8}
**Dataset** & \(n/N\) & **S-IMGP** & **SS-IMGP** & **EGP** & **S-IMGP** & **SS-IMGP** & **EGP** \\ \hline SR-MNIST & 10\% & \(0.04\pm 0.01\) & \(\bm{0.01\pm 0.00}\) & \(0.12\pm 0.01\) & \(-1.42\pm 0.01\) & \(-\bm{1.52\pm 0.01}\) & \(-0.54\pm 0.01\) \\ MR-MNIST & 1\% & \(0.74\pm 0.02\) & \(\bm{0.43\pm 0.02}\) & \(0.74\pm 0.02\) & \(2.24\pm 0.20\) & \(-\bm{0.59\pm 0.01}\) & \(-0.20\pm 0.01\) \\ MR-MNIST & 10\% & \(0.14\pm 0.05\) & \(\bm{0.03\pm 0.00}\) & \(0.13\pm 0.00\) & \(-0.68\pm 0.08\) & \(-\bm{0.79\pm 0.00}\) & \(-0.43\pm 0.01\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results for the rotated MNIST dataset.

The bandwidth's lower bound we compute as

\[\bar{\alpha}=\min_{d\in\mathcal{D}}\sqrt{-\frac{d^{2}}{4\log\tau}},\] (48)

where \(\tau\) is a user-defined parameter.

Let \(\eta\) and \(\beta\) the shape and rate parameters of the gamma distribution. In order to achieve (1), we define

\[\eta=\rho Q_{2}+1\quad\text{and}\quad\beta=\rho\] (49)

where \(\rho\) is used to achieve (2) and is given by

\[\rho\approx\frac{4Q_{2}}{(Q_{2}-\bar{\alpha})^{2}}.\] (50)

Considering the S-MNIST dataset, Figure 7 shows the bandwidth prior distribution for the semi-supervised (labeled and unlabeled points) and the supervised (labeled points) scenarios.

Signal Variance \(\sigma_{\varepsilon}^{2}\).Assuming normalized \(y_{i}\), the signal variance \(\sigma_{\varepsilon}^{2}\) should be close to \(1\). Because of this a natural prior for \(\sigma^{2}\) is the truncated normal (onto the set of positive reals \(\sigma^{2}>0\)), with mode at \(1\). The pre-truncation variance can be chosen, for example, to have \(0\) at \(3\) standard deviations away from the mode, i.e. it can be chosen to be \(1/9\). Note that setting a prior over the signal variance parameter requires evaluating the normalization constant \(C_{\nu,\kappa}\)7 for the kernel at each hyperparameter optimization step, something that can be avoided otherwise.

Footnote 7: Covariance matrix normalization constant \(C_{\nu,\kappa}\) can be approximated as \(C_{\nu,\kappa}\approx\frac{1}{M}\sum_{i=1}^{M}\bm{e}_{i}^{T}\mathbf{P}_{\bm{ \mathrm{X}}\bm{\mathrm{X}}}^{-1}\bm{e}_{i}\), where \(M\) is relatively small, \(\bm{e}_{i}\) are random standard basis vectors and the solve is performed by running the conjugate gradients. Differentiability of the model is preserved. Note however that we did not use this normalization in our tests since the performance improvement we observed was not enough to justify the additional computation overhead. This trade-off can vary considerably from case to case, which is why in our implementation, the covariance normalization is optional.

Noise Variance \(\sigma_{\varepsilon}^{2}\).Choosing a prior for the noise variance \(\sigma_{\varepsilon}^{2}\) is heavily problem-dependent. Truncated normal (onto the set \(\sigma_{\varepsilon}^{2}>0\)) with mode at \(0\) could be a reasonable option. The pre-truncation variance can be chosen, for example, to be \(1\), \(1/4\) or \(1/9\), placing the value \(1\), which is the variance of the normalized observations \(y_{i}\), at \(1\), \(2\) or \(3\) standard deviations away from the mode.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & \multicolumn{3}{c}{**RMSE**} & \multicolumn{3}{c}{**NLL**} \\ \cline{2-7} \(n/N\) & **S-IMGP (full)** & **SS-IMGP (full)** & **EGP** & **S-IMGP (full)** & **SS-IMGP (full)** & **EGP** \\ \hline
5\% & \(0.27\pm 0.02\) & \(0.39\pm 0.04\) & \(\bm{0.24\pm 0.02}\) & \(0.64\pm 0.83\) & \(\bm{-2.48\pm 0.08}\) & \(-0.80\pm 0.02\) \\
10\% & \(0.22\pm 0.02\) & \(0.19\pm 0.01\) & \(\bm{0.16\pm 0.00}\) & \(0.88\pm 0.29\) & \(\bm{-2.35\pm 0.04}\) & \(-0.96\pm 0.00\) \\
25\% & \(0.14\pm 0.01\) & \(\bm{0.08\pm 0.02}\) & \(0.09\pm 0.01\) & \(-0.42\pm 0.10\) & \(\bm{-1.99\pm 0.04}\) & \(-1.20\pm 0.09\) \\
50\% & \(0.08\pm 0.01\) & \(\bm{0.07\pm 0.01}\) & \(0.08\pm 0.04\) & \(-0.96\pm 0.11\) & \(\bm{-2.04\pm 0.02}\) & \(-1.02\pm 0.04\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results for Relative location of CT slices on axial axis (\(d=385\), \(N=48150\)) from UCI Machine Learning Repository.

Figure 7: The histogram of \(\mathcal{D}\) and the prior for the bandwidth hyperparameter \(\alpha\).

Length scaleThe interpretation and the scale of the length scale parameter is manifold-specific. This makes it very difficult to come up with any reasonable prior. Because of this, we suggest actually leaving the length scale parameter free.

Parameter initializationWhen doing MAP estimation, one can initialize parameters randomly, sampling them from respective priors.