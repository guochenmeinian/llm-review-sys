# Efficient Online Clustering with Moving Costs

 Dimitris Christou

UT Austin

christou@cs.utexas.edu

&Stratis Skoulakis

LIONS, EPFL

efstratios.skoulakis@epfl.ch

&Volkan Cevher

LIONS, EPFL

volkan.cevher@epfl.ch

First author contribution.

###### Abstract

In this work we consider an online learning problem, called _Online \(k\)-Clustering with Moving Costs_, at which a _learner_ maintains a set of \(k\) facilities over \(T\) rounds so as to minimize the connection cost of an adversarially selected sequence of clients. The _learner_ is informed on the positions of the clients at each round \(t\) only after its facility-selection and can use this information to update its decision in the next round. However, updating the facility positions comes with an additional moving cost based on the moving distance of the facilities. We present the first \(\mathcal{O}(\log n)\)-regret polynomial-time online learning algorithm guaranteeing that the overall cost (connection \(+\) moving) is at most \(\mathcal{O}(\log n)\) times the time-averaged connection cost of the _best fixed solution_. Our work improves on the recent result of Fotakis et al. [31] establishing \(\mathcal{O}(k)\)-regret guarantees _only_ on the connection cost.

## 1 Introduction

Due to their various applications in diverse fields (e.g. machine learning, operational research, data science etc.), _clustering problems_ have been extensively studied. In the well-studied \(k\)-\(\mathrm{median}\) problem, given a set of clients, \(k\) facilities should be placed on a metric with the objective to minimize the sum of the distance of each client from its closest center [55, 14, 13, 67, 6, 44, 52, 65, 51, 15, 54, 3].

In many modern applications (e.g., epidemiology, social media, conference, etc.) the positions of the clients are not _static_ but rather _evolve over time_[57, 56, 64, 59, 23, 5]. For example the geographic distribution of the clients of an online store or the distribution of Covid-19 cases may drastically change from year to year or respectively from day to day [31]. In such settings it is desirable to update/change the positions of the facilities (e.g., compositions of warehouses or Covid test-units) so as to better serve the time-evolving trajectory of the clients.

The clients' positions may change in complex and unpredictable ways and thus an _a priori knowledge_ on their trajectory is not always available. Motivated by this, a recent line of research studies clustering problems under the _online learning framework_ by assuming that the sequence of clients' positions is _unknown_ and _adversarially selected_[18, 28, 16, 31]. More precisely, a _learner_ must place \(k\) facilities at each round \(t\geq 1\) without knowing the positions of clients at round \(t\) which are revealed to the learner only after its facility-selection. The learner can use this information to update its decision in the next round; however, moving a facility comes with an additional moving cost that should be taken into account in the learner's updating decision, e.g. moving Covid-19 test-units comes with a cost [18, 28].

Building on this line of works, we consider the following online learning problem:

**Problem 1** (_Online \(k\)-Clustering with Moving Costs_).: _Let \(G(V,E,w)\) be a weighted graph with \(|V|=n\) vertices and \(k\) facilities. At each round \(t=1,\ldots,T\):_

1. _The learner selects_ \(F_{t}\subseteq V\)_, with_ \(|F_{t}|=k\)_, at which facilities are placed._2. _The adversary selects the clients' positions,_ \(R_{t}\subseteq V\)_._
3. _The learner learns the clients' positions_ \(R_{t}\) _and suffers_ \[\text{cost}=\sum_{j\in R_{t}}\ \min_{\begin{subarray}{c}i\in F_{t}\\ \text{\rm{connection cost of client }j}\end{subarray}}d_{G}(j,i)\] \[+\underbrace{\gamma\cdot M_{G}(F_{t-1},F_{t})}_{\text{ moving cost of facilities}}\]

_where \(d_{G}(j,i)\) is the distance between vertices \(i,j\in V\); \(M_{G}(F_{t-1},F_{t})\) is the minimum overall distance required to move \(k\) facilities from \(F_{t-1}\) to \(F_{t}\); and \(\gamma\geq 0\) is the facility-weight._

An _online learning algorithm_ for Problem 1 tries to minimize the overall (connection \(+\) moving) cost by placing \(k\) facilities at each round \(t\geq 1\) based only on the previous positions of clients \(R_{1},\ldots,R_{t-1}\). To the best of our knowledge, Problem 1 was first introduced in [18]2. If for any sequence of clients, the overall cost of the algorithm is at most \(\alpha\) times the overall connection cost of the _optimal fixed placement of facilities \(F^{*}\)_ then the algorithm is called \(\alpha\)-regret, while in the special case of \(\alpha=1\) the algorithm is additionally called _no-regret_.

Footnote 2: In [18], an easier version of Problem 1 with \(1\)-_lookahead_ is considered, meaning that the learner learns the positions of the clients \(R_{t}\) before selecting \(F_{t}\). Moreover, \(G\) is considered to be the line graph and \(\gamma=1\).

Problem 1 comes as a special case of the well-studied _Metrical Task System_ by considering each of the possible \(\binom{n}{k}\) facility placements as a different state. In their seminal work, [11] guarantee that the famous _Multiplicative Weights Update algorithm_ (\(\operatorname{MWU}\)) achieves \((1+\epsilon)\)-regret in Problem 1 for any \(\epsilon>0\). Unfortunately, running the \(\operatorname{MWU}\) algorithm for Problem 1 is not really an option since it requires \(\mathcal{O}(n^{k})\) time and space complexity. As a result, the following question naturally arises:

**Q.**_Can we achieve \(\alpha\)-regret for Problem 1 with polynomial-time online learning algorithms?_

Answering the above question is a challenging task. Even in the very simple scenario of time-invariant clients, i.e. \(R_{t}=R\) for all \(t\geq 1\), an \(\alpha\)-regret online learning algorithm must essentially compute an \(\alpha\)-_approximate solution_ of the \(k\)-\(\operatorname{median}\) problem. Unfortunately the \(k\)-\(\operatorname{median}\) problem cannot be approximated with ratio \(\alpha<1+2/e\simeq 1.71\) (unless \(\operatorname{NP}\subseteq\operatorname{DTIME}[n^{\log\log n}]\)[43]) which excludes the existence of an \((1+2/e)\)-regret polynomial-time online learning algorithm for Problem 1. Despite the fact that many \(\mathcal{O}(1)\)-approximation algorithms have been proposed for the \(k\)-median problem (the best current ratio is \(1+\sqrt{3}\)[54]), these algorithms crucially rely on the (offline) knowledge of the whole sequence of clients and most importantly are not designed to handle the moving cost of the facilities [55, 14, 13, 67, 6, 44, 52, 65, 51, 15, 54, 3].

In their recent work, Fotakis et al. [31] propose an \(\mathcal{O}(k)\)-regret polynomial-time online learning algorithm for Problem 1 _without_ moving costs (i.e. the special case of \(\gamma=0\)). Their approach is based on designing a _no-regret_ polynomial-time algorithm for a _fractional relaxation_ of Problem 1 and then using an _online client-oblivious_ rounding scheme in order to convert a fractional solution to an integral one. Their analysis is based on the fact that the connection cost of _any possible client_ is at most \(\mathcal{O}(k)\) times its fractional connection cost. However in order to establish the latter guarantee their rounding scheme performs abrupt changes on the facilities leading to huge moving cost.

Our Contribution and Techniques.In this work, we provide a positive answer to question (**Q**), by designing the first polynomial-time online learning algorithm for Online \(k\)-Clustering with Moving Costs that achieves \(\mathcal{O}\left(\log n\right)\)-regret for any \(\gamma\geq 0\). The cornerstone idea of our work was to realize that \(\mathcal{O}(1)\)-regret can be established with a polynomial-time online learning algorithm in the special case of \(G\) being a Hierarchical Separation Tree (HST). Then, by using the standard metric embedding result of [25], we can easily convert such an algorithm to an \(\mathcal{O}(\log n)\)-regret algorithm for general graphs. Our approach for HSTs consists of two main technical steps:

1. We introduce a fractional relaxation of Problem 1 for HSTs. We then consider a specific regularizer on the fractional facility placements, called _Dilated Entropic Regularizer_[26], that takes into account the specific structure of the HST. Our first technical contribution is to establish that the famous _Follow the Leader algorithm_[35] with dilated entropic regularization admits \(\mathcal{O}(1)\)-regret for any \(\gamma\geq 0\).

2. Our second technical contribution is the design of a novel _online client-oblivious_ rounding scheme, called \(\mathrm{Cut}\&\mathrm{Round}\), that converts a fractional solution for HSTs into an integral one. By exploiting the specific HST structure we establish that \(\mathrm{Cut}\&\mathrm{Round}\), despite not knowing the clients' positions \(R_{t}\), simultaneously guarantees that \((i)\) the connection cost of each client \(j\in R_{t}\) is upper bounded by its fractional connection cost, and \((ii)\) the expected moving cost of the facilities is at most \(\mathcal{O}(1)\) times the fractional moving cost.

Experimental Evaluation.In Section F of the Appendix we experimentally compare our algorithm with the algorithm of Fotakis et al. [31]. Our experiments verify that our algorithm is robust to increases of the facility weight \(\gamma\) while the algorithm of [31] presents a significant cost increase. We additionally experimentally evaluate our algorithm in the \(\mathrm{MNIST}\) and \(\mathrm{CIFAR}10\) datasets. Our experimental evaluations suggest that the \(\mathcal{O}(\log n)\)-regret bound is a pessimistic upper bound and that in practise our algorithm performs significantly better. Finally, we evaluate our algorithm both in the random arrival case (where the requested vertices are drawn uniformly at random from the graph) as well as in adversarial settings, where the request sequences are constructed through some arbitrary deterministic process.

Related Work.As already mentioned, our work most closely relates with the work of Fotakis et al. [31] that provides an \(\mathcal{O}(k)\)-regret algorithm running in polynomial-time for \(\gamma=0\). [16] also consider Problem 1 for \(\gamma=0\) with the difference that the connection cost of clients is captured through the \(k\)-\(\mathrm{means}\) objective i.e. the sum of the squared distances. They provide an \((1+\epsilon)\)-regret algorithm with \(\mathcal{O}\left((k^{2}/\epsilon^{2})^{2k}\right)\) time-complexity that is still exponential in \(k\). [18; 28] study the special case of Problem 1 in which \(G\) is the line graph and \(\gamma=1\) while assuming \(1\)_-lookahead_ on the request \(R_{t}\). For \(k=1\), [18] provide an \((1+\epsilon)\)-competitive online algorithm meaning that its cost is at most \((1+\epsilon)\) times the cost of the _optimal dynamic solution_ and directly implies \((1+\epsilon)\)-regret. [28] extended the previous result by providing a \(63\)-competitive algorithm for \(k=2\) on line graphs. Our work also relates with the works of [23] and [4] that study offline approximation algorithms for clustering problems with _time-evolving metrics_. Finally our work is closely related with the research line of online learning in combinatorial domains and other settings of online clustering. Due to space limitations, we resume this discussion in Section A of the Appendix.

## 2 Preliminaries and Our Results

Let \(G(V,E,w)\) be a weighted undirected graph where \(V\) denotes the set of vertices and \(E\) the set of edges among them. The weight \(w_{e}\) of an edge \(e=(i,j)\in E\) denotes the cost of traversing \(e\). Without loss, we assume that \(w_{e}\in\mathbb{N}\) and \(w_{e}\geq 1\) for all edges \(e\in E\). The _distance_ between vertices \(i,j\in V\) is denoted with \(d_{G}(i,j)\) and equals the cost of the minimum cost path from \(i\in V\) to \(j\in V\). We use \(n:=|V|\) to denote the cardinality of \(G\) and \(D_{G}:=\max_{i,j\in V}d_{G}(i,j)\) to denote its diameter.

Given a placement of facilities \(F\subseteq V\), with \(|F|=k\), a client placed at vertex \(j\in V\) connects to the _closest open facility_\(i\in F\). This is formally captured in Definition 1.

**Definition 1**.: _The connection cost of a set of clients \(R\subseteq V\) under the facility-placement \(F\subseteq V\) with \(|F|=k\) equals_

\[C_{R}(F):=\sum_{j\in R}\min_{i\in F}d_{G}(j,i)\]

Next, consider any pair of facility-placements \(F,F^{\prime}\subseteq V\) such that \(|F|=|F^{\prime}|=k\). The moving distance between \(F\) and \(F^{\prime}\) is the minimum overall distance needed to transfer the \(k\) facilities from \(F\) to \(F^{\prime}\), formally defined in Definition 2.

**Definition 2**.: _Fix any facility-placements \(F,F^{\prime}\subseteq V\) where \(|F|=|F^{\prime}|=k\). Let \(\Sigma\) be the set of all possible matchings from \(F\) to \(F^{\prime}\), i.e. each \(\sigma\in\Sigma\) is a one-to-one mapping \(\sigma:F\mapsto F^{\prime}\) with \(\sigma(i)\in F^{\prime}\) denoting the mapping of facility \(i\in F\). The moving cost between \(F\) and \(F^{\prime}\) equals_

\[M_{G}(F,F^{\prime}):=\min_{\sigma\in\Sigma}\sum_{i\in F}d_{G}(i,\sigma(i))\]

At each round \(t\geq 1\), an online learning algorithm \(\mathcal{A}\) for Problem 1 takes as input all the _previous_ positions of the clients \(R_{1},\ldots,R_{t-1}\subseteq V\) and outputs a facility-placement \(F_{t}:=\mathcal{A}(R_{1},\ldots,R_{t-1})\)such that \(F_{t}\subseteq V\) and \(|F_{t}|=k\). The performance of an online learning algorithm is measured by the notion of _regret_, which we formally introduce in Definition 3.

**Definition 3**.: _An online learning algorithm \(\mathcal{A}\) for Problem 1 is called \(\alpha\)-regret with additive regret \(\beta\) if and only if for any sequence of clients \(R_{1},\ldots,R_{T}\subseteq V\),_

\[\mathbb{E}\left[\sum_{t=1}^{T}C_{R_{t}}(F_{t})+\gamma\cdot\sum_{t=2}^{T}M_{G} (F_{t-1},F_{t})\right]\leq\alpha\cdot\min_{|F^{*}|=k}\sum_{t=1}^{T}C_{R_{t}}(F ^{*})+\beta\cdot\sqrt{T}\]

_where \(F_{t}=\mathcal{A}(R_{1},\ldots,R_{t-1})\) and \(\alpha,\beta\) are constants independent of \(T\)._

An online learning algorithm \(\mathcal{A}\) selects the positions of the \(k\) facilities at each round \(t\geq 1\) solely based on the positions of the clients in the previous rounds, \(R_{1},\ldots,R_{t-1}\). If \(\mathcal{A}\) is \(\alpha\)-regret then Definition 3 implies that its time-averaged overall cost (connection \(+\) moving cost) is at most \(\alpha\) times the time-averaged cost of the _optimal static solution!_3 Furthermore, the dependency on \(\sqrt{T}\) is known to be optimal [11] and \(\beta\) is typically only required to be polynomially bounded by the size of the input, as for \(T\to\infty\) the corresponding term in the time-averaged cost vanishes.

Footnote 3: Specifically, the time-averaged overall cost of \(\mathcal{A}\) approaches this upper bound with rate \(\beta\cdot T^{-1/2}\).

As already mentioned, the seminal work of [11] implies the existence of an \((1+\epsilon)\)-regret algorithm for Problem 1; however, this algorithm requires \(\mathcal{O}(n^{k})\) time and space complexity. Prior to this work, the only polynomial-time4 online learning algorithm for Problem 1 was due to Fotakis et al. [31], for the special case of \(\gamma=0\). Specifically, in their work the authors design an online learning algorithm with the following guarantee:

Footnote 4: Polynomial-time with respect to the input parameters, namely \(T\), \(n\) and \(\log D_{G}\).

**Theorem** (Fotakis et al. [31]).: _There exists a randomized online learning algorithm for Problem 1 that runs in polynomial time (w.r.t. \(T\), \(n\) and \(\log D_{G}\)) such that_

\[\mathbb{E}\left[\sum_{t=1}^{T}C_{R_{t}}(F_{t})\right]\leq\mathcal{O}(k)\cdot \min_{|F^{*}|=k}\sum_{t=1}^{T}C_{R_{t}}(F^{*})+\mathcal{O}(k\cdot n\cdot \sqrt{\log n}\cdot D_{G})\cdot\sqrt{T}\]

Clearly, the algorithm of [31] has not been designed to account for charging the moving of facilities, as indicated by the absence of the moving cost in the above regret guarantee. The main contribution of this work is to obtain (for the first time) regret guarantees that also account for the moving cost.

**Theorem 1**.: _There exists a randomized online learning algorithm for Problem 1 (Algorithm 2) that runs in polynomial time (w.r.t. \(T\), \(n\) and \(\log D_{G}\)) and admits the following regret guarantee:_

\[\mathbb{E}\left[\sum_{t=1}^{T}C_{R_{t}}(F_{t})+\gamma\cdot\sum_{t=2}^{T}M_{G} (F_{t-1},F_{t})\right]\leq\mathcal{O}(\log n)\cdot\min_{|F^{*}|=k}\sum_{t=1}^ {T}C_{R_{t}}(F^{*})+\beta\cdot\sqrt{T}\]

_for \(\beta=\mathcal{O}(k\cdot n^{3/2}\cdot D_{G}\cdot\max(\gamma,1))\) and any \(\gamma\geq 0\)._

**Remark 1**.: _We remark that while our additive regret \(\beta\) is larger than the corresponding term in [31] by a factor of \(o(\sqrt{n})\), our results apply to any \(\gamma\geq 0\) while the algorithm of [31] can generally suffer unbounded moving cost for \(\gamma\to\infty\), as our experimental results verify._

### HSTs and Metric Embeddings

In this section we provide some preliminary introduction to Hierarchical Separation Trees (HSTs), as they consist a key technical tool towards proving Theorem 1. A _weighted tree_\(\mathcal{T}(V,E,w)\) is a weighted graph with no cycles. Equivalently, for any pair of vertices \(i,j\in V\) there exists a unique path that connects them. In Definition 4, we establish some basic notation for tree graphs.

**Definition 4**.: _Fix any tree \(\mathcal{T}(V,E,w)\). For every vertex \(u\in V\), \(\operatorname{cld}(u)\subseteq V\) denotes the set children vertices of \(u\) and \(p(u)\) denotes its unique parent, i.e. \(u\in\operatorname{cld}(p(u))\). The root \(r\in V\) of \(\mathcal{T}\) is the unique node with \(p(r)=\varnothing\) and the set \(L(\mathcal{T}):=\{u\in V\,:\,\operatorname{cld}(u)=\varnothing\}\) denotes the leaves of \(\mathcal{T}\). We use \(\operatorname{dpt}(u)\) to denote the depth of a vertex \(u\in V\), i.e. the length of the (unique) path from the root \(r\) to \(u\), and \(h(\mathcal{T}):=\max_{u\in L(\mathcal{T})}\operatorname{dpt}(u)\) to denote the height of \(\mathcal{T}\). We use \(\operatorname{lev}(u):=h(\mathcal{T})-\operatorname{dpt}(u)\) to denote the level of a vertex \(u\in V\). Finally, \(T(u)\subseteq V\) denotes the set of vertices on the sub-tree rooted at \(u\), i.e. the set of vertices that are descendants of \(u\)._Next, we proceed to define a family of well-structured tree graphs that constitute one of the primary technical tools used in our analysis.

**Definition 5**.: _A Hierarchical Separation Tree (HST) is a weighted tree \(\mathcal{T}(V,E,w)\) such that (i) for any node \(u\) and any of its children \(v\in cld(u)\), the edge \(e=(u,v)\) admits weight \(w_{e}=2^{\mathrm{lev}(v)}\), and (ii) the tree is balanced, namely \(lev(u)=0\) for all leaves \(u\in L(\mathcal{T})\)._

In their seminal works, [10] and later [24] showed that HSTs can approximately preserve the distances of any graph \(G(V,E,w)\) within some logarithmic level of distortion.

**Theorem 2**.: _For any graph \(G(V,E,w)\) with \(|V|=n\) and diameter \(D\), there exists a polynomial-time randomized algorithm that given as input \(G\) produces an HST \(\mathcal{T}\) with height \(h(\mathcal{T})\leq\lceil\log D\rceil\) s.t._

1. \(L(\mathcal{T})=V\)_, meaning that the leaves of_ \(\mathcal{T}\) _correspond to the vertices of_ \(G\)_._
2. _For any_ \(u,v\in V\)_,_ \(d_{G}(u,v)\leq d_{\mathcal{T}}(u,v)\) _and_ \(\mathbb{E}[d_{\mathcal{T}}(u,v)]\leq\mathcal{O}(\log n)\cdot d_{G}(u,v)\)_._

Theorem 2 states that any weighted graph \(G(V,E,w)\) can be embedded into an HST \(\mathcal{T}\) with \(\mathcal{O}(\log n)\)-distortion. This means that the distance \(d_{G}(u,v)\) between any pair of vertices \(u,v\in V\) can be approximated by their respective distance \(d_{\mathcal{T}}(u,v)\) in \(\mathcal{T}\) within an (expected) factor of \(\mathcal{O}(\log n)\).

**Remark 2**.: _We note that traditionally HSTs are neither balanced nor are required to have weights that are specifically powers of \(2\). However, we can transform any general HST into our specific definition, and this has been accounted for in the statement of the above theorem. The details are deferred to Section B of the Appendix._

## 3 Overview of our approach

In this section we present the key steps of our approach towards designing the \(\mathcal{O}(\log n)\)-regret online learning algorithm for Problem 1. Our approach can be summarized in the following three pillars:

1. In Section 3.1 we introduce a _fractional relaxation_ of Problem 1 in the special case of HSTs (Problem 2). Problem 2 is an artificial problem at which the learner can place a _fractional amount of facility_ to the leaves of an HST so as to fractionally serve the arrived clients. Since the _optimal static solution_ of Problem 2 lower bounds the _optimal static solution_ of Problem 1 in the special case of HSTs, the first step of our approach is to design an \(\mathcal{O}(1)\)-regret algorithm for Problem 2.
2. In Section 3.2 we present the formal guarantees of a novel randomized rounding scheme, called \(\mathrm{Cut\&Round}\), that is client-oblivious and converts any _fractional solution_ for Problem 2 into an actual placement of \(k\) facilities on the leaves of the HST with just an \(\mathcal{O}(1)\)-overhead in the connection and the moving cost.
3. In Section 3.3 we present how the _fractional algorithm_ for Problem 2 together with the \(\mathrm{Cut\&Round}\) rounding naturally lead to an \(\mathcal{O}(1)\)-regret online learning algorithm for Problem 1 in the special case of HSTs (Algorithm 1). Our main algorithm, presented in Algorithm 2, then consists of running Algorithm 1 into an \(\mathcal{O}(\log n)\) HST embedding of input graph.

### A Fractional Relaxation for HSTs

In this section we introduce a fractional relaxation for Problem 1, called _Fractional \(k\)-Clustering with Moving Costs on HSTs_ (Problem 2). Fix any HST \(\mathcal{T}(V,E,w)\) (in this section, \(V\) denotes the nodes of the HST). We begin by presenting a _fractional extension_ of placing \(k\) facilities on the leaves of \(\mathcal{T}\).

**Definition 6**.: _The set of fractional facility placements \(\mathcal{FP}(\mathcal{T})\) consists of all vectors \(y\in\mathbb{R}^{|V|}\) such that_

1. \(y_{v}\in[0,1]\) _for all leaves_ \(v\in L(\mathcal{T})\)_._
2. \(y_{v}=\sum\limits_{u\in\mathrm{cld}(v)}y_{u}\) _for all non-leaves_ \(v\notin L(\mathcal{T})\)_._
3. \(\sum_{v\in L(\mathcal{T})}y_{v}=k\)_, i.e. the total amount of facility on the leaves equals_ \(k\)For a leaf vertex \(v\in L(\mathcal{T})\), \(y_{v}\) simply denotes the fractional amount of facilities that are placed on it. For all non-leaf vertices \(v\notin L(\mathcal{T})\), \(y_{v}\) denotes the total amount of facility placed in the leaves of the sub-tree \(T(v)\). Thus, any integral vector \(y\in\mathcal{FP}(\mathcal{T})\cap\mathds{N}\) corresponds to a placement of \(k\) facilities on the leaves of \(\mathcal{T}\).

In Definitions 7 and 8 we extend the notion of connection and moving cost for fractional facility placements. In the special case of integral facility placements, Definitions 7 and 8 respectively collapse to Definitions 1 and 2 (a formal proof is given in Claims 1 and 2 of Section C of the Appendix).

**Definition 7**.: _The fractional connection cost of a set of clients \(R\subseteq L(\mathcal{T})\) under \(y\in\mathcal{FP}(\mathcal{T})\) is defined as_

\[f_{R}(y):=\sum_{j\in R}\sum_{v\in P(j,r)}2^{lev(v)+1}\cdot\max\left(0,1-y_{v}\right)\]

_where \(P(j,r)\) denotes the set of vertices in the (unique) path from the leaf \(j\in L(\mathcal{T})\) to the root \(r\)._

**Definition 8**.: _The fractional moving cost between any \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\) is defined as_

\[||y-y^{\prime}||_{\mathcal{T}}:=\gamma\cdot\sum_{v\in V(\mathcal{T})}2^{lev(v) }\cdot|y_{v}-y^{\prime}_{v}|\]

We are now ready to present our fractional generalization of Problem 1 in the special case of HSTs.

**Problem 2** (_Fractional \(k\)-Clustering with Moving Costs on HSTs).: _Fix any HST \(\mathcal{T}\). At each round \(t=1,\ldots,T\):_

1. _The learner selects a vector_ \(y^{t}\in\mathcal{FP}(\mathcal{T})\)_._
2. _The adversary selects a set of clients_ \(R_{t}\subseteq L(\mathcal{T})\)_._
3. _The learner suffers cost_ \(f_{R_{t}}(y^{t})+||y^{t}-y^{t-1}||_{\mathcal{T}}\)_._

In Section 4, we develop and present an \(\mathcal{O}(1)\)-regret algorithm for Problem 2 (see Algorithm 3). To this end, we present its formal regret guarantee established in Theorem 3.

**Theorem 3**.: _There exists a polynomial-time online learning algorithm for Problem 2 (Algorithm 3), such that for any sequence \(R_{1},\ldots,R_{T}\subseteq L(\mathcal{T})\), its output \(y^{1},\ldots,y^{T}\) satisfies_

\[\sum_{t=1}^{T}f_{R_{t}}(y^{t})+\sum_{t=2}^{T}||y^{t}-y^{t-1}||_{\mathcal{T}} \leq\frac{3}{2}\cdot\min_{y^{*}\in\mathcal{FP}(\mathcal{T})}\sum_{t=1}^{T}f_{ R_{t}}(y^{*})+\beta\cdot\sqrt{T}\]

_for \(\beta=\mathcal{O}\left(k\cdot|L(\mathcal{T})|^{3/2}\cdot D_{\mathcal{T}}\cdot \max(\gamma,1)\right)\)._

### From Fractional to Integral Placements in HSTs

As already mentioned, the basic idea of our approach is to convert at each round \(t\geq 1\) the _fractional placement_\(y^{t}\in\mathcal{FP}(\mathcal{T})\) produced by Algorithm 3 into an integral facility placement \(F_{t}\subseteq L(\mathcal{T})\) with \(|F_{t}|=k\) on the leaves of the HST. In order to guarantee small regret, our rounding scheme should preserve both the connection and the moving cost of the fractional solution within constant factors for _any possible set of arriving clients_. In order to guarantee the latter, our rounding scheme \(\mathrm{Cut\&Round}\) (Algorithm 4) uses shared randomness across different rounds. \(\mathrm{Cut\&Round}\) is rather complicated and is presented in Section 5. To this end, we present its formal guarantee.

**Theorem 4**.: _There exists a linear-time deterministic algorithm, called \(\mathrm{Cut\&Round}\) (Algorithm 4), that takes as input an HST \(\mathcal{T}\), a fractional facility placement \(y\in\mathcal{FP}(\mathcal{T})\) and a vector \(\alpha\in[0,1]^{|V|}\) and outputs a placement of \(k\) facilities \(F\leftarrow\mathrm{Cut\&Round}(\mathcal{T},y,\alpha)\) on the leaves of \(\mathcal{T}\) (\(F\subseteq L(\mathcal{T})\) and \(|F|=k\)) such that_

1. \(\mathrm{E}_{\alpha\sim\mathrm{Unif}(0,1)}\left[C_{R}(F)\right]=f_{R}(y)\) _for all client requests_ \(R\subseteq L(\mathcal{T})\)_._
2. \(\mathrm{E}_{\alpha\sim\mathrm{Unif}(0,1)}\left[\gamma\cdot M_{\mathcal{T}}(F, F^{\prime})\right]\leq 4\cdot||y-y^{\prime}||_{\mathcal{T}}\) _for all other fractional facility placements_ \(y^{\prime}\in\mathcal{FP}(\mathcal{T})\) _and_ \(F^{\prime}\leftarrow\mathrm{Cut\&Round}(\mathcal{T},y^{\prime},\alpha)\)Item \(1\) of Theorem 4 establishes that although \(\mathrm{Cut}\&\mathrm{Round}\) is _oblivious_ to the arrived set of clients \(R_{t}\subseteq L(\mathcal{T})\), the expected connection cost of the output equals the _fractional connection cost_ under \(y^{t}\in\mathcal{FP}(\mathcal{T})\). Item 2 of Theorem 4 states that once the same random seed \(\alpha\) is used into two consecutive time steps, then the expected moving cost between the facility-placements \(F_{t}\) and \(F_{t+1}\) is at most \(\mathcal{O}(1)\)-times the fractional moving cost between \(y^{t}\) and \(y^{t+1}\). Both properties crucially rely on the structure of the HST and consist one of the main technical contributions of our work.

### Overall Online Learning Algorithm

We are now ready to formally introduce our main algorithm (Algorithm 2) and prove Theorem 1. First, we combine the algorithms from Theorems 3 and 4 to design an \(\mathcal{O}(1)\)-regret algorithm for Problem 1 on HSTs (Algorithm 1). Up next we present how Algorithm 1 can be converted into an \(\mathcal{O}(\log n)\)-regret online learning algorithm for general graphs, using the metric embedding technique of Theorem 2, resulting to our final algorithm (Algorithm 2).

```
1:Input: A sequence \(R_{1},\ldots,R_{T}\subseteq L(\mathcal{T})\).
2: The learner samples \(\alpha_{v}\sim\mathrm{Unif}(0,1)\) for all \(v\in V(\mathcal{T})\).
3:for each round \(t=1\)to\(T\)do
4: The learner places the \(k\) facilities to the leaves of the HST \(\mathcal{T}\) based on the output \(F_{t}:=\mathrm{Cut}\&\mathrm{Round}(\mathcal{T},y^{t},\alpha)\).
5: The learner learns \(R_{t}\subseteq L(\mathcal{T})\).
6: The learner updates \(y^{t+1}\in\mathcal{FP}(\mathcal{T})\) by running Algorithm 3 for Problem 2 with input \(R_{1},\ldots,R_{t}\).
7:endfor ```

**Algorithm 1**\(\mathcal{O}(1)\)-regret for HSTs.

**Theorem 5**.: _For any sequence of client requests \(R_{1},\ldots,R_{T}\subseteq L(\mathcal{T})\), the sequence of facility-placements \(F_{1},\ldots,F_{T}\subseteq L(\mathcal{T})\) produced by Algorithm 1 satisfies_

\[\mathbb{E}\left[\sum_{t=1}^{T}C_{R_{t}}(F_{t})+\gamma\cdot\sum_{t=2}^{T}M_{ \mathcal{T}}(F_{t},F_{t-1})\right]\leq 6\cdot\min_{|F^{*}|=k}\sum_{t=1}^{T}C_{R_{t}} (F^{*})+\beta\cdot\sqrt{T}\]

_for \(\beta=\mathcal{O}\left(k\cdot|L(\mathcal{T})|^{3/2}\cdot D_{\mathcal{T}}\cdot \max(\gamma,1)\right)\)._

Theorem 5 establishes that Algorithm 1 achieves constant regret in the special case of HSTs and its proof easily follows by Theorems 3 and 4. Then, the proof of Theorem 1 easily follows by Theorem 2 and Theorem 5. All the proofs are deferred to Section C of the Appendix.

## 4 \(\mathcal{O}(1)\)-Regret for Fractional HST Clustering

In this section we present the \(\mathcal{O}(1)\)-regret algorithm for Problem 2, described in Algorithm 3, and exhibit the key ideas in establishing Theorem 3. Without loss of generality, we can assume that the facility-weight satisfies \(\gamma\geq 1\)5.

Footnote 5: If not, establishing our guarantees for \(\gamma=1\) will clearly upper bound the actual moving cost.

Algorithm 3 is the well-known online learning algorithm _Follow the Regularized Leader_ (\(\mathrm{FTRL}\)) with a specific regularizer \(R_{\mathcal{T}}(\cdot)\) presented in Definition 9. Our results crucially rely on the properties of this regularizer since it takes into account the HST structure and permits us to bound the fractional moving cost of \(\mathrm{FTRL}\).

**Definition 9**.: _Given an HST \(\mathcal{T}\), the dilated entropic regularizer \(R_{\mathcal{T}}(y)\) over \(y\in\mathcal{FP}(\mathcal{T})\) is defined as_

\[R_{\mathcal{T}}(y):=\sum_{v\neq r}2^{\mathrm{lev}(v)}\cdot(y_{v}+\delta_{v}) \cdot\ln\left(\frac{y_{v}+\delta_{v}}{y_{p(v)}+\delta_{p(v)}}\right)\]

_where \(\delta_{v}:=(k/n)\cdot|L(\mathcal{T})\cap T(v)|\) and \(n:=|L(\mathcal{T})|\)._

```
1:Input: An adversarial sequence \(R_{1},\ldots,R_{T}\subseteq L(\mathcal{T})\).
2:for\(t=1\)to\(T\)do
3: The learner selects \(y^{t}\in\mathcal{FP}(\mathcal{T})\).
4: The learner suffers cost \(f_{R_{t}}(y^{t})+||y^{t}-y^{t-1}||_{\mathcal{T}}\).
5: The learner updates \(y^{t+1}\leftarrow\arg\min_{y\in\mathcal{FP}(\mathcal{T})}\left[\sum_{s=1}^{t}f _{R_{s}}(y)+(\gamma\sqrt{nT})\cdot R_{\mathcal{T}}(y)\right]\).
6:endfor ```

**Algorithm 3**\(\mathrm{FTRL}\) with dilated entropic regularization

Algorithm 3 selects at each step \(t\) the facility placement \(y^{t}\in\mathcal{FP}(\mathcal{T})\) that minimizes a convex combination of the total fractional connection cost for the sub-sequence \(R_{1},\ldots,R_{t-1}\) and \(R_{\mathcal{T}}(y)\). The regularization term ensures the stability of the output, which will result in a bounded fractional moving cost.

Analysis of Algorithm 3.Due to space limitations, all proofs are moved to Section D of the Appendix. The primary reason for the specific selection of the regularizer at Definition 9 is that \(R_{\mathcal{T}}(\cdot)\) is strongly convex with respect to the norm \(||\cdot||_{\mathcal{T}}\) of Definition 8, as established in Lemma 1 which is the main technical contribution of the section. We use \(D=D_{\mathcal{T}}\) for the diameter of \(\mathcal{T}\).

**Lemma 1**.: _For any vectors \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\),_

\[R_{\mathcal{T}}(y^{\prime})\geq R_{\mathcal{T}}(y)+\langle\nabla R_{\mathcal{ T}}(y),y^{\prime}-y\rangle+\left(8kD\gamma^{2}\right)^{-1}\cdot||y-y^{\prime}||_{ \mathcal{T}}^{2}\]

The strong convexity of \(R_{\mathcal{T}}(y)\) with respect to \(||\cdot||_{\mathcal{T}}\) is crucial since it permits us to bound the moving cost of Algorithm 3 by its fractional connection cost.

**Lemma 2**.: _For any sequence \(R_{1},\ldots,R_{T}\subseteq L(\mathcal{T})\), the output of Algorithm 3 satisfies_

\[\sum_{t=2}^{T}||y^{t}-y^{t-1}||_{\mathcal{T}}\leq\frac{1}{2}\cdot\sum_{t=1}^{ T}f_{R_{t}}(y^{t})+\mathcal{O}\left(\gamma kD\right)\cdot\sqrt{T}\]

We remark that using another regularizer \(R(\cdot)\) that is strongly convex with respect to another norm \(||\cdot||\) would still imply Lemma 1 with respect to \(||\cdot||\). The problem though is that the _fractional moving cost_\(\sum_{t=1}^{T}||y_{t}-y_{t-1}||\) can no longer be associated with the actual moving cost \(\sum_{t=1}^{T}M_{\mathcal{T}}(F_{t},F_{t-1})\). It is for this reason that using a regularizer that is strongly convex with respect to \(||\cdot||_{\mathcal{T}}\) is crucial.

Next, by adapting the standard analysis of \(\mathrm{FTRL}\) to our specific setting, we derive Lemma 3 establishing that Algorithm 3 admits bounded connection cost.

**Lemma 3**.: _For any sequence \(R_{1},\ldots,R_{T}\subseteq L(\mathcal{T})\), the output of Algorithm 3 satisfies_

\[\sum_{t=1}^{T}f_{R_{t}}(y^{t})\leq\min_{y^{*}\in\mathcal{FP}}\sum_{t=1}^{T}f_{ R_{t}}(y^{*})+\mathcal{O}\left(kn^{3/2}D\gamma\right)\cdot\sqrt{T}\]

The proof of Theorem 3 directly follows by Lemma 2 and 3. We conclude the section by presenting how Step \(5\) of Algorithm 3 can be efficiently implemented, namely

\[\min_{y\in\mathcal{FP}(\mathcal{T})}\Phi_{t}(y):=\sum_{s=1}^{t}f_{R_{s}}(y)+( \gamma\sqrt{nT})\cdot R_{\mathcal{T}}(y).\]

Since \(\Phi_{t}(y)\) is strongly convex and the set \(\mathcal{FP}(\mathcal{T})\) is a polytope, one could use standard optimization algorithms such as the _ellipsoid method_ or _projected gradient descent_ to approximately minimize\(\Phi_{t}(y)\) given access to a _sub-gradient oracle for \(\Phi_{t}(\cdot)\)_. In Claim 11 of Section D of the Appendix, we establish that the sub-gradients of \(\Phi(\cdot)\) can be computed in polynomial time and thus any of the previous methods can be used to approximately minimize \(\Phi(\cdot)\). In Lemma 4 we establish the intuitive fact that approximately implementing Step \(5\) does not affect the guarantees of Theorem 3.

**Lemma 4**.: _Let \(y^{t}\) be the minimizer of \(\Phi_{t}(\cdot)\) in \(\mathcal{FP}(T)\) and let \(z^{t}\in\mathcal{FP}(\mathcal{T})\) be any point such that \(\Phi_{t}(z^{t})\leq\Phi_{t}(y^{t})+\epsilon\) for some \(\epsilon=\mathcal{O}(T^{-1/2})\). Then,_

\[f_{R_{t}}(z^{t})+||z^{t}-z^{t-1}||_{\mathcal{T}}\leq f_{R_{t}}(y^{t})+||y^{t}- y^{t-1}||_{\mathcal{T}}+\mathcal{O}\left(kn^{3/2}D\gamma\right)\cdot T^{-1/2}\]

**Remark 3**.: _In our implementation of the algorithm, we approximately solve Step 5 of Algorithm 3 via Mirror Descent based on the Bregman divergence of \(\mathcal{R}_{\mathcal{T}}(\cdot)\). This admits the same convergence rates as projected gradient descent but the projection step can be computed in linear time with respect to the size of the HST \(\mathcal{T}\). We present the details of our implementation in Section C of the Appendix._

## 5 The Cut\(\&\)Round Rounding

In this section we present our novel rounding scheme (Algorithm \(\mathrm{Cut}\&\mathrm{Round}\)) as well as the main steps that are required in order to establish Theorem 4. To ease notation, for any real number \(x\geq 0\) we denote its decimal part as \(\delta(x)=x-\lfloor x\rfloor\). We comment that our rounding scheme simply maintains and updates a distribution over the vertices of the HST, and can be thus implemented in polynomial-time. Similar rounding schemes, like the one presented in [9], typically maintain a distribution over all possible facility-placements, which generally cannot be implemented in polynomial-time.

```
1:Input: An HST \(\mathcal{T}\), a fractional placement \(y\in\mathcal{FP}(\mathcal{T})\) and thresholds \(\alpha_{v}\in[0,1]\) for all \(v\in V(\mathcal{T})\).
2:\(Y_{r}\gets k\)
3:for levels \(\ell=h(\mathcal{T})\) to 1 do
4:for all nodes \(v\) with \(lev(v)=\ell\)do
5:\(Y_{rem}\gets Y_{v}\)
6:\(y_{rem}\gets y_{v}\)
7:for all children \(u\in\mathrm{cld}(v)\)do
8:\(Y_{u}\leftarrow\mathrm{Alloc}(y_{u},Y_{rem},y_{rem},\alpha_{u})\)
9:\(Y_{rem}\gets Y_{rem}-Y_{u}\)
10:\(y_{rem}\gets y_{rem}-y_{u}\)
11:endfor
12:endfor
13:endfor
14:return\(F:=\{u\in L(\mathcal{T}):Y_{u}=1\}\). ```

**Algorithm 4**\(\mathrm{Cut}\&\mathrm{Round}\).

On principle, \(\mathrm{Cut}\&\mathrm{Round}\) (Algorithm 4) assigns to each vertex \(v\) an integer number of facilities \(Y_{v}\) to be placed at the leaves of its sub-tree. Notice that due to sub-routine \(\mathrm{Alloc}\) (Algorithm 5), \(Y_{v}\) either equals \(\lfloor y_{v}\rfloor\) or \(\lfloor y_{v}\rfloor+1\). \(\mathrm{Cut}\&\mathrm{Round}\) initially assigns \(k\) facilities to the set of leaves that descend from the root \(r\), which is precisely \(L(\mathcal{T})\). Then, it moves in decreasing level order to decide \(Y_{v}\) for each node \(v\). Once \(Y_{v}\) is determined (Step 5), the \(Y_{v}\) facilities are allocated to the sub-trees of its children \(u\in\mathrm{cld}(v)\) (Steps 7-10) via sub-routine \(\mathrm{Alloc}\) using the thresholds \(\alpha_{u}\), in a manner that guarantees that \(Y_{v}=\sum_{u\in\mathrm{cld}(v)}Y_{u}\) (see Section E.1 of the Appendix). This implies the feasibility of \(\mathrm{Cut}\&\mathrm{Round}\), as exactly \(k\) facilities are placed in the leaves of \(\mathcal{T}\) at the end of the process.

Assuming that the set of thresholds \(\alpha_{v}\) is randomly drawn from the uniform distribution in \([0,1]\), sub-routine \(\mathrm{Alloc}\) (Algorithm 5) guarantees that \(Y_{v}\) either equals \(\lfloor y_{v}\rfloor\) or \(\lfloor y_{v}\rfloor+1\) while \(\mathbb{E}_{\alpha}\left[Y_{v}\right]=y_{v}\). This is formally captured in Lemma 5 and is crucial in the proof of Theorem 4.

**Lemma 5**.: _Consider Algorithm 4 given as input a vector \(y\in\mathcal{FP}(\mathcal{T})\) and random thresholds \(\alpha_{v}\sim\mathrm{Unif}(0,1)\). Then,_

\[Y_{v}=\left\{\begin{array}{ll}\left|y_{v}\right|&\text{with probability }1-\delta(y_{v})\\ \left|y_{v}\right|+1&\text{with probability }\delta(y_{v})\end{array}\right.\]

By coupling Lemma 5 with the HST structure we are able to establish Theorem 4. The proof is technically involved and thus deferred to Section E of the Appendix.

## 6 Conclusion

In this work, we designed the first polynomial-time online learning algorithm for _Online \(k\)-Clustering with Moving Costs_ that achieves \(\mathcal{O}(\log n)\)-regret with respect to the cost of the optimal _static_ facility placement, extending the results of Fotakis et al. [31] for the special case of \(\gamma=0\). The cornerstone of our approach was to realize and establish that \(\mathcal{O}(1)\)-regret is plausible for HST metrics. This was achieved through designing a dilated entropic regularizer to capture the structure of the HST and combine it with the FTRL algorithm, as well as designing a lossless (up to constant factors) rounding scheme that simultaneously works for both the connection and the moving cost. Both of these components where central towards acquiring constant regret on HSTs.

A interesting future direction is to investigate whether a polynomial-time online learning algorithm with \(\mathcal{O}(1)\)-regret for the problem is theoretically possible or not. Since the \(\mathcal{O}(\log n)\)-factor is inherently lost when using HST embeddings, this would require a significantly different approach to the one presented in this work. Finally, we comment that our current optimality guarantees are with respect to the optimal _static_ facility placement. Going beyond the notion of regret, an intriguing future direction is establishing guarantees with respect to the _optimal dynamic facility-placement_ that moves facilities from round to round by suffering the corresponding moving cost.

## Acknowledgements

This work was supported by the Swiss National Science Foundation (SNSF) under grant number \(200021\_205011\), by Hasler Foundation Program: Hasler Responsible AI (project number 21043) and Innovation project supported by Innosuisse (contract agreement 100.960 IP-ICT).

## References

* Agarwal et al. [2014] Alekh Agarwal, Daniel J. Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E. Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In _International Conference on Machine Learning_, 2014.
* Ailon [2014] Nir Ailon. Improved bounds for online learning over the permutahedron and other ranking polytopes. In _Proceedings of the 17th International Conference on Artificial Intelligence and Statistics_, AISTATS 2014, 2014.
* Alamdari and Shmoys [2017] Soroush Alamdari and David B. Shmoys. A bicriteria approximation algorithm for the k-center and k-median problems. In _Workshop on Approximation and Online Algorithms_, 2017.
* An et al. [2017] Hyung-Chan An, Ashkan Norouzi-Fard, and Ola Svensson. Dynamic facility location via exponential clocks. _ACM Trans. Algorithms_, 13(2):21:1-21:20, 2017.
* Anwar et al. [2022] Tarique Anwar, Surya Nepal, Cecile Paris, Jian Yang, Jia Wu, and Quan Z. Sheng. Tracking the evolution of clusters in social media streams. _IEEE Transactions on Big Data_, pages 1-15, 2022.
* Arya et al. [2001] Vijay Arya, Naveen Garg, Rohit Khandekar, Adam Meyerson, Kamesh Munagala, and Vinayaka Pandit. Local search heuristic for k-median and facility location problems. In _Proceedings of the Thirty-Third Annual ACM Symposium on Theory of Computing_, STOC '01, page 21-29. Association for Computing Machinery, 2001.
* Awerbuch and Kleinberg [2008] Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. _J. Comput. Syst. Sci._, 2008.

* [8] Maria-Florina Balcan and Avrim Blum. Approximation algorithms and online mechanisms for item pricing. In _ACM Conference on Electronic Commerce_, 2006.
* [9] Nikhil Bansal, Niv Buchbinder, Aleksander Madry, and Joseph Naor. A polylogarithmic-competitive algorithm for the k-server problem. In Rafail Ostrovsky, editor, _IEEE 52nd Annual Symposium on Foundations of Computer Science, FOCS 2011, Palm Springs, CA, USA, October 22-25, 2011_, pages 267-276. IEEE Computer Society, 2011.
* [10] Yair Bartal. Probabilistic approximation of metric spaces and its algorithmic applications. _Proceedings of 37th Conference on Foundations of Computer Science_, pages 184-193, 1996.
* [11] Avrim Blum and Carl Burch. On-line learning and the metrical task system problem. _Mach. Learn._, 39(1):35-58, 2000.
* [12] Sebastien Bubeck, Michael B. Cohen, Yin Tat Lee, James R. Lee, and Aleksander Madry. k-server via multiscale entropic regularization. In Ilias Diakonikolas, David Kempe, and Monika Henzinger, editors, _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, Los Angeles, CA, USA, June 25-29, 2018_, pages 3-16. ACM, 2018.
* [13] Moses Charikar and Sudipto Guha. Improved combinatorial algorithms for the facility location and k-median problems. In _Proceedings of the 40th Annual Symposium on Foundations of Computer Science_, FOCS '99. IEEE Computer Society, 1999.
* [14] Moses Charikar, Sudipto Guha, Eva Tardos, and David B. Shmoys. A constant-factor approximation algorithm for the k-median problem (extended abstract). In _Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing_, STOC '99, page 1-10. Association for Computing Machinery, 1999.
* 39th International Colloquium, ICALP 2012_, volume 7391 of _Lecture Notes in Computer Science_, pages 194-205. Springer, 2012.
* [16] Vincent Cohen-Addad, Benjamin Guedj, Varun Kanade, and Guy Rom. Online k-means clustering. In Arindam Banerjee and Kenji Fukumizu, editors, _The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event_, volume 130 of _Proceedings of Machine Learning Research_, pages 1126-1134. PMLR, 2021.
* [17] Aaron Cote, Adam Meyerson, and Laura J. Poplawski. Randomized k-server on hierarchical binary trees. In Cynthia Dwork, editor, _Proceedings of the 40th Annual ACM Symposium on Theory of Computing, Victoria, British Columbia, Canada, May 17-20, 2008_, pages 227-234. ACM, 2008.
* [18] Bart de Keijzer and Dominik Wojtczak. Facility reallocation on the line. In _Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018_, pages 188-194, 2018.
* Leibniz-Zentrum fur Informatik, 2017.
* [20] Sina Dehghani, MohammadTaghi Hajiaghayi, Hamid Mahini, and Saeed Seddighin. Price of competition and dueling games. _arXiv preprint arXiv:1605.04004_, 2016.
* [21] Miroslav Dudik, Nika Haghtalab, Haipeng Luo, Robert E. Schapire, Vasilis Syrgkanis, and Jennifer Wortman Vaughan. Oracle-efficient online learning and auction design. In _58th IEEE Annual Symposium on Foundations of Computer Science_, FOCS 2017, 2017.
* [22] Miroslav Dudik, Daniel J. Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. In _Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence_, UAI 2011, 2011.

- 41st International ColloquiumICALP 2014_, volume 8573 of _Lecture Notes in Computer Science_, pages 459-470. Springer, 2014.
* [24] Jittat Fakcharoenphol, Satish Rao, and Kunal Talwar. A tight bound on approximating arbitrary metrics by tree metrics. In _Proceedings of the Thirty-Fifth Annual ACM Symposium on Theory of Computing_, STOC '03, page 448-455, New York, NY, USA, 2003. Association for Computing Machinery.
* [25] Jittat Fakcharoenphol, Satish Rao, and Kunal Talwar. A tight bound on approximating arbitrary metrics by tree metrics. _J. Comput. Syst. Sci._, 69(3):485-497, 2004.
* [26] Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Optimistic regret minimization for extensive-form games via dilated distance-generating functions. In _Neural Information Processing Systems_, 2019.
* 13, 2021_, pages 2660-2678. SIAM, 2021.
* [28] Dimitris Fotakis, Loukas Kavouras, Panagiotis Kostopanagiotis, Philip Lazos, Stratis Skoulakis, and Nikos Zarifis. Reallocating multiple facilities on the line. In _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019_, pages 273-279, 2019.
* [29] Dimitris Fotakis, Loukas Kavouras, Grigorios Koumoutsos, Stratis Skoulakis, and Manolis Vardas. The online min-sum set cover problem. In _Proc. of the 47th International Colloquium on Automata, Languages and Programming_, ICALP 2020.
* [30] Dimitris Fotakis, Thanasis Lianeas, Georgios Piliouras, and Stratis Skoulakis. Efficient online learning of optimal rankings: Dimensionality reduction via gradient descent. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020_, 2020.
* [31] Dimitris Fotakis, Georgios Piliouras, and Stratis Skoulakis. Efficient online learning for dynamic k-clustering. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 3396-3406. PMLR, 18-24 Jul 2021.
* [32] Takahiro Fujita, Kohei Hatano, and Eiji Takimoto. Combinatorial online prediction via metarounding. In _24th International Conference on Algorithmic Learning Theory_, ALT 2013, 2013.
* [33] Dan Garber. Efficient online linear optimization with approximation algorithms. In _Proceedings of the 30th International Conference on Neural Information Processing Systems_, NIPS 2017, 2017.
* [34] Xiangyu Guo, Janardhan Kulkarni, Shi Li, and Jiayi Xian. Consistent k-median: Simpler, better and robust. In Arindam Banerjee and Kenji Fukumizu, editors, _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130 of _Proceedings of Machine Learning Research_, pages 1135-1143. PMLR, 13-15 Apr 2021.
* [35] Elad Hazan. Introduction to online convex optimization. _CoRR_, abs/1909.05207, 2019.
* [36] Elad Hazan, Wei Hu, Yuanzhi Li, and Zhiyuan Li. Online improper learning with an approximation oracle. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [37] Elad Hazan and Satyen Kale. Online submodular minimization. _J. Mach. Learn. Res._, 2012.
* [38] Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix prediction. In _25th Annual Conference on Learning Theory_, COLT 2012, 2012.

* [39] Elad Hazan and Tomer Koren. The computational power of optimization in online learning. In _Proceedings of the 48th Annual ACM Symposium on Theory of Computing_, STOC 2016, 2016.
* [40] David P. Helmbold, Robert E. Schapire, and M. Long. Predicting nearly as well as the best pruning of a decision tree. In _Machine Learning_, 1997.
* [41] David P. Helmbold and Manfred K. Warmuth. Learning permutations with exponential weights. In _Proceedings of the 20th Annual Conference on Learning Theory_, COLT 2007, 2007.
* [42] Nicole Immorlica, Adam Tauman Kalai, Brendan Lucier, Ankur Moitra, Andrew Postlewaite, and Moshe Tennenholtz. Dueling algorithms. In _Proceedings of the Forty-Third Annual ACM Symposium on Theory of Computing_, STOC '11, page 215-224, New York, NY, USA, 2011. Association for Computing Machinery.
* [43] Kamal Jain, Mohammad Mahdian, and Amin Saberi. A new greedy approach for facility location problems. In John H. Reif, editor, _Proceedings on 34th Annual ACM Symposium on Theory of Computing, May 19-21, 2002, Montreal, Quebec, Canada_, pages 731-740. ACM, 2002.
* [44] Kamal Jain and Vijay V. Vazirani. Approximation algorithms for metric facility location and k-median problems using the primal-dual schema and lagrangian relaxation. _J. ACM_, 48(2):274-296, 2001.
* [45] Stefanie Jegelka and Jeff A. Bilmes. Online submodular minimization for combinatorial structures. In _Proceedings of the 28th International Conference on Machine Learning_, ICML 2011, 2011.
* [46] Sham Kakade, Adam Tauman Kalai, and Katrina Ligett. Playing games with approximation algorithms. In _Proceedings of the 39th Annual ACM Symposium on Theory of Computing_, STOC 2007, 2007.
* [47] Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. In _J. Comput. Syst. Sci._ Springer, 2003.
* [48] Wouter M. Koolen, Manfred K. Warmuth, and Jyrki Kivinen. Hedging structured concepts. In _the 23rd Conference on Learning Theory, COLT 2010_, 2010.
* [49] Elias Koutsoupias. The k-server problem. _Comput. Sci. Rev._, 3(2):105-118, 2009.
* [50] Elias Koutsoupias and Christos H. Papadimitriou. On the k-server conjecture. _J. ACM_, 42(5):971-983, 1995.
* [51] Amit Kumar. Constant factor approximation algorithm for the knapsack median problem. In _Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms_, SODA '12, page 824-832. Society for Industrial and Applied Mathematics, 2012.
* [52] Amit Kumar, Yogish Sabharwal, and Sandeep Sen. Linear-time approximation schemes for clustering problems in any dimensions. _J. ACM_, 57(2), 2010.
* [53] Silvio Lattanzi and Sergei Vassilvitskii. Consistent k-clustering. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 1975-1984. PMLR, 2017.
* [54] Shi Li and Ola Svensson. Approximating k-median via pseudo-approximation. _SIAM J. Comput._, 45(2):530-547, 2016.
* 249, 1992.
* [56] M.E.J. Newman. The structure and function of complex networks. _SIAM review_, 45(2):167-256, 2003.
* [57] Romualdo Pastor-Satorras and Alessandro Vespignani. Epidemic Spreading in Scale-Free Networks. _Physical Review Letters_, 86(14):3200-3203, 2001.

* [58] Holakou Rahmanian and Manfred K. Warmuth. Online dynamic programming. In _NIPS_, 2017.
* [59] Juliette Stehle, Nicolas Voirin, Alain Barrat, Ciro Cattuto, Lorenzo Isella, Jean-Francois Pinton, Marco Quaggiotto, Wouter Van den Broeck, Corinne Regis, Bruno Lina, and Philippe Vanhems. High-resolution measurements of face-to-face contact patterns in a primary school. _PLOS ONE_, 6(8), 2011.
* [60] Matthew J. Streeter and Daniel Golovin. An online algorithm for maximizing submodular functions. In _22nd Annual Conference on Neural Information Processing Systems_, NIPS 2008, 2008.
* [61] Daiki Suehiro, Kohei Hatano, Shuji Kijima, Eiji Takimoto, and Kiyohito Nagano. Online prediction under submodular constraints. In _Algorithmic Learning Theory_, ALT 2012, 2012.
* [62] Eiji Takimoto and Manfred K. Warmuth. Predicting nearly as well as the best pruning of a planar decision graph. In _Theoretical Computer Science_, 2000.
* [63] Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. _J. Mach. Learn. Res._, 2003.
* [64] Chayant Tantipathananandh, Tanya Berger-Wolf, and David Kempe. A framework for community identification in dynamic social networks. In _Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, KDD '07, page 717-726. Association for Computing Machinery, 2007.
* [65] David P. Williamson and David B. Shmoys. _The Design of Approximation Algorithms_. Cambridge University Press, USA, 1st edition, 2011.
* [66] Shota Yasutake, Kohei Hatano, Shuji Kijima, Eiji Takimoto, and Masayuki Takeda. Online linear optimization over permutations. In _Proceedings of the 22nd International Conference on Algorithms and Computation_, ISAAC 2011, 2011.
* [67] Neal E. Young. K-medians, facility location, and the chernoff-wald bound. In _Proceedings of the Eleventh Annual ACM-SIAM Symposium on Discrete Algorithms_, SODA '00, page 86-95. Society for Industrial and Applied Mathematics, 2000.

## Appendix A Further Related Work

In this chapter of the appendix, we continue our discussion on the literature that relates to this work.

**Efficient Combinatorial Online Learning.** There exists a long line of research studying efficient online learning algorithms in various combinatorial domains (e.g., selection of paths, permutations, binary search trees etc.) [40, 62, 63, 41, 7, 60, 45, 22, 42, 66, 38, 37, 1, 2, 20, 30, 29]. Another related line of work studies _black-box reductions_ converting any \(\alpha\)-approximation (offline) algorithm to an \(\mathcal{O}(\alpha)\)-regret online learning algorithm for a specific class of combinatorial optimization problems called _linear optimization problems_[47, 8, 46, 48, 61, 32, 39, 58, 21, 33, 36]. We remark that a key difference of our setting with the aforementioned works is that in the latter case the learner is not penalized for switching actions from round to round with an additional moving/switching cost. In the context of Problem 1 this means that \(\gamma=0\) which is exactly the setting considered by [31]. As a result, apart from the fact that \(k\)-median does not belong in the class of _linear optimization problems_, the aforementioned _black-box reductions_ do not apply to Problem 1 since they do not account for the moving cost.

**The k-server Problem.** Our work also relates with the rich line of literature on the \(k\)-server problem [50, 17, 49, 9, 19, 12]. In this setting there exists only \(1\) client at each round, while \(1\)-lookahead is assumed, i.e. the request \(R_{t}\) is revealed prior to the action of the algorithm at step \(t\). Moreover in \(k\)-server a facility must be placed in the exact position of the request, leading to a simpler combinatorial structure with respect to Problem 16. However, in the \(k\)-server problem, instead of using the benchmark of _regret_, the more challenging metric of _competitive ratio_ that measures the sub-optimality with respect to the _optimal dynamic solution_ is used. Mostly related to ours is the work of [9] providing the first \(\mathrm{poly}(\log n)\)-competitive algorithm for \(k\)-server by reducing the problem to the special case of HSTs. [9] first design a \(\mathrm{poly}(\log n)\)-competitive algorithm for a fractional version of \(k\)-server at which facilities can be fractionally placed into the vertices of the HST. They then use a randomized rounding scheme to convert the fractional solution into an integral one. The basic difference of the randomized rounding scheme of [9] with the one that we introduce in this work (Algorithm \(\mathrm{Cut\&Round}\)) is that the first provides guarantees only for the moving cost of the facilities while \(\mathrm{Cut\&Round}\) provides guarantees both for the moving cost of the facilities as well as the connection cost of the clients.

Footnote 6: Given offline access to the sequence of requests, the optimal solution for the \(k\)-server can be computed in polynomial-time while the optimal static solution of Problem 1 cannot be approximated in polynomial-time with ratio less than \((1+2/e)\) even under _a-priori_ knowledge of the request sequence (inapproximability of \(k\)-median).

**Consistent \(k\)-Clustering.** Another setting of clustering in the presence of unknown clients is that of _Consistent \(k\)-Clustering_[53, 34, 27]. In this setting, given an _unknown stream of clients_, a set of \(k\) facilities has to be maintained over time so that at any round \(t\), the selected facilities form an approximately optimal solution of the sub-instance consisting of clients appeared in the time interval \(\{1,t\}\). A basic difference of Consistent \(k\)-Clustering with Problem 1 is that in the first case the moving cost is not penalized as long as the number of swaps does not exceed a certain threshold (\(\mathcal{O}(k)\)).

Proof of Theorem 2

In this chapter of the appendix we briefly discuss the details behind Theorem 2 and show how the results of [10] and [24] hold even for the specific definition of HSTs we have considered in Definition 5.

Traditionally, HSTs are not required to be balanced nor are required to have weights that are specifically powers of \(2\). In fact, the seminal work of [10], later improved by [24], states that there exists a randomized procedure such that for every weighted graph \(G(V,E,w)\), it constructs (in polynomial-time) a tree \(\mathcal{T}\) such that:

1. There exists a perfect matching \(\sigma:V\mapsto L(\mathcal{T})\) that maps the vertices of \(G\) to the leaves of \(\mathcal{T}\).
2. For any vertices \(i,j\in V\), their corresponding distance on \(\mathcal{T}\) can only increase, i.e. \(d_{G}(i,j)\leq d_{\mathcal{T}}(\sigma(i),\sigma(j))\).
3. On expectation, distances between vertices are distorted only by a logarithmic factor, i.e. \(\mathbb{E}\left[d_{\mathcal{T}}(\sigma(i),\sigma(j))\right]\leq\mathcal{O}( \log|V|)\cdot d_{G}(i,j)\)
4. The weight of any edge \(e=(v,u)\) between a vertex \(v\in V(\mathcal{T})\) and its parent vertex \(u\) is precisely \(diam(G)\cdot 2^{-dpt(v)}\).
5. The height of \(\mathcal{T}\) satisfies \(h(\mathcal{T})\leq\lceil\log\left(diam(G)\right)\rceil\).

The purpose of this section is to argue that one can easily transform such a tree \(\mathcal{T}\) to match our notion of HSTs (Definition 5), while maintaining the same guarantees for the distortion of the distances. Recall that we have already assumed that the minimum edge weight of \(G\) is 1, i.e. \(\min_{e\in E}w_{e}=1\). Furthermore, we can also assume without loss of generality that the diameter of \(G\) is a power of 2; if not, simple scaling arguments suffice to transform \(G\) into such a graph by only distorting distances by a constant factor. Thus, we assume that \(diam(G)=2^{d}\) for some \(d\geq 0\).

We start from the tree \(\mathcal{T}\) that the algorithm of [24] generates. Recall that by definition, the weight of an edge \(e=(i,j)\) between some vertex \(i\) and its parent node \(j\) is \(2^{d-dpt(i)}\). In order to balance the tree, we take each leaf vertex \(u\in L(\mathcal{T})\) at depth \(dpt(u)\) and extend it downwards by adding new vertices until it reaches a new depth \(dpt^{\prime}(u)=d\). For every new edge that we add during this process, we maintain that the weight of the edge \(e=(i,j)\) from \(i\) to its parent \(j\) is \(diam(G)\cdot 2^{-dpt(i)}\).

Let \(\mathcal{T}^{\prime}\) be used to denote our modified tree. Clearly, the above construction guarantees \(h(\mathcal{T}^{\prime})=d\). Since by definition \(h(\mathcal{T})\leq\lceil\log\left(diam(G)\right)\rceil=d\), we know that all leaves initially lied at depth at most \(d\), and thus by the end of the above process all leaves will lie at the same level of the tree and have depth \(d\). Thus, we have indeed constructed a balanced tree. Furthermore, since by definition \(dpt(v)=h(\mathcal{T})-lev(v)\), we get that the weight of the edge \(e=(i,j)\) from \(i\) to its parent \(j\) is \(w_{e}=diam(G)\cdot 2^{lev(i)-d}=2^{lev(i)}\). So, the constructed tree indeed satisfies all the requirements of Definition 5 and is a valid HST (according to our definition).

We will now argue that \(\mathcal{T}^{\prime}\) also satisfies all items of Theorem 2. Fist of all, the height of our new tree is precisely \(d\), and thus it is true that \(h(\mathcal{T}^{\prime})\leq\lceil\log\left(diam(G)\right)\rceil\). Furthermore, since we only added edges to the initial tree \(\mathcal{T}\), the distance between any two leaves can only increase. Thus, we get that for any vertices \(i,j\in V\) it holds

\[d_{G}(i,j)\leq d_{\mathcal{T}}(i,j)\leq d_{\mathcal{T}^{\prime}}(i,j)\]

Finally, it remains to upper bound the expected distortion on \(\mathcal{T}^{\prime}\). Recall that by construction of [24], we know that

\[\mathbb{E}\left[d_{\mathcal{T}}(\sigma(i),\sigma(j))\right]\leq\mathcal{O}( \log|V|)\cdot d_{G}(i,j)\]

Since edge lengths decrease by a factor of \(2\) every time we move down the tree, we know that the total length of the path we added in order to move leaf \(i\) from depth \(dpt(i)\) to depth \(d\) is precisely \(1+2+\ldots 2^{dpt(i)-1}\leq 2^{dpt(i)}\). This implies that any distance on \(\mathcal{T}^{\prime}\) can be at most twice the corresponding distance on \(\mathcal{T}\), i.e.

\[d_{\mathcal{T}^{\prime}}(\sigma(i),\sigma(j))\leq 2\cdot d_{\mathcal{T}}( \sigma(i),\sigma(j))\]

which completes the proof.

Proofs of Section 3

In this chapter of the appendix we present all the omitted proofs from Section 3 concerning the basic algorithmic primitives we use in order to establish our main result in Theorem 1.

Roadmap.In section C.1 we establish the connection between Problems 1 and 2 and show that our notion of fractional connection and moving cost collapses with our initial definitions in the case of integral facility placements. Then, in section C.2 we present the proof of Theorem 5 and in section C.3 we present the proof of Theorem 1.

### Establishing the relation between Problems 1 and 2

Fix any HST \(\mathcal{T}\) and let \(\mathcal{FP}(\mathcal{T})\) be the corresponding set of fractional facility placements. In this section, we will establish that in the case of integral facility placements \(y\in\mathcal{FP}(\mathcal{T})\cap\mathbb{N}\), the notions of fractional connection cost and fractional moving cost (formally stated in Definitions 7 and 8) collapse to the notions of actual connection and moving costs (formally stated in Definitions 1 and 2) respectively.

Let \(y\in\mathcal{FP}(\mathcal{T})\cap\mathbb{N}\) be an integral facility placement. Then, by definition, for each leaf \(v\in L(\mathcal{T})\) we have \(y_{v}\in\{0,1\}\) facilities that are placed on it, and the total amount of placed facilities is \(k\), i.e. \(\sum_{v\in L(\mathcal{T})}y_{v}=k\). Thus, we can associate with any integral facility placement \(y\) a corresponding set

\[F(y)=\{v\in L(\mathcal{T}):y_{v}=1\}\]

such that \(|F(y)|=k\), meaning that \(F(y)\) is a valid facility placement of the leaves of the \(\mathcal{T}\).

In Claim 1 we will establish that for any set of clients, the connection cost under \(F(y)\) is equal to the fractional connection cost under \(y\). Then, in Claim 2 we will establish that the fractional moving cost between \(y\) and \(y^{\prime}\) gives us precisely the moving cost between facility placements \(F(y)\) and \(F(y^{\prime})\) on \(\mathcal{T}\).

**Claim 1**.: _For any integral facility placement \(y\in\mathcal{FP}(\mathcal{T})\cap\mathbb{N}\) and any set of clients \(R\subseteq L(\mathcal{T})\), it holds that_

\[f_{R}(y)=C_{R}(F(y))\]

Proof.: Fix any \(y\in\mathcal{FP}(\mathcal{T})\cap\mathbb{N}\) and any \(R\subseteq L(\mathcal{T})\). By definition of the connection cost (Definition 1), we have

\[C_{R}(F)=\sum_{j\in R}\min_{i\in F(y)}d_{\mathcal{T}}(i,j)\]

Let's fix a particular client that lies on some leaf \(j\in L(\mathcal{T})\) of \(\mathcal{T}\). Let \(i^{*}=\arg\min_{i\in F(y)}d_{\mathcal{T}}(i,j)\) be the leaf closest to \(j\) that \(F(y)\) places a facility into. Since \(\mathcal{T}\) is an HST and distances increase by a factor of \(2\) as we move up the tree, it is not hard to see that \(i^{*}\) is the leaf in \(F(y)\) whose _lowest common ancestor_ (lca) with \(j\) has the smallest level. Let \(l^{*}=lca(j,i^{*})\). Equivalently, \(l^{*}\) is the minimum-level vertex in \(P(j,r)\) such that \(y_{l^{*}}\geq 1\). Since \(\mathcal{T}\) is balanced, we have that the connection cost of client \(j\) under \(F(y)\) is precisely

\[C_{\{j\}}(F(y))=2\cdot d_{\mathcal{T}}(j,l^{*})=2\cdot\sum_{l=0}^{\mathrm{ lev}(l^{*})-1}2^{l}\]

and since by integrality we have that \(y_{v}=0\) for any \(v\in P(j,l^{*})\setminus\{l^{*}\}\) and \(y_{v}\geq 1\) for all \(v\in P(l^{*},r)\), we have

\[C_{\{j\}}(F)=2\cdot\sum_{v\in P(j,r)}2^{\mathrm{lev}(v)}\cdot\max(0,1-y_{v})\]

Summing over all clients \(j\in R\) we get

\[C_{R}(F(y))=\sum_{j\in R}\sum_{v\in P(j,r)}2^{\mathrm{lev}(v)+1}\cdot\max(0,1- y_{v})=f_{R}(y)\]

which concludes the proof.

**Claim 2**.: _For any integral facility placements \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\cap\mathbb{N}\), it holds that_

\[||y-y^{\prime}||_{\mathcal{T}}=\gamma\cdot M_{\mathcal{T}}(F(y),F(y^{\prime}))\]

Proof.: Fix any two integral facility placements \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\cap\mathbb{N}\). By definition of the moving cost (Definition 2), we have that

\[M_{\mathcal{T}}(F(y),F(y^{\prime}))=\min_{\sigma\in\Sigma}\sum_{i\in F(y)}d_{ \mathcal{T}}(i,\sigma(i))\]

where \(\Sigma\) is the set of all possible matchings from the facilities in \(F(y)\) to the facilities in \(F(y^{\prime})\).

In general graphs, the minimum transportation cost can have a very complicated structure and typically requires solving a minimum transportation problem in order to compute it. However, in the special case of HSTs, we are actually able to obtain a very simple expression for this quantity.

Recall that in an HST \(\mathcal{T}\), edge weights increase by a factor of \(2\) every time we move up a level on the tree. Thus, it is always in out interest to move facilities between leaves whose lowest common ancestor is as low as possible. In other words, the matching \(\sigma\) that minimizes the transportation cost from \(F(y)\) to \(F(y^{\prime})\) can be obtained by selecting an arbitrary leaf in \(F(y)\), matching it to the leaf in \(F(y^{\prime})\) with which it shares the _lowest_ lowest common ancestor and then repeating the process for the rest of the leaves.

Now fix any vertex \(v\in V(\mathcal{T})\). Recall that \(y_{v}\) is equal to the number of facilities in \(F(y)\) that are placed in the descendant leaves of \(v\) (respectively for \(y^{\prime}_{v}\)). Thus, if we apply the above (optimal) transportation plan, the number of facilities that will end up traversing the edge from \(v\) to its parent vertex is going to be precisely \(|y_{v}-y^{\prime}_{v}|\). Since the weight of this edge is by definition \(2^{\mathrm{lev}(v)}\), we get that

\[M_{\mathcal{T}}(F(y),F(y^{\prime}))=\sum_{v\in V(\mathcal{T})}2^{\mathrm{lev} (v)}\cdot|y_{v}-y^{\prime}_{v}|\]

and since

\[||y-y^{\prime}||_{\mathcal{T}}=\gamma\cdot\sum_{v\in V(\mathcal{T})}2^{ \mathrm{lev}(v)}\cdot|y_{v}-y^{\prime}_{v}|\]

we have proven the claim.

### Proof of Theorem 5

We will now formally present the proof of Theorem 5, bounding the expected total cost of Algorithm 1. Fix any sequence of clients \(R_{1},\ldots,R_{T}\). Since the random seed \(\alpha\) is selected uniformly at random (Step \(3\) of Algorithm 1), by Item \(1\) of Theorem 4 we get that

\[\mathbb{E}\left[C_{R_{t}}(F_{t})\right]=f_{R_{t}}(y^{t})\]

Moreover since the same random seed \(\alpha\) is used at all rounds \(t\geq 1\), Item \(2\) of Theorem 4 implies that

\[\gamma\cdot\mathbb{E}\left[M_{\mathcal{T}}(F_{t+1},F_{t})\right] \leq 4\cdot\|y^{t+1}-y^{t}||_{\mathcal{T}}\]

Thus,

\[\mathbb{E}\left[\sum_{t=1}^{T}C_{R_{t}}(F_{t})+\gamma\cdot\sum_{ t=2}^{T}M_{\mathcal{T}}(F_{t},F_{t-1})\right] \leq 4\cdot\left(\sum_{t=1}^{T}f_{R_{t}}(y^{t})+\sum_{t=2}^{T}||y^ {t}-y^{t-1}||_{\mathcal{T}}\right)\] \[\leq 6\cdot\min_{y^{*}\in\mathcal{FP}}\sum_{t=1}^{T}f_{R_{t}}(y^{* })+\beta\cdot\sqrt{T}\]

where the last inequality follows by Theorem 3 for \(\beta=\mathcal{O}\left(k\cdot|L(\mathcal{T})|^{3/2}\cdot D_{\mathcal{T}}\cdot \max(\gamma,1)\right)\). The proof is concluded by the fact that

\[\min_{y^{*}\in\mathcal{FP}}\sum_{t=1}^{T}f_{R_{t}}(y^{*})\leq\min_{|F^{*}|=k} \sum_{t=1}^{T}C_{R_{t}}(F^{*})\]

which is established in Claim 1 of Appendix C.1, stating that for any placement of \(k\)-facilities \(F\subseteq L(\mathcal{T})\) there exists a corresponding \(y\in\mathcal{FP}(\mathcal{T})\) whose fractional connection cost is equal to \(F\)'s under any client request.

### Proof of Theorem 1

We will now formally present the proof of Theorem 1, bounding the regret of Algorithm 2.

Let \(\mathcal{T}\) be the HST that we randomly embed our graph \(G(V,E,w)\) into. Since \(V=L(\mathcal{T})\), we slightly abuse notation and use \(u\) to refer both to some vertex of \(G\) and to the corresponding leaf of \(\mathcal{T}\). From Theorem 5, we know that the output of Algorithm 1 satisfies

\[\mathbb{E}\left[\sum_{t=1}^{T}C_{R_{t}}^{\mathcal{T}}(F_{t})+ \gamma\cdot\sum_{t=2}^{T}M_{\mathcal{T}}(F_{t},F_{t-1})\right]\leq 6\cdot\min_{|F^{*}|=k}\sum_{t=1}^{T}C_{R_{t}}^{\mathcal{T}}(F^{*})\] \[+\mathcal{O}\left(k\cdot|L(\mathcal{T})|^{3/2}\cdot D_{\mathcal{ T}}\cdot\max(1,\gamma)\right)\cdot\sqrt{T}\]

where we use \(\mathcal{T}\) in the connection and moving cost to indicate that all distances are measured on the HST. Here, the expectation is taken over the random choices of Algorithm 1.

Next, notice that both the connection cost and the moving cost are defined as sum of distances. Thus, the results of Theorem 2 about the distance distortion from \(G\) to \(\mathcal{T}\) clearly apply for these quantities as well, namely

\[C_{R_{t}}^{G}(F_{t})\leq C_{R_{t}}^{\mathcal{T}}(F_{t})\text{ and }\mathbb{E} \left[C_{R_{t}}^{\mathcal{T}}(F_{t})\right]\leq\mathcal{O}(\log|V|)\cdot C_{R _{t}}^{G}(F_{t})\]

and

\[M_{G}(F_{t},F_{t-1})\leq M_{\mathcal{T}}(F_{t},F_{t-1})\text{ and }\mathbb{E} \left[M_{\mathcal{T}}(F_{t},F_{t-1})\right]\leq\mathcal{O}(\log|V|)\cdot M_{G }(F_{t},F_{t-1})\]

Thus, taking an expectation over the randomness of \(\mathcal{T}\), we finally get that

\[\mathbb{E}\left[\sum_{t=1}^{T}C_{R_{t}}^{G}(F_{t})+\gamma\cdot \sum_{t=2}^{T}M_{G}(F_{t},F_{t-1})\right] \leq\mathcal{O}(\log|V|)\cdot\min_{|F^{*}|=k}\sum_{t=1}^{T}C_{R _{t}}^{G}(F^{*})\] \[+\mathcal{O}\left(k\cdot|L(\mathcal{T})|^{3/2}\cdot D_{\mathcal{ T}}\cdot\max(1,\gamma)\right)\cdot\sqrt{T}\]

Let \(n=|V|\) and \(D=diam(G)\). From the above, we get that Algorithm 2 is indeed \(\alpha\)-regret for \(\alpha=\mathcal{O}(\log n)\). Furthermore, we have that \(|L(\mathcal{T})|=|V|=n\), and \(D_{\mathcal{T}}=2\cdot(2^{h(\mathcal{T})}-1)\leq 4D\) since \(h(\mathcal{T})\leq\lceil\log D\rceil\). Thus, setting \(\beta=\mathcal{O}(k\cdot n^{3/2}\cdot D\cdot\max(1,\gamma))\), we get that Algorithm 2 has \(\beta\)-additive regret, completing the proof of Theorem 1.

Analysis of FTRL (Proofs of Section 4)

In this chapter of the appendix we present all the omitted proofs from Section 4 concerning our analysis of the _Follow the Regularized Leader_ (\(\mathrm{FTRL}\)) algorithm (Algorithm 3). To avoid repetition, from now on we fix an arbitrary HST \(\mathcal{T}\) and use \(\mathcal{FP}(\mathcal{T})\) to denote the set of all fractional placements of \(k\) facilities on the leaves of \(\mathcal{T}\). We use \(n=|L(\mathcal{T})|\) to denote the number of leaves of \(\mathcal{T}\), \(h=h(\mathcal{T})\) to denote its height and \(D=diam(\mathcal{T})\) to denote its diameter. Since \(\mathcal{T}\) is an HST, we know that its diameter \(D\), i.e. the maximum distance between any two leaves, is precisely \(D=2\cdot(2^{h}-1)\).

To ease notation, let \(w_{v}=2^{\mathrm{lev}(v)}\). For convenience, we remind the reader that our regularizer function \(R_{\mathcal{T}}:\mathcal{FP}(\mathcal{T})\mapsto\mathbb{R}\) is defined as

\[R_{\mathcal{T}}(y)=\sum_{v\neq r}w_{v}\cdot(y_{v}+\delta_{v})\cdot\ln\left( \frac{y_{v}+\delta_{v}}{y_{p(v)}+\delta_{p(v)}}\right)\]

where \(\delta_{v}=k\cdot|L(\mathcal{T})\cap T(v)|/|L(\mathcal{T})|\) is the percentage of leaves that lie on the sub-tree rooted at vertex \(v\) multiplied by \(k\) and \(p(v)\) is the parent of node \(v\). Also, recall that for any \(y\in\mathcal{FP}(\mathcal{T})\) we have defined the norm

\[||y||_{\mathcal{T}}=\gamma\cdot\sum_{v\in V(\mathcal{T})}w_{v}|y_{v}|\]

Roadmap.In Section D.1 we prove Lemma 1, namely the strong convexity of \(R_{\mathcal{T}}\) with respect to \(||\cdot||_{\mathcal{T}}\). Then, in Section D.2 we bound the moving cost of \(\mathrm{FTRL}\), proving Lemma 2. Next, in Section D.3 we bound the connection cost cost of \(\mathrm{FTRL}\), proving Lemma 3. Finally, in Section D.4 we account for approximation errors in the computation of the regularized leader, proving Lemma 4.

### Strong Convexity (Proof of Lemma 1)

The objective of this section is to prove Lemma 1, specifically that for any fractional facility placements \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\) it holds that

\[R_{\mathcal{T}}(y^{\prime})\geq R_{\mathcal{T}}(y)+\langle\nabla R_{\mathcal{ T}}(y),y^{\prime}-y\rangle+\alpha||y-y^{\prime}||_{\mathcal{T}}^{2}\]

where \(\alpha=(8kD\gamma^{2})^{-1}\).

We begin by computing the gradient of \(R_{\mathcal{T}}\) on any fractional facility placement \(y\in\mathcal{FP}(\mathcal{T})\).

**Claim 3**.: _The partial derivatives of \(R_{\mathcal{T}}\) on any point \(y\in\mathcal{FP}(\mathcal{T})\) are given by_

\[\frac{\partial R_{\mathcal{T}}(y)}{\partial y_{v}}=\left\{\begin{array}{ll}- \frac{w_{v}}{2}&\text{for }v=r\\ w_{v}\cdot\ln\left(\frac{y_{v}+\delta_{v}}{y_{p(v)}+\delta_{p(v)}}\right)+w_{v} &\text{for }v\in L(\mathcal{T})\\ w_{v}\cdot\ln\left(\frac{y_{v}+\delta_{v}}{y_{p(v)}+\delta_{p(v)}}\right)+ \frac{w_{v}}{2}&\text{for }v\notin L(\mathcal{T})\cup\{r\}\end{array}\right.\]

Proof.: Clearly, \(R_{\mathcal{T}}\) is well-defined and differentiable on \(\mathcal{FP}(\mathcal{T})\). For any \(v\neq r\), we compute the partial derivatives of \(R_{\mathcal{T}}(y)\) to obtain

\[\frac{\partial R_{\mathcal{T}}(y)}{\partial y_{v}}=w_{v}\cdot\ln\left(\frac{y _{v}+\delta_{v}}{y_{p(v)}+\delta_{p(v)}}\right)+w_{v}-\sum_{v\in\mathrm{cld}( u)}w_{u}\cdot\frac{y_{u}+\delta_{u}}{y_{v}+\delta_{v}}\]

Since \(y\in\mathcal{FP}(\mathcal{T})\), we know \(y_{v}=\sum_{u\in\mathrm{cld}(v)}y_{u}\) and by definition, \(\delta_{v}=\sum_{u\in\mathrm{cld}(v)}\delta_{u}\). Finally, recall that \(w_{u}=w_{v}/2\) for any \(u\in\mathrm{cld}(v)\). By plugging everything in we get

\[\frac{\partial R_{\mathcal{T}}(y)}{\partial y_{v}}=w_{v}\cdot\ln\left(\frac{y _{v}+\delta_{v}}{y_{p(v)}+\delta_{p(v)}}\right)+w_{v}-\frac{w_{v}}{2}\cdot \mathbbm{1}[v\notin L(\mathcal{T})]\]

for any \(v\neq r\). For the root vertex, using similar arguments we get

\[\frac{\partial R_{\mathcal{T}}(y)}{\partial y_{r}}=-\frac{w_{r}}{2}\]Now that we have calculated the gradient of \(R_{\mathcal{T}}\), we can substitute it into the definition of strong convexity. Specifically, by Claim 3, Lemma 1 states that

\[\sum_{v\neq r}w_{v}\cdot(y^{\prime}_{v}+\delta_{v})\cdot\ln\left(\frac{\frac{y^{ \prime}_{v}+\delta_{v}}{y^{\prime}_{p(v)}+\delta_{p(v)}}}{\frac{y_{v}+\delta_{v }}{y_{p(v)}+\delta_{p(v)}}}\right)\geq\frac{1}{8kD\gamma^{2}}\cdot||y^{\prime} -y||_{\mathcal{T}}^{2}\] (1)

To ease the presentation, we define quantities

\[f(y^{\prime},y)=\sum_{v\neq r}w_{v}\cdot(y^{\prime}_{v}+\delta_{v})\cdot\ln \left(\frac{\frac{y^{\prime}_{v}+\delta_{v}}{y^{\prime}_{p(v)}+\delta_{p(v)}}} {\frac{y_{v}+\delta_{v}}{y_{p(v)}+\delta_{p(v)}}}\right)\]

and

\[h(y^{\prime},y)=\sum_{v\neq r}w_{v}\cdot(y_{p(v)}+\delta_{p(v)})\cdot|\frac{y ^{\prime}_{v}+\delta_{v}}{y^{\prime}_{p(v)}+\delta_{p(v)}}-\frac{y_{v}+\delta _{v}}{y_{p(v)}+\delta_{p(v)}}|\]

We will prove that \(f(y^{\prime},y)\geq(1/2kD)\cdot h^{2}(y^{\prime},y)\) and that \(h(y^{\prime},y)\geq(1/2\gamma)\cdot||y^{\prime}-y||_{\mathcal{T}}\) in Claims 4 and 5 respectively. Combining these claims, equation (1) clearly holds, completing the proof of Lemma 1.

**Claim 4**.: _For any \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\), it holds that \(f(y^{\prime},y)\geq\frac{1}{2kD}\cdot\left(h(y^{\prime},y)\right)^{2}\)._

Proof.: We begin by establishing some notation. For any \(v\neq r\), let

\[\mu^{\prime}_{v}=w_{v}\cdot(y^{\prime}_{p(v)}+\delta_{p(v)})\cdot\frac{y^{ \prime}_{v}+\delta_{v}}{y^{\prime}_{p(v)}+\delta_{p(v)}}\]

and

\[\mu_{v}=w_{v}\cdot(y^{\prime}_{p(v)}+\delta_{p(v)})\cdot\frac{y_{v}+\delta_{v }}{y_{p(v)}+\delta_{p(v)}}.\]

Then, we have that

\[f(y^{\prime},y) =\sum_{v\neq r}\mu^{\prime}_{v}\cdot\ln\left(\frac{\mu^{\prime}_ {v}}{\mu_{v}}\right)\] \[=\sum_{v\in I}\mu^{\prime}_{v}\cdot\ln\left(\frac{\mu^{\prime}_ {v}}{\mu_{v}}\right)+\sum_{v\in I}\mu^{\prime}_{v}\cdot\ln\left(\frac{\mu^{ \prime}_{v}}{\mu_{v}}\right)\]

where \(I=\{v\neq r:\mu^{\prime}_{v}\geq\mu_{v}\}\) and \(I^{\prime}=\{v\neq r:\mu^{\prime}_{v}<\mu_{v}\}\). By applying the log-sum inequality in both of these terms, we obtain

\[f(y^{\prime},y)\geq(\sum_{v\in I}\mu^{\prime}_{v})\cdot\ln\left(\frac{\sum_{v \in I}\mu^{\prime}_{v}}{\sum_{v\in I}\mu_{v}}\right)+(\sum_{v\in I^{\prime}} \mu^{\prime}_{v})\cdot\ln\left(\frac{\sum_{v\in I^{\prime}}\mu^{\prime}_{v}}{ \sum_{v\in I^{\prime}}\mu_{v}}\right)\]

Next, observe that

\[\sum_{v\neq r}\mu^{\prime}_{v}=\sum_{v\neq r}w_{v}\cdot(y^{\prime}_{v}+\delta _{v})=2k\cdot(2^{h}-1)=k\cdot D\]

and also

\[\sum_{v\neq r}\mu_{v} =\sum_{v\neq r}w_{v}\cdot(y^{\prime}_{p(v)}+\delta_{p(v)})\cdot \frac{y_{v}+\delta_{v}}{y_{p(v)}+\delta_{p(v)}}\] \[=\sum_{v\notin L(\mathcal{T})}\left(\frac{w_{v}}{2}\cdot(y^{\prime }_{v}+\delta_{v})\cdot\sum_{u\in cl(v)}\frac{y_{u}+\delta_{u}}{y_{v}+\delta_{v}}\right)\] \[=\sum_{v\notin L(\mathcal{T})}\frac{w_{v}}{2}\cdot(y^{\prime}_{v}+ \delta_{v})\] \[=\frac{1}{2}\cdot 2k\cdot(2^{h+1}-2)\] \[=k\cdot D.\]Let \(B^{\prime}=\sum_{v\in I}\mu^{\prime}_{v}\) and \(B=\sum_{v\in I}\mu_{v}\). Then, we have shown that

\[f(y^{\prime},y)\geq B^{\prime}\cdot\ln\left(\frac{B^{\prime}}{B}\right)+(kD-B^{ \prime})\cdot\ln\left(\frac{kD-B^{\prime}}{kD-B}\right)\] (2)

Our next step is to apply Pinsker's inequality to the above expression. Pinsker's inequality states that for any \(p,q\in(0,1)\), it holds that

\[p\cdot\ln\left(\frac{p}{q}\right)+(1-p)\cdot\ln\left(\frac{1-p}{1-q}\right) \geq 2\cdot(p-q)^{2}\]

Since \(B\leq kD\) and \(B^{\prime}\leq kD\), we can scale everything in inequality 2 and apply Pinsker's inequality to obtain

\[f(y^{\prime},y)\geq\frac{2}{kD}\cdot(B-B^{\prime})^{2}\] (3)

To complete the proof, we substitute

\[B^{\prime}-B =\sum_{v\in I}(\mu^{\prime}_{v}-\mu_{v})\] \[=\sum_{v\in I}w_{v}\cdot(y^{\prime}_{p(v)}+\delta_{p(v)})\cdot( \frac{y^{\prime}_{v}+\delta_{v}}{y^{\prime}_{p(v)}+\delta_{p(v)}}-\frac{y_{v}+ \delta_{v}}{y_{p(v)}+\delta_{p(v)}})\] \[=\sum_{v\notin L(\mathcal{T})}\frac{w_{v}}{2}\cdot(y^{\prime}_{v} +\delta_{v})\cdot\sum_{u\in\operatorname{cld}(v)\cap I}(\frac{y^{\prime}_{u} +\delta_{u}}{y^{\prime}_{v}+\delta_{v}}-\frac{y_{u}+\delta_{u}}{y_{v}+\delta_{ v}})\] \[=\frac{1}{2}\cdot\sum_{v\notin L(\mathcal{T})}\frac{w_{v}}{2} \cdot(y^{\prime}_{v}+\delta_{v})\cdot\sum_{u\in\operatorname{cld}(u)}|\frac{ y^{\prime}_{u}+\delta_{u}}{y^{\prime}_{v}+\delta_{v}}-\frac{y_{u}+\delta_{u}}{y_{v}+ \delta_{v}}|\]

where the last equality follows from the fact that the ratio in the inner sum always sum to \(1\), and thus by only summing over the ones with positive difference we get half of the total sum of absolute differences. By swapping the summation order once again, we get

\[B^{\prime}-B =\frac{1}{2}\cdot\sum_{v\neq r}w_{v}\cdot(y^{\prime}_{p(v)}+ \delta_{p(v)})\cdot|\frac{y^{\prime}_{v}+\delta_{v}}{y^{\prime}_{p(v)}+\delta _{p(v)}}-\frac{y_{v}+\delta_{v}}{y_{p(v)}+\delta_{p(v)}}|\] \[=\frac{1}{2}\cdot h(y^{\prime},y)\]

and from inequality (3) we finally get

\[f(y^{\prime},y)\geq\frac{1}{2kD}\cdot\left(h(y^{\prime},y)\right)^{2}\]

as desired. 

**Claim 5**.: _For any \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\), it holds that \(\|y^{\prime}-y\||_{\mathcal{T}}\leq 2\gamma\cdot h(y^{\prime},y)\)._

Proof.: To prove the claim, we first need to establish some extra notation. For any \(y\in\mathcal{FP}(\mathcal{T})\) and \(v\neq r\), let

\[\lambda_{v}(y):=\frac{y_{v}+\delta_{v}}{y_{p(v)}+\delta_{p(v)}}\]

Furthermore, for any vertex \(v\) and any integer \(i\in[0,h-lev(v)]\), we use \(p(v,i)\) to denote the \(i\)-th ancestor of \(v\) on \(\mathcal{T}\), for example \(p(v,0)=v\), \(p(v,1)=p(v)\) and \(p(v,h-lev(v))=r\).

Recall that by definition, \(y_{r}=\delta_{r}=k\). Thus, if we telescope these terms and let \(m_{v}=h-lev(v)-1\), we clearly have that

\[y_{v}+\delta_{v}=2k\cdot\Pi_{i=0}^{m_{v}}\lambda_{p(v,i)}(y)\]which implies

\[y^{\prime}_{v}-y_{v} =2k\cdot\Pi_{i=0}^{m_{v}}\lambda_{p(v,i)}(y^{\prime})-2k\cdot\Pi_{i= 0}^{m_{v}}\lambda_{p(v,i)}(y)\] \[=2k\cdot\sum_{i=0}^{m_{v}}\lambda_{p(v,0)}(y^{\prime})\cdot\ldots \cdot(\lambda_{p(v,i)}(y^{\prime})-\lambda_{p(v,i)}(y))\cdot\ldots\cdot\lambda _{p(v,m_{v})}(y)\] \[=2k\cdot\sum_{i=0}^{m_{v}}\frac{y^{\prime}_{v}+\delta_{v}}{y^{ \prime}_{p(v,i)}+\delta_{p(v,i)}}\cdot(\lambda_{p(v,i)}(y^{\prime})-\lambda_{ p(v,i)}(y))\cdot\frac{y_{p(v,i+1)}+\delta_{p(v,i+1)}}{2k}\] \[=(y^{\prime}_{v}+\delta_{v})\cdot\sum_{i=0}^{m_{v}}\frac{y_{p(v, i+1)}+\delta_{p(v,i+1)}}{y^{\prime}_{p(v,i)}+\delta_{p(v,i)}}\cdot(\lambda_{p(v,i )}(y^{\prime})-\lambda_{p(v,i)}(y))\]

and from the triangular inequality

\[|y^{\prime}_{v}-y_{v}|\leq(y^{\prime}_{v}+\delta_{v})\cdot\sum_{i=0}^{m_{v}} \frac{y_{p(v,i+1)}+\delta_{p(v,i+1)}}{y^{\prime}_{p(v,i)}+\delta_{p(v,i)}} \cdot|\lambda_{p(v,i)}(y^{\prime})-\lambda_{p(v,i)}(y)|\] (4)

Plugging inequality (4) into the definition of norm \(||\cdot||_{\mathcal{T}}\), we get

\[||y^{\prime}-y||_{\mathcal{T}}\leq\gamma\cdot\sum_{v\neq r}w_{v}\cdot(y^{ \prime}_{v}+\delta_{v})\cdot\left(\sum_{i=0}^{m_{v}}\frac{y_{p(v,i+1)}+\delta_ {p(v,i+1)}}{y^{\prime}_{p(v,i)}+\delta_{p(v,i)}}\cdot|\lambda_{p(v,i)}(y^{ \prime})-\lambda_{p(v,i)}(y)|\right)\]

and by carefully exchanging the summation order, we obtain

\[||y^{\prime}-y||_{\mathcal{T}}\leq\gamma\cdot\sum_{v\neq r}\frac{y_{p(v)}+ \delta_{p(v)}}{y^{\prime}_{v}+\delta_{v}}\cdot|\lambda_{v}(y^{\prime})- \lambda_{v}(y)|\cdot\left(\sum_{u\in T(v)}w_{u}(y^{\prime}_{u}+\delta_{u})\right)\]

Finally, observe that \(\sum_{u\in T(v)}w_{u}y^{\prime}_{u}\leq 2w_{v}y^{\prime}_{v}\). To see this, fix the sub-tree \(T(v)\) rooted at vertex \(v\) and recall that since \(y^{\prime}\in\mathcal{FP}(\mathcal{T})\), the total amount of facilities at each level is \(y^{\prime}_{v}\). Furthermore, the weights \(w_{v}\) decrease by a factor of \(2\) at every level. Using the same arguments, we obtain \(\sum_{u\in T(v)}w_{u}\delta_{u}\leq 2w_{v}\delta_{v}\). Combining everything, we finally get

\[||y^{\prime}-y||_{\mathcal{T}}\leq 2\gamma\cdot\sum_{v\neq r}w_{v}\cdot(y_{p(v)} +\delta_{p(v)})\cdot|\lambda_{v}(y^{\prime})-\lambda_{v}(y)|\]

or equivalently, \(||y^{\prime}-y||_{\mathcal{T}}\leq 2\gamma\cdot h(y^{\prime},y)\). 

### Bounding the Moving Cost (Proof of Lemma 2)

In this section we will upper bound the moving cost of \(\mathrm{FTRL}\) by its connection cost. Fix any sequence of client requests \(R_{1},R_{2},\ldots,R_{T}\subseteq L(\mathcal{T})\). Recall that at each step \(t\), \(\mathrm{FTRL}\) selects a fractional facility placement \(y^{t}\) given by

\[y^{t}=\operatorname*{arg\,min}_{y\in\mathcal{FP}(\mathcal{T})}\Phi_{t}(y)\]

where \(\Phi_{t}(y)=\sum_{s=1}^{t-1}f_{R_{s}}(y)+\frac{1}{\eta}\cdot R_{\mathcal{T}}(y)\) is the objective that \(\mathrm{FTRL}\) minimizes over at step \(t\) for \(\eta=(\gamma\cdot\sqrt{nT})^{-1}\). In this section, we prove Lemma 2, by arguing that

\[\sum_{t=2}^{T}||y^{t}-y^{t-1}||_{\mathcal{T}}\leq\frac{1}{2}\cdot\sum_{t=1}^{T }f_{R_{t}}(y^{t})+\frac{\eta}{2\alpha}\cdot T\]

since the proof follows easily by the definitions of \(\eta\) and \(\alpha\).

From Lemma 1 we already know that \(R_{\mathcal{T}}\) is \(\alpha\)-strongly convex with respect to \(||\cdot||_{\mathcal{T}}\) for \(\alpha=(8kD\gamma^{2})^{-1}\). Furthermore, by definition the fractional connection cost

\[f_{R}(y)=\sum_{j\in R}\sum_{v\in P(j,r)}2^{lev(v)+1}\cdot\max\left(0,1-y_{v}\right)\]is clearly convex for any client request \(R\subseteq L(\mathcal{T})\). Thus, it is straight-forward to argue that at any step \(t\), the \(\mathrm{FTRL}\) objective \(\Phi_{t}\) is \(\frac{\alpha}{\eta}\)-strongly convex with respect to \(||\cdot||_{\mathcal{T}}\). Unfortunately, \(f_{R}(y)\) is not differentiable on \(\mathcal{FP}(\mathcal{T})\), but its sub-gradients are well-defined on any \(y\in\mathcal{FP}(\mathcal{T})\). Thus, the strong convexity of \(\Phi_{t}\) provides us with the following guarantee:

**Claim 6**.: _Fix any pair of fractional facility placements \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\) and any time step \(t\in[T]\). Let \(g_{t}\in\partial\Phi_{t}(y)\) be any sub-gradient of \(\Phi_{t}\) at \(y\). Then, it holds that_

\[\Phi_{t}(y^{\prime})\geq\Phi_{t}(y)+\langle g_{t},y^{\prime}-y\rangle+\frac{ \alpha}{\eta}\cdot||y-y^{\prime}||_{\mathcal{T}}^{2}\]

Furthermore, since by definition \(y^{t}\) is the (unique) minimizer of \(\Phi_{t}\), the first order optimality conditions on \(\Phi_{t}\) imply that there exists some \(g_{t}^{t}\in\partial\Phi_{t}(y^{t})\) such that \(\langle g_{t}^{*},y-y^{t}\rangle\geq 0\) for any \(y\in\mathcal{FP}(\mathcal{T})\). Claim 6 for \(y=y^{t}\), \(y^{\prime}=y^{t-1}\) and \(g_{t}=g_{t}^{*}\) gives us

\[\Phi_{t}(y^{t-1})\geq\Phi_{t}(y^{t})+\frac{\alpha}{\eta}\cdot||y^{t}-y^{t-1}|| _{\mathcal{T}}^{2}\]

Thus, we have

\[||y^{t}-y^{t-1}||_{\mathcal{T}}^{2} \leq\frac{\eta}{\alpha}\cdot\big{(}\Phi_{t}(y^{t-1})-\Phi_{t}(y^{ t})\big{)}\] \[=\frac{\eta}{\alpha}\cdot\big{(}\Phi_{t-1}(y^{t-1})+f_{R_{t-1}}(y ^{t-1})-\Phi_{t-1}(y^{t})-f_{R_{t-1}}(y^{t})\big{)}\] \[\leq\frac{\eta}{\alpha}\cdot\big{(}f_{R_{t-1}}(y^{t-1})-f_{R_{t-1 }}(y^{t})\big{)}\]

where for the equality we used the fact that \(\Phi_{t}(y)=\Phi_{t-1}(y)+f_{R_{t-1}}(y)\) and for the second inequality we used the fact that \(y^{t-1}\) is by definition the minimizer of \(\Phi_{t-1}\). Finally, since \(f_{R}(y)\geq 0\) for any client request \(R\subseteq L(\mathcal{T})\), we have

\[||y^{t}-y^{t-1}||_{\mathcal{T}} \leq\sqrt{\frac{\eta}{\alpha}\cdot f_{R_{t-1}}(y^{t-1})}\] \[\leq\frac{\eta}{2\alpha}+\frac{1}{2}\cdot f_{R_{t-1}}(y^{t-1})\]

where the last inequality follows from the _Arithmetic Mean - Geometric Mean_ inequality. Summing over all \(t\) completes the proof of Lemma 2.

### Bounding the Connection Cost (Proof of Lemma 3)

In this section we will upper bound the connection cost of \(\mathrm{FTRL}\) by the connection cost of the optimal fractional facility placement in hindsight. This is a standard analysis found in many textbooks, and we present it just for the sake of completeness.

Fix any sequence of client requests \(R_{1},R_{2},\ldots,R_{T}\subseteq L(\mathcal{T})\). Recall that at each step \(t\), \(\mathrm{FTRL}\) selects a fractional facility placement \(y^{t}\) given by

\[y^{t}=\operatorname*{arg\,min}_{y\in\mathcal{FP}(\mathcal{T})}\Phi_{t}(y)\]

where \(\Phi_{t}(y)=\sum_{s=1}^{t-1}f_{R_{s}}(y)+\frac{1}{\eta}\cdot R_{\mathcal{T}}(y)\) is the objective that \(\mathrm{FTRL}\) minimizes over at step \(t\) for \(\eta=(\gamma\cdot\sqrt{nT})^{-1}\). Let \(y^{*}\) be the optimal facility placement in hindsight, i.e.

\[y^{*}=\operatorname*{arg\,min}_{y\in\mathcal{FP}(\mathcal{T})}\sum_{t=1}^{T}f _{R_{t}}(y)\]

In this section we prove Lemma 3,by arguing that

\[\sum_{t=1}^{T}f_{R_{t}}(y^{t})\leq\sum_{t=1}^{T}f_{R_{t}}(y^{*})+\frac{knD}{ \eta}+32kn^{2}D\eta\cdot T\]

and then the proof follows easily by definition of \(\eta\).

In the standard analysis of \(\mathrm{FTRL}\), the following quantities are of special interest as they appear in the final regret guarantees of the algorithm:* Let \(\operatorname{diam}(R_{\mathcal{T}}):=\max_{y,y^{t}\in\mathcal{FP}(\mathcal{T})}|R_{ \mathcal{T}}(y)-R_{\mathcal{T}}(y^{\prime})|\) be the diameter of the regularizer.
* Let \(G_{f}\) be an upper bound on the dual norm of the sub-gradient of the fractional connection cost for any client request, i.e. for any \(R\subseteq L(\mathcal{T})\) and any \(y\in\mathcal{FP}(\mathcal{T})\), there exists some sub-gradient \(g\in\partial f_{R}(y)\) such that \(||g||^{*}_{\mathcal{T}}\leq G_{f}\). Here, \(||\cdot||^{*}_{\mathcal{T}}\) denotes the dual norm of \(||\cdot||_{\mathcal{T}}\).

We begin by presenting the standard analysis of \(\operatorname{FTRL}\) and deriving an expression for the regret guarantee that depends on the above quantities. Recall that at any step \(t\), the \(\operatorname{FTRL}\) objective \(\Phi_{t}\) doesn't include \(f_{R_{t}}\) since the client request \(R_{t}\) is not revealed to the algorithm at the time of decision. We begin by bounding the connection cost of a theoretical algorithm that has access to this information and thus at time \(t\) can pick facility placement \(y^{t+1}\).

**Claim 7**.: _The output of \(\operatorname{FTRL}\) satisfies_

\[\sum_{t=1}^{T}f_{R_{t}}(y^{t+1})\leq\sum_{t=1}^{T}f_{R_{t}}(y^{*})+\frac{ \operatorname{diam}(R_{\mathcal{T}})}{\eta}\]

Proof.: We have

\[\Phi_{t}(y^{t}) =\Phi_{t-1}(y^{t})+f_{R_{t-1}}(y^{t})\] \[\geq\Phi_{t-1}(y^{t-1})+f_{R_{t-1}}(y^{t})\]

where the equality holds by definition of \(\Phi_{t}\) and the inequality holds from the optimality of \(y^{t-1}\) on \(\Phi_{t-1}\). Similarly, we obtain

\[\Phi_{t-1}(y^{t-1})\geq\Phi_{t-2}(y^{t-2})+f_{R_{t-2}}(y^{t-1})\]

If we keep applying this rule, we finally get that

\[\Phi_{t}(y^{t})\geq\sum_{s=1}^{t-1}f_{R_{s}}(y^{s+1})+\Phi_{1}(y^{1})\]

Furthermore, we have \(\Phi_{1}(y^{1})=R_{\mathcal{T}}(y_{1})/\eta\) and \(\Phi_{t}(y^{*})\geq\Phi_{t}(y^{t})\) for all \(t\). Thus, we get

\[\Phi_{T+1}(y^{*})\geq\sum_{t=1}^{T}f_{R_{t}}(y^{t+1})+\frac{1}{\eta}\cdot R_{ \mathcal{T}}(y^{1})\]

or equivalently (by substituting \(\Phi_{T+1}\)'s definition) we have

\[\sum_{t=1}^{T}f_{R_{t}}(y^{t+1})\leq\sum_{t=1}^{T}f_{R_{t}}(y^{*})+\frac{R_{ \mathcal{T}}(y^{*})-R_{\mathcal{T}}(y^{1})}{\eta}\]

The claim follows from the definition of \(\operatorname{diam}(R_{\mathcal{T}})\). 

Next, we proceed by bounding the increase in the connection cost that we suffer by choosing \(y^{t}\) instead of \(y^{t+1}\) at time \(t\).

**Claim 8**.: _For any \(t\geq 0\), it holds that \(f_{R_{t}}(y^{t})\leq f_{R_{t}}(y^{t+1})+\eta G_{f}^{2}/\alpha\)._

Proof.: For any client request \(R\subseteq L(\mathcal{T})\), the fractional connection cost function \(f_{R}(y)\) is clearly convex and its sub-gradients are well-defined on \(\mathcal{FP}(\mathcal{T})\). By definition of \(G_{f}\), we know that there exists some sub-gradient \(g\in\partial f_{R_{t}}(y^{t})\) such that \(||g||^{*}_{\mathcal{T}}\leq G_{f}\). Using this sub-gradient, we get

\[f_{R_{t}}(y^{t}) \leq f_{R_{t}}(y^{t+1})+\langle g,y^{t}-y^{t+1}\rangle\] \[\leq f_{R_{t}}(y^{t+1})+||g||^{*}_{\mathcal{T}}\cdot||y^{t}-y^{t+ 1}||_{\mathcal{T}}\] \[\leq f_{R_{t}}(y^{t+1})+G_{f}\cdot||y^{t}-y^{t+1}||_{\mathcal{T}}\]where the first inequality is derived from the convexity of the fractional connection cost, the second inequality is an application of Holder's inequality and the third inequality is from \(G_{f}\)'s definition.

As we have already argued in section D.2, we know that for any step \(t\), the \(\mathrm{FTRL}\) objective \(\Phi_{t}\) is \(\alpha/\eta\)-strongly convex with respect to \(||\cdot||_{\mathcal{T}}\). Using the definition of strong convexity, this implies that

\[\Phi_{t+1}(y^{t})\geq\Phi_{t+1}(y^{t+1})+\langle g,y^{t}-y^{t+1} \rangle+\frac{\alpha}{\eta}\cdot||y^{t}-y^{t+1}||_{\mathcal{T}}^{2}\]

for any sub-gradient \(g\in\partial\Phi_{t+1}(y^{t+1})\). Furthermore, since \(y^{t+1}\) is the minimizer of \(\Phi_{t+1}\), we know from the first order optimality conditions that we can select \(g\in\partial\Phi_{t+1}(y^{t+1})\) such that \(\langle g,y-y^{t+1}\rangle\geq 0\) for any \(y\in\mathcal{FP}(\mathcal{T})\). Using such a sub-gradient, we get

\[||y^{t}-y^{t+1}||_{\mathcal{T}}^{2} \leq\frac{\eta}{\alpha}\cdot\left(\Phi_{t+1}(y^{t})-\Phi_{t+1}(y^ {t+1})\right)\] \[=\frac{\eta}{\alpha}\cdot\left(\Phi_{t}(y^{t})+f_{R_{t}}(y^{t})- \Phi_{t}(y^{t+1})-f_{R_{t}}(y^{t+1})\right)\] \[\leq\frac{\eta}{\alpha}\cdot\left(f_{R_{t}}(y^{t})-f_{R_{t}}(y^{t +1})\right)\]

where we just expanded \(\Phi_{t+1}\)'s definition and used the fact that \(y^{t}\) is the minimizer of \(\Phi_{t}\).

Combining everything, we finally obtain

\[f_{R_{t}}(y^{t})-f_{R_{t}}(y^{t+1})\leq G_{f}\cdot\sqrt{\frac{ \eta}{\alpha}\cdot\left(f_{R_{t}}(y^{t})-f_{R_{t}}(y^{t+1})\right)}\]

and the claim follows. 

We complete the analysis of \(\mathrm{FTRL}\) by combining Claims 7 and 8 in order to obtain the following regret guarantee:

**Claim 9**.: _The output of \(\mathrm{FTRL}\) satisfies_

\[\sum_{t=1}^{T}f_{R_{t}}(y^{t})\leq\sum_{t=1}^{T}f_{R_{t}}(y^{*}) +\frac{diam(R_{\mathcal{T}})}{\eta}+\frac{\eta G_{f}^{2}}{\alpha}\cdot T\]

It remains to substitute the specific values of the parameters that appear in the regret guarantee. We have already proven in section D.1 that \(R_{\mathcal{T}}\) is \(\alpha\)-strongly convex with respect to \(||\cdot||_{\mathcal{T}}\) for \(\alpha=(8kD\gamma^{2})^{-1}\). Next, we provide an upper bound for the diameter of the regularizer.

**Claim 10**.: _It holds that \(\mathrm{diam}(R_{\mathcal{T}})\leq knD\)._

Proof.: Fix any \(y\in\mathcal{FP}(\mathcal{T})\). Be definition, we know that \(y_{v}\leq y_{p(v)}\) and \(\delta_{v}\leq\delta_{p(v)}\) for any \(v\neq r\). Thus, the expressions inside the logarithms of the regularizer are always at most \(1\), which implies that \(R_{\mathcal{T}}(y)\leq 0\). Furthermore, for any \(\alpha,\beta>0\) it holds that \(\alpha-\beta\leq\alpha\cdot\ln\left(\alpha/\beta\right)\). Using this inequality, we get that

\[R_{\mathcal{T}}(y)\geq\sum_{v\neq r}w_{v}\cdot\left(y_{v}+ \delta_{v}-y_{p(v)}-\delta_{p(v)}\right)\]

Fix any level \(l\in[0,h-1]\) and let \(V_{l}=\{v\in V(\mathcal{T}):lev(v)=l\}\) denote the set of vertices of the HST at level \(l\). Since \(y\in\mathcal{FP}(\mathcal{T})\), we know that \(\sum_{v\in V_{l}}y_{v}=k\), and by definition of \(\delta\)'s we know that \(\sum_{v\in V_{l}}\delta_{v}=k\) as well. Furthermore, we know that \(\sum_{v\in V_{l}}y_{p(v)}\leq n\cdot\sum_{v\in V_{l+1}}y_{v}=n\cdot k\) since any vertex \(v\) can have at most \(n\) (i.e. the total number of leaves) children. Using the same argument,we have \(\sum_{v\in V_{l}}\delta_{p(v)}\leq n\cdot\sum_{v\in V_{l+1}}y_{v}=n\cdot k\). Thus, combining everything we obtain

\[R_{\mathcal{T}}(y) \geq\sum_{v\neq r}w_{v}\cdot(y_{v}+\delta_{v}-y_{p(v)}-\delta_{p(v )})\] \[=\sum_{l=0}^{h-1}\sum_{v\in V_{l}}2^{l}\cdot(y_{v}+\delta_{v}-y_{p( v)}-\delta_{p(v)})\] \[\geq\sum_{l=0}^{h-1}2^{l}\cdot(2k-2kn)\] \[=2k(1-n)(2^{h}-1)\] \[=k(1-n)D.\]

which proves our claim. 

Finally, we only need to find an upper bound for \(G_{f}\). We begin by computing a set of sub-gradients for the fractional connection cost function.

**Claim 11**.: _Fix any client request \(R\subseteq L(\mathcal{T})\) and any \(y\in\mathcal{FP}(\mathcal{T})\). Define the vector \(g^{R,y}\in\mathbb{R}^{|V(\mathcal{T})|}\) such that_

\[g^{R,y}_{v}=\left\{\begin{array}{ll}0&\text{if }y_{v}\geq 1\\ -2^{|ev(v)+1}\cdot|T(v)\cap R|&\text{if }y_{v}<1\end{array}\right.\]

_Then, \(g^{R,y}\in\partial f_{R}(y)\), i.e. \(g^{R,y}\) is a sub-gradient of \(f_{R}\) on point \(y\)._

Proof.: Fix any client request \(R\subseteq L(\mathcal{T})\). By definition of the fractional connection cost on facility placement \(y\in\mathcal{FP}(\mathcal{T})\), we have

\[f_{R}(y)=\sum_{j\in R}\sum_{v\in P(j,r)}2^{|ev(v)+1}\cdot\max\left(0,1-y_{v}\right)\]

where \(P(j,r)\) denotes the unique path from leaf \(j\in L(\mathcal{T})\) to the root \(r\). This is clearly a convex function on \(\mathcal{FP}(\mathcal{T})\) and thus the sub-gradients of \(f_{R}\) are well-defined. Fix any \(v\in V(\mathcal{T})\). We distinguish between two cases.

* If \(y_{v}<1\), then the partial derivative of \(f_{R}(y)\) is well-defined and given by \[\frac{\partial f_{R}(y)}{\partial y_{v}}=-2^{lev(v)+1}\cdot|T(v)\cap R|\] where \(T(v)\) is the set of vertices on the sub-tree rooted at vertex \(v\).
* If \(y_{v}\geq 1\), then clearly it doesn't contribute to \(f_{R}(y)\). Using standard calculus, it is not hard to argue that in this case there exists a sub-gradient of \(f_{R}(y)\) whose coordinate corresponding to \(v\) is \(0\). Thus, we have argued that that \(g^{R,y}\) is a valid sub-gradient of \(f_{R}\) on point \(y\).

Finally, we provide an upper bound on the dual-norm of the sub-gradients that we computed on Claim 11.

**Claim 12**.: _For any \(y\in\mathcal{FP}(\mathcal{T})\) and any \(R\subseteq L(\mathcal{T})\), it holds that \(||g^{R,v}||^{*}_{\mathcal{T}}\leq\frac{2n}{\gamma}\)._

Proof.: Recall that we have defined the moving cost norm as

\[||y||_{\mathcal{T}}=\gamma\cdot\sum_{v\in V(\mathcal{T})}w_{v}\cdot y_{v}\]which is basically a weighted \(l_{1}\)-norm with weights \(\gamma\cdot w_{v}\). It is well-known that the dual of the \(l_{1}\)-norm is the \(l_{\infty}\) norm. Similarly, the dual of the weighted \(l_{1}\)-norm is a weighted \(l_{\infty}\) norm with inverse weights, i.e. \(||\cdot||^{*}=l_{\infty}((\gamma w)^{-1})\). Thus, we have

\[||x||^{*}_{\mathcal{T}}=\max_{v}\frac{|x_{v}|}{\gamma\cdot w_{v}}\]

Using the calculation of the sub-gradients from Claim 11 and that \(R\subseteq L(\mathcal{T})\) and thus \(|R|\leq n\), we immediately get the claim. 

Claim 10 provides us with an expression for \(\mathrm{diam}(R_{\mathcal{T}})\) and Claim 12 provides us with an expression for \(G_{f}\). Plugging everything in into Claim 9, we complete the proof of Lemma 3.

### Incorporating approximation errors (Proof of Lemma 4)

Fix any sequence of client requests \(R_{1},R_{2},\ldots,R_{T}\subseteq L(\mathcal{T})\). Recall that at each step \(t\), \(\mathrm{FTRL}\) selects a fractional facility placement \(y^{t}\) given by

\[y^{t}=\operatorname*{arg\,min}_{y\in\mathcal{FP}(\mathcal{T})}\Phi_{t}(y)\]

where \(\Phi_{t}(y)=\sum_{s=1}^{t-1}f_{R_{s}}(y)+\frac{1}{\eta}\cdot R_{\mathcal{T}}(y)\) is the objective that \(\mathrm{FTRL}\) minimizes over at step \(t\) for \(\eta=(\gamma\cdot\sqrt{nT})^{-1}\).

Now, assume that instead of minimizing \(\Phi_{t}(y)\) over \(\mathcal{FP}(\mathcal{T})\) to compute \(y^{t}\), we are only able to compute a fractional facility placement \(z^{t}\in\mathcal{FP}(\mathcal{T})\) such that \(\Phi_{t}(z^{t})\leq\Phi_{t}(y^{t})+\epsilon\) for some \(\epsilon>0\).

**Claim 13**.: _For any step \(t\), it holds that_

\[||z^{t}-y^{t}||_{\mathcal{T}}\leq\sqrt{\epsilon\cdot\frac{\eta}{\alpha}}\]

Proof.: As we have already argued in section D.2, we know that for any step \(t\), the \(\mathrm{FTRL}\) objective \(\Phi_{t}\) is \(\alpha/\eta\)-strongly convex with respect to \(||\cdot||_{\mathcal{T}}\). Combining this with the first order optimality condition for \(\Phi_{t}\) on \(y_{t}\), we get

\[\Phi_{t}(z^{t})\geq\Phi_{t}(y^{t})+\frac{\alpha}{\eta}\cdot||z^{t}-y^{t}||_{ \mathcal{T}}^{2}\]

which implies that

\[||z^{t}-y^{t}||_{\mathcal{T}}\leq\sqrt{\epsilon\cdot\frac{\eta}{\alpha}}\]

Using Claim 13, we can easily bound both the connection and the moving cost of the approximated \(\mathrm{FTRL}\) solutions.

* For the connection cost, recall that the fractional connection cost function \(f_{R_{t}}\) at step \(t\) is convex, which implies that \[f_{R_{t}}(z^{t})\leq f_{R_{t}}(y^{t})+\langle g,z^{t}-y^{t}\rangle\] for some \(g\in\partial f_{R_{t}}(z^{t})\). Using Holder's inequality to upper bound the inner-product and using the upper bound of Claim 12 for the dual norm of the sub-gradients of \(f_{R_{t}}\), we get that \[f_{R_{t}}(z^{t})\leq f_{R_{t}}(y^{t})+\frac{2n}{\gamma}\cdot||z^{t}-y^{t}||_{ \mathcal{T}}\] and finally from Claim 13 we get that \[f_{R_{t}}(z^{t})\leq f_{R_{t}}(y^{t})+\frac{2n}{\gamma}\cdot\sqrt{\epsilon \cdot\frac{\eta}{\alpha}}\]* For the moving cost, recall it suffices to use the triangular inequality that \(||\cdot||_{\mathcal{T}}\) (as a norm) satisfies: \[||z^{t}-z^{t-1}||_{\mathcal{T}} \leq||z^{t}-y^{t}||_{\mathcal{T}}+||y^{t}-y^{t-1}||_{\mathcal{T}}+ ||y^{t-1}-z^{t-1}||_{\mathcal{T}}\] \[\leq||y^{t}-y^{t-1}||_{\mathcal{T}}+2\cdot\sqrt{\epsilon\cdot\frac {\eta}{\alpha}}\] The proof of Lemma 4 follows easily by plugging in \(\eta=(\gamma\cdot\sqrt{nT})^{-1}\), \(\alpha=(8kD\gamma^{2})^{-1}\) and \(\epsilon=\mathcal{O}(1/\sqrt{T})\).

### Implementation of Projected Mirror Descent

We conclude this section by considering the _Projected Mirror Descent_ update step, namely

\[y^{\prime}=\operatorname*{arg\,min}_{y^{*}\in\mathcal{FP}(\mathcal{T})}\left[ \eta\cdot\langle c,y^{*}\rangle+\cdot D_{R_{\mathcal{T}}}(y^{*},y)\right]\]

that takes as input a fractional facility placement \(y\in\mathcal{FP}(\mathcal{T})\) and returns some other \(y^{\prime}\in\mathcal{FP}(\mathcal{T})\) that minimizes a linear cost under vector \(c\) plus the Bregman Divergence between the initial and the new point under regularizer \(R_{\mathcal{T}}\). Here, \(\eta>0\) is a tuning parameter that balances the dynamics between the linear cost and the Bregman Divergence.

By letting \(c\) be the sub-gradient of the fractional connection cost over the observed sequence of clients, we can use this update step in order to approximate the FTRL objective; this is, in fact, the implementation we did for our experimental evaluation of Algorithm 2. In this section we will argue that the special structure of \(R_{\mathcal{T}}\) allows us to compute the update step in linear (to the size of the HST) time.

By definition of the Bregman Divergence, we have

\[D_{R_{\mathcal{T}}}(x,y)=R_{\mathcal{T}}(x)-R_{\mathcal{T}}(y)-\langle\nabla R _{\mathcal{T}}(y),x-y\rangle\]

Substituting everything, we get that the update step of _Projected Mirror Descent_ can be written as

\[y^{\prime}=\operatorname*{arg\,min}_{y^{*}\in\mathcal{FP}(\mathcal{T})}F(y^{*})\]

for

\[F(y^{*})=\eta\cdot\sum_{v}c_{v}\cdot y^{*}_{v} +\sum_{v\neq r}w_{v}\cdot(y^{*}_{v}+\delta_{v})\cdot\ln\left( \frac{y^{*}_{v}+\delta_{v}}{y^{*}_{p(v)}+\delta_{p(v)}}\right)\] \[-\sum_{v\neq r}w_{v}\cdot(y_{v}+\delta_{v})\cdot\ln\left(\frac{y _{v}+\delta_{v}}{y_{p(v)}+\delta_{p(v)}}\right)\] \[-\sum_{v\neq r}\left(w_{v}\cdot\ln\left(\frac{y_{v}+\delta_{v}}{ y_{p(v)}+\delta_{p(v)}}\right)+\frac{w_{v}}{2}+\frac{w_{v}}{2}\cdot\mathbbm{1}[v \in L(\mathcal{T})]\right)(y^{*}_{v}-y_{v})\] \[-\frac{w_{r}}{2}(y^{*}_{r}-y_{r})\]

It is always the case that we update \(y^{\prime}\) from some \(y\in\mathcal{FP}(\mathcal{T})\), so we can simplify the above expression to get

\[F(y^{*})=\eta\cdot\sum_{v}c_{v}\cdot y^{*}_{v} +\sum_{v\neq r}w_{v}\cdot(y^{*}_{v}+\delta_{v})\cdot\ln\left( \frac{\frac{y^{*}_{v}+\delta_{v}}{y^{*}_{p(v)}+\delta_{p(v)}}}{\frac{y_{v}+ \delta_{p(v)}}{y_{p(v)}+\delta_{p(v)}}}\right)\] \[-\sum_{v}\frac{w_{v}}{2}\cdot(1+\mathbbm{1}[v\in L(\mathcal{T})]) \cdot(y^{*}_{v}-y_{v})\]Recall that by definition, \(\mathcal{FP}(T)\) is the polytope

\[\mathcal{FP}(\mathcal{T})=\left\{y\in\mathbb{R}^{|V(\mathcal{T})|}:\begin{array}[ ]{ll}y_{v}=\sum_{u\in\operatorname{cld}(v)}y_{u}&v\notin L(\mathcal{T})\\ y_{v}\in[0,1]&v\in L(\mathcal{T})\\ y_{r}=k&\end{array}\right.\]

Since our objective is to minimize function \(F(\cdot)\) over \(\mathcal{FP}(\mathcal{T})\), we can write down the KKT optimality conditions to obtain the following conditions about the minimizer \(y^{*}\):

\[\frac{y_{v}^{*}+\delta_{v}}{y_{p(v)}^{*}+\delta_{p(v)}}=\frac{y_{v}+\delta_{v} }{y_{p(v)}+\delta_{p(v)}}\cdot\exp\left(\frac{1}{w_{v}}(\mu_{p(v)}-\mu_{v}- \eta c_{v})\right)\]

where \(\mu_{v}\) is the Lagrange multiplier for constraint \(y_{v}=\sum_{u\in\operatorname{cld}(v)}y_{u}\) and \(\mu_{v}=0\) for \(v\in L(\mathcal{T})\). To complete our computation of \(y^{*}\), it remains to compute the Lagrange multipliers \(\mu\).

Since \(y^{*}\in\mathcal{FP}(\mathcal{T})\), it is not hard to verify that for any \(v\notin L(\mathcal{T})\) it holds

\[\sum_{u\in\operatorname{cld}(v)}\frac{y_{u}^{*}+\delta_{u}}{y_{v}^{*}+\delta_ {v}}=1\]

and using the KKT optimality condition, this implies that for any \(v\notin L(\mathcal{T})\)

\[\sum_{u\in\operatorname{cld}(v)}\frac{y_{u}+\delta_{u}}{y_{v}+\delta_{v}} \cdot\exp\left(\frac{1}{w_{u}}(\mu_{v}-\mu_{u}-\eta c_{u})\right)=1\]

or equivalently, since \(w_{v}=2w_{u}\) for all \(u\in\operatorname{cld}(v)\),

\[\mu_{v}=-\frac{w_{v}}{2}\cdot\ln\left(\sum_{u\in\operatorname{cld}(v)}\frac{y _{u}+\delta_{u}}{y_{v}+\delta_{v}}\cdot\exp\left(-\frac{\mu_{u}+\eta c_{u}}{w _{u}}\right)\right)\]

Thus, starting from \(\mu_{v}=0\) on the leaves, this expression provides as a bottom-up algorithm to compute all the Lagrange multipliers \(\mu\). Using these multipliers and the KKT optimality conditions, we can then easily compute the ratios

\[\frac{y_{v}^{*}+\delta_{v}}{y_{p(v)}^{*}+\delta_{p(v)}}=\frac{y_{v}+\delta_{v }}{y_{p(v)}+\delta_{p(v)}}\cdot\exp\left(\frac{1}{w_{v}}(\mu_{p(v)}-\mu_{v}- \eta c_{v})\right)\]

for all vertices \(v\neq r\). Finally, we can start from the root vertex \(r\), for which we know that \(y_{r}^{*}=k\), and cascade these ratios downwards until we reach the leaves and we have compute all entries of \(y^{*}\). Clearly, this is all done in linear time to the number of vertices.

Intuitively, this update step can be interpreted as an application of the Multiplicative Weights Update algorithm on every parent vertex \(v\) that decides how its mass should be split to its children. We repeat this process in a bottom-up manner, and then we simply start with \(k\) facilities on the root and begin splitting them based on these ratios while moving downwards.

Analysis of Cut\(\&\)Round (Proofs of Section 5)

In this chapter of the appendix we present all the omitted proofs from Section 5 concerning our online rounding scheme \(\mathrm{Cut}\&\mathrm{Round}\). To avoid repetition, from now on we fix an arbitrary HST \(\mathcal{T}\) and use \(\mathcal{FP}(\mathcal{T})\) to denote the set of all fractional placements of \(k\) facilities on the leaves of \(\mathcal{T}\).

Roadmap.In section E.1, we argue about the correctness of \(\mathrm{Cut}\&\mathrm{Round}\); namely, we show that no matter the input, \(\mathrm{Cut}\&\mathrm{Round}\) always returns a set of \(k\)-leaves of \(\mathcal{T}\) where the facilities are placed. Then, in section E.2 we establish the main property of \(\mathrm{Cut}\&\mathrm{Round}\) and prove Lemma 5. Finally, in section E.3 we analyze the expected connection cost of \(\mathrm{Cut}\&\mathrm{Round}\)'s output and prove Item 1 of Theorem 4 (Lemma 6) while in section E.4 we analyze the expected moving cost of \(\mathrm{Cut}\&\mathrm{Round}\)'s output and prove Item 2 of Theorem 4 (Lemma 7).

### Correctness of \(\mathrm{Cut}\&\mathrm{Round}\)

We begin by proving the correctness of \(\mathrm{Cut}\&\mathrm{Round}\). Fix any \(y\in\mathcal{FP}(\mathcal{T})\) and any set of thresholds \(\alpha\in[0,1]^{|V(\mathcal{T})|}\). Let \(F=\mathrm{Cut}\&\mathrm{Round}(\mathcal{T},y,\alpha)\). In this section, we will prove that \(|F|=k\), i.e. we will argue that \(\mathrm{Cut}\&\mathrm{Round}\) always returns a set of \(k\) leaves at which facilities must be placed, as it is expected to. In order to show this, we will need to analyze the \(Y_{v}\) variables produced by \(\mathrm{Cut}\&\mathrm{Round}\).

**Claim 14**.: _For any leaf \(v\in L(\mathcal{T})\), it holds that \(Y_{v}\in\{0,1\}\)._

Proof.: Observe that for any \(v\in V(\mathcal{T})\), sub-routine \(\mathrm{Alloc}\) sets \(Y_{v}\) to either \(\lfloor y_{v}\rfloor\) or \(\lfloor y_{v}\rfloor+1\). By definition of \(\mathcal{FP}(\mathcal{T})\), we have \(y_{v}\in[0,1]\) for each leaf \(v\in L(\mathcal{T})\). We distinguish between two different cases. If \(y_{v}\in[0,1)\), then clearly \(Y_{v}\in\{0,1\}\). If \(y_{v}=1\), then \(\delta(y_{v})=0\) and thus \(\mathrm{Alloc}\) will always set \(Y_{v}=\lfloor y_{v}\rfloor=1\). Thus, the claim holds for all leaves \(v\in L(\mathcal{T})\). 

**Claim 15**.: _Let \(v\notin L(\mathcal{T})\) be any non-leaf vertex. Then, \(Y_{v}=\sum_{u\in\mathrm{cld}(v)}Y_{u}\)._

Proof.: Fix any non-leaf vertex \(v\notin L(\mathcal{T})\). We will analyze the inner loop of \(\mathrm{Cut}\&\mathrm{Round}\) that iterates over \(v\)'s children. Initially, \(\mathrm{Cut}\&\mathrm{Round}\) sets \(Y_{rem}=Y_{v}\) and \(y_{rem}=y_{v}\). Then, we proceed to iteratively call \(\mathrm{Alloc}\), once per child vertex of \(v\). Each time \(\mathrm{Alloc}\) assigns some value \(Y_{u}\) to a child vertex \(u\in\mathrm{cld}(v)\), we update \(Y_{rem}\) to \(Y_{rem}-Y_{u}\); thus, to prove our claim it suffices to argue that after we update the last child vertex, we have \(Y_{rem}=0\).

Since by definition of sub-routine \(\mathrm{Alloc}\) we know that \(Y_{v}\in\{\lfloor y_{v}\rfloor,\lfloor y_{v}\rfloor+1\}\), we know that initially (before any child vertex is assigned a value \(Y_{u}\)) it holds that \(Y_{rem}\in\{\lfloor y_{rem}\rfloor,\lfloor y_{rem}\rfloor+1\}\). In fact, a simple case analysis over the decision tree of sub-routine \(\mathrm{Alloc}\) suffices to see that this invariant holds not only at the beginning, but even after we begin assigning values to the child vertices and update \(Y_{rem}\) and \(y_{rem}\).

Since \(y\in\mathcal{FP}(\mathcal{T})\), we know that \(y_{v}=\sum_{u\in\mathrm{cld}(v)}y_{u}\) and thus \(y_{rem}=y_{u}\) at the time we iterate over the last child vertex \(u\in\mathrm{cld}(v)\). Furthermore, from the above discussion we know that \(Y_{rem}\in\{\lfloor y_{u}\rfloor,\lfloor y_{u}\rfloor+1\}\). Since \(\delta(y_{u})=\delta(y_{rem})\), it is easy to verify that in any case \(\mathrm{Alloc}\) sets \(Y_{u}=Y_{rem}\) and thus after the last update we have \(Y_{rem}=0\), as desired. 

Proof of Correctness.Recall that by definition, the output of \(\mathrm{Cut}\&\mathrm{Round}\) is \(F=\{v\in L(\mathcal{T}):Y_{v}=1\}\). Since from Claim 14 we know that \(Y_{v}\in\{0,1\}\) for all \(v\in L(\mathcal{T})\), this implies that \(|F|=\sum_{v\in L(\mathcal{T})}Y_{v}\). We apply Claim 15 to the root vertex \(r\), then again to each \(u\in\mathrm{cld}(r)\) and so on until we reach the leaves. This gives us that \(Y_{r}=\sum_{v\in L(\mathcal{T})}Y_{v}\) and thus \(|F|=Y_{r}\). Since by definition \(\mathrm{Cut}\&\mathrm{Round}\) sets \(Y_{r}=k\), we have proven that \(|F|=k\) as desired.

### Proof of Lemma 5 (Computing the Allocation Probabilities)

In this section, we formally prove the main property of algorithm \(\mathrm{Cut\&Round}\), as stated in Lemma 5. Fix any fractional facility placement \(y\in\mathcal{FP}(\mathcal{T})\) and let \(\alpha_{v}\sim\mathrm{Unif}(0,1)\) be independent uniformly random thresholds for all \(v\in V(\mathcal{T})\). Let \(F=\mathrm{Cut\&Round}(\mathcal{T},y,\alpha)\) be the output of algorithm \(\mathrm{Cut\&Round}\) on this set of inputs. Recall that algorithm \(\mathrm{Cut\&Round}\) sets the variables \(Y_{v}\) during its execution, for all \(v\in V(\mathcal{T})\). As we have already discussed, \(Y_{v}\) is the total number of facilities in \(F\) on the leaves of the sub-tree rooted at \(v\), i.e. \(Y_{v}=|T(v)\cap F|\). We will prove that for any \(v\in V(\mathcal{T})\), we have

\[Y_{v}=\left\{\begin{array}{ll}\lfloor y_{v}\rfloor&\text{ with probability }1-\delta(y_{v})\\ \lfloor y_{v}\rfloor+1&\text{ with probability }\delta(y_{v})\end{array}\right.\]

We begin by writing down the following property for sub-routine \(\mathrm{Alloc}\):

**Claim 16**.: _Fix any fractional facility placement \(y\in\mathcal{FP}(\mathcal{T})\) and let \(\alpha_{v}\sim\mathrm{Unif}(0,1)\) for all \(v\in V(\mathcal{T})\). For any vertex \(u\in V(\mathcal{T})\) of \(\mathcal{T}\), let \(Y_{u}=\mathrm{Alloc}(y_{u},y_{rem},Y_{rem},\alpha_{u})\) be the number of facilities assigned to the sub-tree of \(u\) by Line 8 of Algorithm \(\mathrm{Cut\&Round}\) (Algorithm 4). Then,_

\[\mathbb{P}_{\alpha}\left[Y_{u}=\lfloor y_{u}\rfloor\right]=\left\{\begin{array} []{ll}1&\text{ if }Y_{rem}=\lfloor y_{rem}\rfloor\text{and }\delta(y_{u})\leq \delta(y_{rem})\\ \frac{1-\delta(y_{v})}{1-\delta(y_{rem})}&\text{ if }Y_{rem}=\lfloor y_{rem} \rfloor\text{and }\delta(y_{u})>\delta(y_{rem})\\ 0&\text{ if }Y_{rem}\neq\lfloor y_{rem}\rfloor\text{and }\delta(y_{u})> \delta(y_{rem})\\ \frac{\delta(y_{rem})-\delta(y_{u})}{\delta(y_{rem})}&\text{ if }Y_{rem}\neq \lfloor y_{rem}\rfloor\text{and }\delta(y_{u})\leq\delta(y_{rem})\end{array}\right.\]

Proof.: This claim is a direct consequence of sub-routine \(\mathrm{Alloc}\)'s description (Algorithm 5) and the fact that \(\alpha_{v}\sim\mathrm{Unif}(0,1)\) for all \(v\in V(\mathcal{T})\). 

Using this claim, we are now ready to prove Lemma 5.

Proof of Lemma 5.We prove the lemma via a top-down induction on the vertices of \(\mathcal{T}\) (decreasing level order). For the root vertex, we know that since \(y\in\mathcal{FP}(\mathcal{T})\) we have \(y_{r}=k\) and also by definition of \(\mathrm{Cut\&Round}\) we have \(Y_{r}=k\). Thus, we get that \(Y_{r}=y_{r}=\lfloor y_{r}\rfloor\) with probability \(1-\delta(y_{r})=1\) and the claim holds. Now, fix any non-leaf vertex \(v\notin L(\mathcal{T})\) and assume that \(Y_{v}=\lfloor y_{v}\rfloor\) with probability \(1-\delta(y_{v})\) and \(Y_{v}=\lfloor y_{v}\rfloor+1\) with probability \(\delta(y_{v})\). To complete our induction, we will now proceed to prove the claim for all the children vertices of \(v\).

We begin by proving the claim for the first child of vertex \(v\), and then we will show how the same arguments extend for all its children. Let \(u\in\mathrm{cld}(v)\) be the _first_ child vertex of \(v\) that \(\mathrm{Cut\&Round}\) iterates over. Then, by definition of \(\mathrm{Cut\&Round}\) we have that \(Y_{rem}=Y_{v}\) and \(y_{rem}=y_{v}\). Using the inductive hypothesis on \(v\), this implies that \(Y_{rem}=\lfloor y_{rem}\rfloor\) with probability \(1-\delta(y_{rem})\) and \(Y_{rem}=\lfloor y_{rem}\rfloor+1\) with probability \(\delta(y_{rem})\). Conditioning on the value of \(Y_{rem}\), we get

\[\mathbb{P}_{\alpha}\left[Y_{u}=\lfloor y_{u}\rfloor\right] =\mathbb{P}_{\alpha}\left[Y_{u}=\lfloor y_{u}\rfloor\mid Y_{rem}= \lfloor y_{rem}\rfloor\right]\cdot(1-\delta(y_{rem}))\] \[+\mathbb{P}_{\alpha}\left[Y_{u}=\lfloor y_{u}\rfloor\mid Y_{rem}= \lfloor y_{rem}\rfloor+1\right]\cdot\delta(y_{rem})\]

We distinguish between two different cases based on whether \(\delta(y_{u})\leq\delta(y_{rem})\) or \(\delta(y_{u})>\delta(y_{rem})\). In any case, we can use Claim 16 to substitute the conditional probabilities on the above expression and easily get that

\[\mathbb{P}_{\alpha}\left[Y_{u}=\lfloor y_{u}\rfloor\right]=1-\delta(y_{u})\]

Thus, we have already proven the claim for the first child of \(v\). However, to complete our induction, we need to prove the claim for all children of \(v\) and not just the first one. The only property that we used and holds specifically for the first child was that \(Y_{rem}=\lfloor y_{rem}\rfloor\) with probability \(1-\delta(y_{rem})\) and \(Y_{rem}=\lfloor y_{rem}\rfloor+1\) with probability \(\delta(y_{rem})\). Let \(Y^{\prime}_{rem}\) and \(y^{\prime}_{rem}\) be the updated remaining facilities after the value \(Y_{u}\) of the first child has been assigned. If we can prove that \(Y^{\prime}_{rem}=\lfloor y^{\prime}_{rem}\rfloor\) with probability \(1-\delta(y^{\prime}_{rem})\) and \(Y^{\prime}_{rem}=\lfloor y^{\prime}_{rem}\rfloor+1\) with probability \(\delta(y^{\prime}_{rem})\), then we can keep applying the same argument and inductively prove the claim for all the children of \(v\).

By definition, we have that \(Y^{\prime}_{rem}=Y_{rem}-Y_{u}\) and \(y^{\prime}_{rem}=y_{rem}-y_{u}\). Once again, we distinguish between two different cases.

* Let \(\delta(y_{u})\leq\delta(y_{rem})\). In that case, we get that \([y^{\prime}_{rem}]=\lfloor y_{rem}\rfloor-\lfloor y_{u}\rfloor\) and also that \(\delta(y^{\prime}_{rem})=\delta(y_{rem})-\delta(y_{u})\). Since we know that \(Y_{rem}\in\{\lfloor y_{rem}\rfloor,\lfloor y_{rem}\rfloor+1\}\) and \(Y_{u}\in\{\lfloor y_{u}\rfloor,\lfloor y_{u}\rfloor+1\}\), this implies that \[\mathbb{P}_{\alpha}[Y^{\prime}_{rem}=\lfloor y^{\prime}_{rem}]] =\mathbb{P}_{\alpha}[Y_{rem}=\lfloor y_{rem}\rfloor\ \cap\ Y_{u}=\lfloor y_{u}\rfloor]\] \[+\mathbb{P}_{\alpha}[Y_{rem}=\lfloor y_{rem}\rfloor+1\ \cap\ Y_{u}= \lfloor y_{u}\rfloor+1]\] Using conditional probabilities and the inductive hypothesis on the distribution of \(Y_{rem}\), we obtain \[\mathbb{P}_{\alpha}\left[Y^{\prime}_{rem}=\lfloor y^{\prime}_{rem}\rfloor\right] =\mathbb{P}_{\alpha}\left[Y_{u}=\lfloor y_{u}\rfloor\mid Y_{rem }=\lfloor y_{rem}\rfloor\right]\cdot\left(1-\delta(y_{rem})\right)\] \[+\mathbb{P}_{\alpha}\left[Y_{u}=\lfloor y_{u}+1\rfloor\mid Y_{rem }=\lfloor y_{rem}\rfloor+1\right]\cdot\delta(y_{rem})\] Using Claim 16 to substitute the conditional probabilities, we finally get \[\mathbb{P}_{\alpha}\left[Y^{\prime}_{rem}=\lfloor y^{\prime}_{rem}\rfloor \right]=1-\delta(y_{rem})+\delta(y_{u})=1-\delta(y^{\prime}_{rem})\] as desired.
* Let \(\delta(y_{u})>\delta(y_{rem})\). In that case, we get that \(\lfloor y^{\prime}_{rem}\rfloor=\lfloor y_{rem}\rfloor-\lfloor y_{u}\rfloor-1\) and also that \(\delta(y^{\prime}_{rem})=1+\delta(y_{rem})-\delta(y_{u})\). Since we know that \(Y_{rem}\in\left\{\lfloor y_{rem}\rfloor,\lfloor y_{rem}\rfloor+1\right\}\) and \(Y_{u}\in\{\lfloor y_{u}\rfloor,\lfloor y_{u}\rfloor+1\}\), this implies that \[\mathbb{P}_{\alpha}\left[Y^{\prime}_{rem}=\lfloor y^{\prime}_{rem}\rfloor \right]=\mathbb{P}_{\alpha}\left[Y_{rem}=\lfloor y_{rem}\rfloor\ \cap\ Y_{u}=\lfloor y_{u}\rfloor+1\right]\] Using conditional probabilities and the inductive hypothesis on the distribution of \(Y_{rem}\), we obtain \[\mathbb{P}_{\alpha}\left[Y^{\prime}_{rem}=\lfloor y^{\prime}_{rem}\rfloor \right]=\mathbb{P}_{\alpha}\left[Y_{u}=\lfloor y_{u}\rfloor+1\mid Y_{rem}= \lfloor y_{rem}\rfloor\right]\cdot\left(1-\delta(y_{rem})\right)\] Using Claim 16 to substitute the conditional probabilities, we finally get \[\mathbb{P}_{\alpha}\left[Y^{\prime}_{rem}=\lfloor y^{\prime}_{rem}\rfloor \right]=\delta(y_{u})-\delta(y_{rem})=1-\delta(y^{\prime}_{rem})\] as desired.

Thus, we have concluded the proof of Lemma 5.

### Proof of Item \(1\) in Theorem 4 (Bounding the Expected Connection Cost)

**Lemma 6**.: _Let \(F=\mathrm{Cut}\&\mathrm{Round}(y,\alpha)\) where for all \(v\in V(\mathcal{T})\), \(\alpha_{v}\sim\mathrm{Unif}(0,1)\) independently. Then,_

\[\mathbb{E}_{\alpha}[C_{R}(F)]=f_{R}(y)\ \text{ for any }R\subseteq L(\mathcal{T})\]

Proof.: Fix any \(y\in\mathcal{FP}(\mathcal{T})\) and let \(\alpha\in[0,1]^{|V(\mathcal{T})|}\) be a set of thresholds such that for each \(v\in V(\mathcal{T})\), \(\alpha_{v}\) is drawn independently at random from the uniform distribution, i.e. \(\alpha_{v}\sim\mathrm{Unif}(0,1)\). Let \(F=\mathrm{Cut}\&\mathrm{Round}(\mathcal{T},y,\alpha)\). We will prove that for any set of clients \(R\subseteq L(\mathcal{T})\), it holds that \(\mathbb{E}_{\alpha}[C_{R}(F)]=f_{R}(y)\).

Recall that the \(Y_{v}\) variables set by \(\mathrm{Cut}\&\mathrm{Round}\) denote the total number of facilities in \(F\) that are placed on the sub-tree rooted at vertex \(v\), i.e. \(Y_{v}=|F\cap T(v)|\). As argued in section E.1, we know that \(Y\in\mathcal{FP}(\mathcal{T})\cap\mathbbm{N}\), i.e. \(Y\) is a valid integral facility placement. Thus, from Claim 1 of section C.1, we know that \(C_{R}(F)=f_{R}(Y)\). This implies that by definition of the fractional connection cost under client request \(R\), we have that

\[C_{R}(F) =\sum_{j\in R}\sum_{v\in P(j,r)}2^{\mathrm{lev}(v)+1}\cdot\max \left(0,1-Y_{v}\right)\]

Thus, we get

\[\mathbb{E}_{\alpha}[C_{R}(F)] =\sum_{j\in R}\sum_{v\in P(j,r)}2^{\mathrm{lev}(v)+1}\cdot\mathbb{E }_{\alpha}[\max\left(0,1-Y_{v}\right)]\] \[=\sum_{j\in R}\sum_{v\in P(j,r)}2^{\mathrm{lev}(v)+1}\cdot \mathbb{P}_{\alpha}[Y_{v}=0]\]where the first equality holds by linearity of expectation, and the second equality holds by the fact that \(Y_{v}\in\mathbb{N}\) for all \(v\in V(\mathcal{T})\). Since \(Y_{v}\in\{\lfloor y_{v}\rfloor,\lfloor y_{v}\rfloor+1\}\), we know that for any \(v\in V(\mathcal{T})\), \(Y_{v}\) can be \(0\) only if \(y_{v}\in[0,1)\). Furthermore, from Lemma 5, we know that in the case of uniformly random thresholds, this happens with probability precisely \(1-y_{v}\). Combining these facts, we get \(\mathbb{P}_{\alpha}[Y_{v}=0]=\max(0,1-y_{v})\) and thus

\[\mathbb{E}_{\alpha}[C_{R}(F)] =\sum_{j\in R}\sum_{v\in P(j,r)}2^{\mathrm{lev}(v)+1}\cdot\max(0, 1-y_{v})\] \[=f_{R}(y)\]

concluding the proof of Lemma 6. 

### Proof of Item \(2\) in Theorem 4 (Bounding the Expected Moving Cost)

**Lemma 7**.: _Let \(F=\mathrm{Round}\&\mathrm{Cut}(y,\alpha)\) and also let \(F^{\prime}=\mathrm{Round}\&\mathrm{Cut}(\mathcal{T},y^{\prime},\alpha)\) where \(\alpha_{v}\sim\mathrm{Unif}(0,1)\) for all \(v\in V(\mathcal{T})\). Then,_

\[\gamma\cdot\mathbb{E}_{\alpha}\left[M_{\mathcal{T}}(F,F^{\prime})\right]\leq 4 \cdot||y-y^{\prime}||_{\mathcal{T}}\]

Proof.Fix any pair of fractional facility placements \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\) and let corresponding outputs of \(\mathrm{Cut}\&\mathrm{Round}\) be denoted as \(F=\mathrm{Cut}\&\mathrm{Round}(\mathcal{T},y,\alpha)\) and \(F^{\prime}=\mathrm{Cut}\&\mathrm{Round}(\mathcal{T},y^{\prime},\alpha)\). Observe that the same set of (uniformly random) thresholds \(\alpha_{v}\) is used in both cases, as this will play a crucial part in our analysis. To prove Lemma 7, we need to prove that

\[\gamma\cdot\mathbb{E}_{\alpha}\left[M_{\mathcal{T}}(F,F^{\prime})\right]\leq 4 \cdot||y-y^{\prime}||_{\mathcal{T}}\]

where the expectation is taken over the value of the uniformly random thresholds \(\alpha_{v}\).

The proof of Lemma 7 is technically involved, and thus we will break down our approach into smaller sections to ease the presentation. We begin by proving the Lemma in the special case where the transition from \(y\) to \(y^{\prime}\) has a very simple structure, which we now proceed to define:

**Definition 10**.: _We say that two fractional facility placements \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\) are \(\epsilon\)-neighboring if there are two leaves \(s,t\in L(\mathcal{T})\) with least common ancestor \(p\in V(\mathcal{T})\) such that the following hold:_

1. \(y^{\prime}_{v}=y_{v}-\epsilon\) _for all_ \(v\in P(s,p)\setminus\{p\}\)_._
2. \(y^{\prime}_{v}=y_{v}+\epsilon\) _for all_ \(v\in P(t,p)\setminus\{p\}\)_._
3. \(y^{\prime}_{v}=y_{v}\) _for all other_ \(v\in V(\mathcal{T})\)_._

_Furthermore, we say that \(y,y^{\prime}\) are strictly \(\epsilon\)-neighboring if \(\epsilon\) is sufficiently small to satisfy_

1. \(\epsilon\leq\delta(y_{v})\) _for all_ \(v\in P(s,p)\setminus\{p\}\) _with_ \(\delta(y_{v})>0\)_._
2. \(\epsilon\leq 1-\delta(y_{v})\) _for all_ \(v\in P(t,p)\setminus\{p\}\) _with_ \(\delta(y_{v})>0\)_._
3. \(\epsilon<1\)_._

Basically, if \(y\) and \(y^{\prime}\) are \(\epsilon\)-neighboring then \(y^{\prime}\) is obtained by pushing \(\epsilon\)-mass on \(y\) from \(s\) to \(t\) along the unique path that connects these two leaves. Furthermore, if \(\epsilon\) is sufficiently small so that for any \(v\in V(\mathcal{T})\) either \(\lfloor y_{v}\rfloor=\lfloor y^{\prime}_{v}\rfloor\) or \(\lvert y_{v}-y^{\prime}_{v}\rvert\leq 1\) and at least one of the two is integral, then we say that the two fractional facility placements are _strictly_\(\epsilon\)-neighboring. As we will shortly argue, Lemma 7 holds in the special case where \(y,y^{\prime}\) are strictly \(\epsilon\)-neighboring.

**Claim 17**.: _If \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\) are strictly \(\epsilon\)-neighboring for some \(\epsilon\geq 0\), then_

\[\gamma\cdot\mathbb{E}_{\alpha}\left[M_{\mathcal{T}}(F,F^{\prime})\right]\leq 4 \cdot||y-y^{\prime}||_{\mathcal{T}}.\]

Before proving Claim 17, let us first show why it suffices to argue about the general case and prove Lemma 7. Let \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\) be any two fractional placements. Recall that \(||y-y^{\prime}||_{\mathcal{T}}\) captures precisely the minimum transportation cost from \(y\) to \(y^{\prime}\) on \(\mathcal{T}\). If we break down this transportation plan into small movements of masses between leaves, then we can view it as a sequence of transitions between strictly \(\epsilon\)-neighboring placements. This is formalized in the following claim:

**Claim 18**.: _For any \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\), there exists a finite sequence \(y_{0},y_{1},\ldots,y_{m}\in\mathcal{FP}(\mathcal{T})\) of fractional facility placements with \(y=y_{0}\) and \(y^{\prime}=y_{m}\) such that_

1. \(y_{j},y_{j+1}\) _are strictly_ \(\epsilon\)_-neighboring for some_ \(\epsilon\geq 0\) _for_ \(j=0,1,\ldots,m-1\)_._
2. \(||y-y^{\prime}||_{\mathcal{T}}=\sum_{j=1}^{m}||y_{j}-y_{j-1}||_{\mathcal{T}}\)_._

We will now prove Lemma 7. Let \(F_{j}=\mathrm{Cut}\&\mathrm{Round}(\mathcal{T},y_{j},\alpha)\) be the corresponding output of \(\mathrm{Cut}\&\mathrm{Round}\) on \(y_{j}\) using the same (uniformly random) thresholds \(\alpha_{v}\). Then,

\[\gamma\cdot\mathbb{E}_{\alpha}\left[M_{\mathcal{T}}(F,F^{\prime})\right] \leq\gamma\cdot\mathbb{E}_{\alpha}\left[\sum_{j=0}^{m-1}M_{ \mathcal{T}}(F_{j},F_{j+1})\right]\] \[=\gamma\cdot\sum_{j=0}^{m-1}\mathbb{E}_{\alpha}\left[M_{\mathcal{ T}}(F_{j},F_{j+1})\right]\] \[\leq 4\cdot\sum_{j=0}^{m-1}||y_{j}-y_{j+1}||_{\mathcal{T}}\] \[=4\cdot||y-y^{\prime}||_{\mathcal{T}}\]

In the above calculation, the first inequality holds from the fact that the minimum transportation cost satisfies the triangular inequality. The first equality holds from linearity of expectation. The second inequality holds from Claim 17 and the second equality holds from Claim 18.

Thus, we have shown that proving Lemma 7 for the special case of strictly \(\epsilon\)-neighboring fractional facility placements \(y,y^{\prime}\) suffices to prove Lemma 7 for the general case of any \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\) and conclude this section. The rest of this section is dedicated to proving Claim 17, which is the main technical challenge towards proving Lemma 7.

Proof of Claim 17.Fix any pair of strictly \(\epsilon\)-neighboring fractional facility placements \(y,y^{\prime}\in\mathcal{FP}(\mathcal{T})\) and let the corresponding outputs of \(\mathrm{Cut}\&\mathrm{Round}\) be \(F=\mathrm{Cut}\&\mathrm{Round}(\mathcal{T},y,\alpha)\) and \(F^{\prime}=\mathrm{Cut}\&\mathrm{Round}(\mathcal{T},y^{\prime},\alpha)\). In section E.1 we have already shown that \(F,F^{\prime}\subseteq L(\mathcal{T})\) are valid facility placements since \(|F|=|F^{\prime}|=k\). Let \(Y,Y^{\prime}\in\mathcal{FP}(\mathcal{T})\) be used to denote the corresponding integral placements, i.e.

\(Y_{v}:=|L(\mathcal{T})\cap F|=\) number of facilities in \(F\) placed on the leaves of the sub-tree rooted at \(v\)

and

\(Y^{\prime}_{v}:=|L(\mathcal{T})\cap F^{\prime}|=\) number of facilities in \(F^{\prime}\) placed on the leaves of the sub-tree rooted at \(v\)

Recall that \(Y\) and \(Y^{\prime}\) are precisely the values of the \(Y\)-variables that algorithm \(\mathrm{Cut}\&\mathrm{Round}\) sets. As shown in Claim 2 of Section E.4, we know that \(\gamma\cdot M_{\mathcal{T}}(F,F^{\prime})=||Y-Y^{\prime}||_{\mathcal{T}}\). Thus, in order to prove Claim 17, we need to show that

\[\mathbb{E}_{\alpha}\left[||Y-Y^{\prime}||_{\mathcal{T}}\right]\leq 4\cdot||y-y^{ \prime}||_{\mathcal{T}}\]

Since \(y,y^{\prime}\) are strictly \(\epsilon\)-neighboring fractional facility placements, we know that there exist two leaves \(s,t\in L(\mathcal{T})\) with lowest common ancestor \(p\in V(\mathcal{T})\) such that \(|y_{v}-y^{\prime}_{v}|\) is \(\epsilon\) among vertices on the (unique) path from \(s\) to \(t\) (excluding vertex \(p\)) and is \(0\) otherwise. Let \(L=\mathrm{lev}(p)\). Then, by definition of \(||\cdot||_{\mathcal{T}}\) we have

\[||y-y^{\prime}||_{\mathcal{T}}=\sum_{v\in V(\mathcal{T})}2^{\mathrm{lev}(v)} \cdot|y_{v}-y^{\prime}_{v}|=2\epsilon\cdot\sum_{l=0}^{L-1}2^{l}=2\epsilon\cdot (2^{L}-1)\] (5)

Furthermore, recall that from Lemma 5, \(\mathrm{Cut}\&\mathrm{Round}\) rounds \(y_{v}\) to either \(Y_{v}=\lfloor y_{v}\rfloor+1\) with probability \(\delta(y_{v})\) or to \(\lfloor y_{v}\rfloor\) with probability \(1-\delta(y_{v})\). Since \(\epsilon\) is sufficiently small so that either \(\lfloor y_{v}\rfloor=\lfloor y^{\prime}_{v}\rfloor\) or \(|y_{v}-y^{\prime}_{v}|\leq 1\) and at lowest one of the two is integral (and it is thus always rounded to itself), we get that \(|Y_{v}-Y^{\prime}_{v}|\leq 1\) for all \(v\in V(\mathcal{T})\). This implies that\[\mathbb{E}_{\alpha}\left[||Y-Y^{\prime}||_{\mathcal{T}}\right] =\mathbb{E}_{\alpha}\left[\sum_{v\in V(\mathcal{T})}2^{\mathrm{lev} (v)}\cdot|Y_{v}-Y^{\prime}_{v}|\right]\] \[=\sum_{v\in V(\mathcal{T})}2^{\mathrm{lev}(v)}\cdot\mathbb{E}_{ \alpha}\left[|Y_{v}-Y^{\prime}_{v}|\right]\] \[=\sum_{v\in V(\mathcal{T})}2^{\mathrm{lev}(v)}\cdot\mathbb{P}_{ \alpha}\left[|Y_{v}-Y^{\prime}_{v}|=1\right]\]

Let \(l\in[0,h(\mathcal{T})]\) be any level on the HST \(\mathcal{T}\) and let \(C_{l}\) be used to denote the expected number of vertices at level \(l\) that are rounded to different values, i.e.

\[C_{l}:=\mathbb{E}_{\alpha}\left[|\{v\in V(\mathcal{T}):\mathrm{lev}(v)=l\text { and }Y_{v}\neq Y^{\prime}_{v}\}|\right]\]

Then, the above imply that

\[\mathbb{E}_{\alpha}\left[||Y-Y^{\prime}||_{\mathcal{T}}\right]=\sum_{l=0}^{h( \mathcal{T})}2^{l}\cdot C_{l}\] (6)

It remains to compute \(C_{l}\) for all \(l\in[0,h(\mathcal{T})]\). This is done in Claim 19, where we prove that \(C_{l}=0\) for \(l\geq L\) (the level of \(s\) and \(t\)'s lowest common ancestor) and \(C_{l}\leq 4\epsilon\cdot(L-l)\) otherwise. Combining this claim with equations (5) and (6) immediately implies that

\[\mathbb{E}_{\alpha}\left[||Y-Y^{\prime}||_{\mathcal{T}}\right]\leq 4\cdot||y-y^{ \prime}||_{\mathcal{T}}\]

which completes the proof of Claim 17.

**Claim 19**.: _For any \(l\geq L\), \(C_{l}=0\). For any \(l<L\), \(C_{l}\leq 4\epsilon\cdot(L-l)\)._

Proof.: Recall that for fixed thresholds \(\alpha_{v}\), the output of \(\mathrm{Cut}\&\mathrm{Round}\) is deterministic. Since \(L\) is the level of vertex \(p\) (the lowest common ancestor of leaves \(s,t\)) and by definition of strictly \(\epsilon\)-neighboring placements \(y,y^{\prime}\) we know \(y_{v}=y^{\prime}_{v}\) for any vertex \(v\) such that \(\mathrm{lev}(v)\geq L\), we immediately get that \(C_{l}=0\) for any \(l\geq L\).

We will now proceed to analyze \(C_{l}\) for any \(l<L\). We partition the set of vertices \(v\in V(\mathcal{T})\) with \(\mathrm{lev}(v)=l\) into three sets:

* A vertex \(v\) is called _active_ if it lies on the (unique) path between leaves \(s\) and \(t\).
* A vertex \(v\) is called _inactive_ if it is not a descendant of \(p\) (the lowest common ancestor of leaves \(s\) and \(t\)).
* A vertex \(v\) is called _affected_ if it is not active and is a descendant of \(p\).

Obviously, each vertex \(v\) with \(\mathrm{lev}(v)=l\) must lie in exactly one of these sets.

Inactive Vertices.We will prove that for every inactive vertex \(v\), \(\mathbb{P}_{\alpha}[Y_{v}\neq Y^{\prime}_{v}]=0\). Since the same set of thresholds \(\alpha\) is used to round both \(y\) and \(y^{\prime}\), the output of \(\mathrm{Cut}\&\mathrm{Round}\) is deterministic. Furthermore, if a vertex \(v\) is inactive, then we know that \(y_{v}=y^{\prime}_{v}\) and also \(y_{u}=y^{\prime}_{u}\) for any ancestor vertex of \(u\) of \(v\) (by Definition 10 of neighboring facility placements). Thus, this immediately implies that \(Y_{v}=Y^{\prime}_{v}\) with probability \(1\) and thus we do not need to account for inactive vertices when computing \(C_{l}\).

Active Vertices.We will prove that for every active vertex \(v\), \(\mathbb{P}_{\alpha}[Y_{v}\neq Y^{\prime}_{v}]=\epsilon\). Recall that any active vertex is either an ancestor of leaf \(s\) or leaf \(t\). We will only prove the claim in the case when \(v\) is an ancestor of \(t\); the other case is completely analogous. A formal proof by induction is given in Claim 20, presented at the end of this section. As a direct corollary, since there are only two active vertices per level, the expected number of active vertices in level \(l\) that are rounded two different values is precisely \(2\epsilon\).

[MISSING_PAGE_FAIL:37]

4. \(\mathbb{P}_{\alpha}[Y_{v}=\lfloor y_{v}\rfloor+1\text{ and }Y_{v}^{\prime}= \lfloor y_{v}\rfloor+1]=(1-\delta(y_{p}))\cdot 0+\delta(y_{p})\cdot\frac{\delta(y_{v})}{ \delta(y_{p})}=\delta(y_{v})\).
* Let \(\delta(y_{v})\geq\delta(y_{p})\). Then, if \(Y_{p}=Y_{p}^{\prime}=\lfloor y_{p}\rfloor+1\) we know from the description of \(\operatorname{Alloc}\) that \(Y_{v}=Y_{v}^{\prime}=\lfloor y_{v}\rfloor+1\) with probability \(1\). On the other hand, if \(Y_{p}=Y_{p}^{\prime}=\lfloor y_{p}\rfloor\), we know that \(Y_{v}=\lfloor y_{v}\rfloor+1\) if \(\alpha_{v}\leq(\delta(y_{v})-\delta(y_{p}))/(1-\delta(y_{p}))\) and likewise \(Y_{v}^{\prime}=\lfloor y_{v}\rfloor+1\) if \(\alpha_{v}\leq(\delta(y_{v})+\epsilon-\delta(y_{p}))/(1-\delta(y_{p}))\). Thus, by conditioning on the values of \(Y_{p}\) and \(Y_{p}^{\prime}\), we get 1. \(\mathbb{P}_{\alpha}[Y_{v}=\lfloor y_{v}\rfloor\text{ and }Y_{v}^{\prime}= \lfloor y_{v}\rfloor]=(1-\delta(y_{p}))\cdot(1-\frac{\delta(y_{v})+\epsilon- \delta(y_{p})}{1-\delta(y_{p})})+\delta(y_{p})\cdot 0=1-\delta(y_{v})-\epsilon\). 2. \(\mathbb{P}_{\alpha}[Y_{v}=\lfloor y_{v}\rfloor\text{ and }Y_{v}^{\prime}= \lfloor y_{v}\rfloor]=(1-\delta(y_{p}))\cdot(\frac{\delta(y_{v})+\epsilon- \delta(y_{p})}{1-\delta(y_{p})}-\frac{\delta(y_{v})-\delta(y_{p})}{1-\delta(y_ {p})})+\delta(y_{p})\cdot 0=\epsilon\). 3. \(\mathbb{P}_{\alpha}[Y_{v}=\lfloor y_{v}\rfloor+1\text{ and }Y_{v}^{\prime}= \lfloor y_{v}\rfloor]=(1-\delta(y_{p}))\cdot 0+\delta(y_{p})\cdot 0=1-\delta(y_{v})=0\). 4. \(\mathbb{P}_{\alpha}[Y_{v}=\lfloor y_{v}\rfloor+1\text{ and }Y_{v}^{\prime}= \lfloor y_{v}\rfloor]=(1-\delta(y_{p}))\cdot\frac{\delta(y_{v})-\delta(y_{p})} {1-\delta(y_{p})}+\delta(y_{p})\cdot 1\delta(y_{v})\).

So in both cases, the base of the induction holds.

Inductive Step.Using the exact same approach, we can prove the claim for any active ancestor \(u\) of \(t\), assuming that the claim holds for \(u\)'s father \(v=p(u)\). The only difference, is that now we can't claim that \(Y_{v}=Y_{v}^{\prime}\) with probability \(1\). Instead, there are three different cases that we need to consider; namely

1. \(Y_{v}=Y_{v}^{\prime}=\lfloor y_{v}\rfloor\) with probability \(1-\epsilon-\delta(y_{v})\).
2. \(Y_{v}=Y_{v}^{\prime}=\lfloor y_{v}\rfloor+1\) with probability \(\delta(y_{v})\).
3. \(Y_{v}=\lfloor y_{v}\rfloor\) and \(Y_{v}^{\prime}=\lfloor y_{v}\rfloor+1\) with probability \(\epsilon\).

where the probabilities hold from the inductive hypothesis on the parent vertex \(v\). Next, we will need to once again consider the case of whether \(\delta(y_{u})<\delta(y_{v})\) or not (notice that the same relation will hold for \(y_{u}^{\prime}\) and \(y_{v}^{\prime}\)) and use the description of \(\operatorname{Alloc}\) to get the assignment probabilities. Since this is a simple matter of arithmetic, the details are omitted.

[MISSING_PAGE_FAIL:39]

**Real-World Datasets.** We evaluate the performance of Algorithm 2 on the \(\mathrm{MNIST}\) and \(\mathrm{CIFAR}10\) datasets. We randomly sample \(n=10000\) images and construct a graph where each image corresponds to a vertex with the edge weights given by the Euclidean distance of the respective images. At each round \(t\), an image is sampled uniformly at random and a client arrives in the corresponding vertex. We then evaluate Algorithm 2 in the latter setting for \(T=3000\) rounds and \(k=10\) facilities. In Table 2 we present the ratio of the overall cost of Algorithm 2 over the ratio cost of the fractional hindsight optimal7. As our experiments indicate, the sub-optimality of Algorithm 2 is way smaller than the theoretical \(\mathcal{O}(\log n)\) upper bound on the regret.

Footnote 7: The cost of the fractional hindsight optimal can be efficiently computed [31] and lower bounds the cost of the optimal facility placement. As a result, the presented ratios in Tables 2 and 3 are upper bounds on the actual ratio of Algorithm 2 and the optimal facility-placement.

Beyond unit batch sizes and random arrivals.Finally, we once again evaluate the performance of Algorithm 2 on the \(\mathrm{MNIST}\) and \(\mathrm{CIFAR}10\) datasets. This time, the requests arrive in batches of size \(R=10\) for \(T=3000\) rounds. In order to go beyond the _random arrival model_,we first sample the \(R\cdot T\) requested vertices uniformly at random from \([n]\) and then we proceed to order them based on their respective categories, using the lexicographical vector order to break ties. Then, we partition these requests into \(T\) batches of size \(R\) and sequentially reveal them to the algorithm as usual. As a result, all images/vertices from the first category are requested first, then the second etc.

In Table 3, we present our experimental evaluations on the above constructed sequence. As our experiments indicate, our algorithm admits way better performance than the theoretical \(O(\log n)\) guarantees even in sequences with higher batch sizes and non-random arrivals.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Algorithm 2 & \(\gamma=0\) & \(\gamma=1\) & \(\gamma=10\) \\ \hline \(\mathrm{MNIST}\) & \(1.118\pm 0.01\) & \(1.403\pm 0.04\) & \(1.5631\pm 0.03\) \\ \hline \(\mathrm{CIFAR}10\) & \(1.113\pm 0.01\) & \(1.189\pm 0.04\) & \(1.59\pm 0.31\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: The ratio of the cost of Algorithm 2 with respect to the cost of the fractional hindsight optimal facility placement (\(20\) runs).

\begin{table}
\begin{tabular}{c c c c} \hline \hline Our Algorithm & \(\gamma=0\) & \(\gamma=1\) & \(\gamma=10\) \\ \hline \(\mathrm{CIFAR}10\) & \(1.050\) & \(1.048\) & \(1.051\) \\ \hline \(\mathrm{MNIST}\) & \(1.082\) & \(1.045\) & \(1.12\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: The ratio of the cost of our Algorithm with respect to the cost of the fractional hindsight optimal facility placement.