# Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts

 Chaoqi Wang\({}^{1}\)   Ziyu Ye\({}^{1}\)   Zhe Feng\({}^{2}\)   Ashwinkumar Badanidiyuru\({}^{3}\)   Haifeng Xu\({}^{1}\)

University of Chicago\({}^{1}\)   Google Research\({}^{2}\)   Google\({}^{3}\)

{chaoqi, ziyuye, haifengxu}@uchicago.edu

{zhef, ashwinkumarbv}@google.com

###### Abstract

Standard contextual bandit problem assumes that all the relevant contexts are observed before the algorithm chooses an arm. This modeling paradigm, while useful, often falls short when dealing with problems in which valuable additional context can be observed after arm selection. For example, content recommendation platforms like Youtube, Instagram, Tiktok also observe valuable follow-up information pertinent to the user's reward after recommendation (e.g., how long the user stayed, what is the user's watch speed, etc.). To improve online learning efficiency in these applications, we study a novel contextual bandit problem with post-serving contexts and design a new algorithm, poLinUCB, that achieves tight regret under standard assumptions. Core to our technical proof is a robustified and generalized version of the well-known Elliptical Potential Lemma (EPL), which can accommodate noise in data. Such robustification is necessary for tackling our problem, and we believe it could also be of general interest. Extensive empirical tests on both synthetic and real-world datasets demonstrate the significant benefit of utilizing post-serving contexts as well as the superior performance of our algorithm over the state-of-the-art approaches.

## 1 Introduction

Contextual bandits represent a fundamental mathematical model that is employed across a variety of applications, such as personalized recommendations (Li et al., 2010; Wu et al., 2016) and online advertising (Schwartz et al., 2017; Nuara et al., 2018). In their conventional setup, at each round \(t\), a learner observes the context \(\bm{x}_{t}\), selects an arm \(a_{t}\in\mathcal{A}\), and subsequently, observes its associated reward \(r_{t,a_{t}}\). Despite being a basic and influential framework, it may not always capture the complexity of real-world scenarios (Wang et al., 2016; Yang et al., 2020). Specifically, the learner often observes valuable follow-up information pertinent to the payoff post arm selection (henceforth, the _post-serving context_). Standard contextual bandits framework that neglects such post-serving contexts may result in significantly suboptimal performance due to model misspecification.

Consider an algorithm designed to recommend educational resources to a user by utilizing the user's partially completed coursework, interests, and proficiency as pre-serving context (exemplified in platforms such as Coursera). After completing the recommendation, the system can refine the user's profile by incorporating many post-serving context features such as course completion status, how much time spent on different educational resources, performances, etc. This transition naturally delineates a mapping from user attributes (i.e., the pre-serving context) to user's learning experiences and outcomes (i.e., the post-serving context). It is not difficult to see that similar scenarios happen in many other recommender system applications. For instance, in e-commerce platforms (e.g., Amazon, Etsy or any retailing website), the system will first recommend products based on the user's profile information, purchasing pattern and browsing history, etc.; post recommendations, the system canthen update these information by integrating post-serving contexts such as this recent purchase behaviour and product reviews. Similarly, media content recommendation platforms like Youtube, Instagram and Tiktok, also observe many post-serving features (e.g., how long the user stayed) that can refine the system's estimation about users' interaction behavior as well as the rewards.

A common salient point in all the aforementioned scenarios is that the post-serving context are prevalent in many recommender systems; moreover, despite being unseen during the recommendation/serving phase, they can be estimated from the pre-serving context given enough past data. More formaly, we assume that there exists a learnable mapping \(\phi^{\star}(\cdot):\mathbb{R}^{d_{x}}\rightarrow\mathbb{R}^{d_{z}}\) that maps pre-serving feature \(\bm{x}\in\mathbb{R}^{d_{x}}\) to the expectation of the post-serving feature \(\bm{z}\in\mathbb{R}^{d_{z}}\), i.e., \(\mathbb{E}[\bm{z}|\bm{x}]=\phi^{\star}(\bm{x})\).

Unsurprisingly, and as we will also show, integrating the estimation of such post-serving features can significantly help to enhance the performance of contextual bandits. However, most of the existing contextual bandit algorithms, e.g., (Auer, 2002; Li et al., 2010; Chu et al., 2011; Agarwal et al., 2014; Tewari and Murphy, 2017), are not designed to accommodate the situations with post-serving contexts. We observe that directly applying these algorithms by ignoring post-serving contexts may lead to linear regret, whereas simple modification of these algorithms will also be sub-optimal. To address these shortcomings, this work introduces a novel algorithm, polLinUCB. Our algorithm leverages historical data to simultaneously estimate reward parameters and the functional mapping from the pre- to post-serving contexts so to optimize arm selection and achieves sublinear regret. En route to analyzing our algorithm, we also developed new technique tools that may be of independent interest.

Main Contributions.
* First, we introduce a new family of contextual linear bandit problems. In this framework, the decision-making process can effectively integrate post-serving contexts, premised on the assumption that the expectation of post-serving context as a function of the pre-serving context can be gradually learned from historical data. This new model allows us to develop more effective learning algorithms in many natural applications with post-serving contexts.
* Second, to study this new model, we developed a robustified and generalized version of the well-regarded elliptical potential lemma (EPL) in order to accommodate random noise in the post-serving contexts. While this generalized EPL is an instrumental tool in our algorithmic study, we believe it is also of independent interest due to the broad applicability of EPL in online learning.
* Third, building upon the generalized EPL, we design a new algorithm polLinUCB and prove that it enjoys a regret bound \(\widetilde{\mathcal{O}}(T^{1-\alpha}d_{u}^{\alpha}+d_{u}\sqrt{TK})\), where \(T\) denotes the time horizon and \(\alpha\in[0,1/2]\) is the learning speed of the pre- to post-context mapping function \(\phi^{\star}(\cdot)\), whereas \(d_{u}=d_{x}+d_{z}\) and \(K\) denote the parameter dimension and number of arms. When \(\phi^{\star}(\cdot)\) is easy to learn, e.g., \(\alpha=1/2\), the regret bound becomes \(\widetilde{\mathcal{O}}(\sqrt{Td_{u}}+d_{u}\sqrt{TK})\) and is tight. For general functions \(\phi^{\star}(\cdot)\) that satisfy \(\alpha\leq 1/2\), this regret bound degrades gracefully as the function becomes more difficult to learn, i.e., as \(\alpha\) decreases.
* Lastly, we empirically validate our proposed algorithm through thorough numerical experiments on both simulated benchmarks and real-world datasets. The results demonstrate that our algorithm surpasses existing state-of-the-art solutions. Furthermore, they highlight the tangible benefits of incorporating the functional relationship between pre- and post-serving contexts into the model, thereby affirming the effectiveness of our modeling.

## 2 Related Works

Contextual bandits. The literature on linear (contextual) bandits is extensive, with a rich body of works (Abe et al., 2003; Auer, 2002; Dani et al., 2008; Rusmevichientong and Tsitsiklis, 2010; Lu et al., 2010; Filippi et al., 2010; Li et al., 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011; Li et al., 2017; Jun et al., 2017). One of the leading design paradigm is to employ upper confidence bounds as a means of balancing exploration and exploitation, leading to the attainment of minimax optimal regret bounds. The derivation of these regret bounds principally hinges on the utilization of confidence ellipsoids and the elliptical potential lemma. Almost all these works assume that the contextual information governing the payoff is fully observable. In contrast, our work focuses on scenarios where the context is not completely observable during arm selection, thereby presenting new challenges in addressing partially available information.

Contextual bandits with partial information.Contextual bandits with partial information has been relatively limited in the literature. Initial progress in this area was made by Wang et al. (2016), who studied settings with hidden contexts. In their setup there is some context (the post-serving context in our model) that can never be observed by the learner, whereas in our setup the learner can observe post-serving context but only after pulling the arm. Under the assumption that if the parameter initialization is extremely close to the true optimal parameter, then they develop a sub-linear regret algorithm. Our algorithm does not need such strong assumption on parameter initialization. Moreover, we show that their approach may perform poorly in our setup. Subsequent research by Qi et al. (2018); Yang et al. (2020); Park and Faradonbeh (2021); Yang and Ren (2021); Zhu and Kveton (2022) investigated scenarios with noisy or unobservable contexts. In these studies, the learning algorithm was designed to predict context information online through context history analysis, or selectively request context data from an external expert. Our work, on the other hand, introduces a novel problem setting that separates contexts into pre-serving and post-serving categories, enabling the exploration of a wide range of problems with varying learnability. Additionally, we also need to employ new techniques for analyzing our problem to get a near-optimal regret bound.

Generalizations of the elliptical potential lemma (EPL).The EPL, introduced in the seminal work ofLai and Wei (1982), is arguably a cornerstone in analyzing how fast stochastic uncertainty decreases with the observations of new sampled directions. Initially being employed in the analysis of stochastic linear regression, the EPL has since been extensively utilized in stochastic linear bandit problems (Auer, 2002; Dani et al., 2008; Chu et al., 2011; Abbasi-Yadkori et al., 2011; Li et al., 2019; Zhou et al., 2020; Wang et al., 2022). Researchers have also proposed various generalizations of the EPL to accommodate diverse assumptions and problems. For example, Carpentier et al. (2020) extended the EPL by allowing for the use of the \(\bm{X}_{t}^{-p}\)-norm, as opposed to the traditional \(\bm{X}_{t}^{-1}\)-norm. Meanwhile, Hamidi and Bayati (2022) investigated a generalized form of the \(1\wedge\|\varphi(\bm{x}_{t})\|_{\bm{X}_{t-1}^{-1}}^{2}\) term, which was inspired by the pursuit of variance reduction in non-Gaussian linear regression models. However, existing (generalized) EPLs are inadequate for the analysis of our new problem setup. Towards that end, we develop a new generalization of the EPL in this work to accommodate _noisy feature vectors_.

## 3 Linear Bandits with Post-Serving Contexts

Basic setup.We hereby delineate a basic setup of linear contextual bandits within the scope of the partial information setting, whereas multiple generalizations of our framework can be found in Section 6. This setting involves a finite and discrete action space, represented as \(\mathcal{A}=[K]\). Departing from the classic contextual bandit setup, the context in our model is bifurcated into two distinct components: the _pre-serving_ context, denoted as \(\bm{x}\in\mathbb{R}^{d_{x}}\), and the _post-serving_ context, signified as \(\bm{z}\in\mathbb{R}^{d_{z}}\). When it is clear from context, we sometimes refer to pre-serving context simply as _context_ as in classic setup, but always retain the post-serving context notion to emphasize its difference. We will denote \(\bm{X}_{t}=\sum_{s=1}^{t}\bm{x}_{s}\bm{x}_{s}^{\top}+\lambda\bm{I}\) and \(\bm{Z}_{t}=\sum_{s=1}^{t}\bm{z}_{s}\bm{z}_{s}^{\top}+\lambda\bm{I}\). For the sake of brevity, we employ \(\bm{u}=(\bm{x},\bm{z})\) to symbolize the stacked vector of \(\bm{x}\) and \(\bm{z}\), with \(d_{u}=d_{x}+d_{z}\) and \(\|\bm{u}\|_{2}\leq L_{u}\). The pre-serving context is available during arm selection, while the post-serving context is disclosed _post_ the arm selection. For each arm \(a\in\mathcal{A}\), the payoff, \(r_{a}(\bm{x},\bm{z})\), is delineated as follows:

\[r_{a}(\bm{x},\bm{z})=\bm{x}^{\top}\bm{\theta}_{a}^{\star}+\bm{z}^{\top}\bm{ \beta}_{a}^{\star}+\eta,\]

where \(\bm{\theta}_{a}^{\star}\) and \(\bm{\beta}_{a}^{\star}\) represent the parameters associated with the arm, unknown to the learner, whereas \(\eta\) is a random noise sampled from an \(R_{\eta}\)-sub-Gaussian distribution. We use \(\|\bm{x}\|_{p}\) to denote the \(p\)-norm of a vector \(\bm{x}\), and \(\|\bm{x}\|_{\bm{A}}\coloneqq\sqrt{\bm{x}^{\top}\bm{A}\bm{x}}\) is the matrix norm. For convenience, we assume \(\|\bm{\theta}_{a}^{\star}\|_{2}\leq 1\) and \(\|\bm{\beta}_{a}^{\star}\|_{2}\leq 1\) for all \(a\in\mathcal{A}\). Additionally, we posit that the norm of the pre-serving and post-serving contexts satisfies \(\|\bm{x}\|_{2}\leq L_{x}\) and \(\|\bm{z}\|_{2}\leq L_{z}\), respectively, and \(\max_{t\in[T]}\sup_{a,b\in\mathcal{A}}\langle\bm{\theta}_{a}^{\star}-\bm{ \theta}_{b}^{\star},\bm{x}_{t}\rangle\leq 1\) and \(\max_{t\in[T]}\sup_{a,b\in\mathcal{A}}\langle\bm{\beta}_{a}^{\star}-\bm{ \beta}_{b}^{\star},\bm{z}_{t}\rangle\leq 1\), same as in (Lattimore and Szepesvari, 2020).

### Problem Settings and Assumptions.

The learning process proceeds as follows at each time step \(t=1,2,\cdots,T\):

1. The learner observes the context \(\bm{x}_{t}\).
2. An arm \(a_{t}\in[K]\) is selected by the learner.

3. The learner observes the realized reward \(r_{t,a_{t}}\) and the post-serving context, \(\bm{z}_{t}\).

Without incorporating the post-serving context, one may incur linear regret as a result of model misspecification, as illustrated in the following observation. To see this, consider a setup with two arms, \(a_{1}\) and \(a_{2}\), and a context \(x\in\mathbb{R}\) drawn uniformly from the set \(\{-3,-1,1\}\) with \(\phi^{\star}(x)=x^{2}\). The reward functions for the arms are noiseless and determined as \(r_{a_{1}}(x)=x+x^{2}/2\) and \(r_{a_{2}}(x)=-x-x^{2}/2\). It can be observed that \(r_{a_{1}}(x)>r_{a_{2}}(x)\) when \(x\in\{-3,1\}\) and \(r_{a_{1}}(x)<r_{a_{2}}(x)\) when \(x=-1\). Any linear bandit algorithm that solely dependent on the context \(x\) (ignoring \(\phi^{\star}(x)\)) will inevitably suffer from linear regret, since it is impossible to have a linear function (i.e., \(r(x)=\theta x\)) that satisfies the above two inequalities simultaneously.

**Observation 1**.: _There exists linear bandit environments in which any online algorithm without using post-serving context information will have \(\Omega(T)\) regret._

Therefore, it is imperative that an effective learning algorithm must leverage the post-serving context, denoted as \(\bm{z}_{t}\). As one might anticipate, in the absence of any relationship between \(\bm{z}_{t}\) and \(\bm{x}_{t}\), it would be unfeasible to extrapolate any information regarding \(\bm{z}_{t}\) while deciding which arm to pull, a point at which only \(\bm{x}_{t}\) is known. Consequently, it is reasonable to hypothesize a correlation between \(\bm{z}_{t}\) and \(\bm{x}_{t}\). This relationship is codified in the subsequent learnability assumption.

Specifically, we make the following natural assumption -- there exists an algorithm that can learn the mean of the post-serving context \(\bm{z}_{t}\), conditioned on \(\bm{x}_{t}\). Our analysis will be general enough to accommodate different convergence rates of the learning algorithm, as one would naturally expect, the corresponding regret will degrade as this learning algorithm's convergence rate becomes worse. More specifically, we posit that, given the context \(\bm{x}_{t}\), the post-serving context \(\bm{z}_{t}\) is generated as1

Footnote 1: We remark that an alternative view of this reward generation is to treat it as a function of \(\bm{x}\) as follows: \(r_{a}(\bm{x})=\bm{x}^{\top}\bm{\theta}_{a}^{\star}+\phi^{\star}(\bm{x})^{\top }\bm{\beta}_{a}^{\star}+\) noise. Our algorithm can be viewed as a two-phase learning of this structured function, i.e., using \((\bm{x}_{t},\bm{z}_{t})\) to learn \(\phi^{\star}\) that is shared among all arms and then using learned \(\hat{\phi}\) to estimate each arm \(a\)’s reward parameters. This more effectively utilizes data than approaches that directly learns the \(r_{a}(\bm{x})\) function, as shown in our later experimental section (e.g., see Figure 1).

\[\text{post-serving context generation process:}\quad\quad\bm{z}_{t}=\phi^{\star}(\bm{x }_{t})+\bm{\epsilon}_{t},\text{ \emph{i.e.,} }\phi^{\star}(\bm{x})=\mathbb{E}[\bm{z}|\bm{x}].\]

Here, \(\bm{\epsilon}_{t}\) is a zero-mean noise vector in \(\mathbb{R}^{d_{z}}\), and \(\phi^{\star}:\mathbb{R}^{d_{x}}\to\mathbb{R}^{d_{z}}\) can be viewed as the post-serving context generating function, which is unknown to the learner. However, we assume \(\phi^{\star}\) is learnable in the following sense.

**Assumption 1** (Generalized learnability of \(\phi^{\star}\)).: _There exists an algorithm that, given \(t\) pairs of examples \(\{(\bm{x}_{s},\bm{z}_{s})\}_{s=1}^{t}\) with arbitrarily chosen \(\bm{x}_{s}\)'s, outputs an estimated function of \(\phi^{\star}:\mathbb{R}^{d_{x}}\to\mathbb{R}^{d_{z}}\) such that for any \(\bm{x}\in\mathbb{R}^{d_{x}}\), the following holds with probability at least \(1-\delta\),_

\[e_{t}^{\delta}\coloneqq\left\|\widehat{\phi}_{t}(\bm{x})-\phi^{\star}(\bm{x} )\right\|_{2}\leq C_{0}\cdot\left(\|\bm{x}\|_{\bm{X}_{t}^{-1}}^{2}\right)^{ \alpha}\cdot\log\left(t/\delta\right),\]

_where \(\alpha\in(0,1/2]\) and \(C_{0}\) is some universal constant._

The aforementioned assumption encompasses a wide range of learning scenarios, each with different rates of convergence. Generally, the value of \(\alpha\) is directly proportional to the speed of learning; the larger the value of \(\alpha\), the quicker the learning rate. Later, we will demonstrate that the regret of our algorithm is proportional to \(O(T^{1-\alpha})\), exhibiting a graceful degradation as \(\alpha\) decreases. The ensuing proposition demonstrates that for linear functions, \(\alpha=1/2\). This represents the best learning rate that can be accommodated2. In this scenario, the regret of our algorithm is \(O(\sqrt{T})\), aligning with the situation devoid of post-serving contexts (Li et al., 2010; Abbasi-Yadkori et al., 2011).

Footnote 2: To be precise, our analysis can extend to instances where \(\alpha>1/2\). However, this would not enhance the regret bound since the regret is already \(\Omega(\sqrt{T})\), even with knowledge of \(\bm{z}\). This would only further complicate our notation, and therefore, such situations are not explicitly examined in this paper.

**Observation 2**.: _Suppose \(\phi(\cdot)\) is a linear function, i.e., \(\phi(\bm{x})=\bm{\Phi}^{\top}\bm{x}\) for some \(\bm{\Phi}\in\mathbb{R}^{d_{x}\times d_{z}}\), then \(e_{t}^{\delta}=\mathcal{O}\left(\|\bm{x}\|_{\bm{X}_{t}^{-1}}\cdot\log\left(t/ \delta\right)\right)\)._

This observation follows from the following inequalities \(\|\phi_{t}(\bm{x})-\phi^{\star}(\bm{x})\|=\|\widehat{\bm{\Phi}}_{t}^{\top}\bm{ x}-\bm{\Phi}^{\star}\bm{x}\|\leq\|\widehat{\bm{\Phi}}_{t}-\bm{\Phi}^{ \star}\|_{\bm{X}_{t}}\cdot\|\bm{x}\|_{\bm{X}_{t}^{-1}}=\mathcal{O}(\|\bm{x}\|_ {\bm{X}_{t}^{-1}}\cdot\log\left(\frac{t}{\delta}\right))\), where the last equation is due to the confidence ellipsoid bound (Abbasi-Yadkori et al., 2011). We refer curious readers to Appendix D.2 for a discussion on other more challenging \(\phi(\cdot)\) functions with possibly worse learning rates \(\alpha\).

### Warm-up: Why Natural Attempts May Be Inadequate?

Given the learnability assumption of \(\phi^{\star}\), one natural idea for solving the above problem is to estimate \(\phi^{\star}\), and then run the standard LinUCB algorithm to estimate \((\bm{\theta}_{a},\bm{\beta}_{a})\) together by treating \((\bm{x}_{t},\widehat{\phi}_{t}(\bm{x}_{t}))\) as the true contexts. Indeed, this is the approach adopted by Wang et al. (2016) for addressing a similar problem of missing contexts \(\bm{z}_{t}\), except that they used a different unsurprised-learning-based approach to estimate the context \(\bm{z}_{t}\) due to not being able to observing any data about \(\bm{z}_{t}\). Given the estimation of \(\widehat{\phi}\), their algorithm -- which we term it as _LinUCB_ (\(\widehat{\phi}\)) -- iteratively carries out the steps below at each iteration \(t\) (see Algorithm 2 for additional details): 1) Estimation of the context-generating function \(\widehat{\phi}_{t}(\cdot)\) from historical data; 2) Solve of the following regularized least square problem for each arm \(a\in\mathcal{A}\), with regularization coefficient \(\lambda\geq 0\):

\[\ell_{t}(\bm{\theta}_{a},\bm{\beta}_{a})=\sum_{s\in[t]:a_{s}=a}\left(r_{s,a}- \bm{x}_{t}^{\top}\bm{\theta}_{a}-\widehat{\phi}_{s}(\bm{x}_{s})^{\top}\bm{ \beta}_{a}\right)^{2}+\lambda\left(\|\bm{\theta}_{a}\|_{2}^{2}+\|\bm{\beta}_{ a}\|_{2}^{2}\right),\] (1)

Under the assumption that the initialized parameters in their estimations are very close to the global optimum, Wang et al. (2016) were able to show the \(O(\sqrt{T})\) regret of this algorithm. However, it turns out that this algorithm will fail to yield an satisfying regret bound without their strong assumption on very close parameter initialization, because the errors arising from \(\widehat{\phi}(\cdot)\) will significantly enlarge the confidence set of \(\widehat{\bm{\theta}}_{a}\) and \(\widehat{\bm{\beta}}_{a}\).3 Thus after removing their initialization assumption, the best possible regret bound we can possibly achieve is of order \(\widetilde{\mathcal{O}}(T^{3/4})\), as illustrated in the subsequent proposition.

Footnote 3: Specifically, Wang et al. (2016) assume that the initial estimation of \(\widehat{\phi}(\cdot)\) is already very close to \(\widehat{\phi}^{\star}(\cdot)\) such that the error arising from \(\widehat{\phi}(\cdot)\) diminishes exponentially fast due to the local convergence property of alternating least squares algorithm (Uschmajew, 2012). This strong assumption avoids significant expansion of the confidence sets, but is less realistic in applications so we do not impose such assumption.

**Proposition 1** (Regret of LinUCB-(\(\widehat{\phi}\))).: _The regret of LinUCB-(\(\widehat{\phi}\)) in Algorithm 2 is upper bounded by \(\widetilde{\mathcal{O}}\left(T^{1-\alpha}d_{u}^{\alpha}+T^{1-\alpha/2}\sqrt{ Kd_{u}^{1+\alpha}}\right)\) with probability at least \(1-\delta\), by carefully setting the regularization coefficient \(\lambda=\Theta(L_{u}d_{u}^{\alpha}T^{1-\alpha}\log{(T/\delta)})\) in Equation 1._

Since \(\alpha\in[0,1/2]\), the best possible regret upper bound above is \(\widetilde{\mathcal{O}}(T^{3/4})\), which is considerably inferior to the sought-after regret bound of \(\widetilde{\mathcal{O}}(\sqrt{T})\). Such deficiency of LinUCB-(\(\widehat{\phi}\)) is further observed in all our experiments in Section 7 as well. These motivate our following design of a new online learning algorithm to address the challenge of post-serving context, during which we also developed a new technical tool which may be of independent interest to the research community.

## 4 A Robustified and Generalized Elliptical Potential Lemma

It turns out that solving the learning problem above requires some novel designs; core to these novelties is a robustified and generalized version of the well-known elliptical potential lemma (EPL), which may be of independent interest. This widely used lemma states a fact about a sequence of vectors \(\bm{x}_{1},\cdots,\bm{x}_{T}\in\mathbb{R}^{d}\). Intuitively, it captures the rate of the sum of additional information contained in each \(\bm{x}_{t}\), relative to its predecessors \(\bm{x}_{1},\cdots,\bm{x}_{t-1}\). Formally,

**Lemma** (Original Elliptical Potential Lemma).: _Suppose (1) \(\bm{X}_{0}\in\mathbb{R}^{d\times d}\) is any positive definite matrix; (2) \(\bm{x}_{1},\ldots,\bm{x}_{T}\in\mathbb{R}^{d}\) is any sequence of vectors; and (3) \(\bm{X}_{t}=\bm{X}_{0}+\sum_{s=1}^{\ell}\bm{x}_{s}\bm{x}_{s}^{\top}\). Then the following inequality holds_

\[\sum_{t=1}^{T}1\wedge\|\bm{x}_{t}\|_{\bm{x}_{t-1}^{-1}}^{2}\leq 2\log\left(\frac{ \det\bm{X}_{T}}{\det\bm{X}_{0}}\right),\]

_where \(a\wedge b=\min\{a,b\}\) is the \(\min\) among \(a,b\in\mathbb{R}\)._

To address our new contextual bandit setup with post-serving contexts, it turns out that we will need to robustify and generalize the above lemma to accommodate noises in \(\bm{x}_{t}\) vectors and slower learning rates. Specifically, we present the following variant of the EPL lemma.

**Lemma 1** (Generalized Elliptical Potential Lemma).: _Suppose (1) \(\bm{X}_{0}\in\mathbb{R}^{d\times d}\) is any positive definite matrix; (2) \(\bm{x}_{1},\dots,\bm{x}_{T}\in\mathbb{R}^{d}\) is a sequence of vectors with bounded \(l_{2}\) norm \(\max_{t}\|\bm{x}_{t}\|\leq L_{x}\); (3) \(\bm{\epsilon}_{1},\dots,\bm{\epsilon}_{T}\in\mathbb{R}^{d}\) is a sequence of independent (not necessarily identical) bounded zero-mean noises satisfying \(\max_{t}\|\bm{\epsilon}_{t}\|\leq L_{\epsilon}\) and \(\mathbb{E}[\bm{\epsilon}_{t}\bm{\epsilon}_{t}^{\top}]\succcurlyeq\sigma_{\epsilon }^{2}\bm{I}\) for any \(t\); and (4) \(\widetilde{\bm{X}}_{t}\) is defined as follows:_

\[\widetilde{\bm{X}}_{t}=\bm{X}_{0}+\sum_{s=1}^{t}(\bm{x}_{s}+\bm{ \epsilon}_{s})(\bm{x}_{s}+\bm{\epsilon}_{s})^{\top}\in\mathbb{R}^{d\times d}.\]

_Then, for any \(p\in[0,1]\), the following inequality holds with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}\left(1\wedge\|\bm{x}_{t}\|_{\widetilde{\bm{X}}_{t- 1}^{-1}}^{2}\right)^{p}\leq 2^{p}T^{1-p}\log^{p}\left(\frac{\det\bm{X}_{T}}{ \det\bm{X}_{0}}\right)+\frac{8L_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2}}{ \sigma_{\epsilon}^{4}}\log\left(\frac{32dL_{\epsilon}^{2}(L_{\epsilon}+L_{x})^ {2}}{\delta\sigma_{\epsilon}^{4}}\right)\] (2)

Note that the second term is independent of time horizon \(T\) and only depends on the setup parameters. Generally, this can be treated as a constant. Before describing main proof idea of the lemma, we make a few remarks regarding Lemma 1 to highlight the significance of these generalizations.

1. The original Elliptical Potential Lemma (EPL) corresponds to the specific case of \(p=1\), while Lemma 1 is applicable for any \(p\in[0,1]\). Notably, the \((1-p)\) rate in the \(T^{1-p}\) term of Inequality 2 is tight for _every_\(p\). In fact, this rate is tight even for \(\bm{x}_{t}=1\in\mathbb{R},\forall t\) and \(\bm{X}_{0}=1\in\mathbb{R}\) since, under these conditions, \(\|\bm{x}_{t}\|_{\bm{X}_{t-1}^{-1}}^{2}=1/t\) and, consequently, \(\sum_{t=1}^{T}\left(1\wedge\|\bm{x}_{t}\|_{\bm{X}_{t-1}^{-1}}^{2}\right)^{p}= \sum_{t=1}^{T}t^{-p}\), yielding a rate of \(T^{1-p}\). This additional flexibility gained by allowing a general \(p\in[0,1]\) (with the original EPL corresponding to \(p=1\)) helps us to accommodate slower convergence rates when learning the mean context from observed noisy contexts, as formalized in Assumption 1.
2. A crucial distinction between Lemma 1 and the original EPL lies in the definition of the noisy data matrix \(\widetilde{\bm{X}}_{t}\) in Equation 1, which permits noise. However, the measured context vector \(\bm{x}_{t}\) does _not_ have noise. This is beneficial in scenarios where a learner observes noisy contexts but seeks to establish an upper bound on the prediction error based on the underlying noise-free context or the mean context. Such situations are not rare in real applications; our problem of contextual bandits with post-serving contexts is precisely one of such case -- while choosing an arm, we can estimate the mean post-serving context conditioned on the observable pre-serving context but are only able to observe the noisy realization of post-serving contexts after acting.
3. Other generalized variants of the EPL have been recently proposed and found to be useful in different contexts. For instance, Carpentier et al. (2020) extends the EPL to allow for the \(\bm{X}_{t}^{-p}\)-norm, as opposed to the \(\bm{X}_{t}^{-1}\)-norm, while Hamidi and Bayati (2022) explores a generalized form of the \(1\wedge\|\varphi(\bm{x}_{t})\|_{\bm{X}_{t-1}^{-1}}^{2}\) term, which is motivated by variance reduction in non-Gaussian linear regression models. Nevertheless, to the best of our knowledge, our generalized version is novel and has not been identified in prior works.

Proof Sketche of Lemma 1.: The formal proof of this lemma is involved and deferred to Appendix B.1. At a high level, our proof follows procedure for proving the original EPL. However, to accommodate the noises in the data matrix, we have to introduce new matrix concentration tools to the original (primarily algebraic) proof, and also identify the right conditions for the argument to go through. A key lemma to our proof is a high probability bound regarding the constructed noisy data matrix \(\widetilde{\bm{X}}_{t}\) (Lemma 2 in Appendix B.1) that we derive based on Bernstein's Inequality for matrices under spectral norm (Tropp et al., 2015). We prove that, under mild assumptions on the noise, \(\|\bm{x}_{t}\|_{\widetilde{\bm{X}}_{t-1}^{-1}}^{2}\leq\|\bm{x}_{t}\|_{\bm{X}_{t -1}^{-1}}^{2}\) with high probability for any \(t\). Next, we have to apply the union bound and this lemma to show that the above matrix inequality holds for _every_\(t\geq 1\) with high probability. Unfortunately, this turns out to not be true because when \(t\) is very small (e.g., \(t=1\)), the above inequality cannot hold with high probability. Therefore, we have to use the union bound in a carefully tailored way by excluding all \(t\)'s that are smaller than a certain threshold (chosen optimally by solving certain inequalities) and handling these terms with small \(t\) separately (which is the reason of the second \(\mathcal{O}(\log(1/\delta))\) term in Inequality 2). Finally, we refine the analysis of sthe standard EPL by allowing the exponent \(p\) in\((1\wedge\|\bm{x}_{t}\|_{\widetilde{\bm{X}}_{t-1}^{-1}}^{2})^{p}\) and derive an upper bound on the sum \(\sum_{t=1}^{T}(1\wedge\|\bm{x}_{t}\|_{\widetilde{\bm{X}}_{t-1}^{-1}}^{2})^{p}\) with high probability. These together yeilds a robustified and generalized version of EPL as in Lemma 1. 

## 5 No Regret Learning in Linear Bandits with Post-Serving Contexts

### The Main Algorithm

In the ensuing section, we introduce our algorithm, poLinUCB, designed to enhance linear contextual bandit learning through the incorporation of post-serving contexts and address the issue arose from the algorithm introduced in Section 3.2. The corresponding pseudo-code is delineated in Algorithm 1. Unlike the traditional LinUCB algorithm, which solely learns and sustains confidence sets for parameters (i.e., \(\widehat{\bm{\beta}}_{a}\) and \(\widehat{\bm{\theta}}_{a}\) for each \(a\)), our algorithm also _simultaneously_ manages the same for the post-serving context generating function, \(\widehat{\phi}(\cdot)\). Below, we expound on our methodology for parameter learning and confidence set construction.

**Parameter learning**. During each iteration \(t\), we fit the function \(\widehat{\phi}_{t}(\cdot)\) and the parameters \(\{\widehat{\bm{\theta}}_{t,a}\}_{a\in\mathcal{A}}\) and \(\{\widehat{\bm{\beta}}_{t,a}\}_{a\in\mathcal{A}}\). To fit \(\widehat{\phi}_{t}(\cdot)\), resort to the conventional empirical risk minimization (ERM) framework. As for \(\{\widehat{\bm{\theta}}_{t,a}\}_{a\in\mathcal{A}}\) and \(\{\widehat{\bm{\beta}}_{t,a}\}_{a\in\mathcal{A}}\), we solve the following least squared problem for each arm \(a\),

\[\ell_{t}(\bm{\theta}_{a},\bm{\beta}_{a})=\sum_{s\in[t]:a_{s}=a}\left(r_{s,a}- \bm{x}_{s}^{\top}\bm{\theta}_{a}-\bm{z}_{s}^{\top}\bm{\beta}_{a}\right)^{2}+ \lambda\left(\|\bm{\theta}_{a}\|_{2}^{2}+\|\bm{\beta}_{a}\|_{2}^{2}\right).\] (3)

For convenience, we use \(\bm{w}\) and \(\bm{u}\) to denote \((\bm{\theta},\bm{\beta})\) and \((\bm{x},\bm{z})\) respectively. The closed-form solutions to \(\widehat{\bm{\theta}}_{t,a}\) and \(\widehat{\bm{\beta}}_{t,a}\) for each arm \(a\in\mathcal{A}\) are

\[\widehat{\bm{w}}_{t,a}\coloneqq\begin{bmatrix}\widehat{\bm{\theta}}_{t,a}\\ \widehat{\bm{\beta}}_{t,a}\end{bmatrix}=\bm{A}_{t,a}^{-1}\bm{b}_{t,a}\text{, where }\bm{A}_{t,a}=\lambda\bm{I}+\sum_{s:a_{s}=a}^{t}\bm{u}_{s}\bm{u}_{s}^{\top}\quad \text{and}\quad\bm{b}_{t,a}=\sum_{s:a_{s}=a}^{t}r_{s,a}\bm{u}_{s}.\] (4)

**Confidence set construction.** At iteration \(t\), we construct the confidence set for \(\widehat{\phi}_{t}(\bm{x}_{t})\) by

\[\mathcal{C}_{t}\left(\widehat{\phi}_{t},\bm{x}_{t}\right)\coloneqq\left\{\bm {z}\in\mathbb{R}^{d}:\left\|\widehat{\phi}_{t}(\bm{x}_{t})-\bm{z}\right\|_{2} \leq e_{t}^{\delta}\right\}.\] (5)

Similarly, we can construct the confidence set for the parameters \(\widehat{\bm{w}}_{t,a}\) for each arm \(a\in\mathcal{A}\) by

\[\mathcal{C}_{t}\left(\widehat{\bm{w}}_{t,a}\right)\coloneqq\left\{\bm{w}\in \mathbb{R}^{d_{x}+d_{z}}:\left\|\bm{w}-\widehat{\bm{w}}_{t,a}\right\|_{\bm{A} _{t,a}}\leq\zeta_{t,a}\right\},\] (6)

where \(\zeta_{t,a}=2\sqrt{\lambda}+R_{q}\sqrt{d_{u}\log\left((1+n_{t}(a)L_{u}^{2}/ \lambda)/\delta\right)}\) and \(n_{t}(a)=\sum_{s=1}^{t}\mathbb{1}[a_{s}=a]\). Additionally, we further define \(\zeta_{t}\coloneqq\max_{a\in\mathcal{A}}\zeta_{t,a}\). By the assumption 1 and Lemma 3, we have the followings hold with probability at least \(1-\delta\) for each of the following events,

\[\phi^{\star}(\bm{x}_{t})\in\mathcal{C}_{t}\left(\widehat{\phi}_{t},\bm{x}_{t} \right)\quad\text{and}\quad\bm{w}^{\star}\in\mathcal{C}_{t}\left(\widehat{\bm {w}}_{t,a}\right).\] (7)

### Regret Analysis

In the forthcoming section, we establish the regret bound. Our proof is predicated upon the conventional proof of LinUCB (Li et al., 2010) in conjunction with our robust elliptical potential lemma. The pseudo-regret (Audibert et al., 2009) within this partial contextual bandit problem is defined as,

\[R_{T}=\text{Regret}(T)=\sum_{t=1}^{T}\left(r_{t,a_{t}^{\star}}-r_{t,a_{t}} \right),\] (8)

in which we reload the notation of reward by ignoring the noise,

\[r_{t,a}=\langle\bm{\theta}_{a}^{\star},\bm{x}_{t}\rangle+\langle\bm{\beta}_{a} ^{\star},\phi^{\star}(\bm{x}_{t})\rangle\quad\text{and}\quad a_{t}^{\star}= \operatorname*{arg\,max}_{a\in\mathcal{A}}\,\langle\bm{\theta}_{a}^{\star}, \bm{x}_{t}\rangle+\langle\bm{\beta}_{a}^{\star},\phi^{\star}(\bm{x}_{t})\rangle.\] (9)

It is crucial to note that our definition of the optimal action, \(a_{t}^{\star}\), in Eq. 9 depends on \(\phi^{\star}(\bm{x}_{t})\) as opposed to \(\bm{z}_{t}\). This dependency ensures a more pragmatic benchmark, as otherwise, the noise present in \(\bm{z}\) would invariably lead to a linear regret, regardless of the algorithm implemented. In the ensuing section, we present our principal theoretical outcomes, which provide an upper bound on the regret of our poLinUCB algorithm.

**Theorem 1** (Regret of poLinUCB).: _The regret of poLinUCB in Algorithm 1 is upper bounded by \(\widetilde{\mathcal{O}}\left(T^{1-\alpha}d_{u}^{\alpha}+d_{u}\sqrt{TK}\right)\) with probability at least \(1-\delta\), if \(T=\Omega(\log(1/\delta))\)._

The first term in the bound is implicated by learning the function \(\phi^{\star}(\cdot)\). Conversely, the second term resembles the one derived in conventional contextual linear bandits, with the exception that our dependency on \(d_{u}\) is linear. This linear dependency is a direct consequence of our generalized robust elliptical potential lemma. The proof is deferred in Appendix B.2.

## 6 Generalizations

So far we have focused on a basic linear bandit setup with post-serving features. Our results and analysis can be easily generalized to other variants of linear bandits, including those with feature mappings, and below we highlight some of these generalizations. They use similar proof ideas, up to some technical modifications; we thus defer all their formal proofs to Appendix B.3.

### Generalization to Action-Dependent Contexts

Our basic setup in Section 3 has a single context \(\bm{x}_{t}\) at any time step \(t\). This can be generalized to action-dependent contexts settings as studied in previous works (e.g., Li et al. (2010)). That is, during each iteration indexed by \(t\), the learning algorithm observes a context \(\bm{x}_{t,a}\) for each individual arm \(a\in\mathcal{A}\). Upon executing the action of pulling arm \(a_{t}\), the corresponding post-serving context \(\bm{z}_{t,a_{t}}\) is subsequently revealed. Notwithstanding, the post-serving context for all alternative arms remains unobserved. The entire procedure is the same as that of Section 3.

In extending this framework, we persist in our assumption that for each arm \(a\in\mathcal{A}\), there exists a specific function \(\phi_{a}^{\star}(\cdot):\mathbb{R}^{d_{x}}\to\mathbb{R}^{d_{z}}\) that generates the post-serving context \(\bm{z}\) upon receiving \(\bm{x}\) associated with arm \(a\in\mathcal{A}\). The primary deviation from our preliminary setup lies in the fact that we now require the function \(\phi_{a}^{\star}(\cdot)\) to be learned for each arm independently. The reward is generated as

\[r_{t,a_{t}}=\langle\bm{\theta}_{a_{t}}^{\star},\bm{x}_{t,a_{t}}\rangle+\langle \bm{\beta}_{a_{t}}^{\star},\bm{z}_{t,a_{t}}\rangle+\eta_{t}.\]

The following proposition shows our regret bound for this action-dependent context case. Its proof largely draws upon the proof idea of Theorem 1 and also relies on the generalized EPL Lemma 1.

**Proposition 2**.: _The regret of poLinUCB in Algorithm 1 for action-dependent contexts is upper bounded by \(\widetilde{\mathcal{O}}\left(T^{1-\alpha}d_{u}^{\alpha}\sqrt{K}+d_{u}\sqrt{TK}\right)\) with probability at least \(1-\delta\) if \(T=\Omega(\log(1/\delta))\)._

The main difference with the bound in Theorem 1 is the additional \(\sqrt{K}\) appeared in the first term, which is caused by learning multiple \(\phi_{a}^{\star}(\cdot)\) functions with \(a\in\mathcal{A}\).

### Generalization to Linear Stochastic Bandits

Another variant of linear bandits is the _linear stochastic bandits_ setup (see, e.g., (Abbasi-Yadkori et al., 2011)). This model allows infinitely many arms, which consists of a decision set \(D_{t}\subseteq\mathbb{R}^{d}\) at time \(t\), and the learner picks an action \(\bm{x}_{t}\in D_{t}\). This setup naturally generalizes to our problem with post-serving contexts. That is, at iteration \(t\), the learner selects an arm \(\bm{x}_{t}\in D_{t}\) first, receives reward \(r_{t,\bm{x}_{t}}\), and then observe the post-serving feature \(\bm{z}_{t}\) conditioned on \(\bm{x}_{t}\). Similarly, we assume the existence of a mapping \(\phi^{\star}(\bm{x}_{t})=\mathbb{E}[\bm{z}_{t}|\bm{x}_{z}]\) that satisfies the Assumption 1. Consequently, the realized reward is generated as follows where \(\bm{\theta}^{\star},\bm{\beta}^{\star}\) are unknown parameters:

\[r_{t,\bm{x}_{t}}=\langle\bm{x}_{t},\bm{\theta}^{\star}\rangle+\langle\bm{z}_{t },\bm{\beta}^{\star}\rangle+\eta_{t}.\]

Therefore, the learner needs to estimate the linear parameters \(\widehat{\bm{\theta}}\) and \(\widehat{\bm{\beta}}\), as well as the function \(\widehat{\phi}(\cdot)\). We obtain the following proposition for the this setup.

**Proposition 3**.: _The regret of poLinUCB in Algorithm 1 for the above setting is upper bounded by \(\widetilde{\mathcal{O}}\left(T^{1-\alpha}d_{u}^{\alpha}+d_{u}\sqrt{T}\right)\) with probability at least \(1-\delta\) if \(T=\Omega(\log(1/\delta))\)._

### Generalization to Linear Bandits with Feature Mappings

Finally, we briefly remark that while we have so far assumed that the arm parameters are directly linear in the context \(\bm{x}_{t},\bm{z}_{t}\), just like classic linear bandits our analysis can be easily generalized to accommodate feature mapping \(\pi^{x}(\bm{x}_{t})\) and \(\pi^{z}(\bm{z}_{t})=\pi^{z}(\phi(\bm{x}_{t})+\varepsilon_{t})\). Specifically, if the reward generation process is \(r_{a}=\langle\bm{\theta}_{a}^{\star},\pi^{x}(\bm{x}_{t})\rangle+\langle\bm{ \beta}_{a}^{\star},\pi^{z}(\bm{z}_{t})\rangle+\eta_{t}\) instead, then we can simply view \(\tilde{\bm{x}}_{t}=\pi^{x}(\bm{x}_{t})\) and \(\tilde{\bm{z}}_{t}=\pi^{z}(\bm{z}_{t})\) as the new features, with \(\phi(\bm{x}_{t})=\mathsf{E}_{\bm{\epsilon}_{t}}[\pi^{z}(\phi(\bm{x}_{t})+\bm{ \epsilon}_{t})]\). By working with \(\tilde{\bm{x}}_{t},\tilde{\bm{z}}_{t},\tilde{\phi}\), we shall obtain the same guarantees as Theorem 1.

## 7 Experiments

This section presents a comprehensive evaluation of our proposed poLinUCB algorithm on both synthetic and real-world data, demonstrating its effectiveness in incorporating follow-up information and outperforming the LinUCB(\(\widehat{\phi}\)) variant. More empirical results can be found in Appendix D.1.

### Synthetic Data with Ground Truth Models

Evaluation Setup.We adopt three different synthetic environments that are representative of a range of mappings from the pre-serving context to the post-serving context: polynomial, periodicical and linear functions. The pre-serving contexts are sampled from a uniform noise in the range \([-10,10]^{d_{x}}\), and Gaussian noise is employed for both the post-serving contexts and the rewards. In each environment, the dimensions of the pre-serving context (\(d_{x}\)) and the post-serving context (\(d_{z}\)) are of \(100\) and \(5\), respectively with 10 arms (\(K\)). The evaluation spans \(T=1000\) or \(5000\) time steps, and each experiment is repeated with \(10\) different seeds. The cumulative regret for each policy in each environment is then calculated to provide a compararison.

Results and Discussion.Our experimental results, which are presented graphically in Figures 1, provide strong evidence of the superiority of our proposed poLinUCB algorithm. Across all setups, we

Figure 1: Cumulative Regret in three synthetic environments. Comparisons of different algorithms in terms of cumulative regret across the three synthetic environments. Our proposed poLinUCB (ours) consistently outperforms other strategies (except for LinUCB which has access to the post-serving context during arm selection), showcasing its effectiveness in utilizing post-serving contexts. The shaded area denotes the standard error computed using 10 different random seeds.

observe that the LinUCB (\(x\) and \(z\)) strategy, which has access to the post-serving context during arm selection, consistently delivers the best performance, thus serving as the upper bound for comparison. On the other hand, the Random policy, which does not exploit any environment information, performs the worst, serving as the lower bound. Our proposed poLinUCB (ours) outperforms all the other strategies, including the LinUCB (\(\hat{\phi}\)) variant, in all three setups, showcasing its effectiveness in adaptively handling various mappings from the pre-serving context to the post-serving context. Importantly, poLinUCB delivers significantly superior performance to LinUCB (\(x\) only), which operates solely based on the pre-serving context.

### Real World Data without Ground Truth

**Evaluation Setup.** The evaluation was conducted on a real-world dataset, MovieLens (Harper and Konstan, 2015), where the task is to recommend movies (arms) to a incoming user (context). Following Yao et al. (2023), we first map both movies and users to 32-dimensional real vectors using a neural network trained for predicting the rating. Initially, \(K=5\) movies were randomly sampled to serve as our arms and were held fixed throughout the experiment. The user feature vectors were divided into two parts serving as the pre-serving context (\(d_{x}=25\)) and the post-serving context (\(d_{z}=7\)). We fit the function \(\phi(\bm{x})\) using a two-layer neural network with 64 hidden units and ReLU activation. The network was trained using the Adam optimizer with a learning rate of 1e-3. At each iteration, we randomly sampled a user from the dataset and exposed only the pre-serving context \(\bm{x}\) to our algorithm. The reward was computed as the dot product of the user's feature vector and the selected movie's feature vector and was revealed post the movie selection. The evaluation spanned \(T=500\) iterations and repeated with \(10\) seeds.

**Results and Discussion.** The experimental results, presented in Figure 2, demonstrate the effectiveness of our proposed algorithm. The overall pattern is similar to it observed in our synthetic experiments. Our proposed policy consistently outperforms the other strategies (except for LinUCB with both pre-serving and post-serving features). Significantly, our algorithm yields superior performance compared to policies operating solely on the pre-serving context, thereby demonstrating its effectiveness in leveraging the post-serving information.

## 8 Conclusions and Limitations

**Conlusions.** In this work, we have introduced a novel contextual bandit framework that incorporates post-serving contexts, thereby widening the range of complex real-world challenges it can address. By leveraging historical data, our proposed algorithm, poLinUCB, estimates the functional mapping from pre-serving to post-serving contexts, leading to improved online learning efficiency. For the purpose of theoretical analysis, the elliptical potential lemma has been expanded to manage noise within post-serving contexts, a development which may have wider applicability beyond this particular framework. Extensive empirical tests on synthetic and real-world datasets have demonstrated the significant benefits of utilizing post-serving contexts and the superior performance of our algorithm compared to state-of-the-art approaches.

**Limitations.** Our theoretical analysis hinges on a crucial assumption that the function \(\phi^{*}(\cdot)\) is learnable, which may not always be satisfied. This is particularly a concern when the post-serving contexts may hold additional information that cannot be deduced from the pre-serving context, irrespective of the amount of data collected. In such scenarios, the function mapping from the preserving context to the post-serving context may be much more difficult to learn, or even not learnable. Consequently, a linear regret may be inevitable due to model misspecification. However, from a practical point of view, our empirical findings from the real-world MovieLens dataset demonstrate that modeling the functional relationship between the pre-serving and post-serving contexts can still significantly enhance the learning efficiency. We hope our approaches can inform the design of practical algorithms that more effectively utilizes post-serving data in recommendations.

**Acknowledgement.** This work is supported by an NSF Award CCF-2303372, an Army Research Office Award W911NF-23-1-0030, and an Office of Naval Research Award N00014-23-1-2802.

Figure 2: Results on MovieLens.

## References

* Abbasi-Yadkori et al. (2011) Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* Abe et al. (2003) Naoki Abe, Alan W Biermann, and Philip M Long. Reinforcement learning with immediate rewards and linear hypotheses. _Algorithmica_, 37:263-293, 2003.
* Agarwal et al. (2014) Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In _International Conference on Machine Learning_, pages 1638-1646. PMLR, 2014.
* Audibert et al. (2009) Jean-Yves Audibert, Remi Munos, and Csaba Szepesvari. Exploration-exploitation tradeoff using variance estimates in multi-armed bandits. _Theoretical Computer Science_, 410(19):1876-1902, 2009.
* Auer (2002) Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. _Journal of Machine Learning Research_, 3(Nov):397-422, 2002.
* Carpentier et al. (2020) Alexandra Carpentier, Claire Vernade, and Yasin Abbasi-Yadkori. The elliptical potential lemma revisited. _arXiv preprint arXiv:2010.10182_, 2020.
* Chu et al. (2011) Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pages 208-214. JMLR Workshop and Conference Proceedings, 2011.
* Dani et al. (2008) Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. 2008.
* Filippi et al. (2010) Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric bandits: The generalized linear case. _Advances in Neural Information Processing Systems_, 23, 2010.
* Hamidi and Bayati (2022) Nima Hamidi and Mohsen Bayati. The elliptical potential lemma for general distributions with an application to linear thompson sampling. _Operations Research_, 2022.
* Harper and Konstan (2015) F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. _Acm transactions on interactive intelligent systems (tiis)_, 5(4):1-19, 2015.
* Jun et al. (2017) Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, and Rebecca Willett. Scalable generalized linear bandits: Online computation and hashing. _Advances in Neural Information Processing Systems_, 30, 2017.
* Lai and Wei (1982) Tze Leung Lai and Ching Zong Wei. Least squares estimates in stochastic regression models with applications to identification and control of dynamic systems. _The Annals of Statistics_, 10(1):154-166, 1982.
* Lattimore and Szepesvari (2020) Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* Li et al. (2010) Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In _Proceedings of the 19th international conference on World wide web_, pages 661-670, 2010.
* Li et al. (2017) Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In _International Conference on Machine Learning_, pages 2071-2080. PMLR, 2017.
* Li et al. (2019) Yingkai Li, Yining Wang, and Yuan Zhou. Nearly minimax-optimal regret for linearly parameterized bandits. In _Conference on Learning Theory_, pages 2173-2174. PMLR, 2019.
* Lu et al. (2010) Tyler Lu, David Pal, and Martin Pal. Contextual multi-armed bandits. In _Proceedings of the Thirteenth international conference on Artificial Intelligence and Statistics_, pages 485-492. JMLR Workshop and Conference Proceedings, 2010.
* Nuara et al. (2018) Alessandro Nuara, Francesco Trovo, Nicola Gatti, and Marcello Restelli. A combinatorial-bandit algorithm for the online joint bid/budget optimization of pay-per-click advertising campaigns. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* Ni et al. (2019)* Park and Faradonbeh (2021) Hongju Park and Mohamad Kazem Shirani Faradonbeh. Analysis of thompson sampling for partially observable contextual multi-armed bandits. _IEEE Control Systems Letters_, 6:2150-2155, 2021.
* Qi et al. (2018) Yi Qi, Qingyun Wu, Hongning Wang, Jie Tang, and Maosong Sun. Bandit learning with implicit feedback. _Advances in Neural Information Processing Systems_, 31, 2018.
* Rusmevichientong and Tsitsiklis (2010) Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. _Mathematics of Operations Research_, 35(2):395-411, 2010.
* Schwartz et al. (2017) Eric M Schwartz, Eric T Bradlow, and Peter S Fader. Customer acquisition via display advertising using multi-armed bandit experiments. _Marketing Science_, 36(4):500-522, 2017.
* Tewari and Murphy (2017) Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. _Mobile Health: Sensors, Analytic Methods, and Applications_, pages 495-517, 2017.
* Tibshirani (2017) Ryan Tibshirani. Nonparametric regression (and classification). 2017.
* Tropp et al. (2015) Joel A Tropp et al. An introduction to matrix concentration inequalities. _Foundations and Trends(r) in Machine Learning_, 8(1-2):1-230, 2015.
* Uschmajew (2012) Andre Uschmajew. Local convergence of the alternating least squares algorithm for canonical tensor approximation. _SIAM Journal on Matrix Analysis and Applications_, 33(2):639-652, 2012.
* Wang et al. (2016) Huazheng Wang, Qingyun Wu, and Hongning Wang. Learning hidden features for contextual bandits. In _Proceedings of the 25th ACM international on conference on information and knowledge management_, pages 1633-1642, 2016.
* Wang et al. (2022) Huazheng Wang, Haifeng Xu, and Hongning Wang. When are linear stochastic bandits attackable? In _International Conference on Machine Learning_, pages 23254-23273. PMLR, 2022.
* Wu et al. (2016) Qingyun Wu, Huazheng Wang, Quanquan Gu, and Hongning Wang. Contextual bandits in a collaborative environment. In _Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval_, pages 529-538, 2016.
* Yang and Ren (2021) Jianyi Yang and Shaolei Ren. Robust bandit learning with imperfect context. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 10594-10602, 2021.
* Yang et al. (2020) Shangdong Yang, Hao Wang, Chenyu Zhang, and Yang Gao. Contextual bandits with hidden features to online recommendation via sparse interactions. _IEEE Intelligent Systems_, 35(5):62-72, 2020.
* Yang and Dunson (2016) Yun Yang and David B Dunson. Bayesian manifold regression. 2016.
* Yao et al. (2023) Fan Yao, Chuanhao Li, Denis Nekipelov, Hongning Wang, and Haifeng Xu. How bad is top-\(k\) recommendation under competing content creators? _International Conference on Machine Learning_, 2023.
* Zhou et al. (2020) Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration. In _International Conference on Machine Learning_, pages 11492-11502. PMLR, 2020.
* Zhu and Kveton (2022) Rong Zhu and Branislav Kveton. Robust contextual linear bandits. _arXiv preprint arXiv:2210.14483_, 2022.

Algorithm and Regret Analysis of LinUCB(\(\widehat{\phi}\))

We present the details of the algorithm described in Section 3.2 and the proof of the regret bound.

### Main Algorithm

Parameter learning.We consider solving the following regularized least squared problem for estimating \(\{\widehat{\bm{\theta}}_{t,a}\}_{a\in\mathcal{A}}\) and \(\{\widehat{\bm{\beta}}_{t,a}\}_{a\in\mathcal{A}}\) for each arm \(a\):

\[\ell_{t}(\bm{\theta}_{a},\bm{\beta}_{a})=\sum_{s:a_{s}=a}^{t}\left(r_{s,a}-\bm {x}_{t}^{\top}\bm{\theta}_{a}-\widehat{\phi}_{s}(\bm{x}_{s})^{\top}\bm{\beta}_ {a}\right)^{2}+\lambda\left(\|\bm{\theta}_{a}\|_{2}^{2}+\|\bm{\beta}_{a}\|_{2} ^{2}\right),\] (10)

where \(\lambda\geq 0\) are penalty factors ensuring the uniqueness of minimizers \(\widehat{\bm{\theta}}_{t,a}\) and \(\widehat{\bm{\beta}}_{t,a}\).

In the same convention, we use \(\bm{w}\) to denote \((\bm{\theta},\bm{\beta})\), and \(\bm{u}\) to denote \(\left(\bm{x},\widehat{\phi}(\bm{x})\right)\). The closed-form solutions for \(\widehat{\bm{\theta}}_{t,a}\) and \(\widehat{\bm{\beta}}_{t,a}\) in this least squared problem then become:

\[\widehat{\bm{w}}_{t,a}\coloneqq\begin{bmatrix}\widehat{\bm{\theta}}_{t,a}\\ \widehat{\bm{\beta}}_{t,a}\end{bmatrix}=\bm{A}_{t,a}^{-1}\bm{b}_{t,a},\]

where we reload the notations of \(\bm{A}_{t,a}\) and \(\bm{b}_{t,a}\),

\[\bm{A}_{t,a}=\lambda\bm{I}+\sum_{s:a_{s}=a}^{t}\bm{u}_{s}\bm{u}_{s}^{\top} \quad\text{and}\quad\bm{b}_{t,a}=\sum_{s:a_{s}=a}^{t}r_{s,a}\bm{u}_{s}.\] (11)

Confidence set construction.At iteration \(t\), we construct the confidence set for \(\widehat{\phi}_{t}(\bm{x}_{t})\) by

\[\mathcal{C}_{t}\left(\widehat{\phi}_{t},\bm{x}_{t}\right)\coloneqq\left\{\bm {z}\in\mathbb{R}^{d}:\left\|\widehat{\phi}_{t}(\bm{x}_{t})-\phi^{\star}(\bm{x }_{t})\right\|_{2}\leq e_{t}^{\delta}\right\}.\] (12)

The construction of the confidence set for \(\widehat{\bm{w}}_{t,a}\) will be different, as we are using the predicted value \(\widehat{\phi}_{t}(\cdot)\) for linear regression. Consider the following,

\[\bm{A}_{t,a}\left(\begin{bmatrix}\widehat{\bm{\theta}}_{t,a}\\ \widehat{\bm{\beta}}_{t,a}\end{bmatrix}-\begin{bmatrix}\bm{\theta}_{a}^{*} \\ \bm{\beta}_{a}^{*}\end{bmatrix}\right) =\underbrace{\sum_{s:a_{s}=a}^{t}\left(\bm{\epsilon}_{s}^{\top} \bm{\beta}_{a}^{*}\begin{bmatrix}\bm{x}_{s}\\ \widehat{\phi}_{s}(\bm{x}_{s})\end{bmatrix}\right)}_{\mbox{\footnotesize$ \begin{array}{c}1\\ 2\end{array}$}}+\underbrace{\sum_{s:a_{s}=a}^{t}\left(\left(\phi^{*}(\bm{x}_{s}) -\widehat{\phi}_{s}(\bm{x}_{s})\right)^{\top}\bm{\beta}_{a}^{*}\begin{bmatrix} \bm{x}_{s}\\ \widehat{\phi}_{s}(\bm{x}_{s})\end{bmatrix}\right)}_{\mbox{\footnotesize$ \begin{array}{c}2\\ 2\end{array}$}}\] \[+\underbrace{\sum_{s:a_{s}=a}^{t}\eta_{s}\left[\underbrace{\bm{x}_ {s}}_{\widehat{\phi}_{s}(\bm{x}_{s})}\right)}_{\mbox{\footnotesize$ \begin{array}{c}3\\ 3\end{array}$}}-\underbrace{\lambda\underbrace{\begin{bmatrix}\bm{\theta}_{a}^{*} \\ \bm{\beta}_{a}^{*}\end{bmatrix}}_{\mbox{\footnotesize$\begin{array}{c}4\\ 4\end{array}$}}}\]

Therefore, the confidence set will be enlarged due to the error introduced by \(\widehat{\phi}_{t}(\cdot)\). In the below, we derive the confidence set. To build the confidence set, we need to bound

\[\|\widehat{\bm{w}}_{t,a}-\bm{w}_{t,a}^{*}\|_{\bm{A}_{t,a}}=\left\|\begin{bmatrix} \widehat{\bm{\theta}}_{t,a}\\ \widehat{\bm{\beta}}_{t,a}\end{bmatrix}-\begin{bmatrix}\bm{\theta}_{a}^{*} \\ \bm{\beta}_{a}^{*}\end{bmatrix}\right\|_{\bm{A}_{t,a}}=\left\|\begin{gathered} \mbox{\footnotesize$\begin{array}{c}1\\ 2\end{array}$}+\mbox{\footnotesize$\begin{array}{c}2\\ 3\end{array}$}+\mbox{\footnotesize$\begin{array}{c}3\\ 4\end{array}$}\right\|_{\bm{A}_{t,a}^{-1}}\]

Since both \(\{\bm{\epsilon}_{s}\}_{s=1}^{t}\) and \(\{\eta_{s}\}_{s=1}^{t}\) are i.i.d sub-Gaussian random variables, respectively, we can use the self-normalized inequality to bound the corresponding terms, i.e., the followings hold with probability at least \(1-2\delta\),

\[\|\widehat{\bm{w}}_{t,a}-\bm{w}_{t,a}^{*}\|_{\bm{A}_{t,a}} \leq\sqrt{2(L_{\epsilon}^{2}+R_{\eta}^{2})\log\left(\frac{\det(\bm {A}_{t,a})^{1/2}\det(\lambda\bm{I})^{-1/2}}{\delta/2}\right)}+\frac{L_{u}}{ \sqrt{\lambda}}\left(\sum_{s=1}^{t}e_{s}^{\delta/t}\right)+2\sqrt{\lambda}\] \[\leq\sqrt{2(L_{\epsilon}^{2}+R_{\eta}^{2})\log\left(\frac{1+n_{t}( a)L_{u}^{2}/\lambda}{\delta/2}\right)}+\frac{L_{u}}{\sqrt{\lambda}}\left(\sum_{s=1}^{ t}e_{s}^{\delta/t}\right)+2\sqrt{\lambda}\]Therefore, the confidence set is

\[\mathcal{C}_{t,a}(\widehat{\bm{w}}_{t,a})=\left\{\bm{w}\in\mathbb{R}^{d_{u}}:\| \bm{w}-\widehat{\bm{w}}_{t,a}\|_{\bm{A}_{t,a}}\leq\zeta_{t,a}\right\},\] (13)

where \(\zeta_{t,a}=\sqrt{2(L_{\epsilon}^{2}+R_{\eta}^{2})\log\left((1+n_{t}(a)L_{u}^{ 2}/\lambda)/(\delta/2)\right)}+L_{u}\left(\sum_{s=1}^{t}e_{s}^{\delta/t} \right)/\sqrt{\lambda}+2\sqrt{\lambda}\). In comparison to the original confidence set, there is one additional term due to the generalization error introduced from \(\widehat{\phi}_{s}(\cdot)\). In the next section, we will provide a regret analysis, which following from the proof of LinUCB (Li et al., 2010). We simply have the following

\[R_{T}=\sum_{t=1}^{T}\left(r_{t,a_{t}^{*}}-r_{t,a_{t}}\right)= \sum_{t=1}^{T}\Delta_{t}\leq\sqrt{T\sum_{t=1}^{T}\Delta_{t}^{2}}\] \[\leq\sqrt{T\sum_{t=1}^{T}\left(\left\|\widetilde{\phi}_{t}(\bm{x }_{t})-\phi^{*}(\bm{x}_{t})\right\|\left\|\widetilde{\bm{\beta}}_{a_{t}} \right\|+\left\|\begin{bmatrix}\bm{x}_{t}\\ \phi^{*}(\bm{x}_{t})\end{bmatrix}\right\|_{\bm{A}_{t-1,a_{t}}^{-1}}\left\| \begin{bmatrix}\widetilde{\bm{\theta}}_{a_{t}}-\bm{\theta}_{a_{t}}\\ \widetilde{\bm{\beta}}_{a_{t}}-\bm{\beta}_{a_{t}}\end{bmatrix}\right\|_{\bm{A} _{t-1,a_{t}}}\right)^{2}}\] \[\leq\sqrt{T\left(\sum_{t=1}^{T}2\left\|\widetilde{\phi}_{t}(\bm{x }_{t})-\phi^{*}(\bm{x}_{t})\right\|^{2}\left\|\widetilde{\bm{\beta}}_{a_{t}} \right\|^{2}+2\zeta_{T,a}^{2}\left(1\wedge\left\|\begin{bmatrix}\bm{x}_{t}\\ \phi^{*}(\bm{x}_{t})\end{bmatrix}\right\|_{\bm{A}_{t-1,a_{t}}^{-1}}^{2}\right)\right)}\] \[\leq\sqrt{T\left(\sum_{t=1}^{T}2\left\|\widetilde{\phi}_{t}(\bm{x }_{t})-\phi^{*}(\bm{x}_{t})\right\|^{2}\left\|\widetilde{\bm{\beta}}_{a_{t}} \right\|^{2}+2\zeta_{T,a}^{2}\left(1\wedge\left\|\begin{bmatrix}\bm{x}_{t}\\ \phi^{*}(\bm{x}_{t})\end{bmatrix}\right\|_{\bm{A}_{t-1,a_{t}}^{-1}}^{2}\right)\right)}\] \[\leq\sqrt{T\cdot\left(8C_{0}T^{1-2\alpha}\log^{2\alpha}\left( \frac{\det\bm{X}_{t}}{\det\bm{X}_{0}}\right)\log^{2}\left(\frac{T}{\delta} \right)+2K\zeta_{T}^{2}d_{u}\log\left(1+\frac{TL_{u}^{2}}{\lambda d_{u}} \right)\right)}\]

In the next, we expand the term \(\zeta_{T}^{2}\),

\[\zeta_{T}^{2} \leq\left(\sqrt{2(L_{\epsilon}^{2}+R_{\eta}^{2})\log\left(\frac{ 1+TL_{u}^{2}/\lambda}{\delta/2}\right)}+\frac{L_{u}\left(\sum_{s=1}^{T}e_{s}^{ \delta/T}\right)}{\sqrt{\lambda}}+2\sqrt{\lambda}\right)^{2}\] \[\leq 6(L_{\epsilon}^{2}+R_{\eta}^{2})\log\left(\frac{1+TL_{u}^{2}/ \lambda}{\delta/2}\right)+\frac{3L_{u}^{2}}{\lambda}\left(\sum_{s=1}^{T}e_{s} ^{\delta/T}\right)^{2}+12\lambda\]In the next, we bound the second term in the above equation under the learnability assumption 1,

\[\left(\sum_{s=1}^{T}e_{s}^{5/T}\right)^{2}\leq 16T^{2-2\alpha}\log^{2\alpha} \left(\frac{\det\bm{X}_{T}}{\det\bm{X}_{0}}\right)\log^{2}\left(\frac{T}{\delta }\right).\]

Therefore, naively choosing the value of \(\lambda\) will lead to a linear regret due to the term \(T^{3/2-\alpha}\) in the equation. To minimize the upper bound, we can choose the value of \(\lambda\) to be

\[\lambda=2L_{u}T^{1-\alpha}\log^{\alpha}\left(\frac{\det\bm{X}_{T}}{\det\bm{X}_ {0}}\right)\log\left(\frac{T}{\delta}\right).\]

Then, we can bound \(\zeta_{T}^{2}\) by

\[\zeta_{T}^{2}\leq 6(L_{\epsilon}^{2}+R_{\eta}^{2})\log\left(\frac{1+TL_{u} ^{2}/\lambda}{\delta/2}\right)+48L_{u}T^{1-\alpha}\log^{\alpha}\left(\frac{ \det\bm{X}_{T}}{\det\bm{X}_{0}}\right)\log\left(\frac{T}{\delta}\right).\]

By plugging it in and following the simplication as in the proof of Theorem 1, we can get the regret is upper bounded by

\[\widetilde{\mathcal{O}}\left(T^{1-\alpha}d_{u}^{\alpha}+T^{1- \alpha/2}\sqrt{Kd_{u}^{1+\alpha}}\right).\]

The above result is summarized as the following proposition.

**Proposition 1** (Regret of LinUCB-\((\widehat{\phi})\)).: _The regret of LinUCB-\((\widehat{\phi})\) in Algorithm 2 is upper bounded by \(\widetilde{\mathcal{O}}\left(T^{1-\alpha}d_{u}^{\alpha}+T^{1-\alpha/2}\sqrt{ Kd_{u}^{1+\alpha}}\right)\) with probability at least \(1-\delta\), by carefully setting the regularization coefficient \(\lambda=\Theta(L_{u}d_{u}^{\alpha}T^{1-\alpha}\log\left(T/\delta\right))\) in Equation 1._

## Appendix B Missing Proofs

### Missing Proofs in the Generalized Elliptical Potential Lemma

**Lemma 1** (Generalized Elliptical Potential Lemma).: _Suppose (1) \(\bm{X}_{0}\in\mathbb{R}^{d\times d}\) is any positive definite matrix; (2) \(\bm{x}_{1},\ldots,\bm{x}_{T}\in\mathbb{R}^{d}\) is a sequence of vectors with bounded \(l_{2}\) norm \(\max_{t}\|\bm{x}_{t}\|\leq L_{x}\); (3) \(\bm{\epsilon}_{1},\ldots,\bm{\epsilon}_{T}\in\mathbb{R}^{d}\) is a sequence of independent (not necessarily identical) bounded zero-mean noises satisfying \(\max_{t}\|\bm{\epsilon}_{t}\|\leq L_{\epsilon}\) and \(\mathbb{E}[\bm{\epsilon}_{t}\bm{\epsilon}_{t}^{\top}]\succcurlyeq\sigma_{ \epsilon}^{2}\bm{I}\) for any \(t\); and (4) \(\widetilde{\bm{X}}_{t}\) is defined as follows:_

\[\widetilde{\bm{X}}_{t}=\bm{X}_{0}+\sum_{s=1}^{t}(\bm{x}_{s}+ \bm{\epsilon}_{s})(\bm{x}_{s}+\bm{\epsilon}_{s})^{\top}\in\mathbb{R}^{d\times d}.\]

_Then, for any \(p\in[0,1]\), the following inequality holds with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}\left(1\wedge\|\bm{x}_{t}\|_{\widetilde{\bm{X}}_{t -1}^{2}}^{2}\right)^{p}\leq 2^{p}T^{1-p}\log^{p}\left(\frac{\det\bm{X}_{T}}{ \det\bm{X}_{0}}\right)+\frac{8L_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2}}{\sigma _{\epsilon}^{4}}\log\left(\frac{32dL_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2}}{ \delta\sigma_{\epsilon}^{4}}\right)\] (2)

Proof.: Our proof follows the high level idea for proving the original EPL. However, to accommodate the noises in the data matrix, we have to introduce new matrix concentration tools to the original (primarily algebraic) proof, and also identify the right conditions for the argument to go through. A key lemma to our proof is the following high probability bound regarding the noisy data matrix:

**Lemma 2**.: _Let \(\bm{x}_{1},...,\bm{x}_{T}\in\mathbb{R}^{d}\) be a fixed sequence of vectors, and \(\bm{\epsilon}_{1},...,\bm{\epsilon}_{T}\in\mathbb{R}^{d}\) are independent random variables satisfying \(\max_{t}\|\bm{x}_{t}\|_{2}\leq L_{x}\), \(\max_{t}\|\bm{\epsilon}_{t}\|_{2}\leq L_{\epsilon}\), and \(\mathbb{E}[\bm{\epsilon}_{t}\bm{\epsilon}_{t}^{\top}]\succcurlyeq\sigma_{ \epsilon}^{2}\bm{I}\). Then the following hold with probability at least \(1-2d\exp\left(\frac{-T\sigma_{\epsilon}^{4}}{8L_{\epsilon}^{2}(L_{\epsilon}+L_ {x})^{2}}\right)\),_

\[\sum_{t=1}^{T}(\bm{x}_{t}+\bm{\epsilon}_{t})(\bm{x}_{t}+\bm{ \epsilon}_{t})^{\top}\succcurlyeq\sum_{t=1}^{T}\bm{x}_{t}\bm{x}_{t}^{\top}.\]The Proof of Lemma 2 employs the Bernstein's Inequality for matrices (Tropp et al., 2015), which is technical; for ease of presentation, we defer its proof of Appendix B.1. By Lemma 2, we have that, for every \(t\in[T]\), the following inequality holds with probability at least \(1-2d\exp\left(\frac{-t\sigma_{\epsilon}^{4}}{8L_{\epsilon}^{2}(L_{\epsilon}+L_ {x})^{2}}\right)\):

\[\widetilde{\bm{X}}_{t}=\bm{X}_{0}+\sum_{s=1}^{t}(\bm{x}_{s}+\bm{ \epsilon}_{s})(\bm{x}_{s}+\bm{\epsilon}_{s})^{\top}\succcurlyeq\bm{X}_{0}+ \sum_{s=1}^{t}\bm{x}_{s}\bm{x}_{s}^{\top}\coloneqq\bm{X}_{t},\]

under which we have

\[\|\bm{x}_{t+1}\|_{\widetilde{\bm{X}}_{t}^{-1}}^{2}\leq\|\bm{x}_{t+1}\|_{\bm{X }_{t}^{-1}}^{2}.\]

To prove our Lemma 1, we need to apply union bound to guarantee the above hold simultaneously for every \(t\geq 1\) with high probability. Unfortunately, this turns out to not be true because when \(t\) is very small (e.g., \(t=1\)), the above inequality cannot hold with high probability. Therefore, to obtain high-probability guarantee by the union bound, we will have to exclude these small \(t\)'s and apply the union bound for only the events from some \(t^{\prime}\in[T]\), as follows

\[\mathbb{P}\left[\forall t\in[t^{\prime},T],\ \ \|\bm{x}_{t}\|_{ \widetilde{\bm{X}}_{t-1}^{-1}}^{2}\leq\|\bm{x}_{t}\|_{\bm{X}_{t-1}^{-1}}^{2}\right]\] \[\geq 1-\sum_{t=t^{\prime}-1}^{T-1}2d\exp\left(\frac{-t\sigma_{ \epsilon}^{4}}{(8L_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2})}\right)\] \[\geq 1-\sum_{t=t^{\prime}-1}^{\infty}2d\exp\left(\frac{-t\sigma_{ \epsilon}^{4}}{(8L_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2})}\right)\] \[=1-2d\left(\frac{\exp\big{(}-(t^{\prime}-1)\sigma_{\epsilon}^{4}/ (8L_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2})\big{)}}{1-\exp\big{(}-\sigma_{ \epsilon}^{4}/(8L_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2})\big{)}}\right).\] \[\geq 1-2d\times\exp\big{(}-(t^{\prime}-1)\sigma_{\epsilon}^{4}/(8 L_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2})\big{)}\times\frac{16L_{\epsilon}^{2}(L_{ \epsilon}+L_{x})^{2}}{\sigma_{\epsilon}^{4}},\]

where the last inequality uses the fact that \(\sigma_{\epsilon}^{4}/(L_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2})\leq(\sigma_ {\epsilon}/L_{\epsilon})^{4}\leq 1\) and \(1-e^{-x}\geq x/2\) for any \(x\in[0,1]\). By solving the following inequality,

\[\exp\big{(}-(t^{\prime}-1)\sigma_{\epsilon}^{4}/(8L_{\epsilon}^{2}(L_{ \epsilon}+L_{x})^{2})\big{)}\times\frac{16L_{\epsilon}^{2}(L_{\epsilon}+L_{x} )^{2}}{\sigma_{\epsilon}^{4}}\leq\frac{\delta}{2d},\]

we have,

\[t^{\prime}\geq 1+\frac{8L_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2}}{ \sigma_{\epsilon}^{4}}\log\left(\frac{32dL_{\epsilon}^{2}(L_{\epsilon}+L_{x}) ^{2}}{\delta\sigma_{\epsilon}^{4}}\right)\]

Let \(T_{0}\) denote the ceiling of the right-hand-side of the above term. Therefore, we have the following hold with high probability at least \(1-\delta\):

\[\sum_{t=1}^{T}\left(1\wedge\|\bm{x}_{t}\|_{\widetilde{\bm{X}}_{t -1}^{-1}}^{2}\right)^{p} \leq(T_{0}-1)+\sum_{t=T_{0}}^{T}\left(1\wedge\|\bm{x}_{t}\|_{ \widetilde{\bm{X}}_{t-1}^{-1}}^{2}\right)^{p}\] \[\leq(T_{0}-1)+\sum_{t=T_{0}}^{T}\left(1\wedge\|\bm{x}_{t}\|_{\bm{X }_{t-1}^{-1}}^{2}\right)^{p}\] \[\leq(T_{0}-1)+\sum_{t=T_{0}}^{T}\left(1\wedge\|\bm{x}_{t}\|_{\bm{X }_{t-1}^{-1}}^{2}\right)^{p}\]

In the next, we are going to bound the second term in the above equation, whose proof can be adapted from the proof of the original elliptical potential lemma. Using the fact that for any \(z\in[0,+\infty]\), \(z\wedge 1\leq 2\ln(1+z)\), we get

\[\sum_{t=1}^{T}1\wedge\left(\|\bm{x}_{t}\|_{\bm{X}_{t-1}^{-1}}^{2}\right)^{p} \leq\sum_{t=1}^{T}\left(2\log\left(1+\|\bm{x}_{t}\|_{\bm{X}_{t-1}^{-1}}^{2} \right)\right)^{p}.\]Additionally, by definition, we have

\[\bm{X}_{t}=\bm{X}_{t-1}+\bm{x}_{t}\bm{x}_{t}^{\top}=\bm{X}_{t-1}^{1/2}\left(\bm{I} +\bm{X}_{t-1}^{-1/2}\bm{x}_{t}\bm{x}_{t}^{\top}\bm{X}_{t-1}^{-1/2}\right)\bm{X}_ {t-1}^{1/2}.\]

This implies the following relationship between the determinant,

\[\det\bm{X}_{t}=\det\left(\bm{X}_{t-1}\right)\det\left(\bm{I}+\bm{X}_{t-1}^{-1/ 2}\bm{x}_{t}\bm{x}_{t}^{\top}\bm{X}_{t-1}^{-1/2}\right).\]

Since the only eigenvalues of a matrix of the form \(\bm{I}+\bm{y}\bm{y}^{\top}\) are \(1+\|\bm{y}\|_{2}\) and \(1\), we have

\[\log\left(1+\|\bm{x}_{t}\|_{\bm{X}_{t-1}^{-1}}^{2}\right)=\log\det\bm{X}_{t}- \log\det\bm{X}_{t-1}.\]

By taking the power \(p\) for both sides and taking the sum, we have

\[\sum_{t=1}^{T}\left(\log\left(1+\|\bm{x}_{t}\|_{\bm{X}_{t-1}^{-1}}^{2}\right) \right)^{p}=\sum_{t=1}^{T}\left(\log\det\bm{X}_{t}-\log\det\bm{X}_{t-1}\right) ^{p}.\]

Since \(p\in[0,1]\), the function \(g(x)=x^{p}\) is a concave function. Thus, we have

\[\frac{1}{T}\sum_{t=1}^{T}\left(\log\det\bm{X}_{t}-\log\det\bm{X}_{t-1}\right)^ {p}\leq\left(\frac{1}{T}\sum_{t=1}^{T}\log\det\bm{X}_{t}-\log\det\bm{X}_{t-1} \right)^{p}=\frac{1}{Tp}\log^{p}\left(\frac{\det\bm{X}_{T}}{\det\bm{X}_{0}} \right).\]

Therefore, we can conclude that

\[\sum_{t=1}^{T}1\wedge\left(\|\bm{x}_{t}\|_{\bm{X}_{t-1}^{-1}}^{2}\right)^{p} \leq 2^{p}T^{1-p}\log^{p}\left(\frac{\det\bm{X}_{T}}{\det\bm{X}_{0}} \right).\]

By combining the above results, we have the following hold with probability at least \(1-\delta\):

\[\sum_{t=1}^{T}\left(1\wedge\|\bm{x}_{t}\|_{\bm{X}_{t-1}^{-1}}^{2}\right)^{p} \leq T_{0}-1+\left(\sum_{t=1}^{T}1\wedge\|\bm{x}_{t}\|_{\bm{X}_{t-1}^{-1}}^{2} \right)^{p}\leq T_{0}-1+2^{p}T^{1-p}\log^{p}\left(\frac{\det\bm{X}_{T}}{\det \bm{X}_{0}}\right).\]

Invoking

\[T_{0}-1\leq\frac{8L_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2}}{\sigma_{\epsilon}^ {4}}\log\left(\frac{32dL_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2}}{\delta\sigma _{\epsilon}^{4}}\right),\]

we obtained the desired inequality with probability at least \(1-\delta\):

\[\sum_{t=1}^{T}\left(1\wedge\|\bm{x}_{t}\|_{\bm{X}_{t-1}^{-1}}^{2}\right)^{p} \leq 2^{p}T^{1-p}\log^{p}\left(\frac{\det\bm{X}_{T}}{\det\bm{X}_{0}} \right)+\frac{8L_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2}}{\sigma_{\epsilon}^{4 }}\log\left(\frac{32dL_{\epsilon}^{2}(L_{\epsilon}+L_{x})^{2}}{\delta\sigma_{ \epsilon}^{4}}\right).\]

**Lemma 2**.: _Let \(\bm{x}_{1},...,\bm{x}_{T}\in\mathbb{R}^{d}\) be a fixed sequence of vectors, and \(\bm{\epsilon}_{1},...,\bm{\epsilon}_{T}\in\mathbb{R}^{d}\) are independent random variables satisfying \(\max_{t}\|\bm{x}_{t}\|_{2}\leq L_{x}\), \(\max_{t}\|\bm{\epsilon}_{t}\|_{2}\leq L_{\epsilon}\), and \(\mathbb{E}[\bm{\epsilon}_{t}\bm{\epsilon}_{t}^{\top}]\succcurlyeq\sigma_{ \epsilon}^{2}\bm{I}\). Then the following hold with probability at least \(1-2d\exp\left(\frac{-T\sigma_{\epsilon}^{4}}{8L_{\epsilon}^{2}(L_{\epsilon}+L _{x})^{2}}\right)\),_

\[\sum_{t=1}^{T}(\bm{x}_{t}+\bm{\epsilon}_{t})(\bm{x}_{t}+\bm{\epsilon}_{t})^{ \top}\succcurlyeq\sum_{t=1}^{T}\bm{x}_{t}\bm{x}_{t}^{\top}.\]

Proof.: We will analyze the term-wise difference, denoted as

\[\bm{S}_{t}\coloneqq(\bm{x}_{t}+\bm{\epsilon}_{t})(\bm{x}_{t}+\bm{\epsilon}_{t} )^{\top}-\bm{x}_{t}\bm{x}_{t}^{\top}=\bm{\epsilon}_{t}\bm{x}_{t}^{\top}+\bm{x} _{t}\bm{\epsilon}_{t}^{\top}+\bm{\epsilon}_{t}\bm{\epsilon}_{t}^{\top}\] (14)

Moreover, since \(\mathbb{E}[\bm{\epsilon}_{t}]=0\) for any \(t\in[T]\), the expectation of \(\bm{S}_{t}\) (over randomness of noise) can be lower bounded as

\[\mathbb{E}[\bm{S}_{t}]=\mathbb{E}[\bm{\epsilon}_{t}]\bm{x}_{t}^{\top}+\bm{x}_{t }\mathbb{E}[\bm{\epsilon}_{t}^{\top}]+\mathbb{E}[\bm{\epsilon}_{t}\bm{ \epsilon}_{t}^{\top}]=\mathbb{E}[\bm{\epsilon}_{t}\bm{\epsilon}_{t}^{\top}] \succcurlyeq\sigma_{\epsilon}^{2}\bm{I}.\]Since \(\|\bm{x}_{t}\|_{2}\leq L_{x}\) and \(\|\bm{\epsilon}_{t}\|_{2}\leq L_{\epsilon}\), we know that \(\bm{S}_{t}\leq(2L_{\epsilon}L_{x}+L_{\epsilon}^{2})\bm{I}\) with probability \(1\). Thus \(\bm{S}_{t}\) is uniformly upper bounded under the spectral-norm denoted by \(\|\cdot\|\), or formally

\[\|\bm{S}_{t}\|\leq 2L_{\epsilon}L_{x}+L_{\epsilon}^{2}.\]

Consider the "centered" matrix sum \(\bm{Z}_{T}=\sum_{t=1}^{T}\left[\bm{S}_{t}-\mathbb{E}[\bm{\epsilon}_{t}\bm{ \epsilon}_{t}^{\top}]\right]\), with mean \(0\). Since the spectral-norm of \(\bm{S}_{t}\) is upper bounded by \(2L_{\epsilon}L_{x}+L_{\epsilon}^{2}\), its variance \(\mathbb{V}(\bm{S}_{t})=\left\|\mathbb{E}\left[\bm{S}_{t}\bm{S}_{t}^{\top} \right]\right\|_{2}\) is upper bounded by \((2L_{\epsilon}L_{x}+L_{\epsilon}^{2})^{2}\). Thus, the variance of \(\bm{Z}_{T}\) equals the variance of sum \(\sum_{t=1}^{T}\bm{S}_{t}\), which is then upper bounded by \(T(2L_{\epsilon}L_{x}+L_{\epsilon}^{2})^{2}\). By the Bernstein's Inequality for random matrices (Tropp et al., 2015), we have the following high probability upper bound for the spectral norm \(\|\cdot\|\) of \(\bm{Z}_{T}\):

\[\mathbb{P}\left[\|\bm{Z}_{T}\|\geq\iota\right]\leq 2d\exp\left(\frac{-\iota^{2} /2}{\mathbb{V}(\bm{Z}_{T})+(2L_{\epsilon}L_{x}+L_{\epsilon}^{2})\iota/3} \right).\] (15)

Since \(\bm{Z}_{T}\) is a symmetric matrix, its spectral norm upper bounds the absolute value of any eigenvalue. Thus we can lower bound the smallest eigenvalues as follows:

\[\mathbb{P}\left[\|\bm{Z}_{T}\|\geq\iota\right] \geq\mathbb{P}\left[\lambda_{\min}\left(\sum_{t=1}^{T}\bm{S}_{t}- \mathbb{E}[\bm{\epsilon}_{t}\bm{\epsilon}_{t}^{\top}]\right)\leq-\iota\right]\] \[\geq\mathbb{P}\left[\lambda_{\min}\left(\sum_{t=1}^{T}\bm{S}_{t}- \sigma_{\epsilon}^{2}\bm{I}\right)\leq-\iota\right]\] \[=\mathbb{P}\left[\lambda_{\min}\left(\sum_{t=1}^{T}\bm{S}_{t} \right)\leq T\sigma_{\epsilon}^{2}-\iota\right].\]

where the second inequality is due to two facts: (1) \(A\succcurlyeq B\) implies \(\lambda_{\min}(A)\geq\lambda_{\min}(B)\) where \(A=\left(\sum_{t=1}^{T}\bm{S}_{t}-\sigma_{\epsilon}^{2}\bm{I}\right)\) and \(B=\left(\sum_{t=1}^{T}\bm{S}_{t}-\mathbb{E}[\bm{\epsilon}_{t}\bm{\epsilon}_{t} ^{\top}]\right)\); and (2) the event \(\lambda_{\min}(A)\leq-\iota\) thus is included within the event \(\lambda_{\min}(B)\leq-\iota\). Consequently, we have

\[\mathbb{P}\left[\lambda_{\min}\left(\sum_{t=1}^{T}\bm{S}_{t}\right)\leq T \sigma_{\epsilon}^{2}-\iota\right]\leq 2d\exp\left(\frac{-\iota^{2}/2}{\mathbb{V}( \bm{Z}_{T})+(2L_{\epsilon}L_{x}+L_{\epsilon}^{2})\iota/3}\right),\]

or equivalently,

\[\mathbb{P}\left[\lambda_{\min}\left(\sum_{t=1}^{T}\bm{S}_{t}\right)\geq T\sigma _{\epsilon}^{2}-\iota\right]\geq 1-2d\exp\left(\frac{-\iota^{2}/2}{ \mathbb{V}(\bm{Z}_{T})+(2L_{\epsilon}L_{x}+L_{\epsilon}^{2})\iota/3}\right),\]

By choosing the value of \(\iota=T\sigma_{\epsilon}^{2}\), we get

\[\mathbb{P}\left[\lambda_{\min}\left(\sum_{t=1}^{T}\bm{S}_{t}\right) \geq 0\right]\] \[\geq 1-2d\exp\left(\frac{-T^{2}\sigma_{\epsilon}^{4}/2}{\mathbb{V }(\bm{Z}_{T})+(2L_{\epsilon}L_{x}+L_{\epsilon}^{2})T\sigma_{\epsilon}^{2}/3}\right)\] \[\geq 1-2d\exp\left(\frac{-T^{2}\sigma_{\epsilon}^{4}/2}{T(2L_{ \epsilon}L_{x}+L_{\epsilon}^{2})^{2}+(2L_{\epsilon}L_{x}+L_{\epsilon}^{2})T \sigma_{\epsilon}^{2}/3}\right).\]

Using the fact that \(\sigma_{\epsilon}\leq L_{\epsilon}\), we can further simplify the above equation by

\[\mathbb{P}\left[\lambda_{\min}\left(\sum_{t=1}^{T}\bm{S}_{t}\right)\geq 0 \right]\geq 1-2d\exp\left(\frac{-T\sigma_{\epsilon}^{4}}{8L_{\epsilon}^{2}(L _{\epsilon}+L_{x})^{2}}\right).\]

### Missing Proofs in the Regret Analysis

**Theorem 1** (Regret of poLinUCB).: _The regret of poLinUCB in Algorithm 1 is upper bounded by \(\widetilde{\mathcal{O}}\left(T^{1-\alpha}d_{u}^{\alpha}+d_{u}\sqrt{TK}\right)\) with probability at least \(1-\delta\), if \(T=\Omega(\log(1/\delta))\)._

Proof.: In the next, we prove the regret bound. For each time step \(t\), the immediate regret is

\[\Delta_{t} =r_{t,a_{t}^{*}}-r_{t,a_{t}}\] \[=\left\langle\begin{bmatrix}\bm{x}_{t}\\ \phi^{*}(\bm{x}_{t})\end{bmatrix},\begin{bmatrix}\bm{\theta}_{a_{t}^{*}}-\bm {\theta}_{a_{t}}^{*}\\ \bm{\beta}_{a_{t}^{*}}-\bm{\beta}_{a_{t}}^{*}\end{bmatrix}\right\rangle\] \[\overset{\mathrm{(a)}}{\leq}\left\langle\begin{bmatrix}\bm{x}_{t} \\ \widetilde{\phi}_{t}(\bm{x}_{t})\end{bmatrix},\begin{bmatrix}\widetilde{\bm{ \theta}}_{a_{t}}-\bm{\theta}_{a_{t}}^{*}\\ \widetilde{\bm{\beta}}_{a_{t}}-\bm{\beta}_{a_{t}}^{*}\end{bmatrix}\right\rangle+ \left\langle\widetilde{\phi}_{t}(\bm{x}_{t})-\phi^{*}(\bm{x}_{t}),\bm{\beta}_ {a_{t}}^{*}\right\rangle\] \[=\left\langle\begin{bmatrix}\bm{0}\\ \widetilde{\phi}_{t}(\bm{x}_{t})-\phi^{*}(\bm{x}_{t})\end{bmatrix}+\begin{bmatrix} \bm{x}_{t}\\ \phi^{*}(\bm{x}_{t})\end{bmatrix},\begin{bmatrix}\widetilde{\bm{\theta}}_{a_{t} }-\bm{\theta}_{a_{t}}^{*}\\ \widetilde{\bm{\beta}}_{a_{t}}-\bm{\beta}_{a_{t}}^{*}\end{bmatrix}\right\rangle +\left\langle\widetilde{\phi}_{t}(\bm{x}_{t})-\phi^{*}(\bm{x}_{t}),\bm{\beta} _{a_{t}}^{*}\right\rangle\] \[=\left\langle\widetilde{\phi}_{t}(\bm{x}_{t})-\phi^{*}(\bm{x}_{t}),\widetilde{\bm{\beta}}_{a_{t}}\right\rangle+\left\langle\begin{bmatrix}\bm{ x}_{t}\\ \phi^{*}(\bm{x}_{t})\end{bmatrix},\begin{bmatrix}\widetilde{\bm{\theta}}_{a_{t}}-\bm {\theta}_{a_{t}}^{*}\\ \widetilde{\bm{\beta}}_{a_{t}}-\bm{\beta}_{a_{t}}^{*}\end{bmatrix}\right\rangle\] \[\overset{\mathrm{(c)}}{\leq}\left\|\widetilde{\phi}_{t}(\bm{x}_{t })-\phi^{*}(\bm{x}_{t})\right\|\cdot\left\|\widetilde{\bm{\beta}}_{a_{t}} \right\|+\left\|\begin{bmatrix}\bm{x}_{t}\\ \phi^{*}(\bm{x}_{t})\end{bmatrix}\right\|_{\bm{A}_{t-1,a_{t}}^{-1}}\left\| \begin{bmatrix}\widetilde{\bm{\theta}}_{a_{t}}-\bm{\theta}_{a_{t}}^{*}\\ \widetilde{\bm{\beta}}_{a_{t}}-\bm{\beta}_{a_{t}}^{*}\end{bmatrix}\right\|_{ \bm{A}_{t-1,a_{t}}}.\]

where the inequality (a) is due to the definition of UCB, and (b) and (c) are obtained using the Cauchy-Schwarz inequality. Therefore, the cumulative regret can be further upper bounded by

\[R_{T}=\sum_{t=1}^{T}\Delta_{t}\leq\sqrt{T\sum_{t=1}^{T}\Delta_{ t}^{2}}\] \[\leq\sqrt{T\sum_{t=1}^{T}\left(\left\|\widetilde{\phi}_{t}(\bm{x} _{t})-\phi^{*}(\bm{x}_{t})\right\|\left\|\widetilde{\bm{\beta}}_{a_{t}}\right\| +\left\|\begin{bmatrix}\bm{x}_{t}\\ \phi^{*}(\bm{x}_{t})\end{bmatrix}\right\|_{\bm{A}_{t-1,a_{t}}^{-1}}\left\| \begin{bmatrix}\widetilde{\bm{\theta}}_{a_{t}}-\bm{\theta}_{a_{t}}\\ \widetilde{\bm{\beta}}_{a_{t}}-\bm{\beta}_{a_{t}}\end{bmatrix}\right\|_{\bm{A} _{t-1,a_{t}}}\right)^{2}}\] \[\leq\sqrt{T\left(\sum_{t=1}^{T}2\left\|\widetilde{\phi}_{t}(\bm{x }_{t})-\phi^{*}(\bm{x}_{t})\right\|^{2}\left\|\widetilde{\bm{\beta}}_{a_{t}} \right\|^{2}+2\zeta_{T}^{2}\left(1\wedge\left\|\begin{bmatrix}\bm{x}_{t}\\ \phi^{*}(\bm{x}_{t})\end{bmatrix}\right\|_{\bm{A}_{t-1,a_{t}}^{-1}}^{2}\right)\right)}\] \[\leq\sqrt{T\left(\sum_{t=1}^{T}2\left\|\widetilde{\phi}_{t}(\bm{x }_{t})-\phi^{*}(\bm{x}_{t})\right\|^{2}\left\|\widetilde{\bm{\beta}}_{a_{t}} \right\|^{2}+2\zeta_{T}^{2}\left(1\wedge\left\|\begin{bmatrix}\bm{x}_{t}\\ \phi^{*}(\bm{x}_{t})\end{bmatrix}\right\|_{\bm{A}_{t-1,a_{t}}^{-1}}^{2}\right)\right)}\]

In the next, we bound each term separately. Firstly, we have the following hold with probability at least \(1-\delta\) by using the union bound,

\[\sum_{t=1}^{T}2\left\|\widetilde{\phi}_{t}(\bm{x}_{t})-\phi^{*}(\bm{x}_{t}) \right\|^{2}\cdot\left\|\widetilde{\bm{\beta}}_{a_{t}}\right\|^{2}\leq 8\sum_{t=1}^{T} \left(e_{t}^{\delta/T}\right)^{2}.\]

In the next, to bound the remaining term, we use the result from Lemma 1. We first group the sums based the arm,

\[\sum_{t=1}^{T}1\wedge\left\|\begin{bmatrix}\bm{x}_{t}\\ \phi^{*}(\bm{x}_{t})\end{bmatrix}\right\|_{\bm{A}_{t-1,a_{t}}^{-1}}^{2}=\sum_{a \in\mathcal{A}}\sum_{t\in[T]:a_{t}=a}1\wedge\left\|\begin{bmatrix}\bm{x}_{t}\\ \phi^{*}(\bm{x}_{t})\end{bmatrix}\right\|_{\bm{A}_{t-1,a}^{-1}}^{2}\] (16)

By denoting \(n_{T}(a)\) as the number of times that arm \(a\) is pulled, we can divide the arms into two groups,

\[\mathcal{G}_{0}\coloneqq\{a\in\mathcal{A}:n_{T}(a)=\Omega(\log(1/\delta))\}\quad \text{and}\quad\mathcal{G}_{1}\coloneqq\mathcal{A}\setminus\mathcal{G}_{0}.\]Then, we can further decompose the r.h.s term of Equation 16 based on if the corresponding arm is in \(\mathcal{G}_{0}\) or \(\mathcal{G}_{1}\). Then, by applying Lemma 1, we have the following holds, with probability at least \(1-\delta\),

\[\sum_{a\in\mathcal{A}}\sum_{t\in[T]:a_{t}=a}1\wedge\left\|\left[ \begin{matrix}\bm{x}_{t}\\ \phi^{\star}(\bm{x}_{t})\end{matrix}\right]\right\|_{\bm{A}_{t-1,a}^{-1}}^{2}\] \[=\sum_{a\in\mathcal{G}_{0}}\sum_{t\in[T]:a_{t}=a}1\wedge\left\| \left[\begin{matrix}\bm{x}_{t}\\ \phi^{\star}(\bm{x}_{t})\end{matrix}\right]\right\|_{\bm{A}_{t-1,a}^{-1}}^{2} +\sum_{a\in\mathcal{G}_{1}}\sum_{t\in[T]:a_{t}=a}1\wedge\left\|\left[ \begin{matrix}\bm{x}_{t}\\ \phi^{\star}(\bm{x}_{t})\end{matrix}\right]\right\|_{\bm{A}_{t-1,a}^{-1}}^{2}\] \[\leq 2Kd_{u}\log\left(1+\frac{TL_{u}^{2}}{\lambda d_{u}}\right)+ \frac{8KL_{\epsilon}^{2}(L_{\epsilon}+L_{\epsilon})^{2}}{\sigma_{\epsilon}^{4 }}\log\left(\frac{32Kd_{u}L_{\epsilon}^{2}(L_{\epsilon}+L_{\epsilon})^{2}}{ \delta\sigma_{\epsilon}^{4}}\right)\!.\]

where the last inequality is due to Lemma 1 and apply the union bound on the \(K\) arms. To bound the remainder term, since \(\alpha\in[0,1/2]\), by the learnability assumption as stated in Assumption 1 and Lemma 1, we have

\[\sum_{t=1}^{T}\left(e_{t}^{\delta/t}\right)^{2} \leq\sum_{t=1}^{T}C_{0}^{2}\cdot\left(1\wedge\|\bm{x}\|_{\bm{X}_{ t-1}^{-1}}^{2}\right)^{2\alpha}\cdot\log^{2}\left(\frac{tT}{\delta}\right)\] \[\leq 4C_{0}^{2}T^{1-2\alpha}\log^{2\alpha}\left(\frac{\det\bm{X}_ {T}}{\det\bm{X}_{0}}\right)\log^{2}\left(\frac{T}{\delta}\right)\] \[\leq 4C_{0}^{2}T^{1-2\alpha}d_{u}^{2\alpha}\log^{2\alpha}\left( \frac{TL_{u}^{2}/d+\lambda}{\lambda}\right)\log^{2}\left(\frac{T}{\delta}\right)\]

Therefore, the total regret bound is bounded by the following term with probability at least \(1-2\delta\),

\[\sqrt{T\cdot\left(8C_{0}T^{1-2\alpha}\log^{2\alpha}\left(\frac{ \det\bm{X}_{T}}{\det\bm{X}_{0}}\right)\log^{2}\left(\frac{T}{\delta}\right)+K \zeta_{T}^{2}\left(d_{u}\log\left(1+\frac{TL_{u}^{2}}{\lambda d_{u}}\right)+ \frac{48L_{\epsilon}^{4}L_{\epsilon}^{2}}{\sigma_{\epsilon}^{4}}\log\left( \frac{192Kd_{u}L_{\epsilon}^{4}L_{\epsilon}^{2}}{\delta\sigma_{\epsilon}^{4}} \right)\right)\right)}\]

By hiding the logarithmic terms, we can further simplify it to be

\[\widetilde{\mathcal{O}}\left(T^{1-\alpha}d_{u}^{\alpha}+d_{u}\sqrt{TK}\right)\]

### Missing Proofs in Generalizations

**Proposition 2**.: _The regret of poLinUCB in Algorithm 1 for action-dependent contexts is upper bounded by \(\widetilde{\mathcal{O}}\left(T^{1-\alpha}d_{u}^{\alpha}\sqrt{K}+d_{u}\sqrt{ TK}\right)\) with probability at least \(1-\delta\) if \(T=\Omega(\log(1/\delta))\)._

Proof.: Our proof follows from the proof of Theorem 1. The immediate regret at each time step \(t\) is

\[\Delta_{t}=r_{t,a_{t}^{\star}}-r_{t,a_{t}}\] (17)

Recall that the definition of \(a_{t}^{\star}\),

\[a_{t}^{\star}\coloneqq\operatorname*{arg\,max}_{a\in\mathcal{A}}\langle\bm{ \theta}_{a}^{\star},\bm{x}_{t,a}\rangle+\langle\bm{\beta}_{a}^{\star},\phi_{a }^{\star}(\bm{x}_{t,a})\rangle.\] (18)

To bound the immediate regret, we have

\[\Delta_{t}=\langle\bm{\theta}_{a_{t}^{\star}}^{\star},\bm{x}_{t,a_{t}^{\star} }\rangle+\langle\bm{\beta}_{a_{t}^{\star}}^{\star},\phi_{a_{t}^{\star}}^{ \star}(\bm{x}_{t,a_{t}})\rangle-\langle\bm{\theta}_{a_{t}}^{\star},\bm{x}_{t, a_{t}}\rangle-\langle\bm{\beta}_{a_{t}}^{\star},\phi_{a_{t}}^{\star}(\bm{x}_{t,a_{t}})\rangle.\] (19)

By the definition of UCB, we further have

\[\Delta_{t}\leq\langle\widetilde{\bm{\theta}}_{t,a_{t}},\bm{x}_{t,a_{t}} \rangle+\langle\widetilde{\bm{\beta}}_{t,a_{t}},\widetilde{\phi}_{t,a_{t}}^{ \star}(\bm{x}_{t,a_{t}})\rangle-\langle\bm{\theta}_{a_{t}}^{\star},\bm{x}_{t, a_{t}}\rangle-\langle\bm{\beta}_{a_{t}}^{\star},\phi_{a_{t}}^{\star}(\bm{x}_{t,a_{t}})\rangle.\] (20)

By rearranging the terms, we get

\[\Delta_{t}\leq\underbrace{\langle\widetilde{\bm{\theta}}_{t,a_{t}}-\bm{ \theta}_{a_{t}}^{\star},\bm{x}_{t,a_{t}}\rangle+\langle\widetilde{\bm{\beta}}_{t,a_{t}}-\bm{\beta}_{a_{t}}^{\star},\phi_{a_{t}}^{\star}(\bm{x}_{t,a_{t}}) \rangle}_{\mbox{$\widetilde{1}$}}+\underbrace{\langle\widetilde{\bm{\beta}}_{t,a_{t}},\widetilde{\phi}_{t,a_{t}}(\bm{x}_{t,a_{t}})-\phi_{a_{t}}^{\star}(\bm{x}_ {t,a_{t}})\rangle}_{\mbox{$\widetilde{2}$}}\] (21)Bounding the first term 1 is the same as the proof in Theorem 1, while bounding the second term 2 will be slightly different, as we now have \(K\) such functions of \(\bm{\phi}_{a}^{\star}(\cdot)\) for \(a\in\mathcal{A}\) to learn. By denoting the error for each estimate of \(\phi_{a}^{\star}(\cdot)\) at iteration \(t\) as \(e_{t,a}^{\delta}\). Therefore, the contribution from the second term to the total regret can be bounded by

\[\sum_{t=1}^{T}\left(e_{t,a_{t}}^{\delta/t}\right)^{2} =\sum_{a\in\mathcal{A}}\sum_{t\in[T]:a_{t}=a}\left(e_{t,a}^{t/ \delta}\right)^{2}\] (22) \[\leq 4KC_{0}T^{1-2\alpha}\log^{2\alpha}\left(\frac{\det\bm{X}_{T }}{\det\bm{X}_{0}}\right)\log^{2}\left(\frac{T}{\delta}\right).\] (23)

Hence, by following the remaining steps in the proof of Theorem 1, we can conclude that the regret is upper bounded by

\[\widetilde{\mathcal{O}}\left(T^{1-\alpha}d_{u}^{\alpha}\sqrt{K}+d_{u}\sqrt{ TK}\right),\] (24)

where the only difference is the additional \(\sqrt{K}\) appeared in the first term. 

**Proposition 3**.: _The regret of poLinUCB in Algorithm 1 for the above setting is upper bounded by \(\widetilde{\mathcal{O}}\left(T^{1-\alpha}d_{u}^{\alpha}+d_{u}\sqrt{T}\right)\) with probability at least \(1-\delta\) if \(T=\Omega(\log(1/\delta))\)._

Proof.: This proof also follows from the proof of Theorem 1. The immediate regret at each time step \(t\) is

\[\Delta_{t}=r_{t,\bm{x}_{t}^{\star}}-r_{t,\bm{x}_{t}}\] (25)

Recall that the definition of \(\bm{x}_{t}^{\star}\),

\[\bm{x}_{t}^{\star}\coloneqq\operatorname*{arg\,max}_{\bm{x}\in D_{t}}\langle \bm{\theta}^{\star},\bm{x}\rangle+\langle\bm{\beta}^{\star},\phi^{\star}(\bm{x })\rangle.\] (26)

To bound the immediate regret, we have

\[\Delta_{t}=\langle\bm{\theta}^{\star},\bm{x}_{t}^{\star}\rangle+\langle\bm{ \beta}^{\star},\phi^{\star}(\bm{x}_{t}^{\star})\rangle-\langle\bm{\theta}^{ \star},\bm{x}_{t}\rangle-\langle\bm{\beta}^{\star},\phi^{\star}(\bm{x}_{t})\rangle\] (27)

By the definition of UCB, we further have

\[\Delta_{t}\leq\langle\widetilde{\bm{\theta}}_{t},\bm{x}_{t}\rangle+\langle \widetilde{\bm{\beta}}_{t},\widetilde{\phi}_{t}^{\star}(\bm{x}_{t})\rangle- \langle\bm{\theta}^{\star},\bm{x}_{t}\rangle-\langle\bm{\beta}^{\star},\phi^{ \star}(\bm{x}_{t})\rangle.\] (28)

By rearranging the terms, we get

\[\Delta_{t}\leq\underbrace{\langle\widetilde{\bm{\theta}}_{t}-\bm{\theta}^{ \star},\bm{x}_{t}\rangle+\langle\widetilde{\bm{\beta}}_{t}-\bm{\beta}^{\star},\phi^{\star}(\bm{x}_{t})\rangle}_{\mbox{\footnotesize$\begin{array}{c}1 \end{array}$}}+\underbrace{\langle\widetilde{\bm{\beta}}_{t},\widetilde{\phi}_ {t}(\bm{x}_{t})-\phi^{\star}(\bm{x}_{t})\rangle}_{\mbox{\footnotesize$ \begin{array}{c}2\end{array}$}}\] (29)

Since we only need to fit a single \(\bm{\theta}^{\star}\), \(\bm{\beta}^{\star}\) and \(\phi^{\star}(\cdot)\). We thus have the following bound for the total regret,

\[\widetilde{\mathcal{O}}\left(T^{1-\alpha}d_{u}^{\alpha}+d_{u}\sqrt{T}\right).\] (30)

## Appendix C Technical Lemmas

**Lemma 3** (**Confidence Ellipsoid**, based on Theorem 2 of (Abbasi-Yadkori et al., 2011)).: _Let \(\bm{w}^{\star}\in\mathbb{R}^{d}\), \(\bm{V}_{0}=\lambda\bm{I}\), \(\lambda>0\). For any \(t\geq 0\), let \(\bm{u}_{1},\cdots,\bm{u}_{t}\in\mathbb{R}^{d}\), define \(r_{t}=\langle\bm{u}_{t},\bm{w}^{\star}\rangle+\eta_{t}\) where \(\eta_{t}\) is \(R_{\eta}\)-sub-Gaussian and assume that \(\|\bm{w}^{\star}\|_{2}\leq L_{\bm{w}}\); let \(\bm{V}_{t}=\bm{V}_{0}+\sum_{s=1}^{t}\bm{u}_{s}\bm{u}_{s}^{\top}\) and \(\hat{\bm{w}}_{t}\) be the corresponding regularised least-square estimator. Then, for any \(\delta>0\) and \(t\geq 0\), with probability at least \(1-\delta\), \(\bm{w}^{\star}\) lies in the set:_

\[\mathcal{C}_{t}=\left\{\bm{w}\in\mathbb{R}^{d}:\|\hat{\bm{w}}_{t}-\bm{w}\|_{ \bm{V}_{t}}\leq\sqrt{\lambda}L_{\bm{w}}+R_{\eta}\sqrt{2\log\left(\frac{\det \left(\bm{V}_{t}\right)^{1/2}\det(\lambda\bm{I})^{-1/2}}{\delta}\right)}\right\}.\] (31)_Furthermore, if for all \(t\geq 1\), \(\left\|\bm{u}_{t}\right\|\leq L_{\bm{u}}\), then for any \(\delta>0\) and \(t\geq 0\), with probability at least \(1-\delta\), \(\bm{w}^{*}\) lies in the set:_

\[\mathcal{C}_{t}=\left\{\bm{w}\in\mathbb{R}^{d}:\left\|\hat{\bm{w}}_{t}-\bm{w} \right\|_{\bm{V}_{t}}\leq\sqrt{\lambda}L_{\bm{w}}+R_{\eta}\sqrt{d\log\left( \frac{1+tL_{\bm{u}}^{2}/\lambda}{\delta}\right)}\right\}.\] (32)

**Lemma 4** (Bernstein's Inequality for Matrices, Theorem 6.1.1 of (Tropp et al., 2015)).: _Let \(\bm{X}_{1},\cdots,\bm{X}_{n}\in\mathbb{R}^{d_{1}\times d_{2}}\) be independent and centered random matrices. Assume that for each \(i\in[n]\), \(\bm{X}_{i}\) is uniformly bounded, that is:_

\[\mathbb{E}\left[\bm{X}_{i}\right]=\bm{0}\quad\text{and}\quad\left\|\bm{X}_{i }\right\|\leq B,\] (33)

_where \(\left\|\cdot\right\|\) denotes the spectral-norm distance here. Introduce the sum_

\[\bm{Z}=\sum_{i=1}^{n}\bm{X}_{i},\] (34)

_and let \(\mathbb{V}(\bm{Z})\) denote the matrix variance statistics of the sum \(\bm{Z}\):_

\[\mathbb{V}(\bm{Z}) =\max\left\{\left\|\mathbb{E}\left[\bm{Z}^{*}\right]\right\|, \left\|\mathbb{E}\left[\bm{Z}^{*}\bm{Z}\right]\right\|\right\}\] (35) \[=\max\left\{\left\|\sum_{i=1}^{n}\bm{X}_{i}\bm{X}_{i}^{*}\right\|,\left\|\sum_{i=1}^{n}\bm{X}_{i}^{*}\bm{X}_{i}\right\|\right\},\] (36)

_where the asterisk \({}^{*}\) denotes the conjugate transpose operation. Then, for every \(\epsilon\geq 0\), we have,_

\[\mathbb{P}\Big{(}\|\bm{Z}\|\geq\epsilon\Big{)}{\leq(d_{1}+d_{2})}\cdot\exp \bigg{(}\frac{-\epsilon^{2}/2}{\mathbb{V}(\bm{Z})+B\epsilon/3}\bigg{)}.\] (37)

## Appendix D Additional Results and Discussions

### Additional Experiments

We further investigate the case where the true reward is \(\langle\phi^{\star}(\bm{x}),\bm{\beta}\rangle\). For this case, the reward's dependency on the post-serving context is noiseless. The results are presented in Figure 4. We observe that the algorithm adapted from Wang et al. (2016) still performs worse than our method, though the gap seems become smaller.

### Additional Discussions on Assumption 1

Our regret analysis can accommodate different values of \(\alpha\) in Assumption 1. The rate of \(1/\sqrt{T}\) (when \(\alpha=0.5\)) is a commonly observed rate for many classical machine learning algorithms, including linear regression, logistic regression, and SVM with a linear kernel. This rate is rooted in the law of large numbers and the central limit theorem. For smaller \(\alpha\) values, the generalization error will converge more slowly than \(1/\sqrt{T}\), indicating that the learning problem becomes increasingly difficult. In the extreme case when \(\alpha=0\), \(\phi^{\star}\) function cannot be learned accurately, thus we will inevitably suffer a linear regret (simply due to model misspecification).

For standard linear functions, \(\alpha=0.5\) and our regret bound in such situations is tight w.r.t. \(\alpha\). However, we intentionally made assumption 1 to be more general in order to accommodate other much more complex ways of estimating the \(\phi\) functions such as manifold regression (see, e.g., Yang and Dunson (2016)), non-parametric ways like k-nearest-neighbors to estimate phi, or to estimate complex non-smooth function (e.g., functions in Holder spaces). For example, when \(\phi\) is a function in Holder space \(H(\beta)\), then the learning rate is \(T^{-2\beta/(2\beta+d)}\), which is generally slower than \(T^{-0.5}\) and depends on \(\beta\) as well as the data dimension \(d\) (see, e.g., the note of Tibshirani (2017)). A visual illustration can be found in Figure 3.

Figure 4: Comparison under the setup where the reward’s dependency on the post-serving context is noiseless.

Figure 3: Comparison of convergence under different \(\phi\) functions (e.g., linear and those in Holder space).