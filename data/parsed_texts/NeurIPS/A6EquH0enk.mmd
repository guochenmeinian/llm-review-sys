# Effective Bayesian Heteroscedastic Regression

with Deep Neural Networks

Alexander Immer

Equal contribution. \(\dagger\) Shared last author. Correspondence to: alexander.immer@inf.ethz.ch alexander.marx@inf.ethz.ch. Code at https://github.com/aleximmer/heteroscedastic-nn.

Emanuele Palumbo

Equal contribution. \(\dagger\) Shared last author. Correspondence to: alexander.immer@inf.ethz.ch alexander.marx@inf.ethz.ch

Alexander Marx

Equal contribution. \(\dagger\) Shared last author. Correspondence to: alexander.immer@inf.ethz.ch alexander.marx@inf.ethz.ch. Code at https://github.com/aleximmer/heteroscedastic-nn.

Julia E. Vogt

Equal contribution. \(\dagger\) Shared last author. Correspondence to: alexander.immer@inf.ethz.ch alexander.marx@inf.ethz.ch

###### Abstract

Flexibly quantifying both irreducible aleatoric and model-dependent epistemic uncertainties plays an important role for complex regression problems. While deep neural networks in principle can provide this flexibility and learn heteroscedastic aleatoric uncertainties through non-linear functions, recent works highlight that maximizing the log likelihood objective parameterized by mean and variance can lead to compromised mean fits since the gradient are scaled by the predictive variance, and propose adjustments in line with this premise. We instead propose to use the natural parametrization of the Gaussian, which has been shown to be more stable for heteroscedastic regression based on non-linear feature maps and Gaussian processes. Further, we emphasize the significance of principled regularization of the network parameters and prediction. We therefore propose an efficient Laplace approximation for heteroscedastic neural networks that allows automatic regularization through empirical Bayes and provides epistemic uncertainties, both of which improve generalization. We showcase on a range of regression problems--including a new heteroscedastic image regression benchmark--that our methods are scalable, improve over previous approaches for heteroscedastic regression, and provide epistemic uncertainty without requiring hyperparameter tuning.

## 1 Introduction

Capturing the _epistemic_ (model uncertainty) and _aleatoric_ uncertainty (observation noise) allows for computing the predictive variance of a model, which is crucial for areas such as active learning (Houlsby et al., 2011; Kirsch, 2023), reinforcement learning (Osband et al., 2016; Yu et al., 2020) and decision making. Bayesian neural networks allow for modelling both epistemic and aleatoric uncertainty, as such, they lend themselves naturally to this task. Typically, they are applied under the assumption of homoscedasticity, i.e., constant noise (MacKay, 1995; Foong et al., 2019; Kirsch, 2023, and others), but also adaptations of variational inference (VI) to model heteroscedasticity, such as mean-field VI (Graves, 2011), deterministic VI (Wu et al., 2019, DVI), and Monte-Carlo Dropout (Gal and Ghahramani, 2016), have been studied. In this paper, we are interested in learning the epistemic and aleatoric uncertainty in heteroscedastic regression for potentially complex tasks such as image regression, where we are given inputs \(\mathbf{x}\in\mathbb{R}^{D}\) and a scalar response \(\mathbf{y}\in\mathbb{R}\), and model \(\mathbf{y}\mid\mathbf{x}=\mathbf{x}\) as a conditional Gaussian distribution with mean \(\mu(\mathbf{x})\) and standard deviation \(\sigma(\mathbf{x})\) being dependent on input \(\mathbf{x}\). Inherently, the problem involves robustly modelling the aleatoric uncertainty, corresponding to the variance, which is a relevant problem in econometrics, statistics (Harvey, 1976; Amemiya, 1985), and causal discovery (Guyon et al., 2019; Xu et al., 2022).

Classical approaches based on linear models and non-linear feature maps (Cawley et al., 2004) attempt to learn \(\mu(\mathbf{x})\) and \(\sigma(\mathbf{x})\) directly by first fitting the mean function and subsequently fitting the standard deviation from the log-residuals, e.g., feasible generalized least squares (FGLS) (Woolridge, 2015). The corresponding objective is, however, only convex if one of the parameters is kept constant, but it is not jointly convex in both parameters (Cawley et al., 2004; Yuan and Wahba, 2004). To alleviate this problem in the context of Gaussian processes, (Le et al., 2005) propose to reparametrize the objective in terms of the natural parameters, which induces a jointly convex objective. Advantages of the natural parametrization compared to standard FGLS have also been demonstrated for non-linear features maps (Immer et al., 2023).

In the neural network literature, the standard approach is to model \(\mu(\mathbf{x})\) and \(\sigma(\mathbf{x})\) as outputs of the network and maximize the corresponding Gaussian log likelihood via stochastic gradient descent (Nix and Weigend, 1994; Lakshminarayanan et al., 2017; Kendall and Gal, 2017), or use two separate neural networks to model mean and standard deviation (Skafte et al., 2019). As in classical estimators, this parametrization might not be ideal and can lead to overconfident variance estimates (Skafte et al., 2019; Stirn and Knowles, 2020) and possibly compromised mean fits. Two recent strategies aim to adjust for this problem by reducing the influence of the predictive variance on the gradient of the mean. In particular, Seitzer et al. (2022) introduce a surrogate loss, the \(\beta-\)NLL loss, which regulates the influence of the variance on the gradients of the loss by introducing a stop-gradient operation. As an alternative solution, Stirn et al. (2023) propose architectural constraints coupled with two stop gradient operations, regularizing the heteroscedastic model such that its mean fit is not compromised compared to a homoscedastic baseline. We provide more details to both approaches in Sec. 2.

Our ContributionsIn comparison to previous work on heteroscedastic regression with neural networks, we take a different perspective postulating that current estimators lack principled regularization. While recent work aims at regularizing the influence of the predictive variance, as we discuss in Sec. 2, we show that this focus can shift the problem to compromising capacity for the variance estimate. Instead, we propose three major modifications to tackle the problem of fitting heteroscedastic neural networks: akin to previous work on GPs (Le et al., 2005) and linear models (Immer et al., 2023), we propose to re-parameterize the loss using the _natural parametrization_ (cf. Sec. 3) which is known to be jointly concave in both parameters. Empirically, we find that this parametrization can be more stable during optimization. Further, we derive an _efficient Laplace approximation to the marginal likelihood_ for heteroscedastic regression that can automatically learn regularization via empirical Bayes and provide an early-stopping signal to prevent overfitting without requiring a grid search based on a validation set in Sec. 4.2 Additionally, the Laplace approximation provides epistemic uncertainty through the _Bayesian posterior predictive_, which generally improves predictive performance. We provide a fast closed-form approximation to the posterior predictive that also provides a simple split into aleatoric and epistemic uncertainties. This predictive is illustrated on the right in Figure 1 and shown in comparison to the prediction with only aleatoric uncertainty (MAP) as well as the corresponding homoscedastic regression solution on the left.

Footnote 2: DVI (Wu et al., 2019) also employs empirical Bayes regularization, but does only apply to MLPs.

Besides showing that our approach performs favorably on commonly used UCI benchmarks and the CRISPR-Cas13 knockdown efficacy datasets (Stirn et al., 2023), we notice the lack of more complex (heteroscedastic) regression benchmark datasets. To that end, we propose _new image-regression benchmarks_ based on image classification datasets. The input images are randomly rotated and the targets are the random rotations with heteroscedastic noise that depends on the label.

Figure 1: Illustration of the proposed training and posterior predictive of a heteroscedastic Bayesian neural network (right) in comparison to a homoscedastic one (left).

## 2 Heteroscedastic Regression with Neural Networks

Due to their universal approximation guarantees (Hornik et al., 1989), deep neural networks have the capacity to solve complex regression problems. As in any regression task, however, unregularized function approximators also have the tendency to overfit to the data. Due to the additional degree of freedom granted by learning both mean and variance in heteroscedastic regression, this problem is amplified, and persists when doing a classical grid search over regularization hyperparameters. In the following, we review prior work that proposes to regularize the influence of the variance and highlight some limitations of these approaches by proposing a new image regression task.

### Regularizing the Influence of the Variance

Skafte et al. (2019) note that naively minimizing the Gaussian log likelihood by learning the mean and variance as output of a neural network can lead to overconfident variance estimates that compromise the mean fit. Taking into account the gradients of the negative log likelihood (NLL),

\[\ell_{\mu,\sigma}(\bm{\theta})=\tfrac{1}{2}\log\sigma^{2}(\mathbf{x};\bm{ \theta})+\tfrac{(y-\mu(\mathbf{x};\bm{\theta}))^{2}}{2\sigma^{2}(\mathbf{x}; \bm{\theta})}+\mathit{const},\] (1)

where \(\bm{\theta}\) denote the neural network parameters used to estimate mean and variance, Seitzer et al. (2022) trace the problem to the fact that \(\nabla_{\mu}\ell_{\mu,\sigma}(\bm{\theta})=\tfrac{\mu(\mathbf{x};\bm{\theta} )-y}{\sigma^{2}(\mathbf{x};\bm{\theta})}\) heavily depends on the learned variance. To overcome this effect, Seitzer et al. (2022) introduce the \(\beta-\)NLL loss \(\ell_{\beta}(\bm{\theta})\), which is equal to \(\lfloor\sigma^{2\beta}(\mathbf{x};\bm{\theta})\rfloor\cdot\ell_{\mu,\sigma}( \bm{\theta})\), where \(\lfloor\cdot\rfloor\) denotes a stop-gradient operation, and \(\beta\) is a hyperparameter controlling the dependency of gradients on the predictive variance. As a result, the gradients for \(\beta-\)NLL are equal to

\[\nabla_{\mu}\ell_{\beta}(\bm{\theta})=\tfrac{\mu(\mathbf{x};\bm{\theta})-y}{ \sigma^{2-2\beta}(\mathbf{x};\bm{\theta})},~{}~{}\nabla_{\sigma^{2}}\ell_{ \beta}(\bm{\theta})=\tfrac{\sigma^{2}(\mathbf{x};\bm{\theta})-(y-\mu(\mathbf{ x};\bm{\theta}))^{2}}{2\sigma^{4-2\beta}(\mathbf{x};\bm{\theta})}~{}.\] (2)

With \(\beta=0\), \(\ell_{\beta}(\bm{\theta})\) is equivalent to \(\ell_{\mu,\sigma}(\bm{\theta})\), whereas for \(\beta=1\) the gradient with respect to the mean is proportional to the gradient for homoscedastic regression. When setting \(0<\beta\leq 1\) we interpolate between both settings. As an alternative approach, Stim et al. (2023) propose to decouple the estimation into three networks: a shared representation learner \(f_{\mathbf{z}}\) computes a representation \(\mathbf{z}\) from \(\mathbf{x}\), which is passed into two individual networks \(f_{\mu}\) and \(f_{\Sigma}\), which receive \(\mathbf{z}\) as input and output the mean and covariance matrix, respectively. To ensure that the gradient with respect to the mean is equal to the gradient of the homoscedastic model, they introduce two stop-gradient operations: the first one has an equivalent effect on the mean gradient for \(f_{\mathbf{z}}\) and \(f_{\mu}\) as setting \(\beta=1\) in \(\beta\)-NLL, and the second one stops any gradient from the variance network \(f_{\Sigma}\) from propergating to \(f_{\mathbf{z}}\)(Stim et al., 2023). We provide more details in App. B.

Taking a different perspective, one can view both proposals as implicit regularization techniques for the variance parameter. The surrogate score of Seitzer et al. (2022) regularizes the influence of the predictive variance on the gradient, while the network architecture and stop-gradient operations introduced by Stim et al. (2023) have a similar, yet stronger regularization effect for the variance--i.e., stopping its influence on the joint representation learner. An additional hurdle presents itself due to the fact that we need to tune the regularization to calibrate the models for a certain dataset, which can have a strong influence on the result, as we show below. Concurrent work (Wong-Toi et al., 2023) also highlights the necessity for regularization in heteroscedastic regression when considering individual mean and variance networks. They find that the networks relative regularization is critical.

### Learning Complex Variance Dependencies

Despite alleviating the problem of compromised mean fits, the question arises if such regularization limits the capabilities for learning the aleatoric uncertainty.

In the \(\beta\)-NLL objective, we need to select the influence of the estimated variance on both gradients, while the effect of scaling is different as can be seen from Equation 2. Due to the typically applied standardization of the data, we expect that \(\sigma^{2}\leq 1\) and hence the gradient with respect to the variance is amplified stronger compared to the mean. Especially in near-deterministic processes (\(\sigma^{2}\to 0\)) this might be problematic. Further, it is not clear which value of \(\beta\) is best suited for a problem a priori introducing another hyperparameter that has to be tuned in addition to tuning regularization hyperparameters. For the approach by Stim et al. (2023, _Faithful_) the regularization of the variance ismore severe due to forcing \(\mu(\mathbf{x})\) and \(\Sigma(\mathbf{x})\) to share a joint representation that receives no gradient from the variance. If, for example, \(\Sigma(\mathbf{x})\) depends on a set of variables that is independent of those influencing the mean, no information about the variance might be contained in the joint representation. To illustrate that only regularizing the variance can be suboptimal, consider the following example.

**Problem 2.1** (**Heteroscedastic Regression from Image Data**): _Consider a version of rotated MNIST with rotation angle drawn as \(\texttt{rot}\sim\text{Unif}(-90,90)\). We generate the target \(y\) as \(y=\texttt{rot}+(11c+1)\epsilon\), where \(c\in\{0,1,\dots,9\}\) is the image class as integer number and \(\epsilon\sim\mathcal{N}(0,1)\) is an independent noise source. The heteroscedastic regression task of learning the distribution of \(y\) given observations (images) \(\mathbf{x}\) involves learning a complex non-linear mapping for the variance, while learning the mean only requires learning the rotation angle._

We train a simple 3-layer MLP of width \(500\) using the baseline objectives and our approach (_Natural Laplace_ introduced later in Secs. 3 and 4) on data generated as described above. First, we note that the test-NLL of \(\beta\)-NLL and Faithful strongly depends on the regularization strength (cf. Figure 2), which emphasizes the importance of additional regularization. Further, we report how much information about the image classification task is contained in the last layer of the MLP in Figure 2--which provides a proxy on the information about the variance that has been picked up by each approach. The homoscedastic model serves as a control that does not require knowledge about the label. As expected, Faithful achieves a low accuracy on this downstream task since it limits the capability for learning the variance. \(\beta\)-NLL shows a better performance but is still significantly outperformed by our proposal. The difference becomes even more evident when using a CNN architecture on rotated FashionMNIST as we demonstrate in Sec. 5.3. We also provide a minimal example illustrating the limitations of _Faithful_ in App. C, which shows that the bottleneck size of the joint network \(f_{\mathbf{z}}\) has a strong influence of the estimation quality of the variance.

As alternatives to Problem 2.1, we consider two generative processes for the rotated image regression task: first, we use a homoscedastic case as ablation with \(y=\texttt{rot}+10\varepsilon\). Further, we use a heteroscedastic noise that is based on the rotational magnitude, \(y=\texttt{rot}+\sqrt{|\texttt{rot}|}\varepsilon\). In this case, both mean and variance depend on the same feature, which is a setting where _Faithful_(Stirn et al., 2023) can theoretically work.

## 3 Naturally Parameterized Heteroscedastic Regression

We model a dataset \(\mathcal{D}=\{(\mathbf{x}_{n},y_{n})\}_{n=1}^{N}\) with \(N\) pairs of input \(\mathbf{x}_{n}\in\mathbb{R}^{D}\) and scalar response \(y_{n}\in\mathbb{R}\) using the natural form of the Gaussian likelihood with unknown variance, as first introduced for the case of Gaussian processes (Le et al., 2005) and recently applied in the context of causal discovery with maximum likelihood (Immer et al., 2023). The relationship between the natural parameters \(\bm{\eta}\) and the mean \(\mu\) and variance \(\sigma^{2}\) of the parametrization is

\[\eta_{1}=\tfrac{\mu}{\sigma^{2}}\text{ and }\eta_{2}=-\tfrac{1}{2\sigma^{2}}<0,\] (3)

which can be understood as the signal-to-variance ratio and the negative precision (inverse variance). We model these natural parameters using a neural network \(\mathbf{f}(\mathbf{x};\bm{\theta})\in\mathbb{R}^{2}\) that is parameterized by weights \(\bm{\theta}\in\mathbb{R}^{P}\) and acts on the inputs \(\mathbf{x}\in\mathbb{R}^{D}\). To satisfy the constraint that \(\eta_{2}<0\), we _link_\(\mathbf{f}\) to \(\bm{\eta}\) using a positive function \(g_{+}:\mathbb{R}\mapsto\mathbb{R}_{+}\) and have the following mapping:

\[\eta_{1}(\mathbf{x};\bm{\theta})=f_{1}(\mathbf{x};\bm{\theta})\text{ and }\eta_{2}( \mathbf{x};\bm{\theta})=-g_{+}(f_{2}(\mathbf{x};\bm{\theta})).\] (4)

Figure 2: Left: Example heteroscedastic image regression data point with target \(y\sim\texttt{rot}+56\epsilon\). Middle: Test log likelihood for different values of prior precision for \(\beta\)-NLL and Faithful. Right: Downstream accuracy using a linear model on the last layer representation after learning the heteroscedastic image regression. Compromised variance fit leads to worse downstream accuracy.

We use either the exponential \(g_{+}(\cdot)=\frac{1}{2}\exp(\cdot)\) or softplus \(g_{+}(\cdot)=\frac{1}{\beta}\log(1+\exp(\cdot))\) as typical for heteroscedastic regression with mean-variance parametrization. Mathematically, the _heteroscedastic Gaussian log likelihood of our model_ is given by

\[\log p(y|\mathbf{x},\bm{\theta})=\begin{bmatrix}\eta_{1}(\mathbf{x};\bm{ \theta})\\ \eta_{2}(\mathbf{x};\bm{\theta})\end{bmatrix}^{\mathsf{T}}\begin{bmatrix}y\\ y^{2}\end{bmatrix}+\frac{\eta_{1}(\mathbf{x};\bm{\theta})^{2}}{4\eta_{2}( \mathbf{x};\bm{\theta})}+\frac{1}{2}\log(-2\eta_{2}(\mathbf{x};\bm{\theta}))+ \mathit{const},\] (5)

which we also denote by \(-\ell_{\bm{\eta}}(\bm{\theta})\). Assuming the data are _i.i.d._, we have \(\log p(\mathcal{D}|\bm{\theta})=\sum_{n=1}^{N}\log p(y_{n}|\mathbf{x}_{n},\bm{ \theta})\). Immer et al. (2023) used this maximum likelihood objective for bivariate causal discovery using linear models and small neural networks. Further, the gradient and Hessian take on simple forms due to the properties of the natural parametrization.

### Gradients of the Natural parametrization

Similar to Seitzer et al. (2022) and Stirn et al. (2023), we also inspect the gradients of the corresponding negative log likelihood with respect to \(\bm{\eta}\), leading to

\[\nabla_{\eta_{1}}\ell_{\bm{\eta}}(\bm{\theta})=-\frac{\eta_{1}(\mathbf{x};\bm {\theta})}{2\eta_{2}(\mathbf{x};\bm{\theta})}-y,\ \ \nabla_{\eta_{2}}\ell_{\bm{\eta}}(\bm{\theta})=\frac{( \eta_{1}(\mathbf{x};\bm{\theta}))^{2}}{4(\eta_{2}(\mathbf{x};\bm{\theta}))^{2} }-\frac{1}{2\eta_{2}(\mathbf{x};\bm{\theta})}-y^{2}\.\] (6)

The gradients by themselves cannot be directly linked to mean and variance updates. We note, however, that if we relate the natural parameters to mean and variance, i.e., we compute \(\mu(\mathbf{x};\bm{\theta})\) as \(-\frac{\eta_{1}(\mathbf{x};\bm{\theta})}{2\eta_{2}(\mathbf{x};\bm{\theta})}\) and \(\sigma^{2}(\mathbf{x};\bm{\theta})\) as \(-\frac{1}{2\eta_{2}(\mathbf{x};\bm{\theta})}\), then \(\nabla_{\eta_{1}}\ell_{\bm{\eta}}(\bm{\theta})\) reduces to \(\mu(\mathbf{x};\bm{\theta})-y\), and similarly, \(\nabla_{\eta_{2}}\ell_{\bm{\eta}}(\bm{\theta})\) to \(\sigma^{2}(\mathbf{x};\bm{\theta})-(y^{2}-(\mu(\mathbf{x};\bm{\theta}))^{2})\), which would be desired because these are simply separate residuals for mean and variance (Seitzer et al., 2022; Stirn et al., 2023). Empirically, we observe that this parametrization can be more stable to train and less prone to insufficient regularization as also observed previously for Gaussian process and ridge regression (Le et al., 2005; Immer et al., 2023).

### Regularization using Bayesian Inference

Due to the expressiveness of heteroscedastic regression, regularization is crucial to obtain well-generalizing models (cf. Figure 2). We achieve regularization in two ways: first, we regularize parameters towards low norm using classical \(\ell^{2}\) regularization, which corresponds to a Gaussian prior on the parameters. Further, we use a Bayesian posterior predictive, which additionally accounts for uncertainties of the model as depicted in the illustration (Figure 1, right).

For effective regularization of deep neural networks, we use a layer-wise Gaussian prior on the parameters given by \(p(\bm{\theta}|\bm{\delta})=\prod_{t}\mathcal{N}(\bm{\theta}_{t};\bm{0},\delta _{t}^{-1}\mathbf{I})\). In the case of the last layer or a simple linear model with the natural heteroscedastic likelihood, it induces a mode on \(\eta_{1}(\mathbf{x})=0\) and \(\eta_{2}(\mathbf{x})=-\frac{1}{2}\), which corresponds to zero mean and unit variance and is reasonable for standardized response variables. The layer-wise prior further allows to differently regularize parts of the neural network and has been observed to improve generalization in image classification (Immer et al., 2021; Daxberger et al., 2021; Antoran et al., 2022). Concurrent work by also suggests that this might be the case for heteroscedastic regression (Wong-Toi et al., 2023). However, optimizing a regularization parameter per layer is intractable using a validation-based grid search.

To optimize layer-wise prior precisions and obtain a posterior predictive, we make use of Bayesian inference. Combining the natural Gaussian likelihood \(p(\mathcal{D}|\bm{\theta})\) (below Equation 5) with the prior \(p(\bm{\theta}|\bm{\delta})\), we have the joint distribution \(p(\mathcal{D},\bm{\theta}|\bm{\delta})\), which corresponds to a regularized objective. According to Bayes' theorem, we have the posterior \(p(\bm{\theta}|\mathcal{D},\bm{\delta})\propto p(\mathcal{D},\bm{\theta}|\bm{ \delta})\). The normalization constant, also referred to as _marginal likelihood_, is given by \(p(\mathcal{D}|\bm{\delta})=\int p(\mathcal{D},\bm{\theta}|\bm{\delta})\mathrm{d }\bm{\theta}\) and gives us the Type II maximum likelihood objective to optimize the prior precisions \(\bm{\delta}\). This procedure is referred to as _empirical Bayes_ (EB). Inferring the posterior distribution of the neural network parameters, we further have access to the posterior predictive \(p(y_{*}|\mathbf{x}_{*},\mathcal{D})\) for a new data point \(\mathbf{x}_{*}\). By averaging over multiple hypotheses from the posterior, the predictive can be better regularized than a single model (Wilson, 2020). Unfortunately, inference is intractable for deep neural networks.

## 4 Approximate Inference with a Laplace Approximation

We develop a scalable Laplace approximation for the posterior in heteroscedastic regression with deep neural networks. The Laplace approximation (MacKay, 1995) is an effective method for approximating the marginal likelihood, posterior, and predictive in deep learning (Daxberger et al., 2021). In comparison to other approximate inference methods, it can rely on effective training algorithms developed for deep learning and also offers a differentiable marginal likelihood estimate that enables empirical Bayes (EB). Efficient curvature approximations further make it scalable to deep learning (Ritter et al., 2018). We extend these to the heteroscedastic regression setting.

Laplace approximates the posterior locally at a mode of the posterior with a Gaussian distribution, \(p(\bm{\theta}|\mathcal{D},\bm{\delta})\approx\mathcal{N}(\bm{\theta};\bm{ \theta}_{*},\bm{\Sigma})\). The mean is given by a stationary point of the posterior, \(\bm{\theta}_{*}=\arg\max_{\bm{\theta}}\log p(\mathcal{D},\bm{\theta}|\bm{ \delta})\), and the covariance by the curvature at that mode, \(\bm{\Sigma}^{-1}=\nabla^{2}_{\bm{\theta}}\log p(\mathcal{D},\bm{\theta}|\bm{ \delta})_{\bm{\theta}=\bm{\theta}_{*}}\). This is due to a second-order Taylor approximation of the log posterior around the mode. The mean is hence the result of neural network training, however, the covariance requires estimating and inverting a Hessian, which is typically intractable.

### Linearized Laplace for Natural Heteroscedastic Regression

The linearized Laplace approximation (MacKay, 1995; Khan et al., 2019; Foong et al., 2019; Immer et al., 2021) overcomes issues of the vanilla Laplace approximation. Linearizing the neural network about the parameters at the mode, the Hessian, which is the generalized Gauss-Newton in this case (Martens, 2020), becomes positive semidefinite and offers efficient structured approximations. In particular, we have the following Hessian approximation due to linearization

\[\bm{\Sigma}^{-1}\approx\sum_{n=1}^{N}\mathbf{J}_{*}(\mathbf{x}_{n})^{\intercal }[-\nabla^{2}_{\bm{\eta}}\log p(y_{n}|\mathbf{x}_{n},\bm{\theta}_{*})] \mathbf{J}_{*}(\mathbf{x}_{n})+\nabla^{2}_{\bm{\theta}}\log p(\bm{\theta}|\bm{ \delta})|_{\bm{\theta}=\bm{\theta}_{*}},\] (7)

where \([\mathbf{J}_{*}(\mathbf{x})]_{cp}=\frac{\partial\eta_{c}(\mathbf{x};\bm{ \theta})}{\partial\theta_{p}}|_{\bm{\theta}=\bm{\theta}_{*}}\) is the Jacobian of the neural network \(\bm{\eta}(\mathbf{x};\bm{\theta})\) at the mode. The first summand is the generalized Gauss-Newton and the second term is the Hessian of the prior, which is simply a diagonal matrix constructed from entries \(\delta_{l}>0\), the layer-wise regularization parameters.

Due to the natural parametrization of the likelihood, the Hessian approximation of the linearized Laplace approximation is guaranteed to be positive definite. Since the log prior Hessian is diagonal with entries \(\delta_{l}>0\), one only has to show that the negative log likelihood Hessian, \(-\nabla^{2}_{\bm{\eta}}\log p(y|\mathbf{x},\bm{\theta}_{*})\), is positive semidefinite, which is simply a property of naturally parameterized exponential families (Martens, 2020). Note that this would not be the case for the mean-variance parametrization3. For efficient computation, we can decompose the log likelihood Hessian as

Footnote 3: In Sec. 4.5 we show how Laplace can be applied for the mean-variance parametrization despite that.

\[\bm{\Lambda}_{*}(\mathbf{x}) \overset{\text{\tiny def}}{=}\nabla^{2}_{\bm{\eta}}\log p(y| \mathbf{x},\bm{\theta}_{*})=\begin{bmatrix}\frac{1}{2\eta_{2}(x;\bm{\theta}_{ *})}&-\frac{\eta_{1}(x;\bm{\theta}_{*})}{2\eta_{2}(x;\bm{\theta}_{*})^{2}}\\ -\frac{\eta_{1}(x;\bm{\theta}_{*})}{2\eta_{2}(x;\bm{\theta}_{*})^{2}}&\frac{ \eta_{1}(x;\bm{\theta}_{*})^{2}}{2\eta_{2}(x;\bm{\theta}_{*})^{3}}-\frac{1}{2 \eta_{2}(x;\bm{\theta}_{*})^{2}}\end{bmatrix}\] (8) \[=\begin{bmatrix}-\frac{1}{\sqrt{-2\eta_{2}(x;\bm{\theta}_{*})}}& \frac{\eta_{1}(x;\bm{\theta}_{*})}{\eta_{2}(x;\bm{\theta}_{*})\sqrt{-2\eta_{2 }(x;\bm{\theta}_{*})}}\end{bmatrix}^{2}-\begin{bmatrix}0&\frac{1}{\sqrt{2\eta _{2}(x;\bm{\theta}_{*})}}\end{bmatrix}^{2}\] (9) \[\overset{\text{\tiny def}}{=}\lambda_{1}(\mathbf{x})\lambda_{1}( \mathbf{x})^{\intercal}-\lambda_{2}(\mathbf{x})\lambda_{2}(\mathbf{x})^{\intercal},\] (10)

where the square indicates the outer product. This alleviates the need to compute this matrix and instead allows to work with outer products of Jacobian-vector products to compute the covariance in Equation 7. However, the full Hessian approximation remains quadratic in the number of neural network weights. We tackle this issue with a scalable Kronecker-factored approximation.

### Scalable Kronecker-Factored Hessian Approximation

To enable the application of the Laplace approximation to heteroscedastic deep neural networks and large datasets, we use a layer-wise Kronecker-factored approximation (KFAC; Martens and Grosse, 2015; Botev et al., 2017). To overcome the quadratic scaling in the number of parameters, KFAC makes two efficient approximations: first, it constructs a block-diagonal approximation to \(\mathbf{H}\) from blocks \(\mathbf{H}_{l}\) per layer \(l\). Second, each block \(\mathbf{H}_{l}\) is approximated as a Kronecker product that enables efficient storage and computation using only the individual factors. In the following, we revise KFAC for linear layers and define it for the specific case of the heteroscedastic natural Gaussian likelihood. The same derivation applies similarly to other layer types (Osawa, 2021).

Using the Hessian decomposition of the natural log likelihood in Equation 10, we derive a KFAC approximation that can efficiently be computed in a closed-form. We can write the Jacobian of a fully connected layer that maps a \(D\) to a \(D^{\prime}\)-dimensional representation as a Kronecker product \(\mathbf{J}_{l}(\mathbf{x}_{n})^{\mathsf{T}}=\mathbf{a}_{l,n}\otimes\mathbf{g}_{l,n}\) with \(\mathbf{a}_{l,n}\in\mathbb{R}^{D\times,1}\) as the layer's input and \(\mathbf{g}_{l,n}\in\mathbb{R}^{D^{\prime}\times 2}\) as transposed Jacobian w.r.t. the output, both for the input \(\mathbf{x}_{n}\). Following Martens and Grosse (2015) and Botev et al. (2017), we then have the KFAC approximation

\[[\boldsymbol{\Sigma}^{-1}]_{l} =\sum_{n=1}^{N}[\mathbf{a}_{l,n}\otimes\mathbf{g}_{l,n}] \boldsymbol{\Lambda}_{n}[\mathbf{a}_{l,n}\otimes\mathbf{g}_{l,n}]^{\mathsf{T}} +\delta_{l}\mathbf{I}=\sum_{n=1}^{N}[\mathbf{a}_{l,n}\mathbf{a}_{l,n}^{ \mathsf{T}}]\otimes[\mathbf{g}_{l,n}\boldsymbol{\Lambda}_{n}\mathbf{g}_{l,n}^ {\mathsf{T}}]+\delta_{l}\mathbf{I}\] (11) \[\approx\tfrac{1}{N}\left[\sum_{n=1}^{N}\mathbf{a}_{l,n}\mathbf{ a}_{l,n}^{\mathsf{T}}\right]\otimes\left[\sum_{n=1}^{N}\sum_{k=1}^{2}\mathbf{g}_{l,n} \boldsymbol{\lambda}_{n,k}\boldsymbol{\lambda}_{n,k}^{\mathsf{T}}\mathbf{g}_{l,n}^{\mathsf{T}}\right]+\delta_{l}\mathbf{I}\stackrel{{\text{def} }}{{=}}\mathbf{A}_{l}\otimes\mathbf{B}_{l}+\delta_{l}\mathbf{I},\]

where \(\boldsymbol{\lambda}_{n,k}\in\mathbb{R}^{2\times 1}\) is due to to the decomposition of \(\boldsymbol{\Lambda}_{n}\) into outer products Equation 10 and the approximation is due to exchanging the sum and product. Conveniently, the terms \(\mathbf{g}_{l,n}\boldsymbol{\lambda}_{n,k}\) can be computed efficiently using two Jacobian-vector products and the Kronecker factors can then be extracted within the second-order framework of Osawa (2021). To the best of our knowledge, this is the first instantiation of KFAC for heteroscedastic regression. While we derive it for the Laplace approximation, it could also be useful for optimization Martens and Grosse (2015).

### Empirical Bayes for Automatic Regularization

To automatically regularize the heteroscedastic neural network, we use an empirical Bayes (EB) procedure that optimizes the layer-wise prior precisions, \(\delta_{l}\), during training by maximizing the Laplace approximation to the _marginal likelihood_(Immer et al., 2021). This procedure can exhibit a Bayesian variant of Occam's razor (Rasmussen and Ghahramani, 2000) and trades off model fit and complexity. Although online training violates the stationarity assumption of the Laplace approximation, it has been observed to work well in practice (Immer et al., 2021, 2023; Daxberger et al., 2021; Lin et al., 2023). We use gradient-based optimization of the log marginal likelihood,

\[\log p(\mathcal{D}|\boldsymbol{\delta})\approx\log p(\mathcal{D}|\boldsymbol{ \theta}_{*})+\log p(\boldsymbol{\theta}_{*}|\boldsymbol{\delta})+\tfrac{1}{2} \log|\boldsymbol{\Sigma}|+\tfrac{P}{2}\log 2\pi\propto\log p(\boldsymbol{ \theta}_{*}|\boldsymbol{\delta})+\tfrac{1}{2}\log|\boldsymbol{\Sigma}|,\] (12)

which crucially requires differentiating the log-determinant w.r.t. \(\boldsymbol{\delta}\), which is only tractable for small neural networks. For deep neural networks, it can be done efficiently using the KFAC approximation derived in Sec. 4.2 by eigendecomposition of the individual Kronecker factors (Immer et al., 2021, 2022). In practice, we compute the marginal likelihood approximation every few epochs to adapt the regularization, which effectively mitigates overfitting, and use it as an early-stopping criterion. The detailed empirical Bayes training algorithm is also described in Alg. 1.

### Posterior Predictive for Epistemic Uncertainties

We use the linearized posterior predictive that is in line with the linearized Laplace posterior approximation and performs typically better than sampling weights directly (Immer et al., 2021). We define the linearized neural network as \(\boldsymbol{\eta}_{*}^{\text{lin}}(\mathbf{x};\boldsymbol{\theta})\stackrel{{ \text{def}}}{{=}}\boldsymbol{\eta}(\mathbf{x};\boldsymbol{\theta}_{*})+ \mathbf{J}_{*}(\mathbf{x})(\boldsymbol{\theta}-\boldsymbol{\theta}_{*})\). Due to the Gaussianity of the Laplace approximation, \(\mathcal{N}(\boldsymbol{\theta}_{*};\boldsymbol{\Sigma})\), and the linearization, we can express the function-space posterior as a Gaussian on the natural parameters \(\boldsymbol{\eta}_{*}^{\text{lin}}(\mathbf{x})\sim\mathcal{N}(\boldsymbol{ \eta}(\mathbf{x};\boldsymbol{\theta}_{*});\mathbf{J}_{*}(\mathbf{x})\boldsymbol {\Sigma}\mathbf{J}_{*}(\mathbf{x})^{\mathsf{T}})\stackrel{{\text{ def}}}{{=}}q(\boldsymbol{\eta}|\mathbf{x})\). A straightforward way to approximate the posterior predictive is then a Monte-Carlo estimate of \(p(y_{*}|\mathbf{x}_{*},\mathcal{D})\approx\int p(y_{*}|\mathbf{x}_{*}, \boldsymbol{\eta})q(\boldsymbol{\eta}|\mathbf{x}_{*})\,\mathrm{d}\boldsymbol{\eta}\) by sampling multiple \(\boldsymbol{\eta}_{*}^{\text{lin}}(\mathbf{x})\).

Alternatively, we propose to use an approximation to the posterior predictive that can be computed in a closed-form without sampling, similar to the _probit approximation_ commonly used in the classification setting (Daxberger et al., 2021). To enable a closed-form posterior predictive, we restrict ourselves to the epistemic uncertainty about the mean as proposed by Le et al. (2005) for heteroscedastic Gaussian process regression. Instead of linearizing the natural parameters, we first transform them to the mean and variance using the inverse mapping of Equation 3, i.e., we have \(\mu(\mathbf{x};\boldsymbol{\theta})=-\tfrac{\eta_{1}(\mathbf{x};\boldsymbol{ \theta})}{2\eta_{2}(\mathbf{x};\boldsymbol{\theta})}\) and \(\sigma^{2}(\mathbf{x};\boldsymbol{\theta}_{*})=-\tfrac{1}{2\eta_{2}(\mathbf{x}; \boldsymbol{\theta}_{*})}\). Next, we only linearize the mean function \(\mu_{*}^{\text{lin}}(\mathbf{x};\boldsymbol{\theta})=\mu(\mathbf{x}; \boldsymbol{\theta}_{*})+\mathbf{J}_{*,\mu}(\mathbf{x})(\boldsymbol{\theta}- \boldsymbol{\theta}_{*})\), where \(\mathbf{J}_{*,\mu}(\mathbf{x})\) is the Jacobian of the mean, and have

\[p(y_{*}|\mathbf{x}_{*},\mathcal{D}) \approx\int\!\!\mathcal{N}(y_{*}|\mu_{*}^{\text{lin}}(\mathbf{x }_{*}),\sigma^{2}(\mathbf{x};\boldsymbol{\theta}_{*}))\mathcal{N}(\boldsymbol{ \theta};\boldsymbol{\theta}_{*},\boldsymbol{\Sigma})\,\mathrm{d}\boldsymbol{\theta}\] \[=\mathcal{N}(y_{*};\mu(\mathbf{x};\boldsymbol{\theta}_{*}), \underbrace{\mathbf{J}_{*,\mu}(\mathbf{x}_{*})\boldsymbol{\Sigma}\mathbf{J}_{*, \mu}(\mathbf{x}_{*})^{\mathsf{T}}}_{\text{epistemic}}+\underbrace{\sigma^{2}( \mathbf{x};\boldsymbol{\theta}_{*})}_{\text{aleinderic}}),\] (13)

where the epistemic and aleatoric uncertainty about the mean are clearly split. An example of this posterior predictive approximation is shown in Figure 1.

### Laplace Approximation for Mean-Variance parametrization

Using the natural parameter mapping, it is possible to apply above Laplace approximations to the mean-variance parametrization and profit from the empirical Bayes and posterior predictive procedures. However, because the negative log likelihood Hessian w.r.t. the mean and variance parameters can be indefinite, this does not work naively. By mapping \(\mu,\sigma^{2}\) to the natural parameters using Equation 3, all above derivations apply and correspond to a separate Gauss-Newton approximation of the log likelihood with Jacobians of the mapping and the natural log likelihood Hessian (see App. A).

### Limitations

As common for heteroscedastic regression, we make the assumption that the conditional distribution of the target given the observations follows a Gaussian distribution. A practitioner should keep in mind that the full Laplace approximation has high computational complexity, and should resort to the KFAC approximation, which we also use for more complex settings in experiments. Further, the true posterior of neural networks is in most cases multimodal while our Laplace approximation only covers a single mode. However in practical settings, the Laplace approximation provides strong results (Daxberger et al., 2021) and can even be ensembled (Eschenhagen et al., 2021).

## 5 Experiments

We evaluate the effectiveness of the natural parameterization compared to the mean-variance (naive) one, and empirical Bayes (EB) to optimizing a single regularization parameter using a grid search on the validation set (GS), and the MAP prediction vs a Bayesian posterior predictive (PP) in comparison to state-of-the-art baselines on three experimental settings: the UCI regression benchmark (Hernandez-Lobato and Adams, 2015), which is also well-established for heteroscedastic regression (Seitzer et al., 2022; Stim et al., 2023), the recently introduced CRISPR-Cas13 gene expression datasets (Stim et al., 2023), and our proposed heteroscedastic image-regression dataset (cf. Problem 2.1) in three noise variants. For UCI regression, we use the full Laplace approximation and we resort to the KFAC approximation for the remaining tasks for computational efficiency. Further, we use the proposed closed-form posterior predictive for our Laplace approximations. If applicable, as for our methods, we test the effect of having a Bayesian posterior predictive (PP) in comparison to the point estimate.

BaselinesAs control to assess the benefit of heteroscedastic aleatoric uncertainty, we include a homoscedastic model with EB and Laplace posterior predictive. In addition, we include the mean-variance parameterization of the negative log likelihood loss for heteroscedastic regression (_Naive NLL_). As competitive baselines, we include the recently proposed \(\beta\)_-NLL_(Seitzer et al., 2022) and _Faithful_(Stim et al., 2023) heteroscedastic losses. Finally we include well-established approaches for heteroscedastic regression with Bayesian neural networks, namely mean-field variational inference (_VI_) (Graves, 2011) and Monte-Carlo Dropout (Gal and Ghahramani, 2016) (_MC-Dropout_). Note that for our VI baseline we employ Flipout (Wen et al., 2018) to improve gradient estimation.

Details on model architectures, hyperparameter tuning, and additional results are in App. D.

### UCI Regression

In Table 1 we report results on the UCI regression datasets (Hernandez-Lobato and Adams, 2015) for the compared models in terms of test log likelihood. We underline the best performance for each dataset, and bold results which are not statistically distinguishable from best performance. That is, if the mean performance of a model falls within the region of the mean plus minus two standard errors of the best performing model. The two rightmost columns report the total number of _wins_ and _ties_ for each model: wins are the number of datasets in which the given model achieves the best performance, while ties are the number of datasets in which the model achieves results which are not statistically distinguishable from the best performing model. The results validate the effectiveness of EB regularization on different heteroscedastic loss parameterizations. Notably, with the Bayesian posterior predictive (PP) our proposed methods considerably improve in performance in comparison to using a point estimate, and significantly outperform existing state-of-the-art approaches. Finally the results confirm that, particularly when the point prective is used, using the Natural parameterization of the heteroscedastic NLL improves training stability.

[MISSING_PAGE_FAIL:9]

To create the modified datasets, we follow the procedure as outlined in Problem 2.1. An observation \(\mathbf{x}\) is generated by rotating an MNIST (resp. FashionMNIST) image with rotation angle drawn as \(\texttt{rot}\sim\text{Unif}(-90,90)\), and the corresponding target \(y\) is generated as \(y=\texttt{rot}+(11c+1)\epsilon\), where \(c\in\{0,1,\dots,9\}\) is the image class of \(\mathbf{x}\) as integer number and \(\epsilon\sim\mathcal{N}(0,1)\) is an independent noise source. To solve the problem, a heteroscedastic regression model needs to 1) learn the mapping of rotated image to angle and 2) learn the label of the image to correctly fit the heteroscedasticity of the observation noise, both of which require learning complex functions. We evaluate the methods based on the test log likelihood and RMSE. In addition, we compute the KL-divergence of the predicted distribution from the ground truth.4 We train a 3-layer 500-neuron wide MLP with ReLU activation for the MNIST-based dataset and a CNN (3 conv. and 2 linear) for the task based on FashionMNIST. In App. D.2.4, we provide additional details and results on two alternative tasks.

Footnote 4: Note, however, that the original images are not completely free of rotation which creates an additional, potentially heteroscedastic, noise source that we cannot quantify.

The results on image regression with heteroscedastic label noise indicate that the proposed empirical Bayes (EB) algorithm leads to better generalizing models and the posterior predictive (PP) strictly improves performance for both parameterizations. It is also notable that _Faithful_ fails to fit the model due to the restrictions on its variance function, and therefore performs worse than a homoscedastic model on FashionMNIST. \(\beta\)-NLL, MC-Dropout, and VI all give relatively good results but fall slightly short of the proposed methods.

## 6 Conclusions

We tackled the problem of heteroscedastic regression using neural networks with a focus on regularization. To this end we proposed three individual improvements: 1) we use the natural form of the Gaussian likelihood; we derive a scalable Laplace approximation that 2) can be used for automatic principled regularization without validation data, and 3) enables epistemic uncertainty estimation through its posterior predictive. We show on image data that our method is scalable despite the Bayesian procedures and benchmark our approach on UCI regression as well as CRISPR-Cas13 datasets achieving state-of-the-art performance. For future work, it would be interesting to relax the assumption about Gaussianity and apply our approach to other real-world datasets.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & & & \multicolumn{3}{c}{MNIST with MLP} & \multicolumn{3}{c}{FashionMNIST with CNN} \\ \cline{3-8} Objective & Regular- & Posterior & LL (\(\uparrow\)) & \(D_{\text{KL}}\) (\(\downarrow\)) & RMSE (\(\downarrow\)) & LL (\(\uparrow\)) & \(D_{\text{KL}}\) (\(\downarrow\)) & RMSE (\(\downarrow\)) \\ \hline Homoscedastic & EB & & & -5.55 (0.01) & 0.77 (0.02) & 25.6 (0.8) & -5.54 (0.00) & 0.67 (0.00) & 18.7 (0.2) \\ Naive NLL & GS & ✗ & -5.56 (0.01) & 0.68 (0.00) & 19.0 (0.1) & -5.37 (0.01) & 0.49 (0.00) & 20.1 (0.1) \\ \(\beta\)-NLL (0.5) & GS & ✗ & -5.36 (0.01) & 0.49 (0.01) & 20.7 (0.8) & -5.47 (0.03) & 0.59 (0.03) & 23.5 (0.3) \\ \(\beta\)-NLL (1) & GS & ✗ & -5.38 (0.01) & 0.51 (0.01) & 21.5 (0.5) & -5.53 (0.03) & 0.65 (0.03) & 25.5 (0.2) \\ Faithful & GS & ✗ & -5.56 (0.01) & 0.69 (0.00) & 21.1 (0.3) & -5.78 (0.00) & 0.91 (0.00) & 51.8 (0.0) \\ MC-Dropout & GS & ✓ & -5.42 (0.02) & 0.55 (0.01) & 21.0 (0.5) & -5.37 (0.01) & 0.48 (0.00) & 20.1 (0.1) \\ VI & GS & ✓ & -5.42 (0.04) & 0.56 (0.04) & 21.9 (0.4) & -5.39 (0.01) & 0.51 (0.01) & 21.4 (0.5) \\ \hline \multirow{2}{*}{Naive NLL} & \multirow{2}{*}{EB} & ✓ & **-5.30 (0.00)** & \multirow{2}{*}{**0.44 (0.01)**} & **18.2 (0.3)** & **-5.34 (0.00)** & \multirow{2}{*}{**0.47 (0.01)**} & **18.3 (0.2)** \\  & & ✗ & -5.31 (0.01) & & & **-5.35 (0.01)** & & \\ \hline \multirow{3}{*}{Natural NLL} & \multirow{3}{*}{GS} & ✓ & -5.42 (0.00) & 0.55 (0.00) & 19.2 (0.1) & -5.40 (0.01) & 0.56 (0.01) & 24.3 (1.4) \\  & & ✗ & -5.42 (0.00) & & & -5.44 (0.01) & & \\ \cline{1-1} \cline{2-8}  & EB & ✓ & **-5.30 (0.01)** & \multirow{2}{*}{**0.45 (0.01)**} & \multirow{2}{*}{19.1 (0.2)} & **-5.34 (0.00)** & \multirow{2}{*}{**0.46 (0.00)**} & **18.1 (0.1)** \\ \cline{1-1} \cline{2-8}  & & ✗ & -5.32 (0.01) & & & **-5.35 (0.00)** & & \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance metrics on heteroscedastic image regression (Problem 2.1) on MNIST and FashionMNIST with MLP and CNN architectures, respectively. We report the mean and standard error and bold all numbers that are statistically indistinguishable from the best result. The bottom half shows the proposed methods. We find that the empirical Bayes (EB) procedure leads to better regularized and performing models than using grid search (GS). The posterior predictive also strictly improves the test log likelihood, albeit not as significantly.

#### Acknowledgements

AI gratefully acknowledges funding by the Max Planck ETH Center for Learning Systems (CLS). EP and AM were supported by a fellowship from the ETH AI Center.

## References

* Amemiya (1985) Takeshi Amemiya. _Advanced econometrics_. Harvard university press, 1985.
* Antoran et al. (2022) Javier Antoran, David Janz, James U Allingham, Erik Daxberger, Riccardo Rb Barbano, Eric Nalisnick, and Jose Miguel Hernandez-Lobato. Adapting the linearised laplace model evidence for modern deep learning. In _International Conference on Machine Learning_, pages 796-821. PMLR, 2022.
* Botev et al. (2017) Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep learning. In _International Conference on Machine Learning_, pages 557-565. PMLR, 2017.
* Bradbury et al. (2018) James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.
* Cawley et al. (2004) Gavin C Cawley, Nicola LC Talbot, Robert J Foxall, Stephen R Dorling, and Danilo P Mandic. Heteroscedastic kernel ridge regression. _Neurocomputing_, 57:105-124, 2004.
* Daxberger et al. (2021) Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and Philipp Hennig. Laplace redux-effortless bayesian deep learning. _Advances in Neural Information Processing Systems_, 34:20089-20103, 2021.
* Eschenhagen et al. (2021) Runa Eschenhagen, Erik Daxberger, Philipp Hennig, and Agustinus Kristiadi. Mixtures of laplace approximations for improved post-hoc uncertainty in deep learning. _arXiv preprint arXiv:2111.03577_, 2021.
* Foong et al. (2019) Andrew YK Foong, Yingzhen Li, Jose Miguel Hernandez-Lobato, and Richard E Turner. 'in-between'uncertainty in bayesian neural networks. _arXiv preprint arXiv:1906.11537_, 2019.
* Gal and Ghahramani (2016) Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In _International Conference on Machine Learning_, 2016.
* Graves (2011) Alex Graves. Practical variational inference for neural networks. In _Advances in Neural Information Processing Systems_, 2011.
* Guyon et al. (2019) Isabelle Guyon, Alexander Statnikov, and Berna Bakir Batu. _Cause effect Pairs in machine learning_. Springer, 2019.
* Harvey (1976) Andrew C Harvey. Estimating regression models with multiplicative heteroscedasticity. _Econometrica: Journal of the Econometric Society_, pages 461-465, 1976.
* Hernandez-Lobato and Adams (2015) Jose Miguel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of bayesian neural networks. In _International Conference on Machine Learning_. PMLR, 2015.
* Hornik et al. (1989) Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. _Neural networks_, 2(5):359-366, 1989.
* Houlsby et al. (2011) Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and Mate Lengyel. Bayesian active learning for classification and preference learning. _arXiv preprint arXiv:1112.5745_, 2011.
* Immer et al. (2021a) Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar Ratsch, and Khan Mohammad Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In _International Conference on Machine Learning_, pages 4563-4573. PMLR, 2021a.
* Immer et al. (2021b) Alexander Immer, Maciej Korzepa, and Matthias Bauer. Improving predictions of bayesian neural nets via local linearization. In _International Conference on Artificial Intelligence and Statistics_, pages 703-711. PMLR, 2021b.
* Immer et al. (2021c)Alexander Immer, Tycho van der Ouderaa, Gunnar Ratsch, Vincent Fortuin, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations. _Advances in Neural Information Processing Systems_, 35:12449-12463, 2022.
* Immer et al. (2023a) Alexander Immer, Christoph Schultheiss, Julia E Vogt, Bernhard Scholkopf, Peter Buhlmann, and Alexander Marx. On the identifiability and estimation of causal location-scale noise models. In _International Conference on Machine Learning_, pages 14316-14332. PMLR, 2023a.
* Immer et al. (2023b) Alexander Immer, Tycho FA Van Der Ouderaa, Mark Van Der Wilk, Gunnar Ratsch, and Bernhard Scholkopf. Stochastic marginal likelihood gradients using neural tangent kernels. In _International Conference on Machine Learning_, pages 14333-14352. PMLR, 2023b.
* Kendall and Gal (2017) Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? _Advances in neural information processing systems_, 30, 2017.
* E Khan et al. (2019) Mohammad Emtiyaz E Khan, Alexander Immer, Ehsan Abedi, and Maciej Korzepa. Approximate inference turns deep networks into gaussian processes. _Advances in neural information processing systems_, 32, 2019.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Kirsch (2023) Andreas Kirsch. Black-box batch active learning for regression. _arXiv preprint arXiv:2302.08981_, 2023.
* Krishnan et al. (2022) Ranganath Krishnan, Pi Esposito, and Mahesh Subedar. Bayesian-torch: Bayesian neural network layers for uncertainty estimation. https://github.com/IntelLabs/bayesian-torch, January 2022.
* Lakshminarayanan et al. (2017) Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. _Advances in neural information processing systems_, 30, 2017.
* Le et al. (2005) Quoc V Le, Alex J Smola, and Stephane Canu. Heteroscedastic gaussian process regression. In _International Conference on Machine Learning_, pages 489-496. PMLR, 2005.
* Lin et al. (2023) Jihao Andreas Lin, Javier Antoran, and Jose Miguel Hernandez-Lobato. Online laplace model selection revisited. _arXiv preprint arXiv:2307.06093_, 2023.
* MacKay (1995) David JC MacKay. Probable networks and plausible predictions-a review of practical bayesian methods for supervised neural networks. _Network: computation in neural systems_, 6(3):469, 1995.
* Martens (2020) James Martens. New insights and perspectives on the natural gradient method. _The Journal of Machine Learning Research_, 21(1):5776-5851, 2020.
* Martens and Grosse (2015) James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In _International conference on machine learning_, pages 2408-2417. PMLR, 2015.
* Nix and Weigend (1994) David A Nix and Andreas S Weigend. Estimating the mean and variance of the target probability distribution. In _IEEE international conference on neural networks_, volume 1, pages 55-60. IEEE, 1994.
* Osawa (2021) Kazuki Osawa. Automatic Second-Order Differentiation Library (ASDL), 2021. URL http://github.com/kazukiosawa/asdl.
* Osband et al. (2016) Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. _Advances in neural information processing systems_, 29, 2016.
* Paszke et al. (2017) Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In _NeurIPS Workshop Autodiff_, 2017.
* Rasmussen and Ghahramani (2000) Carl Rasmussen and Zoubin Ghahramani. Occam's razor. _Advances in neural information processing systems_, 13, 2000.
* Rasmussen et al. (2017)Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural networks. In _International Conference on Learning Representations_, volume 6, 2018.
* Seitzer et al. (2022) Maximilian Seitzer, Arash Tavakoli, Dimitrije Antic, and Georg Martius. On the pitfalls of heteroscedastic uncertainty estimation with probabilistic neural networks. In _International Conference on Learning Representations_, 2022.
* Skafte et al. (2019) Nicki Skafte, Martin J\(\o\)rgensen, and So ren Hauberg. Reliable training and estimation of variance networks. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Stirn and Knowles (2020) Andrew Stirn and David A Knowles. Variational variance: Simple, reliable, calibrated heteroscedastic noise variance parameterization. _arXiv preprint arXiv:2006.04910_, 2020.
* Stirn et al. (2023) Andrew Stirn, Harm Wessels, Megan Schertzer, Laura Pereira, Neville Sanjana, and David Knowles. Faithful heteroscedastic regression with neural networks. In _International Conference on Artificial Intelligence and Statistics_, pages 5593-5613. PMLR, 2023.
* Tomczak et al. (2018) Marcin B Tomczak, Siddharth Swaroop, and Richard E Turner. Neural network ensembles and variational inference revisited. In _1st Symposium on Advances in Approximate Bayesian Inference_, pages 1-11, 2018.
* Wen et al. (2018) Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient pseudo-independent weight perturbations on mini-batches. In _International Conference on Learning Representations_, 2018.
* Wilson (2020) Andrew Gordon Wilson. The case for bayesian deep learning. _arXiv preprint arXiv:2001.10995_, 2020.
* Wong-Toi et al. (2023) Eliot Wong-Toi, Alex Boyd, Vincent Fortuin, and Stephan Mandt. Understanding pathologies of deep heteroskedastic regression. _arXiv preprint arXiv:2306.16717_, 2023.
* Wooldridge (2015) Jeffrey M Wooldridge. _Introductory econometrics: A modern approach_. Cengage learning, 2015.
* Wu et al. (2019) Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard E. Turner, Jose Miguel Hernandez-Lobato, and Alexander L. Gaunt. Deterministic variational inference for robust bayesian neural networks. In _International Conference on Learning Representations_, 2019.
* Xu et al. (2022) Sascha Xu, Osman Mian, Alexander Marx, and Jilles Vreeken. Inferring cause and effect in the presence of heteroscedastic noise. In _International Conference on Machine Learning_, 2022.
* Yu et al. (2020) Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. _Advances in Neural Information Processing Systems_, 33:14129-14142, 2020.
* Yuan and Wahba (2004) Ming Yuan and Grace Wahba. Doubly penalized likelihood estimator in heteroscedastic regression. _Statistics & probability letters_, 69(1):11-20, 2004.

Appendix

## Table of Contents

* 1 Laplace with Mean and Variance Parametrization
* 2 Comparison of Gradient Updates
* 3 Orthogonal Mean-Variance Sources
* 4 Details on Experiments and Additional Results
	* 4.1 Experimental Details
		* 4.2.1 Skafte Illustrative Example
				* 4.3.1.1.2 UCI Regression and CRISPR-Cas13 Knockdown Efficacy Experiments
		* 4.3.2 Image Regression
		* 4.3.3 Image Regression
		* 4.3.4 Image Regression
* 5 Implementation
	* 5.1 Pseudo-Code for Our Method
	* 5.2 Implementation of Natural Gaussian Log Likelihood
Laplace with Mean and Variance Parametrization

In Sec. 4, we introduced the Laplace approximation for the naturally parameterized Gaussian likelihood. The natural parametrization allows an efficient generalized Gauss-Newton approximation to the Hessian due to the properties of the second derivative (Martens, 2020), i.e., that is positive definite w.r.t. the natural parameters. The positive definiteness is necessary so that the generalized Gauss-Newton, which pre and post-multiplies the Jacobians to this Hessian, can only be positive semidefinite. This is a necessary requirement for the Laplace approximation.

The Laplace approximation provides different useful capabilities for heteroscedastic regression: 1) automatic regularization through marginal likelihood optimization (empirical Bayes), 2) an approximation to the posterior predictive, and 3) epistemic uncertainties on the functional output. While we derived these in the context of the natural parametrization in the main text, it is also possible to obtain these benefits for the common mean-variance parametrization of a neural network.

Since the Hessian of the Gaussian likelihood under the mean-variance parametrization is not positive definite, we cannot directly use this parametrization for the Gauss-Newton approximation. To verify this statement, define and compute the Hessian of the Gaussian negative log likelihood w.r.t. \(\mathbf{m}\)

\[-\nabla_{\mathbf{m}}^{2}\log p(y|\mathbf{x},\bm{\theta})=\begin{bmatrix} \frac{1}{\sigma^{2}(\mathbf{x};\bm{\theta})}&\frac{y-\mu(\mathbf{x};\bm{ \theta})}{(\sigma^{2}(\mathbf{x};\bm{\theta}))^{2}}\\ \frac{y-\mu(\mathbf{x};\bm{\theta})}{(\sigma^{2}(\mathbf{x};\bm{\theta}))^{2} }&\frac{(\mu(\mathbf{x};\bm{\theta})-y)^{2}-\sigma^{2}(\mathbf{x};\bm{\theta })}{(\sigma^{2}(\mathbf{x};\bm{\theta}))^{3}}\end{bmatrix}\] (14)

By Silvester's criterion, a Hermitian matrix is positive definite if and only if the determinants of all sub-matrices are positive. For the Hessian in Equation 14, this criterion is not fulfilled as

\[\det\left(-\nabla_{\mathbf{f}}^{2}\log p(y|\mathbf{x},\bm{\theta})\right)=- \frac{1}{2(\sigma^{2}(\mathbf{x};\bm{\theta}))^{3}}<0\,.\]

A simple way to overcome this issue is to map the mean-variance parameters \(\mathbf{m}\) to the natural parameters \(\bm{\eta}\) using Equation 3 and apply the approximations as derived in Sec. 4, i.e.,

\[\eta_{1}(\mathbf{x};\bm{\theta})=\frac{\mu(\mathbf{x};\bm{\theta})}{\sigma^{ 2}(\mathbf{x};\bm{\theta})}\text{ and }\eta_{2}(\mathbf{x};\bm{\theta})=-\frac{1}{2\sigma^{ 2}(\mathbf{x};\bm{\theta})},\] (15)

and then apply the Gauss-Newton approximation as in Sec. 4 to obtain

\[-\nabla_{\mathbf{m}}^{2}\log p(y|\mathbf{x},\bm{\theta})\approx-J_{\bm{\eta} |\mathbf{m}}^{\top}\nabla_{\bm{\eta}}^{2}\log p(y|\mathbf{x},\bm{\theta})J_{ \bm{\eta}|\mathbf{m}}=\begin{bmatrix}\frac{1}{\sigma^{2}(\mathbf{x};\bm{ \theta})}&0\\ 0&\frac{1}{2(\sigma^{2}(\mathbf{x};\bm{\theta}))^{2}}\end{bmatrix},\] (16)

where the Jacobians \(\mathbf{J}_{\bm{\eta}|\mathbf{m}}\) are of \(\bm{\eta}\) w.r.t. \(\mathbf{m}\). It is easy to see that the Hessian approximation in Equation 16 is positive definite by evaluating the determinants of all sub-matrices.5 The implementation is very simple: we simply use the output of the neural network \(\mathbf{m}\), apply the transformation to the natural parameters \(\bm{\eta}\), and then compute the quantities as derived for the natural Laplace approximation.

Footnote 5: Interestingly, the individual determinants for the \(1\times 1\) and \(2\times 2\) sub-matrices are equal to the absolute value of the corresponding determinants of \(-\nabla_{\bm{t}}^{2}\log p(y|\mathbf{x},\bm{\theta})\) reminiscient of saddle-free methods.

## Appendix B Comparison of Gradient Updates

In the following, we first review the problem of the mean-variance parametrization identified by Seitzer et al. (2022) and present their proposed solution. Then we recap the proposal by Stirn et al. (2023), and subsequently explain how the natural parametrization avoids the gradient scaling problem.

The ProblemSeitzer et al. (2022) identify that a problem of previous neural estimators for heteroscedastic regression is due to the gradient of the negative log likelihood (NLL) which are parametrized to estimate the mean and standard deviation. First, recap that the corresponding NLL with respect to neural network parameters \(\bm{\theta}\) can be written as

\[\ell_{\mu,\sigma}(\bm{\theta})=\tfrac{1}{2}\log\sigma^{2}(\mathbf{x};\bm{ \theta})+\tfrac{(y-\mu(\mathbf{x};\bm{\theta}))^{2}}{2\sigma^{2}(\mathbf{x}; \bm{\theta})}+\mathit{const}.\] (17)As Seitzer et al. (2022) point out, the gradient of \(\ell_{\mu,\sigma}(\bm{\theta})\) with respect to \(\mu(\mathbf{x};\bm{\theta})\) is scaled by the estimated precision, i.e,

\[\nabla_{\mu}\ell_{\mu,\sigma}(\bm{\theta})=\tfrac{\mu(\mathbf{x};\bm{\theta})-y }{\sigma^{2}(\mathbf{x};\bm{\theta})},\ \ \nabla_{\sigma^{2}}\ell_{\mu,\sigma}(\bm{\theta})=\tfrac{\sigma^{2}( \mathbf{x};\bm{\theta})-(y-\mu(\mathbf{x};\bm{\theta}))^{2}}{2(\sigma^{2}( \mathbf{x};\bm{\theta}))^{2}}\] (18)

which they argue can lead to a poor mean estimate. In particular, they state that if the variance is well-calibrated, i.e., \(\sigma^{2}(\mathbf{x};\bm{\theta})\approx(\mu(\mathbf{x};\bm{\theta})-y)^{2}\), the gradient with respect to the mean reduces to

\[\nabla_{\mu}\ell_{\mu,\sigma}(\bm{\theta})\approx\tfrac{\mu(\mathbf{x};\bm{ \theta})-y}{\sigma^{2}(\mathbf{x};\bm{\theta})}=\tfrac{1}{\mu(\mathbf{x};\bm{ \theta})-y}\] (19)

which rewards points with an already accurate mean fit and undersamples points for which the mean fit is inaccurate Seitzer et al. (2022). We would like to note, however, that the same holds if the irreducible aleatoric uncertainty is low resp. high in a certain region, which does not necessarily imply a poor mean fit.

\(\beta\)-NllTo improve the mean fit, Seitzer et al. (2022) introduce the \(\beta\)-NLL loss

\[\ell_{\beta}(\bm{\theta})=\lfloor\sigma^{2\beta}(\mathbf{x};\bm{\theta}) \rfloor\left(\tfrac{1}{2}\log\sigma^{2}(\mathbf{x};\bm{\theta})+\tfrac{(y-\mu (\mathbf{x};\bm{\theta}))^{2}}{2\sigma^{2}(\mathbf{x};\bm{\theta})}+c\right)\] (20)

which introduces a stop-gradient operation denoted as \(\lfloor\cdot\rfloor\) to trade-off the influence of the variance estimate on the gradients. The corresponding gradients for the \(\beta\)-NLL loss are

\[\nabla_{\mu}\ell_{\beta}(\bm{\theta})=\tfrac{\mu(\mathbf{x};\bm{\theta})-y}{ \sigma^{2}-2\beta(\mathbf{x};\bm{\theta})},\ \ \nabla_{\sigma^{2}}\ell_{\beta}(\bm{\theta})=\tfrac{\sigma^{2}(\mathbf{x};\bm{ \theta})-(y-\mu(\mathbf{x};\bm{\theta}))^{2}}{2\sigma^{4-2\beta}(\mathbf{x}; \bm{\theta})}\.\] (21)

Thus, for \(\beta=1\), the gradient for the mean is proportional to the gradient for the homoscedastic objective (which does not hold for the gradient of the variance), for \(\beta=0\) it is the standard heteroscedastic gradient and for \(0<\beta<1\) the approach interpolates between both extremes. A special case appears at \(\beta=0.5\), where the gradient of the mean is weighted by the standard deviation.

Proposal of (Stirn et al., 2023)To approach the gradient scaling, Stirn et al. (2023) argue that a heteroscedastic regression model that learns \(\mu(\mathbf{x}),\sigma(\mathbf{x})\) should be faithful to a homoscedastic model that learns \(\mu_{0}(\mathbf{x})\). As faithfulness they define that (cf. Stirn et al. (2023, Def. 2))

\[\mathbb{E}_{(\mathbf{x},y)\sim\mathcal{D}}[(y-\mu_{0}(\mathbf{x}))^{2}]\geq \mathbb{E}_{(\mathbf{x},y)\sim\mathcal{D}}[(y-\mu(\mathbf{x}))^{2}]\.\] (22)

Figure 4: Illustration for the faithful heteroscedastic regression method proposed by Stirn et al. (2023). Functions parameterized by neural networks are color-coded in blue, backpropagated gradients are color-coded in violet, and the two black arrows only indicate that the values for \(\mu\) and \(\Sigma\) are used as input for loss computation. The two modifications to conventional heteroscedastic regression introduced by Stirn et al. (2023) are color-coded in orange: the introduction of the scaling of the gradient \(\nabla_{\mu}\ell_{\text{FF}}(\bm{\theta})\) by the covariance, and of a stop gradient operation to avoid \(\nabla_{\Sigma}\ell_{\text{FF}}(\bm{\theta})\) gradients being propagated to \(f_{\theta_{\bm{\pi}}}\). In this setup, specific dependencies for \(\Sigma\) on \(\mathbf{x}\) which are independent of the mean function are discarded, especially in the case in which \(\mathbf{z}\) acts as a bottleneck. Note that in this image we make the dependency of neural network functions on their respective parameters explicit. In the rest of the work we omit the \(\theta\) for the sake of brevity and readability.

To achieve this property, they propose to decouple the estimation into three networks, as shown in Figure 4: a shared representation learner \(f_{\bm{\pi}}\) computes a representation \(\bm{\mathrm{z}}\) from \(\bm{\mathrm{x}}\), which is passes into two individual networks \(f_{\mu}\) and \(f_{\Sigma}\), which receive \(\bm{\mathrm{z}}\) as input and output the mean respectively the covariance matrix. Based on this network architecture, they minimize the following loss

\[\ell_{\text{FF}}(\bm{\theta})=\tfrac{(y-\mu(\bm{\mathrm{x}};\bm{\theta}))^{2}} {2}-\log\mathcal{N}\left(y\right|\lfloor(f_{\mu}\circ f_{\bm{\mathrm{z}}})( \bm{\mathrm{x}})\rfloor,\Sigma(\lfloor f_{\theta_{z}}(\bm{\mathrm{x}})\rfloor) \right)\,\] (23)

which incorporates two stop-gradient operations. They show that the proposal is equivalent to the following two modifications, which are illustrated in Figure 4: First, they stop the gradient of \(f_{\Sigma}\) from properating to \(f_{\bm{\mathrm{z}}}\). Second, they propose to multiply the gradient of \(f_{\mu}\circ f_{\bm{\mathrm{z}}}\) with \(\Sigma\). In combination, both modifications imply that the gradient with respect to the mean is equivalent to the homoscedastic model (Stirn et al., 2023). As discussed in Sec. 2 and App. C, these modifications restrict the capacity of the joint network such that it cannot pick-up complex dependencies for the variance that do not affect the mean.

Last, the procedure has some commonalities to \(\beta\)-NLL. In particular, for the sub-networks \(f_{\bm{\mathrm{z}}}\) and \(f_{\mu}\) the gradients are multiplied with \(\Sigma\) which is the multivariate analogue of the effect of the \(\beta\)-NLL loss on the mean gradient for \(\beta=1\).

Proposed: Natural ParametrizationThe natural parametrization, that we use in this paper, has the following gradients with respect to \(\ell_{\bm{\eta}}(\bm{\theta})\) with respect to \(\eta_{1}\) and \(\eta_{2}\) as

\[\nabla_{\eta_{1}}\ell_{\bm{\eta}}(\bm{\theta})=-\tfrac{\eta_{1}(\bm{\mathrm{ x}};\bm{\theta})}{2\eta_{2}(\bm{\mathrm{x}};\bm{\theta})}-y,\ \ \nabla_{\eta_{2}}\ell_{\bm{\eta}}(\bm{\theta})=\tfrac{(\eta_{1}(\bm{\mathrm{x}}; \bm{\theta}))^{2}}{4(\eta_{2}(\bm{\mathrm{x}};\bm{\theta}))^{2}}-\tfrac{1}{2 \eta_{2}(\bm{\mathrm{x}};\bm{\theta})}-y^{2}\] (24)

Although these gradients do not have a direct implication on updates with respect to mean and variance, note that if we compute the mean estimate \(\mu(\bm{\mathrm{x}};\bm{\theta})\) from the estimated natural parameters, it is equal to \(-\tfrac{\eta_{1}(\bm{\mathrm{x}};\bm{\theta})}{2\eta_{2}(\bm{\mathrm{x}};\bm {\theta})}\). Similarly, note that the variance \(\sigma^{2}(\bm{\mathrm{x}};\bm{\theta})\) can be computed from the natural parametization as \(-\tfrac{1}{2\eta_{2}(\bm{\mathrm{x}};\bm{\theta})}\). If we replace the corresponding quantities in Equation 24, we get that

\[\nabla_{\eta_{1}}\ell_{\bm{\eta}}(\bm{\theta})=\mu(\bm{\mathrm{x}};\bm{\theta} )-y,\ \ \nabla_{\eta_{2}}\ell_{\bm{\eta}}(\bm{\theta})=\sigma^{2}(\bm{ \mathrm{x}};\bm{\theta})-(y^{2}-(\mu(\bm{\mathrm{x}};\bm{\theta}))^{2})\,\] (25)

which is desired according to Seitzer et al. (2022) and Stirn et al. (2023) since these are simply separate residuals for mean and variance. Empirically, we observe that this parametrization can be more stable to train and less prone to insufficient regularization, which is in line with previous work on Gaussian processes and ridge regression (Le et al., 2005; Immer et al., 2023).

## Appendix C Orthogonal Mean-Variance Sources

To illustrate the limitations of using a joint encoder network for mean and variance, which receives no gradients from the variance estimate (cf. (Stirn et al., 2023, _Faithful_)) we consider a toy setting in which mean and variance are generated from independent random variables:

**Problem C.1**: _Let \(\bm{\mathrm{X}}\in\mathbb{R}^{2}\) be a centered multivariate Gaussian distributed with \(\Sigma=I\). We construct the target variable \(Y\) as_

\[y=\sin(x_{1})+\nicefrac{{1}}{{2}}\left(\left|x_{2}\right|+0.2\right)\epsilon\]

_where \(\epsilon\sim\mathcal{N}(0,1)\) is an independent noise variable._

From the mechanism described in Problem C.1, we construct 20 datasets consisting of \(1000\) i.i.d. samples, where each dataset is generated using a different random seed. We split the data into train-validation/test according to the ratio (0.9/0.1). Then the train-validation data are split into training and validation sets (0.9/0.1).

As baselines we consider our empirical Bayes training with the natural homoscedastic and heteroscedastic objective using only a point predictive,both trained with the full Laplace approximation, where we use an MLP with 1 hidden-layer consisting of 100 hidden units. For _Faithful_, we use the same MLP with 1 hidden layer for the joint encoder \(f_{\bm{\mathrm{z}}}\) and instantiate \(f_{\mu}\) and \(f_{\Sigma}\) as linear projections from dimension \(\left|\bm{\mathrm{z}}\right|\) to 1. We ablate the size of the hidden dimension \(\left|\bm{\mathrm{z}}\right|\) from 1 to 128 neurons to create a bottleneck. In theory, a size of 2 should be sufficient to encode both mean and variance.

As shown in Figure 5,the bottleneck size has an influence on how much information about the variance is propagated, however, a large bottleneck size does not guarantee that the network learnsthe relevant information about the variance. We rather conjecture that the amount of information that is propagated is dependent on the initialization of the network, since no gradient actively encourages the network \(f_{\mathbf{z}}\) to pick up information about \(X_{2}\), which influences the variance. Since for a larger bottleneck size the network can encode much more information than just \(X_{1}\), we observe that it encodes information about \(X_{2}\) more consistently, and for some seeds (see Figure 4(b)) it performs comparably well with our heteroscedastic variant. On average, however, its performance lies, as expected, between the homoscedastic and heteroscedastic models with natural parametrization trained with the full Laplace approximation.

## Appendix D Details on Experiments and Additional Results

In this section, we first provide technical details for our experimental comparisons, and then show some additional results for our experiments.

### Experimental Details

#### d.1.1 Skafte Illustrative Example

In Figure 1, we compare a homoscedastic to a heteroscedastic likelihood and the point-wise prediction to the Laplace-approximated posterior predictive. The dataset is created similar to Skafte et al. (2019) with the following generative model:

\[y=\sin(x)+(0.1+|0.5x|)\varepsilon\quad\text{with}\quad x\sim\text{Unif}[2.5,12. 5]\quad\text{and}\quad\varepsilon\sim\mathcal{N}(0,1).\] (26)

For training, we center the input data by subtracting the mean \(7.5\) from \(x\) and standardize the observations to zero mean and unit variance as common for regression problems. We create a dataset with \(1000\) samples.

We model the data with a neural network with \(100\) hidden units on a single hidden layer using tanh activation. Both homoscedastic and heteroscedastic models are trained using the marginal-likelihood optimization algorithm (Immer et al., 2021), also detailed in Alg. 1. We train the neural network parameter using learning rate \(0.01\) with Adam (Kingma and Ba, 2014) and optimize the hyperparameters using learning rate \(0.1\) with the same optimizer every \(50\) epochs for \(50\) gradient steps. In this example, we use the efficient Kronecker-factored Laplace approximation and use the model that obtained the best marginal likelihood approximation during training, akin to early stopping.

#### d.1.2 UCI Regression and CRISPR-Cas13 Knockdown Efficacy Experiments

For both UCI and CRISPR regression datasets all compared models use an MLP with 1 hidden layer, with width of \(50\) units. We use the GeLU activation function for models trained on the UCI regression datasets, while we adopt the ReLU activation function for models trained on the CRISPR datasets. We train all models, except for the VI and MC-Dropout baselines, with Adam optimizer using a

Figure 5: Visualization of the results for data generated according to Problem C.1, where only _Faithful_ is ablated with respect to the bottleneck size. Left: Average test log likelihood for each of the compared models, with standard error as error bars. Right: Best test log likelihood result achieved across seeds for each of the models.

batch size of \(256\) for \(5000\) epochs and an initial learning rate of \(10^{-2}\) that is decayed to \(10^{-5}\) using a cosine schedule. As we observe that gives overall better performance, MC-Dropout and VI on UCI datasets are trained for 1000 epochs, with a learning rate of \(10^{-3}\) that is not decayed. In the models using classic grid-search (GS) for hyperparameter tuning the grid-search procedure is used to tune the prior precision, i.e., weight decay, hyperparameter. On the CRISPR datasets, VI is trained for 500 epochs with a learning rate of \(10^{-3}\). For all methods using grid-search, we first split the training data into a 90/10 train-validation split. We then train the model for each order of magnitude of the prior precision from \(10^{-3}\) to \(10^{5}\). The best prior precision is selected on the validation data and the model is retrained on the entire combined training dataset and assessed on the test data. The observation noise for the homoscedastic model is chosen on the training data as maximum likelihood solution. With the Empirical Bayes (EB) training approach, we only need to fit the model once on the entire training data and optimize the prior precision values per layer of the neural network during training. We use the algorithm in Immer et al. (2021) with a hyperparameter learning rate of \(0.01\) decayed to \(0.001\) using a cosine schedule, \(100\) burn-in epochs, every \(50\) epochs for \(50\) hyperparameter steps. The pseudocode for the optimization is provided in App. E.1. For the UCI regression datasets, we use the full Hessian Laplace approximation, while for CRISPR dataset, due to higher computational cost of training, we resort to the Kronecker-factored Laplace approximation. Finally for MC-Dropout, we set the dropout probability for dropout layers to 0.005. For all compared models, results are averaged over 20 independent runs on the UCI regression datasets and 10 independent runs on the CRISPR datasets, where independent runs use different seeds. We report standard errors. Runs were executed on a computing cluster, but without the need for GPU support. Note that in our experiments to implement the VI baseline we make use of the PyTorch implementation from Krishnan et al. (2022).

#### d.1.3 Image Regression

We use the same hyperparameters for MNIST and FashionMNIST but two different architectures. The architecture for MNIST is an MLP with 3 hidden layers, each with width of \(500\) units and using ReLU activation function. The architecture for FashionMNIST is a LeNet with three convolutional layers with \(6,32,256\) channels, respectively, with max-pooling in-between followed by two hidden layers of width \(128\) and \(64\). In all cases except for VI and MC-Dropout, we train the models with SGD using a batch size of \(512\) for \(500\) epochs and initial learning rate of \(0.1\) that is decayed to \(10^{-4}\) using a cosine schedule. For MC-Dropout, we use a learning rate of \(0.01\) and train the model for \(50\) epochs with the Adam optimizer, with dropout probability for Dropout layers of \(0.005\). For VI, we use a learning rate of \(10^{-2}\) that is decayed to \(10^{-4}\) using a cosine schedule when training on the FashionMNIST dataset, while we use a non-decayed learning rate of \(10^{-2}\) for the MNIST dataset. In both cases, the VI baseline is trained for \(500\) epochs with the Adam optimizer. In practice, we observe that the VI and MC-Dropout baselines are more sensitive to choices of hyperparameters than the other compared models, and we adopted the settings that would lead to the best results according to our experiments. In the models using classic grid-search (GS) for hyperparameter tuning (GS) the grid-search procedure is used to tune the prior precision, i.e., weight decay, hyperparameter. For all methods using grid-search, we first split the training data into a 90/10 train-validation split. We then train the model for each order of magnitude of the prior precision from \(10^{-2}\) to \(10^{4}\). The best prior precision is selected on the validation data and the model is retrained on the entire combined training dataset and assessed on the test data. The observation noise for the homoscedastic model is chosen on the training data as maximum likelihood solution. With the Empirical Bayes (EB) training approach, we only need to fit the model once on the entire training data and optimize the prior precision values per layer of the neural network during training. We use the algorithm in Immer et al. (2021) with a hyperparameter learning rate of \(0.1\), \(50\) burn-in epochs, every \(25\) epochs for \(50\) hyperparameter steps. These choices are in line with their suggestions. Due to the large models on the image datasets, we use the Kronecker-factored Laplace approximation so that the training and inference time between Laplace and baselines are almost identical. The training was done \(5\) times (different seeds) per model-dataset pair to estimate mean and standard error and were run on a computing cluster with V100 and A100 NVIDIA GPUs.

### Additional Experimental Results

#### d.2.1 Skafte Illustrative Example

In Figure 7, we additionally compare the proposed approach to Bayesian neural network baselines, MC-dropout and mean-field VI. We find that the proposed heteroscedastic Laplace approximation with natural parameterization provides more visually appealing in-between epistemic uncertainties and has a more sensible split into aleatoric and epistemic uncertainties. In Figure 6, we display the difference when the generated data were homoscedastic: despite this, our well-regularized heteroscedastic method performs on par or better with a homoscedastic baseline trained in an identical way.

#### d.2.2 UCI Regression Datasets

In this section, we report additional results to the ones we show in Sec. 5.1 for UCI regression datasets. In Table 3 we report test Mean Squared Error (MSE) results for the models compared in Table 1, while Table 4 reports an ablation where for our proposed natural parameterized NLL heteroscedastic loss model EB regularization, comparing results obtained with the full Hessian Laplace approximation (_full_) and results obtained with the Kronecker-factored Laplace approximation (_kfac_). Results show that the full approximation can improve results, and thus should be preferred when feasible. At the same time, the _kfac_ approximation leads to comparable results in most settings, justifying its use for larger datasets. In Table Table 5, we compare the results obtained with two different variants of posterior predictive for the Natural NLL (EB) loss on the UCI regression datasets, namely the closed-form posterior predictive proposed in Sec. 4.4 (_mean_) and the asymptotically exact Monte-Carlo estimate of the NLL (Tomczak et al., 2018) (_lse_). Finally Table 6 shows that the results from our work for models trained with EB regularization favourably compare to results from the work of Wu et al. (2019) on a deterministic variational inference approach for BNNs that also adopts an EB procedure for regularization. Note however, that this last approach does not scale well to more complex architectures than an MLP and hence is not reported in the other experimental settings.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{3}{*}{Objective} & \multirow{3}{*}{\begin{tabular}{c} Regular- \\ ization \\ \end{tabular} } & \multirow{3}{*}{
\begin{tabular}{c} Posterior \\ Predictive \\ \end{tabular} } & \multicolumn{1}{c}{LL (\(\uparrow\))} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \cline{3-10}  & & & & & & & & & & & & \\ \hline Homoscedastic & EB & ✓ & -2.51 (0.04) -3.03 (0.03) -0.71 (0.04) & 1.29 (0.01) & 5.58 (0.45) & -2.83 (0.02) -0.94 (0.02) -1.05 (0.14) \\ Deterministic VI & EB & ✓ & -2.41 (0.02) -3.06 (0.01) -1.01 (0.06) & 1.13 (0.00) & 6.29 (0.04) & -2.80 (0.00) -0.90 (0.01) -0.47 (0.03) \\ Naive NLL & EB & ✓ & -2.47 (0.04) -2.88 (0.03) -0.46 (0.03) & 1.37 (0.01) & 6.38(0.16) -2.79 (0.02) -0.93 (0.02) -0.05 (0.15) \\ \multirow{2}{*}{Naive NLL} & EB & ✗ & -6.03 (1.82) -3.62 (0.26) -0.97 (0.15) & 1.36 (0.01) -10.9 (0.81) -2.83 (0.04) -2.26 (0.51) -1.27 (0.51) \\  & & ✓ & -2.36 (0.03) -2.93 (0.02) -0.71 (0.03) & 1.36 (0.01) & 6.66 (0.01) -2.76 (0.02) -0.94 (0.02) -0.51 (0.04) \\ \multirow{2}{*}{Naive NLL} & EB & ✗ & -2.51 (0.09) -2.99 (0.04) -0.65 (0.05) & 1.35 (0.01) & 6.68 (0.01) -2.77 (0.02) -1.00 (0.03) -0.60 (0.08) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison of our models trained with EB regularization, with the work of Wu et al. (2019), that also adopts an EB regularization procedure.

Figure 7: Comparison of heteroscedastic Laplace (right) to Monte-Carlo Dropout (Gal and Ghahramani, 2016), mean-field variational inference, and homoscedastic Laplace on the Skafte (Skafte et al., 2019) regression example. Laplace obtains better “in-between” uncertainties as reported also by Foong et al. (2019).

#### d.2.3 CRISPR Gene Knockdowndown Efficacy Datasets

Here we report additional results for the compared models in Sec. 5.2 on the CRISPR datasets for gene knockdown efficacy, reporting results for models trained on single experiment replicates. Therefore, we report test log likelihood performance for models trained on the available replicates for each of the three CRISPR datasets in Figure 8. Again, for each model in each dataset we run 10 independent seeds. As it is expected, these results are in line with, and complement, the ones shown in Figure 3.

#### d.2.4 Image Regression

In Table 9, we additionally provide results with the exact same training setting as for Table 2 but with a homoscedastic generative process. In Table 8, we provide results for the same task but with heteroscedastic noise that depends on the mean output, i.e., the rotational angle. Overall, the pattern is the same: empirical Bayes and the posterior predictive strictly improve generalization performance and the natural parameterization seems to overall perform best and most consistently. In Table 7, we report the runtimes for training (EB vs GS) and testing (MAP vs. PP). Training with EB is much faster than using a grid search but the posterior predictive requires more runtime per sample.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{3}{*}{Objective} & Regular- & \multirow{3}{*}{Approximation} & Posterior & \multicolumn{5}{c}{LL (\(\uparrow\))} \\  & & & Predictive & _boston_ & _concrete_ & _energy_ & _kinSum_ & _naval_ & _plant_ & _wine_ & _yacht_ \\ \hline \multirow{3}{*}{Natural NLL} & \multirow{3}{*}{EB} & kfac & & -2.44 (0.04) - 3.05 (0.02) - 0.94 (0.02) & 1.36 (0.01) & 6.64 (0.01) - 2.76 (0.02) - 0.94 (0.02) - 0.69 (0.05) \\  & & full & & -2.36 (0.03) - 2.93 (0.02) - 0.71 (0.03) & 1.36 (0.01) & 6.66 (0.01) - 2.76 (0.02) - 0.94 (0.02) - 0.51 (0.04) \\  & & ffac & & -2.51 (0.07) - 3.04 (0.03) - 0.83 (0.03) & 1.35 (0.01) & 6.69 (0.01) - 2.76 (0.02) - 5.41 (3.54) - 0.64 (0.05) \\  & & full & & -2.51 (0.09) - 2.99 (0.04) - 0.65 (0.05) & 1.35 (0.01) & 6.68 (0.01) - 2.77 (0.02) - 1.00 (0.03) - 0.60 (0.08) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablation comparing test log likelihood results for the natural parametrization with empirical Bayes (_Natural Laplace_) adopting the full Hessian Laplace approximation (_full_) and the Kronecker-factored Laplace approximation (_ffac_).

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{3}{*}{Objective} & Regular- & \multirow{3}{*}{_boston_} & \multicolumn{5}{c}{MSE (\(\downarrow\))} \\  & & & & & & & & & & \\  & & _location_ & _concrete_ & _energy_ & _kinSum_ & _naval_ & _plant_ & _wine_ & _yacht_ \\ \hline Homoscedastic & EB & 0.130 (0.012) & 0.096 (0.005) & 0.002 (0.000) & 0.064 (0.001) & 0.001 (0.000) & 0.057 (0.002) & 0.606 (0.024) & 0.002 (0.000) \\ Naive NLL & GS & 0.170 (0.020) & 0.130 (0.013) & 0.009 (0.005) & 0.072 (0.001) & 5.673 (5.216) & 0.056 (0.002) & 0.605 (0.022) & 0.003 (0.000) \\ \(\beta\)-NLL (\(\circ\)) GS & GS & 0.131 (0.013) & 0.122 (0.007) & 0.002 (0.000) & 0.069 (0.001) & 0.2693 (3.367) & 0.055 (0.002) & 0.608 (0.023) & 0.002 (0.001) \\ \(\beta\)-NLL (\(\circ\)) GS & GS & 0.144 (0.012) & 0.108 (0.009) & 0.003 (0.001) & 0.069 (0.001) & 0.003 (0.000) & 0.057 (0.002) & 0.609 (0.022) & 0.000 (0.000) \\ Faith & GS & 0.140 (0.015) & 0.097 (0.007) & 0.004 (0.001) & 0.070 (0.001) & 0.003 (0.001) & 0.055 (0.002) & 0.616 (0.023) & 0.002 (0.001) \\ MC-Dropout & GS & 0.131 (0.016) & 0.131 (0.006) & 0.057 (0.033) & 0.117 (0.004) & 0.145 (0.011) & 0.059 (0.002) & 0.606 (0.025) & 0.124 (0.038) \\ VI & GS & 0.131 (0.016) & 0.134 (0.005) & 0.056 (0.04) & 0.047 (0.002) & 0.146 (0.009) & 0.058 (0.002) & 0.645 (0.037) & 0.020 (0.012) \\ \hline Naive NLL & EB & 0.136 (0.016) & 0.100 (0.006) & 0.002 (0.000) & 0.065 (0.001) & 0.027 (0.017) & 0.055 (0.002) & 0.621 (0.027) & 0.003 (0.001) \\ \hline \multirow{3}{*}{Natural NLL} & GS & 0.111 (0.010) & 0.081 (0.008) & 0.002 (0.000) & 0.067 (0.001) & 0.001 (0.000) & 0.051 (0.002) & 0.612 (0.022) & 0.003 (0.001) \\  & EB & 0.102 (0.011) & 0.094 (0.007) & 0.002 (0.000) & 0.061 (0.001) & 0.001 (0.000) & 0.052 (0.002) & 0.652 (0.046) & 0.003 (0.000) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Test MSE results for the compared models on the UCI regression datasets, where we report mean and standard error (dataset names in italic).

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{3}{*}{**Method**} & Regular- & Posterior & \multicolumn{5}{c}{LL (\(\uparrow\))} \\  & & Predictive & _boston_ & _concrete_ & _energy_ & _kinSum_ & _naval_ & _plant_ & _wine_ & _yacht_ \\ \hline \multirow{3}{*}{Natural NLL} & GS & mean & -2.45 (0.04) - 2.92 (0.04) - 0.73 (0.05) & 1.32 (0.01) & 6.66 (0.01) - 2.76 (0.02) - 0.94 (0.02) - 0.29 (0.07) \\  & & lse & -2.46 (0.04) - 2.92 (0.04) - 0.74 (0.06) & 1.32 (0.01) & 6.66 (0.01) - 2.76 (0.02) - 0.94 (0.02) - 0.29 (0.07) \\ Natural NLL & EB & mean & -2.39 (0.04) - 2.92 (0.03) - 0.71 (0.03) & 1.36 (0.01) & 6.66 (0.01) - 2.76 (0.02) -0.94 (0.02) - 0.51 (0.04) \\  & & lse & -2.36 (0.03) - 2.93 (0.02) - 0.71 (0.03) & 1.36 (0.01) & 6.66 (0.01) - 2.76 (0.02) -0.94 (0.02) - 0.51 (0.04) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Ablation comparing the two variants of posterior predictive for Natural NLL objective with Empirical Bayes regularization: _mean_ and _lse_.

## Appendix E Implementation

In the following, we detail the interleaved Laplace marginal likelihood optimization (Immer et al., 2021) but adapted to the heteroscedastic setting in App. E.1. Further, we provide an efficient and stable implementation of the natural parametrization of the heteroscedastic Gaussian likelihood in App. E.2. We implement our method in the laplace-torch package (Daxberger et al., 2021) and the KFAC for heteroscedastic regression in the automatic second-order differentiation library (asdl; Osawa, 2021). Due to anonymity reasons, we implemented these changes as private forks and will only propose them to the open source packages upon publication of the paper. This ensures that the proposed method can be used with any supported network by these packages and different scalable Laplace approximation variants.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \(\beta\)-NLL & Homoscedastic GS + PP & Homoscedastic EB + PP & Natural GS + PP & Natural EB + PP \\ \hline Training & 44 min. & 44 min. & 8 min. & 49 min. & 9 min. \\ Inference & 0.023 sec. & 0.124 sec. & 0.124 sec. & 0.134 sec. & 0.134 sec. \\ \hline \hline \end{tabular}
\end{table}
Table 7: Average runtime over 5 trials including hyperparameter optimization.

Figure 8: Box-plots reporting test log likelihood results on the CRISPR datasets on single experiment replicated. For the survival-screen-A375 dataset, only two replicates are available. Note that the NLL mean-variance parameterization with MAP prediction, as well as the MC-Dropout baseline, markedly underperform in this setting, and are hence not reported in the comparisons.

### Pseudo-Code for Our Method

Alg. 1 is the heteroscedastic regression variant of the algorithm proposed by Immer et al. (2021) and includes the empirical Bayes hyperparameter optimization of the prior precision, or equivalently weight-decay, \(\delta\) per layer. The algorithm optimizes neural network parameters \(\bm{\theta}\) and hyperparameters \(\delta\) jointly during training and further the intermediate marginal likelihood values, \(\log p(\mathcal{D}|\bm{\delta})\) can be used for early stopping. After training, either the Bayesian or point-wise posterior predictive can be used. The posterior predictive is efficient to compute and does not require sampling due to the simplification proposed in Sec. 4.4. Experimentally, we find that the Bayesian posterior predictive improves over the point-wise predictive in almost all cases.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & & & \multicolumn{3}{c}{MNIST with MLP} & \multicolumn{3}{c}{FashionMNIST with CNN} \\ \cline{4-9} Objective & Regular- & Posterior & LL (\(\uparrow\)) & \(D_{\text{KL}}\) (\(\downarrow\)) & RMSE (\(\downarrow\)) & LL (\(\uparrow\)) & \(D_{\text{KL}}\) (\(\downarrow\)) & RMSE (\(\downarrow\)) \\ \hline Homoscedastic & EB & ✓ & -4.02 (0.10) & 3.75 (1.92) & 11.9 (0.2) & -3.87 (0.02) & 1.37 (0.08) & **12.5 (0.2)** \\ Naive NLL & GS & ✗ & -4.56 (0.25) & 1.39 (0.25) & 29.7 (7.4) & -4.06 (0.16) & 0.88 (0.15) & 12.7 (0.2) \\ \(\beta\)-NLL (0.5) & GS & ✗ & -4.30 (0.16) & 1.13 (0.17) & 16.4 (1.9) & -4.01 (0.02) & 0.84 (0.02) & 15.0 (0.1) \\ \(\beta\)-NLL (1) & GS & ✗ & -4.21 (0.09) & 1.04 (0.09) & 15.0 (1.2) & -4.05 (0.04) & 0.88 (0.03) & **12.3 (0.1)** \\ FaithI & GS & ✗ & -4.17 (0.04) & 0.99 (0.04) & 14.1 (0.5) & -4.24 (0.05) & 1.25 (0.05) & 17.8 (1.6) \\ MC-Dropout & GS & ✓ & -4.14 (0.05) & 0.97 (0.05) & 15.9 (0.36) & -3.08 (0.05) & **0.61 (0.05)** & 15.5 (1.1) \\ VI & GS & ✓ & -4.15 (0.01) & 0.98 (0.01) & 13.9 (0.1) & -3.86 (0.03) & 0.69 (0.03) & 16.3 (0.7) \\ \hline Naive NLL & EB & ✓ & -3.85 (0.02) & **0.75 (0.01)** & 12.2 (0.3) & **-3.66 (0.01)** & **0.60 (0.02)** & 13.1 (0.3) \\  & ✗ & -3.92 (0.02) & **0.75 (0.01)** & 12.2 (0.3) & -3.77 (0.02) & **0.60 (0.02)** & 13.1 (0.3) \\ \hline \multirow{4}{*}{Natural NLL} & GS & ✓ & -4.16 (0.01) & 0.99 (0.01) & 14.5 (0.1) & **-3.66 (0.01)** & **0.61 (0.02)** & 13.2 (0.4) \\  & ✗ & -4.16 (0.01) & 0.99 (0.01) & 14.5 (0.1) & -3.78 (0.02) & **0.61 (0.02)** & 13.2 (0.4) \\ \cline{2-9}  & EB & ✓ & **-3.73 (0.01)** & **0.76 (0.02)** & **11.0 (0.2)** & -3.66 (0.01)** & **0.61 (0.01)** & 13.1 (0.2) \\ \cline{1-1} \cline{2-9}  & & ✗ & -3.94 (0.02) & **0.76 (0.02)** & **11.0 (0.2)** & -3.78 (0.01) & **0.61 (0.01)** & 13.1 (0.2) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Image regression with heteroscedastic noise variance that depends on the rotation applied, i.e., \(y\sim\mathrm{rot}+\sqrt{|\mathrm{rot}|}\varepsilon\). Similar to the results with heteroscedastic label-based noise in Table 2, empirical Bayes and the posterior predictive improve the performance consistently.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & & & \multicolumn{3}{c}{MNIST with MLP} & \multicolumn{3}{c}{FashionMNIST with CNN} \\ \cline{4-9} Objective & Regular- & Posterior & LL (\(\uparrow\)) & \(D_{\text{KL}}\) (\(\downarrow\)) & RMSE (\(\downarrow\)) & LL (\(\uparrow\)) & \(D_{\text{KL}}\) (\(\downarrow\)) & RMSE (\(\downarrow\)) \\ \hline Homoscedastic & EB & ✓ & -4.15 (0.02) & 0.86 (0.07) & 12.8 (0.4) & -4.06 (0.01) & 0.64 (0.02) & **12.7 (0.2)** \\ Naive NLL & GS & ✗ & -5.38 (0.00) & 1.66 (0.00) & 51.8 (0.0) & -4.18 (0.03) & 0.45 (0.03) & **12.8 (0.2)** \\ \(\beta\)-NLL (0.5) & GS & ✗ & -4.34 (0.10) & 0.62 (0.10) & 16.0 (1.5) & -4.27 (0.02) & 0.54 (0.01) & 15.5 (0.1) \\ \(\beta\)-NLL (1) & GS & ✗ & -4.31 (0.05) & 0.59 (0.05) & 14.8 (0.8) & -4.26 (0.04) & 0.53 (0.03) & **12.6 (0.3)** \\ FaithI & GS & ✗ & -4.31 (0.03) & 0.58 (0.03) & 14.6 (0.5) & -4.57 (0.01) & 0.84 (0.01) & 21.0 (0.4) \\ MCDO & GS & ✓ & -4.29 (0.04) & 0.56 (0.04) & 16.1 (0.9) & -4.08 (0.02) & **0.36 (0.02)** & 15.5 (1.1) \\ VI & GS & ✓ & -4.30 (0.01) & 0.58 (0.01) & 14.5 (0.1) & -4.10 (0.01) & 0.38 (0.01) & 15.0 (0.3) \\ \hline \multirow{4}{*}{Naive NLL} & EB & ✓ & -4.09 (0.01) & 0.42 (0.02) & 12.7 (0.1) & -4.02 (0.01) & **0.33 (0.01)** & 13.4 (0.2) \\  & ✗ & -4.14 (0.02) & 0.42 (0.02) & 12.7 (0.1) & -4.06 (0.01) & **0.33 (0.01)** & 13.4 (0.2) \\ \cline{1-1} \cline{2-9}  & GS & ✓ & -4.26 (0.00) & 0.54 (0.00) & 14.3 (0.1) & **-3.99 (0.01)** & **0.33 (0.01)** & **13.0 (0.2)** \\ \cline{1-1} \cline{2-9}  & EB & ✓ & **-4.26 (0.00)** & 0.54 (0.00) & 14.3 (0.1) & -4.06 (0.01) & **0.33 (0.01)** & 13.0 (0.2) \\ \cline{1-1} \cline{2-9}  & EB & ✓ & **-4.06 (0.01)** & **0.38 (0.00)** & **12.0 (0.1)** & **-4.06 (0.01)** & **0.33 (0.01)** & 13.2 (0.2) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Image regression with homoscedastic noise variance that is fixed at scale of \(10\). Similar to the results with heteroscedastic label-based noise in Table 2, empirical Bayes and the posterior predictive improve the performance consistently. Also, homoscedastic and \(\beta\)-NLL variants perform relatively well due to the reduced heteroscedasticity.

```

```
0:model\(\mathbf{f}(\mathbf{x};\bm{\theta})\) (natural parameters), dataset\(\mathcal{D}\), (initial) prior precision\(\delta\), epochs\(E\), burn-in\(B\), frequency\(F\), steps\(S\), learning rate\(\alpha\), hyperparameter learning rate\(\gamma\)\(\forall l:\delta_{l}\leftarrow\delta\)\(\triangleright\) Set prior precision (regularizer) for each layer for epoch\(e\leq E\)do for batch\(\mathcal{B}\subseteq\mathcal{D}\)do\(\triangleright\) Optimize model parameters \(\bm{\theta}\)\(\bm{\theta}\leftarrow\bm{\theta}+\alpha\nabla_{\bm{\theta}}|\frac{|\mathcal{D} |}{|\mathcal{B}|}\log p(\mathcal{B}|\bm{\theta})+\sum_{l}\log\mathcal{N}(\bm{ \theta}_{l}|\bm{0},\delta_{l}\mathbf{I})|\)\(\triangleright\) SGD parameter update endfor if\(e\mod F=0\) and \(e\geq B\)then\(\triangleright\) Optimize regularization \(\bm{\delta}\) for step\(s\leq S\)do\(\forall l:\delta_{l}\leftarrow\delta_{l}+\frac{\partial}{\partial\delta_{l}}\log p( \mathcal{D}|\bm{\delta})\) (Equation 12)\(\triangleright\) Hyperparameter update endfor endif endfor if(Bayesian) posterior predictive then  Compute\(\mu(\mathbf{x}_{*}),\sigma^{2}(\mathbf{x}_{*})\) as in Equation 13\(\triangleright\) Fast approximate posterior predictive else\(\mu(\mathbf{x}_{*})\leftarrow-\frac{\mathrm{f}_{1}(\mathbf{x}_{*};\bm{ \theta})}{2\mathrm{f}_{2}(\mathbf{x}_{*};\bm{\theta})}\) and \(\sigma^{2}(\mathbf{x}_{*})\leftarrow-\frac{1}{2\mathrm{f}_{2}(\mathbf{x}_{*}; \bm{\theta})}\)\(\triangleright\) Point-wise MAP predictive endif  Predictive \(\mathcal{N}(\mu(\mathbf{x}_{*}),\sigma^{2}(\mathbf{x}_{*}))\) ```

**Algorithm 1** Optimization of Heteroscedastic Regression Models

### Implementation of Natural Gaussian Log Likelihood

Common automatic differentiation packages like pytorch (Paszke et al., 2017) and jax (Bradbury et al., 2018) do include typical losses and distributions but not the natural parametrization of a Gaussian. Below, we provide an implementation for it using pytorch:

```

```
0:frommathimportlog,pi importtorch C=-0.5*log(2*pi) defheteroscedastic_mse_loss(input,target,reduction='mean'):"""HeteroscedasticNormalnegativeloglikelihood Parameters input:torch.Tensor(n,2) twonaturalparametersperdatapoint target:torch.Tensor(n,1) targets assertinput.ndim==target.ndim==2 assertinput.shape[0]==target.shape[0] n,_=input.shape target=torch.cat([target,target.square()],dim=1) inner=torch.einsum('nk,nk->n',target,input) log_A=input[:,0].square()/(4*input[:,1]) +0.5*torch.log(-2*input[:,1]) log_lik=n*C+inner.sum()+log_A.sum() ifreduction=='mean': return-log_lik/n elifreduction=='sum': return-log_lik else: raiseValueError('Invalidreduction',reduction) ```