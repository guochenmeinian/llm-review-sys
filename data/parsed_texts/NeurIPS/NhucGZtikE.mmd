# Deep Learning Through A Telescoping Lens:

A Simple Model Provides Empirical Insights On

Grokking, Gradient Boosting & Beyond

 Alan Jeffares

University of Cambridge

aj659@cam.ac.uk &Alicia Curth

University of Cambridge

amc253@cam.ac.uk &Mihaela van der Schaar

University of Cambridge

mv472@cam.ac.uk

Equal contribution

###### Abstract

Deep learning sometimes _appears to_ work in unexpected ways. In pursuit of a deeper understanding of its surprising behaviors, we investigate the utility of a simple yet accurate model of a trained neural network consisting of a sequence of first-order approximations _telescoping_ out into a single empirically operational tool for practical analysis. Across three case studies, we illustrate how it can be applied to derive new empirical insights on a diverse range of prominent phenomena in the literature - including double descent, grokking, linear mode connectivity, and the challenges of applying deep learning on tabular data - highlighting that this model allows us to construct and extract metrics that help predict and understand the a priori unexpected performance of neural networks. We also demonstrate that this model presents a pedagogical formalism allowing us to isolate components of the training process even in complex contemporary settings, providing a lens to reason about the effects of design choices such as architecture & optimization strategy, and reveals surprising parallels between neural network learning and gradient boosting.

## 1 Introduction

Deep learning _works_, but it sometimes works in mysterious ways. Despite the remarkable recent success of deep learning in applications ranging from image recognition [17] to text generation [2], there remain many contexts in which it performs in apparently unpredictable ways: neural networks sometimes exhibit surprisingly non-monotonic generalization performance [1, 18], continue to be outperformed by gradient boosted trees on tabular tasks despite successes elsewhere [19], and sometimes behave surprisingly similarly to linear models [16]. The pursuit of a deeper understanding of deep learning and its phenomena has since motivated many subfields, and progress on fundamental questions has been distributed across many distinct yet complementary perspectives that range from purely theoretical to predominantly empirical research.

**Outlook.** In this work, we take a hybrid approach and investigate how we can _apply_ ideas primarily used in theoretical research to investigate the behavior of a simple yet accurate model of a neural network _empirically_. Building upon previous work that studies linear approximations to learning in neural networks through tangent kernels (e.g. [1, 19], see Sec. 2), we consider a model that uses first-order approximations for the functional updates made during training. However, unlike most previous work, we define this model incrementally by simply _telescoping out_ approximations to individual updates made during training (Sec. 3) such that it more closely approximates the true behavior of a fully trained neural network in practical settings. This provides us with a pedagogical lens through which we can view modern optimization strategies and other design choices (Sec. 5), and a mechanism with which we can conduct empirical investigations into several prominent deep learning phenomena that showcase how neural networks sometimes generalize _seemingly_ unpredictably.

Across three case studies in Sec. 4, we then show that this model allows us to _construct and extract metrics that help predict and understand_ the a priori unexpected performance of neural networks. First, in Sec. 4.1, we demonstrate that it allows us to extend [3]'s recent model complexity metric to neural networks, and use this to investigate surprising generalization curves - discovering that the non-monotonic behaviors observed in both deep double descent [1] and grokking [2] are associated with _quantifiable_ divergence of train- and test-time model complexity. Second, in Sec. 4.2, we show that it reveals perhaps surprising parallels between gradient boosting [1] and neural network learning, which we then use to investigate the known performance differences between neural networks and gradient boosted trees on tabular data in the presence of dataset irregularities [13]. Third, in Sec. 4.3, we use it to investigate the connections between gradient stabilization and the success of weight averaging (i.e. linear mode connectivity [14]).

## 2 Background

**Notation and preliminaries.** Let \(f_{\bm{\theta}}:\mathcal{X}\subseteq\mathbb{R}^{d}\rightarrow\mathcal{Y} \subseteq\mathbb{R}^{k}\) denote a neural network parameterized by (stacked) model weights \(\bm{\theta}\in\mathbb{R}^{p}\). Assume we observe a training sample of \(n\) input-output pairs \(\{\mathbf{x}_{i},y_{i}\}_{i=1}^{n}\), i.i.d. realizations of the tuple \((X,Y)\) sampled from some distribution \(P\), and wish to learn good model parameters \(\bm{\theta}\) for predicting outputs from this data by minimizing an empirical prediction loss \(\frac{1}{n}\sum_{i=1}^{n}\ell(f_{\bm{\theta}}(\mathbf{x}_{i}),y_{i})\), where \(\ell:\mathbb{R}^{k}\times\mathbb{R}^{k}\rightarrow\mathbb{R}\) denotes some differentiable loss function. Throughout, we let \(k=1\) for ease of exposition, but unless otherwise indicated our discussion generally extends to \(k>1\). We focus on the case where \(\bm{\theta}\) is optimized by initializing the model with some \(\bm{\theta}_{0}\) and then iteratively updating the parameters through stochastic gradient descent (SGD) with learning rates \(\gamma_{t}\) for \(T\) steps, where at each \(t\in[T]=\{1,\dots,T\}\) we subsample batches \(B_{t}\subseteq[n]=\{1,\dots,n\}\) of the training indices, leading to parameter updates \(\Delta\bm{\theta}_{t}\coloneqq\bm{\theta}_{t}-\bm{\theta}_{t-1}\) as:

\[\bm{\theta}_{t}=\bm{\theta}_{t-1}+\Delta\bm{\theta}_{t}=\bm{\theta}_{t-1}- \frac{\gamma_{t}}{|B_{t}|}\sum_{i\in B_{t}}\nabla_{\bm{\theta}}f_{\bm{\theta} _{t-1}}(\mathbf{x}_{i})g^{\ell}_{it}=\bm{\theta}_{t-1}-\gamma_{t}\mathbf{T}_{t }\mathbf{g}^{\ell}_{t}\] (1)

where \(g^{\ell}_{it}=\frac{\partial\ell(f_{\bm{\theta}_{t-1}}(\mathbf{x}_{i}),y_{i})} {\partial f_{\bm{\theta}_{t-1}}(\mathbf{x}_{i})}\) is the gradient of the loss w.r.t. the model prediction for the \(i^{th}\) training example, which we will sometimes collect in the vector \(\mathbf{g}^{\ell}_{t}=[g^{\ell}_{1t},\dots,g^{\ell}_{nt}]^{\top}\), and the \(p\times n\) matrix \(\mathbf{T}_{t}=[\frac{1\{1\in B_{t}\}}{|B_{t}|}\nabla_{\bm{\theta}}f_{\bm{ \theta}_{t-1}}(\mathbf{x}_{1}),\dots,\frac{1\{n\in B_{t}\}}{|B_{t}|}\nabla_{ \bm{\theta}}f_{\bm{\theta}_{t-1}}(\mathbf{x}_{n})]\) has as columns the gradients of the model prediction with respect to its parameters for examples in the training batch (and \(\bm{0}\) otherwise). Beyond vanilla SGD, modern deep learning practice usually relies on a number of modifications to the update described above, such as momentum and weight decay; we discuss these in Sec. 5.

**Related work: Linearized neural networks and tangent kernels.** A growing body of recent work has explored the use of _linearized_ neural networks (linear in their parameters) as a tool for theoretical [1, 12, 13] and empirical [14, 15, 16] study. In this paper, we similarly make extensive use of the following observation (as in e.g. [14]): we can linearize the difference \(\Delta f_{t}(\mathbf{x})\coloneqq f_{\bm{\theta}_{t}}(\mathbf{x})-f_{\bm{ \theta}_{t-1}}(\mathbf{x})\) between two parameter updates as

\[\Delta f_{t}(\mathbf{x})=\nabla_{\bm{\theta}}f_{\bm{\theta}_{t-1}}(\mathbf{x} )^{\top}\Delta\bm{\theta}_{t}+\mathcal{O}(||\Delta\bm{\theta}_{t}||^{2}) \approx\nabla_{\bm{\theta}}f_{\bm{\theta}_{t-1}}(\mathbf{x})^{\top}\Delta\bm{ \theta}_{t}\coloneqq\Delta\tilde{f}_{t}(\mathbf{x})\] (2)

where the quality of the approximation \(\Delta\tilde{f}_{t}(\mathbf{x})\) is good whenever the parameter updates \(\Delta\bm{\theta}_{t}\) from a single batch are sufficiently small (or when the Hessian product \(||\Delta\bm{\theta}_{t}^{\top}\nabla_{\bm{\theta}}^{2}\bm{\theta}_{\bm{\theta} _{t-1}}(\mathbf{x})\Delta\bm{\theta}_{t}||\) vanishes). If Eq. (2) holds exactly (e.g. for infinitesimal \(\gamma_{t}\)), then running SGD in the network's parameter space to obtain \(\Delta\bm{\theta}_{t}\) corresponds to executing steepest descent on the function output \(f_{\bm{\theta}}(\mathbf{x})\) itself using the _neural tangent kernel_\(K^{\bm{\theta}}_{t}(\mathbf{x},\mathbf{x}_{i})\) at time-step \(t\)[1], i.e. results in functional updates

\[\Delta\tilde{f}_{t}(\mathbf{x})\approx-\gamma_{t}\sum_{i\in[n]}K^{\bm{\theta} }_{t}(\mathbf{x},\mathbf{x}_{i})g^{\ell}_{it}\text{ where }K^{\bm{\theta}}_{t}(\mathbf{x},\mathbf{x}_{i})\coloneqq \frac{\mathbf{1}\{i\in B_{t}\}}{|B_{t}|}\nabla_{\bm{\theta}}f_{\bm{\theta}_{t- 1}}(\mathbf{x})^{\top}\nabla_{\bm{\theta}}f_{\bm{\theta}_{t-1}}(\mathbf{x}_{ i}).\] (3)

_Lazy learning_[1] occurs as the model gradients remain approximately constant during training, i.e. \(\nabla_{\bm{\theta}}f_{\bm{\theta}_{t}}(\mathbf{x})\approx\nabla_{\bm{\theta}}f_{ \bm{\theta}_{0}}(\mathbf{x})\), \(\forall t\in[T]\). For learned parameters \(\bm{\theta}_{T}\), this implies that the approximation \(f^{lin}_{\bm{\theta}_{T}}(\mathbf{x})=f_{\bm{\theta}_{0}}(\mathbf{x})+\nabla_{ \bm{\theta}}f_{\bm{\theta}_{0}}(\mathbf{x})^{\top}(\bm{\theta}_{T}-\bm{\theta }_{0})\) holds - which is a _linear function of the model parameters_, and thus corresponds to a linear regression in which features are given by the model gradients \(\nabla_{\bm{\theta}}f_{\bm{\theta}_{0}}(\mathbf{x})\) instead of the inputs \(\mathbf{x}\) directly - whose training dynamics can be more easily understood theoretically. For sufficiently wide neural networks the \(\nabla_{\bm{\theta}}f_{\bm{\theta}_{t}}(\mathbf{x})\), and thus the tangent kernel, have been theoretically shown to be constant throughout training in some settings [1, 13, 14], but in practice they generally vary during training, as shown theoretically in [14] and empirically in [14]. A growing theoretical literature [15] investigates constant tangent kernel assumptions to study convergence and generalization of neural networks (e.g.

[1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. This present work relates more closely to _empirical_ studies making use of tangent kernels and linear approximations, such as [1, 12], OJMDF21] who highlight differences between lazy learning and real networks, and [13] who empirically investigate the relationship between loss landscapes and the evolution of \(K_{t}^{\theta}(\mathbf{x},\mathbf{x}_{i})\).

## 3 A Telescoping Model of Deep Learning

In this work, we explore whether we can exploit the approximation in Eq. (2) beyond the laziness assumption to gain new insight into neural network learning. Instead of applying the approximation across the entire training trajectory at once as in \(f_{\boldsymbol{\theta}_{T}}^{lin}(\mathbf{x})\), we consider using it _incrementally_ at each batch update during training to approximate _what has been learned at this step_. This still provides us with a greatly simplified and transparent model of a neural network, and results in a much more reasonable approximation of the true network. Specifically, we explore whether - instead of studying the final model \(f_{\boldsymbol{\theta}_{T}}(\mathbf{x})\) as a whole - we can gain insight by _telescoping out_ the functional updates made throughout training, i.e. exploiting that we can always equivalently express \(f_{\boldsymbol{\theta}_{T}}(\mathbf{x})\) as:

\[f_{\boldsymbol{\theta}_{T}}(\mathbf{x})=f_{\boldsymbol{\theta}_{0}}(\mathbf{ x})+\sum_{t=1}^{T}[f_{\boldsymbol{\theta}_{t}}(\mathbf{x})-f_{\boldsymbol{ \theta}_{t-1}}(\mathbf{x})]=f_{\boldsymbol{\theta}_{0}}(\mathbf{x})+\sum_{t=1 }^{T}\Delta f_{t}(\mathbf{x})\] (4)

This representation of a trained neural network in terms of its learning trajectory rather than its final parameters is interesting because we are able to better reason about the impact of the training procedure on the intermediate updates \(\Delta f_{t}(\mathbf{x})\) than the final function \(f_{\boldsymbol{\theta}_{T}}(\mathbf{x})\) itself. In particular, we investigate whether empirically monitoring behaviors of the sum in Eq. (4) while making use of the approximation in Eq. (2) will enable us to gain practical insights into learning in neural networks, while incorporating a variety of modern design choices into the training process. That is, we explore the use of the following _telescoping model_\(\tilde{f}_{\boldsymbol{\theta}_{T}}(\mathbf{x})\) as an approximation of a trained neural network:

\[\tilde{f}_{\boldsymbol{\theta}_{T}}(\mathbf{x}):=f_{\boldsymbol{\theta}_{0}}( \mathbf{x})+\sum_{t=1}^{T}\underbrace{\nabla_{\boldsymbol{\theta}}f_{ \boldsymbol{\theta}_{t-1}}(\mathbf{x})^{\top}\Delta\boldsymbol{\theta}_{t}}_ {\text{(i) The weight-averaging}}=f_{\boldsymbol{\theta}_{0}}(\mathbf{x})-\sum_{t=1 }^{T}\underbrace{\sum_{i\in[n]}K_{t}^{T}(\mathbf{x},\mathbf{x}_{i})g_{it}^{ \ell}}_{\text{(ii) The kernel representation}}\] (5)

where \(K_{t}^{T}(\mathbf{x},\mathbf{x}_{i})\) is determined by the neural tangent kernel as \(\gamma_{t}K_{t}^{\theta}(\mathbf{x},\mathbf{x}_{i})\) in the case of standard SGD (in which case (ii) can also be interpreted as a discrete-time approximation of [13]'s _path kernel_), but can take other forms for different choices of learning algorithm as we explore in Sec. 5.

**Practical considerations.** Before proceeding, it is important to emphasize that the telescoping approximation described in Eq. (5) is intended as _a tool for (empirical) analysis of learning in neural networks_ and is _not_ being proposed as an alternative approach to training neural networks. Obtaining \(\tilde{f}_{\boldsymbol{\theta}_{T}}(\mathbf{x})\) requires computing \(\nabla_{\theta}f_{\boldsymbol{\theta}_{t-1}}(\mathbf{x})\) for each training _and_ testing example at each training step \(t\in[T]\), leading to increased computation over standard training. Additionally, these computational costs are likely prohibitive for extremely large networks and datasets without further adjustments; for this purpose, further approximations such as [14] could be explored. Nonetheless, computing \(\tilde{f}_{\boldsymbol{\theta}_{T}}(\mathbf{x})\) - or relevant parts of it - is still feasible in many pertinent settings as later illustrated in Sec. 4.

Figure 1: **Illustration of the telescoping model of a trained neural network. Unlike the more standard framing of a neural network in terms of an iteratively learned set of parameters, the telescoping model takes a functional perspective on training a neural network in which an arbitrary test exampleâ€™s initially random prediction, \(f_{\boldsymbol{\theta}_{0}}(\mathbf{x})\), is additively updated by a linearized adjustment \(\Delta\tilde{f}_{t}(\mathbf{x})\) at each step \(t\) as in Eq. (5).**

**How good is this approximation?** In Fig. 2, we examine the quality of \(\tilde{f}_{\bm{\theta}_{i}}(\mathbf{x})\) for a 3-layer fully-connected ReLU network of width 200, trained to discriminate 3-vs-5 from 1000 MNIST examples using the squared loss with SGD or AdamW [1]. In red, we plot its mean average approximation error (\(\frac{1}{1000}\sum_{\mathbf{x}\in\mathcal{X}_{test}}|f_{\bm{\theta}_{i}}( \mathbf{x})-\tilde{f}_{\bm{\theta}_{i}}(\mathbf{x})|\)) and observe that for small learning rates \(\gamma\) the difference remains negligible. In \(\mathrm{gray}\) we plot the same quantity for \(f_{\bm{\theta}_{i}}^{in}(\mathbf{x})\) (i.e. the first-order expansion around \(\bm{\theta}_{0}\)) for reference and find that iteratively telescoping out the updates instead improves the approximation _by orders of magnitude_ - which is also reflected in their prediction performance (see Appendix D.1). Unsurprisingly, \(\gamma\) controls approximation quality as it determines \(||\Delta\bm{\theta}_{t}||\). Further, \(\gamma\)_interacts_ with the optimizer choice - e.g. Adam(W) [14, 1] naturally makes larger updates due to rescaling (see Sec. 5) and therefore requires smaller \(\gamma\) to ensure approximation quality than SGD.

## 4 A Closer Look at Deep Learning Phenomena Through a Telescoping Lens

Next, we turn to _applying_ the telescoping model. Below, we present three case studies revisiting existing experiments that provided evidence for a range of unexpected behaviors of neural networks. These case studies have in common that they highlight cases in which neural networks appear to generalize somewhat _unpredictably_, which is also why each phenomenon has received considerable attention in recent years. For each, we then show that the telescoping model allows us to construct and extract metrics that can help predict and understand the unexpected performance of the networks. In particular, we investigate (i) surprising generalization curves (Sec. 4.1), (ii) performance differences between gradient boosting and neural networks on some tabular tasks (Sec. 4.2), and (iii) the success of weight averaging (Sec. 4.3). We include an extended literature review in Appendix A, a detailed discussion of all experimental setups in Appendix C, and additional results in Appendix D.

### Case study 1: Exploring surprising generalization curves and benign overfitting

Classical statistical wisdom provides clear intuitions about overfitting: models that can fit the training data too well - because they have too many parameters and/or because they were trained for too long - are expected to generalize poorly (e.g. [13, Ch. 7]). Modern phenomena like double descent [1], however, highlighted that pure capacity measures (capturing what _could_ be learned instead of what _is_ actually learned) would not be sufficient to understand the complexity-generalization relationship in deep learning [1]. Raw parameter counts, for example, cannot be enough to understand the complexity of what has been learned by a neural network during training because, even when using _the same architecture_, what is learned could be wildly different across various implementation choices within the optimization process - and even at different points during the training process of the same model, as prominently exemplified by the grokking phenomenon [2]. Here, with the goal of finding clues that may help predict phenomena like double descent and grokking, we explore whether the telescoping model allows us to gain insight into the relative complexity of what is learned.

A complexity measure that avoids the shortcomings listed above - because it allows to consider a _specific trained_ model - was recently used by [15] in their study of _non-deep_ double descent. As their measure \(p^{0}_{\hat{\mathbf{s}}}\) builds on the literature on smoothers [13], it requires to express learned predictions as a linear combination of the training labels, i.e. as \(f(\mathbf{x})=\hat{\mathbf{s}}(\mathbf{x})\mathbf{y}=\sum_{i\in[n]}\hat{s}^{i }(\mathbf{x})y_{i}\). Then, [15] define the _effective parameters_\(p^{0}_{\hat{\mathbf{s}}}\) used by the model when issuing predictions for some set of inputs \(\{\mathbf{x}^{0}_{j}\}_{j\in\mathcal{I}_{0}}\) with indices collected in \(\mathcal{I}_{0}\) (here, \(\mathcal{I}_{0}\) is either \(\mathcal{I}_{train}=\{1,\ldots,n\}\) or \(\mathcal{I}_{test}=\{n+1,\ldots,n+m\}\)) as \(p^{0}_{\hat{\mathbf{s}}}\equiv p(\mathcal{I}_{0},\hat{\mathbf{s}}(\cdot))= \frac{n}{|\mathcal{I}_{0}|}\sum_{j\in\mathcal{I}_{0}}||\hat{\mathbf{s}}(\mathbf{ x}^{0}_{j})||^{2}\). Intuitively, the larger \(p^{0}_{\hat{\mathbf{s}}}\), the less smoothing across the training labels is performed, which implies higher model complexity.

Due to the black-box nature of trained neural networks, however, it is not obvious how to link learned predictions to the labels observed during training. Here, we demonstrate how the telescoping model allows us to do precisely that - enabling us to make use of \(p^{0}_{\hat{\mathbf{s}}}\) as a proxy for complexity. We consider the special case of a single output (\(k=1\)) and training with squared loss \(\ell(f(\mathbf{x}),y)=\frac{1}{2}(y-f(\mathbf{x}))^{2}\)

Figure 2: **Approximation error** of the telescoping \((\tilde{f}_{\bm{\theta}_{i}}(\mathbf{x}),\text{red})\) and the linear model \((f^{lin}_{\bm{\theta}_{i}}(\mathbf{x}),\text{gray})\).

and note that we can now exploit that the SGD weight update simplifies to

\[\Delta\bm{\theta}_{t}=\gamma_{t}\mathbf{T}_{t}(\mathbf{y}-\mathbf{f}_{\bm{\theta} _{t-1}})\text{ where }\mathbf{y}=[y_{1},\ldots,y_{n}]^{\top}\text{ and }\mathbf{f}_{\bm{\theta}_{t}}=[f_{\bm{\theta}_{t}}(\mathbf{x}_{1}), \ldots,f_{\bm{\theta}_{t}}(\mathbf{x}_{n})]^{\top}.\] (6)

Assuming the telescoping approximation holds exactly, this implies functional updates

\[\Delta\tilde{f}_{t}(\mathbf{x})=\gamma_{t}\nabla_{\bm{\theta}}f_{\bm{\theta}_{ t-1}}(\mathbf{x})^{\top}\mathbf{T}_{t}(\mathbf{y}-\tilde{\mathbf{f}}_{\bm{ \theta}_{t-1}})\] (7)

which use a linear combination of the training labels. Note further that after the _first_ SGD update

\[\tilde{f}_{\bm{\theta}_{1}}(\mathbf{x})=f_{\bm{\theta}_{0}}(\mathbf{x})+ \Delta\tilde{f}_{1}(\mathbf{x})=\underbrace{\gamma_{1}\nabla_{\bm{\theta}}f_{ \bm{\theta}_{0}}(\mathbf{x})^{\top}\mathbf{T}_{1}}_{\mathbf{s}\bm{\theta}_{1 }(\mathbf{x})}\mathbf{y}+\underbrace{\underline{f_{\bm{\theta}_{0}}(\mathbf{x} )-\gamma_{1}\nabla_{\bm{\theta}}f_{\bm{\theta}_{0}}(\mathbf{x})^{\top}\mathbf{ T}_{1}\bm{\theta}_{0}}}_{c^{0}_{\bm{\theta}_{1}}(\mathbf{x})}\] (8)

which means that the first telescoping predictions \(\tilde{f}_{\bm{\theta}_{t}}(\mathbf{x})\) are indeed simply linear combinations of the training labels (and the predictions at initialization)! As detailed in Appendix B.1, this also implies that recursively substituting Eq. (7) into Eq. (5) further allows us to write _any_ prediction \(\tilde{f}_{\bm{\theta}_{t}}(\mathbf{x})\) as a linear combination of the training labels and \(f_{\bm{\theta}_{0}}(\cdot)\), i.e. \(\tilde{f}_{\bm{\theta}_{t}}(\mathbf{x})=\mathbf{s}_{\bm{\theta}_{t}}(\mathbf{x })\mathbf{y}+c^{0}_{\bm{\theta}_{t}}(\mathbf{x})\) where the \(1\times n\) vector \(\mathbf{s}_{\bm{\theta}_{t}}(\mathbf{x})\) is a function of the kernels \(\{K_{\bm{t}^{\prime}}^{t}(\cdot,\cdot)\}_{\bm{t}^{\prime}\leq t}\), and the scalar \(c^{0}_{\bm{\theta}_{t}}(\mathbf{x})\) is a function of the \(\{K_{\bm{t}^{\prime}}^{t}(\cdot,\cdot)\}_{\bm{t}^{\prime}\leq t}\) and \(f_{\bm{\theta}_{0}}(\cdot)\). We derive precise expressions for \(\mathbf{s}_{\bm{\theta}_{t}}(\mathbf{x})\) and \(c^{0}_{\bm{\theta}_{t}}(\mathbf{x})\) for different optimizers in Appendix B.1 - enabling us to use \(\mathbf{s}_{\bm{\theta}_{t}}(\mathbf{x})\) to compute \(p^{0}_{\mathbf{s}}\) as a proxy for complexity below.

**Double descent: Model complexity vs model size.** While training error always monotonically decreases as model size (measured by parameter count) increases, [14] made a surprising observation regarding test error in their seminal paper on _double descent_: they found that test error initially improves with additional parameters and then worsens when the model is increasingly able to overfit to the training data (as is expected) but can _improve again_ as model size is increased further past the so-called interpolation threshold where perfect training performance is achieved. This would appear to contradict the classical U-shaped relationship between model complexity and test error [13, Ch. 7]. Here, we investigate whether tracking \(p^{0}_{\mathbf{s}}\) on train and test data separately will allow us to gain new insight into the phenomenon in neural networks.

In Fig. 3, we replicate the binary classification example of double descent in neural networks of [14], training single-hidden-layer ReLU networks of increasing width to distinguish cats and dogs on CIFAR-10 (we present additional results using MNIST in Appendix D.2). First, we indeed observe the characteristic behavior of error curves as described in [14] (top panel). Measuring learned complexity using \(p^{0}_{\mathbf{s}}\), we then find that while \(p^{train}_{\mathbf{s}}\) monotonically increases as model size is increasing, the effective parameters used on the test data \(p^{test}_{\mathbf{s}}\) implied by the trained neural network _decrease_ as model size is increased past the interpolation threshold (bottom panel). Thus, paralleling the findings made in [15] for linear regression and tree-based methods, we find that distinguishing between train- and test-time complexity of a neural network using \(p^{0}_{\mathbf{s}}\) provides new quantitative evidence that bigger networks are _not_ necessarily learning more complex prediction functions for unseen test examples, which resolves the ostensible tension between deep double descent and the classical U-curve. Importantly, note that \(p^{test}_{\mathbf{s}}\) can be computed without access to test-time labels, which means that the observed difference between \(p^{train}_{\mathbf{s}}\) and \(p^{test}_{\mathbf{s}}\) allows to quantify whether there is _benign overfitting_[13, 13] in a neural network.

**Grokking: Model complexity throughout training.** The grokking phenomenon [15] then showcased that improvements in test performance _during a single training run_ can occur long after perfect training performance has been achieved (contradicting early stopping practice!). While [13] attribute this to weight decay causing \(||\bm{\theta}_{t}||\) to shrink late in training - which they demonstrate on an MNIST example using unusually large \(\bm{\theta}_{0}\) - [16] highlight that grokking can also occur as the weight norm \(||\bm{\theta}_{t}||\)_grows_ later in training - which they demonstrate on a polynomial regression task. In Fig. 4 we replicate2 both experiments while tracking \(p^{0}_{\mathbf{s}}\) to investigate whether

Figure 3: **Double descent** in MSE (top) and effective parameters \(p^{0}_{\mathbf{s}}\) (bottom) on CIFAR-10.

this provides new insight into this apparent disagreement. Then, we observe that the continued improvement in test error, past the point of perfect training performance, is associated with divergence of \(p_{\mathbf{s}}^{train}\) and \(p_{\mathbf{s}}^{test}\) in _both_ experiments (analogous to the double descent experiment in Fig. 3), suggesting that grokking may reflect _transition into_ a measurably benign overfitting regime during training. In Appendix D.2, we additionally investigate mechanisms known to induce grokking, and show that later onset of generalization indeed coincides with later divergence of \(p_{\mathbf{s}}^{train}\) and \(p_{\mathbf{s}}^{test}\).

**Inductive biases & learned complexity.** We observed that the large \(\boldsymbol{\theta}_{0}\) in [11]'s MNIST example of grokking result in very large initial predictions \(|\!|f_{\boldsymbol{\theta}_{0}}(\mathbf{x})|\!\gg\!1\). Because _no_ sigmoid is applied, the model needs to learn that all \(y_{i}\!\in\![0,1]\) by reducing the magnitude of predictions substantially - large \(\boldsymbol{\theta}_{0}\) thus constitute a very poor inductive bias for this task. One may expect that the better an inductive bias is, the less complex the component of the final prediction that is learned from data. To test whether this intuition is quantifiable, we repeat the MNIST experiment with standard initialization scale, with and without sigmoid activation \(\sigma(\cdot)\), in column (3) of Fig. 4 (training results shown in Appendix D.2 for readability). We indeed find that both not only speed up learning significantly (a generalizing solution is found in \(10^{2}\) instead of \(10^{5}\) steps), but also substantially reduce effective parameters used, where the stronger inductive bias - using \(\sigma(\cdot)\) - indeed leads to the least learned complexity.

_Takeaway Case Study 1._ The telescoping model enables us to use \(p_{\mathbf{s}}^{0}\) as a proxy for learned complexity, whose relative behavior on train and test data can quantify benign overfitting in neural networks.

### Case study 2: Understanding differences between gradient boosting and neural networks

Despite their overwhelming successes on image and language data, neural networks are - perhaps surprisingly - still widely considered to be outperformed by _gradient boosted trees_ (GBTs) on _tabular data_, an important modality in many data science applications. Exploring this apparent Achilles heel of neural networks has therefore been the goal of multiple extensive benchmarking studies [14, 15]. Here, we concentrate on a specific empirical finding of [17]: their results suggest that GBTs may particularly outperform deep learning on heterogeneous data with greater irregularity in input features, a characteristic often present in tabular data. Below, we first show that the telescoping model offers a useful lens to compare and contrast two methods, and then use this insight to provide and test a new explanation of why GBTs can perform better in the presence of dataset irregularities.

**Identifying (dis)similarities between learning in GBTs and neural networks.** We begin by introducing gradient boosting [13] closely following [13, Ch. 10.10]. Gradient boosting (GB) also aims to learn a predictor \(\hat{f}^{GB}:\mathcal{X}\rightarrow\mathbb{R}^{k}\) minimizing expected prediction loss \(\ell\). While deep learning solves this problem by iteratively updating a randomly initialized set of _parameters_ that transform inputs to predictions, the GB formulation iteratively updates _predictions_ directly without requiring any iterative learning of parameters - thus operating in function space rather than parameter space. Specifically, GB, with learning rate \(\gamma\) and initialized at predictor \(h_{0}(\mathbf{x})\), consists of a sequence \(\hat{f}^{GB}_{T}(\mathbf{x})=h_{0}(\mathbf{x})+\gamma\sum_{t=1}^{T}\hat{h}_{t }(\mathbf{x})\) where each \(\hat{h}_{t}(\mathbf{x})\) improves upon the existing predictions \(\hat{f}^{GB}_{t-1}(\mathbf{x})\). The solution to the loss minimization problem can be achieved by executing steepest descent in function space _directly_, where each update \(\hat{h}_{t}\) simply outputs the negative training gradients of the loss function with respect to the previous model, i.e. \(\hat{h}_{t}(\mathbf{x}_{i})=-g^{\ell}_{it}\) where \(g^{\ell}_{it}=\nicefrac{{\partial\ell(\hat{f}^{GB}_{t-1}(\mathbf{x}_{i}),\,y_ {i})}}{{\partial\hat{f}^{GB}_{t-1}(\mathbf{x}_{i})}}\).

Figure 4: **Grokking in mean squared error on a polynomial regression task (1, replicated from [14]) and in misclassification error on MNIST using a network with large initialization (2, replicated from [11]) (top), against effective parameters (bottom). Column (3) shows test results on MNIST with standard initialization (with and without sigmoid activation) where time to generalization is quick and grokking does not occur.**

However, this process is only defined at the training points \(\{\mathbf{x}_{i},y_{i}\}_{i\in[n]}\). To obtain an estimate of the loss gradient for an arbitrary test point \(\mathbf{x}\), each iterative update instead fits a weak learner \(\hat{h}_{t}(\cdot)\) to the current input-gradient pairs \(\{\mathbf{x}_{i},-g_{i\in[n]}^{\ell}\}_{i\in[n]}\) which can then also be evaluated new, unseen inputs. While this process could in principle be implemented using any base learner, the term _gradient boosting_ today appears to exclusively refer to the approach outlined above implemented using shallow trees as \(\hat{h}_{t}(\cdot)\)[13]. Focusing on trees which issue predictions by averaging the training outputs in each leaf, we can make use of the fact that these are sometimes interpreted as adaptive nearest neighbor estimators or kernel smoothers [10, 11, 12], allowing us to express the learned predictor as:

\[\hat{f}^{GB}(\mathbf{x})=h_{0}(\mathbf{x})-\gamma\sum_{t=1}^{T}\sum_{i\in[n]} \frac{\mathbf{1}\{l_{h_{t}}(\mathbf{x})=l_{h_{t}}(\mathbf{x}_{i})\}}{n_{l( \mathbf{x})}}g_{it}^{\ell}=h_{0}(\mathbf{x})-\gamma\sum_{t=1}^{T}\sum_{i\in[n] }K_{\hat{h}_{t}}(\mathbf{x},\mathbf{x}_{i})g_{it}^{\ell}\] (9)

where \(l_{\hat{h}_{t}}(\mathbf{x})\) denotes the leaf example \(\mathbf{x}\) falls into, \(n_{l(\mathbf{x})}=\sum_{i\in[n]}\mathbf{1}\{l_{h_{t}}(\mathbf{x})=l_{h_{t}}( \mathbf{x}_{i})\}\) is the number of training examples in said leaf and \(K_{\hat{h}_{t}}(\mathbf{x},\mathbf{x}_{i})=\nicefrac{{1}}{{n_{leaf}(\mathbf{ x})}}\mathbf{1}\{l_{\hat{h}_{t}}(\mathbf{x})=l_{\hat{h}_{t}}(\mathbf{x}_{i})\}\) is thus the kernel learned by the \(t^{th}\) tree \(\hat{h}_{t}(\cdot)\). Comparing Eq. (9) to the kernel representation of the telescoping model of neural network learning in Eq. (5), we make a perhaps surprising observation: the telescoping model of a neural network and GBTs have _identical_ structure and differ only in their used kernel! Below, we explore whether this new insight allows to understand some of their performance differences.

**Why can GBTs outperform deep learning in the presence of dataset irregularities?** Comparing Eq. (5) and Eq. (9) thus suggests that at least some of the performance differences between neural networks and GBTs are likely to be rooted in the differences between the behavior of the neural network tangent kernels \(K_{t}^{\boldsymbol{\theta}}(\mathbf{x},\mathbf{x}_{i})\) and GBT's tree kernels \(K_{\hat{h}_{t}}(\mathbf{x},\mathbf{x}_{i})\). One difference is obvious and purely architectural: it is possible that either kernel encodes a better inductive bias to fit the underlying outcome-generating process of a dataset at hand. Another difference is more subtle and relates to the behavior of the learned model on new inputs \(\mathbf{x}\): the tree kernels are likely to behave _much_ more predictable at test-time than the neural network tangent kernels. To see this, note that for the tree kernels we have that \(\forall\mathbf{x}\in\mathcal{X}\) and \(\forall i\in[n]\), \(0\leq K_{\hat{h}_{t}}(\mathbf{x},\mathbf{x}_{i})\leq 1\) and \(\sum_{i\in[n]}K_{\hat{h}_{t}}(\mathbf{x},\mathbf{x}_{i})=1\); importantly, this is true regardless of whether \(\mathbf{x}=\mathbf{x}_{i}\) for some \(i\) or not. For the tangent kernels on the other hand, \(K_{t}^{\boldsymbol{\theta}}(\mathbf{x},\mathbf{x}_{i})\) is in general unbounded and could behave _very_ differently for \(\mathbf{x}\) not observed during training. This leads us to hypothesize that this difference may be able to explain [12]'s observation that GBTs perform better whenever features are heavy-tailed: if a test point \(\mathbf{x}\) is very different from training points, the kernels implied by the neural network \(\mathbf{k}_{t}^{\boldsymbol{\theta}}(\mathbf{x})\coloneqq[K_{t}^{\boldsymbol{ \theta}}(\mathbf{x},\mathbf{x}_{1}),\dots,K_{t}^{\boldsymbol{\theta}}(\mathbf{ x},\mathbf{x}_{n})]^{\top}\) may behave very differently than at train-time while the tree kernels \(\mathbf{k}_{\hat{h}_{t}}(\mathbf{x})\coloneqq[K_{\hat{h}_{t}}(\mathbf{x}, \mathbf{x}_{1}),\dots,K_{\hat{h}_{t}}(\mathbf{x},\mathbf{x}_{n})]^{\top}\) will be less affected. For instance, \(\frac{1}{\sqrt{n}}\leq||\mathbf{k}_{\hat{h}_{t}}(\mathbf{x})||_{2}\leq 1\) for all \(\mathbf{x}\) while \(||\mathbf{k}_{t}^{\boldsymbol{\theta}}(\mathbf{x})||_{2}\) is generally unbounded.

We empirically test this hypothesis on standard tabular benchmark datasets proposed in [1]. We wish to examine the performance of the models and the behavior of the kernels as inputs become increasingly irregular, evaluating if GBT's kernels indeed display more consistent behavior compared to the network's tangent kernels. As a simple notion for input irregularity, we apply principal component analysis to the inputs to obtain a lower dimensional representation of the data and sort the observations according to their distance from the centroid. For a fixed trained model, we then evaluate on test sets consisting of increasing proportions \(p\) of the most irregular inputs (those in the top 10% furthest from the centroid). We compare the GBTs to neural networks by examining (i) the most extreme values their kernel weights take at test-time relative to the training data (measured as \(\frac{\frac{1}{T}\sum_{i=1}^{T}\max_{i\in T_{test}}||\mathbf{k}_{t}(x_{i})||_{2}} {\frac{1}{T}\sum_{i=1}^{T}\max_{i\in T_{train}}||\mathbf{k}_{t}(\mathbf{x}_{i})|| _{2}}\)) and (ii) how their relative mean squared error (measured as \(\frac{MSE_{NN}^{p}-MSE_{BBT}^{p}}{MSE_{NN}^{p}-MSE_{BBT}^{p}}\)) changes as the proportion \(p\) of irregular examples increases. In Fig. 5 using houses and in Appendix D.3

Figure 5: **Neural Networks vs GBTs: Relative performance (top) and behavior of kernels (bottom) with increasing test data irregularity using the houses dataset.**

using additional datasets, we first observe that GBTs outperform the neural network already in the absence of irregular examples; this highlights that there may indeed be differences in the suitability of the kernels in fitting the outcome-generating processes. Consistent with our expectations, we then find that, as the test data becomes more irregular, the performance of the neural network decays faster than that of the GBTs. Importantly, this is well tracked by their kernels, where the unbounded nature of the network's tangent kernel indeed results in it changing its behavior on new, challenging examples.

_Takeaway Case Study 2._ Eq. (5) provides a new lens for comparing neural networks to GBTs, and highlights that unboundedness in \(\mathbf{k}_{t}^{\bm{\theta}}(\mathbf{x})\) can predict performance differences due to dataset irregularities.

### Case study 3: Towards understanding the success of weight averaging

The final interesting phenomenon we investigate is that it is sometimes possible to simply average the weights \(\bm{\theta}_{1}\) and \(\bm{\theta}_{2}\) obtained from two stochastic training runs of the same model, resulting in a weight-averaged model that performs no worse than the individual models [10, 1] - which has important applications in areas such as federated learning. This phenomenon is known as linear mode connectivity (LMC) and is surprising as, a priori, it is not obvious that simply _averaging the weights_ of independent neural networks (instead of their predictions, as in a deep ensemble [13]), which are highly nonlinear functions of their parameters, would not greatly worsen performance. While recent work has demonstrated empirically that it is sometimes possible to weight-average an even broader class of models after permuting weights [11, 12], we focus here on understanding when LMC can be achieved for two models trained from the same initialization \(\bm{\theta}_{0}\).

In particular, we are interested in [10]'s observation that LMC can emerge during training: the weights of two models \(\bm{\theta}_{jT}^{t^{\prime}},j\in\{1,2\}\), which are initialized identically and follow identical optimization routine up until checkpoint \(t^{\prime}\) but receive different batch orderings and data augmentations after \(t^{\prime}\), can be averaged to give an equally performant model as long as \(t^{\prime}\) exceeds a so-called _stability point_\(t^{*}\), which was empirically discovered to occur early in training in [10]. Interestingly, [10, Sec. 5] implicitly hint at an explanation for this phenomenon in their empirical study of tangent kernels and loss landscapes, where they found an association between the disappearance of loss barriers between solutions during training and the rate of change in \(K_{t}^{\bm{\theta}}(\cdot,\cdot)\). We further explore potential implications of this observation through the lens of the telescoping model below.

**Why a transition into a constant-gradient regime would imply LMC.** Using the weight-averaging representation of the telescoping model, it becomes easy to see that not only would stabilization of the tangent kernel be _associated_ with lower linear loss barriers, but the transition into a lazy regime during training - i.e. reaching a point \(t^{*}\) after which the model gradients no longer change - can be _sufficient_ to imply LMC during training as observed in [10] under a mild assumption on the performance of the two networks' _ensemble_. To see this, let \(L(f):=\mathbb{E}_{X,Y\sim P}[\ell(f(X),Y)]\) denote the expected loss of \(f\) and recall that if \(sup_{\alpha\in[0,1]}L(f_{\alpha\bm{\theta}_{1T}^{t^{\prime}}+(1-\alpha)\bm{ \theta}_{2T}^{t^{\prime}}})-[\alpha L(\bm{f}_{\bm{\theta}_{1T}^{t^{\prime}}}) +(1-\alpha)L(f_{\bm{\theta}_{2T}^{t^{\prime}}})]\leq 0\) then LMC is said to hold. If we assume that ensembles \(\bar{f}^{\alpha}(\mathbf{x}):=\alpha f_{\bm{\theta}_{1T}^{t^{\prime}}}( \mathbf{x})+(1-\alpha)f_{\bm{\theta}_{2T}^{t^{\prime}}}(\mathbf{x})\) perform no worse than the individual models (i.e. \(L(\bar{f}^{\alpha})\leq\alpha L(f_{\bm{\theta}_{1T}^{t^{\prime}}})+(1-\alpha)L( f_{\bm{\theta}_{2T}^{t^{\prime}}})\ \forall\alpha\in[0,1]\), as is usually the case in practice [1]), then one case in which LMC is guaranteed is if the predictions of weight-averaged model and ensemble are identical. In Appendix B.2, we show that if there exists some \(t^{*}\in[0,T)\) after which the model gradients \(\nabla_{\bm{\theta}}f_{\bm{\theta}_{jt}^{t^{*}}}(\cdot)\) no longer change (i.e. for all \(t^{\prime}\geq t^{*}\) the learned updates \(\bm{\theta}_{jt}^{t^{\prime}}\) lie in a convex set \(\Theta_{j}^{stable}\) in which \(\nabla_{\bm{\theta}}f_{\bm{\theta}_{jt^{*}}^{t^{\prime}}}(\cdot)\approx\nabla _{\bm{\theta}}f_{\bm{\theta}_{*}}(\cdot)\)), then indeed

\[\bar{f}^{\alpha}(\mathbf{x})\approx f_{\alpha\bm{\theta}_{1T}^{t^{\prime}}+(1 -\alpha)\bm{\theta}_{2T}^{t^{\prime}}}(\mathbf{x})\approx f_{\bm{\theta}_{t^{ \prime}}}(\mathbf{x})+\nabla_{\bm{\theta}}f_{\bm{\theta}_{*}}(\mathbf{x})^{ \top}\sum_{t=t^{\prime}+1}^{T}(\alpha\Delta\bm{\theta}_{1t}^{t^{\prime}}+(1- \alpha)\Delta\bm{\theta}_{2t}^{t^{\prime}}).\] (10)

That is, transitioning into a regime with constant model gradients during training can imply LMC because the ensemble and weight-averaged model become near-identical. This also has as an immediate corollary that models with the same \(\bm{\theta}_{0}\) which train fully within this regime (e.g. those discussed in [10, 1]) will have \(t^{*}=0\). Note that, when using nonlinear (final) output activation \(\sigma(\cdot)\) the post-activation model gradients will generally _not_ become constant during training (as we discuss in Sec. 5 for the sigmoid and as was shown theoretically in [11] for general nonlinearities). If, however, the _pre-activation model gradients_ become constant during training and the _pre-activation ensemble_ - which averages the two model's pre-activation outputs _before_ applying \(\sigma(\cdot)\) - performs no worse than the individual models (as is also usually the case in practice [11]), then the above also immediately implies LMC for such models.

This suggests a candidate explanation for why LMC emerged at specific points in [1]. To test this, we replicate their CIFAR-10 experiment using a ResNet-20 in Fig. 6. In addition to plotting the maximal decrease in accuracy when comparing \(f_{\alpha\bm{\theta}_{t^{\prime}}^{\prime}+(1-\alpha)\bm{\theta}_{2T}^{\prime}}( \mathbf{x})\) to the weighted average of the accuracies of the original models as [1] to measure LMC in (1), we also plot the squared change in (pre-softmax) gradients \((\nabla_{\theta}\bm{\theta}_{t^{\prime}+39}(\mathbf{x})-\nabla_{\theta}\bm{ \theta}_{t^{\prime}}(\mathbf{x}))^{2}\) over the next epoch (390 batches) after checkpoint \(t^{\prime}\), averaged over the test set and the parameters in each layer in (2). We find that the disappearance of the loss barrier indeed coincides with the time in training when the model gradients become _more stable_ across all layers. Most saliently, the appearance of LMC appears to correlate with the stabilization of the gradients of the linear output layer. However, we also continue to observe some changes in other model gradients, which indicates that these models do not train fully linearly.

**Pre-training and weight averaging.** Because weight averaging methods have become increasingly popular when using _pre-trained_ instead of randomly initialized models [23, 16, 17], we are interested in testing whether pre-training may improve mode connectability through stabilizing the model gradients. To test this, we replicate the above experiment with the same architecture pre-trained on the SVHN dataset (in green in Fig. 6(1)). Mimicking findings of [23], we first find the loss barrier to be substantially lower after pre-training. In Fig. 6(3), we then observe that the gradients in the hidden and final layers indeed change less and stabilize earlier in training than in the randomly initialized model - yet the gradients of the BatchNorm parameters change _more_. Overall, the findings in this section thus highlight that while there may be a connection between gradient stabilization and LMC, it cannot fully explain it - suggesting that further investigation into the phenomenon using this lens, particularly into the role of BatchNorm layers, may be fruitful.

_Takeaway Case Study 3._ Reasoning through the learning process by telescoping out functional updates suggests that averaging model parameters trained from the same checkpoint can be effective if their models' gradients remain stable, however, this cannot fully explain LMC in the setting we consider.

## 5 The Effect of Design Choices on Linearized Functional Updates

The literature on the neural tangent kernel primarily considers plain SGD, while modern deep learning practice typically relies on a range of important modifications to the training process (see e.g. [14, Ch. 6]) - this includes many of the experiments demonstrating surprising deep learning phenomena we examined in Sec. 4. To enable us to use modern optimizers above, we derived their implied linearized functional updates through the weight-averaging representation \(\Delta\tilde{f}_{t}(\mathbf{x})=\nabla_{\theta}\bm{\theta}_{\theta_{t-1}}( \mathbf{x})^{\top}\Delta\bm{\theta}_{t}\), which in turn allows us to define \(K_{t}^{T}(\cdot,\cdot)\) in Eq. (5) for these modifications using straightforward algebra. As a by-product, we found that this provides us with an interesting and pedagogical formalism to reason about the relative effect of different design choices in neural network training, and elaborate on selected learnings below.

\(\bullet\)**Momentum** with scalar hyperparameter \(\beta_{1}\) smoothes weight updates by employing an exponentially weighted average over the previous parameter gradients as \(\Delta\bm{\theta}_{t}=-\gamma_{t}\frac{1-\beta_{1}}{1-\beta_{1}^{\top}}\sum_ {k=1}^{t}\beta_{1}^{t-k}\mathbf{T}_{k}\mathbf{g}_{k}^{\ell}\) instead of using the current gradients alone. This implies linearized functional updates

\[\Delta\tilde{f}_{t}(\mathbf{x})=-\gamma_{t}\frac{1-\beta_{1}}{1-\beta_{1}^{ \top}}\sum_{i\in[n]}(K_{t}^{\bm{\theta}}(\mathbf{x},\mathbf{x}_{i})g_{it}^{ \ell}+\sum_{k=1}^{t-1}\beta_{1}^{t-k}K_{t,k}^{\bm{\theta}}(\mathbf{x},\mathbf{ x}_{i})g_{ik}^{\ell})\] (11)

where \(K_{t,k}^{\bm{\theta}}(\mathbf{x},\mathbf{x}_{i})\coloneqq\frac{\mathbf{1}\{i \in E_{k}\}}{|B_{k}|}\nabla_{\theta}\bm{\theta}_{t-1}(\mathbf{x})^{\top} \nabla_{\theta}\bm{\theta}_{b_{k-1}}(\mathbf{x}_{i})\) denotes the cross-temporal tangent kernel. Thus, the functional updates also utilize _previous_ loss gradients, where their weight is determined

Figure 6: **Linear mode connectivity and gradient changes by \(t^{\prime}\). (1) Decrease in accuracy when using averaged weights \(\alpha\bm{\theta}_{1T}^{\prime}+(1-\alpha)\bm{\theta}_{2T}^{\prime}\) for randomly initialized (orange) and pre-trained ResNet-20 (green). (2) & (3) Changes in model gradients by layer for a randomly initialized (2) and pretrained (3) model.**

using an inner product of the model gradient features from different time steps. If \(\nabla_{\bm{\theta}}f_{\bm{\theta}_{t}}(\mathbf{x})\) is constant throughout training and we use full-batch GD, then the contribution of each training example \(i\) to \(\Delta\tilde{f}_{t}(\mathbf{x})\) reduces to \(-\gamma_{t}K_{0}^{\bm{\theta}}(\mathbf{x},\mathbf{x}_{i})\frac{1-\beta_{k}}{1- \beta_{l}}[\sum_{k=1}^{d}\beta_{i}^{t-k}g_{ik}^{\ell}]\), an exponentially weighted moving average over its past loss gradients - making the effect of momentum on functional updates analogous to its effect on updates in parameter space. However, if \(\nabla_{\bm{\theta}}f_{\bm{\theta}_{t}}(\mathbf{x})\) changes over time, it is e.g. possible that \(K_{k,t}^{\bm{\theta}}(\mathbf{x},\mathbf{x}_{i})\) has opposite sign from \(K_{t}^{\bm{\theta}}(\mathbf{x},\mathbf{x}_{i})\) in which case momentum reduces instead of amplifies the effect of a previous \(g_{it}^{\ell}\). This is more obvious when re-writing Eq. (11) to collect all terms containing a specific \(g_{it}^{\ell}\), leading to \(K_{t}^{T}(\mathbf{x},\mathbf{x}_{i})=\sum_{k=t}^{T}\gamma_{k}\frac{1-\beta_{ 1}}{1-\beta_{1}^{\ell}}\beta_{1}^{k-t}K_{k,t}(\mathbf{x},\mathbf{x}_{i})\) for Eq. (5).

\(\bullet\) **Weight decay** with scalar hyperparameter \(\lambda\) uses \(\Delta\bm{\theta}_{t}=-\gamma_{t}(\mathbf{T}_{t}\mathbf{g}_{t}^{\ell}+ \lambda\bm{\theta}_{t-1})\). For constant learning rate \(\gamma\) this gives \(\bm{\theta}_{t}=\bm{\theta}_{0}-\sum_{k=1}^{t}\gamma(\mathbf{T}_{t}\mathbf{g} _{k}^{\ell}+\lambda\bm{\theta}_{k-1})=(1-\lambda\gamma)^{t}\bm{\theta}_{0}- \gamma\sum_{k=1}^{t}(1-\lambda\gamma)^{t-k}\mathbf{T}_{k}\mathbf{g}_{k}^{\ell}\). This then implies linearized functional updates

\[\begin{split}\Delta\tilde{f}_{t}(\mathbf{x})=-\gamma\sum_{i\in[ n]}(K_{t}(\mathbf{x},\mathbf{x}_{i})g_{it}^{\ell}-\lambda\gamma\sum_{k=1}^{t-1}(1- \lambda\gamma)^{t-1-k}K_{t,k}(\mathbf{x},\mathbf{x}_{i})g_{ik}^{\ell})\\ -\gamma\lambda(1-\lambda\gamma)^{t-1}\nabla_{\bm{\theta}}\bm{ \theta}_{\bm{\theta}_{t-1}}(\mathbf{x})^{\top}\bm{\theta}_{0}\end{split}\] (12)

For full-batch GD and constant tangent kernels, \(-\gamma K_{0}^{\bm{\theta}}(\mathbf{x},\mathbf{x}_{i})[g_{it}-\lambda\gamma \sum_{k=1}^{t-1}(1-\lambda\gamma)^{t-1-k}g_{ik}]\) is the contribution of each training example to the functional updates, which effectively decays the previous contributions of this example. Further, comparing the signs in Eq. (12) to Eq. (11) highlights that momentum can _offset_ the effect of weight decay on the learned updates in function space (in which case weight decay mainly acts through the term decaying the initial weights \(\bm{\theta}_{0}\)).

\(\bullet\) **Adaptive & parameter-dependent learning rates** are another important modification in practice which enable the use of different step-sizes across parameters by dividing \(\Delta\bm{\theta}_{t}\)_elementwise_ by a \(p\times 1\) scaling vector \(\bm{\phi}_{t}\). Most prominently, this is used to adaptively normalize the magnitude of updates (e.g. Adam [1] uses \(\bm{\phi}_{t}=\sqrt{\frac{1-\beta_{2}}{1-\beta_{2}^{\ell}}\sum_{k=1}^{d}\beta_ {2}^{\ell-k}[\mathbf{T}_{k}\mathbf{g}_{k}^{\ell}]^{2}+\epsilon}\)). When combined with plain SGD, this results in kernel \(K_{t}^{\bm{\phi}}(\mathbf{x},\mathbf{x}_{i})=\frac{1\left\{i\in B_{t}\right\}}{ \left|B_{t}\right|}\nabla_{\bm{\theta}}f_{\bm{\theta}_{t-1}}(\mathbf{x})^{\top }\text{diag}(\frac{1}{\bm{\phi}_{t}})\nabla_{\bm{\theta}}f_{\bm{\theta}_{t-1}} (\mathbf{x}_{i})\). This expression highlights that \(\bm{\phi}_{t}\) admits an elegant interpretation as _re-scaling the relative influence of features_ on the tangent kernel, similar to structured kernels in non-parametric regression [14, Ch. 6.4.1].

\(\bullet\) **Architecture design choices** also impact the form of the kernel. One important practical example is whether \(f_{\bm{\theta}}(\mathbf{x})\) applies a non-linear activation function to the output \(g_{\bm{\theta}}(\mathbf{x})\in\mathbb{R}\) of its final layer. Consider the choice of using the sigmoid \(\sigma(z)=\frac{1}{1+e^{-z}}\) for a binary classification problem and recall \(\frac{\partial}{\partial z}\sigma(z)=\sigma(z)(1-\sigma(z))\in(0,\nicefrac{{1 }}{{4}}]\), which is largest where \(\sigma(z)=\nicefrac{{1}}{{2}}\) and smallest when \(\sigma(z)\to 0\lor 1\). If \(K_{t}^{\bm{\theta},g}(\mathbf{x},\mathbf{x}_{i})\coloneqq\frac{\mathbf{1} \left\{i\in B_{t}\right\}}{\left|B_{t}\right|}\nabla_{\bm{\theta}}g_{\bm{ \theta}_{t-1}}(\mathbf{x})^{\top}\nabla_{\bm{\theta}}g_{\bm{\theta}_{t-1}}( \mathbf{x}_{i})\) denotes the tangent kernel of the model without activation, it is easy to see that the tangent kernel of the model \(\sigma(g_{\bm{\theta}_{t}}(\mathbf{x}))\) is

\[K_{t}^{\bm{\theta},\sigma}(\mathbf{x},\mathbf{x}_{i})=\sigma(g_{\bm{\theta}_{t}} (\mathbf{x}))(1-\sigma(g_{\bm{\theta}_{t}}(\mathbf{x})))\sigma(g_{\bm{\theta} _{t}}(\mathbf{x}_{i}))(1-\sigma(g_{\bm{\theta}_{t}}(\mathbf{x}_{i})))K_{t}^{ \bm{\theta},g}(\mathbf{x},\mathbf{x}_{i})\] (13)

indicating that \(K_{t}^{\bm{\theta},\sigma}(\mathbf{x},\mathbf{x}_{i})\) will give relatively higher weight in functional updates to training examples \(i\) for which the model is uncertain (\(\sigma(g(\mathbf{x}_{i}))\approx\nicefrac{{1}}{{2}})\)) and lower weight to examples where the model is certain (\(\sigma(g_{\bm{\theta}_{t}}(\mathbf{x}_{i}))\approx 0\lor 1\)) - _regardless_ of whether \(\sigma(g_{\bm{\theta}_{t}}(\mathbf{x}_{i}))\) is the correct label. Conversely, Eq. (13) also implies that when comparing the functional updates of \(\sigma(g_{\bm{\theta}}(\mathbf{x}))\) to those of \(g_{\bm{\theta}}(\mathbf{x})\) across inputs \(\mathbf{x}\in\mathcal{X}\), updates with \(\sigma(\cdot)\) will be relatively larger for \(\mathbf{x}\) where the model is uncertain (\(\sigma(g_{\bm{\theta}_{t}}(\mathbf{x}))\approx\nicefrac{{1}}{{2}})\)). Finally, Eq. (13) also highlights that the (post-activation) tangent kernel of a model with sigmoid activation will generally not be constant in \(t\) unless the model predictions \(\sigma(g_{\bm{\theta}_{t}}(\mathbf{x}))\) do not change.

## 6 Conclusion

This work investigated the utility of a telescoping model for neural network learning, consisting of a sequence of linear approximations, as a tool for understanding several recent deep learning phenomena. By revisiting existing empirical observations, we demonstrated how this perspective provides a lens through which certain surprising behaviors of deep learning can become more intelligible. In each case study, we intentionally restricted ourselves to specific, noteworthy empirical examples which we proceeded to re-examine in greater depth. We believe that there are therefore many interesting opportunities for future research to expand on these initial findings by building upon the ideas we present to investigate such phenomena in more generality, both empirically and theoretically.

#### Acknowledgements

We would like to thank James Bayliss, who first suggested to us to look into explicitly unravelling SGD updates to write trained neural networks as approximate smoothers to study deep double descent after a seminar on our paper [3] on non-deep double descent. This suggestion ultimately inspired many investigations far beyond the original double descent context. We are also grateful to anonymous reviewers for helpful comments and suggestions. AC and AJ gratefully acknowledge funding from AstraZeneca and the Cystic Fibrosis Trust, respectively. This work was supported by a G-Research grant, and Azure sponsorship credits granted by Microsoft's AI for Good Research Lab.

## References

* [ABNH23] Gul Sena Altintas, Gregor Bachmann, Lorenzo Noci, and Thomas Hofmann. Disentangling linear mode connectivity. In _UniReps: the First Workshop on Unifying Representations in Neural Models_, 2023.
* [ABPC23] Taiga Abe, E Kelly Buchanan, Geoff Pleiss, and John Patrick Cunningham. Pathologies of predictive diversity in deep ensembles. _Transactions on Machine Learning Research_, 2023.
* [AHS22] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. GitHub re-basin: Merging models modulo permutation symmetries. _arXiv preprint arXiv:2209.04836_, 2022.
* [AP20] Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained bias-variance decomposition. _Advances in neural information processing systems_, 33:11022-11032, 2020.
* [ASS20] Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of generalization error in neural networks. _Neural Networks_, 132:428-446, 2020.
* [BBL03] Olivier Bousquet, Stephane Boucheron, and Gabor Lugosi. Introduction to statistical learning theory. In _Summer school on machine learning_, pages 169-207. Springer, 2003.
* [BD10] Gerard Biau and Luc Devroye. On the layered nearest neighbour estimate, the bagged nearest neighbour estimate and the random forest method in regression and classification. _Journal of Multivariate Analysis_, 101(10):2499-2518, 2010.
* [Bel21] Mikhail Belkin. Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation. _Acta Numerica_, 30:203-248, 2021.
* [BHMM19] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias-variance trade-off. _Proceedings of the National Academy of Sciences_, 116(32):15849-15854, 2019.
* [BHX20] Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. _SIAM Journal on Mathematics of Data Science_, 2(4):1167-1180, 2020.
* [BLLT20] Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. _Proceedings of the National Academy of Sciences_, 117(48):30063-30070, 2020.
* [BM19] Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. _Advances in Neural Information Processing Systems_, 32, 2019.
* [BMM18] Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand kernel learning. In _International Conference on Machine Learning_, pages 541-549. PMLR, 2018.
* [BMR\({}^{+}\)20] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 2020.
* [BO96] Siegfried Bos and Manfred Opper. Dynamics of training. _Advances in Neural Information Processing Systems_, 9, 1996.
* [Bre01] Leo Breiman. Random forests. _Machine learning_, 45:5-32, 2001.

* [BSM\({}^{+}\)22] Frederik Benzing, Simon Schug, Robert Meier, Johannes Von Oswald, Yassir Akram, Nicolas Zucchet, Laurence Aitchison, and Angelika Steger. Random initialisations performing above chance and how to find them. _arXiv preprint arXiv:2209.07509_, 2022.
* [CJvdS23] Alicia Curth, Alan Jeffares, and Mihaela van der Schaar. A u-turn on double descent: Rethinking parameter counting in statistical learning. _Advances in Neural Information Processing Systems_, 36, 2023.
* [CJvdS24] Alicia Curth, Alan Jeffares, and Mihaela van der Schaar. Why do random forests work? understanding tree ensembles as self-regularizing adaptive smoothers. _arXiv preprint arXiv:2402.01502_, 2024.
* [CL21] Niladri S Chatterji and Philip M Long. Finite-sample analysis of interpolating linear classifiers in the overparameterized regime. _The Journal of Machine Learning Research_, 22(1):5721-5750, 2021.
* [CMBK21] Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi. Multiple descent: Design your own generalization curve. _Advances in Neural Information Processing Systems_, 34:8898-8912, 2021.
* [COB19] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. _Advances in neural information processing systems_, 32, 2019.
* [Cur24] Alicia Curth. Classical statistical (in-sample) intuitions don't generalize well: A note on bias-variance tradeoffs, overfitting and moving from fixed to random designs. _arXiv preprint arXiv:2409.18842_, 2024.
* [CVSK22] Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. Fusing finetuned models for better pretraining. _arXiv preprint arXiv:2204.03044_, 2022.
* [Die02] Thomas G Dietterich. Ensemble learning. _The handbook of brain theory and neural networks_, 2(1):110-125, 2002.
* [DLL\({}^{+}\)19] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In _International conference on machine learning_, pages 1675-1685. PMLR, 2019.
* [DLM20] Michal Derezinski, Feynman T Liang, and Michael W Mahoney. Exact expressions for double descent and implicit regularization via surrogate random design. _Advances in neural information processing systems_, 33:5152-5164, 2020.
* [Dom20] Pedro Domingos. Every model learned by gradient descent is approximately a kernel machine. _arXiv preprint arXiv:2012.00152_, 2020.
* [dRBK20] Stephane d'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in double descent: Bias and variance (s) in the lazy regime. In _International Conference on Machine Learning_, pages 2280-2290. PMLR, 2020.
* [DVSH18] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In _International conference on machine learning_, pages 1309-1318. PMLR, 2018.
* [ESSN21] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. _arXiv preprint arXiv:2110.06296_, 2021.
* [FB16] C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. _arXiv preprint arXiv:1611.01540_, 2016.
* [FDP\({}^{+}\)20] Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. _Advances in Neural Information Processing Systems_, 33:5850-5861, 2020.
* [FDRC20] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In _International Conference on Machine Learning_, pages 3259-3269. PMLR, 2020.
* [FHL19] Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspective. _arXiv preprint arXiv:1912.02757_, 2019.

* [Fri01] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. _Annals of statistics_, pages 1189-1232, 2001.
* [GBD92] Stuart Geman, Elie Bienenstock, and Rene Doursat. Neural networks and the bias/variance dilemma. _Neural computation_, 4(1):1-58, 1992.
* [GIP\({}^{+}\)18] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. _Advances in neural information processing systems_, 31, 2018.
* [GK24] Samuel James Greydanus and Dmitry Kobak. Scaling down deep learning with mnist-1d. In _Forty-first International Conference on Machine Learning_, 2024.
* [GMMM19] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy training of two-layers neural network. _Advances in Neural Information Processing Systems_, 32, 2019.
* [GOV22] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? _Advances in neural information processing systems_, 35:507-520, 2022.
* [GPK22] Eugene Golikov, Eduard Pokonechnnyy, and Vladimir Korviakov. Neural tangent kernel: A survey. _arXiv preprint arXiv:2208.13614_, 2022.
* [GSJW20] Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy training in deep neural networks. _Journal of Statistical Mechanics: Theory and Experiment_, 2020(11):113301, 2020.
* [HHLS24] Moritz Haas, David Holzmuller, Ulrike Luxburg, and Ingo Steinwart. Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension. _Advances in Neural Information Processing Systems_, 36, 2024.
* [HMRT22] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. _The Annals of Statistics_, 50(2):949-986, 2022.
* [HT90] Trevor Hastie and Robert Tibshirani. Generalized additive models. _Monographs on statistics and applied probability. Chapman & Hall_, 43:335, 1990.
* [HTF09] Trevor Hastie, Robert Tibshirani, and Jerome H Friedman. _The elements of statistical learning: data mining, inference, and prediction_, volume 2. Springer, 2009.
* [HXZQ22] Zheng He, Zeke Xie, Quanzhi Zhu, and Zengchang Qin. Sparse double descent: Where network pruning aggravates overfitting. In _International Conference on Machine Learning_, pages 8635-8659. PMLR, 2022.
* [HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [Ide] Yerlan Idelbayev. Proper ResNet implementation for CIFAR10/CIFAR100 in PyTorch. https://github.com/akamaster/pytorch_resnet_cifar10. Accessed: 2024-05-15.
* [IPG\({}^{+}\)18] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. _arXiv preprint arXiv:1803.05407_, 2018.
* [IWG\({}^{+}\)22] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. _Advances in Neural Information Processing Systems_, 35:29262-29277, 2022.
* [JGH18] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* [JLCvdS24] Alan Jeffares, Tennison Liu, Jonathan Crabbe, and Mihaela van der Schaar. Joint training of deep ensembles fails due to learner collusion. _Advances in Neural Information Processing Systems_, 36, 2024.

* [KAF\({}^{+}\)24] Devin Kwok, Nikhil Anand, Jonathan Frankle, Gintare Karolina Dziugaite, and David Rolnick. Dataset difficulty and the role of inductive bias. _arXiv preprint arXiv:2401.01867_, 2024.
* [KB14] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [KBGP24] Tanishq Kumar, Blake Bordelon, Samuel J Gershman, and Cengiz Pehlevan. Grokking as the transition from lazy to rich training dynamics. In _The Twelfth International Conference on Learning Representations_, 2024.
* [KH\({}^{+}\)09] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [KSH12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Advances in neural information processing systems_, 25, 2012.
* [LBBH98] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [LBBS24] Noam Levi, Alon Beck, and Yohai Bar-Sinai. Grokking in linear estimators-a solvable model that groks without understanding. _International Conference on Learning Representations_, 2024.
* [LD21] Licong Lin and Edgar Dobriban. What causes the test error? going beyond bias-variance via anova. _The Journal of Machine Learning Research_, 22(1):6925-7006, 2021.
* [LH17] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [LJ06] Yi Lin and Yongho Jeon. Random forests and adaptive nearest neighbors. _Journal of the American Statistical Association_, 101(474):578-590, 2006.
* [LJL\({}^{+}\)24] Kaifeng Lyu, Jikai Jin, Zhiyuan Li, Simon Shaolei Du, Jason D Lee, and Wei Hu. Dichotomy of early and late phase implicit biases can provably induce grokking. In _The Twelfth International Conference on Learning Representations_, 2024.
* [LKN\({}^{+}\)22] Ziming Liu, Ouail Kitouni, Niklas S Nolte, Eric Michaud, Max Tegmark, and Mike Williams. Towards understanding grokking: An effective theory of representation learning. _Advances in Neural Information Processing Systems_, 35:34651-34663, 2022.
* [LMT22] Ziming Liu, Eric J Michaud, and Max Tegmark. Omnigrok: Grokking beyond algorithmic data. In _The Eleventh International Conference on Learning Representations_, 2022.
* [LPB17] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. _Advances in neural information processing systems_, 30, 2017.
* [LSP\({}^{+}\)20] Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. _Advances in Neural Information Processing Systems_, 33:15156-15172, 2020.
* [LVM\({}^{+}\)20] Marco Loog, Tom Viering, Alexander Mey, Jesse H Krijthe, and David MJ Tax. A brief prehistory of double descent. _Proceedings of the National Academy of Sciences_, 117(20):10625-10626, 2020.
* [LXS\({}^{+}\)19] Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. _Advances in neural information processing systems_, 32, 2019.
* [LZB20] Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. _Advances in Neural Information Processing Systems_, 33:15954-15964, 2020.
* [Mac91] David MacKay. Bayesian model comparison and backprop nets. _Advances in neural information processing systems_, 4, 1991.

* [MBB18] Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effectiveness of sgd in modern over-parametrized learning. In _International Conference on Machine Learning_, pages 3325-3334. PMLR, 2018.
* [MBS23] Mohamad Amin Mohamadi, Wonho Bae, and Danica J Sutherland. A fast, well-founded approximation to the empirical neural tangent kernel. In _International Conference on Machine Learning_, pages 25061-25081. PMLR, 2023.
* [MBW20] Wesley J Maddox, Gregory Benton, and Andrew Gordon Wilson. Rethinking parameter counting in deep models: Effective dimensionality revisited. _arXiv preprint arXiv:2003.02139_, 2020.
* [MKV\({}^{+}\)23] Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C, Ganesh Ramakrishnan, Micah Goldblum, and Colin White. When do neural nets outperform boosted trees on tabular data? _Advances in Neural Information Processing Systems_, 36, 2023.
* [MOB24] Jack Miller, Charles O'Neill, and Thang Bui. Grokking beyond neural networks: An empirical exploration with model complexity. _Transactions on Machine Learning Research (TMLR)_, 2024.
* [Moo91] John Moody. The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. _Advances in neural information processing systems_, 4, 1991.
* [MSA\({}^{+}\)22] Neil Mallinar, James Simon, Amirhesam Abedsoltan, Parthe Pandit, Misha Belkin, and Preetum Nakkiran. Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting. _Advances in Neural Information Processing Systems_, 35:1182-1195, 2022.
* [NCL\({}^{+}\)23] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. _arXiv preprint arXiv:2301.05217_, 2023.
* [Nea19] Brady Neal. On the bias-variance tradeoff: Textbooks need an update. _arXiv preprint arXiv:1912.08286_, 2019.
* [NKB\({}^{+}\)21] Preetum Nakkiran, Gal Kaplan, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. _Journal of Statistical Mechanics: Theory and Experiment_, 2021(12):124003, 2021.
* [NMB\({}^{+}\)18] Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks. _arXiv preprint arXiv:1810.08591_, 2018.
* [NSZ20] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? _Advances in neural information processing systems_, 33:512-523, 2020.
* [NVKM20] Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can mitigate double descent. _arXiv preprint arXiv:2003.01897_, 2020.
* [NWC\({}^{+}\)11] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. Reading digits in natural images with unsupervised feature learning. In _NIPS workshop on deep learning and unsupervised feature learning_, volume 2011, page 7. Granada, Spain, 2011.
* [OJFF24] Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. Task arithmetic in the tangent space: Improved editing of pre-trained models. _Advances in Neural Information Processing Systems_, 36, 2024.
* [OJMDF21] Guillermo Ortiz-Jimenez, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. What can linearized neural networks actually say about generalization? _Advances in Neural Information Processing Systems_, 34:8998-9010, 2021.
* [PBE\({}^{+}\)22] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. _arXiv preprint arXiv:2201.02177_, 2022.
* [Pri23] Simon JD Prince. _Understanding Deep Learning_. MIT press, 2023.

* [PVG\({}^{+}\)11] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* [RKR\({}^{+}\)22] Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. _Advances in Neural Information Processing Systems_, 35:10821-10836, 2022.
* [SGd\({}^{+}\)18] Stefano Spigler, Mario Geiger, Stephane d'Ascoli, Levent Sagun, Giulio Biroli, and Matthieu Wyart. A jamming transition from under-to over-parametrization affects loss landscape and generalization. _arXiv preprint arXiv:1810.09665_, 2018.
* [SIvdS23] Nabeel Seedat, Fergus Imrie, and Mihaela van der Schaar. Dissecting sample hardness: Fine-grained analysis of hardness characterization methods. In _The Twelfth International Conference on Learning Representations_, 2023.
* [SJ20] Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. _Advances in Neural Information Processing Systems_, 33:22045-22055, 2020.
* [SKR\({}^{+}\)23] Rylan Schaeffer, Mikail Khona, Zachary Robertson, Akhilan Boopathy, Kateryna Pistunova, Jason W Rocks, Ila Rani Fiete, and Oluwasanmi Koyejo. Double descent demystified: Identifying, interpreting & ablating the sources of a deep learning puzzle. _arXiv preprint arXiv:2303.14151_, 2023.
* [TLZ\({}^{+}\)22] Vimal Thilak, Etai Littwin, Shuangfei Zhai, Omid Saremi, Roni Paiss, and Joshua Susskind. The slingshot mechanism: An empirical study of adaptive optimizers and the grokking phenomenon. _arXiv preprint arXiv:2206.04817_, 2022.
* [Vap95] Vladimir Vapnik. _The nature of statistical learning theory_. Springer science & business media, 1995.
* [VCR89] F Vallet, J-G Cailton, and Ph Refregier. Linear and nonlinear extension of the pseudo-inverse solution for learning boolean functions. _Europhysics Letters_, 9(4):315, 1989.
* [VSK\({}^{+}\)23] Vikrant Varma, Rohin Shah, Zachary Kenton, Janos Kramar, and Ramana Kumar. Explaining grokking through circuit efficiency. _arXiv preprint arXiv:2309.02390_, 2023.
* [VvRBT13] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. Openml: networked science in machine learning. _SIGKDD Explorations_, 15(2):49-60, 2013.
* [WIG\({}^{+}\)22] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _International Conference on Machine Learning_, pages 23965-23998. PMLR, 2022.
* [WOBM17] Abraham J Wyner, Matthew Olson, Justin Bleich, and David Mease. Explaining the success of adaboost and random forests as interpolating classifiers. _Journal of Machine Learning Research_, 18(48):1-33, 2017.
* [YHT\({}^{+}\)21] Yaoqing Yang, Liam Hodgkinson, Ryan Theisen, Joe Zou, Joseph E Gonzalez, Kannan Ramchandran, and Michael W Mahoney. Taxonomizing local versus global structure in neural network loss landscapes. _Advances in Neural Information Processing Systems_, 34:18722-18733, 2021.

[MISSING_PAGE_FAIL:17]

**Understanding modern model complexity.** Many measures for model complexity capture some form of _capacity_ of a hypothesis class, which gives insight into the most complex function that _could be_ learned - e.g. raw parameter counts and VC dimensions [1]. The double descent and benign overfitting phenomena prominently highlighted that complexity measures that consider only what _could_ be learned and not what _is actually_ learned for test examples, would be unlikely to help understand generalization in deep learning [1]. Further, [9] highlighted that many other measures for model complexity - so-called measures of effective parameters (or: degrees of freedom) including measures from the literature of smoothers [14, Ch. 3.5] as well as measures relying on the model's Hessian [15, 16] (which have been considered for use in deep learning in [17]) - were derived in the context of in-sample prediction (where train- and test inputs would be the same) and do thus not allow to distinguish differences in the behavior of learned functions on training examples from new examples. [18] highlight that this difference in setting - the move from in-sample prediction to measuring performance in terms of out-of-sample generalization - is crucial for the emergence of apparently counterintuitive modern machine learning phenomena such as double descent and benign overfitting. For this reason, [9] proposed an adapted effective parameter measure for smoothers that can distinguish the two, and highlighted that differentiating between the amount of smoothing performed on train- vs test examples is crucial to understanding double descent in linear regression, trees and gradient boosting. In Sec. 4.1, we show that the telescoping model makes it possible to use [9]'s effective parameter measure for neural networks, allowing interesting insight into implied differences in train- and test-time complexity of neural networks.

**Grokking.** Similar to double descent in the number of training epochs as observed in [13] (where the test error first improves then gets worse and then improves again during training), the _grokking_ phenomenon [12] demonstrated the emergence of another type of unexpected behavior during the training run of a single model. Originally demonstrated on arithmetic tasks, the phenomenon highlights that improvements in test performance can sometimes occur long after perfect training performance has already been achieved. [10] later demonstrated that this can also occur on more standard tasks such as image classification. This phenomenon has attracted much recent attention both because it appears to challenge the common practice of early stopping during training and because it showcases further gaps in our current understanding of learning dynamics. A number of explanations for this phenomenon have been put forward recently: [11] attribute grokking to delayed learning of representations, [11] use mechanistic explanations to examine case studies of grokking, [12] attribute grokking to more efficient circuits being learned later in training, [10] attribute grokking to the effects of weight decay setting in later in training and [10] attribute grokking to the use of adaptive optimizers. [1] highlight that the latter two explanations cannot be the sole reason for grokking by constructing an experiment where grokking occurs as the weight norm grows without the use of adaptive optimizers. Instead, [1], LJL\({}^{+}\)2] conjecture that grokking occurs as a model transitions from the lazy regime to a feature learning regime later in training. Finally, [12] show analytically and experimentally that grokking can also occur in simple linear estimators, and [15] similarly study grokking outside neural networks, including Bayesian models. Our perspective presented in Sec. 4.1 is complementary to these lines of work: we highlight that grokking coincides with the widening of a gap in effective parameters used for training and testing examples and that there is thus a quantifiable benign overfitting effect at play.

### Weight averaging in deep learning (Sec. 4.3)

Ensembling [15], i.e. averaging the _predictions_ of multiple independent models, has long established itself as a popular strategy to improve prediction performance over using single individual models. While ensembles have historically been predominantly implemented using weak base learners like trees to form random forests [13], _deep_ ensembles [13] - i.e. ensembles of neural networks - have more recently emerged as a popular strategy for improving upon the performance of a single network [13, 14]. Interestingly, deep ensembles have been shown to perform well both when averaging the predictions of the underlying models and when averaging the pre-activations of the final network layers [10].

A much more surprising empirical observation made in recent years is that, instead of averaging model predictions as in an ensemble, it is sometimes also possible to average the learned _weights_\(\bm{\theta}_{1}\) and \(\bm{\theta}_{2}\) of two trained neural networks and obtain a model that performs well [13, 12].

This is unexpected because neural networks are _highly nonlinear_ functions of their weights, so it is unclear a priori when and why averaging two sets of weights would lead to a sensible model at all. When weight averaging works, it is a much more attractive solution relative to ensembling: an ensemble consisting of \(k\) models requires \(k\times p\) model parameters, while a weight-averaged model requires only \(p\) parameters - making weight-averaged models both more efficient in terms of storage and at inference time. Additionally, weight averaging has interesting applications in federated learning because it could enable the merging of models trained on disjoint datasets. [18] were the first to demonstrate that weight averaging can work in the context of neural networks by showing that model weights obtained by simple averaging of multiple points along the trajectory of SGD during training - a weight-space version of the method of fast geometric ensembling [17] - could improve upon using the final solution directly.

**Mode connectivity.** The literature on mode connectivity first empirically demonstrated that there are simple (but nonlinear) paths of nonincreasing loss connecting different final network weights obtained from different random initializations [18, 19, 17]. As discussed in the main text, [18] then demonstrated empirically that two learned sets of weights can sometimes be _linearly_ connected by simply interpolating between the learned weights, as long as two models were trained together until some stability point \(t^{*}\). [1] perform an empirical study investigating which networks and optimization protocols lead to mode connectivity from initialization (i.e. \(t^{*}=0\)) and which modifications ensure \(t^{*}>0\). As highlighted in Sec. 4.3, our theoretical reasoning indicates that one sufficient condition for linear mode connectivity from initialization is that models stay in a regime in which the model gradients do not change during training. In the context of _task arithmetic_, where parameters from models finetuned on separate tasks are added or subtracted (_not_ averaged) to add or remove a skill, [19] find that pretrained CLIP models that are finetuned on separate tasks and allow to perform task arithmetic _do not_ operate in a regime in which gradients are constant.

**Methods that average weights.** Beyond [18]'s stochastic weight averaging method, which averages weights from checkpoints within a single training run, weight averaging has also recently gained increased popularity in the context of averaging multiple models finetuned from the same pre-trained model [20, 21, 22]: while [20] showed that multiple models finetuned from the same pretrained model lie in the same loss basin and are linearly mode connectible, the model soups method of [21] highlighted that simply averaging the weights of multiple models fine-tuned from the same pre-trained parameters with different hyperparameters leads to performance improvements over choosing the best individual fine-tuned model. A number of methods have since been proposed that use weight-averaging of models fine-tuned from the same pretrained model for diverse purposes (e.g. [17, 20]). Our results in Sec. 4.3 complement the findings of [20] by investigating whether fine-tuning from a pre-trained model leads to better mode connectivity because the gradients of a pre-trained model remain more stable than those trained from a random initialization.

**Weight averaging after permutation matching.** Most recently, a growing number of papers have investigated whether attempts to merge models through weight-averaging can be improved by first performing some kind of permutation matching that corrects for potential permutation symmetries in neural networks. [16] conjecture that all solutions learned by SGD are linearly mode connectible once permutation symmetries are corrected for. [16, 21, 2] use different methods for permutation matching and find that this improves the quality of weight-averaged models.

## Appendix B Additional theoretical results

### Derivation of smoother expressions using the telescoping model

Below, we explore how we can use the telescoping model to express a function learned by a neural network as \(\tilde{f}_{\bm{\theta}_{t}}(\mathbf{x})=\mathbf{s}_{\bm{\theta}_{t}}(\mathbf{ x})\mathbf{y}+c^{0}_{\bm{\theta}_{t}}(\mathbf{x})\), where the \(1\times n\) vector \(\mathbf{s}_{\bm{\theta}_{t}}(\mathbf{x})\) is a function of the kernels \(\{K^{t}_{\ell^{\prime}}(\cdot,\cdot)\}_{t^{\prime}\leq t}\), and the scalar \(c^{0}_{\bm{\theta}_{t}}(\mathbf{x})\) is a function of the \(\{K^{t}_{\ell^{\prime}}(\cdot,\cdot)\}_{t^{\prime}\leq t}\) and the networks' initialization \(f_{\bm{\theta}_{0}}(\cdot)\). Note that, as discussed further in the remark at the end of this section, the kernels \(K^{t}_{\ell^{\prime}}(\cdot,\cdot)\) for \(t>1\) are _data-adaptive_ as they can change throughout training.

**Vanilla SGD.** Recall that letting \(\mathbf{y}=[y_{1},\dots,y_{n}]^{\top}\) and \(\mathbf{f}_{\bm{\theta}_{t}}=[f_{\bm{\theta}_{t}}(\mathbf{x}_{1}),\dots,f_{ \bm{\theta}_{t}}(\mathbf{x}_{n})]\), the SGD weight update with squared loss \(\ell(f(\mathbf{x}),y)=\frac{1}{2}(y-f(\mathbf{x}))^{2}\), in the special case of single outputs \(k=1\), simplifies to \(\Delta\bm{\theta}_{t}=\gamma_{t}\mathbf{T}_{t}(\mathbf{y}-\mathbf{f}_{\bm{ \theta}_{t-1}})\), where \(\mathbf{T}_{t}\) is the \(p\times n\) matrix \(\mathbf{T}_{t}=[\frac{1\{1\in B_{t}\}}{|B_{t}|}\nabla_{\bm{\theta}}f_{\bm{\theta}_{t -1}}(\mathbf{x}),\ldots,\frac{1\{n\in B_{t}\}}{|B_{t}|}\nabla_{\bm{\theta}}f_{ \bm{\theta}_{t-1}}(\mathbf{x})]\). If we assume that the telescoping model holds exactly, this implies functional updates \(\Delta\tilde{f}_{t}(\mathbf{x})=\gamma_{t}\nabla_{\bm{\theta}}f_{\bm{\theta}_{ t-1}}(\mathbf{x})^{\top}\mathbf{T}_{t}(\mathbf{y}-\tilde{\bm{\mathbf{\tilde{f}}}}_{ \bm{\theta}_{t-1}})\). If we could write \(\tilde{\bm{\mathbf{f}}}_{\bm{\theta}_{t-1}}=\bm{\mathbf{S}}_{\bm{\theta}_{t-1} }\mathbf{y}+\mathbf{c}_{\bm{\theta}_{t-1}}\), then we would have

\[\Delta\tilde{\bm{\mathbf{f}}}_{t}(\mathbf{x})=\gamma_{t}\nabla_{\bm{\theta}}f _{\bm{\theta}_{t-1}}(\mathbf{x})^{\top}\mathbf{T}_{t}(\mathbf{y}-(\mathbf{S} _{\bm{\theta}_{t-1}}\mathbf{y}+\mathbf{c}_{\bm{\theta}_{t-1}}))\] (14)

where \(\mathbf{I}_{n}\) is the \(n\times n\) identity matrix. Noting that we must have \(\mathbf{c}_{\bm{\theta}_{0}}=\mathbf{f}_{\bm{\theta}_{0}}\) and \(\mathbf{S}_{\bm{\theta}_{0}}=\mathbf{0}^{n\times n}\) at initialization, we can recursively substitute Eq. (14) into Eq. (5) which then allows to write the vector of training predictions as

\[\tilde{\bm{\mathbf{f}}}_{\bm{\theta}_{T}}=\underbrace{\left( \sum_{t=1}^{T}\left(\prod_{k=1}^{T-t}(\mathbf{I}_{n}-\gamma_{t+k}\tilde{ \mathbf{T}}_{t+k}^{\top}\mathbf{T}_{t+k})\right)\gamma_{t}\tilde{\mathbf{T}}_{ t}^{\top}\mathbf{T}_{t}\right)\mathbf{y}}_{\mathbf{s}_{\sigma_{T}}\mathbf{y}}\] (15) \[+\underbrace{\left(\prod_{k=0}^{T-1}(\mathbf{I}_{n}-\gamma_{T-k} \tilde{\mathbf{T}}_{T-k}^{\top}\mathbf{T}_{T-k})\right)\mathbf{f}_{\bm{\theta }_{0}}}_{\mathbf{c}_{\bm{\theta}_{T}}}\]

where the \(p\times n\) matrix \(\tilde{\mathbf{T}}_{t}=[\nabla_{\bm{\theta}}f_{\bm{\theta}_{t-1}}(\mathbf{x} _{1}),\ldots,\nabla_{\bm{\theta}}f_{\bm{\theta}_{t-1}}(\mathbf{x}_{n})]\) differs from \(\mathbf{T}_{t}\) only in that it includes _all_ training examples and is not normalized by batch size. Then note that Eq. (15) is indeed a function of the training labels \(\mathbf{y}\), the predictions at initialization \(\bm{\mathbf{f}}_{\bm{\theta}_{0}}\) and the model gradients \(\{\bar{\mathbf{T}}_{t}\}_{t=1}^{T}\) traversed during training (captured in the \(n\times n\) matrix \(\mathbf{S}_{\bm{\theta}_{T}}\) and the \(n\times 1\) vector \(\bm{c}_{\bm{\theta}_{T}}\)) _alone_. Similarly, we can also write the weight updates (and, by extension, the weights \(\bm{\theta}_{T}\)) using the same quantities, i.e. \(\Delta\bm{\theta}_{t}=\gamma_{t}\mathbf{T}_{t}(\mathbf{I}_{n}-\mathbf{S}_{\bm {\theta}_{t-1}})\mathbf{y}-\gamma_{t}\mathbf{T}_{t}\mathbf{c}_{\bm{\theta}_{t- 1}}\). By Eq. (5), this also implies that we can write predictions at _arbitrary_ test input points as a function of the same quantities:

\[\tilde{f}_{\bm{\theta}_{T}}(\mathbf{x})=\underbrace{\left(\sum_{t=1}^{T} \gamma_{t}\nabla_{\bm{\theta}}f_{\bm{\theta}_{t-1}}(\mathbf{x})^{\top}\mathbf{ T}_{t}(\mathbf{I}_{n}-\mathbf{S}_{\bm{\theta}_{t-1}})\right)\mathbf{y}}_{\mathbf{s}_{ \bm{\theta}_{T}}(\mathbf{x})\mathbf{y}}+\underbrace{\left(f_{\bm{\theta}_{0}}( \mathbf{x})-\sum_{t=1}^{T}\gamma_{t}\nabla_{\bm{\theta}}f_{\bm{\theta}_{t-1}} (\mathbf{x})^{\top}\mathbf{T}_{t}\mathbf{c}_{\bm{\theta}_{t-1}}\right)}_{ \mathbf{c}_{\bm{\theta}_{T}}(\mathbf{x})}\]

where the matrix \(\mathbf{S}_{\bm{\theta}_{t-1}}\) is as defined in Eq. (15), which indeed has \(\mathbf{s}_{\bm{\theta}_{t-1}}(\mathbf{x}_{i})\) as its \(i\)-th row (and analogously for \(\mathbf{c}_{\bm{\theta}_{t-1}}\)).

**General optimization strategies.** Adapting the previous expressions to enable the use of adaptive learning rates is straightforward and requires only inserting \(\text{diag}(\frac{1}{\bm{\mathbf{\phi}}_{t}})\mathbf{T}_{t}\) into the expression for \(\Delta\tilde{f}_{t}(\mathbf{x})\) instead of \(\mathbf{T}_{t}\) alone; then defining the matrices similarly proceeds by recursively unraveling updates using \(\Delta\tilde{f}_{t}(\mathbf{x})=\gamma_{t}\nabla_{\bm{\theta}}f_{\bm{\theta}_{ t-1}}(\mathbf{x})^{\top}\text{diag}(\frac{1}{\bm{\mathbf{\phi}}_{t}}) \mathbf{T}_{t}(\mathbf{y}-\tilde{\bm{\mathbf{f}}}_{\bm{\theta}_{t-1}})\). Both momentum and weight decay lead to somewhat more tedious updates and necessitate the introduction of additional notation. Let \(\Delta\mathbf{s}_{t}(\mathbf{x})=\mathbf{s}_{\bm{\theta}_{t}}(\mathbf{x})- \mathbf{s}_{\bm{\theta}_{t-1}}(\mathbf{x})\), with \(\mathbf{s}_{\bm{\theta}_{0}}(\mathbf{x})=\mathbf{0}^{1\times n}\) and \(\Delta c_{t}(\mathbf{x})=c_{\bm{\theta}_{t}}(\mathbf{x})-c_{\bm{\theta}_{t-1}}( \mathbf{x})\), with \(c_{\bm{\theta}_{0}}(\mathbf{x})=f_{\bm{\theta}_{0}}(\mathbf{x})\), so that \(\mathbf{s}_{\bm{\theta}_{T}}(\mathbf{x})=\sum_{t=1}^{T}\Delta\mathbf{s}_{t}( \mathbf{x})\) and \(\mathbf{c}_{\bm{\theta}_{T}}(\mathbf{x})=f_{\bm{\theta}_{0}}(\mathbf{x})+\sum_{t= 1}^{T}\Delta\mathbf{c}_{t}(\mathbf{x})\). Further, we can write

\[\Delta\tilde{f}_{t}(\mathbf{x})=\Delta\mathbf{s}_{t}(\mathbf{x})\mathbf{y}+ \mathbf{c}_{t}(\mathbf{x})=\gamma_{t}\nabla_{\bm{\theta}}f_{\bm{\theta}_{t-1}}( \mathbf{x})^{\top}\mathbf{U}_{t}^{S}\mathbf{y}+\gamma_{t}\nabla_{\bm{\theta}}f_{ \bm{\theta}_{t-1}}(\mathbf{x})^{\top}\mathbf{U}_{t}^{C}\] (16)

which means that to derive \(\mathbf{s}_{\bm{\theta}_{t}}(\mathbf{x})\) for each \(t\), we can use the weight update formulas to define the \(p\times n\) update matrix \(\mathbf{U}_{t}^{S}\) and the \(p\times 1\) update vector \(\mathbf{U}_{t}^{C}\) that can then be used to compute \(\Delta\mathbf{s}_{t}(\mathbf{x})\) as \(\gamma_{t}\nabla_{\bm{\theta}}f_{\bm{\theta}_{t-1}}(\mathbf{x})^{\top}\mathbf{U}_{t }^{S}\) and \(\Delta\mathbf{c}_{t}(\mathbf{x})\) as \(\gamma_{t}\nabla_{\bm{\theta}}f_{\bm{\theta}_{t-1}}(\mathbf{x})^{\top}\mathbf{U}_{t }^{C}\). For vanilla SGD,

\[\mathbf{U}_{t}^{S}=\mathbf{T}_{t}(\mathbf{I}_{n}-\mathbf{S}_{\bm{\theta}_{t-1}}) \text{ and }\mathbf{U}_{t}^{C}=-\mathbf{T}_{t}\mathbf{c}_{\bm{\theta}_{t-1}}\] (17)

while SGD with only adaptive learning rates uses

\[\mathbf{U}_{t}^{S}=\text{diag}(\frac{1}{\bm{\mathbf{\phi}}_{t}})\mathbf{T}_{t}( \mathbf{I}_{n}-\mathbf{S}_{\bm{\theta}_{t-1}})\text{ and }\mathbf{U}_{t}^{C}=-\text{diag}(\frac{1}{\bm{\mathbf{\phi}}_{t}})\mathbf{T}_{t} \mathbf{c}_{\bm{\theta}_{t-1}}\] (18)

Momentum, without other modifications, uses \(\mathbf{U}_{t}^{S}=\frac{1}{1-\beta_{t}^{\prime}}\tilde{\mathbf{U}}_{t}^{S}\) and \(\mathbf{C}_{t}^{S}=\frac{1}{1-\beta_{t}^{\prime}}\tilde{\mathbf{U}}_{t}^{C}\), where

\[\tilde{\mathbf{U}

with \(\tilde{\mathbf{U}}_{0}^{S}=\mathbf{0}^{p\times n}\) and \(\tilde{\mathbf{U}}_{0}^{S}=\mathbf{0}^{p\times 1}\).

Weight decay, without other modifications, uses

\[\mathbf{U}_{t}^{S}=\mathbf{T}_{t}(\mathbf{I}_{n}-\mathbf{S}_{\bm{\theta}_{t-1}}+ \lambda\mathbf{D}_{t}^{S})\text{ and }\mathbf{U}_{t}^{C}=-\mathbf{T}_{t}(\mathbf{c}_{\bm{\theta}_{t-1}}+ \lambda\mathbf{D}_{t}^{C})\] (20)

where \(\mathbf{D}_{t}^{S}=\gamma_{t-1}\mathbf{U}_{t-1}^{S}+(1-\lambda\gamma_{t-1}) \mathbf{D}_{t-1}^{S}\) and \(\mathbf{D}_{t}^{C}=\gamma_{t-1}\mathbf{U}_{t-1}^{C}+(1-\lambda\gamma_{t-1}) \mathbf{D}_{t-1}^{C}\) with \(\mathbf{D}_{0}^{S}=\mathbf{0}^{p\times n}\) and \(\mathbf{D}_{0}^{C}=\bm{\theta}_{0}\).

Putting all together leads to AdamW [11] (which decouples weight decay and momentum, so that weight decay does not enter the momentum term), which uses

\[\mathbf{U}_{t}^{S}=\text{diag}(\frac{1}{\bm{\phi}_{t}})\frac{1}{1-\beta_{t}^ {t}}\tilde{\mathbf{U}}_{t}^{S}+\lambda\mathbf{T}_{t}\mathbf{D}_{t}^{S}\text{ and }\mathbf{C}_{t}^{S}=\frac{1}{1-\beta_{t}^{t}}\text{diag}(\frac{1}{\bm{\phi}_{t }})\tilde{\mathbf{U}}_{t}^{C}+\lambda\mathbf{T}_{t}\mathbf{D}_{t}^{C}\] (21)

where all terms are as in Eq. (19) and Eq. (20).

_Remark:_ Writing \(\tilde{\mathbf{f}}_{\bm{\theta}_{T}}=\mathbf{S}_{\bm{\theta}_{T}}\mathbf{y}+ \mathbf{c}_{\bm{\theta}_{T}}\) is reminiscent of a _smoother_ as used in the statistics literature [11]. Prototypical smoothers issue predictions \(\hat{\mathbf{y}}=\mathbf{S}\mathbf{y}\) - which include k-Nearest Neighbor regressors, kernel smoother, and (local) linear regression as prominent members -, and are usually _linear smoothers_ because \(\mathbf{S}\) does not depend on \(\mathbf{y}\). The smoother implied by the telescoping model is _not_ necessarily a linear smoother because \(\mathbf{S}_{\bm{\theta}_{T}}\) can depend on \(\mathbf{y}\) through changes in gradients during training, making \(\tilde{\mathbf{f}}_{\bm{\theta}_{T}}\) an _adaptive_ smoother. This adaptivity in the implied smoother is similar to trees as recently studied in [10], CJvdS24]. In this context, effective parameters as measured by \(p_{\mathbf{s}}^{0}\) can be interpreted as measuring how non-uniform and extreme the learned smoother weights are when issuing predictions for specific inputs [10].

Comparing predictions of ensemble and weight-averaged model after train-time transition into a constant-gradient regime

Here, we compare the predictions of the weight-averaged model \(f_{\alpha\bm{\theta}_{1T}^{t^{\prime}}+(1-\alpha)\bm{\theta}_{2T}^{t^{ \prime}}}(\mathbf{x})\) to the ensemble \(\bar{f}^{\alpha}(\mathbf{x})=\alpha f_{\alpha\bm{\theta}_{1T}^{t^{\prime}}}( \mathbf{x})+(1-\alpha)f_{\alpha\bm{\theta}_{2T}^{t^{\prime}}}(\mathbf{x})\) if the models transition into a lazy regime at time \(t^{*}\leq t^{\prime}\).

We begin by noting that the assumption that the gradients no longer change after \(t^{*}\) (i.e. \(\nabla_{\bm{\theta}}f_{\bm{\theta}_{t^{\prime}}}(\cdot)\approx\nabla_{\bm{ \theta}}f_{\bm{\theta}_{t}}(\cdot)\) for all \(t\geq t^{*}\)) implies that the rate of change of \(\nabla_{\bm{\theta}}f_{\bm{\theta}_{t^{*}}}(\mathbf{x})\) in the direction of the weight updates must be approximately \(\mathbf{0}\). That is, \(\nabla_{\bm{\theta}}^{2}f_{\bm{\theta}_{t^{*}}}(\mathbf{x})(\bm{\theta}-\bm{ \theta}_{t^{*}})\approx\mathbf{0}\) for all \(\bm{\theta}\in\Theta_{j}^{stable}\), or equivalently all weight changes in each \(\Theta_{j}^{stable}\) are in directions that are in the null-space of the Hessian (or in directions corresponding to diminishingly small eigenvalues). To avoid clutter in notation, we use splitting point \(t^{\prime}=t^{*}\) below, but note that the same arguments hold for \(t^{\prime}>t^{*}\).

First, we now consider rewriting the predictions of the ensemble, and note that we can now write the _second-order_ Taylor approximation of each model \(f_{\bm{\theta}_{jT}^{*}}(\mathbf{x})\) around \(\bm{\theta}_{t^{*}}\) as

\[\begin{split} f_{\bm{\theta}_{jT}^{*}}(\mathbf{x})=f_{\bm{ \theta}^{*}}(\mathbf{x})+\nabla_{\bm{\theta}}f_{\bm{\theta}_{t^{*}}}(\mathbf{x} )^{\top}&\sum_{t=t^{*}+1}^{T}\Delta\bm{\theta}_{jt}^{t^{*}}+ \underbrace{\frac{1}{2}\left[\sum_{t=t^{*}+1}^{T}\Delta\bm{\theta}_{jt}^{t^{*}} \right]^{\top}}_{\approx 0}\nabla_{\bm{\theta}}^{2}f_{\bm{\theta}_{t^{*}}}(\mathbf{x}) \left[\sum_{t=t^{*}+1}^{T}\Delta\bm{\theta}_{jt}^{t^{*}}\right]\\ &\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+R_{2 }(\sum_{t=t^{*}+1}^{T}\Delta\bm{\theta}_{jt}^{t^{*}})\end{split}\]

\[\approx f_{\bm{\theta}^{*}}(\mathbf{x})+\nabla_{\bm{\theta}}f_{\bm{\theta}_{t^{*} }}(\mathbf{x})^{\top}\sum_{t=t^{*}+1}^{T}\Delta\bm{\theta}_{jt}^{t^{*}}+R_{2 }(\sum_{t=t^{*}+1}^{T}\Delta\bm{\theta}_{jt}^{t^{*}})\]

where \(R_{2}(\sum_{t=t^{*}+1}^{T}\Delta\bm{\theta}_{jt}^{t^{*}})\) contains remainders of order 3 and above. Then the prediction of the ensemble can be written as

\[\begin{split}\bar{f}^{\alpha}(\mathbf{x})\approx f_{\bm{\theta}_{t^{* }}}(\mathbf{x})+f_{\bm{\theta}_{t^{*}}}(\mathbf{x})^{\top}&\sum_{t=t^ {*}+1}^{T}(\alpha\Delta\bm{\theta}_{1t}^{t^{*}}+(1-\alpha)\Delta\bm{\theta}_{2t}^ {t^{*}})\\ +\alpha R_{2}(\sum_{t=t^{*}+1}^{T}\Delta\bm{\theta}_{1t}^{t^{*}}) +(1-\alpha)R_{2}(\sum_{t=t^{*}+1}^{T}\Delta\bm{\theta}_{2t}^{t^{*}}))\end{split}\] (22)Now consider the weight-averaged model \(f_{\alpha\bm{\theta}_{1T}^{t^{\prime}}+(1-\alpha)\bm{\theta}_{2T}^{t^{\prime}}}( \mathbf{x})\). Note that we can always write \(\bm{\theta}_{jT}^{t^{\star}}=\bm{\theta}_{0}+\sum_{t=1}^{T}\Delta\bm{\theta}_{ jt}^{t^{\star}}=\bm{\theta}_{t^{\star}}+\sum_{t=t^{\star}+1}^{T}\Delta\bm{\theta}_{ jt}^{t^{\star}}\) and thus \(\alpha\bm{\theta}_{1T}^{t^{\star}}+(1-\alpha)\bm{\theta}_{2T}^{t^{\star}}=\bm{ \theta}_{t^{\star}}+\sum_{t=t^{\star}+1}^{T}\left(\alpha\Delta\bm{\theta}_{1t} ^{t^{\star}}+(1-\alpha)\Delta\bm{\theta}_{2t}^{t^{\star}}\right)\). Further, because \(\nabla_{\bm{\theta}}^{2}\bm{\theta}_{t^{\star}}(\mathbf{x})\sum_{t=t^{\star}+1 }^{T}\Delta\bm{\theta}_{tj}^{t^{\star}}\approx\bm{0}\) for each \(j\in\{0,1\}\), we also have that

\[\nabla_{\bm{\theta}}^{2}\bm{\theta}_{t^{\star}}(\mathbf{x})\left(\sum_{t=t^{ \star}+1}^{T}\alpha\Delta\bm{\theta}_{t1}+(1-\alpha)\Delta\bm{\theta}_{t2} \right)\approx\alpha\bm{0}+(1-\alpha)\bm{0}=\bm{0}\] (23)

Then, the second-order Taylor approximation of \(f_{\alpha\bm{\theta}_{1T}^{t^{\prime}}+(1-\alpha)\bm{\theta}_{2T}^{t^{\prime} }}(\mathbf{x})\) around \(\bm{\theta}_{t^{\star}}\) gives

\[\begin{split} f_{\alpha\bm{\theta}_{1T}^{t^{\prime}}+(1-\alpha) \bm{\theta}_{2T}^{t^{\prime}}}(\mathbf{x})\approx f_{\bm{\theta}_{t^{\star}}}( \mathbf{x})+\nabla_{\bm{\theta}}f_{\bm{\theta}_{t^{\star}}}(\mathbf{x})^{\top} \sum_{t=t^{\star}+1}^{T}(\alpha\Delta\bm{\theta}_{t1}+(1-\alpha)\Delta\bm{ \theta}_{t2})\\ +R_{2}(\sum_{t=t^{\star}+1}^{T}\Delta\bm{\theta}_{t1}+(1-\alpha) \Delta\bm{\theta}_{t2})\end{split}\] (24)

Thus, \(f_{\alpha\bm{\theta}_{1T}^{t^{\prime}}+(1-\alpha)\bm{\theta}_{2T}^{t^{\prime} }}(\mathbf{x})\approx\bar{f}^{\alpha}(\mathbf{x})\) up to remainder terms of third order and above.

## Appendix C Additional Experimental details

In this section, we provide a complete description of the experimental details throughout this work. Code is provided at https://github.com/alanjeffares/telescoping-lens. Each section also reports their respective required compute which was performed on either Azure VMs powered by 4 \(\times\) NVIDIA A100 GPUs or an NVIDIA RTX A4000 GPU.

### Case study 1 (Sec. 4.1) and approximation quality experiment (Sec. 3, Fig. 2)

Double descent experiments.In Fig. 3, we replicate [2, Sec. S.3.3]'s only binary classification experiment which used fully connected ReLU networks with a single hidden layer trained using the squared loss, _without_ sigmoid activation, on cat and dog images from CIFAR-10 [10]. Like [2], we grayscale and downsize images to \(d=8\times 8\) format and use \(n=1000\) training examples and use SGD with momentum \(\beta_{1}=0.95\). We use batch size \(100\) (resulting in \(B=10\) batches), learning rate \(\gamma=0.0025\), and test on \(n_{test}=1000\) held out examples. We train for up to \(e=30000\) epochs, but stop when training accuracy reaches \(100\%\) or when the training squared loss does not improve by more than \(10^{-4}\) for 500 consecutive epochs (the former strategy was also employed in [2], we additionally employ the latter to detect converged networks). We report results using \(\{1,2,5,7,10,15,20,25,30,35,40,45,50,55,70,85,100,200,500,1000,2000,5000\}\) hidden units. We repeat the experiment for 4 random seeds and report mean and standard errors in all figures.

In Appendix D.2, we additionally repeat this experiment with the same hyperparameters using MNIST images [1]. To create a binary classification task, we similarly train the model to distinguish 3-vs-5 from \(n=1000\) images downsampled to \(d=8\times 8\) format and test on \(1000\) examples. Likely because the task is very simple, we observe no deterioration in test error in this setting for any hidden size (see Fig. 9). Because [23] found that double descent can be more apparent in the presence of label noise, we repeat this experiment while adding \(20\%\) label noise to the training data, in which case the double descent shape in test error indeed emerges. As above, we repeat both experiments for 4 random seeds and report mean and standard errors in all figures.

Further, in Appendix D.2 we additionally utilize the MNIST-1D dataset [1] which was proposed recently as a sandbox for investigating empirical deep learning phenomena. We replicate a binary classification version of their MLP double descent experiment with added 15% label noise from [1] (which was itself adapted from the textbook [11]). We select only examples with label 0 and 1, and train fully connected neural networks with a single hidden layer with batch size 100, learning rate \(\gamma=0.01\) for 500 epochs, considering models with \([1,2,3,5,10,20,30,40,50,70,100,200,300,400]\) hidden units.

Compute: We train num_settings \(\times\) num_hidden_sizes \(\times\) num_seeds \((\approx 4\times 22\times 4=352)\) models for up to \(T=B\times e=300000\) gradient steps. Training times, which included all gradient computations to create the telescoping approximation, depended on the dataset and hidden sizes, but completing a single seed for all hidden sizes for one setting took an average of 36 hours.

Grokking experiments.In panel (1) of Fig. 4, we replicate the polynomial regression experiment from [13, Sec. 5] exactly. [14] use a neural network with a single hidden layer, using custom nonlinearities, of width \(n_{h}=500\) in which the weights of the final layer are fixed, that is they use

\[f_{\bm{\theta}}(\mathbf{x})=\frac{1}{n_{h}}\sum_{j=1}^{n_{h}}\phi(\bm{\theta}_ {j}^{\top}\mathbf{x})\text{ where }\phi(h)=h+\frac{\epsilon}{2}h^{2}\] (25)

Inputs \(x\in R^{d}\) are sampled from an isotropic Gaussian with variance \(\frac{1}{d}\) and targets \(y\) are generated as \(y(\mathbf{x})=\frac{1}{2}(\bm{\beta}^{\top}\mathbf{x})^{2}\). In this setup, \(\epsilon\) used in the activation function of the network controls how easy it is to fit the outcome function (the larger \(\epsilon\), the better aligned it is for the task at hand), which in turn controls whether grokking appears. In the main text, we present results using \(\epsilon=.2\); in Appendix D.2 we additionally present results using \(\epsilon=.05\) and \(\epsilon=0.5\). Like [14], we use \(d=100\), \(n_{train}=550\), \(n_{test}=500\), initialize all weights using standard normals, and train using full-batch gradient descent with \(\gamma=B=500\) on the squared loss. We repeat the experiment for 5 random seeds and report mean and standard errors in all figures.

In panel (2) of Fig. 4, we report an adapted version of [11]'s experiment reporting grokking on MNIST data. To enable the use of our model, we once more consider the binary classification task 3-vs-5 from \(n=1000\) images downsampled to \(d=8\times 8\) features and test on 1000 held-out examples. Like [11], we use a 3-layer fully connected ReLU network trained with squared loss (_without_ sigmoid activation) and larger than usual initialization by using \(\alpha\bm{\theta}_{0}\) instead of the default initialization \(\bm{\theta}_{0}\). We report \(\alpha=6\) in the main text and include results with \(\alpha=5\) and \(\alpha=7\) in Appendix D.2. Like [11] we use the AdamW optimizer [10] with batches of size \(200\), \(\beta_{1}=.9\) and \(\beta_{2}=.99\), and use weight decay \(\lambda=.1\). While [11] use learning rate \(10^{-3}\), we need to reduce this by factor 10 to \(\gamma=10^{-4}\) and additionally use linear learning rate warmup over the first 100 batches to ensure that weight updates are small enough to ensure the quality of the telescoping approximation; this is particularly critical because of the large initialization which otherwise results in instability in the approximation early in training. Panel (C) of Fig. 4 uses an identical setup but lets \(\alpha=1\) (i.e. standard initialization) and additionally applies a sigmoid to the output of the network. We repeat these experiments for 4 random seeds and report mean and standard errors in all figures.

Compute: Replicating [14]'s experiments required training \(\texttt{num\_settings}\times\texttt{num\_seeds}\) (\(3\times 5=15\)) models for \(T=100,000\) gradient steps. Each training run including all gradient computations took less than 1 hour to complete. Replicating [11]'s experiments required training \(\texttt{num\_settings}\times\texttt{num\_seeds}\) (\(3\times 4=12\)) for \(T=100,000\) gradient steps. Each training run including all gradient computations took around 5 hours to complete. The MNIST experiments with standard initialization required training \(\texttt{num\_settings}\times\texttt{num\_seeds}\) (\(2\times 4=8\)) for \(T=1000\) gradient steps, these took no more than 2 hours to complete in total.

Approximation quality experiment (Fig. 2)The approximation quality experiment uses the identical MNIST setup, training process and architecture as in the grokking experiments (differing only in that we use standard initialization \(\alpha\) and no learning rate warmup). In addition to the vanilla SGD and AdamW experiments presented in the main text, we present additional settings - using momentum alone, weight decay alone and using sigmoid activation - in Appendix D.1. In particular, we use the following hyperparameter settings for the different panels:

* _"SGD":_\(\lambda=0\), \(\beta_{1}=0\), no sigmoid.
* _"AdamW":_\(\lambda=0.1\), \(\beta_{1}=0.9\), \(\beta_{2}=.99\), no sigmoid.
* _"SGD + Momentum":_\(\lambda=0\), \(\beta_{1}=0.9\), no sigmoid.
* _"SGD + Weight decay":_\(\lambda=0.1\), \(\beta_{1}=0\), no sigmoid.
* _"SGD + \(\sigma(\cdot)\)":_\(\lambda=0\), \(\beta_{1}=0\), with sigmoid activation.

We repeat the experiment for 4 random seeds and report mean and standard errors in all figures.

Compute: Creating Fig. 7 required training num_settings\(\times\)num_seeds \((5\times 4=20)\) for \(T=5,000\) gradient steps. Each training run including all gradient computations took approximately 15 minutes to complete.

### Case study 2 (Sec. 4.2)

In Figs. 5 and 14 we provide results on tabular benchmark datasets from [1]. We select four datasets with > 20,000 examples (houses, superconduct, california, house_sales) to ensure there is sufficient hold-out data for evaluation across irregularity proportions. We apply standard preprocessing including log transformations of skewed features and target rescaling. As discussed in the main text, irregular examples are defined by first projecting each (normalized) dataset's input features onto its first principal component and then calculating each example's absolute distance to the empirical median in this space. We note that several recent works have discussed metrics of an examples irregularity or "hardness" (e.g. [12, 13]) finding the choice of metric to be highly context-dependent. Therefore we select a principal component prototypicality approach based on its simplicity and transparency. The top \(K\) irregular examples are removed from the data (these form the "irregular examples at test-time") and the remainder (the "regular examples") is split into training and testing. We then construct test datasets containing 4000 examples, constructed from a mixture of standard test examples and irregular examples according to each proportion \(p\).

We train both a standard neural network (while computing its telescoping approximation as described in Eq. (5)) and a gradient boosted tree model (using [23]) on the training data. We select hyperparameters by further splitting the training data to obtain a validation set of size 2000 and applying a random search consisting of 25 runs. We use the search spaces suggested in [1]. Specifically, for GBTs we consider \(\texttt{learning\_rate}\in\text{LogNormal}[\log(0.01),\log(10)]\), \(\texttt{num\_estimators}\in\text{LogUniformInt}[10.5,1000.5]\), and \(\texttt{max\_depth}\in[\text{None},2,3,4,5]\) with respective probabilities \([0.1,0.1,0.6,0.1,0.1]\). For the neural network, we consider \(\texttt{learning\_rate}\in\text{LogUniform}[1e-5,1e-2]\) and set \(\texttt{batch\_size}=128\), \(\texttt{num\_layers}=3\), and \(\texttt{hidden\_dim}=64\) with ReLU activations throughout. Each model is then trained on the full training set with its optimal parameters and is evaluated on each of test sets corresponding to the various proportions of irregular examples. All models are trained and evaluated for 4 random seeds and we report the mean and a standard error in our results.

As discussed in the main text, we report how the relative relative mean squared error of neural network and GBT (measured as \(\frac{MSE^{p}_{KB}-MSE^{p}_{GBT}}{MSE^{p}_{NN}-MSE^{p}_{GBT}}\)) changes as the proportion \(p\) of irregular examples increases and relate this to changes in \(\frac{1}{T}\sum_{t=1}^{T}\max_{j\in t_{\text{train}}}||\mathbf{k}_{t}(x_{j})||\), which measures how the kernels behave at their extreme during testing relative to the maximum of the equivalent values measured for the training examples such that the test values can be interpreted relative to the kernel at train time (i.e. values > 1 can be interpreted as being larger than the largest value observed across the entire training set).

Compute: The hyperparameter search results in \(\texttt{num\_searches}\times\texttt{num\_datasets}\times\texttt{num\_ models}\) (\(25\times 4\times 2=200\)) training runs and evaluations. Then the main experiment requires \(\texttt{num\_seeds}\times\texttt{num\_datasets}\times\texttt{num\_ models}(4\times 4\times 2=32)\) training runs and \(\texttt{num\_seeds}\times\texttt{num\_datasets}\times\texttt{num\_ models}\times\texttt{num\_ proportions}(4\times 4\times 2\times 5=160)\) evaluations. This results in a total of 232 training runs and 360 evaluations. Individual training and evaluation times depend on the model and dataset but generally require < 1 hour.

### Case study 3 (Sec. 4.3)

In Fig. 6 we follow the experimental setup described in [1]. Specifically, for each model we train for a total of 63,000 iterations over batches of size 128 with stochastic gradient descent. At a predetermined set of checkpoints (\(t^{\prime}\in[0,4,25,50,100,224,500,1000,2000,4472,10000,25100]\)) we create two copies of the current state of the network and train until completion with different batch orderings, where linear mode connectivity measurements are calculated. This process sometimes also referred to as _spawning_[1] and is repeated for 3 seeds at each \(t^{\prime}\). The entire process is repeated for 3 seeds resulting in a total of \(3\times 3=9\) total values over which we report the mean and a standard error. Momentum is set to 0.9 and a stepwise learning rate is applied beginning at 0.1 and decreasing by a factor of 10 at iterations 32,000 and 48,000. For the ResNet-20 architecture [13], we use an implementation from [Ide]. Experiments are conducted on CIFAR-10 [KH\({}^{+}\)09] where the inputs are normalized with random crops and random horizontal flips used as data augmentations.

Pretraining of the finetuned model model is performed on the SVHN dataset [NWC\({}^{+}\)11] which is also an image classification task with identically shaped input and output dimensions as CIFAR-10. We use a training setup similar to that of the CIFAR-10 model but set the number of training iterations to 30,000 and perform the stepwise decrease in learning rate at iterations 15,000 and 25,000 decaying by a factor of 5. Three models are trained following this protocol which achieve validation accuracy of 95.5%, 95.5%, and 95.4% on SVHN. We then repeat the CIFAR-10 training protocol for finetuning but parameterize the three initialization with the respective pretrained weights rather than random initialization. We also find that a shorter finetuning period is sufficient and therefore finetune for 12,800 steps with the learning rate decaying by a factor of 5 at steps 6,400 and 9,600.

Also following the protocol of [FDRC20], for each pair of trained spawned networks (\(f_{\bm{\theta}_{1}}\&f_{\bm{\theta}_{2}}\)) we consider interpolating their losses (i.e. \(\ell_{\alpha}^{\text{inv}}\coloneqq\alpha\cdot\ell(f_{\bm{\theta}_{1}}(\mathbf{ x}),y)+(1-\alpha)\cdot\ell(f_{\bm{\theta}_{2}}(\mathbf{x}),y)\)) and parameters (i.e. \(\ell_{\alpha}^{\text{inv}}\coloneqq\ell(f_{\bm{\theta}_{\text{inv}}}(\mathbf{ x}),y)\) where \(\bm{\theta}^{\text{inv}}=\alpha\bm{\theta}_{1}+(1-\alpha)\bm{\theta}_{2}\)) for 30 equally spaced values of \(\alpha\in[0,1]\). In the upper panel of Fig. 6 we plot the accuracy gap at each checkpoint \(t^{\prime}\) (i.e. the point from which two identical copies of the model are made and independently trained to completion) which is simply defined as the average final validation accuracy of the two individual child models minus the final validation accuracy of the weight averaged version of these two child models. Beyond the original experiment, we also wish to evaluate how the gradients \(\nabla f_{\bm{\theta}_{i}}(\cdot)\) evolve throughout training. Therefore, in panels (2) and (3) Fig. 6, at each checkpoint \(t^{\prime}\) we also measure the mean squared change in (pre-softmax) gradients \((\nabla_{\bm{\theta}}f_{\bm{\theta}_{t^{\prime}+390}}(\mathbf{x})-\nabla_{\bm {\theta}}f_{\bm{\theta}_{t^{\prime}}}(\mathbf{x}))^{2}\) between the current iteration \(t^{\prime}\) and those at the next epoch \(t^{\prime}+390\), averaged over a set of \(n=256\) test examples and the parameters in each layer.

Compute: We train num_outer_seeds \(\times\) num_inner_seeds \(\times\) num_child_models \(\times\) num_checkpoints (\(3\times 3\times 2\times 12=216\)) networks for the randomly initialized model. For the finetuned model this results in \(3\times 3\times 2\times 10=180\) training runs. Additionally, we require the pertaining of the 3 base models on SVHN. Combined this results in a total of \(216+180+3=399\) training runs. Training each ResNet-20 on CIFAR-10 required <1 hour including additional gradient computations.

### Data licenses

All image experiments are performed on CIFAR-10 [KH\({}^{+}\)09], MNIST [LBBH98], MNIST1D [GK24], or SVHN [NWC\({}^{+}\)11]. Tabular experiments are run on houses, superconduct, california, and house_sales from OpenML [VvRBT13] as described in [GOV22]. CIFAR-10 is released with an MIT license. MNIST is released with a Creative Commons Attribution-Share Alike 3.0 license. MNIST1D is released with an Apache-2.0 license. SVHN is released with a CC0:Public Domain license. OpenML datasets are released with a 3-Clause BSD License. All the datasets used in this work are publicly available.

## Appendix D Additional results

### Additional results on approximation quality (supplementing Fig. 2)

Figure 7: **Approximation error** of the telescoping (\(\tilde{f}_{\bm{\theta}_{t}}(\mathbf{x})\), red) and the model linearized around the initialization (\(f_{\bm{\theta}_{t}}^{lin}(\mathbf{x}),\,\mathrm{gray}\)) by optimization step for different optimization strategies and other design choices. Iteratively telescoping out the updates using \(\tilde{f}_{\bm{\theta}_{t}}(\mathbf{x})\) improves upon the lazy approximation around the initialization by orders of magnitude.

In Fig. 7, we present results investigating the evolution of approximation errors of the telescoping and linear approximation around the initialization during training using additional configurations compared to the results presented in Fig. 2 in the main text (replicated in the first two columns of Fig. 7). We observe the same trends as in the main text, where the telescoping approximation matches the predictions by the neural network by orders of magnitudes better than the linear approximation around the initialization. Importantly, we highlight in Fig. 8 that this is also reflected in how well each approximation matches the accuracy of the predictions of the real neural network: while the small errors of the telescoping model lead to no visible differences in accuracy compared to the real neural network, using the Taylor expansion around the initialization leads to significantly different accuracy later in training.

Additional results for case study 1: Exploring surprising generalization curves and benign overfitting

Double descent on MNIST.In Fig. 9, we replicate the CIFAR-10 experiment from the main text while training models to distinguish 3-vs-5 on MNIST. We find that in the absence of label noise, no problematic overfitting occurs for any hidden size; both train and test error monotonically improve with increased width. Only when we add label noise to the training data, do we observe the

Figure 8: **Test accuracy** of the telescoping (\(\tilde{f}_{\bm{\theta}_{t}}(\mathbf{x})\), red, top row) and the model linearized around the initialization (\(\tilde{f}_{\bm{\theta}_{t}}^{lin}(\mathbf{x})\), blue, bottom row) against accuracy of the actual neural network (gray) by optimization step for different optimization strategies and other design choices. While the telescoping model visibly matches the accuracy of the actual neural network, the linear approximation around the initialization leads to substantial differences in accuracy later in training.

Figure 9: **Double descent** experiments using MNIST, distinguishing 3-vs-5, with 20% added label noise during training (left) and no added label noise (right). Without label noise, there is no double descent in error on this task; when label noise is added we observe the prototypical double descent shape in test error.

characteristic double descent behavior in error - this is in line with [14]'s observation that double descent can be more pronounced when there is noise in the data. Importantly, we observe that as in the main text, the improvement of test error past the interpolation threshold is associated with the divergence of effective parameters used on train and test data. In Fig. 10 we additionally repeat the experiment using the MNIST-1D dataset with 15% labelnoise as in [1], and find that the decrease in test error after the interpolation threshold is again accompanied by a _decrease in effective parameters_ as the number of raw model parameters is further _increased_ in the interpolation regime.

Additional grokking results.In Fig. 11, we replicate the polynomial grokking results of [14] with additional values of \(\epsilon\). Like [14], we observe that larger values of \(\epsilon=0.5\) lead to less delayed generalization. This is reflected in a gap between effective parameters on test and train emerging earlier. With very small \(\epsilon=.05\), conversely, we even observe a double descent-like phenomenon where test error first worses before it improves later in training. This is reflected also in the effective parameters, where \(p_{8}^{test}\) first exceeds \(p_{8}^{train}\) before dropping below it as benign overfitting sets in later in training. In Fig. 12, we replicate the MNIST results with additional values of \(\alpha\); like [13] we observe that grokking behavior is more extreme for larger \(\alpha\). This is indeed also reflected in the gap between \(p_{8}^{test}\) and \(p_{8}^{train}\) emerging later in training.

Additional training results on MNIST with standard initialization.In Fig. 13, we present train and test results on MNIST with standard initialization to supplement the test results presented in the main text. Both with and without sigmoid, train and test behavior is almost identical, and learning is orders of magnitude faster than with the larger initialization. The stronger inductive biases of small initialization, and additionally using sigmoid activation, lead to much lower learned complexity on both train and test data as measured by effective parameters.

Additional results for Case study 2: Understanding differences between gradient boosting and neural networks

In Fig. 14, we replicate the experiment from Sec. 4.2 on three further datasets from [13]'s tabular benchmark. We find that the results match the trends present in Fig. 5 in the main text: the neural network is outperformed by the GBTs already at baseline, and the performance gap grows as the test dataset becomes increasingly more irregular. The growth in the gap is tracked by the behavior of the normalized maximum kernel weight norm of the neural network's kernel. Only on the california dataset do we observe a slightly different behavior of the neural network's kernel: unlike the other three datasets, \(\frac{\frac{1}{T}\sum_{i=1}^{T}\max_{j\in\mathcal{I}_{test}^{p}}||\mathbf{k}_{ \mathbf{i}}(x_{j})||_{2}}{\frac{1}{T}\sum_{i=1}^{T}\max_{i\in\mathcal{I}_{train }}||\mathbf{k}_{\mathbf{i}}(x_{i})||_{2}}\) stays substantially below 1 at all \(p\); this indicates that there may have been examples in the training set that are irregular in ways not captured by our experimental protocol. Nonetheless, we observe the same trend that \(\frac{\frac{1}{T}\sum_{i=1}^{T}\max_{j\in\mathcal{I}_{test}^{p}}||\mathbf{k}_{ \mathbf{i}}(x_{j})||_{2}}{\frac{1}{T}\sum_{i=1}^{T}\max_{i\in\mathcal{I}_{ train}}||\mathbf{k}_{\mathbf{i}}(x_{i})||_{2}}\) increases in relative terms as \(p\) increases.

Figure 10: **Double descent experiment using MNIST-1D, distinguishing class 0 and 1, with 15% added label noise during training. Mean squared error (top) and effective parameters (bottom) for train and test examples by number of hidden neurons.**

Figure 11: **Grokking** in mean squared error (top) on a polynomial regression task (replicated from [12]) against effective parameters (bottom) with different task alignment parameters \(\epsilon\).

Figure 12: **Grokking** in misclassification error on MNIST using a network with large initialization ( replicated from [12]) (top), against effective parameters (bottom) with different initialization scales \(\alpha\).

Figure 13: **No grokking** in misclassification error on MNIST (top), against effective parameters (bottom) using a network with standard initialization (\(\alpha=1\)) with and without sigmoid activation.

Figure 14: **Neural Networks vs GBTs: Relative performance (top) and behavior of kernels (bottom) with increasing test data irregularity for three additional datasets.**

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: As indicated in the abstract, we present a model for learning in neural networks in Sec. 3. We discuss how this allows to reason about the effects of design choices in Sec. 5, and use it to derive new empirical insights on the listed phenomena in Sec. 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Sec. 3, we highlight important practical limitations regarding the use of the telescoping approximation - it requires significant additional computation. Further, in Sec. 4 and in Sec. 6, we highlight that in each case study we intentionally limited ourselves to specific noteworthy empirical observations, which implies that future work is needed to further establish their generality beyond the case studies we consider. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Beyond the arguments presented in the main text, all additional derivations are included in Appendix B. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All details needed to replicate the experiments are described in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code is provided at https://github.com/alanjeffares/telescoping-lens. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: A detailed account of all experimental details is presented in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All figures report mean and one standard error across multiple replications of the same experiment, as outlined in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Estimates of used compute resources are presented in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: the research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper presents work whose goal is to advance the field of Machine Learning by advancing our understanding of surprising deep learning phenomena. While there are many potential societal consequences of the use of machine learning in general, there are none that we feel must be specifically highlighted here. If anything, we believe that a better understanding of unexpected deep learning behaviors will help limit unexpected behaviors of systems deployed in practice.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The used datasets and their licenses are discussed in Appendix C.4. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: the paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.