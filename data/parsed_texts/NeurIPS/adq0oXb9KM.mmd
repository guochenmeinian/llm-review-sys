# Tree Variational Autoencoders

Laura Manduchi, Moritz Vandenhirtz, Alain Ryser, Julia E. Vogt

Department of Computer Science

ETH Zurich

Switzerland

Equal contribution. Correspondence to {laura.manduchi,moritz.vandenhirtz}@inf.ethz.ch

###### Abstract

We propose _Tree Variational Autoencoder_ (TreeVAE), a new generative hierarchical clustering model that learns a flexible tree-based posterior distribution over latent variables. TreeVAE hierarchically divides samples according to their intrinsic characteristics, shedding light on hidden structures in the data. It adapts its architecture to discover the optimal tree for encoding dependencies between latent variables. The proposed tree-based generative architecture enables lightweight conditional inference and improves generative performance by utilizing specialized leaf decoders. We show that TreeVAE uncovers underlying clusters in the data and finds meaningful hierarchical relations between the different groups on a variety of datasets, including real-world imaging data. We present empirically that TreeVAE provides a more competitive log-likelihood lower bound than the sequential counterparts. Finally, due to its generative nature, TreeVAE is able to generate new samples from the discovered clusters via conditional sampling.

## 1 Introduction

Discovering structure and hierarchies in the data has been a long-standing goal in machine learning (Bishop, 2006; Bengio et al., 2012; Jordan and Mitchell, 2015). Interpretable supervised methods, such as decision trees (Zhou and Feng, 2017; Tanno et al., 2019), have proven to be successful in unveiling hierarchical relationships within data. However, the expense of annotating large quantities of data has resulted in a surge of interest in unsupervised approaches (LeCun et al., 2015). Hierarchical clustering (Ward, 1963) offers an unsupervised path to find hidden groups in the data and their hierarchical relationship (R. J. G. B. Campello et al., 2015). Due to its versatility, interpretability, and ability to uncover meaningful patterns in complex data, hierarchical clustering has been widely used in a variety of applications, including phylogenetics (Sneath and Sokal, 1962), astrophysics (McConnachie et al., 2018), and federated learning (Briggs et al., 2020). Similar to how the human brain automatically categorizes and connects objects based on shared attributes, hierarchical clustering algorithms construct a dendrogram - a tree-like structure of clusters - that organizes data into nested groups based on their similarity. Despite its potential, hierarchical clustering has taken a step back in light of recent advances in self-supervised deep learning (Chen et al., 2020), and only a few deep learning based methods have been proposed in recent years (Goyal et al., 2017; Mautz et al., 2020).

Deep latent variable models (Kingma and Welling, 2019), a class of generative models, have emerged as powerful frameworks for unsupervised learning and they have been extensively used to uncover hidden structures in the data (Dilokthanakul et al., 2016; Manduchi et al., 2021). They leverage the flexibility of neural networks to capture complex patterns and generate meaningful representations of high-dimensional data. By incorporating latent variables, these models can uncover the underlying factors of variation of the data, making them a valuable tool for understanding and modeling complex data distributions. In recent years, a variety of deep generative methods have been proposed to incorporate more complex posterior distributions by modeling structural _sequential_ dependenciesbetween latent variables (Sonderby et al., 2016; He et al., 2018; Maaloe et al., 2019; Vahdat & Kautz, 2020a), thus offering different levels of abstraction for encoding the data distribution.

Our work advances the state-of-the-art in structured VAEs by combining the complementary strengths of hierarchical clustering algorithms and deep generative models. We propose TreeVAE1, a novel tree-based generative model that encodes _hierarchical_ dependencies between latent variables. We introduce a training procedure to learn the optimal tree structure to model the posterior distribution of latent variables. An example of a tree learned by TreeVAE is depicted in Fig. 1. Each edge and split are encoded by neural networks, while the circles depict latent variables. Each sample is associated with a probability distribution over _paths_. The resulting tree thus organizes the data into an interpretable hierarchical structure in an unsupervised fashion, optimizing the amount of shared information between samples. In CIFAR-10, for example, the method divides the vehicles and animals into two different subtrees and similar groups (such as planes and ships) share common ancestors.

Footnote 1: The code is publicly available at [https://github.com/lauramanduchi/treevae-pytorch](https://github.com/lauramanduchi/treevae-pytorch).

**Our main contributions** are as follows: (\(i\)) We propose a novel, deep probabilistic approach to hierarchical clustering that learns the optimal generative binary tree to mimic the hierarchies present in the data. (\(ii\)) We provide a thorough empirical assessment of the proposed approach on MNIST, Fashion-MNIST, 20Newsgroups, and Omniglot. In particular, we show that TreeVAE (a) outperforms related work on deep hierarchical clustering, (b) discovers meaningful patterns in the data and their hierarchical relationships, and (c) achieves a more competitive log-likelihood lower bound compared to VAE and LadderVAE, its sequential counterpart. (\(iii\)) We propose an extension of TreeVAE that integrates contrastive learning into its tree structure. Relevant prior knowledge, expertise, or specific constraints can be incorporated into the generative model via augmentations, allowing for more accurate and contextually meaningful clustering. We test the contrastive version of TreeVAE on CIFAR-10, CIFAR-100, and CelebA, and we show that the proposed approach achieves competitive hierarchical clustering performance compared to the baselines.

## 2 TreeVAE

We propose TreeVAE, a novel deep generative model that learns a flexible tree-based posterior distribution over latent variables. Each sample travels through the tree from root to leaf in a probabilistic manner as TreeVAE learns sample-specific probability distributions of paths. As a result, the data is divided in a hierarchical fashion, with more refined concepts for deeper nodes in the tree. The proposed graphical model is depicted in Fig. 2. The inference and generative models share the same top-down tree structure, enabling interaction between the bottom-up and top-down architecture, similarly to Sonderby et al. (2016).

### Model Formulation

Given \(H\), the maximum depth of the tree, and a dataset \(\mathbf{X}\), the model is defined by three components that are learned during training:

Figure 1: The hierarchical structure discovered by TreeVAE on the CIFAR-10 dataset. We display random subsets of images that are probabilistically assigned to each leaf of the tree.

* the _global_ structure of the binary tree \(\mathcal{T}\), which specifies the set of nodes \(\mathbb{V}=\{0,\ldots,V\}\), the set of leaves \(\mathbb{L}\), where \(\mathbb{L}\subset\mathbb{V}\), and the set of edges \(\mathcal{E}\). See Fig. 1/4/5/6/7 for different examples of tree structures learned by the model.
* the _sample-specific_ latent embeddings \(\mathbf{z}=\{\mathbf{z}_{0},\ldots,\mathbf{z}_{V}\}\), which are random variables assigned to each node in \(\mathbb{V}\). Each embedding is characterized by a Gaussian distribution whose parameters are a function of the realization of the parent node. The dimensions of the latent embeddings are defined by their depth, with \(\mathbf{z}_{i}\in\mathbb{R}^{h_{\mathrm{depth}(i)}}\) where \(\mathrm{depth}(i)\) is the depth of the node \(i\), and \(h_{\mathrm{depth}(i)}\) is the embedding dimension for that depth.
* the _sample-specific_ decisions \(\mathbf{c}=\{\mathbf{c}_{0},\ldots,\mathbf{c}_{V-\left\|\mathbb{L}\right\|}\}\), which are Bernoulli random variables defined by the probability of going to the right (or left) child of the underlying node. They take values \(c_{i}\in\{0,1\}\) for \(i\in\mathbb{V}\setminus\mathbb{L}\), with \(c_{i}=0\) if the left child is selected. A decision path, \(\mathcal{P}_{l}\), indicates the path from root to leaf given the tree \(\mathcal{T}\) and is defined by the nodes in the path, e.g., in Fig. 2, \(\mathcal{P}_{l}=\{0,1,4,5\}\). The probability of \(\mathcal{P}_{l}\) is the product of the probabilities of the decisions in the path.

The tree structure is shared across the entire dataset and is learned iteratively by growing the tree node-wise. The latent embeddings and the decision paths, on the other hand, are learned using variational inference by conditioning the model on the current tree structure. The generative/inference model and the learning objective conditioned on \(\mathcal{T}\) are explained in Sec. 2.2/2.3/2.4 respectively, while in 2.5, we elaborate on the efficient growing procedure of the tree.

### Generative Model

The generative process of TreeVAE for a given \(\mathcal{T}\) is depicted in Fig. 2 (right). The generation of a new sample \(\mathbf{x}\) starts from the root. First, the latent embedding of the root node \(\mathbf{z}_{0}\) is sampled from a standard Gaussian \(p_{\theta}\left(\mathbf{z}_{0}\right)=\mathcal{N}\left(\mathbf{z}_{0}\mid\mathbf{0},\mathbf{I}\right)\). Then, given the sampled \(\mathbf{z}_{0}\), the decision of going to the left or the right node is sampled from a Bernoulli distribution \(p(\mathbf{c}_{0}\mid\mathbf{z}_{0})=Ber(r_{p,0}(\mathbf{z}_{0}))\), where \(\{r_{p,i}\mid i\in\mathbb{V}\setminus\mathbb{L}\}\) are functions parametrized by neural networks defined as _routers_, and cause the splits in Fig. 2. The subscript \(p\) is used to indicate the parameters of the generative model. The latent embedding of the selected child, let us assume it is \(\mathbf{z}_{1}\), is then sampled from a Gaussian distribution \(p_{\theta}(\mathbf{z}_{1}\mid\mathbf{z}_{0})=\mathcal{N}\left(\mathbf{z}_{1}\mid\mu_{p,1}\left(\mathbf{z}_{0}\right),\sigma_{p,1}^{2}\left(\mathbf{z}_{0}\right)\right)\), where \(\{\mu_{p,i},\sigma_{p,i}\mid i\in\mathbb{V}\setminus\{0\}\}\) are functions parametrized by neural networks defined as _transformations_. They are indicated by the top-down arrows in Fig. 2. This process continues until a leaf is reached.

Let us define the set of latent variables selected by the path \(\mathcal{P}_{l}\), which goes from the root to the leaf \(l\), as \(\mathbf{z}_{\mathcal{P}_{l}}=\{\mathbf{z}_{i}\mid i\in\mathcal{P}_{l}\}\), the parent node of the node \(i\) as \(pa(i)\), and \(p(c_{pa(i)\to i}\mid\mathbf{z}_{pa(i)})\) the probability of going from \(pa(i)\) to \(i\). Note that the path \(\mathcal{P}_{l}\) defines the sequence of decisions. The prior probability of the latent embeddings and the path given the tree \(\mathcal{T}\) can be summarized as

\[p_{\theta}(\mathbf{z}_{\mathcal{P}_{l}},\mathcal{P}_{l})=p(\mathbf{z}_{0})\prod_{i\in \mathcal{P}_{l}\setminus\{0\}}p(c_{pa(i)\to i}\mid\mathbf{z}_{pa(i)})p(\mathbf{z}_{i} \mid\mathbf{z}_{pa(i)}). \tag{1}\]

Finally, \(\mathbf{x}\) is sampled from a distribution that is conditioned on the selected leaf. If we assume that \(\mathbf{x}\) is real-valued, then

\[p_{\theta}(\mathbf{x}\mid\mathbf{z}_{\mathcal{P}_{l}},\mathcal{P}_{l})=\mathcal{N} \left(\mathbf{x}\mid\mu_{x,l}\left(\mathbf{z}_{l}\right),\sigma_{x,l}^{2}\left( \mathbf{z}_{l}\right)\right), \tag{2}\]

where \(\{\mu_{x,l},\sigma_{x,l}\mid l\in\mathbb{L}\}\) are functions parametrized by leaf-specific neural networks defined as _decoders_.

Figure 2: The proposed inference (left) and generative (right) models for TreeVAE. Circles are stochastic variables while diamonds are deterministic. The global topology of the tree is learned during training.

### Inference Model

The inference model is described by the variational posterior distribution of both the latent embeddings and the paths. It follows a similar structure as in the prior probability defined in (1), with the difference that the probability of the root and of the decisions are now conditioned on the sample \(\mathbf{x}\):

\[q(\mathbf{z}_{\mathcal{P}_{l}},\mathcal{P}_{l}\mid\mathbf{x})=q(\mathbf{z}_{0} \mid\mathbf{x})\prod_{i\in\mathcal{P}_{l}\setminus\{0\}}q(c_{pa(i)\to i} \mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)}). \tag{3}\]

To compute the variational probability distribution of the latent embeddings \(q(\mathbf{z}_{0}\mid\mathbf{x})\) and \(q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)})\), where

\[q(\mathbf{z}_{0}\mid\mathbf{x})=\mathcal{N}\left(\mathbf{z}_{0}\mid\mu_{q,0} (\mathbf{x}),\sigma_{q,0}^{2}(\mathbf{x})\right) \tag{4}\] \[q_{\phi}\left(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)}\right)=\mathcal{N} \left(\mathbf{z}_{i}\mid\mu_{q,i}\left(\mathbf{z}_{pa(i)}\right),\sigma_{q,i}^{2}\left( \mathbf{z}_{pa(i)}\right)\right),\forall i\in\mathcal{P}_{l}, \tag{5}\]

we follow a similar approach to the one proposed by Sonderby et al. (2016). Note that we use the subscript \(q\) to indicate the parameters of the inference model.

First, a deterministic bottom-up pass computes the node-specific approximate likelihood contributions

\[\mathbf{d}_{h} =\mathrm{MLP}\left(\mathbf{d}_{h+1}\right) \tag{6}\] \[\mathbf{\hat{\mu}}_{q,i} =\mathrm{Linear}\left(\mathbf{d}_{depth(i)}\right),i\in\mathbb{V}\] (7) \[\mathbf{\hat{\sigma}}_{q,i}^{2} =\mathrm{Softplus}\left(\mathrm{Linear}\left(\mathbf{d}_{depth(i) }\right)\right),i\in\mathbb{V}, \tag{8}\]

where \(\mathbf{d}_{H}\) is parametrized by a domain-specific neural network defined as _encoder_, and \(\mathrm{MLP}(\mathbf{d}_{h})\) for \(h\in\{1,\ldots,H\}\), indicated by the bottom-up arrows in Fig. 2, are neural networks, shared among the parameter predictors, \(\hat{\mu}_{q,i},\hat{\sigma}_{q,i}^{2},\) of the same depth. They are characterized by the same architecture as the _transformations_ defined in Sec.2.2.

A stochastic downward pass then recursively computes the approximate posteriors defined as

\[\mathbf{\sigma}_{q,i}^{2}=\frac{1}{\mathbf{\hat{\sigma}}_{q,i}^{-2}+\mathbf{ \sigma}_{p,i}^{-2}},\quad\mathbf{\mu}_{q,i}=\frac{\mathbf{\hat{\mu}}_{q,i}\mathbf{\hat{ \sigma}}_{q,i}^{-2}+\mathbf{\mu}_{p,i}\mathbf{\sigma}_{p,i}^{-2}}{\mathbf{\hat{\sigma}}_{q,i}^{-2}+\mathbf{\sigma}_{p,i}^{-2}}, \tag{9}\]

where all operations are performed elementwise. Finally, the variational distributions of the decisions \(q(\mathrm{c}_{i}\mid\mathbf{x})\) are defined as

\[q(\mathrm{c}_{i}\mid\mathbf{x})=q(\mathrm{c}_{i}\mid\mathbf{d}_{\mathrm{depth(i) }})=Ber(r_{q,i}(\mathbf{d}_{\mathrm{depth(i)}})), \tag{10}\]

where \(\{r_{q,i}\mid i\in\mathbb{V}\setminus\mathbb{L}\}\) are functions parametrized by neural networks and are characterized by the same architecture as the _routers_ of the generative model defined in Sec. 2.2.

### Evidence Lower Bound

The parameters of both the generative model (defined as \(p\)) and inference model (defined as \(q\)), consisting of the encoder (\(\mu_{q,0},\sigma_{q,0}\)), the transformations (\(\{(\mu_{p,i},\sigma_{p,i}),(\mu_{q,i},\sigma_{q,i})\mid i\in\mathbb{V} \setminus\{0\}\}\)), the decoders (\(\{\mu_{x,l},\sigma_{x,l}\mid l\in\mathbb{L}\}\)) and the routers (\(\{r_{p,i},r_{q,i}\mid i\in\mathbb{V}\setminus\mathbb{L}\}\)), are learned by maximizing the Evidence Lower Bound (ELBO) (Kingma & Welling, 2014; Rezende et al., 2014). Each leaf \(l\) is associated with only one path \(\mathcal{P}_{l}\), hence we can write the data likelihood conditioned on \(\mathcal{T}\) as

\[p(\mathbf{x}\mid\mathcal{T})=\sum_{l\in\mathbb{L}}\int_{\mathbf{z}_{\mathcal{P}_{l}}}p( \mathbf{x},\mathbf{z}_{\mathcal{P}_{l}},\mathcal{P}_{l})=\sum_{l\in\mathbb{L}}\int_{ \mathbf{z}_{\mathcal{P}_{l}}}p_{\theta}(\mathbf{z}_{\mathcal{P}_{l}},\mathcal{P}_{l})p _{\theta}(\mathbf{x}\mid\mathbf{z}_{\mathcal{P}_{l}},\mathcal{P}_{l}). \tag{11}\]

We use variational inference to derive the ELBO of the log-likelihood:

\[\mathcal{L}(\mathbf{x}\mid\mathcal{T}):=\mathbb{E}_{q(\mathbf{z}_{p_{l}},\mathcal{P}_{ l}|\mathbf{x})}[\log p(\mathbf{x}\mid\mathbf{z}_{\mathcal{P}_{l}},\mathcal{P}_{l})]- \mathrm{KL}\left(q\left(\mathbf{z}_{\mathcal{P}_{l}},\mathcal{P}_{l}\mid\mathbf{x} \right)\|p\left(\mathbf{z}_{\mathcal{P}_{l}},\mathcal{P}_{l}\right)\right). \tag{12}\]

The first term of the ELBO is the reconstruction term:

\[\mathcal{L}_{rec} =\mathbb{E}_{q(\mathbf{z}_{\mathcal{P}_{l}},\mathcal{P}_{l}|\mathbf{x})}[ \log p(\mathbf{x}\mid\mathbf{z}_{\mathcal{P}_{l}},\mathcal{P}_{l})] \tag{13}\] \[=\sum_{l\in\mathbb{L}}\int_{\mathbf{z}_{\mathcal{P}_{l}}}q(\mathbf{z}_{0} \mid\mathbf{x})\prod_{i\in\mathcal{P}_{l}\setminus\{0\}}q(c_{pa(i)\to i}\mid\mathbf{x} )q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)})\log p(\mathbf{x}\mid\mathbf{z}_{\mathcal{P}_{l}}, \mathcal{P}_{l})\] (14) \[\approx\frac{1}{M}\sum_{m=1}^{M}\sum_{l\in\mathbb{L}}P(l;\mathbf{c}) \log\mathcal{N}\left(\mathbf{x}\mid\mu_{x,l}\left(\mathbf{z}_{l}^{(m)}\right), \sigma_{x,l}^{2}\left(\mathbf{z}_{l}^{(m)}\right)\right),\] (15) \[P(i;\mathbf{c}) =\prod_{j\in\mathcal{P}_{i}\setminus\{0\}}q(c_{pa(j)\to j}\mid\mathbf{x}) \ \ \text{for }i\in\mathbb{V}, \tag{16}\]where \(\mathcal{P}_{i}\) for \(i\in\mathbb{V}\) is the path from root to node \(i\), \(P(i;\mathbf{c})\) is the probability of reaching node \(i\), which is the product over the probabilities of the decisions in the path until \(i\), \(\mathbf{z}_{l}^{(m)}\) are the Monte Carlo (MC) samples, and \(M\) the number of the MC samples. Intuitively, the reconstruction loss is the sum of the leaf-wise reconstruction losses weighted by the probabilities of reaching the respective leaf. Note that here we sum over all possible paths in the tree, which is equal to the number of leaves.

The second term of (12) is the Kullback-Leibler divergence (KL) between the prior and the variational posterior of the tree. It can be written as a sum of the KL of the root, the nodes, and the decisions:

\[\mathrm{KL}\left(q\left(\mathbf{z}_{\mathcal{P}_{l}},\mathcal{P}_{l} \mid\mathbf{x}\right)\right\|\!\!\left\|p\left(\mathbf{z}_{\mathcal{P}_{l}},\mathcal{ P}_{l}\right)\right)=\mathrm{KL}_{root}+\mathrm{KL}_{nodes}+\mathrm{KL}_{ decisions} \tag{17}\] \[\mathrm{KL}_{root}=\mathrm{KL}(q(\mathbf{z}_{0}\mid\mathbf{x})\|p(\mathbf{z}_ {0}))\] (18) \[\mathrm{KL}_{nodes}\approx\frac{1}{M}\sum_{m=1}^{M}\sum_{i\in \mathbb{V}\setminus\{0\}}P(i;\mathbf{c})\,\mathrm{KL}(q(\mathbf{z}_{i}^{(m)}\mid pa( \mathbf{z}_{i}^{(m)}))\|p(\mathbf{z}_{i}^{(m)}\mid pa(\mathbf{z}_{i}^{(m)})))\] (19) \[\mathrm{KL}_{decisions}\approx\frac{1}{M}\sum_{m=1}^{M}\sum_{i\in \mathbb{V}\setminus\mathbb{L}}P(i;\mathbf{c})\sum_{c_{i}\in\{0,1\}}q(c_{i}\mid\mathbf{ x})\log\left(\frac{q(c_{i}\mid\mathbf{x})}{p(c_{i}\mid\mathbf{z}_{i}^{(m)})}\right), \tag{20}\]

where \(M\) is the number of MC samples. We refer to Appendix A for the full derivation. The \(\mathrm{KL}_{root}\) is the KL between the standard Gaussian prior \(p(\mathbf{z}_{0})\) and the variational posterior of the root \(q(\mathbf{z}_{0}\mid\mathbf{x})\), thus enforcing the root to be compact. The \(\mathrm{KL}_{nodes}\) is the sum of the node-specific KLs weighted by the probability of reaching their node \(i\): \(P(i;\mathbf{c})\). The node-specific KL of node \(i\) is the KL between the two Gaussians \(q(\mathbf{z}_{i}\mid pa(\mathbf{z}_{i}))\), \(p(\mathbf{z}_{i}\mid pa(\mathbf{z}_{i}))\). Finally, the last term, \(\mathrm{KL}_{decisions}\), is the weighted sum of all the KLs of the decisions, which are Bernoulli random variables, \(KL(q(\mathbf{c}_{i}\mid\mathbf{x})\mid p(\mathbf{c}_{i}\mid\mathbf{z}_{i})))=\sum_{c_{i}\in\{0,1\}}q(c_{i}\mid\mathbf{x})\log\left(\frac{q(c_{i}\mid\mathbf{x})}{p(c_{i}\mid\mathbf{z}_{ i}))}\right)\). The hierarchical specification of the binary tree allows encoding highly expressive models while retaining the computational efficiency of fully factorized models. The computational complexity is described in Appendix A.2.

### Growing The Tree

In the previous sections, we discussed the variational objective to learn the parameters of both the generative and the inference model given a defined tree structure \(\mathcal{T}\). Here we discuss how to learn the structure of the binary tree \(\mathcal{T}\). TreeVAE starts by training a tree composed of a root and two leaves, see Fig. 3 (left), for \(N_{t}\) epochs by optimizing the ELBO. Once the model converged, a leaf is selected, e.g., \(\mathbf{z}_{1}\) in Fig. 3, and two children are attached to it. The leaf selection criteria can vary depending on the application and can be determined by, e.g., the reconstruction loss or the ELBO. In our experiments, we chose to select the nodes with the maximum number of samples to retain balanced leaves. The sub-tree composed of the new leaves and the parent node is then trained for \(N_{t}\) epochs by freezing the weights of the rest of the model, see Fig. 3 (right), resulting in computing the ELBO of the nodes of the subtree. For efficiency, the subtree is trained using only the subset of data that have a high probability (higher than a threshold \(t\)) of being assigned to the parent node. The process is repeated until the tree reaches its maximum capacity (defined by the maximum depth) or until a condition (such as a predefined maximum number of leaves) is met. The entire model is then fine-tuned for \(N_{f}\) epochs by unfreezing all weights. During fine-tuning, the tree is pruned by removing empty branches (with the expected number of assigned samples lower than a threshold).

### Integrating Prior Knowledge

Retrieving semantically meaningful clustering structures of real-world images is extremely challenging, as there are several underlying factors according to which the data can be clustered. Therefore, it

Figure 3: The first two steps of the growing process to learn the global structure of the tree during training. Highlighted in red are the trainable weights.

is often crucial to integrate domain knowledge that guides the model toward desirable cluster assignments. Thus, we propose an extension of TreeVAE where we integrate recent advances in contrastive learning (van den Oord et al., 2018; Chen et al., 2020; Li et al., 2021), whereby prior knowledge on data invariances can be encoded through augmentations. For a batch \(\mathbf{X}\) with \(N\) samples, we randomly augment every sample twice to obtain the augmented batch \(\tilde{\mathbf{X}}\) with \(2N\) samples. For all positive pairs \((i,j)\) where \(\tilde{\mathbf{x}}_{i}\) and \(\tilde{\mathbf{x}}_{j}\) stem from the same original sample, we utilize the _NT-Kent_(Chen et al., 2020), which introduces losses \(\ell_{i,j}=-\log\frac{\exp(s_{i,j}/\tau)}{\sum_{k=1}^{N}\frac{\exp(s_{i,j}/ \tau)}{1[k\neq i]}\exp(s_{i,k}/\tau)}\), where \(s_{i,j}\) denotes the cosine similarity between the representations of \(\tilde{\mathbf{x}}_{i}\) and \(\tilde{\mathbf{x}}_{j}\), and \(\tau\) is a temperature parameter. We integrate \(\ell_{i,j}\) in both the bottom-up and the routers of TreeVAE. In the bottom-up, similar to Chen et al. (2020), we compute \(\ell_{i,j}\) on the projections \(g_{h}(\mathbf{d}_{h})\). For the routers, we directly compute the loss on the predicted probabilities \(r_{q,i}(\mathbf{d}_{h})\). Finally, we average the terms over all positive pairs and add them to the negative ELBO (12) in real-world image experiments. Implementation details can be found in Appendix E, while a loss ablation is shown in Appendix C.3.

## 3 Related Work

Deep latent variable models automatically learn structure from data by combining the flexibility of deep neural networks and the statistical foundations of generative models (Mattei and Frellsen, 2018). Variational autoencoders (VAEs) (Rezende et al., 2014; Kingma and Welling, 2014) are among the most used frameworks (Nasiri and Bepler, 2022; Bae et al., 2023; Bredell et al., 2023). A variety of works has been proposed to integrate more complex empirical prior distributions, thus reducing the gap between approximate and true posterior distributions (Ranganath et al., 2015; Webb et al., 2017; Klushyn et al., 2019). Among these, the most related to our work is the VAE-nCRP (Goyal et al., 2017; Shin et al., 2019) and the TMC-VAE (Vikram et al., 2018). Both works use Bayesian nonparametric hierarchical clustering based on the nested Chinese restaurant process (nCRP) prior (Blei et al., 2003), and on the time-marginalized coalescent (TMC). However, even if they allow more flexible prior distributions these models suffer from restrictive posterior distributions (Kingma et al., 2016).To overcome the above issue, deep hierarchical VAEs (Gregor et al., 2015; Kingma et al., 2016) have been proposed to employ structured approximate posteriors, which are composed of hierarchies of conditional stochastic variables that are connected _sequentially_. Among a variety of proposed methods (Vahdat and Kautz, 2020; Falck et al., 2022; T. Z. Xiao and Bamler, 2023), Ladder VAE (Sonderby et al., 2016) is most related to TreeVAE. The authors propose to model the approximate posterior by combining a "bottom-up" recognition distribution with the "top-down" prior. Further extensions include BIVA (Maaloe et al., 2019), which introduces a bidirectional inference network, and GraphVAE (He et al., 2019), that introduces gated dependencies over a fixed number of latent variables. Contrary to the previous approaches, TreeVAE models a _tree-based_ posterior distribution of latent variable, thus allowing hierarchical clustering of samples. For further work on hierarchical clustering and its supervised counterpart, decision trees, we refer to Appendix B.

## 4 Experimental Setup

Datasets and Metrics:We evaluate the clustering and generative performance of TreeVAE on MNIST (LeCun et al., 1998), Fashion-MNIST (H. Xiao et al., 2017), 20Newsgroups (Lang, 1995), Omniglot (Lake et al., 2015), and Omniglot-5, where only \(5\) vocabularies (Braille, Glagolitic, Cyrillic, Odia, and Bengali) are selected and used as true labels. We assess the hierarchical clustering performance by computing dendrogram purity (DP) and leaf purity (LP), as defined by (Kobren et al., 2017a) using the datasets labels, where we assume the number of true clusters is unknown. We also report standard clustering metrics, accuracy (ACC) and normalized mutual information (NMI), by setting the number of leaves for TreeVAE and for the baselines to the true number of clusters. In terms of generative performance, we compute the approximated true log-likelihood calculated using \(1000\) importance-weighted samples, together with the ELBO (12) and the reconstruction loss (16). We also perform hierarchical clustering experiments on real-world imaging data, namely CIFAR-10, CIFAR-100 (Krizhevsky and Hinton, 2009) with \(20\) superclasses as labels, and CelebA (Z. Liu et al., 2015) using the contrastive extension (Sec. 2.6). We refer to Appendix D for more dataset details.

Baselines:We compare the generative performance of TreeVAE to the VAE (Rezende et al., 2014; Kingma and Welling, 2014), its non-hierarchical counterpart, and the LadderVAE (Sonderby et al., 2016), its sequential counterpart. For a fair comparison, all methods share the same architecture and hyperparameters whenever possible. We compare TreeVAE to non-generative hierarchical clustering baselines for which the code was publicly available: Ward's minimum variance agglomerative clustering (Agg) (Ward, 1963; Murtagh & Legendre, 2014), and the DeepECT (Mautz et al., 2020). We propose two additional baselines, where we perform Ward's agglomerative clustering on the latent space of the VAE (VAE + Agg) and of the last layer of the LadderVAE (LadderVAE + Agg). For the contrastive clustering experiments, we apply a contrastive loss similar to TreeVAE to the VAE and the LadderVAE, while for DeepECT we use the contrastive loss proposed by the authors.

**Implementation Details:** While we believe that more complex architectures could have a substantial impact on the performance of TreeVAE, we choose to employ rather simple settings to validate the proposed approach. We set the dimension of all latent embeddings \(\mathbf{z}=\{\mathbf{z}_{0},\ldots,\mathbf{z}_{V}\}\) to \(8\) for MNIST, Fashion, and Omniglot, to \(4\) for 20Newsgroups, and to \(64\) for CIFAR-10, CIFAR-100, and CelebA. The maximum depth of the tree is set to \(6\) for all datasets, except 20Newsgroups where we increased the depth to \(7\) to capture more clusters. To compute DP and LP, we allow the tree to grow to a maximum of \(30\) leaves for 20Newsgroups and CIFAR-100, and \(20\) for the rest, while for ACC and NMI we fix the number of leaves to the number of true classes. The transformations consist of one-layer MLPs of size \(128\) and the routers of two-layers of size \(128\) for all datasets except for the real-world imaging data where we slightly increase the MLP complexity to \(512\). Finally, the encoder and decoders consist of simple CNNs and MLPs. The trees are trained for \(N_{t}=150\) epochs at each growth step, and the final tree is finetuned for \(N_{f}=200\) epochs. For the real-world imaging experiments, we set the weight of the contrastive loss to \(100\). See Appendix E for additional details.

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline Dataset & Method & DP & LP & ACC & NMI \\ \hline MNIST & Agg & \(63.7\pm 0.0\) & \(78.6\pm 0.0\) & \(69.5\pm 0.0\) & \(71.1\pm 0.0\) \\  & VAE + Agg & \(79.9\pm 2.2\) & \(90.8\pm 1.4\) & \(86.6\pm 4.9\) & \(81.6\pm 2.0\) \\  & LadderVAE + Agg & \(81.6\pm 3.9\) & \(90.9\pm 2.5\) & \(80.3\pm 5.6\) & \(82.0\pm 2.1\) \\  & DeepECT & \(74.6\pm 5.9\) & \(90.7\pm 3.2\) & \(74.9\pm 6.2\) & \(76.7\pm 4.2\) \\  & TreeVAE (ours) & \(\mathbf{87.9\pm 4.9}\) & \(\mathbf{96.0\pm 1.9}\) & \(\mathbf{90.2\pm 7.5}\) & \(\mathbf{90.0\pm 4.6}\) \\ \hline Fashion & Agg & \(45.0\pm 0.0\) & \(67.6\pm 0.0\) & \(51.3\pm 0.0\) & \(52.6\pm 0.0\) \\  & VAE + Agg & \(44.3\pm 2.5\) & \(65.9\pm 2.3\) & \(54.9\pm 4.4\) & \(56.1\pm 3.2\) \\  & LadderVAE + Agg & \(49.5\pm 2.3\) & \(67.6\pm 1.2\) & \(55.9\pm 3.0\) & \(60.7\pm 1.4\) \\  & DeepECT & \(44.9\pm 3.3\) & \(67.8\pm 1.4\) & \(51.8\pm 5.7\) & \(57.7\pm 3.7\) \\  & TreeVAE (ours) & \(\mathbf{54.4\pm 2.4}\) & \(\mathbf{71.4\pm 2.0}\) & \(\mathbf{63.6\pm 3.3}\) & \(\mathbf{64.7\pm 1.4}\) \\ \hline
20Newsgroups & Agg & \(13.1\pm 0.0\) & \(30.8\pm 0.0\) & \(26.1\pm 0.0\) & \(27.5\pm 0.0\) \\  & VAE + Agg & \(7.1\pm 0.3\) & \(18.1\pm 0.5\) & \(15.2\pm 0.4\) & \(11.6\pm 0.3\) \\  & LadderVAE + Agg & \(9.0\pm 0.2\) & \(20.0\pm 0.7\) & \(17.4\pm 0.9\) & \(17.8\pm 0.6\) \\  & DeepECT & \(9.3\pm 1.8\) & \(17.2\pm 3.8\) & \(15.6\pm 3.0\) & \(18.1\pm 4.1\) \\  & TreeVAE (ours) & \(\mathbf{17.5\pm 1.5}\) & \(\mathbf{38.4\pm 1.6}\) & \(\mathbf{32.8\pm 2.3}\) & \(\mathbf{34.4\pm 1.5}\) \\ \hline Omniglot-5 & Agg & \(41.4\pm 0.0\) & \(63.7\pm 0.0\) & \(53.2\pm 0.0\) & \(33.3\pm 0.0\) \\  & VAE + Agg & \(46.3\pm 2.3\) & \(68.1\pm 1.6\) & \(52.9\pm 4.2\) & \(34.4\pm 2.9\) \\  & LadderVAE + Agg & \(49.8\pm 3.9\) & \(71.3\pm 2.0\) & \(59.6\pm 4.9\) & \(44.2\pm 4.7\) \\  & DeepECT & \(33.3\pm 2.5\) & \(55.1\pm 2.8\) & \(41.1\pm 4.2\) & \(23.5\pm 4.3\) \\  & TreeVAE (ours) & \(\mathbf{58.8\pm 4.0}\) & \(\mathbf{77.7\pm 3.9}\) & \(\mathbf{63.9\pm 7.0}\) & \(\mathbf{50.0\pm 5.9}\) \\ \hline \hline CIFAR-10* & VAE + Agg & \(10.54\pm 0.12\) & \(16.33\pm 0.15\) & \(14.43\pm 0.19\) & \(1.86\pm 1.66\) \\  & LadderVAE + Agg & \(12.81\pm 0.20\) & \(25.37\pm 0.62\) & \(19.29\pm 0.60\) & \(7.41\pm 0.42\) \\  & DeepECT & \(10.01\pm 0.02\) & \(10.30\pm 0.40\) & \(10.31\pm 0.39\) & \(0.18\pm 0.10\) \\  & TreeVAE (ours) & \(\mathbf{35.30\pm 1.15}\) & \(\mathbf{53.85\pm 1.23}\) & \(\mathbf{52.98\pm 1.34}\) & \(\mathbf{41.44\pm 1.13}\) \\ \hline CIFAR-100* & VAE + Agg & \(5.27\pm 0.02\) & \(9.86\pm 0.19\) & \(8.82\pm 0.11\) & \(2.46\pm 0.10\) \\  & LadderVAE + Agg & \(6.36\pm 0.07\) & \(16.08\pm 0.28\) & \(14.01\pm 0.41\) & \(8.99\pm 0.41\) \\  & DeepECT & \(5.28\pm 0.18\) & \(6.97\pm 0.69\) & \(6.97\pm 0.69\) & \(1.71\pm 0.86\) \\  & TreeVAE (ours) & \(\mathbf{10.44\pm 0.38}\) & \(\mathbf{24.16\pm 0.65}\) & \(\mathbf{21.82\pm 0.77}\) & \(\mathbf{17.80\pm 0.42}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Test set hierarchical clustering performances (\(\%\)) of TreeVAE compared with baselines. Means and standard deviations are computed across 10 runs with different random model initialization. The star "*‚Äù indicates real-world image datasets on which contrastive approaches were applied.

## 5 Results

Hierarchical Clustering ResultsTable 1 shows the quantitative hierarchical clustering results averaged across \(10\) seeds. First, we assume the true number of clusters is _unknown_ and report DP and LP. Second, we assume we have access to the true number of clusters \(K\) and compute ACC and NMI. As can be seen, TreeVAE outperforms the baselines in both experiments. This suggests that the proposed approach successfully builds an optimal tree based on the data's intrinsic characteristics. Among the different baselines, agglomerative clustering using Ward's method (Agg) trained on the last layer of LadderVAE shows competitive performances. To the best of our knowledge, we are the first to report these results. It is noteworthy to observe that it consistently improves over VAE + Agg, indicating that the last layer of LadderVAE captures more cluster information than the VAE.

Generative ResultsIn Table 2, we evaluate the generative performance of the proposed approach, TreeVAE, compared to the VAE, its non-hierarchical counterpart, and LadderVAE, its sequential counterpart. TreeVAE outperforms the baselines on the majority of datasets, indicating that the proposed ELBO (12) can achieve a tighter lower bound of the log-likelihood. The most notable improvement appears to be reflected in the reconstruction loss, showing the advantage of using cluster-specialized decoders. However, this improvement comes at the expense of a larger neural network architecture and an increase in the number of parameters (as TreeVAE has \(L\) decoders). While this requires more computational resources at training time, during deployment the tree structure of TreeVAE permits lightweight inference through conditional sampling, thus matching the inference time of LadderVAE. It is also worth mentioning that results differ from (Sonderby et al., 2016) as we adapt their architecture to match our experimental setting and consequently use smaller latent dimensionality. Finally, we notice that more complex methods are prone to overfitting on the 20Newsgroups dataset, hence the best performances are achieved by the VAE.

Real-world Imaging Data & Contrastive LearningClustering real-world imaging data is extremely difficult as there are endless possibilities of how the data can be partitioned (such as the colors, the landscape, etc). We therefore inject prior information through augmentations to guide TreeVAE and the baselines to semantically meaningful splits. Table 1 (bottom) shows the hierarchical clustering performance of TreeVAE and its baselines, all employing contrastive learning, on CIFAR-10 and CIFAR-100. We observe that DeepECT struggles in separating the data as their contrastive approach leads to all samples falling into the same leaf. In Table 3, we present the leaf-frequencies of various face attributes using the tree learned by TreeVAE. For all datasets, TreeVAE is able to group the data into contextually meaningful hierarchies and groups, evident from its superior performance compared to the baselines and from the distinct attribute frequencies in the leaves and subtrees.

Discovery of HierarchiesIn addition to solely clustering data, TreeVAE is able to discover meaningful hierarchical relations between the clusters, thus allowing for more insights into the

\begin{table}
\begin{tabular}{l l c c c} \hline \hline Dataset & Method & LL & RL & ELBO \\ \hline MNIST & VAE & \(-101.9\pm 0.2\) & \(87.2\pm 0.3\) & \(-104.6\pm 0.3\) \\  & LadderVAE & \(-99.9\pm 0.5\) & \(87.8\pm 0.7\) & \(-103.2\pm 0.7\) \\  & TreeVAE (ours) & \(\mathbf{-92.9\pm 0.2}\) & \(\mathbf{80.3\pm 0.2}\) & \(\mathbf{-96.8\pm 0.2}\) \\ \hline Fashion & VAE & \(-242.2\pm 0.2\) & \(231.7\pm 0.5\) & \(-245.4\pm 0.5\) \\  & LadderVAE & \(-239.4\pm 0.5\) & \(231.5\pm 0.6\) & \(-243.0\pm 0.6\) \\  & TreeVAE (Ours) & \(\mathbf{-234.7\pm 0.1}\) & \(\mathbf{226.5\pm 0.3}\) & \(\mathbf{-239.2\pm 0.4}\) \\ \hline
20Newsgroups & VAE & \(\mathbf{-44.26\pm 0.01}\) & \(45.52\pm 0.03\) & \(\mathbf{-44.61\pm 0.01}\) \\  & LadderVAE & \(-44.30\pm 0.03\) & \(\mathbf{43.52\pm 0.03}\) & \(\mathbf{-44.62\pm 0.02}\) \\  & TreeVAE (Ours) & \(-51.67\pm 0.59\) & \(45.83\pm 0.36\) & \(-52.79\pm 0.66\) \\ \hline Omniglot & VAE & \(-115.3\pm 0.3\) & \(101.6\pm 0.3\) & \(-118.2\pm 0.3\) \\  & LadderVAE & \(-113.1\pm 0.5\) & \(100.7\pm 0.7\) & \(-117.5\pm 0.6\) \\  & TreeVAE (Ours) & \(\mathbf{-110.4\pm 0.5}\) & \(\mathbf{96.9\pm 0.5}\) & \(\mathbf{-114.6\pm 0.4}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Test set generative performances of TreeVAE with 10 leaves compared with baselines. Means and standard deviations are computed across 10 runs with different random model initialization.

dataset. In the introductory Fig. 1, 5, and 6, we present the hierarchical structures learned by TreeVAE, while in Fig. 4 and 7, we additionally display conditional cluster generations from the leaf-specific decoders. In Fig. 4, TreeVAE separates the fashion items into two subtrees, one containing shoes and bags, and the other containing the tops, which are further refined into long and short sleeves. In Fig. 5, we depict the most prevalent ground-truth topic label in each leaf. TreeVAE learns to

Figure 4: Hierarchical structures learned by TreeVAE on Fashion. Subtree (a) encodes tops, while (b) encodes shoes, purses, and pants.

Figure 5: Hierarchical structure learned by TreeVAE on 20Newsgroups.

Figure 6: Hierarchical structures learned by TreeVAE on Omniglot-5. Subtree (a) learns a hierarchy over Braille and the Indian alphabets, while (b) groups Slavic alphabets.

\begin{table}
\begin{tabular}{l c c c c c c c c} Attribute & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\ \hline Female & \(97.2\) & \(55.0\) & \(97.7\) & \(86.6\) & \(23.1\) & \(30.7\) & \(46.6\) & \(43.7\) \\ Bangs & \(1.6\) & \(1.2\) & \(24.1\) & \(61.7\) & \(3.4\) & \(11.1\) & \(9.1\) & \(11.4\) \\ Blonde & \(1.1\) & \(3.7\) & \(66.7\) & \(2.2\) & \(5.8\) & \(2.6\) & \(26.1\) & \(7.1\) \\ Makeup & \(75.7\) & \(43.4\) & \(76.6\) & \(59.7\) & \(15.0\) & \(12.4\) & \(16.3\) & \(12.8\) \\ Smiling & \(54.3\) & \(66.6\) & \(66.4\) & \(51.2\) & \(54.7\) & \(42.4\) & \(37.3\) & \(22.4\) \\ Hair Loss & \(3.6\) & \(17.8\) & \(3.0\) & \(0.2\) & \(18.9\) & \(6.9\) & \(21.2\) & \(10.6\) \\ Beard & \(1.1\) & \(20.6\) & \(0.4\) & \(3.7\) & \(39.5\) & \(36.5\) & \(21.3\) & \(21.4\) \\ \hline \end{tabular}
\end{table}
Table 3: We present the frequency (in %) of selected attributes for each leaf of TreeVAE with eight leaves in CelebA.

Figure 6: Hierarchical structures learned by TreeVAE on Omniglot-5. Subtree (a) learns a hierarchy over Braille and the Indian alphabets, while (b) groups Slavic alphabets.

Figure 7: Hierarchical structure learned by TreeVAE with eight leaves on the CelebA dataset with generated images through conditional sampling. Generally, most females are in the left subtree, while most males are in the right subtree. We observe that leaf \(1\) is associated with dark-haired females, leaf \(2\) with smiling, dark-haired individuals, leaf \(3\) with blonde females, leaf \(4\) with bangs, leaf \(7\) with a receding hairline, and leaf \(8\) with non-smiling people. See Table 3 for further details.

separate technological and societal subjects and discovers semantically meaningful subtrees. In Fig. 6, TreeVAE learns to split alphabets into Indian (Odia and Bengali) and Slavic (Glagolitic and Cyrillic) subtrees, while Braille is grouped with the Indian languages due to similar circle-like structures. For CelebA, Fig. 7 and Table 3, the resulting tree separates genders in the root split. Females (left) are further divided by hair color and hairstyle (bangs). Males (right) are further divided by smile intensity, beard, hair loss, and age. In Fig. 8 and Appendix C we show how TreeVAE can additionally be used to sample unconditional generations for all clusters simultaneously, by sampling from the root and propagating through the entire tree. The generations differ across the leaves by their cluster-specific features, whereas cluster-independent properties are retained across all generations.

## 6 Conclusion

In this paper, we introduced TreeVAE, a new generative method that leverages a tree-based posterior distribution of latent variables to capture the hierarchical structures present in the data. TreeVAE optimizes the balance between shared and specialized architecture, enhancing the learning and adaptation capabilities of generative models. Empirically, we showed that our model offers a substantial improvement in hierarchical clustering performance compared to the related work, while also providing a tighter lower bound to the log-likelihood of the data. We presented qualitatively how the hierarchical structures learned by TreeVAE enable a more comprehensive understanding of the data, thereby facilitating enhanced analysis, interpretation, and decision-making. Our findings highlight the versatility of the proposed approach, which we believe to hold significant potential for unsupervised representation learning, paving the way for exciting advancements in the field.

**Limitations & Future Work:** Currently, TreeVAE uses a simple heuristic on which node to split that might not work on datasets with unbalanced clusters. Additionally, the contrastive losses on the routers encourage balanced clusters. Thus, more research is necessary to convert the heuristics to data-driven approaches. While deep latent variable models, such as VAEs, provide a framework for modeling explicit relationships through graphical structures, they often exhibit poor performance on synthetic image generation. However, more complex architectural design (Vahdat and Kautz, 2020) or recent advancement in diffusion latent models (Rombach et al., 2021) present potential solutions to enhance image quality generation, thus striking an optimal balance between generating high-quality images and capturing meaningful representations.

## Acknowledgments and Disclosure of Funding

We thank Thomas M. Sutter for the insightful discussions throughout the project, Jorge da Silva Goncalves for providing interpretable visualizations of the TreeVAE model, and Gabriele Manduchi for the valuable feedback on the notation of the ELBO. LM is supported by the SDSC PhD Fellowship #1-001568-037. MV is supported by the Swiss State Secretariat for Education, Research and Innovation (SERI) under contract number MB22.00047. AR is supported by the StimuLoop grant #1-007811-002 and the Vontobel Foundation.

Figure 8: Selected unconditional generations of CelebA. One row corresponds to one sample from the root, for which we depict the visualizations obtained from the \(8\) leaf-decoders. The overall face shape, skin color, and face orientation are retained among leaves from the same row, while several properties (such as make-up, beard, mustache, glasses, and hair) vary across the different leaves.

## References

* (1)
* Arenas et al. (2022) Arenas, M., Barcelo, P., Orth, M. A. R., & Subercaseaux, B. (2022). On computing probabilistic explanations for decision trees. In _Neurips_. Retrieved from [http://papers.nips.cc/paper_files/paper/2022/hash/b8963f6a0a72e686dfa98ac3e7260f73-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/b8963f6a0a72e686dfa98ac3e7260f73-Abstract-Conference.html)
* Bae et al. (2023) Bae, J., Zhang, M. R., Ruan, M., Wang, E., Hasegawa, S., Ba, J., & Grosse, R. B. (2023). Multi-rate VAE: Train once, get the full rate-distortion curve. In _The eleventh international conference on learning representations_. Retrieved from [https://openreview.net/forum?id=0J8aSjCaMNK](https://openreview.net/forum?id=0J8aSjCaMNK)
* Basak & Krishnapuram (2005) Basak, J., & Krishnapuram, R. (2005). Interpretable hierarchical clustering by constructing an unsupervised decision tree. _IEEE Trans. Knowl. Data Eng._, _17_(1), 121-132.
* Bengio et al. (2012) Bengio, Y., Courville, A. C., & Vincent, P. (2012). Representation learning: A review and new perspectives. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, _35_, 1798-1828.
* Bishop (2006) Bishop, C. M. (2006). _Pattern recognition and machine learning (information science and statistics)_. Berlin, Heidelberg: Springer-Verlag.
* Blei et al. (2003) Blei, D. M., Jordan, M. I., Griffiths, T. L., & Tenenbaum, J. B. (2003). Hierarchical topic models and the nested chinese restaurant process. In _Proceedings of the 16th international conference on neural information processing systems_ (p. 17-24). Cambridge, MA, USA: MIT Press.
* Blockeel & De Raedt (1998) Blockeel, H., & De Raedt, L. (1998). Top-down induction of clustering trees. In _Proceedings of the fifteenth international conference on machine learning_ (pp. 55-63).
* Bredell et al. (2023) Bredell, G., Flouris, K., Chaitanya, K., Erdil, E., & Konukoglu, E. (2023). Explicitly minimizing the blur error of variational autoencoders. In _The eleventh international conference on learning representations_. Retrieved from [https://openreview.net/forum?id=9krnQ-ue9M](https://openreview.net/forum?id=9krnQ-ue9M)
* Breiman et al. (1984) Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). _Classification and regression trees_. Wadsworth.
* Briggs et al. (2020) Briggs, C., Fan, Z., & Andras, P. (2020). Federated learning with hierarchical clustering of local updates to improve training on non-iid data. _2020 International Joint Conference on Neural Networks (IJCNN)_, 1-9.
* Campello et al. (2015) Campello, R. J., Moulavi, D., Zimek, A., & Sander, J. (2015). Hierarchical density estimates for data clustering, visualization, and outlier detection. _ACM Transactions on Knowledge Discovery from Data (TKDD)_, _10_(1), 1-51.

* Chen et al. (2020) Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020). A simple framework for contrastive learning of visual representations. In _Proceedings of the 37th international conference on machine learning_. JMLR.org.
* Dilokthanakul et al. (2016) Dilokthanakul, N., Mediano, P. A. M., Garnelo, M., Lee, M. C. H., Salimbeni, H., Arulkumaran, K., & Shanahan, M. (2016). _Deep unsupervised clustering with Gaussian mixture variational autoencoders_. (arXiv:1611.02648)
* Ester et al. (1996) Ester, M., Kriegel, H., Sander, J., & Xu, X. (1996). A density-based algorithm for discovering clusters in large spatial databases with noise. In E. Simoudis, J. Han, & U. M. Fayyad (Eds.), _Proceedings of the second international conference on knowledge discovery and data mining (kdd-96), portland, oregon, USA_ (pp. 226-231). AAAI Press. Retrieved from [http://www.aaai.org/Library/KDD/1996/kdd96-037.php](http://www.aaai.org/Library/KDD/1996/kdd96-037.php)
* Falck et al. (2022) Falck, F., Williams, C., Danks, D., Deligiannidis, G., Yau, C., Holmes, C. C.,... Willetts, M. (2022). A multi-resolution framework for u-nets with applications to hierarchical VAEs. In A. H. Oh, A. Agarwal, D. Belgrave, & K. Cho (Eds.), _Advances in neural information processing systems_. Retrieved from [https://openreview.net/forum?id=PQFr7FbGbO](https://openreview.net/forum?id=PQFr7FbGbO)
* Fraiman et al. (2013) Fraiman, R., Ghattas, B., & Svarc, M. (2013). Interpretable clustering using unsupervised binary trees. _Adv. Data Anal. Classif._, _7_(2), 125-145. Retrieved from [https://doi.org/10.1007/s11634-013-0129-3](https://doi.org/10.1007/s11634-013-0129-3)
* Falsh (2013)Frosst, N., & Hinton, G. E. (2017). Distilling a neural network into a soft decision tree. In T. R. Besold & O. Kutz (Eds.), _Proceedings of the first international workshop on comprehensibility and explanation in AI and ML 2017 co-located with 16th international conference of the italian association for artificial intelligence (ai"ia 2017), bari, Italy, november 16th and 17th, 2017_ (Vol. 2071). CEUR-WS.org. Retrieved from [https://ceur-ws.org/Vol-2071/CExAIIA_2017_paper_3.pdf](https://ceur-ws.org/Vol-2071/CExAIIA_2017_paper_3.pdf)
* [Ghojogh et al.2021] Ghojogh, B., Ghodsi, A., Karray, F., & Crowley, M. (2021). Uniform manifold approximation and projection (UMAP) and its variants: Tutorial and survey. _CoRR, abs/2109.02508_. Retrieved from [https://arxiv.org/abs/2109.02508](https://arxiv.org/abs/2109.02508)
* [Goyal et al.2017] Goyal, P., Hu, Z., Liang, X., Wang, C., Xing, E. P., & Mellon, C. (2017). Nonparametric variational auto-encoders for hierarchical representation learning. _2017 IEEE International Conference on Computer Vision (ICCV)_, 5104-5112.
* [Gregor et al.2015] Gregor, K., Danihelka, I., Graves, A., Rezende, D., & Wierstra, D. (2015). Draw: A recurrent neural network for image generation. In F. Bach & D. Blei (Eds.), _Proceedings of the 32nd international conference on machine learning_ (Vol. 37, pp. 1462-1471). Lille, France: PMLR. Retrieved from [https://proceedings.mlr.press/v37/gregor15.html](https://proceedings.mlr.press/v37/gregor15.html)
* [He et al.2018] He, J., Gong, Y., Marino, J., Mori, G., & Lehrmann, A. M. (2018). Variational autoencoders with jointly optimized latent dependency structure. In _International conference on learning representations_.
* [He et al.2019] He, J., Gong, Y., Marino, J., Mori, G., & Lehrmann, A. M. (2019). Variational autoencoders with jointly optimized latent dependency structure. In _Iclr_.
* [Heller & Ghahramani2005] Heller, K. A., & Ghahramani, Z. (2005). Bayesian hierarchical clustering. In _Proceedings of the 22nd international conference on machine learning_ (pp. 297-304).
* [Jordan & Mitchell2015] Jordan, M. I., & Mitchell, T. M. (2015). Machine learning: Trends, perspectives, and prospects. _Science, 349_(6245), 255-260. Retrieved from [https://www.science.org/doi/abs/10.1126/science.aaa8415](https://www.science.org/doi/abs/10.1126/science.aaa8415)
* [Kingma et al.2016] Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., & Welling, M. (2016). Improved variational inference with inverse autoregressive flow. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, & R. Garnett (Eds.), _Advances in neural information processing systems_ (Vol. 29). Curran Associates, Inc. Retrieved from [https://proceedings.neurips.cc/paper_files/paper/2016/file/ddeebdeefdb7e7e697e1c3e3d8ef54-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2016/file/ddeebdeefdb7e7e697e1c3e3d8ef54-Paper.pdf)
* [Kingma & Welling2014] Kingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_.
* [Kingma & Welling2019] Kingma, D. P., & Welling, M. (2019). An introduction to variational autoencoders. _Found. Trends Mach. Learn._, _12_, 307-392.
* [Klushyn et al.2019] Klushyn, A., Chen, N., Kurle, R., Cseke, B., & Smagt, P. v. d. (2019). Learning hierarchical priors in vaes. In _Proceedings of the 33rd international conference on neural information processing systems_. Red Hook, NY, USA: Curran Associates Inc.
* 17, 2017_ (pp. 255-264). ACM. Retrieved from [https://doi.org/10.1145/3097983.3098079](https://doi.org/10.1145/3097983.3098079)
* [Kobren et al.2017b] Kobren, A., Monath, N., Krishnamurthy, A., & McCallum, A. (2017b). A hierarchical algorithm for extreme clustering. In _Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining_ (pp. 255-264).
* [Krizhevsky & Hinton2009] Krizhevsky, A., & Hinton, G. (2009). _Learning multiple layers of features from tiny images_ (Tech. Rep. No. 0). Toronto, Ontario: University of Toronto.
* [Lake et al.2015] Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. _Science_, _350_(6266), 1332-1338. Retrieved from [https://www.science.org/doi/abs/10.1126/science.aab3050](https://www.science.org/doi/abs/10.1126/science.aab3050)
* [Lang1995] Lang, K. (1995). Newsweeder: Learning to filter netnews. In _Proceedings of the twelfth international conference on machine learning_ (p. 331-339).
* [Lake et al.2015]Laptev, D., & Buhmann, J. M. (2014). Convolutional decision trees for feature learning and segmentation. In _Pattern recognition: 36th german conference, gcpr 2014, minster, germany, september 2-5, 2014, proceedings 36_ (pp. 95-106).
* LeCun et al. (2015) LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. _Nature, 521_(7553), 436-444.
* LeCun et al. (1998) LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. _Proc. IEEE_, 86, 2278-2324.
* Li et al. (2021) Li, Y., Hu, P., Liu, J. Z., Peng, D., Zhou, J. T., & Peng, X. (2021). Contrastive clustering. In _Thirty-fifth AAAI conference on artificial intelligence, AAAI 2021, thirty-third conference on innovative applications of artificial intelligence, IAAI 2021, the eleventh symposium on educational advances in artificial intelligence, EAAI 2021, virtual event, february 2-9, 2021_ (pp. 8547-8555). AAAI Press. Retrieved from [https://ojs.aaai.org/index.php/AAAI/article/view/17037](https://ojs.aaai.org/index.php/AAAI/article/view/17037)
* Li et al. (2022) Li, Y., Yang, M., Peng, D., Li, T., Huang, J., & Peng, X. (2022). Twin contrastive learning for online clustering. _International Journal of Computer Vision, 130_(9), 2205-2221.
* Liu et al. (2000) Liu, B., Xia, Y., & Yu, P. S. (2000). Clustering through decision tree construction. In _International conference on information and knowledge management_.
* Liu et al. (2015) Liu, Z., Luo, P., Wang, X., & Tang, X. (2015). Deep learning face attributes in the wild. In _Proceedings of international conference on computer vision (iccv)_.
* Maaloe et al. (2019) Maaloe, L., Fraccaro, M., Lievin, V., & Winther, O. (2019). Biva: A very deep hierarchy of latent variables for generative modeling. In _Neurips_.
* Manduchi et al. (2021) Manduchi, L., Chin-Cheong, K., Michel, H., Wellmann, S., & Vogt, J. E. (2021). Deep conditional gaussian mixture model for constrained clustering. In _Neural information processing systems_.
* Mathieu et al. (2019) Mathieu, E., Le Lan, C., Maddison, C. J., Tomioka, R., & Teh, Y. W. (2019). Continuous hierarchical representations with poincare variational auto-encoders. _Advances in neural information processing systems, 32_.
* Mattei & Frellsen (2018) Mattei, P.-A., & Frellsen, J. (2018). Leveraging the exact likelihood of deep latent variable models. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, & R. Garnett (Eds.), _Advances in neural information processing systems_ (Vol. 31). Curran Associates, Inc. Retrieved from [https://proceedings.neurips.cc/paper_files/paper/2018/file/0609154fa35b319402634669cac2a248-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2018/file/0609154fa35b319402634669cac2a248-Paper.pdf)

* McConnachie et al. (2018) McConnachie, A. W., Ibata, R. A., Martin, N., Ferguson, A. M. N., Collins, M. L. M., Gwyn, S. D. J.,... Widrow, L. M. (2018). The large-scale structure of the halo of the andromeda galaxy. ii. hierarchical structure in the pan-andromeda archaeological survey. _The Astrophysical Journal_, _868_.
* Monath et al. (2019) Monath, N., Zaheer, M., Silva, D., McCallum, A., & Ahmed, A. (2019). Gradient-based hierarchical clustering using continuous representations of trees in hyperbolic space. In _Proceedings of the 25th acm sigkdd international conference on knowledge discovery & data mining_ (pp. 714-722).
* Moshkovitz et al. (2021) Moshkovitz, M., Yang, Y., & Chaudhuri, K. (2021). Connecting interpretability and robustness in decision trees through separation. In M. Meila & T. Zhang (Eds.), _Proceedings of the 38th international conference on machine learning, ICML 2021, 18-24 July 2021, virtual event_ (Vol. 139, pp. 7839-7849). PMLR. Retrieved from [http://proceedings.mlr.press/v139/moshkovitz21a.html](http://proceedings.mlr.press/v139/moshkovitz21a.html)
* Murtagh & Contreras (2012) Murtagh, F., & Contreras, P. (2012). Algorithms for hierarchical clustering: an overview. _Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery_, 2(1), 86-97.
* Murtagh & Legendre (2014) Murtagh, F., & Legendre, P. (2014). Ward's hierarchical agglomerative clustering method: Which algorithms implement ward's criterion? _Journal of Classification, 31_, 274-295.
* Nasiri & Bepler (2022) Nasiri, A., & Bepler, T. (2022). Unsupervised object representation learning using translation and rotation group equivariant VAE. In A. H. Oh, A. Agarwal, D. Belgrave, & K. Cho (Eds.), _Advances in neural information processing systems_. Retrieved from [https://openreview.net/forum?id=qmm_jMjMlL](https://openreview.net/forum?id=qmm_jMjMlL)
* Neal (2003) Neal, R. M. (2003). Density Modeling and Clustering using Dirichlet Diffusion Trees. In J. M. Bernardo et al. (Eds.), _Bayesian Statistics 7_ (p. 619-629). Oxford University Press.
* Nemirovsky (1996)Nister, D., & Stewenius, H. (2006). Scalable recognition with a vocabulary tree. _2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), 2_, 2161-2168.
* Pace et al. (2022) Pace, A., Chan, A. J., & van der Schaar, M. (2022). POETREE: interpretable policy learning with adaptive decision trees. In _The tenth international conference on learning representations, ICLR 2022, virtual event, april 25-29, 2022_. OpenReview.net. Retrieved from [https://openreview.net/forum?id=AJsI-ymakn_](https://openreview.net/forum?id=AJsI-ymakn_).
* Ram & Gray (2011) Ram, P., & Gray, A. G. (2011). Density estimation trees. In _Knowledge discovery and data mining_.
* Ranganath et al. (2015) Ranganath, R., Tran, D., & Blei, D. M. (2015). Hierarchical variational models. _ArXiv_, _abs/1511.02386_.
* Rezende et al. (2014) Rezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In _Proceedings of the 31st international conference on machine learning_ (Vol. 32, pp. 1278-1286). PMLR.
* Rombach et al. (2021) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021). High-resolution image synthesis with latent diffusion models. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 10674-10685.
* Rota Bulo & Kontschieder (2014) Rota Bulo, S., & Kontschieder, P. (2014). Neural decision forests for semantic image labelling. In _Proceedings of the ieee conference on computer vision and pattern recognition_ (pp. 81-88).
* Shin et al. (2019) Shin, S.-J., Song, K., & Moon, I.-C. (2019). Hierarchically clustered representation learning. In _Aaai conference on artificial intelligence_.
* Sneath (1957) Sneath, P. H. (1957). The application of computers to taxonomy. _Microbiology_, _17_(1), 201-226.
* Sneath & Sokal (1962) Sneath, P. H., & Sokal, R. R. (1962). Numerical taxonomy. _Nature_, _193_, 855-860.
* Sohn (2016) Sohn, K. (2016). Improved deep metric learning with multi-class n-pair loss objective. In D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, & R. Garnett (Eds.), _Advances in neural information processing systems 29: Annual conference on neural information processing systems 2016, december 5-10, 2016, barcelona, Spain_ (pp. 1849-1857). Retrieved from [https://proceedings.neurips.cc/paper/2016/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html](https://proceedings.neurips.cc/paper/2016/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html)
* Sonderby et al. (2016) Sonderby, C. K., Raiko, T., Maaloe, L., Sonderby, S. K., & Winther, O. (2016). Ladder variational autoencoders. _Advances in neural information processing systems_, _29_.
* Souza et al. (2022) Souza, V. F., Cicalese, F., Laber, E. S., & Molinaro, M. (2022). Decision trees with short explainable rules. In _Neurips_. Retrieved from [http://papers.nips.cc/paper_files/paper/2022/hash/500637d931d4feb99d5cce84af1f53ba-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/500637d931d4feb99d5cce84af1f53ba-Abstract-Conference.html)
* Steinbach et al. (2000) Steinbach, M. S., Karypis, G., & Kumar, V. (2000). A comparison of document clustering techniques..
* Suarez & Lutsko (1999) Suarez, A., & Lutsko, J. F. (1999). Globally optimal fuzzy decision trees for classification and regression. _IEEE Trans. Pattern Anal. Mach. Intell._, _21_(12), 1297-1311. Retrieved from [https://doi.org/10.1109/34.817409](https://doi.org/10.1109/34.817409)
* Tanno et al. (2019) Tanno, R., Arulkumaran, K., Alexander, D., Criminisi, A., & Nori, A. (2019). Adaptive neural trees. In K. Chaudhuri & R. Salakhutdinov (Eds.), _Proceedings of the 36th international conference on machine learning_ (Vol. 97, pp. 6166-6175). PMLR. Retrieved from [https://proceedings.mlr.press/v97/tanno19a.html](https://proceedings.mlr.press/v97/tanno19a.html)
* Vahdat & Kautz (2020a) Vahdat, A., & Kautz, J. (2020a). Nvae: A deep hierarchical variational autoencoder. In _Proceedings of the 34th international conference on neural information processing systems_. Red Hook, NY, USA: Curran Associates Inc.
* Vahdat & Kautz (2020b) Vahdat, A., & Kautz, J. (2020b). Nvae: A deep hierarchical variational autoencoder. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, & H. Lin (Eds.), _Advances in neural information processing systems_ (Vol. 33, pp. 19667-19679). Curran Associates, Inc. Retrieved from [https://proceedings.neurips.cc/paper/2020/file/e3b21256183cf7c2c7a66be163579d37-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/e3b21256183cf7c2c7a66be163579d37-Paper.pdf)
* van den Oord et al. (2018) van den Oord, A., Li, Y., & Vinyals, O. (2018). Representation learning with contrastive predictive coding. _CoRR_, _abs/1807.03748_. Retrieved from [http://arxiv.org/abs/1807.03748](http://arxiv.org/abs/1807.03748)
* Vikram et al. (2018) Vikram, S., Hoffman, M. D., & Johnson, M. J. (2018). The loracs prior for vaes: Letting the trees speak for the data. _ArXiv_, _abs/1810.06891_.
* Vahdat & Kautz (2020)Wan, A., Dunlap, L., Ho, D., Yin, J., Lee, S., Petryk, S.,... Gonzalez, J. E. (2021). NBDT: neural-backed decision tree. In _9th international conference on learning representations, ICLR 2021, virtual event, austria, may 3-7, 2021_. OpenReview.net. Retrieved from [https://openreview.net/forum?id=mCLVeEpp1NE](https://openreview.net/forum?id=mCLVeEpp1NE)
* Ward (1963) Ward, J. H. (1963). Hierarchical grouping to optimize an objective function. _Journal of the American Statistical Association_, _58_, 236-244.
* Webb et al. (2017) Webb, S., Golinski, A., Zinkov, R., Narayanaswamy, S., Rainforth, T., Teh, Y. W., & Wood, F. (2017). Faithful inversion of generative models for effective amortized inference. In _Neural information processing systems_.
* Williams (1999) Williams, C. (1999). A mcmc approach to hierarchical mixture modelling. _Advances in Neural Information Processing Systems_, _12_.
* Wu et al. (2018) Wu, Z., Xiong, Y., Yu, S. X., & Lin, D. (2018). Unsupervised feature learning via non-parametric instance discrimination. In _2018 IEEE conference on computer vision and pattern recognition, CVPR 2018, salt lake city, ut, usa, june 18-22, 2018_ (pp. 3733-3742). Computer Vision Foundation / IEEE Computer Society. Retrieved from [http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html](http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html) doi:10.1109/CVPR.2018.00393
* Xiao et al. (2017) Xiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _CoRR_, _abs/1708.07747_. Retrieved from [http://arxiv.org/abs/1708.07747](http://arxiv.org/abs/1708.07747)
* Xiao & Bamler (2023) Xiao, T. Z., & Bamler, R. (2023). Trading information between latents in hierarchical variational autoencoders. In _The eleventh international conference on learning representations_. Retrieved from [https://openreview.net/forum?id=wWtMdr6yCmL](https://openreview.net/forum?id=wWtMdr6yCmL)
* You et al. (2015) You, C., Robinson, D. P., & Vidal, R. (2015). Scalable sparse subspace clustering by orthogonal matching pursuit. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 3918-3927.
* Zharmagambetov & Carreira-Perpinan (2022) Zharmagambetov, A., & Carreira-Perpinan, M. A. (2022). Semi-supervised learning with decision trees: Graph laplacian tree alternating optimization. In _Neurips_. Retrieved from [http://papers.nips.cc/paper_files/paper/2022/hash/104f7b25495a0e40e65fb7c7eee37ed9-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/104f7b25495a0e40e65fb7c7eee37ed9-Abstract-Conference.html)
* Zhou & Feng (2017) Zhou, Z.-H., & Feng, J. (2017). Deep forest: Towards an alternative to deep neural networks. In _Proceedings of the twenty-sixth international joint conference on artificial intelligence, IJCAI-17_ (pp. 3553-3559). Retrieved from [https://doi.org/10.24963/ijcai.2017/497](https://doi.org/10.24963/ijcai.2017/497) doi:10.24963/ijcai.2017/497

[MISSING_PAGE_FAIL:16]

where \(q(\mathsf{c}_{0}=0\mid\mathbf{z}_{0})+q(\mathsf{c}_{0}=1\mid\mathbf{z}_{0})=1\) and \(\mathrm{KL}\left(q(\mathbf{z}_{0}\mid\mathbf{x})\|p(\mathbf{z}_{0})\right)\) is the KL between two Gaussians, which can be computed analytically.

#### a.1.2 KL Decisions

The second term (25) corresponds to the KL of the decisions. We can pull out the product from the log, yielding

\[\mathrm{KL}_{decisions}=\sum_{l\in\mathbb{L}}\int_{\mathbf{z}_{\mathcal{ P}_{l}}}q(\mathbf{z}_{0}\mid\mathbf{x})\prod_{i\in\mathcal{P}_{l}\setminus\{0\}}q(c_{pa(i) \to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)})\\ \times\log\left(\prod_{j\in\mathcal{P}_{l}\setminus\{0\}}\frac{q(c _{pa(j)\to j}\mid\mathbf{x})}{p(c_{pa(j)\to j}\mid\mathbf{z}_{pa(j)})}\right)\\ =\sum_{l\in\mathbb{L}}\int_{\mathbf{z}_{\mathcal{P}_{l}}}\sum_{j\in \mathcal{P}_{l}\setminus\{0\}}q(\mathbf{z}_{0}\mid\mathbf{x})\prod_{i\in\mathcal{P}_{l }\setminus\{0\}}q(c_{pa(i)\to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)}) \log\left(\frac{q(c_{pa(j)\to j}\mid\mathbf{x})}{p(c_{pa(j)\to j}\mid\mathbf{z}_{pa(j) })}\right) \tag{31}\]

Let us define as \(\mathcal{P}_{l\in j}\) all paths that go through node \(j\), as \(\mathcal{P}_{\leq j}\) (denoted as \(\mathcal{P}_{j}\) in the main text for brevity) the unique path that ends in the node \(j\), and as \(\mathcal{P}_{>j}\) all the possible paths that start from the node \(j\) and continue to a leaf \(l\in\mathbb{L}\). Similarly, let us define as \(\mathbf{z}_{\leq j}\) all the latent embeddings that are contained in the path from the root to node \(j\) and as \(\mathbf{z}_{>j}\) all the latent embeddings of the nodes \(i>j\) that can be reached from node \(j\).

To factorize the above equation, we first change from a pathwise view to a nodewise view. Instead of summing over all possible leaves in the tree (\(\sum_{l\in\mathbb{L}}\)) and then over each contained node (\(\sum_{j\in\mathcal{P}_{l}\setminus\{0\}}\)), we sum over all nodes (\(\sum_{j\in\mathcal{V}\setminus\{0\}}\)) and then over each path that leads through the selected node (\(\sum_{\mathcal{P}_{l\in j}}\)).

\[\sum_{l\in\mathbb{L}}\int_{\mathbf{z}_{\mathcal{P}_{l}}}\sum_{j\in \mathcal{P}_{l}\setminus\{0\}}q(\mathbf{z}_{0}\mid\mathbf{x})\prod_{i\in\mathcal{P}_{l }\setminus\{0\}}q(c_{pa(i)\to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)}) \log\left(\frac{q(c_{pa(j)\to j}\mid\mathbf{x})}{p(c_{pa(j)\to j}\mid\mathbf{z}_{pa(j) })}\right)\\ =\sum_{j\in\mathcal{V}\setminus\{0\}}\sum_{\mathcal{P}_{l\in j}} \int_{\mathbf{z}_{\mathcal{P}_{l}}}q(\mathbf{z}_{0}\mid\mathbf{x})\prod_{i\in\mathcal{P}_{ l}\setminus\{0\}}q(c_{pa(i)\to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)}) \log\left(\frac{q(c_{pa(j)\to j}\mid\mathbf{x})}{p(c_{pa(j)\to j}\mid\mathbf{z}_{pa(j )})}\right) \tag{33}\]

The above can be proved with the following Lemma, where we rewrite \(\sum_{\mathcal{P}_{l\in j}}=\sum_{l\in\mathbb{L}}\mathds{1}[j\in\mathcal{P}_{ l}]\).

**Lemma A.1**.: _Given a binary tree \(\mathcal{T}\) as defined in Section 2.1, composed of a set of nodes \(\mathbb{V}=\{0,\ldots,V\}\) and leaves \(\mathbb{L}\subset\mathbb{V}\), where \(\mathcal{P}_{l}\) is the decision path from root \(0\) to leaf \(l\), and \(\mathbf{z}_{\mathcal{P}_{l}}=\{\mathbf{z}_{i}\mid i\in\mathcal{P}_{l}\}\) the set of latent variables selected by the path \(\mathcal{P}_{l}\). Then it holds_

\[\sum_{l\in\mathbb{L}}\int_{\mathbf{z}_{\mathcal{P}_{l}}}\sum_{j\in \mathcal{P}_{l}\setminus\{0\}}f(j,l,\mathbf{z}_{\mathcal{P}_{l}})=\sum_{j\in \mathcal{V}\setminus\{0\}}\sum_{l\in\mathbb{L}}\int_{\mathbf{z}_{\mathcal{P}_{l}} }\mathds{1}[j\in\mathcal{P}_{l}]f(j,l,\mathbf{z}_{\mathcal{P}_{l}}), \tag{34}\]Proof.: The proof is as follows:

\[\sum_{j\in\V\setminus\{0\}}\sum_{l\in\mathbb{L}}\int_{\mathbf{z}_{\mathcal{ P}_{l}}}\mathds{1}[j\in\mathcal{P}_{l}]f(j,l,\mathbf{z}_{\mathcal{P}_{l}}) =\sum_{j\in\V\setminus\{0\}}\sum_{l\in\mathbb{L}}\int_{\mathbf{z}_{ \mathcal{P}_{l}}}f(j,l,\mathbf{z}_{\mathcal{P}_{l}})\sum_{i\in\mathcal{P}_{l} \setminus\{0\}}\mathds{1}[i=j] \tag{35}\] \[=\sum_{l\in\mathbb{L}}\sum_{j\in\V\setminus\{0\}}\int_{\mathbf{z}_{ \mathcal{P}_{l}}}\sum_{i\in\mathcal{P}_{l}\setminus\{0\}}f(j,l,\mathbf{z}_{ \mathcal{P}_{l}})\mathds{1}[i=j]\] (36) \[=\sum_{l\in\mathbb{L}}\sum_{j\in\V\setminus\{0\}}\sum_{\mathbf{z}_{ \mathcal{P}_{l}}}\sum_{i\in\mathcal{P}_{l}\setminus\{0\}}f(i,l,\mathbf{z}_{ \mathcal{P}_{l}})\mathds{1}[i=j]\] (37) \[=\sum_{l\in\mathbb{L}}\int_{\mathbf{z}_{\mathcal{P}_{l}}}\sum_{i\in \mathcal{P}_{l}\setminus\{0\}}f(i,l,\mathbf{z}_{\mathcal{P}_{l}})\sum_{j\in\V \setminus\{0\}}\mathds{1}[i=j]\] (38) \[=\sum_{l\in\mathbb{L}}\int_{\mathbf{z}_{\mathcal{P}_{l}}}\sum_{i\in \mathcal{P}_{l}\setminus\{0\}}f(i,l,\mathbf{z}_{\mathcal{P}_{l}})\] (39) \[=\sum_{l\in\mathbb{L}}\int_{\mathbf{z}_{\mathcal{P}_{l}}}\sum_{j\in \mathcal{P}_{l}\setminus\{0\}}f(j,l,\mathbf{z}_{\mathcal{P}_{l}}). \tag{40}\]

Having proven the equality, we can continue with the KL of the decisions as follows:

\[\mathrm{KL}_{decisions}= \tag{41}\] \[=\sum_{j\in\V\setminus\{0\}}\sum_{\mathcal{P}_{l}\in j}\int_{\mathbf{ z}_{\mathcal{P}_{l}}}[q(\mathbf{z}_{0}\mid\mathbf{x})\hskip-1.0pt\prod_{i\in\mathcal{P}_{ \leq j}\setminus\{0\}}\hskip-1.0ptq(c_{pa(i)\to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{ z}_{pa(i)})\log\left(\frac{q(c_{pa(j)\to j}\mid\mathbf{x})}{p(c_{pa(j)\to j}\mid\mathbf{z}_{pa(j)})}\right)\] \[\qquad\times\prod_{k\in\mathcal{P}_{>j}}q(c_{pa(k)\to k}\mid\mathbf{x}) q(\mathbf{z}_{k}\mid\mathbf{z}_{pa(k)})]\] (42) \[=\sum_{j\in\V\setminus\{0\}}\sum_{\mathcal{P}_{>j}}\sum_{\mathbf{z}_{ \leq j},\mathbf{z}_{\leq j},\mathbf{z}_{\geq j}}\hskip-1.0ptq(c_{pa(i)\to i}\mid\mathbf{x} )q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)})\log\left(\frac{q(c_{pa(j)\to j}\mid\mathbf{x})}{p( c_{pa(j)\to j}\mid\mathbf{z}_{pa(j)})}\right)\] (43) \[\qquad\times\prod_{k\in\mathcal{P}_{>j}}q(c_{pa(k)\to k}\mid\mathbf{x}) q(\mathbf{z}_{k}\mid\mathbf{z}_{pa(k)})]\] \[=\sum_{j\in\V\setminus\{0\}}\int_{\mathbf{z}_{\leq j}}q(\mathbf{z}_{0} \mid\mathbf{x})\hskip-1.0pt\prod_{i\in\mathcal{P}_{\leq j}\setminus\{0\}}\hskip-1.0ptq (c_{pa(i)\to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)})\log\left(\frac{q(c_{ pa(j)\to j}\mid\mathbf{x})}{p(c_{pa(j)\to j}\mid\mathbf{z}_{pa(j)})}\right)\] \[\qquad\times\sum_{\mathcal{P}_{>j}}\int_{\mathbf{z}_{>j}}\Big{[}\prod _{k\in\mathcal{P}_{>j}}q(c_{pa(k)\to k}\mid\mathbf{x})q(\mathbf{z}_{k}\mid\mathbf{z}_{pa(k )})\Big{]}\]

From Eq. 41 to Eq. 42, we split the inner product into the nodes of the paths \(\mathcal{P}_{l\in j}\) that are before and after the node \(j\).

From Eq. 42 to Eq. 43, we observe that the sum over all paths going through \(j\) can be reduced to the sum over all paths starting from \(j\), because there is only one path to \(j\), which is specified in the product that comes after.

From Eq. 43 to Eq. 44, we observe that the sum over paths starting from \(j\) and integral over \(\mathbf{z}_{>j}\) concern only the terms of the second line. Observe that the term on the second line of Eq. 44integrates out to 1 and we get

\[\begin{split}&\mathrm{KL}_{decisions}=\\ &\quad\sum_{j\in\mathbb{V}\setminus\{0\}}\int_{\mathbf{z}\leq j}q(\mathbf{z }_{0}\mid\mathbf{x})\underset{i\in\mathcal{P}_{\leq j}\setminus\{0\}}{\prod}q(c_{pa (i)\to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)})\log\left(\frac{q(c_{pa(j) \to j}\mid\mathbf{x})}{p(c_{pa(j)\to j}\mid\mathbf{z}_{pa(j)})}\right)\\ &=\sum_{j\in\mathbb{V}\setminus\{0\}}\int_{\mathbf{z}_{<j}}\int_{\mathbf{z }_{j}}q(\mathbf{z}_{0}\mid\mathbf{x})\underset{i\in\mathcal{P}_{\leq j}\setminus\{0\}}{ \prod}q(c_{pa(i)\to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)})\log\left(\frac {q(c_{pa(j)\to j}\mid\mathbf{x})}{p(c_{pa(j)\to j}\mid\mathbf{z}_{pa(j)})}\right)\\ &=\sum_{j\in\mathbb{V}\setminus\{0\}}\int_{\mathbf{z}_{<j}}q(\mathbf{z}_{ 0}\mid\mathbf{x})\underset{i\in\mathcal{P}_{<j}\setminus\{0\}}{\prod}q(c_{pa(i) \to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)})\log\left(\frac{q(c_{pa(j) \to j}\mid\mathbf{x})}{p(c_{pa(j)\to j}\mid\mathbf{z}_{pa(j)})}\right)\\ &\quad\quad\times\int_{\mathbf{z}_{j}}q(c_{pa(j)\to j}\mid\mathbf{x})q( \mathbf{z}_{j}\mid\mathbf{z}_{pa(j)})\\ &=\sum_{j\in\mathbb{V}\setminus\{0\}}\int_{\mathbf{z}_{<j}}q(\mathbf{z}_{ 0}\mid\mathbf{x})\underset{i\in\mathcal{P}_{<j}\setminus\{0\}}{\prod}q(c_{pa(i) \to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)})q(c_{pa(j)\to j}\mid\mathbf{x})\\ &\quad\quad\times\log\left(\frac{q(c_{pa(j)\to j}\mid\mathbf{x})}{p( c_{pa(j)\to j}\mid\mathbf{z}_{pa(j)})}\right).\end{split} \tag{47}\]

From Eq. 46 to Eq. 47, we single out the term in the product that corresponds to \(j=i\), which is the only term that depends on \(\int_{z_{i}}\).

From Eq. 47 to Eq. 48, we observe that in the singled-out term, \(\int_{z_{j}}q(z_{j}\mid\mathbf{z}_{pa(j)})=1\), which leaves only \(q(c_{pa(j)\to j}\mid\mathbf{x})\).

This equation can be rewritten in a more interpretable way. Let us define the probability of reaching node \(j\) and observing \(\mathbf{z}_{j}\) as

\[P(j;\mathbf{z},\mathbf{c})=q(\mathbf{z}_{0}\mid\mathbf{x})\underset{i\in\mathcal{P}_{\leq j} \setminus\{0\}}{\prod}q(c_{pa(i)\to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i )}). \tag{49}\]

Then the KL term of the decisions can be simplified as

\[\begin{split}\mathrm{KL}_{decisions}&=\sum_{j\in \mathbb{V}\setminus\{0\}}\int_{\mathbf{z}_{<j}}P(pa(j);\mathbf{z},\mathbf{c})q(c_{pa(j) \to j}\mid\mathbf{x})\log\left(\frac{q(c_{pa(j)\to j}\mid\mathbf{x})}{p(c_{pa(j)\to j} \mid\mathbf{z}_{pa(j)})}\right)\\ &=\sum_{i\in\mathbb{V}\setminus\mathbb{L}}\sum_{k\in\{0,1\}}\int _{\mathbf{z}_{<i}}P(i;\mathbf{z},\mathbf{c})q(c_{i}=k\mid\mathbf{x})\log\left(\frac{q(c_{i}=k \mid\mathbf{x})}{p(c_{i}=k\mid\mathbf{z}_{i})}\right).\end{split} \tag{50}\]

This term requires Monte Carlo sampling for the expectations over the latent variables \(\mathbf{z}\), while we can analytically compute the sum over all decisions \(\mathcal{P}_{l}\).

\[\begin{split}&\mathrm{KL}_{decisions}=\sum_{i\in\mathbb{V} \setminus\mathbb{L}}\int_{\mathbf{z}_{<i}}P(i;\mathbf{z},\mathbf{c})\\ &\quad\times\left[q(c_{i}=0)\mid\mathbf{x})\log\left(\frac{q(\mathbf{c}_{i }=0\mid\mathbf{x})}{p(\mathbf{c}_{i}=0\mid\mathbf{z}_{i})}\right)+q(\mathbf{c}_{i}=1\mid\mathbf{x} )\log\left(\frac{q(\mathbf{c}_{i}=1\mid\mathbf{x})}{p(\mathbf{c}_{i}=1\mid\mathbf{z}_{i})} \right)\right]\\ &\approx\frac{1}{M}\sum_{m=1}^{M}\sum_{i\in\mathbb{V}\setminus \mathbb{L}}P(i;\mathbf{z}^{(m)},\mathbf{c})\\ &\quad\times\left[q(c_{i}=0)\mid\mathbf{x})\log\left(\frac{q(\mathbf{c}_{ i}=0\mid\mathbf{x})}{p(\mathbf{c}_{i}=0\mid\mathbf{z}_{i}^{(m)})}\right)+q(\mathbf{c}_{i}=1\mid\mathbf{x} )\log\left(\frac{q(\mathbf{c}_{i}=1\mid\mathbf{x})}{p(\mathbf{c}_{i}=1\mid\mathbf{z}_{i}^{(m) })}\right)\right],\end{split} \tag{52}\]

where \(P(i;\mathbf{z}^{(m)},\mathbf{c})=\prod_{j\in\mathcal{P}_{\leq i}}q(c_{pa(j)\to j}\mid \mathbf{x})\) is the probability of reaching node \(i\), defined as \(P(i;\mathbf{c})\) in Eq. 16/19/20 for simplicity.

#### a.1.3 KL Nodes

Finally, we can analyze the last term of the KL term, which corresponds to the KL of the nodes (26). The reasoning is similar to the equations above and we will use the same notation. The KL of the nodes can be written as

\[\begin{split}\mathrm{KL}_{nodes}&=\sum_{l\in\mathbb{ L}}\int_{\mathbf{z}\mathcal{P}_{l}}q(\mathbf{z}_{0}\mid\mathbf{x})\prod_{i\in\mathcal{P}_{l} \setminus\{0\}}q(c_{pa(i)\to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)})\\ &\qquad\qquad\times\log\left(\frac{\prod_{j\in\mathcal{P}_{l} \setminus\{0\}}q(\mathbf{z}_{j}\mid\mathbf{z}_{pa(j)})}{\prod_{k\in\mathcal{P}_{l} \setminus\{0\}}p(\mathbf{z}_{k}\mid\mathbf{z}_{pa(k)})}\right)\\ &=\sum_{l\in\mathbb{L}}\int_{\mathbf{z}\mathcal{P}_{l}}\sum_{j\in \mathcal{P}_{l}\setminus\{0\}}q(\mathbf{z}_{0}\mid\mathbf{x})\prod_{i\in\mathcal{P}_{l }\setminus\{0\}}q(c_{pa(i)\to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)}) \log\left(\frac{q(\mathbf{z}_{j}\mid\mathbf{z}_{pa(j)})}{p(\mathbf{z}_{j}\mid\mathbf{z}_{pa(j) })}\right)\end{split} \tag{54}\]

We now change from a pathwise view to a nodewise view.

\[=\sum_{j\in\mathcal{V}\setminus\{0\}}\sum_{\mathcal{P}_{l\in j}} \int_{\mathbf{z}_{\leq j},\mathbf{z}_{>j}}\!\!\!q(\mathbf{z}_{0}\mid\mathbf{x})\!\!\!\prod_{i \in\mathcal{P}_{l}\setminus\{0\}}\!\!\!q(c_{pa(i)\to i}\mid\mathbf{x})q(\mathbf{z}_{i} \mid\mathbf{z}_{pa(i)})\log\left(\frac{q(\mathbf{z}_{j}\mid\mathbf{z}_{pa(j)})}{p(\mathbf{z}_{j }\mid\mathbf{z}_{pa(j)})}\right) \tag{56}\] \[=\sum_{j\in\mathcal{V}\setminus\{0\}}\int_{\mathbf{z}_{\leq j}}\!\!\! q(\mathbf{z}_{0}\mid\mathbf{x})\!\!\!\prod_{i\in\mathcal{P}_{\leq j} \setminus\{0\}}\!\!\!q(c_{pa(i)\to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)}) \log\left(\frac{q(\mathbf{z}_{j}\mid\mathbf{z}_{pa(j)})}{p(\mathbf{z}_{j}\mid\mathbf{z}_{pa(j) })}\right)\] (57) \[\qquad\times\sum_{\mathcal{P}_{>j}}\int_{\mathbf{z}_{>j}}\left[\prod_ {k\in\mathcal{P}_{>j}}q(c_{pa(k)\to k}\mid\mathbf{x})q(\mathbf{z}_{k}\mid\mathbf{z}_{pa(k) })\right]\] \[=\sum_{j\in\mathcal{V}\setminus\{0\}}\int_{\mathbf{z}_{<j}}\int_{\mathbf{z }_{j}}\!\!\!\!q(\mathbf{z}_{0}\mid\mathbf{x})\!\!\!\prod_{i\in\mathcal{P}_{\leq j} \setminus\{0\}}\!\!\!q(c_{pa(i)\to i}\mid\mathbf{x})q(\mathbf{z}_{i}\mid\mathbf{z}_{pa(i)} )\log\left(\frac{q(\mathbf{z}_{j}\mid\mathbf{z}_{pa(j)})}{p(\mathbf{z}_{j}\mid\mathbf{z}_{pa(j )})}\right)\] (58) \[=\sum_{j\in\mathcal{V}\setminus\{0\}}\int_{\mathbf{z}_{<j}}\int_{\mathbf{z }_{j}}\!\!\!P(pa(j);\mathbf{z},\mathbf{c})q(c_{pa(j)\to j}\mid\mathbf{x})q(\mathbf{z}_{j}\mid \mathbf{z}_{pa(j)})\log\left(\frac{q(\mathbf{z}_{j}\mid\mathbf{z}_{pa(j)})}{p(\mathbf{z}_{j} \mid\mathbf{z}_{pa(j)})}\right)\] (59) \[=\sum_{j\in\mathcal{V}\setminus\{0\}}\int_{\mathbf{z}_{<j}}\!\!\!P(pa( j);\mathbf{z},\mathbf{c})q(c_{pa(j)\to j}\mid\mathbf{x})\int_{\mathbf{z}_{j}}q(\mathbf{z}_{j}\mid\mathbf{z}_{pa(j )})\log\left(\frac{q(\mathbf{z}_{j}\mid\mathbf{z}_{pa(j)})}{p(\mathbf{z}_{j}\mid\mathbf{z}_{pa( j)})}\right)\] (60) \[=\sum_{j\in\mathcal{V}\setminus\{0\}}\int_{\mathbf{z}_{<j}}\!\!\!P(pa( j);\mathbf{z},\mathbf{c})q(c_{pa(j)\to j}\mid\mathbf{x})\,\mathrm{KL}\left(q(\mathbf{z}_{j}\mid\mathbf{z}_{pa(j )})\mid p(\mathbf{z}_{j}\mid\mathbf{z}_{pa(j)})\right)\] (61) \[\approx\frac{1}{M}\sum_{m=1}^{M}\sum_{i\in\mathcal{V}\setminus\{0\} }P(pa(i);\mathbf{z}^{(m)},\mathbf{c})q(c_{pa(i)\to i}\mid\mathbf{x})\] (62) \[\qquad\times\mathrm{KL}(q(\mathbf{z}_{i}^{(m)}\mid pa(\mathbf{z}_{i}^{(m)} ))\|p(\mathbf{z}_{i}^{(m)}\mid pa(\mathbf{z}_{i}^{(m)})))\] \[=\frac{1}{M}\sum_{m=1}^{M}\sum_{i\in\mathcal{V}\setminus\{0\}}P(i; \mathbf{z}^{(m)},\mathbf{c})\,\mathrm{KL}(q(\mathbf{z}_{i}^{(m)}\mid pa(\mathbf{z}_{i}^{(m)})) \|p(\mathbf{z}_{i}^{(m)}\mid pa(\mathbf{z}_{i}^{(m)}))), \tag{63}\]

where \(P(pa(j);\mathbf{z},\mathbf{c})\) is defined in Eq. 49 and where \(P(i;\mathbf{z}^{(m)},\mathbf{c})=P(i;\mathbf{c})=\prod_{j\in\mathcal{P}_{\leq i}}q(c_{pa(j) \to j}\mid\mathbf{x})\) is the probability of reaching node \(i\).

#### a.1.4 KL terms

Using the above factorization, the KL term of the ELBO can be written as

\[\begin{split}\operatorname{KL}\left(q\left(\boldsymbol{z},\mathcal{P}_ {l}\mid\boldsymbol{x}\right)\|p\left(\boldsymbol{z},\mathcal{P}_{l}\right) \right)\approx\operatorname{KL}\left(q(\boldsymbol{z}_{0}\mid\boldsymbol{x}) \|p(\boldsymbol{z}_{0})\right)\\ +\frac{1}{M}\sum_{m=1}^{M}\sum_{i\in\operatorname{V}\setminus \operatorname{L}}P(i;\boldsymbol{z}^{(m)},\boldsymbol{c})\sum_{c_{i}\in\{0,1\}} q(c_{i}\mid\boldsymbol{x})\log\left(\frac{q(c_{i}\mid\boldsymbol{x})}{p(c_{i}\mid \boldsymbol{z}_{i}^{(m)}))}\right)\\ +\frac{1}{M}\sum_{m=1}^{M}\sum_{i\in\operatorname{V}\setminus\{0 \}}P(i;\boldsymbol{z}^{(m)},\boldsymbol{c})\operatorname{KL}(q(\boldsymbol{z} _{i}^{(m)}\mid pa(\boldsymbol{z}_{i}^{(m)}))\|p(\boldsymbol{z}_{i}^{(m)}\mid pa (\boldsymbol{z}_{i}^{(m)}))),\end{split} \tag{64}\]

where \(P(i;\boldsymbol{z}^{(m)},\boldsymbol{c})=P(i;\boldsymbol{c})=\prod_{j\in \mathcal{P}_{\leq i}}q(c_{pa(j)\to j}\mid\boldsymbol{x})\).

#### a.1.5 Reconstruction Loss

Finally, to compute the full ELBO, the KL terms are added to the reconstruction loss defined in (16). Here, assumptions about the distribution of the inputs are required. For the grayscale datasets such as MNIST, Fashion-MNIST, and Omniglot, as well as the one-hot-encoded 20Newsgroup, we assume that the inputs are Bernoulli distributed, such that the resulting reconstruction loss is the binary cross entropy. On the other hand, for the colored datasets CIFAR-10, CIFAR-100, and CelebA, we assume that their pixel values are normally distributed, which leads to the mean squared error as loss function, where we assume that \(\sigma=1\).

### Computational Complexity

All terms of the Evidence Lower Bound, Equation 12, can be computed efficiently and the computational complexity of a single joint update of the parameters is \(O(MBVDC_{p})\), where \(M\) is the number of MC samples, \(B\) is the batch size, \(V\) is the number of nodes in the tree, \(D\) is the maximum depth of the tree, and \(C_{p}\) is the cost to compute the KL between Gaussians. It should be noted that the computational complexity is, in practice, reduced to \(O(LBVC_{p})\), as the term \(P(i;\boldsymbol{c})\) can be computed dynamically from parent nodes.

## Appendix B Related Work

In addition to the review presented in Section 3, which encompasses a broad range of related work in the field of deep latent variable models, we provide a review of relevant work in the domains of hierarchical clustering and decision trees. By doing so, we hope to shed further light on the current state-of-the-art approaches and contribute to a deeper understanding of the challenges and opportunities that lie in the intersection of hierarchical clustering, decision trees, and latent variable models.

### Hierarchical Clustering

Hierarchical clustering algorithms have long been employed in the field of data mining and machine learning to extract hierarchical structures from data (Sneath, 1957; Ward, 1963; Murtagh & Contreras, 2012). Agglomerative clustering is among the earliest and most well-known hierarchical clustering algorithms. These methods start with each data point as an individual cluster and then iteratively merge the closest pairs of clusters, according to a predefined distance metric, until a stopping criterion is met. While single-linkage and complete-linkage agglomeration clustering are widely used as baselines, we observe better performance when using the bottom-up strategy proposed by Ward (1963). Ward's minimum variance criterion minimizes the total within-cluster variance (Murtagh & Legendre, 2014), thus providing balanced and compact clusters. In contrast, the Bayesian Hierarchical Clustering (BHC) proposed by Heller & Ghahramani (2005) takes a different approach by employing hypothesis testing to determine when to merge the clusters. The divisive clustering algorithms, on the other hand, provide a different strategy to hierarchical clustering. Unlike agglomerative methods, divisive clustering starts with all data points in a single cluster and recursively splits clusters into smaller ones. The proposed TreeVAE is an example of a divisive clustering method. Among a variety of proposed methods, the Bisecting-K-means algorithm (Steinbach et al., 2000; Nister & Stewenius,2006) is widely used for its simplicity; it applies k-means with two clusters recursively. Similarly, Williams (1999) learn a hierarchical probabilistic Gaussian mixture model. Neal (2003) introduce a hierarchical generalization of Dirichlet process mixture models.

More recent approaches include PERCH (Kobren et al., 2017), which is a non-greedy, incremental algorithm that scales to both the number of data points and the number of clusters, GHC (Monath et al., 2019), which leverages continuous representations of trees in a hyperbolic space and optimizes a differentiable cost function, and Mathieu et al. (2019), which similarly use a Poincare ball model of hyperbolic geometry as a latent space to learn continuous hierarchical representations with a VAE.Furthermore, RSSCOMP (You et al., 2015), which explores a subspace clustering method based on an orthogonal matching pursuit. Finally, Deep ECT (Mautz et al., 2020) proposes a divisive hierarchical embedded clustering method, which jointly optimizes an autoencoder that compresses the data into an embedded space and a hierarchical clustering layer on top of it. Density-based clustering algorithms, such as DBSCAN, belong to a distinct category of clustering techniques. They aim to identify regions in a dataset where points are densely concentrated and classify outliers as noise (Ester et al., 1996). R. J. Campello et al. (2015) build on this idea to learn a hierarchy based on the distances between datapoints where distance is roughly determined by the density. However, one limitation of density-based methods lie in their performance when confronted with complex datasets requiring high-dimensional representations. In such cases, estimating density requires an exponentially growing number of data points, which leads to scalability issues that TreeVAE does not have.

### Decision Trees

Decision trees (Breiman et al., 1984) are interpretable, non-parametric supervised learning techniques commonly employed in classification and regression tasks. They rely on the data itself to build a hierarchical structure. This is done by recursively partitioning the data into subsets, each of which corresponds to a specific node in the tree. At each node, a decision rule is generated based on one of the input features that best discriminates the data in that subset. One of the key advantages of decision trees is their interpretability. The learned tree structure can be easily visualized to get insights into the data and the model's decision-making process. Suarez & Lutsko (1999) argue that deterministic splits lead to overfitting and introduce fuzzy decision trees with probabilistic decisions, implicitly allowing for backpropagation. With the advancement of neural networks, many works (Rota Bulo & Kontschieder, 2014; Laptev & Buhmann, 2014; Frosst & Hinton, 2017) leverage MLPs or CNNs for learning a more complex decision rule. However, the input at every node remains the original features, which limits their performance, as they are unable to learn meaningful representations. Thus, Tanno et al. (2019) introduces Adaptive Neural Trees (ANT), a method that learns flexible, hierarchical representations through NNs, hereby facilitating hierarchical separation of task-relevant features. Additionally, ANTs architectures grow dynamically such that they can adapt to the complexity of the training dataset. At inference time, ANTs allow for lightweight conditional computation via the most likely path, leading to inbuilt interpretable decision-making.

While decision trees were initially designed with the goal of achieving high predictive performance, they are also used for many auxiliary goals such as semi-supervised learning (Zharmagambetov & Carreira-Perpinan, 2022), robustness (Moshkovitz et al., 2021) or interpretability (Wan et al., 2021; Souza et al., 2022; Arenas et al., 2022; Pace et al., 2022). In the context of interpretability, various approaches have been proposed to enhance accuracy and interpretability. Wan et al. (2021) introduce Neural-Backed Decision Trees (NBDTs), which improve accuracy by replacing the final layer of a neural network with a differentiable sequence of decisions to increase its interpretability while retaining predictive performance. Souza et al. (2022) focus on optimizing the structural parameters of decision trees, introducing the concept of "explanation size" as being the expected number of attributes required for prediction to measure interpretability. Pace et al. (2022) develop the POETREE framework for interpretable policy learning via decision trees in time-varying clinical decision environments. Arenas et al. (2022) investigate explanations in decision trees, considering both deterministic and probabilistic approaches and showing the limitations thereof. These works collectively contribute to advancing the use of decision trees in interpretable machine learning, providing insights into trade-offs, criteria, and frameworks for improving accuracy and interpretability.

[MISSING_PAGE_FAIL:23]

paths of the tree regardless of their probabilities. As a result, we generate \(L\) images, where \(L\) is the number of leaves, corresponding to the number of decoders. These generated samples exhibit distinct characteristics based on their respective cluster-specific features while maintaining cluster-independent properties across all generated instances.

Figure 11: Hierarchical structure learned by TreeVAE with \(20\) leaves on Fashion-MNIST dataset with generated images.

Figure 12: Hierarchical structure learned by TreeVAE on the full Omniglot dataset. We display random subsets of images that are probabilistically assigned to each leaf of the tree. Similar to the results in the main text, we can again find regional hierarchies in some individual subtrees. For instance, the leftmost subtree seems to find structures in different Indian alphabets. We can also observe that this subtree seems to cluster smaller characters in its left child, whereas the right child contains bigger shapes. Another example is the rightmost tree, which seems to encode the more straight shapes of Japanese writing styles. There, the left child encodes more complex shapes that contain many different strokes, and the right subtree groups simpler shapes with only a few lines.

Figure 13: Hierarchical structure learned by TreeVAE with \(20\) leaves on CIFAR-100 dataset. We display random subsets of images that are probabilistically assigned to each leaf of the tree.

### Ablation Study

In this section, we provide additional studies on the effect of selected hyperparameters and assumptions on the behavior of TreeVAE. In Table 4, we provide an ablation on selected hyperparameters of TreeVAE on Fashion MNIST. In the following subsection we will provide further in-depth explorations of TreeVAE.

Figure 14: Selected unconditional generations of MNIST. One row corresponds to one sample from the root, for which we depict the visualizations obtained from the \(10\) leaf-decoders. Each row retains similar properties across the different leaves. In the first row, all digits are rotated; the second are bold; the third are straight; and the last are squeezed.

Figure 15: Selected unconditional generations of CelebA. One row corresponds to one sample from the root, for which we depict the visualizations obtained from the \(8\) leaf-decoders. The overall face shape, skin color, and face orientation are retained among leaves from the same row, while several properties (such as beard, mustache, hair) vary across the different leaves.

[MISSING_PAGE_FAIL:26]

random weight initialization on the learned dendrogram, we have made further efforts to find a dendrogram that best summarizes the learned dendrogram of multiple runs. We show it in Fig. 17. For this, we first align the leaves of the different trees by maximizing the overlapping samples, then we store the number of edges between any two clusters for every tree and then average this number over all trees. Thus, we have computed a distance matrix of all clusters, which is averaged over all trees. We then used average and complete linkage to cluster according to the average distance matrix. The recovered dendrograms of the two algorithms are identical, apart from the Bag and the Trouser cluster, which switch places. That is, all tops are in one subtree and all shoes in the other, while bags and trousers are assigned to either the shoes or tops subtree, depending on the clustering algorithm used. What this indicates is that the groups of shoes and tops are consistently recovered, no matter the random initialization, while the assignment of bags and trousers varies, which is aligned with the interpretation of the root embedding in Fig. 16, as there is no clear assignment thereof.

#### c.3.3 Contrastive Losses

In this section, we present an ablation on the effect of the contrastive losses that are applied in real world datasets. The experiments are performed on a slightly modified version of CIFAR-10 and shown in Table 6. As can be seen, the experiments showed that the proposed combination of (A) and (B) leads to the best results as the two methods complement each other.

#### c.3.4 Unknown number of clusters

In an unsupervised setting, it is unclear a priori what the optimal number of clusters is. Thus, we are interested in the sensitivity of TreeVAE with respect to misspecifications of the ground truth number of clusters. The CIFAR-100 dataset is an optimal candidate for this ablation, as the ground truth number of clusters can be defined differently depending on whether we look at coarse or fine-grained classes. As a metric, we will only use NMI, because DP, LP, as well as Accuracy values are not fairly comparable across different number of predicted clusters. To determine the sensitivity of TreeVAE with respect to the misspecification of number of clusters, we evaluate its performance with \(10\), \(20\), and \(32\) predicted classes, compared to the ground truth \(20\) superclasses. The results are depicted in Table 8. Clearly, TreeVAE is not very sensitive to the correct specification of the number of clusters. This is also supported by Fig. 10/11, which show that even as we grow the tree further than the ground truth number of classes, TreeVAE still finds meaningful subsets of inside the classes.

\begin{table}
\begin{tabular}{l l c c c c c} \hline \hline Dataset & Method & ACC & NMI & LL & RL & ELBO \\ \hline \hline \multirow{2}{*}{Fashion} & fixed TreeVAE & \(35.1\pm 4.8\) & \(54.1\pm 5.8\) & \(-237.4\pm 0.4\) & \(228.9\pm 0.5\) & \(-241.0\pm 0.5\) \\  & TreeVAE & \(63.6\pm 3.3\) & \(64.7\pm 1.4\) & \(-234.7\pm 0.1\) & \(226.5\pm 0.3\) & \(-239.2\pm 0.4\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Test set performances (\(\%\)) of TreeVAE with a fixed structure standard TreeVAE. Means and standard deviations are computed across \(10\) runs with different random model initializations.

Figure 16: UMAP visualization of root embedding for Fashion MNIST.

Figure 17: ‚ÄúAverage‚Äù dendrogram learnt on Fashion MNIST by averaging cluster distances across \(10\) runs and applying average linkage clustering.

As a next ablation, we are interested in the case of the number of ground truth clusters being misspecified or unclear. Note that the difference to the previous ablation is whether the ground truth or the predicted number of classes is misspecified. Again, CIFAR-100 is a natural choice for this ablation, as the ground truth number of classes can either be \(20\) or \(100\). Thus, in Table 9, we present how the performance of TreeVAE would change if we chose a different granularity of ground truth classes while keeping the number of predicted clusters fixed to \(20\). We can observe that the NMI for finer granularity is even higher than for the superclasses used, indicating that TreeVAE learned to differentiate classes of the same superclass.

## Appendix D Datasets

This section provides supplementary background information and preprocessing steps of the different datasets used in our experiments.

### Mnist

The MNIST (LeCun et al., 1998) dataset is a widely used benchmark dataset in machine learning. It is composed of grayscale images representing handwritten digits from zero to nine. Each image is of size \(28\times 28\) pixels that we rescale to \([0,1]\). The dataset is balanced, meaning that there is an equal number of examples for each digit class, with a total of \(60^{\prime}000\) training images and \(10^{\prime}000\) test images. The digits in the dataset are manually labeled, providing ground truth information for clustering evaluation.

### Fashion-MNIST

The Fashion-MNIST (H. Xiao et al., 2017) dataset consists of a collection of grayscale images depicting various clothing items and accessories. The dataset contains a balanced distribution of ten different classes, representing different fashion categories such as T-shirts, trousers, dresses, footwear, and more. Each image in the dataset has a resolution of \(28\times 28\) pixels that we again rescale to \([0,1]\). Similar to the original MNIST dataset, Fashion MNIST also includes \(60,000\) training images and \(10^{\prime}000\) test images.

\begin{table}
\begin{tabular}{l l c} \hline \hline Dataset & Contrastive Method & NMI \\ \hline CIFAR-10* & Ours (\(\equiv\)A+B) & \(44.1\pm 0.7\) \\  & (A) & \(21.7\pm 4.0\) \\  & (B) & \(6.0\pm 6.0\) \\  & (C) & \(23.0\pm 1.2\) \\  & (D) & \(0.2\pm 0.0\) \\  & (E) & \(1.0\pm 1.3\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Ablation study of contrastive losses. (A) corresponds to the NT-Xent regularization on the routers, (B) to the NT-Xent regularization on the bottom-up embeddings, (C) on a transposed version of the NT-Xent regularization on the routers, following Li et al. (2021), (D) to the NT-Xent regularization of only the output of the encoder, and (E) to minimizing the Jensen-Shannon divergence between the router probabilities of the two augmented inputs.

\begin{table}
\begin{tabular}{l l c} \hline \hline Dataset & \# Predicted clusters & NMI \\ \hline CIFAR-100* & 10 & \(15.99\pm 0.85\) \\  & 20 & \(17.80\pm 0.42\) \\  & 32 & \(19.33\pm 0.41\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Test set hierarchical clustering performances (\(\%\)) of TreeVAE for different number of predicted clusters. Means and standard deviations are computed across \(10\) runs with different random model initializations. The star ‚Äù*‚Äù indicates that contrastive learning was applied.

### 20Newsgroups

The 20Newsgroups (Lang, 1995) dataset is a widely-used benchmark dataset in the field of natural language processing and text classification, known for its inherent hierarchical structure. It consists of approximately \(20^{\prime}000\) newsgroup documents organized into \(20\) different topics or categories. These categories include sports, politics, technology, science, religion, and more. One of the most important characteristics of this dataset is the presence of hierarchical relationships within the topics. We employ a TF-IDF vectorizer for the \(2^{\prime}000\) most frequent words and use this text embedding as input to our model. We retain \(60\%\) of the original dataset for training and separate the other \(40\%\) for the test set.

### Omniglot

The Omniglot Lake et al. (2015) dataset contains handwritten characters from \(50\) different alphabets. Every alphabet contains various individual characters, amounting to a total of \(1623\) different characters in the dataset. Given the heterogeneous nature of the dataset, Omniglot contains relatively few samples. The dataset consists of \(32^{\prime}460\) samples, which is about \(650\) samples per alphabet or \(20\) samples per character. In our experiments, we are interested in whether TreeVAE can find alphabet-specific patterns that stay invariant across different characters. Given that many alphabets developed either from or parallel to each other, many alphabets are very hard to distinguish, both for humans and machines. While we use the full Omniglot dataset for our generative experiments to have more data, we only use a subset of the alphabets for our clustering experiments. We call this subset Omniglot-\(5\) and it consists of the five alphabets Braille, Glagolitic, Cyrillic, Odia, and Bengali. We selected them by ensuring that we have languages from different origins (artificial, Slavic, and Indian) and that there are learnable hierarchies. Old Church Slavonic (Cyrillic) developed from Glagolitic and Odia and Bengali are languages with their own alphabet from geographically similar regions in India. The resulting Omniglot-\(5\) results in \(4^{\prime}160\) samples. For both dataset versions and all experiments, we split the dataset into train/test splits with \(80\%\)/\(20\%\) of the samples, respectively, and stratifying across the characters of the dataset. We resized the images to \(28\times 28\) to align with images in MNIST and Fashion-MNIST, transformed the images to grayscale, and rescaled pixel values to \([0,1]\). For a description of the augmentations we use, we refer to Appendix E.1.2.

### Cifar-10

The CIFAR-10 (Krizhevsky & Hinton, 2009) dataset contains colored images of ten balanced classes. These classes are \(\{Airplane,Automobile,Bird,Cat,Deer,Dog,Frog,Horse,Ship,Truck\}\). Therefore, some classes are more similar (for example Automobile and Truck) than others (for example Frog and Airplane), indicating that there is a structure across clusters that can be captured. The dataset consists of \(50^{\prime}000\) training and \(10^{\prime}000\) test images of resolution \(32\times 32\) pixels and \(3\) color channels, which we rescaled to \([0,1]\).

### Cifar-100

The CIFAR-100 (Krizhevsky & Hinton, 2009) dataset originally contains colored images of \(100\) classes. In order to not have to grow until TreeVAE has \(100\) clusters, we apply the common grouping of the 100 classes into \(20\) balanced superclasses. In Table 9, we also present an ablation if instead we chose the full \(100\) clusters. The dataset consist of \(50^{\prime}000\) training and \(10^{\prime}000\) test images of resolution \(32\times 32\) pixels and \(3\) color channels, which we rescaled to \([0,1]\).

\begin{table}
\begin{tabular}{l c c} \hline \hline Dataset & \# Ground truth clusters & NMI \\ \hline CIFAR-100* & 20 & \(17.80\pm 0.42\) \\  & 100 & \(22.74\pm 0.36\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Test set hierarchical clustering performances (\(\%\)) of TreeVAE for different number of ground truth clusters. Means and standard deviations are computed across \(10\) runs with different random model initializations. The star ‚Äú*‚Äù indicates that contrastive learning was applied.

### CelebA

The CelebA (Z. Liu et al., 2015) dataset contains images of celebrities with \(40\) attribute annotations per image. This dataset is well-suited for exploratory analysis, as there are no agreed-upon ground truth clusters. As a result, we can visualize the clusters that are learned by TreeVAE and validate whether they align with our intuition as well as with the given attributes. Thus, in Table 3, we additionally define "Hair Loss" as balding or having a receding hairline, and "Beard" as having a beard or the so-called 5 o'clock shadow. For training, we select a subset of \(100^{\prime}000\) random images from the training set and evaluate on the given test set of \(19^{\prime}962\) images. Lastly, we crop the images to \(64\times 64\) pixels with \(3\) color channels and rescale them to \([0,1]\).

## Appendix E Implementation Details

The following section provides a comprehensive overview of the various aspects and components regarding the practical implementation of our proposed framework. We provide a detailed description of our proposed model's training process and architectural design. Furthermore, we outline the specific techniques and methodologies used for the contrastive learning extension. We also provide the implementation details of the Variational Autoencoder and the Ladder Variational Autoencoder that are used as baselines. Finally, we present the computational resources required for executing the proposed framework. Together, these subsections aim to provide a comprehensive view of the implementation details, enabling readers to replicate and build upon our work effectively.

### Training Details

Hierarchical variational models, such as LadderVAE, are prone to local minima and posterior collapse, and TreeVAE shows similar behavior. Therefore, few technical choices, such as batch normalization and KL annealing, are needed to converge to a desirable optimum. For all datasets, we train TreeVAE only on the training set and evaluate the trained model on the separate test set.

#### e.1.1 Training the Tree

For MNIST, Fashion-MNIST, Omniglot, and 20Newsgroup, the trees are trained for \(M=150\) epochs at each growth step, and the final tree is finetuned for \(M_{f}=200\) epochs. To reduce the risk of posterior collapse during training, we anneal the KL terms of the ELBO. Starting from 0, we increase the weight of the KL terms every epoch by \(0.001\) except for the final finetuning of the full tree. Here, we linearly increase the weight by \(0.01\) until we reach 1, such that the final model is trained on the complete ELBO. Additionally, for the aforementioned datasets, we finetune the full tree after every third growing step for \(80\) epochs with KL annealing of \(0.001\), so the otherwise frozen parameters can adapt to the new structure. For CIFAR-10, CIFAR-100, and CelebA, the splits are mostly determined by the contrastive losses, and therefore, we avoid finetuning the full tree to reduce the computational time. The KL term is annealed every epoch by \(0.01\) such that the weight reaches \(1\) after \(100\) of the \(150\) epochs every growth step. While the annealing procedure is required to achieve good performances, the exact annealing decay plays a smaller role in the final performances, as well as the in-between finetuning of the trees, and the number of epochs at each training step.

#### e.1.2 Augmentations

Due to the scarcity and heterogeneity of Omniglot, we trained our models using data augmentation by randomly rotating images by up to \(\pm 10\) degrees, horizontally/vertically shifting images by up to \(\pm 2\) pixels, applying up to \(1\%\) shearing, and zooming by up to \(\pm 10\%\). Note that for Omniglot, no contrastive learning was applied; the augmentations were solely used for overcoming the scarcity of data points.

On the other hand, in CIFAR-10, CIFAR-100, and CelebA, augmentations were applied to use contrastive learning as introduced in Section 2.6 and Appendix E.3. For CIFAR-10 and CIFAR-100, first, each image is randomly cropped and resized with scale \(\in(0.2,1)\) and aspect ratio \(\in(3/4,4/3)\). Second, each image is randomly flipped in the horizontal direction with probability \(0.5\). Third, with probability \(0.8\), color changes are performed, which consist of changes in brightness, contrast, and saturation, all with their respective factors \(\in(0.6,1.4)\), as well as change in hue with its parameter\(\in(-0.1,0.1)\). Lastly, with probability \(0.2\), we transform the image to grayscale. The chosen parameters are largely adopted from Li et al.'s (2022) weak augmentation scheme.

While these augmentations lead to superior group separation, in CelebA, we are also interested in generative results. Therefore, to obtain generations without non-sensical artifacts, we reduce the augmentation strength by removing the hue and grayscale augmentations, reducing the other color parameters by a factor \(2\), and reducing the cropping parameters to scale \(\in(3/4,1)\) and aspect ratio \(\in(4/5,5/4)\).

### Model Architecture

In general, model architectures for all datasets are similar and consist of an encoder, followed by the tree structure with the transformations, routers, and the bottom-up, as depicted in Figures 2 and 3, and identical decoders for each leaf of the tree. We always apply batch normalization followed by Leaky ReLU non-linearities after all convolutional and dense layers. For further details on the specific implementation of the sections below, we direct the reader to consult the accompanying code.

#### e.2.1 Transformations/Bottom-Up

The architecture of the transformations and the bottom-up is identical across all experiments. They take as input the previous latent vector of dimension \(4\) (20Newsgroup), \(8\) (MNIST, Fashion-MNIST, Omniglot), or \(64\) (CIFAR-10, CIFAR-100, CelebA) and map them to some intermediate representation of dimension \(128\) (20Newsgroup, MNIST, Fashion-MNIST, Omniglot), or \(512\) (CIFAR-10, CIFAR-100, CelebA) using a dense layer, followed by a dense layer without activation to compute \(\mu\) and one with Softplus activation to compute \(\sigma\) of the approximate posterior/conditional prior of the respective tree level.

#### e.2.2 Routers

Similar to transformations, the architectures of routers are identical across all experiments. The generative routers take as input \(\mathbf{z}_{i}\), the latent representation of a node \(i\), while the inference routers use \(\mathbf{d}_{depth(i)}\). Then, both sequentially apply two dense layers that both map them to \(128\) (20Newsgroup, MNIST, Fashion-MNIST, Omniglot), or \(512\) (CIFAR-10, CIFAR-100, CelebA) dimensions, after which a final dense layer with a Sigmoid activation function computes the probability of the binary decision that infers the next child node.

#### e.2.3 MNIST/Fashion-MNIST

For MNIST and Fashion-MNIST we use identical encoder/decoder architectures. Both datasets have inputs of dimension \(28\times 28\) and \(8\) dimensional latent spaces. We use a small encoder as depicted in Figure 18, and a symmetric decoder. The encoder uses \(3\times 3\) convolutions with stride \(2\), whereas the decoder uses transposed convolutions.

Figure 18: MNIST and Fashion-MNIST encoder architecture.

#### e.2.4 20Newsgroups

20Newsgroups is a text dataset, so the encoder/decoder architecture differs from the other experiments. The samples of 20Newsgroups are preprocessed in such a way, that the input becomes a \(2000\)D vector (see Section D.3). Hence, the encoder for 20Newsgroups is a simple MLP with \(5\) dense layers, as depicted in Figure 19, and the decoder again mirrors the encoder architecture.

#### e.2.5 Omniglot

For Omniglot, we use an architecture similar to the one for MNIST and Fashion-MNIST. In contrast to Section E.2.3, we increase the receptive field of the encoder by using \(4\times 4\) convolutions and make it \(4\) times wider and \(2\) times deeper. We refer to Figure 20 for a schematic of the encoder. As in the other architectures, the decoder is symmetric to the encoder.

#### e.2.6 Cifar-10/Cifar-100/CelebA

For real-world image datasets, more complex architectures are required. We first define a ResnetBlock as two consecutive convolutions with kernel size \(3\) and stride \(1\), whose output gets weighted by \(0.1\) and combined with a residual connection to the input of the first convolution. In case input and output dimensions do not match, the residual connection consists of a convolution with kernel size \(1\) and stride \(1\) to match dimensionality. The encoder consists of consecutive ResnetBlocks with 2D average-pooling of kernel size \(3\) and stride \(2\). Here, number of filters per ResnetBlock are increasing by a factor of \(2\), starting from \(32\) and going up to \(256\) until the representation is \(4\times 4\times 256\), at which point we flatten it to get \(\mathbf{d}_{L}\). We refer to Figure 21 for a schematic of the encoder. The decoder structure is similar to the reversed encoder, where the pooling layers are replaced by 2D upsampling with bilinear interpolation and before the ResnetBlocks, we pass the input through a dense layer and consecutive reshaping in order to match the expected input dimensions. Slight differences are required for CelebA due to the higher resolution. To arrive at the same internal representation, we add one more ResnetBlock with 256 filters to the encoder, as well as one more ResnetBlock at the start of the decoder, starting from \(16\) instead of \(32\) filters.

Figure 19: 20Newsgroups encoder architecture.

Figure 20: Omniglot encoder architecture.

### Contrastive Learning

In real-world datasets, data splits, as determined by the ELBO and especially by the reconstruction loss, might not coincide with the human-aligned notion of clusters. For example, we observed that plain TreeVAE utilizes specialized decoders to focus on good reconstructions of the various background colors. These data splits are rather intuitive from a reconstruction point of view, however, these might not encode contextually meaningful splits. Thus, we introduce an additional contrastive loss to guide TreeVAE toward meaningful clusters. The idea of contrastive learning (Sohn, 2016; van den Oord et al., 2018; Wu et al., 2018; Chen et al., 2020) is simple; constrain the space of potential clustering rules by defining certain data augmentations with respect to which the model's cluster assignment should become invariant. The model should assign a sample to the same cluster independent of whether and which subset of the chosen augmentations are applied to it. For the augmentations that were used for contrastive learning, we refer to Appendix E.1.2.

To achieve the desired behavior, we utilize the normalized temperature-scaled cross-entropy loss (NT-Xent) (Sohn, 2016; van den Oord et al., 2018; Wu et al., 2018; Chen et al., 2020), whose definition we repeat here for ease of reading: \(\ell_{i,j}=-\log\frac{\exp\left(s_{i,j}/\tau\right)}{\sum_{k=1}^{2\pi}1_{1|k \neq i|}\exp\left(s_{i,k}/\tau\right)}\), where \(s_{i,j}\) denotes a similarity measure, which we define as the cosine similarity between the representations of \(\tilde{\mathbf{x}}_{i}\) and \(\tilde{\mathbf{x}}_{j}\), and \(\tau\) is a temperature parameter. This is computed for all \(2N\) pairs of augmented samples \(\tilde{\mathbf{x}}_{i}\) and \(\tilde{\mathbf{x}}_{j}\) originating from the same initial sample \(\mathbf{x}\) and then averaged. We compute this depth-wise for every embedding \(\mathbf{d}_{l}\) by first passing it through a separate MLP with one hidden layer of dimension \(512\), followed by batch normalization and LeakyReLU and the final linear layer with output dimension \(64\), on which the NT-Xent with \(\tau=0.5\) is applied. Then, we average the pairwise losses and sum them up over all \(\mathbf{d}_{l}\), where we, to be invariant to the depth of the bottom-up, divide by the maximum depth. With this, we regularize the bottom-up embeddings learned by the model to encode every augmented pair of samples close to each other, thus constraining the learning of information that the model should be invariant to.

Even with the previous regularization, \(\mathbf{d}_{l}\) might still contain some of information that is not relevant for clustering (such as background color), and therefore the routers might base their split on such characteristics. Therefore, we build upon ideas from Li et al. (2021, 2022), which propose not only to regularize the embeddings, but also the cluster probabilities themselves. While Li et al. (2021, 2022) change from a sample-wise view to a cluster-wise view for calculating the NT-Xent on the \(2K\) clusters, we instead keep the sample-wise view and apply the NT-Xent with \(\tau=1\) directly on the

Figure 21: CIFAR-10, CIFAR-100, and CelebA encoder architecture. The initial convolutional layer comprises a ResnetBlock, while the remaining convolutional layers additionally incorporate average pooling prior to the ResnetBlock.

outputs of every bottom-up router \(q(\mathbf{c}_{i}\mid\mathbf{d}_{l})\). We would like to emphasize here that we do not project them to another space, because, in contrast to previous methods (Chen et al., 2020), we wish to remove all irrelevant information with respect to the augmentations from the routers. The computed pairwise loss is then weighted by the probability of falling into the given node. Lastly, we sum up the weighted pairwise losses and divide them by the sum of the weights to ensure equal regularization in every node. This has three advantages: (\(i\)) We can use the proposed loss during the training of the subtrees, thus gaining computational efficiency. (\(ii\)) We implicitly encourage the splitting of the data into balanced classes, following the overall design choice for splitting the data, outlined in Section 2.5. (\(iii\)) We can omit the computationally intensive finetuning of the full tree, as the splits, as enforced by the contrastive losses, are local, which alleviates the need of finetuning the full tree for global optimization.

Lastly, we add these two contrastive parts together and weigh them by \(100\) to match the gradients of the ELBO. Thus, we naturally introduce a tradeoff between reconstruction quality and clustering quality, as we constrain the model to a clustering solution that does not optimally make use of specialized decoders.

### Baselines

For a fair comparison, we retain the same architectural choices made for TreeVAE for both the Variational Autoencoder and Ladder Variational Autoencoder, used as baselines. In particular, we retain the same encoder and decoder structure as explained in Section E.2. For datasets on which TreeVAE applies contrastive learning, we similarly apply the regularization on the bottom-up embeddings of the baselines. Below we describe specific training details that deviate from TreeVAE, due to the differences in the graphical models.

#### e.4.1 Variational Autoencoder

The non-hierarchical counterpart of TreeVAE is a classical Variational Autoencoder (VAE). As it does not contain a hierarchical structure, the VAE consists solely of an encoder and decoder architecture, which follows the architectural choices of TreeVAE. We train the VAE for \(500\) epochs and use the KL annealing procedure proposed by LadderVAE, which linearly increases the weight of the KL terms from \(0\) to \(1\) in \(200\) epochs.

#### e.4.2 Ladder Variational Autoencoder

The sequential counterpart of TreeVAE is the LadderVAE (Sonderby et al., 2016). We retain the same encoder, decoder, and transformations used by TreeVAE, and we set the depth to \(5\) for all datasets. We train the LadderVAE for \(1000\) epochs and use the KL annealing procedure proposed by the authors, which linearly increases the weight of the KL terms from \(0\) to \(1\) in \(200\) epochs.

#### e.4.3 Agglomerative Clustering

We train Ward's minimum variance agglomerative clustering (Ward, 1963; Murtagh & Legendre, 2014) on the plain input space (Agg), the latent space of the VAE (VAE + Agg) and the last layer of LadderVAE (LadderVAE + Agg). We observed that clustering on the last layer of LadderVAE performed substantially better than when using the root (or the top layer). We chose Ward's method as it performed better than other classical hierarchical clustering methods (such as single-linkage agglomeration clustering or bisecting K-means). However, Ward's method cannot be tested on a held-out dataset, as it inherently does not allow for prediction on new data. For this reason, we train and test Ward's method on the test set embeddings given by the baseline models, which were trained on the training data, for a fairer comparison with the other methods.

### Resource Usage

In the following, we describe the resource usage of our experiments. All our experiments were run on RTX3080 GPUs, except for CelebA, where we increased the memory requirement and use a RTX3090. Training TreeVAE with \(10\) leaves on MNIST, Fashion-MNIST, and Omniglot-50 takes between \(1\)h and \(2\)h, Omniglot-5 \(30\) minutes, CIFAR-10 \(5\)h. Training TreeVAE with \(20\) leaves on 20Newsgroup takes approximately \(30\) minutes, and on CIFAR-100 9h. Training TreeVAE on CelebAtakes approx 8h. Please note that we only report the numbers to generate the final results but not the development time.