# Active Negative Loss Functions for Learning with Noisy Labels

Xichen Ye

Shanghai University

Shanghai, China

yexichen0930@shu.edu.cn

&Xiaoqiang Li

Shanghai University

Shanghai, China

xqli@shu.edu.cn

&Songmin Dai

Shanghai University

Shanghai, China

laodar@shu.edu.cn

&Tong Liu

Shanghai University

Shanghai, China

tong_liu@shu.edu.cn

&Yan Sun

Shanghai University

Shanghai, China

yansun@shu.edu.cn

&Weiqin Tong

Shanghai University

Shanghai, China

wqtong@shu.edu.cn

Corresponding Author

###### Abstract

Robust loss functions are essential for training deep neural networks in the presence of noisy labels. Some robust loss functions use Mean Absolute Error (MAE) as its necessary component. For example, the recently proposed Active Passive Loss (APL) uses MAE as its passive loss function. However, MAE treats every sample equally, slows down the convergence and can make training difficult. In this work, we propose a new class of theoretically robust passive loss functions different from MAE, namely _Normalized Negative Loss Functions_ (NNLFs), which focus more on memorized clean samples. By replacing the MAE in APL with our proposed NNLFs, we improve APL and propose a new framework called _Active Negative Loss_ (ANL). Experimental results on benchmark and real-world datasets demonstrate that the new set of loss functions created by our ANL framework can outperform state-of-the-art methods. The code is available at https://github.com/Virusdoll/Active-Negative-Loss.

## 1 Introduction

Relying on large-scale datasets with high quality annotations, such as ImageNet [1], deep neural networks (DNNs) achieve good performance in various supervised classification tasks. However, in practice, the process of labeling large-scale datasets is costly and inevitably introduces noisy (mislabeled) samples. Moreover, empirical studies show that over-parameterized DNNs can easily fit a randomly labeled dataset [2], which implies that DNNs may have a poor evaluation performance when trained on a noisy dataset. As a result, noisy label learning has received a lot of attention.

Different approaches have been proposed to solve the noisy label learning problem, and one popular research line is to design noise-robust loss functions, which is also the main focus of this paper. Ghosh et al. [3] have theoretically proved that, symmetric loss functions such as Mean Absolute Error (MAE), are robust to noise, while others like commonly used Cross Entropy (CE) are not. However, MAE treats every sample equally, leading to significantly longer training time before convergence and even making learning difficult, which suggests that MAE is not suitable for training DNNs with challenging datasets [4]. Motivated by this, several works proposed partially robust loss functions, including Generalized Cross Entropy (GCE) [4], a generalized mixture of CE and MAE, and Symmetric Cross Entropy (SCE) [5], a combination of CE and a scaled MAE, Reverse CrossEntropy (RCE). Recently, Active Passive Loss (APL) [6] framework has been proposed to create fully robust loss functions.

APL is one of the state-of-the-art methods, and shows that any loss function can be made robust to noisy labels by a simple normalization operation. Moreover, to address the underfitting problem of normalized loss functions, APL first characterizes existing loss functions into two types and then combines them. The two types are 1) "Active" loss, which only explicitly maximizes the probability of being in the labeled class, and 2) "Passive" loss, which also explicitly minimizes the probabilities of being in other classes. However, by investigating several robust loss functions created by the APL framework, we find that their passive loss functions are always scaled versions of MAE. As we mentioned before, MAE is not conducive to training. As a result, to address the underfitting problem, APL combines active loss functions with MAE, which again may lead to difficulty in training and further limits its performance.

The fact that APL still struggles with MAE motivates us to investigate new robust passive loss functions. In this paper, we propose a new class of passive loss functions different from MAE, called _Negative Loss Functions_ (NLFs). We show that, by combining 1) complementary label learning [7; 8] and 2) a simple "vertical flipping" operation, any active loss function can be made into a passive loss function. Moreover, to make it theoretically robust to noisy labels, we further apply the normalization operation on NLFs to obtain _Normalized Negative Loss Functions_ (NNLFs). By replacing the MAE in APL with NNLF, we propose a novel framework called _Active Negative Loss_ (ANL). ANL combines a normalized active loss function and a NNLF to build a new set of noise-robust loss functions, which can be seen as an improvement of APL. We show that under our proposed ANL framework, several commonly-used loss functions can be made robust to noisy labels while ensuring sufficient fitting ability to achieve state-of-the-art performance for training DNNs with noisy datasets. Our key contributions are highlighted as follows:

* We provide a method to build a new class of robust passive loss functions called _Normalized Negative Loss Function_s (NNLFs). By replacing the MAE in APL with our proposed NNLFs, we propose a novel framework, _Active Negative Loss_ (ANL), to construct a new set of robust loss functions.
* We demonstrate the theoretical robustness of our proposed NNLFs and ANL to noisy labels, and discuss how replacing the MAE in APL with our NNLFs enhances performance in noisy label learning.
* Our empirical results show that the new set of loss functions, created using our proposed ANL framework, outperform existing state-of-the-art methods.

## 2 Preliminaries

### Risk Minimization and Label Noise Model

Consider a typical K-class classification problem. Let \(\mathcal{X}\subset\mathbb{R}^{d}\) be the \(d\)-dimensional feature space from which the samples are drawn, and \(\mathcal{Y}=[k]=\{1,\cdots,K\}\) be the label space. Given a clean training dataset, \(\mathcal{S}=\{(\bm{x}_{n},y_{n})\}_{n=1}^{N}\), where each \((\bm{x}_{n},y_{n})\) is drawn _i.i.d._ from an unknown distribution, \(\mathcal{D}\), over \(\mathcal{X}\times\mathcal{Y}\). We denote the distribution over different labels for sample \(\bm{x}\) by \(\bm{q}(k|\bm{x})\), and \(\sum_{k=1}^{K}\bm{q}(k|\bm{x})=1\). Since there is only one corresponding label \(y\) for a \(\bm{x}\), we have \(\bm{q}(y|\bm{x})=1\) and \(\bm{q}(k\neq y|\bm{x})=0\).

A classifier, \(h(\bm{x})=\arg\max_{i}f(\bm{x})_{i}\), where \(f:\mathcal{X}\rightarrow\mathcal{C}\), \(\mathcal{C}\subseteq[0,1]^{K},\forall\bm{c}\in\mathcal{C}\), \(\bm{1}^{T}\bm{c}=1\), is a function that maps feature space to label space. In this work, we consider \(f\) as a DNN ending with a softmax output layer. For each sample \(\bm{x}\), \(f(\bm{x})\) computes its probability \(\bm{p}(k|\bm{x})\) of each label \(k\in\{1,\cdots,K\}\), and \(\sum_{k=1}^{K}\bm{p}(k|\bm{x})=1\). Throughout this paper, as a notation, we call \(f\) itself the classifier. Training a classifier \(f\) is to find an optimal classifier \(f^{*}\) that minimize the empirical risk defined by a loss function: \(\sum_{n=1}^{N}\mathcal{L}(f(\bm{x}_{n}),y_{n})\), where \(\mathcal{L}:\mathcal{C}\times\mathcal{Y}\rightarrow\mathbb{R}^{+}\) is a loss function, and \(\mathcal{L}(f(\bm{x}),k)\) is the loss of \(f(\bm{x})\) with respect to label \(k\).

When label noise is present, our model can only access a corrupted dataset \(\mathcal{S}_{\eta}=\{(\bm{x}_{n},\hat{y}_{n})\}_{n=1}^{N}\), where each sample is drawn _i.i.d._ from an unknown distribution, \(\mathcal{D}_{\eta}\). In this paper, we consider a popular approach for modeling label noise, which simply assumes that, given the true label \(y\), the corruption process is conditionally independent of input features \(\bm{x}\)[9]. So we can formulate noisy label \(\hat{y}\) as:

\[\hat{y}=\begin{cases}y&\text{with probability }(1-\eta_{y})\\ j,j\in[k],j\neq y&\text{with probability }\eta_{yj}\end{cases},\] (1)

where \(\eta_{yj}\) denotes the probability that true label \(y\) is corrupted into label \(j\), and \(\eta_{y}=\sum_{j\neq y}\eta_{yj}\) denotes the noise rate of label \(y\). Under our assumption of label noise model, label noise can be either _symmetric_ or _asymmetric_. The noise is called _symmetric_, if \(\eta_{ij}=\frac{\eta_{i}}{K-1},\forall j\neq y\) and \(\eta_{i}=\eta,\forall i\in[k]\), where \(\eta\) is a constant. And for _asymmetric_ noise, \(\eta_{ij}\) is conditioned on both the true label \(i\) and corrupted label \(j\).

### Active Passive Loss Functions

Ghosh et al. [3] have shown, under some mild assumptions, a loss function \(\mathcal{L}\) is noise tolerant if it is symmetric: \(\sum_{k=1}^{K}\mathcal{L}(f(\bm{x}),k)=C,\forall\bm{x}\in\mathcal{X}\), where \(C\) is some constant. Based on this, Ma et al. [6] proposed the normalized loss functions, which normalize a loss function \(\mathcal{L}\) by:

\[\mathcal{L}_{\text{norm}}=\frac{\mathcal{L}(f(\bm{x}),y)}{\sum_{k=1}^{K} \mathcal{L}(f(\bm{x}),k)}.\] (2)

This simple normalization operation can make any loss function robust to noisy labels, since we always have \(\sum_{k}^{K}\mathcal{L}_{\text{norm}}=1\). For example, the Normalized Cross Entropy (NCE) is:

\[NCE=\frac{\sum_{k=1}^{K}\bm{q}(k|\bm{x})(-\log\bm{p}(k|\bm{x}))}{\sum_{j=1}^{K} \sum_{k=1}^{K}\bm{q}(y=j|\bm{x})\log\bm{p}(k|\bm{x})}.\] (3)

Similarly, we can normalize FL, MAE, and RCE to obtain Normalized Focal Loss (NFL), Normalized Mean Absolute Error (NMAE), and Normalized Reverse Cross Entropy (NRCE), respectively.

But a normalized loss function alone suffers from the underfitting problem. To address this, Ma et al. [6] characterize existing loss functions into two types: _Active_ and _Passive_. Denote the function of loss \(\mathcal{L}(f(\bm{x}),y)\) by \(\ell(f(\bm{x}),k)\), that is \(\mathcal{L}(f(\bm{x}),y)=\sum_{k=1}^{K}\ell(f(\bm{x}),k)\) (e.g., let \(\mathcal{L}\) be CE, then \(\mathcal{L}(f(\bm{x}),y)\)\(=\sum_{k=1}^{K}\bm{q}(k|\bm{x})(-\log\bm{p}(k|\bm{x}))\), and \(\ell(f(\bm{x}),k)=\bm{q}(k|\bm{x})(-\log\bm{p}(k|\bm{x}))\)), we have the following definitions:

**Definition 1** (Active loss function).: \(\mathcal{L}_{\text{Active}}\) is an active loss function if \(\forall(\bm{x},y)\in\mathcal{D},\forall k\neq y,\ell(f(\bm{x}),k)=0\).

**Definition 2** (Passive loss function).: \(\mathcal{L}_{\text{Passive}}\) is a passive loss function if \(\forall(\bm{x},y)\in\mathcal{D},\exists k\neq y,\ell(f(\bm{x}),k)\neq 0\).

Active loss functions only explicitly maximize \(\bm{p}(y|\bm{x})\), the classifier's output probability at the class position specified by the label \(y\). In contrast, passive loss functions also explicitly minimize \(\bm{p}(k\neq y|\bm{x})\), the probability at least one other class positions. Accordingly, the active loss functions include CE, FL, NCE, and NFL, while the passive loss functions include MAE, RCE, NMAE, and NRCE. These two types of loss functions can mutually boost each other to mitigate underfitting, and we refer the reader to [6] for more detailed discussions. By combining them, Ma et al. proposed the Active Passive Loss (APL):

\[\mathcal{L}_{\text{APL}}=\alpha\cdot\mathcal{L}_{\text{Active}}+\beta\cdot \mathcal{L}_{\text{passive}},\] (4)

where \(\alpha,\beta>0\) are parameters. As an example, by combining NCE and RCE, Ma et al get NCE+RCE, one of the state-of-the-art methods.

### APL struggles with MAE

As shown in the previous subsection, there are four passive loss functions available to APL, including MAE, RCE, NMAE, and NRCE. However, we can show that all these passive loss functions are scaled versions of MAE. Specifically, \(NMAE=\frac{1}{2(K-1)}\cdot MAE\), \(RCE=-\frac{A}{2}\cdot MAE\), and \(NRCE=\frac{1}{2(K-1)}\cdot MAE\) (detailed derivations can be found in appendix A.1). Thus, we can rewrite APL as follows:

\[\mathcal{L}_{\text{APL}}=\alpha\cdot\mathcal{L}_{\text{Active}}+\beta\cdot MAE.\] (5)This indicates that MAE is a necessary component of the current APL. However, as we mentioned before, MAE requires longer training time and even makes learning difficult. Thus, on the one hand, APL needs passive loss functions to mitigate active loss functions underfitting, yet on the other hand, MAE is not training friendly, which may limit the performance of APL. This motivates us to investigate new robust passive loss functions.

## 3 Active Negative Loss Functions

### Method

**Normalized Negative Loss Functions.** Our goal is to find a method that creates robust passive loss functions from existing active loss functions. This method must consist of three components that: 1) let the loss function optimize the classifier's output probability for at least one other class position that is not specified by the label \(y\), 2) let the loss function minimize the classifier's output probability instead of maximizing it, and 3) let the loss function robust to noisy labels. Inspired by NLNL [10] and APL [6], we use 1) complementary label learning, 2) "vertical flipping" and 3) normalization operation as these three components respectively.

Complementary label learning [7; 8] is an indirect learning method for training CNNs, which randomly selects complementary labels and trains the CNN to recognize that "input does not belong to this complementary label". Unlike the usual training approach, complementary label learning focuses on the loss of classifier's predictions with complementary labels, which naturally fits with the passive loss function. Here, we only use its basic idea of letting the loss function focus on all classes \(\{1,\cdots,K\}\) except the labeled class \(y\).

"Vertical flipping" is a simple operation that can convert the loss function from "maximizing" to "minimizing". As shown in the fig. 1, given an active loss function \(\mathcal{L}(f(\bm{x}),k)\), the new loss function \(A-\mathcal{L}(f(\bm{x}),k)\) is obtained by flipping \(\mathcal{L}(f(\bm{x}),k)\) vertically with axis loss \(=\frac{1}{2}A\). It should be noted that, \(A-\mathcal{L}(f(\bm{x}),k)\) is the opposite of \(\mathcal{L}(f(\bm{x}),k)\), and it focuses on optimizing \(\bm{p}(k|\bm{x})\) to \(0\).

Based on these two components, given an active loss function \(\mathcal{L}\), we propose Negative Loss Functions (NLFs) as follows:

\[\mathcal{L}_{\text{neg}}(f(\bm{x}),y) =\sum_{k=1}^{K}(1-\bm{q}(k|\bm{x}))(A-\mathcal{L}(f(\bm{x}),k)),\] (6) \[A =\mathcal{L}([\cdots,\bm{p}_{\text{min}},\cdots]^{T},y).\] (7)

Here, \([\cdots,\bm{p}_{\text{min}},\cdots]^{T}\) is some probability distribution that may be output by the classifier \(f\), where \(\bm{p}(y|\bm{x})=\bm{p}_{\text{min}}\), the minimum value of \(\bm{p}(k|\bm{x})\) (e.g., \(0\)). Therefore \(A\) is some constant, the maximum loss value of \(\mathcal{L}\). In practice, setting \(\bm{p}_{\text{min}}=0\) could cause some computational problems, for example, if \(\mathcal{L}\) is CE and \(\bm{p}_{\text{min}}=0\), then \(A=-\log 0=+\infty\). So in this paper, unless otherwise specified, we define \(\bm{p}_{\text{min}}=1\times 10^{-7}\). This technique is similar to the clipping operation implemented by most deep learning frameworks.

Our proposed NLF can transform any active loss function into a passive loss function, where 1) \((1-\bm{q}(k|\bm{x}))\) ensures that the loss function focuses on classes \(\{1,\cdots,K\}\setminus\{y\}\), and 2) \((A-\mathcal{L}(f(\bm{x}),k))\) ensures that the loss function aims to minimize the output probability \(\bm{p}(k|\bm{x})\).

Next, to make our proposed passive loss functions robust to noisy labels, we perform a normalization operation on NLFs. Given an active loss function \(\mathcal{L}\), we propose Normalized Negative Loss Functions (NNLFs) as follows:

\[\mathcal{L}_{\text{nn}}(f(\bm{x}),y)=1-\frac{A-\mathcal{L}(f(\bm{x}),y)}{\sum_{k =1}^{K}A-\mathcal{L}(f(\bm{x}),k)},\] (8)

where \(A\) has the same definition as eq. (7). The detailed derivation of NNLFs can be found in appendix A.2. Additionally, NNLFs have the property that \(\mathcal{L}_{\text{nn}}\in[0,1]\). Accordingly, we can create NNLFs from active loss functions as follows.

The Normalized Negative Cross Entropy (NNCE) is:

\[NNCE=1-\frac{A-(-\log\bm{p}(y|\bm{x}))}{\sum_{k=1}^{K}A-(-\log\bm{p}(k|\bm{x}))},\] (9)

where \(A=-\log\bm{p}_{\text{min}}\).

The Normalized Negative Focal Loss (NNFL) is:

\[NNFL=1-\frac{A-(-(1-\bm{p}(y|\bm{x}))^{\gamma}\log\bm{p}(y|\bm{x}))}{\sum_{k=1 }^{K}A-(-(1-\bm{p}(k|\bm{x}))^{\gamma}\log\bm{p}(k|\bm{x}))},\] (10)

where \(A=-(1-\bm{p}_{\text{min}})^{\gamma}\log\bm{p}_{\text{min}}\).

**ANL Framework.** We can now create new robust loss functions by replacing the MAE in APL with our proposed NNLF. Given an active loss function \(\mathcal{L}\), we propose the Active Negative Loss (ANL) functions as follows:

\[\mathcal{L}_{\text{ANL}}=\alpha\cdot\mathcal{L}_{\text{norm}}+\beta\cdot \mathcal{L}_{\text{nn}}.\] (11)

Here, \(\alpha\) and \(\beta\) are parameters greater than 0, \(\mathcal{L}_{\text{norm}}\) denotes the normalized \(\mathcal{L}\) and \(\mathcal{L}_{\text{nn}}\) denotes the Normalized Negative Loss Function corresponding to \(\mathcal{L}\). Accordingly, we can create ANL from the two mentioned active loss functions as follows.

For Cross Entropy (CE), we have ANL-CE:

\[\mathcal{L}_{\text{ANL-CE}}=\alpha\cdot\mathcal{L}_{\text{NCE}}+\beta\cdot \mathcal{L}_{\text{NNCE}}.\] (12)

For Focal Loss (FL), we have ANL-FL:

\[\mathcal{L}_{\text{ANL-FL}}=\alpha\cdot\mathcal{L}_{\text{NFL}}+\beta\cdot \mathcal{L}_{\text{NNFL}}.\] (13)

### Robustness to noisy labels

**NNLFs are symmetric.** We first prove that our proposed Normalized Negative Loss Functions (NNLFs) are symmetric. Detailed proofs can be found in appendix B.

**Theorem 1**.: _Normalized negative loss function \(\mathcal{L}_{nn}\) is symmetric._

**NNLFs are robust to noise.** In reference to Theorem 1 and Theorem 3 from [3], it has been proven that symmetric loss functions, under some mild assumptions, exhibit noise tolerant in the face of both symmetric and asymmetric noise. Given that our NNLFs fall under the category of symmetric loss functions, they inherently possess the attribute of noise tolerant.

**ANL is robust to noise.** In light of Lemma 3 from [6], it is understood that the combination of two noise tolerant loss functions retains the noise tolerant attribute. It is noteworthy that both \(\mathcal{L}_{\text{norm}}\) and \(\mathcal{L}_{\text{nn}}\) within our ANL are noise tolerant, which makes ANL as a whole noise tolerant.

### NNLFs focus more on well-learned samples

As shown in the fig. 2, by replacing MAE with our proposed NNCE, NCE+NNCE and ANL-CE show better fitting ability. This raises the question: _why does NNLF perform better than MAE?_ In the following, taking NNCE as an example, we analysis the gradients of MAE and NNCE to provide a preliminary answer to this question. Detailed derivations and proofs can be found in appendix C.

The gradient of the MAE with respect to the classifier's output probability can be derived as:

\[\frac{\partial\mathcal{L}_{\text{MAE}}}{\partial\bm{p}(j|\bm{x})}=\left\{ \begin{array}{cc}1,&j\neq y\\ -1,&j=y.\end{array}\right.\] (14)The gradient of the NNCE with respect to the classifier's output probability can be derived as:

\[\frac{\partial\mathcal{L}_{\text{NNCE}}}{\partial\bm{p}(j|\bm{x})} =\left\{\begin{array}{cc}\frac{1}{\bm{p}(j|\bm{x})}\cdot\frac{A+ \log\bm{p}(y|\bm{x})}{\left(\sum_{k=1}^{K}A+\log\bm{p}(k|\bm{x})\right)^{2}},& j\neq y\\ -\frac{1}{\bm{p}(j|\bm{x})}\cdot\frac{\sum_{k\neq y}A+\log\bm{p}(k|\bm{x})}{ \left(\sum_{k=1}^{K}A+\log\bm{p}(k|\bm{x})\right)^{2}},&j=y.\end{array}\right.\] (15)

For the purpose of analysis, we consider how the gradients of NNCE would differ from MAE in the following two scenarios: 1) given the classifier's output probability of sample \(\bm{x}\), we analyze the difference in gradient for each class, 2) given the classifier's output probabilities of sample \(\bm{x}_{1}\) and \(\bm{x}_{2}\), we analyze the difference in gradient between these two samples.

**Theorem 2**.: _Given the classifier's output probability \(\bm{p}(\cdot|\bm{x})\) for sample \(\bm{x}\) and normalized negative cross entropy \(\mathcal{L}_{\text{NNCE}}\). If \(\bm{p}(j_{1}|\bm{x})<\bm{p}(j_{2}|\bm{x})\), \(j_{1}\neq j_{2}\neq y\), then \(\frac{\partial\mathcal{L}_{\text{NNCE}}}{\partial\bm{p}(j_{1}|\bm{x})}>\frac{ \partial\mathcal{L}_{\text{NNCE}}}{\partial\bm{p}(j_{2}|\bm{x})}\)._

**Theorem 3**.: _Given the classifier's output probabilities \(\bm{p}(\cdot|\bm{x}_{1})\) and \(\bm{p}(\cdot|\bm{x}_{2})\) of sample \(\bm{x}_{1}\) and \(\bm{x}_{2}\), where \(\bm{p}(y|\bm{x}_{1})\geq\bm{p}(k|\bm{x}_{1})\), \(p(y|\bm{x}_{2})\geq\bm{p}(k|\bm{x}_{2})\), \(\forall k\in\{1,\cdots,K\}\), \(\bm{p}(j|\bm{x}_{1})=\bm{p}(j|\bm{x}_{2})\), \(j\neq y\), and normalized negative cross entropy \(\mathcal{L}_{\text{NNCE}}\). If \(\bm{p}(y|\bm{x}_{1})>\bm{p}(y|\bm{x}_{2})\) and \(\bm{p}(k|\bm{x}_{1})\leq\bm{p}(k|\bm{x}_{2})\), \(\forall k\in\{1,\cdots,K\}\)\(\backslash\{j,y\}\), then \(\frac{\partial\mathcal{L}_{\text{NNCE}}}{\partial\bm{p}(j|\bm{x}_{1})}>\frac{ \partial\mathcal{L}_{\text{NNCE}}}{\partial\bm{p}(j|\bm{x}_{2})}\)._

theorem 2 and theorem 3 demonstrate that, for the gradient of the non-labeled classes, our NNCE focuses more on the classes and samples that have been well learned compared to MAE, which treats every class and sample equally This property may enhance the model's performance in noisy label learning. Some studies [11] have shown that during the training process, DNNs would first memorize clean samples and then noisy samples. According to the property revealed by theorem 2 and theorem 3, apart from robustness, our NNLF may potentially help the model to continuously learn the clean samples that the model has memorized in the previous stages of training and ignore the unmemorized noisy samples.

## 4 Experiments

In this section, we empirically investigate our proposed ANL functions on benchmark datasets, including MNIST [12], CIFAR-10/CIFAR-100 [13] and a real-world noisy dataset WebVision [14].

### Empirical Understandings

In this subsection, we explore some properties of our proposed loss functions. Unless otherwise specified, all detailed experimental settings are the same as those in section 4.2. More experiments and discussions about gradient, parameter analysis, and \(\bm{p}_{\text{min}}\) can be found in the appendix D.1.

Figure 2: Training and test accuracies of different loss functions. (a) - (e): CIFAR-10 under 0.8 symmetric noise. (f) - (j): CIFAR-100 under 0.6 symmetric noise. The accuracies of noisy samples in training set should be as low as possible, since they are mislabeled.

**Overfitting problem.** In practice, we find that ANL can lead to overfitting in some experimental settings. To motivate this problem, as an example, we train networks using different loss functions on CIFAR-10 under 0.8 symmetric noise and CIFAR-100 under 0.6 symmetric noise, and the experimental results are shown in fig. 2. As can be observed, in the setting of CIFAR-10 under 0.8 symmetric noise, the training set accuracy of ANL-CE (w/ L2) keeps increasing while the test set accuracy keeps decreasing. We identify this problem as an _overfitting problem_.

It is worth noting that although overfitting occurs, unlike CE, the gap between the clean sample accuracies and the noisy sample accuracies of the training set does not shrink, which indicates that our ANL has some robustness to noisy labels even in the case of overfitting. Moreover, we conjecture that the overfitting is caused by the property of NNLF focusing more on well-learned samples. When the noise rate is high, one might assume that the model has been trained on only a fairly small number of clean samples, with a very large gradient, which can lead to overfitting.

**The choice of regularization method.** We also find that the commonly used L2 regularization may struggle to mitigate the overfitting problem. To address this, we decide to try using other regularization methods. We consider 2 other regularization methods: L1 and Jacobian [15, 16]. To compare the performance of these methods, we apply them to ANL-CE for training on CIFAR-10 under 0.8 symmetric noise. For simplicity and fair comparison, we use \(\delta\) as the coefficient of the regularization term and consider it as an additional parameter of ANL. We keep \(\alpha\) and \(\beta\) the same as in section 4.2, tune \(\delta\) for all three methods by following the parameter tuning setting in appendix D.2. We also train networks without using any regularization method. The results reported in fig. 3. As can be observed, among the three regularization methods, only L1 can somewhat better mitigate the overfitting problem. If not otherwise specified, all ANLs in this paper use L1 regularization.

**Robustness and fitting ability.** We conduct a set of experiments on CIFAR-10/-100 to verify the robustness and fitting ability of our proposed loss functions. We set the noise type to be symmetric and the noise rate to 0.8 for CIFAR-10 and 0.6 for CIFAR-100. In each setting, we train the network using different loss functions, including: 1) CE, 2) MAE, 3) NCE+RCE, 4) ANL-CE (w/ L2), and 5) ANL-CE (w/ L1). For ANL-CE (w/ L2), we set its parameters \(\alpha\) and \(\beta\) to be the same as ANL-CE (w/ L1) and set its weight decay to be the same as NCE+RCE.

As can be observed in fig. 2: 1) CE is not robust to noise, the accuracies of clean and noisy samples in the training set are continuously close to each other, 2) MAE is robust to noise, the accuracies of clean and noisy samples in the training set keep moving away from each other, but its fitting ability is insufficient, especially when the dataset becomes complex, 3) NCE+RCE is robust to noise and has better fitting ability compared to MAE, 4) ANL-CE (w/ L2) is robust to noise and has stronger fitting ability, but suffers from over-fitting. and 5) ANL-CE is robust to noise and mitigates the impact of overfitting to achieve the best performance. To summarize, our proposed loss functions are robust to noise, NNLF shows better fitting ability than MAE, and L1 regularization addresses the overfitting problem of NNLF.

\begin{table}
\begin{tabular}{c|c|c c c c} \hline Methods & Clean (\(\eta=0.0\)) & \(\eta=0.2\) & \(\eta=0.4\) & \(\eta=0.6\) & \(\eta=0.8\) \\ \hline NCE & 88.68 & 81.65 & 74.80 & 63.14 & 37.52 \\ NNCE & 91.51 & 90.09 & 86.91 & 82.16 & 57.06 \\ ANL-CE & 91.66\(\pm\)0.04 & **90.02\(\pm\)0.23** & **87.28\(\pm\)0.02** & **81.12\(\pm\)0.30** & **61.27\(\pm\)0.55** \\ \hline \end{tabular}
\end{table}
Table 1: Test accuracies (%) of different methods on CIFAR-10 datasets with clean and symmetric (\(\eta\in\{0.2,0.4,0.6,0.8\}\)) label noise. The top-1 best results are in **bold**.

Figure 3: Test accuracies of ANL-CE on CIFAR-10 under 0.8 symmetric noise with different regularization methods and different parameters. \(\delta\) is the weight of regularization term of ANL-CE.

**Active and passive parts separately.** In table 1, we show the results of the active and passive parts separately. We separately train NCE and NNCE on CIFAR-10 with different symmetric noise rates while maintaining the same parameters as ANL-CE. Specifically, for \(\alpha\cdot\text{NCE}\), we set \(\alpha\) to \(5.0\) and for \(\beta\cdot\text{NNCE}\), we set \(\beta\) to \(5.0\), while \(\delta\) is set to \(5\times 10^{-5}\) for both. As indicated in the results, the test set accuracies of NNCE are very close to those of ANL-CE, except in the case with a 0.8 noise rate. This suggests that NNCE performs well on its own at low noise rates. However, at very high noise rates, a combination of active losses is needed to achieve better performance.

### Evaluation on Benchmark Datasets

**Baselines.** We consider several state-of-the-art methods: 1) Generalized Cross Entropy (GCE) [4]; 2) Symmetric Cross Entropy (SCE) [5]; 3) Negative Learning for Noisy Labels (NLNL) [10]; 4) Active Passive Loss (APL) [6], including NCE+MAE, NCE+RCE, and NFL+RCE; 5) Asymmetric Loss Functions (AFLs) [17], including NCE+AEL, NCE+AGCE, and NCE+AUL. For our proposed ANL, we consider two loss functions: 1) ANL-CE and 2) ANL-FL. Additionally, we train networks using Cross Entropy (CE), Focal Loss (FL) [18], and Mean Absolute Error (MAE).

**Experimental Details.** The full experimental results and the detailed settings of noise generation, networks, training and parameters can be found in the appendix D.2.

**Results.** The main experimental results under symmetric and asymmetric label noise are reported in table 2. For more experimental results, please see appendix D.2. As can be observed, our ANL-CE and ANL-FL show significant improvement for most label noise settings of CIFAR-10/-100, especially when the data become more complex and the noise rate becomes larger. For example, on CIFAR-10 under 0.8 symmetric noise, our ANL-CE outperform the state-of-the-art method (55.62% of NCE+AGCE) by more than 5.0%. Overall, the experimental results demonstrate that our ANL can show outstanding performance on different datasets, noise types, and noise rates, which validates the effectiveness of our proposed NNLFs and ANL.

\begin{table}
\begin{tabular}{c|c|c c c c c c c} \hline \multirow{2}{*}{Datasets} & \multirow{2}{*}{Methods} & \multicolumn{6}{c}{Symmetric Noise Rate (\(\eta\))} & \multicolumn{3}{c}{Aymmetric Noise Rate (\(\eta\))} \\  & & & 0.4 & 0.6 & 0.8 & 0.2 & 0.3 & 0.4 \\ \hline \multirow{11}{*}{MNIST} & CE & 99.20\(\pm\)0.02 & 74.46\(\pm\)0.28 & 49.19\(\pm\)0.05 & 22.51\(\pm\)0.23 & 94.02\(\pm\)0.18 & 88.90\(\pm\)0.07 & 81.79\(\pm\)0.34 \\  & MAE & 99.16\(\pm\)0.03 & 98.80\(\pm\)0.02 & 97.69\(\pm\)0.20 & 70.35\(\pm\)1.16 & **91.01\(\pm\)0.08** & 98.42\(\pm\)0.09 & 87.40\(\pm\)0.41 \\  & GCE [4] & 99.81\(\pm\)0.01 & 98.61\(\pm\)0.13 & 80.80\(\pm\)0.31 & 33.95\(\pm\)0.48 & 96.95\(\pm\)0.07 & 88.99\(\pm\)0.27 & 81.91\(\pm\)0.58 \\  & SCE [5] & 99.30\(\pm\)0.07 & 97.48\(\pm\)0.16 & 88.38\(\pm\)0.77 & 32.88\(\pm\)0.81 & 97.95\(\pm\)0.23 & 94.00\(\pm\)0.41 & 84.54\(\pm\)0.14 \\  & NCL [10] & 98.61\(\pm\)0.13 & 97.17\(\pm\)0.09 & 95.49\(\pm\)0.04 & 88.64\(\pm\)1.43 & 98.95\(\pm\)0.01 & 97.51\(\pm\)0.15 & 98.54\(\pm\)0.26 \\  & NCE+RCE [17] & 99.43\(\pm\)0.02 & 98.51\(\pm\)0.09 & 95.61\(\pm\)0.12 & 74.01\(\pm\)1.38 & 98.90\(\pm\)0.10 & 95.16\(\pm\)0.08 & 91.60\(\pm\)0.22 \\  & NCE+AGCE [17] & 99.10\(\pm\)0.03 & **98.91\(\pm\)0.04** & **98.90\(\pm\)0.07** & **96.93\(\pm\)0.13** & 99.04\(\pm\)0.02 & **99.94\(\pm\)0.04** & **98.41\(\pm\)0.04** \\  & **ANL-CE** & 99.98\(\pm\)0.05 & 98.45\(\pm\)0.05 & 98.42\(\pm\)0.08 & **96.21\(\pm\)0.12** & 99.00\(\pm\)0.04 & 98.91\(\pm\)0.07 & 98.91\(\pm\)0.10 \\ \hline \multirow{11}{*}{CIFAR-10} & **ANL-FL** & 99.13\(\pm\)0.05 & **98.90\(\pm\)0.05** & **98.46\(\pm\)0.12** & 95.32\(\pm\)0.22 & **99.05\(\pm\)0.09** & **99.93\(\pm\)0.02** & **98.18\(\pm\)0.01** \\ \cline{1-1} \cline{2-10}  & CE & 90.83\(\pm\)0.11 & 85.19\(\pm\)0.12 & 8

### Evaluation on Real-world Noisy Labels

Here, we evaluate our proposed ANL methods on large-scale real-world noisy dataset WebVision [14], which contains more than 2.4 million web images crawled from the internet by using queries generated from the 1,000 class labels of the ILSVRC 2012 [1] benchmark. Here, we follow the "Mini" setting in [19], and only take the first 50 classes of the Google resized image subset. We evaluate the trained networks on the same 50 classes of both the ILSVRC 2012 validation set and the WebVision validation set, and these can be considered as clean validation sets. We compare our ANL-CE and ANL-FL with GCE, SCE, NCE+RCE, and NCE+AGCE. The experimental details can be found in the appendix D.3. The results are reported in table 3. As can be observed, our proposed methods outperform the existing loss functions. This verifies that our proposed ANL framework can help the trained model against real-world label noise.

Moreover, in addition to WebVision, to further validate the effectiveness of our method on real-world noisy datasets, we also conduct a set of experiments on CIFAR-10N/-100N [20], Animal-10N [21], and Clothing-1M [22]. The experimental details and results can be found in the appendix D.5, which demonstrate the effectiveness of our method on different real-world noisy datasets.

## 5 Limitations

We believe that the main limitation of our approach lies in the choice of regularization method. Although we have experimentally verified that L1 is the most efficient among the three regularization methods (L1, L2, and Jacobian), we lack further theoretical analysis of it. Furthermore, although we only consider relatively simple regularization methods for the sake of fair comparison, other regularization methods, such as dropout [23] or regmixup [24], might be more effective in mitigating the overfitting problem caused by NNLF. And we believe that a better solution to overfitting can further improve the performance of our method.

## 6 Related Work

In recent years, some robust loss-based methods have been proposed for robust learning with noisy labels. Here, we briefly review the relevant approaches. Ghosh et al. [3] theoretically proved that symmetric loss functions, such as MAE, are robust to label noise. Zhang and Sabuncu [4] proposed Generalized Cross Entropy (GCE), a generalization of CE and MAE. Wang et al. [5] suggested a combination of CE and scaled MAE, and proposed Symmetric Cross Entropy (SCE). Menon et al. [25] proposed composite loss-based gradient clipping and applied it to CE to obtain PHuber-CE. Ma et al. [6] proposed Active Passive Loss (APL) to create fully robust loss functions. Feng et al. [26] applied the Taylor series to derive an alternative representation of CE and proposed Taylor-CE accordingly. Zhou et al. [17] proposed Asymmetric Loss Functions (ALFs) to overcome the symmetric condition. Inspired by complementary label learning, NLNL [10] and JNPL [27] use complementary labels to reduce the risk of providing the wrong information to the model.

## 7 Conclusion

In this paper, we propose a new class of theoretically robust passive loss functions different from MAE, which we refer to as _Normalized Negative Loss Functions_ (NNLFs). By replacing the MAE in APL with our NNLF, we propose _Active Negative Loss_ (ANL), a robust loss function framework with stronger fitting ability. We theoretically demonstrate that our NNLFs and ANLs are robust to noisy labels and also highlight the property that NNLFs focus more on well-learned samples. We found in our experiments that NNLFs have a potential overfitting problem, and we suggest using L1 regularization to mitigate it. Experimental results verified that our ANL can outperform the state-of-the-art methods on benchmark datasets.

## Acknowledgements

This work is supported in part by Shanghai Science and Technology Committee under grant No. 21511100600 and No. 22511106005.

## References

* [1] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. _Proceedings of the 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pages 248-255, 2009.
* [2] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. _Proceedings of the 5th International Conference on Learning Representations_, 2017.
* [3] Aritra Ghosh, Himanshu Kumar, and P. S. Sastry. Robust Loss Functions under Label Noise for Deep Neural Networks. _Proceedings of the 31st AAAI Conference on Artificial Intelligence_, pages 1919-1925, 2017.
* [4] Zhilu Zhang and Mert R. Sabuncu. Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels. _Advances in Neural Information Processing Systems 31_, pages 8792-8802, 2018.
* [5] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric Cross Entropy for Robust Learning with Noisy Labels. _Proceedings of the 2019 IEEE International Conference on Computer Vision_, pages 322-330, 2019.
* [6] Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah M. Erfani, and James Bailey. Normalized Loss Functions for Deep Learning with Noisy Labels. _Proceedings of the 37th International Conference on Machine Learning_, pages 6543-6553, 2020.
* [7] Takashi Ishida, Gang Niu, Weihua Hu, and Masashi Sugiyama. Learning from Complementary Labels. _Advances in Neural Information Processing Systems 30_, pages 5639-5649, 2017.
* [8] Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with Biased Complementary Labels. _15th Proceedings of the European Conference on Computer Vision_, pages 69-85, 2018.
* [9] Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep Ravikumar, and Ambuj Tewari. Learning with Noisy Labels. _Advances in Neural Information Processing Systems 26_, pages 1196-1204, 2013.
* [10] Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. NLNL: Negative Learning for Noisy Labels. _Proceedings of the 2019 IEEE International Conference on Computer Vision_, pages 101-110, 2019.
* [11] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. _International conference on artificial intelligence and statistics_, pages 4313-4324, 2020.
* [12] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [13] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
* [14] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision Database: Visual Learning and Understanding from Web Data. _CoRR_, abs/1708.02862, 2017. URL http://arxiv.org/abs/1708.02862.
* [15] Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Robust Large Margin Deep Neural Networks. _IEEE Trans. Signal Process._, 65(16):4265-4280, 2017.
* [16] Judy Hoffman, Daniel A. Roberts, and Sho Yaida. Robust Learning with Jacobian Regularization. _CoRR_, abs/1908.02729, 2019. URL http://arxiv.org/abs/1908.02729.
* [17] Xiong Zhou, Xianming Liu, Junjun Jiang, Xin Gao, and Xiangyang Ji. Asymmetric Loss Functions for Learning with Noisy Labels. _Proceedings of the 38th International Conference on Machine Learning_, pages 12846-12856, 2021.
* [18] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollar. Focal Loss for Dense Object Detection. _Proceedings of the 2017 IEEE International Conference on Computer Vision_, pages 2999-3007, 2017.

* [19] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels. _Proceedings of the 35th International Conference on Machine Learning_, pages 2309-2318, 2018.
* [20] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. _Proceedings of the 10th International Conference on Learning Representations_, 2022.
* [21] Hwanjun Song, Minseok Kim, and Jae-Gil Lee. SELFIE: Refurbishing Unclean Samples for Robust Deep Learning. _Proceedings of the 36th International Conference on Machine Learning_, 2019.
* [22] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. _Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition_, pages 2691-2699, 2015.
* [23] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. _Journal of Machine Learning Research_, 15(56):1929-1958, 2014.
* [24] Francesco Pinto, Harry Yang, Ser Nam Lim, Philip H. S. Torr, and Puneet K. Dokania. Using mixup as a regularizer can surprisingly improve accuracy & out-of-distribution robustness. _Advances in Neural Information Processing Systems 35_, 2022.
* [25] Aditya Krishna Menon, Ankit Singh Rawat, Sanjiv Kumar, and Sashank Reddi. Can gradient clipping mitigate label noise? _Proceedings of the 8th International Conference on Learning Representations_, 2020.
* [26] Lei Feng, Senlin Shu, Zhuoyi Lin, Fengmao Lv, Li Li, and Bo An. Can cross entropy loss be robust to label noise? _29th International Joint Conference on Artificial Intelligence_, 2020.
* [27] Youngdong Kim, Juseung Yun, Hyounguk Shon, and Junmo Kim. Joint Negative and Positive Learning for Noisy Labels. _Proceedings of the 2021 IEEE Conference on Computer Vision and Pattern Recognition_, pages 9442-9451, 2021.
* [28] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach. _Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition_, pages 2233-2241, 2017.
* [29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. _Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition_, pages 770-778, 2016.
* [30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _Proceedings of the 3rd International Conference on Learning Representations_, 2015.
* [31] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework for learning with noisy labels. _Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition_, pages 5552-5560, 2018.

Loss functions

### NMAE, RCE, and NRCE are scaled versions of MAE

Here, inspired by [5; 6], we show how to reformulate NMAE, RCE, and NRCE as scaled versions of MAE. First, for convenience, we reformulate the Mean Absolute Error (MAE) as follows:

\[MAE =\sum_{k=1}^{K}|\bm{p}(k|\bm{x})-\bm{q}(k|\bm{x})|\] (16) \[=(1-\bm{p}(y|\bm{x}))+\sum_{k\neq y}\bm{p}(k|\bm{x})\] \[=2(1-\bm{p}(y|\bm{x})),\]

where the last equality holds due to \(\sum_{k=1}^{K}\bm{p}(k|\bm{x})=1\).

For Normalized Mean Absolute Error (NMAE), we have:

\[NMAE =\frac{\sum_{k=1}^{K}|\bm{p}(k|\bm{x})-\bm{q}(k|\bm{x})|}{\sum_{j= 1}^{K}\sum_{k=1}^{K}|\bm{p}(k|\bm{x})-\bm{q}(y=j|\bm{x})|}\] (17) \[=\frac{2(1-\bm{p}(y|\bm{x}))}{\sum_{j=1}^{K}2(1-\bm{p}(j|\bm{x}))}\] \[=\frac{1}{K-1}(1-\bm{p}(y|\bm{x}))\] \[=\frac{1}{2(K-1)}\cdot MAE.\]

This shows that the NMAE is a scaled version of the MAE with a factor of \(\frac{1}{2(K-1)}\).

For Reverse Cross Entropy (RCE), we have:

\[RCE =\sum_{k=1}^{K}\bm{p}(x|\bm{x})(-\log\bm{q}(k|\bm{x}))\] (18) \[=\bm{p}(y|\bm{x})(-\log 1)+\sum_{k\neq y}\bm{p}(k|\bm{x})(-\log 0)\] \[=\sum_{k\neq y}\bm{p}(k|\bm{x})(-A)\] \[=-A(1-\bm{p}(y|\bm{x}))\] \[=-\frac{A}{2}\cdot MAE,\]

recall that in RCE we set \(\log 0=A\), where \(A<0\) is some constant (e.g., \(A=-4\)). This shows that the RCE is a scaled version of the MAE with a factor of \(-\frac{A}{2}\).

For Normalized Reverse Cross Entropy (NRCE), we have:

\[NRCE =\frac{\sum_{k=1}^{K}\bm{p}(k|\bm{x})(-\log\bm{q}(k|\bm{x}))}{ \sum_{j=1}^{K}\sum_{k=1}^{K}\bm{p}(k|\bm{x})(-\log\bm{q}(y=j|\bm{x}))}\] (19) \[=\frac{-A(1-\bm{p}(y|\bm{x}))}{\sum_{j=1}^{K}(-A(1-\bm{p}(j|\bm{ x})))}\] \[=\frac{1}{K-1}(1-\bm{p}(y|\bm{x}))\] \[=\frac{1}{2(K-1)}\cdot MAE.\]

This shows that NRCE is a scaled version of the MAE with a factor of \(\frac{1}{2(K-1)}\).

### Normalized Negative Loss Functions

Here, we show how to derive Normalized Negative Loss Functions (NNLFs) into its proper form.

\[NNLF =\frac{\sum_{k=1}^{K}(1-\bm{q}(k|\bm{x}))(A-\mathcal{L}(f(\bm{x}),k) )}{\sum_{j=1}^{K}\sum_{k=1}^{K}(1-\bm{q}(k=j|\bm{x}))(A-\mathcal{L}(f(\bm{x}),k ))}\] (20) \[=\frac{\sum_{k\neq y}A-\mathcal{L}(f(\bm{x}),k)}{\sum_{j=1}^{K} \sum_{k\neq j}A-\mathcal{L}(f(\bm{x}),k)}\] \[=\frac{\sum_{k\neq y}A-\mathcal{L}(f(\bm{x}),k)}{(K-1)\sum_{k=1}^ {K}A-\mathcal{L}(f(\bm{x}),k)}\] \[=\frac{1}{K-1}\cdot\Big{(}1-\frac{A-\mathcal{L}(f(\bm{x}),y)}{ \sum_{k=1}^{K}A-\mathcal{L}(f(\bm{x}),k)}\Big{)}\] \[\propto 1-\frac{A-\mathcal{L}(f(\bm{x}),y)}{\sum_{k=1}^{K}A- \mathcal{L}(f(\bm{x}),k)}.\]

## Appendix B Noise tolerant

**Theorem 1**.: _Normalized negative loss function \(\mathcal{L}_{\text{nn}}\) is symmetric._

Proof.: For all \(\bm{x}\in\mathcal{X}\) and all \(f\), we have:

\[\begin{split}\sum_{k=1}^{K}\mathcal{L}_{\text{nn}}(f(\bm{x}),k)& =\sum_{k=1}^{K}\Big{(}1-\frac{A-\mathcal{L}(f(\bm{x}),k)}{\sum_{ j=1}^{K}A-\mathcal{L}(f(\bm{x}),j)}\Big{)}\\ &=K-\frac{\sum_{k=1}^{K}A-\mathcal{L}(f(\bm{x}),k)}{\sum_{j=1}^{K }A-\mathcal{L}(f(\bm{x}),j)}\\ &=K-1,\end{split}\] (21)

where \(K-1\) is a constant and \(\mathcal{L}_{\text{nn}}\) satisfies the definition of the symmetric loss function. 

## Appendix C Gradient analysis

### Gradient of MAE

The complete derivation of the Mean Absolute Error (MAE) with respect to the classifier's output probabilities is as follows:

In the case of \(j\neq y\):

\[\begin{split}\frac{\partial\mathcal{L}_{\text{MAE}}}{\partial \bm{p}(j|\bm{x})}&=\frac{\partial}{\partial\bm{p}(j|\bm{x})} \sum_{k=1}^{K}|\bm{p}(k|\bm{x})-\bm{q}(k|\bm{x})|\\ &=\frac{1}{\partial\bm{p}(j|\bm{x})}\big{(}(1-\bm{p}(y|\bm{x}))+ \sum_{k\neq y}\bm{p}(k|\bm{x})\big{)}\\ &=1.\end{split}\] (22)

In the case of \(j=y\):

\[\begin{split}\frac{\partial\mathcal{L}_{\text{MAE}}}{\partial \bm{p}(j|\bm{x})}&=\frac{\partial}{\partial\bm{p}(j|\bm{x})} \sum_{k=1}^{K}|\bm{p}(k|\bm{x})-\bm{q}(k|\bm{x})|\\ &=\frac{1}{\partial\bm{p}(j|\bm{x})}\big{(}(1-\bm{p}(y|\bm{x}))+ \sum_{k\neq y}\bm{p}(k|\bm{x})\big{)}\\ &=-1.\end{split}\] (23)

[MISSING_PAGE_EMPTY:14]

where the input \(\bm{p}\) is a discrete probability distribution, \(\sum_{k=1}^{K}\bm{p}_{k}=1\) and \(0\leq\bm{p}_{k}\leq 1\), \(\forall k\in\{1,\)\(\cdots,K\}\). Given a discrete probability distribution \(\bm{p}\) and a real number \(D\) as input, the function \(f_{2}\) is defined as follows:

\[f_{2}(\bm{p},D)=\log(\bm{p}_{y}+D)+\log\bm{p}_{j}+\sum_{k\neq j\neq y}\log(\bm{ p}_{k}-d_{k}),\] (28)

where \(\bm{p}_{y}\geq\bm{p}_{k}\), \(\forall k\in\{1,\cdots,K\}\), \(0<D\leq 1-\bm{p}_{y}\). And \(\{d_{k}\}\), \(k\in\{1,\cdots,K\}\setminus\{j,y\}\) is a set of random variables which satisfy following conditions: \(0\leq\bm{p}_{k}-d_{k}\leq\bm{p}_{k}\) and \(\sum_{k\neq j\neq y}d_{k}=D\).

Next, we consider whether \(f_{1}(\bm{p})-f_{2}(\bm{p},D)>0\). Given \(\bm{p}\) and \(D\), we have,

\[f_{1}(\bm{p})-f_{2}(\bm{p},D) =\Big{(}\sum_{k=1}^{K}\log\bm{p}_{k}\Big{)}-\Big{(}\log(\bm{p}_{ y}+D)+\log\bm{p}_{j}+\sum_{k\neq j\neq y}\log(\bm{p}_{k}-d_{k})\Big{)}\] \[=\log\bm{p}_{y}-\log(\bm{p}_{y}+D)+\sum_{k\neq j\neq y}\log\bm{p} _{k}-\log(\bm{p}_{k}-d_{k})\] \[=\log\frac{\bm{p}_{y}}{\bm{p}_{y}+D}+\sum_{k\neq j\neq y}\log \frac{\bm{p}_{k}}{\bm{p}_{k}-d_{k}}\] \[\geq\log\frac{\bm{p}_{y}}{\bm{p}_{y}+D}+\sum_{k\neq j\neq y}\log \frac{\bm{p}_{y}}{\bm{p}_{y}-d_{k}}\] \[=\log\bm{p}_{y}-\log(\bm{p}_{y}+D)+\sum_{k\neq j\neq y}\log\bm{p} _{y}-\log(\bm{p}_{y}-d_{k})\] \[=\Bigg{(}\log\bm{p}_{y}+\sum_{k\neq j\neq y}\log\bm{p}_{y}\Big{)} -\Big{(}\log(\bm{p}_{y}+D)+\sum_{k\neq j\neq y}\log(\bm{p}_{y}-d_{k})\Big{)}\] \[\geq\Big{(}\log\bm{p}_{y}+\sum_{k\neq j\neq y}\log\bm{p}_{y}\Big{)} -\sup_{\{d_{k}\},k\neq j\neq y}\Big{(}\log(\bm{p}_{y}+D)+\sum_{k\neq j\neq y} \log(\bm{p}_{y}-d_{k})\Big{)}.\] (29)

The first inequality holds because for a fraction \(\frac{a}{b}\geq 1\), we always have \(\frac{a}{b}\geq\frac{a+c}{b+c}\), where \(c\geq 0\).

To get the maximum value of \(\log(\bm{p}_{y}+D)+\sum_{k\neq j\neq y}\log(\bm{p}_{y}-d_{k})\), we must solve the following minimization problem subject to constraints:

\[\min_{\{d_{k}\},k\neq j\neq y} -\Big{(}\log(\bm{p}_{y}+D)+\sum_{k\neq j\neq y}\log(\bm{p}_{y}-d_{ k})\Big{)}.\] (30) s.t. \[\sum_{k\neq j\neq y}d_{k}=D.\] (31)

We can define the Lagrange function \(L\) as follows:

\[L(d_{1},\cdots,d_{K},\lambda)=-\Big{(}\log(\bm{p}_{y}+D)+\sum_{k\neq j\neq y} \log(\bm{p}_{y}+d_{k})\Big{)}+\lambda\cdot\Big{(}\sum_{k\neq j\neq y}d_{k}-D \Big{)}.\] (32)

Now we can calculate the gradients:

\[\left\{\begin{array}{l}\frac{\partial L}{\partial d_{k}}=-\frac{1}{\bm{p}_{y }+d_{k}}+\lambda,k\neq j\neq y\\ \frac{\partial L}{\partial\lambda}=\sum_{k\neq j\neq y}d_{k}-D.\end{array}\right.\] (33)

Let \(\frac{\partial L}{\partial d_{k}}=0\) and \(\frac{\partial L}{\partial\lambda}=0\), we have:

\[\left\{\begin{array}{l}-\frac{1}{\bm{p}_{y}+d_{k}}+\lambda=0,k\neq j\neq y \\ \sum_{k\neq j\neq y}d_{k}-D=0,\end{array}\right.\] (34)

and therefore:

\[d_{k}=\frac{D}{K-2},k\neq j\neq y.\] (35)So, the minimization value is:

\[-\Big{(}\log(\bm{p}_{y}+D)+\sum_{k\neq j\neq y}\log(\bm{p}_{y}-\frac{D}{K-2}) \Big{)}.\] (36)

Now, back to the eq. (29), we have:

\[\begin{split} f_{1}(\bm{p})-f_{2}(\bm{p},D)&\geq \log\bm{p}_{y}+\sum_{k\neq j\neq y}\log\bm{p}_{y}-\sup_{\{d_{k}\},k\neq j\neq y }\Big{(}\log(\bm{p}_{y}+D)+\sum_{k\neq j\neq y}\log(\bm{p}_{y}-d_{k})\Big{)}\\ &=\log\bm{p}_{y}+\sum_{k\neq j\neq y}\log\bm{p}_{y}-\Big{(}\log( \bm{p}_{y}+D)+\sum_{k\neq j\neq y}\log(\bm{p}_{y}-\frac{D}{K-2})\Big{)}\\ &=\log\bm{p}_{y}-\log(\bm{p}_{y}+D)+(K-2)\Big{(}\log\bm{p}_{y}- \log(\bm{p}_{y}-\frac{D}{K-2})\Big{)}\\ &=(K-2)\Big{(}\log\bm{p}_{y}-\log(\bm{p}_{y}-\frac{D}{K-2}) \Big{)}-\Big{(}\log(\bm{p}_{y}+D)-\log\bm{p}_{y}\Big{)}\\ &=D\cdot\Big{(}\frac{\log\bm{p}_{y}-\log(\bm{p}_{y}-\frac{D}{K-2 })}{\frac{D}{K-2}}-\frac{\log(\bm{p}_{y}+D)-\log\bm{p}_{y}}{D}\Big{)}.\end{split}\] (37)

Recall the nature of the difference, we have \(\frac{\log\bm{p}_{y}-\log(\bm{p}_{y}-\frac{D}{K-2})}{\frac{D}{K-2}}=\frac{d \log x}{dx}\Big{|}_{x=\bm{p}_{y}-\alpha}=\frac{1}{\bm{p}_{y}-\alpha}\), where \(0\leq\alpha\leq\frac{D}{K-2}\), and \(\frac{\log(\bm{p}_{y}+D)-\log\bm{p}_{x}}{D}=\frac{d\log x}{dx}\Big{|}_{x=\bm{ p}_{y}+\beta}=\frac{1}{\bm{p}_{y}+\beta}\), where \(0\leq\beta\leq D\). And therefore:

\[\frac{\log\bm{p}_{y}-\log(\bm{p}_{y}-\frac{D}{K-2})}{\frac{D}{K-2}}=\frac{1}{ \bm{p}_{y}-\alpha}>\frac{1}{\bm{p}_{y}}>\frac{1}{\bm{p}_{y}+\beta}=\frac{\log (\bm{p}_{y}+D)-\log\bm{p}_{y}}{D}.\] (38)

So we have:

\[\begin{split}\frac{\log\bm{p}_{y}-\log(\bm{p}_{y}-\frac{D}{K-2}) }{\frac{D}{K-2}}&>\frac{\log(\bm{p}_{y}+D)-\log\bm{p}_{y}}{D}\\ f_{1}(\bm{p})&>f_{2}(\bm{p},D).\end{split}\] (39)

Now, let \(\bm{p}_{k}=\bm{p}(k|\bm{x}_{2}),\forall k\in\{1,\cdots,K\}\), \(D=\bm{p}(y|\bm{x}_{1})-\bm{p}(y|\bm{x}_{2})\), and \(d_{k}=\bm{p}(k|\bm{x}_{2})-\bm{p}(k|\bm{x}_{1})\), \(k\neq j\neq y\). Following eq. (39), we have:

\[\begin{split} f_{1}(\bm{p})&>f_{2}(\bm{p},D)\\ \sum_{k=1}^{K}\log\bm{p}_{k}&>\log(\bm{p}_{y}+D)+ \log\bm{p}_{j}+\sum_{k\neq j\neq y}\log(\bm{p}_{k}-d_{k})\\ \sum_{k=1}^{K}\log\bm{p}(k|\bm{x}_{2})&>\sum_{k=1}^{ K}\log\bm{p}(k|\bm{x}_{1})\\ \frac{1}{\big{(}\sum_{k=1}^{K}A+\log\bm{p}(k|\bm{x}_{1})\big{)}^ {2}}&>\frac{1}{\big{(}\sum_{k=1}^{K}A+\log\bm{p}(k|\bm{x}_{2}) \big{)}^{2}}\\ \frac{1}{\bm{p}(j|\bm{x}_{1})}&\cdot\frac{A+\log \bm{p}(y|\bm{x}_{1})}{\big{(}\sum_{k=1}^{K}A+\log\bm{p}(k|\bm{x}_{1})\big{)}^ {2}}&>\frac{1}{\bm{p}(j|\bm{x}_{2})}\cdot\frac{A+\log\bm{p}(y|\bm{ x}_{2})}{\big{(}\sum_{k=1}^{K}A+\log\bm{p}(k|\bm{x}_{2})\big{)}^{2}}\\ \frac{\partial\mathcal{L}_{\text{NNCE}}}{\partial\bm{p}(j|\bm{x} _{1})}&>\frac{\partial\mathcal{L}_{\text{NNCE}}}{\partial\bm{p}(j| \bm{x}_{2})}.\end{split}\] (40)

The eq. (40) holds because we have \(\bm{p}(j|\bm{x}_{1})=\bm{p}(j|\bm{x}_{2})\), and \(\log\bm{p}(y|\bm{x}_{1})>\log\bm{p}(y|\bm{x}_{2})\), where \(\bm{p}(y|\bm{x}_{1})>\bm{p}(y|\bm{x}_{2})\). This completes the proof.

## Appendix D Experiments

### Empirical Understandings

**Gradients of NNCE.** We conduct experiments with ANL-CE on CIFAR-10 under 0.6 and 0.8 symmetric noise. For each sample, we calculate the gradient of our NNCE with respect to the predicted probability of each non-labeled class and take their maximum value, which is \(\max_{j,j\neq y}\big{(}\frac{\partial\mathcal{L}_{\text{NNCE}}}{\partial\bm{p} (j|\bm{x})}\big{)}\). The results are shown in fig. 4 in the form of box plots. As can be observed, as the number of training epochs increases, the gradients of the clean samples are concentrated at larger values and the gradients of the noisy samples are concentrated at smaller values. The means of the gradients for noisy samples are higher than the medians and smaller than the means for clean samples' gradients. This indicates that although the model is still incorrectly learning some noisy samples, these learned noisy samples constitute only a small fraction of all noisy samples in the training set. In general, this result verifies that our NNLFs have the ability to focus more on clean samples and ignore noisy samples during the training process, and also verifies our discussion in section 3.3.

**Parameter Analysis.** We apply different values of \(\alpha\) and \(\beta\) to NCE+NNCE for training on CIFAR-10 under 0.8 symmetric noise. We test the combinations between \(\alpha\in\{0.1,0.5,1.0,5.0,10.0\}\) and \(\beta\in\{0.1,0.5,1.0,5.0,10.0\}\), where \(\alpha\) is the weight of NCE and \(\beta\) is the weight of NNCE. The weight decay of NCE+NNCE is set to be the same as NCE+RCE. As can be observed in fig. 5, regardless of how \(\alpha\) (the weight of NCE) varies, when \(\beta\) (the weight of NNCE) increases, both the robustness and the fitting ability of the model improve, although overfitting occurs. This verifies that our NNLFs are robust to noisy labels and have good fitting ability. Although in ANL we use L1 regularization instead of L2 regularization (weight decay), the effect of \(\alpha\) and \(\beta\) in the loss functions created by ANL should be similar to that in NCE+NNCE.

Figure 4: The gradient \(\max_{j,j\neq y}\big{(}\frac{\partial\mathcal{L}_{\text{NNCE}}}{\partial\bm{p} (j|\bm{x})}\big{)}\) for clean and noisy samples. Blue boxes indicate clean samples, red boxes indicate noisy samples, blue triangles indicate the means of clean samples, and red triangles indicate the means of noisy samples. The outliers are ignored.

Figure 5: Training and test accuracies of NCE+NNCE on CIFAR-10 under 0.8 symmetric noise with different parameters. \(\alpha\) is the weight of NCE and \(\beta\) is the weight of NNCE. The accuracies of noisy samples in training set (red dashed line) should be as low as possible, since they are mislabeled.

Figure 6: Test accuracies of NCE+RCE on CIFAR-10 under 0.8 symmetric noise with different regularization methods and different parameters. \(\delta\) is the weight of regularization term of NCE+RCE. As a comparison, the blue line shows the test accuracy of our ANL-CE with the same data set and noise setting.

**Can APL outperform our ANL by changing the regularization method?** We apply different regularization methods to NCE+RCE for training on CIFAR-10 under 0.8 symmetric noise and compare the results with our ANL-CE. Specifically, we train networks using loss functions in the form of \(\alpha\cdot NCE+\beta\cdot RCE+\delta\cdot\text{Reg}\), where \(\alpha,\beta,\delta>0\) are parameters and Reg is the regularization term. Similarly, we consider 3 regularization methods: 1) L2, 2) L1, and 3) Jacobian [15; 16]. We keep the \(\alpha\) and \(\beta\) of NCE+RCE the same as in section 4.2 and tune \(\delta\in\{~{}1\times 10^{-7},~{}5\times 10^{-7},~{}1\times 10^{-6}~{}5\times 10 ^{-6}~{}1\times 10^{-5}~{}5\times 10^{-5}~{}1\times 10^{-4}\}\) for all three methods. As can be observed in fig. 6, the performance of NCE+RCE can be improved by adapting the regularization method. However, the improvement from changing the regularization method is limited, and the test accuracies of NCE+RCE are consistently lower than our ANL-CE, no matter how the regularization method is changed and how the parameter \(\delta\) is varied. This verifies that APL cannot outperform our ANL by changing the regularization method and also verifies the performance of our proposed NNLFs.

**Does the \(\bm{p_{\text{min}}}\) affect the performance of ANL?** As shown in the definitions of NLF and NNLF, \(\bm{p_{\text{min}}}\) controls the lower bound of the classifier's output probability, and further controls the constant \(A\). In fig. 7, we show different \(\bm{p_{\text{min}}}\in\{1\times 10^{-5},1\times 10^{-6},1\times 10^{-7},1\times 10 ^{-8},1\times 10^{-9}\}\) applied to ANL-CE for training on CIFAR-10/-100 under 0.6 symmetric noise. As can be observed, for both datasets, \(\bm{p_{\text{min}}}\) does not significant affect the performance of ANL.

### Evaluation on Benchmark Datasets

**Noise generation.** The noisy labels are generated following standard approaches in previous works [28; 6; 17]. For symmetric noise, we flip the labels in each class randomly to incorrect labels of other classes. For asymmetric noise, we flip the labels within a specific set of classes. For MNIST, flipping \(7\to 1\), \(2\to 7\), \(5\leftrightarrow 6\), \(3\to 8\). For CIFAR-10, flipping \(\text{TRUCK}\rightarrow\text{AUTMOMBILE}\), BIRD \(\rightarrow\text{AIRPLANE}\), \(\text{DEER}\rightarrow\text{HORSE}\), \(\text{CAT}\leftrightarrow\text{DOG}\). For CIFAR-100, the 100 classes are grouped into 20 super-classes with each has 5 sub-classes, and each class are flipped within the same super-class into the next in a circular fashion. We vary the noise rate \(\eta\in\{0.2,0.4,0.6,0.8\}\) for symmetric noise and \(\eta\in\{0.1,0.2,0.3,0.4\}\) for asymmetric noise.

**Networks and training.** We follow the experimental setting in previous works [6; 17]. A 4-layer CNN is used for MNIST, an 8-layer CNN is used for CIFAR-10, and a ResNet-34 [29] is used for CIFAR-100. For MNIST, CIFAR-10, and CIFAR-100, the networks are trained for 50, 120, and 200 epochs, respectively. For all the training, we use SGD optimizer with momentum 0.9 and cosine learning rate annealing. Weight decay is set to \(1\times 10^{-3}\), \(1\times 10^{-4}\), and \(1\times 10^{-5}\) for MNIST, CIFAR-10, and CIFAR-100, respectively. Particularly, for our proposed ANL methods, weight decay is set to \(0\) for all datasets. The initial learning rate is set to \(0.01\) for MNIST/CIFAR-10 and \(0.1\) for CIFAR-100. Batch size is set to \(128\). For all settings, we clip the gradient norm to \(5.0\). Typical data augmentations including random width/height shift and horizontal flip are applied.

**Parameter tuning.** For each dataset, we tune the parameters of ANL-CE under 0.8 symmetric noise and then use them directly for all other noise settings and ANL functions. Specifically, we use 10% of the original training set as the validation set, and generate 0.8 symmetric noise on the remaining 90% of the original training set as the training set by the standard noise generation approach. We tune the parameters \(\alpha\in\{0.1,\,0.5,\,1.0,\,5.0,\,10.0\}\), \(\beta\in\{0.1,\,0.5,\,1.0,\,5.0,\,10.0\}\), and \(\delta\in\{~{}1\times 10^{-7},~{}5\times 10^{-7},~{}1\times 10^{-6},5\times 10 ^{-6},~{}1\times 10^{-5},~{}5\times 10^{-5},1\times 10^{-4}~{}\}\).

Figure 7: Test accuracies of different \(\bm{p_{\text{min}}}\) applied to ANL-CE on CIFAR-10/CIFAR-100 under 0.6 symmetric noise.

[MISSING_PAGE_EMPTY:20]

\begin{table}
\begin{tabular}{c|c|c|c c c c} \hline \multirow{2}{*}{Datasets} & \multirow{2}{*}{Methods} & \multirow{2}{*}{Clean (\(\eta\)=0.0)} & \multicolumn{4}{c}{Symmetric Noise Rate (\(\eta\))} \\  & & & 0.2 & 0.4 & 0.6 & 0.8 \\ \hline \multirow{9}{*}{MNIST} & CE & 99.20\(\pm\)0.02 & 91.40\(\pm\)0.28 & 74.46\(\pm\)0.28 & 49.19\(\pm\)0.05 & 22.51\(\pm\)0.23 \\  & FL & 99.16\(\pm\)0.08 & 91.66\(\pm\)0.18 & 75.42\(\pm\)0.25 & 50.58\(\pm\)0.38 & 22.93\(\pm\)0.11 \\  & MAE & 99.16\(\pm\)0.03 & 99.03\(\pm\)0.01 & 98.80\(\pm\)0.02 & 97.69\(\pm\)0.20 & 70.35\(\pm\)1.16 \\  & GCE & 99.18\(\pm\)0.01 & 98.84\(\pm\)0.03 & 96.81\(\pm\)0.13 & 80.86\(\pm\)0.31 & 33.59\(\pm\)0.48 \\  & SCE & 99.30\(\pm\)0.07 & 98.91\(\pm\)0.04 & 97.48\(\pm\)0.16 & 88.35\(\pm\)0.77 & 48.28\(\pm\)0.81 \\  & NLNL & 98.61\(\pm\)0.13 & 98.02\(\pm\)0.14 & 97.17\(\pm\)0.09 & 95.42\(\pm\)0.30 & 86.34\(\pm\)1.43 \\  & NCE+MAE & 99.22\(\pm\)0.06 & 98.96\(\pm\)0.04 & 98.15\(\pm\)0.14 & 92.94\(\pm\)0.26 & 59.54\(\pm\)0.76 \\  & NCE+RCE & 99.43\(\pm\)0.02 & **99.20\(\pm\)0.05** & 98.53\(\pm\)0.09 & 95.61\(\pm\)0.12 & 74.04\(\pm\)1.83 \\  & NFL+RCE & 99.33\(\pm\)0.04 & **99.17\(\pm\)0.03** & 98.62\(\pm\)0.03 & 95.54\(\pm\)0.26 & 75.05\(\pm\)1.45 \\  & NCE+AE & 99.06\(\pm\)0.06 & 99.03\(\pm\)0.08 & 98.87\(\pm\)0.05 & 98.39\(\pm\)0.11 & 96.71\(\pm\)0.10 \\  & NCE+AGC & 99.10\(\pm\)0.03 & 99.00\(\pm\)0.05 & **98.91\(\pm\)0.04** & **98.50\(\pm\)0.07** & **96.93\(\pm\)0.13** \\  & NCE+AUL & 99.19\(\pm\)0.01 & 99.04\(\pm\)0.05 & **99.00\(\pm\)0.03** & **98.52\(\pm\)0.14** & **96.97\(\pm\)0.12** \\ \cline{2-6}  & **ANL-CE** & 99.08\(\pm\)0.05 & 98.97\(\pm\)0.02 & 98.84\(\pm\)0.05 & 98.42\(\pm\)0.08 & 96.62\(\pm\)0.12 \\  & **ANL-FL** & 99.13\(\pm\)0.05 & 98.94\(\pm\)0.07 & 98.90\(\pm\)0.05 & 98.46\(\pm\)0.12 & 95.73\(\pm\)0.22 \\ \hline \multirow{9}{*}{CIFAR-10} & CE & 90.38\(\pm\)0.11 & 75.05\(\pm\)0.26 & 58.19\(\pm\)0.21 & 38.75\(\pm\)0.19 & 19.09\(\pm\)0.35 \\  & FL & 89.84\(\pm\)0.28 & 74.52\(\pm\)0.10 & 57.54\(\pm\)0.75 & 38.83\(\pm\)0.49 & 19.33\(\pm\)0.58 \\  & MAE & 89.15\(\pm\)0.27 & 87.19\(\pm\)0.19 & 81.76\(\pm\)3.17 & 76.82\(\pm\)0.84 & 46.42\(\pm\)3.66 \\  & GCE & 89.66\(\pm\)0.20 & 87.17\(\pm\)0.01 & 82.44\(\pm\)0.26 & 68.62\(\pm\)0.35 & 25.45\(\pm\)0.51 \\  & SCE & 91.38\(\pm\)0.12 & 87.86\(\pm\)0.12 & 79.96\(\pm\)0.25 & 62.16\(\pm\)0.33 & 27.98\(\pm\)0.98 \\  & NLNL & 90.73\(\pm\)0.20 & 73.70\(\pm\)0.05 & 63.90\(\pm\)0.44 & 50.68\(\pm\)0.47 & 29.53\(\pm\)1.55 \\  & NCE+MAE & 88.94\(\pm\)0.13 & 87.37\(\pm\)0.19 & 83.70\(\pm\)0.21 & 76.35\(\pm\)0.08 & 44.68\(\pm\)1.12 \\  & NCE+RCE & 90.94\(\pm\)0.01 & 89.19\(\pm\)0.18 & 86.03\(\pm\)0.13 & 79.89\(\pm\)0.11 & 55.52\(\pm\)2.74 \\  & NFL+RCE & 90.86\(\pm\)0.51 & 89.04\(\pm\)0.09 & 86.08\(\pm\)0.33 & 79.79\(\pm\)0.16 & 54.18\(\pm\)2.06 \\  & NCE+AEL & 88.51\(\pm\)0.26 & 86.59\(\pm\)0.24 & 83.07\(\pm\)0.46 & 75.06\(\pm\)0.26 & 41.79\(\pm\)1.40 \\  & NCE+AGCE & 91.08\(\pm\)0.06 & 89.11\(\pm\)0.07 & 86.16\(\pm\)0.10 & 80.14\(\pm\)0.27 & 55.62\(\pm\)4.78 \\  & NCE+AUL & 91.26\(\pm\)0.12 & 89.08\(\pm\)0.14 & 86.11\(\pm\)0.27 & 79.39\(\pm\)0.41 & 54.49\(\pm\)2.77 \\ \cline{2-6}  & **ANL-CE** & 91.66\(\pm\)0.04 & **90.02\(\pm\)0.23** & **87.28\(\pm\)0.02** & **81.12\(\pm\)0.30** & **61.27\(\pm\)0.55** \\  & **ANL-FL** & 91.79\(\pm\)0.19 & **89.95\(\pm\)0.20** & **87.25\(\pm\)0.11** & **81.67\(\pm\)0.19** & **61.22\(\pm\)0.85** \\ \hline \multirow{9}{*}{CIFAR-100} & CE & 71.14\(\pm\)0.38 & 55.97\(\pm\)1.11 & 40.72\(\pm\)0.74 & 22.98\(\pm\)0.07 & 7.55\(\pm\)0.21 \\  & FL & 71.02\(\pm\)0.36 & 55.94\(\pm\)0.53 & 39.55\(\pm\)1.24 & 23.21\(\pm\)0.49 & 7.80\(\pm\)0.27 \\  & MAE & 7.35\(\pm\)1.19 & 7.91\(\pm\)0.66 & 3.61\(\pm\)0.21 & 3.63\(\pm\)0.35 & 2.83\(\pm\)1.35 \\  & GCE & 61.62\(\pm\)0.43 & 61.50\(\pm\)1.50 & 56.46\(\pm\)0.95 & 46.27\(\pm\)1.30 & 19.51\(\pm\)0.86 \\  & SCE & 70.80\(\pm\)0.37 & 55.04\(\pm\)0.37 & 39.84\(\pm\)0.19 & 21.97\(\pm\)0.92 & 7.87\(\pm\)0.48 \\  & NLNL & 68.72\(\pm\)0.60 & 46.99\(\pm\)0.91 & 30.29\(\pm\)1.64 & 16.60\(\pm\)0.90 & 11.01\(\pm\)2.48 \\  & NCE+MAE & 67.52\(\pm\)0.21 & 52.68

\begin{table}
\begin{tabular}{c|c|c c c c} \hline \multirow{2}{*}{Datasets} & \multirow{2}{*}{Methods} & \multicolumn{4}{c}{Asymmetric Noise Rate (\(\eta\))} \\  & & 0.1 & 0.2 & 0.3 & 0.4 \\ \hline \multirow{9}{*}{MNIST} & CE & 97.70\(\pm\)0.02 & 94.02\(\pm\)0.18 & 88.90\(\pm\)0.07 & 81.79\(\pm\)0.34 \\  & FL & 97.62\(\pm\)0.05 & 94.41\(\pm\)0.11 & 88.82\(\pm\)0.35 & 81.99\(\pm\)0.61 \\  & MAE & 99.05\(\pm\)0.06 & **99.11\(\pm\)0.03** & 98.42\(\pm\)0.09 & 87.40\(\pm\)4.01 \\  & GCE & 98.97\(\pm\)0.02 & 96.59\(\pm\)0.07 & 88.99\(\pm\)0.27 & 81.91\(\pm\)0.58 \\  & SCE & 99.05\(\pm\)0.02 & 97.95\(\pm\)0.23 & 94.00\(\pm\)0.41 & 84.54\(\pm\)0.14 \\  & NLNL & 98.63\(\pm\)0.06 & 98.35\(\pm\)0.01 & 97.51\(\pm\)0.15 & 95.84\(\pm\)0.26 \\  & NCE+MAE & 99.12\(\pm\)0.09 & 98.33\(\pm\)0.14 & 95.16\(\pm\)0.21 & 85.79\(\pm\)0.47 \\  & NCE+RCE & **99.31\(\pm\)0.06** & 98.79\(\pm\)0.10 & 95.16\(\pm\)0.08 & 91.36\(\pm\)0.22 \\  & NFL+RCE & **99.27\(\pm\)0.02** & 98.79\(\pm\)0.15 & 96.97\(\pm\)0.09 & 91.36\(\pm\)0.40 \\  & NCE+AEL & 99.06\(\pm\)0.05 & 99.05\(\pm\)0.07 & 98.88\(\pm\)0.02 & 98.29\(\pm\)0.27 \\  & NCE+AGCE & 99.11\(\pm\)0.04 & 99.04\(\pm\)0.02 & **98.94\(\pm\)0.03** & **98.41\(\pm\)0.04** \\  & NCE+AUL & 99.11\(\pm\)0.05 & **99.09\(\pm\)0.03** & **98.98\(\pm\)0.06** & **98.70\(\pm\)0.01** \\ \cline{2-6}  & **ANL-CE** & 99.10\(\pm\)0.05 & 99.04\(\pm\)0.04 & 98.91\(\pm\)0.07 & 98.01\(\pm\)0.10 \\  & **ANL-FL** & 99.00\(\pm\)0.05 & 99.05\(\pm\)0.09 & 98.93\(\pm\)0.02 & 98.18\(\pm\)0.01 \\ \hline \multirow{9}{*}{CIFAR-10} & CE & 86.85\(\pm\)0.15 & 83.00\(\pm\)0.33 & 78.15\(\pm\)0.17 & 73.69\(\pm\)0.20 \\  & FL & 86.32\(\pm\)0.28 & 83.03\(\pm\)0.10 & 78.53\(\pm\)0.16 & 73.78\(\pm\)0.16 \\  & MAE & 88.22\(\pm\)0.05 & 79.63\(\pm\)0.74 & 67.35\(\pm\)3.41 & 57.36\(\pm\)2.37 \\  & GCE & 88.19\(\pm\)0.21 & 85.55\(\pm\)0.24 & 79.32\(\pm\)0.52 & 72.83\(\pm\)0.17 \\  & SCE & 89.57\(\pm\)0.11 & 86.22\(\pm\)0.44 & 80.20\(\pm\)0.20 & 74.01\(\pm\)0.52 \\  & NLNL & 88.54\(\pm\)0.25 & 84.74\(\pm\)0.08 & 81.26\(\pm\)0.43 & 76.97\(\pm\)0.52 \\  & NCE+MAE & 88.22\(\pm\)0.25 & 86.16\(\pm\)0.18 & 82.98\(\pm\)0.15 & 75.23\(\pm\)0.24 \\  & NCE+RCE & 89.98\(\pm\)0.19 & 88.36\(\pm\)0.13 & 84.84\(\pm\)0.16 & **77.75\(\pm\)0.37** \\  & NFL+RCE & 90.11\(\pm\)0.06 & 88.26\(\pm\)0.27 & 84.72\(\pm\)0.18 & 77.29\(\pm\)0.30 \\  & NCE+AEL & 87.64\(\pm\)0.16 & 85.64\(\pm\)0.07 & 81.95\(\pm\)0.38 & 74.55\(\pm\)0.42 \\  & NCE+AGCE & 90.21\(\pm\)0.16 & 88.48\(\pm\)0.09 & 84.79\(\pm\)0.15 & **78.60\(\pm\)0.41** \\  & NCE+AUL & 90.12\(\pm\)0.06 & 88.29\(\pm\)0.15 & 84.84\(\pm\)0.06 & 76.99\(\pm\)0.26 \\ \cline{2-6}  & **ANL-CE** & **90.90\(\pm\)0.13** & **89.13\(\pm\)0.11** & **85.52\(\pm\)0.24** & 77.63\(\pm\)0.31 \\  & **ANL-FL** & **90.81\(\pm\)0.20** & **89.09\(\pm\)0.31** & **85.81\(\pm\)0.23** & 77.73\(\pm\)0.31 \\ \hline \multirow{9}{*}{CIFAR-100} & CE & 65.17\(\pm\)0.45 & 58.25\(\pm\)1.00 & 50.30\(\pm\)0.19 & 41.53\(\pm\)0.34 \\  & FL & 64.55\(\pm\)0.39 & 58.00\(\pm\)1.38 & 50.77\(\pm\)0.41 & 41.88\(\pm\)0.57 \\ \cline{1-1}  & MAE & 6.63\(\pm\)1.60 & 6.19\(\pm\)0.42 & 5.82\(\pm\)0.96 & 3.96\(\pm\)0.35 \\ \cline{1-1}  & GCE & 64.29\(\pm\)0.90 & 59.06\(\pm\)0.46 & 53.88\(\pm\)0.96 & 41.51\(\pm\)0.52 \\ \cline{1-1}  & SCE & 64.64\(\pm\)0.57 & 57.78\(\pm\)0.83 & 50.15\(\pm\)0.12 & 41.33\(\pm\)0.86 \\ \cline{1-1}  & NLNL & 59.55\(\pm\)1.22 & 50.19\(\pm\)0.56 & 42.81\(\pm\)1.13 & 35.10\(\pm\)0.20 \\ \cline{1-1}  & NCE+MAE & 60.52\(\pm\)0.36 & 52.92\(\pm\)0.50 & 44.41\(\pm\)0.22 & 36.71\(\pm\)0.16 \\ \cline{1-1}  & NCE+RCE & 66.18\(\pm\)0.23 & 62.77\(\pm\)0.53 & 55.62\(\pm\)0.56 & 42.46\(\pm\)0.42 \\ \cline{1-1}  & NFL+RCE & 66.16\(\pm\)0.44 & 63.43\(\pm\)0.71 & 55.63\(\pm\)0.37 & 42.54\(\pm\)0.52 \\ \cline{1-1}  & NCE+AEL & 57.47\(\pm\)0.47 & 50.49\(\pm\)0.12 & 42.46\(\pm\)0.51 & 35.04\(\pm\)0.29 \\ \cline{1-1}  & NCE+AGCE & 66.86\(\pm\)0.23 & 64.05\(\pm\)0.25 & 56.36\(\pm\)0.59 & 44.90\(\pm\)0.62 \\ \cline{1-1}  & NCE+AUL & 66.23\(\pm\)0.21 & 57.79\(\pm\)0.40 & 47.64\(\pm\)0.24 & 38.65\(\pm\)0.30 \\ \cline{1-1} \cline{2-6}  & **ANL-CE** & **68.78\(\pm\)0.11** & **66.27\(\pm\)0.19** & **59.76\(\pm\)0.34** & **45.41\

### Comparisons on more real-world datasets.

**CIFAR-10N/-100N**[20] are CIFAR-10/-100 equipped with human-annotated real-world noisy labels. CIFAR-10N contains five noisy label sets with noise rates of 9.03% (Aggregate), 17.23% (Random 1), 18.12% (Random 2), 17.64% (Random 3) and 40.21% (Worst). CIFAR-100N contains one noisy label set with noise rate of 40.20% (Noisy).

We use the same experimental settings and parameters as CIFAR-10 and CIFAR-100 for CIFAR-10N and CIFAR-100N, since the only difference is the noise label distribution. The results are reported in Table 8 and Table 9.

**Animal-10N**[21] is a real-world noisy data set of human-labeled online images for 10 confusing animals, with \(50,000\) training and \(5,000\) testing images, and its noise rate was estimated at 8%.

We follow the experimental setting in previous works [21]. We use VGG-19 with batch normalization. The SGD optimizer is employed. We train the network for 100 epochs and use an initial learning rate of 0.1, which is divided by 5 at 50% and 75% of the total number of epochs. Batch size is set to 128. Typical data augmentations including random horizontal flip are applied.

We compare our ANL-CE with CE and GCE. We use L2 regularization (weight decay) for GCE, and L1 regularization for ANL-CE. We denote the regularization coefficient by \(\delta\). We tune the parameters \(\{\delta\}\), \(\{q,\delta\}\), \(\{\alpha,\beta,\delta\}\) for CE, GCE and ANL-CE respectively. We use the best parameters \(\{1\times 10^{-3}\}\), \(\{0.5,1\times 10^{-4}\}\), \(\{0.5,1.0,1\times 10^{-6}\}\) for each method in our experiments. The results are reported in Table 10.

Moreover, we experiment with NCE+RCE on this dataset and tune the parameters \(\{\alpha,\beta,\delta\}\), but we find that the performance is very poor for some unknown reason. The best test accuracy we achieve is 28.28% with \(\{10.0,0.1,5\times 10^{-6}\}\). Since this result is too low and inconsistent with its performance on other datasets, we do not include it in the table for comparison.

**Clothing-1M**[22] is a large-scale clothing dataset contains 14 categories and 1 million training samples with nearly 40% mislabeled samples.

We follow the experimental setting in previous works [31]. We use the \(14k\) and \(10k\) clean data for validation and test, respectively, and we do not use the \(50k\) clean training data. We use ResNet-50 pre-trained on ImageNet. For preprocessing, we resized the images to \(256\times 256\), performed mean subtraction, and cropped the middle \(224\times 224\). We use SGD with a momentum of 0.9, a weight decay of \(1\times 10^{-3}\), and batch size of 32. We train the network for 10 epochs with learning rate \(1\times 10^{-3}\) and \(1\times 10^{-4}\) for 5 epochs each. Typical data augmentations including random horizontal flip are applied.

We compare our ANL-CE with CE, GCE and NCE+RCE. In this experiment, we use L2 regularization (weight decay) for ANL-CE and set the coefficient the same as those for CE, GCE and NCE+RCE. We tune the parameters \(\{q\}\), \(\alpha,\beta\) and \(\{\alpha,\beta\}\) for GCE, NCE+RCE and ANL-CE respectively. We use the best parameters \(\{0.6\}\), \(\{10.0,1.0\}\) and \(\{5.0,0.1\}\) for each method in our experiments. The results are reported in Table 11.

\begin{table}
\begin{tabular}{c|c c c} \hline Methods & CE & GCE & ANL-CE (ours) \\ \hline Test Acc. (\%) & 78.92\(\pm\)0.76 & 80.39\(\pm\)0.17 & **80.72\(\pm\)0.37** \\ \hline \end{tabular}
\end{table}
Table 11: Test accuracies (%) of different methods on Clothing-1M dataset. The top-1 best results are **boldfaced**.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline Methods & Clean & Aggregate & Random 1 & Random 2 & Random 3 & Worst \\ \hline CE & 90.38\(\pm\)0.11 & 85.09\(\pm\)0.30 & 79.09\(\pm\)0.28 & 78.59\(\pm\)0.42 & 78.39\(\pm\)0.50 & 61.43\(\pm\)0.52 \\ GCE & 89.66\(\pm\)0.20 & 87.38\(\pm\)0.07 & 85.87\(\pm\)0.27 & 85.43\(\pm\)0.13 & 85.51\(\pm\)0.15 & 75.19\(\pm\)0.23 \\ SCE & 91.38\(\pm\)0.12 & 88.48\(\pm\)0.26 & 85.65\(\pm\)0.30 & 85.71\(\pm\)0.19 & 85.87\(\pm\)0.13 & 73.65\(\pm\)0.29 \\ NCE+RCE & 90.94\(\pm\)0.01 & 89.17\(\pm\)0.28 & 87.62\(\pm\)0.34 & **87.66\(\pm\)0.12** & **87.70\(\pm\)0.18** & 79.74\(\pm\)0.09 \\ NCE+AGCE & 91.08\(\pm\)0.06 & **89.27\(\pm\)0.28** & **87.92\(\pm\)0.02** & 87.61\(\pm\)0.20 & 87.62\(\pm\)0.16 & **79.91\(\pm\)0.37** \\ ANL-CE (ours) & 91.66\(\pm\)0.04 & **89.66\(\pm\)0.12** & **88.68\(\pm\)0.13** & **88.19\(\pm\)0.08** & **88.24\(\pm\)0.15** & **80.23\(\pm\)0.28** \\ \hline \end{tabular}
\end{table}
Table 8: Test accuracies (%) of different methods on CIFAR-10N dataset. The results (mean\(\pm\)std) are reported over 3 random runs under different random seeds (\(1,2,3\)) and the top-2 best results are **boldfaced**.

\begin{table}
\begin{tabular}{c|c|c} \hline Methods & Clean & Noisy \\ \hline CE & 71.14\(\pm\)0.38 & 48.63\(\pm\)0.53 \\ GCE & 61.62\(\pm\)0.43 & 50.97\(\pm\)0.60 \\ SCE & 70.80\(\pm\)0.37 & 48.52\(\pm\)0.11 \\ NCE+RCE & 68.22\(\pm\)0.28 & 54.27\(\pm\)0.09 \\ NCE+AGCE & 68.61\(\pm\)0.12 & **55.96\(\pm\)0.20** \\ ANL-CE (ours) & 70.68\(\pm\)0.23 & **56.37\(\pm\)0.42** \\ \hline \end{tabular}
\end{table}
Table 9: Test accuracies (%) of different methods on CIFAR-100N dataset. The results (mean\(\pm\)std) are reported over 3 random runs under different random seeds (\(1,2,3\)) and the top-2 best results are **boldfaced**.