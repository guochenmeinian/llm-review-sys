# Sparse maximal update parameterization: A holistic approach to sparse training dynamics

 Nolan Dey  Shane Bergsma  Joel Hestness

Cerebras Systems

{nolan,joel}@cerebras.net

###### Abstract

Several challenges make it difficult for sparse neural networks to compete with dense models. First, setting a large fraction of weights to zero impairs forward and gradient signal propagation. Second, sparse studies often need to test multiple sparsity levels, while also introducing new hyperparameters (HPs), leading to prohibitive tuning costs. Indeed, the standard practice is to re-use the learning HPs originally crafted for dense models. Unfortunately, we show sparse and dense networks do not share the same optimal HPs. Without stable dynamics and effective training recipes, it is costly to test sparsity at scale, which is key to surpassing dense networks and making the business case for sparsity acceleration in hardware. A holistic approach is needed to tackle these challenges and we propose sparse maximal update parameterization (SuPar) as one such approach. For random unstructured static sparsity, SuPar ensures activations, gradients, and weight updates all scale independently of sparsity level. Further, by reparameterizing the HPs, SuPar enables the same HP values to be optimal as we vary both sparsity level and model width. HPs can be tuned on small dense networks and transferred to large sparse models, greatly reducing tuning costs. On large-scale language modeling, SuPar shows increasing improvements over standard parameterization as sparsity increases, leading up to 11.9% relative loss improvement at 99.2% sparsity. A minimal implementation of SuPar is available at https://github.com/EleutherAI/nanoGPT-mup/tree/super.

## 1 Intro

_Sparsity_ has emerged as a key technique to mitigate the increasing computational costs of training and inference in deep neural networks. This work focuses on _weight sparsity_, whereby a significant fraction of model weights are kept at zero. It has long been known that dense neural networks can be heavily pruned _after_ training [30]. With the goal of reducing costs _during_ training, recent work has explored static weight sparsity from initialization. In this work we focus on random unstructured static sparsity, which has re-emerged as a surprisingly effective strategy [33, 58]. This type of sparsity can be accelerated by CPUs, Cerebras, Graphcore, and SambaNova. Furthermore, GPUs and TPUs support 2:4 block structured sparsity which is quite similar to 50% unstructured sparsity.

Unfortunately, several challenges have hindered progress in weight-sparse neural networks. First, sparsity impairs signal propagation during training [31, 11, 1]. Second, with today's techniques, sparse training is costly. Sparse techniques typically introduce extra hyperparameters (HPs), e.g., number of pruning iterations at initialization [60, 7, 56], and it is common to train models across different sparsity levels. Since tuning should be performed at each level and the search space grows exponentially with the number of HPs, the tuning costs essentially "defeat the purpose" of sparsity, i.e., to _reduce_ computation [60]. Finally, today there is only a nascent ecosystem of hardware acceleration for unstructured sparsity, so most researchers get little sparsity benefit when tuning.

These costs have led to the standard practice of _simply re-using HPs that were previously optimized for the baseline dense models_ (Section 2). One might hope that sparse models thrive with the same learning rates and other HPs as their dense counterparts. Unfortunately, they do not: optimal HPs _systematically_ vary with sparsity level (Figure 2, left). With impaired training dynamics, prohibitive tuning cost, and lacking the established training recipes enjoyed by dense models, it is often inefficient to train sparse networks at scale (Figure 2).

To remedy this situation, we propose sparse maximal update parameterization (SuPar, pronounced "soo-pahr"), a novel, holistic approach to stabilize sparse training dynamics. SuPar fulfills the Feature Learning Desiderata (Section 3) by parameterizing weight initialization and learning rates with respect to change in width _and_ sparsity level. As a generalization of maximal update parameterization (uP) [64; 63], SuPar enjoys well-controlled activation, gradient, and weight update scales in expectation, avoiding exploding or vanishing signal when changing both sparsity and model width.

By reparameterizing HPs in this way, SuPar enables the same HP values to be optimal as sparsity varies (Figure 2, right). We therefore enjoy uTransfer: we can tune small proxy models and transfer optimal HPs directly to models at scale. In fact, we discovered our uP HPs, tuned for dense models in prior work (and equivalent to SuPar with sparsity=0%), correspond to the optimal learning rate and initial weight variance for _all_ sparse models tuned in this paper! As sparsity increases, our formulation shows the standard parameterization (SP) and uP suffer from vanishing signal, further clarifying prior observations of gradient flow issues in sparse networks. The improvements enabled by SuPar set the Pareto-frontier best loss across sparsity levels. Figure 3 previews this improvement for large language models trained from compute-optimal configurations [23]. Here, SuPar benefits grow with increasing sparsity, to 11.9% better loss than SP and 1.9% better loss than uP at 99.2% random unstructured sparsity. See Section 4.3 for details on this experiment.

## 2 Related work

Sparse training landscapeSparse training can be divided into static sparsity, where the connectivity is fixed (our focus) and dynamic sparsity, where the sparsity mask can evolve [22]. We use _unstructured_ sparsity, though our approach generalizes to structured approaches where a particular sparsity pattern increases efficiency on specific hardware [67; 26; 38; 14; 29; 1]. Unstructured connectivity may be based on both random pruning [40; 18; 57; 33; 58] and various pruning-at-initialization criteria [32; 60; 61; 56; 7]. Liu et al. [33] found that as models scale, the relative performance of randomly pruned networks grow. Furthermore, Frantar et al. [15] found the optimal level of sparsity increases with the amount of training data. Together, these findings suggest that as neural networkscontinue to get wider and deeper, and trained on more and more data, very sparse randomly-pruned networks may emerge as an attractive option.

Improving sparse training dynamicsMany prior works identify various sparse training dynamics issues. In particular, prior works note sparsity impacts weight initialization [35; 31; 49; 11], activation variance [29], gradient flow [61; 37; 57; 11; 1], and step sizes during weight updates [15]. These prior works each only address a subset of these issues in targeted ways, often showing benefits to sparse model training loss. We advocate for a holistic approach, and discuss the relationship between these prior works and our approach in Section 5 after describing and evaluating \(\mathsf{S\upmu Par}\).

Sparse sensitivity to HPsDue to the costs of training with fixed weight sparsity, re-using dense HPs is standard practice. Such re-use is typically indicated in appendices or supplemental materials, e.g., [40; 32; 35; 31; 16; 60; 61; 56; 13; 7; 18; 57; 33; 58]. Also, dynamic sparsity approaches often compare to fixed sparsity; these baselines are likewise reported to re-use the dense HPs [2; 41; 10; 34; 11; 59]. However, some prior work has suggested such training is sensitive to HPs, e.g., learning rates [35; 57], learning rate schedules [16], or training length [28], although systematic tuning was not performed. For dynamic sparse training (DST), it is also conventional to re-use dense HPs, whether in dense-to-sparse [37; 15] or sparse-to-sparse (evolving mask) training [2; 8; 34; 11; 59]. As with fixed sparsity, work here has also suggested sensitivity to HPs, e.g., to dropout and label smoothing [16]. DST may also benefit from extra training steps [10] or smaller batch sizes [34], although in DST this may mainly be due to a greater number of opportunities for connectivity exploration [34].

## 3 Sparse maximal update parameterization (\(\mathsf{S\upmu Par}\))

We now provide background, motivation, and derivation for \(\mathsf{S\upmu Par}\), first introducing notation (Section 3.1) and then defining Feature Learning Desiderata (Section 3.2) with a brief overview of \(\mathsf{\upmu P}\) (Section 3.3). Finally we motivate \(\mathsf{S\upmu Par}\) and provide an overview of the parameterization (Section 3.4).

### Notation

The operations for a single sparse training step are illustrated in Figure 4. The definition and dimensions are: batch size \(B\), learning rate \(\eta\), loss function \(\mathcal{L}\), forward pass function \(\mathcal{F}\), input dimension \(d_{\text{in}}\), input activations \(\mathbf{X}\in\mathbb{R}^{B\times d_{\text{in}}}\), input activation gradient \(\frac{\partial\mathcal{L}}{\partial\mathbf{X}}=\nabla_{\mathbf{X}}\mathcal{L} \in\mathbb{R}^{B\times d_{\text{in}}}\), output dimension \(d_{\text{out}}\), output activations \(\mathbf{Y}\in\mathbb{R}^{B\times d_{\text{out}}}\), output activation gradient \(\frac{\partial\mathcal{L}}{\partial\mathbf{Y}}=\nabla_{\mathbf{Y}}\mathcal{L} \in\mathbb{R}^{B\times d_{\text{out}}}\), weights \(\mathbf{W}\in\mathbb{R}^{d_{\text{in}}\times d_{\text{out}}}\), initialization variance \(\sigma_{W}\) for weights \(\mathbf{W}\), weight update \(\Delta\mathbf{W}\in\mathbb{R}^{d_{\text{in}}\times d_{\text{out}}}\), and \(\Delta\mathbf{Y}\in\mathbb{R}^{B\times d_{\text{out}}}\) is the effect of the weight update on output activations: \(\Delta\mathbf{Y}=\mathbf{X}(\Delta\mathbf{W}\odot\mathbf{M})\). Unless otherwise specified, \(\mathbf{M}\in\{0,1\}^{d_{\text{in}}\times d_{\text{out}}}\) is an unstructured random static mask with sparsity \(s\) and density \(\rho=1-s\). When changing model scale or sparsity, we refer to a width multiplier \(m_{d}=\frac{d_{\text{in}}}{d_{\text{in},\text{base}}}=\frac{d_{\text{out}}}{d_ {\text{out},\text{base}}}\) and density multiplier \(m_{\rho}=\frac{\rho}{\rho_{\text{base}}}\).

If we apply sparsity to a linear layer (i.e., \(\mathcal{F}\) is a fully-connected layer), our aim is to control:

1. **Forward pass:**\(\mathbf{Y}=\mathcal{F}(\mathbf{X},\mathbf{W}\odot\mathbf{M})=\mathbf{X}( \mathbf{W}\odot\mathbf{M})\).

Figure 4: The three operations associated with training a layer with weights that perform the function \(\mathcal{F}\): Forward activation calculation, backward gradient propagation, and the weight update.

2. **Backward pass:**\(\nabla_{\mathbf{X}}\mathcal{L}=(\nabla_{\mathbf{Y}}\mathcal{L})\cdot(\mathbf{W} \odot\mathbf{M})^{\top}\).
3. **Effect of weight update \(\Delta\mathbf{W}\) on \(\mathbf{Y}\):**\(\Delta\mathbf{Y}=\mathbf{X}(\Delta\mathbf{W}\odot\mathbf{M})\)1. Footnote 1: After a weight update \(\Delta\mathbf{W}\) is applied, new output activations can be written as \(\mathbf{Y}+\Delta\mathbf{Y}=\mathbf{X}(\mathbf{W}\odot\mathbf{M})+\mathbf{X} (\Delta\mathbf{W}\odot\mathbf{M})\). Our goal is to control \(\Delta\mathbf{Y}\).

### Feature learning: Defining the goal of \(\mathbf{\mu}\)P and \(\mathbf{\mu}\)Par

Prior works [64; 63; 65] introduce the Feature Learning Desiderata (FLD) to ensure stable training dynamics as width is varied. Building on prior works, we include gradients \(\nabla_{\mathbf{X}}\mathcal{L}\) in the desiderata.

**Feature Learning Desiderata (FLD):** For layer \(l\) and token \(i\), we desire that \(\|\mathbf{Y}_{i}^{l}\|_{2}=\Theta(\sqrt{d_{\text{out}}}),\|\nabla_{\mathbf{X}} \mathcal{L}_{i}^{l}\|_{2}=\Theta(\sqrt{d_{\text{in}}}),\|\Delta\mathbf{Y}_{i} ^{l}\|_{2}=\Theta(\sqrt{d_{\text{out}}}),\forall i,\forall l\).

Recall that if all the entries of some vector \(\mathbf{v}\in\mathbb{R}^{n}\) are some constant \(c\), then \(\|\mathbf{v}\|_{2}=\Theta(\sqrt{n})\) with respect to width \(n\). Therefore we can satisfy the FLD by ensuring the _typical element size_ of \(\mathbf{Y}\), \(\nabla_{\mathbf{X}}\mathcal{L}\), and \(\Delta\mathbf{Y}\) is \(\Theta(1)\) with respect to **some variable(s)** we would like to scale. Variables to scale include width [64; 63; 65], depth [66; 4], and sparsity (this work). The FLD prescribes a **holistic** signal propagation approach of controlling each of the three operations in a training step, not a subset2.

Footnote 2: For example, initialization methods alone can only control \(\|\mathbf{Y}\|_{F}\) and \(\|\nabla_{\mathbf{X}}\mathcal{L}\|_{F}\) at the first time step.

### Maximal update parameterization (\(\mathbf{\mu}\)P)

Here we provide a brief overview of maximal update parameterization (\(\mathbf{\mu}\)P) [64; 63; 65]. With the standard parameterization (SP), Yang and Hu [64] show the scale of activations throughout training increases as model width increases, motivating the development of \(\mathbf{\mu}\)P. \(\mathbf{\mu}\)P [64; 63] is defined as the unique parameterization that satisfies the FLD by ensuring the _typical element size_ of \(\mathbf{Y}\), \(\nabla_{\mathbf{X}}\mathcal{L}\), and \(\Delta\mathbf{Y}\) is \(\Theta(1)\)**with respect to change in width \(m_{d}\). The FLD can also be satisfied by controlling the spectral norm of weights [65]. \(\mathbf{\mu}\)P enables \(\mathbf{\mu}\)Transfer: the optimum learning rate, initialization weight variance, scalar multipliers, and learning rate schedule all remain consistent as width is increased for \(\mathbf{\mu}\)P models [63]. \(\mathbf{\mu}\)Transfer can be leveraged to take a _tune small, train large_ approach where hyperparameters are extensively tuned for a small model then transferred, enabling reduced tuning budgets and superior tuning for large models compared to standard practice.

### Sparse maximal update parameterization (\(\mathbf{\mu}\)Par)

Yang et al. [63] show activation magnitudes explode with increasing model width. In Figure 5 we show sparsity has the opposite effect: increasing sparsity causes shrinking activation magnitudes.

Figure 5: Mean absolute value activations for attention and feed forward blocks after training step \(t\) (10 seeds). In SP and \(\mathbf{\mu}\)P models, decreasing density causes activations to vanish (note axes on log-scale). In \(\mathbf{\mu}\)Par models, density has little effect on activation scales and there is no vanishing.

SuPar is defined as the unique parameterization that satisfies the FLD by ensuring the _typical element size_ of \(\mathbf{Y}\), \(\nabla_{\mathbf{X}}\mathcal{L}\), and \(\Delta\mathbf{Y}\) is \(\Theta(1)\)**with respect to change in width \(m_{d}\) and change in density \(m_{\rho}\)**. SuPar enables stable activation scales across sparsity levels (Figure 5, right). In this section, we walk through the changes required to control each of the three operations in a sparse training step, providing an overview of the SuPar derivation. We focus on the AdamW [36] optimizer used in our experiments. For a more detailed derivation, including both SGD and Adam, see Appendix D.

Forward pass at initializationTo ensure the _typical element size_ of \(\mathbf{Y}\) is \(\Theta(1)\) with respect to change in width \(m_{d_{\text{in}}}\) and change in density \(m_{\rho}\), we can control the mean and variance of \(\mathbf{Y}_{ij}\). Since at initialization \(\mathbb{E}[\mathbf{W}]=0\), \(\mathbb{E}[\mathbf{Y}]=0\), and \(\mathbf{W}\perp\mathbf{Y}\), the mean is controlled. The variance of \(\mathbf{Y}_{ij}\) can be written as:

\[\mathrm{Var}(\mathbf{Y}_{ij})=m_{d_{\text{in,base}}}m_{\rho}\rho_{\text{base} }\sigma_{W}^{2}(\mathrm{Var}(\mathbf{X})+\mathbb{E}[\mathbf{X}]^{2})\] (1)

To ensure \(\mathrm{Var}(\mathbf{Y}_{ij})\) scales independent of \(m_{d_{\text{in}}}\) and \(m_{\rho}\), we choose \(\sigma_{\mathbf{W}}^{2}=\frac{\sigma_{\mathbf{W},\text{base}}^{2}}{m_{d_{ \text{in}}}m_{\rho}}\).

Backward gradient pass at initializationTo ensure the _typical element size_ of \(\nabla_{\mathbf{X}}\mathcal{L}\) is \(\Theta(1)\) with respect to change in width \(m_{d_{\text{out}}}\) and change in density \(m_{\rho}\), we can control the mean and variance of \(\nabla_{\mathbf{X}}\mathcal{L}\). Since at initialization \(\mathbb{E}[\mathbf{W}]=0\), \(\mathbb{E}[\nabla_{\mathbf{X}}\mathcal{L}]=0\) and the mean is controlled3. The variance of \(\nabla_{\mathbf{X}}\mathcal{L}_{ij}\) can be written as:

Footnote 3: Although the gradients \(\nabla_{\mathbf{Y}}\mathcal{L}\) will have some correlation with weights \(\mathbf{W}\) even at initialization, we assume for simplicity that they are fully independent. Future work could investigate this assumption more deeply.

\[\mathrm{Var}(\nabla_{\mathbf{X}}\mathcal{L}_{ij})=m_{d_{\text{out}}}d_{\text{ out,base}}m_{\rho}\rho_{\text{base}}\sigma_{\mathbf{W}}^{2}\mathrm{Var}(\nabla_{ \mathbf{Y}}\mathcal{L})\] (2)

To ensure \(\mathrm{Var}(\nabla_{\mathbf{X}}\mathcal{L}_{ij})\) scales independent of \(m_{d_{\text{out}}}\) and \(m_{\rho}\), we choose \(\sigma_{\mathbf{W}}^{2}=\frac{\sigma_{\mathbf{W},\text{base}}^{2}}{m_{d_{ \text{out}}}m_{\rho}}\). Typically \(m_{d_{\text{out}}}=m_{d_{\text{in}}}\), allowing the same \(\sigma_{\mathbf{W}}^{2}\) to control both forward and backward scales.

Effect of Adam weight update \(\Delta\mathbf{W}\) on \(\mathbf{Y}\)To ensure the _typical element size_ of \(\Delta\mathbf{Y}\) is \(\Theta(1)\) with respect to change in width \(m_{d_{\text{out}}}\) and change in density \(m_{\rho}\). By the law of large numbers, the expected size of each element can be written as:

\[\mathbb{E}[\Delta\mathbf{Y}_{ij}]\rightarrow\eta m_{d_{\text{in,base}}}m_{ \rho}\rho_{\text{base}}\mathbb{E}\left[\mathbf{X}_{ik}\left(\frac{\sum_{t}^{T }\gamma_{t}\sum_{b}^{B}\mathbf{X}_{ik}^{t}\nabla_{\mathbf{Y}}\mathcal{L}_{bj} ^{t}}{\sqrt{\sum_{t}^{T}\omega_{t}\sum_{b}^{B}(\mathbf{X}_{ik}^{t}\nabla_{ \mathbf{Y}}\mathcal{L}_{bj}^{t})^{2}}}\right)\right],\text{ as }(d_{\text{in}}\rho)\rightarrow\infty\] (3)

To ensure \(\Delta\mathbf{Y}_{ij}\) and \(\|\Delta\mathbf{Y}\|_{F}\) are scale invariant to \(m_{d_{\text{in}}},m_{\rho}\), we choose \(\eta=\frac{\eta_{\text{base}}}{m_{d_{\text{in}}}m_{\rho}}\).

Implementation summaryTable 1 summarizes the differences between SP, uP, and SuPar. Since we only sparsify hidden weights, SuPar matches uP for input, output, bias, layer-norm, and attention logits. Also note width multipliers \(m_{d}\) and density multipliers \(m_{\rho}\) are usually the same for all layers, allowing simplified notation. This correction is equivalent to uP [63] when \(\rho=1\) and \(m_{\rho}=1\). The correction to hidden weight initialization we derive is similar to the sparsity-aware initialization in prior work [35; 49; 11]. SuPar should also easily extend to 2:4 sparsity patterns because, in expectation, the rows and columns of \(M^{l}\) should have equal density. A minimal implementation of SuPar is available at https://github.com/EleutherAI/nanoGPT-mup/tree/supar.

## 4 SuPar Training Results

Here, we present empirical results showing the effectiveness of SuPar over SP and uP when training sparse models. When using SP or uP, optimal HPs drift as we change the sparsity level, possibly leading to inconclusive or even reversed findings. SuPar has stable optimal HPs across both model width and sparsity level, and we show it improves over SP and uP across different scaling approaches. Taken together, we see that SuPar sets the Pareto frontier best loss across all sparsities and widths,

[MISSING_PAGE_FAIL:6]

### Studying SupPar indicates how some sparse scaling techniques appear to work

So far, we see SupPar can transfer optimal HPs across sparsity levels, but we have also designed it to transfer HPs across different model widths (hidden sizes), similar to up. Here, we further demonstrate that SupPar transfers optimal HPs across width. More generally, sparse scaling that keeps a fixed number of non-zero weights per neuron allows SP and up to also transfer HPs.

Figure 9 shows learning rate transfer tests when changing both the model's hidden size, \(d_{\text{model}}\), and sparsity level in a common scaling approach called _Iso-Parameter scaling_[18, 10, 59]. Iso-Parameter scaling keeps the model's number of non-zero parameters approximately the same, as width and sparsity are varied5. Here, we see the common result that SP models starting from dense HPs _do_ tend to significantly improve as we increase width and sparsity. Note, though, the optimal learning rate for each sparsity level still shifts. When we correct dense HPs using up or SupPar, the dense baseline significantly improves, but only SupPar shows consistent loss improvement and stable HPs.

Footnote 5: Not perfectly Iso-Parameter due to unsparsified layers (embedding, bias, layer-norm, etc.)

Based on the SupPar formulation: When the number of non-zero weights per neuron (WPN) in the network is the same, up and SupPar become synonymous, because initialization and learning rate adjustment factors will be constant (i.e., \(d_{\text{model}}\cdot\rho=\text{WPN}=O(1)\)). Optimized SP HPs will also tend to work well. We define this new scaling setting, which we call Iso-WPN, to verify this hypothesis. In Figure 11, we test SP HPs with Iso-WPN scaling and see the optimal learning rate stays consistently between \(2^{-7}\) and \(2^{-6}\) with roughly aligned curves (we omit similar up and SupPar plots for space, because their corrections are the same). The conclusion is that when scaling SP models in an Iso-WPN sparse setting, HPs should maintain similar training dynamics. More generally, as WPN decreases

Figure 8: Summarizing loss results from Figure 6 with the optimal learning rate for each parameterization and sparsity.

Figure 7: Across sparsity \(s\), SP and up show unstable optimal initialization. SupPar is stable (3 seeds).

Figure 9: SupPar ensures stable optimal learning rate in Iso-Parameter sparse + wide scaling (3 seeds).

(e.g., by increasing sparsity), the optimal learning rate will tend to increase proportionally, and vice versa6.

Footnote 6: Our results generalize the Yang et al. finding that optimal LR decreases as width increases [63, Figure 1].

Figures 5, 6, 7, and 9 show SuPar is the only parameterization that ensures stable activation scales and stable optimal HPs across model widths and sparsities, satisfying the FLD.

### SuPar scaling to large language model pretraining

We conclude this section reflecting on the demonstration of SuPar improvements in a large-scale language model. We train 610M parameter models starting from a Chinchilla [23] compute-optimal training configuration with 20 tokens per parameter from the SlimPajama dataset. This larger model--with hidden size 2048, 10 layers, and attention head size 64--permits sweeping over a larger range of sparsity levels, so we test up to 99.2% sparsity (density \(2^{-7}\)).

Figure 3 shows validation loss for each parameterization as we sweep sparsity levels. Additionally, in Table 2, we evaluate the models from Figure 3 on five downstream tasks: ARC-easy, lambada, RACE, PIQA, and BoolQ, which collectively test for common sense reasoning, world knowledge, and reading comprehension. As sparsity increases, results across pretraining loss and average downstream task accuracy consistently show SP and uP fall farther behind SuPar. Since these models are trained with a large number of tokens, we attribute the widening loss gap mostly to increasingly under-tuned learning rates for SP and uP as sparsity increases-the weight updates lose gradient information throughout training. Figure 8 shows retuning SP and uP could recover some of the gap to SuPar, but that could be costly: These runs take 3-6 hours on a Cerebras CS-3 system (or \(>9\) days on an NVIDIA A100 GPU).

Finally, returning to the Iso-Parameter scaling setting, Figure 10 shows losses for 111M parameter models trained on 1B tokens and scaled up while using dense optimal HPs. The SP and uP models experience detuning as sparsity increases, allowing SuPar to achieve superior losses7.

Footnote 7: Note this is not an Iso-FLOP comparison because increasing \(d_{\text{model}}\) also increases attention dot product and embedding FLOPs, which aren’t be sparsified. This is so significant that our 87.5% sparse model from Figure 10 has double the training FLOPs of the dense baseline, with virtually unchanged loss.

### Dynamic sparsity hyperparameter transfer

In Figure 12 we test the transfer of optimal learning rate across sparsity levels for two popular dynamic sparse training methods: Rigging the Lottery (RigL) [10]8 and Gradual Magnitude Pruning (GMP) [68]9. We show that none of SP, uP, or SuPar achieve transfer of optimal learning rate across sparsity levels. For SP and uP we see that higher sparsity levels have higher optimal learning rates.

This is because sparsity reduces activation and gradient scales such that a larger learning rate is needed to counteract this. SuPar sees the opposite trend where higher sparsity levels have lower optimal learning rates, indicating that SuPar is "overcorrecting".

Dynamic sparse methods can make updates to the weight mask such that the distribution of unmasked/non-zero weights changes to something non-Gaussian, which prevents SuPar from being correct in expectation. Compared to random pruning, a mask obtained from magnitude pruning will better preserve the size of activations and gradients seen in the dense network. Since SuPar assumes weights are drawn from a Gaussian distribution, SuPar ends up "overcorrecting" the initialization and learning rate. In future work it would be impactful to develop a parameterization which generalizes SuPar to work for an arbitrary sparse training algorithm.

## 5 Discussion

To improve sparse training, prior works make targeted corrections which arise from observations that sparsity can cause degraded activation, gradient, and/or weight update signal propagation. We review these observations and corrections to advocate for holistic control of sparse training dynamics.

Sparsifying Can Cause Vanishing ActivationsEvci et al. [11] note that by initializing weights using dense methods (e.g., [17; 21]), the "vast majority" of sparse networks have vanishing activations. Lasby et al. [29; App. A] analyze activation variance as a guide for selecting structured sparsity. The FLD suggest activation norms be measured and controlled with respect to sparsity, so activation variance can be considered a proxy to whether sparsity might negatively impact training dynamics. Evci et al. [11] ultimately initialize variances via neuron-specific sparse connectivity, while Liu et al. [35] and Ramanujan et al. [49] propose scaling weight variances proportional to layer sparsity. These corrections, however, only target controlling activations but not weight updates.

Gradient Flow Partially Measures the Weight Update \(\upmu\)DesideratumSparsity also impairs _gradient flow_--the magnitude of the gradient to the weights--during training [11; 1]. Since gradient flow is measured using the norm of the weight gradients, it measures a piece of the weight update. Unfortunately, gradient flow does not directly measure the effect of the weight update step, which can also involve adjustments for things like optimizer state (e.g., momentum and velocity), the learning rate, and weight decay. Prior works propose techniques to improve gradient flow during sparse

Figure 12: For dynamic sparse training methods RigL and GMP, none of SP, \(\upmu\)P, or SuPar achieve stable optimal learning rate across sparsity (3 seeds). Missing points indicate diverged training runs.

training and pruning by adjusting individual hyperparameters or adding normalization [61, 37, 11, 1]. However, these techniques might overlook the effects of the optimizer and learning rates in weight updates. Notably, Tessera et al. [57]_do_ consider some of these effects, but their proposed techniques maintain gradient flow only in the Iso-Parameter scaling setting rather than arbitrary sparsification.

Frantar et al. [15, App. A.1] also endeavor to control weight updates, where they observe diminished step sizes when optimizing sparse networks with Adafactor [52]. They correct this by computing Adafactor's root-mean-square scaling adjustments over _unpruned_ weights and updates. However, such normalization does not prevent activations from scaling with model width [63, 65]. In this sense, sparsity-aware fixes to Adafactor can improve dynamics, but will not address instability holistically. In Figure 14 we show the \(\mathsf{S\upmu Par}\) LR correction alone is not even sufficient to achieve stable optimal \(\eta\).

Weight Initialization Only Controls Dynamics at InitializationWe noted works above that adjust sparse weight initializations [11, 35, 49]. Additionally, Lee et al. [31] explore orthogonal weight initialization [46], both before pruning (to ensure SNIP [32] pruning scores are on a similar scale across layers) and after (to improve trainability of the sparse network). While adjusting weights can improve sparse training dynamics at initialization, such adjustments are insufficient to stabilize signals _after multiple steps of training_, in the same way that standard weight initializations fail to stabilize training of dense networks. In Figure 13 we show the \(\mathsf{S\upmu Par}\) init. alone is not even sufficient to achieve stable optimal \(\sigma_{W}\).

## 6 Limitations

As Section 4.4 shows, \(\mathsf{S\upmu Par}\) requires further extension for dynamic sparse training due to unpredictable changes in weight distributions. The same applies to methods which prune at initialization or after pretraining in a non-random fashion. Iterative magnitude pruning (IMP) is an interesting case since it involves rewinding weights back to their initial values while maintaining the same mask [12]. If the IMP mask at initialization still allows the non-zero weights to have a Gaussian distribution, then \(\mathsf{S\upmu Par}\) would apply to this case. Therefore, it's possible \(\mathsf{S\upmu Par}\)_could_ prevent "HP detuning" in later IMP iterations, and _potentially_ improve IMP losses, though we do not explore this. \(\mathsf{S\upmu Par}\) would also work for random structured pruning of entire neurons at initialization because this case simply reduces to training with a narrower dense model.

For weight sparsity more generally, the most pressing limitation is the lack of hardware acceleration [38]. While new software [50, 29, 43] continues to better leverage existing hardware, the growth of software and hardware co-design is also encouraging [59, 6], as effective sparsity techniques can be specifically optimized in deep learning hardware. But to effectively plan hardware, we need to train and test sparse prototypes at next-level sizes, at scales where the optimum sparsity level may be higher than in current networks [15]. Performing such _scaling law_-style studies requires incredible resources even for dense models with well-established training recipes [27, 23]. As \(\mathsf{S\upmu Par}\) reduces training and tuning costs, it can help unlock these studies and guide future hardware design.

Finally, the scaling factors for the weight update need to be derived for each optimizer, which might limit the usability of \(\mathsf{S\upmu Par}\) in practice. For a discussion of the broader impacts of \(\mathsf{S\upmu Par}\), see Appendix A.

## 7 Conclusion

Nobody said training with sparsity was easy. We showed that with the standard parameterization and \(\mathsf{\upmu P}\), increasing sparsity level directly correlates with vanishing activations. Impaired training dynamics prevent sparse models from sharing the same optimal hyperparameters, suggesting prior results based on re-use of dense HPs merit re-examination. In contrast, by holistically controlling the training process, \(\mathsf{S\upmu Par}\) prevents vanishing activations and enables HP transfer (across both width and sparsity). LLMs trained with \(\mathsf{S\upmu Par}\) improve over \(\mathsf{\upmu P}\) and the standard parameterization. As such, we hope \(\mathsf{S\upmu Par}\) makes things a little easier for sparsity research going forward.

## Acknowledgements

We would like to thank Gavia Gray, who provided helpful feedback on the manuscript, and Gurpreet Gosal, who tuned the \(\mu\)Transferred hyperparameters seen throughout the document.

## References

* [1]A. Rajeshkumar Bambhaniya, A. Yazdanbakhsh, S. Subramanian, S. Kao, S. Agrawal, U. Evici, and T. Krishna (2024) Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers. arXiv preprint arXiv:2402.04744. Cited by: SS1.
* [2]G. Bellec, D. Kappel, W. Maass, and R. Legenstein (2017) Deep rewiring: training very sparse deep networks. arXiv preprint arXiv:1711.05136. Cited by: SS1.
* [3]E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell (2021) On the dangers of stochastic parrots: can language models be too big?. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pp. 610-623. Cited by: SS1.
* [4]B. Bordelon, L. Noci, M. B. Li, B. Hanin, and C. Pehlevan (2024) Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit. In The Twelfth International Conference on Learning Representations, External Links: Link Cited by: SS1.
* [5]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in Neural Information Processing Systems33, pp. 1877-1901. Cited by: SS1.
* [6]C. Systems (2024) Train a Model with Weight Sparsity. Cerebras Systems Documentation. External Links: Link Cited by: SS1.
* [7]P. de Jorge, A. Sanyal, H. S. Behl, P. H. Torr, G. Rogez, and P. K. Dokania (2020) Progressive skeletonization: Trimming more fat from a network at initialization. arXiv preprint arXiv:2006.09081. Cited by: SS1.
* [8]T. Dettmers and L. Zettlemoyer (2019) Sparse networks from scratch: faster training without losing performance. arXiv preprint arXiv:1907.04840. Cited by: SS1.
* [9]N. Dey, D. Soboleva, F. Al-Khateeb, B. Yang, R. Pathria, H. Khachane, S. Muhammad, Z. Chen, R. Myers, J. Robert Steeves, N. Vassilieva, M. Tom, and J. Hestness (2023) BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model. External Links: 2309.11568 Cited by: SS1.
* [10]U. Evci, T. Gale, J. Menick, P. Samuel Castro, and E. Elsen (2020) Rigging the lottery: making all tickets winners. In International conference on machine learning, pp. 2943-2952. Cited by: SS1.
* [11]U. Evci, Y. Ioannou, C. Keskin, and Y. Dauphin (2022) Gradient flow in sparse neural networks and how lottery tickets win. In Proceedings of the AAAI conference on artificial intelligence, Vol. 36, pp. 6577-6586. Cited by: SS1.
* [12]J. Frankle and M. Carbin (2018) The lottery ticket hypothesis: finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635. Cited by: SS1.
* [13]J. Frankle, G. Dziugaite, D. M. Roy, and M. Carbin (2020) Pruning neural networks at initialization: why are we missing the mark?. arXiv preprint arXiv:2009.08576. Cited by: SS1.
* [14]E. Frantar and D. Alistarh (2023) SparseGPT: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pp. 10323-10337.

* [15] Elias Frantar, Carlos Riquelme, Neil Houlsby, Dan Alistarh, and Utku Evci. 2023. Scaling laws for sparsely-connected foundation models. _arXiv preprint arXiv:2309.08520_ (2023).
* [16] Trevor Gale, Erich Elsen, and Sara Hooker. 2019. The state of sparsity in deep neural networks. _arXiv preprint arXiv:1902.09574_ (2019).
* [17] Xavier Glorot and Yoshua Bengio. 2010. Understanding the Difficulty of Training Deep Feedforward Neural Networks. In _Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (PMLR)_.
* [18] Anna Golubeva, Behnam Neyshabur, and Guy Gur-Ari. 2020. Are wider nets better given the same number of parameters? _arXiv preprint arXiv:2010.14495_ (2020).
* [19] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. _arXiv preprint arXiv:1903.03862_ (2019).
* [20] Yiwen Guo, Chao Zhang, Changshui Zhang, and Yurong Chen. 2018. Sparse dnns with improved adversarial robustness. _Advances in neural information processing systems_ 31 (2018).
* [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. In _Proceedings of the IEEE international conference on computer vision_. 1026-1034.
* [22] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikolir Dryden, and Alexandra Peste. 2021. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. _Journal of Machine Learning Research_ 22, 241 (2021), 1-124.
* [23] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. 2022. An Empirical Analysis of Compute-optimal Large Language Model Training. In _The Conference on Neural Information Processing Systems (NeurIPS)_.
* [24] Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome. 2019. What do compressed deep neural networks forget? _arXiv preprint arXiv:1911.05248_ (2019).
* [25] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. 2020. Characterising bias in compressed models. _arXiv preprint arXiv:2010.03058_ (2020).
* [26] Hyeong-Ju Kang. 2019. Accelerator-aware pruning for convolutional neural networks. _IEEE Transactions on Circuits and Systems for Video Technology_ 30, 7 (2019), 2093-2103.
* [27] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling Laws for Neural Language Models. arXiv:2001.08361 [cs.LG] https://arxiv.org/abs/2001.08361
* [28] Denis Kuznedelev, Eldar Kurtic, Eugenia Iofinova, Elias Frantar, Alexandra Peste, and Dan Alistarh. 2023. Accurate neural network pruning requires rethinking sparse optimization. _arXiv preprint arXiv:2308.02060_ (2023).
* [29] Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, and Yani Ioannou. 2023. Dynamic Sparse Training with Structured Sparsity. _arXiv preprint arXiv:2305.02299_ (2023).
* [30] Yann LeCun, John Denker, and Sara Solla. 1989. Optimal brain damage. _Advances in Neural Information Processing Systems_ 2 (1989).
* [31] Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, and Philip HS Torr. 2019. A signal propagation perspective for pruning neural networks at initialization. _arXiv preprint arXiv:1906.06307_ (2019).
* [32] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. 2018. SNIP: Single-shot network pruning based on connection sensitivity. _arXiv preprint arXiv:1810.02340_ (2018).

* Liu et al. [2022] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, and Mykola Wang, Zhangyang an d Pechenizkiy. 2022. The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training. _arXiv preprint arXiv:2202.02643_ (2022).
* Liu et al. [2021] Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. 2021. Do we actually need dense over-parameterization? In-time over-parameterization in sparse training. In _International Conference on Machine Learning_. PMLR, 6989-7000.
* Liu et al. [2018] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. 2018. Rethinking the value of network pruning. _arXiv preprint arXiv:1810.05270_ (2018).
* Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. 2017. Decoupled Weight Decay Regularization. In _International Conference on Learning Representations_.
* Lubana and Dick [2020] Ekdeep Singh Lubana and Robert P Dick. 2020. A gradient flow framework for analyzing network pruning. _arXiv preprint arXiv:2009.11839_ (2020).
* Mishra et al. [2021] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. 2021. Accelerating sparse deep neural networks. _arXiv preprint arXiv:2104.08378_ (2021).
* Mitchell et al. [2019] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In _Proceedings of the conference on fairness, accountability, and transparency_. 220-229.
* Mocanu et al. [2016] Decebal Constantin Mocanu, Elena Mocanu, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. 2016. A topological insight into restricted Boltzmann machines. _Machine Learning_ 104 (2016), 243-270.
* Mocanu et al. [2018] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. 2018. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. _Nature communications_ 9, 1 (2018), 2383.
* Nadeem et al. [2020] Moin Nadeem, Anna Bethke, and Siva Reddy. 2020. StereoSet: Measuring stereotypical bias in pretrained language models. _arXiv preprint arXiv:2004.09456_ (2020).
* Magic [2024] Neural Magic. 2024. DeepSparse. GitHub repository. https://github.com/neuralmagic/deepsparse
* Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training Language Models to Follow Instructions With Human Feedback. arXiv:2203.02155 [cs.CL] https://arxiv.org/abs/2203.02155
* Patterson et al. [2021] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. _arXiv preprint arXiv:2104.10350_ (2021).
* Pennington et al. [2017] Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. 2017. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. _Advances in neural information processing systems_ 30 (2017).
* Press et al. [2022] Ofir Press, Noah Smith, and Mike Lewis. 2022. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In _International Conference on Learning Representations_.
* Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. https://d4mucfpksyuv.cloudfront.net/better-language-models/language-models.pdf
* Ramanujan et al. [2020] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Rastegari. 2020. What's hidden in a randomly weighted neural network?. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 11893-11902.

- Learning with 670k Labels on a Single Commodity GPU. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_. Springer, 689-704.
* Shazeer [2020] Noam Shazeer. 2020. GLU Variants Improve Transformer. arXiv:2002.05202 [cs.LG] https://arxiv.org/abs/2002.05202
* Shazeer and Stern [2018] Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In _International Conference on Machine Learning_. PMLR, 4596-4604.
* Sheng et al. [2019] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in language generation. _arXiv preprint arXiv:1909.01326_ (2019).
* Soboleva et al. [2023] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama. https://huggingface.co/datasets/cerebras/SlimPajama-627B
* Strubell et al. [2019] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. _arXiv preprint arXiv:1906.02243_ (2019).
* Tanaka et al. [2020] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. 2020. Pruning neural networks without any data by iteratively conserving synaptic flow. _Advances in neural information processing systems_ 33 (2020), 6377-6389.
* Tessera et al. [2021] Kale-ab Tessera, Sara Hooker, and Benjamin Rosman. 2021. Keep the gradients flowing: Using gradient flow to study sparse network optimization. _arXiv preprint arXiv:2102.01670_ (2021).
* Thangarasa et al. [2023] Vithursan Thangarasa, Abhay Gupta, William Marshall, Tianda Li, Kevin Leong, Dennis DeCoste, Sean Lie, and Shreyas Saxena. 2023. SPDF: Sparse pre-training and dense fine-tuning for large language models. In _Uncertainty in Artificial Intelligence_. 2134-2146.
* Thangarasa et al. [2023] Vithursan Thangarasa, Shreyas Saxena, Abhay Gupta, and Sean Lie. 2023. Sparse-IFT: Sparse Iso-FLOP transformations for maximizing training efficiency. _arXiv preprint arXiv:2303.11525_ (2023).
* Verdenius et al. [2020] Stijn Verdenius, Maarten Stol, and Patrick Forre. 2020. Pruning via iterative ranking of sensitivity statistics. _arXiv preprint arXiv:2006.00896_ (2020).
* Wang et al. [2020] Chaoqi Wang, Guodong Zhang, and Roger Grosse. 2020. Picking winning tickets before training by preserving gradient flow. _arXiv preprint arXiv:2002.07376_ (2020).
* Wei et al. [2023] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: How does LLM safety training fail? _Advances in Neural Information Processing Systems_ 36 (2023).
* Yang et al. [2021] Greg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. 2021. Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer. In _Advances in Neural Information Processing Systems_.
* Yang and Hu [2020] Greg Yang and Edward J Hu. 2020. Feature learning in infinite-width neural networks. _arXiv preprint arXiv:2011.14522_ (2020).
* Yang et al. [2023] Greg Yang, James B Simon, and Jeremy Bernstein. 2023. A spectral condition for feature learning. _arXiv preprint arXiv:2310.17813_ (2023).
* Yang et al. [2023] Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. 2023. Feature Learning in Infinite Depth Neural Networks. In _International Conference on Learning Representations_.
* Yao et al. [2019] Zhuliang Yao, Shijie Cao, Wencong Xiao, Chen Zhang, and Lanshun Nie. 2019. Balanced sparsity for efficient DNN inference on GPU. In _Proceedings of the AAAI conference on artificial intelligence_, Vol. 33. 5676-5683.
* Zhu and Gupta [2017] Michael Zhu and Suyog Gupta. 2017. To prune, or not to prune: exploring the efficacy of pruning for model compression. _arXiv preprint arXiv:1710.01878_ (2017).

[MISSING_PAGE_FAIL:15]

## Appendix D SuPar detailed derivation

### Forward pass at initialization

The first stage where we would like to control training dynamics is in the layer's forward function. For a random unstructured sparsity mask \(\mathbf{M}\), since each _column_ of \(\mathbf{M}\) has \(d_{\text{in}}\rho\) non-zero elements in expectation, we can rewrite the forward pass as:

\[\mathbf{Y}_{ij}=\left[\mathbf{X}(\mathbf{W}\odot\mathbf{M})\right]_{ij}=\sum_{ q=1}^{d_{\text{in}}}\mathbf{X}_{iq}(\mathbf{W}_{qj}\cdot\mathbf{M}_{qj})=\sum_{k: \mathbf{M}_{kj}=1}^{d_{\text{in}}\rho}\mathbf{X}_{ik}\mathbf{W}_{kj}\] (4)

To satisfy the FLD, we desire the _typical element size_ of \(\mathbf{Y}\) is \(\Theta(1)\) with respect to change in width \(m_{d_{\text{in}}}\) and change in density \(m_{\rho}\). To achieve this we can ensure the mean and variance of \(\mathbf{Y}_{ij}\) are invariant to \(m_{d_{\text{in}}}\) and \(m_{\rho}\).

**Mean:** As expectation is linear and \(\mathbf{X}\) and \(\mathbf{W}\) are independent at initialization:

\[\mathbb{E}[\mathbf{Y}_{ij}]=\mathbb{E}\left[\sum_{k:\mathbf{M}_{kj}=1}^{d_{ \text{in}}\rho}\mathbf{Y}_{ik}\mathbf{W}_{kj}\right]=\sum_{k:\mathbf{M}_{kj}=1 }^{d_{\text{in}}\rho}\mathbb{E}[\mathbf{X}_{ik}\mathbf{W}_{kj}]=\sum_{k: \mathbf{M}_{kj}=1}^{d_{\text{in}}\rho}\mathbb{E}[\mathbf{X}_{ik}]\mathbb{E}[ \mathbf{W}_{kj}]\] (5)

Therefore, since at initialization \(\mathbb{E}[\mathbf{W}_{ij}]=0\), \(\mathbb{E}[\mathbf{Y}_{ij}]=0\) and the mean is controlled.

**Variance:** As expectation is linear and each weight element is IID:

\[\mathrm{Var}(\mathbf{Y}_{ij})=\mathrm{Var}\left(\sum_{k:\mathbf{M}_{kj}=1}^{d_ {\text{in}}\rho}\mathbf{X}_{ik}\mathbf{W}_{kj}\right)=\sum_{k:\mathbf{M}_{kj}= 1}^{d_{\text{in}}\rho}\mathrm{Var}(\mathbf{X}_{ik}\mathbf{W}_{kj})\] (6)

Figure 14: SuPar ensures stable optimal learning rate (**Bottom**), unlike SP, μP, μP + SuPar init only, and μP + SuPar LR only.

Figure 13: SuPar ensures stable optimal weight initialization standard deviation, unlike SP, μP, μP + SuPar init only, and μP + SuPar LR only.

Then, since \(\mathbf{X}\) and \(\mathbf{W}\) are independent at initialization:

\[\mathrm{Var}(\mathbf{Y}_{ij})=\sum_{k:\mathbf{M}_{kj}=1}^{d_{\text{out}}\rho}( \mathrm{Var}(\mathbf{X}_{ik})+\mathbb{E}[\mathbf{X}_{ik}]^{2})(\mathrm{Var}( \mathbf{W}_{kj})+\mathbb{E}[\mathbf{W}_{kj}]^{2})-(\mathbb{E}[\mathbf{X}_{ik}] \mathbb{E}[\mathbf{W}_{kj}])^{2}\] (7)

Finally, since at initialization \(\mathbb{E}[\mathbf{W}_{kj}]=0\) and redefining \(\mathrm{Var}(\mathbf{W}_{kj})=\sigma_{\mathbf{W}}^{2}\):

\[\mathrm{Var}(\mathbf{Y}_{ij})=\sum_{k:\mathbf{M}_{kj}=1}^{d_{\text{in}}\rho}( \mathrm{Var}(\mathbf{X}_{ik})+\mathbb{E}[\mathbf{X}_{ik}]^{2})\mathrm{Var}( \mathbf{W}_{kj})=d_{\text{in}}\rho\sigma_{\mathbf{W}}^{2}(\mathrm{Var}( \mathbf{X})+\mathbb{E}[\mathbf{X}]^{2})\] (8)

Rewriting in terms of multipliers for the width \(m_{d_{\text{in}}}=\frac{d_{\text{in}}}{d_{\text{in, base}}}\) and the change in density \(m_{\rho}=\frac{\rho}{\rho_{\text{base}}}\):

\[\mathrm{Var}(\mathbf{Y}_{ij})=m_{d_{\text{in, base}}}m_{\rho}\rho_{\text{base}} \sigma_{\mathbf{W}}^{2}(\mathrm{Var}(\mathbf{X})+\mathbb{E}[\mathbf{X}]^{2})\] (9)

**Solution:** To satisfy the FLD and ensure \(\mathrm{Var}(\mathbf{Y}_{ij})\) scales independently of \(m_{d_{\text{in}}}\) and \(m_{\rho}\), we choose to set \(\sigma_{\mathbf{W}}^{2}=\frac{\sigma_{\mathbf{W},\text{base}}^{2}}{m_{d_{ \text{in}}}m_{\rho}}\). This ensures typical entry size of \(\mathbf{Y}\) is invariant to changes in width \(m_{d_{\text{in}}}\) and density \(m_{\rho}\).

Note that this correction is equivalent to uP [63] when \(m_{\rho}=1\). Further, the sparsity factor in the denominator matches the correction for sparsity-aware initialization from Evici et al. [11].

### Backward gradient pass at initialization

The next stage we would like to control training dynamics is in the layer's backward pass. For a random unstructured sparsity mask \(\mathbf{M}\), since each _row_ of \(\mathbf{M}\) has \(d_{\text{out}}\rho\) non-zero elements in expectation, we can rewrite the backward pass as:

\[\nabla_{\mathbf{X}}\mathcal{L}_{ij}=\left[\nabla_{\mathbf{Y}}\mathcal{L}( \mathbf{W}\odot\mathbf{M})^{\top}\right]_{ij}=\sum_{q}^{d_{\text{out}}}\nabla _{\mathbf{Y}}\mathcal{L}_{iq}(\mathbf{W}_{jq}\cdot\mathbf{M}_{jq})=\sum_{k: \mathbf{M}_{jk}=1}^{d_{\text{out}}\rho}\nabla_{\mathbf{Y}}\mathcal{L}_{ik} \mathbf{W}_{jk}\] (10)

To satisfy the FLD, we desire the _typical element size_ of \(\nabla_{\mathbf{X}}\mathcal{L}\) is \(\Theta(1)\) with respect to change in width \(m_{d_{\text{out}}}\) and change in density \(m_{\rho}\). To achieve this, we can ensure the mean and variance of \(\nabla_{\mathbf{X}}\mathcal{L}\) are invariant to \(m_{d_{\text{out}}}\) and \(m_{\rho}\).

**Mean:** Although the gradients \(\nabla_{\mathbf{Y}}\mathcal{L}\) will have some correlation with weights \(\mathbf{W}\) even at initialization, we assume for simplicity that they are fully independent. Future work could investigate this assumption more deeply. As expectation is linear:

\[\mathbb{E}[\nabla_{\mathbf{X}}\mathcal{L}_{ij}]=\mathbb{E}\left[\sum_{k: \mathbf{M}_{jk}=1}^{d_{\text{out}}\rho}\nabla_{\mathbf{Y}}\mathcal{L}_{ik} \mathbf{W}_{jk}\right]=\sum_{k:\mathbf{M}_{jk}=1}^{d_{\text{out}}\rho}\mathbb{ E}[\nabla_{\mathbf{Y}}\mathcal{L}_{ik}\mathbf{W}_{jk}]=\sum_{k:\mathbf{M}_{jk}=1}^{d_{ \text{out}}\rho}\mathbb{E}[\nabla_{\mathbf{Y}}\mathcal{L}_{ik}]\mathbb{E}[ \mathbf{W}_{jk}]\] (11)

Therefore, since at initialization \(\mathbb{E}[\mathbf{W}_{jk}]=0\), \(\mathbb{E}[\nabla_{\mathbf{X}}\mathcal{L}_{ij}]=0\), the mean is controlled.

**Variance:** As expectation is linear and each weight element is IID:

\[\mathrm{Var}(\nabla_{\mathbf{X}}\mathcal{L}_{ij})=\mathrm{Var}\left(\sum_{k: \mathbf{M}_{jk}=1}^{d_{\text{out}}\rho}\nabla_{\mathbf{Y}}\mathcal{L}_{ik} \mathbf{W}_{jk}\right)=\sum_{k:\mathbf{M}_{jk}=1}^{d_{\text{out}}\rho}\mathrm{ Var}(\nabla_{\mathbf{Y}}\mathcal{L}_{ik}\mathbf{W}_{jk})\] (12)

From the backward pass mean derivation, we know \(\mathbb{E}[\nabla_{\mathbf{Y}}\mathcal{L}_{ij}]=0\). Then, similar to the forward pass variance derivation, we can simplify using the facts that at initialization, \(\nabla_{\mathbf{Y}}\mathcal{L}\) and \(\mathbf{W}\) are (roughly) independent and \(\mathbb{E}[\mathbf{W}]=0\). Similarly we can also define \(\mathrm{Var}(\mathbf{W}^{l}_{kj})=\sigma_{\mathbf{W}}^{2}\) and rewrite in terms of width multiplier \(m_{d_{\text{out}}}=\frac{d_{\text{out}}}{d_{\text{base}}}\) and changes in density \(m_{\rho}=\frac{\rho}{\rho_{\text{base}}}\):

\[\mathrm{Var}(\nabla_{\mathbf{X}}\mathcal{L}_{ij})=m_{d_{\text{out}}}d_{\text{out,base}}m_{\rho}\rho_{\text{base}}\sigma_{\mathbf{W}}^{2}\mathrm{Var}(\nabla_{ \mathbf{Y}}\mathcal{L})\] (13)

**Solution:** To satisfy the FLD and ensure \(\mathrm{Var}(\nabla_{\mathbf{X}}\mathcal{L}_{ij})\) scales independently of \(m_{d_{\text{out}}}\) and \(m_{\rho}\), we choose to set \(\sigma_{\mathbf{W}}^{2}=\frac{\sigma_{\mathbf{W},\text{base}}^{2}}{m_{d_{\text{ out}}}m_{\rho}}\). This ensures the typical entry size of \(\nabla_{\mathbf{X}}\mathcal{L}\) is invariant to changesin width \(m_{d_{\text{in}}}\) and density \(m_{\rho}\). Typically, we scale model width such that \(m_{d_{\text{in}}}=m_{d_{\text{in}}}\). This equal scaling allows the same initialization variance to correct both forward activation and backward gradient scales, making them independent of width. Further, since we assume random sparsity across layer's weights, the sparsity initialization correction factor, \(m_{\rho}\), is the same for both the forward activations and backward gradients.

### Effect of weight update \(\Delta\mathbf{W}\) on \(\mathbf{Y}\)

To satisfy the FLD, we desire the _typical element size_ of the weight update \(\Delta\mathbf{Y}\) is \(\Theta(1)\) with respect to change in width \(m_{d_{\text{in}}}\) and change in density \(m_{\rho}\). To achieve this we examine the expected size of each element. Here, we use \(\eta\) to be the learning rate for this layer. For a random unstructured sparsity mask \(\mathbf{M}\), since each _column_ of \(\mathbf{M}\) has \(d_{\text{in}}\rho\) non-zero elements in expectation:

\[\Delta\mathbf{Y}_{ij}=\left[\eta\mathbf{X}(\Delta\mathbf{W}\odot\mathbf{M}) \right]_{ij}=\eta\sum_{q=1}^{d_{\text{in}}}\mathbf{X}_{iq}(\Delta\mathbf{W}_{ qj}\cdot\mathbf{M}_{qj})=\eta\sum_{k:\mathbf{M}_{kj}=1}^{d_{\text{in}}\rho} \mathbf{X}_{ik}\Delta\mathbf{W}_{kj}\] (14)

**Mean:** As expectation in linear:

\[\mathbb{E}[\Delta\mathbf{Y}_{ij}]=\mathbb{E}\left[\eta\sum_{k:\mathbf{M}_{kj} =1}^{d_{\text{in}}\rho}\mathbf{X}_{ik}\Delta\mathbf{W}_{kj}\right]=\eta\sum_{ k:\mathbf{M}_{kj}=1}^{d_{\text{in}}\rho}\mathbb{E}[\mathbf{X}_{ik}\Delta \mathbf{W}_{kj}]\] (15)

Since \(\Delta\mathbf{W}\) was derived from \(\mathbf{X}\), there is covariance between these variables and \(\mathbb{E}[\mathbf{X}_{ik}\Delta\mathbf{W}_{kj}]\) is non-zero. By the Law of Large Numbers:

\[\mathbb{E}[\Delta\mathbf{Y}_{ij}]\rightarrow\eta d_{\text{in}}\rho\mathbb{E} \left[\mathbf{X}_{ik}\Delta\mathbf{W}\right],\text{ as }(d_{\text{in}}\rho)\rightarrow\infty\] (16)

Rewriting in terms of width and density multipliers:

\[\mathbb{E}[\Delta\mathbf{Y}_{ij}]\rightarrow\eta m_{d_{\text{in}}}d_{\text{ in,base}}m_{\rho}\rho_{\text{base}}\mathbb{E}\left[\mathbf{X}_{ik}\Delta \mathbf{W}\right],\text{ as }(d_{\text{in}}\rho)\rightarrow\infty\] (17)

Equation 17 will be used as intermediate result in the following sections.

#### d.3.1 Effect SGD weight update \(\Delta\mathbf{W}\) on \(\mathbf{Y}\)

Following the formulation in [63], stochastic gradient descent (SGD) weight updates take the form:

\[\Delta\mathbf{W}_{kj}^{l}=\left[\frac{(\mathbf{X})^{\top}(\nabla_{\mathbf{Y}} \mathcal{L})}{d_{\text{in}}}\right]_{kj}=\frac{1}{d_{\text{in}}}\sum_{b=1}^{B }\mathbf{X}_{bk}(\nabla_{\mathbf{Y}}\mathcal{L})_{bj}\] (18)

so we can rewrite Equation 17 as:

\[\mathbb{E}[\Delta\mathbf{Y}_{ij}]\rightarrow\eta m_{\rho}\rho_{\text{base}} \mathbb{E}\left[\mathbf{X}_{ik}\sum_{b=1}^{B}\mathbf{X}_{bk}(\nabla_{\mathbf{Y }}\mathcal{L})_{bj}\right],\text{ as }(d_{\text{in}}\rho)\rightarrow\infty\] (19)

**Solution:** For SGD updates, to satisfy the FLD and ensure \(\mathbb{E}[\Delta\mathbf{Y}_{ij}]\) and the typical entry size of \(\Delta\mathbf{Y}\) are scale invariant to \(m_{d}\) and \(m_{\rho}\), we choose \(\eta=\eta_{\text{base}}/m_{\rho}\). Note this correction is equivalent to \(\upmu\)P [63] when \(\rho=1,m_{\rho}=1\).

#### d.3.2 Effect of Adam weight update \(\Delta\mathbf{W}\) on \(\mathbf{Y}\)

Following the formulation in Yang et al. [63], Adam weight updates take the form:

\[\Delta\mathbf{W}_{kj}=\frac{\sum_{t}^{T}\gamma_{t}\sum_{b}^{B}\mathbf{X}_{bk}^ {l,t}(\nabla_{\mathbf{Y}}\mathcal{L})_{bj}^{t}}{\sqrt{\sum_{t}^{T}\omega_{t} \sum_{b}^{B}(\mathbf{X}_{bk}^{t}(\nabla_{\mathbf{Y}}\mathcal{L})_{bj}^{t})^{2 }}}\] (20)

where \(T\) is the current training step and \(\gamma_{t},\omega_{t}\) are the moving average weights at each training step. Here, we can just consider the weight update associated with an unpruned weight, since a prunedweight will have value and update 0 (i.e., pruned weights trivially satisfy that their effect on forward activations cannot depend on width or sparsity). We can rewrite Equation 17 as:

\[\mathbb{E}[\Delta\mathbf{Y}_{ij}]\rightarrow\eta m_{d_{\text{in,base}}}m_{\rho} \rho_{\text{base}}\mathbb{E}\left[\mathbf{X}_{ik}\left(\frac{\sum_{t}^{T} \gamma_{t}\sum_{b}^{B}\mathbf{X}_{ik}^{t}\nabla_{\mathbf{Y}}\mathcal{L}_{bj}^{ t}}{\sqrt{\sum_{t}^{T}\omega_{t}\sum_{b}^{B}(\mathbf{X}_{ik}^{t}\nabla_{ \mathbf{Y}}\mathcal{L}_{bj}^{t})^{2}}}\right)\right],\text{ as }(d_{\text{in}}\rho)\rightarrow\infty\] (21)

Solution:For Adam updates, to satisfy the FLD and ensure \(\mathbb{E}[\Delta\mathbf{Y}_{ij}]\) and the typical entry size of \(\Delta\mathbf{Y}\) are scale invariant to \(m_{d_{\text{in}}}\) and \(m_{\rho}\), we choose \(\eta=\frac{\eta_{\text{base}}}{m_{d_{\text{in}}}m_{\rho}}\). Note that this correction is equivalent to uP [63] when \(\rho=1,m_{\rho}=1\).

### Additional notes about derivation

We make a few supplementary notes about the above derivation:

* Throughout our derivation, we use \(\rho\) to refer to the density level. Note that since this derivation is local to a single layer in the model, the density (or sparsity) level can also be parameterized independently for each layer. If a sparsity technique will use layer-wise independent sparsity levels, appropriate corrections should be made for each layer.
* Similar to the \(\rho\) notation, we use \(\eta\) to denote the learning rate, but this learning rate can be layer-specific depending on sparsity level. Appropriate corrections must be made if using layer-wise independent sparsities.
* The use of the Law of Large Numbers in portions of the above derivation indicate that SuPar is expected to provide stable training dynamics as the number of non-zero weights per neuron (WPN) tends to infinity. However, in sparse settings, the WPN can tend to be small. If WPN is small, training dynamics may be affected, and this might be a direction for future work.
* In this work, we only consider sparsifying linear projection layers. As a result, SuPar matches uP for other layers like input, output, bias, layer-norm, and attention logits. Depending on the sparsification technique, these other layers might need to be reviewed for their effects on training dynamics.

## Appendix E Experimental details

SuPar base hyperparameter tuningTo find the optimized set of hyperparameters for SuPar, we actually tune uP HPs on a dense proxy model. By formulation of SuPar, these HPs transfer optimally to all the sparse models trained for this work. This dense proxy model is a GPT-2 model, but with small changes: ALiBi position embeddings [47] and SwiGLU nonlinearity [51]. We configure it with width: \(d_{\text{model}}=d_{\text{model,base}}=256\), number of layers: \(n_{\text{layers}}=24\), and head size: \(d_{\text{head}}=64\), resulting in 39M parameters. We trained this proxy model on 800M tokens with a batch size of 256 sequences and sequence length 2048 tokens. We randomly sampled 350 configurations of base learning rates, base initialization standard deviation, and embedding and output logits scaling factors. From this sweep we obtained the tuned hyperparameters listed in Table 3.

Experimental details for all figuresIn Table 4, we provide extensive details on hyperparameters, model size, and training schedule for all experiments in this paper. All models in this paper were trained on the SlimPajama dataset [54], a cleaned and deduplicated version of the RedPajama dataset.

\begin{table}
\begin{tabular}{c c} \hline \hline Hyperparameter & Value \\ \hline \(\sigma_{W,\text{base}}\) & 0.08665602 \\ \(\eta_{\text{base}}\) & 1.62E-2 \\ \(\alpha_{\text{input}}\) & 9.1705 \\ \(\alpha_{\text{output}}\) & 1.0951835 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Tuned hyperparameters for our dense proxy model.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Sections 6 and A contain a discussion of limitations and broader impact. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: In Section 3 we provide a short proof sketch to provide intuition behind both SuPar and \(\upmu\)P. Then in Section D we provide a detailed derivation for SuPar (which also reduces to \(\upmu\)P when \(\rho=1\)). Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: To ensure reproducibility, in Section E we provide extensive details on all experiments contained in the paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: All tests in this paper use GPT-like transformer language models [48, 9], trained on the SlimPajama dataset [54], which are both open. Section E contains sufficient detail to faithfully reproduce the main experimental results. A minimal implementation of SupPar is available at https://github.com/EleutherAI/nanoGPT-mup/tree/supar. We also provide a simple breakdown of the code changes required in Table 1. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: To ensure reproducibility, in Section E we provide extensive details on all experiments contained in the paper. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In most of our experiments, we sample multiple seeds and plot the mean loss as a solid line, as well as the standard error of the mean as a shaded area of the same color. We disclose the number of seeds used for each experiment in the corresponding figure captions. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In Section E we provide extensive experimental details which can be used to easily derive compute requirements based on a reader's specific hardware setting. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We do not conduct any research with human participants. Also, we consider the broader impacts and limitations of our work in Sections A and 6. Finally, we provide extensive experimental details in Section E to improve reproducibility. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes]Justification: See Section A. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper presents a new neural network parameterization SuPar and does not release any models or datasets. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite all datasets and architectures in accordance with their licenses. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not introduce new assets in this paper. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not conduct any research involving crowdsourcing or human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not conduct any research involving human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.