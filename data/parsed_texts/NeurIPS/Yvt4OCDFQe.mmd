# Quantum Diffusion Models for Few-Shot Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Modern quantum machine learning (QML) methods involve the variational optimization of parameterized quantum circuits on training datasets, followed by predictions on testing datasets. Most state-of-the-art QML algorithms currently lack practical advantages due to their limited learning capabilities, especially in few-shot learning tasks. In this work, we propose three new frameworks employing quantum diffusion model (QDM) as a solution for the few-shot learning: label-guided generation inference (LGGI); label-guided denoising inference (LGDI); and label-guided noise addition inference (LGAAI). Experimental results demonstrate that our proposed algorithms significantly outperform existing methods.

## 1 Introduction

Quantum machine learning (QML) has emerged as a powerful tool for automated decision-making across diverse fields such as finance, healthcare, and drug discovery[1; 2; 3; 4]. However, in the realm of few-shot learning, where only a limited amount of data is available for training, QML demonstrates suboptimal performance. In classical machine learning, diffusion models have been validated as effective zero-shot classifiers and hold significant potential for addressing few-shot learning problems[5; 6]. Nevertheless, in the domain of QML, the utilization of quantum diffusion models (QDMs) for few-shot learning remains largely unexplored[7]. This is primarily due to the limitations of quantum computing resources and the inherent noise associated with quantum computers, despite the QDM's demonstrated success in generative tasks[8].

In this work, we propose three new algorithms based on the QDM to address the few-shot learning problem. Our contributions are as follows:

* The QDM has demonstrated strong performance in generative tasks. Building on QDM's generative capabilities, we propose the **Label-Guided Generation Inference (LGGI)** algorithm to address the few-shot learning problem. Additionally, we introduce two algorithms: **Label-Guided Noise Addition Inference (LGAAI)** and **Label-Guided Denoising Inference (LGDI)**, to perform test inference respectively in diffusion and denoising stages.
* We compare our algorithms with other baselines in experiments on different datasets, which verified the superior performance of our proposed approaches.
* We conduct a comprehensive ablation study to evaluate the impact of various components and hyperparameters on the performance of the proposed algorithms.

## 2 Background

**Quantum Neural Network (QNN).** A Quantum Neural Network (QNN) has been used to perform various machine learning tasks. It typically consists of a data encoder \(E(x)\) that embeds a classicalinput \(x\) into a quantum state \(|x\rangle\), a variational quantum circuit (VQC) \(Q\) that generates the output state, and a measurement layer \(M\) that maps the output quantum state to a classical vector. Fig. 1 shows some VQC ansatz examples[9; 10; 11; 12] used for QNNs. Given a training dataset, the input data \(x\) is transformed into a quantum input feature map using \(E(x)\). A parameterized VQC ansatz is then utilized to manipulate the quantum input feature through unitary transformations. Finally, the predicted classification is obtained by measuring the quantum state. The loss function is predefined to calculate the difference between the output of the QNN and the true target value \(y\). Training a QNN involves iteratively searching for the optimal parameters in the VQC through a hybrid quantum-classical optimization procedure.

**Quantum Few-shot Learning (QFSL).** Few-shot learning (FSL) is a machine learning approach designed to address supervised learning challenges with a very limited number of training samples. Specifically, it involves a support set and a query set. The support set consists of a small number of labeled examples from which the model learns, encompassing \(n\) classes, each with \(k\) samples, hence called \(n\)-way \(k\)-shot learning. The query set is a collection of unlabeled examples that the model needs to classify into one of the \(n\) classes. Existing solutions to the QFSL problem can be categorized into data-based, model-based, and algorithm-based methods[13]. Quantum Few-shot learning (QFSL) involves using QNNs as classifiers to solve QFSL problems[14; 15]. However, traditional algorithms used in QFSL often underperform due to the limited computational resources available and the noise present in real quantum devices.

**Quantum Diffusion Model (QDM).** Diffusion model (DM)[16; 17] is a popular approach for generating images and other high-dimensional data. It comprises two main processes: the diffusion process and the denoising process. During the diffusion process, noise is gradually added to the data over a series of steps, transforming it into a simpler distribution, as formulated by (1), in which \(\mathcal{N}(\cdot;\mu,\Sigma)\) denotes the normal distribution of mean \(\mu\) and covariance \(\Sigma\), \(\beta_{t}\) is a small positive constant that controls the amount of noise added at step \(t\), and \(\mathbf{I}\) is the identity matrix.

\[q(x_{t}|x_{t-1})=\mathcal{N}(x_{t};\sqrt{1-\beta_{t}}x_{t-1},\beta_{t}\mathbf{ I})\] (1)

The denoising process aims to learn how to reverse the forward process and incrementally remove noise to generate new data from the noise, with its training objective formulated by

\[\mathbb{E}_{q(x_{0:T})}\left[\sum_{t=1}^{T}D_{\mathrm{KL}}\big{(}q(x_{t-1}|x_ {t},x_{0})\|p_{\theta}(x_{t-1}|x_{t})\big{)}\right],\] (2)

in which \(q(x_{t-1}|x_{t},x_{0})\) is the posterior distribution of the forward process and the parameterized model \(p_{\theta}(x_{t-1}|x_{t})\) can predict the data point at the previous step given the current noisy data point. The denoising process is described by

\[p_{\theta}(x_{t-1}|x_{t})=\mathcal{N}\big{(}x_{t-1};\mu_{\theta}(x_{t},t), \Sigma_{\theta}(x_{t},t)\big{)}.\] (3)

The QDM, which integrates QML and DM, is utilized for generative tasks within the quantum domain, including quantum state generation and quantum circuit design. The quantum denoising diffusion model (QDDM)[7] is acknowledged as the leading quantum diffusion method for image generation. It outperforms classical models with similar parameter counts, while leveraging the efficiencies of quantum computing. Fig. 3 shows the framework of QDDM and its image generation process is illustrated in Fig. 2. In our work, we extend the QDDM with a label-guided mechanism to fully leverage the capabilities of QDDM in addressing the QFSL problems. This is achieved by introducing an additional qubit and applying a Pauli-X rotation by an angle of \(2\pi y/n\), where \(y\) represents the specified label and \(n\) denotes the total number of classes.

Figure 1: Various types of variational quantum circuits (VQC).

## 3 Method

To address the QFSL problems, we propose methods from both data and algorithmic perspectives. From the data perspective, we utilize QDDM to augment the training samples and use the generated data to train QNN, thereby improving the prediction accuracy of QNN on real data. From an algorithmic perspective, we employ two strategies to complete the inference process by guiding QDDM in two distinct stages: diffusion and denoise.

### QDiff-Based Label-Guided Generation Inference (QDiff-LGGI)

The size of the training dataset is a critical factor that limits the performance of QNN. The primary reason for the suboptimal performance of QFSL is the limited availability of training data. Thus, from a data perspective, expanding the training dataset can significantly enhance the performance of QFSL. The QDDM is highly effective in generation tasks, making it suitable for augmenting the training dataset. Initially, a small amount of training data is used to train the QDDM. Once trained, the QDDM is employed to expand the training dataset for QNN. This expanded dataset is then used to train the QNN, which in turn improves its inference accuracy on real data.

To enhance the quality of data generated by the QDDM, we employ a label-guided generation method. During the QDDM training process, we perform amplitude encoding on the classical data and angle encoding on the labels. During the data generation process, we use random noise and the label as input, enabling the QDDM to generate data according to the specified label. Fig. 2 illustrates the data generation process under different label guidance. Fig. 4 describes the QDiff-LGGI algorithm.

### QDiff-Based Label-Guided Noise Addition Inference (QDiff-LGNAI)

The learning objective of the QDDM outlined in Equation 2 relies on using a noise predictor to estimate the noise in noisy data compared to the actual noise. The noise predictor's estimation is guided by a label, with different labels corresponding to different noise predictions. By using the correct label for guidance, the error between the predicted noise and the actual noise is minimized. Based on this principle, we propose the QDM-Based Label-Guided Noise Addition Inference (Diff-LGNAI) method, shown in the Fig. 5.

We first utilize a small amount of training data to complete the training of the QDDM. Once trained, the noise predictor \(\mathcal{P}\) within the QDDM is used for subsequent inference. For a given input \(x_{0}\), the possible labels are \(\{L_{1},L_{2},\ldots,L_{m}\}\). Noise is gradually added to \(x_{0}\) over \(\mathcal{T}\) iterations. Specifically, at each time step \(t\), \(x_{t}\) is calculated as \(x_{t-1}+\epsilon_{t}\), where \(\epsilon_{t}\sim\mathcal{N}(x_{t-1},\mathcal{W}[t])\), and \(\mathcal{W}\) represents the noise weight. The noise predictor \(\mathcal{P}\) is then employed to estimate the noise in the noisy data \(x_{t}\), guided by various possible labels, resulting in the predicted noise set \(\{\mathcal{P}(x_{t}|L_{1}),\ldots,\mathcal{P}(x_{t}|L_{m})\}\). We calculate the mean squared error (MSE) between the predicted noise and the actual noise, \(\mathrm{MSE}(\mathcal{P}(x_{t}|L_{i}),\epsilon_{t})\). The error is computed for each possible label, and the label with the minimumaverage error over \(\mathcal{T}\) iterations is selected as the predicted label:

\[\arg\min_{L_{i}\in\mathcal{L}}\sum_{t=1}^{\mathcal{T}}\mathrm{MSE}(\mathcal{P}(x _{t}|L_{i}),\epsilon_{t}).\]

### QDiff-Based Label-Guided Denoising Inference (QDiff-LGDI)

During the denoising phase of QDDM, the noise predictor is used to estimate the noise present in the noisy data, which is then subtracted from the noisy data. This denoising process is repeated over \(\mathcal{T}\) iterations. The noise prediction is guided by labels, with each label producing distinct noise estimates. The data generated under the guidance of the true label is expected to be most similar to the original data. In this framework, we propose the QDiff-Based Label-Guided Denoising Inference (QDiff-LGDI) method.

For an input \(x_{0}\), we gradually add noise to \(x_{0}\) over \(\mathcal{T}\) iterations, resulting in progressively noisier data \(\{x_{1},x_{2},\dots,x_{\mathcal{T}}\}\). Then, we use the noise predictor \(\mathcal{P}\) to predict the noise in the noisy data under the guidance of label \(L_{i}\), obtaining \(\mathcal{P}(x_{\mathcal{T}}|L_{i})\). The predicted noise is subtracted from the noisy data. This denoising process is also performed over \(\mathcal{T}\) iterations, producing progressively noise-reduced data \(\{x_{\mathcal{T}+1},x_{\mathcal{T}+2},\dots,x_{\mathcal{T}}\}\), where \(x_{\mathcal{T}+t+1}|L_{i}=x_{\mathcal{T}+t}-\mathcal{P}(x_{\mathcal{T}+t}|L_ {i})\). We then use the MSE loss to calculate the error between the generated data and the noisy data under the guidance of different labels \(L_{i}\), and the predicted label is chosen such that

\[\arg\min_{L_{i}\in\mathcal{L}}\sum_{t=0}^{\mathcal{T}}\mathrm{MSE}(x_{t},x_{ 2\mathcal{T}-t}|L_{i}).\]

## 4 Experiment

In this section, we first outline the fundamental settings of our experiment. We then design a series of experiments to explore the following specific questions, each addressed in a dedicated subsection:

* What are the performance advantages of our proposed three QDiff-based algorithms compared to other baseline methods?
* What factors influence the performance of our algorithms?
* How effectively does our algorithms solve the zero-shot problem?

### Basic Experimental Settings

In this section, we provide a detailed description of the dataset used for the experiments, the baseline algorithms, and the parameter settings of the algorithms.

**Dataset.** During the experiment, we use the Digits MNIST[18], MNIST[19], and Fashion MNIST[20] datasets. For the 2-way \(k\)-shot tasks, we select classes 0 and 1 from both the Digits MNIST and MNIST datasets, and the T-shirt and Trouser classes from the Fashion MNIST dataset. For the 3-way \(k\)-shot tasks, we choose classes 0, 1, and 2 from both the Digits MNIST and MNIST datasets, and the T-shirt, Trouser, and Pullover classes from the Fashion MNIST dataset. During training, for the

Figure 5: Framework of QDDM-based Label-Guided Figure 6: Framework of QDDM-based Label-Guided Noise Addition Inference (QDiff-LGDI). The term \(\widehat{\epsilon}_{m}^{n}\) Denoising Inference (QDiff-LGDI). Solid circles in differents the predicted noise at step \(m\) associated with latent colors represent distinct embedded labels. The label \(n\). \(L_{0}\)/\(L_{1}\)-loss denotes the difference between the output images, each framed by a square of varying color-true noise and the predicted noise under the guidance of \(\mathrm{ors}\), indicate the generated images guided by different different labels \(L_{i}\). labels \(L_{i}\).

one-shot task, we select one image from each category, and for the ten-shot task, we select ten images from each category. In the inference phase, we use 200 images from each category to construct the evaluation dataset.

**Baselines and Parameters Setting.** For the selection of baselines, we choose four representative QNN structures in the current QML domain to accomplish the QFSL task [9; 10; 11; 12]. The frameworks of the four QNNs are shown in Fig. 1. During the training of the QNN, we resize the image data to \(8\times 8\) and utilize amplitude encoding to convert classical data into quantum states. Adam optimizer is employed with a learning rate of \(0.001\) and cross entropy loss is minimized over 40 iterations.

**QDDM Training.** Before applying QDiff-based algorithms to finish the QFSL task, it is essential to obtain a well-trained QDDM model. For training the QDDM, we utilize a label-guided quantum dense architecture, where the label is embedded using an \(RX\) rotation, and the strongly entangling layers[21] are used to transform the data. The training process of QDDM involves using the Adam optimizer with \(10\),\(000\) iterations. The model architecture and learning rate are tailored to each dataset. For the Digits MNIST dataset, the circuit consists of 47 layers with a learning rate of \(0.00097\). For the MNIST dataset, it comprises 60 layers with a learning rate of \(0.00211\), and for the Fashion MNIST dataset, the circuit includes 121 layers with a learning rate of \(0.00014\).

### Performance Analysis of QDiff-based QFSL Algorithms

During the QDDM training phase, in the \(n\)-way, \(k\)-shot setting, \(k\) images are selected from each of the \(n\) categories, resulting in a total of \(n\times k\) images. Fig. 8 illustrates the trend of training loss while training QDDM on Digits MNIST dataset. As training progresses, the decreasing training loss reflects the improved accuracy of the noise predictor in estimating noise, resulting in denoised images that closely resemble the target images.

Table 1 presents the performance of the QDiff-based QFSL algorithm compared to other baselines for 2-way 1-shot, 2-way 10-shot, 3-way 1-shot, and 3-way 10-shot scenarios. The results in the table demonstrate that the QDiff-based algorithm achieves state-of-the-art performance. We also assess the performance of the QDiff-based algorithms on a 3-way, 1-shot task using the Digits MNIST dataset on a real quantum computer (IBM_Almaden). The results, as shown in Fig. 9, reveal a slight performance decline due to noise inherent in the quantum hardware. Nevertheless, the decrease is marginal, indicating that our algorithms perform robustly even in noisy processors.

### Factors Impacting the Effectiveness of QDiff-based QFSL Algorithms

In this section, we explore the factors that influence the performance of QDiff-based algorithms, including the impact of diffusion and denoising steps, the quantity of training data, and the selection of QNNs utilized in QDiff-LGGI.

With variations in the number of diffusion and denoising steps, the performance of QDiff-based algorithms on the Digits MNIST and MNIST datasets varies, as shown in Fig. 12. The experimental results demonstrate that QDiff-LGGI is highly sensitive to the number of diffusion and denoising steps. As the number of steps increases, QDiff-LGGI is improved, indicating that more steps result in the generation of higher-quality images that are closer to the target data domain. However, an excessive number of steps may cause the original image to degrade too much into noise during the diffusion stage. Consequently, during the denoising stage, the reconstruction process may overemphasize the label, resulting in a mismatch with the original image. This mismatch negatively impacts inference performance, and the phenomenon is more pronounced in QDiff-LGAI and QDiff-LGDI.

The quantity of training data used to train the QDDM significantly influences the performance of the QDiff-based QFSL algorithm. We compare the performance of the QDDM when trained with one-shot versus ten-shot learning. Table 1 presents the performance comparison across different datasets. The results indicate that increasing the amount of training data enhances the training of the QDDM, which subsequently leads to improved performance of the QDiff-based algorithms when the QDDM is well-trained.

QDiff-LGGI uses generated images to train QNN, which is then used for inference. The performance of inference varies depending on the QNN architecture. Fig. 10 shows that different QNNs produce varying inference results, likely due to differences in the quantum circuits' expressibity and entangling capabilities[10].

### Zero-Shot Learning with QDiff-based QFSL Algorithms

We evaluate the effectiveness of our methods in solving zero-shot tasks. The QDDM model is initially trained on the MNIST dataset and then applied within QDiff-based algorithms for evaluation on the Digits MNIST dataset. Conversely, we also train the QDDM model on the Digits MNIST dataset and assess its performance on the MNIST dataset. We evaluate performance on both 2-way and 3-way zero-shot classification tasks. The results of these experiments are shown in Figs. 12 and 12. Based on these results, we conclude that QDiff-based algorithms demonstrate strong performance in zero-shot scenarios when the training and evaluation datasets belong to similar domains.

## 5 Conclusion and Future Work

In this work, we introduce quantum diffusion model (QDM) to tackle the challenges of quantum few-shot learning. We propose three algorithms--QDiff-LGDI, QDiff-LGAI, and QDiff-LGGI--developed from both data-driven and algorithmic perspectives. These algorithms demonstrate significant performance improvements over existing baselines. Nevertheless, the current limitations of the QDM confine its applicability to relatively simple datasets. Future research could focus on enhancing the QDM's capability and expanding its application to other QML tasks, such as quantum object detection and quantum semantic segmentation.

## References

* [1]R. Wang, F. Baba-Yara, and F. Chen (2024) Justq: automated deployment of fair and accurate quantum neural networks. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), pp. 121-126. Cited by: SS1.
* [2]S. Focardi, F. J. Fabozzi, and D. Mazza (2020) Quantum option pricing and quantum finance. Journal of Derivatives28 (1), pp. 79-98. Cited by: SS1.
* [3]D. Frederick Parsons (2011) Possible medical and biomedical uses of quantum computing. Neuroquantology9 (3). Cited by: SS1.
* [4]Y. Cao, J. Romero, and A. Aspuru-Guzik (2018) Potential of quantum computing for drug discovery. IBM Journal of Research and Development62 (6), pp. 6-1. Cited by: SS1.
* [5]A. C. Li, M. Prabhudesai, S. Duggal, E. Brown, and D. Pathak (2023) Your diffusion model is secretly a zero-shot classifier. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2206-2217. Cited by: SS1.
* [6]K. Clark and P. Jaini (2024) Text-to-image diffusion models are zero shot classifiers. Advances in Neural Information Processing Systems36. Cited by: SS1.
* [7]M. Kolle, G. Stenzel, J. Stein, S. Zielinski, B. Ommer, and C. Linnhoff-Popien (2024) Quantum denoising diffusion models. arXiv preprint arXiv:2401.07049. Cited by: SS1.
* [8]J. Ho, A. Jain, and P. Abbeel (2020) Denoising diffusion probabilistic models. Advances in neural information processing systems33, pp. 6840-6851. Cited by: SS1.
* [9]T. Patel, D. Silver, and D. Tiwari (2022) Optic: a practical quantum binary classifier for near-term quantum computers. In 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 334-339. Cited by: SS1.
* [10]T. K. Patel, D. Silver, and D. Tiwari (2022) Optic: a practical quantum binary classifier for near-term quantum computers. In 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 334-339. Cited by: SS1.
* [11]T. K. Patel, D. Silver, and D. Tiwari (2022) Optic: a practical quantum binary classifier for near-term quantum computers. In 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 334-339. Cited by: SS1.
* [12]T. K. Patel, D. Silver, and D. Tiwari (2022) Optic: a practical quantum binary classifier for near-term quantum computers. In 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 334-339. Cited by: SS1.
* [13]T. K. Patel, D. Silver, and D. Tiwari (2022) Optic: a practical quantum binary classifier for near-term quantum computers. In 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 334-339. Cited by: SS1.
* [14]T. K. Patel, D. Silver, and D. Tiwari (2022) Optic: a practical quantum binary classifier for near-term quantum computers. In 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 334-339. Cited by: SS1.
* [15]J. Song, C. Meng, and S. Ermon (2020) Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502. Cited by: SS1.
* [16]J. Wang, J. Gu, Y. Ding, Z. Li, F. T. Chong, D. Z. Pan, and S. Han (2022) Quantumnat: quantum noise-aware training with noise injection, quantization and normalization. In Proceedings of the 59th ACM/IEEE design automation conference, pp. 1-6. Cited by: SS1.

[MISSING_PAGE_POST]

* [18] E. Alpaydin and Fevzi. Alimoglu, "Pen-Based Recognition of Handwritten Digits," UCI Machine Learning Repository, 1996, DOI: https://doi.org/10.24432/C5MG6K.
* [19] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner, "Gradient-based learning applied to document recognition," _Proceedings of the IEEE_, vol. 86, no. 11, pp. 2278-2324, 1998.
* [20] H Xiao, "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms," _arXiv preprint arXiv:1708.07747_, 2017.
* [21] Ville Bergholm, J Lzaac, M Schuld, C Gogolin, S Ahmed, V Ajith, MS Alam, G Alonso-Linaje, B AkashNarayanan, A Asadi, et al., "Pennylane: Automatic differentiation of hybrid quantum-classical computations. arxiv 2018," _arXiv preprint arXiv:1811.04968_, 2018.