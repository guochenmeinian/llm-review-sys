# Incentives in Federated Learning: Equilibria, Dynamics, and Mechanisms for Welfare Maximization

 Aniket Murhekar\({}^{1}\)   Zhuowen Yuan\({}^{1}\)   Bhaskar Ray Chaudhury\({}^{1}\)   Bo Li\({}^{1}\)   Ruta Mehta\({}^{1}\)

\({}^{1}\)University of Illinois, Urbana-Champaign

{aniket2,zhuowen3,bryacha,lbo,rutameht}@illinois.edu

###### Abstract

Federated learning (FL) has emerged as a powerful scheme to facilitate the collaborative learning of models amongst a set of agents holding their own private data. Although the agents benefit from the global model trained on shared data, by participating in federated learning, they may also incur costs (related to privacy and communication) due to data sharing. In this paper, we model a collaborative FL framework, where every agent attempts to achieve an optimal trade-off between her learning payoff and data sharing cost. We show the existence of Nash equilibrium (NE) under mild assumptions on agents' payoff and costs. Furthermore, we show that agents can discover the NE via best response dynamics. However, some of the NE may be bad in terms of overall welfare for the agents, implying little incentive for some fraction of the agents to participate in the learning. To remedy this, we design a _budget-balanced mechanism_ involving _payments_ to the agents, that ensures that any \(p\)-mean welfare function of the agents' utilities is maximized at NE. In addition, we introduce a FL protocol FedBR-BG that incorporates our budget-balanced mechanism, utilizing best response dynamics. Our empirical validation on MNIST and CIFAR-10 substantiates our theoretical analysis. We show that FedBR-BG outperforms the basic best-response-based protocol without additional incentivization, the standard federated learning protocol FedAvg (McMahan et al. (2017)), as well as a recent baseline MWFed (Blum et al. (2021)) in terms of achieving superior \(p\)-mean welfare.

## 1 Introduction

Federated Learning (FL) has emerged as an effective collaborative training paradigm, where a group of agents can jointly train a common machine learning model. The success of collaborative learning paradigms is visible in the domains of autonomous vehicles (Elbir et al. (2020)), digital healthcare (Dayan et al. (2021); Xu et al. (2021); Nvidia (2019)), multi-devices (Learning (2017)), and biology (Bergen and Petryshen (2012)). Although such collaborative learning is immensely beneficial to the agents, individually, they may not be incentivized to share their data. This is because sharing data may incur costs attributed to bandwidth use, privacy leakage, and the use of computing resources. In turn, high costs and low learning payoffs may cause some of the agents to drop out of FL, resulting in subpar learning. As noted by Blum et al. (2021), the success of FL systems depends on its ability to attract and retain a large number of federating agents.

_Thus, it is crucial to achieve fairness and welfare guarantees for all participating agents._

This calls for game-theoretic modeling and analysis of the agent's payoff and costs, and subsequent mechanism design to incentivize participation. Towards the former, Blum et al. (2021) introduce a model, where every agent receives a _payoff_ from the collaboration measuring the "learning benefit" the agent derives from the total data shared. In particular, when each agent \(i\) contributes \(s_{i}\) units of data, the payoff for agent \(i\) is captured by \(a_{i}(\bm{s})\), where \(\bm{s}=\langle s_{1},s_{2},\ldots,s_{n}\rangle\) is the data contributionprofile, or _sample vector_. Blum et al. (2021) consider the _constrained cost-minimizing model_, where each agent minimizes her data contribution (\(s_{i}\)) subject to the requirement that her payoff should be larger than a threshold (\(a_{i}(s_{i},\bm{s}_{-i})\geq\mu_{i}\)). They show that a Nash equilibrium (NE) (Nash (1951)), arguably the most sought after solution concept in game theory, may not always exist in this model. They derive sufficient conditions for existence of NE, as well as provide novel structural results about the equilibria.

We note that an agent's cost for sharing data may be more complex than just the size of the total data shared. There are studies dedicated to quantifying the losses (attributed to data collection, processing, and privacy) that are incurred with increased sharing of data (Li and Raghunathan (2014); Laudon (1996); Jaisingh et al. (2008)). Furthermore, within game theory (and economics) typically agents are considered as net utility maximizers, _i.e.,_ maximizing payoff minus cost, for example value minus payment in auctions (Myerson (1981)), and revenue minus cost in markets (Huber et al. (2001); Borgers (2015)). To capture these aspects, in this paper we propose an _unconstrained utilitarian model_, where we define the _utility_\(u_{i}(\bm{s})\) of each agent \(i\) as the difference between her payoff and her cost of sharing data, i.e., \(u_{i}(\bm{s})=a_{i}(\bm{s})-c_{i}(s_{i})\), where \(c_{i}(s_{i})\) denotes the cost incurred by agent \(i\) on sharing \(s_{i}\) data samples. Each agent aims to maximize their utility. This model is natural in FL settings where there is a natural coupling between payoffs and costs, and there are no hard cost constraints, i.e., having an upper bound on the total cost that an agent can incur. Since utility functions are directly indicative of the welfare for the agents, this facilitates defining global fairness and welfare criteria in terms of the utilities of the agents. This motivates the following questions:

When agents strategize on data contribution to maximize their net utility, does a Nash equilibrium always exist? If yes, then can agents actually _discover a Nash equilibrium_ when acting independently? Can any welfare function of utilities of the agents be optimized at such a Nash equilibrium by _designing new rules_ (_mechanisms_)?

The goal of this paper is to address the above questions. Before we describe our contributions, we note that Karimireddy et al. (2022) also consider a framework similar to ours, by defining the utility function to be payoff minus cost. However, their focus is _data maximization_ and avoiding _free-riding_, while the focus of our paper is to design mechanisms that achieve fairness and welfare for all agents. Furthermore, our model generalizes the model of Karimireddy et al. (2022) as we do not require the agents' data to be identically distributed and in turn to have identical payoff functions.

### Our Contributions

We work under the _concavity assumption_ on the utility functions. In particular, we assume that the payoff functions are concave and cost functions are convex. These assumptions are standard in the literature (Blum et al. (2021); Karimireddy et al. (2022)). Convexity of cost functions is a natural choice since it captures the property of increasing marginal costs. For instance, data sharing through _ordered selection_, i.e., sharing records in ascending order of costs involved for collecting the records, results in convex cost functions. There are more models that result in strictly convex cost functions (Li and Raghunathan (2014)). Similarly, several important ML models exhibit concave payoff functions; for instance payoffs in linear or random discovery models, random coverage models, and general PAC learning are all concave - see Section 2.1. Furthermore, there is empirical evidence that the accuracy function in neural networks under the cross-entropy loss is also concave (Kaplan et al. (2020)).

Existence and Reachability of Nash Equilibrium:Under the concavity assumptions, we show that a Nash equilibrium (NE) is guaranteed to exist. We note that our existence result holds for more general settings than those of Blum et al. (2021) - we do not assume that the cost functions are linear, or that the utility functions have bounded derivatives. In particular, our result shows the existence of NE under the general PAC learning framework (see Section 2.1); under this setting an equilibrium in the model of Blum et al. (2021) fails to exist. We also demonstrate that if the concavity assumption is relaxed, there exist instances which do not admit NE.

Furthermore, we show that the agents can _discover_ a NE through an intuitive _best response dynamics_, where agents update their data contribution proportional to the gradient of their utility functions in the direction that increases their utility. We show that for strongly concave utility functions and under the mild assumption of the gradient of the utilities being bounded, the best response dynamics convergesto an approximate NE in time polynomial in \(O(\log(\varepsilon^{-1}))\). This contribution may be of independent interest to equilibrium computation in concave games.

A Fair and Welfare Maximizing Mechanism:In general, a Nash equilibrium need not be fair or maximize any notion of welfare for the agents (see Example 1). Therefore, we next focus on designing mechanisms that optimize welfare of the agents at its NE. The generalized \(p\)-mean welfare, defined to be the \(p^{th}\)-norm of the utilities of the agents \((\frac{1}{n}\sum_{i\in[n]}u_{i}(\bm{s})^{p})^{\frac{1}{p}}\) for \(p\in(-\infty,1]\), constitutes a family of functions characterized by natural fairness axioms including the Pigou-Dalton principle (Moulin (2003)). This notion encompasses well studied notions of fairness and economic welfare, such as (i) the average social welfare \((\frac{1}{n}\sum_{i\in[n]}u_{i}(\bm{s}))\) when \(p=1\), (ii) the egalitarian welfare \((\min_{i\in[n]}u_{i}(\bm{s}))\) when \(p=-\infty\), which is a fundamental measure of fairness, and (iii) the Nash welfare \((\prod_{i\in[n]}u_{i}(\bm{s}))^{1/n}\) in the limiting \(p\to 0\) case, which is a popular notion in social choice theory that achieves a balance between welfare and fairness (Varian (1974); Caragiannis et al. (2019)).

As our second main contribution, for linear costs, we design a _budget-balanced mechanism_ that always admits a Nash equilibrium. The mechanism involves _payments_, however, budget-balancedness ensures that the total payment of all the agents is zero, and thereby the central server operates on no-profit-no-loss. Moreover we show that when the sample vector at NE is positive, i.e., all agents contribute positive amount of data samples, then the NE maximizes the \(p\)-mean welfare among all positive sample vectors. Since we can ensure the server does not communicate with an agent who does not contribute any data points, insisting on positive sample vectors is a mild assumption.

Our mechanism builds on ideas from a mechanism of Falkinger et al. (2000) for the efficient provisioning of public goods. The key idea is to compensate an agent who incurs a cost higher than the average cost incurred by other agents via a subsidy proportional to her excess cost; likewise, agents incurring a lower cost than average of others are proportionally taxed. By setting the level of tax/subsidy carefully, we show that the NE of the mechanism are \(p\)-mean welfare maximizing. To the best of our knowledge, our work is the first to explore this intimate connection between FL and public goods provisioning. Our results highlight the promise of bridging the fields of machine learning and social choice theory.

Once we have established the mechanism, we show that a corollary of our first main result implies that the _best response dynamics_ under this mechanism will lead to the discovery of the \(p\)-mean welfare maximizing Nash equilibrium.

Empirical Evaluation:We design a distributed training protocol for FL based on our mechanism, called FedBR-BG. We compare our algorithm with three other algorithms: the distributed protocol for the vanilla mechanism without budget balancing FedBR, the standard federated learning protocol FedAvg (McMahan et al. (2017)), and a recent adaptation of FedAvg called MWFed (Blum et al. (2021)). We show that our budget-balancing algorithm achieves superior \(p\)-mean welfare under different settings on two datasets, MNIST (LeCun et al. (2010)) and CIFAR-10 (Krizhevsky (2009)).

### Related Work

We mention some additional literature on welfare maximization and incentives in FL, as well as other related mechanisms in public goods theory.

Welfare maximization in FL.Typically federated learning protocols compute a model which maximize some weighted sum of agent accuracies (i.e. utilities). Examples of such protocols include the standard FedAvg (McMahan et al. (2017)), AFL (agnostic FL) (Mohri et al. (2019)), and \(q\)-FFL (Li et al. (2020)). However, unlike our work, all these methods ignore the strategic aspects arising from costs involved in data sharing, and instead assume agents honestly contribute all their data.

Incentives in FL.This line of work adopts game theory for incentivizing the contribution of data owners. Common models include the Stackelberg game, non-cooperative game, and coalition game. More specifically, the Stackelberg game is employed to optimize the utility of both the server and agents Feng et al. (2019). On the other hand, in non-cooperative games, the server or the agent seeks to maximize its own utility Zou et al. (2019). While most previous methods analyze the properties of a certain scenario, we aim to design mechanisms that achieve fairness and welfare for all agents.

Mechanisms for public good provisioning.There is a long line of work for the efficient provisioning of public goods, beginning from Samuelson (1954). Several works such as Falkinger et al. (2000); Andreoni and Bergstrom (1996) and Bergstrom et al. (1986) use the idea of imposing a tax/subsidy on the agents but differ in the specific manner in which this tax/subsidy is imposed. While our mechanism is inspired by Falkinger et al. (2000), our model is more general than theirs. The design of our mechanism for this general model and the proof of its properties are our novel contributions.

## 2 Problem formulation

We consider a federated learning problem with \(n\) agents who wish to jointly solve a common learning problem. Let \(\mathcal{D}_{i}\) be the distribution of data points available to agent \(i\). Towards jointly solving the learning problem, each agent \(i\) contributes some set \(T_{i}\sim\mathcal{D}_{i}^{s_{i}}\) of \(s_{i}\) data samples. Under the assumption that agent \(i\)'s data is i.i.d. from their distribution \(\mathcal{D}_{i}\), each agent's contribution can be captured simply by their contribution level, i.e., amount of data samples they contribute. Let \(S_{i}\subseteq\mathbb{R}_{\geq 0}\) be the set of feasible contribution levels of agent \(i\), and let \(\mathcal{S}:=\bigtimes_{j}S_{j}\). Given a _sample vector_\(\bm{s}=(s_{1},\dots,s_{n})\in\mathcal{S}\), the central server returns model(s) trained using the samples \(\bigcup_{i}T_{i}\).

In our model, each agent \(i\) derives a _payoff_ from the jointly learned model, e.g. the payoff could be the accuracy of the model. We assume a general framework which models the payoff of agent \(i\) as a function \(a_{i}:\mathcal{S}\to\mathbb{R}_{\geq 0}\). We typically assume each payoff function \(a_{i}\) is bounded, continuous in \(\bm{s}\), and non-decreasing in the contribution \(s_{i}\) of agent \(i\). Moreover each agent \(i\) incurs a _cost_ associated with data sharing captured through a non-decreasing cost function \(c_{i}:S_{i}\to\mathbb{R}_{\geq 0}\). The net utility of agent \(i\) is the payoff minus cost, _i.e.,_

\[u_{i}(\bm{s})=a_{i}(\bm{s})-c_{i}(s_{i}).\]

Given the above framework, the goal of an agent \(i\) is to decide how many samples to contribute so that her net utility \(u_{i}(\cdot)\) is maximized. Note that the utility of agent \(i\) depends on the contributions of other agents as well. Further, we can assume without loss of generality that each set \(S_{i}=[0,\tau_{i}]\) for some threshold \(\tau_{i}>0\). This is because we can discard contribution levels where an agent gets negative utility. Since the payoff \(a_{i}(\bm{s})\) is bounded above by some constant \(A_{i}>0\) and costs are increasing, agent \(i\) cannot obtain a positive utility by contributing more than \(\tau_{i}:=c_{i}^{-1}(A_{i})\) samples.

Payoff and cost functions are assumed to be concave and convex respectively (Blum et al. (2021); Karimireddy et al. (2022)). As discussed in Section 1.1, it is natural to assume that cost functions are convex to capture increasing marginal costs (Li and Raghunathan (2014)). Similarly, there are ample justifications to concave payoff functions, as discussed in Section 2.1 where we analyze payoff functions arising from some of the canonical learning paradigms, and their _concavity properties_.

_Remark 1_.: Our framework generalizes those of Blum et al. (2021) and Karimireddy et al. (2022). In the former, an agent's goal is to contribute the fewest number of data samples subject to ensuring that their payoff crosses a certain threshold; and in the latter all agents have a common payoff function that is a function of \(\left\lVert s\right\rVert_{1}=s_{1}+\dots+s_{n}\), and linear cost functions.

**Nash Equilibrium (NE).** Arguably the most sought after solution concept within game theory is of _Nash Equilibrium_(Nash (1951)), a stable state or an equilibrium state of the system where no agent gains by unilaterally changing their data contribution level. Formally,

**Definition 1** (Nash equilibrium (NE)).: A sample vector \(\bm{s}\in\mathcal{S}\) is said to be at a Nash equilibrium if for every \(i\in[n]\), and every \(s^{\prime}_{i}\), we have \(u_{i}(\bm{s})\geq u_{i}(s^{\prime}_{i},\bm{s}_{-i})\) where \((s^{\prime}_{i},\bm{s}_{-i})=(s_{1},\dots,s^{\prime}_{i},\dots,s_{n})\).

An alternate view of a Nash equilibrium relies on the concept of _best response_. Given the sample contributions \(\bm{s}_{-i}\) of other agents, the set \(f_{i}(\bm{s}_{-i})\) of contribution levels of agent \(i\) that maximize her utility is the best response set of agent \(i\):

\[f_{i}(\bm{s}_{-i})=\arg\max_{x\in S_{i}}\left\{a_{i}(x,\bm{s}_{-i})-c_{i}(x) \right\}\subseteq S_{i}.\]

The _best response correspondence_\(f\) is then defined as a set-valued function \(f:\mathcal{S}\to\bigtimes_{i}2^{S_{i}}\), where \([f(\bm{s})]_{i}=f_{i}(\bm{s}_{-i})\). It is then clear that:

**Proposition 1**.: _A sample vector \(\bm{s}\in\mathcal{S}\) is a Nash equilibrium if and only if it is a fixed point of the best response correspondence, i.e., \(\bm{s}\in f(\bm{s})\)._

### Canonical examples of payoff functions

We now discuss a few examples of payoff functions that are captured by our general framework. In all the examples below, the payoff functions \(a_{i}(\bm{s})\) are non-negative, bounded, continuous, non-decreasing, and concave in \(s_{i}\) for any fixed strategy profile \(\bm{s}_{-i}\) of the other agents.

Linear or Random discovery.In this model, the payoff is linear in the sample vector and is given by \(a_{i}(\bm{s})=(W\bm{s})_{i}\) for a matrix \(W\). For example, Blum et al. (2021) consider a setting where each agent has a sampling probability distribution \(\bm{q}_{i}\) over the instance space \(X\) and gets a reward equalling \(q_{ix}\) whenever the instance \(x\) is sampled by any agent. Then the expected payoff to agent \(i\) is \(a_{i}(\bm{s})=(QQ^{T}\bm{s})_{i}\), where \(Q\) is a matrix given by \(Q[i,x]=q_{ix}\) for \(i\in[n]\) and \(x\in X\). Here \(W=QQ^{T}\) is a symmetric PSD matrix with an all one diagonal.

Random coverage.In the above setting, agent \(i\) obtains a reward \(q_{ix}\) each time some agent samples instance \(x\). In the random coverage model arising in binary classification (Blum et al. (2021)), an agent gets this reward only once. Under this model, the payoff given by expected accuracy takes the form:

\[a_{i}(\bm{s})=1-\frac{1}{2}\sum_{x\in X}q_{ix}\prod_{j=1}^{n}(1-q_{jx})^{s_{j} }\in[0,1].\] (1)

Generalization bounds from general PAC learning.Consider a general learning setting where we want to learn a model \(h\) from a hypothesis class \(\mathcal{H}\) which minimizes the error over a data distribution \(\mathcal{D}\) given by \(R(h)=\mathbb{E}_{(x,y)\in\mathcal{D}}e(h(x),y)\), for some loss function \(e(\cdot)\). Given \(m\) i.i.d. data points, the empirical risk minimizer (ERM) can be computed as the model \(h_{m}=\arg\min_{h\in\mathcal{H}}\sum_{\ell\in[m]}e(h(x_{\ell}),y_{\ell})\). Mohri et al. (2018) show the following bound on the error of \(h_{m}\), which holds with high probability:

\[1-R(h_{m})\geq a(m):=a_{0}-\frac{4+\sqrt{2k(2+\log(m/k))}}{\sqrt{m}},\] (2)

where \((1-a_{0})\) is the accuracy of the optimal model from \(\mathcal{H}\), and \(k\) is a (constant) measure of the difficulty of the learning problem depending on \(e(\cdot)\) and \(\mathcal{H}\). Using this we can define the agent payoff \(a_{i}\) function as the accuracy of the learning task as \(a_{i}(\bm{s})=a(\left\lVert\bm{s}\right\rVert_{1})\)1.

Footnote 1: We define \(a(\bm{0})=0\).

Empirical evidence.Kaplan et al. (2020) discuss empirical scaling laws relating the cross-entropy loss on neural language models. They observe that the loss scales with the dataset size \(m\) as a power law \(\ell(m)=\alpha\cdot m^{-\beta}\), for some parameters \(\alpha>0\) and \(\beta\in(0,1]\). This naturally defines the payoff function as the accuracy of the learning task as

\[a_{i}(\bm{s})=1-\alpha_{i}\cdot\left\lVert\bm{s}\right\rVert_{1}^{-\beta_{i}}.\] (3)

In addition, the pay-off functions of current large language models (e.g., accuracies) are also non-negative and non-decreasing as a function of the data size (Henighan et al. (2020)).

## 3 Nash Equilibrium: Existence and Best Response Dynamics

In this section, we investigate the existence and computation of a Nash equilibrium. We start with two positive results showing the existence of a Nash equilibrium for a broad class of payoff and cost functions.

**Theorem 3.1**.: _A Nash equilibrium exists in any federated learning problem where for every agent \(i\) the payoff function \(a_{i}(\bm{s})\) is continuous in \(\bm{s}\) and concave in \(s_{i}\), and cost function \(c_{i}\) is increasing and convex in \(s_{i}\)._

Proof.: We will show the existence of a Nash equilibrium by showing that the best response correspondence \(f\) has a fixed point. First observe that \(f\) is defined over a compact, convex domain \(\mathcal{S}=\bigtimes_{j}S_{j}\) since each \(S_{j}\) is convex. Next, note that agent \(i\)'s utility function \(u_{i}(\bm{s})=a_{i}(\bm{s})-c_{i}(s_{i})\) is continuous in \(\bm{s}\) due to the continuity of \(a_{i}\) and \(c_{i}\). The continuity of \(u_{i}\) in \(\bm{s}\) and the concavity of\(u_{i}\) in \(s_{i}\) implies the upper semi-continuity of the best response correspondence \(f_{i}\). Moreover, \(u_{i}\) is concave in \(s_{i}\) for each fixed \(\bm{s}_{-i}\), since \(a_{i}\) and \(-c_{i}\) are concave in \(s_{i}\). Thus for each fixed \(s_{-i}\), the best response set \(f_{i}(s_{-i})\subseteq\mathbb{R}_{\geq 0}\) is a non-empty interval, and hence is also convex. Thus \(f\) is a upper semi-continuous non-empty and convex valued correspondence defined over a compact, convex domain. By the Kakutani fixed-point theorem (Kakutani [1941]), \(f\) admits a fixed point.

The above result can also be proved directly by invoking Rosen ([1965]), who showed the existence of a Nash equilibrium of \(n\)-person concave games, where the utility function of an agent \(i\) is defined over closed, compact set, is continuous and concave in \(i\)'s own strategy. 

Implications.Theorem 3.1 shows that a Nash equilibrium exists when payoffs are concave and costs are convex. All our motivating examples from Section 2.1 lie under this concave/convex regime and therefore admit a NE. In Appendix A we discuss existence and non-existence of Nash equilibrium in our model when we go beyond the concave-convex regime of payoff and cost functions. In particular, we show that Nash equilibrium exists even with decreasing payoff function of an agent as long as the function is separable between her and other agents' data contribution. We also show that a NE need not exist even with linear cost functions if the payoff functions are non-concave.

Best response dynamics.We now turn to computation and consider a natural procedure by which agents can find a Nash equilibrium: _best response (BR) dynamics_. Agents start with some initial sample vector \(\bm{s}^{0}\). In each step \(t\) of the BR dynamics, every agent \(i\) updates their sample contribution proportional to the gradient \(\frac{\partial u_{i}}{\partial s_{i}}\) in the direction of increasing utility. Concretely, for a scalar step size \(\delta^{t}>0\), the updates take the following form:

\[\bm{s}^{t+1}=\bm{s}^{t}+\delta^{t}\cdot g(\bm{s}^{t},\bm{\mu}^{t}),\] (4)

where \(g(\bm{s}^{t},\bm{\mu}^{t})_{i}=\frac{\partial u_{i}(\bm{s}^{t})}{\partial s_{ i}}+\mu_{i}^{t}\) and \(\bm{\mu}^{t}\) is chosen so that the updated sample vector \(\bm{s}^{t+1}\) lies in the feasible region \(\mathcal{S}\). Specifically, \(\bm{\mu}^{t}\in\arg\min_{\bm{\mu}\in K}\left\|g(\bm{s}^{t},\bm{\mu})\right\|_{2}\), where \(K=\{\bm{\mu}:\bm{s}^{t}+\delta^{t}\cdot g(\bm{s}^{t},\bm{\mu})\in\mathcal{S}\}\). For instance, if \(0\leq s_{i}\leq\tau_{i}\), then:

\[\mu_{i}^{t}=\begin{cases}-\frac{\partial u_{i}}{\partial s_{i}},\text{ if }s_{i}=0\text{ and }\frac{\partial u_{i}}{\partial s_{i}}<0,\text{ or }s_{i}=\tau_{i}\text{ and }\frac{ \partial u_{i}}{\partial s_{i}}>0\\ 0,\text{ otherwise.}\end{cases}\]

We measure convergence of the above dynamics to a Nash equilibrium via the \(L^{2}\) norm of the update direction \(g(\bm{s}^{t},\bm{\mu}^{t})\). Under mild assumptions on the utility functions, we show the dynamics (4) converges to an approximate Nash equilibrium where \(\|g(\bm{s}^{t},\bm{\mu}^{t})\|_{2}<\varepsilon\):

**Theorem 3.2**.: _Let \(G(\bm{s})\) be the Jacobian of \(\bm{u}:\mathcal{S}\rightarrow\mathbb{R}^{n}\), i.e., \(G(\bm{s})_{ij}=\frac{\partial^{2}u_{i}(\bm{s})}{\partial s_{j}\partial s_{i}}\). Assuming agent utility functions \(u_{i}\) satisfy_

1. _Strong concavity:_ \((G+\lambda\cdot I_{n\times n})\) _is negative semi-definite,_
2. _Bounded derivatives:_ \(|G_{ij}|\leq L\)_,_

_for constants \(\lambda,L>0\), the best response dynamics (4) with step size \(\delta^{t}=\frac{\lambda}{n^{2}L^{2}}\) converges to an approximate Nash equilibrium \(\bm{s}^{T}\) where \(\left\|g(\bm{s}^{T},\bm{\mu}^{T})\right\|_{2}<\varepsilon\) in \(T\) iterations, where_

\[T=\frac{2n^{2}L^{2}}{\lambda^{2}}\log\bigg{(}\frac{\left\|g(\bm{s}^{0},\bm{ \mu}^{0})\right\|_{2}}{\varepsilon}\bigg{)}.\]

Below we sketch the proof and defer the full proof to Appendix A.

Proof sketch.: We measure convergence of the above dynamics by the error term \(\left\|g(\bm{s}^{t},\bm{\mu}^{t})\right\|_{2}\). We show the following bound:

\[\left\|g(\bm{s}^{t+1},\bm{\mu}^{t})\right\|_{2}^{2}\leq\left\|g(\bm{s}^{t},\bm {\mu}^{t})\right\|_{2}^{2}+\delta_{t}^{2}\cdot\left\|G(\bm{s}^{\prime})g(\bm{s }^{t},\bm{\mu}^{t})\right\|_{2}^{2}+2\delta_{t}g(\bm{s}^{t},\bm{\mu}^{t})^{T} G(\bm{s}^{\prime})g(\bm{s}^{t},\bm{\mu}^{t}).\]

We then use strong concavity to show \(g(\bm{s}^{t},\bm{\mu}^{t})^{T}G(\bm{s}^{\prime})g(\bm{s}^{t},\bm{\mu}^{t}) \leq-\lambda\left\|g(\bm{s}^{t},\bm{\mu}^{t})\right\|_{2}^{2}\), and the bounded derivatives property to show \(\left\|G(\bm{s}^{\prime})g(\bm{s}^{t},\bm{\mu}^{t})\right\|_{2}\leq nL\left\|g( \bm{s}^{t},\mu^{t})\right\|_{2}\). For our choice of the step size \(\delta^{t}=\frac{\lambda}{n^{2}L^{2}}\), we can relate the error in subsequent iterations as follows:

\[\left\|g(\bm{s}^{t+1},\bm{\mu}^{t+1})\right\|_{2}^{2}\leq\left(1-\frac{\lambda ^{2}}{n^{2}L^{2}}\right)\cdot\left\|g(\bm{s}^{t},\bm{\mu}^{t})\right\|_{2}^{2}.\]This then allows us to conclude that after \(T=\frac{2n^{2}L^{2}}{\lambda^{2}}\log\left(\frac{\|g(\bm{s}^{0},\mu^{0})\|_{2}}{ \varepsilon}\right)\) iterations, we will have an approximate Nash equilibrium \(\bm{s}^{T}\) with \(\|g(\bm{s}^{T},\bm{\mu}^{T})\|_{2}\leq\varepsilon\). 

Implications.The above theorem implies that agents playing the natural best response update strategy will converge quickly (in \(O(\log(\varepsilon^{-1}))\) iterations) to a NE. We note that our theorem applies to the payoff functions defined by generalization bounds (eq. (2)) and observed in practice (eq. (3)) as they are strongly concave and have bounded derivatives. Moreover, under the assumptions of Theorem 3.2, our proof also implies the fast convergence of the best response dynamics in the budget-balanced mechanism we discuss in the next section.

## 4 Welfare Maximization: A Budget-Balanced Mechanism

We first note through an example that Nash equilibria need not be welfare maximizing.

_Example 1_.: Consider two agents with identical payoff functions \(a(\bm{s})=8-(s_{1}+s_{2})^{-1}\), and linear cost functions given by \(c_{1}(s_{1})=5c_{1}\) and \(c_{2}(s_{2})=4c_{2}\). The NE is given by \(\bm{s}^{*}=(0,0.5)\), i.e., agent 1 does not contribute any data samples. The Nash welfare (which is the \(p\)-mean welfare in \(p\to 0\) limiting case) of this NE is \(u_{1}(\bm{s}^{*})\cdot u_{2}(\bm{s}^{*})=24\). However consider another sample vector \(\bm{s}^{\prime}=(0.2,0.4)\) where agent \(1\) increases her contribution and agent \(2\) reduces her contribution. Then \(\bm{s}^{\prime}\) has a higher Nash welfare of \(u_{1}(\bm{s}^{\prime})\cdot u_{2}(\bm{s}^{\prime})=25.25>24\).

To address the issue of NE not being welfare-maximizing, by designing a budget-balanced mechanism for agents with linear cost functions. Our mechanism is inspired from a mechanism for the efficient provisioning public goods (Falkinger et al. (2000)). We show that our mechanism always admits a NE. Moreover, when the sample vector at NE is positive, i.e. every agent participates by contributing data, the NE maximizes the \(p\)-mean welfare among all positive sample vectors. For the FL setting, assuming positive sample vectors is natural since the only way for an agent to participate is by making some positive data contribution.

We present our result for a more general model than one discussed so far. Not only does this generalization strengthen our result, it also naturally allows expressing agent utilities in terms of the taxes/subsidies they receive from our mechanism. In this generalization, each agent \(i\) has a budget \(B_{i}\) of which \(b_{i}\) is unspent and the remaining is used towards the cost of sampling \(s_{i}\) data points, i.e., \(b_{i}+c_{i}(s_{i})=B_{i}\). We assume agents have arbitrary continuous, concave utility functions of the form \(u_{i}(b_{i},\left\|\bm{s}\right\|_{1})\). Agents have linear cost functions \(c_{i}(s_{i})=c_{i}\cdot s_{i}+d_{i}\), with \(c_{i}>0\). This model already captures some previously discussed settings (Sec 2.1) as follows. For e.g., when payoff functions are derived from generalization bounds (Mohri et al. (2018)) or empirical evidence (Kaplan et al. (2020)), they take the form \(a_{i}(\bm{s})=\hat{a}_{i}(\left\|\bm{s}\right\|_{1})\), for some function \(\hat{a}_{i}\) which depends on \(\left\|\bm{s}\right\|_{1}\). Then for \(u_{i}(b_{i},\left\|\bm{s}\right\|_{1})=b_{i}+\hat{a}(\left\|\bm{s}\right\|_{1})\), the budget constraint implies the utility takes the form \(\hat{a}_{i}(\left\|\bm{s}\right\|_{1})-c_{i}(s_{i})+B_{i}\), which is the same as the utility under the original model with a constant offset. We also assume that for all \(i\), \(\frac{\partial u_{i}(b_{i},S)}{\partial S}>0\) and \(\frac{\partial u_{i}(b_{i},S)}{\partial b_{i}}\neq 0\).

**Mechanism \(\mathcal{M}_{\beta}\).** We design a mechanism parametrized by a scalar \(\beta\in(0,1)\). It uses the following payment scheme. At a data contribution vector \(\bm{s}\), each agent \(i\) is rewarded an amount

\[p_{i}(\bm{s})=\beta(c_{i}(s_{i})-\frac{1}{n-1}\sum_{j\neq i}c_{j}(s_{j})).\]

Thus if an agent incurs higher (resp. lower) sampling cost than the average cost borne by other agents, we compensate (resp. penalize) her by a subsidy (resp. tax) of \(\beta\) times her excess cost. By design, our mechanism is budget-balanced: at any sample vector \(\bm{s}\), we have:

\[\sum_{i}p_{i}(\bm{s})=\beta(\sum_{i}c_{i}(s_{i})-\frac{1}{n-1}\sum_{j\neq i}c_ {j}(s_{j}))=0.\]Under this mechanism, given a vector of contributions \(\bm{s}_{-i}\) of agents other than agent \(i\), the best response of agent \(i\) is any solution to the following optimization problem:

\[\max u_{i}(b_{i},s_{i}+\left\|s_{-i}\right\|_{1})\] s.t. \[\forall i:\;b_{i}+(1-\beta)c_{i}(s_{i})+\frac{\beta}{n-1}\sum_{j \neq i}c_{j}(s_{j})=B_{i}\] (5) \[\forall i:\;b_{i}\geq 0\]

We next define a \(\beta\) value that plays a crucial role in our mechanism.

**Definition 2**.: (Optimal parameter \(\beta^{*}\)) Let \(A:=(\sum_{i}c_{i}^{-1})^{-1}\) and \(C:=\sum_{i}c_{i}\). Then we define \(\beta^{*}\) as the solution to the following equation.

\[C\beta^{2}-(An(n-2)+C)\beta+A(n-1)^{2}=0,\] (6)

which satisfies \(0\leq\beta^{*}\leq 1-1/n\).

The next lemma shows that (6) indeed has such a root - the proof is deferred to Appendix B.

**Lemma 1**.: _The equation \(C\beta^{2}-(An(n-2)+C)\beta+A(n-1)^{2}=0\) of (6) has a real root \(\beta^{*}\) where \(0\leq\beta^{*}\leq 1-1/n\)._

We now state the main result of this section. We show that for every \(\beta\in[0,1]\), a Nash equilibrium of \(\mathcal{M}_{\beta}\) exists. Additionally, for a specific value of \(\beta=\beta^{*}\) (Definition 2), our mechanism admits Nash equilibria which are socially efficient: it maximizes the \(p\)-mean welfare: \(W_{p}(\bm{b},\bm{s})=(\sum_{i}u_{i}(b_{i},\left\|s\right\|_{1})^{p})^{1/p}\), for \(p\leq 1\) among all positive sample vectors \(\bm{s}\).

**Theorem 4.1**.: _For each \(\beta\in[0,1]\), the mechanism \(\mathcal{M}_{\beta}\) admits a Nash equilibrium. For \(\beta=\beta^{*}\) (Definition 2), whenever the NE \(\bm{s}^{*}\) of \(\mathcal{M}_{\beta^{*}}\) satisfies \(\bm{s}^{*}>0\), the NE \(\bm{s}^{*}\) maximizes the \(p\)-mean welfare among all vectors \(\bm{s}>0\), for any \(p\leq 1\)._

We now sketch the proof of the above theorem. The full proof is deferred to Appendix B.

Proof sketch.: When \(0\leq\beta\leq 1\), the first constraint in the above program is a convex constraint even for general convex cost functions. Since \(u_{i}(\cdot)\) is concave, a proof similar to the proof of Theorem 3.1 shows the existence of a Nash equilibrium.

The program maximizing \(p\)-mean welfare as follows.

\[\max W_{p}(\bm{b},\bm{s}):=(\sum_{i}u_{i}(b_{i},\left\|s\right\|_{1})^{p})^{1 /p}\] s.t. \[\forall i:\;b_{i}+(1-\beta)c_{i}(s_{i})+\frac{\beta}{n-1}\sum_{j \neq i}c_{j}(s_{j})=B_{i}\] (7) \[\forall i:\;b_{i}\geq 0\]

We first show that the above program is convex. With \(\mu_{i}\) and \(\gamma_{i}\) as the dual variables to the first and second constraints respectively, we write down the KKT conditions of program (7) with \(s_{i}>0\). We then use the KKT conditions satisfied by a \(\operatorname{NE}\left(\bm{b}^{*},\bm{s}^{*}\right)\) to find values of the dual variables \(\bm{\mu}^{*}\) and \(\bm{\gamma}^{*}\) so that \((\bm{b}^{*},\bm{s}^{*},\bm{\mu}^{*},\bm{\gamma}^{*})\) satisfy the KKT conditions of (7). Since KKT conditions are sufficient for optimality, this shows that the NE \((\bm{b}^{*},\bm{s}^{*})\) also maximizes \(p\)-mean welfare. 

**Implications.** Theorem 4.1 shows that by augmenting the federated learning with a simple payment protocol that is budget-balanced, one can obtain NE that maximize any \(p\)-mean welfare function of the net-utilities of the agents. Furthermore, note that the payment augmented utility function is still essentially of the form \(u_{i}(\bm{s})=\hat{a}_{i}(\bm{s})-(1-\beta)c_{i}(s_{i})-\frac{\beta}{n-1}c_{j} (s_{j})\), with a constant offset. Since \(\beta\in[0,1]\), when \(a_{i}(\cdot)\) are \(\lambda\)-strongly concave and \(c_{i}(\cdot)\) are \(\lambda\)-strongly convex, the functions \(u_{i}(\cdot)\) are \(\lambda\)-strongly concave. Therefore Theorem 3.2 applies, which ensures that the welfare maximizing NE can be reached through the simple best response dynamics quickly. Finally, we remark that when cost functions are identical, the value of the optimal parameter \(\beta^{*}\) is \((1-1/n)\), which is exactly the value used by Falkinger et al. (2000).

_Remark 2_.: Our mechanism requires that costs of the agents be publicly known in order to calculate the value of \(\beta^{*}\) by solving (6) in Definition 2. This is a common assumption made in previous work (e.g. Karimireddy et al. (2022) and Blum et al. (2021)) and is justified in practice. Indeed, costs are common knowledge in many real-world applications e.g. (1) in many ML applications, each agent derives their training data from manually labeling a subset of a publicly available dataset like CIFAR or ImageNet, and the cost of labeling dataset is usually known; (2) in autonomous driving, where each data point is a random path taken under random external conditions.

## 5 Distributed Algorithm and Empirical Evaluation

In this section, we realize our budget-balanced mechanism in a real-world FL system. We perform the evaluation on the MNIST (LeCun et al. (2010)) and CIFAR-10 (Krizhevsky (2009)) datasets. We compare our mechanism with two baselines: the standard FedAvg and MWFed Blum et al. (2021). We denote the vanilla mechanism without budget balancing as FedBR and the budget-balanced mechanism as FedBR-BG. We demonstrate that compared to the FedBR, FedBR-BG achieves better \(p\)-mean welfare for \(p\leq 1\). We also show that the standard FL protocol FedAvg gives significantly lower welfare since it does not allow agents to change their contribution.

### Algorithm Details

We first define the concrete forms for payoff and cost functions. According to standard practice in FL, an agents payoff is measured through the accuracy evaluated on her local test data given model \(\theta\), which has the form \(a_{i}(\bm{s},\theta)=\frac{1}{\left|D_{i}^{*}\right|}\sum_{(x,y)\in D_{i}^{*} }\mathds{1}[\theta(x)=y]\), where \(D_{i}^{*}\) is the test data for agent \(i\). We note that this form inherently aligns with \(a_{i}(\bm{s})=\hat{a}_{i}(\left\lVert\bm{s}\right\rVert_{1})\) because the received global model \(\theta\) is trained on \(\left\lVert\bm{s}\right\rVert_{1}\) samples. We consider linear cost functions where \(c_{i}(s_{i})=c_{i}s_{i}\).

Now we derive the update rule for agent contributions for FedBR-BG based on best-response dynamics. We rewrite the utility of agent \(i\) in the budget-balancing mechanism as \(u_{i}(\bm{s})=\hat{a}_{i}(\sum_{i}s_{i})-(1-\beta)c_{i}s_{i}-\frac{\beta}{n-1 }\sum_{j\neq i}c_{j}s_{j}\). We then compute its gradient with respect to \(s_{i}\): \(\frac{\partial u_{i}}{\partial s_{i}}=\frac{\partial\hat{a}_{i}(\sum_{i}s_{i} )}{\partial s_{i}}-(1-\beta)c_{i}\). Since the accuracy function is generally unknown in practice, the server can train a public accuracy function \(\tilde{a}:\tilde{\mathcal{S}}\rightarrow\mathbb{R}_{\geq 0}\) and broadcast it to all agents. We obtain an estimate of the accuracy \(\tilde{a}\) by evaluating models trained on different numbers of samples, in increasing intervals of \(\Delta s\). For example, if the server trains models on \(0,2,4,\cdots\) samples and evaluates their accuracy to obtain the estimated accuracy \(\tilde{a}\), the interval \(\Delta s=2\) in this case. We assume \((\tau_{1},\dots,\tau_{n})\in\tilde{\mathcal{S}}\) as the server can be a service provider with plenty of data sources. With \(\tilde{a}\), we can approximate the gradient as \(\frac{\partial u_{i}}{\partial s_{i}}\approx\frac{\tilde{a}(\sum_{i}s_{i}+ \Delta s)-\tilde{a}(\sum_{i}s_{i})}{\Delta s}-(1-\beta)c_{i}\). Similarly, \(\frac{\partial u_{i}}{\partial s_{i}}\approx\frac{\tilde{a}(\sum_{i}s_{i}+ \Delta s)-\tilde{a}(\sum_{i}s_{i})}{\Delta s}-c_{i}\) for FedBR, and \(u_{i}(\bm{s})=a_{i}(\sum_{i}s_{i})-c_{i}s_{i}\) for FedAvg, MWFed and FedBR.

We further assume that \(s_{i}\) is a multiple of \(\Delta s\) to ensure that \(\tilde{a}\) is always well-defined for our chosen contributions. Finally, we leverage best-response dynamics to update agent contributions. We present the full description of the algorithm for FedBR-BG and FedBR as Algorithm 1 and Algorithm 2 in Appendix C, respectively.

### Experiment Setup

We conduct the experiments with 10 agents for MNIST and 100 agents for CIFAR-10. For MNIST, we use a CNN as the global model, which has two \(5\times 5\) convolution layers followed by two fully connected layers with ReLU activation. For CIFAR-10, we use VGG11 (Simonyan and Zisserman (2014)). We consider the _i.i.d._ setting, i.e., the local data of agents are sampled from the same distribution. For both datasets, each agent has 100 training images and 10 testing images, i.e.,\(\tau_{i}=100,\forall i\in[n]\).

We randomly initialize the contributions as a multiplier of 10 in \([0,100]\). In each contribution updating step, we re-initialize the global model and perform FedAvg for 50 communication rounds. We set global learning rate \(\eta\) to 1.0, local learning rate \(\alpha\) to 0.01, and momentum to 0.9. We set the number of contribution updating steps to 100 and the sample number interval to 10. For evaluating FedAvg, we simply optimize the global model with the same hyperparameters while keeping individual contribution to \(\tau_{i}\).

### Experiment Results

We show the \(p\)-mean welfare of our method and baselines on MNIST and CIFAR-10 in Table 1. We observe that FedBR-BG achieves better \(p\)-mean welfare on both datasets compared to FedBR and FedAvg, verifying our theoretical results. Note that the \(p\)-mean of FedAvg is significantly lower since agents always contribute all their data in FedAvg, which incurs a high cost so that the marginal gain becomes limited.

## 6 Discussion

In this paper, we formulated a federated learning framework which incorporates both payoffs an agent receives from sharing data as well as the cost she incurs due to sharing data. We show the existence of Nash equilibria under the assumption of concave payoffs and convex costs, which are mild assumptions observed in practice. We then note that while NE exist, they may not maximize any notion of welfare for the agents, leaving agents with less incentive to participate. We address this issue via a budget-balanced mechanism with payments whose NE maximize the \(p\)-mean welfare of the agent utilities. Our experiments show that FedBR-BG achieves better \(p\)-mean welfare compared to FedBR and FedAvg, verifying our theoretical results.

We conclude by discussion some directions for future work. Our mechanism required that costs of the agents be publicly known, or at least verifiable. Studying incentives when costs are not common knowledge is an interesting question. Another assumption of our mechanism was that an agent's payoff depends on the number of data samples. Designing welfare-maximizing mechanisms for general settings where payoff functions take more general forms is another direction for future work.

**Acknowledgements.** This work is partially supported by the National Science Foundation under grant No. 1750436, No. 1910100, No. 2046726, No. 2229876, DARPA GARD, the National Aeronautics and Space Administration (NASA) under grant no. 80NSSC20M0229, the Alfred P. Sloan Fellowship, and the Amazon research award.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{6}{c}{MNIST} & \multicolumn{6}{c}{CIFAR-10} \\ \cline{2-11}  & \(p=0.2\) & \(p=0.4\) & \(p=0.6\) & \(p=0.8\) & \(p=1.0\) & \(p=0.2\) & \(p=0.4\) & \(p=0.6\) & \(p=0.8\) & \(p=1.0\) \\ \hline FedAvg & 48985.23 & 154.99 & 22.763 & 8.726 & 4.909 & 42386.21 & 135.92 & 23.528 & 8.381 & 4.582 \\ MWFeed & 51326.49 & 158.92 & 21.648 & 8.803 & 5.230 & 48178.29 & 142.91 & 23.981 & 8.879 & 4.891 \\ FedBR & 53395.21 & 168.85 & 24.784 & 9.495 & 5.340 & 58297.23 & 178.32 & 26.187 & 9.675 & 5.681 \\ FedBR-BG & **54589.31** & **172.63** & **25.340** & **9.708** & **5.459** & **60385.32** & **183.23** & **27.958** & **9.981** & **5.891** \\ \hline \hline \end{tabular}
\end{table}
Table 1: \(p\)-mean welfare of our budget-balanced mechanism FedBR-BG and baselines on MNIST and CIFAR-10. We report the results for different \(p\). The cost for adding one data sample \(c_{i}\) is 0.005 for every agent.

## References

* Andreoni and Bergstrom (1996) James Andreoni and Ted Bergstrom. Do government subsidies increase the private supply of public goods? _Public Choice_, 88(3-4):295-308, 1996. URL https://EconPapers.repec.org/RePEc:kap:pubcho:v:88:y:1996:i:3-4:p:295-308.
* Bergen and Petryshen (2012) Sarah E Bergen and Tracey L Petryshen. Genome-wide association studies of schizophrenia: does bigger lead to better results? _Current opinion in psychiatry_, 25(2):76-82, 2012.
* Bergstrom et al. (1986) Theodore Bergstrom, Lawrence Blume, and Hal Varian. On the private provision of public goods. _Journal of Public Economics_, 29(1):25-49, 1986. ISSN 0047-2727. doi: https://doi.org/10.1016/0047-2727(86)90024-1. URL https://www.sciencedirect.com/science/article/pii/0047272786900241.
* Blum et al. (2021) Avrim Blum, Nika Haghtalab, Richard Lanas Phillips, and Han Shao. One for one, or all for all: Equilibria and optimality of collaboration in federated learning. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 1005-1014. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/blum21a.html.
* Borgers (2015) Tilman Borgers. _An introduction to the theory of mechanism design_. Oxford University Press, USA, 2015.
* Boyd and Vandenberghe (2004) S.P. Boyd and L. Vandenberghe. _Convex Optimization_. Number pt. 1 in Berichte uber verteilte messysteme. Cambridge University Press, 2004. ISBN 9780521833783. URL https://books.google.com/books?id=mYm0bLd3fcoC.
* Caragiannis et al. (2019) Ioannis Caragiannis, David Kurokawa, Herve Moulin, Ariel D. Procaccia, Nisarg Shah, and Junxing Wang. The unreasonable fairness of maximum Nash welfare. _ACM Trans. Econ. Comput._, 7(3), sep 2019. ISSN 2167-8375.
* Dayan et al. (2021) Ittai Dayan, Holger R Roth, Aoxiao Zhong, Ahmed Harouni, Amilcare Gentili, Anas Z Abidin, Andrew Liu, Anthony Beardsworth Costa, Bradford J Wood, Chien-Sung Tsai, et al. Federated learning for predicting clinical outcomes in patients with covid-19. _Nature medicine_, 27(10):1735-1743, 2021.
* Elbir et al. (2020) Ahmet M Elbir, Burak Soner, and Sinem Coleri. Federated learning in vehicular networks. _arXiv preprint arXiv:2006.01412_, 2020.
* Falkinger et al. (2000) Josef Falkinger, Ernst Fehr, Simon Gachter, and Rudolf Winter-Ebmer. A simple mechanism for the efficient provision of public goods: Experimental evidence. _The American Economic Review_, 90(1):247-264, 2000. ISSN 00028282. URL http://www.jstor.org/stable/117291.
* Feng et al. (2019) Shaohan Feng, Dusit Niyato, Ping Wang, Dong In Kim, and Ying-Chang Liang. Joint service pricing and cooperative relay communication for federated learning. In _2019 International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)_, pages 815-820. IEEE, 2019.
* Henighan et al. (2020) Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. _arXiv preprint arXiv:2010.14701_, 2020.
* Huber et al. (2001) Frank Huber, Andreas Herrmann, and Robert E Morgan. Gaining competitive advantage through customer value oriented management. _Journal of consumer marketing_, 18(1):41-53, 2001.
* Jaisingh et al. (2008) Jeevan Jaisingh, Jack Barron, Shailendra Mehta, and Alok Chaturvedi. Privacy and pricing personal information. _European Journal of Operational Research_, 187(3):857-870, 2008.
* 459, 1941. doi: 10.1215/S0012-7094-41-00838-4. URL https://doi.org/10.1215/S0012-7094-41-00838-4.
* Kakutani et al. (2019)Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _CoRR_, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.
* Karimireddy et al. (2022) Sai Praneeth Karimireddy, Wenshuo Guo, and Michael Jordan. Mechanisms that incentivize data sharing in federated learning. In _Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022)_, 2022. URL https://openreview.net/forum?id=Bx4Sz-N5K3J.
* Krizhevsky (2009) Alex Krizhevsky. Learning multiple layers of features from tiny images. _Technical Report TR-2009, University of Toronto, Toronto_, 2009. URL https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
* Laudon (1996) Kenneth C Laudon. Markets and privacy. _Communications of the ACM_, 39(9):92-104, 1996.
* Learning (2017) Federated Learning. Collaborative machine learning without centralized training data. _Publication date: Thursday, April_, 6, 2017.
* LeCun et al. (2010) Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. _ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist_, 2, 2010.
* Li et al. (2020) Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning. In _ICLR_. OpenReview.net, 2020.
* Li and Raghunathan (2014) Xiao-Bai Li and Srinivasan Raghunathan. Pricing and disseminating customer data with privacy awareness. _Decision support systems_, 59:63-73, 2014.
* McMahan et al. (2017) H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2017.
* Mohri et al. (2018) M. Mohri, A. Rostamizadeh, and A. Talwalkar. _Foundations of Machine Learning, second edition_. Adaptive Computation and Machine Learning series. MIT Press, 2018. ISBN 9780262039406. URL https://books.google.com/books?id=V2B9DwAAQBAJ.
* Mohri et al. (2019) Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In _International Conference on Machine Learning_, pages 4615-4625. PMLR, 2019.
* Moulin (2003) Herve Moulin. _Fair Division and Collective Welfare_. The MIT Press, 01 2003. ISBN 9780262280297. doi: 10.7551/mitpress/2954.001.0001. URL https://doi.org/10.7551/mitpress/2954.001.0001.
* Myerson (1981) Roger B Myerson. Optimal auction design. _Mathematics of operations research_, 6(1):58-73, 1981.
* Nash (1951) John Nash. Non-cooperative games. _Ann. Math._, 54(2):286-295, 1951.
* Nvidia (2019) N Nvidia. Nvidia clara federated learning to deliver ai to hospitals while protecting patient data. _NVIDIA Clara Federated Learning to Deliver AI to Hospitals While Protecting Patient Data_, 2019.
* Rosen (1965) J. B. Rosen. Existence and uniqueness of equilibrium points for concave n-person games. _Econometrica_, 33(3):520-534, 1965. ISSN 00129682, 14680262. URL http://www.jstor.org/stable/1911749.
* Samuelson (1954) Paul A. Samuelson. The pure theory of public expenditure. _The Review of Economics and Statistics_, 36(4):387-389, 1954. ISSN 00346535, 15309142. URL http://www.jstor.org/stable/1925895.
* Simonyan and Zisserman (2014) Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* 91, 1974.
* Xu et al. (2021) Jie Xu, Benjamin S Glicksberg, Chang Su, Peter Walker, Jiang Bian, and Fei Wang. Federated learning for healthcare informatics. _Journal of Healthcare Informatics Research_, 5(1):1-19, 2021.
* Xu et al. (2018)Yuze Zou, Shaohan Feng, Dusit Niyato, Yutao Jiao, Shimin Gong, and Wenqing Cheng. Mobile device training strategies in federated learning: An evolutionary game approach. In _2019 International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)_, pages 874-879. IEEE, 2019.

Appendix to Section 3

We first show that a Nash equilibrium exists when agent payoff functions are _separable_, i.e., for every agent \(i\) there are functions \(g_{i}:S_{i}\rightarrow\mathbb{R}_{\geq 0}\) and \(h_{i}:\bigtimes_{j\neq i}S_{j}\rightarrow\mathbb{R}_{\geq 0}\) s.t. for all \(\bm{s}\in\mathcal{S}\), \(a_{i}(\bm{s})=g_{i}(s_{i})+h_{i}(\bm{s}_{-i})\).

**Theorem A.1**.: _In any federated learning problem where agent payoff functions are separable, a Nash equilibrium exists._

Proof.: When the payoff function of an agent \(i\) is separable, the best response to any contribution vector \(\bm{s}_{-i}\) is independent of \(\bm{s}_{-i}\):

\[f_{i}(\bm{s}_{-i}) =\arg\max_{x\in S_{i}}a_{i}(x,\bm{s}_{-i})-c_{i}(x)=\arg\max_{x\in S _{i}}g_{i}(x)+h_{i}(\bm{s}_{-i})-c_{i}(x)\] \[=\arg\max_{x\in S_{i}}g_{i}(x)-c_{i}(x).\qquad\text{(since $h_{i}(\bm{s}_{-i})$ is independent of $x$)}\]

Let \(F_{i}:=\arg\max_{x\in S_{i}}g_{i}(x)-c_{i}(x)\). Clearly \(F_{i}\neq\emptyset\) since \(S_{i}\neq\emptyset\). Then any \(\bm{s}\in\bigtimes_{i}F_{i}\) satisfies \(\bm{s}\in f(\bm{s})\) by definition. By Proposition 1 any such sample vector is a Nash equilibrium. 

Next, we present a negative result showing that there are federated learning settings where a Nash equilibrium is not guaranteed to exist.

**Theorem A.2**.: _There exists a federated learning problem in which a Nash equilibrium does not exist. Moreover, the instance has three agents with continuous, non-decreasing, non-concave payoff functions and linear cost functions._

Proof.: Let \(\varepsilon\in(0,\frac{1}{16})\). Let \(e:[0,1]\rightarrow[0,1]\) be a function given by:

\[e(x)=\begin{cases}0,\text{ if }0\leq x\leq\frac{1}{2}-\varepsilon,\\ \frac{1}{2}+\frac{1}{2e}(x-\frac{1}{2}),\text{ if }\frac{1}{2}-\varepsilon\leq x \leq\frac{1}{2}+\varepsilon,\\ 1,\text{ if }\frac{1}{2}+\varepsilon\leq x\leq 1.\end{cases}\] (8)

Essentially the function \(e\) is a continuous, piece-wise linear function connecting \((0,0),(\frac{1}{2}-\varepsilon,0),(\frac{1}{2}+\varepsilon,1)\) and \((1,1)\).

Now consider the following federated learning instance with \(n=3\) agents, where \(S_{1}=S_{2}=S_{3}=[0,1]\). The payoff functions are given by:

\[\begin{split} a_{1}(\bm{s})&=e(s_{1})+e(s_{3})-e(s_{1 })\cdot e(s_{3})\\ a_{2}(\bm{s})&=e(s_{2})+e(s_{1})-e(s_{2})\cdot e(s_{1 })\\ a_{3}(\bm{s})&=e(s_{3})+e(s_{2})-e(s_{3})\cdot e(s_{2 }),\end{split}\] (9)

and the cost functions are \(c_{i}(s_{i})=\frac{1}{4}s_{i}\) for all \(i\in[3]\). Notice that the payoff functions are increasing in \(s_{j}\) for every \(j\in[3]\) and are continuous since \(e\) is continuous.

We now show that this instance does not admit a Nash equilibrium. Let us first evaluate the best response set \(f_{1}(s_{2},s_{3})\). Note that \(u_{1}(\bm{s})=e(s_{1})\cdot(1-e(s_{3}))+e(s_{3})-\frac{1}{4}s_{1}\). Since \(u_{1}(\bm{s})\) is independent of \(s_{2}\), \(f_{1}(s_{2},s_{3})\) only depends on \(s_{3}\).

* Case 1. \(s_{3}\leq\frac{1}{2}-\varepsilon\). Then \(u_{1}(\bm{s})=e(s_{1})-\frac{1}{4}s_{1}\), which is maximized at \(s_{1}=\frac{1}{2}+\varepsilon\) and results in a utility of \(\frac{7}{8}-\frac{\varepsilon}{4}\).
* Case 2. \(s_{3}\geq\frac{1}{2}+\varepsilon\). Then \(u_{1}(\bm{s})=1-\frac{1}{4}s_{1}\), which is maximized at \(s_{1}=0\) and results in a utility of \(1\).
* Case 3. \(\frac{1}{2}-\varepsilon\leq s_{3}\leq\frac{1}{2}+\varepsilon\). We consider the intervals in which the best response \(s_{1}\) to such an \(s_{3}\) can lie:
* \(s_{1}\leq\frac{1}{2}-\varepsilon\). In this range, \(u_{1}(\bm{s})=e(s_{3})-\frac{1}{4}s_{1}\), which is maximized at \(s_{1}=0\) and results in a utility of \(e(s_{3})\).
* \(s_{1}\geq\frac{1}{2}+\varepsilon\). In this range, \(u_{1}(\bm{s})=1-\frac{1}{4}s_{1}\), which is maximized at \(s_{1}=\frac{1}{2}+\varepsilon\) and results in a utility of \(\frac{7}{8}-\frac{\varepsilon}{4}\).
* \(\frac{1}{2}-\varepsilon\leq s_{1}\leq\frac{1}{2}+\varepsilon\). In this range, using the definition of \(e(s_{1})\) (eq. 8) we obtain: \[u_{1}(\bm{s})=\bigg{(}\frac{1-e(s_{3})}{2\varepsilon}-\frac{1}{4}\bigg{)} \cdot s_{1}+(1-e(s_{3}))\cdot\bigg{(}\frac{1}{2}-\frac{1}{4\varepsilon}\bigg{)} +e(s_{3}).\] Thus \(u_{1}(\bm{s})\) is a linear function in \(s_{1}\) with slope \(\frac{1-e(s_{3})}{2\varepsilon}-\frac{1}{4}\). If the slope is positive, then the best response in the current interval is \(s_{1}=\frac{1}{2}+\varepsilon\), and gives a utility of \(\frac{7}{8}-\frac{\varepsilon}{4}\). If the slope is negative, then \(s_{1}=\frac{1}{2}-\varepsilon\) is the best response in the current interval and gives a utility of \(e(s_{3})-\frac{1}{4}(\frac{1}{2}-\varepsilon)\). However \(s_{1}=0\) gives a utility of \(e(s_{3})\) implying that \(s_{1}=\frac{1}{2}-\varepsilon\) cannot be a best response. Finally if the slope is zero, then it must mean that \(e(s_{3})=1-\frac{\varepsilon}{2}\), and the utility is \(\frac{\varepsilon}{2}(\frac{1}{2}-\frac{1}{4\varepsilon})+1-\frac{\varepsilon }{2}=\frac{7}{8}-\frac{\varepsilon}{4}\). However responding with \(s_{1}=0\) gives a utility of \(e(s_{3})=1-\frac{\varepsilon}{2}\), which exceeds \(\frac{7}{8}-\frac{\varepsilon}{4}\), since \(\varepsilon<\frac{16}{16}\). Thus, the best response does not lie in \((\frac{1}{2}-\varepsilon,\frac{1}{2}+\varepsilon)\) and \(s_{1}=0\) is the overall best response.

The above discussion shows that the best response \(f_{1}(s_{2},s_{3})\subseteq\{0,\frac{1}{2}+\varepsilon\}\). By symmetry, the same holds for \(f_{2}\) and \(f_{3}\). Suppose there exists a Nash equilibrium \(\bm{s}^{*}=(s_{1}^{*},s_{2}^{*},s_{3}^{*})\). By Proposition 1, \(\bm{s}^{*}\in f(\bm{s}^{*})\). Since the above discussion implies \(s_{3}^{*}\in\{0,\frac{1}{2}+\varepsilon\}\), we consider two cases:

* Suppose \(s_{3}^{*}=0\). Then \[s_{3}^{*}=0 \implies s_{1}^{*}=\frac{1}{2}+\varepsilon\] (Case 1 for agent 1) \[\implies s_{2}^{*}=0\] (Case 2 for agent 2) \[\implies s_{3}^{*}=\frac{1}{2}+\varepsilon,\] (Case 1 for agent 3) which is a contradiction.
* Suppose \(s_{3}^{*}=\frac{1}{2}+\varepsilon\). Then \[s_{3}^{*}=\frac{1}{2}+\varepsilon \implies s_{1}^{*}=0\] (Case 2 for agent 1) \[\implies s_{2}^{*}=\frac{1}{2}+\varepsilon\] (Case 1 for agent 2) \[\implies s_{3}^{*}=0,\] (Case 2 for agent 3) which is also a contradiction.

This shows that there is no \(\bm{s}^{*}\) such that \(\bm{s}^{*}\in f(\bm{s}^{*})\), implying that the above instance does not admit a Nash equilibrium. 

We now prove the fast convergence of best response dynamics.

**Theorem 3.2**.: _Let \(G(\bm{s})\) be the Jacobian of \(\bm{u}:\mathcal{S}\to\mathbb{R}^{n}\), i.e., \(G(\bm{s})_{ij}=\frac{\partial^{2}u_{i}(\bm{s})}{\partial s_{j}\partial s_{i}}\). Assuming agent utility functions \(u_{i}\) satisfy_

1. _Strong concavity:_ \((G+\lambda\cdot I_{n\times n})\) _is negative semi-definite,_
2. _Bounded derivatives:_ \(|G_{ij}|\leq L\)_,_

_for constants \(\lambda,L>0\), the best response dynamics (4) with step size \(\delta^{t}=\frac{\lambda}{n^{2}L^{2}}\) converges to an approximate Nash equilibrium \(\bm{s}^{T}\) where \(\left\|g(\bm{s}^{T},\bm{\mu}^{T})\right\|_{2}<\varepsilon\) in \(T\) iterations, where_

\[T=\frac{2n^{2}L^{2}}{\lambda^{2}}\log\bigg{(}\frac{\left\|g(\bm{s}^{0},\bm{\mu} ^{0})\right\|_{2}}{\varepsilon}\bigg{)}.\]Proof.: Observe that \(\bm{\mu}^{t}\) is chosen s.t. \(\left\|g(\bm{s}^{t},\bm{\mu}^{t})\right\|_{2}\) is minimized among all \(\mu\) s.t. the updated sample vector \(\bm{s}^{t+1}\) remains in \(\mathcal{S}\). Thus:

\[\left\|g(\bm{s}^{t+1},\bm{\mu}^{t+1})\right\|_{2}\leq\left\|g(\bm{s}^{t+1},\bm{ \mu}^{t})\right\|_{2}\] (10)

Using Taylor's expansion, we have:

\[g(\bm{s}^{t+1},\bm{\mu}^{t})=g(\bm{s}^{t},\bm{\mu}^{t})+H(\bm{s}^{\prime},\mu^ {t})\cdot(\bm{s}^{t+1}-\bm{s}^{t}),\]

where \(H_{ij}(\bm{s}^{\prime},\bm{\mu}^{t})=\frac{\partial g(\bm{s}^{\prime},\bm{\mu }^{t})}{\partial s_{j}}\), and \(\bm{s}^{\prime}=\bm{s}^{t}+\alpha(\bm{s}^{t+1}-\bm{s}^{t})\) for some \(\alpha\in[0,1]\).

By definition, \(g(\bm{s}^{t},\mu^{t})_{i}=\frac{\partial u_{i}(\bm{s}^{t})}{\partial s_{i}}+ \mu^{t}_{i}\). Thus \(H_{ij}(\bm{s}^{\prime},\bm{\mu}^{t})=\frac{\partial^{2}u_{i}(\bm{s}^{t})}{ \partial s_{j}\partial s_{i}}=G_{ij}(\bm{s}^{\prime})\), hence \(H(\bm{s}^{\prime},\bm{\mu}^{t})=G(\bm{s}^{\prime})\). The BR dynamics update rule (4) implies \(\bm{s}^{t+1}-\bm{s}^{t}=\delta^{t}\cdot g(\bm{s}^{t},\bm{\mu}^{t})\). We therefore have \(g(\bm{s}^{t+1},\bm{\mu}^{t})=(I_{n\times n}+\delta^{t}\cdot G(\bm{s}^{\prime}) \cdot g(\bm{s}^{t},\bm{\mu}^{t})\). Taking the \(L^{2}\) norm, we get:

\[\left\|g(\bm{s}^{t+1},\bm{\mu}^{t})\right\|_{2}^{2}=\left\|g(\bm{s}^{t},\bm{ \mu}^{t})\right\|_{2}^{2}+\delta_{t}^{2}\cdot\left\|G(\bm{s}^{\prime})g(\bm{s }^{t},\bm{\mu}^{t})\right\|_{2}^{2}+2\delta_{t}g(\bm{s}^{t},\bm{\mu}^{t})^{T} G(\bm{s}^{\prime})g(\bm{s}^{t},\bm{\mu}^{t}),\] (11)

By the strong concavity assumption, for a constant \(\lambda>0\), \(G+\lambda\cdot I_{n\times n}\) is negative semi-definite, i.e., \(v^{T}(G+\lambda\cdot I_{n\times n})v\leq 0\) for any \(v\in\mathbb{R}^{n}\). With \(v=g(\bm{s}^{t},\bm{\mu}^{t})\), we have:

\[g(\bm{s}^{t},\bm{\mu}^{t})^{T}G(\bm{s}^{\prime})g(\bm{s}^{t},\bm{\mu}^{t}) \leq-\lambda\cdot\left\|g(\bm{s}^{t},\bm{\mu}^{t})\right\|_{2}^{2}.\] (12)

Next we use the fact that the \(L^{2}\) norm \(\left\|A\right\|_{2}\) of an \(n\times n\) matrix \(A\) is bounded by its Frobenius norm \(\left\|A\right\|_{F}\):

\[\left\|A\right\|_{2}:=\sup_{x\neq 0}\frac{\left\|Ax\right\|_{2}}{\left\|x \right\|_{2}}\leq\left\|A\right\|_{F}:=\sqrt{\sum_{i}\sum_{j}|A_{ij}|^{2}}\]

By the bounded derivatives assumption, we have \(\left|G(\bm{s}^{\prime})_{ij}\right|\leq L\), which implies that \(\left\|G(\bm{s}^{\prime})\right\|_{F}=\sqrt{\sum_{i}\sum_{j}L^{2}}=nL\). This gives:

\[\left\|G(\bm{s}^{\prime})g(\bm{s}^{t},\mu^{t})\right\|_{2}\leq nL\left\|g(\bm{ s}^{t},\mu^{t})\right\|_{2}.\] (13)

Using (12) and (13) in (11), we get:

\[\left\|g(\bm{s}^{t+1},\bm{\mu}^{t})\right\|_{2}^{2}=(1+\delta_{t}^{2}\cdot n^ {2}L^{2}-2\delta^{t}\lambda)\cdot\left\|g(\bm{s}^{t},\bm{\mu}^{t})\right\|_{2}^ {2},\]

Since \(\delta^{t}=\frac{\lambda}{n^{2}L^{2}}\), the above equation together with (10) gives:

\[\left\|g(\bm{s}^{t+1},\bm{\mu}^{t+1})\right\|_{2}^{2}\leq\left(1-\frac{ \lambda^{2}}{n^{2}L^{2}}\right)\cdot\left\|g(\bm{s}^{t},\bm{\mu}^{t})\right\|_{2} ^{2}.\]

Using \((1-x)^{r}\leq e^{-xr}\) repeatedly we obtain that:

\[\left\|g(\bm{s}^{t},\bm{\mu}^{t})\right\|_{2}\leq e^{-\frac{\lambda^{2}}{2n^{ 2}L^{2}}\cdot t}\cdot\left\|g(\bm{s}^{0},\bm{\mu}^{0})\right\|_{2}.\]

Thus if we want the error \(\left\|g(\bm{s}^{t},\bm{\mu}^{t})\right\|_{2}\leq\varepsilon\), \(T=\frac{2n^{2}L^{2}}{\lambda^{2}}\log\left(\frac{\left\|g(\bm{s}^{0},\mu^{0}) \right\|_{2}}{\varepsilon}\right)\) iterations suffice, as claimed. 

## Appendix B Appendix to Section 4

**Lemma 1**.: _The equation \(C\beta^{2}-(An(n-2)+C)\beta+A(n-1)^{2}=0\) of (6) has a real root \(\beta^{*}\) where \(0\leq\beta^{*}\leq 1-1/n\)._

Proof.: Using the quadratic formula, we see that \(\beta^{*}\) given by:

\[\beta^{*}=\frac{An(n-2)+C-\sqrt{(An(n-2)+C)^{2}-4AC(n-1)^{2}}}{2C}\] (14)

We first argue \(\beta^{*}\) is real, by showing \((An(n-2)+C)^{2}-4AC(n-1)^{2}\geq 0\). This is equivalent to showing \(q(y):=(y+n(n-2))^{2}-4(n-1)^{2}y\geq 0\), where \(y=C/A\). Expanding \(q\), we have \(q(y)=y^{2}-2(n^{2}-2n+2)y+n^{2}(n-2)^{2}\). The roots of \(q\) are:

\[y_{1},y_{2}=\frac{2(n^{2}-2n+2)\pm\sqrt{4(n^{2}-2n+2)^{2}-4n^{2}(n-2)^{2}}}{2}=(n ^{2}-2n+2)\pm 2(n-1),\]i.e., \(y_{1}=(n-2)^{2}\) and \(y_{2}=n^{2}\). Since \(q(y)\) has a positive leading coefficient, we have that \(q(y)\geq 0\) for all \(y\geq y_{2}=n^{2}\). Thus it remains to show that \(y=C/A\geq n^{2}\). To see this, we use the AM-HM inequality:

\[\frac{C}{n}=\frac{c_{1}+\cdots+c_{n}}{n}\geq\frac{n}{\frac{1}{c_{1}}+\cdots+ \frac{1}{c_{n}}}=\frac{n}{A},\] (15)

implying \(C/A\geq n^{2}\) as desired. This shows that the root \(\beta^{*}\) of equation (6) is real, hence well-defined.

We now show \(0\leq\beta^{*}\leq 1-1/n\). From (14), we see:

\[\beta^{*} =\frac{An(n-2)+C-\sqrt{(An(n-2)+C)^{2}-4AC(n-1)^{2}}}{2C}\] \[\geq\frac{An(n-2)+C-\sqrt{(An(n-2)+C)^{2}}}{2C}=0\]

Further, from (14) we also have:

\[\beta^{*} =\frac{An(n-2)+C-\sqrt{(An(n-2)+C)^{2}-4AC(n-1)^{2}}}{2C}\] \[\leq\frac{An(n-2)+C}{2C}=\frac{Cn(n-2)/n^{2}+C}{2C}=1-\frac{1}{n},\]

where we used \(A/C\leq 1/n^{2}\) (15) in the last inequality. This concludes the proof of Lemma 1. 

**Theorem 4.1**.: _For each \(\beta\in[0,1]\), the mechanism \(\mathcal{M}_{\beta}\) admits a Nash equilibrium. For \(\beta=\beta^{*}\) (Definition 2), whenever the NE \(\bm{s}^{*}\) of \(\mathcal{M}_{\beta^{*}}\) satisfies \(\bm{s}^{*}>0\), the NE \(\bm{s}^{*}\) maximizes the \(p\)-mean welfare among all vectors \(\bm{s}>0\), for any \(p\leq 1\)._

Proof.: When \(0\leq\beta\leq 1\), the program (5) is a convex program for general convex cost functions. Since \(u_{i}(\cdot)\) is concave, a proof similar to the proof of Theorem 3.1 shows the existence of a Nash equilibrium.

We now show the welfare-maximizing property. For simplicity, we only consider feasible strategies where each agent participates in the mechanism, i.e., \(s_{i}>0\). Let \(\rho_{i}\) and \(\lambda_{i}\) as the dual variables to the first and second constraints respectively for each \(i\), and let \(S=\left\|\bm{s}\right\|_{1}\). Writing the KKT conditions and eliminating all \(\rho_{i}\), we get that a NE \((\bm{b}^{*},\bm{s}^{*})\) together with dual variables \(\lambda^{*}\) satisfies:

\[\forall i: \frac{\partial u_{i}(b_{i}^{*},S^{*})}{\partial S}=(1-\beta) \cdot c_{i}\cdot\left(\frac{\partial u_{i}(b_{i}^{*},S^{*})}{\partial b_{i}}+ \lambda_{i}^{*}\right)\] (from stationarity conditions) (16) \[\forall i: \lambda_{i}^{*}\geq 0\] (dual feasibility) (17) \[\forall i: \lambda_{i}^{*}\cdot b_{i}=0\] (complimentary slackness) (18)

Now we turn to the \(p\)-mean welfare maximizing solution which is an optimal solution to the following program.

\[\max W_{p}(\bm{b},\bm{s}):=(\sum_{i}u_{i}(b_{i},\left\|\bm{s}\right\|_{1})^{p })^{1/p}\] s.t. \[\forall i: b_{i}+(1-\beta)c_{i}(s_{i})+\frac{\beta}{n-1}\sum_{j\neq i}c_{ j}(s_{j})=B_{i}\] (19) \[\forall i: b_{i}\geq 0\]

The following lemma establishes that (19) is a convex program. For ease of readability we defer its proof to B.1.

**Lemma 2**.: _For \(\beta\in[0,1]\) and \(p\leq 1\), the program (19) is convex._

We can now write the KKT conditions of program (19). By letting \(\mu_{i}\) and \(\gamma_{i}\) denote the dual variables corresponding to the first and second constraints respectively for each \(i\) and \(S=\left\|\bm{s}\right\|_{1}\), the KKT conditions (considering only solutions with \(s_{i}>0\)) are:

\[\forall i: (\sum_{j}u_{j}^{p})^{1/p-1}\sum_{k}u_{k}^{p-1}\frac{\partial u_{k}}{ \partial S}=c_{i}\cdot[\mu_{i}(1-\beta)+\frac{\beta}{n-1}\sum_{k\neq i}\mu_{k} ]\quad\text{(stationarity)}\] (20)\[\forall i: (\sum_{j}u_{j}^{p})^{1/p-1}u_{i}^{p-1}\frac{\partial u_{i}}{\partial b _{i}}=\mu_{i}-\gamma_{i} \text{(stationarity)}\] (21) \[\forall i: \gamma_{i}\geq 0 \text{(dual feasibility)}\] (22) \[\forall i: \gamma_{i}\cdot b_{i}=0 \text{(complimentary slackness)}\] (23)

Since KKT conditions are sufficient for optimality, to prove Theorem 4.1 it suffices to show that for an NE \((\bm{b}^{*},\bm{s}^{*})\), there exist dual variables \(\bm{\mu}^{*}\) and \(\bm{\gamma}^{*}\) which satisfy (20)-(23) for \(\beta=\beta^{*}\).

Let \(\alpha:=(\sum_{j}u_{j}(b_{j}^{*},\bm{s}^{*})^{p})^{1/p-1}\sum_{k}u_{k}(b_{j}^{ *},\bm{s}^{*})^{p-1}\frac{\partial u_{k}(b_{i}^{*},\bm{s}^{*})}{\partial S}\), i.e., the common value of the equality (20) at the NE \((\bm{b}^{*},\bm{s}^{*})\). The equation (20) then becomes \(\alpha\cdot c_{i}^{-1}=\mu_{i}(1-\beta)+\frac{\beta}{n-1}\sum_{k\neq i}\mu_{k}\). Summing these over all \(i\) and letting \(T=\sum_{j}\mu_{j}\), we obtain:

\[\alpha\cdot(\sum_{i}c_{i}^{-1})=\sum_{i}[\mu_{i}(1-\beta)+\frac{\beta}{n-1} \sum_{k\neq i}\mu_{k}]=T.\]

Putting this back in (20), we obtain the following expression for \(\mu_{i}^{*}\), which can be computed from the NE \((\bm{b}^{*},\bm{s}^{*})\) with \(T=\alpha\cdot(\sum_{i}c_{i}^{-1})\):

\[\mu_{i}^{*}=\frac{\frac{Tc_{i}^{-1}}{\sum_{i}c_{i}^{-1}}-\frac{\beta T}{n-1}}{ 1-\frac{\beta n}{n-1}}.\] (24)

Recall that the NE \((\bm{b}^{*},\bm{s}^{*})\) satisfies (16)-(18) for some dual variables \(\lambda^{*}\). We define \(\gamma_{i}^{*}\) as follows:

\[\gamma_{i}^{*}=\mu_{i}^{*}\cdot\left(\frac{\lambda_{i}^{*}}{\lambda_{i}^{*}+ \frac{\partial u_{i}(b_{i}^{*},\bm{s})}{\partial b_{i}}}\right)\] (25)

The next lemma proves Theorem 4.1.

**Lemma 3**.: _A NE \((\bm{b}^{*},\bm{s}^{*})\) with \(\bm{\mu}^{*}\) and \(\bm{\gamma}^{*}\) defined by (24) and (25) satisfy the KKT conditions (20)-(23) of program (19)._

Proof.: First observe that at the NE, \((1-\beta)c_{i}\cdot\left(\frac{\partial u_{i}(b_{i}^{*},S^{*})}{\partial b_{i} }+\lambda_{i}^{*}\right)=\frac{\partial u_{i}(b_{i}^{*},S^{*})}{\partial S}>0\) by assumption. Since \(\beta\in(0,1)\) and \(c_{i}>0\), we have \(\frac{\partial u_{i}(b_{i}^{*},S^{*})}{\partial b_{i}}+\lambda_{i}^{*}>0\). Together with \(\lambda_{i}^{*}\geq 0\) (17), this shows \(\gamma_{i}^{*}\geq 0\) thus satisfying dual feasibility (22).

Next we show complimentary slackness (23) holds. For any \(i\), \(\lambda_{i}^{*}\cdot b_{i}=0\) due to (18). Then by the definition of \(\gamma_{i}^{*}\), we have \(\gamma_{i}^{*}\cdot b_{i}=0\) for all \(i\).

Finally, we show that equations (20) and (21) are satisfied for a specific choice of \(\beta=\beta^{*}\). Together, (20) and (21) imply that an optimal solution to program (19) satisfies:

\[\forall i:\ \sum_{k}(\mu_{k}-\gamma_{k})\cdot\frac{\partial u_{k}/\partial S}{ \partial u_{k}/\partial b_{k}}=c_{i}\cdot[\mu_{i}(1-\beta)+\frac{\beta}{n-1} \sum_{k\neq i}\mu_{k}]\] (26)

The choice of \(\gamma_{i}^{*}\) from equation 25 implies that \(\mu_{i}^{*}-\gamma_{i}^{*}=\mu_{i}^{*}\cdot(\frac{\partial u_{i}(b_{i}^{*},\bm {s})/\partial b_{i}}{\partial u_{i}(b_{i}^{*},\bm{s})/\partial b_{i}+\lambda_{ i}^{*}})\). Moreover at the NE, equation (16) implies that:

\[(\mu_{i}^{*}-\gamma_{i}^{*})\cdot\frac{\partial u_{i}(b_{i}^{*}, \bm{s})/\partial S}{\partial u_{i}(b_{i}^{*},\bm{s})/\partial b_{i}} =\mu_{i}^{*}\cdot\left(\frac{\partial u_{i}(b_{i}^{*},\bm{s})/ \partial b_{i}}{\partial u_{i}(b_{i}^{*},\bm{s})/\partial b_{i}+\lambda_{i}^{*}} \right)\cdot(1-\beta)c_{i}\cdot\left(1+\frac{\lambda_{i}^{*}}{\partial u_{i}(b_{ i}^{*},\bm{s})/\partial b_{i}}\right)\] \[=\mu_{i}^{*}\cdot(1-\beta)c_{i}.\]

Using the above in (26), it only remains to be argued that \(\bm{\mu}^{*}\), \(\bm{b}^{*}\) and \(\bm{s}^{*}\) satisfy:

\[\forall i:\ (1-\beta)\cdot\sum_{k}\mu_{k}^{*}\cdot c_{k}=c_{i}\cdot[\mu_{i}^{*}(1- \beta)+\frac{\beta}{n-1}\sum_{k\neq i}\mu_{k}^{*}]=\alpha,\]for \(\beta=\beta^{*}\). By plugging in the value of \(\mu_{i}^{*}\) from (24) and using \(\alpha=T\cdot(\sum_{k}c_{k}^{-1})^{-1}\), we get:

\[(1-\beta)\cdot\sum_{k}\left\{\frac{Tc_{k}^{-1}(\sum_{i}c_{i}^{-1})^{-1}-\frac{ \beta T}{n-1}}{1-\frac{\beta n}{n-1}}\right\}\cdot c_{k}=T\cdot(\sum_{k}c_{k}^ {-1})^{-1}.\]

Let us define \(A:=(\sum_{i}c_{i}^{-1})^{-1}\) and \(C:=\sum_{i}c_{i}\). Manipulating the above expression, the above equation then becomes:

\[C\beta^{2}-(An(n-2)+C)\beta+A(n-1)^{2}=0,\]

which is true for \(\beta=\beta^{*}\) since it is exactly the definition of \(\beta^{*}\) (Definition 2).

Thus for \(\beta=\beta^{*}\), the NE \((\bm{b}^{*},\bm{s}^{*})\) with dual variables \(\bm{\mu}\) and \(\bm{\gamma}\) as defined in (24) and (25) respectively satisfy the KKT conditions of program (19). 

\(\Box\)

### Proof of Lemma 2

**Lemma 2**.: _For \(\beta\in[0,1]\) and \(p\leq 1\), the program (19) is convex._

Proof.: For \(\beta\in[0,1]\) the constraints of program 19 are convex since \(c_{i}(\cdot)\) are convex functions. It remains to be shown that the objective \(W_{p}(\bm{b},\bm{s}):=(\sum_{i}u_{i}(b_{i},\left\|s\right\|_{1})^{p})^{1/p}\) to be maximized is concave.

We use the following standard fact about the concavity of composition of functions (see e.g. Boyd and Vandenberghe (2004), Page 86).

**Proposition 2**.: _Let \(h:\mathbb{R}^{n}\to\mathbb{R}\) and \(g_{i}:\mathbb{R}^{k}\to\mathbb{R}\) and let \(f:\mathbb{R}^{n}\to\mathbb{R}\) be given by \(f(x)=h(g(x))=h(g_{1}(x),\ldots,g_{n}(x))\). Then \(f\) is concave if \(h\) is concave, \(h\) is non-decreasing in each argument and \(g_{i}\) are concave._

Note that \(W_{p}(\bm{b},\bm{s})=h(g(\bm{b},\bm{s}))\), where \(h(x_{1},\ldots,x_{n})=(\sum_{i}x_{i}^{p})^{1/p}\) and \(g_{i}(\bm{b},\bm{s})=u_{i}(\bm{b},\bm{s})\).

We now observe that:

* \(h\) is non-decreasing in each argument. This is because: \[\frac{\partial h}{\partial x_{i}}=h^{1-p}x_{i}^{p-1}\geq 0.\]
* \(h\) is concave. Using the above, we can compute the Hessian \(H\) given by: \[H_{ij}=\frac{\partial^{2}h}{\partial x_{j}\partial x_{i}}=\begin{cases}(1-p)h^ {1-2p}(x_{i}x_{j})^{p-1}&\text{(if $i\neq j$)}\\ (1-p)h^{1-2p}x_{i}^{p-2}\cdot(x_{i}^{p}-h^{p})&\text{(if $i=j$)}\end{cases}\] Thus for any \(v\in\mathbb{R}^{n}\), we have: \[v^{T}Hv =\sum_{i}\sum_{j}v_{i}H_{ij}v_{j}\] \[=(1-p)h^{1-2p}\cdot\bigg{(}\sum_{i}v_{i}\sum_{j\neq i}H_{ij}v_{j}+ \sum_{i}v_{i}^{2}H_{ii}\bigg{)}\] \[=(1-p)h^{1-2p}\cdot\bigg{(}\sum_{i}v_{i}x_{i}^{p-1}\cdot\bigg{(} \big{(}\sum_{j}v_{j}x_{j}^{p-1}\big{)}-v_{i}x_{i}^{p-1}\bigg{)}+\sum_{i}v_{i}^ {2}(x_{i}^{2p-2}-h^{p}x_{i}^{p-2})\bigg{)}\] \[=(1-p)h^{1-2p}\cdot\bigg{(}\big{(}\sum_{i}v_{i}x_{i}^{p-1}\big{)} ^{2}-\sum_{i}(v_{i}x_{i}^{p-1})^{2}+\sum_{i}v_{i}^{2}x_{i}^{2p-2}-\sum_{i}v_{i}^ {2}h^{p}x_{i}^{p-2}\bigg{)}\] \[=(1-p)h^{1-2p}\cdot\bigg{(}\big{(}\sum_{i}v_{i}x_{i}^{p-1}\big{)} ^{2}-\big{(}\sum_{i}v_{i}^{2}x_{i}^{p-2}\big{)}\big{(}\sum_{j}x_{j}^{p}\big{)} \bigg{)}\] \[\leq 0,\]since \(p\leq 1\), \(h\geq 0\), and by the Cauchy-Schwarz inequality \((\sum_{i}a_{i}\cdot b_{i})^{2}\leq(\sum_{i}a_{i}^{2})\cdot(\sum_{i}b_{i}^{2})\) with \(a_{i}=v_{i}x_{i}^{p/2-1}\) and \(b_{i}=x_{i}^{p/2-1}\). Thus \(H\) is negative semi-definite and hence \(h\) is concave.
* For each \(i\), \(g_{i}(\bm{b},\bm{s})=u_{i}(\bm{b},\bm{s})\) is concave.

Using Proposition 2 and the fact that \(W_{p}(\bm{b},\bm{s})=h(g(\bm{b},\bm{s}))\) we conclude that \(W_{p}(\bm{b},\bm{s})\) is concave. 

## Appendix C Distributed Algorithms

In this section, we present the distributed algorithms of our two mechanisms, FedBR and FedBR-BG.

``` Input: Number of iterations in game \(H\), number of iterations of gradient descent \(T\), learning rate \(\alpha\), step size \(\delta\), data increasing interval \(\Delta s\) Output: Model weights \(\theta^{T}\), individual contributions \(\bm{s}\) for\(h=1,2,\cdots,H\)do  Server sends \(\theta^{t}\) to agents; for\(t=0,1,\cdots,T-1\)do for\(i\in[n]\)in paralleldo \(i\) computes \(\nabla_{\theta^{t}}\mathcal{L}_{i}(\theta^{t})\) on its local dataset \(\mathcal{D}_{i}\); \(i\) sends \(\nabla_{\theta^{t}}\mathcal{L}_{i}(\theta^{t})\) to server; endfor  Server aggregates the gradients following \[\nabla_{\theta^{t}}\mathcal{L}(\theta^{t})\leftarrow\frac{1}{\sum_{i\in[n]}| \mathcal{D}_{i}|}\sum_{i\in[n]}|\mathcal{D}_{i}|\cdot\nabla_{\theta^{t}} \mathcal{L}_{i}(\theta^{t});\] Server updates \(\theta^{t+1}\) following \[\theta^{t+1}\leftarrow\theta^{t}-\alpha\cdot\nabla_{\theta^{t}}\mathcal{L}( \theta^{t});\] endfor for\(i\in[n]\)in paralleldo \(\frac{\partial u_{i}}{\partial s_{i}}\leftarrow\frac{a(\sum_{i}s_{i}+\Delta s) -a(\sum_{i}s_{i})}{\Delta s}-c_{i}\) if(\(s_{i}=0\)and\(\frac{\partial u_{i}}{\partial s_{i}}<0\))or(\(s_{i}=\tau_{i}\)and\(\frac{\partial u_{i}}{\partial s_{i}}>0\))then \(s_{i}^{h+1}\gets s_{i}^{h}\); else \(s_{i}^{h+1}=s_{i}^{h}+\delta\cdot\frac{\partial u_{i}}{\partial s_{i}}\); endif endfor endfor ```

**Algorithm 2** FedBR