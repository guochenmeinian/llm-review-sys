# Learning List-Level Domain-Invariant Representations for Ranking

 Ruicheng Xian\({}^{1}\)\({}^{\dagger}\) Honglei Zhuang\({}^{2}\) Zhen Qin\({}^{2}\) Hamed Zamani\({}^{3}\)\({}^{\dagger}\) Jing Lu\({}^{2}\)

Ji Ma\({}^{2}\) Kai Hui\({}^{2}\) Han Zhao\({}^{1}\) Xuanhui Wang\({}^{2}\) Michael Bendersky\({}^{2}\)

\({}^{1}\)University of Illinois Urbana-Champaign

{rxian2,hanzhao}@illinois.edu

\({}^{2}\)Google Research

{hlz,zhenqin,ljwinnie,maji,kaihuibj,xuanhui,bemike}@google.com

\({}^{3}\)University of Massachusetts Amherst

zamani@cs.umass.edu

###### Abstract

Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call _item-level alignment_, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose _list-level alignment_--learning domain-invariant representations at the higher level of lists. The benefits are twofold: it leads to the first domain adaptation generalization bound for ranking, in turn providing theoretical support for the proposed method, and it achieves better empirical transfer performance for unsupervised domain adaptation on ranking tasks, including passage reranking.

## 1 Introduction

_Learning to rank_ applies machine learning techniques to solve ranking problems that are at the core of many everyday products and applications, including search engines and recommendation systems [34]. The availability of ever-increasing amounts of training data and advances in modeling are constantly refreshing the state-of-the-art of many ranking tasks [40]. Yet, the training of practical and effective ranking models on tasks with little to no annotated data remains a challenge [57]. A popular transfer learning framework for this problem is _domain adaptation_[41]: given source domains with labeled data that are relevant to the target domain of interest, domain adaptation methods optimize the model on the source domain and make the knowledge transferable by utilizing (unlabeled) target data.

For domain adaptation, there are task-specific and general-purpose (unsupervised) methods. E.g., for text ranking in information retrieval where we only have access to target documents without any training query or relevance annotation, a method is to use generative language models to synthesizerelevant queries for the unannotated documents, then train the ranking model on the synthesized query-document pairs [37; 54; 65]. For general-purpose methods, which are applicable to all kinds of tasks, perhaps the most well-known is _domain-invariant representation learning_[38; 17]. While it is studied extensively on classification and regression problems [71; 73], applications of invariant representation learning on ranking problems are sporadic [12; 58; 68], and the existing implementations lack theoretical justifications, which casts doubt on their applicability to learning to rank.

In this paper, we revisit the method of learning invariant representations for domain adaptation on ranking problems. We found that, from our review of prior work [12; 58; 68], existing implementations of the method all perform what we call _item-level alignment_ (Fig. 1 and Section 3.2.1), which aligns the distributions of the items being ranked (e.g., query-document pairs) aggregated from all lists and discards their _list structure_ (as in, e.g., q-d pairs with the same query all belong to the same list). However, both the data and the metrics for ranking are defined and computed on lists, not the items by themselves, so intuitively the list structure should be leveraged in the feature learning process as it is intrinsic to the problem. To close this discrepancy, we propose and analyze _list-level alignment_, which aligns the distributions of the lists and preserves their list structure (Section 3.2.2).

The conceptual leap from item-level to list-level alignment is significant. First, by analyzing list-level representation invariance, we establish the first domain adaptation generalization bound for ranking (Section 4), in turn providing theoretical support for our proposed method as well as a foundation for future studies. Second, list-level alignment provides empirical benefits to domain adaptation on ranking problems, which we demonstrate on two ranking tasks, passage reranking (Section 5) and Yahoo! LETOR web search ranking (Appendix D), where it outperforms zero-shot learning, item-level alignment, as well as a method specific to text ranking based on query synthesis [37].

## 2 Problem Setup

We define a ranking problem by a joint distribution \(\mu\) of length-\(\ell\) lists and their nonnegative scores, denoted by \(X=(X_{1},\cdots,X_{\ell})\in\mathcal{X}\) and \(Y=(Y_{1},\cdots,Y_{\ell})\in\mathbb{R}_{\geq 0}^{\ell}\).1 For example, in information retrieval, the items in each list are query-document pairs of the same query, and the scores indicate the relevance of the documents to the query. Note that the lists are _permutation-invariant_, because switching the order of the items (and the corresponding scores) does not change the content of the list; concretely, let \(S_{\ell}\) denotes the set of permutations on \([\ell]\coloneqq\{1,2,\cdots,\ell\}\), then permutation invariance means that \(\mu((x_{1},\cdots,x_{\ell}),(y_{1},\cdots,y_{\ell}))=\mu((x_{\pi_{1}},\cdots,x _{\pi_{\ell}}),(y_{\pi_{1}},\cdots,y_{\pi_{\ell}}))\) for all \((x,y)\) and \(\pi\in S_{\ell}\). We assume that the scores are a function of the list, \(Y=y(X)\), so the problem can be equivalently given by the marginal distribution \(\mu^{X}\) of lists and a scoring function \(y:\mathcal{X}\rightarrow\mathbb{R}_{\geq 0}^{\ell}\).

Footnote 1: Although here the input lists are written to be decomposable into the \(\ell\) items they contain, generally and more abstractly, they need not be so.

Learning to rank aims to learn a _ranker_ that takes lists \(x\) as input and outputs rank assignments \(r\in S_{\ell}\), where \(r_{i}\in[\ell]\) is the predicted rank of item \(i\), such that \(r\) recovers the descending order of the scores \(y\), i.e., \(y_{i}>y_{j}\iff r_{i}<r_{j}\) for all \(i,j\). The more common setup, which we will adopt, is

Figure 1: Item-level and list-level alignment on a ranking problem. Each list contains \(\ell\) items to be ranked. Whereas item-level alignment aligns the distributions of items but ignores the list structure, list-level alignment preserves this structure and aligns the distributions of lists.

to train a _score_, \(f:\mathcal{X}\to\mathbb{R}^{\ell}\), with the rank assignments obtained by first computing the ranking scores \(s=f(x)\in\mathbb{R}^{\ell}\), where \(s_{i}\) is the predicted score of item \(i\), then taking the descending order of \(s\) (alternatively, the rank assignments can be generated probabilistically; see Section 4).

To quantitatively measure the quality of the predicted ranks, we use (listwise) ranking metrics, \(u:S_{\ell}\times\mathbb{R}_{\geq 0}^{\ell}\to\mathbb{R}_{\geq 0}\), which computes a utility score via comparing the predicted rank assignments to the ground-truth scores of the list. Common metrics in information retrieval include reciprocal rank and normalized discounted cumulative gain [63; 26]:

**Definition 2.1** (Reciprocal Rank).: If the ground-truth scores are binary, i.e., \(y\in\{0,1\}^{\ell}\), then the reciprocal rank (RR) of a predicted rank assignments \(r\in S_{\ell}\) is

\[\text{RR}(r,y)=\max(\{r_{i}^{-1}:1\leq i\leq\ell,y_{i}=1\}\cup\{0\}).\]

The average RR of a ranking model \(f^{\prime}:\mathcal{X}\to S_{\ell}\) on a dataset, \(\mathbb{E}_{(X,Y)\sim\mu}[\text{RR}(f^{\prime}(X),Y)]\), is referred to as the mean reciprocal rank (MRR).

**Definition 2.2** (NDCG).: The discounted cumulative gain (DCG) and the normalized DCG (with identity gain function) of a predicted rank assignments \(r\in S_{\ell}\) are

\[\text{DCG}(r,y)=\sum_{i=1}^{\ell}\frac{y_{i}}{\log(r_{i}+1)},\quad\text{and} \quad\text{NDCG}(r,y)=\frac{\text{DCG}(r,y)}{\text{IDCG}(y)},\]

where \(\text{IDCG}(y)=\max_{r^{\prime}\in S_{\ell}}\text{DCG}(r^{\prime},y)\), called the ideal DCG, is the maximum DCG of a list, which is attained by the descending order of \(y\).

**Domain Adaptation.** In this learning setting, we have a source domain \((\mu_{S}^{X},y_{S})\) and a target domain \((\mu_{T}^{X},y_{T})\) with different data distributions,2 and the goal is to learn a good ranking model for the target domain by leveraging all available resources: whereas access to source domain labeled training data is always assumed, we may only be given unlabeled data for the target domain (this scenario is called _unsupervised_ domain adaptation). A popular method for this adaptation is domain-invariant representation learning, which matches and aligns the source and target domain data distributions on the feature space so that their distributions appear similar to the model.

Footnote 2: For adapting from multiple source domains, see [71].

## 3 Learning Domain-Invariant Representations for Ranking

We begin by reviewing invariant representation learning for domain adaptation and the optimization technique of adversarial training, then describe and compare two instantiations of this framework to ranking problems in Section 3.2--item-level alignment, which is implemented in prior work, and our proposed list-level alignment.

For representation learning, we train composite models of the form \(f=h\circ g\), where \(g:\mathcal{X}\to\mathcal{Z}\) is a shared feature map and \(h\) is a task head on the shared features. As an example, if the model \(f\) is end-to-end implemented as an \(m\)-layer multilayer perceptron (MLP), then we could treat the first \((m-1)\) layers as \(g\) and the last as \(h\).

And, recall the definitions of Lipschitz continuity and Wasserstein distance [15]:

**Definition 3.1** (Lipschitz Continuity).: A function \(f:\mathcal{X}\to\mathcal{Y}\) from metric space \((\mathcal{X},d_{\mathcal{X}})\) to \((\mathcal{Y},d_{\mathcal{Y}})\) is \(L\)-Lipschitz, denoted by \(f\in\operatorname{Lip}(L)\), if \(d_{\mathcal{Y}}(f(x),f(x^{\prime}))\leq L\ d_{\mathcal{X}}(x,x^{\prime})\) for all \(x,x^{\prime}\in\mathcal{X}\).

**Definition 3.2** (Wasserstein Distance).: The Wasserstein-\(1\) distance between probability measures \(p,q\) on metric space \(\mathcal{X}\) is denoted and given by \(W_{1}(p,q)=\sup_{\psi:\mathcal{X}\to\mathbb{R},\psi\in\operatorname{Lip}(1)} \int_{\mathcal{X}}\psi(x)(p(x)-q(x))\,\mathrm{d}x\).

### Invariant Representation Learning via Adversarial Training

Given a source domain \(\mu_{S}\) and a target domain \(\mu_{T}\), invariant representation learning aims to learn a shared feature map \(g\) s.t. the distributions are aligned on the feature space, i.e., \(D(\mu_{S}^{Z},\mu_{T}^{Z})\approx 0\), where \(D\) is a divergence measure or probability metric, e.g., Wasserstein distance, and the source feature distribution \(\mu_{S}^{Z}\) (analogously for target \(\mu_{T}^{Z}\)) is defined to be the distribution of \(Z=g(X)\), \(X\sim\mu_{S}^{X}\). The idea is that a composite model with an aligned feature representation is transferable between domains--this is supported by the following generalization bound on classification problems [52, 5]. We will establish a similar result for ranking in Section 4.

**Theorem 3.3**.: _Let binary classification problems on a source and a target domain be given by joint distributions \(\mu_{S},\mu_{T}\) of inputs and labels, \((X,Y)\in\mathcal{X}\times\{0,1\}\). Define the error rate of a predictor \(f:\mathcal{X}\to[0,1]\) on \(\mu\) by \(\mathcal{R}(f)=\mathbb{E}_{(X,Y)\sim\mu}[f(X)\operatorname{\mathds{1}}[Y\neq 1 ]+(1-f(X))\operatorname{\mathds{1}}[Y\neq 0]]\)3 where \(\operatorname{\mathds{1}}[\cdot]\) is the indicator function._

Footnote 3: This definition assumes that the output class is probabilistic according to \(\mathbb{P}(\widehat{Y}=1\mid X=x)=f(x)\).

_Let \(\mathcal{H}\subset[0,1]^{\mathcal{Z}}\) be an \(L\)-Lipschitz class of prediction heads, and for any \(g\in\mathcal{G}\), define the minimum joint error rate by \(\lambda_{g}^{*}=\min_{h^{\prime}}(\mathcal{R}_{S}(h^{\prime}\circ g)+ \mathcal{R}_{T}(h^{\prime}\circ g))\), then for all \(h\in\mathcal{H}\),_

\[\mathcal{R}_{T}(h\circ g)\leq\mathcal{R}_{S}(h\circ g)+2L\ W_{1}(\mu_{S}^{Z}, \mu_{T}^{Z})+\lambda_{g}^{*}.\]

This result bounds the target domain risk of the composite model \(h\circ g\) by its source task risk, plus the divergence of the feature distributions and the optimal joint risk. It therefore suggests that a good model for the target could be obtained by learning an informative domain-invariant feature map \(g\) (s.t. \(\lambda_{g}^{*}\) and \(W_{1}(\mu_{S}^{Z},\mu_{T}^{Z})\approx 0\)) and training \(h\) to minimize the source task risk \(\mathcal{R}_{S}\).

Invariant representation learning algorithms capture the aforementioned objectives with the joint training objective of \(\min_{h,g}(\mathcal{L}_{S}(h\circ g)+\lambda\,D(\mu_{S}^{Z},\mu_{T}^{Z}))\)[35, 17, 13], where \(\mathcal{L}_{S}\) is the source task loss, and \(\lambda>0\) is a hyperparameter that controls the strength of invariant representation learning. This objective learns domain-invariant features by optimizing \(g\) to minimize the distributional discrepancy, and minimizing source risk at the same time s.t. the learned features are useful for the task. Since it does not require labeled data but only unlabeled ones for estimating \(W_{1}(\mu_{S}^{Z},\mu_{T}^{Z})\), it is applicable to unsupervised domain adaptation. If labeled target data are available, they can be incorporated by, e.g., adding a target loss \(\mathcal{L}_{T}\), which will help keep \(\lambda_{g}^{*}\) low.

Adversarial Training.When the divergence measure \(D\) is simple and admits closed-form expressions, e.g., maximum-mean discrepancy [19], the objective above can be minimized directly [35], but they are typically too weak at modeling complex distributions that arise in real-world problems. To achieve feature alignment under stronger measures, e.g., JS divergence or Wasserstein distance, a well-known approach is adversarial training [18, 17, 2], which reformulates the above objective into

\[\mathcal{L}_{\text{joint}}(h,g)=\min_{h\in\mathcal{H},g\in\mathcal{G}}\biggl{(} \mathcal{L}_{S}(h\circ g)-\lambda\min_{f_{\text{ad}}\in\mathcal{F}_{\text{ad }}}\mathcal{L}_{\text{ad}}(g,f_{\text{ad}})\biggr{)},\]

and optimizes it using gradient descent-ascent with a gradient reversal layer added on top of \(g\) in the adversarial component.

The adversarial component, \(-\min_{f_{\text{ad}}}\mathcal{L}_{\text{ad}}(g,f_{\text{ad}})\), can be shown to upper bound the divergence between \(\mu_{S}^{Z},\mu_{T}^{Z}\). It involves an adversary \(f_{\text{ad}}:\mathcal{Z}\to\mathbb{R}\) (parameterized by neural networks) and an adversarial loss function \(\ell_{\text{ad}}:\mathbb{R}\times\{0,1\}\to\mathbb{R}\), whose inputs are the output of \(f_{\text{ad}}\) and the domain identity \(a\) (we set target domain to \(a=1\)). The adversarial loss is computed over both domains:

\[\mathcal{L}_{\text{ad}}(g,f_{\text{ad}}):=\mathbb{E}_{X\sim\mu_{S}^{Z}}[\ell_{ \text{ad}}(f_{\text{ad}}\circ g(X),0)]+\mathbb{E}_{X\sim\mu_{T}^{Z}}[\ell_{ \text{ad}}(f_{\text{ad}}\circ g(X),1)].\]

If we choose the 0-1 loss, \(\ell_{\text{ad}}(\hat{a},a)=(1-a)\operatorname{\mathds{1}}[\hat{a}\geq 0]+a \operatorname{\mathds{1}}[\hat{a}<0]\), then the adversarial component upper bounds \(W_{1}(\mu_{S}^{Z},\mu_{T}^{Z})\) (see Proposition A.1); here, \(f_{\text{ad}}\) acts as a _domain discriminator_ for distinguishing the domain identity, and \(\mathcal{L}_{\text{ad}}\) is the balanced classification error of \(f_{\text{ad}}\). For training, the 0-1 loss is replaced by a surrogate loss, e.g., the logistic loss [18], \(\ell_{\text{ad}}(\hat{a},a)=\log(1+e^{(1-2a)\hat{a}})\).

### Invariant Representation Learning for Ranking

This section describes and compares two instantiations of the invariant representation learning framework above for ranking: item-level alignment, and our proposed list-level alignment. The key difference between them is the choice of \(\mu^{Z}\) whose divergence \(W_{1}(\mu_{S}^{Z},\mu_{T}^{Z})\) is to be minimized.

Model Setup.We consider a composite scoring model \(f=h\circ g\) where the feature map \(g\) is s.t. given an input list \(x=(x_{1},\cdots,x_{\ell})\), it outputs a list of \(k\)-dimensional feature vectors, \(z=g(x)=(v_{1},\cdots,v_{\ell})\in\mathbb{R}^{\ell\times k}\), where \(v_{i}\in\mathbb{R}^{k}\) corresponds to item \(i\). Ranking scores are then obtained by, e.g., projecting each \(v_{i}\) to \(\mathbb{R}\) with a (shared) linear layer. This setup, which we will adopt in our experiments, is common with listwise ranking models and in many ranking systems [10]: in neural text ranking, each feature vector can be the embedding of the input text computed by a language model [39, 21, 76].

#### 3.2.1 Item-Level Alignment

In item-level alignment (ItemDA), the distributions of the \(k\)-dimensional feature vectors from all lists in aggregate are aligned [12, 58, 68], i.e., \(\mu_{S}^{Z,\text{item}}\approx\mu_{T}^{Z,\text{item}}\), where

\[\mu^{Z,\text{item}}(v)\coloneqq\mathbb{P}_{Z\sim\mu^{Z}}(v\in Z)=\mathbb{P}_{ X\sim\mu^{X}}(v\in g(X)),\quad\text{supp}(\mu^{Z,\text{item}})\subseteq\mathbb{R}^{k}.\]

In other words (when both domain have the same number of lists), it finds a matching between the items of the source and target domains that pairs each target item with a source one (Fig. 2). Note that the list structure is not retained in this definition, because one cannot necessarily tell whether two items drawn from the bag of feature vectors, \(v,v^{\prime}\sim\mu^{Z,\text{item}}\), belong to the same list.

To implement item-level alignment, the discriminator \(f_{\text{ad}}\) (usually an MLP) is set up to take individual feature vectors \(v\in\mathbb{R}^{k}\) as input. In each forward-pass, the feature map \(g\) computes a batch of feature vector lists, \(\{z_{i}\}_{i\in[b]}=\{(v_{i1},\cdots,v_{it})\}_{i\in[b]}\), then the discriminator takes the items batched from all lists (this step discards the list structure), \(\{v_{ij}\}_{i\in[b],j\in[t]}\), and predicts their domain identities.

Item-level alignment is identical to invariant representation learning implementations for classification and regression, e.g., DANN [17], which also operate on "items", since neither the data nor the metrics in those problem settings have any explicit structural assumptions. However, ranking problems are inherently defined with list structures: the inputs to the model are organized by lists, and ranking metrics are evaluated on lists rather than the items by themselves. This discrepancy casts doubt on the applicability of item-level alignment to ranking problems, and moreover, it is unclear whether domain adaptation for ranking via item-level alignment is justified from a theoretical perspective.

#### 3.2.2 List-Level Alignment

Closing the discrepancy discussed above, we propose list-level alignment (ListDA), which aligns the distributions of the lists of feature vectors on the \(\mathbb{R}^{\ell\times k}\) space, i.e., \(\mu_{S}^{Z,\text{list}}\approx\mu_{T}^{Z,\text{list}}\), where

\[\mu^{Z,\text{list}}(z)\coloneqq\mathbb{P}_{Z\sim\mu^{Z}}(z)=\mathbb{P}_{X\sim \mu^{X}}(z=g(X)),\quad\text{supp}(\mu^{Z,\text{list}})\subseteq\mathbb{R}^{ \ell\times k}.\]

This means finding a matching that not only pairs all source and target items, but also pairs items from the same target list to items from the same source list. Therefore, list-level alignment is a stronger requirement than item-level alignment, \(\mu_{S}^{Z,\text{list}}=\mu_{T}^{Z,\text{list}}\implies\mu_{S}^{Z,\text{ item}}=\mu_{T}^{Z,\text{item}}\), while the converse is generally not true (see Fig. 2 for a picture, or Example A.2)!

To implement list-level alignment, the discriminator has to make predictions on lists, \(z\in\mathbb{R}^{\ell\times k}\), i.e., in each forward-pass it would be fed a batch of feature vector lists, \(\{z_{i}\}_{i\in[b]}=\{(v_{i1},\cdots,v_{i\ell})\}_{i\in[b]}\), and output \(b\) predictions. For this, a possible design choice is to flatten each feature vector list into a long \((\ell k)\)-dimensional vector and use an MLP as \(f_{\text{ad}}\). But recall from Section 2 that the

Figure 2: List-level alignment is a stronger requirement than item-level alignment, which, in addition to pairing target domain items to source items, it also pairs target lists to source lists.

input lists are permutation-invariant, and so are the feature vector lists. Since the above setup does not incorporate this property and an exponential amount of samples is required to see all possible permutations, this would be inefficient to optimize; yet, the optimality of \(f_{\text{ad}}\) is essential to the success of adversarial training. So instead, we use transformers with mean-pooling less positional encoding as the discriminator [60], which is permutation-invariant to the input.

Compared to item-level alignment, our list-level alignment is supported by a domain adaptation generalization bound (Theorem 4.7, established in the next section), and achieves better transfer performance in our Section 5 and Appendix D evaluations. These results suggest that the list structure is essential for an effective domain adaptation on ranking problems.

## 4 Generalization Bound for Ranking with List-Level Alignment

Based on the list-level alignment proposed above, we establish a domain adaptation generalization bound for ranking by analyzing list-level representation invariance. The discussions in this section consider learning a composite scoring model, \(f=h\circ g:\mathcal{X}\to\mathbb{R}^{\ell}\), but need not assume that the list feature representation \(z=g(x)\) can be decomposed into item-level feature representations as in the model setup of Section 3.2.2, only that it resides in a metric space \(\mathcal{Z}\).

Our result can be viewed as an instantiation of the framework established originally for classification by Ben-David et al. [5]. But since ranking is a very different task with its unique technical challenges, our analysis differs from those in prior work [52]. We will introduce appropriate assumptions for handling these difficulties leading up to the main Theorem 4.7, which the readers may skip to.

The first hurdle is the discontinuity of the sorting operation. So rather than taking the descending order of the predicted scores to obtain the rank assignments, we generate them probabilistically using a _Plackett-Luce model_[43, 36] with the exponentiated scores as its parameters [10, 20]. This makes the computation of the utility (Lipschitz) continuous w.r.t. the raw output scores of the model.

**Definition 4.1** (Plackett-Luce Model).: A Plackett-Luce (P-L) model with parameters \(w\in\mathbb{R}^{\ell}_{>0}\) specifies a distribution over \(S_{\ell}\), whose probability mass function \(p_{w}\) is

\[p_{w}(r)=\prod_{i=1}^{\ell}\frac{w_{I(r)_{i}}}{\sum_{j=i}^{\ell}w_{I(r)_{j}}}, \quad\forall r\in S_{\ell},\]

where \(I(r)_{i}\) is the index of the item with rank \(i\), \(r_{I(r)_{i}}=i\).

**Assumption 4.2**.: Given predicted scores \(s=f(x)\in\mathbb{R}^{\ell}\), we generate the rank assignments probabilistic from a P-L model by \(R\sim p_{\exp(s)}\in S_{\ell}\), where \(\exp\) is taken coordinate-wise.

Under this assumption, the utility of a scorer \(f\) on \((\mu^{X},y)\) w.r.t. ranking metric \(u:S_{\ell}\times\mathbb{R}^{\ell}_{\geq 0}\to\mathbb{R}_{\geq 0}\) is computed by (overloading \(u\) for future brevity)

\[\mathbb{E}_{\mu}[u(f)]\coloneqq\mathbb{E}_{X\sim\mu^{X}}\ \mathbb{E}_{R\sim p_{\exp(f(X))}}[u(R,y(X))].\]

We define the risk (or suboptimality) of \(f\) as the difference between its utility to the maximum-attainable utility on the problem (e.g., the maximum is \(1\) when \(u=\text{NDCG}\)):

\[\mathcal{R}(f)=\mathbb{E}_{X\sim\mu^{X}}\biggl{[}\max_{r\in S_{\ell}}u(r,y(X) )\biggr{]}-\mathbb{E}_{\mu}[u(f)].\]

The next technical challenge is that unlike in classification where the perfect classifier is unique (i.e., achieving zero error), the perfect ranker is generally not: e.g., both rank assignments \((1,2,3)\) and \((2,1,3)\) achieve maximum utility (i.e., \(\mathcal{R}=0\)) on a list with ground-truth scores \((1,1,0)\). Prior analyses of domain adaptation leverage this uniqueness [5, 52]; instead, ours does not rely on uniqueness (else the bound would be loose) with the following Lipschitz assumptions:

**Assumption 4.3**.: The ranking metric \(u:S_{\ell}\times\mathbb{R}^{\ell}_{\geq 0}\to\mathbb{R}_{\geq 0}\) is upper bounded by \(B\), and is \(L_{u}\)-Lipschitz w.r.t. the ground-truth scores \(y\) in the second argument (in Euclidean distance).

**Assumption 4.4**.: The input lists \(X\) reside in a metric space \(\mathcal{X}\), and the ground-truth scoring function \(y:\mathcal{X}\to\mathbb{R}^{\ell}_{\geq 0}\) is \(L_{y}\)-Lipschitz (in Euclidean distance on the output space).

We will show that Assumption 4.3 is satisfied by both RR and NDCG. Assumption 4.4 says that similar lists (i.e., close in \(\mathcal{X}\)) should have similar ground-truth scores, and is satisfied, e.g., when \(\mathcal{X}\) is finite; such is the case with text data, which are one-hot encoded after tokenization.

**Assumption 4.5**.: The list features \(Z\) reside in a metric space \(\mathcal{Z}\), and the class \(\mathcal{H}\) of scoring functions, \(h:\mathcal{Z}\to\mathbb{R}^{\ell}\), is \(L_{h}\)-Lipschitz (in Euclidean distance on the output space).

**Assumption 4.6**.: The class \(\mathcal{G}\) of feature maps, \(g:\mathcal{X}\to\mathcal{Z}\), satisfies that \(\forall g\in\mathcal{G}\), the restrictions of \(g\) to the supports of \(\mu_{S}^{X}\) and \(\mu_{T}^{X}\), \(g|_{\mathrm{supp}(\mu_{S}^{X})},g|_{\mathrm{supp}(\mu_{T}^{X})}\) respectively, are both invertible with \(L_{g}\)-Lipschitz inverses.

Assumption 4.5 is standard in generalization and complexity analyses, and could be enforced, e.g., with \(L^{2}\)-regularization [1, 4]. The last assumption is technical, saying that the inputs can be recovered from their feature representations by a Lipschitz inverse \(g^{-1}\). This means that the feature map \(g\) should retain as much information from the inputs (on each domain), which is a desideratum of representation learning. Note that this assumption does not conflict with the goal of invariant representation learning: there is always a sufficiently expressive \(\mathcal{G}\) s.t. \(\exists g\in\mathcal{G}\) satisfying the assumption and \(\mu_{S}^{Z,\mathrm{list}}=\mu_{T}^{Z,\mathrm{list}}\).

We are now ready to state our domain adaptation generalization bound for ranking:

**Theorem 4.7**.: _Under Assumptions 4.2 to 4.6, for any \(g\in\mathcal{G}\), define the minimum joint risk by \(\lambda_{g}^{*}=\min_{h^{\prime}}(\mathcal{R}_{S}(h^{\prime}\circ g)+\mathcal{ R}_{T}(h^{\prime}\circ g))\), then for all \(h\in\mathcal{H}\),_

\[\mathcal{R}_{T}(h\circ g)\leq\mathcal{R}_{S}(h\circ g)+4(L_{u}L_{y}L_{g}+B \ell L_{h})\ W_{1}(\mu_{S}^{Z,\mathrm{list}},\mu_{T}^{Z,\mathrm{list}})+ \lambda_{g}^{*},\]

_where \(\mu^{Z,\mathrm{list}}\) is the marginal distribution of the list features \(Z=g(X)\), \(\mu^{Z,\mathrm{list}}(z)\coloneqq\mu^{X}(g^{-1}(z))\)._

The key feature of this bound is that it depends on list-level alignment, \(W_{1}(\mu_{S}^{Z,\mathrm{list}},\mu_{T}^{Z,\mathrm{list}})\), hence it provides theoretical support for the list-level alignment proposed in Section 3.2.2. It can be instantiated to specific ranking metrics by simply verifying the Lipschitz condition \(L_{u}\) of Assumption 4.3:

**Corollary 4.8** (Bound for MRR).: RR _is \(1\)-Lipschitz in \(y\), thereby_

\[\mathbb{E}_{\mu_{T}}[\mathrm{RR}(h\circ g)]\geq\mathbb{E}_{\mu_{S}}[\mathrm{ RR}(h\circ g)]-4(L_{y}L_{g}+\ell L_{h})\ W_{1}(\mu_{S}^{Z,\mathrm{list}},\mu_{T}^{Z, \mathrm{list}})-\lambda_{g}^{*}.\]

**Corollary 4.9** (Bound for NDCG).: _If \(C^{-1}\leq\mathrm{IDCG}\leq C\) for some \(C\in(0,\infty)\) on \((\mu_{S}^{X},y_{S})\) and \((\mu_{T}^{X},y_{T})\) almost surely,4 then NDCG is \(\widetilde{O}(C\sqrt{\ell})\)-Lipschitz in \(y\) almost surely, thereby_

Footnote 4: IDCG is lower bounded, e.g., if every list contains at least one relevant item, and upper bounded when the ground-truth scores are upper bounded.

\[\mathbb{E}_{\mu_{T}}[\mathrm{NDCG}(h\circ g)]\geq\mathbb{E}_{\mu_{S}}[ \mathrm{NDCG}(h\circ g)]-\widetilde{O}(C\sqrt{\ell}L_{y}L_{g}+\ell L_{h})\ W_{1}(\mu_{S}^{Z, \mathrm{list}},\mu_{T}^{Z,\mathrm{list}})-\lambda_{g}^{*}.\]

## 5 Experiments on Passage Reranking

To demonstrate the empirical benefits of invariant representation learning with list-level alignment (ListDA) for unsupervised domain adaptation, we evaluate it on the passage reranking task5 and compare to zero-shot transfer, item-level alignment (ItemDA), as well as a recent method based on query synthesis [37]. Note that our method is not specialized to text (re)ranking; in Appendix D, we evaluate ListDA on a web ranking task from the Yahoo! Learning to Rank Challenge.

Footnote 5: We will use the terms _document_ and _passage_ interchangeably.

In passage reranking, given a text query, we are to rank candidate passages based on their relevance to the query from a retrieved set. Reranking is employed in scenarios where the corpus is too large for using accurate but expensive models such as cross-attention neural rankers to rank every (millions of) documents. Rather, a two-stage approach is taken: a simple but efficient model such as sparse or dense retrievers (e.g., BM25 [49] and DPR [28]) is first used to retrieve a candidate set of (hundreds of) passages, then a more sophisticated (re)ranking model is used to refine and improve their ranks.

We use BM25 to retrieve 1,000 candidate passages in the first-stage, and focus on the adaptation of a RankT5 listwise reranker [76], a cross-attention model derived from the T5 base language model with 250 million parameters [48]. It takes the concatenation of the query and the document (q-d pair) as input and outputs an embedding. We treat the q-d embeddings computed on the same list as the list feature--consistent with the setting in Section 3.2. For the task training loss function, we use the softmax cross-entropy ranking loss: \(\ell(s,y)=-\sum_{i=1}^{\ell}y_{i}\log(\exp(s_{i})/\!\sum_{j=1}^{\ell}\exp(s_{j} ))\).

**Datasets.** In our domain adaptation experiments, we use the MS MARCO dataset for passage ranking [3] as the source domain, which contains 8 million passages crawled from the web covering a wide range of topics with 532,761 search query and relevant passage pairs. The target domains are biomedical (TREC-COVID [62], BioASQ [59]) and news articles (Robust04 [64]). The data are preprocessed consistently with the BEIR benchmark [57]; their paper includes dataset statistics.

Since the training split of the target domain datasets does not contain q-d relevance annotations, our setting is unsupervised. It also does not contain any queries, but these are required for performing domain adaptation, because the input lists are defined to consist of q-d pairs. To acquire training queries on the target domains, we synthesize them in a zero-shot manner following [37], by using a T5 XL query generator (QGen) trained on MS MARCO relevant q-d pairs. QGen synthesizes each query as a seq-to-seq task given a passage, and the generated queries are _expected_ to be relevant to the input passages. See Table 7 for sample QGen q-d pairs.

**Methods.6** In **zero-shot** transfer, the reranker is trained on MS MARCO and evaluated on the target domain without adaptation. In **QGen pseudolabel** (QGen PL), we treat QGen q-d pairs synthesized on the target domain as relevant pairs, and train the reranker on them in addition to MS MARCO. This method is specific to text ranking, and is used in several recent works on domain adaptation of text retrievers and rerankers [37, 54, 65].

Footnote 6: All adaptation methods are applied on each source-target domain pair separately.

**ItemDA** (item-level alignment) is the prior implementation of invariant representation learning for ranking [12, 58, 68], and is set up according to Section 3.2.1 with adversarial training described in Section 3.1. The adversarial loss is aggregated from the losses of five three-layer MLP discriminators (no improvements from using wider or more layers), \(\sum_{i=1}^{5}\mathcal{L}_{\text{ad}}(g,f_{\text{ad}}^{(i)})\), for reducing the sensitivity to the randomness in the initialization and training process [16]. Our **ListDA** (list-level alignment) is set up according to Section 3.2.2, and the discriminator is an ensemble of five stacks of three T5 (transformer) encoding blocks. To predict the domain identity of a list feature \(z=(v_{1},\cdots,v_{\ell})\), we feed the vectors into the transformer blocks all at once as a sequence, take the mean-pool of the \(\ell\) output embeddings, then project it to a logit with a linear layer. A block diagram of ListDA is in Fig. 3.

Further details including hyperparameters and the construction of training lists (i.e., sampling negative pairs) are relegated to Appendix C, where we also include additional results such as using the pairwise logistic ranking loss, a hybrid adaptation method combining ListDA and QGen PL, and using transformers as the discriminator for ItemDA.

### Results

The main results are presented in Table 1, where we report metrics that are standard in the literature (e.g., TREC-COVID uses NDCG@20), and evaluate rank assignments by the descending order of the predicted scores. Since TREC-COVID and Robust04 are annotated with 3-level relevancy, the scores are binarized for mean average precision (MAP) and MRR as follows: on TREC-COVID, we map 0 (not relevant) and 1 (partially relevant) to negative, and 2 (fully relevant) to positive; on Robust04, 0 (not relevant) to negative, and 1 (relevant) and 2 (highly relevant) to positive.

Figure 3: Block diagram of the cross-attention RankT5 text ranking model with ListDA.

Across all three datasets, ListDA achieves the best performance, and the fact that it uses the same training resource as QGen PL demonstrates the added benefits of list-level invariant representations for domain adaptation. The favorable comparison of ListDA to ItemDA corroborates the discussion in Sections 3.2 and 4 that for domain adaptation on ranking problems, item-level alignment is insufficient for transferability, sometimes even resulting in negative transfer (vs. zero-shot). Rather, list-level alignment is the more appropriate approach.

### Analyses of ListDA

Quality of QGen.

An explanation for why ListDA outperforms QGen PL despite the same resources is that the sampling procedure (Appendix C.3) of negative q-d pairs could introduce false negatives into the training data. This is supported by the observation in [54] that QGen synthesized queries lack specificity and could be relevant to many documents. While these false negative labels are used for training in QGen PL, they are discarded in ListDA, which is thereby less likely to be affected by false negatives, or even false positives--when synthesized queries are not relevant to the input passages (see Table 9 for samples). Although out of the scope, better query generation is expected to improve the performance of both QGen PL and ListDA.

Size of Target Data.

Unsupervised domain adaptation requires sufficient unlabeled data, but not all domains have the same amount: BioASQ has 14 million documents (so is the total number of QGen queries, as we synthesize one per document), but Robust04 only has 528,155, and TREC-COVID 171,332.

In Fig. 4, we plot the performance of ListDA under varying numbers of target QGen queries (which is the number of target training lists). Surprisingly, on Robust04 and TREC-COVID, using just ~100 target QGen queries (0.03% and 0.06% of all, respectively) is sufficient for ListDA to achieve full performance! Although the number of queries is small, since we retrieve 1,000 documents per query, the amount of distinct target documents in those 100 lists could be substantial--up to 100,00, or 29.5% and 60.7% of the respective corpora.

The performance begins to drop when reduced to ~10 queries, which caps the amount of distinct documents at 10,000 (2.7% and 5.8%, respectively). The same data efficiency, however, is

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline Target domain & Method & MAP & MRR@10 & NDCG@5 & NDCG@10 & NDCG@20 \\ \hline \multirow{4}{*}{Robust04} & BM25 & 0.2282 & 0.6801 & 0.4396 & 0.4088 & 0.3781 \\  & Zero-shot & 0.2759 & 0.7977\({}^{\dagger}\) & 0.5857\({}^{\dagger}\) & 0.5340\({}^{\dagger}\) & 0.4856\({}^{\dagger}\) \\ \cline{2-7}  & QGen PL & 0.2693 & 0.7644 & 0.5406 & 0.5034 & 0.4694 \\  & ItemDA & 0.2822\({}^{\dagger\dagger}\) & 0.8037\({}^{\dagger}\) & 0.5822\({}^{\dagger}\) & 0.5396\({}^{\dagger}\) & 0.4922\({}^{\dagger}\) \\  & ListDA & **0.2901\({}^{\dagger\dagger}\)** & **0.8234\({}^{\dagger\dagger}\)** & **0.5979\({}^{\dagger\dagger}\)** & **0.5573\({}^{\dagger\dagger}\)** & **0.5126\({}^{\dagger\dagger}\)** \\ \hline \multirow{4}{*}{TREC-COVID} & BM25 & 0.2485 & 0.8396 & 0.7163 & 0.6559 & 0.6236 \\  & Zero-shot & 0.3083 & 0.9217 & 0.8328 & 0.8200 & 0.7826 \\ \cline{1-1} \cline{2-7}  & QGen PL & 0.3180\({}^{\dagger}\) & 0.8907 & 0.8373 & 0.8118 & 0.7861 \\  & ItemDA & 0.3087\({}^{\dagger}\) & 0.9080 & 0.8276 & 0.8142 & 0.7697 \\  & ListDA & **0.3187\({}^{\dagger}\)** & **0.9335** & **0.8693\({}^{\dagger}\)** & **0.8412\({}^{\dagger\dagger}\)** & **0.7985\({}^{\dagger}\)** \\ \hline \multirow{4}{*}{BioASQ} & BM25 & 0.4088 & 0.5612 & 0.4580 & 0.4653 & 0.4857 \\  & Zero-shot & 0.5008\({}^{\ddagger}\) & 0.6465 & 0.5484\({}^{\ddagger}\) & 0.5542\({}^{\dagger}\) & 0.5796\({}^{\ddagger}\) \\ \cline{1-1} \cline{2-7}  & QGen PL & 0.5143\({}^{\dagger\dagger}\) & 0.6551 & 0.5538\({}^{\dagger}\) & 0.5643\({}^{\dagger}\) & 0.5915\({}^{\dagger}\) \\ \cline{1-1}  & ItemDA & 0.4781 & 0.6383 & 0.5315 & 0.5343 & 0.5604 \\ \cline{1-1}  & ListDA & **0.5191\({}^{\dagger\dagger}\)** & **0.6666\({}^{\dagger}\)** & **0.5639\({}^{\dagger}\)** & **0.5714\({}^{\dagger}\)** & **0.5985\({}^{\dagger}\)** \\ \hline Source domain is MS MARCO. Gain function in NDCG is the identity map. \({}^{\dagger}\)Improves upon zero-shot with statistical significance (\(p\leq 0.05\)) under the two-tailed Student’s \(t\)-test. \({}^{\dagger}\)Improves upon QGen PL. \({}^{\dagger}\)Improves upon ItemDA. & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \end{tabular}
\end{table}
Table 1: Reranking performance of RankT5 on top 1000 BM25-retrieved passages.

Figure 4: ListDA performance under different target sizes. Lower grey horizontal line is zero-shot, upper red line is ListDA using all QGen queries.

not observed on BioASQ, likely due to the size and hardness of the dataset, such as the use of specialized biomedical terms (see examples in Tables 7 to 9).

## 6 Related Work

**Learning to Rank and Text Ranking.** Learning to rank historically focuses on tabular data, for which, a wide range of models are developed [34], from SVMs [27], gradient boosted decision trees [9], to neural rankers [8; 42; 45]. Another traditional research direction is the design of ranking losses (surrogate to ranking metrics), which can be categorized into pointwise, pairwise, and listwise approaches [10; 7; 74; 24].

Recent advances in large neural language models have spurred interest in applying them on text ranking tasks [33; 46], leading to cross-attention models [22; 39; 40; 44] and generative models based on query likelihood [14; 77; 78; 50]. A different line of work is neural text retrieval models, which emphasizes efficiency, and has seen the development of dual-encoder [28; 70], late-interaction [29; 23], and models leveraging transformer memory [56].

**Domain Adaptation in Information Retrieval.** Work on this subject can be categorized into supervised and unsupervised domain adaptation. The former assumes access to labeled source data and (a small amount of) labeled target data (a.k.a. few-shot learning) [54]. This work focuses on the unsupervised setting, where target domain data do not have annotations. Cohen et al. [12] apply invariant representation learning to unsupervised domain adaptation for text ranking, followed by Tran et al. [58] for enterprise email search, and Xin et al. [68] for dense retrieval. However, unlike our proposed list-level alignment method (ListDA), they learn invariant representations at the item-level. A recent family of adaptation methods is based on query generation [37; 65], originally proposed for dense retrieval.

**Invariant Representation Learning.** Domain-invariant representation learning is a popular family of adaptation methods [35; 17; 13], to which our proposed ListDA belongs. Besides ranking, these methods are also applied in fields including vision and language, and on tasks ranging from cross-domain sentiment analysis, question-answering [31; 61], and to cross-lingual learning and machine translation [67; 30].

Zhao et al. [72] and Tachet des Combes et al. [55] point out that on classification problems, attaining perfect feature alignment and high source accuracy are insufficient to guarantee good target performance. This occurs when the marginal distributions of labels differ, or the learned features contain domain-specific and nontransferable components. Although their findings do not directly apply to ranking, still, they suggest two potential directions for future work: one is whether Theorem 4.7 would admit a fundamental lower bound under distributional shifts in the ground-truth scores, and another is to modify ListDA to include a component for isolating out nontransferable features, as in domain separation networks [6].

## 7 Conclusion

We proposed an implementation of invariant representation learning for ranking via list-level feature alignment, and based on which, established a domain adaptation generalization bound for ranking. Our theoretical and empirical results illustrate the significance of preserving the list structure for achieving effective domain adaptation, which is overlooked in prior work.

A broader message is that when working with (feature) representations, either for domain adaptation or other purposes, they should be analyzed at the same level (or structure) at which the data are defined for the task of interest and the metrics are computed. The results of this paper is such an example--by moving from item-level alignment to the more appropriate list-level alignment, we extracted more potentials from invariant representation learning for domain adaptation on ranking problems.

## Acknowledgments

We thank the anonymous reviewers for their comments and suggestions on improving the presentation. This research was supported in part by the Google Visiting Researcher program and in part by the Center for Intelligent Information Retrieval. Han Zhao's work was partly supported by the Defense Advanced Research Projects Agency (DARPA) under Cooperative Agreement Number: HR00112320012. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.

## References

* Anthony and Bartlett [1999] Martin Anthony and Peter L. Bartlett. _Neural Network Learning: Theoretical Foundations_. Cambridge University Press, 1999.
* Arjovsky et al. [2017] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein Generative Adversarial Networks. In _Proceedings of the 34th International Conference on Machine Learning_, pages 214-223, 2017.
* Bajaj et al. [2018] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading Comprehension Dataset, 2018. _arXiv:1611.09268 [cs.CL]_.
* Bartlett et al. [2017] Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. In _Advances in Neural Information Processing Systems_, 2017.
* Ben-David et al. [2007] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of Representations for Domain Adaptation. In _Advances in Neural Information Processing Systems_, 2007.
* Bousmalis et al. [2016] Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. Domain Separation Networks. In _Advances in Neural Information Processing Systems_, 2016.
* Bruch et al. [2020] Sebastian Bruch, Shuguang Han, Michael Bendersky, and Marc Najork. A Stochastic Treatment of Learning to Rank Scoring Functions. In _Proceedings of the 13th International Conference on Web Search and Data Mining_, pages 61-69, 2020.
* Burges et al. [2005] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. Learning to Rank using Gradient Descent. In _Proceedings of the 22nd International Conference on Machine Learning_, pages 89-96, 2005.
* Burges [2010] Christopher J.C. Burges. From RankNet to LambdaRank to LambdaMART: An Overview. Technical Report MSR-TR-2010-82, Microsoft Research, 2010.
* Cao et al. [2007] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to Rank: From Pairwise Approach to Listwise Approach. In _Proceedings of the 24th International Conference on Machine Learning_, pages 129-136, 2007.
* Chapelle and Chang [2011] Olivier Chapelle and Yi Chang. Yahoo! Learning to Rank Challenge Overview. _Journal of Machine Learning Research: Workshop and Conference Proceedings_, 14:1-24, 2011.
* Cohen et al. [2018] Daniel Cohen, Bhaskar Mitra, Katja Hofmann, and W. Bruce Croft. Cross Domain Regularization for Neural Ranking Models using Adversarial Learning. In _The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval_, pages 1025-1028, 2018.
* Courty et al. [2017] Nicolas Courty, Remi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal transportation for domain adaptation. In _Advances in Neural Information Processing Systems_, 2017.

* [14] Cicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nallapati, Zhiheng Huang, and Bing Xiang. Beyond [CLS] through Ranking by Generation. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing_, pages 1722-1727, 2020.
* [15] D.A. Edwards. On the Kantorovich-Rubinstein theorem. _Expositiones Mathematicae_, 29(4):387-398, 2011.
* [16] Yanai Elazar and Yoav Goldberg. Adversarial Removal of Demographic Attributes from Text Data. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 11-21, 2018.
* [17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario March, and Victor Lempitsky. Domain-Adversarial Training of Neural Networks. _Journal of Machine Learning Research_, 17(59):1-35, 2016.
* [18] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In _Advances in Neural Information Processing Systems_, 2014.
* [19] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola. A Kernel Two-Sample Test. _Journal of Machine Learning Research_, 13(25):723-773, 2012.
* [20] John Guiver and Edward Snelson. Bayesian inference for Plackett-Luce ranking models. In _Proceedings of the 26th International Conference on Machine Learning_, pages 377-384, 2009.
* [21] Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W. Bruce Croft, and Xueqi Cheng. A Deep Look into Neural Ranking Models for Information Retrieval. _Information Processing & Management_, 57(6), 2020.
* [22] Shuguang Han, Xuanhui Wang, Mike Bendersky, and Marc Najork. Learning-to-Rank with BERT in TF-Ranking, 2020. _arXiv:2004.08476 [cs.IR]_.
* [23] Kai Hui, Honglei Zhuang, Tao Chen, Zhen Qin, Jing Lu, Dara Bahri, Ji Ma, Jai Gupta, Cicero Nogueira dos Santos, Yi Tay, and Donald Metzler. ED2LM: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference. In _Findings of the Association for Computational Linguistics: ACL 2022_, pages 3747-3758, 2022.
* [24] Rolf Jagerman, Zhen Qin, Xuanhui Wang, Michael Bendersky, and Marc Najork. On Optimizing Top-K Metrics for Neural Ranking Models. In _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 2303-2307, 2022.
* [25] Rolf Jagerman, Xuanhui Wang, Honglei Zhuang, Zhen Qin, Michael Bendersky, and Marc Najork. Rax: Composable Learning-to-Rank Using JAX. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 3051-3060, 2022.
* [26] Kalervo Jarvelin and Jaana Kekalainen. Cumulated Gain-Based Evaluation of IR Techniques. _ACM Transactions on Information Systems_, 20(4):422-446, 2002.
* [27] Thorsten Joachims. Training Linear SVMs in Linear Time. In _Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 217-226, 2006.
* [28] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing_, pages 6769-6781, 2020.
* [29] Omar Khattab and Matei Zaharia. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In _Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 39-48, 2020.

* [30] Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc'Aurelio Ranzato. Unsupervised Machine Translation Using Monolingual Corpora Only. In _International Conference on Learning Representations_, 2018.
* [31] Zheng Li, Yu Zhang, Ying Wei, Yuxiang Wu, and Qiang Yang. End-to-End Adversarial Memory Network for Cross-domain Sentiment Classification. In _Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence_, pages 2237-2243, 2017.
* [32] Chen Liang, Haoming Jiang, Simiao Zuo, Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Tuo Zhao. No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models. In _International Conference on Learning Representations_, 2022.
* [33] Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. _Pretrained Transformers for Text Ranking: BERT and Beyond_. Springer International Publishing, 2022.
* [34] Tie-Yan Liu. Learning to Rank for Information Retrieval. _Foundations and Trends(r) in Information Retrieval_, 3(3):225-331, 2009.
* [35] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning Transferable Features with Deep Adaptation Networks. In _Proceedings of the 32nd International Conference on Machine Learning_, pages 97-105, 2015.
* [36] R. Duncan Luce. _Individual Choice Behavior: A Theoretical Analysis_. Wiley, 1959.
* [37] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation. In _Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics_, pages 1075-1088, 2021.
* [38] Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain Generalization via Invariant Feature Representation. In _Proceedings of the 30th International Conference on Machine Learning_, pages 10-18, 2013.
* [39] Rodrigo Nogueira and Kyunghyun Cho. Passage Re-ranking with BERT, 2020. _arXiv:1901.04085 [cs.IR]_.
* [40] Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. Document Ranking with a Pretrained Sequence-to-Sequence Model. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 708-718, 2020.
* [41] Sinno Jialin Pan and Qiang Yang. A Survey on Transfer Learning. _IEEE Transactions on Knowledge and Data Engineering_, 22(10):1345-1359, 2010.
* [42] Liang Pang, Jun Xu, Qingyao Ai, Yanyan Lan, Xueqi Cheng, and Jirong Wen. SetRank: Learning a Permutation-Invariant Ranking Model for Information Retrieval. In _Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 499-508, 2020.
* [43] Robin L. Plackett. The Analysis of Permutations. _Journal of the Royal Statistical Society Series C (Applied Statistics)_, 24(2):193-202, 1975.
* [44] Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models, 2021. _arXiv:2101.05667 [cs.IR]_.
* [45] Zhen Qin, Le Yan, Honglei Zhuang, Yi Tay, Rama Kumar Pasumarthi, Xuanhui Wang, Michael Bendersky, and Marc Najork. Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees? In _International Conference on Learning Representations_, 2021.
* [46] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael Bendersky. Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting, 2023. _arxiv:2306.17563 [cs.IR]_.

* [47] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5835-5847, 2021.
* [48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020.
* [49] Stephen Robertson and Hugo Zaragoza. The Probabilistic Relevance Framework: BM25 and Beyond. _Foundations and Trends(r) in Information Retrieval_, 3(4):333-389, 2009.
* [50] Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. Improving Passage Retrieval with Zero-Shot Question Generation. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 3781-3797, 2022.
* [51] Shai Shalev-Shwartz and Shai Ben-David. _Understanding Machine Learning: From Theory to Algorithms_. Cambridge University Press, 2014.
* [52] Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein Distance Guided Representation Learning for Domain Adaptation. In _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence_, pages 4058-4065, 2018.
* [53] Axel Suarez, Dyaa Albakour, David Corney, Miguel Martinez, and Jose Esquivel. A Data Collection for Evaluating the Retrieval of Related Tweets to News Articles. In _Advances in Information Retrieval_, pages 780-786, 2018.
* [54] Si Sun, Yingzhuo Qian, Zhenghao Liu, Chenyan Xiong, Kaitao Zhang, Jie Bao, Zhiyuan Liu, and Paul Bennett. Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing_, volume 1, pages 5030-5043, 2021.
* [55] Remi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoff Gordon. Domain Adaptation with Conditional Distribution Matching and Generalized Label Shift. In _Advances in Neural Information Processing Systems_, 2020.
* [56] Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. Transformer Memory as a Differentiable Search Index. In _Advances in Neural Information Processing Systems_, 2022.
* [57] Nandan Thakur, Nils Reimers, Andreas Ruckle, Abhishek Srivastava, and Iryna Gurevych. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks_, 2021.
* [58] Brandon Tran, Maryam Karimzadehan, Rama Kumar Pasumarthi, Michael Bendersky, and Donald Metzler. Domain Adaptation for Enterprise Email Search. In _Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 25-34, 2019.
* [59] George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R. Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, Yannis Almirantis, John Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Artieres, Axel-Cyrille Nogna Ngomo, Norman Heino, Eric Gaussier, Liliana Barrio-Alvers, Michael Schroeder, Ion Androutsopoulos, and Georgios Paliouras. An overview of the BioASQ large-scale biomedical semantic indexing and question answering competition. _BMC Bioinformatics_, 16(1):138, 2015.

* [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In _Advances in Neural Information Processing Systems_, 2017.
* [61] Giorgos Vernikos, Katerina Margatina, Alexandra Chronopoulou, and Ion Androutsopoulos. Domain Adversarial Fine-Tuning as an Effective Regularizer. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 3103-3112, 2020.
* [62] Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R. Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. TREC-COVID: Constructing a Pandemic Information Retrieval Test Collection. _ACM SIGIR Forum_, 54:1-12, 2021.
* [63] Ellen M. Voorhees. The TREC-8 Question Answering Track Report. In _Proceedings of the Eighth Text Retrieval Conference_, pages 77-82, 1999.
* [64] Ellen M. Voorhees. The TREC Robust Retrieval Track. _ACM SIGIR Forum_, 39:11-20, 2005.
* [65] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2345-2360, 2022.
* [66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-Art Natural Language Processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, 2020.
* [67] Ruicheng Xian, Heng Ji, and Han Zhao. Cross-Lingual Transfer with Class-Weighted Language-Invariant Representations. In _International Conference on Learning Representations_, 2022.
* [68] Ji Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita Sharma, Damien Jose, and Paul Bennett. Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations. In _Findings of the Association for Computational Linguistics: ACL 2022_, pages 4008-4020, 2022.
* [69] Peilin Yang, Hui Fang, and Jimmy Lin. Anserini: Enabling the Use of Lucene for Information Retrieval Research. In _Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 1253-1256, 2017.
* [70] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. Optimizing Dense Retrieval Model Training with Hard Negatives. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 1503-1512, 2021.
* [71] Han Zhao, Shanghang Zhang, Guanhang Wu, Jose M. F. Moura, Joao P. Costeira, and Geoffrey J. Gordon. Adversarial Multiple Source Domain Adaptation. In _Advances in Neural Information Processing Systems_, 2018.
* [72] Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J. Gordon. On Learning Invariant Representations for Domain Adaptation. In _Proceedings of the 36th International Conference on Machine Learning_, pages 7523-7532, 2019.
* [73] Sicheng Zhao, Xiangyu Yue, Shanghang Zhang, Bo Li, Han Zhao, Bichen Wu, Ravi Krishna, Joseph E. Gonzalez, Alberto L. Sangiovanni-Vincentelli, Sanjit A. Seshia, and Kurt Keutzer. A Review of Single-Source Deep Unsupervised Visual Domain Adaptation. _IEEE Transactions on Neural Networks and Learning Systems_, 33(2):473-493, 2022.
* [74] Xiaofeng Zhu and Diego Klabjan. Listwise Learning to Rank by Exploring Unique Ratings. In _Proceedings of the 13th International Conference on Web Search and Data Mining_, pages 798-806, 2020.

* [75] Honglei Zhuang, Xuanhui Wang, Michael Bendersky, and Marc Najork. Feature Transformation for Neural Ranking Models. In _Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 1649-1652, 2020.
* [76] Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky. RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 2308-2313, 2023.
* [77] Shengyao Zhuang and Guido Zuccon. TILDE: Term Independent Likelihood moDEL for Passage Re-ranking. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 1483-1492, 2021.
* [78] Shengyao Zhuang, Hang Li, and Guido Zuccon. Deep Query Likelihood Model for Information Retrieval. In _Advances in Information Retrieval_, pages 463-470, 2021.

Proofs for Section 3

**Proposition A.1**.: _Let \(\mu,\mu^{\prime}\) be distributions supported on a metric space \((\mathcal{X},d)\), define and assume \(R=\sup_{(x,x^{\prime})\in\operatorname{supp}(\mu\times\mu^{\prime})}d(x,x^{ \prime})\leq\infty\), and let \(\mathcal{L}(f)\) denote the balanced 0-1 loss of a discriminator \(f:\mathcal{X}\to\{0,1\}\), given by_

\[\mathcal{L}(f)\coloneqq\mathbb{E}_{X\sim\mu}[f(X)]+\mathbb{E}_{X^{\prime}\sim \mu^{\prime}}[1-f(X^{\prime})],\]

_then \(W_{1}(\mu,\mu^{\prime})\leq R(1-\min_{f}\mathcal{L}(f))\)._

Proof.: Let \(\Gamma(\mu,\mu^{\prime})\) denote the collection of couplings between \(\mu,\mu^{\prime}\), then by the definition of the Wasserstein-1 distance,

\[W_{1}(\mu,\mu^{\prime}) =\inf_{\gamma\in\Gamma(\mu,\mu^{\prime})}\int_{\mathcal{X}\times \mathcal{X}}d(x,x^{\prime})\,\mathrm{d}\gamma(x,x^{\prime})\] \[\leq R\inf_{\gamma\in\Gamma(\mu,\mu^{\prime})}\int_{\mathcal{X} \times\mathcal{X}}\mathds{1}[x\neq x^{\prime}]\,\mathrm{d}\gamma(x,x^{\prime})\] \[=R\Bigg{(}1-\sup_{\gamma\in\Gamma(\mu,\mu^{\prime})}\int_{ \mathcal{X}\times\mathcal{X}}\mathds{1}[x=x^{\prime}]\,\mathrm{d}\gamma(x,x^{ \prime})\Bigg{)}\] \[=R\bigg{(}1-\int_{\mathcal{X}}\min(\mu(x),\mu^{\prime}(x))\, \mathrm{d}x\bigg{)}\] \[=R\int_{\mathcal{X}}\max(0,\mu^{\prime}(x)-\mu(x))\,\mathrm{d}x\] \[=\frac{R}{2}\int_{\mathcal{X}}\lvert\mu^{\prime}(x)-\mu(x)\rvert \,\mathrm{d}x,\] (1)

because \(\int(\mu^{\prime}(x)-\mu(x))\,\mathrm{d}x=0\). Note that the last line is \(R\) times the total variation between \(\mu,\mu^{\prime}\). On the other hand, rewrite

\[\mathcal{L}(f)=\int_{\mathcal{X}}(f(x)\mu(x)+(1-f(x))\mu^{\prime}(x))\, \mathrm{d}x=1-\int_{\mathcal{X}}\bigg{(}\frac{1}{2}-f(x)\bigg{)}(\mu(x)-\mu^ {\prime}(x))\,\mathrm{d}x.\]

We know that the minimum 0-1 loss is attained by the Bayes predictor, \(f^{*}(x)=\mathds{1}[\mu^{\prime}(x)\geq\mu(x)]\), so

\[\min_{f}\mathcal{L}(f)=\mathcal{L}(f^{*})=1-\frac{1}{2}\int_{\mathcal{X}} \lvert\mu(x)-\mu^{\prime}(x)\rvert\,\mathrm{d}x\leq 1-\frac{1}{R}\ W_{1}(\mu, \mu^{\prime})\] (2)

by Eq. (1). The result then follows from an algebraic rearrangement of terms in Eq. (2). 

Next, we provide a concrete example (supplementing the picture in Fig. 2) for showing that item-level alignment does not necessarily imply list-level alignment.

_Example A.2_.: Consider two uniform distributions \(\mu_{S}^{\text{list}},\mu_{T}^{\text{list}}\) of two lists containing three real-valued items, where the lists are

\[\operatorname{supp}(\mu_{S}^{\text{list}})=\{(1,2,3),(4,5,6)\},\quad \operatorname{supp}(\mu_{T}^{\text{list}})=\{(1,3,5),(2,4,6)\}.\]

The item-level distributions, \(\mu_{S}^{\text{item}},\mu_{T}^{\text{item}}\), which are obtained by aggregating the items from all lists, are both the uniform distribution of the same six items:

\[\operatorname{supp}(\mu_{S}^{\text{item}})=\operatorname{supp}(\mu_{T}^{ \text{item}})=\{1,2,3,4,5,6\}.\]

Note that item-level representations are automatically aligned since \(\mu_{S}^{\text{item}}=\mu_{T}^{\text{item}}\), but the list-level representations are not, because \(\operatorname{supp}(\mu_{S}^{\text{list}})\cap\operatorname{supp}(\mu_{T}^{ \text{list}})=\emptyset\).

## Appendix B Proofs for Section 4

This section provides the proof to Theorem 4.7, the domain adaptation generalization bound for ranking with list-level alignment. First, recall the following properties of Lipschitz functions:

**Fact B.1** (Properties of Lipschitz Functions).:
1. _If_ \(f:\mathbb{R}^{d}\to\mathbb{R}\) _is differentiable, then it is_ \(L\)_-Lipschitz (in Euclidean distance) if and only if_ \(\|\nabla f\|_{2}\leq L\)_._
2. _If_ \(f:\mathcal{X}\to\mathbb{R}\) _is_ \(L\)_-Lipschitz and_ \(g:\mathcal{X}\to\mathbb{R}\) _is_ \(M\)_-Lipschitz, then_ \((af+bg)\in\operatorname{Lip}(|a|L+|b|M)\)_, and_ \(\max(f,g)\in\operatorname{Lip}(\max(L,M))\)_._
3. _If_ \(f:\mathcal{X}\to\mathcal{Y}\) _is_ \(L\)_-Lipschitz and_ \(g:\mathcal{Y}\to\mathcal{Z}\) _is_ \(M\)_-Lipschitz, then_ \(g\circ f\in\operatorname{Lip}(LM)\)_._

Proof.:
1. For the backward direction, suppose bounded gradient norm, \(\|\nabla f\|_{2}\leq L\), then by the mean value theorem, \(\exists t\in[0,1]\) s.t. \(f(y)-f(x)=\nabla f(z)^{\top}(y-x)\) with \(z\coloneqq(1-t)x+ty\), so by Cauchy-Schwarz inequality, \[\|f(y)-f(x)\|_{2}\leq\|\nabla f(z)\|_{2}\,\|y-x\|_{2}\leq L\|y-x\|_{2}.\] For the forward direction, suppose \(f\in\operatorname{Lip}(L)\), then by differentiability, \(\nabla f(x)^{\top}z=f(x+z)-f(x)+o(\|z\|_{2})\). Set \(z\coloneqq t\nabla f(x)\), we have \[t\|\nabla f(x)\|_{2}^{2}=f(x+t\nabla f(x))-f(x)+o(t\nabla f(x))\leq Lt\|\nabla f (x)\|_{2}+o(t\|\nabla f(x)\|_{2}),\] and the result follows by dividing both sides by \(t\|\nabla f(x)\|_{2}\) and taking \(t\to 0\).
2. First, \[|af(x)+bg(x)-(af(y)+bg(y))| \leq|a||f(x)-f(y)|+|b||g(x)-g(y)|\] \[\leq(|a|L+|b|M)\ d_{\mathcal{X}}(x,y).\] Next, assume w.l.o.g. \(\max(f(x),g(x))-\max(f(y),g(y))\geq 0\), then \[|\max(f(x),g(x))-\max(f(y),g(y))|\] \[=\begin{cases}f(x)-\max(f(y),g(y))\leq f(x)-f(y)\leq L\ d_{ \mathcal{X}}(x,y)&\text{if }f(x)\geq g(x)\\ g(x)-\max(f(y),g(y))\leq g(x)-g(y)\leq M\ d_{\mathcal{X}}(x,y)&\text{else}\\ \leq\max(L,M)\ d_{\mathcal{X}}(x,y).\end{cases}\] 3. \(d_{\mathcal{Z}}(g\circ f(x),g\circ f(y))\leq M\ d_{\mathcal{Y}}(f(x),f(y)) \leq LM\ d_{\mathcal{X}}(x,y)\). 

We first prove Theorem 3.3 as a warm-up (restated below), the domain adaptation generalization bound for binary classification [52], since its proof shares the same organization as that of our Theorem 4.7, and it helps familiarizing readers of prior domain adaptation results and analysis techniques.

**Theorem B.2**.: _Let binary classification problems on a source and a target domain be given by joint distributions \(\mu_{S},\mu_{T}\) of inputs and labels, \((X,Y)\in\mathcal{X}\times\{0,1\}\). Define the error rate of a predictor \(f:\mathcal{X}\to[0,1]\) on \(\mu\) by_

\[\mathcal{R}(f)=\mathbb{E}_{(X,Y)\sim\mu}[f(X)\,\mathds{1}[Y\neq 1]+(1-f(X))\, \mathds{1}[Y\neq 0]].\]

_Let \(\mathcal{F}\subset[0,1]^{\mathcal{X}}\) be an \(L\)-Lipschitz class of predictors. Define the minimum joint error rate by \(\lambda^{*}=\min_{f^{\prime}}(\mathcal{R}_{S}(f^{\prime})+\mathcal{R}_{T}(f^{ \prime}))\), then for all \(f\in\mathcal{F}\),_

\[\mathcal{R}_{T}(f)\leq\mathcal{R}_{S}(f)+2L\ W_{1}(\mu_{S}^{X},\mu_{T}^{X})+ \lambda^{*}.\]

Proof.: Define random variable \(\eta=\mathds{1}[Y=1]\), then we may rewrite

\[\mathcal{R}(f)=\mathbb{E}_{(X,Y)\sim\mu}[\eta-(2\eta-1)f(X)].\]

Note that because \(2\eta-1\in\{-1,+1\}\),

\[\mathcal{R}(f)-\mathcal{R}(f^{\prime}) =\mathbb{E}_{(X,Y)\sim\mu}[\eta-(2\eta-1)f(X)]-\mathbb{E}_{(X,Y) \sim\mu}[\eta-(2\eta-1)f^{\prime}(X)]\] \[=\mathbb{E}_{(X,Y)\sim\mu}[(2\eta-1)(f^{\prime}(X)-f(X))]\] \[\leq\mathbb{E}_{X\sim\mu^{X}}[|f^{\prime}(X)-f(X)|].\]On the other hand,

\[\mathbb{E}_{X\sim\mu^{X}}[|f(X)-f^{\prime}(X)|] =\mathbb{E}_{(X,Y)\sim\mu}[|(2\eta-1)(f(X)-f^{\prime}(X))-\eta+\eta|]\] \[\leq\mathbb{E}_{(X,Y)\sim\mu}[|(2\eta-1)f(X)-\eta|]+\mathbb{E}_{(X,Y)\sim\mu}[|-(2\eta-1)f^{\prime}(X)+\eta|]\] \[=\mathbb{E}_{(X,Y)\sim\mu}[\eta-(2\eta-1)f(X)]+\mathbb{E}_{(X,Y) \sim\mu}[\eta-(2\eta-1)f^{\prime}(X)]\] \[=\mathcal{R}(f^{\prime})+\mathcal{R}(f).\]

Then by Fact B.1, the fact that the operation of taking the absolute value is \(1\)-Lipschitz, and Definition 3.2 of \(W_{1}\) distance, for all \(f,f^{\prime}\in\mathcal{F}\),

\[\mathcal{R}_{T}(f) =\mathcal{R}_{S}(f)+(\mathcal{R}_{T}(f)-\mathcal{R}_{T}(f^{ \prime}))-(\mathcal{R}_{S}(f)+\mathcal{R}_{S}(f^{\prime}))+(\mathcal{R}_{S}(f ^{\prime})+\mathcal{R}_{T}(f^{\prime}))\] \[\leq\mathcal{R}_{S}(f)+\mathbb{E}_{X\sim\mu^{X}_{T}}[|f(X)-f^{ \prime}(X)|]-\mathbb{E}_{X\sim\mu^{X}_{S}}[|f(X)-f^{\prime}(X)|]+(\mathcal{R} _{S}(f^{\prime})+\mathcal{R}_{T}(f^{\prime}))\] \[\leq\mathcal{R}_{S}(f)+\sup_{q\in\mathrm{Lip}(2L)}(\mathbb{E}_{X \sim\mu^{X}_{T}}[q(X)]-\mathbb{E}_{X\sim\mu^{X}_{S}}[q(X)])+(\mathcal{R}_{S}( f^{\prime})+\mathcal{R}_{T}(f^{\prime}))\] \[\leq\mathcal{R}_{S}(f)+2L\ W_{1}(\mu^{X}_{S},\mu^{X}_{T})+( \mathcal{R}_{S}(f^{\prime})+\mathcal{R}_{T}(f^{\prime})),\]

and the result follows by taking the min over \(f^{\prime}\). 

Next, we prove Theorem 4.7. The main idea is that under the setup and assumptions in Section 4, \(\mathcal{R}_{S}\) and \(\mathcal{R}_{T}\) can be written as the expectation of some Lipschitz function of \(Z\sim\mu^{Z}_{S}\) and \(\mu^{Z}_{T}\), respectively, so by Definition 3.2, their difference is upper bounded by a Lipschitz constant times \(W_{1}(\mu^{Z}_{S},\mu^{Z}_{T})\).

Although omitted, we remark that Theorem 4.7 can be extended to the cutoff version of the ranking metric \(u\) (e.g., NDCG@\(p\)) with a simple modification of the proof. Also, a finite sample generalization bound could be obtained using, e.g., Rademacher complexity, and additionally assuming Lipschitzness of the end-to-end scoring model [5, 51].

Proof of Theorem 4.7.: Fix \(g\in\mathcal{G}\), and consider \(\mu^{X}_{S}\) for now (analogous results hold for \(\mu^{X}_{T}\)). Which by Assumption 4.6, when restricted to \(\operatorname{supp}(\mu^{X}_{S})\), has an \(L_{g}\)-Lipschitz inverse \(g^{-1}\). Define function \(\epsilon_{h,g}:\mathcal{Z}\to\mathbb{R}_{\geq 0}\) for a given \(h:\mathcal{Z}\to\mathbb{R}^{\ell}\) as

\[\epsilon_{h,g}(z) =\max_{r\in S_{\ell}}u(r,y_{S}\circ g^{-1}(z))-\mathbb{E}_{R\sim p _{\operatorname{exp}(h(z))}}[u(R,y_{S}\circ g^{-1}(z))]\] (3) \[=\max_{r\in S_{\ell}}u(r,y_{S}\circ g^{-1}(z))-\sum_{r\in S_{ \ell}}u(r,y_{S}\circ g^{-1}(z))\prod_{i=1}^{\ell}\frac{\exp(h(z)_{I(r)_{i}})}{ \sum_{j=i}^{\ell}\exp(h(z)_{I(r)_{j}})},\]

and note that \(\mathcal{R}_{S}(h\circ g)=\mathbb{E}_{X\sim\mu^{X}_{S}}[\epsilon_{h,g}(g(X))]= :\mathbb{E}_{Z\sim\mu^{Z}_{S}}[\epsilon_{h,g}(z)]\). We show that \(\epsilon_{h,g}\) is Lipschitz provided that \(h\) is Lipschitz, from establishing the Lipschitzness of both terms in Eq. (3).

For the first term, because \(u\) is \(L_{u}\)-Lipschitz w.r.t. \(y_{S}\circ g^{-1}(z)\) and \(y_{S}\circ g^{-1}(z)\) is in turn \(L_{y}L_{g}\)-Lipschitz w.r.t. \(z\) by Assumptions 4.3 and 4.4, it follows that \(z\mapsto u(r,y_{S}\circ g^{-1}(z))\) is \(L_{u}L_{y}L_{g}\)-Lipschitz w.r.t. \(z\) for any fixed \(r\in S_{\ell}\), and therefore so is the function \(z\mapsto\max_{r\in S_{\ell}}u(r,y_{S}\circ g^{-1}(z))\) by Fact B.1.

For the second term, we show that it is Lipschitz (in Euclidean distance) w.r.t. both \(y_{S}\circ g^{-1}(z)=:y\) and \(h(z)=:s\). By Jensen's inequality and Fact B.1,

\[||\nabla_{y}\mathbb{E}_{R\sim p_{\operatorname{exp}(s)}}[u(R,y)] ||_{2}=\|\mathbb{E}_{R\sim p_{\operatorname{exp}(s)}}[\nabla_{y}u(R,y)]\|_{2} \leq\mathbb{E}_{R\sim p_{\operatorname{exp}(s)}}[\left\|\nabla_{y}u(R,y)\right\| _{2}]\leq L_{u}.\]And by the definition that \(\nabla_{x}f(x)=[\frac{\partial}{\partial x_{1}}f(x),\cdots,\frac{\partial}{ \partial x_{d}}f(x)]\) for any \(f:\mathbb{R}^{d}\to\mathbb{R}\),

\[\|\nabla_{s} \mathbb{E}_{R\sim p_{\exp(s)}}[u(R,y)]\|_{2}\] \[\leq\|\nabla_{s}\mathbb{E}_{R\sim p_{\exp(s)}}[u(R,y)]\|_{1}\] \[=\sum_{m=1}^{\ell}\Biggl{|}\sum_{r\in S_{\ell}}u(r,y)\Biggl{(} \frac{\partial}{\partial s_{I(r)_{m}}}\prod_{i=1}^{\ell}\frac{\exp(s_{I(r)_{i }})}{\sum_{j=i}^{\ell}\exp(s_{I(r)_{j}})}\Biggr{)}\Biggr{|}\] \[=\sum_{m=1}^{\ell}\Biggl{|}\sum_{r\in S_{\ell}}u(r,y)\sum_{i=1}^{ \ell}\Biggl{(}\frac{\partial}{\partial s_{I(r)_{m}}}\frac{\exp(s_{I(r)_{i}})} {\sum_{j=i}^{\ell}\exp(s_{I(r)_{j}})}\Biggr{)}\prod_{n\neq i}\frac{\exp(s_{I(r) _{n}})}{\sum_{j=n}^{\ell}\exp(s_{I(r)_{j}})}\Biggr{|}\] \[=\sum_{m=1}^{\ell}\sum_{r\in S_{\ell}}u(r,y)\Biggl{(}\prod_{n=1}^ {\ell}\frac{\exp(s_{I(r)_{n}})}{\sum_{j=n}^{\ell}\exp(s_{I(r)_{j}})}\Biggr{)} \Biggl{|}\sum_{i=1}^{m}\Biggl{(}\mathds{1}[m=i]-\frac{\exp(s_{I(r)_{m}})}{\sum_ {j=i}^{\ell}\exp(s_{I(r)_{j}})}\Biggr{)}\Biggr{|}\] \[\leq B\sum_{m=1}^{\ell}\sum_{r\in S_{\ell}}\Biggl{(}\prod_{n=1}^{ \ell}\frac{\exp(s_{I(r)_{n}})}{\sum_{j=n}^{\ell}\exp(s_{I(r)_{j}})}\Biggr{)} \sum_{i=1}^{m}\Biggl{(}\mathds{1}[m=i]+\frac{\exp(s_{I(r)_{m}})}{\sum_{j=i}^{ \ell}\exp(s_{I(r)_{j}})}\Biggr{)}\] \[=B\sum_{r\in S_{\ell}}\Biggl{(}\prod_{n=1}^{\ell}\frac{\exp(s_{I( r)_{n}})}{\sum_{j=n}^{\ell}\exp(s_{I(r)_{j}})}\Biggr{)}\sum_{i=1}^{\ell}\sum_{m=i}^{ \ell}\Biggl{(}\mathds{1}[m=i]+\frac{\exp(s_{I(r)_{m}})}{\sum_{j=i}^{\ell}\exp (s_{I(r)_{j}})}\Biggr{)}\] \[=B\sum_{r\in S_{\ell}}\Biggl{(}\prod_{n=1}^{\ell}\frac{\exp(s_{I( r)_{n}})}{\sum_{j=n}^{\ell}\exp(s_{I(r)_{j}})}\Biggr{)}\Biggl{(}\ell+\sum_{i=1}^{ \ell}\frac{\sum_{m=i}^{\ell}\exp(s_{I(r)_{m}})}{\sum_{j=i}^{\ell}\exp(s_{I(r)_ {j}})}\Biggr{)}\] \[=2B\ell\sum_{r\in S_{\ell}}\Biggl{(}\prod_{n=1}^{\ell}\frac{\exp( s_{I(r)_{n}})}{\sum_{j=n}^{\ell}\exp(s_{I(r)_{j}})}\Biggr{)}\] \[=2B\ell,\]

where the second equality is due to the product rule, the third equality to \(\frac{\mathrm{d}}{\mathrm{d}x_{j}}\mathrm{softmax}(x)_{i}=\mathrm{softmax}(x) _{i}(\mathds{1}[i=j]-\mathrm{softmax}(x)_{j})\), the last inequality to Assumption 4.3, and the last equality to identifying the pmf of the P-L model (Definition 4.1). Because \(h\in\mathrm{Lip}(L_{h})\) by Assumption 4.5, the two results above imply that the second term in Eq. (3) is Lipschitz: for all \(z,z^{\prime}\in\mathcal{Z}\),

\[\Bigl{|}\mathbb{E}_{R\sim p_{\exp(h(z))}}[u(R,y_{S}\circ g^{-1}(z) )]-\mathbb{E}_{R\sim p_{\exp(h(z^{\prime}))}}[u(R,y_{S}\circ g^{-1}(z^{\prime}) )]\Bigr{|}\] \[\leq\Bigl{|}\mathbb{E}_{R\sim p_{\exp(h(z))}}[u(R,y_{S}\circ g^{- 1}(z))]-\mathbb{E}_{R\sim p_{\exp(h(z))}}[u(R,y_{S}\circ g^{-1}(z^{\prime}))] \Bigr{|}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+ \Bigl{|}\mathbb{E}_{R\sim p_{\exp(h(z))}}[u(R,y_{S}\circ g^{-1}(z^{\prime}))] -\mathbb{E}_{R\sim p_{\exp(h(z^{\prime}))}}[u(R,y_{S}\circ g^{-1}(z^{\prime})) ]\Bigr{|}\] \[\leq L_{u}\|y_{S}\circ g^{-1}(z)-y_{S}\circ g^{-1}(z^{\prime})\|_ {2}+2B\ell\|h(z)-h(z^{\prime})\|_{2}\] \[\leq(L_{u}L_{y}L_{g}+2B\ell L_{h})\;d_{\mathcal{Z}}(z,z^{\prime}).\]

Combining the two parts, we have that \(\epsilon_{h,g}\) is \(2(L_{u}L_{y}L_{g}+B\ell L_{h})\)-Lipschitz in \(z\).

Finally, by Fact B.1 and Definition 3.2, for all \(g\in\mathcal{G}\) and \(h,h^{\prime}\in\mathcal{H}\),

\[\mathcal{R}_{T}(h\circ g) =\mathcal{R}_{S}(h\circ g)+(\mathcal{R}_{T}(h\circ g)-\mathcal{R}_{ T}(h^{\prime}\circ g))-(\mathcal{R}_{S}(h\circ g)+\mathcal{R}_{S}(h^{\prime}\circ g))\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+(\mathcal{R}_{ S}(h^{\prime}\circ g)+\mathcal{R}_{T}(h^{\prime}\circ g))\] \[\leq\mathcal{R}_{S}(h\circ g)+(\mathcal{R}_{T}(h\circ g)-\mathcal{ R}_{T}(h^{\prime}\circ g))-(\mathcal{R}_{S}(h\circ g)-\mathcal{R}_{S}(h^{\prime}\circ g))\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+( \mathcal{R}_{S}(h^{\prime}\circ g)+\mathcal{R}_{T}(h^{\prime}\circ g))\] \[=\mathcal{R}_{S}(h\circ g)+\mathbb{E}_{Z\sim\mu_{T}^{Z}}[\epsilon_ {h,g}(Z)-\epsilon_{h^{\prime},g}(Z)]-\mathbb{E}_{Z\sim\mu_{S}^{Z}}[\epsilon_{h,g}(Z)-\epsilon_{h^{\prime},g}(Z)]\] \[\leq\mathcal{R}_{S}(h\circ g)+\sup_{q\in\mathrm{Lip}(4(L_{u}L_{y}L_{ g}+B\ell L_{h}))}(\mathbb{E}_{Z\sim\mu_{T}^{Z}}[q(Z)]-\mathbb{E}_{Z\sim\mu_{S}^{Z}}[q(Z)])\] \[\leq\mathcal{R}_{S}(h\circ g)+4(L_{u}L_{y}L_{g}+B\ell L_{h})\;W_{1} (\mu_{S}^{Z},\mu_{T}^{Z})+(\mathcal{R}_{S}(h^{\prime}\circ g)+\mathcal{R}_{T}(h^{ \prime}\circ g)),\]

and the result follows by taking the min over \(h^{\prime}\).

Finally, we verify the Lipschitzness of RR and NDCG.

Proof of Corollary 4.8.: Because \(\text{RR}\leq 1\) uniformly and \(\|y-y^{\prime}\|_{2}\geq 1\) for all \(y,y^{\prime}\in\{0,1\}^{\ell},y\neq y^{\prime}\), so \(y\mapsto\text{RR}(r,y)\) is \(1\)-Lipschitz. 

Proof of Corollary 4.9.: We show that NDCG is Lipschitz w.r.t. \(y\) under the assumptions. Recall that

\[\text{NDCG}(r,y)=\frac{\text{DCG}(r,y)}{\text{IDCG}(y)}=\left(\sum_{i=1}^{\ell }\frac{y_{i}}{\log(r_{i}^{*}+1)}\right)^{-1}\sum_{i=1}^{\ell}\frac{y_{i}}{\log (r_{i}+1)},\]

where \(r^{*}\) is the descending order of \(y\).

Note that IDCG is continuous and piecewise differentiable w.r.t. \(y\) (each piece is associated with some \(r^{\prime}\in S_{\ell}\) and supported on \(\{y:\operatorname*{arg\,max}_{r}\text{DCG}(r,y)=r^{\prime}\}\)), then so is \(\text{IDCG}^{-1}\) on the set \(\mathcal{Y}^{\prime}\coloneqq\{y:C^{-1}\leq\text{IDCG}(y)\leq C\}\) (which is our assumption) by chain rule. Clearly, DCG is continuous and differentiable w.r.t. \(y\), so NDCG is continuous and piecewise differentiable on \(\mathcal{Y}^{\prime}\) by product rule. By Fact B.1, to show that NDCG is Lipschitz, we just need to show that its gradient norm w.r.t. \(y\) is bounded when evaluated on (the interior of) each piece.

Let \(r\in S_{\ell}\) be arbitrary, and \(y\in\mathcal{Y}^{\prime}\) be s.t. it lies in the interior of the respective piece, namely, by denoting \(r^{*}\) the descending order of \(y\), the interior of the set \(\{y^{\prime}:\operatorname*{arg\,max}_{r}\text{DCG}(r,y^{\prime})=r^{*}\}=\{y^ {\prime}:y^{\prime}_{I(r^{*})_{1}}\geq y^{\prime}_{I(r^{*})_{2}}\geq\dots\geq y ^{\prime}_{I(r^{*})_{\ell}}\}\) (recall that \(I(r)_{i}\) denotes the index of the item with rank \(i\)). Then, for all \(k\in[\ell]\),

\[\left|\frac{\partial}{\partial y_{k}}\text{NDCG}(r,y)\right| =\left|\text{IDCG}(y)^{-1}\frac{\partial}{\partial y_{k}}\sum_{ i=1}^{\ell}\frac{y_{i}}{\log(r_{i}+1)}-\text{DCG}(r,y)\Bigg{(}\frac{\partial}{ \partial y_{k}}\sum_{i=1}^{\ell}\frac{y_{i}}{\log(r_{i}^{*}+1)}\Bigg{)}^{-2}\right|\] \[\leq\left|\text{IDCG}(y)^{-1}\log(r_{k}+1)^{-1}\right|+\left| \text{DCG}(r,y)\log(r_{k}^{*}+1)^{2}\right|\] \[\leq\left|\text{IDCG}(y)^{-1}\log(2)^{-1}\right|+\left|\text{DCG }(r,y)\log(\ell+1)^{2}\right|\] \[\leq C+C\log(\ell+1)^{2},\]

hence \(\left\|\nabla_{y}\text{NDCG}(r,y)\right\|_{2}\leq\sqrt{\ell}(C+C\log(\ell+1) ^{2})\leq\widetilde{O}(C\sqrt{\ell})\), and NDCG is \(\widetilde{O}(C\sqrt{\ell})\)-Lipschitz w.r.t. \(y\). Here, \(\widetilde{O}\) hides polylogarithmic terms. 

## Appendix C Additional Experiments and Details on Passage Reranking

We perform additional experiments for unsupervised domain adaptation on the passage reranking task considered in Section 5 in Appendix C.1, provide case studies on ListDA vs. zero-shot and QGen PL (Tables 8 and 9), hyperparameter settings in Appendix C.2, and details on the construction of training lists in Appendix C.3.

### Additional Results

**Pairwise Logistic Ranking Loss.**  Besides the listwise softmax cross-entropy ranking loss used in Section 5:

\[\ell(s,y)=-\sum_{i=1}^{\ell}y_{i}\log\Bigg{(}\frac{\exp(s_{i})}{\sum_{j=1}^{ \ell}\exp(s_{j})}\Bigg{)},\]

we experiment with the pairwise logistic ranking loss [8]:

\[\ell(s,y)=-\sum_{i=1}^{\ell}\sum_{j=1}^{\ell}\mathbb{1}[y_{i}>y_{j}]\log\bigg{(} \frac{\exp(s_{i})}{\exp(s_{i})+\exp(s_{j})}\bigg{)}.\]

The results with using the pairwise logistic loss for training on the Robust04 dataset are provided in Table (a)a, which are not better than using the softmax cross-entropy loss (cf. Table 1; see also [25]), hence further experiments with this loss are not pursued.

As an implementation remark, in this set of experiments, we do not perform pairwise comparisons to obtain the predicted rank assignments during inference (where lists have length-1000) due to the high time complexity (the loss is aggregated pairwise during training, since we truncate training lists to length-31; see Appendix C.3). Whether or not the forward pass of the model involves pairwise computations does not matter to ListDA, which is applicable to any (pointwise, pairwise, or listwise) model as long as we can gather list-level representations; ListDA could be instantiated on pairwise models e.g. DuoT5 [44], although not pursued in this work.

ItemDA with Transformer Discriminator.To show that the improvements brought by ListDA over ItemDA is due to learning list-level invariant representations and not because of the use of the more expressive transformer as the discriminator vs. MLP, we experiment with a variant of ItemDA that uses the same transformer discriminator used in our ListDA runs. Although, we remark that whereas each input to the ListDA discriminator is a list of feature vectors as a length-\(\ell\) sequence, each input to the ItemDA transformer discriminator is a single feature vector, or, a sequence of length-1. So, the attention mechanism of the transformer becomes pointless, and the ItemDA transformer discriminator collapses to an MLP (with skip connections and LayerNorm).

The results are presented in Table 2(b) (cf. Table 1). Across all datasets and metrics, no consistent improvement of ItemDA (transformer) over ItemDA (MLP) is observed.

\begin{table}

\end{table}
Table 2: Reranking performance of RankT5 on top 1000 BM25-retrieved passages; additional results extending Table 1.

\begin{table}

\end{table}
Table 3: Reranking performance of RankT5 on top 1000 BM25-retrieved passages on Signal-1M.

[MISSING_PAGE_FAIL:23]

The sweep results for ListDA are plotted in Fig. 6. It is observed that a balanced choice of \(\lambda\) is needed to elicit the best performance from ListDA, and the same choice works fairly well across datasets. But for \(\eta_{\text{ad}}\), each dataset prefers different settings, probably due to their distinct domain characteristics.

In terms of running time, the adaptation methods of ListDA, ItemDA, and QGen PL all have double the training time compared to zero-shot learning due to data loading. This is because in addition to source domain data, the adaptation methods require target domain (unlabeled) data. Among ListDA, ItemDA, and QGen PL, the training times are roughly the same, because the overhead of the domain discriminators is not significant.

### Training List Construction

Recall from the setup of ListDA on ranking problems that, the inputs are defined over lists and the invariant representations are learned at the list level. In other words, the ranking loss and the adversarial loss are to be computed on lists of ranking scores and feature vectors for each query (containing both relevant and irrelevant ones).

In our reranking setup, each list would be the top-\(r\) passages retrieved by BM25 on a query, where we set \(r=1000\) in our evaluations. However, there are two complications. The first is that due to memory constraints, it is not feasible to feed all 1000 q-d pairs from each list through the RankT5 model in one pass during training (or time-consuming if via gradient accumulation). The second is that the source domain MS MARCO dataset only labels one relevant passage for each query, meaning that out of the 1000 passages retrieved by BM25 for each query, we may only know that (at most) one of them is relevant; the labels for the remaining 999 passages are unknown.

Negative Sampling.To address both issues, we truncate the list to length-31 for training and perform random sampling to gather irrelevant q-d pairs from the top 1000 BM25 retrieved passages.

Figure 5: Zero-shot performance under different hyperparameter settings for \(\eta_{\text{rank}}\).

Figure 6: ListDA performance under different hyperparameter settings for \(\lambda\) and \(\eta_{\text{ad}}\). Grey horizontal line is zero-shot performance with the best \(\eta_{\text{rank}}\) setting.

On the MS MARCO source domain, given a query \(q\) and the top 1000 passages retrieved by BM25, \(d_{1},\cdots,d_{1000}\), we yield a training list by bundling the one passage \(d^{*}\) labeled as relevant with 30 randomly sampled passages \(d_{N_{1}},\cdots,d_{N_{30}}\) to be treated as irrelevant, i.e., \(x=([q,d^{*}],[q,d_{N_{1}}],\cdots,[q,d_{N_{30}}])\) and \(y=(1,0,\cdots,0)\).

On the target domains, we follow the same procedure but with QGen synthesized queries. Given a QGen query \(q\) and the top 1000 retrieved passages, the training list consists of the pseudolabeled passage \(\hat{d}\) (i.e., the passage on which the query \(q\) was synthesized) and 30 randomly sampled irrelevant passages, i.e., \(x=([q,\hat{d}],[q,d_{N_{1}}],\cdots,[q,d_{N_{20}}])\) and \(y=(1,0,\cdots,0)\). Note that the pseudolabels \(y\) on the target domain are used by QGen PL for training but discarded by ListDA.

Reducing False Negatives on MS MARCO.One potential problem arising from sampling is that the constructed lists may contain false negatives (i.e., relevant documents that are incorrectly treated as irrelevant). In fact, false negatives are prevalent in the MS MARCO dataset (e.g., due to duplicate passages). While these false negatives are mostly harmless to source domain supervised training because the effects are canceled out when there are positively labeled q-d pairs to which the false negative q-d pairs are similar, false negatives can affect invariant representation learning.

One way that it may negatively affect invariant representation learning is that the duplicates in the lists (which have identical feature vectors) will cause ListDA to collapse distinct passages on the target domain to the same feature vector for achieving alignment, which is an artifact that will cause information loss in the target domain feature representations.

To lower the chance of sampling false negatives on MS MARCO, we rerank BM25-retrieved passages using a ranker pre-trained on MS MARCO, and sample negatives from passages ranked at 300 or lower, as the duplicates and (unlabeled) relevant passages are concentrated at the top [47]. We apply this sampling method only when constructing source domain lists for feature alignment (namely, in

\begin{table}

\end{table}
Table 4: Hyperparameter settings of RankT5 model and domain discriminators for passage reranking experiments.

ListDA and ItemDA), and it only affects the computation of adversarial loss. This sampling method is not applicable to the target domains because we do not have reliable pre-trained rankers for them. It is also not used for constructing the lists for source domain supervised training (i.e., the computation of ranking loss), because it excludes "hard negatives" from the training lists and results in a weaker reranker, as observed in our preliminary experiments.

## Appendix D Experiments on Yahoo! LETOR

We evaluate ListDA on the Yahoo! Learning to Rank Challenge v2.0 [11], and compare it to zero-shot and ItemDA (QGen PL is not applicable to tabular data). It is a web search ranking dataset with two subsets, "Set 1" and "Set 2", whose data originate from the US and a country in Asia, respectively; the training set sizes are 19,944 and 1,266, and the test set sizes are 6,983 and 3,798 (of which there are 47 and 17 lists of length-1). The dataset is tabular, and each item is represented by a 700-d vector with values in the range of \([0,1]\). Among the 700 features, 415 are defined on both sets (shared), and the other 285 are defined on either Set 1 or 2 (disjoint); we hence write each item \(x\coloneqq[x_{\text{shared}},x_{\text{disjoint}}]\) as a concatenation of the shared features and the disjoint ones, \(x_{\text{shared}}\in\mathbb{R}^{415},x_{\text{disjoint}}\in\mathbb{R}^{285}\).

We consider unsupervised domain adaptation from Set 1 to Set 2 (i.e., we discard the labels in Set 2). Our implementation uses PyTorch and the Hugging Face Transformers library [66].7

Footnote 7: Our code for this set of experiments is available in the supplementary material, and the data can be requested at https://webscope.sandbox.yahoo.com/catalog.php?datatype=c.

Models.Our models have the same setup as that of the passage reranking experiments in Section 5, except that the RankT5 text ranking model is replaced by a three-hidden-layer MLP following [75], and we treat the list of 256-d outputs at the last hidden layer as feature representations:

\[g(x)_{i} \eqdefdef v_{i}=\text{ReLU}\bigg{(}W_{3}\begin{bmatrix}\text{ReLU}(W_ {2}\text{ReLU}(W_{1}x_{i,\text{shared}}+b_{1})+b_{2})\\ \text{ReLU}(W_{2}^{\prime}\text{ReLU}(W_{1}^{\prime}x_{i,\text{disjoint}} +b_{1}^{\prime})+b_{2}^{\prime})\end{bmatrix}+b_{3}\bigg{)},\] \[h(x)_{i} \eqdef s_{i}=W_{4}v_{i}+b_{4},\]

where \(W_{1}\in\mathbb{R}^{1024\times 415}\), \(W_{1}^{\prime}\in\mathbb{R}^{1024\times 285}\), \(W_{2},W_{2}^{\prime}\in\mathbb{R}^{256\times 1024}\), \(W_{3}\in\mathbb{R}^{256\times 512}\), and \(W_{4}\in\mathbb{R}^{1\times 256}\), and are randomly initialized. Note that we use separate input layers (\(W_{1},W_{1}^{\prime}\)) and first hidden layers (\(W_{2},W_{2}^{\prime}\)) for the shared and disjoint features, then concatenate the hidden representations. This tweak improves performance of the zero-shot baseline.

For ItemDA, we use the same ensemble of five three-layer MLPs as in Section 5. For the ListDA discriminator, an ensemble of five transformers is used, where each transformer is a stack of three T5 encoder blocks, with 4 attention heads (num_heads), size-32 key, query and value projections per attention head (d_kv), and size-1024 intermediate feedforward layers (d_ff).

Results.The results are presented in Table 5, which are ensembles of 10 separately trained models to reduce the variance from the randomness in the initialization and training process, due the small dataset size. Yahoo! LETOR is annotated with 5-level relevancy, and the scores are binarized for MAP and MRR metrics by mapping 0 (bad) and 1 (fair) to negative, and 2 (good), 3 (excellent), 4 (perfect) to positive. Thanks to the availability of labeled data on Set 2, we also include **supervised** results from training on Set 1 and 2 to serve as an upper bound for unsupervised domain adaptation.

ListDA again outperforms ItemDA for unsupervised domain adaptation, collaborating the discussions in the main sections of the paper. Although, here, their performance gap is smaller compared to passage reranking results, which we suspect is because the contextual (i.e., query) information for defining the list structure of the data is too weak or discarded when the numerical features are generated (e.g., the numerical features could represent the difference between query and document, hence may not retain information about the original query).

Hyperparameters.The model is trained from scratch on an NVIDIA A6000 GPU for 10,000 steps with a batch size of 32 per domain (length of training lists varies from one to 140 items). We apply a learning rate schedule on \(\eta_{\text{rank}}\) that decays (exponentially) by a factor of 0.7 every 500 steps. We tune the hyperparameters by performing grid search over: \(\eta_{\text{rank}}\in\{\)1e-5, 2e-5, 4e-5, 8e-5, 1e-4, 2e-4, 4e-4, 8e-4, 1e-3\(\}\), \(\eta_{\text{dd}}\in\{\)0.2, 0.4, 0.8, 1, 2, 4, 8, 10\(\}\times\eta_{\text{rank}}\) as multiples of the ranker learning rate, and \(\lambda\in\{\)0.08, 0.1, 0.2, 0.4, 0.8, 1\(\}\). The tuned settings are included in Table 6.

\begin{table}
\begin{tabular}{l l l c c} \hline \hline Target domain & Method & \(\eta_{\text{dark}}\) & \(\eta_{\text{sd}}\) & \(\lambda\) \\ \hline \multirow{3}{*}{Yahoo! LETOR (Set 2)} & Zero-shot & 8e-4 & - & - \\  & Supervised & 4e-5 & - & - \\ \cline{1-1}  & ItemDA & 8e-4 & 1.6e-3 & 0.4 \\ \cline{1-1}  & ListDA & 8e-4 & 1.6e-3 & 0.8 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Hyperparameter settings of MLP ranker and domain discriminators for Yahoo! LETOR experiments.

\begin{table}
\begin{tabular}{l l c c c c c} \hline \hline Target domain & Method & MAP & MRR@10 & NDCG@5 & NDCG@10 & NDCG@20 \\ \hline \multirow{3}{*}{Yahoo! LETOR (Set 2)} & Zero-shot & 0.5464 & 0.6796 & 0.7466 & 0.7778 & 0.8308 \\  & Supervised & 0.5714 & 0.7045 & 0.7760 & 0.8012 & 0.8479 \\ \cline{1-1}  & ItemDA & 0.5604\({}^{*}\) & 0.6984\({}^{*}\) & 0.7563\({}^{*}\) & 0.7822 & 0.8350\({}^{*}\) \\ \cline{1-1}  & ListDA & **0.5633\({}^{*}\)** & **0.7026\({}^{*}\)** & **0.7599\({}^{*}\)** & **0.7845\({}^{*}\)** & **0.8355\({}^{*}\)** \\ \hline \hline \end{tabular} Results are from an ensemble of five scoring models. Bold indicates the best unsupervised result. Source domain is Yahoo! LETOR (Set 1). Gain function in NDCG is the identity map. \({}^{*}\)Improves upon zero-shot with statistical significance (\(p\leq 0.05\)) under the two-tailed Student’s \(t\)-test. \({}^{5}\)Improves upon ItemDA. Significance tests are not performed on supervised results.

\end{table}
Table 5: Ranking performance of MLP ranker on Yahoo! LETOR (Set 2).

[MISSING_PAGE_EMPTY:28]

[MISSING_PAGE_EMPTY:29]

\begin{table}
\begin{tabular}{l l l} \hline Dataset & QGen PL top-1 passage (that are irrelevant) & ListDA top-1 passage (that are relevant) \\ \hline Robust04 & Q: Identify outbreaks of Legionnaires’ disease. \\ \cline{2-3}  & D: [...] 3. Care of Patients with Tracheostomy & D: [...] 16 AT REUNION IN COLORADO; 3 DIE. \\  & III. Modifying Host Risk for Infection A. Precaucutions for Prevention of Endogenous Pneumonia 1. Prevention of Aspiration 2. Prevention of Gastric Colonization B. Prevention of Postoperative Pneumonia C. Other Propulactic Procedures for Pneumonia 1. Vaccination of Patients 2. Systemic Antimicrobial Propulaxis 3. Use of Rotating “Kinetic” Beds Prevention and Control of Legionnaires’ Disease [...] & DGen: what kind of precautions are used to prevent pneumonia \\ \hline TREC-COVID & Q: what drugs have been active against SARS-CoV or SARS-CoV-2 in animal studies? \\ \cline{2-3}  & D: Different treatments are currently used for clinical management of SARS-CoV-2 infection, but little is known about their efficacy & D: [...] the antiviral efficacies of lopinaviritonavir, hydroxychloroquine sulfate, and emtricitabine-tenofovir for SARS-CoV-2 infection were assessed in the ferret infection model. [...] all antiviral drugs tested marginally reduced the overall clinical scores of infected ferrets but did not significantly affect in vivo virus titers. Despite the potential discrepancy of drug efficacies between animals and humans, these preclinical ferret data should be highly informative to future therapeutic treatment of COVID-19 patients. \\ \hline BioASQ & Q: What is the function of the Spt6 gene in yeast? \\ \cline{2-3}  & D: As a means to study surface proteins involved in the yeast to hypha transition, human monoclonal antibody fragments (single-chain variable fragments, scFv) have been generated that bind to antigens expressed on the surface of Candida albicans yeast and/or hyphae. [...] To assess C. albicans SPT6 function, expression of the C. albicans gene was induced in a defined S. cerevisiaespt6 mutant. Partial complementation was seen, confirming that the C. albicans and S. cerevisiae genes are functionally related in these species. \\ \cline{2-3}  & QGen: what is the function of spt6 gene in candid albicans \\ \hline \end{tabular}
\end{table}
Table 9: Examples where ListDA top-1 reranked result is better than QGen PL, along with the QGen synthesized query for the top-1 passage. Text truncation or omission are indicated by “[...]”.