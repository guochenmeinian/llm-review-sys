# How Does Adaptive Optimization Impact Local Neural Network Geometry?

 Kaiqi Jiang

Department of Electrical and Computer Engineering

Princeton University

Princeton, NJ 08540

kaiqij@princeton.edu

&Dhruv Malik

Machine Learning Department

Carnegie Mellon University

Pittsburgh, PA 15213

dhruvm@andrew.cmu.edu

&Yuanzhi Li

Machine Learning Department

Carnegie Mellon University

Pittsburgh, PA 15213

yuanzhi1@andrew.cmu.edu

###### Abstract

Adaptive optimization methods are well known to achieve superior convergence relative to vanilla gradient methods. The traditional viewpoint in optimization, particularly in convex optimization, explains this improved performance by arguing that, unlike vanilla gradient schemes, adaptive algorithms mimic the behavior of a second-order method by adapting to the _global_ geometry of the loss function. We argue that in the context of neural network optimization, this traditional viewpoint is insufficient. Instead, we advocate for a _local_ trajectory analysis. For iterate trajectories produced by running a generic optimization algorithm OPT, we introduce \(R^{\text{OPT}}_{\text{med}}\), a statistic that is analogous to the condition number of the loss Hessian evaluated at the iterates. Through extensive experiments on language models where adaptive algorithms converge faster than vanilla gradient methods like SGD, we show that adaptive methods such as Adam bias the trajectories towards regions where \(R^{\text{Adam}}_{\text{med}}\) is small, where one might expect faster optimization. By contrast, SGD (with momentum) biases the trajectories towards regions where \(R^{\text{SGD}}_{\text{med}}\) is comparatively large. We complement these empirical observations with a theoretical result that provably demonstrates this phenomenon in the simplified setting of a two-layer linear network. We view our findings as evidence for the need of a new explanation of the success of adaptive methods, one that is different than the conventional wisdom.

## 1 Introduction

The efficient minimization of a parameterized loss function is a core primitive in statistics, optimization and machine learning. Gradient descent (GD), which iteratively updates a parameter vector with a step along the gradient of the loss function evaluated at that vector, is a simple yet canonical algorithm which has been applied to efficiently solve such minimization problems with enormous success. However, in modern machine learning, and especially deep learning, one frequently encounters problems where the loss functions are high dimensional, non-convex and non-smooth. The optimization landscape of such problems is thus extremely challenging, and in these settings gradient descent often suffers from prohibitively high iteration complexity.

To deal with these difficulties and improve optimization efficiency, practitioners in recent years have developed many variants of GD. One prominent class of these GD variants is the family of _adaptive_ algorithms [11, 12, 13]. At a high level, adaptive methods scale the gradient with an adaptively selected preconditioning matrix, which is constructed via a moving average of past gradients. These methods are reminiscent of second order gradient descent, since they construct approximations to the Hessian of the loss functions, while remaining computationally feasible since they eschew full computation of the Hessian. A vast line of empirical work has demonstrated the superiority of adaptive methods over GD to optimize deep neural networks, especially on Natural Language Processing (NLP) tasks with transformers [23, 10].

From a theoretical perspective, adaptive methods are well understood in the traditional context of convex optimization. For instance, Duchi et al. [12] show that when the loss function is convex, then the Adagrad algorithm yields regret guarantees that are provably as good as those obtained by using the best (diagonal) preconditioner in hindsight. The key mechanism that underlies this improved performance, is that the loss function has some global geometric property (such as sparsity or a coordinate wise bounded Lipschitz constant), and the algorithm adapts to this global geometry by adaptively selecting learning rates for features that are more informative.

However, in non-convex optimization, and deep learning in particular, it is highly unclear whether this simple characterization is sufficient to explain the superiority of adaptive methods over GD. Indeed, for large scale neural networks, global guarantees on the geometric properties of the loss are typically vacuous. For instance, for a 20-layer feedforward neural network, if we scale up the weights in each layer by a factor of \(1.5\), then the global Lipschitz constant of the network is scaled up by a factor of at least \(e^{10}\). Hence it only makes sense to study convergence by looking at the local geometry of the loss along the trajectory of the optimization algorithm [1].

Moreover, the interaction between an optimization algorithm and neural network geometry is highly complex -- recent work has shown that geometric characteristics of iterates encountered during optimization is highly dependent on the choice of optimization algorithm and associated hyperparameters [1, 2, 10]. For instance, Cohen et al. [10] demonstrate that while training neural networks with GD, the maximum eigenvalue of the Hessian evaluated at the GD iterates first increases and then plateaus at a level that is inversely proportional to the step size. The viewpoint from convex optimization, where a loss function has some (potentially) non-uniform but fixed underlying geometry that we must adapt to, is thus insufficient for neural networks, since the choice of optimization algorithm can actually _interact_ with and _influence_ the observed geometry **significantly**.

To provide another example of this interactive phenomenon, we consider the following experiment. On the same network training loss function \(f\), we run stochastic gradient descent with momentum (SGD+M) and Adam to obtain two different trajectories. We select an iterate \(x_{\text{Adam}}\) from the Adam trajectory and an iterate \(x_{\text{SGD}}\) from the SGD trajectory, such that \(f(x_{\text{Adam}})=f(x_{\text{SGD}})\). We then run SGD+M with the same configuration twice, once from \(x_{\text{Adam}}\) and once from \(x_{\text{SGD}}\). If the underlying geometry of the loss function \(f\) was truly fixed, then we would not expect a significant difference in the performance of running SGD+M from either of the two iterates. However, as shown in Figure 2, there is a noticeable difference in performance, and running SGD+M from \(x_{\text{Adam}}\) achieves lower

Figure 1: (left) Training losses of SGD+M starting from \(x_{\text{SGD}}\) and \(x_{\text{Adam}}\). (right) The 10th largest value over median in the diagonal of loss Hessian (which can be viewed as a variant of \(R^{\text{OPT}}_{\text{med}}(t)\) defined in eq. (1)) for Adam and SGD+M. Since the full Hessian is too big, here we selected several layers and randomly sampled 200 coordinates per layer to compute.

Figure 2: Training losses of Adam and SGD+M on the sentence classification task described in Section 4.1.

loss than running SGD+M from \(x_{\text{SGD}}\). This suggests that Adam may bias the optimization trajectory towards a region which is more favorable for rapid training. This motivates the following question.

_How does adaptive optimization impact the observed geometry of a neural network loss function, relative to SGD (with momentum)?_

The remainder of this paper is dedicated to answering the above question. To this end, for each iterate in a trajectory produced by running an optimization algorithm OPT, where the Hessian of the \(t\)th iterate is given by \(H^{(t)}\in\mathbb{R}^{d\times d}\), we define the second order statistic \(R_{\text{med}}^{\text{OPT}}(t)\) in the following fashion. For the \(t\)th iterate in the trajectory, let \(R_{\text{med}}^{\text{OPT}}(t)\) be the ratio of _maximum_ of the absolute entries of the diagonal of \(H^{(t)}\), to the _median_ of the absolute entries of the diagonal of \(H^{(t)}\). Concretely, we define

\[R_{\text{med}}^{\text{OPT}}(t)=\frac{\max\{|H_{ii}^{(t)}|\}_{i=1}^{d}}{\text{ median }\{|H_{ii}^{(t)}|\}_{i=1}^{d}}.\] (1)

This statistic thus measures the _uniformity_ of the diagonal of Hessian, where a smaller value of \(R_{\text{med}}^{\text{OPT}}(t)\) implies that the Hessian has a more uniform diagonal. It can also be viewed as a stable1 variant of the condition number. Instead of singular values, we choose diagonal entries because adaptive methods used in practice are _coordinate-wise_, which can be viewed as the diagonal scaling approaches.2 In Appendix A.9 we discuss this intuition in detail and compare \(R_{\text{med}}^{\text{OPT}}(t)\) with singular value-based metrics. As a supplementary result, in Appendix F, we demonstrate that the loss Hessian approaches diagonal during training for Adam and SGD+M. There has been prior theoretical work on overparameterized neural networks showing that a smaller condition number of Hessian, Neural Tangent Kernel [1] etc. could yield to faster convergence rate for (S)GD [15]. As for (diagonal) adaptive methods (_e.g._ Adagrad), they were original designed to adapt to the nonuniform diagonal geometry. Intuitively, a smaller \(R_{\text{med}}^{\text{OPT}}(t)\), which implies more uniform diagonal geometry, could lead to faster convergence.

Footnote 1: Consider the case where one parameter has little impact on the loss, then the second derivative w.r.t. this parameter is almost zero, making \(\frac{\max\{|H_{i}^{(t)}|\}_{i=1}^{d}}{\min\{|H_{i}^{(t)}|\}_{i=1}^{d}}\) in infinity. So we consider _median_ which is more stable.

Footnote 2: Recall that the main theoretical bound in the original Adagrad paper [1] is in terms of the diagonal scaling.

Armed with this statistic, we make the following contributions:

1. We focus on language models as on NLP tasks, adaptive algorithms show significantly faster convergence than SGD (with momentum). On a wide variety of neural network transformer architectures and language modeling datasets, we conduct experiments to compare how \(R_{\text{med}}^{\text{Adam}}(t)\) and \(R_{\text{med}}^{\text{SGD}}(t)\) evolve over time, when Adam and SGD+M are run from the same initialization and with their optimal (initial) learning rates respectively. In each case, we demonstrate that the Adam trajectory attains \(R_{\text{med}}^{\text{Adam}}(t)\) values that are significantly smaller than the \(R_{\text{med}}^{\text{SGD}}(t)\) values found by SGD+M. We show a simple example of this phenomenon in Figure 1(right). This suggests that relative to SGD+M, Adam biases the optimization trajectory to a region where the Hessian diagonal is more uniform. We call this phenomenon _the uniformity of diagonal geometry_ for adaptive methods. Moreover, we demonstrate that a more uniform Hessian diagonal, characterized by smaller \(R_{\text{med}}^{\text{OPT}}(t)\), is a contributing factor to faster optimization (see Section 4.3 for discussion). This suggests that a region where the Hessian diagonal is more uniform is also a region that is more amenable to rapid optimization.

2. We complement our empirical results with a theoretical analysis of this phenomenon in the simplified setting of large batch Adam and SGD+M, on a two-layer linear network with \(d\)-dimensional input and hidden layer, and one dimensional output. We show that for a wide range of \(t\), \(R_{\text{med}}^{\text{Adam}}(t)=1\pm o(1)\) but \(R_{\text{med}}^{\text{SGD}}(t)=\Omega(\log d)\). Our proof reveals that Adam induces the weight matrices to have low rank whose leading singular vectors have certain type of uniformity (see Section 6 for discussion), a fact that we also observe empirically in large scale neural networks, suggesting that this may be a mechanism by which adaptive methods bias trajectories to have uniformity of diagonal geometry.

## 2 Related work

**Existing analyses of adaptive methods.** The vast majority of prior theoretical work on adaptive methods has focused on the blackbox setting [1, 1, 2, 3, 4, 5].

ENV21]. These works make minimal assumptions about the structure of the loss function, beyond (possibly) some global properties such as convexity or smoothness. These global properties (governed by parameters such as the smoothness parameter) are assumed to hold over the entire domain. Hence this style of analysis is worst case, since the resulting convergence bounds depend on polynomially on these global parameters. However, as we show in Section 3.1, in neural networks these parameters are prohibitively large. This worst case analysis is hence unlikely to explain the success of adaptive methods on neural networks. By contrast, our focus is on analyzing the local trajectory that is induced by running the optimization method.

**Existing analyses of (S)GD on neural networks.** There is an extensive literature on the analysis of GD/SGD in the non-blackbox setting, _e.g._ overparameterized neural networks, [1, 1, 2, 3, 4, 5]. However, it is unclear how to translate these analyses of GD/SGD, to an analysis that explains the gap between GD/SGD and adaptive methods.

**Influence of algorithms on the loss geometry.** In many simple convex settings, _e.g._ linear or logistic regression and the Neural Tangent Kernel [1], the loss geometry is usually fixed and not influenced by learning algorithms. However, in neural networks the interaction between algorithms and loss landscapes is more complicated. Lewkowycz et al. [5] find a so-called catapult effect of initial learning rate on the training trajectory of SGD and related loss curvature. Cohen et al. [1] demonstrate that while training neural networks with GD, the maximum eigenvalue of the Hessian evaluated at the GD iterates first increases and then plateaus at a level that is inversely proportional to the step size. However, Cohen et al. [1] leave open the problem of whether similar interactive phenomena occur in algorithms that are not GD, including adaptive methods.

## 3 Overview of main results

### Issues of prior analyses on adaptive methods

As is mentioned in Section 2, existing work on adaptive algorithms has mainly focused on black-box analysis assuming some global worst-case parameters. However, these global bounds can be extremely bad in complicated deep learning models, as is discussed in Section 1. To see this, we initialized a transformer model3 with default initialization in Pytorch but chose a large gain4, and computed the smoothness parameter (denoted as \(l\)) and the condition number (denoted as \(\kappa\)) of loss Hessian on one layer. We observed that setting the gain as a large constant (_e.g._ 800) results in extremely large \(l\) and \(\kappa\) (\(l\geq 10^{7}\) and \(\kappa\geq 10^{10}\)), which makes the convergence rates in prior black-box analysis vacuous.

Footnote 3: https://pytorch.org/tutorials/beginner/transformer_tutorial.html

Footnote 4: This refers to the gain parameter in some commonly used initialization functions of Pytorch, _e.g._ torch.nn.init.xavier_uniform_().

The failure of global worst-case analysis implies that we need to focus on the local trajectory of algorithms. However, it is unclear whether when two optimization algorithms are used, they will have the same geometry in local trajectory or not. In particular, although in theory, adaptive algorithms can yield to a convergence rate with better dependency on certain local geometry of the function comparing to SGD (with momentum), it could still be the case that the local geometry along the trajectory of adaptive algorithm can be much worse than that of SGD (with momentum).

That motivates us to study the local geometry, especially that obtained by adaptive methods comparing to SGD (with momentum) in the paper. Motivated by the diagonal scaling of Adagrad and Adam for neural network training, we ask the follow **main question** in our paper:

How does the local diagonal geometry (diagonal of the loss Hessian) along the local trajectory of adaptive algorithms compare to that of SGD (with momentum)?

### Overview of the experiments

As is discussed in Section 1, we consider \(R^{\text{OPT}}_{\text{med}}(t)\) defined in eq. (1) as a measurement of the uniformity of the loss Hessian diagonal and conduct experiments on different NLP tasks where adaptive methods converge faster. The detailed experimental setup will be stated in Section 4. To explore potential different patterns of different layers, we do the computation layer by layer. On a wide variety of transformer architectures and language modeling datasets from the same initialization, we observe that for the vast majority of layers:

**When we train the neural network using Adam, the uniformity of diagonal geometry, measured by \(R_{\text{med}}^{\text{OPT}}(t)\) is smaller than that when we train using SGD+M from the same initialization.**

Table 1 shows a typical example of \(R_{\text{med}}^{\text{Adam}}(t)\) compared to \(R_{\text{med}}^{\text{SGDM}}(t)\) on a sentence classification task using BERT-small [12, 1] (See Section 4.1 for details). We repeated the experiments for 12 times starting from the same initialization. Table 1 shows the averaged \(R_{\text{med}}^{\text{Adam}}(t)\) and \(R_{\text{med}}^{\text{SGDM}}(t)\) in some randomly selected layers. We also report the averaged \(\frac{R_{\text{med}}^{\text{SGDM}}(t)}{R_{\text{med}}^{\text{Adam}}(t)}\) and their standard deviations in the brackets.5 Figure 2 shows the corresponding training losses of one in these 12 experiments.

Footnote 5: \(R_{\text{med}}^{\text{SGDM}}(t)\) values in Table 1 for most layers are roughly 1.4 to 2 times \(R_{\text{med}}^{\text{Adam}}(t)\) in corresponding layers. In practice, it can be considered significant because it might imply 1.4 to 2 times faster convergence.

To understand this phenomenon in a more principled point of view, in Section 5 we provide a formal proof of the statement in a simplified setting: large batch Adam and SGD+M on a 2-layer linear network with 1-dimensional output.

## 4 The uniformity of diagonal geometry

As is mentioned in Section 3.2, we computed \(R_{\text{med}}^{\text{OPT}}(t)\) defined in eq. (1) on different language models. In this section, we present the results of SGD+M and Adam on different architectures and datasets. In Appendix A, we present the results of other adaptive algorithms.

During training we started from the same initial weights and used the same learning rate schedule (constant or decreasing) for SGD+M and Adam. We tuned and chose the best (initial) learning rate of SGD+M. The (initial) learning rate of Adam was set as a value under which Adam converged faster than SGD+M with its best learning rate. The concrete values will be stated in later parts of this section. We used large batch sizes to make the training procedure stable. When computing Hessian, we also used large batch sizes. Due to the extremely large dimension, we did the computation on some uniformly selected coordinates, more precisely, 200 coordinates per layer.

### Experiments on real datasets

**Sentence classification task on BERT-small.** We fine-tuned BERT-small [12, 1] on the IMDB dataset [1]: the task is to classify whether movie reviews are positive or negative.6 The momentum parameter \(\beta\) in SGD was set as 0.9. The two momentum parameters \((\beta_{1},\beta_{2})\) of Adam were set as (0.9, 0.999). We trained the model using linearly decreasing learning rates for 10 epochs (2500 iterations). The initial learning rates of SGD+M and Adam were 0.001 and 5e-5, respectively. As mentioned in Section 3.2, Figure 2 and Table 1 show the training losses and the comparison between \(R_{\text{med}}^{\text{Adam}}(t)\) and \(R_{\text{med}}^{\text{SGDM}}(t)\), respectively.

Footnote 6: https://huggingface.co/docs/transformers/v4.16.2/en/training

**Translation task.** We trained a Seq2Seq network that uses Transformer to solve a machine translation task on Multi30k [1]: this task is to train a German to English translation model.7 The

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline Layer\# & \multicolumn{2}{c}{Iteration 0} & \multicolumn{2}{c}{Iteration 750} & \multicolumn{2}{c}{Iteration 1250} \\  & \(\frac{R_{\text{med}}^{\text{SGDM}}(t)}{R_{\text{med}}^{\text{Adam}}(t)}\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(\frac{R_{\text{med}}^{\text{SGDM}}(t)}{R_{\text{med}}^{\text{Adam}}(t)}\) \\ \hline
9 & 15.7 & 15.7 & 12.76 & 9.65 & 1.45 (0.65) & 11.43 & 14.24 & 0.94 (0.40) \\ \hline
12 & 22.63 & 22.63 & 13.17 & 7.41 & 1.92 (0.67) & 10.62 & 9.67 & 1.33 (0.75) \\ \hline
15 & 9.35 & 9.35 & 80.57 & 53.52 & 1.65 (0.65) & 100.65 & 61.80 & 2.01 (1.00) \\ \hline
17 & 82.37 & 82.37 & 405.02 & 223.56 & 1.91 (0.53) & 423.28 & 337.32 & 1.43 (0.63) \\ \hline
18 & 31.32 & 31.32 & 17.07 & 13.24 & 1.43 (0.58) & 18.15 & 15.63 & 1.21 (0.36) \\ \hline
22 & 47.13 & 47.13 & 233.72 & 72.67 & 3.54 (1.21) & 158.38 & 93.13 & 2.28 (1.18) \\ \hline
24 & 31.17 & 31.17 & 17.52 & 17.34 & 1.13 (0.40) & 13.51 & 14.23 & 1.05 (0.36) \\ \hline \hline \end{tabular}
\end{table}
Table 1: \(R_{\text{med}}^{\text{Adam}}(t)\) and \(R_{\text{med}}^{\text{SGDM}}(t)\) in some layers, on the sentence classification task (see Section 4.1).

momentum parameter \(\beta\) in SGD was set as 0.9. The two momentum parameters \((\beta_{1},\beta_{2})\) of Adam were set as (0.9, 0.98). We trained the model using constant learning rates (0.03 for SGD+M and 1e-4 for Adam) for 60 epochs (1800 iterations). The experiments were repeated for 8 times starting from the same initialization. Figure 3(left) shows the training losses for one among them. Table (a)a shows the averaged \(R_{\text{med}}^{\text{Adam}}(t)\), \(R_{\text{med}}^{\text{SGDM}}(t)\) and \(\frac{R_{\text{med}}^{\text{SGDM}}(t)}{R_{\text{med}}^{\text{Adam}}(t)}\) (with standard deviation in the brackets) in some randomly selected layers.

### Experiments on random datasets

We used the same model and momentum parameters as in the translation task described in Section 4.1 but generated random integers as targets. Similar to the setting on real targets, the model was trained using constant learning rates (0.015 for SGD+M and 5e-5 for Adam) for 60 epochs (1800 iterations), and we repeated the experiments for 8 times starting from the same initialization. Figure 3(right) shows the training losses for one among them. Table (b)b shows the averaged \(R_{\text{med}}^{\text{Adam}}(t)\), \(R_{\text{med}}^{\text{SGDM}}(t)\) and \(\frac{R_{\text{med}}^{\text{SGDM}}(t)}{R_{\text{med}}^{\text{Adam}}(t)}\) (with standard deviation in the brackets) of the same 10 layers as in Table (a)a.8

Footnote 8: To prevent \(R_{\text{med}}^{\text{OPT}}(t)\) from getting too large due to tiny median, we added an additional term \(0.001\max\{|H_{ii}^{(t)}|\}_{i=1}^{d}\) to the denominator of eq. (1) when computing.

### Summarization of the empirical results and discussion

Overall, through extensive experiments on language models, we demonstrate that **starting from the same initialization, for the vast majority of layers, the \(R_{\text{med}}^{\text{OPT}}(t)\) values found by Adam are smaller than those found by SGD+M.** This suggests that Adam biases the trajectory towards a region with more uniform Hessian diagonal than SGD+M. In Appendix A.10 we also validate this observation on the in-distribution test data.

**Contribution of uniform Hessian diagonal to fast convergence.** We observe that on dataset with random targets, SGD+M plateaus after about 400 steps and thus converges much slower when compared to Adam than on real dataset (see Figure 3). On the other hand, the gaps of \(R_{\text{med}}^{\text{SGDM}}(t)\) and \(R_{\text{med}}^{\text{Adam}}(t)\) are more significant on random data than on real data (see Table 2a and Table 2b) as well. In Appendix A.4, we conduct another experiment where we switch from SGD to Adam in the middle and compare it with the model trained by Adam from the beginning. The observation is that both the loss gap and the gap of \(R_{\text{med}}^{\text{OPT}}(t)\) are gradually closed after switching (see Figure 7 and Table 8). Hence we find a positive correlation between fast convergence and uniform Hessian diagonal (small \(R_{\text{med}}^{\text{OPT}}(t)\)). In Appendix A we **study other adaptive algorithms (Adagrad, RMSprop and AMSGrad) and get similar observation**: all these adaptive methods converge faster than SGD or SGD+M and also bias the trajectory to regions with smaller \(R_{\text{med}}^{\text{OPT}}(t)\), suggesting the universality of our observation.

Despite the above positive correlation, it is reasonable to ask whether small \(R_{\text{med}}^{\text{OPT}}(t)\) indeed contributes to fast convergence or is just a byproduct of adaptive methods. To address this concern, in Appendix B.1, we add a supplementary experiment similar to that in Figure 1. We select two iterates \(x_{1}\) and \(x_{2}\) from two trajectories that both come from SGD+M (instead of one from Adam and one from SGD+M in Figure 1), such that the loss \(f(x_{1})=f(x_{2})\) but \(x_{2}\) has a smaller \(R_{\text{med}}^{\text{OPT}}(t)\) than \(x_{1}\). We then run SGD+M with the same configuration twice, once from \(x_{1}\) and once from \(x_{2}\). Under this setting, we get similar observation as before: running SGD+M from \(x_{2}\) (with smaller \(R_{\text{med}}^{\text{OPT}}(t)\)) achieves faster convergence than from \(x_{1}\). This suggests that the uniformity of the diagonal of loss Hessian (measured by \(R_{\text{med}}^{\text{OPT}}(t)\)) reveals some intrinsic trajectory property beyond the algorithm choice and is indeed a contributing factor to fast optimization. In Appendix B.2 we theoretically prove the contribution of small \(R_{\text{med}}^{\text{OPT}}(t)\) to fast optimization in a simplified setting.

**More discussions on the trajectory difference.** Considering the fact that our comparison between \(R_{\text{med}}^{\text{Adam}}(t)\) and \(R_{\text{med}}^{\text{SGD}}(t)\) is conditioned on the same iteration when SGD+M has larger training loss than Adam, there is a potential alternative explanation of the Hessian diagonal uniformity. That is, the global minimum has uniform Hessian, and Adam simply converges faster to it than SGD+M. To rule out this possibility, in Appendix A.3 we add a comparison of our measurements \(R_{\text{med}}^{\text{Adam}}(t)\) and \(R_{\text{med}}^{\text{SGD}}(t^{\prime})\), where \(t,t^{\prime}\) are picked such that \(t\)th Adam iterate and \(t^{\prime}\)th SGD+M iterate have the _same training loss_. The results (in Table 7) show that \(R_{\text{med}}^{\text{Adam}}(t)<R_{\text{med}}^{\text{SGD}}(t^{\prime})\) for most layers, thus demonstrating that the trajectories of Adam and SGD+M are truly different and that the difference is because Adam biases the local geometry (as opposed to faster convergence).

**Adding regularization.** People in practice usually add weight decay (equivalent to \(l_{2}\) regularization) to encourage better generalization ability. In Appendix A.7 we compare SGD+M and Adam when both using small weight decay values (0.001). The results in Figure 1(a) and Table 9 suggest that in this case, our observation still holds: Adam converges faster than SGD+M and in the vast majority of layers, \(R_{\text{med}}^{\text{Adam}}(t)\) values are smaller than \(R_{\text{med}}^{\text{SGD}}(t)\). This reveals the robustness of our observation under weak regularization. However, under large weight decay parameters, we observed cases where Adam still converged faster but \(R_{\text{med}}^{\text{Adam}}(t)\) values were larger rather than smaller. In the case of strong regularization, the adaptivity of Adam requires further exploration and we hope to find new mechanisms in the future.

## 5 Theoretical analysis

In Section 4, we empirically demonstrate the uniformity of diagonal geometry. In this section, we theoretically analyze this property for large batch Adam and SGD+M on a two-layer linear network with 1-dimensional output. Although simple, the choice of 2-layer linear networks to understand learning dynamics is common in prior works (_e.g._[15]). Moreover, in language transformer models when the weights are small, the softmax in the key-value-query structure is near the linear regime. Then this structure might be approximated by the product of 3 linear operators, similar to a three-layer linear network. Hence the theoretical analysis of this phenomenon on linear networks would be a good starting point for further understanding of more complicated language models.

### Problem setup

NotationLet \([d]=\{1,2,...,d\}\). We use \(\|\cdot\|_{2}\) to denote the \(l_{2}\) norm of a vector, and \(\|\cdot\|_{F}\) to denote the Frobenius norm of a matrix. Let \(\langle\cdot,\cdot\rangle\) be the Euclidean inner product between vectors or matrices. Let \(\mathcal{N}(\mu,\sigma^{2})\) be the one-dimensional Gaussian distribution with mean \(\mu\) and variance \(\sigma^{2}\). For a scalar (vector, matrix) \(A\) which evolves over time, we use \(A^{(t)}\) to denote its value at time \(t\).

Let there be \(m\) data points. The data matrix is \(X\in\mathbb{R}^{d_{x}\times m}\) and the label matrix is \(Y\in\mathbb{R}^{d_{y}\times m}\). We assume that the input dataset is whitened, i.e. \(\Lambda_{xx}:=\frac{1}{m}XX^{T}\in\mathbb{R}^{d_{x}\times d_{x}}\) is an identity matrix.

The parameters of a 2-layer linear network are given by \(W:=(W_{2},W_{1})\). Assume \(W_{i}\in\mathbb{R}^{d_{i}\times d_{i-1}}\) for \(i=1,2\). We have \(d_{2}=d_{y},d_{0}=d_{x}\). We consider the square loss \(L(W):=\frac{1}{2m}\|W_{2}W_{1}X-Y\|_{F}^{2}\).

Denote \(A:=\frac{1}{m}YX^{T}\in\mathbb{R}^{d_{y}\times d_{x}}\). [AGCH19] show that with whitened dataset,

\[L(W):=\frac{1}{2m}\|W_{2}W_{1}X-Y\|_{F}^{2}=\bar{L}(W)+c,\quad\bar{L}(W):= \frac{1}{2}\|W_{2}W_{1}-A\|_{F}^{2}.\] (2)

where \(c\) does not depend on \(W\). We consider the following model with small Gaussian initialization.

**Assumption 1** (Setup).: _The input covariance \(\Lambda_{xx}:=\frac{1}{m}XX^{T}\in\mathbb{R}^{d_{x}\times d_{x}}\) is an identity matrix. The input and hidden layers are both of dimension \(d\), i.e. \(d_{1}=d_{0}=d\). Without loss of generality, we can assume that \(A\) is a row vector (i.e. \(d_{2}=1\)) whose coordinates are positive9 and \(\Theta(1)\) in terms of \(d\)._

Footnote 9: In Assumption 2 we assume Gaussian initialization. Due to the rotational invariance of Gaussian distribution, we can assume that all coordinates of \(A\) are positive without loss of generality.

**Assumption 2** (Gaussian Initialization).: \(\forall i,j:w_{2i}^{(0)}\sim\mathcal{N}(0,\frac{1}{d^{2\alpha}}),W_{1}^{(0)} [i,j]\sim\mathcal{N}(0,\frac{1}{d^{2\alpha}})\) _are independently initialized with sufficiently large \(\alpha>0\)._

Denote \(\tilde{A}\) and \(\tilde{\Lambda}_{xx}\) as the batch versions of \(A\) and \(\Lambda_{xx}\). We make the following large-batch assumption. We emphasize that large batches are commonly used in NLP tasks (_e.g._ [BMR\({}^{+}\)20]).

**Assumption 3** (Large Batch).: _For the randomly selected batches, assume \(\mathbb{E}[\tilde{A}]=A\), \(\mathbb{E}[\tilde{\Lambda}_{xx}]=\Lambda_{xx}\). \(\forall i,j\in[d]:\mathbb{E}\left[(\tilde{A}_{i}-A_{i})^{2}\right]\leq\sigma^{2}\), \(\mathbb{E}\left[(\tilde{\Lambda}_{xx}[i,j]-\Lambda_{xx}[i,j])^{2}\right]\leq \sigma^{2}\), and \(\sigma^{2}=\mathcal{O}\left(\frac{1}{poly(d)}\right)\)._

Denote \(\tilde{g}^{(t)}\) as the batch gradient at time \(t\). The update rules of SGD+M and Adam are given by

SGD+M: \[u^{(t+1)}=\beta u^{(t)}+\tilde{g}^{(t)},\quad W^{(t+1)}=W^{(t)}- \eta u^{(t)},\] Adam: \[\eta_{t}=\eta\cdot\sqrt{1-\beta_{2}^{t+1}}/(1-\beta_{1}^{t+1}), \quad m^{(t+1)}=\beta_{1}m^{(t)}+(1-\beta_{1})\tilde{g}^{(t)},\] (3) \[v^{(t+1)}=\beta_{2}v^{(t)}+(1-\beta_{2})\tilde{g}^{(t)}\odot \tilde{g}^{(t)},\quad W^{(t+1)}=W^{(t)}-\eta_{t}\frac{m^{(t)}}{\sqrt{v^{(t)}}+ \xi},\]

where \(\eta\) is the learning rate, \(\beta,\beta_{1},\beta_{2}\) are momentum parameters, and \(\xi\) is for numerical stability. All operations on vectors are element-wise. Here and throughout, the notation \(f(x)=\mathcal{O}(g(x))\) (resp. \(f(x)=\Omega(g(x)),f(x)=\Theta(g(x))\)) means that \(\exists C_{1},C_{2}>0\) such that \(f(x)\leq C_{2}g(x)\) (resp. \(f(x)\geq C_{1}g(x)\), \(C_{1}g(x)\leq f(x)\leq C_{2}g(x)\)). Here \(C_{1},C_{2}\) may depend on \(\beta,\beta_{1},\beta_{2}\). We will also use the notation with \(\sim\), i.e. \(\tilde{\mathcal{O}}(\cdot),\tilde{\Omega}(\cdot),\tilde{\Theta}(\cdot)\) to hide factors of order \(\log d\). In our theoretical analysis, "with high probability", or "w.h.p." for short, means that with probability at least \(1-\frac{1}{poly(d)}\).

Since the weights and Hessians in different layers may have different magnitudes, we compute the \(R_{\text{med},k}^{\text{OPT}}(t)\) layer by layer. Denote \(R_{\text{med},k}^{\text{SGDM}}(t)\) (resp. \(R_{\text{med},k}^{\text{Adam}}(t)\)) as the \(R_{\text{med}}^{\text{OPT}}(t)\) found by SGD+M (resp. Adam) w.r.t. \(W_{k}\) at time \(t\) where \(k=1,2\).

### Main results

**Theorem 1**.: _Under Assumption 1, 2 and 3, consider the weights \(\left\{W_{\text{SGD}}^{(t)}\right\}_{t\geq 0}\) (resp. \(\left\{W_{\text{Adam}}^{(t)}\right\}_{t\geq 0}\)) obtained by SGD+M (resp. Adam) defined in (3)._

_1. For any_ \(p>0\)_, pick_ \(0<\epsilon<\frac{1}{d^{p}}\)_,_ \(\eta\leq\mathcal{O}\left(\frac{\epsilon}{d^{\alpha/4+4}}\right)\) _and_ \(\alpha\geq 4(p+2)\)_. Suppose_ \(\sigma\leq\frac{\eta^{3/2}}{d^{\alpha/2+1}}\)_, then there exists_ \(T_{\text{SGD},1},T_{\text{SGD},2}\) _such that w.h.p.,_ \(\bar{L}\left(W_{\text{SGD}}^{(T_{\text{SGD},1})}\right)=\Theta(d)\)_,_ \(\bar{L}\left(W_{\text{SGD}}^{(T_{\text{SGD},2})}\right)\leq\tilde{\mathcal{O}} \left(\frac{1}{d^{p}}\right)\)_, and_

\[\forall t\in[T_{\text{SGD},1},T_{\text{SGD},2}]:\quad R_{\text{med},k}^{ \text{SGDM}}(t)=\Omega(\log d),\quad k=1,2.\]_2. For any \(p>0\), pick \(\eta\leq\mathcal{O}\left(\frac{1}{d^{3\alpha}}\right),\xi\leq\sqrt{\frac{\eta}{d^{ 3\alpha-1}}}\), \(\alpha\geq\frac{p+4}{3}\) and \(\beta_{2}=\beta_{1}^{2}\).10 Suppose \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{13/4}}\). Then \(\exists T_{\text{Adam},1},T_{\text{Adam},2}\) such that w.h.p., \(\bar{L}\left(W_{\text{Adam},1}^{(T_{\text{Adam},1})}\right)=\Theta(d)\), \(\bar{L}\left(W_{\text{Adam}}^{(T_{\text{Adam},2})}\right)\leq\tilde{\mathcal{ O}}\left(\frac{1}{d^{p}}\right)\), and_

Footnote 10: The assumption \(\beta_{2}=\beta_{1}^{2}\) is only for convenience to make the proof easier but is not essential. Our result can also generalize to cases with other \(\beta_{1}\) and \(\beta_{2}\). See Appendix C for more discussions.

\[\forall t\in[T_{\text{Adam},1},T_{\text{Adam},2}]:\quad R_{\text{med},k}^{ \text{Adam}}(t)=1+\tilde{\mathcal{O}}\left(\eta^{\frac{1}{4}}+\frac{1}{d^{ \frac{\alpha}{2}-\frac{1}{4}}}\right),\quad k=1,2.\]

RemarkTheorem 1 holds for all values of hyperparameters (such as \(\alpha,\sigma\)) in certain ranges instead of just particular values. The ranges of SGD+M and Adam overlap with each other. That means we can choose the same hyperparameters for SGD+M and Adam in the overlapped region to make a fair comparison, for example, same \(\alpha,\sigma\) such that \(\alpha\geq 4(p+2)\) and \(\sigma\leq\min\{\frac{\eta^{3/2}}{d^{\alpha/2+1}},\frac{\eta^{3/2}\xi^{2}}{d^{ 13/4}}\}\).

An immediate corollary of this theorem below gives the difference between iterates of Adam and SGD+M that have the same loss.

**Corollary 1**.: _Under the setup in Theorem 1, w.h.p., for any \(t\in[T_{\text{SGD},1},T_{\text{SGD},2}]\) and \(t^{\prime}\in[T_{\text{Adam},1},T_{\text{Adam},2}]\) such that \(\bar{L}\left(W_{\text{SGD}}^{(t)}\right)=\bar{L}\left(W_{\text{Adam}}^{(t^{ \prime})}\right)\in\left[\tilde{\Omega}\left(\frac{1}{d^{p}}\right),\Theta(d)\right]\), we have_

\[R_{\text{med},k}^{\text{SGDM}}(t)=\Omega(\log d),\quad R_{\text{med},k}^{ \text{Adam}}(t^{\prime})=1+\tilde{\mathcal{O}}\left(\eta^{\frac{1}{4}}+\frac{1 }{d^{\frac{\alpha}{2}-\frac{1}{4}}}\right),\quad k=1,2.\]

Theorem 1 and Corollary 1 tell us that during a long training period when the loss decreases from \(\Theta(d)\) to \(\tilde{\mathcal{O}}\left(\frac{1}{d^{p}}\right)\), the diagonal of loss Hessian for Adam keeps nice uniformity: \(R_{\text{med},k}^{\text{Adam}}(t)=1\pm o(1),k=1,2\). On the other hand, the diagonal of loss Hessian for SGD+M is less uniform. Appendix C gives a proof sketch of Theorem 1. The detailed proof can be found in Appendix D and E.

## 6 Low-rank weight matrices and uniformity of leading singular vectors

The proof sketch in Appendix C highlights one crucial intuition of Theorem 1: After \(T_{\text{SGD},1}\) (resp. \(T_{\text{Adam},1}\)) steps, \(W_{1}\) of SGD+M (resp. Adam) becomes an approximately rank-1 matrix. Consider the left singular vector \(\bm{u}:=[u_{1},u_{2},...,u_{d}]^{T}\) which corresponds to the leading singular value \(\sigma_{1}\). We can show that the distribution of \(u_{1}^{2},u_{2}^{2},...,u_{d}^{2}\) for Adam is more uniform than that of SGD+M. This property, we call _the uniformity of the leading singular vector_, is related to the uniformity of the diagonal of loss Hessian, see Appendix G for more details.

Similar low rank bias after training has been studied in prior works (_e.g._[1, 17, 18, CGMR20]). For more complicated models, we want to check whether the weight matrices also have low rank structures and if so, whether we can still observe _the uniformity of leading singular vectors_. More formally, consider the weight matrix in some layer \(W\in\mathbb{R}^{m\times n}\), we want to check

(A) Whether \(W\in\mathbb{R}^{m\times n}\) is approximately a rank \(k\) matrix with \(k\ll\min\{m,n\}\), and if true,

(B) Consider the top \(k\) singular values \(\sigma_{1},...,\sigma_{k}\) and left singular vectors \(\bm{u}_{1},...,\bm{u}_{k}\). Define \(\tilde{\bm{u}}:=\sum_{i=1}^{k}\sigma_{i}^{2}\bm{u}_{i}\odot\bm{u}_{i}:=[ \tilde{u}_{1},...,\tilde{u}_{d}]^{T}\) and compute \(R_{u}:=\frac{\max_{i}\tilde{u}_{i}}{\text{median}\tilde{u}_{i}}\), a generalized version of \(\frac{\max_{i}u_{i}^{2}}{\text{median}\ u_{i}^{2}}\) in the rank-1 case. We want to see whether \(R_{u}\) obtained by Adam is smaller than that of SGD+M.

After reviewing the weight matrices we got in different settings, we observed that (A) and (B) hold for many layers in those models. For example, on the translation task mentioned in Section 4.1, we found 12 layers which had approximately low rank structures and for 10 of them, \(R_{u}\) values (defined in (B)) obtained by Adam were smaller than those found by SGD+M. Figure 4 shows the result on one typical layer. Results of more layers can be found in Appendix A.5.

RemarkThe definition of \(R_{u}\) is based on the connection between diagonal of loss Hessian and weight matrices. Appendix G shows that for a 2-layer linear network, \(R_{\text{med},2}^{\text{OPT}}(t)=\frac{\max_{i}\|W_{1}^{(t)}[:,\cdot]\|_{2}^{2} }{\text{median}\|W_{1}^{(t)}[:,\cdot]\|_{2}^{2}}\). When \(W_{1}\in\mathbb{R}^{m\times n}\) is approximately rank \(k\), i.e. \(W_{1}\approx\sum_{i=1}^{k}\sigma_{i}\bm{u}_{i}\bm{v}_{i}^{T}\), denote \(\bm{u}_{i}=[u_{i1},u_{i2},...,u_{im}]^{T}\) and \(\bm{v}_{i}=[v_{i1},v_{i2},...,v_{in}]^{T}\), we have that for the \(j\)-th row,\(\|W_{1}[j,:]\|_{2}^{2}\ \approx\ \left\|\sum_{i=1}^{k}\sigma_{i}u_{ij}\bm{v}_{i}^{T} \right\|_{2}^{2}=\sum_{i=1}^{k}\sigma_{i}^{2}u_{ij}^{2}\). By defining \(\tilde{\bm{u}}=[\tilde{u}_{1},\tilde{u}_{2},...,\tilde{u}_{d}]^{T}:=\sum_{i=1} ^{k}\sigma_{i}^{2}\bm{u}_{i}\odot\bm{u}_{i}\), we have that \(\|W_{1}[j,:]\|_{2}^{2}\approx\tilde{u}_{j}\). Although in multi-layer nonlinear neural networks, the connection between diagonal of loss Hessian and the weight matrices is more complicated and \(R_{\text{med},2}^{\text{OPT}}(t)\) may depend on the product of many weight matrices rather than one single matrix, we still believe that this definition of \(R_{u}\) is a reasonable ratio to consider.

## 7 Conclusion and future work

We demonstrate that adaptive optimization methods bias the training trajectory towards a region where the diagonal of loss Hessian is more uniform, through extensive experiments on language models and theoretical analysis in a simplified setting of two-layer linear networks. Although our findings may not directly lead to an improved algorithm for practical use, they provide a new way of thinking when designing new algorithms: in contrast with the traditional view which tries to design a method that performs better _in_ the bad loss geometry, our findings suggest that we can design algorithms which _implicitly avoid_ regions with bad geometry. There are a lot of future directions along this line. For example, our theoretical results on the two-layer linear networks may be able to generalize to multi-layer networks. As is discussed in Section 5, the key-value-query structure in the transformer models might be approximated by a three-layer linear network and the analysis of multi-layer networks might provide more connection to real deep models and could be an interesting and challenging future direction. Moreover, it is also possible to relax our large-batch assumption (Assumption 3) and prove similar results in the general stochastic setting.

## Acknowledgments and Disclosure of Funding

This project was supported by NSF CCF 2145703.

## References

* [ACH18] Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In _35th International Conference on Machine Learning, ICML 2018_, pages 372-389. International Machine Learning Society (IMLS), 2018.
* [ADH\({}^{+}\)19] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In _International Conference on Machine Learning_, pages 322-332. PMLR, 2019.
* [AGCH19] Sanjeev Arora, Noah Golowich, Nadav Cohen, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In _7th International Conference on Learning Representations, ICLR 2019_, 2019.
* [AZLL19] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [AZLS19] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International Conference on Machine Learning_, pages 242-252. PMLR, 2019.
* [BDR21] Prajjwal Bhargava, Aleksandr Drozd, and Anna Rogers. Generalization in nli: Ways (not) to go beyond simple heuristics, 2021.
* [BMR\({}^{+}\)20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901. Curran Associates, Inc., 2020.
* [CGMR20] Hung-Hsu Chou, Carsten Gieshoff, Johannes Maly, and Holger Rauhut. Gradient descent for deep matrix factorization: Dynamics and implicit bias towards low rank. _arXiv preprint arXiv:2011.13772_, 2020.
* [CKL\({}^{+}\)21] Jeremy M. Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* [CZT\({}^{+}\)20] Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. Closing the generalization gap of adaptive gradient methods in training deep neural networks. In Christian Bessiere, editor, _Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020_, pages 3267-3275. ijcai.org, 2020.
* [DBBU20] Alexandre Defossez, Leon Bottou, Francis Bach, and Nicolas Usunier. A simple convergence proof of adam and adagrad. _arXiv preprint arXiv:2003.02395_, 2020.
* [DCLT19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 4171-4186. Association for Computational Linguistics, 2019.
* [DHS11] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. _Journal of machine learning research_, 12(7), 2011.
* [DZPS18] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In _International Conference on Learning Representations_, 2018.
* [EFSS16] Desmond Elliott, Stella Frank, Khalil Sima'an, and Lucia Specia. Multi30k: Multilingual english-german image descriptions. In _Proceedings of the 5th Workshop on Vision and Language_, pages 70-74. Association for Computational Linguistics, 2016.
* [ENV21] Alina Ene, Huy L. Nguyen, and Adrian Vladu. Adaptive gradient methods for constrained convex optimization and variational inequalities. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pages 7314-7321. AAAI Press, 2021.
* [GWB\({}^{+}\)17] Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. _Advances in Neural Information Processing Systems_, 30, 2017.
* [JGH18] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* [JT20] Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* [Kaw16] Kenji Kawaguchi. Deep learning without poor local minima. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.

* [KB15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015.
* [LBD\({}^{+}\)20] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. _arXiv preprint arXiv:2003.02218_, 2020.
* [Ler19] Matthieu Lerasle. Lecture notes: Selected topics on robust statistical learning theory. _arXiv preprint arXiv:1908.10761_, 2019.
* [LMZ18] Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, _Proceedings of the 31st Conference On Learning Theory_, volume 75 of _Proceedings of Machine Learning Research_, pages 2-47. PMLR, 06-09 Jul 2018.
* [LZB22] Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. _Applied and Computational Harmonic Analysis_, 2022.
* [MDP\({}^{+}\)11] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_, pages 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
* [MXBS17] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018.
* [TCG21] Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics without contrastive pairs. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 10268-10278. PMLR, 18-24 Jul 2021.
* [TCLT19] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: On the importance of pre-training compact models. _arXiv preprint arXiv:1908.08962v2_, 2019.
* [TH\({}^{+}\)12] Tijmen Tieleman, Geoffrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. _COURSERA: Neural networks for machine learning_, 4(2):26-31, 2012.
* [VSP\({}^{+}\)17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [WWB20] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes. _Journal of Machine Learning Research_, 21(219):1-30, 2020.
* [ZCS\({}^{+}\)22] Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. Adam can converge without any modification on update rules. In _NeurIPS_, 2022.

[MISSING_PAGE_FAIL:13]

[MISSING_PAGE_FAIL:14]

### Comparison conditioned on the same loss

In this section, we compare \(R_{\text{med}}^{\text{SGDM}}(t)\) and \(R_{\text{med}}^{\text{Adam}}(t)\) conditioned on the same training loss. More precisely, we make comparison of \(R_{\text{med}}^{\text{Adam}}(t)\) and \(R_{\text{med}}^{\text{SGDM}}(t^{\prime})\), where \(t,t^{\prime}\) are picked such that \(t\)th Adam iterate and \(t^{\prime}\)th SGD+M iterate have the same training loss. The details of the tasks are described in Section 4.1. Table 7 shows the results of \(R_{\text{med}}^{\text{Adam}}(t)\) and \(R_{\text{med}}^{\text{SGDM}}(t^{\prime})\) in some layers.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{Layer} & \multicolumn{3}{c}{EERO 10} & \multicolumn{3}{c}{EERO 20} & \multicolumn{3}{c}{EERO 40} \\  & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) \\ \hline
3 & 3.97 & 2.69 & 2.56 & 2.33 & 1.89 & 1.68 & 2.83 & 1.62 & 1.56 \\ \hline
5 & 26.17 & 21.19 & 11.36 & 3.71 & 17.83 & 10.85 & 51.94 & 10.22 & 12.31 \\ \hline
7 & 4.10 & 6.98 & 6.12 & 3.94 & 4.95 & 2.92 & 7.58 & 2.29 & 2.58 \\ \hline
9 & 29.41 & 35.72 & 25.86 & 37.81 & 19.89 & 16.90 & 30.68 & 16.24 & 9.97 \\ \hline
12 & 4.93 & 6.20 & 12.67 & 4.63 & 6.61 & 4.64 & 6.44 & 5.13 & 4.06 \\ \hline
15 & 85.06 & 33.63 & 19.51 & 140.99 & 12.22 & 6.72 & 44.07 & 6.98 & 5.37 \\ \hline
18 & 8.71 & 2.99 & 9.48 & 3.86 & 2.44 & 4.16 & 3.51 & 2.10 & 2.35 \\ \hline
21 & 95.34 & 11.68 & 6.62 & 47.20 & 6.37 & 4.74 & 22.20 & 4.58 & 3.58 \\ \hline
24 & 8.70 & 5.67 & 6.95 & 8.13 & 3.59 & 5.13 & 6.46 & 2.30 & 2.83 \\ \hline
28 & 4.44 & 2.42 & 2.64 & 4.67 & 1.85 & 1.81 & 2.63 & 1.46 & 2.13 \\ \hline \hline \end{tabular}
\end{table}
Table 6: \(R_{\text{med}}^{\text{SGDM}}(t)\), \(R_{\text{med}}^{\text{SGDM}}(t)\) and \(R_{\text{med}}^{\text{SGDM}}(t)\) in some layers, on the translation task described in Section 4.1

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{Layer} & \multicolumn{3}{c}{EERO 10} & \multicolumn{3}{c}{EERO 20} \\  & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) \\ \hline
3 & 4.01 & 4.45 & 5.90 & 3

### Experiments of switching from SGD to Adam

In this section we describe another learning schedule: the "Adam after SGD" schedule, where we switch from SGD to Adam in the middle to see whether the loss and \(R_{\text{med}}^{\text{OPT}}(t)\) can catch up with the model trained by Adam from the very beginning. Again, we used the same model as in the translation task in Section 4.1. In this section, we did not add momentum term to SGD in order to get a larger gap between SGD and Adam than the case using momentum. We want to see whether this larger gap can be closed after switching to Adam in the middle.

As is shown in Figure 7 and Table 8, both the loss gap and the gap of \(R_{\text{med}}^{\text{OPT}}(t)\) were closed after a period of training after switching algorithms, which provides evidence of the connection between convergence speed and uniformity of diagonal of loss Hessian.

### The low rank structure

In this section, we present more results for the experiments in Section 6.

We examined the weights of the model trained for the translation task in Section 4.1. Among roughly 30 layers, we observed that for 12 layers, at least the weight matrices obtained by Adam after training have approximately low rank structures.

Figure 8 shows the examples of layers with or without the low rank structure.

We then studied the uniformity of leading singular vectors of these 12 layers, i.e. computed \(R_{u}\) defined in (B) of Section 6. The observation was that for 10 out of these 12 layers, \(R_{u}\) values of Adam were smaller those of SGD, which implies the uniformity of leading left singular vectors of Adam. Here we may also want to consider the right singular vectors \(\bm{v}_{1},\bm{v}_{2},...\bm{v}_{k}\) and corresponding \(\tilde{\bm{v}}=[\tilde{v}_{1},\tilde{v}_{2},...,\tilde{v}_{d}]^{T}:=\sum_{i=1 }^{k}\sigma_{i}^{2}\bm{v}_{i}\odot\bm{v}_{i}\) and compute \(R_{v}:=\frac{\max_{i}\tilde{v}_{i}}{\text{median}\tilde{v}_{i}}\) for Adam and SGD+M. However, on this translation task, among the 12 layers which were approximately low rank, for only 6 of them, \(R_{v}\) of Adam were smaller, i.e. we did not observe uniformity of the leading right singular

Figure 8: Examples of singular values on (left) layers without low rank structure and (right) layers with approximately low rank structure.

vector for Adam. One possible reason is that for a weight matrix, its right singular vectors are closer to the input data than left singular vectors and more easily influenced by the data, therefore may not show uniformity. Figure 9 shows how \(R_{u}\) and \(R_{v}\) changed over time in some layers.

### How does the (adaptive) gradient align with diagonal of loss Hessian

In this section we present the uniformity of diagonal geometry of adaptive methods from another perspective. Denote \(H_{ii}\) as the \((i,i)\)-th element of the loss Hessian \(H\) and \(g_{i}\) as the \(i\)-th element of the gradient. It is conjectured that when \(|H_{ii}|\) is large, the corresponding \(|g_{i}|\) is usually large as well. For adaptive methods, we can regard the update per step as the learning rate times the "adaptive gradient". Let's use \(g_{\text{adapt},i}\) to represent the \(i\)-th component of the adaptive gradient. Through experiments on language models, we found that \(|g_{\text{adapt},i}|\) for different \(i\) are quite uniform and do not align with \(|H_{ii}|\) as the true gradient \(|g_{i}|\) does.

In the experiments, we first sorted \(|H_{ii}|\) in the ascent order: \(|H_{i_{1},i_{1}}|\leq|H_{i_{2},i_{2}}|\leq...\leq|H_{i_{d},i_{d}}|\) (suppose \(H\in\mathbb{R}^{d\times d}\)), and then plotted the corresponding \(|g_{i_{k}}|\) and \(|g_{\text{adapt},i_{k}}|\) for \(k\in[d]\).

#### a.6.1 SGD vs. Adagrad

Here we compare SGD and Adagrad on the language modeling task on wikitext-2 described in Section A.1. We observed that the figures of all layers are quite similar so we select one layer as an example, as is shown in Figure 10.

#### a.6.2 SGD with momentum vs. Adam

Here we compare Adam and SGD+M on the tasks described in Section 4.1. Again, we select one layer as an example for each task. Figure 11 shows the results on the sentence classification task and Figure 12 shows the results on the translation task.

Figure 9: \(R_{u}\) and \(R_{v}\) for Adam and SGD+M in some layers

### Adding regularization and other tricks

In this section, we add weight decay to both Adam and SGD+M on the translation task described in Section 4. The momentum parameter \(\beta\) in SGD was set as 0.9. The two momentum parameters \((\beta_{1},\beta_{2})\) of Adam were set as (0.9, 0.98). For both algorithms, we set the weight decay parameter as 0.001. We trained the model using constant learning rates for 60 epochs (1800 iterations). We tuned and chose the best learning rate 0.03 for SGD+M. The learning rate of Adam was set as 0.0001, under which Adam converged faster than SGD+M with its best learning rate 0.03. Figure 13a shows the training losses and Table 9 shows the values of \(R_{\text{med}}^{\text{Adam}}(t)\), \(R_{\text{med}}^{\text{SGDM}}(t)\) and \(\frac{R_{\text{med}}^{\text{SGDM}}(t)}{R_{\text{med}}^{\text{Adam}}(t)}\) in some randomly selected layers.

Figure 11: How the true gradient (\(\{|g_{i_{k}}|\}_{k=1}^{d}\)) and “adaptive gradient” (\(\{|g_{\text{adapt},i_{k}}|\}_{k=1}^{d}\)) align with diagonal of Hessian (\(\{|H_{i_{k},i_{k}}|\}_{k=1}^{d}\)). Here coordinates are sorted such that \(|H_{i_{1},i_{1}}|\leq|H_{i_{2},i_{2}}|\leq...\leq|H_{i_{d},i_{d}}|\) (suppose \(H\in\mathbb{R}^{d\times d}\)). Experiments were conducted on the sentence classification task described in Section 4.1. This figure shows the results on the 12-th layer.

### Results on image tasks

Although in this paper we focus on language models where Adam shows significant fast convergence, we also add results in this section on image tasks where SGD+M performs better. We trained a ResNet12 on CIFAR-10 dataset and compared the convergence speed and \(R^{\text{OPT}}_{\text{med}}(t)\) of SGD+M and Adam. The momentum parameter \(\beta\) in SGD was set as 0.9. The two momentum parameters (\(\beta_{1},\beta_{2}\)) of Adam were set as (0.9, 0.98). The model was trained using constant learning rates for 41 epochs (2050 iterations). We tuned and chose the best learning rates for both algorithms: 0.5 for SGD+M and 0.005 for Adam. Figure 12(b) shows the training losses and Table 10 shows the values of \(R^{\text{Adam}}_{\text{med}}(t)\), \(R^{\text{SGD}}_{\text{med}}(t)\) and \(\frac{R^{\text{SGD}}_{\text{med}}(t)}{R^{\text{Adam}}_{\text{med}}(t)}\).

Footnote 12: We borrowed the implementation here https://pytorch-tutorial.readthedocs.io/en/latest/tutorial/chapter03_intermediate/3\(2\)2_cnn_resnet_cifar10/ and replace the “layers” array [2,2,2] with [1,1,1].

As we can see, on this image task, Adam does not converge faster than SGD+M and in the meantime, \(R^{\text{Adam}}_{\text{med}}(t)\) values were no longer smaller than \(R^{\text{SGD}}_{\text{med}}(t)\) during training. This reveals the connection between the local diagonal geometry and the convergence speed from another perspective. That is, when the diagonal of Hessian of Adam is not more uniform than SGD+M, its convergence speed is not better, either.

Figure 12: How the true gradient (\(\{|g_{i_{k}}|\}_{k=1}^{d}\)) and “adaptive gradient” (\(\{|g_{\text{adapt},i_{k}}|\}_{k=1}^{d}\)) align with diagonal of Hessian (\(\{|H_{i_{k},i_{k}}|\}_{k=1}^{d}\)). Here coordinates are sorted such that \(|H_{i_{1},i_{1}}|\leq|H_{i_{2},i_{2}}|\leq...\leq|H_{i_{d},i_{d}}|\) (suppose \(H\in\mathbb{R}^{d\times d}\)). Experiments were conducted on the translation task described in Section 4.1. This figure shows the results on the 5-th layer.

### Comparison between \(R_{\text{med}}^{\text{OPT}}(t)\) and singular value-based metrics

In Section 4, through extensive experiments on language models, we demonstrate that when we train the neural network using Adam, the uniformity of diagonal geometry, measured by \(R_{\text{med}}^{\text{OPT}}(t)\) is smaller than that when we train using SGD+M from the same initialization, for the vast majority of layers. We are aware that people also usually consider Hessian singular values instead of diagonal entries to measure the loss geometry. Hence in this section we make a comparison between our diagonal-based metric and singular value-based metrics.

First, we believe that our metric has a natural connection to the mechanism that underlies adaptive methods. Adaptive methods in practice choose coordinate-wise adaptive learning rates. From a high-level perspective, this procedure can be viewed as adapting to the loss smoothness with respect to each coordinate. The smoothness of certain coordinate is measured by the second derivative with respect to this coordinate and therefore corresponds to the diagonal entries of loss Hessian. Our metric, which measures these diagonal entries, is thus fundamentally intertwined with the mechanism that underlies adaptive methods.

Next, we empirically demonstrate that our metric \(R_{\text{med}}^{\text{OPT}}(t)\) is a reasonable proxy of singular value-based metrics. Define a singular value-based metric \(S_{\text{med}}^{\text{OPT}}(t):=\frac{\max\{\sigma_{i}(t)\}_{i=1}^{d}}{ \text{median}\{\sigma_{i}(t)\}_{i=1}^{d}}\) as an analogy of our diagonal-based metric \(R_{\text{med}}^{\text{OPT}}(t)\), where \(\{\sigma_{i}(t)\}_{i=1}^{d}\) denotes the singular values of loss Hessian \(H(t)\in\mathbb{R}^{d\times d}\) at the \(t\)th iterate. We compare \(S_{\text{med}}^{\text{OPT}}(t)\) along the trajectories of Adam and SGD+M in the translation task described in Section 4.1. Table 11 suggests that if measured by singular values, Adam is also biased to a region with smaller \(S_{\text{med}}^{\text{OPT}}(t)\) than SGD+M, similar to the observation for \(R_{\text{med}}^{\text{OPT}}(t)\). This is expected because in Appendix F, we demonstrate that the loss Hessian approaches diagonal during training. The fact that our diagonal-based metric and singular value-based metric give the same result also reveals the robustness of our observation to the choice of metric, demonstrating that there does exist some geometry bias of Adam towards more uniform regions even when measured by different metrics.

Figure 13: (a) Training losses of Adam and SGD+M for the translation task, both with weight decay. (b) Training losses of Adam and SGD+M for a ResNet trained on CIFAR-10.

Finally, there is strong reason why our metric is often easier to compute empirically and analyze theoretically than singular value-based metrics such as \(S_{\text{med}}^{\text{OPT}}(t)\).

1. From the empirical computation perspective, suppose the loss Hessian is \(d\times d\). Then computing its singular values, in general, requires computing the whole matrix with \(d^{2}\) elements. However, our metric only requires computing the \(d\) diagonal entries.
2. From the theoretical analysis perspective, in Appendix G, we show that the diagonal of loss Hessian in linear networks can be connected to weight matrices by simple formulas. These straightforward formulas simplify the analysis and allow us to connect our metric to the low-rank structure of weight matrices and the uniformity of their leading singular vectors (see Section 6 for more discussions). However, all these nice connections fail to hold for singular value-based metrics. The formulas of singular values are very complicated even in linear networks, making it almost impossible to theoretically analyze any singular value-based metrics.

### The uniformity of diagonal geometry on in-distribution test data

In this section we compare \(R_{\text{med}}^{\text{Adam}}(t)\) and \(R_{\text{med}}^{\text{SGDM}}(t)\) on the in-distribution test data. The task is the translation task described in Section 4.1. Table 12 validates that on in-distribution test data, Adam is also biased to a region with smaller \(R_{\text{med}}^{\text{OPT}}(t)\) than SGD+M, similar to what happens on the training data shown in Table 2a. This is expected because of the same distribution. One other thing we want to emphasize is that, in real language tasks, the dataset is typically very large and the model see each training example only once. Hence the training behavior usually implies similar in-distribution test behavior.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{Layer\#} & \multicolumn{2}{c}{Epoch 0} & \multicolumn{3}{c}{Epoch 30} & \multicolumn{3}{c}{Epoch 55} \\  & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{Adam}}(t)\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{Adam}}(t)\) & \(\frac{R_{\text{med}}^{\text{Adam}}(t)}{R_{\text{med}}^{\text{SGD}}(t)}\) & \(R_{\text{med}}^{\text{SGDM}}(t)\) & \(R_{\text{med}}^{\text{Adam}}(t)\) & \(\frac{R_{\text{med}}^{\text{SGDM}}(t)}{R_{\text{med}}^{\text{SGD}}(t)}\) \\ \hline
3 & 4.39 & 4.39 & 5.80 & 3.06 & 1.89 & 6.79 & 2.81 & 2.42 \\ \hline
5 & 7.90 & 7.90 & 38.01 & 11.71 & 3.24 & 41.40 & 10.21 & 4.06 \\ \hline
7 & 5.77 & 5.77 & 6.00 & 4.61 & 1.30 & 5.53 & 3.20 & 1.73 \\ \hline
9 & 25.09 & 25.09 & 28.81 & 17.17 & 1.68 & 16.67 & 14.85 & 1.12 \\ \hline
12 & 10.24 & 10.24 & 9.13 & 8.63 & 1.06 & 13.78 & 9.09 & 1.52 \\ \hline
15 & 79.71 & 79.71 & 77.18 & 13.56 & 5.69 & 37.93 & 9.91 & 3.83 \\ \hline
18 & 14.78 & 14.78 & 3.94 & 7.15 & 0.55 & 5.42 & 6.04 & 0.90 \\ \hline
21 & 83.25 & 83.25 & 26.04 & 5.44 & 4.79 & 13.11 & 5.57 & 2.36 \\ \hline
24 & 29.91 & 29.91 & 6.89 & 5.42 & 1.27 & 6.51 & 7.16 & 0.91 \\ \hline
28 & 22.57 & 22.57 & 5.39 & 3.94 & 1.37 & 6.13 & 2.14 & 2.87 \\ \hline \hline \end{tabular}
\end{table}
Table 12: \(R_{\text{med}}^{\text{Adam}}(t)\) and \(R_{\text{med}}^{\text{SGDM}}(t)\) in some layers for the translation task on in-distribution test data.

[MISSING_PAGE_FAIL:22]

### Theoretical analysis on a simple setting

In this section, we add the following lemma on a simplified setting where smaller \(R_{\text{med}}^{\text{OPT}}(t)\) provably contributes to faster optimization.

**Lemma 1**.: _Consider a diagonal quadratic loss \(f(x)=\sum_{i=1}^{d}\frac{\lambda_{i}}{2}x_{i}^{2}\) where \(\lambda_{1}\geq\lambda_{2}\geq...\geq\lambda_{d}\geq 0\). Run GD with learning rate \(\eta=\frac{1}{\lambda_{1}}\) and produce \(\{x_{i}^{(t)}\}_{t\geq 0},1\leq i\leq d\). We initialize the parameters as \(x_{i}^{(0)}=\pm 1,\forall i\in[d]\). Without loss of generality, suppose \(d\) is even. Then we have that for any \(p>0\), after \(T=\frac{p}{2}\cdot R_{\text{med}}^{\text{OPT}}\cdot\log d=\frac{p}{2}\cdot \frac{\lambda_{1}}{\text{median}\{\lambda_{i}\}_{i=1}^{d}}\log d\) steps, the loss \(f(x)\) will shrink by a half. More precisely,_

\[f\left(x^{(T)}\right)\leq\left(\frac{1}{2}+d^{-p}\right)f\left(x^{(0)}\right).\]

Lemma 1 demonstrates that under this simplified setting, even when we just use the vanilla GD with no adaptive methods, a more uniform diagonal geometry (characterized by smaller \(R_{\text{med}}^{\text{OPT}}\)) can still lead to faster optimization.

Proof.: By the update of GD with learning rate \(\eta=\frac{1}{\lambda_{1}}\), we have that

\[\forall i\in[d]: x_{i}^{(t+1)}=\left(1-\frac{\lambda_{i}}{\lambda_{1}}\right)x_{i }^{(t)},\] \[\Rightarrow \left(x_{i}^{(t)}\right)^{2}=\left(1-\frac{\lambda_{i}}{\lambda_{ 1}}\right)^{2t}\left(x_{i}^{(0)}\right)^{2}\leq\exp\left(-\frac{2\lambda_{i}t }{\lambda_{1}}\right)\left(x_{i}^{(0)}\right)^{2}.\]

Then after \(T=\frac{p}{2}\cdot\frac{\lambda_{1}}{\text{median}\{\lambda_{i}\}_{i=1}^{d}} \log d\) steps, we have that

\[\forall i\leq\frac{d}{2}:\quad\left(x_{i}^{(T)}\right)^{2}\leq d^{-p}\left(x_ {i}^{(0)}\right)^{2},\Rightarrow\sum_{i=1}^{d/2}\lambda_{i}\left(x_{i}^{(T)} \right)^{2}\leq d^{-p}\sum_{i=1}^{d/2}\lambda_{i}\left(x_{i}^{(0)}\right)^{2} \leq d^{-p}f\left(x^{(0)}\right).\]

On the other hand,

\[\sum_{i=d/2+1}^{d}\lambda_{i}\left(x_{i}^{(T)}\right)^{2}\leq\sum_{i=d/2+1}^{ d}\lambda_{i}\left(x_{i}^{(0)}\right)^{2}\overset{(i)}{\leq}\sum_{i=1}^{d/2} \lambda_{i}\left(x_{i}^{(0)}\right)^{2},\]

where \((i)\) is by definition of median and the initialization \(\forall i\in[d]:x_{i}^{(0)}=\pm 1\). Combining with the fact \(\sum_{i=d/2+1}^{d}\lambda_{i}\left(x_{i}^{(0)}\right)^{2}+\sum_{i=1}^{d/2} \lambda_{i}\left(x_{i}^{(0)}\right)^{2}=f\left(x^{(0)}\right)\) yields that \(\sum_{i=d/2+1}^{d}\lambda_{i}\left(x_{i}^{(T)}\right)^{2}\leq\frac{1}{2}f \left(x^{(0)}\right)\). Hence we have

\[f\left(x^{(T)}\right)=\sum_{i=1}^{d/2}\lambda_{i}\left(x_{i}^{(T)}\right)^{2} +\sum_{i=d/2+1}^{d}\lambda_{i}\left(x_{i}^{(T)}\right)^{2}\leq\left(\frac{1}{ 2}+d^{-p}\right)f\left(x^{(0)}\right).\]

## Appendix C Proof sketch of Theorem 1

Now we give a proof sketch of Theorem 1, which contains three major steps. The detailed proof can be found in Appendix G, D and E.

First we relate the diagonal of Hessian to weight matrices \(W_{1},W_{2}\). Under Assumption 1, denote \(W_{1}[i,:]\) as the \(i\)-th row of \(W_{1}\) and \(W_{2}:=[w_{2i},w_{22},...,w_{2d}]\). Since the input dataset is whitened, we can show that

\[R_{\text{med},1}^{\text{OPT}}(t)=\frac{\max_{i}\left(w_{2i}^{(t)}\right)^{2}}{ \text{median}\left(w_{2i}^{(t)}\right)^{2}},\quad R_{\text{med},2}^{\text{ OPT}}(t)=\frac{\max_{i}\left\|W_{1}^{(t)}[i,:]\right\|_{2}^{2}}{\text{ median}\left\|W_{1}^{(t)}[i,:]\right\|_{2}^{2}}.\]Next, due to the one-dimensional output, we can prove that \(W_{1}\) converges to an approximately rank-1 matrix. More precisely, we have

\[W_{1}^{(t)} =\bm{u}^{(t)}\bm{v}^{(t)T}+R_{1}^{(t)},\] \[W_{2}^{(t)} =c^{(t)}\bm{u}^{(t)T}+R_{2}^{(t)T}.\]

where \(c^{(t)}\) is a scalar, \(\bm{u}^{(t)},\bm{v}^{(t)},R_{2}^{(t)}\in\mathbb{R}^{d}\) and \(R_{1}^{(t)}\in\mathbb{R}^{d\times d}\).Denote the \(i\)-th coordinate of \(\bm{u}^{(t)},\bm{v}^{(t)},R_{2}^{(t)}\) as \(u_{i}^{(t)},v_{i}^{(t)},R_{2i}^{(t)}\), respectively. Denote the \((i,j)\)-th element of \(R_{1}^{(t)}\) as \(R_{1}^{(t)}[i,j]\). We have that \(\forall i,j\in[d]:\quad\left|R_{2i}^{(t)}\right|\ll c^{(t)}\left|u_{i}^{(t)}\right|\) and \(\left|R_{1}^{(t)}[i,j]\right|\ll\left|u_{i}^{(t)}v_{i}^{(t)}\right|\).

Using the rank 1 structure, we can further simplify \(R_{\text{med},1}^{\text{OPT}}(t)\) and \(R_{\text{med},2}^{\text{OPT}}(t)\) by

\[R_{\text{med},k}^{\text{OPT}}(t)\approx\frac{\max_{i}\left(u_{i}^{(t)}\right) ^{2}}{\text{median}\left(u_{i}^{(t)}\right)^{2}},k=1,2.\] (4)

The final step is the detailed analysis of \(\bm{u}^{(t)}\).

For SGD+M, we can prove that \(\bm{u}^{(t)}\approx C(t)[X_{1},X_{2},...,X_{d}]^{T}\) where \(C(t)\in\mathbb{R}\) and \(X_{i},i\in[d]\) are i.i.d. Gaussian variables. Then we have with high probability, \(\frac{\max_{i}\left(u_{i}^{(t)}\right)^{2}}{\text{median}\left(u_{i}^{(t)} \right)^{2}}=\Omega(\log d)\).

For Adam, we can try to convert it to signed descent (_e.g._ in eq. (29) and Lemma 24). The main intuition is that as long as the learning rate is small enough, the movement of the gradient will be slow and Adam will be similar to signed descent. Since the update of signed descent per step is \(\pm\eta\), we can prove that \(\forall i\in[d]:u_{i}^{(t)}\in\{\pm 1\}\), which gives us \(\frac{\max_{i}\left(u_{i}^{(t)}\right)^{2}}{\text{median}\left(u_{i}^{(t)} \right)^{2}}=1\). Substituting into eq. (4) completes the proof.

Finally, we want to emphasize that the assumption \(\beta_{2}=\beta_{1}^{2}\) is not essential. As discussed above, as long as the learning rate is sufficiently small, Adam can be approximated to signed descent regardless of the ratio between \(\beta_{1},\beta_{2}\). Hence our result can generalize to cases with other \(\beta_{1},\beta_{2}\), for example, \(\beta_{2}>\beta_{1}^{2}\) where Zhang et al. [ZCS\({}^{+}\)22] proved the convergence of Adam to stationary points.

## Appendix D Analysis of SGD+M

Note that \(A=\frac{1}{m}YX^{T}\), \(\Lambda_{xx}:=\frac{1}{m}XX^{T}\). Denote \(g_{k}^{(t)}:=\nabla_{W_{k}}L(W^{(t)}),k=1,2\). We have that

\[g_{1}^{(t)}=W_{2}^{(t)T}\left(W_{2}^{(t)}W_{1}^{(t)}-A\right),\quad g_{2}^{(t )}=\left(W_{2}^{(t)}W_{1}^{(t)}-A\right)W_{1}^{(t)T}.\]

Let \(\tilde{A}^{(t)}\), \(\tilde{\Lambda}_{xx}^{(t)}\) and \(\tilde{g}_{k}^{(t)},k=1,2\) be the corresponding batch versions at time \(t\). Let \(E^{(t)}:=W_{2}^{(t)}W_{1}^{(t)}-A\), and use \(E_{i}^{(t)}\), \(A_{i}\) and \(\left(W_{2}^{(t)}W_{1}^{(t)}\right)_{i}\) to represent the \(i\)-th coordinates of \(E^{(t)}\), \(A\) and \(W_{2}^{(t)}W_{1}^{(t)}\), respectively. By eq. (2), the update rules of \(W_{1}\) and \(W_{2}\) for SGD+M are given by:

\[W_{1}^{(t+1)} =W_{1}^{(t)}-\eta\sum_{\tau=0}^{t}\beta^{t-\tau}W_{2}^{(\tau)T} \left(W_{2}^{(\tau)}W_{1}^{(\tau)}-A\right)-\eta\sum_{\tau=0}^{t}\beta^{t-\tau }Dg_{1}^{(\tau)},\] \[W_{2}^{(t+1)} =W_{2}^{(t)}-\eta\sum_{\tau=0}^{t}\beta^{t-\tau}\left(W_{2}^{( \tau)}W_{1}^{(\tau)}-A\right)W_{1}^{(\tau)T}-\eta\sum_{\tau=0}^{t}\beta^{t-\tau }Dg_{2}^{(\tau)},\]

where

\[Dg_{1}^{(t)} :=\tilde{g}_{1}^{(t)}-g_{1}^{(t)}=W_{2}^{(t)T}\left(W_{2}^{(t)}W_{ 1}^{(t)}\left(\tilde{\Lambda}_{xx}^{(t)}-\Lambda_{xx}\right)-\left(\tilde{A}^{( t)}-A\right)\right),\] \[Dg_{2}^{(t)} :=\tilde{g}_{2}^{(t)}-g_{2}^{(t)}=\left(W_{2}^{(t)}W_{1}^{(t)} \left(\tilde{\Lambda}_{xx}^{(t)}-\Lambda_{xx}\right)-\left(\tilde{A}^{(t)}-A \right)\right)W_{1}^{(t)T}.\]

Based on the magnitude of \(W_{2}\) and \(W_{1}\), we can intuitively divide the training procedure into 2 phases.

1. **First phase**: the first several iterations when \(W_{1}\) and \(W_{2}\) are "small" so that \(W_{2}W_{1}-A\approx-A\).
2. **Second phase**: later iterations when \(W_{2}W_{1}\) cannot be ignored.

More formally, the boundary between the first and second phase is defined below.

**Definition 1** (End of the first phase).: _The end of the first phase (denoted as \(T_{1}\)) is defined as \(T_{1}:=\inf\Big{\{}t\geq 0:\exists i,j\in[d]:\Big{|}w_{2i}^{(t)}\Big{|}\geq \frac{1}{d^{\frac{3}{2}}}\text{or }\Big{|}W_{1}^{(t)}[i,j]\Big{|}\geq\frac{1}{d^{\frac{ 3}{2}}}\Big{\}}\)._

By Assumption 2 and the assumption that \(\forall j\in[d]:A_{j}>0,A_{j}=\Theta(1)\), at the beginning, w.h.p., \(\forall j\in[d]:(W_{2}W_{1})_{j}-A_{j}<0\). During the training, each \((W_{2}W_{1})_{j}\) increases and approaches \(A_{j}\). We hope that by choosing a small learning rate, when \((W_{2}W_{1})_{j}\) overshoots for some coordinate \(j\), i.e. \((W_{2}W_{1})_{j}>A_{j}\), it will be close to convergence. To analyze this overshooting issue more carefully, let's first define the following "almost overshooting time".

**Definition 2** (Almost overshooting time).: _For \(\epsilon>0\), denote \(\epsilon_{0}:=\frac{1}{d^{\frac{1}{4}\alpha-1}}+\epsilon\log\sqrt{\frac{d}{ \epsilon}}\). Define_

\[T_{2}:=\inf\Big{\{}t\geq 0:\exists j\in[d]:\Big{(}W_{2}^{(t)}W_{1}^{(t)} \Big{)}_{j}-A_{j}\geq-\sqrt{\epsilon_{0}}\Big{\}}.\]

**Definition 3** (Convergence time).: _For \(\epsilon>0\), we define the "convergence time"_

\[T_{3}:=\inf\Big{\{}t\geq 0:\big{\|}E^{(t)}\big{\|}_{2}^{2}\leq\epsilon\Big{\}}.\]

We can first show that after the first phase, i.e. when \(t=T_{1}\), \(W_{1}\) will become an approximately rank-1 matrix, as described in the following lemma.

**Lemma 2**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}}{d^{\alpha/2+1}}\). By picking \(\eta\leq\mathcal{O}\left(\frac{1}{d^{\alpha}}\right)\), we have that when \(t=T_{1}\), \(\bar{L}\left(W^{(T_{1})}\right)=\Theta(d)\), and that_

\[W_{1}^{(T_{1})} =R_{1}^{(T_{1})}+\bm{u}^{(T_{1})}\bm{v}^{(T_{1})T},\] \[W_{2}^{(T_{1})} =R_{2}^{(T_{1})T}+c^{(T_{1})}\bm{u}^{(T_{1})T},\]

_where \(c^{(T_{1})}\in\mathbb{R}\), \(\bm{u}^{(T_{1})},\bm{v}^{(T_{1})},R_{1}^{(T_{1})}\in\mathbb{R}^{d}\) and \(R_{1}^{(T_{1})}\in\mathbb{R}^{d\times d}\). Denote the \(i\)-th coordinate of \(\bm{u}^{(T_{1})},\bm{v}^{(T_{1})},R_{2}^{(T_{1})}\) as \(u_{i}^{(T_{1})},v_{i}^{(T_{1})},R_{2i}^{(T_{1})}\), respectively, and the \((i,j)\)-th element of \(R_{1}^{(T_{1})}\) as \(R_{1}^{(T_{1})}[i,j]\). Then w.h.p.,_

\[\forall 1\leq i,j\leq d:\quad\frac{\Big{|}R_{1}^{(T_{1})}[i,j]\Big{|}}{u_{i}^{(T_ {1})}v_{j}^{(T_{1})}\Big{|}}\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{1}{4 }\alpha-1}}\right),\quad\frac{\Big{|}R_{2i}^{(T_{1})}\Big{|}}{\Big{|}c^{(T_{1}) }u_{i}^{(T_{1})}\Big{|}}\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{1}{4} \alpha-1}}\right).\]

The following lemma tells us that this approximate rank-1 structure is preserved when \(T_{1}\leq t\leq\min\{T_{2},T_{3}\}\).

**Lemma 3**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}}{d^{\alpha/2+1}}\). By picking \(\eta\leq\mathcal{O}\left(\frac{\epsilon}{d^{\frac{\epsilon}{d}}+4}\right)\), we have that w.h.p. for \(T_{1}\leq t\leq\min\{T_{2},T_{3}\}\),_

\[W_{1}^{(t)}=\bm{u}^{(T_{1})}\bm{v}^{(t)T}+R_{1}^{(t)},\] \[W_{2}^{(t)}=c^{(t)}\bm{u}^{(T_{1})T}+R_{2}^{(t)T}.\]

_where_

\[\forall 1\leq i,j\leq d:\quad\frac{\Big{|}R_{1}^{(t)}[i,j]\Big{|}}{u_{i}^{(T_{1}) }v_{j}^{(t)}\Big{|}}\leq\tilde{\mathcal{O}}(\epsilon_{0}),\quad\frac{\Big{|}R_ {2i}^{(t)}\Big{|}}{\Big{|}c^{(t)}u_{i}^{(T_{1})}\Big{|}}\leq\tilde{\mathcal{O} }(\epsilon_{0}),\]

_and \(\epsilon_{0}\) is defined in Definition 2. Moreover, when \(t=\min\{T_{2},T_{3}\}\), \(\bar{L}\left(W^{(t)}\right)=\mathcal{O}(\epsilon_{0}d)\)._

The following lemma gives us a more detailed description of \(\bm{u}^{(T_{1})}\).

**Lemma 4**.: _The \(\bm{u}^{(T_{1})}\) in Lemma 2 and 3 can be written as \(\bm{u}^{(T_{1})}=X+Y\) where \(X_{i},i\in[d]\) are i.i.d Gaussian random variables and that w.h.p. \(\forall i\in[d]:\frac{|Y_{i}|}{|X_{i}|}\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{ \frac{1}{4}\alpha-\frac{1}{2}}}\right)\)._

Now we are ready to prove the SGD+M part of Theorem 1.

### Proof of the SGD+M part of Theorem 1

Define \(T_{\text{SGD},1}=T_{1},T_{\text{SGD},2}=\min\{T_{2},T_{3}\}\). By picking \(\eta\leq\mathcal{O}\left(\frac{\epsilon}{d^{\frac{1}{d^{\frac{1}{d^{\frac{1}{d^{ \frac{1}{d^{\frac{1}{d^{\frac{1}{d^{1}{d}}}}}}}}}}}}}\right)\), we can apply Lemma 2 and 3 to conclude that \(\bar{L}\left(W^{(T_{\text{SGD},1})}\right)=\Theta(d)\) and \(\bar{L}\left(W^{(T_{\text{SGD},2})}\right)=\mathcal{O}(\epsilon_{0}d)\). For any \(p>0\), by picking \(0<\epsilon<\frac{1}{d^{p}}\) and \(\alpha\geq 4(p+2)\), we have \(\bar{L}\left(W^{(T_{\text{SGD},2})}\right)=\mathcal{O}(\epsilon_{0}d)\leq \tilde{\mathcal{O}}\left(\frac{1}{d^{p}}\right)\).

Moreover, when \(t\in[T_{\text{SGD},1},T_{\text{SGD},2}]\), the conditions in Lemma 31 are satisfied with \(\delta=\tilde{\mathcal{O}}(\epsilon_{0})\). Then we can apply Lemma 31 and get that

\[R_{\text{med},1}^{\text{SGD}}(t),R_{\text{med},2}^{\text{SGD}}(t)\geq\left( \frac{1-\tilde{\mathcal{O}}(\epsilon_{0})}{1+\tilde{\mathcal{O}}(\epsilon_{0} )}\right)^{2}\cdot\frac{\max_{i}\left(u_{i}^{(T_{1})}\right)^{2}}{\text{ median}\left(u_{i}^{(T_{1})}\right)^{2}}.\]

By Lemma 4, \(\bm{u}^{(T_{1})}=X+Y\) where w.h.p. \(\forall i\in[d]:\frac{|Y_{i}|}{|X_{i}|}\leq\tilde{\mathcal{O}}\left(\frac{1}{d ^{\frac{1}{4}\alpha-\frac{1}{2}}}\right)\). This fact yields

\[\forall i\in[d]:\frac{\max_{i}\left(u_{i}^{(T_{1})}\right)^{2}}{\text{ median}\left(u_{i}^{(T_{1})}\right)^{2}}\geq\left(\frac{1-\tilde{\mathcal{O}} \left(\frac{1}{d^{\frac{1}{4}\alpha-\frac{1}{2}}}\right)}{1+\tilde{\mathcal{O} }\left(\frac{1}{d^{\frac{1}{4}\alpha-\frac{1}{2}}}\right)}\right)^{2}\frac{ \max_{i}X_{i}^{2}}{\text{median}\,X_{i}^{2}}.\]

Here \(X_{i},i\in[d]\) are i.i.d Gaussian random variables by Lemma 4. To prove the concentration of median \(X_{i}^{2}\), we borrow the Proposition 12 in Chapter 2.3 of [10]. By setting \(K=N=d\) in this proposition, we have

\[\mathbb{P}\left(\left|\text{median }X_{i}^{2}-\mathbb{E}[X_{1}^{2}]\right|>2 \sqrt{\text{Var}(X_{1}^{2})}\right)\leq e^{-\frac{d}{\delta}}.\]

Denote \(\sigma^{2}\) as the variance of \(X_{i},i\in[d]\). Then \(\mathbb{E}[X_{i}^{2}]=\sigma^{2}\) and \(\text{Var}(X_{i}^{2})=2\sigma^{4}\). Hence

\[\mathbb{P}\left(\left|\text{median }X_{i}^{2}-\sigma^{2}\right|>2\sqrt{2}\sigma^{2} \right)\leq e^{-\frac{d}{\delta}}.\]

That means with high probability, median \(X_{i}^{2}\leq C\sigma^{2}\) for some \(C>0\). By Lemma 35 in Appendix H, we know that w.h.p.

\[\max_{1\leq i\leq d}X_{i}^{2}=\sigma^{2}\Omega(\log d),\]

which gives us w.h.p.

\[\frac{\max_{1\leq i\leq d}X_{i}^{2}}{\text{median }X_{i}^{2}}=\Omega(\log d).\]

Hence we have proved that \(R_{\text{med},1}^{\text{SGD}}(t),R_{\text{med},2}^{\text{SGD}}(t)\geq\Omega (\log d)\).

### Proof of Lemma 2

In the first phase, \(W_{2}W_{1}\) is "small", and we write the update equations in the following way

\[\begin{split} W_{1}^{(t+1)}&=W_{1}^{(t)}-\eta\sum_{ \tau=0}^{t}\beta^{t-\tau}W_{2}^{(\tau)T}\left(W_{2}^{(\tau)}W_{1}^{(\tau)}-A \right)-\eta\sum_{\tau=0}^{t}\beta^{t-\tau}Dg_{1}^{(\tau)}\\ &=W_{1}^{(t)}+\eta\sum_{\tau=0}^{t}\beta^{t-\tau}W_{2}^{(\tau)T}A- \eta\sum_{\tau=0}^{t}\beta^{t-\tau}W_{2}^{(\tau)T}W_{2}^{(\tau)}W_{1}^{(\tau)} -\eta\sum_{\tau=0}^{t}\beta^{t-\tau}Dg_{1}^{(\tau)}\\ &=W_{1}^{(t)}+\eta W_{2}^{(t)T}A\sum_{\tau=0}^{t}\beta^{t-\tau}+ \eta\sum_{\tau=0}^{t}\beta^{t-\tau}\left(W_{2}^{(\tau)T}-W_{2}^{(t)T}\right)A \\ &\quad-\eta\sum_{\tau=0}^{t}\beta^{t-\tau}W_{2}^{(\tau)T}W_{2}^{( \tau)}W_{1}^{(\tau)}-\eta\sum_{\tau=0}^{t}\beta^{t-\tau}Dg_{1}^{(\tau)}\\ &=W_{1}^{(t)}+\frac{\eta}{1-\beta}W_{2}^{(t)T}A+\frac{\eta}{1-\beta }r_{1}^{(t)},\end{split}\] (5)where

\[r_{1}^{(t)} =-\beta^{t+1}{W_{2}^{(t)T}}A+(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau} \left({W_{2}^{(\tau)T}-W_{2}^{(t)T}}\right)A\] \[\quad-(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}{W_{2}^{(\tau)T}}{W _{2}^{(\tau)}}{W_{1}^{(\tau)}}-(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}Dg_{1}^ {(\tau)}.\]

Similarly, we have

\[W_{2}^{(t+1)}=W_{2}^{(t)}- \eta\sum_{\tau=0}^{t}\beta^{t-\tau}\left({W_{2}^{(\tau)}}{W_{1}^{ (\tau)}}-A\right){W_{1}^{(\tau)T}}={W_{2}^{(t)}}+\frac{\eta}{1-\beta}AW_{1}^{( t)T}+\frac{\eta}{1-\beta}r_{2}^{(t)},\] (6)

where

\[r_{2}^{(t)} =-\beta^{t+1}AW_{1}^{(t)T}+(1-\beta)\sum_{\tau=0}^{t}\beta^{t- \tau}A\left({W_{1}^{(\tau)T}}-{W_{1}^{(t)T}}\right)\] \[\quad-(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}W_{2}^{(\tau)}{W_{ 1}^{(\tau)}}{W_{1}^{(\tau)T}}-(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}Dg_{2}^ {(\tau)}.\]

The following lemma gives us an explicit formula of \(W_{2}^{(t)}\).

**Lemma 5**.: _Let \(\lambda_{1}<\lambda_{2}\) be the two roots of the quadratic equation \(x^{2}-2x+1-\frac{\eta^{2}}{(1-\beta)^{2}}\|A\|_{2}^{2}=0\). Pick \(\eta<\frac{1-\beta}{\|A\|_{2}}\), then we have that_

\[W_{2}^{(t)}=C_{1}\lambda_{1}^{t}+\left(C_{2}+r_{5}^{(t)}\right)\lambda_{2}^{t},\]

_where \(C_{1}=-\frac{W_{2}^{(1)}-\lambda_{2}W_{2}^{(0)}}{\lambda_{2}-\lambda_{1}}\), \(C_{2}=\frac{W_{2}^{(1)}-\lambda_{1}W_{2}^{(0)}}{\lambda_{2}-\lambda_{1}}\). \(r_{5}^{(t)}\) will be specified in the proof._

We can prove that in the first phase, \(r_{5}^{(t)}\) is "small". More specifically, denote its \(i\)-th coordinate as \(r_{5i}^{(t)}\), and the \(i\)-th coordinate of \(C_{2}\) as \(C_{2i}\). Then the following lemmas tell us that \(\forall i\in[d],\left|r_{5i}^{(t)}\right|\leq\mathcal{O}\left(\frac{1}{d^{p( \alpha)}}\right)\), where w.h.p. \(\mathcal{O}\left(\frac{1}{d^{p(\alpha)}}\right)\ll\min_{i\in[d]}|C_{2i}|\).

We first have the following bounds of \(\left|r_{1i}^{(t)}\right|,\left|r_{2i}^{(t)}\right|\) and \(\left|r_{5i}^{(t)}\right|\) for \(i\in[d]\).

**Lemma 6**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}}{d^{\alpha/2+1}}\) and pick \(\eta\leq\mathcal{O}\left(\frac{1}{d^{\alpha}}\right)\). We have w.h.p. for all \(t\leq T_{1}\), \(\forall i\in[d]:\left|r_{1}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}\left( \frac{1}{d^{\frac{1}{\alpha-1}}}\right),\left|r_{2i}^{(t)}\right|\leq\tilde{ \mathcal{O}}\left(\frac{1}{d^{\frac{1}{\alpha}}\alpha-2}\right)\)._

**Lemma 7**.: _Under conditions of Lemma 6, we have that w.h.p. for all \(t\leq T_{1}\), \(\forall i\in[d]:\left|r_{5i}^{(t)}\right|\leq\tilde{\mathcal{O}}\left(\frac{1} {d^{\frac{3}{\alpha-1}}}\right)\)._

Next we prove upper and lower bounds of \(|C_{1i}|\) and \(|C_{2i}|\) for \(i\in[d]\).

**Lemma 8**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}}{d^{\alpha/2+1}}\). Pick \(\eta<\frac{1-\beta}{\|A\|_{2}}\), we have that i) w.h.p., \(\forall i\in[d]:|C_{1i}|\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\alpha}} \right),|C_{2i}|\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\alpha}}\right)\); ii) \(C_{2}\) can be written as \(C_{2}:=\frac{1}{2}\left(C_{3}+C_{4}\right)\) where \(C_{3i},i\in[d]\) are i.i.d Gaussian random variables and that w.h.p. \(\forall i\in[d]:\frac{|C_{4i}|}{|C_{3i}|}\leq\tilde{\mathcal{O}}\left(\frac{1} {d^{\frac{1}{\alpha}-\frac{1}{\alpha}}}\right)\); iii) w.h.p., \(\forall i\in[d],|C_{1i}|\geq\tilde{\Omega}\left(\frac{1}{d^{\frac{1}{\alpha}}} \right),|C_{2i}|\geq\tilde{\Omega}\left(\frac{1}{d^{\frac{1}{\alpha}}}\right)\)._

Now we are ready to prove Lemma 2. Lemma 5 tells us that

\[W_{2}^{(t)}=C_{1}\lambda_{1}^{t}+\left(C_{2}+r_{5}^{(t)}\right)\lambda_{2}^{t},\]

where \(\lambda_{1}=1-\frac{\eta}{1-\beta}\|A\|_{2}\) and \(\lambda_{2}=1+\frac{\eta}{1-\beta}\|A\|_{2}\).

Under the conditions of Theorem 1 and pick \(\eta\leq\mathcal{O}\left(\frac{1}{d^{\alpha}}\right)\), by Lemma 7 and 8, we know that w.h.p. \(\forall t\leq T_{1},\forall 1\leq i\leq d\),

\[\left|r_{5i}^{(t)}\right|\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3}{2} \alpha-1}}\right),\quad|C_{2i}|\geq\tilde{\Omega}\left(\frac{1}{d^{\frac{5}{ 4}\alpha}}\right),\quad|C_{2i}|\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{ \alpha}}\right),\quad\frac{\left|r_{5i}^{(t)}\right|}{|C_{2i}|}\leq\tilde{ \mathcal{O}}\left(\frac{1}{d^{\frac{1}{4}\alpha-1}}\right).\] (7)

We first prove that \(\left|w_{2i}^{(t)}\right|\) reaches \(\frac{1}{d^{\alpha/2}}\) for some coordinate \(i\) before \(\left|W_{1}^{(t)}[k,j]\right|\) for \(\forall k,j\in[d]\). To see this, first note that

\[W_{1}^{(t)} =W_{1}^{(t-1)}+\frac{\eta}{1-\beta}W_{2}^{(t-1)T}A+\frac{\eta}{1 -\beta}r_{1}^{(t-1)}\] \[=W_{1}^{(0)}+\frac{\eta}{1-\beta}\sum_{\tau=0}^{t-1}W_{2}^{(\tau )T}A+\frac{\eta}{1-\beta}\sum_{\tau=0}^{t-1}r_{1}^{(\tau)}\] \[=W_{1}^{(0)}+\frac{\eta}{1-\beta}\left(C_{1}\sum_{\tau=0}^{t-1} \lambda_{1}^{\tau}+C_{2}\sum_{\tau=0}^{t-1}\lambda_{2}^{\tau}+\sum_{\tau=0}^{t -1}\lambda_{2}^{\tau}r_{5}^{(\tau)}\right)^{T}A+\frac{\eta}{1-\beta}\sum_{ \tau=0}^{t-1}r_{1}^{(\tau)}\] \[=W_{1}^{(0)}+\frac{\eta}{1-\beta}\sum_{\tau=0}^{t-1}r_{1}^{(\tau )}+\frac{\eta}{1-\beta}\left(C_{1}\sum_{\tau=0}^{t-1}\lambda_{1}^{\tau}+\sum_{ \tau=0}^{t-1}\lambda_{2}^{\tau}r_{5}^{(\tau)}\right)^{T}A+\frac{\eta}{1-\beta }\sum_{\tau=0}^{t-1}\lambda_{2}^{\tau}C_{2}^{T}A\] \[:=W_{1}^{(0)}+\frac{\eta}{1-\beta}\sum_{\tau=0}^{t-1}r_{1}^{( \tau)}+\left(C_{1}\sum_{\tau=0}^{t-1}\lambda_{1}^{\tau}+\sum_{\tau=0}^{t-1} \lambda_{2}^{\tau}r_{5}^{(\tau)}\right)^{T}\bm{v}^{(t)T}+\bm{u}^{(t)}\bm{v}^{( t)T},\]

where \(\bm{v}^{(t)T}=\frac{\eta}{1-\beta}A\) and

\[\bm{u}^{(t)}=\sum_{\tau=0}^{t-1}\lambda_{2}^{\tau}C_{2}^{T}.\] (8)

Moreover, we have that

\[W_{2}^{(t)}=C_{1}\lambda_{1}^{t}+\left(C_{2}+r_{5}^{(t)}\right)\lambda_{2}^{t }:=C_{1}\lambda_{1}^{t}+r_{5}^{(t)}\lambda_{2}^{t}+c^{(t)}\bm{u}^{(t)T},\quad c ^{(t)}=\frac{\lambda_{2}^{t}}{\sum_{\tau=0}^{t-1}\lambda_{2}^{\tau}}.\]

For \(t\leq T_{1}\), by eq. (7), we get that w.h.p.,

\[\forall 1\leq i,j\leq d:\quad\frac{\left|\sum_{\tau=0}^{t-1} \lambda_{2}^{\tau}r_{5i}^{(\tau)}v_{j}^{(t)}\right|}{\left|u_{i}^{(t)}v_{j}^{( t)}\right|} \leq\frac{\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3}{2}\alpha-1}} \right)\sum_{\tau=0}^{t-1}\lambda_{2}^{\tau}}{\tilde{\Omega}\left(\frac{1}{d^ {\frac{3}{2}\alpha-1}}\right)\sum_{\tau=0}^{t-1}\lambda_{2}^{\tau}}\leq \tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{1}{4}\alpha-1}}\right),\] \[\frac{\left|\lambda_{2}^{t}r_{5i}^{(t)}\right|}{\left|c^{(t)}u_{i }^{(t)}\right|} =\frac{\left|r_{5i}^{(t)}\right|}{\left|C_{2i}\right|}\leq\tilde{ \mathcal{O}}\left(\frac{1}{d^{\frac{1}{4}\alpha-1}}\right).\]

For \(t\leq T_{1}\), by Lemma 6, \(\forall 1\leq i,j\leq d:\left|r_{1}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}} \left(\frac{1}{d^{\frac{3}{2}\alpha-1}}\right)\). Then we have that w.h.p.

\[\frac{\left|\frac{\eta}{1-\beta}\sum_{\tau=0}^{t-1}r_{1}^{(\tau)}[ i,j]\right|}{\left|u_{i}^{(t)}v_{j}^{(t)}\right|} =\frac{\left|\sum_{\tau=0}^{t-1}r_{1}^{(\tau)}[i,j]\right|}{\left| \sum_{\tau=0}^{t-1}\lambda_{2}^{\tau}C_{2i}A_{j}\right|}\leq\frac{\left|\sum_{ \tau=0}^{t-1}r_{1}^{(\tau)}[i,j]\right|}{\left|\sum_{\tau=0}^{t-1}C_{2i}A_{j} \right|}\leq\frac{\sum_{\tau=0}^{t-1}\tilde{\mathcal{O}}\left(\frac{1}{d^{ \frac{3}{2}\alpha-1}}\right)}{\sum_{\tau=0}^{t-1}\tilde{\Omega}\left(\frac{1}{ d^{\frac{3}{4}\alpha}}\right)}\] \[=\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{1}{4}\alpha-1}}\right).\]

Here we used \(\forall i\in[d]:A_{i}=\Theta(1)\) by Assumption 1.

Since \(\lambda_{1}=1-\frac{\eta}{1-\beta}\|A\|_{2}\), we have that \(|C_{1i}\lambda_{1}^{t}|\leq|C_{1i}|\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{ \alpha}}\right)\) and that

\[\left|C_{1i}\sum_{\tau=0}^{t-1}\lambda_{1}^{\tau}v_{j}^{(t)}\right|=\frac{\eta A _{j}}{1-\beta}\left|C_{1i}\sum_{\tau=0}^{t-1}\lambda_{1}^{\tau}\right|\leq\frac{ \eta A_{j}|C_{1i}|}{(1-\beta)(1-\lambda_{1})}\leq\frac{A_{j}|C_{1i}|}{\|A\|_{2}} \leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\alpha+\frac{1}{2}}}\right).\]Using the Gaussian tail bound and union bound, we have w.h.p. \(\forall 1\leq i,j\leq d:\left|W_{1}^{(0)}[i,j]\right|=\tilde{\mathcal{O}}\left( \frac{1}{d^{\alpha}}\right)\). Combining the above bounds together yields that for \(t\leq T_{1}\) and \(\forall i,j\in[d]\),

\[W_{1}^{(t)}[i,j] =R_{11}^{(t)}[i,j]+u_{i}^{(t)}v_{j}^{(t)}(1+e_{1}^{(t)}[i,j]),\] (9) \[w_{2i}^{(t)} =R_{21,i}^{(t)}+c^{(t)}u_{i}^{(t)}(1+e_{2i}^{(t)}).\]

where for \(\forall i,j\in[d]\). \(\left|R_{11}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\alpha +\frac{1}{2}}}\right)\), \(\left|R_{21,i}^{(t)}\right|\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\alpha}}\right)\) and \(\left|e_{1}^{(t)}[i,j]\right|,\left|e_{2i}^{(t)}\right|\leq\tilde{\mathcal{O} }\left(\frac{1}{d^{\frac{1}{4}\,\alpha-1}}\right)\).

Further we notice that for \(t\leq T_{1}\), we have \(\forall j\in[d]\),

\[\frac{\left|v_{j}^{(t)}\right|}{\left|c^{(t)}\right|}=\frac{\eta A_{j}}{1- \beta}\cdot\frac{\sum_{\tau=0}^{t-1}\lambda_{\tau}^{\tau}}{\lambda_{2}^{6}}= \frac{\eta A_{j}}{1-\beta}\frac{\lambda_{\tau}^{t}-1}{\lambda_{2}^{t}(\lambda _{2}-1)}=\frac{A_{j}\left(\lambda_{2}^{t}-1\right)}{\lambda_{2}^{t}\|A\|_{2}} \leq\frac{A_{j}}{\|A\|_{2}}=\mathcal{O}\left(\frac{1}{\sqrt{d}}\right).\]

which yields that \(\left|u_{i}^{(t)}v_{j}^{(t)}\right|\leq\mathcal{O}\left(\frac{1}{\sqrt{d}} \right)\left|c^{(t)}u_{i}^{(t)}\right|\). Together with eq. (9) gives us that \(\left|w_{2i}^{(t)}\right|\) reaches \(\frac{1}{d^{\alpha/2}}\) for some \(i\in[d]\) before \(\left|W_{1}^{(t)}[k,j]\right|\) for \(\forall k,j\in[d]\), i.e. \(T_{1}=\inf\left\{t\geq 0:\exists i\in[d]:\left|w_{2i}^{(t)}\right|\geq\frac{1}{d^{ \frac{1}{2}}}\right\}\).

Further, we know that at time \(T_{1}\), \(\left|c^{(T_{1})}u_{i_{0}}^{(T_{1})}\right|=|C_{2i_{0}}|\lambda_{2}^{T_{1}}= \Theta\left(\frac{1}{d^{\alpha/2}}\right)\) for some \(i_{0}\in[d]\), which means w.h.p.

\[\begin{array}{ll}\frac{\Theta\left(\frac{1}{d^{\frac{3}{2}}} \right)}{\tilde{\mathcal{O}}\left(\frac{1}{d^{\alpha}}\right)}\leq\lambda_{2}^ {T_{1}}\leq\frac{\Theta\left(\frac{1}{d^{\frac{3}{2}}}\right)}{\tilde{\Omega} \left(\frac{1}{d^{\frac{3}{2}}}\right)},&\Rightarrow\quad\tilde{\Omega}\left( d^{\frac{\alpha}{2}}\right)\leq\lambda_{2}^{T_{1}}=\left(1+\frac{\eta}{1-\beta}\|A\|_{2} \right)^{T_{1}}\leq\tilde{\mathcal{O}}\left(d^{\frac{3}{\alpha}}\right),\\ &\Rightarrow\quad T_{1}=\Theta\left(\frac{\log d}{\eta\|A\|_{2}}\right). \end{array}\] (10)

This is the length of the first phase. As for \(c^{(T_{1})}u_{i}^{(T_{1})}\) and \(u_{i}^{(T_{1})}v_{j}^{(T_{1})}\) for other coordinates, we have that w.h.p. \(\forall 1\leq i,j\leq d\),

\[\begin{array}{l}\left|u_{i}^{(T_{1})}v_{j}^{(T_{1})}\right| =\frac{\eta}{1-\beta}\sum_{\tau=0}^{T_{1}-1}\lambda_{\tau}^{\tau} \left|C_{2i}A_{j}\right|=\frac{\eta}{1-\beta}\cdot\frac{\lambda_{2}^{T_{1}}-1}{ \lambda_{2}-1}\left|C_{2i}A_{j}\right|\stackrel{{(\mathrm{i})}}{{= }}\frac{\lambda_{2}^{T_{1}}-1}{\|A\|_{2}}\left|C_{2i}A_{j}\right|\\ \geq\frac{\tilde{\Omega}\left(d^{\alpha/2}\right)}{\Theta\left( \sqrt{d}\right)}\tilde{\Omega}\left(\frac{1}{d^{\frac{3}{4}\alpha}}\right)= \tilde{\Omega}\left(\frac{1}{d^{\frac{3}{4}\alpha+\frac{1}{4}}}\right),\\ \left|c^{(T_{1})}u_{i}^{(T_{1})}\right|=|C_{2i}|\lambda_{2}^{T_{1}}\geq \tilde{\Omega}\left(d^{\alpha/2}\right)\tilde{\Omega}\left(\frac{1}{d^{\frac{ 5}{4}\alpha}}\right)=\tilde{\Omega}\left(\frac{1}{d^{\frac{3}{4}\alpha}} \right).\end{array}\]

Here in \((i)\) we used \(\lambda_{2}=1+\frac{\eta}{1-\beta}\|A\|_{2}\). Then we have at time \(T_{1}\), \(\forall i,j\in[d]\), \(\frac{\left|R_{11}^{(T_{1})}[i,j]\right|}{\left|u_{i}^{(T_{1})}v_{j}^{(T_{1})} \right|}\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3}{4}\,\alpha}}\right)\) and that \(\frac{\left|R_{21,i}^{(T_{1})}\right|}{\left|c^{(T_{1})}u_{i}^{(T_{1})}\right|} \leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{5}{4}\alpha}}\right)\). Together with eq. (9), we have the following weight structure:

\[\begin{array}{l}W_{1}^{(T_{1})}=R_{1}^{(T_{1})}+\bm{u}^{(T_{1})}\bm{v}^{(T_{ 1})T},\\ W_{2}^{(T_{1})}=R_{2}^{(T_{1})T}+c^{(T_{1})}\bm{u}^{(T_{1})T},\end{array}\]

where w.h.p.,

\[\forall 1\leq i,j\leq d:\quad\frac{\left|R_{1}^{(T_{1})}[i,j]\right|}{\left|u_{i}^{ (T_{1})}v_{j}^{(T_{1})}\right|}\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{ \frac{1}{4}\alpha-1}}\right),\quad\frac{\left|R_{2i}^{(T_{1})}\right|}{\left|c^{ (T_{1})}u_{i}^{(T_{1})}\right|}\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{ \frac{1}{4}\alpha-1}}\right).\]

Finally, we consider the loss. Since \(\forall j\in[d]:\left(W_{2}^{(T_{1})}W_{1}^{(T_{1})}\right)_{j}-A_{j}=-\Theta(1)\), we know that \(\bar{L}\left(W^{(T_{1})}\right)=\Theta(d)\).

### Proof of Lemma 4

Eq. (8) tells us that \(\bm{u}^{(T_{1})}=\sum_{\tau=0}^{T_{1}-1}\lambda_{2}^{\tau}C_{2}^{T}\). Lemma 8 tells us that \(C_{2}\) can be written as \(C_{2}:=\frac{1}{2}\left(C_{3}+C_{4}\right)\) where \(C_{3i},i\in[d]\) are i.i.d Gaussian random variables and that w.h.p. \(\forall i\in[d]:\frac{|C_{4i}|}{|C_{3i}|}\leq\tilde{\mathcal{O}}\left(\frac{1} {d^{\frac{1}{4}\alpha-\frac{1}{2}}}\right)\). Combining these two facts together finishes the proof.

### Proof of Lemma 5

Replacing \(t\) by \(t-1\) in eq. (6), we get

\[W_{2}^{(t)}=W_{2}^{(t-1)}+\frac{\eta}{1-\beta}AW_{1}^{(t-1)T}+\frac{\eta}{1- \beta}r_{2}^{(t-1)}.\] (11)

Eq. (6)-(11) and substituting eq. (5) yield

\[W_{2}^{(t+1)}-W_{2}^{(t)} =W_{2}^{(t)}-W_{2}^{(t-1)}+\frac{\eta^{2}}{(1-\beta)^{2}}\|A\|_{2 }^{2}W_{2}^{(t-1)}+\frac{\eta^{2}}{(1-\beta)^{2}}Ar_{1}^{(t-1)T}\] \[\quad\quad+\frac{\eta}{1-\beta}\left(r_{2}^{(t)}-r_{2}^{(t-1)} \right),\] \[\Rightarrow\quad W_{2}^{(t+1)} =2W_{2}^{(t)}-\left(1-\frac{\eta^{2}}{(1-\beta)^{2}}\|A\|_{2}^{2} \right)W_{2}^{(t-1)}+r_{3}^{(t)},\]

where \(r_{3}(t):=\frac{\eta^{2}}{(1-\beta)^{2}}Ar_{1}^{(t-1)T}+\frac{\eta}{1-\beta} \left(r_{2}^{(t)}-r_{2}^{(t-1)}\right)\).

For the equation \(x^{2}-2x+1-\frac{\eta^{2}}{(1-\beta)^{2}}\|A\|_{2}^{2}=0\), the roots are \(\lambda_{1}=1-\frac{\eta}{1-\beta}\|A\|_{2}\) and \(\lambda_{2}=1+\frac{\eta}{1-\beta}\|A\|_{2}\). We have that

\[W_{2}^{(t+1)}-\lambda_{2}W_{2}^{(t)} =\lambda_{1}\left(W_{2}^{(t)}-\lambda_{2}W_{2}^{(t-1)}\right)+r_ {3}^{(t)}\] \[\Rightarrow\quad W_{2}^{(t)}-\lambda_{2}W_{2}^{(t-1)} =\lambda_{1}^{t-1}\left(W_{2}^{(1)}-\lambda_{2}W_{2}^{(0)}\right) +\sum_{\tau=1}^{t-1}\lambda_{1}^{t-1-\tau}r_{3}^{(\tau)}\] \[:=\lambda_{1}^{t-1}\left(W_{2}^{(1)}-\lambda_{2}W_{2}^{(0)}\right) +r_{4}^{(t)}.\]

We further have

\[W_{2}^{(t)} =\lambda_{2}^{t}W_{2}^{(0)}+\sum_{\tau=0}^{t-1}\lambda_{2}^{t-1- \tau}\lambda_{1}^{\tau}\left(W_{2}^{(1)}-\lambda_{2}W_{2}^{(0)}\right)+\sum_{ \tau=1}^{t}\lambda_{2}^{t-\tau}r_{4}^{(\tau)}\] \[=\lambda_{2}^{t}W_{2}^{(0)}+\frac{\lambda_{2}^{t}-\lambda_{1}^{t} }{\lambda_{2}-\lambda_{1}}\left(W_{2}^{(1)}-\lambda_{2}W_{2}^{(0)}\right)+\sum_ {\tau=1}^{t}\lambda_{2}^{t-\tau}r_{4}^{(\tau)}\] \[=C_{1}\lambda_{1}^{t}+C_{2}\lambda_{2}^{t}+\sum_{\tau=1}^{t} \lambda_{2}^{t-\tau}r_{4}^{(\tau)}\] \[=C_{1}\lambda_{1}^{t}+\left(C_{2}+r_{5}^{(t)}\right)\lambda_{2}^{t},\]

where \(r_{5}^{(t)}=\sum_{\tau=1}^{t}\lambda_{2}^{-\tau}r_{4}^{(\tau)}\), \(C_{1}=-\frac{W_{2}^{(1)}-\lambda_{2}W_{2}^{(0)}}{\lambda_{2}-\lambda_{1}}\) and \(C_{2}=\frac{W_{2}^{(1)}-\lambda_{1}W_{2}^{(0)}}{\lambda_{2}-\lambda_{1}}\).

### Proof of Lemma 6

Write \(r_{1}^{(t)}=-\beta^{t+1}W_{2}^{(t)T}A+q_{12}^{(t)}+q_{13}^{(t)}+q_{14}^{(t)}\) where \(q_{12}^{(t)}=(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}\left(W_{2}^{(\tau)T}-W_{2 }^{(t)T}\right)A\), \(q_{13}^{(t)}=-(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}W_{2}^{(\tau)T}W_{2}^{( \tau)}W_{1}^{(\tau)}\) and \(q_{14}^{(t)}=-(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}Dg_{1}^{(\tau)}\). And write \(r_{2}^{(t)}=-\beta^{t+1}AW_{1}^{(t)T}+q_{22}^{(t)}+q_{23}^{(t)}+q_{24}^{(t)}\), where \(q_{22}^{(t)}=(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}A\left(W_{1}^{(\tau)T}-W_ {1}^{(t)T}\right)\), \(q_{23}^{(t)}=-(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}W_{2}^{(\tau)}W_{1}^{( \tau)T}\) and \(q_{24}^{(t)}=-(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}Dg_{2}^{(\tau)}\).

Let's first try to bound \(\left|q_{12}^{(t)}[i,j]\right|\) and \(\left|q_{22,i}^{(t)}\right|\). For any \(\tau\leq T_{1}\), we have that

\[\forall i\in[d]:\quad\left|\left(W_{2}^{(\tau)}W_{1}^{(\tau)}\right)_{i}\right| =\left|\sum_{j=1}^{d}w_{2j}^{(\tau)}W_{1}^{(\tau)}[j,i]\right|\leq\sum_{j=1}^ {d}\left|w_{2j}^{(\tau)}\right|\left|W_{1}^{(\tau)}[j,i]\right|\leq\sum_{i=1}^ {d}\frac{1}{d^{\alpha}}=\frac{1}{d^{\alpha-1}},\]

and thus \(\forall i\in[d]:\left|E_{i}^{(\tau)}\right|=\mathcal{O}(1)\). Then we have for all \(i,j\in[d]\),

\[\left|W_{1}^{(\tau+1)}[i,j]-W_{1}^{(\tau)}[i,j]\right| \leq\eta\sum_{k=0}^{\tau}\beta^{\tau-k}\left|w_{2i}^{(k)}E_{j}^{ (k)}\right|\leq\eta\sum_{k=0}^{\tau}\beta^{\tau-k}\mathcal{O}\left(\frac{1}{d ^{\alpha/2}}\right)\] \[=\eta\mathcal{O}\left(\frac{1}{d^{\alpha/2}}\right),\] \[\left|w_{2i}^{(\tau+1)}-w_{2i}^{(\tau)}\right| \leq\eta\sum_{k=0}^{\tau}\beta^{\tau-k}\sum_{j=1}^{d}\left|E_{j} ^{(k)}W_{1}^{(k)}[i,j]\right|\leq\eta\sum_{k=0}^{\tau}\beta^{\tau-k}\mathcal{O }\left(\frac{1}{d^{\alpha/2-1}}\right)\] \[=\eta\mathcal{O}\left(\frac{1}{d^{\alpha/2-1}}\right).\]

That gives us \(\forall i,j\in[d]\),

\[\left|q_{12}^{(t)}[i,j]\right| \leq(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}\left|\left(w_{2i}^{ (\tau)}-w_{2i}^{(t)}\right)A_{j}\right|\leq\eta(1-\beta)\sum_{\tau=0}^{t} \mathcal{O}\left(\frac{\beta^{t-\tau}(t-\tau)}{d^{\alpha/2-1}}\right)\] \[=\mathcal{O}\left(\frac{\eta}{d^{\alpha/2-1}}\right),\] \[\left|q_{22,i}^{(t)}\right| \leq(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}\sum_{j=1}^{d}\left| A_{j}\left(W_{1}^{(\tau)}[i,j]-W_{1}^{(t)}[i,j]\right)\right|\] \[\leq\eta(1-\beta)\sum_{\tau=0}^{t}\mathcal{O}\left(\frac{\beta^ {t-\tau}(t-\tau)}{d^{\alpha/2-1}}\right)=\mathcal{O}\left(\frac{\eta}{d^{ \alpha/2-1}}\right).\]

Then we bound \(\left|q_{13}^{(t)}[i,j]\right|\) and \(\left|q_{23,i}^{(t)}\right|\). We have for \(\forall i,j\in[d]\),

\[\left|q_{13}^{(t)}[i,j]\right| \leq(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}\left|w_{2i}^{(\tau) }\left(W_{2}^{(\tau)}W_{1}^{(\tau)}\right)_{j}\right|\leq(1-\beta)\sum_{\tau=0 }^{t}\beta^{t-\tau}\frac{1}{d^{\frac{\alpha}{2}}}\cdot\frac{1}{d^{\alpha-1}}\] \[=\mathcal{O}\left(\frac{1}{d^{\frac{\alpha}{2}\alpha-1}}\right),\] \[\left|q_{23,i}^{(t)}\right| \leq(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}\sum_{j=1}^{d}\left| \left(W_{2}^{(t)}W_{1}^{(t)}\right)_{j}W_{1}^{(t)}[i,j]\right|\] \[\leq(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}\sum_{i=1}^{d}\frac{1 }{d^{\alpha-1+\frac{\alpha}{2}}}=\mathcal{O}\left(\frac{1}{d^{\frac{\alpha}{2 }\alpha-2}}\right).\]

Finally we use Lemma 32 to bound \(\left|q_{14}^{(t)}[i,j]\right|\) and \(\left|q_{24,i}^{(t)}\right|\). For \(t\leq T_{1}\), the \(M_{1}^{(t)},M_{2}^{(t)}\) in Lemma 32 are upper bounded by \(\frac{1}{d^{\frac{\alpha}{2}}}\). In the theorem we consider the training period before \(T_{\text{SGD},2}\) so the time \(T\) in Lemma 32 is set as \(T_{\text{SGD},2}\). In the following sections, we will prove that \(T_{\text{SGD},2}\leq\mathcal{O}\left(\frac{d^{\alpha}\log(\sqrt{d/c})}{\eta}\right)\). Then by Lemma 32, we have with probability at least \(1-\frac{1}{d}\), for \(\forall t\leq T_{1}\) and \(\forall i,j\in[d]\),

\[\left|Dg_{1}^{(t)}[i,j]\right| =\left|\tilde{g}_{1}^{(t)}[i,j]-g_{1}^{(t)}[i,j]\right|\leq\mathcal{ O}\left(\frac{1}{d^{\frac{3n}{2}-3}}\sigma\sqrt{\frac{d^{\alpha+1}}{\eta}\log \frac{d}{\epsilon}}\right)+\mathcal{O}\left(\frac{1}{d^{\frac{n}{2}}}\sigma \sqrt{\frac{d^{\alpha+2}}{\eta}\log\frac{d}{\epsilon}}\right)\] \[\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{n}{2}}}\sigma\sqrt {\frac{d^{\alpha+2}}{\eta}}\right),\] \[\left|Dg_{2i}^{(t)}\right| =\left|\tilde{g}_{2i}^{(t)}-g_{2i}^{(t)}\right|\leq\mathcal{O} \left(\frac{1}{d^{\frac{3n}{2}-4}}\sigma\sqrt{\frac{d^{\alpha+1}}{\eta}\log \frac{d}{\epsilon}}\right)+\mathcal{O}\left(\frac{1}{d^{\frac{n}{2}-1}}\sigma \sqrt{\frac{d^{\alpha+2}}{\eta}\log\frac{d}{\epsilon}}\right)\] \[\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{n}{2}-1}}\sigma \sqrt{\frac{d^{\alpha+2}}{\eta}}\right).\]

By picking \(\sigma\leq\frac{\eta^{3/2}}{d^{\alpha/2+1}}\), we have w.h.p. for \(\forall t\leq T_{1}\) and \(\forall i,j\in[d]\), \(\left|Dg_{1}^{(t)}[i,j]\right|\leq\eta\tilde{\mathcal{O}}\left(\frac{1}{d^{ \frac{n}{2}}}\right)\) and \(\left|Dg_{2i}^{(t)}\right|\leq\eta\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{ n}{2}-1}}\right)\), which yields

\[\left|g_{14}^{(t)}[i,j]\right| \leq(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}\left|Dg_{1}^{(\tau) }[i,j]\right|\leq(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}\eta\tilde{\mathcal{ O}}\left(\frac{1}{d^{\frac{n}{2}}}\right)=\eta\tilde{\mathcal{O}}\left(\frac{1}{d^{ \frac{n}{2}}}\right),\] \[\left|g_{24,i}^{(t)}\right| \leq(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}\left|Dg_{2i}^{(\tau) }\right|\leq(1-\beta)\sum_{\tau=0}^{t}\beta^{t-\tau}\eta\tilde{\mathcal{O}} \left(\frac{1}{d^{\frac{n}{2}-1}}\right)=\eta\tilde{\mathcal{O}}\left(\frac{1 }{d^{\frac{n}{2}-1}}\right).\]

Combining all the above bounds and substituting \(\eta\leq\mathcal{O}\left(\frac{1}{d^{\alpha}}\right)\) gives us for \(\forall t\leq T_{1}\) and \(\forall i,j\in[d]\),

\[\left|r_{1}^{(t)}[i,j]\right|\leq\beta^{t+1}\left|w_{2i}^{(t)}A_{j}\right|+ \tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3}{2}\alpha-1}}\right),\quad\left| r_{2i}^{(t)}\right|\leq\beta^{t+1}\left|\sum_{j=1}^{d}A_{j}W_{1}^{(t)}[i,j] \right|+\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3}{2}\alpha-2}}\right).\] (12)

For \(t\leq T_{1}\), we have \(\forall i,j\in[d]\), \(\left|w_{2i}^{(t)}A_{j}\right|\leq\mathcal{O}\left(\frac{1}{d^{\alpha/2}}\right)\) and \(\left|\sum_{j=1}^{d}A_{j}W_{1}^{(t)}[i,j]\right|\leq\mathcal{O}\left(\frac{1}{ d^{\alpha/2-1}}\right)\), which gives us \(\left|r_{1}^{(t)}[i,j]\right|\leq\mathcal{O}\left(\frac{1}{d^{\alpha/2}}\right)\) and \(\left|r_{2i}^{(t)}\right|\leq\mathcal{O}\left(\frac{1}{d^{\alpha/2-1}}\right)\). Substituting into eq. (5) and eq. (6) yields that for \(t\leq T_{1}\) and \(\forall i,j\in[d]\),

\[\left|W_{1}^{(t+1)}[i,j]-W_{1}^{(t)}[i,j]\right|\leq\mathcal{O}\left(\frac{ \eta}{d^{\alpha/2}}\right),\quad\left|w_{2i}^{(t+1)}-w_{2i}^{(t)}\right|\leq \mathcal{O}\left(\frac{\eta}{d^{\alpha/2-1}}\right).\]

Hence for \(t\leq\min\left\{\frac{\alpha\log d}{\log(1/\beta)},T_{1}\right\}\), we have \(\forall i,j\in[d]\),

\[\left|W_{1}^{(t)}[i,j]\right| \leq\left|W_{1}^{(0)}[i,j]\right|+\frac{\alpha\log d}{\log(1/\beta )}\mathcal{O}\left(\frac{\eta}{d^{\alpha/2}}\right)\leq\tilde{\mathcal{O}} \left(\frac{1}{d^{\frac{3n}{2}}}\right),\] \[\left|w_{2i}^{(t)}\right| \leq\left|w_{2i}^{(0)}\right|+\frac{\alpha\log d}{\log(1/\beta)} \mathcal{O}\left(\frac{\eta}{d^{\alpha/2-1}}\right)\leq\tilde{\mathcal{O}} \left(\frac{1}{d^{\frac{3n}{2}-1}}\right).\]

Then we know that \(T_{1}>\frac{\alpha\log d}{\log(1/\beta)}\) and also get tighter bounds of \(\left|W_{1}^{(t)}[i,j]\right|,\left|w_{2i}^{(t)}\right|\) for \(t\leq\frac{\alpha\log d}{\log(1/\beta)}\). Now we use these new bounds to analyze \(\left|r_{1}^{(t)}[i,j]\right|\) and \(\left|r_{2i}^{(t)}\right|\) again.

When \(t\leq\frac{\alpha\log d}{\log(1/\beta)}\), we have for all \(i,j\in[d]\), \(\beta^{t+1}\left|w_{2i}^{(t)}A_{j}\right|\leq\left|w_{2i}^{(t)}A_{j}\right| \leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3n}{2}-1}}\right)\) and \(\beta^{t+1}\left|\sum_{j=1}^{d}A_{j}W_{1}^{(t)}[i,j]\right|\leq\left|\sum_{j=1}^ {d}A_{j}W_{1}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3n }{2}-1}}\right)\). When \(\frac{\alpha\log d}{\log(1/\beta)}<t\leq T_{1}\), we have \(\beta^{t+1}\leq\frac{1}{d^{\alpha}}\), suggesting that \(\forall i,j\in[d]\), \(\beta^{t+1}\left|w_{2i}^{(t)}A_{j}\right|\leq\frac{1}{d^{\alpha}}\tilde{\mathcal{ O}}\left(\frac{1}{d^{\frac{3n}{2}}}\right)\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{ \frac{3n}{2}}}\right)\) and

\(\beta^{t+1}\left|\sum_{j=1}^{d}A_{j}W_{1}^{(t)}[i,j]\right|\leq\frac{1}{d^{ \alpha}}\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3n}{2}-1}}\right)\leq\tilde{ \mathcal{O}}\left(\frac{1}{d^{\frac{3n}{2}-1}}\right)\). Substituting into (12) completes the proof.

### Proof of Lemma 7

Based on the bound in Lemma 6, we have

\[\left|r_{3i}^{(t)}\right| =\left|\frac{\eta^{2}}{(1-\beta)^{2}}\sum_{j=1}^{d}A_{j}r_{1}^{(t-1 )}[i,j]+\frac{\eta}{1-\beta}\left(r_{2i}^{(t)}-r_{2i}^{(t-1)}\right)\right|\] \[\leq\frac{\eta^{2}}{(1-\beta)^{2}}\sum_{j=1}^{d}\left|A_{j}r_{1}^{ (t-1)}[i,j]\right|+\frac{\eta}{1-\beta}\left|r_{2i}^{(t)}\right|+\frac{\eta}{ 1-\beta}\left|r_{2i}^{(t-1)}\right|\] \[\leq\eta^{2}\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3}{2} \alpha-2}}\right)+2\eta\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3}{2}\alpha- 2}}\right)=\eta\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3}{2}\alpha-2}} \right).\]

Since \(\lambda_{1}=1-\frac{\eta}{1-\beta}\|A\|_{2},\lambda_{2}=1+\frac{\eta}{1-\beta }\|A\|_{2}\), and note that \(\|A\|_{2}=\Theta\left(\sqrt{d}\right)\), we have that

\[\left|r_{4i}^{(t)}\right| =\left|\sum_{\tau=1}^{t-1}\lambda_{1}^{t-1-\tau}r_{3i}^{(\tau)} \right|\leq\sum_{\tau=1}^{t-1}\lambda_{1}^{t-1-\tau}\tilde{\mathcal{O}}\left( \frac{\eta}{d^{\frac{3}{2}\alpha-2}}\right)\leq\frac{\eta}{1-\lambda_{1}} \tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3}{2}\alpha-2}}\right)=\tilde{ \mathcal{O}}\left(\frac{1}{d^{\frac{3}{2}(\alpha-1)}}\right),\] \[\left|r_{5i}^{(t)}\right| =\left|\sum_{\tau=1}^{t}\lambda_{2}^{-\tau}r_{4i}^{(\tau)}\right| \leq\eta\sum_{\tau=1}^{t}\lambda_{2}^{-\tau}\tilde{\mathcal{O}}\left(\frac{1} {d^{\frac{3}{2}(\alpha-1)}}\right)\leq\frac{\eta}{\lambda_{2}-1}\tilde{ \mathcal{O}}\left(\frac{1}{d^{\frac{3}{2}(\alpha-1)}}\right)=\tilde{\mathcal{O }}\left(\frac{1}{d^{\frac{3}{2}\alpha-1}}\right).\]

### Proof of Lemma 8

For the equation \(x^{2}-2x+1-\frac{\eta^{2}}{(1-\beta)^{2}}\|A\|_{2}^{2}=0\), the roots are \(\lambda_{1}=1-\frac{\eta}{1-\beta}\|A\|_{2}\) and \(\lambda_{2}=1+\frac{\eta}{1-\beta}\|A\|_{2}\), which gives us

\[C_{2} =\frac{W_{2}^{(1)}-\lambda_{1}W_{2}^{(0)}}{\lambda_{2}-\lambda_{1 }}\] (13) \[=\frac{W_{2}^{(0)}+\eta AW_{1}^{(0)T}+\eta\tilde{r}_{2}^{(0)}-W_{ 2}^{(0)}+\frac{\eta}{1-\beta}\|A\|_{2}W_{2}^{(0)}}{\frac{2\eta}{1-\beta}\|A\| _{2}}\] \[=\frac{1}{2}W_{2}^{(0)}+\frac{1-\beta}{2\|A\|_{2}}AW_{1}^{(0)T}+ \frac{1-\beta}{2\|A\|_{2}}\tilde{r}_{2}^{(0)},\]

where \(\tilde{r}_{2}^{(0)}=-W_{2}^{(0)}W_{1}^{(0)}W_{1}^{(0)T}-Dg_{2}^{(0)}\). Note that this is slightly different from the definition of \(r_{2}^{(0)}\) in eq. (6). Now let's bound the \(i\)-th coordinate of \(\tilde{r}_{2}^{(0)}\).

In Section D.5 we have shown that w.h.p. for \(\forall t\leq T_{1}\) and \(\forall i,j\in[d]\), \(\left|Dg_{2i}^{(t)}\right|\leq\eta\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3 }{2}-1}}\right)=\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3}{2}-1}}\right)\), which also applies to \(t=0\). Using the Gaussian tail bound and union bound, w.p. at least \(1-\delta\), for ever \(1\leq i,j\leq d\), we have that

\[\left|w_{2i}^{(0)}\right|\leq\sqrt{\frac{2}{d^{2\alpha}}\log\frac{2d}{\delta}}, \quad\left|W_{1}^{(0)}[i,j]\right|\leq\sqrt{\frac{2}{d^{4\alpha}}\log\frac{2d ^{2}}{\delta}}.\]

Then we have that w.p. at least \(1-\delta\), \(\forall 1\leq i,j\leq d:\),

\[\left|\left(W_{2}^{(0)}W_{1}^{(0)}\right)_{i}\right| =\left|\sum_{j=1}^{d}w_{2j}^{(0)}W_{1}^{(0)}[j,i]\right|\leq\sum_{ j=1}^{d}\left|w_{2j}^{(0)}\right|\left|W_{1}^{(0)}[j,i]\right|\] (14) \[\leq\sum_{i=1}^{d}\sqrt{\frac{2}{d^{2\alpha}}\log\frac{2d}{\delta }}\sqrt{\frac{2}{d^{4\alpha}}\log\frac{2d^{2}}{\delta}}\leq\frac{2}{d^{3 \alpha-1}}\log\frac{2d^{2}}{\delta},\] \[\Rightarrow\quad\left|\tilde{r}_{2i}^{(0)}\right| \leq\sum_{j=1}^{d}\left|\left(W_{2}^{(0)}W_{1}^{(0)}\right)_{j} \right|\left|W_{1}^{(0)}[i,j]\right|+\left|Dg_{2i}^{(0)}\right|\] \[\leq\sum_{i=1}^{d}\frac{2}{d^{3\alpha-1}}\log\frac{2d^{2}}{\delta }\sqrt{\frac{2}{d^{4\alpha}}\log\frac{2d^{2}}{\delta}}+\tilde{\mathcal{O}} \left(\frac{1}{d^{\frac{3\alpha}{2}-1}}\right)=\tilde{\mathcal{O}}\left(\frac{1 }{d^{\frac{3\alpha}{2}-1}}\right).\]Next, we bound the \(i\)-th coordinate of \(W_{2}^{(0)}+\frac{1-\beta}{\|A\|_{2}}AW_{1}^{(0)T}\), i.e. \(w_{2i}^{(0)}+\frac{1-\beta}{\|A\|_{2}}A\left(W_{1}^{(0)}[i;]\right)^{T}\).

By independence under Assumption 2, we have that

\[\text{Var}\left(w_{2i}^{(0)}+\frac{1-\beta}{\|A\|_{2}}A\left(W_{1 }^{(0)}[i,\cdot]\right)^{T}\right) =\text{Var}\left(w_{2i}^{(0)}\right)+\frac{(1-\beta)^{2}}{\|A\|_ {2}^{2}}\sum_{j=1}^{d}A_{j}^{2}\text{Var}\left(W_{1}^{(0)}[i,j]\right)\] \[=\frac{1}{d^{2\alpha}}+\frac{(1-\beta)^{2}}{\|A\|_{2}^{2}}\sum_{i =1}^{d}A_{j}^{2}\frac{1}{d^{4\alpha}}=\mathcal{O}\left(\frac{1}{d^{2\alpha}} \right).\]

Using the Gaussian tail bound and union bound, w.p. at least \(1-\delta\), for ever \(1\leq i\leq d\), we have that

\[\left|w_{2i}^{(0)}+\frac{1-\beta}{\|A\|_{2}}A\left(W_{1}^{(0)}[i,\cdot]\right) ^{T}\right|\leq\mathcal{O}\left(\sqrt{\frac{1}{d^{2\alpha}}\log\frac{d}{\delta }}\right)=\tilde{\mathcal{O}}\left(\frac{1}{d^{\alpha}}\right).\]

Since for \(X\sim\mathcal{N}(0,\sigma^{2})\), we have that \(P(|X|\leq t)\leq\frac{2t}{\sqrt{2\pi}\sigma}\), then for a fixed \(i\),

\[P\left(\left|w_{2i}^{(0)}+\frac{1-\beta}{\|A\|_{2}}A\left(W_{1}^{(0)}[i,\cdot] \right)^{T}\right|\leq\frac{1}{d^{\frac{5}{4}\alpha}}\right)\leq\mathcal{O} \left(\frac{2/d^{\frac{5}{4}\alpha}}{\sqrt{2\pi}\cdot\sqrt{1/d^{2\alpha}}} \right)=\Theta\left(\frac{1}{d^{\frac{5}{4}}}\right).\]

Then by union bound, we have that w.p. at least \(1-\frac{1}{d^{\frac{5}{4}\alpha}}\), for every \(1\leq i\leq d\),

\[\left|w_{2i}^{(0)}+\frac{1-\beta}{\|A\|_{2}}A\left(W_{1}^{(0)}[i,\cdot]\right) ^{T}\right|\geq\Theta\left(\frac{1}{d^{\frac{5}{4}\alpha}}\right).\]

Now define \(C_{3}:=W_{2}^{(0)}+\frac{1-\beta}{\|A\|_{2}}AW_{1}^{(0)T}\) and \(C_{4}:=\frac{1-\beta}{2\|A\|_{2}}\tilde{r}_{2i}^{(0)}\). We get that \(C_{3i},i\in[d]\) are i.i.d Gaussian random variables and that \(C_{2}=\frac{1}{2}(C_{3}+C_{4})\), where w.h.p. for all \(i\in[d]\),

\[|C_{3i}|\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\alpha}}\right),\quad|C_{3i}| \geq\Theta\left(\frac{1}{d^{\frac{5}{4}\alpha}}\right),\quad|C_{4i}|\overset{ (i)}{\leq}\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3\alpha}{2}-\frac{1}{2}} }\right),\] (15)

where \((i)\) follows from eq. (14) and the fact that \(\|A\|_{2}=\sqrt{d}\). Then we get that w.h.p.

\[\forall i\in[d]:\frac{|C_{4i}|}{|C_{3i}|}\leq\frac{\tilde{\mathcal{O}}\left( \frac{1}{d^{\frac{5}{4}\alpha}}\right)}{\Omega\left(\frac{1}{d^{\frac{5}{4} \alpha}}\right)}=\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{5}{4}\alpha-\frac{ 1}{2}}}\right).\]

Substituting eq. (15) into eq. (13), we get that w.h.p.,

\[|C_{2i}|=\Theta\left(\left|w_{2i}^{(0)}+\frac{1-\beta}{\|A\|_{2}}A\left(W_{1} ^{(0)}[i,\cdot]\right)^{T}\right|\right)\in\left[\tilde{\Omega}\left(\frac{1 }{d^{\frac{5}{4}\alpha}}\right),\tilde{\mathcal{O}}\left(\frac{1}{d^{\alpha}} \right)\right].\]

Similarly, note that

\[C_{1} =-\frac{W_{2}^{(1)}-\lambda_{2}W_{2}^{(0)}}{\lambda_{2}-\lambda_{1}}\] \[=-\frac{W_{2}^{(0)}+\eta AW_{1}^{(0)T}+\eta\tilde{r}_{2}^{(0)}-W_ {2}^{(0)}-\frac{\eta}{1-\beta}\|A\|_{2}W_{2}^{(0)}}{\frac{2\eta}{1-\beta}\|A\|_ {2}}\] \[=\frac{1}{2}W_{2}^{(0)}-\frac{1-\beta}{2\|A\|_{2}}AW_{1}^{(0)T}- \frac{1-\beta}{2\|A\|_{2}}\tilde{r}_{2}^{(0)},\]

we can use the same techniques to get that i) w.p. at least \(1-\delta\), \(\forall i\in[d]:|C_{1i}|\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\alpha}}\right)\), ii) w.p. at least \(1-\delta-\frac{1}{d^{\frac{5}{4}\alpha}}\), \(\forall i\in[d],|C_{1i}|\geq\tilde{\Omega}\left(\frac{1}{d^{\frac{5}{4} \alpha}}\right)\).

### Proof of Lemma 3

The proof in Section D.2 tells us that at the end of the first phase (when \(t=T_{1}\)),

\[\begin{split} W_{1}^{(T_{1})}&=\bm{u}^{(T_{1})}\bm{v} ^{(T_{1})T}+R_{1}^{(T_{1})},\\ W_{2}^{(T_{1})}&=c^{(T_{1})}\bm{u}^{(T_{1})T}+R_{2}^ {(T_{1})T},\\ \text{where }\bm{v}^{(T_{1})T}=\frac{\eta A}{1-\beta},\quad c^{(T_{1} )}=\frac{\lambda_{2}^{T_{1}}}{\sum_{\tau=0}^{T_{1}-1}\lambda_{2}^{\tau}}.\end{split}\] (16)

Denote the \(i\)-th coordinate of \(\bm{u}^{(t)},\bm{v}^{(t)},R_{2}^{(t)}\) as \(u_{i}^{(t)},v_{i}^{(t)},R_{2i}^{(t)}\), respectively. Denote the \((i,j)\)-th element of \(R_{1}^{(t)}\) as \(R_{1}^{(t)}[i,j]\). For \(t\geq T_{1}\), we prove by induction that,

\[\begin{split} W_{1}^{(t)}&=\bm{u}^{(T_{1})}\bm{v} ^{(t)T}+R_{1}^{(t)},\\ W_{2}^{(t)}&=c^{(t)}\bm{u}^{(T_{1})T}+R_{2}^{(t)T}, \end{split}\] (17)

where

\[\begin{split}\bm{v}^{(t+1)T}&=\bm{v}^{(t)T}-\eta_{t }c^{(t)}E^{(t)},\\ R_{1}^{(t+1)}&=R_{1}^{(t)}-\eta_{t}R_{2}^{(t)}E^{(t )}+r_{1}^{(t)},\\ c^{(t+1)}&=c^{(t)}-\eta_{t}E^{(t)}\bm{v}^{(t)}, \\ R_{2}^{(t+1)T}&=R_{2}^{(t)T}-\eta_{t}E^{(t)}R_{1}^{(t )T}+r_{2}^{(t)},\end{split}\] (18)

with \(r_{1}^{(t)}:=\eta\sum_{\tau=0}^{t}\beta^{t-\tau}\left(W_{2}^{(t)T}E^{(t)}-W_{ 2}^{(\tau)T}E^{(\tau)}\right)-\eta\sum_{\tau=0}^{t}\beta^{t-\tau}Dg_{1}^{(\tau )}\), \(E^{(t)}:=W_{2}^{(t)}W_{1}^{(t)}-A\), \(\eta_{t}=\eta\sum_{\tau=0}^{t}\beta^{t-\tau}\) and \(r_{2}^{(t)}=\eta\sum_{\tau=0}^{t}\beta^{t-\tau}\left(E^{(t)}W_{1}^{(t)T}-E^{ (\tau)}W_{1}^{(\tau)T}\right)-\eta\sum_{\tau=0}^{t}\beta^{t-\tau}Dg_{2}^{(\tau )}\). Note that the \(r_{1}^{(t)}\) and \(r_{2}^{(t)}\) here are different from those defined in Section D.2, but we abuse the notation and still use \(r_{1}^{(t)}\) and \(r_{2}^{(t)}\) to represent the error terms.

The base case is already given by eq. (16).

Suppose our lemma holds for \(t\), then for \(t+1\), using the same techniques as in eq. (5) and eq. (6), we have that

\[\begin{split} W_{1}^{(t+1)}&=W_{1}^{(t)}-\eta\sum_{ \tau=0}^{t}\beta^{t-\tau}W_{2}^{(\tau)T}E^{(\tau)}-\eta\sum_{\tau=0}^{t}\beta^ {t-\tau}Dg_{1}^{(\tau)}\\ &=W_{1}^{(t)}-\eta_{t}W_{2}^{(t)T}E^{(t)}+r_{1}^{(t)},\\ W_{2}^{(t+1)}&=W_{2}^{(t)}-\eta\sum_{\tau=0}^{t}\beta^ {t-\tau}E^{(\tau)}W_{1}^{(\tau)T}-\eta\sum_{\tau=0}^{t}\beta^{t-\tau}Dg_{2}^{ (\tau)}\\ &=W_{2}^{(t)}-\eta_{t}E^{(t)}W_{1}^{(t)T}+r_{2}^{(t)},\end{split}\] (19)

Plugging in the inductive hypothesis yields

\[\begin{split} W_{1}^{(t+1)}&=W_{1}^{(t)}-\eta_{t}W_{ 2}^{(t)T}E^{(t)}+r_{1}^{(t)}\\ &=\bm{u}^{(T_{1})}\bm{v}^{(t)T}+R_{1}^{(t)}-\eta_{t}\left(c^{(t)} \bm{u}^{(T_{1})}+R_{2}^{(t)}\right)E^{(t)}+r_{1}^{(t)}\\ &=\bm{u}^{(T_{1})}\left(\bm{v}^{(t)T}-\eta_{t}c^{(t)}E^{(t)} \right)+R_{1}^{(t)}-\eta_{t}R_{2}^{(t)}E^{(t)}+r_{1}^{(t)},\\ W_{2}^{(t+1)}&=W_{2}^{(t)}-\eta_{t}E^{(t)}W_{1}^{(t )T}+r_{2}^{(t)}\\ &=c^{(t)}\bm{u}^{(T_{1})T}+R_{2}^{(t)T}-\eta_{t}E^{(t)}\left(\bm{ v}^{(t)}\bm{u}^{(T_{1})T}+R_{1}^{(t)T}\right)+r_{2}^{(t)}\\ &=\left(c^{(t)}-\eta_{t}E^{(t)}\bm{v}^{(t)}\right)\bm{u}^{(T_{1})T }+R_{2}^{(t)T}-\eta_{t}E^{(t)}R_{1}^{(t)T}+r_{2}^{(t)}.\end{split}\]

It implies that our lemma holds for \(t+1\), which completes the proof.

Now we analyze the error terms \(\left|R_{1}^{(t)}[i,j]\right|\) and \(\left|R_{2i}^{(t)}\right|\). Eq. (16) tells us that \(c^{(T_{1})}\) and \(\forall i\in[d],v_{i}^{(T_{1})}\) are all positive. We first prove by induction that for all \(T_{1}\leq t\leq T_{2}\), \(c^{(t)}>0,\forall i\in[d],v_{i}^{(t)}>0\).

The above discussion already proves the base case. Suppose at time \(t\), we have \(c^{(t)}>0,\forall i\in[d],v_{i}^{(t)}>0\). Note that when \(T_{1}\leq t<T_{2}\), \(\forall i\in[d]:E_{i}^{(t)}\leq 0\), then for \(t+1\),

\[v_{i}^{(t+1)} =v_{i}^{(t)}-\eta_{t}c^{(t)}E_{i}^{(t)}>0,\] \[c^{(t+1)} =c^{(t)}-\eta_{t}\sum_{i=1}^{d}E_{i}^{(t)}v_{i}^{(t)}>0.\]

Therefore by induction, we have proved that for all \(T_{1}\leq t\leq T_{2}\), \(c^{(t)}>0,\forall i\in[d],v_{i}^{(t)}>0\).

Now we prove that for all \(T_{1}\leq t\leq T_{2}\),

\[\forall 1\leq i,j\leq d:\quad 0\leq\frac{\left|R_{1}^{(t)}[i,j]\right|}{\left| u_{i}^{(T_{1})}\right|v_{j}^{(t)}}\leq\delta_{i}+\sum_{\tau=T_{1}}^{t-1} \epsilon_{i}^{(\tau)},\quad 0\leq\frac{\left|R_{2i}^{(t)}\right|}{c^{(t)}\left|u_{i}^{(T_ {1})}\right|}\leq\delta_{i}+\sum_{\tau=T_{1}}^{t-1}\epsilon_{i}^{(\tau)},\] (18)

where

\[\delta_{i}:=\max\left\{\max_{j}\frac{\left|R_{1}^{(T_{1})}[i,j]\right|}{\left| u_{i}^{(T_{1})}\right|v_{j}^{(T_{1})}},\frac{\left|R_{2i}^{(T_{1})}\right|}{c^{(T_ {1})}\left|u_{i}^{(T_{1})}\right|}\right\},\quad\epsilon_{i}^{(t)}:=\max\left\{ \max_{j}\frac{\left|r_{1}^{(t)}[i,j]\right|}{\left|u_{i}^{(T_{1})}\right|v_{j} ^{(t)}},\frac{\left|r_{2i}^{(t)}\right|}{c^{(t)}\left|u_{i}^{(T_{1})}\right|} \right\}.\]

The left hand sides of the inequalities are trivial since we have proved that \(c^{(t)}>0,\forall i\in[d],v_{i}^{(t)}>0\) for all \(T_{1}\leq t\leq T_{2}\). Now we prove the right hand sides by induction.

The base case is already verified by the definition of \(\delta_{i}\). Suppose eq.(18) holds for \(T_{1}\leq t<T_{2}\). Then for \(t+1\), using \(\forall i\in[d]:E_{i}^{(t)}\leq 0\) and \(v^{(t+1)}\geq v^{(t)},c^{(t+1)}\geq c^{(t)}\), we can get that \(\forall 1\leq i,j\leq d\)

\[\frac{\left|R_{1}^{(t+1)}[i,j]\right|}{\left|u_{i}^{(T_{1})}\right| v_{j}^{(t+1)}} \leq\frac{\left|R_{1}^{(t)}[i,j]\right|\left|\frac{1}{\left|u_{i}^ {(T_{1})}\right|}+\eta_{t}\left|R_{2i}^{(t)}\right|\left|\frac{1}{\left|u_{i}^ {(T_{1})}\right|}\right|-\left(E_{j}^{(t)}\right)}{v_{j}^{(t)}+\eta_{t}c^{(t )}\left(-E_{j}^{(t)}\right)}+\frac{\left|r_{1}^{(t)}[i,j]\right|}{\left|u_{i}^ {(T_{1})}\right|v_{j}^{(t)}}\] \[\leq\frac{\left(\delta_{i}+\sum_{\tau=T_{1}}^{t-1}\epsilon_{i}^{( \tau)}\right)v_{j}^{(t)}+\eta_{t}\left(\delta_{i}+\sum_{\tau=T_{1}}^{t-1} \epsilon_{i}^{(\tau)}\right)c^{(t)}\left(-E_{j}^{(t)}\right)}{v_{j}^{(t)}+ \eta_{t}c^{(t)}\left(-E_{j}^{(t)}\right)}+\epsilon_{i}^{(t)}\] \[=\delta_{i}+\sum_{\tau=T_{1}}^{t}\epsilon_{i}^{(\tau)}.\]

Similarly, we have that \(\forall 1\leq i\leq d\)

\[\frac{\left|R_{2i}^{(t+1)}\right|}{c^{(t+1)}\left|u_{i}^{(T_{1}) }\right|} \leq\frac{\left|R_{2i}^{(t)}\right|\frac{1}{\left|u_{i}^{(T_{1})} \right|}+\eta_{t}\sum_{j=1}^{d}\left(-E_{j}^{(t)}\right)\left|R_{1}^{(t)}[i,j ]\right|\frac{1}{\left|u_{i}^{(T_{1})}\right|}}{c^{(t)}+\eta_{t}\sum_{j=1}^{d }\left(-E_{j}^{(t)}\right)v_{j}^{(t)}}+\frac{\left|r_{2i}^{(t)}\right|}{\left| u_{i}^{(T_{1})}\right|c^{(t)}}\] \[\leq\frac{\left(\delta_{i}+\sum_{\tau=T_{1}}^{t-1}\epsilon_{i}^{( \tau)}\right)c^{(t)}+\eta_{t}\left(\delta_{i}+\sum_{\tau=T_{1}}^{t-1}\epsilon_{ i}^{(\tau)}\right)\sum_{j=1}^{d}\left(-E_{j}^{(t)}\right)v_{j}^{(t)}}{c^{(t)}+ \eta_{t}\sum_{j=1}^{d}\left(-E_{j}^{(t)}\right)v_{j}^{(t)}}+\epsilon_{i}^{(t)}\] \[=\delta_{i}+\sum_{\tau=T_{1}}^{t}\epsilon_{i}^{(\tau)}.\]

Therefore by induction, eq. (18) holds for all \(t\) in the second phase.

So far we have proved the rank 1 structure stated in Lemma 3. The remaining part of the proof is given by the following lemma, whose proof is deferred to Section D.9.

**Lemma 9**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}}{d^{\alpha/2+1}}\) and pick \(\eta\leq\mathcal{O}\left(\frac{1}{d^{\frac{1}{d^{\alpha}}+4}}\right)\). Consider any \(t\) such that \(T_{1}\leq t<\min\{T_{2},T_{3}\}\). Suppose for all \(T_{1}\leq\tau\leq t\), we have \(\forall i,j\in[d]:\left|w_{2i}^{(\tau)}\right|\leq\mathcal{O}\left(d^{1/4} \right),\left|W_{1}^{(\tau)}[i,j]\right|\leq\mathcal{O}\left(\frac{1}{d^{1/4}}\right)\), then we have that \(\forall i,j\in[d]:\left|r_{1}^{(t)}[i,j]\right|=\tilde{\mathcal{O}}\left(\eta^ {2}d^{11/4}\right),\left|r_{2i}^{(t)}\right|=\tilde{\mathcal{O}}\left(\eta^{2 }d^{13/4}\right)\). Moreover, we can get that \(\forall i\in[d]:\epsilon_{i}^{(t)}=\tilde{\mathcal{O}}\left(\eta^{2}d^{1 \alpha+\frac{13}{4}}\right)\), where \(\epsilon_{i}^{(t)}\) is defined in eq. (18)._

**Lemma 12**.: _Under the conditions of Lemma 11 and pick \(\eta\leq\mathcal{O}\left(\frac{\epsilon}{d^{\frac{1}{d^{\alpha}}+4}}\right)\), we have that at time \(t+1\),_

\[0\leq\frac{\left|R_{1}^{(t+1)T}\bm{u}^{(T_{1})}\right|}{c^{(t+1)}\left\|\bm{u} ^{(T_{1})}\right\|^{2}}\leq\tilde{\mathcal{O}}(\epsilon_{0}),\quad 0\leq \frac{\left|R_{2i}^{(t+1)}\right|}{c^{(t+1)}\left\|\bm{u}^{(T_{1})}\right\|^{2 }v_{j}^{(t)}}\leq\tilde{\mathcal{O}}(\epsilon_{0}).\]

_Moreover,_

\[\forall j\in[d]:\frac{\left|R_{3j}^{(t+1)T}\bm{u}^{(T_{1})}\right|}{A_{j}} \leq\tilde{\mathcal{O}}(\epsilon_{0}).\] (21)

**Lemma 13**.: _Under the conditions of Lemma 11 and pick \(\eta\leq\mathcal{O}\left(\frac{\epsilon}{d^{\frac{1}{d^{\alpha}}+4}}\right)\), we have that at time \(t+1\),_

\[0\leq\frac{\left|R_{2}^{(t+1)T}\bm{u}^{(T_{1})}\right|}{c^{(t+1)}\left\|\bm{u} ^{(T_{1})}\right\|^{2}}\leq\tilde{\mathcal{O}}(\epsilon_{0}),\quad\forall j\in [d]:\quad 0\leq\frac{\left|R_{3j}^{(t+1)}\right|}{c^{(t+1)}\left\|\bm{u}^{(T_{1})} \right\|^{2}v_{j}^{(t)}}\leq\tilde{\mathcal{O}}(\epsilon_{0}).\]

_Moreover,_

\[\forall j\in[d]:\frac{\left|R_{3j}^{(t+1)}\right|}{A_{j}}\leq\tilde{\mathcal{O }}(\epsilon_{0}).\] (22)

**Lemma 14**.: _Under the conditions of Lemma 11 and pick \(\eta\leq\mathcal{O}\left(\frac{\epsilon}{d^{\frac{1}{d^{\alpha}}+4}}\right)\), if we further suppose that \(\forall j\in[d]:\frac{v_{j}^{(t)}}{c^{(t)}}=\Theta\left(\frac{1}{\sqrt{d}}\right)\), \(\frac{\left|R_{3j}^{(t)}\right|}{A_{j}}\) and \(\frac{\left|R_{3j}^{(t)}\right|}{a^{(t)}A_{j}}\) are of order \(\tilde{\mathcal{O}}(\epsilon_{0})\), then we have that at time \(t+1\),_1. \(\forall i,j\in[d]:\frac{E_{j}^{(t)}}{E_{i}^{(t)}}=\Theta(1)\),
2. \(\forall j\in[d]:\frac{v_{j}^{(t+1)}}{e^{(t+1)}}=\Theta\left(\frac{1}{\sqrt{d}}\right)\),
3. \(\forall i,j\in[d]:\left|w_{2i}^{(t+1)}\right|\leq\mathcal{O}\left(d^{1/4} \right),\left|W_{1}^{(t+1)}[i,j]\right|\leq\mathcal{O}\left(\frac{1}{d^{1/4}}\right)\),
4. \(\forall j\in[d]\), \(\frac{\left|R_{3j}^{(t+1)}\right|}{A_{j}}\) and \(\frac{\left|R_{3j}^{(t+1)}\right|}{a^{(t+1)}A_{j}}\) are of order \(\tilde{\mathcal{O}}(\epsilon_{0})\).

By combining Lemma 11, 12 and 14, we can prove by induction that for all \(T_{1}\leq t\leq\min\{T_{2},T_{3}\}\), eq. (19) holds (which follows from Lemma 12), and

\[\forall i,j\in[d]:\frac{E_{j}^{(t)}}{E_{i}^{(t)}}=\Theta(1),\] (22)

which follows from the part (A) of Lemma 14. Now the only thing to verify is the base case, i.e. when \(t=T_{1}\). More specifically, we want to prove that 1) \(\forall i,j\in[d]:\left|w_{2i}^{(T_{1})}\right|\leq\mathcal{O}\left(d^{1/4} \right),\left|W_{1}^{(T_{1})}[i,j]\right|\leq\mathcal{O}\left(\frac{1}{d^{1/4 }}\right)\) and that 2) \(\forall j\in[d]:\frac{v_{j}^{(T_{1})}}{e^{(T_{1})}}=\Theta\left(\frac{1}{\sqrt {d}}\right)\), and that 3) \(\frac{\left|R_{3j}^{(T_{1})}\right|}{A_{j}}\) and \(\frac{\left|R_{3j}^{(T_{1})}\right|}{a^{(T_{1})}A_{j}}\) are of order \(\tilde{\mathcal{O}}(\epsilon_{0})\). All of them can be verified by the proof in Section D.2 and the definition of \(R_{v}^{(t)},R_{3}^{(t)}\).

So far we have proved eq. (19) in Lemma 9. Now let's prove when \(t=\min\{T_{2},T_{3}\}\), we have that \(\left\|E^{(t)}\right\|_{2}^{2}=\mathcal{O}(\epsilon_{0}d)\).

If \(\min\{T_{2},T_{3}\}=T_{3}\), by Definition 3, we have \(\left\|E^{(t)}\right\|_{2}^{2}\leq\epsilon\). If \(\min\{T_{2},T_{3}\}=T_{2}\), by Definition 2, there exists \(j\in[d]\) such that \(E_{j}^{(t)}=-\Theta\left(\sqrt{\epsilon_{0}}\right)\). Combining with eq. (22) gives us \(\forall i\in[d]:E_{i}^{(t)}=-\Theta\left(\sqrt{\epsilon_{0}}\right)\). Combining these two cases, we get that when \(t=\min\{T_{2},T_{3}\}\), \(\left\|E^{(t)}\right\|_{2}^{2}\leq\max\{\epsilon,\Theta\left(\epsilon_{0}d \right)\}=\mathcal{O}\left(\epsilon_{0}d\right)\).

### Proof of Lemma 10

We prove this lemma by induction. The base case (\(t=T_{1}\)) of \(\bm{v}^{(t)}\) is verified by eq. (16).

Suppose at time \(t\), \(\bm{v}^{(t)T}=a^{(t)}A+R_{v}^{(t)T}\), then by eq. 17, we have that

\[W_{2}^{(t)}W_{1}^{(t)} =\left(c^{(t)}\bm{u}^{(T_{1})T}+R_{2}^{(t)T}\right)\left(\bm{u}^{ (T_{1})}\bm{v}^{(t)T}+R_{1}^{(t)}\right)\] \[=\left(c^{(t)}\left\|\bm{u}^{(T_{1})}\right\|^{2}+R_{2}^{(t)T}\bm {u}^{(T_{1})}\right)\bm{v}^{(t)}+c^{(t)}\bm{u}^{(T_{1})T}R_{1}^{(t)}+R_{2}^{(t )T}R_{1}^{(t)}\] \[=d^{(t)}\bm{v}^{(t)T}+R_{3}^{(t)T}\] \[=d^{(t)}a^{(t)}A+d^{(t)}R_{v}^{(t)T}+R_{3}^{(t)T},\]

where \(d^{(t)}:=c^{(t)}\left\|\bm{u}^{(T_{1})}\right\|^{2}+R_{2}^{(t)T}\bm{u}^{(T_{1 })},\quad R_{3}^{(t)T}:=c^{(t)}\bm{u}^{(T_{1})T}R_{1}^{(t)}+R_{2}^{(t)T}R_{1}^ {(t)}\). That gives us

\[\bm{v}^{(t+1)T} =\bm{v}^{(t)T}-\eta_{t}c^{(t)}E^{(t)}\] \[=a^{(t)}A+R_{v}^{(t)T}-\eta_{t}c^{(t)}\left(d^{(t)}a^{(t)}A+d^{( t)}R_{v}^{(t)T}+R_{3}^{(t)T}-A\right)\] \[=\left(\left(1-\eta_{t}c^{(t)}d^{(t)}\right)a^{(t)}+\eta_{t}c^{( t)}\right)A+\left(1-\eta_{t}c^{(t)}d^{(t)}\right)R_{v}^{(t)T}-\eta_{t}c^{(t)}R_{3}^ {(t)T}\] \[:=a^{(t+1)}A+R_{v}^{(t+1)T}.\]

Therefore we have proved by induction that for \(t\) in the second phase, \(\bm{v}^{(t)}=a^{(t)}A+R_{v}^{(t)T}\). The above steps also proved eq. (20).

### Proof of Lemma 11

Write \(r_{1}^{(t)}=q_{11}^{(t)}+q_{12}^{(t)}\) where we have \(q_{11}^{(t)}=\eta\sum_{\tau=0}^{t}\beta^{t-\tau}\left(W_{2}^{(t)T}E^{(t)}-W_{2} ^{(\tau)T}E^{(\tau)}\right)\), \(q_{12}^{(t)}=-\eta\sum_{\tau=0}^{t}\beta^{t-\tau}Dg_{1}^{(\tau)}\). Write \(r_{2}^{(t)}=q_{21}^{(t)}+q_{22}^{(t)}\) where \(q_{22}^{(t)}=-\eta\sum_{\tau=0}^{t}\beta^{t-\tau}Dg_{2}^{(\tau)}\), \(q_{21}^{(t)}=\eta\sum_{\tau=0}^{t}\beta^{t-\tau}\left(E^{(t)}W_{1}^{(t)T}-E^{( \tau)}W_{1}^{(\tau)T}\right)\).

Let's first bound \(\left|q_{11}^{(t)}[i,j]\right|\) and \(\left|q_{21,i}^{(t)}\right|\). By definition of \(T_{2}\), we know that for \(T_{1}\leq\tau\leq t\), \(\forall i\in[d]:\left|E_{i}^{(\tau)}\right|=\mathcal{O}(1)\). Then we have for all \(i,j\in[d]\),

\[\left|W_{1}^{(\tau+1)}[i,j]-W_{1}^{(\tau)}[i,j]\right|\leq\eta \sum_{k=0}^{\tau}\beta^{\tau-k}\left|w_{2i}^{(k)}E_{j}^{(k)}\right|\leq\eta \sum_{k=0}^{\tau}\beta^{\tau-k}\mathcal{O}\left(d^{1/4}\right)=\eta\mathcal{O} \left(d^{1/4}\right),\] \[\left|w_{2i}^{(\tau+1)}-w_{2i}^{(\tau)}\right|\leq\eta\sum_{k=0}^ {\tau}\beta^{\tau-k}\sum_{j=1}^{d}\left|E_{j}^{(k)}W_{1}^{(k)}[j,i]\right|\leq \eta\sum_{k=0}^{\tau}\beta^{\tau-k}\mathcal{O}\left(d^{3/4}\right)=\eta \mathcal{O}\left(d^{3/4}\right).\] (23)

Note that

\[\left|E_{j}^{(\tau+1)}-E_{j}^{(\tau)}\right|= \sum_{i=1}^{d}\left(\left(w_{2i}^{(\tau+1)}-w_{2i}^{(\tau)} \right)W_{1}^{(\tau)}[i,j]+w_{2i}^{(\tau)}\left(W_{1}^{(\tau+1)}[i,j]-W_{1}^{( \tau)}[i,j]\right)\right)\] \[+\sum_{i=1}^{d}\left(\left(w_{2i}^{(\tau+1)}-w_{2i}^{(\tau)} \right)\left(W_{1}^{(\tau+1)}[i,j]-W_{1}^{(\tau)}[i,j]\right)\right).\]

We can further get that for \(\forall j\in[d]\),

\[\left|E_{j}^{(\tau+1)}-E_{j}^{(\tau)}\right| \leq\eta d\mathcal{O}\left(d^{3/4}\right)\mathcal{O}\left(d^{-1/ 4}\right)+\eta d\mathcal{O}\left(d^{1/4}\right)\mathcal{O}\left(d^{1/4}\right) +\eta^{2}d\mathcal{O}\left(d^{3/4}\right)\mathcal{O}\left(d^{1/4}\right)\] \[=\mathcal{O}\left(\eta d^{3/2}+\eta^{2}d^{2}\right)=\mathcal{O} \left(\eta d^{3/2}\right).\]

Combining the above inequalities gives us \(\forall i,j\in[d]\),

\[\left|q_{11}^{(t)}[i,j]\right| =\eta\left|\sum_{\tau=0}^{t}\beta^{t-\tau}\left(w_{2i}^{(t)}E_{j} ^{(t)}-w_{2i}^{(\tau)}E_{j}^{(\tau)}\right)\right|\] \[\leq\eta^{2}\sum_{\tau=0}^{t}\beta^{t-\tau}(t-\tau)\left(\mathcal{ O}\left(d^{3/4}\right)\mathcal{O}(1)+\mathcal{O}\left(d^{1/4}\right) \mathcal{O}\left(d^{3/2}\right)\right)=\mathcal{O}\left(\eta^{2}d^{7/4} \right),\]

\[\left|q_{12,i}^{(t)}\right| =\eta\left|\sum_{\tau=0}^{t}\beta^{t-\tau}\sum_{j=1}^{d}\left(E_ {j}^{(t)}W_{1}^{(t)}[i,j]-E_{j}^{(\tau)}W_{1}^{(\tau)}[i,j]\right)\right|\] \[\leq\eta\sum_{\tau=0}^{t}\beta^{t-\tau}\sum_{j=1}^{d}\left(\left| E_{j}^{(t)}\right|\left|W_{1}^{(t)}[i,j]-W_{1}^{(\tau)}[i,j]\right|+\left|E_{j}^{(t)}-E_ {j}^{(\tau)}\right|\left|W_{1}^{(\tau)}[i,j]\right|\right)\] \[\leq\eta^{2}d\sum_{\tau=0}^{t}\beta^{t-\tau}(t-\tau)\left( \mathcal{O}(1)\mathcal{O}\left(d^{1/4}\right)+\mathcal{O}\left(d^{3/2}\right) \mathcal{O}\left(d^{-1/4}\right)\right)=\mathcal{O}\left(\eta^{2}d^{9/4} \right).\]

Next let's bound \(\left|q_{12}^{(t)}[i,j]\right|\) and \(\left|q_{22,i}^{(t)}\right|\). By the assumption of this lemma and the analysis before \(T_{1}\), we know that for all \(\tau\leq t\), the \(M_{1}^{(\tau)},M_{2}^{(\tau)}\) in Lemma 32 are upper bounded by \(\mathcal{O}\left(\frac{1}{d^{1/4}}\right)\) and \(\mathcal{O}\left(d^{1/4}\right)\), respectively. In the theorem we consider the training period before \(T_{\mathrm{SGD},2}\) so the time \(T\) inLemma 32 is set as \(T_{\text{SGD},2}\). In the following sections, we will prove that \(T_{\text{SGD},2}\leq\mathcal{O}\left(\frac{d^{\alpha}\log\left(\sqrt{d/\epsilon} \right)}{\eta}\right)\). Then by Lemma 32, we have with probability at least \(1-\frac{1}{d}\), for \(\forall\tau\leq t\) and \(\forall i,j\in[d]\),

\[\left|Dg_{1}^{(\tau)}[i,j]\right|=\left|\tilde{g}_{1}^{(\tau)}[i, j]-g_{1}^{(\tau)}[i,j]\right| \leq\mathcal{O}\left(d^{\frac{13}{4}}\sigma\sqrt{\frac{d^{\alpha +1}}{\eta}\log\frac{d}{\epsilon}}\right)+\mathcal{O}\left(d^{\frac{1}{4}} \sigma\sqrt{\frac{d^{\alpha+2}}{\eta}\log\frac{d}{\epsilon}}\right)\] \[\leq\tilde{\mathcal{O}}\left(d^{\frac{13}{4}}\sigma\sqrt{\frac{d^ {\alpha+1}}{\eta}}\right),\]

\[\left|Dg_{2i}^{(\tau)}\right|=\left|\tilde{g}_{2i}^{(\tau)}-g_{2 i}^{(\tau)}\right| \leq\mathcal{O}\left(d^{\frac{15}{4}}\sigma\sqrt{\frac{d^{\alpha +1}}{\eta}\log\frac{d}{\epsilon}}\right)+\mathcal{O}\left(d^{\frac{3}{4}} \sigma\sqrt{\frac{d^{\alpha+2}}{\eta}\log\frac{d}{\epsilon}}\right)\] \[=\tilde{\mathcal{O}}\left(d^{\frac{15}{4}}\sigma\sqrt{\frac{d^{ \alpha+1}}{\eta}}\right).\]

By picking \(\sigma\leq\frac{\eta^{3/2}}{d^{\alpha/2+1}}\), we have \(\left|Dg_{1}^{(\tau)}[i,j]\right|\leq\eta\tilde{\mathcal{O}}\left(d^{\frac{11} {4}}\right)\) and \(\left|Dg_{2i}^{(\tau)}\right|\leq\eta\tilde{\mathcal{O}}\left(d^{\frac{13}{4}}\right)\), which yields

\[\left|q_{12}^{(t)}[i,j]\right| \leq\eta\sum_{\tau=0}^{t}\beta^{t-\tau}\left|Dg_{1}^{(\tau)}[i,j] \right|\leq\tilde{\mathcal{O}}\left(\eta^{2}d^{\frac{11}{4}}\right),\] \[\left|q_{22,i}^{(t)}\right| \leq\eta\sum_{\tau=0}^{t}\beta^{t-\tau}\left|Dg_{2i}^{(\tau)} \right|\leq\tilde{\mathcal{O}}\left(\eta^{2}d^{\frac{13}{4}}\right).\]

Combining the above bounds, we get that \(\forall i,j\in[d]\),

\[\left|r_{1}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}\left(\eta^{2}d^{\frac{1 3}{4}}\right),\quad\left|r_{2i}^{(t)}\right|\leq\tilde{\mathcal{O}}\left( \eta^{2}d^{\frac{13}{4}}\right).\]

By the analysis in Section D.2, we know that at time \(T_{1}\), for some \(i_{0}\in[d]\), \(c^{(T_{1})}\left|u_{i_{0}}^{(T_{1})}\right|=\Theta\left(\frac{1}{d^{\frac{3}{ 2}}}\right)\), and for \(\forall i,j\in[d]\), we have \(c^{(T_{1})}\left|u_{i}^{(T_{1})}\right|=\tilde{\Omega}\left(\frac{1}{d^{\frac{ 3}{4}}}\right)\) and \(\left|u_{i}^{(T_{1})}\right|v_{j}^{(T_{1})}=\tilde{\Omega}\left(\frac{1}{d^{ \frac{3}{4}}+\frac{1}{2}}\right)\), which gives us \(\forall i,j\in[d]\),

\[\frac{\left|r_{1}^{(t)}[i,j]\right|}{\left|u_{i}^{(T_{1})}\right|v_{j}^{(t)}} \leq\frac{\left|r_{1}^{(t)}[i,j]\right|}{\left|u_{i}^{(T_{1})}\right|v_{j}^{( T_{1})}}=\tilde{\mathcal{O}}\left(\eta^{2}d^{\frac{3}{4}\alpha+\frac{13}{4}} \right),\frac{\left|r_{2i}^{(t)}\right|}{c^{(t)}\left|u_{i}^{(T_{1})}\right|} \leq\frac{\left|r_{2i}^{(t)}\right|}{c^{(T_{1})}\left|u_{i}^{(T_{1})}\right|} =\tilde{\mathcal{O}}\left(\eta^{2}d^{\frac{3}{4}\alpha+\frac{13}{4}}\right).\]

Hence we get the bound \(\forall i\in[d]:\epsilon_{i}^{(t)}\leq\tilde{\mathcal{O}}\left(\eta^{2}d^{ \frac{3}{4}\alpha+\frac{13}{4}}\right)\).

### Proof of Lemma 12

Let's first try to bound the length of \(\min\{T_{2},T_{3}\}\). More formally, we prove that under the conditions of Lemma 11 and pick \(\eta\leq\mathcal{O}\left(\frac{\epsilon}{d^{\frac{3}{4}\alpha+\epsilon}}\right)\), we have that \(\min\{T_{2},T_{3}\}\leq\mathcal{O}\left(\frac{d^{\alpha}\log\left(\sqrt{d/ \epsilon}\right)}{\eta}\right)\).

Under the conditions of Lemma 11, we know that

\[\forall j\in[d]: \left|\left(r_{2}^{(t)}W_{1}^{(t)}\right)_{j}\right|\leq\sum_{i= 1}^{d}\left|r_{2i}^{(t)}W_{1}^{(t)}[i,j]\right|=\mathcal{O}\left(\eta^{2}d^{4} \right),\] \[\left|\left(W_{2}^{(t)}r_{1}^{(t)}\right)_{j}\right|\leq\sum_{i= 1}^{d}\left|w_{2i}^{(t)}r_{1}^{(t)}[i,j]\right|=\mathcal{O}\left(\eta^{2}d^{4} \right).\]Combining with eq. (23), we get

\[E^{(t+1)}\] \[= E^{(t)}+\left(W_{2}^{(t+1)}-W_{2}^{(t)}\right)W_{1}^{(t)}+W_{2}^{(t )}\left(W_{1}^{(t+1)}-W_{1}^{(t)}\right)+\left(W_{2}^{(t+1)}-W_{2}^{(t)}\right) \left(W_{1}^{(t+1)}-W_{1}^{(t)}\right)\] \[= E^{(t)}-\eta_{t}E^{(t)}W_{1}^{(t)T}W_{1}^{(t)}+r_{2}^{(t)}W_{1}^{ (t)}-\eta_{t}W_{2}^{(t)}W_{2}^{(t)T}E^{(t)}+W_{2}^{(t)}r_{1}^{(t)}+\mathcal{O} \left(\eta^{2}d\right)\] \[= E^{(t)}\left(I-\eta_{t}W_{1}^{(t)T}W_{1}^{(t)}-\eta_{t}\left\|W_ {2}^{(t)}\right\|_{2}^{2}I\right)+\mathcal{O}\left(\eta^{2}d^{4}\right)+ \mathcal{O}\left(\eta^{2}d\right).\]

Then we have

\[\left\|E^{(t+1)}\right\|_{2} \leq\left\|E^{(t)}\right\|_{2}\left\|I-\eta_{t}W_{1}^{(t)T}W_{1}^ {(t)}-\eta_{t}\left\|W_{2}^{(t)}\right\|_{2}^{2}I\right\|_{2}+\mathcal{O} \left(\eta^{2}d^{4}\right)\] \[\leq\left(1-\eta_{t}\left\|W_{2}^{(t)}\right\|_{2}^{2}\right) \left\|E^{(t)}\right\|_{2}+\mathcal{O}\left(\eta^{2}d^{4}\right).\]

When \(T_{1}\leq t<T_{2}\), we have proved that \(c^{(t)}\) is increasing over time in Section D.8, which implies that \(\left\|W_{2}^{(t)}\right\|_{2}^{2}\geq C\left\|W_{2}^{(T_{1})}\right\|_{2}^{2}\) since \(c^{(t)}\bm{u}^{(T_{1})T}\) is the leading term of \(W_{2}^{(t)}\). Combining with \(\eta_{t}\geq\eta\) gives us

\[\left\|E^{(t+1)}\right\|_{2} \leq\left(1-\eta C\left\|W_{2}^{(T_{1})}\right\|_{2}^{2}\right) \left\|E^{(t)}\right\|_{2}+\mathcal{O}\left(\eta^{2}d^{4}\right),\] \[\Rightarrow\quad\left\|E^{(t)}\right\|_{2} \leq\frac{\mathcal{O}\left(\eta^{2}d^{4}\right)}{\eta C\left\|W_{ 2}^{(T_{1})}\right\|_{2}^{2}}+\left(1-\eta C\left\|W_{2}^{(T_{1})}\right\|_{2 }^{2}\right)^{t-T_{1}}\left(\left\|E^{(T_{1})}\right\|_{2}-\frac{\mathcal{O} \left(\eta^{2}d^{4}\right)}{\eta C\left\|W_{2}^{(T_{1})}\right\|_{2}^{2}}\right)\] \[\overset{(i)}{\leq}\mathcal{O}\left(\frac{\eta d^{4}}{\left\|W_{ 2}^{(T_{1})}\right\|_{2}^{2}}\right)+\exp\left(-\eta C\left\|W_{2}^{(T_{1})} \right\|_{2}^{2}(t-T_{1})\right)\mathcal{O}\left(\sqrt{d}\right),\]

where \((i)\) uses \(\left\|E^{(T_{1})}\right\|_{2}=\mathcal{O}(\sqrt{d})\). By picking \(\eta\leq\mathcal{O}\left(\frac{\epsilon}{d^{\frac{1}{d^{\alpha}}+4}}\right)\) and noticing that \(\left\|W_{2}^{(T_{1})}\right\|_{2}^{2}\geq\Omega\left(\frac{1}{d^{\alpha}}\right)\), we have \(\frac{\eta d^{4}}{\left\|W_{2}^{(T_{1})}\right\|_{2}^{2}}<\frac{\sqrt{\epsilon }}{2}\). Hence when \(t-T_{1}\geq\Theta\left(\frac{\log\left(\sqrt{d/\epsilon}\right)}{\eta\left\|W_ {2}^{(T_{1})}\right\|_{2}^{2}}\right)\), we have that \(\left\|E^{(t)}\right\|_{2}\leq\sqrt{\epsilon}\), i.e. \(\left\|E^{(t)}\right\|_{2}^{2}\leq\epsilon\).

That means after at most \(\mathcal{O}\left(\frac{\log\left(\sqrt{d/\epsilon}\right)}{\eta\left\|W_{2}^{( T_{1})}\right\|_{2}^{2}}\right)\) steps from \(T_{1}\), either \(t\geq T_{2}\), or we have \(\left\|E^{(t)}\right\|_{2}^{2}\leq\epsilon\). In other words, \(\min\{T_{2},T_{3}\}\leq T_{1}+\mathcal{O}\left(\frac{\log\left(\sqrt{d/ \epsilon}\right)}{\eta\left\|W_{2}^{(T_{1})}\right\|_{2}^{2}}\right)\leq \mathcal{O}\left(\frac{d^{\alpha}\log\left(\sqrt{d/\epsilon}\right)}{\eta}\right)\).

Now we are ready to bound eq. 18.

Combining \(\min\{T_{2},T_{3}\}\leq\mathcal{O}\left(\frac{d^{\alpha}\log\left(\sqrt{d/ \epsilon}\right)}{\eta}\right)\) and Lemma 11 yields that for \(t+1\leq\min\{T_{2},T_{3}\}\), \(\forall i\in[d]\),

\[\sum_{\tau=T_{1}}^{t+1}\epsilon_{i}^{(\tau)}\leq\left(t+1-T_{1}\right)\bar{ \mathcal{O}}\left(\eta^{2}d^{\frac{3}{\alpha}\alpha+\frac{13}{4}}\right)\leq \bar{\mathcal{O}}\left(\eta d^{\frac{7}{4}\alpha+\frac{13}{4}}\log\sqrt{\frac{d }{\epsilon}}\right)=\bar{\mathcal{O}}\left(\epsilon\log\sqrt{\frac{d}{ \epsilon}}\right).\]

Lemma 2 tells us that \(\delta_{i}=\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{1}{4}\alpha-1}}\right)\). Substituting these bounds into eq. (18) completes the proof.

### Proof of Lemma 13

The proof in Section D.8 tells us that for \(T_{1}\leq\tau\leq T_{2}\), \(c^{(\tau)}>0,\forall j\in[d]:v_{j}^{(\tau)}>0\), which gives us \(0\leq\frac{\left|R_{2}^{(t+1)T}\bm{u}^{(\tau_{1})}\right|}{c^{(t+1)}\left\|\bm{ u}^{(\tau_{1})}\right\|^{2}}\) and \(0\leq\frac{\left|R_{3j}^{(t+1)}\right|}{c^{(t+1)}\left\|\bm{u}^{(\tau_{1})} \right\|^{2}v_{j}^{(t+1)}}\). By Lemma 12, we have that

\[\forall 1\leq i,j\leq d:\quad 0\leq\frac{\left|R_{1}^{(t+1)}[i,j]\right|}{ \left|u_{i}^{(T_{1})}\right|v_{j}^{(t+1)}}\leq\tilde{\mathcal{O}}(\epsilon_{0 }),\quad 0\leq\frac{\left|R_{2i}^{(t+1)}\right|}{c^{(t+1)}\left|u_{i}^{(T_{1} )}\right|}\leq\tilde{\mathcal{O}}(\epsilon_{0}),\]

which gives us

\[\frac{\left|R_{2}^{(t+1)T}\bm{u}^{(T_{1})}\right|}{c^{(t+1)}\left\|\bm{u}^{(T _{1})}\right\|^{2}}\leq\frac{\sum_{i=1}^{d}\left|u_{i}^{(T_{1})}\right|\left| R_{2i}^{(t+1)}\right|}{c^{(t+1)}\sum_{i=1}^{d}\left|u_{i}^{(T_{1})}\right|^{2}} \leq\frac{\tilde{\mathcal{O}}(\epsilon_{0})c^{(t+1)}\sum_{i=1}^{d}\left|u_{i}^{ (T_{1})}\right|^{2}}{c^{(t+1)}\sum_{i=1}^{d}\left|u_{i}^{(T_{1})}\right|^{2}}= \tilde{\mathcal{O}}(\epsilon_{0}).\]

Lemma 10 tells us that

\[R_{3}^{(t+1)T}=c^{(t+1)}\bm{u}^{(T_{1})T}R_{1}^{(t+1)}+R_{2}^{(t+1)T}R_{1}^{(t +1)}.\]

And we have that

\[\frac{\left|\left(c^{(t+1)}\bm{u}^{(T_{1})T}R_{1}^{(t+1)}\right) _{j}\right|}{c^{(t+1)}\left\|\bm{u}^{(T_{1})}\right\|^{2}v_{j}^{(t+1)}} \leq\frac{c^{(t+1)}\sum_{i=1}^{d}\left|u_{i}^{(T_{1})}\right|\left| R_{1}^{(t+1)}[i,j]\right|}{c^{(t+1)}\sum_{i=1}^{d}\left|u_{i}^{(T_{1})} \right|^{2}v_{j}^{(t+1)}}\] \[\leq\frac{\tilde{\mathcal{O}}(\epsilon_{0})c^{(t+1)}\sum_{i=1}^{d }\left|u_{i}^{(T_{1})}\right|^{2}v_{j}^{(t+1)}}{c^{(t+1)}\sum_{i=1}^{d}\left| u_{i}^{(T_{1})}\right|^{2}v_{j}^{(t+1)}}=\tilde{\mathcal{O}}(\epsilon_{0}),\] \[\frac{\left|\left(R_{2}^{(t+1)T}R_{1}^{(t+1)}\right)_{j}\right|}{ c^{(t+1)}\left\|\bm{u}^{(T_{1})}\right\|^{2}v_{j}^{(t+1)}} \leq\frac{\sum_{i=1}^{d}\left|R_{2i}^{(t+1)}\right|\left|R_{1}^{(t +1)}[i,j]\right|}{c^{(t+1)}\sum_{i=1}^{d}\left|u_{i}^{(T_{1})}\right|^{2}v_{j }^{(t+1)}}\leq\frac{\tilde{\mathcal{O}}\left(\epsilon_{0}^{2}\right)c^{(t+1)} \sum_{i=1}^{d}\left|u_{i}^{(T_{1})}\right|^{2}v_{j}^{(t+1)}}{c^{(t+1)}\sum_{i =1}^{d}\left|u_{i}^{(T_{1})}\right|^{2}v_{j}^{(t+1)}}\] \[=\tilde{\mathcal{O}}\left(\epsilon_{0}^{2}\right).\]

Therefore

\[\frac{\left|R_{3j}^{(t+1)}\right|}{c^{(t+1)}\left\|\bm{u}^{(T_{1})}\right\|^{2 }v_{j}^{(t+1)}}\leq\frac{\left|\left(c^{(t+1)}\bm{u}^{(T_{1})T}R_{1}^{(t+1)} \right)_{j}\right|}{c^{(t+1)}\left\|\bm{u}^{(T_{1})}\right\|^{2}v_{j}^{(t+1)} }+\frac{\left|\left(R_{2}^{(t+1)T}R_{1}^{(t+1)}\right)_{j}\right|}{c^{(t+1)} \left\|\bm{u}^{(T_{1})}\right\|^{2}v_{j}^{(t+1)}}\leq\tilde{\mathcal{O}}( \epsilon_{0}).\]

By Lemma 10,

\[W_{2}^{(t+1)}W_{1}^{(t+1)}=c^{(t+1)}\left\|\bm{u}^{(T_{1})}\right\|^{2}\bm{v}^{ (t+1)T}+R_{2}^{(t+1)T}\bm{u}^{(T_{1})}\bm{v}^{(t+1)T}+R_{3}^{(t+1)T}.\]

Then we have that \(\forall j\in[d]\),

\[\left(W_{2}^{(t+1)}W_{1}^{(t+1)}\right)_{j}=c^{(t+1)}\left\|\bm{u}^{(T_{1})} \right\|^{2}v_{j}^{(t+1)}\left(1+e_{j}^{(t+1)}\right),\quad\text{where }\left|e_{j}^{(t+1)}\right|\leq\tilde{\mathcal{O}}( \epsilon_{0}).\] (24)

Since \(t<T_{2}\), we have \(\forall j\in[d]:\frac{\left(W_{2}^{(t+1)}W_{1}^{(t+1)}\right)_{j}}{A_{j}}= \mathcal{O}(1)\), which yields

\[0\leq\frac{c^{(t+1)}\left\|\bm{u}^{(T_{1})}\right\|^{2}v_{j}^{(t+1)}}{A_{i}} \leq\mathcal{O}(1),\] (25)

which proves eq. (21), since \(\forall j\in[d]:\quad 0\leq\frac{\left|R_{3j}^{(t+1)}\right|}{c^{(t+1)}\left\|\bm{u}^{(T _{1})}\right\|^{2}v_{j}^{(t+1)}}\leq\tilde{\mathcal{O}}(\epsilon_{0})\).

### Proof of Lemma 14

(A) Under the conditions of Lemma 11 and pick \(\eta\leq\mathcal{O}\left(\frac{\epsilon}{d^{\frac{T_{0}}{4}+4}}\right)\), we can apply the technique when proving eq. (24) to show that eq. (24) also holds at time \(t\). Since \(\frac{\left|R_{ij}^{(t)}\right|}{a^{(t)}A_{j}}\leq\tilde{\mathcal{O}}(\epsilon_ {0})\), we get that

\[v_{j}^{(t)}=a^{(t)}A_{j}+R_{vj}^{(t)}=a^{(t)}A_{j}\left(1+e_{vj}^{(t)}\right), \quad\text{where }\left|e_{vj}^{(t)}\right|\leq\tilde{\mathcal{O}}(\epsilon_ {0}).\]

Substituting into the time \(t\) version of eq.(24) yields

\[\forall j\in[d]:\quad\left(W_{2}^{(t)}W_{1}^{(t)}\right)_{j}=a^{(t)}c^{(t)} \left\|\boldsymbol{u}^{(T_{1})}\right\|^{2}A_{j}\left(1+\tilde{e}_{j}^{(t)} \right),\quad\text{where }\left|\tilde{e}_{j}^{(t)}\right|\leq\tilde{ \mathcal{O}}(\epsilon_{0}),\]

That gives us

\[\forall j\in[d]:E_{j}^{(t)}=A_{j}\left(a^{(t)}c^{(t)}\left\|\boldsymbol{u}^{( T_{1})}\right\|^{2}-1+a^{(t)}c^{(t)}\left\|\boldsymbol{u}^{(T_{1})}\right\|^{2} \tilde{e}_{j}^{(t)}\right).\]

Since \(t<T_{2}\), we have \(E_{j}^{(t)}<-\sqrt{\epsilon_{0}}\). Combining with \(A_{j}=\Theta(1)\), gives us \(a^{(t)}c^{(t)}\left\|\boldsymbol{u}^{(T_{1})}\right\|^{2}-1=-\Omega\left(\sqrt {\epsilon_{0}}\right)\). Then we can rewrite \(E_{j}^{(t)}\) as \(\forall j\in[d]\),

\[E_{j}^{(t)} =A_{j}\left(a^{(t)}c^{(t)}\left\|\boldsymbol{u}^{(T_{1})}\right\| ^{2}-1\right)\left(1+\frac{a^{(t)}c^{(t)}\left\|\boldsymbol{u}^{(T_{1})} \right\|^{2}}{a^{(t)}c^{(t)}\left\|\boldsymbol{u}^{(T_{1})}\right\|^{2}-1} \tilde{e}_{j}^{(t)}\right)\] \[:=A_{j}\left(a^{(t)}c^{(t)}\left\|\boldsymbol{u}^{(T_{1})} \right\|^{2}-1\right)\left(1+e_{Ej}^{(t)}\right),\]

where \(\left|e_{Ej}^{(t)}\right|=\tilde{\mathcal{O}}(\sqrt{\epsilon_{0}})\). Hence \(\forall i,j\in[d]:\frac{E_{j}^{(t)}}{E_{j}^{(t)}}=\Theta(1)\).

(B) Note that we assume \(\forall j\in[d]:\quad\frac{v_{j}^{(t)}}{c^{(t)}}=\Theta\left(\frac{1}{\sqrt{d }}\right)\), then we have for \(j\in[d]\),

\[\frac{-E^{(t)}\boldsymbol{v}^{(t)}}{c^{(t)}\left(-E_{j}^{(t)} \right)} =\frac{\sum_{i=1}^{d}\left(-E_{i}^{(t)}\right)v_{i}^{(t)}}{c^{(t) }\left(-E_{j}^{(t)}\right)}=\sum_{i=1}^{d}\frac{E_{i}^{(t)}}{E_{j}^{(t)}}\cdot \frac{v_{i}^{(t)}}{c^{(t)}}=\sum_{i=1}^{d}\Theta\left(\frac{1}{\sqrt{d}} \right)=\Theta\left(\sqrt{d}\right),\] \[\Rightarrow \frac{c^{(t)}\left(-E_{j}^{(t)}\right)}{-E^{(t)}\boldsymbol{v}^{( t)}} =\Theta\left(\frac{1}{\sqrt{d}}\right).\]

Then for \(t+1\), we have that for \(j\in[d]\),

\[\frac{v_{j}^{(t+1)}}{c^{(t+1)}}=\frac{v_{j}^{(t)}+\eta_{t}c^{(t)}\left(-E_{j} ^{(t)}\right)}{c^{(t)}+\eta_{t}\left(-E^{(t)}\right)\boldsymbol{v}^{(t)}}= \Theta\left(\frac{1}{\sqrt{d}}\right).\]

(C) Combining eq. (25) and \(\forall j\in[d]:A_{j}=\Theta(1)\), we know that

\[c^{(t+1)}\left\|\boldsymbol{u}^{(T_{1})}\right\|^{2}v_{j}^{(t+1)}\leq\mathcal{ O}(1),\]

which yields \(\forall j\in[d]\),

\[\left\|\boldsymbol{u}^{(T_{1})}\right\|^{2}\left(v_{j}^{(t+1)}\right)^{2}\leq \frac{v_{j}^{(t+1)}}{c^{(t+1)}}\mathcal{O}(1)=\mathcal{O}\left(\frac{1}{\sqrt{ d}}\right),\left(c^{(t+1)}\right)^{2}\left\|\boldsymbol{u}^{(T_{1})} \right\|^{2}\leq\frac{c^{(t+1)}}{v_{j}^{(t+1)}}\mathcal{O}(1)=\mathcal{O} \left(\sqrt{d}\right).\] (26)

Hence \(\forall i,j\in[d]\),

\[c^{(t+1)}\left|u_{i}^{(T_{1})}\right|=\mathcal{O}\left(d^{1/4} \right)\quad\Rightarrow\left|w_{2i}^{(t+1)}\right|\leq c^{(t+1)}\left|u_{i}^{(T _{1})}\right|+\left|R_{2i}^{(t+1)}\right|\stackrel{{(i)}}{{=}} \mathcal{O}\left(d^{1/4}\right),\] \[\left|u_{i}^{(T_{1})}\right|v_{j}^{(t+1)}=\mathcal{O}\left( \frac{1}{d^{1/4}}\right)\quad\Rightarrow\left|W_{1}^{(t+1)}[i,j]\right|\leq \left|u_{i}^{(T_{1})}\right|v_{j}^{(t+1)}+\left|R_{1}^{(t+1)}[i,j]\right| \stackrel{{(ii)}}{{=}}\mathcal{O}\left(\frac{1}{d^{1/4}}\right),\]where \((i)\) and \((ii)\) use Lemma 12.

(D) The fact that \(\forall j\in[d]:\frac{\left|R_{3j}^{(t+1)}\right|}{A_{j}}\leq\tilde{\mathcal{O}}( \epsilon_{0})\) was already proved in Lemma 13 in eq.(21). To analyze \(\frac{\left|R_{3j}^{(t+1)}\right|}{a^{(t+1)}A_{i}}\), we first prove that \(1-\eta_{t}c^{(t)}d^{(t)}>0\).

It is not hard to prove that eq.(26) also holds for time \(t\). Recall that \(d^{(t)}=c^{(t)}\left\|\boldsymbol{u}^{(T_{1})}\right\|^{2}+R_{2}^{(t)T} \boldsymbol{u}^{(T_{1})}\) and Lemma 13 tells us that \(0\leq\frac{\left|R_{2}^{(t)T}\boldsymbol{u}^{(T_{1})}\right|}{c^{(t)}\left\| \boldsymbol{u}^{(T_{1})}\right\|^{2}}\leq\tilde{\mathcal{O}}(\epsilon_{0})\), then we have

\[c^{(t)}d^{(t)}=\left(c^{(t)}\right)^{2}\left\|\boldsymbol{u}^{(T_{1})} \right\|^{2}+c^{(t)}R_{2}^{(t)T}\boldsymbol{u}^{(T_{1})}\leq\mathcal{O}\left( \sqrt{d}\right).\]

Under the conditions of Lemma 11, and pick \(\eta\leq\mathcal{O}\left(\frac{\epsilon}{d^{\frac{\epsilon_{0}}{d}+4}}\right)\), we have that \(1-\eta_{t}c^{(t)}d^{(t)}\geq 1-\eta c^{(t)}d^{(t)}>0\).

The assumption \(\forall j\in[d]:\frac{\left|R_{3j}^{(t)}\right|}{A_{j}}\leq\tilde{\mathcal{O} }(\epsilon_{0})\) together with \(c^{(t)}>0\) gives us

\[\frac{\eta_{t}c^{(t)}\left|R_{3j}^{(t)}\right|}{\eta_{t}c^{(t)}A_{j}}\leq \tilde{\mathcal{O}}(\epsilon_{0}).\]

Combining with the assumption \(\frac{\left|R_{3j}^{(t)}\right|}{a^{(t)}A_{i}}\leq\tilde{\mathcal{O}}(\epsilon _{0})\) yields

\[\forall i\in[d]:\quad\frac{\left|R_{vi}^{(t+1)}\right|}{a^{(t+1)}A_{i}}\leq \frac{\left(1-\eta_{t}c^{(t)}d^{(t)}\right)\left|R_{v}^{(t)}\right|+\eta_{t}c ^{(t)}\left|R_{3i}^{(t)}\right|}{\left(1-\eta_{t}c^{(t)}d^{(t)}\right)a^{(t)} A_{i}+\eta_{t}c^{(t)}A_{i}}\leq\tilde{\mathcal{O}}(\epsilon_{0}).\]

## Appendix E Analysis of Adam

Note that \(A=\frac{1}{m}YX^{T}\), \(\Lambda_{xx}:=\frac{1}{m}XX^{T}\). Denote \(g_{k}^{(t)}:=\nabla_{W_{k}}L(W^{(t)}),k=1,2\). We have that

\[g_{1}^{(t)}=W_{2}^{(t)T}\left(W_{2}^{(t)}W_{1}^{(t)}-A\right),\quad g_{2}^{(t )}=\left(W_{2}^{(t)}W_{1}^{(t)}-A\right)W_{1}^{(t)T}.\]

Let \(\tilde{A}^{(t)}\), \(\tilde{\lambda}_{xx}^{(t)}\) and \(\tilde{g}_{k}^{(t)},k=1,2\) be the corresponding batch versions at time \(t\). Let \(E^{(t)}:=W_{2}^{(t)}W_{1}^{(t)}-A\), and denote \(E_{j}^{(t)}\) as the \(j\)-th component of \(E^{(t)}\). We also denote \(\Delta w_{2i}^{(t)}:=w_{2i}^{(t+1)}-w_{2i}^{(t)}\), \(\Delta W_{1}^{(t)}[i,j]:=W_{1}^{(t+1)}[i,j]-W_{1}^{(t)}[i,j]\). By eq. (2), the update equations of Adam are given by

\[\eta_{t}=\eta\cdot\frac{\sqrt{1-\beta_{2}^{t+1}}}{1-\beta_{1}^{t +1}},\quad g_{1}^{(t)}[i,j]=w_{2i}^{(t)}E_{j}^{(t)},\quad g_{2i}^{(t)}=\left<E ^{(t)},W_{1}^{(t)}[i,:]\right>,\] \[W_{1}^{(t+1)}[i,j]-W_{1}^{(t)}[i,j] =-\eta_{t}\frac{m_{1}^{(t)}[i,j]}{\sqrt{v_{1}^{(t)}[i,j]}}=-\eta _{t}\frac{(1-\beta_{1})\sum_{\tau=0}^{t}\beta_{1}^{t-\tau}\tilde{g}_{1}^{( \tau)}[i,j]}{\sqrt{(1-\beta_{2})\sum_{\tau=0}^{t}\beta_{2}^{t-\tau}\left( \tilde{g}_{1}^{(\tau)}[i,j]\right)^{2}+\xi}}\] \[=-\eta_{t}\frac{(1-\beta_{1})\sum_{\tau=0}^{t}\beta_{1}^{t-\tau}g _{1}^{(\tau)}[i,j]+r_{1n}^{(t)}[i,j]}{\sqrt{(1-\beta_{2})\sum_{\tau=0}^{t} \beta_{2}^{t-\tau}\left(g_{1}^{(\tau)}[i,j]\right)^{2}+r_{1n}^{(t)}[i,j]}},\] \[w_{2i}^{(t+1)}-w_{2i}^{(t)} =-\eta_{t}\frac{m_{2i}^{(t)}}{\sqrt{v_{2i}^{(t)}}}=-\eta_{t}\frac{ (1-\beta_{1})\sum_{\tau=0}^{t}\beta_{1}^{t-\tau}\tilde{g}_{2i}^{(\tau)}}{\sqrt{ (1-\beta_{2})\sum_{\tau=0}^{t}\beta_{2}^{t-\tau}\left(\tilde{g}_{2i}^{(\tau)} \right)^{2}+\xi}}\] \[=-\eta_{t}\frac{(1-\beta_{1})\sum_{\tau=0}^{t}\beta_{1}^{t-\tau}g _{2i}^{(\tau)}+r_{2n,i}^{(t)}}{\sqrt{(1-\beta_{2})\sum_{\tau=0}^{t}\beta_{2}^{ t-\tau}\left(g_{2i}^{(\tau)}\right)^{2}++r_{2d,i}^{(t)}}+\xi}.\]where \(Dg_{1}^{(t)}:=\tilde{g}_{1}^{(t)}-g_{1}^{(t)}\) and \(Dg_{2}^{(t)}:=\tilde{g}_{2}^{(t)}-g_{2}^{(t)}\), and

\[\begin{split}& r_{1n}^{(t)}[i,j]:=(1-\beta_{1})\sum_{\tau=0}^{t} \beta_{1}^{t-\tau}Dg_{1}^{(\tau)}[i,j],\\ & r_{1d}^{(t)}[i,j]=(1-\beta_{2})\sum_{\tau=0}^{t}\beta_{2}^{t- \tau}\left(2g_{1}^{(\tau)}[i,j]Dg_{1}^{(\tau)}[i,j]+\left(Dg_{1}^{(\tau)}[i,j] \right)^{2}\right),\\ & r_{2n,i}^{(t)}:=(1-\beta_{1})\sum_{\tau=0}^{t}\beta_{1}^{t-\tau }Dg_{2i}^{(\tau)},\\ & r_{2d,i}^{(t)}=(1-\beta_{2})\sum_{\tau=0}^{t}\beta_{2}^{t-\tau }\left(2g_{2i}^{(\tau)}Dg_{2i}^{(\tau)}+\left(Dg_{2i}^{(\tau)}\right)^{2} \right).\end{split}\] (28)

Denote the \(i\)-th coordinate of \(W_{2}W_{1}\) and \(A\) as \((W_{2}W_{1})_{i}\) and \(A_{i}\), respectively. By Assumption 2 and the assumption that \(\forall i\in[d]:A_{i}>0,A_{i}=\Omega(1)\), at the beginning, w.h.p., \(\forall i\in[d]:(W_{2}W_{1})_{i}-A_{i}<0\). Based on this, we divide the training procedure into two phases (note that these two phases are different from those of SGD+M).

1. **First phase**: when the error \((W_{2}W_{1})_{i}-A_{i}\) is negative and its absolute value is big for all \(i\in[d]\).
2. **Second phase**: when \((W_{2}W_{1})_{i}-A_{i}\) is close to zero for some coordinate \(i\in[d]\).

More formally, we define the boundary between the two phases below.

**Definition 4** (End of the first phase).: _The end of the first phase (denoted as \(T_{1}\)) is defined as \(T_{1}=\inf\left\{t>0:\exists i\in[d]:E_{i}^{(t)}\geq-\sqrt{\eta d}\right\}\)._

In the second phase, we define some time points.

**Definition 5**.: _Define \(T_{g}:=\inf\left\{t>T_{1}:\exists i\in[d]:\left|g_{2i}^{(t)}\right|\leq d\sqrt {\eta}\right\}\)._

For \(t<T_{1}\), we have \(\forall i\in[d]:E_{i}^{(t)}<0\) by Definition 4. For \(t>T_{1}\), some \(E_{i}^{(t)}\) may flip the sign and become positive. For certain coordinate \(i\), we define the following "flip time".

**Definition 6**.: _Define \(T_{f,i}:=\inf\left\{t>T_{1}:E_{i}^{(t)}\geq-\sqrt{\eta d}\right\}\). Define \(T_{f}:=\max_{i}T_{f,i}\) as the largest 'flip time" over all \(i\in[d]\), i.e. the "flip time" of the last \(E_{i}\) which flips the sign. Moreover, denote \(\tilde{T}:=\min\left\{T_{g},T_{f}\right\}\)._

We can first show that after a few steps in the first phase, \(W_{1}\) will become an approximately rank-1 matrix, as described in the following lemma.

**Lemma 15**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{13/4}}\). By picking \(\eta\leq\mathcal{O}\left(\frac{1}{d^{3\alpha}}\right),\xi\leq\sqrt{\frac{\eta }{d^{3\alpha-1}}}\), and \(\beta_{2}=\beta_{1}^{2}\), there exists \(t_{\text{inc}}>0\) such that w.h.p. for \(t_{\text{inc}}\leq t<T_{1}\),_

\[\forall i,j\in[d]:\quad w_{2i}^{(t)}=\text{sign}\left(w_{2i}^{(0)}\right)\eta \left(t-t_{\text{inc}}\right)+R_{2i}^{(t)},\]

\[W_{1}^{(t)}[i,j]=\text{sign}\left(w_{2i}^{(0)}\right)\eta\left(t-t_{\text{inc }}\right)+R_{1}^{(t)}[i,j],\]

_where \(\frac{\left|R_{1}^{(t)}[i,j]\right|}{\eta\left(t-t_{\text{inc}}\right)}= \tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{\eta\left(t-t_{\text{inc}}\right) d^{\alpha}}\right),\quad\frac{\left|R_{2i}^{(t)}\right|}{\eta\left(t-t_{\text{ inc}}\right)}=\tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{\eta\left(t-t_{ \text{inc}}\right)d^{\alpha}}\right)\)._

_Specially, when \(t=T_{1}\), we have that_

\[\forall i,j\in[d]:\quad w_{2i}^{(T_{1})}=\text{sign}\left(w_{2i}^{(0)}\right) \eta\left(T_{1}-t_{\text{inc}}\right)+R_{2i}^{(T_{1})},\]

\[W_{1}^{(T_{1})}[i,j]=\text{sign}\left(w_{2i}^{(0)}\right)\eta(T_{1}-t_{\text{ inc}})+R_{1}^{(T_{1})}[i,j],\]

_where \(\eta\left(T_{1}-t_{\text{inc}}\right)=\Theta\left(\frac{1}{\sqrt{d}}\right)\) and_

\[\frac{\left|R_{1}^{(T_{1})}[i,j]\right|}{\eta\left(T_{1}-t_{\text{inc}}\right)}= \tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-\frac{1}{2}}}\right), \quad\frac{\left|R_{2i}^{(T_{1})}\right|}{\eta\left(T_{1}-t_{\text{inc}}\right) }=\tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-\frac{1}{2}}}\right).\]The following lemma tells us that this approximate rank-1 structure is preserved when \(T_{1}\leq t\leq\tilde{T}\).

**Lemma 16**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{13/4}}\). By picking \(\eta\leq\mathcal{O}\left(\frac{1}{d^{\alpha\alpha}}\right),\xi\leq\sqrt{\frac{ \eta}{d^{\alpha\alpha-1}}}\), and \(\beta_{2}=\beta_{1}^{2}\), we have w.h.p. for \(T_{1}\leq t<\tilde{T}\),_

\[\forall i,j\in[d]:\quad w_{2i}^{(t)} =\text{sign}\left(w_{2i}^{(0)}\right)c^{(t)}+R_{2i}^{(t)},\] \[W_{1}^{(t)}[i,j] =\text{sign}\left(w_{2i}^{(0)}\right)V_{j}^{(t)}+R_{1}^{(t)}[i,j],\] \[\text{where}\quad\quad\frac{\left|R_{2i}^{(t)}\right|}{\left|c^{( t)}\right|} =\tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-1/2}}\right),\quad\frac {\left|R_{1}^{(t)}[i,j]\right|}{\left|V_{j}^{(t)}\right|}\leq\tilde{\mathcal{O }}\left(\eta^{\frac{1}{4}}+\frac{1}{d^{\frac{\alpha}{2}-\frac{1}{4}}}\right),\]

_and that \(L\left(W^{(\tilde{T})}\right)\leq\tilde{\mathcal{O}}\left(\eta d^{4}\right)\)._

Now we are ready to prove the Adam part of Theorem 1.

### Proof of the Adam part of Theorem 1

Define \(T_{\text{Adam},1}=t_{\text{inc}}+\frac{1}{\eta d^{\frac{\alpha}{2}}}\). Note that this choice of \(T_{\text{Adam},1}\) gives \(\eta\left(T_{\text{Adam},1}-t_{\text{inc}}\right)=\frac{1}{d^{\frac{\alpha}{2 }}}\). By picking \(\eta\leq\mathcal{O}\left(\frac{1}{d^{\alpha\alpha}}\right),\xi\leq\sqrt{\frac{ \eta}{d^{3\alpha-1}}}\) and \(\beta_{2}=\beta_{1}^{2}\), we can apply Lemma 15 to get that \(\forall i,j\in[d]:\left|w_{2i}^{(T_{\text{Adam},1})}\right|=\Theta\left(\frac{1 }{d^{\frac{\alpha}{2}}}\right),\left|W_{1}^{(T_{\text{Adam},1})}[i,j]\right|= \Theta\left(\frac{1}{d^{\frac{\alpha}{2}}}\right)\), and therefore \(\forall i\in[d]:E_{i}^{(T_{\text{Adam},1})}=-\Theta(1)\) and \(L\left(W^{(T_{\text{Adam},1})}\right)=\Theta(d)\). Define \(T_{\text{Adam},2}=\tilde{T}\). By Lemma 16, we have \(L\left(W^{(T_{\text{Adam},2})}\right)=\tilde{\mathcal{O}}\left(\eta d^{4}\right)\). For any \(p>0\), by picking \(\alpha\geq\frac{p+4}{3}\), we have \(L\left(W^{(T_{\text{Adam},2})}\right)=\tilde{\mathcal{O}}\left(\eta d^{4} \right)\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{p}}\right)\).

Moreover, combining Lemma 15 and 16, we get that when \(t\in[T_{\text{Adam},1},T_{\text{Adam},2}]\), the conditions in Lemma 31 are satisfied with \(\delta=\tilde{\mathcal{O}}\left(\eta^{\frac{1}{4}}+\frac{1}{d^{\frac{\alpha}{ 2}}-\frac{1}{4}}\right)\). The \(i\)-th component of the \(\bm{u}\) vector (denoted as \(u_{i}\)) is \(\text{sign}\left(w_{2i}^{(0)}\right)\). That means \(\forall i\in[d]:u_{i}^{2}=1\) and \(\frac{\max_{i}(u_{i})^{2}}{\text{median}(u_{i})^{2}}=1\). Then we can apply Lemma 31 and get that

\[R_{\text{med},1}^{\text{Adam}}(t),R_{\text{med},2}^{\text{Adam}}(t) \in\left[\left(\frac{1-\delta}{1+\delta}\right)^{2}\frac{\max_{i} (u_{i})^{2}}{\text{median}(u_{i})^{2}},\left(\frac{1+\delta}{1-\delta}\right)^ {2}\frac{\max_{i}(u_{i})^{2}}{\text{median}(u_{i})^{2}}\right]\] \[=\left[\left(\frac{1-\delta}{1+\delta}\right)^{2},\left(\frac{1+ \delta}{1-\delta}\right)^{2}\right].\]

Note that by definition, \(R_{\text{med},1}^{\text{Adam}}(t)\) and \(R_{\text{med},2}^{\text{Adam}}(t)\) are always larger than or equal to 1, then we have

\[R_{\text{med},1}^{\text{Adam}}(t),R_{\text{med},2}^{\text{Adam}}(t)=1+\mathcal{ O}(\delta)=1+\tilde{\mathcal{O}}\left(\eta^{\frac{1}{4}}+\frac{1}{d^{\frac{ \alpha}{2}-\frac{1}{4}}}\right).\]

### Proof of Lemma 15

For some time \(t\), we introduce two conditions.

**Condition 1**.: \[\forall\tau\in[H]:\text{sign}\left(g_{1}^{(t-\tau)}[i,j]\right)=s_{1}^{(t)}[i,j ],\quad(1-\beta_{1})\left|\sum_{\tau=0}^{H}\beta_{1}^{(\tau)}g_{1}^{(t-\tau)} [i,j]\right|\geq\Omega(\xi).\]

**Condition 2**.: \[\forall\tau\in[H]:\text{sign}\left(g_{2i}^{(t-\tau)}\right)=s_{2i}^{(t)}, \quad(1-\beta_{1})\left|\sum_{\tau=0}^{H}\beta_{1}^{(\tau)}g_{2i}^{(t-\tau)} \right|\geq\Omega(\xi).\]Next prove that, under Assumption 1 and 2, by picking \(\eta\leq\mathcal{O}\left(\frac{1}{d^{3\alpha}}\right),\xi\leq\sqrt{\frac{\eta}{d^{3 \alpha-t}}}\), and \(\beta_{2}=\beta_{1}^{2}\), there exists \(t_{\text{inc}}>0\) such that for \(t_{\text{inc}}\leq t<T_{1}\), the update of Adam can be approximated as that of signed descent.

\[\begin{split} W_{1}^{(t+1)}[i,j]&=W_{1}^{(t)}[i,j]- \eta\left(\text{sign}\left(g_{1}^{(t)}[i,j]\right)+e_{1}^{(t)}[i,j]\right),\\ w_{2i}^{(t+1)}&=w_{2i}^{(t)}-\eta\left(\text{ sign}\left(g_{2i}^{(t)}\right)+e_{2i}^{(t)}\right),\end{split}\] (29)

where \(\left|e_{1}^{(t)}[i,j]\right|=\tilde{\mathcal{O}}\left(\sqrt{\eta}\right), \quad\left|e_{2i}^{(t)}\right|=\tilde{\mathcal{O}}\left(\sqrt{\eta}\right)\).

Before we dive into the proof, let's introduce some useful lemmas.

The following lemma reflects our key idea: converting the exponential average in Adam to a finite-step average, and trying to bound the stochastic error terms in eq. (28).

**Lemma 17**.: _Under Assumption 1, 2 and 3 and pick \(\beta_{2}=\beta_{1}^{2}\). Let \(M_{1}^{(t)}:=\max_{i,j\in[d],\tau\leq t}\left|W_{1}^{(\tau)}[i,j]\right|\), \(M_{2}^{(t)}:=\max_{i,j\in[d],\tau\leq t}\left|w_{2i}^{(\tau)}\right|\), \(G_{1}^{(t)}:=\max_{i,j\in[d],\tau\leq t}\left|g_{1}^{(\tau)}[i,j]\right|\) and \(G_{2}^{(t)}:=\max_{i,j\in[d],\tau\leq t}\left|g_{2i}^{(\tau)}\right|\). We have that w.h.p., for all \(t\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d\eta}}\right)\) and \(\forall i,j\in[d]\),_

\[\Delta W_{1}^{(t)}[i,j]=-\eta_{t}\frac{(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1 }^{\tau}g_{1}^{(t-\tau)}[i,j]+\epsilon_{1n}^{(t)}[i,j]}{\sqrt{(1-\beta_{2}) \sum_{\tau=0}^{H}\beta_{2}^{\tau}\left(g_{1}^{(t-\tau)}[i,j]\right)^{2}+ \epsilon_{1d}^{(t)}[i,j]+\xi},\]

\[\Delta w_{2i}^{(t)}=-\eta_{t}\frac{(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{ \tau}g_{2i}^{(t-\tau)}+\epsilon_{2n,i}^{(t)}}{\sqrt{(1-\beta_{2})\sum_{\tau= 0}^{H}\beta_{2}^{\tau}\left(g_{2i}^{(t-\tau)}\right)^{2}+\epsilon_{2d,i}^{(t) }}+\xi},\]

_where \(H\geq\frac{1}{1-\beta_{1}}\log\frac{\max\left\{G_{1}^{(t)},G_{2}^{(t)}\left(G_ {1}^{(t)}\right)^{2},\left(G_{2}^{(t)}\right)^{2}\right\}}{\eta\xi^{2}}\) and_

\[\left|\epsilon_{1n}^{(t)}[i,j]\right|\leq\mathcal{O}(\eta\xi^{2})+\mathcal{O} \left(D_{1}^{(t)}\right),\quad\left|\epsilon_{1d}^{(t)}[i,j]\right|\leq \mathcal{O}(\eta\xi^{2})+\mathcal{O}\left(D_{1}^{(t)}G_{1}^{(t)}+\left(D_{1} ^{(t)}\right)^{2}\right),\]

\[\left|\epsilon_{2n,i}^{(t)}\right|\leq\mathcal{O}(\eta\xi^{2})+\mathcal{O} \left(D_{2}^{(t)}\right),\quad\left|\epsilon_{2d,i}^{(t)}\right|\leq\mathcal{O }(\eta\xi^{2})+\mathcal{O}\left(D_{2}^{(t)}G_{2}^{(t)}+\left(D_{2}^{(t)} \right)^{2}\right),\]

_with_

\[D_{1}^{(t)} \leq\tilde{\mathcal{O}}\left(d^{3}M_{1}^{(t)}\left(M_{2}^{(t)} \right)^{2}\sigma\sqrt{\frac{d^{1/2}}{\eta}}\right)+\tilde{\mathcal{O}}\left(M _{2}^{(t)}\sigma\sqrt{\frac{d^{3/2}}{\eta}}\right),\] \[D_{2}^{(t)} \leq\tilde{\mathcal{O}}\left(d^{4}\left(M_{1}^{(t)}\right)^{2}M _{2}^{(t)}\sigma\sqrt{\frac{d^{1/2}}{\eta}}\right)+\tilde{\mathcal{O}}\left( dM_{1}^{(t)}\sigma\sqrt{\frac{d^{3/2}}{\eta}}\right).\]

**Corollary 2**.: _Under the conditions of Lemma 17 and suppose \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{13/4}}\). Consider any \(t\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d\eta}}\right)\). If \(M_{1}^{(t)},M_{2}^{(t)}\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\), \(G_{1}^{(t)}\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right),G_{2}^{(t)} \leq\tilde{\mathcal{O}}\left(\sqrt{d}\right)\), then \(H\) in Lemma 17 can be picked as \(\frac{1}{1-\beta_{1}}\log\frac{d}{\eta\xi^{2}}\) and we can get that \(\forall i,j\in[d]\), \(\left|\epsilon_{1n}^{(t)}[i,j]\right|,\left|\epsilon_{1d}^{(t)}[i,j]\right|, \left|\epsilon_{2n,i}^{(t)}\right|,\left|\epsilon_{2d,i}^{(t)}\right|\leq \tilde{\mathcal{O}}(\eta\xi^{2})\)._

The following lemma analyzes the magnitude of weights during a short period at the beginning.

**Lemma 18**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{13/4}}\). Pick \(\xi\leq\frac{1}{d^{\frac{1}{2}\alpha}}\), then there exists some time point \(t_{\text{inc}}\in(H,T_{1})\), such that w.h.p., for \(t\leq t_{\text{inc}}\), for every \(i,j\in[d]\),_

\[\left|\Delta W_{1}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}(\eta), \left|\Delta w_{2i}^{(t)}\right|\leq\tilde{\mathcal{O}}(\eta),\] \[\left|W_{1}^{(t)}[i,j]\right|\leq\mathcal{O}\left(\frac{1}{d^{ \frac{3}{2}\alpha+1}}\right),\Omega\left(\frac{1}{d^{\frac{3}{2}\alpha}}\right) \leq\left|w_{2i}^{(t)}\right|\leq\mathcal{O}\left(\frac{1}{d^{\alpha}}\right).\]_Specifically, when \(t=t_{\text{inc}}\), we have \(\text{sign}\left(w_{2i}^{(t_{\text{inc}})}\right)=\text{sign}\left(W_{1}^{(t_{ \text{inc}})}[i,j]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\), \(\left|W_{1}^{(t_{\text{inc}})}[i,j]\right|=\Theta\left(\frac{1}{d^{\frac{3}{2} \alpha+1}}\right)\) and \(\left|g_{1}^{(t_{\text{inc}})}[i,j]\right|\geq\Omega(\xi),\left|g_{2i}^{(t_{ \text{inc}})}\right|\geq\Omega(\xi)\). Moreover, Condition 1 and 2 are satisfied for \(t=t_{\text{inc}}\). The \(s_{1}^{(t)}[i,j]\) and \(s_{2i}^{(t)}\) in the conditions are both \(-\text{sign}\left(w_{2i}^{(0)}\right)\)._

The following lemma gives us lower bounds of \(\left|g_{1}^{(t)}[i,j]\right|\) and \(\left|g_{2i}^{(t)}\right|\).

**Lemma 19**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d13/4}\). Pick \(\xi\leq\sqrt{\frac{\eta}{d^{3\alpha-1}}}\), \(\eta\leq\mathcal{O}\left(\frac{1}{d^{3\alpha}}\right)\). Consider \(t_{\text{inc}}\) in Lemma 18. We have w.h.p. for any \(t\in[t_{\text{inc}},T_{1})\), and for \(\forall i,j\in[d]\), \(\text{sign}\left(\Delta W_{1}^{(t)}[i,j]\right)=\text{sign}\left(\Delta w_{2 i}^{(t)}\right)=\text{sign}\left(w_{2i}^{(0)}\right)\) and that \(\forall i,j\in[d]:\left|g_{1}^{(t)}[i,j]\right|\geq\tilde{\Omega}\left(\sqrt{ \eta}\right),\left|g_{2i}^{(t)}\right|\geq\tilde{\Omega}\left(\sqrt{\eta}d\right)\). Moreover, we have \(\forall\tau\leq t\), \(\forall i,j\in[d]:\left|W_{1}^{(\tau)}[i,j]\right|\leq\tilde{\mathcal{O}} \left(\frac{1}{\sqrt{d}}\right),\left|w_{2i}^{(\tau)}\right|\leq\tilde{ \mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\) and \(\left|g_{1}^{(\tau)}[i,j]\right|\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d }}\right),\left|g_{2i}^{(\tau)}\right|\leq\tilde{\mathcal{O}}\left(\sqrt{d}\right)\)._

The following lemma shows that when \(t_{\text{inc}}\leq t<T_{1}\), we have \(\forall i,j\in[d]:\left|g_{2i}^{(t)}\right|\gg\left|g_{2i}^{(t)}-g_{2i}^{(t-1) }\right|\) and that \(\left|g_{1}^{(t)}[i,j]\right|\gg\left|g_{1}^{(t)}[i,j]-g_{1}^{(t-1)}[i,j]\right|\).

**Lemma 20**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{13/4}}\). Pick \(\xi\leq\sqrt{\frac{\eta}{d^{3\alpha-1}}}\), \(\eta\leq\mathcal{O}\left(\frac{1}{d^{3\alpha}}\right)\). For \(t_{\text{inc}}\) in Lemma 18, we have that w.h.p. for \(t_{\text{inc}}\leq t<T_{1}\) and \(\tau\leq t\), \(\forall i,j\in[d]\),_

\[\frac{\left|g_{1}^{(t)}[i,j]-g_{1}^{(t-\tau)}[i,j]\right|}{\left|g_{1}^{(t)}[ i,j]\right|}=\tilde{\mathcal{O}}\left(\sqrt{\eta}\tau\right),\quad\frac{\left|g_{2i}^{ (t)}-g_{2i}^{(t-\tau)}\right|}{\left|g_{2i}^{(t)}\right|}=\tilde{\mathcal{O}} \left(\sqrt{\eta}\tau\right),\] (30)

\[\frac{\left|\left(g_{1}^{(t)}[i,j]\right)^{2}-\left(g_{1}^{(t- \tau)}[i,j]\right)^{2}\right|}{\left(g_{1}^{(t)}[i,j]\right)^{2}}=\tilde{ \mathcal{O}}\left(\sqrt{\eta}\tau\right)+\tilde{\mathcal{O}}\left(\eta\tau^{2} \right),\] (31) \[\frac{\left|\left(g_{2i}^{(t)}\right)^{2}-\left(g_{2i}^{(t-\tau)} \right)^{2}\right|}{\left(g_{2i}^{(t)}\right)^{2}}=\tilde{\mathcal{O}}\left( \sqrt{\eta}\tau\right)+\tilde{\mathcal{O}}\left(\eta\tau^{2}\right).\]

Equipped with these lemmas, now let's prove eq. (29).

For any \(t\in[t_{\text{inc}},T_{1})\), by Lemma 19, we know that \(M_{1}^{(t)},M_{2}^{(t)}\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\), and that \(G_{1}^{(t)}\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right),G_{2}^{(t)} \leq\tilde{\mathcal{O}}\left(\sqrt{d}\right)\). At the end of the proof for this lemma, we will show that \(T_{1}=\Theta\left(\frac{1}{\sqrt{d_{\eta}}}\right)\). Then we can pick \(H:=\frac{1}{1-\beta_{1}}\log\frac{d}{\eta\xi^{2}}\) and apply Lemma 17 and Corollary 2 to get that, w.h.p., for all \(t\in[t_{\text{inc}},T_{1})\) and \(\forall i,j\in[d]\), eq. (27) can be written as

\[\Delta W_{1}^{(t)}[i,j] =-\eta_{t}\frac{(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}g_{ 1}^{(t-\tau)}[i,j]+\epsilon_{1n}^{(t)}[i,j]}{\sqrt{(1-\beta_{2})\sum_{\tau=0}^{ H}\beta_{2}^{\tau}\left(g_{1}^{(t-\tau)}[i,j]\right)^{2}+\epsilon_{1d}^{(t)}[i,j]+ \xi}},\] (32) \[\Delta w_{2i}^{(t)} =-\eta_{t}\frac{(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}g_{ 2i}^{(t-\tau)}+\epsilon_{2n,i}^{(t)}}{\sqrt{(1-\beta_{2})\sum_{\tau=0}^{H}\beta_{2}^ {\tau}\left(g_{2i}^{(t-\tau)}\right)^{2}+\epsilon_{2d,i}^{(t)}+\xi}},\]

where \(\forall i,j\in[d]\), \(\left|\epsilon_{1n}^{(t)}[i,j]\right|,\left|\epsilon_{1d}^{(t)}[i,j]\right|, \left|\epsilon_{2n,i}^{(t)}\right|,\left|\epsilon_{2d,i}^{(t)}\right|\leq \tilde{\mathcal{O}}(\eta\xi^{2})\).

Let's first look at the update of \(W_{1}^{(t)}[i,j]\). For \(t\) in the first phase, we write the RHS of eq. (32) as

\[\frac{(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}g_{1}^{(t-\tau )}[i,j]+\epsilon_{1n}^{(t)}[i,j]}{\sqrt{(1-\beta_{2})\sum_{\tau=0}^{H}\beta_{2}^ {\tau}\left(g_{1}^{(t-\tau)}[i,j]\right)^{2}+\epsilon_{1d}^{(t)}[i,j]}+\xi}\] \[= \frac{(1-\beta_{1})g_{1}^{(t)}[i,j]\sum_{\tau=0}^{H}\beta_{1}^{ \tau}+(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}\left(g_{1}^{(t-\tau)}[i,j] -g_{1}^{(t)}[i,j]\right)+\epsilon_{1n}^{(t)}[i,j]}{\sqrt{(1-\beta_{2})\left(g_ {1}^{(t)}[i,j]\right)^{2}\sum_{\tau=0}^{H}\beta_{2}^{\tau}+(1-\beta_{2})\sum_{ \tau=0}^{H}\beta_{2}^{\tau}\left(\left(g_{1}^{(t-\tau)}[i,j]\right)^{2}-\left( g_{1}^{(t)}[i,j]\right)^{2}\right)+\epsilon_{1d}^{(t)}[i,j]+\xi}}\] \[:= \frac{g_{1}^{(t)}[i,j](1-\beta_{1}^{H+1})+\epsilon_{1n}^{(t)}[i,j ]+\epsilon_{1n}^{(t)}[i,j]}{\sqrt{\left(g_{1}^{(t)}[i,j]\right)^{2}(1-\beta_{ 2}^{H+1})+\epsilon_{1d}^{(t)}[i,j]+\epsilon_{1d}^{(t)}[i,j]+\xi}},\]

where

\[\begin{split}& e_{1n}^{(t)}[i,j]:=(1-\beta_{1})\sum_{\tau=0}^{H} \beta_{1}^{\tau}\left(g_{1}^{(t-\tau)}[i,j]-g_{1}^{(t)}[i,j]\right),\\ & e_{1d}^{(t)}[i,j]:=(1-\beta_{2})\sum_{\tau=0}^{H}\beta_{2}^{ \tau}\left(\left(g_{1}^{(t-\tau)}[i,j]\right)^{2}-\left(g_{1}^{(t)}[i,j] \right)^{2}\right).\end{split}\]

We have already shown that \(\left|\epsilon_{1n}^{(t)}[i,j]\right|,\left|\epsilon_{1d}^{(t)}[i,j]\right| \leq\tilde{\mathcal{O}}(\eta\xi^{2})\). By Lemma 20, we have that \(\forall i,j\in[d]\),

\[\begin{split}\left|e_{1n}^{(t)}[i,j]\right|&\leq (1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}\left|g_{1}^{(t-\tau)}[i,j]-g_{1 }^{(t)}[i,j]\right|\\ &\leq\left|g_{1}^{(t)}[i,j]\right|\tilde{\mathcal{O}}\left(\sqrt {\eta}\right)(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}\tau=\left|g_{1}^ {(t)}[i,j]\right|\tilde{\mathcal{O}}\left(\sqrt{\eta}\right).\end{split}\]

Similarly, we have \(\forall i,j\in[d]\),

\[\begin{split}\left|e_{1d}^{(t)}[i,j]\right|&\leq \left(g_{1}^{(t)}[i,j]\right)^{2}\tilde{\mathcal{O}}\left(\sqrt{\eta}\right)(1 -\beta_{2})\sum_{\tau=0}^{H}\beta_{1}^{\tau}\tau+\left(g_{1}^{(t)}[i,j]\right) ^{2}\tilde{\mathcal{O}}\left(\sqrt{\eta}\right)(1-\beta_{2})\sum_{\tau=0}^{H} \beta_{1}^{\tau}\tau^{2}\\ &=\left(g_{1}^{(t)}[i,j]\right)^{2}\tilde{\mathcal{O}}\left( \sqrt{\eta}\right).\end{split}\]

By Lemma 19, we know that \(\left|g_{1}^{(t)}[i,j]\right|=\Omega\left(\sqrt{\eta}\right)\). Then we have that

\[\forall i,j\in[d]:\quad\left|\epsilon_{1n}^{(t)}[i,j]\right|\leq\tilde{ \mathcal{O}}(\eta\xi^{2})\leq\tilde{\mathcal{O}}\left(\sqrt{\eta}\right)\left| g_{1}^{(t)}[i,j]\right|,\quad\left|\epsilon_{1d}^{(t)}[i,j]\right|\leq\tilde{ \mathcal{O}}\left(\sqrt{\eta}\right)\xi^{2}.\]

Therefore by Lemma 34 in Appendix H, we have

\[\frac{g_{1}^{(t)}[i,j]\left(1-\beta_{1}^{H+1}\right)+\epsilon_{1 n}^{(t)}[i,j]+\epsilon_{1n}^{(t)}[i,j]}{\sqrt{\left(g_{1}^{(t)}[i,j]\right)^{2} \left(1-\beta_{2}^{H+1}\right)+\epsilon_{1d}^{(t)}[i,j]+\epsilon_{1d}^{(t)}[ i,j]+\xi}}\] \[= \frac{1-\beta_{1}^{H+1}}{\sqrt{1-\beta_{2}^{H+1}}}\left(\text{ sign}\left(g_{1}^{(t)}[i,j]\right)+\tilde{\epsilon}_{1}^{(t)}[i,j]\right),\]

where \(\left|\tilde{e}_{1}^{(t)}[i,j]\right|=\tilde{\mathcal{O}}\left(\sqrt{\eta}\right)\).

Since \(\beta\in(0,1)\), we know that \(\log\beta\leq\beta-1<0\). Then our choice of \(H\) gives us \(H=\frac{1}{1-\beta_{1}}\log\frac{d}{\eta\xi^{2}}\geq\frac{\log\frac{\eta\xi^{2}} {\beta}}{\log\beta_{1}}\) and \(H>\frac{1}{1-\beta_{2}}\log\frac{d}{\eta\xi^{2}}\geq\frac{\log\frac{\eta\xi^{2}} {\log\beta_{2}}}{\log\beta_{2}}\), which implies that \(\beta_{1}^{H},\beta_{2}^{H}\leq\eta\xi^{2}/d\). Hence for \(t\geq t_{\text{inc}}>H\), \(\eta_{t}\frac{1-\beta_{1}^{H+1}}{\sqrt{1-\beta_{2}^{H+1}}}=\eta\frac{\sqrt{1- \beta_{2}^{H+1}}}{\sqrt{1-\beta_{2}^{H+1}}}\frac{1-\beta_{1}^{H+1}}{1-\beta_{1} ^{G^{\prime}+1}}=\eta(1\pm\mathcal{O}(\eta))\).

Combining all of the above yields that

\[W_{1}^{(t+1)}[i,j] =W_{1}^{(t)}[i,j]-\eta_{t}\frac{(1-\beta_{1})\sum_{\tau=0}^{t}\beta_ {1}^{\tau}g_{1}^{(t-\tau)}[i,j]}{\sqrt{(1-\beta_{2})\sum_{\tau=0}^{t}\beta_{2}^{ \tau}\left(g_{1}^{(t-\tau)}[i,j]\right)^{2}+\xi}}\] \[=W_{1}^{(t)}[i,j]-\eta_{t}\frac{1-\beta_{1}^{H+1}}{\sqrt{1-\beta_ {2}^{H+1}}}\left(\text{sign}\left(g_{1}^{(t)}[i,j]\right)+\tilde{e}_{1}^{(t)}[ i,j]\right)\] \[=W_{1}^{(t)}[i,j]-\eta\left(\text{sign}\left(g_{1}^{(t)}[i,j] \right)+e_{1}^{(t)}[i,j]\right),\]

where \(\left|e_{1}^{(t)}[i,j]\right|=\tilde{\mathcal{O}}\left(\sqrt{\eta}\right)\). The proof for \(w_{2i}^{(t)}\) is similar.

So far we have successfully proved eq. (29). By \(\text{sign}\left(\Delta W_{1}^{(t)}[i,j]\right)=\text{sign}\left(\Delta w_{2i }^{(t)}\right)=\text{sign}\left(w_{2i}^{(0)}\right)\) in Lemma 19, we know that \(\text{sign}\left(-g_{1}^{(t)}[i,j]\right)=\text{sign}\left(-g_{2i}^{(t)} \right)=\text{sign}\left(w_{2i}^{(0)}\right)\), which gives us

\[\forall i,j\in[d]:\quad w_{2i}^{(t)} =\text{sign}\left(w_{2i}^{(0)}\right)\eta\left(t-t_{\text{inc}} \right)+R_{2i}^{(t)},\] \[W_{1}^{(t)}[i,j] =\text{sign}\left(w_{2i}^{(0)}\right)\eta\left(t-t_{\text{inc}} \right)+R_{1}^{(t)}[i,j],\]

where \(\frac{\left|R_{1}^{(t)}[i,j]\right|}{\eta(t-t_{\text{inc}})}=\tilde{\mathcal{O }}\left(\sqrt{\eta}+\frac{\left|w_{1}^{(t_{\text{inc}})}[i,j]\right|}{\eta(t- t_{\text{inc}})}\right)\) and \(\frac{\left|R_{2i}^{(t)}\right|}{\eta(t-t_{\text{inc}})}=\tilde{\mathcal{O}} \left(\sqrt{\eta}+\frac{\left|w_{2i}^{(t_{\text{inc}})}\right|}{\eta(t-t_{ \text{inc}})}\right)\). Now it suffices to show that \(\forall i,j\in[d]:\left|w_{2i}^{(t_{\text{inc}})}\right|\leq\mathcal{O}\left( \frac{1}{d^{c}}\right),\left|W_{1}^{(t_{\text{inc}})}[i,j]\right|\leq\mathcal{O }\left(\frac{1}{d^{c}}\right)\), which is implied by Lemma 18.

Finally to complete the proof, we show that \(T_{1}=\Theta\left(\frac{1}{\sqrt{d\eta}}\right)\). When \(t=T_{1}\), we have \(\forall j\in[d]:\sum_{i=1}^{d}w_{2i}^{(T_{1})}W_{1}^{(T_{1})}[i,j]=\Theta(1)\). Combining with the above results, we know that \(d\eta^{2}(T_{1}-t_{\text{inc}})^{2}=\Theta(1)\), i.e. \(\eta(T_{1}-t_{\text{inc}})=\Theta\left(\frac{1}{\sqrt{d}}\right)\). In Section E.5, we will prove \(t_{\text{inc}}=\Theta\left(\frac{1}{nd^{\frac{3}{2}\alpha+1}}\right)\). Then we have \(T_{1}=\Theta\left(\frac{1}{\sqrt{d\eta}}\right)\).

### Proof of Lemma 17

For certain \(t\) and \(H\), we write eq. (27) as

\[\Delta W_{1}^{(t)}[i,j] =-\eta_{t}\frac{(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}g_{ 1}^{(t-\tau)}[i,j]+\epsilon_{1n}^{(t)}[i,j]}{\sqrt{(1-\beta_{2})\sum_{\tau=0}^ {H}\beta_{2}^{\tau}\left(g_{1}^{(t-\tau)}[i,j]\right)^{2}+\epsilon_{1d}^{(t)}[ i,j]+\xi}},\] \[\Delta w_{2i}^{(t)} =-\eta_{t}\frac{(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}g_{ 2i}^{(t-\tau)}+\epsilon_{2n,i}^{(t)}}{\sqrt{(1-\beta_{2})\sum_{\tau=0}^{H}\beta _{2}^{\tau}\left(g_{2i}^{(t-\tau)}\right)^{2}+\epsilon_{2d,i}^{(t)}+\xi}},\]where

\[\epsilon_{1n}^{(t)}[i,j] :=\underbrace{(1-\beta_{1})\sum_{\tau=H+1}^{t}\beta_{1}^{\tau}g_{1}^ {(t-\tau)}[i,j]}_{:=q_{1n}^{(t)}[i,j]}+r_{1n}^{(t)}[i,j],\epsilon_{2n,i}^{(t)} :=\underbrace{(1-\beta_{1})\sum_{\tau=H+1}^{t}\beta_{1}^{\tau}g_{2i}^{(t- \tau)}}_{:=q_{2n,i}^{(t)}}+r_{2n,i}^{(t)},\] \[\epsilon_{1d}^{(t)}[i,j] :=\underbrace{(1-\beta_{2})\sum_{\tau=H+1}^{t}\beta_{2}^{\tau} \left(g_{1}^{(t-\tau)}[i,j]\right)^{2}}_{:=q_{1d}^{(t)}[i,j]}+r_{1d}^{(t)}[i,j],\] \[\epsilon_{2d,i}^{(t)} :=\underbrace{(1-\beta_{2})\sum_{\tau=H+1}^{t}\beta_{2}^{\tau} \left(g_{2i}^{(t-\tau)}\right)^{2}}_{:=q_{2d,i}^{(t)}}+r_{2d,i}^{(t)},\]

and \(r_{1n}^{(t)}[i,j],r_{1d}^{(t)}[i,j],r_{2n,i}^{(t)},r_{2d,i}^{(t)}\) are defined in eq. (28).

Since \(\beta_{2}=\beta_{1}^{2}<\beta_{1}\), then if we pick \(H\geq\frac{1}{1-\beta_{1}}\log\frac{\max\left\{G_{1}^{(1)},G_{2}^{(t)},\left( G_{1}^{(t)}\right)^{2},\left(G_{2}^{(t)}\right)^{2}\right\}}{\eta\xi^{2}}\), we can get that \(H\geq\frac{1}{1-\beta_{1}}\log\frac{G_{1}^{(t)}}{\eta\xi^{2}},H\geq\frac{1}{1 -\beta_{2}}\log\frac{\left(G_{1}^{(t)}\right)^{2}}{\eta\xi^{2}}\), \(H\geq\frac{1}{1-\beta_{1}}\log\frac{G_{1}^{(t)}}{\eta\xi^{2}},H\geq\frac{1}{1 -\beta_{2}}\log\frac{\left(G_{1}^{(t)}\right)^{2}}{\eta\xi^{2}}\). Hence we can apply Lemma 33 in Appendix H to get that \(\left|q_{1n}^{(t)}[i,j]\right|,\left|q_{1d}^{(t)}[i,j]\right|,\left|q_{2n,i}^{ (t)}\right|,\left|q_{2d,i}^{(t)}\right|\leq\eta\xi^{2}\).

Pick \(T\) in Lemma 32 as of order \(\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}\eta}\right)\). By Lemma 32, we have with probability at least \(1-\frac{1}{d}\), for all \(t\leq T\), \(\forall\tau\leq t\) and \(\forall i,j\in[d]\),

\[\left|Dg_{1}^{(\tau)}[i,j]\right| =\left|\tilde{g}_{1}^{(\tau)}[i,j]-g_{1}^{(\tau)}[i,j]\right|\leq \tilde{\mathcal{O}}\left(d^{3}M_{1}^{(t)}\left(M_{2}^{(t)}\right)^{2}\sigma \sqrt{\frac{d^{1/2}}{\eta}}\right)+\tilde{\mathcal{O}}\left(M_{2}^{(t)}\sigma \sqrt{\frac{d^{3/2}}{\eta}}\right)\] \[:=D_{1}^{(t)},\] \[\left|Dg_{2i}^{(\tau)}\right| =\left|\tilde{g}_{2i}^{(\tau)}-g_{2i}^{(\tau)}\right|\leq\tilde{ \mathcal{O}}\left(d^{4}\left(M_{1}^{(t)}\right)^{2}M_{2}^{(t)}\sigma\sqrt{ \frac{d^{1/2}}{\eta}}\right)+\tilde{\mathcal{O}}\left(dM_{1}^{(t)}\sigma \sqrt{\frac{d^{3/2}}{\eta}}\right):=D_{2}^{(t)}.\]

Plugging into eq. (28) gives us

\[\left|r_{1n}^{(t)}[i,j]\right| \leq(1-\beta_{1})\sum_{\tau=0}^{t}\beta_{1}^{t-\tau}\left|Dg_{1} ^{(\tau)}[i,j]\right|\leq\mathcal{O}\left(D_{1}^{(t)}\right),\] \[\left|r_{1d}^{(t)}[i,j]\right| \leq(1-\beta_{2})\sum_{\tau=0}^{t}\beta_{2}^{t-\tau}\left|2g_{1} ^{(\tau)}[i,j]Dg_{1}^{(\tau)}[i,j]\right|+\left|Dg_{1}^{(\tau)}[i,j]\right|^{2}\] \[\leq\mathcal{O}\left(D_{1}^{(t)}G_{1}^{(t)}+\left(D_{1}^{(t)} \right)^{2}\right),\] \[\left|r_{2n,i}^{(t)}\right| \leq(1-\beta_{1})\sum_{\tau=0}^{t}\beta_{1}^{t-\tau}\left|Dg_{2i} ^{(\tau)}\right|\leq\mathcal{O}\left(D_{2}^{(t)}\right),\] \[\left|r_{2d,i}^{(t)}\right| \leq(1-\beta_{2})\sum_{\tau=0}^{t}\beta_{2}^{t-\tau}\left|2g_{2i} ^{(\tau)}Dg_{2i}^{(\tau)}\right|+\left|Dg_{2i}^{(\tau)}\right|^{2}\leq \mathcal{O}\left(D_{2}^{(t)}G_{2}^{(t)}+\left(D_{2}^{(t)}\right)^{2}\right).\]

### Proof of Corollary 2

Since \(G_{1}^{(t)}\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right),G_{2}^{(t)} \leq\tilde{\mathcal{O}}\left(\sqrt{d}\right)\), then \(H:=\frac{1}{1-\beta_{1}}\log\frac{d}{\eta\xi^{2}}\) is bigger than

\(\frac{1}{1-\beta_{1}}\log\frac{\max\left\{G_{1}^{(t)},G_{2}^{(t)},\left(G_{1}^{ (t)}\right)^{2},\left(G_{2}^{(t)}\right)^{2}\right\}}{\eta\xi^{2}}\).

By \(M_{1}^{(t)},M_{2}^{(t)}\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\), \(G_{1}^{(t)}\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right),G_{2}^{(t)} \leq\tilde{\mathcal{O}}\left(\sqrt{d}\right)\) and the assumption \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{13/4}}\), we get that \(D_{1}^{(t)}\) and \(D_{2}^{(t)}\) are upper bounded by \(D_{1}^{(t)}\leq\tilde{\mathcal{O}}\left(d^{7/4}\sigma\eta^{-1/2}\right)\) and \(D_{2}^{(t)}\leq\tilde{\mathcal{O}}\left(d^{11/4}\sigma\eta^{-1/2}\right)\), which yields \(\forall i,j\in[d]\), \(\left|\epsilon_{1n}^{(t)}[i,j]\right|,\left|\epsilon_{1d}^{(t)}[i,j]\right|, \left|\epsilon_{2n,i}^{(t)}\right|,\left|\epsilon_{2d,i}^{(t)}\right|\leq \tilde{\mathcal{O}}(\eta\xi^{2})\).

### Proof of Lemma 18

The proof is based on the following two lemmas.

**Lemma 21**.: _Under Assumption 1 and 2, we have that w.p. at least \(1-\frac{1}{d^{\frac{3}{2}-1}}\), for every \(1\leq i\leq d\), \(\frac{\sqrt{\pi}}{d^{\frac{3}{2}\alpha}}\leq\left|w_{2i}^{(0)}\right|\leq\sqrt {\frac{2}{d^{2\alpha}}\log\frac{2d}{\delta}}\), and that w.p. at least \(1-\delta\) for any given \(\delta>0\), \(\left|W_{1}^{(0)}[i,j]\right|\leq\sqrt{\frac{2}{d^{4\alpha}}\log\frac{2d^{2}}{ \delta}}\)._

**Lemma 22**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{3/4}}\). Pick \(\beta_{2}=\beta_{1}^{2}\), \(\xi\in(0,1),\eta<\frac{1}{4}\). Consider any time point \(t\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}\eta}\right)\). If \(\forall\tau\leq t,\forall i,j\in[d]:\left|W_{1}^{(\tau)}[i,j]\right|\leq\tilde {\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right),\left|w_{2i}^{(\tau)}\right| \leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\) and \(\left|g_{1}^{(\tau)}[i,j]\right|\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{ d}}\right),\left|g_{2i}^{(\tau)}\right|\leq\tilde{\mathcal{O}}\left(\sqrt{d}\right)\), we will have_

\[\left|\Delta W_{1}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}(\eta)\quad \left|\Delta w_{2i}^{(t)}\right|\leq\tilde{\mathcal{O}}(\eta),\]

_where the \(\tilde{\mathcal{O}}\) notation depends on \(H=\frac{1}{1-\beta_{1}}\log\frac{d}{\eta\xi^{2}}\). Furthermore, if for certain \(i,j\in[d]\), Condition 1 (resp. Condition 2) is satisfied, we will have_

\[\text{sign}\left(\Delta W_{1}^{(t)}[i,j]\right)=-s_{1}^{(t)}[i,j], \left|\Delta W_{1}^{(t)}[i,j]\right|=\tilde{\Theta}(\eta)\] \[\left(\text{resp. sign}\left(\Delta w_{2i}^{(t)}\right)=-s_{2i}^{(t)}, \left|\Delta w_{2i}^{(t)}\right|=\tilde{\Theta}(\eta)\right)\]

Now we prove Lemma 18. Define \(t_{d}:=\inf\left\{t:\exists i,j:\left|W_{1}^{(t)}[i,j]\right|>\frac{1}{d}\text { or }\left|w_{2i}^{(t)}\right|>\frac{1}{d}\right\}\). Now we want to find a time point \(t_{\text{inc}}\) before \(t_{d}\) for the lemma to hold. During the period \(t<t_{d}\), we have \(\forall j\in[d],E_{j}=-\Theta(1)\) (which means \(t_{d}<T_{1}\)) and therefore for all \(i,j\in[d]\), \(\left|g_{1}^{(t)}[i,j]\right|\leq\frac{1}{d}\) and \(\left|g_{2i}^{(t)}\right|\leq 1\). Then we can use Lemma 22 to get that for \(t\leq\min\left\{t_{d},\frac{1}{\sqrt{d}\eta}\right\}\), we have \(\left|\Delta W_{1}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}(\eta),\left|\Delta w _{2i}^{(t)}\right|\leq\tilde{\mathcal{O}}(\eta)\). Hence \(t_{d}\geq\tilde{\Omega}\left(\frac{1}{\eta d}\right)\).

Define \(t_{\text{sign}}=\inf\left\{t<\min\left\{t_{d},\frac{1}{\sqrt{d}\eta}\right\}: \exists i\in[d]:\left|w_{2i}^{(t)}\right|\leq\frac{1}{d^{\frac{3}{2}\alpha}}\right\}\). By Lemma 21, w.h.p. \(\forall i\in[d]:\left|w_{2i}^{(0)}\right|\geq\frac{\sqrt{\pi}}{d^{\frac{3}{2} \alpha}}\), combining with \(\left|\Delta w_{2i}^{(t)}\right|\leq\tilde{\mathcal{O}}(\eta)\) gives us that w.h.p., \(t_{\text{sign}}\geq\frac{\sqrt{\pi}-1}{d^{\frac{3}{2}\alpha}}/\tilde{ \mathcal{O}}(\eta)=\tilde{\Omega}\left(\frac{1}{\eta d^{\frac{3}{2}\alpha}}\right)\).

Now let's analyze the behavior of \(W_{1}\) during the period \(t<t_{\text{sign}}\). Consider any \(i,j\in[d]\). By definition, \(\text{sign}\left(w_{2i}^{(t)}\right)=\text{sign}\left(w_{2i}^{(0)}\right)\). Note that \(E_{j}^{(t)}=-\Theta(1)\), then we have \(\text{sign}\left(g_{1}^{(t)}[i,j]\right)=-\text{sign}\left(w_{2i}^{(0)}\right)\) and that \(\left|g_{1}^{(t)}[i,j]\right|=\Omega\left(\frac{1}{d^{\frac{3}{2}\alpha}}\right)= \Omega(\xi)\) by our choice of \(\xi\). Then we know that Condition 1 is satisfied with \(s_{1}^{(t)}[i,j]=-\text{sign}\left(w_{2i}^{(0)}\right)\) (for all \(H<t\leq t_{\text{sign}}\)), which by Lemma 22 yields \(\text{sign}\left(\Delta W_{1}^{(t)}[i,j]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\) and \(\left|\Delta W_{1}^{(t)}[i,j]\right|=\tilde{\Theta}(\eta)\).

Lemma 21 tells us that w.h.p., \(\forall i,j\in[d]:\left|W_{1}^{(0)}[i,j]\right|=\tilde{\mathcal{O}}\left(\frac{ 1}{d^{2\alpha}}\right)\). For any \(i,j\), if initially \(\text{sign}\left(W_{1}^{(0)}[i,j]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\), then for the following steps before \(t_{\text{sign}}\), we will have \(\text{sign}\left(W_{1}^{(t)}[i,j]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\). If initially \(\text{sign}\left(W_{1}^{(0)}[i,j]\right)\neq\text{sign}\left(w_{2i}^{(0)}\right)\), then after at most \(t_{0}=\tilde{\mathcal{O}}\left(\frac{1}{\eta d^{2\alpha}}\right)\) steps, \(W_{1}[i,j]\) will flip the sign. Note that \(t_{0}=\tilde{\mathcal{O}}\left(\frac{1}{\eta d^{2\alpha}}\right)\) is smaller than \(t_{\text{sign}}\).

Hence we have shown that at some time point \(t_{0}\), we have \(\forall i,j\in[d]:\text{sign}\left(W_{1}^{(t)}[i,j]\right)=\text{sign}\left(w_{2i} ^{(t)}\right)=\text{sign}\left(w_{2i}^{(0)}\right)\). Now we analyze the period \(t\geq t_{0}\).

When \(t_{0}<t\leq t_{\text{sign}}\), we still have \(\text{sign}\left(\Delta W_{1}^{(t)}[i,j]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\) and \(\left|\Delta W_{1}^{(t)}[i,j]\right|=\tilde{\Theta}(\eta)\). Combining these two with the fact \(\text{sign}\left(W_{1}^{(t_{0})}[i,j]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\), we know that for all \(t\in[t_{0},t_{\text{sign}}]\), \(\text{sign}\left(W_{1}^{(t)}[i,j]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\) and that \(\forall i,j\in[d]:\left|W_{1}^{(t+1)}[i,j]\right|=\left|W_{1}^{(t)}[i,j] \right|+\tilde{\Theta}(\eta)\). Then at certain step \(t_{\text{inc}}\) which satisfies \(t_{\text{inc}}=t_{0}+\tilde{\Theta}\left(\frac{1}{nd^{\frac{1}{2}\alpha+1}}\right) \in(H,t_{\text{sign}})\), we will have \(\forall t_{\text{inc}}-H\leq\tau\leq t_{\text{inc}},\forall i,j\in[d]:\left|W _{1}^{(\tau)}[i,j]\right|=\Theta\left(\frac{1}{d^{\frac{1}{2}\alpha+1}}\right)\) and therefore \(\left|g_{2i}^{(\tau)}\right|=\left|\sum_{j=1}^{d}W_{1}^{(\tau)}[i,j]E_{j}^{( \tau)}\right|=\sum_{j=1}^{d}\left|W_{1}^{(\tau)}[i,j]E_{j}^{(\tau)}\right|= \Theta\left(\frac{1}{d^{\frac{1}{2}\alpha}}\right)=\Omega(\xi)\). For \(t\leq t_{\text{inc}}\), we have \(\forall i,j\in[d]:\left|W_{1}^{(t)}[i,j]\right|=\mathcal{O}\left(\frac{1}{d^{ \frac{1}{2}\alpha+1}}\right)\).

Since \(t_{\text{inc}}<t_{\text{sign}}\), we have \(\left|w_{2i}^{t_{\text{inc}}}\right|=\Omega\left(\frac{1}{d^{\frac{1}{2} \alpha}}\right)\). For \(t\leq t_{\text{inc}}\), note that \(\left|\Delta w_{2i}^{(t)}\right|\leq\tilde{\mathcal{O}}(\eta)\), \(t_{\text{inc}}=t_{0}+\tilde{\Theta}\left(\frac{1}{nd^{\frac{3}{2}\alpha+1}} \right)=\tilde{\Theta}\left(\frac{1}{nd^{\frac{3}{2}\alpha+1}}\right)\), combining with the upper bound in Lemma 21 yields

\[\left|w_{2i}^{(t)}\right|\leq\left|w_{2i}^{(0)}\right|+t_{\text{inc}}\tilde{ \mathcal{O}}(\eta)\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{3}{2}\alpha+ 1}}\right)\leq\mathcal{O}\left(\frac{1}{d^{\alpha}}\right).\]

Moreover, \(\forall t_{\text{inc}}-H\leq\tau\leq t_{\text{inc}},\forall i\in[d]:\text{ sign}\left(g_{2i}^{(\tau)}\right)=-\text{sign}\left(w_{2i}^{(0)}\right)\). Then Condition 2 is satisfied with \(s_{2i}^{(t)}=-\text{sign}\left(w_{2i}^{(0)}\right)\) for \(t=t_{\text{inc}}\). In the analysis of \(g_{1}^{(t)}[i,j]\), we have already shown that for all \(t\leq t_{\text{sign}}\) (and thus for \(t=t_{\text{inc}}\)), Condition 1 is satisfied, which completes the proof.

### Proof of Lemma 21

Since for \(X\sim\mathcal{N}\left(0,\sigma^{2}\right)\), we have that \(P(\left|X\right|\leq t)\leq\frac{2t}{\sqrt{2\pi}\sigma}\), then for a fixed \(i\),

\[P\left(\left|w_{2i}^{(0)}\right|\leq\frac{\sqrt{\pi}}{d^{\frac{3}{3}\alpha}} \right)\leq\frac{2\sqrt{\pi}/d^{\frac{3}{3}\alpha}}{\sqrt{2\pi}\cdot\sqrt{2/d ^{2\alpha}}}=\frac{1}{d^{\frac{\pi}{2}}}.\]

Then by union bound, we have that w.p. at least \(1-\frac{1}{d^{\frac{1}{2}-1}}\), for every \(1\leq i\leq d\), \(\left|w_{2i}^{(0)}\right|\geq\frac{\sqrt{\pi}}{d^{\frac{3}{3}\alpha}}\).

As for the upper bounds, using the Gaussian tail bound and union bound, we have w.p. at least \(1-\delta\),

\[\forall i,j\in[d]:\quad\left|w_{2i}^{(0)}\right|\leq\sqrt{\frac{2}{d^{\alpha }}\log\frac{2d}{\delta}},\quad\left|W_{1}^{(0)}[i,j]\right|\leq\sqrt{\frac{2}{ d^{4\alpha}}\log\frac{2d^{2}}{\delta}}.\]

### Proof of Lemma 22

Now we analyze the magnitude order of \(\Delta W_{1}^{(t)}[i,j]\). The analysis of \(\Delta w_{2i}^{(t)}\) is similar.

For \(t\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}\eta}\right)\). By assumption, \(M_{1}^{(t)},M_{2}^{(t)}\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\), \(G_{1}^{(t)}\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right),G_{2}^{(t)} \leq\tilde{\mathcal{O}}\left(\sqrt{d}\right)\), and \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{13/4}}\). Hence we can pick \(H:=\frac{1}{1-\beta_{1}}\log\frac{d}{\eta\xi^{2}}\) and apply Lemma 17 and Corollary 2 to get that, w.h.p., for all \(t\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}\eta}\right)\) and \(\forall i,j\in[d]\), eq. (27) can be written as

\[\begin{split}\Delta W_{1}^{(t)}[i,j]&=-\eta_{t}\frac{ (1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}g_{1}^{(t-\tau)}[i,j]+\epsilon_{1 }^{(t)}[i,j]}{\sqrt{(1-\beta_{2})\sum_{\tau=0}^{H}\beta_{2}^{\tau}\left(g_{1}^{ (t-\tau)}[i,j]\right)^{2}+\epsilon_{1d}^{(t)}[i,j]+\xi}},\\ \Delta w_{2i}^{(t)}&=-\eta_{t}\frac{(1-\beta_{1})\sum_ {\tau=0}^{H}\beta_{1}^{\tau}g_{2i}^{(t-\tau)}+\epsilon_{2n,i}^{(t)}}{\sqrt{(1- \beta_{2})\sum_{\tau=0}^{H}\beta_{2}^{\tau}\left(g_{2i}^{(t-\tau)}\right)^{2}+ \epsilon_{2d,i}^{(t)}+\xi}},\end{split}\] (33)where \(\forall i,j\in[d]\), \(\left|\epsilon_{1n}^{(t)}[i,j]\right|,\left|\epsilon_{1d}^{(t)}[i,j]\right|, \left|\epsilon_{2n,i}^{(t)}\right|,\left|\epsilon_{2d,i}^{(t)}\right|\leq \tilde{\mathcal{O}}(\eta\xi^{2})\).

On one hand, using \(\left|\epsilon_{1n}^{(t)}[i,j]\right|,\left|\epsilon_{1d}^{(t)}[i,j]\right|\leq \tilde{\mathcal{O}}(\eta\xi^{2})\) and \(\beta_{2}=\beta_{1}^{2}\), and \(\sqrt{x+y}\geq\sqrt{x}-\sqrt{|y|}\) when \(x\geq 0,x+y\geq 0\), we get from eq. (33) that

\[\left|\Delta W_{1}^{(t)}[i,j]\right| \leq\eta_{t}\frac{(1-\beta_{1})\left|\sum_{r=0}^{H}\beta_{1}^{ \tau}g_{1}^{(t-\tau)}[i,j]\right|+\tilde{\mathcal{O}}(\eta\xi^{2})}{\sqrt{(1- \beta_{2})\sum_{r=0}^{H}\left(\beta_{1}^{\tau}g_{1}^{(t-\tau)}[i,j]\right)^{2 }-\tilde{\mathcal{O}}(\sqrt{\eta}\xi)+\xi}}\] \[\overset{(i)}{\leq}\eta_{t}\frac{(1-\beta_{1})\sqrt{H+1}\sqrt{ \sum_{r=0}^{H}\left(\beta_{1}^{\tau}g_{1}^{(t-\tau)}[i,j]\right)^{2}+\tilde{ \mathcal{O}}(\eta\xi^{2})}}{\sqrt{(1-\beta_{2})\sum_{r=0}^{H}\left(\beta_{1}^ {\tau}g_{1}^{(t-\tau)}[i,j]\right)^{2}+\xi/2}}\leq\mathcal{O}\left(\sqrt{H} \eta\right)=\tilde{\mathcal{O}}(\eta),\]

where \((i)\) uses Cauchy-Schwarz inequality for the numerator.

On the other hand, when \(\text{sign}\left(g_{1}^{(t-H)}[i,j]\right)=\text{sign}\left(g_{1}^{(t-H+1)}[i, j]\right)=...=\text{sign}\left(g_{1}^{(t)}[i,j]\right)=s_{1}^{(t)}[i,j]\), we have

\[\text{sign}\left(\sum_{\tau=0}^{H}\beta_{1}^{\tau}g_{1}^{(t-\tau)}[i,j]\right) =s_{1}^{(t)}[i,j],\quad\left|\sum_{\tau=0}^{H}\beta_{1}^{\tau}g_{1}^{(t-\tau) }[i,j]\right|\geq\sqrt{\sum_{\tau=0}^{H}\left(\beta_{1}^{\tau}g_{1}^{(t-\tau)} [i,j]\right)^{2}}.\]

If we further have \((1-\beta_{1})\left|\sum_{\tau=0}^{H}\beta_{1}^{(\tau)}g_{1}^{(t-\tau)}[i,j] \right|\geq\Omega(\xi)\), then combining with \(\left|\epsilon_{1n}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}(\eta\xi^{2})<\xi\) we will get

\[\text{sign}\left(\Delta W_{1}^{(t)}[i,j]\right) =-\text{sign}\left(\sum_{\tau=0}^{H}\beta_{1}^{\tau}g_{1}^{(t- \tau)}[i,j]+\epsilon_{1n}^{(t)}[i,j]\right)=-\text{sign}\left(\sum_{\tau=0}^{ H}\beta_{1}^{\tau}g_{1}^{(t-\tau)}[i,j]\right)\] \[=-s_{1}^{(t)}[i,j].\]

Using \(\sqrt{x+y}\leq\sqrt{|x|}+\sqrt{|y|}\), we obtain that

\[\left|\Delta W_{1}^{(t)}[i,j]\right| \geq\eta_{t}\frac{(1-\beta_{1})\left|\sum_{\tau=0}^{H}\beta_{1}^ {(\tau)}g_{1}^{(t-\tau)}[i,j]\right|-\tilde{\mathcal{O}}(\eta\xi^{2})}{\sqrt{ (1-\beta_{2})\sum_{\tau=0}^{H}\left(\beta_{1}^{\tau}g_{1}^{(t-\tau)}[i,j] \right)^{2}+\tilde{\mathcal{O}}(\sqrt{\eta}\xi)+\xi}}\] \[\geq\eta_{t}\frac{\frac{1-\beta_{1}}{2}\left|\sum_{\tau=0}^{H} \beta_{1}^{(\tau)}g_{1}^{(t-\tau)}[i,j]\right|}{2\max\left\{\sqrt{(1-\beta_{2}) \sum_{\tau=0}^{H}\left(\beta_{1}^{\tau}g_{1}^{(t-\tau)}[i,j]\right)^{2}}, \frac{3}{2}\xi\right\}}=\Omega(\eta).\]

Together with the upper bound completes the proof.

### Proof of Lemma 19

The proof is based on the following lemma, which gives a coarse analysis on the magnitude of weights and their increments per step during the first phase.

**Lemma 23**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{3^{3/2}\xi^{2}}{d^{13/3}\xi^{2}}\). Pick \(\xi\leq\min\left\{\sqrt{\frac{\eta}{d^{3\alpha-1}}},\,\frac{1}{d^{\frac{1}{2} \,\alpha}}\right\}\), for \(t_{\text{inc}}\) in Lemma 18, we have that w.h.p. for all \(t_{\text{inc}}\leq t\leq T_{1}\), \(\forall i,j\in[d]\)._

_sign \(\left(\Delta W_{1}^{(t)}[i,j]\right)=\text{sign}\left(\Delta w_{2i}^{(t)} \right)=\text{sign}\left(w_{2i}^{(0)}\right),\quad\left|\Delta W_{1}^{(t)}[i,j] \right|=\tilde{\Theta}(\eta),\left|\Delta w_{2i}^{(t)}\right|=\tilde{\Theta}( \eta),\)_

\[\text{sign}\left(W_{1}^{(t)}[i,j]\right)=\text{sign}\left(w_{2i}^{(t)}\right) =\text{sign}\left(w_{2i}^{(0)}\right),\quad\left|W_{1}^{(t)}[i,j]\right|= \tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right),\left|w_{2i}^{(t)}\right| =\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right).\]

_Specially, at the end of the first phase (\(t=T_{1}\)), we have \(\forall i,j\in[d]\), \(\left|w_{2i}^{(T_{1})}\right|=\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\right)\) and \(\left|W_{1}^{(T_{1})}[i,j]\right|=\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\right)\)._Now we go back to the proof of Lemma 19. For \(t_{\text{inc}}\leq t<T_{1}\), since \(E_{j}^{(t)}=\left(W_{2}^{(t)}W_{1}^{(t)}\right)_{j}-A_{j}=\sum_{i=1}^{d}w_{2i}^{( t)}W_{1}^{(t)}[i,j]-A_{j}\), we have,

\[\begin{split}\Delta E_{j}^{(t)}&:=E_{j}^{(t+1)}-E_{j }^{(t)}\\ &=\sum_{i=1}^{d}\left(w_{2i}^{(t+1)}W_{1}^{(t+1)}[i,j]-w_{2i}^{(t +1)}W_{1}^{(t)}[i,j]+w_{2i}^{(t+1)}W_{1}^{(t)}[i,j]-w_{2i}^{(t)}W_{1}^{(t)}[i,j] \right)\\ &=\sum_{i=1}^{d}\left(w_{2i}^{(t+1)}\Delta W_{1}^{(t)}[i,j]+ \Delta w_{2i}^{(t)}W_{1}^{(t)}[i,j]\right).\end{split}\] (34)

Combining Lemma 23 and eq. (34) gives us \(\forall j\in[d]\),

\[\begin{split}\Delta E_{j}^{(t)}>0,\quad\left|\Delta E_{j}^{(t)} \right|&=\sum_{i=1}^{d}\left|w_{2i}^{(t+1)}\Delta W_{1}^{(t)}[i,j ]\right|+\left|\Delta w_{2i}^{(t)}W_{1}^{(t)}[i,j]\right|\\ &\leq\sum_{i=1}^{d}\tilde{\mathcal{O}}\left(\eta\frac{1}{\sqrt{d }}\right)=\tilde{\mathcal{O}}\left(\eta\sqrt{d}\right).\end{split}\] (35)

Let's first analyze \(g_{1}^{(t)}[i,j]\). Note that

\[\begin{split}\Delta g_{1}^{(t)}[i,j]&=w_{2i}^{(t+1 )}E_{j}^{(t+1)}-w_{2i}^{(t+1)}E_{j}^{(t)}+w_{2i}^{(t+1)}E_{j}^{(t)}-w_{2i}^{(t )}E_{j}^{(t)}\\ &=w_{2i}^{(t+1)}\Delta E_{j}^{(t)}+\Delta w_{2i}^{(t)}E_{j}^{(t)},\end{split}\] (36)

where \(\text{sign}\left(w_{2i}^{(t+1)}\Delta E_{j}^{(t)}\right)=\text{sign}\left(w_{ 2i}^{(0)}\right)\) while \(\text{sign}\left(\Delta w_{2i}^{(t)}E_{j}^{(t)}\right)=-\text{sign}\left(w_{2i }^{(0)}\right)\).

Now we analyze the sign of \(g_{1}^{(t)}[i,j]\) when \(t_{\text{inc}}\leq t<T_{1}\). Using \(\left|w_{2i}^{(t_{\text{inc}}+1)}\right|=\tilde{\mathcal{O}}\left(\frac{1}{d^ {*}}\right)\) and eq. (35), we get that \(\left|w_{2i}^{(t_{\text{inc}}+1)}\Delta E_{j}^{(t_{\text{inc}})}\right|\leq \tilde{\mathcal{O}}\left(\frac{1}{d^{*}}\cdot\sqrt{d}\eta\right)\). While on the other hand, \(\left|\Delta w_{2i}^{(t_{\text{inc}})}E_{j}^{(t_{\text{inc}})}\right|=\tilde {\Theta}(\eta)\). That means \(\text{sign}\left(\Delta g_{1}^{(t_{\text{inc}})}[i,j]\right)=-\text{sign} \left(w_{2i}^{(0)}\right)\). Note that \(\text{sign}\left(g_{1}^{(t_{\text{inc}})}[i,j]\right)=-\text{sign}\left(w_{2i }^{(t_{\text{inc}})}\right)=-\text{sign}\left(w_{2i}^{(0)}\right)\), we know that \(\left|g_{1}^{(t)}[i,j]\right|\) will increase when \(t=t_{\text{inc}}\).

In the following steps, \(\left|g_{1}^{(t)}[i,j]\right|\) will keep increasing as long as \(\left|\Delta w_{2i}^{(t)}E_{j}^{(t)}\right|>\left|w_{2i}^{(t+1)}\Delta E_{j}^{ (t)}\right|\). Since \(\left|W_{1}^{(t)}[i,j]\right|,\left|w_{2i}^{(t)}\right|\) keep increasing while \(\left|\Delta W_{1}^{(t)}[i,j]\right|,\left|\Delta w_{2i}^{(t)}\right|\) remain \(\tilde{\Theta}(\eta)\), by eq. (35), we know that the trend of \(\left|\Delta E_{j}^{(t)}\right|\) is to increase. On the other hand, \(\left|E_{j}^{(t)}\right|\) keeps decreasing since \(E_{j}^{(t)}<0\) while \(\Delta E_{j}^{(t)}>0\). Then after some time point we will have \(\left|\Delta w_{2i}^{(t)}E_{j}^{(t)}\right|<\left|w_{2i}^{(t+1)}\Delta E_{j}^{ (t)}\right|\) and in the following steps \(\left|g_{1}^{(t)}[i,j]\right|\) will have the trend to decrease. Specially, when \(t=T_{1}-1\), we have \(\left|E_{j}^{(t)}\right|=\Theta\left(\sqrt{\eta d}\right)\) and \(\left|W_{1}^{(t)}\right|=\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\right),\left| w_{2i}^{(t+1)}\right|=\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\right)\) by Lemma 23, which gives us

\[\left|\Delta E_{j}^{(t)}\right|=\sum_{i=1}^{d}\left|w_{2i}^{(t+1)}\Delta W_{1}^ {(t)}[i,j]\right|+\left|\Delta w_{2i}^{(t)}W_{1}^{(t)}[i,j]\right|\leq\sum_{i= 1}^{d}\tilde{\Theta}\left(\eta\frac{1}{\sqrt{d}}\right)=\tilde{\Theta}\left( \eta\sqrt{d}\right).\]

Hence \(\left|w_{2i}^{(t+1)}\Delta E_{j}^{(t)}\right|=\tilde{\Theta}(\eta)>\left| \Delta w_{2i}^{(t)}E_{j}^{(t)}\right|=\tilde{\Theta}\left(\eta\sqrt{\eta d}\right)\).

Therefore we have proved that when \(t_{\text{inc}}\leq t<T_{1}\), the trend of \(\left|g_{1}^{(t)}[i,j]\right|\) is to first increase and then decrease. In order to prove \(\left|g_{1}^{(t_{\text{inc}})}[i,j]\right|=\tilde{\Omega}\left(\sqrt{\eta}\right)\), it suffices to show that \(\left|g_{1}^{(t_{\text{inc}})}[i,j]\right|=\tilde{\Omega}\left(\sqrt{\eta}\right)\) and \(\left|g_{1}^{(T_{1})}[i,j]\right|=\tilde{\Omega}\left(\sqrt{\eta}\right)\).

When \(t=t_{\text{inc}}\),

\[\left|g_{1}^{(t_{\text{inc}})}[i,j]\right|=\left|w_{2i}^{(t_{\text{inc}})} \right|\cdot\left|E_{j}^{(t_{\text{inc}})}\right|=\Omega\left(\frac{1}{d^{ \frac{1}{d^{\frac{1}{d^{\frac{1}{d^{\frac{1}{d^{\frac{1}{d^{\frac{1}}}}}}}}}}}}} \right)\cdot\Theta(1)=\Omega\left(\sqrt{\eta}\right).\]When \(t=T_{1}\), we have

\[\left|g_{1}^{(T_{1})}[i,j]\right|=\left|w_{2i}^{(T_{1})}\right|\cdot\left|E_{j}^{ (t_{1})}\right|=\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\cdot\sqrt{\eta d}\right)= \tilde{\Theta}\left(\sqrt{\eta}\right).\]

As for \(g_{2i}^{(t)}\), since for \(\forall i\in[d]\), \(W_{1}^{(t)}[i,j]\) for different \(j\) have the same sign. Combining with \(\forall j\in[d]:E_{j}^{(t)}<0\) gives us

\[\left|g_{2i}^{(t)}\right|=\left|\sum_{j=1}^{d}E_{j}^{(t)}W_{1}^{(t)}[i,j] \right|=\sum_{j=1}^{d}\left|E_{j}^{(t)}W_{1}^{(t)}[i,j]\right|.\]

Then it suffices to show that for \(t_{\text{inc}}\leq t<T_{1}\), \(\left|E_{j}^{(t)}W_{1}^{(t)}[i,j]\right|=\tilde{\Omega}\left(\sqrt{\eta}\right)\), which can be proven using the same technique as above.

Finally, for \(\forall\tau\leq t,\forall i,j\in[d]\), note that the upper bounds of \(\left|W_{1}^{(\tau)}[i,j]\right|\) and \(\left|w_{2i}^{(\tau)}\right|\) are already given in Lemma 23. As for \(\left|g_{1}^{(\tau)}[i,j]\right|\) and \(\left|g_{2i}^{(\tau)}\right|\), we have \(\left|g_{1}^{(\tau)}[i,j]\right|=\left|w_{2i}^{(\tau)}E_{j}^{(\tau)}\right|= \tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right),\left|g_{2i}^{(\tau)} \right|\leq\sum_{j=1}^{d}\left|E_{j}^{(\tau)}W_{1}^{(\tau)}[i,j]\right|=\tilde {\mathcal{O}}\left(\sqrt{d}\right)\).

### Proof of Lemma 23

For any \(i,j\in[d]\), and any \(t\) in the interval \([t_{\text{inc}},T_{1}]\), we prove by induction that

1. \(\left|W_{1}^{(t)}[i,j]\right|=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}} \right),\left|w_{2i}^{(t)}\right|=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\).
2. \(\forall\tau\in[t-H,t]:\text{sign}\left(W_{1}^{(\tau)}[i,j]\right)=\text{sign} \left(w_{2i}^{(\tau)}\right)=\text{sign}\left(w_{2i}^{(0)}\right)\).
3. \(\left|g_{1}^{(t)}[i,j]\right|\geq\Omega(\xi),\left|g_{2i}^{(t)}\right|\geq \Omega(\xi)\).

The base case \(t=t_{\text{inc}}\) was already proven by Lemma 18.

For \(t\in[t_{\text{inc}},T_{1})\), suppose (B) and (C) hold for time \(t\) and (A) holds for all \(\tau\in[t_{\text{inc}},t]\). From (A), we get that \(\forall\tau\in[t_{\text{inc}},t]:\left|g_{1}^{(\tau)}[i,j]\right|=\left|w_{2i}^ {(\tau)}E_{j}^{(\tau)}\right|=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}} \right),\left|g_{2i}^{(\tau)}\right|\leq\sum_{j=1}^{d}\left|E_{j}^{(\tau)}W_{1 }^{(\tau)}[i,j]\right|=\tilde{\mathcal{O}}\left(\sqrt{d}\right)\). Since when \(t<T_{1}\), \(\forall j\in[d]:E_{j}^{(t)}<0\), from (B) we know that \(\forall\tau\in[t-H,t]:\text{sign}\left(g_{1}^{(\tau)}[i,j]\right)=\text{sign} \left(g_{2i}^{(\tau)}\right)=-\text{sign}\left(w_{2i}^{(0)}\right)\). Combining with (C) tells us that Condition 1 and 2 are satisfied.

In Section E.2 we have shown that \(T_{1}=\Theta\left(\frac{1}{\sqrt{d}\eta}\right)\). Then for \(t\in[t_{\text{inc}},T_{1})\), we can use Lemma 22 to get that \(\forall t_{\text{inc}}\leq\tau\leq t\), \(\forall i,j\in[d]\),

\[\text{sign}\left(\Delta W_{1}^{(\tau)}[i,j]\right)=\text{sign}\left(\Delta w_ {2i}^{(\tau)}\right)=\text{sign}\left(w_{2i}^{(0)}\right),\quad\left|\Delta W _{1}^{(\tau)}[i,j]\right|=\tilde{\Theta}(\eta),\left|\Delta w_{2i}^{(\tau)} \right|=\tilde{\Theta}(\eta).\]

Since when \(t=t_{\text{inc}}\), \(\text{sign}\left(W_{1}^{(t_{\text{inc}})}[i,j]\right)=\text{sign}\left(w_{2 i}^{(t_{\text{inc}})}\right)=\text{sign}\left(w_{2i}^{(0)}\right)\). We get that for \(t_{\text{inc}}\leq\tau\leq t\),

\[\forall i,j\in[d]:\quad\left|W_{1}^{(\tau+1)}[i,j]\right|=\left|W_{1}^{(\tau)} [i,j]\right|+\tilde{\Theta}(\eta),\quad\left|w_{2i}^{(\tau+1)}\right|=\left|w_{ 2i}^{(\tau)}\right|+\tilde{\Theta}(\eta).\]

Now for \(t+1\), we have \(\forall i,j\in[d]\),

\[\text{sign}\left(W_{1}^{(t+1)}[i,j]\right)=\text{sign}\left(w_{2i}^{(0)} \right),\quad\left|W_{1}^{(t+1)}[i,j]\right|=\left|W_{1}^{(t_{\text{inc}})}[i,j]\right|+(t+1-t_{\text{inc}})\,\tilde{\Theta}(\eta),\] \[\text{sign}\left(w_{2i}^{(t+1)}\right)=\text{sign}\left(w_{2i}^{(0 )}\right),\quad\left|w_{2i}^{(t+1)}\right|=\left|w_{2i}^{(t_{\text{inc}})} \right|+(t+1-t_{\text{inc}})\,\tilde{\Theta}(\eta).\]

That means \(\forall\tau\in[t+1-H,t+1]:\text{sign}\left(W_{1}^{(\tau)}[i,j]\right)=\text{ sign}\left(w_{2i}^{(\tau)}\right)=\text{sign}\left(w_{2i}^{(0)}\right)\). This proves (B) for time \(t+1\).

On the other hand, we get that \(\left|W_{1}^{(t+1)}[i,j]\right|\geq\left|W_{1}^{(t_{\text{inc}})}[i,j]\right|= \Theta\left(\frac{1}{d^{\frac{3}{2}\alpha+1}}\right)\) and \(\left|w_{2i}^{(t+1)}\right|\geq\left|w_{2i}^{(t_{\text{inc}})}\right|=\Omega \left(\frac{1}{d^{\frac{3}{2}\alpha}}\right)\). Since \(t+1\leq T_{1}\) which means \(\forall j\in[d]:\left|E_{j}^{(t+1)}\right|\geq\sqrt{\eta d}\). Then

\[\left|g_{1}^{(t+1)}[i,j]\right|=\left|w_{2i}^{(t+1)}E_{j}^{(t+1)} \right|\geq\Omega\left(\frac{1}{d^{\frac{3}{2}\alpha}}\right)\sqrt{\eta d}= \Omega(\xi),\] \[\left|g_{2i}^{(t+1)}\right|=\left|\sum_{j=1}^{d}E_{j}^{(t+1)}W_{1 }^{(t+1)}[i,j]\right|=\sum_{j=1}^{d}\left|E_{j}^{(t+1)}W_{1}^{(t+1)}[i,j] \right|\geq d\Theta\left(\frac{1}{d^{\frac{3}{2}\alpha+1}}\right)\sqrt{\eta d }=\Omega(\xi).\]

This proves (C) at time \(t+1\).

Since \(t+1\leq T_{1}\) which means \(\forall j\in[d]:\left(W_{2}^{(t+1)}W_{1}^{(t+1)}\right)_{j}\leq\mathcal{O}(1)\), we obtain that

\[\sum_{i=1}^{d}w_{2i}^{(t+1)}W_{1}^{(t+1)}[i,j]=\sum_{i=1}^{d} \left|w_{2i}^{(t+1)}\right|\left|W_{1}^{(t+1)}[i,j]\right|\] \[= \sum_{i=1}^{d}\left(\left|w_{2i}^{(t_{\text{inc}})}\right|+(t+1-t _{\text{inc}})\tilde{\Theta}(\eta)\right)\left(\left|W_{1}^{(t_{\text{inc}}) }[i,j]\right|+(t+1-t_{\text{inc}})\tilde{\Theta}(\eta)\right)\leq\mathcal{O}(1).\]

Note that \(\left|W_{1}^{(t_{\text{inc}})}[i,j]\right|,\left|w_{2i}^{(t_{\text{inc}})} \right|<\frac{1}{d}\) (since \(t_{\text{inc}}<t_{d}\)), we get that \((t+1-t_{\text{inc}})\tilde{\Theta}(\eta)=\mathcal{O}\left(\frac{1}{\sqrt{d}}\right)\), which gives us \(\left|w_{2i}^{(t+1)}\right|=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\) and \(\left|W_{1}^{(t+1)}[i,j]\right|=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\) and hence (A) holds at time \(t+1\).

Therefore by induction, we can prove that (A), (B), (C) hold for \(t_{\text{inc}}\leq t\leq T_{1}\). Then applying Lemma 22, we get that for all \(t_{\text{inc}}\leq t\leq T_{1}\), \(\forall i,j\in[d]:\quad\left|\Delta W_{1}^{(t)}[i,j]\right|=\tilde{\Theta}( \eta),\quad\left|\Delta w_{2i}^{(t)}\right|=\tilde{\Theta}(\eta)\).

Specially, at the end of the first phase, we have \(\forall j\in[d]:\left(W_{2}^{(t+1)}W_{1}^{(t+1)}\right)_{j}=\Theta(1)\). Repeating the above proof techniques gives us \(\left|w_{2i}^{(T_{1})}\right|=\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\right)\) and \(\left|W_{1}^{(T_{1})}[i,j]\right|=\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\right)\) for \(\forall i,j\in[d]\).

### Proof of Lemma 20

Let's first prove eq. (30).

By Lemma 19, for \(t_{\text{inc}}\leq t<T_{1}\), we have \(\forall i,j\in[d]\), \(\left|g_{1}^{(t)}[i,j]\right|=\tilde{\Omega}\left(\sqrt{\eta}\right),\left|g_ {2i}^{(t)}\right|=\tilde{\Omega}\left(\sqrt{\eta}d\right)\). Then it suffices to show that for \(t_{\text{inc}}\leq t<T_{1}\), \(\left|g_{1}^{(t)}[i,j]-g_{1}^{(t-\tau)}[i,j]\right|=\tau\tilde{\mathcal{O}}(\eta)\) and \(\left|g_{2i}^{(t)}-g_{2i}^{(t-\tau)}\right|=\tau\tilde{\mathcal{O}}(\eta d)\). It suffices to show that when \(t<T_{1}\), \(\left|g_{1}^{(t+1)}[i,j]-g_{1}^{(t)}[i,j]\right|=\tilde{\mathcal{O}}(\eta)\) and \(\left|g_{2i}^{(t+1)}-g_{2i}^{(t)}\right|=\tilde{\mathcal{O}}(\eta d)\).

By Lemma 18 and 23, we know that when \(t<T_{1}\), \(\forall i,j\in[d]\), \(\left|\Delta W_{1}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}(\eta),\left|\Delta w_ {2i}^{(t)}\right|\leq\tilde{\mathcal{O}}(\eta)\) and that \(\left|W_{1}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right), \left|w_{2i}^{(t)}\right|\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\). Then the bound \(\left|\Delta E_{j}^{(t)}\right|\leq\tilde{\mathcal{O}}\left(\eta\sqrt{d}\right)\) in eq. (35) hold for all \(t<T_{1}\) (not only \(t_{\text{inc}}\leq t<T_{1}\)). Substituting these bounds into eq. (36) gives us \(\forall t<T_{1}\),

\[\left|g_{1}^{(t+1)}[i,j]-g_{1}^{(t)}[i,j]\right| \leq\left|w_{2i}^{(t+1)}\right|\left|\Delta E_{j}^{(t)}\right|+ \left|\Delta w_{2i}^{(t)}\right|\left|E_{j}^{(t)}\right|\] \[=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\tilde{\mathcal{O }}\left(\eta\sqrt{d}\right)+\tilde{\Theta}(\eta)\mathcal{O}(1)=\tilde{\mathcal{O }}(\eta).\]

Similarly, we have that \(\left|g_{2i}^{(t+1)}-g_{2i}^{(t)}\right|=\tilde{\mathcal{O}}(\eta d)\), which proves eq. (30).

Note that for \(a,b\in\mathbb{R}\):

\[\frac{\left|a^{2}-b^{2}\right|}{a^{2}}=\frac{\left|a^{2}-(a-b-a)^{2}\right|}{a^ {2}}=\frac{\left|2a(a-b)-(a-b)^{2}\right|}{a^{2}}\leq 2\frac{\left|a-b\right|}{|a|}+ \left(\frac{\left|a-b\right|}{|a|}\right)^{2}.\]Then eq. (31) immediately follows from eq. (30).

### Proof of Lemma 16

We divide Lemma 16 into the following three lemmas. Combining them together immediately gives us the whole proof.

The first lemma below gives us the structure of \(W_{2}\) in the second phase and that of \(W_{1}\) under some conditions.

**Lemma 24**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{13/4}}\). By picking \(\eta\leq\mathcal{O}\left(\frac{1}{d^{3\alpha}}\right),\xi\leq\sqrt{\frac{\eta }{d^{3\alpha-1}}}\), and \(\beta_{2}=\beta_{1}^{2}\), we have w.h.p. for \(T_{1}\leq t<\tilde{T}\),_

\[\forall i\in[d]:\quad w_{2i}^{(t+1)}=w_{2i}^{(t)}-\eta\left(\text{sign}\left(g _{2i}^{(t)}\right)+e_{2i}^{(t)}\right),\quad\text{where }\left|e_{2i}^{(t)}\right|=\tilde{ \mathcal{O}}\left(\sqrt{\eta}\right),\]

_and moreover_

\[\forall i\in[d]:\quad w_{2i}^{(t)}=\text{sign}\left(w_{2i}^{(0)}\right)c^{(t) }+R_{2i}^{(t)},\quad\text{where }\quad\frac{\left|R_{2i}^{(t)}\right|}{c^{(t)}}=\tilde{ \mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-1/2}}\right).\]

_As for \(W_{1}\), if for certain \(i.j\in[d]\) and certain \(t\in[T_{1},\tilde{T})\) we have, then_

\[W_{1}^{(t+1)}[i,j]=W_{1}^{(t)}[i,j]-\eta\left(\text{sign}\left(g_{1}^{(t)}[i,j] \right)+e_{1}^{(t)}[i,j]\right),\quad\text{where }\left|e_{1}^{(t)}[i,j]\right|=\tilde{\mathcal{O}}\left(\sqrt{\eta}\right).\]

The second lemma below also analyzes the structure of \(W_{1}\) but removes the conditions in Lemma 24.

**Lemma 25**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{13/4}}\). By picking \(\eta\leq\mathcal{O}\left(\frac{1}{d^{3\alpha}}\right),\xi\leq\sqrt{\frac{\eta }{d^{3\alpha-1}}}\), and \(\beta_{2}=\beta_{1}^{2}\), we have w.h.p. for \(T_{1}\leq t<\tilde{T}\), \(\forall i,j\in[d]\), \(\left|W_{1}^{(t)}[i,j]\right|=\tilde{\Omega}\left(\frac{1}{\sqrt{d}}\right)\) and for any \(j\in[d]\),_

\[W_{1}^{(t)}[i,j]=\text{sign}\left(w_{2i}^{(0)}\right)V_{j}^{(t)}+R_{1}^{(t)}[ i,j],\quad\text{where }\frac{\left|R_{1}^{(t)}[i,j]\right|}{\left|V_{j}^{(t)}\right|}\leq\tilde{ \mathcal{O}}\left(\eta^{\frac{1}{4}}+\frac{1}{d^{\frac{3}{2}-\frac{1}{4}}} \right).\]

The third lemma proves the convergence of Adam at time \(\tilde{T}\).

**Lemma 26**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{13/4}}\). By picking \(\eta\leq\mathcal{O}\left(\frac{1}{d^{3\alpha}}\right),\xi\leq\sqrt{\frac{\eta }{d^{3\alpha-1}}}\), and \(\beta_{2}=\beta_{1}^{2}\), at time \(\tilde{T}\), we have that w.h.p. \(\forall j\in[d]:\left|E_{j}^{(\tilde{T})}\right|\leq\tilde{\mathcal{O}}\left( d\sqrt{\eta d}\right)\), which implies \(\left\|E^{(\tilde{T})}\right\|_{2}^{2}\leq\tilde{\mathcal{O}}\left(\eta d^{4}\right)\)._

### Proof of Lemma 24

The proof is based on the following lemma, which gives a coarse analysis on the magnitude of weights and their increments per step during the second phase.

**Lemma 27**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{13/4}}\). By picking \(\eta\leq\mathcal{O}\left(\frac{1}{d^{3\alpha}}\right),\xi\leq\sqrt{\frac{\eta }{d^{3\alpha-1}}}\), and \(\beta_{2}=\beta_{1}^{2}\), we have w.h.p. for all \(T_{1}\leq t<\tilde{T}\),_

\[\forall i,j\in[d]:\quad\left|w_{2i}^{(t+1)}\right|>\left|w_{2i}^{(t)}\right|, \quad\left|\Delta w_{2i}^{(t)}\right|=\tilde{\Theta}(\eta),\quad\left|\Delta W _{1}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}(\eta).\]

_Moreover, we have that \(\forall i,j\in[d]:\quad\left|w_{2i}^{(t)}\right|=\tilde{\Theta}\left(\frac{1} {\sqrt{d}}\right),\left|W_{1}^{(t)}[i,j]\right|=\tilde{\mathcal{O}}\left( \frac{1}{\sqrt{d}}\right)\)._

Equipped with Lemma 27, we are ready to prove Lemma 24. We will only prove the results of \(w_{2i}^{(t)}\). The proof for \(W_{1}^{(t)}[i,j]\) uses the same techniques.

Lemma 27 gives us upper bounds of \(\left|w_{2i}^{(t)}\right|,\left|W_{1}^{(t)}[i,j]\right|\), as well as \(\left|\Delta w_{2i}^{(t)}\right|\) and \(\left|\Delta W_{1}^{(t)}[i,j]\right|\) for all \(i,j\in[d]\). Then we know that eq.(35) still holds, which gives us \(\forall j\in[d]:\left|E_{j}^{(t+1)}-E_{j}^{(t)}\right|=\tilde{\mathcal{O}} \left(\sqrt{d}\eta\right)\). Then we can use the same strategy in Lemma 20 to prove that \(\left|g_{2i}^{(t+1)}-g_{2i}^{(t)}\right|=\tilde{\mathcal{O}}(\eta d)\).

By definition, for \(T_{1}\leq t<\tilde{T}\), we know that \(\left|g_{2i}^{(t)}\right|=\Omega\left(d\sqrt{\eta}\right)\). Combining with the bound \(\left|g_{2i}^{(t+1)}-g_{2i}^{(t)}\right|=\tilde{\mathcal{O}}(\eta d)\), we know that the \(g_{2i}^{(t)}\) parts in eq.(30) and eq.(31) still hold. Then we can use the same strategy in Section E.2 to prove that the \(w_{2i}^{(t)}\) part of eq. (29) still holds, which gives us

\[\forall i\in[d]:\quad w_{2i}^{(t+1)}=w_{2i}^{(t)}-\eta\left(\text{sign}\left( g_{2i}^{(t)}\right)+e_{2i}^{(t)}\right),\quad\text{where}\ \left|e_{2i}^{(t)}\right|=\tilde{\mathcal{O}}\left(\sqrt{\eta}\right).\]

By Lemma 15, we have that at the end of the first phase (\(t=T_{1}\)),

\[\forall i\in[d]:\quad w_{2i}^{(T_{1})}=\text{sign}\left(w_{2i}^{(0)}\right)c^ {(T_{1})}+R_{2i}^{(T_{1})},\quad\text{where}\quad\frac{\left|R_{2i}^{(T_{1})} \right|}{c^{(T_{1})}}=\tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha -1/2}}\right).\]

Combining with \(\forall i\in[d],\forall t\leq T_{i}:\text{sign}\left(g_{2i}^{(t)}\right)=- \text{sign}\left(w_{2i}^{(0)}\right)\) yields that during the second phase, for \(t\leq\tilde{T}\), we have

\[\forall i\in[d]:\quad w_{2i}^{(t)}=\text{sign}\left(w_{2i}^{(0)}\right)c^{(t) }+R_{2i}^{(t)},\quad\text{where}\quad\frac{\left|R_{2i}^{(t)}\right|}{c^{(t)} }=\tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-1/2}}\right).\]

### Proof of Lemma 27

By definition of \(T_{f}\), there exists \(j_{0}\in[d]\) such that \(E_{j_{0}}^{(\tau)}<-\sqrt{\eta d}\) for \(T_{1}\leq t\leq\tilde{T}\). We prove by induction that during this period, \(\forall i\in[d]:\text{sign}\left(w_{2i}^{(t)}\right)=\text{sign}\left(W_{1}^{( t)}[i,j_{0}]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\) and that \(\forall i,j\in[d]:\left|w_{2i}^{(t)}\right|=\tilde{\Theta}\left(\frac{1}{ \sqrt{d}}\right),\left|W_{1}^{(t)}[i,j]\right|=\tilde{\mathcal{O}}\left(\frac {1}{\sqrt{d}}\right)\).

The base case (\(t=T_{1}\)) was already proven by Lemma 23. Now suppose for some \(t\) such that \(T_{1}\leq t<\tilde{T}\), for all \(\tau\) such that \(T_{1}\leq\tau\leq t\), we have \(\forall i\in[d]:\text{sign}\left(w_{2i}^{(\tau)}\right)=\text{sign}\left(W_{1 }^{(\tau)}[i,j_{0}]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\) and that \(\forall i,j\in[d]:\left|w_{2i}^{(\tau)}\right|=\tilde{\Theta}\left(\frac{1}{ \sqrt{d}}\right),\left|W_{1}^{(\tau)}[i,j]\right|=\tilde{\mathcal{O}}\left( \frac{1}{\sqrt{d}}\right)\). Using these bounds, we get that \(\forall j\in[d]:\left|E_{j}^{(\tau)}\right|\leq\sum_{i=1}^{d}\left|w_{2i}^{( \tau)}W_{1}^{(\tau)}[i,j]\right|+\left|A_{j}\right|=\mathcal{O}(1)\), which then yields two upper bounds \(\left|g_{1}^{(\tau)}[i,j]\right|=\left|w_{2i}^{(\tau)}E_{j}^{(\tau)}\right|= \tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\) and \(\left|g_{2i}^{(\tau)}\right|\leq\sum_{j=1}^{d}\left|E_{j}^{(\tau)}W_{1}^{( \tau)}[i,j]\right|=\tilde{\mathcal{O}}\left(\sqrt{d}\right)\).

By definition of \(T_{g}\), we know that for all \(T_{1}\leq\tau\leq t\), \(\forall i\in[d]:\left|g_{2i}^{(\tau)}\right|\geq d\sqrt{\eta}=\Omega(\xi)\) and that \(\text{sign}\left(g_{2i}^{(\tau)}\right)=-\text{sign}\left(w_{2i}^{(0)}\right)\), which implies that Condition 2 is satisfied for \(\forall i\in[d]\). At the end of the proof of this lemma, we will show that \(\tilde{T}=\tilde{\Theta}\left(\frac{1}{\sqrt{d}\eta}\right)\). Together with the upper bound of \(\left|g_{2i}^{(\tau)}\right|\), we can apply Lemma 22 to get that w.h.p. for \(T_{1}\leq\tau\leq t\), \(\text{sign}\left(\Delta w_{2i}^{(\tau)}\right)=\text{sign}\left(w_{2i}^{(0)}\right)\) and \(\left|\Delta w_{2i}^{(\tau)}\right|=\tilde{\Theta}(\eta)\). Combining with the inductive hypothesis \(\text{sign}\left(w_{2i}^{(\tau)}\right)=\text{sign}\left(w_{2i}^{(0)}\right)\) gives us that \(\left|w_{2i}^{(\tau+1)}\right|=\left|w_{2i}^{(\tau)}\right|+\tilde{\Theta}(\eta)\). Specially, when \(\tau=t\), we get the lower bound \(\left|w_{2i}^{(t+1)}\right|\geq\left|w_{2i}^{(t)}\right|=\tilde{\Omega}\left( \frac{1}{\sqrt{d}}\right)\) and that \(\text{sign}\left(w_{2i}^{(t+1)}\right)=\text{sign}\left(w_{2i}^{(0)}\right)\).

Since \(E_{j_{0}}^{(\tau)}<-\sqrt{\eta d}\), we have that \(\forall i\in[d]:\left|g_{1}^{(\tau)}[i,j_{0}]\right|=\left|w_{2i}^{(\tau)} \right|\left|E_{j_{0}}^{(\tau)}\right|=\tilde{\Omega}\left(\sqrt{\eta}\right)= \Omega(\xi)\) and that \(\text{sign}\left(g_{1}^{(\tau)}[i,j_{0}]\right)=-\text{sign}\left(w_{2i}^{(0)}\right)\). That means Condition 1 is satisfied for \(\forall i\in[d]\) and \(j_{0}\). Using the same technique as when we deal with \(w_{2i}^{(\tau)}\), we get that for \(T_{1}\leq\tau\leq t\)\(\forall i\in[d]:\left|W_{1}^{(\tau+1)}[i,j_{0}]\right|=\left|W_{1}^{(\tau)}[i,j_{0} ]\right|+\tilde{\Theta}(\eta)\), \(\text{sign}\left(W_{1}^{(t+1)}[i,j_{0}]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\) and that \(\forall i,j\in[d],\left|\Delta W_{1}^{(\tau)}[i,j]\right|=\tilde{\mathcal{O}}(\eta)\).

Now we analyze the magnitude order of \(\left|w_{2i}^{(t+1)}\right|,\left|W_{1}^{(t+1)}[i,j]\right|\). Let's first analyze \(\left|w_{2i}^{(t+1)}\right|\).

By Lemma 15, when \(t=T_{1},\forall i,j\in[d]\),

\[\frac{\left|w_{2i}^{(T_{1})}\right|}{\left|w_{2j}^{(T_{1})}\right|}=1\pm \tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-1/2}}\right),\quad \frac{\left|W_{1}^{(T_{1})}[i,j_{0}]\right|}{\left|w_{2i}^{(T_{1})}\right|}=1 \pm\tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-1/2}}\right).\]

Combining with the facts that for \(T_{1}\leq\tau\leq t\), \(\left|W_{1}^{(\tau+1)}[i,j_{0}]\right|=\left|W_{1}^{(\tau)}[i,j_{0}]\right|+ \tilde{\Theta}(\eta)\) and \(\left|w_{2i}^{(\tau+1)}\right|=\left|w_{2i}^{(\tau)}\right|+\tilde{\Theta}(\eta)\) yields \(\frac{\left|W_{1}^{(t+1)}[i,j_{0}]\right|}{\left|w_{2i}^{(t+1)}\right|}=\tilde {\Theta}(1)\). Since we just proved \(\forall i\in[d]:\text{sign}\left(w_{2i}^{(t+1)}\right)=\text{sign}\left(W_{1 }^{(t+1)}[i,j_{0}]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\), we get that

\[(W_{2}W_{1})_{j_{0}}^{(t+1)}=\sum_{i=1}^{d}w_{2i}^{(t+1)}W_{1}^{(t+1)}[i,j_{0 }]=\sum_{i=1}^{d}\left|w_{2i}^{(t+1)}\right|\left|W_{1}^{(t+1)}[i,j_{0}] \right|=\mathcal{O}(1),\]

which gives us that \(\left|w_{2i}^{(t+1)}\right|=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\). Recall that we have shown \(\left|w_{2i}^{(t+1)}\right|\geq\tilde{\Omega}\left(\frac{1}{\sqrt{d}}\right)\), then \(\left|w_{2i}^{(t+1)}\right|=\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\right)\).

Now we prove \(\left|W_{1}^{(t+1)}[i,j]\right|=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\). We have proved that \(T_{1}\leq\tau\leq t\), \(\forall i,j\in[d]\), \(\left|\Delta W_{1}^{(\tau)}[i,j]\right|=\tilde{\mathcal{O}}(\eta)\) and \(\left|w_{2i}^{(\tau+1)}\right|-\left|w_{2i}^{(\tau)}\right|=\tilde{\Theta}(\eta)\), then \(\forall i,j\in[d]\),

\[\frac{\left|W_{1}^{(t+1)}[i,j]\right|}{\left|w_{2i}^{(t+1)}\right|} \leq\frac{\left|W_{1}^{(T_{1})}[i,j]\right|+\sum_{\tau=T_{1}}^{t} \left|W_{1}^{(\tau+1)}[i,j]-W_{1}^{(\tau)}[i,j]\right|}{\left|w_{2i}^{(T_{1})} \right|+\sum_{\tau=T_{1}}^{t}\left|w_{2i}^{(\tau+1)}\right|-\left|w_{2i}^{( \tau)}\right|}\] \[\leq\frac{\left|W_{1}^{(T_{1})}[i,j]\right|+(t+1-T_{1})\tilde{ \mathcal{O}}(\eta)}{\left|w_{2i}^{(T_{1})}\right|+(t+1-T_{1})\tilde{\Theta}( \eta)}=\tilde{\mathcal{O}}(1),\]

where the last equality uses \(\frac{\left|W_{1}^{(T_{1})}[i,j]\right|}{\left|w_{2i}^{(T_{1})}\right|}=1\pm \tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-1/2}}\right)\). Since we already proved that \(\left|w_{2i}^{(t+1)}\right|=\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\right)\), we get \(\left|W_{1}^{(t+1)}\right|=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\).

Therefore by induction, for all \(t\) in the interval \([T_{1},\tilde{T})\), we have \(\forall i,j\in[d]:\left|w_{2i}^{(t)}\right|=\tilde{\Theta}\left(\frac{1}{\sqrt {d}}\right)\),

\(\left|W_{1}^{(t)}[i,j]\right|=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\). From the proof we also get \(\forall i\in[d]:\left|w_{2i}^{(t+1)}\right|>\left|w_{2i}^{(t)}\right|\), and that \(\left|\Delta w_{2i}^{(t)}\right|=\tilde{\Theta}(\eta),\left|\Delta W_{1}^{(t) }[i,j]\right|\leq\tilde{\mathcal{O}}(\eta)\).

Now we verify that \(\tilde{T}=\tilde{\Theta}\left(\frac{1}{\sqrt{d}\eta}\right)\). Combining \(\forall i,j\in[d]:\left|w_{2i}^{(\tilde{T})}\right|=\tilde{\Theta}\left(\frac{ 1}{\sqrt{d}}\right)\) and \(\forall t\in[T_{1},\tilde{T}),\left|w_{2i}^{(t+1)}\right|-\left|w_{2i}^{(t)} \right|=\tilde{\Theta}(\eta)\), we immediately get that \(\tilde{T}-T_{1}=\tilde{\Theta}\left(\frac{1}{\sqrt{d}\eta}\right)\). In Section E.2 we have shown that \(T_{1}=\Theta\left(\frac{1}{\sqrt{d}\eta}\right)\), then we get \(\tilde{T}=\tilde{\Theta}\left(\frac{1}{\sqrt{d}\eta}\right)\).

### Proof of Lemma 25

We prove this lemma by induction. The base case (\(t=T_{1}\)) can be verified by Lemma 15. Now suppose for \(t\) in the interval \([T_{1},\tilde{T})\), we have \(\forall i,j\in[d]\), \(\left|W_{1}^{(t)}[i,j]\right|=\tilde{\Omega}\left(\frac{1}{\sqrt{d}}\right)\).

For \(t\in[T_{1},\tilde{T})\), by the proof of Lemma 27 (Section E.13), we know that for \(\forall\tau\leq t\), \(\forall i,j\in[d]:\left|w_{2i}^{(\tau)}\right|=\tilde{\mathcal{O}}\left(\frac{1 }{\sqrt{d}}\right),\left|W_{1}^{(\tau)}[i,j]\right|=\tilde{\mathcal{O}}\left( \frac{1}{\sqrt{d}}\right)\) and that \(\left|g_{1}^{(\tau)}[i,j]\right|\leq\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d }}\right),\left|g_{2i}^{(\tau)}\right|\leq\tilde{\mathcal{O}}\left(\sqrt{d}\right)\), and that \(\tilde{T}_{1}=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d_{B}}}\right)\). Then we can pick \(H:=\frac{1}{1-\beta_{1}}\log\frac{d}{\pi\xi^{2}}\) and apply Lemma 17 and Corollary 2 to get that, w.h.p., for all \(t\in[T_{1},\tilde{T})\) and \(\forall i,j\in[d]\), the update of \(W_{1}\) can be written as

\[W_{1}^{(t+1)}[i,j]=W_{1}^{(t)}[i,j]-\eta_{t}\frac{(1-\beta_{1}) \sum_{\tau=0}^{H}\beta_{1}^{\tau}g_{1}^{(t-\tau)}[i,j]+\epsilon_{1n}^{(t)}[i, j]}{\sqrt{(1-\beta_{2})\sum_{\tau=0}^{H}\beta_{2}^{\tau}\left(g_{1}^{(t- \tau)}[i,j]\right)^{2}+\epsilon_{1d}^{(t)}[i,j]+\xi}},\]

where \(\left|\epsilon_{1n}^{(t)}[i,j]\right|,\left|\epsilon_{1d}^{(t)}[i,j]\right| \leq\tilde{\mathcal{O}}(\eta\xi^{2})\). By Lemma 24, we have that for \(1\leq i,j\leq d\),

\[g_{1}^{(t)}[i,j]=w_{2i}^{(t)}E_{j}^{(t)}=c^{(t)}\text{sign}\left(w_{2i}^{(0)} \right)E_{j}^{(t)}+R_{g,1}^{(t)}[i,j],\quad\frac{\left|R_{g,1}^{(t)}[i,j] \right|}{c^{(t)}\left|E_{j}^{(t)}\right|}=\tilde{\mathcal{O}}\left(\sqrt{\eta }+\frac{1}{d^{\alpha-1/2}}\right),\]

\[\Rightarrow\quad\sum_{\tau=0}^{H}\beta_{1}^{\tau}g_{1}^{(t-\tau)}[i,j]=\text {sign}\left(w_{2i}^{(0)}\right)\sum_{\tau=0}^{H}\beta_{1}^{\tau}c^{(t-\tau)}E _{j}^{(t-\tau)}+\sum_{\tau=0}^{H}\beta_{1}^{\tau}R_{g,1}^{(t-\tau)}[i,j].\] (37)

Using the fact that for \(a,b\in\mathbb{R},\frac{\left|a^{2}-b^{2}\right|}{a^{2}}\leq 2\frac{\left|a-b \right|}{\left|a\right|}+\left(\frac{\left|a-b\right|}{\left|a\right|}\right)^ {2}\), we get that

\[\left(g_{1}^{(t)}[i,j]\right)^{2}=\left(c^{(t)}\text{sign}\left(w_{2i}^{(0)} \right)E_{j}^{(t)}+R_{g,1}^{(t)}[i,j]\right)^{2}:=\left(c^{(t)}E_{j}^{(t)} \right)^{2}+R_{\text{gsqr},1}^{(t)}[i,j],\]

where \(\frac{\left|R_{\text{gsqr},1}^{(t)}[i,j]\right|}{\left(c^{(t)}E_{j}^{(t)} \right)^{2}}=\tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-1/2}}\right)\). That yields

\[\sum_{\tau=0}^{H}\beta_{2}^{\tau}\left(g_{1}^{(t-\tau)}[i,j]\right)^{2}=\sum_ {\tau=0}^{H}\beta_{2}^{\tau}\left(c^{(t-\tau)}E_{j}^{(t-\tau)}\right)^{2}+ \sum_{\tau=0}^{H}\beta_{2}^{\tau}R_{\text{gsqr},1}^{(t-\tau)}[i,j].\] (38)

Since \(\left(c^{(t-\tau)}E_{j}^{(t-\tau)}\right)^{2}>0\), in eq. (38) we have that

\[\frac{\left|\sum_{\tau=0}^{H}\beta_{2}^{\tau}R_{\text{gsqr},1}^{(t-\tau)}[i,j] \right|}{\left|\sum_{\tau=0}^{H}\beta_{2}^{\tau}\left(c^{(t-\tau)}E_{j}^{(t- \tau)}\right)^{2}\right|}=\tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{ \alpha-1/2}}\right).\] (39)

However we cannot similarly prove that \(\left|\sum_{\tau=0}^{H}\beta_{1}^{\tau}R_{g,1}^{(t-\tau)}[i,j]\right|\ll \left|\sum_{\tau=0}^{H}\beta_{1}^{\tau}c^{(t-\tau)}E_{j}^{(t-\tau)}\right|\) in eq. (37) because \(c^{(t-\tau)}E_{j}^{(t-\tau)}\) may not have the same sign for \(\tau=0,1,...,H\). To deal with eq.(37), we need to consider the two cases where \(\left|\sum_{\tau=0}^{H}\beta_{1}^{\tau}R_{g,1}^{(t-\tau)}[i,j]\right|\ll\left| \sum_{\tau=0}^{H}\beta_{1}^{\tau}c^{(t-\tau)}E_{j}^{(t-\tau)}\right|\) or \(\left|\sum_{\tau=0}^{H}\beta_{1}^{\tau}R_{g,1}^{(t-\tau)}[i,j]\right|\ll\left| \sum_{\tau=0}^{H}\beta_{1}^{\tau}c^{(t-\tau)}E_{j}^{(t-\tau)}\right|\).

Case 1.\(\left|(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}R_{g,1}^{(t-\tau)}[i,j]+ \epsilon_{1n}^{(t)}[i,j]\right|\leq\delta\left|(1-\beta_{1})\sum_{\tau=0}^{H} \beta_{1}^{\tau}c^{(t-\tau)}E_{j}^{(t-\tau)}\right|\) where \(\delta=\left(\eta^{\frac{1}{4}}+\frac{1}{d^{\frac{\alpha}{2}-\frac{1}{4}}}\right)\).

Note that from eq. (39) we have

\[\left|(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{2}^{\tau}R_{\text{gsqr},1}^{(t-\tau)}[i,j]\right|\leq\tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-1/2}} \right)\left|(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{2}^{\tau}\left(c^{(t-\tau)}E_{ j}^{(t-\tau)}\right)^{2}\right|.\]Combining with \(\left|\epsilon_{1d}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}(\eta\xi^{2})\leq \tilde{\mathcal{O}}\left(\eta^{\frac{1}{4}}+\frac{1}{d^{\frac{9}{2}-\frac{1}{4} }}\right)^{2}\xi^{2}\), we can apply Lemma 34 to get that

\[W_{1}^{(t+1)}[i,j]-W_{1}^{(t)}[i,j]\] \[= -\eta_{t}\frac{(1-\beta_{1})\text{sign}\left(w_{2i}^{(0)}\right) \sum_{\tau=0}^{H}\beta_{1}^{\tau}c^{(t-\tau)}E_{j}^{(t-\tau)}+(1-\beta_{1}) \sum_{\tau=0}^{H}\beta_{1}^{\tau}R_{g,1}^{(t-\tau)}[i,j]+\epsilon_{1n}^{(t)}[ i,j]}{\sqrt{(1-\beta_{2})\sum_{\tau=0}^{H}\beta_{2}^{\tau}\left(c^{(t-\tau)}E_{j}^{(t- \tau)}\right)^{2}+(1-\beta_{2})\sum_{\tau=0}^{H}\beta_{2}^{\tau}R_{gsqr,1}^{(t -\tau)}[i,j]+\epsilon_{1d}^{(t)}[i,j]+\xi}\] \[= -\eta_{t}\frac{1-\beta_{1}}{\sqrt{1-\beta_{2}}}\cdot\frac{\text{ sign}\left(w_{2i}^{(0)}\right)\sum_{\tau=0}^{H}\beta_{1}^{\tau}c^{(t-\tau)}E_{j}^{(t- \tau)}}{\sqrt{\sum_{\tau=0}^{H}\beta_{2}^{\tau}\left(c^{(t-\tau)}E_{j}^{(t- \tau)}\right)^{2}+\xi}}\left(1+e_{1}^{(t)}[i,j]\right)\] \[:= -\text{sign}\left(w_{2i}^{(0)}\right)v_{j}^{(t)}\left(1+e_{1}^{(t )}[i,j]\right),\]

where \(\left|e_{1}^{(t)}[i,j]\right|=\tilde{\mathcal{O}}\left(\eta^{\frac{1}{4}}+ \frac{1}{d^{\frac{9}{2}-\frac{1}{4}}}\right)\). Since \(\left|W_{1}^{(t+1)}[i,j]-W_{1}^{(t)}[i,j]\right|=\tilde{\mathcal{O}}(\eta)\), we get that \(\left|v_{j}^{(t)}\right|=\tilde{\mathcal{O}}(\eta)\).

Case 2.\(\left|(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}R_{g,1}^{(t-\tau)}[i,j]+ \epsilon_{1n}^{(t)}[i,j]\right|>\delta\left|(1-\beta_{1})\sum_{\tau=0}^{H} \beta_{1}^{\tau}c^{(t-\tau)}E_{j}^{(t-\tau)}\right|\) where \(\delta=\left(\eta^{\frac{1}{4}}+\frac{1}{d^{\frac{9}{2}-\frac{1}{4}}}\right)\).

Since \(\frac{\left|R_{g,1}^{(t)}[i,j]\right|}{c^{(t)}\left|E_{j}^{(t)}\right|}=\tilde {\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-1/2}}\right)\), we have that

\[\left|(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}R_{g,1}^{(t- \tau)}[i,j]\right| \leq\tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-1/2} }\right)(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}\left|c^{(t-\tau)}E_{j} ^{(t-\tau)}\right|\] \[\overset{(i)}{\leq}\tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1} {d^{\alpha-1/2}}\right)\sqrt{(H+1)(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{2}^{ \tau}\left(c^{(t-\tau)}E_{j}^{(t-\tau)}\right)^{2}}\] \[\overset{(ii)}{=}\tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^ {\alpha-1/2}}\right)\sqrt{(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{2}^{\tau} \left(g_{1}^{(t-\tau)}[i,j]\right)^{2}},\]

where \((i)\) uses Cauchy-Schwarz inequality and \(\beta_{2}=\beta_{1}^{2}\), \((ii)\) uses eq. (38) and (39).

Combining with \(\left|\epsilon_{1n}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}(\eta\xi^{2})\leq \tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-1/2}}\right)\left( \xi-\sqrt{\left|\epsilon_{1d}^{(t)}[i,j]\right|}\right)\) gives us

\[\left|(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{\tau}c^{(t-\tau)} E_{j}^{(t-\tau)}\right|<\frac{\left|(1-\beta_{1})\sum_{\tau=0}^{H}\beta_{1}^{ \tau}R_{g,1}^{(t-\tau)}[i,j]+\epsilon_{1n}^{(t)}[i,j]\right|}{\eta^{\frac{1}{4} }+\frac{1}{d^{\frac{9}{2}-\frac{1}{4}}}}\] \[\leq \frac{\tilde{\mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-1/2 }}\right)}{\eta^{\frac{1}{4}}+\frac{1}{d^{\frac{9}{2}-\frac{1}{4}}}}\left( \sqrt{\sum_{\tau=0}^{H}\beta_{2}^{\tau}\left(g_{1}^{(t-\tau)}[i,j]\right)^{2}}- \sqrt{\left|\epsilon_{1d}^{(t)}[i,j]\right|}+\xi\right)\] \[\leq \tilde{\mathcal{O}}\left(\eta^{\frac{1}{4}}+\frac{1}{d^{\frac{9} {2}-\frac{1}{4}}}\right)\left(\sqrt{\sum_{\tau=0}^{H}\beta_{2}^{\tau}\left(g_{1} ^{(t-\tau)}[i,j]\right)^{2}}+\epsilon_{1d}^{(t)}[i,j]+\xi\right),\]

which implies

\[\left|W_{1}^{(t+1)}[i,j]-W_{1}^{(t)}[i,j]\right|\leq\eta\tilde{\mathcal{O}} \left(\eta^{\frac{1}{4}}+\frac{1}{d^{\frac{9}{2}-\frac{1}{4}}}\right).\]Consider certain \(i,j\in[d]\) and the period from \(T_{1}\) to \(t\). Denote \(\mathcal{T}\) as the set of time points when Case 1 is satisfied. By Lemma 27, we know that \(\eta(t-T_{1})=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\), which gives us

\[\sum_{\tau\notin\mathcal{T}}\Delta W_{1}^{(\tau)}[i,j]\leq(t-T_{1})\eta \tilde{\mathcal{O}}\left(\eta^{\frac{1}{4}}+\frac{1}{d^{\frac{\alpha}{2}- \frac{1}{4}}}\right)=\tilde{\mathcal{O}}\left(\frac{\eta^{\frac{1}{4}}}{d^{ \frac{1}{2}}}+\frac{1}{d^{\frac{\alpha}{2}+\frac{1}{4}}}\right).\]

By the first phase analysis, we have that

\[W_{1}^{(T_{1})}[i,j]=\text{sign}\left(w_{2i}^{(0)}\right)V_{j}^{(T_{1})}+R_{1}^ {(T_{1})}[i,j],\]

where \(V_{j}^{(T_{1})}=\mathcal{O}\left(\frac{1}{\sqrt{d}}\right),\left|R_{1}^{(T_{1 })}[i,j]\right|=\tilde{\mathcal{O}}\left(\sqrt{\frac{\eta}{d}}+\frac{1}{d^{ \alpha}}\right)\). Combining with the analysis of Case 1, we have that

\[W_{1}^{(T_{1})}[i,j]+\sum_{\tau\in\mathcal{T}}\Delta W_{1}^{(\tau)}[i,j]=\text {sign}\left(w_{2i}^{(0)}\right)\left(V_{j}^{(T_{1})}-\sum_{\tau\in\mathcal{T} }v_{j}^{(\tau)}\right)+R_{\mathcal{T}}[i,j],\]

where \(\left|R_{\mathcal{T}}[i,j]\right|\leq\mathcal{O}\left(\eta^{\frac{1}{4}}+ \frac{1}{d^{\frac{\alpha}{2}-\frac{1}{4}}}\right)\left(\left|V_{j}^{(T_{1})} \right|+\sum_{\tau\in\mathcal{T}}\left|v_{j}^{(\tau)}\right|\right)\).

Since for \(\tau\in\mathcal{T},\left|v_{j}^{(\tau)}\right|=\tilde{\mathcal{O}}(\eta)\), \(V_{j}^{(T_{1})}=\mathcal{O}\left(\frac{1}{\sqrt{d}}\right)\) and \(\eta(t-T_{1})=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\), we can bound \(\left|R_{\mathcal{T}}[i,j]\right|\) by

Combining the above results together yields

\[W_{1}^{(t)}[i,j] =W_{1}^{(T_{1})}+\sum_{\tau=T_{1}}^{t-1}\Delta W_{1}^{(\tau)}[i,j]=W_{1}^{(T_{1})}+\sum_{\tau\in\mathcal{T}}\Delta W_{1}^{(\tau)}[i,j]+\sum_ {\tau\notin\mathcal{T}}\Delta W_{1}^{(\tau)}[i,j]\] \[=\text{sign}\left(w_{2i}^{(0)}\right)\left(V_{j}^{(T_{1})}-\sum_ {\tau\in\mathcal{T}}v_{j}^{(\tau)}\right)+\tilde{\mathcal{O}}\left(\frac{\eta ^{\frac{1}{4}}}{d^{\frac{\alpha}{2}}}+\frac{1}{d^{\frac{\alpha}{2}+\frac{1}{4 }}}\right)\] \[:=\text{sign}\left(w_{2i}^{(0)}\right)V_{j}^{(t)}+R_{1}^{(t)}[i,j ],\text{ where }\left|R_{1}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}\left(\frac{\eta^{ \frac{1}{4}}}{d^{\frac{\alpha}{2}}}+\frac{1}{d^{\frac{\alpha}{2}+\frac{1}{4} }}\right).\]

By the inductive hypothesis \(\left|W_{1}^{(t)}[i,j]\right|=\tilde{\Omega}\left(\frac{1}{\sqrt{d}}\right)\), we get that \(\left|V_{j}^{(t)}\right|=\tilde{\Omega}\left(\frac{1}{\sqrt{d}}\right)\), which gives us \(\frac{\left|R_{1}^{(t)}[i,j]\right|}{\left|V_{j}^{(t)}\right|}\leq\tilde{ \mathcal{O}}\left(\eta^{\frac{1}{4}}+\frac{1}{d^{\frac{\alpha}{2}-\frac{1}{4} }}\right)\).

Therefore, we have that for any \(j\in[d]\) and any \(i_{1},i_{2}\in[d]\),

By Lemma 24, we know that \(\left|w_{2i}^{(t)}\right|\) with different \(i\) are also roughly equal, i.e. \(\frac{\left|w_{2i}^{(t)}\right|}{\left|w_{2i_{2}}^{(t)}\right|}=1\pm\tilde{ \mathcal{O}}\left(\sqrt{\eta}+\frac{1}{d^{\alpha-1/2}}\right)\). Then we have for any \(j\in[d]\),

\[\left(W_{2}W_{1}\right)_{j}^{(t)}=\sum_{i=1}^{d}w_{2i}^{(t)}W_{1 }^{(t)}[i,j]=\sum_{i=1}^{d}\left|w_{2i}^{(t)}\right|\left|W_{1}^{(t)}[i,j]\right| =\Theta\left(d\left|w_{2k}^{(t)}\right|\left|W_{1}^{(t)}[k,j] \right|\right)\] \[=\tilde{\Theta}\left(\sqrt{d}\left|W_{1}^{(t)}[k,j]\right|\right).\]

where \(k\) can be any index in \(\{1,2,...,d\}\) and the last equality uses \(\forall i\in[d]:\left|w_{2i}^{(t)}\right|=\tilde{\Theta}\left(\frac{1}{\sqrt{d }}\right)\).

Now we analyze the lower bound of \(\left|W_{1}^{(t+1)}[k,j]\right|\). Although it may decrease during some period, we observe that once \(\left|W_{1}^{(t)}[k,j]\right|\) decreases to some value of order \(\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\right)\) such that \(\left(W_{2}W_{1}\right)_{j}^{(t)}<A_{j}-\sqrt{\eta d}\), i.e. \(E_{j}^{(t)}<-\sqrt{\eta d}\), we can apply the technique in Section E.13 when analyzing \(W_{1}^{(t)}[i,j_{0}]\) to get that \(\left|W_{1}^{(t)}[k,j]\right|\) will increase in the next step. This mechanism ensures a \(\tilde{\Omega}\left(\frac{1}{\sqrt{d}}\right)\) lower bound of \(\left|W_{1}^{(t+1)}[k,j]\right|\). Since \(k,j\) are arbitrary, we have proved that at time \(t+1\), \(\forall i,j\in[d]:\left|W_{1}^{(t+1)}[i,j]\right|\geq\tilde{\Omega}\left(\frac {1}{\sqrt{d}}\right)\).

Therefore by induction, we conclude that when \(T_{1}\leq t<\tilde{T}\), for \(\forall i,j\in[d]\), \(\left|W_{1}^{(t)}[i,j]\right|=\tilde{\Omega}\left(\frac{1}{\sqrt{d}}\right)\). The remaining part of this lemma has also been proved by the analysis above.

### Proof of Lemma 26

Lemma 27 tells us that for any \(i\in[d]\), \(\left|w_{2i}^{(t)}\right|\) keeps increasing when \(t<\tilde{T}\). However, the behavior of \(W_{1}^{(t)}[i,j]\) is more complicated. The following lemma tells us that \(\left|W_{1}^{(t)}[i,j]\right|\) will increase until \(T_{f,j}\). After that \(\left|W_{1}^{(t)}[i,j]\right|\) and \(E_{j}^{(t)}\) may zigzag, but \(E_{j}^{(t)}\) will not fluctuate dramatically and will be trapped in a small interval around zero.

**Lemma 28**.: _Under Assumption 1, 2 and 3, suppose \(\sigma\leq\frac{\eta^{3/2}\xi^{2}}{d^{13/4}}\). Pick \(\eta\leq\mathcal{O}\left(\frac{1}{d^{3\alpha}}\right),\xi\leq\sqrt{\frac{\eta} {d^{3\alpha-1}}}\), and \(\beta_{2}=\beta_{1}^{2}\). Consider certain coordinate \(j\). For \(T_{1}\leq t<\min\left\{\tilde{T},T_{f,j}\right\}\), we have \(\forall i\in[d]:\left|W_{1}^{(t)}[i,j]\right|\) keeps increasing. If \(T_{f,j}<\tilde{T}\), then for \(T_{f,j}\leq t<\tilde{T}\), we will have \(-\tilde{\mathcal{O}}\left(\sqrt{\eta d}\right)\leq E_{j}^{(t)}\leq\tilde{ \mathcal{O}}\left(\sqrt{\eta d}\right)\)._

Now we start proving Lemma 26. At time \(\tilde{T}\), denote \(S:=\left\{j:T_{f,j}<\tilde{T}\right\}\), i.e. the set of coordinates whose \(E_{j}\) have passed its "flip time". By Lemma 28, we know that \(\forall j\in S,\left|E_{j}^{(\tilde{T})}\right|\leq\tilde{\mathcal{O}}\left( \sqrt{\eta d}\right)\). If \(S^{c}=\phi\), which means \(\forall j\in[d]:\left|E_{j}^{(\tilde{T})}\right|\leq\tilde{\mathcal{O}}\left( \sqrt{\eta d}\right)\), then our lemma will immediately follow. If \(S^{c}\neq\phi\), we have \(\tilde{T}=\min\left\{T_{g},T_{f}\right\}=T_{g}\) and that \(\forall j\in S^{c}:E_{j}^{(\tilde{T})}<0\). By the definition of \(T_{g}\), we know that \(\exists i_{0}\in[d]:\left|g_{2i_{0}}^{(\tilde{T})}\right|\leq\mathcal{O}\left( d\sqrt{\eta}\right)\). Then

\[\left|\sum_{j\in S^{c}}^{d}E_{j}^{(\tilde{T})}W_{1}^{(\tilde{T}) }[i_{0},j]\right|=\left|\sum_{j=1}^{d}E_{j}^{(\tilde{T})}W_{1}^{(\tilde{T})}[ i_{0},j]-\sum_{j\in S}E_{j}^{(\tilde{T})}W_{1}^{(\tilde{T})}[i_{0},j]\right|\] \[\leq \left|g_{2i_{0}}^{(\tilde{T})}\right|+\left|\sum_{j\in S}E_{j}^{ (\tilde{T})}W_{1}^{(\tilde{T})}[i_{0},j]\right|\leq\mathcal{O}\left(d\sqrt{ \eta}\right)+d\tilde{\mathcal{O}}\left(\sqrt{\eta d}\right)\tilde{\mathcal{O}} \left(\frac{1}{\sqrt{d}}\right)=\tilde{\mathcal{O}}\left(d\sqrt{\eta} \right).\]

By Lemma 25, we know that when \(T_{1}\leq t<\tilde{T}\), for \(\forall i,j\in[d]\), \(\left|W_{1}^{(t)}[i,j]\right|=\tilde{\Omega}\left(\frac{1}{\sqrt{d}}\right)\). Since the update per step \(\left|\Delta W_{1}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}(\eta)\), we know that \(\text{sign}\left(W_{1}^{(t)}[i,j]\right)\) remains unchanged during this period and \(\text{sign}\left(W_{1}^{(t)}[i,j]\right)=\text{sign}\left(W_{1}^{(\tilde{T})}[ i,j]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\) independent of \(j\). Combining with \(\forall j\in S^{c}:E_{j}^{(\tilde{T})}<0\) gives us that \(E_{j}^{(\tilde{T})}W_{1}^{(\tilde{T})}[i_{0},j]\) for different \(j\) have the same sign. Therefore for any \(j_{0}\in S^{c}\),

\[\tilde{\mathcal{O}}\left(d\sqrt{\eta}\right)\geq\left|\sum_{j\in S ^{c}}^{d}E_{j}^{(\tilde{T})}W_{1}^{(\tilde{T})}[i_{0},j]\right| =\sum_{j\in S^{c}}^{d}\left|E_{j}^{(\tilde{T})}W_{1}^{(\tilde{T})}[ i_{0},j]\right|\geq\left|E_{j_{0}}^{(\tilde{T})}W_{1}^{(\tilde{T})}[i_{0},j_{0}]\right|\] \[\geq\left|E_{j_{0}}^{(\tilde{T})}\right|\tilde{\Omega}\left(\frac {1}{\sqrt{d}}\right)\quad\Rightarrow\left|E_{j_{0}}^{(\tilde{T})}\right|\leq \tilde{\mathcal{O}}\left(d\sqrt{\eta d}\right).\]Note that the above inequality holds for any \(j_{0}\in S^{c}\), which means \(\forall j\in S^{c}:\left|E_{j}^{(\tilde{T})}\right|\leq\tilde{\mathcal{O}}\left(d \sqrt{\eta d}\right)\). Combining with the fact that \(\forall j\in S:\left|E_{j}^{(\tilde{T})}\right|\leq\tilde{\mathcal{O}}\left(d \sqrt{\eta d}\right)\) completes the proof.

### Proof of Lemma 28

Consider certain \(j\in[d],\) when \(t<\min\left\{\tilde{T},T_{f,j}\right\}\), we have that \(E_{j}^{(t)}<-\sqrt{\eta d}\). Therefore we can use the same argument as in Section E.13 to prove that \(\left|W_{1}^{(t)}[i,j]\right|\) keeps increasing, and \(\text{sign}\left(W_{1}^{(t)}[i,j]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\) for all \(i\in[d]\).

At time the "flip time" \(t=T_{f,j}\), by definition, \(E_{j}^{(t)}\geq-\sqrt{\eta d}\). After that \(E_{j}^{(t)}\) may oscillate. Now we prove that once \(E_{j}^{(t)}\geq\sqrt{\eta d}\) (or \(E_{j}^{(t)}\leq-\sqrt{\eta d}\)), after a short period \(E_{j}^{(t)}\) will decrease (or increase) until \(E_{j}^{(t)}\leq\tilde{\mathcal{O}}\left(\sqrt{\eta d}\right)\) (or \(E_{j}^{(t)}\geq-\tilde{\mathcal{O}}\left(\sqrt{\eta d}\right)\)). Moreover, during this period, \(E_{j}^{(t)}\) won't change too much.

We first recall that when \(T_{1}\leq t<\tilde{T}\), Lemma 27 gives us for all \(i\in[d]\), \(\left|w_{2i}^{(t)}\right|=\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\right)\) and \(\left|W_{1}^{(t)}[i,j]\right|=\tilde{\mathcal{O}}\left(\frac{1}{\sqrt{d}}\right)\). Then eq.(35) we obtained in the first phase analysis still holds, which tells us that the change of \(E_{j}^{(t)}\) per step satisfies \(\left|E_{j}^{(t+1)}-E_{j}^{(t)}\right|=\tilde{\mathcal{O}}\left(\eta\sqrt{d}\right)\) for all \(T_{1}\leq t<\tilde{T}\).

We divide the analysis into two cases, based on whether these \(E_{j}^{(t)}\geq\sqrt{\eta d}\) or \(E_{j}^{(t)}\leq-\sqrt{\eta d}\). By Lemma 25, we know that when \(T_{1}\leq t<\tilde{T}\), \(\forall i\in[d]\), \(\left|W_{1}^{(t)}[i,j]\right|=\tilde{\Omega}\left(\frac{1}{\sqrt{d}}\right)\). Since the update per step \(\left|\Delta W_{1}^{(t)}[i,j]\right|\leq\tilde{\mathcal{O}}(\eta)\), we know that \(\text{sign}\left(W_{1}^{(t)}[i,j]\right)\) remains unchanged during this period and \(\text{sign}\left(W_{1}^{(t)}[i,j]\right)=\text{sign}\left(W_{1}^{(T_{1})}[i,j ]\right)=\text{sign}\left(w_{2i}^{(0)}\right)\).

By the analysis of \(w_{2i}^{(t)}\) in Lemma 24, we have for all \(i\in[d]\), \(w_{2i}^{(t+1)}=w_{2i}^{(t)}+\text{sign}\left(w_{2i}^{(0)}\right)\Delta_{2i}^{ (t)}\), where \(\Delta_{2i}^{(t)}=\eta\left(1\pm\tilde{\mathcal{O}}\left(\sqrt{\eta}\right)\right)\).

Case 1.Consider some time point \(t\) such that \(E_{j}^{(t)}\leq-\sqrt{\eta d}\). Note that for all \(i\in[d]\), \(\left|g_{1}^{(t)}[i,j]\right|=\left|w_{2i}^{(t)}E_{j}^{(t)}\right|=\tilde{ \Omega}\left(\sqrt{\eta}\right)\) and that \(\text{sign}\left(g_{1}^{(t)}[i,j]\right)=-\text{sign}\left(w_{2i}^{(t)}\right) =-\text{sign}\left(w_{2i}^{(0)}\right)\). By Lemma 24, for all \(i\in[d]\) we have \(W_{1}^{(t+1)}[i,j]=W_{1}^{(t+1)}[i,j]+\text{sign}\left(w_{2i}^{(0)}\right) \Delta_{1}^{(t)}[i,j]\) with \(\Delta_{1}^{(t)}[i,j]=\eta\left(1\pm\tilde{\mathcal{O}}\left(\sqrt{\eta}\right)\right)\). That gives us

\[E_{j}^{(t+1)}=\sum_{i=1}^{d}w_{2i}^{(t+1)}W_{1}^{(t+1)}[i,j]-A_{j}\] \[= \sum_{i=1}^{d}\left(w_{2i}^{(t)}+\text{sign}\left(w_{2i}^{(0)} \right)\Delta_{2i}^{(t)}\right)\left(W_{1}^{(t)}[i,j]+\text{sign}\left(w_{2i}^ {(0)}\right)\Delta_{1}^{(t)}[i,j]\right)-A_{j}\] \[= \sum_{i=1}^{d}\left(w_{2i}^{(t)}W_{1}^{(t)}[i,j]+\text{sign}\left( w_{2i}^{(0)}\right)\left(w_{2i}^{(t)}\Delta_{1}^{(t)}[i,j]+\Delta_{2i}^{(t)}W_{1}^{(t )}[i,j]\right)+\Delta_{2i}^{(t)}\Delta_{1}^{(t)}[i,j]\right)-A_{j}\] \[\stackrel{{(i)}}{{=}} E_{j}^{(t)}+\sum_{i=1}^{d}\left(\left|w_{2i}^{(t)}\left| \Delta_{1}^{(t)}[i,j]+\Delta_{2i}^{(t)}\left|W_{1}^{(t)}[i,j]\right|+\Delta_{2i }^{(t)}\Delta_{1}^{(t)}[i,j]\right),\] \[\Rightarrow E_{j}^{(t+1)}>E_{j}^{(t)},\]

[MISSING_PAGE_EMPTY:66]

Combining with the fact that for all \(T_{1}\leq\tau\leq\tilde{T}\), \(\left|E_{j}^{(\tau+1)}-E_{j}^{(\tau)}\right|=\tilde{\mathcal{O}}\left(\eta\sqrt{ d}\right)\) gives us

\[E_{j}^{(t+t_{s})}-E_{j}^{(t)}\leq\tilde{\mathcal{O}}\left(\eta t_{s}\sqrt{d} \right)=\tilde{\mathcal{O}}\left(\sqrt{\eta d}\right).\]

For ii), we reach \(\tilde{T}\) before \(\forall i\in[d]:\left|w_{2i}^{(t+t_{s})}\right|\geq\left|W_{1}^{(t+t_{s})}[i,j ]\right|+\sqrt{\eta}\). Then we have \(\tilde{T}-t\leq\sqrt{\eta}/\tilde{\Omega}(\eta)=\tilde{\mathcal{O}}\left( \frac{1}{\sqrt{\eta}}\right)\), which yields \(E_{j}^{(\tilde{T})}-E_{j}^{(t)}\leq\tilde{\mathcal{O}}\left(\eta(\tilde{T}-t )\sqrt{d}\right)\leq\tilde{\mathcal{O}}\left(\sqrt{\eta d}\right)\).

Combining the above two cases, we find that if for some \(t\), \(E_{j}^{(t)}\geq\sqrt{\eta d}\), then after at most \(t_{s}\) steps \(E_{j}\) will decrease and keeps decreasing until \(E_{j}<\sqrt{\eta d}\) or we reach \(\tilde{T}\). During these steps, \(E_{j}\) can increase at most \(\tilde{\mathcal{O}}\left(\sqrt{\eta d}\right)\). If for some \(t\), \(E_{j}^{(t)}\leq-\sqrt{\eta d}\), then after one step it will increase and keeps increasing until \(E_{j}>\sqrt{\eta d}\) or we reach \(\tilde{T}\). That means once for some coordinate \(j\), \(E_{j}\) overshoots, it will zigzag in a small region around zero, which is \(\left[-\tilde{\mathcal{O}}\left(\sqrt{\eta d}\right),\tilde{\mathcal{O}}\left( \sqrt{\eta d}\right)\right]\).

## Appendix F Hessian tends to become more and more diagonal during training

In this section, we empirically demonstrate that the trend of loss Hessian in practice is to become more and more diagonal during training. We also give a rigorous theoretical analysis on a two-layer network under Assumption 1 and 2.

### Empirical Results

Let's first define the diagonal domination of the \(i\)-th coordinate at time \(t\).

\[r_{\text{diag},i}^{\text{OPT}}(t):=\frac{\sqrt{\sum_{j\neq i}\left(H^{(t)}[i,j ]\right)^{2}}}{\left|H^{(t)}[i,i]\right|}.\]

To measure the diagonal domination of the whole Hessian, we need to consider the distribution of \(r_{\text{diag},i}^{\text{OPT}}(t)\) for different \(i\). Figure 15 shows the mean and median of \(r_{\text{diag},i}^{\text{SGDM}}(t)\) and \(r_{\text{diag},i}^{\text{Adam}}(t)\) on the sentence classification task (see Section 4.1). Here we chose 4 layers (Layer #6, 12, 17 and 22) and computed the Hessians across these 4 layers. Since the number of parameters is very large, we did the computation by random sampling. As we can see, for both \(r_{\text{diag},i}^{\text{SGDM}}(t)\) and \(r_{\text{diag},i}^{\text{Adam}}(t)\), the trend of their mean or median is to decrease over time, although there might be some oscillation.

### Theoretical Analysis

To simplify the theoretical analysis, we consider the mean of \(r_{\text{diag},i}^{\text{OPT}}(t)\) over all coordinate and define

\[R_{\text{diag}}^{\text{OPT}}(t):=\text{mean}\left(r_{\text{diag},i}^{\text{ OPT}}(t)\right).\] (40)

Figure 15: Mean and median of \(r_{\text{diag},i}^{\text{SGDM}}(t)\) and \(r_{\text{diag},i}^{\text{Adam}}(t)\) for the full hessian across the four layers (#6,12,17,22)

We consider a 2-layer network under Assumption 1 and 2, and have two goals in our proof:

1. To show that \(R^{\text{OPT}}_{\text{diag}}(t)\) after training is smaller than that before training (\(t=0\)).
2. Note that in our setting (see in Assumption 1), the Hessian is a \((d^{2}+d)\times(d^{2}+d)\) matrix. For a completely "uniform" matrix with the same size, we have that \(R^{\text{OPT}}_{\text{diag}}(t)=\Theta\left(\sqrt{d^{2}+d}\right)=\Theta(d)\). Hence our second goal is to show that the \(R^{\text{OPT}}_{\text{diag}}(t)\) after training is on lower order than \(\Theta(d)\).

**Theorem 2**.: _Consider the ratio \(R^{\text{OPT}}_{\text{diag}}(t)\) defined in eq. (40). Under Assumption 1 and 2, we have that before training (\(t=0\)), with high probability,_

\[R^{\text{OPT}}_{\text{diag}}(0)\geq\tilde{\Omega}\left(d^{4\alpha-\frac{3}{2} }\right).\] (41)

_For SGD+M defined in eq. (3). For any \(p>0\), by picking the same hyperparameters as in Theorem 1, for \(T_{\text{SGD},1},T_{\text{SGD},2}\) mentioned in Theorem 1, we have with constant probability, for any \(t\in[T_{\text{SGD},1},T_{\text{SGD},2}]\),_

\[R^{\text{SGDM}}_{\text{diag}}(t)\leq\tilde{\mathcal{O}}\left(\sqrt{d}\right)+ q^{(t)},\] (42)

_where the trend of \(q^{(t)}\) is to decrease over time and \(q^{(T_{\text{SGD},2})}\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{p/2-1}}\right) =o(d)\)._

_For Adam defined in eq. (3). For any \(p>0\), by picking the same hyperparameters as in Theorem 1, for \(T_{\text{Adam},1},T_{\text{Adam},2}\) mentioned in Theorem 1, we have with high probability, for any \(t\in[T_{\text{Adam},1},T_{\text{Adam},2}]\),_

\[R^{\text{Adam}}_{\text{diag}}(t)\leq\tilde{\mathcal{O}}\left(\sqrt{d}\right)+ r^{(t)},\] (43)

_where the trend of \(r^{(t)}\) is to decrease over time and \(r^{(T_{\text{Adam},2})}\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{p-1}{2} }}\right)=o\left(\sqrt{d}\right)\)._

### Proof of Theorem 2

Lemma 4.3 of [11] gives us the following forms of Hessian.

For any \(k\in\{1,2,...,H+1\}\), we know that \(\nabla_{vec(W_{k})}(\nabla_{vec(W_{k})}L(W))\) equals

\[((W_{H+1}\dots W_{k+1})^{T}(W_{H+1}\dots W_{k+1})\otimes(W_{k-1}\dots W_{1})( W_{k-1}\dots W_{1})^{T},\]

and for \(k\in\{2,3,...,H+1\}\),

\[\nabla_{vec(W_{k})}(\nabla_{vec(W_{1})}L(W))\] \[= (C^{T}(W_{H+1}\dots W_{k+1})\otimes(W_{k-1}\dots W_{1})^{T})\] \[+ [(W_{k-1}\dots W_{2})^{T}\otimes I][I_{d_{k-1}}\otimes(r(W_{H+1} \dots W_{k+1}))_{.,1}\cdots I_{d_{k-1}}\otimes(r(W_{H+1}\dots W_{k+1}))_{.,d_{ k}}],\]

where \(r=(W_{H+1}\dots W_{1}-A)^{T},C=W_{H+1}W_{H}\cdots W_{2}\).

For the 2-layer linear network, write the Hessian as

\[H:=\left[\begin{array}{cc}H_{22}&H_{21}^{T}\\ H_{21}&H_{11}\end{array}\right],\]

then we have that

\[H_{11} =(W_{2}^{T}W_{2})\otimes I_{d}\in\mathbb{R}^{d^{2}\times d^{2}},\] \[H_{22} =W_{1}W_{1}^{T}\in\mathbb{R}^{d\times d},\] \[H_{21} =W_{2}^{T}\otimes W_{1}^{T}+I_{d}\otimes(W_{2}W_{1}-A)^{T}\in \mathbb{R}^{d^{2}\times d}.\]

Intuitively, before training the elements of \(W_{1}\) and \(W_{2}\) are very close to zero, and \(W_{2}W_{1}-A\approx-A\). Since the elements of \(A\) are \(\Theta(1)\), we know that the magnitudes of elements of \(H_{21}\) are much bigger than those of \(H_{11}\) and \(H_{22}\).

After training, for both SGD+M and Adam, \(W_{2}W_{1}-A\approx 0\). Then \(H_{21}\approx(W_{2})^{T}\otimes(W_{1})^{T}\) and the magnitudes of its elements are no longer much larger than those of \(H_{11}\) and \(H_{22}\). From the formula of \(H_{11}\), we know that all the diagonal entries are nonzero, and among the \(d^{4}-d^{2}\) off-diagonal entries, there are only \(d^{3}-d^{2}\) nonzero entries, which helps us to bound \(R^{\text{OPT}}_{\text{diag}}(t)\).

#### f.3.1 Proof of eq. (41)

Let's first analyze the weights and Hessian before training (\(t=0\)). For ease of notation, we omit the superscript \((t)\).

For the \(i\)-th row where \(1\leq i\leq d\), i.e. the \(i\)-th row of the submatrix \([H_{22}\quad H_{21}^{T}]\), we have

\[\sum_{j\neq i}H^{2}[i,j] =\sum_{j\neq i}H_{22}^{2}[i,j]+\sum_{j=1}^{d}H_{21}^{2}[j,i]\] \[\geq\sum_{j=(i-1)d}^{id}H_{21}^{2}[j,i]=\sum_{j=1}^{d}\left(w_{2i }W_{1}[i,j]+(W_{2}W_{1}-A)_{j}\right)^{2}=\Theta(d).\]

On the other hand, for the diagonal elements, we have w.h.p.

\[|H[i,i]|=|H_{22}[i,i]|=\|W_{1}[i,:]\|_{2}^{2}=\sum_{j=1}^{d}W_{1}^{2}[i,j]\leq \tilde{\mathcal{O}}\left(\frac{1}{d^{4\alpha-1}}\right).\]

Then we have that for \(1\leq i\leq d\),

\[\frac{\sqrt{\sum_{j\neq i}H^{2}[i,j]}}{|H[i,i]|}\geq\frac{\sqrt{\Omega(d)}}{ \tilde{\mathcal{O}}\left(\frac{1}{d^{4\alpha-1}}\right)}=\tilde{\Omega}\left( d^{4\alpha-\frac{1}{2}}\right).\]

For the \((id+k)\)-th row where \(1\leq i\leq d,1\leq k\leq d\), i.e. the \(((i-1)d+k)\)-th row of the submatrix \([H_{21}\quad H_{11}]\), we have

\[\sum_{j\neq id+k}H^{2}[i,j] =\sum_{j\neq(i-1)d+k}H_{11}^{2}[(i-1)d+k,j]+\sum_{j=1}^{d}H_{21}^ {2}[(i-1)d+k,j]\] \[\geq H_{21}^{2}[(i-1)d+k,i]=(w_{2i}W_{1}[i,k]+(W_{2}W_{1}-A)_{k}) ^{2}=\Theta(1).\]

On the other hand, for the diagonal elements, we have w.h.p.

\[|H[id+k,id+k]|=|H_{11}[(i-1)d+k,(i-1)d+k]|=w_{2i}^{2}\leq\tilde{\mathcal{O}} \left(\frac{1}{d^{2\alpha}}\right).\]

Then we have that for \(1\leq i\leq d,1\leq k\leq d\),

\[\frac{\sqrt{\sum_{j\neq id+k}H^{2}[i,j]}}{|H[id+k,id+k]|}\geq\frac{\sqrt{ \Omega(1)}}{\tilde{\mathcal{O}}\left(\frac{1}{d^{2\alpha}}\right)}=\tilde{ \Omega}\left(d^{2\alpha}\right).\]

Taking the average, we obtain that before training, i.e. when \(t=0\),

\[R_{\text{diag}}^{\text{OPT}}(0)\geq\frac{d\tilde{\Omega}\left(d^{4\alpha- \frac{1}{2}}\right)+d^{2}\tilde{\Omega}\left(d^{2\alpha}\right)}{d^{2}+d}= \tilde{\Omega}\left(d^{4\alpha-\frac{3}{2}}\right).\]

#### f.3.2 Proof of eq. (42)

The proof is based on the lemma below.

**Lemma 29**.: _Suppose the weight matrices have the following structure:_

\[W_{1}=\bm{uv}^{T}+R_{1},\] \[W_{2}=c\bm{u}^{T}+R_{2}^{T},\]

_where \(\forall 1\leq i,j\leq d:\quad\frac{|R_{1}[i,j]|}{|u_{i}v_{j}|}\leq\delta,\quad \frac{|R_{2i}|}{|cu_{i}|}\leq\delta,\quad\delta\in(0,1).\)_

_Then we have for \(1\leq i\leq d\),_

\[\frac{\sqrt{\sum_{j\neq i}H^{2}[i,j]}}{|H[i,i]|}\leq\frac{1+\delta}{1-\delta} \left(1+\frac{|c|}{\|\bm{v}\|_{2}}\right)\sqrt{\frac{\sum_{j=1}^{d}u_{j}^{2}} {u_{i}^{2}}}+\frac{\|E\|_{2}}{(1-\delta)^{2}u_{i}^{2}\|v\|_{2}^{2}},\]

_and for \(1\leq i\leq d,1\leq k\leq d\),_

\[\frac{\sqrt{\sum_{j\neq id+k}H^{2}[i,j]}}{|H[id+k,id+k]|}\leq\frac{1+\delta}{1- \delta}\left(1+\frac{|v_{k}|}{|c|}\right)\sqrt{\frac{\sum_{j=1}^{d}u_{j}^{2}} {u_{i}^{2}}}+\frac{|E_{k}|}{(1-\delta)^{2}c^{2}u_{i}^{2}}.\]Now we are ready to prove eq. (42).

By the analyses in Section D.1, we know that for \(t\in[T_{\text{SGD},1},T_{\text{SGD},2}]\), the weights obtained by GD with momentum satisfy

\[W_{1}^{(t)} =\bm{u}^{(T_{1})}\bm{v}^{(t)T}+R_{1}^{(t)},\] \[W_{2}^{(t)} =c^{(t)}\bm{u}^{(T_{1})T}+R_{2}^{(t)T},\]

where \(T_{\text{SGD},1}=T_{1}\) and

\[\forall 1\leq i,j\leq d:\quad\frac{\left|R_{1}^{(t)}[i,j]\right|}{\left|u_{i}^{ (T_{1})}v_{j}^{(t)}\right|}\leq\tilde{\mathcal{O}}(\epsilon_{0}),\quad\frac{ \left|R_{2i}^{(t)}\right|}{\left|c^{(t)}u_{i}^{(T_{1})}\right|}\leq\tilde{ \mathcal{O}}(\epsilon_{0}).\]

Here \(\epsilon_{0}\) is defined in Definition 2. Since \(\bm{u}^{(T_{1})}\) doesn't depend on time \(t\) in the period \((T_{\text{SGD},1},T_{\text{SGD},2}]\), we write \(\bm{u}^{(T_{1})}\) as \(\bm{u}\) for ease of notation.

Hence by Lemma 29, when \(t\in[T_{\text{SGD},1},T_{\text{SGD},2}]\), we have for \(1\leq i\leq d\),

\[\frac{\sqrt{\sum_{j\neq i}\left(H^{(t)}[i,j]\right)^{2}}}{\left|H^ {(t)}[i,i]\right|} \leq\frac{1+\tilde{\mathcal{O}}(\epsilon_{0})}{1-\tilde{\mathcal{ O}}(\epsilon_{0})}\left(1+\frac{\left|c^{(t)}\right|}{\left\|\bm{v}^{(t)} \right\|_{2}}\right)\sqrt{\frac{\sum_{j=1}^{d}u_{j}^{2}}{u_{i}^{2}}}+\frac{ \left\|E^{(t)}\right\|_{2}}{\left(1-\tilde{\mathcal{O}}(\epsilon_{0})\right)^ {2}u_{i}^{2}\left\|\bm{v}^{(t)}\right\|_{2}^{2}}\] (44)

and for \(1\leq i\leq d,1\leq k\leq d\),

\[\frac{\sqrt{\sum_{j\neq id+k}\left(H^{(t)}[i,j]\right)^{2}}}{\left|H^ {(t)}[id+k,id+k]\right|^{2}} \leq\frac{1+\tilde{\mathcal{O}}(\epsilon_{0})}{1-\tilde{\mathcal{ O}}(\epsilon_{0})}\left(1+\frac{\left|v_{k}^{(t)}\right|}{\left|c^{(t)} \right|}\right)\sqrt{\frac{\sum_{j=1}^{d}u_{j}^{2}}{u_{i}^{2}}}+\frac{\left|E_ {k}^{(t)}\right|}{\left(1-\tilde{\mathcal{O}}(\epsilon_{0})\right)^{2}\left( c^{(t)}\right)^{2}u_{i}^{2}}\] (45)

By Lemma 4, we have \(\bm{u}=X+Y\) where \(X_{i},i\in[d]\) are i.i.d Gaussian random variables and w.h.p.,

\[\forall i\in[d]:\frac{|Y_{i}|}{|X_{i}|}\leq\tilde{\mathcal{O}}\left(\frac{1}{d ^{\frac{1}{4}\alpha-\frac{1}{2}}}\right):=\delta_{xy},\] (46)

which yields that

\[\forall i\in[d]:\frac{\sqrt{\sum_{j=1}^{d}u_{j}^{2}}}{|u_{i}|}\leq\left(\frac {1+\delta_{xy}}{1-\delta_{xy}}\right)\frac{\sqrt{\sum_{j=1}^{d}X_{j}^{2}}}{|X _{i}|},\quad\frac{1}{u_{i}^{2}}\leq\left(\frac{1}{1-\delta_{xy}}\right)^{2} \frac{1}{X_{i}^{2}}.\] (47)

By the proof in Section D.8, we know that for \(t\in[T_{\text{SGD},1},T_{\text{SGD},2}]\), \(\forall i\in[d]:v_{i}^{(t)},c^{(t)}\) are positive. The induction in Section D.9 further gives us that for \(t\in[T_{\text{SGD},1},T_{\text{SGD},2}]\), w.h.p. \(\forall k\in[d]:\frac{v_{k}^{(t)}}{c^{(t)}}=\Theta\left(\frac{1}{\sqrt{d}}\right)\), which yields \(\frac{c^{(t)}}{\left\|\bm{v}^{(t)}\right\|_{2}}=\Theta(1)\). Combining with eq. (47), we obtain

\[\begin{split}&\left(1+\frac{\left|c^{(t)}\right|}{\left\|\bm{v}^{ (t)}\right\|_{2}}\right)\sqrt{\frac{\sum_{j=1}^{d}u_{j}^{2}}{u_{i}^{2}}}\leq \mathcal{O}\left(\frac{\sqrt{\sum_{j=1}^{d}X_{j}^{2}}}{|X_{i}|}\right),\\ &\left(1+\frac{\left|v_{k}^{(t)}\right|}{\left|c^{(t)}\right|} \right)\sqrt{\frac{\sum_{j=1}^{d}u_{j}^{2}}{u_{i}^{2}}}\leq\mathcal{O}\left( \frac{\sqrt{\sum_{j=1}^{d}X_{j}^{2}}}{|X_{i}|}\right).\end{split}\] (48)By the proof in Section D.8, we know that for \(t\in[T_{\text{SGD},1},T_{\text{SGD},2}]\), \(\forall i\in[d]:v^{(t)},c^{(t)}\) are positive and monotonically increasing. On the other hand, the proof in Section D.2 and D.9 tells us that w.h.p. \(\left\|E^{(t)}\right\|_{2}\) (resp. \(\forall k\in[d],\left|E_{k}^{(t)}\right|\)) decreases from \(\Theta(\sqrt{d})\) (resp. \(\Theta(1)\)) when \(t=T_{\text{SGD},1}\) to \(\mathcal{O}(\sqrt{\epsilon_{0}}d)\) (resp. \(\mathcal{O}(\sqrt{\epsilon_{0}})\)) when \(t=T_{\text{SGD},2}\). Therefore, the trend of \(\frac{\left\|E^{(t)}\right\|_{2}}{u_{i}^{2}\left\|\mathbf{v}^{(t)}\right\|_{2 }^{2}}\) and \(\frac{\left|E_{k}^{(t)}\right|}{\left(c^{(t)}\right)^{2}u_{i}^{2}}\) is to decrease over time, and when \(t=T_{\text{SGD},2}\), we have w.h.p.

\[\forall k\in[d]:\left|E_{k}^{(t)}\right|=\mathcal{O}\left(\sqrt{\epsilon_{0}} \right),\quad\left\|E^{(t)}\right\|_{2}=\mathcal{O}\left(\sqrt{\epsilon_{0}d} \right).\] (49)

Moreover, when \(t=T_{\text{SGD},2}\), the inequality in eq. (26) becomes equality, i.e. \(c^{2}\|\boldsymbol{u}\|_{2}^{2}=\Theta\left(\sqrt{d}\right)\)and \(\forall j\in[d]:\|\boldsymbol{u}\|_{2}^{2}v_{j}^{2}=\Theta\left(\frac{1}{ \sqrt{d}}\right)\).

Using \(\boldsymbol{u}=X+Y\) and eq. (46), we have

\[c^{2}\|X\|_{2}^{2}=\Theta\left(\sqrt{d}\right),\quad\forall j\in[d]:\|X\|_{2} ^{2}v_{j}^{2}\Theta\left(\frac{1}{\sqrt{d}}\right),\quad\Rightarrow\quad\|X \|_{2}^{2}\|\boldsymbol{v}\|_{2}^{2}=\Theta\left(\sqrt{d}\right),\]

which together with the second inequality in eq. (47) yields

\[\frac{1}{u_{i}^{2}\|\boldsymbol{v}\|_{2}^{2}} \leq\left(\frac{1}{1-\delta_{xy}}\right)^{2}\frac{1}{X_{i}^{2}\| \boldsymbol{v}\|_{2}^{2}}=\Theta\left(\frac{\sum_{j=1}^{d}X_{j}^{2}}{X_{i}^{2} \sqrt{d}}\right),\] \[\frac{1}{c^{2}u_{i}^{2}} \leq\left(\frac{1}{1-\delta_{xy}}\right)^{2}\frac{1}{c^{2}X_{i}^{ 2}}=\Theta\left(\frac{\sum_{j=1}^{d}X_{j}^{2}}{X_{i}^{2}\sqrt{d}}\right).\]

Combining with eq. (49), we get that

\[\frac{\left\|E^{(t)}\right\|_{2}}{u_{i}^{2}\left\|\boldsymbol{v}^{(t)}\right\| _{2}^{2}}\leq\mathcal{O}\left(\frac{\sum_{j=1}^{d}X_{j}^{2}}{X_{i}^{2}}\cdot \sqrt{\epsilon_{0}}\right),\quad\frac{\left|E_{k}^{(t)}\right|_{2}}{\left(c^{ (t)}\right)^{2}u_{i}^{2}}\leq\mathcal{O}\left(\frac{\sum_{j=1}^{d}X_{j}^{2}}{ X_{i}^{2}}\cdot\sqrt{\frac{\epsilon_{0}}{d}}\right).\] (50)

Substituting eq. (48) and (50) into eq. (44) and (45) gives us

\[\forall 1\leq i\leq d:\frac{\sqrt{\sum_{j\neq i}\left(H^{(t)}[i,j]\right)^{2}}}{ \left|H^{(t)}[i,i]\right|}\leq\mathcal{O}\left(\frac{\sqrt{\sum_{j=1}^{d}X_{j }^{2}}}{|X_{i}|}\right)+q_{1i}^{(t)},\]

where the trend of \(q_{1i}^{(t)}\) is to decrease over time and \(q_{1i}^{(T_{\text{SGD},2})}\leq\mathcal{O}\left(\frac{\sum_{j=1}^{d}X_{j}^{2} }{X_{i}^{2}}\cdot\sqrt{\epsilon_{0}}\right)\).

We also have

\[\forall 1\leq i\leq d,1\leq k\leq d:\frac{\sqrt{\sum_{j\neq id+k}\left(H^{(t)}[i,j]\right)^{2}}}{\left|H^{(t)}[id+k,id+k]\right|^{2}}\leq\mathcal{O}\left( \frac{\sqrt{\sum_{j=1}^{d}X_{j}^{2}}}{|X_{i}|}\right)+q_{2i}^{(t)},\]

where the trend of \(q_{2i}^{(t)}\) is to decrease over time and \(q_{2i}^{(T_{\text{SGD},2})}\leq\mathcal{O}\left(\frac{\sum_{j=1}^{d}X_{j}^{2} }{X_{i}^{2}}\cdot\sqrt{\frac{\epsilon_{0}}{d}}\right)\).

Hence

\[R_{\text{diag}}^{\text{SCDM}}(t) =\mathcal{O}\left(\frac{1}{d}\sum_{i=1}^{d}\frac{\sqrt{\sum_{j=1}^ {d}X_{j}^{2}}}{|X_{i}|}\right)+\frac{1}{d^{2}+d}\sum_{i=1}^{d}q_{1i}^{(t)}+ \frac{d}{d^{2}+d}\sum_{i=1}^{d}q_{2i}^{(t)}\] \[:=\mathcal{O}\left(\frac{1}{d}\sum_{i=1}^{d}\frac{\sqrt{\sum_{j=1}^ {d}X_{j}^{2}}}{|X_{i}|}\right)+q^{(t)},\]where the trend of \(q^{(t)}\) is to decrease over time and

\[q^{(T_{\text{Kadm},2})} \leq\frac{1}{d^{2}+d}\sum_{i=1}^{d}\mathcal{O}\left(\frac{\sum_{j=1 }^{d}X_{j}^{2}}{X_{i}^{2}}\cdot\sqrt{\epsilon_{0}}\right)+\frac{d}{d^{2}+d}\sum _{i=1}^{d}\mathcal{O}\left(\frac{\sum_{j=1}^{d}X_{j}^{2}}{X_{i}^{2}}\cdot\sqrt{ \frac{\epsilon_{0}}{d}}\right)\] \[\leq\mathcal{O}\left(\frac{1}{d^{2}+d}\sum_{i=1}^{d}\frac{\sum_{ j=1}^{d}X_{j}^{2}}{X_{i}^{2}}\cdot\sqrt{\epsilon_{0}}d\right)=\mathcal{O} \left(\frac{1}{d}\sum_{i=1}^{d}\frac{\sum_{j=1}^{d}X_{j}^{2}}{X_{i}^{2}}\cdot \sqrt{\frac{\epsilon_{0}}{d}}\right).\]

Denote \(\sigma^{2}\) as the variance of \(X_{i}\) for \(i\in[d]\). By concentration of chi-squared distribution, we know that with probability at least \(1-\delta\) for \(\delta>0\),

\[\sum_{i=1}^{d}X_{i}^{2}\leq\sigma^{2}d+\sigma^{2}\mathcal{O}\left(\sqrt{d\log \frac{1}{\delta}}\right).\]

By Lemma 36 in Appendix H, we know that with constant probability \(\frac{1}{d}\sum_{i=1}^{d}\frac{1}{|X_{i}|}=\mathcal{O}\left(\frac{1}{\sigma} \log d\right)\). Then with constant probability, \(\frac{1}{d}\sum_{i=1}^{d}\frac{1}{X_{i}^{2}}\leq\frac{1}{d}\left(\sum_{i=1}^{d }\frac{1}{|X_{i}|}\right)^{2}=\mathcal{O}\left(\frac{d}{\sigma^{2}}\log^{2}d\right)\). Hence

\[\frac{1}{d}\sum_{i=1}^{d}\frac{\sqrt{\sum_{j=1}^{d}X_{j}^{2}}}{|X_{i}|}= \tilde{\mathcal{O}}(\sqrt{d}),\quad\frac{1}{d}\sum_{i=1}^{d}\frac{\sum_{j=1}^ {d}X_{j}^{2}}{X_{i}^{2}}=\tilde{\mathcal{O}}\left(d^{2}\right).\]

Therefore with constant probability,

\[R_{\text{diag}}^{\text{SGDM}}(t)=\tilde{\mathcal{O}}\left(\sqrt{d}\right)+q^{( t)},\]

where the trend of \(q^{(t)}\) is to decrease over time and \(q^{(T_{\text{Kadm},2})}\leq\tilde{\mathcal{O}}\left(d\sqrt{\epsilon_{0}d}\right)\). For any \(p>0\), by picking the same hyperparameters as in Theorem 1, we have \(\epsilon_{0}d\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{p}}\right)\) and hence \(q^{(T_{\text{Kadm},2})}\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{p/2-1}}\right)= o(d)\).

#### f.3.3 Proof of eq. (43)

By the analyses in Section E.1, we know that for \(t\in[T_{\text{Adam},1},T_{\text{Adam},2}]\), the weights obtained by Adam satisfy

\[W_{1}^{(t)} =\bm{u}\bm{v}^{(t)T}+R_{1}^{(t)},\] \[W_{2}^{(t)} =c^{(t)}\bm{u}^{T}+R_{2}^{(t)T},\]

where \(\forall i\in[d]:u_{i}=\text{sign}(w_{2i}^{(0)})\in\{\pm 1\}\) and

\[\forall 1\leq i,j\leq d:\quad\frac{\left|R_{1}^{(t)}[i,j]\right|}{\left|u_{i}v_{ j}^{(t)}\right|}\leq\delta:=\tilde{\mathcal{O}}\left(\eta^{\frac{1}{4}}+\frac{1}{d^{ \frac{\alpha}{2}-\frac{1}{4}}}\right),\quad\frac{\left|R_{2i}^{(t)}\right|}{ \left|c^{(t)}u_{i}\right|}\leq\delta.\]

Hence by Lemma 29, when \(t\in[T_{\text{Adam},1},T_{\text{Adam},2}]\), we have for \(1\leq i\leq d\),

\[\frac{\sqrt{\sum_{j\neq i}\left(H^{(t)}[i,j]\right)^{2}}}{\left|H^ {(t)}[i,i]\right|} \leq\frac{1+\delta}{1-\delta}\left(1+\frac{\left|c^{(t)}\right|}{ \left\|\bm{v}^{(t)}\right\|_{2}}\right)\sqrt{\frac{\sum_{j=1}^{d}u_{j}^{2}}{u_ {i}^{2}}}+\frac{\left\|E^{(t)}\right\|_{2}}{\left(1-\delta\right)^{2}u_{i}^{2} \left\|\bm{v}^{(t)}\right\|_{2}^{2}}\] (51) \[=\mathcal{O}\left(1+\frac{\left|c^{(t)}\right|}{\left\|\bm{v}^{( t)}\right\|_{2}}\right)\sqrt{d}+\mathcal{O}\left(\frac{\left\|E^{(t)}\right\|_{2}}{ \left\|\bm{v}^{(t)}\right\|_{2}^{2}}\right),\]

and for \(1\leq i\leq d,1\leq k\leq d\),

\[\frac{\sqrt{\sum_{j\neq id+k}\left(H^{(t)}[i,j]\right)^{2}}}{ \left|H^{(t)}[id+k,id+k]\right|} \leq\frac{1+\delta}{1-\delta}\left(1+\frac{\left|v_{k}^{(t)} \right|}{\left|c^{(t)}\right|}\right)\sqrt{\frac{\sum_{j=1}^{d}u_{j}^{2}}{u_ {i}^{2}}}+\frac{\left|E_{k}^{(t)}\right|}{\left(1-\delta\right)^{2}\left(c^{(t )}\right)^{2}u_{i}^{2}}\] (52) \[=\mathcal{O}\left(1+\frac{\left|v_{k}^{(t)}\right|}{\left|c^{(t)} \right|}\right)\sqrt{d}+\mathcal{O}\left(\frac{\left|E_{k}^{(t)}\right|}{ \left(c^{(t)}\right)^{2}}\right).\]

Recall the following facts of Adam.

* By Lemma 15, we know that for \(t\in[T_{\text{Adam},1},T_{1}]\) (where \(T_{1}\) is defined in Definition 4), w.h.p. \(\forall k\in[d]:v_{k}^{(t)}=c^{(t)}=\eta(t-t_{\text{inc}})\). Specially, when \(t=T_{\text{Adam},1}\), \(\forall k\in[d]:v_{k}^{(t)}=c^{(t)}=\frac{1}{d^{2}}\). Lemma 25 and 27 tell us that for \(t\in[T_{1},T_{\text{Adam},2}]\) w.h.p. \(\forall i,j\in[d]:|W_{1}[i,j]|=\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\right),| w_{2i}|=\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\right)\), which gives us \(\forall k\in[d]:\left|v_{k}^{(t)}\right|=\tilde{\Theta}\left(\frac{1}{\sqrt{d }}\right)\) and \(\left|c^{(t)}\right|=\tilde{\Theta}\left(\frac{1}{\sqrt{d}}\right)\). That means when \(t\in[T_{\text{Adam},1},T_{\text{Adam},2}]\), \(\forall k\in[d]:\left|v_{k}^{(t)}\right|\) and \(\left|c^{(t)}\right|\) increase from \(\frac{1}{d^{\frac{1}{2}}}\) to \(\tilde{\Theta}(\frac{1}{\sqrt{d}})\) and \(\left|\frac{\left|v_{k}^{(t)}\right|}{c^{(t)}}\right|=\tilde{\Theta}(1)\), \(\frac{\left|c^{(t)}\right|}{\left|v^{(t)}\right|_{2}}=\tilde{\Theta}\left( \frac{1}{\sqrt{d}}\right)\).
* Lemma 15 and 26 tell us that w.h.p. \(\left\|E^{(t)}\right\|_{2}\) (resp. \(\forall k\in[d],\left|E_{k}^{(t)}\right|\)) decreases from \(\Theta(d)\) (resp. \(\Theta(1)\)) when \(t=T_{\text{Adam},1}\) to \(\tilde{\mathcal{O}}\left(d^{2}\sqrt{\eta}\right)\) (resp. \(\tilde{\mathcal{O}}\left(d\sqrt{\eta d}\right)\)) when \(t=T_{\text{Adam},2}\).

Combining (A) and (B), we get that the trend of \(\frac{\left\|E^{(t)}\right\|_{2}}{\left\|v^{(t)}\right\|_{2}^{2}}\) and \(\frac{\left|E_{k}^{(t)}\right|}{\left(c^{(t)}\right)^{2}}\) is to decrease over time, and when \(t=T_{\text{Adam},2}\), we have w.h.p.

\[\frac{\left\|E^{(t)}\right\|_{2}}{\left\|v^{(t)}\right\|_{2}^{2}} \leq\tilde{\mathcal{O}}\left(d^{2}\sqrt{\eta}\right),\quad\frac{\left|E_{k}^{ (t)}\right|}{\left(c^{(t)}\right)^{2}}\leq\tilde{\mathcal{O}}\left(d^{2}\sqrt {\eta d}\right).\] (53)

Substituting (A) and eq. (53) into eq. (51) and (52) gives us w.h.p.,

\[\forall 1\leq i\leq d:\frac{\sqrt{\sum_{j\neq i}\left(H^{(t)}[i,j]\right)^{2}}}{ \left|H^{(t)}[i,i]\right|}\leq\mathcal{O}\left(\sqrt{d}\right)+r_{1i}^{(t)},\]

where the trend of \(r_{1i}^{(t)}\) is to decrease over time and \(r_{1i}^{(T_{\text{Adam},2})}\leq\tilde{\mathcal{O}}\left(d^{2}\sqrt{\eta}\right)\).

We also have

\[\forall 1\leq i\leq d,1\leq k\leq d:\frac{\sqrt{\sum_{j\neq id+k}\left(H^{(t)}[ i,j]\right)^{2}}}{\left|H^{(t)}[id+k,id+k]\right|}\leq\tilde{\mathcal{O}}\left( \sqrt{d}\right)+r_{2i}^{(t)},\]

where the trend of \(r_{2i}^{(t)}\) is to decrease over time and \(r_{2i}^{(T_{\text{Adam},2})}\leq\tilde{\mathcal{O}}\left(d^{2}\sqrt{\eta d}\right)\).

Hence \(R_{\text{diag}}^{\text{Adam}}(t)=\tilde{\mathcal{O}}\left(\sqrt{d}\right)+ \frac{1}{d^{2}+d}\sum_{i=1}^{d}r_{1i}^{(t)}+\frac{d}{d^{2}+d}\sum_{i=1}^{d}r_{2 i}^{(t)}:=\tilde{\mathcal{O}}\left(\sqrt{d}\right)+r^{(t)}\) where the trend of \(r^{(t)}\) is to decrease over time and

\[r^{(T_{\text{Adam},2})}\leq\frac{1}{d^{2}+d}\sum_{i=1}^{d}\tilde{\mathcal{O}} \left(d^{2}\sqrt{\eta}\right)+\frac{d}{d^{2}+d}\sum_{i=1}^{d}\tilde{\mathcal{O}} \left(d^{2}\sqrt{\eta d}\right)\leq\tilde{\mathcal{O}}\left(d^{2}\sqrt{\eta d} \right).\]

For any \(p>0\), by picking the same hyperparameters as in Theorem 1, we have \(\eta d^{4}\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{p}}\right)\) and hence \(r^{(T_{\text{Adam},2})}\leq\tilde{\mathcal{O}}\left(\frac{1}{d^{\frac{p-1}{2}}} \right)=o\left(\sqrt{d}\right)\).

### Proof of Lemma 29

By the assumed weight structure, we get that

\[\forall i\in[d]: (1-\delta)^{2}(cu_{i})^{2}\leq(w_{2i})^{2}\leq(1+\delta)^{2}(cu_{i })^{2},\] \[(1-\delta)^{2}(u_{i})^{2}\|\bm{v}\|_{2}^{2}\leq\|W_{1}[i,:]\|_{2}^ {2}\leq(1+\delta)^{2}(u_{i})^{2}\|\bm{v}\|_{2}^{2}.\]For the \(i\)-th row where \(1\leq i\leq d\), i.e. the \(i\)-th row of the submatrix \([H_{22}\quad H_{21}^{T}]\), by triangle inequality, we have

\[\sqrt{\sum_{j\neq i}H^{2}[i,j]} \leq\sqrt{\sum_{j\neq i}H_{22}^{2}[i,j]}+\sqrt{\sum_{j=1}^{d^{2}} H_{21}^{2}[j,i]}\] \[\leq\sqrt{\sum_{j\neq i}\langle W_{1}[i,:],W_{1}[j,:]\rangle^{2}} +\sqrt{\sum_{j=1}^{d}w_{2j}^{2}\sum_{k=1}^{d}W_{1}^{2}[i,k]}+\|E\|_{2}\] \[\leq\|W_{1}[i,:]\|_{2}\left(\sqrt{\sum_{j\neq i}\|W_{1}[j,:]\|_{2 }^{2}}+\sqrt{\sum_{j=1}^{d}w_{2j}^{2}}\right)+\|E\|_{2}.\]

Then we have that for \(1\leq i\leq d\),

\[\frac{\sqrt{\sum_{j\neq i}H^{2}[i,j]}}{|H[i,i]|} \leq\frac{\|W_{1}[i,:]\|_{2}\sqrt{\sum_{j\neq i}\|W_{1}[j,:]\|_{2 }^{2}}+\sqrt{\sum_{j=1}^{d}w_{2j}^{2}}}{\|W_{1}[i,:]\|_{2}^{2}}+\frac{\|E\|_{2 }}{\|W_{1}[i,:]\|_{2}^{2}}\] \[=\sqrt{\frac{\sum_{j\neq i}\|W_{1}[j,:]\|_{2}^{2}}{\|W_{1}[i,:]\|_ {2}^{2}}}+\sqrt{\frac{\sum_{j=1}^{d}w_{2j}^{2}}{\|W_{1}[i,:]\|_{2}^{2}}}+\frac {\|E\|_{2}}{\|W_{1}[i,:]\|_{2}^{2}}\] \[\leq\sqrt{\frac{(1+\delta)^{2}}{(1-\delta)^{2}}}\cdot\frac{\sum_{ j\neq i}u_{j}^{2}\|\bm{v}\|_{2}^{2}}{u_{i}^{2}\|\bm{v}\|_{2}^{2}}+\sqrt{\frac{(1+ \delta)^{2}}{(1-\delta)^{2}}}\cdot\frac{c^{2}\sum_{j=1}^{d}u_{j}^{2}}{u_{i}^{2 }\|\bm{v}\|_{2}^{2}}+\frac{\|E\|_{2}}{(1-\delta)^{2}u_{i}^{2}\|\bm{v}\|_{2}^{2}}\] \[\leq\frac{1+\delta}{1-\delta}\left(1+\frac{|c|}{\|\bm{v}\|_{2}} \right)\sqrt{\frac{\sum_{j=1}^{d}u_{j}^{2}}{u_{i}^{2}}}+\frac{\|E\|_{2}}{(1- \delta)^{2}u_{i}^{2}\|v\|_{2}^{2}}.\]

For the \((id+k)\)-th row where \(1\leq i\leq d,1\leq k\leq d\), i.e. the \(((i-1)d+k)\)-th row of the submatrix \([H_{21}\quad H_{11}]\), by triangle inequality again, we have

\[\sqrt{\sum_{j\neq d+k}H^{2}[i,j]} \leq\sqrt{\sum_{j\neq(i-1)d+k}H_{11}^{2}[(i-1)d+k,j]}+\sqrt{\sum _{j=1}^{d}H_{21}^{2}[(i-1)d+k,j]}\] \[\leq\sqrt{\sum_{j\neq i}w_{2i}^{2}w_{2j}^{2}}+\sqrt{\sum_{j=1}^{d }w_{2i}^{2}W_{1}^{2}[j,k]}+|E_{k}|\] \[=|w_{2i}|\left(\sqrt{\sum_{j\neq i}w_{2j}^{2}}+\sqrt{\sum_{j=1}^{ d}W_{1}^{2}[j,k]}\right)+|E_{k}|.\]

Then we have that for \(1\leq i\leq d,1\leq k\leq d\),

\[\frac{\sqrt{\sum_{j\neq id+k}H^{2}[i,j]}}{|H[id+k,id+k]|} \leq\frac{|w_{2i}|\sqrt{\sum_{j\neq i}w_{2j}^{2}}+\sqrt{\sum_{j=1 }^{d}W_{1}^{2}[j,k]}}{w_{2i}^{2}}+\frac{|E_{k}|}{w_{2i}^{2}}\] \[=\sqrt{\frac{\sum_{j\neq i}w_{2j}^{2}}{w_{2i}^{2}}}+\sqrt{\frac{ \sum_{j=1}^{d}W_{1}^{2}[j,k]}{w_{2i}^{2}}}+\frac{|E_{k}|}{w_{2i}^{2}}\] \[\leq\sqrt{\frac{(1+\delta)^{2}}{(1-\delta)^{2}}}\cdot\frac{\sum_ {j\neq i}c^{2}u_{j}^{2}}{c^{2}u_{i}^{2}}+\sqrt{\frac{(1+\delta)^{2}}{(1-\delta )^{2}}}\cdot\frac{v_{k}^{2}\sum_{j=1}^{d}u_{j}^{2}}{c^{2}u_{i}^{2}}+\frac{|E_{ k}|}{(1-\delta)^{2}c^{2}u_{i}^{2}}\] \[\leq\frac{1+\delta}{1-\delta}\left(1+\frac{|v_{k}|}{|c|}\right) \sqrt{\frac{\sum_{j=1}^{d}u_{j}^{2}}{u_{i}^{2}}}+\frac{|E_{k}|}{(1-\delta)^{ 2}c^{2}u_{i}^{2}}.\]Connection between diagonal of loss Hessian and weights

The partial derivative at \(W_{i}\) of the cost function for each \(i\) is given by:

\[\nabla_{W_{i}}L(W)=W_{i+1}^{T}\ldots W_{H+1}^{T}(W_{H+1}W_{H}\ldots W_{1}-A)W_{1 }^{T}\ldots W_{i-1}^{T}\] (54)

In our experiments, we were interested in the diagonal elements of the hessian. These are given by:

\[\nabla_{(W_{i})_{a,b}}(\nabla_{W_{i}}L(W))_{a,b}=\nabla_{(W_{i})_{a,b}}(W_{i+1 }^{T}\ldots W_{H+1}^{T}(W_{H+1}W_{H}\ldots W_{1}-A)W_{1}^{T}\ldots W_{i-1}^{T}) _{a,b}\]

for each possible \(i,a,b\). For ease in notation, define for each \(i\), the quantities \(M_{i}:=W_{i+1}^{T}\ldots W_{H+1}^{T}\) and \(N_{i}:=W_{1}^{T}\ldots W_{i-1}^{T}\). Then we have the following lemma.

**Lemma 30**.: _The diagonal elements of the hessian of the cost function are given by:_

\[\nabla_{(W_{i})_{a,b}}(\nabla_{W_{i}}L(W))_{a,b}=(M_{i}M_{i}^{T})_{a,a}(N_{i} ^{T}N_{i})_{b,b}\]

_for each possible \(i,a,b\)._

Proof.: We have:

\[\nabla_{W_{i}}L(W) =W_{i+1}^{T}\ldots W_{H+1}^{T}(W_{H+1}W_{H}\ldots W_{1}-A)W_{1}^{T }\ldots W_{i-1}^{T}\] \[=M_{i}(W_{H+1}W_{H}\ldots W_{1}-A)N_{i}\] \[=M_{i}W_{H+1}W_{H}\ldots W_{1}N_{i}-M_{i}YN_{i}.\]

This implies that:

\[\nabla_{(W_{i})_{a,b}}(\nabla_{W_{i}}L(W))_{a,b} =\nabla_{(W_{i})_{a,b}}\left(M_{i}W_{H+1}W_{H}\ldots W_{1}N_{i}-M_ {i}YN_{i}\right)_{a,b}\] \[=\nabla_{(W_{i})_{a,b}}\left(M_{i}W_{H+1}W_{H}\ldots W_{1}N_{i} \right)_{a,b},\]

where the last step follows since \(M_{i}\) and \(N_{i}\) are not functions of \(W_{i}\).

Since \(M_{i}:=W_{i+1}^{T}\ldots W_{H+1}^{T}\), \(N_{i}:=W_{1}^{T}\ldots W_{i-1}^{T}\), by defining \(C_{i}:=M_{i}W_{H+1}W_{H}\ldots W_{i+1}=M_{i}M_{i}^{T}\) and \(D_{i}:=W_{i-1}\ldots W_{2}W_{1}N_{i}=N_{i}^{T}N_{i}\) we have that:

\[\nabla_{(W_{i})_{a,b}}(\nabla_{W_{i}}L(W))_{a,b}=\nabla_{(W_{i})_{a,b}}(C_{i} W_{i}D_{i})_{a,b},\]

where \(C_{i}\) and \(D_{i}\) are not functions of \(W_{i}\). Now, Equation \(74\) in the Matrix Cookbook13 shows us that for any matrices \(A\) and \(X\) we have:

Footnote 13: https://www.math.uwaterloo.ca/~hwolkovic/matrixcookbook.pdf

\[\nabla_{X_{mn}}(XA)_{ij}=\delta_{im}A_{nj}.\]

Note that \(W_{i}\in\mathbb{R}^{d_{i}\times d_{i-1}}\), then we can apply this to obtain that:

\[\nabla_{(W_{i})_{a,b}}(\nabla_{W_{i}}L(W))_{a,b} =\nabla_{(W_{i})_{a,b}}(C_{i}W_{i}D_{i})_{a,b}\] \[=\nabla_{(W_{i})_{a,b}}\left[\sum_{k=1}^{d_{i}}(C_{i})_{a,k}(W_{i }D_{i})_{k,b}\right]\] \[=\sum_{k=1}^{d_{i}}(C_{i})_{a,k}\nabla_{(W_{i})_{a,b}}(W_{i}D_{i}) _{k,b}\] \[=\sum_{k=1}^{d_{i}}(C_{i})_{a,k}\delta_{ak}(D_{i})_{b,b}\] \[=(C_{i})_{a,a}(D_{i})_{b,b}\] \[=(M_{i}M_{i}^{T})_{a,a}(N_{i}^{T}N_{i})_{b,b}.\]

This completes the proof.

For ease of notation, let's now drop the superscript OPT and write \(R_{\text{med},1}^{\text{OPT}}(t)\) as \(R_{\text{med},1}\) and \(R_{\text{med},2}^{\text{OPT}}(t)\) as \(R_{\text{med},2}\). For a 2-layer linear network, \(H=1\). Consider the Hessian w.r.t \(W_{1}\), we have \(M_{1}M_{1}^{T}=W_{2}^{T}W_{2}\) and \(N_{1}^{T}N_{1}\) is an identity matrix. Under Assumption 1, we know that \(W_{2}\) is a row vector, which can be denoted as \(W_{2}=[w_{21},w_{22},...,w_{2d_{1}}]\). Then we have

\[(M_{1}M_{1}^{T})_{a,a}=w_{2a}^{2},(N_{1}^{T}N_{1})_{b,b}=1,\quad\Rightarrow \quad R_{\text{med},1}=\frac{\max_{i}(w_{2i})^{2}}{\text{median}(w_{2i})^{2}}.\]

Similarly, consider the Hessian w.r.t. \(W_{2}\), we have that \(M_{1}M_{1}^{T}\) is an identity matrix and \(N_{1}^{T}N_{1}=W_{1}W_{1}^{T}\). Therefore,

\[(M_{1}M_{1}^{T})_{a,a}=1,(N_{1}^{T}N_{1})_{b,b}=\|W_{1}[b,:]\|_{2}^{2},\quad \Rightarrow\quad R_{\text{med},2}=\frac{\max_{i}\|W_{1}[i,:]\|_{2}^{2}}{\text{ median}\|W_{1}[i,:]\|_{2}^{2}}.\]

Hence we have related the uniformity of diagonal Hessian to that of weight matrices. In the detailed analysis, for both GD and Adam, we can prove that \(W_{1}\) converges to an approximately rank 1 matrix. The following lemma allows us to use this rank 1 structure to compute \(R_{\text{med},1}\) and \(R_{\text{med},2}\).

**Lemma 31**.: _Suppose \(W_{1}\in\mathbb{R}^{d\times d}\) and \(W_{2}\in\mathbb{R}^{1\times d}\) have the following structure:_

\[W_{1}=\bm{u}\bm{v}^{T}+R_{1},\] \[W_{2}=c\bm{u}^{T}+R_{2},\]

_where \(\bm{u}\in\mathbb{R}^{d},\bm{v}\in\mathbb{R}^{d},R_{1}\in\mathbb{R}^{d\times d}, R_{2}\in\mathbb{R}^{1\times d}\) and that_

\[\forall 1\leq i,j\leq d:\quad\frac{|R_{1}[i,j]|}{|u_{i}v_{j}|}\leq\delta,\quad \frac{|R_{2i}|}{|cu_{i}|}\leq\delta,\quad\delta\in(0,1).\]

_Then we have_

\[R_{\text{med},1},R_{\text{med},2}\in\left[\frac{(1-\delta)^{2}}{(1+\delta)^{ 2}}\cdot\frac{\max_{i}u_{i}^{2}}{\text{median}\;u_{i}^{2}},\frac{(1+\delta)^{ 2}}{(1-\delta)^{2}}\cdot\frac{\max_{i}u_{i}^{2}}{\text{median}\;u_{i}^{2}} \right].\]

Proof.: Let's first consider \(R_{\text{med},1}\). we have

\[\forall i\in[d]:(1-\delta)^{2}(cu_{i})^{2}\leq w_{2i}^{2}\leq(1+ \delta)^{2}(cu_{i})^{2}\] \[\Rightarrow\quad(1-\delta)^{2}\max_{i}(cu_{i})^{2}\leq\max_{i}w_ {2i}^{2}\leq(1+\delta)^{2}\max_{i}(cu_{i})^{2}\] \[(1-\delta)^{2}\text{median}\;(cu_{i})^{2}\leq\text{median}\;w_ {2i}^{2}\leq(1+\delta)^{2}\text{median}\;(cu_{i})^{2},\]

which yields

\[\frac{(1-\delta)^{2}}{(1+\delta)^{2}}\cdot\frac{\max_{i}u_{i}^{2}}{\text{ median}\;u_{i}^{2}}\leq R_{\text{med},1}=\frac{\max_{i}w_{2i}^{2}}{\text{ median}\;w_{2i}^{2}}\leq\frac{(1+\delta)^{2}}{(1-\delta)^{2}}\cdot\frac{\max_{i}u_{i}^ {2}}{\text{median}\;u_{i}^{2}}.\]

Similarly, for \(R_{\text{med},2}\). We have that

\[\forall i,j\in[d]:(1-\delta)^{2}(u_{i}v_{j})^{2}\leq W_{1}^{2}[i,j]\leq(1+\delta)^{2}(u_{i}v_{j})^{2}\] \[\Rightarrow\quad(1-\delta)^{2}u_{i}^{2}\|\bm{v}\|_{2}^{2}\leq\| W_{1}[i,:]\|_{2}^{2}\leq(1+\delta)^{2}u_{i}^{2}\|\bm{v}\|_{2}^{2}\] \[\Rightarrow\quad(1-\delta)^{2}\max_{i}u_{i}^{2}\|\bm{v}\|_{2}^{2} \leq\max_{i}\|W_{1}[i,:]\|_{2}^{2}\leq(1+\delta)^{2}\max_{i}u_{i}^{2}\|\bm{v}\|_ {2}^{2}\] \[(1-\delta)^{2}\text{median}\;u_{i}^{2}\|\bm{v}\|_{2}^{2}\leq\text{ median}\;\|W_{1}[i,:]\|_{2}^{2}\leq(1+\delta)^{2}\text{median}\;u_{i}^{2}\|\bm{v}\|_{2}^{2},\]

which yields

\[\frac{(1-\delta)^{2}}{(1+\delta)^{2}}\cdot\frac{\max_{i}u_{i}^{2}\|\bm{v}\|_{2}^ {2}}{\text{median}\;u_{i}^{2}\|\bm{v}\|_{2}^{2}}\leq R_{\text{med},2}=\frac{ \max_{i}\|W_{1}[i,:]\|_{2}^{2}}{\text{median}\|W_{1}[i,:]\|_{2}^{2}}\leq \frac{(1+\delta)^{2}}{(1-\delta)^{2}}\cdot\frac{\max_{i}u_{i}^{2}\|\bm{v}\|_{2}^ {2}}{\text{median}\;u_{i}^{2}\|\bm{v}\|_{2}^{2}}.\]

That means

\[\frac{(1-\delta)^{2}}{(1+\delta)^{2}}\cdot\frac{\max_{i}u_{i}^{2}}{\text{ median}\;u_{i}^{2}}\leq R_{\text{med},2}\leq\frac{(1+\delta)^{2}}{(1-\delta)^{2}}\cdot \frac{\max_{i}u_{i}^{2}}{\text{median}\;u_{i}^{2}}.\]

## Appendix H Auxiliary lemmas

**Lemma 32**.: _Let \(A=\frac{1}{m}YX^{T}\), \(\Lambda_{xx}:=\frac{1}{m}XX^{T}\), \(g_{k}^{(t)}=\nabla_{W_{k}}L(W^{(t)}),k=1,2\). Denote \(\tilde{A}^{(t)}\), \(\tilde{\Lambda}_{xx}^{(t)}\) and \(\tilde{g}_{k}^{(t)},k=1,2\) as the corresponding batch versions at time \(t\). Let \(M_{1}^{(t)}=\max_{i,j}\left|W_{1}^{(t)}[i,j]\right|\) and \(M_{2}^{(t)}=\max_{i}\left|w_{2i}^{(t)}\right|\). Under Assumption 3, we have with probability at least \(1-\frac{1}{d}\), for \(\forall t\leq T\) and \(\forall i,j\in[d]\),_

\[\left|\tilde{g}_{1}^{(t)}[i,j]-g_{1}^{(t)}[i,j]\right| \leq d^{3}M_{1}^{(t)}\left(M_{2}^{(t)}\right)^{2}\sigma\sqrt{dT} +M_{2}^{(t)}\sigma\sqrt{d^{2}T},\] \[\left|g_{2i}^{(t)}-g_{2i}^{(t)}\right| \leq d^{4}\left(M_{1}^{(t)}\right)^{2}M_{2}^{(t)}\sigma\sqrt{dT} +dM_{1}^{(t)}\sigma\sqrt{d^{2}T}.\]

Proof.: By Assumption 3 and Chebyshev's inequality, we have for fixed \(i,j\in[d]\) and \(t\leq T\),

\[\mathbb{P}\left(\left|\tilde{A}_{i}^{(t)}-A_{i}\right|>\lambda\right)\leq \frac{\sigma^{2}}{\lambda^{2}},\quad\mathbb{P}\left(\left|\tilde{\Lambda}_{xx }^{(t)}[i,j]-\Lambda_{xx}[i,j]\right|>\lambda\right)\leq\frac{\sigma^{2}}{ \lambda^{2}}.\]

Applying the union bound gives us

\[\mathbb{P}\left(\exists i\in[d],\exists t\leq T:\quad\left|\tilde{A}_{i}^{(t) }-A_{i}\right|>\lambda\right)\leq\frac{Td\sigma^{2}}{\lambda^{2}},\]

\[\mathbb{P}\left(\exists i,j\in[d],\exists t\leq T:\quad\left|\tilde{\Lambda}_{ xx}^{(t)}[i,j]-\Lambda_{xx}[i,j]\right|>\lambda\right)\leq\frac{Td^{2}\sigma^{2}}{ \lambda^{2}},\]

which gives us with probability at least \(1-\frac{1}{d}\), for \(\forall t\leq T,\forall i,j\in[d]\),

\[\left|\tilde{A}_{i}^{(t)}-A_{i}\right|\leq\sigma\sqrt{d^{2}T},\quad\left| \tilde{\Lambda}_{xx}^{(t)}[i,j]-\Lambda_{xx}[i,j]\right|\leq\sigma d\sqrt{dT}.\]

Now we are ready to bound \(\tilde{g}_{k}^{(t)}-g_{k}^{(t)}\) for \(k=1,2\) and \(t\leq T\).

Note that for all \(t\leq T\) and \(\forall i\in[d]\),

\[\left|\left(W_{2}^{(t)}W_{1}^{(t)}\right)_{i}\right|=\left|\sum_{j=1}^{d}w_{2 j}^{(t)}W_{1}^{(t)}[j,i]\right|\leq\sum_{j=1}^{d}\left|w_{2j}^{(t)}\right| \left|W_{1}^{(t)}[j,i]\right|\leq dM_{1}^{(t)}M_{2}^{(t)}.\]

Then we have with probability at least \(1-\frac{1}{d}\), for all \(t\leq T\) and \(\forall i\in[d]\),

\[\left|\left(W_{2}^{(t)}W_{1}^{(t)}\left(\tilde{\Lambda}_{xx}^{(t)}-\Lambda_{ xx}\right)\right)_{i}\right| \leq\sum_{j=1}^{d}\left|\left(W_{2}^{(t)}W_{1}^{(t)}\right)_{j} \right|\left|\tilde{\Lambda}_{xx}^{(t)}[j,i]-\Lambda_{xx}[j,i]\right|\]

\[\leq d^{3}M_{1}^{(t)}M_{2}^{(t)}\sigma\sqrt{dT}.\]

Combining with \(\tilde{g}_{1}^{(t)}-g_{1}^{(t)}=W_{2}^{(t)T}\left(W_{2}^{(t)}W_{1}^{(t)}\left( \tilde{\Lambda}_{xx}^{(t)}-\Lambda_{xx}\right)-\left(\tilde{A}^{(t)}-A\right)\right)\), we get that with probability at least \(1-\frac{1}{d}\), for all \(t\leq T\) and \(\forall i,j\in[d]\),

\[\left|\tilde{g}_{1}^{(t)}[i,j]-g_{1}^{(t)}[i,j]\right| \leq\left|w_{2i}^{(t)}\right|\left|\left(W_{2}^{(t)}W_{1}^{(t)} \left(\tilde{\Lambda}_{xx}^{(t)}-\Lambda_{xx}\right)\right)_{j}\right|+\left|w_ {2i}^{(t)}\right|\left|\tilde{A}_{j}^{(t)}-A_{j}\right|\] \[\leq d^{3}M_{1}^{(t)}\left(M_{2}^{(t)}\right)^{2}\sigma\sqrt{dT }+M_{2}^{(t)}\sigma\sqrt{d^{2}T}.\]

Similarly, note that \(\tilde{g}_{2i}^{(t)}-g_{2i}^{(t)}=\left(W_{2}^{(t)}W_{1}^{(t)}\left(\tilde{ \Lambda}_{xx}^{(t)}-\Lambda_{xx}\right)-\left(\tilde{A}^{(t)}-A\right)\right)W _{1}^{(t)T}\), we then have that with probability at least \(1-\frac{1}{d}\), for all \(t\leq T\) and \(\forall i,j\in[d]\),

\[\left|\tilde{g}_{2i}^{(t)}-g_{2i}^{(t)}\right| \leq\sum_{j=1}^{d}\left|\left(W_{2}^{(t)}W_{1}^{(t)}\left(\tilde{ \Lambda}_{xx}^{(t)}-\Lambda_{xx}\right)\right)_{j}\right|\left|W_{1}^{(t)}[i,j] \right|+\sum_{j=1}^{d}\left|\tilde{A}_{j}^{(t)}-A_{j}\right|\left|W_{1}^{(t)}[i,j]\right|\] \[\leq d^{4}\left(M_{1}^{(t)}\right)^{2}M_{2}^{(t)}\sigma\sqrt{dT }+dM_{1}^{(t)}\sigma\sqrt{d^{2}T}.\]

**Lemma 33**.: _Consider two sequences \(\{a^{(t)}\}_{t\geq 0},\{b^{(t)}\}_{t\geq 0}\), which satisfy_

\[a^{(t)}=(1-\beta)\sum_{\tau=0}^{t}\beta^{\tau}b^{(t-\tau)},\beta\in(0,1).\]

_Suppose \(\forall\tau\leq t:\left|b^{(t)}\right|\leq B\), then for any \(\epsilon>0\), the following truncated version_

\[\tilde{a}^{(t)}=(1-\beta)\sum_{\tau=0}^{H}\beta^{\tau}b^{(t-\tau)}\]

_with \(H\geq\frac{1}{1-\beta}\log\frac{B}{\epsilon}=\tilde{\Omega}\left(\frac{1}{1- \beta}\right)\) satisfies_

\[\left|a^{(t)}-\tilde{a}^{(t)}\right|\leq\epsilon.\]

Proof.: We have that

\[\left|a^{(t)}-\tilde{a}^{(t)}\right|\leq\left|(1-\beta)\sum_{\tau=H+1}^{t} \beta^{\tau}b^{(t-\tau)}\right|\leq(1-\beta)\sum_{\tau=H+1}^{t}\beta^{\tau}B \leq B\beta^{H+1}.\]

To make it less than \(\epsilon\), it suffices to choose \(H\geq\log(\frac{\epsilon}{B})/\log\beta\).

Since \(\beta\in(0,1)\), we know that \(\log\beta\leq\beta-1<0\). We also have \(\log\frac{\epsilon}{B}<0\). Then it suffices to choose

\[H\geq\frac{\log(\epsilon/B)}{\beta-1}\geq\frac{\log(\epsilon/B)}{\log\beta} \quad\Rightarrow\quad H\geq\frac{1}{1-\beta}\log\frac{B}{\epsilon}=\tilde{ \Omega}\left(\frac{1}{1-\beta}\right).\]

**Lemma 34**.: _Suppose \(a,b,c,e_{a},e_{b},e_{c}\in\mathbb{R},b>0,c>0\) satisfy \(b+e_{b}+e_{c}>0,|e_{a}|\leq\delta|a|,|e_{b}|\leq\delta b,|e_{c}|\leq\delta^{2 }c^{2}\) with \(0<\delta\ll 1\), then we have_

\[\frac{a+e_{a}}{\sqrt{b+e_{b}+e_{c}}+c}=\frac{a}{\sqrt{b}+c}(1+R),\quad\text{ where }|R|=\mathcal{O}(\delta).\]

Proof.: We have

\[\frac{a+e_{a}}{\sqrt{b+e_{b}+e_{c}}+c} =\frac{a}{\sqrt{b}+c}+\frac{a}{\sqrt{b+e_{b}+e_{c}}+c}-\frac{a}{ \sqrt{b}+c}+\frac{e_{a}}{\sqrt{b+e_{b}+e_{c}}+c}\] \[=\frac{a}{\sqrt{b}+c}\left(1+\underbrace{\frac{\sqrt{b}+c}{ \sqrt{b+e_{b}+e_{c}}+c}-1}_{q_{1}}+\underbrace{\frac{e_{a}}{a}\cdot\frac{ \sqrt{b}+c}{\sqrt{b+e_{b}+e_{c}}+c}}_{q_{2}}\right).\]

Define \(R:=q_{1}+q_{2}\). The term \(|q_{1}|\) can be bounded by

\[|q_{1}| =\frac{\left|\sqrt{b}-\sqrt{b+e_{b}+e_{c}}\right|}{\sqrt{b+e_{b}+ e_{c}}+c}\] \[=\frac{|e_{b}+e_{c}|}{(\sqrt{b+e_{b}+e_{c}}+c)\left(\sqrt{b}+ \sqrt{b+e_{b}+e_{c}}\right)}\] \[\leq\frac{|e_{b}|}{(\sqrt{b+e_{b}+e_{c}}+c)\left(\sqrt{b}+\sqrt{ b+e_{b}+e_{c}}\right)}+\frac{|e_{c}|}{(\sqrt{b+e_{b}+e_{c}}+c)\left(\sqrt{b}+ \sqrt{b+e_{b}+e_{c}}\right)}\] \[\leq\frac{|e_{b}|}{(\sqrt{b+e_{b}+e_{c}}+c)\sqrt{b}}+\delta \,\frac{\sqrt{|e_{c}|}}{\sqrt{b+\sqrt{b+e_{b}+e_{c}}}},\]where \(|q_{3}|\) can be bounded by

\[|q_{3}|\stackrel{{(i)}}{{\leq}}\frac{\delta b}{(\sqrt{b+e_{b}}-\sqrt{ |e_{c}|}+c)\sqrt{b}}\leq\frac{\delta\sqrt{b}}{\sqrt{b(1-\delta)}+c(1-\delta)} \leq\frac{\delta\sqrt{b}}{\sqrt{b(1-\delta)}}=\mathcal{O}(\delta).\]

Here the denominator of \((i)\) uses \(b+e_{b}\geq b(1-\delta)>0\) and \(\sqrt{x+y}\geq\sqrt{x}-\sqrt{|y|}\) when \(x\geq 0,x+y\geq 0\).

Now let's bound \(|q_{4}|\). If \(e_{c}>0\), we have \(e_{c}=|e_{c}|\) and \(|q_{4}|\leq\frac{\sqrt{e_{c}}}{\sqrt{e_{c}}}=1\) since \(b+e_{b}\geq b(1-\delta)>0\).

If \(e_{c}\leq 0\), note that \(b+e_{b}+e_{c}>0\), we have \(|e_{c}|<b+b_{e}\leq b(1+\delta)\), which yields \(|q_{4}|\leq\frac{\sqrt{|e_{c}|}}{\sqrt{b}}=\mathcal{O}(1)\). Combining the above bounds give us \(|q_{1}|\leq|q_{3}|+\delta|q_{4}|=\mathcal{O}(\delta)\).

On the other hand, \(|q_{2}|\) can be bounded by

\[|q_{2}|\leq\delta\frac{\sqrt{b}+c}{\sqrt{b+e_{b}}-\sqrt{|e_{c}|}+c}\leq\delta \frac{\sqrt{b}+c}{\sqrt{b(1-\delta)}+c(1-\delta)}=\mathcal{O}(\delta).\]

Then \(|R|\leq|q_{1}|+|q_{2}|=\mathcal{O}(\delta)\) 

**Lemma 35**.: _Suppose \(X_{1},X_{2},...,X_{d}\) are i.i.d Gaussian with mean 0 and variance \(\sigma^{2}\), then for \(0<\delta<\frac{1}{e}\), we have with probability at least \(1-\delta\),_

\[\max_{1\leq i\leq d}X_{i}^{2}\geq\sigma^{2}\left(C_{1}\log d-C_{2}\log\log \frac{1}{\delta}\right)\]

_for some \(C_{1},C_{2}>0\)._

Proof.: It suffices to assume that \(\sigma^{2}=1\) and prove that w.p. at least \(1-\delta\), \(\max_{1\leq i\leq d}X_{i}^{2}\geq C_{1}\log d-C_{2}\log\log\frac{1}{\delta}\).

First, by the lower bound of Gaussian tail, there exists \(\alpha,\beta>0\) such that \(\mathbb{P}(|X_{i}|>x)=2\mathbb{P}(X_{i}>x)\geq\alpha e^{-\beta x^{2}}\) for \(x\geq 0\). Then by i.i.d., we have

\[\mathbb{P}(\max_{i}|X_{i}|\leq x) =\mathbb{P}\left(\bigcap_{i=1}^{d}\{|X_{i}|\leq x\}\right)\] \[=\prod_{i=1}^{d}\mathbb{P}(|X_{i}|\leq x)=(1-\mathbb{P}(|X_{i}|>x) )^{d}\] \[\leq(1-\alpha e^{-\beta x^{2}})^{d}\] \[\leq\exp(-d\alpha e^{-\beta x^{2}}),\]

where the last inequality uses \(1-x\leq e^{-x}\) for \(x\in[0,1]\). Let \(\exp(-d\alpha e^{-\beta x^{2}})=\delta\), we get that w.p. at least \(1-\delta\),

\[\max_{1\leq i\leq d}|X_{i}|\geq\sqrt{\frac{1}{\beta}\left(\log(\alpha d)-\log \log\frac{1}{\delta}\right)}.\]

Then we have w.p. at least \(1-\delta\),

\[\max_{1\leq i\leq d}X_{i}^{2}=\left(\max_{1\leq i\leq d}|X_{i}|\right)^{2} \geq\frac{1}{\beta}\left(\log(\alpha d)-\log\log\frac{1}{\delta}\right).\]

**Lemma 36**.: _Suppose \(X_{1},X_{2},...,X_{d}\) are i.i.d Gaussian with mean 0 and variance \(\sigma^{2}\), then we have with constant probability,_

\[\frac{1}{d}\sum_{i=1}^{d}\frac{1}{|X_{i}|}\leq\mathcal{O}\left(\frac{1}{\sigma }\log d\right).\]Proof.: It suffices to assume that \(\sigma^{2}=1\) and prove that with constant probability, \(\frac{1}{d}\sum_{i=1}^{d}\frac{1}{|X_{i}|}\leq\mathcal{O}\left(\log d\right)\).

Consider \(X_{i}\) for some fixed \(i\). Since \(X_{i}\sim\mathcal{N}(0,1)\), we have \(\mathbb{P}(|X_{i}|\leq t)\leq\frac{2t}{\sqrt{2\pi}}\). Then we know that with probability at least \(1-\Theta\left(d^{-1}\right)\), \(|X_{i}|\geq\frac{C}{d}\) for some \(C>0\). Then by union bound, with constant probability, \(\forall i\in[d]:|X_{i}|\geq\frac{C}{d}\).

Now we split the interval \([\frac{C}{d},1]\) into several subintervals \(\mathcal{I}_{k}=\{i:|X_{i}|\in[2^{-k-1},2^{-k}]\}\) for \(k=0,1,...,\lceil\log_{2}\frac{d}{C}\rceil-1\). Let \(p_{k}=\mathbb{P}(|X_{i}|\in[2^{-k-1},2^{-k}])\), we know that \(|\mathcal{I}_{k}|\sim\text{Binomial}(d,p_{k})\) and \(p_{k}\leq C_{1}\cdot 2^{-k-1}\). Then by the concentration of binomial variables, we have w.p. at least \(1-d^{-p}\) for \(p>0\), \(|\mathcal{I}_{k}|=\mathcal{O}\left(dp_{k}+\sqrt{dp_{k}\log d}+\log d\right)= \mathcal{O}\left(d\cdot 2^{-k-1}+\sqrt{d\cdot 2^{-k-1}\log d}+\log d\right)\). Then we have

\[\sum_{i\in\mathcal{I}_{k}}\frac{1}{|X_{i}|}\leq|\mathcal{I}_{k}|2^{k+1}= \mathcal{O}\left(d+\sqrt{d\cdot 2^{k+1}\log d}+2^{k+1}\log d\right),\quad k=0,1,...,\lceil\log_{2}\frac{d}{C}\rceil-1.\]

Therefore, with constant probability,

\[\sum_{i=1}^{d}\frac{1}{|X_{i}|} =\sum_{k=0}^{\lceil\log_{2}\frac{d}{C}\rceil-1}\sum_{i\in\mathcal{ I}_{k}}\frac{1}{|X_{i}|}+\sum_{|X_{i}|>1}\frac{1}{|X_{i}|}\] \[\leq\sum_{k=0}^{\lceil\log_{2}\frac{d}{C}\rceil-1}\mathcal{O} \left(d+\sqrt{d\cdot 2^{k+1}\log d}+2^{k+1}\log d\right)+d\] \[=\mathcal{O}\left(d\log_{2}\frac{d}{C}\right)+\mathcal{O}\left( \sqrt{d\log d}\cdot(\sqrt{2})^{\lceil\log_{2}\frac{d}{C}\rceil+C_{2}}\right) +2^{\lceil\log_{2}\frac{d}{C}\rceil+1}\log d+d\] \[=\mathcal{O}\left(d\log d\right),\]

which means with constant probability, \(\frac{1}{d}\sum_{i=1}^{d}\frac{1}{|X_{i}|}=\mathcal{O}\left(\log d\right)\).