# On the Limitation of Backdoor Detection Methods

Georg Pichler\({}^{1}\) Marco Romanelli\({}^{2}\)

**Divya Prakash Manivannan\({}^{2}\)** **Prashanth Krishnamurthy\({}^{2}\)** **Farshad Khorrami\({}^{2}\)**

**Siddharth Garg\({}^{2}\)**

\({}^{1}\)TU Wien \({}^{2}\)New York University

###### Abstract

We introduce a formal statistical definition for the problem of backdoor detection in machine learning systems and use it to analyze the feasibility of such problem, providing evidence for the utility and applicability of our definition. The main contributions of this work are an impossibility result and an achievability result for backdoor detection. We show a no-free-lunch theorem, proving that universal backdoor detection is impossible, except for very small alphabet sizes. Furthermore, we link our definition to the probably approximately correct (PAC) learnability of the out-of-distribution detection problem, establishing a formal connection between backdoor and out-of-distribution detection.

## 1 Introduction

Safe and trustworthy Machine Learning (ML) systems remain elusive [8; 15], for reasons that are intrinsic, like poor interpretability [1; 13], and due to external threats, including inference time adversarial inputs [6; 8; 4] and training time poisoning and backdoor attacks [5]. As the scale, complexity and training data requirements of modern deep neural network architectures has grown, many users resort to using and/or fine-tuning pre-trained models. Consequently, purposefully implanted backdoors pose a security risk for ML systems.

In the classic backdoor threat model, a malicious actor may provide poisoned data, affecting the behavior of trained ML model. For certain, _poisoned_ inputs, which are modified in a specific way, known to the attacker, the model then provides erroneous predictions. While there are many ways such a backdoor could be embedded into a model, prior work shows that poisoning even a small fraction of training data yields models with stealthy and effective backdoors [12]. To detect a backdoor, the model user (i.e., the _defender_) has access to a, typically small, validation dataset of clean inputs. In the Model Backdoor Detection (MBD) problem [9; 7], the defender wishes to detect if the model itself contains a backdoor. In the Input Backdoor Detection (IBD) problem [10; 11], the defender wants to test if a specific test input is poisoned or not. Yet, despite several years of research, the field is still plagued by the cat-and-mouse game between attacks and defenses, with no end in sight. Unlike the work on adversarial perturbation attacks, for instance, "certifiable" defenses have remained elusive. We argue that this is in part because, despite the large body of work in the area, backdoor detection has not been formally defined, at least not in a precise and well-posed manner. This lack of formal treatment has negative consequences as it impedes fair and consistent comparison between methods.

**Contributions.** In this paper, we present the first precise statistical formulation of the MBD and IBD problems (Section 2.1). This formulation enables several new insights on backdoor detection.

(1) _Relationship to well-known statistical problems:_ Our formulation unifies MBD, IBD and even Out-Of-Distribution (OOD) detection within a common framework and we reduce these problems to standard statistical hypothesis testing problems. (2) _Infeasibility:_ Leveraging these reductions, we conclude that under realistic assumptions, universal (adversary-unaware) backdoor detection is not possible for an infinite alphabet of the training data. (3) _Bound for finite alphabet size:_ For a finitedata alphabet, we provide a bound on the achievable error probability given a fixed training set size. These bounds are evaluated for commonly used datasets in ML, showing that universal backdoor detection is only achievable for very small alphabets. (4) _Connections to Probably Asymptotically Correct (PAC) learning theory of OOD detection:_ Detecting a backdoor in training data is equivalent to a binary Neyman-Pearson hypothesis test if OOD detection is PAC learnable as defined in [3].

## 2 Theoretical Formulation and Results

We focus on MBD and IBD in the case where the attacker has limited control over the training data and is able to poison a certain portion of the dataset. The training itself is performed using a standard method, e.g., Stochastic Gradient Descent (SGD). For an extensive overview of other empirical backdoor problems, the reader is referred to, e.g., [15].

### Formulating Model Backdoor Detection (MBD)

**Overview.** After \(N\) samples of training data are collected, the backdoor attacker has the option of poisoning a portion of the training data, by replacing each _clean_ sample with a _poisoned_ sample. This poisoning may alter, e.g., images as well as their labels. Subsequently, an Artificial Neural Network (ANN) is trained on the resulting training set. Given the resulting trained network (i.e., the network parameters), the task of the backdoor detector is to determine whether the training data had been poisoned. The detector may obtain \(M\) additional _clean_ samples, e.g., by independently collecting additional data. We assume that the backdoor attacker has no access to these samples.

**Dataset and training.** Consider a, possibly stochastic, training algorithm \(\mathcal{A}\) (e.g., SGD), that trains a model on training data1\(\mathcal{D}_{N}=(X_{1},X_{2},\ldots,X_{N})\), consisting of \(N\) i.i.d. random variables, distributed like \(X\sim P\), as input and produces a parameter vector \(\theta=\mathcal{A}(\mathcal{D}_{N})\) as output.

Footnote 1: The training sample \(X\) may be a vector that includes data and label.

**Clean data.** Let \(P_{0}\in\mathcal{P}(\mathcal{X})\) be the probability distribution on \(\mathcal{X}\) of clean samples and let \(\mathcal{D}_{N}^{(0)}=(X_{1}^{(0)},X_{2}^{(0)},\ldots,X_{N}^{(0)})\) be a clean dataset, consisting of \(N\) i.i.d. random variables, drawn from \(P_{0}\).

**Backdoor.** To _backdoor_ a model trained on the clean dataset, an adversary may replace some training samples with poisoned samples drawn from a different distribution \(P_{b}\in\mathcal{P}(\mathcal{X})\). As a training sample may include the data and the label, the adversary could change the data and label.

**Poisoned training data.** Assuming that a fraction \(\gamma\in(0,1]\) of the training data is poisoned, the poisoned training dataset \(\mathcal{D}_{N}^{(1)}=(X_{1}^{(1)},X_{2}^{(1)},\ldots,X_{N}^{(1)})\) is independently drawn according to \(P_{1}=\gamma P_{b}+(1-\gamma)P_{0}\), i.e., according to \(P_{b}\) with probability \(\gamma\) and from \(P_{0}\) with probability \(1-\gamma\).

**Additional clean data.** Furthermore, let \(\mathcal{D}_{M}^{\prime}=(X_{1}^{\prime},X_{2}^{\prime},\ldots,X_{M}^{\prime}) \sim P_{0}^{M}\) be \(M\) i.i.d. additional clean samples distributed according to \(P_{0}\). These samples correspond to clean validation data or may have been collected by the backdoor detector prior to making a decision.

**Model Backdoor Detection.** The backdoor detector is a function \(g\), that takes \(\theta=\mathcal{A}(\mathcal{D}_{N}^{(j)})\) and additional data \(\mathcal{D}_{M}^{\prime}\) as its input and outputs \(0\) for _"backdoor"_ and \(1\) for _"no backdoor"_. For MBD, we require the detector to determine \(j\) with high probability. For ease of notation, we use a Bernoulli variable \(J\sim\mathcal{B}(\frac{1}{2})\) and define the input for the detector as \(\mathbf{Q}=(\mathcal{A}(\mathcal{D}_{N}^{(J)}),\mathcal{D}_{M}^{\prime})\), such that the error probability \(\Pr\{g(\mathbf{Q})\neq J\}\) of the detector is well-defined.

**Possible data distributions.** The last observation to obtain a well-defined backdoor detection problem is that we need to avoid the possibility of \(P_{0}=P_{b}\). Detection is impossible if the clean and the backdoor distributions are identical. We opt for the general approach of defining a suitable set \(\mathcal{P}\subseteq\mathcal{P}(\mathcal{X})^{2}\) that contains all possible clean and backdoor distribution pairs \((P_{0},P_{b})\in\mathcal{P}\).

These discussions then naturally lead to the following central definition.

**Definition 1**.: _The MBD problem for a training algorithm \(\mathcal{A}\) is determined by the following quantities: \(\gamma\in(0,1]\), \(N\in\mathbb{N}\), \(M\in\mathbb{N}\), and \(\mathcal{P}\subseteq\mathcal{P}(\mathcal{X})^{2}\).__Fixing these quantities, we define the risk of a backdoor detector \(g\) associated with \((P_{0},P_{b})\) as_

\[R(g;P_{0},P_{b}):=\Pr\{g(\mathbf{Q})\neq J\}=\frac{1}{2}\sum_{j=0,1}\Pr\{g( \mathcal{A}(\mathcal{D}_{N}^{(j)}),\mathcal{D}_{M}^{\prime})\neq j\}.\] (1)

_We say that a backdoor detector is \(\alpha\)-error for some \(\alpha\in[0,\frac{1}{2}]\) if, for every pair \((P_{0},P_{b})\in\mathcal{P}\), the risk is bounded by_

\[R(g;P_{0},P_{b})\leq\alpha.\] (2)

_Remark 1_.: Instead of bounding the risk as in (2), it may seem more natural to require \(\Pr\{g(\mathbf{Q})\neq j|J=j\}\leq\alpha\) for both \(j=0,1\), but note that \(\Pr\{g(\mathbf{Q})\neq j|J=j\}\leq 2\alpha\) for \(j=0,1\) immediately follows from (2).

### (In)feasibility of Model Backdoor Detection

It will be useful to consider easier problems than \(\alpha\)-error detection, as defined in Definition 1 and establish reductions. To this end, we consider four different _Types_ of detectors. All these detectors need to infer \(J\), but different inputs are available to them:

1. \(g_{0}(\mathbf{Q}_{0})\) with \(\mathbf{Q}_{0}=\mathbf{Q}=(\mathcal{A}(\mathcal{D}_{N}^{(J)}),\mathcal{D}_{M }^{\prime})\): The default detector as used in Definition 1.
2. \(g_{1}(\mathbf{Q}_{1})\) with \(\mathbf{Q}_{1}=(\mathcal{D}_{N}^{(J)},\mathcal{D}_{M}^{\prime})\): Provide the detector with the training dataset \(\mathcal{D}_{N}^{(J)}\) and \(M\) independent clean samples \(\mathcal{D}_{M}^{\prime}\).
3. \(g_{2}(\mathbf{Q}_{2})\) with \(\mathbf{Q}_{2}=(\mathcal{D}_{N}^{(J)},P_{0})\): Provide the detector with the training dataset \(\mathcal{D}_{N}^{(J)}\), and with the clean data distribution \(P_{0}\).
4. \(g_{3}(\mathbf{Q}_{3})\) with \(\mathbf{Q}_{3}=(\mathcal{D}_{N}^{(J)},P_{0},P_{b})\): Provide the detector with the training dataset \(\mathcal{D}_{N}^{(J)}\), the clean distribution \(P_{0}\) and with the backdoor distribution \(P_{b}\). This is a binary Neyman-Pearson hypothesis testing problem between \(P_{0}^{N}\) and \(P_{1}^{N}\).

We assume that detectors of Types 2 and 3 have access to \(P_{0}\) (and \(P_{b}\) for a Type 3 detector) in terms of evaluation of the distribution, and also have the ability to sample from the distribution. We thus consider Types 2 and 3 as randomized detectors to account for sampling. The definitions of risk and \(\alpha\)-error detection of \(g_{2},g_{3}\) apply mutatis mutandis as in Definition 1, where the probability in (1) is also taken over the randomness of \(g\).

Remark 3 proposes an ordering of the Types of detectors, according to the information provided.

In Section 2.2.1 we will show that for a reasonable \(\mathcal{P}\), \(\alpha\)-error Type 2 detection is impossible with \(\alpha<\frac{1}{2}\). The reduction argument in Remark 3 thus ensures that \(\alpha\)-error detection with \(\alpha<\frac{1}{2}\) is also impossible for Type 0 and Type 1 detectors.

We can resolve the situation for a Type 3 detector using the Neyman-Pearson lemma.

**Lemma 1**.: _Given a Type 3 backdoor detector \(g_{3}(\mathcal{D}_{N},P_{0},P_{b})\), for any pair \((P_{0},P_{b})\in\mathcal{P}(\mathcal{X})^{2}\)_

\[R(g_{3};P_{0},P_{b})\geq\frac{1}{2}-\frac{1}{2}\operatorname{TV}(P_{0}^{N},P_ {1}^{N})\geq\frac{1}{2}-\frac{\gamma N}{2}\operatorname{TV}(P_{0},P_{b}),\] (3)

_where the first equality in (3) can be achieved by the Neyman-Pearson detector. Thus, an \(\alpha\)-error detector of Type 3 can only exist if \(\alpha\geq\frac{1}{2}-\frac{\gamma N}{2}\operatorname{TV}(P_{0},P_{b})\) for all \((P_{0},P_{b})\in\mathcal{P}\)._

See proof in Appendix A.3.

Before analyzing Types 1 and 2, we specify the set of allowable distributions \(\mathcal{P}\) using Lemma 1.

Merely excluding the identity \(P_{0}\neq P_{b}\), i.e., \(\mathcal{P}=\{(P_{0},P_{b})\in\mathcal{P}(\mathcal{X})^{2}:P_{0}\neq P_{b}\}\) is not sufficient.

_Example 1_.: Let \(g_{3}(\mathcal{D}_{N},P_{0},P_{1})\) be an \(\alpha\)-error Type 3 detector and assume that \(\mathcal{X}\) is infinite, i.e., \(|\mathcal{X}|=\infty\). Let \(\mathcal{P}\) be given as above, ensuring only that \(P_{0}\neq P_{b}\). For any \(\varepsilon>0\), we can then choose2\((P_{0},P_{b})\in\mathcal{P}\) with \(0<\operatorname{TV}(P_{0},P_{b})\leq\frac{2}{\gamma N}\varepsilon\). By Lemma 1, we have \(\alpha\geq\frac{1}{2}-\frac{\gamma N}{2}\operatorname{TV}(P_{0},P_{b})\geq \frac{1}{2}-\varepsilon\). As \(\varepsilon>0\) was arbitrary, we have \(\alpha=\frac{1}{2}\).

Lemma 1 and Example 1 show that even for a Type 3 detector, we need \(\mathrm{TV}(P_{0},P_{b})>\frac{1-2\alpha}{\gamma N}\) for all \((P_{0},P_{b})\in\mathcal{P}\), in order for \(\alpha\)-error detection to be achievable. In the following we will assume that \(\mathcal{P}\) is the set of probability distributions \(P_{0},P_{b}\) with \(\mathrm{TV}(P_{0},P_{b})\geq 1-\beta\), for some fixed \(\beta\in[0,1)\). This strong requirement is motivated by the fact that in this case, \(\frac{1-\gamma+\gamma\beta}{2}\)-error Type 3 detection is achievable with only \(N=1\) sample.

_Remark 2_.: Thorough reasoning and examples, illustrating why total variation distance is the preferred distance measure for distribution hypothesis testing can be found in [2, Section 1.2].

#### 2.2.1 Impossibility

In the following we prove an impossibility result, which implies that _for an infinite alphabet \(\mathcal{X}\), the error probability (as given in Definition 1) of any detector (of Type 0, Type 1 or Type 2) is \(\frac{1}{2}\), the error probability of a random guess._ Additionally, for finite \(\mathcal{X}\), we provide a lower bound on the size of the training set \(N\), as a function of \(\alpha\).

**Theorem 1**.: _Fix \(N\in\mathbb{N}\), \(\alpha\in(0,\frac{1}{2}]\), \(\beta\in[0,1]\), and \(\mathcal{P}=\{(P_{0},P_{b}):\mathrm{TV}(P_{0},P_{b})\geq 1-\beta\}\). Let \(g_{2}(\mathcal{D}_{N},P_{0})\) be an \(\alpha\)-error Type 2 detector. For \(|\mathcal{X}|=\infty\), we then have necessarily \(\alpha=\frac{1}{2}\), while for \(|\mathcal{X}|<\infty\), we have_

\[N\geq\frac{\log 2\alpha}{2}+\sqrt{\frac{(\log 2\alpha)^{2}}{4}+(\beta| \mathcal{X}|-1)\log\frac{1}{2\alpha}}.\] (4)

See proof in Appendix A.3.

For a fixed dataset alphabet size \(|\mathcal{X}|\) and allowed error probability \(\alpha\), the bound (4) gives the minimum size of the training set \(N\) for the error level \(\alpha\) to be achievable. Note the following special cases in terms of \(\alpha\), \(\beta\): _i)_ For \(\alpha=\frac{1}{2}\), the bound (4) is always satisfied as the RHS is \(0\), showing that \(\frac{1}{2}\)-error detection is always achievable. This coincides with the error probability of a random guess. _ii)_ The bound (4) is monotonically decreasing in \(\alpha\) and for \(\alpha\to 0\), it approaches \(\beta|\mathcal{X}|\). _iii)_ In case \(\beta=0\), the bound (4) is always satisfied as the RHS is zero for \(\alpha\in(0,\frac{1}{2}]\) in this case. This shows that \(\alpha\)-error detection is always possible if \(P_{0}\) and \(P_{b}\) have disjoint support, i.e. \(\mathrm{TV}(P_{0},P_{b})=1\).

For an infinite alphabet \(\mathcal{X}\), (4) needs to be satisfied for arbitrarily large values of \(|\mathcal{X}|\). For finite training set size \(N\), this is only possible if \(\alpha=\frac{1}{2}\) as then, \(\log\frac{1}{2\alpha}=0\). Thus, in this case, for any Type 2 detector, there is a particular clean distribution and backdoor strategy, such that this detector performs no better than random guessing. For fixed \(\alpha\) and \(\beta\), we can use (4) to determine the minimum size of the training set \(N\) for popular datasets, for \(\alpha\) error probability to be achievable by a Type 2 detector. To this end, we use the width \(W\), height \(H\), number of channels \(C\) and color depth \(P\) of an image dataset to compute \(|\mathcal{X}|=P^{WHC}\). For categorical datasets, we may multiply the number of categories for all the properties recorded in the dataset to obtain \(|\mathcal{X}|\). The resulting value for the bound in (4) is given in Table 1 for several popular datasets. As can be seen by these numbers, this universal backdoor detection is infeasible for all, but the smallest tabular datasets. Note also, that the impossibility of Type 2 backdoor detection automatically precludes the existence of Type 1 or Type 0 error detectors with equal performance by the reduction argument in Remark 3.

#### 2.2.2 Achievability

In this section we are going to show that achievability is possible and that it is related to the size of the alphabet \(|\mathcal{X}|\). We consider a Type 2 detector and give a criterion for \(\alpha\)-error detection achievability:

**Theorem 2**.: _Considering the backdoor detection setup of Definition 1 with \(\mathcal{P}=\{(P_{0},P_{b}):\mathrm{TV}(P_{0},P_{b})\geq 1-\beta\}\) and a finite alphabet \(|\mathcal{X}|<\infty\). There exists an \(\alpha\)-error Type 2 detector if_

\[\alpha>2|\mathcal{X}|\exp\left(-\frac{2N\gamma^{2}(1-\beta)^{2}}{| \mathcal{X}|^{2}}\right),\] (5)

See proof in Appendix A.3.

Note the following special cases in terms of \(\alpha\), \(\beta\) and \(\gamma\): _i)_ For \(\alpha=0\), (5) cannot be satisfied, showing that \(0\)-error detection cannot be achieved. _ii)_ The case \(\beta=1\) allows for \(P_{0}=P_{b}\) and thus no \(\alpha\)-error detector exists for \(\alpha\in[0,\frac{1}{2})\) in this case and (5) cannot be satisfied. _iii)_ For \(\gamma=1\), \(P_{0}=P_{b}\) are identical, no \(\alpha\)-error detector exists for \(\alpha\in[0,\frac{1}{2})\), and (5) cannot be satisfied.

#### 2.2.3 Connections to PAC-Learnability of OOD Detection

Note that a Type 1 detector essentially needs to solve an OOD detection problem. In this case, we are provided samples and the detector \(g_{1}\) needs to determine if the \(N\) samples \(\mathcal{D}_{N}\) were drawn from the same distribution as \(\mathcal{D}^{\prime}_{M}\).

The goal of this section is to prove Theorem 3. This theorem has an interesting implication in case the OOD detection problem is PAC-learnable: If an \(\alpha\)-error Type 3 backdoor detector \(g_{3}\) exists, then \((\alpha+\epsilon)\)-error detection is also possible for a Type 1 detector for any \(\epsilon>0\). Thus, essentially Types 1 to 3 all become equivalent if OOD detection is PAC-learnable. Note here that Type 3 detection is completely characterized by Lemma 1.

The PAC-learnability of the detector in Type 1 was analyzed in [3]. We fist restate a special case of the definition of (weak) PAC-learnability as given in [3, Def. 1].

**Definition 2**.: _For distributions \(P_{0},P_{b}\) on \(\mathcal{X}\), the OOD-risk of a function \(f:\mathcal{X}\to\{0,1\}\), w.r.t. the Hamming distance, is defined as_

\[\bar{R}(f,P_{0},P_{b}):=\Pr\{f(X^{(J)})\neq J\}=\frac{1}{2}\Pr\{f(X^{(0)})=1\} +\frac{1}{2}\Pr\{f(X^{(1)})=0\}.\] (6)

_Given a space of probability function \(\mathcal{P}\), OOD-detection is PAC-learnable on \(\mathcal{P}\) if there exists an algorithm \(\mathcal{G}\colon\bigcup_{m=1}^{\infty}\mathcal{X}^{m}\to\{0,1\}^{\mathcal{X}}\) and a monotonically decreasing sequence \(\epsilon(m)\) such that \(\lim_{m\to\infty}\epsilon(m)=0\) and for all \((P_{0},P_{b})\in\mathcal{P}\), and all \(m\in\mathbb{N}\) we have_

\[\mathbb{E}[\bar{R}(\mathcal{G}(\mathcal{D}^{\prime}_{m}),P_{0},P_{b})]-\inf_{ f}\bar{R}(f,P_{0},P_{b})\leq\epsilon(m),\] (7)

_where the expectation is taken w.r.t. \(\mathcal{D}^{\prime}_{m}\) and the infimum is over \(\{0,1\}^{\mathcal{X}}\), i.e., all \(f\colon\mathcal{X}\to\{0,1\}\)._

Appendix A.2 shows how Definition 2 is a special case of [3, Def. 1]. We consider PAC-learnability on the \(N\)-dimensional product space, i.e., on \(\mathcal{X}^{N}\) with distributions \(P_{0}^{N},P_{b}^{N}\). We can now connect PAC-learnability to the existence of \(\alpha\)-error detectors of Types 1 and 3.

**Theorem 3**.: _Consider the setup of Definition 1, with fixed \(\gamma\in(0,1]\), \(N\in\mathbb{N}\) and \(\mathcal{P}\). Let \(\mathcal{P}^{\prime}\) be the set of \(N\)-fold products of \((P_{0},P_{1})\), i.e., \(\mathcal{P}^{\prime}=\{(P_{0}^{N},(\gamma P_{b}+(1-\gamma)P_{0})^{N}):(P_{0},P_ {b})\in\mathcal{P}\}\). Then, OOD-detection is PAC-learnable on \(\mathcal{P}^{\prime}\) if and only if the following holds for any \(\epsilon>0\) and any Type 3 detector \(g_{3}(\mathcal{D}_{N},P_{0},P_{b})\): We can find \(M\in\mathbb{N}\) and a Type 1 detector \(g_{1}(\mathcal{D}_{N},\mathcal{D}^{\prime}_{M})\), which satisfies \(R(g_{1},P_{0},P_{b})\leq R(g_{3},P_{0},P_{b})+\epsilon\) for every \((P_{0},P_{b})\in\mathcal{P}\)._

See proof in Appendix A.3.

**Corollary 1**.: _If OOD-detection is PAC-learnable on \(\mathcal{P}^{\prime}\), we have the following: If \(\alpha\)-error backdoor detection is possible in the easier case of Type 3 detection, which is completely characterized by Lemma 1, then \((\alpha+\epsilon)\)-error detection is also possible for a Type 1 detector for any \(\epsilon>0\). Consequently, the achievability of backdoor detection for Types 1 to 3 detectors are all equivalent up to topological closure if OOD-detection is PAC-learnable on \(\mathcal{P}^{\prime}\)._

### Generalizing to Input Backdoor Detection

We generalize Definition 1 to IBD. Let \(g^{\prime}(\mathbf{Q}^{\prime})\) take input \(\mathbf{Q}^{\prime}=(\mathbf{Q},X^{(I)})=(\mathcal{A}(\mathcal{D}^{(J)}_{N}), \mathcal{D}^{\prime}_{M},X^{(I)})\), where a random variable \(I\) on \(\{0,1\}\) determines if \(X^{(I)}\) was drawn as \(X^{(0)}\sim P_{0}\) (\(I=0\)) or as3\(X^{(1)}\sim P_{b}\) (\(I=1\)). We define a general target function \(t(j,i)\in\{0,1\}\) and require that a backdoor detector satisfies \(g^{\prime}(\mathbf{Q}^{\prime})=t(J,I)\) with high probability. In this case, it is beneficial to allow for an arbitrary probability distribution \(P_{JI}\) of \((J,I)\) on \(\{0,1\}^{2}\). This leads to the following definition

\begin{table}
\begin{tabular}{l l l} \hline \hline Dataset & \(|\mathcal{X}|\) & N \\ \hline Lisa Traffic Sign & \(256^{307200}\) & \(\geq 10^{369904}\) \\ ImageNet & \(256^{150528}\) & \(\geq 10^{181252}\) \\ CIFAR10 & \(256^{3072}\) & \(\geq 10^{3697}\) \\ MNIST & \(256^{784}\) & \(\geq 10^{942}\) \\ B/W MNIST & \(27^{84}\) & \(\geq 10^{116}\) \\ Adult & \(\geq 10^{21.86}\) & \(\geq 10^{9}\) \\ Heart Disease & \(\geq 10^{13.51}\) & \(\geq 10^{5}\) \\ Iris & \(\geq 10^{6.35}\) & \(\geq 10^{1}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Lower bound (4) on \(N\) evaluated for popular datasets with \(\alpha=0.1\) and \(\beta=0.001\).

**Definition 3**.: _A backdoor detection problem for a training algorithm \(\mathcal{A}\) is determined by the following quantities: \(\gamma\in(0,1]\), \(N\in\mathbb{N}\), \(M\in\mathbb{N}\), \(\mathcal{P}\subseteq\mathcal{P}(\mathcal{X})^{2}\), \(P_{JI}\in\mathcal{P}(\{0,1\}^{2})\), and \(t\colon\{0,1\}^{2}\to\{0,1\}\). Fixing these quantities, we define the risk of a backdoor detector \(g^{\prime}\) associated with \((P_{0},P_{b})\) as \(R(g^{\prime};P_{0},P_{b}):=\Pr\{g^{\prime}(\mathbf{Q}^{\prime})\neq t(J,I)\}\), where the probability is w.r.t. \(\mathbf{Q}^{\prime}=(\mathcal{A}(\mathcal{D}_{N}^{(J)}),\mathcal{D}_{M}^{ \prime},X^{(I)})\) and \((J,I)\sim P_{JI}\). We say that a backdoor detector is \(\alpha\)-error for some \(\alpha\in[0,\frac{1}{2}]\) if, for every pair \((P_{0},P_{b})\in\mathcal{P}\), the risk is bounded by \(R(g^{\prime};P_{0},P_{b})\leq\alpha\)._

OOD can be modeled using the target function \(t(j,i)\) for MBD, IBD and OOD Figure 1.

Note that several cells in the diagrams in Fig. 1 are grayed out. This reflects the fact that for certain flavors of backdoor detection, specific combinations of \((j,i)\) are not relevant. For MBD for instance, we are not interested in whether the target sample \(X^{(I)}\) contains a backdoor and we can thus assume \(I=0\) in this case, effectively reducing this case to the problem introduced in Section 2.1 with \(M+1\) samples being drawn from \(P_{0}\), i.e., \((\mathcal{D}_{M}^{\prime},X^{(0)})=\mathcal{D}_{M+1}^{\prime}\), available to the detector. Conversely, the case of a clean model, i.e., \(j=0\) and a sample with a backdoor, i.e., \(i=1\) is not realistic for IBD and we set \(P_{JI}(0,1)=0\) in this case. By setting \(J=0\) (i.e., model is trained on clean data and \(P_{JI}(1,0)=P_{JI}(1,1)=0\)) and using \(t_{\text{OOD}}(j,i)=t_{\text{IBD}}(i,0)=i\), we obtain an OOD detection problem, where the detector has access to a model \(\mathcal{A}(\mathcal{D}_{N}^{(0)})\) trained on clean data and additional clean data \(\mathcal{D}_{M}^{\prime}\). The detector then needs to determine whether \(X^{(I)}\) is in-distribution (\(I=0\)) or out-of-distribution (\(J=1\)). To showcase, how our result from Sections 2.2.1 and 2.2.2 carry over to other variants of backdoor detection, we will directly use Theorem 1 to derive a similar result for IBD. In analogy to the different Types of MBD detectors introduced in Section 2, we have a Type 2 detector \(g_{2}^{\prime}(\mathbf{Q}_{2}^{\prime})\) with \(\mathbf{Q}_{2}^{\prime}=(\mathcal{D}_{N}^{(J)},P_{0},X^{(I)})\) for IBD. For such a detector we can leverage a reduction argument to obtain the following.

**Corollary 2**.: _Let \(g_{2}^{\prime}(\mathcal{D}_{N^{\prime}}^{(J)},P_{0},X^{(I)})\) be a Type 2 detector for an IBD problem with \(r=\min\{P_{JI}(0,0),P_{JI}(1,1)\}>0\) and \(\mathcal{P}=\{(P_{0},P_{b}):\operatorname{TV}(P_{0},P_{b})\geq 1-\beta\}\). Then, if \(g_{2}^{\prime}\) is \(\alpha\)-error, we have \(\alpha\geq r\) if \(|\mathcal{X}|=\infty\), and for \(|\mathcal{X}|<\infty\), we obtain_

\[N\geq\frac{\log\frac{\alpha}{r}}{2}+\sqrt{\frac{(\log\frac{\alpha}{r})^{2}}{4} +(\beta|\mathcal{X}|-1)\log\frac{r}{\alpha}}.\] (8)

See proof in Appendix A.3.

## 3 Conclusions

We provided a formal statistical definition of backdoor detection and investigated the feasibility of backdoor detection. We concluded that under realistic assumptions, universal (adversary-unaware) backdoor detection is not possible. Thus, effective backdoor detectors need to be adversary-aware.

## Acknowledgement

This work is supported in part by grants from the National Science Foundation (NSF) and the ARO 77191, in part by the Army Research Office under grant #W911NF-21-1-0155, and in part by the NYUAD Center for Artificial Intelligence and Robotics, funded by Tamkeen under the NYUAD Research Institute Award CG010.

Figure 1: Target function \(t(j,i)\) for different backdoor detection flavors.

## References

* [1]N. Burkart and M. F. Huber (2021) A survey on the explainability of supervised machine learning. Journal of Artificial Intelligence Research70, pp. 245-317. Cited by: SS1.
* [2]C. L. Canonne (2022) Topics and techniques in distribution testing: a biased but representative sample. Foundations and Trends in Communications and Information Theory19 (6), pp. 1032-1198. Cited by: SS1.
* [3]Z. Fang, Y. Li, J. Lu, J. Dong, B. Han, and F. Liu (2022) Is out-of-distribution detection learnable?. Note: October 2022 External Links: Link Cited by: SS1.
* [4]I. J. Goodfellow, J. Shlens, and C. Szegedy (2015) Explaining and harnessing adversarial examples. In Proceedings of Machine Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Cited by: SS1.
* [5]T. Gu, B. Dolan-Gavitt, and S. Garg (2017-06) Badnets: identifying vulnerabilities in the machine learning model supply chain. Note: August 2017 External Links: Link Cited by: SS1.
* [6]D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song (2019-06) Natural adversarial examples. External Links: Link Cited by: SS1.
* [7]K. Huang, Y. Li, B. Wu, Z. Qin, and K. Ren (2022) Backdoor defense via decoupling the training process. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, External Links: Link Cited by: SS1.
* [8]A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry (2019) Adversarial examples are not bugs, they are features. Advances in neural information processing systems32. Cited by: SS1.
* [9]Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, and X. Ma (2021) Anti-backdoor learning: training clean models on poisoned data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16363-16372. Cited by: SS1.
* [10]X. Liu, M. Li, H. Wang, S. Hu, D. Ye, H. Jin, L. Wu, and C. Xiao (2023) Detecting backdoors during the inference stage based on corruption robustness consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16363-16372. Cited by: SS1.
* [11]W. Ma, D. Wang, R. Sun, M. Xue, S. Wen, and Y. Xiang (2022) The "Beatrix" resurrections: robust backdoor detection via gram matrices. Note: September 2022 External Links: Link Cited by: SS1.
* [12]X. Qi, T. Xie, Y. Li, S. Mahloujifar, and P. Mittal (2023) Revisiting the assumption of latent separability for backdoor defenses. In 11th International Conference on Learning Representations, ICLR, External Links: Link Cited by: SS1.
* [13]R. Roscher, B. Bohn, M. F. Duarte, and J. Garcke (2020) Explainable machine learning for scientific insights and discoveries. IEEE Access8, pp. 42200-42216. Cited by: SS1.
* [14]C. Villani (2021) Topics in optimal transportation. Vol. 58, American Mathematical Soc. Cited by: SS1.
* [15]B. Wu, H. Chen, M. Zhang, Z. Zhu, S. Wei, D. Yuan, and C. Shen (2022) BackdoorBench: a comprehensive benchmark of backdoor learning. In 36th Conference on Neural Information Processing Systems Datasets and Benchmarks Track, Cited by: SS1.

Appendix

### Ordering of detector Types

_Remark 3_ (Ordering of detector Types).: Types 0 to 3 are listed in order of decreasing difficulty as, e.g., more information is provided to a Type 3 detector than to a Type 2 detector. Thus, an \(\alpha\)-error detector \(g\) immediately provides an \(\alpha\)-error Type 1 detector \(g_{1}\), which in turn immediately provides an \(\alpha\)-error Type 2 detector \(g_{2}\), which yields an \(\alpha\)-error detector \(g_{3}\) of Type 3. Thus, we can define a total ordering on the different Types of detectors, using \(A\prec B\) to signify that \(A\) can be derived from \(B\): \(\mathbf{Q}_{0}\prec\mathbf{Q}_{1}\prec\mathbf{Q}_{2}\prec\mathbf{Q}_{3}\). The formal argument, showing this claim can be found in Lemma 2.

### Definition 2 is a special case of [3, Def. 1]

_Remark 4_.: Definition 2 is a special case of [3, Def. 1] in several ways4: _i)_ The hypothesis space is the complete function space \(\mathcal{H}=\{0,1\}^{\mathcal{X}}\), of functions \(f\colon\mathcal{X}\to\{0,1\}\). _ii)_ The loss function, as used in [3, Eq. (1)] is the Hamming distance, i.e., \(\ell(y,y^{\prime})=1\) if and only if \(y\neq y^{\prime}\). _iii)_ We are purely concerned with one-class novelty detection, i.e., \(K=1\) in [3, Sec. 2]. Therefore we do not take \(Y_{O}\) and \(Y_{I}\) into account, as \(Y_{I}\equiv 1\) and \(Y_{O}\equiv 2\). _iv)_ Note that \((P_{0},P_{b})\in\mathcal{P}\) play the role of \((D_{X_{O}},D_{X_{I}})\) and the complete domain space is then given by \(\mathscr{D}_{XY}=\{D_{XY}:D_{XY}=\frac{1}{2}P_{0}+\frac{1}{2}P_{b},(P_{0},P_{b} )\in\mathcal{P}\}\). Besides, strong PAC-learnability [3, Def. 2] implies weak learnability.

Footnote 4: The following symbols use the notation from [3, Sec. 2]: \(\mathcal{H}\), \(X_{O}\), \(X_{I}\), \(Y_{O}\), \(Y_{I}\), \(D_{X_{O}}\), \(D_{X_{I}}\), \(D_{XY}\), \(\mathscr{D}_{XY}\), \(\ell(\cdot,\cdot)\), \(K\).

### Proofs

**Lemma 2**.: _Let \(g_{l}\) be a detector as listed in Section 2 with input \(\mathbf{Q}_{l}\) for \(l\in\{0,1,2\}\), where we set \(g_{0}=g\) and \(\mathbf{Q}_{0}=\mathbf{Q}\). If \(g_{l}\) is \(\alpha\)-error in the sense of Definition 1, then for \(m\in\{1,2,3\}\) and \(m>l\) we can find a backdoor detector \(g_{m}\) with input \(\mathbf{Q}_{m}\) that is also \(\alpha\)-error._

Proof of Lemma 2.: It is sufficient to show the lemma for \(m=l+1\). The claim then follows by applying the result repeatedly.

In the case \(l=2\) (and \(m=3\)) we obtain \(g_{3}\) with \(R(g_{3},P_{0},P_{b})=R(g_{2},P_{0},P_{b})\) by \(g_{3}(\mathcal{D},P_{0},P_{b})=g_{2}(\mathcal{D},P_{0})\).

For \(l=1\), we can define the randomized detector \(g_{2}(\mathcal{D},x,P_{0})\) to first draw \(M\) i.i.d. samples \(\mathcal{D}^{\prime}_{M}\sim P_{0}^{M}\) and then yield \(g_{2}(\mathcal{D},P_{0})=g_{1}(\mathcal{D},\mathcal{D}^{\prime}_{M})\).

Finally, for \(l=0\) we obtain \(g_{1}\) with equal risk by defining \(g_{1}(\mathcal{D},\mathcal{D}^{\prime})=g(\mathcal{A}(\mathcal{D}),\mathcal{D} ^{\prime})\).

**Lemma 1**.: _Given a Type 3 backdoor detector \(g_{3}(\mathcal{D}_{N},P_{0},P_{b})\), for any pair \((P_{0},P_{b})\in\mathcal{P}(\mathcal{X})^{2}\)_

\[R(g_{3};P_{0},P_{b})\geq\frac{1}{2}-\frac{1}{2}\operatorname{TV}(P_{0}^{N},P_ {1}^{N})\geq\frac{1}{2}-\frac{\gamma N}{2}\operatorname{TV}(P_{0},P_{b}),\] (3)

_where the first equality in (3) can be achieved by the Neyman-Pearson detector. Thus, an \(\alpha\)-error detector of Type 3 can only exist if \(\alpha\geq\frac{1}{2}-\frac{\gamma N}{2}\operatorname{TV}(P_{0},P_{b})\) for all \((P_{0},P_{b})\in\mathcal{P}\)._

Proof of Lemma 1.: Fix \((P_{0},P_{b})\) and let \(\mathcal{Q}=\{\mathbf{x}\in\mathcal{X}^{N}:g_{3}(\mathbf{x},P_{0},P_{b})=1\}\) to obtain

\[1-R(g_{3};P_{0},P_{b}) =\frac{1}{2}\sum_{j\in\{0,1\}}\operatorname{Pr}\{g_{3}(\mathcal{ D}_{N}^{(j)},P_{0},P_{b})=j\}\] (9) \[=\frac{1}{2}\int\mathbbm{1}_{\mathcal{Q}}\,dP_{1}^{N}+\frac{1}{2 }\int\mathbbm{1}_{\mathcal{Q}^{c}}\,dP_{0}^{N}\] (10) \[=\frac{1}{2}+\frac{1}{2}\int\mathbbm{1}_{\mathcal{Q}}\,dP_{1}^{N }-\frac{1}{2}\int\mathbbm{1}_{\mathcal{Q}}\,dP_{0}^{N}\] (11) \[=\frac{1}{2}+\frac{1}{2}\int\mathbbm{1}_{\mathcal{Q}}\,dP_{1}^{N }-P_{0}^{N})\] (12)\[\leq\Pr\left\{g_{2}(\mathbf{Z})=1\Big{|}|\mathbf{V}|=N\right\}+1- \left(1-\frac{N}{M}\right)^{N}\] (20)\[=\Pr\left\{g_{2}(\mathcal{D}_{N}^{(0)})=1\right\}+1-\left(1-\frac{N} {M}\right)^{N}\] (21) \[=2-\Pr\{g_{2}(\mathcal{D}_{N}^{(0)})=0\}-\left(1-\frac{N}{M}\right) ^{N}\] (22) \[\leq 2-\Pr\{g_{2}(\mathcal{D}_{N}^{(0)})=0\}-\exp\frac{-N^{2}}{M-N},\] (23)

where we used the union bound as well as the inequality \(\log(1+x)\geq\frac{x}{1+x}\).

Using the second fact from above, we condition on \(\mathbf{Y}=\mathbf{y}\) and then have \(\mathbf{Z}\) i.i.d. according to \(P_{1}=(1-\gamma)P_{0}+\gamma P_{b}\) for a valid backdoor distribution \(P_{b}=\mathcal{Q}_{\mathbf{y}}\). We then write

\[\frac{1}{2}\Pr\{g_{2}(\mathcal{D}_{N}^{(0)})=0\}+\frac{1}{2}\Pr\{g _{2}(\mathbf{Z})=1\}\] (24) \[=\frac{1}{2}\Pr\{g_{2}(\mathcal{D}_{N}^{(0)})=0\}+\frac{1}{2}K^ {-M}\sum_{\mathbf{y}\in\mathcal{X}^{M}}\Pr\left\{g_{2}(\mathbf{Z})=1\big{|} \mathbf{Y}=\mathbf{y}\right\}\] (25) \[=K^{-M}\sum_{\mathbf{y}\in\mathcal{X}^{M}}\left(\frac{1}{2}\Pr \left\{g_{2}(\mathcal{D}_{N}^{(0)})=0\right\}+\frac{1}{2}\Pr\left\{g_{2}( \mathbf{Z})=1\big{|}\mathbf{Y}=\mathbf{y}\right\}\right)\] (26) \[=K^{-M}\sum_{\mathbf{y}\in\mathcal{X}^{M}}\left(\frac{1}{2}\Pr \left\{g_{2}(\mathcal{D}_{N}^{(0)})=0\right\}+\frac{1}{2}\Pr\left\{g_{2}( \mathcal{D}_{N}^{(1)})=1\big{|}\mathbf{Y}=\mathbf{y}\right\}\right)\] (27) \[\geq K^{-M}\sum_{\mathbf{y}\in\mathcal{X}^{M}}(1-\alpha)\] (28) \[=1-\alpha.\] (29)

In total we have

\[1-\alpha \overset{\eqref{eq:1}}{\leq}\frac{1}{2}\Pr\{g_{2}(\mathcal{D}_{N }^{(0)})=0\}+\frac{1}{2}\Pr\{g_{2}(\mathbf{Z})=1\}\] (30) \[\overset{\eqref{eq:1}}{\leq}\frac{1}{2}\bigg{(}\Pr\{g_{2}( \mathcal{D}_{N}^{(0)})=0\}+2-\Pr\{g_{2}(\mathcal{D}_{N}^{(0)})=0\}-\exp\frac{- N^{2}}{M-N}\bigg{)}\] (31) \[=1-\frac{1}{2}\exp\frac{-N^{2}}{M-N}\] (32)

and thus

\[\alpha\geq\frac{1}{2}\exp\frac{-N^{2}}{M-N}.\] (33)

This already resolves the case \(|\mathcal{X}|=\infty\) as we can then let \(K\to\infty\) and \(M=\lfloor\beta K\rfloor\to\infty\), showing that \(\alpha=\frac{1}{2}\) for \(|\mathcal{X}|=\infty\).

On the other hand, for \(|\mathcal{X}|<\infty\), we choose \(K=|\mathcal{X}|\), \(M=\lfloor\beta K\rfloor\) and obtain (4) by

\[\alpha \geq\frac{1}{2}\exp\frac{-N^{2}}{M-N}\] (34) \[-\log 2\alpha \leq\frac{N^{2}}{\lfloor\beta K\rfloor-N}\] (35) \[0 \leq N^{2}-N\log 2\alpha+\lfloor\beta K\rfloor\log 2\alpha\] (36) \[N \geq\frac{\log 2\alpha}{2}+\sqrt{\frac{(\log 2\alpha)^{2}}{4}- \lfloor\beta K\rfloor\log 2\alpha}\] (37) \[N \geq\frac{\log 2\alpha}{2}+\sqrt{\frac{(\log 2\alpha)^{2}}{4}+( \beta K-1)\log\frac{1}{2\alpha}}\] (38)

**Theorem 2**.: _Considering the backdoor detection setup of Definition 1 with \(\mathcal{P}=\{(P_{0},P_{b}):\operatorname{TV}(P_{0},P_{b})\geq 1-\beta\}\) and a finite alphabet \(|\mathcal{X}|<\infty\). There exists an \(\alpha\)-error Type 2 detector if_

\[\alpha>2|\mathcal{X}|\exp\left(-\frac{2N\gamma^{2}(1-\beta)^{2}}{|\mathcal{X} |^{2}}\right),\] (5)

In the proof of this theorem, the auxiliary Lemmas 3 and 4 are used, which are provided in Appendix A.4.

Proof of Theorem 2.: In the following we will show that the detector

\[g(\mathcal{D}_{N},P_{0})=\begin{cases}1&\operatorname{TV}(P_{0},S_{N})\geq \gamma\frac{1-\beta}{2}\\ 0&\text{otherwise}\end{cases}\] (39)

is \(\alpha\)-error if (5) is satisfied. Here, the distribution \(S_{N}\) is the so-called _type_ of \(\mathcal{D}_{N}\), i.e.,

\[S_{N}(x)=\frac{1}{N}\sum_{n=1}^{N}\mathbbm{1}_{x}(X_{i}),\] (40)

where for any \(x\in\mathcal{X}\), \(\mathbbm{1}_{x}(X_{n})\) is the indicator function that takes value \(1\) if \(X_{n}=x\) and \(0\) otherwise.

In Lemma 4 it is shown that the type \(S_{N}\) is close to the true distribution \(P\) with high probability. We can now analyze the error probability of the detector (39) for \(P=P_{1}\), i.e.,

\[\operatorname{Pr}\{g(\mathcal{D}_{N}^{(1)},P_{0})=0\} =\operatorname{Pr}\left\{\operatorname{TV}(S_{N}^{(1)},P_{0})\leq \gamma\frac{1-\beta}{2}\right\}\] (41) \[\leq\operatorname{Pr}\left\{\operatorname{TV}(S_{N}^{(1)},P_{0}) \leq\frac{\operatorname{TV}(P_{0},P_{1})}{2}\right\}\] (42) \[\leq\operatorname{Pr}\left\{\operatorname{TV}(S_{N}^{(1)},P_{1}) \geq\frac{\operatorname{TV}(P_{0},P_{1})}{2}\right\}\] (43) \[\leq\operatorname{Pr}\left\{\operatorname{TV}(S_{N}^{(1)},P_{1}) \geq\gamma\frac{1-\beta}{2}\right\}\] (44) \[\leq 2|\mathcal{X}|\exp\left(-\frac{2N\gamma^{2}(1-\beta)^{2}}{| \mathcal{X}|^{2}}\right),\] (45)

where we used Lemma 4 in (45) and the fact that \(\operatorname{TV}(P_{0},P_{1})=\gamma\operatorname{TV}(P_{0},P_{b})\geq(1- \beta)\gamma\) by Lemma 3 in (42) and (44). Similarly, we obtain that the error probability for \(j=0\) is upper bounded by the same expression

\[\operatorname{Pr}\{g(\mathcal{D}_{N}^{(0)},P_{0})=1\} =\operatorname{Pr}\left\{\operatorname{TV}(S_{N}^{(0)},P_{0})\geq \gamma\frac{1-\beta}{2}\right\}\] (46) \[\leq 2|\mathcal{X}|\exp\left(-\frac{2N\gamma^{2}(1-\beta)^{2}}{| \mathcal{X}|^{2}}\right),\] (47)

applying Lemma 4 in (47).

Thus, we have shown that \(g\), as defined in (39), is \(\alpha\)-error, provided that (5) holds. 

**Theorem 3**.: _Consider the setup of Definition 1, with fixed \(\gamma\in(0,1]\), \(N\in\mathbb{N}\) and \(\mathcal{P}\). Let \(\mathcal{P}^{\prime}\) be the set of \(N\)-fold products of \((P_{0},P_{1})\), i.e., \(\mathcal{P}^{\prime}=\{(P_{0}^{N},(\gamma P_{b}+(1-\gamma)P_{0})^{N}):(P_{0},P _{b})\in\mathcal{P}\}\). Then, OOD-detection is PAC-learnable on \(\mathcal{P}^{\prime}\) if and only if the following holds for any \(\epsilon>0\) and any Type 3 detector \(g_{3}(\mathcal{D}_{N},P_{0},P_{b})\): We can find \(M\in\mathbb{N}\) and a Type 1 detector \(g_{1}(\mathcal{D}_{N},\mathcal{D}_{M}^{\prime})\), which satisfies \(R(g_{1},P_{0},P_{b})\leq R(g_{3},P_{0},P_{b})+\epsilon\) for every \((P_{0},P_{b})\in\mathcal{P}\)._

In the proof of this theorem, the auxiliary Lemma 5 is used, which is provided in Appendix A.4Proof of Theorem 3.: Assume first that OOD-detection is PAC-learnable on \(\mathcal{P}^{\prime}\), fix \(\epsilon>0\) and let \(g_{3}\) be any Type 3 detector. By Lemma 5, we know that there is a Type 1 detector \(g_{1}^{M}\) with some \(M\) such that \(\epsilon(M)\leq\epsilon\), satisfying (85). Noting that \(\frac{1}{2}-\frac{1}{2}\operatorname{TV}(P_{0}^{N},P_{1}^{N})\leq R(g_{3},P_{ 0},P_{b})\) by Lemma 1 completes this part of the proof.

On the other hand, let \(g_{3}\) be the Type 3 Neyman-Pearson detector that satisfies \(R(g_{3},P_{0},P_{b})=\frac{1}{2}-\frac{1}{2}\operatorname{TV}(P_{0}^{N},P_{1} ^{N})\), which exists by Lemma 1. By our assumptions, for any \(k\in\mathbb{N}\) we can find a Type 1 detector \(\hat{g}_{1}^{k}\) with \(M=M(k)\) satisfying

\[\frac{1}{2}\operatorname{TV}(P_{0}^{N},P_{1}^{N})-\frac{1}{2}+R(\hat{g}_{1}^{ k},P_{0},P_{b})\leq\frac{1}{k}.\] (48)

We can find a monotonically increasing sequence \(k_{m}\) for \(m=1,2,\dots\) with \(\lim_{m\to\infty}k_{m}=\infty\), that satisfies \(M(k_{m})\leq m\). Using the sequence of Type 1 detectors5\(g_{1}^{m}(\mathcal{D}_{N},\mathcal{D}_{m}^{\prime})=\hat{g}_{1}^{k_{m}}( \mathcal{D}_{N},\left[\mathcal{D}_{m}^{\prime}\right]_{1}^{M(k_{m})})\) and \(\epsilon(m)=\frac{1}{k_{m}}\), we have for every \((P_{0},P_{b})\in\mathcal{P}\),

Footnote 5: We use the notation \([\mathbf{x}]_{k}^{l}=[(x_{1},x_{2},\dots,x_{N})]_{k}^{l}=(x_{k},x_{k+1},\dots, x_{l})\) for slicing.

\[\epsilon(m) \geq\frac{1}{2}\operatorname{TV}(P_{0}^{N},P_{1}^{N})-\frac{1}{2 }+R(\hat{g}_{1}^{k_{m}},P_{0},P_{b})\] (49) \[=\frac{1}{2}\operatorname{TV}(P_{0}^{N},P_{1}^{N})-\frac{1}{2}+R (g_{1}^{m},P_{0},P_{b}).\] (50)

This completes the proof as \(\lim_{m\to\infty}\epsilon(m)=\lim_{m\to\infty}\frac{1}{k_{m}}=0\) and thus, PAC learnability is guaranteed by Lemma 5. 

**Corollary 2**.: _Let \(g_{2}^{\prime}(\mathcal{D}_{N^{\prime}}^{(J)},P_{0},X^{(I)})\) be a Type 2 detector for an IBD problem with \(r=\min\{P_{JI}(0,0),P_{JI}(1,1)\}>0\) and \(\mathcal{P}=\{(P_{0},P_{0}):\operatorname{TV}(P_{0},P_{b})\geq 1-\beta\}\). Then, if \(g_{2}^{\prime}\) is \(\alpha\)-error, we have \(\alpha\geq r\) if \(|\mathcal{X}|=\infty\), and for \(|\mathcal{X}|<\infty\), we obtain_

\[N\geq\frac{\log\frac{\alpha}{r}}{2}+\sqrt{\frac{(\log\frac{\alpha}{r})^{2}}{4 }+(\beta|\mathcal{X}|-1)\log\frac{r}{\alpha}}.\] (8)

Proof of Corollary 2.: Assuming that this detector is \(\alpha\)-error implies

\[\alpha\geq R(g_{2}^{\prime},P_{0},P_{b}) \geq P_{JI}(0,0)\Pr\{g_{2}^{\prime}(\mathbf{Q}_{2}^{\prime})\neq 0|J= I=0\}\] \[\quad+P_{JI}(1,1)\Pr\{g_{2}^{\prime}(\mathbf{Q}_{2}^{\prime})\neq 1 |J=I=1\}\] (51) \[\geq r\big{(}\Pr\{g_{2}^{\prime}(\mathbf{Q}_{2}^{\prime})\neq 0|J= I=0\}+\Pr\{g_{2}^{\prime}(\mathbf{Q}_{2}^{\prime})\neq 1|J=I=1\}\big{)}.\] (52)

Now consider the MBD problem with \(\gamma=1\) and the training set size \(N^{\prime}=N+1\). We can define a Type 2 detector6\(g_{2}(\mathcal{D}_{N^{\prime}},P_{0})=g_{2}^{\prime}(\mathcal{D}_{N},P_{0},X _{N^{\prime}})\) with risk

Footnote 6: If \(\gamma>0\) for the IBD problem, randomly replace elements of \(\mathcal{D}_{N}\) by independently drawn realizations of \(P_{0}\).

\[R(g_{2},P_{0},P_{b}) =\frac{1}{2}\Pr\{g_{2}^{\prime}(\mathbf{Q}_{2}^{\prime})\neq 0|J=I=0\}+ \frac{1}{2}\Pr\{g_{2}^{\prime}(\mathbf{Q}_{2}^{\prime})\neq 1|J=I=1\}\] (53) \[\leq\frac{1}{2r}\alpha.\] (54)

From Theorem 1, we now know that \(\frac{1}{2r}\alpha\geq\frac{1}{2}\) if \(\mathcal{X}=\mathbb{N}\) and obtain (8) for \(|\mathcal{X}|<\infty\).

### Auxiliary Results

This appendix contains auxiliary results, which are utilized in the proofs provided in Appendix A.3.

**Lemma 3** (Properties of Total Variation).: _The total variation between two probability distributions \(P_{0},P_{1}\in\mathcal{P}(\mathcal{X})\), is given by_

\[\operatorname{TV}(P_{0},P_{1})=\|P_{0}-P_{1}\|_{\operatorname{TV}}\coloneqq \sup_{A}|P_{0}(A)-P_{1}(A)|,\] (55)_where the supremum is over all measurable sets \(A\subseteq\mathcal{X}\). We then have_

\[\|P_{0}-P_{1}\|_{\mathrm{TV}}=2\inf_{X_{0},X_{1}:P_{X_{0}}=P_{0},P_{X_{1}}=P_{1} }\Pr\{X_{0}\neq X_{1}\},\] (56)

_where the infimum is over all random variables \(X_{0},X_{1}\) on \(\mathcal{X}\), such that the marginal distributions satisfy \(P_{X_{0}}=P_{0}\), \(P_{X_{1}}=P_{1}\). For \(P_{0}^{\prime},P_{1}^{\prime}\in\mathcal{P}(\mathcal{Y})\), we have_

\[\|P_{0}-P_{1}\|_{\mathrm{TV}}\leq\|P_{0}\times P_{0}^{\prime}-P_{1}\times P_{1 }^{\prime}\|_{\mathrm{TV}}\leq\|P_{0}-P_{1}\|_{\mathrm{TV}}+\|P_{0}^{\prime}- P_{1}^{\prime}\|_{\mathrm{TV}}.\] (57)

_and thus \(\|P_{0}-P_{1}\|_{\mathrm{TV}}\leq\|P_{0}^{N}-P_{1}^{N}\|_{\mathrm{TV}}\leq N \|P_{0}-P_{1}\|_{\mathrm{TV}}\). Furthermore, for \(\gamma\in[0,1]\),_

\[\|P_{0}-(1-\gamma)P_{0}-\gamma P_{1}\|_{\mathrm{TV}}=\gamma\|P_{0}-P_{1}\|_{ \mathrm{TV}}\] (58)

Proof.: The characterization (56) can be found in [14].

To show the first inequality in (57), observe that

\[\|P_{0}\times P_{0}^{\prime}-P_{1}\times P_{1}^{\prime}\|_{\mathrm{ TV}} =\sup_{B}|[P_{0}\times P_{0}^{\prime}](B)-[P_{1}\times P_{1}^{\prime}](B)|\] (59) \[\geq\sup_{A}|[P_{0}\times P_{0}^{\prime}](A\times\mathcal{Y})-[P_ {1}\times P_{1}^{\prime}](A\times\mathcal{Y})|\] (60) \[=\|P_{0}-P_{1}\|_{\mathrm{TV}}.\] (61)

To show the second inequality in (57), we use (56) and for an arbitrary \(\varepsilon>0\), choose \((X_{0},X_{1})\perp(Y_{0},Y_{1})\) such that \(P_{X_{0}}=P_{0}\), \(P_{X_{1}}=P_{1}\), \(P_{Y_{0}}=P_{0}^{\prime}\), \(P_{Y_{1}}=P_{1}^{\prime}\), and

\[\|P_{0}-P_{1}\|_{\mathrm{TV}}+\varepsilon\geq 2\Pr\{X_{0}\neq X_ {1}\},\] (62) \[\|P_{0}^{\prime}-P_{1}^{\prime}\|_{\mathrm{TV}}+\varepsilon\geq 2 \Pr\{Y_{0}\neq Y_{1}\}.\] (63)

Clearly \(P_{X_{0},Y_{0}}=P_{0}\times P_{0}^{\prime}\) as well as \(P_{X_{1},Y_{1}}=P_{1}\times P_{1}^{\prime}\) and thus by (56),

\[\|P_{0}\times P_{0}^{\prime}-P_{1}\times P_{1}^{\prime}\|_{\mathrm{ TV}} \leq 2\Pr\{(X_{0},Y_{0})\neq(X_{1},Y_{1})\}\] (64) \[\leq 2\Pr\{X_{0}\neq X_{1}\}+2\Pr\{Y_{0}\neq Y_{1}\}\] (65) \[\leq\|P_{0}-P_{1}\|_{\mathrm{TV}}+\|P_{0}^{\prime}-P_{1}^{\prime }\|_{\mathrm{TV}}+2\varepsilon.\] (66)

As \(\varepsilon>0\) was arbitrary, this proves (57).

To show (58), we use (55) and have

\[\|P_{0}-(1-\gamma)P_{0}-\gamma P_{1}\|_{\mathrm{TV}} =\sup_{A}|P_{0}(A)-(1-\gamma)P_{0}(A)-\gamma P_{1}(A)|\] (67) \[=\sup_{A}|\gamma P_{0}(A)-\gamma P_{1}(A)|\] (68) \[=\gamma\|P_{0}-P_{1}\|_{\mathrm{TV}}.\] (69)

**Lemma 4**.: _Let \(S_{N}\) be the type of \(\mathbf{X}=(X_{1},X_{2},\ldots,X_{N})\), distributed according to \(P^{N}\). For any \(t\in[0,1]\), we then have the bound_

\[\Pr\left\{\mathrm{TV}(S_{N},P)\geq t\right\}\leq 2|\mathcal{X}|\exp\left(- \frac{8Nt^{2}}{|\mathcal{X}|^{2}}\right).\] (70)

Proof.: By using the Hoeffding's inequality we can bound the probability of the deviation of \(S_{N}\) from its expected value. In particular, we have that

\[\Pr\left\{|S_{N}(x)-P(x)|\geq t\right\} =\Pr\left\{|S_{N}(x)-\mathbb{E}[S_{N}(x)]|\geq t\right\}\] (71) \[\leq 2\exp\left(\frac{-2t^{2}}{\sum_{n=1}^{N}(\frac{1}{N}-0)^{2}}\right)\] (72) \[=2\exp\left(\frac{-2t^{2}}{\frac{1}{N}}\right)\] (73)\[=2\exp\left(-2Nt^{2}\right),\] (74)

where we note that \(\mathbb{E}[S_{N}(x)]=\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}[\mathbbm{1}_{x}[X_{n}]]=P(x)\).

The next and final step is to extend the bound to the whole alphabet \(\mathcal{X}\). In order to do so, we define the event \(\mathcal{A}_{x}=\{|S_{N}(x)-P(x)|\geq t\}\). We want to bound the probability of the event

\[\mathcal{A}=\bigcup_{x\in\mathcal{X}}\mathcal{A}_{x}=\left\{\exists x\in \mathcal{X}:\mathcal{A}_{x}\right\}.\] (75)

By applying the union bound we obtain

\[\Pr\mathcal{A} =\Pr\left\{\bigcup_{x\in\mathcal{X}}\mathcal{A}_{x}\right\}\] (76) \[\leq\sum_{x\in\mathcal{X}}\Pr\left\{\mathcal{A}_{x}\right\}\] (77) \[\leq\sum_{x\in\mathcal{X}}2\exp\left(-2Nt^{2}\right)\] (78) \[=2|\mathcal{X}|\exp\left(-2Nt^{2}\right).\] (79)

Let us consider the event \(\mathcal{A}=\{\exists x\in\mathcal{X}:|S_{N}(x)-P(x)|\geq t\}\): this is the error event, i.e., the divergence between the observed samples frequency and its expected value diverges more than a given value \(t>0\) for at least one \(x\in\mathcal{X}\). The complement of this event is the event that the divergence is less than \(t\) for all \(x\in\mathcal{X}\), i.e., the event that the observed frequency is close to the expected value for all \(x\in\mathcal{X}\). This can be written as

\[\mathcal{A}^{c}=\{\forall x\in\mathcal{X},\;|S_{N}(x)-P(x)|<t\}.\] (80)

Now, \(\mathcal{A}^{c}\) implies that

\[\sum_{x\in\mathcal{X}}|S_{N}(x)-P(x)| <t|\mathcal{X}|\] (81) \[\frac{1}{2}\sum_{x\in\mathcal{X}}|S_{N}(x)-P(x)| <\frac{1}{2}t|\mathcal{X}|\] (82) \[\operatorname{TV}(S_{N},P) <t^{\prime}\] (83)

where \(t^{\prime}=\frac{1}{2}t|\mathcal{X}|\). Thus, \(\Pr\mathcal{A}^{c}\leq\Pr\{\operatorname{TV}(S_{N},P)<t^{\prime}\}\) and therefore

\[\Pr\{\operatorname{TV}(S_{N},P)\geq t^{\prime}\}\leq\Pr\mathcal{A}\leq 2| \mathcal{X}|\exp\left(-2Nt^{2}\right),\] (84)

where we have used (79). By writing \(t\) in terms of \(t^{\prime}\) in (84), we obtain (70). 

**Lemma 5**.: _Given \(\mathcal{P}\) and \(N\in\mathbb{N}\) and letting \(\gamma\in(0,1]\), OOD-detection is PAC-learnable on \(\mathcal{P}^{\prime}=\{(P_{0}^{N},P_{1}^{N}):(P_{0},P_{b})\in\mathcal{P}\}\) with \(P_{1}=(1-\gamma)P_{0}+\gamma P_{b}\) if and only if the following holds: For the MBD problem, there exists a sequence of Type 1 backdoor detectors \(g_{1}^{M}(\mathcal{D}_{N},\mathcal{D}_{M}^{\prime})\) for \(M=1,2,\dots\) and a decreasing sequence \(\epsilon(m)\) with \(\lim_{m\to\infty}\epsilon(m)=0\) such that for any \(M\in\mathbb{N}\) and any pair \((P_{0},P_{b})\in\mathcal{P}\), we have_

\[\frac{1}{2}\operatorname{TV}(P_{0}^{N},P_{1}^{N})-\frac{1}{2}+R(g_{1}^{M},P_{ 0},P_{b})\leq\epsilon(M).\] (85)

Proof.: Assume that OOD-detection is PAC-learnable on \(\mathcal{P}^{\prime}\). By definition we have a function \(\mathcal{G}\colon\bigcup_{m=1}^{\infty}\mathcal{X}^{Nm}\to\{0,1\}^{\mathcal{X} ^{N}}\) and a monotonically decreasing sequence \(\epsilon^{\prime}(m)\) that tends to zero and satisfies for every \((P_{0},P_{b})\in\mathcal{P}\), \(m\in\mathbb{N}\), that

\[\mathbb{E}[\bar{R}(\mathcal{G}(\mathcal{D}_{mN}^{\prime}),P_{0}^{N},P_{1}^{N} )]-\inf_{f}\bar{R}(f,P_{0}^{N},P_{1}^{N})\leq\epsilon^{\prime}(m),\] (86)

where the infimum is over all functions \(f\colon\mathcal{X}^{N}\to\{0,1\}\).

For any \(M\in\mathbb{N}\), we define7\(g_{1}^{M}(\mathcal{D}_{N},\mathcal{D}^{\prime}_{M}):=\mathcal{G}([ \mathcal{D}^{\prime}_{M}]_{1}^{mN})(\mathcal{D}_{N})\) as well as \(\epsilon(M)=\epsilon^{\prime}(m)\), where \(m\) is the largest integer such that \(mN\leq M\). Notice that \(R(g_{1}^{M},P_{0},P_{b})=\mathbb{E}[\bar{R}(\mathcal{G}(\mathcal{D}^{\prime}_{ mN}),P_{0}^{N},P_{1}^{N})]\) and that \(\inf_{f}\bar{R}(f,P_{0}^{N},P_{1}^{N})=\frac{1}{2}-\frac{1}{2}\operatorname{TV }(P_{0}^{N},P_{1}^{N})\) by Lemma 1. We thus obtain from (86), that for any \(M\in\mathbb{N}\),

Footnote 7: We use the notation \([\mathbf{x}]_{k}^{l}=[(x_{1},x_{2},\ldots,x_{N})]_{k}^{l}=(x_{k},x_{k+1}, \ldots,x_{l})\) for slicing.

\[\epsilon(M)=\epsilon^{\prime}(m) \geq\mathbb{E}[\bar{R}(\mathcal{G}(\mathcal{D}^{\prime}_{mN}),P_ {0}^{N},P_{1}^{N})]-\frac{1}{2}+\frac{1}{2}\operatorname{TV}(P_{0}^{N},P_{1}^ {N})\] (87) \[=R(g_{1}^{M},P_{0},P_{b})-\frac{1}{2}+\frac{1}{2}\operatorname{TV }(P_{0}^{N},P_{1}^{N}).\] (88)

Noting that \(\epsilon(M)\) approaches zero completes this part of the proof.

On the other hand, assume that \(g_{1}^{M}(\mathcal{D}_{N},\mathcal{D}^{\prime}_{M})\) and \(\epsilon(M)\) satisfy the requirement (85). For any \(m\in\mathbb{N}\), we can then define \(\mathcal{G}(\mathcal{D}^{\prime}_{mN})(\mathcal{D}_{N}):=g_{mN}(\mathcal{D}_{ N},\mathcal{D}^{\prime}_{mN})\) and \(\epsilon^{\prime}(m)=\epsilon(mN)\). We can now rewrite (85) using \(\mathbb{E}[\bar{R}(\mathcal{G}(\mathcal{D}^{\prime}_{mN}),P_{0}^{N},P_{1}^{N })]=R(g_{1}^{M},P_{0},P_{b})\) and Lemma 1 to obtain

\[\epsilon^{\prime}(m)=\epsilon(mN) \geq\frac{1}{2}\operatorname{TV}(P_{0}^{N},P_{1}^{N})-\frac{1}{ 2}+R(g_{1}^{M},P_{0},P_{b})\] (89) \[=\mathbb{E}[\bar{R}(\mathcal{G}(\mathcal{D}^{\prime}_{mN}),P_{0}^ {N},P_{b}^{N})]-\inf_{f}\bar{R}(f,P_{0}^{N},P_{1}^{N}).\] (90)

Thus, we have shown that the algorithm \(\mathcal{G}\) and the sequence \(\epsilon^{\prime}\) satisfy Definition 2 and OOD-detection is PAC-learnable on \(\mathcal{P}^{\prime}\). \(\square\)