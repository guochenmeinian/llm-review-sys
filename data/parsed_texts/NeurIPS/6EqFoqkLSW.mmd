# Learning Discrete Latent Variable Structures with Tensor Rank Conditions

Zhengming Chen\({}^{1,2}\), Ruichu Cai\({}^{1,}\)1, Feng Xie\({}^{3}\), Jie Qiao\({}^{1}\),

**Anpeng Wu\({}^{2,4}\), Zijian Li\({}^{2}\), Zhifeng Hao\({}^{1,5}\), Kun Zhang\({}^{2,6,}\)**

1. School of Computer Science, Guangdong University of Technology

2. Machine Learning Department, Mohamed bin Zayed University of Artificial Intelligence

3. Department of Applied Statistics, Beijing Technology and Business University

4. Department of Computer Science and Technology, Zhejiang University

5. College of Science, Shantou University, Shantou, Guangdong, China

6. Department of Philosophy, Carnegie Mellon University

Corresponding author

###### Abstract

Unobserved discrete data are ubiquitous in many scientific disciplines, and how to learn the causal structure of these latent variables is crucial for uncovering data patterns. Most studies focus on the linear latent variable model or impose strict constraints on latent structures, which fail to address cases in discrete data involving non-linear relationships or complex latent structures. To achieve this, we explore a tensor rank condition on contingency tables for an observed variable set \(\mathbf{X}_{p}\), showing that the rank is determined by the minimum support of a specific conditional set (not necessary in \(\mathbf{X}_{p}\)) that d-separates all variables in \(\mathbf{X}_{p}\). By this, one can locate the latent variable through probing the rank on different observed variables set, and further identify the latent causal structure under some structure assumptions. We present the corresponding identification algorithm and conduct simulated experiments to verify the effectiveness of our method. Our results elegantly expand the application scope of causal discovery with latent variables.

## 1 Introduction

Social scientists, psychologists, and researchers from various disciplines are often interested in understanding causal relationships between the latent variables that cannot be measured directly, such as depression, coping, and stress (Silva et al., 2006). A common approach to grasp these latent concepts is to construct a measurement model. For instance, experts design a set of measurable items or survey questions that serve as indicators of the latent variable and then use them to infer causal relationships among latent variables (Bollen, 2002; Bartholomew et al., 2011; Cui et al., 2018).

Numerous approaches exist for addressing structure learning among latent variables. In particular, if the data generation process is assumed to be a linear relationship, known as _linear latent variable models_, several approaches have been developed. These include the second-order statistic-based approaches (Silva et al., 2006; Kummerfeld and Ramsey, 2016; Chen et al., 2024; Sullivant et al., 2010), high-order moments-based ones (Xie et al., 2020; Chen et al., 2022; Cai et al., 2019; Adams et al., 2021), matrix decomposition-based methods (Anandkumar et al., 2013, 2014, 2015), and copula model-based approaches (Cui et al., 2018). Moreover, the hierarchical latent variable structure has been well-studied within the linear setting (Huang et al., 2022; Xie et al., 2022; Chen et al., 2023; Jin et al., 2023). However, the linear assumption is rather restrictive and the discrete data in the real world could be more frequently encountered (e.g., responses from psychological and educationalassessments or social science surveys (Eysenck et al., 2021; Skinner, 2019)), which does not satisfy the linear assumption.

When the data generation process is discrete, however, due to the challenging nonlinear transition relationship in discrete data, few identifiability results exist and are mostly only applicable in strict cases. In particular, under some prespecified structure, the identifiability of parameters is established, such as in the hidden Markov model(HMM) (Anandkumar et al., 2012) model, topic models (Anandkumar et al., 2014), and multiview mixtures model (Anandkumar et al., 2015). By further specifying the latent variable structure as a tree, Wang et al. (2017); Song et al. (2013) show that the structural model is identifiable. Recently, Gu (2022); Gu and Dunson (2023) further considered the identifiability of pyramid structure under the condition that each latent variable has at least three observed children. However, challenges persist in extending identifiability to more general structures among discrete latent variables. Existing approaches, unfortunately, cannot identify the causal structure of latent variables as shown in Fig. 2(a).

Recently, some studies have shown that causal structures involving discrete latent confounders can be effectively identified, building on the identifiability results of mixture models, as discussed in (Kant et al., 2024; Gordon et al., 2023; Mazaheri et al., 2023; Anandkumar et al., 2012). Most of these works focus on the causal structure among observed variables, usually assuming a single latent confounder. For the identification of latent structure, (Kivva et al., 2021) shows that causal structures of discrete latent variables can be identified by recovering the latent distribution from a mixture oracle. However, while a general discrete latent variable model can be identified theoretically, estimating the parameters of the mixture model is challenging. Approximating methods are often applied, but these may be unrealistic and impractical in real-world situations.

In this paper, we aim to establish a general identification criterion for discrete latent structures in cases where latent structures exhibit flexible dependencies, while also developing a simple but robust structure learning algorithm. To achieve this, we explore a tensor rank condition on the contingency tables for an observed variable set \(\mathbf{X}_{p}\), to probe the latent causal structure from observed data. Interestingly, as shown in Fig. 1, we found that the rank of the contingency tables of the joint distribution \(\mathbb{P}(X_{1},X_{2})\) is deeply connected to the support of a variable \(L\) (not necessary among \(X_{1},X_{2}\)) that d-separate \(X_{1}\) and \(X_{2}\). By this observation, we first develop a general tensor rank condition for the discrete causal model and show that such a rank is determined by the minimal support of a specific conditional set (not necessary in \(\mathbf{X}_{p}\)) that d-separates all variables in \(\mathbf{X}_{p}\). Such findings intrigue the possibility to identify the discrete latent variables structure. We further propose a discrete latent structure model that accommodates more general latent structures and shows that the discrete latent variable structure can be identified locally and iteratively through tensor rank conditions. Subsequently, we present an identification algorithm to complete the identifiability of discrete latent structure models, including the measurement model and the structure model. We theoretically show that under proper causal assumptions, such as faithfulness and the Markov assumption, the

Figure 1: Illustrating the graphical criteria of the tensor rank condition, the rank of the joint distribution is determined by the support of a specific conditional set that d-separates all observed variables, i.e., \(\mathrm{Rank}(\mathbb{P}(X_{1},X_{2}))=|\mathrm{supp}(L)|=2\). See Example 3.4 for details.

measurement model is fully identifiable and the structure model can be identified up to a Markov equivalence class.

The contributions of this work are three-fold. (1) We first establish a connection between the tensor rank condition and the graphical patterns in a general discrete causal model, including specific d-separation relations. (2) We then exploit the tensor rank condition to learn the discrete latent variable model, allowing flexible relations between latent variables. (3) We present a structure learning algorithm using tensor rank conditions and demonstrate the effectiveness of the proposed algorithm through simulation studies.

## 2 Discrete Latent Structure Model

For an integer \(m\), denote \([m]=\{1,2,\cdots,m\}\). Consider a discrete statistic model with \(k\) latent variable set \(\mathbf{L}=\{L_{1},\cdots,L_{k}\},L_{i}\in[r_{i}]\) and \(m\) discrete observed variable set \(\mathbf{X}=\{X_{1},\cdots,X_{m}\}\) with \(X_{i}\in[d_{i}]\) (\(r_{i},d_{i}\geq 2\)), in which any marginal probabilities are non-zero. We say a discrete statistic model is a _discrete causal model_ if and only if \(\mathbf{V}=\mathbf{L}\cup\mathbf{X}\) can be represented by a directed acyclic graph (DAG), denoted by \(\mathcal{G}\). We use \(\operatorname{supp}(V_{i})=\{v\in\mathbb{Z}^{+}:\mathbb{P}(V_{i}=v)>0\}\) to denote the set of possible values of the random variable \(V_{i}\). Our work is in the framework of causal graphical models. Concepts used here without explicit definition, such as d-separation, which can refer to standard literature (Spirtes et al., 2000).

In this paper, we focus on learning causal structure among latent variables in one class of discrete causal models. The model is defined as follows.

**Definition 2.1** (Discrete Latent Structure Model with Three-Pure Children).: _A discrete causal model is the Discrete **Latent Structure **M**odel with Three-Pure Children (Discrete 3PLSM) if it further satisfies the following three assumptions:_

1. _[_Purity Assumption_]_ _there is no direct edges between the observed variables;_
2. _[_Three-Pure Child Variable Assumption_]_ _each latent variable has at least three pure variables as children;_
3. _[Sufficient Observation Assumption_]_ _The dimension of observed variables support is larger than the dimension of any latent variables support._

These structural constraints inherent in the discrete 3PLSM are also widely used in linear latent variable models, e.g., Silva et al. (2006); Kummerfeld and Ramsey (2016); Cai et al. (2019); Xie et al. (2020). In the binary latent variable case, recently, a similar definition is also employed in Gu and Dunson (2023); Gu (2022). The key difference is that there are no constraints on the latent structure in our work. An example of a discrete 3PLSM model is shown in Fig. 2(a), where \(L_{1},\cdots,L_{4}\) represent discrete latent variables, and \(X_{1},\cdots,X_{12}\) are discrete observed (measured) variables.

Generally speaking, the discrete 3PLSM model can be divided into two sub-models (Spirtes et al., 2000), i.e., the measurement model and the structure model, e.g., red edge and blue edge in Fig. 2 (a). By this, one can first identify the measurement model to determine the latent variables and then use the measured variable to infer the causal structure of latent variables. As shown in Fig. 2 (b), we will separately discuss the identification of the two sub-models and show that the measurement model is

Figure 2: An example of discrete latent structure model involving 4 latent variables and 12 observed variables (sub-fig (a)). Here, the red edges form a measurement model, while the blue edges form a structural model. The theoretical result of this paper is shown in sub-fig (c).

fully identifiable and the structure model is identified up to a Markov equivalence class. The symbols used in our work is summarised in Table 1.

To ensure the identification of causal structure and the asymptotic correctness of identification algorithms, some common causal assumptions are required.

**Assumption 2.2** (Causal Markov Assumption).: _Let \(\mathcal{G}\) be a causal graph with vertex set \(\mathbf{V}\) and \(\mathbb{P}_{\mathbf{V}}\) be probability distribution over the vertices in \(\mathbf{V}\) generated by \(\mathcal{G}\). We say \(\mathcal{G}\) and \(\mathbb{P}_{\mathbf{V}}\) satisfy the Causal Markov Assumption if and only if for every \(V_{i}\in\mathbf{V}\), \(\mathbb{P}(V_{i},\mathbf{V}\setminus\operatorname{Des}_{V_{i}}\left|\operatorname {Pa}_{V_{i}}=i\right)=\mathbb{P}(W\left|\operatorname{Pa}_{V_{i}}=i)\mathbb{P} (\mathbf{V}\setminus\operatorname{Des}_{V_{i}}\left|\operatorname{Pa}_{V_{i}}=i\right)\)._

**Assumption 2.3** (Faithfulness Assumption).: _Let \(\mathcal{G}\) be a causal graph with vertex set \(\mathbf{V}\) and \(\mathbb{P}_{\mathbf{V}}\) be probability distribution over the vertices in \(\mathbf{V}\) generated by \(\mathcal{G}\). We say < \(\mathcal{G},\mathbb{P}_{\mathbf{V}}>\) satisfies the Faithfulness Assumption if and only if (i). every conditional independence relation true in \(\mathbb{P}_{\mathbf{V}}\) is entailed by the Causal Markov Assumption applied to \(\mathcal{G}\), and (ii). for any joint distribution \(\mathbb{P}(\mathbf{L}_{p})\), there does not exist \(\mathbb{P}(\mathbf{L}_{q})\) with \(\left|\operatorname{supp}(\mathbf{L}_{q})\right|\) < \(\left|\operatorname{supp}(\mathbf{L}_{p})\right|\) such that \(\mathbb{P}(\mathbf{L}_{p})=\mathbb{P}(\mathbf{L}_{q})\)._

**Assumption 2.4** (Full Rank Assumption).: _For any conditional probability \(\mathbb{P}(X\left|\operatorname{Pa}_{X}\right.)\), the corresponding contingency table is full rank, i.e., each column of \(\mathbb{P}(X\left|\operatorname{Pa}_{X}\right.)\) is linearly independent with the other column vectors in the parameter space._

In general, Assumptions 2.2\(\sim\)2.3 are widely used in the constraint-based causal discovery methods, e.g., PC algorithm and FCI algorithm (Spirtes et al., 2000; Spirtes and Glymour, 1991). One can see that we further constraint the parameter space of joint distribution cannot be reduced to a low-dimension space, for maintaining the diversity of parameter space. This is also the reason for Assumption 2.4, which aligns with the non-degeneracy condition used in (Kivva et al., 2021).

**Our goal:** The goal of our work is to develop a robust approach for identifying discrete latent structure models, including both the measurement and structural models.

## 3 Tensor Rank Condition with Graphical Criteria

To address the identification problem in the discrete 3PLSM, this section introduces the building block-the tensor rank condition of the discrete causal model. Then, we establish the connection between tensor rank and d-separation relations under a general discrete causal model.

Before formalizing the tensor rank condition, we first give the explicit definition of tensor rank.

**Definition 3.1** (Rank-one Tensor).: _An \(n\)-way tensor \(\mathcal{T}\in\mathbb{R}^{I_{1}\times\cdots\times I_{n}}\) is a rank-one tensor if it can be written as the outer product of \(n\) vectors, i.e.,_

\[\mathcal{T}=\mathbf{u}_{1}\otimes\mathbf{u}_{2}\otimes\cdots\otimes\mathbf{u} _{n},\]

_where \(\mathbf{u}_{i}\) are vectors that each represent a dimension of the tensor, \(\otimes\) represents the outer product._

**Definition 3.2** (Tensor Rank Kolda and Bader (2009)).: _For an \(n\)-way tensor \(\mathcal{T}\in\mathbb{R}^{I_{1}\times\cdots\times I_{n}}\), the rank of a tensor \(\mathcal{T}\) is defined as the **smallest** number of rank-one tensors that sum to exactly represent \(\mathcal{T}\). Formally, the rank of tensor \(\mathcal{T}\), denoted \(\operatorname{rank}(\mathcal{T})\), is the smallest integer \(r\) such that:_

\[\mathcal{T}=\sum_{i=1}^{r}\mathbf{u}_{1}^{(i)}\otimes\mathbf{u}_{2}^{(i)} \otimes\cdots\otimes\mathbf{u}_{n}^{(i)},\]

_where each \(\mathbf{u}_{k}^{(i)}\) is a vector in the corresponding vector space associated with the \(k\)-th mode of \(\mathcal{T}\)._

\begin{table}
\begin{tabular}{|c|c|} \hline \(\mathbf{V}\): The set of variables \(\mathbf{V}=\mathbf{X}\cup\mathbf{L}\) & \(\mathbf{X}\): The set of observed variables \\ \hline \(\mathbf{L}\): The set of latent variables & \(V_{i}\parallel V_{j}\|\mathbf{V}_{p}:\text{Conditional independence}\) \\ \hline \(|\mathbf{X}_{p}|:\text{Dimension of }\mathbf{X}_{p}\) & \(\mathcal{T}(\mathbf{x}_{p}):\text{the tensor form of }\mathbb{P}(\mathbf{X}_{p})\) \\ \hline \(\mathbb{P}(\mathbf{X}_{p})\): the joint distribution of \(\mathbf{X}_{p}\) & \(\operatorname{Rank}(\mathcal{T}(\mathbf{x}_{p})):\text{The rank of tensor }\mathcal{T}(\mathbf{x}_{p})\) \\ \hline \(\operatorname{Pa}_{X}\): The parent set of \(X\) & \(\operatorname{Des}_{X}\): The descendant set of \(X\) \\ \hline \(\operatorname{Diag}(\text{M})\): The diagonal matrix of \(\mathrm{M}\) & \(\mathbf{u}_{i}\otimes\mathbf{u}_{j}:\text{The outer product of two vectors}\) \\ \hline \end{tabular}
\end{table}
Table 1: Mathematical notations used in this paper.

In other words, the tensor rank denotes the minimal number of rank-one decompositions. In the discrete causal model, the joint distribution can be represented as a tensor, e.g., the joint distribution of two random variables is a two-way contingency tensor. Interestingly, by carefully analyzing the (non-negative) rank-one decomposition of the joint distribution, we find that the tensor rank essentially reveals structural information within the causal graph. The result is presented below.

**Theorem 3.3** (Graphical implication of tensor rank condition).: _In the discrete causal model, suppose Assumptions 2.2 \(\sim\) Assumption 2.4 hold. Consider an observed variable set \(\mathbf{X}_{p}=\{X_{1},\cdots,X_{n}\}\) (\(\mathbf{X}_{p}\subseteq\mathbf{X}\) and \(n\geq 2\)) and the corresponding \(n\)-way probability tensor \(\mathcal{T}_{(\mathbf{X}_{p})}\) that is the tabular representation of the joint probability mass function \(\mathbb{P}(X_{1},\cdots,X_{n})\). Then, \(\mathrm{Rank}(\mathcal{T}_{(\mathbf{X}_{p})})=r\) (\(r>1\)) if and only if (i) there exist a conditional set \(\mathbf{S}\subset\mathbf{V}\) with \(|\mathrm{supp}(\mathbf{S})|=r\) that d-separates any pair of variables in \(\{X_{1},\cdots,X_{n}\}\), and (ii) does not exist conditional set \(\mathbf{\bar{S}}\) that satisfies \(|\mathrm{supp}(\mathbf{\bar{S}})|<r\)._

We further provide an example to illustrate Theorem 3.3.

**Example 3.4** (Illustrating the graphical criteria).: _Consider a single latent variable structure as shown in Fig. 1 (a) where \(L\) is a latent variable with \(\mathrm{supp}(L)=\{0,1\}\) and \(X_{1},X_{2}\) are observed variables with \(\mathrm{supp}(X_{i})=\{0,1,2\},i\in\{1,2\}\). For convenience, we denote \(p_{i}=\mathbb{P}(L=i)\), \(\hat{p}_{\hat{\mathrm{i}}\mid j}=\mathbb{P}(X_{1}=i|L=j)\), and \(\tilde{p}_{\hat{\mathrm{i}}\mid j}=\mathbb{P}(X_{2}=i|L_{1}=j)\). The joint distribution \(\mathbb{P}(X_{1},X_{2})\) can be expressed as the product of conditional probabilities, as shown in Fig. 1(b). By applying the tensor decomposition, we observe that \(\mathbb{P}(X_{1},X_{2})\) can be decomposed as the sum of two rank-one tensors: \(\mathbb{P}(X_{1},X_{2}|L=0)\) and \(\mathbb{P}(X_{1},X_{2}|L=1)\). Thus, the rank of the tensor \(\mathbb{P}(X_{1},X_{2})\) is two, corresponding to the cardinality of the latent variable's support. The reason \(\mathbb{P}(X_{1},X_{2}|L=i)\) is a rank-one tensor is that \(L\) d-separates \(X_{1}\) and \(X_{2}\), i.e., \(\mathbb{P}(X_{1},X_{2}|L=i)=\mathbb{P}(X_{1}|L=i)\otimes\mathbb{P}(X_{2}|L=i)\). This illustrates the connection between tensor rank and d-separation relations._

Intuitively, the graphical criteria theorem suggests that, in the discrete causal model, the tensor rank condition implies the minimal conditional probability decomposition within the probability parameter space, which hopefully induces the structural identifiability of the discrete 3PLSM model.

## 4 Structure Learning of Discrete Latent Structure Model

In this section, we address the identification problem of the discrete 3PLSM model using a carefully designed algorithm that leverages the tensor rank condition. Specifically, we first show that latent variables can be identified by finding causal clusters among observed variables (Sec. 4.1). Then, we use these causal clusters to conduct conditional independence tests among latent variables based on the tensor rank condition, identifying the structure model (Sec. 4.2). Finally, we discuss the practical implementation of testing tensor rank (Sec. 4.3). For simplicity, we focus on the case where all latent variables have the same number of categories. The result can be directly extended to cases with different numbers of categories (see details in Appendix E).

### Identification of the measurement model

To answer the identification of the measurement model, one common strategy is to find the causal cluster that shares the common latent parent, which has been well-studied within the linear model, such as Silva et al. (2006); Cai et al. (2019); Xie et al. (2020). We follow this strategy and show that, in the discrete 3PLSM, the causal cluster can be found by testing the tensor rank conditions iteratively. The definition of a causal cluster is as follows.

**Definition 4.1** (Causal cluster).: _In the discrete 3PLSM, the variable set \(\{X_{1},\cdots,X_{n}\}\) is a causal cluster, termed \(C_{i}\), if and only if all variables in \(\{X_{1},\cdots,X_{n}\}\) share the common latent parent._

It is not hard to see that, the measurement model can be identified if all causal cluster is found. In order to find these causal clusters by making use of the tensor rank condition, the key issue is to determine the support of latent variables in advance. This issue can be addressed by identifying the rank of the two-way tensor formed by the joint distribution of two observed variables.

**Proposition 4.2** (Identification of support of latent variables).: _In the discrete 3PLSM model suppose Assumptions 2.2 \(\sim\) Assumption 2.4 hold. The support of the latent variable corresponds to the rank of the two-way probability contingency table for any pair of observed variables \(X_{i}\) and \(X_{j}\), i.e., \(|\mathrm{supp}(L)|=\mathrm{Rank}(\mathcal{T}_{(X_{i},X_{j})})\), \(\forall X_{i},X_{j}\in\mathbf{X}\)._This result holds because any pair of variables in the discrete 3DLSM model is d-separated by any one of their latent parent variables, and all latent variables have the same support. Next, we formalize the property of clusters and give the criterion for finding clusters.

**Proposition 4.3** (Identification of causal cluster).: _In the discrete 3DLSM mode, suppose Assumption 2.2 \(\sim\) Assumption 2.4 hold. Let \(r=\left|\operatorname{supp}(L_{i})\right|\) denote the cardinality of the latent support. Given three disjoint observed variables \(X_{i},X_{j},X_{k}\in\mathbf{X}\),_

* \(\mathcal{R}ule1\)_: if the rank of tensor_ \(\mathcal{T}_{(X_{i},X_{j},X_{k})}\) _is not equal to_ \(r\)_, i.e.,_ \(\operatorname{Rank}(\mathcal{T}_{(X_{i},X_{j},X_{k})})\neq r\)_, then_ \(X_{i}\)_,_ \(X_{j}\) _and_ \(X_{k}\) _belong to the different latent parents._
* \(\mathcal{R}ule2\)_: for any_ \(X_{s}\)_,_ \(X_{s}\in\mathbf{X}\setminus\{X_{i},X_{j},X_{k}\}\)_, if the rank of tensor_ \(\mathcal{T}_{(X_{i},X_{j},X_{k},X_{s})}\) _is_ \(r\)_, i.e.,_ \(\operatorname{Rank}(\mathcal{T}_{(X_{i},X_{j},X_{k},X_{s})})=r\)_, then_ \(\{X_{i},X_{j},X_{k}\}\) _share the same latent parent._

**Example 4.4** (Finding causal clusters).: _Let's take Fig. 2(a) as an example. One can find that for \(\{X_{1},X_{2},X_{3},X_{k}\}\), where \(X_{k}\in\mathbf{X}\setminus\{X_{1},X_{2},X_{3}\}\), the rank of tensor \(\mathcal{T}_{(X_{1},X_{2},X_{3},X_{k})}\) is \(r\). Thus, \(\{X_{1},X_{2},X_{3}\}\) is identified as a causal cluster._

Next, we consider the practical issues involved in determining the number of latent variables by causal clusters. That is, there are some causal clusters that should be merged because they share one latent parent. We find that the overlapping clusters can be directly merged into one cluster. This is because the overlapping clusters have the same latent variable as the parent under the discrete 3DLSM model. The validity of the merge step is guaranteed by Proposition 4.5.

**Proposition 4.5** (Merging Rule).: _In the discrete 3DLSM model, for two causal clusters \(C_{1}\) and \(C_{2}\), if \(C_{1}\cap C_{2}\neq\varnothing\), then \(C_{1}\) and \(C_{2}\) share the same latent parent._

Based on the above results, one can iteratively identify causal clusters and apply the merger rule to detect all latent variables. The identification procedure is summarized in Algorithm 1.

```
0: Data from a set of measured variables \(\mathbf{X}_{\mathcal{G}}\), and the dimension of latent support \(r\)
0: Causal cluster \(\mathcal{C}\)
1: Initialize the causal cluster set \(\mathcal{C}:=\varnothing\), and \(\mathcal{G}^{\prime}=\varnothing\);
2://Identify Causal Skeleton
3:Begin the recursive procedure
4:repeat
5:for each \(X_{i},X_{j}\) and \(X_{k}\in\mathbf{X}\)do
6:if\(\operatorname{Rank}(\mathcal{T}_{\{X_{i},X_{j},X_{k}\}})\neq r\)then
7: Continue;//Rule1 of Prop. 4.3
8:endif
9:if\(\operatorname{Rank}(\mathcal{T}_{\{X_{i},X_{j},X_{k},X_{s}\}}=r\), for all \(X_{s}\in\mathbf{X}\setminus\{X_{i},X_{j},X_{k}\}\)then
10:\(\mathbf{C}\) = \(\mathbf{C}\cup\{\{X_{i},X_{j},X_{k}\}\}\);
11:endif
12:endfor
13:until no causal cluster is found.
14://Merging cluster and introducing latent variables
15:Merge all the overlapping sets in \(\mathbf{C}\) by Prop. 4.5.
16:for each \(C_{i}\in\mathbf{C}\)do
17: Introduce a latent variable\(L_{i}\) for \(C_{i}\);
18:\(\mathcal{G}\) = \(\mathcal{G}\cup\{L_{i}\to X_{j}|X_{j}\in C_{i}\}\).
19:endfor
20:return Graph \(\mathcal{G}\) and causal cluster \(\mathcal{C}\).
```

**Algorithm 1** Finding the causal cluster

**Theorem 4.6** (Identification of the measurement model).: _In the discrete 3DLSM, suppose Assumption 2.2 \(\sim\) Assumption 2.4 hold. The measurement model is fully identifiable by Algorithm 1._

### Identification of the structure model

Once the measurement model is identified, the observed children can serve as proxies for the latent variables, enabling the identification of the causal structure among them. Here, we employ constraint-based framework to learn the causal structure of latent variables.

Constraint-based structure learning algorithms find the Markov equivalence class over a set of variables by making decisions about independence and conditional independence among them. Given a pure and accurate measurement model with at least two measures per latent variable, we can test for independence and conditional independence (CI) among the latent variables. Specifically, to test statistical independence between discrete variables, one can examine whether the rank of their joint distribution contingency table is one (Sullivan, 2018). For testing conditional independence (CI) relations among latent variables, further leveraging the algebraic properties of the tensor rank condition is required (see Theorem 4.7).

**Theorem 4.7** (d-separation among latent variable).: _In the discrete 3PLSM, suppose Assumption 2.2 \(\sim\)Assumption 2.4 hold. Let \(r\) denote the cardinality of the latent support. Then, \(L_{i}\ \mathbbm{1}\ L_{j}|\mathbf{L}_{p}\) if and only if \(\mathrm{Rank}(\mathcal{T}_{(X_{i},X_{j},\mathbf{X}_{p1},\mathbf{X}_{p2})})=r^{ \left\lvert\mathbf{L}_{p}\right\rvert}\), where \(X_{i}\) and \(X_{j}\) are the pure children of \(L_{i}\) and \(L_{j}\), \(\mathbf{X}_{p1}\) and \(\mathbf{X}_{p2}\) are two disjoint child sets of \(\mathbf{L}_{p}\) that satisfy \(\forall L_{i}\in\mathbf{L}_{p},\mathrm{Ch}_{L_{i}}\cap\mathbf{X}_{p1}\neq \emptyset,\mathrm{Ch}_{L_{i}}\cap\mathbf{X}_{p2}\neq\emptyset\)._

Intuitively, based on the graphical criteria of tensor rank condition, \(\mathbf{L}_{p}\) is a minimal conditional set in the causal graph that d-separates \(\mathbf{X}_{p1}\) and \(\mathbf{X}_{p2}\) and hence the rank of tensor \(\mathcal{T}_{(X_{i},X_{j},\mathbf{X}_{p1},\mathbf{X}_{p2})}\) is the dimension of support of \(\mathbf{L}_{p}\), if \(X_{i}\) and \(X_{j}\) also be d-separated by \(\mathbf{L}_{p}\).

**Example 4.8** (CI test among latent variables).: _Consider the structure in Fig. 2(a) and suppose \(r=2\). By selecting \(\mathbf{X}_{p1}\) = \(\{X_{4},X_{7}\}\) and \(\mathbf{X}_{p2}\) = \(\{X_{5},X_{8}\}\) as two disjoint child sets of \(\{L_{2},L_{3}\}\) respectively, let \(\mathbf{X}_{p}\) = \(\{X_{1},X_{10},X_{4},X_{5},X_{7},X_{8}\}\) and \(\mathbf{L}_{p}\) = \(\{L_{2},L_{3}\}\). One can see that the rank of tensor \(\mathcal{T}_{(\mathbf{X}_{p})}\) is four since \(\mathbf{L}_{p}\) (i.e., \(\{L_{2},L_{3}\}\)) is minimal conditional set that d-separates any pair variable in \(\mathbf{X}_{p}^{\prime}\), which imply that \(L_{1}\ \mathbbm{1}\ L_{4}|\{L_{2},L_{3}\}\)._

Based on Theorem 4.7, we introduce the PC-TENSOR-RANK algorithm. This method accepts a measurement model learned by the previous procedure, and outputs the Markov equivalence class of the structural model associated with the latent variables within the measurement model, in accordance with the PC algorithm. The implementation is summarised as Algorithm 2. Consequently, we establish the identification of the structure model as shown in Theorem 4.9.

```
0: Data set \(\mathbf{X}=\{X_{1},\ldots,X_{m}\}\) and causal cluster \(\mathcal{C}\)
0: A partial DAG \(\mathcal{G}\).
1: Initialize the maximal conditions set dimension \(k\);
2: Let \(L_{i}\) denote as \(C_{i}\), \(C_{i}\in\mathcal{C}\);
3: Form the complete undirected graph \(\mathcal{G}\) on the latent variable set \(\mathbf{L}\);
4:for\(\forall L_{i},L_{j}\in\mathbf{L}\) and adjacent in \(\mathcal{G}\)do
5://Test the CI relations among latent variables by Theorem 4.7
6:if\(\exists\mathbf{L}_{p}\subseteq\mathbf{L}\setminus\{L_{i},L_{j}\}\) and (\(|\mathbf{L}_{p}|<k\)) such that \(L_{i}\ \mathbbm{1}\ L_{j}|\mathbf{L}_{p}\) hold then
7: delete edge \(L_{i}-L_{j}\) from \(G\);
8:endif
9:endfor
10: Search V structures and apply meek rules Meek (1995).
11:return a partial DAG \(\mathcal{G}\) of latent variables.
```

**Algorithm 2** PC-TENSOR-RANK

**Theorem 4.9** (Identification of structure model).: _In the discrete 3PLSM, suppose Assumption 2.2 \(\sim\)Assumption 2.4 hold. Given the measurement model, the causal structure over the latent variable is identified up to a Markov equivalent class by the PC-TENSOR-RANK algorithm._

### Practical Test for Tensor Rank

In our theoretical results, the key issue is to test the rank of a tensor, which involves estimating the dimension of latent support and the rank of a tensor. Here, we aim to explore methods to (i) estimate the rank of the contingency matrix for determining the dimension of latent support, and (ii) apply the goodness-of-fit test to assess the tensor rank.

Estimate the rank of contingency matrix.We start with the estimation of the dimension of latent variables support based on Prop. 4.2. There are many practical approaches used to estimate the rank of a general matrix \(\mathrm{M}\), such as Camba-Mendez and Kapetanios (2009). In our implementation, we use the characteristic root statistic, abbreviated as CR statistic Robin and Smith (2000), to test the rank of the probability contingency matrix of two observed variables. Specifically, Let \(\widetilde{\mathrm{M}}\) be an asymptotically normal estimator of \(\mathrm{M}\), then the CR statistic is the sum of \(d-r\) smallest singular values of \(\widetilde{\mathrm{M}}\), multiplied by the sample size. Under the null hypothesis, the above statistic converges in distribution to a weighted (given by the eigenvalues) sum of independent \(\mathcal{X}_{1}^{2}\) random variables.

Goodness-of-fit test for tensor rank.Once the dimension of the support of latent variables is identified, in the structure learning procedure, we perform the following hypotheses test: \(\mathcal{H}_{0}\): \(\mathrm{Rank}(\mathcal{T})\) = _r v.s._\(\mathcal{H}_{1}\): \(\mathrm{Rank}(\mathcal{T})\) # \(r\). To achieve this, we first apply the canonical polyadic (CP) decomposition technology to the target tensor \(\mathcal{T}\) as a sum of \(r\) rank-one tensors given specified \(r\), then we evaluate how well the reconstructed tensor from this decomposition approximates the original tensor to conduct the hypotheses test.

To perform the rank-decomposition with specified \(r\) on the probability contingency tensor, one can use the non-negative CP decomposition to decompose the tensor into the sum of \(r\) rank-one tensor Shashua and Hazan (2005). Given the decomposition, one can obtain a reconstructed tensor, denoted by \(\tilde{\mathcal{T}}\), from the outer product of decomposed vectors.

With the reconstructed tensor, we constructed square-chi goodness of fit test Cochran (1952) for testing \(\mathrm{Rank}(\mathcal{T})\) = \(r\). Such a test is frequently used to summarize the discrepancy between observed values and the expected values, which measure the sum of differences between observed and expected outcome frequencies. Let \(\mathbf{vec}(\mathcal{T})\) be the vectorization of tensor \(\mathcal{T}\), suppose \(\mathbf{vec}(\tilde{\mathcal{T}})\) be the asymptotic normality estimator of \(\mathbf{vec}(\mathcal{T})\), we have the chi-square statistic as \(\mathcal{X}^{2}=\sum_{i\in\mathbf{vec}(\tilde{\mathcal{T}})}\frac{(\mathbf{vec} (\mathcal{T})_{i}-\mathbf{vec}(\tilde{\mathcal{T}})_{i})^{2}}{\mathbf{vec}( \tilde{\mathcal{T}})_{i}}\), which follows the \(\mathcal{X}^{2}\) distribution with freedom degrees \(\prod_{i\in[n]}d_{i}-(\sum_{i,j\in[n]}d_{i}d_{j})\).

## 5 Simulation Studies

In this section, we conducted simulation studies to assess the correctness of the proposed methods. The baseline approaches include Building Pure Cluster (BPC) Silva et al. (2006), Latent Tree Model (LTM) Choi et al. (2011), and Bayesian Pyramid Model (BayPy) Gu and Dunson (2023).

In the following simulation studies, we consider the different combinations of various types of structure models(SM) and measurement models(MM). Specifically, for the structure model, we consider the following five typical cases: [SM1]: \(L_{1}\xrightarrow{}L_{2}\); [SM2]: \(L_{1}\xrightarrow{}L_{2}\xrightarrow{}L_{3}\); [SM3]: the structure of latent variables is shown in Fig. 2(a); [Collider]: \(L_{1}\xrightarrow{}L_{2}\gets L_{3}\); [Star]: \(L_{1}\xrightarrow{}L_{2},L_{1}\xrightarrow{}L_{3},L_{1}\xrightarrow{}L_{4}\). For the measurement model, we consider the following two cases: [MM1]: each latent variable has three pure observed variables, i.e., \(L_{i}\xrightarrow{}\{X_{1},X_{2},X_{3}\}\); [MM2]: each latent variable has four pure observed variables, i.e., \(L_{i}\xrightarrow{}\{X_{1},X_{2},X_{3},X_{4}\}\).

In all cases, the data generation process follows the discrete 3PLSM model: (i) we generate the probability contingency table of latent variables in advance, according to different latent structures (e.g., SM1), then (ii) we generate the conditional contingency table of observed variables (condition on their latent parent), and finally (iii) we sample the observed data according to the probability contingency table, where the dimension of latent support \(r\) is set to 3 and the dimension of all observed variables support is set to 4, sample size ranged from \(\{5\mathrm{k},10\mathrm{k},50\mathrm{k}\}\).

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{**Latent emission**} & \multicolumn{4}{c}{**Latent emission**} & \multicolumn{4}{c}{**Missunements**} \\ \cline{3-14} Algorithm & **Our** & **BayPy** & **LTM** & **BPC** & **Our** & **BayPy** & **LTM** & **BPC** & **Our** & **BayPy** & **LTM** & **BPC** \\ \hline \multirow{3}{*}{\(SM_{1}+MM_{2}\)} & 3k & 0.15(0) & 0.12(0.15) & 0.96(0.00) & 0.00(0.0) & 0.16(0.2) & 0.00(0.00) & 0.05(1) & 0.02(0.00) & 0.00(0.00) & 0.00(0.00) \\  & 10k & 0.05(1) & 0.05(1) & 0.10(2) & 0.90(1) & 0.00(0) & 0.05(1) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) \\  & 50k & 0.00(0) & 0.00(0) & 0.00(0) & 0.90(1) & 0.00(0) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) \\ \hline \multirow{3}{*}{\(SM_{2}+MM_{3}\)} & 5k & 0.23(0.196) & 0.26(0.96) & 0.90(1) & 0.00(0.0) & 0.196(0.31) & 0.00(0.00) & 0.05(1) & 0.196(0.236) & 0.00(0.00) \\  & 10k & 0.13(0.134) & 0.13(0.134) & 0.86(1) & 0.00(0.00) & 0.03(0.4) &

[MISSING_PAGE_FAIL:9]

## Acknowledgments

This research was supported in part by National Key R&D Program of China (2021ZD0111501), National Science Fund for Excellent Young Scholars (62122022), Natural Science Foundation of China (61876043, 61976052, 62476163), the major key project of PCL (PCL2021A12). ZM would like to acknowledge the support of the China Scholarship Council (CSC). FX would like to acknowledge the support by the Natural Science Foundation of China (62306019). JQ would like to acknowledge the support by the Natural Science Foundation of China (62406080). KZ would like to acknowledge the support from NSF Award No. 2229881, AI Institute for Societal Decision Making (AI-SDM), the National Institutes of Health (NIH) under Contract R01HL159805, and grants from Salesforce, Apple Inc., Quris AI, and Florin Court Capital. We appreciate the comments from anonymous reviewers and Area Chairs, which greatly helped to improve the paper.

## References

* Silva et al. (2006) Silva, R.; Scheine, R.; Glymour, C.; Spirtes, P. Learning the structure of linear latent variable models. _JMLR_**2006**, \(7\), 191-246.
* Bollen (2002) Bollen, K. A. Latent variables in psychology and the social sciences. _Annual review of psychology_**2002**, _53_, 605-634.
* Bartholomew et al. (2011) Bartholomew, D. J.; Knott, M.; Moustaki, I. _Latent variable models and factor analysis: A unified approach_; John Wiley & Sons, 2011.
* Cui et al. (2018) Cui, R.; Groot, P.; Schauer, M.; Heskes, T. Learning the Causal Structure of Copula Models with Latent Variables. Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018. 2018; pp 188-197.
* Kummerfeld and Ramsey (2016) Kummerfeld, E.; Ramsey, J. Causal clustering for 1-factor measurement models. KDD. 2016; pp 1655-1664.
* Chen et al. (2024) Chen, Z.; Qiao, J.; Xie, F.; Cai, R.; Hao, Z.; Zhang, K. Testing Conditional Independence Between Latent Variables by Independence Residuals. _IEEE Transactions on Neural Networks and Learning Systems_**2024**,
* Sullivant et al. (2010) Sullivant, S.; Talaska, K.; Draisma, J.; others Trek separation for Gaussian graphical models. _The Annals of Statistics_**2010**, _38_, 1665-1685.
* Xie et al. (2020) Xie, F.; Cai, R.; Huang, B.; Glymour, C.; Hao, Z.; Zhang, K. Generalized Independent Noise Conditionfor Estimating Latent Variable Causal Graphs. NeurIPS. 2020; pp 14891-14902.
* Chen et al. (2022) Chen, Z.; Xie, F.; Qiao, J.; Hao, Z.; Zhang, K.; Cai, R. Identification of Linear Latent Variable Model with Arbitrary Distribution. Proceedings 36th AAAI Conference on Artificial Intelligence (AAAI). 2022.
* Cai et al. (2019) Cai, R.; Xie, F.; Glymour, C.; Hao, Z.; Zhang, K. Triad Constraints for Learning Causal Structure of Latent Variables. NeurIPS. 2019; pp 12863-12872.
* Adams et al. (2021) Adams, J.; Hansen, N.; Zhang, K. Identification of partially observed linear causal models: Graphical conditions for the non-gaussian and heterogeneous cases. Advances in Neural Information Processing Systems. 2021.
* Anandkumar et al. (2013) Anandkumar, A.; Hsu, D.; Javanmard, A.; Kakade, S. Learning linear bayesian networks with latent variables. International Conference on Machine Learning. 2013; pp 249-257.
* Anandkumar et al. (2014) Anandkumar, A.; Ge, R.; Hsu, D. J.; Kakade, S. M.; Telgarsky, M.; others Tensor decompositions for learning latent variable models. _J. Mach. Learn. Res._**2014**, _15_, 2773-2832.
* Anandkumar et al. (2015) Anandkumar, A.; Ge, R.; Janzamin, M. Learning overcomplete latent variable models through tensor methods. Conference on Learning Theory. 2015; pp 36-112.
* Huang et al. (2022) Huang, B.; Low, C. J. H.; Xie, F.; Glymour, C.; Zhang, K. Latent hierarchical causal structure discovery with rank constraints. Advances in Neural Information Processing Systems. 2022.
* Huang et al. (2020)Xie, F.; Huang, B.; Chen, Z.; He, Y.; Geng, Z.; Zhang, K. Identification of linear non-Gaussian latent hierarchical structure. International Conference on Machine Learning. 2022; pp 24370-24387.
* Chen et al. (2023) Chen, Z.; Xie, F.; Qiao, J.; Hao, Z.; Cai, R. Some general identification results for linear latent hierarchical causal structure. Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. 2023; pp 3568-3576.
* Jin et al. (2023) Jin, S.; Xie, F.; Chen, G.; Huang, B.; Chen, Z.; Dong, X.; Zhang, K. Structural Estimation of Partially Observed Linear Non-Gaussian Acyclic Model: A Practical Approach with Identifiability. The Twelfth International Conference on Learning Representations. 2023.
* Eysenck et al. (2021) Eysenck, S. B.; Barrett, P. T.; Saklofske, D. H. The junior eyesnck personality questionnaire. _Personality and Individual Differences_**2021**, _169_, 109974.
* Skinner (2019) Skinner, C. Analysis of categorical data for complex surveys. _International Statistical Review_**2019**, _87_, S64-S78.
* Anandkumar et al. (2012) Anandkumar, A.; Hsu, D.; Kakade, S. M. A method of moments for mixture models and hidden Markov models. Conference on learning theory. 2012; pp 33-1.
* Wang et al. (2017) Wang, X.; Guo, J.; Hao, L.; Zhang, N. L. Spectral methods for learning discrete latent tree models. _Statistics and Its Interface_**2017**, _10_, 677-698.
* Song et al. (2013) Song, L.; Ishteva, M.; Parikh, A.; Xing, E.; Park, H. Hierarchical tensor decomposition of latent tree graphical models. International Conference on Machine Learning. 2013; pp 334-342.
* Gu (2022) Gu, Y. Blessing of Dependence: Identifiability and Geometry of Discrete Models with Multiple Binary Latent Variables. _arXiv preprint arXiv:2203.04403_**2022**,
* Gu and Dunson (2023) Gu, Y.; Dunson, D. B. Bayesian pyramids: Identifiable multilayer discrete latent structure models for discrete data. _Journal of the Royal Statistical Society Series B: Statistical Methodology_**2023**, _85_, 399-426.
* Kant et al. (2024) Kant, M.; Ma, E. Y.; Staicu, A.; Schulman, L. J.; Gordon, S. Identifiability of Product of Experts Models. International Conference on Artificial Intelligence and Statistics. 2024; pp 4492-4500.
* Gordon et al. (2023) Gordon, S. L.; Mazaheri, B.; Rabani, Y.; Schulman, L. Causal Inference Despite Limited Global Confounding via Mixture Models. Conference on Causal Learning and Reasoning. 2023; pp 574-601.
* Mazaheri et al. (2023) Mazaheri, B.; Gordon, S.; Rabani, Y.; Schulman, L. Causal Discovery under Latent Class Confounding. _arXiv preprint arXiv:2311.07454_**2023**,
* Anandkumar et al. (2012) Anandkumar, A.; Hsu, D.; Huang, F.; Kakade, S. M. Learning high-dimensional mixtures of graphical models. _arXiv preprint arXiv:1203.0697_**2012**,
* Kivva et al. (2021) Kivva, B.; Rajendran, G.; Ravikumar, P.; Aragam, B. Learning latent causal graphs via mixture oracles. _Advances in Neural Information Processing Systems_**2021**, _34_.
* Spirtes et al. (2000) Spirtes, P.; Glymour, C.; Scheines, R. _Causation, Prediction, and Search_; MIT press, 2000.
* Spirtes (1991) Spirtes, P.; Glymour, C. An algorithm for fast recovery of sparse causal graphs. _Social science computer review_**1991**, \(9\), 62-72.
* Kolda and Bader (2009) Kolda, T. G.; Bader, B. W. Tensor decompositions and applications. _SIAM review_**2009**, _51_, 455-500.
* Sullivant (2018) Sullivant, S. _Algebraic statistics_; American Mathematical Soc., 2018; Vol. 194.
* Meek (1995) Meek, C. Causal inference and causal explanation with background knowledge. UAI. 1995; pp 403-410.
* Camba-Mendez and Kapetanios (2009) Camba-Mendez, G.; Kapetanios, G. Statistical tests and estimators of the rank of a matrix and their applications in econometric modelling. _Econometric Reviews_**2009**, _28_, 581-611.
* Robin and Smith (2000) Robin, J.-M.; Smith, R. J. Tests of rank. _Econometric Theory_**2000**, _16_, 151-175.
* Ravan et al. (2012)Shashua, A.; Hazan, T. Non-negative tensor factorization with applications to statistics and computer vision. Proceedings of the 22nd international conference on Machine learning. 2005; pp 792-799.
* Cochran (1952) Cochran, W. G. The \(\chi\)2 test of goodness of fit. _The Annals of mathematical statistics_**1952**, 315-345.
* Choi et al. (2011) Choi, M. J.; Tan, V. Y.; Anandkumar, A.; Willsky, A. S. Learning Latent Tree Graphical Models. _Journal of Machine Learning Research_**2011**, _12_, 1771-1812.
* Pearl (2009) Pearl, J. _Causality: Models, Reasoning, and Inference_, 2nd ed.; Cambridge University Press: New York, 2009.
* Leonard and Novick (1986) Leonard, T.; Novick, M. R. Bayesian full rank marginalization for two-way contingency tables. _Journal of Educational Statistics_**1986**, _11_, 33-56.
* Bartolucci et al. (2007) Bartolucci, F.; Colombi, R.; Forcina, A. An extended class of marginal link functions for modelling contingency tables by equality and inequality constraints. _Statistica Sinica_**2007**, 691-711.
* Kruskal (1977) Kruskal, J. B. Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics. _Linear algebra and its applications_**1977**, _18_, 95-138.
* Hackbusch (2012) Hackbusch, W. _Tensor spaces and numerical tensor calculus_; Springer, 2012; Vol. 42.
* Koch and Koch (1990) Koch, K.-R.; Koch, K.-R. Bayes' theorem. _Bayesian Inference with Geodetic Applications_**1990**, 4-8.
* Aish and Joreskog (1990) Aish, A.-M.; Joreskog, K. G. A panel model for political efficacy and responsiveness: An application of LISREL 7 with weighted least squares. _Quality and Quantity_**1990**, _24_, 405-426.
* Joreskog and Sorbom (1996) Joreskog, K. G.; Sorbom, D. _LISREL 8: User's reference guide_; Scientific Software International, 1996.
* Salles et al. (2024) Salles, J.; Stephan, F.; Moliere, F.; Bennabi, D.; Haffen, E.; Bouvard, A.; Walter, M.; Allauze, E.; Llorca, P. M.; Genty, J. B.; others Indirect effect of impulsivity on suicide risk through self-esteem and depressive symptoms in a population with treatment-resistant depression: A FACE-DR study. _Journal of affective disorders_**2024**, _347_, 306-313.

### Supplementary Material

The supplementary material contains

* Graphical Notations;
* Example of Tensor Representations of Joint Distribution;
* Discussion of Our Assumptions;
* Proofs of Main Results;
* Proof of Theorem 3.3;
* Proof of Proposition 4.2;
* Proof of Proposition 4.3;
* Proof of Proposition 4.5;
* Proof of Theorem 4.6;
* Proof of Theorem 4.7;
* Proof of Theorem 4.9.
* Extension of Different Latent State Space;
* Discussion with the Hierarchical Structures;
* Practical Estimation of Tensor Rank;
* More Experimental Results;
* Experimental Results on Real-world Dataset;

## Appendix A Graphical Notations

Below, we provide some graphical notation used in our work, which is mainly derived from the Pearl (2009); Spirtes et al. (2000).

**Definition A.1** (Path and Directed Path).: _In a DAG, a **path**\(P\) is a sequence of nodes \((V_{1},...V_{r})\) such that \(V_{i}\) and \(V_{i+1}\) are adjacent in \(\mathcal{G}\), where \(1\leq i<r\). Further, we say a path \(P\) = \((V_{i_{0}},V_{i_{1}},\ldots,V_{i_{k}})\) in \(G\) is a **directed path** if it is a sequence of nodes of \(G\) where there is a directed edge from \(V_{i_{j}}\) to \(V_{i_{(j+1)}}\) for any \(0\leq j\leq k-1\)._

**Definition A.2** (Collider).: _A **collider** on a path \(\{V_{1},...V_{p}\}\) is a node \(V_{i}\), \(1<i<p\), such that \(V_{i-1}\) and \(V_{i+1}\) are parents of \(V_{i}\)._

Graphically, we also say a collider is a 'V-structure'.

**Definition A.3** (d-separation).: _A path \(p\) is said to be d-separated (or blocked) by a set of nodes \(\mathbf{Z}\) if and only if the following two conditions hold:_

* \(p\) _contains a chain_ \(V_{i}\to V_{k}\to V_{j}\) _or a fork_ \(V_{i}\gets V_{k}\to V_{j}\) _such that the middle node_ \(V_{k}\) _is in_ \(\mathbf{Z}\)_;_
* \(p\) _contains a collider_ \(V_{i}\to V_{k}\gets V_{j}\) _such that the middle node_ \(V_{k}\) _is not in_ \(\mathbf{Z}\) _and such that no descendant of_ \(V_{k}\) _is in_ \(\mathbf{Z}\)_._

A set \(\mathbf{Z}\) is said to d-separate \(\mathbf{A}\) and \(\mathbf{B}\) if and only if \(\mathbf{Z}\) blocks every path from a node in \(\mathbf{A}\) to a node in \(\mathbf{B}\). We also denote as \(\mathbf{A}\ \rotatebox[origin={c}]{90.0}{$\models$}\ \mathbf{B}|\mathbf{Z}\) in the causal graph model.

## Appendix B Example of Tensor Representations of Joint Distribution

Consider a single latent variable structure that has three pure observed variables, i.e., \(L_{1}\to\{X_{1},X_{2},X_{3}\}\). We aim to illustrate the tensor representation of the probability contingency table and the tensor rank condition for the joint distribution \(\mathbb{P}(X_{1}X_{2}X_{3})\). For convenience, let \(\mathrm{supp}(L_{1})\) = \(\{0,1\}\) and \(\mathrm{supp}(X_{i})\) = \(\{0,1,2\}\). We further denote \(p_{ij}\) = \(\mathbb{P}(X_{1}\) = \(i,X_{2}\) = \(j)\), \(p_{i|j}\) = \(\mathbb{P}(X_{1}\) = \(i|L_{1}\) = \(j)\), \(p_{i|j}\) = \(\mathbb{P}(X_{2}\) = \(i|L_{1}\) = \(j)\), \(\bar{p}_{i}\) = \(\mathbb{P}(L_{1}\) = \(i)\), and \(p_{ijk}\) = \(\mathbb{P}(X_{1}\) = \(i,X_{2}\) = \(j,X_{3}\) = \(k)\). For the joint distribution of \(\mathbb{P}(X_{1}X_{2})\), we have the tensor representation as follows:\[\mathcal{T}_{(X_{1}X_{2})}=\begin{bmatrix}p_{00}&p_{01}&p_{02}\\ p_{10}&p_{11}&p_{12}\\ p_{20}&p_{21}&p_{22}\end{bmatrix}=\underbrace{\begin{bmatrix}p_{01}&p_{01}\\ p_{10}&p_{11}\\ p_{2|0}&p_{2|1}\end{bmatrix}}_{\mathcal{T}_{(X_{1}|L_{1})}}\cdot\underbrace{ \begin{bmatrix}\bar{p_{0}}&0\\ 0&\bar{p_{1}}\\ 0&\bar{p_{1}}\end{bmatrix}}_{\text{Diag}(\mathcal{T}_{L_{1})}}\cdot\underbrace {\begin{bmatrix}\bar{p_{0}}&0\\ 0&\bar{p_{1}}\\ 0&1&\bar{p_{1}}\\ \end{bmatrix}}_{\text{Diag}(\mathcal{T}_{L_{1})}}, \tag{1}\]

where \(\mathcal{T}_{(X_{1}|L_{1})}\) is the tensor representation of \(\mathbb{P}(X_{1}|L_{1})\), \(\mathcal{T}_{(X_{2}|L_{1})}\) is the tensor representation of \(\mathbb{P}(X_{2}|L_{1})\) and \(\text{Diag}(\mathcal{T}_{(L_{1})})\) is the diagonalization of \(\mathbb{P}(L_{1})\).

Under the Full Rank assumption, we have \(\mathcal{T}_{(X_{1}|L_{1})}\) and \(\mathcal{T}_{(X_{2}|L_{1})}\) are column full rank. Thus, the rank of \(\mathcal{T}_{X_{1}X_{2}}\) is two, i.e., \(\text{Rank}(\mathcal{T}_{X_{1}X_{2}})=|\text{supp}(L_{1})|=2\). This illustrate the Prop. 1.

Next, we consider the three-way tensor \(\mathcal{T}_{(X_{1}X_{2}X_{3})}\) of the joint distribution \(\mathbb{P}(X_{1}X_{2}X_{3})\). We will represent the three-way tensor as its frontal slices Kolda and Bader (2009), i.e., three matrices for \(\mathbb{P}(X_{1}X_{2}X_{3}=0)\), \(\mathbb{P}(X_{1}X_{2}X_{3}=1)\) and \(\mathbb{P}(X_{1}X_{2}X_{3}=2)\).

\[\underbrace{\begin{bmatrix}p_{000}&p_{010}&p_{020}\\ p_{100}&p_{110}&p_{120}\\ p_{200}&p_{210}&p_{220}\end{bmatrix}}_{\mathcal{T}_{(X_{1}X_{2}X_{3}=0)}}, \underbrace{\begin{bmatrix}p_{001}&p_{011}&p_{021}\\ p_{101}&p_{111}&p_{121}\\ p_{201}&p_{211}&p_{221}\end{bmatrix}}_{\mathcal{T}_{(X_{1}X_{2}X_{3}=1)}}, \underbrace{\begin{bmatrix}p_{002}&p_{012}&p_{022}\\ p_{102}&p_{112}&p_{122}\\ p_{202}&p_{212}&p_{222}\end{bmatrix}}_{\mathcal{T}_{(X_{1}X_{2}X_{3}>2)}}. \tag{2}\]

In the contingency tensor above, the element of \(\mathcal{T}_{(X_{1}X_{2}X_{3})}\) is

\[\mathbb{P}(X_{1}=i,X_{2}=j,X_{3}=k) \tag{3}\] \[=\sum_{r=0}^{1}\mathbb{P}(X_{1}=i,X_{2}=j,X_{3}=k|L_{1}=r)\mathbb{ P}(L_{1}=r)\] \[=\sum_{r=0}^{1}\mathbb{P}(X_{1}=i|L_{1}=r)\mathbb{P}(X_{2}=j|L_{1 }=r)\mathbb{P}(X_{3}=k|L_{1}=r)\mathbb{P}(L_{1}=r).\]

One can represent the tensor \(\mathcal{T}_{X_{1}X_{2}X_{3}|L_{1}=r}\) as

\[\mathcal{T}_{(X_{1}X_{2}X_{3}|L_{1}=r)}=\begin{bmatrix}\mathbb{P}(X_{1}=0|L_{1 }=r)\\ \mathbb{P}(X_{1}=1|L_{1}=r)\\ \mathbb{P}(X_{1}=2|L_{1}=r)\end{bmatrix}\otimes\begin{bmatrix}\mathbb{P}(X_{ 2}=0|L_{1}=r)\\ \mathbb{P}(X_{2}=1|L_{1}=r)\\ \mathbb{P}(X_{2}=2|L_{1}=r)\end{bmatrix}\otimes\begin{bmatrix}\mathbb{P}(X_{ 3}=0|L_{1}=r)\\ \mathbb{P}(X_{2}=1|L_{1}=r)\\ \mathbb{P}(X_{3}=2|L_{1}=r)\end{bmatrix}\otimes\begin{bmatrix}\mathbb{P}(X_{ 3}=0|L_{1}=r)\\ \mathbb{P}(X_{2}=1|L_{1}=r)\\ \mathbb{P}(X_{3}=2|L_{1}=r)\end{bmatrix}\otimes\begin{bmatrix}\mathbb{P}(X_{ 3}=0|L_{1}=r)\\ \mathbb{P}(X_{2}=1|L_{1}=r)\end{bmatrix}\otimes\begin{bmatrix}\mathbb{P}(X_{ 3}=1|L_{1}=r)\\ \mathbb{P}(X_{3}=1|L_{1}=r)\\ \mathbb{P}(X_{3}=2|L_{1}=r)\end{bmatrix}}_{\mathbb{P}(X_{3}=1|L_{1}=r)}, \tag{4}\]

where \(\otimes\) represent the outer product, e.g., for two vector \(\mathbf{u}\) and \(\mathbf{v}\), \((\mathbf{u}\otimes\mathbf{v})_{ij}=u_{i}v_{j}\) with \(\mathbf{u}=\{u_{1},\cdots,u_{n}\}\) and \(\mathbf{v}=\{v_{1},\cdots,v_{n}\}\).

According to the definition of tensor rank, one can see that \(\mathcal{T}_{(X_{1}X_{2}X_{3}|L_{1}=r)}\) is a rank-one tensor. Furthermore, since \(\mathcal{T}_{(X_{1}X_{2}X_{3})}=\sum_{r=1}^{2}\mathcal{T}_{(X_{1}X_{2}X_{3}|L_{ 1}=r)}\mathcal{T}_{(L_{1}=r)}\), the rank of \(\mathcal{T}_{(X_{1}X_{2}X_{3})}\) is two under Assumption 2.2 \(\sim\) Assumption 2.4. This is because \(L_{1}\) d-separates \(X_{i}\) and \(X_{j}\) with \(\forall i,j\in[1,2,3]\), which illustrate the graphical criteria of tensor rank condition.

## Appendix C Discussion of Our Assumptions

To study the discrete statistical model, certain commonly-used parameter assumptions are necessary. For instance, the Full Rank assumption (Assumption 2.4), as utilized in our study, ensures diversity within the parameter space. This is crucial to prevent the contingency table of the joint distribution from collapsing into a lower-dimensional space. There are related works that also use such an assumption Leonard and Novick (1986); Bartolucci et al. (2007); Gu (2022). Essentially, in our work, such an assumption ensures the effectiveness of rank decomposition (e.g., minimal decomposition or unique decomposition Kruskal (1977)), which induces the structural identifiability of a discrete causal model. Moreover, the sufficient observation assumption that the dimension of latent support is larger than the dimension of observed support is reasonable, as it ensures sufficient measurement of the latent variable. We also discuss this assumption in the remark E.4 that demonstrates the CI relation is testable if this assumption holds.

It is worth noting that, although Kivva et al. (2021) shows that the causal structure among latent variables can be identifiable under weaker assumptions, by leveraging the identifiability of the mixture model, we emphasize that our algorithm, based on the tensor rank condition, is simpler and more efficient. It can also produce more robust results when performing causal discovery on the observational data. Moreover, in Kivva et al. (2021), the parameters of the mixture oracle, such as the number of components, are estimated using approximate methods like the K-means algorithm, which may not be applied directly to discrete data.

## Appendix D Proofs of Main Results

### Proof of Theorem 3.3

Proof.: "If" part:

We prove this by contradiction, i.e., suppose (i). there exists a variable set \(\mathbf{S}\) in the causal graph \(\mathcal{G}\) with \(|\mathrm{supp}(\mathbf{S})|\) = \(r\) that d-separates any pair of variables in \(\{X_{1},\cdots,X_{n}\}\), and (ii).does no exist conditional set \(\tilde{\mathbf{S}}\) that satisfies \(|\mathrm{supp}(\tilde{\mathbf{S}})|\) < \(r\), then \(\mathrm{Rank}(\mathcal{T}_{\{X_{1}\ldots X_{n}\}})\neq r\). There are two cases we need to consider: Case 1: \(\mathrm{Rank}(\mathcal{T}_{\{X_{1}\ldots X_{n}\}})>r\), and Case 2: \(\mathrm{Rank}(\mathcal{T}_{\{X_{1}\ldots X_{n}\}})<r\).

Case 1: Due to \(\mathbf{S}\) is conditional set that d-separates all variables in \(\{X_{1},\cdots,X_{n}\}\), then we have \(\mathbb{P}(X_{1}\cdots X_{n})=\sum_{i=1}^{r}(\prod_{j=1}^{n}\mathbb{P}(X_{j}| \mathbf{S}=i))\mathbb{P}(\mathbf{S}=i)\). When \(\mathrm{Rank}(\mathcal{T}_{\{X_{1}\ldots X_{n}\}})>r\), let \(\mathrm{Rank}(\mathcal{T}_{\{X_{1}\ldots X_{n}\}})=k>r\),there are

\[\mathcal{T}_{\{X_{1}\ldots X_{n}\}} \tag{5}\] \[=\sum_{i=1}^{r}\mathcal{T}_{\{X_{1}|\mathbf{S}=i\}}\otimes\cdots \otimes\mathcal{T}_{\{X_{n}|\mathbf{S}=i\}}\mathcal{T}_{\{\mathbf{S}=i\}}\] \[=\sum_{j=1}^{k}\mathbf{u}_{1}^{(j)}\otimes\cdots\otimes\mathbf{u} _{n}^{(j)},\]

which violates the definition of tensor rank (i.e., \(k\) is not a minimal rank-one decomposition, it can be reduced to the smaller decomposition with \(r\)). Meanwhile, if there exists other conditional sets \(\tilde{\mathbf{S}}\) such that \(\mathbb{P}(X_{1}\cdots X_{n})=\sum_{i=1}^{k}(\prod_{j=1}^{n}\mathcal{T}_{\{X_ {j}|\mathbf{S}=i\}})\mathcal{T}_{\{\mathbf{S}=i\}}\) is minimal rank decomposition, it violates the condition (ii) that \(|\mathrm{supp}(\mathbf{S})|\) is minimal.,

Case 2: When \(\mathrm{Rank}(\mathcal{T}_{\{X_{1}\ldots X_{n}\}})<r\), let \(\mathrm{Rank}(\mathcal{T}_{\{X_{1}\ldots X_{n}\}})=t<r\), due to \(\mathbf{S}\) is conditional set with smallest support \(r\) in the causal graph, one have

\[\mathcal{T}_{\{X_{1}\ldots X_{n}\}} \tag{6}\] \[=\sum_{i=1}^{r}\mathcal{T}_{\{X_{1}|\mathbf{S}=i\}}\otimes\cdots \otimes\mathcal{T}_{\{X_{n}|\mathbf{S}=i\}}\mathcal{T}_{\{\mathbf{S}=i\}}\] \[=\sum_{j=1}^{t}\mathbf{u}_{1}^{(j)}\otimes\cdots\otimes\mathbf{u} _{n}^{(j)},\]

which means that there at least exist \(\mathcal{T}_{\{X_{1}\ldots X_{n}|\mathbf{S}=i\}}\) and \(\mathcal{T}_{\{X_{1}\ldots X_{n}|\mathbf{S}=j\}}\) such that \(\mathcal{T}_{\{X_{j}|\mathbf{S}=i\}}=\alpha\mathcal{T}_{\{X_{k}|\mathbf{S}=i\}}\) for any \(i,k\in[n]\) where \(\alpha\) is a constant, i.e, the columns of the conditional contingency table are linearly dependent. This violates the assumption 2.4 (the Full Rank assumption).

Therefore, \(\mathrm{Rank}(\mathcal{T}_{\{X_{1}\ldots X_{n}\}})=r\) if the condition (i) and condition (ii) holds.

"Only if" part:

We will show if one of the conditions is violated, the tensor rank is not \(r\), i.e., if (i). there does not exist a variable set \(\mathbf{S}\) in the causal graph with \(|\mathrm{supp}(\mathbf{L})|=r\) that d-separates any pair of variables in \(\{X_{1},\cdots,X_{n}\}\), or (ii). exist \(\tilde{\mathbf{S}}\) that satisfies \(|\mathrm{supp}(\tilde{\mathbf{S}})|<r\), then \(\mathrm{Rank}(\mathcal{T}_{\{X_{1}\ldots X_{n}\}})\neq r\).

We first show the case that condition (i) is violated. There are two cases we need to consider, i.e., Case 1: \(\mathbf{S}\) is not a conditional set in the causal graph, and Case 2: \(\mathbf{S}\) is a variable constructed from parameter space.

Case 1: if \(\mathbf{S}\) is not a conditional set and \(\mathbf{S}\cap\operatorname{Des}_{X_{1}}\cap\cdots\cap\operatorname{Des}_{X_{n}}=\varnothing\), by the Markov assumption, \(\mathbb{P}(X_{1},\cdots,X_{n}|\mathbf{S}=i)\neq\prod_{j=1}^{n}\mathbb{P}(X_{j} |\mathbf{S}=i)\). By Lemma. D.3, \(\mathbb{P}(X_{1},\cdots,X_{n}|\mathbf{S}=i)\) is not a rank-one tensor, which violates the definition of tensor rank.

If \(\mathbf{S}\) is not a conditional set and \(\mathbf{S}\cap\operatorname{Des}_{X_{1}}\cap\cdots\cap\operatorname{Des}_{X_{n }}\neq\varnothing\), we will show that \(\mathbb{P}(X_{1},\cdots,X_{n}|\mathbf{S}=i)\) is not a rank-one tensor, to prove such a rank-decomposition does not exist. Under the faithfulness assumption and Markov assumption, let \(\tilde{\mathbf{S}}\) be the a minimal conditional set with \(|\mathrm{supp}(\tilde{\mathbf{S}})|=k\) for any pair variable in \(\{X_{1},\cdots,X_{n}\}\), we have

\[\mathcal{T}_{(X_{1}\cdots X_{n}|\mathbf{S}=i)} \tag{7}\] \[=\sum_{j=1}^{k}\mathcal{T}_{(X_{1}\cdots X_{n}|\mathbf{S}=i, \tilde{\mathbf{S}}=j)}\mathcal{T}_{(\mathbf{S}=i|\tilde{\mathbf{S}}=j)}.\]

If \(\mathcal{T}_{(X_{1}\cdots X_{n}|\mathbf{S}=i)}\) is a rank-one tensor, i.e., \(\mathcal{T}_{(X_{1}\cdots X_{n}|\mathbf{S}=i)}=\mathbf{u}_{1}\otimes\cdots \otimes\mathbf{u}_{n}\), we further have

\[\sum_{j=1}^{k}\mathcal{T}_{(X_{1}\cdots X_{n}|\mathbf{S}=i,\tilde{\mathbf{S}}= j)}\mathcal{T}_{(\mathbf{S}=i|\tilde{\mathbf{S}}=j)}=\mathbf{u}_{1}\otimes \cdots\otimes\mathbf{u}_{n}. \tag{8}\]

Note that if there exists \(X_{p},X_{q}\in\{X_{1},\cdots,X_{n}\}\) such that \(\mathbf{S}\) is the common descendant variable of \(X_{p}\) and \(X_{q}\), it will lead to a collider structure in which \(\mathbb{P}(X_{i}|\mathbf{S})\) and \(\mathbb{P}(X_{j}|\mathbf{S})\) is relevant (i.e., the v-structure is activated). Thus, let \(\mathbf{X}_{t}=\{X_{1},\cdots,X_{n}\}\setminus\{X_{i},X_{j}\}\), \(\mathcal{T}_{(X_{1}\cdots X_{n}|\mathbf{S}=i)}\) is not a rank-one tensor due to the sub-tensor \(\mathcal{T}_{(X_{i}X_{j},\mathbf{X}_{t}=\mathbf{c}|\mathbf{S}=i)}\) (a slice of tensor \(\mathcal{T}_{(X_{1}\cdots X_{n}|\mathbf{S}=i)}\)) is not a rank-one tensor 2Hackbusch (2012); Kruskal (1977), under the faithfulness assumption. If so, one have \(\mathcal{T}_{(X_{i}X_{j},\mathbf{X}_{t}=\mathbf{c}|\mathbf{S}=i)}=\mathbf{u}_{1 }\otimes\mathbf{u}_{2}\). Let \(\mathbf{u}_{1}=\mathbb{P}(X_{i},\mathbf{X}_{t}=\mathbf{c}|\mathbf{S}=i)\) and \(\mathbf{u}_{2}=\mathbb{P}(X_{j},\mathbf{X}_{t}=\mathbf{c}|\mathbf{S}=i)\), it violates the faithfulness assumption.

Footnote 2: The lower bound of tensor rank is not less than the rank of any slice of tensor.

Thus, \(\mathbf{S}\) can not be the common descendant of any pair variables in \(\{X_{1},\cdots,X_{n}\}\). So, for any \(X_{p},X_{q}\in\{X_{1},\cdots,X_{n}\}\), there are \(\mathcal{T}_{(X_{p}X_{q}|\mathbf{S}=j\tilde{\mathbf{S}}=i)}=\mathcal{T}_{(X_{p }|\tilde{\mathbf{S}}=i,\operatorname{Des}_{X_{p}})=c}\otimes\mathcal{T}_{(X_{q }|\tilde{\mathbf{S}}=i,\operatorname{Des}_{X_{q}})=c}\) according to Markov assumption (\(\tilde{\mathbf{S}}\) is a d-separation set for \(X_{p}\) and \(X_{q}\)).

Now, for the Eq. 8, we have

\[\mathcal{T}_{(X_{1}\cdots X_{n}|\mathbf{L}=i)} \tag{9}\] \[=\sum_{j=1}^{k}\otimes_{t=1}^{n}\mathcal{T}_{(X_{t}|\mathbf{S}=j,\operatorname{Des}_{X_{t}}=c)}\mathcal{T}_{(\mathbf{S}=j,\operatorname{Des}_ {X_{t}}=c|\tilde{\mathbf{S}}=j)}\] \[=\mathbf{u}_{1}\otimes\cdots\otimes\mathbf{u}_{n}.\]

This equality holds if the \(k\) sum of the rank-one tensor can be reduced to a rank-one tensor, i.e., for any \(X_{p}\), \(\mathcal{T}_{(X_{p}|\tilde{\mathbf{S}}=1,\operatorname{Des}_{X_{p}}=c)}= \alpha_{2}\mathcal{T}_{(X_{p}|\tilde{\mathbf{S}}=2,\operatorname{Des}_{X_{p}}= c)}=\cdots=\alpha_{r}\mathcal{T}_{(X_{p}|\tilde{\mathbf{S}}=r, \operatorname{Des}_{X_{p}}=c)}\), where \(\alpha_{i}\) and \(c\) are constant. However, this equality can not hold due to the following reasons.

If \(\operatorname{Pa}_{X_{p}}\neq\varnothing\), let \(L_{p}\) be the parent of \(X_{p}\), we have

\[\mathcal{T}_{(X_{p}|\tilde{\mathbf{S}}=i,\operatorname{Des}_{X_{ p}}=c)} \tag{10}\] \[=\sum_{j=1}^{|\mathrm{supp}(L_{p})|}\mathcal{T}_{(X_{p}|L_{p}=j, \operatorname{Des}_{X_{p}}=c)}\mathcal{T}_{(L_{p}=j|\tilde{\mathbf{S}}=i, \operatorname{Des}_{X_{p}}=c)}\] \[\neq\sum_{j=1}^{|\mathrm{supp}(L_{p})|}\mathcal{T}_{(X_{p}|L_{p}=j,\operatorname{Des}_{X_{p}}=c)}\mathcal{T}_{(L_{p}=j|\tilde{\mathbf{S}}=r, \operatorname{Des}_{X_{p}}=c)}=\alpha\mathcal{T}_{(X_{p}|\tilde{\mathbf{S}}=r, \operatorname{Des}_{X_{p}}=c)},\]in which the inequality holds because of the Full Rank assumption and all marginal distribution probabilities are not zero (see Lemma. D.4). Therefore, \(\mathcal{T}_{(X_{p}|\tilde{\mathbf{S}}=1,\mathrm{Des}_{X_{p}}=c)}\not\vdash \cdots\not\vdash\alpha_{r}\mathcal{T}_{(X_{p}|\tilde{\mathbf{S}}=r,\mathrm{Des}_ {X_{p}}=c)}\). Thus, \(\mathcal{T}_{(X_{1}\cdots X_{n}|\mathbf{S}=i)}\) is not a rank-one tensor. By the definition of tensor rank, \(\mathrm{Rank}(\mathcal{T}_{(X_{1}\cdots X_{n})})\not=r\).

If \(\mathrm{Pa}_{X_{p}}=\emptyset\), i.e., \(X_{p}\) is the root variable, (e.g., \(X_{p}\rightarrow\mathbf{S},\mathrm{Des}_{X_{p}}\)s ), we will show that the probability contingency table \(\mathcal{T}_{(X_{p}|\mathbf{S})}\) is full rank, and then the equality in Eq. 8 can not hold. According to the Full Rank assumption and \(X_{p}\) is the parent variable of \(\mathbf{S}\), we have the probability contingency table \(\mathcal{T}_{(\mathbf{S}|X_{p})}\) is full rank.

Due to \(\mathbb{P}(X_{p},\mathbf{S})=\mathbb{P}(\mathbf{S}|X_{p})\mathbb{P}(X_{p})= \mathbb{P}(X_{p}|\mathbf{S})\mathbb{P}(\mathbf{S})\) and the probability in the marginal distribution is not zero, we have \(\mathcal{T}_{(X_{p}|\mathbf{S})}=\mathcal{T}_{(\mathbf{S}|X_{p})}\mathrm{ Diag}(\mathcal{T}_{(X_{p})})\mathrm{Diag}(\mathcal{T}_{(\mathbf{S})})^{\dagger}\), where \(\mathrm{Diag}(\mathcal{T}_{(X_{p})})\) is a diagonalization of marginal distribution probability vector of \(X_{p}\). One can see that \(\mathcal{T}_{(X_{p}|\mathbf{S})}\) is full rank due to the diagonal matrices are all of full rank. Thus, \(\mathcal{T}_{(X_{p}|\tilde{\mathbf{S}}=1)}\not\vdash\cdots\not\vdash\alpha_{r} \mathcal{T}_{(X_{p}|\tilde{\mathbf{S}}=r)}\) and \(\mathcal{T}_{(X_{1}\cdots X_{n}|\mathbf{S}=i)}\) is not a rank-one tensor. By the definition of tensor rank, \(\mathrm{Rank}(\mathcal{T}_{(X_{1}\cdots X_{n})})\not=r\).

Case 2: we further show that for the parameter space, the tensor \(\mathcal{T}_{(X_{1}\cdots X_{n})}=\sum_{i=1}^{r}\mathbf{u}_{1}\otimes\cdots \otimes\mathbf{u}_{n}\) does not hold with \(r\), where \(\mathbf{u}_{i}\) represents any vector. Let \(\mathbf{u}_{i}\otimes\cdots\otimes\mathbf{u}_{n}\) be a rank-one tensor of \(\mathbb{P}(X_{1},\cdots,X_{n}|\tilde{\mathbf{S}}=i)\mathbb{P}(\tilde{\mathbf{ S}}=i)\) due to \(\mathbb{P}(\tilde{\mathbf{S}}=i)\) is a constant. In other words, one can construct a variable set \(\tilde{\mathbf{S}}\) with \(|\mathrm{supp}(\tilde{\mathbf{S}})|=r\). For any \(\tilde{\mathbf{S}}\) constructed from parameter space (i.e., \(\tilde{\mathbf{S}}\) is not a true node set in the causal graph), if \(\mathcal{T}_{(X_{1},\cdots,X_{n}|\tilde{\mathbf{S}}=i)}=\mathbf{u}_{1}\otimes \cdots\otimes\mathbf{u}_{n}\), one can let \(\mathbf{u}_{1}=\mathcal{T}_{(X_{1}|\tilde{\mathbf{S}}=i)},\cdots,\mathbf{u}_{n }=\mathcal{T}_{(X_{n}|\tilde{\mathbf{S}}=i)}\), which violates the faithfulness assumption. Based on the above analysis, there does not exist \(\tilde{\mathbf{S}}\) by any constructed such that \(\mathcal{T}_{(X_{1}\cdots X_{n})}\) have the summation \(r\) rank-one decomposition, i.e., \(\mathrm{Rank}(\mathcal{T}_{(X_{1}\cdots X_{n})})\not=r\).

Now, we analyze the condition (ii), i.e., there exists a conditional set \(\tilde{\mathbf{S}}\) with \(\mathrm{supp}(\tilde{\mathbf{S}})<r\). Let \(\mathrm{supp}(\tilde{\mathbf{S}})=k\), \(k<r\), we have \(\mathcal{T}_{(X_{1}\cdots X_{n})}=\sum_{i=1}^{k}\mathcal{T}_{(X_{1}|\tilde{ \mathbf{S}}=i)}\cdots\otimes\mathcal{T}_{(X_{n}|\tilde{\mathbf{S}}=i)}\mathcal{ T}_{(\tilde{\mathbf{S}}=i)}\) is a smaller rank-one decomposition than \(\mathcal{T}_{(X_{1}\cdots X_{n})}=\sum_{j=1}^{r}\mathbf{u}^{\dagger}_{1} \otimes\cdots\otimes\mathbf{u}^{j}_{n}\). According to the definition of tensor rank, we have \(\mathrm{Rank}(\mathcal{T}_{(X_{1}\cdots X_{n})})=k\not=r\).

In summary, the theorem is proven.

**Lemma D.1**.: _Let \(L_{p}\) be the parent of \(X_{p}\). Consider \(\mathbb{P}(X_{p}|\mathbf{S}=i)\) and \(\mathbb{P}(X_{p}|\mathbf{S}=r)\), where \(\mathbf{S}\) can be any variable set. Under the Assumption 2.4, for a constant \(c\), the following inequality holds:_

\[\sum_{j=1}^{|\mathrm{supp}(L_{p})|}\mathcal{T}_{(X_{p}|L_{p}=j)}\mathcal{T}_{(L _{p}=j|\mathbf{S}=i)}\not=c\sum_{j=1}^{|\mathrm{supp}(L_{p})|}\mathcal{T}_{(X_ {p}|L_{p}=j)}\mathcal{T}_{(L_{p}=j|\mathbf{S}=r)}.\]

Proof.: We prove it by contradiction. For convenience of symbols, let \(\mathbf{v}_{j}=\mathcal{T}_{(X_{p}|L_{p}=j)}\), and \(\alpha_{j|i}=\mathbb{P}(L_{p}=j|\mathbf{L}=i)\) and \(\beta_{j|r}=\mathbb{P}(L_{p}=j|\mathbf{L}=r)\) (\(\alpha_{j|i}\not=\beta_{j|r}\)), if the equality hold, we have

\[\sum_{j=1}^{|\mathrm{supp}(L_{p})|}\alpha_{j|i}\mathbf{v}_{j}=c\sum_{j=1}^{| \mathrm{supp}(L_{p})|}\beta_{j|r}\mathbf{v}_{j}, \tag{11}\]

which means that

\[\mathbf{v}_{t}=\frac{\sum_{k\not=t}^{|\mathrm{supp}(L_{p})|}(\alpha_{k|i}-c \beta_{k|r})\mathbf{v}_{k}}{\alpha_{t|i}-c\beta_{t|r}}. \tag{12}\]

That is, \(\mathbf{v}_{t}\) is a linear combination of other vectors \(\mathbf{v}_{k}\) with \(t\not=k\), i.e., the linear combination of other column vectors in the conditional probability contingency table, which is contrary to the Assumption 2.4.

**Lemma D.2**.: _Let \(L_{p}\) be the parent of \(X_{p}\). Suppose Assumption 2.2 \(\sim\) Assumption 2.4 hold. \(\mathcal{T}_{(X_{p}|\mathbf{S}=i)}\) cannot be expressed as a linear combination of other \(q\) vectors \(\mathcal{T}_{(X_{p}|\mathbf{S}=r)}\), i.e., \(\mathcal{T}_{(X_{p}|\mathbf{S}=i)}\neq\sum_{r=1}^{q}\gamma_{r}\mathcal{T}_{(X_ {p}|\mathbf{S}=r)}\)._

Proof.: We prove it by contradiction. For convenience of symbols, let \(\mathbf{v}_{j}=\mathcal{T}_{(X_{p}|L_{p}=j)}\), and \(\alpha_{j|i}\) = \(\mathbb{P}(L_{p}=j|\mathbf{S}=i)\) and \(\beta_{j|r}\) = \(\mathbb{P}(L_{p}=j|\mathbf{S}=r)\), if the equality hold, one have

\[\begin{split}&\sum_{j=1}^{|\mathrm{supp}(L_{p})|}\alpha_{j|i} \mathbf{v}_{j}=\sum_{r=1}^{q}\gamma_{r}\sum_{j=1}^{|\mathrm{supp}(L_{p})|}\beta _{j|r}\mathbf{v}_{j}\\ &=\sum_{j=1}^{|\mathrm{supp}(L_{p})|}\mathbf{v}_{j}\sum_{r=1}^{q} \gamma_{r}\beta_{j|r},\end{split} \tag{13}\]

there exist a vector \(\mathbf{v}_{t}\) with \(\alpha_{t|i}-\sum_{r=1}^{q}\gamma_{r}\beta_{t|r}\neq 0\), such that

\[\mathbf{v}_{t}=\frac{\sum_{k\neq t}^{|\mathrm{supp}(L_{p})|}(\sum_{r=1}^{q} \gamma_{r}\beta_{j|r}-\alpha_{k|i})\mathbf{v}_{k}}{\alpha_{t|i}-\sum_{r=1}^{q} \gamma_{r}\beta_{t|r}}. \tag{14}\]

It means \(\mathcal{T}_{(X_{p}|L_{p}=t)}\) is a linear combination of other column vectors in the conditional probability contingency table, which is contrary to Assumption 2.4.

**Lemma D.3**.: _For the set of variables \(\{X_{1},\cdots,X_{n}\}\), if \(\tilde{\mathbf{S}}\) is not a conditional set that d-separates any pair of variables in \(\{X_{1},\cdots,X_{n}\}\) in the causal graph and \(\tilde{\mathbf{S}}\cap\mathrm{Des}_{\{X_{1}\cdots X_{n}\}}\) = \(\varnothing\), then \(\mathcal{T}_{(X_{1}\cdots X_{n}|\mathbf{S}=j)}\) is not a rank-one tensor._

Proof.: Let \(\mathbf{S}\) be the minimal conditional set, (e.g., \(\mathbf{S}=\{X_{1},\cdots,X_{n-1}\}\)), denote \(|\mathrm{supp}(\mathbf{S})|=r\), under the faithfulness assumption and the Markov assumption, \(\mathbb{P}(\mathbf{S})\neq\mathbb{P}(\tilde{\mathbf{S}})\), if \(\mathcal{T}_{(X_{1}\cdots X_{n}|\mathbf{S}=j)}\) is a rank-one tensor, we have

\[\begin{split}&\mathcal{T}_{(X_{1}\cdots X_{n}|\tilde{\mathbf{S}}=j )}\\ &=\sum_{i=1}^{r}\mathfrak{G}_{t=1}^{n}\mathcal{T}_{(X_{t}|\mathbf{ S}=i)}\mathcal{T}_{(\mathbf{S}=i|\tilde{\mathbf{S}}=j)}\\ &=\mathbf{u}_{1}\otimes\cdots\otimes\mathbf{u}_{n},\end{split} \tag{15}\]

which means that for any \(X_{p}\in\{X_{1},\cdots X_{n}\}\), \(\mathcal{T}_{(X_{p}|\mathbf{S}=1)}=\alpha_{2}\mathcal{T}_{(X_{p}|\mathbf{S}= 2)}=\cdots\mathcal{T}_{(X_{p}|\mathbf{S}=r)}\).

If \(X_{p}\) has a parent variable \(L_{p}\) in the causal graph, by Lemma.D.1, the equality does not hold. Thus, \(\mathcal{T}_{(X_{1}\cdots X_{n}|\mathbf{L}=j)}\) is not a rank-one tensor.

If \(X_{p}\) is root node in the causal graph, and \(\mathbf{S}\) is conditional set that d-separates \(X_{p}\) and \(\{X_{1},\cdots X_{n}\}\setminus\{X_{p}\}\), we have \(\mathcal{T}_{(X_{p}|\mathbf{S})}\) is full rank. The reason is the following.

Since \(\mathbb{P}(X_{p}|\mathbf{S})\mathbb{P}(\mathbf{S})=\mathbb{P}(\mathbf{S}|X_{p}) \mathbb{P}(X_{p})\) by Bayes' theorem Koch and Koch (1990), and due to all marginal probabilities are not zero, then we have \(\mathcal{T}_{(X_{p}|\mathbf{S})}\) = \(\mathcal{T}_{(\mathbf{S}|X_{p})}\mathrm{Diag}(\mathcal{T}_{(X_{p})})\mathrm{ Diag}(\mathcal{T}_{(\mathbf{S})})^{\dagger}\), where \(\dagger\) is inverse of matrix, and \(X_{p}\) is ancestor of \(\mathbf{S}\). By Lemma.D.2, \(\mathbf{S}\) has a parent variables and hence \(\mathcal{T}_{(\mathbf{S}|X_{p})}\) is full rank (one can vectorize the variable set \(\mathbf{S}\) as a variable). Now, we have \(\mathcal{T}_{(X_{p}|\mathbf{S})}\) is full rank due to the three matrices on the right are all full rank.

Thus, \(\mathcal{T}_{(X_{p}|\mathbf{S}=1)}\) = \(\alpha_{2}\mathcal{T}_{(X_{p}|\mathbf{S}=2)}\) = \(\cdots\mathcal{T}_{(X_{p}|\mathbf{S}=r)}\) does not hold, i.e., \(\mathcal{T}_{(X_{1}\cdots X_{n}|\tilde{\mathbf{S}}=j)}\) is not a rank-one tensor.

**Lemma D.4**.: _In a discrete causal graph, for the variable \(X_{p}\), let \(L_{p}\) represent the vectorized parent set of \(X_{p}\), and let \(\mathrm{Des}_{X_{p}}\) denote the set of descendant variables of \(X_{p}\). Under this setup, \(\mathcal{T}_{(X_{p}|L_{p},\mathrm{Des}_{X_{p}}=j)}\) is full rank._Proof.: By Bayes' theorem Koch and Koch (1990), we have

\[\begin{split}&\mathcal{T}_{(X_{p},L_{p}|\mathrm{Des}_{X_{p}}=j)}\\ &=\mathcal{T}_{(X_{p}|L_{p},\mathrm{Des}_{X_{p}}=j)}\mathrm{Diag}( \mathcal{T}_{(L_{p}|Des_{X_{p}}=j)})\\ &=\mathcal{T}_{(L_{p}|X_{p},\mathrm{Des}_{X_{p}}=j)}\mathrm{Diag}( \mathcal{T}_{(X_{p}|\mathrm{Des}_{X_{p}}=j)}).\end{split} \tag{16}\]

Since \(X_{p}\) and \(\mathrm{Des}_{X_{p}}\) both are the descendant set of \(L_{p}\), we have

\[\mathcal{T}_{(L_{p}|\mathrm{Des}_{L_{p}})}=\mathcal{T}_{(\mathrm{Des}_{L_{p}}| L_{p})}\mathrm{Diag}(\mathcal{T}_{(\mathrm{L_{p}})}\mathrm{Diag}(\mathcal{T}_{( \mathrm{Des}_{L_{p}})}^{\dagger}, \tag{17}\]

due to the fact that all marginal distribution probabilities are not zero. Then \(\mathcal{T}_{(L_{p}|\mathrm{Des}_{L_{p}})}\) is full rank, i.e., \(\mathcal{T}_{(L_{p}|X_{p},\mathrm{Des}_{X_{p}}=j)}\) also full rank.

Moreover, for any \(\mathcal{T}_{(L_{p}=i|\mathrm{Des}=j)}\), we have

\[\mathcal{T}_{(L_{p}=i|\mathrm{Des}_{X_{p}}=j)}=\frac{\mathcal{T}_{(L_{p}=i, \mathrm{Des}_{X_{p}}=j)}}{\mathcal{T}_{(\mathrm{Des}_{X_{p}}=j)}}\neq 0, \tag{18}\]

because all marginal distribution probabilities are not zero. Thus, we have

\[\begin{split}&\mathcal{T}_{(X_{p},L_{p}|\mathrm{Des}_{X_{p}}=j)}\\ &=\mathcal{T}_{(L_{p}|X_{p},\mathrm{Des}_{X_{p}}=j)}\mathrm{Diag} (\mathcal{T}_{(X_{p}|\mathrm{Des}_{X_{p}}=j)})\mathrm{Diag}(\mathcal{T}_{(L_{ p}|\mathrm{Des}_{X_{p}}=j)})^{\dagger}.\end{split} \tag{19}\]

One can see that \(\mathcal{T}_{(X_{p},L_{p}|\mathrm{Des}_{X_{p}}=j)}\) is full rank due to three matrices on the right side being full rank.

### Proof of Proposition 4.2

Proof.: The proof is straightforward. In the discrete 3PLSM model, suppose all latent variable has the same state space. Any two observed are d-separated by any one of their latent parents. According to the graphical criteria, the rank of tensor \(\mathcal{T}_{(X_{i}X_{j})}\) is the dimension of latent support.

### Proof of Proposition 4.3

Proof.: **Proof of \(\mathcal{R}\)ule 1:** In the discrete 3PLSM and suppose the assumption 2.2 - assumption 2.4 holds, if there does not exist a latent variable \(L_{1}\) that d-separates any pair variables in \(\{X_{i},X_{j},X_{k}\}\), i.e., the rank of tensor \(\mathcal{T}_{(X_{i}X_{j}X_{k})}\) is not \(r\) (by Theorem 1), it must be the full-connection structure among latent variables, as shown in Fig. 4 (d). Otherwise, one can find one latent variable \(L_{1}\) that can d-separates \(\{X_{i},X_{j},X_{k}\}\), as shown in Fig. 4 (a) \(\sim\) (c). We will show that, if only consider one of the latent variables of \(\{L_{1},L_{2},L_{3}\}\) in Fig. 4 (d), the tensor of \(\mathcal{T}_{(X_{i}X_{j}X_{k})}\) can not have rank-one decomposition.

According to the graphical criteria of tensor rank and \(r<d_{i}\) in the definition of discrete LSM, and suppose all latent variables have the same state space, for \(X_{i}\), \(X_{j}\) and \(X_{k}\), there is not only one latent

Figure 4: Illustrative example for \(\mathcal{R}\)ule 1.

variable is conditional set, i.e., but there also is not one latent variable \(L\) that d-separates any pair variables in \(\{X_{i},X_{j},X_{k}\}\). Thus, \(\mathrm{Rank}(\mathcal{T}_{(X_{i}X_{j}X_{k})})\neq r\).

**Proof of \(\mathcal{R}\)ule 2:**

We aim to show if \(\mathrm{Rank}(\mathcal{T}_{(X_{i}X_{j}X_{k}X_{s})})=r\) for any \(X_{s}\in\mathbf{X}\setminus\{X_{i},X_{j},X_{k}\}\) then there exists a latent variable \(L_{p}\) that d-separates \(\{X_{i},...,X_{s}\}\) and \(L_{p}\) is the parent variable of \(\{X_{i},X_{j},X_{k}\}\) in the discrete 3PLSM. We first prove it by the contradiction. If \(\{X_{i},X_{j},X_{k}\}\) does not share one common latent parent, e.g., \(L_{1}\xrightarrow{}\{X_{i},X_{j}\}\) and \(L_{2}\xrightarrow{}\{X_{k}\}\), due to the structure assumption in discrete 3PLSM, there exist \(X_{s}\in Ch_{L_{2}}\), as shown in Fig. 5.

By the graphical criteria of tensor rank condition and \(r<d_{i}\), one has \(\mathrm{Rank}(\mathcal{T}_{(X_{i}X_{j}X_{k}X_{s})})\neq r\) since \(L_{1}\) or \(L_{2}\) is not the conditional set that d-separates any pair variable of \(\{X_{i},X_{j},X_{k},X_{s}\}\) (e.g., \(X_{k},X_{s}\) can not be d-separates given \(L_{1}\)), which is contrary to the condition \(\mathrm{Rank}(\mathcal{T}_{(X_{i}X_{j}X_{k}X_{s})})=r\).

### Proof of Proposition 4.5

Proof.: Since \(C_{1}\) and \(C_{2}\) are two causal clusters, then the elements in \(C_{1}\) have only one common latent variable. Without loss of generality, we let \(L_{1}\) denote the parental latent variable of \(C_{1}\). Similarly, \(L_{2}\) denotes the parental latent variable of \(C_{2}\). Since \(C_{1}\) and \(C_{2}\) are overlapping, then they have at least one shared element. Let \(X_{i}\) denote the shared element of \(C_{1}\). then \(X_{i}\) has two latent parents \(L_{1}\) and \(L_{2}\), which contradicts with the pure child assumption in the discrete 3PLSM model. This finishes the proof. 

### Proof of Theorem 4.6

Proof.: Based on Prop. 4.3, one can identify the causal cluster by testing the tensor rank condition (Line 5 \(\sim\) 12). Moreover, the Prop. 4.5 ensures no redundant latent variables are introduced (Line 15). Thus, the causal cluster can be identified by Algorithm 1, under the discrete 3PLSM, with assumption 2.2 \(\sim\) assumption 2.4. 

### Proof of Theorem 4.7

Proof.: We first prove this result by a specific case and then extend it to a general case result.

**Proof by Specific case**

Figure 5: Illustrative example for \(\mathcal{R}\)ule 2.

Figure 6: d-separation and d-connection example.

'If' part: as shown in Fig.6, suppose all support of latent space is \(r\) and \(r\) < \(d_{i}\), \(d_{i}\) is the dimension of any support of observed variables. We will show that, for \(\{X_{1},...,X_{6}\}\), the rank of \(\mathcal{T}_{\{X_{1},...,X_{6}\}}\) are \(r^{2}\). Let \(\mathbf{L}\) be the vectorization of joint distribution \(\mathbb{P}(L_{3},L_{4})\) with \(|\mathrm{supp}(\mathbf{L})|=r^{2}\).

Since \(L_{2},L_{3}\) is a conditional set that d-separates all variables in \(\{X_{1},\cdots,X_{6}\}\) and there is no other conditional set with smaller support, according to the graphical criteria of tensor rank, \(\mathrm{Rank}(\mathcal{T}_{\{X_{1}\cdots X_{6}\}})=|\mathrm{supp}(\mathbf{L})|= r^{2}\).

'Only if' part: now, we consider the case that if \(L_{1}\) and \(L_{2}\) are d-connection (Fig.6). In this case, given \(\mathbb{P}\big{(}L_{3}L_{4}\big{)}\) (represented by \(\mathbb{P}(\mathbf{L})\)), \(\mathcal{T}_{\{X_{1}X_{2}|\mathbf{L}=i\}}\) cannot be decomposition as the outer product of two vectors since the contingency table of \(\mathcal{T}_{\{X_{1}X_{2}|\mathbf{L}=i\}}\) is not a rank-one tensor by Markov assumption. That is, \(\mathbf{L}\) is not a conditional set that d-separates \(X_{1}\) and \(X_{2}\).

According to the graphical criteria of tensor rank condition, one can infer that \(\mathrm{Rank}(\mathcal{T}_{\{X_{1}\cdots X_{6}\}})\neq|\mathrm{supp}(\mathbf{L} )|=r^{2}\). Based on the above analysis, one can see that the conditional independent relations hold if and only if the rank of the tensor equals the cardinality of support of the conditional set.

**Proof by general case**

'if' part: in the discrete 3PLSM model, assume that all latent variables have the same support. For any pair of variables \(X_{i}\) and \(X_{j}\) that are pure children of \(L_{k}\), it is evident that \(L_{k}\) is the only minimal conditional set that d-separates \(X_{i}\) from \(X_{j}\). Consider a set of latent variables \(\mathbf{L}_{p}\) and their corresponding child sets \(\mathbf{X}_{p1}\) and \(\mathbf{X}_{p2}\), where each latent variable has at least two child variables included in \(\mathbf{X}_{p1}\) and \(\mathbf{X}_{p2}\). The minimal conditional set that d-separates all variables in \(\mathbf{X}_{p1}\) from those in \(\mathbf{X}_{p2}\) is their common latent parent set \(\mathbf{L}_{p}\). Moreover, for two variables \(X_{i}\) and \(X_{j}\) that do not share a common latent parent, if \(\mathbf{L}_{p}\) d-separates \(X_{i}\) and \(X_{j}\), then \(\mathbf{L}_{p}\) also d-separates all variables in \(\mathbf{X}_{p1}\cup\mathbf{X}_{p2}\cup\{X_{i},X_{j}\}\). Since all latent variables have the same support, the minimal conditional set in the graph corresponds to the set with the minimal support. Therefore, by the graphical criteria of tensor rank, \(\mathrm{Rank}(\mathcal{T}(X_{i},X_{j},\mathbf{X}_{p1},\mathbf{X}_{p2}))=| \mathrm{supp}(\mathbf{L}_{p})|\). As all latent variables share the same state space, we deduce that \(|\mathrm{supp}(\mathbf{L}_{p})|=r^{|\mathbf{L}_{p}|}\).

'Only if' part: if \(X_{i}\) and \(X_{j}\) cannot be d-separated by \(\mathbf{L}_{p}\), then \(\mathbf{L}_{p}\) does not constitute a conditional set for \(\mathbf{X}_{p1}\cup\mathbf{X}_{p2}\cup\{X_{i},X_{j}\}\), under Assumptions 2.2\(\sim\) 2.4. According to the graphical criteria of tensor rank, \(\mathrm{Rank}(\mathbb{P}(X_{i},X_{j},\mathbf{X}_{p1},\mathbf{X}_{p2}))\neq| \mathrm{supp}(\mathbf{L}_{p})|\).

### Proof of Theorem 4.9

Proof.: Such an identification is derived from the original PC algorithm. By Theorem 4.7, given the measurement model, one can test the CI relations among latent variables, when \(r\) < \(d_{i}\) (remark 1). Thus, the causal structure among latent variables can be identified up to a Markov equivalent class by Algorithm 2 Spirtes et al. (2000). 

## Appendix E Extension of Different Latent State Space

To extend our theoretical result to the case in which the state space of the latent variable may be different (i.e., \(r_{i}\neq r_{j}\)). We present the minimal state space criteria by which the state space of the latent variable in the conditional set is identifiable.

**Theorem E.1** (Minimal state space criteria).: _In the discrete 3PLSM, suppose Assumption 2.2 \(\sim\) 2.4 holds. For any two observed variables \(X_{i}\) and \(X_{j}\), let \(L_{i}\) and \(L_{j}\) be their latent parent respectively, and let \(r_{1}\) be the cardinality of \(\mathrm{supp}(L_{i})\) and \(r_{2}\) be the cardinality of \(\mathrm{supp}(L_{j})\), there is \(\mathrm{Rank}(\mathcal{T}_{\{X_{1},X_{2}\}})=min(r_{1},r_{2})\)._

Proof.: In the discrete 3PLSM model, for two observed variables, we have \(r\) < \(x_{i}\) and any one of the latent parents can d-separate \(X_{i}\) from \(X_{i}\). Based on the graphical criteria of the tensor rank condition, one can see that the rank is the latent parent with minimal cardinality of support. 

Based on the minimal state space criteria, one can directly extend the identification of the discrete 3PLSM to the case where the cardinality of latent support may be different.

### Identification of causal cluster

**Proposition E.2** (Identification of causal cluster in different state space).: _In the discrete 3PLSM, suppose Assumption 2.2 \(\sim\) Assumption 2.4 holds. For three disjoint observed variables \(X_{i},X_{j},X_{k}\in\mathbf{X}\), then \(\{X_{i},X_{j},X_{k}\}\) share the same latent parent if \(\mathrm{Rank}(\mathcal{T}_{\{X_{i},X_{j},X_{k},X_{s}\}})=r\) for any \(X_{s}\in\mathbf{X}\setminus\{X_{i},X_{j},X_{k}\}\), where \(r=\mathrm{Rank}(\mathcal{T}_{\{X_{i},X_{j}\}})=\mathrm{Rank}(\mathcal{T}_{\{X _{i},X_{k}\}})=\mathrm{Rank}(\mathcal{T}_{\{X_{k},X_{j}\}})\)._

Proof.: If \(\mathrm{Rank}(\mathcal{T}_{\{X_{i},X_{j},X_{k},X_{s}\}})=r\), there exist a variable set \(\mathbf{S}\) with \(|\mathrm{supp}(\mathbf{S})|=r\) that d-separates any pair variables in \(\{X_{i},X_{j},X_{k},X_{s}\}\), according to the graphical criteria of tensor rank condition. In the discret LSM model and \(r<d_{i}\) (any support dimension of latent variable is less than the support dimension of observed variable), \(r=\mathrm{Rank}(\mathcal{T}_{\{X_{i},X_{j}\}})=\mathrm{Rank}(\mathcal{T}_{\{X _{i},X_{k}\}})=\mathrm{Rank}(\mathcal{T}_{\{X_{k},X_{j}\}})\), we have for any \(X_{i},X_{j}\in\{X_{i},X_{j},X_{k},X_{s}\}\), the conditional set is one of the latent parent of \(X_{i}\) and \(X_{j}\). If \(X_{i},X_{j}\) and \(X_{k}\) do not share the common latent parent, without loss of generality, let \(L_{1}\) be the parent of \(X_{i},X_{j}\) and \(L_{2}\) be the parent of \(X_{k}\), there exist the \(X_{s}\in Ch_{L_{2}}\) such that \(X_{k}\) and \(X_{s}\) cannot be d-separated given \(L_{1}\) or \(X_{i}\) and \(X_{j}\) cannot be d-separated given \(L_{2}\). By the graphical criteria of tensor rank condition, \(\mathrm{Rank}(\mathcal{T}_{\{X_{i},X_{j},X_{k},X_{s}\}})\neq|\mathrm{supp}(L_{1 })|\), or \(\mathrm{Rank}(\mathcal{T}_{\{X_{i},X_{j},X_{k},X_{s}\}})\neq|\mathrm{supp}(L_{2 })|\). Let \(r=\min(|\mathrm{supp}(L_{1})|,|\mathrm{supp}(L_{2})|)\), we have \(\mathrm{Rank}(\mathcal{T}_{\{X_{i},X_{j},X_{k},X_{s}\}})\neq r\) if \(X_{i},X_{j}\) and \(X_{j}\) are not a causal cluster. 

One can properly adjust the search algorithm such that the causal cluster can be identified, by the minimal state space criteria. The algorithm is presented as follows (Algorithm 3).

```
0: Data from a set of measured variables \(\mathbf{X}_{\mathcal{G}}\)
0: Causal cluster \(\mathcal{C}\)
1: Initialize the causal cluster set \(\mathcal{C}\coloneqq\varnothing\), and \(\mathcal{G}^{\prime}=\varnothing\);
2:Begin the recursive procedure
3:repeat
4:for each \(X_{i},X_{j}\) and \(X_{k}\in\mathbf{X}\)do
5:// Apply the minimal state space criteria
6: r = \(\mathrm{min}(\mathrm{Rank}(\mathcal{T}_{\{X_{i},X_{j}\}}),\mathrm{Rank}( \mathcal{T}_{\{X_{i},X_{k}\}}),\mathrm{Rank}(\mathcal{T}_{\{X_{k},X_{j}\}}))\) ;
7:if\(\mathrm{Rank}(\mathcal{T}_{\{X_{i},X_{j},X_{k},X_{s}\}})=r\), for all \(X_{s}\in\mathbf{X}\setminus\{X_{i},X_{j},X_{k}\}\)then
8:\(\mathbf{C}=\mathbf{C}\cup\{\{X_{i},X_{j},X_{k}\}\}\);
9:endif
10:endfor
11:until no causal cluster is found.
12:// Merging cluster and introducing latent variables
13: Merge all the overlapping sets in \(\mathbf{C}\) by Prop. 4.5.
14:for each \(C_{i}\in\mathbf{C}\)do
15: Introduce a latent variable\(L_{i}\) for \(C_{i}\);
16:\(\mathcal{G}=\mathcal{G}\cup\{L_{i}\to X_{j}|X_{j}\in C_{i}\}\).
17:endfor
18:return Graph \(\mathcal{G}\) and causal cluster \(\mathcal{C}\).
```

**Algorithm 3** Identifying the causal cluster (different latent space)

### Conditional independence test among latent variables

**Proposition E.3** (conditional independence among latent variables in different state space).: _In the discrete 3PLSM, suppose Assumption 2.2 \(\sim\) Assumption 2.4 holds. Let \(X_{i}\) and \(X_{j}\) be the pure child of \(L_{i}\) and \(L_{j}\) respectively, \(\mathbf{X}_{p1}\) and \(\mathbf{X}_{p2}\) be two disjoint child set of the latent set \(\mathbf{L}_{p}\) with \(|\mathbf{X}_{p1}|=|\mathbf{X}_{p2}|=|\mathbf{L}_{p}|\), then \(L_{i}\bot L_{j}|\mathbf{L}_{p}\) if and only if \(\mathrm{Rank}(\mathcal{T}_{\{X_{i},X_{j},\mathbf{X}_{p1},\mathbf{X}_{p2}\}})=r\), where \(r=\prod_{L_{i}\in\mathbf{L}_{p}}|\mathrm{supp}(L_{i})|\)._

Proof.: 'If' part: in the discrete 3PLSM, we have \(r_{i}<d_{j}\) for any \(i\in[k],j\in[p]\), where \(k\) is the number of latent variables while \(p\) is the number of observed variables. In the causal graph, for \(X_{q1}\in\mathbf{X}_{p1}\) and \(X_{q2}\in\mathbf{X}_{p2}\), \(X_{q1},X_{q2}\in Ch(L_{t})\) for \(\forall L_{t}\in\mathbf{L}_{p}\), we have \(L_{t}\) is the only conditional set that d-separates \(X_{q1}\) and \(X_{q2}\) with minimal support dimension. Thus, \(\mathbf{L}_{p}\) also bethe minimal conditional set that d-separates any pair variables in \(\mathbf{X}_{p1}\cup\mathbf{X}_{p2}\). Now, if \(X_{i}\) and \(X_{j}\) are d-separated by \(\mathbf{L}_{p}\), according to the graphical criteria of tensor rank condition, one have \(\mathrm{Rank}(\mathcal{T}_{(X_{i},X_{j},\mathbf{X}_{p1},\mathbf{X}_{p2})})=| \mathrm{supp}(\mathbf{L}_{p})|\). Since \(\mathbf{L}_{p}\) is the joint distribution of latent variable set, we have \(r=\prod_{L_{i}\in\mathbf{L}_{p}}|\mathrm{supp}(L_{i})|\).

'Only if' part: on the other hand, if \(X_{i}\) and \(X_{j}\) are not d-separated by \(\mathbf{L}_{p}\), for example, \(L_{i}\to L_{j}\) in the causal graph, then \(\mathbf{L}_{p}\) is not a conditional set that d-separates all pair variables in \(\{X_{i},X_{j},\mathbf{X}_{p1},\mathbf{X}_{p2}\}\). According to the graphical criteria of tensor rank condition, we have \(\mathrm{Rank}(\mathcal{T}_{(X_{i},X_{j},\mathbf{X}_{p1},\mathbf{X}_{p2})})\neq| \mathrm{supp}(\mathbf{L}_{p})|\), i.e., \(\mathrm{Rank}(\mathcal{T}_{(X_{i},X_{j},\mathbf{X}_{p1},\mathbf{X}_{p2})})\neq \prod_{L_{i}\in\mathbf{L}_{p}}|\mathrm{supp}(L_{i})|\). This completes the proof. 

In particular, \(|\mathrm{supp}(L_{i})|\) can be identified by their pure child variable, according to minimal state space criteria. An intuition illustration is by mapping the conditional set variable \(\mathbf{L}_{p}\) to one new latent variable \(\tilde{L}\) (i.e., vectorization), the graphical criteria of causal cluster still hold, e.g., \(\{\mathbf{X}_{p1},\mathbf{X}_{p2},X_{i}\}\) is a causal cluster that shares a common parent \(\tilde{L}\). However, such a map will exponentially increase the dimension of the latent variable support. One issue will be raised: the observed variable may have a smaller support dimension than the latent variables such that the d-separation relations among latent variables cannot be examined. Thus, it is necessary to study when and how the testability of d-separation holds. The result is provided in Remark. E.4.

**Remark E.4** (Testability of d-separation).: _For an n-way tensor \(\mathcal{T}_{\{X_{1},...,X_{n}\}}\) that is used to test the \(d\)-separation relations among latent variables, such a CI relation is testable if \(\prod_{j=1}^{|\mathbf{L}_{p}|}r_{j}^{|\mathbf{L}_{p}|}<\prod_{i=1}^{n}d_{i}- \text{max}(d_{1},...,d_{n})\)._

Proof.: We prove it by contradiction. If \(\prod_{j=1}^{|\mathbf{L}_{p}|}r_{j}^{|\mathbf{L}_{p}|}>\prod_{i=1}^{n}d_{i}- \text{max}(d_{1},...,d_{n})\), assume that \(d_{n}=\text{max}(d_{1},\cdots,d_{n})\) where \(d_{i}\) is the support of observed \(X_{i}\), there are

\[\mathcal{T}_{(X_{1}\cdots X_{n})}= \sum_{\mathbb{P}(X_{1}|X_{1}\cdots X_{n-1})}\mathbb{P}(X_{1}|X_{ 1}\cdots X_{n-1})\cdots\mathbb{P}(X_{n}|X_{1}\cdots X_{n-1})\mathbb{P}(X_{1} \cdots X_{n-1})\] \[= \sum_{i=1}^{\prod_{j=1}^{n-1}d_{j}}\mathbf{u}_{1}^{(i)}\otimes \cdots\otimes\mathbf{u}_{1}^{(i)}, \tag{20}\]

which is a smaller rank-one decomposition than \(\mathbf{L}_{p}\) with support \(r^{|\mathbf{L}_{p}|}\). According to the definition of tensor rank and the graphical criteria of tensor rank, we have \(\mathrm{Rank}(\mathcal{T}_{(X_{1}\cdots X_{n})})=\prod_{i=1}^{n}d_{i}-\text{ max}(d_{1},...,d_{n})\). That is, no matter whether the conditional independent relations hold given \(\mathbf{L}_{p}\), the rank of tensor still be the dimension of the support of \(\mathbb{P}(X_{1}\cdots X_{n-1})\). It means that the CI relations can not be detected.

In other words, if the support of the conditional set is more than the dimension of observed variables, then the minimal rank-one decomposition of the joint distribution will lead to \(\mathbb{P}(X_{1},\cdots,X_{n})=\sum_{\{X_{1},\cdots,X_{n-1}\}}\mathbb{P}(X_{1}, \cdots,X_{n}|X_{1},...,X_{n-1})P(X_{1},\cdots,X_{n-1})\). Thus, if the increasing of latent state space is less than the sum of tensor dimensions, the CI relations among latent variable are testable. Due to assuming that \(d>r\), the CI relations among latent variables are generally testable when the causal structure is sparse.

## Appendix F Discussion with the Hierarchical Structures

Actually, our result can be extended to a specific hierarchical structure, by constraining the structure of hidden variables. For instance, consider a hierarchical structure in which each latent variable is required to have at least three pure children (whether latent or observed) and one additional neighboring variable. An illustration of this type of structure is provided in Fig. 7. Assume that all latent variables have the same dimension of support, and that this dimension is smaller than that of the observed variables. Under these conditions, causal clusters at the bottom level can still be identified, as demonstrated by Proposition 2. For instance, the sets \(\{X_{1},X_{2},X_{3}\}\)\(\{X_{4},X_{5},X_{6}\}\), \(\{X_{7},X_{8},X_{9}\}\), and \(\{X_{10},X_{11},X_{12}\}\) are recognized as four distinct causal clusters. The pure measured variables from each cluster can act as surrogates for their corresponding latent parents, allowing the causal cluster learning procedure to be repeated. For example, if \(X_{1}\) serves as the surrogate for \(L_{2}\), and \(X_{4},X_{7},X_{10}\) as surrogates for \(L_{3},L_{4},L_{5}\), then \(\{L_{2},L_{3},L_{4},L_{5}\}\) can be identified as a cluster according to the graphical criteria of tensor rank. Thus, the specific hierarchical structure is identifiable by designing the proper search algorithm making use of the tensor rank condition. We will explore these results in future works.

## Appendix G Practical Estimation of Tensor Rank

Here, we describe the practical implementation of tensor rank estimation. To alleviate the problem of local optima during tensor decomposition, we initiate the process from multiple random starting points. We then perform the tensor decomposition from each of these points and subsequently select the decomposition that yields the smallest reconstruction error as our final result. The procedure is summarised in the Algorithm 4.

```
0:\(\mathbf{X}_{p}\) = \(\{X_{i},X_{j},...,X_{n}\}\), iteration number \(n\), threshold \(\varepsilon_{r}\) and tested rank \(r\)
0: Boolean of rank test
1: Initialize the minimal reconstructed error \(E_{min}\) = + \(\infty\);
2:for\(i<n\) or \(E_{min}\leq\varepsilon\)do
3:\(i\gets i+1\);
4:\(\tilde{\mathcal{T}}\leftarrow\) non-negative-parafac(\(\mathcal{T}_{(\mathbf{X}_{p})},r\));
5: E \(\leftarrow\)\(\|\mathcal{T}_{(\mathbf{X}_{p})}-\tilde{\mathcal{T}}\)\(\|\);
6:if\(E\leq E_{min}\)then
7:\(E_{min}=E\);
8:endif
9:endfor
10: p-val \(\leftarrow\) Chi-Square(\(\mathcal{X}^{2}\))-Test(vec(\(\mathcal{T}\)), vec(\(\tilde{\mathcal{T}}\)));
11:if p-val \(<\varepsilon\)then
12:return True;
13:endif
14:return False.
```

**Algorithm 4** Practical tensor rank estimation

Besides, in the PC-TENSOR-RANK algorithm, to further identify the V-structure among latent variables, the statistic-independent test among latent variables is required, which can be tested by following.

**Remark G.1** (Statistic independent between latent variables).: _Give the measured variable \(X_{i}\) and \(X_{j}\) of latent variable \(L_{i}\) and \(L_{j}\), then \(L_{i}\)\(\mathbbm{1}\)\(L_{j}\) if \(\text{Rank}(\mathbb{P}(X_{i},X_{j}))\) = \(1\)._

Proof.: Since \(L_{i}\)\(\mathbbm{1}\)\(L_{j}\), we have \(X_{i}\)\(\mathbbm{1}\)\(X_{j}\) also hold in the causal graph. We have \(\mathbb{P}(X_{i}X_{j})\) = \(\mathcal{P}(X_{i})\mathbb{P}(X_{j})\) = \(\mathcal{T}_{(X_{i})}\otimes\mathcal{T}_{(X_{j})}\). According to the definition of tensor rank, \(\text{Rank}(\mathcal{T}_{(X_{i},X_{j})})\) = \(1\). 

### Goodness of fit test for CI test among latent variables

Although the proposed tool is theoretically testable, it still is an approximate estimation of tensor rank by heuristic-based CP decomposition in practice. How to consider a more robust approach to examine the tensor rank still be an open problem in the related literature. It significantly restricts

Figure 7: Example of hierarchical structure.

the application scope and performance of our structure learning algorithm. However, we want to emphasize that the main contribution of our work is building the graphical criteria of tensor rank and using it to answer the identification of causal structure in a discrete 3PLSM model.

Next, we will show how the CI relations among latent variables can be distinguished by testing the goodness of fit test. Consider a four latent variables structure as shown in Fig. 8, a chain structure among four latent variables in which each latent variable has two pure observed variables. The data generation process follows the discrete 3PLSM model (see the description in the simulation studies section) and the sample size is \(50\)k. We check the CI relations between any \(L_{i},L_{j}\in\{L_{1},L_{2},L_{3},L_{4}\}\) given \(L_{p}\in\{L_{1},L_{2},L_{3},L_{4}\}\setminus\{L_{i},L_{j}\}\). The results are reported on the right side of Fig. 8, in which each red point represents a CI test result, e.g., the second point in the graph represents to test \(L_{2}\ \mathbb{L}_{4}\big{|}\ L_{1}\big{|}\ L_{1}\) by examining \(\mathrm{Rank}(\mathcal{T}_{X_{3},X_{7},X_{1},X_{2}})=2\). One can see that the p-value returned by the goodness of fit test is lower than 0.05, which means that we will tend to reject the null hypothesis, i.e., \(\mathrm{Rank}(\mathcal{T}_{\{X_{3},X_{7},X_{1},X_{2}\}})\neq 2\). By sorting all CI test results, one can see that the true CI relations can be identified by setting the significant level to be 0.05.

## Appendix H More Experimental Results

In this section, we provide the information required to reproduce our results reported in the main text. We further conduct additional simulation experiments to validate the efficiency of the proposed algorithm (Appendix E).

We first give a details definition of evaluation metrics. Specifically, the performance of causal cluster is evaluated by following scores for the output model \(G_{out}\) from each algorithm, where the true graph is labelled \(G\):

* **latent omission**, the number of latents in \(G\) that do not appear in \(G_{out}\) divided by the total number of true latents in \(G\);
* **latent commission**, the number of latents in \(G_{out}\) that could not be mapped to a latent in \(G\) divided by the total number of true latents in \(G\);
* **mismeasurement**, the number of observed variables in \(G_{out}\) that are measuring at least one wrong latent divided by the number of observed variables in \(G\);

Moreover, we use the following metric to evaluate the performance of causal structure among latent variables:

* **edge omission (EO)**, the number of edges in the structural model of \(G\) that do not appear in \(G_{out}\) divided by the possible number of edge omissions;
* **edge commission (EC)**, the number of edges in the structural model of \(G_{out}\) that do not exist in \(G\) divided by the possible number of edge commissions;
* **orientation omission (OO)**, the number of arrows in the structural model of \(G\) that do not appear in \(G_{out}\) divided by the possible number of orientation omissions in \(G\);

Figure 8: Goodness of fit test for conditional independent test among latent variables

These evaluation indicators are derived from Silva et al. (2006).

Next, we give a concrete implementation of baseline methods.

**BayPy**: The Bayesian Pyramid Mode (BayPy) is a discrete latent variable structure learning method that assumes the latent structure is a pyramid structure and the latent variable is binary. We use the implementation of Gu and Dunson (2023). We set the iteration parameter to 1500 and set the search upper bound of the number of latent variables to 5.

**LTM**: The latent tree model, is a classic method for learning gaussian or binary latent tree structure. We use the implementation from Choi et al. (2011). Specifically, we use the Recursive Grouping (RG) Algorithm in Choi et al. (2011) (since it has better performance), and use the discrete information distance to learn the structure of the discrete 3PLSM model.

**BPC**: The Building Pure Cluster (BPC) algorithm Silva et al. (2006) is a classic causal discovery method for the linear latent variable model. We use the implementation from the Tetrad Project package 3.

Footnote 3: [https://github.com/cmu-phil/tetrad](https://github.com/cmu-phil/tetrad)

**Non-negative CP decomposition**: To perform non-negative CP decomposition in our algorithm, we use the implementation from the python package, _tensorly_4, and set the maximum iteration parameter to 1000, the cvg criterion parameter to "rec_error".

Footnote 4: [https://github.com/tensorly/tensorly](https://github.com/tensorly/tensorly)

**Data Generation**: To generate the probability contingency table or conditional probability contingency table for latent variables and observed variables in our simulation studies, we use the implementation from the python package, _pgmpy_5. The package provides the function to generate observed data according to the probability contingency table.

Footnote 5: [https://github.com/pgmpy/pgmpy](https://github.com/pgmpy/pgmpy)

Finally, we aim to demonstrate the correctness of our methods in handling cases involving latent variables with varying state spaces. Specifically, we consider the structure model with star structure and the \(SM_{3}\) structure, and the measurement model with \(MM_{1}\). The data generation process follows the description in the main context and we let the support of latent variable \(L_{1},L_{2}\) to be \(\{0,1\}\) and the support of latent variable \(L_{3},L_{4}\) to be \(\{0,1,2\}\), for simulating the case that latent variable has different latent support. Besides, the support of the observed variable is \(\{0,1,2,3\}\). In our implementation, the significant levels for testing the rank of the matrix and tensor are set to 0.005 and 0.05, respectively. The coefficient of probability contingency tables is generated randomly, ranging from \([0.1,0.8]\), constraining the sum of them to be one.

The results are presented in Table 4 and Table 5. The performance of our method appears superior in scenarios where latent variables have differing state spaces. This improvement is attributed to the reduction in the frequency of higher-order tensor rank testing, facilitated by evaluating the consistency of ranks such as \(\mathrm{Rank}(\mathcal{T}(X_{i},X_{j}))=\mathrm{Rank}(\mathcal{T}(X_{i},X_{k})) =\mathrm{Rank}(\mathcal{T}_{(X_{k},X_{j})})\). In contrast, the BayPy approach underperforms in latent structure learning, likely due to its assumption of a pyramid structure with a top-down directionality and no peer-level connections among latent variables. Additionally, the Latent Tree Model (LTM) shows weaker performance in cluster learning, possibly because it was originally designed to handle only binary discrete variables.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{**Latent emission**} & \multicolumn{4}{c}{**Latent emission**} & \multicolumn{4}{c}{**Misenseusements**} \\ \cline{3-13} Algorithm & **Our** & BayPy & LTM & BPC & **Our** & BayPy & LTM & BPC & **Our** & BayPy & LTM & BPC \\ \hline
5k & 0.09(3) & 0.20(4) & 0.23(25) & 0.96(10) & 0.00(0.00) & 0.20(4) & 0.00(0) & 0.00(0) & 0.03(1) & 0.18(4) & 0.23(5) & 0.00(0) \\ \(Star+MM_{1}\) & 10k & 0.06(2) & 0.17(3) & 0.13(4) & 0.96(10) & 0.00(0.00) & 0.

## Appendix I Experimental Results on Real-world Dataset

We first briefly present the results from two real datasets. The first is the political efficacy dataset, collected by Aish and Joreskog (1990) through a cross-national survey designed to capture information on both conventional and unconventional forms of political participation in industrial societies. This dataset includes 1719 cases obtained in a USA sample. The second dataset, referred to as the depress dataset, is detailed by Joreskog and Sorbom (1996) and comprises twelve observed variables grouped into three latent factors: self-esteem, depression, and impulsiveness, with a total of 204 samples. Our algorithm learns the correct causal structure (including the measurement model and the structure model) for both datasets by first identifying the dimension of latent support as two in the political efficacy dataset and four in the depress dataset.

For the political efficacy data Aish and Joreskog (1990), by identifying the support of latent variable to be two, one can identify the causal cluster {'NOCARE', 'TOUCH', 'INTEREST'}, {'NOSAY', 'VOTING', 'COMPLEX'}. These clusters correspond to the latent variables 'EFFI-CACY' and 'RESPONSE', respectively. In our implementation, we set the significance level to 0.0015. The result is aligned with the ground truth provided in Joreskog and Sorbom (1996).

For the depress dataset, the ground truth structure Joreskog and Sorbom (1996) includes three latent factors: Self-esteem, Depression, and Impulsiveness, with the corresponding observed clusters:

* {'SELF1', 'SELF2', 'SELF3', 'SELF4', 'SELF5'};
* {'DEPRES1', 'DEPRES2', 'DEPRES3', 'DEPRES4'};
* {'IMPULS1', 'IMPULS2', 'IMPULS3'}.

In our implementation, we utilize a bootstrapping resampling approach to enhance the statistical properties of the data. Following the extended results outlined in Appendix E, we first identify the dimension of support for the factors Self-esteem and Depression as four, and set the dimension of support for Impulsiveness at three. The significance level is set to 0.025. The results of our algorithm are presented as follows.

* {'SELF1', 'SELF2', 'SELF3', 'SELF5'};
* {'DEPRES1', 'DEPRES3', 'DEPRES4'};
* {'IMPULS1', 'IMPULS2', 'IMPULS3'}.

One can see that our algorithm can learn three causal clusters corresponding to three latent factors. Such a result shows that our method finds all latent variables from the depress data. In the latent structure learning process, the PC-TENSOR-RANK algorithm outputs the fully connected graph of the three latent factors, indicating the absence of conditional independence (CI) relations between them. One possible reason is there may be more potential factor interaction structure Salles et al. (2024).

## Appendix J Broader Impacts

The proposed _Tensor Rank Condition_ identifies the causal structure of latent variables in the discrete LSM model. Our theoretical results extend the identification bounds of causal discovery with latent variables and expand the application scope of latent variable models. For instance, in psychology and social sciences, our algorithm can learn the causal structure of the latent factors of interest from

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{**Edge emission**} & \multicolumn{4}{c}{**Edge emission**} & \multicolumn{4}{c}{**Orientation emission**} \\ Algorithm & **Our** & **BuyP** & **LSTM** & **BPC** & **Our** & **BuyP** & **LSTM** & **BPC** & **Our** & BuyP** & **LSTM** & **BPC** \\ \hline \multirow{2}{*}{_Star\(\leftrightarrow\)M\({}_{1}\)_} & 5k & 0.00(0) & 1.00(0) & 0.26(8) & 1.00(0) & 0.10(1) & 0.00(0) & 0.00(0) & 0.00(0) & 0.10(1) & 1.00(10) & – & 1.00(0) \\  & 10k & 0.00(0) & 1.00(10) & 0.23(6) & 1.00(10) & 0.00(0) & 0.02(1) &observed data (e.g., survey data), enabling researchers to better understand the causal mechanisms behind them. Additionally, based on the discovered causal relationships, one can design more effective intervention strategies to improve mental health or stimulate the market economy.

## Appendix K Limitation

The proposed method can hardly be applied to the high-dimensional discrete data. Therefore, how to relax this restriction and make it scalable to high-dimensional real-world datasets would be a meaningful future direction.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We emphasize our contributions in the abstract and introduction.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations of our works in Appendix.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have provided the full set of assumptions and a complete proof.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided the implementation details in Appendix.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: We have provided the implementation details in Appendix.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have provided the training details in the simulation study section.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We conduct the experiments over three different random seeds.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: We do not provide the information on the computer resources.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?)Answer: [Yes] Justification: The datasets, that we used in the paper are properly cited.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have provided the broader impacts in the appendix.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: the paper does not pose such risk.
12. **License for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The datasets, that we used in the paper are properly cited.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We have provided the synthetic dataset.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.