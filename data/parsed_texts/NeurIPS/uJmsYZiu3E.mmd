# Fair Allocation of Indivisible Chores:

Beyond Additive Costs

 Bo Li\({}^{1}\)  Fangxiao Wang\({}^{1}\)  Yu Zhou\({}^{1}\)

\({}^{1}\)Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China

comp-bo.li@polyu.edu.hk,

fangxiao.wang@connect.polyu.hk, csyzhou@comp.polyu.edu.hk

###### Abstract

We study the maximin share (MMS) fair allocation of \(m\) indivisible chores to \(n\) agents who have costs for completing the assigned chores. It is known that exact MMS fairness cannot be guaranteed, and so far the best-known approximation for additive cost functions is \(\frac{13}{11}\) by Huang and Segal-Halevi [EC, 2023]; however, beyond additivity, very little is known. In this work, we first prove that no algorithm can ensure better than \(\min\{n,\frac{\log m}{\log\log m}\}\)-approximation if the cost functions are submodular. This result also shows a sharp contrast with the allocation of goods where constant approximations exist as shown by Barman and Krishnamurthy [TEAC, 2020] and Ghodsi et al. [AIJ, 2022]. We then prove that for subadditive costs, there always exists an allocation that is \(\min\{n,\lceil\log m\rceil\}\)-approximation, and thus the approximation ratio is asymptotically tight. Besides multiplicative approximation, we also consider the ordinal relaxation, 1-out-of-\(d\) MMS, which was recently proposed by Hosseini et al. [JAIR and AAMAS, 2022]. Our impossibility result implies that for any \(d\geq 2\), a 1-out-of-\(d\) MMS allocation may not exist. Due to these hardness results for general subadditive costs, we turn to studying two specific subadditive costs, namely, bin packing and job scheduling. For both settings, we show that constant approximate allocations exist for both multiplicative and ordinal relaxations of MMS.

## 1 Introduction

### Background and related research

Although fair resource allocation has been widely studied in the past decade, the research is centered around additive functions [3]. However, in many real-world scenarios, the functions are more complicated. Particularly, the functions appear in machine learning and artificial intelligence (e.g, clustering [37], Sketches [18], Coresets [36], data distillation [41]) are often submodular, and we refer the readers to a recent survey that reviews some major problems within machine learning that have been touched by submodularity [12]. Therefore, in this work, we study the fair allocation problem when the agents have beyond additive cost functions. The mainly studied solution concept is maximin share (MMS) fairness [14], which is traditionally defined for the allocation of goods as a relaxation of proportionality (PROP). A PROP allocation requires that the utility of each agent is no smaller than the average utility when all items are allocated to her. PROP is too demanding in the sense that such an allocation does not exist even when there is a single item and two agents. Due to this strong impossibility result, the maximin share (MMS) of an agent is proposed to relax the average utility by the maximum utility the agent can guarantee herself if she is to partition the items into \(n\) bundles but is to receive the least favorite bundle.

For the allocation of goods, although MMS significantly weakens the fairness requirement, it was first shown by Kurokawa et al. [31; 32] that there are instances where no allocation is MMS fair for all agents. Accordingly, designing (efficient) algorithms to compute approximately MMS fair allocations steps into the center of the field of algorithmic fair allocation. Kurokawa et al. [32] first proved that there exists a \(\frac{2}{3}\)-approximate MMS fair allocation for additive utilities, and then Amanatidis et al. [1] designed a polynomial time algorithm with the same approximation guarantee. Later, Ghodsi et al. [23] improved the approximation ratio to \(\frac{3}{4}\), and Garg and Taki [22] further improved it to \(\frac{3}{4}+o(1)\). On the negative side, Feige et al. [20] proved that no algorithm can ensure better than \(\frac{39}{40}\) approximation. Beyond additive utilities, Barman and Krishnamurthy [6] initiated the study of approximately MMS fair allocation with submodular utilities, and proved that a \(0.21\)-approximate MMS fair allocation can be computed by the round-robin algorithm. Ghodsi et al. [24] improved the approximation ratio to \(\frac{1}{3}\), and moreover, they gave constant and logarithmic approximation guarantees for XOS and subadditive utilities, respectively. The approximations for XOS and subadditive utilities are recently improved by Seddighin and Seddighin [40]. There are also several works that delve into concrete combinatorial problems and seek to improve approximation ratios compared to more general functions. Li et al. [33] and Hummel and Hettland [30] introduced interval scheduling and independent set structures, respectively, into MMS fair allocation problems. In both cases, the induced utility functions correspond to special cases of XoS functions. Both of these two works enhanced the approximation ratios for their specific utility functions. Some other combinatorial problems that have been studied for goods include the knapsack problem [21; 9; 8] and matroid constraints [19].

For the parallel problem of chores, where agents need to spend costs on completing the assigned chores, less effort has been devoted. Aziz et al. [4] first proved that the round-robin algorithm ensures 2-approximation for additive costs. Barman and Krishnamurthy [6], Huang and Lu [28] and Huang and Segal-Halei [29] respectively improved the approximation ratio to \(\frac{4}{3}\), \(\frac{11}{9}\) and \(\frac{13}{11}\). Recently, Feige et al. [20] proved that with additive costs, no algorithm can be better than \(\frac{44}{43}\)-approximate. However, except a recent work that studies binary supermodular costs [10], very little is known beyond additivity.

Besides multiplicative approximations, we also consider the ordinal approximation of MMS, namely, \(1\)-out-of-\(d\) MMS fairness which is recently studied in [5; 26; 27], and proportionality up to any item (PROPX) which is an alternative way to relax proportionality [38; 34]. For indivisible chores, Hosseini et al. [26] and Li et al. [34] respectively proved the existence of \(1\)-out-of-\(\lceil\frac{3n}{4}\rceil\) MMS fair and exact PROPX allocations. All the above works also assume additive costs. More works related to this paper in the literature can be seen in Appendix A.

### Main results

In this work, we aim at understanding the extent to which MMS fairness can be satisfied when the cost functions are beyond additive. In a sharp contrast to the allocation of goods, we first show that no algorithm can ensure better than \(\min\{n,\frac{\log m}{\log\log m}\}\)-approximation when the cost functions are submodular.1 Further, we show that for general subadditive cost functions, there always exists an allocation that is \(\min\{n,\log m\}\)-approximate MMS, and thus the approximation ratio is asymptotically tight. Next, we consider the ordinal relaxation, 1-out-of-\(d\) MMS. It is trivial that 1-out-of-1 MMS is satisfied no matter how the items are allocated, and somewhat surprisingly, our impossibility result implies that for any \(d\geq 2\), there is an instance for which no allocation is 1-out-of-\(d\) MMS.

Footnote 1: In this paper we use \(\log(\cdot)\) to denote \(\log_{2}(\cdot)\).

**Result 1** For general subadditive cost functions, the asymptotically tight multiplicative approximation ratio of MMS is \(\min\{n,\log m\}\). Further, for any \(d\geq 2\), a 1-out-of-\(d\) MMS allocation may not exist.

Result 1 combines Theorems 1, 2 and Corollary 1. The strong impossibility in Result 1 does not rule out the possibility of constant multiplicative or ordinal approximation of MMS fair allocations for specific subadditive costs. For this reason, we turn to studying two concrete settings with subadditive costs. The first setting encodes a bin packing problem, which has applications in many areas (e.g., semiconductor chip design, loading vehicles with weight capacity limits, and filling containers [17]). In the first setting, the items have sizes which can be different to different agents. The agents have bins that can be used to pack the items allocated to them with the goal of using as few bins as possible. The second setting encodes a job scheduling problem, which appears in many research areas, including data science, big data, high-performance computing, and cloud computing [25]. In the second setting, the items are jobs that need to be processed by the agents. Each job may be of different lengths to different agents and each agent controls a set of machines with possibly different speeds. Upon receiving a set of jobs, an agent's cost is determined by the corresponding minimum completion time when processing the jobs using her own machines (i.e., makespan). As will be clear, job scheduling setting is more general than the additive cost setting. Besides, it uncovers new research directions for group-wise fairness.

**Result 2** For the bin packing and job scheduling settings, a 1-out-of-\(\lfloor\frac{n}{2}\rfloor\) MMS allocation and a 2-approximate MMS allocation always exist.

Result 2 combines Theorems 3, 4 and Corollaries 2, 3. Besides studying MMS fairness, in Appendix F, we also prove hardness results for two other relaxations of proportionality, i.e., PROP1 and PROPX.

## 2 Preliminaries

For any integer \(k\geq 1\), let \([k]=\{1,\ldots,k\}\). In a fair allocation instance \(I=(N,M,\{v_{i}\}_{i\in N})\), there are \(n\) agents denoted by \(N=[n]\) and \(m\) items denoted by \(M=\{e_{1},\ldots,e_{m}\}\). Each agent \(i\in N\) has a cost function over the items, \(v_{i}:2^{M}\rightarrow\mathbb{R}^{+}\cup\{0\}\). Note that for simplicity, we abuse \(v_{i}(\cdot)\) to denote a cost function. The items are chores, and particularly, upon receiving a set of items \(S\subseteq M\), \(v_{i}(S)\) represents the effort or cost agent \(i\) needs to spend on completing the chores in \(S\). The cost functions are normalized and monotone, i.e., \(v_{i}(\emptyset)=0\) and \(v_{i}(S_{1})\leq v_{i}(S_{2})\) for any \(S_{1}\subseteq S_{2}\subseteq M\). Note that no bounded approximation can be achieved for general cost functions, and we provide one such example in Appendix B.1. Thus we restrict our attention to the following three classes. A cost function \(v_{i}\) is subadditive if for any \(S_{1},S_{2}\subseteq M\), \(v_{i}(S_{1}\cup S_{2})\leq v_{i}(S_{1})+v_{i}(S_{2})\). It is submodular if for any \(S_{1}\subseteq S_{2}\subseteq M\) and any \(e\in M\setminus S_{2}\), \(v_{i}(S_{2}\cup\{e\})-v_{i}(S_{2})\leq v_{i}(S_{1}\cup\{e\})-v_{i}(S_{1})\). It is additive if for any \(S\subseteq M\), \(v_{i}(S)=\sum_{e\in S}v_{i}(\{e\})\). It is widely known that any additive function is also submodular, and any submodular function is also subadditive.

An allocation \(\mathbf{A}=(A_{1},\ldots,A_{n})\) is an \(n\)-partition of the items where \(A_{i}\) contains the items allocated to agent \(i\) such that \(A_{i}\cap A_{j}=\emptyset\) for any \(i\neq j\) and \(\bigcup_{i\in N}A_{i}=M\). For any set \(S\) and integer \(d\), let \(\Pi_{d}(S)\) be the set of all \(d\)-partitions of \(S\). The maximin share (MMS) of agent \(i\) is

\[\mathsf{MMS}_{i}^{n}(I)=\min_{(X_{1},\ldots,X_{n})\in\Pi_{n}(M)}\max_{j\in[n]} v_{i}(X_{j}).\]

Note that we may neglect \(n\) and \(I\) in \(\mathsf{MMS}_{i}^{n}(I)\) when there is no ambiguity. Note that the computation of MMS is NP-hard even when the costs are additive, which can be verified by a reduction from the Partition problem. Given an \(n\)-partition of \(M\), \(\mathbf{X}=(X_{1},\ldots,X_{n})\), if \(v_{i}(X_{j})\leq\mathsf{MMS}_{i}\) for any \(j\in[n]\), then \(\mathbf{X}\) is called an _MMS defining partition_ for agent \(i\). Note that the original definition of \(\mathsf{MMS}_{i}\) for chores is defined with non-positive values, where the minimum value of the bundles is maximized. In this work, to simplify the notions, we choose to use non-negative numbers (representing costs), and thus the definition is equivalently changed to be the maximum cost of the bundles being minimized. To be consistent with the literature, we still call it maximin share.

**Definition 1** (\(\alpha\)-Mms): _An allocation \(\mathbf{A}=(A_{1},\ldots,A_{n})\) is \(\alpha\)-approximate maximin share (\(\alpha\)-MMS) fair if \(v_{i}(A_{i})\leq\alpha\cdot\mathsf{MMS}_{i}\) for all \(i\in N\). The allocation is MMS fair if \(\alpha=1\)._

Given the definition of MMS, for any agent \(i\) with subadditive cost \(v_{i}(\cdot)\), we have the following bounds for \(\mathsf{MMS}_{i}\),

\[\mathsf{MMS}_{i}\geq\max\big{\{}\max_{e\in M}v_{i}(\{e\}),\frac{1}{n}\cdot v_{ i}(M)\big{\}}.\] (1)

Following recent works [5, 27, 26], we also consider the ordinal approximation of MMS, namely, \(1\)-out-of-\(d\) MMS fairness. Intuitively, MMS fairness can be regarded as 1-out-of-\(n\) MMS (i.e., partitioning the items into \(n\) bundles but receiving the largest bundle). Since 1-out-of-\(n\) MMS allocations may not exist, we can instead find a maximum integer \(d\leq n\) such that a 1-out-of-\(d\) MMS allocation is guaranteed to exist. Given a \(d\)-partition of \(M\), \(\mathbf{X}=(X_{1},\ldots,X_{d})\), if \(v_{i}(X_{j})\leq\mathsf{MMS}_{i}^{d}\) for any \(j\in[d]\), then \(\mathbf{X}\) is called a _1-out-of-\(d\) MMS defining partition_ for agent \(i\). An allocation \(\mathbf{A}\) is _\(1\)-out-of-\(d\) MMS_ fair if for every agent \(i\in N\), \(v_{i}(A_{i})\leq\mathsf{MMS}_{i}^{d}\). More generally, given any \(\alpha\geq 1\), we have the bi-factor approximation, \(\alpha\)-approximate \(1\)-out-of-\(d\) MMS, if \(v_{i}(A_{i})\leq\alpha\cdot\mathsf{MMS}_{i}^{d}\) for every \(i\in N\). By the definition, we have the following simple observation, whose proof is deferred to Appendix B.2.

**Observation 1**: _For any instance with subadditive costs, given any integer \(1\leq d\leq n\), a 1-out-of-\(d\) MMS allocation is \(\lceil\frac{n}{d}\rceil\)-MMS fair._

## 3 General subadditive cost setting

By Inequality 1, if the costs are subadditive, allocating all items to a single agent ensures an approximation of \(n\), which is the most unfair algorithm. Surprisingly, such an unfair algorithm achieves the optimal approximation ratio of MMS even if the costs are submodular.

**Theorem 1**: _For any \(n\geq 2\), there is an instance with submodular costs for which no allocation is better than \(n\)-MMS or \(\frac{\log m}{\log\log m}\)-MMS._

**Proof.** For any fixed \(n\geq 2\), we construct an instance that contains \(n\) agents and \(m=n^{n}\) items. By taking logarithm of \(m=n^{n}\) twice, it is easy to obtain

\[n=\frac{\log m}{\log\log m-\log\log n}\geq\frac{\log m}{\log\log m}.\]

Thus, in the following, it suffices to show that no allocation can be better than \(n\)-MMS. Let each item correspond to a point in an \(n\)-dimensional coordinate system, i.e.,

\[M=\{(x_{1},x_{2},\ldots,x_{n})\mid x_{i}\in[n]\text{ for all }i\in[n]\}.\]

For each agent \(i\in N\), we define \(n\) covering planes \(\{C_{il}\}_{l\in[n]}\) and for each \(l\in[n]\),

\[C_{il}=\{(x_{1},x_{2},\ldots,x_{n})\mid x_{i}=l\text{ and }x_{j}\in[n]\text{ for all }j\in[n]\setminus\{i\}\}.\] (2)

Note that \(\{C_{il}\}_{l\in[n]}\) forms an exact cover of the points in \(M\), i.e., \(\bigcup_{l}C_{il}=M\) and \(C_{il}\cap C_{iz}=\emptyset\) for all \(l\neq z\). For any set of items \(S\subseteq M\), \(v_{i}(S)\) equals the minimum number of planes in \(\{C_{il}\}_{l\in[n]}\) that can cover \(S\). Therefore, \(v_{i}(S)\in[n]\) for all \(S\). We first show \(v_{i}(\cdot)\) is submodular for every \(i\). For any \(S\subseteq T\subseteq M\) and any \(e\in M\setminus T\), if \(e\) is not in the same covering plane as any point in \(T\), \(e\) is not in the same covering plane as any point in \(S\), either. Thus, \(v_{i}(T\cup\{e\})-v_{i}(T)=1\) implies \(v_{i}(S\cup\{e\})-v_{i}(S)=1\), and accordingly,

\[v_{i}(T\cup\{e\})-v_{i}(T)\leq v_{i}(S\cup\{e\})-v_{i}(S).\]

Since \(\{C_{il}\}_{l\in[n]}\) is an exact cover of \(M\), \(\mathsf{MMS}_{i}=1\) for every \(i\), where the MMS defining partition is simply \(\{C_{il}\}_{l\in[n]}\). Then to prove the theorem, it suffices to show that for any allocation of \(M\), there is at least one agent whose cost is \(n\). For the sake of contradiction, we assume there is an allocation \(\mathbf{A}=(A_{1},\ldots,A_{n})\) where every agent has cost at most \(n-1\). This means that for every \(i\in N\), there exists a plane \(C_{il_{i}}\) such that \(A_{i}\cap C_{il_{i}}=\emptyset\). Consider the point \(\mathbf{b}=(l_{1},\ldots,l_{n})\), it is clear that \(\mathbf{b}\in C_{il_{i}}\) and thus \(\mathbf{b}\notin A_{i}\) for all \(i\). This means that \(\mathbf{b}\) is not allocated to any agent, a contradiction. Therefore, such an allocation \(\mathbf{A}\) does not exist which completes the proof of the theorem.

To facilitate the understanding of Theorem 1, in Appendix C.1, we visualize an instance with 3 agents and 27 items where no allocation is better than 3-MMS. The hard instance in Theorem 1 also implies the following lower bound for \(1\)-out-of-\(d\) MMS, whose proof is in Appendix C.2.

**Corollary 1**: _For any \(2\leq d\leq n\), there is an instance with submodular cost functions for which no allocation is \(1\)-out-of-\(d\) MMS._

**Theorem 2**: _For any instance with subadditive cost functions, there always exists a \(\min\{n,\lceil\log m\rceil\}\)-MMS allocation._

**Proof.** We describe the algorithm that computes a \(\min\{n,\lceil\log m\rceil\}\)-MMS allocation in Algorithm 1. First, if \(\log m\geq n\), we are safe to arbitrarily allocate the items to the agents, which ensures \(n\)-approximation.

The tricky case is when \(\log m<n\), where we cannot allocate too many items to a single agent. For this case, we first look at agent 1's MMS defining partition \(\mathbf{D}^{1}=(D^{1}_{1},\ldots,D^{1}_{n})\), where for all \(j\in[n]\) and we assume that they are ordered by the number of items, i.e., \(|D_{1}^{1}|\geq\cdots\geq|D_{n}^{1}|\). In order to ensure that agent 1's cost is no more than \(\lceil\log m\rceil\) times her MMS, we ask her to take away \(\lceil\log m\rceil\) largest bundles (in terms of number of items) in \(\mathbf{D}^{1}\), i.e., \(A_{1}=\bigcup_{j\in[\lceil\log m\rceil]}D_{j}^{1}\). Since the cost function is subadditive,

\[v_{1}(A_{1})\leq\sum_{j\in[\lceil\log m\rceil]}v_{1}(D_{j}^{1}) \leq\lceil\log m\rceil\cdot\mathsf{MMS}_{1}.\]

Moreover, since on average each bundle in \(\mathbf{D}^{1}\) contains \(\frac{m}{n}\) items and \(A_{1}\) contains the bundles with largest number of items, \(|A_{1}|\geq\lceil\log m\rceil\cdot\frac{m}{n}\geq\frac{\log m}{n}\cdot m\). That is, at least \(\frac{\log m}{n}\) fraction of the items are taken away by agent 1. Let \(M_{1}=M\setminus A_{1}\) be the set of remaining items, and we have

\[|M_{1}|\leq\left(1-\frac{\log m}{n}\right)\cdot m.\]

We next ask agent 2 to take away items in a similar way to agent 1. Let \(\mathbf{D}^{2}=(D_{1}^{2},\ldots,D_{n}^{2})\) be one of agent 2's MMS defining partitions, and \(\mathbf{R}^{2}=(R_{1}^{2},\ldots,R_{n}^{2})\) be the remaining items in these bundles, i.e., \(R_{j}^{2}=D_{j}^{2}\cap M_{1}\). Again, we assume \(|R_{1}^{2}|\geq\cdots\geq|R_{n}^{2}|\). Letting \(A_{2}=\bigcup_{j\in[\lceil\log m\rceil]}R_{j}^{2}\) and \(M_{2}=M_{1}\setminus A_{2}\), we have \(v_{2}(A_{2})\leq\lceil\log m\rceil\cdot\mathsf{MMS}_{2}\). Moreover, since on average each bundle in \(\mathbf{R}^{2}\) contains \(\frac{|M_{1}|}{n}\) items and \(A_{2}\) contains the bundles with largest number of items,

\[|A_{2}|\geq\lceil\log m\rceil\cdot\frac{|M_{1}|}{n}\geq\frac{ \log m}{n}\cdot|M_{1}|,\]

which gives

\[|M_{2}|\leq\left(1-\frac{\log m}{n}\right)\cdot|M_{1}|\leq\left( 1-\frac{\log m}{n}\right)^{2}\cdot m.\] (3)

We continue with the above procedure for agents \(i=3,\ldots,n\) with the formal description shown in Algorithm 1. It is straightforward that every agent \(i\) who gets a bundle \(A_{i}\) has cost at most \(\lceil\log m\rceil\cdot\mathsf{MMS}_{i}\). Further, by induction, Equation 3 holds for all agents \(i\leq n\), i.e.,

\[|M_{i}|\leq\left(1-\frac{\log m}{n}\right)\cdot|M_{i-1}|\leq \left(1-\frac{\log m}{n}\right)^{i}\cdot m.\]

To show the validity of the Algorithm, it remains to show that the algorithm can allocate all items, i.e., \(M_{n}=\emptyset\). This can be seen from the following inequalities,

\[|M_{n}|\leq\left(1-\frac{\log m}{n}\right)^{n}\cdot m=\left(1- \frac{\log m}{n}\right)^{\frac{n}{\log m}\cdot\log m}\cdot m<\left(\frac{1}{e }\right)^{\log m}\cdot m<\frac{1}{m}\cdot m=1.\]

Since \(|M_{n}|<1\), \(M_{n}\) must be empty, which completes the proof of the theorem.

Note that Theorem 1 does not rule out the possibility of beating the approximation ratio for specific subadditive costs. In the next two sections, we turn to studying two specific settings, where we are able to beat the lower bounds in Theorem 1 and Corollary 1 by designing algorithms that can guarantee constant ordinal and multiplicative approximations of MMS. We will mostly consider the ordinal approximation of MMS. By Observation 1, the ordinal approximation gives a result of the multiplicative one, which can be improved by slightly modifying the designed algorithms.

## 4 Bin packing setting

### Model

The first setting encodes a bin packing problem where the items have sizes and need to be packed into bins by the agents. The items may be of different sizes to different agents. Specifically, each item \(e_{j}\in M\) has size \(s_{i,j}\geq 0\) to each agent \(i\in N\). For a set of items \(S\), \(s_{i}(S)=\sum_{e_{j}\in S}s_{i,j}\). Each agent \(i\in N\) has unlimited number of bins with the same capacity \(c_{i}\). Without loss of generality, we assume that \(c_{1}\geq\cdots\geq c_{n}\) and \(c_{i}\geq\max_{e_{j}\in M}s_{i,j}\) for all \(i\in N\).

Upon receiving a set of items \(S\subseteq M\), agent \(i\)'s cost \(v_{i}(S)^{2}\) is determined by the minimum number of bins (with capacity \(c_{i}\)) that can pack all items in \(S\). Note that the calculation of \(v_{i}(S)\) involves solving a classic bin packing problem which is NP-hard. For any two sets \(S_{1}\) and \(S_{2}\), \(v_{i}(S_{1}\cup S_{2})\leq v_{i}(S_{1})+v_{i}(S_{2})\) since the optimal packing of \(S_{1}\cup S_{2}\) is no worse than packing \(S_{1}\) and \(S_{2}\) separately and thus \(v_{i}(\cdot)\) is subadditive. Accordingly, \(\mathsf{MMS}_{i}^{d}\) is essentially the minimum number \(k_{i}\) such that the items can be partitioned into \(d\) bundles and the items in each bundle can be packed into no more than \(k_{i}\) bins. The definition of \(\mathsf{MMS}_{i}^{d}\) gives \(\mathsf{MMS}_{i}^{d}\cdot c_{i}\geq\frac{s_{i}(M)}{d}\) for all \(i\in N\).

We say an item \(e_{j}\in M\) is _large_ for an agent \(i\) if the size of \(e_{j}\) to \(i\) exceeds half of the capacity of \(i\)'s bins, i.e., \(s_{i,j}>\frac{c_{i}}{2}\); otherwise, we say \(e_{j}\) is _small_ for \(i\). Let \(H_{i}\) denote the set of \(i\)'s large items in \(M\), and \(L_{i}\) denote the set of \(i\)'s small items; that is, \(H_{i}=\{e_{j}\in M:s_{i,j}>\frac{c_{i}}{2}\}\) and \(L_{i}=\{e_{j}\in M:s_{i,j}\leq\frac{c_{i}}{2}\}\). Since two large items cannot be put together into the same bin, the number of each agent \(i\)'s large items is at most \(\mathsf{MMS}_{i}^{d}\cdot d\); that is, \(|H_{i}|\leq\mathsf{MMS}_{i}^{d}\cdot d\).

We apply a widely-used reduction [13; 28] to restrict our attention on identical ordering (IDO) instances where \(s_{i,1}\geq\cdots\geq s_{i,m}\) for all \(i\). Specifically, it means that any algorithm that ensures \(\alpha\)-approximate 1-out-of-\(d\) MMS allocations for IDO instances can be converted to compute \(\alpha\)-approximate 1-out-of-\(d\) MMS allocations for general instances. The reduction may not work for all subadditive costs, but we prove in Appendix D.1 that it does work for the bin packing and job scheduling settings. Therefore, for these two settings, we only consider IDO instances.

### Algorithm

Next, we elaborate on the algorithm that proves Theorem 3.

**Theorem 3**: _A \(1\)-out-of-\(\lfloor\frac{n}{2}\rfloor\) MMS allocation always exists for any bin packing instance._

Let \(d=\lfloor\frac{n}{2}\rfloor\). In a nutshell, our algorithm consists of two parts: in the first part, we partition the items into \(d\) bundles in a bag-filling fashion and select one or two agents for each bundle. In the second part, for each of the \(d\) bundles and each of the agents selected for it, we present an imaginary assignment of the items in the bundle to the bins of the agent. These imaginary assignments are used to guide the allocation of the items to the agents, such that each agent receives cost no more than her 1-out-of-\(d\) MMS.

#### 4.2.1 Part 1: partitioning the items into \(d\) bundles

The algorithm in the first part is formally presented in Algorithm 2, which runs in \(d\) rounds of bag initialization (Steps 5 to 8) and bag filling (Steps 12 to 18). For each round \(j\in[d]\), we define _candidate agents_ - those who think the size of the bag \(B_{j}\) is not large enough and have unallocated small items (Step 4). Note that the set of candidate agents changes with the items in the bag and the unallocated items. In the bag initialization procedure, we put into the bag the item \(e_{j}\) and the items every \(d\) items after \(e_{j}\) (i.e., \(e_{j+d},e_{j+2d},\ldots\)), as long as they have not been allocated and are large for at least one remaining agent. We select one such agent. After the bag initialization procedure, if there is at most one candidate agent, the round ends and the candidate agent (if exists and has not been selected) is added as another selected agent. Otherwise, we enter the bag filling procedure.

In the bag filling procedure, as long as there exist at least two candidate agents, we let two of them be the selected agents and put the smallest unallocated item into the bag. If there is at most one candidate agent after the smallest item is put into the bag, the round ends and the only candidate agent (if exists and has not been selected) replaces one of the selected agents.

```
0: An IDO bin packing instance \((N,M,\{v_{i}\}_{i\in N},\{s_{i}\}_{i\in N})\).
0: A \(d\)-partition of the items \(\mathbf{B}=(B_{1},\ldots,B_{d})\) and disjoint sets of selected agents \(\mathbf{G}=(G_{1},\ldots,G_{d})\).
1: Initialize \(L_{i}\leftarrow\{e_{j}\in M:s_{i,j}\leq\frac{c_{i}}{2}\}\) for each \(i\in N\), and \(R\gets M\).
2:for\(j=1\) to \(d\)do
3: Initialize \(B_{j}\leftarrow\emptyset\), \(G_{j}\leftarrow\emptyset\), \(t\gets j\).
4:\(N(B_{j})\leftarrow\{i\in N:s_{i}(B_{j})\leq\frac{s_{i}(M)}{d}\) and \(L_{i}\cap R\neq\emptyset\}\). // Candidate agents
5:while\(e_{t}\in R\) and there exists an agent \(i\in N\) who thinks \(e_{t}\) is large do
6:\(B_{j}\gets B_{j}\cup\{e_{t}\}\), \(R\gets R\setminus\{e_{t}\}\), \(t\gets t+d\).
7:\(G_{j}\leftarrow\{i\}\).
8:endwhile
9:if\(|N(B_{j})|=1\) and \(N(B_{j})\neq G_{j}\)then
10: Pick \(i\in N(B_{j})\), \(G_{j}\gets G_{j}\cup\{i\}\).
11:endif
12:while\(|N(B_{j})|\geq 2\)do
13: Pick \(i_{1},i_{2}\in N(B_{j})\), \(G_{j}\leftarrow\{i_{1},i_{2}\}\).
14: Pick the smallest item \(e\in R\), \(B_{j}\gets B_{j}\cup\{e\}\), \(R\gets R\setminus\{e\}\).
15:if\(|N(B_{j})|=1\) and \(N(B_{j})\nsubseteq G_{j}\)then
16: Pick \(i\in N(B_{j})\) and replace one arbitrary agent in \(G_{j}\) with agent \(i\).
17:endif
18:endwhile
19:\(N\gets N\setminus G_{j}\).
20:endfor ```

**Algorithm 2** Partitioning the items into \(d\) bundles.

The way we establish the bag and select the agents makes the following two important properties satisfied for every round \(j\in[d]\).

* **Property 1**: for each selected agent \(i\in G_{j}\), there are at most \(\mathsf{MMS}_{i}^{d}\) items in \(B_{j}\) that are large for \(i\). Besides, letting \(e_{j}^{*}\) be the item lastly added to \(B_{j}\), if \(e_{j}^{*}\) is small for \(i\), then \(s_{i}(B_{j}\setminus\{e_{j}^{*}\})\leq\frac{s_{i}(M)}{d}\).
* **Property 2**: for each remaining agent \(i^{\prime}\) (i.e., \(i^{\prime}\notin\bigcup_{l\in[j]}G_{l}\)), either \(s_{i^{\prime}}(B_{j})>\frac{s_{i^{\prime}}(M)}{d}\) or no unallocated item is small for \(i\) at the end of round \(j\). Besides, no item in \(\{e_{j},e_{j+d},\ldots\}\) that is large for \(i^{\prime}\) remains unallocated at the end of round \(j\).

**Proof.** For the first property, observe that large items are added into the bag only in the bag initialization procedure, where one out of every \(d\) items is picked. Since there are at most \(d\cdot\mathsf{MMS}_{i}^{d}\) large items for every agent \(i\), the bag contains at most \(\mathsf{MMS}_{i}^{d}\) of \(i\)'s large items. There are two cases where \(e_{j}^{*}\) is small for an agent \(i\in G_{j}\). First, \(i\) is the only candidate agent after \(e_{j}^{*}\) is added, for which case, we have \(s_{i}(B_{j})\leq\frac{s_{i}(M)}{d}\). Second, \(i\) is one of the two selected candidate agents before \(e_{j}^{*}\) is added, for which case, we have \(s_{i}(B_{j}\setminus\{e_{j}^{*}\})\leq\frac{s_{i}(M)}{d}\). In both cases, \(s_{i}(B_{j}\setminus\{e_{j}^{*}\})\leq\frac{s_{i}(M)}{d}\) holds. The second property is quite direct by the algorithm, since there is no candidate agent outside \(G_{j}\) at the end of round \(j\) (i.e., \(N(B_{j})\setminus G_{j}=\emptyset\)), and all unallocated large items in \(\{e_{j},e_{j+d},\ldots\}\) are put into the bag in the bag initialization procedure.

Property 1 ensures that the items in each bundle \(B_{j}\in\mathbf{B}\) can be allocated to the selected agents in \(G_{j}\), such that each agent \(i\in G_{j}\) can use no more than \(\mathsf{MMS}^{d}_{i}\) bins to pack all the items allocated to her, which will be shown in the following part. Property 2 ensures the following claim.

**Claim 1**: _All the items can be allocated in Algorithm 2._

**Proof.** Observe that when the last round begins, there are at least \(n-(d-1)\cdot 2\geq 2\) remaining agents. If all the unallocated items are large for some remaining agent, all of them are added into the bag during the bag initialization procedure of the last round and thus no item remains unallocated. Now consider the case where some unallocated item is small for any remaining agent. By Property 2, for any \(j\in[d-1]\) and any remaining agent \(i^{\prime}\), we have \(s_{i^{\prime}}(B_{j})>\frac{s_{i^{\prime}}(M)}{d}\). This gives that the total size of the unallocated items to \(i^{\prime}\) is smaller than \(\frac{s_{i^{\prime}}(M)}{d}\). Besides, after the bag initialization procedure of the last round, no large item remains and every remaining item is small for any remaining agent. Combining these two facts, we know that there are always at least 2 candidate agents and thus all small items can be allocated in the bag filling procedure, which completes the proof.

#### 4.2.2 Part 2: Allocating the items to the agents

Next, we allocate the items in each bundle \(B_{j}\in\mathbf{B}\) to the selected agents in \(G_{j}\). Let \(i\) be any agent in \(G_{j}\) and \(B^{\prime}_{j}=B_{j}\setminus\{e^{*}_{j}\}\) where \(e^{*}_{j}\) is the item lastly added to \(B_{j}\). We first imaginatively assign the items in \(B^{\prime}_{j}\) to \(i\)'s bins as illustrated by Figure 1. We first put \(i\)'s large items in \(B^{\prime}_{j}\) into individual empty bins. Then we greedily put into the bins the remaining small items in \(B^{\prime}_{j}\) in decreasing order of their sizes, as long as the total size of the assigned items does not exceed the bin's capacity. The first time when the total size exceeds the capacity, we move to the next bin and so on (if all the bins with large items are filled, we move to an empty bin). We call the item lastly added to each bin that makes the total size exceed the capacity an _extra item_. Denote by \(J_{i}(B^{\prime}_{j})\) the set of extra items and by \(W_{i}(B^{\prime}_{j})=B^{\prime}_{j}\setminus J_{i}(B^{\prime}_{j})\) the other items in \(B^{\prime}_{j}\).

If all items in \(B_{j}\) are large for some agent \(i\in G_{j}\), we allocate all of them to \(i\). Otherwise, we know that round \(j\) enters the bag filling procedure, thus there are two agents in \(G_{j}\) and the last item \(e^{*}_{j}\) is small for both of them. Letting \(i_{1}\) be the agent who has more large items in \(B_{j}\) and \(i_{2}\) be the other agent, we allocate \(i_{1}\) the items in \(W_{i_{1}}(B^{\prime}_{j})\) and allocate \(i_{2}\) the items in \(J_{i_{1}}(B^{\prime}_{j})\cup\{e^{*}_{j}\}\).

Now we are ready to prove Theorem 3.

**Proof of Theorem 3.** Consider any round \(j\in[d]\). If all items in \(B_{j}\) are large for some agent \(i\in G_{j}\), by Property 1, we know that there are at most \(\mathsf{MMS}^{d}_{i}\) items in \(B_{j}\). Thus \(i\) can pack all items in \(B_{j}\) using no more than \(\mathsf{MMS}^{d}_{i}\) bins.

For the other case, recall that the agent \(i_{1}\in G_{j}\) who has more large items in \(B_{j}\) receives the items in \(W_{i_{1}}(B^{\prime}_{j})\), and the other agent \(i_{2}\) receives the items in \(J_{i_{1}}(B^{\prime}_{j})\cup\{e^{*}_{j}\}\). We first discuss agent \(i_{1}\). By Property 1, we know that for each agent \(i\in\{i_{1},i_{2}\}\), there are at most \(\mathsf{MMS}^{d}_{i}\) large items in \(B^{\prime}_{j}\) and \(s_{i}(B^{\prime}_{j})\leq\frac{s_{i}(M)}{d}\). These two facts imply that in the imaginative assignment of \(B^{\prime}_{j}\) to \(i_{1}\), no more than \(\mathsf{MMS}^{d}_{i_{1}}\) bins are used. Since otherwise, \(s_{i_{1}}(B^{\prime}_{j})>\mathsf{MMS}^{d}_{i_{1}}\cdot c_{i_{1}}\geq\frac{s_ {i_{1}}(M)}{d}\), a contradiction. Therefore, \(i_{1}\) can pack all items in \(W_{i_{1}}(B^{\prime}_{j})\) using no more than \(\mathsf{MMS}^{d}_{i_{1}}\) bins.

Next we discuss agent \(i_{2}\). Observe that in the imaginary assignment of \(B^{\prime}_{j}\) to \(i_{1}\), for each extra item in \(J_{i_{1}}(B^{\prime}_{j})\), there exists another item in the same bin with a larger size. Therefore, we have

Figure 1: Imaginary assignment of \(B^{\prime}_{j}\) to agent \(i\)â€™s bins

\(s_{i_{2}}(J_{i_{1}}(B^{\prime}_{j}))\leq\frac{s_{i_{2}}(B^{\prime}_{j})}{2}\leq \frac{s_{i_{2}}(M)}{2d}\). Combining with the fact that there are at most \(\mathsf{MMS}^{d}_{i_{2}}\) large items in \(B^{\prime}_{j}\) for \(i_{2}\), we know that \(i_{2}\) can use no more than \(\mathsf{MMS}^{d}_{i_{2}}\) bins to pack all items in \(J_{i_{1}}(B^{\prime}_{j})\) and there exists one bin with at least half the capacity not occupied. Since otherwise, \(s_{i_{2}}(J_{i_{1}}(B^{\prime}_{j}))>\mathsf{MMS}^{d}_{i_{2}}\cdot\frac{c_{i_{2 }}}{2}\geq\frac{s_{i_{2}}(M)}{2d}\), a contradiction. Recall that the last item \(e^{*}_{j}\) is small for \(i_{2}\), it can be put into the bin that has enough unoccupied capacity. Therefore, \(i_{2}\) can also pack all items in \(J_{i_{1}}(B^{\prime}_{j})\cup\{e^{*}_{j}\}\) using at most \(\mathsf{MMS}^{d}_{i_{2}}\) bins, which completes the proof.

For the multiplicative relaxation of MMS, by Theorem 3 and Observation 1, a \(\lceil\frac{n}{\lceil\frac{n}{\lceil\frac{n}{\rceil}\rceil}}\rceil\)-MMS allocation is guaranteed. Actually, we can slightly modify Algorithm 2 to compute a 2-MMS allocation.

**Corollary 2**: _A 2-MMS allocation always exists for any bin packing instance._

**Proof.** To compute a 2-MMS allocation, we replace the value of \(d\) with \(n\) in Algorithm 2 and select only one agent in each round who receives the bag in that round. The modified algorithm is formally presented in Algorithm 3. Following the same reasonings in Parts 1 and 2 (i.e., Subsubsections 4.2.1 and 4.2.2), it is not hard to see that all items can be allocated in Algorithm 3 and for any \(i\in N\), there are at most \(\mathsf{MMS}_{i}\) large items in \(A_{i}\). Besides, if the last item \(e^{*}_{i}\) is small for \(i\), we have \(s_{i}(A_{i}\setminus\{e^{*}_{i}\})\leq\frac{s_{i}(M)}{n}\). Again, in the imaginary assignment of \(A_{i}\setminus\{e^{*}_{i}\}\) to \(i\), no more than \(\mathsf{MMS}_{i}\) bins are used and at least one of them does not have an extra item. Therefore, agent \(i\) can use \(\mathsf{MMS}_{i}\) bins to pack all items in \(W_{i}(A_{i}\setminus\{e^{*}_{i}\})\) and another \(\mathsf{MMS}_{i}\) bins to pack all items in \(J_{i}(A_{i}\setminus\{e^{*}_{i}\})\cup\{e^{*}_{i}\}\), which completes the proof.

In Appendix D.2, we show that the above multiplicative ratio is actually tight in the sense that there exists an instance where no allocation is better than 2-MMS. Besides, in Appendix D.3, we show that the algorithm that proves Corollary 2 actually computes an allocation where every agent \(i\) can use at most \(\frac{3}{2}\mathsf{MMS}_{i}+1\) bins to pack all the items allocated to her.

## 5 Job scheduling setting

The second setting encodes a job scheduling problem where the items are jobs that need to be processed by the agents. Each item \(e_{j}\in M\) has a size \(s_{i,j}\geq 0\) to each agent \(i\in N\), and for a set of items \(S\subseteq M\), \(s_{i}(S)=\sum_{e_{j}\in S}s_{i,j}\). As the bin packing setting, we only consider IDO instances where \(s_{i,1}\geq\cdots\geq s_{i,m}\) for all \(i\). Each agent \(i\in N\) exclusively controls a set of \(k_{i}\) machines \(P_{i}=[k_{i}]\) with possibly different speed \(\rho_{i,l}\) for each \(l\in P_{i}\). Without loss of generality, we assume \(\rho_{i,1}\geq\cdots\geq\rho_{i,k_{i}}\). Upon receiving a set of items \(S\subseteq M\), agent \(i\)'s cost \(v_{i}(S)\) is the minimum completion time of processing \(S\) using her own machines \(P_{i}\) (i.e., _the makespan of \(P_{i}\)_). Formally,

\[v_{i}(S)=\min_{(T_{1},\ldots,T_{k_{i}})\in\Pi_{k_{i}}(S)}\max_{l\in[k_{i}]} \frac{\sum_{e_{l}\in T_{l}}s_{i,t}}{\rho_{i,l}}.\]Note that the computation of \(v_{i}(S)\) is NP-hard if \(k_{i}\geq 2\). For any two sets \(S_{1}\) and \(S_{2}\), \(v_{i}(S_{1}\cup S_{2})\leq v_{i}(S_{1})+v_{i}(S_{2})\) since the makespan of scheduling \(S_{1}\cup S_{2}\) is no larger than the sum of the makespans of scheduling \(S_{1}\) and \(S_{2}\) separately, thus \(v_{i}(\cdot)\) is subadditive.

Regarding the value of \(\mathsf{MMS}_{i}^{d}\), intuitively, it is obtained by partitioning the items into \(d\cdot k_{i}\) bundles, and allocating them to \(k_{i}\) different types of machines (with possibly different speeds) where each type has \(d\) identical machines so that the makespan is minimized.3 Note that when each agent controls a single machine, i.e., \(k_{i}=1\) for all \(i\), the problem degenerates to the additive cost case, and thus the job scheduling setting strictly generalizes the additive setting.

Footnote 3: We provide another interpretation in Appendix E.1, which shows that the job scheduling setting uncovers new research directions for group-wise fairness.

For the job scheduling setting, we have the following two main results.

**Theorem 4**: _A 1-out-of-\(\lfloor\frac{n}{2}\rfloor\) MMS allocation always exists for any job scheduling instance._

**Corollary 3**: _A 2-MMS allocation always exists for any job scheduling instance._

Note that simply partitioning the items into \(n\) bundles in a round-robin fashion does not guarantee 1-out-of-\(\lfloor\frac{n}{2}\rfloor\) MMS even for the simpler additive cost setting. Consider an instance where there are four identical agents and five items with costs 4, 1, 1, 1, 1, respectively. For this instance, the value of 1-out-of-\(\lfloor\frac{n}{2}\rfloor\) MMS for each agent is 4 as the 1-out-of-\(2\) MMS defining partition is \(\{\{4\},\{1,1,1,1\}\}\). However, the round-robin algorithm allocates two items with costs 4 and 1 to one agent, who receives a cost more than her 1-out-of-\(d\) MMS. Our algorithm overcomes this problem by first partitioning the items into \(\lfloor\frac{n}{2}\rfloor\) bundles and then carefully allocating the items in each bundle to two agents.

Let \(d=\lfloor\frac{n}{2}\rfloor\). For each agent \(i\in N\) and each machine \(l\in P_{i}\), let \(c_{i,l}=\rho_{i,l}\cdot\mathsf{MMS}_{i}^{d}\) denote \(l\)'s capacity. In a nutshell, our algorithm consists of three parts: in the first part, we partition all items into \(d\) bundles in a round-robin fashion. In the second part, for each of the \(d\) bundles and each agent, we present an imaginary assignment of the items in the bundle to the agent's machines. These imaginary assignments are used in the third part to guide the allocation of the items in each of the \(d\) bundles to two agents, such that each agent can assign her allocated items to her machines in a way that the total workload on each machine does not exceed its capacity (in other words, each agent's cost is no more than her 1-out-of-\(d\) MMS). We defer the detailed algorithms and proofs to Appendix E.

## 6 Conclusion

In this work, we study fair allocation of indivisible chores when the costs are beyond additive and the fairness is measured by MMS. There are many open problems and further directions. First, there are only existential results for \(\min\{n,\lceil\log m\rceil\}\)-MMS allocations in the general subadditive cost setting and 1-out-of-\(\lfloor\frac{n}{2}\rfloor\) MMS allocations in the job scheduling setting. Polynomial-time algorithms that achieve the same results remain as open problems. Second, for the general subadditive cost setting and the bin packing setting, we provide the tight approximation ratios, but for the job scheduling setting, we only have a lower bound of \(\frac{44}{43}\), which is inherited from the additive cost setting [20]. One immediate direction is to design better approximation algorithms or lower bound instances for the job scheduling setting. Third, in the appendix, we show that for the bin packing setting, there exists an allocation where every agent's cost is no more than \(\frac{3}{2}\) times her MMS plus 1. We suspect that the multiplicative factor can be improved to 1. Fourth, for the job scheduling setting, we restrict us on the case of related machines in the current work, it is interesting to consider the general model of unrelated machines. As we have mentioned, the notion of collective maximin share fairness in the job scheduling setting can be viewed as a group-wise fairness notion, which could be of independent interest. Finally, we can investigate other combinatorial costs that can better characterize real-world problems.

## Acknowledgement

The authors are ordered alphabetically. This work is funded by NSFC under Grant No. 62102333, HKSAR RGC under Grant No. PolyU 25211321, and CCF-Huawei Populus Grove Fund.

## References

* [1] Georgios Amanatidis, Evangelos Markakis, Afshin Nikzad, and Amin Saberi. Approximation algorithms for computing maximin share allocations. _ACM Trans. Algorithms_, 13(4):52:1-52:28, 2017.
* [2] Georgios Amanatidis, Evangelos Markakis, and Apostolos Ntokos. Multiple birds with one stone: Beating 1/2 for EFX and GMMS via envy cycle elimination. _Theor. Comput. Sci._, 841:94-109, 2020.
* [3] Georgios Amanatidis, Haris Aziz, Georgios Birmpas, Aris Filos-Ratsikas, Bo Li, Herve Moulin, Alexandros A. Voudouris, and Xiaowei Wu. Fair division of indivisible goods: A survey. _CoRR_, abs/2208.08782, 2022.
* [4] Haris Aziz, Gerhard Rauchecker, Guido Schryen, and Toby Walsh. Algorithms for max-min share fair allocation of indivisible chores. In _AAAI_, pages 335-341. AAAI Press, 2017.
* [5] Moshe Babaioff, Noam Nisan, and Inbal Talgam-Cohen. Fair allocation through competitive equilibrium from generic incomes. In _FAT_, page 180. ACM, 2019.
* [6] Siddharth Barman and Sanath Kumar Krishnamurthy. Approximation algorithms for maximin fair division. _ACM Trans. Economics and Comput._, 8(1):5:1-5:28, 2020.
* [7] Siddharth Barman, Arpita Biswas, Sanath Kumar Krishna Murthy, and Yadati Narahari. Group-wise maximin fair allocation of indivisible goods. In _AAAI_, pages 917-924. AAAI Press, 2018.
* [8] Siddharth Barman, Arindam Khan, Sudarshan Shyam, and K. V. N. Sreenivas. Finding fair allocations under budget constraints. In _AAAI_, pages 5481-5489. AAAI Press, 2023.
* [9] Siddharth Barman, Arindam Khan, Sudarshan Shyam, and K. V. N. Sreenivas. Guaranteeing envy-freeness under generalized assignment constraints. In _EC_, pages 242-269. ACM, 2023.
* [10] Siddharth Barman, Vishnu V. Narayan, and Paritosh Verma. Fair chore division under binary supermodular costs. _CoRR_, abs/2302.11530, 2023.
* Leibniz-Zentrum fur Informatik, 2021.
* [12] Jeff A. Bilmes. Submodularity in machine learning and artificial intelligence. _CoRR_, abs/2202.00132, 2022.
* [13] Sylvain Bouveret and Michel Lemaitre. Characterizing conflicts in fair division of indivisible goods using a scale of criteria. _Auton. Agents Multi Agent Syst._, 30(2):259-290, 2016.
* [14] Eric Budish. The combinatorial assignment problem: Approximate competitive equilibrium from equal incomes. _Journal of Political Economy_, 119(6):1061-1103, 2011.
* [15] Ioannis Caragiannis, David Kurokawa, Herve Moulin, Ariel D. Procaccia, Nisarg Shah, and Junxing Wang. The unreasonable fairness of maximum nash welfare. _ACM Trans. Economics and Comput._, 7(3):12:1-12:32, 2019.
* [16] Hau Chan, Jing Chen, Bo Li, and Xiaowei Wu. Maximin-aware allocations of indivisible goods. In _IJCAI_, pages 137-143. ijcai.org, 2019.
* [17] Edward G Coffman, Janos Csirik, Gabor Galambos, Silvano Martello, and Daniele Vigo. Bin packing approximation algorithms: survey and classification. In _Handbook of combinatorial optimization_, pages 455-531. 2013.
* [18] Graham Cormode. Data sketching. _Commun. ACM_, 60(9):48-55, 2017.
* [19] Amitay Dror, Michal Feldman, and Erel Segal-Halevi. On fair division under heterogeneous matroid constraints. _J. Artif. Intell. Res._, 76:567-611, 2023.

* [20] Uriel Feige, Ariel Sapir, and Laliiv Tauber. A tight negative example for MMS fair allocations. In _WINE_, volume 13112 of _Lecture Notes in Computer Science_, pages 355-372. Springer, 2021.
* [21] Jiarui Gan, Bo Li, and Xiaowei Wu. Approximation algorithm for computing budget-feasible EF1 allocations. In _AAMAS_, pages 170-178. ACM, 2023.
* [22] Jugal Garg and Setareh Taki. An improved approximation algorithm for maximin shares. _Artif. Intell._, 300:103547, 2021.
* [23] Mohammad Ghodsi, Mohammad Taghi Hajiaghayi, Masoud Seddighin, Saeed Seddighin, and Hadi Yami. Fair allocation of indivisible goods: Improvement. _Math. Oper. Res._, 46(3):1038-1053, 2021.
* [24] Mohammad Ghodsi, Mohammad Taghi Hajiaghayi, Masoud Seddighin, Saeed Seddighin, and Hadi Yami. Fair allocation of indivisible goods: Beyond additive valuations. _Artif. Intell._, 303:103633, 2022.
* [25] Sonke Hartmann and Dirk Briskorn. An updated survey of variants and extensions of the resource-constrained project scheduling problem. _European Journal of operational research_, 297(1):1-14, 2022.
* [26] Hadi Hosseini, Andrew Searns, and Erel Segal-Halevi. Ordinal maximin share approximation for chores. In _AAMAS_, pages 597-605. International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS), 2022.
* [27] Hadi Hosseini, Andrew Searns, and Erel Segal-Halevi. Ordinal maximin share approximation for goods. _J. Artif. Intell. Res._, 74, 2022.
* [28] Xin Huang and Pinyan Lu. An algorithmic framework for approximating maximin share allocation of chores. In _EC_, pages 630-631. ACM, 2021.
* [29] Xin Huang and Erel Segal-Halevi. A reduction from chores allocation to job scheduling. _CoRR_, abs/2302.04581, 2023.
* [30] Halvard Hummel and Magnus Lie Hetland. Fair allocation of conflicting items. _Auton. Agents Multi Agent Syst._, 36(1):8, 2022.
* [31] David Kurokawa, Ariel D. Procaccia, and Junxing Wang. When can the maximin share guarantee be guaranteed? In _AAAI_, pages 523-529. AAAI Press, 2016.
* [32] David Kurokawa, Ariel D. Procaccia, and Junxing Wang. Fair enough: Guaranteeing approximate maximin shares. _J. ACM_, 65(2):8:1-8:27, 2018.
* [33] Bo Li, Minming Li, and Ruilong Zhang. Fair scheduling for time-dependent resources. In _NeurIPS_, pages 21744-21756, 2021.
* [34] Bo Li, Yingkai Li, and Xiaowei Wu. Almost (weighted) proportional allocations for indivisible chores. In _WWW_, pages 122-131. ACM, 2022.
* [35] Richard J. Lipton, Evangelos Markakis, Elchanan Mossel, and Amin Saberi. On approximately fair allocations of indivisible goods. In _EC_, pages 125-131. ACM, 2004.
* [36] Gaelle Loosli and Stephane Canu. Comments on the "core vector machines: Fast SVM training on very large data sets". _J. Mach. Learn. Res._, 8:291-301, 2007.
* [37] Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed submodular maximization: Identifying representative elements in massive data. In _NIPS_, pages 2049-2057, 2013.
* [38] Herve Moulin. Fair division in the age of internet. _Annu. Rev. Econ._, 2018.
* [39] Benjamin Plaut and Tim Roughgarden. Almost envy-freeness with general valuations. _SIAM J. Discret. Math._, 34(2):1039-1068, 2020.

* [40] Masoud Seddighin and Saeed Seddighin. Improved maximin guarantees for subadditive and fractionally subadditive fair allocation problem. In _AAAI_, pages 5183-5190. AAAI Press, 2022.
* [41] Edward Lloyd Snelson and Zoubin Ghahramani. Sparse gaussian processes using pseudo-inputs. In _NIPS_, pages 1257-1264, 2005.
* [42] Shengwei Zhou and Xiaowei Wu. Approximately EFX allocations for indivisible chores. _CoRR_, abs/2109.07313, 2021.

## Appendix A More related works

Besides proportionality, in another parallel line of research, envy-freeness and its relaxations, namely envy-free up to one item (EF1) and envy-free up to any item (EFX), are also widely studied. It was shown in [35] and [11] for goods and chores, respectively, that an EF1 allocation exists for the monotone combinatorial functions. However, the existence of EFX allocations is still unknown even with additive functions. Therefore, approximation algorithms were proposed in [2, 42] for additive functions and in [39, 16] for subadditive functions. We refer the readers to [3] for a detailed survey on fair allocation of indivisible items.

## Appendix B Missing materials in preliminaries

### Impossibility result for general cost functions

We provide an example to show that no bounded approximation ratio can be achieved for general cost functions. Note that there exist simpler examples, but we choose the following one because it represents a particular combinatorial structure - minimum spanning tree. Let \(G=(V,E)\) be a graph shown in the left sub-figure of Figure 2, where the vertices \(V\) are the items that are to be allocated, i.e., \(M=V\). There are two agents \(N=\{1,2\}\) who have different weights on the edges as shown in the middle and right sub-figures of Figure 2. The cost functions are measured by the minimum spanning tree in their received subgraphs. Particularly, for any \(S\subseteq V\), \(v_{i}(S)\) equals the weight of the minimum spanning tree on \(G[S]\) - the induced subgraph of \(S\) in \(G\) - under agent \(i\)'s weights. Thus, \(\mathsf{MMS}_{i}=0\), for both \(i=1,2\), where an MMS defining partition for agent \(1\) is \(\{v_{1},v_{2}\}\) and \(\{v_{3},v_{4}\}\) and that for agent \(2\) is \(\{v_{1},v_{4}\}\) and \(\{v_{2},v_{3}\}\). However, it can be verified that no matter how the vertices are allocated to the agents, there is one agent whose cost is at least 1, which implies that no bounded approximation is possible for general costs.

### Proof of Observation 1

To prove the observation, it suffices to show \(\mathsf{MMS}^{d}_{i}\leq\lceil\frac{n}{d}\rceil\cdot\mathsf{MMS}^{n}_{i}\) for any agent \(i\in N\). Let \(\mathbf{X}=(X_{1},\ldots,X_{n})\) be an MMS defining partition for agent \(i\), which satisfies \(v_{i}(X_{j})\leq\mathsf{MMS}^{n}_{i}\) for every \(j\in[n]\). Consider a \(d\)-partition \(\mathbf{X}^{\prime}=(X^{\prime}_{1},\ldots,X^{\prime}_{d})\) built by evenly distributing the \(n\) bundles in \(\mathbf{X}\) to the \(d\) bundles in \(\mathbf{X}^{\prime}\); that is, the number of bundles distributed to the bundles in \(\mathbf{X}^{\prime}\) differs by at most one. Clearly, \(\mathbf{X}^{\prime}\) satisfies \(v_{i}(X^{\prime}_{j})\leq\lceil\frac{n}{d}\rceil\cdot\mathsf{MMS}^{n}_{i}\) for every \(j\in[d]\). By the definition of 1-out-of-\(d\) MMS, it follows that

\[\mathsf{MMS}^{d}_{i}\leq\max_{j\in[d]}v_{i}(X^{\prime}_{j})\leq\lceil\frac{n }{d}\rceil\cdot\mathsf{MMS}^{n}_{i},\]

thus completing the proof.

Figure 2: An instance with unbounded approximation ratio

## Appendix C Missing materials in general subadditive cost setting

### An example that helps understand Theorem 1

The example is illustrated in Figure 3 where each agent has three covering planes. Take agent 1 for example, her three covering planes contain the items whose \(x\) coordinates are 1, 2, 3, respectively. If there exists an allocation that is better than 3-MMS, then each agent is allocated items from at most 2 of her covering planes. Without loss of generality, we assume that agent 1 (or agents 2 and 3 respectively) is not allocated any item whose \(x\) (or \(y\) and \(z\) respectively) coordinate is 1. Then, the item \((1,1,1)\) is not allocated to any agent, a contradiction.

### Proof of Corollary 1

We consider the same instance that is designed in Theorem 1. In this instance, we have proved that no matter how the items are allocated among the agents, there is at least one agent, say \(i\), whose cost is \(n\). Moreover, by the design of the cost functions, for any integer \(d\), it can be observed that \(\mathsf{MMS}_{i}^{d}=\lceil\frac{n}{d}\rceil\). Note that \(\lceil\frac{n}{d}\rceil\) is always smaller than \(n\) for all \(d\geq 2\), thus the allocation is not 1-out-of-\(d\) MMS to \(i\).

## Appendix D Missing materials in bin packing setting

### The IDO reduction

For a bin packing or job scheduling instance \(I\), the IDO instance \(I^{\prime}\) is constructed by setting the size of each item \(e_{j}\in M\) to each agent \(i\in N\) in \(I^{\prime}\) to the \(j\)-th largest size of the items to \(i\) in \(I\). Then the IDO reduction is formally presented in the following lemma.

**Lemma 1**: _For the bin packing or job scheduling setting, if there exists an allocation \(\mathbf{A}^{\prime}=(A^{\prime}_{1},\ldots,A^{\prime}_{n})\) in the IDO instance \(I^{\prime}\) such that \(v^{\prime}_{i}(A^{\prime}_{i})\leq\alpha\cdot\mathsf{MMS}_{i}^{d}(I^{\prime})\) for all \(i\in N\), then there exists an allocation \(\mathbf{A}=(A_{1},\ldots,A_{n})\) in the original instance 1 such that \(v_{i}(A_{i})\leq\alpha\cdot\mathsf{MMS}_{i}^{d}(I)\) for all \(i\in N\)._

**Proof.** We design Algorithm 4 that given \(I\), \(I^{\prime}\) and \(\mathbf{A}^{\prime}\), computes the desired allocation \(\mathbf{A}\). In the algorithm, we look at the items from \(e_{m}\) to \(e_{1}\). For each item, we let the agent who receives it in \(I^{\prime}\) pick her smallest unallocated item in \(I\).

To prove the lemma, we first show that \(v_{i}(A_{i})\leq v^{\prime}_{i}(A^{\prime}_{i})\) for all \(i\in N\). Consider the iteration where we look at the item \(e_{g}\). We suppose that in this iteration agent \(i\) picks item \(e_{g^{\prime}}\); that is, \(e_{g}\in A^{\prime}_{i}\), \(e_{g^{\prime}}\in A_{i}\) and \(e_{g^{\prime}}\) is the smallest unallocated item for \(i\). Since an item is removed from the set \(R\) after it is allocated, exactly \(m-g\) items have been allocated before \(e_{g^{\prime}}\) is allocated. Therefore, \(e_{g^{\prime}}\) is among the top \(m-g+1\) smallest items for agent \(i\). Recall that \(e_{g}\) is the item with the exactly \((m-g+1)\)-th smallest size to \(i\), hence \(s_{i,g^{\prime}}\leq s^{\prime}_{i,g}\). The same reasoning can be applied to other items in \(A^{\prime}_{i}\) and \(A_{i}\), and to other agents. It follows that for any \(i\in N\), any \(e_{g}\in A^{\prime}_{i}\) and the corresponding \(e_{g^{\prime}}\in A_{i}\), \(s_{i,g^{\prime}}\leq s^{\prime}_{i,g}\). For the bin packing or job scheduling setting, this implies \(v_{i}(A_{i})\leq v^{\prime}_{i}(A^{\prime}_{i})\). Since the maximin share depends on the sizes of the items but not on the order, the maximin share

Figure 3: An instance with 3 agents and 27 itemsof agent \(i\) in \(I^{\prime}\) is the same as that in \(I\), i.e., \(\mathsf{MMS}_{i}^{d}(I^{\prime})=\mathsf{MMS}_{i}^{d}(I)\). Hence, the condition that \(v_{i}^{\prime}(A_{i}^{\prime})\leq\alpha\cdot\mathsf{MMS}_{i}^{d}(I^{\prime})\) gives \(v_{i}(A_{i})\leq\alpha\cdot\mathsf{MMS}_{i}^{d}(I)\), which completes the proof.

```
0: A general instance \(I\), the IDO instance \(I^{\prime}\) and an allocation \(\mathbf{A}^{\prime}=(A_{1}^{\prime},...,A_{n}^{\prime})\) for the IDO instance such that \(v_{i}^{\prime}(A_{i}^{\prime})\leq\alpha\cdot\mathsf{MMS}_{i}^{d}(I^{\prime})\) for all \(i\in N\).
0: An allocation \(\mathbf{A}=(A_{1},...,A_{n})\) such that \(v_{i}(A_{i})\leq\alpha\cdot\mathsf{MMS}_{i}^{d}(I)\) for all \(i\in N\).
1: For all \(i\in N\) and \(e_{g}\in A_{i}^{\prime}\), set \(p_{g}\gets i\).
2: Initialize \(A_{t}=\emptyset\) for all \(i\in N\), and \(R\gets M\).
3:for\(g=m\) to 1do
4: Pick \(e_{g^{\prime}}\in\arg\min_{e_{k}\in R}\{s_{p_{g},k}\}\).
5:\(A_{p_{g}}\gets A_{p_{g}}\cup\{e_{g^{\prime}}\}\), \(R\gets R\setminus\{e_{g^{\prime}}\}\).
6:endfor ```

**Algorithm 4** IDO reduction for the bin packing and job scheduling settings

### Lower bound instance

We present an instance for the bin packing setting where no allocation can be better than 2-MMS. We first recall the impossibility instance given by Feige et al. [20]. In this instance there are three agents and nine items as arranged in a three by three matrix. The three agents' costs are shown in the matrices \(V_{1},V_{2}\) and \(V_{3}\).

\[V_{1} =\begin{pmatrix}6&15&22\\ 26&10&7\\ 12&19&12\end{pmatrix} V_{2} =\begin{pmatrix}6&15&23\\ 26&10&8\\ 11&18&12\end{pmatrix}\] \[V_{3} =\begin{pmatrix}6&16&22\\ 27&10&7\\ 11&18&12\end{pmatrix}\]

Feige et al. [20] proved that for this instance the MMS value of every agent is \(43\), however, in any allocation, at least one of the three agents gets cost no smaller than 44.

We can adapt this instance to the bin packing setting and obtain a lower bound of 2. In particular, we also have three agents and nine items. The numbers in matrices \(V_{1},V_{2}\) and \(V_{3}\) are the sizes of the items to agents \(1,2\) and \(3\), respectively. Let the capacities of the bins be \(c_{i}=43\) for all \(i\in\{1,2,3\}\). Accordingly, we have \(\mathsf{MMS}_{i}=1\) for all \(i\in\{1,2,3\}\). Since in any allocation, there is at least one agent who gets items with total size no smaller than 44, for this agent, she has to use two bins to pack the assigned items, which means that no allocation can be better than 2-MMS.

### Computing \(\frac{3}{2}\mathsf{MMS}+1\) allocations

Recall that in the proof of Corollary 2, it has been shown that each agent \(i\in N\) can use \(\mathsf{MMS}_{i}\) bins to pack all items in \(W_{i}(A_{i}\setminus\{e_{i}^{*}\})\) and another \(\mathsf{MMS}_{i}\) bins to pack all items in \(J_{i}(A_{i}\setminus\{e_{i}^{*}\})\cup\{e_{i}^{*}\}\). Actually, since all items in \(J_{i}(A_{i}\setminus\{e_{i}^{*}\})\cup\{e_{i}^{*}\}\) are small for \(i\) and at least two small items can be put into one bin, \(i\) only needs \(\lceil\frac{\mathsf{MMS}_{i}}{2}\rceil\) bins to pack all items in \(J_{i}(A_{i}\setminus\{e_{i}^{*}\})\cup\{e_{i}^{*}\}\). Therefore, each agent \(i\) can use no more than \(\frac{3}{2}\mathsf{MMS}_{i}+1\) bins to pack all the items allocated to her.

## Appendix E Missing materials in job scheduling setting

### Another interpretation to the job scheduling setting

An alternative way to explain the job scheduling setting is to view each agent \(i\) as a group of \(k_{i}\) small agents and \(\mathsf{MMS}_{i}^{d}\) as the _collective maximin share_ for these \(k_{i}\) small agents. We believe this notion of collective maximin share is of independent interest as a group-wise fairness notion. We remark that this notion is different from the group-wise (and pair-wise) maximin share defined in [7] and [15], where the max-min value is defined for each single agent. In our definition, however, a set of agents share the same value for the items allocated to them.

### Algorithm

#### e.2.1 Part 1: partitioning the items into \(d\) bundles

We first partition the items into \(d\) bundles \(\mathbf{B}=(B_{1},\ldots,B_{d})\) in a round-robin fashion. Specifically, we allocate the items in descending order of their sizes to the bundles by turns, from the first bundle to the last one. Each time, we allocate one item to one bundle, and when every bundle receives an item, we start over from the first bundle and so on. For any set of items \(S\), let \(S[l]\) be the \(l\)-th largest item in \(S\), then the algorithm is formally presented in Algorithm 5.

**Input:** An IDO job scheduling instance \((N,M,\{v_{i}\}_{i\in N},\{s_{i}\}_{i\in N})\).

**Output:** A \(d\)-partition of \(M\): \(\mathbf{B}=(B_{1},\ldots,B_{d})\).

```
1: Initialize \(B_{j}\leftarrow\emptyset\) for every \(j\in[d]\), and \(r\gets 1\).
2:while\(r\leq m\)do
3:for\(j=1\) to \(d\)do
4:if\(r\leq m\)then
5:\(B_{j}\gets B_{j}\cup\{M[r]\}\).
6:\(r\gets r+1\).
7:endif
8:endfor
9:endwhile ```

**Algorithm 5** Partitioning the items into \(d\) bundles

By the characteristic of the round-robin fashion, we have the following important observation.

**Observation 2**: _For each bundle \(B_{j}\in\mathbf{B}\) and each item \(e_{k}\in B_{j}\setminus\{B_{j}[1]\}\) (if exists), the \(d-1\) items before \(e_{k}\) (i.e., items \(e_{k-1},e_{k-2},\ldots,e_{k-d+1}\)) have at least the same sizes as \(e_{k}\)._

#### e.2.2 Part 2: imaginary assignment

Next, for each bundle \(B_{j}\in\mathbf{B}\) computed in the first part and each agent \(i\in N\), we imaginatively assign the items in \(B_{j}\setminus B_{j}[1]\) to \(i\)'s machines as follows. We greedily assign the items with larger sizes to \(i\)'s machines with faster speeds (in other words, with larger capacities), as long as the total workload on one machine does not exceed the its capacity. The first time when the workload exceeds the capacity, we move to the next machine and so on. The algorithm is formally presented in Algorithm 6 and illustrated in Figure 4. For each \(l\in P_{i}\), \(C^{I}_{i,l}\) contains the items imaginatively assigned to machine \(l\) that do not make the total workload exceed \(l\)'s capacity, and \(t_{i,l}\) is the last item assigned to \(l\) that makes the total workload exceed the capacity. Note that \(C^{I}_{i,l}\) may be empty and \(t_{i,l}\) may be null. For simplicity, let \(t_{i,0}=B_{j}[1]\); that is, \(B_{j}[1]\) is assigned to an imaginary machine 0. The items in \(\bigcup_{l\in[k_{i}]}C^{I}_{i,l}\) are called _internal items_ (as shown by the _dark_ boxes in Figure 4), and \(\{t_{i,0},\ldots,t_{i,k_{i}}\}\) are called _external items_ (as shown by the _light_ boxes).

```
1:A bundle \(B_{j}\in\mathbf{B}\) computed in the first part and an agent \(i\in N\).
2:Sets of internal items \(\{C^{I}_{i,1},\ldots,C^{I}_{i,k_{i}}\}\) and external items \(\{t_{i,0},\ldots,t_{i,k_{i}}\}\).
3:Initialize \(C^{I}_{i,l}\leftarrow\emptyset\), \(t_{i,l}\leftarrow\) null for every \(l\in[k_{i}]\), and \(r\gets 1\).
4:while\(r\leq|B_{j}|\)do
5:for\(l=1\) to \(k_{i}\)do
6:\(t_{i,l-1}\gets B_{j}[r]\), \(r\gets r+1\).
7:while\(r\leq|B_{j}|\) and \(s_{i}(C^{I}_{i,l}\cup\{B_{j}[r]\})\leq c_{i,l}\)do
8:\(C^{I}_{i,l}\gets C^{I}_{i,l}\cup\{B_{j}[r]\}\), \(r\gets r+1\).
9:endwhile
10:endfor
11:endwhile ```

**Algorithm 6** Imaginary assignment

For each bundle \(B_{j}\in\mathbf{B}\) and each agent \(i\in N\), the imaginary assignment has the following important properties.

* **Property 1**: all items in \(B_{j}\setminus\{B_{j}[1]\}\) can be assigned to agent \(i\)'s machines. Besides, the last machine \(k_{i}\) does not have an external item; that is, \(t_{i,k_{i}}\) is null.
* **Property 2**: for any \(1\leq l\leq k_{i}\), the total size of the internal items \(C^{I}_{i,l}\) does not exceed the capacity of machine \(l\), i.e., \(s_{i}(C^{I}_{i,l})\leq c_{i,l}\)
* **Property 3**: for any \(1\leq l\leq k_{i}\), the external item \(t_{i,l-1}\) (if not null) has size no larger than the capacity of machine \(l\), i.e., \(s_{i}(H_{i,l-1})\leq c_{i,l}\).

**Proof.** The first property holds since otherwise, \(s_{i}(B_{j}\setminus\{B_{j}[1]\})>\sum_{l\in[k_{i}]}c_{i,l}\). By Observation 2, it follows that

\[s_{i}(M)>d\cdot s_{i}(B_{j}\setminus\{B_{j}[1]\})>d\cdot\sum_{l\in[k_{i}]}c_{ i,l}.\]

However, since all items can be assigned to \(i\)'s machines in \(i\)'s 1-out-of-\(d\) MMS defining partition, we have \(s_{i}(M)\leq d\cdot\sum_{l\in[k_{i}]}c_{i,l}\), a contradiction.

The second property directly follows the algorithm. For the third property, \(s_{i}(t_{i,0})\leq c_{i,1}\) follows two facts that \(t_{i,0}\) is assigned to some machine in \(i\)'s 1-out-of-\(d\) MMS defining partition and \(c_{i,1}\) is the largest capacity of the machines. We then consider \(l\in[k_{i}-1]\) and show \(s_{i}(t_{i,l})\leq c_{i,l+1}\) (if \(t_{i,l}\) is not null). The same reasoning can be applied to any other \(l^{\prime}\in[k_{i}-1]\). Let \(S_{1}=\bigcup_{p\in[l]}(C^{I}_{i,p}\cup\{t_{i,p}\})\). From the algorithm, we know that \(s_{i}(S_{1})>\sum_{p\in[l]}c_{i,p}\) and \(t_{i,l}\) is the smallest item in \(S_{1}\). By Observation 2, there exist another \(d-1\) disjoint sets of items \(\{S_{2},\ldots,S_{d}\}\) such that \(s_{i}(S_{k})\geq s_{i}(S_{1})\) for every \(k\in[2,d]\) and \(t_{i,l}\) is also the smallest item in \(\bigcup_{k\in[d]}S_{k}\). Hence, \(\sum_{k\in[d]}s_{i}(S_{k})>d\cdot\sum_{p\in[l]}c_{i,p}\). This implies that in \(i\)'s 1-out-of-\(d\) MMS defining partition, at least one item in \(\bigcup_{k\in[d]}S_{k}\) is assigned to machine \(p\geq l+1\). Combining with the fact that \(t_{i,l}\) is the smallest item in \(\bigcup_{k\in[d]}S_{k}\), we have \(s_{i}(t_{i,l})\leq c_{i,l+1}\).

By these properties, for each machine \(l\in P_{i}\), we can assign either the internal items \(C^{I}_{i,l}\) or the external item \(t_{i,l-1}\) to \(l\), such that its completion time does not exceed \(\mathsf{MMS}^{d}_{i}\). This intuition guides the allocation of the items to the agents in the following part.

#### e.2.3 Part 3: allocating the items to the agents

Lastly, for any bundle \(B_{j}\in B\), we arbitrarily choose two agents \(i_{1},i_{2}\in N\) and allocate them the items in \(B_{j}\) as formally described in Algorithm 7. Recall that in the imaginary assignment of \(B_{j}\) to each agent \(i\in\{i_{1},i_{2}\}\), the items in \(B_{j}\) are divided into internal items \(\bigcup_{l\in[k_{i}]}C^{I}_{i,l}\) and external items \(\{t_{i,0},\ldots,t_{i,k_{i}}\}\). Let \(E=\{e^{*}_{1},\ldots,e^{*}_{|E|}\}\) contain all external items shared by \(i_{1}\) and \(i_{2}\). Note that \(e^{*}_{1}=t_{i_{1},0}=t_{i_{2},0}\). We allocate the items in \(B_{j}\) to agents \(i_{1}\) and \(i_{2}\) in \(|E|\) rounds. In each round \(q\in[|E|]\), we first find the machines of \(i_{1}\) and \(i_{2}\) to which the shared external items \(e^{*}_{q}\) and \(e^{*}_{q+1}\) are assigned (denoted by \(l_{1}\), \(l_{2}\), \(l^{\prime}_{1}\) and \(l^{\prime}_{2}\), respectively. If \(q=|E|\), simply let \(l^{\prime}_{1}=k_{i_{1}}\) and \(l^{\prime}_{2}=k_{i_{2}}\)). We then find the agent \(i_{k}\in\{i_{1},i_{2}\}\) whose machine \(l_{k}+1\) has more internal items. We allocate \(i_{k}\) her internal items from machine \(l_{k}+1\) to machine \(l^{\prime}_{k}\), and allocate the other agent \(i_{k}\)'s external items from machine \(l_{k}\) to machine \(l^{\prime}_{k}-1\).

Since \(2\cdot d=2\cdot\lfloor\frac{n}{2}\rfloor\leq n\), no more than \(n\) agents are needed to allocate all items. Thus to prove Theorem 4, it remains to show that each agent can assign her allocated items to her machines such that the total workload on each of the machines does not exceed its capacity.

Figure 4: The imaginary assignment of \(B_{j}\) to agent \(i\)

**Input:** A \(d\)-partition of the items \(\mathbf{B}=(B_{1},\ldots,B_{d})\) returned by Algorithm 5.

**Output:** An allocation \(\mathbf{A}=(A_{1},\ldots,A_{n})\) such that \(v_{i}(A_{i})\leq\mathsf{MMS}_{i}^{d}\) for all \(i\in N\).

```
1:Initialize \(A_{i}\leftarrow\emptyset\) for every \(i\in N\).
2:for\(j=1\) to \(d\)do
3: Arbitrarily choose 2 agents \(i_{1},i_{2}\in N\), \(N\gets N\setminus\{i_{1},i_{2}\}\).
4:\(\{C_{i_{1},1}^{I},\ldots,C_{i_{1},k_{i_{1}}}^{I}\},\{t_{i_{1},0},\ldots,t_{i_ {1},k_{i_{1}}}\}\leftarrow\textit{Algorithm}\) 6(\(B_{j}\), \(i_{1}\)).
5:\(\{C_{i_{2},1}^{I},\ldots,C_{i_{2},k_{i_{2}}}^{I}\},\{t_{i_{2},0},\ldots,t_{i_ {2},k_{i_{2}}}\}\leftarrow\textit{Algorithm}\) 6(\(B_{j}\), \(i_{2}\)).
6:\(E\leftarrow\{t_{i_{1},0},\ldots,t_{i_{1},k_{i_{1}}}\}\cap\{t_{i_{2},0},\ldots, t_{i_{2},k_{i_{2}}}\}\). Re-label \(E\leftarrow\{e_{1}^{*},\ldots,e_{|E|}^{*}\}\). // Shared external items \(j_{1}\) and \(i_{2}\)
7:for\(q=1\) to \(|E|\)do
8: Find \(l_{1}\in[0,k_{i_{1}}]\) and \(l_{2}\in[0,k_{i_{2}}]\) such that \(e_{q}^{*}=t_{i_{1},l_{1}}=t_{i_{2},l_{2}}\).
9:if\(q<|E|\)then
10: Find \(l_{1}^{\prime}\in[0,k_{i_{1}}]\) and \(l_{2}^{\prime}\in[0,k_{i_{2}}]\) such that \(e_{q+1}^{*}=t_{i_{1},l_{1}^{\prime}}=t_{i_{2},l_{2}^{\prime}}\).
11:else
12:\(l_{1}^{\prime}=k_{i_{1}}\) and \(l_{2}^{\prime}=k_{i_{2}}\).
13:endif
14:if\(|C_{i_{1},l_{1}+1}^{I}|\geq|C_{i_{2},l_{2}+1}^{I}|\)then
15:\(A_{i_{1}}\leftarrow\bigcup_{l=l_{1}+1}^{l_{1}^{\prime}}C_{i_{1},l}^{I}\), \(A_{i_{2}}\leftarrow\bigcup_{l=l_{1}}^{l_{1}^{\prime}-1}t_{i_{1},l}\).
16:else
17:\(A_{i_{2}}\leftarrow\bigcup_{l=l_{2}+1}^{l_{2}^{\prime}}C_{i_{2},l}^{I}\), \(A_{i_{1}}\leftarrow\bigcup_{l=l_{2}}^{l_{2}^{\prime}-1}t_{i_{2},l}\).
18:endif
19:endfor
20:endfor ```

**Algorithm 7** Allocating the items to the agents

**Proof of Theorem 4.** Consider any bundle \(B_{j}\in\mathbf{B}\) and assume the two chosen agents are \(i_{1},i_{2}\in N\). We first look at the first round of the process of allocating the items in \(B_{j}\) to \(i_{1}\) and \(i_{2}\). Without loss of generality, assume that the first machine of \(i_{1}\) contains more internal items than that of \(i_{2}\), i.e., \(C_{i_{1},1}^{I}\geq C_{i_{2},1}^{I}\). From the algorithm, the items \(i_{1}\) takes are \(\bigcup_{l=1}^{l_{1}^{\prime}}C_{i_{1},l}^{I}\). By the second property of the imaginary assignment, these items can be assigned to the first \(l_{1}^{\prime}\) machines of \(i_{1}\) such that the total workload on each machine does not exceed its capacity. Besides, the items \(i_{2}\) takes are \(\bigcup_{l=0}^{l_{1}^{\prime}-1}t_{i_{1},l}\), which are \(e_{1}^{*}\) and a subset of \(\bigcup_{l=2}^{l_{2}^{\prime}}C_{i_{2},l}^{I}\). By the second and third properties of the imaginary assignment, these items can be assigned to the first \(l_{2}^{\prime}\) machines of \(i_{2}\) such that the total workload on each machine does not exceed its capacity. The same reasoning can be applied to all following rounds. By induction, it follows that both \(i_{1}\) and \(i_{2}\) can assign their allocated items to their machines such that the total workload on each machine does not exceed its capacity. This means that both \(i_{1}\) and \(i_{2}\) receive costs no more than their 1-out-of-\(d\) MMS, which completes the proof.

For the multiplicative relaxation of MMS, by Theorem 4 and Observation 1, a \(\lceil\frac{n}{\lfloor\frac{n}{2}\rfloor}\rceil\)-MMS allocation is guaranteed. As the bin packing setting, after a slight modification, Algorithm 5 computes a 2-MMS allocation, which is better than \(\lceil\frac{n}{\lfloor\frac{n}{2}\rfloor}\rceil\)-MMS.

**Proof of Corollary 3.** We show that by replacing the value of \(d\) with \(n\), Algorithm 5 computes a \(2\)-MMS allocation. Particularly, in the new version of Algorithm 5, we partition the items in \(M\) into \(n\) bundles in a round-robin fashion and allocate each of the \(n\) bundles to one agent in \(N\). By the properties of the imaginary assignment, for each agent, the makespan of processing either the internal items or the external items in her bundle using her machines does not exceed \(\mathsf{MMS}_{i}^{n}\). This implies that for each agent, the cost of her bundle does not exceed \(2\cdot\mathsf{MMS}_{i}^{n}\), which completes the proof.

## Appendix F Proportionality up to one or any item

We now discuss two other relaxations for proportionality, i.e., proportional up to one item (PROP1) and proportional up to any item (PROPX), which are also widely studied for additive costs.

**Definition 2** (\(\alpha\)-PROP1 and \(\alpha\)-PROPX): _An allocation \(\mathbf{A}=(A_{1},\ldots,A_{n})\) is \(\alpha\)-approximate proportional up to one item (\(\alpha\)-PROP1) if \(v_{i}(A_{i}\backslash\{e\})\leq\alpha\cdot\frac{v_{i}(M)}{n}\) for all agents \(i\in N\) and some item \(e\in A_{i}\). It is \(\alpha\)-approximate proportional up to any item (\(\alpha\)-PROPX) if \(v_{i}(A_{i}\backslash\{e\})\leq\alpha\cdot\frac{v_{i}(M)}{n}\) for all agents \(i\in N\) and any item \(e\in A_{i}\). The allocation is PROP1 or PROPX if \(\alpha=1\)._

It is easy to see that a PROPX allocation is also PROP1. Although exact PROPX or PROP1 allocations are guaranteed to exist for additive costs, when the costs are subadditive, no algorithm can be better than \(n\)-PROP1 or \(n\)-PROPX. Consider an instance with \(n\) agents and \(n+1\) items. The cost function is \(v_{i}(S)=1\) for all agents \(i\in N\) and any non-empty subset \(S\subseteq M\). Clearly, the cost function is subadditive since \(v_{i}(S)+v_{i}(T)\geq v_{i}(S\cup T)\) for any \(S,T\subseteq M\). By the pigeonhole principle, at least one agent \(i\) receives two or more items in any allocation of \(M\). After removing any item \(e\in A_{i}\), \(A_{i}\) is still not empty. That is, \(v_{i}(A_{i}\backslash\{e\})=1=n\cdot\frac{v_{i}(M)}{n}\) for any \(e\in A_{i}\). This example can be easily extended to the bin packing and job scheduling settings, and thus we have the following theorem.

**Theorem 5**: _For the bin packing and job scheduling settings, no algorithm performs better than \(n\)-PROP1 or \(n\)-PROPX._

**Proof.** For the bin packing setting, consider an instance with \(n\) agents and \(n+1\) items. The capacity of each agent's bins is 1, i, e, \(c_{i}=1\) for all \(i\in N\). Each item is very tiny so that every agent can pack all items in just one bin, e.g., \(s_{i,j}=\frac{1}{n+1}\) for any \(i\in N\) and \(e_{j}\in M\). Therefore, we have \(v_{i}(M)=1\) and \(\mathsf{PROP}_{i}=\frac{1}{n}\) for each agent \(i\in N\). By the pigeonhole principle, at least one agent \(i\) receives two or more items in any allocation of \(M\). After removing any item \(e\in A_{i}\), agent \(i\) still needs one bin to pack the remaining items. Hence, we have \(v_{i}(A_{i}\backslash\{e\})=1=n\cdot\mathsf{PROP}_{i}\) for any \(e\in A_{i}\).

For the job scheduling setting, consider an instance with \(2n\) agents and \(2n+1\) items where each agent possesses \(2n\) machines with the same speed of 1, and the size of each item is 1 for every agent. It can be easily seen that for every agent \(i\in N\), the maximum completion time of her machines is minimized when assigning two items to one machine and one item to each of the remaining \(2n-1\) machines. Therefore, we have \(v_{i}(M)=2\) and \(\mathsf{PROP}_{i}=\frac{2}{2n}=\frac{1}{n}\) for any \(i\in N\). Similarly, by the pigeonhole principle, at least one agent \(i\) receives two or more items in any allocation of \(M\). This implies that \(v_{i}(A_{i}\backslash\{e\})=1=n\cdot\mathsf{PROP}_{i}\) for any \(e\in A_{i}\), thus completing the proof.