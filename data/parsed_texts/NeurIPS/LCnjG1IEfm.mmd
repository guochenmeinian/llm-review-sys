# Normalization-Equivariant Neural Networks with Application to Image Denoising

 Sebastien Herbreteau

Emmanuel Moebel

Charles Kervrann

Centre Inria de l'Universite de Rennes, France

{sebastien.herbreteau, emmanuel.moebel, charles.kervrann}@inria.fr

###### Abstract

In many information processing systems, it may be desirable to ensure that any change of the input, whether by shifting or scaling, results in a corresponding change in the system response. While deep neural networks are gradually replacing all traditional automatic processing methods, they surprisingly do not guarantee such normalization-equivariance (scale + shift) property, which can be detrimental in many applications. To address this issue, we propose a methodology for adapting existing neural networks so that normalization-equivariance holds by design. Our main claim is that not only ordinary convolutional layers, but also all activation functions, including the ReLU (rectified linear unit), which are applied element-wise to the pre-activated neurons, should be completely removed from neural networks and replaced by better conditioned alternatives. To this end, we introduce affine-constrained convolutions and channel-wise sort pooling layers as surrogates and show that these two architectural modifications do preserve normalization-equivariance without loss of performance. Experimental results in image denoising show that normalization-equivariant neural networks, in addition to their better conditioning, also provide much better generalization across noise levels.

## 1 Introduction

Sometimes wrongly confused with the invariance property which designates the characteristic of a function \(f\) not to be affected by a specific transformation \(\mathcal{T}\) applied beforehand, the equivariance property, on the other hand, means that \(f\) reacts in accordance with \(\mathcal{T}\). Formally, invariance is \(f\circ\mathcal{T}=f\) whereas equivariance reads \(f\circ\mathcal{T}=\mathcal{T}\circ f\), where \(\circ\) denotes the function composition operator. Both invariance and equivariance play a crucial role in many areas of study, including physics, computer vision, signal processing and have recently been studied in various settings for deep learning-based models [3, 6, 10, 13, 14, 16, 21, 23, 31, 38, 40, 41, 43].

In this paper, we focus on the equivariance of neural networks \(f_{\theta}\) to a specific transformation \(\mathcal{T}\), namely normalization. Although highly desirable in many applications and in spite of its omnipresence in machine learning, current neural network architectures do not equivary to normalization. With application to image denoising, for which _normalization-equivariance_ is generally guaranteed for a lot of conventional methods [5, 18, 20, 37], we propose a methodology for adapting existing neural networks, and in particular denoising CNNs [8, 30, 46, 47, 48], so that _normalization-equivariance_ holds by design. In short, the proposed adaptation is based on two innovations:

1. affine convolutions: the weights from one layer to each neuron from the next layer, _i.e._ the convolution kernels in a CNN, are constrained to encode affine combinations of neurons (the sum of the weights is equal to \(1\)).

2. channel-wise sort pooling: all activation functions that apply element-wise, such as the ReLU, are substituted with higher-dimensional nonlinearities, namely two by two sorting along channels that constitutes a fast and efficient _normalization-equivariant_ alternative.

Despite strong architectural constraints, we show that these simple modifications do not degrade performance and, even better, increase robustness to noise levels in image denoising both in practice and in theory.

## 2 Related Work

A non-exhaustive list of application fields where equivariant neural networks were studied includes graph theory, point cloud analysis and image processing. Indeed, graph neural networks are usually expected to equivaly, in the sense that a permutation of the nodes of the input graph should permute the output nodes accordingly. Several specific architectures were investigated to guarantee such a property [3; 21; 38]. In parallel, rotation and translation-equivariant networks for dealing with point cloud data were proposed in a recent line of research [6; 13; 40]. A typical application is the ability for these networks to produce direction vectors consistent with the arbitrary orientation of the input point clouds, thus eliminating the need for data augmentation. Finally, in the domain of image processing, it may be desirable that neural networks produce outputs that equivaly with regard to rotations of the input image, whether these outputs are vector fields [31], segmentation maps [41; 43], or even bounding boxes for object tracking [16].

In addition to their better conditioning, equivariant neural networks by design are expected to be more robust to outliers. A spectacular example has been revealed by S. Mohan _et al._[33] in the field of image denoising. By simply removing the additive constant ("bias") terms in neural networks with ReLU activation functions, they showed that a much better generalization at noise levels outside the training range was ensured. Although they do not fully elucidate why biases prevent generalization, and their removal allows it, the authors establish some clues that the answer is probably linked to the _scale-equivariant_ property of the resulting encoded function: rescaling the input image by a positive constant value rescales the output by the same amount.

## 3 Overview of normalization-equivariance

### Definitions and properties of three types of fundamental equivariances

We start with formal definitions of the different types of equivariances studied in this paper. Please note that our definition of "scale" and "shift" may differ from the definitions given by some authors in the image processing literature.

**Definition 1**: _A function \(f:\mathbb{R}^{n}\mapsto\mathbb{R}^{m}\) is said to be:_

* _scale-equivariant if_ \(\forall x\in\mathbb{R}^{n},\forall\lambda\in\mathbb{R}_{+}^{+},\;f(\lambda x )=\lambda f(x)\,,\)__
* _shift-equivariant if_ \(\forall x\in\mathbb{R}^{n},\forall\mu\in\mathbb{R},\;f(x+\mu)=f(x)+\mu\,,\)__
* _normalization-equivariant if it is both scale-equivariant and shift-equivariant:_ \[\forall x\in\mathbb{R}^{n},\forall\lambda\in\mathbb{R}_{+}^{+},\forall\mu \in\mathbb{R},\;f(\lambda x+\mu)=\lambda f(x)+\mu\,,\]

_where addition with the scalar shift \(\mu\) is applied element-wise._

Note that the _scale-equivariance_ property is more often referred to as positive homogeneity in pure mathematics. Like linear maps that are completely determined by their values on a basis, the above described equivariant functions are actually entirely characterized by the values their take on specific subsets of \(\mathbb{R}^{n}\), as stated by the following lemma (see proof in Appendix C.1).

**Lemma 1** (**Characterizations**): \(f:\mathbb{R}^{n}\mapsto\mathbb{R}^{m}\) _is entirely determined by its values on the:_

* _unit sphere_ \(\mathcal{S}\) _of_ \(\mathbb{R}^{n}\) _if it is scale-equivariant,_
* _orthogonal complement of_ \(\operatorname{Span}(\mathbf{1}_{n})\)_, i.e._ \(\operatorname{Span}(\mathbf{1}_{n})^{\perp}\)_, if it is shift-equivariant,_
* _intersection_ \(\mathcal{S}\cap\operatorname{Span}(\mathbf{1}_{n})^{\perp}\) _if it is normalization-equivariant,_

_where \(\mathbf{1}_{n}\) denotes the all-ones vector of \(\mathbb{R}^{n}\)._

Finally, Lemma 2 highlights three basic equivariance-preserving mathematical operations that can be used as building blocks for designing neural network architectures (see proof in Appendix C.1).

**Lemma 2** (Operations preserving equivariance): _Let \(f\) and \(g\) be two equivariant functions of the same type (either in scale, shift or normalization). Then, subject to dimensional compatibility, all of the following functions are still equivariant:_

* \(f\circ g\) _(_\(f\) _composed with_ \(g\)_),_
* \(x\mapsto(f(x)^{\top}\,g(x)^{\top})^{\top}\) _(concatenation of_ \(f\) _and_ \(g\)_),_
* \((1-t)f+tg\) _for all_ \(t\in\mathbb{R}\) _(affine combination of_ \(f\) _and_ \(g\)_)._

### Examples of normalization-equivariant conventional denoisers

A ("blind") denoiser is basically a function \(f:\mathbb{R}^{n}\mapsto\mathbb{R}^{n}\) which, given a noisy image \(y\in\mathbb{R}^{n}\), tries to map the corresponding noise-free image \(x\in\mathbb{R}^{n}\). Since scaling up an image by a positive factor \(\lambda\) or adding it up a constant shift \(\mu\) does not change its contents, it is natural to expect _scale_ and _shift equivariance_, _i.e. normalization equivariance_, from the denoising procedure emulated by \(f\). In image denoising, a majority of methods usually assume an additive white Gaussian noise model with variance \(\sigma^{2}\). The corruption model then reads \(y\sim\mathcal{N}(x,\sigma^{2}I_{n})\), where \(I_{n}\) denotes the identity matrix of size \(n\), and the noise standard deviation \(\sigma>0\) is generally passed as an additional argument to the denoiser ("non-blind" denoising). In this case, the augmented function \(f:(y,\sigma)\in\mathbb{R}^{n}\times\mathbb{R}^{+}_{*}\mapsto\mathbb{R}^{n}\) is said _normalization-equivariant_ if:

\[\forall(y,\sigma)\in\mathbb{R}^{n}\times\mathbb{R}^{+}_{*},\forall\lambda\in \mathbb{R}^{+}_{*},\forall\mu\in\mathbb{R},\ f(\lambda y+\mu,\lambda\sigma)= \lambda f(y,\sigma)+\mu\,,\] (1)

as, according to the laws of statistics, \(\lambda y+\mu\sim\mathcal{N}(\lambda x+\mu,(\lambda\sigma)^{2}I_{n})\). In what follows, we give some well-known examples of traditional denoisers that are _normalization-equivariant_ (see proofs in Appendix C.2).

Noise-reduction filters:The most rudimentary methods for image denoising are the smoothing filters, among which we can mention the averaging filter or the Gaussian filter for the linear filters and the median filter which is nonlinear. These elementary "blind" denoisers all implement a _normalization-equivariant_ function. More generally, one can prove that a linear filter is _normalization-equivariant_ if and only if its coefficients add up to \(1\). In others words, _normalization-equivariant_ linear filters process images by affine combinations of pixels.

Patch-based denoising:The popular N(on)-L(ocal) M(eans) algorithm [5] and its variants [12; 20; 27] consist in computing, for each pixel, an average of its neighboring noisy pixels, weighted by the degree of similarity of the patches to which they belong. In other words, they process images by convex combinations of pixels. More precisely, NLM can be defined as:

\[f_{\mathrm{NLM}}(y,\sigma)_{i}=\frac{1}{W_{i}}\sum_{y_{j}\in\Omega(y_{i})}e^{ \frac{1\,\left\|\,p(y_{i})-p(y_{j})\right\|_{2}^{2}}{h^{2}}}y_{j}\quad\text{ with}\quad W_{i}=\sum_{y_{j}\in\Omega(y_{i})}e^{-\frac{\|\,p(y_{i})-p(y_{j})\, \|_{2}^{2}}{h^{2}}}\] (2)

where \(y_{i}\) denotes the \(i^{th}\) component of vector \(y\), \(\Omega(y_{i})\) is the set of its neighboring pixels, \(p(y_{i})\) represents the vectorized patch centered at \(y_{i}\), and the smoothing parameter \(h\) is proportional to \(\sigma\) as proposed by several authors [4; 12; 29]. Defined as such, \(f_{\mathrm{NLM}}\) is a _normalization-equivariant_ function. More recently, N(on)-L(ocal) Ridge [18] proposes to process images by linear combinations of similar patches and achieves state-of-the-art performance in unsupervised denoising. When restricting the coefficients of the combinations to sum to \(1\), that is imposing affine combination constraints, the resulting algorithm encodes a _normalization-equivariant_ function as well.

TV denoising:Total variation (TV) denoising [37] is finally one of the most famous image denoising algorithm, appreciated for its edge-preserving properties. In its original form [37], a TV denoiser is defined as a function \(f:\mathbb{R}^{n}\times\mathbb{R}^{+}_{*}\mapsto\mathbb{R}^{n}\) that solves the following equality-constrained problem:

\[f_{\mathrm{TV}}(y,\sigma)=\operatorname*{arg\,min}_{x\in\mathbb{R}^{n}}\ \|x\|_{\mathrm{TV}}\quad\text{s.t.}\quad\|y-x\|_{2}^{2}=n\sigma^{2}\] (3)

where \(\|x\|_{\mathrm{TV}}:=\|\nabla x\|_{2}\) is the total variation of \(x\in\mathbb{R}^{n}\). Defined as such, \(f_{\mathrm{TV}}\) is a _normalization-equivariant_ function.

### The case of neural networks

Deep learning hides a subtlety about normalization equivariance that deserves to be highlighted. Usually, the weights of neural networks are learned on a training set containing data all normalized to the same arbitrary interval \([a_{0},b_{0}]\). This training procedure improves the performance and allows for more stable optimization of the model. At inference, unseen data are processed within the interval \([a_{0},b_{0}]\) via a \(a\)-\(b\) linear normalization with \(a_{0}\leq a<b\leq b_{0}\) denoted \(\mathcal{T}_{a,b}\) and defined by:

\[\mathcal{T}_{a,b}:y\mapsto(b-a)\frac{y-\min(y)}{\max(y)-\min(y)}+a\,.\] (4)

Note that this transform is actually the unique linear one with positive slope that exactly bounds the output to \([a,b]\). The data is then passed to the trained network and its response is finally returned to the original range via the inverse operator \(\mathcal{T}_{a,b}^{-1}\). This proven pipeline is actually relevant in light of the following proposition.

**Proposition 1**: \(\forall\,a<b\in\mathbb{R},\forall\,f:\mathbb{R}^{n}\mapsto\mathbb{R}^{m}, \mathcal{T}_{a,b}^{-1}\circ f\circ\mathcal{T}_{a,b}\) _is a normalization-equivariant function._

While normalization-equivariance appears to be solved, a question is still remaining: how to choose the hyperparameters \(a\) and \(b\) for a given function \(f\)? Obviously, a natural choice for neural networks is to take the same parameters \(a\) and \(b\) as in the learning phase whatever the input image is, _i.e._\(a=a_{0}\) and \(b=b_{0}\), but are they really optimal? The answer to this question is generally negative. Figure 1 depicts an example of the phenomenon in image denoising, taken from a real-world application. In this example, the straightforward choice is largely sub-optimal. This suggests that there are always inherent performance leaks for deep neural networks due to the two degrees of freedom induced by the normalization (_i.e._, choice of \(a\) and choice of \(b\)). In addition, this poor conditioning can be a source of confusion and misinterpretation in critical applications.

### Categorizing image denoisers

Table 1 summarizes the equivariance properties of several popular denoisers, either conventional [5; 11; 15; 18; 37; 44] or deep learning-based [24; 45; 46; 47; 26]. Interestingly, if _scale-equivariance_ is generally guaranteed for traditional denoisers, not all of them are equivariant to shifts. In particular, the widely used algorithms DCT [44] and BM3D [11] are sensitive to offsets, mainly because the hard thresholding function at their core is not _shift-equivariant_. Regarding the deep-learning-based networks, only DRUNet [46] is insensitive to scale because it is a bias-free convolutional neural

Figure 1: Influence of normalization for deep-learning-based image denoising. The raw input data is a publicly available real-world noisy image of the _Convallaria_ dataset [36]. “Blind” DnCNN [47] with official pre-trained weights is used for denoising and is applied on four different normalization intervals displayed in red, each of which being included in \([0,1]\) over which it was learned. PSNR is calculated with the average of \(100\) independent noisy static acquisitions of the same sample (called ground truth). Interestingly, the straightforward interval \([0,1]\) does not give the best results. Normalization intervals are (a) \([0,1]\), (b) \([0.08,0.12]\), (c) \([0.48,0,52]\) and (d) \([0.64,0.96]\). In the light of the denoising results \((b)\)-\((c)\) and \((b)\)-\((d)\), DnCNN is neither _shift-equivariant_, nor _scale-equivariant_.

network with only ReLU activation functions [33]. In particular, all transformer models [7; 24; 26; 45], even bias-free, are not _scale-equivariant_ due to their inherent attention-based modules. In the next section, we show how to adapt existing neural architectures to guarantee _normalization-equivariance_ without loss of performance and study the resulting class of parameterized functions \((f_{\theta})\).

## 4 Design of Normalization-Equivariant Networks

### Affine convolutions

To justify the introduction of a new type of convolutional layers, let us study one of the most basic neural network, namely the linear (parameterized) function \(f_{\Theta}:x\in\mathbb{R}^{n}\mapsto\Theta x\), where parameters \(\Theta\) are a matrix of \(\mathbb{R}^{m\times n}\). Indeed, \(f_{\Theta}\) can be interpreted as a dense neural network with no bias, no hidden layer and no activation function. Obviously, \(f_{\Theta}\) is always _scale-equivariant_, whatever the weights \(\Theta\). As for the _shift-equivariance_, a simple calculation shows that:

\[x\mapsto\Theta x\text{ is {shift-equivariant}}\Leftrightarrow\forall x\in\mathbb{R}^{n}, \forall\mu\in\mathbb{R},\Theta(x+\mu\mathbf{1}_{n})=\Theta x+\mu\mathbf{1}_{m} \Leftrightarrow\Theta\mathbf{1}_{n}=\mathbf{1}_{m}\,.\] (5)

Therefore, \(f_{\Theta}\) is _normalization-equivariant_ if and only if each row of matrix \(\Theta\) sums to \(1\). In other words, for the _normalization-equivariance_ to hold, the rows of \(\Theta\) must encode weights of affine combinations. Transposing the demonstration to any convolutional neural network follows from the observation that a convolution from an input layer of size \(H\times W\times C\) to an output layer of size \(H^{\prime}\times W^{\prime}\times C^{\prime}\) can always be represented with a dense connection by vectorizing the input and output layers. The elements of the \(C^{\prime}\) convolutional kernels of size \(k\times k\times C\) each are then stored separately along the rows of the (sparse) transition matrix \(\Theta\) of size \((H^{\prime}\times W^{\prime}\times C^{\prime})\times(H\times W\times C)\). Therefore, a convolutional layer preserves the _normalization-equivariance_ if and only if the weights of the each convolutional kernel sums to \(1\). In the following, we call such convolutional layers "affine convolutions".

In order to guarantee the affine constraint on each convolutional kernel throughout the training phase, one possibility is to "telescope" the circular shifted version of an unconstrained kernel to itself (this way, the sum of the resulting trainable coefficients cancels out) and then add the inverse of the kernel size element-wise as a non-trainable offset. Despite this over-parameterized form (involving an extra degree of freedom), we found this solution to be easier to use in practice. Moreover, it ensures that all coefficients of the affine kernels follow the same law at initialization.

Since _normalization-equivariance_ is preserved through function composition, concatenation and affine combination (see Lemma 2), a (linear) convolutional neural network composed of only affine convolutions with no bias and possibly skip or _affine_ residual connections (trainable affine combination of two layers), is guaranteed to be _normalization-equivariant_, provided that padding is performed with existing features (reflect, replicate or circular padding for example). Obviously, in their current state, these neural networks are of little interest, as linear functions do not encode best-performing functions for many applications, image denoising being no exception. Nevertheless, based on such networks, we show in the next subsection how to introduce nonlinearities without breaking the _normalization-equivariance_.

### Channel-wise sort pooling as a normalization-equivariant alternative to ReLU

The first idea that comes to mind is to apply a nonlinear activation function \(\varphi:\mathbb{R}\mapsto\mathbb{R}\) preserving _normalization-equivariance_ after each affine convolution. In other words, we look for a nonlinear solution \(\varphi\) of the characteristic functional equation of _normalization-equivariant_ functions (see Def. 1) for \(n=1\). Unfortunately, according to Prop. 2 (see proof in Appendix C.1 which is based on Lemma 1), the unique solution is the identity function which is linear. Therefore, activation functions that apply element-wise are to be excluded.

\begin{table}
\begin{tabular}{l c c c c c c|c c c c}  & TV & NLM & NLR & DCT & BM3D & WNNM & DnCNN & NLRN & SwinIR & Restormer & DRUNet \\ \hline Scale & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ & ✗ & ✓ \\ Shift & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ \hline \end{tabular}
\end{table}
Table 1: Equivariance properties of several image denoisers (left: traditional, right: learning-based)

**Proposition 2**: _Let \(\mathrm{NE}(n)\) be the set of normalization-equivariant functions from \(\mathbb{R}^{n}\) to \(\mathbb{R}^{n}\)._

\[\mathrm{NE}(1) =\left\{x\mapsto x\right\}\ \text{and}\] \[\mathrm{NE}(2) =\left\{\left.\left(x_{1},x_{2}\right)\mapsto A\left(\begin{matrix} x_{1}\\ x_{2}\end{matrix}\right)\ \text{if}\ x_{1}\leq x_{2}\ \text{else}\ B\left(\begin{matrix}x_{1}\\ x_{2}\end{matrix}\right)\ \right|\ A,B\in\mathbb{R}^{2\times 2}\ \text{s.t.}\ A \mathbf{1}_{2}=B\mathbf{1}_{2}=\mathbf{1}_{2}\right\}.\]

To find interesting nonlinear functions, one needs to examine multi-dimensional activation functions, _i.e._ ones of the form \(\varphi:\mathbb{R}^{n}\mapsto\mathbb{R}^{m}\) with \(n\geq 2\). In order to preserve the dimensions of the neural layers and to limit the computational costs, we focus on the case \(n=m=2\), meaning that \(\varphi\) processes pre-activated neurons by pairs. According to Prop. 2, the _normalization-equivariant_ functions from \(\mathbb{R}^{2}\) to \(\mathbb{R}^{2}\) are parameterized by two matrices \(A,B\in\mathbb{R}^{2\times 2}\) such that \(A\mathbf{1}_{2}=B\mathbf{1}_{2}=\mathbf{1}_{2}\) and apply a different (affine-constrained) linear mapping depending on whether or not the input is in ascending order. As long as \(A\neq B\), the resulting function is nonlinear and makes it _de facto_ a candidate to replace the conventional one-dimensional activation functions such as the popular ReLU (rectified linear unit) function. Interestingly, when arbitrarily choosing \(A\) and \(B\) to be the permutation matrices of \(\mathbb{R}^{2\times 2}\), the resulting _normalization-equivariant_ function simply reads:

\[\varphi:(x_{1},x_{2})\in\mathbb{R}^{2}\mapsto\begin{pmatrix}\min(x_{1},x_{2}) \\ \max(x_{1},x_{2})\end{pmatrix}\,\] (6)

which is nothing else than the sorting function in \(\mathbb{R}^{2}\). Clearly, it is among the simplest _normalization-equivariant_ nonlinear function from \(\mathbb{R}^{2}\) to \(\mathbb{R}^{2}\) and it is the one we consider as surrogate for the one-dimensional activation functions (choosing other functions, that is considering other choices for \(A\) and \(B\), does not bring improvements in terms of performance in our experiments). More generally, it is easy to show that all the sorting functions of \(\mathbb{R}^{n}\) are _normalization-equivariant_ and are nonlinear as soon as \(n\geq 2\). Note that such sorting operators have been promoted by [2, 9] in totally different contexts for their norm-preserving properties of the backpropagated gradients.

Since the sorting function (6) is to be applied on non-overlapping pairs of neurons, the partitioning of layers needs to be determined. In order not to mix unrelated neurons, we propose to apply this two-dimensional activation function channel-wisely across layers and call this operation "sort pooling" in reference to the max pooling operation, widely used for downsampling, and from which it can be effectively implemented. Figure 2 illustrates the sequence of the two proposed innovations, namely affine convolution followed by channel-wise sort pooling, to replace the traditional scheme "conv+ReLU", while guaranteeing _normalization-equivariance_.

### Encoding adaptive affine filters

Based on Lemma 2, we can formulate the following proposition which tells more about the class of parameterized functions \((f_{\theta})\) encoded by the proposed networks.

Figure 2: Illustration of the proposed alternative for replacing the traditional scheme “convolution + element-wise activation function” in convolutional neural networks: affine convolutions supersede ordinary ones by restricting the coefficients of each kernel to sum to one and the proposed sort pooling patterns introduce nonlinearities by sorting two by two the pre-activated neurons along the channels.

**Proposition 3**: _Let \(f_{\theta}^{\text{NE}}:\mathbb{R}^{n}\mapsto\mathbb{R}^{m}\) be a CNN composed of only:_

* _affine convolution kernels with no bias and where padding is made of existing features,_
* _sort pooling nonlinearities,_
* _possibly skip or affine residual connections, and max or average pooling layers._

_Then, \(f_{\theta}^{\text{NE}}\) is a normalization-equivariant continuous piecewise-linear function with finitely many pieces. Moreover, on each piece represented by the vector \(y_{r}\),_

\[f_{\theta}^{\text{NE}}(y)=A_{\theta}^{y_{r}}y,\;\;\text{with}\;A_{\theta}^{y_ {r}}\in\mathbb{R}^{m\times n}\;\text{such that}\;A_{\theta}^{y_{r}}\mathbf{1}_{n}= \mathbf{1}_{m}\,.\]

In Prop. 3, the subscripts on \(A_{\theta}^{y_{r}}\) serve as a reminder that this matrix depends on the sort pooling activation patterns, which in turn depend on both the input vector \(y\) and the weights \(\theta\). As already revealed for bias-free networks with ReLU [33], \(A_{\theta}^{y_{r}}\) is the Jacobian matrix of \(f_{\theta}^{\text{NE}}\) taken at any point \(y\) in the interior of the piece represented by vector \(y_{r}\). Moreover, as \(A_{\theta}^{y_{r}}\mathbf{1}_{n}=\mathbf{1}_{m}\), the output vector of such networks are locally made of fixed affine combinations of the entries of the input vector. And since a CNN has a limited receptive field centered on each pixel, \(f_{\theta}^{\text{NE}}\) can be thought of as an adaptive filter that produces an estimate of each pixel through a custom affine combination of pixels. By examining these filters in the case of image denoising (see Fig. 3), it becomes apparent that they vary in their characteristics and are intricately linked to the contents of the underlying images. Indeed, these filters are specifically designed to cater to the specific local features of the noisy image: averaging is done over uniform areas without affecting the sharpness of edges. Note that this behavior has already been extensively studied by [33] for unconstrained filters.

The total number of fixed adaptive affine filters depends on the weights \(\theta\) of the network \(f_{\theta}^{\text{NE}}\) and is bounded by \(2^{S}\) where \(S\) represents the total number of sort pooling patterns traversed to get from the receptive filed to its final pixel (assuming no max pooling layers). Obviously, this upper bound grows exponentially with \(S\), suggesting that a limited number of sort pooling operations may generate an extremely large number of filters. Interestingly, if ReLU activation functions where used instead, the upper bound would reach \(2^{2S}\).

## 5 Experimental results

We demonstrate the effectiveness and versatility of the proposed methodology in the case of image denoising. To this end, we modify two well-established neural network architectures for image denoising, chosen for both their simplicity and efficiency, namely DRUNet [46]: a state-of-the-art

Figure 3: Visual comparisons of the generalization capabilities of a _scale-equivariant_ neural network (left) and its _normalization-equivariant_ counterpart (right) for Gaussian noise. Both networks were trained for Gaussian noise at noise level \(\sigma=25\) exclusively. The adaptive filters (rows of \(A_{\theta}^{y_{r}}\) in Prop. 3) are indicated for two particular pixels as well as the sum of their coefficients (note that some weights are negative, indicated in red). The _scale-equivariant_ network tends to excessively smooth out the image when evaluated at a lower noise level, whereas the _normalization-equivariant_ network is more adaptable and considers the underlying texture to a greater extent.

U-Net with residual connections [17]; and FDnCNN, the unpublished flexible variant of the popular DnCNN [47]: a simple feedforward CNN that chains "conv+ReLU" layers with no downsampling, no residual connections and no batch normalization during training [19], and with a tunable noise level map as additional input [48]. We show that adapting these networks to become _normalization-equivariant_ does not adversely affect performance and, better yet, increases their generalization capabilities. For each scenario, we train three variants of the original Gaussian denoising network for grayscale images: _ordinary_ (original network with additive bias), _scale-equivariant_ (bias-free variation with ReLU [33]) and our _normalization-equivariant_ architecture (see Fig. 2). Details about training and implementations can be found in Appendix A and Appendix B. Unless otherwise noted, all results presented in this paper are obtained with DRUNet [46]; similar outcomes can be achieved with FDnCNN [47] architecture (see Appendix D).

Finally, note that both DRUNet [46] and FDnCNN [47] can be trained as "blind" but also as "non-blind" denoisers and thus achieve increased performance, by passing an additional noisemap as input. In the case of additive white Gaussian noise of variance \(\sigma^{2}\), the noisemap is constant equal to \(\sigma\mathbf{1}_{n}\) and the resulting parameterized functions can then be put mathematically under the form \(f_{\theta}:(y,\sigma)\in\mathbb{R}^{n}\times\mathbb{R}^{+}_{*}\mapsto\mathbb{ R}^{n}\). In order to integrate this feature to _normalization-equivariant_ networks as well, a slight modification of the first affine convolutional layer must be made. Indeed, by adapting the proof (5) to the case (1), we can show that the first convolutional layer must be affine with respect to the input image \(y\) only - the coefficients of the kernels acting on the image pixels add up to \(1\) - while the other coefficients of the kernels need not be constrained.

### The proposed architectural modifications do not degrade performance

The performance, assessed in terms of PSNR values, of our _normalization-equivariant_ alternative (see Fig. 2) and of its _scale-equivariant_ and _ordinary_ counterparts is compared in Table 2 for "non-blind" architectures on two popular datasets [32]. We can notice that the performance gap between two different variants is less than 0.05 dB at most for all noise levels, which is not significant. This result suggests that the class of parameterized functions \((f_{\theta})\) currently used in image denoising can drastically be reduced at no cost. Moreover, it shows that it is possible to dispense with activation functions, such as the popular ReLU: nonlinearities can simply be brought by sort pooling patterns. In terms of subjective visual evaluation, we can draw the same conclusion since images produced by two architectural variants inside the training range are hardly distinguishable (see Fig. 3 at \(\sigma=25\)).

### Increased robustness across noise levels

S. Mohan _et al._[33] revealed that bias-free neural networks with ReLU, which are _scale-equivariant_, could much better generalize when evaluated at new noise levels beyond their training range, than their counterparts with bias that systematically overfit. Even if they do not fully elucidate how such networks achieve this remarkable generalization, they suggest that _scale-equivariance_ certainly plays a major role. What about _normalization-equivariance_ then? We have compared the robustness faculties of the three variants of networks when trained at a fixed noise level \(\sigma\) for Gaussian noise. Figure

Figure 4: Comparison of the performance of our _normalization-equivariant_ alternative with its _scale-equivariant_ and _ordinary_ counterparts for Gaussian denoising with the same architecture on Set12 dataset. The vertical blue line indicates the unique noise level on which the “blind” networks were trained exclusively (from left to right: \(\sigma=50\), \(\sigma=25\) and \(\sigma=10\)). In all cases, _normalization-equivariant_ networks generalize much more robustly beyond the training noise level.

4 summarizes the explicit results obtained: _normalization-equivariance_ pushes generalization capabilities of neural networks one step further. While performance is identical to their _scale-equivariant_ counterparts when evaluated at higher noise levels, the _normalization-equivariant_ networks are, however, much more robust at lower noise levels. This phenomenon is also illustrated in Fig. 3.

Demystifying robustnessLet \(x\) be a clean patch of size \(n\), representative of the training set on which a CNN \(f_{\theta}\) was optimized to denoise its noisy realizations \(y=x+\varepsilon\) with \(\varepsilon\sim\mathcal{N}(0,\sigma^{2}I_{n})\) (denoising at a fixed noise level \(\sigma\) exclusively). Formally, we note \(x\in\mathcal{D}\subset\mathbb{R}^{n}\), where \(\mathcal{D}\) is the space of representative clean patches of size \(n\) on which \(f_{\theta}\) was trained. We are interested in the output of \(f_{\theta}\) when it is evaluated at \(x+\lambda\varepsilon\) (denoising at noise level \(\lambda\sigma\)) with \(\lambda>0\). Assuming that \(f_{\theta}\) encodes a _normalization-equivariant_ function, we have:

\[\forall\lambda\in\mathbb{R}_{*}^{+},\forall\mu\in\mathbb{R},\;f_{\theta}(x+ \lambda\varepsilon)=\lambda f_{\theta}((x-\mu)/\lambda+\varepsilon)+\mu\,.\] (7)

The above equality shows how such networks can deal with noise levels \(\lambda\sigma\) different from \(\sigma\): _normalization-equivariance_ simply brings the problem back to the denoising of an implicitly renormalized image patch with fixed noise level \(\sigma\). Note that this artificial change of noise level does not make this problem any easier to solve as the signal-to-noise ratio is preserved by normalization.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & \multicolumn{3}{c}{Set12} & BSD68 \\ \hline \hline \multicolumn{2}{c}{Noise level \(\sigma\)} & 15 / 25 / 50 & 15 / 25 / 50 \\ \hline \multirow{3}{*}{DRUNet [46]} & _ordinary_ & 33.23 / 30.92 / 27.87 & 31.89 / 29.44 / 26.54 \\  & _scale-equiv_ & 33.25 / 30.94 / 27.90 & 31.91 / 29.48 / 26.59 \\  & _norm-equiv_ & 33.20 / 30.90 / 27.85 & 31.88 / 29.45 / 26.55 \\  & _ordinary_ & 32.87 / 30.49 / 27.28 & 31.69 / 29.22 / 26.27 \\ FDnCNN [47] & _scale-equiv_ & 32.85 / 30.49 / 27.29 & 31.67 / 29.20 / 26.25 \\  & _norm-equiv_ & 32.85 / 30.50 / 27.27 & 31.69 / 29.22 / 26.25 \\ \hline \end{tabular}
\end{table}
Table 2: The PSNR (dB) results of “non-blind” deep learning-based methods applied to popular grayscale datasets corrupted by synthetic white Gaussian noise with \(\sigma=15\), \(25\) and \(50\).

Obviously, the denoising result of \(x+\lambda\varepsilon\) will be all the more accurate as \((x-\mu)/\lambda\) is a representative patch of the training set. In other words, if \((x-\mu)/\lambda\) can still be considered to be in \(\mathcal{D}\), then \(f_{\theta}\) should output a consistent denoised image patch. For a majority of methods [46; 47; 48], training is performed within the interval \([0,1]\) and therefore \(x/\lambda\) still belongs generally to \(\mathcal{D}\) for \(1<\lambda<10\) (contraction), but this is much less true for \(\lambda<1\) (stretching) for the reason that it may exceed the bounds of the interval \([0,1]\). This explains why _scale-equivariant_ functions do not generalize well to noise levels lower than their training one. In contrast, _normalization-equivariant_ functions can benefit from the implicit extra adjustment parameter \(\mu\). Indeed, there exists some cases where the stretched patch \(x/\lambda\) is not in \(\mathcal{D}\) but \((x-\mu)/\lambda\) is (see Fig. 4(b)). This is why _normalization-equivariant_ networks are more able to generalize at low noise levels. Note that, based on this argument, _ordinary_ neural networks trained at a fixed noise level \(\sigma\) can also be used to denoise images at noise level \(\lambda\sigma\), provided that a correct normalization is done beforehand [42]. However, this time the normalization is explicit: the exact scale factor \(\lambda\), and possibly the shift \(\mu\), must be known (see Fig. 4(a)).

It turns out that this theoretical argument is valid for a wide range of noise types, not only Gaussian noise. Indeed, the same argument holds for any additive noise \(\varepsilon\) that possesses the scaling property: \(\lambda\varepsilon\) belongs to the same family of probability distributions as \(\varepsilon\) (e.g., Gaussian, uniform, Laplace or even Rayleigh noise which is not zero-mean). By the way, the authors of [33] had already verified the noise generalization capabilities of _scale-equivariant_ networks for uniform noise in addition to Gaussian noise, without fully elucidating why it works. In Appendix D, we checked experimentally that "blind" _normalization-equivariant_ networks trained on additive uniform, Laplace or Rayleigh noise at a single noise level are much more robust at unseen noise levels than their _scale-equivariant_ and _ordinary_ counterparts.

## 6 Conclusion and perspectives

In this work, we presented an original approach to adapt the architecture of existing neural networks so that they become _normalization-equivariant_, a property highly desirable and expected in many applications such that image denoising. We argue that the classical pattern "conv+ReLU" can be favorably replaced by the two proposed innovations: affine convolutions that ensure that all coefficients of the convolutional kernels sum to one; and channel-wise sort pooling nonlinearities as a substitute for all activation functions that apply element-wise, including ReLU or sigmoid functions. Despite these two important architectural changes, we show that the performance of these alternative networks is not affected in any way. On the contrary, thanks to their better-conditioning, they benefit, in the context of image denoising, from an increased interpretability and especially robustness to variable noise levels both in practice and in theory.

## Limitations

We would like to mention that the proposed architectural modifications for enforcing _normalization-equivariance_ require a longer training for achieving comparable performance with its original counterparts (see Appendix B), and may be incompatible with some specific network layers such as batch-norm [19] or attention-based modules [24; 26; 7; 45]. Moreover, our method has shown its potential mainly to image denoising as it stands, even though in principle _normalization-equivariance_ may be applicable and helpful in other tasks as well (see preliminary results about image classification in Appendix D). Discovering similar advantages of _normalization-equivariance_ in other computer vision tasks, possibly related to outlier robustness, is an interesting avenue of research for future work.

## Acknowledgments and Disclosure of Funding

This work was supported by Bipfrance agency (funding) through the LiChIE contract. Computations were performed on the Inria Rennes computing grid facilities partly funded by France-BioImaging infrastructure (French National Research Agency - ANR-10-INBS-04-07, "Investments for the future"). We would like to thank R. Fraisse (Airbus) for fruitful discussions.

## References

* [1]E. Agustsson and R. Timofte (2017) NTIRE 2017 Challenge on single image super-resolution: dataset and study. In Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Vol., pp. 1122-1131. Cited by: SS1.
* [2]C. Anil, J. Lucas, and R. Grosse (2019) Sorting out Lipschitz function approximation. In International Conference on Machine Learning (ICML), Vol., pp. 291-301. Cited by: SS1.
* [3]S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P. Mailoa, M. Kornbluth, N. Molinari, T. E. Smidt, and B. Kozinsky (2022) E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature Communications13 (1), pp. 2453. Cited by: SS1.
* [4]A. Buades, B. Coll, and J. Morel (2005) A new of image denoising algorithms, with a new one. Multiscale Modeling & Simulation4 (2), pp. 490-530. Cited by: SS1.
* [5]A. Buades, B. Coll, and J. Morel (2005) A review of image denoising algorithms with a new one. Multiscale Modeling & Simulation4 (2), pp. 490-530. Cited by: SS1.
* [6]G. Bokman, F. Kahla, and A. Flinth (2022) ZZ-Net: a universal rotation equivariant architecture for 2D point clouds. In Conference on Computer Vision and Pattern Recognition (CVPR), Vol., pp. 10966-10975. Cited by: SS1.
* [7]L. Chen, X. Chu, X. Zhang, and J. Sun (2022) Simple baselines for image restoration. In European Conference on Computer Vision (ECCV), Vol., pp. 17-33. Cited by: SS1.
* [8]Y. Chen and T. Pock (2017) Trainable nonlinear reaction diffusion: a flexible framework for fast and effective image restoration. IEEE Transactions on Pattern Analysis and Machine Intelligence39 (6), pp. 1256-1272. Cited by: SS1.
* [9]A. Chernodub and D. Nowicki (2016) Norm-preserving orthogonal permutation linear unit activation functions (OPLU). arXiv preprint arXiv:1604.02313. Cited by: SS1.
* [10]T. Cohen and M. Welling (2016) Group equivariant convolutional networks. In International Conference on Machine Learning (ICML), Vol., pp. 2990-2999. Cited by: SS1.
* [11]K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian (2007) Image denoising by sparse 3-D transform-domain collaborative filtering. IEEE Transactions on Image Processing16 (8), pp. 2080-2095. Cited by: SS1.
* [12]V. Duval, J. Aujol, and Y. Gousseau (2011) A bias-variance approach for the nonlocal means. SIAM Journal on Imaging Sciences4 (2), pp. 760-788. Cited by: SS1.
* [13]F. Fuchs, D. Worrall, V. Fischer, and M. Welling (2020) SE(3)-Transformers: 3D roto-translation equivariant attention networks. In Advances in Neural Information Processing Systems (NeurIPS), Vol., pp. 1970-1981. Cited by: SS1.
* [14]K. Fukushima (1980) Neocognitron: a self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics36 (4), pp. 193-202. Cited by: SS1.
* [15]S. Gu, L. Zhang, W. Zuo, and X. Feng (2014) Weighted nuclear norm minimization with application to image denoising. In Conference on Computer Vision and Pattern Recognition (CVPR), Vol., pp. 2862-2869. Cited by: SS1.
* [16]D. K. Gupta, D. Arya, and E. Gavves (2021) Rotation equivariant siamese networks for tracking. In Conference on Computer Vision and Pattern Recognition (CVPR), Vol., pp. 12357-12366. Cited by: SS1.
* [17]K. He, X. Zhang, S. Ren, and J. Sun (2016) Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), Vol., pp. 770-778. Cited by: SS1.
* [18]S. Herbreteau and C. Kervrann (2022) Towards a unified view of unsupervised non-local methods for image denoising: the NL-Ridge approach. In IEEE International Conference on Image Processing (ICIP), Vol., pp. 3376-3380. Cited by: SS1.
* [19]S. Ioffe and C. Szegedy (2015) Batch normalization: accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), Vol., pp. 448-456. Cited by: SS1.

[MISSING_PAGE_POST]

* [22] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations (ICLR)_, 2015.
* [23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. _Neural Computation_, 1(4):541-551, 1989.
* [24] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte. SwinIR: Image restoration using Swin Transformer. In _International Conference on Computer Vision Workshops (ICCVW)_, pages 1833-1844, 2021.
* [25] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced deep residual networks for single image super-resolution. In _Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pages 1132-1140, 2017.
* [26] D. Liu, B. Wen, Y. Fan, C. C. Loy, and T. S. Huang. Non-local recurrent network for image restoration. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 31, 2018.
* [27] C. Louchet and L. Moisan. Total variation as a local filter. _SIAM Journal on Imaging Sciences_, 4(2):651-694, 2011.
* [28] K. Ma, Z. Duanmu, Q. Wu, Z. Wang, H. Yong, H. Li, and L. Zhang. Waterloo exploration database: New challenges for image quality assessment models. _IEEE Transactions on Image Processing_, 26(2):1004-1016, 2017.
* [29] J. V. Manjon, J. Carbonell-Caballero, J. J. Lull, G. Garcia-Marti, L. Marti-Bonmatti, and M. Robles. MRI denoising using non-local means. _Medical image analysis_, 12(4):514-523, 2008.
* [30] X. Mao, C. Shen, and Y.-B. Yang. Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections. In _Advances in Neural Information Processing Systems (NIPS)_, volume 29, 2016.
* [31] D. Marcos, M. Volpi, N. Komodakis, and D. Tuia. Rotation equivariant vector field networks. In _International Conference on Computer Vision (ICCV)_, pages 5058-5067, 2017.
* [32] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In _International Conference on Computer Vision (ICCV)_, volume 2, pages 416-423 vol.2, 2001.
* [33] S. Mohan, Z. Kadkhodaie, E. P. Simoncelli, and C. Fernandez-Granda. Robust and interpretable blind image denoising via bias-free convolutional neural networks. In _International Conference on Learning Representations (ICLR)_, 2020.
* [34] A. Nokland and L. H. Eidnes. Training neural networks with local error signals. In _International Conference on Machine Learning (ICML)_, pages 4839-4850, 2019.
* [35] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 32, 2019.
* [36] M. Prakash, M. Lalit, P. Tomancak, A. Krul, and F. Jug. Fully unsupervised probabilistic Noise2Void. In _International Symposium on Biomedical Imaging (ISBI)_, pages 154-158, 2020.
* [37] L. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. _Physica D: Nonlinear Phenomena_, 60:259-268, 1992.
* [38] V. G. Satorras, E. Hoogeboom, and M. Welling. E(n) equivariant graph neural networks. In _International Conference on Machine Learning (ICML)_, volume 139, pages 9323-9332, 2021.
* [39] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1874-1883, 2016.
* [40] N. Thomas, T. Smidt, S. Kearnes, L. Yang, L. Li, K. Kohlhoff, and P. Riley. Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds. _arXiv preprint arXiv:1802.08219_, 2018.
* [41] B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, and M. Welling. Rotation equivariant CNNs for digital pathology. In _Medical Image Computing and Computer Assisted Intervention (MICCAI)_, pages 210-218, 2018.

* [42] Y.-Q. Wang and J.-M. Morel. Can a single image denoising neural network handle all levels of Gaussian noise? _IEEE Signal Processing Letters_, 21(9):1150-1153, 2014.
* [43] M. Weiler, F. A. Hamprecht, and M. Storath. Learning steerable filters for rotation equivariant CNNs. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 849-858, 2018.
* [44] G. Yu and G. Sapiro. DCT image denoising: a simple and effective image denoising algorithm. _Image Processing On Line_, 1:292-296, 2011.
* [45] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M. Yang. Restormer: Efficient transformer for high-resolution image restoration. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5718-5729, 2022.
* [46] K. Zhang, Y. Li, W. Zuo, L. Zhang, L. Van Gool, and R. Timofte. Plug-and-Play image restoration with deep denoiser prior. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(10):6360-6376, 2022.
* [47] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond a Gaussian denoiser: residual learning of deep CNN for image denoising. _IEEE Transactions on Image Processing_, 26(7):3142-3155, 2017.
* [48] K. Zhang, W. Zuo, and L. Zhang. FFDNet: Toward a fast and flexible solution for CNN-based image denoising. _IEEE Transactions on Image Processing_, 27(9):4608-4622, 2018.

Description of the denoising architectures and implementation

### Description of models

DRUNet:DRUNet [46] is a U-Net architecture, and as such has an encoder-decoder type pathway, with residual connections [17]. Spatial downsampling is performed using \(2\times 2\) convolutions with stride \(2\), while spatial upsampling leverages \(2\times 2\) transposed convolutions with stride \(2\) (which is equivalent to a \(1\times 1\) sub-pixel convolution [39]). The number of channels in each layer from the first scale to the fourth scale are \(64\), \(128\), \(256\) and \(512\), respectively. Each scale is composed of 4 successive residual blocks "\(3\times 3\) conv + ReLU + \(3\times 3\) conv".

FDnCNN:FDnCNN [47] is the unpublished flexible variant of the popular DnCNN [47]. It consists of 20 successive \(3\times 3\) convolutional layers with \(64\) channels each and ReLU nonlinearities. As opposed to DnCNN, FDnCNN does not use neither batch normalization [19] for training, nor residual connections [17] and can handle an optional noisemap (concatenated with the input noisy image). Note that this architecture does not use downsampling or upsampling. Finally, the authors [47] recommend to train it by minimizing the \(\ell_{1}\) loss instead of the mean squared error (MSE).

### Description of variants

Ordinary:The _ordinary_ variant is built by appending additive constant ("bias") terms after each convolution of the original architecture. Note that the original FDnCNN [47] model is already in the _ordinary_ mode.

Scale-equivariant:Since both models (DRUNet and FDnCNN) use only ReLU activation functions, removing all additive constant ("bias") terms is sufficient to ensure _scale-equivariance_[33]. Note that the original DRUNet [46] model is already in the _scale-equivariant_ mode.

Normalization-equivariant:All convolutions are replaced by the proposed affine-constrained convolutions without "bias" and with reflect padding, and the proposed channel-wise sort pooling patterns supersede ReLU nonlinearities. Moreover, classical residual connections are replaced by _affine_ residual connections (the sum of two layers \(l_{1}\) and \(l_{2}\) is replaced by their affine combination \((1-t)l_{1}+tl_{2}\) where \(t\) is a trainable scalar parameter).

### Practical implementation of normalization-equivariant networks

The channel-wise sort pooling operations can be efficiently implemented by concatenating the sub-layer obtained with channel-wise one-dimensional max pooling with kernel size \(2\) and its counterpart obtained with min pooling. Note that intertwining these two sub-layers to comply with the original definition is not necessary in practice (although performed anyway in our implementation), since the order of the channels in a CNN is arbitrary.

Regarding the implementation of affine convolutions for training, each unconstrained kernel can be in practice "telescoped" with its circular shifted version (this way, the sum of the resulting trainable coefficients cancels out) and then the inverse of the kernel size is added element-wise as a non-trainable offset. Despite this over-parameterized form (involving an extra degree of freedom), we found this solution to be more easy to use in practice. Moreover, it ensures that all coefficients of the affine kernels follow the same law at initialization. Another possibility is to set an arbitrary coefficient of the kernel (the last one for instance) equal to one minus the sum of all the other coefficients. Note that the solution consisting in dividing each kernel coefficient by the sum of all the other coefficients does not work because it generates numerical instabilities as the divisor may be zero, or close to zero.

All our implementations are written in Python and are based on the PyTorch library [35]. The code and pre-trained models can be downloaded here: https://github.com/sherbret/normalization_equivariant_nn/.

## Appendix B Description of datasets and training details

We use the same large training set as in [46] for all the models and all the experiments, composed of \(8,694\) images, including \(400\) images from the Berkeley Segmentation Dataset BSD400 [32], \(4,774\) images from the Waterloo Exploration Database [28], \(900\) images from the DIV2K dataset [1], and \(2,750\) images from the Flickr2K dataset [25]. This training set is augmented via random vertical and horizontal flips and random \(90^{\circ}\) rotations. The dataset BSD32 [32], composed of the \(32\) images, is used as validation set to control training and select the best model at the end. Finally, the two datasets Set12 and BSD68 [32], strictly disjoint from the training and validation sets, are used for testing.

All the models \(f_{\theta}\) are optimized by minimizing the average reconstruction error between the denoised images \(\hat{x}=f_{\theta}(x+\varepsilon)\), where \(\varepsilon\sim\mathcal{N}(0,\sigma^{2}I_{n})\), and ground-truths \(x\) with Adam algorithm [22]. For "non-blind" models, the noise level \(\sigma\) is randomly chosen from \([1,50]\) during training. The training parameters, specific to each model and its variants, are guided by the instructions of the original papers [46; 47], to the extent possible, and are summarized in Table 3. Note that each training iteration consists in a gradient pass on a batch composed of patches randomly cropped from training images. _Normalization-equivariant_ variants need a longer training and always use a constant learning rate (speed improvements are however certainly possible by adapting the learning rate throughout optimization, but we did not investigated much about it). Furthermore, contrary to [46] where the \(\ell_{1}\) loss function is recommended to achieve better performance, supposedly due to its outlier robustness properties, we obtained slightly better results with the usual mean squared error (MSE) loss when dealing with _normalization-equivariant_ networks. Training was performed with a Quadro RTX 6000 GPU.

In Table B, we compare the computational costs of different variants for training and inference. Interestingly, the computational cost on GPU for training is much more sensitive to the "affine mode" (involving affine-constrained convolutions with reflect padding and affine residual connection) than to sort pooling nonlinearities, while it is the opposite for inference. All in all, for gaining _normalization-equivariance_, the learning and inference time is almost doubled for the DRUNet architecture [46] on GPU. Note however that we do not claim to have the most optimized implementation and there

\begin{table}
\begin{tabular}{c c c c c c}  & & Affine & SortPool & Backward pass \(\downarrow\) & Inference pass \(\downarrow\) \\ \cline{3-6}  & _(scale-equiv)_ & ✗ & ✗ & 0.229 & 0.067 \\  & & ✗ & ✓ & 0.268 & 0.102 \\  & & ✓ & ✗ & 0.344 & 0.083 \\  & _(norm-equiv)_ & ✓ & ✓ & 0.386 & 0.122 \\ \cline{2-6}  & & \multicolumn{2}{c}{Variant} & \multicolumn{2}{c}{Backward pass \(\downarrow\)} & Inference pass \(\downarrow\) \\ \cline{3-6}  & _ordinary_ & & 38.4 & 12.5 \\  & _scale-equiv_ & & 37.4 & 12.2 \\  & _norm-equiv_ & & 45.3 & 14.3 \\ \hline \end{tabular}

* Affine: affine-constrained convolutions with reflect padding and affine residual connections. SortPool: channel-wise sort pooling nonlinearities instead of ReLU.

\end{table}
Table 4: Execution time (in seconds) comparison on a training batch of size \(16\times 1\times 128\times 128\) for different variants of the same DRUNet architecture [46] (GPU: Quadro RTX 6000, CPU: 2,3 GHz Intel Core i7). The difference in speed between variants is more pronounced on GPU than on CPU.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \multicolumn{2}{c}{Model} & \multicolumn{1}{c}{\begin{tabular}{c} Batch \\ size \\ \end{tabular} } & \multicolumn{1}{c}{\begin{tabular}{c} Patch \\ size \\ \end{tabular} } & \multicolumn{1}{c}{\begin{tabular}{c} Loss \\ function \\ \end{tabular} } & \multicolumn{1}{c}{\begin{tabular}{c} Learning \\ rate \\ \end{tabular} } & \multicolumn{1}{c}{
\begin{tabular}{c} Number of \\ iterations \\ \end{tabular} } \\ \hline \multirow{4}{*}{DRUNet [46]} & _ordinary_ & \(16\) & \(128\times 128\) & \(\ell_{1}\) & \(1e\)-\(4\)* & \(800,000\) \\  & _scale-equiv_ & \(16\) & \(128\times 128\) & \(\ell_{1}\) & \(1e\)-\(4\)* & \(800,000\) \\  & _norm-equiv_ & \(16\) & \(128\times 128\) & MSE & \(1e\)-\(4\) & \(1,800,000\) \\ \cline{1-1} \cline{3-6}  & _ordinary_ & \(128\) & \(70\times 70\) & \(\ell_{1}\) & \(1e\)-\(4\) & \(500,000\) \\ \cline{1-1} \cline{3-6}  & _scale-equiv_ & \(128\) & \(70\times 70\) & \(\ell_{1}\) & \(1e\)-\(4\) & \(500,000\) \\ \cline{1-1} \cline{3-6}  & _norm-equiv_ & \(128\) & \(70\times 70\) & MSE & \(1e\)-\(4\) & \(900,000\) \\ \hline \end{tabular}
\end{table}
Table 3: Training parameters. * indicates that it is divided by half every \(100,000\) iterations.

is probably room for improvement. Surprisingly, the difference in speed between all variants is much less pronounced on CPU. In particular, the inference pass takes only about \(20\%\) longer for the _normalization-equivariant_ DRUNet on CPU.

## Appendix C Mathematical proofs for normalization-equivariant neural networks

### Proofs of Lemmas and Propositions

**Lemma 1**(Characterizations)

**Proof:** For each type of equivariance, both existence and uniqueness of \(f\) must be proven. Let \(\mathbf{0}_{n}\) be the zero vector of \(\mathbb{R}^{n}\) and \((y_{x})_{x\in\mathcal{C}}\) the values that \(f\) takes on its characteristic set \(\mathcal{C}\).

_Scale-equivariance:_

* Uniqueness: Let \(f\) and \(g\) two _scale-equivariant_ functions such that \(\forall x\in\mathcal{S},f(x)=g(x)\). First of all, for any _scale-equivariant_ function \(h\), \(h(\mathbf{0}_{n})=h(2\cdot\mathbf{0}_{n})=2h(\mathbf{0}_{n})\), hence \(h(\mathbf{0}_{n})=\mathbf{0}_{m}\). Therefore, \(f(\mathbf{0}_{n})=g(\mathbf{0}_{n})=\mathbf{0}_{m}\). Let \(x\in\mathbb{R}^{n}\setminus\{\mathbf{0}_{n}\}\). As \(\frac{x}{\|x\|}\in\mathcal{S}\), we have \(f(\frac{x}{\|x\|})=g(\frac{x}{\|x\|})\Rightarrow\frac{1}{\|x\|}f(x)=\frac{1}{ \|x\|}g(x)\Rightarrow f(x)=g(x)\). Finally, \(f=g\).
* Existence: Let \(f:x\in\mathbb{R}^{n}\mapsto\left\{\begin{array}{ll}\|x\|\cdot y_{\frac{x}{ \|x\|}}&\mbox{if $x\neq\mathbf{0}_{n}$}\\ \mathbf{0}_{m}&\mbox{otherwise}\end{array}\right..\) Note that \(\forall x\in\mathcal{S},f(x)=y_{x}\). Let \(x\in\mathbb{R}^{n}\) and \(\lambda\in\mathbb{R}_{*}^{+}\). If \(x\neq\mathbf{0}_{n}\), \(f(\lambda x)=\|\lambda x\|\cdot y_{\frac{x}{\|x\|}}=\lambda\|x\|\cdot y_{\frac {x}{\|x\|}}=\lambda f(x)\) and if \(x=\mathbf{0}_{n}\)\(f(\lambda x)=\mathbf{0}_{m}=\lambda f(x)\), hence \(f\) is _scale-equivariant_.

_Shift-equivariance:_

* Uniqueness: Let \(f\) and \(g\) two _shift-equivariant_ functions such that \(\forall x\in\mathrm{Span}(\mathbf{1}_{n})^{\perp},f(x)=g(x)\). Let \(x\in\mathbb{R}^{n}\). By orthogonal decomposition of \(\mathbb{R}^{n}\) into \(\mathrm{Span}(\mathbf{1}_{n})^{\perp}\) and \(\mathrm{Span}(\mathbf{1}_{n})\): \[\exists!\;(x_{1},x_{2})\in\mathrm{Span}(\mathbf{1}_{n})^{\perp}\times\mathrm{ Span}(\mathbf{1}_{n}),\;x=x_{1}+x_{2}\,.\] Then, \(f(x)=f(x_{1}+x_{2})=f(x_{1})+x_{2}=g(x_{1})+x_{2}=g(x_{1}+x_{2})=g(x)\).
* Existence: Let \(f:x\in\mathbb{R}^{n}\mapsto y_{x_{1}}+x_{2}\), where \(x=x_{1}+x_{2}\) is the unique decomposition such that \(x_{1}\in\mathrm{Span}(\mathbf{1}_{n})^{\perp}\) and \(x_{2}\in\mathrm{Span}(\mathbf{1}_{n})\). Note that \(\forall x\in\mathrm{Span}(\mathbf{1}_{n})^{\perp},f(x)=y_{x}\). Let \(x\in\mathbb{R}^{n}\) and \(\mu\in\mathbb{R}\). \(f(x+\mu)=y_{x_{1}}+x_{2}+\mu\mathbf{1}_{m}=f(x)+\mu\) as if \(x\) orthogonally decomposes into \(x_{1}+x_{2}\) with \(x_{1}\in\mathrm{Span}(\mathbf{1}_{n})^{\perp}\) and \(x_{2}\in\mathrm{Span}(\mathbf{1}_{n})\), then \(x+\mu\) orthogonally decomposes into \(x_{1}+(x_{2}+\mu\mathbf{1}_{m})\). \(f\) is then _shift-equivariant_.

_Normalization-equivariance:_

* Uniqueness: Let \(f\) and \(g\) two _normalization-equivariant_ functions such that \(\forall x\in\mathcal{S}\cap\mathrm{Span}(\mathbf{1}_{n})^{\perp},f(x)=g(x)\). First, as \(f\) and \(g\) are _a fortiori scale-equivariant_, \(f(\mathbf{0}_{n})=g(\mathbf{0}_{n})=\mathbf{0}_{m}\). Let \(x\in\mathbb{R}^{n}\setminus\{\mathbf{0}_{n}\}\). By orthogonal decomposition of \(\mathbb{R}^{n}\) into \(\mathrm{Span}(\mathbf{1}_{n})^{\perp}\) and \(\mathrm{Span}(\mathbf{1}_{n})\): \[\exists!\;(x_{1},x_{2})\in\mathrm{Span}(\mathbf{1}_{n})^{\perp}\times\mathrm{ Span}(\mathbf{1}_{n}),\;x=x_{1}+x_{2}\,.\] If \(x_{1}=\mathbf{0}_{n}\), \(f(x)=f(\mathbf{0}_{n}+x_{2})=f(\mathbf{0}_{n})+x_{2}=\mathbf{0}_{m}+x_{2}=x_{2}\). Likewise, \(g(x)=x_{2}\), hence \(f(x)=g(x)\). Else, if \(x_{1}\neq\mathbf{0}_{n},f(x)=f(x_{1}+x_{2})=f(x_{1})+x_{2}=\|x_{1}\|f(\frac{x_{ 1}}{\|x_{1}\|})+x_{2}=\|x_{1}\|g(\frac{x_{1}}{\|x_{1}\|})+x_{2}=g(x_{1})+x_{2}= g(x_{1}+x_{2})=g(x)\), as \(\frac{x_{1}}{\|x_{1}\|}\in\mathcal{S}\cap\mathrm{Span}(\mathbf{1}_{n})^{\perp}\). Finally, \(f=g\).
* Existence: Let \(f:x\in\mathbb{R}^{n}\mapsto\left\{\begin{array}{ll}\|x_{1}\|\cdot y_{\frac{x_ {1}}{\|x_{1}\|}}+x_{2}&\mbox{if $x_{1}\neq\mathbf{0}_{n}$}\\ x_{2}&\mbox{otherwise}\end{array}\right.\), where \(x=x_{1}+x_{2}\) is the unique decomposition such that \(x_{1}\in\mathrm{Span}(\mathbf{1}_{n})^{\perp}\) and \(x_{2}\in\mathrm{Span}(\mathbf{1}_{n})\). Note that \(\forall x\in\mathcal{S}\cap\mathrm{Span}(\mathbf{1}_{n})^{\perp},f(x)=y_{x}\). Let \(x\in\mathbb{R}^{n}\), \(\lambda\in\mathbb{R}_{*}^{+}\) and \(\mu\in\mathbb{R}\). \(x\) decomposesorthogonally into \(x_{1}+x_{2}\) with \(x_{1}\in\operatorname{Span}(\mathbf{1}_{n})^{\perp}\) and \(x_{2}\in\operatorname{Span}(\mathbf{1}_{n})\), and we have \(f(\lambda x+\mu)=f(\lambda x_{1}+(\lambda x_{2}+\mu))\), where \(\lambda x_{1}+(\lambda x_{2}+\mu)\) is the orthogonal decomposition of \(\lambda x+\mu\) into \(\operatorname{Span}(\mathbf{1}_{n})^{\perp}\) and \(\operatorname{Span}(\mathbf{1}_{n})\). If \(x_{1}=\mathbf{0}_{n}\), then \(\lambda x_{1}=\mathbf{0}_{n}\) and \(f(\lambda x+\mu)=\lambda x_{2}+\mu=\lambda f(x)+\mu\). Else, if \(x_{1}\neq\mathbf{0}_{n}\), then \(\lambda x_{1}\neq\mathbf{0}_{n}\), and \(f(\lambda x+\mu)=\|\lambda x_{1}\|\cdot y_{\frac{\lambda x_{1}}{\|\lambda x_{ 1}\|}}+(\lambda x_{2}+\mu)=\lambda(\|x_{1}\|\cdot y_{\frac{x_{1}}{\|x_{1}\|}}+ x_{2})+\mu=\lambda f(x)+\mu\). Finally, \(f\) is _normalization-equivariant_. 

**Lemma 2** (Operations preserving equivariance): **Proof:** Let \(x\in\mathbb{R}^{n}\), \(\lambda\in\mathbb{R}^{+}_{*}\) and \(\mu\in\mathbb{R}\).

* If \(f\) and \(g\) are both _scale-equivariant_, \((f\circ g)(\lambda x)=f(g(\lambda x))=f(\lambda g(x))=\lambda f(g(x))=\lambda( f\circ g)(x)\) and if they are both _shift-equivariant_, \((f\circ g)(x+\mu)=f(g(x+\mu))=f(g(x)+\mu)=f(g(x))+\mu=(f\circ g)(x)+\mu\).
* Let \(h:x\mapsto(f(x)^{\top}\,g(x)^{\top})^{\top}\). If \(f\) and \(g\) are both _scale-equivariant_, \(h(\lambda x)=(f(\lambda x)^{\top}\,g(x)^{\top})^{\top}=(\lambda f(x)^{\top}\,g (x)^{\top})^{\top}=\lambda h(x)\) and if they are both _shift-equivariant_, \(h(\lambda x)=h(x+\mu)=(f(x+\mu)^{\top}\,g(x+\mu)^{\top})^{\top}=(f(x)^{\top}+\mu \quad g(x)^{\top}+\mu)^{\top}=h(x)+\mu\).
* Let \(t\in\mathbb{R}\) and \(h:x\mapsto(1-t)f+tg\). If \(f\) and \(g\) are both _scale-equivariant_, \(h(\lambda x)=(1-t)f(\lambda x)+tg(\lambda x)=(1-t)\lambda f(x)+t\lambda g(x)= \lambda((1-t)f(x)+tg(x))=\lambda h(x)\) and if they are both _shift-equivariant_, \(h(x+\mu)=(1-t)f(x+\mu)+tg(x+\mu)=(1-t)(f(x)+\mu)+t(g(x)+\mu)=(1-t)f(x)+tg(x)+(1- t)\mu+t\mu=h(x)+\mu\). \(\Box\)

**Proposition 1**: **Proof:** Let \(a<b\in\mathbb{R}\), \(f:\mathbb{R}^{n}\mapsto\mathbb{R}^{m}\), \(x\in\mathbb{R}^{n}\), \(\lambda\in\mathbb{R}^{+}_{*}\) and \(\mu\in\mathbb{R}\).

We have \(\mathcal{T}_{a,b}(\lambda x+\mu)=(b-a)\frac{\lambda x+\mu-\min(\lambda x+\mu)} {\max(\lambda+\mu)-\min(\lambda x+\mu)}+a=(b-a)\frac{x-\min(x)}{\max(x)-\min( x)}+a=\mathcal{T}_{a,b}(x)\) (_i.e._\(\mathcal{T}_{a,b}\) is _normalization-invariant_). \(\mathcal{T}_{a,b}^{-1}\) denotes the inverse transformation intricately linked to the input \(x\) of \(\mathcal{T}_{a,b}\) (note that this is an improper notation as \(\mathcal{T}_{a,b}\) is not bijective). Thus, if \(x\) is the input of \(\mathcal{T}_{a,b}\), then \(\mathcal{T}_{a,b}^{-1}:y\mapsto(\max(x)-\min(x))\frac{y-a}{b-a}+\min(x)\).

\[(\mathcal{T}_{a,b}^{-1}\circ f\circ\mathcal{T}_{a,b})(\lambda x+\mu) =\frac{\max(\lambda x+\mu)-\min(\lambda x+\mu)}{b-a}\left((f\circ \mathcal{T}_{a,b})(\lambda x+\mu)-a\right)+\min(\lambda x+\mu)\,,\] \[=\lambda\frac{\max(x)-\min(x)}{b-a}\left((f\circ\mathcal{T}_{a,b} )(\lambda x+\mu)-a\right)+\lambda\min(x)+\mu\,,\] \[=\lambda\left(\frac{\max(x)-\min(x)}{b-a}\left((f\circ\mathcal{T} _{a,b})(x)-a\right)+\min(x)\right)+\mu\,,\] \[=\lambda(\mathcal{T}_{a,b}^{-1}\circ f\circ\mathcal{T}_{a,b})(x)+ \mu\,.\]

Finally, \(\mathcal{T}_{a,b}^{-1}\circ f\circ\mathcal{T}_{a,b}\) is _normalization-equivariant_. \(\Box\)

**Proposition 2**: **Proof:** Let \(\operatorname{id}:x\in\mathbb{R}\mapsto x\) be the identity function. \(\operatorname{id}\) is a _normalization-equivariant_ function so \(\{x\mapsto x\}\subseteq\operatorname{NE}(1)\). Reciprocally, let \(f\in\operatorname{NE}(1)\). By _scale-equivariance_, \(f(0)=f(2\times 0)=2f(0)\), hence \(f(0)=0\). By _shift-equivariance_, \(\forall x\in\mathbb{R}\), \(f(x)=f(x+0)=f(0)+x=x\), hence \(f=\operatorname{id}\). Finally, \(\operatorname{NE}(1)\subseteq\{x\mapsto x\}\), hence \(\operatorname{NE}(1)=\{x\mapsto x\}\). Note that it is coherent with Lemma 1 which states that \(f\) is entirely determined by its values on \(\mathcal{S}\cap\operatorname{Span}(\mathbf{1}_{n})^{\perp}\), which reduces to the empty set for \(n=1\).

Let \(F=\left\{\begin{array}{ll}(x_{1},x_{2})\mapsto A\begin{pmatrix}x_{1}\\ x_{2}\end{pmatrix}\text{ if }x_{1}\leq x_{2}\text{ else }B\begin{pmatrix}x_{1}\\ x_{2}\end{pmatrix}\bigg{|}\ A,B\in\mathbb{R}^{2\times 2}\text{ s.t. }A\mathbf{1}_{2}=B\mathbf{1}_{2}=\mathbf{1}_{2}\right\}\) and \(f\in F\). Let \(x\in\mathbb{R}^{2}\), \(\lambda>0\) and \(\mu\in\mathbb{R}\).

\[f(\lambda x+\mu)=\left\{\begin{array}{ll}A(\lambda x+\mu)&\text{if }\lambda x_{1}+\mu\leq \lambda x_{2}+\mu&=\left\{\begin{array}{ll}\lambda Ax+\mu&\text{if }x_{1}\leq x_{2}\\ \lambda Bx+\mu&\text{otherwise}\end{array}\right.&=\lambda f(x)+\mu,\end{array}\right.\]

hence \(f\in\operatorname{NE}(2)\).

Reciprocally, let \(f\in\mathrm{NE}(2)\). For \(n=2\) and when considering the Euclidean distance, \(\mathcal{S}\cap\mathrm{Span}(\mathbf{1}_{n})^{\perp}=\{-u,u\}\) with \(u=(-1/\sqrt{2},1/\sqrt{2})\). Let

\[A=\frac{1}{u_{2}-u_{1}}\begin{pmatrix}u_{2}-f(u)_{1}&f(u)_{1}-u_{1}\\ u_{2}-f(u)_{2}&f(u)_{2}-u_{1}\end{pmatrix}\text{ and }B=\frac{1}{u_{2}-u_{1}} \begin{pmatrix}u_{2}+f(-u)_{1}&-f(-u)_{1}-u_{1}\\ u_{2}+f(-u)_{2}&-f(-u)_{2}-u_{1}\end{pmatrix}\text{.}\]

Let \(g:(x_{1},x_{2})\mapsto A\begin{pmatrix}x_{1}\\ x_{2}\end{pmatrix}\) if \(x_{1}\leq x_{2}\) else \(B\begin{pmatrix}x_{1}\\ x_{2}\end{pmatrix}\). We have \(g\in F\) since \(A\mathbf{1}_{2}=B\mathbf{1}_{2}=\mathbf{1}_{2}\) (in particular \(g\) is then _normalization-equivariant_) and \(\forall x\in\{-u,u\},g(x)=f(x)\). According to Lemma 1, \(g=f\), hence \(f\in F\). Finally, \(\mathrm{NE}(2)\subseteq F\), hence \(\mathrm{NE}(2)=F\). \(\square\)

**Proposition 3**:
* \(f_{\theta}^{\mathrm{NE}}\) is composed of three types of building blocks of the following form:
* affine convolutions: \(a_{\Theta}:x\in\mathbb{R}^{n}\mapsto\Theta x\) with \(\Theta\in\mathbb{R}^{m\times n}\) subject to \(\Theta\mathbf{1}_{n}=\mathbf{1}_{m}\),
* sort pooling nonlinearities: \(\mathrm{sortpool}:\mathbb{R}^{n}\mapsto\mathbb{R}^{n}\),
* max pooling layers: \(\mathrm{maxpool}:\mathbb{R}^{n}\mapsto\mathbb{R}^{m}\) with \(m<n\),

which are assembled using:

* function compositions: \(\mathrm{comp}(f,g)\mapsto f\circ g\),
* skip connections: \(\mathrm{skip}(f,g)\mapsto(x\mapsto(f(x)^{\top}\,g(x)^{\top})^{\top})\),
* affine residual connections: \(\mathrm{ares}_{t}(f,g)\mapsto(1-t)f+tg\) with \(t\in\mathbb{R}\).

Note that the rows of \(\Theta\) in \(a_{\Theta}\) encode the convolution kernels in a CNN and the trainable parameters, denoted by \(\theta\), are only composed of matrices \(\Theta\) and scalars \(t\). Moreover, note that average pooling layers are nothing else than affine convolutions with fixed parameters.

Since \(a_{\Theta}\), \(\mathrm{sortpool}\) and \(\mathrm{maxpool}\) are _normalization-equivariant_ functions, Lemma 2 states that the resulting function \(f_{\theta}^{\mathrm{NE}}\) is also _normalization-equivariant_. Moreover, since they are continuous and the assembling operators preserve continuity, \(f_{\theta}^{\mathrm{NE}}\) is continuous. Then, for a given input \(x\in\mathbb{R}^{n}\), we have \((\mathrm{sortpool}\circ a_{\Theta})(x)=a_{\pi(\Theta)}(x)=\pi(\Theta)x\), where \(\pi\) an operator acting on matrix \(\Theta\) by permuting its rows (note that the permutation \(\pi\) is both dependent on \(x\) and \(\Theta\)). Therefore, applying a pattern "conv affine + sortpool" simply amounts locally to a linear transformation. Moreover, since applying a max pooling layer amounts to removing some rows from matrix \(\Theta\), the local linear behavior is preserved. Thus, as the nonlinearities of \(f_{\theta}^{\mathrm{NE}}\) are exclusively brought by sort pooling patterns (and possibly max pooling layers), \(f_{\theta}^{\mathrm{NE}}\) is actually locally linear. In other words, \(f_{\theta}^{\mathrm{NE}}\) is piecewise-linear. Moreover, as there is a finite number (although high) of possible permutations (and possibly eliminations) of the rows of all matrices \(\Theta\), \(f_{\theta}^{\mathrm{NE}}\) has finitely many pieces. Finally, on each piece represented by the vector \(y_{r}\), \(f_{\theta}^{\mathrm{NE}}(y)=A_{\theta}^{y_{r}}y\). It remains to prove that \(A_{\theta}^{y_{r}}\mathbf{1}_{n}=\mathbf{1}_{m}\). But this property is easily obtained by noticing that, subject to dimensional compatibility on matrices \(\Theta\):

* \(\Theta\mathbf{1}_{n}=\mathbf{1}_{m}\Rightarrow\pi(\Theta)\mathbf{1}_{n}= \mathbf{1}_{m}\) ("conv affine + sortpool"),
* \(\Theta\mathbf{1}_{n}=\mathbf{1}_{m}\Rightarrow\rho(\Theta)\mathbf{1}_{n}= \mathbf{1}_{l}\) ("conv affine + maxpool") where \(\rho\) removes some rows,
* \(\Theta_{1}\mathbf{1}_{n}=\mathbf{1}_{m}\) and \(\Theta_{2}\mathbf{1}_{m}=\mathbf{1}_{l}\Rightarrow\Theta_{2}\Theta_{1}\mathbf{1 }_{n}=\mathbf{1}_{l}\) (composition),
* \(\Theta_{1}\mathbf{1}_{n_{1}}=\mathbf{1}_{m_{1}}\) and \(\Theta_{2}\mathbf{1}_{n_{2}}=\mathbf{1}_{m_{2}}\Rightarrow\begin{pmatrix} \Theta_{1}\\ \Theta_{2}\end{pmatrix}\mathbf{1}_{n_{1}+n_{2}}=\mathbf{1}_{m_{1}+m_{2}}\) (skip connection),
* \(\Theta_{1}\mathbf{1}_{n}=\mathbf{1}_{m}\) and \(\Theta_{2}\mathbf{1}_{n}=\mathbf{1}_{m}\Rightarrow(1-t)\Theta_{1}\mathbf{1}_{n }+t\Theta_{2}\mathbf{1}_{n}=\mathbf{1}_{m}\) (affine residual connection).

Thus, the affine combinations are preserved all along the layers of \(f_{\theta}^{\mathrm{NE}}\). In the end,

\[f_{\theta}^{\mathrm{NE}}(y)=A_{\theta}^{y_{r}}y,\ \text{ with }A_{\theta}^{y_{r}} \in\mathbb{R}^{m\times n}\text{ such that }A_{\theta}^{y_{r}}\mathbf{1}_{n}=\mathbf{1}_{m}\.\]

### Examples of normalization-equivariant conventional denoisers

Noise-reduction filters:All linear smoothing filters can be put under the form \(f_{\Theta}:x\in\mathbb{R}^{n}\mapsto\Theta x\) with \(\Theta\in\mathbb{R}^{n\times n}\) (the rows of \(\Theta\) encode the convolution kernel). Obviously, \(f_{\Theta}\) is always _scale-equivariant_, whatever the filter \(\Theta\). As for the _shift-equivariance_, a simple calculation shows that:

\[x\mapsto\Theta x\text{ is {shift-equivariant} }\Leftrightarrow\,\forall x\in\mathbb{R}^{n},\forall\mu\in \mathbb{R},\Theta(x+\mu\mathbf{1}_{n})=\Theta x+\mu\mathbf{1}_{m}\ \Leftrightarrow\ \Theta\mathbf{1}_{n}=\mathbf{1}_{m}\,.\]

Since the sum of the coefficients of a Gaussian kernel and an averaging kernel is one, we have \(\Theta\mathbf{1}_{n}=\mathbf{1}_{m}\), hence these linear filters are _normalization-equivariant_. The median filter is also _normalization-equivariant_ because \(\operatorname{median}(\lambda x+\mu)=\lambda\operatorname{median}(x)+\mu\) for \(\lambda\in\mathbb{R}_{*}^{+}\) and \(\mu\in\mathbb{R}\).

**Patch-based denoising:**

\(-\) NLM [5]: Assuming that the smoothing parameter \(h\) is proportional to \(\sigma\), _i.e._\(h=\alpha\sigma\), we have \(e^{-\frac{\|p(y_{i}+p)-p(y_{j}+\mu)\|_{2}^{2}}{(\alpha\alpha)\sigma^{2}}}=e^{ -\frac{\lambda^{2}\|p(y_{i})-p(y_{j})\|_{2}^{2}}{\lambda^{2}(\alpha\sigma)^{2 }}}=e^{-\frac{\|p(y_{i})-p(y_{j})\|_{2}^{2}}{h^{2}}}\), hence the aggregation weights are _normalization-invariant_. Then,

\[f_{\mathrm{NLM}}(\lambda y+\mu,\lambda\sigma)_{i} =\frac{1}{W_{i}}\sum_{y_{j}\in\Omega(y_{i})}e^{-\frac{\|p(y_{i})- p(y_{j})\|_{2}^{2}}{h^{2}}}(\lambda y_{j}+\mu)\text{ with }W_{i}=\sum_{y_{j}\in\Omega(y_{i})}e^{-\frac{\|p(y_{i})-p(y_{j})\|_{2}^{2}}{h^{2}}}\,,\] \[=\frac{1}{W

[MISSING_PAGE_FAIL:20]

## 6 Conclusion

\begin{table}
\begin{tabular}{c c c c c}  & Dataset & MNIST & Kuzushiji-MNIST & Fashion-MNIST \\ \hline \multirow{3}{*}{VGG8b} & _ordinary_ & 0.26\% & 1.53\% & 4.53\% \\  & _scale-equiv_ & 0.37\% & 1.52\% & 5.01\% \\ \cline{1-1}  & _norm-equiv_ & 0.44\% & 2.78\% & 6.57\% \\ \hline \end{tabular} Note that for _norm-equiv_ variants, learning rate is initialized to \(3e\)-\(5\) instead of \(5e\)-\(4\) and dropout rate is halved.

\end{table}
Table 6: Test error of different variants of the same VGG8b architecture for image classification. Note that norm-equivariance is with respect to the classification vector and becomes norm-invariance after label selection via argmax. See [34] for details about architecture, datasets and training.

Figure 7: Comparison of the performance of our _normalization-equivariant_ alternative with its _scale-equivariant_ (under three different forms) and _ordinary_ counterparts for different types of additive noises on the Set12 dataset with “blind” FdnCNN architecture. The vertical blue line indicates the unique noise level on which the “blind” networks were trained exclusively. Note that JPEG noise (d) (i.e. JPEG artifacts) is treated with the networks of (a), assimilating JPEG noise with uniform noise (similar outcomes can be achieved when using the networks specialized for Laplace noise (b)).

Figure 8: Visual comparisons of the generalization capabilities of a _scale-equivariant_ FDnCNN [47] (left) and its _normalization-equivariant_ counterpart (right) for Gaussian noise. Both networks were trained for Gaussian noise at noise level \(\sigma=25\) exclusively. The adaptive filters (rows of \(A_{\theta}^{y_{r}}\) in Prop. 3) are indicated for two particular pixels as well as the sum of their coefficients (note that some weights are negative, indicated in red). The _scale-equivariant_ network tends to excessively smooth out the image when evaluated at a lower noise level, whereas the _normalization-equivariant_ network is more adaptable and considers the underlying texture to a greater extent.

Figure 9: Denoising results for example images of the form \(y=x+\lambda\varepsilon\) with \(\sigma=25/255\) and \(x\in[0,1]^{n}\), by FDnCNN [47] specialized for noise level \(\sigma\) only. Here, \(f_{\theta}^{\varnothing}\), \(f_{\theta}^{\text{SE}}\) and \(f_{\theta}^{\text{NE}}\) denote the _ordinary_, _scale-equivariant_ and _normalization-equivariant_ variants, respectively.

Figure 10: Qualitative comparison of image denoising results with synthetic white Gaussian noise for “non-blind” models. Regardless of the variant of a model, the denoising results are visually similar.