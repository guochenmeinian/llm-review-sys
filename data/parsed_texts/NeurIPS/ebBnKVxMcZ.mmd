# Confidence Calibration of Classifiers with Many Classes

 Adrien Le Coz\({}^{1,2,3}\)  Stephane Herbin\({}^{2,3}\)  Faouzi Adjed\({}^{1}\)

\({}^{1}\)IRT SystemX \({}^{2}\)ONERA - DTIS \({}^{3}\) Paris-Saclay University

adrien.2mvb@passinbox.com stephane.herbin@onera.fr faouzi.adjed@irt-systemx.fr

###### Abstract

For classification models based on neural networks, the maximum predicted class probability is often used as a confidence score. This score rarely predicts well the probability of making a correct prediction and requires a post-processing calibration step. However, many confidence calibration methods fail for problems with many classes. To address this issue, we transform the problem of calibrating a multiclass classifier into calibrating a single surrogate binary classifier. This approach allows for more efficient use of standard calibration methods. We evaluate our approach on numerous neural networks used for image or text classification and show that it significantly enhances existing calibration methods. Our code can be accessed at the following link: https://github.com/allglc/tva-calibration.

## 1 Introduction

The considerable performance increase of modern deep neural networks (DNNs) and their potential deployment in real-world applications has made reliably estimating the probability of wrong decisions a key concern. When such components are expected to be embedded in safety-critical systems (e.g., medical or transportation), estimating this probability is crucial to mitigate catastrophic behavior. One way to address this question is to treat it as an uncertainty quantification problem [2, 12], where the uncertainty value computed for each prediction is considered as a confidence. This confidence can be used to reject uncertain decisions proposed by the DNN [13], for out-of-distribution detection [22], or to control active learning [34] or reinforcement learning based systems [76]. When confidence values reliably reflect the true probability of correct decisions, i.e., their accuracy, a predictive system is said to be _calibrated_. In this case, confidence values can be used as a reliable control for decision-making.

We are interested in producing an uncertainty indicator for decision problems where the input is high dimensional and the decision space large, typically classifiers with tens to thousands of classes. For this kind of problem, DNNs are common predictors, and their outputs can be used to provide an uncertainty value at no cost, i.e., without necessitating heavy estimation such as Bayesian sampling [15] or ensemble methods [33]. Indeed, most neural architectures for classification instantiate their decision as a softmax layer, where the maximum value can be interpreted as the maximum of the posterior probability and, therefore, as a confidence. Unfortunately, uncertainty values computed in this way are often miscalibrated. DNNs have been shown to be over-confident [17], meaning their confidence is higher than their accuracy: predictions with 90% confidence might be correct only 80% of the time. A later study [44] suggests that model architecture impacts calibration more than model size, pre-training, and accuracy. For ImageNet classifiers, the accuracy and the number of model parameters are not correlated to calibration, but model families are [11].

These studies show that it is difficult to anticipate the calibration level of confidence values computed directly from DNNs and exhibit the benefits of a complementary post-processing calibration. This calibration process can be seen as a learning step that exploits data from a calibration set, distinct from the training set, and is used to learn a function that maps classifier outputs into better-calibrated values. This process is typically lightweight and decoupled from the issue of improving modelperformance. A standard baseline for post-processing calibration is Temperature Scaling [17], where the penultimate logit layer is scaled by a coefficient optimized on the calibration set.

Many post-processing calibration methods have been developed for binary classification models [50; 69; 70]. Applying these methods to multiclass classifiers requires some adaptation. One standard approach reformulates the multiclass setting into many One-versus-All binary problems (one per class) [70]. One limitation of this approach is that it does not scale well. When the number of classes is large, the calibration data is divided into highly unbalanced subsets that do not contain enough positive examples to solve the One-versus-All binary problems. Other methods based on Platt scaling [50] involve learning a set of parameters whose size grows with the number of classes. For problems with many classes, they tend to overfit, as we demonstrate in this work.

The main idea of our work is to reformulate the multiclass confidence estimation into a _single_ binary problem. This problem can be phrased as the unique question: "Is the prediction correct?". In this formulation, the confidence score is defined as the maximum class probability of the binary problem that outputs \(1\) if the predicted class is correct and \(0\) otherwise. The intent is that the confidence score accurately describes whether the prediction is correct, regardless of the class. We show that this novel approach, which we call _Top-versus-All_ (TvA), significantly improves the performance of standard calibration methods: Temperature and Vector Scaling [17], Dirichlet Calibration [29], Histogram Binning [69], Isotonic Regression [70], Beta Calibration [28], and Bayesian Binning into Quantiles [46]. We also introduce a simple regularization for Vector Scaling or Dirichlet Calibration that mitigates overfitting when the number of classes is high relative to the calibration data size. We conduct experiments on multiple image and text classification datasets and many pre-trained models.

Our main contributions are the following:

* We discuss four issues of the standard approach to confidence calibration.
* To solve these issues, we develop the Top-versus-All approach to confidence calibration of multiclass classifiers, transforming the problem into a single binary classifier's calibration. This straightforward reformulation enables more efficient use of existing calibration methods, achieved with minimal modifications to the methods' original algorithms.
* Applied to scaling methods for calibration (such as Temperature Scaling), TvA allows the use of the binary cross-entropy loss, which is more efficient in decreasing the confidence of wrong predictions and leads to stronger gradients in the case of Temperature Scaling. Applied to binary methods for calibration (such as Histogram Binning), TvA significantly improves their performance and makes them accuracy-preserving.
* We demonstrate our approach's scalability and generality with extensive experiments on image classification with state-of-the-art models for complex datasets and on text classification with Pre-trained Language Models (PLMs) and Large Language Models (LLMs).

## 2 Related work

CalibrationThere are various notions of multiclass calibration. One can consider confidence [17], class-wise [28], top-\(r\)[19], top-label [18], decision [75], projection smooth [16], or strong [60; 65] calibration. For recent surveys, we refer to [10] and [63]. In this work, we focus on confidence calibration and not on the calibration of the full probability vector. Indeed, confidence calibration is useful for many applications that only require a single confidence value: selective classification [13], out-of-distribution detection [22], or active learning [34]. For these applications, stronger notions of calibration are both difficult and useless. Also, class-wise calibration metrics do not appropriately scale to large numbers of classes, a setting we consider in this work, as explained in Appendix E.

MetricsSeveral metrics have been proposed to quantify calibration error. The most common is the Expected Calibration Error (ECE) [46] (see Equation 2). ECE has flaws: the estimation quality is influenced by the binning scheme, and it is not a proper scoring rule [14; 60; 48]. Despite its flaws, it remains the standard comparison metric for confidence calibration. Variants of ECE have also been developed: classwise-ECE [29], ECE with equal mass bins [48; 44], or top-label-ECE, which adds a conditioning on the predicted class [18]. The Brier score [4] is also used to measure calibration. The proximity-informed expected calibration error (PIECE) evaluates the miscalibration due to proximity bias [68]. We mainly use the standard ECE in this work, and the Appendix contains more metrics.

Training calibrated networksSeveral solutions have been proposed in the literature to improve calibration by training neural networks in specific ways, generally by making use of a new loss term [31, 58, 26, 6, 5]. While these methods directly optimize calibration during the training phase of the networks, they require a high development time, often compromise accuracy, and are not adapted to pre-trained foundation models. That is why we prefer to focus on calibrating already-trained models.

Post-processing (or post-hoc) calibration methodsAnother approach is to calibrate already-trained models. This lowers the development time by decoupling accuracy optimization and calibration. In this paper, we divide post-hoc calibration methods into two categories: scaling and binary.

Scaling methods are derived from Platt scaling [50] and optimize some parameters to scale the logits. Temperature Scaling [17] is a popular simple post-processing calibration method. The logits vector is scaled by a coefficient, which modifies the probability vector. Vector Scaling [17] is more expressive and has good performance in many cases [17, 48, 29]. Matrix Scaling can also be considered for more expressiveness but is difficult to apply without overfitting [17]. Dirichlet Calibration [29] proposes a regularization strategy for Matrix Scaling. [72] developed Ensemble Temperature Scaling. Scaling can be combined with binning [30]. Besides logits or probabilities, features can also be used [35].

Another family of methods tackles binary classification. We designate them as binary methods. Histogram Binning [69] divides the prediction into \(B\) bins according to the predicted probability. For each bin, a calibrated probability is computed from the calibration data. The probability becomes discrete: it can only take \(B\) values. With some modifications, it outperforms scaling methods [18, 49].

Isotonic Regression [70] learns a piecewise constant function to remap probabilities. Bayesian Binning into Quantiles [46] brings Bayesian model averaging to Histogram Binning. Beta Calibration [28] uses a beta distribution to obtain a calibration mapping.

Our work reformulates the multiclass calibration problem and allows more efficient use of all these calibration methods, with little to no change in their algorithms.

Multiclass to BinaryUsing binary calibration methods for a multiclass classifier requires adapting the multiclass setting. This is usually done with a One-versus-All approach [70, 17]. The multiclass setting is decomposed into \(L\) One-versus-All independent problems: one binary problem for each class. [18] introduce the notion of top-label calibration, i.e., confidence calibration with additional conditioning on the predicted class (top-label). They describe a general multiclass-to-binary framework to develop top-label calibrators. [6] derive \(L(L-1)/2\) pairwise binary problems. The approach requires training the classifier from scratch, and its performance decreases with the number of classes. Our work tackles this multiclass-to-binary research problem. Contrary to [6], the One-versus-All approach [70] and top-label calibrators [18], our approach works well for problems with many classes. The methods I-Max [49] and IRM [72] use a shared class-wise strategy to compute a single calibrator. The calibrator is applied to all class probabilities separately, so the class probabilities ranking and prediction might change. In contrast, \(\mathrm{TvA}\) applied to binary methods rescales the confidence after the class prediction is made. I-Max and IRM consider the full class probabilities vectors, while \(\mathrm{TvA}\) only considers confidence values. Also, they build on top of Histogram Binning and Isotonic Regression, respectively, while we apply our approach to many calibration methods. A concurrent work building on an intuition similar to ours derives a calibration method based on a Correctness-Aware Loss [38]. Appendix C discusses the differences between our approach and others.

## 3 Problem setting

### Background

Confidence calibration of a classifierWe consider the classification problem where an input \(x\) is associated with a class label \(y\in\mathcal{Y}=\{1,2,...,L\}\). The neural network classifier \(f\) provides a class prediction from a final softmax layer \(\sigma\) that transforms intermediate logits \(z\) into probabilities. The classifier prediction is the most probable class \(\hat{y}=\operatorname*{arg\,max}_{k\in\mathcal{Y}}f_{k}(x)\) with \(f_{k}(x)\) referring to the probability of class \(k\), and the confidence score defined as \(s=\max_{k\in\mathcal{Y}}f_{k}(x)\). Note that we use the term _confidence_ to denote the maximum class probability. With \(y\) the real label, we consider the confidence calibration definition from [17] that says that the classifier \(f\) is calibrated if:

\[P(\hat{y}=y|s=p)=p,\quad\forall p\in[0,1]\] (1)

where the probability is over the data distribution. Equation (1) expresses that the probability of being correct when the confidence is around \(p\) is indeed \(p\). For instance, if we consider the set of predictionswith a confidence of \(90\%\), they should be correct \(90\%\) of the time. The conditional probability of (1) is not rigorously defined mathematically (the event \(\{s=p\}\) has zero probability), and interval-based empirical estimators are often used to define metrics capable of evaluating how well (1) is satisfied. This is the case of ECE, which approximates the calibration error by partitioning the confidence distribution into \(B\) bins. The absolute difference between the accuracy and confidence is computed for each subset of data in the bins. The final value is a weighted sum of the differences of each bin.

\[\text{ECE}=\sum_{b=1}^{B}\frac{n_{b}}{N}|\text{acc}(b)-\text{conf}(b)|\] (2)

Where \(n_{b}\) is the number of samples in bin \(b\), \(N\) is the total number of samples, \(\text{acc}(b)\) is the accuracy in bin \(b\), and \(\text{conf}(b)\) is the average confidence in bin \(b\). ECE can be interpreted visually by looking at diagrams such as those of Figure 1: ECE computes the sum of the red bars (difference between bin accuracy and average confidence) weighted by the proportion of samples in the bin.

Post-processing calibration methodsWe are considering the scenario where a classifier has already been trained, and the objective is to enhance its calibration. Post-processing calibration methods aim to remap the classifier probabilities to better-calibrated values without modifying the classifier. They typically use a calibration set different from the training set to optimize parameters or learn a function. We note the calibration data \(D_{cal}=\{(x_{i},y_{i})\}_{i=1}^{N}\). We focus on post-processing calibration because it enables better utilization of off-the-shelf models and separates model training (optimized for accuracy) from calibration. These advantages significantly reduce the development cost of obtaining a well-performing and well-calibrated model, contrary to optimizing calibration during training. We categorize the post-processing calibration techniques considered in this paper into two groups: scaling methods and binary methods.

### Issues related to current approaches

Behavior of current scaling methodsScaling methods for calibration optimize one or more coefficients that scale the logits vector to minimize on calibration data the cross-entropy loss defined as \(l_{CE}=-\sum_{k=1}^{L}1_{k=y}\cdot\log(f_{k}(x))=-\log(f_{y}(x))\). Minimizing \(l_{CE}\) therefore increases the probability of the true class. We can distinguish two cases to understand what happens during the optimization: whether the prediction \(\hat{y}\) is correct or not. In the first case, the confidence score is \(s=f_{y}(x)\): minimizing \(l_{CE}\) increases the confidence \(f_{y}(x)\). In the second case, the prediction is incorrect, which implies that \(f_{y}(x)<s\). Minimizing \(l_{CE}\) increases the probability of the true class \(f_{y}(x)\) but does not directly change the confidence (because \(s\neq f_{y}(x)\)). Instead, the confidence (which was attributed to a wrong class) is _indirectly_ lowered through the softmax normalization.

\(\hookrightarrow\)_Issue 1: Cross-entropy loss only indirectly lowers confidence in wrong predictions._

We identified another issue of some scaling methods. By design, the number of parameters optimized by Vector Scaling and Dirichlet Calibration grows with the number of classes. When the number of classes is high, these methods overfit the calibration set as shown in Figure 2.

\(\hookrightarrow\)_Issue 2: Vector Scaling and Dirichlet Calibration overfit calibration sets with many classes._

One-versus-All approach for binary methodsThe One-versus-All (OvA) calibration approach [70] allows adapting calibration methods for binary classifiers to multiclass classifiers. To do so, it decomposes the calibration of multiclass classifiers into sets of \(L\) binary calibration problems: one for each class \(k\). For each problem, the considered probability is \(f_{k}(x)\), and the associated label \(1_{y=k}\in\{0,1\}\). When calibrating a classifier from data, each binary problem is highly imbalanced with a ratio between positive and negative examples equal to \(\frac{1}{L-1}\) if the classes are equally sampled. For instance, for ImageNet, the ratio is 1/999: out of 25000 examples, only 25 have a positive label.

\(\hookrightarrow\)_Issue 3: OvA approach leads to highly imbalanced binary problems._

At test time, each of the \(L\) class probabilities is calibrated by a separate calibration model. The resulting probability vector can be normalized to ensure a unit norm. Because each probability is calibrated independently, their ranking can change, thus modifying the predicted class. In Table 9, we see that accuracy is often negatively impacted in practice.

\(\hookrightarrow\)_Issue 4: OvA approach can change the predicted class and negatively impact the accuracy._

## 4 Top-versus-All approach to confidence calibration

``` Input: \(D_{cal}\): \(\{(x_{i},y_{i})\}_{i=1}^{N}\) the calibration data \(f\): the multiclass classifier \(g\): a calibration function \(\triangleright\) e.g., Temperature Scaling Preprocessing: \(\hat{y_{i}}\leftarrow\operatorname*{arg\,max}_{k\in\mathcal{Y}}f_{k}(x_{i})\)\(\triangleright\) Compute class predictions \(y_{i}^{b}\gets 1_{\hat{y}_{i}=y_{i}}\)\(\triangleright\) Compute predictions correctness \(f^{b}\leftarrow\max_{k\in\mathcal{Y}}f_{k}\)\(\triangleright\) Create surrogate binary classifier \(D_{cal}^{\text{TvA}}\leftarrow\{(x_{i},y_{i}^{b})\}_{i=1}^{N}\)\(\triangleright\) Build binary calibration set Learn calibration function:  Learn \(g\) to calibrate the surrogate binary classifier \(f^{b}\) on \(D_{cal}^{\text{TvA}}\) Inference:  Use \(g\) to calibrate the confidences of the original multiclass classifier \(f\) ```

**Algorithm 1** Top-versus-All approach to confidence calibration

### General presentation

In the calibration definition (1) and the standard ECE metrics, only the confidence, i.e., the maximal probability, reflects the likelihood of making an accurate prediction. The probabilities of other classes are not taken into account. However, the standard approach to calibration uses the entire set of probabilities, not just confidence, which introduces unnecessary complexity. We aim to simplify the process by reformulating the problem of calibrating multiclass classifiers into a _single_ binary problem. This problem can be phrased as: "Is the prediction correct?". In this setting, we do not calibrate the predicted probabilities vector but only a scalar: the confidence. The remaining probabilities are discarded. This is equivalent to calibrating a _surrogate_ binary classifier that predicts whether the class prediction is correct. Since this correctness classifier only considers the maximal probability versus all others, we call our approach _Top-versus-All_ (TvA).

Replacing the standard approach by TvA is straightforward. Given the standard calibration data \(D_{cal}=\{(x_{i},y_{i})\}_{i=1}^{N}\), we add a few data preprocessing steps. First, compute the class predictions \(\hat{y}\) and their correctness: \(y^{b}=1_{\hat{y}=y}\). Second, create the surrogate binary classifier \(f^{b}(x)=\max_{k\in\mathcal{Y}}f_{k}(x)\). Finally, build the calibration set for the surrogate binary classifier:

\[D_{cal}^{\text{TvA}}=\{(x_{i},y_{i}^{b})\}_{i=1}^{N}\] (3)

After this preprocessing, we choose a standard calibration function \(g\), e.g., Temperature Scaling, to calibrate the surrogate binary classifier. The learning of the calibration function follows its original underlying algorithm but uses the modified calibration data \(D_{cal}^{\text{TvA}}\). The learned calibration function is then applied to the confidences of the original multiclass classifier. Algorithm 1 describes our approach. In the Appendix, Algorithm 3 provides more details and highlights differences with the standard approach of Algorithm 2.

After this general presentation, we explain how TvA impacts the two categories of calibration methods, scaling and binary, in Subsections 4.2 and 4.3, respectively. We also justify its behavior.

### Top-versus-All approach for scaling methods

Because our Top-versus-All setting reformulates the calibration of multiclass classifiers into a binary problem, the natural loss is the binary cross-entropy:

\[l_{BCE}=-\big{(}y^{b}\cdot\log s+(1-y^{b})\cdot\log(1-s)\big{)}\] (4)

Minimizing this loss results in confidence estimates that more accurately describe the probability of being correct, regardless of the \(L-1\) less likely class predictions. Using the binary cross-entropy as a calibration loss makes an important difference compared to the usual multiclass cross-entropy. The cross-entropy loss takes into account the probability of the _correct_ class, while with TvA the binary cross-entropy takes into account the probability of the _predicted_ class (i.e., the confidence).

As for the standard approach, only two cases are possible. When the prediction is correct, \(l_{BCE}=-\log(s)=-\log(f_{y})=l_{CE}\). We get the same result as the cross-entropy loss: minimizing it directly increases the confidence. But when the prediction is incorrect, \(l_{BCE}=-\log(1-s)\neq l_{CE}\). Minimizing the loss now _directly_ decreases the confidence. This is a key difference compared to using the multiclass cross-entropy loss.

The impact of the reformulation can be seen for Temperature Scaling, which optimizes a coefficient \(T\) that scales the logits \(z_{k}\). The reformulation generates stronger gradients when the prediction is incorrect:

\[\left|\frac{\partial l_{BCE}}{\partial T}\right|>\left|\frac{\partial l_{CE}} {\partial T}\right|\text{ for }s>0.5\] (5)

with \(\frac{\partial l_{BCE}}{\partial T}=\frac{1}{T^{2}}\cdot\frac{1}{s-1}\cdot( \max_{k}z_{k}-\sum_{k}z_{k}\cdot f_{k})\) and \(\frac{\partial l_{CE}}{\partial T}=\frac{1}{T^{2}}\left(z_{y}-\sum_{k}z_{k} \cdot f_{k}\right)\). See Appendix D for the mathematical calculations. Because for interesting problems, the confidence verifies \(s>0.5\) most of the time (as shown in Figure 1), our approach strengthens the gradients. The optimization of the temperature \(T\) is more efficient as confident incorrect predictions are more heavily penalized. This effect is not mitigated by the choice of learning rate, which does not vary with \(s\). Applying standard Temperature Scaling usually results in overconfident probabilities, but our approach limits this overconfidence. This is verified experimentally in Table 8, which displays the average confidences for TS without and with TvA.

\(\hookrightarrow\)_Solution for Issue 1: Use the binary cross-entropy loss resulting from TvA approach._

Regularization of scaling methodsOverfitting of Vector Scaling and Dirichlet Calibration can be reduced with a simple L2 regularization that penalizes the coefficients of the vector \(v\) that are far from the reference value \(1\).

\[l_{reg}(v)=\frac{1}{L}\sum_{i=1}^{L}(v_{i}-1)^{2}\] (6)

This regularization allows these methods to take advantage of their additional expressiveness without being subject to overfitting. The loss for Vector Scaling becomes \(l_{BCE}+\lambda l_{reg}(v)\) where \(\lambda\) is a hyperparameter. The loss for Dirichlet Calibration uses additional matrix regularization terms. \(\lambda\) is the only additional hyperparameter introduced by our method, and it applies only to Vector Scaling and Dirichlet Calibration.

\(\hookrightarrow\)_Solution for Issue 2: Use L2 regularization._

### Top-versus-All approach for binary methods

Our TvA approach replaces the One-versus-All approach to apply binary methods to the multiclass setting. TvA transforms the multiclass setting into a _single_ binary problem that uses the binary calibration dataset (3). In this dataset, the proportion of positive labels equals the classifier's accuracy \(a\). The ratio between negative and positive examples is \(\frac{(1-a)N}{aN}=\frac{1}{a}-1\). For a classifier with 80% accuracy on ImageNet and a calibration dataset of 25000 examples, there are 5000 negative and 20000 positive examples (ratio of 1/4). This is still a bit imbalanced but orders of magnitude smaller than the class-wise binary calibration datasets of the One-versus-All approach (ratio of 1/999).

\(\hookrightarrow\)_Solution for Issue 3: By not dividing the calibration data into class-wise datasets, the TvA approach yields a much better balanced binary calibration problem._

The Top-versus-All approach operates on confidence alone, not the full class probabilities vector. This means that the class prediction is already done, and the ranking of the class probabilities does not change, unlike with One-versus-All. The classifier's prediction and accuracy are unaffected. This scheme allows decoupling accuracy improvements (during training time) and calibration (during post-processing calibration), thus avoiding compromises and reducing development time.

\(\hookrightarrow\)_Solution for Issue 4: By operating on confidence alone, the Top-versus-All approach does not impact the classifier's prediction or accuracy for binary methods applied to the multiclass scenario._

## 5 Experiments

### Setting

Datasets and modelsFor image classification, we used the datasets _CIFAR-10 (C10)_ and _CIFAR-100 (C100)_[27] with \(10\) and \(100\) classes respectively, _ImageNet (IN)_[7] with \(1000\) classes, and _ImageNet-21K (IN21K)_[54] with \(10450\) classes. For text classification, we used _Amazon Fine Foods (AFF)_[43] and _DynaSent (DF)_[51] for sentiment analysis with \(3\) classes, _MNLI_[66] for natural language inference with \(3\) classes, and _Yahoo Answers (YA)_[73] for topic classification on \(10\) classes. Experiment results are averaged over five random seeds that randomly split the concatenation of the original validation and test sets into calibration and test sets.

We used the following models for image classification: _ResNet_[21], _Wide-ResNet-26-10 (WRN)_[71], _DenseNet-121_[24], _MobileNetV3 (MN3)_[23], _ViT_[9], _ConvNeXt_[41], _EfficientNet_[56, 57], _Swin_[40, 39], and _CLIP_[52] which matches input images to text descriptions in a shared embedding space, assigning labels based on the highest similarity score. For text classification, we used the PLMs _RoBERTa_[37] and _T5_[53].

More details about datasets, calibration sets sizes, and model weights can be seen in Appendix F.

BaselinesOur Top-versus-All (\(\text{r}_{\text{VA}}\)) reformulation and regularization (\(\text{r}_{\text{gg}}\)) can be applied to different calibration methods. We have tested the following scaling methods: _Temperature Scaling (TS)_ and _Vector Scaling (VS)_[17], and _Dirichlet Calibration (DC)_[29] with the best-performing variant Dir-ODIR, which regularizes off-diagonal and bias coefficients. We also tested the following binary methods: _Histogram Binning (HB)_[69] using for each case the best-performing variant between equal-mass or equal-size bins, _Isotonic Regression (Iso)_[70], _Beta Calibration (Beta)_[28], and _Bayesian Binning into Quantiles (BBQ)_[46]. For comparison, we include methods with state-of-the-art results on problems with many classes: _I-Max_[49] and _IRM_[72]. See Appendix C for more details on these methods. More details on code implementations can be seen in Appendix F.

Figure 1: Reliability diagrams for ResNet-50 and ViT-B/16 when using Temperature Scaling (TS), Vector Scaling (VS), and Histogram Binning (HB) on ImageNet. The subscript \(\text{r}_{\text{VA}}\) signifies that the \(\text{T}\text{v}\text{A}\) reformulation was used, and \(\text{r}_{\text{reg}}\) means our regularization (6) was applied. Red bars show the differences between bin accuracy (blue bar) and accuracy for perfect calibration (dashed red line). As the methods improve the calibration, these differences are reduced and the average confidence (vertical dotted line) will get closer to the global accuracy (vertical dashed line).

[MISSING_PAGE_FAIL:8]

Some methods' current implementations could not handle the large scale of ImageNet-21K, resulting in out-of-memory errors written as "err." in the Table. For I-Max and IRM, this is because they consider the full probability vectors while \(\mathrm{TvA}\) efficiently uses data by considering only confidence values. Indeed, \(\mathrm{TvA}\) handles this scale without difficulty.

Additional results are included in Appendix H. Tables 5 and 6 contain the full results for ECE, with the standard deviations in Table 7. Table 8 reveals that ImageNet networks are mostly underconfident. This is aligned with [11] and goes against previous knowledge on overconfidence, which was initially believed to be linked to network size [17]. Table 9 provides the accuracies after calibration. Table 10 exhibits that ECE with equal-mass bins has similar values as standard ECE. Table 11 shows that \(\mathrm{TvA}\) mostly lowers the Brier score, except for Iso, which has the lowest score overall.

Calibration methods can also be applied to Large Languages Models (LLMs) using In-Context Learning (ICL) to tackle text classification tasks [77; 20; 25; 78; 1]. The primary goal of these methods is to improve model accuracy. \(\mathrm{TvA}\) was not designed for this objective, but it can still be applied on top of an existing method that improves the accuracy. \(\mathrm{TvA}\) then lowers the calibration error while keeping the accuracy gain. Results for GPT-J [62] and Llama-2 [59] are in Table 12.

To summarize the results for practical use, our experiments show that Histogram Binning (within the \(\mathrm{TvA}\) or \(\mathrm{I\text{-}Max}\) setting) is the best calibration method overall, providing ECE values mostly below 1%. This is the method we advise using. However, suppose the underlying application requires a confidence with continuous values, e.g., to rank the predictions for selective classification. In that case, we advise using a method that improves the AUROC, shown in Appendix G, such as TS or Iso.

### Solving overfitting with regularization and \(\mathrm{TvA}\)

On ImageNet, VS and DC overfit the calibration set, degrading the calibration on the test set. The lower performance of VS relative to TS indicates this overfitting. As visualized in Figure 2, combining the binary cross-entropy loss used in the \(\mathrm{TvA}\) reformulation and an additional regularization term prevents overfitting. We fixed the value \(\lambda=0.01\) as it works well across models. Initializing the vector coefficients to \(\frac{1}{T}\) with \(T\) obtained by \(\mathrm{TS}_{\textsc{TA}}\) helps further improve performance.

### Influence of the calibration set size

The size of the calibration set influences the performance of the different methods, as seen in Figure 3. TS and \(\mathrm{TS}_{\textsc{TA}}\) do not benefit from more data due to their low expressiveness. VS does not improve the ECE because of the overfitting problem. In contrast, \(\mathrm{VS}_{\textsc{TA}}\) benefits from more calibration data. With enough data (\(\approx\) 15000), it outperforms \(\mathrm{TS}_{\textsc{TA}}\). Binary methods using the standard One-versus-All approach have poor performance and need a large amount of data to be competitive. Using \(\mathrm{TvA}\), they get excellent performance with little data.

Figure 3: Influence of the calibration set size for ResNet-101 on ImageNet. Binary methods at the top and scaling methods at the bottom.

Figure 2: Test ECE evolution during training with ResNet-50 on ImageNet. The combination of regularization and \(\mathrm{TvA}\) prevents overfitting of Vector Scaling. Temperature Scaling with \(\mathrm{TvA}\) is shown for reference.

## 6 Limitations

Our approach tackles confidence calibration and is unlikely to improve performance for stronger notions of calibration, such as class-wise calibration. However, confidence calibration is useful for many practical cases, such as selective classification [13], out-of-distribution detection [22], or active learning [34]. Also, calibration improvements are less significant for problems with few classes (\(\leq 10\)) than for problems with many classes, but our approach still provides the best results.

## 7 Conclusion

Reducing the miscalibration of neural networks is essential to improve trust in their predictions. This can be done after the model training with an optimization using calibration data. However, many current calibration methods do not scale well to complex datasets: binary methods under the One-versus-All setting do not have enough per-class calibration data, and scaling methods are inefficient. We demonstrate that reformulating the confidence calibration of multiclass classifiers as a single binary problem significantly improves the performance of baseline calibration techniques. The competitiveness of scaling methods is increased, and binary methods use per-class calibration data more efficiently without altering the model's accuracy. In short, our TvA reformulation enhances many existing calibration methods with little to no change in their algorithm. Extensive experiments with state-of-the-art image classification models on complex datasets and with text classification demonstrate our approach's scalability and generality.

## Acknowledgments and Disclosure of Funding

We thank Tahar Nabil, Houssem Ouertatani, and Pol Labarbarie for their constructive feedback on the paper draft.

This work has been supported by the French government under the "France 2030" program, as part of the SystemX Technological Research Institute.

This work was granted access to the HPC/AI resources of IDRIS under the allocation 2024-AD011013372R1 made by GENCI.

## References

* [1]M. Abbas, Y. Zhou, P. Ram, N. Baracaldo, H. Samulowitz, T. Salonidis, and T. Chen (2024) Enhancing in-context learning via linear probe calibration. arXiv preprint arXiv:2401.12406. Cited by: SS1.
* [2]M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu, M. Ghavamzadeh, P. Fieguth, X. Cao, A. Khosravi, U. Rajendra Acharya, V. Makarenkov, and S. Nahavandi (2021-12) A review of uncertainty quantification in deep learning: techniques, applications and challenges. Information Fusion76, pp. 243-297. Cited by: SS1.
* [3]J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Mane, Y. Pan, C. Puhrsch, M. Reso, M. Saroufin, M. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala (2024-09) PyTorch 2: faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation. In 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24), Cited by: SS1.
* [4]G. W. Brier (1950) Verification of forecasts expressed in terms of probability. Monthly weather review78 (1), pp. 1-3. Cited by: SS1.
* [5]Y. Chen, L. Yuan, G. Cui, Z. Liu, and H. Ji (2023) A Close Look into the Calibration of Pre-trained Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, pp. 1343-1367. External Links: Document, Link Cited by: SS1.
* [6]J. Cheng and N. Vasconcelos (2022) Calibrating Deep Neural Networks by Pairwise Constraints. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13709-13718. Cited by: SS1.
* [7]J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei (2009-06) ImageNet: a large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255. Cited by: SS1.
* [8]Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui (2022) A survey on in-context learning. arXiv preprint arXiv:2301.00234. Cited by: SS1.
* [9]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. (2020) An image is worth 16x16 words: transformers for image recognition at scale. In International Conference on Learning Representations, Cited by: SS1.
* [10]T. Silva Filho, H. Song, M. Perello-Nieto, R. Santos-Rodriguez, M. Kull, and P. Flach (2023-12) Classifier Calibration: a survey on how to assess and improve predicted class probabilities. Machine Learning112 (9), pp. 3211-3260. Cited by: SS1.
* [11]I. Galil, M. Dabbah, and R. El-Yaniv (2023-06) What Can we Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers?. In The Eleventh International Conference on Learning Representations, Cited by: SS1.
* [12]J. Gawlikowski, C. Rovile Nijeutcheu Tassi, M. Ali, J. Lee, M. Humt, J. Feng, A. Kruspe, R. Triebel, P. Jung, R. Roscher, M. Shahzad, W. Yang, R. Bamler, and X. Xiang Zhu (2023-06) A survey of uncertainty in deep neural networks. Artificial Intelligence Review. Cited by: SS1.
** [13] Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. _Advances in neural information processing systems_, 30, 2017.
* [14] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. _Journal of the American statistical Association_, 102(477):359-378, 2007.
* [15] Ethan Goan and Clinton Fookes. Bayesian neural networks: An introduction and survey. _Case Studies in Applied Bayesian Data Science: CIRM Jean-Morlet Chair, Fall 2018_, pages 45-87, 2020.
* [16] Parikshit Gopalan, Lunjia Hu, and Guy N Rothblum. On computationally efficient multi-class calibration. _arXiv preprint arXiv:2402.07821_, 2024.
* [17] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International conference on machine learning_, pages 1321-1330. PMLR, 2017.
* [18] Chirag Gupta and Aaditya Ramdas. Top-label calibration and multiclass-to-binary reductions. In _International Conference on Learning Representations_, January 2022.
* [19] Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu, and Richard Hartley. Calibration of Neural Networks using Splines. In _International Conference on Learning Representations_, December 2021.
* [20] Zhixiong Han, Yaru Hao, Li Dong, Yutao Sun, and Furu Wei. Prototypical calibration for few-shot learning of language models. In _The Eleventh International Conference on Learning Representations_, 2022.
* [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [22] Dan Hendrycks and Kevin Gimpel. A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. In _International Conference on Learning Representations_, 2017.
* [23] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1314-1324, 2019.
* [24] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.
* [25] Zhongtao Jiang, Yuanzhe Zhang, Cao Liu, Jun Zhao, and Kang Liu. Generative calibration for in-context learning. _arXiv preprint arXiv:2310.10266_, 2023.
* [26] Archit Karandikar, Nicholas Cain, Dustin Tran, Balaji Lakshminarayanan, Jonathon Shlens, Michael C Mozer, and Becca Roelofs. Soft calibration objectives for neural networks. _Advances in Neural Information Processing Systems_, 34:29768-29779, 2021.
* [27] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009.
* [28] Meelis Kull, Telmo M. Silva Filho, and Peter Flach. Beyond sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration. _Electronic Journal of Statistics_, 11(2):5052-5080, January 2017.
* [29] Meelis Kull, Miquel Perello Nieto, Markus Kangsepp, Telmo Silva Filho, Hao Song, and Peter Flach. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration. _Advances in neural information processing systems_, 32, 2019.
* [30] Ananya Kumar, Percy S Liang, and Tengyu Ma. Verified uncertainty calibration. _Advances in Neural Information Processing Systems_, 32, 2019.
* [31] Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable Calibration Measures for Neural Networks from Kernel Mean Embeddings. In _Proceedings of the 35th International Conference on Machine Learning_, pages 2805-2814. PMLR, July 2018.
* [32] Fabian Kuppers, Jan Kronenberger, Amirhossein Shantia, and Anselm Haselhoff. Multivariate confidence calibration for object detection. In _The IEEECVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops_, June 2020.
* [33] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. _Advances in neural information processing systems_, 30, 2017.

* [34] Mingkun Li and I.K. Sethi. Confidence-based active learning. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 28(8):1251-1261, August 2006.
* [35] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Taking a Step Back with KCal: Multi-Class Kernel-Based Calibration for Deep Neural Networks. In _The Eleventh International Conference on Learning Representations_, September 2022.
* [36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35, 2023.
* [37] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* [38] Yuchi Liu, Lei Wang, Yuli Zou, James Zou, and Liang Zheng. Optimizing calibration by gaining aware of prediction correctness. _arXiv preprint arXiv:2404.13016_, 2024.
* [39] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12009-12019, 2022.
* [40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* [41] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11976-11986, 2022.
* [42] TorchVision maintainers and contributors. Torchvision: Pytorch's computer vision library. https://github.com/pytorch/vision, 2016.
* [43] Julian John McAuley and Jure Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. In _Proceedings of the 22nd International Conference on World Wide Web_, WWW '13, page 897-908, New York, NY, USA, 2013. Association for Computing Machinery.
* [44] Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the Calibration of Modern Neural Networks. In _Advances in Neural Information Processing Systems_, volume 34, pages 15682-15694. Curran Associates, Inc., 2021.
* [45] Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet Dokania. Calibrating Deep Neural Networks using Focal Loss. In _Advances in Neural Information Processing Systems_, volume 33, pages 15288-15299. Curran Associates, Inc., 2020.
* [46] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining Well Calibrated Probabilities Using Bayesian Binning. _Proceedings of the AAAI Conference on Artificial Intelligence_, 29(1), February 2015.
* [47] Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learning. In _Proceedings of the 22nd International Conference on Machine Learning_, ICML '05, pages 625-632, New York, NY, USA, August 2005. Association for Computing Machinery.
* [48] Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring calibration in deep learning. In _CVPR workshops_, volume 2, 2019.
* [49] Kanil Patel, William H Beluch, Bin Yang, Michael Pfeiffer, and Dan Zhang. Multi-class uncertainty calibration via mutual information maximization-based binning. In _International Conference on Learning Representations_, 2020.
* [50] John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. _Advances in large margin classifiers_, 10(3):61-74, 1999.
* [51] Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. DynaSent: A dynamic benchmark for sentiment analysis. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 2388-2404, Online, August 2021. Association for Computational Linguistics.

* [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020.
* [54] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)_, 2021.
* [55] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 conference on empirical methods in natural language processing_, pages 1631-1642, 2013.
* [56] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _International conference on machine learning_, pages 6105-6114. PMLR, 2019.
* [57] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In _International conference on machine learning_, pages 10096-10106. PMLR, 2021.
* [58] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [59] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajijwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [60] Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and Thomas Schon. Evaluating model calibration in classification. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 3459-3467. PMLR, 2019.
* [61] Ellen M. Voorhees and Dawn M. Tice. Building a question answering test collection. In _Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR '00, page 200-207, New York, NY, USA, 2000. Association for Computing Machinery.
* [62] Ben Wang and Aran Komatuszaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
* [63] Cheng Wang. Calibration in deep learning: A survey of the state-of-the-art. _arXiv preprint arXiv:2308.01222_, 2023.
* [64] Shuoyuan Wang, Jindong Wang, Guoqing Wang, Bob Zhang, Kaiyang Zhou, and Hongxin Wei. Open-vocabulary calibration for vision-language models. _arXiv preprint arXiv:2402.04655_, 2024.
* [65] David Widmann, Fredrik Lindsten, and Dave Zachariah. Calibration tests in multi-class classification: A unifying framework. _Advances in neural information processing systems_, 32, 2019.
* [66] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 1112-1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.
* [67] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online, October 2020. Association for Computational Linguistics.
* [68] Miao Xiong, Ailin Deng, Pang Wei W Koh, Jiaying Wu, Shen Li, Jianqing Xu, and Bryan Hooi. Proximity-informed calibration for deep neural networks. _Advances in Neural Information Processing Systems_, 36:68511-68538, 2023.

* [69] Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers. In _Proceedings of the Eighteenth International Conference on Machine Learning_, ICML '01, pages 609-616, San Francisco, CA, USA, June 2001. Morgan Kaufmann Publishers Inc.
* [70] Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability estimates. In _Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, KDD '02, pages 694-699, New York, NY, USA, July 2002. Association for Computing Machinery.
* [71] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. _arXiv preprint arXiv:1605.07146_, 2016.
* [72] Jize Zhang, Bhavya Kailkhura, and T Yong-Jin Han. Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning. In _International conference on machine learning_, pages 11117-11128. PMLR, 2020.
* [73] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015.
* [74] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015.
* [75] Shengjia Zhao, Michael Kim, Roshni Sahoo, Tengyu Ma, and Stefano Ermon. Calibrating predictions to decisions: A novel approach to multi-class calibration. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 22313-22324. Curran Associates, Inc., 2021.
* [76] Xujiang Zhao, Shu Hu, Jin-Hee Cho, and Feng Chen. Uncertainty-based Decision Making Using Deep Reinforcement Learning. In _2019 22th International Conference on Information Fusion (FUSION)_, pages 1-8, July 2019.
* [77] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In _International conference on machine learning_, pages 12697-12706. PMLR, 2021.
* [78] Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, and Subhrajit Roy. Batch calibration: Rethinking calibration for in-context learning and prompt engineering. _arXiv preprint arXiv:2309.17249_, 2023.

[MISSING_PAGE_FAIL:16]

Comparison with the standard approachAlgorithm 2 describes the standard approach to post-processing calibration, and Algorithm 3 describes our approach in more details and shows in blue the differences with the standard approach. Our approach adds a preprocessing step to keep only the confidences instead of the full probabilities vector. It can be seen as creating a surrogate "correctness" classifier and its associated calibration data. The calibrator is learned for the surrogate classifier and applied to the original classifier at inference time. Also, we add regularization for some scaling methods and we have only one binary calibrator instead of one per class.

Comparison with IRM and I-MaxIRM [72] and I-Max [49] are, like TvA, multiclass-to-binary reductions. This is why TvA cannot be applied on top of them: they already transform the multiclass problem into a binary one using a different strategy.

The shared class-wise strategy of [49] and the data ensemble strategy of [72] are described very briefly in subsections 3.2 and 3.3.2 of their respective papers and not rigorously justified. Our understanding is that these two strategies do exactly the same thing. To build the calibration set, they concatenate all the class probability vectors so that we get a big probability vector of size \(N.L\) (\(N\) samples and \(L\) classes) as predictions and similarly concatenate the one-hot embedding of the target class (a big vector with \(N\) ones and \(N.(L-1)\) zeros) as targets. Then, they learn a single calibrator. For each example, this calibrator aims to simultaneously increase the probabilities for the target class (target is \(1\)) and decrease all the other class probabilities (target is \(0\)). The single calibrator is applied to each class probability separately, meaning that the ranking of class probabilities can change, modifying the classifier prediction.

Our strategy derives from transforming the multiclass calibration into a single binary problem. The intuition is to learn the calibrator on a surrogate binary classifier and apply this calibrator to the original classifier. This binary classifier is built on top of the original classifier (by applying the max function to the class probabilities vector). They thus share their confidence. However, the binary classifier aims to solve a different task: predicting the correctness of the original classifier. To build the calibration set, we concatenate all the confidences (a vector of size N) as predictions and concatenate all the correctnesses as targets (also a vector of size N). The correctness value of a given example is \(1\) if the class prediction is correct; otherwise, it is \(0\). Then, we learn a single calibrator, similar to the strategy above. However, there is a key difference: this calibrator aims to increase the probabilities for correct predictions and decrease them for incorrect predictions. Note that our probabilities are all confidences (the maximum class probabilities), meaning we only consider the confidences, which the calibrator directly increases or decreases. In the strategy from [49] and [72], the calibrator has to manage all class probabilities (L times more), even the ones that do not matter, including the lowest class probabilities close to 0. This is less efficient (actually, while this can surely be fixed, the original implementations of IRM and I-Max could not run on ImageNet-21K). This point is closely linked to the analysis of the binary cross entropy loss for scaling methods in Subsection 4.2: when the prediction is incorrect, increasing the probability of the correct class indirectly decreases the confidence (strategy from [49] and [72]) while our strategy directly decreases the confidence.

I-Max is more complex because it modifies the Histogram Binning algorithm, while our approach does not. Additionally, [35] found that I-Max produces unusable probability vectors. Indeed, they do not sum up to \(1\), and normalizing them degrades the method's performance.

We wrote our paper with practicality and generality in mind. Contrary to [49] and [72], we demonstrate the generality of our strategy by applying it on top of existing calibration baselines of different natures (scaling and binary). One of our main goals is that practitioners can easily and quickly try our TvA approach, using just a few lines of code, which can significantly improve the calibration performance of their existing calibration pipeline while having no impact on the predicted class by design (except for VS and DC).

Comparison with ProCalAnother recent calibration method is ProCal [68]. However, its primary objective differs from ours: it "focuses on the problem of proximity bias in model calibration, a phenomenon wherein deep models tend to be more overconfident on data of low proximity". Its goal is to lower the difference in the confidence score values between regions of low and high density, i.e., to make the confidence score independent of a local density indicator called "proximity." There is no theoretical guarantee, however, that minimizing the proximity bias improves the confidence calibration, the focus of our work. Theorem 4.2 about the PIECE metric is a direct consequence of Jensen's inequality and is true for any random variable D, not necessarily a proximity score.

Theorem 5.1 is an interesting bias/variance decomposition of the Brier score. However, as this type of decomposition usually states, the error may come from bias (here, a wrong initial calibration) or high estimation variance (which can be related to low density but is not expressed as such in the decomposition). We experimentally compare our approach to the ProCal algorithm using the code provided by its authors and observe in Table 2 that our approach gives much better ECE confidence calibration and, for half of the models, also better PIECE values.

ProCal aims to achieve three goals: mitigate proximity bias, improve confidence calibration, and provide a plug-and-play method. We share the last two goals. Concerning improving confidence calibration, our approach has better results, as shown in Table 2. Both approaches are plug-and-play, but they apply very differently. ProCal is applied _after_ existing calibration methods to further improve calibration. It thus does not solve any of the four issues we identified (e.g., cross-entropy loss is still inefficient, and One-versus-All still leads to highly imbalanced problems). Our Top-versus-All approach is a reformulation of the calibration problem that uses a surrogate binary classifier. Existing approaches are applied to this surrogate classifier, which is how the four issues are solved. We do not propose a new method but a new way of applying existing methods. Our approach does not introduce new hyperparameters (except in the particular case of regularizing scaling methods). ProCal introduces several new hyperparameters, such as the choice of the distance, the number of KNN neighbors, or a shrinkage coefficient.

Comparison with Correctness-Aware LossA concurrent work [38] builds a calibration method on top of an intuition similar to ours: binarize the calibration problem. However, what the authors do with this intuition differs vastly from our approach. They derive a Correctness-Aware Loss (Eq. 7 of their paper), which is almost the standard binary cross-entropy loss we use for scaling methods but without a logarithm. They use this loss to learn a separate model that predicts a sample-wise temperature coefficient. This is a new calibration method, which is not straightforward to implement due to the numerous hyperparameters (network architecture, image transformations...). It also requires multiple inferences at test-time, which can be problematic in some production models. Our approach is, again, not a calibration method but a general reformulation of the calibration problem that enhances existing methods. By looking at their Table 1, they get an ECE of 2.22 on ImageNet (in-distribution), while our approach achieves values around 0.5 for most models in our paper's Table 1. Their method, contrary to TvA, improves the AUROC, but in our understanding, it seems mostly due to the use of image transformations, not from their proposed loss. Their method seems to work best in out-of-distribution scenarios, which is not the main objective of our paper. However, these good results for AUROC and out-of-distribution scenarios make this method complementary to our approach, and combining the two in some way could be promising.

\begin{table}
\begin{tabular}{l l r r r r} \hline \hline Model & Method & ECE (\(\downarrow\)) & MCE (\(\downarrow\)) & ACE (\(\downarrow\)) & PIECE (\(\downarrow\)) \\ \hline BeiT & conf & 3.60 & 1.52 & 3.58 & 4.26 \\  & TS & 2.99 & 0.76 & 3.08 & 3.56 \\  & MIR & 0.59 & **0.14** & 0.64 & 1.88 \\  & HB & 4.96 & 1.83 & 6.13 & 7.20 \\ \cline{2-5}  & conf+ProCal & 1.02 & 0.33 & 0.94 & 1.69 \\  & TS+ProCal & 1.52 & 0.76 & 1.45 & 2.05 \\  & MIR+ProCal & 0.61 & 0.15 & 0.71 & **1.41** \\  & HB+ProCal & 5.53 & 4.13 & 5.81 & 6.39 \\ \cline{2-5}  & TS\({}_{\text{TA}}\) & 2.10 & 0.49 & 2.20 & 2.88 \\  & Iso\({}_{\text{TA}}\) & 0.65 & **0.14** & 0.68 & 1.92 \\  & HB\({}_{\text{TA}}\) & **0.44** & 0.19 & **0.58** & 1.70 \\ \hline Mixer & conf & 10.78 & 5.00 & 10.78 & 10.86 \\  & TS & 4.81 & 1.92 & 4.72 & 5.57 \\  & MIR & 1.14 & 0.25 & 1.29 & 3.41 \\  & HB & 9.65 & 4.00 & 9.97 & 13.09 \\ \cline{2-5}  & conf+ProCal & 2.64 & 0.94 & 2.55 & 3.23 \\  & TS+ProCal & 1.32 & 0.38 & 1.22 & 2.13 \\  & MIR+ProCal & 0.83 & **0.14** & 0.88 & **2.08** \\  & HB+ProCal & 6.57 & 4.43 & 7.32 & 7.83 \\ \cline{2-5}  & TS\({}_{\text{TA}}\) & 2.51 & 0.74 & 2.50 & 4.32 \\  & Iso\({}_{\text{TA}}\) & 0.86 & 0.15 & 0.86 & 3.18 \\  & HB\({}_{\text{TA}}\) & **0.64** & 0.19 & **0.75** & 3.17 \\ \hline ResNet50 & conf & 8.59 & 4.58 & 8.50 & 8.74 \\  & TS & 5.03 & 2.48 & 5.01 & 5.34 \\  & MIR & 0.75 & 0.18 & 0.82 & 1.79 \\  & HB & 7.63 & 2.61 & 9.32 & 10.17 \\ \cline{2-5}  & conf+ProCal & 2.63 & 1.31 & 2.61 & 3.26 \\  & TS+ProCal & 1.66 & 0.66 & 1.53 & 2.50 \\  & MIR+ProCal & 0.76 & **0.17** & 0.74 & 1.78 \\  & HB+ProCal & 6.32 & 4.38 & 7.52 & 7.65 \\ \cline{2-5}  & TS\({}_{\text{TA}}\) & 6.89 & 1.17 & 6.95 & 7.13 \\  & Iso\({}_{\text{TA}}\) & 0.76 & 0.19 & 0.73 & 1.79 \\  & HB\({}_{\text{TA}}\) & **0.55** & 0.21 & **0.59** & **1.35** \\ \hline ViT & conf & 1.14 & 0.33 & 1.09 & 1.83 \\  & TS & 1.46 & 0.46 & 1.41 & 2.03 \\  & MIR & 0.64 & 0.13 & 0.75 & 1.54 \\  & HB & 4.59 & 2.32 & 7.07 & 7.20 \\ \cline{2-5}  & conf+ProCal & 0.81 & 0.22 & 0.81 & 1.71 \\  & TS+ProCal & 0.83 & 0.25 & 0.84 & 1.74 \\  & MIR+ProCal & 0.78 & 0.14 & 0.76 & 1.59 \\  & HB+ProCal & 6.80 & 4.72 & 7.42 & 7.55 \\ \cline{2-5}  & TS\({}_{\text{TA}}\) & 1.08 & 0.25 & 1.05 & 1.86 \\ Iso\({}_{\text{TA}}\) & 0.51 & **0.11** & 0.62 & 1.46 \\  & HB\({}_{\text{TA}}\) & **0.37** & 0.17 & **0.51** & **1.20** \\ \hline \hline \end{tabular}
\end{table}
Table 2: ECE, MCE, ACE, and PIECE in %. The experimental setting is the one used for Table 4 of [68]. The baselines are no calibration (conf), Temperature Scaling (TS), Multi Isotonic Regression (MIR), and Histogram Binning (HB). The ProCal calibration method [68] is applied _after_ one of the baselines, as symbolized by the “+” symbol. Our approach, Top-versus-All, changes what the baselines optimize and is symbolized by “\({}_{\text{TA}}\)”. We apply it for TS, Isotonic Regression (Iso), and HB. The best values for each model and metric are in bold.

Overall, HB\({}_{\text{TA}}\) is the best calibration method as it always gets the lowest ECE and ACE. Our TvA approach lowers PIECE and even achieves the lowest value for half of the models.

Theoretical justification for Top-versus-All in the case of Temperature Scaling

We define \(L\) the number of classes, \(f_{k}(x)\) the classifier estimated probability for class \(k\) and data sample \(x\), \(y\) the correct class, and the confidence \(s(x)\coloneqq\max_{k}f_{k}(x)\). The cross-entropy loss is \(l_{CE}(x,y)=-\sum_{k=1}^{L}1\{k=y\}\cdot\log(f_{k}(x))=-\log(f_{y}(x))\). Because the last layer of the classifier is a softmax function, \(f_{y}(x)=\frac{e^{x_{y}}}{\sum_{k}e^{x_{k}}}\) with \(z\) the logits vector. Note that we omit the writing variable \(x\) in the following for clarity.

Temperature scaling optimizes a coefficient \(T>0\) that scales the logits vector. Predicted probabilities become \(f_{y}(x)=\frac{e^{x_{y}/T}}{\sum_{k}e^{x_{k}/T}}\).

Let us first develop the standard cross-entropy loss when temperature scaling is applied:

\[l_{CE}=-\log(f_{y})=-\log(\frac{e^{z_{y}/T}}{\sum_{k}e^{z_{k}/T}})=-\left(\log (e^{z_{y}/T})-\log(\sum_{k}e^{z_{k}/T})\right)=-\frac{z_{y}}{T}+\log(\sum_{k} e^{z_{k}/T})\]

Let us compute its gradient:

\[\frac{\partial l_{CE}}{\partial T} =\frac{z_{y}}{T^{2}}+\frac{\partial\log(\sum_{k}e^{z_{k}/T})}{ \partial\sum_{k}e^{z_{k}/T}}\cdot\frac{\partial\sum_{k}e^{z_{k}/T}}{\partial T}\] by application of the chain rule on the second term. \[=\frac{z_{y}}{T^{2}}+\frac{1}{\sum_{k}e^{z_{k}/T}}\cdot\sum_{k} \frac{\partial e^{z_{k}/T}}{\partial T}\] \[=\frac{z_{y}}{T^{2}}+\frac{1}{\sum_{k}e^{z_{k}/T}}\cdot\sum_{k} \frac{\partial(e^{z_{k}})^{1/T}}{\partial T}\] \[=\frac{z_{y}}{T^{2}}+\frac{1}{\sum_{k}e^{z_{k}/T}}\cdot\sum_{k} \frac{\log(e^{z_{k}})}{-T^{2}}(e^{z_{k}})^{1/T}\] \[=\frac{z_{y}}{T^{2}}+\frac{1}{\sum_{k}e^{z_{k}/T}}\cdot\sum_{k} \frac{z_{k}\cdot e^{z_{k}/T}}{-T^{2}}\] \[=\frac{1}{T^{2}}\left(z_{y}-\sum_{k}z_{k}\cdot e^{z_{k}/T}\right)\] \[=\frac{1}{T^{2}}\left(z_{y}-\sum_{k}\frac{z_{k}\cdot e^{z_{k}/T} }{\sum_{j}e^{z_{j}/T}}\right)\] \[=\frac{1}{T^{2}}\left(z_{y}-\sum_{k}z_{k}\cdot f_{k}\right)\] (7)

For our TvA approach, the problem becomes binary. The classification output becomes the confidence \(s(x)=\max_{j\in\mathcal{Y}}f_{j}(x)\) and the ground truth label becomes a binary representation of the prediction correctness: \(y^{b}=1_{j=y}\) with \(\hat{y}(x)=\arg\max_{k\in\mathcal{Y}}f_{k}(x)\) and \(1\) the indicator function. The loss we use is the binary cross entropy \(l_{BCE}(x,y)=-\big{(}y^{b}\cdot\log s(x)+(1-y^{b})\cdot\log(1-s(x))\big{)}\)Let us compute the gradient:

\[\frac{\partial l_{BCE}}{\partial T} =\frac{\partial l_{BCE}}{\partial s}\cdot\frac{\partial s}{\partial T}\] \[=-\left(y^{b}\frac{1}{s}+(1-y^{b})\frac{-1}{1-s}\right)\cdot \frac{\partial s}{\partial T}\] \[=-\left(\frac{y^{b}(1-s)}{s(1-s)}-\frac{s(1-y^{b})}{s(1-s)} \right)\cdot\frac{\partial s}{\partial T}\] \[=\frac{s-y^{b}}{s(1-s)}\cdot\frac{\partial s}{\partial T}\] \[=\frac{s-y^{b}}{s(1-s)}\cdot\frac{\partial\frac{e^{z_{m}/T}}{ \sum_{k}e^{z_{k}/T}}}{\partial T}\text{ because }s=\max_{j}\frac{e^{z_{j}/T}}{ \sum_{k}e^{z_{k}/T}}=\frac{e^{z_{m}/T}}{\sum_{k}e^{z_{k}/T}}\text{ with }z_{m}=\max_{k}z_{k}\] \[=\frac{s-y^{b}}{s(1-s)}\cdot\frac{\frac{\partial e^{z_{m}/T}}{ \partial T}\sum_{k}e^{z_{k}/T}-e^{z_{m}/T}\frac{\partial\sum_{k}e^{z_{k}/T}}{ \partial T}}{(\sum_{k}e^{z_{k}/T})^{2}}\] \[=\frac{s-y^{b}}{s(1-s)}\cdot\frac{\frac{\log(e^{z_{m}})}{-T^{2}} (e^{z_{m}})^{1/T}\sum_{k}e^{z_{k}/T}-e^{z_{m}/T}\sum_{k}\frac{\log(e^{z_{k}})} {-T^{2}}(e^{z_{k}})^{1/T}}{(\sum_{k}e^{z_{k}/T})^{2}}\] \[=\frac{s-y^{b}}{s(1-s)}\cdot\frac{e^{z_{m}/T}}{T^{2}}\cdot\frac{ -\log(e^{z_{m}})\sum_{k}e^{z_{k}/T}+\sum_{k}\log(e^{z_{k}})(e^{z_{k}})^{1/T}}{ (\sum_{k}e^{z_{k}/T})^{2}}\] \[=\frac{s-y^{b}}{s(1-s)}\cdot\frac{e^{z_{m}/T}}{T^{2}}\cdot\frac{ -z_{m}\sum_{k}e^{z_{k}/T}+\sum_{k}z_{k}\cdot e^{z_{k}/T}}{(\sum_{k}e^{z_{k}/T} )^{2}}\] \[=\frac{s-y^{b}}{s(1-s)}\cdot\frac{1}{T^{2}}\cdot s\cdot\frac{-z_ {m}\sum_{k}e^{z_{k}/T}+\sum_{k}z_{k}\cdot e^{z_{k}/T}}{\sum_{k}e^{z_{k}/T}}\] \[=\frac{1}{T^{2}}\cdot\frac{s-y^{b}}{1-s}\cdot\left(\frac{\sum_{k }z_{k}\cdot e^{z_{k}/T}}{\sum_{k}e^{z_{k}/T}}-z_{m}\right)\] \[=\frac{1}{T^{2}}\cdot\frac{y^{b}-s}{1-s}\cdot\left(z_{m}-\frac{ \sum_{k}z_{k}\cdot e^{z_{k}/T}}{\sum_{k}e^{z_{k}/T}}\right)\] \[=\frac{1}{T^{2}}\cdot\frac{y^{b}-s}{1-s}\cdot\left(z_{m}-\sum_{k }\frac{z_{k}\cdot e^{z_{k}/T}}{\sum_{j}e^{z_{j}/T}}\right)\] \[=\frac{1}{T^{2}}\cdot\frac{y^{b}-s}{1-s}\cdot\left(z_{m}-\sum_{k }z_{k}\cdot f_{k}\right)\] (8)

- First case, the prediction is correct: \(y^{b}=1\) and \(z_{m}=z_{y}\). Let us inject these in (8): \(\frac{\partial l_{BCE}}{\partial T}=\frac{1}{T^{2}}\cdot(z_{y}-\sum_{k}z_{k} \cdot f_{k})=\frac{\partial l_{CE}}{\partial T}\). We thus get the same gradient as the standard cross-entropy loss.

- Second case, the prediction is incorrect: \(y^{b}=0\) and \(z_{m}>z_{y}\). (8) becomes: \(\frac{\partial l_{BCE}}{\partial T}=\frac{1}{T^{2}}\cdot\frac{s}{s-1}\cdot(z_ {m}-\sum_{k}z_{k}\cdot f_{k})\). By comparing to (7), we have the term \(\frac{1}{T^{2}}\cdot(z_{m}-\sum_{k}z_{k}\cdot f_{k})>\frac{1}{T^{2}}\cdot(z_{y} -\sum_{k}z_{k}\cdot f_{k})=\frac{\partial l_{CE}}{\partial T}\) and the remaining part of (8) \(|\frac{s}{s-1}|>1\) when \(s>0.5\).

So to recapitulate, \(|\frac{\partial l_{BCE}}{\partial T}|>|\frac{\partial l_{CE}}{\partial T}|\) when \(s>0.5\), which corresponds to the vast majority of data points as the classifier gets better calibrated. This is shown in Figure 1.

We also have \(\lim_{s\to 1}|\frac{s}{s-1}|=\infty\). In practice, \(s\) is not close enough to \(1\) to generate exploding gradients, so it just means that as confidences for wrong predictions gets higher, so does the gradient to reduce the confidences.

The conclusion is that for correct predictions, our approach does not change the optimization, but for incorrect predictions, the gradient is stronger and penalizes more heavily confident predictions that are wrong. This is also proven experimentally by looking at Table 8 where we see that average confidences of temperature scaling with the TvA approach (TS\({}_{\text{TvA}}\)) are lower than the ones using the standard approach (TS), for almost all networks. This makes the average confidences closer to the accuracy, showing reduced overconfidence.

## Appendix E Limits of classwise-ECE and top-label-ECE for a high number of classes

Let us define the ECE for class \(j\):

\[\text{ECE}_{j}=\sum_{b=1}^{B}\frac{n_{b}}{N}|\text{acc}(b,j)-\text{conf}(b,j)|\]

The difference compared to (2) is that now \(\text{acc}(b,j)\) corresponds to the proportion of class \(j\) in the bin. Also, \(\text{conf}(b,j)\) now is the average probability given to class \(j\) for all samples in the bin. Then, classwise-ECE [29] takes the average for all classes:

\[\text{ECE}_{\text{cw}}=\sum_{j=1}^{L}\text{ECE}_{j}\]

Classwise-ECE considers the full probabilities vectors: all the class probabilities for each prediction. However, this metric does not scale to large numbers of classes. Let us see why with an example.

Let us use a test set of \(N\) samples, \(N/L\) for each of the \(L\) classes (the dataset is balanced), and a high-accuracy classifier fairly calibrated. The classifier predicts \(N\) probability vectors of length \(L\). Predicted probabilities for class \(j\) are all the values of the vector at dimension \(j\). Because the classifier has a high accuracy and is fairly calibrated, around \(N/L\) values are close to \(1\) (corresponding to mostly correct predictions), and the remaining ones, around \(N-N/L\), are close to \(0\) (because the predicted class is not class \(j\), and the predicted probability is high for another class).

To compute \(\text{ECE}_{j}\) with equal size 15 bins, the predicted probabilities for class \(j\) are partitioned into 15 bins. The first bin (with probabilities close to 0), contains \(n_{1}\approx N-N/L\) samples while the last one (with probabilities close to 1) contains \(n_{B}\approx N/L\) samples. The remaining bins are usually even more empty. That means that the calibration error in the first bin is weighted \(n_{1}/n_{B}=L-1\) times more than the last one. For the 1000 classes of ImageNet, \(L-1=999\). Figure 4 shows the number of samples (\(n_{b}\)) in each bin for an ImageNet classifier.

Because the impact of the calibration error in the bin with the high probabilities is negligible relative to the bin with the low probabilities, the classwise-ECE mostly measures whether probabilities close to 0 are calibrated. We argue this is not what we are interested in: what matters more is the calibration of higher values of the probabilities.

Top-label ECE [18] is another interesting metric that does not scale to large numbers of classes either. Top-label-ECE divides data into subsets according to the predicted class, computes the ECEs of these subsets, and averages them. For an ImageNet test set of 25000 samples (25 per class), data is divided into 1000 subsets of \(\approx\) 25 samples each (the classifier is high-accuracy, most of the time the predicted class is equal to the true class). The ECE is computed for each subset containing only 25 samples. To compute the ECE, samples are typically partitioned into 15 bins. The number of samples per bin does not allow a correct estimation of the average confidence or accuracy.

Figure 4: Histogram of class probabilities for 3 random classes, for ViT-16/B on ImageNet.

Implementation details

### Models weights

* Model weights for CIFAR are from [45].
* Model weights for ImageNet come from torchvision [42].
* Model weights for ImageNet-21K are from [54].
* CLIP weights are from OpenAI's Hugging Face.
* Original weights for T5 and RoBERTa come from the Transformers library [67]. The models are fine-tuned for each task using prompt-based learning [36]. For more details, see [5].
* We used the model GPT-J https://huggingface.co/EleutherAI/gpt-j-6b (Apache-2.0 license) and Llama-2 https://huggingface.co/meta-llama/Llama-2-13b (license).

### Datasets

* _CIFAR-10 (C10)_ and _CIFAR-100 (C100)_[27] contain 60000 32x32 images corresponding to 10 and 100 classes, respectively. Data is split into subsets of 45000/5000/10000 images for train/validation/test. We concatenate the original validation and test sets, and randomly split that into a calibration set of size 5000, and a test set of size 10000.
* _ImageNet (IN)_[7] contains 1.3 million images from 1000 classes. Following [17], we randomly split the original validation test of size 50000 into a calibration set and a test set, both of size 25000.
* _ImageNet-21K (IN21K)_[54], in its winter21 version, contains 11 million images in the train set, and 522500 in the test set (50 for each of the 10450 classes). We randomly split the test set into equal-sized calibration and test set (261250 samples each, 25 per class).
* _Amazon Fine Foods_[43] is a collection of customer reviews for fine foods sold on Amazon. Reviews are categorized into bad, neutral, and good. The original validation set size is 78741 and test size 91606. We randomly split them into 78741 samples for calibration and 91606 for test.
* _DynaSent_[51] is a dynamic benchmark for sentiment analysis consisting of sentences annotated as positive, neutral, and negative. The original validation set size is 11160 and test size 4320. We randomly split them into 11160 samples for calibration and 4320 for test.
* _MNLI_[66] contains pairs of sentences labeled as contradiction, neutral, and entailment. The original validation set size is 19635 and test size 9815. We randomly split them into 19635 samples for calibration and 9815 for test.
* _Yahoo Answers (YA)_[73] contains question-answers pairs corresponding to 10 different topics. The original validation set size is 14000 and test size 60000. We randomly split them into 14000 samples for calibration and 60000 for test.
* TREC [61] contains questions categorized into 6 classes. The training set contains 5500 labeled questions, and the test set contains another 500.
* SST-5 [55] contains 11855 sentences corresponding to 5 sentiments (from very negative to very positive).
* DBpedia [74] contains text for topic classification with 14 classes. The training set contains 560000 samples, and the test set 5000.

### Code

* We used the library netcal [32] (Apache-2.0 license) for their implementation of binary methods for calibration and adapted their reliability diagrams code. For HB, we tested equal-size and equal-mass bins, and chose the best variant for each case. All hyperparameters were kept at their default values (10 bins for HB).
* We took inspiration from the official implementation of temperature scaling: https://github.com/gpleiss/temperature_scaling (MIT license).

* We took inspiration from the official implementation of Dirichlet calibration: https://github.com/dirichletcal/experiments_dnn (MIT license).
* We used the official implementation of I-Max: https://github.com/boschresearch/imax-calibration (AGPL-3.0 license).
* We used the official implementation of IRM: https://github.com/zhang64-llnl/Mix-n-Match-Calibration (MIT license).
* For evaluation, we used codes from https://github.com/JeremyNixon/uncertainty-metrics-1 (Apache-2.0 license) and https://github.com/IdoGali/benchmarking-uncertainty-estimation-performance (MIT license).
* We used PyTorch 2.0.0 [3] (BSD-style license).
* We used CIFAR models from [45] https://github.com/torrvision/focal_calibration (MIT license).
* We used ImageNet-21K models [54] https://github.com/Alibaba-MIIL/ImageNet21K (MIT license).
* We used CLIP models from HuggingFace's Transformers library [67] (Apache-2.0 license).
* We used pretrained language models from Transformers [67] and calibration codes from [5] https://github.com/lifan-yuan/PLMCalibration (MIT license).
* We used code from https://github.com/mominabbass/LinC for the calibration of LLMs using ICL, itself built upon https://github.com/tonyzhaozh/few-shot-learning (Apache-2.0 license).

## Appendix G Impact on selective classification

Selective classification aims to improve a model's prediction performance by trading-off coverage: a reject option allows to discard data that might result in wrong predictions, thus improving the accuracy on the remaining data. A strong standard baseline uses thresholding on the maximum softmax probability outputted by the classifier [13]. Improving confidence calibration means uncertainty is better quantified and should result in better selective classification.

Results in Table 1 show the superiority of Histogram Binning (applied with the right framework) in reducing the calibration error ECE. Unfortunately, it does not translate into improvements in selective classification. AUROC is a standard metric for selective classification [11]. Table 4 shows that Histogram Binning actually degrades the AUROC, while the best method is Isotonic Regression. Our TvA framework does not significantly impact the AUROC.

This paper addresses confidence calibration, usually measured by ECE. AUROC is a global rank-based metric for selective classification: it relies on the relative values of the scores, not their absolute values. Even though calibration and selective classification are related, improvement in calibration does not directly translate to better selective classification. This has been clearly demonstrated experimentally by [11].

A good example of that difference is the behavior of HB\({}_{\text{TvA}}\): it is the best calibration method overall but actually degrades the AUROC in most cases. Such a difference can be explained by the fact that selective classification benefits from a continuous score able to discriminate between certain and uncertain examples finely, but HB quantizes the confidences into, e.g., 10 different values.

\begin{table}
\begin{tabular}{l c|c|c|c|c|c|c|c|c|c|c|c|c|c|c} \hline \hline
**Model** & **Preproc.** & **IBM** & **I-Max** & **TS** & **TS\({}_{\text{N}_{\text{max}}}\)** & **VS** & **VS\({}_{\text{N}_{\text{max}}}\)** & **DC** & **DC\({}_{\text{N}_{\text{max}}}\)** & **JHB** & **HB\({}_{\text{N}_{\text{max}}}\)** & **Jho** & **Inv\({}_{\text{N}_{\text{max}}}\)** & **Beta** & **Bde\({}_{\text{N}_{\text{max}}}\)** & **BBQ** & **BBQ\({}_{\text{N}_{\text{max}}}\) \\ \hline ResNet-50 & 141 & 2021 & 543 & 215 & 218 & 214 & 217 & 226 & 226 & 129 & 1 & 66 & 1 & 873 & 22 & 1156 & 2 \\ VIF-B16 & 151 & 7119 & 524 & 225 & 226 & 217 & 222 & 232 & 225 & 127 & 1 & 61 & 1 & 917 & 23 & 1169 & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Computing time (in seconds) of the calibration on ImageNet, using one NVIDIA V100 GPU. The first column denotes the data preprocessing time, which includes computing the model logits for all calibration examples. Post-hoc calibration methods do not usually require much computing power compared to classifier training.

\begin{table}

\end{table}
Table 4: AUROC in % (higher is better). Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not. Improvements from the uncalibrated model are colored in blue and degradations in orange.

[MISSING_PAGE_EMPTY:26]

\begin{table}

\end{table}
Table 6: ECE in % (lower is better, best in bold) - full results for text classification datasets. Averages on 5 seeds. Mean relative improvements from TvA are shown (negative values for reductions of ECE). Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not. Values are averaged over five random seeds.

\begin{table}

\end{table}
Table 7: Standard deviations of ECE in % for 5 seeds.

\begin{table}

\end{table}
Table 8: Average confidence in %. Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not. Overconfidence (average confidence \(>\) accuracy) is shown in violet and underconfidence (average confidence \(<\) accuracy) in brown.

\begin{table}

\end{table} TABLE IX: Table 9: Accuracy in % (higher is better). Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not. Because classifiers can be well calibrated when not accurate (by having low accuracy and low confidence), it is important to monitor the accuracy. It is even better when the methods preserve the accuracy by design.

\begin{table}

\end{table}
Table 10: ECE with 15 equal mass bins in % (lower is better). Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not.

\begin{table}

\end{table}
Table 10: ECE with 15 equal mass bins in % (lower is better). Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not.

\begin{table}

\end{table}
Table 11: Brier score of the predicted class in \(10^{-2}\) (lower is better). Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not.

Large Language Models (LLMs) exhibit an in-context learning (ICL) capability, meaning they can learn from just a few examples in the context. It works by constructing a prompt that includes input-output pairs demonstrating the considered task, followed by a query for a new input. See [8] for a survey. Recent works develop calibration methods whose main goal is to improve the performance of ICL for LLMs, without requiring a complicated model fine-tuning. [77] uses a customized variant of Platt scaling (more specifically, Vector Scaling). Their method infers good values of the vector scaling parameters in a data-free procedure. The idea is that for a "content-free" input, e.g., "N/A", the calibrated probability has a 50% chance (for a binary classification task) of removing a bias toward the positive or negative class. In our paper, we denote this method as _ConC_. [1] builds on top of this work but uses a calibration set to learn the scaling parameters by minimizing the cross-entropy loss. This can be considered as Matrix Scaling. We denote this method as _LinC_. [78] proposes a per-class normalization of the probabilities on a given batch. [25] estimates the in-context model label marginal p(y) from limited data and uses it to calibrate the model probabilities. Paper [20] uses a Gaussian mixture model.

In our experiments, we have tested a two-step calibration. First, we use the state-of-the-art method LinC to maximize the accuracy by learning scaling parameters on a calibration set. Then, we apply \(\text{HB}_{\text{TVA}}\) to scale the confidences to lower the calibration error ECE, while preserving the accuracy gains. We use the same calibration set for the two methods. LinC performance depends on hyperparameter values, but to keep the experiments simple, we fixed the following values: 100 epochs, a learning rate of 0.001, and 300 calibration samples. It means that the reported performance of LinC is suboptimal and could be enhanced even more. We used the same experimental setting as [1]. We used the models GPT-J with 6B parameters [62] and Llama-2 with 13B parameters [59]. The text classification datasets are TREC [61] for question classification with 6 classes, SST-5 [55] for sentiment analysis with 5 classes, and DBpedia [74] for topic classification with 14 classes. The 0-shot, 1-shot, 4-shot, and 8-shot learning settings were tested. Five different sets of 300 test samples were randomly selected, and results are averaged over 5 seeds. We evaluated the accuracy and ECE for each configuration. Please see Table 12 for the results. In most cases, LinC+\(\text{HB}_{\text{TVA}}\) achieves the best accuracy and ECE.

\begin{table}
\begin{tabular}{l l|l|c c c|c c c} \hline \hline  & & Dataset & \multicolumn{2}{c|}{TREC} & \multicolumn{2}{c}{SST-5} & \multicolumn{2}{c}{DBpedia} \\ Model & Shots & Method & & & & & & \\ \hline \multirow{8}{*}{GPT-J 6B} & \multirow{8}{*}{0} & Uncalibrated & 24.7 & 29.7 & 33.7 & 22.5 & 19.7 & 27.4 \\  & & ConC & 40.0 & 14.0 & 40.7 & 10.3 & 47.7 & 24.6 \\  & & LinC & **58.9** & 26.4 & **46.3** & 11.0 & **62.2** & 12.8 \\  & & LinC+\(\text{HB}_{\text{TVA}}\) & **58.9** & **6.5** & **46.3** & **7.0** & **62.2** & **5.7** \\ \cline{2-10}  & \multirow{8}{*}{1} & Uncalibrated & 43.7 & 12.1 & 36.3 & 30.9 & 58.7 & 14.2 \\  & & ConC & 41.7 & 13.6 & **50.7** & 14.2 & 82.7 & 6.9 \\  & & LinC & **59.9** & 9.1 & 50.1 & 12.3 & **84.4** & 6.6 \\  & & LinC+\(\text{HB}_{\text{TVA}}\) & **59.9** & **3.9** & 50.1 & **7.3** & **84.4** & **5.1** \\ \cline{2-10}  & \multirow{8}{*}{8} & Uncalibrated & 26.0 & 41.6 & 51.3 & 28.2 & 89.0 & 15.7 \\  & & ConC & 40.3 & 14.4 & **54.3** & 8.8 & 94.0 & 6.9 \\  & & LinC & **57.9** & 9.7 & 53.6 & 10.6 & **94.3** & 5.7 \\  & & LinC+\(\text{HB}_{\text{TVA}}\) & **57.9** & **5.2** & 53.6 & **7.1** & **94.3** & **4.8** \\ \cline{2-10}  & \multirow{8}{*}{0} & Uncalibrated & 36.0 & 26.0 & 48.3 & 9.7 & 92.3 & 9.2 \\  & & ConC & 46.7 & 15.5 & 43.7 & 11.7 & 92.0 & 6.8 \\  & & LinC & **60.7** & **6.3** & **51.7** & 9.5 & **93.9** & 5.8 \\  & & LinC+\(\text{HB}_{\text{TVA}}\) & **60.7** & 6.6 & **51.7** & **7.6** & **93.9** & **2.6** \\ \hline \multirow{8}{*}{0} & \multirow{8}{*}{0} & Uncalibrated & 48.7 & 21.4 & 34.0 & 17.6 & 54.3 & 19.7 \\  & & ConC & 71.7 & 18.7 & 33.3 & 17.2 & 75.3 & 17.2 \\  & & LinC & **73.3** & 11.4 & **47.6** & 11.3 & **84.4** & 16.2 \\  & & LinC+\(\text{HB}_{\text{TVA}}\) & **73.3** & **9.3** & **47.6** & **6.7** & **84.4** & **4.1** \\ \cline{2-10}  & \multirow{8}{*}{1} & Uncalibrated & 63.0 & 8.6 & 41.3 & 29.4 & 90.7 & 11.4 \\  & & ConC & 76.0 & **5.9** & 41.0 & 12.6 & 92.3 & 5.2 \\  & & LinC & **79.7** & 6.3 & **48.7** & 12.1 & **93.1** & 4.4 \\  & & LinC+\(\text{HB}_{\text{TVA}}\) & **79.7** & 6.0 & **48.7** & **9.6** & **93.1** & **3.0** \\ \cline{2-10}  & \multirow{8}{*}{4} & Uncalibrated & 60.0 & 12.0 & 50.7 & 37.6 & 94.0 & 9.9 \\  & & ConC & 71.3 & 6.9 & 51.3 & 18.6 & **95.3** & 3.8 \\  & & LinC & **75.6** & 8.0 & **52.9** & 15.1 & **95.3** & 3.8 \\  & & LinC+\(\text{HB}_{\text{TVA}}\) & **75.6** & **4.0** & **52.9** & **7.6** & **95.3** & **2.2** \\ \cline{2-10}  & \multirow{8}{*}{8} & Uncalibrated & 70.0 & **5.2** & **55.0** & 7.0 & 94.7 & 5.9 \\  & & ConC & **73.7** & 12.6 & 44.0 & 22.8 & 94.3 & 3.9 \\ \cline{2-10}  & & LinC & 73.5 & 9.4 & 50.2 & 14.4 & **95.3** & 3.9 \\ \cline{2-10}  & \multirow{8}{*}{1} & LinC+\(\text{HB}_{\text{TVA}}\) & 73.5 & 7.0 & 50.2 & **4.2** & **95.3** & **2.1** \\ \hline \hline \end{tabular}
\end{table}
Table 12: Calibration methods applied to in-context learning of LLMs. Accuracy and ECE are in %.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our main claims in the abstract and introduction are that our approach solves the failure of many calibration methods when applied to problems with many classes and that we conduct extensive experiments. This reflects our analyses in Sections 3 and 4 and experimental results in Section 5 and Appendices G and H. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Section 6 is a separate "Limitations" section in the paper. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] * Justification: No strong theoretical result, but mathematical calculations are in Appendix D. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We use publicly available models and data and experiments details are provided in Section 5 and Appendix F. Our provided code can be used to reproduce our results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We use publicly available models and data and provide a clean code implementation aiming for easy reuse and reproducibility. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Details are provided in Section 5, Appendix F; and in the provided code. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All our tables show values averaged over five random seeds that generate different calibration and test datasets, as mentioned in Section 5. Standard deviations are reported in Table 7. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Table 3 in the Appendix provides computing times and the GPU model used. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper does conform with the NeurIPS Code of Ethics and we preserve anonymity. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Appendix B discusses the broader impacts of the work. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We believe that improved confidence calibration does not have a high risk of misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The papers producing the used datasets are cited. The code implementations used are cited and the URLs are provided in Appendix F. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Our provided code is well documented. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.