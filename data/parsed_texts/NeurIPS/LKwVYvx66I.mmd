# Almost Free: Self-concordance in Natural Exponential Families and an Application to Bandits

 Shuai Liu

University of Alberta

shuailiu7250@gmail.com &Alex Ayoub

University of Alberta

aayoub@ualberta.ca &Flore Sentenac

HEC Paris

sentenac@hec.fr &Xiaoqi Tan

University of Alberta

xiaoqi.tan@ualberta.ca &Csaba Szepesvari

University of Alberta

csaba.szepesvari@gmail.com

###### Abstract

We prove that single-parameter natural exponential families with subexponential tails are self-concordant with polynomial-sized parameters. For subgaussian natural exponential families we establish an exact characterization of the growth rate of the self-concordance parameter. Applying these findings to bandits allows us to fill gaps in the literature: We show that optimistic algorithms for generalized linear bandits enjoy regret bounds that are both second-order (scale with the variance of the optimal arm's reward distribution) and free of an exponential dependence on the bound of the problem parameter in the leading term. To the best of our knowledge, ours is the first regret bound for generalized linear bandits with subexponential tails, broadening the class of problems to include Poisson, exponential and gamma bandits.

## 1 Introduction

Single-parameter natural exponential families (NEFs) [14] abound in statistical applications [13, 15, 16, 17]. In this paper we study properties of NEFs and in doing so we make two main contributions:

1. We study how tail properties of the base distribution of a NEF impose limits on the NEF: if the base distribution is subexponential (subgaussian), we show that the NEF is _self-concordant_ with a stretch factor that grows inverse quadratically (respectively, linearly)
2. In generalized linear bandits whose reward distributions follow a NEF with subexponential base distribution, we show how this new result can be utilized to derive a novel second order regret bound whose leading term is free of exponential dependencies on the problem parameter -- the first such result for this setting.

The class of distributions our results extend to includes: normal, Poisson, exponential, gamma and negative binomial distributions. Our findings _partially_ address conjectures on whether generalized linear models with unbounded targets are self-concordant [1, 13]. This significantly generalizes the case when the targets are assumed to be bounded1[13, 15, 16] and thus extends the applicability of the results therein.

Self-concordance in NEFs turns out to be useful for both optimization and statistical estimation. The self-concordance property controls the remainder term, or approximation error, of a NEF's second-order Taylor expansion. This is useful in designing and analyzing both estimation and optimization methods. Historically the self-concordance property was first found to be useful for optimization [18; 19; 20] and later for statistical estimation [1; 21; 22]. In this paper, we will employ the self-concordance property in bandit problems [14] where it helps with controlling the error terms related to estimation.

Generalized linear bandits (GLBs) [23] has emerged as a standard framework for studying the role that nonlinear function approximation plays in decision making problems. Earlier works on GLBs (or its special case of of logistic bandits) [23; 24; 25] naively approximates the nonlinear function with a linear first order Taylor expansion. This approach ends up paying a price in the leading term of the regret bound that is exponential in the size of the true underlying parameter. In logistic bandits, [11] were the first to exploit the self-concordance property of the Bernoulli distribution in order to get regret bounds free of an exponential dependence on the size of the problem parameter in the leading term. [11] use self-concordance to get a tighter second order Taylor expansion that better captures the curvature of the sigmoid function. Employing improved self-concordant analysis, [1] get second-order regret bounds for logistic bandits and [12] extend these results to GLBs, under the assumption that the underlying reward distributions are self-concordant. We build upon this line of research and fill a gap in the bandit literature by designing and analyzing algorithms for GLBs with subexponential reward distributions. To the best of the authors' knowledge, our work considers the most general setting in the sense that all the previous works on GLBs considered subgaussian or bounded reward distributions while ours consider subexponential ones. For subgaussian rewards, these works [23; 19; 24; 25] still depend on \(\kappa\). Note that [23], also employs self-concordance in GLBs but they assume bounded reward and focuses on addressing non-stationarity of the environment. In addition, their bounds also scale with \(\kappa\) in the leading term, which can be exponentially large for logistic bandits. [12] consider a similar setting to ours. They assume the moment generating function of the base distribution \(Q\) is defined over the entire real line. This implies that \(Q\) does not have a tail as heavy as an exponential distribution hence less general than our setting. A concurrent work [11] develops novel confidence sets for self-concordant GLBs to improve the theoretical bounds while we prove that all GLBs with light-tailed base distribution are self-concordant. An interesting future direction would be applying their techniques to improve the bandit result of our work. [13] considers GLBs with bounded reward, which is known to be self-concordant, in a regime that only a limited number of decision policy updates is available.

The paper is organized as follows: In Section 2, we introduce single-parameter NEFs and review key properties relevant to our analysis. Section 3 demonstrates the self-concordance property of subexponential (subgaussian) NEFs, with a quadratic (respectively, linear) growth rate of the stretch factor. Additionally, we establish the tightness of the linear growth rate for subgaussian NEFs. In Section 4 we apply these findings to subexponential GLBs and derive novel second-order regret bounds devoid of exponential dependencies on the problem parameter in the leading term. Proofs omitted from the main text are provided in the appendix, except for those of well-known results, which are referenced accordingly.

## 2 Preliminaries

In this section we first introduce the notation we will use. We then introduce natural exponential families, review some of their basic properties and illustrate the concepts introduced by means of an example.

### Notation

For a real-valued differentiable function \(f\) defined over an open interval, we use \(\dot{f},\ddot{f}\) and \(\dddot{f}\) to denote the first, second and third derivative of \(f\). The set of reals is denoted by \(\mathbb{R}\), the set of nonnegative reals by \(\mathbb{R}_{+}\). For a set \(\mathcal{U}\subseteq\mathbb{R}\), we denote its interior by \(\mathcal{U}^{\circ}\). For real numbers \(a,b\), we use \(a\wedge b\) and \(a\lor b\) to denote the \(\min\{a,b\}\) and \(\max\{a,b\}\), respectively. With \(\phi\) a logical expression, \(\mathbb{I}\{\phi\}=1\) if \(\phi\) evaluates to true and \(\mathbb{I}\{\phi\}=0\), otherwise. We use \(f\lesssim g\) to indicate that \(g\) dominates \(f\) up to a constant factor over their common domain. For \(S\subseteq\mathbb{R}\), \(x\in\mathbb{R}\), we let \(S\pm x\) denote the set \(\{s\pm x\,:\,s\in S\}\). A distribution over the reals is centered if it has zero mean. We use \(\mathbb{P}()\) to denote the probability measure over the probability space that holds our random variables and we let \(\mathbb{E}\) to denote the expectation corresponding to this measure. By \(Y\sim Q\) we mean that the distribution of \(Y\), a random variable, is \(Q\).

### Single-parameter NEFs

In this section we give our definitions for the NEF. We only consider single-parameter natural exponential families when the base distribution is defined over the reals. We follow the approach of the beautifully written monograph of [1] that the reader is also referred to for any statements made about NEFs with no proofs.

Given a probability distribution \(Q\) over \(\mathbb{R}\) let \(M_{Q}:\mathbb{R}\to\mathbb{R}_{+}\cup\{+\infty\}\) denote its _moment generating function_ (MGF):

\[M_{Q}(u)=\int\exp(uy)Q(dy)\,,\qquad u\in\mathbb{R}\,.\]

We will find it convenient to also use the logarithm of the moment generating function, which is known as its _cumulant generating function_ (CGF). We denote this by \(\psi_{Q}:\mathbb{R}\to\mathbb{R}\cup\{+\infty\}\). Thus, \(\psi_{Q}(u)=\log M_{Q}(u)\).

Let \(\mathcal{U}_{Q}=\{u\in\mathbb{R}:M_{Q}(u)<\infty\}\) denote the domain of \(M_{Q}\) (and, thus the domain of \(\psi_{Q}\)). As it is well-known, \(\psi_{Q}\) is convex and hence \(\mathcal{U}_{Q}\) is always an interval (which, trivially, always contains \(0\)). For a subset of \(\mathcal{U}_{Q}\), denoted by \(\mathcal{U}\subseteq\mathcal{U}_{Q}\), we call \(\mathcal{Q}=(Q_{u})_{u\in\mathcal{U}}\) a natural exponential family (NEF) generated by \(Q\) where for any \(u\in\mathcal{U}\) we have

\[Q_{u}(dy)=\frac{1}{M_{Q}(u)}\exp(uy)Q(dy)\,.\]

It follows that \(Q_{u}\) is also a probability distribution over the reals for any \(u\in\mathcal{U}\) by definition. An equivalent, useful form for \(Q_{u}\) is \(Q_{u}(dy)=\exp(uy-\psi_{Q}(u))Q(dy)\). In applications, \(u\) denotes an unknown parameter that is to be estimated based on observations from \(Q_{u}\). Thus, \(\mathcal{U}\) allows one to express extra restrictions on the admissible parameters beyond the limits imposed by \(Q\). We call \(\mathcal{U}\) the _parameter space_, and \(\mathcal{U}_{Q}\) the _natural parameter space_.

The distributions \(Q\), \(Q_{u}\) and parameter \(u\) are referred to as the _base distribution_, the (exponentially) _tilted (base) distribution_ and the _tilting parameter_. An NEF is said to be _regular_ when \(\mathcal{U}_{Q}\) is open. It is easy to see that for any \(u,u_{0}\in\mathcal{U}_{Q}\), \(Q_{u}=(Q_{u_{0}})_{u-u_{0}}\), where the distribution on the right-hand side stands for the tilt of \(Q_{u_{0}}\) with parameter \(u-u_{0}\). As such, up to a constant shift of the parameter space, in a regular NEF, one can always assume that \(0\in\mathcal{U}_{Q}^{\circ}\). In fact, the same can be assumed for the parameter set, as long as \(\mathcal{U}^{\circ}\) is nonempty. If \(\mathcal{U}\) is an interval then \(\mathcal{U}^{\circ}=\emptyset\) means that \(\mathcal{U}\) is a singleton: An uninteresting case if we want to model a host of _non-identical_ distributions.

In a regular family, an equivalent way to parameterize a NEF is using the mean function (cf. Theorem 3.6, page 74, of [1]), \(\mu_{Q}:\mathcal{U}_{Q}\to\mathbb{R}\), which is defined as

\[\mu_{Q}(u)=\int y\ Q_{u}(dy)=\frac{\int y\exp(uy)Q(dy)}{M_{Q}(u)}.\]

Since \(Q_{0}=Q\), clearly, \(\mu_{Q}(0)\) is just the mean of \(Q\). To minimize clutter, when \(Q\) is clear from the context, we will write \(\mu\) instead of \(\mu_{Q}\). To illustrate the developments so far, we consider the example when the base distribution is an exponential distribution.

**Example 1** (Exponential distributions).: _For \(\lambda>0\), let \(Q\) be an exponential distribution with parameter \(\lambda\): \(Q(dy)=\mathbb{I}\{y\geq 0\}\lambda e^{-\lambda y}dy\). As is well known, the MGF of \(Q\) takes the form \(M_{Q}(u)=\frac{\lambda}{\lambda-u}\) when \(u<\lambda\) and \(M_{Q}(u)=\infty\) otherwise. Thus, \(\mathcal{U}_{Q}=\{u\in\mathbb{R}:M_{Q}(u)<\infty\}=(-\infty,\lambda)\). The mean function takes the form of_

\[\mu(u)=\frac{\int_{0}^{\infty}\lambda y\exp(-\lambda y)\exp(uy)dy}{M_{Q}(u)}= \frac{1}{\lambda-u},\qquad u<\lambda\,.\]

In what follows, we will need the following proposition to relate the central moments of \(Q_{u}\) to the derivatives of \(\psi_{q}\), the CGF.

**Proposition 2**.: _Let \(\mathcal{U}^{\prime}_{Q}\) be non-empty. Then, \(\psi_{Q}\) is infinitely differentiable on \(\mathcal{U}^{\prime}_{Q}\). Furthermore, the first three derivatives of \(\psi_{Q}\) at \(u\in\mathcal{U}_{Q}\) give the first moment, second and third central moments of \(Q_{u}\)._

In the context of Example 1, Proposition 2 gives that \(\psi_{Q}(u)=\log(\lambda)-\log(\lambda-u)\) when \(u<\lambda\). Then, \(\dot{\psi}_{Q}(u)=\frac{1}{\lambda-u}\), agreeing with our earlier computation.

## 3 Self-concordance of NEFs

This section contains the first set of main results of this paper. We start by giving our definition of self-concordance of NEFs, followed by a study of when this property is satisfied. We include a result that shows how self-concordance allows one to derive tail properties of members of the family from that of the base distribution.

In general, if the magnitude of a higher order derivative of a real function can be bounded in terms of a lower order derivative of the function, the function is said to be self-concordant [11]. This property is useful for studying how fast the function changes with its argument, as well as for deriving useful bounds on how well the function can be approximated by low order polynomials [10, 11, 12]. In the context of single-parameter natural exponential families, we propose the following natural definition:

**Definition 1** (Self-concordant NEF).: _Let \(\mathcal{Q}=(Q_{u})_{u\in\mathcal{U}}\) be a NEF with parameter set \(\mathcal{U}\subset\mathcal{U}^{\prime}_{Q}\) and some base distribution \(Q\). We say that \(\mathcal{Q}\) is self-concordant if there exists a nonnegative valued function \(\Gamma:\mathcal{U}\to\mathbb{R}_{+}\) such that_

\[|\ddot{\mu}(u)|\leq\Gamma(u)\dot{\mu}(u)\qquad\text{for all}\quad u\in \mathcal{U}\,.\] (1)

_Any function \(\Gamma:\mathcal{U}\to\mathbb{R}_{+}\) that satisfies Eq. (1) is called a stretch function of the NEF._

This definition takes inspiration from the works of [1, 11, 12] and [13] who define an analogous property (cf. Assumption 2 of [13]). By Proposition 2, we know that \(\dot{\mu}(u)\) is the variance of \(Q_{u}\), while \(\ddot{\mu}(u)\) is the third central moment. Hence, \(\dot{\mu}(u)\) is nonnegative, which explains why there is no absolute value on the right-hand side of Eq. (1).

According to our definition, we require that the (absolute value) of the second derivative of \(\mu\) is bounded by the first derivative, up to the "stretch factor" \(\Gamma(u)\). Clearly, provided that \(\dot{\mu}\) is positive over \(\mathcal{U}\), self-concordance is equivalent to stating that \(\Gamma_{Q}(u)\doteq\frac{|\dot{\mu}(u)|}{\dot{\mu}(u)}\) is finite valued over \(\mathcal{U}\). When \(\dot{\mu}(u)=0\) for some \(u\in\mathcal{U}\), one can show that \(Q_{u}\) must be a Dirac distribution and hence so does \(Q\) (\(Q\) and \(Q_{u}\) share their support). In this case, we also have \(\ddot{\mu}\equiv 0\) and hence any nonnegative valued function is a valid stretch-function. In particular, \(\Gamma_{Q}\equiv 0\) is also a valid stretch function.

It turns out that studying self-concordance property of distributions in detail can turn out much finer bounds than naively bounding \(\ddot{\mu}\) and \(\dot{\mu}\) separately.

**Example 3** (Avoiding exponential dependencies).: _Consider a NEF \(\mathcal{Q}\) with base distribution \(Q\), a Bernoulli distribution with parameter \(1/2\), we have \(\mathcal{U}_{Q}=\mathbb{R}\), \(\mu(u)=\frac{1}{1+e^{-u}}\) and thus \(\dot{\mu}=\mu(1-\mu)\), \(\ddot{\mu}=\dot{\mu}(1-2\mu)\). Hence \(\mathcal{Q}\) is \(\Gamma_{Q}\)-self-concordant with \(\Gamma_{Q}(u)\leq 1\) for all \(u\in\mathbb{R}\). This is an example where naively bounding \(\Gamma_{Q}\), by bounding the numerator and denominator separately over \([-s,s]\) for \(s>0\) gives a quantity of size \(e^{s}\), which lags far behind the constant we obtained with a direct calculation, or what we can get from the result in Section 3.2, which show a scaling of order \(O(s)\)._

As opposed to earlier literature where self-concordance is used [10, 11, 12], we allow a non-constant stretch-factor \(\Gamma\). As it turns out, this is necessary if \(\mathcal{U}=\mathcal{U}^{\prime}_{Q}\) is to be allowed:

**Example 4** (Non-constant stretch factor).: _Consider a NEF \(\mathcal{Q}\) with base distribution \(Q\). For \(Q\) an exponential distribution with parameter \(\lambda>0\), \(\mathcal{Q}\) is self-concordant over \(\mathcal{U}=\mathcal{U}_{Q}=(-\infty,\lambda)\) with \(\Gamma_{Q}(u)=\frac{2}{\lambda-u}\), \(u<\lambda\). Indeed, a simple calculation gives that for \(u<\lambda\), \(\dot{\mu}(u)=(\lambda-u)^{-2}\) and \(\ddot{\mu}(u)=2(\lambda-u)^{-3}\), and so \(|\ddot{\mu}(u)|/\dot{\mu}(u)=2/(\lambda-u)\)._

As was shown above, for the NEF built on the exponential distribution with parameter \(\lambda\), there is no constant stretch factor that makes the NEF self-concordant over the entirety of the natural parameter space. The main result of the next section shows that a non-constant stretch factor with growth similar

[MISSING_PAGE_FAIL:5]

In words, \(\mathcal{E}_{\text{right}}\) is the class of distributions over the reals whose right-tail displays an exponential decay governed with the rate parameter \(c_{1}>0\) and scaling constant \(C_{1}>0\). Similarly, we let \(\mathcal{E}_{\text{left}}(c_{1},C_{1})\) be the class of distributions \(Q\) over the reals such that for \(X\sim Q\), \(Y=\mathbb{E}X-X\) satisfies Eq. (3). With this notation, the first part of the previous proposition is equivalent to that if \(Q\in\mathcal{E}_{\text{right}}(c_{1},C_{1})\) is centered then \(M_{Q}\) stays below the function \(\lambda\mapsto 1+\frac{C_{1}\lambda^{2}}{c_{1}(c_{1}-\lambda)}\) over the interval \([0,c_{1})\). In particular, this means that \(\mathcal{U}_{Q}\) contains \([0,c_{1})\).

With this, we are ready to present our theorem that establishes the self-concordance property of NEFs with subexponential tails.

**Theorem 7**.: _Let \(Q\in\mathcal{E}_{\text{right}}(c_{1},C_{1})\cap\mathcal{E}_{\text{left}}(c_{ 2},C_{2})\) for some positive constants \(c_{i},C_{i}\), \(i=1,2\). Then, the NEF \(\mathcal{Q}=(Q_{u})_{u\in\mathcal{U}}\) is self-concordant. Moreover, the function \(\Gamma:\mathcal{U}\to\mathbb{R}_{+}\) defined by_

\[\Gamma(u)=\begin{cases}\frac{3}{2}\left[2eC_{1}c_{1}\left(\frac{1}{c_{1}-u} \right)^{2}+\frac{ub}{c_{1}-u}\right]+G_{Q}(C_{1},C_{2},c_{1},c_{2})&\text{if }0\leq u<c_{1}\\ \frac{3}{2}\left[2ec_{2}C_{2}\left(\frac{1}{c_{2}+u}\right)^{2}+\frac{-ub}{c_{ 2}+u}\right]+G_{Q}(C_{2},C_{1},c_{2},c_{1})&\text{if }-c_{2}<u<0,\end{cases}\]

_is a stretch function for the NEF \(\mathcal{Q}\), where \(G_{Q}(M_{1},M_{2},m_{1},m_{2})\) is a polynomial whose coefficients depend on \(Q\)._

The exact expression of \(G_{Q}\) can be found in Eq. (8). Let \(\mathcal{Q}\) be a NEF with base distribution \(Q\) when \(Q\) is a zero-mean Laplace distribution with variance \(2\lambda^{2}\). In this case, we can choose \(c_{1}=c_{2}=1/\lambda\), \(C_{1}=C_{2}=1/2\). The actual stretch function is \(\Gamma_{Q}(u)=\frac{8\lambda^{2}|u|}{(1/\lambda-u)(1/\lambda+u)(3\lambda^{4}u^ {2}+\lambda^{2})}\). Thus, the above theorem gives the correct behavior in that both the stretch function from the theorem and the actual stretch function \(\Gamma_{Q}\) blow up with the inverse of the distance to the boundaries of \(\mathcal{U}_{Q}\), except that the actual growth scales linearly with the inverse distance, while the theorem gives a quadratic scaling. It remains an open problem to see whether this quadratic order can be improved.

The following corollary is an immediate result of Lemma 5 and Theorem 7 (Appendix C.3), but can also be proved directly from the definitions (and we include a direct proof as part of the proof of Theorem 7).

**Corollary 8** (Distributions in regular NEFs are subexponential).: _Let \(u\in\mathcal{U}_{Q}^{\circ}\). Then \(Q_{u}\) is subexponential both on left and right._

Because of this result, there is essentially no loss in generality in only considering the subexponential case when working with NEFs. In particular, self-concordance in NEFs is "almost free".

Proof sketch for Theorem 7We sketch here the result for \(\mu(0)=0\), \(\dot{\mu}(0)>0\) and \(u\geq 0\). The arguments used to extend the result to all cases can be found in Appendix C. By Proposition 2, bounding the stretch function of a NEF amounts to showing

\[\Gamma_{Q}(u):=\frac{\int|(y-\mu(u))|^{3}Q_{u}(dy)}{\int(y-\mu(u))^{2}Q_{u}(dy )}\leq\Gamma(u),\]

where the division by \(\operatorname{Var}(Q_{u})=\int(y-\mu(u))^{2}Q(dy)\) is justified by Lemma 12. We split the proof of that upper bound into two steps: controlling the variance and the absolute third moment.

Step 1: Controlling the varianceSince \(\dot{\mu}(0)=\operatorname{Var}(Q)>0\), there exists a \(Q\)-dependent constant \(\eta>0\) and an interval \([-b,-a]\subset\mathbb{R}_{<0}\) s.t. \(Q([-b,-a])\geq\eta\) (Lemma 16). With this observation, we can show (Lemma 17):

\[\dot{\mu}(u)\geq a^{2}\eta\frac{e^{-ub}}{M_{Q}(u)}.\] (4)

Thus, the second moment decreases at most exponentially with the parameter \(u\).

Step 2: Controlling the absolute third central momentFirst, we use a classical result on moments of random variables (see the proof of \(i\Rightarrow ii\) for prop.2.5.2 in [21]):

\[\int|y-\mu(u)|^{3}Q_{u}(dy) =\int_{0}^{B}3t^{2}\mathbb{P}(|Y-\mu(u)|\geq t)dt+\int_{B}^{\infty }3t^{2}\mathbb{P}(|Y-\mu(u)|\geq t)dt\] \[\leq\frac{3}{2}B\dot{\mu}(u)+\int_{B}^{\infty}3t^{2}\mathbb{P}(| Y-\mu(u)|\geq t)dt,\] (5)where \(Y\sim Q_{u}\) and \(B\) is a constant to be optimized. It should remain small enough for the first term not to blow up. On the other hand, it should be large enough for the second term divided by the lower bound obtained on \(\dot{\mu}(u)\) to remain controlled.

To upper bound the second term in Eq. (5), we start by showing that the right tail of the tilted distribution \(Q_{u}\) is also subexponential (Lemma 18):

\[Q_{u}((t,\infty))\lesssim\frac{1}{M_{Q}(u)}e^{-(c_{1}-u)t}\frac{1}{c_{1}-u} \quad\text{for}\quad 0\leq u<c_{1}\,.\] (6)

Following this lemma, we get an upper bound on \(\mu(u)\) (Lemma 20):

\[0\leq\mu(u)\lesssim\left(\frac{1}{c_{1}-u}\right)^{2}\quad\text{for}\quad 0 \leq u<c_{1}\,.\] (7)

In the proof, we bound separately the positive and negative values in the second term of Eq. (5):

\[\int_{B}^{\infty}3t^{2}\mathbb{P}(|Y-\mu(u)|\geq t)dt=\underbrace{\int_{B}^{ \infty}3t^{2}\mathbb{P}(Y\geq\mu(u)+t)dt}_{\bigtriangleup}+\underbrace{\int_ {B}^{\infty}3t^{2}\mathbb{P}(Y\leq\mu(u)-t)dt}_{\bigtriangledup}\,.\]

We give here the sketch of proof on the bound of \(\spadesuit\) as the proof for bounding \(\heartsuit\) is nearly identical. From plugging in Eq. (6), we get:

\[\spadesuit\lesssim\frac{1}{M_{Q}(u)}\frac{1}{c_{1}-u}\int_{B}^{\infty}3t^{2}e^{ -(c_{1}-u)(t+\mu(u))}dt.\]

By choosing \(B\gtrsim\frac{ub}{c_{1}-u}+\left(\frac{1}{c_{1}-u}\right)^{2}\), some algebra gives:

\[\spadesuit\lesssim\frac{e^{-ub}}{M_{Q}(u)}\left(\frac{1+u^{2}b^{2}}{c_{1}^{3}C_ {1}^{3}}\right).\]

Setting \(B\gtrsim\left(\frac{1}{c_{1}-u}\right)^{2}+\frac{ub}{c_{2}+u}\) gives a similar bound on \(\heartsuit\). Chaining the bounds on \(\spadesuit\) and \(\heartsuit\) with Eq. (5) and Lemma 17 finishes the proof. 

### Self-concordance with a subgaussian base

In this section we refine the previous result for NEFs by considering the case when the base distribution is subgaussian. Let \(\sigma>0\). Recall that a centered distribution \(Q\) is \(\sigma\)-subgaussian if for all \(u\in\mathbb{R}\), \(M_{Q}(u)\leq e^{\sigma^{2}u^{2}/2}\) (or, equivalently, \(\psi_{Q}(u)\leq\sigma^{2}u^{2}/2\) for any \(u\in\mathbb{R}\)). A non-centered distribution is \(\sigma\)-subgaussian, if it is subgaussian after centering. Similarly to the subexponential case, one can show that a centered distribution \(Q\) is subgaussian if and only if for some \(\tau,C>0\), it holds that for any \(t\geq 0\), \(\mathbb{P}(|X|\geq t)\leq C\exp(-t^{2}/(2\tau^{2}))\) where \(X\sim Q\) (cf. Proposition 2.5.2 of [23]).3

Footnote 3: As for the quantitative relation between the parameters, it can be shown ([12]) that if for all \(u\in\mathbb{R}\), \(M_{Q}(u)\leq e^{\sigma^{2}u^{2}/2}\), then \(\mathbb{P}(|X|\geq t)\leq 2\exp(-t^{2}/(2\sigma^{2}))\), and if \(\mathbb{P}(|X|\geq t)\leq 2\exp(-t^{2}/(2\sigma^{2}))\), then for all \(u\in\mathbb{R}\), \(M_{Q}(u)\leq e^{4\sigma^{2}u^{2}}\).

Our promised result is as follows:

**Theorem 9**.: _Let \(Q\) be subgaussian. Then, the NEF \(\mathcal{Q}=(Q_{u})_{u\in\mathbb{R}}\) is self-concordant and \(\Gamma_{Q}(u)=O(|u|)\), \(u\in\mathbb{R}\)._

As it turns out, the linear growth exhibited in the previous result is tight for NEFs with a subgaussian base distribution:

**Theorem 10**.: _There exists a distribution \(Q\) that is subgaussian such that \(\limsup_{u\to\infty}\Gamma_{Q}(u)/u>0\)._

Again, this shows that even if we stay with subgaussian distributions, it would be limiting to only consider NEFs that are self-concordant with a bounded (or constant) stretch function over their natural parameter space.

Generalized Linear Bandits

In this section we apply the self-concordance property of subexponential NEFs in order to derive novel confidence sets and regret bounds for subexponential generalized linear bandits. To our knowledge, these are the first such results for parametric bandits with subexponential rewards.

### Bandit model

Following Filippi, Cappe, Garivier, and Szepesvari [11], we consider stochastic generalized linear bandit (GLB) models \(\mathcal{G}\) specified by a tuple \((\mathcal{X},\Theta,\mathcal{Q})\), where \(\mathcal{X}\subseteq\mathbb{R}^{d}\) is a non-empty arm set, \(\Theta\subseteq\mathbb{R}^{d}\) is a non-empty set of potential parameters, both closed for convenience, \(\mathcal{Q}=(Q_{u})_{u\in\mathcal{U}_{Q}}\) is a NEF with base distribution \(Q\). Without the loss of generality we assume that \(\mathcal{X}\) is a compact subset of the Euclidean unit ball of \(\mathbb{R}^{d}\).

In each round \(t\in\mathbb{N}^{+}\), the learner selects and plays an arm \(X_{t}\in\mathcal{X}\). As a response, they receive a reward \(Y_{t}\) sampled from the distribution \(Q_{X_{t}^{\top}\theta_{\star}}\), where \(\theta_{\star}\in\mathbb{R}^{d}\) is a parameter of the bandit environment, which is initially unknown to the learner. The learner's goal is to maximize its total expected reward. The GLB is _well-posed_ when

\[\mathcal{U}\doteq\{x^{\top}\theta\,:\,x\in\mathcal{X},\theta\in\Theta\} \subseteq\mathcal{U}_{Q}^{\circ}\]

holds, which we assume from now on. The condition that \(\mathcal{U}\subseteq\mathcal{U}_{Q}\) simply ensures that the reward distributions \(Q_{x^{\top}\theta}\) are defined regardless of the value of \((x,\theta)\in\mathcal{X}\times\Theta\). We require the stronger condition \(\mathcal{U}\subseteq\mathcal{U}_{Q}^{\circ}\) to exclude the boundaries of the interval \(\mathcal{U}_{Q}\). This way we avoid pathologies that arise when a parameter reaches the boundary of \(\mathcal{U}_{Q}\) (e.g., when \(Q\) is the exponential distribution with parameter \(\lambda\), the mean and variance of \(Q_{u}\) grow unbounded as \(u\) approaches \(\lambda\) from below).

The expected reward in round \(t\) given that the learner plays \(X_{t}\) is \(\mathbb{E}[Y_{t}|X_{t}]=\mu(X_{t}^{\top}\theta_{\star})\). The performance of the learner will be assessed by their pseudo regret \(R(T)\), which is the total cumulative shortfall of the mean reward of the arms the learner chose relative to optimal choice:

\[R(T)=\sum_{t=1}^{T}\mu(x_{\star}^{\top}\theta_{\star})-\mu(X_{t}^{\top}\theta_ {\star})\,.\]

Here, \(x_{\star}\in\arg\max_{x\in\mathcal{X}}\mu(x^{\top}\theta_{\star})\) is the arm that results in the best possible expected reward in a round. For simplicity, we assume that such an arm exists. We establish guarantees of our algorithm for a subclass of GLBs, captured by the following assumption:

**Assumption 1** (Subexponential base).: _The base distribution \(Q\) is subexponential both on the left and the right. In particular, \(Q\in\mathcal{E}_{\text{right}}(c_{1},C_{1})\cap\mathcal{E}_{\text{left}}(c_{2},C_{2})\) for some \(c_{i},C_{i}\) (\(i=1,2\)) positive numbers. Furthermore, \(-c_{2}<\inf\mathcal{U}\leq\sup\mathcal{U}<c_{1}\) and \(c_{1},c_{2}\) are known to the learner._

Note that in a well-posed GLB, \(\inf\mathcal{U}_{Q}<\inf\mathcal{U}\leq\sup\mathcal{U}<\sup\mathcal{U}_{Q}\). In light of this, the assumption just stated boils down to whether \(0\in\mathcal{U}_{Q}^{\circ}\), which, as discussed, is free when \(\mathcal{U}_{Q}^{\circ}\neq\emptyset\). Indeed, when \(0\in\mathcal{U}_{Q}^{\circ}\) holds, \(\inf\mathcal{U}_{Q}<0<\sup\mathcal{U}_{Q}\) and one can always find positive values \(c_{1},c_{2}\) such that \(\inf\mathcal{U}_{Q}<-c_{2}<\inf\mathcal{U}\leq\sup\mathcal{U}<c_{1}<\sup \mathcal{U}_{Q}\). Then, from Proposition 6 and some extra calculation one can conclude that the \(Q\in\mathcal{E}_{\text{right}}(c_{1},e^{-c_{1}\mathbb{E}X}M_{Q}(c_{1}))\cap \mathcal{E}_{\text{left}}(c_{2},e^{-c_{2}\mathbb{E}X}M_{Q}(-c_{2}))\). Since it is assumed that the learner has access to \(Q\), we see that Assumption 1 can be satisfied whenever \(0\in\mathcal{U}_{Q}^{\circ}\), which we think is a rather mild assumption. We will also for the sake of simplicity assume that the learner has access to \(S_{0}=\sup\{\|\theta\|\,:\,\theta\in\Theta\}\), \(S_{2}=\inf\mathcal{U}\), \(S_{1}=\sup\mathcal{U}\). These values will be used in setting the parameters of the algorithm. Note that it is not critical that the learner knows these exact values; appropriate bounds suffice. We also assume that the learner is given access to an upper bound on the worst-case variance over the parameter space \(\mathcal{U}\):

**Assumption 2** (Bounded Variance).: _The learner is given \(L\geq 1\) such that \(\sup_{u\in\mathcal{U}}\dot{\mu}(u)\leq L\)._

Note that since the GLB is well-posed, \(\sup_{u\in\mathcal{U}}\dot{\mu}(u)<\infty\) is automatically satisfied. Also, there is no loss of generality in assuming \(L\geq 1\). A crude upper bound on \(\sup_{u\in\mathcal{U}}\dot{\mu}(u)\) is \(C_{1}e/(c_{1}-S_{1})^{3}\lor C_{2}e/(c_{2}+S_{2})^{3}\), so this assumption is implied by the previous one.

### The OFU-GLB algorithm and its regret

Just like the previous works [11; 12; 13] which considered special cases of the generalized linear bandit problem, our algorithm follows the "optimism in the face of uncertainty" principle. In each time step, the algorithm constructs a confidence set \(\mathcal{C}_{t}\), based on past information, that contains the unknown parameter \(\theta_{\star}\) with a controlled probability. Next, the algorithm chooses a parameter \(\theta_{t}\), in the confidence set \(\mathcal{C}_{t}\), and an underlying action \(X_{t}\in\mathcal{X}\) such that the mean reward underlying \(X_{t}\) and \(\theta_{t}\) is as large as plausibly possible. Since \(\mu=\mu_{Q}\) is guaranteed to be an increasing function (recall that \(\hat{\mu}(u)\) is the variance of \(Q_{u}\) and is hence nonnegative), it suffices to find the maximizer of \(x^{\top}\theta\) where \((x,\theta)\in\mathcal{X}\times\Theta\). We call our algorithm, shown in Algorithm 1, OFU-GLB (Optimism in the Face of Uncertainty in Generalized Linear Bandits). The main novelty here is that our bandit model makes minimal assumptions.

```
0: GLB instance \(\mathcal{G}=(\mathcal{X},\Theta,\mathcal{Q})\) for\(t=1,2,\dots\)do  Construct \(\mathcal{C}_{t}\subset\Theta\) based on \(((X_{s},Y_{s}))_{s<t}\) and \(\mathcal{G}\)  Compute \((X_{t},\theta_{t})\in\operatorname*{arg\,max}_{(x,\theta)\in\mathcal{X}\times \mathcal{C}_{t}}x^{\top}\theta\)  Select arm \(X_{t}\) and receive reward \(Y_{t}\sim Q_{X_{t}^{\top}\theta_{\star}}\) endfor ```

**Algorithm 1** The OFU-GLB Algorithm

The confidence set is based on ideas from the work of Janz, Liu, Ayoub, and Szepesvari [14]. Note that this paper analyzed a randomized method for those GLBs where \(\mathcal{U}_{Q}=\mathbb{R}\) and \(\sup_{u\in\mathcal{U}_{t}}\Gamma_{Q}(u)<\infty\). The assumption that \(\mathcal{U}_{Q}=\mathbb{R}\) is restrictive, as it does not allow many common distributions (e.g., the exponential distribution). Thanks to Theorem 7, under our assumptions, \(\sup_{u\in\mathcal{U}}\Gamma_{Q}(u)<\infty\) follows. Then, an appropriate confidence set can be constructed based on Lemma 5, which also extended the corresponding result of Janz, Liu, Ayoub, and Szepesvari [14]. While the confidence set construction is based on the ideas of Janz, Liu, Ayoub, and Szepesvari [14], the main steps of the analysis are taken from [1] who analyzed logistic bandits, which are \(1\)-self-concordant. Our result follows by carefully modifying the proof of [14] and carefully propagating both the effect of replacing their confidence set with a different one, and the effect of \(\sup_{u\in\mathcal{U}}\Gamma_{Q}(u)>1\). This leads to the main result on GLBs:

**Theorem 11** (Regret upper bound of OFU-GLB).: _Let \(\delta\in(0,1]\) and \(T\) a positive integer and consider a well-posed GLB model \(\mathcal{G}=(\mathcal{X},\Theta,\mathcal{Q})\) and assume that Assumptions 1 and 2 hold. For \(\theta_{\star}\in\Theta\), let \(\kappa(\theta_{\star})=\frac{1}{\hat{\mu}(x^{\top}\theta_{\star})}\) and let \(\operatorname{Regret}(T,\theta_{\star})\) stand for the \(T\)-round regret of OFU-GLB when it interacts with a GLB specified by \(\theta_{\star}\). Then, with an appropriate construction of \(\mathcal{C}_{t}\), for any \(\theta_{\star}\in\Theta\), it holds that with probability at least \(1-\delta\),_

\[\operatorname{Regret}(T)=\tilde{\mathcal{O}}\left(d\sqrt{\hat{\mu}(x_{\star}^{ \top}\theta_{\star})T}+d\kappa(\theta_{\star})\right)\,,\]

_where \(\tilde{O}(\cdot)\) hides polylogarithmic factors in \(T,d,L,1/\delta\) and constants that depend on the base distribution \(Q\)._

This result extends the class of distributions for which OFU algorithms with parametric models achieve sublinear regret. Previous results in the literature [11; 12; 13; 14; 15] all assume that the base distribution is a natural exponential family with subgaussian tail and prove that the OFU algorithm enjoys sublinear regret. Thus, Theorem 11 extends the class of distributions for which optimistic algorithms enjoy sublinear regret to any natural exponential family with subexponential base distribution.

An essential quality of the result is that it makes the dependence of the regret on the instance \(\theta_{\star}\) explicit. Recalling that in a NEF, \(\hat{\mu}(u)\) is the variance of the tilted distribution \(Q_{u}\), we see that the leading term (shown as the first term on the right-hand side of the last display) scales with the variance of the optimal arm's reward distribution. In queuing theory, the service times of agents (actions) in an environment are often modeled as exponentially distributed random variables [10]. When aiming to minimize service times (maximize negative reward) with an exponential bandit model (with mean function \(\mu(x)=-1/x\)), the variance of the optimal arm's service time lower bounds that of the other arms. Furthermore, the dependence on \(\kappa\), a term that is inversely proportional to the optimal variance,is pushed to a second order term. In logistic bandits, \(\kappa\) can be exponentially large in the size of the parameter set \(S_{0}\) and thus much attention has been focused on mitigating its effect [13, 10, 11, 12]. Our regret bound also matches the lower bound in logistic bandits given by [1], thus our analysis is tight for this special case.

## 5 Conclusions and Future Work

The main contribution of this work establishes that all subexponential NEFs are self-concordant with a polynomial-sized stretch factor. We then applied this finding and derived regret bounds for subexponential GLBs that scale with the variance of the optimal arm's reward, which is the smallest variance amongst all arm's rewards in problems such as: minimizing service times [14] or minimizing insurance claim severity (dollars lost per claim) [1].

Our findings also have implications when performing maximum likelihood estimation with subexponential NEFs, which includes a rich family of generalized linear models (GLMs). Since the log loss in a NEF is the sum of a linear function and the NEF's CGF, the GLM's loss is self-concordant in the sense of (say) [1] whenever the NEF is self-concordant. While this is outside of the scope of our paper, it follows that this family of GLMs enjoy: \((i)\) fast rates of convergence to the minimizer for regularized empirical risk minimization [10], \((ii)\) fast rates for averaged stochastic gradient descent [1] and \((iii)\) fast rates for constrained optimization with first-order methods [15], without restrictive conditions on bounded responses, which previous works had to assume to achieve these results.

One interesting direction for future work would be either deriving a matching lower bound on the stretch function for subexponential NEFs or tighter analysis that matches the lower bound for subgaussian NEFs. Another potential avenue for future work would be in extending our results to other exponential families, beyond NEFs.

## Acknowledgments and Disclosure of Funding

The authors would like to thank Josif Pinelis for his helpful insights to our work and Samuel Robertson for reviewing and earlier version of this manuscript. Shuai Liu would like to acknowledge Alireza Bakhtiari for helpful discussions. Csaba Szepesvari also gratefully acknowledges funding from the Canada CIFAR AI Chairs Program, Amii and NSERC.

## References

* [AFC21] Marc Abeille, Louis Faury, and Clement Calauzenes. "Instance-wise minimax-optimal algorithms for logistic bandits". In: _International Conference on Artificial Intelligence and Statistics (AISTATS)_. 2021.
* [APS11] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. "Improved algorithms for linear stochastic bandits". In: _Advances in neural information processing systems (NeurIPS)_ (2011).
* [Bac10] Francis Bach. "Self-concordant analysis for logistic regression". In: _Electronic Journal of Statistics_ (2010).
* [Bac14] Francis Bach. "Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression". In: _The Journal of Machine Learning Research (JMLR)_ (2014).
* [BFR23] Blair Bilodeau, Dylan J Foster, and Daniel M Roy. "Minimax rates for conditional density estimation via empirical entropy". In: _The Annals of Statistics_ (2023).
* A Nonasymptotic Theory of Independence_. Oxford University Press, 2013.
* [Bro86] Lawrence D. Brown. "Fundamentals of statistical exponential families with applications in statistical decision theory". In: _Lecture Notes-Monograph Series_ (1986).
* [Dvu+20] Pavel Dvurechensky, Petr Ostroukhov, Kamil Safin, Shimrit Shtern, and Mathias Staudigl. "Self-concordant analysis of Frank-Wolfe algorithms". In: _International Conference on Machine Learning (ICML)_. 2020.
* [Fau+20] Louis Faury, Marc Abeille, Clement Calauzenes, and Olivier Fercoq. "Improved optimistic algorithms for logistic bandits". In: _International Conference on Machine Learning (ICML)_. 2020.
* [Fil+10] Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. "Parametric bandits: The generalized linear case". In: _Advances in Neural Information Processing Systems (NeurIPS)_. 2010.
* [GK98] Charles M Goldie and Claudia Kluppelberg. "Subexponential distributions". In: _A practical guide to heavy tails: Statistical techniques and applications_. Birkhauser Boston Inc., Aug. 1998, pp. 435-459.
* [GKT16] M. Goldburd, A. Khare, and D. Tevet. _Generalized linear models for insurance rating_. Casually Actuarial Society, 2016.
* [GN67] William J. Gordon and Gordon F. Newell. "Closed queuing systems with exponential servers". In: _Operations Research_ (1967).
* [Jan+24] David Janz, Shuai Liu, Alex Ayoub, and Csaba Szepesvari. "Exploration via linearly perturbed loss minimisation". In: _International Conference on Artificial Intelligence and Statistics (AISTATS)_. 2024.
* [Jun+21] Kwang-Sung Jun, Lalit Jain, Blake Mason, and Houssam Nassif. "Improved confidence bounds for the linear logistic model and applications to bandits". In: _International Conference on Machine Learning (ICML)_. 2021.
* [LLZ17] Lihong Li, Yu Lu, and Dengyong Zhou. "Provably optimal algorithms for generalized linear contextual bandits". In: _International Conference on Machine Learning (ICML)_. 2017.
* [LS20] Tor Lattimore and Csaba Szepesvari. _Bandit Algorithms_. Cambridge University Press, 2020.
* [LYJ24] Junghyun Lee, Se-Young Yun, and Kwang-Sung Jun. "A Unified Confidence Sequence for Generalized Linear Models, with Applications to Bandits". In: _Advances in Neural Information Processing Systems (NeurIPS)_. 2024.
* [Mar+19] Ulysse Marteau-Ferey, Dmitrii Ostrovskii, Francis Bach, and Alessandro Rudi. "Beyond least-squares: Fast rates for regularized empirical risk minimization through self-concordance". In: _Conference on Learning Theory (COLT)_. 2019.
* [ML09] Carl N. Morris and Kari F. Lock. "Unifying the named natural exponential families and their relatives". In: _The American Statistician_ (2009).
* [MN89] P. McCullagh and J. A. Nelder. _Generalized linear models_. Chapman & Hall / CRC, 1989.

* [Mor82] Carl N. Morris. "Natural exponential families with quadratic variance functions". In: _The Annals of Statistics_ (1982).
* A Basic Course_. Springer, 2004.
* [NN89] Yurii. Nesterov and A. Nemirovski. _Self-concordant functions and polynomial-time methods in convex programming_. USSR Academy of Sciences, Central Economic & Mathematic Institute, 1989.
* [NN94] Y. Nesterov and A. Nemirovski. _Interior point polynomial algorithms in convex programming_. Society for Industrial and Applied Mathematics, 1994.
* [NT08] Arkadi S Nemirovski and Michael J Todd. "Interior-point methods for optimization". In: _Acta Numerica_ (2008).
* [OB21] Dmitrii M. Ostrovskii and Francis Bach. "Finite-sample analysis of \(M\)-estimators using self-concordance". In: _Electronic Journal of Statistics_ (2021).
* [RH23] Philippe Rigollet and Jan-Christian Hutter. _High-Dimensional Statistics_. 2023. arXiv: 2310.19244 [math.ST].
* [Rus+21] Yoan Russac, Louis Faury, Olivier Cappe, and Aurelien Garivier. "Self-concordant analysis of generalized linear bandits with forgetting". In: _International Conference on Artificial Intelligence and Statistics (AISTATS)_. 2021.
* [Saw+24] Ayush Sawarni, Nirjhar Das, Siddharth Barman, and Gaurav Sinha. "Generalized Linear Bandits with Limited Adaptivity". In: (2024). arXiv: 2404.06831 [cs.LG].
* [SPL23] Aadirupa Saha, Aldo Pacchiano, and Jonathan Lee. "Dueling RL: Reinforcement learning with trajectory preferences". In: _International Conference on Artificial Intelligence and Statistics (AISTATS)_. 2023.
* [ST19] Tianxiao Sun and Quoc Tran-Dinh. "Generalized self-concordant functions: A recipe for Newton-type methods". In: _Mathematical Programming_ (2019).
* [Ver18] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_. Cambridge university press, 2018.
* [Wai19] Martin J. Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_. Cambridge University Press, 2019.
* [WJ08] Martin J Wainwright and Michael I Jordan. _Graphical models, exponential families, and variational inference_. Now Publishers, Inc., 2008.

Extra notation

The following extra notation will be used in the appendix: For vector \(x\in\mathbb{R}^{d}\), we let \(\|x\|\) denote its \(\ell_{2}\)-norm and for positive definite matrix \(M\in\mathbb{R}^{d\times d}\), we use \(\|x\|_{M}=\sqrt{x^{\top}Mx}\) to denote its \(M\)-weighted \(\ell_{2}\)-norm.

## Appendix B On subexponential (or "light tailed") distributions

We first prove Proposition 6, which we repeat for the convenience of the reader:

**Proposition 6**.: _Let \(c_{1},C_{1},c>0\), \(Y\sim Q\) and assume that \(\mathbb{E}Y=0\). If_

\[\mathbb{P}(Y\geq t)\leq C_{1}\exp(-c_{1}t)\qquad\text{for all }t\geq 0\] (3)

_then for any \(0\leq\lambda<c_{1}\), \(M_{Q}(\lambda)<1+\frac{C_{1}\lambda^{2}}{c_{1}(c_{1}-\lambda)}\). Furthermore, for any \(c>0\) such that \(M_{Q}(c)<\infty\), \(\mathbb{P}(Y\geq t)\leq M_{Q}(c)e^{-ct}\) holds for all \(t\geq 0\)._

We follow the proof of Theorem 2.13 from the book of Wainwright [20].

Proof.: We start with the second part. For this let \(t\geq 0\), \(c>0\). Then, by Chernoff's method, we have

\[\mathbb{P}(Y\geq t)\leq\mathbb{E}[e^{cX}]e^{-ct}=M_{Q}(c)e^{-ct}\,.\]

The first part requires more work. Let us start by bounding the \(p\)-th moment of the positive part of \(Y\), which we denote by \(Y_{+}\) (hence, \(Y_{+}=\max(Y,0)\)). We have

\[\mathbb{E}[Y_{+}^{p}] =\int_{0}^{\infty}\mathbb{P}(Y_{+}^{p}\geq u)du\] \[=p\int_{0}^{\infty}\mathbb{P}(Y_{+}\geq t)t^{p-1}dt \text{(change of variables with }u=t^{p}\text{)}\] \[=p\int_{0}^{\infty}\mathbb{P}(Y\geq t)t^{p-1}dt \text{(for }t>0\text{, }\{Y_{+}\geq t\}=\{Y\geq t\}\text{)}\] \[\leq C_{1}p\int_{0}^{\infty}e^{-c_{1}t}t^{p-1}dt \text{(assumption on }Y\text{)}\] \[\leq\frac{C_{1}p}{c_{1}^{p}}\int_{0}^{\infty}e^{-u}u^{p-1}du \text{(change of variables with }u=c_{1}t\text{)}\] \[=\frac{C_{1}p}{c_{1}^{p}}\Gamma(p-1) \text{(definition of the }\Gamma\text{ function)}\] \[=\frac{C_{1}}{c_{1}^{p}}p! \text{(property of the }\Gamma\text{ function)}\]

Now let \(0\leq\lambda<c_{1}\). Since \(Y\leq Y_{+}\), we have \(M_{Q}(\lambda)=\mathbb{E}[e^{\lambda Y}]\leq\mathbb{E}[e^{\lambda Y_{+}}]\). Hence, by the power-series expansion of the exponential, we get

\[M_{Q}(\lambda) \leq\mathbb{E}\left[e^{\lambda Y_{+}}\right]\] \[=1+\sum_{p=2}^{\infty}\lambda^{p}\frac{\mathbb{E}\left[Y_{+}^{p} \right]}{p!}\] \[\leq 1+C_{1}\sum_{p=2}^{\infty}\left(\frac{\lambda}{c_{1}}\right)^ {p}\] \[\leq 1+C_{1}\frac{\lambda}{c_{1}}\frac{\lambda}{c_{1}-\lambda}\,.\qed\]

We note in passing that since \(\lambda<c_{1}\), \(1+C_{1}\frac{\lambda}{c_{1}}\frac{\lambda}{c_{1}-\lambda}\leq 1+C_{1}\frac{ \lambda}{c_{1}-\lambda}\). That Eq. (3) implies that \(M_{Q}(\lambda)\leq 1+C_{1}\frac{\lambda}{c_{1}-\lambda}\) can also be obtained by refining the proof of Theorem 2.13 from the book of Wainwright [20].

Proof of Theorem 7

For the convenience of the reader we restate the theorem to be proven:

**Theorem 7**.: _Let \(Q\in\mathcal{E}_{\text{right}}(c_{1},C_{1})\cap\mathcal{E}_{\text{left}}(c_{2},C_ {2})\) for some positive constants \(c_{i},C_{i}\), \(i=1,2\). Then, the NEF \(\mathcal{Q}=(Q_{u})_{u\in\mathcal{U}}\) is self-concordant. Moreover, the function \(\Gamma:\mathcal{U}\to\mathbb{R}_{+}\) defined by_

\[\Gamma(u)=\begin{cases}\frac{3}{2}\left[2eC_{1}c_{1}\left(\frac{1}{c_{1}-u} \right)^{2}+\frac{ub}{c_{1}-u}\right]+G_{Q}(C_{1},C_{2},c_{1},c_{2})&\text{if }0\leq u<c_{1}\\ \frac{3}{2}\left[2ec_{2}C_{2}\left(\frac{1}{c_{2}+u}\right)^{2}+\frac{-ub}{c_{ 2}+u}\right]+G_{Q}(C_{2},C_{1},c_{2},c_{1})&\text{if }-c_{2}<u<0,\end{cases}\]

_is a stretch function for the NEF \(\mathcal{Q}\), where \(G_{Q}(M_{1},M_{2},m_{1},m_{2})\) is a polynomial whose coefficients depend on \(Q\)._

Note that the function \(\Gamma\) as defined above is non-decreasing on \(\mathcal{U}\cap\mathbb{R}^{+}\) and non-increasing on \(\mathcal{U}\cap\mathbb{R}^{-}\).

The actual form of \(G_{Q}\) is as follows: let \(a,b,\eta>0\) be such that \(Q([-b+\mu(0),-a+\mu(0)])>\eta\) and \(-a<0\). Then,

\[G_{Q}(M_{1},M_{2},m_{1},m_{2})=\frac{3}{2}b+\frac{1}{a^{2}\eta}\left(\frac{20 4}{e^{3}m_{1}^{3}M_{1}^{3}}+\frac{6b^{2}}{e^{3}m_{1}M^{3}}+\frac{81M_{2}+9M_{2 }m_{1}^{2}b^{2}}{m_{2}^{3}}\right)\] (8)

We note in passing that these values are not controlled by the tail behavior of the base distribution \(Q\). This can be seen, for example, by considering \(Q(dy)=(1-\eta)\mathbb{I}(y\geq 0)e^{-y}dy+\eta e^{-b}\delta_{\{-b\}}(dy)\). Tedious calculation shows that \(\lim_{u\to-\infty}\Gamma_{Q}(u)=\Omega(b)\) as \(b\to\infty\). And because \(Q\in\mathcal{E}_{\text{right}}(1,1)\cap\mathcal{E}_{\text{left}}(1,1)\), this shows that the tail behavior of \(Q\) is indeed insufficient to control the behavior of \(\Gamma_{Q}\).

We will prove this result in three parts: _(i)_\(\operatorname{Var}(Q)=0\)_(ii)_\(\operatorname{Var}(Q)>0\) and \(\mathcal{U}=[0,c_{1})\), _(iii)_\(\operatorname{Var}(Q)>0\) and \(\mathcal{U}=(-c_{2},0]\). The result follows from combining these cases.

The main work is to prove the result for \(\mathcal{U}=[0,c_{1})\), which is done in Proposition 13. Case _(iii)_ is handled by "reflection around the origin" (Corollary 14). Case _(i)_ is handled in Lemma 12 by showing that \(\Gamma_{Q}\equiv 0\) if \(\operatorname{Var}(Q)=0\).

We start with case _(i)_, the degenerate case when the variance of \(Q\) is zero.

**Lemma 12**.: _If \(\operatorname{Var}(Q)=0\) then \(\mathcal{U}_{Q}=\mathbb{R}\), \(Q_{u}=Q\) for all \(u\in\mathbb{R}\), and \(\Gamma_{Q}\equiv 0\). If \(\operatorname{Var}(Q)>0\) then \(\dot{\mu}\) is strictly positive over the entire set \(\mathcal{U}_{Q}^{\circ}\)._

Proof.: If \(\operatorname{Var}(Q)=0\), then \(Q\) is a Dirac on some \(\{v\}\). Then for all \(u\in\mathbb{R}\), \(M_{Q}(u)=\exp(uv)<\infty\) hence \(\psi_{Q}=\log M_{Q}\) is supported on \(\mathbb{R}\) and

\[Q_{u}(A)=\begin{cases}\frac{1}{M_{Q}(u)}\exp(uv)=1&\text{if }v\in A\\ 0&\text{otherwise.}\end{cases}\]

Hence \(Q_{u}=Q\) and \(\mathcal{Q}=(Q_{u})_{u\in\mathbb{R}}\) is trivially self-concordant with the stretch function defined to be \(\Gamma_{Q}\equiv 0\).

For the second part, by Proposition 2, we have that \(\dot{\mu}(u)=\int(y-\mu(u))^{2}Q_{u}(dy)\geq 0\). We will show \(\dot{\mu}(u)\neq 0\) by contradiction. Assume there exists \(u_{0}\in\mathcal{U}_{Q}^{\circ}\) such that \(\dot{\mu}(u_{0})=\int(y-\mu(u_{0}))^{2}Q_{u_{0}}(dy)=0\), then it follows that \(Q_{u_{0}}(dy)\) is a Dirac on \(\{\mu(u_{0})\}\), which implies for all \(A\in\mathcal{B}(\mathbb{R})\)

\[Q(A)=M_{Q}(u_{0})\cdot Q_{u_{0}}(A)=\begin{cases}M_{Q}(u_{0})&\{\mu(u_{0})\} \not\subseteq A\\ 0&\text{otherwise}\end{cases}\]

where \(\mathcal{B}(\mathbb{R})\) denotes the Borel sets on \(\mathbb{R}\). Then it follows that \(Q(dy)\) is also a Dirac on \(\mathbb{R}\), which contradicts that \(\operatorname{Var}(Q)>0\). 

Consider now the case when \(\operatorname{Var}(Q)>0\). By the result just stated \(\dot{\mu}\) is bounded away from zero over \(\mathcal{U}_{Q}^{\circ}\) and hence it is safe then to define \(\Gamma_{Q}\) with the ratio \(\frac{|\dot{\mu}(u)|}{\dot{\mu}(u)}\):

\[\Gamma_{Q}(u)=\frac{|\dot{\mu}(u)|}{\dot{\mu}(u)}\,,\qquad u\in\mathcal{U}_{Q}^ {\circ}\,.\]Thus, in order to show our result, it suffices to show \(\Gamma_{Q}\leq\Gamma\) with the function \(\Gamma\) as stated in the theorem. Thus, we will study \(\Gamma_{Q}\). First, notice that for all \(u\in\mathcal{U}_{Q}^{c}\), by Proposition 2,

\[\Gamma_{Q}(u)=\frac{|\int(y-\mu(u))^{3}Q_{u}(dy)|}{\int(y-\mu(u))^{2}Q_{u}(dy)} \leq\frac{\int|y-\mu(u)|^{3}Q_{u}(dy)}{\int|y-\mu(u)|^{2}Q_{u}(dy)}.\]

Let us now state the results that are concerned with cases _(ii)_ and _(iii)_ mentionned above. For case _(ii)_, i.e., when \(\mathcal{U}=[0,c_{1})\) we have the following result:

**Proposition 13**.: _Let \(Q\in\mathcal{E}_{\text{right}}(c_{1},C_{1})\cap\mathcal{E}_{\text{left}}(c_{ 2},C_{2})\) and \(\mathcal{U}=[0,c_{1})\). Define \(\Gamma:\mathcal{U}\to\mathbb{R}_{+}\) by_

\[\Gamma(u) =\frac{3}{2}\left[2c_{0}\left(\frac{1}{c_{1}-u}\right)^{2}+\frac {ub}{c_{1}-u}+\frac{|u|b}{c_{1}+|u|}\right]+\frac{1}{a^{2}\eta}\left(\frac{20 4+6u^{2}b^{2}}{c_{0}^{3}}+\frac{81C_{2}+9C_{2}u^{2}b^{2}}{(u+c_{2})^{3}}\right)\] \[\leq\frac{3}{2}\left[2c_{0}\left(\frac{1}{c_{1}-u}\right)^{2}+ \frac{ub}{c_{1}-u}\right]+G_{Q}(C_{1},C_{2},c_{1},c_{2})\,,\]

_where \(c_{0}=C_{1}\cdot c_{1}\cdot e\). Then, for appropriate values of \(\eta,a,b>0\) that depend on the base distribution \(Q\), we have \(\Gamma_{Q}\leq\Gamma\) over \(\mathcal{U}\)._

For case _(iii)_, i.e., when \(\mathcal{U}=(-c_{2},0]\), we have the following result:

**Corollary 14**.: _Let \(Q\in\mathcal{E}_{\text{right}}(c_{1},C_{1})\cap\mathcal{E}_{\text{left}}(c_{ 2},C_{2})\) and \(\mathcal{U}=(-c_{2},0]\). Define \(\Gamma:\mathcal{U}\to\mathbb{R}_{+}\) by_

\[\Gamma(u) =\frac{3}{2}\left[2c_{0}\left(\frac{1}{c_{2}-|u|}\right)^{2}+ \frac{|u|b}{c_{2}-|u|}+\frac{|u|b}{c_{1}+|u|}\right]+\frac{1}{a^{2}\eta}\left( \frac{204+6u^{2}b^{2}}{c_{0}^{3}}+\frac{81C_{1}+9C_{1}u^{2}b^{2}}{(|u|+c_{1} )^{3}}\right)\] \[\leq\frac{3}{2}\left[2c_{0}\left(\frac{1}{c_{2}-|u|}\right)^{2}+ \frac{|u|b}{c_{2}-|u|}\right]+G_{Q}(C_{2},C_{1},c_{2},c_{1})\,,\]

_where \(c_{0}=C_{2}\cdot c_{2}\cdot e\). Then, for appropriate values of \(\eta,a,b>0\) that depend on the base distribution \(Q\), we have \(\Gamma_{Q}\leq\Gamma\) over \(\mathcal{U}\)._

### Proof of Proposition 13

The key idea is to convert \(\ddot{\mu}(\cdot)\) and \(\dot{\mu}(\cdot)\) to third and second central moments respectively by Proposition 2 and bound the third central moment in terms of the second central moment (variance).

We start by an elementary observation that says that the self-concordance properties of a NEF do not change when the base distribution is shifted by a constant:

**Lemma 15**.: _Let \(Y\sim Q\), \(c\in\mathbb{R}\) and define \(Q^{+c}\) to be the distribution of \(Y+c\). Then, \(\mathcal{U}_{Q}=\mathcal{U}_{Q^{+c}}\), \(M_{Q_{+c}}(u)=e^{-uc}M_{Q}(u)\) for all \(u\in\mathbb{R}\), and \(\Gamma_{Q}=\Gamma_{Q^{+c}}\) (here, we take \(\mathcal{U}=\mathcal{U}_{Q}=\mathcal{U}_{Q^{+c}}\))._

Proof.: By definition \(M_{Q}(u)=\mathbb{E}e^{uY}\) and \(M_{Q^{+c}}(u)=\mathbb{E}e^{u(Y+c)}\). Hence,

\[M_{Q^{+c}}(u)=\mathbb{E}e^{u(Y+c)}=e^{uc}\mathbb{E}e^{uY}=e^{uc}M_{Q}(u)\,.\]

This shows that \(\mathcal{U}_{Q}=\mathcal{U}_{Q^{+c}}\) and that the desired relationship between \(M_{Q}\) and \(M_{Q^{+c}}\) hold. Now, from the definition that the CGF is the logarithm of the MGF, it follows that \(\psi_{Q^{+c}}(u)=uc+\psi_{Q}(u)\). Hence, \(\ddot{\psi}_{Q^{+c}}=\ddot{\psi}_{Q}\) and \(\dot{\psi}_{Q^{+c}}=\dot{\psi}_{Q}\), which implies that \(\Gamma_{Q}=\Gamma_{Q^{+c}}\). 

Thanks to this result, from a bound on the self-concordance function of centered distributions, we can deduce a bound on the self-concordance function of the non-centered ones.

We thus first work on establishing the bound when \(Q\) is centered.

Since the theorem statement holds trivially when the variance \(\mathrm{Var}(Q)\) of \(Q\) is zero, we will also assume with no loss of generality in some of the results below that \(Q\) has positive variance.

**Lemma 16**.: _Take a distribution \(Q\) with zero mean and positive variance. Then, there exist \(\eta>0\) and \(0<a\leq b\) distribution dependent constants such that \(Q([-b,-a])\geq\eta\)._Proof.: Since \(\dot{\mu}(0)=\operatorname{Var}(Q)>0\), there exists \(a>0\) and \(\alpha>0\) such that \(Q((-\infty,-a]))>\alpha\). As \(\lim_{x\to 0}Q((-\infty,-x]))=0\), we can find some \(b>0\) s.t.

\[Q\left((-\infty,-b]\right)\leq\frac{\alpha}{2}.\]

This implies:

\[Q\left([-b,-a]\right)\geq\frac{\alpha}{2}.\]

The lemma thus holds with \(a\) and \(b\) described above, and \(\eta=\frac{\alpha}{2}\). 

**Lemma 17**.: _Take a distribution \(Q\) with zero mean and positive variance. With \(\eta,a,b\) as in Lemma 16, for all \(u\in\mathcal{U}_{Q}^{\circ}\), it holds that_

\[\dot{\mu}(u)\geq a^{2}\eta\frac{e^{-ub}}{M_{Q}(u)}\,.\]

Proof.: For \(a,b\) described in Lemma 16, we have:

\[\int_{\mathbb{R}}(y-\mu(u))^{2}Q_{u}(dy) \geq\int_{-b}^{-a}(y-\mu(u))^{2}Q_{u}(dy),\] \[\geq\int_{-b}^{-a}(y-\mu(0))^{2}\frac{\exp(uy)}{M_{Q}(u)}Q(dy),\] \[\geq a^{2}\frac{\exp(-ub)}{M_{Q}(u)}\int_{-b}^{-a}Q(dy),\] \[\geq\frac{a^{2}e^{-ub}}{M_{Q}(u)}\eta.\]

The first inequality holds as \((y-\mu(u))^{2}\) is non-negative; the second as \(\mu(u)\) increases with \(u\); the third as \(-b\leq-a<\mu(0)\), \(\mu(0)=0\) and \(u\geq 0\); and the last one by Lemma 16. 

For the upper bound, we present lemmas that bound the (upper and lower) tails of \(Q_{u}\) and the mean \(\mu(u)\).

**Lemma 18**.: _Take \(Q\in\mathcal{E}_{\text{right}}(c_{1},C_{1})\) a centered distribution. Then, for all \(0\leq u<c_{1}\) and \(t\geq 0\), we have_

\[Q_{u}\left((t,+\infty)\right)\leq\frac{C_{1}e}{M_{Q}(u)}e^{-(c_{1}-u)t}\left( 1+\frac{u}{c_{1}-u}\right)\,.\]

Proof.: The inequality is trivially satisfied when \(u=0\). Indeed, in this case \(Q=Q_{0}\), \(M_{Q}(0)=1\), which together with \(\dot{Q}\in\mathcal{E}_{\text{right}}(c_{1},C_{1})\) implies the inequality.

Let us now assume that \(0<u<c_{1}\). Let \(v>0\) be a constant to be chosen later. Then we have that

\[M_{Q}(u)Q_{u}((t,\infty)) =\int_{t}^{\infty}e^{uy}Q(dy)\] \[=\sum_{k=0}^{\infty}\int_{t+kv}^{t+(k+1)v}e^{uy}Q(dy)\] \[\leq\sum_{k=0}^{\infty}e^{u(t+kv+v)}\int_{t+kv}^{+\infty}Q(dy)\] \[\leq\sum_{k=0}^{\infty}e^{u(t+kv+v)}C_{1}e^{-c_{1}(t+kv)}\] \[=C_{1}e^{-(c_{1}-u)t}e^{uv}\sum_{k=0}^{\infty}e^{-(c_{1}-u)vk}.\]We choose \(v=1/u>0\). Then \(M_{Q}(u)Q_{u}((t,\infty))\) can be upper bounded by

\[M_{Q}(u)Q_{u}((t,\infty)) \leq C_{1}e\cdot e^{-(c_{1}-u)t}\sum_{k=0}^{\infty}e^{-\frac{c_{1}-u }{u}k}\] \[=C_{1}e\cdot e^{-(c_{1}-u)t}\frac{1}{1-e^{-\frac{c_{1}-u}{u}}}\] \[\leq C_{1}e\cdot e^{-(c_{1}-u)t}\left(1+\frac{u}{c_{1}-u}\right)\,,\]

where in the last inequality, we used the fact that for all \(x>0\), \(\frac{e^{x}}{e^{x}-1}\leq 1+\frac{1}{x}\). 

**Remark 1**.: _For \(Q\) a centered exponential distribution \(\mathrm{Exp}(c)\) with rate parameter \(c\), the moment generating function is \(M_{Q}(u)=e^{-\frac{u}{c}}\frac{c}{c-u}\). On the other hand, \(Q_{u}(t,\infty)=e^{-\frac{u}{c}}e^{-(c-u)t}\). So Lemma 18 is order tight in its dependency on \(c-u\)._

**Lemma 19**.: _Take \(Q\in\mathcal{E}_{\text{left}}(c_{2},C_{2})\) a centered distribution. Then, for all \(u,t\geq 0\), we have_

\[Q_{u}\left((-\infty,-t)\right)\leq\frac{1}{M_{Q}(u)}C_{2}e^{-(u+c_{2})t}\,.\]

Proof.: We again separate the \(u=0\) case. When \(u=0\), \(Q=Q_{0}\), \(M_{Q}(0)=1\) and the inequality is equivalent to \(Q\in\mathcal{E}_{\text{left}}(c_{2},C_{2})\).

Consider now \(u>0\). Then,

\[M_{Q}(u)Q_{u}\left((-\infty,-t)\right)= \int_{-\infty}^{-t}e^{uy}Q(dy),\] \[\leq e^{-ut}\int_{-\infty}^{-t}Q(dy),\] \[\leq C_{2}e^{-ut-c_{2}t}.\]

**Lemma 20**.: _Take \(Q\in\mathcal{E}_{\text{right}}(c_{1},C_{1})\) with zero mean and positive variance. Define \(c_{0}=c_{1}\cdot C_{1}\cdot e\). Then, for all \(0\leq u<c_{1}\), it holds that_

\[0=\mu(0)\leq\mu(u)\leq c_{0}\left(\frac{1}{c_{1}-u}\right)^{2}\,.\]

Proof.: For \(u=0\), \(\mu(0)=0\) which satisfies the inequality. We now consider \(0<u<c_{1}\). We have

\[\mu(u)= \int_{0}^{+\infty}Q_{u}\left((y,+\infty)\right)dy-\int_{-\infty}^ {0}Q_{u}\left((-\infty,-y)\right)dy\] \[\leq \int_{0}^{+\infty}Q_{u}\left((y,+\infty)\right)dy.\]

By Lemma 18, this implies:

\[\mu(u) \leq\frac{C_{1}e}{M_{Q}(u)}\int_{0}^{\infty}e^{-(c_{1}-u)t} \left(\frac{c_{1}}{c_{1}-u}\right)dt\] \[=\frac{C_{1}e}{M_{Q}(u)}\left(\frac{c_{1}}{c_{1}-u}\right)\frac{1 }{c_{1}-u}\] \[=\frac{c_{1}C_{1}e}{M_{Q}(u)}\left(\frac{1}{c_{1}-u}\right)^{2}.\]

By Jensen's inequality, \(M_{Q}(u)=\mathbb{E}_{Q}[e^{uY}]\geq e^{u\mathbb{E}_{Q}[Y]}=1\), finishing the proof.

Proof of Proposition 13.: As noted beforehand, since the statement holds trivially when the variance of \(Q\) is zero, we assume it is positive. By our discussion beforehand, we also assume first that \(Q\) is centered, so \(\mu(0)=0\).

Let \(Y\sim Q_{u}\). Take \(B>0\) a constant to be optimized later. We have

\[\int\left|y-\mu(u)\right|^{3}\,Q_{u}(dy)= \int_{0}^{+\infty}3t^{2}\,\mathbb{P}\left(\left|Y-\mu(u)\right| \geq t\right)dt,\] \[= \underbrace{\int_{0}^{B}3t^{2}\,\mathbb{P}\left(\left|Y-\mu(u) \right|\geq t\right)dt}_{(i)}+\underbrace{\int_{B}^{+\infty}3t^{2}\,\mathbb{P} \left(\left|Y-\mu(u)\right|\geq t\right)dt}_{(ii)}.\]

The following bound holds:

\[(i)\leq 3B\int_{0}^{B}t\,\mathbb{P}\left(\left|Y-\mu(u)\right|\geq t \right)dt,\] \[\leq \frac{3B}{2}\dot{\mu}(u).\] (9)

We also have:

\[(ii)\leq \underbrace{\int_{B}^{+\infty}3t^{2}\,\mathbb{P}\left(Y-\mu(u) \geq t\right)dt}_{(ii,a)}+\underbrace{\int_{B}^{+\infty}3t^{2}\,\mathbb{P} \left(Y\leq-\left(t-\mu(u)\right)\right)dt}_{(ii,b)}.\]

Set \(B=2c_{0}\left(\frac{1}{c_{1}-u}\right)^{2}+\frac{ub}{c_{1}-u}+\frac{ub}{u+c_{2}}\) and \(B^{\prime}=2c_{0}\left(\frac{1}{c_{1}-u}\right)^{2}\). Then we can upper bound \((ii,a)\) using Lemma 18.

\[(ii,a) \leq\frac{c_{0}}{M_{Q}(u)}\left(\frac{1}{c_{1}-u}\right)\int_{B}^ {\infty}3t^{2}e^{-(c_{1}-u)(t+\mu(u))}dt\] \[\leq\frac{c_{0}}{M_{Q}(u)}\left(\frac{1}{c_{1}-u}\right)\int_{B}^ {\infty}3t^{2}e^{-(c_{1}-u)t}dt\] \[\leq\frac{c_{0}e^{-ub}}{M_{Q}(u)}\left(\frac{1}{c_{1}-u}\right) \int_{B}^{\infty}3t^{2}e^{-(c_{1}-u)(t-\frac{ub}{c_{1}-u})}dt\] \[=\frac{c_{0}e^{-ub}}{M_{Q}(u)}\left(\frac{1}{c_{1}-u}\right)\int_ {B-\frac{ub}{c_{1}-u}}^{\infty}3\left(t+\frac{ub}{c_{1}-u}\right)^{2}e^{-(c_{ 1}-u)t}dt\] \[\leq\frac{6c_{0}e^{-ub}}{M_{Q}(u)}\left(\frac{1}{c_{1}-u}\right) \int_{B^{\prime}}^{\infty}\left[t^{2}+\left(\frac{ub}{c_{1}-u}\right)^{2} \right]e^{-(c_{1}-u)t}dt\] \[=\frac{6c_{0}e^{-ub}}{M_{Q}(u)}\left(\underbrace{\left(\frac{1}{c _{1}-u}\right)\int_{B^{\prime}}^{\infty}t^{2}e^{-(c_{1}-u)t}dt}_{(iii,a)}+ \underbrace{\left(\frac{1}{c_{1}-u}\right)\int_{B^{\prime}}^{\infty}\left( \frac{ub}{c_{1}-u}\right)^{2}e^{-(c_{1}-u)t}dt}_{(iii,b)}\right)\]where in the third inequality we used the fact that \((a+b)^{2}\leq 2a^{2}+2b^{2}\) for all \(a,b\in\mathbb{R}\) and that \(B^{\prime}<B-\frac{ub}{c_{1}-u}\). We now bound \((iii,a)\). We have that \(B^{\prime}(c_{1}-u)=2c_{0}\left(\frac{1}{c_{1}-u}\right)\) and

\[(iii,a) =\frac{1}{(c_{1}-u)^{4}}e^{-B^{\prime}(c_{1}-u)}\left(\left[B^{ \prime}(c_{1}-u)+1\right]^{2}+1\right)\leq\frac{2}{(c_{1}-u)^{4}}e^{-B^{\prime }(c_{1}-u)}(B^{\prime}(c_{1}-u)+1)^{2}\] \[\leq\frac{4}{(c_{1}-u)^{4}}e^{-B^{\prime}(c_{1}-u)}\left(\left[B^ {\prime}(c_{1}-u)\right]^{2}+1\right)\] \[=\frac{4}{(c_{1}-u)^{4}}e^{-2c_{0}\left(\frac{1}{c_{1}-u}\right) }\left(4c_{0}^{2}\left(\frac{1}{c_{1}-u}\right)^{2}+1\right)\] \[=\frac{16c_{0}^{2}}{(c_{1}-u)^{6}}e^{-2c_{0}\left(\frac{1}{c_{1}- u}\right)}+\frac{4}{(c_{1}-u)^{4}}e^{-2c_{0}\left(\frac{1}{c_{1}-u}\right)}\] \[\leq 16\cdot c_{0}^{2}\cdot\frac{1}{c_{0}^{6}}\left(c_{0}\left( \frac{1}{c_{1}-u}\right)\right)^{6}\cdot e^{-2c_{0}\left(\frac{1}{c_{1}-u} \right)}+4\cdot\frac{1}{c_{0}^{4}}\left(c_{0}\left(\frac{1}{c_{1}-u}\right) \right)^{4}\cdot e^{-2c_{0}\left(\frac{1}{c_{1}-u}\right)}\] \[\leq\frac{32}{c_{0}^{4}}+\frac{2}{c_{0}^{4}}\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad(x^{6}e^{-2x}\leq 2\text{ and }x^{4}e^{-2x}\leq 0.5\text{ for all }x\geq 0.)\]

Similarly, for \((iii,b)\), we have that

\[(iii,b) \leq\frac{2}{c_{1}-u}\int_{B^{\prime}}^{\infty}\left(\frac{u^{2} b^{2}}{(c_{1}-u)^{2}}\right)e^{-(c_{1}-u)t}dt\] \[=\frac{2u^{2}b^{2}}{(c_{1}-u)^{3}}\frac{e^{-B^{\prime}(c_{1}-u)}} {c_{1}-u}\] \[\leq\frac{2u^{2}b^{2}}{(c_{1}-u)^{4}}e^{-2c_{0}\left(\frac{1}{c_{ 1}-u}\right)}\] \[\leq\frac{2u^{2}b^{2}}{c_{0}^{4}}\left(\frac{c_{0}}{c_{1}-u} \right)^{4}e^{-2c_{0}\left(\frac{1}{c_{1}-u}\right)}\] \[\leq\frac{u^{2}b^{2}}{c_{0}^{4}}\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad(x^{4}e^{-2x}\leq 0.5\text{ for all }x\geq 0.)\]

Putting the result together, \((ii,a)\) can be upper bounded as

\[(ii,a) \leq\frac{6c_{0}e^{-ub}}{M_{Q}(u)}\left(\frac{32}{c_{0}^{4}}+ \frac{2}{c_{0}^{4}}+\frac{u^{2}b^{2}}{c_{0}^{4}}\right)\] \[\leq\frac{e^{-ub}}{M_{Q}(u)}\left(\frac{204+6u^{2}b^{2}}{c_{0}^{ 3}}\right).\]

By Lemma 20, \(B\geq\mu(u)+\frac{ub}{c_{2}+u}\). Hence By Lemma 19 we have:

\[(ii,b)\leq \frac{C_{2}}{M_{Q}(u)}\int_{B}^{+\infty}3t^{2}C_{2}e^{-(u+c_{2})( t-\mu(u))}dt,\] \[\leq \frac{C_{2}e^{-ub}}{M_{Q}(u)}\int_{B}^{+\infty}3t^{2}C_{2}e^{-(u+ c_{2})(t-\mu(u)-\frac{ub}{u+c_{2}})}dt,\] \[\leq \frac{9C_{2}e^{-ub}}{M_{Q}(u)}\int_{B-\mu(u)-\frac{ub}{u+c_{2}}} ^{+\infty}\left(t^{2}+\mu(u)^{2}+\left(\frac{ub}{u+c_{2}}\right)^{2}\right)e^{ -(u+c_{2})t}dt.\]

We now focus on

\[\underbrace{\int_{B-\mu(u)-\frac{ub}{u+c_{2}}}^{+\infty}}_{(iv,a)}t^{2}e^{-(u +c_{2})t}dt+\underbrace{\int_{B-\mu(u)-\frac{ub}{u+c_{2}}}^{+\infty}\mu(u)^{2 }e^{-(u+c_{2})t}dt}_{(iv,b)}+\underbrace{\int_{B-\mu(u)-\frac{ub}{u+c_{2}}}^{ \infty}\left(\frac{ub}{u+c_{2}}\right)^{2}e^{-(u+c_{2})t}dt}_{(iv,c)}.\]By definition of \(B\), we have that \(B-\mu(u)-\frac{ub}{u+c_{2}}\geq c_{0}\left(\frac{1}{c_{1}-u}\right)^{2}=:B^{\prime \prime}\). We then have that

\[(iv,a) \leq\int_{B^{\prime\prime}}^{+\infty}t^{2}e^{-(u+c_{2})t}dt\] \[\leq\frac{e^{-B^{\prime\prime}(u+c_{2})}}{(u+c_{2})^{3}}((B^{ \prime\prime}(u+c_{2})+1)^{2}+1)\] \[\leq\frac{2e^{-B^{\prime\prime}(u+c_{2})}}{(u+c_{2})^{3}}(B^{ \prime\prime}(u+c_{2})+1)^{2}\] \[\leq\frac{4e^{-B^{\prime\prime}(u+c_{2})}}{(u+c_{2})^{3}}([B^{ \prime\prime}(u+c_{2})]^{2}+1)\] \[\leq\frac{4}{(u+c_{2})^{3}}\left(e^{-B^{\prime\prime}(u+c_{2})}[ B^{\prime\prime}(u+c_{2})]^{2}+e^{-B^{\prime\prime}(u+c_{2})}\right)\] \[\leq\frac{8}{(u+c_{2})^{3}}\]

For \((iv,b)\), note that \(B^{\prime\prime}\geq\mu(u)\) by Lemma 20 and we have that

\[(iv,b) \leq\mu(u)^{2}\int_{B^{\prime\prime}}^{\infty}e^{-(u+c_{2})t}dt\] \[\leq\frac{{B^{\prime\prime}}^{2}}{(u+c_{2})}e^{-B^{\prime\prime} (u+c_{2})}\] \[\leq\frac{1}{(u+c_{2})^{3}}(B^{\prime\prime}(u+c_{2}))^{2}e^{-B^{ \prime\prime}(u+c_{2})}\] \[\leq\frac{1}{(u+c_{2})^{3}}.\]

For \((iv,c)\),

\[(iv,c) \leq\frac{u^{2}b^{2}}{(u+c_{2})^{2}}\int_{B^{\prime\prime}}^{ \infty}e^{-(u+c_{2})t}dt\] \[\leq\frac{u^{2}b^{2}}{(u+c_{2})^{2}}\frac{1}{u+c_{2}}e^{-B^{ \prime\prime}(u+c_{2})}\] \[\leq\frac{u^{2}b^{2}}{(u+c_{2})^{3}}.\]

Putting bounds on \((iv,a)\), \((iv,b)\) and \((iv,c)\) together, we have that

\[(ii,b)\leq\frac{9C_{2}e^{-ub}}{M_{Q}(u)}\frac{9+u^{2}b^{2}}{(u+c_{2})^{3}}\leq \frac{e^{-ub}}{M_{Q}(u)}\frac{81C_{2}+9C_{2}u^{2}b^{2}}{(u+c_{2})^{3}}\]

Combining the bounds on \((ii,a)\) and \((ii,b)\) with Lemma 17 we get:

\[\frac{(ii)}{\dot{\mu}(u)}\leq\frac{1}{a^{2}\eta}\left(\frac{204+6u^{2}b^{2}}{c _{0}^{3}}+\frac{81C_{2}+9C_{2}u^{2}b^{2}}{(u+c_{2})^{3}}\right).\]

Chaining the result with the bound on \((i)\) together as well as the choice that \(B=2c_{0}\left(\frac{1}{c_{1}-u}\right)^{2}+\frac{ub}{c_{1}-u}+\frac{ub}{u+c_{2}}\), we obtain

\[\frac{\ddot{\mu}(u)}{\dot{\mu}(u)} \leq\frac{3}{2}\left[2c_{0}\left(\frac{1}{c_{1}-u}\right)^{2}+ \frac{ub}{c_{1}-u}\right]+\frac{3}{2}b+\frac{1}{a^{2}\eta}\left(\frac{204+6u^ {2}b^{2}}{c_{0}^{3}}+\frac{81C_{2}+9C_{2}u^{2}b^{2}}{(u+c_{2})^{3}}\right)\] \[\leq\frac{3}{2}\left[2c_{0}\left(\frac{1}{c_{1}-u}\right)^{2}+ \frac{ub}{c_{1}-u}+\frac{ub}{u+c_{2}}\right]+\underbrace{\frac{1}{a^{2}\eta} \left(\frac{204+6c_{1}^{2}b^{2}}{(c_{1}\cdot C_{1}\cdot e)^{3}}+\frac{81C_{2}+9 C_{2}c_{1}^{2}b^{2}}{c_{2}^{3}}\right)}_{=G_{Q}(C_{1},C_{2},c_{1},c_{2})}.\]Let us now study the case \(\mu_{Q}(0)\neq 0\). Let \(a,b,\eta>0\) be such that \(Q([-b+\mu(0),-a+\mu(0)])>\eta\) and \(-a<0\). With \(Q^{-\mu(0)}\) the centered version of \(Q\), this gives \(Q^{-\mu(0)}([-b,-a])>\eta\). We have just shown that for all \(u\in[0;c_{1})\),:

\[\Gamma_{Q^{-\mu(0)}}(u)\leq\frac{3}{2}\left[2c_{0}\left(\frac{1}{c_{1}-u} \right)^{2}+\frac{ub}{c_{1}-u}+\frac{ub}{u+c_{2}}\right]+G_{Q}(C_{1},C_{2},c_{ 1},c_{2}).\]

By Lemma 15, we have \(\Gamma_{Q^{-\mu(0)}}=\Gamma_{Q}\), hence:

\[\Gamma_{Q}(u)\leq\frac{3}{2}\left[2c_{0}\left(\frac{1}{c_{1}-u}\right)^{2}+ \frac{ub}{c_{1}-u}+\frac{ub}{u+c_{2}}\right]+G_{Q}(C_{1},C_{2},c_{1},c_{2}).\]

### Proof of Corollary 14

**Lemma 21**.: _Let \(Y\sim Q\), and \(Q^{-}\) let be the distribution of \(-Y\). Then \(\mathcal{U}_{Q}=-\mathcal{U}_{Q^{-}}\) and for any \(u\in\mathcal{U}_{Q}^{\circ}\), we have_

\[\Gamma_{Q}(u)=\Gamma_{Q^{-}}(-u).\]

Proof.: Recall that if \(Q\) has zero variance, \(\Gamma_{Q}\equiv 0\) and hence the statement is trivial. Otherwise, for \(u\in\mathcal{U}_{Q}^{\circ}\), \(\Gamma_{Q}(u)=|\overset{\cdot}{\psi}_{Q}(u)|/\overset{\cdot}{\psi}_{Q}(u)\). Now, for \(v\in\mathbb{R}\),

\[M_{Q^{-}}(v)=\mathbb{E}[e^{(-Y)v}]=\mathbb{E}[e^{(-v)Y}]=M_{Q}(-v)\,.\]

Hence, \(\mathcal{U}_{Q}=-\mathcal{U}_{Q^{-}}\) and for any \(v\in\mathcal{U}_{Q}\), \(\psi_{Q}(v)=\psi_{Q^{-}}(-v)\). Taking derivatives of both sides,

\[\dot{\psi}_{Q}(v) =-\dot{\psi}_{Q^{-}}(-v)\,,\] \[\ddot{\psi}_{Q}(v) =\ddot{\psi}_{Q^{-}}(-v)\,,\] \[\dddot{\psi}_{Q}(v) =-\dddot{\psi}_{Q^{-}}(-v)\,,\]

which immediately implies the statement, noting that the variance of \(Q\) is positive if and only if the variance of \(Q^{-}\) is positive. 

Proof of Corollary 14.: Assume that \(Q\in\mathcal{E}_{\text{right}}(c_{1},C_{1})\cap\mathcal{E}_{\text{left}}(c_{ 2},C_{2})\). Since the statement holds trivially when \(\operatorname{Var}(Q)=0\), assume \(\operatorname{Var}(Q)>0\). Then, \(Q^{-}\in\mathcal{E}_{\text{right}}(c_{2},C_{2})\cap\mathcal{E}_{\text{right}}( c_{1},C_{1})\). We then get the stated result by applying Proposition 13, combined with Lemma 21. To be more specific, for all \(u\in(-c_{2},0]\), from these two results it follows that

\[\Gamma_{Q}(u) =\Gamma_{Q^{-}}(-u)\] \[\leq\frac{3}{2}\left[2c_{0}\left(\frac{1}{c_{2}-|u|}\right)^{2}+ \frac{|u|b}{c_{2}-|u|}+b\right]+\frac{1}{a^{2}\eta}\left(\frac{204+6e^{2}b^{2 }}{c_{0}^{3}}+\frac{81C_{1}+9C_{1}u^{2}b^{2}}{(u+c_{1})^{3}}\right)\] \[\leq\frac{3}{2}\left[2c_{0}\left(\frac{1}{c_{2}-|u|}\right)^{2}+ \frac{|u|b}{c_{2}-|u|}+b\right]+\frac{1}{a^{2}\eta}\left(\frac{204+6c_{2}^{2}b ^{2}}{c_{0}^{3}}+\frac{81C_{1}+9C_{1}c_{2}^{2}b^{2}}{c_{1}^{3}}\right)\,,\]

where \(c_{0}=C_{2}\cdot c_{2}\cdot e\).

### Proof of Corollary 8

Let \(\mathcal{Q}\) be a regular NEF with base distribution \(Q\). By definition, this means that \(\mathcal{U}_{Q}\) is an open interval. Take any \(u\in\mathcal{U}_{Q}^{\circ}\). There exist some \(\epsilon>0\) s.t. \(M_{Q}(u-\epsilon),M_{Q}(u+\epsilon)<\infty\). We also have:

\[M_{Q}(u+\epsilon) =\int\exp(\epsilon u)\exp(uy)Q(dy)\] \[=\int\exp(\epsilon u)M_{Q}(u)Q_{u}(dy)\] \[=M_{Q}(u)M_{Q_{u}}(\epsilon).\]

[MISSING_PAGE_FAIL:22]

Proof.: We have

\[M_{Q}(u)Q_{u}\left((t,+\infty)\right)= \int_{t}^{+\infty}e^{uy}Q(dy),\] \[= \sum_{k=0}^{+\infty}\int_{t+k}^{t+k+1}e^{uy}Q(dy),\] \[\leq \sum_{k=0}^{+\infty}e^{u(t+k+1)}\int_{t+k}^{t+k+1}Q(dy),\] \[\leq \sum_{k=0}^{+\infty}e^{u(t+k+1)}Q\left([t+k;+\infty)\right),\] \[\leq C\sum_{k=0}^{+\infty}e^{-\left(\frac{c(t+k)^{2}}{2}-u(t+k+1) \right)}.\] (11)

The first inequality holds as \(u\geq 0\), the second by subgaussianity assumption. As \(t\geq\left(\frac{4}{c}+1\right)u+\frac{4}{c}\), we have \(u\leq\frac{c}{4}t\) and:

\[t^{2}\geq\left(\frac{4}{c}+1\right)ut=\frac{4t}{c}u+\underbrace{t}_{\geq \frac{4}{c}}u\geq\frac{4}{c}(t+1)u.\] (12)

This implies \(u(t+1)\leq\frac{c}{4}t^{2}\) on top of \(u\leq\frac{c}{4}t\). Then

\[u(t+1+k)\leq\frac{c}{4}\left(t^{2}+kt\right)\leq\frac{c}{4}\left(t+k\right)^{2}.\]

Reinjecting in Eq. (11), we get:

\[M_{Q}(u)Q_{u}\left((t,+\infty)\right)\leq C\sum_{k=0}^{+\infty}e^{-\frac{c(t+k)^{2}}{4}},\] \[\leq Ce^{-\frac{c\tau^{2}}{4}}\sum_{k=0}^{+\infty}e^{-\frac{ck^{2}}{4}}.\] \[\leq Ce^{-\frac{c\tau^{2}}{4}}\left(1+\int_{0}^{\infty}e^{-\frac{c \tau^{2}}{4}}dx\right)\] \[\leq C\left(1+\sqrt{\frac{\pi}{c}}\right)e^{-\frac{c\tau^{2}}{4}}.\]

**Lemma 24**.: _Take \(Q\in\mathcal{G}(c,C)\) a centered distribution. For all \(u,t\geq 0\), we have:_

\[Q_{u}\left((-\infty,-t)\right)\leq\frac{C}{M_{Q}(u)}e^{-ut-\frac{c\tau^{2}}{2 }}.\]

Proof.: We have that

\[M_{Q}(u)Q_{u}\left((-\infty,-t)\right)= \int_{-\infty}^{-t}e^{uy}Q(dy),\] \[\leq e^{-ut}\int_{-\infty}^{-t}Q(dy),\] \[\leq Ce^{-ut-\frac{c\tau^{2}}{2}}\,,\] (13)

where the last inequality holds by subgaussianity of \(Q\)

**Lemma 25**.: _Take \(Q\in\mathcal{G}(c,C)\) a centered distribution. For all \(u\geq 0\) the following holds:_

\[0=\mu(0)\leq\mu_{Q}(u)\leq\left(\frac{4}{c}+1\right)u+\frac{4}{c}+\mathfrak{C}_{ 3}e^{-\frac{4u^{2}}{c}},\]

_where \(\mathfrak{C}_{3}=\frac{\sqrt{\pi}}{\sqrt{c}}\mathfrak{C}_{1}\)._

Proof.: We have:

\[\mu(u)= \int_{0}^{+\infty}Q_{u}\left(\left(y,+\infty\right)\right)dy-\int_ {-\infty}^{0}Q_{u}\left(\left(-\infty,-y\right)\right)dy\] \[\leq \int_{0}^{+\infty}Q_{u}\left(\left(y,+\infty\right)\right)dy\] \[\leq \left(\frac{4}{c}+1\right)u+\frac{4}{c}+\int_{\left(\frac{4}{c} +1\right)u+\frac{4}{c}}^{+\infty}Q_{u}\left(\left(y,+\infty\right)\right)dy.\]

By Lemma 23, this implies:

\[\mu(u)\leq \left(\frac{4}{c}+1\right)u+\frac{4}{c}+\frac{\mathfrak{C}_{1}}{ M_{Q}(u)}\int_{\left(\frac{4}{c}+1\right)u+\frac{4}{c}}^{+\infty}e^{-\frac{cy^{2} }{4}}dy\] \[\leq \left(\frac{4}{c}+1\right)u+\frac{4}{c}+\frac{\mathfrak{C}_{1}e ^{-c\left(\left(\frac{4}{c}+1\right)u+\frac{4}{c}\right)^{2}}}{M_{Q}(u)}\int_ {0}^{+\infty}e^{-\frac{cy^{2}}{4}}dy\] \[\leq \left(\frac{4}{c}+1\right)u+\frac{4}{c}+\frac{\mathfrak{C}_{1}}{ M_{Q}(u)}e^{-\frac{4u^{2}}{c}}\int_{0}^{+\infty}e^{-\frac{cy^{2}}{4}}dy.\]

By Jensen's inequality, \(M_{Q}(u)=\mathbb{E}_{Q}[e^{uY}]\geq e^{u\mathbb{E}_{Q}[Y]}=1\). Noting that \(\int_{0}^{+\infty}e^{-\frac{cy^{2}}{4}}dy\leq\frac{\sqrt{\pi}}{\sqrt{c}}\) terminates the proof. 

Proof of Theorem 9.: As previously noted, we first show the result when \(\mu(0)=0\) and \(\dot{\mu}(0)>0\). We start with \(u>0\), then by Lemma 21 will extend the bound to \(u<0\).

Take \(u>0\) and \(B>0\) a constant to be optimized later. We have:

\[\int|Y-\mu_{Q}(u)|^{3}\;Q_{u}(dy)= \int_{0}^{+\infty}3t^{2}\,\mathbb{P}\left(|Y-\mu_{Q}(u)|\geq t \right)dt,\] \[\leq \underbrace{\int_{0}^{B}3t^{2}\,\mathbb{P}\left(|Y-\mu_{Q}(u)| \geq t\right)dt}_{(i)}+\underbrace{\int_{B}^{+\infty}3t^{2}\,\mathbb{P}\left( |Y-\mu_{Q}(u)|\geq t\right)dt}_{(ii)}.\]

The first equality is a classical result on the relationship between moments and tails of r.v., see for instance Exercise 1.2.3 of [21]. The following bound holds:

\[(i)\leq 3B\int_{0}^{B}t\,\mathbb{P}\left(|Y-\mu_{Q}(u)|\geq t\right)dt,\] \[\leq \frac{3B}{2}\dot{\mu}_{Q}(u).\] (14)

We also have:

\[(ii)\leq \underbrace{\int_{B}^{+\infty}3t^{2}\,\mathbb{P}\left(Y-\mu_{Q}(u) \geq t\right)dt}_{(ii,a)}+\underbrace{\int_{B}^{+\infty}3t^{2}\,\mathbb{P} \left(Y\leq-\left(t-\mu_{Q}(u)\right)\right)dt}_{(ii,b)}.\]

Set \(B=\mu_{Q}(u)+\left(\frac{4}{c}+1\right)u+\frac{4}{c}+b\), where \(b\) was defined in Lemma 17. As \(B\geq\left(\frac{4}{c}+1\right)u+\frac{4}{c}\), by Lemma 23, we have:\[(ii,a)\leq \int_{B}^{+\infty}3t^{2}\,Q_{u}\left((t,+\infty)\right)dt,\] \[\leq \frac{\mathfrak{C}_{1}}{M_{Q}(u)}\int_{B}^{+\infty}3t^{2}e^{-\frac {c(B+t)^{2}}{4}}dt,\] \[\leq \frac{3\mathfrak{C}_{1}}{M_{Q}(u)}\int_{0}^{+\infty}(B+t)^{2}e^{ -\frac{c(B+t)^{2}}{4}}dt,\] \[\leq \frac{6\mathfrak{C}_{1}e^{-\frac{cB^{2}}{4}}}{M_{Q}(u)}\int_{0}^{ +\infty}(B^{2}+t^{2})e^{-\frac{cB^{2}}{4}}dt,\] \[= \frac{6\mathfrak{C}_{1}e^{-\frac{cB^{2}}{4}}}{M_{Q}(u)}(B^{2}+ \frac{2}{c})\int_{0}^{+\infty}e^{-\frac{cB^{2}}{4}}dt,\] \[= \frac{6\mathfrak{C}_{1}}{M_{Q}(u)}\sqrt{\frac{\pi}{c}}(B^{2}+ \frac{2}{c})e^{-\frac{cB^{2}}{4}}.\]

where the first equality holds as for any \(a>0\), using integration by part we have that

\[\int_{0}^{+\infty}e^{-at^{2}}dt=2a\int_{0}^{+\infty}t^{2}e^{-at^{2}dt}.\] (15)

Denote \(B^{\prime}=B-\mu_{Q}(u)=\left(\frac{4}{c}+1\right)u+\frac{4}{c}+b\). By Lemma 24 we have:

\[(ii,b)\leq \int_{B}^{+\infty}3t^{2}\frac{\mathfrak{C}_{2}}{M_{Q}(u)}e^{-u(t -\mu_{Q}(u))-\frac{c(t-\mu_{Q}(u))^{2}}{2}}dt,\] \[= \frac{3\mathfrak{C}_{2}}{M_{Q}(u)}\int_{B^{\prime}}^{+\infty} \left(t+\mu_{Q}(u)\right)^{2}e^{-ut-\frac{cB^{2}}{2}}dt,\] \[\leq \frac{3\mathfrak{C}_{2}e^{-\frac{4}{c}u^{2}-bu}}{M_{Q}(u)}\int_{B ^{\prime}}^{+\infty}\left(t+\mu_{Q}(u)\right)^{2}e^{-\frac{cB^{2}}{2}}dt,\] \[\leq \frac{3\mathfrak{C}_{2}e^{-\frac{4}{c}u^{2}-bu}}{M_{Q}(u)}e^{- \frac{c(B^{\prime})^{2}}{2}}\int_{0}^{+\infty}\left(t+B^{\prime}+\mu_{Q}(u) \right)^{2}e^{-\frac{cB^{2}}{2}}dt,\] \[\leq \frac{6\mathfrak{C}_{2}e^{-\frac{4}{c}u^{2}-bu}}{M_{Q}(u)}e^{- \frac{c(B^{\prime})^{2}}{2}}\int_{0}^{+\infty}\left(t^{2}+B^{2}\right)e^{- \frac{cB^{2}}{2}}dt,\] \[\leq \frac{6\mathfrak{C}_{2}e^{-\frac{4}{c}u^{2}-bu}}{M_{Q}(u)}\left( \frac{1}{c}+B^{2}\right)\underbrace{\int_{0}^{+\infty}\mathfrak{C}_{2}e^{- \frac{cB^{2}}{2}}dt}_{\mathfrak{C}_{4}}.\]

where in the second line we use change of variable; third line we use the fact that \(B^{\prime}>\frac{4}{c}u+b\), hence \(e^{-ut}\leq e^{-B^{\prime}u}\leq e^{-\frac{4}{c}u^{2}-ub}\) for all \(t\geq B^{\prime}\); fourth line change of variable and the fact that \(-(a+b)^{2}\leq-a^{2}-b^{2}\) for all \(a,b\geq 0\); fifth line the fact that \((a+b)^{2}\leq 2a^{2}+2b^{2}\) for all \(a,b\in\mathbb{R}\); sixth line Eq. (15); and seventh line \(B^{\prime}>\frac{4}{c}u\).

Combining the bounds on \((ii,a)\) and \((ii,b)\) with Lemma 17 we get:

\[\frac{(ii)}{\dot{\mu}_{Q}(u)}\leq\frac{6}{a^{2}\eta}\left(\mathfrak{C}_{4} \left(\frac{1}{c}+B^{2}\right)e^{-\frac{12}{c}u^{2}}+\mathfrak{C}_{3}\left( \frac{2}{c}+B^{2}\right)e^{-\frac{cB^{2}}{4}+ub}\right).\]

As \(B\geq\frac{4}{c}u+b\), we have \(\frac{cB^{2}}{4}+ub\geq\frac{4}{c}u^{2}\). This implies

\[\frac{(ii)}{\dot{\mu}_{Q}(u)}\leq\frac{6(\mathfrak{C}_{3}+\mathfrak{C}_{4})}{a ^{2}\eta}\left(\frac{2}{c}+B^{2}\right)e^{-\frac{4}{c}u^{2}}.\]With Equation 14, we get

\[\Gamma_{Q}(u)\leq\frac{6(\mathfrak{C}_{3}+\mathfrak{C}_{4})}{a^{2}\eta}\left( \frac{2}{c}+B^{2}\right)e^{-\frac{4}{c}u^{2}}+\frac{3B}{2}.\]

By Lemma 25, \(\mu_{Q}(u)\leq\left(\frac{4}{c}+1\right)u+\frac{4}{c}+\mathfrak{C}_{3}\), which implies :

\[B\leq 2\left(\frac{4}{c}+1\right)u+\frac{8}{c}+b+\mathfrak{C}_{3}.\]

For any constants \(w_{1},w_{2}>0\), we have \(w_{1}u^{2}e^{-w_{2}u^{2}}\leq\frac{w_{1}}{\sqrt{w_{2}}}u\). This implies that for \(u>0\):

\[\Gamma_{Q}(u)=O(u).\]

Note that if \(Q\in\mathcal{G}(c,C)\), with \(Q^{-}\) the distribution of \(-Y\), \(Y\sim Q\), we have \(Q^{-}\in\mathcal{G}(c,C)\). By Lemma 21, that implies:

\[\Gamma_{Q}(-u)=\Gamma_{Q^{-}}(u)=O(u).\]

Therefore, for \(u\in\mathbb{R}\):

\[\Gamma_{Q}(u)=O(|u|).\]

As, by Lemma 12, \(\Gamma_{Q}\) is constant for \(\dot{\mu}(0)=0\), it remains now only to show the theorem if \(\mu(0)\neq 0\) and \(\dot{\mu}(0)>0\). We have just shown

\[\Gamma_{Q^{-\mu(0)}}(u)=O(|u|),\]

with \(Q^{-\mu(0)}\) the centered version of \(Q\). by Lemma 15, we have \(\Gamma_{Q}=\Gamma_{Q^{-\mu(0)}}\). Hence, for any \(Q\in\mathcal{G}(c,C)\):

\[\Gamma_{Q}(u)=O(|u|).\]

### Proof for Theorem 10

We construct a distribution \(Q\) s.t. for any \(s>0\), we can find some \(u>s\) with :

\[\frac{\int(y-\mu(u))^{3}Q_{u}(dy)}{\int(y-\mu(u))^{2}Q_{u}(dy)}\geq 0.038u.\]

Let \(Q\) the base measure be

\[Q(dy)=\sum_{i\geq 1}p_{i}\delta_{2^{i}}(dy),\] (16)

with

\[p_{i}=\begin{cases}C_{1}\exp(-4^{i})&\text{if $i$ is even},\\ \frac{C_{1}}{4}\exp(-3\times 4^{i-1})&\text{if $i$ is odd},\end{cases}\] (17)

where \(C_{1}\) is a normalizing constant. By definition, we have that \(Q_{u}(dy)=\sum_{i\geq 1}q_{i}\delta_{2^{i}}(dy)\) where

\[q_{k}=\begin{cases}\frac{C_{1}}{M_{Q}(u)}\exp(u2^{k})\exp(-4^{k})&\text{if $k$ is even}\\ \frac{C_{1}}{4M_{Q}(u)}\exp(u2^{k})\exp(-3\times 4^{k-1})&\text{if $k$ is odd}.\end{cases}\] (18)

We are going to inspect \(\frac{|\mathbb{E}([Y-\mu(u))^{3}]|}{\mathbb{E}([Y-\mu(u))^{2}]}\) for \(i\) a positive even number large enough and \(u=2^{i+1}\). By definition, we have that \(Q_{u}(dy)=\sum_{i\geq 1}q_{i}\delta_{2^{i}}(dy)\) where

\[q_{k}=\begin{cases}\frac{C_{1}}{M_{Q}(u)}\exp(2^{i+k+1})\exp(-4^{k})&\text{if $k$ is even},\\ \frac{C_{1}}{4M_{Q}(u)}\exp(2^{i+k+1})\exp(-3\times 4^{k-1})&\text{if $k$ is odd}.\end{cases}\] (19)First, we remark a few simple equalities and inequalities that will prove useful in the subsequent computations.

Not that \(\mathbb{P}\left(U=2^{i}\right)=\frac{C_{1}}{M_{Q}(u)}e^{4^{i}}\), hence:

\[\frac{\mathbb{P}\left(U=2^{i+1}\right)}{\mathbb{P}\left(U=2^{i}\right)}=\frac{1 }{4}e^{2^{2(i+1)}-3\times 4^{i}-4^{i}}=\frac{1}{4}.\] (20)

This implies:

\[\mathbb{P}\left(U=2^{i}\right)\leq\frac{4}{5}\,,\] (21)

and, combining with Equation Eq. (19),

\[\frac{\mathbb{P}\left(U=2^{j}\right)}{\mathbb{P}\left(U=2^{i}\right)}=\begin{cases} e^{2^{i+j+1}-4^{j}-4^{i}}&\text{if $j$ is even},\\ \frac{1}{4}e^{2^{i+j+1}-3\times 4^{j-1}-4^{i}}&\text{if $j$ is odd}.\end{cases}\] (22)

From this last equation we get upper bound:

\[\frac{\mathbb{P}\left(U=2^{j}\right)}{\mathbb{P}\left(U=2^{i}\right)}\leq e^{ 2^{i+j+1}-3\times 4^{j-1}-4^{i}}.\] (23)

The two next Lemma combined show that for any even \(i\geq 4\), \(1.24\times 2^{i}\leq\mu(u)\leq 1.26\times 2^{i}\).

**Lemma 26**.: _Let \(i\geq 4\) be an even number and \(u=2^{i+1}\). For random variable \(U\sim Q_{u}(dy)\), it holds that:_

\[\mu(u)\geq 1.24\times 2^{i}.\] (24)

Proof.: Consider \(j<i\), \(j\) even. By Eq. (23),

\[\frac{\mathbb{P}\left(U=2^{j}\right)}{\mathbb{P}\left(U=2^{i}\right)}\leq e^{ 2^{i+j+1}-3\times 4^{j-1}-4^{i}}.\]

Denote \(f_{1}(j)=2^{i+j+1}-3\times 4^{j-1}-4^{i}\). The monotonicity of the right hand side, \(e^{f_{1}(j)}\), is the same as \(f_{1}(j)\). Take derivative of \(f_{1}(j)\), we have that

\[f_{1}^{\prime}(j)=2^{j}\cdot 2^{i+1}\ln 2-3\times 4^{j-1}\ln 4=2^{j}(2^{i+1}\ln 2 -3\times 2^{j-2}\ln 4)=2^{j}\ln 4(2^{i}-3\times 2^{j-2}).\]

Since \(i>j\) we have that \(f_{1}^{\prime}(j)\geq 0\) for \(j\in[0,i]\). Hence the right hand side increases with \(j\), as \(j\leq i-1\), we have that

\[\frac{\mathbb{P}\left(U=2^{j}\right)}{\mathbb{P}\left(U=2^{i}\right)}\leq e^{ -3\times 4^{i-2}}.\] (25)

This implies:

\[\frac{\mathbb{P}\left(U<2^{i}\right)}{\mathbb{P}\left(U=2^{i}\right)}=\frac{ \sum_{j<i}\mathbb{P}\left(U=2^{j}\right)}{\mathbb{P}\left(U=2^{i}\right)}\leq (i-1)e^{-3\times 4^{i-2}}\leq 0.001\,.\] (26)

We now prove the lower bound on the mean.

\[\mu(u)\geq 2^{i+1}\mathbb{P}(U=2^{i+1})+2^{i}\mathbb{P}(U=2^{i})+2^{i} \mathbb{P}(U>2^{i+1})\] \[= 2^{i+1}\mathbb{P}(U=2^{i+1})+2^{i}\mathbb{P}(U=2^{i})+2^{i}(1- \mathbb{P}(U<2^{i})-\mathbb{P}(U=2^{i})-\mathbb{P}(U=2^{i+1}))\] \[\geq 2^{i}+\left[2^{i+1}-2^{i}\right]\mathbb{P}(U=2^{i+1})-2^{i} \mathbb{P}(U<2^{i})\] \[= 2^{i}+2^{i}\mathbb{P}(U=2^{i})\left(\frac{\mathbb{P}(U=2^{i+1})} {\mathbb{P}(U=2^{i})}-\frac{\mathbb{P}(U<2^{i})}{\mathbb{P}(U=2^{i})}\right)\] \[\geq 2^{i}+2^{i}\left(\frac{1}{4}-0.001\right)\mathbb{P}(U=2^{i})\] \[\geq 1.24\times 2^{i},\]

where the fourth line holds because of Eqs. (20) and (26).

**Lemma 27**.: _Let \(i\) be a positive even number and \(u=2^{i+1}\). For random variable \(U\sim Q_{u}(dy)\), it holds that:_

\[\mu(u)\leq 1.26\cdot 2^{i}\,.\] (27)

Proof.: By Eq. (23), for any \(j\geq i+2\), we have:

\[\frac{\mathbb{P}\left(U=2^{j}\right)}{\mathbb{P}\left(U=2^{i} \right)} \leq e^{2^{i+j+1}-3\times 4^{j-1}-4^{i}}=e^{-2^{j-1}\left(3\times 2^{j-1} -2\times 2^{i+1}\right)-4^{i}},\] \[\leq e^{-4^{i}-2^{j-1}}.\] (28)

This implies:

\[\sum_{j>i+1}2^{j}\frac{\mathbb{P}_{u}\left(U=2^{j}\right)}{\mathbb{P}\left(U= 2^{i}\right)}\leq e^{-4^{i}}\sum_{j>i+1}2^{j}e^{-2^{j-1}}\leq\left(\sum_{j=4}^ {\infty}2^{j}e^{-2^{j-1}}\right)e^{-4^{i}}.\]

We turn to bounding \(\sum_{j=4}^{\infty}2^{j}e^{-2^{j-1}}\):

\[\sum_{j=4}^{\infty}2^{j}e^{-2^{j-1}} =2\sum_{j=4}^{\infty}2^{j-1}e^{-2^{j-1}}=2\sum_{j=3}^{\infty}2^{j} e^{-2^{j}},\] \[\leq 2\int_{2}^{\infty}2^{x}e^{-2^{x}}dx,\] \[=2\int_{2}^{\infty}\frac{1}{y\ln 2}ye^{-y}dy,\] \[=\frac{2}{e^{4}\ln 2},\]

where the second inequality holds as \(2^{x}e^{-2^{x}}\) is decreasing for \(x>0\). Hence, for any \(i\geq 2\):

\[\sum_{j>i+1}2^{j}\frac{\mathbb{P}_{u}\left(U=2^{j}\right)}{\mathbb{P}\left(U= 2^{i}\right)}\leq\frac{2}{e^{4}\ln 2}e^{-4^{i}}\leq\frac{1}{100}2^{i}.\] (29)

We are now ready to upper bound \(\mu_{u}\):

\[\mu_{u} \leq 2^{i-1}\mathbb{P}(U<2^{i})+2^{i}\mathbb{P}(U=2^{i})+2^{i+1} \mathbb{P}(U=2^{i+1})+\sum_{j>i+1}2^{j}\mathbb{P}(U=2^{j})\] \[\leq 2^{i}P(U=2^{i})\left(\frac{1}{2}\frac{P(U<2^{i})}{P(U=2^{i}) }+1+2\frac{P(U=2^{i+1})}{P(U=2^{i})}+\frac{1}{2^{i}}\sum_{j>i+1}2^{j}\frac{ \mathbb{P}_{u}\left(U=2^{j}\right)}{\mathbb{P}\left(U=2^{i}\right)}\right)\] \[\leq 2^{i}\frac{4}{5}\left(\frac{1}{2}\times 0.001+1+\frac{1}{2}+ \frac{1}{100}\right)\] \[<1.26\cdot 2^{i}\,,\]

where we get the third inequality from Eqs. (20), (21), (26) and (29).

Proof for Theorem 10.: Let \(U\sim Q_{u}\) where \(u=2^{i+1}\) for \(i\geq 4\) an even number. We first derive an upper bound on the variance \(\mathbb{E}[(U-\mu(u))^{2}]\leq\mathbb{E}[U^{2}]\).

\[\mathbb{E}[U^{2}] \leq 2^{2i}\cdot\mathbb{P}(U<2^{i})+2^{2i}\cdot\mathbb{P}(U=2^{i})+ 2^{2i+2}\cdot\mathbb{P}(U=2^{i+1})+\sum_{j>i+1}2^{2j}\mathbb{P}(U=2^{j})\] \[\leq 2^{2i}P(U=2^{i})\left(\frac{\mathbb{P}(U<2^{i})}{\mathbb{P}( U=2^{i})}+1+4\frac{\mathbb{P}(U=2^{i+1})}{\mathbb{P}(U=2^{i})}+\frac{1}{2^{2i}} \sum_{j>i+1}2^{2j}\frac{\mathbb{P}(U=2^{j})}{\mathbb{P}(U=2^{i})}\right)\] \[\leq 2^{2i}\frac{4}{5}\left(0.112+1+1+\frac{e^{-4^{i}}}{2^{2i}} \sum_{j>i+1}2^{2j}e^{-2^{j-1}}\right)\,,\] (30)

where the last inequality is obtained from Eqs. (20), (21), (26) and (28).

We inspect the infinite series in the above inequality. Note that \(2^{2j}e^{-2^{j-1}}\) is decreasing for \(j\geq 1\).

\[\sum_{j>i+1}2^{2j}e^{-2^{j-1}} \leq 4\sum_{j\geq 2}4^{j}e^{-2^{j}}\] \[\leq 4\int_{1}^{\infty}4^{x}e^{-2^{x}}dx\] \[=4\int_{2}^{\infty}\frac{1}{y\ln 2}y^{2}e^{-y}dy\] \[=\frac{12}{\ln 2\cdot e^{2}}.\]

For all \(i\geq 1\), it follows that

\[\frac{e^{-4^{i}}}{2^{2i}}\frac{12}{\ln 2\cdot e^{2}}\leq 0.02.\]

Plugging into Equation 30, we have:

\[\mathbb{E}[(U-\mu(u))^{2}]\leq 1.7\times 2^{2i},\] (31)

On the other hand, by Lemmas 26 and 27, we have that

\[U-\mu(u)\geq\begin{cases}\underline{0.74\cdot 2^{i}}&\text{if }U\geq 2^{i+1}\\ \underline{-0.26\cdot 2^{i}}&\text{if }U=2^{i}\\ \underline{-1.26\cdot 2^{i}}&\text{if }U<2^{i}.\end{cases}\]

Then we can lower bound the third central moment \(\mathbb{E}[|U-\mu(u)|^{3}]\).

\[\mathbb{E}[(U-\mu(u))^{3}] \geq\mathbb{P}(U\geq 2^{i+1})A_{\text{large}}^{3}+\mathbb{P}(U=2^ {i})A_{\text{medium}}^{3}+\mathbb{P}(U<2^{i})A_{\text{small}}^{3}\] \[=[1-\mathbb{P}(U=2^{i})-\mathbb{P}(U<2^{i})]\cdot A_{\text{large} }^{3}+\mathbb{P}(U=2^{i})A_{\text{medium}}^{3}+\mathbb{P}(U<2^{i})A_{\text{ small}}^{3}\] \[=(0.74\cdot 2^{i})^{3}-\mathbb{P}(U=2^{i})[(0.74\cdot 2^{i})^{3}+(0. 26\cdot 2^{i})^{3}]-\mathbb{P}(U<2^{i})((0.74\cdot 2^{i})^{3}+(1.26\cdot 2^{i})^{3})\] \[\geq(2^{i})^{3}\left[0.74^{3}-0.8(0.74^{3}+0.26^{3})-0.8\times 0.001(1.26^{3}+0.74^{3})\right]\] \[\geq 0.065(2^{i})^{3}.\]

where the third inequality holds by Eqs. (21) and (26). Combining this last bound with Eq. (31), we obtain:

\[\frac{\mathbb{E}[(U-\mu(u))^{3}]}{\mathbb{E}[(U-\mu(u))^{2}]}\geq\frac{0.065}{ 1.7}2^{i}=0.038u.\]Bandit algorithm

In this section, we present our analysis of Algorithm 1. The section is organized as follows. In Appendix E.1, we detail how we construct the confidence set. The proof that \(\theta_{*}\) lies in the confidence set with high probability will be presented only in Appendix E.4 so that we can continue in Appendix E.2 with the proof of the main result, Theorem 29, bounding the regret. This result differs from Theorem 11 by providing additional detail about the choice of the parameters in the algorithm. The proof presented in Appendix E.2 requires a number of technical lemmas that are presented as the proof develops. The proofs of these are postponed to subsequent sections. Before the proof of these, we devote the next section (Appendix E.3) to technical results on consequences of self-concordance which will be useful for the rest of the proofs. This is followed in Appendix E.4 by the proof that the confidence set constructed indeed has the required coverage. The next section (Appendix E.5) is devoted to proving Lemma 30 ("ellipsoidal diameter bound on the confidence set"), which is one of the two key results required for the proof of the main regret bound (besides the result on the coverage of the confidence set). A self-bounding property of self-concordance functions (Lemma 31), which is the second main ingredient of the regret bound proof is shown in Appendix E.6. Finally, for completeness, we present the (well known) elliptical potential lemma (stated here as Lemma 38) in Appendix E.7.

### Constructing the confidence set

The confidence set construction is based on first obtaining the parameter vector \(\hat{\theta}_{t}\). This parameter vector is chosen to be the minimizer of the regularized negative log-likelihood function: \(\hat{\theta}_{t}=\arg\min_{\theta\in\mathbb{R}^{d}}\mathcal{L}(\theta; \mathcal{D}_{t})\) where \(\mathcal{D}_{t}=((X_{i},Y_{i}))_{i=1}^{t-1}\) is the data available in step \(t\) and

\[\mathcal{L}(\theta;\mathcal{D}_{t},\lambda)=\frac{\lambda}{2}\|\theta\|^{2}- \sum_{i=1}^{t-1}\log q(Y_{i};X_{i}^{\top}\theta)\] (32)

where \(\lambda>0\) is a tuning parameter (to be chosen later) and \(q(y;u)=\frac{dQ_{u}}{dQ}(y)\) is the density of \(Q_{u}\) with respect to \(Q\) at \(y\in\mathbb{R}\). It should be clear from the definitions that \(q\) is well-defined. The purpose of regularization is to ensure that the loss function has a unique optimizer even in the data poor regime.

For the construction of the confidence set it will be useful to derive an equivalent expression for the loss \(\mathcal{L}\). For this, first note that the density \(q\) satisfies \(q(y;u)=\exp(yu-\psi_{Q}(u))\). Plugging this into the definition of \(\mathcal{L}\), we get

\[\mathcal{L}(\theta;\mathcal{D}_{t},\lambda)=\frac{\lambda}{2}\|\theta\|^{2}+ \sum_{i=1}^{t-1}(\psi(X_{i}^{\top}\theta)-Y_{i}X_{i}^{\top}\theta)\,.\]

As it is well known, \(\psi\) is a convex function of its argument (Theorem 1.13 of [1]) and hence \(\theta\mapsto\mathcal{L}(\theta;\mathcal{D})\) is strictly convex provided that \(\lambda>0\).

For the confidence set construction we will need the non-constant part of the gradient of \(\mathcal{L}(\cdot;\mathcal{D}_{t})\), which we denote by \(g_{t}\). We will also need the curvature of \(\mathcal{L}(\cdot;\mathcal{D}_{t})\), which we denote by \(H_{t}\). These are

\[g_{t}(\theta)=\sum_{i=1}^{t-1}\mu(X_{i}^{\top}\theta)X_{i}+ \lambda\theta\quad\text{ so that }\quad\nabla_{\theta}\mathcal{L}(\theta;\mathcal{D}_{t})=g_{t}(\theta)- \sum_{i=1}^{t-1}X_{i}Y_{i}\,\] \[\text{ and }\] \[H_{t}(\theta)=\nabla_{\theta}^{2}\mathcal{L}(\theta;\mathcal{D} _{t})=\lambda I+\sum_{i=1}^{t-1}\dot{\mu}(X_{i}^{\top}\theta)X_{i}X_{i}^{\top }\,.\]

The minimizer \(\hat{\theta}_{t}=\arg\max_{\theta\in\mathbb{R}^{d}}\mathcal{L}(\theta; \mathcal{D},\lambda)\) has the property that

\[\frac{\partial\mathcal{L}(\theta;\mathcal{D}_{t},\lambda)}{\partial\theta} \Bigg{|}_{\theta=\hat{\theta}_{t}}=0\,.\]This implies that

\[g_{t}(\hat{\theta}_{t})-\sum_{i=1}^{t}X_{i}Y_{i}=0.\]

With this, we can introduce our confidence set construction, which is based on the work of [10]. For \(\delta\in(0,1]\), we let

\[\mathcal{C}_{t}^{\delta}(\hat{\theta}_{t})=\left\{\theta\in\Theta\::\:\left\|g _{t}(\theta)-g_{t}(\hat{\theta}_{t})\right\|_{H_{t}^{-1}(\theta)}\leq\gamma_{t }(\delta)\right\}\,,\] (33)

where for \(M\) to be chosen later (in Lemma 28),

\[\lambda_{T} =1\vee\frac{2dM}{S_{0}}\log\left(e\sqrt{1+\frac{TL}{d}}\lor 1/ \delta\right),\] (34) \[\gamma_{t}(\delta) =\sqrt{\lambda_{T}}\left(\frac{1}{2M}+S_{0}\right)+\frac{4Md}{ \sqrt{\lambda_{T}}}\log\left(e\sqrt{1+\frac{tL}{d}}\lor 1/\delta\right)\quad\text{ for all}\quad t\in[T]\,.\] (35)

Here, \(S_{0}=\sup\{\|\theta\|\::\:\theta\in\Theta\}\), as defined in the main body of the paper. In the algorithm we then choose \(\mathcal{C}_{t}\) to be \(\mathcal{C}_{t}^{\delta}(\hat{\theta}_{t})\) with a fixed value of \(\delta\in[0,1]\) that bounds the failure probability of the algorithm. The following lemma, whose proof is postponed to Appendix E.4, as mentioned beforehand, shows that the confidence sets \(\cap_{t\geq 1}\mathcal{C}_{t}^{\delta}(\hat{\theta}_{t})\) have coverage \(1-\delta\):

**Lemma 28**.: _Let Assumptions 1 and 2 hold and choose \(M\geq\max(K/\log(2),1/(c_{1}-S_{1}),1/(c_{2}+S_{2}))\) in Eqs. (34) and (35), where \(\Gamma\) is any stretch function for \((Q_{u})_{u\in[S_{2},S_{1}]}\) and \(K=\sup_{S_{2}\leq u\leq S_{1}}\Gamma(u)\). Then, for the confidence set defined in Eq. (33) and for all \(\delta\in(0,1]\),_

\[\mathbb{P}(\forall t\geq 1,\theta_{\star}\in\mathcal{C}_{t}^{\delta}(\hat{ \theta}_{t}))\geq 1-\delta.\]

Note that Theorem 7 and Assumption 2 guarantees the existence of a stretch function \(\Gamma\) mentioned in the theorem.

In the remainder of this section, we will fix \(\Gamma\) to one such stretch function.

In general, here, one wants to use the smallest such stretch function (i.e., \(\Gamma=\Gamma_{Q}\)). When \(\Gamma_{Q}\) is not available, in the lack of a better choice for \(\Gamma\), the choice worked out in Theorem 7 can be used.

### Proof of regret upper bound

Let \(E_{\delta}\) be the event that \(E_{\delta}=\{\theta_{\star}\in\mathcal{C}_{t}^{\delta}(\hat{\theta}_{t})\}\) which by Lemma 28 holds with probability at least \(1-\delta\). For the next theorem, recall that \(S_{0}=\sup_{\theta\in\Theta}\|\theta\|\) and \(S_{1}=\sup\mathcal{U},S_{2}=\inf\mathcal{U}\).

**Theorem 29**.: _Let \(\delta\in(0,1]\) and \(T\) a positive integer and consider a well-posed GLB model \(\mathcal{G}=(\mathcal{X},\Theta,\mathcal{Q})\) and assume that Assumptions 1 and 2 hold. Then, by setting \(\mathcal{C}_{t}=\mathcal{C}_{t}^{\delta}(\hat{\theta}_{t})\), for any \(\theta_{\star}\in\Theta\), with probability at least \(1-\delta\), the regret \(\operatorname{Regret}(T)\) of Algorithm 1 when it interacts with the GLB instance specified by \(\theta_{\star}\) can be upper bounded by,_

\[\operatorname{Regret}(T) \leq 8c\,\gamma_{T}(\delta)\sqrt{d\dot{\mu}(x_{\star}^{\top}\theta_{ \star})(1+L/\lambda)\log\left(1+\frac{LT}{d\lambda}\right)T}\] \[+ 8c^{2}\,\gamma_{T}^{2}(\delta)L^{2}K\kappa\log(\lambda+T/d)\] \[+ 32c^{2}\,\gamma_{T}^{2}(\delta)\cdot Kd(1+L/\lambda)\log\left(1+ \frac{LT}{d\lambda}\right)\!,\]

_where \(c=(1+2K(S_{1}-S_{2}))\) and_

\[K=\sup_{S_{2}\leq u\leq S_{1}}\Gamma(u)\,.\] (36)Proof.: We first consider the case that the base distribution has \(0\) variance, which implies that \(Q\) is a Dirac. As discussed beforehand, and as it is easy to see it, in this case \(\mathcal{U}_{Q}=\mathbb{R}\), \(Q_{u}=Q\) for any \(u\in\mathbb{R}\). Hence, all arms have the same payoff and all algorithm incur zero regret. In the rest of this proof, we assume that \(\mathrm{Var}(Q)>0\). Since \(\mu(\cdot)=\dot{\psi}_{Q}(\cdot)\) is infinitely differentiable (Proposition 2), we can perform a second-order Taylor expansion on the regret

\[\mathrm{Regret}(T) =\sum_{t=1}^{T}\mu(x_{\star}^{\top}\theta_{\star})-\mu(X_{t}^{ \top}\theta_{\star})\] \[=\underbrace{\sum_{t=1}^{T}\dot{\mu}(X_{t}^{\top}\theta_{\star})( x_{\star}-X_{t})^{\top}\theta_{\star}}_{R_{1}(T)}+\underbrace{\frac{1}{2}\sum_{t=1}^{T} \ddot{\mu}(\xi_{t})((x_{\star}-X_{t})^{\top}\theta_{\star})^{2}}_{R_{2}(T)},\]

where \(\xi_{t}\) is between \(X_{t}^{\top}\theta_{\star}\) and \(x_{\star}^{\top}\theta_{\star}\) for all \(t\in[T]\). On event \(E_{\delta}\), by definition of \(X_{t},\theta_{t}\) (in Algorithm 1), it holds that \(x_{\star}^{\top}\theta_{\star}\leq X_{t}^{\top}\theta_{t}\). Observe that \(\gamma_{t}(\delta)\) (Eq. (35)) is increasing in \(t\), we have that \(\gamma_{t}(\delta)\leq\gamma_{T}(\delta)\) for all \(t\in[T]\). Then we can bound \(R_{1}(T)\) as follows

\[R_{1}(T) =\sum_{t=1}^{T}\dot{\mu}(X_{t}^{\top}\theta_{\star})(x_{\star}-X_ {t})^{\top}\theta_{\star}\] \[\leq\sum_{t=1}^{T}\dot{\mu}(X_{t}^{\top}\theta_{\star})X_{t}^{ \top}(\theta_{t}-\theta_{\star})\] \[\leq\sum_{t=1}^{T}\dot{\mu}(X_{t}^{\top}\theta_{\star})\lVert X_{ t}\rVert_{H_{t}^{-1}(\theta_{\star})}\lVert\theta_{t}-\theta_{\star} \rVert_{H_{t}(\theta_{\star})}\]

where the last inequality is due to Cauchy-Schwarz. Since \(\theta_{t}\) and \(\theta_{\star}\) are all in the confidence set on \(E_{\delta}\), we are able to bound \(\lVert\theta_{t}-\theta_{\star}\rVert_{H_{t}(\theta_{\star})}\) by the following lemma that exploits the properties of confidence set as well as self-concordant functions. This lemma is a variation of proposition 4 of Abeille, Faury, and Calauzenes [1] where they show the result for logistic function that is \(1\)-self-concordant.

**Lemma 30** (\(\mathcal{C}_{t}^{\delta}(\hat{\theta}_{t})\) has small ellipsoidal diameters).: _Under Assumptions 1 and 2, for all \(\theta_{1},\theta_{2}\in\mathcal{C}_{t}^{\delta}(\hat{\theta}_{t})\), it follows that_

\[\lVert\theta_{1}-\theta_{2}\rVert_{H_{t}(\theta_{1})}\vee\lVert\theta_{1}- \theta_{2}\rVert_{H_{t}(\theta_{2})}\leq 2(1+2K\cdot(S_{1}-S_{2}))\gamma_{t}( \delta),\]

_where \(K\) is defined in Eq. (36)._

By Lemma 30, we can upper bound \(R_{1}(T)\) to be

\[R_{1}(T)\leq\sum_{t=1}^{T}\dot{\mu}(X_{t}^{\top}\theta_{\star})\lVert X_{t} \rVert_{H_{t}^{-1}(\theta_{\star})}\cdot 2(1+2K(S_{1}-S_{2}))\gamma_{T}(\delta).\]

Denote \(A_{t}=\sqrt{\dot{\mu}(X_{t}^{\top}\theta_{\star})}X_{t}\) and we have that \(H_{t}(\theta_{\star})=\sum_{s=1}^{t}A_{t}A_{t}^{\top}+\lambda I\) as well as \(\lVert A_{t}\rVert\leq\sqrt{L}\leq L\) where the second inequality is because WLOG we assume \(L\geq 1\) in Assumption 2. We can bound \(R_{1}(T)\) in the terms of \(A_{t}\).

\[R_{1}(T) \leq 2(1+2K(S_{1}-S_{2}))\gamma_{T}(\delta)\sum_{t=1}^{T}\sqrt{\dot{ \mu}(X_{t}^{\top}\theta_{\star})\|X_{t}\|_{H_{t}^{-1}(\theta_{\star})}^{2}} \sqrt{\dot{\mu}(X_{t}^{\top}\theta_{\star})}\] \[\leq 2(1+2K(S_{1}-S_{2}))\gamma_{T}(\delta)\sqrt{\sum_{t=1}^{T} \dot{\mu}(X_{t}^{\top}\theta_{\star})\|X_{t}\|_{H_{t}^{-1}(\theta_{\star})}^{2} \sqrt{\sum_{t=1}^{T}\dot{\mu}(X_{t}^{\top}\theta_{\star})}}\] (Cauchy-Schwarz) \[=2(1+2K(S_{1}-S_{2}))\gamma_{T}(\delta)\sqrt{\sum_{t=1}^{T}\|A_{ t}\|_{H_{t}^{-1}(\theta_{\star})}^{2}}\sqrt{\sum_{t=1}^{T}\dot{\mu}(X_{t}^{ \top}\theta_{\star})}\] \[\leq 4(1+2K(S_{1}-S_{2}))\gamma_{T}(\delta)\sqrt{d(1+L/\lambda) \log\left(1+\frac{LT}{d\lambda}\right)}\sqrt{\sum_{t=1}^{T}\dot{\mu}(X_{t}^{ \top}\theta_{\star})}\]

where in the last step we use elliptical potential lemma of Abbasi-Yadkori, Pal, and Szepesvari [1], which, for easy of reference, we give in Lemma 38. We now start to bound \(R_{2}(T)\). For convenience, we throw away the factor of \(1/2\).

\[R_{2}(T) \leq\sum_{t=1}^{T}\ddot{\mu}(\xi_{t})((x_{\star}-X_{t})^{\top} \theta_{\star})^{2}\] \[\leq\sum_{t=1}^{T}\ddot{\mu}(\xi_{t})(X_{t}^{\top}(\theta_{t}- \theta_{\star}))^{2} (X_{t}^{\top}\theta_{t}\geq x_{\star}^{\top}\theta_{\star}\geq X_{t}^{ \top}\theta_{\star})\] \[\leq\sum_{t=1}^{T}\ddot{\mu}(\xi_{t})\left\|X_{t}\right\|_{H_{t}^ {-1}(\theta_{\star})}^{2}\left\|\theta_{t}-\theta_{\star}\right\|_{H_{t}(\theta _{\star})}^{2}\] \[\leq 4(1+2K(S_{1}-S_{2}))^{2}\gamma_{T}(\delta)^{2}\sum_{t=1}^{T} \ddot{\mu}(\xi_{t})\left\|X_{t}\right\|_{H_{t}^{-1}(\theta_{\star})}^{2}\]

where in the last inequality we use Lemma 30. By definition of self-concordant function, we have that \(\ddot{\mu}(\xi_{t})\leq I(\xi_{t})\dot{\mu}(\xi_{t})\). Since \(\xi_{t}\) is between \(x_{\star}^{\top}\theta_{\star}\) and \(X_{t}^{\top}\theta_{\star}\). Note that \(\Gamma\) defined in Theorem 7 is increasing on \([0,c_{1})\) and decreasing on \((-c_{2},0)\), which gives us \(\Gamma(\xi_{t})\leq\Gamma(X_{t}^{\top}\theta_{\star})\vee\Gamma(x_{\star}^{ \top}\theta_{\star})\leq K\). Let \(V_{t}=\lambda I+\sum_{i=1}^{t}X_{i}X_{i}^{\top}\). We hence have

\[R_{2}(T) \leq 4(1+2K(S_{1}-S_{2}))^{2}\gamma_{T}(\delta)^{2}\sum_{t=1}^{T}K \dot{\mu}(\xi_{t})\left\|X_{t}\right\|_{H_{t}^{-1}(\theta_{\star})}^{2}\] \[\leq 4(1+2K(S_{1}-S_{2}))^{2}\gamma_{T}(\delta)^{2}KL\sum_{t=1}^{T} \left\|X_{t}\right\|_{H_{t}^{-1}(\theta_{\star})}^{2} (\dot{\mu}(\cdot)\leq L\text{ (Assumption \ref{eq:L1})})\] \[\leq 4(1+2K(S_{1}-S_{2}))^{2}\gamma_{T}(\delta)^{2}KL\kappa\cdot \sum_{t=1}^{T}\left\|X_{t}\right\|_{V_{t}^{-1}}^{2} (H_{t}^{-1}(\theta_{\star})\preceq\kappa V_{t}^{-1})\] \[\leq 4(1+2K(S_{1}-S_{2}))^{2}\gamma_{T}(\delta)^{2}KL\kappa\cdot L \log(\lambda+T/d).\] (Lemma 38 )

Putting the bound on \(R_{1}(T)\) and \(R_{2}(T)\) together, we have that

\[\mathrm{Regret}(T) =R_{1}(T)+R_{2}(T)\] \[\leq 4(1+2K(S_{1}-S_{2}))\gamma_{T}(\delta)\sqrt{d(1+L/\lambda) \log\left(1+\frac{LT}{d\lambda}\right)}\sqrt{\sum_{t=1}^{T}\dot{\mu}(X_{t}^{ \top}\theta_{\star})}\] \[+4(1+2K(S_{1}-S_{2}))^{2}\gamma_{T}(\delta)^{2}L^{2}K\kappa\log( \lambda+T/d).\] (37)

We mimic the trick used in Janz, Liu, Ayoub, and Szepesvari [11] to bound \(\sqrt{\sum_{t=1}^{T}\dot{\mu}(X_{t}^{\top}\theta_{\star})}\) which was originally proposed by Abeille, Faury, and Calauzenes [1]. We present the following lemma that is abstracted out from Claim 14 of Janz, Liu, Ayoub, and Szepesvari [11].

**Lemma 31** (Self-bounding property of self-concordance functions).: _Let \(\mathcal{V}=[a,b]\), a closed, nonempty interval over the reals, \(f\) a real valued function defined over an interval of the reals that is twice continuously differentiable over \(\mathcal{V}\) such that for some \(\Gamma:\mathcal{V}\rightarrow\mathbb{R}_{+}\), \(|\tilde{f}(v)|\leq\Gamma(v)\dot{f}(v)\) holds for all \(v\in\mathcal{V}\). Assume that \(A=\sup_{v\in\mathcal{V}}\Gamma(v)<\infty\). Furthermore, assume that either \(\dot{f}\) is identically zero over \(\mathcal{V}\), or \(\dot{f}\) is positive valued over \(\mathcal{V}\). For \(n\) a positive integer, let \(\{a_{t}\}_{t=1}^{n}\subset\mathcal{V}\). Then,_

\[\sum_{t=1}^{n}\dot{f}(a_{t})\leq n\dot{f}(b)+A\sum_{t=1}^{n}f(b)-f(a_{t}).\]

We apply this lemma with \(f=\mu\), \([a,b]=[S_{2},x_{\star}^{\top}\theta_{\star}]\subset[S_{2},S_{1}]\) and \(\Gamma\) restricted to \([a,b]\). Then, all the conditions of the lemma are satisfied by our choice of \(\Gamma\). Furthermore, \(A=\sup_{v\in[S_{2},x_{\star}^{\top}\theta_{\star}]}\Gamma(v)\leq K<+\infty\). Hence, all the conditions of the lemma are verified. Hence,

\[\sqrt{\sum_{t=1}^{T}\dot{\mu}(X_{t}^{\top}\theta_{\star})} \leq\sqrt{T\dot{\mu}(x_{\star}^{\top}\theta_{\star})+K\mathrm{ Regret}(T)}\] \[\leq\sqrt{T\dot{\mu}(x_{\star}^{\top}\theta_{\star})}+\sqrt{K \mathrm{Regret}(T)}.\] (38)

Plug Eq. (38) into Eq. (37),

\[\mathrm{Regret}(T) \leq 4(1+2K(S_{1}-S_{2}))\gamma_{T}(\delta)\sqrt{d\dot{\mu}(x_{ \star}^{\top}\theta_{\star})(1+L/\lambda)\log\left(1+\frac{LT}{d\lambda} \right)T}\] \[+4(1+2K(S_{1}-S_{2}))^{2}\gamma_{T}(\delta)^{2}L^{2}K\kappa\log( \lambda+T/d)\] \[+4(1+2K(S_{1}-S_{2}))\gamma_{T}(\delta)\sqrt{Kd(1+L/\lambda)\log \left(1+\frac{LT}{d\lambda}\right)}\sqrt{\mathrm{Regret}(T)}.\]

Let

\[A =4(1+2K(S_{1}-S_{2}))\gamma_{T}(\delta)\sqrt{Kd(1+L/\lambda)\log \left(1+\frac{LT}{d\lambda}\right)},\] \[B =4(1+2K(S_{1}-S_{2}))\gamma_{T}(\delta)\sqrt{d\dot{\mu}(x_{ \star}^{\top}\theta_{\star})(1+L/\lambda)\log\left(1+\frac{LT}{d\lambda} \right)T}\] \[+4(1+2K(S_{1}-S_{2}))^{2}\gamma_{T}(\delta)^{2}L^{2}K\kappa\log( \lambda+T/d),\]

we can write out the inequality

\[\mathrm{Regret}(T)\leq A\sqrt{\mathrm{Regret}(T)}+B.\]

Solving it we have that

\[\mathrm{Regret}(T)\leq 2A^{2}+2B.\]

Plugging in the definition of \(A\) and \(B\) back,

\[\mathrm{Regret}(T) \leq 8(1+2K(S_{1}-S_{2}))\gamma_{T}(\delta)\sqrt{d\dot{\mu}(x_{ \star}^{\top}\theta_{\star})(1+L/\lambda)\log\left(1+\frac{LT}{d\lambda} \right)T}\] \[+8(1+2K(S_{1}-S_{2}))^{2}\gamma_{T}(\delta)^{2}L^{2}K\kappa\log( \lambda+T/d)\] \[+32(1+2K(S_{1}-S_{2}))^{2}\gamma_{T}(\delta)^{2}\cdot Kd(1+L/ \lambda)\log\left(1+\frac{LT}{d\lambda}\right)\!\!.\qed\]

### Self-concordance control

In this section we provide technical results about self-concordant functions which play important roles in confidence set construction and controlling the regret of Algorithm 1. Specifically, Corollary 33 and Lemma 34 are used to show Lemma 30, one of the key lemmas we use in the proof of Theorem 29. Lemma 5 is used to justify the confidence set contains \(\theta_{\star}\) with high probability (Lemma 28).

The next lemma shows that for self-concordant NEFs, \(\mu\) is a smooth function of its argument. The lemma is essentially the same as Lemma 3 from Janz, Liu, Ayoub, and Szepesvari [11] (itself based on a result of Sun and Tran-Dinh [16]) and is updated only to match our definitions of self-concordance, which is a refinement of that used by Janz, Liu, Ayoub, and Szepesvari [11]. The proof (based on the proof of a similar result of Sun and Tran-Dinh [16]) is included for the convenience of the reader.

**Lemma 32** (Self-concordance to smoothness).: _Let \(\mathcal{U}\) be an interval over the reals, \(\mu\) a real valued function defined over an interval of the reals that is twice continuously differentiable over \(\mathcal{U}\) such that for some \(\Gamma:\mathcal{U}\to\mathbb{R}_{+}\), \(|\ddot{\mu}(u)|\leq\Gamma(u)\dot{\mu}(u)\) holds for all \(u\in\mathcal{U}\). Assume that \(K=\sup_{u\in\mathcal{U}}\Gamma(u)<\infty\). Assume that either \(\dot{\mu}\) is identically zero over \(\mathcal{U}\), or \(\dot{\mu}\) is positive valued over \(\mathcal{U}\). Then, for any \(u,u^{\prime}\in\mathcal{U}\),_

\[\dot{\mu}(u^{\prime})\leq\dot{\mu}(u)e^{K|u-u^{\prime}|}.\]

An immediate corollary of this lemma is that self-concordance of a NEF implies that the variance function, \(\dot{\mu}\), of the NEF is smooth:

**Corollary 33** (Self-concordance to smoothness).: _Let \(\mathcal{Q}=(Q_{u})_{u\in\mathcal{U}}\) be self-concordant with stretch function \(\Gamma:\mathcal{U}\to\mathbb{R}_{+}\), where \(\mathcal{U}\) is an interval and assume \(K=\sup_{u\in\mathcal{U}}\Gamma(u)<\infty\). Then for any \(u,u^{\prime}\in\mathcal{U}\),_

\[\dot{\mu}(u^{\prime})\leq\dot{\mu}(u)e^{K|u-u^{\prime}|}.\]

Note that the inequality is well-posed since \(u,u^{\prime}\in\mathcal{U}_{Q}^{\circ}\), and \(\mu\) is known to be differentiable over \(\mathcal{U}_{Q}^{\circ}\) and, by the definition of self-concordance, \(\mathcal{U}\subset\mathcal{U}_{Q}^{\circ}\).

Proof.: This result follows from Lemma 32 once we notice that the variance function of a NEF is such that if \(\dot{\mu}(u)=0\) for any \(u\in\mathcal{U}\), then \(\dot{\mu}\) is identically zero over \(\mathcal{U}\). Indeed, if \(\dot{\mu}(u)=0\), then \(Q_{u}\) is a Dirac distribution and so is \(Q_{v}\) for any \(v\in\mathcal{U}\). 

Proof of Lemma 32.: When \(\dot{\mu}\) is identically zero over \(\mathcal{U}\), the statement is trivial. Hence, consider now the case when \(\dot{\mu}\) is positive valued over \(\mathcal{U}\):

\[\dot{\mu}(v)>0\qquad\text{for all}\quad\ v\in\mathcal{U}\,.\] (39)

Then, it suffices to show that \(\ln\frac{\dot{\mu}(u^{\prime})}{\dot{\mu}(u)}\leq K|u-u^{\prime}|\). To show this, define \(\phi(t)=\dot{\mu}(u+t(u^{\prime}-u))\) so that \(\phi(0)=\dot{\mu}(u)\) and \(\phi(1)=\dot{\mu}(u^{\prime})\). Since \(\mathcal{U}_{Q}\) is an interval with non-empty interior, \(\phi(t)\) is well-defined for all \(t\in[0,1]\). Furthermore, by Eq. (39) and since \(\mathcal{U}\) is an interval, we have that \(\phi(t)>0\) for all \(t\in[0,1]\). Consider now the map \(t\mapsto\ln\phi(t)\) where \(t\in[0,1]\). The derivative of this map exist and is continuous over \((0,1)\), and in particular, \(\frac{d}{dt}\ln\phi(t)=\frac{\dot{\phi}(t)}{\phi(t)}\) by the chain rule. Indeed, the derivative of \(\phi\) exists and is continuous over \((0,1)\), because the same holds for \(\dot{\mu}\) by the properties of NEFs, and as we just discussed, \(\phi\) is positive over \([0,1]\) and is continuous. Now, by the fundamental theorem of calculus applied to \(t\mapsto\frac{d}{dt}\ln\phi(t)\) and by the monotonicity of integrals,

\[\ln\frac{\dot{\mu}(u^{\prime})}{\dot{\mu}(u)}=\ln\frac{\phi(1)}{\phi(0)}=\int_ {0}^{1}\frac{d\ln\phi(t)}{dt}dt\leq\int_{0}^{1}\left|\frac{d\ln\phi(t)}{dt} \right|dt.\] (40)

It remains to bound the integrand in the rightmost expression. For this, as discussed earlier we have

\[\left|\frac{d\ln\phi(t)}{dt}\right|=\left|\frac{\phi^{\prime}(t)}{\phi(t)} \right|=\frac{|\phi^{\prime}(t)|}{\phi(t)}\,.\] (41)

To bound the ratio on the right, we again use the chain rule and calculate

\[|\phi^{\prime}(t)|=|\ddot{\mu}(u+t(u^{\prime}-u))||u^{\prime}-u|\leq K\, \underbrace{\dot{\mu}(u+t(u^{\prime}-u))}_{\phi(t)}|u^{\prime}-u|\,,\]

where the inequality follows by the definition of \(K\) by definition of self-concordant function, we have that for all \(u\in\mathcal{U}\), \(|\ddot{\mu}(u)|\leq K\dot{\mu}(u)\). Now, the result follows since we have shown that the integrand is upper bounded by \(K|u-u^{\prime}|\) and thus \(\int_{0}^{1}\left|\frac{d\ln\phi(t)}{dt}\right|dt\leq K|u-u^{\prime}|\)We continue with two results, both of which use the lemma just proved. The first result gives a lower bound for the integral remainder term when Taylor's theorem is used to approximate \(\mu\). The second result gives a quadratic upper bound on the CGF of \(Q_{u}\), and will be the basis for constructing our confidence set.

**Lemma 34**.: _Let \(\mathcal{Q}=(Q_{u})_{u\in\mathcal{U}}\) be self-concordant with stretch function \(\Gamma:\mathcal{U}\to\mathbb{R}_{+}\) where \(\mathcal{U}\) is an interval, and assume \(K=\sup_{u\in\mathcal{U}}\Gamma(u)<\infty\). Then for any \(u,u^{\prime}\in\mathcal{U}\),_

\[\int_{0}^{1}\dot{\mu}(u+t(u^{\prime}-u))dt\geq\frac{\dot{\mu}(u)}{1+K|u-u^{ \prime}|}\,.\]

Proof.: By Corollary 33, it follows that

\[\dot{\mu}(u+t(u^{\prime}-u))\geq\dot{\mu}(u)\exp(-Kt|u^{\prime}-u|).\]

Integrating both sides between \(0\) and \(1\) gives us

\[\int_{0}^{1}\dot{\mu}(u+t(u^{\prime}-u))\geq\dot{\mu}(u)\int_{0}^ {1}\exp(-Kt|u^{\prime}-u|)dv\] \[\qquad=\dot{\mu}(u)\left[\frac{-\exp(-Kt|u^{\prime}-u|)}{K|u^{ \prime}-u|}\right]_{0}^{1}\] \[\qquad=\dot{\mu}(u)\frac{1-\exp(-K|u^{\prime}-u|)}{K|u^{\prime}- u|}\] \[\qquad\geq\frac{\dot{\mu}(u)}{1+K|u^{\prime}-u|}\,,\]

where the last inequality follows from the elementary inequality \((1-e^{-x})/x\geq 1/(1+x)\) that holds for all \(x\geq 0\). 

We are now ready to prove Lemma 5. As noted beforehand, we adopt this lemma from the work of Janz, Liu, Ayoub, and Szepesvari [11]. In particular, it is an adaptation of their Lemma 1, which was proved for distributions where \(\mathcal{U}_{Q}=\mathbb{R}\). Here, we deal with the case when \(\mathcal{U}_{Q}\) is possibly a strict subset of \(\mathbb{R}\).

**Lemma 5** (From self-concordance to light tails).: _Let \(\mathcal{Q}=(Q_{u})_{u\in\mathcal{U}}\) be a NEF which is self-concordant with stretch function \(\Gamma:\mathcal{U}\to\mathbb{R}_{+}\) where \(\mathcal{U}\) is a subinterval of \(\mathcal{U}_{Q}^{\prime}=(a,b)\). Then, for any \(u\in\mathcal{U}\),_

\[\psi_{Q_{u}}(s)\leq s\mu(u)+s^{2}\dot{\mu}(u)\quad\text{for all}\quad s\in[- \log(2)/K,\log(2)/K]\cap(a-\inf\mathcal{U},b-\sup\mathcal{U})\,,\] (2)

_where \(K=\sup_{u\in\mathcal{U}}\Gamma(u)\)._

Proof.: Let \(u\in\mathcal{U}\). Hence, by our assumption on \(\mathcal{U}\), \(u\in\mathcal{U}_{Q}\). Now let \(s\in\mathbb{R}\). Then,

\[\psi_{Q_{u}}(s) =\log\int\exp(sy)Q_{u}(dy)=\log\left[\frac{1}{M_{Q}(u)}\int\exp( sy)\exp(uy)Q(dy)\right]\] \[=\psi_{Q}(u+s)-\psi_{Q}(u)\,.\]

Hence, \(\psi_{Q_{u}}(s)\) is finite valued whenever \(u+s\in\mathcal{U}_{Q}\). Assume that this holds and in fact \(u+s\in\mathcal{U}_{Q}^{\circ}\). Since \(u,u+s\in\mathcal{U}_{Q}^{\circ}\), \(\psi_{Q}\) is twice continuously differentiable over an open interval containing \(u\) and \(u+s\). Then, by Taylor's theorem there exists \(\xi\) in the closed interval between \(u\) and \(u+s\) such that

\[\psi_{Q}(u+s)-\psi_{Q}(u)=s\dot{\psi}_{Q}(u)+\frac{s^{2}}{2}\ddot{\psi}_{Q}( \xi)\,.\]

Since \(\dot{\psi}_{Q}=\mu\) and \(\ddot{\psi}_{Q}=\dot{\mu}\) (cf. Proposition 2) we get

\[\psi_{Q}(u+s)-\psi_{Q}(u)=s\mu(u)+\frac{s^{2}}{2}\dot{\mu}(\xi)\,.\]Now, by Corollary 33, we have that

\[\dot{\mu}(\xi)\leq\dot{\mu}(u)\cdot e^{K|u-\xi|}\leq\dot{\mu}(u)e^{Ks}\leq 2\dot{ \mu}(u),\]

where the final inequality follows when \(|s|\leq\log(2)/K\). Putting things together, it follows that if \(|s|\leq\log(2)/K\) and \(s\in\mathcal{U}_{Q}^{\circ}-\{u\}\) then

\[\psi_{Q_{u}}(s)=\psi_{Q}(u+s)-\psi_{Q}(u)\leq s\mu(u)+s^{2}\dot{ \mu}(u)\,.\]

For \(S\subset\mathbb{R}\), \(r\in\mathbb{R}\), let \(S\pm r=\{s\pm r\,:\,s\in S\}\). Since \(u\) is an arbitrary point in \(\mathcal{U}\), the above conditions on \(s\) will be satisfied if \(|s|\leq\log(2)/K\) and \(s\in Z\doteq\cap_{u\in\mathcal{U}}\mathcal{U}_{Q}^{\circ}-u\). Now, from \(\mathcal{U}_{Q}^{\circ}=(a,b)\), we have \(Z=\cap_{u\in\mathcal{U}}(a-u,b-u)=(\sup_{u\in\mathcal{U}}a-u,\inf_{u\in \mathcal{U}}b-u)=(a-\inf\mathcal{U},b-\sup\mathcal{U})\), finishing the proof. 

From the calculation at the end of the proof it follows that the statement of the lemma is non-vacuous if for \(\mathcal{U}_{Q}^{\circ}=(a,b)\), \(a-\inf\mathcal{U}<0<b-\sup\mathcal{U}\), which is equivalent to that \(a<\inf\mathcal{U}\) and \(\sup\mathcal{U}<b\), which is always satisfied when \(\mathcal{U}\) is a strict subset of \(\mathcal{U}_{Q}^{\circ}\).

### Confidence set construction

We now turn to proving Lemma 28 which is concerned with showing that the confidence sets \(\mathcal{C}_{t}^{\delta}\) contain the true parameter \(\theta_{\star}\) with probability \(1-\delta\). The proof is essentially the same as that of Lemma 4 of [10]. We will need the following result, which is taken verbatim from the paper of Janz, Liu, Ayoub, and Szepesvari [10].

**Proposition 35** (Theorem 2 of [10]).: _Fix \(\lambda,M>0\). Let \((X_{t})_{t\in\mathbb{N}^{+}}\) be a \(B_{2}^{d}\)-valued random sequence, \((Y_{t})_{t\in\mathbb{N}^{+}}\) a real valued random sequence and \((\nu_{t})_{t\in\mathbb{N}}\) be a nonnegative valued random sequence. Let \(\mathbb{F}=(\mathbb{F}_{t})_{t\in\mathbb{N}}\) be a filtration such that \((i)\)\((X_{t})_{t\in\mathbb{N}^{+}}\) is \(\mathbb{F}\)-predictable and \((ii)\)\((Y_{t})_{t\in\mathbb{N}^{+}}\) are \(\mathbb{F}\)-adapted. Let \(\epsilon_{t}=Y_{t}-\mathbb{E}[Y_{t}|\mathbb{F}_{t-1}]\) and assume that the following condition holds:_

\[\mathbb{E}[\exp(s\epsilon_{t})|\mathbb{F}_{t-1}]\leq\exp(s^{2} \nu_{t-1})\quad\text{for all}\quad|s|\leq 1/M\text{ and }t\in\mathbb{N}^{+}.\]

_Then, for \(\tilde{H}_{t}=\sum_{i=1}^{t}\nu_{i-1}X_{i}X_{i}^{\top}+\lambda I\) and \(\mathbf{S}_{t}=\sum_{i=1}^{t}\epsilon_{i}X_{i}\) and \(\delta>0\),_

\[\mathbb{P}\left(\exists t\in\mathbb{N}^{+}:\|\mathbf{S}_{t}\|_{ \tilde{H}_{t}^{-1}}\geq\frac{\sqrt{\lambda}}{2M}+\frac{2M}{\sqrt{\lambda}} \log\left(\frac{\det(\tilde{H}_{t})^{1/2}\lambda^{-d/2}}{\delta}\right)+\frac{ 2M}{\sqrt{\lambda}}d\log(2)\right)\leq\delta.\]

We now turn to proving Lemma 28. For the convenience of the reader, we start by recalling the definition of the confidence sets \(\mathcal{C}_{t}^{\delta}\) involved (cf. Eq. (33)). Recall that \(c_{2}<S_{2}\leq x^{\top}\theta\leq S_{1}<c_{1}\) for \(x\in\mathcal{X}\) and \(\theta\in\Theta\). For \(\delta\in(0,1]\), we have

\[\mathcal{C}_{t}^{\delta,\lambda}(\hat{\theta}_{t})=\left\{\theta\in\Theta\,: \,\left\|g_{t}(\theta)-g_{t}(\hat{\theta}_{t})\right\|_{H_{t}^{-1}(\theta)} \leq\gamma_{t}(\delta)\right\}\]

where

\[\gamma_{t}(\delta) =\sqrt{\lambda_{T}}\left(\frac{1}{2M}+S_{0}\right)+\frac{4Md}{ \sqrt{\lambda_{T}}}\log\left(e\sqrt{1+\frac{tL}{d}}\lor 1/\delta\right)\quad\text{ for all}\quad t\in[T]\,,\] \[\lambda_{T} =1\lor\frac{2dM}{S_{0}}\log\left(e\sqrt{1+\frac{TL}{d}}\lor 1/ \delta\right),\]

and recall that \(S_{0}=\sup_{\theta\in\Theta}\|\theta\|\) or an upper bound on this quantity, and \(M\) is specified in the next result:

**Lemma 28**.: _Let Assumptions 1 and 2 hold and choose \(M\geq\max(K/\log(2),1/(c_{1}-S_{1}),1/(c_{2}+S_{2}))\) in Eqs. (34) and (35), where \(\Gamma\) is any stretch function for \((Q_{u})_{u\in[S_{2},S_{1}]}\) and \(K=\sup_{S_{2}\leq u\leq S_{1}}\Gamma(u)\). Then, for the confidence set defined in Eq. (33) and for all \(\delta\in(0,1]\),_

\[\mathbb{P}(\forall t\geq 1,\theta_{\star}\in\mathcal{C}_{t}^{\delta}(\hat{ \theta}_{t}))\geq 1-\delta.\]Proof.: From definition it follows that \(\theta_{\star}\in\Theta\). Now we prove that with probability at least \(1-\delta\), it holds that \(\|g_{t}(\theta_{\star})-g_{t}(\hat{\theta}_{t})\|\leq\gamma_{T}(\delta)\) for all \(t\geq 1\). The proof goes through by using Proposition 35 and we now match the conditions of Proposition 35. By Assumption 1, we have that \((X_{t})_{t\in\mathbb{N}^{+}}\) is a \(B_{d}^{d}\)-valued random sequence. Let \(\mathcal{F}_{t-1}=\sigma(X_{1},Y_{1},...,X_{t-1},Y_{t-1},X_{t})\) for \(t\geq 1\). Consider the filtration \(\mathcal{F}=(\mathcal{F}_{t})_{t\in\mathbb{N}}\). Then by definition, \((X_{t})_{t\in\mathbb{N}^{+}}\) are \(\mathcal{F}\)-predictable and \((Y_{t})_{t\in\mathbb{N}^{+}}\) are \(\mathcal{F}\)-adapted. Note that \(\mu(X_{i}^{\top}\theta_{\star})=\mathbb{E}[Y_{i}|\mathcal{F}_{i-1}]\) for all \(i\in[n]\). Let \(\varepsilon_{i}=Y_{i}-\mathbb{E}[Y_{i}|\mathcal{F}_{i-1}]\). This gives \((\varepsilon_{t})_{t\in\mathbb{N}^{+}}\) are also \(\mathcal{F}\)-adapted and the following identity follows by definition

\[g_{t}(\hat{\theta}_{t})-g_{t}(\theta_{\star}) =\sum_{i=1}^{t}\varepsilon_{i}X_{i}+\lambda\theta_{\star}\] \[=\mathbf{S}_{t}+\lambda\theta_{\star},\]

where \(\mathbf{S}_{t}=\sum_{i=1}^{t}\varepsilon_{i}X_{i}\). Let

\[\nu_{t-1}=\dot{\mu}(X_{t}^{\top}\theta_{\star}).\]

Now we would like to apply Lemma 5 to show that, for \(|s|\leq M\), it follows that

\[\mathbb{E}[\exp(s\varepsilon_{t})|\mathcal{F}_{t-1}]\leq\exp(s^{2}\nu_{t-1}).\] (42)

Applying the definition of \(\varepsilon_{t}\), it follows that

\[\mathbb{E}[\exp(s\varepsilon_{t})|\mathcal{F}_{t-1}] =\mathbb{E}[\exp(sY_{t}-s\mu(X_{t}^{\top}\theta_{\star})| \mathcal{F}_{t-1}]\] \[=\exp(-s\mu(X_{t}^{\top}\theta_{\star}))\mathbb{E}[Y_{t}| \mathcal{F}_{t-1}]\quad\text{a.s.}\,,\]

where the last equality is because \(X_{t}\) is \(\mathcal{F}_{t-1}\)-measurable. Since, by definition, the distribution of \(Y_{t}\) given \(\mathcal{F}_{t-1}\) is \(Q_{X_{t}^{\top}\theta_{\star}}\), we have that

\[\mathbb{E}[\exp(s\varepsilon_{t})|\mathcal{F}_{t-1}] =\exp(-s\mu(X_{t}^{\top}\theta_{\star}))\mathbb{E}[\exp(sY_{t})| \mathcal{F}_{t-1}]\] \[=\exp(-s\mu(X_{t}^{\top}\theta_{\star}))\int_{\mathbb{R}}e^{sy}Q _{X_{t}^{\top}\theta_{\star}}(dy)\] \[=\exp(-s\mu(X_{t}^{\top}\theta_{\star})+\psi_{Q_{X_{t}^{\top} \theta_{\star}}}(s))\] \[\leq\exp(s^{2}\dot{\mu}(X_{t}^{\top}\theta_{\star})),\]

where the last inequality is because \(|s|\leq M\) implies that \(s\in[-\log 2/K,\log 2/K]\cap(\mathcal{U}_{Q}^{\circ}-S_{2})\cap(\mathcal{U}_{Q}^{ \circ}-S_{1})\) so Lemma 5 is applicable (it is applied with \((Q_{u})_{u\in[S_{2},S_{1}]}\) and \(\Gamma\) as chosen in the statement). Then, Eq. (42) follows by noting that \(\nu_{t-1}=\dot{\mu}(X_{t}^{\top}\theta_{\star})\).

Lastly as defined above, \(\tilde{H}_{t}\) corresponds to \(H_{t}(\theta_{\star})\). Taking the \(\ell_{2}\)-norm weighted by \(H_{t}^{-1}(\theta_{\star})\) and applying triangle inequality,

\[\|g_{t}(\hat{\theta}_{t})-g_{t}(\theta_{\star})\|_{H_{t}^{-1}(\theta_{\star})} \leq\|\mathbf{S}_{t}\|_{H_{t}^{-1}(\theta_{\star})}+\lambda\|\theta_{\star}\|_ {H_{t}^{-1}(\theta_{\star})}\leq\|\mathbf{S}_{t}\|_{H_{t}^{-1}(\theta_{\star}) }+\sqrt{\lambda}S_{0},\]

where the last inequality follows by \(H_{t}^{-1}(\theta_{\star})\preceq\lambda^{-1}I\). By Proposition 35, with probability at least \(1-\delta\), it follows that for all \(t\geq 1\),

\[\|\mathbf{S}_{t}\|_{H_{t}^{-1}(\theta_{\star})}<\frac{\sqrt{\lambda}}{2M}+ \frac{2M}{\sqrt{\lambda}}\log\left(\frac{\det(H_{t}(\theta_{\star}))^{1/2}/ \lambda^{d/2}}{\delta}\right)+\frac{2M}{\sqrt{\lambda}}d\log(2).\]

We now bound \(\det(H_{t}(\theta_{\star}))/\lambda^{d}\). Let \(A_{i}=\sqrt{\dot{\mu}(X_{i}^{\top}\theta_{\star})}X_{i}\) for all \(i\in[t]\), then \(H_{t}(\theta_{\star})\) can be written as

\[H_{t}(\theta_{\star})=\lambda I+\sum_{s=1}^{t}A_{i}A_{i}^{\top}.\]

By Assumption 2, it holds that \(\sqrt{\dot{\mu}(X_{t}^{\top}\theta_{\star})}\leq\sqrt{L}\), thus \(\|A_{i}\|_{2}\leq\sqrt{L}\leq L\) for all \(i\in[t]\). Eq. (20.9) (Note 1 of section 20.2) in [10] gives

\[\det(H_{t}(\theta_{\star}))/\lambda^{d}\leq\left(1+\frac{tL}{\lambda d}\right)^ {d}.\]

The stated result follows by chaining all the inequalities together and noting that

\[\gamma_{t}(\delta)\geq\sqrt{\lambda}\left(\frac{1}{2M}+S_{0}\right)+\frac{2Md} {\sqrt{\lambda}}\left(1+\frac{1}{2}\log\left(1+\frac{tL}{\lambda d}\right) \right)+\frac{2M}{\sqrt{\lambda}}\log(1/\delta),\quad\text{for all}\quad t \in[T].\] (43)

### Proof of Lemma 30

The following two lemmas (Lemmas 36 and 37) are variations of Claim 4 and Claim 3 of Janz, Liu, Ayoub, and Szepesvari [Jan+24]. The difference is that Janz, Liu, Ayoub, and Szepesvari [Jan+24] show them for all \(\theta_{1},\theta_{2}\in\mathbb{R}^{d}\) because the MGF \(M_{Q}\) therein is finite on \(\mathbb{R}\). In our setting, there could be \(x\in\mathcal{X}\) for some \(\theta\notin\Theta\) such that \(M_{Q}(x^{\top}\theta)=\infty\), hence we show it within the parameter set \(\Theta\).

**Lemma 36**.: _For all \(\theta_{1},\theta_{2}\in\Theta\), it follows that_

\[g_{t}(\theta_{1})-g_{t}(\theta_{2})=G_{t}(\theta_{1},\theta_{2})(\theta_{1}- \theta_{2}).\]

_In particular, we have that_

\[\|g_{t}(\theta_{1})-g_{t}(\theta_{2})\|_{G_{t}^{-1}(\theta_{1},\theta_{2})}= \|\theta_{1}-\theta_{2}\|_{G_{t}(\theta_{1},\theta_{2})}.\]

Proof.: The "In particular" part follows from definition of \(\ell_{2}\)-norm weighted by \(G_{t}^{-1}(\theta_{1},\theta_{2})\). We now prove \(g_{t}(\theta_{1})-g_{t}(\theta_{2})=G_{t}(\theta_{1},\theta_{2})(\theta_{1}- \theta_{2})\). By definition of the difference quotient \(\alpha(\cdot,\cdot)\), we have that

\[\mu(u)-\mu(u^{\prime})=\alpha(u,u^{\prime})(u-u^{\prime}).\] (44)

Writing out the expression of \(g_{t}(\theta_{1})-g_{t}(\theta_{2})\) gives

\[g_{t}(\theta_{1})-g_{t}(\theta_{2}) =\sum_{i=1}^{t}\Big{(}\mu(X_{i}^{\top}\theta_{1})-\mu(X_{i}^{\top }\theta_{2})\Big{)}X_{i}+\lambda(\theta_{1}-\theta_{2})\] \[=\sum_{i=1}^{t}\Big{(}\alpha(X_{i}^{\top}\theta_{1},X_{i}^{\top} \theta_{2})X_{i}^{\top}(\theta_{1}-\theta_{2})\Big{)}X_{i}+\lambda(\theta_{1} -\theta_{2})\] (Eq. ( 44 )) \[=\left(\sum_{i=1}^{t}\alpha(X_{i}^{\top}\theta_{1},X_{i}^{\top} \theta_{2})X_{i}X_{i}^{\top}\right)(\theta_{1}-\theta_{2})+\lambda(\theta_{1} -\theta_{2})\] \[=G_{t}(\theta_{1},\theta_{2})(\theta_{1}-\theta_{2}).\qed\]

**Lemma 37**.: _Under Assumptions 1 and 2, for all \(\theta_{1},\theta_{2}\in\Theta\), it follows that_

\[G_{t}(\theta_{1},\theta_{2}) \succeq(1+2K\cdot(S_{1}-S_{2}))^{-1}H_{t}(\theta_{1})\] (45) \[G_{t}(\theta_{1},\theta_{2}) \succeq(1+2K\cdot(S_{1}-S_{2}))^{-1}H_{t}(\theta_{2}),\] (46)

_where \(K\) is defined in Eq. (36)._

Proof.: Since \(\{x^{\top}\theta:x\in\mathcal{X},\theta\in\Theta\}\subset[S_{2},S_{1}]\) by Assumption 1, we have

\[\sup\{\Gamma(x^{\top}\theta):x\in\mathcal{X},\theta\in\Theta\}\leq K.\]

By Lemma 34, we have that for all \(x\in\mathcal{X}\),

\[\alpha(x^{\top}\theta_{1},x^{\top}\theta_{2})\geq(1+K|x^{\top}(\theta_{1}- \theta_{2})|)^{-1}\dot{\mu}(x^{\top}\theta_{1})\geq(1+2K\cdot(S_{1}-S_{2}))^{- 1}\dot{\mu}(x^{\top}\theta_{1}).\]

Then the following holds

\[\sum_{i=1}^{t}\alpha(X_{i}^{\top}\theta_{1},X_{i}^{\top}\theta_{2 })X_{i}X_{i}^{\top} \succeq(1+2K\cdot(S_{1}-S_{2}))^{-1}\sum_{i=1}^{t}\dot{\mu}(x^{ \top}\theta_{1})X_{i}X_{i}^{\top}\] \[G_{t}(\theta_{1},\theta_{2}) \succeq(1+2K\cdot(S_{1}-S_{2}))H_{t}(\theta_{1}),\]

where the last inequality follows by \((1+2K\cdot(S_{1}-S_{2}))^{-1}\leq 1\). The proof of Eq. (46) follows by substituting \(\theta_{1}\) with \(\theta_{2}\). 

**Lemma 30** (\(\mathcal{C}_{t}^{\delta}(\hat{\theta}_{t})\) has small ellipsoidal diameters).: _Under Assumptions 1 and 2, for all \(\theta_{1},\theta_{2}\in\mathcal{C}_{t}^{\delta}(\hat{\theta}_{t})\), it follows that_

\[\|\theta_{1}-\theta_{2}\|_{H_{t}(\theta_{1})}\vee\|\theta_{1}-\theta_{2}\|_{H_{ t}(\theta_{2})}\leq 2(1+2K\cdot(S_{1}-S_{2}))\gamma_{t}(\delta),\]

_where \(K\) is defined in Eq. (36)._Proof.: We first prove the statement for \(\|\theta_{1}-\theta_{2}\|_{H_{t}(\theta_{1})}\). By Lemma 37, we have that

\[\|\theta_{1}-\theta_{2}\|_{H_{t}(\theta_{1})} \leq\sqrt{(1+2K\cdot(S_{1}-S_{2}))}\|\theta_{1}-\theta_{2}\|_{G_{t }(\theta_{1},\theta_{2})}\] \[=\sqrt{(1+2K\cdot(S_{1}-S_{2}))}\|g_{t}(\theta_{1})-g_{t}(\theta_ {2})\|_{G_{t}^{-1}(\theta_{1},\theta_{2})}\] (Lemma 36) \[\leq\sqrt{(1+2K\cdot(S_{1}-S_{2}))}\Big{(}\|g_{t}(\theta_{1})-g_{t }(\hat{\theta}_{t})\|_{G_{t}^{-1}(\theta_{1},\theta_{2})}+\|g_{t}(\hat{\theta }_{t})-g_{t}(\theta_{2})\|_{G_{t}^{-1}(\theta_{1},\theta_{2})}\Big{)}\]

Note that \(\theta_{1},\theta_{2}\in\mathcal{C}_{t}^{\delta}(\hat{\theta}_{t})\) by hypothesis, then Lemma 37 and the definition of \(\mathcal{C}_{t}^{\delta}(\hat{\theta}_{t})\) gives that

\[\|g_{t}(\theta_{1})-g_{t}(\hat{\theta}_{t})\|_{G_{t}^{-1}(\theta _{1},\theta_{2})} \leq\sqrt{(1+2K\cdot(S_{1}-S_{2}))}\|g_{t}(\theta_{1})-g_{t}(\hat {\theta}_{t})\|_{H_{t}^{-1}(\theta_{1})}\] \[\leq\sqrt{(1+2K\cdot(S_{1}-S_{2}))}\gamma_{t}(\delta)\] \[\|g_{t}(\hat{\theta}_{t})-g_{t}(\theta_{2})\|_{G_{t}^{-1}(\theta _{1},\theta_{2})} \leq\sqrt{(1+2K\cdot(S_{1}-S_{2}))}\|g_{t}(\hat{\theta}_{t})-g_{t }(\theta_{2})\|_{H_{t}^{-1}(\theta_{2})}\] \[\leq\sqrt{(1+2K\cdot(S_{1}-S_{2}))}\gamma_{t}(\delta).\]

Chaining all the inequalities together finishes the proof. The proof for the statement for \(\|\theta_{1}-\theta_{2}\|_{H_{t}(\theta_{2})}\) follows similarly by substituting \(\theta_{1}\) with \(\theta_{2}\). 

### Proof of self-bounding property of self-concordance functions: Lemma 31

As mentioned beforehand, the following lemma is abstracted out from Claim 14 of Janz, Liu, Ayoub, and Szepesvari [10]:

**Lemma 31** (Self-bounding property of self-concordance functions).: _Let \(\mathcal{V}=[a,b]\), a closed, nonempty interval over the reals, \(f\) a real valued function defined over an interval of the reals that is twice continuously differentiable over \(\mathcal{V}\) such that for some \(\Gamma:\mathcal{V}\rightarrow\mathbb{R}_{+}\), \(|\tilde{f}(v)|\leq\Gamma(v)\dot{f}(v)\) holds for all \(v\in\mathcal{V}\). Assume that \(A=\sup_{v\in\mathcal{V}}\Gamma(v)<\infty\). Furthermore, assume that either \(\dot{f}\) is identically zero over \(\mathcal{V}\), or \(\dot{f}\) is positive valued over \(\mathcal{V}\). For \(n\) a positive integer, let \(\{a_{t}\}_{t=1}^{n}\subset\mathcal{V}\). Then,_

\[\sum_{t=1}^{n}\dot{f}(a_{t})\leq n\dot{f}(b)+A\sum_{t=1}^{n}f(b)-f(a_{t}).\]

Proof.: We have

\[\sum_{t=1}^{n}\dot{f}(a_{t}) =\sum_{t=1}^{n}\dot{f}(b)+\sum_{t=1}^{n}(a_{t}-b)\int_{0}^{1} \ddot{f}\left(b+v(a_{t}-b)\right)dv\] \[\leq n\dot{f}(b)+\sum_{t=1}^{n}\left|\left(a_{t}-b\right)\int_{0}^ {1}\ddot{f}\left(b+v(a_{t}-b)\right)dv\right|\] \[\leq n\dot{f}(b)+\sum_{t=1}^{n}(b-a_{t})\int_{0}^{1}\left|\ddot{f }\left(b+v(a_{t}-b)\right)\right|dv\] (Lemma 32) \[\leq n\dot{f}(b)+\sum_{t=1}^{n}(b-a_{t})\int_{0}^{1}A\dot{f}\left( b+v(a_{t}-b)\right)dv\] \[\leq n\dot{f}(b)+K\sum_{t=1}^{n}f(b)-f(a_{t})\,,\] (fundamental theorem of calculus)

finishing the proof.

### Auxiliary Lemma

**Lemma 38** (Elliptical potential lemma).: _Fix \(\lambda,A>0\). Let \(\{a_{t}\}_{t=1}^{\infty}\) be a sequence in \(AB_{2}^{d}\) and let \(V_{0}=\lambda I\). Define \(V_{t+1}=V_{t}+a_{t+1}a_{t+1}^{\top}\) for each \(t\in\mathbb{N}\). Then, for all \(n\in\mathbb{N}^{+}\),_

\[\sum_{t=1}^{n}\left\|a_{t}\right\|_{V_{t-1}^{-1}}^{2}\leq 2d\max\left\{1, \frac{A^{2}}{\lambda}\right\}\log\left(1+\frac{nA^{2}}{d\lambda}\right)\,.\]

Proof.: See, e.g., Lemma 19.4 of Lattimore and Szepesvari [13]. 

## Appendix F Numerical Simulations

In this section, we report our results on numerical simulations. We run our algorithm on exponential bandits. The setting is as follows. The base distribution is an exponential distribution with parameter \(\lambda>0\) where the probability density function can be written as

\[f(x;\lambda)=\mathbb{I}(x\geq 0)\lambda\exp(-\lambda x).\]

For each arm \(x\in\mathcal{X}\), the reward distribution is an exponential distribution with parameter \(\lambda-x^{\top}\theta_{\star}\), where we ensure that \(\sup_{x\in\mathcal{X}}x^{\top}\theta_{\star}<\lambda\), as illustrated by Example 1. The number of arms \(|\mathcal{X}|=20\) and \(\mathcal{X}\subseteq\mathbb{R}^{2}\), the max variance of the reward distributions among all arms is \(0.25\) and \(\kappa=\sup_{x\in\mathcal{X}}1/\hat{\mu}(x^{\top}\theta_{\star})\approx 100\). We run our algorithms with theory suggested parameters for 60 runs where each run has horizon \(5000\). To be more specific on the parameters, we list them here:

1. The failure probability \(\delta\) is \(0.05\).
2. The regularizer is set to be \(2\).
3. The confidence width \(\gamma_{t}(\delta,\lambda)\) is set according to Eq. (43).

Here are the results of the experiments.

In Fig. 2 we plot the average regret along with standard deviation. From the plot we can see that the regret attained seems to be sublinear. In Fig. 2 we display the log-log plot where the x-axis is \(\log(\text{horizon})\) and the y-axis is \(\log(\text{regret})\). The slope gradually approaches to \(0.5\), i.e., the growth rate of regret approaches to \(\sqrt{T}\) which confirms our theoretical bound.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and/or introduction clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We do not have a lower bound. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provide detailed assumptions and very detailed proofs in the appendix. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: We do not have experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: We do not have experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: We do not have experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: We do not have experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: We do not have experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.