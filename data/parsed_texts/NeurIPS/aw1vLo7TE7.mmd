# Risk-Averse Active Sensing for Timely Outcome Prediction under Cost Pressure

Yuchao Qin

University of Cambridge, UK

&Mihaela van der Schaar

University of Cambridge, UK

Alan Turing Institute, UK

&Changhee Lee

Chung-Ang University, South Korea

###### Abstract

Timely outcome prediction is essential in healthcare to enable early detection and intervention of adverse events. However, in longitudinal follow-ups to patients' health status, cost-efficient acquisition of patient covariates is usually necessary due to the significant expense involved in screening and lab tests. To balance the timely and accurate outcome predictions with acquisition costs, an effective active sensing strategy is crucial. In this paper, we propose a novel risk-averse active sensing approach RAS that addresses the composite decision problem of _when_ to conduct the acquisition and _which measurements_ to make. Our approach decomposes the policy into two sub-policies: acquisition scheduler and feature selector, respectively. Moreover, we introduce a novel risk-aversion training strategy to focus on the underrepresented subgroup of high-risk patients for whom timely and accurate prediction of disease progression is of greater value. Our method outperforms baseline active sensing approaches in experiments with both synthetic and real-world datasets, and we illustrate the significance of our policy decomposition and the necessity of a risk-averse sensing policy through case studies.

## 1 Introduction

Timely decision-making through accumulated observation history has attracted significant attention in the machine learning community, with broad impact and applications in healthcare [8]. Consider a typical decision-making scenario in which an agent employs a _sensing policy_ to actively collect diagnosis-related information from an underlying feature trajectory. In the presence of _cost pressure_, the goal of the agent is to achieve timely and accurate predictions of a time-varying outcome of interest based on the sensing history, i.e., feature observations accumulated over time, while maintaining a reasonable expense of feature acquisition. At each decision step, we refer to the problem of deciding _when to make new acquisitions_ and _which features to measure_ as _active sensing_[2, 24]. The optimal sensing policy can be achieved by negotiating the subjective trade-off between outcome prediction (accuracy and timeliness) and acquisition cost over time.

A special class of the active sensing problem has been extensively explored in the optimal stopping literature [2, 6, 15]. In optimal stopping, the agent focuses on the timely diagnosis of a terminal status, where observation ceases immediately after a confident diagnosis can be made [1, 5]. Thereby, the acquisition cost in optimal stopping is typically considered as the time span of the information collection process. Recently, Jarrett and van der Schaar [9] introduced a novel sensing framework to allow for the consideration of measurement cost of individual feature variables in optimal stopping, enabling better scheduling of the acquisition sequences. However, their approach still focuses ona static terminal status and is unsuitable for general purpose active sensing where the outcome of interest varies over time.

In this paper, we focus on more general healthcare scenarios where timely diagnosis and intervention need to be achieved through active sensing under certain budget. The budget serves as the source of cost pressure and prioritize safe and low-cost measurements in the sensing process. For instance, the management of Alzheimer's disease (AD) requires continuous monitoring and staging of disease progression, which is crucial for early detection and intervention of high-risk patients [8]. However, certain measurements involved in diagnosing AD are notably expensive. A typical example is positron emission tomography (PET) scans. PET scans provide precise information about AD-associated proteins in the brain and are widely used for monitoring AD progression [3]. Nevertheless, the high accuracy of PET scans compared to cognitive tests is accompanied by greater financial costs as well as potentially harmful exposure to radiation [7]. A desirable active sensing policy should adaptively balance measurement costs and diagnostic accuracy at different disease stages to provide timely diagnosis and intervention under cost pressure. Such property is not only useful in healthcare but also other applications such as delay-sensitive wireless communication [14].

Another crucial but relatively unexplored aspect of active sensing is the consideration of _tail risk_. To illustrate this concept, let's examine the prediction of AD progression. In active sensing, the trade-off between diagnosis error and acquisition cost is usually achieved through the minimization of their weighted sum [9, 23, 24]. As depicted in Figure 1(b), the weighted objective (\(Q^{\pi}(\bm{X})\)) typically exhibits a long-tailed distribution with a notable concentration of _high-value_ patients at the tail side. We consider these patients to be of higher value in the sensing task for two main reasons. Firstly, accurate diagnosis of many high-risk patients heavily relies on biomarkers obtained through expensive PET scans such as fluorodeoxyglucose (FDG) PET and florbetapir (AV45) PET [7, 10]. A vanilla sensing policy optimized for the entire population would easily fail to arrange enough PET scans for these high-risk patients due to the high costs, leading to degraded sensing performance (the long tail as shown in Figure 1(b)). In the meantime, the majority of patients tend to remain in stable conditions. Their disease stages can usually be diagnosed even with very sparse acquisitions over time. A biased sensing policy may overemphasize the importance of high-risk patients and order frequent PET scans for all patients to avoid false negatives in its diagnosis, which contributes to the long-tailed distribution from another perspective. To avoid such situations, an active sensing strategy must be _risk-averse_ such that it can effectively cater to the needs of both the majority of stable patients and the high-risk subgroups requiring frequent follow-ups, ensuring timely and accurate detection of disease progression under cost pressure.

**Contributions.** In this paper, we propose a novel continuous-time active sensing approach to address the aforementioned desiderata. Our sensing approach adaptively balances measurement costs and diagnostic accuracy by factorizing sensing policy into two sub-policies: _acquisition scheduler_ which

Figure 1: Overview of active sensing strategies. Active sensing in the dynamical setting (a) involves two distinct decisions on: what features to measure; and when to perform next acquisition. The trade-off between diagnosis accuracy and acquisition cost usually yields a (b) long-tailed sensing deficiency distribution. The sensing deficiency \(Q^{\pi}(\bm{X})\) accumulates the weighted sum of the diagnosis error and measurement cost over time. Higher sensing deficiency values indicate failures in the sensing process.

determines _when to conduct feature acquisitions_ (i.e., the time interval between adjacent follow-ups), and _feature selector_ that decides _which observations to make_. Furthermore, our sensing approach adopts the _risk-averse_ strategy and focuses on the trade-offs happen at the long tail, where accurate diagnosis becomes particularly crucial. By dynamically identifying patient trajectories located at the tail side, we optimize our sensing strategy using the conditional value-at-risk (CVaR) loss to provide guarantees on timely and accurate prediction for high-risk patients while avoiding biased decisions for patients with stable conditions.

We validate our approach through experiments on synthetic and real-world datasets, where the sensing performance on high-risk patients is significantly impacted by their relatively small cohort size. With the consideration of acquisition costs, our method is able to adaptively identify the patient subgroups at the long tail of sensing deficiency distribution and achieves improved diagnostic performance compared to the state-of-the-art active sensing benchmarks.

## 2 Active Sensing in Practice

In this section, with a focus on healthcare applications, we introduce a new active sensing approach, which we refer to as _Risk-averse Active Sensing_ (RAS), to solve the decision-making problem of when to conduct feature acquisitions and which features to observe considering prediction accuracy, timeliness, and cost pressure. We will start by providing a general framework for active sensing problems.

### Preliminaries: Active Sensing

Notation.Let \(\bm{x}(t)\in\mathbb{R}^{d}\) be the characteristic of a patient at time \(t\in[0,T]\) which is a collection of \(d\) (costly) observable time-varying covariates about an underlying disease progression of our interest.1\(\bm{X}_{t}=\big{\{}\bm{x}(\tau)|\tau\in[0,t]\big{\}}\) represents a temporal trajectory of observable features until time \(t\leq T\). For ease of description, we will occasionally abuse the notation and use \(\bm{X}\) to denote the entire patient trajectory \(\bm{X}_{T}\). Let \(y_{t}\in\mathcal{Y}\) be the outcome of our interest which represents the underlying health status of a patient (e.g., diagnosis, adverse outcome, etc.) at time \(t\). We assume the joint distribution \(p(\bm{X}_{t},y_{t})\) from which the trajectory \(\bm{X}_{t}\) and the outcome \(y_{t}\) are drawn. Hereafter, we focus our description on \(K\)-class classification tasks, i.e., \(\mathcal{Y}=\{1,2,\ldots,K\}\).

Footnote 1: In general, observation of a patient’s underlying disease progression ends at some time \(T>0\) due to death or lost to follow-up.

We denote an active sensing policy as \(\pi\). In the sequential outcome prediction task, for each patient \(\bm{X}\), policy \(\pi\) arranges a set of follow-up times \(\{t_{1},t_{2},\ldots,t_{I}\}\subset[0,T]\) and determines the subsets of features \(\bm{m}_{i}\) to be measured at each time step \(t_{i},\) where \(\bm{m}_{i}\) is a binary vector. The collected observations of patient trajectory \(\bm{X}\) until time \(t\) form a sensing history \(X_{t}^{\pi}\). It is worth noting that \(X_{t}^{\pi}\) is piece-wise constant over time as it is only updated at discrete follow-ups. At each step, the sensing policy \(\pi\) shall generate a diagnosis of the patient by approximating the conditional \(p(y_{t}|X_{t}^{\pi})\).

Dataset.For the active sensing task, we assume access to an _observational dataset_\(\mathcal{D}\) containing electrical health records (EHRs) of \(N\) patients, i.e., \(\mathcal{D}=\{X_{n}=(t_{i},\bm{x}(t_{i}),y_{t_{i}})_{i=1}^{L}\}_{n=1}^{N}\). The record of each patient consists observations collected from \(L\) follow-ups. It is worth noting that the record \(X_{n}\in\mathcal{D}\) is equivalent to a sensing history.

Following the convention in active sensing literature [9; 24], we consider the _cost of measurement_ on each time-varying observable feature to be time-invariant and thus can be represented as a _cost vector_\(\bm{c}\), where the \(j\)-th element, denoted as \((\bm{c})_{j}\), is the cost for measuring the \(j\)-th time-varying feature in vector \(\bm{x}\in\bm{X}\). The central goal of active sensing is then to balance between the outcome estimation accuracy based on sensing history \(X_{t}^{\pi}\) and the accumulated acquisition costs over time.

Challenges.In practice, there are two major challenges that confound the active sensing problem:

1. **Adaptive follow-up intervals.** Most existing active sensing policies assume a constant follow-up interval. However, in real-world healthcare settings, the desirable acquisition intervals usually change along with disease progression. Sensing policy with constant decision intervals creates redundant follow-ups and may induce extra costs for feature acquisition.

2. **Failures at the long tail.** Vanilla active sensing policies are optimized for the entire population. For diseases like AD, the imbalanced proportion of high-risk and stable patients may lead to over-conservative sensing policies that make no observations. Similarly, sensing policies targeted on high-risk patients may generate biased decision with higher acquisition costs for patients in stable conditions. Both situations lead to failures for samples at the long tail of sensing deficiency distribution.

In the following, we first introduce a generalized sensing policy \(\pi\) with the decomposition of decisions on follow-up interval and feature selection. Then, we formulate the active sensing task into a risk-averse optimization problem, the solution to which addresses the second challenge.

### Decomposing the Active Sensing Policy

Vast research in active sensing has assumed a constant follow-up interval between two consecutive patient visits and has primarily focused on the sequential feature selection problem, determining which feature variables to measure [6; 24]. However, considering the dynamic and complex nature of disease progression, sensing policies with dynamic follow-up intervals are broadly desired in real-world applications. In this paper, we propose a generalized formulation of the sensing policy, i.e., \(\pi=(\pi_{m},\pi_{\Delta})\), where the _acquisition scheduler_\(\pi_{\Delta}\) determines the time for next follow-up, the _feature selector_\(\pi_{m}\) determines which features to measure during the subsequent visit.

Consider the \(i\)-th follow-up of patient \(\bm{X}\) at time \(t_{i}\). The next follow-up time \(t_{i+1}\) is firstly determined by acquisition scheduler \(\pi_{\Delta}\) as \(t_{i+1}=t_{i}+\Delta_{i},\Delta_{i}\sim\pi_{\Delta}(X^{\pi}_{t_{i}})\). Then, at time \(t_{i+1}\), the feature selector \(\pi_{m}\) determines which subset of patient covariates to measure using a binary vector \(\bm{m}_{i+1}\sim\pi_{m}(X^{\pi}_{t_{i}}),\bm{m}_{i+1}\in\{0,1\}^{d}\). The new observation is calculated as \(\bm{m}_{i+1}\odot\bm{x}(t_{i+1})\), where the \(j\)-th element

\[(\bm{m}_{i+1}\odot\bm{x}(t_{i+1}))_{j}=\left\{\begin{array}{cl}(\bm{x}(t_{i+ 1}))_{j}&\text{if }(\bm{m}_{i+1})_{j}=1,\\ *&\text{otherwise}.\end{array}\right.\]

### Towards Risk-Averse Sensing Policy

**Trade-off between accuracy and costs.** Consider the \(i\)-th follow-up at time \(t_{i}\). The feature selection is determined by \(\bm{m}_{i}\sim\pi_{m}(\tilde{X}^{\pi}_{t_{i-1}})\). The acquisition cost is evaluated as \(r_{m}(\bm{m}_{i})=\bm{c}^{\top}\bm{m}_{i}\), where \(\bm{c}\) is the cost vector. The updated sensing history \(X^{\pi}_{t_{i}}=X^{\pi}_{t_{i-1}}\cup\{\bm{m}_{i}\odot\bm{x}(t_{i})\}\) is then used to determine the acquisition interval \(t_{i+1}-t_{i}=\Delta_{i}\sim\pi_{\Delta}(X^{\pi}_{t})\). Assume a diagnosis error function \(r_{y}(X^{\pi}_{t_{i}},\Delta_{i})\) that evaluates the mismatch between \(p(y_{t}|\bm{X}_{t})\) and \(p(y_{t}|X^{\pi}_{t})\) during the interval \(\Delta_{i}\). The trade-off between diagnosis accuracy and measurement costs of the \(i\)-th interval is defined as the following reward signal

\[r(X^{\pi}_{t_{i}},\bm{m}_{i},\Delta_{i})=r_{m}(\bm{m}_{i})+\lambda r_{y}(X^{ \pi}_{t_{i}},\Delta_{i})\] (1)

where \(\lambda>0\) is a coefficient chosen to balance the two terms.

**Sensing deficiency.** We define the sensing deficiency of policy \(\pi\) along a patient trajectory \(\bm{X}\) as the _expected cumulative reward_ as shown in (2).

\[Q^{\pi}(\bm{X})=\mathbb{E}_{t_{1},t_{2},\ldots,t_{I},\bm{m}_{1},\bm{m}_{2}, \ldots,\bm{m}_{I}\sim\pi}\left[\sum_{i=1}^{I}\gamma^{i-1}r(X^{\pi}_{t_{i}},\bm {m}_{i},\Delta_{i})\right],\] (2)

where discount factor \(\gamma\in(0,1]\) is used to tackle long trajectories. In this paper, we take \(\gamma=0.99\).

**Trajectories at the long tail.** As mentioned earlier, one major challenge for active sensing in practice is the potential sensing failures for patients at the tail of sensing deficiency distribution. Since the distribution tail of \(Q^{\pi}(\bm{X})\) changes along with the updates of policy \(\pi\), in this paper, we propose to optimize the sensing policy over a dynamic subset of patient trajectories located at the long tail. These tail trajectories can be identified through the upper \(\alpha\) quantile \(\rho_{\alpha}\), where \(\mathbb{E}_{\bm{X}}[\mathbb{I}(Q^{\pi}(\bm{X})\geq\rho_{\alpha})]=\alpha\). Assume the marginal trajectory distribution \(p(\bm{X})\), we denote with \(S^{\pi}_{\alpha}=\{\bm{X}|\bm{X}\sim p(\bm{X}),Q^{\pi}(\bm{X})\geq\rho_{\alpha}\}\) the set of tail trajectories under policy \(\pi\).

Conditioned on the quantile factor \(\alpha\in[0,1]\), the sensing deficiency of trajectories in \(S^{\pi}_{\alpha}\) is considered as the value at risk that needs to be improved. Based on the evaluation of \(\mathrm{CVaR}\) in sensing deficiency distribution, we formulate our active sensing task as a risk-averse optimization problem.

\[\underset{\pi}{\text{minimize}}\ \ \mathrm{CVaR}_{\alpha}\triangleq\mathbb{E}_{\bm{X} \in S^{\pi}_{\alpha}}[Q^{\pi}(\bm{X})].\] (3)

[MISSING_PAGE_FAIL:5]

extend the acquisition cost function \(r_{m}\) as \(r_{m}(\bm{m})=\bm{c}^{\top}\bm{m}\cdot\mathbb{I}(\bm{m}\neq\bm{0})+\nu\cdot \mathbb{I}(\bm{m}=\bm{0})\), where \(\nu\geq\bm{c}^{\top}\bm{1}\) penalizes the invalid visits scheduled by the sensing policy \(\pi\).

**Network structure.** Similar to the predictor \(f_{P}\), the neural CDE and interpolator \(\mathcal{I}\) are also employed in our proposed policy network \(\pi\) to construct an embedding of the discrete sensing history \(X_{t}^{\pi}\). As illustrated in Figure 2, our sensing policy \(\pi\) (parameterized by \(\theta\)) is constructed with the following three networks:

* The _history encoder_, \(f_{H}\), is a neural CDE encoder that maps sensing history \(X_{t}^{\pi}\) into a summary vector \(\bm{h}(\tau)=f_{H}(\tau,\mathcal{I}(X_{t}^{\pi}))\).
* The _feature selector_, \(f_{M}\), is a sub-policy network that governs the decision about which subset of patient features to be measured. In a follow-up, \(f_{M}\) takes a summary vector \(\bm{h}(\tau)\) as input and outputs a vector \(\bm{p}(\tau)=f_{M}(\bm{h}(\tau))\) that governs the element-wise Bernoulli distribution to generate a binary feature selection vector \(\bm{m}\), i.e., \(\bm{m}_{\tau^{\prime}}\sim\mathrm{Bern}(\bm{p}(\tau))\) for next follow-up at \(\tau^{\prime}\).
* The _acquisition scheduler_, \(f_{\Delta}\), is a sub-policy network that determines the time for next follow-up. Particularly, after each follow-up, it takes the summary vector \(\bm{h}(\tau)\) as input and generates two scalar parameters \(\alpha,\beta=f_{\Delta}(\bm{h}(\tau))\) of a Beta distribution, i.e., \(\xi\sim\mathrm{Beta}(\alpha,\beta)\). The realizations of the Beta distribution are then used to generate the time interval \(\Delta\in[\Delta^{\min},\Delta^{\max}]\) via \(\Delta=\Delta^{\min}+(\Delta^{\max}-\Delta^{\min})\cdot\xi\), where \(\Delta^{\min}\) and \(\Delta^{\max}\) are the minimum and maximum allowed intervals between two follow-ups, respectively. We set \(\alpha,\beta\geq 1\) to ensure that the time interval follows a unimodal distribution.

**Policy gradient.** Since the diagnostic error function in (5) is non-differentiable, to _back-propagate_ gradients from the objective \(\mathrm{CVaR}_{\alpha}=\mathbb{E}_{\bm{X}\in S_{\alpha}^{\pi}}[Q^{\pi}(\bm{X})]\) through the active sensing process, we apply the advantage-based approach [21; 22] to estimate the policy gradient of \(\pi\) as the following:

\[\nabla_{\theta}\mathrm{CVaR}_{\alpha}=\mathbb{E}_{\pi}\left[\frac{1}{|S_{ \alpha}^{\pi}|}\sum_{s=1}^{|S_{\alpha}^{\pi}|}\sum_{i=1}^{I}\left(R(X_{s,t_{i} }^{\pi})-R_{C}(X_{s,t_{i}}^{\pi})\right)\cdot\nabla_{\theta}\log\pi(\bm{m}_{ s,i},\Delta_{s,i})\right],\] (6)

where \(\theta\) is the collection of learnable parameters of policy \(\pi\). Given the \(s\)-th patient in \(S_{\alpha}^{\pi}\), the cumulative reward \(R(X_{s,t_{i}}^{\pi})=\sum_{t=i}^{I}\gamma^{-i}r(X_{s,t_{i}}^{\pi},\bm{m}_{t}, \Delta_{t})\) is an empirical evaluation of the sensing deficiency in (2) for the corresponding sub-trajectory in horizon \([t_{i},T]\). Following the actor-critic framework, we utilize a baseline sensing deficiency estimation \(R_{C}(X_{t_{i}}^{\pi})\) in (6) to reduce the variance of policy gradient estimation [21].

The baseline estimation \(R_{C}(X_{t}^{\pi})\) is obtained from a _critic_ network \(f_{C}\) (parameterized by \(\psi\)). Taking the embedding \(\bm{h}(t)=f_{H}(t,\mathcal{I}(X_{t}^{\pi}))\) from the history encoder \(f_{H}\) as input. The critic \(f_{C}\) is trained

Figure 2: The model structure of RAS.

to minimize the following mean squared error (MSE) loss.

\[\mathcal{L}_{C}=\mathbb{E}_{\pi}\left[\frac{1}{|S_{\alpha}^{\pi}|}\sum_{s=1}^{|S_{ \alpha}^{\pi}|}\sum_{i=0}^{I}(R(X_{n,t_{i}}^{\pi})-R_{C}(X_{n,t_{i}}^{\pi}))^{2} \right].\] (7)

``` Input: dataset \(\mathcal{D}\), predictor \(f_{P}\), quantile \(\alpha\), learning rates \(\mathrm{lr}_{\pi}\), \(\mathrm{lr}_{c}\). Initialize: learnable parameters \(\theta\) (of \(\pi\)), \(\psi\) (of \(f_{C}\)). for\(k=1\)to\(K\)do if\(k-1\mod M=0\)then \(\rho_{\alpha}\leftarrow\texttt{quantile}(Q^{\pi},\mathcal{D},\alpha)\) \(S_{\alpha}^{\pi}\leftarrow\{X_{n}|Q^{\pi}(X_{n})\geq\rho_{\alpha},X_{n}\in \mathcal{D}\}\) endif \(\theta\leftarrow\theta-\mathrm{lr}_{\pi}\nabla_{\theta}\mathrm{CVaR}_{\alpha}(S_ {\alpha}^{\pi})\) \(\psi\leftarrow\psi-\mathrm{lr}_{c}\nabla_{\psi}\mathcal{L}_{C}(S_{\alpha}^{ \pi})\) endfor Output: risk-averse policy \(\pi\) with parameters \(\theta\) ```

**Algorithm 1** Risk-averse active sensing

Given the baseline predictor \(f_{P}\), the sensing deficiency \(Q^{\pi}(\bm{X})\) of policy \(\pi\) is optimized over the subset \(S_{\alpha}^{\pi}\) of tail trajectories as illustrated in Algorithm 1. Specifically, parameters \(\theta\) of policy \(\pi\) are updated based on the policy gradient \(\nabla_{\theta}\mathrm{CVaR}_{\alpha}\) in (6), and the critic network \(f_{C}\) is trained with loss function \(\mathcal{L}_{C}\) to provide appropriate baselines for the sensing policy \(\pi\). The collection of patient trajectories in \(S_{\alpha}^{\pi}\) is iteratively updated every \(M\) training epochs such that it can stay informative about the sensing deficiency distribution. Note that the learning rates of critic \(f_{C}\) and policy \(\pi\) are intentionally set to be different to facilitate convergence.

## 4 Related Work

We first provide in Table 1 an overview of the distinction between our proposed approach and other related works in active sensing literature. A special class of active sensing task is formulated under the optimal stopping framework. With a set of explicitly defined terminal states, optimal stopping approaches focuses on achieving confident diagnosis of these states while minimizing the time span of the sensing process [1; 6; 15]. The consideration of information acquisition cost was introduced recently into optimal stopping in [9]. Similarly, the trade-off between diagnosis accuracy and costly measurement has been studied in the more general sequential prediction tasks [23; 24]. In this paper, we propose a novel sensing policy decomposition and extend the active sensing analysis into continuous-time settings with adaptive follow-up intervals and highly sparse sensing histories.

Temporal feature selection.As pointed out by Yoon et al. [24], active sensing can be considered as the temporal generalization of feature selection tasks [13; 19]. In each decision step, the sensing policy is expected to select a set of important feature variables to make the prediction. The resulting sensing history marks critical time points and key feature variables for accurate outcome estimation. However, existing active sensing literature primarily focuses on the feature selection perspective and usually assumes a fixed follow-up interval for every patient trajectory [1; 24]. A fixed acquisition

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline Method & Problem Class & History Embedding & Acquisition Interval & (1) & (2) & (3) & (4) \\ \hline Ahmad and Yu [1] & Optimal Stopping & Bayesian Update & Fixed \((\Delta=\bar{\Delta})\) & ✗ & ✗ & ✗ \\ Jarrett and van der Scharar [9] & Optimal Stopping & Bayesian Update & Fixed \((\Delta=\bar{\Delta})\) & ✗ & ✓ & ✗ & ✗ \\ Yoon et al. [23; 24] & Sequential Prediction & RNN & Fixed \((\Delta=\bar{\Delta})\) & ✗ & ✓ & ✗ & ✗ \\ RAS (Ours) & Sequential Prediction & Neural CDE & Adaptive \((\Delta\sim\pi_{\Delta})\) & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of active sensing approaches. Typical active sensing approaches are compared based on the problem formulation, sensing history representation, the modelling of acquisition intervals as well as the following key concerns highlighted in this paper: (1) supports continuous-time patient trajectory. (2) considers cost pressure on features acquisition. (3) achieves decomposition of feature selection and acquisition scheduling. (4) provides worst-case performance guarantees with a risk-averse policy.

interval can lead to delayed diagnosis or extra costs due to redundant feature measurements. As mentioned earlier, a desirable sensing policy shall achieve flexible follow-up intervals for each patient such that the shifts in underlying disease stages can be properly addressed.

Risk aversion.Conventional active sensing strategies typically consider the diagnosis accuracy to be equally important for all trajectories in a dataset. However, in many healthcare applications, there are needs to prioritize smaller subgroup of high-risk patients for whom timely detection of adverse outcomes is particularly valuable. As a central component of our proposed method, the concept of CVaR [20] enables the selective optimization for patient trajectories in the long tail of sensing deficiency distribution. Optimizing sensing policies over the corresponding tail set \(S_{\alpha}^{\tau}\) yields a risk-averse active sensing strategy \(\pi^{*}\) that achieves guaranteed diagnosis accuracy for high-risk patients. While risk-aware decision-making has been extensively explored in the field of RL [4; 25], to the best of our knowledge, our proposed method is the first risk-averse active sensing algorithm for adaptive feature acquisition in continuous-time settings.

## 5 Experiment

In the experiments, we evaluate the effectiveness of our proposed risk-averse active sensing approach RAS on both synthetic and real-world healthcare datasets.

Synthetic dataset.We construct a synthetic dataset \(\mathcal{D}_{S}\) of \(N=2000\) samples. Every sample \(X\) contains \(L=20\) discrete observations in \(t\in[0,2]\). Each observation yields a feature vector \(\bm{x}=[x_{1},x_{2},x_{3},x_{4}]^{\top}\). We set \(x_{1}(t)=\min(1,(e^{w(t-\tau)}-w(t-\tau)-1))\mathbb{I}(t\geq\tau)\), where \(w\) follows a Gaussian mixture distribution of \(0.8\cdot\mathcal{N}(0.3,0.1^{2})+0.2\cdot\mathcal{N}(0.8,0.1^{2}),\tau\sim \mathrm{Exp}(1.0)\). As a proxy of \(x_{1}\), we set \(x_{2}(t)=w(t-\tau)\mathbb{I}(t\geq\tau)\). Variables \(x_{3}(t)=\sin(3t+\varrho)\) and \(x_{4}(t)=\cos(3t+\varrho)\), where \(\varrho\sim\mathcal{N}(0,1^{2})\). The outcome \(y\in\{0,1\}\) follows a Bernoulli distribution \(y_{t}\sim\mathrm{Bern}(p)\), and the likelihood for \(y_{t}=1\) is calculated as \(p=e^{-3|1-x_{1}|^{2}}\). The cost vector is set to be \(\bm{c}=[1.0,0.1,1.0,1.0]^{\top}\), which allows \(x_{2}\) to be measured as a cheap proxy of \(x_{1}\) for outcome prediction.

ADNI dataset.The Alzheimer's Disease Neuroimaging Initiative2 (ADNI) dataset includes records on AD progression of \(N=1002\) patients with regular follow-ups every six months. We consider the active sensing task to predict patient outcomes in the initial \(L=12\) follow-ups with four biomarkers from PET (FDG and AV45) and MRI (Hippocampus and Entorhinal) imaging, respectively. Based on the disease staging guideline [16; 17], target outcome \(y_{t}\in\{\text{Normal},\text{mild cognitive impairment (MCI)},\text{AD}\}\) at each time point is determined via the corresponding Clinical Dementia Rating scale Sum of Boxes (CDR-SB) score with cutoff thresholds at 0.5 and 4.5. To reflect the higher cost and harmful radiation exposure in PET scan, measurement costs for PET and MRI biomarkers are set to be 1.0 and 0.5, respectively.

Footnote 2: https://adni.loni.usc.edu

Experiment setup.We first fit the outcome estimator \(f_{P}\) on each dataset with 64/16/20 train/validation/test splits and find the optimal estimator with the best prediction accuracy given sparse observation of test samples as inputs. Then, we freeze the parameters of the optimal \(f_{P}\) and evaluate the sensing performance of RAS and other baselines on the corresponding data split. We consider four baselines in the experiments: **FO**) predictor \(f_{P}\) with dense sensing history; **ASAC**) [24]; **NLL**) adaptive sensing with negative log-likelihood (NLL) as diagnosis error function [12]; **AS**) RAS(\(\alpha=1.0\)) with constant acquisition interval. Further details of the experiment setup and parameter selection results can be found in the Appendix.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Method & ROC & PRC & Cost & \(d_{\delta=0.3}\) & \(d_{\delta=0.5}\) & \(d_{\delta=0.7}\) \\ \hline \hline FO & **0.680\(\pm\)0.000** & **0.655\(\pm\)0.000\({}^{\ddagger}\)** & \(31.000\pm\)0.000 & 0.502\(\pm\)0.000 & 0.349\(\pm\)0.000 & 0.285\(\pm\)0.000 \\ ASAC & 0.605\(\pm\)0.096 & 0.559\(\pm\)0.080 & **0.460\(\pm\)1.078\({}^{\ddagger}\)** & 1.099\(\pm\)0.664 & 1.066\(\pm\)0.699 & 1.052\(\pm\)0.641 \\ AS & 0.671\(\pm\)0.001 & 0.614\(\pm\)0.001 & 4.501\(\pm\)0.497 & 0.577\(\pm\)0.029 & 0.522\(\pm\)0.012 & 0.479\(\pm\)0.015 \\ NLL & 0.636\(\pm\)0.023 & 0.588\(\pm\)0.016 & 2.968\(\pm\)0.774 & 0.993\(\pm\)0.131 & 0.974\(\pm\)0.141 & 0.975\(\pm\)0.147 \\ RAS (OURS) & 0.680\(\pm\)0.003 & 0.647\(\pm\)0.006 & 6.077\(\pm\)0.953 & **0.325\(\pm\)0.084\({}^{\ddagger}\)** & **0.264\(\pm\)0.086\({}^{\ddagger}\)** & **0.246\(\pm\)0.071\({}^{\dagger}\)** \\ \hline \hline \end{tabular}

* The 95% confidence interval (CI) is evaluated with five different random seeds. Best result in each column are highlighted in **bold**. The marker \({}^{\ddagger}\) indicates p-value \(p<0.05\).

\end{table}
Table 2: Benchmark of sensing performance on the synthetic dataset \(\mathcal{D}_{S}\).

[MISSING_PAGE_FAIL:9]

(\(d_{\delta=0.5}\)) and average acquisition cost cannot be simultaneously improved by swapping parameters with other policies. Benefitted from the risk-averse training strategy, most sensing policies obtained via RAS are centered around the knee point of the Pareto front, which helps to explain the outstanding cost efficiency of RAS as reported in Table 2.

Reshaping the sensing deficiency distribution.To illustrate the effectiveness of the risk-averse objective in (3), we compare the empirical distribution of sensing deficiency \(Q^{\pi}(\bm{X})\) of RAS with the ablations of risk-neutral sensing (\(\alpha=1.0\)) and AS baseline (\(\alpha=1.0\), constant acquisition interval \(\Delta=1.0\)) on the synthetic dataset. All three models are trained with the same trade-off coefficient \(\lambda=300\). As illustrated in Figure 3(b), RAS is able to effectively optimize the sensing performance for trajectories in the tail set \(S_{\alpha}^{\pi}\) and reduces the upper \(\alpha\) quantile of \(Q^{\pi}(\bm{X})\) to \(\rho_{\alpha=0.1}=10.40\). Factor \(\alpha=1.0\) completely disables the risk-aversion training strategy in Algorithm 1. Thereby, a clear increase of sensing deficiency (quantile \(\rho_{\alpha=0.1}\) grows from 10.40 to 20.01) is observed with the risk-neutral ablation of RAS. Similarly, without adaptive scheduling of acquisition intervals and risk-averse optimization strategies, the AS baseline illustrates the failure of conventional active sensing paradigms at the long tail of \(Q^{\pi}(\bm{X})\) distribution.

## 6 Conclusion

In this paper, we introduce a novel risk-averse active sensing approach RAS to address the challenging continuous-time active sensing problem. Through effective decomposition of decisions on feature selection and acquisition intervals, our approach offers valuable insights on both feature importance and timeliness of patient follow-ups. The novel risk-aversion training strategy in RAS enables the prioritization of high-value patients at the long tail of sensing deficiency distribution and provides guarantees on diagnosis accuracy for worst-case scenarios. The effectiveness of our method is evaluated through experiments on synthetic and real-world healthcare datasets, where RAS outperforms all baseline active sensing approaches and achieves accurate and timely outcome diagnosis while maintaining reasonable costs in feature acquisition.

## Broader Impacts

The development of an advanced active sensing strategy is essential for precision medicine. Appropriate sensing policies can help clinicians to make tailored follow-up schedules for their patients and effectively utilize the costly and potentially harmful measurements on some important patient characteristics under certain levels of cost pressure. We note that the reduction in acquisition cost is usually associated with degraded diagnosis accuracy and delayed detection of some adverse clinical events, especially for high-risk patients that could benefit from more frequent follow-ups on disease progression. In this paper, we attempt to address this challenge through the risk-averse policy learning strategy in our proposed active sensing approach RAS and achieve improved sensing performance (accuracy and timeliness) on the entire population. Nevertheless, practical applications of active sensing approaches would require careful audits and assessments by human experts to avoid potential negative impacts. The sensing decisions from RAS are only suggestions optimized based on observational data and should not be directly applied without the evaluation of clinicians.

## Acknowledgements

Yuchao Qin was supported by the Cyctic Fibrosis Trust. Changhee Lee was supported by the IITP grant funded by the Korea government (MSIT) (No.2021-0-01341, AI Graduate School Program, CAU). This work was supported by Azure sponsorship credits granted by Microsoft's AI for Good Research Lab.

## References

* [1] S. Ahmad and A. J. Yu. Active sensing as bayes-optimal sequential decision-making. In _Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence_, pages 12-21, 2013.

* [2] A. M. Alaa and M. van der Schaar. Balancing suspense and surprise: Timely decision making with endogenous information acquisition. _Advances in Neural Information Processing Systems_, 29, 2016.
* [3] W. Bao, F. Xie, C. Zuo, Y. Guan, and Y. H. Huang. Pet neuroimaging of alzheimer's disease: radiotracers and their utility in clinical research. _Frontiers in Aging Neuroscience_, 13:624330, 2021.
* [4] L. Brunke, M. Greeff, A. W. Hall, Z. Yuan, S. Zhou, J. Panerati, and A. P. Schoellig. Safe learning in robotics: From learning-based control to safe reinforcement learning. _Annual Review of Control, Robotics, and Autonomous Systems_, 5:411-444, 2022.
* [5] H. Chernoff. Sequential design of experiments. _The Annals of Mathematical Statistics_, 30(3):755-770, 1959.
* [6] S. Dayanik and A. J. Yu. Reward-rate maximization in sequential identification under a stochastic deadline. _SIAM Journal on Control and Optimization_, 51(4):2922-2948, 2013.
* [7] E. Dolgin. Alzheimer's disease is getting easier to spot. _Nature_, 559(7715):S10-S10, 2018.
* [8] B. Dubois, A. Padovani, P. Scheltens, A. Rossi, and G. Dell'Agnello. Timely diagnosis for alzheimer's disease: a literature review on benefits and challenges. _Journal of Alzheimer's disease_, 49(3):617-631, 2016.
* [9] D. Jarrett and M. van der Schaar. Inverse active sensing: Modeling and understanding timely decision-making. In _International Conference on Machine Learning_, 2020.
* [10] J. Jing, F. Zhang, L. Zhao, J. Xie, J. Chen, R. Zhong, Y. Zhang, and C. Dong. Correlation between brain 18f-av45 and 18f-fdg pet distribution characteristics and cognitive function in patients with mild and moderate alzheimer's disease. _Journal of Alzheimer's Disease_, 79(3):1317-1325, 2021.
* [11] P. Kidger, J. Morrill, J. Foster, and T. Lyons. Neural controlled differential equations for irregular time series. _Advances in Neural Information Processing Systems_, 33:6696-6707, 2020.
* [12] J. Kossen, C. Cangea, E. Vertes, A. Jaegle, V. Patraucean, I. Ktena, N. Tomasev, and D. Belgrave. Active acquisition for multimodal temporal data: A challenging decision-making task. _arXiv preprint arXiv:2211.05039_, 2022.
* [13] S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions. _Advances in neural information processing systems_, 30, 2017.
* [14] N. Mastronarde and M. van der Schaar. Joint physical-layer and system-level power management for delay-sensitive wireless communications. _IEEE transactions on mobile computing_, 12(4):694-709, 2012.
* [15] M. Naghshvar and T. Javidi. Active sequential hypothesis testing. _The Annals of Statistics_, 41(6):2703-2738, 2013.
* [16] S. E. O'Bryant, S. C. Waring, C. M. Cullum, J. Hall, L. Lacritz, P. J. Massman, P. J. Lupo, J. S. Reisch, R. Doody, T. A. R. Consortium, et al. Staging dementia using clinical dementia rating scale sum of boxes scores: a texas alzheimer's research consortium study. _Archives of neurology_, 65(8):1091-1095, 2008.
* [17] S. E. O'Bryant, L. H. Lacritz, J. Hall, S. C. Waring, W. Chan, Z. G. Khodr, P. J. Massman, V. Hobson, and C. M. Cullum. Validation of the new interpretive guidelines for the clinical dementia rating scale sum of boxes score in the national alzheimer's coordinating center database. _Archives of neurology_, 67(6):746-749, 2010.
* [18] L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta. Robust adversarial reinforcement learning. In _International Conference on Machine Learning_, pages 2817-2826. PMLR, 2017.
* [19] M. T. Ribeiro, S. Singh, and C. Guestrin. " why should i trust you?" explaining the predictions of any classifier. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1135-1144, 2016.

* [20] R. T. Rockafellar and S. Uryasev. Conditional value-at-risk for general loss distributions. _Journal of banking & finance_, 26(7):1443-1471, 2002.
* [21] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. _arXiv preprint arXiv:1506.02438_, 2015.
* [22] R. S. Sutton and A. G. Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* [23] J. Yoon, W. R. Zame, and M. van der Schaar. Deep sensing: Active sensing using multi-directional recurrent neural networks. In _International Conference on Learning Representations_, 2018.
* [24] J. Yoon, J. Jordon, and M. van der Schaar. Asac: Active sensing using actor-critic models. In _Machine Learning for Healthcare Conference_, pages 451-473. PMLR, 2019.
* [25] S. Zhang, B. Liu, and S. Whiteson. Mean-variance policy iteration for risk-averse reinforcement learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 10905-10913, 2021.

## Appendix

### Notation table

A summary of major notations used in this paper is provided below.

* \(\bm{x}\): Feature variable of the patient.
* \(y\): Patient's outcome.
* \(\bm{X}\): The continuous-time patient trajectory.
* \(X_{t}^{\pi}\): The sensing history at time \(t\).
* \(\pi\): Sensing policy.
* \(\pi_{m}\): Feature selector.
* \(\pi_{\Delta}\): Acquisition scheduler.
* \(\bm{m}\):Feature selection mask vector.
* \(Q^{\pi}(\bm{X})\): Sensing deficiency.
* \(\mathrm{CVaR}\): Conditional value-at-risk.
* \(\alpha\): Tail risk quantile.
* \(T\): End time of observation.
* \(I\): Number of observations in sensing history.
* \(\mathcal{I}\): Interpolator.

### Limitation

The risk-averse policy learning strategy proposed in this paper relies on the correct identification of tail-risk patients. Data quality of the EHR datasets has direct impact on our method. Low quality dataset would lead to poor diagnosis accuracy of the baseline predictor \(f_{P}\) which compromises the validity and usefulness of RAS in practice. In the meantime, in RAS, patients located at the long tail of sensing deficiency distribution are selected based on empirical evaluations of \(Q^{\pi}(\bm{X})\). The selection results could be biased due to potential over-fitting of sensing policies on the training set. To tackle this issue, an "honest" and unbiased sensing performance evaluation on the validation set is necessary. However, the inference of acquisition costs from validation samples is beyond the main focus of this paper. We leave this challenge for our future work on risk-averse active sensing tasks.

Code Availability.The source code of RAS can be found in the two GitHub repositories listed below:

* The van der Schaar lab repo: https://github.com/vanderschaarlab/cvar_sensing
* The author's personal repo: https://github.com/yvchao/cvar_sensing

### Estimation of Time-Varying Outcome

A baseline outcome predictor \(f_{P}\) is required in (5) of the manuscript to evaluate the timeliness and accuracy of a sensing policy. Following the model-based approach in [24], we introduce a network \(f_{P}\) - which we refer to as the baseline _predictor_ - to estimate the unknown conditional distributions of patient outcomes. Specifically, we construct the predictor \(f_{P}\) with the neural CDE [11] model to flexibly handle sensing history with irregular observation intervals compared to the RNN-based counterparts such as [23, 24].

The predictor consists of four components, i.e., \(f_{P}=f_{Y}\circ f_{E}\circ\mathrm{CDE}\circ\mathcal{I}\). \(\mathcal{I}\) is a linear interpolator. \(\mathrm{CDE}\) is a neural CDE that takes the continuous trajectories as inputs. \(f_{E}\) is a multi-layer perceptron (MLP) that takes the CDE embedding as input. \(f_{Y}\) is an MLP with \(\mathrm{softmax}\) output layer for categorical outcomes. Utilizing the linear interpolator \(\mathcal{I}^{3}\), both the discrete EHR record \(X\in\mathcal{D}\) and sparse sensing history \(X_{t}^{\pi}\) can be converted into continuous trajectories.

The solution to the latent CDE provides embeddings of discrete time-series observations and the sensing history as \(\bm{z}_{n,t}=f_{E}(\mathrm{CDE}\circ\mathcal{I}(X_{n,t}))\) and \(\bm{z}_{n,t}^{\pi}=f_{E}(\mathrm{CDE}\circ\mathcal{I}(X_{n,t}^{\pi}))\), respectively. The corresponding conditional distributions are then estimated via the MLP as \(f_{P}(X_{n,t})=f_{Y}(\bm{z}_{n,t})\) and \(f_{P}(\bm{X}_{n,t}^{\pi})=f_{Y}(\bm{z}_{n,t}^{\pi})\), respectively.

The predictor \(f_{P}\) is trained to minimize the following loss:

\[\mathcal{L}_{P} =\!\frac{1}{NL}\!\sum_{n=1}^{N}\sum_{l=1}^{L}\ell(y_{n,t_{l}},f_{Y }(z_{n,t_{l}}))+\mu\ell(y_{n,t_{l}},f_{Y}(z_{n,t_{l}}^{\pi^{0}})),\] (8)

where \(z_{n,t_{l}}=f_{E}(\mathrm{CDE}\circ\mathcal{I}(X_{n,t_{l}}))\), \(z_{n,t_{l}}^{\pi^{0}}=f_{E}(\mathrm{CDE}\circ\mathcal{I}(X_{n,t_{l}}^{\pi^{0}}))\), and \(\ell\) is the \(K\)-class cross-entropy based on the observed outcome \(y_{n,t_{l}}\) and outputs of the predictor \(f_{P}\). Here, we introduce the second term as a regularization for each patient trajectory with random dropouts. More specifically, we introduce a random acquisition strategy \(\pi^{0}\) which randomly collects (sparse) discrete observations from the interpolated trajectories, i.e., \(\mathcal{I}(X_{n})\), and constructs auxiliary sensing histories with random dropouts, i.e., \(X_{n,t_{l}}^{\pi^{0}}\). The strategy \(\pi^{0}\) randomly select feature variables to keep in the observation history based on the drop rate. This regularization improves the generalization of the predictor by learning to estimate conditional distributions given randomly drawn sensing histories. Here, \(\mu\geq 0\) is a coefficient that controls the strength of regularization.

### Parameter Selection

Outcome predictor.We consider the drop rate \(p\) of the auxiliary observation strategy \(\pi^{0}\) as the hyperparameter of the outcome predictor. For both the synthetic dataset and ADNI dataset, we select the drop rate of auxiliary strategy \(\pi^{0}\) form the set of \(p\in\{0.0,0.3,0.5,0.7\}\) The optimal hyperparameter is selected based on the average accuracy of the outcome predictor on five test sets of random observations generated by an auxiliary observation strategy \(\pi^{0}\) with drop rate \(p=0.7\). The hyperparameter selection result is as follows:

* **Synthetic dataset**: drop rate \(p=0.7\)
* **ADNI dataset**: drop rate \(p=0.7\)

Sensing policy.Then, we perform hyperparameter selection for the active sensing approaches. Among the trained outcome predictors, we select the one with highest ROC score on its test set as the shared predictor and keep its parameters frozen. The training and test set corresponding to the selected optimal outcome predictor are used for the training and evaluation of the active sensing methods.

In our experiments, the hyperparameters of each active sensing method are selected based on the cost efficiency, i.e., ROC / Cost, obtained on the test set.

For the synthetic data considered in our manuscript, we set the minimum and maximum allowed acquisition intervals as \(\Delta^{\min}=0.2,\Delta^{\max}=1.0\), respectively. ASAC is not affected since it needs to directly work on original EHR records). The hyperparameters of each method are reported as follows:

* **ASAC**: coefficient for acquisition cost \(\mu=0.01\in\{0.1,0.01,0.005,0.001\}\).
* **AS**: acquisition interval \(\tilde{\Delta}=1.0\in\{0.2,0.5,1.0\}\), shared predictor \(f_{P}\) with RAS.
* **NLL**: coefficient for diagnostic error \(\lambda=100\in\{100,300\}\), shared predictor \(f_{P}\) with RAS.
* **RAS**: the coefficient for diagnostic error \(\lambda=300\in\{200,250,280,300,310,320,350,400\}\), discount factor \(\gamma=0.99\), tail-risk quantile \(\alpha=0.1\), penalty for invalid visits \(\nu=10\).

For the ADNI data considered in our manuscript, we set the minimum and maximum allowed acquisition intervals as \(\Delta^{\min}=0.5,\Delta^{\max}=1.5\), respectively. ASAC is not affected since it needs to directly work on original EHR records).

* **ASAC**: coefficient for acquisition cost \(\mu=0.1\in\{0.1,0.01,0.005,0.001\}\).
* **AS**: acquisition interval \(\tilde{\Delta}=1.5\in\{0.5,1.5\}\), shared predictor \(f_{P}\) with RAS.

* **NLL**: coefficient for diagnostic error \(\lambda=200\in\{100,200,300,400\}\), shared predictor \(f_{P}\) with RAS.
* **RAS**: the coefficient for diagnostic error \(\lambda=400\in\{200,250,300,350,400,450\}\), discount factor \(\gamma=0.99\), tail-risk quantile \(\alpha=0.1\), penalty for invalid visits \(\nu=10\).

All methods are trained with \(K=200\) iterations in the experiments. For RAS, we set the tail subset update interval \(M=10\) for Algorithm 1 in the manuscript.

**Adjustments in experiment results.** We have fixed some minor issues with model convergence when using CDE models. Specifically, the negative feedback is included in the CDE integration as follows:

\[\bm{z}(t)=\bm{z}(t_{0})+\int_{t_{0}}^{t}(f_{\phi}(\bm{z}(\tau))-10^{-3}\mathrm{ diag}(\bm{z}))\mathrm{d}\bm{u}(\tau),\text{for}\,\tau\in[t_{0},t],\] (9)

where \(\mathrm{diag}\) creates a diagonal matrix from the input vector \(\bm{z}\). Such negative feedback loop avoids the unlimited growth of CDE integration over time and helps with model convergence. This modification causes some minor changes in the experiment results. However, the updated experiment results are consistent with the previous versions and our conclusion is not affected by these changes.

**Additional results.** We will release more evaluation on real-world datasets on our GitHub repo