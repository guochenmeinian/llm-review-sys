# Regularized Behavior Cloning for

Blocking the Leakage of Past Action Information

Seokin Seo\({}^{1}\), HyeongJoo Hwang\({}^{1}\), Hongseok Yang\({}^{1,2}\), Kee-Eung Kim\({}^{1,2}\)

\({}^{1}\)Kim Jaechul Graduate School of AI, KAIST

\({}^{2}\)School of Computing, KAIST

siseo@ai.kaist.ac.kr, hjhwang@ai.kaist.ac.kr,

hongseok.yang@kaist.ac.kr, kekim@kaist.ac.kr

###### Abstract

For partially observable environments, imitation learning with observation histories (ILOH) assumes that control-relevant information is sufficiently captured in the observation histories for imitating the expert actions. In the offline setting where the agent is required to learn to imitate without interaction with the environment, behavior cloning (BC) has been shown to be a simple yet effective method for imitation learning. However, when the information about the actions executed in the past timesteps leaks into the observation histories, ILOH via BC often ends up imitating its own past actions. In this paper, we address this catastrophic failure by proposing a principled regularization for BC, which we name Past Action Leakage Regularization (PALR). The main idea behind our approach is to leverage the classical notion of conditional independence to mitigate the leakage. We compare different instances of our framework with natural choices of conditional independence metric and its estimator. The result of our comparison advocates the use of a particular kernel-based estimator for the conditional independence metric. We conduct an extensive set of experiments on benchmark datasets in order to assess the effectiveness of our regularization method. The experimental results show that our method significantly outperforms prior related approaches, highlighting its potential to successfully imitate expert actions when the past action information leaks into the observation histories.

## 1 Introduction

Imitation learning (IL) aims at learning a policy that recovers an expert's behavior from a demonstration dataset. Leveraging the information about state and expert action available in the dataset, IL has been successful in many real-world applications [4, 12, 26, 27, 32, 38, 43]. Although IL problems can be addressed using either online [17, 36] or offline algorithms [20, 33, 48], real-world tasks often impose restrictions on interacting with the environment due to safety, cost, and ethical concerns. Consequently, the practical necessity lies in the development of effective offline IL algorithms. Behavior cloning (BC) [33] is one of the most prominent offline IL algorithms, which learns to predict an expert's action for each given state using an offline expert dataset via supervised learning. BC has gained widespread recognition [7, 42] for providing a straightforward and effective solution, especially when full access to state information is available and the dataset is sufficiently extensive. However, when the state information is only partially available in an observation at each timestep, which is closer to the realistic scenario (e.g. an autonomous vehicle with a limited numberof sensors), training with any IL algorithm can be complicated. To enhance the ability of agent to infer missing control-relevant information from observations, incorporating the history of observations from adjacent past timesteps as input can be beneficial [2; 24].

When utilizing observation history for behavior cloning, a notable challenge emerges: the unnecessary dependence of the imitator's actions on the preceding actions, resulting in a suboptimal behavior in test time. In extreme cases, this dependency can lead to undesirable behaviors, such as the imitation policy that merely replicates its own actions from previous time steps 1, which leads to a catastrophic result particularly in safety-critical tasks like autonomous driving. This is often rooted at the inability to differentiate between actual and spurious causal relationship between observation features and expert actions within the collected data [8]. Past action information is a representative instance of such nuisance features, since it is a strongly correlated to the target expert action. This correlation can mislead the IL algorithm into recognizing past action information, which is not causally related to the target expert action, as a crucial feature for prediction. In this work, we refer to this misleading phenomenon as past action information leakage.

Footnote 1: The problem has many names: copycat problem [6; 44], inertia problem [7], latching effect [40]. Beyond the falsely leaded action repeating behavior, we introduce the past action information leakage to separate the reason and the result.

A natural way to mitigate this phenomenon is to adopt a mechanism that blocks the leakage of past action information irrelevant to the target expert action. In fact, Wen et al. [44] proposed an adversarial training approach to discard such redundant information in the representation of the observation history. This is achieved by maximizing the conditional entropy of the expert's previous action given the joint of the representation and target expert action. However, due to the intractability of direct computation of the conditional entropy, this approach relies on unstable adversarial learning. More recently, Chuang et al. [6] focuses on image-based observations and attempts to mitigate the past action dependency by splitting the policy representation into two parts: a representation of the observation history and a representation of the current observation. This method assumes a particular policy structure and does not provide a systematic general approach to block the leakage of past action information into the representation.

In this paper, we present a regularized behavior cloning framework that effectively mitigates the leakage of past action information. We formally define the problem of the leakage of past action information and establish a metric to quantify the magnitude of the leakage, employing the kernel-based method called HSCIC (Hilbert-Schmidt Conditional Independence Criterion) [30]. Building upon the metric, we devise an objective function that addresses the aforementioned problem, allowing for a more comprehensive understanding and interpretation of existing work [44]. Moreover, we propose a stable and efficient kernel-based regularization method that circumvents challenges such as adversarial learning, nested optimization and reliance on a neural estimator. Lastly, we conduct an extensive set of experiments which empirically show that our regularization method effectively blocks the leakage of past action information across a variety of control task benchmarks.

Our contributions are summarized as follows:

* We formally define the problem of the leakage of past action information based on the concept of conditional independence by quantifying the amount of leaked past action information.
* We introduce a principled framework for a behavior cloning with Past Action Leakage Regularization (PALR), which prevents the imitator from overfitting to leaked past information.
* We provide experimental results on established benchmarks, demonstrating the effectiveness of our method.

## 2 Related Work

Invariant representation learningLearning representation invariant to any unwanted factors has been widely studied in various domains such as fair classification [22; 25; 37; 47], domain adaptation [11; 13; 18; 49], and imitation learning [6; 44]. One of the dominant approaches in invariant representation learning is adversarial learning [11; 13; 44; 47]. Adversarial learning algorithmscommonly train additional networks that predict the unwanted factors from the representations while enforcing representations to make those prediction models fail. Consequently, they require alternating optimization between the main and the additional models [13; 44; 47] or show numerical instability [11]. To bypass those shortcomings of adversarial learning, several information-theoretic approaches [18; 25] have been proposed. Based on Variational Auto-Encoders (VAE) [21], these methods proposed end-to-end learning algorithms that jointly optimize all of their components with numerical stability. However, these methods assume that the distribution of the representation is Gaussian, which restricts the flexibility of the representation. Wen et al. [46] concentrated on eliminating shortcuts in supervised learning by incorporating supplementary key information, demonstrating its enhancement of behavior cloning with observation histories. Recent methods have proposed to learn counterfactually or conditionally invariant representation, leveraging on kernel-based conditional independence metric [31; 35]. These methods demonstrated promising empirical results in synthetic domains, effectively mitigating the impact of nuisance correlations.

Information leakage in imitation learningThere is a growing understanding of the importance of addressing the correlation between expert actions and nuisance features, which are not essential for control and may even hinder performance. [8; 29; 39] Especially, termed as causal confusion [8], imitation learning exhibits a paradoxical phenomenon: having more information can lead to worse performance. Aligning with such observation, recent works have also demonstrated that accessing more information from the observation history leaks past action information so that the imitator may learn an undesirable policy that simply repeats the same action in the past [6; 7; 40; 44; 45]. To avoid learning degenerate solutions from the past action information, Wen et al. [44] proposed the regularization method based on the conditional entropy of the previous action given the representation of the history and the current action. However, their method involves a nested minimax optimization along with an additional neural network, which complicates the training process. We closely investigate their formulation in Section 4.2.1 to show their limitations as well as their connection to our method. Wen et al. [44] also introduced the action predictability metric to quantify the dependence between past action histories and imitator actions, relative to expert actions. However, this metric is not based on conditional independence, which is central to our argument concerning the past action leakage. Wen et al. [45] addressed this problem using a weighted behavior cloning method that upweights "keyframe" samples, which are more likely to be predicted from the action histories. Swamy et al. [40] showed that online interaction is both necessary and sufficient to resolve repeating behavior. However, in many real-world applications [4; 12; 26; 27; 32; 38; 43], online interaction is often infeasible due to safety, cost, and ethical considerations. In this work, we develop an offline algorithm that can robustly handle the leakage of past action information.

## 3 Preliminaries

### Conditional independence

For random variables \(X,Y\) which taking a value in \(\mathcal{X},\mathcal{Y}\) respectively, we write \(P_{X}\), \(P_{XY}\), and \(P_{X|Y}\) for the marginal distribution of \(X\), the joint distribution of \(X\) and \(Y\), and the conditional distribution of \(X\) given \(Y\), respectively. We say that random variables \(X\) and \(Y\) are conditionally independent given a random variable \(Z\) or simply \(Z\)-conditionally independent if \(P_{XY|Z}(x,y)=P_{X|Z}(x)P_{Y|Z}(y)\) for all \(x,y\in\mathcal{X}\times\mathcal{Y}\). Also, we say that \(X\) and \(Y\) are independent if \(P_{XY}(x,y)=P_{X}(x)P_{Y}(y)\) for all \(x,y\). We denote \(Z\)-conditional independence by \(X\perp\!\!\!\perp Y\mid Z\), and independence by \(X\perp\!\!\!\perp Y\).

Conditional Mutual Information (CMI)Mutual information (MI) is an information-theoretic quantity to measure the dependency between two random variables. MI between random variables \(X\) and \(Y\) is defined as \(I(X;Y)=D_{\mathrm{KL}}(P_{XY},P_{X}P_{Y})=\mathbb{E}_{x,y\sim P_{XY}}[\log P_{ XY}(x,y)-\log P_{X}(x)P_{Y}(y)]\). MI is always non-negative (i.e., \(I(X;Y)\geq 0\)), and it becomes zero if and only if \(X\perp\!\!\!\perp Y\). Conditional mutual information (CMI) is defined similarly, but with conditional distributions. CMI between random variables \(X\) and \(Y\) given a random variable \(Z\) is defined as \(I(X;Y\mid Z)=\mathbb{E}_{P_{Z}}[D_{\mathrm{KL}}(P_{XY|Z},P_{X|Z}P_{Y|Z})]\). As in the case of MI, \(I(X;Y\mid Z)\geq 0\), and also \(I(X;Y\mid Z)=0\) if and only if \(X\perp\!\!\!\perp Y\mid Z\).

Hilbert-Schmidt Conditional Independence Criterion (HSCIC)Hilbert-Schmidt Independence Criterion (HSIC) [14] is a kernel-based measure that quantifies the dependency between two random variables. Let \(\mathcal{H}_{\mathcal{X}}\) and \(\mathcal{H}_{\mathcal{Y}}\) be reproducing kernel Hilbert spaces (RKHS) on \(\mathcal{X}\) and \(\mathcal{Y}\) with the corresponding reproducing kernels \(k_{\mathcal{X}}\) and \(k_{\mathcal{Y}}\), respectively. The tensor product \(k_{\mathcal{X}}\otimes k_{\mathcal{Y}}\) of the kernels is a binary function on \(\mathcal{X}\times\mathcal{Y}\) defined by \((k_{\mathcal{X}}\otimes k_{\mathcal{Y}})((x_{1},y_{1}),(x_{2},y_{2})):=k_{ \mathcal{X}}(x_{1},x_{2})k_{\mathcal{Y}}(y_{1},y_{2})\). The RKHS on \(\mathcal{X}\times\mathcal{Y}\) with the kernel \(k_{\mathcal{X}}\otimes k_{\mathcal{Y}}\) is called the tensor product RKHS of \(\mathcal{H}_{\mathcal{X}}\) and \(\mathcal{H}_{\mathcal{Y}}\), and it is denoted by \(\mathcal{H}_{\mathcal{X}}\otimes\mathcal{H}_{\mathcal{Y}}\). HSIC between \(X\) and \(Y\) is defined to be the maximum mean discrepancy (MMD) between \(P_{XY}\) and \(P_{X}P_{Y}\) under \(\mathcal{H}_{\mathcal{X}}\otimes\mathcal{H}_{\mathcal{Y}}\), i.e, the distance in the Hilbert space \(\mathcal{H}_{\mathcal{X}}\otimes\mathcal{H}_{\mathcal{Y}}\) between the so-called kernel mean embeddings of \(P_{XY}\) and \(P_{X}P_{Y}\) to that space:

\[\mathrm{HSIC}(X,Y) :=\mathrm{MMD}^{2}(P_{XY},P_{X}P_{Y};\mathcal{H}_{\mathcal{X}} \otimes\mathcal{H}_{\mathcal{Y}})\] \[=\|\mu_{P_{XY}}-\mu_{P_{X}}\otimes\mu_{P_{Y}}\|^{2}_{\mathcal{H} _{\mathcal{X}}\otimes\mathcal{H}_{\mathcal{Y}}}\]

where \(\mu_{P_{XY}}\) is the kernel mean embedding of the distribution \(P_{XY}\) to \(\mathcal{H}_{\mathcal{X}}\otimes\mathcal{H}_{\mathcal{Y}}\), i.e. \(\mu_{P_{XY}}(x,y):=\mathbb{E}_{P_{X}\mathcal{Y}}[(k_{\mathcal{X}}\otimes k_{ \mathcal{Y}})((X,Y),(x,y))]\), and \(\mu_{P_{X}}\), \(\mu_{P_{Y}}\) are defined similarly but using \(\mathcal{H}_{\mathcal{X}}\), \(\mathcal{H}_{\mathcal{Y}}\) instead. The \((\mu_{P_{X}}\otimes\mu_{P_{Y}})\) is simply the function in \(\mathcal{H}_{\mathcal{X}}\otimes\mathcal{H}_{\mathcal{Y}}\) that maps \((x,y)\) to \(\mu_{P_{X}}(x)\mu_{P_{Y}}(y)\). When \(k_{\mathcal{X}}\otimes k_{\mathcal{Y}}\) satisfies some condition (i.e., characteristic), \(\mathrm{HSIC}(X,Y)=0\) if and only if \(X\perp\!\!\!\perp Y\)[14].

HSCIC (Hilbert-Schmidt Conditional Independence Criterion) is an extension of HSIC that quantifies the amount of conditional dependency between two random variables given another random variable. In this paper, we follow the definition of HSCIC based on conditional mean embedding [30], and use an estimator of HSCIC that draws samples from the joint distribution \(P_{XYZ}\) and performs vector-valued RKHS regression. The conditional mean embedding of \(X\) given \(Z\) is a function in the RKHS \(\mathcal{H}_{\mathcal{X}}\) that is parameterized by the value of \(Z\). Given a value \(z\) of \(Z\), it maps \(x\in\mathcal{X}\) to \(\mu_{P_{X\mid Z=x}}(x):=\mathbb{E}_{P_{X\mid Z=z}}[k_{\mathcal{X}}(X,x)]\). Then, HSCIC is a mapping from a value of \(Z\) to a non-negative real defined as follows: for all \(z\in\mathcal{Z}\),

\[\mathrm{HSCIC}(X,Y|Z=z):=\|\mu_{P_{XY\mid Z=z}}-\mu_{P_{X\mid Z=z}}\otimes\mu_ {P_{Y\mid Z=z}}\|^{2}_{\mathcal{H}_{\mathcal{X}}\otimes\mathcal{H}_{\mathcal{ Y}}}\]

When \(k_{\mathcal{X}}\otimes k_{\mathcal{Y}}\) satisfies the condition mentioned from above, \(X\perp\!\!\!\perp Y\mid Z\) if and only if \(\mathbb{E}_{P_{Z}}[\mathrm{HSCIC}(X,Y|Z)]=0\)[30]. We use an empirical estimator of the expectation here, denoted by \(\widetilde{\mathrm{HSCIC}}(X,Y|Z)\). See Section A in the supplementary material for more details for definition of HSCIC and its empirical estimator.

### Imitation learning from observation histories

We consider a Partially Observable Markov Decision Process (POMDP) [19] without reward, which is defined as a tuple of \(\langle\mathcal{S},\mathcal{Z},\mathcal{A},O,P,\rho_{0}\rangle\). Here \(\mathcal{S}\), \(\mathcal{Z}\) and \(\mathcal{A}\) are an (underlying) state space, an observation space and an action space, respectively. The next \(O:\mathcal{S}\rightarrow\Delta\mathcal{Z}\) specifies the conditional probability \(O(z|s)\) of an observation \(z\in\mathcal{Z}\) given a state \(s\in\mathcal{S}\). Finally, \(\rho_{0}\in\Delta S\) defines the probability \(\rho_{0}(s_{0})\) that the process starts from \(s_{0}\in\mathcal{S}\), and \(P:\mathcal{S}\times\mathcal{A}\rightarrow\Delta\mathcal{S}\) defines the probability \(P(s^{\prime}|s,a)\) of transitioning to state \(s^{\prime}\) when action \(a\in\mathcal{A}\) is performed in state \(s\in\mathcal{S}\).

Assume a (stochastic) expert policy \(\pi^{E}:\mathcal{S}\rightarrow\Delta\mathcal{A}\) that is defined as a conditional probability \(\pi^{E}(a|s)\) of the expert's performing an action \(a\in\mathcal{A}\) given a state \(s\in\mathcal{S}\). Also, assume that we have a dataset \(\mathcal{D}=\{\tau^{(1)},\ldots,\tau^{(N)}\}\) of the expert trajectories where each \(\tau=\{(z_{t},a_{t})\}_{t=0}^{T}\) is sampled by

\[s_{0}\sim\rho_{0},\ a_{t}^{E}\sim\pi^{E}(\cdot|s_{t}),\ z_{t}\sim O(\cdot|s_{t }),\ s_{t+1}\sim P(\cdot|s_{t},a_{t})\ \ \text{for}\ t\in\{0,1,...,T\}.\]

That is, \(\tau\) in the dataset is drawn from the following joint distribution of all observations and actions:

\[\tau\sim p_{D}(z_{0:T},a_{0:T}^{E})=\int\rho_{0}(s_{0})\prod_{t=0}^{T}O(z_{t}|s _{t})\pi^{E}(a_{t}^{E}|s_{t})P(s_{t+1}|s_{t},a_{t}^{E})ds_{0:T+1}.\]

In our study, we consider POMDP scenarios where an ideal imitator policy is able to match the expert policy's performance, even when the imitator's actions are solely determined by observation histories. Specifically, we focus on situations where the observation histories \(z_{t-w-1:t}\) (for some fixed \(1\leq w\leq T-1\)) encompass all information about the true states \(s_{t}\) utilized by the expert policy.

In this setting, our goal is to learn an imitator policy \(\pi^{I}:\mathcal{Z}^{w}\rightarrow\Delta\mathcal{A}\) that acts as closely to \(\pi^{E}\) as possible on the given dataset \(\mathcal{D}\). To achieve this goal, we consider the following joint distribution of actions of both \(\pi^{I}\) and \(\pi^{E}\):

\[p(a^{I}_{0:T},a^{E}_{0:T})=\\ \int\rho_{0}(s_{0})\prod_{t=0}^{T}O(z_{t}|s_{t})\pi^{I}(a^{I}_{t} |z_{t-w+1:t})\pi^{E}(a^{E}_{t}|s_{t})P(s_{t+1}|s_{t},a^{E}_{t})ds_{0:T+1}dz_{0: T+1}.\] (1)

## 4 Behavior Cloning with Past Action Leakage Regularization

In this section, we propose a framework that effectively mitigates the past action leakage problem in IL. We first define the problem by formalizing the absence of leaked past-action information via conditional independence (Section 4.1). Then, we compare several regularization-based approaches that attempt to achieve the absence of such information in the context of offline IL (Section 4.2). These approaches performs regularized BC where the regularizer is derived from a metric for measuring the amount of conditional dependence among random variables. The choice of the metric differentiates these approaches, and our comparison advocates the use of the HSCIC-based approach.

### Past action leakage problem in imitation learning

When a policy takes an observation history as an input, the input history may include the information about past actions unexpectedly. The inclusion of such information can have detrimental effects in the context of offline imitation learning by confusing the imitator and making it fail to predict expert actions accurately. Intuitively, the past action leakage problem in imitation learning refers to this failure of the imitator due to the leakage of such harmful past action information.

To express this intuition formally, for each timestep \(t\), let \(A^{E}_{t}\) and \(A^{I}_{t}\) denote random variables of expert action and imitator action at timestep \(t\). Note that for each \(0<t\leq T\), the joint distribution of the three random variables \(A^{E}_{t-1}\), \(A^{E}_{t}\) and \(A^{I}_{t}\) is

\[p(a^{E}_{t-1},a^{E}_{t},a^{I}_{t})=\int p(a^{I}_{0:T},a^{E}_{0:T})da^{E}_{0:t- 2}da^{I}_{0:t-1}da^{E}_{t+1:T}da^{I}_{t+1:T}\]

where \(p(a^{I}_{0:T},a^{E}_{0:T})\) is the distribution in Eq. (1).

We formalize the absence of the leakage of harmful past-action information by conditional independence between the imitator's current actions and the expert's previous actions:

\[A^{I}_{t}\perp\!\!\!\perp A^{E}_{t-1}\mid A^{E}_{t}\quad\text{for all }0<t\leq T.\] (2)

This conditional independence says that the imitator's current action never depends on some information that is only about the expert's past action but not about the expert's current action. This past-specific information corresponds to harmful information in our intuitive explanation from above.

Ideally we would like to achieve conditional independence in Eq. (2), which ensures the absence of leaked past-action information that was harmful to the imitator. However, in practice, we can achieve it only approximately, so that we need a quantitative measure for conditional independence or the lack of conditional independence between \(A^{E}_{t-1}\) and \(A^{E}_{t}\) given \(A^{I}_{t}\). Such a measure is also needed to design an offline IL algorithm that does not suffer from such leaked harmful past-action information. In the following subsection, we consider two quantitative measures for the lack of conditional independence, namely, (1) conditional mutual information (CMI) and (2) Hilbert-Schmidt Conditional Independence Criterion (HSCIC).

### Behavior cloning with past action leakage regularization

We aim to learn the representation \(\varphi_{t}\) of observation history \(z_{t-w+1:t}\) that removes any unnecessary information on the past action. To simplify the notation, let \(t_{w}\) denote \(t-w+1\). Our method is based on the following observation.

**Theorem 1**.: _Let \(A_{t}^{I}\) be the action from the imitator policy \(\pi^{I}(a_{t}|\varphi_{t})\) based on the representation \(\varphi_{t}\) of observation history. Then, \(\varphi_{t}\perp\!\!\!\perp A_{t-1}^{E}\mid A_{t}^{E}\Longrightarrow A_{t}^{I} \perp\!\!\!\perp A_{t-1}^{E}\mid A_{t}^{E}\)._

Proof.: See Section B in the supplementary material. 

To this end, we formulate our objective of regularized BC framework as follows:

\[\mathcal{L}(\pi,\varphi;\mathcal{D},\alpha):=\mathcal{L}_{\rm bc}(\pi,\varphi ;\mathcal{D})+\alpha\cdot\mathcal{L}_{\rm reg}(\varphi;\mathcal{D}),\] (3)

where \(\mathcal{L}_{\rm bc}\) is an BC objective such that \(\mathcal{L}_{\rm bc}(\pi,\varphi;\mathcal{D}):=\mathbb{E}_{(z_{t\!\!\!\perp },a_{t}^{E})\sim\mathcal{D},\varphi_{t}\sim\varphi(z_{t\!\!\!\perp})}\left[- \log\pi(a_{t}^{E}|\varphi_{t})\right]\) and \(\mathcal{L}_{\rm reg}\) is a past action leakage regularization objective. For notational simplicity, we abbreviate the expectation with respect to \((a_{t-1}^{E},z_{t\!\!\!\perp},a_{t}^{E})\sim\mathcal{D},\varphi_{t}\sim\varphi (z_{t\!\!\!\perp},a_{t}^{E})\) to \(\mathbb{E}_{\varphi_{t}}\). In the following subsections, we discuss candidates for \(\mathcal{L}_{\rm reg}\).

#### 4.2.1 Information-theoretic regularization

A straightforward way to address the conditional independence we discussed in Section 4.1 is minimizing the CMI, which can be decomposed by its definition into two conditional entropy terms:

\[I(a_{t-1}^{E};\varphi_{t}\mid a_{t}^{E})=H(a_{t-1}^{E}\mid a_{t}^{E})-H(a_{t-1} ^{E}\mid\varphi_{t},a_{t}^{E}).\]

It is important to note that \(H(a_{t-1}^{E}\mid a_{t}^{E})\) is determined by the data distribution \(\mathcal{D}\) and thus constant. As a result, we can simply consider the minimization of \(-H(a_{t-1}^{E}\mid\varphi_{t},a_{t}^{E})\).

\[\mathcal{L}_{\rm reg-Ent}(\varphi_{t};a_{t-1}^{E},a_{t}^{E}):=-H(a_{t-1}^{E} \mid\varphi_{t},a_{t}^{E})=\mathbb{E}\left[\log p(a_{t-1}^{E}\mid\varphi_{t}, a_{t}^{E})\right].\] (4)

Interestingly, Eq. (4) coincides with the negative conditional entropy maximization objective in FCA [44]. Since the direct computation of Eq. (4) requires to know the intractable distribution \(p(a_{t-1}^{E}\mid\varphi_{t},a_{t}^{E})\), FCA trains an additional prediction model \(\hat{p}(\hat{a}_{t-1}\mid\varphi_{t},a_{t})\) to estimate the negative entropy with \(-\hat{H}(a_{t-1}^{E}\mid\varphi_{t},a_{t}^{E}):=\mathbb{E}_{\varphi_{t}}[\log \hat{p}(a_{t-1}^{E}\mid\varphi_{t},a_{t}^{E})]\). However, this approach faces the following challenges:

1. The estimated negative entropy \(-\hat{H}\) lower bounds Eq. (4), while an upper bound would be desirable for minimizing the objective. Consequently, it imposes nested (minimax) optimization; minimizing with respect to \(\varphi_{t}\) after maximizing with respect to \(\hat{p}\).
2. It is required to train an additional neural network to model \(\hat{p}\), which consumes additional computational cost.
3. Assuming that the family of variational distribution \(\hat{p}\) to be Gaussian, FCA tries to tighten the lower bound estimation \(-\hat{H}\) with respect to \(\hat{p}\) by minimizing reconstruction error of \(a_{t-1}^{E}\). The assumption restricts the flexibility of \(\hat{p}\) and thus the estimation can be inaccurate.

To avoid inaccurate estimation of the entropy, we can also decompose CMI into two MI terms by the chain rule of MI as an alternative:

\[I(a_{t-1}^{E};\varphi_{t}\mid a_{t}^{E})=I(a_{t-1}^{E};\varphi_{t},a_{t}^{E})- I(a_{t-1}^{E};a_{t}^{E}),\]

Similar to \(H(a_{t-1}^{E}\mid a_{t}^{E})\), \(I(a_{t-1}^{E};a_{t}^{E})\) can be safely ignored in the regularization. As a result, the regularization is about simply minimizing \(I(a_{t-1}^{E};\varphi_{t},a_{t}^{E})\) while ignoring the constant MI term.

\[\mathcal{L}_{\rm reg-MI}(\varphi_{t};a_{t-1}^{E},a_{t}^{E}):=I(a_{t-1}^{E}; \varphi_{t},a_{t}^{E}).\] (5)

Since the direct computation of Eq. (5) requires to know densities of \(a_{t-1}^{E},\varphi_{t},a_{t}^{E}\), one needs to train sample-based MI estimators [3, 16, 28, 34]. Thanks to those estimators, the estimated MI can be minimized without confining any distribution (to be Gaussian). However, this approach still has issues similar to FCA such that (1) it lower bounds MI and consequently imposes (nested) minimax optimization and (2) it introduces an additional neural network particularly sensitive to hyperparameters.

#### 4.2.2 HSCIC regularization

To bypass those shortcomings in information-theoretic regularization, we consider HSCIC [30], a kernel-based conditional independence metric for the past action leakage regularization.

\[\mathcal{L}_{\mathrm{reg-HSCIC}}(\varphi_{t};a_{t-1}^{E},a_{t}^{E}):= \mathrm{HSCIC}(\varphi_{t},a_{t-1}^{E}|a_{t}^{E}).\] (6)

Let \(\mathcal{H}_{\mathcal{A}},\mathcal{H}_{\Phi}\) be RKHSs over \(\mathcal{A},\Phi\) and \(k_{\mathcal{A}},k_{\Phi}\) be their associated kernels, where \(\Phi\) is the representation space induced by the encoder \(\varphi\). Given \(n\) samples \(\{(\varphi_{t}(i),a_{t-1}^{E}(i),a_{t}^{E}(i))\}_{i=1}^{n}\) from \(\mathcal{D}\) and \(\varphi\), let \(\mathbf{K}_{\varphi_{t}},\mathbf{K}_{a_{t-1}^{E}},\mathbf{K}_{a_{t}^{E}}\) be the \(n\times n\) kernel matrices where \([\mathbf{K}_{\varphi_{t}}]_{i,j}=k_{\Phi}(\varphi_{t}(i),\varphi_{t}(j)),[ \mathbf{K}_{a_{t-1}^{E}}]_{i,j}=k_{\mathcal{A}}(a_{t-1}^{E}(i),a_{t-1}^{E}(j)),[\mathbf{K}_{a_{t}^{E}}]_{i,j}=k_{\mathcal{A}}(a_{t}^{E}(i),a_{t}^{E}(j))\). Then, HSCIC estimator for past action leakage regularization can be defined as follows:

\[\mathrm{HSCIC}(\varphi_{t},a_{t-1}^{E}|a_{t}^{E}):=\frac{1}{n} \mathrm{trace}\Big{(} \mathbf{K}_{a_{t}^{E}}^{\top}\mathbf{W}(\mathbf{K}_{\varphi_{t} }\odot\mathbf{K}_{a_{t-1}^{E}})\mathbf{W}^{\top}\mathbf{K}_{a_{t}^{E}}\] \[-2\mathbf{K}_{a_{t}^{E}}^{\top}\mathbf{W}(\mathbf{K}_{\varphi_{t }}\mathbf{W}^{\top}\mathbf{K}_{a_{t}^{E}}\odot\mathbf{K}_{a_{t-1}^{E}}\mathbf{ W}^{\top}\mathbf{K}_{a_{t}^{E}})\] \[+(\mathbf{K}_{a_{t}^{E}}^{\top}\mathbf{W}\mathbf{K}_{\varphi_{t }}\mathbf{W}^{\top}\mathbf{K}_{a_{t}^{E}})\odot(\mathbf{K}_{a_{t}^{E}}^{\top} \mathbf{W}\mathbf{K}_{a_{t-1}^{E}}\mathbf{W}^{\top}\mathbf{K}_{a_{t}^{E}}) \Big{)}\] (7)

where \(\mathbf{W}=(\mathbf{K}_{a_{t}^{E}}+n\lambda\mathbf{I})^{-1}\), \(\lambda>0\) is a ridge regression coefficient, \(\odot\) is the element-wise matrix multiplication.

Since Eq. (7) can be estimated using the samples from the joint distribution \(p(\varphi_{t},a_{t-1}^{E},a_{t}^{E})\), we can directly plug-in HSCIC estimates into the regularization objective. By leveraging this estimator, HSCIC regularization offers several advantages compared to the conditional entropy regularization and MI regularization objectives discussed earlier:

1. Direct computation of the closed-form solution in Eq. (7) allows HSCIC regularization to bypass any nested optimization.
2. HSCIC regularization does not employ any additional deep neural networks that require careful hyperparameters.
3. Since HSCIC is a non-parametric measure, it does not impose any parametric assumption on the data distribution and does not require any density estimation.

These advantages strongly imply that promoting conditional independence via the HSCIC estimator will be more desirable compared to other estimators, thereby improving the overall effectiveness of the regularization. Hence, we propose HSCIC regularization to address the past action leakage problem, which we call PALR.

## 5 Experiment

In this section, we present the experimental results of our approach. Initially, we investigate the correlation between the extent of past action information leakage and BC's performance. Subsequently, we compare our approach with several offline ILOH baseline methods across four continuous control tasks from the MuJoCo simulator [41]: hopper, walker2d, halfcheetah, and ant, as well as one pixel-based autonomous driving task from the CARLA simulator [9]: carla-lane2.

Footnote 2: The D4RL benchmark also featured another task, carla-toun, but none of the algorithms surpassed the random policyâ€™s performance, indicating a lack of meaningful imitation results. Further details can be found in Section D.2.

DatasetFor tasks from the MuJoCo simulator, we transformed them into POMDP scenarios for ILOH by excluding specific state variables (such as velocity information). The remaining state variables, like positional information and joint angles, were treated as observation variables at individual timesteps3. We organized these observations into fixed-size stacks to configure eachproblem setting, denoted as [envname]-W[stacksize] with stack sizes \(w\in\{2,4\}\). For a task from the CARLA simulator, we used pixel images as observations for ILOH. To extract features from these observations, we employed a pretrained ResNet [15], keeping its parameters fixed during training. In carla-lane task, we stacked the extracted features with a fixed stack size of \(w=3\) and used it as input for the policy. All our experiments utilized expert demonstrations from the D4RL benchmark dataset [10] to ensure the validity and reliability of our results.

Evaluation MetricFor the performance evaluation of the learned policy, we measure the normalized score that ranges from 0 to 100. To evaluate how much the past action information is leaked into a policy \(\pi\), we measure \(\widetilde{\text{HSCIC}}(a_{t}^{I};a_{t-1}^{E}|a_{t}^{E})\) using the estimator (7) in the held-out dataset. To estimate HSCIC estimator, we fix a ridge regression coefficient \(\lambda=10^{-5}\) and all kernels are chosen as Gaussian kernels with the bandwidth \(\sigma^{2}=1\).

### Relationship between past action information leakage and performance

To see if the problem of the past action information leakage occurs, we conduct an empirical study using BC from observation histories with stack size \(w=2\). The objective of this experiment is to confirm the correlation between the performance and the degree of past action information leakage, similar to FCA [44].

Experimental setupTo achieve multiple policies at different levels, we train BC policies for 500K steps with 5 different seeds and 7 different training dataset size \(N\in\{100\text{K},50\text{K},30\text{K},10\text{K},5\text{K},3\text{K},1\text {K}\}\) (the number of observation history-action pairs). Using a held-out dataset composed of 2,000 samples, the degree of past action information leakage is measured by \(\widetilde{\text{HSCIC}}(a_{t}^{I};a_{t-1}^{E}|a_{t}^{E})\) of each policy \(\pi^{I}\).

ResultsTo compare \(\widetilde{\text{HSCIC}}\) of fully trained BC in all tasks, we normalize \(\widetilde{\text{HSCIC}}\) of each policy using the maximum and minimum values in each task. Grouping trained policies by the dataset size, we report the mean value of both the normalized score and the normalized HSCIC estimates in Figure 1. It shows that there are clear negative correlations between the normalized HSCIC and the normalized score in all environments. The result implies that BC from observation histories tends to train a policy in which there is a negative correlation between the conditional dependence related to its action and the performance. This insight suggests a potential opportunity for employing regularization methods that enforce conditional independence.

### Performance Evaluation

Experimental setupWe evaluate the performance of our method and offline baseline methods across 5 environments see effectiveness of our method. We train policies using 5 different algorithms, including our method, for 500K, 1M training steps MuJoCo and CARLA tasks respectively with 5 different seeds.

Baseline methodsWe compare our method with 6 offline ILOH baseline methods: BC, KF [45], PrimeNet [46], RAP [6]4, FCA [44], MINE [3]. While all of these are commonly based on BC, we specify their differences as follows:

Figure 1: Negative correlation between HSCIC estimates and performance of BC with 7 different dataset sizes.

* **BC** : the standard BC algorithm from observation history, which optimizes an objective without any regularization term (\(\alpha=0\)).
* **KF**[45] : the weighted BC algorithm that assigns higher weights to keyframes, which contain actions that are highly predictable from their corresponding action histories.
* **PrimeNet**[46] : a supervised learning method designed to prevent undesirable shortcuts by leveraging additional key inputs.
* **RAP**[6] : it employs a dual-stream of policy representation that learns from both observation history and individual observations, maximizing a lower bound that enforces conditional dependence between the representation and expert action, given the past action.
* **FCA**[44] : it maximizes the conditional entropy that corresponds to Eq. (4) with adversarial training, which is an instance of our regularized BC framework.
* **MINE**[3] : it minimizes the MI estimate corresponding to Eq. (5) using MINE estimator [3], which is one of the representative sample-based neural estimators for MI. It is also an instance of our regularized BC framework.

Our method regularizes HSCIC with its estimator defined as Eq. (7), where Gaussian kernel with fixed bandwidth \(\sigma^{2}=1\) are used for all kernels of \(\varphi_{t},a_{t-1}^{E},a_{t}^{E}\) and a ridge regression coefficient \(\lambda=10^{-5}\). Across all regularization methods, we searched for the optimal \(\alpha\) according to the best mean normalized score. Further implementation details can be found in Section C.

ResultsTable 1 summarizes the results of performance evaluation for each problem settings. Our method significantly outperforms other baselines in 7 settings out of 9 and shows competitive performance in the rest 2 settings. In particular, our method shows strong performance in carla-lane, highlighting its effectiveness in enhancing performance within high-dimensional offline ILOH scenarios. We observe that RAP shows the least competitive performance across all tasks. This is because they do not have any penalization of the dependence between the imitator action and expert action in their objective function. However, we also observe that FCA and MINE fail to show consistent improvement over BC in most tasks except walker2d. This is because the lower bound estimators of their regularization objectives are not sufficiently accurate even at the cost of their inefficient alternating optimization. On the other hand, our method consistently outperforms BC, which clearly indicates the effect of promoting the conditional independence as we discussed in Section 4.1.

To better understand the degenerate performance of FCA and MINE, we evaluate \(\widehat{\text{HSCIC}}(a_{t}^{I};a_{t-1}^{E}|a_{t}^{E})\) of each regularization method during training using the held-out dataset in Figure 1(a). In contrast to our method, the result clearly demonstrates that FCA and MINE commonly

\begin{table}
\begin{tabular}{l c|c c c c|c c c} \hline
**Task** & w & **BC** & **KF** & **PrimeNet** & **RAP** & **FCA** & **MINE** & **PARL (Ours)** \\ \hline \hline hopper & 2 & \(32.5\pm 2.9\) & \(32.0\pm 1.9\) & \(30.0\pm 1.6\) & \(20.2\pm 1.4\) & \(31.9\pm 2.5\) & \(25.0\pm 1.9\) & \(\mathbf{42.0\pm 2.4}\) \\  & 4 & \(47.7\pm 3.4\) & \(45.7\pm 1.0\) & \(45.3\pm 2.8\) & \(32.6\pm 2.6\) & \(36.9\pm 2.4\) & \(37.6\pm 3.1\) & \(\mathbf{58.4\pm 2.8}\) \\ \hline walker2d & 2 & \(53.0\pm 2.7\) & \(50.0\pm 2.3\) & \(48.5\pm 3.3\) & \(15.8\pm 2.0\) & \(63.1\pm 2.7\) & \(58.6\pm 5.5\) & \(\mathbf{79.8\pm 2.3}\) \\  & 4 & \(63.2\pm 6.3\) & \(77.4\pm 2.0\) & \(79.2\pm 3.3\) & \(25.4\pm 2.1\) & \(\mathbf{81.9\pm 3.3}\) & \(68.7\pm 6.7\) & \(\mathbf{83.4\pm 5.4}\) \\ \hline halfcheetah & 2 & \(74.1\pm 2.3\) & \(64.3\pm 1.4\) & \(61.5\pm 1.9\) & \(63.9\pm 2.1\) & \(78.2\pm 2.8\) & \(76.3\pm 1.9\) & \(\mathbf{86.4\pm 1.1}\) \\  & 4 & \(68.4\pm 2.6\) & \(55.7\pm 4.1\) & \(45.5\pm 1.7\) & \(59.0\pm 2.7\) & \(69.9\pm 2.6\) & \(73.4\pm 2.4\) & \(\mathbf{79.1\pm 4.3}\) \\ \hline ant & 2 & \(56.3\pm 3.5\) & \(54.9\pm 1.7\) & \(51.7\pm 2.4\) & \(44.1\pm 1.2\) & \(51.1\pm 2.2\) & \(53.9\pm 1.9\) & \(\mathbf{59.6\pm 3.0}\) \\  & 4 & \(\mathbf{64.4\pm 1.8}\) & \(48.6\pm 3.8\) & \(58.2\pm 1.9\) & \(48.6\pm 2.6\) & \(57.7\pm 1.3\) & \(56.6\pm 1.8\) & \(\mathbf{64.6\pm 2.5}\) \\ \hline \hline carla-lane & 3 & \(52.5\pm 6.2\) & \(66.6\pm 2.1\) & \(58.2\pm 2.2\) & \(25.3\pm 5.4\) & \(57.1\pm 3.1\) & \(60.1\pm 4.1\) & \(\mathbf{72.9\pm 2.6}\) \\ \hline \end{tabular}
\end{table}
Table 1: Performance evaluation of baseline and regularization methods. The normalized scores averaged over the final 50 evaluations during training and we report mean and standard error over 5 different seeds. The rightmost three algorithms are incorporated into our regularization framework. The method with the highest mean score and its competitive methods (within standard error) are highlighted in bold in each problem setting.

fail to promote conditional independence throughout the learning process, elucidating their underperformance. To further investigate, we measure the HSCIC and conditional MI estimates of the finalized imitator policies across 8 MuJoCo settings (refer to Section D.5). In essence, our method adeptly mitigates the leakage of past action information, surpassing the efficacy of alternative approaches.

Regularization coefficientIn this experiment, we aim to ascertain how the selection of coefficient \(\alpha\) influences the performance of our method. We evaluate the asymptotic performance of our method with varying values of \(\alpha\in\{0,10^{0},10^{1},10^{2},10^{3},10^{4},10^{5},10^{6}\}\) on the walker2d-W2 problem setting, which shows the largest performance gap between PALR and BC among our problem settings. As depicted in Figure 2b, the converged regularization loss \(\mathcal{L}_{\mathrm{reg-HSCIC}}\) progressively decreases as \(\alpha\) increases. Notably, the \(\alpha\) that minimizes the regularization loss is not necessarily equal to the optimal hyperparameter that maximizes performance. We observe that the selection of \(\alpha\) is important, as it adjusts the trade-off between robustness to the leakage and alignment with expert data.

## 6 Conclusion and Future Work

Grounded in the classical notion of conditional independence, we proposed a principled regularization framework for BC that mitigates past action information leakage problem. Within our framework, we have explored multiple choices of conditional independence metric and compared their estimators. Finally, we identified that our method with HSCIC estimator is the most favorable regularization of BC over other choices in terms of robustness to the leaked information of past action. In an extensive set of experiments on D4RL datasets, we empirically showed that our method significantly outperforms baseline methods. We also observed in our experiments that all the comparing methods including ours were sensitive to the choice of \(\alpha\). Without assuming any interaction with the environment, it is challenging to find the optimal \(\alpha\) only with the given offline dataset. We believe that the discovery of optimal \(\alpha\) in offline manner would be an interesting research topic, which we leave as future work.

## Acknowledgments and Disclosure of Funding

This work was partly supported by IITP grant funded by MSIT (No.2020-0-00940, Foundations of Safe Reinforcement Learning and Its Applications to Natural Language Processing; No.2022-0-00311, Development of Goal-Oriented Reinforcement Learning Techniques for Contact-Rich Robotic Manipulation of Everyday Objects; No.2019-0-00075, AI Graduate School Program (KAIST); No.2021-0-02068, AI Innovation Hub), NRF of Korea (NRF2019R1A2C1087634; RS-2023-00279680; NRF-2018R1A5A1059921), Field-oriented Technology Development Project for Customs Administration through NRF of Korea funded by the MSIT and Korea Customs Service (NRF2021M31A1097938), ETRI grant (22ZS1100, Core Technology Research for Self-Improving Integrated AI System), KAIST-NAVER Hypercreative AI Center.

Figure 2: Investigating past action leakage regularization methods on D4RL dataset.

## References

* [1] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. In _International Conference on Learning Representations_, 2016.
* [2] Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst. _arXiv preprint arXiv:1812.03079_, 2018.
* [3] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In _International Conference on Machine Learning_, 2018.
* [4] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. _arXiv preprint arXiv:1604.07316_, 2016.
* [5] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* [6] Chia-Chi Chuang, Donglin Yang, Chuan Wen, and Yang Gao. Resolving copycat problems in visual imitation learning via residual action prediction. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXIX_, pages 392-409. Springer, 2022.
* [7] Felipe Codevilla, Eder Santana, Antonio M Lopez, and Adrien Gaidon. Exploring the limitations of behavior cloning for autonomous driving. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9329-9338, 2019.
* [8] Pim De Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* [9] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In _Conference on robot learning_, pages 1-16. PMLR, 2017.
* [10] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.
* [11] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In _International conference on machine learning_, pages 1180-1189. PMLR, 2015.
* [12] Alessandro Giusti, Jerome Guzzi, Dan Ciresan, Fang-Lin He, Juan P Rodriguez, Flavio Fontana, Matthias Faessler, Christian Forster, Jurgen Schmidhuber, Gianni Di Caro, et al. A machine learning approach to visual perception of forest trails for mobile robots. _IEEE Robotics and Automation Letters_, 1 (2):661-667, 2015.
* [13] Abel Gonzalez-Garcia, Joost van de Weijer, and Yoshua Bengio. Image-to-image translation for cross-domain disentanglement. 2018.
* [14] Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Scholkopf. Measuring statistical dependence with hilbert-schmidt norms. In _Algorithmic Learning Theory: 16th International Conference, ALT 2005, Singapore, October 8-11, 2005. Proceedings 16_, pages 63-77. Springer, 2005.
* [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [16] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. _arXiv preprint arXiv:1808.06670_, 2018.
* [17] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. _Advances in neural information processing systems_, 29, 2016.

* Hwang et al. [2020] HyeongJoo Hwang, Geon-Hyeong Kim, Seunghoon Hong, and Kee-Eung Kim. Variational interaction information maximization for cross-domain disentanglement. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 22479-22491. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/fe663a72b27bdc613873bbb512f6f67-Paper.pdf.
* Kaelbling et al. [1998] Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in partially observable stochastic domains. _Artificial Intelligence_, 1998.
* Kim et al. [2022] Geon-Hyeong Kim, Seokin Seo, Jongmin Lee, Wonseok Jeon, HyeongJoo Hwang, Hongseok Yang, and Kee-Eung Kim. Demodice: Offline imitation learning with supplementary imperfect demonstrations. In _International Conference on Learning Representations_, 2022.
* Kingma and Welling [2014] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun, editors, _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings_, 2014.
* Louizos et al. [2016] Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S. Zemel. The variational fair autoencoder. In Yoshua Bengio and Yann LeCun, editors, _4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings_, 2016. URL http://arxiv.org/abs/1511.00830.
* Micchelli and Pontil [2005] Charles A Micchelli and Massimiliano Pontil. On learning vector-valued functions. _Neural computation_, 17(1):177-204, 2005.
* Mnih et al. [2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. _arXiv preprint arXiv:1312.5602_, 2013.
* Moyer et al. [2018] Daniel Moyer, Shuyang Gao, Rob Brekelmans, Aram Galstyan, and Greg Ver Steeg. Invariant representations without adversarial training. _Advances in Neural Information Processing Systems_, 31, 2018.
* Muller et al. [2005] Urs Muller, Jan Ben, Eric Cosatto, Beat Flepp, and Yann Cun. Off-road obstacle avoidance through end-to-end learning. In Y. Weiss, B. Scholkopf, and J. Platt, editors, _Advances in Neural Information Processing Systems_, volume 18. MIT Press, 2005. URL https://proceedings.neurips.cc/paper_files/paper/2005/file/df1bc5669e8ff5ba45d02fded729feb-Paper.pdf.
* Mulling et al. [2013] Katharina Mulling, Jens Kober, Oliver Kroemer, and Jan Peters. Learning to select and generalize striking movements in robot table tennis. _The International Journal of Robotics Research_, 32(3):263-279, 2013. doi: 10.1177/0278364912472380.
* van den Oord et al. [2018] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* Ortega et al. [2021] Pedro A Ortega, Markus Kunesch, Gregoire Delletang, Tim Genewein, Jordi Grau-Moya, Joel Veness, Jonas Buchli, Jonas Degrave, Bilal Piot, Julien Perolat, et al. Shaking the foundations: delusions in sequence models for interaction and control. _arXiv preprint arXiv:2110.10819_, 2021.
* Park and Muandet [2020] Junhyung Park and Krikamol Muandet. A measure-theoretic approach to kernel conditional mean embeddings. _Advances in neural information processing systems_, 33:21247-21259, 2020.
* Pogodin et al. [2023] Roman Pogodin, Namrata Deka, Yazhe Li, Danica J. Sutherland, Victor Veitch, and Arthur Gretton. Efficient conditionally invariant representation learning. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=dJruFeSRym1.
* Pomerleau [1988] Dean A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In _NIPS_, 1988.
* Pomerleau [1988] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. _Advances in neural information processing systems_, 1, 1988.
* Poole et al. [2019] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In _International Conference on Machine Learning_, 2019.

* Quinzan et al. [2022] Francesco Quinzan, Cecilia Casolo, Krikamol Muandet, Niki Kilbertus, and Yucen Luo. Learning counterfactually invariant predictors. _arXiv preprint arXiv:2207.09768_, 2022.
* Ross et al. [2011] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 627-635. JMLR Workshop and Conference Proceedings, 2011.
* Roy and Boddeti [2019] Proteek Chandan Roy and Vishnu Naresh Boddeti. Mitigating information leakage in image representations: A maximum entropy approach. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2586-2594, 2019.
* Schaal [1999] Stefan Schaal. Is imitation learning the route to humanoid robots? _Trends in Cognitive Sciences_, 3:233-242, 1999.
* Spencer et al. [2021] Jonathan Spencer, Sanjiban Choudhury, Arun Venkatraman, Brian Ziebart, and J Andrew Bagnell. Feedback in imitation learning: The three regimes of covariate shift. _arXiv preprint arXiv:2102.02872_, 2021.
* Swamy et al. [2022] Gokul Swamy, Sanjiban Choudhury, J Bagnell, and Steven Z Wu. Sequence model imitation learning with unobserved contexts. _Advances in Neural Information Processing Systems_, 35:17665-17676, 2022.
* Todorov et al. [2012] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ international conference on intelligent robots and systems_, pages 5026-5033. IEEE, 2012.
* Wang et al. [2019] Dequan Wang, Coline Devin, Qi-Zhi Cai, Philipp Krahenbuhl, and Trevor Darrell. Monocular plan view networks for autonomous driving. In _2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 2876-2883. IEEE, 2019.
* Welleck et al. [2019] Sean Welleck, Kiante Brantley, Hal Daume Iii, and Kyunghyun Cho. Non-monotonic sequential text generation. In _International Conference on Machine Learning_, pages 6716-6726. PMLR, 2019.
* Wen et al. [2020] Chuan Wen, Jierui Lin, Trevor Darrell, Dinesh Jayaraman, and Yang Gao. Fighting copycat agents in behavioral cloning from observation histories. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 2564-2575. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/1b113258af3968af3969ca67e744ff8-Paper.pdf.
* Wen et al. [2021] Chuan Wen, Jierui Lin, Jianing Qian, Yang Gao, and Dinesh Jayaraman. Keyframe-focused visual imitation learning. In _International Conference on Machine Learning_, pages 11123-11133. PMLR, 2021.
* Wen et al. [2022] Chuan Wen, Jianing Qian, Jierui Lin, Jiaye Teng, Dinesh Jayaraman, and Yang Gao. Fighting fire with fire: Avoiding dnn shortcuts through priming. In _International Conference on Machine Learning_, pages 23723-23750. PMLR, 2022.
* Xie et al. [2017] Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. Controllable invariance through adversarial feature learning. _Advances in neural information processing systems_, 30, 2017.
* Xu et al. [2022] Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin. Discriminator-weighted offline imitation learning from suboptimal demonstrations. In _International Conference on Machine Learning_, pages 24725-24742. PMLR, 2022.
* Zhao et al. [2019] Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant representations for domain adaptation. In _International conference on machine learning_, pages 7523-7532. PMLR, 2019.

## Supplementary Material

### Contents

* A Details of HSCIC and Its Empirical Estimation
* A.1 Definitions
* A.2 Empirical estimation
* B Proof of Theorem 1
* C Implementation Details
* C.1 Observation specification
* C.1.1 Hopper
* C.1.2 Walker2d
* C.1.3 HalfCheetah
* C.1.4 Ant
* C.2 Dataset preprocessing
* C.3 Hyperparameter selection
* D Additional Experiments
* D.1 Direct regularization on imitator action
* D.2 Experiment on carla-town
* D.3 Comparison with VIB (Variational Information Bottleneck)
* D.4 Effectiveness on DT (Decision Transformer) policy
* D.5 Quantitative analysis on past action information leakage
* D.6 Details on the leakage-performance correlation
* E Broader Impacts
* F Limitations
* G Computation Resources
* H LicensesDetails of HSCIC and Its Empirical Estimation

### Definitions

**Definition 1**.: _(Reproducing kernel Hilbert space) Let \(\mathcal{H}_{\mathcal{X}}\) be a Hilbert space of functions \(f:\mathcal{X}\rightarrow\mathbb{R}\) functions which the metric is defined by inner product \(\langle\cdot,\cdot\rangle_{\mathcal{H}_{\mathcal{X}}}\). A symmetric function \(k_{\mathcal{X}}:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}\), which is called a reproducing kernel of \(\mathcal{H}_{\mathcal{X}}\), that satisfies_

1. \(\forall x\in\mathcal{X}\)__\(k_{\mathcal{X}}(x,\cdot)\in\mathcal{H}_{\mathcal{X}}\)__
2. \(\forall x\in\mathcal{X},\forall f\in\mathcal{H}_{\mathcal{X}}\)__\(\langle f,k_{\mathcal{X}}(x,\cdot)\rangle_{\mathcal{H}_{\mathcal{X}}}=f(x)\) _(called_ **reproducing property**_)_

_A space \(\mathcal{H}_{\mathcal{X}}\) is called a_ **reproducing kernel Hilbert spaces (RKHS)** _corresponding a reproducing kernel \(k_{\mathcal{X}}\)._

**Definition 2**.: _(Kernel Mean Embedding) Given a distribution \(P_{X}\) on \(\mathcal{X}\), assume \(\int_{\mathcal{X}}\sqrt{k_{\mathcal{X}}(x,x)}dP_{X}(x)<\infty\). We define_ **the kernel mean embedding**_\(\mu_{P_{X}}\)_of \(P_{X}\) as \(\mu_{P_{X}}(\cdot)=\int_{\mathcal{X}}k_{\mathcal{X}}(x,\cdot)dP_{X}(x)\)._

**Definition 3**.: _(Maximum Mean Discrepancy) Given distributions \(P,Q\) on \(\mathcal{X}\) and suppose kernel mean embeddings of \(P,Q\) exist on \(\mathcal{H}\), denote \(\mu_{P},\mu_{Q}\). Then, the_ **maximum mean discrepancy (MMD)** _between \(P_{X},Q_{Y}\) is defined as follows:_

\[\mathrm{MMD}(P,Q;\mathcal{H})=\|\mu_{P}-\mu_{Q}\|_{\mathcal{H}}\]

**Definition 4**.: _(Hilbert-Schmidt Norm and Operator) Let \(\mathcal{F},\mathcal{G}\) be Hilbert spaces. Define \((f_{i})_{i\in I},(g_{j})_{j\in J}\) to be orthonormal basis for \(\mathcal{F},\mathcal{G}\) respectively. Define two linear operators \(L:\mathcal{G}\rightarrow\mathcal{F},M:\mathcal{G}\rightarrow\mathcal{F}\)._

1. _The_ **Hilbert-Schmidt norm** _of the operator_ \(L\) _is defined as:_ \[\|L\|_{\mathrm{HS}}^{2} =\sum_{j\in J}\|Lg_{j}\|_{\mathcal{F}}^{2}\] \[=\sum_{j\in J}\sum_{i\in I}|\langle Lg_{j},f_{i}\rangle_{ \mathcal{F}}|^{2}\]
2. _The operator_ \(L\) _is called_ **Hilbert-Schmidt Operator** _when_ \(\|L\|_{\mathrm{HS}}^{2}\) _is finite._

**Definition 5**.: _(Cross-covariance Operator) Given RKHS \(\mathcal{F},\mathcal{G}\) with kernels \(k,l\), feature maps \(\phi,\psi\) respectively. Define the uncentered covariance operator \(\tilde{C}_{XY}:\mathcal{G}\rightarrow\mathcal{F}\) and the_ **cross-covariance operator**_\(C_{XY}:\mathcal{G}\rightarrow\mathcal{F}\) as follows:_

\[\langle\tilde{C}_{XY},A\rangle_{HS} =\mathbb{E}_{x,y}\langle\phi(x)\otimes\psi(y),A\rangle_{\mathrm{ HS}}\] \[C_{XY} :=\tilde{C}_{XY}-\mu_{X}\otimes\mu_{Y}\]

**Definition 6**.: _(Hilbert-Schmidt Independence Criterion) The_ **Hilbert-Schmidt Independence Criterion (HSIC)** _is defined as Hilbert-Schmidt norm of the cross-covariance operator \(C_{XY}\), i.e., \(\mathrm{HSIC}(P_{XY};\mathcal{F},\mathcal{G}):=\|C_{XY}\|_{\mathrm{HS}}^{2}\). Equivalently, we can define HSIC as MMD between \(P_{XY}\) and \(P_{X}P_{Y}\) with the product kernel, i.e._

\[\mathrm{HSIC}(P_{XY};\mathcal{F},\mathcal{G})=\mathrm{MMD}^{2}(P_{XY},P_{X}P_{ Y};\mathcal{H}_{\kappa}),\]

_where \(\kappa((x,y),(x^{\prime},y^{\prime}))=k(x,x^{\prime})l(y,y^{\prime})\)._

**Definition 7**.: _(Conditional Mean Embedding) Assuming a random variable \(X\) satisfies \(\int_{\mathcal{X}}\sqrt{k(x,x)}dP<\infty\). Define the_ **conditional mean embedding** _of \(X\) given \(Z\) as \(\mu_{P_{X|Z}}(\cdot):=\mathbb{E}_{X|Z}[k_{\mathcal{X}}(X,\cdot)|Z]\)._Note that \(\mu_{P_{X|Z}}\) is a random variable. In the main text of our paper, we denote the realization of \(\mu_{P_{X|Z}}\) with \(Z=z\) as \(\mu_{P_{X|Z=z}}\).

**Definition 8**.: _(Hilbert-Schmidt Conditional Independence Criterion) Define the **Hilbert-Schmidt Conditional Independence Criterion (HSCIC)** between \(X\) and \(Y\) given \(Z\) to be_

\[\mathrm{HSCIC}(X,Y|Z)=\|\mu_{P_{XY|Z}}-\mu_{P_{X|Z}}\otimes\mu_{P_{Y|Z}}\|_{ \mathcal{H}_{X\otimes\mathcal{H}_{Y}}}^{2}.\]

### Empirical estimation

We start to explain from decoupling the conditional mean embedding into their deterministic function and conditional random variable.

**Lemma 1**.: _(Decomposition of conditional mean embedding, Theorem 4.1 of [30]) There exists a deterministic function \(F_{P_{X|Z}}:\mathcal{Z}\rightarrow\mathcal{H}_{\mathcal{X}}\), which satisfies \(\mu_{P_{X|Z}}=F_{P_{X|Z}}\circ Z\)._

By this lemma, we can decompose conditional mean embeddings and HSCIC into some deterministic function and conditioned random variable \(Z\), i.e., \(\mu_{P_{X|Z}}=F_{P_{X|Z}}\circ Z,\mu_{P_{Y|Z}}=F_{P_{Y|Z}}\circ Z,\mu_{P_{XY|Z} }=F_{P_{XY|Z}}\circ Z,\mathrm{HSCIC}(X,Y|Z)=F_{\mathrm{HSCIC}_{X,Y|Z}}\circ Z\) respectively. By using vector-valued RKHS regression with a regularization parameter \(\lambda>0\) and the representer theorem [23], we can obtain closed-form estimates of \(F_{P_{X|Z}},F_{P_{Y|Z}},F_{P_{XY|Z}}\) as follows (see [30] for more details):

\[\mathbf{W}_{Z}:= (\mathbf{K}_{Z}+n\lambda\mathbf{I})^{-1}\] \[\hat{F}_{P_{X|Z}}(z)= \mathbf{k}_{Z}^{\top}(z)\mathbf{W}_{Z}\mathbf{k}_{X}(\cdot)\] \[\hat{F}_{P_{Y|Z}}(z)= \mathbf{k}_{Z}^{\top}(z)\mathbf{W}_{Z}\mathbf{k}_{Y}(\cdot)\] \[\hat{F}_{P_{XY|Z}}(z)= \mathbf{k}_{Z}^{\top}(z)\mathbf{W}_{Z}(\mathbf{k}_{X}(\cdot) \odot\mathbf{k}_{Y}(\cdot))\]

where \(\mathbf{k}_{X}(\cdot):=(k_{\mathcal{X}}(x_{1},\cdot),...,k_{\mathcal{X}}(x_{n},\cdot))^{\top},\mathbf{k}_{Y}(\cdot):=(k_{\mathcal{Y}}(y_{1},\cdot),...,k_{ \mathcal{Y}}(y_{n},\cdot))^{\top}\), \(\mathbf{k}_{Z}(\cdot):=(k_{\mathcal{Z}}(z_{1},\cdot),...,k_{\mathcal{Z}}(z_{n},\cdot))^{\top},[\mathbf{K}_{Z}]_{ij}:=k_{Z}(z_{i},z_{j})\) and \(\odot\) is the element-wise multiplication operator of matrices. By plugging-in conditional mean embedding estimators, we can obtain a closed-form estimator of \(F_{\mathrm{HSCIC}_{X,Y|Z}}\) as follows:

\[\hat{F}_{\mathrm{HSCIC}_{X,Y|Z}}(z):= \|\hat{F}_{P_{XY|Z}}(z)-\hat{F}_{P_{X|Z}}(z)\otimes\hat{F}_{P_{Y|Z }}(z)\|_{\mathcal{H}_{X\otimes\mathcal{H}_{Y}}}^{2}\] \[= \mathbf{k}_{Z}^{\top}(z)\mathbf{W}_{Z}(\mathbf{K}_{X}\odot \mathbf{K}_{Y})\mathbf{W}_{Z}^{\top}\mathbf{k}_{Z}(z)\] \[-2\mathbf{k}_{Z}^{\top}(z)\mathbf{W}_{Z}(\mathbf{K}_{X}\mathbf{W} _{Z}^{\top}\mathbf{k}_{Z}(z)\odot\mathbf{K}_{Y}\mathbf{W}_{Z}^{\top}\mathbf{k} _{Z}(z))\] \[+(\mathbf{k}_{Z}^{\top}(z)\mathbf{W}_{Z}\mathbf{K}_{X}\mathbf{W} _{Z}^{\top}\mathbf{k}_{Z}(z))(\mathbf{k}_{Z}^{\top}(z)\mathbf{W}_{Z}\mathbf{K} _{Y}\mathbf{W}_{Z}^{\top}\mathbf{k}_{Z}(z))\] \[\widehat{\mathrm{HSCIC}}(X,Y|Z)= \frac{1}{n}\sum_{i=1}^{n}\hat{F}_{\mathrm{HSCIC}_{X,Y|Z}}(z_{i})\] \[= \frac{1}{n}\text{trace}\Big{(}\mathbf{K}_{Z}^{\top}\mathbf{W}_{Z}( \mathbf{K}_{X}\odot\mathbf{K}_{Y})\mathbf{W}_{Z}^{\top}\mathbf{K}_{Z}\] \[\qquad-2\mathbf{K}_{Z}^{\top}\mathbf{W}_{Z}(\mathbf{K}_{X}\mathbf{W} _{Z}^{\top}\mathbf{K}_{Z}\odot\mathbf{K}_{Y}\mathbf{W}_{Z}^{\top}\mathbf{K}_{Z})\] (8) \[\qquad+(\mathbf{K}_{Z}^{\top}\mathbf{W}_{Z}\mathbf{K}_{X}\mathbf{W} _{Z}^{\top}\mathbf{K}_{Z})\odot(\mathbf{K}_{Z}^{\top}\mathbf{W}_{Z}\mathbf{K}_{Y }\mathbf{W}_{Z}^{\top}\mathbf{K}_{Z})\Big{)}\]

where \([\mathbf{K}_{X}]_{ij}:=k_{\mathcal{X}}(x_{i},x_{j}),[\mathbf{K}_{Y}]_{ij}:=k_{ \mathcal{Y}}(y_{i},y_{j})\).

Proof of Theorem 1

Proof.: By the chain rule of MI, following equalities hold:

\[I(\varphi_{t};a_{t-1}^{E}\mid a_{t}^{E}) =I(a_{t-1}^{E};\varphi_{t},a_{t}^{E})-I(a_{t-1}^{E};a_{t}^{E})\] \[I(a_{t}^{I};a_{t-1}^{E}\mid a_{t}^{E}) =I(a_{t-1}^{E};a_{t}^{I},a_{t}^{E})-I(a_{t-1}^{E};a_{t}^{E})\]

Thus, \(I(\varphi_{t};a_{t-1}^{E}\mid a_{t}^{E})\geq I(a_{t}^{I};a_{t-1}^{E}\mid a_{t}^ {E})\) if and only if \(I(a_{t-1}^{E};\varphi_{t},a_{t}^{E})\geq I(a_{t-1}^{E};a_{t}^{I},a_{t}^{E})\).

By the chain rule of MI,

\[I(a_{t-1}^{E};\varphi_{t},a_{t}^{I},a_{t}^{E}) =I(a_{t-1}^{E};\varphi_{t},a_{t}^{E})+I(a_{t-1}^{E};a_{t}^{E}+ \widetilde{\varphi_{t},a_{t}^{E}})\] \[=I(a_{t-1}^{E};a_{t}^{I},a_{t}^{E})+\widetilde{I(a_{t-1}^{E}; \varphi_{t}\mid a_{t}^{I},a_{t}^{E})}\] \[\geq I(a_{t-1}^{E};a_{t}^{I},a_{t}^{E})\]

Thus, \(I(\varphi_{t};a_{t-1}^{E}\mid a_{t}^{E})\geq I(a_{t}^{I};a_{t-1}^{E}\mid a_{t} ^{E})\:\forall t\geq 0\). 

Directly regularizing the imitator policy to enforce conditional independence may cause harmful results of training. This is because any policy that ensures the conditional independence does not necessarily yield optimal behavior. For instance, constant actions or actions sampled from uniform random distribution are independent of other random variables and thus satisfy Eq. (2), although they are not optimal in general. Based on Theorem 1, we adjust the representation of policy to inject the conditional independence. The effectiveness of our approach is compared with direct action regularization through an empirical study. The results are provided in Table 9 of Section D.1.

Figure 3: Graphical model of behavior cloning with observation histories.

Implementation Details

This section provides detailed specifications and implementation settings.

### Observation specification

Based on the observation specification from Open AI Gym documentation5, we take a partial observation in each task by excluding certain variables (e.g. velocity information) in the original state variables. We mark the variables we used in our observation as \(\bigcirc\) and not used are represented as \(\times\) in the observation column of the corresponding table.

Footnote 5: https://www.gymlibrary.dev/environments/mujoco/

#### c.1.1 Hopper

#### c.1.2 Walker2d

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Index** & **Description** & **Unit** & **Observation** \\ \hline
0 & z-coordinate of the top & position (m) & \\
1 & angle of the top & angle (rad) & \\
2 & angle of the thigh joint & angle (rad) & \\
3 & angle of the leg joint & angle (rad) & \\
4 & angle of the foot joint & angle (rad) & \\
5 & velocity of the x-coordinate of the top & velocity (m/s) & \(\times\) \\
6 & velocity of the z-coordinate of the top & velocity (m/s) & \(\times\) \\
7 & angular velocity of the angle of the top & angular velocity (rad/s) & \(\times\) \\
8 & angular velocity of the thigh hinge & angular velocity (rad/s) & \(\times\) \\
9 & angular velocity of the leg hinge & angular velocity (rad/s) & \(\times\) \\
10 & angular velocity of the foot hinge & angular velocity (rad/s) & \(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Composition of observation variables (hopper)

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Index** & **Description** & **Unit** & **Observation** \\ \hline
0 & z-coordinate of the top & position (m) & \\
1 & angle of the top & angle (rad) & \\
2 & angle of the thigh joint & angle (rad) & \\
3 & angle of the leg joint & angle (rad) & \\
4 & angle of the foot joint & angle (rad) & \\
5 & angle of the left thigh joint & angle (rad) & \\
6 & angle of the left leg joint & angle (rad) & \\
7 & angle of the left foot joint & angle (rad) & \\
8 & velocity of the x-coordinate of the top & velocity (m/s) & \(\times\) \\
9 & velocity of the z-coordinate of the top & velocity (m/s) & \(\times\) \\
10 & angular velocity of the angle of the top & angular velocity (rad/s) & \(\times\) \\
11 & angular velocity of the thigh hinge & angular velocity (rad/s) & \(\times\) \\
12 & angular velocity of the leg hinge & angular velocity (rad/s) & \(\times\) \\
13 & angular velocity of the foot hinge & angular velocity (rad/s) & \(\times\) \\
14 & angular velocity of the thigh hinge & angular velocity (rad/s) & \(\times\) \\
15 & angular velocity of the leg hinge & angular velocity (rad/s) & \(\times\) \\
16 & angular velocity of the foot hinge & angular velocity (rad/s) & \(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Composition of observation variables (walker2d)

#### c.1.3 HalfCheetah

Open AI Gym documentation does not match with the XML file of halfcheetah6, we denote both descriptions on the following tables.

Footnote 6: https://github.com/openai/gym/blob/master/gym/envs/mujoco/assets/half_cheetah.xml

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Index** & **Description** & **Unit** & **Observation** \\ \hline
0 & z-coordinate of the front tip & position (m) & \\
1 & angle of the front tip & angle (rad) & \\
2 & angle of the second rotor & angle (rad) & \\
3 & angle of the second rotor & angle (rad) & \\
4 & velocity of the tip along the x-axis & velocity (m/s) & \(\times\) \\
5 & velocity of the tip along the y-axis & velocity (m/s) & \(\times\) \\
6 & angular velocity of the front tip & angular velocity (rad/s) & \(\times\) \\
7 & angular velocity of the second rotor & angular velocity (rad/s) & \(\times\) \\
8 & x-coordinate of the front tip & position (m) & \\
9 & y-coordinate of the front tip & position (m) & \\
10 & angle of the front tip & angle (rad) & \\
11 & angle of the second rotor & angle (rad) & \\
12 & angle of the second rotor & angle (rad) & \\
13 & velocity of the tip along the x-axis & velocity (m/s) & \(\times\) \\
14 & velocity of the tip along the y-axis & velocity (m/s) & \(\times\) \\
15 & angular velocity of the front tip & angular velocity (rad/s) & \(\times\) \\
16 & angular velocity of the second rotor & angular velocity (rad/s) & \(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Composition of observation variables (halfcheetah) on Gym Documentation.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Index** & **Description** & **Unit** & **Observation** \\ \hline

[MISSING_PAGE_POST]

oot & angular velocity (rad/s) & \(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Composition of observation variables (halfcheetah) on the XML file.

#### c.1.4 Ant

#### Dataset preprocessing

MuJoCo TasksWe utilized the expert dataset provided in D4RL benchmark that has a suffix -expert-v2. For all experiment, we configured partial observations from the original data based on Section C.1. These observations are standardized with statistics from the entire dataset. Each dataset in D4RL benchmark has a total of 1M transitions. Given the size of the training data \(D\), we take the first \(D\) samples. Among the last 100K samples in the entire dataset, we also sampled heldout data which are not included in the training data. Given a stacksize \(w\in\{2,4\}\), we configure each transition as (observation history, action, past action) tuple. For the main performance evaluation experiment (Table 1), we use 30K transitions for training.

CARLA TasksWe extract features from observation images using the pretrained network ResNet34 [15] and consider these features as policy inputs during training. For the performance evaluation (Table 1, 10), we utilize 1K and 100K transitions for carla-lane, carla-town respectively.

### Hyperparameter selection

This section outlines the settings of the hyperparameters for each method. To ensure a fair comparison, we aim to maintain consistent network architecture and policy-related hyperparameters, whenever feasible. For all baselines and our method, we employ a policy architecture consisting of fully-connected layers with [128, 64, 128] hidden units. The layer with 64 units is treated as a policy representation, denoted as \(\varphi\).

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Num** & **Description** & **Unit** & **Observation** \\ \hline
0 & z-coordinate of the torso (centre) & position (m) & \\
1 & x-orientation of the torso (centre) & angle (rad) & \\
2 & y-orientation of the torso (centre) & angle (rad) & \\
3 & z-orientation of the torso (centre) & angle (rad) & \\
4 & w-orientation of the torso (centre) & angle (rad) & \\
5 & angle between torso and first link on front left & angle (rad) & \\
6 & angle between the two links on the front left & angle (rad) & \\
7 & angle between torso and first link on front right & angle (rad) & \\
8 & angle between the two links on the front right & angle (rad) & \\
9 & angle between torso and first link on back left & angle (rad) & \\
10 & angle between the two links on the back left & angle (rad) & \\
11 & angle between torso and first link on back right & angle (rad) & \\
12 & angle between the two links on the back right & angle (rad) & \\
13 & x-coordinate velocity of the torso & velocity (m/s) & \\
14 & y-coordinate velocity of the torso & velocity (m/s) & \\
15 & z-coordinate velocity of the torso & velocity (m/s) & \\
16 & x-coordinate angular velocity of the torso & angular velocity (rad/s) & \\
17 & y-coordinate angular velocity of the torso & angular velocity (rad/s) & \\
18 & z-coordinate angular velocity of the torso & angular velocity (rad/s) & \\
19 & angular velocity of angle between torso and front left link & angular velocity (rad/s) & \\
20 & angular velocity of the angle between front left links & angular velocity (rad/s) & \\
21 & angular velocity of angle between torso and front right link & angular velocity (rad/s) & \\
22 & angular velocity of the angle between front link links & angular velocity (rad/s) & \\
23 & angular velocity of angle between torso and back left link & angular velocity (rad/s) & \\
24 & angular velocity of the angle between back left links & angular velocity (rad/s) & \\
25 & angular velocity of angle between torso and back right link & angular velocity (rad/s) & \\
26 & angular velocity of the angle between back right links & angular velocity (rad/s) & \\
27-110 & contact forces applied to the center of mass of each of the links & force / torque & \\ \hline \hline \end{tabular}
\end{table}
Table 6: Composition of observation variables (ant)Kf [45]We used a softmax function as a weighting function of the weighted BC in MuJoCo tasks and a step function in CARLA tasks, aligning with the the original paper's experiments. We adopt the action prediction network for the additional network. Additionally, we tuned hyperparameters for the the softmax temperature in MuJoCo tasks, analogous to \(\alpha\). In CARLA tasks, we used the same hyperparameters (threshold=0.1, weight=5) consistent with the original paper [45].

PrimeNet [46]We treat observation histories as raw inputs and individual last observations as key inputs, consistent with the approach used in the original experiments. The architecture of the prime network is implemented as the same with the policy.

RAP [6]RAP incorporates two streams of policy representation: (1) one stream derived from observation history (referred to as the memory extraction stream) and (2) the other originating from a single observation (referred to as the policy stream). For the memory extraction stream, we use an architecture of [128, 64, 300] hidden units that outputs predictions of a residual of expert actions. The layer with 64 units is employed as a representation. For the policy stream, we similarly utilize an architecture of [128, 64(+64), 128] which outputs a policy action. The middle units indicate that the policy utilizes both the single observation representation and the memory-extracted representation (with a stop-gradient layer) as inputs.

A summary of hyperparameters is provided in Table 7. The first 3 rows of the table display the hyperparameters universally applied across all methods. The next 4 rows detail hyperparameters for methods that incorporate additional neural networks. The last 2 rows of the table present hyperparameters specific to our method, encompassing hyperparameters for HSCIC estimates.

Optimal \(\alpha\)As discussed in Section 5.2, we train 5 policies incorporating different regularization coefficients \(\alpha\) within the set \(\{0.01,0.1,1,10,100\}\) for MuJoCo tasks and \(\{0.001,0.1,10,1000\}\) for CARLA task. We then select the best \(\alpha\) for each problem setting. Table 8 shows the chosen values of \(\alpha\) for each method and each problem setting, as utilized in the experimental results of the paper.

\begin{table}
\begin{tabular}{l|l l l l l} \hline
**Hyperparameter** & **KF** & **PrimeNet** & **RAP** & **FCA** & **MINE** & **PARL (Ours)** \\ \hline Policy distribution & \multicolumn{5}{c}{Tanh Normal} \\ Batch size & \multicolumn{5}{c}{1024} \\ BC learning rate & \multicolumn{5}{c}{3e-4} \\ \hline Additional network & \multicolumn{5}{c}{1028,128} \\ hidden units & [128, 128] & [128, 64, 128] & [128, 64, 300] & [300] & [100, 100] & - \\ num of inner updates & 1 & 1 & 1 & 5 & 5 & - \\ learning rate & 3e-4 & 3e-4 & 3e-4 & 1e-4 & 1e-4 & - \\ IB coefficient & - & - & - & 0.01 & - & - \\ \hline Kernel bandwidth \(\sigma^{2}\) & - & - & - & - & - & 1. \\ Ridge coefficient \(\lambda\) & - & - & - & - & - & 1e-5 \\ \hline \end{tabular}
\end{table}
Table 7: The summary of hyperparameters.

\begin{table}
\begin{tabular}{l|l l l l} \hline
**Problem Setting** & **KF** & **FCA** & **MINE** & **Ours** \\ \hline hopper-W2 & 0.01 & 0.1 & 1 & 10 \\ hopper-W4 & 0.01 & 0.1 & 0.01 & 10 \\ walker2d-W2 & 0.01 & 0.1 & 1 & 100 \\ walker2d-W4 & 0.1 & 0.1 & 1 & 100 \\ halfcheetah-W2 & 0.01 & 0.1 & 1 & 10 \\ halfcheetah-W4 & 0.1 & 0.1 & 0.1 & 10 \\ ant-W2 & 1 & 0.1 & 0.01 & 10 \\ ant-W4 & 0.01 & 1 & 0.01 & 10 \\ \hline carla-lane-W3 & - & 0.001 & 10 & 1000 \\ \hline \end{tabular}
\end{table}
Table 8: The summary of the best regularization coefficient \(\alpha\).

[MISSING_PAGE_FAIL:22]

### Comparison with VIB (Variational Information Bottleneck)

To highlight the leakage of past action information is not trivially prevented by general representation regularization methods, we compare our approach with VIB [1], a representative method. We train BC with VIB with two different \(\beta\) values, specifically, \(\beta\in\{10^{-3},10^{-5}\}\), 5 seeds for each problem setting. whereas our method demonstrates significant improvements in 6 out of 8 problem settings.

### Effectiveness on DT (Decision Transformer) policy

To assess the efficacy of our method in regularizing policies with complex network architectures, we conducted additional experiments employing the Decision Transformer (DT) [5], one of the prominent offline RL methods. Given that reward information is absent in the offline IL dataset, we inserted a reward input of 0 into DT's structure to retain its original configuration. Our regularization approach, termed DT-PALR, was applied to the last hidden state of DT. We evaluated both the standard DT and DT-PALR across 3 POMDP versions of MuJoCo tasks, consistent with the scenarios outlined in Table 1.

The results are presented in Table 12. With the exception of the halfcheetah task, it becomes evident that DT's performance lags behind that of BC, as demonstrated in Table 1. This divergence might be attributed to DT's utilization of a larger network size, rendering it more susceptible to capturing spurious causal relationships in tasks where access to complete states and rewards is restricted. Encouragingly, our results indicate a substantial enhancement in the performance of DT for the hopper and walker2d tasks when subjected to the DT-PALR method. This observation strongly suggests the adaptability of our approach to intricate architectures.

\begin{table}
\begin{tabular}{l c|c c c c} \hline \hline
**Task** & w & **BC** & **PALR (Ours)** & **VIB (\(\beta=10^{-3}\) )** & **VIB (\(\beta=10^{-5}\))** \\ \hline \hline hopper & 2 & 32.47 \(\pm\) 2.85 & **42.01 \(\pm\) 2.44** & 30.57 \(\pm\) 1.61 & 27.89 \(\pm\) 1.55 \\  & 4 & 47.65 \(\pm\) 3.43 & **58.39 \(\pm\) 2.76** & 51.20 \(\pm\) 2.18 & 46.59 \(\pm\) 2.26 \\ \hline walker2d & 2 & 53.04 \(\pm\) 2.69 & **79.83 \(\pm\) 2.29** & 44.28 \(\pm\) 2.32 & 35.98 \(\pm\) 4.69 \\  & 4 & 63.15 \(\pm\) 6.28 & **83.42 \(\pm\) 5.43** & 67.78 \(\pm\) 2.67 & 65.99 \(\pm\) 1.97 \\ \hline halfcheetah & 2 & 74.08 \(\pm\) 2.33 & **86.44 \(\pm\) 1.09** & 73.76 \(\pm\) 2.401 & 75.43 \(\pm\) 1.77 \\  & 4 & 68.35 \(\pm\) 2.60 & **79.05 \(\pm\) 4.28** & 67.89 \(\pm\) 1.52 & **73.67 \(\pm\) 2.86** \\ \hline ant & 2 & **56.25 \(\pm\) 3.45** & **59.57 \(\pm\) 3.03** & **59.54 \(\pm\) 1.65** & **55.29 \(\pm\) 4.27** \\  & 4 & **64.39 \(\pm\) 1.77** & **64.64 \(\pm\) 2.53** & **61.63 \(\pm\) 2.77** & **61.25 \(\pm\) 2.88** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Performance comparison of BC, our method (PALR) and BC with VIB (Variational Information Bottleneck). The normalized scores averaged over the final 50 evaluations during training and we report mean and standard error over 5 different seeds. The method with the highest mean score and its competitive methods (within standard error) are highlighted in bold in each setting.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline
**Method** & **hopper** & **walker2d** & **halfcheetah** \\ \hline \hline DT & 20.68 \(\pm\) 5.25 & 23.93 \(\pm\) 4.13 & 94.10 \(\pm\) 3.54 \\ DT-PALR & 26.09 \(\pm\) 6.59 & 32.58 \(\pm\) 5.63 & 96.05 \(\pm\) 3.60 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Effectiveness of PALR on Decision Transformer (DT) architecture. We train DT over 3 MuJoCo tasks based on our observation configurations and apply PALR to the last hidden state.

[MISSING_PAGE_EMPTY:24]

### Details on the leakage-performance correlation

We first clarify the relationship between the number of training data and HSCIC scores in Section 5.1. To clarify, we plot the correlation into Figure 4. The correlation indicates that when the training data is insufficient, the problem of past action information leakage becomes more severe. This phenomenon can be attributed to the higher risk of overfitting in cases with smaller training instances, which can lead to the capture of false causal relationships within the training data. These findings align with similar results reported in [8] (see Figure 4 in [8]).

Furthermore, to provide more insights for understanding Figure 1, we plot all points that indicate the normalized HSCIC and the normalized score of each BC policy in Figure 5. In addition, we calculate the sample Pearson's correlation coefficients using every point of each task (the first 4 columns) and all tasks (the last column) on Table 15.

Figure 4: Relationship between the number of training data and the normalized HSCIC score.

Figure 5: Scatterplot that shows a correlation between HSCIC estimates and performance. Each small point indicates HSCIC and the performance of each policy, and the diamond point (\(\diamond\)) indicates the mean point of each group (which consists of policies trained with the same dataset size).

## Appendix E Broader Impacts

This work primarily addresses the reduction of the pervasive past action leakage effect in imitation learning with observation histories. Notably, our findings suggest potential vulnerabilities that may be exploited by adversarial actors. For instance, malicious attackers could introduce the conditional dependencies into the system, leading to potentially catastrophic outcomes, such as repeating the same actions. This will produce potentially irreversible consequences on the system, especially for security-critic tasks such as autonomous driving and medical devices, or industrial automation. Therefore, understanding and mitigating this risk is crucial for the safe and efficient operation of such systems.

## Appendix F Limitations

While our approach is easily applicable to real-world tasks, the range of experiments in this paper is limited to robotics simulation. A broader scope of experiments, including more realistic observation settings like image-based or noise-inclusive observations, could provide a stronger argument for the effectiveness and real-world applicability of our method. Additionally, our method assumes that control-relevant information is sufficiently captured in the observation histories. However, this assumption might not hold in some real-world scenarios where key control information is missing and not retrievable from the observation histories. In such cases, the learning model requires additional information, inductive bias or expert knowledge to function effectively.

## Appendix G Computation Resources

For our experiments, we used a cluster system with 20 nodes that have the following system specs:

* CPU: Intel i7-9700K CPU (3.60GHz)
* GPU: TITAN Xp (VRAM 12 GB)

## Appendix H Licenses

Our code has been developed based on publicly available code repositories released under MIT licenses. The code used to generate our experimental results also follows to the MIT license. For more detailed information, see the README.md and LICENSE files included in our code files.

\begin{table}
\begin{tabular}{c c c c|c} \hline \hline
**hopper** & **walker2d** & **halfcheetah** & **ant** & **Total** \\ \hline -0.8358 & -0.8163 & -0.8564 & -0.9273 & -0.8130 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Sample Pearsonâ€™s correlation coefficients between the normalized HSCIC and the normalized score.