# MediQ: Question-Asking LLMs and a Benchmark

for Reliable Interactive Clinical Reasoning

 Shuyue Stella Li\({}^{1}\) Vidhisha Balachandran\({}^{2}\) Shangbin Feng\({}^{1}\) Jonathan S. Ilgen\({}^{1}\) Emma Pierson\({}^{3}\) Pang Wei Koh\({}^{1,4}\) Yulia Tsvetkov\({}^{1}\)

\({}^{1}\)University of Washington \({}^{2}\)Carnegie Mellon University \({}^{3}\)Cornell Tech

\({}^{4}\)Allen Institute for AI

stelli@cs.washington.edu

https://github.com/stellalsy/mediQ

https://huggingface.co/datasets/stellalisy/mediQ

###### Abstract

Users typically engage with LLMs interactively, yet most existing benchmarks evaluate them in a static, single-turn format, posing reliability concerns in interactive scenarios. We identify a key obstacle towards reliability: LLMs are trained to answer any question, even with incomplete context or insufficient knowledge. In this paper, we propose to change the static paradigm to an interactive one, develop systems that _proactively ask questions_ to gather more information and respond reliably, and introduce an benchmark--**MediQ**--to evaluate question-asking ability in LLMs. MediQ simulates clinical interactions consisting of a Patient System and an adaptive Expert System; with potentially incomplete initial information, the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details via follow-up questions. We provide a pipeline to convert single-turn medical benchmarks into an interactive format. Our results show that directly prompting state-of-the-art LLMs to ask questions _degrades_ performance, indicating that adapting LLMs to proactive information-seeking settings is nontrivial. We experiment with abstention strategies to better estimate model confidence and decide when to ask questions, improving diagnostic accuracy by 22.3%; however, performance still lags compared to an (unrealistic in practice) upper bound with complete information upfront. Further analyses show improved interactive performance with filtering irrelevant contexts and reformatting conversations. Overall, we introduce a novel problem towards LLM reliability, an interactive MediQ benchmark and a novel question-asking system, and highlight directions to extend LLMs' information-seeking abilities in critical domains.

## 1 Introduction

General-purpose large language models (LLMs) are designed to serve a broad audience by following instructions and providing the most likely and general answers (Brown et al., 2020; Ouyang et al., 2022). However, in high-stakes decision making scenarios such as clinical conversations, LLM assistants can be harmful if they provide general responses instead of gathering missing information to make informed decisions. As shown in Figure 1, standard medical question-answering (QA) tasks are formulated in a single-turn setup where all necessary information is provided upfront, and the model is not expected to interact with users (Jin et al., 2021; Pal et al., 2022; Jin et al., 2019; Hendrycks et al., 2020). This QA paradigm diverges from real-world scenarios, where users may provide **incomplete information**, and effective decision-making often requires an **investigative process** involving follow-up questions to clarify and gather necessary details (Trimble and Hamilton, 2016; Bornstein and Emler, 2001; Masic, 2022).

This gap between existing benchmarks and reality calls for a paradigm shift to designing systems adept at navigating high-stakes interactive scenarios. Focusing on clinical interactions where context is often incomplete, we introduce **MediQ**, an interactive benchmark for **m**edical **e**valuation with **d**ynamic information-seeking **q**uestions, to address limitations of static single-turn QA benchmarks (Figure 2). Unlike conventional systems, which assume that all necessary information is readily available, MediQ acknowledges the inherent uncertainty in medical consultations where a typical patient does not have the expertise to distill all necessary and relevant information they need to provide. To achieve this, MediQ comprises two components: a **Patient system** that simulates a patient and responds to follow-up questions, and an **Expert system** that serves as a doctor's assistant and asks questions to the patient before making a medical decision. In this **interactive clinical reasoning task**, a successful information-seeking Expert should decide, at each turn, whether it has enough information to provide a confident answer; if not, it should ask a follow-up question.

We convert two medical QA datasets, MedQA (Jin et al., 2021) and Craft-MD (Johri et al., 2023, 2024), into an interactive benchmark by parsing the patient records to only provide partial information in the beginning. We first develop and validate a Patient system that accurately answers Expert inquiries by retrieving the correct facts from the patient record. We then benchmark Expert systems based on state-of-the-art (SOTA) LLMs, including Llama-3 (Touvron et al., 2023), GPT-3.5 (Brown et al., 2020) and GPT-4 (OpenAI et al., 2024), to evaluate their proactive information seeking ability. It is striking that prompting these models to ask questions results in an 11.3% accuracy drop compared to starting with the same limited information and asking no questions, showing that adapting LLMs to interactive information-seeking settings is nontrivial. A key challenge is deciding when to ask a follow-up question instead of directly providing an answer. With confidence estimation strategies such as rationale generation and self-consistency, we improve Expert performance by 22.3%, although a 10.3% gap remains compared to an upper bound when full information is presented at once.

Our results show that while SOTA LLMs perform relatively well with complete information, they struggle to proactively seek missing information in a more realistic, interactive settings with incomplete initial information. By providing a modular, interactive benchmark, we hope to facilitate the development of reliable LLM assistants for complex decision-making in healthcare and other high-stakes domains. Our main contributions are:

1. We identify the critical problem of **information-seeking questions** in reliable interactive LLM assistants. We propose a paradigm shift and a practical conversion pipeline from standard single-turn benchmarks into interactive settings with incomplete initial information.
2. We develop the **MediQ Benchmark** to simulate more realistic clinical interactions between a Patient System and an Expert System. We rigorously develop and test the Patient System to benchmark any Expert's information-seeking and clinical decision-making abilities.
3. We show that SOTA LLMs such as Llama-3-Instruct, GPT-3.5 and GPT-4 struggle at proactive information seeking, revealing a significant gap in this area.
4. We propose **MediQ-Expert**, our best Expert system with novel abstaining capabilities to reduce unconfident answers, to partially close the gap between the more realistic incomplete information setup and the existing full information setup.

Figure 1: Information Seeking Task. In standard medical QA tasks (left), all necessary information is given to the assistant model at the same time. When given partial information, current LLMs only provides general responses (middle). In a more realistic scenario (right), the presentation of patient information relies on proactive elicitation from the doctor; our proposed MediQ framework operationalizes this scenario.

## 2 MediQ: Dynamic Medical Consultation Framework Overview

Task DefinitionThe dynamic medical consultation task simulates the iterative nature of real-world clinical interactions. This task starts by providing an initial patient description \(k_{0}\) of their conditions to the Expert system. The initial information typically contains the patient's age, gender, and chief complaint for the visit. The Patient system has access to the entire patient record \(\mathcal{K}=\{k_{0},k_{1},\ldots,k_{n}\}\), and the necessary information to answer the multiple choice question is \(\mathcal{K}^{*}\subseteq\mathcal{K}\). At the start of the \(t\)-th turn, the knowledge available to the Expert system is denoted as \(\mathcal{K}_{t-1}=\{k_{0},\ldots,k_{i}\}\). Given follow-up question \(q_{t}\), the Patient system responds with \(r_{t}=\{k|k\in\mathcal{K}\}\). The Expert knowledge is then updated as \(\mathcal{K}_{t}=\mathcal{K}_{t-1}\cup r_{t}\). The **main challenge of the task** is for the Expert system to ask information-seeking questions to expand \(\mathcal{K}_{t}\) until the knowledge gap is filled, i.e. \(\mathcal{K}_{t}=\mathcal{K}^{*}\), at which point the Expert system is asked to make a final decision.

### The Patient System

Patient TaskAs part of the MediQ framework, the Patient system simulates a human patient in clinical conversations. The Patient system has access to the full patient record that is sufficient for the diagnosis, including symptoms, onset duration, medical history, family history, and/or relevant lifestyle factors. The Patient system uses the patient record and a single information-seeking question from the Expert system to produce a coherent response consistent with the given patient information as shown in Figure 2. A reliable Patient system is critical to simulate a real and accurate medical consultation process. We propose that any Patient system should be evaluated on (1) _Factuality_ - measuring if a patient's responses are faithful to the patient's record and history and (2) _Relevance_ - measuring if the patient's response answers the expert's question. Given the full patient record and the expert question, we propose and evaluate three Patient system variants: **Direct**, **Instruct**, and **Fact-Select**, to obtain the patient response. Exact prompts and examples are in Appendix A.2.

1. [leftmargin=*]
2. **Direct:** Serving as a baseline, the Patient treats the response-generation as a reading comprehension task with no additional instruction. The prompt includes the patient's record followed by the Expert's question and asks the model to directly respond to the question using the given paragraph.
3. **Instruct:** The Patient is instructed to respond truthfully to the Expert's question using the patient record only. When the context does not contain an answer to the question, the Patient is instructed to refrain from answering.
4. **Fact-Select:** The Patient aims to improve the factuality of the response by decomposing the patient record into atomic facts and responds by selecting facts that are relevant to the Expert's question.

### The Expert System

Expert TaskThe Expert system simulates the medical decision-making process of experienced clinicians, who seek additional patient information and iteratively update their differential diagnosis. The Expert system is first presented with a medical question and limited patient information. As each turn, it assesses whether the provided information is sufficient to answer the question. If the Expert system is unconfident, it can elicit evidence with a follow-up information-seeking question; otherwise, the Expert system deems the acquired information sufficient and provides a final answer. The performance of the Expert system is evaluated on the (1) _efficiency_ of the conversation (number of follow-up questions) and (2) the _accuracy_ of the final diagnosis.

#### 2.2.1 Expert System Breakdown

Medical decision making is a complex process involving clinical reasoning and proactive information-seeking (Bordage, 1999; Norman, 2005; Schmidt et al., 1990; Boshuizen & Schmidt, 1992; Patel

Figure 2: The MediQ Benchmark. MediQ operationalizes a more realistic dynamic clinical interaction between a Patient system and an Expert system to evaluate info-seeking and question-asking.

the next section, (2). We describe our proposed **MediQ-Expert**, which operationalizes the Expert system by breaking down the task into five medically-grounded steps: (1) initial assessment, (2) abstention, (3) question generation, (4) information integration, and (5) decision making (Figure 3). Each step is modular and easily modifiable.

**Step 1. Initial Assessment Module**: Given limited patient intake information and the multiple choice question (MCQ) as the input, the goal of this module is to provide an initial assessment of the patient. The Expert system is asked to produce a paragraph that elaborates on the symptoms and options, and identifying potential knowledge gaps (e.g., additional symptoms, lab tests) missing for answering the question. This step is done only once at the beginning of the interaction, and we keep the output in the conversation thread for future turns to refer back to.

**Step 2. Abstention Module**: When the model is not confident, it should _abstain_ from giving an answer and asks a information-seeking question instead. The goal of the Abstention Module is to evaluate the **confidence level** of the Expert system to make a decision given the available information. The input to this module is the MCQ and the patient information consisting of the initial presentation and a conversation log of follow-up questions and responses. We probe the confidence level of the model to reliably answer the question via prompting (SS 2.2.2). The output of this module is a yes/no answer for whether to proceed to final answer. If the model is confident, it skips to decision making; otherwise, it continues to question generation.

**Step 3. Question Generation Module**: When more information is deemed necessary, the goal of the question generation module is to craft an information-seeking question to elicit additional medical evidence such as lifestyle factors and physical exam results. The input to this module is all previous reasoning steps, and the acquired patient information; the notion of atomic questions is defined with respect to the medical domain in the prompt, and the output is an atomic question to the patient.

**Step 4. Information Integration Module**: When a patient response is returned to the Expert system, the information integration module aggregates all gathered patient information up to this point to update the understanding of the patient condition. This step simply appends a question-answer pair to the end of an existing conversation log, which will then be passed to the Abstention Module.

**Step 5. Decision Making Module**: When enough evidence is gathered, the Expert system leverages integrated patient information and medical knowledge to provide an accurate answer to the question. The input to this module is previous reasoning steps, the MCQ, and the gathered patient information, and the output is the chosen option. Exact prompts for all above sections are in Appendix B.

#### 2.2.2 Expert System Variants with Different Abstention Strategies

One component of active information-seeking is the ability to decide _when_ to ask questions, which we operationalize with the Abstention Module to either _ask_ or _answer_ at each turn. Abstention reduces LLM hallucinations in low-confidence scenarios (Umapathi et al., 2023; Rawte et al., 2023) and mitigates misleading or insufficiently substantiated conclusions (Feng et al., 2024). We develop the following variants of the Abstention Module via different instructions to the LLM to probe its confidence in whether their parametric knowledge is sufficient to reliably answer the MCQ. Exact prompts are in Appendix B.2.

**0. Basic:** As a baseline, the model is asked to implicitly indicate its abstain decision by either generating an atomic question or producing an answer to the MCQ.

**1. Numerical:** To get an explicit understanding of the model's confidence, we first prompt the model to generate a numerical confidence score between 0 and 1 following (Tian et al., 2023). Then, an arbitrary threshold is set to either proceed with a final answer or ask a question.

Figure 3: Expert system information flow breakdown.

2. **Binary:** Previous work has shown that LLMs struggle at producing numerical confidence scores (Srivastava et al., 2022; Geng et al., 2023). To address this, the Binary variant enables a simple classification of whether enough information is present. This setup simplifies the decision process, but may lack the nuanced understanding of confidence levels.
3. **Scale:** Binary classification does not provide granularity where the decision is ambiguous. Scale abstention solves this issue by combining direct quantification with a manageable set of discrete, interpretable options. The model is given definitions of confidence levels on a 5-point Likert scale (e.g., "Very Confident", "Somewhat Confident"), and is asked to select a rating to express its confidence. An arbitrary threshold is set to either proceed with a final answer or ask a question.
4. **Rationale Generation (RG):** Model performance is shown to improve when prompted to generate a reasoning chain about the decision process (Wei et al., 2022; Marasovic et al., 2021). This gives the model a longer context window for reasoning, allowing the final decision to be conditioned on previous generations. We attempt to generalize this finding to the more complex interactive medical information-seeking setup by applying it to Numerical, Binary and Scale abstention prompts.
5. **Self-Consistency (SC)**. To further improve the Expert system's abstaining decision, we apply Self-Consistency to the above variants. Self-consistency repeatedly prompts the LLM \(n\) times and take the average (Numerical and Scale) or the mode (Binary) of the output as the final output, and is shown to improve model performance (Wang et al., 2022).

## 3 Experiments

We conduct experiments to validate each component of MediQ. First, we evaluate the Patient system with _factuality_ and _relevance_ metrics (SS 3.1). Then, we establish the correlation between information availability and accuracy by studying model performance with varying levels of input information (SS 3.2.1). Finally, we improve the information-seeking ability of LLMs under MediQ (SS 3.2.2).

Evaluation DatasetWe convert MedQA (CC-BY 4.0) (Jin et al., 2021) and Craft-MD (CC-BY 4.0) (Johri et al., 2023, 2024) into an interactive setup for our experiments. MedQA is a standard benchmark for medical question answering with 10178/1272/1273 train/dev/test samples. Each sample contains a paragraph of patient record ending with a multiple choice question. Craft-MD contains 140 dermatology patient records in a similar format, among which 100 are collected from an online question bank and 40 are created by expert clinicians. We parse each patient record into age, gender, the chief complaint (primary reason for the clinical visit), and additional evidence. Only the age, gender, and chief complaint are presented to the Expert system, from which it is expected to elicit missing information. The resulting tasks are called iMedQA and iCraft-MD, respectively. See Appendix C for detail.

### Patient System Reliability Evaluation

We automate the evaluation of patient responses with factuality score and relevance score for the ease of scalability, and conduct manual annotations to validate our metrics (Appendix A.4).

_Factuality Score_ measures whether the Patient system's response is consistent with the patient record. Each Patient response is first decomposed into a list of atomic statements, then we compute the percentage of atomic statements that are supported by the information in the patient record. The factuality score is the percent of supported statements averaged over all patients.

_Relevance Score_ measures whether the Patient system's response answers the Expert's question. Since there is no oracle data on the correct answer for Expert follow-up questions, we construct a synthetic parallel evaluation dataset of questions and responses to evaluate the relevance of Patient responses: given a patient record decomposed into atomic statements, we rephrase each statement into an atomic question, for which the statement is the ground truth answer. Then, the Patient system produces a response using the patient record and the generated atomic question. The average embedding semantic similarity between the generated response and the ground truth statement over the evaluation dataset is the resulting relevance score. See Appendix A.1 for more detail.

SetupWe use GPT-3.5 as the base LLM for all three variants (Direct, Instruct, and Fact-Select) and compare the factuality and relevance scores. For factuality, we sample 1272 patient cases from MediQ interactions with follow-up questions generated by different Expert systems so the Patientsystem sees diverse Expert questions and compute the average across all generated questions. For relevance, we use all 1272 patient records from the development set of MedQA.

### Expert System Experiments

#### 3.2.1 Benchmarking Existing LLMs in Incomplete Information Scenarios

We evaluate the performance of non-interactive Expert systems with varying information availability levels to observe the relationship between information availability and accuracy and to establish baselines. The baselines are evaluated at three initial information availability levels (Figure 4): **Full**, **Initial**, and **None**. The **Full** setup is equivalent to the standard QA task, wherein all patient information is provided to the Expert system in the beginning; **Initial** only discloses the gender, age, and the chief complaint that leads to the clinical visit (e.g. fever, headache, etc.); **None** provides no patient information but only the MCQ to the Expert system.

#### 3.2.2 Interactive Expert Systems

**Expert Variants**  Without explicitly providing the option to ask follow-up questions, vanilla LLMs always answer with incomplete information and _never_ ask for additional evidence. Therefore, we establish a question-asking Expert system baseline--**Basic**--by prompting the LLM to either ask a question or make a decision at each turn. To study abstention, we combine Numerical, Binary, and Scale abstention with rationale generation and self-consistency techniques described in SS 2.2.2.

**Expert System Setup**  We evaluate Llama-3-Instruct (8B, 70B), GPT-3.5, and GPT-4 on iMedQA and iCraft-MD for both the non-interactive and interactive settings. Analysis and ablations use GPT-3.5 results on iMedQA only. Details on model version and compute are in Appendix C.

**Expert System Evaluation Metric**  An ideal Expert system should be able to ask informative questions that allow it to arrive at accurate medical decisions efficiently. Since it is not trivial to measure the quality of medical information-seeking questions, we use the efficiency of the interaction (number of questions) and accuracy of the solution as proxies to evaluate the clinical reasoning capabilities. Accuracy is strongly dependent on the amount of information available to the model (SS 3.2.1), so higher accuracy is correlated with stronger information-seeking ability of the LLM.

## 4 Results

### How reliable is the MediQ Patient system?

Our results in Table 1 show that both the **Direct** and **Instruct** settings struggle with factuality. Qualitative analysis revealed that since the Direct setting did not receive any instructions on _how_ to respond to the follow-up question, it sometimes responds with "Yes" or "No" instead of the atomic statements that contain the requested information. In the Instruct setting, the Patient system sometimes provide inferences instead of reciting the facts from the failure cases are shown in Appendix A.3. On the other hand, the **Fact-Select** setting which generates the responses in a more controlled environment increases factuality by 0.33 points and relevance by 0.04 points. Overall, these results suggest that _using atomic facts as units of information significantly reduces hallucination_, improving the reliability of the Patient system in providing accurate and relevant responses to expert questions. We use the Fact-Select setting for the Patient system in all subsequent experiments and shift our focus to evaluate the Expert variants introduced in SS 3.2.

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Model** & **Factuality** & **Relevance** \\ \hline
**Direct** & 55.9 & 75.5 \\
**Instruct** & 62.8 & 78.6 \\
**Fact-Select** & **89.1** & **79.9** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Patient system reliability.

Figure 4: Non-interactive Expert system evaluation at various information availability levels. The question and options are provided to the Expert model in all three settings.

### How do existing Non-Interactive LLMs perform with Limited Information?

As shown in Table 2, with decreasing amounts of patient information provided to the model, there is a pronounced drop in performance from the **Full** to **Initial** to **None** information availability levels. Shifting our attention to the **Basic** interactive setup, the final accuracy is even lower than its non-interactive counterpart (Initial) with the same initial information (a average of 11.310.3%relative drop). We analyze performance sensitivity to prompt variations to ensure a fair comparison and report results from additional LLMs in Appendix E.

Figure 5 shows the number of follow-up questions asked by the LLMs in the Basic interactive setup. For majority of the samples, _no_ model chooses to ask any questions, showing the lack of ability in LLMs to proactively identify and elicit missing information. Within each LLM family (Llama/GPT), there is a correlation between model size, number of questions asked and accuracy. Overall, these results show a significant gap between model performance in idealized settings and realistic, information-limited scenarios. None of the examined models excel at proactive information seeking in an interactive environment, suggesting that it is nontrivial to integrate information gathered from continuous interactions. Despite having some medical knowledge encoded during pretraining, LLMs struggle to compensate for the absence of detailed patient information, highlighting the need for advanced proactive information-seeking abilities in medical LLM applications.

### How much of the performance gap can be closed by asking questions?

In Figure 6, We present a summary of the information-seeking ability of MediQ Expert models with different abstain strategies by reporting the _accuracy_ and _number of questions_ (conversation efficiency). Recall that both the Numerical and Scale abstention methods require setting a confidence threshold, above which the Expert system will proceed to the final answer. We do a grid search for the threshold hyperparameter in Appendix D and report the best performance for each setting. Integrating a dedicated Abstention Module significantly enhances performance over the Basic setup which directly prompts for follow-up questions or diagnoses. As the abstain strategies improve - by expressing confidence on a scale, verbal reasoning, and adding self-consistency - the expert model is able to better gauge the (lack of) patient information and continue the conversation by asking more questions and thereby improving the final accuracy.

Base abstention methods (Numerical, Binary, Scale) show little variance in effectiveness until combined with rationale generation, which consistently boosts performance across strategies, as supported by previous studies (Marasovic et al., 2021; Wei et al., 2022; Feng et al., 2024). Notably, self-consistency alone _degrades_ performance unless paired with rationale generation. Overall, the Scale Abstention (1-5 confidence rating) with Rationale Generation and a Self-Consistency factor of 3 achieves the best performance. Overall, Scale Abstention (1-5 confidence rating) with rationale generation and a self-consistency factor of 3 achieves the best performance, outperforming the Basic interactive setup by 22.3% and the non-interactive Initial setup by 12.1%. In information-scarce scenarios, models tend to resort to the most common option instead of specializing to the patient, and interaction enhances specialization (Appendix F).

This pattern is generalizable across different LLMs as shown in the Best column of Table 2. Note that model size plays a big factor in the performance of the interactive setting--models larger than 70B surpass the non-interactive Initial setup with the best abstention, but smaller models still struggle.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{**Task**} & \multirow{2}{*}{**Model**} & \multicolumn{3}{c}{**Non-Interactive**} & **Interactive** \\  & & **Full** & **Initial** & **None** & **Basic** & **Best** \\ \hline \multirow{4}{*}{**iMedQA**} & **Llama-3-8b** & 68.14\(\pm\)1.5 & 52.0\(\pm\)1.4 & 40.4\(\pm\)1.4 & 53.0\(\pm\)1.3 & 45.8\(\pm\)1.4 \\  & **Llama-3-70b** & 84.7\(\pm\)1.5 & 58.5\(\pm\)1.4 & 46.3\(\pm\)1.4 & 55.1\(\pm\)1.3 & **69.9\(\pm\)1.4** \\  & **GPT-3.5** & 55.8\(\pm\)1.4 & 45.6\(\pm\)1.4 & 36.7\(\pm\)1.4 & 42.2\(\pm\)1.3 & **50.2\(\pm\)1.4** \\  & **GPT-4** & 79.7\(\pm\)1.1 & 54.5\(\pm\)1.4 & 42.2\(\pm\)1.4 & **55.4\(\pm\)1.3** & **66.1\(\pm\)1.3** \\ \hline \multirow{4}{*}{**iCraft-MD**} & **Llama-3-8b** & 76.4\(\pm\)3.6 & 51.4\(\pm\)4.2 & 29.3\(\pm\)3.8 & 42.9\(\pm\)4.2 & 50.0\(\pm\)4.2 \\  & **Llama-3-70b** & 82.1\(\pm\)3.2 & 60.7\(\pm\)1.2 & 52.9\(\pm\)4.2 & 62.4\(\pm\)1.7 & **72.1\(\pm\)3.8** \\ \cline{1-1}  & **GPT-3.5** & 82.1\(\pm\)2.5 & 53.6\(\pm\)4.2 & 29.3\(\pm\)3.8 & 45.0\(\pm\)4.2 & **59.3\(\pm\)4.2** \\ \cline{1-1}  & **GPT-4** & 91.4\(\pm\)2.4 & 67.9\(\pm\)3.9 & 43.6\(\pm\)3.7 & **73.6\(\pm\)3.7** & **84.3\(\pm\)3.1** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Accuracy at varying information availabilities. Basic gives LLM the option to ask questions: with the same starting information, Basic performance degrades from non-interactive Initial. Bold entries surpass non-interactive Initial, but there is still a gap between Full (complete information upper bound) and interactive Best.

Figure 5: Frequency of conversation lengths in the Basic setting. Most models don’t tend to ask follow-up questions.

While informing LLMs _when_ to ask questions through abstention helps improve their decision-making with limited information, our best MediQ Expert system (Scale+RG+SC) still only closes 51.2% of the gap between the _Non-Interactive Initial_ and the _Full_ information scenarios. This indicates plenty of room for improvement to further enhance the information-seeking ability of LLMs.

## 5 Analysis

In this section, we further analyze the factors that impact the performance of the interactive Expert system. Since we observe similar trends across models and datasets, all analysis will be performed on the iMedQA dataset with GPT-3.5 due to cost and computation constraints.

### Why does the Basic interactive setup fail to perform clinical reasoning?

Recall from SS 4.2 that there is a striking 11.3% relative drop in accuracy from Basic to the _non-interactive_ Initial information setting (NI-Initial) across all benchmarked LLMs (7.43% for GPT-3.5 on iMedQA). In this section, we analyze failure modes of the Basic system, where the Expert is simply given the option to ask follow-up questions, to understand the performance drop. We show that the ability to ignore irrelevant context and extract useful information from conversation format affects model performance.

Irrelevant ContextThere are two types of irrelevant context on model performance: _unanswerable_ and _repeated_ questions. As MediQ allows the Expert to ask any open-ended questions to the Patient to elicit information, some questions cannot be answered using the patient record. We filter out these unanswerable question-response pairs, keeping only record-based questions and responses to assess the effect of ignoring irrelevant questions (**Relevant**). Secondly, although the Expert is instructed to not repeat any questions, upon inspection of the interaction history, many questions are repetitive, especially when the answer is not in the patient record. We hypothesize that the repetition shifts the model's attention to certain questions and thus hinders the performance. We remove repetitive questions and only keep the unique questions (using fuzzy lexical matching) to verify this hypothesis (**Unique**). Finally, we remove both unanswerable and repeated questions (**Both**).

Conversation FormatWe further hypothesize that the dialogue format, different from the typical document format seen during LLM pre-training, also affects performance. To control for this, we convert the conversation format into paragraph format by discarding the Expert questions and only keeping the patient response for answerable questions, and rewriting the unanswerable questions into statements (e.g., The patient's vaccine record is unavailable.) for each setting above.

As shown in Figure 7, Relevant and Unique both improve performance by 2 percentage points (pp), but the combined effect is indistinguishable from using either filter, which might be due to the fact that unanswerable questions tend to be repeated. Converting the conversations into paragraph format further improves the performance (Para). Removing repetitive questions and converting to paragraph format (Unique-Para) surpasses Basic by 5.7pp and NI-Initial by 2.3pp. This shows that, when given the option to ask follow-up questions, the information-seeking ability of the Expert system does help make more informed and accurate conclusions, but the model suffers from not being able to learn from realistic clinical dialogues.

[MISSING_PAGE_EMPTY:9]

## 6 Related Work

Medical Question Answering SystemsAdvancements in medical question answering (QA) systems have progressed from rule-based systems to LLM-powered agents. Notable medical QA benchmarks include MultiMedQA (Singhal et al., 2023a), which contains both multiple-choice and open-ended questions collected from various sources. To customize or adapt a general-purpose LLM to the medical domain, prior work often finetune the model on medical knowledge data such as PubMed (Bolton et al., 2022; Yasunaga et al., 2022; Wu et al., 2023a; Singhal et al., 2023a,b), or more recently on conversational medical datasets (Yunxiang et al., 2023; Han et al., 2023). Kim et al. (2024) further improves model performance on complex medical questions by dynamically forming multi-agent collaboration structures. Despite their proficiency in direct answer retrieval, the proactive information-seeking capability is not something these models are inherently designed to do. Our proposed methodological framework, MediQ, is designed to work as an overlay to these domain-specific models, enhancing them with the capability to actively seek additional information in a structured and clinically relevant manner.

Interactive Models and AgentsInteractive conversational models extend beyond the standard QA framework by engaging in a dialogue such as customer support and negotiation (Singh and Beniwal, 2022; Chakrabarti and Luger, 2015; He et al., 2018; Abdelnabi et al., 2023; JU et al., 2024), where iterative information gathering is crucial. Li et al. (2023) and Andukuri et al. (2024) attempt to use LLMs to elicit more information-rich human preference examples in everyday tasks. However, the application of these models in the medical domain remains limited (Li et al., 2021). Wu et al. (2023b) attempts to evaluate general-purpose LLMs and chain-of-thought reasoning on DDXPlus (Fansi et al., 2022), a rule-based synthetic patient dataset. Hu et al. (2024) navigates the information-seeking scenario as a search problem by developing a reward model guided by uncertainty and includes medical diagnosis as one of the tasks, but if limited to binary questions. Johri et al. (2023) observed a similar phenomenon where LLM-doctors cannot elicit complete patient information, but do not focus on improving the information-seeking ability. The system proposed by (Tu et al., 2024) performs a multitude of medical tasks but does not explore the crucial problem of abstention. Lastly, multi-agent and human-AI collaboration frameworks have shown impressive interactive performance (Zhou et al., 2024; Wu et al., 2024, 2023c; Deng et al., 2024; Lin et al., 2024), and can greatly benefit from our novel interactive abstention methods to seek additional information. Our work fills this gap via providing a benchmark, a dataset, and a framework to comprehensively studying information-seeking abilities in clinical decision-making, and most importantly, opens the door for future endeavors in this direction.

## 7 Conclusion

In this paper, we identify a significant gap in current LLMs' capability to ask questions and proactively seek information in settings where personalization, precision, and reliability are critical. We propose a paradigm shift to interactive benchmarks by simulating more realistic clinical interactions where only partial information is provided initially by introducing MediQ. MediQ provides a benchmark to the community to evaluate the question-asking ability of LLMs, contributing towards developing reliable models. We showed that SOTA LLMs like Llama-3 and GPT-4 struggle to gather necessary information for accurate medical decisions. To address this problem, we presented MediQ-Expert--a novel Expert system with improved confidence judgment and medical expertise, substantially improving clinical reasoning performance. MediQ operationalizes interactive and explicit clinical reasoning processes, with added interpretability in the reasoning flow of language models and decision making. We encourage future research to extend MediQ to more diverse Patient systems, expand medical knowledge integration, and customize the interactions to better serve the healthcare community.

Figure 9: Example of rationale generation helping identify knowledge gaps to ask better follow-up questions.

## Limitations

One limitation is the scarcity of datasets that contain detailed patient information sufficient for a medical diagnosis which, to the best of our knowledge, was only met by MedQA and Craft-MD. The majority of available medical datasets are designed to test models' medical domain knowledge. Second, the Patient system in our benchmark relies on a paid API; future work should establish an open-source Patient. Lastly, our evaluation framework, while designed to be more realistic, is still limited to the multiple-choice format. However, the flexibility of MediQ allows easy extensions into open-ended settings with appropriate datasets and well-defined conversation-level metric. Future work can focus on collecting a rich dataset in open-ended medical consultations and expanding the MediQ framework.

## Ethics Statement

Along with many potential benefits of an ideal future variant of the MediQ framework (e.g., providing reliable and personalized medical consultation when access to medical experts is unavailable, or assisting medical experts in initial collection of information), it is important to emphasize multiple risks associated with mis-use of this framework.

First, MediQ is a carefully designed initial prototype, it is not meant to be deployed to interact with users; its intended use is to provide an experimental framework to test clinical reasoning abilities of LLMs which are currently extremely limited.

MediQ built on top of closed-source LLMs runs the risk of leaking confidential medical information, violating patient privacy. Future research expanding MediQ to new medical datasets should be aware of these risks, resorting to local securely stored LLMs or to reliable data anonymization methods.

There are many sources of potential biases in the framework, including social and cultural biases in LLMs, in the datasets, and possibly in prompts for LLM interactions and abstention that we designed. While outside the scope of this paper, in addition to utility metrics we proposed, future research could incorporate fairness-oriented evaluations, e.g., breaking down the evaluation by user demographics.

If a similar framework is used in real-world applications, users and clinicians should be trained to prevent the over-reliance on technology that is liable to make mistakes, and to understand its privacy and fairness risks.

## Acknowledgements

We gratefully acknowledge support from the University of Washington Population Health Initiative. SSL, SF, and YT are supported by the National Science Foundation under CAREER Grant No. IIS2142739, NSF grants No. IIS2125201, IIS2203097, and gift funding from Google, MSR, and OpenAI. PWK is supported by the Singapore National Research Foundation and the National AI Group in the Singapore Ministry of Digital Development and Innovation under the AI Visiting Professorship Programme (award number AIVP-2024-001). EP was supported by a Google Research Scholar award, Optum, NSF CAREER #2142419, a CIFAR Azrieli Global scholarship, a gift to the LinkedIn-Cornell Bowers CIS Strategic Partnership, and the Abby Joseph Cohen Faculty Fund.

## References

* Abdelnabi et al. (2023) Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Schonherr, and Mario Fritz. Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games, 2023.
* Andukuri et al. (2024) Chinmaya Andukuri, Jan-Philipp Franken, Tobias Gerstenberg, and Noah D. Goodman. Star-gate: Teaching language models to ask clarifying questions, 2024.
* Bolton et al. (2022) E. Bolton, D. Hall, M. Yasunaga, T. Lee, C. Manning, and P Liang. Stanford crfm introduces pubmedgpt 2.7b, 2022. URL https://hai.stanford.edu/news/stanford-crfm-introduces-pubmedgpt-27b.
* Bordage (1999) Georges Bordage. Why did i miss the diagnosis? some cognitive explanations and educational implications. _Academic Medicine_, 74(10):S138-43, 1999.
* Bordage (1999)Brian H Bornstein and A Christine Emler. Rationality in medical decision making: a review of the literature on doctors' decision-making biases. _Journal of evaluation in clinical practice_, 7(2):97-107, 2001.
* Boshuizen and Schmidt (1992) Henny PA Boshuizen and Henk G Schmidt. On the role of biomedical knowledge in clinical reasoning by experts, intermediates and novices. _Cognitive science_, 16(2):153-184, 1992.
* Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
* Chakrabarti and Luger (2015) Chayan Chakrabarti and George F Luger. Artificial conversations for customer service chatter bots: Architecture, algorithms, and evaluation metrics. _Expert Systems with Applications_, 42(20):6878-6897, 2015.
* Chen et al. (2023) Zeming Chen, Alejandro Hernandez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Kopf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, and Antoine Bosselut. Meditron-70b: Scaling medical pretraining for large language models, 2023.
* Deng et al. (2024) Yang Deng, Lizi Liao, Zhonghua Zheng, Grace Hui Yang, and Tat-Seng Chua. Towards human-centered proactive conversational agents, 2024.
* Tchango et al. (2022) Arsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn. Ddxplus: A new dataset for automatic medical diagnosis. _Advances in Neural Information Processing Systems_, 35:31306-31318, 2022.
* Feng et al. (2023) Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Knowledge card: Filling lllms' knowledge gaps with plug-in specialized language models. In _The Twelfth International Conference on Learning Representations_, 2023.
* Feng et al. (2024) Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. Don't hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration, 2024.
* Geng et al. (2023) Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych. A survey of language model confidence estimation and calibration. _arXiv preprint arXiv:2311.08298_, 2023.
* an open-source collection of medical conversational ai models and training data, 2023.
* He et al. (2018) He He, Derek Chen, Anusha Balakrishnan, and Percy Liang. Decoupling strategy and generation in negotiation dialogues, 2018.
* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.
* Hu et al. (2024) Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, and Bryan Hooi. Uncertainty of thoughts: Uncertainty-aware planning enhances information seeking in large language models. _arXiv preprint arXiv:2402.03271_, 2024.
* Huang et al. (2024) Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good are low-bit quantized llama3 models? an empirical study, 2024.
* Jin et al. (2021) Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. _Applied Sciences_, 11(14):6421, 2021.
* Jin et al. (2020)Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA: A dataset for biomedical research question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 2567-2577, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1259. URL https://aclanthology.org/D19-1259.
* Johri et al. (2023) Shreya Johri, Jaehwan Jeong, Benjamin A Tran, Daniel I Schlessinger, Shannon Wongvibulsin, Zhuo Ran Cai, Roxana Daneshjou, and Pranav Rajpurkar. Testing the limits of language models: A conversational framework for medical ai assessmentr. _medRxiv_, 2023.
* Johri et al. (2024) Shreya Johri, Jaehwan Jeong, Benjamin A. Tran, Daniel I Schlessinger, Shannon Wongvibulsin, Zhuo Ran Cai, Roxana Daneshjou, and Pranav Rajpurkar. CRAFT-MD: A conversational evaluation framework for comprehensive assessment of clinical LLMs. In _AAAI 2024 Spring Symposium on Clinical Foundation Models_, 2024. URL https://openreview.net/forum?id=Bk2nbTDtm8.
* Da et al. (2024) Da JU, Song Jiang, Andrew Cohen, Aaron Foss, Sasha Mitts, Arman Zharmagambetov, Brandon Amos, Xian Li, Justine T Kao, Maryam Fazel-Zarandi, and Yuandong Tian. To the globe (ttg): Towards language-driven guaranteed travel planning, 2024. URL https://arxiv.org/abs/2410.16456.
* Kim et al. (2024) Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu Chan, Xuhai Xu, Daniel McDuff, Hyeonhoon Lee, Marzyeh Ghassemi, Cynthia Breazeal, and Hae Won Park. Mdagents: An adaptive collaboration of llms for medical decision-making, 2024. URL https://arxiv.org/abs/2404.15155.
* Li et al. (2023) Belinda Z Li, Alex Tamkin, Noah Goodman, and Jacob Andreas. Eliciting human preferences with language models. _arXiv preprint arXiv:2310.11589_, 2023.
* Li et al. (2021) Dongdong Li, Zhaochun Ren, Pengjie Ren, Zhumin Chen, Miao Fan, Jun Ma, and Maarten de Rijke. Semi-supervised variational reasoning for medical dialogue generation. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pp. 544-554, 2021.
* Lin et al. (2024) Jessy Lin, Nicholas Tomlin, Jacob Andreas, and Jason Eisner. Decision-oriented dialogue for human-ai collaboration, 2024.
* Marasovic et al. (2021) Ana Marasovic, Iz Beltagy, Doug Downey, and Matthew E Peters. Few-shot self-rationalization with natural language prompts. _arXiv preprint arXiv:2111.08284_, 2021.
* Masic (2022) Izet Masic. Medical decision making-an overview. _Acta Informatica Medica_, 30(3):230, 2022.
* Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. _arXiv preprint arXiv:2305.14251_, 2023.
* Norman (2005) Geoffrey Norman. Research in clinical reasoning: past history and current trends. _Medical education_, 39(4):418-427, 2005.
* OpenAI et al. (2023) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Foits Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Lukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Paraparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchanian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024.
* Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.
* Pal et al. [2022] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. In _Conference on health, inference, and learning_, pp. 248-260. PMLR, 2022.
* Patel et al. [1994] Vimla L Patel, Jose F Arocha, and David R Kaufman. Diagnostic reasoning and medical expertise. In _Psychology of learning and motivation_, volume 31, pp. 187-252. Elsevier, 1994.
* Rawte et al. [2023] Vipula Rawte, Amit Sheth, and Amitava Das. A survey of hallucination in large foundation models. _arXiv preprint arXiv:2309.05922_, 2023.
* Schmidt et al. [1990] Henk G Schmidt, Geoffrey R Norman, and Henny P Boshuizen. A cognitive perspective on medical expertise: theory and implication [published erratum appears in acad med 1992 apr; 67 (4): 287]. _Academic medicine_, 65(10):611-21, 1990.
* Sclar et al. [2023] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting, 2023.
* Singh and Beniwal [2022] Satwinder Singh and Himanshu Beniwal. A survey on near-human conversational agents. _Journal of King Saud University-Computer and Information Sciences_, 34(10):8852-8866, 2022.
* Singhal et al. [2023a] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. _Nature_, 620(7972):172-180, 2023a.
* Singh et al. [2023b]Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with large language models, 2023b.
* Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_, 2022.
* Talebriad and Nadiri (2023) Yashar Talebriad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of intelligent llm agents. _arXiv preprint arXiv:2306.03314_, 2023.
* Tian et al. (2023) Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. _arXiv preprint arXiv:2305.14975_, 2023.
* Touvron et al. (2020) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.
* Trimble and Hamilton (2016) Michael Trimble and Paul Hamilton. The thinking doctor: clinical decision making in contemporary medicine. _Clinical Medicine_, 16(4):343, 2016.
* Tu et al. (2024) Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, Shekoofeh Azizi, Karan Singhal, Yong Cheng, Le Hou, Albert Webson, Kavita Kulkarni, S Sara Mahdavi, Christopher Semturs, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S Corrado, Yossi Matias, Alan Karthikesalingam, and Vivek Natarajan. Towards conversational diagnostic ai, 2024.
* Umapathi et al. (2023) Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu. Med-halt: Medical domain hallucination test for large language models. _arXiv preprint arXiv:2307.15343_, 2023.
* Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. _arXiv preprint arXiv:2203.11171_, 2022.
* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural information processing systems_, 35:24824-24837, 2022.
* Wu et al. (2023a) Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Towards building open-source language models for medicine. _arXiv preprint arXiv:2305.10415_, 6, 2023a.
* Wu et al. (2023b) Cheng-Kuang Wu, Wei-Lin Chen, and Hsin-Hsi Chen. Large language models perform diagnostic reasoning, 2023b.
* Wu et al. (2023)Guande Wu, Chen Zhao, Claudio Silva, and He He. Your co-workers matter: Evaluating collaborative capabilities of language models in blocks world, 2024.
* Wu et al. (2023c) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. _arXiv preprint arXiv:2308.08155_, 2023c.
* Xie et al. (2024) Johnathan Xie, Annie S Chen, Yoonho Lee, Eric Mitchell, and Chelsea Finn. Calibrating language models with adaptive temperature scaling. In _ICLR 2024 Workshop on Secure and Trustworthy Large Language Models_, 2024. URL https://openreview.net/forum?id=BgfGqNpoMi.
* Yang et al. (2024) Ruixin Yang, Dheeraj Rajagopa, Shirley Anugrah Hayati, Bin Hu, and Dongyeop Kang. Confidence calibration and rationalization for llms via multi-agent deliberation. _arXiv preprint arXiv:2404.09127_, 2024.
* Yasunaga et al. (2022) Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D. Manning, Percy Liang, and Jure Leskovec. Deep bidirectional language-knowledge graph pretraining. _ArXiv_, abs/2210.09338, 2022. URL https://api.semanticscholar.org/CorpusID:252968266.
* Yunxiang et al. (2023) Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. _arXiv preprint arXiv:2303.14070_, 2023.
* Zhou et al. (2024) Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, and Maarten Sap. Stopgia: Interactive evaluation for social intelligence in language agents, 2024.

Patient System

### Evaluation Metrics

The Patient System is responsible for answering follow-up questions from the Expert System to provide the inquired patient information. When answering the follow-up questions, the Patient System has access to 1) the full patient context and 2) the expert question, and is instructed to produce a factual answer grounded in the context information and make no inferences. To ensure a reliable information-seeking process, we evaluate the Patient System along two axes: **Factuality Score** and **Relevance Score**. The basis of both the factuality and relevance evaluation relies on atomic facts from the patient record as units of information, which we generate by prompting GPT-4 following Min et al. (2023).

Factuality score measures whether the responses produced by the patient model are factual with respect to the given patient context information:

\[\text{factuality}(\mathcal{R},\mathcal{C})=\frac{1}{|R|}\sum_{r_{i}\in R} \frac{\sum_{j=1}^{|r_{i}|}\mathcal{I}(r_{i}^{j},\,\mathcal{C})}{|r_{i}|},\] (1)

where \(\mathcal{R}\) is the responses from the Patient System on the patient case \(\mathcal{C}\); the size of \(\mathcal{R}\) depends on the number of deduplicated Expert questions on the patient \(\mathcal{C}\). \(\mathcal{I}\) is the indicator function of whether atomic fact \(r_{i}^{j}\) from the response \(r_{i}\) is factually consistent (\(\approx\)) with any statement \(c\) in \(\mathcal{C}\):

\[\mathcal{I}(r_{i}^{j},\,\mathcal{C})=\begin{cases}1,&\text{if}\,\exists\,c \in\mathcal{C}\text{ s.t. }r_{i}^{j}\approx c\\ 0,&\text{otherwise,}\end{cases}\] (2)

Factual consistency of the response and any statement \(c\) in \(\mathcal{C}\) (\(r_{i}^{j}\approx c\)) is determined using semantic similarity for non-first person response patient variants and GPT-4 binary classification for first-person responses, since the atomic facts \(c_{i}\) are written in third person. Specifically, \(r_{i}^{j}\approx c\) in \(\mathcal{I}=1\) true if the semantic similarity, calculated by the cosine similarity of the SentenceTransformer stsb-roberta-large embeddings, is greater or equal to \(0.8\) (or if the two sentences are determined to be consistent by a GPT-4 judge in the first-person response case), and vice versa.

**Relevance Score** measures whether the generated patient response answers the expert question. Defined similarly to the hallucination rate:

\[\text{relevance}=\frac{1}{|R|}\sum_{r_{i}\in\mathcal{R}}\frac{\sum_{j=1}^{|r_ {i}|}\mathcal{I}(r_{i}^{j},\,\mathcal{F}_{i})}{|r_{i}|},\] (3)

where \(\mathcal{F}_{i}\) is the ground truth statements from the patient context that answer the same expert question that \(r_{i}\) is trying to answer.

### Patient System Variants

#### a.2.1 Patient System Variant: Direct

``` \(\text{HOT}\)  Answer the question with the given context:  Context: "She says that, despite going to bed early at night, she is unable  to fall asleep. She denies feeling anxious or having disturbing thoughts  while in bed. Even when she manages to fall asleep, she wakes up early  in the morning and is unable to fall back asleep. She says she has grown  increasingly irritable and feels increasingly hopeless, and her concentration  and interest at work have diminished. The patient denies thoughts of suicide  or death. Because of her diminished appetite, she has lost 4 kg (8.8 lb)  in the last few weeks and has started drinking a glass of wine every night  instead of eating dinner. She has no significant past medical history and is  not on any medications."  Question: "What time do you usually go to bed at night?"Example 10.1.1 Output ```

The patient goes to bed early at night despite experiencing difficulty falling asleep.

#### a.2.2 Patient System Variant: Instruct

```

```

You are a truthful assistant that understands the patient's information, and you are trying to answer questions from a medical doctor about the patient. Below is a context paragraph describing the patient and their conditions: "She says that, despite going to bed early at night, she is unable to fall asleep. She denies feeling anxious or having disturbing thoughts while in bed. Even when she manages to fall asleep, she wakes up early in the morning and is unable to fall back asleep. She says she has grown increasingly irritable and feels increasingly hopeless, and her concentration and interest at work have diminished. The patient denies thoughts of suicide or death. Because of her diminished appetite, she has lost 4 kg (8.8 1b) in the last few weeks and has started drinking a glass of wine every night instead of eating dinner. She has no significant past medical history and is not on any medications." Question from the doctor: "What time do you usually go to bed at night?" Use the context paragraph to answer the doctor question. If the paragraph does not answers the question, simply say "The patient cannot answer this question, please do not ask this question again." Answer only what the question asks for. Do not provide any analysis, inference, or implications. Respond with a straightforward answer to the question ONLY and NOTHING ELSE. Example 10.1.1 Output ```

#### a.2.3 Patient System Variants: Fact-Select

A simple fact decomposition prompt

You are a truthful medical assistant that understands the patient's information. Break the following patient information into a list of independent atomic facts, with one piece of information in each statement. Each fact should only include the smallest unit of information, but should be self-contained. "She says that, despite going to bed early at night, she is unable to fall asleep. She denies feeling anxious or having disturbing thoughts while in bed. Even when she manages to fall asleep, she wakes up early in the morning and is unable to fall back asleep. She says she has grown increasingly irritable and feels increasingly hopeless, and her concentration and interest Atomic Red decomposition Record (could)

 at work have diminished. The patient denies thoughts of suicide or death. Because of her diminished appetite, she has lost 4 kg (8.8 lb) in the last  few weeks and has started drinking a glass of wine every night instead of  eating dinner. She has no significant past medical history and is not on any  medications."

 Response with the list of atomic facts and nothing else, prepend each fact by  an index starting from 1. No sub-list allowed.

 Example-LIM Output

 1.Patient goes to bed early at night but is unable to fall asleep.  2.Patient denies feeling anxious or having disturbing thoughts while in bed.  3.Patient wakes up early in the morning and is unable to fall back asleep.  4.Patient has grown increasingly irritable and feels increasingly hopeless.  5.Patient's concentration and interest at work have diminished.  6.Patient denies thoughts of suicide or death.  7.Patient has lost 4 kg (8.8 lb) in the last few weeks.  8.Patient started drinking a glass of wine every night instead of eating  dinner.  9.Patient has no significant past medical history.  10.Patient is not on any medications."

 Patient Response Generation from - Red-Saleset

 You are a truthful assistant that understands the patient's information, and  you are trying to answer questions from a medical doctor about the patient.

 Below is a list of factual statements about the patient:  1.Patient goes to bed early at night but is unable to fall asleep.  2.Patient denies feeling anxious or having disturbing thoughts while in bed.  3.Patient wakes up early in the morning and is unable to fall back asleep.  4.Patient has grown increasingly irritable and feels increasingly hopeless.  5.Patient's concentration and interest at work have diminished.  6.Patient denies thoughts of suicide or death.  7.Patient has lost 4 kg (8.8 lb) in the last few weeks.  8.Patient started drinking a glass of wine every night instead of eating  dinner.  9.Patient has no significant past medical history.  10.Patient is not on any medications.

 Question from the doctor: "What time do you usually go to bed at night?"

 Which of the above atomic factual statements answer the question? Select  at most two statements. If no statement answers the question, simply say  "The patient cannot answer this question, please do not ask this question  again." Answer only what the question asks for. Do not provide any analysis,  inference, or implications. Respond by selecting all statements that answer  the question from above ONLY and NOTHING ELSE.

 Example-LIM Output

 Patient goes to bed early at night but is unable to fall asleep.

#### a.2.4 Additional Patient System Variant: Fact-FP

In addition to the three Patient System variants presented in the main paper, we have explored two other variants: First Person (Fact-FP) and Binary Fact Classification (Fact-Classify). Fact-FP breaks down the patient record into atomic facts, select the relevant facts that answers the follow-up question, and respond by converting the atomic statements into first person perspective. Atomic fact decomposition prompt is the same as in Appendix A.2.3.

```  Patient Response Generation Prompt - Fact-FP  You are a patient with a list of symptoms, and you task is to truthfully answer questions from a medical doctor. Below is a list of atomic facts about you, use ONLY the information in this list and answer the doctor's question. 1.Patient goes to bed early at night but is unable to fall asleep. 2.Patient denies feeling anxious or having disturbing thoughts while in bed. 3.Patient wakes up early in the morning and is unable to fall back asleep. 4.Patient has grown increasingly irritable and feels increasingly hopeless. 5.Patient's concentration and interest at work have diminished. 6.Patient denies thoughts of suicide or death. 7.Patient has lost 4 kg (8.8 lb) in the last few weeks. 8.Patient started drinking a glass of wine every night instead of eating dinner. 9.Patient has no significant past medical history. 10.Patient is not on any medications. Question from the doctor: "What time do you usually go to bed at night?" Which of the above atomic factual statements answer the question? Select at most two statements. If no statement answers the question, simply say "I cannot answer this question, please do not ask this question again." Do not provide any analysis, inference, or implications. Respond by reciting the matching statements, then convert the selected statements into first person perspective as if you are the patient but keep the same information. Generate your answer in this format: STATEMENTS: FIRST PERSON: ```

Example: LIAM Output  STATEMENTS: "Patient goes to bed early at night but is unable to fall asleep." FIRST PERSON: "I go to bed early at night, but I can't fall asleep." ```

#### a.2.5 Additional Patient System Variant: Fact-Classify

Atomic fact decomposition prompt is the same as in Appendix A.2.3.

Patient response generation prompt: for each atomic fact, repeat the following prompting process to decide whether it answers the doctor question, and collect a yes/no answer from the model. The final output is the set of atomic facts with a yes response.

[MISSING_PAGE_EMPTY:21]

[MISSING_PAGE_FAIL:22]

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_EMPTY:24]

#### b.2.5 Rationale Generation

Mention Expert - Rationale Generation from (appended to previous conversation)

Considering factors above, are you confident to pick the correct option to the inquiry factually using the conversation log? Answer in the following format:

REASON: a one-sentence explanation of why you are or are not confident. DECISION: YES or NO.

Example LXX Output

REASON: "I am confident in selecting the correct option based on the patient's presentation of symptoms suggestive of depression, which would warrant treatment with medications like paroxetine or trazodone according to standard medical guidelines for managing depressive symptoms." DECISION: "YES"

### Follow-up Question Generation

Mention Expert - Question Generation Models Prompt (appended to previous conversation)

If there are missing features that prevent you from picking a confident and factual answer to the inquiry, consider which features are not yet asked about in the conversation log; then, consider which missing feature is the most important to ask the patient in order to provide the most helpful information toward a correct medical decision. You can ask about any relevant information about the patient's case, such as family history, tests and exams results, treatments already done, etc. Consider what are the common questions asked in the specific subject relating to the patient's known symptoms, and what the best and most intuitive doctor would ask. Ask ONE SPECIFIC ATOMIC QUESTION to address this feature. The question should be bite-sized, and NOT ask for too much at once. Make sure to NOT repeat any questions from the above conversation log. Answer in the following format:

ATOMIC QUESTION: the atomic question and NOTHING ELSE.

Example LXX Output

"What time do you usually go to bed at night?"

### Information Integration

None-LLM step. Rearrange the atomic question and patient response into the conversation log. Known patient information \(\mathcal{K}\):

A patient comes into the clinic presenting with some basic information:

"A 40-year-old woman presents with difficulty falling asleep, diminished appetite, and tiredness for the past 6 weeks."

Conversation log:

Doctor Question: "What time do you usually go to bed at night?" Patient Response: "Patient goes to bed early at night but is unable to fall asleep."

[MISSING_PAGE_EMPTY:26]

trials in Table 4 to show that the Binomial standard deviation is a reasonable estimation of the confidence interval when we only have one run available.

## Appendix D Detailed Abstention Results

In order to determine the confidence threshold above which the model should proceed to the final answer, we perform a grid search over the iMedQA dataset. The results are shown in Figure 10. Generally, when the threshold becomes higher, the Expert system asks more questions, and the performance also increases. But as the number of questions becomes too high, the performance stagnates, which might be due to the fact that a lot of the questions will become irrelevant and/or repetitive.

## Appendix E Results on Additional LLMs

We report the results of Experiment 3.2.1 on the Llama-2-Chat models below. Generally, we observe similar behavior in these models compared to models with higher utility (Llama-3, GPT-3.5, and GPT-4). However, due to the limited model capacity, results for a few settings are less reliable as they are close to at-chance performance (e.g., interactive Basic for Llama-2-7B).

### Sensitivity to Prompt Variations

LLMs are sensitive to spurious features in the prompt Sclar et al. (2023). We first experiment on various single-turn baselines with prompt variations to evaluate the robustness of the LLaMa-2-chat models Touvron et al. (2023) on the prompt in order to finalize a prompt to standardize future runs. We explore three **system prompts**: Empty (default system prompt), Basic (You

Figure 10: Performance of abstain strategies on iMedQA. Each line is an abstain strategy with increasing confidence thresholds. Darker colors are results with rationale generation (RG); dashed lines are with self-consistency (SC). The Best system (Scale+RG+SC,) significantly outperforms the Basic baseline ().

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{**Task**} & \multirow{2}{*}{**Model**} & \multicolumn{2}{c}{**Non-Interactive**} & \multicolumn{2}{c}{**Interactive**} \\  & & **Full** & **Initial** & **None** & **Basic** & **Best** \\ \hline \multirow{3}{*}{**iMedQA**} & **Llama-2-7b** & 30.8\(\pm\)1.3 & 26.3\(\pm\)1.2 & 26.8\(\pm\)1.2 & 27.8\(\pm\)1.3 & **31.9\(\pm\)**1.3 \\  & **Llama-2-13b** & 37.1\(\pm\)1.4 & 33.0\(\pm\)1.3 & 29.9\(\pm\)1.3 & 31.3\(\pm\)1.3 & 32.6\(\pm\)1.3 \\  & **Llama-2-70b** & 42.9\(\pm\)1.4 & 36.7\(\pm\)1.4 & 31.6\(\pm\)1.3 & 33.0\(\pm\)1.3 & **35.6\(\pm\)**1.3 \\ \hline \multirow{3}{*}{**iCraft-MD**} & **Llama-2-7b** & 42.1\(\pm\)4.2 & 35.0\(\pm\)4.0 & 30.7\(\pm\)3.9 & 32.1\(\pm\)3.9 & 37.1\(\pm\)4.1 \\  & **Llama-2-13b** & 55.0\(\pm\)4.2 & 42.9\(\pm\)4.2 & 26.3\(\pm\)3.7 & 45.7\(\pm\)4.2 & 38.6\(\pm\)4.1 \\  & **Llama-2-70b** & 60.7\(\pm\)4.1 & 44.3\(\pm\)4.2 & 27.9\(\pm\)3.8 & 45.7\(\pm\)4.2 & 42.1\(\pm\)4.2 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Accuracy of Llama-2-Chat models with varying information availability. Basic is LLM with basic prompting to ask additional questions. Bold results surpass the non-interactive Initial setup.

are a helpful medical assistant), and the Meditron Prompt adopted from Chen et al. (2023). We also explore three **response prompts** (in the user message) for each combination of model size and system prompt: Answer Only (Respond with the correct option and nothing else), Rationale (Explain the rationale, then select the correct option), and Permutate (shuffling the option-answer pairs to remove potential dataset bias).

Results on the development set, depicted in Figure 6, show language models, particularly the Llama-2-Chat series, display consistency despite prompt variations. Performance modestly increases with rationale generation, and is unaffected by answer pair shuffling.

## Appendix F How does prior knowledge influence model choice?

General-purposed LLMs tend to provide the most general answers. We hypothesize that this is because the model does not have enough information provided in the context, so it needs to resort to its parametric knowledge and chooses the most common option to maximize the likelihood of providing the correct answer. Therefore, when given more information in the context, the model should rely less on its parametric knowledge and customize to the patient. We quantitatively show this trend by obtaining the most common option according the Expert and calculate the _generality agreement_--percent agreement between the model choice and its belief of the most common option. Selecting the most common option regardless of context yields the correct response only 33.7% of the time, emphasizing the need for the model to focus on provided context rather than prior assumptions from its parametric knowledge. Results show that in setups with increasing information availability (**None**, **Initial**, **Full**), generality agreement shows an decreasing pattern: 50.0%, 40.2%, and 43.4%, respectively.

## Appendix G How does the nature of the question influence model behavior?

In this section, we show that the nature of the question, such as medical specialty and difficulty, impacts the Expert's interactive behavior. Overall, interaction benefits more difficult questions and certain specialties such as ophthalmology. Figure 11 shows the impact of interactions on diagnostic accuracy across various medical specialties and demographics. Notably, Ophthalmology showed a significant improvement in accuracy from 18.2% to 45.5% after interaction (Figure 12), highlighting the model's potential in specialties with initially low accuracy. Similar trends are evident

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Model Size** & **System** & **Response** & **Full** & **Basic** & **Question** \\ \cline{3-5}  & **Prompt** & **Prompt** & **Context** & **Info** & **Only** \\ \hline \multirow{5}{*}{**LLaMa-2**} & \multirow{3}{*}{Empty} & Answer Only & 33.04 & 32.10 & 31.94 \\  & & Rationale & 35.09 & 33.83 & 30.84 \\  & & Shuffle & 33.04 & 29.90 & 30.84 \\ \cline{2-5}  & \multirow{2}{*}{Basic} & Answer Only & 32.49 & 33.12 & 28.48 \\ \cline{2-5}  & & \multirow{2}{*}{Meditron} & Answer Only & 30.81 & 26.83 & 26.28 \\  & & Rationale & 36.11 & 31.86 & 29.66 \\  & & Shuffle & 30.02 & 26.72 & 28.64 \\ \hline \multirow{5}{*}{**LLaMa-2**} & \multirow{3}{*}{Empty} & Answer Only & 38.00 & 36.03 & 31.63 \\  & & Shuffle & 37.14 & 34.23 & 30.53 \\ \cline{1-1} \cline{2-5}  & \multirow{2}{*}{Basic} & Answer Only & 37.77 & 35.25 & 31.00 \\ \cline{1-1} \cline{2-5}  & & \multirow{2}{*}{Meditron} & Answer Only & 37.07 & 33.01 & 29.93 \\  & & Shuffle & 37.84 & 33.67 & 31.86 \\ \hline \multirow{5}{*}{**LLaMa-2**} & \multirow{3}{*}{Empty} & Answer Only & **43.87** & 36.66 & 33.88 \\ \cline{2-5}  & & \multirow{2}{*}{Basic} & Answer Only & **41.92** & 35.96 & 33.10 \\ \cline{1-1} \cline{2-5}  & & \multirow{2}{*}{Meditron} & Answer Only & **42.88** & 36.69 & 31.58 \\ \cline{1-1}  & & & \multirow{2}{*}{42.20} & 35.92 & 30.95 \\ \cline{1-1} \cline{2-5}  & & & & & \\ \hline
**GPT-3.5** & Meditron & Answer Only & **53.42** & **43.12** & 35.72 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Prompt Variations & Single-Turn Baselines.

in neurosurgery and genomics, suggesting that interactive information-seeking can enhance decision-making. However, interactions in specialties such as family medicine and psychiatry sometimes result in confusion. The effect of interaction on accuracy also extends across different difficulty levels of medical inquiries. For questions that are more challenging and clinically focused (Step 2 & 3), accuracy improved from 43.4% to 48.3%. Overall, this analysis highlights the importance of tailoring interactive diagnostic systems to specific specialties and types of questions for the maximal benefit.

Figure 11: Impact of interactive system on diagnostic accuracy across medical specialties and demographics. Specialties benefiting the most (left) and least (middle) from interaction; improvement by difficulty (right).

Figure 12: Example successful interaction in Ophthalmology.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims match experimental results. We show that the conclusion is generalizable across models and datasets by experimenting with 7 LLMs and two datasets.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of this work is discussed in the Conclusions section (SS 7).
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The paper does not include theoretical results.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper fully disclose all the information needed to reproduce the main experimental results of the paper. All models, datasets, and hyperparameters used are discussed in the Experiments section SS 3.2 & SS 3.2.2 and the Appendix C.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We will release the code to our interactive framework and release a benchmark for the community to evaluate the interactive medical information-seeking ability of any Expert system. Our experiments use publicly available datasets.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All details to run the framework and hyperparameters used in experiments are discussed in the Experiments section SS 3.2 & SS 3.2.2 and the Appendix C, and specific instructions will be release with our code in a GitHub repo.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We include confidence intervals in our main results as shown in Table 2, details on calculating the statistical significance are in Appendix C.2.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Information on the computer resources, including type of compute workers, memory, time of execution, is provided in Appendix C
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics, we provide an ethics statement (SS 7)
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss potential societal impacts in the Introduction section (SS 1) and Ethics Statement section (SS 7).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not release new models or datasets that have a high risk of misuse.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: License information of datasets used are in the Experiments section (SS 3).
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] Justification: Manual annotations were used to validate proposed metrics, which is documented in Appendix A.4.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.