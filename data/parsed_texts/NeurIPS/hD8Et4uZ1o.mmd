# An Equivalence Between Static and Dynamic Regret Minimization

 Andrew Jacobsen

Universita degli Studi di Milano

Politecnico di Milano

contact@andrew-jacobsen.com

&Francesco Orabona

KAUST

francesco@orabona.com

Work done while visiting Optimal Lab at KAUST.

###### Abstract

We study the problem of dynamic regret minimization in online convex optimization, in which the objective is to minimize the difference between the cumulative loss of an algorithm and that of an arbitrary sequence of comparators. While the literature on this topic is very rich, a unifying framework for the analysis and design of these algorithms is still missing. In this paper we show that _for linear losses, dynamic regret minimization is equivalent to static regret minimization in an extended decision space_. Using this simple observation, we show that there is a frontier of lower bounds trading off penalties due to the variance of the losses and penalties due to variability of the comparator sequence, and provide a framework for achieving any of the guarantees along this frontier. As a result, we also prove for the first time that adapting to the squared path-length of an arbitrary sequence of comparators to achieve regret \(R_{T}(\bm{u}_{1},\ldots,\bm{u}_{T})\leq\mathcal{O}(\sqrt{T\sum_{t}\|\bm{u}_{t} -\bm{u}_{t+1}\|^{2}})\) is impossible. However, using our framework we introduce an alternative notion of variability based on a locally-smoothed comparator sequence \(\bar{\bm{u}}_{1},\ldots,\bar{\bm{u}}_{T}\), and provide an algorithm guaranteeing dynamic regret of the form \(R_{T}(\bm{u}_{1},\ldots,\bm{u}_{T})\leq\bar{\mathcal{O}}(\sqrt{T\sum_{i}\| \bar{\bm{u}}_{i}-\bar{\bm{u}}_{i+1}\|^{2}})\), while still matching in the worst case the usual path-length dependencies up to polylogarithmic terms.

## 1 Introduction

This paper introduces new techniques for _Online Convex Optimization_ (OCO), a framework for designing and analyzing algorithms which learn on-the-fly from a stream of data [14, 51, 5, 31, 6]. Formally, consider \(T\) rounds of interaction between the learner and their environment. In each round, the learner chooses \(\bm{w}_{t}\in\mathcal{W}\) from a convex feasible set \(\mathcal{W}\subseteq\mathbb{R}^{d}\), the environment reveals a \(G\)-Lipschitz convex loss function \(\ell_{t}:\mathcal{W}\rightarrow\mathbb{R}\), and the learner incurs a loss of \(\ell_{t}(\bm{w}_{t})\). The classic objective in this setting is to minimize the learner's _regret_ relative to any fixed benchmark \(\bm{u}\in\mathcal{W}\):

\[R_{T}(\bm{u}):=\sum_{t=1}^{T}(\ell_{t}(\bm{w}_{t})-\ell_{t}(\bm{u}))\;.\]

In this paper, we study the more general problem of minimizing the learner's regret relative to any _sequence_ of benchmarks \(\bm{u}_{1},\ldots,\bm{u}_{T}\in\mathcal{W}\)[17, 18]:

\[R_{T}(\bm{u}_{1},\ldots,\bm{u}_{T}):=\sum_{t=1}^{T}(\ell_{t}(\bm{w}_{t})-\ell _{t}(\bm{u}_{t}))\;.\]This objective is typically referred to as _dynamic_ regret, to distinguish it from the special case where the comparator sequence is fixed \(\bm{u}_{1}=\cdots=\bm{u}_{T}\) (referred to as _static_ regret). We focus in particular on the special case of _Online Linear Optimization_ (OLO), in which \(\ell_{t}(\bm{w})=\langle\bm{g}_{t},\bm{w}\rangle\) for some \(\bm{g}_{t}\in\mathbb{R}^{d}\). Note that OCO problems can always be reduced to OLO via the well-known inequality \(\ell_{t}(\bm{w}_{t})-\ell_{t}(\bm{u})\leq\langle\bm{g}_{t},\bm{w}_{t}-\bm{u}\rangle\) for \(\bm{g}_{t}\in\partial\ell_{t}(\bm{w}_{t})\), where \(\partial\ell_{t}(\bm{w}_{t})\) is the subdifferential set of \(\ell_{t}\) at \(\bm{w}_{t}\) [see, _e.g._, 36], so throughout this paper we will focus on the OLO setting.

Intuitively, if the sequence of comparators \(\bm{u}_{1},\ldots,\bm{u}_{T}\) varies too much, it should be impossible to achieve low dynamic regret. On the other hand, we know it is possible to achieve sublinear regret if the sequence of comparators is constant, _i.e._, \(\bm{u}_{1}=\cdots=\bm{u}_{T}\), because this is simply the static case. Hence, we need a way to quantify the complexity, or _variability_, of the comparator sequence. The most commonly used notion of complexity in this regard is the _path-length_ of the comparator sequence [17; 18], defined as

\[P_{T}^{\|\cdot\|}:=\sum_{t=2}^{T}\|\bm{u}_{t}-\bm{u}_{t-1}\|\;.\]

It is possible to show that Online Gradient Descent has a dynamic regret of \(\mathcal{O}((D+P_{T}^{\|\cdot\|})G\sqrt{T})\) in _bounded_ domains, where \(D\) is an upper bound on the diameter of the feasible set and \(G\) is the Lipschitz constant of the losses [51]. This bound was improved to \(\mathcal{O}(\sqrt{DP_{T}^{\|\cdot\|}}G\sqrt{T})\) and shown to be minimax optimal by Zhang et al. [46].

Notice that the path-length bounds scale with a rather pessimistic constant of \(D=\sup_{w,w^{\prime}\in\mathcal{W}}\|\bm{w}-\bm{w}^{\prime}\|\). A better bound would instead scale with the _squared_ path-length:

\[P_{T}^{\|\cdot\|^{2}}:=\sum_{t=1}^{T-1}\|\bm{u}_{t}-\bm{u}_{t-1}\|^{2},\]

which can be significantly smaller2 than the penalty in the bound above: \(P_{T}^{\|\cdot\|^{2}}\leq DP_{T}^{\|\cdot\|}\). However, guarantees scaling with \(P_{T}^{\|\cdot\|^{2}}\) are not well understood in general compared with the more common \(P_{T}^{\|\cdot\|}\) bounds, and have only been obtained by restricting the comparator sequence to \(\bm{u}_{t}=\operatorname*{argmin}_{\bm{w}\in\mathcal{W}}\,\ell_{t}(\bm{w})\) or under additional assumptions such as strong-convexity [44; 45; 7].

Footnote 2: Note that the bound of Zhang et al. [46] trivially implies a squared path-length dependence since \(P_{T}^{\|\cdot\|}\leq\sqrt{TP_{T}^{\|\cdot\|^{2}}}\), so one can obtain a bound of \(\mathcal{O}(\sqrt{DP_{T}^{\|\cdot\|}}G\sqrt{T})\leq\mathcal{O}(D^{1/2}(P_{T} ^{\|\cdot\|^{2}})^{1/4}GT^{3/4})\). However, this bound is not interesting because it does not remove the dependence on \(D\) and it is never better than the existing \(\sqrt{DP_{T}^{\|\cdot\|}}G\sqrt{T}\) bound.

In this paper, we focus on the challenging case that the domain is _unbounded_, where recent works have achieved the dynamic regret \(\widetilde{\mathcal{O}}(\sqrt{\max_{t,t^{\prime}}\|\bm{u}_{t}-\bm{u}_{t^{ \prime}}\|}\,P_{T}^{\|\cdot\|}T)\) in the worst case [20; 25; 21; 47]. Of particular interest, Jacobsen and Cutkosky [20], Zhang et al. [47] achieve bounds of the form

\[R_{T}(\bm{u}_{1},\ldots,\bm{u}_{T})\leq\widetilde{\mathcal{O}}\left(\sqrt{P_{T }^{\|\cdot\|}\sum_{t=1}^{T}\|\bm{g}_{t}\|^{2}\,\|\bm{u}_{t}-\bar{\bm{u}}\|} \right),\] (1)

which avoids the pessimistic multiplicative penalty of \(\max_{t,t^{\prime}}\|\bm{u}_{t}-\bm{u}_{t^{\prime}}\|\), but results in a coupling between the gradient and variability penalties. It is unclear if it is possible to obtain a guarantee which cleanly separates the variability and variance penalties, to achieve dynamic regret scaling as \(R_{T}(\bm{u}_{1},\ldots,\bm{u}_{T})\leq\mathcal{O}\left(\sqrt{P_{T}^{\|\cdot\| ^{2}}\sum_{t=1}^{T}\|\bm{g}_{t}\|^{2}}\right)\). In fact, it is not clear in general how to reason about potential trade-offs that may result from adapting to different measures of variability.

Contributions.In this paper, we show how to reformulate the dynamic regret miniminization problem as an _equivalent_ static regret problem (Section 2). This equivalence allows us to use results for the static regret setting to prove both upper and lower bounds for dynamic regret.

In our first application of this equivalence, we show that the ideal guarantee scaling the with squared path-length \(\mathcal{O}\big{(}\sqrt{P_{T}^{\|\cdot\|^{2}}\sum_{t=1}^{T}\|\bm{g}_{t}\|^{2} }\big{)}\) is **not** possible in general (Section 3). We do this by proving a novel lower bound showing that there is a fundamental trade-off between the penalties incurred due to comparator variability and penalties incurred due to loss variance, leading to a new frontier of dynamic regret lower bounds.

Our second application is to provide a framework for achieving any of the variance/variability trade-offs along the lower bound frontier, up to polylogarithmic terms (Section 4). Our framework allows us to develop dynamic regret algorithms by simply choosing suitable dual-norm pairs \((\left\lVert\cdot\right\rVert,\left\lVert\cdot\right\rVert_{*})\) in the static regret problem. Along with our matching lower bound, this framework provides a concrete way to reason about different measures of comparator variability and the trade-offs they entail, and to design algorithms achieving those trade-offs.

While our lower bound demonstrates that the ideal squared path-length guarantee cannot be achieved, using our framework we show that it is possible to achieve an alternative guarantee that scales with

\[\bar{P}^{\left\lVert\cdot\right\rVert^{2}}(\bm{u}_{1},\ldots,\bm{u}_{T}) \approx\sum_{i}\left\lVert\bar{\bm{u}}_{i}^{(\tau)}-\bar{\bm{u}}_{i+1}^{(\tau )}\right\rVert_{2}^{2},\]

where \(\bar{\bm{u}}_{i}^{(\tau)}\) is a _local average_ of the comparator sequence at a timescale of \(\tau\) (see Section 4.1). Similar to \(P_{T}^{\left\lVert\cdot\right\rVert^{2}}\), this variability measure maintains the property that it matches the worst-case guarantees based on path-length up to polylogarithmic terms, _i.e._, \(\bar{P}_{T}^{\left\lVert\cdot\right\rVert^{2}}\leq\widetilde{\mathcal{O}}( \max_{t,t^{\prime}}\left\lVert\bm{u}_{t}-\bm{u}_{t^{\prime}}\right\rVert P_{T} ^{\left\lVert\cdot\right\rVert})\). These are the first guarantees for general OCO that fully decouple the variance and variability penalties for dynamic regret without explicitly incurring pessimistic \(\max_{t,t^{\prime}}\left\lVert\bm{u}_{t}-\bm{u}_{t^{\prime}}\right\rVert\) penalties.

Related Work.Our approach is inspired by the Haar OLR algorithm of Zhang et al. [47]. In that work, dynamic regret is approached by interpreting the comparator sequence as a high-dimensional "signal" which is decomposed into a frequency domain representation using a dictionary of features. Then, for each feature vector in the dictionary, a 1-dimensional parameter-free [32, 28] algorithm is used to learn how well that feature correlates with the losses. This allows one to compete with an arbitrary comparator sequence, so long as it can be represented in terms of the chosen dictionary of features. We take a similar but slightly more general approach. Our framework also represents the comparator sequence as a high-dimensional signal, but we instead use this signal to define an _equivalent static regret problem_, a perspective that allows us design algorithms for dynamic regret by simply choosing suitable dual-norm pairs.

Other prior works in the general OCO setting have also studied various alternative forms of variability such as the temporal variability \(\sum_{t=1}^{T-1}\sup_{\bm{w}\in\mathcal{W}}\left\lvert\ell_{t}(\bm{w})-\ell_ {t+1}(\bm{w})\right\rvert\)[3, 22, 4] or deviation of the comparator from a given dynamical model \(\sum_{t=1}^{T-1}\left\lVert\bm{u}_{t}-\Phi_{t}(\bm{u}_{t-1})\right\rVert\)[16]. Alternative variance penalties have also been studied in the dynamic setting, such as the small-loss penalties \(\sum_{t=1}^{T}\ell_{t}(\bm{u}_{t})\) or gradient variation penalties \(\sum_{t=1}^{T}\sup_{\bm{w}\in\mathcal{W}}\left\lVert\nabla\ell_{t}(\bm{w})- \nabla\ell_{t+1}(\bm{w})\right\rVert\)[15, 49, 21, 50]. It is also possible to achieve a smaller regret with stronger assumptions on the losses [2]. It is important to note however that almost all prior works, with the exception of Jacobssen and Cutkosky [20], Luo et al. [25] and Zhang et al. [47], study dynamic regret only in the easier bounded domain setting.

There is also an often ignored connection between measures of comparator variability and the function classes studied in non-parametric regression. In particular, considering the case that the losses are \(\ell_{t}(x)=(x-u_{t})^{2}\), then the sequence of comparators \(u_{1},\ldots,u_{T}\) with bounded path length \(C_{T}\) and bounded squared path length \((C_{T}^{\prime})^{2}\) corresponds to the sequence with discrete total variation bounded by \(C_{T}\) and the discrete Sobolev class with bound \(C_{T}^{\prime}\), respectively. In this setting, the minimax rates are known [24, 35] and Koolen et al. [24] obtain the minimax regret for the Sobolev classes, while Baby and Wang [1] for both classes with slightly stronger assumptions. However, these results are not directly related to this paper because we consider linear losses.

Notations.We will use the following definitions and notations. The elements of a matrix \(\bm{A}\in\mathbb{R}^{n\times m}\) are denoted by \(A_{ij}\) for \(i=1,\ldots,n\) and \(j=1,\ldots,m\). Similarly, the elements of a vector \(\bm{u}\in\mathbb{R}^{d}\) are \(u_{i}\) for \(i=1,\ldots,d\). The Kronecker product of matrices \(\bm{A}\in\mathbb{R}^{m\times n}\) and \(\bm{B}\in\mathbb{R}^{p\times q}\) is the block matrix defined by

\[\bm{A}\otimes\bm{B}:=\begin{pmatrix}A_{1,1}\bm{B}&\ldots&A_{1,n}\bm{B}\\ &\vdots&\vdots\\ A_{m,1}\bm{B}&\ldots&A_{m,n}\bm{B}\end{pmatrix}.\]We let \(\mathbf{e}_{t}\) denote the \(t^{\text{th}}\) standard basis vector of \(\mathbb{R}^{T}\) and \(\bm{I}_{d}\) is the \(d\times d\) identity matrix. For a square matrix \(\bm{A}\), \(\text{Diag}\left(\bm{A}\right)\) is the diagonal matrix that contains the elements of the diagonal of \(\bm{A}\). For a positive definite matrix \(\bm{M}\), we define the weighted norm \(\left\|\bm{x}\right\|_{\bm{M}}:=\sqrt{\left\langle\bm{x},\bm{M}\bm{x}\right\rangle}\). For a matrix \(\bm{A}\in\mathbb{R}^{m\times n}\), we denote its Frobenius norm by \(\left\|\bm{A}\right\|_{F}:=\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}A_{i,j}^{2}}\). The vec operator is the mapping defined by stacking the columns of a matrix \(\bm{A}\) in a vector. We will denote by \(\left\|\bm{A}\right\|_{p,p}\) the entry-wise \(p\)-norm of \(\bm{A}\), _i.e._, \(\left\|\bm{A}\right\|_{p,p}:=\left\|\text{vec}(\bm{A})\right\|_{p}\).

## 2 A dynamic-to-static reduction

In this section, we present a general reduction from dynamic regret to static regret. The key idea is to embed the comparator sequence in a high dimensional space \(\mathcal{W}^{T}\), where \(T\) is the number of rounds, so that competing with a _fixed_ comparator \(\widetilde{\bm{u}}\in\mathcal{W}^{T}\) in this high-dimensional space is equivalent to competing with a _sequence_ of comparators in the original space \(\mathcal{W}\). In this way, we can reduce the problem of minimizing the dynamic regret to the one of minimizing the static regret.

Our reduction is shown in Algorithm 1. We simply embed the linear losses \(\bm{g}_{t}\) in a high-dimensional space by setting

\[\widetilde{\bm{g}}_{t}=\mathbf{e}_{t}\otimes\bm{g}_{t}=(\bm{0}_{d}^{\top}, \ldots,\bm{0}_{d}^{\top},\bm{g}_{t}^{\top},\bm{0}_{d}^{\top},\ldots,\bm{0}_{d }^{\top})^{\top},\] (2)

where \(\mathbf{e}_{t}\in\mathbb{R}^{T}\) is the \(t^{\text{th}}\) standard basis vector of \(\mathbb{R}^{T}\) and \(\bm{0}_{d}\in\mathbb{R}^{d}\) denotes the vector of zeros. We pass these losses to the online learning algorithm \(\mathcal{A}\), which predicts with a vector \(\widetilde{\bm{w}}_{t}\in\mathcal{W}^{T}\). Finally, we set \(\bm{w}_{t}\in\mathbb{R}^{d}\) equal to the \(t^{\text{th}}\) "component" of \(\widetilde{\bm{w}}_{t}\), and play \(\bm{w}_{t}\).

We show that the dynamic regret of the resulting algorithm will be equal to the static regret of the algorithm \(\mathcal{A}\). In particular, for any sequence \(\widetilde{\bm{u}}=(\bm{u}_{1},\ldots,\bm{u}_{T})\) in \(\mathcal{W}\) we will denote the concatenation of \(\widetilde{\bm{u}}\) into a single vector in \(\mathcal{W}^{T}\) as

\[\widetilde{\bm{u}}=\sum_{t=1}^{T}\mathbf{e}_{t}\otimes\bm{u}_{t}=(\bm{u}_{1}^ {\top},\ldots,\bm{u}_{T}^{\top})^{\top}\.\] (3)

Then, the following proposition shows that the dynamic regret of Algorithm 1_w.r.t_ any sequence \(\widetilde{\bm{u}}=(\bm{u}_{1},\ldots,\bm{u}_{T})\) is _equal_ to the static regret of \(\mathcal{A}\)_w.r.t_\(\widetilde{\bm{u}}\).

**Proposition 1**.: _Let \(\mathcal{W}\subseteq\mathbb{R}^{d}\) and let \(\mathcal{A}\) be an online learning algorithm with domain \(\mathcal{W}^{T}\). Then, for any sequence \(\widetilde{\bm{u}}=(\bm{u}_{1},\ldots,\bm{u}_{T})\in\mathcal{W}^{T}\), Algorithm 1 guarantees_

\[R_{T}(\widetilde{\bm{u}})=\sum_{t=1}^{T}\left\langle\widetilde{\bm{g}}_{t}, \bm{w}_{t}-\bm{u}_{t}\right\rangle=\sum_{t=1}^{T}\left\langle\widetilde{\bm{ g}}_{t},\widetilde{\bm{w}}_{t}-\widetilde{\bm{u}}\right\rangle=:R_{T}^{\rm Seq }(\widetilde{\bm{u}})\.\]

Proof.: The proof is immediate from Equations (2) and (3). In fact, observe that the cumulative loss of the comparator sequence is precisely

\[\sum_{t=1}^{T}\left\langle\widetilde{\bm{g}}_{t},\bm{u}_{t}\right\rangle= \left\langle\left(\begin{smallmatrix}\bm{g}_{1}\\ \bm{g}_{T}\end{smallmatrix}\right)\!,\left(\begin{smallmatrix}\bm{u}_{1}\\ \bm{u}_{T}\end{smallmatrix}\right)\right\rangle=\left\langle\sum_{t=1}^{T} \mathbf{e}_{t}\otimes\bm{g}_{t},\sum_{t=1}^{T}\mathbf{e}_{t}\otimes\bm{u}_{t} \right\rangle=\left\langle\sum_{t=1}^{T}\widetilde{\bm{g}}_{t},\widetilde{\bm{ u}}\right\rangle\.\]

We get a similar relationship for the algorithm's cumulative loss. Hence, we have \(R_{T}(\widetilde{\bm{u}})=\sum_{t=1}^{T}\left\langle\widetilde{\bm{g}}_{t},\bm {w}_{t}-\bm{u}_{t}\right\rangle=\sum_{t=1}^{T}\left\langle\widetilde{\bm{g}}_{t}, \widetilde{\bm{w}}_{t}-\widetilde{\bm{u}}\right\rangle=R_{T}^{\rm Seq}( \widetilde{\bm{u}})\).

**Remark 1**.: _It is important to note that the regret equivalence holds in the context of **linear losses**. However, our reduction can still be leveraged for arbitrary convex losses by first applying the standard reduction to OLO: \(\sum_{t=1}^{T}\ell_{t}(\bm{w}_{t})-\ell_{t}(\bm{u}_{t})\leq\sum_{t=1}^{T}\left\langle \bm{g}_{t},\bm{w}_{t}-\bm{u}_{t}\right\rangle=R_{T}^{\mathrm{Seq}}(\widetilde{ \bm{u}})\) for \(\bm{g}_{t}\in\partial\ell_{t}(\bm{w}_{t})\)._

While our reduction is exceptionally simple, its utility should not be understated. Proposition 1 is a regret _equivalence_ -- we lose nothing by taking this perspective, yet it allows us to immediately apply all the usual techniques and approaches from the static regret setting. For instance, given any dual norm pair \(\left(\left\lVert\cdot\right\rVert,\left\lVert\cdot\right\rVert_{*}\right)\), it is well-understood how to develop algorithms which adapt simultaneously to the comparator norm \(\left\lVert\widetilde{\bm{u}}\right\rVert\) and to the gradient variance \(\sum_{t=1}^{T}\left\lVert\widetilde{\bm{g}}_{t}\right\rVert_{*}^{2}\) to guarantee

\[R_{T}(\bm{u}_{1},\ldots,\bm{u}_{T})=R_{T}^{\mathrm{Seq}}(\widetilde{\bm{u}}) \leq\widetilde{\mathcal{O}}\left(\left\lVert\widetilde{\bm{u}}\right\rVert \sqrt{\sum_{t=1}^{T}\left\lVert\widetilde{\bm{g}}_{t}\right\rVert_{*}^{2}} \right).\]

Such algorithms are commonly referred to as "parameter-free", or "comparator adaptive", because they achieve this adaptation by completely removing the parameter that depends on the unknown comparator \(\widetilde{\bm{u}}\)[_e.g._, 26, 32, 9, 12, 20, 8, 21, 48]. In this way, we have effectively reduced the problem of minimizing dynamic regret to the problem of selecting a dual-norm pair \(\left(\left\lVert\cdot\right\rVert,\left\lVert\cdot\right\rVert_{*}\right)\) that meaningfully measures the "difficulty" of the sequence in \(\widetilde{\bm{u}}\) and the losses \(\widetilde{\bm{g}}_{t}\). In particular, \(\left(\left\lVert\cdot\right\rVert,\left\lVert\cdot\right\rVert_{*}\right)\) should be chosen with the following considerations in mind:

1. \(\left\lVert\widetilde{\bm{u}}\right\rVert\) should produce a meaningful measure of variability of the comparator sequence \(\bm{u}_{1},\ldots,\bm{u}_{T}\). For instance, we will show in Proposition 2 that the squared path-length arises from a particular weighted norm applied to \(\widetilde{\bm{u}}\).
2. \(\left\lVert\widetilde{\bm{g}}_{t}\right\rVert_{*}\) should not "blow up". Ideally \(\left\lVert\widetilde{\bm{g}}_{t}\right\rVert_{*}\) should match the magnitude of the true losses \(\bm{g}_{t}\) up to polylog factors.
3. \(\left(\left\lVert\cdot\right\rVert,\left\lVert\cdot\right\rVert_{*}\right)\) should be chosen with computational considerations in mind. For instance, to apply an FTRL-based algorithm to the losses \(\widetilde{\bm{g}}_{t}\in\mathbb{R}^{dT}\), efficient implementation will typically require \(\left\lVert\cdot\right\rVert_{*}\) to have sparse subgradients. In general, an ideal dual-norm pair should facilitate updating only \(\mathcal{O}(\log T)\) variables at a time, so as to match the \(\mathcal{O}(d\log T)\) per-step computation enjoyed by existing dynamic regret algorithms. We will see one such example in Section 4.1.

In the next section, we show that there is in fact a fundamental trade-off between the penalties induced by the dual-norm pair \(\left(\left\lVert\cdot\right\rVert,\left\lVert\cdot\right\rVert_{*}\right)\), creating a tension between the first two considerations.

## 3 Lower bounds for unconstrained dynamic regret

In the static regret setting, there is a well-known trade-off between the way in which we measure the complexity of the comparator \(\bm{u}\) and the way in which we measure the complexity of the linear losses \(\bm{g}_{t}\). For example, in Online Mirror Descent [29, 42] one can get a regret guarantee that depends on the maximum diameter of the feasible set with respect to a norm \(\left\lVert\cdot\right\rVert\), while the linear losses are measured using the dual norm \(\left\lVert\cdot\right\rVert_{*}\). The equivalence in Proposition 1 suggests that a similar tension exists for the dynamic regret.

Given the structure of our reduction, it makes sense to focus on the weighted norms \(\left\lVert\cdot\right\rVert_{\bm{M}}\) and \(\left\lVert\cdot\right\rVert_{\bm{M}^{-1}}\), where \(\bm{M}\) is a symmetric positive definite matrix. In particular, the next theorem shows that there is a fundamental trade-off between a _variability penalty_\(\left\lVert\widetilde{\bm{u}}\right\rVert_{\bm{M}}\) and a _variance penalty_\(G^{2}\operatorname{Tr}(\bm{M}^{-1})\) related to the losses. The proof is provided in Appendix A.1 and it is based on a lower bound to the tail of Rademacher chaos of order 2.

**Theorem 1**.: _Let the number of rounds \(T\geq T_{0}\), where \(T_{0}\) is a universal constant. Let \(\mathcal{A}\) be an online learning algorithm, and suppose \(\mathcal{A}\) guarantees \(R_{T}(0)\leq G\epsilon_{T}\) for any sequence of linear losses \(g_{1},\ldots,g_{T}\in\mathbb{R}\) satisfying \(\left|g_{t}\right|\leq G\). Let \(\bm{M}^{-1}\in\mathbb{R}^{T\times T}\) be any symmetric positive definite matrix, denote \(\widetilde{\bm{M}}^{-1}:=\bm{M}^{-1}-\text{Diag}\left(\bm{M}^{-1}\right)\) and \(V_{T}:=\operatorname{Tr}(\bm{M}^{-1})+\|\widetilde{\bm{M}}^{-1}\|_{F}\). Suppose that \(\|\widetilde{\bm{M}}^{-1}\|_{F}^{2}\geq\frac{T}{2}\max_{i}\sum_{j}(\widetilde{ M}_{ij}^{-1})^{2}\). Then, for any \(P\) satisfying \(T_{0}\leq\log_{2}\frac{\sqrt{PV_{T}}}{2\epsilon_{T}}\leq T\), there is a sequence of losses \(g_{1},\ldots,g_{T}\in\mathbb{R}\), and \(\widetilde{\bm{u}}=(u_{1},\ldots,u_{T})^{\top}\in\mathbb{R}^{T}\) satisfying \(\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}=\sqrt{P}\) such that we have_

\[R_{T}(u_{1},\ldots,u_{T})\geq\Omega\left(G\epsilon_{T}+G\sqrt{P\left[ \operatorname{Tr}(\bm{M}^{-1})+\left\|\widetilde{\bm{M}}^{-1}\right\|_{F} \log_{2}^{\frac{1}{2}}\frac{\sqrt{PV_{T}}}{2\epsilon_{T}}\right]}\right)\.\]

Let us first briefly discuss the conditions on \(\bm{M}\). First, note that the restriction that \(\bm{M}\) be positive definite and symmetric simply specifies that \(\left\|\cdot\right\|_{\bm{M}}\) defines a valid norm. The condition on \(\|\widetilde{\bm{M}}^{-1}\|_{F}=\|\bm{M}^{-1}-\text{Diag}\left(\bm{M}^{-1} \right)\|_{F}\) is less straight forward to interpret, but it essentially states that the total "variance" of \(\widetilde{\bm{M}}^{-1}\) is at least as much as that of any of its columns. on a technical level this assumption leads to the restriction that \(P\) satisfies \(\log_{2}\left(\sqrt{PV_{T}}/2\epsilon_{T}\right)\leq T\). This is a natural restriction which encodes the fact that if \(P\) is too large relative to \(T\) (_i.e._, when \(\log_{2}(\sqrt{PV}/2\epsilon_{T})\geq T\)), one can ensure "low" regret by simply playing \(\bm{w}_{t}=\bm{0}\) on every round:

\[R_{T}(\widetilde{\bm{u}})=-\sum_{t=1}^{T}\left\langle\bm{g}_{t},\bm{u}_{t} \right\rangle\leq\max_{t}\left\|\bm{u}_{t}\right\|G\,T\leq G\max_{t}\left\| \bm{u}_{t}\right\|\log_{2}\left(\sqrt{PV_{T}}/2\epsilon\right),\]

and hence the only lower bounds in such settings are trivial ones and it suffices to consider only \(P\) satisfying \(\log\left(\sqrt{PV_{T}}/2\epsilon\right)\leq T\). We will see in Proposition 2 that the matrix that produces the squared path-length satisfies this condition, and it can be seen that symmetric matrices with equal column sums (as is the case in Proposition 3) satisfy this condition as well.

The result of Theorem 1 shows that there is a frontier of lower bounds which trade off penalties related to variability of the comparator sequence and penalties related to the variance of the subgradients. That is, one can not guarantee a small variability penalty in all situations without also accepting a large subgradient variance penalty. The next proposition shows that i) the squared path-length can be represented by a particular choice of the weighted norm \(\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}\), and ii) the fundamental tension between \(\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}\) and its corresponding variance penalty \(\operatorname{Tr}(\bm{M}^{-1})\) prevents any algorithm from attaining the ideal variability dependence of \(\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}=\left(\sqrt{\sum_{t=1}^{T-1}\left\| \bm{u}_{t}-\bm{u}_{t+1}\right\|^{2}}\right)\). In fact, the corresponding variance penalty is \(G^{2}\operatorname{Tr}(\bm{M}^{-1})=\mathcal{O}(G^{2}T^{2})\), resulting in a vacuous guarantee. Proof of the proposition can be found in Appendix A.2.

**Proposition 2**.: _(Adapting to Squared Path-length Requires Superlinear Regret) Define the finite-difference operator \(\bm{\Sigma}\in\mathbb{R}^{T}\) as the matrix with entries_

\[\Sigma_{ij}=\begin{cases}1&\text{if }i=j\\ -1&\text{if }i=j-1\\ 0&\text{otherwise}\end{cases}.\]

_Let \(\mathbf{S}=\bm{\Sigma}^{\top}\bm{\Sigma}\) and \(\bm{M}=\mathbf{S}\otimes\bm{I}_{d}\). Then, \(\bm{M}\) satisfies the assumptions of Theorem 1 and_

\[\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}^{2}=\left\|\bm{u}_{T}\right\|_{2}^{2 }+\sum_{t=1}^{T-1}\left\|\bm{u}_{t}-\bm{u}_{t+1}\right\|_{2}^{2}\qquad\text{ and }\qquad\operatorname{Tr}\left(\bm{M}^{-1}\right)=\frac{T(T+1)}{2}\.\]

Proposition 2 shows that adapting to the squared path-length of an arbitrary comparator sequence _necessarily requires_ incurring a linear penalty, so adapting to the squared path-length is impossible without facing a vacuous guarantee. However, we will show in Section 4.1 that it is possible to adapt to a measure of variability which is similar in spirit to the squared path-length, yet only incurs a \(\operatorname{Tr}(\bm{M}^{-1})=\mathcal{O}(\log T)\) variance penalty.

**Remark 2**.: _The matrix \(\bm{M}\) in Proposition 2 uniquely exposes the the squared path-length up to the bias term \(\left\|\bm{u}_{T}\right\|^{2}\). Such a bias term must appear because in the static regret setting, wherein \(\bm{u}_{1}=\ldots=\bm{u}_{T}=\bm{u}\), the variability measure \(\left\|\cdot\right\|_{M}\) must still reduce to a dependence on \(\left\|\bm{u}\right\|\), otherwisethe guarantee would violate existing \(\widetilde{\Omega}(\left\|\bm{u}\right\|\sqrt{T})\) lower bounds for static regret. More generally, we show in Appendix A.3 that any other choice of bias would similarly lead to \(\operatorname{Tr}\left(\bm{M}^{-1}\right)\geq\Omega(T^{2})\), so Proposition 2 along with our lower bound in Theorem 1 are sufficient to show that adapting to squared path-length requires accepting a vacuous guarantee._

## 4 Dynamic regret for unconstrained OLO via weighted norms

So far, we've seen that there exists a frontier of lower bounds trading off a variability penalty, measured by \(\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}\), and a loss variance penalty, measured by \(\operatorname{Tr}\left(\bm{M}^{-1}\right)\), and that the tension between these two quantities makes it impossible to adapt to the squared path-length of the comparator sequence without accepting a vacuous regret guarantee. A natural next question is whether there are choices of \(\bm{M}\) which lead to a more favorable trade-off of these two quantities. In this section, we provide a simple framework for achieving lower bounds along the frontier described by Theorem 1, and an instance which successfully achieves an improved variance/variability trade-off. The guarantees on the lower bound frontier can be achieved using any parameter-free algorithm along with the 1-dimensional reduction of Cutkosky and Orabona [9] to extend the algorithm to dual-norm pair \((\left\|\cdot\right\|_{\bm{M}},\left\|\cdot\right\|_{\bm{M}^{-1}})\). The generic procedure is summarized in Algorithm 2 for convenience.

``` Input 1-d Parameter-free OLO algorithm \(\mathcal{A}\), positive definite symmetric matrix \(\bm{M}\in\mathbb{R}^{dT\times dT}\) Initialize \(\widetilde{\bm{\theta}}_{1}=\widetilde{\bm{v}}_{1}=\bm{0}\in\mathbb{R}^{dT}\), \(V_{1}=0\) for\(t=1:T\)do  Get \(\beta_{t}\in\mathbb{R}\) from \(\mathcal{A}\)  Play \(\widetilde{\bm{w}}_{t}=\beta_{t}\widetilde{\bm{v}}_{t}\) and observe \(\widetilde{\bm{g}}_{t}\)  Send \(\left\langle\widetilde{\bm{v}}_{t},\widetilde{\bm{g}}_{t}\right\rangle\) to \(\mathcal{A}\) as the \(t^{\text{th}}\) loss  Set \(\widetilde{\bm{\theta}}_{t+1}=\widetilde{\bm{\theta}}_{t}-\bm{M}^{-1} \widetilde{\bm{g}}_{t}\)  Set \(V_{t+1}=V_{t}+\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{M}^{-1}}^{2}\)  Set \(\widetilde{\bm{v}}_{t+1}=\frac{\widetilde{\bm{g}}_{t+1}}{\sqrt{V_{t+1}}} \left[1\wedge\frac{\sqrt{V_{t+1}}}{\left\|\widetilde{\bm{\theta}}_{t+1} \right\|_{\bm{M}^{-1}}}\right]\) // (Projected) Scale-free FTRL update  end for ```

**Algorithm 2**Dynamic regret OLO through 1-dimensional reduction [9]

**Theorem 2**.: _Let \(\mathbf{S}\in\mathbb{R}^{T\times T}\) be a symmetric positive definite matrix, \(\bm{M}=\mathbf{S}\otimes\bm{I}_{d}\), and \(\epsilon>0\). There is an algorithm \(\mathcal{A}\) such that for any \(\bm{g}_{1},\ldots,\bm{g}_{T}\in\mathbb{R}^{d}\) satisfying \(\left\|\bm{g}_{t}\right\|_{2}\leq G\) for all \(t\) and any sequence \(\widetilde{\bm{u}}=\left(\bm{u}_{1},\ldots,\bm{u}_{T}\right)\in\mathbb{R}^{dT}\), the dynamic regret is bounded as_

\[R_{T}(\widetilde{\bm{u}})\leq\mathcal{O}\left(\mathfrak{G}\epsilon+\left\| \widetilde{\bm{u}}\right\|_{\bm{M}}\left[\sqrt{V_{T}\log\left(\frac{\left\| \widetilde{\bm{u}}\right\|_{\bm{M}}\sqrt{V_{T}}}{\mathfrak{G}\epsilon}+1 \right)}\vee\mathfrak{G}\log\left(\frac{\left\|\widetilde{\bm{u}}\right\|_{ \bm{M}}\sqrt{V_{T}}}{\epsilon\mathfrak{G}}\right)\right]\right),\]

_where \(V_{T}=\sum_{t=1}^{T}\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{M}^{-1}}^{2}\) and \(\mathfrak{G}=G\left\|\mathbf{S}^{-1}\right\|_{\infty,\infty}\)._

For the proof, we will need the following technical lemma.

**Lemma 1**.: _Let \(\mathbf{S}\in\mathbb{R}^{T\times T}\) be a symmetric positive definite matrix and let \(\bm{M}=\mathbf{S}\otimes\bm{I}_{d}\). For \(t=1,\ldots,T\), let \(\bm{g}_{t}\in\mathbb{R}^{d}\) and let \(\widetilde{\bm{g}}_{t}=\mathbf{e}_{t}\otimes\bm{g}_{t}\). Then, we have \(\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{M}}^{2}=\left\|\bm{g}_{t}\right\|_{ 2}^{2}S_{tt}\)._

Proof.: Using the mixed-product property \((A\otimes B)(C\otimes D)=AC\otimes BD\) and the transpose property \((A\otimes B)^{\top}=A^{\top}\otimes B^{\top}\) of the Kronecker product, we have that

\[\left\langle\widetilde{\bm{g}}_{t},\bm{M}\widetilde{\bm{g}}_{t}\right\rangle =\left\langle\mathbf{e}_{t}\otimes\bm{g}_{t},\left[\mathbf{S} \otimes\bm{I}_{d}\right]\mathbf{e}_{t}\otimes\bm{g}_{t}\right\rangle=\left\langle \mathbf{e}_{t}\otimes\bm{g}_{t},\mathbf{S}\mathbf{e}_{t}\otimes\bm{g}_{t} \right\rangle=(\mathbf{e}_{t}^{\top}\otimes\bm{g}_{t}^{\top})(\mathbf{S} \mathbf{e}_{t}\otimes\bm{g}_{t})\] \[=\mathbf{e}_{t}^{\top}\mathbf{S}\mathbf{e}_{t}\otimes\bm{g}_{t}^{ \top}\bm{g}_{t}=S_{tt}\left\|\bm{g}_{t}\right\|^{2}\.\qed\]

Proof of Theorem 2.: Applying Proposition 1, we have \(R_{T}(\widetilde{\bm{u}})=\sum_{t=1}^{T}\left\langle\widetilde{\bm{g}}_{t}, \widetilde{\bm{w}}_{t}-\widetilde{\bm{u}}\right\rangle=R_{T}^{\mathrm{Seq}}( \widetilde{\bm{u}})\). Since \(\bm{M}\) is symmetric and positive definite, \((\left\|\cdot\right\|_{\bm{M}},\left\|\cdot\right\|_{\bm{M}^{-1}})\) is a valid dual-norm pair. By Lemma 1,we have \(\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{M}^{-1}}^{2}=\left\|\bm{g}_{t}\right\|_ {2}^{2}S_{tt}^{-1}\leq G^{2}\left\|\mathbf{S}^{-1}\right\|_{\infty,\infty}:= \mathfrak{G}^{2}\). Hence, let \(\mathcal{A}\) be any algorithm which guarantees a parameter-free regret _w.r.t._\((\left\|\cdot\right\|,\left\|\cdot\right\|_{*})\) on losses satisfying \(\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{M}^{-1}}\leq\mathfrak{G}\). Note that any parameter-free algorithm can be extended to handle arbitrary dual-norm pairs by leveraging the one-dimensional reduction of Cutkosky and Orabona (2019, Section 3), that reduces the OLO problem to a unconstrained 1d problem plus an OLO problem in the unitary ball defined by the primal norm. For instance, applying Jacobsen and Cutkosky (2019, Algorithm 1) with the one-dimensional reduction one can easily show (see details in Appendix B.1)

\[R_{T}(\bm{\vec{u}})\leq\mathcal{O}\left(\mathfrak{G}\epsilon+\left\|\bm{\vec{ u}}\right\|_{\bm{M}}\left[\sqrt{V_{T}\log\left(\frac{\left\|\bm{\vec{u}} \right\|_{\bm{M}}\sqrt{V_{T}}\Lambda_{T}}{\mathfrak{G}\epsilon}+1\right)} \vee\mathfrak{G}\log\left(\frac{\left\|\bm{\vec{u}}\right\|_{\bm{M}}\sqrt{V_{ T}}\Lambda_{T}}{\epsilon\mathfrak{G}}\right)\right]\right),\]

where \(V_{T}=\sum_{t=1}^{T}\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{M}^{-1}}^{2}\) and \(\Lambda_{T}=\log^{2}(\sum_{t=1}^{T}\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{ M}^{-1}}^{2}/\mathfrak{G}^{2})\leq\mathcal{O}(\log^{2}T)\). 

Note in particular that by Lemma 1, we have \(\sum_{t=1}^{T}\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{M}^{-1}}^{2}=\sum_{t= 1}^{T}S_{tt}^{-1}\left\|\bm{g}_{t}\right\|^{2}\leq G\sum_{t=1}^{T}S_{tt}^{-1}=G \operatorname{Tr}(\mathbf{S}^{-1})\), so this bound matches the lower bound from Section 3, up to polylogarithmic terms.3 Thus, any valid choice of \(\bm{M}\) will be on the lower bound frontier of Section 3.

Footnote 3: Note that the lower bound is stated for \(d=1\), in which case \(\operatorname{Tr}(\bm{S}^{-1})=\operatorname{Tr}(\bm{M}^{-1})\).

### Trading-off Variance and Variability

Leveraging the algorithm characterized by Theorem 2, we now show that it is indeed possible to choose \(\bm{M}\) such that \(\sum_{t=1}^{T}\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{M}^{-1}}^{2}\) is only \(\mathcal{O}(\log{(T)}\sum_{t=1}^{T}\left\|\bm{g}_{t}\right\|^{2})\), in exchange for a variability penalty which is still similar in spirit to the squared path-length.

Inspired by the Haar OLR algorithm of [47], we apply Theorem 2 using \(\mathbf{S}=\mathbf{H}_{n}\mathbf{H}_{n}^{\top}\), where \(\mathbf{H}_{n}\) is the unnormalized Haar basis matrix of order \(n=\lceil\log_{2}T\rceil\). The Haar wavelet transform and its basis matrix are common tools in the signal processing literature; we recall the basic definitions and facts for convenience in Appendix B.2. With this choice, we have the following bounds on \(\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}\) and \(\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{M}^{-1}}^{2}\). The proof can be found in Appendix B.3.

**Proposition 3**.: _Let \(n=\log_{2}T\) and \(\mathbf{H}_{n}\) be the unnormalized Haar basis matrix of order \(n\). For any \(\tau\in\left\{2^{i}:i=0,\ldots,\log_{2}T\right\}\), let \(N_{\tau}=T/\tau\) and let \(\mathcal{I}_{1}^{(\tau)},\ldots,\mathcal{I}_{N_{\tau}}^{(\tau)}\) be a partition of \([T]\) into intervals of length \(\tau\). Define the average comparator in interval \(\mathcal{I}_{i}^{(\tau)}\) to be \(\bar{\bm{u}}_{i}^{(\tau)}=\frac{1}{\tau}\sum_{t\in\mathcal{I}_{i}^{(\tau)}} \bm{u}_{t}\), and define the squared path-length at time-scale \(\tau<T\) to be_

\[\bar{P}(\bm{\vec{u}},\tau):=\sum_{i=1}^{N_{\tau}/2}\left\|\bar{\bm{u}}_{2i-1}^{ (\tau)}-\bar{\bm{u}}_{2i}^{(\tau)}\right\|_{2}^{2},\]

_and \(\bar{P}(\bm{\vec{u}},T)=\left\|\bar{\bm{u}}_{1}^{(T)}\right\|_{2}^{2}=\left\| \bar{\bm{u}}\right\|_{2}^{2}\). Then, setting \(\mathbf{S}=[\mathbf{H}_{n}\mathbf{H}_{n}^{\top}]^{-1}\) and \(\bm{M}=\mathbf{S}\otimes\bm{I}_{d}\), we have_

\[\left\|\bm{\vec{u}}\right\|_{\bm{M}}^{2} \leq\left\|\bar{\bm{u}}\right\|_{2}^{2}+\frac{1}{4}\sum_{i=0}^{ \log_{2}(T)}\bar{P}(\bm{\vec{u}},2^{i})\leq\left\|\bar{\bm{u}}\right\|_{2}^{2}+ \frac{1}{4}\log{(T)}\max_{\tau}\bar{P}(\bm{\vec{u}},\tau),\] \[\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{M}^{-1}}^{2} =\left\|\bm{g}_{t}\right\|_{2}^{2}(1+\log{T})\;.\]

Summarizing, by applying Algorithm 1 with \(\mathbf{S}=[\mathbf{H}_{n}\mathbf{H}_{n}^{\top}]^{-1}\) we ensure regret

\[R_{T}(\bm{\vec{u}})\leq\widetilde{\mathcal{O}}\left(\sqrt{\left(\left\|\bar{ \bm{u}}\right\|_{2}^{2}+\max_{\tau}^{N_{\tau}/2}\left\|\bar{\bm{u}}_{2i+1}^{( \tau)}-\bar{\bm{u}}_{2i}^{(\tau)}\right\|_{2}^{2}\right)\sum_{t=1}^{T}\left\| \bm{g}_{t}\right\|_{2}^{2}}\right)\;.\]

This is the first _fully decoupled_ guarantee for general dynamic regret which incurs no pessimistic multiplicative penalties of the form \(\max_{t,t^{\prime}}\left\|\bm{u}_{t}-\bm{u}_{t^{\prime}}\right\|\). That is, the terms depending on the comparators and the terms depending on the gradients appear in separate sums. Moreover, observe that this measure of variability can immediately be related to the more standard (first-order/non-squared) path-length using the local averaging lemma of Zhang et al. [47] (Lemma D.7). We have

\[\|\tilde{\bm{u}}\|_{\bm{M}}^{2} \leq\|\tilde{\bm{u}}\|_{2}^{2}+\frac{\log_{2}T}{4}\max_{\tau}\sum_ {i=1}^{N_{\tau}/2}\left\|\tilde{\bm{u}}_{2i-1}^{(\tau)}-\tilde{\bm{u}}_{2^{ \prime}}^{(\tau)}\right\|_{2}^{2}\leq\widetilde{\mathcal{O}}\left(\bar{D}^{2}+ \max_{\tau}\bar{D}\sum_{i=1}^{N_{\tau}/2}\left\|\tilde{\bm{u}}_{2i-1}^{(\tau)}- \tilde{\bm{u}}_{2^{\prime}}^{(\tau)}\right\|_{2}\right)\] \[\leq\widetilde{\mathcal{O}}\left(\bar{D}^{2}+\bar{D}\sum_{t=1}^{T -1}\left\|\bm{u}_{t}-\bm{u}_{t+1}\right\|_{2}\right)\leq\widetilde{\mathcal{O }}\left(\bar{D}^{2}+\bar{D}P_{T}\right),\]

where \(\bar{D}=\max_{\tau,i}\left\|\tilde{\bm{u}}_{i}^{(\tau)}-\bar{\bm{u}}_{i+1}^{( \tau)}\right\|\leq\max_{i,j}\left\|\bm{u}_{i}-\bm{u}_{j}\right\|\). Thus, applying Algorithm 1 with dual-norm pair \((\left\|\cdot\right\|_{\bm{H}_{n}^{-\tau}\mathbf{H}_{n}^{-\tau}},\left\|\cdot \right\|_{\bm{H}_{n}\mathbf{H}_{n}^{\top}})\) still guarantees worst-case regret

\[R_{T}(\bm{u})\leq\widetilde{\mathcal{O}}\left(\left\|\tilde{\bm{u}}\right\|_{ \bm{H}_{n}^{-\tau}\mathbf{H}_{n}^{-1}}\sqrt{\sum_{t=1}^{T}\left\|\tilde{\bm{g }}_{t}\right\|_{\bm{H}_{n}\mathbf{H}_{n}^{\top}}^{2}}\right)\leq\widetilde{ \mathcal{O}}\left(\sqrt{\left(\left\|\tilde{\bm{u}}\right\|_{2}^{2}+\bar{D}P_{ T}\right)\sum_{t=1}^{T}\left\|\bm{g}_{t}\right\|_{2}^{2}}\right),\]

which matches the guarantees of prior works, up to polylogarithmic terms.

Importantly, with \(\bm{M}=\bm{H}_{n}^{-\top}\bm{H}_{n}^{-1}\otimes\bm{I}_{d}\) the dual-norm pair \((\left\|\cdot\right\|_{\bm{M}},\left\|\cdot\right\|_{\bm{M}^{-1}})\) leads to updates that can be implemented efficiently, in requiring only \(O(\log T)\) variables to be updated on each round. This is because the Haar basis matrices are _locally supported_ -- the columns of \(\bm{H}_{n}=\left(\bm{h}^{(1)}\quad\ldots\quad\bm{h}^{(T)}\right)\in\mathbb{R}^ {T\times T}\) form an orthogonal basis with the property that for any \(t\), \([\bm{h}^{(i)}]_{t}\neq 0\) for only \(1+\log_{2}T\) indices \(i\) (see Proposition 5). Hence, \((\bm{H}^{\top}\otimes\bm{I}_{d})\widetilde{\bm{g}}_{t}=(\bm{H}^{\top}\otimes I _{d})(\mathbf{e}_{t}\otimes\bm{g}_{t})=(\bm{H}^{\top}\mathbf{e}_{t})\otimes \bm{g}_{t}\), is a block vector with only \(1+\log_{2}T\) active blocks, requiring that we update only \(O(d\log T)\) indices to maintain each of the variables needed to implement Algorithm 2. We provide the full details of this computation in Appendix B.4, which we summarize below in Proposition 4.

**Proposition 4**.: _The algorithm characterized by applying Theorem 2 with \(\bm{S}=[\bm{H}_{n}\bm{H}_{n}^{\top}]^{-1}\) can be implemented with \(\mathcal{O}\left(d\log T\right)\) per-round computation._

## 5 Recovering Variance-Variability Coupling Guarantees

Our main focus throughout the paper has been on designing algorithms that achieve a regret bounds of the form \(R_{T}(\bm{\bar{u}})\leq O\left(\sqrt{f(\bm{u}_{1},\ldots,\bm{u}_{T})V(\bm{g}_ {1},\ldots,\bm{g}_{T})}\right)\) for some functions \(f\) and \(V\), which cleanly separates the penalties associated with difficult _loss_ sequences from the penalties associated with difficult _comparator_ sequences. However, the first works to achieve unconstrained dynamic regret guarantees uncovered guarantees of a slightly different form, containing a _gradient-comparator correlation_ penalty:

\[R_{T}(\bm{\bar{u}})\leq\widetilde{O}\left(\sqrt{\sum_{t=1}^{T-1}\left\|\bm{u} _{t}-\bm{u}_{t+1}\right\|\underbrace{\sum_{t=1}^{T}\left\|\bm{g}_{t}\right\|^{ 2}\left\|\bm{u}_{t}-\bar{\bm{u}}\right\|}_{\text{Variance/Variability coupling}}}\right),\] (4)

for some reference point \(\bar{\bm{u}}\)[20, 47]. Guarantees of this form allow some degree of coupling between the variability and variance penalties. This can be appealing in certain situations. For instance, guarantees of the form above have the appealing property that the variance penalty completely disappears on any rounds where the comparator \(\bm{u}_{t}\) matches the reference point \(\bar{\bm{u}}\). This can be a very powerful property when one has _a priori_ access to a benchmark model (represented by \(\bar{\bm{u}}\)) which can be expected to predict well _on average_, so that we accumulate the variance penalties only when facing atypical/unexpected conditions.

The prior works achieving a coupling guarantee do so using rather mysterious means. For instance, the guarantee of Jacobsen and Cutkosky [20] achieves the coupling guarantee almost by coincidence, as it appears in response to a composite regularizer they add to the update to cancel out certain unstable terms in the analysis, and the analysis of Zhang et al. [47] recovers a guarantee of a similar form using a rather difficult analysis of the frequency-domain representation of \(\widetilde{\bm{u}}\) after projecting onto the Haar basis vectors. So far there is no unifying explanation of the principles leading to these sorts of guarantees.

Our equivalence in Proposition 1 instead shows that guarantees of the form Equation (4) can instead be understood through the lens of reward-regret duality, a standard tool used to design algorithms in the static regret setting. The reward-regret duality states that in order to guarantee regret of the form \(R_{T}(\bm{u})\leq f(\bm{u})\) for all \(\bm{u}\in\mathcal{W}\), it suffices to design an algorithm that guarantees \(-\sum_{t=1}^{T}\left\langle\bm{g}_{t},\bm{w}_{t}\right\rangle\geq f^{*}(- \sum_{t=1}^{T}\bm{g}_{t})\) for any \(\bm{g}_{1},\ldots,\bm{g}_{T}\). Using Proposition 1, we immediately have the following analogous design principle for dynamic regret. Proof is deferred to Appendix C.1.

**Theorem 3**.: _Let \(\mathrm{Wealth}_{T}:=-\sum_{t=1}^{T}\left\langle\widetilde{\bm{g}}_{t},\widetilde {\bm{w}}_{t}\right\rangle\) denote the "wealth" of an algorithm \(\mathcal{A}\) and let \((f,f^{*})\) be a Fenchel conjugate pair. Then \(\mathcal{A}\) guarantees \(\mathrm{Wealth}_{T}\geq f_{T}^{*}\big{(}-\sum_{t=1}^{T}\widetilde{\bm{g}}_{t}\big{)}\) for any sequence \(\widetilde{\bm{g}}_{1},\ldots,\widetilde{\bm{g}}_{T}\) if and only if \(R_{T}(\widetilde{\bm{u}})\leq f_{T}(\widetilde{\bm{u}})\) for any sequence \(\widetilde{\bm{u}}=(\bm{u}_{1},\ldots,\bm{u}_{T})\) in \(\mathcal{W}\), where \(\widetilde{\bm{u}}=(\bm{u}_{1}^{\top},\ldots,\bm{u}_{T}^{\top})^{\top}\) is the concatenation of the sequence \(\widetilde{\bm{u}}\) into a vector._

So, suppose we would like to design an algorithm that guarantees for any sequence \(\widetilde{\bm{u}}=(\bm{u}_{1},\ldots,\bm{u}_{T})\) and any \(\widetilde{\bm{g}}=(\bm{g}_{1},\ldots,\bm{g}_{T})\) regret of the form

\[R_{T}(\widetilde{\bm{u}})\leq\sqrt{f_{T}(\widetilde{\bm{u}})V_{T}(\widetilde {\bm{u}})},\]

for some \(f_{T}(\widetilde{\bm{u}})\) and \(V_{T}(\widetilde{\bm{u}})=V_{T}(\widetilde{\bm{u}};\widetilde{\bm{g}})\). Then, since \(\sqrt{ab}=\min_{\eta\geq 0}\frac{a}{2\eta}+\frac{\eta}{2}b\), any such algorithm must have \(R_{T}(\widetilde{\bm{u}})\leq\frac{f_{T}(\widetilde{\bm{u}})}{2\eta}+\frac{ \eta}{2}V_{T}(\widetilde{\bm{u}})\) for every \(\eta\geq 0\). So, via Proposition 1 and the the reward-regret duality of Theorem 3, we have that the desired guarantee is equivalent to guaranteeing for all \(\eta\geq 0\) a wealth lower bound of

\[\mathrm{Wealth}_{t}=-\sum_{t=1}^{T}\left\langle\widetilde{\bm{g}}_{t}, \widetilde{\bm{w}}_{t}\right\rangle\geq\left[\frac{f_{T}(\cdot)}{2\eta}+\frac{ \eta}{2}V_{T}(\cdot)\right]^{*}\big{(}-\widetilde{\bm{g}}_{1:T}\big{)}=\frac{f _{T}^{*}\big{(}-2\eta\widetilde{\bm{g}}_{1:T}\big{)}}{2\eta}\ \Box 2\eta V_{T}^{*}\left(\frac{ \widetilde{\bm{g}}_{1:T}}{2\eta}\right),\]

where \(f_{T}^{*}\) and \(V_{T}^{*}\) are the Fenchel conjugates of \(f_{T}\) and \(V_{T}\) respectively, and \((f_{1}\ \Box\ f_{2})\) denotes the _infimal convolution_[34, 19] of \(f_{1}\) and \(f_{2}\):

\[(f_{1}\ \Box\ f_{2})(z)=\inf\left\{f_{1}(y)+f_{2}(z-y)\right\}.\]

Thus, the variance/variability coupling guarantees observed in Equation (4) can be interpreted as achieving wealth lower-bounds for potential functions involving infimal convolution.

The above discussion provides a general characterization of variance/variability coupling guarantees, though it is admittedly less clear how difficult it is to design algorithms from this perspective due to the rather complicated potential function that appears. Nonetheless, we believe that this provides a valuable perspective and insight that could be of general interest. An important direction for future work is to develop useful tools for working with potential functions of this form.

## 6 Conclusion

In this paper, we have shown a way to reduce the problem of dynamic regret minimization to the static one. We proved a novel frontier of lower bounds showing a fundamental trade-off between penalties on the comparators and penalties on the variance of the gradients. In particular, we have shown that it is not possible to achieve a guarantee that scales with \(\sqrt{\sum_{t=1}^{T-1}\left\|\bm{u}_{t}-\bm{u}_{t+1}\right\|^{2}}\) without incurring a variance penalty of \(\mathcal{O}(GT)\). We developed a simple framework for achieving guarantees along the lower bound frontier, and used it to develop the first algorithm making a non-trivial variance/variability decoupling guarantee against arbitrary comparator sequences. Our framework is simple but powerful, allowing one to fully utilize the rich literature of static regret algorithms for online learning.

We conclude by noting some directions for future work. There is a lot of exciting potential to explore different measures of variability induced by different choices of the matrix \(\bm{M}\), as well as going beyond weighted norms. As mentioned in Section 5, developing a useful toolset for potential functions involving infimal convolution is an important next-step for developing and understanding guarantees with a coupled variance/variability penalty, such as Equation (4). Also, our lower bound in Section 3 illustrates the variance-variability trade-off, but achieving the correct logarithmic dependencies proved to be very challenging -- many of the standard tools for proving lower bounds in unconstrained settings revolve around anti-concentration results that do not readily extend to arbitrary weighted norms and higher-dimensions. We look forward to exciting development in these future directions.

## Acknowledgments

We thank Yu-Xiang Wang for the discussion on the function classes studied in non-parametric regression theory.

## References

* [1] Dheeraj Baby and Yu-Xiang Wang. Online forecasting of total-variation-bounded sequences. In _Advances in Neural Information Processing Systems_, volume 32, 2019.
* [2] Dheeraj Baby and Yu-Xiang Wang. Optimal dynamic regret in exp-concave online learning. In _Conference on Learning Theory_, pages 359-409. PMLR, 2021.
* [3] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. _Operations Research_, 63(5):1227-1244, 2015. doi: 10.1287/opre.2015.1408.
* [4] Nicolo Campolongo and Francesco Orabona. A closer look at temporal variability in dynamic online learning, 2021.
* [5] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge University Press, 2006.
* [6] Nicolo Cesa-Bianchi and Francesco Orabona. Online learning algorithms. _Annual Review of Statistics and Its Application_, 8:165-190, 2021.
* [7] Ting-Jui Chang and Shahin Shahrampour. On online optimization: Dynamic regret analysis of strongly convex and smooth problems. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 6966-6973, 2021.
* [8] Keyi Chen, Ashok Cutkosky, and Francesco Orabona. Implicit parameter-free online learning with truncated linear models. In _International Conference on Algorithmic Learning Theory_, pages 148-175. PMLR, 2022.
* [9] Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning in banach spaces. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, _Proceedings of the 31st Conference On Learning Theory_, volume 75 of _Proceedings of Machine Learning Research_, pages 1493-1529. PMLR, 06-09 Jul 2018.
* [10] Irit Dinur, Ehud Friedgut, Guy Kindler, and Ryan O'Donnell. On the Fourier tails of bounded functions over the discrete cube. In _Proceedings of the thirty-eighth annual ACM symposium on Theory of computing_, pages 437-446, 2006.
* [11] Bogdan J Falkowski. Generalized haar spectral representations and their applications. _Nanyang Technological University. Singapore_, 1998.
* [12] Dylan J. Foster, Alexander Rakhlin, and Karthik Sridharan. Online learning: Sufficient statistics and the burkholder method. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, _Proceedings of the 31st Conference On Learning Theory_, volume 75 of _Proceedings of Machine Learning Research_, pages 3028-3064. PMLR, 06-09 Jul 2018.
* [13] Gene H Golub and Charles F Van Loan. _Matrix computations_. JHU press, 2013.
* [14] Geoffrey J. Gordon. Regret bounds for prediction problems. In _Proc. of the twelfth annual conference on Computational learning theory (COLT)_, pages 29-40, 1999.
* [15] Andras Gyorgy and Csaba Szepesvari. Shifting regret, mirror descent, and matrices. In Maria Florina Balcan and Kilian Q. Weinberger, editors, _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 2943-2951, New York, New York, USA, 20-22 Jun 2016. PMLR.
* [16] Eric C. Hall and Rebecca M. Willett. Online optimization in dynamic environments, 2016.

* Herbster and Warmuth [1998] Mark Herbster and Manfred K Warmuth. Tracking the best regressor. In _Proceedings of the eleventh annual conference on Computational learning theory_, pages 24-31, 1998.
* Herbster and Warmuth [2001] Mark Herbster and Manfred K Warmuth. Tracking the best linear predictor. _Journal of Machine Learning Research_, 1(281-309):10-1162, 2001.
* Hiriart-Urruty and Lemarechal [2004] Jean-Baptiste Hiriart-Urruty and Claude Lemarechal. _Fundamentals of convex analysis_. Springer Science & Business Media, 2004.
* Jacobsen and Cutkosky [2022] Andrew Jacobsen and Ashok Cutkosky. Parameter-free mirror descent. In Po-Ling Loh and Maxim Raginsky, editors, _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 4160-4211. PMLR, 02-05 Jul 2022.
* Jacobsen and Cutkosky [2023] Andrew Jacobsen and Ashok Cutkosky. Unconstrained online learning with unbounded losses. In _International Conference on Machine Learning (ICML)_. PMLR, 2023.
* Jadbabaie et al. [2015] Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online Optimization : Competing with Dynamic Comparators. In Guy Lebanon and S. V. N. Vishwanathan, editors, _Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics_, volume 38 of _Proceedings of Machine Learning Research_, pages 398-406, San Diego, California, USA, 09-12 May 2015. PMLR.
* Johnson [1970] Charles Royal Johnson. Positive definite matrices. _The American Mathematical Monthly_, 77(3):259-264, 1970.
* Koolen et al. [2015] Wouter M Koolen, Alan Malek, Peter L Bartlett, and Yasin Abbasi-Yadkori. Minimax time series prediction. In _Advances in Neural Information Processing Systems_, volume 28, 2015.
* Luo et al. [2022] Haipeng Luo, Mengxiao Zhang, Peng Zhao, and Zhi-Hua Zhou. Corralling a larger band of bandits: A case study on switching regret for linear bandits. In Po-Ling Loh and Maxim Raginsky, editors, _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 3635-3684. PMLR, 02-05 Jul 2022.
* Mcmahan and Streeter [2012] Brendan Mcmahan and Matthew Streeter. No-regret algorithms for unconstrained online convex optimization. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 25. Curran Associates, Inc., 2012.
* McMahan and Orabona [2014] H. Brendan McMahan and Francesco Orabona. Unconstrained online linear learning in Hilbert spaces: Minimax algorithms and normal approximations. In Maria Florina Balcan, Vitaly Feldman, and Csaba Szepesvari, editors, _Proceedings of The 27th Conference on Learning Theory_, volume 35 of _Proceedings of Machine Learning Research_, pages 1020-1039, Barcelona, Spain, 13-15 Jun 2014. PMLR.
* Mhammedi and Koolen [2020] Zakaria Mhammedi and Wouter M. Koolen. Lipschitz and comparator-norm adaptivity in online learning. In _Conference on Learning Theory_, pages 2858-2887. PMLR, 2020.
* Nemirovskij and Yudin [1983] Arkadij Semenovic Nemirovskij and David Borisovich Yudin. _Problem complexity and method efficiency in optimization_. Wiley, New York, NY, USA, 1983.
* O'Donnell and Zhao [2015] Ryan O'Donnell and Yu Zhao. Polynomial bounds for decoupling, with applications. _arXiv preprint arXiv:1512.01603_, 2015.
* Orabona [2019] Francesco Orabona. A modern introduction to online learning. _arXiv preprint arXiv:1912.13213_, 2019. Version 6.
* Orabona and Pal [2016] Francesco Orabona and David Pal. Coin betting and parameter-free online learning. In _Proceedings of the 30th International Conference on Neural Information Processing Systems_, NIPS'16, page 577-585, Red Hook, NY, USA, 2016. Curran Associates Inc.
* 69, 2018. ISSN 0304-3975. doi: https://doi.org/10.1016/j.tcs.2017.11.021. Special Issue on ALT 2015.
* [34] R. Tyrrell Rockafellar. _Convex Analysis_. Princeton University Press, 1970.
* [35] Veeranjaneyulu Sadhanala, Yu-Xiang Wang, and Ryan J Tibshirani. Total variation classes beyond 1d: Minimax rates, and the limitations of linear smoothers. In _Advances in Neural Information Processing Systems_, volume 29, 2016.
* [36] Shai Shalev-Shwartz. Online learning and online convex optimization. _Foundations and Trends in Machine Learning_, 4(2), 2011.
* [37] Radomir S. Stankovic and Bogdan J. Falkowski. The Haar wavelet transform: its status and achievements. _Computers & Electrical Engineering_, 29(1):25-44, 2003. ISSN 0045-7906.
* [38] Willi-Hans Steeb and Tan Kiat Shi. _Matrix calculus and Kronecker product with applications and C++ programs_. World Scientific, 1997.
* [39] Josef Stoer, Roland Bulirsch, R Bartels, Walter Gautschi, and Christoph Witzgall. _Introduction to numerical analysis_, volume 2. Springer, 1980.
* [40] Matthew Streeter and H Brendan McMahan. Less regret via online conditioning. _arXiv preprint arXiv:1002.4862_, 2010.
* [41] David F Walnut. _An introduction to wavelet analysis_. Springer Science & Business Media, 2013.
* [42] Manfred K Warmuth and Arun K Jagota. Continuous and discrete-time nonlinear gradient descent: Relative loss bounds and convergence. In _Electronic proceedings of the 5th International Symposium on Artificial Intelligence and Mathematics_, 1997.
* [43] Henry Wolkowicz and George PH Styan. Bounds for eigenvalues using traces. _Linear algebra and its applications_, 29:471-506, 1980.
* [44] Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi. Tracking slowly moving clairvoyant: Optimal dynamic regret of online learning with true and noisy gradient. In Maria Florina Balcan and Kilian Q. Weinberger, editors, _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 449-457, New York, New York, USA, 2016. PMLR.
* [45] Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, and Zhi-Hua Zhou. Improved dynamic regret for non-degenerate functions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [46] Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, pages 1330-1340, 2018.
* [47] Zhiyu Zhang, Ashok Cutkosky, and Yannis Paschalidis. Unconstrained dynamic regret via sparse coding. _Advances in Neural Information Processing Systems_, 36, 2024.
* [48] Zhiyu Zhang, Heng Yang, Ashok Cutkosky, and Ioannis C Paschalidis. Improving adaptive online learning using refined discretization. In _International Conference on Algorithmic Learning Theory_, pages 1208-1233. PMLR, 2024.
* [49] Peng Zhao, Yan-Feng Xie, Lijun Zhang, and Zhi-Hua Zhou. Efficient methods for non-stationary online learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 11573-11585. Curran Associates, Inc., 2022.
* [50] Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou. Adaptivity and non-stationarity: Problem-dependent dynamic regret for online convex optimization. _Journal of Machine Learning Research_, 25(98):1-52, 2024.
* [51] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In _Proceedings of the 20th international conference on machine learning (icml-03)_, pages 928-936, 2003.

Proofs for Section 3 (Lower bounds for unconstrained dynamic regret)

In this section, we provide proof of our main lower bound result from Section 3. We first introduce a technical tool from the literature on decoupling theory and a key lemma (Lemma 2). Proof of our main result is in Appendix A.1.

Consider a function \(f:[-1,1]^{d}\to\mathbb{R}\), defined as

\[f(\bm{x})=\sum_{i,j}A_{i,j}x_{i}x_{j}\,\]

Define \(\bm{A}\) the matrix with elements \(A_{i,j}\). In this section we will use the following notations for quantities related to a polynomial induced by the quadratic form \(\bm{x}\mapsto\langle\bm{x},\bm{A}\bm{x}\rangle\) (see page 6 of O'Donnell and Zhao [30])

\[\operatorname{Var}[f] =\sum_{i,j}A_{i,j}^{2}=\|A\|_{F}^{2},\] \[\operatorname{Inf}_{i}[f] =\sum_{j=1}^{d}(A_{i,j}^{2}+A_{j,i}^{2})\.\]

One of the key difficulties in deriving the lower bound is that squared weighted norms \(\bm{x}\mapsto\langle\bm{x},\bm{A}\bm{x}\rangle\) introduce dependencies between the coordinates of \(\bm{x}\), which breaks the usual lower bound arguments which rely on anti-concentration of _independent_ Rademacher random variables. Instead, we must leverage an anti-concentration result that holds for _polynomials_ of random variables.

**Theorem 4** (Theorem 3 of Dinur et al. [10]).: _There is a universal constant \(C\) such that the following holds. Suppose \(G:\left\{\pm 1\right\}^{d}\to\mathbb{R}\) is a polynomial of degree at most \(2\) and assume \(\operatorname{Var}[g]=1\). Let \(t\geq 1\) and suppose that \(\operatorname{Inf}_{i}[g]\leq C^{-2}t^{-2}\) for all \(i\in[d]\). Then_

\[\mathbb{P}\left\{|g(x)|\geq t\right\}\geq\exp\left(-C^{2}t^{2}4\log 2\right)\.\]

Using this anti-concentration result, the following key lemma provides a general lower bound on the wealth obtainable by any algorithm, subject to the weighting imposed by a matrix \(\bm{A}\).

**Lemma 2**.: _Let \(\mathcal{A}\) be an online learning algorithm, and suppose \(\mathcal{A}\) guarantees \(R_{T}(0)\leq G\epsilon_{T}\) for any sequence of linear losses \(g_{1},\dots,g_{T}\in\mathbb{R}\) satisfying \(|g_{t}|\leq G\). Let \(\bm{A}\in\mathbb{R}^{T\times T}\) be any symmetric positive definite matrix, and let \(\bm{B}=\bm{A}-\text{Diag}\left(\bm{A}\right)\). Then, there is a universal constant \(C>0\) such that for any \(1\leq q\leq\frac{\left\|\bm{B}\right\|_{F}}{C\sqrt{2\max_{i}\sum_{j=1}^{T}B_{ ij}^{2}}}\), there is a sequence of losses \(g_{1},\dots,g_{T}\in\mathbb{R}\) such that_

\[\left\|\begin{pmatrix}g_{1}\\ \vdots\\ g_{T}\end{pmatrix}\right\|_{\bm{A}}^{2}\geq G^{2}\left[\operatorname{Tr}(\bm{A })+q\left\|\bm{A}-\text{Diag}\left(\bm{A}\right)\right\|_{F}\right]\]

_and_

\[R_{T}(0)\geq G\epsilon_{T}\left[1-2^{4C^{2}q^{2}}\right]\.\]

Proof.: Let \(Y_{1},\dots,Y_{T}\) be independent Rademacher random variables and set \(g_{t}=G\,Y_{t}\), so that \(\mathbb{E}\left[R_{T}(0)\right]=\mathbb{E}\left[\sum_{t=1}^{T}g_{t}w_{t} \right]=0\). Then, using the regret equivalence of Proposition 1 and conditioning on any event \(\mathcal{E}\) with \(\mathbb{P}\left\{\mathcal{E}\right\}>0\), we have

\[0 =\mathbb{E}\left[R_{T}(0)\right]\] \[\leq\mathbb{E}\left[R_{T}(0)|\mathcal{E}\right]\mathbb{P}\left\{ \mathcal{E}\right\}+G\,\epsilon_{T}\left(1-\mathbb{P}\left\{\mathcal{E}\right\} \right),\]

where the last line uses the fact that \(\mathcal{A}\) guarantees \(R_{T}(0)\leq G\epsilon_{T}\) for any \(g_{1},\dots,g_{T}\) satisfying \(|g_{t}|\leq G\) for all \(t\). Re-arranging, we have

\[\mathbb{E}\left[R_{T}(0)\middle|\mathcal{E}\right]\geq G\epsilon_{T}\left(1- \frac{1}{\mathbb{P}\left\{\mathcal{E}\right\}}\right)\.\] (5)Next, let \(\widetilde{\bm{g}}_{t}=\mathbf{e}_{t}\otimes g_{t}\) for all \(t\) and consider the event

\[\mathcal{E}=\left\{\left\|\sum_{t=1}^{T}\widetilde{\bm{g}}_{t}\right\|_{\bm{A}}^ {2}=\left\|(g_{1},\ldots,g_{T})^{\top}\right\|_{\bm{A}}^{2}\geq\mathrm{Tr}(\bm {A})+q\left\|\bm{A}-\text{Diag}\left(\bm{A}\right)\right\|_{F}\right\}\]

for some \(q>0\). We proceed by lower bounding the probability of this event.

Observe that

\[\left\|\sum_{t=1}^{T}\widetilde{\bm{g}}_{t}\right\|_{\bm{A}}^{2}=G^{2}\sum_{i, j}Y_{i}Y_{j}A_{ij}=G^{2}\left[\mathrm{Tr}(\bm{A})+\sum_{i,j\neq i}Y_{i}Y_{j}A_{ij }\right].\]

Denote \(\bm{B}=\bm{A}-\text{Diag}\left(\bm{A}\right)\) and note that \(f(Y_{1},\ldots,Y_{T})=\sum_{i,j}Y_{i}Y_{j}B_{ij}\) is a polynomial of degree at most \(2\) and variance \(\mathrm{Var}[f]=\sum_{i,j}B_{ij}^{2}=\left\|\bm{A}-\text{Diag}\left(\bm{A} \right)\right\|_{F}^{2}=\left\|\bm{B}\right\|_{F}^{2}\). Moreover, since \(\bm{A}\) is symmetric we have \(\mathrm{Inf}_{i}[f]=\sum_{j=1}^{T}B_{ij}^{2}+B_{ji}^{2}=2\sum_{j=1}^{T}B_{ij}^ {2}\) for any \(i\). It follows that if we let \(g(\bm{Y})=\frac{f(\bm{Y})}{\sqrt{\left\|\bm{B}\right\|_{F}^{2}}}=\frac{f(\bm {Y})}{\left\|\bm{B}\right\|_{F}}\), then \(g\) is a polynomial of degree at most \(2\), \(\mathrm{Var}[g]=1\), and for any \(i\in[T]\) we have \(\mathrm{Inf}_{i}[g]=\frac{2\sum_{j=1}^{T}B_{ij}^{2}}{\left\|\bm{B}\right\|_{F} ^{2}}\). Hence by Theorem 4, there is a universal constant \(C\) such that for any \(1\leq q\leq\frac{\left\|\bm{B}\right\|_{F}}{C\sqrt{2\max_{i}\sum_{j=1}^{T}B_{ ij}^{2}}}\) it holds that

\[\mathbb{P}\left\{f(\bm{Y})\geq q\left\|\bm{B}\right\|_{F}\right\}=\mathbb{P} \left\{g(\bm{Y})\geq q\right\}\geq\exp\left(-4C^{2}q^{2}\log 2\right)=2^{-4C^{2}q^{ 2}}\.\]

Now observe that \(\mathbb{P}\left\{\mathcal{E}\right\}=\mathbb{P}\left\{f(\bm{Y})\geq q\left\| \bm{B}\right\|_{F}\right\}\) by construction, so Equation (5) can be bound as

\[\mathbb{E}\left[R_{T}(0)\Big{|}\mathcal{E}\right]\geq G\epsilon_{T}\left(1- \frac{1}{\mathbb{P}\left\{\mathcal{E}\right\}}\right)=G\epsilon_{T}\left(1-2^{ 4C^{2}q^{2}}\right),\]

which implies the existence of a sequence \(g_{1},\ldots,g_{T}\in\mathbb{R}\) such that \(R_{T}(0)\geq G\epsilon_{T}\left[1-2^{4C^{2}q^{2}}\right]\) and

\[\left\|\sum_{t=1}^{T}\widetilde{\bm{g}}_{t}\right\|_{\bm{A}}^{2}\geq G^{2} \left[\mathrm{Tr}(\bm{A})+q\left\|\bm{B}\right\|_{F}\right]=G^{2}\left[ \mathrm{Tr}(\bm{A})+q\left\|\bm{A}-\text{Diag}\left(\bm{A}\right)\right\|_{F} \right],\]

for any \(1\leq q\leq\frac{\left\|\bm{B}\right\|_{F}}{C\sqrt{2\max_{i}\sum_{j=1}^{T}B_{ ij}^{2}}}\). 

### Proof of Theorem 1

In this section we prove our main lower bound.

**Theorem 1**.: _Let the number of rounds \(T\geq T_{0}\), where \(T_{0}\) is a universal constant. Let \(\mathcal{A}\) be an online learning algorithm, and suppose \(\mathcal{A}\) guarantees \(R_{T}(0)\leq G\epsilon_{T}\) for any sequence of linear losses \(g_{1},\ldots,g_{T}\in\mathbb{R}\) satisfying \(|g_{t}|\leq G\). Let \(\bm{M}^{-1}\in\mathbb{R}^{T\times T}\) be any symmetric positive definite matrix, denote \(\widetilde{\bm{M}}^{-1}:=\bm{M}^{-1}-\text{Diag}\left(\bm{M}^{-1}\right)\) and \(V_{T}:=\mathrm{Tr}(\bm{M}^{-1})+\|\widetilde{\bm{M}}^{-1}\|_{F}\). Suppose that \(\|\widetilde{\bm{M}}^{-1}\|_{F}^{2}\geq\frac{T}{2}\max_{i}\sum_{j}(\widetilde{ M}_{ij}^{-1})^{2}\). Then, for any \(P\) satisfying \(T_{0}\leq\log_{2}\frac{\sqrt{PV_{T}}}{2\epsilon_{T}}\leq T\), there is a sequence of losses \(g_{1},\ldots,g_{T}\in\mathbb{R}\), and \(\widetilde{\bm{u}}=(u_{1},\ldots,u_{T})^{\top}\in\mathbb{R}^{T}\) satisfying \(\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}=\sqrt{P}\) such that we have_

\[R_{T}(u_{1},\ldots,u_{T})\geq\Omega\left(G\epsilon_{T}+G\sqrt{P\left[\mathrm{ Tr}(\bm{M}^{-1})+\left\|\widetilde{\bm{M}}^{-1}\right\|_{F}\log_{2}^{\frac{1}{2}} \frac{\sqrt{PV_{T}}}{2\epsilon_{T}}\right]}\right)\.\]

Proof.: Denote \(\bm{A}=\bm{M}^{-1}\) and \(\bm{B}=\bm{A}-\text{Diag}\left(\bm{A}\right)\). By Lemma 2, there is a universal constant \(C\) and a sequence \(g_{1},\ldots,g_{T}\in\mathbb{R}\) such that for any \(1\leq q\leq\frac{\left\|\bm{B}\right\|_{F}}{C\sqrt{2\max_{i}\sum_{j=1}^{T}B_{ ij}^{2}}}\), it holds that

\[\left\|\sum_{t=1}^{T}\widetilde{\bm{g}}_{t}\right\|_{\bm{A}}^{2}\geq G^{2}\left[ \mathrm{Tr}(\bm{A})+q\left\|\bm{A}-\text{Diag}\left(\bm{A}\right)\right\|_{F}\right]\]\[R_{T}(0)\geq G\epsilon_{T}\left[1-2^{4C^{2}q^{2}}\right]\;.\]

Hence, choosing comparator sequence \(u_{1},\ldots,u_{T}\in\mathbb{R}\) to satisfy and \(\widetilde{\bm{u}}=(u_{1},\ldots,u_{T})^{\top}=-\sqrt{P}\frac{A\sum_{t=1}^{T} \widetilde{\bm{g}}_{t}}{\left\|\sum_{t=1}^{T}\widetilde{\bm{g}}_{t}\right\|_{ \bm{A}}}\in\mathbb{R}^{T}\), we have \(\left\|\widetilde{\bm{u}}\right\|_{\bm{A}^{-1}}=\left\|\widetilde{\bm{u}} \right\|_{\bm{M}}=\sqrt{P}\) and

\[R_{T}(u_{1},\ldots,u_{T}) =R_{T}(0)-\left\langle\sum_{t=1}^{T}\widetilde{\bm{g}}_{t}, \widetilde{\bm{u}}\right\rangle\] \[=G\sqrt{P}\left\|\sum_{t=1}^{T}\widetilde{\bm{g}}_{t}\right\|_{ \bm{A}}+R_{T}(0)\] \[\geq G\sqrt{P\left[\operatorname{Tr}(\bm{A})+q\left\|\bm{A}- \operatorname{Diag}\left(\bm{A}\right)\right\|_{F}\right]}+R_{T}(0)\] \[\geq G\epsilon_{T}+G\sqrt{P\left[\operatorname{Tr}(\bm{A})+q \left\|\bm{A}-\operatorname{Diag}\left(\bm{A}\right)\right\|_{F}\right]}-G \epsilon_{T}2^{4C^{2}q^{2}}\;.\]

Now, for \(P\) satisfying \(T_{0}:=4C^{2}\leq\log_{2}\left(\frac{\sqrt{P\left[\operatorname{Tr}(\bm{A})+ \left\|\bm{B}\right\|_{F}\right]}}{2\epsilon_{T}}\right)\leq T\) we may choose

\[q=\sqrt{\frac{\log_{2}\left(\frac{\sqrt{P\left[\operatorname{Tr}(\bm{A})+ \left\|\bm{B}\right\|_{F}\right]}}{2\epsilon_{T}}\right)}{4C^{2}}}\;.\]

Indeed, observe that this choice satisfies \(1\leq q\leq\frac{\left\|\bm{B}\right\|_{F}}{C\sqrt{2\max_{i}\sum_{j=1}^{T}B_{ ij}^{2}}}\) as required:

\[1\leq q=\sqrt{\frac{\log_{2}\left(\frac{\sqrt{P\left[\operatorname{Tr}(\bm{A} )+\left\|\bm{B}\right\|_{F}\right]}}{2\epsilon_{T}}\right)}{4C^{2}}}\leq\sqrt{ \frac{T}{4C^{2}}}\leq\frac{\left\|\bm{B}\right\|_{F}}{C\sqrt{2\max_{i}\sum_{j=1 }^{T}B_{ij}^{2}}},\]

where the final inequality uses the assumption \(\left\|\bm{B}\right\|_{F}^{2}/2\max_{i}\sum_{ij}B_{ij}^{2}\geq\frac{T}{4}\). Hence, we have that

\[G\epsilon_{T}2^{4C^{2}q^{2}}\leq\frac{G}{2}\sqrt{P\left[\operatorname{Tr}(\bm{ A})+\left\|\bm{B}\right\|_{F}\right]}\leq\frac{G}{2}\sqrt{P\left[ \operatorname{Tr}(\bm{A})+q\left\|\bm{B}\right\|_{F}\right]},\]

so that the overall the regret can be lower-bounded as

\[R_{T}(u_{1},\ldots,u_{T}) \geq G\epsilon_{T}+\frac{1}{2}G\sqrt{P\left[\operatorname{Tr}(\bm{ A})+q\left\|\bm{B}\right\|_{F}\right]}\] \[=G\epsilon_{T}+\frac{G}{2}\sqrt{P\left[\operatorname{Tr}(\bm{A}) +\left\|\bm{B}\right\|_{F}\frac{\log^{\frac{1}{2}}\left(\sqrt{P[\operatorname{ Tr}(\bm{A})+\left\|\bm{B}\right\|_{F}]}/2\epsilon_{T}\right)}{\sqrt{T_{0}}} \right]}\;.\]

### Proof of Proposition 2

**Proposition 2**.: _(Adapting to Squared Path-length Requires Superlinear Regret) Define the finite-difference operator \(\bm{\Sigma}\in\mathbb{R}^{T}\) as the matrix with entries_

\[\Sigma_{ij}=\begin{cases}1&\text{if }i=j\\ -1&\text{if }i=j-1\\ 0&\text{otherwise}\end{cases}.\]

_Let \(\mathbf{S}=\bm{\Sigma}^{\top}\bm{\Sigma}\) and \(\bm{M}=\mathbf{S}\otimes\bm{I}_{d}\). Then, \(\bm{M}\) satisfies the assumptions of Theorem 1 and_

\[\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}^{2}=\left\|\bm{u}_{T}\right\|_{2}^{2} +\sum_{t=1}^{T-1}\left\|\bm{u}_{t}-\bm{u}_{t+1}\right\|_{2}^{2}\qquad\text{ and }\qquad\operatorname{Tr}\left(\bm{M}^{-1}\right)=\frac{T(T+1)}{2}\;.\]Proof.: We first show the properties that \(\left\|\widetilde{\bm{u}}\right\|_{F}^{2}=\left\|\bm{u}_{T}\right\|_{2}^{2}+\sum_{ t=1}^{T-1}\left\|\bm{u}_{t}-\bm{u}_{t+1}\right\|_{2}^{2}\) and \(\operatorname{Tr}(\bm{\Sigma}^{-1}\bm{\Sigma}^{-\top})=\sum_{t=1}^{T}\left[\bm {\Sigma}^{-1}\bm{\Sigma}^{-\top}\right]_{tt}=\sum_{t=1}^{T}T-t+1=\frac{T(T+1)}{2}\), and show that \(\bm{M}\) satisfies the conditions of Theorem 1 at the end.

Observe that

\[(\bm{\Sigma}\otimes\bm{I}_{d})\widetilde{\bm{u}}=\begin{pmatrix}\bm{I}_{d}&- \bm{I}_{d}&\bm{0}&\bm{0}&\cdots\\ \bm{0}&\bm{I}_{d}&-\bm{I}_{d}&\bm{0}&\cdots\\ \bm{0}&\bm{0}&\bm{I}_{d}&-\bm{I}_{d}&\cdots\\ \vdots&&\ddots&\\ \bm{0}&\bm{0}&\bm{0}&\cdots&\bm{I}_{d}\end{pmatrix}\begin{pmatrix}\bm{u}_{1} \\ \vdots\\ \bm{u}_{T}\end{pmatrix}=\begin{pmatrix}\bm{u}_{1}-\bm{u}_{2}\\ \bm{u}_{2}-\bm{u}_{3}\\ \vdots\\ \bm{u}_{T-1}-\bm{u}_{T}\\ \end{pmatrix},\]

and since \((\bm{\Sigma}^{\top}\otimes\bm{I}_{d})(\bm{\Sigma}\otimes\bm{I}_{d})=(\bm{ \Sigma}^{\top}\bm{\Sigma})\otimes\bm{I}_{d}=\bm{M}\), we have

\[\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}^{2} =\left\langle\widetilde{\bm{u}},(\bm{\Sigma}^{\top}\otimes\bm{I} _{d})(\bm{\Sigma}\otimes\bm{I}_{d})\widetilde{\bm{u}}\right\rangle=\left\langle (\bm{\Sigma}\otimes\bm{I}_{d})\widetilde{\bm{u}},(\bm{\Sigma}\otimes\bm{I}_{d })\widetilde{\bm{u}}\right\rangle\] \[=\left\|\bm{u}_{T}\right\|_{2}^{2}+\sum_{t=1}^{T-1}\left\|\bm{u}_ {t}-\bm{u}_{t+1}\right\|_{2}^{2}\;.\]

Using the inverse property of the Kronecker product, we also have

\[\bm{M}^{-1}=\left[\bm{\Sigma}^{\top}\bm{\Sigma}\otimes\bm{I}_{d}\right]^{-1} =\left[\bm{\Sigma}^{\top}\bm{\Sigma}\right]^{-1}\otimes\bm{I}_{d}=\bm{\Sigma }^{-1}\bm{\Sigma}^{-\top}\otimes\bm{I}_{d},\]

and by Lemma 8 we have that \(\bm{\Sigma}^{-1}\) is the upper-triangular matrix of all \(1\)'s, that is, the matrix with entries

\[\Sigma_{ij}^{-1}=\begin{cases}1&\text{if }i\leq j\\ 0&\text{otherwise}\end{cases},\]

and likewise, \(\bm{\Sigma}^{-\top}\) is a lower-triangular matrix of \(1^{\prime}s\). In other words, for any \(t\) we have

\[\left[\bm{\Sigma}^{-1}\bm{\Sigma}^{-\top}\right]_{tt}=\sum_{i=1}^{T}\Sigma_{ ti}^{-1}\Sigma_{it}^{-\top}=\sum_{i\leq t}\Sigma_{ti}^{-1}=T-t+1\;.\]

So, summing over \(t\) we have

\[\operatorname{Tr}(\bm{\Sigma}^{-1}\bm{\Sigma}^{-\top})=\sum_{t=1}^{T}\left[ \bm{\Sigma}^{-1}\bm{\Sigma}^{-\top}\right]_{tt}=\sum_{t=1}^{T}T-t+1=\frac{T(T+ 1)}{2}\;.\]

Now we show that \(\bm{M}\) satisfies the conditions of Theorem 1. \(\bm{M}=\mathbf{S}\otimes\bm{I}_{d}=[\bm{\Sigma}^{\top}\bm{\Sigma}]\otimes\bm{I }_{d}\) is clearly symmetric since it is the Kronecker product of two symmetric matrices. Observe that for any \(\bm{x}\neq\bm{0}\in\mathbb{R}^{T}\) we have \(\bm{\Sigma}\bm{x}\neq\bm{0}\) by positive definiteness of \(\bm{\Sigma}\) (Lemma 8) and thus \(\left\langle\bm{x},\mathbf{S}\bm{x}\right\rangle=\left\langle\bm{\Sigma}\bm{x},\bm{\Sigma}\bm{x}\right\rangle>0\). Thus, \(\bm{M}=\mathbf{S}\otimes\bm{I}_{d}\) is the Kronecker product of symmetric positive definite matrices, so \(\bm{M}\) is symmetric positive definite [see, _e.g._, 38, Chapter 2].

Lastly, let \(\bm{B}=\Sigma^{-1}\Sigma^{-\top}-\text{Diag}\left(\Sigma^{-1}\Sigma^{-\top}\right)\). We are to show that \(\left\|\bm{B}\right\|_{F}\geq\frac{T}{2}\sum_{j}B_{ij}^{2}\) for any \(i\). First observe that calculation of \([\bm{\Sigma}^{-1}\bm{\Sigma}^{-\top}]_{tt}\) is generalized to

\[\left[\bm{\Sigma}^{-1}\bm{\Sigma}^{-\top}\right]_{ij}=\sum_{k=1}^{T}\Sigma_{ ik}^{-1}\Sigma_{kj}^{-\top}=\sum_{k=1}^{T}\Sigma_{ki}^{-\top}\Sigma_{kj}^{-\top}= \sum_{k=1}^{j\wedge i}1=T-\max\left\{j,i\right\}+1,\]

for any \(i,j\), and likewise \(\bm{B}_{ij}=T-\max\left\{j,i\right\}+1\) for \(i\neq j\) and \(0\) otherwise, from which it is easily seen that \(\max_{i}\sum_{j}B_{ij}^{2}=\sum_{j}B_{ij}^{2}\), so for any \(i\) we have

\[\sum_{j}B_{ij}^{2}\leq\sum_{j}B_{1j}^{2}=\sum_{j=2}^{T}(T-j+1)^{2}=\frac{1}{6}T(2 T^{2}-3T+1)\;.\]On the other hand,

\[\left\|\bm{B}\right\|_{F}^{2} =\sum_{i}\sum_{j}B_{ij}^{2}=\frac{1}{6}T^{2}(T^{2}-1)\] \[=\frac{T}{2}\frac{T}{6}(2T^{2}-2)=\frac{T}{2}\frac{T}{6}(2T^{2}-3T +3T-2)\geq\frac{T}{2}\frac{T}{6}(2T^{2}-3T+1)\] \[\geq\frac{T}{2}\sum_{j}B_{ij}^{2},\]

for any \(i\), where the last line applies the inequality in the previous display. 

### Sufficiency of Proposition 2

The choice of \(\bm{M}\) in Proposition 2 uniquely exposes the squared path-length up to the constant offset term \(\left\|\bm{u}_{T}\right\|^{2}\). In this section we demonstrate that the choice of offset term in Proposition 2 does not make any significant difference for the claim that adapting to the squared path-length requires incurring a \(\operatorname{Tr}(\bm{M}^{-1})\geq\Omega(T^{2})\) penalty, and hence that Proposition 2 is sufficient to demonstrate that adapting to the squared path-length is not possible without incurring vacuous regret.

To expedite the discussion, we first introduce two technical lemmas, proven in Appendices A.3.1 and A.3.2 respectively.

**Lemma 3**.: _Let \(\bm{v}\in\mathbb{R}^{T}\) be an arbitrary non-zero vector and let \(\bm{B}\in\mathbb{R}^{T\times T}\) be a symmetric matrix with eigenvalues \(0=\lambda_{1}(\bm{B})<\lambda_{2}(\bm{B})\leq\ldots\leq\lambda_{T}(\bm{B})\). Then_

\[\operatorname{Tr}((\bm{B}+\bm{v}\bm{v}^{\top})^{-1})\geq\left\|\bm{v}\right\|^ {2}+\sum_{t=2}^{T}\frac{1}{\lambda_{t}(\bm{B})}\.\]

**Lemma 4**.: _Let \(\bm{\Sigma}\in\mathbb{R}^{T\times T}\) denote the finite-difference matrix defined in Proposition 2 and let \(\bm{M}=\bm{\Sigma}^{\top}\bm{\Sigma}\). Then, for any \(T>1\), we have_

\[\lambda_{\max}(\bm{M}^{-1})\leq\frac{9}{10}\operatorname{Tr}(\bm{M}^{-1}),\]

_where \(\lambda_{\max}(\bm{M}^{-1})\) is the maximal eigenvalue of \(\bm{M}^{-1}\)._

Now, Consider the 1-dimensional setting and note that for any positive definite \(\bm{M}\) we can find a unique \(\bm{\Sigma}\) such that \(\bm{M}=\bm{\Sigma}^{\top}\bm{\Sigma}\). Hence,

\[\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}^{2}=\langle\widetilde{\bm{u}},\bm{ M}\widetilde{\bm{u}}\rangle=\left\langle\bm{\Sigma}\widetilde{\bm{u}},\bm{ \Sigma}\widetilde{\bm{u}}\right\rangle,\]

so without loss of generality we can focus on \(\bm{\Sigma}\) for which

\[\left\langle\bm{\Sigma}\widetilde{\bm{u}},\bm{\Sigma}\widetilde{\bm{u}} \right\rangle=\langle\bm{v},\widetilde{\bm{u}}\rangle^{2}+\sum_{t=2}^{T}\|u_{ t}-u_{t-1}\|^{2},\]

where \(\bm{v}\neq\bm{0}\in\mathbb{R}^{T}\). 4 Note such a constant offset term is unavoidable: it is what captures the static regret guarantee in the case where \(u_{1}=\ldots=u_{T}=u\). Proposition 2 considers \(\bm{v}=(0,\ldots,0,1)\) to get \(\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}^{2}=\|u_{T}\|^{2}+\sum_{t=2}^{T}\|u _{t}-u_{t-1}\|^{2}\), though below we will show that any vector \(\bm{v}\) would still lead to \(\operatorname{Tr}(\bm{M}^{-1})=\Omega(T^{2})\).

Footnote 4: Note that any such \(\bm{M}\) is unique. Indeed, if there are positive definite matrices \(\bm{M}_{1}\in\mathbb{R}^{T\times T}\) and \(\bm{M}_{2}\in\mathbb{R}^{T\times T}\) such that \(\left\|\widetilde{\bm{u}}\right\|_{\bm{M}_{1}}^{2}=\langle\bm{v},\widetilde{ \bm{u}}\rangle^{2}+P_{T}^{\|\cdot\|_{2}^{2}}=\left\|\widetilde{\bm{u}}\right\|_ {\bm{M}_{2}}^{2}\) for all \(\widetilde{\bm{u}}\in\mathbb{R}^{T}\), then \((\widetilde{\bm{u}},(\bm{M}_{1}-\bm{M}_{2})\widetilde{\bm{u}})=0\) and hence \(\bm{M}_{1}=\bm{M}_{2}\) since \(\bm{M}_{1}\) and \(\bm{M}_{2}\) are positive definite.

It is clear that the only way to construct expressions of the form above is via matrices \(\bm{\Sigma}\) satisfying

\[\bm{\Sigma}\widetilde{\bm{u}}=c\begin{pmatrix}u_{1}-u_{2}\\ u_{2}-u_{3}\\ \vdots\\ u_{T-1}-u_{T}\\ \langle\bm{v},\widetilde{\bm{u}}\rangle\end{pmatrix},\]where \(c\in\{-1,1\}\) and the order of the rows indices of the vector can be permuted without loss of generality. In particular, the only matrices that can produce these expressions (again noting that the rows can be permuted without loss of generality) are of the form

\[\bm{\Sigma}=c\begin{pmatrix}1&-1&0&0&\dots&0&0\\ 0&1&-1&0&\dots&0&0\\ 0&0&1&-1&\dots&0&0\\ \vdots&&&\ddots&&&\\ 0&0&0&0&\dots&1&-1\\ \hline\end{pmatrix}=:c\begin{pmatrix}\bm{\Delta}\\ \bm{v}^{\top}\end{pmatrix},\]

so \(\bm{M}=\bm{\Sigma}^{\top}\bm{\Sigma}=\bm{\Delta}^{\top}\bm{\Delta}+\bm{v}\bm{ v}^{\top}\). Moreover, \(\bm{\Delta}^{\top}\bm{\Delta}\) is a symmetric matrix with a unique zero eigenvalue (corresponding to vectors in the span of \(\bm{1}=(1,\dots,1)\in\mathbb{R}^{T}\)), so applying Lemma 3,

\[\operatorname{Tr}(\bm{M}^{-1})=\operatorname{Tr}((\bm{\Delta}^{\top}\bm{ \Delta}+\bm{v}\bm{v}^{\top})^{-1})\geq\left\lVert\bm{v}\right\rVert^{2}+\sum_ {t=2}^{T}\frac{1}{\lambda_{t}(\bm{\Delta}^{\top}\bm{\Delta})}.\]

Now, define \(\bm{v}_{0}=(0,\dots,0,1)\in\mathbb{R}^{T}\) and observe that \(\bm{M}_{0}:=\bm{\Delta}^{\top}\bm{\Delta}+\bm{v}_{0}\bm{v}_{0}^{\top}\) is precisely the matrix studied in Proposition 2. We have via the interlacing property of rank-1 updates to symmetric matrices that \(\lambda_{t}(\bm{\Delta}^{\top}\bm{\Delta})\leq\lambda_{t}(\bm{M}_{0})\)[13, Theorem 8.1.8], so overall we have

\[\operatorname{Tr}(\bm{M}^{-1}) \geq\sum_{t=2}^{T}\frac{1}{\lambda_{t}(\bm{M}_{0})}=\sum_{t=2}^{ T}\lambda_{t}(\bm{M}_{0}^{-1})\] \[=\operatorname{Tr}(\bm{M}_{0}^{-1})-\lambda_{\max}(\bm{M}_{0}^{ -1})\] \[\geq\operatorname{Tr}(\bm{M}_{0}^{-1})-\frac{9}{10}\operatorname{ Tr}(\bm{M}_{0}^{-1})\] \[=\frac{1}{10}\operatorname{Tr}(\bm{M}_{0}^{-1})=\frac{1}{10} \frac{T(T+1)}{2}\]

where the last inequality applies Lemma 4 to bound \(\lambda_{\max}(\bm{M}_{0}^{-1})\) and recalls \(\operatorname{Tr}(\bm{M}_{0}^{-1})=\frac{T(T+1)}{2}\) from Proposition 2.

Hence, the variance penalty will still be \(\Omega(T^{2})\) regardless of the choice of bias \(\langle\bm{v},\widetilde{\bm{u}}\rangle^{2}\) in the variability measure. Combined with our lower bound in Theorem 1, it follows that adapting to the squared path-length necessarily implies a variance penalty of \(\operatorname{Tr}(\bm{M}^{-1})\geq\Omega(T^{2})\), leading to a vacuous regret upper bound.

#### a.3.1 Proof of Lemma 3

**Lemma 3**.: _Let \(\bm{v}\in\mathbb{R}^{T}\) be an arbitrary non-zero vector and let \(\bm{B}\in\mathbb{R}^{T\times T}\) be a symmetric matrix with eigenvalues \(0=\lambda_{1}(\bm{B})<\lambda_{2}(\bm{B})\leq\dots\leq\lambda_{T}(\bm{B})\). Then_

\[\operatorname{Tr}((\bm{B}+\bm{v}\bm{v}^{\top})^{-1})\geq\left\lVert\bm{v} \right\rVert^{2}+\sum_{t=2}^{T}\frac{1}{\lambda_{t}(\bm{B})}\;.\]

Proof.: Let \(\bm{A}:=\bm{B}+\bm{v}\bm{v}^{\top}\). Since \(\bm{B}\) is symmetric, we have via the interlacing property that there is an \(a_{1},\dots,a_{T}\geq 0\) such that \(\sum_{t=1}^{T}a_{t}=\left\lVert\bm{v}\right\rVert^{2}\) and \(\lambda_{t}(\bm{A})=\lambda_{t}(\bm{B})+a_{i}\)[see, _e.g._, Theorem 8.1.8 in [13]]. Hence,

\[\operatorname{Tr}((\bm{B}+\bm{v}\bm{v}^{\top})^{-1}) =\operatorname{Tr}(\bm{A}^{-1})=\sum_{t=1}^{T}\lambda_{t}(\bm{A} ^{-1})=\sum_{t=1}^{T}\frac{1}{\lambda_{t}(\bm{A})}\] \[=\sum_{t=1}^{T}\frac{1}{\lambda_{t}(\bm{B})+a_{i}}\geq\min_{ \begin{subarray}{c}a_{1},\dots,a_{T}\geq 0\\ \sum_{t=1}^{T}a_{t}=\left\lVert\bm{v}\right\rVert^{2}\end{subarray}}\sum_{t=1} ^{T}\frac{1}{\lambda_{t}(\bm{B})+a_{i}}\;.\]To analyze the constrained optimization in the last line, let \(\alpha_{1},\ldots,\alpha_{T}\geq 0\), \(\beta\in\mathbb{R}\), and define the Lagrangian

\[L(a_{1},\ldots,a_{T},\alpha_{1},\ldots,\alpha_{T},\beta)=\sum_{t=1}^{T}\frac{1}{ \lambda_{t}(\bm{B})+a_{t}}-\sum_{t=1}^{T}\alpha_{t}a_{t}+\beta\left(\sum_{t=1}^ {T}a_{t}-\left\lVert\bm{v}\right\rVert^{2}\right).\]

For any \(t\), we have

\[\frac{\partial L}{\partial a_{t}} =\frac{-1}{(\lambda_{t}(\bm{B})+a_{t})^{2}}-\alpha_{t}+\beta=0 \iff a_{t}=\frac{1}{\sqrt{\beta-\alpha_{t}}}-\lambda_{t}(\bm{B})\;.\]

Plugging this into the dual \(D(\alpha_{1},\ldots,\alpha_{T},\beta)=\min_{a_{1},\ldots,a_{T}}L(a_{1},\ldots, a_{T},\alpha_{1},\ldots,\alpha_{T},\beta)\) we have

\[D(\alpha_{1},\ldots,\alpha_{T},\beta) =\sum_{t=1}^{T}\sqrt{\beta-\alpha_{t}}-\sum_{t=1}^{T}\alpha_{t} \left(\frac{1}{\sqrt{\beta-\alpha_{t}}}-\lambda_{t}(\bm{B})\right)+\beta\left( \sum_{t=1}^{T}\frac{1}{\sqrt{\beta-\alpha_{t}}}-\lambda_{t}(\bm{B})-\left\lVert \bm{v}\right\rVert^{2}\right)\] \[=\sum_{t=1}^{T}\sqrt{\beta-\alpha_{t}}+\sum_{t=1}^{T}(\beta- \alpha_{t})\frac{1}{\sqrt{\beta-\alpha_{t}}}+\sum_{t=1}^{T}(\alpha_{t}-\beta) \lambda_{t}(\bm{B})-\beta\left\lVert\bm{v}\right\rVert^{2}\] \[=2\sum_{t=1}^{T}\sqrt{\beta-\alpha_{t}}+\sum_{t=1}^{T}(\alpha_{t }-\beta)\lambda_{t}(\bm{B})-\beta\left\lVert\bm{v}\right\rVert^{2}\;.\]

The derivatives of the dual _w.r.t_\(\alpha_{t}\) are

\[\frac{\partial D}{\partial\alpha_{t}}=\frac{-1}{\sqrt{\beta-\alpha_{t}}}+ \lambda_{t}(\bm{B})\;.\]

Observe that for \(\lambda_{1}(\bm{B})=0\), we have \(\frac{\partial D}{\partial\alpha_{t}}=-\frac{1}{\sqrt{\beta-\alpha_{t}}}\leq 0\), so \(D\) is decreasing in \(\alpha_{1}\), so the dual is maximized when \(\alpha_{1}=0\). Using the relation \(a_{1}=\frac{1}{\sqrt{\beta-\alpha_{1}}}-\lambda_{1}(\bm{B})\) above we have \(a_{1}=\frac{1}{\sqrt{\beta-\alpha_{1}}}-\lambda_{1}(\bm{B})=\frac{1}{\sqrt{ \beta}}\). Equating the other derivatives for \(t>1\) to zero we have

\[\frac{1}{\sqrt{\beta-\alpha_{t}}} =\lambda_{t}(\bm{B})\implies\lambda_{t}(\bm{B})+a_{t}=\lambda_{t }(\bm{B})\] \[\implies a_{t}=0\quad\forall t>1\]

where we used the relationship \(a_{t}=\frac{1}{\sqrt{\beta-\alpha_{t}}}-\lambda_{t}(\bm{B})\) from above. Finally, the optimal \(\beta\) is such that \(\sum_{t=1}^{T}a_{t}=\frac{1}{\sqrt{\beta}}=\left\lVert\bm{v}\right\rVert^{2}\), so overall we have

\[\min_{\begin{subarray}{c}a_{1},\ldots,a_{T}\geq 0\\ \sum_{t=1}^{T}a_{t}=\left\lVert\bm{v}\right\rVert^{2}\end{subarray}}\sum_{t=1} ^{T}\frac{1}{\lambda(\bm{B})+a_{i}} =\frac{1}{\sqrt{\beta-\alpha_{1}}}+\sum_{t=2}^{T}\frac{1}{ \lambda_{t}(\bm{B})+a_{i}}=\frac{1}{\sqrt{\beta}}+\sum_{t=2}^{T}\frac{1}{ \lambda_{t}(\bm{B})}\] \[=\left\lVert\bm{v}\right\rVert^{2}+\sum_{t=2}^{T}\frac{1}{\lambda _{t}(\bm{B})}\;.\qed\]

#### a.3.2 Proof of Lemma 4

**Lemma 4**.: _Let \(\bm{\Sigma}\in\mathbb{R}^{T\times T}\) denote the finite-difference matrix defined in Proposition 2 and let \(\bm{M}=\bm{\Sigma}^{\top}\bm{\Sigma}\). Then, for any \(T>1\), we have_

\[\lambda_{\max}(\bm{M}^{-1})\leq\frac{9}{10}\operatorname{Tr}(\bm{M}^{-1}),\]

_where \(\lambda_{\max}(\bm{M}^{-1})\) is the maximal eigenvalue of \(\bm{M}^{-1}\)._

Proof.: The matrix \(\bm{M}^{-1}=\bm{\Sigma}^{-1}\bm{\Sigma}^{-\top}\) is symmetric and positive definite, hence has real eigenvalues. The eigenvalues of \(\bm{M}^{-1}\) can be bound in terms of its trace as follows (see, _e.g._, Theorem 2.1 Wolkowicz and Styan [43], provided for convenience in Theorem 5):

\[\lambda_{\max}\left(\bm{M}^{-1}\right)\leq\frac{\operatorname{Tr}\left(\bm{M}^{ -1}\right)}{T}+\sqrt{(T-1)\left[\frac{\operatorname{Tr}\left(\bm{M}^{-\top} \bm{M}^{-1}\right)}{T}-\left(\frac{\operatorname{Tr}\left(\bm{M}^{-1}\right)}{ T}\right)^{2}\right]}\;.\]Next, observe that by Lemma 8, matrix \(\bm{\Sigma}^{-1}\) is an upper-triangular matrix of all \(1\)'s, so that

\[[\bm{M}^{-1}]_{ij} =\left[\bm{\Sigma}^{-1}\bm{\Sigma}^{-\top}\right]_{ij}=\sum_{k\in[ t]}\Sigma_{ik}^{-1}\Sigma_{kj}^{-\top}=\sum_{k\in[T]}\Sigma_{ik}^{-1} \Sigma_{jk}^{-1}\] \[=\sum_{k\in[T]}\bm{1}\left\{k\geq i\right\}\bm{1}\left\{k\geq j \right\}=T-\max\left\{i,j\right\}+1,\]

Hence,

\[\operatorname{Tr}\left(\bm{M}^{-1}\right)=\sum_{t=1}^{T}[\bm{M}^{-1}]_{ii}= \sum_{t=1}^{T}(T-t+1)=\sum_{t=1}^{T}t=\frac{T(T+1)}{2}.\]

Moreover,

\[\operatorname{Tr}\left(\bm{M}^{-\top}\bm{M}^{-1}\right) =\sum_{t=1}^{T}[\bm{M}^{-\top}\bm{M}^{-1}]_{tt}=\sum_{t=1}^{T}\sum _{k=1}^{t}M_{tk}^{-\top}M_{kt}^{-1}\] \[=\sum_{t=1}^{T}\sum_{k=1}^{t}(M_{kt}^{-1})^{2}=\sum_{t=1}^{T} \sum_{k=1}^{t}(T-\max\left\{t,k\right\}+1)^{2}\] \[=\frac{T(T+1)^{2}(T+2)}{12}\;.\]

Thus,

\[\left[\frac{\operatorname{Tr}\left(\bm{M}^{-\top}\bm{M}^{-1} \right)}{T}-\left(\frac{\operatorname{Tr}\left(\bm{M}^{-1}\right)}{T}\right)^ {2}\right] =\frac{(T+1)^{2}(T+2)}{12}-\frac{(T+1)^{2}}{4}\] \[=\frac{(T+1)^{2}}{4}\left[\frac{T+2}{3}-1\right]\] \[=\frac{(T+1)^{2}}{4}\frac{T-1}{3}\;.\]

Overall, \(\lambda_{\max}(\bm{M}^{-1})\) is bounded by

\[\lambda_{\max}(\bm{M}^{-1}) \leq\frac{\operatorname{Tr}\left(\bm{M}^{-1}\right)}{T}+\sqrt{(T- 1)\frac{(T+1)^{2}}{4}\frac{T-1}{3}}\] \[=\frac{T+1}{2}+\frac{(T+1)(T-1)}{2\sqrt{3}}\] \[=\frac{T(T+1)}{2\sqrt{3}}+\frac{T+1}{2}\left[1-\frac{1}{\sqrt{3}}\right]\] \[\leq\frac{T(T+1)}{2\sqrt{3}}+\frac{T(T+1)}{2}\frac{1}{4}\leq\frac {9}{10}\,\frac{T(T+1)}{2}=\frac{9}{10}\operatorname{Tr}(\bm{M}^{-1}),\]

where the last line observes that \(1-\frac{1}{\sqrt{3}}\leq\frac{1}{2}\leq\frac{T}{4}\) for \(T\geq 2\) and the fact that \(\frac{1}{\sqrt{3}}+\frac{1}{4}\approx 0.83\leq\frac{9}{10}\). 

## Appendix B Proofs for Section 4 (Dynamic regret for unconstrained OLO via weighted norms)

### Details on the 1-Dimensional Reduction

In this section, for completeness we provide the details of the 1-dimensional reduction of Cutkosky and Orabona [9], specialized to dual weighted-norm pairs \((\left\lVert\cdot\right\rVert_{\bm{M}},\left\lVert\cdot\right\rVert_{\bm{M}^{ -1}})\) as well as its regret guarantee.

For concreteness, we choose adaptive FTRL with AdaGrad-norm stepsizes [40] as the direction learner. For simplicity we use the scale-free version of [33], so that the direction learner's update is slightly simpler, not requiring prior knowledge of the Lipschitz constant \(\mathfrak{G}\geq\|\widetilde{\boldsymbol{g}}_{t}\|_{\boldsymbol{M}^{-1}}\).

Using Cutkosky and Orabona [9, Theorem 2], we have that the regret of Algorithm 2 is equal to

\[R_{T}(\widetilde{\boldsymbol{u}})=R_{T}^{\mathcal{A}}(\|\widetilde{ \boldsymbol{u}}\|_{\boldsymbol{M}})+\|\boldsymbol{u}\|_{\boldsymbol{M}}R_{T}^{ \text{direction}}\left(\frac{\widetilde{\boldsymbol{u}}}{\|\widetilde{ \boldsymbol{u}}\|_{\boldsymbol{M}}}\right),\;\forall\widetilde{\boldsymbol{u}} \in\mathbb{R}^{dT},\]

where \(R_{T}^{\mathcal{A}}\) is the regret of \(\mathcal{A}\) over a sequence of \(G\)-Lipschitz linear losses and \(R_{T}^{\text{direction}}\) is the regret of (scale-free) adaptive FTRL with a feasible set equal to the unitary ball defined by \(\|\cdot\|_{\boldsymbol{M}}\).

Choosing the algorithm \(\mathcal{A}\) to be [20, Algorithm 1], we have

\[R_{T}^{\mathcal{A}}(\|\widetilde{\boldsymbol{u}}\|_{\boldsymbol{M}})\leq \mathcal{O}\left(\mathfrak{G}\epsilon+\|\widetilde{\boldsymbol{u}}\|_{ \boldsymbol{M}}\left[\sqrt{V_{T}\log\left(\frac{\|\widetilde{\boldsymbol{u}} \|_{\boldsymbol{M}}\sqrt{V_{T}}\Lambda_{T}}{\mathfrak{G}\epsilon}+1\right)} \vee\mathfrak{G}\log\left(\frac{\|\widetilde{\boldsymbol{u}}\|_{\boldsymbol{ M}}\sqrt{V_{T}}\Lambda_{T}}{\epsilon\mathfrak{G}}\right)\right]\right),\]

where \(V_{T}=\sum_{t=1}^{T}\|\widetilde{\boldsymbol{g}}_{t}\|_{\boldsymbol{M}^{-1}}^ {2}\) and \(\Lambda_{T}=\log^{2}(\sum_{t=1}^{T}\|\widetilde{\boldsymbol{g}}_{t}\|_{ \boldsymbol{M}^{-1}}^{2}/\mathfrak{G}^{2})\leq\mathcal{O}(\log^{2}T)\).

Focusing now on the regret of the direction learner, define the distance generating function \(\psi(\widetilde{\boldsymbol{x}})=\frac{1}{2}\|\widetilde{\boldsymbol{x}}^{2} \|_{\boldsymbol{M}}\). Using [31, Theorem 4.3], we have that \(\psi\) is 1-strongly convex _w.r.t_\(\|\cdot\|_{\boldsymbol{M}}\). Hence, using the regret guarantee of Scale-free FTRL, _i.e._, Theorem 1 of Orabona and Pal [33], for any \(\widetilde{\boldsymbol{v}}\in\mathbb{R}^{dT}\) such that \(\|\widetilde{\boldsymbol{v}}\|_{\boldsymbol{M}}\leq 1\) the regret of the direction learner is

\[R_{T}^{\text{Direction}}(\widetilde{\boldsymbol{v}})\leq\left[\frac{1}{2}\left\| \widetilde{\boldsymbol{v}}\right\|_{\boldsymbol{M}}^{2}+2.75\right]\sqrt{ \sum_{t=1}^{T}\|\widetilde{\boldsymbol{g}}_{t}\|_{\boldsymbol{M}^{-1}}^{2}}+ 3.5\max_{t\leq T}\|\widetilde{\boldsymbol{g}}_{t}\|_{\boldsymbol{M}^{-1}} \leq\mathcal{O}\left(\sqrt{\sum_{t=1}^{T}\|\widetilde{\boldsymbol{g}}_{t}\|_{ \boldsymbol{M}^{-1}}^{2}}\right).\]

Applying this with \(\widetilde{\boldsymbol{v}}=\frac{\widetilde{\boldsymbol{u}}}{\|\widetilde{ \boldsymbol{u}}\|_{\boldsymbol{M}}}\) and combining with the previous two displays leads to the bound stated in the proof of Theorem 2.

### The Haar Matrices and their Properties

In this section we provide some useful supporting lemmas related to the Haar matrices \(\boldsymbol{H}_{n}\). We first introduce the _Haar basis vectors_, which make up the columns of the matrix \(\boldsymbol{H}_{n}\). Throughout this section we assume for simplicity that \(T\) is a power of 2.

**Definition 1**.: _For any \(\tau\in\left\{2^{i}:i=1:\log_{2}(T)\right\}\) and \(i\in[T/\tau]\), the Haar basis vector at timescale \(\tau\) and location \(i\) is the vector in \(\mathbb{R}^{T}\) with entries_

\[[\boldsymbol{h}_{i}^{(\tau)}]_{t}=\begin{cases}1&\text{if }t\in[\frac{1}{2}\tau(i-1)+1, \frac{1}{2}\tau i]\\ -1&\text{if }t\in[\frac{1}{2}\tau i+1,\tau i]\\ 0&\text{otherwise}\end{cases}\] (6)

The Haar basis vectors are often arranged into the columns of a matrix as follows:

\[\mathbf{H}_{n}=\begin{pmatrix}\boldsymbol{h}_{0}&\boldsymbol{h}_{1}^{(T)}& \boldsymbol{h}_{1}^{(T/2)}&\boldsymbol{h}_{2}^{(T/2)}&\boldsymbol{h}_{1}^{(T/ 4)}&\boldsymbol{h}_{2}^{(T/4)}&\boldsymbol{h}_{3}^{(T/4)}&\boldsymbol{h}_{4}^{ (T/4)}&\cdots&\boldsymbol{h}_{T/2}^{(2)}\end{pmatrix},\]

where \(\boldsymbol{h}_{0}=(1,1,\ldots,1)^{\top}\in\mathbb{R}^{T}\). This matrix is referred to as the (unnormalized) Haar basis matrix of order \(n=\log_{2}(T)\). It is well-known that \(\boldsymbol{H}_{n}\) has the following equivalent recursive form [38, 11, 37]:

\[\mathbf{H}_{0} =(1),\] \[\mathbf{H}_{n} =\begin{pmatrix}\mathbf{H}_{n-1}\otimes\begin{pmatrix}1\\ 1\end{pmatrix}&\boldsymbol{I}_{2^{n-1}}\otimes\begin{pmatrix}1\\ -1\end{pmatrix}\end{pmatrix}\,.\] (7)

So, for instance, we have

\[\mathbf{H}_{1} =\begin{pmatrix}\mathbf{H}_{0}\otimes\begin{pmatrix}1\\ 1\end{pmatrix}&\boldsymbol{I}_{2^{0}}\otimes\begin{pmatrix}1\\ -1\end{pmatrix}\end{pmatrix}=\begin{pmatrix}(1)\otimes\begin{pmatrix}1\\ 1\end{pmatrix}&(1)\otimes\begin{pmatrix}1\\ -1\end{pmatrix}\end{pmatrix}=\begin{pmatrix}1&1\\ 1&-1\end{pmatrix},\] \[\mathbf{H}_{2} =\begin{pmatrix}\begin{pmatrix}1&1\\ 1&-1\end{pmatrix}\otimes\begin{pmatrix}1\\ 1\end{pmatrix}&\begin{pmatrix}1&0\\ 0&1\end{pmatrix}\otimes\begin{pmatrix}1\\ -1\end{pmatrix}\end{pmatrix}=\begin{pmatrix}\begin{pmatrix}1&1&1&0\\ 1&1&-1&0\\ 1&-1&0&1\\ 1&-1&0&-1\end{pmatrix},\]and so on. For our purposes, we will primaly work in terms of the matrices \(\bm{H}_{n}\) rather than the basis vectors \(\bm{h}_{i}^{(\tau)}\). The main utility of defining the basis vectors \(\bm{h}_{i}^{(\tau)}\) is that their definition easily implies the following useful result, which states that the Haar basis vectors are _sparsely supported w.r.t_ time.

**Proposition 5**.: _Let \(n=\log_{2}T\) and let \(\bm{H}_{n}\in\mathbb{R}^{T\times T}\) be the unnormalized Haar basis matrix of order \(n\). Then for any \(t\in[T]\), there at most \(1+\log T\) indices \(i\) for which \([\bm{H}_{n}]_{t,i}\neq 0\)._

The proof follows immediately from Definition 1 (_i.e._, any \(t\) can fall into only one of the intervals covered at each of the \(\log_{2}(T)\) time-scales) and accounting for the additional column \(\bm{h}_{0}\) of all \(1\)'s.

In what follows, we will also use the following well-known relationship between the vec operator and the Kronecker product (see, _e.g._, Steeb and Shi [38, Chapter 2.11]).

**Proposition 6**.: _Let \(\bm{A}\), \(\bm{B}\), and \(\bm{C}\) be matrices of appropriate dimensions such that the product \(\bm{ABC}\) exists. Then, \(\text{vec}(\bm{ABC})=(\bm{C}^{\top}\otimes\bm{A})\text{vec}(\bm{B})\)._

The following three lemmas will be used to prove the guarantees of the algorithm characterized in Section 4.1 (Propositions 3 and 4).

**Lemma 5**.: _Let \(n=\log_{2}(T)\), \(\bm{v}=(v_{1},\dots,v_{T})^{\top}\in\mathbb{R}^{T}\), and let \(\mathbf{H}_{n}\) be the unnormalized Haar basis matrix of order \(n\). Then_

\[\mathbf{H}_{n}^{T}\bm{v}=\begin{pmatrix}\mathbf{H}_{n-1}^{\top}\bm{v}_{+}\\ I_{2^{n-1}}\bm{v}_{-}\end{pmatrix},\]

_where_

\[\bm{v}_{+}=\begin{pmatrix}v_{1}+v_{2}\\ v_{3}+v_{4}\\ \vdots\\ v_{T-1}+v_{T}\end{pmatrix},\quad\bm{v}_{-}=\begin{pmatrix}v_{1}-v_{2}\\ v_{3}-v_{4}\\ \vdots\\ v_{T-1}-v_{T}\end{pmatrix}.\]

Proof.: From Equation (7), we have that

\[\mathbf{H}_{n}^{\top}\bm{v} =\begin{pmatrix}\mathbf{H}_{n-1}\otimes\begin{pmatrix}1\\ 1\end{pmatrix}&I_{2^{n-1}}\otimes\begin{pmatrix}1\\ -1\end{pmatrix}\end{pmatrix}^{\top}\bm{v}=\begin{pmatrix}\mathbf{H}_{n-1}^{\top }\otimes\begin{pmatrix}1&1\\ I_{2^{n-1}}\otimes\begin{pmatrix}1&-1\end{pmatrix}\end{pmatrix}\bm{v}\] \[=\begin{pmatrix}\begin{bmatrix}\mathbf{H}_{n-1}^{\top}\otimes \begin{pmatrix}1&1\end{pmatrix}\end{bmatrix}\bm{v}\\ \begin{pmatrix}I_{2^{n-1}}\otimes\begin{pmatrix}1&-1\end{pmatrix}\end{bmatrix} \bm{v}\end{pmatrix}\.\]

Moreover, leveraging Proposition 6 we have

\[\mathbf{H}_{n}^{\top}\bm{v} =\begin{pmatrix}\text{vec}\begin{pmatrix}(1&1)\begin{pmatrix}v_{1} &v_{3}&\cdots&v_{T-1}\\ v_{2}&v_{4}&\cdots&v_{T}\end{pmatrix}\mathbf{H}_{n-1}\end{pmatrix}\\ \text{vec}\begin{pmatrix}(1&-1)\begin{pmatrix}v_{1}&v_{3}&\cdots&v_{T-1}\\ v_{2}&v_{4}&\cdots&v_{T}\end{pmatrix}\mathbf{H}_{n-1}\end{pmatrix}\end{pmatrix}\] \[=\begin{pmatrix}\text{vec}\begin{pmatrix}\overbrace{(v_{1}+v_{2} &v_{3}+v_{4}&\cdots&v_{T-1}+v_{T})}^{=v_{+}^{\top}}\mathbf{H}_{n-1}\\ \text{vec}\begin{pmatrix}\underbrace{(v_{1}-v_{2}&v_{3}-v_{4}&\cdots&v_{T-1}-v _{T})}_{=v_{-}^{\top}}I_{2^{n-1}}\end{pmatrix}\end{pmatrix}\end{pmatrix}\] \[=\begin{pmatrix}\mathbf{H}_{n-1}^{\top}\bm{v}_{+}\\ I_{2^{n-1}}\bm{v}_{-}\end{pmatrix}\.\]

**Lemma 6**.: _Let \(\mathbf{H}_{n}\) be the unnormalized Haar basis matrix of order \(n\). Then, \(\mathbf{H}_{n}\mathbf{H}_{n}^{\top}\) satisfies_

\[\mathbf{H}_{n}\mathbf{H}_{n}^{\top} =\mathbf{H}_{n-1}\mathbf{H}_{n-1}^{\top}\otimes\begin{pmatrix}1&1 \\ 1&1\end{pmatrix}+I_{2^{n-1}}\otimes\begin{pmatrix}1&-1\\ -1&1\end{pmatrix}\] (8) \[=\begin{pmatrix}\mathbf{H}_{n-1}\mathbf{H}_{n-1}^{\top}+\mathbf{1} _{2^{n-1}}&\mathbf{0}_{2^{n-1}}\\ \mathbf{0}_{2^{n-1}}&\mathbf{H}_{n-1}\mathbf{H}_{n-1}^{\top}+\mathbf{1}_{2^{n-1 }},\end{pmatrix},\] (9)

_where \(\mathbf{1}_{2^{n-1}}\) and \(\mathbf{0}_{2^{n-1}}\) are \(2^{n-1}\times 2^{n-1}\) matrices of 1's and 0's respectively._Proof.: For brevity, let us denote \(\bm{B}_{n}=\mathbf{H}_{n}\mathbf{H}_{n}^{\top}\). The first equality follows from elementary properties of block matrices and the Kronecker product: using the recursive form of \(\mathbf{H}_{n}\), we have

\[\bm{B}_{n} =\mathbf{H}_{n}\mathbf{H}_{n}^{\top}\] \[=\begin{pmatrix}\mathbf{H}_{n-1}\otimes\begin{pmatrix}1\\ 1\end{pmatrix}&\bm{I}_{2^{n-1}}\otimes\begin{pmatrix}1\\ -1\end{pmatrix}\right)\begin{pmatrix}\mathbf{H}_{n-1}^{\top}\otimes(1&1)\\ \bm{I}_{2^{n-1}}\otimes(1&-1)\end{pmatrix}\] \[=\mathbf{H}_{n-1}\otimes\begin{pmatrix}1\\ 1\end{pmatrix}\mathbf{H}_{n-1}^{\top}\otimes(1&1)+\bm{I}_{2^{n-1}}\otimes \begin{pmatrix}1\\ -1\end{pmatrix}\bm{I}_{2^{n-1}}\otimes(1&-1)\] \[=\mathbf{H}_{n-1}\mathbf{H}_{n-1}^{\top}\otimes\begin{pmatrix}1\\ 1\end{pmatrix}(1&1)+\bm{I}_{2^{n-1}}\otimes\begin{pmatrix}1\\ -1\end{pmatrix}(1&-1)\] \[=\mathbf{H}_{n-1}\mathbf{H}_{n-1}^{\top}\otimes\begin{pmatrix}1& 1\\ 1&1\end{pmatrix}+\bm{I}_{2^{n-1}}\otimes\begin{pmatrix}1&-1\\ -1&1\end{pmatrix}\] \[=\bm{B}_{n-1}\otimes\begin{pmatrix}1&1\\ 1&1\end{pmatrix}+\bm{I}_{2^{n-1}}\otimes\begin{pmatrix}1&-1\\ -1&1\end{pmatrix}\;.\]

To get the second expression, let us proceed by induction. We have \(\bm{B}_{0}=(1)\) and

\[\bm{B}_{1}=\mathbf{H}_{1}\mathbf{H}_{1}^{\top}=\begin{pmatrix}1&1\\ 1&-1\end{pmatrix}\begin{pmatrix}1&1\\ 1&-1\end{pmatrix}^{\top}=\begin{pmatrix}2&0\\ 0&2\end{pmatrix}=\begin{pmatrix}\bm{B}_{0}+\mathbf{1}_{1}&\bm{0}_{1}\\ \bm{0}_{1}&\bm{B}_{0}+\mathbf{1}_{1}\end{pmatrix}\;.\]

Next, let us assume that \(\bm{B}_{n}\) satisfies

\[\bm{B}_{n}=\begin{pmatrix}\bm{B}_{n-1}+\mathbf{1}_{2^{n-1}}&\bm{0}_{2^{n-1}}\\ \bm{0}_{2^{n-1}}&\bm{B}_{n-1}+\mathbf{1}_{2^{n-1}}\end{pmatrix}\;.\]

Then, applying the recursive form Equation (8) for \(\bm{B}_{n+1}\), we have

\[\bm{B}_{n+1} =\bm{B}_{n}\otimes\begin{pmatrix}1&1\\ 1&1\end{pmatrix}+\bm{I}_{2^{n}}\otimes\begin{pmatrix}1&-1\\ -1&1\end{pmatrix}\] \[=\begin{pmatrix}\bm{B}_{n-1}+\mathbf{1}_{2^{n-1}}&\bm{0}_{2^{n-1}} \\ \bm{0}_{2^{n-1}}&B_{n-1}+\mathbf{1}_{2^{n-1}}\end{pmatrix}\otimes\begin{pmatrix} 1&1\\ 1&1\end{pmatrix}+\bm{I}_{2^{n}}\otimes\begin{pmatrix}1&-1\\ -1&1\end{pmatrix}\] \[=\begin{pmatrix}\bm{B}_{n-1}\otimes\begin{pmatrix}1&1\\ 1&1\end{pmatrix}+\mathbf{1}_{2^{n-1}}\otimes\begin{pmatrix}1&1\\ 1&1\end{pmatrix}\qquad\qquad\qquad\bm{0}_{2^{n}}\] \[\qquad\quad+\begin{pmatrix}\bm{I}_{2^{n-1}}\otimes\begin{pmatrix} 1&-1\\ -1&1\end{pmatrix}&\bm{0}_{2^{n}}\\ \bm{0}_{2^{n}}&\bm{I}_{2^{n}}\otimes\begin{pmatrix}1&-1\\ -1&1\end{pmatrix}\end{pmatrix}\] \[=\begin{pmatrix}\bm{B}_{n}+\mathbf{1}_{2^{n}}&\bm{0}_{2^{n}}\\ \bm{0}_{2^{n}}&\bm{B}_{n}+\mathbf{1}_{2^{n},}\end{pmatrix},\]

where the last line observes that \(\mathbf{1}_{2^{n-1}}\otimes\begin{pmatrix}1&1\\ 1&1\end{pmatrix}=\mathbf{1}_{2^{n}}\) and that after adding the two block matrices, the top left and bottom right blocks are both

\[\bm{B}_{n-1}\otimes\begin{pmatrix}1&1\\ 1&1\end{pmatrix}+\bm{I}_{2^{n-1}}\otimes\begin{pmatrix}1&-1\\ -1&1\end{pmatrix}+\mathbf{1}_{2^{n}}=\bm{B}_{n}+\mathbf{1}_{2^{n}},\]

via Equation (8). Hence, the stated result follows by induction. 

Now using this, we have the following bound on the norm of the high-dimensional surrogate losses.

**Lemma 7**.: _Let \(n=\log_{2}(T)\), \(\mathbf{e}_{t}\) be the \(t^{\text{th}}\) standard basis vector of \(\mathbb{R}^{T}\), and for \(\bm{g}_{t}\in\mathbb{R}^{d}\) let \(\widetilde{\bm{g}}_{t}=\mathbf{e}_{t}\otimes\bm{g}_{t}\in\mathbb{R}^{dT}\). Let \(\mathbf{H}_{n}\) be a Haar matrix of order \(n\) and let \(\bm{B}=\mathbf{H}_{n}\otimes I_{d}\) be it's block extension to sequence in \(\mathbb{R}^{d}\). Then, we have_

\[\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{B}\bm{B}^{\top}}^{2}=(\log T+1) \left\|\bm{g}_{t}\right\|_{2}^{2}\;.\]Proof.: Using Lemma 1, we have that

\[\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{B}\bm{B}^{\top}}^{2}=\left[\bm{H}_{n} \bm{H}_{n}^{\top}\right]_{tt}\left\|\bm{g}_{t}\right\|^{2}\.\]

Moreover, using Equation (9) it can easily be seen that the diagonal entries of \(\bm{H}_{n}\bm{H}_{n}^{\top}\) are \(\log_{2}T+1\), so we have

\[\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{B}\bm{B}^{\top}}^{2}\leq\left(1+ \log_{2}T\right)\left\|\bm{g}_{t}\right\|_{2}^{2}\.\qed\]

### Proof of Proposition 3

**Proposition 3**.: _Let \(n=\log_{2}T\) and \(\bm{\mathrm{H}}_{n}\) be the unnormalized Haar basis matrix of order \(n\). For any \(\tau\in\left\{2^{i}:i=0,\ldots,\log_{2}T\right\}\), let \(N_{\tau}=T/\tau\) and let \(\mathcal{I}_{1}^{(\tau)},\ldots,\mathcal{I}_{N_{\tau}}^{(\tau)}\) be a partition of \([T]\) into intervals of length \(\tau\). Define the average comparator in interval \(\mathcal{I}_{i}^{(\tau)}\) to be \(\bar{\bm{u}}_{i}^{(\tau)}=\frac{1}{\tau}\sum_{t\in\mathcal{I}_{i}^{(\tau)}} \bm{u}_{t}\), and define the squared path-length at time-scale \(\tau<T\) to be_

\[\bar{P}(\vec{\bm{u}},\tau):=\sum_{i=1}^{N_{\tau}/2}\left\|\bar{\bm{u}}_{2i-1}^ {(\tau)}-\bar{\bm{u}}_{2i}^{(\tau)}\right\|_{2}^{2},\]

_and \(\bar{P}(\vec{\bm{u}},T)=\left\|\bar{\bm{u}}_{1}^{(T)}\right\|_{2}^{2}=\left\| \bar{\bm{u}}\right\|_{2}^{2}\). Then, setting \(\mathbf{S}=[\bm{\mathrm{H}}_{n}\bm{\mathrm{H}}_{n}^{\top}]^{-1}\) and \(\bm{M}=\mathbf{S}\otimes\bm{I}_{d}\), we have_

\[\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}^{2} \leq\left\|\bar{\bm{u}}\right\|_{2}^{2}+\frac{1}{4}\sum_{i=0}^{ \log_{2}(T)}\bar{P}(\vec{\bm{u}},2^{i})\leq\left\|\bar{\bm{u}}\right\|_{2}^{2} +\frac{1}{4}\log\left(T\right)\max_{\tau}\bar{P}(\vec{\bm{u}},\tau),\] \[\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{M}^{-1}}^{2} =\left\|\bm{g}_{t}\right\|_{2}^{2}(1+\log T)\.\]

Proof.: The proof of the claim is provided in Lemma 7.

To see the form of \(\left\|\widetilde{\bm{u}}\right\|_{\bm{M}}^{2}\), let us first write

\[\left\|\widetilde{\bm{u}}\right\|_{M}^{2}=\left\langle\widetilde{\bm{u}},[\bm{ H}\bm{H}^{\top}]^{-1}\widetilde{\bm{u}}\right\rangle=\left\langle\bm{H}^{-1} \widetilde{\bm{u}},\bm{H}^{-1}\widetilde{\bm{u}}\right\rangle=\left\|\bm{H}^{ -1}\widetilde{\bm{u}}\right\|_{2}^{2}\.\]

The result then follows by showing that

\[\bm{H}^{-1}\widetilde{\bm{u}}=\frac{1}{2}\begin{pmatrix}2\bar{\bm{u}}\\ \bar{\bm{u}}_{1}^{(T/2)}-\bar{\bm{u}}_{2}^{(T/2)}\\ \bar{\bm{u}}_{1}^{(T/4)}-\bar{\bm{u}}_{2}^{(T/4)}\\ \bar{\bm{u}}_{3}^{(T/4)}-\bar{\bm{u}}_{4}^{(T/4)}\\ \vdots\\ \bm{u}_{1}-\bm{u}_{2}\\ \bm{u}_{3}-\bm{u}_{4}\\ \vdots\\ \bm{u}_{T-1}-\bm{u}_{T}\end{pmatrix},\] (10)

so that

\[\left\|\bm{H}^{-1}\widetilde{\bm{u}}\right\|_{2}^{2}=\underbrace{ \left\|\bar{\bm{u}}\right\|_{2}^{2}}_{\widetilde{P}(T)}+\frac{1}{4}\underbrace{ \left\|\bar{\bm{u}}_{1}^{(T/2)}-\bar{\bm{u}}_{2}^{(T/2)}\right\|_{2}^{2}}_{ \widetilde{P}(T/2)}+\underbrace{\frac{1}{4}\left\|\bar{\bm{u}}_{1}^{(T/4)}- \bar{\bm{u}}_{2}^{(T/4)}\right\|_{2}^{2}+\frac{1}{4}\left\|\bar{\bm{u}}_{3}^{( T/4)}-\bar{\bm{u}}_{4}^{(T/4)}\right\|_{2}^{2}}_{\widetilde{P}(T/4)}\] \[\qquad\qquad\qquad+\ldots+\underbrace{\frac{1}{4}\left\|\bm{u}_{ 1}-\bm{u}_{2}\right\|_{2}^{2}+\frac{1}{4}\left\|\bm{u}_{3}-\bm{u}_{4}\right\|_{ 2}^{2}+\ldots+\frac{1}{4}\left\|\bm{u}_{T-1}-\bm{u}_{T}\right\|_{2}^{2}}_{ \widetilde{P}(1)},\]

where for brevity we have dropped the argument \(\vec{\bm{u}}\) on \(\bar{P}(\vec{\bm{u}},\tau)\).

Equation (10) is best shown via example; the general case is mostly a tedious exercise which we provide at the end. Assume \(T=4\), then the Haar matrix of order \(n=\log_{2}(T)=2\) is

\[\mathbf{H}_{2}=\begin{pmatrix}1&1&1&0\\ 1&1&-1&0\\ 1&-1&0&1\\ 1&-1&0&-1\end{pmatrix}=\underbrace{\begin{pmatrix}\frac{1}{2}&\frac{1}{2}& \frac{1}{\sqrt{2}}&0\\ \frac{1}{2}&\frac{1}{2}&\frac{1}{\sqrt{2}}&0\\ \frac{1}{2}&\frac{1}{2}&-\frac{1}{2}&0&\frac{1}{\sqrt{2}}\\ \frac{1}{2}&-\frac{1}{2}&0&\frac{1}{\sqrt{2}}\\ \underbrace{\frac{1}{2}&-\frac{1}{2}&0&\frac{1}{\sqrt{2}}}_{=:\widetilde{ \mathbf{H}}_{2}}\end{pmatrix}}_{=:\widetilde{\mathbf{H}}_{2}}\underbrace{\begin{pmatrix} 2&0&0\\ 0&2&0&0\\ 0&0&\sqrt{2}&0\\ 0&0&0&\sqrt{2}\end{pmatrix}}_{=:\widetilde{\mathbf{D}}_{2}}.\]

It is well-known that for any \(T\) the columns of \(\mathbf{H}_{\log_{2}(T)}\) form an orthogonal basis of \(\mathbb{R}^{T}\)[41, Chapter 6.1.1], which implies that \(\widetilde{\mathbf{H}}_{2}\) is orthonormal. So, \(\mathbf{H}_{2}^{-1}=\widetilde{\mathbf{H}}_{2}^{\top}\) and

\[\mathbf{H}_{2}^{-1} =(\widetilde{\mathbf{H}}_{2}\mathbf{D}_{2})^{-1}=\mathbf{D}_{2}^ {-1}\widetilde{\mathbf{H}}_{2}^{-1}=\mathbf{D}_{2}^{-1}\widetilde{\mathbf{H}}_ {2}^{\top}\] \[=\begin{pmatrix}\frac{1}{2}&0&0&0\\ 0&\frac{1}{2}&0&0\\ 0&0&\frac{1}{\sqrt{2}}&0\\ 0&0&0&\frac{1}{\sqrt{2}}\end{pmatrix}\begin{pmatrix}\frac{1}{2}&\frac{1}{2}& \frac{1}{2}&\frac{1}{2}\\ \frac{1}{2}&\frac{1}{2}&-\frac{1}{2}&-\frac{1}{2}\\ \frac{1}{2}&\frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}}&0&0\\ 0&0&0&\frac{1}{\sqrt{2}}\end{pmatrix}=\begin{pmatrix}\frac{1}{4}&\frac{1}{4}& \frac{1}{4}&\frac{1}{4}\\ \frac{1}{4}&\frac{1}{4}&-\frac{1}{4}\\ \frac{1}{2}&-\frac{1}{2}&0&0\\ 0&0&\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\end{pmatrix},\]

which leads to Equation (10) after applying the Kronecker product:

\[H^{-1}\widetilde{\bm{u}}=\begin{pmatrix}\frac{I_{4}}{\tilde{\bm{A}}}&\frac{I _{4}}{\tilde{\bm{A}}}&\frac{I_{4}}{\tilde{\bm{A}}}&\frac{I_{4}}{\tilde{\bm{A} }}\\ \frac{\tilde{\bm{A}}}{\tilde{\bm{A}}}&\frac{I_{4}}{\tilde{\bm{A}}}&-\frac{I_{4} }{\tilde{\bm{A}}}&-\frac{I_{4}}{\tilde{\bm{A}}}\\ \frac{\tilde{\bm{A}}}{\tilde{\bm{A}}}&-\frac{I_{4}}{2}&\bm{0}&\bm{0}\\ \bm{0}&\bm{0}&\frac{I_{4}}{2}&-\frac{I_{4}}{2}\end{pmatrix}\begin{pmatrix} \bm{u}_{1}\\ \vdots\\ \bm{u}_{T}\end{pmatrix}=\begin{pmatrix}\frac{\bm{u}_{1}+\bm{u}_{2}+\bm{u}_{3} +\bm{u}_{4}}{\bm{u}_{1}+\bm{u}_{4}}\\ \frac{\bm{u}_{1}+\bm{u}_{2}-\bm{u}_{3}-\bm{u}_{4}}{\bm{u}_{1}\pm\bm{u}_{2}} \\ \frac{\bm{u}_{3}-\bm{u}_{4}}{\bm{u}_{3}-\bm{u}_{4}}\end{pmatrix}=\frac{1}{2} \begin{pmatrix}2\bar{\bm{u}}\\ \bar{\bm{u}}_{1}^{(T/2)}-\bar{\bm{u}}_{2}^{(T/2)}\\ \bm{u}_{1}-\bm{u}_{2}\\ \bm{u}_{3}-\bm{u}_{4}\end{pmatrix}.\]

More generally, start with \(d=1\) begin again by factoring

\[\mathbf{H}_{n}^{-1}=\bm{D}_{n}^{-1}\widetilde{\mathbf{H}}_{n}^{\top}=\bm{D}_{n }^{-2}\mathbf{H}_{n}^{\top},\]

where now \(\widetilde{\mathbf{H}}_{n}\) is the normalized Haar basis matrix of order \(n=\log_{2}(T)\) and

\[\bm{D}_{n}=\text{Diag}\left(\sqrt{T},\underbrace{\sqrt{T}}_{2^{0}}, \underbrace{\sqrt{T/2},\sqrt{T/2}}_{2^{1}},\underbrace{\sqrt{T/4},\ldots,\sqrt {T/4}}_{2^{2}},\ldots,\underbrace{\sqrt{2},\ldots,\sqrt{2}}_{2^{\sum_{-1}}} \right).\]

The result is then attained by unrolling the recursion for \(\mathbf{H}_{n}^{\top}\widetilde{\bm{u}}\) given by Lemma 5 and factoring in the normalization factors \(\bm{D}_{n}^{-2}\). The result for \(d>1\) is then immediately implied by observing that the block matrix \(\mathbf{H}_{n}^{-1}\otimes\bm{I}_{d}\) will act upon the vector components of \(\widetilde{\bm{u}}\in\mathbb{R}^{dT}\) in an identical way to how \(\mathbf{H}_{n}^{-1}\) acts upon a vector of scalars. 

### Proof of Proposition 4

**Proposition 4**.: _The algorithm characterized by applying Theorem 2 with \(\bm{S}=[\bm{H}_{n}\bm{H}_{n}^{\top}]^{-1}\) can be implemented with \(\mathcal{O}\left(d\log T\right)\) per-round computation._

Proof.: Note that the losses passed to the 1-dimensional parameter-free algorithm are \(\langle\widetilde{\bm{v}}_{t},\widetilde{\bm{g}}_{t}\rangle=\langle\widetilde{ \bm{v}}_{t},\bm{e}_{t}\otimes\bm{g}_{t}\rangle\), and since \(\bm{e}_{t}\otimes\bm{g}_{t}\) has only \(d\) active indices we can compute the 1-dimensional learner's losses in \(\mathcal{O}(d)\). As such, the 1-dimensional learner can be implemented in \(\mathcal{O}(d)\) per-round computation.

For the direction learner, we are to show that each of the relevant variables can be maintained using only \(\mathcal{O}(d\log T)\) per-round computation.

Using Proposition 3, we immediately have \(V_{t+1}=V_{t}+\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{M}^{-1}}^{2}=V_{t}+ \left(\log T+1\right)\left\|\bm{g}_{t}\right\|^{2}\), so \(V_{t+1}\) can be maintained using only \(\mathcal{O}(d)\) per-round computation (_i.e._, to compute \(\left\|\bm{g}_{t}\right\|^{2}\)).

For the scaling factor \(\left\|\widetilde{\bm{\theta}}_{t+1}\right\|_{\bm{M}^{-1}}\), observe that

\[\left\|\widetilde{\bm{\theta}}_{t+1}\right\|_{\bm{M}^{-1}}^{2}=\left\|\widetilde{ \bm{g}}_{t}\right\|_{\bm{M}^{-1}}^{2}+\left\|\widetilde{\bm{\theta}}_{t} \right\|_{\bm{M}^{-1}}^{2}+2\left\langle\widetilde{\bm{\theta}}_{t},\bm{M}^{-1} \widetilde{\bm{g}}_{t}\right\rangle.\]Hence, we again have \(\mathcal{O}(d)\) per-round computation to compute \(\left\|\widetilde{\bm{g}}_{t}\right\|_{\bm{M}^{-1}}^{2}\), and letting \(\bm{h}_{t}=\bm{H}_{n}^{\top}\bm{e}_{t}\) we can decompose the last term as

\[\left\langle\widetilde{\bm{\theta}}_{t},\left(\bm{H}_{n}\bm{H}_{n}^ {\top}\otimes\bm{I}_{d}\right)(\mathbf{e}_{t}\otimes\bm{g}_{t})\right\rangle =\left\langle\widetilde{\bm{\theta}}_{t},\left(\bm{H}_{n}\bm{H}_ {n}^{\top}\mathbf{e}_{t}\otimes\bm{g}_{t}\right)\right\rangle\] \[=\left\langle\sum_{i=1}^{t-1}\bm{e}_{i}\otimes\bm{g}_{i},\bm{H}_ {n}\bm{h}_{t}\otimes\bm{g}_{t}\right\rangle\] \[=\sum_{i=1}^{t-1}(\bm{e}_{i}^{\top}\otimes\bm{g}_{i}^{\top}) \left(\bm{H}_{n}\bm{h}_{t}\otimes\bm{g}_{t}\right)\] \[=\sum_{i=1}^{t-1}\bm{e}_{i}^{\top}\bm{H}_{n}\bm{h}_{t}\otimes \left\langle\bm{g}_{i},\bm{g}_{t}\right\rangle\] \[=\sum_{i=1}^{t-1}\left\langle\bm{h}_{i},\bm{h}_{t}\right\rangle \left\langle\bm{g}_{i},\bm{g}_{t}\right\rangle\] \[=\left\langle\sum_{i=1}^{t-1}\bm{h}_{i}\left\langle\bm{g}_{i},\bm {g}_{t}\right\rangle,\bm{h}_{t}\right\rangle\] \[=\left\langle\sum_{i=1}^{t-1}\bm{h}_{i}\bm{g}_{i}^{\top}\bm{g}_{ t},\bm{h}_{t}\right\rangle\] \[=\left\langle\bm{g}_{t},\underbrace{\left[\sum_{i=1}^{t-1}\bm{g}_ {i}\bm{h}_{i}^{\top}\right]}_{=:\bm{\Lambda}_{t}}\right]\bm{h}_{t}\right\rangle.\]

From Proposition 5, for any \(t\) the vector \(\bm{h}_{t}=\bm{H}_{n}^{\top}\mathbf{e}_{t}\) has only \(\log T+1\) active non-zero elements by construction of the Haar basis, so given \(\bm{\Lambda}_{t}\), the product \(\bm{\Lambda}_{t}\bm{h}_{t}\) takes a linear combination of \(\log T+1\) vectors in \(\mathbb{R}^{d}\), for \(\mathcal{O}(d\log T)\) operations. Note that the variable \(\bm{\Lambda}_{t}\) can also be maintained with \(\mathcal{O}(d\log T)\) operations since each term is \(\bm{g}_{t}\bm{h}_{t}^{\top}\), which involves updating \(\mathcal{O}(\log T)\) columns of \(\bm{\Lambda}_{t-1}\in\mathbb{R}^{d\times T}\). Hence overall we can maintain \(\left\|\widetilde{\bm{\theta}}_{t+1}\right\|_{\bm{M}^{-1}}\) using \(\mathcal{O}(d\log T)\) per-round computation.

Lastly, consider the variable \(\widetilde{\bm{\theta}}_{t+1}\). Observe that we can maintain a variable \(\widehat{\bm{\theta}}_{t+1}=-\left(\bm{H}_{n}^{\top}\otimes\bm{I}_{d}\right) \sum_{s=1}^{t}\widetilde{\bm{g}}_{s}\) using \(\mathcal{O}(d\log T)\) computation:

\[\widehat{\bm{\theta}}_{t+1}=-\left(\bm{H}_{n}^{\top}\otimes\bm{I}_{d}\right) \sum_{s=1}^{t}\widetilde{\bm{g}}_{s}=\widehat{\bm{\theta}}_{t}-\left(\bm{H}_{n }^{\top}\bm{e}_{t}\otimes\bm{g}_{t}\right)=\widehat{\bm{\theta}}_{t}-\left( \bm{h}_{t}\otimes\bm{g}_{t}\right),\]

since \(\bm{h}_{t}\otimes\bm{g}_{t}\) is a block vector containing \(\log\left(T\right)+1\) non-zeros blocks of length \(d\). Hence,

\[\widetilde{\bm{\theta}}_{t+1} =(\bm{H}_{n}\bm{H}_{n}^{\top}\otimes\bm{I}_{n})\sum_{s=1}^{t}\bm{ g}_{s}=(\bm{H}_{n}\otimes\bm{I}_{n})(\bm{H}_{n}^{\top}\otimes\bm{I}_{n})\sum_{s=1}^{t} \widetilde{\bm{g}}_{s}\] \[=(\bm{H}_{n}\otimes\bm{I}_{n})\widehat{\bm{\theta}}_{t+1},\]

and again via the construction of the Haar basis, each row of \(\bm{H}_{n}\) (_i.e._, each column of \(\bm{H}_{n}^{\top}\)) has only \(\log T+1\) non-zero entries, we can compute each \(d\times 1\) block of \(\widetilde{\bm{\theta}}_{t+1}\) using \(\mathcal{O}(d\log T)\) computation. Finally, observe that in order to implement the direction learner, we need only compute the \(t^{\text{th}}\)\(d\times 1\) block of \(\widetilde{\bm{\theta}}_{t}\). Indeed, since for each \(t\), the vector \(\widetilde{\bm{g}}_{t}=\mathbf{e}_{t}\otimes\bm{g}_{t}\) has only \(d\) non-zero indices, it suffices to retrieve the corresponding indices of \(\widetilde{\bm{v}}_{t}\) to implement direction learner. 

We note that the memory overhead of maintaining each of these variables can also likely be reduced by more careful bookkeeping, and acknowledging the fact that the algorithm only really needs to retrieve the \(t^{\text{th}}\) block of \(\widetilde{\bm{w}}_{t}\), since the losses are \(\widetilde{\bm{g}}_{t}=\mathbf{e}_{t}\otimes\bm{g}_{t}\). We omit these considerations here for brevity.

Proofs for Section 5 (Recovering Variance-Variability Coupling Guarantees)

### Proof of Theorem 3

**Theorem 3**.: _Let \(\mathrm{Wealth}_{T}:=-\sum_{t=1}^{T}\left\langle\widetilde{\bm{g}}_{t},\widetilde{ \bm{w}}_{t}\right\rangle\) denote the "wealth" of an algorithm \(\mathcal{A}\) and let \((f,f^{*})\) be a Fenchel conjugate pair. Then \(\mathcal{A}\) guarantees \(\mathrm{Wealth}_{T}\geq f_{T}^{*}\big{(}-\sum_{t=1}^{T}\widetilde{\bm{g}}_{t} \big{)}\) for any sequence \(\widetilde{\bm{g}}_{1},\ldots,\widetilde{\bm{g}}_{T}\) if and only if \(R_{T}(\widetilde{\bm{u}})\leq f_{T}(\widetilde{\bm{u}})\) for any sequence \(\widetilde{\bm{u}}=(\bm{u}_{1},\ldots,\bm{u}_{T})\) in \(\mathcal{W}\), where \(\widetilde{\bm{u}}=(\bm{u}_{1}^{\top},\ldots,\bm{u}_{T}^{\top})^{\top}\) is the concatenation of the sequence \(\widetilde{\bm{u}}\) into a vector._

Proof.: Thanks to Proposition 1, the proof is essentially the same as the usual one. We provide the argument here for completeness.

From Proposition 1, \(R_{T}(\widetilde{\bm{u}})=R_{T}^{\mathrm{Seq}}(\widetilde{\bm{u}})=\sum_{t=1}^ {T}\left\langle\widetilde{\bm{g}}_{t},\widetilde{\bm{w}}_{t}-\widetilde{\bm{ u}}\right\rangle\) for \(\widetilde{\bm{g}}_{t}=\mathbf{e}_{t}\otimes\bm{g}_{t}\) and \(\widetilde{\bm{u}}=\sum_{t=1}^{T}\mathbf{e}_{t}\otimes\bm{u}_{t}\). Hence, recalling the definition of the Fenchel conjugate, we have

\[R_{T}(\widetilde{\bm{u}}) =\sum_{t=1}^{T}\left\langle\bm{g}_{t},\bm{w}_{t}-\bm{u}_{t} \right\rangle=\sum_{t=1}^{T}\left\langle\widetilde{\bm{g}}_{t},\widetilde{\bm {w}}_{t}-\widetilde{\bm{u}}\right\rangle=-\mathrm{Wealth}_{T}-\sum_{t=1}^{T} \left\langle\widetilde{\bm{g}}_{t},\widetilde{\bm{u}}\right\rangle\] \[\leq\Big{\langle}-\sum_{t=1}^{T}\widetilde{\bm{g}}_{t},\widetilde {\bm{u}}\Big{\rangle}-f_{T}^{*}\Big{(}-\sum_{t=1}^{T}\widetilde{\bm{g}}_{t} \Big{)}\leq\sup_{\bm{\theta}}\left\langle\bm{\theta},\widetilde{\bm{u}}\right\rangle -f_{T}^{*}(\bm{\theta})=f_{T}(\widetilde{\bm{u}})\.\]

Similarly, for the other direction, suppose we have \(R_{T}(\widetilde{\bm{u}})=R_{T}^{\mathrm{Seq}}(\widetilde{\bm{u}})\leq f_{T}( \widetilde{\bm{u}})\) for any \(\widetilde{\bm{u}}\). Then re-arranging, we have \(\mathrm{Wealth}_{T}\geq\Big{\langle}-\sum_{t=1}^{T}\widetilde{\bm{g}}_{t}, \widetilde{\bm{u}}\Big{\rangle}-f_{T}(\widetilde{\bm{u}})\), and since this holds for any \(\widetilde{\bm{u}}\), we can choose the one that tightens the bound to get \(\mathrm{Wealth}_{T}\geq\sup_{\widetilde{\bm{u}}}\Big{\langle}-\sum_{t=1}^{T} \widetilde{\bm{g}}_{t},\widetilde{\bm{u}}\Big{\rangle}-f_{T}(\widetilde{\bm{ u}})=f_{T}^{*}(-\sum_{t=1}^{T}\widetilde{\bm{g}}_{t})\). 

## Appendix D Supporting Lemmas

**Lemma 8**.: _Let \(\bm{\Sigma}\in\mathbb{R}^{T\times T}\) be the finite-difference operator, having entries_

\[\Sigma_{ij}=\begin{cases}1&\text{if }i=j\\ -1&\text{if }j=i+1\\ 0&\text{otherwise}\end{cases}\.\]

_Then,_

1. _The inverse of_ \(\bm{\Sigma}\) _the upper-triangular matrix of_ \(1\)_'s:_ \[\Sigma_{ij}^{-1}=\begin{cases}1&\text{if }j\geq i\\ 0&\text{otherwise}\end{cases},\quad\forall i,j\.\]
2. _The eigenvalues of_ \(\bm{\Sigma}\) _and_ \(\bm{\Sigma}^{-1}\) _are_ \(\lambda_{i}=1\) _for all_ \(i\in[T]\)_._
3. \(x\mapsto x^{\top}\bm{\Sigma}x\) _is positive definite._

_Moreover, the analogous properties hold for the block matrix \(\bm{\Sigma}\otimes\bm{I}_{d}\in\mathbb{R}^{dT\times dT}\)._

Proof.: The inverse of \(\bm{\Sigma}\) is the upper-triangular matrix \(\bm{\Delta}\) characterized by entries

\[\Delta_{ij}=\begin{cases}1&\text{if }j\geq i\\ 0&\text{otherwise}\end{cases}.\]

To see why, observe that we have \(\Sigma_{T,T}\Delta_{T,T}=1\) and for \(i<T\) we have

\[[\bm{\Sigma}\bm{\Delta}]_{ij}=\sum_{i,j}\Sigma_{ik}\Delta_{kj}=\Delta_{ij}- \Delta_{i+1,j}=\begin{cases}1&\text{if }i=j\\ 0&\text{otherwise}\end{cases},\]and likewise for \([\bm{\Delta}\bm{\Sigma}]_{ij}\). Hence \(\bm{\Sigma}\bm{\Delta}=\bm{\Delta}\bm{\Sigma}=I\) and \(\bm{M}^{-1}=\bm{\Delta}\).

Next, since \(\bm{\Sigma}\) and \(\bm{\Sigma}^{-1}\) are upper-triangular, their eigenvalues are equal to their diagonal entries, and hence both have eigenvalues \(\lambda_{i}=1\) for all \(i\).

To see that the asymmetric matrix \(\bm{\Sigma}\) is positive definite, it suffices to show that the symmetric part of \(\bm{\Sigma}\), _i.e._, the matrix \(\bm{\Sigma}_{S}=(\bm{\Sigma}+\bm{\Sigma}^{\top})/2\), is positive definite [23]. Luckily, \(\bm{\Sigma}_{S}\) is also a well-known variation of the discrete difference operator and is known to be positive definite [see, _e.g._, Theorem 7.4.7 in 39].

For the block matrix \(\bm{B}=\bm{\Sigma}\otimes\bm{I}_{d}\), the inverse is given immediately by the inverse property of the Kronecker product: \(\bm{B}^{-1}=(\bm{\Sigma}\otimes\bm{I}_{d})^{-1}=\bm{\Sigma}^{-1}\otimes\bm{I }_{d}\). We also have that \(\bm{B}=\bm{\Sigma}\otimes\bm{I}_{d}\) and \(\bm{B}^{-1}\) have eigenvalues \(\lambda_{i}=1\) for all \(i\in[dT]\), since both are again upper-triangular with \(1\)'s on their main diagonal. Finally, we have positive definiteness of \(\bm{B}\) using the fact that the symmetric part of \(\bm{B}=\bm{\Sigma}\otimes\bm{I}_{d}\) is \(\frac{1}{2}(\bm{B}+\bm{B}^{\top})=\frac{1}{2}(\bm{\Sigma}\otimes\bm{I}_{d}+ \bm{\Sigma}^{\top}\otimes\bm{I}_{d})=\frac{1}{2}(\bm{\Sigma}+\bm{\Sigma}^{ \top})\otimes\bm{I}_{d}\) by the distributive property, hence \(\bm{B}\) is the Kronecker product of two symmetric positive definite matrices, so \(\bm{B}\) is positive definite [38, Chapter 2]. 

We borrow the following eigenvalue bound from [43].

**Theorem 5**.: _(Wolkowicz and Styan [43, Theorem 2.1]) Let \(\bm{A}\) be a symmetric \(n\times n\) matrix with eigenvalues \(\lambda_{1}(\bm{A})\leq\ldots\leq\lambda_{n}(\bm{A})\). Then_

\[\lambda_{\max}(\bm{A})\leq\frac{\operatorname{Tr}(\bm{A})}{n}+\sqrt{(n-1) \left[\frac{\operatorname{Tr}(\bm{A}^{\top}\bm{A})}{n}-\left(\frac{ \operatorname{Tr}(\bm{A})}{n}\right)^{2}\right]}\,.\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract and introduction clear state the main claims of our paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We clearly state the problem setting and the assumptions therein. We clearly state which results are optimal up to logarithmic factors. We discuss the computational complexity of our newly proposed algorithm at the end of Section 4.1. Attaining the tightest matching logarithmic dependencies in our lower bound is still an open problem, as mentioned in the conclusion. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All the theorems provide the full set of assumptions. All of our main results are proven explicitly either in the main text or in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: This is a theoretical paper that studies theoretical guarantees for online learning algorithms. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: This is a theoretical paper that studies theoretical guarantees for online learning algorithms. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This is a theoretical paper that studies theoretical guarantees for online learning algorithms. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: This is a theoretical paper that studies theoretical guarantees for online learning algorithms. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: This is a theoretical paper that studies theoretical guarantees for online learning algorithms. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Given the theoretical nature of this work, there are no ethical concerns to be addressed. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a theoretical paper that studies theoretical guarantees for online learning algorithms. As stated in the guidelines, this is foundational research and it is not tied to particular applications, let alone deployments.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No models nor data is associated to this paper. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: No assets were used in this paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.