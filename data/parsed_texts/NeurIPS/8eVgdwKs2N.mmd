# G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks

Zhaoyu Li\({}^{1,2}\), Jinpei Guo\({}^{4}\), Xujie Si\({}^{1,2,3}\)

\({}^{1}\)University of Toronto, \({}^{2}\)Vector Institute, \({}^{3}\)Mila, \({}^{4}\)Shanghai Jiao Tong Univeristy

{zhaoyu, six}@cs.toronto.edu, mike0728@sjtu.edu.cn

###### Abstract

Graph neural networks (GNNs) have recently emerged as a promising approach for solving the Boolean Satisfiability Problem (SAT), offering potential alternatives to traditional backtracking or local search SAT solvers. However, despite the growing volume of literature in this field, there remains a notable absence of a unified dataset and a fair benchmark to evaluate and compare existing approaches. To address this crucial gap, we present G4SATBench, the first benchmark study that establishes a comprehensive evaluation framework for GNN-based SAT solvers. In G4SATBench, we meticulously curate a large and diverse set of SAT datasets comprising 7 problems with 3 difficulty levels and benchmark a broad range of GNN models across various prediction tasks, training objectives, and inference algorithms. To explore the learning abilities and comprehend the strengths and limitations of GNN-based SAT solvers, we also compare their solving processes with the heuristics in search-based SAT solvers. Our empirical results provide valuable insights into the performance of GNN-based SAT solvers and further suggest that existing GNN models can effectively learn a solving strategy akin to greedy local search but struggle to learn backtracking search in the latent space.

## 1 Introduction

The Boolean Satisfiability Problem (SAT) is a crucial problem at the nexus of computer science, logic, and operations research, which has garnered significant attention over the past five decades. To solve SAT instances efficiently, modern SAT solvers have been developed with backtracking (especially with conflict-driven clause learning, a.k.a. CDCL) or local search (LS) heuristics that effectively exploit the instance's structure and traverse its vast search space [4]. However, designing such heuristics remains a highly non-trivial and time-consuming task, with a lack of significant improvement in recent years. Conversely, the recent rapid advances in graph neural networks (GNNs) [23; 27; 41] have shown impressive performances in analyzing structured data, offering a promising opportunity to enhance or even replace modern SAT solvers. As such, there have been massive efforts to leverage GNNs to solve SAT over the last few years [16; 19].

Despite the recent progress, the question of _how (well) GNNs can solve SAT_ remains unanswered. One of the main reasons for this is the variety of learning objectives and usage scenarios employed in existing work, making it difficult to evaluate different methods in a fair and comprehensive manner. For example, NeuroSAT [34] predicts satisfiability, QuerySAT [30] constructs a satisfying assignment, NeuroCore [33] classifies unsat-core variables, and NSNet [28] predicts marginal distributions of all satisfying solutions to solve the SAT problem. Moreover, most previous research has experimented on different datasets that vary in a range of settings (e.g., data distribution, instance size, and dataset size),which leads to a lack of unified and standardized datasets for training and evaluation. Additionally, some work [2; 35; 42] has noted the difficulty of re-implementing prior approaches as baselines, rendering it arduous to draw consistent conclusions about the performance of peer approaches. All of these issues impede the development of GNN-based solvers for SAT solving.

To systematically quantify the progress in this field and facilitate rapid, reproducible, and generalizable research, we propose **G4SATBench**, the first comprehensive benchmark study for SAT solving with GNNs. G4SATBench is characterized as follows:

* First, we construct a large and diverse collection of SAT datasets that includes instances from distinct sources and difficulty levels. Specifically, our benchmark consists of 7 different datasets from 3 benchmark families, including random instances, pseudo-industrial instances, and combinatorial problems. It not only covers a wide range of prior datasets but also introduces 3 levels of difficulty for each dataset to enable fine-grained analyses.
* Second, we re-implement various GNN-based SAT solvers with unified interfaces and configuration settings, establishing a general evaluation protocol for fair and comprehensive comparisons. Our framework allows for evaluating different GNN models in SAT solving with various prediction tasks, training objectives, and inference algorithms, encompassing the diverse learning frameworks employed in the existing literature.
* Third, we present baseline results and conduct thorough analyses of GNN-based SAT solvers, providing a detailed reference of prior work and laying a solid foundation for future research. Our evaluations assess the performances of different choices of GNN models (e.g., graph constructions, message-passing schemes) with particular attention to some critical parameters (e.g., message-passing iterations), as well as their generalization ability across different distributions.
* Lastly, we conduct a series of in-depth experiments to explore the learning abilities of GNN-based SAT solvers. Specifically, we compare the training and solving processes of GNNs with the heuristics employed in both CDCL and LS-based SAT solvers. Our experimental results reveal that _GNNs tend to develop a solving heuristic similar to greedy local search to find a satisfying assignment but fail to effectively learn the backtracking heuristic in the latent space._

We believe that G4SATBench will enable the research community to make significant strides in understanding the capabilities and limitations of GNNs for solving SAT and facilitate further development in this area. Our codebase is available at https://github.com/zhaoyu-li/G4SATBench.

## 2 Related Work

SAT solving with GNNs.Existing GNN-based SAT solvers can be broadly categorized into two branches [16]: _standalone neural solvers_ and _neural-guided solvers_. Standalone neural solvers utilize GNNs to solve SAT instances directly. For example, a stream of research [6; 34; 21; 7; 35] focuses on predicting the satisfiability of a given formula, while several alternative approaches [1; 2; 30; 26; 42] aim to construct a satisfying assignment. Neural-guided solvers, on the other hand, integrate GNNs with modern SAT solvers, trying to improve their search heuristics with the prediction of GNNs. These methods typically train GNN models using supervised learning on some tasks such as unsat-core variable prediction [33; 38], satisfying assignment prediction [44], glue variable prediction [17], and assignment marginal prediction [28], or through reinforcement learning [43; 24] by modeling the entire search procedure as a Markov decision process. Despite the rich literature on SAT solving with GNNs, there is no benchmark study to evaluate and compare the performance of these GNN models. We hope the proposed G4SATBench would address this gap.

SAT datasets.Several established SAT benchmarks, including the prestigious SATLIB [20] and the SAT Competitions over the years, have provided a variety of practical instances to assess the performance of modern SAT solvers. Regrettably, these datasets are not particularly amenable for GNNs to learn from, given their relatively modest scale (less than 100 instances for a specific domain) or overly extensive instances (exceeding 10 million variables and clauses). To address this issue,researchers have turned to synthetic SAT instance generators [34; 25; 14; 37], which allow for the creation of a flexible number of instances with customizable settings. However, most of the existing datasets generated from these sources are limited to a few domains (less than 3 generators), small in size (less than 10k instances), or easy in difficulty (less than 40 variables within an instance), and there is no standardized dataset for evaluation. In G4SATBench, we include a variety of synthetic generators with carefully selected configurations, aiming to construct a broad collection of SAT datasets that are highly conducive for training and evaluating GNNs.

## 3 Preliminaries

The SAT problem.In propositional logic, a Boolean formula is constructed from Boolean variables and logical operators such as conjunctions (\(\wedge\)), disjunctions (\(\vee\)), and negations (\(\neg\)). It is typical to represent Boolean formulas in conjunctive normal form (CNF), expressed as a conjunction of clauses, where each clause is a disjunction of literals, which can be either a variable or its negation. Given a CNF formula, the SAT problem is to determine if there exists an assignment of boolean values to its variables such that the formula evaluates to true. If this is the case, the formula is called satisfiable; otherwise, it is unsatisfiable. For a satisfiable instance, one is expected to construct a satisfying assignment to prove its satisfiability. On the other hand, for an unsatisfiable formula, one can find a minimal subset of clauses whose conjunction is still unsatisfiable. Such a set of clauses is termed the unsat core, and variables in the unsat core are referred to as unsat-core variables.

Graph representations of CNF formulas.Traditionally, a CNF formula can be represented using 4 types of graphs [4]: Literal-Clause Graph (LCG), Variable-Clause Graph (VCG), Literal-Incidence Graph (LIG), and Variable-Incidence Graph (VIG). The LCG is a bipartite graph with literal and clause nodes connected by edges indicating the presence of a literal in a clause. The VCG is formed by merging the positive and negative literals of the same variables in LCG. The LIG, on the other hand, only consists of literal nodes, with edges indicating co-occurrence in a clause. Lastly, the VIG is derived from LIG using the same merging operation as VCG.

## 4 G4SATBench: A Comprehensive Benchmark on GNNs for SAT Solving

The goal of G4SATBench is to establish a general framework that enables comprehensive comparisons and evaluations of various GNN-based SAT solvers. In this section, we will delve into the details of G4SATBench, including its datasets, GNN models, prediction tasks, as well as training and testing methodologies. The overview of the G4SATBench framework is shown in Figure 1.

### Datasets

G4SATBench is built on a diverse set of synthetic CNF generators. It currently consists of 7 datasets sourced from 3 distinct domain areas: random problems, pseudo-industrial problems, and combinatorial problems. Specifically, we utilize the SR generator in NeuroSAT [34] and the 3-SAT generator in CNFGen [25] to produce random CNF formulas. For pseudo-industrial problems, we employ the Community Attachment (CA) model [14] and the Popularity-Similarity (PS) model [15],

Figure 1: Framework overview of G4SATBench.

which generate synthetic instances that exhibit similar statistical features, such as the community and the locality, to those observed in real-world industrial SAT instances. For combinatorics, we resort to 3 synthetic generators in CNFGen [25] to create SAT instances derived from the translation of \(k\)-Clique, \(k\)-Dominating Set, and \(k\)-Vertex Cover problems.

In addition to the diversity of datasets, G4SATBench offers distinct difficulty levels for all datasets to enable fine-grained analyses. These levels include easy, medium, and hard, with the latter representing more complex problems with increased instance sizes. For example, the easy SR dataset contains instances with 10 to 40 variables, the medium SR dataset contains formulas with 40 to 200 variables, and the hard SR dataset consists of formulas with variables ranging from 200 to 400. For each easy and medium dataset, we generate 80k pairs of satisfiable and unsatisfiable instances for training, 10k pairs for validation, and 10k pairs for testing. For each hard dataset, we produce 10k testing pairs. It is also worth noting that the parameters for our synthetic generators are meticulously selected to avoid generating trivial cases. For instance, we produce random 3-SAT formulas at the phase-transition region where the relationship between the number of clauses \((m)\) and variables \((n)\) is \(m=4.258n+58.26n^{-2/3}\)[10], and utilize the \(v\) vertex Erdos-Renyi graph with an edge probability of \(p=\binom{v}{k}^{-1/\binom{v}{2}}\) to generate \(k\)-Clique problems, making the expected number of \(k\)-Cliques in a graph equals 1 [5]. To provide a detailed characterization of our generated datasets, we compute several statistics of the SAT instances across difficulty levels in G4SATBench. For more information about the generators we used and the dataset statistics, please refer to Appendix A.

### GNN Baselines

Graph constructions.It is important to note that traditional graph representations of a CNF formula often lack the requisite details for optimally constructing GNNs. Specifically, the LIG and VIG exclude clause-specific information, while the LCG and VIG fail to differentiate between positive and negative literals of the same variable. To address these limitations, existing approaches typically build GNN models on the refined versions of the LCG and VCG encodings. In the LCG, a new type of edge is added between each literal and its negation, while the VCG is modified by using two types of edges to indicate the polarities of variables within a clause. These modified encodings are termed the LCG* and VCG* respectively, and an example of them is shown in Figure 2.

Message-passing schemes.G4SATBench enables performing various _hetergeneous_ message-passage algorithms between neighboring nodes on the LCG* or VCG* encodings of a CNF formula. For the sake of illustration, we will take GNN models on the LCG* as an example. We first define a \(d\)-dimensional embedding for every literal node and clause node, denoted by \(h_{l}\) and \(h_{c}\) respectively. Initially, all these embeddings are assigned to two learnable vectors \(h_{l}^{0}\) and \(h_{c}^{0}\), depending on their node types. At the \(k\)-th iteration of message passing, these hidden representations are updated as:

\[\begin{split} h_{c}^{(k)}&=\text{UPD}\left(\underset{ l\in\mathcal{N}(c)}{\text{AGG}}\left(\left\{\text{MLP}_{l}\left(h_{l}^{(k-1)}\right)\right\} \right),h_{c}^{(k-1)}\right),\\ h_{l}^{(k)}&=\text{UPD}\left(\underset{c\in \mathcal{N}(l)}{\text{AGG}}\left(\left\{\text{MLP}_{c}\left(h_{c}^{(k-1)} \right)\right\}\right),h_{\neg l}^{(k-1)},h_{l}^{(k-1)}\right),\end{split}\] (1)

where \(\mathcal{N}(\cdot)\) denotes the set of neighbor nodes, \(\text{MLP}_{l}\) and \(\text{MLP}_{c}\) are two different multi-layer perceptions (MLPs), \(\text{UPD}(\cdot)\) is the update function, and \(\text{AGG}(\cdot)\) is the aggraation function. Most GNN models on LCG* use Equation 1 with different choices of the update function and aggregation function. For instance, NeuroSAT employs LayerNormLSTM [3] as the update function and summation as the aggregation function. In G4SATBench, we provide a diverse range of GNN models, including NeuroSAT [34], Graph Convolutional Network (GCN) [23], Gated Graph Neural Network (GGNN) [27], and Graph Isomorphism Network (GIN) [41], on the both LCG* and VCG*. More details of these GNN models are included in Appendix B.

### Supported Tasks, Training and Testing Settings

Prediction tasks.In G4SATBench, we support three essential prediction tasks for SAT solving: satisfiability prediction, satisfying assignment prediction, and unsat-core variable prediction. These tasks are widely used in both standalone neural solvers and neural-guided solvers. Technically, we model satisfiability prediction as a binary graph classification task, where 1/0 denotes the satisfiability/unsatisfiability of the given SAT instance \(\phi\). Here, we take GNN models on the LCG* as an example. After \(T\) iterations of message passing, we obtain the graph embedding by applying mean pooling on all literal embeddings, and then predict the satisfiability using an MLP followed by the sigmoid function \(\sigma\):

\[y_{\phi}=\sigma\left(\text{MLP}\left(\text{MEN}\left(\{h_{l}^{(T)},l\in\phi \}\right)\right)\right).\] (2)

For satisfying assignment prediction and unsat-core variable prediction, we formulate them as binary node classification tasks, predicting the label for each variable in the given CNF formula \(\phi\). In the case of GNNs on the LCG*, we concatenate the embeddings of each pair of literals \(h_{l}\) and \(h_{\neg l}\) to construct the variable embedding, and then readout using an MLP and the sigmoid function \(\sigma\):

\[y_{v}=\sigma\left(\text{MLP}\left(\left[h_{l}^{(T)},h_{\neg l}^{(T)}\right] \right)\right).\] (3)

Training objectives.To train GNN models on the aforementioned tasks, one common approach is to minimize the binary cross-entropy loss between the predictions and the ground truth labels. In addition to supervised learning, G4SATBench supports two unsupervised training paradigms for satisfying assignment prediction [1, 30]. The first approach aims to differentiate and maximize the satisfiability value of a CNF formula [1]. It replaces the \(\neg\) operator with the function \(N(a)=1-a\) and uses smooth max and min functions to replace the \(\vee\) and \(\wedge\) operators. The smooth max and min functions are defined as follows:

\[S_{max}(x_{1},x_{2},\ldots,x_{d})=\frac{\sum_{i=1}^{d}x_{i}\cdot e^{x_{i}/ \tau}}{\sum_{i=1}^{d}e^{x_{i}/\tau}},\quad S_{min}(x_{1},x_{2},\ldots,x_{d})= \frac{\sum_{i=1}^{d}x_{i}\cdot e^{-x_{i}/\tau}}{\sum_{i=1}^{d}e^{-x_{i}/\tau}},\] (4)

where \(\tau\geq 0\) is the temperature parameter. Given a predicted soft assignment \(x=(x_{1},x_{2},\ldots,x_{n})\), we evaluate its satisfiability value \(S(x)\) using the smoothed version of logical operators and minimize the following loss function:

\[\mathcal{L}_{\phi}(x)=\frac{(1-S(x))^{\kappa}}{(1-S(x))^{\kappa}+S(x)^{\kappa }}.\quad(\kappa\geq 1\text{ is a predefined constant})\] (5)

The second unsupervised loss is defined as follows [30]:

\[V_{c}(x)=1-\prod_{i\in c^{+}}(1-x_{i})\prod_{i\in c^{-}}x_{i},\quad\mathcal{L }_{\phi}(x)=-\log\Bigl{(}\prod_{c\in\phi}V_{c}(x)\Bigr{)}=-\sum_{c\in\phi} \log\left(V_{c}(x)\right),\] (6)

where \(c^{+}\) and \(c^{-}\) are the sets of variables that occur in the clause \(c\) in positive and negative form respectively. Note that these two losses reach the minimum only when the prediction \(x\) is a satisfying assignment, thus minimizing such losses could help to construct a possible satisfying assignment.

Inference algorithms.In addition to using the standard readout process like training, G4SATBench offers two alternative inference algorithms for satisfying assignment prediction [34, 2]. The first method performs 2-clustering on the literal embeddings to obtain two centers \(\Delta_{1}\) and \(\Delta_{2}\) and then partitions the positive and negative literals of each variable into distinct groups based on the predicate \(||x_{i}-\Delta_{1}||^{2}+||\neg x_{i}-\Delta_{2}||^{2}<||x_{i}-\Delta_{2}||^{ 2}+||\neg x_{i}-\Delta_{1}||^{2}\)[34]. This allows the construction of two possible assignments by mapping one group of literals to true. The second approach is to employ the readout function at each iteration of message passing, resulting in multiple assignment predictions for a given instance [2].

Evaluation metrics.For satisfiability prediction and unsat-core variable prediction, we report the classification accuracy of each GNN model in G4SATBench. For satisfying assignment prediction, we report the solving accuracy of the predicted assignments. If multiple assignments are predicted for a SAT instance, the instance is considered solved if any of the predictions satisfy the formula.

Benchmarking Evaluation on G4SATBench

In this section, we present the benchmarking results of G4SATBench. To ensure a fair comparison, we conduct a grid search to tune the hyperparameters of each GNN baseline. The best checkpoint for each GNN model is selected based on its performance on the validation set. To mitigate the impact of randomness, we use 3 different random seeds to repeat the experiment in each setting and report the average performance. Each experiment is performed on a single RTX8000 GPU and 16 AMD EPYC 7502 CPU cores, and the total time cost is approximately 8,000 GPU hours. For detailed experimental setup and hyperparameters, please refer to Appendix C.1.

### Satisfiability Prediction

Evaluation on the same distribution.Table 1 shows the benchmarking results of each GNN baseline when trained and evaluated on datasets possessing identical distributions. All GNN models exhibit strong performance across most easy and medium datasets, except for the medium SR dataset. This difficulty can be attributed to the inherent characteristic of this dataset, which includes satisfiable and unsatisfiable pairs of medium-sized instances distinguished by just a single differing literal. Such a subtle difference presents a substantial challenge for GNN models in satisfiability classification. Among all GNN models, the different graph constructions do not seem to have a significant impact on the results, and NeuroSAT (on LCG*) and GGNN (on VCG*) achieve the best overall performance.

Evaluation across different distributions.To assess the generalization ability of GNN models, we evaluate the performance of NeuroSAT (on LCG*) and GGNN (on VCG*) across different datasets and difficulty levels. As shown in Figure 3 and Figure 4, NeuroSAT and GGNN struggle to generalize effectively to datasets distinct from their training data in most cases. However, when trained on the SR dataset, they exhibit better generalization performance across different datasets. Furthermore, while both GNN models demonstrate limited generalization to larger formulas beyond their training data, they perform relatively better on smaller instances. These observations suggest that the generalization performance of GNN models for satisfiability prediction is influenced by the distinct nature and complexity of its training data. Training on more challenging instances could potentially enhance their generalization ability.

Due to the limited space, Figure 4 exclusively displays the performance of NeuroSAT and GGNN on the SR and 3-SAT datasets. Comprehensive results on the other five datasets, as well as the experimental results on different massage passing iterations, are provided in Appendix C.2.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Gray} & \multirow{2}{*}{Method} & \multicolumn{6}{c}{Easy Datasets} & \multicolumn{6}{c}{Medium Datasets} \\ \cline{3-14}  & & SR & -3.5AT & CA & PS & -Cique & k-Donset & k-Donset & -Nevector & SR & -3.5AT & CA & PS & -Cique & k-Donset & k-Donset & k-Vevector \\ \hline \multirow{4}{*}{LCG*} & NewsSAT & 96.00 & **96.33** & **98.83** & 96.59 & 97.92 & **99.77** & **99.99** & **78.02** & **84.90** & **99.57** & **96.81** & **89.39** & 99.67 & 99.80 \\  & GCN & 94.33 & 94.97 & 98.93 & 97.93 & 98.24 & 99.39 & 99.98 & 69.39 & 82.67 & 99.35 & 91.66 & 85.72 & 99.16 & 99.74 \\  & GNN & 96.36 & 95.70 & 98.81 & 97.47 & **98.80** & 97.97 & 99.77 & 97.41 & 83.45 & 90.50 & 96.21 & 81.20 & **99.69** & **99.83** \\  & GNN & 95.78 & 95.37 & 98.4 & 96.98 & 97.60 & 99.71 & 99.97 & 97.05 & 58.20 & 99.49 & 95.80 & 83.87 & 99.61 & 99.62 \\ \hline \multirow{4}{*}{VCG*} & GCN & 93.19 & 94.92 & 97.82 & 95.79 & 98.72 & 99.54 & **99.99** & 66.35 & 83.75 & 99.49 & 95.48 & 82.99 & 99.42 & **99.89** \\  & GGNN & **96.75** & **96.25** & **98.77** & 96.44 & **98.88** & **99.68** & 99.98 & **77.12** & 85.11 & **99.57** & 96.48 & 83.63 & **99.62** & 98.92 \\ \cline{1-1}  & GNN & 96.04 & 95.71 & 98.47 & **96.95** & 97.33 & 99.59 & 99.98 & 73.56 & **82.66** & 99.49 & **96.55** & **89.41** & 99.38 & 99.30 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results on the datasets of the same distribution.

Figure 3: Results across different datasets. The x-axis denotes testing datasets and the y-axis denotes training datasets.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

to exactly replicate the same search operations due to the dynamic changes in the graph structure introduced by the clause learning technique.

### Comparison with the LS Heuristic

Evaluation with random initialization.LS-based SAT solvers typically begin by randomly initializing an assignment and then iteratively flip variables guided by specific heuristics until reaching a satisfying assignment. To compare the behaviors of GNNs with this solving procedure, we first conduct an evaluation of GNN models with randomized initial embeddings in both training and testing, emulating the initialization of LS SAT solvers.

The results presented in Table 6 demonstrate that using random initialization has a limited impact on the overall performances of GNN-based SAT solvers. This suggests that GNN models do not aim to learn a fixed latent representation for each formula in SAT solving. Instead, they have developed a solving strategy that effectively exploits the inherent graph structure of each SAT instance.

Evaluation on the predicted assignments.Under random initialization, we further analyze the solving strategies of GNNs by evaluating their predicted assignments decoded from the latent space. For the task of satisfiability prediction, we employ the 2-clustering decoding algorithm to extract the predicted assignments from the literal embeddings of NeuroSAT at each iteration of message passing. For satisfying assignment prediction, we evaluate both NeuroSAT and GGNN using multiple-prediction decoding. Our evaluation focuses on three key aspects: (a) the number of distinct predicted assignments, (b) the number of flipped variables between two consecutive iterations, and (c) the number of unsatisfiable clauses associated with the predicted assignments.

As shown in Figure 5, all three GNN models initially generate a wide array of assignment predictions by flipping a considerable number of variables, resulting in a notable reduction in the number of unsatisfiable clauses. However, as the iterations progress, the number of flipped variables diminishes substantially, and most GNN models eventually converge towards predicting a specific assignment or making minimal changes to their predictions when there are no or very few unsatisfiable clauses remaining. This trend is reminiscent of the greedy solving strategy adopted by the LS solver GSAT [32], where changes are made to minimize the number of unsatisfied clauses in the new assignment. However, unlike GSAT's approach of flipping one variable at a time and incorporating random selection to break ties, GNN models simultaneously modify multiple variables and potentially converge to a particular unsatisfied assignment and find it challenging to deviate from such a prediction. It is also noteworthy that despite being trained for satisfiability prediction, NeuroSAT* demonstrates similar behavior to the GNN models trained for assignment prediction. This observation indicates that GNNs also learn to search for a satisfying assignment implicitly in the latent space while performing satisfiability prediction.

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline \multirow{2}{*}{Task} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{Early Datasets} & \multicolumn{2}{c}{Medium Datasets} \\ \cline{3-6}  & & SR & 3-SAT & SR & 3-SAT \\ \hline \multirow{2}{*}{T1} & NeuroSAT* & 97.24 (+1.20) & 96.44 (+0.11) & 77.29 (+0.91) & 84.85 (+0.49) \\  & GNN & 96.76 (+0.00) & 96.38 (+0.13) & 75.05 (+0.15) & 55.80 (+0.06) \\ \hline \hline \multirow{2}{*}{T2} & NeuroSAT* & 79.89 (+0.70) & 80.79 (+0.20) & 37.27 (+0.02) & 40.75 (+0.86) \\  & GNN & 80.10 (+0.50) & 79.33 (+0.51) & 32.85 (+0.52) & 36.59 (+0.64) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results using random initialization. Values in parentheses denote the difference between the results with learned initialization.

Figure 5: Results on the predicted assignments with the increased message passing iteration \(T\). NeuroSAT* refers to the model trained for satisfiability prediction.

## 7 Discussions

Limitations and future work.While G4SATBench represents a significant step in evaluating GNNs for SAT solving, there are still some limitations and potential future directions to consider. Firstly, G4SATBench primarily focuses on evaluating standalone neural SAT solvers, excluding the exploration of neural-guided SAT solvers that integrate GNNs with search-based SAT solvers. It also should be emphasized that the instances included in G4SATBench are relatively small compared to most practical instances found in real-world applications, where GNN models alone are not sufficient for solving such large-scale instances. Future research could explore techniques to effectively leverage GNNs in combination with modern SAT solvers to scale up to real-world instances. Secondly, G4SATBench benchmarks general GNN models on the LCG* and VCG* graph representations for SAT solving, but does not consider sophisticated GNN models designed for specific graph constructions in certain domains, such as Circuit SAT problems. Investigating domain-specific GNN models tailored to the characteristics of specific problems could lead to improved performance in specialized instances. Lastly, all existing GNN-based SAT solvers in the literature are static GNNs, which have limited learning ability to capture the CDCL heuristic. Exploring dynamic GNN models that can effectively learn the CDCL heuristic is also a potential direction for future research.

Conclusion.In this work, we present G4SATBench, a groundbreaking benchmark study that comprehensively evaluates GNN models in SAT solving. G4SATBench offers curated synthetic SAT datasets sourced from various domains and difficulty levels and benchmarks a wide range of GNN-based SAT solvers under diverse settings. Our empirical analysis yields valuable insights into the performances of GNN-based SAT solvers and further provides a deeper understanding of their capabilities and limitations. We hope the proposed G4SATBench will serve as a solid foundation for GNN-based SAT solving and inspire future research in this exciting field.

## References

* Amizadeh et al. [2019] Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve Circuit-SAT: An unsupervised differentiable approach. In _International Conference on Learning Representations (ICLR)_, 2019.
* Amizadeh et al. [2019] Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. PDP: A general neural framework for learning constraint satisfaction solvers. _arXiv preprint arXiv:1903.01969_, 2019.
* Ba et al. [2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* Biere et al. [2009] Armin Biere, Marijn Heule, and Hans van Maaren. _Handbook of Satisfiability_, volume 185. IOS press, 2009.
* Bollobas and Erdos [1976] Bela Bollobas and Paul Erdos. Cliques in random graphs. In _Mathematical Proceedings of the Cambridge Philosophical Society_, 1976.
* Bunz and Lamm [2017] Benedikt Bunz and Matthew Lamm. Graph neural networks and boolean satisfiability. _arXiv preprint arXiv:1702.03592_, 2017.
* Cameron et al. [2020] Chris Cameron, Rex Chen, Jason Hartford, and Kevin Leyton-Brown. Predicting propositional satisfiability via end-to-end learning. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2020.
* Chen et al. [2020] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In _International Conference on Machine Learning (ICML)_, 2020.
* Chen and Yang [2019] Ziliang Chen and Zhanfu Yang. Graph neural reasoning may fail in certifying boolean unsatisfiability. _arXiv preprint arXiv:1909.11588_, 2019.

* [10] James M. Crawford and Larry D. Auton. Experimental results on the crossover point in random 3-SAT. _Artificial Intelligence_, 1996.
* [11] Haonan Duan, Pashootan Vaezipoor, Max B. Paulus, Yangjun Ruan, and Chris J. Maddison. Augment with care: Contrastive learning for combinatorial problems. In _International Conference on Machine Learning (ICML)_, 2022.
* [12] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. _arXiv preprint arXiv:1903.02428_, 2019.
* [13] ABKFM Fleury and Maximilian Heisinger. Cadical, kissat, paracooba, plingeling and treengeling entering the sat competition 2020. _SAT COMPETITION_, 2020.
* [14] Jesus Giraldez-Cru and Jordi Levy. A modularity-based random SAT instances generator. In _International Joint Conference on Artificial Intelligence (IJCAI)_, 2015.
* [15] Jesus Giraldez-Cru and Jordi Levy. Locality in random SAT instances. In _International Joint Conference on Artificial Intelligence (IJCAI)_, 2017.
* [16] Wenxuan Guo, Junchi Yan, Hui-Ling Zhen, Xijun Li, Mingxuan Yuan, and Yaohui Jin. Machine learning methods in solving the boolean satisfiability problem. _arXiv preprint arXiv:2203.04755_, 2022.
* [17] Jesse Michael Han. Enhancing SAT solvers with glue variable predictions. _arXiv preprint arXiv:2007.02559_, 2020.
* [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In _IEEE International Conference on Computer Vision (ICCV)_, 2015.
* [19] Sean B Holden et al. Machine learning for automated theorem proving: Learning to solve SAT and QSAT. _Foundations and Trends(r) in Machine Learning_, 14(6):807-989, 2021.
* [20] Holger H Hoos and Thomas Stutzle. SATLIB: An online resource for research on SAT. _Workshop on Satisfiability (SAT)_, 2000.
* [21] Sebastian Jaszczur, Michal Luszczyk, and Henryk Michalewski. Neural heuristics for SAT solving. _arXiv preprint arXiv:2005.13406_, 2020.
* [22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations (ICLR)_, 2015.
* [23] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations (ICLR)_, 2017.
* [24] Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. Can q-learning with graph networks learn a generalizable branching heuristic for a SAT solver? In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [25] Massimo Lauria, Jan Elffers, Jakob Nordstrom, and Marc Vinyals. Cnfgen: A generator of crafted benchmarks. In _Theory and Applications of Satisfiability Testing (SAT)_, 2017.
* [26] Min Li, Zhengyuan Shi, Qiuxia Lai, Sadaf Khan, and Qiang Xu. Deepsat: An eda-driven learning framework for SAT. _arXiv preprint arXiv:2205.13745_, 2022.
* [27] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural networks. In _International Conference on Learning Representations (ICLR)_, 2016.
* [28] Zhaoyu Li and Xujie Si. NSNet: A general neural probabilistic framework for satisfiability problems. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.

* Nair and Hinton [2010] Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In _International Conference on Machine Learning (ICML)_, 2010.
* Ozolins et al. [2022] Emils Ozolins, Karlis Freivalds, Andis Draguns, Eliza Gaile, Ronalds Zakovskis, and Sergejs Kozlovics. Goal-aware neural SAT solver. In _International Joint Conference on Neural Networks (IJCNN)_, 2022.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* Selman et al. [1992] Bart Selman, Hector J. Levesque, and David G. Mitchell. A new method for solving hard satisfiability problems. In _National Conference on Artificial Intelligence (AAAI)_, 1992.
* Selsam and Bjorner [2019] Daniel Selsam and Nikolaj S. Bjorner. Guiding high-performance SAT solvers with unsat-core predictions. In _Theory and Applications of Satisfiability Testing (SAT)_, 2019.
* Selsam et al. [2019] Daniel Selsam, Matthew Lamm, Benedikt Bunz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT solver from single-bit supervision. In _International Conference on Learning Representations (ICLR)_, 2019.
* Shi et al. [2022] Zhengyuan Shi, Min Li, Sadaf Khan, Hui-Ling Zhen, Mingxuan Yuan, and Qiang Xu. Satformer: Transformers for SAT solving. _arXiv preprint arXiv:2209.00953_, 2022.
* Marques Silva and Sakallah [1999] Joao P. Marques Silva and Karem A. Sakallah. GRASP: A search algorithm for propositional satisfiability. _IEEE Transactions on Computers_, 1999.
* Skladanivskyy [2019] Volodymyr Skladanivskyy. Minimalistic round-reduced sha-1 pre-image attack. _SAT RACE_, 2019.
* Wang et al. [2021] Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth McMillan, and Risto Miikkulainen. Neuromcomb: Improving SAT solving with graph neural networks. _arXiv preprint arXiv:2110.14053_, 2021.
* Wetzler et al. [2014] Nathan Wetzler, Marijn Heule, and Warren A. Hunt Jr. Drat-trim: Efficient checking and trimming using expressive clausal proofs. In _Theory and Applications of Satisfiability Testing (SAT)_, 2014.
* Wieland and Godbole [2001] Ben Wieland and Anant P. Godbole. On the domination number of a random graph. _The Electronic Journal of Combinatorics_, 2001.
* Xu et al. [2019] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations (ICLR)_, 2019.
* Yan et al. [2023] Zhiyuan Yan, Min Li, Zhengyuan Shi, Wenjie Zhang, Yingcong Chen, and Hongce Zhang. Addressing variable dependency in gnn-based SAT solving. _arXiv preprint arXiv:2304.08738_, 2023.
* Yolcu and Poczos [2019] Emre Yolcu and Barnabas Poczos. Learning local search heuristics for boolean satisfiability. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* Zhang et al. [2020] Wenjie Zhang, Zeyu Sun, Qihao Zhu, Ge Li, Shaowei Cai, Yingfei Xiong, and Lu Zhang. Nlocalsat: Boosting local search with solution prediction. In _International Joint Conference on Artificial Intelligence (IJCAI)_, 2020.

### Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Section 7. 3. Did you discuss any potential negative societal impacts of your work? [N/A] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Section 1. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 5, Appendix C.1, and Appendix D. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 5.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [N/A] 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]