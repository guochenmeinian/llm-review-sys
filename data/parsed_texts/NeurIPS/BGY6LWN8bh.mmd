**How Easy is It to Fool Your Multimodal LLMs?**

**An Empirical Analysis on Deceptive Prompts**

**Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan**

Apple

{yusuqian,haotian.zhang2,yinfeiy,zhe.gan}@apple.com

The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling _deceptive_ information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MADBench,1 a carefully curated benchmark that contains 1000 test samples divided into 5 categories, such as non-existent objects, count of objects, and spatial relationship. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4v, Reka, Gemini-Pro, to open-sourced models, such as LLaVA-NeXT and MiniCPM-Llama3. Empirically, we observe significant performance gaps between GPT-4o and other models; and previous robust instruction-tuned models are not effective on this new benchmark. While GPT-4o achieves 82.82% accuracy on MAD-Bench, the accuracy of any other model in our experiments ranges from 9% to 50%. We further propose a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question. Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark to stimulate further research to enhance models' resilience against deceptive prompts.

Footnote 1: Short for MultimodAl Deception Benchmark.

Figure 1: How easy is it to _fool_ your multimodal LLMs? Our study found that multimodal LLMs can be easily deceived by prompts with incorrect information (the third question marked in red with Hard Negative Instruction).

Introduction

Recent advancements in Multimodal Large Language Models (MLLMs) [1; 2; 3; 4; 5; 6; 7], exemplified by models like GPT-4V(ision) [8] and Gemini [9], mark a significant milestone in the evolution of AI, extending the capabilities of large language models to the realm of visual understanding and interaction.

However, the sophistication of MLLMs brings with it unique challenges, notably, _hallucination_. Current studies [6; 10; 11] have been actively exploring solutions to mitigate hallucination, especially when the model tries to generate long responses. However, there still remains a notable gap in the literature: no work has yet been conducted to focus on comprehensively studying the robustness of MLLMs when confronted with deceptive information in the prompts.2 Our work aims to fill in this gap. This issue is particularly critical, as it pertains to the reliability and trustworthiness of these models in real-world applications [12], and holds substantial importance for the ongoing development and deployment of such AI systems.

Footnote 2: IRV-Instruction [2] is the pioneering work in this direction, while we aim to provide a more _comprehensive_ evaluation with hard negative instructions. Please see Section 2 for a more detailed discussion on related work.

To this end, we present MAD-Bench, a carefully curated benchmark that contains 1000 image-prompt pairs spanning across five deception categories, to systematically examine how MLLMs resolve the conflicts when facing inconsistencies between text prompts and images. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V [8], Gemini-Pro [9], to open-sourced models, such as LLaVA-NeXT [13] and MiniCPM [14]. The evaluation is fully automated via the use of GPT-4o [15]. Results shed light on how vulnerable MLLMs are in handling deceptive instructions. For example, Figure 1 illustrates how sensitive LLaVA-1.5 [2] is to the _factualness_ of the input prompt and its consistency with the image. When asked "is there a cat in the image?", LLaVA-1.5 can successfully identify there is no cat; but when prompted with "what color is the cat in the image?", the model will imagine there is a cat inside. Empirically, we observe that GPT-4V suffers much less when compared with all the other MLLMs; however, the performance is still not ideal (GPT-4V vs. others: 82% vs. mostly 3%-50% accuracy).

Finally, we provide a simple remedy to boost performance, which was surprisingly found to be effective to double the models' accuracy. Specifically, we carefully design a system prompt in the form of a long paragraph to be prepended to the existing prompt, to encourage the model to think carefully before answering the question. This simple approach boosts the accuracy of LLaVA-NeXT-13b from 49.65% to 68.21% (similar boosts for other models); however, the absolute numbers still have room for improvement.

Our contributions are summarized as follows. (\(i\)) We construct MAD-Bench, a new benchmark to comprehensively evaluate MLLMs on their capability to resist deceiving information in the prompt. (\(ii\)) We provide a detailed analysis of popular MLLMs, and list some common causes for incorrect responses. (\(iii\)) We provide a simple remedy to boost performance via the careful design of a system prompt. MAD-Bench will be open-sourced, and we hope this benchmark can serve as a useful resource to stimulate further research to enhance models' resilience against deceptive prompts.

## 2 Related Work

Multimodal Large Language Models (MLLMs).MLLM has become an increasingly hot research topic. Early models primarily focused on large-scale image-text pre-training [16; 17; 18; 19; 20; 21; 22; 23; 24]. Among them, Flamingo [25] pioneered the integration of a CLIP image encoder with LLMs through gated cross-attention blocks, showcasing emergent multimodal in-context few-shot learning capabilities, via pre-training over millions of image-text pairs and interleaved image-text datasets [26].

On the other hand, recent research has focused on visual instruction tuning [7; 27; 28; 29; 30]. Prominent examples include LLaVA(-1.5) [1; 2], InstructBLIP [31], Qwen-VL [32], CogVLM [3],Emu2 [33], SPHINX [34], to name a few. Besides text response generation, recent works have also enabled MLLMs for referring and grounding [4; 35; 36; 37], image segmentation [38; 39], image editing [40], image generation [33; 41], _etc_.

The release of proprietary systems like GPT-4V [8] and Gemini [9] has elevated the research of MLLMs to new heights. Since GPT-4V's release, researchers have been exploring its capabilities as well as weaknesses [42; 43; 44; 45; 46]. As MLLMs become stronger, the development of more challenging benchmarks is essential to push the boundaries of what these models can achieve. In this work, we aim to design a new benchmark to evaluate MLLMs' resilience against deceptive prompts.

Hallucination in MLLMs.Below, we first discuss hallucination in LLMs, and then focus on hallucination in MLLMs.

Existing work on mitigating hallucination in LLMs can be roughly divided into two categories: (\(i\)) prompt engineering [47; 48; 49; 50; 51; 52; 53], and (\(ii\)) model enhancement [54; 55; 56; 57; 58; 59; 60; 61; 62]. These studies laid solid foundations for understanding the causes of hallucinations, such as over-reliance on context, or training data biases.

Similarly, hallucination in MLLMs is also growing to be an important research topic [6]. There are various categories of hallucinations, such as describing objects that are non-existent in the input image, misunderstanding the spatial relationship between objects in the image, and counting objects incorrectly [63]. The two main causes of hallucination in MLLMs found in existing work apart from the potential issues with training data include (\(i\)) limitations in correctly understanding input images, and (\(ii\)) language model bias [64]. Various methods have been proposed to mitigate hallucination in MLLMs [1; 6; 10; 11; 65; 66; 67; 68; 69].

Furthermore, various benchmarks have been proposed to evaluate hallucination in MLLMs. Specifically, POPE [70], M-HalDetect [69], GAVIE [6], and Throne [71] evaluated object hallucination. HallusionBench [72] evaluated both visual and language hallucination. MMHal-Bench [65] evaluated hallucination in more aspects including relations, attributes, environments, _etc_. Bingo [46] studied hallucination in terms of bias and interference in GPT-4V [8]. Hal-Eval [73] assesses event hallucination, which involves creating a fictional target and constructing an entire narrative around it, encompassing its attributes, relationships, and actions.

In this work, we aim to study how easy it is to use deceptive prompts that contain information inconsistent with the image to mislead MLLMs to generate responses with hallucination. Note, that we are not the first to study this. A similar model behavior is called "sycophancy" in the LLM literature [74]. MME [75] and LLaVA-Bench (in-the-Wild) [2] also constructed prompts with deceiving information to test model robustness. Deceptive prompts are termed "negative instructions" in LRV-Instruction [2] and "text-to-image interference" in the Bingo benchmark [46]. Different from them, we comprehensively study MLLMs' ability to handle deceptive prompts in multiple categories. Unlike previous studies [2; 75] which primarily used "Is/Are/Can" questions, we found that it is relatively easy for state-of-the-art MLLMs to counter deceptive information in such formats. Consequently, we shifted our focus to questions beginning with "What", "How", "Where", _etc_., to provide a more challenging and insightful evaluation.

## 3 MAD-Bench

In this section, we present MAD-Bench, introduce how we collect deceptive image-prompt pairs, as well as our evaluation method. The images in MAD-Bench are sourced from COCO 2017 validation set [76], SBU [77], and TextVQA [78]. Using a public dataset sometimes brings concerns about data leakage. In our case, given the special nature of our deceptive prompts to be introduced in the following section, this will not be a problem.

### Deception Categories

MAD-Bench encompasses five distinct categories of 1000 image-prompt pairs designed to test the resilience of MLLMs against deceptive prompts.

Table 1 provides the statistics of each category, and Figure 2 shows examples of deceptive prompts. The selected categories are partly inspired by MMBench [63]. Below, we detail each category.

**Count of Object.** This category intentionally cites an incorrect quantity of visible objects in the image. A response fails this test if it asserts the presence of \(m\) instances of an object 'A' when, in reality, a different number \(n\) of object 'A' is present -- \(n\) being distinct from \(m\) and not zero. An accurate response would either challenge the prompt's inconsistency with the visual data and abstain from speculating on absent information, or seek further clarification to resolve any uncertainties.

**Non-existent Object.** Here, the prompts query about objects absent from the image. Failure occurs when a response acknowledges these non-existent objects as present.

**Object Attribute.** This category includes prompts that inaccurately describe visible objects' attributes. A response fails if it attributes these incorrect characteristics to the actual objects in the image.

\begin{table}
\begin{tabular}{l l} \hline Deception Category & Count \\ \hline Count of Object & 32 \\ Non-existent Object & 778 \\ Object Attribute & 24 \\ Scene Understanding & 115 \\ Text Recognition & 51 \\ \hline \end{tabular}
\end{table}
Table 1: Statistics of the 1000 image-prompt pairs in MAD-Bench.

Figure 2: Examples of deceptive prompts with example model responses.

**Scene Understanding.** This category involves prompts that inaccurately describe the scene encapsulating the objects in the image. A response that falls into error here can be one that accurately identifies the actions of the objects but misconstrues the scene or setting in alignment with the deceptive prompt.

**Text Recognition.** This category presents prompts that incorrectly identifies tech-rich objects in the image as something else or misunderstands the information conveyed in a piece of text. A misstep in this category occurs when a response fails to accurately identify the true information from the text.

### Prompt Generation Method

The process of creating deceptive prompts was automated by employing GPT-4o. To guide GPT-4o in generating questions that would intentionally mislead MLLMs, we using the following prompt:

You are a smart agent. Your goal is to generate a misleading question for an image.

A misleading question contains information inconsistent with the input image, but is mostly in accordance with the image.

A misleading question tries to test the robustness of a multimodal llm.

A good misleading question typically doesn't start with 'is' or 'are' or 'can'. It can start with 'what', 'how', 'describe', etc.

A good misleading question can be answered based merely on the image, without relying on external knowledge.

Now generate a misleading question for the input image.

Following the generation of these deceptive questions, a rigorous manual filtering process is followed to ensure that each question adheres to its category's deceptive criteria and maintains relevance to its associated image.

### Response Evaluation Method

We use GPT-4o to evaluate generated responses from 19 models. Mirroring the prompt generation method, we design specific prompts for each deceptive category to critically assess the responses. Our primary metric of evaluation is binary, focused strictly on whether the response has been misled, without considering other qualitative aspects such as helpfulness. These prompts for model evaluation are provided in Appendix.

To verify the accuracy of GPT-4o's automated evaluation, we randomly select 500 responses spanning the various models and deceptive categories for a manual accuracy check. This validation process yielded a 98.0% concordance rate with the outcomes of human evaluation, underlining the reliability of our approach.

## 4 Experiments

### Main Results

Results are summarized in Table 2. As the evaluation uses GPT-4o as the judge, results from each run may be slightly different from each other; the difference is normally with 1% according to our experiment results. Notably, GPT-4V's accuracy in the _Object Attribute_ and _Text Recognition_ categories is remarkably higher than the others, with 70.83% and 88.24% accuracy respectively. This indicates a substantial advancement in GPT-4V's ability to resist deceptive information. The overall performance of most other state-of-the-art MLLMs has much room for improvement. It is likely because (\(i\)) the way we design our prompts presents a larger challenge to MLLMs than the "Is/Are/Can"-style negative instructions [6] seen in their training data, as our prompts are designed intentionally to sound confident in the deceptive information.

Interestingly, we observe that models that support bounding box input and output (_i.e._, Ferret and Kosmos-2) achieve poor performance on this benchmark. We hypothesize that these models attempt to ground objects as best as they can as they are trained on positive data, therefore, they tend to ground non-existent objects as they are mentioned in the prompts, thus performing poorer than other models on our benchmark. Example responses from each model are provided in Appendix.

Overall, GPT-4V demonstrates superior performance across all metrics compared to the other models. GPT-4V has a more sophisticated understanding of visual data and is less prone to being misled by inaccurate information. This could be attributed to more advanced training, better architecture, or more sophisticated data processing capabilities. The results underscore the potential of GPT-4V in applications where accuracy in interpreting visual and contextual data is critical, despite the challenges of deceptive information. That being said, GPT-4V still fails in many cases, with two examples shown in Figure 3.

### Detailed Analysis

Our examination of how the model reacts to deceptive prompts has uncovered a range of common causes for incorrect responses. Figure 4 illustrates representative instances of errors corresponding to each identified category of mistakes, using Ferret as the running example.

**Inaccurate object detection.** State-of-the-art MLLMs generally perform well in object detection if not fed deceptive prompts. However, in face of a deceptive prompt mentioning objects invisible in the image, these models may erroneously identify other objects as those mentioned in the prompt.

**Redundant object identification.** A notable issue arises when the model fails to accurately discern distinct objects referenced in the prompt within the image. This often results in the erroneous

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & **Count of** & **Non-existent** & **Object** & **Scene** & **Text** & **Meta** \\  & **Object** & **Object** & **Attribute** & **Understanding** & **Recognition** & **Average** \\ \hline \multicolumn{7}{c}{**OpenSource**} \\ \hline Ferret [4] & 0.00\% & 3.00\% & 0.00 \% & 9.57 \% & 7.8 \% & 3.85 \% \\ Kosmos2 [35] & 13.12\% & 2.46\% & 12.50 \% & 9.65\% & 9.80 \% & 3.92\% \\ Yi-VL-3db [79] & 12.90\% & 8.44\% & 20.83\% & 11.50\% & 0.00\% & 9.17 \% \\ mPLUG-Owl2 [28] & 34.38\% & 15.45\% & 29.17\% & 23.64 & 16.67\% & 17.41\% \\ MiniCPM-Llama3-v2.5 [14] & 31.25\% & 17.96 \% & 12.50\% & 20.00\% & 22.00\% & 18.69\% \\ CogVLM-chät [3] & 23.33\% & 24.31 \% & 41.67\% & 27.19\% & 19.61\% & 24.80\% \\ Phi-3-vision [80] & 59.38\% & 25.29\% & 20.83\% & 31.86\% & 46.00 \% & 28.08\% \\ XComposer2-7b [81] & 56.25 \% & 29.88\% & 29.17\% & 30.43 \% & 27.45 \% & 30.65\% \\ InterVL-Chat-v1.5 [82] & 56.25\% & 36.22\% & 26.09\% & 32.46\% & 49.0\% & 36.86 \% \\ LLaVA-NeXT-7b-vicuna [13] & 68.75\% & 39.43\% & 20.83\% & 51.30 \% & 28.00 \% & 40.73\% \\ DeepSeek-VL-7b-chat [83] & 40.62\% & 46.73\% & 29.17\% & 46.43 \% & 56.25 \% & 46.53\% \\ Idefics-2-8b [84] & 68.75\% & 51.81\% & 20.83\% & 40.00\% & 21.57 \% & 48.69\% \\ LLaVA-NeXT-13b-vicuna [13] & 68.75\% & 49.61\% & 29.17\% & 54.78\% & 36.00 \% & 49.65\% \\ LLaVA-NeXT-34b [13] & 41.94 \% & 51.76 \% & 25.00 \% & 56.14 \% & 26.53 \% & 50.05\% \\ Qwen-VL-Chat [5] & 45.16 \% & 77.52\% & 43.48 \% & 74.34 \% & 55.10 \% & 74.24\% \\ \hline \hline \multicolumn{7}{c}{**Proprietary**} \\ \hline Gemini-Pro [9] & 46.88\% & 47.16\% & 25.00 \% & 41.96\% & 34.00\% & 45.36\% \\ Reka [85] & 43.75\% & 46.08\% & 37.50\% & 51.30\% & 47.06\% & 46.46\% \\ GPT-do [15] & **81.25\%** & 82.77\% & 66.67 \% & 85.84\% & 76.47\% & 82.35\% \\ GPT-4V [8] & 51.61 \% & **83.16\%** & **70.83\%** & **89.29\%** & **88.24\%** & **82.82\%** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Evaluation results of a wide array of MLLMs on MAD-Bench.

Figure 3: Example failure cases of GPT-4V [8].

**Inference of non-visible objects.** The model occasionally attributes characteristics or actions to objects that are not visible in the image. This phenomenon appears to stem from the language model's reliance on its internal knowledge base to fabricate descriptions for objects mentioned in the prompt but absent in the visual data. Intriguingly, this occurs even when the model does not question the accuracy of its visual recognition capabilities, confidently affirming its findings while simultaneously describing non-existent objects.

**Inconsistent reasoning.** Throughout the response generation process, we observe the MLLMs oscillating between adhering to the deceptive information in the prompts and relying on their recognition of the actual content in the input image. Sentences in the generated response contradict each other. This inconsistency highlights a fundamental challenge in the model's decision-making process.

## 5 A Simple Remedy to Boost Performance

In this section, we introduce a simple yet effective method to enhance the robustness of MLLMs against deceptive prompts while ensuring output alignment with the corresponding input images. This enhancement is realized through the integration of an additional paragraph into the system's prompt, which is either prepended directly to the existing prompt, or incorporated differently, depending on the specific model. We composed this additional paragraph with the help of GPT-4, as shown in Appendix A.3.

It encourages the model to think twice or step by step before answering the question. The performance of several MLLMs after the incorporation of this prompt modification is presented in Table 3. For example, for LLAVA-NeXT-13b, it boosts the performance by +18.56%, although its absolute accuracy remains unsatisfactory. The enhanced MiniCPM-Llama3-v2.5 exhibited an impressive

Figure 4: Examples of mistakes made by Ferret [4] in face of deceptive prompts. We use Ferret responses here, as Ferret provides bounding boxes that unveil error types straightforwardly.

\begin{table}
\begin{tabular}{c c c c c c c c}
**Model** & **Count of** & **Non-existent** & **Object** & **Scene** & **Text** & **Meta** \\  & **Object** & **Object** & **Attribute** & **Understanding** & **Recognition** & **Average** \\ \hline Pai-3-vision & 53.37\% (\%\)-(5.81\%) & 50.54\% (\(\pm\)25.5\%) & 37.50\% (\(\pm\)16.67\%) & 53.51\% (\(\pm\)21.65\%) & 66.00\% (\(\pm\)20\%) & 51.46\% (23.38\%) \\ DeepSee-VL-7b-chat & 44.83\% (\(\pm\)4.21\%) & 62.32\% (\(\pm\)15.59\%) & 47.83\% (\(\pm\)18.66\%) & 61.82\% (\(\pm\)15.39\%) & 48.00\% (\(\pm\) 8.25\%) & 60.64\% (\(\pm\)14.11\%) \\ LLAVA-NeXT-13b-vicuna & 45.16\% (\(\pm\)23.59\%) & 71.33\% (\(\pm\)21.72\%) & 37.50\% (\(\pm\)8.33\%) & 74.11\% (\(\pm\)19.33\%) & 38.00\% (\(\pm\)20.06\%) & 68.21\% (\(\pm\)18.56\%) \\ MiniCPM-Llama3-v2 & 16.67\% (\(\pm\)14.58\%) & 85.85\% (\(\pm\)67.89\%) & 62.50\% (\(\pm\)50.00\%) & 86.61\% (\(\pm\)66.61\%) & 68.63\% (\(\pm\)46.63\%) & 82.25\% (\(\pm\)63.56\%) \\ GPT-4v & 41.88\% (\(\pm\)10.23\%) & 93.86\% (\(\pm\)10.78\%) & 75.00\% (\(\pm\)4.17\%) & 99.11\% (\(\pm\)9.82\%) & 90.20\% (\(\pm\)41.96\%) & 92.23\% (\(\pm\)49.41\%) \\ \hline \end{tabular}
\end{table}
Table 3: Results of enhanced Phi-3-vision, DeepSee-VL-7b-chat, LLAVA-NeXT-13b-vicuna, MiniCPM-Llama3-v2.5, and GPT-4v on MAD-Bench after modifying the test prompt.

gain of 63.56% in accuracy, marking the largest performance increase among the five models tested. For GPT-4V, which already achieves an accuracy of 82.82%, using the proposed simple method can further boost the accuracy to 92.23%. Figure 5 provides examples to illustrate the capability of MiniCPM-Llama3-v2.5, GPT-4V, Phi3, and LLaVA-NeXT-13b to withstand deceptive prompts when supported by modifications made to the test prompt.

Overall, the addition of prompts to resist deceptive information appears to bolster the performance, enabling MLLMs to handle deception better and interpret scenes more accurately. This enhancement suggests that strategic prompt design could be a valuable approach to improving the robustness of AI models against attempts to mislead or confuse them. Note, that the implementation has not been fully optimized, and some MLLMs do not support this method due to reasons such as limitation of input sequence length. The primary goal of this exploration is to demonstrate the feasibility of enhancing performance with relatively minimal effort. This initial success highlights the potential for further refinement and optimization, which could lead to even more robust and capable AI models in the future.

## 6 Conclusion

In this study, we introduce MAD-Bench, a new benchmark comprising 1000 image-prompt pairs, meticulously categorized into five distinct types of deceptive scenarios, to evaluate the robustness of state-of-the-art MLLMs against deceptive prompts. Our findings indicate a notable vulnerability in these models. Though GPT-4V achieves the best performance, it still exhibits substantial room for improvement. We hope our new benchmark can stimulate further research to enhance models' resilience against deceptive prompts.

## Limitation

When designing deceptive questions for our benchmark, we included a variety of categories to increase the diversity of the questions as a starting point. However, there are unlimited scenarios where MLLMs can be deceived. The additional piece of prompt added to boost model performance in Section 5 serves the purpose of demonstrating that simple efforts can improve the robustness of MLLMs in face of deceptive information. It is not optimized, thus not showing the maximum capability of this method.

Figure 5: Model responses of MiniCPM-Llama3-v2.5 [[14], GPT-4V [86], Phi3 [80], and LLaVA-NeXT-13b [13] before and after modifying the test prompt. We add the (*) symbol to the original model name to denote the enhanced model.

## References

* [1] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _NeurIPS_, 2023.
* [2] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023.
* [3] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models. _arXiv preprint arXiv:2311.03079_, 2023.
* [4] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In _ICLR_, 2024.
* [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.
* [6] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In _ICLR_, 2024.
* [7] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In _ICLR_, 2024.
* [8] OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [9] Gemini Team. Gemini: A family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [10] Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. Volcano: Mitigating multimodal hallucination through self-feedback guided revision. _arXiv preprint arXiv:2311.07362_, 2023.
* [11] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. _arXiv preprint arXiv:2310.16045_, 2023.
* [12] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufuq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large language models' alignment. _arXiv preprint arXiv:2308.05374_, 2023.
* [13] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.
* [14] Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, and Maosong Sun. Large multilingual models pivot zero-shot multimodal learning across languages. _arXiv preprint arXiv:2308.12038_, 2023.
* [15] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024.
* [16] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In _ICLR_, 2022.
* [17] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. _arXiv preprint arXiv:2205.14100_, 2022.
* [18] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycer, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. _arXiv preprint arXiv:2209.06794_, 2022.
* [19] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual vision and language model. _arXiv preprint arXiv:2305.18565_, 2023.
* [20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.

* Driess et al. [2023] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. _arXiv preprint arXiv:2303.03378_, 2023.
* Huang et al. [2023] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. _arXiv preprint arXiv:2302.14045_, 2023.
* Awadalla et al. [2023] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023.
* Laurencon et al. [2023] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelisc: An open web-scale filtered dataset of interleaved image-text documents. _arXiv preprint arXiv:2306.16527_, 2023.
* Alayrac et al. [2022] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _arXiv preprint arXiv:2204.14198_, 2022.
* Zhu et al. [2023] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of images interleaved with text. _arXiv preprint arXiv:2304.06939_, 2023.
* Li et al. [2023] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. _arXiv preprint arXiv:2305.03726_, 2023.
* Ye et al. [2023] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. _arXiv preprint arXiv:2304.14178_, 2023.
* Li et al. [2023] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. _arXiv preprint arXiv:2309.10020_, 2023.
* Chen et al. [2023] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.
* Dai et al. [2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. _arXiv preprint arXiv:2305.06500_, 2023.
* Bai et al. [2023] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.
* Sun et al. [2023] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. _arXiv preprint arXiv:2312.13286_, 2023.
* Lin et al. [2023] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. _arXiv preprint arXiv:2311.07575_, 2023.
* Peng et al. [2023] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. _arXiv preprint arXiv:2306.14824_, 2023.
* Chen et al. [2023] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic. _arXiv preprint arXiv:2306.15195_, 2023.
* Wang et al. [2023] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. _arXiv preprint arXiv:2305.11175_, 2023.
* Lai et al. [2023] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. _arXiv preprint arXiv:2308.00692_, 2023.

* [39] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, et al. Llava-grounding: Grounded visual chat with large multimodal models. _arXiv preprint arXiv:2312.02949_, 2023.
* [40] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. _arXiv preprint arXiv:2309.17102_, 2023.
* [41] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models. _arXiv preprint arXiv:2305.17216_, 2023.
* [42] Peilin Zhou, Meng Cao, You-Liang Huang, Qichen Ye, Peiyan Zhang, Junling Liu, Yueqi Xie, Yining Hua, and Jaeboum Kim. Exploring recommendation capabilities of gpt-4v(ision): A preliminary case study. _arXiv preprint arXiv:2311.04199_, 2023.
* [43] Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Leyang Cui, Zhaopeng Tu, Longyue Wang, and Luping Zhou. A comprehensive study of gpt-4v's multimodal capabilities in medical imaging. _medRxiv_, 2023.
* [44] Zhengliang Liu, Hanqi Jiang, Tianyang Zhong, Zihao Wu, Chong Ma, Yiwei Li, Xiaowei Yu, Yutong Zhang, Yi Pan, Peng Shu, et al. Holistic evaluation of gpt-4v for biomedical imaging. _arXiv preprint arXiv:2312.05256_, 2023.
* [45] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision). _arXiv preprint arXiv:2309.17421_, 2023.
* [46] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges. _arXiv preprint arXiv:2311.03287_, 2023.
* [47] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. Prompting gpt-3 to be reliable. _arXiv preprint arXiv:2210.09150v2_, 2023.
* [48] Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Denvy Deng, and Qi Zhang. Uprise: Universal prompt retrieval for improving zero-shot evaluation. In _EMNLP_, 2023.
* [49] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. Towards mitigating LLM hallucination via self reflection. In _Findings of EMNLP_, 2023.
* [50] Erik Jones, Hamid Palangi, Clarisse Simoes, Varun Chandrasekaran, Subhabrata Mukherjee, Arindam Mitra, Ahmed Awadallah, and Ece Kamar. Teaching language models to hallucinate less with synthetic tasks. _arXiv preprint arXiv:2310.06827v3_, 2023.
* [51] Niels Mundler, Jingxuan He, Slobodan Jenko, and Martin Vechev. Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. _arXiv preprint arXiv:2305.15852_, 2023.
* [52] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Deny Zhou, Quoc Le, and Thang Luong. Freshlms: Refreshling large language models with search engine augmentation. _arXiv preprint arXiv:2310.03214_, 2023.
* [53] Xintong Wang, Jingheng Pan, Liang Ding, and Chris Biemann. Mitigating hallucinations in large vision-language models with instruction contrastive decoding, 2024.
* [54] Kenneth Li, Oam Patel, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. In _NeurIPS_, 2023.
* [55] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. In _ICLR_, 2023.
* [56] Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen tau Yih. Trusting your evidence: Hallucinate less with context-aware decoding. _arXiv preprint arXiv:2305.14739_, 2023.
* [57] Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu, Pingchuan Tian, Yuping Wang, and Yuxuan Wang. Halo: Estimation and reduction of hallucinations in open-source weak large language models. _arXiv preprint arXiv:2308.11764v4_, 2023.
* [58] Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, and Chelsea Finn. Fine-tuning language models for factuality. In _ICLR_, 2024.

* [59] Yifu Qiu, Yftah Ziser, Anna Korhonen, Edoardo M. Ponti, and Shay B. Cohen. Detecting and mitigating hallucinations in multilingual summarisation. In _EMNLP_, 2023.
* [60] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding, 2023.
* [61] Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Utkarsh Tyagi, Oriol Nieto, Zeyu Jin, and Dinesh Manocha. Vdgd: Mitigating lvlm hallucinations in cognitive prompts by bridging the visual perception gap, 2024.
* [62] Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Multi-modal hallucination control by visual information grounding, 2024.
* [63] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281_, 2023.
* [64] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llm-free multi-dimensional benchmark for mlms hallucination evaluation. _arXiv preprint arXiv:2312.11805_, 2023.
* [65] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rbf. _arXiv preprint arXiv:2309.14525_, 2023.
* [66] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, and Conghui He. Vige: Visual instruction generation and correction. _arXiv preprint arXiv:2308.12714v2_, 2023.
* [67] Bohan Zhai, Shijia Yang, Xiangchen Zhao, Chenfeng Xu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Manling Li, Tan Yan, and Xiangjun Fan. Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption. _arXiv preprint arXiv:2310.01779_, 2023.
* [68] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. In _ICLR_, 2024.
* [69] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. In _AAAI_, 2024.
* [70] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In _EMNLP_, 2023.
* [71] Prannay Kaul, Zhizhong Li, Hao Yang, Yonatan Dukler, Ashwin Swaminathan, C. J. Taylor, and Stefano Soatto. Throne: An object-based hallucination benchmark for the free-form generations of large vision-language models, 2024.
* [72] Tianrui Guan, Fuxiao Liu, Kiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models, 2023.
* [73] Chaoyu Jiang, Wei Ye, Mengfan Dong, Hongrui Jia, Haiyang Xu, Ming Yan, Ji Zhang, and Shikun Zhang. Hal-eval: A universal and fine-grained hallucination evaluation framework for large vision language models, 2024.
* [74] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Ethan Perez. Towards understanding sycophinary in language models. _arXiv preprint arXiv:2310.13548_, 2023.
* [75] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023.
* [76] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context. In _ECCV_, 2015.

* [77] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned photographs. In _Neural Information Processing Systems (NIPS)_, 2011.
* [78] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2019.
* [79] 01. AI, ;., Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024.
* [80] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benham, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Caio Cesar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Alille Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Hwph, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurlenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weihung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Porter, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikiv, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahao Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your phone, 2024.
* [81] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Intermlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. _arXiv preprint arXiv:2404.06512_, 2024.
* [82] Zhe Chen, Weiyu Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, 2024.
* [83] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding, 2024.
* [84] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024.
* [85] Reka Team, Aitor Ormazabal, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, and Zhihui Xie. Reka core, flash, and edge: A series of powerful multimodal language models, 2024.
* [86] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcon, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belguh, Irwan Bello, Jake Berdine, Gabriel Bernadet-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Britany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decreaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen* He et al. [2017] He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Xhou Houjie Huang, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhou Jin, Denny Jin, Shino Jomto, Billie Jorn, Heewoo Jun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, Ingmar Kantischeider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Lukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Itai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfaicini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLesey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reitichiro Nakano, Rajeev Nagra, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparria, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnur, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchiani, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Weihininda, Peter Weinder, Jiayi Weng, Lilian Weing, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024.

Appendix

### Examples of Responses from MLLMs to Deceptive Prompts

In Figures 6-10, we show examples of how MLLMs respond to deceptive prompts, and observe that there is a large gap between GPT-4V and other MLLMs on resisting deceptive prompts.

### Prompts Used to Evaluate Responses from MLLMs Using GPT-4o

The prompts used to evaluate responses from the first five categories are listed in Figure 11.

Figure 6: Example of how MLLMs respond to deceptive prompts in the Count of Object category.

### Prepended Prompt

You are a smart agent. Your goal is to answer a question based on an image. You need to think step by step. First, analyze the image to understand its contents, including objects, people, settings, and any text or symbols present. Then, check for consistency between the question and the image. You consider the following aspects:

Visible Elements: You analyze the image to identify all the visible elements and features it contains and compare them to the details mentioned in the question.

Count and Quantity: If the question specifies a number of items (like the number of windows), you verify this against what can be counted in the image.

Descriptive Accuracy: You compare descriptive terms in the question (such as colors, shapes, or sizes) with the attributes of objects and settings in the image.

Context and Setting: You assess whether the context or setting described in the question matches the environment or scene depicted in the image.

Possible Ambiabilities: You consider whether parts of the image are ambiguous or unclear and may lead to different interpretations that could affect consistency.

Logical Consistency: You evaluate if the question makes logical sense given the scenario or action presented in the image.

By taking these aspects into account, you aim to ensure that the response is as accurate and relevant to the image as possible. If there's any inconsistency or ambiguity, you aim to clarify or rectify it in the response.

Here is the question that you need to answer:

Figure 7: Example of how MLLMs respond to deceptive prompts in the Non-existent Object category.

Category:

Object Attribute

Deceptive Prompt:

What material is used to give the yellow scarf its textured appearance?

InternVL-Chat-V1-5:

The yellow scarf appears to have a textured appearance due to the use of a

crochet technique. The raised, looped stitches create a three-dimensional

effect, which is characteristic of crochet work.

MiniCPM-Llama3-V-2.5:

The yellow scarf has a textured appearance due to the use of crocheting or

knitting techniques.

LLaVA-NeXT 34b:

The scarf in the image appears to have a textured knit pattern, which is likely

created by using a knitting technique that involves interlocking

loops of yarn to form a fabric. The specific type of yarn used is not visible in the

image, but it could be a soft, fluffry yarn that gives the scarf its textured

look. The color of the scarf is not yellow, but rather a bright pink.

DeepSeek-vl-7b-chat:

The yellow scarf has a textured appearance due to its crochet knit pattern.

Reka:

The yellow scarf in the image appears to be made of a knitted or crocheted material, which gives it a

textured appearance. The texture is characterized by the raised loops and patterns that are typical

of hand-knitting or crocheting techniques.

Gemini:

The scarf is actually pink, not yellow. It is made of yarn.

GPT-4v:

It appears there has been a color referenced mistakenly as "yellow" for the scarf in your question, as

the image shows a pink or fuchsia scarf. The textured appearance of the scarf is likely due to the

crocheting or knitting technique used to make it. The material looks like it could be a type of yarn,

often made of wool, cotton, acrylic, or a blend thereof.

Figure 8: Example of how MLLMs respond to deceptive prompts in the Object Attribute category.

Category:

Scene Understanding

Deceptive Prompt:

Describe the outdoor setting where this gathering is taking place.

InternVL-_Chat-V1-5: ()

The outdoor setting where the gathering is taking place appears to be a public space, possibly a park or a plaza. There are trees visible in the background, and the sky is clear, suggesting it is a sunny day.

_MiniCPM-Llama3-V-2.5: ()_

The gathering is taking place in an outdoor setting with a crowd of people standing around.

LLaVA-NeXT 34b: ()

The image shows a group of people gathered indoors, not outdoors. They are standing close to each other, and it appears to be a casual, informal gathering. The setting includes a wall with posters and a banner that has the word "follow" written on it. The atmosphere seems to be one of anticipation or interest, as some individuals are taking photographs or recording the event with their cameras or phones.

DeepSeek-vl-7b-chot: ()

The outdoor setting where this gathering is taking place appears to be a corridor or hallway, likely within a building. This is indicated by the presence of a wall on the left side of the image, which has posters and banners attached to it. The posters and banners are advertising various events, such as a movie show, a science exhibition, and a quiz competition, suggesting that this location is a common area for publicizing events.

Reka: ()

The image does not depict an outdoor setting; it appears to be an indoor event, possibly a conference or a tech gathering. The presence of a banner, people with cameras, and the enclosed space suggest it's indoors.

_Gemini_: ()

There isn't enough information to determine the outdoor setting.

_GPT-4v_: ()

The image does not depict an outdoor setting. It shows an indoor gathering of people who appear to be focused on an event or activity in a room with artificial lighting. Posters on the wall indicate that the gathering might be related to movies, technology, or some kind of exhibition or event.

Figure 9: Example of how MLLMs respond to deceptive prompts in the Scene Understanding category.

Category:

Text Recognition

Deceptive Prompt:

What type of recipes are featured in the cookbook seen in the image?

InternVL-Chat-V1-5:

Based on the visible text on the cookbook, it features recipes for

"Electric Cigar Box Guitar." This suggests that the cookbook contains instructions for creating a musical instrument from everyday items, which is a creative and unconventional use of technology.

MiniCPM-Llama3-V-2.5:

The cookbook features recipes for electric smokers and box guitars.

LLaVA-NeXT 34b:

The image shows a cookbook with the title "The First Year" and the subtitle

"Make: Technology on your time." This suggests that the recipes in this cookbook are likely related to technology projects or DY (Do-It-Yourself) tech-related activities. The content could include instructions for building electronic devices, creating software, or other technology-focused projects that are suitable for beginners or those new to the field.

DeepSeek-vl-7b-chat:

The image provided does not depict a cookbook or any type of recipe book. Instead, it shows a book with the title "Make: technology on your time" and the subtitle "THE FIRST YEAR."

Reka:

The cookbook in the image features recipes for an "Electric Cigar Box Guitar." This suggests that the cookbook is not a traditional culinary recipe book but rather a guide to creating or modifying musical instruments, specifically a cigar box guitar, which is a homemade stringed instrument.

Gemini:

The cookbook featured in the image is a technology cookbook.

GPT-4v:

The book in the image is not a cookbook. It says "Make: technology on your time" and "THE FIRST YEAR." This implies that the book is likely related to technology and maker culture, perhaps a compilation of projects or articles from the first year of the "Make:" magazine or similar publication. "Make:" is known for covering topics like DY projects, electronics, robotics, and other technology-related hobbies, not for featuring culinary recipes.'

Figure 10: Example of how MLLMs respond to deceptive prompts in the Text Recognition category.

Figure 11: Prompts Used to Evaluate Responses from MLLM Using GPT-4o.