# \(p\)-Poisson surface reconstruction

in curl-free flow from point clouds

Yesom Park\({}^{1}\)1, Taekyung Lee\({}^{2}\)2, Jooyoung Hahn\({}^{3}\), Myungjoo Kang\({}^{1}\)

\({}^{1}\) Department of Mathematical Sciences, Seoul National University

\({}^{2}\) Interdisciplinary Program in Artificial Intelligence, Seoul National University

\({}^{3}\) Department of Mathematics and Descriptive Geometry,

Slovak University of Technology in Bratislava

{yeisom, d1xorud1231, mkang}@snu.ac.kr

jooyoung.hahn@stuba.sk

Equal contribution authors. Correspondence to: <mkang@snu.ac.kr>.

###### Abstract

The aim of this paper is the reconstruction of a smooth surface from an unorganized point cloud sampled by a closed surface, with the preservation of geometric shapes, without any further information other than the point cloud. Implicit neural representations (INRs) have recently emerged as a promising approach to surface reconstruction. However, the reconstruction quality of existing methods relies on ground truth implicit function values or surface normal vectors. In this paper, we show that proper supervision of partial differential equations and fundamental properties of differential vector fields are sufficient to robustly reconstruct high-quality surfaces. We cast the \(p\)-Poisson equation to learn a signed distance function (SDF) and the reconstructed surface is implicitly represented by the zero-level set of the SDF. For efficient training, we develop a variable splitting structure by introducing a gradient of the SDF as an auxiliary variable and impose the \(p\)-Poisson equation directly on the auxiliary variable as a hard constraint. Based on the curl-free property of the gradient field, we impose a curl-free constraint on the auxiliary variable, which leads to a more faithful reconstruction. Experiments on standard benchmark datasets show that the proposed INR provides a superior and robust reconstruction. The code is available at https://github.com/Yebbi/PINC.

## 1 Introduction

Surface reconstruction from an unorganized point cloud has been extensively studied for more than two decades [10, 29, 40, 9, 62] due to its many downstream applications in computer vision and computer graphics[8, 16, 62, 58]. Classical point cloud or mesh-based representations are efficient but they do not guarantee a watertight surface and are usually limited to fixed geometries. Implicit function-based representations of the surface [28, 64, 43, 14] as a level set \(\mathcal{S}=\left\{\mathbf{x}\in\mathbb{R}^{3}\mid u\left(\mathbf{x}\right)=c\right\}\) of a continuous implicit function \(u:\mathbb{R}^{3}\rightarrow\mathbb{R}\), such as signed distance functions (SDFs) or occupancy functions, have received considerable attention for providing watertight results and great flexibility

Figure 1: Comparison of reconstruction using an eikonal equation (9), the \(p\)-Poisson equation (8), and the proposed \(p\)-Poisson equation with the curl-free condition (11).

in representing different topologies. In recent years, with the rise of deep learning, a stream of work called _implicit neural representations_ (INRs) [2; 44; 16; 61; 19; 55; 53; 50] has revisited them by parameterizing the implicit function \(u\) with neural networks. INRs have shown promising results by offering efficient training and expressive surface reconstruction.

Early INRs [44; 42; 16] treat the points-to-surface problem as a supervised regression problem with ground-truth distance values, which are difficult to use in many situations. To overcome this limitation, some research efforts have used partial differential equations (PDEs), typically the eikonal equation, as a means to relax the 3D supervision [23; 37; 48]. While these efforts have been successful in reconstructing various geometries, they encounter an issue of non-unique solutions in the eikonal equation and rely heavily on the oriented normal vector at each point. They often fail to capture fine details or reconstruct plausible surfaces without normal vectors. A raw point cloud usually lacks normal vectors or numerically estimated normal vectors [1; 18] contain approximation errors. Moreover, the prior works are vulnerable to noisy observations and outliers.

The goal of this work is to propose an implicit representation of surfaces that not only provides smooth reconstruction but also recovers high-frequency features only from a raw point cloud. To this end, we provide a novel approach that expresses an approximated SDF as the unique solution to the \(p\)-Poisson equation. In contrast to previous studies that only describe the SDF as a network, we define the gradient of the SDF as an auxiliary variable, motivated by variable splitting methods [47; 60; 22; 12] in the optimization literature. We then parameterize the auxiliary output to automatically satisfy the \(p\)-Poisson equation by reformulating the equation in a divergence-free form. The divergence-free splitting representation contributes to efficient training by avoiding deeply nested gradient chains and allows the use of sufficiently large \(p\), which permits an accurate approximation of the SDF. In addition, we impose a curl-free constraint [25] because the auxiliary variable should be learned as a conservative vector field which has vanishing curl. The curl-free constraint serves to achieve a faithful reconstruction. We carefully evaluate the proposed model on widely used benchmarks and robustness to noise. The results demonstrate the superiority of our model without a priori knowledge of the surface normal at the data points.

## 2 Background and related works

Implicit neural representationsIn recent years, implicit neural representations (INRs) [41; 16; 3; 55; 54], which define a surface as zero level-sets of neural networks, have been extensively studied. Early work requires the ground-truth signed implicit function [44; 16; 41], which is difficult to obtain in real-world scenarios. Considerable research [3; 4] is devoted to removing 3D supervision and relaxing it with a ground truth normal vector at each point. In particular, several efforts use PDEs to remove supervision and learn implicit functions only from raw point clouds. Recently, IGR [23] revisits a conventional numerical approach [14] that accesses the SDF by incorporating the eikonal equation into a variational problem by using modern computational tools of deep learning. Without the normal vector, however, IGR misses fine details. To alleviate this problem, FFN [56] and SIREN [55] put the high frequencies directly into the network. Other approaches exploit additional loss terms to regulate the divergence [6] or the Hessian [63]. The vanishing viscosity method, which perturbs the eikonal equation with a small diffusion term, is also considered [37; 49] to mitigate the drawback that the eikonal loss has unreliable minima. The classical Poisson reconstruction [31], which recovers the implicit function by integration over the normal vector field, has also been revisited to accelerate the model inference time [48], but supervision of the normal vector field is required. Neural-Pull [39] constructs a new loss function by borrowing the geometrical property that the SDF and its gradient define the shortest path to the surface.

\(p\)-Poisson equationThe SDF is described by a solution of various PDEs. The existing work [23; 55; 6] uses the eikonal equation, whose viscosity solution describes the SDF. However, the use of the residual of the eikonal equation as a loss function raises concerns about the convergence to the SDF due to non-unique solutions of the eikonal equation. Recent works [55; 6] utilize the notion of vanishing viscosity to circumvent the issue of non-unique solutions. In this paper, we use the \(p\)-Poisson equation to approximate the SDF, which is a nonlinear generalization of the Poisson equation (\(p=2\)):

\[\begin{cases}-\triangle_{p}u=-\nabla\cdot\left(\left\|\nabla u\right\|^{p-2} \nabla u\right)=1\text{ in }\Omega\\ u=0\text{ on }\Gamma,\end{cases}\] (1)where \(p\geq 2\), the computation domain \(\Omega\subset\mathbb{R}^{3}\) is bounded, and \(\Gamma\) is embedded in \(\Omega\).

The main advantage of using the \(p\)-Poisson equation is that the solution to (1) is unique in Sobolev space \(W^{1,p}\left(\Omega\right)\)[36]. The unique solution with \(p\geq 2\) brings a viscosity solution of the eikonal equation in the limit \(p\rightarrow\infty\), which is the SDF, and it eventually prevents finding non-viscosity solutions of the eikonal equation; see a further discussion with an example in Appendix C.1. Moreover, in contrast to the eikonal equation, it is possible to describe a solution of (1) as a variational problem and compute an accurate approximation [5; 20]:

\[\min_{u}\int_{\Omega}\frac{\left\|\nabla u\right\|^{p}}{p}d\mathbf{x}-\int_{ \Omega}ud\mathbf{x}.\] (2)

As \(p\rightarrow\infty\), it has been shown [11; 30] that the solution \(u\) of (1) converges to the SDF whose zero level set is \(\Gamma\). As a result, increasing \(p\) gives a better approximation of the SDF, which is definitely helpful for surface reconstruction. However, it is still difficult to use a fairly large \(p\) in numerical computations and in this paper we will explain one of the possible solutions to the mentioned problem.

## 3 Method

In this section, we propose a \(p\)-Poisson equation based **I**mplicit **N**eural representation with **C**url-free constraint (**PINC**). From an unorganized point cloud \(\mathcal{X}=\left\{\mathbf{x}_{i}:i=1,2,\ldots,N\right\}\) sampled by a closed surface \(\Gamma\), a SDF \(u:\mathbb{R}^{3}\rightarrow\mathbb{R}\) whose zero level set is the surface \(\Gamma=\left\{\mathbf{x}\in\mathbb{R}^{3}\mid u\left(\mathbf{x}\right)=0\right\}\) is reconstructed by the proposed INR. There are two key elements in the proposed method: First, using a variable-splitting representation [45] of the network, an auxiliary output is used to learn the gradient of the SDF that satisfies the \(p\)-Poisson equation (1). Second, a curl-free constraint is enforced on an auxiliary variable to ensure that the differentiable vector identity is satisfied.

### \(p\)-Poisson equation

A loss function in the physics-informed framework [51] of the existing INRs for the \(p\)-Poisson equation (1) can be directly written:

\[\min_{u}\int_{\Gamma}\left|u\right|d\mathbf{x}+\lambda_{0}\int_{\Omega}\left| \nabla\cdot\left(\left\|\nabla u\right\|^{p-2}\nabla u\right)+1\right|d \mathbf{x},\] (3)

where \(\lambda_{0}>0\) is a regularization constant. To reduce the learning complexity of the second integrand, we propose an augmented network structure that separately parameterizes the gradient of the SDF as an auxiliary variable that satisfies the \(p\)-Poisson equation (1).

Variable-splitting strategyUnlike existing studies [23; 37; 6] that use neural networks with only one output \(u\) for the SDF, we introduce a separate auxiliary network output \(G\) for the gradient of the SDF; see that the same principle is used in [45]. In the optimization literature, it is called the variable splitting method [47; 60; 22; 12] and it has the advantage of decomposing a complex minimization into a sequence of relatively simple sub-problems. With the auxiliary variable \(G=\nabla u\) and the penalty method [13], the variational problem (3) is converted into an unconstrained problem:

\[\min_{u,G}\int_{\Gamma}\left|u\right|d\mathbf{x}+\lambda_{0}\int_{\Omega} \left|\nabla\cdot\left(\left\|G\right\|^{p-2}G\right)+1\right|d\mathbf{x}+ \lambda_{1}\int_{\Omega}\left\|\nabla u-G\right\|^{2}d\mathbf{x},\] (4)

where \(\lambda_{1}>0\) is a penalty parameter representing the relative importance of the loss terms.

\(p\)-Poisson as a hard constraintLooking more closely at the minimization (4), if \(G\) is already a gradient to satisfy (1), then the second term in (4) is no longer needed and it brings the simplicity of one less parameter. Now, for a function \(F:\Omega\rightarrow\mathbb{R}^{3}\) such that \(\nabla\cdot F=1\), for example \(F\left(\mathbf{x}\right)=\frac{1}{3}\mathbf{x}\), the \(p\)-Poisson equation (1) is reformulated by the divergence-free form:

\[\nabla\cdot\left(\left\|\nabla u\right\|^{p-2}\nabla u+F\right)=0.\] (5)

Then, there exists a vector potential \(\Psi:\mathbb{R}^{3}\rightarrow\mathbb{R}^{3}\) satisfying

\[\left\|G\right\|^{p-2}G+F=\nabla\times\Psi,\] (6)

[MISSING_PAGE_FAIL:4]

Note that the optimal \(\tilde{G}\) should have a unit norm according to the eikonal equation. To facilitate training, we relax this nonconvex equality condition into a convex constraint \(\|\ \tilde{G}\ \|\leq 1\). To this end, we parameterize the second network auxiliary output \(\tilde{\Psi}\) and define \(\tilde{G}\) by

\[\tilde{G}=\mathcal{P}\left(\tilde{\Psi}\right)\coloneqq\frac{\tilde{\Psi}}{ \max\left\{1,\|\ \tilde{\Psi}\ \|\right\}},\] (12)

where \(\mathcal{P}\) is the projection operator to the three-dimensional unit ball. Appendix A provides further discussion on the importance of the curl-free term to learn a conservative vector field.

Figure 2 illustrates the proposed network architecture. The primary and the auxiliary variables are trained in a single network, instead of being trained separately in individual networks. The number of network parameters remains almost the same since only the output dimension of the last layer is increased by six, while all hidden layers are shared.

### Proposed loss function

In the case of a real point cloud to estimate a closed surface by range scanners, it is inevitable to have occluded parts of the surface where the surface has a concave part depending on possible angles of the measurement [35]. It ends up having relatively large holes in the measured point cloud. Since there are no points in the middle of the hole, it is necessary to have a certain criterion for how to fill in the hole. In order to focus to check the quality of \(\mathcal{L}_{\text{PINC}}\) (11) in this paper, we choose a simple rule to minimize the area of zero level set of \(u\):

\[\mathcal{L}_{\text{total}}=\mathcal{L}_{\text{PINC}}+\lambda_{4}\int_{\Omega} \delta_{\epsilon}\left(u\right)\|\nabla u\|\,d\mathbf{x},\] (13)

where \(\lambda_{4}>0\) and \(\delta_{\epsilon}(x)=1-\tanh^{2}\left(\frac{x}{\epsilon}\right)\) is a smeared Dirac delta function with \(\epsilon>0\). The minimization of the area is used in [21; 49] and the advanced models [15; 27; 63] on missing parts of the point cloud to provide better performance of the reconstruction.

## 4 Experimental results

In this section, we evaluate the performance of the proposed model to reconstruct 3D surfaces from point clouds. We study the following questions: **(i)** How does the proposed model perform compared to existing INRs? **(ii)** Is it stable from noise? **(iii)** What is the role of the parts that make up the model and the loss? Each is elaborated in order in the following sections.

ImplementationAs in previous studies [44; 23; 37], we use an 8-layer network with 512 neurons and a skip connection to the middle layer, but only the output dimension of the last layer is increased by six due to the auxiliary variables. For (13), we empirically set the loss coefficients to \(\lambda_{1}=0.1\), \(\lambda_{2}=0.0001\). \(\lambda_{3}=0.0005\), and \(\lambda_{4}=0.1\) and use \(p=\infty\) in (7) for numerical simplicity. We implement all numerical experiments on a single NVIDIA RTX 3090 GPU. In all experiments, we use the Adam optimizer [32] with learning rate \(10^{-3}\) decayed by \(0.99\) every \(2000\) iterations.

Figure 2: The visualization of the augmented network structure with two auxiliary variables.

DatasetsWe leverage two widely used benchmark datasets to evaluate the proposed model for surface reconstruction: Surface Reconstruction Benchmark (SRB) [7] and Thingi10K [65]. The geometries in the mentioned datasets are challenging because of their complex topologies and incomplete observations. Following the prior works, we adopt five objects per dataset. We normalize the input data to center at zero and have a maximum norm of one.

BaselinesWe compare the proposed model with the following baselines: IGR [23], SIREN [55], SAL [3], PHASE [37], and DiGS [6]. All models are evaluated from only raw point cloud data without surface normal vectors. A comparison with models that leverage surface normals as supervision is included in Appendix C.

MetricsTo estimate the quantitative accuracy of the reconstructed surface, we measure Chamfer (\(d_{C}\)) and Hausdorff (\(d_{H}\)) distances between the ground-truth point clouds and the reconstructed surfaces. Moreover, we report one-sided distances \(d_{\overrightarrow{\mathcal{C}}}\) and \(d_{\overrightarrow{H}}\) between the noisy data and the reconstructed surfaces. Please see Appendix B.2 for precise definitions.

### Surface reconstruction

We validate the performance of the proposed PINC (13) in surface reconstruction in comparison to other INR baselines. For a fair comparison, we consider the baseline models that were trained without a normal prior. Table 1 summarizes the numerical comparison on SRB in terms of metrics. We report the results of baselines from [37, 49, 6]. The results show that the reconstruction quality obtained is on par with the leading INRs, and we achieved state-or-the-art performance for Chamfer distances.

Figure 3: 3D Reconstruction results for SRB and Thingi10K datasets.

[MISSING_PAGE_FAIL:7]

shows that the variable splitting method, which satisfies the \(p\)-Poisson equation as a hard constraint (without the curl-free condition), recovers a fairly decent surface, but it generates oversmoothed surfaces and details are lost. However, as we can see from the qualitative result reconstructed with the curl-free constraint, this constraint allows us to capture the details that PINC without the curl-free condition cannot recover. The metric values presented in Table 3 also provide clear evidence of the need for the curl-free term. To further examine the necessity of another auxiliary variable \(\tilde{G}\), we conduct an additional experiment by applying the curl-free loss term directly on \(G\) without the use of \(\tilde{G}\). The results are presented in the second row of the Table 3. The results indicate that taking curl on \(G\), which is constructed by taking curl on \(\Psi\) in (7), leads to a suboptimal reconstruction. This is likely due to a challenging optimization landscape that is difficult to optimize as a result of consecutive automatic differentiation [59]. The results provide numerical evidences of the necessity of introducing \(\tilde{G}\).

Effect of minimal area criterionWe study the effect of the minimal area criterion suggested in Section 3.3. In real scenarios, there are defected regions where the surface has not been measured. To fill this part of the hole, the minimum surface area is considered. Figure 6 clearly shows this effect. Some parts in the daratech of SRB have a hole in the back. Probably because of this hole, parts that are not manifolds are spread out as manifolds as shown in the left figure without considering the minimal area. However, we can see that adding a minimal area loss term alleviates this problem. We would like to note that, except for daratech, we did not encounter this problem because other data are point clouds sampled from a closed surface and also are not related to hole filling. Indeed, we

Figure 4: Reconstruction results from noisy observations. Two levels of additive Gaussian noise with standard deviations \(\sigma=0.005\) (low) and \(0.01\) (high) are considered.

Figure 5: Comparison of surface reconstruction without (left) and with (right) curl-free constraint.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{2}{c}{GT} & \multicolumn{2}{c}{Scans} \\ Model & \(d_{C}\) & \(d_{H}\) & \(d_{\widetilde{C}}\) & \(d_{\widetilde{H}}\) \\ \hline wo/ curl free & 0.20 & 4.96 & 0.12 & 2.98 \\ w/ curl free on \(G\) & 4.17 & 52.26 & 0.48 & 6.03 \\ w/ curl free on \(\tilde{G}\) & 0.16 & 4.78 & 0.05 & 0.80 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Quantitative results on the ablation study of the curl-free term.

empirically observe that the results are quite similar with and without the minimal area term for all data other than charatech.

Effect of large \(p\)The \(p\)-Poisson equation (1) draws the SDF as \(p\) becomes infinitely large. Therefore, it is natural to think that it would be good to use a large \(p\). Here, we conducted experiments on the effect of \(p\). We define \(G\) with various \(p=2,10\), and \(100\) and learn the SDF with it. Figure 7 shows surfaces that were recovered from the Gargoyle data in the SRB with different \(p\) values. When \(p\) is as small as 2, it is obvious that it is difficult to reconstruct a compact surface from points. When \(p\) is 10, a much better surface is constructed than that of \(p=2\), but the by-products still remain on the small holes. Furthermore, a large value of \(p=100\) provides a quite proper reconstruction. This experimental result demonstrates that a more accurate approximation can be obtained by the use of a large \(p\), which is consistent with the theory. This once again highlights the advantage of the variable splitting method we have proposed, which allows an arbitrarily large \(p\) to be used. This highlights the advantage of the variable splitting method (7) we have proposed in Section 3.1, which allows an arbitrarily large \(p\) to be used. Note that the previous approaches have not been able to use large \(p\) because the numeric value of \(p\)-power easily exceeds the limit of floating precision. On the other hand, the proposed method is amenable to large \(p\) and hence the reconstruction becomes closer to the point cloud.

## 5 Conclusion and limitations

We presented a \(p\)-Poisson equation-based shape representation learning, termed PINC, that reconstructs high-fidelity surfaces using only the locations of given points. We introduced the gradient of the SDF as an auxiliary network output and incorporated the \(p\)-Poisson equation into the auxiliary variable as a hard constraint. The curl-free constraint was also used to provide a more accurate representation. Furthermore, the minimal surface area regularization was considered to provide a compact surface and overcome the ill-posedness of the surface reconstruction problem caused by unobserved points. The proposed PINC successively achieved a faithful surface with intricate details and was robust to noisy observations.

The minimization of the surface area is used to reconstruct missing parts of points under the assumption that a point cloud is measured by a closed surface. Regarding the hole-filling strategy, it still needs further discussion and investigation of various constraints such as mean curvature or total variation of the gradient. At present, the proposed PDE-based framework is limited to closed surfaces and is inadequate to reconstruct open surfaces. We leave the development to open surface reconstruction as future work. Establishing a neural network initialization that favors the auxiliary gradient of the SDF would be an interesting venue. Furthermore, the computational cost of convergence would differ when using and not using auxiliary variables. Analyzing the convergence speed or computational cost of utilizing auxiliary variables versus not utilizing them is a worthwhile direction for future research.

## 6 Societal Impacts

The proposed PINC allows high-quality representation of 3D shapes only from raw unoriented 3D point cloud. It has many potential downstream applications, including product design, security, medical imaging, robotics, and the film industry. We are aware that accurate 3D surface reconstruction

Figure 6: Comparison of surface recovery without (a) and with (b) minimum area criterion.

can be used in malicious environments such as unauthorized reproduction of machines without consent and digital impersonation. However, it is not a work to develop a technique to go to abuse, and we hope and encourage users of the proposed model to concenter on the positive impact of this work.

## 7 Acknowledgements

This work was supported by the NRF grant [2012R1A2C3010887] and the MSIT/IITP ([1711117093], [2021-0-00077], [No. 2021-0-01343, Artificial Intelligence Graduate School Program(SNU)]). Also, this project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No. 945478.

## References

* [1] Nina Amenta and Marshall Bern. Surface reconstruction by Voronoi filtering. In _Proceedings of the Fourteenth Annual Symposium on Computational Geometry_, pages 39-48, 1998.
* [2] Matan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, and Yaron Lipman. Controlling neural level sets. _Advances in Neural Information Processing Systems_, 32, 2019.
* [3] Matan Atzmon and Yaron Lipman. SAL: Sign agnostic learning of shapes from raw data. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2565-2574, 2020.
* [4] Matan Atzmon and Yaron Lipman. SALD: Sign agnostic learning with derivatives. _arXiv preprint arXiv:2006.05400_, 2020.
* [5] Alexander G. Belyaev and Pierre-Alain Fayolle. On variational and PDE-based distance function approximations. In _Computer Graphics Forum_, volume 34, pages 104-118. Wiley Online Library, 2015.
* [6] Yizhak Ben-Shabat, Chamin Hewa Koneputugodage, and Stephen Gould. DiGS: Divergence guided shape implicit neural representation for unoriented point clouds. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 19301-19310, 2022.
* [7] Matthew Berger, Joshua A Levine, Luis Gustavo Nonato, Gabriel Taubin, and Claudio T Silva. A benchmark for surface reconstruction. _ACM Transactions on Graphics (TOG)_, 32(2):1-17, 2013.
* [8] Matthew Berger, Andrea Tagliasacchi, Lee M. Seversky, Pierre Alliez, Gael Guennebaud, Joshua A Levine, Andrei Sharf, and Claudio T Silva. A survey of surface reconstruction from point clouds. In _Computer graphics forum_, volume 36, pages 301-329. Wiley Online Library, 2017.

Figure 7: Surface reconstruction of anchor data with various \(p\). The results show the importance of using a sufficiently large \(p\) for an accurate approximation.

- State of the Art Reports_. The Eurographics Association, 2014.
* [10] Fausto Bernardini, Joshua Mittleman, Holly Rushmeier, Claudio Silva, and Gabriel Taubin. The ball-pivoting algorithm for surface reconstruction. _IEEE transactions on visualization and computer graphics_, 5(4):349-359, 1999.
* [11] Tilak Bhattacharya, Emmanuele DiBenedetto, and Juan Manfredi. Limits as \(p\rightarrow\infty\) of \(\triangle_{p}u_{p}=f\) and related external problems. _Rendiconti del Seminario Matematico Universita e Politecnico di Torino_, 47:15-68, 1989.
* [12] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimization and statistical learning via the alternating direction method of multipliers. _Foundations and Trends(r) in Machine learning_, 3(1):1-122, 2011.
* [13] Stephen Boyd and Lieven Vandenberghe. _Convex optimization_. Cambridge University Press, 2004.
* [14] Fatih Calakli and Gabriel Taubin. SSD: Smooth signed distance surface reconstruction. In _Computer Graphics Forum_, volume 30, pages 1993-2002. Wiley Online Library, 2011.
* [15] Vicent Caselles, Gloria Haro, Guillermo Sapiro, and Joan Verdera. On geometric variational models for inpainting surface holes. _Computer Vision and Image Understanding_, 111(3):351-373, 2008.
* [16] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5939-5948, 2019.
* [17] Alexander G. Churbanov and Petr N. Vabishchevich. Numerical solution of boundary value problems for the eikonal equation in an anisotropic medium. _Journal of Computational and Applied Mathematics_, 362:55-67, 2019.
* [18] Tamal K. Dey, Gang Li, and Jian Sun. Normal estimation for point clouds: A comparison study for a voronoi based method. In _Proceedings Eurographics/IEEE VGTC Symposium Point-Based Graphics_, pages 39-46. IEEE, 2005.
* [19] Philipp Erler, Paul Guerrero, Stefan Ohrhallinger, Niloy J Mitra, and Michael Wimmer. Points2surf learning implicit surfaces from point clouds. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V_, pages 108-124. Springer, 2020.
* [20] Pierre-Alain Fayolle. Signed distance function computation from an implicit surface. _arXiv preprint arXiv:2104.08057_, 2021.
* [21] Henry Fuchs, Zvi M Kedem, and Samuel P Uselton. Optimal surface reconstruction from planar contours. _Communications of the ACM_, 20(10):693-702, 1977.
* [22] Tom Goldstein and Stanley Osher. The split Bregman method for L1-regularized problems. _SIAM journal on imaging sciences_, 2(2):323-343, 2009.
* [23] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization for learning shapes. ICML'20, page 11. JMLR.org, 2020.
* [24] Jooyoung Hahn, Karol Mikula, and Peter Frolkovic. Laplacian regularized eikonal equation with Soner boundary condition on polyhedral meshes. _arXiv preprint arXiv:2301.11656_, 2023.
* [25] Jooyoung Hahn, Jie Qiu, Eiji Sugisaki, Lei Jia, Xue-Cheng Tai, and Hock Soon Seah. Stroke-based surface reconstruction. _Numerical Mathematics: Theory, Methods and Applications_, 6:297-324, 2013.

* [26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In _Proceedings of the IEEE international conference on computer vision_, pages 1026-1034, 2015.
* [27] Yuchen He, Sung Ha Kang, and Hao Liu. Curvature regularized surface reconstruction from point clouds. _SIAM Journal on Imaging Sciences_, 13(4):1834-1859, 2020.
* [28] Hugues Hoppe, Tony DeRose, Tom Duchamp, John McDonald, and Werner Stuetzle. Surface reconstruction from unorganized points. In _Proceedings of the 19th annual conference on computer graphics and interactive techniques_, pages 71-78, 1992.
* [29] Hui Huang, Dan Li, Hao Zhang, Uri Ascher, and Daniel Cohen-Or. Consolidation of unorganized point clouds for surface reconstruction. _ACM transactions on graphics (TOG)_, 28(5):1-7, 2009.
* [30] Bernhard Kawohl. On a family of torsional creep problems. _Journal fur die reine und angewandte Mathematik_, 410:1-22, 1990.
* [31] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe. Poisson surface reconstruction. In _Proceedings of the fourth Eurographics symposium on Geometry processing_, volume 7, 2006.
* [32] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [33] Leo Koenigsberger. _Hermann von Helmholtz_. Clarendon pPress, 1906.
* [34] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks. _Advances in Neural Information Processing Systems_, 34:26548-26560, 2021.
* [35] Marc Levoy, Kari Pulli, Brian Curless, Szymon Rusinkiewicz, David Koller, Lucas Pereira, Matt Ginzton, Sean Anderson, James Davis, Jeremy Ginsberg, Jonathan Shade, and Duane Fulk. The digital Michelangelo project: 3D scanning of large statues. In _Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques_, SIGGRAPH '00, pages 131-144, USA, 2000. ACM Press/Addison-Wesley Publishing Co.
* [36] Peter Lindqvist. _Notes on the p-Laplace equation_. Number 161. University of Jyvaskyla, 2017.
* [37] Yaron Lipman. Phase transitions, distance functions, and implicit neural representations. In _International Conference on Machine Learning_, 2021.
* [38] William E. Lorensen and Harvey E. Cline. Marching cubes: A high resolution 3D surface construction algorithm. _ACM SIGGRAPH computer graphics_, 21(4):163-169, 1987.
* [39] Baorui Ma, Zhizhong Han, Yu-Shen Liu, and Matthias Zwicker. Neural-Pull: Learning signed distance functions from point clouds by learning to pull space onto surfaces. In _International Conference on Machine Learning_, 2020.
* [40] Zoltan Csaba Marton, Radu Bogdan Rusu, and Michael Beetz. On fast surface reconstruction methods for large and noisy point clouds. In _2009 IEEE international conference on robotics and automation_, pages 3218-3223. IEEE, 2009.
* [41] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3D reconstruction in function space. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4460-4470, 2019.
* [42] Mateusz Michalkiewicz, Jhony K Pontes, Dominic Jack, Mahsa Baktashmotlagh, and Anders Eriksson. Deep Level Sets: Implicit surface representations for 3D shape inference. _CoRR_, 2019.
* [43] Carsten Moenning and Neil A. Dodgson. Fast marching farthest point sampling for implicit surfaces and point clouds. _Computer Laboratory Technical Report_, 565:1-12, 2003.

* [44] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 165-174, 2019.
* [45] Yesom Park, Chang Hoon Song, Jooyoung Hahn, and Myungjoo Kang. ReSDF: Redistancing implicit surfaces using neural networks. _arXiv preprint arXiv:2305.08174_, 2023.
* [46] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
* [47] Donald W. Peaceman and Henry H. Rachford, Jr. The numerical solution of parabolic and elliptic differential equations. _Journal of the Society for Industrial and Applied Mathematics_, 3(1):28-41, 1955.
* [48] Songyou Peng, Chiyu Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, and Andreas Geiger. Shape As Points: A differentiable Poisson solver. _Advances in Neural Information Processing Systems_, 34:13032-13044, 2021.
* [49] Albert Pumarola, Artsiom Sanakoyeu, Lior Yariv, Ali K. Thabet, and Yaron Lipman. VisCo grids: Surface reconstruction with viscosity and coarea grids. In _NeurIPS_, 2022.
* [50] Fu Qiancheng, Xu Qingshan, Ong Yew-Soon, and Tao Wenbing. Geo-Neus: Geometry-consistent neural implicit surfaces learning for multi-view reconstruction. _Advances in Neural Information Processing Systems_, 35:3403-3416, 2022.
* [51] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. _Journal of Computational physics_, 378:686-707, 2019.
* [52] Jack Richter-Powell, Yaron Lipman, and Ricky T. Q. Chen. Neural conservation laws: A divergence-free perspective. In _Advances in Neural Information Processing Systems_, 2022.
* [53] Liu Shaohui, Zhang Yinda, u Peng Songyo, Shi Boxin, Pollefeys Marc, and Cui. Zhaopeng. DIST: Rendering deep implicit signed distance function with differentiable sphere tracing. _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2019-2028, 2020.
* [54] Vincent Sitzmann, Eric R. Chan, Richard Tucker, Noah Snavely, and Gordon Wetzstein. MetaSDF: Meta-learning signed distance functions. _Advances in Neural Information Processing Systems_, 2020.
* [55] Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, number 626 in NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc.
* [56] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. _Advances in Neural Information Processing Systems_, 33:7537-7547, 2020.
* [57] J. Van Bladel. On Helmholtz's theorem in finite regions. Technical report, CM-P00066539, 1958.
* [58] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normalized object coordinate space for category-level 6D object pose and size estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2642-2651, 2019.
* [59] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow pathologies in physics-informed neural networks. _SIAM Journal on Scientific Computing_, 43(5):A3055-A3081, 2021.

* [60] Yilun Wang, Junfeng Yang, Wotao Yin, and Yin Zhang. A new alternating minimization algorithm for total variation image reconstruction. _SIAM Journal on Imaging Sciences_, 1(3):248-272, 2008.
* [61] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich Neumann. DISN: Deep implicit surface network for high-quality single-view 3D reconstruction. _Advances in neural information processing systems_, 32, 2019.
* [62] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. _Advances in Neural Information Processing Systems_, 33:2492-2502, 2020.
* [63] Jingyang Zhang, Yao Yao, Shiwei Li, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. Critical regularizations for neural surface reconstruction in the wild. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6270-6279, 2022.
* [64] Hong-Kai Zhao, Stanley Osher, and Ronald Fedkiw. Fast surface reconstruction using the level set method. In _Proceedings IEEE Workshop on Variational and Level Set Methods in Computer Vision_, pages 194-201. IEEE, 2001.
* [65] Qingnan Zhou and Alec Jacobson. Thingi10k: A dataset of 10,000 3D-printing models. _arXiv preprint arXiv:1605.04797_, 2016.

[MISSING_PAGE_FAIL:15]

### Experimental Setup

Parameter TuningThe proposed training loss \(\mathcal{L}_{\text{total}}\) (13) is a weighted sum of five loss terms with four regularization parameters \(\lambda_{1},\lambda_{2},\lambda_{3}\), and \(\lambda_{4}\). In all surface reconstruction experiments, we use \(\lambda_{1}=0.1,\ \lambda_{2}=0.0001,\ \lambda_{3}=0.0005\), and \(\lambda_{4}=0.1\). In the proposed model, \(p\) is also a hyperparameter to be chosen. Considering the theoretical fact that \(p\) should be infinitely large and numerical simplicity, we set \(p=\infty\). We empirically confirm no significant difference between when \(p=100\) and when \(p=\infty\). Moreover, we set the smoothing parameter \(\epsilon=1\) for approximating Dirac delta in (13).

Network ArchitectureAs in previous studies [44; 23; 37], we represent the primary and auxiliary outputs by a single 8-layered multi-layer perceptron (MLP) \(\mathbb{R}^{3}\rightarrow\mathbb{R}^{7}\) with 512 neurons and a skip connection to the fourth layer, but only the output dimension of the last layer is increased by six due to the two auxiliary variables; see Figure 2. We use softplus activation function \(\alpha\left(x\right)=\frac{1}{\beta}\ln\left(1+e^{\beta x}\right)\) with \(\beta=100\). Network weights are initialized by the geometric initialization proposed in [3].

Training detailsThe gradient and the curl of networks are computed with auto-differentiation library (autograd) [46]. In all experiments, we use the Adam optimizer [32] with learning rate \(10^{-3}\) decayed by \(0.99\) every \(2{,}000\) iterations. At each iteration, we uniform randomly sample \(16{,}384\) points \(\mathbf{x}\in\mathcal{X}\) from the point cloud \(\mathcal{X}\). We sample the collocation points of \(\Omega\) as provided in [23]. The collocation points consist of global points and local points. The local collocation points are sampled by perturbing each of the \(16{,}384\) points drawn from the point cloud with a zero mean Gaussian distribution with a standard deviation equal to the distance to the \(50\)th nearest neighbor. The global collocation points are made up of approximately \(2{,}000\) points from the uniform distribution \(U\left(-\eta,\eta\right)\) with \(\eta=1.1\). \(F=\frac{1}{3}\mathbf{x}\) is utilized in all experiments.

Figure 8: The trained results of a cross section cut in planes \(x=0.2\) (left) and \(x=0.4\) (right). The level-sets show the signed distance fields \(u\) learned by the proposed model with (top) and without (bottom) the curl-free term. Dashed contours depict the learned zero level set. Quivers represent the vector field of the trained auxiliary variable \(G\) and the true gradient fields are plotted in red arrows.

Baseline modelsFor baseline models on the Thingi10K dataset, we use the official codes of IGR 2 (MIT License), SIREN3 (MIT License), and DiGS 4 (MIT License). We faithfully follow the official implementation to train each model without normal prior. For the variable splitting representation of the eikonal equation (9), there is a single auxiliary output. Consequently, we use the same 8 layer MLP with 512 nodes, but a network with an output dimension of 4. We normalize the auxiliary output to make it a unit norm, and use the normalized one to represent \(H\).

Footnote 3: https://github.com/vsitzmann/siren

Footnote 4: https://github.com/Chumbyte/DiGS

### Evaluation

MetricsWe measure the distance between two point clouds \(\mathcal{X}\) and \(\mathcal{Y}\) by using the standard one-sided and double-sided \(\ell_{1}\) Chamfer distances \(d_{\overrightarrow{C}},\,d_{C}\) and Hausdorff distances \(d_{\overrightarrow{H}},\,d_{H}\). Each are defined as follows:

\[d_{\overrightarrow{C}}\left(\mathcal{X},\mathcal{Y}\right) =\frac{1}{\left|\mathcal{X}\right|}\sum_{\mathbf{x}\in\mathcal{ X}}\min_{\mathbf{y}\in\mathcal{Y}}\left\|\mathbf{x}-\mathbf{y}\right\|_{2},\] \[d_{C}\left(\mathcal{X},\mathcal{Y}\right) =\frac{1}{2}\left(d_{\overrightarrow{C}}\left(\mathcal{X}, \mathcal{Y}\right)+d_{\overrightarrow{C}}\left(\mathcal{Y},\mathcal{X} \right)\right),\] \[d_{\overrightarrow{H}}\left(\mathcal{X},\mathcal{Y}\right) =\max_{\mathbf{x}\in\mathcal{X}}\min_{\mathbf{y}\in\mathcal{Y}} \left\|\mathbf{x}-\mathbf{y}\right\|_{2},\] \[d_{H}\left(\mathcal{X},\mathcal{Y}\right) =\max\left\{d_{\overrightarrow{H}}\left(\mathcal{X},\mathcal{Y} \right)+d_{\overrightarrow{H}}\left(\mathcal{Y},\mathcal{X}\right)\right\}.\]

When we estimate the distance from a surface, we sample \(10M\) uniformly random points from the surface and then measure the distance from the sampled point clouds by the metrics defined above.

Furthermore, in order to measure the accuracy of the trained gradient field, we evaluate Normal Consistency (NC) [41] between the learned \(G\) and the surface normal as follows: from given an oriented point cloud \(\mathcal{X}=\left\{\mathbf{x}_{i},\mathbf{n}_{i}\right\}_{i=1}^{N}\) comprising of sampled points \(\mathbf{x}_{i}\) and the corresponding outward normal vectors \(\mathbf{n}_{i}\), NC is defined by

\[NC\left(G,\mathbf{n}\right)=\frac{1}{N}\sum_{i=1}^{N}\left|G\left(\mathbf{x}_{ i}\right)^{\mathrm{T}}\mathbf{n}_{i}\right|,\] (17)

the average of the absolute dot product of the trained \(G\) and the surface normals.

Level set extractionWe extract the zero level set of a trained neural network \(u\) by using the classical marching cubes meshing algorithm [38] on a \(512\times 512\times 512\) uniform grid.

## Appendix C Additional Results

### Uniqueness of the solution of \(p\)-Poisson equation

In this section, we provide a numerical example supporting the strength of the proposed model regarding the uniqueness of the solution to the \(p\)-Poisson equation. The given points are located on a cube center at the origin with an edge of the length \(1\). We consider IGR [23] as an eikonal-rooted baseline, and we train IGR and the proposed PINC with the following three different network initializations: The geometric initialization [23] that IGR originally used and the Kaiming uniform initialization [26] with two different random seeds. The results are summarized in Figure 9. The results show that IGR converges to different solutions depending on the model initializations. In particular, IGR fails to learn the SDF of the cube except for the geometric initialization. On the other hand, the results of PINC with the same initializations show that the proposed model converges to the SDF in all three cases. The numerical results of the chosen example show that the proposed method can pursue the unique solution of the PDE.

### Additional comparison with models utilizing surface normals

In Section 4, we made a comparison with models that do not use the surface normal \(\mathbf{n}\) as a supervision. Here, we additionally consider a comparison with models that leverage normal supervision. Weconsider three baseline models as follows: (i) IGR that evaluates using the surface normal, (ii) VisCo [49], a grid-based method based on the viscosity regularized eikonal equation, and (iii) Shape As Points (SAP) [48], a model that revisits the classical Poisson Surface Reconstruction (PSR) [31] using deep learning. The results are reported in Table 4. We can see that the proposed model performs on par with baselines, despite not utilizing the surface normal. Considering that all of the baselines are PDE-based INR models, the results exhibit the effectiveness of the proposed model (11) in reconstructing a surface from the sole use of raw point clouds.

It is worth note that the proposed model may be interpreted as PSR because of (8). More precisely, the Euler-Lagrange equation of (8) says that the variational problem (8) for finding a scalar function \(u\) whose gradient best approximates a given vector field \(G\) transforms into the following Poisson problem:

\[\begin{cases}\triangle u=\nabla\cdot G&\text{in }\Omega\\ u=0&\text{on }\Gamma.\end{cases}\] (18)

In the conventional PSR, the gradient field is set to surface normals. Thus, the auxiliary variable \(G\) can be regarded as playing a role of surface normals for PSR. However, the vector field \(G\) in the proposed model is not obtained from the oriented point cloud, but the learnable function that is trained with \(u\) at the same time. Moreover, since we bake the \(p\)-Poisson equation into \(G\) as a hard constraint in (7), we obtain a continuous SDF rather than an indicator function like PSR and SAP. The results confirm that simultaneous training of the gradient field and the SDF, that is, the variable

\begin{table}
\begin{tabular}{c|c c c c c|c c c c c|c c c c c|c c c c c} \hline \multirow{3}{*}{} & \multicolumn{4}{c|}{Anchor} & \multicolumn{4}{c|}{DACT} & \multicolumn{4}{c}{DACT} & \multicolumn{4}{c}{Gargolyte} & \multicolumn{4}{c}{Load Quals} \\  & \multicolumn{2}{c}{GT} & \multicolumn{2}{c}{Scans} & \multicolumn{2}{c}{GT} & \multicolumn{2}{c}{Scans} & \multicolumn{2}{c}{GT} & \multicolumn{2}{c}{Scans} & \multicolumn{2}{c}{GT} & \multicolumn{2}{c}{Scans} & \multicolumn{2}{c}{GT} & \multicolumn{2}{c}{Scans} & \multicolumn{2}{c}{GT} & \multicolumn{2}{c}{Scans} \\  & Model & \(d_{C}\) & \(d_{H}\) & \(d_{\overline{x}}\) & \(d_{\overline{y}}\) & \(d_{C}\) & \(d_{H}\) & \(d_{\overline{x}}\) & \(d_{\overline{y}}\) & \(d_{C}\) & \(d_{H}\) & \(d_{\overline{x}}\) & \(d_{\overline{y}}\) & \(d_{C}\) & \(d_{H}\) & \(d_{\overline{x}}\) & \(d_{\overline{y}}\) & \(d_{C}\) & \(d_{H}\) & \(d_{\overline{x}}\) & \(d_{\overline{y}}\) \\ \hline \multirow{3}{*}{w/n} & VisC & 0.21 & 3.00 & 0.15 & 1.07 & 0.26 & 4.06 & 0.14 & 1.76 & 0.15 & 2.22 & 0.09 & 2.76 & 0.17 & 4.40 & 0.11 & 0.96 & 0.12 & 1.06 & 0.7 & 0.64 \\  & IGR & 0.22 & 4.71 & 0.12 & 1.32 & 0.25 & 4.01 & 0.08 & 1.59 & 0.17 & 2.22 & 0.09 & 2.61 & 0.16 & 3.52 & 0.06 & 0.81 & 0.12 & 1.17 & 0.07 & 0.98 \\  & SAP & 0.34 & 8.83 & 0.09 & 2.93 & 0.22 & 3.09 & 0.08 & 1.66 & 0.17 & 3.30 & 0.04 & 2.23 & 0.18 & 5.54 & 0.05 & 1.73 & 0.13 & 3.49 & 0.04 & 1.17 \\ \hline w/n & **PINC** & 0.29 & 7.54 & 0.09 & 1.20 & 0.37 & 7.24 & 0.11 & 1.88 & 0.14 & 2.56 & 0.04 & 2.73 & 0.16 & 4.78 & 0.05 & 0.80 & 0.10 & 0.92 & 0.04 & 0.67 \\ \hline \end{tabular}
\end{table}
Table 4: Comparison with models that use the surface normal supervision \(\mathbf{n}\) on SRB. The proposed model PINC did not utilize the surface normal.

Figure 9: Experimental results show whether a method can find the SDF from different network initializations. IGR and PINC are trained on the synthetic cube data with three network initializations: geometric initialization (initialization 1) and Kaiming initialization with two different random seeds (initializations 2 and 3). Each depicts the trained level-set contours of a cross-section cut in the plane x = 0. Red contours depict the trained zero level set of numerical solutions.

[MISSING_PAGE_FAIL:19]

\begin{table}
\begin{tabular}{l c c c c|c c c c c} \hline \hline  & \multicolumn{4}{c}{Gargoyle} & \multicolumn{4}{c}{Anchor} \\  & \multicolumn{2}{c}{GT} & \multicolumn{2}{c}{Scans} & \multicolumn{2}{c}{GT} & \multicolumn{2}{c}{Scans} \\ Model & \(d_{C}\) & \(d_{H}\) & \(d_{\overrightarrow{C}}\) & \(d_{\overrightarrow{H}}\) & \(d_{C}\) & \(d_{H}\) & \(d_{\overrightarrow{C}}\) & \(d_{\overrightarrow{H}}\) \\ \hline \(p=2\) & 3.96 & 43.13 & 0.51 & 6.31 & 4.32 & 46.66 & 0.77 & 14.58 \\ \(p=10\) & 0.22 & 8.14 & 0.10 & 1.19 & 0.50 & 7.24 & 0.12 & 3.02 \\ \(p=100\) & 0.17 & 4.90 & 0.10 & 0.82 & 0.31 & 7.20 & 0.13 & 1.80 \\ \(p=\infty\) & 0.16 & 4.78 & 0.05 & 0.80 & 0.29 & 7.19 & 0.11 & 1.17 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Quantitative results on the ablation study of \(p\).

Figure 11: MSEs of \(u_{p}\) and \(u_{\infty}\) over different \(p\).

Figure 10: Quality of surface reconstruction with varying \(p\) from \(p=2\) to \(p=\infty\).

### Additional qualitative results

Figure 12 provides additional qualitative results of surface reconstruction on SRB and Thingi10K discussed in Section 4.1.

Reconstruction of large point cloudsWe further provide qualitative results for surface reconstruction from large models taken from Thingi10K. The adopted point clouds consist of from 35K to 980K vertices. Figure 13 depicts the qualitative reconstruction results of PINC on these large point clouds. The model is trained with the same configuration used in Section 4.1.

### Training/Inference time

To investigate the computational time of the proposed model, We carefully measured average execution time compared to baselines. In the Table 9, we report the average training time per iteration and inference time at a resolution of \(32^{3}\) voxels. The proposed model requires more computational cost than baseline models because of the computation on curl using automatic differentiation.

Figure 12: Additional qualitative results of the surface reconstruction on SRB and Thingi10K datasets.

Figure 13: Reconstructed surfaces of large point clouds from Thingi10K.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model & IGR & SIREN & DiGS & **PINC** \\ \hline Training time (ms/iteration) & 48.34 & 13.11 & 52.34 & 295.01 \\ Inference time (ms) & 6.86 & 3.51 & 4.39 & 6.93 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Training and inference times for surface reconstruction on SRB.