# Mixture of Experts Meets

Prompt-Based Continual Learning

 Minh Le\({}^{3}\)  An Nguyen\({}^{2}\)  Huy Nguyen\({}^{1}\)  Trang Nguyen\({}^{3}\)

**Trang Pham\({}^{3}\)**

Equal contribution.

Linh Van Ngo\({}^{2}\)  Nhat Ho\({}^{1}\)

\({}^{1}\) The University of Texas at Austin

\({}^{2}\) Hanoi University of Science and Technology

\({}^{3}\) VinAI Research

###### Abstract

Exploiting the power of pre-trained models, prompt-based approaches stand out compared to other continual learning solutions in effectively preventing catastrophic forgetting, even with very few learnable parameters and without the need for a memory buffer. While existing prompt-based continual learning methods excel in leveraging prompts for state-of-the-art performance, they often lack a theoretical explanation for the effectiveness of prompting. This paper conducts a theoretical analysis to unravel how prompts bestow such advantages in continual learning, thus offering a new perspective on prompt design. We first show that the attention block of pre-trained models like Vision Transformers inherently encodes a special mixture of experts architecture, characterized by linear experts and quadratic gating score functions. This realization drives us to provide a novel view on prefix tuning, reframing it as the addition of new task-specific experts, thereby inspiring the design of a novel gating mechanism termed Non-linear Residual Gates (NoRGa). Through the incorporation of non-linear activation and residual connection, NoRGa enhances continual learning performance while preserving parameter efficiency. The effectiveness of NoRGa is substantiated both theoretically and empirically across diverse benchmarks and pretraining paradigms. Our code is publicly available at https://github.com/Minhchuyentoanchon/MoE_PromptCL.

## 1 Introduction

Humans possess a remarkable ability to learn continuously by integrating new skills and knowledge while retaining past experiences. However, current AI models often fail to retain this ability. Unlike humans, they often suffer from _catastrophic forgetting_[28, 30, 32, 38], a phenomenon where they struggle to retain knowledge from previous tasks while learning new ones. Inspired by human learning, Continual Learning [2, 28, 43, 1, 12] is an ongoing field that aims to train a model across a sequence of tasks while mitigating this challenge. Traditional continual learning methods often rely on storing past data for fine-tuning, which can raise concerns about memory usage and privacy [5, 39, 51]. To address these limitations, prompt-based approaches have emerged as a promising alternative within rehearsal-free continual learning. By attaching prompts - small sets of learnable parameters - to a frozen pre-trained model, these approaches enable efficient adaptation to new tasks with minimal modifications to the underlying model [56, 26, 61]. The effectiveness of prompt-based methods has been demonstrated by several recent works achieving state-of-the-art performance on various continual learning benchmarks [49, 53, 54].

While prompt-based methods have demonstrably achieved impressive results, their emphasis largely lies on prompt utility, leaving a gap in our theoretical comprehension of their effectiveness. Thisabsence of a theoretical foundation hinders our ability to further refine and optimize these methods. In this work, we offer a new perspective by focusing on prefix tuning [26] and its connection to mixture of experts models [19; 15; 13; 11]. We demonstrate that self-attention blocks in Vision Transformers [8] implicitly encode a special mixture of experts architecture, revealing a surprising connection between these seemingly disparate concepts. Leveraging this connection, we propose that applying prefix tuning within pre-trained models can be interpreted as introducing new experts. The newly introduced experts collaborate with the pre-trained experts, facilitating efficient adaptation of the model to new tasks.

Drawing insights from this analysis, we observe that the original prefix tuning suffers from suboptimal sample efficiency, requiring a substantial amount of data for reasonable parameter estimation. To address this challenge, we propose a novel gating mechanism termed Non-linear Residual Gates (NoRGa). This architecture integrates non-linear activation functions and residual connections within the gating score functions. Our work focuses on improving within-task prediction accuracy, a key component of continual learning performance as identified in previous research [22; 49]. We posit that NoRGa can enhance this aspect, which contributes to improved overall continual learning performance while maintaining parameter efficiency. We further provide theoretical justification for this improvement, demonstrating how NoRGa accelerates parameter estimation rates.

**Our contributions** can be summarized as follows: (1) We reveal a novel connection between self-attention and a mixture of experts, providing a fresh perspective on prompt-based continual learning approaches; (2) Leveraging this insight, we propose _Non-linear Residual Gates (NoRGa)_, an innovative gating mechanism that enhances continual learning performance while maintaining parameter efficiency, and provide a theoretical justification for this improvement; (3) Extensive experiments across various continual learning benchmarks and pre-training settings demonstrate that our approach achieves state-of-the-art performance compared to existing methods.

**Notation.** For any \(n\in\mathbb{N}\), we denote \([n]\) as the set \(\{1,2,\ldots,n\}\). Next, for any set \(S\), we let \(|S|\) stand for its cardinality. For any vector \(u:=(u_{1},u_{2},\ldots,u_{d})\in\mathbb{R}^{d}\) and \(\alpha:=(\alpha_{1},\alpha_{2},\ldots,\alpha_{d})\in\mathbb{N}^{d}\), we let \(u^{\alpha}=u_{1}^{\alpha_{1}}u_{2}^{\alpha_{2}}\ldots u_{d}^{\alpha_{d}}\), \(|u|:=u_{1}+u_{2}+\ldots+u_{d}\) and \(\alpha!:=\alpha_{1}!\alpha_{2}!\ldots\alpha_{d}!\), while \(\|u\|\) stands for its \(2\)-norm value. Lastly, for any two positive sequences \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\), we write \(a_{n}=\mathcal{O}(b_{n})\) or \(a_{n}\lesssim b_{n}\) if \(a_{n}\leq Cb_{n}\) for all \(n\in\mathbb{N},\) where \(C>0\) is some universal constant. The notation \(a_{n}=\mathcal{O}_{P}(b_{n})\) indicates that \(a_{n}/b_{n}\) is stochastically bounded.

## 2 Background and Related Works

We first provide background and related works on continual learning. Then, we define the attention mechanism, followed by discussions on prompt-based continual learning and mixture of experts.

**Continual Learning (CL)** addresses the challenge of training a model incrementally on a sequence of \(T\) tasks, denoted by \(\mathcal{D}=\{\mathcal{D}_{1},...,\mathcal{D}_{T}\}\). Each task's training data \(\mathcal{D}_{t}=\{(\bm{x}_{i}^{(t)},y_{i}^{(t)})\}_{i=1}^{N_{t}}\) contains pairs of input sample \(\bm{x}_{i}^{(t)}\in\mathcal{X}^{(t)}\), and corresponding label \(y_{i}^{(t)}\in\mathcal{Y}^{(t)}\). Notably, the class labels are distinct for each task, _i.e._, \(\mathcal{Y}^{(t)}\bigcap\mathcal{Y}^{(t^{\prime})}=\varnothing,\forall t\neq t^ {\prime}\). Consider a neural network with a backbone function \(f_{\theta}\) and an output layer \(h_{\psi}\). The model predicts a label \(\hat{y}=h_{\psi}(f_{\theta}(\bm{x}))\in\mathcal{Y}=\bigcup_{t=1}^{T}\mathcal{Y} ^{(t)}\), where \(\bm{x}\in\mathcal{X}=\bigcup_{t=1}^{T}\mathcal{X}^{(t)}\) is an unseen test sample from arbitrary tasks. Importantly, during training on a new task, the model can only access the current data, without access to data from previous tasks. Prior approaches often rely on storing past task samples for training on new tasks, raising concerns regarding storage and privacy [5; 6; 39; 51; 59].

Our work focuses on the class-incremental learning (CIL) setting, where task identities are not provided during inference, unlike in task-incremental learning (TIL) [46]. A recent theory by [22] analyzes the CIL objective by decomposing the probability of a test sample \(\bm{x}\) of the \(j\)-th class in task \(t\) into two probabilities:

\[P(\bm{x}\in\mathcal{X}_{j}^{(t)}|\mathcal{D})=P(\bm{x}\in\mathcal{X}_{j}^{(t)} |\bm{x}\in\mathcal{X}^{(t)},\mathcal{D})P(\bm{x}\in\mathcal{X}^{(t)}|\mathcal{ D}),\] (1)

where the first term involves within-task prediction (WTP) and the second term pertains to task-identity inference (TII). This equation highlights that by improving either the WTP performance or the TII, we can consequently improve the overall CIL performance, as shown in [22; 49].

**Attention Mechanism.** Within the Transformer architecture, the attention mechanism plays a crucial role. One prevalent variant is scaled dot-product attention[47], formally defined as follows:

**Definition 2.1** (Scaled Dot-Product Attention).: Let \(\bm{K}\in\mathbb{R}^{N\times d_{v}}\) be a _key_ matrix with \(N\) key vectors, and \(\bm{V}\in\mathbb{R}^{N\times d_{v}}\) be a _value_ matrix with \(N\) corresponding value vectors. Given a _query_ matrix \(\bm{Q}\in\mathbb{R}^{M\times d_{k}}\), _Attention_ over \((\bm{K},\bm{V})\) is defined as

\[\mathrm{Attention}(\bm{Q},\bm{K},\bm{V})=\mathrm{softmax}(\frac{\bm{Q}\bm{K} ^{\top}}{\sqrt{d_{k}}})\bm{V}\] (2)

where the softmax function acts on the rows of matrix \(\bm{Q}\bm{K}^{\top}\in\mathbb{R}^{M\times N}\).

Vision Transformer (ViT) [8] employs the same attention mechanism within multiple Multi-head Self-Attention (MSA) layers, which is formally defined as follows:

**Definition 2.2** (Multi-head Self-Attention Layer).: Let \(\bm{X}^{Q},\bm{X}^{K},\bm{X}^{V}\) denote the input query, key, and value matrix, respectively, where \(\bm{X}^{Q}=\bm{X}^{K}=\bm{X}^{V}=[\bm{x}_{1},...,\bm{x}_{N}]^{\top}\in\mathbb{ R}^{N\times d}\), and \(N\) is the length of the input sequence. The output is expressed as

\[\mathrm{MSA}(\bm{X}^{Q},\bm{X}^{K},\bm{X}^{V}):=\mathrm{Concat}(\bm{h}_{1},...,\bm{h}_{m})W^{O}\in\mathbb{R}^{N\times d},\] (3)

\[\bm{h}_{i}:=\mathrm{Attention}(\bm{X}^{Q}W_{i}^{Q},\bm{X}^{K}W_{i}^{K},\bm{X}^ {V}W_{i}^{V}),\ i\in[m].\] (4)

where \(W^{O}\in\mathbb{R}^{md_{v}\times d},W_{i}^{Q}\in\mathbb{R}^{d\times d_{k}}\), \(W_{i}^{K}\in\mathbb{R}^{d\times d_{k}}\), and \(W_{i}^{V}\in\mathbb{R}^{d\times d_{v}}\) are projection matrices, and \(m\) is the number of heads in the MSA layer. In ViTs, they use \(d_{k}=d_{v}=d/m\).

**Prompt-based continual learning.** Prompt-based approaches have emerged as a promising alternative within rehearsal-free continual learning [61, 52, 44]. In vision tasks, prompt-based methods often leverage a pre-trained ViT as a feature extractor \(f_{\theta}\), with its parameters \(\theta\) typically frozen. These methods enhance the model by introducing _prompts_, small sets of learnable parameters that influence the operations of the MSA layer [53]. Prompts are strategically injected into the query, key, and value matrices to guide the ViT in learning new tasks. We denote the prompt parameters by \(\bm{p}\in\mathbb{R}^{L_{p}\times d}\), where \(L_{p}\) is the sequence length and \(d\) is the embedding dimension. Previous work [53] outlines two main prompt-based approaches: Prompt Tuning (ProT) [25] and Prefix Tuning (PreT) [26]. While Prompt Tuning directly concatenates the same prompt parameter \(\bm{p}\) to the query, key, and value, prefix tuning divides \(\bm{p}\) into prefixes \(\{\bm{p}^{K},\bm{p}^{V}\}\in\mathbb{R}^{\frac{L_{p}}{2}\times d}\) and appends it to the key and value vectors:

\[f_{\mathrm{prompt}}^{\mathrm{Pre-T}}(\bm{p},\bm{X}^{Q},\bm{X}^{K},\bm{X}^{V}):= \mathrm{MSA}\left(\bm{X}^{Q},\begin{bmatrix}\bm{P}^{K}\\ \bm{X}^{K}\end{bmatrix},\begin{bmatrix}\bm{p}^{V}\\ \bm{X}^{V}\end{bmatrix}\right)=\mathrm{Concat}(\tilde{\bm{h}}_{1},...,\tilde{ \bm{h}}_{m})W^{O}\] (5)

Existing prompt-based methods in CL address catastrophic forgetting by creating new adaptive prompts for each new task. During testing, the model chooses suitable prompt combinations to handle unseen data from any encountered task [49]. L2P [54] proposes a shared prompt pool for all tasks, utilizing a query-key mechanism for prompt selection. Instead of using the same prompt pool across tasks, DualPrompt [53] introduces G-Prompt and E-Prompt to capture task-agnostic and task-specific information, respectively. S-Prompt [52] focuses on learning task-specific prompts and employs a ProT strategy similar to L2P. CODA-Prompt [42] expands the prompt pool across tasks and performs a weighted summation of the prompt pool using attention factors. A recent work, HiDe-Prompt [49], achieves state-of-the-art performance by introducing a new hierarchical decomposition of CIL objectives and optimizing each component for better performance.

In this study, we focus on prefix tuning as our primary prompt-based methodology and follow the framework presented in HiDe-Prompt [49]. During training, HiDe-Prompt co-optimizes task-specific prompts \(\bm{p}_{t}\) and model's output layer parameters \(\psi\) for each new task \(t\) using the WTP objective. These prompts are stored within a prompt pool \(\mathbf{P}=\{\bm{p}_{1},...,\bm{p}_{T}\}\). At test time, a separate lightweight auxiliary output layer \(\hat{h}_{\omega}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{T}\), trained with the TII objective, takes the uninstructed representation \(f_{\theta}(x)\) of a new data point \(\bm{x}\) as input to infer the task identity, guiding the selection of the most suitable prompt \(\bm{p}_{k}\) from the prompt pool \(\mathbf{P}\). Subsequently, the final prediction is given as \(\hat{y}=h_{\psi}(f_{\theta}(\bm{x},\bm{p}_{k}))\). For further details, please refer to Appendix D.

**Mixture of experts (MoE)** extends classical mixture models with an adaptive gating mechanism [19, 21]. An MoE model consists of a group of \(N\) expert networks \(f_{i}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d_{v}}\), for all \(i\in[N]\), and a gate function \(G:\mathbb{R}^{d}\rightarrow\mathbb{R}^{N}\). Given an input \(\bm{h}\in\mathbb{R}^{d}\), MoE computes a weighted sum of expert outputs \(f_{i}(\bm{h})\) based on learned score function \(s_{i}:\mathbb{R}^{d}\rightarrow\mathbb{R}\) for each expert:

\[\mathbf{y}:=\sum_{j=1}^{N}G(\bm{h})_{j}\cdot f_{j}(\bm{h}):=\sum_{j=1}^{N} \frac{\exp{(s_{j}(\bm{h}))}}{\sum_{\ell=1}^{N}\exp{(s_{\ell}(\bm{h}))}}\cdot f _{j}(\bm{h}),\] (6)where \(G(\bm{h}):=\mathrm{softmax}(s_{1}(\bm{h}),\ldots,s_{N}(h))\). Building on this concept, works by [10; 41] established the MoE layer as a fundamental building block to scale up model capacity efficiently. Please refer to Appendix C for a comprehensive discussion of related works.

## 3 Connection between Prefix Tuning and Mixture of Experts

We first explore the relationship between attention and mixture of experts in Section 3.1, followed by establishing the connection between prefix tuning and the mixture of experts in Section 3.2.

### Mixture of Experts Meets Attention

Following the notation established in Definition 2.2, let's consider the \(l\)-th head within the MSA layer. Let \(\bm{X}=\left[\bm{x}_{1}^{\top},\ldots,\bm{x}_{N}^{\top}\right]^{\top}\in \mathbb{R}^{Nd}\), which is the concatenation of input sequence embeddings into a single one-dimensional vector. We define the matrix \(E_{i}\in\mathbb{R}^{d\times Nd}\) such that \(E_{i}\bm{X}:=\bm{x}_{i}\) for all \(i\in[N]\). Furthermore, we introduce an MoE architecture consisting of a group of \(N\) expert networks \(f_{j}:\mathbb{R}^{Nd}\rightarrow\mathbb{R}^{d_{v}}\), \(N\) gating functions \(G_{i}:\mathbb{R}^{Nd}\rightarrow\mathbb{R}^{N}\) with the score function for the \(j\)-th expert of the \(i\)-th gating \(s_{i,j}:\mathbb{R}^{Nd}\rightarrow\mathbb{R}\), where

\[f_{j}(\bm{X}):={W_{l}^{V}}^{\top}E_{j}\bm{X}={W_{l}^{V}}^{\top}\bm{x}_{j},\;s _{i,j}(\bm{X}):=\frac{\bm{X}^{\top}E_{i}^{\top}W_{l}^{Q}{W_{l}^{K}}^{\top}E_{ j}\bm{X}}{\sqrt{d_{v}}}=\frac{\bm{x}_{i}^{\top}W_{l}^{Q}{W_{l}^{K}}^{\top}\bm{x}_{j}} {\sqrt{d_{v}}}\]

for \(i\) and \(j\in[N]\). From equation (4), we can express the output of the \(l\)-th head as follows:

\[\bm{h}_{l}=\mathrm{softmax}\left(\frac{\bm{X}^{Q}{W_{l}^{Q}{W_{l} ^{K}}^{\top}\bm{X}^{K}}^{\top}}{\sqrt{d_{v}}}\right)\bm{X}^{V}{W_{l}^{V}}=\left[ \bm{h}_{l,1},\ldots,\bm{h}_{l,N}\right]^{\top}\in\mathbb{R}^{N\times d_{v}},\] (7) \[\bm{h}_{l,i}=\sum_{j=1}^{N}\frac{\exp\left(\frac{\bm{x}_{i}^{ \top}W_{l}^{Q}{W_{l}^{K}}^{\top}\bm{x}_{j}}{\sqrt{d_{v}}}\right)}{\sum_{k=1}^{ N}\exp\left(\frac{\bm{x}_{i}^{\top}W_{l}^{Q}{W_{k}^{K}}^{\top}\bm{x}_{k}}{ \sqrt{d_{v}}}\right)}{W_{l}^{V}}^{\top}\bm{x}_{j}=\sum_{j=1}^{N}\frac{\exp(s_{i,j}(\bm{X}))}{\sum_{k=1}^{N}\exp(s_{i,k}(\bm{X}))}f_{j}(\bm{X}),\] (8)

for \(i\in[N]\). Expanding on equation (8), we can discern that each attention head within the MSA layer implicitly embodies a special mixture of experts architecture. This architecture encompasses \(N\) MoE models, each featuring its own quadratic gating function \(G_{i}\). However, instead of employing

Figure 1: An illustrative depiction of the relationship between self-attention and MoE. Each output vector of a head in the MSA layer can be viewed as the output of a MoE model. These MoE models share the same set of experts encoded in the value matrix. Each entry in the attention matrix corresponds to a score function within this architecture.

separate expert networks for each model, this architecture utilizes \(N\) shared linear expert networks \(f_{j}\) for \(j\in[N]\), significantly reducing the number of parameters. Notably, each expert network and its corresponding gating function process the entire input sequence directly, rather than individual embedding \(\bm{x}_{i}\) as in traditional MoE layers [41]. This connection between self-attention and mixture of experts is depicted in Figure 1. In the subsequent section, we explore how prompt-based techniques can be viewed through this lens.

### Prefix Tuning via the Perspective of Mixture of Experts

Building on the connection between self-attention and mixture of experts, we propose that applying prefix tuning can be interpreted as the introduction of new experts to customize the pre-trained model for a specific task, as illustrated in Figure 2. Specifically, similar to Section 3.1, we consider the \(l\)-th head within the MSA layer. We denote \(\bm{p}^{K}=\left[\bm{p}^{K}_{1},\dots,\bm{p}^{K}_{L}\right]^{\top}\in\mathbb{R }^{L\times d}\), \(\bm{p}^{V}=\left[\bm{p}^{V}_{1},\dots,\bm{p}^{V}_{L}\right]^{\top}\in\mathbb{R }^{L\times d}\), where \(L=\frac{L_{p}}{2}\). We define new _prefix_ experts \(f_{N+j}:\mathbb{R}^{Nd}\rightarrow\mathbb{R}^{d_{u}}\) along with their corresponding new score functions \(s_{i,N+j}:\mathbb{R}^{Nd}\rightarrow\mathbb{R}\) as follows:

\[f_{N+j}(\bm{X}):={W_{l}^{V}}^{\top}\bm{p}^{V}_{j},\quad s_{i,N+j}(\bm{X}):= \frac{\bm{X}^{\top}E_{i}^{\top}W_{l}^{Q}{W_{l}^{K}}^{\top}\bm{p}^{K}_{j}}{\sqrt {d_{v}}}=\frac{\bm{x}_{i}^{\top}W_{l}^{Q}{W_{l}^{K}}^{\top}\bm{p}^{K}_{j}}{ \sqrt{d_{v}}}\] (9)

for \(i\in[N]\) and \(j\in[L]\). Then from equation (5), the output of the \(l\)-th head can be expressed as:

\[\tilde{\bm{h}}_{l}=\operatorname{Attention}\left(\bm{X}^{Q}W_{l}^ {Q},\left[\begin{matrix}\bm{p}^{K}\\ \bm{X}^{K}\end{matrix}\right]W_{l}^{K},\left[\begin{matrix}\bm{p}^{V}\\ \bm{X}^{V}\end{matrix}\right]W_{l}^{V}\right)=\left[\tilde{\bm{h}}_{l,1},\dots, \tilde{\bm{h}}_{l,N}\right]^{\top}\in\mathbb{R}^{N\times d_{v}},\] (10) \[\tilde{\bm{h}}_{l,i}=\sum_{j=1}^{N}\frac{\exp(s_{i,j}(\bm{X}))}{ \sum_{k=1}^{N}\exp(s_{i,k}(\bm{X}))+\sum_{k^{\prime}=1}^{L}\exp(s_{i,N+k^{ \prime}}(\bm{X}))}f_{j}(\bm{X})\] \[+\sum_{j^{\prime}=1}^{L}\frac{\exp(s_{i,N+j^{\prime}}(\bm{X}))}{ \sum_{k=1}^{N}\exp(s_{i,k}(\bm{X}))+\sum_{k^{\prime}=1}^{L}\exp(s_{i,N+k^{ \prime}}(\bm{X}))}f_{N+j^{\prime}}(\bm{X})\] (11)

It's worth noting that \(W_{l}^{Q}\), \(W_{l}^{K}\), and \(W_{l}^{V}\) remain fixed, with only \(\bm{p}^{K}\) and \(\bm{p}^{V}\) being learnable. By examining equation (8) and equation (11), we can interpret each head in a multi-head self-attention layer within a pre-trained model as a mixture of experts architecture with pre-trained experts \(f_{j}\) and gating score functions \(s_{i,j}\) for \(i\) and \(j\in[N]\). Prefix tuning extends this MoE by introducing \(L\) additional prefix experts \(f_{N+j^{\prime}}\) defined by prefix vectors \(\bm{p}^{V}_{j^{\prime}}\) and linear score functions \(s_{i,N+j^{\prime}}\) for \(i\in[N]\) and \(j^{\prime}\in[L]\). These new experts collaborate with the pre-trained ones within the MoE model, facilitating the model's adaptation to downstream tasks.

We argue that our introduction of a novel connection between self-attention, prefix tuning, and MoE offers a fresh perspective on the design of previous prompt-based continual learning methods. In the context of continual learning, the pre-trained experts serve as a knowledge base, while prefix tuning augments it with task-specific knowledge encoded in new experts. Moreover, we draw a parallel between the pre-trained experts and the G(eneral)-Prompt utilized in DualPrompt, which captures

Figure 2: Left: An illustrative depiction of prefix tuning as the introduction of new experts into pre-trained MoE models. Right: Visualization of NoRGa implementation, integrating non-linear activation and residual connections into the prefix tuning attention matrix.

task-agnostic information [53]. Both are shared across tasks, making them useful for prediction, especially when task identification is incorrect. Notably, the new experts achieve their efficiency through simple linear gating functions and independence from the input, unlike the pre-trained experts. For simplicity, we call the MoE model (11) as _linear gating prefix MoE_.

**Statistical suboptimality.** The connection between prefix tuning and the MoE within the linear gating prefix MoE model (11) allows us to theoretically explore the statistical behavior of the prefix tuning. In Appendix A, by interpreting the linear gating prefix MoE as a regression problem with sample size \(n\), we demonstrate that the convergence rate for estimating the model parameters, e.g., prompts, can be as slow as \(\mathcal{O}(1/\log^{\tau}(n))\) where \(\tau>0\) is some constant. This suggests that a huge amount of data is required to achieve reasonable parameter estimation in the linear gating prefix MoE model, which can be discouraging in practice. To address this statistical limitation, the next section introduces a novel non-linear residual gating score function to replace the linear gating function.

## 4 Non-linear Residual Gate Meets Prefix Tuning

As discussed earlier, prefix tuning introduces additional experts within the MoE framework, resulting in the linear gating prefix MoE model. However, as outlined in Appendix A, this approach suffers from suboptimal sample efficiency for parameter estimation. To overcome this and enhance overall CIL performance, we propose an innovative approach that significantly improves sample efficiency while promoting WTP performance in Section 4.1 and provide theoretical explanations in Section 4.2.

### NoRGa: Non-linear Residual Gate

We propose a simple yet effective modification to the linear gating prefix MoE model by incorporating non-linear activation and residual connection within the score functions of prefix experts as follows:

\[\hat{s}_{i,N+j}(\bm{X}) :=\frac{\bm{X}^{\top}E_{i}^{\top}W_{l}^{Q}{W_{l}^{K}}^{\top}\bm{ p}_{j}^{K}}{\sqrt{d_{v}}}+\alpha\cdot\sigma\left(\tau\cdot\frac{\bm{X}^{\top}E_{i}^{ \top}W_{l}^{Q}{W_{l}^{K}}^{\top}\bm{p}_{j}^{K}}{\sqrt{d_{v}}}\right)\] \[=s_{i,N+j}(\bm{X})+\alpha\cdot\sigma(\tau\cdot s_{i,N+j}(\bm{X})), \;i\in[N],\;j\in[L],\] (12)

where \(\alpha,\tau\in\mathbb{R}\) are learnable scalar factors, and \(\sigma\) is a non-linear activation function. The new score function in equation (12) consists of a linear and a non-linear component. We call the new prefix MoE model with score functions (12) as _non-linear residual gating prefix MoE_.

The use of a non-linear activation function here is motivated by the algebraic independence condition in Definition 4.2 to theoretically guarantee the optimal sample efficiency of expert and parameter estimations (cf. Theorem 4.3). It's worth noting that removing the linear component \(s_{i,N+j}(\bm{X})\) in the score function (12) could potentially lead to the vanishing gradient problem during training. To mitigate this challenge, we incorporate a residual connection [14] into the formulation. Our modification introduces minimal additional parameters (\(\alpha\) and \(\tau\)) compared to the original score function, ensuring parameter efficiency. This is particularly crucial in continual learning scenarios where the number of parameters grows with each new task. For implementation, we define \(H_{l}=W_{l}^{Q}{W_{l}^{K}}^{\top}\). From equation (5), the attention matrix of the \(l\)-th head can then be written as:

\[A_{l}=\frac{\bm{X}^{Q}H_{l}[\bm{p}^{K}{}^{\top},\;\bm{X}^{K}{}^{\top}]}{\sqrt{ d_{v}}}=\frac{[\bm{X}^{Q}H_{l}\bm{p}^{K}{}^{\top},\;\bm{X}^{Q}H_{l}\bm{X}^{K}{}^{ \top}]}{\sqrt{d_{v}}}=[A_{l}^{\text{prompt}},\;A_{l}^{\text{pretrain}}].\] (13)

Here, \(A_{l}^{\text{prompt}}\) denotes the attention score matrix for the prompts, and \(A_{l}^{\text{pretrain}}\) represents the attention score matrix for the pre-trained experts. To implement NoRGa, we can directly modify the final attention matrix as follows:

\[\hat{A}_{l}=[\hat{A}_{l}^{\text{prompt}},A_{l}^{\text{pretrain}}],\] (14)

\[\hat{A}_{l}^{\text{prompt}}=A_{l}^{\text{prompt}}+\alpha\cdot\sigma(\tau\cdot A _{l}^{\text{prompt}}).\] (15)

The implementation of NoRGa is illustrated in Figure 2. Despite its simplicity, our modification can significantly enhance sample efficiency and promote more reasonable parameter estimation, as demonstrated in our theoretical analysis in Section 4.2. Within the HiDe-Prompt framework, task-specific prompt parameters are trained using the WTP objective for each new task. Consequently, our modification leads to better parameter estimation, which directly contributes to improved WTP performance, ultimately improving overall continual learning efficacy. Importantly, NoRGa maintains the same parameter count as HiDe-Prompt, which is crucial in CL because of the memory constraint. Here, we evaluated \(\sigma\) with \(\tanh\), \(\operatorname{sigmoid}\), and \(\operatorname{GELU}\), finding \(\tanh\) to perform well in most cases.

[MISSING_PAGE_FAIL:7]

**Definition 4.2** (Algebraic independence).: We say that an expert function \(h(\cdot,\eta)\) and an activation function \(\sigma(\cdot)\) are algebraically independent if they are twice differentiable w.r.t their parameters, and if for any \(k\geq 1\) and pair-wise distinct parameters \((\beta_{11},\eta_{1}),\ldots,(\beta_{1k},\eta_{k})\), the following set of functions in \(\bm{X}\) is linearly independent for almost every \(\bm{X}\in\mathbb{R}^{Nd}\):

\[\Big{\{}\bm{X}^{\nu}\Big{[}(1+\sigma^{\prime}(\beta_{1j}^{\top} \bm{X}))^{|\nu|}+\bm{1}_{\{|\nu|=2\}}\sigma^{\prime\prime}(\beta_{1j}^{\top}\bm{ X})\Big{]}\cdot\frac{\partial^{|\gamma|}h}{\partial\eta^{\gamma}}(\bm{X},\eta_{j}):j \in[k_{*}],\\ \nu\in\mathbb{N}^{Nd},\gamma\in\mathbb{N}^{q}:0\leq|\nu|+|\gamma |\leq 2\Big{\}}.\]

Intuitively, the algebraic independence condition ensures that there will be no interactions among parameters of the expert function \(h(\cdot,\eta)\) and the activation function \(\sigma(\cdot)\). Technically, a key step in our argument is to decompose the regression discrepancy \(g_{\widehat{G}_{n}}(\bm{X})-g_{G_{*}}(\bm{X})\) into a combination of linearly independent terms by applying Taylor expansions to the product of the softmax's numerator and the expert function, i.e., \(\exp(\beta_{1}^{\top}\bm{X}+\alpha\sigma(\tau\beta_{1}^{\top}\bm{X}))h(\bm{X},\eta)\). Thus, the above condition guarantees that all the derivative terms in the Taylor expansion are linearly independent. To exemplify the algebraic independence condition, we consider the following simple examples of the expert functions \(h(\cdot,\eta)\) and the activation \(\sigma(\cdot)\) that are algebraically independent.

**Example.** When the expert function \(h(\cdot,\eta)\) is formulated as a neural network \(h(\bm{X},(a,b))=\varphi(a^{\top}\bm{X}+b)\) with the activation \(\varphi(\cdot)\in\{\mathrm{ReLU}(\cdot),\mathrm{GELU}(\cdot),z\mapsto z^{p}\}\), where \((a,b)\in\mathbb{R}^{Nd}\times\mathbb{R}\), and the activation function \(\sigma(\cdot)\) is one among the functions \(\mathrm{sigmoid}(\cdot),\tanh(\cdot),\mathrm{GELU}(\cdot)\), then they satisfy the algebraic independence condition in Definition 4.2.

Finally, we establish the rates for estimating parameters and experts in the non-linear residual gating prefix MoE model in Theorem 4.3. Prior to presenting the theorem statement, let us design a loss function among parameters based on a notion of Voronoi cells [27], which is a commonly employed approach for the convergence analysis of expert estimation in MoE models [36, 34, 35, 33], yet tailored to the setting of this paper. In particular, the Voronoi loss used for our analysis is defined as

\[\mathcal{L}_{1}(G,G_{*}):=\sum_{j^{\prime}\in[L]:|\mathcal{V}_{j^ {\prime}}|=1}\sum_{i\in\mathcal{V}_{j^{\prime}}}\exp(\beta_{0i})\Big{[}\| \Delta\beta_{1ij^{\prime}}\|^{2}+\|\Delta\eta_{ij^{\prime}}\|^{2}\Big{]}\\ +\sum_{j^{\prime}\in[L]:|\mathcal{V}_{j^{\prime}}|=1}\sum_{i\in \mathcal{V}_{j^{\prime}}}\exp(\beta_{0i})\Big{[}\|\Delta\beta_{1ij^{\prime}} \|+\|\Delta\eta_{ij^{\prime}}\|\Big{]}+\sum_{j^{\prime}=1}^{L}\Big{|}\sum_{i \in\mathcal{V}_{j^{\prime}}}\exp(\beta_{0i})-\exp(\beta_{0j^{\prime}}^{*}) \Big{|},\] (20)

where we denote \(\Delta\beta_{1ij^{\prime}}:=\beta_{1i}-\beta_{1j^{\prime}}^{*}\) and \(\Delta\eta_{ij^{\prime}}:=\eta_{i}-\eta_{j^{\prime}}^{*}\). Above, \(\mathcal{V}_{j^{\prime}}\equiv\mathcal{V}_{j^{\prime}}(G)\), for \(j^{\prime}\in[L]\), is a Voronoi cell associated with the mixing measure \(G\) generated by the true component \(\omega_{j}^{*}:=(\beta_{1j^{\prime}}^{*},\eta_{j^{\prime}}^{*})\), which is defined as follows:

\[\mathcal{V}_{j^{\prime}}:=\{i\in\{1,2,\ldots,L^{\prime}\}:\|\omega_{i}-\omega_ {j^{\prime}}^{*}\|\leq\|\omega_{i}-\omega_{\ell}^{*}\|,\;\forall\ell\neq j^{ \prime}\},\] (21)

where we denote \(\omega_{i}:=(\beta_{1i},\eta_{i})\) as a component of \(G\). Note that, the cardinality of each Voronoi cell \(\mathcal{V}_{j^{\prime}}\) indicates the number of components \(\omega_{i}\) of \(G\) approximating the true component \(\omega_{j^{\prime}}^{*}\) of \(G_{*}\). Additionally, since \(\mathcal{L}_{1}(G,G_{*})=0\) if and only if \(G\equiv G_{*}\), it follows that when \(\mathcal{L}_{1}(G,G_{*})\) becomes sufficiently small, the differences \(\Delta\beta_{1ij^{\prime}}\) and \(\Delta\eta_{ij^{\prime}}\) are also small. This observation indicates that, although \(\mathcal{L}_{1}(G,G_{*})\) is a proper metric as it is not symmetric, it is an appropriate loss function for measuring the discrepancy between the least square estimator \(\widehat{G}_{n}\) and the true mixing measures \(G_{*}\).

**Theorem 4.3**.: _Assume that the expert function \(h(x,\eta)\) and the activation \(\sigma(\cdot)\) are algebraically independent, then we achieve the following lower bound for any \(G\in\mathcal{G}_{L^{\prime}}(\Theta)\):_

\[\|g_{G}-g_{G_{*}}\|_{L_{2}(\eta)}\gtrsim\mathcal{L}_{1}(G,G_{*}),\]

_which together with Theorem 4.1 indicates that \(\mathcal{L}_{1}(\widehat{G}_{n},G_{*})=\widetilde{\mathcal{O}}_{P}(n^{-1/2})\)._

Proof of Theorem 4.3 is in Appendix B.2. A few comments on Theorem 4.3 are in order: (i) From the bound \(\mathcal{L}_{1}(\widehat{G}_{n},G_{*})=\widetilde{\mathcal{O}}_{P}(n^{-1/2})\), we deduce that the estimation rates for the over-specified parameters \(\beta_{1j^{\prime}}^{*},\eta_{1j^{\prime}}^{*}\), where \(j^{\prime}\in[L]:|\mathcal{V}_{j^{\prime}}|>1\), are all of order \(\mathcal{O}_{P}(\sqrt[4]{\log(n)/n})\). Since the expert \(h(\cdot,\eta)\) is twice differentiable over a bounded domain, it is also a Lipschitz function. Thus, denote\(\widehat{G}_{n}:=\sum_{i=1}^{L_{n}}\exp(\widehat{\beta}_{0i})\delta_{(\widehat{\beta} ^{*}_{1i},\widehat{\eta}^{*}_{i})}\), we achieve that

\[\sup_{\bm{X}}|h(\bm{X},\widehat{\eta}^{*}_{i})-h(\bm{X},\eta^{*}_{j^{\prime}})| \lesssim\|\widehat{\eta}^{n}_{i}-\eta^{*}_{j^{\prime}}\|\lesssim\mathcal{O}_{P }(\sqrt[4]{\log(n)/n}).\] (22)

The above bound indicates that if the experts \(h(\cdot,\eta^{*}_{j})\) are fitted by at least two other experts, then their estimation rates are also of order \(\mathcal{O}_{P}(\sqrt[4]{\log(n)/n})\); (ii) For exactly-specified parameters \(\beta^{*}_{1j^{\prime}},\eta^{*}_{j^{\prime}}\), where \(j^{\prime}\in[L]:|\mathcal{V}_{j^{\prime}}|=1\), the rates for estimating them are faster than those of their over-specified counterparts, standing at order \(\mathcal{O}_{P}(\sqrt{\log(n)/n})\). By arguing similarly to equation (22), the experts \(h(\cdot,\eta^{*}_{j^{\prime}})\) also enjoy the faster estimation rate of order \(\mathcal{O}_{P}(\sqrt{\log(n)/n})\), which is parametric on the sample size \(n\); (iii) It follows from the above rates that we only need a polynomial number of data (roughly \(\epsilon^{-4}\) where \(\epsilon\) is the desired approximation error) to estimate the parameters and experts of the non-linear residual gating prefix MoE. By contrast, when using the linear gating, as being demonstrated in Appendix A, it requires an exponential number of data. This highlights the statistical benefits of using the non-linear residual gating MoE model over the linear gating prefix MoE model.

## 5 Experiments

**Datasets.** We evaluate various continual learning methods on widely used CIL benchmarks, including Split CIFAR-100 [23] and Split ImageNet-R [23], consistent with prior work [49]. We further explore the model's performance on fine-grained classification tasks with Split CUB-200 [48] and large inter-task differences with 5-Datasets [9]. Please refer to Appendix E for more details.

**Evaluation Metrics.** We utilize several established metrics described in [50]. These include: final average accuracy (FA), which represents the average accuracy after the final task; cumulative average accuracy (CA), which refers to the historical average accuracy; and average forgetting measure (FM). We give more emphasis to FA and CA due to their comprehensiveness, as noted in [42].

**Baselines.** We compare our approach with several representative prompt-based approaches including L2P [54], DualPrompt [53], CODA-Prompt [42], S-Prompt [52], and HiDe-Prompt [49]. Additionally,

\begin{table}
\begin{tabular}{c l c c c c c c} \hline \hline \multirow{2}{*}{PTM} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{Split CIFAR-100} & \multicolumn{3}{c}{Split ImageNet-R} \\ \cline{3-8}  & & **FA** (\(\uparrow\)) & \multicolumn{1}{c}{**CA**(\(\uparrow\))} & \multicolumn{1}{c}{FM(\(\downarrow\))} & \multicolumn{1}{c}{**FA** (\(\uparrow\))} & \multicolumn{1}{c}{**CA**(\(\uparrow\))} & \multicolumn{1}{c}{FM(\(\downarrow\))} \\ \hline \multirow{8}{*}{Sup-21K} & L2P & \(83.06\pm 0.17\) & \(88.27\pm 0.71\) & \(5.61\pm 0.32\) & \(67.53\pm 0.44\) & \(71.98\pm 0.52\) & \(5.84\pm 0.38\) \\  & DualPrompt & \(87.30\pm 0.27\) & \(91.23\pm 0.65\) & \(3.87\pm 0.43\) & \(70.93\pm 0.08\) & \(75.67\pm 0.52\) & \(5.47\pm 0.19\) \\  & S-Prompt & \(87.57\pm 0.42\) & \(91.38\pm 0.69\) & \(3.63\pm 0.41\) & \(69.88\pm 0.51\) & \(74.25\pm 0.55\) & \(4.73\pm 0.47\) \\  & CODA-Prompt & \(86.94\pm 0.63\) & \(91.57\pm 0.75\) & \(4.04\pm 0.18\) & \(70.03\pm 0.47\) & \(74.26\pm 0.24\) & \(5.17\pm 0.22\) \\  & HiDe-Prompt & \(92.61\pm 0.28\) & \(94.03\pm 0.01\) & \(1.50\pm 0.28\) & \(75.06\pm 0.12\) & \(76.60\pm 0.01\) & \(\bm{409\pm 0.13}\) \\  & NoRa (Ours) & \(\bm{94.48}\pm 0.13\) & \(\bm{95.83}\pm 0.37\) & \(\bm{14.44}\pm 0.27\) & \(\bm{75.40}\pm 0.39\) & \(\bm{79.52}\pm 0.07\) & \(4.59\pm 0.07\) \\ \hline \multirow{8}{*}{iBOT-21K} & L2P & \(79.13\pm 1.25\) & \(85.13\pm 0.05\) & \(7.50\pm 1.21\) & \(61.31\pm 0.50\) & \(68.81\pm 0.52\) & \(10.72\pm 0.40\) \\  & DualPrompt & \(78.84\pm 0.47\) & \(86.16\pm 0.02\) & \(8.84\pm 0.67\) & \(58.69\pm 0.61\) & \(66.61\pm 0.67\) & \(11.75\pm 0.92\) \\  & S-Prompt & \(79.14\pm 0.65\) & \(85.85\pm 0.17\) & \(8.23\pm 1.15\) & \(57.96\pm 1.10\) & \(66.42\pm 0.71\) & \(11.27\pm 0.72\) \\  & CODA-Prompt & \(80.83\pm 0.27\) & \(87.02\pm 0.20\) & \(7.50\pm 0.25\) & \(61.22\pm 0.35\) & \(67.66\pm 0.37\) & \(9.66\pm 0.20\) \\  & HiDe-Prompt & \(93.02\pm 0.15\) & \(94.56\pm 0.05\) & \(\bm{1.26}\pm 0.13\) & \(70.83\pm 0.17\) & \(73.23\pm 0.08\) & \(\bm{67.77}\pm 0.23\) \\  & NoRa (Ours) & \(\bm{94.76}\pm 0.15\) & \(\bm{95.86}\pm 0.31\) & \(1.34\pm 0.14\) & \(\bm{73.06}\pm 0.26\) & \(\bm{77.46}\pm 0.42\) & \(6.88\pm 0.49\) \\ \hline \multirow{8}{*}{iBOT-1K} & L2P & \(75.51\pm 0.88\) & \(82.53\pm 1.10\) & \(6.80\pm 1.70\) & \(59.43\pm 0.28\) & \(66.683\pm 0.92\) & \(11.13\pm 1.25\) \\  & DualPrompt & \(76.21\pm 1.00\) & \(83.54\pm 1.23\) & \(8.99\pm 1.81\) & \(60.41\pm 0.76\) & \(66.87\pm 0.41\) & \(9.21\pm 0.43\) \\  & S-Prompt & \(76.60\pm 0.61\) & \(82.89\pm 0.89\) & \(8.60\pm 1.36\) & \(59.56\pm 0.60\) & \(66.60\pm 0.13\) & \(8.83\pm 0.81\) \\  & CODA-Prompt & \(79.11\pm 1.02\) & \(86.21\pm 0.49\) & \(7.69\pm 1.57\) & \(66.56\pm 0.68\) & \(73.14\pm 0.57\) & \(7.22\pm 0.38\) \\  & HiDe-Prompt & \(93.48\pm 0.11\) & \(95.02\pm 0.01\) & \(1.63\pm 0.13\) & \(71.33\pm 0.21\) & \(73.62\pm 0.13\) & \(71.11\pm 0.02\) \\  & NoRa (Ours) & \(\bm{94.01}\pm 0.04\) & \(\bm{95.11}\pm 0.35\) & \(\bm{1.61}\pm 0.30\) & \(\bm{72.77}\pm 0.20\) & \(\bm{76.55}\pm 0.46\) & \(\bm{71.0}\pm 0.39\) \\ \hline \multirow{8}{*}{DINO-1K} & L2P & \(72.23\pm 0.35\) & \(79.71\pm 1.26\) & \(8.37\pm 2.30\) & \(57.21\pm 0.6we evaluate against state-of-the-art pre-trained model-based continual learning methods, including ADAM [60] and RanPAC [29]. We further extend our evaluation by applying HiDe-Prompt with parameter-efficient fine-tuning techniques like LoRA [17] and Adapters [16]. In line with [49], we utilize the checkpoints of ViT that use supervised pre-training of Imagenet-21K (denoted as Sup-21K), and some self-supervised pre-training such as iBOT-21K, iBOT-1K [62], DINO-1K [4], and MoCo-1K [7]. For implementation details, see Appendix E.

**Main Results.** In Table 1, we evaluate several continual learning methods on Split CIFAR-100 and Split ImageNet-R using diverse pre-trained models. NoRGa achieves state-of-the-art FA and CA across all datasets and models, consistently outperforming HiDe-Prompt. On Sup-21K, NoRGa demonstrates impressive FA results on both CIFAR-100 and ImageNet-R. It also maintains the highest CA, with significant margins of 1.80% and 2.92% on CIFAR-100 and ImageNet-R, respectively, compared to HiDe-Prompt. These results highlight NoRGa's strong ability to retain knowledge and exhibit minimal forgetting, as evidenced by the low FM values on both datasets. NoRGa also surpasses HiDe-Prompt on self-supervised models, with FA improvements up to 1.95% and 3.66%. We further investigate two scenarios: fine-grained classification tasks and large inter-task differences through experiments on Split CUB-200 and 5-Datasets, respectively, as shown in Table 2. NoRGa maintains its lead, achieving FA gaps of 4.34% and 2.46% on Split CUB-200, and the highest FA on 5-Datasets. While gains in some metrics may be modest, NoRGa consistently outperforms HiDe-Prompt in either FA or CA, underscoring its robustness. For example, on Split ImageNet-R with Sup-21K weights, the FA improvement is small (75.06% vs. 75.40%), but the CA gains are substantial (76.60% vs. 79.52%), demonstrating the method's effectiveness and robustness.

**Ablation Study.** To assess the impact of non-linear activation functions on NoRGa's performance, we evaluated the model's behavior with different choices for the activation function \(\sigma\), including \(\tanh\), \(\mathrm{sigmoid}\), and \(\mathrm{GELU}\) in Table 3. The results show that NoRGa achieves state-of-the-art performance on both Split CIFAR-100 and Split CUB-200 datasets with all three activation functions. These findings suggest that NoRGa exhibits robustness to the choice of non-linear activation within a reasonable range. While all functions perform well, the \(\tanh\) activation function demonstrates generally strong performance across scenarios. Further results are provided in the Appendix.

## 6 Conclusion

This paper presents an initial exploration of self-attention and prefix-tuning through the lens of mixture of experts. We find that applying prefix tuning can be viewed as introducing new prefix experts to adapt the pre-trained model. However, limitations in sample efficiency exist. We address this by proposing NoRGa, a novel gating mechanism to enhance continual learning performance. Our results demonstrate NoRGa's effectiveness both theoretically and empirically. While the current implementation of the expert network prioritizes simplicity, future research directions could involve investigating more intricate architectures. Furthermore, the choice of activation functions in our work requires fine-tuning, which opens avenues for future research on adaptively learning activation.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Split CUB-200} & \multicolumn{2}{c}{5-Datasets} \\ \cline{2-5}  & Sup-21K & iBOT-21K & Sup-21K & iBOT-21K \\ \hline L2P & 75.46 & 46.60 & 81.84 & 82.25 \\ DualPrompt & 77.56 & 45.93 & 77.91 & 68.03 \\ S-Prompt & 77.13 & 44.22 & 86.06 & 77.20 \\ CODA-Prompt & 74.34 & 47.79 & 64.18 & 51.65 \\ HiDe-Prompt & 86.56 & 78.23 & 93.83 & 94.88 \\ NoRGa (Ours) & **90.90** & **80.69** & **94.16** & **94.92** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Final average accuracy (FA) on Split CUB-200 and 5-Datasets.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Split CIFAR-100} & \multicolumn{2}{c}{Split CUB-200} \\ \cline{2-5}  & Sup-21K & iBOT-21K & Sup-21K & iBOT-21K \\ \hline HiDe-Prompt & 92.61 & 93.02 & 86.56 & 78.23 \\ NoRGa \(\tanh\) & 94.36 & **94.76** & 90.87 & **80.69** \\ NoRGa sigmoid & **94.48** & 94.69 & **90.90** & 80.18 \\ NoRGa \(\mathrm{GELU}\) & 94.05 & 94.63 & 90.74 & 80.54 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation study of different activation functions, measured by final average accuracy (FA).

## References

* [1] R. Aljundi, P. Chakravarty, and T. Tuytelaars. Expert gate: Lifelong learning with a network of experts. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3366-3375, 2017.
* [2] E. Belouadah, A. Popescu, and I. Kanellos. A comprehensive study of class incremental learning algorithms for visual tasks. _Neural Networks_, 135:38-54, 2021.
* [3] Y. Bulatov. Notmnist dataset. _Google (Books/OCR), Tech. Rep.[Online]. Available: http://yaroslavvb. blogspot. it/2011/09/nontmist-dataset. html_, 2, 2011.
* [4] M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.
* [5] H. Cha, J. Lee, and J. Shin. Co2l: Contrastive continual learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9516-9525, October 2021.
* [6] A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. S. Torr, and M. Ranzato. On tiny episodic memories in continual learning, 2019.
* [7] X. Chen, S. Xie, and K. He. An empirical study of training self-supervised vision transformers, 2021.
* [8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. _ICLR_, 2021.
* [9] S. Ebrahimi, F. Meier, R. Calandra, T. Darrell, and M. Rohrbach. Adversarial continual learning. _arXiv preprint arXiv:2003.09553_, 2020.
* [10] D. Eigen, M. Ranzato, and I. Sutskever. Learning factored representations in a deep mixture of experts. In _ICLR Workshops_, 2014.
* [11] Z. Fan, R. Sarkar, Z. Jiang, T. Chen, K. Zou, Y. Cheng, C. Hao, Z. Wang, et al. M\({}^{3}\)vit: Mixture-of-experts vision transformer for efficient multi-task learning with model-accelerator co-design. _Advances in Neural Information Processing Systems_, 35:28441-28457, 2022.
* [12] N. L. Hai, T. Nguyen, L. N. Van, T. H. Nguyen, and K. Than. Continual variational dropout: a view of auxiliary local variables in continual learning. _Machine Learning_, 113(1):281-323, 2024.
* [13] H. Hazimeh, Z. Zhao, A. Chowdhery, M. Sathiamoorthy, Y. Chen, R. Mazumder, L. Hong, and E. Chi. Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning. _Advances in Neural Information Processing Systems_, 34:29335-29347, 2021.
* [14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition, 2015.
* [15] N. Ho, C.-Y. Yang, and M. I. Jordan. Convergence rates for gaussian mixtures of experts. _Journal of Machine Learning Research_, 23(323):1-81, 2022.
* [16] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In _International conference on machine learning_, pages 2790-2799. PMLR, 2019.
* [17] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* [18] Q. Huang, Z. An, N. Zhuang, M. Tao, C. Zhang, Y. Jin, K. Xu, L. Chen, S. Huang, and Y. Feng. Harder tasks need more experts: Dynamic routing in moe models. _arXiv preprint arXiv:2403.07652_, 2024.

* [19] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. _Neural Computation_, 3, 1991.
* [20] P. Janson, W. Zhang, R. Aljundi, and M. Elhoseiny. A simple baseline that questions the use of pretrained-models in continual learning. _arXiv preprint arXiv:2210.04428_, 2022.
* [21] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. _Neural computation_, 6(2):181-214, 1994.
* [22] G. Kim, C. Xiao, T. Konishi, Z. Ke, and B. Liu. A theoretical study on solving continual learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 5065-5079. Curran Associates, Inc., 2022.
* [23] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
* [24] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [25] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 3045-3059. Association for Computational Linguistics, Nov. 2021.
* [26] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021.
* [27] T. Manole and N. Ho. Refined convergence rates for maximum likelihood estimation under finite mixture models. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 14979-15006. PMLR, 17-23 Jul 2022.
* [28] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In _Psychology of learning and motivation_, volume 24, pages 109-165. Elsevier, 1989.
* [29] M. D. McDonnell, D. Gong, A. Parvaneh, E. Abbasnejad, and A. van den Hengel. Rampac: Random projections and pre-trained models for continual learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [30] S. V. Mehta, D. Patil, S. Chandar, and E. Strubell. An empirical investigation of the role of pre-training in lifelong learning. _Journal of Machine Learning Research_, 24(214):1-50, 2023.
* [31] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In _NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011_, 2011.
* [32] C. V. Nguyen, A. Achille, M. Lam, T. Hassner, V. Mahadevan, and S. Soatto. Toward understanding catastrophic forgetting in continual learning, 2019.
* [33] H. Nguyen, P. Akbarian, and N. Ho. Is temperature sample efficient for softmax Gaussian mixture of experts? In _Proceedings of the ICML_, 2024.
* [34] H. Nguyen, P. Akbarian, F. Yan, and N. Ho. Statistical perspective of top-k sparse softmax gating mixture of experts. In _International Conference on Learning Representations_, 2024.
* [35] H. Nguyen, N. Ho, and A. Rinaldo. On least square estimation in softmax gating mixture of experts. In _Proceedings of the ICML_, 2024.
* [36] H. Nguyen, T. Nguyen, and N. Ho. Demystifying softmax gating function in Gaussian mixture of experts. In _Advances in Neural Information Processing Systems_, 2023.
* [37] A. Panos, Y. Kobe, D. O. Reino, R. Aljundi, and R. E. Turner. First session adaptation: A strong replay-free baseline for class-incremental learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 18820-18830, 2023.

* [38] H. Phan, A. P. Tuan, S. Nguyen, N. V. Linh, and K. Than. Reducing catastrophic forgetting in neural networks via gaussian mixture approximation. In _Pacific-Asia Conference on Knowledge Discovery and Data Mining_, pages 106-117. Springer, 2022.
* [39] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert. icarl: Incremental classifier and representation learning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, July 2017.
* [40] T. Ridnik, E. Ben-Baruch, A. Noy, and L. Zelnik-Manor. Imagenet-21k pretraining for the masses, 2021.
* [41] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In _International Conference on Learning Representations (ICLR)_, 2017.
* [42] J. S. Smith, L. Karlinsky, V. Gutta, P. Cascante-Bonilla, D. Kim, A. Arbelle, R. Panda, R. Feris, and Z. Kira. Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11909-11919, June 2023.
* [43] Q. Tran, H. Phan, K. Than, D. Phung, and T. Le. Continual learning with optimal transport based mixture model. _arXiv preprint arXiv:2211.16780_, 2022.
* [44] Q. Tran, L. Tran, K. Than, T. Tran, D. Phung, and T. Le. Koppa: Improving prompt-based continual learning with key-query orthogonal projection and prototype-based one-versus-all. _arXiv preprint arXiv:2311.15414_, 2023.
* [45] S. van de Geer. _Empirical processes in M-estimation_. Cambridge University Press, 2000.
* [46] G. M. van de Ven, T. Tuytelaars, and A. S. Tolias. Three types of incremental learning. _Nature Machine Intelligence_, 4:1185-1197, 2022.
* [47] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [48] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. _The Caltech-UCSD Birds-200-2011 Dataset_. California Institute of Technology, 2011.
* [49] L. Wang, J. Xie, X. Zhang, M. Huang, H. Su, and J. Zhu. Hierarchical decomposition of prompt-based continual learning: Rethinking obscured sub-optimality. _Advances in Neural Information Processing Systems_, 2023.
* [50] L. Wang, X. Zhang, H. Su, and J. Zhu. A comprehensive survey of continual learning: Theory, method and application, 2024.
* [51] L. Wang, X. Zhang, K. Yang, L. Yu, C. Li, H. Lanqing, S. Zhang, Z. Li, Y. Zhong, and J. Zhu. Memory replay with data compression for continual learning. In _International Conference on Learning Representations_, 2021.
* [52] Y. Wang, Z. Huang, and X. Hong. S-prompts learning with pre-trained transformers: An occam's razor for domain incremental learning. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2022.
* [53] Z. Wang, Z. Zhang, S. Ebrahimi, R. Sun, H. Zhang, C.-Y. Lee, X. Ren, G. Su, V. Perot, J. Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. _European Conference on Computer Vision_, 2022.
* [54] Z. Wang, Z. Zhang, C.-Y. Lee, H. Zhang, R. Sun, X. Ren, G. Su, V. Perot, J. Dy, and T. Pfister. Learning to prompt for continual learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 139-149, 2022.
* [55] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.

* [56] Y. Xin, S. Luo, H. Zhou, J. Du, X. Liu, Y. Fan, Q. Li, and Y. Du. Parameter-efficient fine-tuning for pre-trained vision models: A survey, 2024.
* [57] B. Yu. Assouad, Fano, and Le Cam. _Festschrift for Lucien Le Cam_, pages 423-435, 1997.
* [58] J. Yu, Y. Zhuge, L. Zhang, P. Hu, D. Wang, H. Lu, and Y. He. Boosting continual learning of vision-language models via mixture-of-experts adapters. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 23219-23230, 2024.
* [59] G. Zhang, L. Wang, G. Kang, L. Chen, and Y. Wei. S lca: Slow learner with classifier alignment for continual learning on a pre-trained model. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2023.
* [60] D.-W. Zhou, Z.-W. Cai, H.-J. Ye, D.-C. Zhan, and Z. Liu. Revisiting class-incremental learning with pre-trained models: Generalizability and adaptivity are all you need. _International Journal of Computer Vision_, pages 1-21, 2024.
* [61] D.-W. Zhou, H.-L. Sun, J. Ning, H.-J. Ye, and D.-C. Zhan. Continual learning with pre-trained models: A survey, 2024.
* [62] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong. ibot: Image bert pre-training with online tokenizer. _International Conference on Learning Representations (ICLR)_, 2022.

**Supplement to "Mixture of Experts Meets Prompt-Based**

Continual Learning"

In this supplementary material, we first analyze the statistical suboptimality of the Linear Gating Prefix MoE Model (11) in Appendix A. Appendix B provides proofs for the theoretical results presented in Section 4.2. In Appendix C, we discuss related works on mixture of experts. Appendix D outlines the training algorithm for HiDe-Prompt while Appendix E presents the experimental setup and details. Further, Appendix F presents further experiments on the task-incremental learning setting to empirically demonstrate the benefits of using our proposed Non-linear Residual Gating Prefix MoE (12) over the Linear Gating Prefix MoE Model. Appendix G and Appendix H compare NoRGa with other parameter-efficient fine-tuning techniques and pre-trained model-based methods. In Appendix I, we present the efficiency tests, while Appendix J explores the impact of learnable \(\alpha\) and \(\tau\). Finally, Appendix K compares the training times between NoRGa and HiDe-Prompt.

## Appendix A Statistical Suboptimality of Linear Gating Prefix MoE Model

In this appendix, we demonstrate that estimating parameters and experts in the linear gating prefix MoE model (11) can be statistically inefficient in terms of the number of data. To simplify our findings, we particularly focus on the first head, namely, \(l=1\) in equation (11), and the first row of this head, namely, \(i=1\) in equation (11). Then, we proceed to provide a theoretical justification of our claim for the suboptimality of the linear gating prefix MoE by viewing this row as an output of the regression setting. In particular, we assume that \((\bm{X}_{1},Y_{1}),(\bm{X}_{2},Y_{2}),\ldots,(\bm{X}_{n},Y_{n})\in\mathbb{R}^ {Nd}\times\mathbb{R}\) is an i.i.d. sample generated from the following model:

\[Y_{i}=f_{G_{*}}(\bm{X}_{i})+\varepsilon_{i},\quad i=1,2,\ldots,n,\] (23)

where \(\varepsilon_{1},\ldots,\varepsilon_{n}\) are independent Gaussian noise variables such that \(\mathbb{E}[\varepsilon_{i}|\bm{X}_{i}]=0\) and \(\mathrm{Var}(\varepsilon_{i}|\bm{X}_{i})=\nu^{2}\) for all \(1\leq i\leq n\). Additionally, we assume that \(\bm{X}_{1},\bm{X}_{2},\ldots,\bm{X}_{n}\) are i.i.d. samples from some probability distribution \(\mu\). Motivated by linear gating prefix MoE model (11), the regression function \(f_{G_{*}}(\cdot)\) in equation (23) admits the form of the linear gating prefix MoE model with pre-trained \(N\) experts and \(L\) unknown experts, namely

\[f_{G_{*}}(\bm{X}): =\sum_{j=1}^{N}\frac{\exp(\bm{X}^{\top}B_{j}^{0}\bm{X}+c_{j}^{0} )}{\sum_{k=1}^{N}\exp(\bm{X}^{\top}B_{k}^{0}\bm{X}+c_{k}^{0})+\sum_{k^{\prime} =1}^{L}\exp((\beta_{1k^{\prime}}^{*})^{\top}\bm{X}+\beta_{0k^{\prime}}^{*})} \cdot h(\bm{X},\eta_{j}^{0})\] \[+\sum_{j^{\prime}=1}^{L}\frac{\exp((\beta_{1j^{\prime}}^{*})^{ \top}\bm{X}+\beta_{0j^{\prime}}^{*})}{\sum_{k=1}^{N}\exp(\bm{X}^{\top}B_{k}^{0 }\bm{X}+c_{k}^{0})+\sum_{k^{\prime}=1}^{L}\exp((\beta_{1k^{\prime}}^{*})^{ \top}\bm{X}+\beta_{0k^{\prime}}^{*})}\cdot h(\bm{X},\eta_{j^{\prime}}^{*}),\] (24)

where \(G_{*}:=\sum_{j^{\prime}=1}^{L}\exp(\beta_{0j^{\prime}}^{*})\delta_{(\beta_{1j^ {\prime}}^{*},\eta_{j^{\prime}}^{*})}\) denotes a _mixing measure_, i.e., a weighted sum of Dirac measures \(\delta\), associated with unknown parameters \((\beta_{1j^{\prime}}^{*},\beta_{0j^{\prime}}^{*},\eta_{j^{\prime}}^{*})_{j^{ \prime}=1}^{L}\) in \(\mathbb{R}^{Nd}\times\mathbb{R}\times\mathbb{R}^{q}\). Here, the matrix \(B_{j}^{0}\) plays the role of the matrix \(\frac{E_{1}^{\top}W_{1}^{0}W_{1}^{K^{\top}}E_{j}}{\sqrt{d_{v}}}\) in the score function \(s_{1,j}(\bm{X})\). Furthermore, the vector \(\beta_{1j^{\prime}}^{*}\) corresponds to the vector \(\frac{E_{1}^{\top}W_{1}^{0}W_{1}^{K^{\top}}\bm{p}_{j^{\prime}}^{K}}{\sqrt{d_{v}}}\) in the score function \(s_{1,N+j^{\prime}}(\bm{X})\). Furthermore, the experts \(h(\bm{X},\eta_{j}^{0})\) correspond to the role of \(f_{j}(\bm{X})\) and \(h(\bm{X},\eta_{j^{\prime}}^{*})\) correspond to the role of \(f_{N+j^{\prime}}(\bm{X})\). In our formulation, we consider general parametric forms of the experts \(h(\bm{X},\eta_{j}^{0})\) and \(h(\bm{X},\eta_{j^{\prime}}^{*})\), i.e., we do not only constrain these expert functions to be the forms of the simple experts in the linear gating prefix MoE model.

Similar to the linear gating prefix MoE model (11), the matrices \(B_{j}^{0}\), the biases \(c_{j}^{0}\), and the expert parameters \(\eta_{j}^{0}\) are known. Our aim is to estimate the unknown gating parameters \(\beta_{1j^{\prime}}^{*},\beta_{0j^{\prime}}^{*}\), and \(\eta_{j^{\prime}}^{*}\) that correspond to the prompts.

**Least squares estimation:** We will use the least squares method [45] to estimate the unknown parameters \((\beta_{0j^{\prime}}^{*},\beta_{1j^{\prime}}^{*},\eta_{j^{\prime}}^{*})_{j^{ \prime}=1}^{L}\) or, equivalently, the ground-truth mixing measure \(G^{*}\). In particular, we take into account the estimator

\[\widetilde{G}_{n}:=\operatorname*{arg\,min}_{G\in\mathcal{G}_{L^{\prime}}( \Theta)}\sum_{i=1}^{n}\Big{(}Y_{i}-f_{G}(\bm{X}_{i})\Big{)}^{2},\] (25)where we denote \(\mathcal{G}_{L^{\prime}}(\Theta):=\{G=\sum_{i=1}^{\ell}\exp(\beta_{0i})\delta_{( \beta_{1i},\eta_{i})}:1\leq\ell\leq L^{\prime},\ (\beta_{0i},\beta_{1i},\eta_{i})\in\Theta\}\) as the set of all mixing measures with at most \(L^{\prime}\) atoms. In practice, since the true number of true experts \(L\) is typically unknown, we assume that the number of fitted experts \(L^{\prime}\) is sufficiently large, i.e. \(L^{\prime}>L\).

Let us recall that our main objective in this appendix is to show that using the linear gating in the prefix MoE model is not sample efficient. To illustrate that point, we consider a simple scenario when the expert function takes the form \(h(\bm{X},(a,b))=(a^{\top}\bm{X}+b)^{p}\), for some \(p\in\mathbb{N}\). Additionally, we also design a new Voronoi loss function as below to facilitate our arguments.

\[\mathcal{L}_{2,r}(G,G_{*}):=\sum_{j=1}^{L}\Big{|}\sum_{i\in \mathcal{V}_{j}}\exp(\beta_{0i})-\exp(\beta_{0j}^{*})\Big{|}+\sum_{j=1}^{L} \sum_{i\in\mathcal{V}_{j}}\exp(\beta_{0i})\Big{[}\|\Delta\beta_{1ij}\|^{r}+\| \Delta a_{ij}\|^{r}+|\Delta b_{ij}|^{r}\Big{]},\] (26)

where we denote \(\Delta\beta_{1ij^{\prime}}:=\beta_{1i}-\beta_{1j^{\prime}}^{*}\) and \(\Delta\eta_{ij^{\prime}}:=\eta_{i}-\eta_{j^{\prime}}^{*}\).

Now, we are ready to state the result of parameter estimation under the linear gating prefix MoE model in the following theorem:

**Theorem A.1**.: _Assume that the experts take the form \(h(\bm{X},(a,b))=(a^{\top}\bm{X}+b)^{p}\), for some \(p\in\mathbb{N}\), then we achieve the following minimax lower bound of estimating \(G_{*}\):_

\[\inf_{\overline{G}_{n}\in\mathcal{G}_{L^{\prime}}(\Theta)}\sup_{G \in\mathcal{G}_{L^{\prime}}(\Theta)\setminus\mathcal{G}_{L-1}(\Theta)}\mathbb{ E}_{f_{G}}[\mathcal{L}_{2,r}(\overline{G}_{n},G)]\gtrsim n^{-1/2},\]

_for any \(r\geq 1\), where \(\mathbb{E}_{f_{G}}\) indicates the expectation taken w.r.t the product measure with \(f_{G}^{n}\)._

There are two main implications of the result in Theorem A.1:

**(i)** The rates for estimating parameters \(\beta_{1j}^{*}\), \(a_{j}^{*}\) and \(b_{j}^{*}\) are slower than \(\mathcal{O}_{P}(n^{-1/2r})\), for any \(r\geq 1\). This means that they are slower than any polynomial rates, and could be of order \(\mathcal{O}_{P}(1/\log(n))\). Using the same reasoning described after equation (22), we have

\[\sup_{x}|\varphi((\widehat{a}_{i}^{n})^{\top}\bm{X}+\widehat{b}_{i}^{n})- \varphi((a_{j}^{*})^{\top}\bm{X}+b_{j}^{*})|\lesssim\cdot\|\widehat{a}_{i}^{n} -a_{j}^{*}\|+|\widehat{b}_{i}^{n}-b_{j}^{*}|.\] (27)

As a consequence, the rates for estimating experts \(\varphi((a_{j}^{*})^{\top}\bm{X}+b_{j}^{*})\) are no better than those for estimating the parameters \(a_{j}^{*}\) and \(b_{j}^{*}\), and could also be as slow as \(\mathcal{O}_{P}(1/\log(n))\).

**(ii)** The above rates imply that we need an exponential number of data (roughly \(\exp(1/\epsilon^{\tau})\) where \(\epsilon\) is the desired approximation error) to estimate the parameters and experts of the linear gating prefix MoE. This fact demonstrates that using the linear gating in the prefix MoE model is not sample efficient from the perspective of the expert estimation problem.

Proof of Theorem a.1.: Prior to presenting the main proof of Proposition A.1, let us introduce the following key result:

**Lemma A.2**.: _If the following holds for any \(r\geq 1\):_

\[\lim_{\varepsilon\to 0}\inf_{G\in\mathcal{G}_{L^{\prime}}(\Theta):\mathcal{L}_{2, r}(G,G_{*})\leq\varepsilon}\frac{\|f_{G}-f_{G_{*}}\|_{L_{2}(\mu)}}{\mathcal{L}_{2,r}(G,G_ {*})}=0,\] (28)

_then we obtain that_

\[\inf_{\overline{G}_{n}\in\mathcal{G}_{L^{\prime}}(\Theta)}\sup_{G \in\mathcal{G}_{L^{\prime}}(\Theta)\setminus\mathcal{G}_{L-1}(\Theta)}\mathbb{ E}_{f_{G}}[\mathcal{L}_{2,r}(\overline{G}_{n},G)]\gtrsim n^{-1/2}.\] (29)

Proof of Lemma a.2.: Indeed, from the Gaussian assumption on the noise variables \(\epsilon_{i}\), we obtain that \(Y_{i}|\bm{X}_{i}\sim\mathcal{N}(f_{G_{*}}(\bm{X}_{i}),\sigma^{2})\) for all \(i\in[n]\). Next, the assumption in equation (28) indicates for sufficiently small \(\varepsilon>0\) and a fixed constant \(C_{1}>0\) which we will choose later, we can find a mixing measure \(G_{*}^{\prime}\in\mathcal{G}_{L^{\prime}}(\Theta)\) such that \(\mathcal{L}_{2,r}(G_{*}^{\prime},G_{*})=2\varepsilon\) and \(\|f_{G_{*}^{\prime}}-f_{G_{*}}\|_{L^{2}(\mu)}\leq C_{1}\varepsilon\). From Le Cam'slemma [57], as the Voronoi loss function \(\mathcal{L}_{2,r}\) satisfies the weak triangle inequality, we obtain that

\[\inf_{\overline{G}_{n}\in\mathcal{G}_{L^{\prime}}(\Theta)} \sup_{G\in\mathcal{G}_{L^{\prime}}(\Theta)\setminus\mathcal{G}_{L-1 }(\Theta)}\mathbb{E}_{f_{G}}[\mathcal{L}_{2,r}(\overline{G}_{n},G)]\] \[\gtrsim\frac{\mathcal{L}_{2,r}(G_{*}^{\prime},G_{*})}{8}\text{ exp}(-n\mathbb{E}_{\bm{X}\sim\mu}[\text{KL}(\mathcal{N}(f_{G_{*}^{\prime}}(\bm{X}), \sigma^{2}),\mathcal{N}(f_{G_{*}}(\bm{X}),\sigma^{2}))])\] \[\gtrsim\varepsilon\cdot\text{exp}(-n\|f_{G_{*}^{\prime}}-f_{G_{* }}\|_{L^{2}(\mu)}^{2}),\] \[\gtrsim\varepsilon\cdot\text{exp}(-C_{1}n\varepsilon^{2}),\] (30)

where the second inequality is due to the fact that

\[\text{KL}(\mathcal{N}(f_{G_{*}^{\prime}}(\bm{X}),\sigma^{2}),\mathcal{N}(f_{ G_{*}}(\bm{X}),\sigma^{2}))=\frac{(f_{G_{*}^{\prime}}(\bm{X})-f_{G_{*}}(\bm{X}))^{2} }{2\sigma^{2}}.\]

By choosing \(\varepsilon=n^{-1/2}\), we obtain that \(\varepsilon\cdot\text{exp}(-C_{1}n\varepsilon^{2})=n^{-1/2}\exp(-C_{1})\). As a consequence, we achieve the desired minimax lower bound in equation (29). 

**Main proof.** We need to prove that the following limit holds true for any \(r\geq 1\):

\[\lim_{\varepsilon\to 0}\inf_{G\in\mathcal{G}_{L^{\prime}}(\Theta): \mathcal{L}_{2,r}(G,G_{*})\leq\varepsilon}\frac{\|f_{G}-f_{G_{*}}\|_{L_{2}(\mu )}}{\mathcal{L}_{2,r}(G,G_{*})}=0.\] (31)

For that purpose, it suffices to build a sequence of mixing measures \((G_{n})_{n\geq 1}\) such that both \(\mathcal{L}_{2,r}(G_{n},G_{*})\to 0\) and

\[\frac{\|f_{G_{n}}-f_{G_{*}}\|_{L_{2}(\mu)}}{\mathcal{L}_{2,r}(G_{n},G_{*})}\to 0,\]

as \(n\to\infty\). To this end, we consider the sequence \(G_{n}=\sum_{i=1}^{L+1}\exp(\beta_{0i}^{n})\delta_{(\beta_{1i}^{n},a_{i}^{n},b_ {i}^{n})}\), where

* \(\exp(\beta_{01}^{n})=\exp(\beta_{02}^{n})=\frac{1}{2}\exp(\beta_{01}^{*})+ \frac{1}{2n^{r+1}}\) and \(\exp(\beta_{0i}^{n})=\exp(\beta_{0(i-1)}^{n})\) for any \(3\leq i\leq L+1\);
* \(\beta_{11}^{n}=\beta_{12}^{*}=\beta_{11}^{*}\) and \(\beta_{1i}^{n}=\beta_{1(i-1)}^{n}\) for any \(3\leq i\leq L+1\);
* \(a_{1}^{n}=a_{2}^{n}=a_{1}^{*}\) and \(a_{i}^{n}=a_{i-1}^{n}\) for any \(3\leq i\leq L+1\);
* \(b_{1}^{n}=b_{1}^{*}+\frac{1}{n}\), \(b_{2}^{n}=b_{1}^{*}-\frac{1}{n}\) and \(b_{i}^{n}=b_{i-1}^{*}\) for any \(3\leq i\leq L+1\).

As a result, the loss function \(\mathcal{L}_{2,r}(G_{n},G_{*})\) is reduced to

\[\mathcal{L}_{2,r}(G_{n},G_{*})=\frac{1}{n^{r+1}}+\Big{[}\exp(\beta_{01}^{*})+ \frac{1}{n^{r+1}}\Big{]}\cdot\frac{1}{n^{r}}=\mathcal{O}(n^{-r}).\] (32)

which indicates indicates that \(\mathcal{L}_{2,r}(G_{n},G_{*})\to 0\) as \(n\to\infty\).

Now, we prove that \(\|f_{G_{n}}-f_{G_{*}}\|_{L_{2}(\mu)}/\mathcal{L}_{2,r}(G_{n},G_{*})\to 0\). For that purpose, let us consider the quantity

\[Q_{n}(\bm{X}):=\Big{[}\sum_{i^{\prime}=1}^{N}\exp(\bm{X}^{\top}B_{i^{\prime}}^ {0}\bm{X}+c_{i^{\prime}}^{0})+\sum_{j^{\prime}=1}^{L}\exp((\beta_{1j^{\prime}}^ {*})^{\top}\bm{X}+\beta_{0j^{\prime}}^{*})\Big{]}\cdot[g_{G_{n}}(\bm{X})-g_{G_ {*}}(\bm{X})].\]

For simplicity, let us consider the polynomial degree \(p=1\) as the arguments for other values of \(p\) can be adapted accordingly. Recall from equation (46) that \(Q_{n}(\bm{X})\) can be decomposed as follows:

\[Q_{n}(\bm{X}) =\sum_{j=1}^{L}\sum_{i\in\mathcal{A}_{j}}\exp(\beta_{0i}^{n}) \Big{[}\exp((\beta_{1i}^{n})^{\top}\bm{X})((a_{i}^{n})^{\top}\bm{X}+b_{i}^{n})- \exp((\beta_{1j}^{*})^{\top}\bm{X})((a_{j}^{*})^{\top}\bm{X}+b_{j}^{*})\Big{]}\] \[-\sum_{j=1}^{L}\sum_{i\in\mathcal{A}_{j}}\exp(\beta_{0i}^{n}) \Big{[}\exp((\beta_{1i}^{n})^{\top}\bm{X})g_{G_{n}}(\bm{X})-\exp((\beta_{1j}^{ *})^{\top}\bm{X})g_{G_{n}}(\bm{X})\Big{]}\] \[+\sum_{j=1}^{L}\Big{(}\sum_{i\in\mathcal{A}_{j}}\exp(\beta_{0i}^{ n})-\exp(\beta_{0j}^{*})\Big{)}\Big{[}\exp((\beta_{1j}^{*})^{\top}\bm{X})((a_{j}^{*})^{ \top}\bm{X}+b_{j}^{*})-\exp((\beta_{1j}^{*})^{\top}\bm{X})g_{G_{n}}(\bm{X}) \Big{]}\] \[:=A_{n}(\bm{X})-B_{n}(\bm{X})+C_{n}(\bm{X}).\]From the definitions of \(\beta_{1i}^{n},a_{i}^{n}\) and \(b_{i}^{n}\), we can rewrite \(A_{n}(\bm{X})\) as follows:

\[A_{n}(\bm{X}) =\sum_{i=1}^{2}\frac{1}{2}\Big{[}\exp(\beta_{01}^{*})+\frac{1}{n^{ r+1}}\Big{]}\exp((\beta_{11}^{*})^{\top}\bm{X})[((a_{i}^{n})^{\top}\bm{X}+b_{i}^{n}) -((a_{1}^{*})^{\top}\bm{X}+b_{1}^{*})]\] \[=\frac{1}{2}\Big{[}\exp(\beta_{01}^{*})+\frac{1}{n^{r+1}}\Big{]} \exp((\beta_{11}^{*})^{\top}\bm{X})[(b_{1}^{n}-b_{1}^{*})+(b_{2}^{n}-b_{1}^{*})]\] \[=0.\]

Additionally, it can also be checked that \(B_{n}(\bm{X})=0\), and \(C_{n}(\bm{X})=\mathcal{O}(n^{-(r+1)})\). Therefore, it follows that \(C_{n}(\bm{X})/\mathcal{L}_{2,r}(G_{n},G_{*})\to 0\). As a consequence, \(Q_{n}(\bm{X})/\mathcal{L}_{2,r}(G_{n},G_{*})\to 0\) as \(n\to\infty\) for almost every \(\bm{X}\).

Since the term \(\Big{[}\sum_{i^{\prime}=1}^{N}\exp(\bm{X}^{\top}B_{i^{\prime}}^{0}\bm{X}+c_{i^ {\prime}}^{0})+\sum_{j^{\prime}=1}^{L}\exp((\beta_{1j^{\prime}}^{*})^{\top}\bm {X}+\beta_{0j^{\prime}}^{*})\Big{]}\) is bounded, we deduce that \([f_{G_{n}}(\bm{X})-f_{G_{*}}(\bm{X})]/\mathcal{L}_{2,r}\to 0\) for almost every \(\bm{X}\). This result indicates that

\[\|f_{G_{n}}-f_{G_{*}}\|_{L_{2}(\mu)}/\mathcal{L}_{2,r}(G_{n},G_{*} )\to 0\]

as \(n\to\infty\). Hence, the proof of claim (31) is completed. 

## Appendix B Proof of Theoretical Results

In this appendix, we present rigorous proofs for the theoretical results introduced in Section 4, namely Theorem 4.1 and Theorem 4.3, in that order.

### Proof of Theorem 4.1

For the proof of the theorem, we first introduce some notation. Firstly, we denote by \(\mathcal{F}_{L^{\prime}}(\Theta)\) the set of conditional densities of all mixing measures in \(\mathcal{G}_{L^{\prime}}(\Theta)\), that is, \(\mathcal{F}_{L^{\prime}}(\Theta):=\{g_{G}(\bm{X}):G\in\mathcal{G}_{L^{\prime}} (\Theta)\}\). Additionally, for each \(\delta>0\), the \(L^{2}(\mu)\) ball centered around the conditional density \(g_{G_{*}}(Y|X)\) and intersected with the set \(\mathcal{F}_{L^{\prime}}(\Theta)\) is defined as

\[\mathcal{F}_{L^{\prime}}(\Theta,\delta):=\left\{g\in\mathcal{F}_{L^{\prime}}( \Theta):\|g-g_{G_{*}}\|_{L^{2}(\mu)}\leq\delta\right\}.\]

In order to measure the size of the above set, Geer et. al. [45] suggest using the following quantity:

\[\mathcal{J}_{B}(\delta,\mathcal{F}_{L^{\prime}}(\Theta,\delta)):= \int_{\delta^{2}/2^{13}}^{\delta}H_{B}^{1/2}(t,\mathcal{F}_{L^{\prime}}( \Theta,t),\|\cdot\|_{L^{2}(\mu)})\;\mathrm{d}t\vee\delta,\] (33)

where \(H_{B}(t,\mathcal{F}_{L^{\prime}}(\Theta,t),\|\cdot\|_{L^{2}(\mu)})\) stands for the bracketing entropy [45] of \(\mathcal{F}_{L^{\prime}}(\Theta,u)\) under the \(L^{2}(\mu)\)-norm, and \(t\vee\delta:=\max\{t,\delta\}\). By using the similar proof argument of Theorem 7.4 and Theorem 9.2 in [45] with notations being adapted to this work, we obtain the following lemma:

**Lemma B.1**.: _Take \(\Psi(\delta)\geq\mathcal{J}_{B}(\delta,\mathcal{F}_{L^{\prime}}(\Theta,\delta))\) that satisfies \(\Psi(\delta)/\delta^{2}\) is a non-increasing function of \(\delta\). Then, for some universal constant \(c\) and for some sequence \((\delta_{n})\) such that \(\sqrt{n}\delta_{n}^{2}\geq c\Psi(\delta_{n})\), we achieve that_

\[\mathbb{P}\Big{(}\|g_{\widehat{G}_{n}}-g_{G_{*}}\|_{L^{2}(\mu)} >\delta\Big{)}\leq c\exp\left(-\frac{n\delta^{2}}{c^{2}}\right),\]

_for all \(\delta\geq\delta_{n}\)._

We now demonstrate that when the expert functions are Lipschitz continuous, the following bound holds:

\[H_{B}(\varepsilon,\mathcal{F}_{L^{\prime}}(\Theta),\|\cdot\|_{L^{2}(\mu)}) \lesssim\log(1/\varepsilon),\] (34)

for any \(0<\varepsilon\leq 1/2\). Indeed, for any function \(g_{G}\in\mathcal{F}_{L^{\prime}}(\Theta)\), since the expert functions are bounded, we obtain that \(h(\bm{X},\eta)\leq M\) for all \(\bm{X}\) where \(M\) is a bounded constant of the expert functions. Let \(\tau\leq\varepsilon\) and \(\{\pi_{1},\ldots,\pi_{\bar{N}}\}\) be the \(\zeta\)-cover under the \(L^{\infty}\) norm of the set \(\mathcal{F}_{L^{\prime}}(\Theta)\) where \(\bar{N}:=N(\zeta,\mathcal{F}_{L^{\prime}}(\Theta),\|\cdot\|_{L^{\infty}})\) is the \(\eta\)-covering number of the metric space \((\mathcal{F}_{L^{\prime}}(\Theta),\|\cdot\|_{L^{\infty}})\). Then, we construct the brackets of the form \([L_{i}(\bm{X}),U_{i}(\bm{X})]\) for all \(i\in[\bar{N}]\) as follows:

\[L_{i}(x) :=\max\{\pi_{i}(\bm{X})-\zeta,0\},\] \[U_{i}(x) :=\max\{\pi_{i}(\bm{X})+\zeta,M\}.\]From the above construction, we can validate that \(\mathcal{F}_{L^{\prime}}(\Theta)\subset\cup_{i=1}^{N}[L_{i}(\bm{X}),U_{i}(\bm{X})]\) and \(U_{i}(\bm{X})-L_{i}(\bm{X})\leq\min\{2\zeta,M\}\). Therefore, it follows that

\[\|U_{i}-L_{i}\|_{L_{2}(\mu)}^{2}=\int(U_{i}-L_{i})^{2}\mathrm{d}\mu(\bm{X}) \leq\int 4\zeta^{2}\mathrm{d}\mu(\bm{X})=4\zeta^{2},\]

which implies that \(\|U_{i}-L_{i}\|_{L_{2}(\mu)}\leq 2\zeta\). By definition of the bracketing entropy, we deduce that

\[H_{B}(2\zeta,\mathcal{F}_{L^{\prime}}(\Theta),\|\cdot\|_{L_{2}(\mu)})\leq\log N =\log N(\zeta,\mathcal{F}_{L^{\prime}}(\Theta),\|\cdot\|_{L^{\infty}}).\] (35)

Therefore, we need to provide an upper bound for the covering number \(\bar{N}\). In particular, we denote \(\Delta:=\{(\beta_{1},\beta_{0})\in\mathbb{R}^{Nd\times Nd}\times\mathbb{R}^{ Nd}\times\mathbb{R}:(\beta_{1},\beta_{0},\eta)\in\Theta\}\) and \(\Omega:=\{\eta\in\mathbb{R}^{q}:(\beta_{1},\beta_{0},\eta)\in\Theta\}\). Since \(\Theta\) is a compact set, \(\Delta\) and \(\Omega\) are also compact. Therefore, we can find \(\zeta\)-covers \(\Delta_{\zeta}\) and \(\Omega_{\zeta}\) for \(\Delta\) and \(\Omega\), respectively. We can check that

\[|\Delta_{\zeta}|\leq\mathcal{O}_{P}(\tau^{-(Nd+1)L^{\prime}}),\quad|\Omega_{ \zeta}|\lesssim\mathcal{O}_{P}(\tau^{-qL^{\prime}}).\]

For each mixing measure \(G=\sum_{i=1}^{L^{\prime}}\exp(\beta_{0i})\delta_{(\beta_{1i},\eta_{i})}\in \mathcal{G}_{L^{\prime}}(\Theta)\), we consider other two mixing measures:

\[\check{G}:=\sum_{i=1}^{L^{\prime}}\exp(\beta_{0i})\delta_{(\beta_{1i},\overline {\eta}_{i})},\qquad\overline{G}:=\sum_{i=1}^{L^{\prime}}\exp(\overline{\beta}_ {0i})\delta_{(\overline{\beta}_{1i},\overline{\eta}_{i})}.\]

Here, \(\overline{\eta}_{i}\in\Omega_{\zeta}\) such that \(\overline{\eta}_{i}\) is the closest to \(\eta_{i}\) in that set, while \((\overline{\beta}_{1i},\overline{\beta}_{0i})\in\Delta_{\zeta}\) is the closest to \((\beta_{1i},\beta_{0i})\) in that set. From the above formulations, we get that

\[\|g_{G}-g_{\check{G}}\|_{L^{\infty}}\] \[=\sup_{\bm{X}\in\mathcal{X}}\ \sum_{j=1}^{L^{\prime}}\frac{\exp( \beta_{1j}^{\top}\bm{X}+\alpha\sigma(\tau\beta_{1j}^{\top}\bm{X})+\beta_{0j}) \cdot|h(\bm{X},\eta_{j})-h(\bm{X},\overline{\eta}_{j})|}{\sum_{i^{\prime}=1}^{ N}\exp(\bm{X}^{\top}B_{i^{\prime}}^{0}\bm{X}+c_{i^{\prime}}^{0})+\sum_{j^{ \prime}=1}^{L^{\prime}}\exp(\beta_{1j^{\prime}}^{\top}\bm{X}+\alpha\sigma( \tau\beta_{1j^{\prime}}^{\top}\bm{x})+\beta_{0j^{\prime}})}\] \[\leq\sum_{j=1}^{L^{\prime}}\sup_{\bm{X}\in\mathcal{X}}\ \frac{\exp(\beta_{1j}^{\top}\bm{X}+\alpha\sigma(\tau\beta_{1j}^{\top}\bm{X})+ \beta_{0j})\cdot|h(\bm{X},\eta_{j})-h(\bm{X},\overline{\eta}_{j})|}{\sum_{i^{ \prime}=1}^{N}\exp(\bm{X}^{\top}B_{i^{\prime}}^{0}\bm{X}+c_{i^{\prime}}^{0})+ \sum_{j^{\prime}=1}^{L^{\prime}}\exp(\beta_{1j^{\prime}}^{\top}\bm{X}+\alpha \sigma(\tau\beta_{1j^{\prime}}^{\top}\bm{X})+\beta_{0j^{\prime}})}\] \[\leq\sum_{j=1}^{L^{\prime}}\sup_{\bm{X}\in\mathcal{X}}\ |h(\bm{X},\eta_{j})-h(\bm{X},\overline{\eta}_{j})|\] \[\leq\sum_{j=1}^{L^{\prime}}\sup_{\bm{X}\in\mathcal{X}}\ [L_{1}(\bm{X})\cdot\|\eta_{j}- \overline{\eta}_{j}\|]\] \[\lesssim L^{\prime}\zeta\lesssim\zeta.\]

Here, the second inequality occurs as the softmax weight is bounded by one, and the third inequality follows from the fact that the expert \(h(\bm{X},\cdot)\) is a Lipschitz function with some Lipschitz constant \(L_{1}(\bm{X})>0\). Next, let us denote

\[D: =\sum_{i^{\prime}=1}^{N}\exp(\bm{X}^{\top}B_{i^{\prime}}^{0}\bm{X} +c_{i^{\prime}}^{0})+\sum_{j^{\prime}=1}^{L^{\prime}}\exp(\beta_{1j^{\prime}}^ {\top}\bm{X}+\alpha\sigma(\tau\beta_{1j^{\prime}}^{\top}\bm{X})+\beta_{0j^{ \prime}}),\] \[\overline{D}: =\sum_{i^{\prime}=1}^{N}\exp(\bm{X}^{\top}B_{i^{\prime}}^{0}\bm{X }+c_{i^{\prime}}^{0})+\sum_{j^{\prime}=1}^{L^{\prime}}\exp(\overline{\beta}_{1 j^{\prime}}^{\top}\bm{X}+\alpha\sigma(\tau\overline{\beta}_{1j^{\prime}}^{\top}\bm{X})+ \overline{\beta}_{0j^{\prime}}).\]Then, we have

\[\|g_{\bar{G}}-g_{\bar{G}}\|_{L^{\infty}}\] \[=\sup_{\bm{X}\in\mathcal{X}}\bigg{|}\frac{1}{D}\Big{(}\sum_{i=1}^{N }\exp(\bm{X}^{\top}B_{i}^{0}\bm{X}+c_{i}^{0})h(\bm{X},\eta_{i}^{0})+\sum_{j=1}^{ L^{\prime}}\exp(\beta_{1j}^{\top}\bm{X}+\alpha\sigma(\tau\beta_{1j}^{\top}\bm{X}) +\beta_{0j})h(\bm{X},\overline{\eta}_{j})\Big{)}\] \[\quad-\frac{1}{\overline{D}}\Big{(}\sum_{i=1}^{N}\exp(\bm{X}^{ \top}B_{i}^{0}\bm{X}+c_{i}^{0})h(\bm{X},\eta_{i}^{0})+\sum_{j=1}^{L^{\prime}} \exp(\overline{\beta}_{1j}^{\top}\bm{X}+\alpha\sigma(\tau\overline{\beta}_{1j} ^{\top}\bm{X})+\overline{\beta}_{0j})h(\bm{X},\overline{\eta}_{j})\Big{)}\bigg{|}\] \[\leq\Big{|}\frac{1}{D}-\frac{1}{\overline{D}}\Big{|}\cdot\sum_{i= 1}^{N}\sup_{\bm{X}\in\mathcal{X}}\ \Big{|}\exp(\bm{X}^{\top}B_{i}^{0}\bm{X}+c_{i}^{0})h(\bm{X},\eta_{i}^{0}) \Big{|}\] \[\quad+\sum_{j=1}^{L^{\prime}}\sup_{\bm{X}\in\mathcal{X}}\ \bigg{|}\frac{\exp(\beta_{1j}^{\top}\bm{X}+\alpha\sigma(\tau\beta_{1j}^{\top} \bm{X})+\beta_{0j})}{D}-\frac{\exp(\overline{\beta}_{1j}^{\top}\bm{X}+\alpha \sigma(\tau\overline{\beta}_{1j}^{\top}\bm{X})+\overline{\beta}_{0j})}{ \overline{D}}\bigg{|}\cdot|h(\bm{X},\overline{\eta}_{j})|.\] (36)

Now, we will bound two terms in the above right hand side. Firstly, since both the input space \(\mathcal{X}\) and the parameter space \(\Theta\) are bounded, we have that

\[\frac{1}{D}-\frac{1}{\overline{D}} \lesssim|D-\overline{D}|\] \[\leq\sum_{j^{\prime}=1}^{L^{\prime}}\Big{|}\exp(\beta_{1j^{\prime }}^{\top}\bm{X}+\alpha\sigma(\tau\beta_{1j^{\prime}}^{\top}\bm{X})+\beta_{0j^{ \prime}})-\exp(\overline{\beta}_{1j^{\prime}}^{\top}\bm{X}+\alpha\sigma(\tau \overline{\beta}_{1j^{\prime}}^{\top}\bm{X})+\overline{\beta}_{0j^{\prime}}) \Big{|}\] \[\lesssim\sum_{j^{\prime}=1}^{L^{\prime}}\Big{|}(\beta_{1j}- \overline{\beta}_{1j})^{\top}\bm{X}+\alpha[\sigma(\tau\beta_{1j^{\prime}}^{ \top}\bm{X})-\sigma(\tau\overline{\beta}_{1j^{\prime}}^{\top}\bm{X})]+(\beta_{ 0j}-\overline{\beta}_{0j})\Big{|}\] \[\leq\sum_{j^{\prime}=1}^{L^{\prime}}|(\beta_{1j}-\overline{\beta }_{1j})^{\top}\bm{X}|+|\alpha|\cdot|\sigma(\tau\beta_{1j^{\prime}}^{\top}\bm{X })-\sigma(\tau\overline{\beta}_{1j^{\prime}}^{\top}\bm{X})|+|\beta_{0j}- \overline{\beta}_{0j}|\] \[\lesssim\sum_{j=1}^{L^{\prime}}\Big{[}\|\beta_{1j}-\overline{ \beta}_{1j}\|\cdot\|\bm{X}\|+|\alpha\tau|\cdot\|\beta_{1j}-\overline{\beta}_{1j }\|\cdot\|\bm{X}\|+|\beta_{0j}-\overline{\beta}_{0j}|\Big{]}\] \[\leq L^{\prime}(B+|\alpha\tau|B+1)\zeta\lesssim\zeta.\]

As a result, we deduce that

\[\Big{|}\frac{1}{D}-\frac{1}{\overline{D}}\Big{|}\cdot\sum_{i=1}^{N}\sup_{\bm{X }\in\mathcal{X}}\ \Big{|}\exp(\bm{X}^{\top}B_{i}^{0}\bm{X}+c_{i}^{0})h(\bm{X},\eta_{i}^{0}) \Big{|}\lesssim\zeta.\] (37)

Regarding the second term, note that

\[\frac{\exp(\beta_{1j}^{\top}\bm{X}+\alpha\sigma(\tau\beta_{1j}^{ \top}\bm{X})+\beta_{0j})}{D}-\frac{\exp(\overline{\beta}_{1j}^{\top}\bm{X}+ \alpha\sigma(\tau\overline{\beta}_{1j}^{\top}\bm{X})+\overline{\beta}_{0j})}{ \overline{D}}\] \[=\exp(\beta_{1j}^{\top}\bm{X}+\alpha\sigma(\tau\beta_{1j}^{\top }\bm{X})+\beta_{0j})\Big{(}\frac{1}{D}-\frac{1}{\overline{D}}\Big{)}\] \[\qquad+\frac{1}{\overline{D}}\Big{[}\exp(\beta_{1j}^{\top}\bm{X} +\alpha\sigma(\tau\beta_{1j}^{\top}\bm{X})+\beta_{0j})-\exp(\exp(\overline{ \beta}_{1j}^{\top}\bm{X}+\alpha\sigma(\tau\overline{\beta}_{1j}^{\top}\bm{X})+ \overline{\beta}_{0j}))\Big{]}.\]

Since both the input space and the parameter space are bounded, we have

\[\exp(\beta_{1j}^{\top}\bm{X}+\alpha\sigma(\tau\beta_{1j}^{\top}\bm {X})+\beta_{0j})\Big{(}\frac{1}{D}-\frac{1}{\overline{D}}\Big{)}\lesssim\frac{1} {D}-\frac{1}{\overline{D}}\lesssim\zeta,\] \[\frac{1}{\overline{D}}\Big{[}\exp(\beta_{1j}^{\top}\bm{X}+\alpha \sigma(\tau\beta_{1j}^{\top}\bm{X})+\beta_{0j})-\exp(\overline{\beta}_{1j}^{ \top}\bm{X}+\alpha\sigma(\tau\overline{\beta}_{1j}^{\top}\bm{X})+\overline{ \beta}_{0j})\] \[\lesssim(B+|\alpha\tau|B+1)\zeta\lesssim\zeta,\]which yields that

\[\sum_{j=1}^{L^{\prime}}\sup_{\bm{X}\in\mathcal{X}}\left|\frac{\exp( \beta_{1j}^{\top}\bm{X}+\alpha\sigma(\tau\beta_{1j}^{\top}\bm{X})+\beta_{0j})}{D }-\frac{\exp(\overline{\beta}_{1j}^{\top}\bm{X}+\alpha\sigma(\tau\overline{ \beta}_{1j}^{\top}\bm{X})+\overline{\beta}_{0j})}{\overline{D}}\right|\cdot|h( \bm{X},\overline{\eta}_{j})|\] \[\lesssim\zeta\,\sum_{j=1}^{L^{\prime}}\sup_{\bm{X}\in\mathcal{X}} \,|h(\bm{X},\overline{\eta}_{j})|\lesssim\zeta.\] (38)

From equations (36), (37) and (38), we obtain that \(\|g_{\bar{G}}-g_{\bar{G}}\|_{L^{\infty}}\lesssim\zeta\). According to the triangle inequality, we have

\[\|g_{G}-g_{\bar{G}}\|_{L^{\infty}}\leq\|g_{G}-g_{\bar{G}}\|_{L^{\infty}}+\|g_{ \bar{G}}-g_{\bar{G}}\|_{L^{\infty}}\lesssim\zeta.\]

By definition of the covering number, we deduce that

\[N(\zeta,\mathcal{F}_{L^{\prime}}(\Theta),\|\cdot\|_{L^{\infty}})\leq|\Delta_{ \zeta}|\times|\Omega_{\zeta}|\leq\mathcal{O}(n^{-(Nd+1)L^{\prime}})\times \mathcal{O}(n^{-qL^{\prime}})\leq\mathcal{O}(n^{-(Nd+1+q)L^{\prime}}).\] (39)

Combine equations (35) and (39), we achieve that

\[H_{B}(2\zeta,\mathcal{F}_{L^{\prime}}(\Theta),\|\cdot\|_{L_{2}(\mu)})\lesssim \log(1/\tau).\]

Let \(\zeta=\varepsilon/2\), then we obtain that

\[H_{B}(\varepsilon,\mathcal{F}_{L^{\prime}}(\Theta),\|\cdot\|_{L^{2}(\mu)}) \lesssim\log(1/\varepsilon).\]

As a result, it follows that

\[\mathcal{J}_{B}(\delta,\mathcal{F}_{L^{\prime}}(\Theta,\delta))=\int_{\delta ^{2}/2^{13}}^{\delta}H_{B}^{1/2}(t,\mathcal{F}_{L^{\prime}}(\Theta,t),\|\cdot \|_{L_{2}(\mu)})\,\mathrm{d}t\vee\delta\lesssim\int_{\delta^{2}/2^{13}}^{ \delta}\log(1/t)dt\vee\delta.\] (40)

Let \(\Psi(\delta)=\delta\cdot[\log(1/\delta)]^{1/2}\), then \(\Psi(\delta)/\delta^{2}\) is a non-increasing function of \(\delta\). Furthermore, equation (40) indicates that \(\Psi(\delta)\geq\mathcal{J}_{B}(\delta,\mathcal{F}_{L^{\prime}}(\Theta,\delta))\). In addition, let \(\delta_{n}=\sqrt{\log(n)/n}\), then we get that \(\sqrt{n}\delta_{n}^{2}\geq c\Psi(\delta_{n})\) for some universal constant \(c\). Finally, by applying Lemma B.1, we achieve the desired conclusion of the theorem.

### Proof of Theorem 4.3

Our goal is also to demonstrate the following inequality:

\[\inf_{G\in\mathcal{G}_{L^{\prime}}(\Theta)}\|g_{G}-g_{G_{*}}\|_{L_{2}(\mu)}/ \mathcal{L}_{1}(G,G_{*})>0.\] (41)

For that purpose, we divide the proof of the above inequality into local and global parts in the sequel.

**Local part:** In this part, we demonstrate that

\[\lim_{\varepsilon\to 0}\inf_{G\in\mathcal{G}_{L^{\prime}}(\Theta):\mathcal{L}_{1}(G, G_{*})\leq\varepsilon}\|g_{G}-g_{G_{*}}\|_{L_{2}(\mu)}/\mathcal{L}_{1}(G,G_{*})>0.\] (42)

Assume by contrary that the above claim is not true, then there exists a sequence of mixing measures \(G_{n}=\sum_{i=1}^{L}\exp(\beta_{0i}^{n})\delta_{(\beta_{1i}^{n},\eta_{i}^{n})}\) in \(\mathcal{G}_{L^{\prime}}(\Theta)\) such that \(\mathcal{L}_{1n}:=\mathcal{L}_{1}(G_{n},G_{*})\to 0\) and

\[\|g_{G_{n}}-g_{G_{*}}\|_{L_{2}(\mu)}/\mathcal{L}_{1n}\to 0,\] (43)

as \(n\to\infty\). Let us denote by \(\mathcal{V}_{j}^{n}:=\mathcal{V}_{j}(G_{n})\) a Voronoi cell of \(G_{n}\) generated by the \(j\)-th components of \(G_{*}\). Since our arguments are asymptotic, we may assume that those Voronoi cells do not depend on the sample size, i.e., \(\mathcal{V}_{j}=\mathcal{V}_{j}^{n}\). Thus, the Voronoi loss \(\mathcal{L}_{1n}\) can be represented as

\[\mathcal{L}_{1n}:=\sum_{j:|\mathcal{V}_{j}|>1}\sum_{i\in\mathcal{V }_{j}}\exp(\beta_{0i}^{n})\Big{[}\|\Delta\beta_{1ij}^{n}\|^{2}+\|\Delta\eta_{ ij}^{n}\|^{2}\Big{]}\] \[+\sum_{j:|\mathcal{V}_{j}|=1}\sum_{i\in\mathcal{V}_{j}}\exp( \beta_{0i}^{n})\Big{[}\|\Delta\beta_{1ij}^{n}\|+\|\Delta\eta_{ij}^{n}\|\Big{]} +\sum_{j=1}^{k_{*}}\Big{|}\sum_{i\in\mathcal{V}_{j}}\exp(\beta_{1i}^{n})-\exp (\beta_{1j}^{*})\Big{|},\] (44)where we denote \(\Delta\beta^{n}_{1ij}:=\beta^{n}_{1i}-\beta^{n}_{1j}\) and \(\Delta\eta^{n}_{ij}:=\eta^{n}_{i}-\eta^{*}_{j}\).

Since \(\mathcal{L}_{1n}\to 0\), we get that \((\beta^{n}_{1i},\eta^{n}_{i})\rightarrow(\beta^{*}_{1j},\eta^{*}_{j})\) and \(\sum_{i\in\mathcal{V}_{j}}\exp(\beta^{n}_{0i})\rightarrow\exp(\beta^{*}_{0j})\) as \(n\rightarrow\infty\) for any \(i\in\mathcal{V}_{j}\) and \(j\in[L]\). Now, we divide the proof of the local part into three steps as follows:

**Step 1 - Taylor expansion.** In this step, we would like to decompose the quantity

\[Q_{n}(\bm{X}):=\Big{[}\sum_{i^{\prime}=1}^{N}\exp(\bm{X}^{\top}A ^{0}_{i^{\prime}}\bm{X}+c^{0}_{i^{\prime}})+\sum_{j^{\prime}=1}^{L}\exp((\beta^ {*}_{1j^{\prime}})^{\top}\bm{X}+\alpha\sigma(\tau(\beta^{*}_{1j^{\prime}})^{ \top}\bm{X})+\beta^{*}_{0j^{\prime}})\Big{]}\\ \times[g_{G_{n}}(\bm{X})-g_{G_{*}}(\bm{X})]\] (45)

into a combination of linearly independent elements using Taylor expansion. In particular, the quantity \(Q_{n}(\bm{X})\) is decomposed as follows:

\[\sum_{j=1}^{L}\sum_{i\in\mathcal{V}_{j}}\exp(\beta^{n}_{0i})\Big{[} \exp((\beta^{n}_{1i})^{\top}\bm{X}+\alpha\sigma(\tau(\beta^{n}_{1i})^{\top}\bm {X}))h(\bm{X};\eta^{n}_{i})-\exp((\beta^{*}_{1j})^{\top}\bm{X}+\alpha\sigma( \tau(\beta^{*}_{1j})^{\top}\bm{X}))h(\bm{X};\eta^{*}_{j})\Big{]}\] \[-\sum_{j=1}^{L}\sum_{i\in\mathcal{V}_{j}}\exp(\beta^{n}_{0i}) \Big{[}\exp((\beta^{n}_{1i})^{\top}\bm{X}+\alpha\sigma(\tau(\beta^{n}_{1i})^{ \top}\bm{X}))-\exp((\beta^{*}_{1j})^{\top}\bm{X}+\alpha\sigma(\tau(\beta^{*}_{ 1j})^{\top}\bm{X}))\Big{]}g_{G_{n}}(\bm{X})\] \[+\sum_{j=1}^{L}\Big{(}\sum_{i\in\mathcal{V}_{j}}\exp(\beta^{n}_{0 i})-\exp(\beta^{*}_{0j})\Big{)}\exp((\beta^{*}_{1j})^{\top}\bm{X}+\alpha\sigma( \tau(\beta^{*}_{1j})^{\top}\bm{X}))\Big{[}h(\bm{X};\eta^{*}_{j})-g_{G_{n}}(\bm{ X})\Big{]}\] \[:=A_{n}(\bm{X})-B_{n}(\bm{X})+C_{n}(\bm{X}).\] (46)

**Decomposition of \(A_{n}(\bm{X})\).** Let us denote \(E(\bm{X};\beta_{1}):=\exp(\beta^{\top}_{1}\bm{X}+\alpha\sigma(\tau\beta^{\top }_{1}\bm{X}))\), then \(A_{n}\) can be separated into two terms as follows:

\[A_{n}(\bm{X}) :=\sum_{j:|\mathcal{V}_{j}|=1}\sum_{i\in\mathcal{V}_{j}}\exp( \beta^{n}_{0i})\Big{[}E(\bm{X};\beta^{n}_{1i})h(x;\eta^{n}_{i})-E(\bm{X};\beta ^{*}_{1j})h(\bm{X};\eta^{*}_{j})\Big{]}\] \[+\sum_{j:|\mathcal{V}_{j}|>1}\sum_{i\in\mathcal{V}_{j}}\exp(\beta^ {n}_{0i})\Big{[}E(\bm{X};\beta^{n}_{1i})h(\bm{X};\eta^{n}_{i})-E(\bm{X};\beta ^{*}_{1j})h(\bm{X};\eta^{*}_{j})\Big{]}\] \[:=A_{n,1}(\bm{X})+A_{n,2}(\bm{X}).\]

By means of the first-order Taylor expansion, we have

\[A_{n,1}(\bm{X}) =\sum_{j:|\mathcal{V}_{j}|=1}\sum_{i\in\mathcal{V}_{j}}\frac{\exp (\beta^{n}_{0i})}{\alpha!}\sum_{|\alpha|=1}(\Delta\beta^{n}_{1ij})^{\alpha_{1} }(\Delta\eta^{n}_{ij})^{\alpha_{2}}\frac{\partial^{|\alpha_{1}|}E}{\partial \beta^{\alpha_{1}}_{1}}(\bm{X};\beta^{*}_{1j})\frac{\partial^{|\alpha_{2}|}h}{ \partial\eta^{\alpha_{2}}}(\bm{X};\eta^{*}_{j})+R_{n,1}(\bm{X})\] \[=\sum_{j:|\mathcal{V}_{j}|=1}\sum_{|\alpha_{1}|+|\alpha_{2}|=1}S_ {n,j,\alpha_{1},\alpha_{2}}\frac{\partial^{|\alpha_{1}|}E}{\partial\beta^{ \alpha_{1}}_{1}}(\bm{X};\beta^{*}_{1j})\frac{\partial^{|\alpha_{2}|}h}{ \partial\eta^{\alpha_{2}}}(\bm{X};\eta^{*}_{j})+R_{n,1}(\bm{X}),\]

where \(R_{n,1}(\bm{X})\) is a Taylor remainder such that \(R_{n,1}(\bm{X})/\mathcal{L}_{1n}\to 0\) as \(n\rightarrow\infty\), and

\[S_{n,j,\alpha_{1},\alpha_{2}}:=\sum_{i\in\mathcal{V}_{j}}\frac{\exp(\beta^{n}_{0 i})}{\alpha!}(\Delta\beta^{n}_{1ij})^{\alpha_{1}}(\Delta\eta^{n}_{ij})^{\alpha_{2}}.\]

On the other hand, by applying the second-order Taylor expansion, we get that

\[A_{n,2}(\bm{X})=\sum_{j:|\mathcal{V}_{j}|>1}\sum_{1\leq|\alpha_{1}|+|\alpha_{2}| \leq 2}S_{n,j,\alpha_{1},\alpha_{2}}\frac{\partial^{|\alpha_{1}|}E}{\partial \beta^{\alpha_{1}}_{1}}(\bm{X};\beta^{*}_{1j})\frac{\partial^{|\alpha_{2}|}h}{ \partial\eta^{\alpha_{2}}}(\bm{X};\eta^{*}_{j})+R_{n,2}(\bm{X}),\]

in which \(R_{n,2}(\bm{X})\) is a Taylor remainder such that \(R_{n,2}(\bm{X})/\mathcal{L}_{1n}\to 0\) as \(n\rightarrow\infty\).

**Decomposition of \(B_{n}\).** Recall that we have

\[B_{n}(\bm{X}) =\sum_{j:|\mathcal{V}_{j}|=1}\sum_{i\in\mathcal{V}_{j}}\exp(\beta^ {n}_{0i})\Big{[}E(\bm{X};\beta^{n}_{1i})-E(\bm{X};\beta^{*}_{1j})\Big{]}g_{G_{n} }(\bm{X})\] \[+\sum_{j:|\mathcal{V}_{j}|>1}\sum_{i\in\mathcal{V}_{j}}\exp(\beta^ {n}_{0i})\Big{[}E(\bm{X};\beta^{n}_{1i})-E(x;\beta^{*}_{1j})\Big{]}g_{G_{n} }(\bm{X})\] \[:=B_{n,1}(\bm{X})+B_{n,2}(\bm{X}).\]By invoking first-order and second-order Taylor expansions to \(B_{n,1}(\bm{X})\) and \(B_{n,2}(\bm{X})\), it follows that

\[B_{n,1}(\bm{X}) =\sum_{j:|\mathcal{V}_{j}|=1}\sum_{|\ell|=1}T_{n,j,\ell}\cdot\frac{ \partial^{|\ell|}E}{\partial\beta_{1}^{\ell}}(\bm{X};\beta_{1j}^{*})g_{G_{n}}( \bm{X})+R_{n,3}(\bm{X}),\] \[B_{n,2}(\bm{X}) =\sum_{j:|\mathcal{V}_{j}|>1}\sum_{1\leq|\ell|\leq 2}T_{n,j,\ell} \cdot\frac{\partial^{|\ell|}E}{\partial\beta_{1}^{\ell}}(\bm{X};\beta_{1j}^{*} )g_{G_{n}}(\bm{X})+R_{n,4}(\bm{X}),\]

where we define

\[T_{n,j,\ell}:=\sum_{i\in\mathcal{V}_{j}}\frac{\exp(\beta_{0i}^{ n})}{\ell!}(\Delta\beta_{1ij}^{n})^{\ell}.\]

Additionally, \(R_{n,3}(\bm{X})\) and \(R_{n,4}(\bm{X})\) are Taylor remainders such that \(R_{n,3}(\bm{X})/\mathcal{L}_{1n}\to 0\) and \(R_{n,3}(\bm{X})/\mathcal{L}_{1n}\to 0\) as \(n\to\infty\).

Collect the above results together, we can represent \(Q_{n}(x)\) as

\[Q_{n}(\bm{X}) =\sum_{j=1}^{L}\sum_{0\leq|\alpha_{1}|+|\alpha_{2}|\leq 2}S_{n,j, \alpha_{1},\alpha_{2}}\frac{\partial^{|\alpha_{1}|}E}{\partial\beta_{1}^{ \alpha_{1}}}(\bm{X};\beta_{1j}^{*})\frac{\partial^{|\alpha_{2}|}h}{\partial \eta^{\alpha_{2}}}(\bm{X};\eta_{j}^{*}),\] \[-\sum_{j=1}^{L}\sum_{0\leq|\ell|\leq 2}T_{n,j,\ell}\cdot\frac{ \partial^{|\ell|}E}{\partial\beta_{1}^{\ell}}(\bm{X};\beta_{1j}^{*})g_{G_{n}} (\bm{X})+\sum_{i=1}^{4}R_{n,i}(\bm{X}),\] (47)

where we define \(S_{n,j,\bm{0}_{\delta\times d},\bm{0}_{q}}=T_{n,j,\bm{0}_{d\times d}}=\sum_{i \in\mathcal{V}_{j}}\exp(\beta_{0i}^{n})-\exp(\beta_{0j}^{*})\) for any \(j\in[L]\).

**Step 2 - Non-vanishing coefficients.** In this step, we demonstrate that at least one among ratios of the forms \(S_{n,j,\alpha_{1},\alpha_{2}}/\mathcal{L}_{1n}\) and \(T_{n,j,\ell}/\mathcal{L}_{1n}\) goes to zero as \(n\) tends to infinity. Indeed, assume by contrary that

\[\frac{S_{n,j,\alpha_{1},\alpha_{2}}}{\mathcal{L}_{1n}}\to 0, \qquad\frac{T_{n,j,\ell}}{\mathcal{L}_{1n}}\to 0,\]

for any \(j\in[L]\), \(0\leq|\alpha_{1}|,|\alpha_{2}|,|\ell|\leq 2\). Then, we get

\[\frac{1}{\mathcal{L}_{1n}}\sum_{j=1}^{L}\Big{|}\sum_{i\in\mathcal{V}_{j}}\exp (\beta_{0i}^{n})-\exp(\beta_{0j}^{*})\Big{|}=\sum_{j=1}^{L}\Big{|}\frac{S_{n,j,\bm{0}_{d\times d},\bm{0}_{q}}}{\mathcal{L}_{1n}}\Big{|}\to 0.\] (48)

Now, we consider indices \(j\in[L]\) such that its corresponding Voronoi cell has only one element, i.e. \(|\mathcal{V}_{j}|=1\).

* For arbitrary \(u,v\in[Nd]\), let \(\alpha_{1}\in\mathbb{N}^{Nd\times Nd}\) and \(\alpha_{2}=\bm{0}_{q}\) such that \(\alpha_{1}^{(uv)}=1\) while other entries equal to zero. Then, we have \(\frac{1}{\mathcal{L}_{1n}}\cdot\sum_{i\in\mathcal{V}_{j}}\exp(\beta_{0i}^{n})| (\Delta\beta_{1ij}^{n})^{(uv)}|=|S_{n,j,\alpha_{1},\alpha_{2}}|/\mathcal{L}_{1n}\to 0\) as \(n\to\infty\). By taking the summation of the previous term with \(u,v\in[Nd]\), we achieve that \(\frac{1}{\mathcal{L}_{1n}}\sum_{i\in\mathcal{V}_{j}}\exp(\beta_{0i}^{n})\| \Delta\beta_{1ij}^{n}\|_{1}\to 0\). Owing to the topological equivalence between norm-1 and norm-2, it follows that \[\frac{1}{\mathcal{L}_{1n}}\sum_{i\in\mathcal{V}_{j}}\exp(\beta_{0i}^{n})\| \Delta\beta_{1ij}^{n}\|\to 0.\] (49)
* For arbitrary \(u\in[Nd]\), let \(\alpha_{1}=\bm{0}_{Nd\times Nd}\) and \(\alpha_{2}\in\mathbb{N}^{q}\) such that \(\alpha_{2}^{(u)}=1\) while other entries equal to zero. Then, we get \(\frac{1}{\mathcal{L}_{1n}}\cdot\sum_{i\in\mathcal{V}_{j}}\exp(\beta_{0i}^{n})| (\Delta\eta_{ij}^{n})^{(u)}|=|S_{n,j,\alpha_{1},\alpha_{2}}|/\mathcal{L}_{1n} \to 0\) as \(n\to\infty\). By taking the summation of the previous term with \(u\in[q]\), we achieve that \(\frac{1}{\mathcal{L}_{1n}}\sum_{i\in\mathcal{V}_{j}}\exp(\beta_{0i}^{n})\| \Delta\eta_{ij}^{n}\|_{1}\to 0\), or equivalently, \[\frac{1}{\mathcal{L}_{1n}}\sum_{i\in\mathcal{V}_{j}}\exp(\beta_{0i}^{n})\| \Delta\eta_{ij}^{n}\|\to 0.\] (50)Combine the limits in equations (49) and (50), we obtain that

\[\frac{1}{\mathcal{L}_{1n}}\sum_{j:|\mathcal{V}_{j}|=1}\sum_{i\in\mathcal{V}_{j}} \exp(\beta^{n}_{0i})[\|\Delta\beta^{n}_{1ij}\|+\|\Delta\eta^{n}_{ij}\|]\to 0,\] (51)

as \(n\to\infty\).

Next, we consider indices \(j\in[L]\) such that its corresponding Voronoi cell has more than one element, i.e. \(|\mathcal{V}_{j}|>1\).

* For arbitrary \(u,v\in[Nd]\), let \(\alpha_{1}\in\mathbb{N}^{Nd\times Nd}\) and \(\alpha_{2}=\mathbf{0}_{q}\) such that \(\alpha^{(uv)}_{1}=2\) while other entries equal to zero. Then, we have \(\frac{1}{\mathcal{L}_{1n}}\cdot\sum_{i\in\mathcal{V}_{j}}\exp(\beta^{n}_{0i}) |(\Delta\beta^{n}_{1ij})^{(uv)}|^{2}=|S_{n,j,\alpha_{1},\alpha_{2}}|/\mathcal{ L}_{1n}\to 0\) as \(n\to\infty\). By taking the summation of the previous term with \(u,v\in[Nd]\), we achieve that \[\frac{1}{\mathcal{L}_{1n}}\sum_{i\in\mathcal{V}_{j}}\exp(\beta^{n}_{0i})\| \Delta\beta^{n}_{1ij}\|^{2}\to 0.\] (52)
* For arbitrary \(u\in[Nd]\), let \(\alpha_{1}=\mathbf{0}_{Nd\times Nd}\) and \(\alpha_{2}\in\mathbb{N}^{q}\) such that \(\alpha^{(u)}_{2}=2\) while other entries equal to zero. Then, we get \(\frac{1}{\mathcal{L}_{1n}}\cdot\sum_{i\in\mathcal{V}_{j}}\exp(\beta^{n}_{0i}) |(\Delta\eta^{n}_{ij})^{(u)}|^{2}=|S_{n,j,\alpha_{1},\alpha_{2}}|/\mathcal{L} _{1n}\to 0\) as \(n\to\infty\). By taking the summation of the previous term with \(u\in[q]\), we achieve that \[\frac{1}{\mathcal{L}_{1n}}\sum_{i\in\mathcal{V}_{j}}\exp(\beta^{n}_{0i})\| \Delta\eta^{n}_{ij}\|^{2}\to 0.\] (53)

Putting the limits in equations (49) and (50), we have

\[\frac{1}{\mathcal{L}_{1n}}\sum_{j:|\mathcal{V}_{j}|>1}\sum_{i\in\mathcal{V}_{j }}\exp(\beta^{n}_{0i})[\|\Delta\beta^{n}_{1ij}\|+\|\Delta\eta^{n}_{ij}\|] \to 0,\] (54)

as \(n\to\infty\). Taking the summation of three limits in equations (48), (51) and (54), we deduce that \(1=\mathcal{L}_{1n}/\mathcal{L}_{1n}\to 0\) as \(n\to\infty\), which is a contradiction. Thus, at least one among ratios of the forms \(S_{n,j,\alpha_{1},\alpha_{2}}/\mathcal{L}_{1n}\) and \(T_{n,j,\ell}/\mathcal{L}_{1n}\) goes to zero as \(n\) tends to infinity.

**Step 3 - Application of Fatou's lemma.** In this step, we show that all the ratios \(S_{n,j,\alpha_{1},\alpha_{2}}/\mathcal{L}_{1n}\) and \(T_{n,j,\ell}/\mathcal{L}_{1n}\) go to zero as \(n\to\infty\), which contradicts to the conclusion in Step 2. In particular, by denoting \(m_{n}\) as the maximum of the absolute values of those ratios. From the result of Step 2, it follows that \(1/m_{n}\not\to\infty\).

Recall from the hypothesis in equation (43) that \(\|g_{G_{n}}-g_{G_{s}}\|_{L_{2}(\mu)}/\mathcal{L}_{1n}\to 0\) as \(n\to\infty\), which indicates that \(\|g_{G_{n}}-g_{G_{s}}\|_{L^{1}(\mu)}/\mathcal{L}_{1n}\to 0\). Therefore, by applying the Fatou's lemma, we get that

\[0=\lim_{n\to\infty}\frac{\|g_{G_{n}}-g_{G_{s}}\|_{L^{1}(\mu)}}{m_{n}\mathcal{ L}_{1n}}\geq\int\liminf_{n\to\infty}\frac{|g_{G_{n}}(\bm{X})-g_{G_{s}}(\bm{X})|}{m_{n} \mathcal{L}_{1n}}\mathrm{d}\mu(\bm{X})\geq 0.\]

This result implies that \(\frac{1}{m_{n}\mathcal{L}_{1n}}\cdot[g_{G_{n}}(\bm{X})-g_{G_{s}}(\bm{X})]\to 0\) as \(n\to\infty\) for \(\mu\)-almost surely \(\bm{X}\). Looking at the formulation of \(Q_{n}(\bm{X})\) in equation (45), since the term \(\Big{[}\sum_{i^{\prime}=1}^{k_{0}}\exp(\bm{X}^{\top}A^{0}_{i^{\prime}}\bm{X}+c ^{0}_{i^{\prime}})+\sum_{j^{\prime}=1}^{k_{*}}\exp((\beta^{*}_{1j^{\prime}})^{ \top}\bm{X}+\sigma((\beta^{*}_{1j^{\prime}})^{\top}\bm{X})+\beta^{*}_{0j^{ \prime}})\Big{]}\) is bounded, we deduce that the term \(\frac{Q_{n}(\bm{X})}{m_{n}\mathcal{L}_{1n}}\to 0\) for \(\mu\)-almost surely \(\bm{X}\).

Let us denote

\[\frac{S_{n,j,\alpha_{1},\alpha_{2}}}{m_{n}\mathcal{L}_{1n}}\to\phi_{j,\alpha_{1},\alpha_{2}},\qquad\frac{T_{n,j,\ell}}{m_{n}\mathcal{L}_{1n}}\to\varphi_{j, \ell},\]

with a note that at least one among them is non-zero. Then, from the decomposition of \(Q_{n}(\bm{X})\) in equation (47), we have

\[\sum_{j=1}^{L}\sum_{|\alpha_{1}|+|\alpha_{2}|=0}^{1+\bm{1}_{(| \mathcal{V}_{j}|>1)}}\phi_{j,\alpha_{1},\alpha_{2}}\cdot\frac{\partial^{|\alpha_{ 1}|}E}{\partial\beta^{\alpha_{1}}_{1}}(\bm{X};\beta^{*}_{1j})\frac{\partial^{ |\alpha_{2}|}h}{\partial\eta^{\alpha_{2}}}(\bm{X};\eta^{*}_{j}),\] \[-\sum_{j=1}^{L}\sum_{|\ell|=0}^{1+\bm{1}_{(|\mathcal{V}_{j}|>1)}} \varphi_{j,\ell}\cdot\frac{\partial^{|\ell|}E}{\partial\beta^{\ell}_{1}}(\bm{X}; \beta^{*}_{1j})g_{G_{*}}(\bm{X})=0,\]for \(\mu\)-almost surely \(\bm{X}\). It is worth noting that the term \(\frac{\partial^{|\alpha_{1}|}E}{\partial\beta_{1}^{\alpha_{1}}}(\bm{X};\beta_{1 }^{*})\cdot\frac{\partial^{|\alpha_{2}|}h}{\partial\eta^{\alpha_{2}}}(\bm{X}; \eta_{j}^{*})\) can be explicitly expressed as

* When \(|\alpha_{1}|=0,|\alpha_{2}|=0\): \(\exp((\beta_{1j}^{*})^{\top}\bm{X}+\sigma((\beta_{1j}^{*})^{\top}\bm{X}))h(\bm {X};\eta_{j}^{*})\);
* When \(|\alpha_{1}|=1,|\alpha_{2}|=0\): \(\bm{X}^{(u)}\Big{(}1\ +\ \sigma^{\prime}((\beta_{1j}^{*})^{\top}\bm{X})\Big{)}\exp(( \beta_{1j}^{*})^{\top}\bm{X}\ +\sigma((\beta_{1j}^{*})^{\top}\bm{X}))h(\bm{X};\eta_{j}^{*})\);
* When \(|\alpha_{1}|=0,|\alpha_{2}|=1\): \(\exp((\beta_{1j}^{*})^{\top}\bm{X}+\sigma((\beta_{1j}^{*})^{\top}\bm{X}))\frac {\partial h}{\partial\eta^{(w)}}(\bm{X};\eta_{j}^{*})\);
* When \(|\alpha_{1}|=1,|\alpha_{2}|=1\):
* When \(|\alpha_{1}|=0,|\alpha_{2}|=2\): \(\exp((\beta_{1j}^{*})^{\top}\bm{X}+\sigma((\beta_{1j}^{*})^{\top}\bm{X}))\frac {\partial h}{\partial\eta^{(w)}}(x;\eta_{j}^{*})\).

Recall that the expert function \(h\) satisfies the condition in Definition 4.2, i.e. the set

\[\bigg{\{}\bm{X}^{\nu}\Big{[}(1+\sigma^{\prime}((\beta_{1j}^{*})^{\top}\bm{X}) )^{|\nu|}+\bm{1}_{\{|\nu|=2\}}\sigma^{\prime\prime}((\beta_{1j}^{*})^{\top}\bm{ X})\Big{]}\cdot\frac{\partial^{|\gamma|}h}{\partial\eta^{\gamma}}(\bm{X},\eta_{j}^{*} ):j\in[L],\ 0\leq|\nu|+|\gamma|\leq 2\bigg{\}}\]

is linearly independent for almost every \(\bm{X}\). Therefore, we obtain that \(\phi_{j,\alpha_{1},\alpha_{2}}=\varphi_{j,\ell}=0\) for all \(j\in[L],0\leq|\alpha_{1}|+|\alpha_{2}|,|\ell|\leq 1+\bm{1}_{\{|\nu|_{j}|>1\}}\). This result turns out to contradict the fact that at least one among them is different from zero. Hence, we achieve the inequality in equation (42).

**Global part.** It is worth noting that the inequality (42) suggests that there exists a positive constant \(\varepsilon^{\prime}\) such that

\[\inf_{G\in\mathcal{G}_{L^{\prime}}(\Theta):\mathcal{L}_{1}(G,G_{*})\leq \varepsilon^{\prime}}\|g_{G}-g_{G_{*}}\|_{L_{2}(\mu)}/\mathcal{L}_{1}(G,G_{*}) >0.\]

Therefore, it is sufficient to prove that

\[\inf_{G\in\mathcal{G}_{L^{\prime}}(\Theta):\mathcal{L}_{1}(G,G_{*})>\varepsilon ^{\prime}}\|g_{G}-g_{G_{*}}\|_{L_{2}(\mu)}/\mathcal{L}_{1}(G,G_{*})>0.\] (55)

Assume by contrary that the inequality (55) does not hold true, then we can find a sequence of mixing measures \(G_{n}^{\prime}\in\mathcal{G}_{L^{\prime}}(\Theta)\) such that \(\mathcal{L}_{1}(G_{n}^{\prime},G_{*})>\varepsilon^{\prime}\) and

\[\lim_{n\to\infty}\frac{\|g_{G_{n}^{\prime}}-g_{G_{*}}\|_{L_{2}(\mu)}}{ \mathcal{L}_{1}(G_{n}^{\prime},G_{*})}=0,\]

which indicates that \(\|g_{G_{n}^{\prime}}-g_{G_{*}}\|_{L_{2}(\mu)}\to 0\) as \(n\to\infty\). Recall that \(\Theta\) is a compact set, therefore, we can replace the sequence \(G_{n}^{\prime}\) by one of its subsequences that converge to a mixing measure \(G^{\prime}\in\mathcal{G}_{L^{\prime}}(\Omega)\). Since \(\mathcal{L}_{1}(G_{n}^{\prime},G_{*})>\varepsilon^{\prime}\), we deduce that \(\mathcal{L}_{1}(G^{\prime},G_{*})>\varepsilon^{\prime}\).

Next, by invoking the Fatou's lemma, we have that

\[0=\lim_{n\to\infty}\|g_{G_{n}^{\prime}}-g_{G_{*}}\|_{L_{2}(\mu)}^{2}\geq\int \liminf_{n\to\infty}\left|g_{G_{n}^{\prime}}(\bm{X})-g_{G_{*}}(\bm{X})\right|^{ 2}\,\mathrm{d}\mu(\bm{X}).\]

Thus, we get that \(g_{G^{\prime}}(\bm{X})=g_{G_{*}}(\bm{X})\) for \(\mu\)-almost surely \(\bm{X}\). From the identifiability property of the non-linear residual gating prefix MoE (cf. the end of this proof), we deduce that \(G^{\prime}\equiv G_{*}\). Consequently, it follows that \(\mathcal{L}_{1}(G^{\prime},G_{*})=0\), contradicting the fact that \(\mathcal{L}_{1}(G^{\prime},G_{*})>\varepsilon^{\prime}>0\). Hence, the proof is completed.

**Identifiability of Non-linear Residual Gating MoE.**

We now prove the identifiability of the non-linear residual gating prefix MoE. In particular, we will show that if \(g_{G}(\bm{X})=g_{G_{*}}(\bm{X})\) for almost every \(\bm{X}\), then it follows that \(G\equiv G_{*}\).

For ease of presentation, let us denote

\[\operatorname{softmax}_{G}(u): =\frac{\exp(u)}{\sum_{i^{\prime}=1}^{N}\exp(\bm{X}^{\top}B_{i^{ \prime}}^{0}\bm{X}+c_{i^{\prime}}^{0})+\sum_{j^{\prime}=1}^{L}\exp((\beta_{1j^{ \prime}})^{\top}\bm{X}+\alpha\sigma(\tau(\beta_{1j^{\prime}})^{\top}\bm{X})+ \beta_{0j^{\prime}})},\] \[\operatorname{softmax}_{G_{\star}}(u^{*}): =\frac{\exp(u^{*})}{\sum_{i^{\prime}=1}^{N}\exp(\bm{X}^{\top}B_{ i^{\prime}}^{0}\bm{X}+c_{i^{\prime}}^{0})+\sum_{j^{\prime}=1}^{L}\exp((\beta_{1j^{ \prime}}^{*})^{\top}\bm{X}+\alpha\sigma(\tau(\beta_{1j^{\prime}}^{*})^{\top} \bm{X})+\beta_{0j^{\prime}}^{*})},\]

where

\[u\in\left\{\bm{X}^{\top}B_{i^{\prime}}^{0}\bm{X}+c_{i^{\prime}}^{ 0},\;(\beta_{1j^{\prime}})^{\top}\bm{X}+\alpha\sigma(\tau(\beta_{1j^{\prime}}) ^{\top}\bm{X})+\beta_{0j^{\prime}}:i^{\prime}\in[N],j^{\prime}\in[L^{\prime}] \right\},\] \[u^{*}\in\left\{\bm{X}^{\top}B_{i^{\prime}}^{0}\bm{X}+c_{i^{\prime }}^{0},\;(\beta_{1j^{\prime}}^{*})^{\top}\bm{X}+\alpha\sigma(\tau(\beta_{1j^{ \prime}}^{*})^{\top}\bm{X})+\beta_{0j^{\prime}}^{*}:i^{\prime}\in[N],j^{\prime }\in[L]\right\}.\]

Since \(g_{G}(\bm{X})=g_{G_{\star}}(\bm{X})\) for almost every \(\bm{X}\), we have

\[\sum_{i=1}^{N}\operatorname{softmax}_{G}(\bm{X}^{\top}B_{i}\bm{X} +c_{i}^{0})\cdot h(\bm{X},\eta_{i}^{0})+\sum_{j=1}^{L^{\prime}}\operatorname{ softmax}_{G}\Bigl{(}(\beta_{1j})^{\top}\bm{X}+\alpha\sigma(\tau(\beta_{1j})^{ \top}\bm{X})+\beta_{0j}\Bigr{)}\cdot h(\bm{X},\eta_{j})\] \[=\sum_{i=1}^{N}\operatorname{softmax}_{G_{\star}}(\bm{X}^{\top}B _{i}\bm{X}+c_{i}^{0})\cdot h(\bm{X},\eta_{i}^{0})+\sum_{j=1}^{L}\operatorname{ softmax}_{G_{\star}}\Bigl{(}(\beta_{1j}^{*})^{\top}\bm{X}+\alpha\sigma(\tau( \beta_{1j}^{*})^{\top}\bm{X})+\beta_{0j}^{*}\Bigr{)}\cdot h(\bm{X},\eta_{j}^{* }).\] (56)

As the expert function \(h\) satisfies the conditions in Definition 4.2, the set \(\{h(\bm{X},\eta_{i}^{\prime}):i\in[k^{\prime}]\}\), where \(\eta_{1}^{\prime},\ldots,\eta_{k^{\prime}}^{\prime}\) are distinct vectors for some \(k^{\prime}\in\mathbb{N}\), is linearly independent. If \(L^{\prime}\neq L\), then there exists some \(i\in[L^{\prime}]\) such that \(\eta_{i}\neq\eta_{j}^{*}\) for any \(j\in[L]\). This implies that \(\sum_{j=1}^{L^{\prime}}\operatorname{softmax}_{G}\Bigl{(}(\beta_{1j})^{\top} \bm{X}+\alpha\sigma(\tau(\beta_{1j})^{\top}\bm{X})+\beta_{0j}\Bigr{)}\cdot h( \bm{X},\eta_{j})=0\), which is a contradiction. Thus, we must have that \(L=L^{\prime}\). As a result,

\[\Bigl{\{}\operatorname{softmax}_{G}\Bigl{(}(\beta_{1j})^{\top}\bm {X} +\alpha\sigma(\tau(\beta_{1j})^{\top}\bm{X})+\beta_{0j}\Bigr{)}:j\in[L ^{\prime}]\Bigr{\}}\] \[=\Bigl{\{}\operatorname{softmax}_{G_{\star}}\Bigl{(}(\beta_{1j}^{ *})^{\top}\bm{X}+\alpha\sigma(\tau(\beta_{1j}^{*})^{\top}\bm{X})+\beta_{0j}^{* }\Bigr{)}:j\in[L]\Bigr{\}},\]

for almost every \(\bm{X}\). WLOG, we may assume that

\[\operatorname{softmax}_{G}\Bigl{(}(\beta_{1j})^{\top}\bm{X}+\alpha\sigma(\tau( \beta_{1j})^{\top}\bm{X})+\beta_{0j}\Bigr{)}=\operatorname{softmax}_{G_{ \star}}\Bigl{(}(\beta_{1j}^{*})^{\top}\bm{X}+\alpha\sigma(\tau(\beta_{1j}^{* })^{\top}\bm{X})+\beta_{0j}^{*}\Bigr{)},\] (57)

for almost every \(\bm{X}\) for any \(j\in[L]\). Since the softmax function is invariant to translation, this result indicates that \(\beta_{1j}=\beta_{1j}^{*}\) and \(\beta_{0j}=\beta_{0j}^{*}+v_{0}\) for some \(v_{0}\in\mathbb{R}\) for any \(j\in[L]\). Recall from the universal assumption that \(\beta_{0L^{\prime}}=\beta_{0L}=0\), we get that \(\beta_{0j}=\beta_{0j}^{*}\) for any \(j\in[L]\). Then, equation (56) can be rewritten as

\[\sum_{j=1}^{L}\exp(\beta_{0j})\exp\Big{(}(\beta_{1j})^{\top}\bm{ X} +\alpha\sigma(\tau(\beta_{1j})^{\top}\bm{X})\Big{)}h(\bm{X},\eta_{j})\\ =\sum_{j=1}^{L}\exp(\beta_{0j}^{*})\exp\Big{(}(\beta_{1j}^{*})^{ \top}\bm{X}+\alpha\sigma(\tau(\beta_{1j}^{*})^{\top}\bm{X}\Big{)}h(\bm{X}, \eta_{j}^{*}),\] (58)

for almost every \(\bm{X}\). Next, we denote \(P_{1},P_{2},\ldots,P_{m}\) as a partition of the index set \([L]\), where \(m\leq L^{\prime}\), such that \(\exp(\beta_{0i})=\exp(\beta_{0i^{\prime}}^{*})\) for any \(i,i^{\prime}\in P_{j}\) and \(j\in[L]\). On the other hand, when \(i\) and \(i^{\prime}\) do not belong to the same set \(P_{j}\), we let \(\exp(\beta_{0i})\neq\exp(\beta_{0i^{\prime}})\). Thus, we can reformulate equation (58) as

\[\sum_{j=1}^{m}\sum_{i\in P_{j}}\exp(\beta_{0i})\exp\Big{(}(\beta_{ 1i})^{\top}\bm{X} +\alpha\sigma(\tau(\beta_{1i})^{\top}\bm{X})h(\bm{X},\eta_{i})\] \[=\sum_{j=1}^{m}\sum_{i\in P_{j}}\exp(\beta_{0i}^{*})\exp\Big{(}( \beta_{1i}^{*})^{\top}\bm{X}+\alpha\sigma(\tau(\beta_{1i}^{*})^{\top}\bm{X})h( \bm{X},\eta_{i}^{*}),\]for almost every \(\bm{X}\). Recall that \(\beta_{1i}=\beta_{1i}^{*}\) and \(\beta_{0i}=\beta_{0i}^{*}\) for any \(i\in[L]\), then the above equation leads to

\[\{\eta_{i}:i\in P_{j}\}\equiv\{\eta_{i}^{*}:i\in P_{j}\},\]

for almost every \(\bm{X}\) for any \(j\in[m]\). As a consequence,

\[G=\sum_{j=1}^{m}\sum_{i\in P_{j}}\exp(\beta_{0i})\delta_{(\beta_{1i},\eta_{i} )}=\sum_{j=1}^{m}\sum_{i\in P_{j}}\exp(\beta_{0i}^{*})\delta_{(\beta_{1i}^{*}, \eta_{i}^{*})}=G_{*}.\]

Hence, we reach the conclusion of this proposition.

## Appendix C Discussion of related Mixture of Experts works

Recently, the MoE model has been employed to mitigate catastrophic forgetting in continual learning. For example, [58] focused on continual learning in vision-language models by adapting a pre-trained vision-language model to new tasks through learning a mixture of specialized adapter modules. [58] introduced an MoE structure onto a frozen CLIP, utilizing a mixture of adapters to modify the MLP block after the MSA layer. In contrast, our work centers on general continual learning with pre-trained models, leveraging the inherent MoE architecture of MSA layers. Consequently, our MoE model placement differs from that of [58]. By employing prefix tuning, we demonstrate that it is analogous to introducing new prefix experts to scale and adapt these pre-trained MoE models to downstream tasks. Furthermore, while [58] utilizes task-specific routers, our approach employs task-specific prompts that encapsulate both task-specific router and expert parameters.

The parameters cost is usually considered in practical memory-constrained continual learning scenarios. Dynamic routing mechanism can be employed for gating-based neural networks [18]. To improve the parameter efficiency of the final model, we can integrate this mechanism in the proposed method. Specifically, each head in the MSA layers comprises \(N\) MoE models, where \(N\) is the length of the input sequence. This allows for a dynamic routing mechanism to enhance parameter efficiency. For instance, [18] proposed a dynamic routing strategy that adaptively adjusts the number of activated experts based on the input. The computation for any MoE model's gating is directly correlated with the corresponding row in the attention matrix, which encapsulates the MoE model's score functions. For example, selecting the top \(k\) experts via Top-K routing in the \(i\)-th MoE model is equivalent to identifying the top \(k\) largest values in the \(i\)-th row of the attention matrix. To implement [18], we first sort the elements in the \(i\)-th row from highest to lowest, then find the smallest set of experts whose cumulative probability exceeds the threshold. Consequently, unselected experts remain inactive, reducing the need to compute all elements of the value matrix within self-attention.

## Appendix D Training Algorithm of HiDe-Prompt

In this appendix, we outline the detailed algorithm of HiDe-Prompt, utilizing the same notation as in Section 2.

Each previously encountered class \(c\in\mathcal{Y}^{(i)},i=1,\ldots,t-1\) has its instructed and uninstructed representations approximated by Gaussian distributions, denoted as \(\mathcal{G}_{c}\) and \(\hat{\mathcal{G}}_{c}\), respectively.

HiDe-Prompt maintains an expandable pool of task-specific prompts \(\bm{e}_{t}\), each optimized for a specific task \(\mathcal{D}_{t}\) using a cross-entropy loss within the WTP objective. To prevent forgetting, previous prompts \(\bm{e}_{1},\ldots,\bm{e}_{t-1}\) remain frozen. Knowledge transfer across tasks is facilitated by a prompt ensemble (PE) strategy: the current prompt is initialized with the last one \(\bm{e}_{t}\leftarrow\bm{e}_{t-1}\) and refined using a weighted combination of all past prompts \(\bm{p}_{t}=\alpha\sum_{i=1}^{t-1}\bm{e}_{i}+(1-\alpha)\bm{e}_{t}\), where \(\alpha\) is a hyper-parameter. Notably, HiDe-Prompt incorporates contrastive regularization within the WTP objective, pushing features of the new task away from those of past tasks represented by the prototypes of old class distributions \(\mathcal{G}_{c}\). Let \(\mathcal{H}_{t}=\{f_{\theta}(\bm{x}_{i}^{(t)},\bm{p}_{t})|\;i=1,\ldots,N_{t}\}\) be the embedding transformation of \(\mathcal{D}_{t}\) and \(\bm{\mu}_{c}\) be the mean of \(\mathcal{G}_{c}\). The contrastive loss can be written as

\[\mathcal{L}_{\mathrm{CR}}(\bm{p}_{t})=\sum_{h\in\mathcal{H}_{t}}\sum_{i=1}^{t- 1}\sum_{c\in\mathcal{Y}^{(i)}}\log(\frac{\exp(\bm{h}\cdot\bm{\mu}_{c}/\tau)}{ \sum_{\bm{h}^{\prime}\in\mathcal{H}_{t}}\exp(\bm{h}\cdot\bm{h}^{\prime}/\tau) +\sum_{i=1}^{t-1}\sum_{c^{\prime}\in\mathcal{Y}^{(i)}}\exp(\bm{h}\cdot\bm{\mu }_{c^{\prime}}/\tau)}),\] (59)

where \(\tau\) is the temperature that is set to 0.8. The overall objective function of WTP for learning a new task \(t\) is defined as

\[\mathcal{L}_{\mathrm{WTP}}(\psi,\bm{p}_{t})=\mathcal{L}_{\mathrm{CE}}(\psi,\bm {p}_{t})+\lambda\mathcal{L}_{\mathrm{CR}}(\bm{p}_{t}),\] (60)where \(\lambda\) is a hyper-parameter. Following WTP, HiDe-Prompt performs a further refinement step on the output layer parameters \(\psi\) using a separate objective called task-adaptive prediction (TAP). TAP addresses potential classifier bias by considering the Gaussian distribution of all classes encountered so far. The final output layer \(h_{\psi}\) can be further optimized for TAP objective,

\[\mathcal{L}_{\mathrm{TAP}}(\psi)=\sum_{i=1}^{t}\sum_{c\in\mathcal{Y}^{(i)}}\sum_ {\boldsymbol{h}\in\mathcal{H}_{i,c}}-\log(\frac{\exp(h_{\psi}(\boldsymbol{h})[ c])}{\sum_{j=1}^{t}\sum_{c^{\prime}\in\mathcal{Y}^{(j)}}\exp(h_{\psi}( \boldsymbol{h})[c^{\prime}])})\] (61)

where \(\mathcal{H}_{i,c}\) is constructed by sampling an equal number of pseudo representations from \(\mathcal{G}_{c}\) for \(c\in\mathcal{Y}^{(i)}\) and \(i=1,\ldots,t\).

For TII, HiDe-Prompt leverages a lightweight auxiliary output layer \(\hat{h}_{\omega}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{T}\), to map uninstructed representations directly to task identity. This mapping is learned explicitly through a cross-entropy loss function,

\[\mathcal{L}_{\mathrm{TII}}(\omega)=\sum_{c\in\mathcal{Y}_{t}}\sum_{\hat{ \boldsymbol{h}}\in\mathcal{H}_{c}}-\log(\frac{\exp(\hat{h}_{\omega}(\hat{ \boldsymbol{h}})[c])}{\sum_{c^{\prime}\in\mathcal{Y}_{t}}\exp(\hat{h}_{\omega }(\hat{\boldsymbol{h}})[c^{\prime}]})\] (62)

where \(\mathcal{H}_{c}\) is constructed by sampling an equal number of pseudo representations from \(\mathcal{\hat{G}}_{c}\) for \(c\in\mathcal{Y}^{(i)}\) and \(i=1,\ldots,t\). Please refer to Algorithm 1 for more details.

## Appendix E Experimental Details

**Datasets.** We use commonly-used datasets in the field of continual learning, including **(1) Split CIFAR-100**[23]: This dataset comprises images from 100 classes. These classes are divided randomly into 10 separate incremental tasks, with each task featuring a distinct set of classes. **(2) Split ImageNet-R**[23]: This dataset is composed of images from 200 classes. It includes challenging examples from the original ImageNet [40] dataset and newly gathered images representing diverse styles. These classes are also randomly divided into 10 distinct incremental tasks. **(3) Split CUB-200**[48]: This dataset consists of fine-grained images of 200 different bird species. It is randomly divided into 10 incremental tasks, each comprising a unique class subset. **(4) 5-Datasets**[9]: This composite dataset incorporates **CIFAR-10**[23], **MNIST**[24], **Fashion-MNIST**[55], **SVHN**[31], and **notMNIST**[3]. Each of these datasets is treated as a separate incremental task, permitting for the assessment of the effects of significant variations between tasks.

**Prompt-Based Approaches.** We compare NoRGa against recent prompt-based continual learning approaches: L2P [54], DualPrompt [53], CODA-Prompt [42], S-Prompt [52] and HiDe-Prompt [49].

To ensure a fair comparison, we replicate these methods using the configurations reported in their respective papers. S-Prompt in the original paper trains a separate prompt and classifier head for each task. At evaluation, it infers domain id as the nearest centroid obtained by applying K-Means on the training data. To adapt S-Prompt to CIL, we use one common classifier head for all tasks. For NoRGa, we adopt the same configuration as HiDe-Prompt, which utilizes Prefix Tuning [26] as its prompt-based methodology. Learnable scalar factors \(\alpha\) and \(\tau\) are frozen after the first task's training to mitigate catastrophic forgetting. We further optimize NoRGa by selecting the best non-linear activation function \(\sigma\) via cross-validation among \(\tanh\), \(\mathrm{sigmoid}\), and \(\mathrm{GELU}\).

**Evaluation Metric.** We employ three common metrics to measure the performance the methods, including final average accuracy (FA), cumulative average accuracy (CA), and average forgetting measure (FM). Let \(S_{i,t}\) denote the accuracy on the \(i\)-th task after learning the \(t\)-th task, and \(A_{t}\) represent the average accuracy as \(A_{t}=\frac{1}{t}\sum_{i=1}^{t}S_{i,t}\). Upon learning all \(T\) tasks, we compute \(\mathrm{FA}=A_{T}\), \(\mathrm{CA}=\frac{1}{T}\sum_{t=1}^{T}A_{t}\), and \(\mathrm{FM}=\frac{1}{T-1}\sum_{i=1}^{T-1}\max_{t\in\{1,\dots,T-1\}}(S_{i,t}- S_{i,T})\). It's worth noting that FA and CA are prioritized over FM, as they inherently encompass both plasticity and forgetting, with FM providing supplementary context [42].

**Implementation Details.** We train and test on one NVIDIA A100 GPU for baselines and our method. We leverage a pre-trained ViT-B/16 model as the backbone. Training employs an Adam optimizer (\(\beta_{1}=0.9\), \(\beta_{2}=0.999\)), a batch size of 128, and a constant learning rate of 0.005 for all methods except CODA-Prompt. CODA-Prompt utilizes a cosine decaying learning rate starting at 0.001. Additionally, a grid search technique was implemented to determine the most appropriate number of epochs for effective training.

## Appendix F Task-incremental Learning Results

Because HiDe-Prompt optimizes prompt parameters specifically for within-task prediction (WTP), NoRGa inherently aligns with this objective, leading to generally better continual learning performance. We demonstrate this improvement through experiments in a task-incremental learning setting, where task labels are available during inference (as in Table 4). While HiDe-Prompt performs well, NoRGa shows consistent improvement across all scenarios. Notably, NoRGa with sigmoid activation achieves the highest final average accuracy in both Split CIFAR-100 and Split CUB-200 with Sup-21K training. Additionally, NoRGa demonstrates its effectiveness even with self-supervised pre-training, further solidifying its advantage over the original prefix tuning model. Overall, NoRGa variants outperform HiDe-Prompt on both datasets and under both training conditions.

## Appendix G Comparison to Different Parameter-Efficient Fine-Tuning Methods

As the advantages of different parameter-efficient fine-tuning (PEFT) methods remain an open question, we briefly describe them through our revealed connection between self-attention and MoE.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Split CIFAR-100} & \multicolumn{2}{c}{Split CUB-200} \\ \cline{2-5}  & Sup-21K & iBOT-21K & Sup-21K & iBOT-21K \\ \hline HiDe-Prompt & \(97.87\pm 0.31\) & \(97.48\pm 0.33\) & \(97.57\pm 0.08\) & \(92.34\pm 0.34\) \\ NoRGa \(\tanh\) & \(98.55\pm 0.45\) & \(\textbf{98.26}\pm 0.36\) & \(97.86\pm 0.14\) & \(92.85\pm 0.33\) \\ NoRGa \(\mathrm{sigmoid}\) & \(\textbf{98.63}\pm 0.35\) & \(98.15\pm 0.29\) & \(\textbf{97.89}\pm 0.14\) & \(92.85\pm 0.22\) \\ NoRGa \(\mathrm{GELU}\) & \(98.41\pm 0.47\) & \(98.17\pm 0.30\) & \(97.76\pm 0.10\) & \(\textbf{93.00}\pm 0.11\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance comparison in task-incremental learning setting. Here we present Final Average Accuracy (FA).

\begin{table}
\begin{tabular}{l c c} \hline \hline Method & Split CIFAR-100 & Split CUB-200 \\ \hline HiDe-Prompt & 92.61 & 86.56 \\ HiDe-LoRA & 92.71 & 87.37 \\ HiDe-Adapter & 92.73 & 87.10 \\ NoRGa & **94.48** & **90.90** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Performance comparison of different PEFT methods using ViT-B/16 with Sup-21K weights. Here we present Final Average Accuracy (FA).

Prefix tuning introduces additional parameters at the input of MSA layers to adapt the pre-trained model representation, contrasting with Adapter [16], which insert adaptive parameters between layers, often replacing MLP blocks. LoRA [17] approximates weight updates with low-rank matrices and adds them to the backbone weights. Our work shows that the MSA layer in a pre-trained model can be seen as a pre-trained MoE architecture. Applying LoRA to the MSA layer refines both the pre-trained experts and their corresponding score functions for downstream tasks. In contrast, prefix tuning expands the pre-trained MoE models by incorporating new experts while preserving the original components, rather than modifying the pre-trained experts like LoRA.

NoRGa emerges as a simple, parameter-efficient fine-tuning method and can be regarded as a distinct implementation of prompts. However, our novel perspective on the interplay between self-attention, prefix tuning, and mixture of experts enables us to theoretically substantiate the effectiveness of NoRGa as shown in Section 4.

For empirical comparison, we integrate the framework of HiDe-Prompt with different PEFT techniques and Sup-21K weights, evaluating performance on Split CIFAR-100 and Split CUB-200. The results are summarized in Table 5. The table shows that NoRGa consistently outperforms the other PEFT methods on both datasets, suggesting its effectiveness. Nevertheless, further investigation with LoRA and Adapter would be necessary to draw more definitive conclusions.

While exploring alternative PEFT methods might offer improvements in WTP performance, these approaches lack theoretical guarantees and could lead to an increased number of parameters. In contrast, our NoRGa method modifies the original score functions of prefix tuning to enhance WTP performance with theoretical rigor. Importantly, NoRGa maintains the same parameter count as HiDe-Prompt, which is crucial in CL due to memory constraints.

## Appendix H Comparison with Pre-trained Model-based Methods

Previous works have demonstrated that utilizing pre-trained models (PTM) significantly enhances performance for continual learning, often surpassing the performance of non-PTM-based methods. Moreover, studies have shown that first-task adaptation and simple PEFT-style tuning can achieve competitive performance [20; 37; 60; 29] with prompt-based methods. For instance, [20] demonstrated that appending a nearest class mean (NCM) classifier to a ViT model's feature outputs, can serve as a strong baseline. [37; 60] enhanced this strategy by adapting the pre-trained model to the first task using the three PEFT methods for transformer networks [60] and the FiLM method for CNNs [37]. Additionally, [29] improved NCM by incorporating second-order statistics--covariance and Gram matrices. However, these methods, which fine-tune only the backbone for the initial task, may not always ensure satisfactory separation of new tasks' features. Our work focuses on continually adapting the backbone, utilizing task-specific prompts to consistently capture emerging

\begin{table}
\begin{tabular}{l c c} \hline \hline Method & Split CIFAR-100 & Split CUB-200 \\ \hline ADAM + VPT-D & 85.04 & 85.28 \\ ADAM + SSF & 85.27 & 85.67 \\ ADAM + Adapter & 87.29 & 85.84 \\ RanPAC & 92.20 & 90.30 \\ NoRGa & **94.48** & **90.90** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance comparison of pre-trained model-based continual learning methods using ViT-B/16 with Sup-21K weights. Here we present Final Average Accuracy (FA).

\begin{table}
\begin{tabular}{l c c} \hline \hline Method & Split CIFAR-100 & Split CUB-200 \\ \hline HiDe-Prompt & 92.61 & 86.56 \\ Learnable \(\alpha\), Fixed \(\tau\) & 94.38 & 90.45 \\ Fixed \(\alpha\), Learnable \(\tau\) & 94.42 & 90.48 \\ Fixed \(\alpha\), Fixed \(\tau\) & 94.29 & 90.32 \\ Learnable \(\alpha\), Learnable \(\tau\) & **94.48** & **90.90** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Ablation study on the effect of learnable \(\alpha\) and \(\tau\) with Sup-21K weights. Here we present Final Average Accuracy (FA).

[MISSING_PAGE_FAIL:31]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The paper's contributions and scope are reflected accurately in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have already discussed the limitations of our work in the conclusion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The full set of assumptions and proofs is given in Section 4.2 and the Appendices. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All the experimental details for reproducibility are specified in Section 5, and Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results in supplemental material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the experimental details are specified in Section 5, and Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report error bars and relevant information in Section 5, and Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided sufficient information on the computer resources in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read and followed all the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Given the theoretical nature of the paper, we do not think there are any positive or negative societal impacts of the work performed. Experiments are conducted only for empirically justifying the theoretical results. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper does not release any data or models that have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The data and models used in the paper are properly credited. See Section 5, and Appendix E for further details. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.