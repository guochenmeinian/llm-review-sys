# A Fractional Graph Laplacian Approach

to Oversmoothing

Sohir Maskey

Department of Mathematics,

LMU Munich

maskey@math.lmu.de

&Raffaele Paolino

Department of Mathematics & MCML,

LMU Munich

paolino@math.lmu.de

&Aras Bacho

Department of Mathematics,

LMU Munich

&Gitta Kutyniok

Department of Mathematics & MCML,

LMU Munich

Equal contribution.

###### Abstract

Graph neural networks (\(\mathrm{GNNs}\)) have shown state-of-the-art performances in various applications. However, \(\mathrm{GNNs}\) often struggle to capture long-range dependencies in graphs due to oversmoothing. In this paper, we generalize the concept of oversmoothing from undirected to directed graphs. To this aim, we extend the notion of Dirichlet energy by considering a directed symmetrically normalized Laplacian. As vanilla graph convolutional networks are prone to oversmooth, we adopt a neural graph \(\mathrm{ODE}\) framework. Specifically, we propose fractional graph Laplacian neural \(\mathrm{ODEs}\), which describe non-local dynamics. We prove that our approach allows propagating information between distant nodes while maintaining a low probability of long-distance jumps. Moreover, we show that our method is more flexible with respect to the convergence of the graph's Dirichlet energy, thereby mitigating oversmoothing. We conduct extensive experiments on synthetic and real-world graphs, both directed and undirected, demonstrating our method's versatility across diverse graph homophily levels. Our code is available on GitHub.

## 1 Introduction

Graph neural networks (\(\mathrm{GNNs}\)) (Gori et al., 2005; Scarselli et al., 2009; Bronstein et al., 2017) have emerged as a powerful class of machine learning models capable of effectively learning representations of structured data. \(\mathrm{GNNs}\) have demonstrated state-of-the-art performance in a wide range of applications, including social network analysis (Monti et al., 2019), molecular property prediction (Gilmer et al., 2017), and recommendation systems (J. Wang et al., 2018; Fan et al., 2019). The majority of existing work on \(\mathrm{GNNs}\) has focused on undirected graphs (Defferrard et al., 2016; Kipf et al., 2017; Hamilton et al., 2017), where edges have no inherent direction. However, many real-world systems, such as citation networks, transportation systems, and biological pathways, are inherently directed, necessitating the development of methods explicitly tailored to directed graphs.

Despite their success, most existing \(\mathrm{GNN}\) models struggle to capture long-range dependencies, which can be critical for specific tasks, such as node classification and link prediction, and for specific graphs, such as heterophilic graphs. This shortcoming also arises from the problem of _oversmoothing_, where increasing the depth of \(\mathrm{GNNs}\) results in the node features converging to similar values that only convey information about the node's degree (Oono et al., 2019; Cai et al., 2020). Consequently,scaling the depth of \(\mathrm{GNN}\)s is not sufficient to broaden receptive fields, and other approaches are necessary to address this limitation. While these issues have been extensively studied in undirected graphs (Q. Li et al., 2018; G. Li et al., 2019; Luan, M. Zhao, et al., 2019; D. Chen et al., 2020; Rusch et al., 2022), their implications for directed graphs remain largely unexplored. Investigating these challenges and developing effective solutions is crucial for applying \(\mathrm{GNN}\)s to real-world scenarios.

Over-smoothing has been shown to be intimately related to the graph's _Dirichlet energy_, defined as

\[\mathscr{E}(\mathbf{x})\coloneqq\frac{1}{4}\sum_{i,j=1}^{N}a_{i,j}\left\| \frac{\mathbf{x}_{i}}{\sqrt{d_{i}}}-\frac{\mathbf{x}_{j}}{\sqrt{d_{j}}}\right\| _{2}^{2},\]

where \(\mathbf{A}=(a_{i,j})_{i,j=1}^{N}\) represents the adjacency matrix of the underlying graph, \(\mathbf{x}\in\mathbb{R}^{N\times K}\) denotes the node features, and \(d_{i}\in\mathbb{R}\) the degree of node \(i\). Intuitively, the Dirichlet energy measures the smoothness of nodes' features. Therefore, a \(\mathrm{GNN}\) that minimizes the Dirichlet energy is expected to perform well on _homophilic_ graphs, where similar nodes are likely to be connected. Conversely, a \(\mathrm{GNN}\) that ensures high Dirichlet energy should lead to better performances on _heterophilic_ graphs, for which the nodes' features are less smooth.

This paper aims to bridge the gap in understanding oversmoothing for directed graphs. To this aim, we generalize the concept of Dirichlet energy, providing a rigorous foundation for analyzing oversmoothing. Specifically, we consider the directed symmetrically normalized Laplacian, which accommodates directed graph structures and recovers the usual definition in the undirected case. Even though the directed symmetrically normalized Laplacian has been already used (Zou et al., 2022), its theoretical properties remain widely unexplored.

However, a vanilla graph convolutional network (\(\mathrm{GCN}\)) (Kipf et al., 2017) implementing this directed Laplacian alone is not able to prevent oversmoothing. For this reason, we adopt a graph neural \(\mathrm{ODE}\) framework, which has been shown to effectively alleviate oversmoothing in undirected graphs (Bodnar et al., 2022; Rusch et al., 2022; Di Giovanni et al., 2023).

### Graph Neural \(\mathrm{ODE}\)s

The concept of neural \(\mathrm{ODE}\) was introduced by Haber et al. (2018) and R. T. Q. Chen et al. (2018), who first interpreted the layers in neural networks as the time variable in \(\mathrm{ODE}\)s. Building on this foundation, Poli et al. (2021), Chamberlain et al. (2021), and Eliasof et al. (2021) extended the connection to the realm of \(\mathrm{GNN}\)s, resulting in the development of graph neural \(\mathrm{ODE}\)s. In this context, each node \(i\) of the underlying graph is described by a state variable \(\mathbf{x}_{i}(t)\in\mathbb{R}^{K}\), representing the node \(i\) at time \(t\). We can define the dynamics of \(\mathbf{x}(t)\) via the node-wise \(\mathrm{ODE}\)

\[\mathbf{x}^{\prime}(t)=f_{\mathbf{w}}(\mathbf{x}(t))\,,\ t\in[0,T]\,,\]

subject to the initial condition \(\mathbf{x}(0)=\mathbf{x}_{0}\in\mathbb{R}^{N\times K}\), where the function \(f_{\mathbf{w}}:\mathbb{R}^{N\times K}\rightarrow\mathbb{R}^{N\times K}\) is parametrized by the learnable parameters \(\mathbf{w}\).

The graph neural \(\mathrm{ODE}\) can be seen as a continuous learnable architecture on the underlying graph, which computes the final node representation \(\mathbf{x}(T)\) from the input nodes' features \(\mathbf{x}_{0}\). Typical choices for \(f_{\mathbf{w}}\) include attention-based functions (Chamberlain et al., 2021), which generalize graph attention networks (\(\mathrm{GATs}\)) (Velickovic et al., 2018), or convolutional-like functions (Di Giovanni et al., 2023) that generalize \(\mathrm{GCNs}\)(Kipf et al., 2017).

How can we choose the learnable function \(f_{\mathbf{w}}\) to accommodate both directed and undirected graphs, as well as different levels of homophily? We address this question in the following subsection.

### Fractional Laplacians

The continuous fractional Laplacian, denoted by \((-\Delta)^{\alpha}\) for \(\alpha>0\), is used to model non-local interactions. For instance, the fractional heat equation \(\partial_{t}u+(-\Delta)^{\alpha}u=0\) provides a flexible and accurate framework for modeling anomalous diffusion processes. Similarly, the fractional diffusion-reaction, quasi-geostrophic, Cahn-Hilliard, porous medium, Schrodinger, and ultrasound equations are more sophisticated models to represent complex anomalous systems (Pozrikidis, 2018).

Similarly to the continuous case, the fractional graph Laplacian (\(\mathrm{FGL}\)) (Benzi et al., 2020) models non-local network dynamics. In general, the \(\mathrm{FGL}\) does not inherit the sparsity of the underlying graph, allowing a random walker to leap rather than walk solely between adjacent nodes. Hence, the \(\mathrm{FGL}\) is able to build long-range connections, making it well-suited for heterophilic graphs.

### Main Contributions

We present a novel approach to the fractional graph Laplacian by defining it in the singular value domain, instead of the frequency domain (Benzi et al., 2020). This formulation bypasses the need for computing the Jordan decomposition of the graph Laplacian, which lacks reliable numerical methods. We show that our version of the \(\mathrm{FGL}\) can still capture long-range dependencies, and we prove that its entries remain reasonably bounded.

We then propose two \(\mathrm{FGL}\)-based neural \(\mathrm{ODEs}\): the fractional heat equation and the fractional Schrodinger equation. Importantly, we demonstrate that solutions to these \(\mathrm{FGL}\)-based neural \(\mathrm{ODEs}\) offer increased flexibility in terms of the convergence of the Dirichlet energy. Notably, the exponent of the fractional graph Laplacian becomes a learnable parameter, allowing our network to adaptively determine the optimal exponent for the given task and graph. We show that this can effectively alleviate oversmoothing in undirected and directed graphs.

To validate the effectiveness of our approach, we conduct extensive experiments on synthetic and real-world graphs, with a specific focus on supervised node classification. Our experimental results indicate the advantages offered by fractional graph Laplacians, particularly in non-homophilic and directed graphs.

## 2 Preliminaries

We denote a graph as \(\mathcal{G}=(\mathcal{V},\mathcal{E})\), where \(\mathcal{V}\) is the set of nodes, \(\mathcal{E}\) is the set of edges, and \(N=|\mathcal{V}|\) is the number of nodes. The adjacency matrix \(\mathbf{A}:=\left\{a_{i,j}\right\}\) encodes the edge information, with \(a_{i,j}=1\) if there is an edge directed from node \(j\) to \(i\), and \(0\) otherwise. The in- and out-degree matrices are then defined as \(\mathbf{D}_{\text{in}}=\mathrm{diag}(\mathbf{A}\mathbf{1})\), \(\mathbf{D}_{\text{out}}=\mathrm{diag}(\mathbf{A}^{\mathsf{T}}\mathbf{1})\), respectively. The node feature matrix \(\mathbf{x}\in\mathbb{R}^{N\times K}\) contains for every node its feature in \(\mathbb{R}^{K}\).

Given any matrix \(\mathbf{M}\in\mathbb{C}^{n\times n}\), we denote its spectrum by \(\lambda(\mathbf{M})\coloneqq\left\{\lambda_{i}(\mathbf{M})\right\}_{i=1}^{n}\) in ascending order w.r.t. to the real part, i.e., \(\Re\lambda_{1}(\mathbf{M})\leq\Re\lambda_{2}(\mathbf{M})\leq\ldots\leq\Re \lambda_{n}(\mathbf{M})\). Furthermore, we denote by \(\left\|\mathbf{M}\right\|_{2}\) and \(\left\|\mathbf{M}\right\|\) the Frobenius and spectral norm of \(\mathbf{M}\), respectively. Lastly, we denote by \(\mathbf{I}_{n}\) the identity matrix, where we omit the dimension \(n\) when it is clear from the context.

Homophily and HeterophilyGiven a graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) with labels \(\mathbf{y}=\left\{y_{i}\right\}_{i\in\mathcal{V}}\), the _homophily_ of the graph indicates whether connected nodes are likely to have the same labels; formally,

\[\mathcal{H}(\mathcal{G})=\frac{1}{N}\sum_{i=1}^{N}\frac{|\{j\in\{1,\ldots,N\}: a_{i,j}=1\wedge y_{i}=y_{j}\}|}{|\{j\in\{1,\ldots,N\}:a_{i,j}=1\}|},\]

where the numerator represents the number of neighbors of node \(i\in\mathcal{V}\) that have the same label \(y_{i}\)(Pei et al., 2019). We say that \(\mathcal{G}\) is _homophilic_ if \(\mathcal{H}(\mathcal{G})\approx 1\) and _heterophilic_ if \(\mathcal{H}(\mathcal{G})\approx 0\).

## 3 Dirichlet Energy and Laplacian for (Directed) Graphs

In this section, we introduce the concept of Dirichlet energy and demonstrate its relationship to a directed Laplacian, thereby generalizing well-known results for undirected graphs.

**Definition 3.1**.: _The Dirichlet energy is defined on the node features \(\mathbf{x}\in\mathbb{R}^{N\times K}\) of a graph \(\mathcal{G}\) as_

\[\mathcal{E}(\mathbf{x})\coloneqq\frac{1}{4}\sum_{i,j=1}^{N}a_{i,j}\left\|\frac {\mathbf{x}_{i}}{\sqrt{d_{i}^{m}}}-\frac{\mathbf{x}_{j}}{\sqrt{d_{j}^{m}}} \right\|_{2}^{2}\,.\] (1)

The Dirichlet energy measures how much the features change over the nodes of \(\mathcal{G}\), by quantifying the disparity between the normalized outflow of information from node \(j\) and the normalized inflow of information to node \(i\).

**Definition 3.2**.: _We define the symmetrically normalized adjacency (\(\mathrm{SNA}\)) as \(\mathbf{L}\coloneqq\mathbf{D}_{in}^{-1/2}\mathbf{A}\mathbf{D}_{out}^{-1/2}\)._Note that \(\mathbf{L}\) is symmetric if and only if \(\mathcal{G}\) is undirected; the term "symmetrically" refers to the both-sided normalization rather than the specific property of the matrix itself.

It is well-known that the SNA's spectrum of a connected undirected graph lies within \([-1,1]\)(Chung, 1997). We extend this result to directed graphs, which generally exhibit complex-valued spectra.

**Proposition 3.3**.: _Let \(\mathcal{G}\) be a directed graph with SNA_\(\mathbf{L}\). For every \(\lambda\in\lambda(\mathbf{L})\), it holds \(|\lambda|\leq 1\)._

Proposition 3.3 provides an upper bound for the largest eigenvalue of any directed graph, irrespective of its size. However, many other spectral properties do not carry over easily from the undirected to the directed case. For example, the SNA may not possess a one-eigenvalue, even if the graph is strongly connected (see, e.g., Figures 1 to 2). The one-eigenvalue is of particular interest since its eigenvector \(\mathbf{v}\) corresponds to zero Dirichlet energy \(\mathscr{E}(\mathbf{v})=0\). Therefore, studying when \(1\in\lambda(\mathbf{L})\) is crucial to understanding the behavior of the Dirichlet energy. We fully characterize the set of graphs for which \(1\in\lambda(\mathbf{L})\); this is the scope of the following definition.

**Definition 3.4**.: _A graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) is said to be balanced if \(d_{i}^{\text{m}}=d_{i}^{\text{out}}\) for all \(i\in\{1,\ldots,N\}\), and weakly balanced if there exists \(\mathbf{k}\in\mathbb{R}^{N}\) such that \(\mathbf{k}\neq 0\) and_

\[\sum_{j=1}^{N}a_{i,j}\left(\frac{k_{j}}{\sqrt{d_{j}^{\text{out}}}}-\frac{k_{i }}{\sqrt{d_{i}^{\text{m}}}}\right)=0\,,\ \forall i\in\{1,\ldots,N\}\.\]

It is straightforward to see that a balanced graph is weakly balanced since one can choose \(k_{i}=\sqrt{d_{i}^{\text{m}}}\). Hence, all undirected graphs are also weakly balanced. However, as shown in Figure 2, the set of balanced graphs is a proper subset of the set of weakly balanced graphs.

**Proposition 3.5**.: _Let \(\mathcal{G}\) be a directed graph with SNA_\(\mathbf{L}\). Then, \(1\in\lambda(\mathbf{L})\) if and only if the graph is weakly balanced. Suppose the graph is strongly connected, then \(-1\in\lambda(\mathbf{L})\) if and only if the graph is weakly balanced with an even period._

Proposition 3.5 generalizes a well-known result for undirected graphs: \(-1\in\lambda(\mathbf{L})\) if and only if the graph is bipartite, i.e., has even period. The next result shows that the Dirichlet energy defined in (1) and the SNA are closely connected.

**Proposition 3.6**.: _For every \(\mathbf{x}\in\mathbb{C}^{N\times K}\), it holds \(\mathscr{E}(\mathbf{x})=\frac{1}{2}\Re\left(\operatorname{trace}\left( \mathbf{x}^{\mathsf{H}}\left(\mathbf{I}-\mathbf{L}\right)\mathbf{x}\right)\right)\). Moreover, there exists \(\mathbf{x}\neq\mathbf{0}\) such that \(\mathscr{E}(\mathbf{x})=0\) if and only if the graph is weakly balanced._

Proposition 3.6 generalizes the well-known result from the undirected (see, e.g., Cai et al., 2020, Definition 3.1) to the directed case. This result is an important tool for analyzing the evolution of the Dirichlet energy in graph neural networks.

Figure 2: Examples of non-weakly balanced (left), weakly balanced (center), and balanced (right) directed graphs. The Perron-Frobenius eigenvalue of the left graph is \(\lambda_{\text{PF}}\approx 0.97\neq 1\), while for the middle and right graphs \(\lambda_{\text{PF}}=1\).

## 4 Fractional Graph Laplacians

We introduce the fractional graph Laplacian through the singular value decomposition (SVD). This approach has two key advantages over the traditional definition (Pozrikidis, 2018; Benzi et al., 2020) in the spectral domain. First, it allows defining the fractional Laplacian based on any choice of graph Laplacian, including those with negative or complex spectrum such as the SNA. Secondly, the SVD is computationally more efficient and numerically more stable than the Jordan decomposition, which would be necessary if the fractional Laplacian was defined in the spectral domain.

Consider a directed graph with SNA \(\mathbf{L}\) and its SVD \(\mathbf{L}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\mathsf{H}}\), where \(\mathbf{U}\), \(\mathbf{V}\in\mathbb{C}^{N\times N}\) are unitary matrices and \(\mathbf{\Sigma}\in\mathbb{R}^{N\times N}\) is a diagonal matrix. Given \(\alpha\in\mathbb{R}\), we define the _\(\alpha\)-fractional graph Laplacian_2 (\(\alpha\)-\(F\!GL\) in short) as

Footnote 2: We employ the term “fractional graph Laplacian” instead of “fractional symmetrically normalized adjacency” to highlight that, in the singular value domain, one can construct fractional powers of any matrix representations of the underlying graph (commonly referred to as “graph Laplacians”). This fact is not generally true in the eigenvalue domain because eigenvalues can be negative or complex numbers.

\[\mathbf{L}^{\alpha}\coloneqq\mathbf{U}\mathbf{\Sigma}^{\alpha}\mathbf{V}^{ \mathsf{H}}\,.\]

In undirected graphs, the \(\alpha\)-FGL preserves the sign of the eigenvalues \(\lambda\) of \(\mathbf{L}\) while modifying their magnitudes, i.e., \(\lambda\mapsto\operatorname{sign}(\lambda)\left|\lambda\right|^{\alpha}\). 3

Footnote 3: In fact, this property can be generalized to all directed graphs with a normal SNA matrix, including, among others, directed cycle graphs. See Lemma C.1 for more details.

The \(\alpha\)-FGL is generally less sparse than the original SNA, as it connects nodes that are not adjacent in the underlying graph. The next theorem proves that the weight of such "virtual" edges is bounded.

**Theorem 4.1**.: _Let \(\mathcal{G}\) be a directed graph with SNA \(\mathbf{L}\). For \(\alpha>0\), if the distance \(d(i,j)\) between nodes \(i\) and \(j\) is at least 2, then_

\[\left|\left(\mathbf{L}^{\alpha}\right)_{i,j}\right|\leq\left(1+\frac{\pi^{2}}{ 2}\right)\left(\frac{\|\mathbf{L}\|}{2\left(d(i,j)-1\right)}\right)^{\alpha}\,.\]

We provide a proof of Theorem 4.1 in Appendix C. In Figure 2(a), we visually represent the cycle graph with eight nodes and the corresponding \(\alpha\)-FGL entries. We also refer to Figure 2(b), where we

Figure 3: Visual representation of long-range edges built by the fractional Laplacian.

depict the distribution of \(\alpha\)-FGL entries for the real-world graphs Cora (undirected) and Chameleon (directed) with respect to the distance in the original graph. Our empirical findings align with our theoretical results presented in Theorem 4.1.

## 5 Fractional Graph Laplacian Neural ODE

This section explores two fractional Laplacian-based graph neural ODEs. First, we consider the fractional heat equation,

\[\mathbf{x}^{\prime}(t)=-\mathbf{L}^{\alpha}\mathbf{x}(t)\mathbf{W}\,,\; \mathbf{x}(0)=\mathbf{x}_{0}\,,\] (2)

where \(\mathbf{x}_{0}\in\mathbb{R}^{N\times K}\) is the _initial condition_, \(\mathbf{x}(t)\in\mathbb{R}^{N\times K}\) for \(t>0\) and \(\alpha\in\mathbb{R}\). We assume that the _channel mixing matrix_\(\mathbf{W}\in\mathbb{R}^{K\times K}\) is a symmetric matrix. Second, we consider the fractional Schrodinger equation,

\[\mathbf{x}^{\prime}(t)=i\;\mathbf{L}^{\alpha}\mathbf{x}(t)\mathbf{W}\,,\; \mathbf{x}(0)=\mathbf{x}_{0}\,,\] (3)

where \(\mathbf{x}_{0},\mathbf{x}(t)\in\mathbb{C}^{N\times K}\) and \(\mathbf{W}\in\mathbb{C}^{K\times K}\) is unitary diagonalizable. Both (2) and (3) can be analytically solved. For instance, the solution of (2) is given by \(\mathrm{vec}(\mathbf{x})(t)=\exp(-t\,\mathbf{W}\otimes\mathbf{L}^{\alpha}) \mathrm{vec}(\mathbf{x}_{0})\), where \(\otimes\) denotes the Kronecker product and \(\mathrm{vec}(\cdot)\) represents the vectorization operation. However, calculating the exact solution is computationally infeasible since the memory required to store \(\mathbf{W}\otimes\mathbf{L}^{\alpha}\) alone grows as \((NK)^{2}\). Therefore, we rely on numerical schemes to solve (2) and (3).

In the remainder of this section, we analyze the Dirichlet energy for solutions to (2) and (3). We begin with the definition of oversmoothing.

**Definition 5.1**.: _Neural ODE-based GNNs are said to oversmooth if the normalized Dirichlet energy decays exponentially fast. That is, for any initial value \(\mathbf{x}_{0}\), the solution \(\mathbf{x}(t)\) satisfies for every \(t>0\)_

\[\left|\mathscr{E}\left(\frac{\mathbf{x}(t)}{\|\mathbf{x}(t)\|_{2}}\right)- \min\lambda(\mathbf{I}-\mathbf{L})\right|\leq\exp\left(-Ct\right)\,,\;C>0\,.\]

Definition 5.1 captures the _actual_ smoothness of features by considering the normalized Dirichlet energy, which mitigates the impact of feature amplitude (Cai et al., 2020; Di Giovanni et al., 2023). Additionally, Proposition 3.6 shows that the normalized Dirichlet energy is intimately related to the numerical range of \(\mathbf{I}-\mathbf{L}\) of the underlying graph. This shows that the Dirichlet energy and eigenvalues (or _frequencies_) of the SNA are intertwined, and one can equivalently talk about Dirichlet energy or frequencies (see also Lemma D.2). In particular, it holds that

\[0\leq\mathscr{E}\left(\frac{\mathbf{x}(t)}{\|\mathbf{x}(t)\|_{2}}\right)\leq \frac{\|\mathbf{I}-\mathbf{L}\|}{2}\,.\]

As seen in Section 3, the minimal possible value attained by the normalized Dirichlet energy is often strictly greater than \(0\) for directed graphs. This indicates that GNNs on general directed graphs inherently cannot oversmooth to the same extent as in undirected. However, we prove that a vanilla GCN implementing the directed SNA oversmooths with respect to Definition 5.1, see Appendix E.3.

### Frequency Analysis for Graphs with Normal SNA

This subsection focuses on the frequency analysis of FGL-based Neural ODEs for undirected graphs. Most classical GNNs (Kipf et al., 2017; Velickovic et al., 2018) and also graph neural ODEs (Chamberlain et al., 2021; Eliasof et al., 2021) have been shown to oversmooth. Di Giovanni et al. (2023) proved that the normalized Dirichlet energy for GNNs based on (2) with \(\alpha=1\) can not only converge to its minimal value but also to its maximal possible value. A GNN exhibiting this property is then termed _Highest-Frequency-Dominant (HFD)_.

However, in real-world scenarios, most graphs are not purely homophilic nor purely heterophilic but fall somewhere in between. Intuitively, this suggests that mid-range frequencies might be more suitable. To illustrate this intuition, consider the cycle graph as an example. If we have a homophily of \(1\), low frequencies are optimal; with a homophily equal to \(0\), high frequencies are optimal. Interestingly, for a homophily of \(\nicefrac{{1}}{{2}}\), the mid-range frequency is optimal, even though the eigendecomposition is label-independent. More information on this example can be found in Figure 4 and Appendix F. Based on this observation, we propose the following definition to generalize the concept of HFD, accommodating not only the lowest or highest frequency but all possible frequencies.

**Definition 5.2**.: _Let \(\lambda\geq 0\). Neural ODE-based GNNs initialized at \(\mathbf{x}_{0}\) are \(\lambda\)-Frequency-Dominant (\(\lambda\)-FD) if the solution \(\mathbf{x}(t)\) satisfies_

\[\mathscr{E}\left(\frac{\mathbf{x}(t)}{\|\mathbf{x}(t)\|_{2}}\right)\xrightarrow{ t\to\infty}\frac{\lambda}{2}.\]

_Suppose \(\lambda\) is the smallest or the largest eigenvalue with respect to the real part. In that case, we call it Lowset-Frequency-Dominant (LFD) or Highest-Frequency-Dominant (HFD), respectively._

In the following theorem, we show that (2) and (3) are not limited to being LFD or HFD, but can also be mid-frequency dominant.

**Theorem 5.3**.: _Let \(\mathcal{G}\) be an undirected graph with SNA\(\mathbf{L}\). Consider the initial value problem in (2) with \(\mathbf{W}\in\mathbb{R}^{K\times K}\) and \(\alpha\in\mathbb{R}\). Then, for almost all initial values \(\mathbf{x}_{0}\in\mathbb{R}^{N\times K}\) the following holds._

* _The solution to (_2_) is either_ HFD _or_ LFD_._
* _Let_ \(\lambda_{+}(\mathbf{L})\) _and_ \(\lambda_{-}(\mathbf{L})\) _be the smallest positive and negative non-zero eigenvalue of_ \(\mathbf{L}\)_, respectively. The solution to (_2_) is either_ \((1-\lambda_{+}(\mathbf{L}))\)_-FD or_ \((1-\lambda_{-}(\mathbf{L}))\)_-FD._

_Furthermore, the previous results also hold for solutions to the Schrodinger equation (3) if \(\mathbf{W}\in\mathbb{C}^{K\times K}\) has at least one eigenvalue with non-zero imaginary part._

Theorem 5.3\((\alpha>0)\) generalizes the result by Di Giovanni et al. (2023) for \(\alpha=1\) to arbitrary positive values of \(\alpha\). The convergence speed in Theorem 5.3\((\alpha>0)\) depends on the choice of \(\alpha\in\mathbb{R}\). By selecting a variable \(\alpha\) (e.g., as a learnable parameter), we establish a flexible learning framework capable of adapting the convergence speed of the Dirichlet energy. A slower or more adjustable convergence speed facilitates broader frequency exploration as it converges more gradually to its maximal or minimal value. Consequently, the frequency component contributions (for finite time, i.e., in practice) are better balanced, which is advantageous for graphs with different homophily levels. Theorem 5.3\((\alpha<0)\) shows that solutions of the fractional neural ODEs in (2) and (3) are not limited to be LFD or HFD. To demonstrate this and the other results of Theorem 5.3, we solve (2) using an explicit Euler scheme for different choices of \(\alpha\) and \(\mathbf{W}\) on the Cora and Chameleon graphs. The resulting evolution of the Dirichlet energy with respect to time is illustrated in Figure 5. Finally, we refer to Theorem D.5 in Appendix D.1 for the full statement and proof of Theorem 5.3.

**Remark 5.4**.: _Theorem 5.3 is stated for the analytical solutions of (2) and (3), respectively. As noted in Section 5, calculating the analytical solution is infeasible in practice. However, we show in Appendices D.2 to D.3 that approximations of the solution of (2) and (3) via explicit Euler schemes satisfy the same Dirichlet energy convergence properties if the step size is sufficiently small._

**Remark 5.5**.: _Theorem 5.3 can be generalized to all directed graphs with normal SNA, i.e., satisfying the condition \(\mathbf{LL}^{\top}=\mathbf{L}^{\top}\mathbf{L}\). For the complete statement, see Appendix D.1._

Figure 4: Eigendecomposition of \(\mathbf{L}\) for the cycle graph \(C_{8}\) (see Appendix F). The first two rows show the eigenvectors corresponding to the eigenvalues \(\lambda\). The last row shows how the (label-unaware) eigendecomposition can be used to study homophily, whose definition requires the labels.

### Frequency Dominance for Directed Graphs

Section 5.1 analyzes the Dirichlet energy in graphs with normal SNA. However, the situation becomes significantly more complex when considering generic directed graphs. In our experiments (see Figure 5), we observe that the solution to (2) and (3) does not necessarily lead to oversmoothing. On the contrary, the solution can be controlled to exhibit either LFD or HFD for \(\alpha>0\), and mid-frequency-dominance for \(\alpha<0\) as proven for undirected graphs in Theorem 5.3. We present an initial theoretical result for directed graphs, specifically in the case of \(\alpha=1\).

**Theorem 5.6**.: _Let \(\mathcal{G}\) be a directed graph with SNA\(\mathbf{L}\). Consider the initial value problem in (2) with diagonal channel mixing matrix \(\mathbf{W}\in\mathbb{R}^{K\times K}\) and \(\alpha=1\). Suppose \(\lambda_{1}(\mathbf{L})\) is unique. For almost all initial values \(\mathbf{x}_{0}\in\mathbb{R}^{N\times K}\), the solution to (2) is either HFD or LFD._

The proof of Theorem 5.6 is given in Appendix E.1. Finally, we refer to Appendix E.2 for the analogous statement and proof when the solution of (2) is approximated via an explicit Euler scheme.

## 6 Numerical Experiments

This section evaluates the fractional Laplacian ODEs in node classification by approximating (2) and (3) with an explicit Euler scheme. This leads to the following update rules

\[\mathbf{x}_{t+1}=\mathbf{x}_{t}-h\,\mathbf{L}^{\alpha}\mathbf{x}_{t}\mathbf{W }\,,\ \mathbf{x}_{t+1}=\mathbf{x}_{t}+i\;h\;\mathbf{L}^{\alpha}\mathbf{x}_{t} \mathbf{W}\,,\] (4)

for the heat and Schrodinger equation, respectively. In both cases, \(\mathbf{W}\), \(\alpha\) and \(h\) are learnable parameters, \(t\) is the layer, and \(\mathbf{x}_{0}\) is the initial nodes' feature matrix. In accordance with the results in Section 5, we select \(\mathbf{W}\) as a diagonal matrix. The initial features \(\mathbf{x}_{0}\) in (4) are encoded through a \(\mathbb{MLP}\), and the output is decoded using a second MLP. We refer to the resulting model as _FLODE_ (fractional Laplacian ODE). In Appendix A, we present details on the baseline models, the training setup, and the exact hyperparameters.

Figure 5: Convergence of Dirichlet energy for the solution of equation (2) using an explicit Euler scheme with a step size of \(h=10^{-1}\). We consider different \(\alpha\)-FGL in (2) and choose \(\mathbf{W}\) as a random diagonal matrix. In the left plot, \(\mathbf{W}\) has only a negative spectrum, while in the right plot, \(\mathbf{W}\) has only a positive spectrum. The black horizontal line represents the theoretical limit based on Theorem 5.3.

Ablation Study.In Appendix A.3, we investigate the influence of each component (learnable exponent, ODE framework, directionality via the SNA) on the performance of FLODE. The adjustable fractional power in the FGL is a crucial component of FLODE, as it alone outperforms the model employing the ODE framework with a fixed \(\alpha=1\). Further, Appendix A.3 includes ablation studies that demonstrate FLODE's capability to efficiently scale to large depths, as depicted in Figure 8.

Real-World Graphs.We report results on \(6\) undirected datasets consisting of both homophilic graphs, i.e., Cora (McCallum et al., 2000), Citeseer (Sen et al., 2008) and Pubmed (Namata et al., 2012), and heterophilic graphs, i.e., Film (Tang et al., 2009), Squirrel and Chameleon (Rozemberczki et al., 2021). We evaluate our method on the directed and undirected versions of Squirrel, Film, and Chameleon. In all datasets, we use the standard \(10\) splits from (Pei et al., 2019). The choice of the baseline models and their results are taken from (Di Giovanni et al., 2023). Further, we test our method on heterophily-specific graph datasets, i.e., Roman-empire, Minesweeper, Tolokers, and Questions (Platonov et al., 2023). The splits, baseline models, and results are taken from (Platonov et al., 2023). The top three models are shown in Table 1, and the thorough comparison is reported in Table 4. Due to memory limitations, we compute only \(30\%\) of singular values for Pubmed, Roman-Empire, and Questions, which serve as the best low-rank approximation of the original SNA.

Synthetic Directed Graph.We consider the directed stochastic block model (DSBM) datasets (Zhang et al., 2021). The DSBM divides nodes into \(5\) clusters and assigns probabilities for interactions between vertices. It considers two sets of probabilities: \(\{\alpha_{i,j}\}\) for undirected edge creation and \(\{\beta_{i,j}\}\) for assigning edge directions, \(i,j\in\{1,\ldots 5\}\). The objective is to classify vertices based on their clusters. In the first experiment, \(\alpha_{i,j}=\alpha^{*}\) varies, altering neighborhood information's importance. In the second experiment, \(\beta_{i,j}=\beta^{*}\) varies, changing directional information. The results are shown in Figure 6 and Table 6. The splits, baseline models, and results are taken from (Zhang et al., 2021).

Results.The experiments showcase the flexibility of FLODE, as it can accommodate various types of graphs, both directed and undirected, as well as a broad range of homophily levels. While other methods, such as MagNet (Zhang et al., 2021), perform similarly to our approach, they face limitations when applied to certain graph configurations. For instance, when applied to undirected graphs, MagNet reduces to ChebNet, making it unsuitable for heterophilic graphs. Similarly, GRAFF

\begin{table}

\end{table}
Table 1: Test accuracy (Film, Squirrel, Chameleon, Citeseer) and test AUROC (Minesweeper, Tolokers, Questions) on node classification, top three models. The thorough comparison is reported in Table 4, Appendix A: FLODE consistently outperforms the baseline models GCN and GRAFF, and it achieves results comparable to state-of-the-art.

(Di Giovanni et al., 2023) performs well on undirected graphs but falls short on directed graphs. We note that oftentimes FLODE learns a non-trivial exponent \(\alpha\neq 1\), highlighting the advantages of \(\mathrm{FGL}\)-based \(\mathrm{GNNs}\) (see, e.g., Table 5). Furthermore, as shown in Table 9 and Appendix A.3, our empirical results align closely with the theoretical results in Section 5.

## 7 Conclusion

In this work, we introduce the concepts of Dirichlet energy and oversmoothing for directed graphs and demonstrate their relation with the \(\mathrm{SNA}\). Building upon this foundation, we define fractional graph Laplacians in the singular value domain, resulting in matrices capable of capturing long-range dependencies. To address oversmoothing in directed graphs, we propose fractional Laplacian-based graph ODEs, which are provably not limited to \(\mathrm{LFD}\) behavior. We finally show the flexibility of our method to accommodate various graph structures and homophily levels in node-level tasks.

Limitations and Future Work.The computational cost of the SVD grows cubically in \(N\), while the storage of the singular vectors grows quadratically in \(N\). Both costs can be significantly reduced by computing only \(k\ll N\) singular values via truncated SVD (Figure 7), giving the best \(k\)-rank approximation of the \(\mathrm{SNA}\). Moreover, the SVD can be computed offline as a preprocessing step.

The frequency analysis of \(\alpha\)-\(\mathrm{FGL}\) neural \(\mathrm{ODEs}\) in directed graphs is an exciting future direction. It would also be worthwhile to investigate the impact of choosing \(\alpha\neq 1\) on the convergence speed of the Dirichlet energy. Controlling the speed could facilitate the convergence of the Dirichlet energy to an _optimal_ value, which has been shown to exist in synthetic settings (Keriven, 2022; X. Wu et al., 2022). Another interesting future direction would be to analyze the dynamics when approximating the solution to the \(\mathrm{FGL}\) neural \(\mathrm{ODEs}\) using alternative numerical solvers, such as adjoint methods.

Figure 6: Experiments on directed stochastic block model. Unlike other models, FLODE’s performances do not deteriorate as much when changing the inter-cluster edge density \(\alpha^{*}\).

Figure 7: Effect of truncated SVD on test accuracy (orange) for standard directed real-world graphs. The explained variance, defined as \(\sum_{i=1}^{k}\sigma_{i}^{2}/\sum_{j=1}^{N}\sigma_{j}^{2}\), measures the variability the first \(k\) singular values explain. For chameleon, the accuracy stabilized after \(570\) (\(25\%\)) singular values, corresponding to an explained variance of \(0.998\). For squirrel, after \(1600\) (\(31\%\)) singular values, which correspond to an explained variance \(0.999\), the improvement in test accuracy is only marginal.

## Acknowledgments

S. M. acknowledges partial support by the NSF-Simons Research Collaboration on the Mathematical and Scientific Foundations of Deep Learning (MoDL) (NSF DMS 2031985) and DFG SPP 1798, KU 1446/27-2.

G. K. acknowledges partial support by the DAAD programme Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the Federal Ministry of Education and Research. G. Kutyniok also acknowledges support from the Munich Center for Machine Learning (MCML) as well as the German Research Foundation under Grants DFG-SPP-2298, KU 1446/31-1 and KU 1446/32-1 and under Grant DFG-SFB/TR 109 and Project C09.

## References

* Benzi et al. (2020) Benzi, M., Bertaccini, D., Durastante, F., and Simunec, I. (2020). "Non-Local Network Dynamics via Fractional Graph Laplacians". In: _Journal of Complex Networks_ 8.3.
* Bo et al. (2021) Bo, D., Wang, X., Shi, C., and Shen, H. (2021). "Beyond Low-frequency Information in Graph Convolutional Networks". In: _Proceedings of the AAAI Conference on Artificial Intelligence_ 35.5, pp. 3950-3957.
* Bodnar et al. (2022) Bodnar, C., Di Giovanni, F., Chamberlain, B., Lio, P., and Bronstein, M. (2022). "Neural Sheaf Diffusion: A Topological Perspective on Heterophily and Oversmoothing in GNNs". In: _Advances in Neural Information Processing Systems_ 35, pp. 18527-18541.
* Bronstein et al. (2017) Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., and Vandergheynst, P. (2017). "Geometric Deep Learning: Going beyond Euclidean Data". In: _IEEE Signal Processing Magazine_ 34.4, pp. 18-42.
* Cai and Wang (2020) Cai, C. and Wang, Y. (2020). _A Note on Over-Smoothing for Graph Neural Networks_. arXiv: 2006. 13318 [cs, stat].
* Chamberlain et al. (2021) Chamberlain, B., Rowbottom, J., Gorinova, M. I., Bronstein, M., Webb, S., and Rossi, E. (2021). "GRAND: Graph Neural Diffusion". In: _Proceedings of the 38th International Conference on Machine Learning_. PMLR, pp. 1407-1418.
* Chen et al. (2020) Chen, D., Lin, Y., Li, W., Li, P., Zhou, J., and Sun, X. (2020). "Measuring and Relieving the Over-Smoothing Problem for Graph Neural Networks from the Topological View". In: _Proceedings of the AAAI Conference on Artificial Intelligence_ 34.04, pp. 3438-3445.
* Chen et al. (2020) Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y. (2020). "Simple and Deep Graph Convolutional Networks". In: _Proceedings of the 37th International Conference on Machine Learning_. PMLR, pp. 1725-1735.
* Chen et al. (2018) Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. (2018). "Neural Ordinary Differential Equations". In: _Advances in Neural Information Processing Systems_. Vol. 31. Curran Associates, Inc.
* Chien et al. (2021) Chien, E., Peng, J., Li, P., and Milenkovic, O. (2021). "Adaptive Universal Generalized PageRank Graph Neural Network". In: _International Conference on Learning Representations_.
* Choi et al. (2023) Choi, J., Hong, S., Park, N., and Cho, S.-B. (2023). "GREAD: Graph Neural Reaction-Diffusion Networks". In: _Proceedings of the 40th International Conference on Machine Learning_. Vol. 202. ICML'23. Honolulu, Hawaii, USA: JMLR.org, pp. 5722-5747.
* Chung (1997) Chung, F. R. K. (1997). _Spectral Graph Theory_. Regional Conference Series in Mathematics no. 92. Providence, R.I: Published for the Conference Board of the mathematical sciences by the American Mathematical Society.
* Defferrard et al. (2016) Defferrard, M., Bresson, X., and Vandergheynst, P. (2016). "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering". In: _Advances in Neural Information Processing Systems_. Vol. 29. Curran Associates, Inc.
* Di Giovanni et al. (2023) Di Giovanni, F., Rowbottom, J., Chamberlain, B. P., Markovich, T., and Bronstein, M. M. (2023). "Understanding convolution on graphs via energies". In: _Transactions on Machine Learning Research_.
* Du et al. (2022) Du, L., Shi, X., Fu, Q., Ma, X., Liu, H., Han, S., and Zhang, D. (2022). "GBK-GNN: Gated Bi-Kernel Graph Neural Networks for Modeling Both Homophily and Heterophily". In: _Proceedings of the ACM Web Conference 2022_. WWW '22. New York, NY, USA: Association for Computing Machinery, pp. 1550-1558.
* Eliasof et al. (2021) Eliasof, M., Haber, E., and Treister, E. (2021). "PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations". In: _Advances in Neural Information Processing Systems_. Vol. 34. Curran Associates, Inc., pp. 3836-3849.
* Li et al. (2020)Fan, W., Ma, Y., Li, Q., He, Y., Zhao, E., Tang, J., and Yin, D. (2019). "Graph Neural Networks for Social Recommendation". In: _The World Wide Web Conference_. San Francisco CA USA: ACM, pp. 417-426.
* Fey and Lenssen (2019) Fey, M. and Lenssen, J. E. (2019). "Fast Graph Representation Learning with PyTorch Geometric". In: _ICLR 2019 Workshop on Representation Learning on Graphs and Manifolds_.
* Gasteiger et al. (2018) Gasteiger, J., Bojchevski, A., and Gunnemann, S. (2018). "Predict Then Propagate: Graph Neural Networks Meet Personalized PageRank". In: _International Conference on Learning Representations_.
* Gilmer et al. (2017) Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. (2017). "Neural Message Passing for Quantum Chemistry". In: _Proceedings of the 34th International Conference on Machine Learning_. PMLR, pp. 1263-1272.
* Gori et al. (2005) Gori, M., Monfardini, G., and Scarselli, F. (2005). "A New Model for Learning in Graph Domains". In: _Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005_. Vol. 2, 729-734 vol. 2.
* Haber and Ruthotto (2018) Haber, E. and Ruthotto, L. (2018). "Stable Architectures for Deep Neural Networks". In: _Inverse Problems_ 34.1, p. 014004. eprint: 1705.03341 (cs, math).
* Hamilton et al. (2017) Hamilton, W., Ying, Z., and Leskovec, J. (2017). "Inductive Representation Learning on Large Graphs". In: _Advances in Neural Information Processing Systems_. Vol. 30. Curran Associates, Inc.
* He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. (2016). "Deep Residual Learning for Image Recognition". In: _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 770-778.
* Keriven (2022) Keriven, N. (2022). "Not Too Little, Not Too Much: A Theoretical Analysis of Graph (over)Smoothing". In: _Advances in Neural Information Processing Systems_.
* Kingma and Ba (2015) Kingma, D. P. and Ba, J. (2015). _Adam: A Method for Stochastic Optimization_. Ed. by Y. Bengio and Y. LeCun.
* Kipf and Welling (2017) Kipf, T. N. and Welling, M. (2017). "Semi-Supervised Classification with Graph Convolutional Networks". In: _International Conference on Learning Representations_.
* Li et al. (2019) Li, G., Muller, M., Thabet, A., and Ghanem, B. (2019). "DeepGCNs: Can GCNs Go As Deep As CNNs?" In: _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 9267-9276.
* Li et al. (2018) Li, Q., Han, Z., and Wu, X.-m. (2018). "Deeper Insights Into Graph Convolutional Networks for Semi-Supervised Learning". In: _Proceedings of the AAAI Conference on Artificial Intelligence_ 32.1.
* Li et al. (2022) Li, X., Zhu, R., Cheng, Y., Shan, C., Luo, S., Li, D., and Qian, W. (2022). "Finding Global Homophily in Graph Neural Networks When Meeting Interophily". In: _Proceedings of the 39th International Conference on Machine Learning_. PMLR, pp. 13242-13256.
* Lingam et al. (2021) Lingam, V., Ragesh, R., Iyer, A., and Sellamickam, S. (2021). _Simple Truncated SVD Based Model for Node Classification on Heterophilic Graphs_. eprint: 2106.12807 (cs).
* Luan et al. (2022) Luan, S., Hua, C., Lu, Q., Zhu, J., Zhao, M., Zhang, S., Chang, X.-W., and Precup, D. (2022). "Revisiting Heterophily For Graph Neural Networks". In: _Advances in Neural Information Processing Systems_ 35, pp. 1362-1375.
* Luan et al. (2019) Luan, S., Zhao, M., Chang, X.-W., and Precup, D. (2019). "Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks". In: _Advances in Neural Information Processing Systems_. Vol. 32. Curran Associates, Inc.
* Maurya et al. (2021) Maurya, S. K., Liu, X., and Murata, T. (2021). _Improving Graph Neural Networks with Simple Architecture Design_. arXiv: 2105.07634 [cs, stat].
* McCallum et al. (2000) McCallum, A. K., Nigam, K., Rennie, J., and Seymore, K. (2000). "Automating the Construction of Internet Portals with Machine Learning". In: _Information Retrieval 3.2_, pp. 127-163.
* Monti et al. (2019) Monti, F., Frasca, F., Eynard, D., Mannion, D., and Bronstein, M. M. (2019). _Fake News Detection on Social Media Using Geometric Deep Learning_. arXiv: 1902.06673.
* Namata et al. (2012) Namata, G. M., London, B., Getoor, L., and Huang, B. (2012). "Query-driven Active Surveying for Collective Classification". In: _Workshop on Mining and Learning with Graphs_.
* Oono and Suzuki (2019) Oono, K. and Suzuki, T. (2019). "Graph Neural Networks Exponentially Lose Expressive Power for Node Classification". In: _International Conference on Learning Representations_.
* Paszke et al. (2019) Paszke, A. et al. (2019). "PyTorch: An Imperative Style, High-Performance Deep Learning Library". In: _Advances in Neural Information Processing Systems_. Vol. 32. Curran Associates, Inc.
* Pei et al. (2019) Pei, H., Wei, B., Chang, K. C.-C., Lei, Y., and Yang, B. (2019). "Geom-GCN: Geometric Graph Convolutional Networks". In: _International Conference on Learning Representations_.
* Perko (2001) Perko, L. (2001). _Differential Equations and Dynamical Systems_. Ed. by J. E. Marsden, L. Sirovich, and M. Golubitsky. Vol. 7. Texts in Applied Mathematics. New York, NY: Springer New York.
* Prakash et al. (2019)Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. (2023). "A critical look at the evaluation of GNNs under heterophily: Are we really making progress?" In: _The Eleventh International Conference on Learning Representations_. Poli, Massardi, S., Park, J., Yamashita, A., Asama, H., and Park, J. (2021). _Graph Neural Ordinary Differential Equations_. arXiv: 1911.07532 [cs, stat]. Pozrikidis, C. (2018). _The Fractional Laplacian_. First. Boca Raton : Taylor & Francis, 2016.!"A CRC title.": Chapman and Hall/CRC. Rong, Y., Huang, W., Xu, T., and Huang, J. (2020). "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification". In: _International Conference on Learning Representations_. Rozemberczki, B., Allen, C., and Sarkar, R. (2021). "Multi-Scale Attributed Node Embedding". In: _Journal of Complex Networks_ 9.2. Rusch, T. K., Chamberlain, B., Rowbottom, J., Mishra, S., and Bronstein, M. (2022). "Graph-Coupled Oscillator Networks". In: _Proceedings of the 39th International Conference on Machine Learning_. PMLR, pp. 18888-18909. Scarselli, F., Gori, A. C., Hagenbuchner, M., and Monfardini, G. (2009). "The Graph Neural Network Model". In: _IEEE Transactions on Neural Networks_ 20.1, pp. 61-80. Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., and Eliassi-Rad, T. (2008). "Collective Classification in Network Data". In: _AI Magazine_ 29.3, p. 93. Shi, Y., Huang, Z., Feng, S., Zhong, H., Wang, W., and Sun, Y. (2021). "Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification". In: _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence_. Montreal, Canada: International Joint Conferences on Artificial Intelligence Organization, pp. 1548-1554. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). "Dropout: A Simple Way to Prevent Neural Networks from Overfitting". In. Tang, J., Sun, J., Wang, C., and Yang, Z. (2009). "Social Influence Analysis in Large-Scale Networks". In: _Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_. Paris France: ACM, pp. 807-816. Tong, Z., Liang, Y., Sun, C., Li, X., Rosenblum, D., and Lim, A. (2020). "Digraph Inception Convolutional Networks". In: _Advances in Neural Information Processing Systems_. Vol. 33. Curran Associates, Inc., pp. 17907-17918. Tong, Z., Liang, Y., Sun, C., Rosenblum, D. S., and Lim, A. (2020). _Directed Graph Convolutional Network_. arXiv: 2004.13970 [cs, stat]. Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y. (2018). "Graph Attention Networks". In: _International Conference on Learning Representations_. Wang, J., Huang, P., Zhao, H., Zhang, Z., Zhao, B., and Lee, D. L. (2018). "Billion-Scale Commodity Embedding for E-commerce Recommendation in Alibaba". In: _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_. KDD '18. New York, NY, USA: Association for Computing Machinery, pp. 839-848. Wang, X. and Zhang, M. (2022). "How Powerful Are Spectral Graph Neural Networks". In: _Proceedings of the 39th International Conference on Machine Learning_. PMLR, pp. 23341-23362. Wang, Y., K., Liu, X., Wang, Y. G., and Jin, S. (2022). "ACMP: Allen-Cahn Message Passing with Attractive and Repulsive Forces for Graph Neural Networks". In: _The Eleventh International Conference on Learning Representations_. Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Weinberger, K. (2019). "Simplifying Graph Convolutional Networks". In: _Proceedings of the 36th International Conference on Machine Learning_. PMLR, pp. 6861-6871. Wu, X., Chen, Z., Wang, W. W., and Jadbabaie, A. (2022). "A Non-Asymptotic Analysis of Over-smoothing in Graph Neural Networks". In: _The Eleventh International Conference on Learning Representations_. Xhonneux, L.-P., Qu, M., and Tang, J. (2020). "Continuous Graph Neural Networks". In: _Proceedings of the 37th International Conference on Machine Learning_. PMLR, pp. 10432-10441. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. (2018). "How Powerful Are Graph Neural Networks?" In: _International Conference on Learning Representations_. Yan, Y., Hashemi, M., Swersky, K., Yang, Y., and Koutra, D. (2022). "Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks". In: _2022 IEEE International Conference on Data Mining (ICDM)_. Orlando, FL, USA: IEEE, pp. 1287-1292. Zhang, X., He, Y., Brugnone, N., Perlmutter, M., and Hirn, M. (2021). "MagNet: A Neural Network for Directed Graphs". In: _Advances in Neural Information Processing Systems_. Vol. 34. Curran Associates, Inc., pp. 27003-27015.

Zhao, L. and Akoglu, L. (2019). "PairNorm: Tackling Oversmoothing in GNNs". In: _International Conference on Learning Representations_.
* Zhu et al. (2021) Zhu, J., Rossi, R. A., Rao, A., Mai, T., Lipka, N., Ahmed, N. K., and Koutra, D. (2021). "Graph Neural Networks with Heterophily". In: _Proceedings of the AAAI Conference on Artificial Intelligence_ 35.12, pp. 11168-11176.
* Zhu et al. (2020) Zhu, J., Yan, Y., Zhao, L., Heimann, M., Akoglu, L., and Koutra, D. (2020). "Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs". In: _Advances in Neural Information Processing Systems_. Vol. 33. Curran Associates, Inc., pp. 7793-7804.
* Zou et al. (2022) Zou, C., Han, A., Lin, L., and Gao, J. (2022). _A Simple Yet Effective SVD-GCN for Directed Graphs_. arXiv: 2205.09335 [cs].

## Acronyms

\begin{tabular}{l l} AUROC & Area under the ROC curve \\ DSBM & Directed Stochastic Block Model \\ FD & Frequency Dominant \\ FGL & Fractional Graph Laplacian \\ GAT & Graph Attention Network \\ GCN & Graph Convolutional Network \\ GNN & Graph Neural Network \\ HFD & Highest Frequency Dominant \\ LCC & Largest Connected Components \\ LFD & Lowest Frequency Dominant \\ MLP & Multi-Layer Perceptron \\ ODE & Ordinary Differential Equation \\ SNA & Symmetrically Normalized Adjacency \\ SVD & Singular Value Decomposition \\ \end{tabular}

## Notation

\begin{tabular}{l l} \(i\) & Imaginary unit \\ \(\Re(z)\) & Real part of \(z\in\mathbb{C}\) \\ \(\Im(z)\) & Imaginary part of \(z\in\mathbb{C}\) \\ \(\mathrm{diag}(\mathbf{x})\) & Diagonal matrix with \(\mathbf{x}\) on the diagonal. \\
**1** & Constant vector of all \(1\)s. \\ \(\mathbf{M}^{\mathsf{T}}\) & Transpose of \(\mathbf{M}\) \\ \(\mathbf{M}^{*}\) & Conjugate of \(\mathbf{M}\) \\ \(\mathbf{M}^{\mathsf{H}}\) & Conjugate transpose of \(\mathbf{M}\) \\ \(\left\|\mathbf{M}\right\|\) & Spectral norm of \(\mathbf{M}\) \\ \(\left\|\mathbf{M}\right\|_{2}\) & Frobenius norm of \(\mathbf{M}\) \\ \(\lambda\left(\mathbf{M}\right)\) & Spectrum of \(\mathbf{M}\) \\ \(\sigma\left(\mathbf{M}\right)\) & Singular values of \(\mathbf{M}\) \\ \(\mathcal{E}\left(\mathbf{x}\right)\) & Dirichlet energy computed on \(\mathbf{x}\) \\ \(\mathcal{H}\left(\mathcal{G}\right)\) & Homophily coefficient of the graph \(\mathcal{G}\) \\ \(\mathbf{A}\otimes\mathbf{B}\) & Kronecker product between \(\mathbf{A}\) and \(\mathbf{B}\) \\ \(\mathrm{vec}\left(\mathbf{M}\right)\) & Vector obtained stacking columns of \(\mathbf{M}\). \\ \end{tabular}

Implementation Details

In this section, we give the details on the numerical results in Section 6. We begin by describing the exact model.

Model architecture.Let \(\mathcal{G}\) be a directed graphs and \(\mathbf{x}_{0}\in\mathbb{R}^{N\times K}\) the node features. Our architecture first embeds the input node features \(\mathbf{x}\) via a multi-layer perceptron (\(\mathtt{MLP}\)). We then evolve the features \(\mathbf{x}_{0}\) according to a slightly modified version of (3), i.e, \(\mathbf{x}^{\prime}(t)=-i\,\mathbf{L}^{\alpha}\mathbf{x}(t)\mathbf{W}\) for some time \(t\in[0,T]\). In our experiments, we approximate the solution with an Explicit Euler scheme with step size \(h>0\). This leads to the following update rule

\[\mathbf{x}_{t+1}=\mathbf{x}_{t}-ih\mathbf{L}^{\alpha}\mathbf{x}_{t}\mathbf{W}\,.\]

The channel mixing matrix is a diagonal learnable matrix \(\mathbf{W}\in\mathbb{C}^{K\times K}\), and \(\alpha\in\mathbb{R}\), \(h\in\mathbb{C}\) are also learnable parameters. The features at the last time step \(\mathbf{x}_{T}\) are then fed into a second \(\mathtt{MLP}\), whose output is used as the final output. Both \(\mathtt{MLPs}\) use LeakyReLU as non-linearity and dropout (Srivastava et al., 2014). On the contrary, the graph layers do not use any dropout nor non-linearity. A sketch of the algorithm is reported in \(\mathtt{fLode}\).

``` % A, \(\mathbf{x}_{0}\) are given. % Preprocessing
1\(\mathbf{D}_{\text{in}}\) = \(\operatorname{diag}\left(\mathbf{A}\mathbf{1}\right)\)
2\(\mathbf{D}_{\text{out}}\) = \(\operatorname{diag}\left(\mathbf{A}^{\mathsf{T}}\mathbf{1}\right)\)
3\(\mathbf{L}\) = \(\mathbf{D}_{\text{in}}^{-1/2}\mathbf{A}\mathbf{D}_{\text{out}}^{-1/2}\)
4\(\mathbf{U},\mathbf{\Sigma},\mathbf{V}^{\mathsf{H}}\) = \(\mathbf{svd}(\mathbf{L})\)
5 The core of the algorithm is very simple
6def training_step(\(\mathbf{x}_{0}\)):
7\(\mathbf{x}_{0}\) = input_MLP(\(\mathbf{x}_{0}\))
8for\(t\in\{1,\dots,T\}\)do
9\(\mathbf{x}_{t}\) = \(\mathbf{x}_{t-1}-i\,h\,\mathbf{U}\mathbf{\Sigma}^{\alpha}\mathbf{V}^{\mathsf{ H}}\mathbf{x}_{t-1}\mathbf{W}\)
10\(\mathbf{x}_{T}\) = output_MLP(\(\mathbf{x}_{T}\))
11return\(\mathbf{x}_{T}\) ```

**Algorithm 1**\(\mathtt{fLode}\)

Complexity.The computation of the \(\mathtt{SVD}\) is \(\mathcal{O}(N^{3})\). However, one can compute only the first \(p\ll N\) singular values: this cuts down the cost to \(\mathcal{O}(N^{2}\,p)\). The memory required to store the singular vectors is \(\mathcal{O}(N^{2})\), since they are not sparse in general. Each training step has a cost of \(\mathcal{O}(N^{2}\,K)\).

Experimental details.Our model is implemented in PyTorch(Paszke et al., 2019), using PyTorchgeometric(Fey et al., 2019). The computation of the \(\mathtt{SVD}\) for the fractional Laplacian is implemented using the library \(\mathtt{linalg}\) provided by PyTorch. In the case of truncated \(\mathtt{SVD}\), we use the function \(\mathtt{randomized\_svd}\) provided by the library \(\mathtt{extmath}\) from sklearn. The code and instructions to reproduce the experiments are available on GitHub. Hyperparameters were tuned using grid search. All experiments were run on an internal cluster with NVIDIA GeForce RTX 2080 Ti and NVIDIA TITAN RTX GPUs with 16 and 24 GB of memory, respectively.

Training details.All models were trained for \(1000\) epochs using Adam (Kingma et al., 2015) as optimizer with a fixed learning rate. We perform early stopping if the validation metric does not increase for \(200\) epochs.

### Real-World Graphs

Undirected graphsWe conducted \(10\) repetitions using data splits obtained from (Pei et al., 2019). For each split, \(48\%\) of the nodes are used for training, \(32\%\) for validation and \(20\%\) for testing. In all datasets, we considered the largest connected component (\(\mathtt{LCC}\)). Chameleon, Squirrel, and Film are directed graphs; hence, we converted them to undirected. Cora, Citeseer, and Pubmed are already undirected graphs: to these, we added self-loops. We normalized the input node features for all graphs.

As baseline models, we considered the same models as in (Di Giovanni et al., 2023). The results were provided by Pei et al. (2019) and include standard GNNs, such as GAT (Velickovic et al., 2018), GCN (Kipf et al., 2017), and GraphSAGE (Hamilton et al., 2017). We also included models designed to address oversmoothing and heterophilic graphs, such as PairNorm (L. Zhao et al., 2019), GGCN (Yan et al., 2022), Geom-GCN (Pei et al., 2019), \(\mathrm{H}_{2}\mathrm{GCN}\)(Zhu et al., 2020), GPRGNN (Chien et al., 2021), and Sheaf (Bodnar et al., 2022). Furthermore, we included the graph neural ODE-based approaches, CGNN (Xhonneux et al., 2020) and GRAND (Chamberlain et al., 2021), as in (Di Giovanni et al., 2023), and the model GRAFF from (Di Giovanni et al., 2023) itself. Finally, we included GREAD (Choi et al., 2023), GraphCON (Rusch et al., 2022), ACMP (Y. Wang et al., 2022) and GCN and GAT equipped with DropEdge (Rong et al., 2020).

Heterophily-specific ModelsFor heterophily-specific datasets, we use the same models and results as in (Platonov et al., 2023). As baseline models we considered the topology-agnostic ResNet (He et al., 2016) and two graph-aware modifications: ResNet+SGC(F. Wu et al., 2019) where the initial node features are multiplied by powers of the SNA, and ResNet+adj, where rows of the adjacency matrix are used as additional node features; GCN (Kipf et al., 2017), GraphSAGE (Hamilton et al., 2017); GAT (Velickovic et al., 2018) and GT (Shi et al., 2021) as well as their modification GAT-sep and GT-sep which separate ego- and neighbor embeddings; \(\mathrm{H}_{2}\mathrm{GCN}\)(Zhu et al., 2020), CPGNN (Zhu, Rossi, et al., 2021), GPRGNN (Chien et al., 2021), FSGNN (Maurya et al., 2021), GloGNN (X. Li et al., 2022), FAGCN (Bo et al., 2021), GBK-GNN (Du et al., 2022), and JacobiConv (X. Wang et al., 2022).

The exact hyperparameters for FLODE are provided in Table 5.

### Synthetic Directed Graphs

The dataset and code are taken from (Zhang et al., 2021). As baseline models, we considered the ones in (Zhang et al., 2021) for which we report the corresponding results. The baseline models include standard GNNs, such as ChebNet (Defferrard et al., 2016), GCN (Kipf et al., 2017), GraphSAGE (Hamilton et al., 2017), APPNP (Gasteiger et al., 2018), GIN (Xu et al., 2018), GAT (Velickovic et al., 2018), but also models specifically designed for directed graphs, such as DGCN (Tong et al., 2020), DiGraph and DiGraphIB (Tong et al., 2020), MagNet (Zhang et al., 2021)).

The DSBM dataset.The directed stochastic block model (DSBM) is described in detail in (Zhang et al., 2021, Section 5.1.1). To be self-contained, we include a short explanation.

The DSBM model is defined as follows. There are \(N\) vertices, which are divided into \(n_{c}\) clusters \((C_{1},C_{2},...C_{n_{c}})\), each having an equal number of vertices. An interaction is defined between any two distinct vertices, \(u\) and \(v\), based on two sets of probabilities: \(\{\alpha_{i,j}\}_{i,j=1}^{n_{c}}\) and \(\{\beta_{i,j}\}_{i,j=1}^{n_{c}}\).

The set of probabilities \(\{\alpha_{i,j}\}\) is used to create an undirected edge between any two vertices \(u\) and \(v\), where \(u\) belongs to cluster \(C_{i}\) and \(v\) belongs to cluster \(C_{j}\). The key property of this probability set is that \(\alpha_{i,j}=\alpha_{j,i}\), which means the chance of forming an edge between two clusters is the same in either direction.

The set of probabilities \(\{\beta_{i,j}\}\) is used to assign a direction to the undirected edges. For all \(i,j\in\{1,\ldots,n_{c}\}\), we assume that \(\beta_{i,j}+\beta_{j,i}=1\) holds. Then, to the undirected edge \((u,v)\) is assigned the direction from \(u\) to \(v\) with probability \(\beta_{i,j}\) if \(u\) belongs to cluster \(C_{i}\) and \(v\) belongs to cluster \(C_{j}\), and the direction from \(v\) to \(u\) with probability \(\beta_{j,i}\).

The primary objective here is to classify the vertices based on their respective clusters.

There are several scenarios designed to test different aspects of the baseline models and our model. In the experiments, the total number of nodes is fixed at \(N=2500\) and the number of clusters is fixed at \(n_{c}=5\). In all experiments, the training set contains \(20\) nodes per cluster, \(500\) nodes for validation, and the rest for testing. The results are averaged over 5 different seeds and splits.

\begin{table}

\end{table}
Table 4: Test accuracy on node classification: top three models indicated as \(\lfloor\)1, \(\lfloor\)2, \(\lfloor\)3, \(\lfloor\)2, \(\lfloor\)3, \(\lfloor\)4\(\rfloor\).

\begin{table}

\end{table}
Table 5: Selected hyperparameters, learned exponent, step size, and Dirichlet energy in the last layer for real-world datasets.

[MISSING_PAGE_FAIL:20]

via grid search. The test accuracy corresponding to the hyperparameters that yielded maximum validation accuracy is reported in Table 8.

The ablation study on Chameleon demonstrates that all the components of the model (learnable exponent, ODE framework with the Schrodinger equation, and directionality via the SNA) contribute to the performance of FLODE. The fact that performance drops when any of these components are not used suggests that they all play crucial roles in the model's ability to capture the structure and evolution of heterophilic graphs. It is important to note that the performance appears to be more dependent on the adjustable fraction in the FGL than on the use of the ODE framework, illustrating that the fractional Laplacian alone can effectively capture long-range dependencies. However, when the ODE framework is additionally employed, a noticeable decrease in variance is observed.

From Theory to Practice.We conduct an ablation study to investigate the role of depth on Chameleon, Citeseer, Cora, and Squirrel datasets. The results, depicted in Figure 8, demonstrate that the neural ODE framework enables GNNs to scale to large depths (\(256\) layers). Moreover, we see that the fractional Laplacian improves over the standard Laplacian in the heterophilic graphs which is supported by our claims in Section 5.2. We highlight that using only the fractional Laplacian without the neural ODE framework oftentimes outperforms the standard Laplacian with the neural ODE framework. This indicates the importance of the long-range connections built by the fractional Laplacian.

We further demonstrate the close alignment of our theoretical and experimental results, which enables us to precisely anticipate when the models will exhibit HFD or LFD behaviors. In this context, we calculate parameters (according to Theorem D.5) and illustrate at each depth the expected and observed behaviors. For Squirrel and Chameleon, which are heterophilic graphs, we observe that both their theoretical and empirical behaviors are HFD. Additionally, the learned exponent is small. In contrast, for Cora and Citeseer, we see the opposite.

Finally, we employ the best hyperparameters in Table 4(a) to solve both fractional heat and Schrodinger graph ODEs, further substantiating the intimate link between our theoretical advancements and practical applications.

\begin{table}

\end{table}
Table 7: Selected hyperparameters for \(\mathrm{DSBM}\) dataset.

[MISSING_PAGE_EMPTY:22]

Figure 8: Ablation study on the effect of different update rules and different number of layers on undirected datasets. The x-axis shows the number of layers \(2^{L}\) for \(L\in\{0,\ldots,8\}\). FD is calculated according to Theorem 5.3.

[MISSING_PAGE_FAIL:24]

\[=\sum_{j=1}^{N}\frac{a_{i,j}}{\sqrt{d_{i}^{\text{in}}}}\left(\frac{x_{j}}{ \sqrt{d_{j}^{\text{out}}}}-\frac{x_{i}}{\sqrt{d_{i}^{\text{in}}}}\right)\,,\]

hence, the graph is weakly balanced.

By Perron-Frobenius theorem for irreducible non-negative matrices, one gets that \(\mathbf{L}\) has exactly \(h\) eigenvalues with maximal modulus corresponding to the \(h\) roots of the unity, where \(h\) is the period of \(\mathbf{L}\). Hence, \(-1\) is an eigenvalue of \(\mathbf{L}\) if and only if the graph is weakly balanced and \(h\) is even. 

**Proposition 3.6**.: _For every \(\mathbf{x}\in\mathbb{C}^{N\times K}\), we have_

\[\Re\left(\operatorname{trace}\left(\mathbf{x}^{\mathsf{H}}\left(\mathbf{I}- \mathbf{L}\right)\mathbf{x}\right)\right)=\frac{1}{2}\sum_{i,j=1}^{N}a_{i,j} \left\|\frac{\mathbf{x}_{i}}{\sqrt{d_{i}^{\text{in}}}}-\frac{\mathbf{x}_{j}}{ \sqrt{d_{j}^{\text{out}}}}\right\|_{2}^{2},\]

_Moreover, there exists \(\mathbf{x}\neq 0\) such that \(\mathscr{E}(\mathbf{x})=0\) if and only if the graph is weakly balanced._

Proof.: By direct computation, it holds

\[\frac{1}{2}\sum_{i,j=1}^{N}a_{i,j}\left\|\frac{x_{i,:}}{\sqrt{d_{ i}^{\text{in}}}}-\frac{x_{j,:}}{\sqrt{d_{i}^{\text{out}}}}\right\|_{2}^{2}\] \[=\frac{1}{2}\sum_{i,j=1}^{N}a_{i,j}\sum_{k=1}^{K}\left|\frac{x_{i, k}}{\sqrt{d_{i}^{\text{in}}}}-\frac{x_{j,k}}{\sqrt{d_{j}^{\text{out}}}} \right|^{2}\] \[=\frac{1}{2}\sum_{i,j=1}^{N}a_{i,j}\sum_{k=1}^{K}\left(\frac{x_{i, k}}{\sqrt{d_{i}^{\text{in}}}}-\frac{x_{j,k}}{\sqrt{d_{j}^{\text{out}}}} \right)^{*}\left(\frac{x_{i,k}}{\sqrt{d_{i}^{\text{in}}}}-\frac{x_{j,k}}{\sqrt{ d_{j}^{\text{out}}}}\right)\] \[=\frac{1}{2}\sum_{i,j=1}^{N}\sum_{k=1}^{K}a_{i,j}\frac{{|x_{i,k}|} ^{2}}{d_{i}^{\text{in}}}+\frac{1}{2}\sum_{i,j=1}^{N}\sum_{k=1}^{K}a_{i,j}\frac {{|x_{j,k}|}^{2}}{d_{j}^{\text{out}}}\] \[-\frac{1}{2}\sum_{i,j=1}^{N}\sum_{k=1}^{K}a_{i,j}\frac{x_{i,k}^{* }x_{j,k}}{\sqrt{d_{i}^{\text{in}}\,d_{j}^{\text{out}}}}-\frac{1}{2}\sum_{i,j=1 }^{N}\sum_{k=1}^{K}a_{i,j}\frac{x_{i,k}x_{j,k}^{*}}{\sqrt{d_{i}^{\text{in}}\,d _{j}^{\text{out}}}}\] \[=\frac{1}{2}\sum_{i=1}^{N}\sum_{k=1}^{K}{|x_{i,k}|}^{2}+\frac{1}{ 2}\sum_{j=1}^{N}\sum_{k=1}^{K}{|x_{j,k}|}^{2}-\frac{1}{2}\sum_{i,j=1}^{N}\sum_ {k=1}^{K}{k}a_{i,j}\frac{x_{i,k}^{*}x_{j,k}}{\sqrt{d_{i}^{\text{in}}\,d_{j}^{ \text{out}}}}-\frac{1}{2}\sum_{i,j=1}^{N}\sum_{k=1}^{K}a_{i,j}\frac{x_{i,k}x_{ j,k}^{*}}{\sqrt{d_{i}^{\text{in}}\,d_{j}^{\text{out}}}}\] \[=\sum_{i=1}^{N}\sum_{k=1}^{K}{|x_{i,k}|}^{2}-\frac{1}{2}\sum_{i,j =1}^{N}\sum_{k=1}^{K}a_{i,j}\frac{(\mathbf{x}^{\mathsf{H}})_{k,i}x_{j,k}}{\sqrt {d_{i}^{\text{in}}\,d_{j}^{\text{out}}}}-\frac{1}{2}\sum_{i,j=1}^{N}\sum_{k=1}^ {K}a_{i,j}\frac{(\mathbf{x}^{\mathsf{H}})_{k,i}x_{j,k}}{\sqrt{d_{i}^{\text{in}} \,d_{j}^{\text{out}}}}\] \[=\Re\left(\sum_{i=1}^{N}\sum_{k=1}^{K}{|x_{i,k}|}^{2}-\sum_{i,j=1 }^{N}\sum_{k=1}^{K}a_{i,j}\frac{x_{i,k}^{*}x_{j,k}}{\sqrt{d_{i}^{\text{in}}\,d_{j }^{\text{out}}}}\right)\] \[=\Re\left(\operatorname{trace}\left(\mathbf{x}^{\mathsf{H}}\left( \mathbf{I}-\mathbf{L}\right)\mathbf{x}\right)\right)\,.\]

The last claim can be proved as follows. For simplicity, suppose \(\mathbf{x}\in\mathbb{R}^{N}\). The " \(\Longleftarrow\) " is clear since one can choose \(\mathbf{x}\) to be \(\mathbf{k}\). To prove the " \(\Longrightarrow\) ", we reason by contradiction. Suppose there exists a \(\mathbf{x}\neq 0\) such that \(\mathscr{E}(\mathbf{x})=0\) and the underlying graph is not weakly connected, i.e.,

\[\forall\tilde{\mathbf{x}}\neq\mathbf{0}\,,\ \left|\sum_{j=1}^{N}a_{i,j}\left(\frac{ \tilde{x}_{j}}{\sqrt{d_{j}^{\text{out}}}}-\frac{\tilde{x}_{i}}{\sqrt{d_{i}^{\text{ in}}}}\right)\right|>0\,,\ \forall i\in\{1,\dots,N\}\,\]Then, since \(\mathbf{x}\neq 0\),

\[0=\mathscr{E}(\mathbf{x}) =\frac{1}{4}\sum_{i,j=1}^{N}a_{i,j}\left|\frac{x_{i}}{\sqrt{d_{i}^{ \text{in}}}}-\frac{x_{j}}{\sqrt{d_{j}^{\text{out}}}}\right|^{2}\] \[\geq\frac{1}{4}\sum_{i=1}^{N}\frac{1}{d_{i}^{\text{in}}}\left( \sum_{j=1}^{N}a_{i,j}\left|\frac{x_{i}}{\sqrt{d_{i}^{\text{in}}}}-\frac{x_{j}} {\sqrt{d_{j}^{\text{out}}}}\right|^{2}\right)\left(\sum_{j=1}^{N}a_{i,j}\right)\] \[\geq\frac{1}{4}\sum_{i=1}^{N}\frac{1}{d_{i}^{\text{in}}}\left( \sum_{j=1}^{N}a_{i,j}\left|\frac{x_{i}}{\sqrt{d_{i}^{\text{in}}}}-\frac{x_{j}} {\sqrt{d_{j}^{\text{out}}}}\right|^{2}\right)\] \[>0\,,\]

where we used Cauchy-Schwartz and triangle inequalities. 

We give the following simple corollary.

**Corollary B.1**.: _For every \(\mathbf{x}\in\mathbb{R}^{N\times K}\), it holds \(\mathscr{E}(\mathbf{x})=\frac{1}{2}\Re\left(\operatorname{vec}(\mathbf{x})^{ \mathsf{H}}(\mathbf{I}\otimes(\mathbf{I}-\mathbf{L}))\operatorname{vec}( \mathbf{x})\right)\)._

## Appendix C Appendix for Section 4

In this section, we provide some properties about \(\operatorname{FGL}\)s. The first statement shows that the \(\operatorname{FGL}\) of a normal \(\operatorname{SNA}\)\(\mathbf{L}\) only changes the magnitude of the eigenvalues of \(\mathbf{L}\).

**Lemma C.1**.: _Let \(\mathbf{M}\) be a normal matrix with eigenvalues \(\lambda_{1},\ldots,\lambda_{N}\) and corresponding eigenvectors \(\mathbf{v}_{1},\ldots,\mathbf{v}_{N}\). Suppose \(\mathbf{M}=\mathbf{L}\mathbf{\Sigma}\mathbf{R}^{\mathsf{H}}\) is its singular value decomposition. Then it holds_

\[\mathbf{\Sigma}=\left|\mathbf{\Lambda}\right|\,,\;\mathbf{L}=\mathbf{V}\,,\; \mathbf{R}=\mathbf{V}\exp\left(i\,\mathbf{\Theta}\right)\,,\;\mathbf{\Theta}= \operatorname{diag}\left(\left\{\theta_{i}\right\}_{i=1}^{N}\right)\,,\; \theta_{i}=\operatorname{atan2}\left(\Re\lambda_{i},\Im\lambda_{i}\right)\,.\]

Proof.: By hypothesis, there exist a unitary matrix \(\mathbf{V}\) such that \(\mathbf{M}=\mathbf{V}\mathbf{\Lambda}\mathbf{V}^{\mathsf{H}}\), then

\[\mathbf{M}^{\mathsf{H}}\mathbf{M} =\mathbf{V}\mathbf{\Lambda}^{*}\mathbf{V}^{\mathsf{H}}\mathbf{V} \mathbf{\Lambda}\mathbf{V}^{\mathsf{H}}=\mathbf{V}\left|\mathbf{\Lambda} \right|^{2}\mathbf{V}^{\mathsf{H}}\,,\] \[\mathbf{M}^{\mathsf{H}}\mathbf{M} =\mathbf{R}\mathbf{\Sigma}\mathbf{L}^{\mathsf{H}}\mathbf{L} \mathbf{\Sigma}\mathbf{R}^{\mathsf{H}}=\mathbf{L}\mathbf{\Sigma}^{2}\mathbf{ L}^{\mathsf{H}}\,.\]

Therefore, \(\mathbf{\Sigma}=\left|\mathbf{\Lambda}\right|\) and \(\mathbf{L}=\mathbf{V}\)

\[\mathbf{M}=\mathbf{R}\left|\mathbf{\Lambda}\right|\mathbf{V}^{\mathsf{H}}\]

Finally, we note that it must hold \(\mathbf{R}=\mathbf{V}\exp(i\,\mathbf{\Theta})\) where \(\Theta=\operatorname{diag}\left(\left\{\operatorname{atan2}(\Re\lambda_{i}, \Im\lambda_{i})\right\}_{i=1}^{N}\right)\) and \(\operatorname{atan2}\) is the 2-argument arctangent. 

We proceed by proving Theorem 4.1, which follows the proof of a similar result given in (Benzi et al., 2020) for the fractional Laplacian defined in the spectral domain of an in-degree normalized graph Laplacian. However, our result also holds for directed graphs and in particular for fractional Laplacians that are defined via the SVD of a graph \(\operatorname{SNA}\).

**Lemma C.2**.: _Let \(\mathbf{M}\in\mathbb{R}^{n\times n}\) with singular values \(\sigma(\mathbf{M})\subset[a,b]\). For \(f:[a,b]\to\mathbb{R}\), define \(f(\mathbf{M})=\mathbf{U}f(\mathbf{\Sigma})\mathbf{V}^{\mathsf{H}}\), where \(\mathbf{M}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\mathsf{H}}\) is the singular value decomposition of \(\mathbf{M}\). If \(f\) has modulus of continuity \(\omega\) and \(d(i,j)\geq 2\), it holds_

\[\left|f(\mathbf{M})\right|_{i,j}\leq\left(1+\frac{\pi^{2}}{2}\right)\omega \left(\frac{b-a}{2}\left|d(i,j)-1\right|^{-1}\right)\,.\]Proof.: Let \(g:[a,b]\to\mathbb{R}\) be any function, then

\[\left\|f(\mathbf{M})-g(\mathbf{M})\right\|_{2} =\left\|\mathbf{U}f(\mathbf{\Sigma})\mathbf{V}^{\text{H}}-\mathbf{U }g(\mathbf{\Sigma})\mathbf{V}^{\text{H}}\right\|_{2}\] \[=\left\|f(\mathbf{\Sigma})-g(\mathbf{\Sigma})\right\|_{2}\] \[=\left\|f(\lambda)-g(\lambda)\right\|_{\infty,\sigma(M)}.\]

The second equation holds since the \(2\)-norm is invariant under unitary transformations. By Jackson's Theorem, there exists for every \(m\geq 1\) a polynomial \(p_{m}\) of order \(m\) such that

\[\left\|f(\mathbf{M})-p_{m}(\mathbf{M})\right\|_{2}\leq\left\|f-p_{m}\right\|_{ \infty,[a,b]}\leq\,\left(1+\frac{\pi^{2}}{2}\right)\omega\left(\frac{b-a}{2m} \right)\,.\]

Fix \(i,j\in\{1,\ldots,n\}\). If \(d(i,j)=m+1\), then any power of \(\mathbf{M}\) up to order \(m\) has a zero entry in \((i,j)\), i.e., \((\mathbf{M}^{m})_{i,j}=0\). Hence, \(f(\mathbf{M})_{i,j}=f(\mathbf{M})_{i,j}-p_{m}(\mathbf{M})_{i,j}\), and we get

\[\left|f(\mathbf{M})_{i,j}\right|\leq\left\|f(\mathbf{M})-g(\mathbf{M})\right\| _{2}\leq\omega\left(1+\frac{\pi^{2}}{2}\right)\left(\frac{b-a}{2m}\right)= \left(1+\frac{\pi^{2}}{2}\right)\omega\left(\frac{b-a}{2}|d(i,j)-1|^{-1}\right)\]

from which the thesis follows. 

Finally, we give a proof of Theorem 4.1, which is a consequence of the previous statement.

Proof of Theorem 4.1.: The eigenvalues of \(\mathbf{L}\) are in the unit circle, i.e., \(\left\|\mathbf{L}\right\|\leq 1\). Hence, \(\left\|\mathbf{L}\mathbf{L}^{\text{H}}\right\|\leq 1\), and the singular values of \(\mathbf{L}\) are in \([0,1]\). By Lemma C.2 and the fact that \(f(x)=x^{\alpha}\) has modulus of continuity \(\omega(t)=t^{\alpha}\) the thesis follows. 

## Appendix D Appendix for Section 5

In this section, we provide the appendix for Section 5. We begin by analyzing the solution of linear matrix ODEs. For this, let \(\mathbf{M}\in\mathbb{C}^{N\times N}\). For \(\mathbf{x}_{0}\in\mathbb{C}^{N}\), consider the initial value problem

\[\mathbf{x}^{\prime}(t)=-\mathbf{M}\mathbf{x}(t),\ \ \mathbf{x}(0)=\mathbf{x}_{0}.\] (5)

**Theorem D.1** (Existence and uniqueness of linear ODE solution).: _The initial value problem given by (5) has a unique solution \(\mathbf{x}(t)\in\mathbb{C}^{N}\) for any initial condition \(\mathbf{x}_{0}\in\mathbb{C}^{N}\)._

The solution of (5) can be expressed using matrix exponentials, even if \(\mathbf{M}\) is not symmetric. The matrix exponential is defined as:

\[\exp(-\mathbf{M}t)=\sum_{k=0}^{\infty}\frac{(-\mathbf{M})^{k}t^{k}}{k!},\]

where \(\mathbf{M}^{k}\) is the \(k\)-th power of the matrix \(\mathbf{M}\). The solution of (5) can then be written as

\[\mathbf{x}(t)=\exp(-\mathbf{M}t)\mathbf{x}_{0}.\] (6)

### Appendix for Section 5.1

In this section, we analyze the solution to (2) and (3). We further provide a proof for Theorem 5.3. We begin by considering the solution to the fractional heat equation (2). The analysis for the Schrodinger equation (3) follows analogously.

The fractional heat equation \(\mathbf{x}^{\prime}(t)=-\mathbf{L}^{\alpha}\mathbf{x}\mathbf{W}\) can be vectorized and rewritten via the Kronecker product as

\[\mathrm{vec}(\mathbf{x})^{\prime}(t)=-\mathbf{W}\otimes\mathbf{L}^{\alpha} \mathrm{vec}(\mathbf{x})(t).\] (7)

In the undirected case \(\mathbf{L}\) and \(\mathbf{I}-\mathbf{L}\) are both symmetric, and the eigenvalues satisfy the relation \(\lambda_{i}(\mathbf{I}-\mathbf{L})=1-\lambda_{i}(\mathbf{L})\). The corresponding eigenvectors \(\psi_{i}(\mathbf{L})\) and \(\psi_{i}(\mathbf{I}-\mathbf{L})\) can be chosen to be the same for \(\mathbf{L}\) and \(\mathbf{I}-\mathbf{L}\). In the following, we assume that these eigenvectors are orthonormalized.

If \(\mathbf{L}\) is symmetric, we can decompose it via the spectral theorem into \(\mathbf{L}=\mathbf{U}\mathbf{D}\mathbf{U}^{T}\), where \(\mathbf{U}=[\psi_{1}(\mathbf{L}),\ldots,\psi_{N}(\mathbf{L})]\) is an orthogonal matrix containing the eigenvectors of \(\mathbf{L}\), and \(\mathbf{D}\) is the diagonal matrix of eigenvalues.

Due to Lemma C.1, the fractional Laplacian \(\mathbf{L}^{\alpha}\) can be written as \(\mathbf{L}^{\alpha}=\mathbf{U}\operatorname{f}_{\alpha}(\mathbf{D})\mathbf{U}^{T}\), where \(\operatorname{f}_{\alpha}:\mathbb{R}\to\mathbb{R}\) is the map \(x\mapsto\operatorname{sign}(x)\left|x\right|^{\alpha}\) and is applied element-wise. Clearly, the eigendecomposition of \(\mathbf{L}^{\alpha}\) is given by the eigenvalues \(\{\operatorname{f}_{\alpha}(\lambda_{1}(\mathbf{L})),\dots,\operatorname{f}_ {\alpha}(\lambda_{N}(\mathbf{L}))\}\) and the corresponding eigenvectors \(\{\psi_{1}(\mathbf{L}),\dots,\psi_{N}(\mathbf{L})\}\).

Now, by well-known properties of the Kronecker product, one can write the eigendecomposition of \(\mathbf{W}\otimes\mathbf{L}^{\alpha}\) as

\[\{\lambda_{r}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{l}(\mathbf{L })\right)\}_{r\in\{1,\dots,K\}\,,\,\,l\in\{1,\dots,N\}}\,\,,\,\,\{\psi_{r}( \mathbf{W})\otimes\psi_{l}(\mathbf{L})\}_{r\in\{1,\dots,K\}\,,\,\,l\in\{1, \dots,N\}}\,\,.\]

Note that \(1\in\lambda\left(\mathbf{L}\right)\) and, since \(\operatorname{trace}\left(\mathbf{L}\right)=0\), the SNA has at least one negative eigenvalue. This property is useful since it allows to retrieve of the indices \((r,l)\) corresponding to eigenvalues with minimal real (or imaginary) parts in a simple way.

The initial condition \(\operatorname{vec}(\mathbf{x}_{0})\) can be decomposed as

\[\operatorname{vec}(\mathbf{x}_{0})=\sum_{r=1}^{K}\sum_{l=1}^{N}c_{r,l}\,\psi_ {r}(\mathbf{W})\otimes\psi_{l}(\mathbf{L})\,,\,\,c_{r,l}=\left\langle \operatorname{vec}(\mathbf{x}_{0})\,,\,\,\psi_{r}(\mathbf{W})\otimes\psi_{l} (\mathbf{W})\right\rangle\,.\]

Then, the solution \(\operatorname{vec}(\mathbf{x})(t)\) of (7) can be written as

\[\operatorname{vec}(\mathbf{x})(t)=\sum_{r=1}^{K}\sum_{l=1}^{N}c_{r,l}\, \exp\left(-t\lambda_{r}\left(\mathbf{W}\right)\operatorname{f}_{\alpha}\left( \lambda_{l}\left(\mathbf{L}\right)\right)\right)\,\psi_{r}(\mathbf{W})\otimes \psi_{l}(\mathbf{L}).\] (8)

The following result shows the relationship between the frequencies of \(\mathbf{I}-\mathbf{L}\) and the Dirichlet energy and serves as a basis for the following proofs.

**Lemma D.2**.: _Let \(\mathcal{G}\) be a graph with SNA \(\mathbf{L}\). Consider \(\mathbf{x}(t)\in\mathbb{C}^{N\times K}\) such that there exists \(\boldsymbol{\varphi}\in\mathbb{C}^{N\times K}\setminus\{0\}\) with_

\[\frac{\operatorname{vec}(\mathbf{x})(t)}{\|\operatorname{vec}(\mathbf{x})(t) \|_{2}}\xrightarrow{t\to\infty}\operatorname{vec}(\boldsymbol{\varphi})\,,\]

_and \((\mathbf{I}\otimes(\mathbf{I}-\mathbf{L}))\operatorname{vec}(\boldsymbol{ \varphi})=\lambda\operatorname{vec}(\boldsymbol{\varphi})\). Then,_

\[\mathscr{E}\left(\frac{\mathbf{x}(t)}{\|\mathbf{x}(t)\|_{2}}\right) \xrightarrow{t\to\infty}\frac{\Re(\lambda)}{2}\,.\]

Proof.: As \(\operatorname{vec}(\boldsymbol{\varphi})\) is the limit of unit vectors, \(\operatorname{vec}(\boldsymbol{\varphi})\) is a unit vector itself. We calculate its Dirichlet energy,

\[\mathscr{E}(\operatorname{vec}(\boldsymbol{\varphi}))=\frac{1}{2}\Re\left( \operatorname{vec}(\boldsymbol{\varphi})^{\mathsf{H}}(\mathbf{I}\otimes( \mathbf{I}-\mathbf{L}))\operatorname{vec}(\boldsymbol{\varphi})\right)=\frac{1 }{2}\Re\left(\lambda\operatorname{vec}(\boldsymbol{\varphi})^{\mathsf{H}} \operatorname{vec}(\boldsymbol{\varphi})\right)=\frac{1}{2}\Re\left(\lambda \right)\,.\]

Since \(\mathbf{x}\mapsto\mathscr{E}(\mathbf{x})\) is continuous, the thesis follows. 

Another useful result that will be extensively used in proving Theorem 5.3 is presented next.

**Lemma D.3**.: _Suppose \(\mathbf{x}(t)\) can be expressed as_

\[\mathbf{x}(t)=\sum_{k=1}^{K}\sum_{n=1}^{N}c_{k,n}\,\exp\left(-t\,\lambda_{k,n} \right)\mathbf{v}_{k}\otimes\mathbf{w}_{n}\,,\]

_for some choice of \(c_{k,n}\), \(\lambda_{k,n}\), \(\{\mathbf{v}_{k}\}\), \(\{\mathbf{w}_{n}\}\). Let \((a,b)\) be the unique index of \(\lambda_{k,n}\) with minimal real part and corresponding non-null coefficient \(c_{k,n}\), i.e._

\[(a,b)\coloneqq\operatorname*{arg\,min}_{(k,n)\in[K]\times[N]}\left\{\Re\left( \lambda_{k,n}\right):c_{k,n}\neq 0\right\}\,.\]

_Then_

\[\frac{\mathbf{x}(t)}{\|\mathbf{x}(t)\|_{2}}\xrightarrow{t\to\infty}\frac{c_{ a,b}\,\mathbf{v}_{a}\otimes\mathbf{w}_{b}}{\|c_{a,b}\,\mathbf{v}_{a}\otimes \mathbf{w}_{b}\|_{2}}\,.\]Proof.: The key insight is to separate the addend with index \((a,b)\). It holds

\[\mathbf{x}(t) =\sum_{k=1}^{K}\sum_{n=1}^{N}c_{k,n}\,\exp\left(-t\,\lambda_{k,n} \right)\mathbf{v}_{n}\otimes\mathbf{w}_{m}\] \[=\exp\left(-t\,\lambda_{a,b}\right)\left(c_{a,b}\mathbf{v}_{a} \otimes\mathbf{w}_{b}+\sum_{\begin{subarray}{c}(k,n)\in[K]\times[N]\\ (k,n)\neq(a,b)\end{subarray}}c_{k,n}\,\exp\left(-t\,\left(\lambda_{k,n}- \lambda_{a,b}\right)\right)\mathbf{v}_{k}\otimes\mathbf{w}_{n}\right)\,.\]

We note that

\[\lim_{t\to\infty}|\exp\left(-t\,\left(\lambda_{k,n}-\lambda_{a,b} \right)\right)| =\lim_{t\to\infty}|\exp\left(-t\,\Re\left(\lambda_{k,n}-\lambda_{ a,b}\right)\right)\exp\left(-i\,t\,\Im\left(\lambda_{k,n}-\lambda_{a,b}\right)\right)|\] \[=\lim_{t\to\infty}\exp\left(-t\,\Re\left(\lambda_{k,n}-\lambda_{ a,b}\right)\right)\] \[=0\,,\]

for all \((k,n)\neq(a,b)\), since \(\Re\left(\lambda_{k,n}-\lambda_{a,b}\right)>0\). Therefore, one gets

\[\frac{\mathbf{x}(t)}{\|\mathbf{x}(t)\|_{2}}\xrightarrow{t\to\infty}\frac{c_{ a,b}\,\mathbf{v}_{a}\otimes\mathbf{w}_{b}}{\|c_{a,b}\,\mathbf{v}_{a}\otimes \mathbf{w}_{b}\|_{2}}\,,\]

where the normalization removes the dependency on \(\exp\left(-t\,\lambda_{a,b}\right)\) 

When \(\lambda_{a,b}\) is not unique, it is still possible to derive a convergence result. In this case, \(\mathbf{x}\) will converge to an element in the span generated by vectors corresponding to \(\lambda_{a,b}\), i.e.,

\[\frac{\mathbf{x}(t)}{\|\mathbf{x}(t)\|_{2}}\xrightarrow{t\to\infty}\frac{ \sum\limits_{(a,b)\in\mathcal{A}}c_{a,b}\,\mathbf{v}_{a}\otimes\mathbf{w}_{b}} {\|\sum\limits_{(a,b)\in\mathcal{A}}c_{a,b}\,\mathbf{v}_{a}\otimes\mathbf{w}_{ b}\|_{2}}\,,\]

where \(\mathcal{A}\coloneqq\{(k,n):\Re(\lambda_{k,n})=\Re(\lambda_{a,b})\,,\;c_{k,n }\neq 0\}\).

A similar result to Lemma D.3 holds for a slightly different representation of \(\mathbf{x}(t)\).

**Lemma D.4**.: _Suppose \(\mathbf{x}(t)\) can be expressed as_

\[\mathbf{x}(t)=\sum_{k=1}^{K}\sum_{n=1}^{N}c_{k,n}\,\exp\left(i\,t\,\lambda_{k, n}\right)\mathbf{v}_{k}\otimes\mathbf{w}_{n}\,,\]

_for some choice of \(c_{k,n}\), \(\lambda_{k,n}\), \(\{\mathbf{v}_{k}\}\), \(\{\mathbf{w}_{n}\}\). Let \((a,b)\) be the unique index of \(\lambda_{k,n}\) with minimal imaginary part and corresponding non-null coefficient \(c_{k,n}\), i.e._

\[(a,b)\coloneqq\operatorname*{arg\,min}_{(k,n)\in[K]\times[N]}\left\{\Im\left( \lambda_{k,n}\right):c_{k,n}\neq 0\right\}\,.\]

_Then_

\[\frac{\mathbf{x}(t)}{\|\mathbf{x}(t)\|_{2}}\xrightarrow{t\to\infty}\frac{c_{ a,b}\,\mathbf{v}_{a}\otimes\mathbf{w}_{b}}{\|c_{a,b}\,\mathbf{v}_{a}\otimes \mathbf{w}_{b}\|_{2}}\,.\]

Proof.: The proof follows the same reasoning as in the proof of Lemma D.3. The difference is that the dominating frequency is the one with the minimal imaginary part, since

\[\Re\left(i\,\lambda_{k,n}\right)=-\Im\left(\lambda_{k,n}\right)\,,\]

and, consequently,

\[\operatorname*{arg\,max}_{(k,n)\in[K]\times[N]}\left\{\Re\left(i\,\lambda_{k, n}\right)\right\}=\operatorname*{arg\,min}_{(k,n)\in\in[K]\times[N]}\left\{\Im \left(\lambda_{k,n}\right)\right\}\,.\]

#### d.1.1 Proof of Theorem 5.3

We denote the eigenvalues of \(\mathbf{L}\) closest to \(0\) from above and below as

\[\lambda_{+}(\mathbf{L}) \coloneqq\operatorname*{arg\,min}_{l}\left\{\lambda_{l}(\mathbf{L} )\ :\ \lambda_{l}(\mathbf{L})>0\right\}\,,\] (9) \[\lambda_{-}(\mathbf{L}) \coloneqq\operatorname*{arg\,max}_{l}\left\{\lambda_{l}(\mathbf{ L})\ :\ \lambda_{l}(\mathbf{L})<0\right\}\,.\]

We assume that the channel mixing \(\mathbf{W}\in\mathbb{R}^{K\times K}\) and the graph Laplacians \(\mathbf{L},\mathbf{I}-\mathbf{L}\in\mathbb{R}^{N\times N}\) are real matrices. Finally, we suppose the eigenvalues of a generic matrix \(\mathbf{M}\) are sorted in ascending order, i.e., \(\lambda_{i}(\mathbf{M})\leq\lambda_{j}(\mathbf{M}),i<j\).

We now reformulate Theorem 5.3 for the fractional heat equation (2) and provide its full proof, which follows a similar frequency analysis to the one in (Di Giovanni et al., 2023, Theorem B.3)

**Theorem D.5**.: _Let \(\mathcal{G}\) be an undirected graph with SNA \(\mathbf{L}\). Consider the initial value problem in (2) with channel mixing matrix \(\mathbf{W}\in\mathbb{R}^{K\times K}\) and \(\alpha\in\mathbb{R}\). Then, for almost all initial conditions \(\mathbf{x}_{0}\in\mathbb{R}^{N\times K}\) the following is satisfied._

* _The solution to (_2_) is HFD if_ \[\lambda_{K}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{1}(\mathbf{L} )\right)<\lambda_{1}(\mathbf{W})\,,\] _and_ \(\mathit{LFD}\) _otherwise._
* _The solution to (_2_) is_ \((1-\lambda_{-}(\mathbf{L}))\)_-FD if_ \[\lambda_{K}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{-}(\mathbf{L} )\right)<\lambda_{1}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{+}( \mathbf{L})\right)\,,\] _and_ \((1-\lambda_{+}(\mathbf{L}))\)_-FD otherwise._

Proof of \((\alpha>0)\).: As derived in (8), the solution of (2) with initial condition \(\mathbf{x}_{0}\) can be written in a vectorized form as

\[\operatorname{vec}(\mathbf{x})(t) =\exp\left(-t\,\mathbf{W}^{\mathsf{T}}\otimes\mathbf{L}^{\alpha} \right)\operatorname{vec}(\mathbf{x}_{0})\] \[=\sum_{r=1}^{K}\sum_{l=1}^{N}c_{r,l}\,\exp\left(-t\,\lambda_{r}( \mathbf{W})\,\operatorname{f}_{\alpha}\left(\lambda_{l}(\mathbf{L})\right) \right)\,\psi_{r}(\mathbf{W})\otimes\psi_{l}(\mathbf{L}),\]

where \(\lambda_{r}(\mathbf{W})\) are the eigenvalues of \(\mathbf{W}\) with corresponding eigenvectors \(\psi_{r}(\mathbf{W})\), and \(\lambda_{l}(\mathbf{L})\) are the eigenvalues of \(\mathbf{L}\) with corresponding eigenvectors \(\psi_{l}(\mathbf{L})\). The coefficients \(c_{r,l}\) are the Fourier coefficients of \(\mathbf{x}_{0}\), i.e.,

\[c_{r,l}\coloneqq\left\langle\operatorname{vec}(\mathbf{x}_{0})\,,\ \psi_{r}(\mathbf{W})\otimes\psi_{l}(\mathbf{L})\right\rangle\,.\]

The key insight is to separate the eigenprojection corresponding to the most negative frequency. By Lemma D.3, this frequency component dominates for \(t\) going to infinity.

Suppose

\[\lambda_{K}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{1}(\mathbf{L}) \right)<\lambda_{1}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{N}( \mathbf{L})\right)=\lambda_{1}(\mathbf{W})\,.\]

In this case, \(\lambda_{K}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{1}(\mathbf{L})\right)\) is the most negative frequency. Assume for simplicity that \(\lambda_{K}(\mathbf{W})\) has multiplicity one; the argument can be applied even if this is not the case, since the corresponding eigenvectors are orthogonal for higher multiplicities.

For almost all initial conditions \(\mathbf{x}_{0}\), the coefficient \(c_{K,1}\) is not null; hence

\[\frac{\operatorname{vec}(\mathbf{x})(t)}{\|\operatorname{vec}(\mathbf{x})(t) \|_{2}}\ \frac{t\to\infty}{\|c_{K,1}\,\psi_{K}\left(\mathbf{W}\right) \otimes\psi_{1}\left(\mathbf{L}\right)}{\|c_{K,1}\,\psi_{K}\left(\mathbf{W} \right)\otimes\psi_{1}\left(\mathbf{L}\right)\|_{2}}\,.\]

By standard properties of the Kronecker product, we have

\[(\mathbf{I}\otimes\mathbf{L})\left(\psi_{K}\left(\mathbf{W}\right)\otimes \psi_{1}\left(\mathbf{L}\right)\right)=(\mathbf{I}\psi_{K}\left(\mathbf{W} \right))\otimes(\mathbf{L}\,\psi_{1}\left(\mathbf{L}\right))=\lambda_{1}( \mathbf{L})\,\psi_{K}\left(\mathbf{W}\right)\otimes\psi_{1}\left(\mathbf{L} \right)\,,\] (10)

i.e., \(\psi_{K}\left(\mathbf{W}\right)\otimes\psi_{1}\left(\mathbf{L}\right)\) is an eigenvector of \(\mathbf{I}\otimes\mathbf{L}\) corresponding to the eigenvalue \(\lambda_{1}(\mathbf{L})\). Then, by Proposition 3.3, \(\psi_{K}\left(\mathbf{W}\right)\otimes\psi_{1}\left(\mathbf{L}\right)\) is also an eigenvector of \(\mathbf{I}\otimes\mathbf{I}-\mathbf{L}\) corresponding to the eigenvalue \(1-\lambda_{1}(\mathbf{L})=\lambda_{N}(\mathbf{I}-\mathbf{L})\). An application of Lemma D.2 finishes the proof.

Similarly, we can show that if \(\alpha>0\) and \(\lambda_{K}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{1}\left( \mathbf{L}\right)\right)>\lambda_{1}\left(\mathbf{W}\right)\) the lowest frequency component \(\lambda_{1}(\mathbf{I}-\mathbf{L})\) is dominant.

Proof of \((\alpha<0)\).: In this case either \(\operatorname{f}_{\alpha}\left(\lambda_{+}\left(\mathbf{L}\right)\right)\lambda_{1} \left(\mathbf{W}\right)\) or \(\operatorname{f}_{\alpha}\left(\lambda_{-}\left(\mathbf{L}\right)\right) \lambda_{K}\left(\mathbf{W}\right)\) are the most negative frequency components. Hence, if \(\operatorname{f}_{\alpha}\left(\lambda_{-}\left(\mathbf{L}\right)\right) \lambda_{K}\left(\mathbf{W}\right)>\operatorname{f}_{\alpha}\left(\lambda_{+} \left(\mathbf{L}\right)\right)\lambda_{1}\left(\mathbf{W}\right)\) the frequency \(\operatorname{f}_{\alpha}\left(\lambda_{+}\left(\mathbf{L}\right)\right) \lambda_{1}\left(\mathbf{W}\right)\) is dominating and otherwise the frequency \(\operatorname{f}_{\alpha}\left(\lambda_{-}\left(\mathbf{L}\right)\right) \lambda_{K}\left(\mathbf{W}\right)\). We can see this by following the exact same reasoning of _(i)_. 

**Remark D.6**.: _In the proof of \((\alpha<0)\), we are tacitly assuming that \(\mathbf{L}\) has only non-zero eigenvalues. If not, we can truncate the \(\operatorname{\mathit{SVD}}\) and remove all zeros singular values (which correspond to zeros eigenvalues). In doing so, we obtain the best invertible approximation of \(\mathbf{L}\) to which the theorem can be applied._

We now generalize the previous result to all directed graphs with normal \(\operatorname{\textsc{SNA}}\).

**Theorem D.7**.: _Let \(\mathcal{G}\) be a a strongly connected directed graph with normal \(\operatorname{\textsc{SNA}}\)\(\mathbf{L}\) such that \(\lambda_{1}(\mathbf{L})\in\mathbb{R}\). Consider the initial value problem in (2) with channel mixing matrix \(\mathbf{W}\in\mathbb{R}^{K\times K}\) and \(\alpha>0\). Then, for almost all initial values \(\mathbf{x}_{0}\in\mathbb{R}^{N\times K}\) the solution to (2) is HFD if_

\[\lambda_{K}(\mathbf{W})|\lambda_{1}(\mathbf{L})|^{\alpha}<\lambda_{1}( \mathbf{W})|\lambda_{N}(\mathbf{L})|^{\alpha},\]

_and \(\operatorname{\mathit{LFD}}\) otherwise._

Proof.: Any normal matrix is unitary diagonalizable, i.e., there exist eigenvalues \(\lambda_{1},\ldots,\lambda_{N}\) and corresponding eigenvectors \(\mathbf{v}_{1},\ldots,\mathbf{v}_{N}\) such that \(\mathbf{L}=\mathbf{V}\mathbf{\Lambda}\mathbf{V}^{\mathsf{H}}\). Then, by Lemma C.1, the singular value decomposition of \(\mathbf{L}\) is given by \(\mathbf{L}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\mathsf{H}}\), where

\[\mathbf{\Sigma}=\left|\mathbf{\Lambda}\right|\,,\ \mathbf{U}=\mathbf{V} \exp\left(i\mathbf{\Theta}\right)\,,\ \mathbf{\Theta}=\operatorname{diag}\left(\left\{\theta_{i}\right\}_{i=1}^{N} \right)\,,\ \theta_{i}=\operatorname{atan2}\left(\Re\lambda_{i},\Im\lambda_{i}\right)\,.\]

Hence,

\[\mathbf{L}^{\alpha}=\mathbf{U}\mathbf{\Sigma}^{\alpha}\mathbf{V}^{\mathsf{H} }=\mathbf{V}\left|\mathbf{\Lambda}\right|^{\alpha}\exp\left(i\mathbf{\Theta} \right)\mathbf{V}^{\mathsf{H}}.\]

Then, equivalent to the derivation of (8), the solution to the vectorized fractional heat equation

\[\operatorname{vec}(\mathbf{x})^{\prime}(t)=-\mathbf{W}\otimes\mathbf{L}^{ \alpha}\operatorname{vec}(\mathbf{x})(t)\]

is given by

\[\operatorname{vec}(\mathbf{x})(t)=\sum_{r=1}^{K}\sum_{l=1}^{N}c_{r,l}\,\exp \left(-t\lambda_{r}\left(\mathbf{W}\right)\operatorname{f}_{\alpha}\left( \lambda_{l}\left(\mathbf{L}\right)\right)\right)\,\psi_{r}(\mathbf{W})\otimes \psi_{l}(\mathbf{L}).\]

with

\[\operatorname{f}_{\alpha}(\lambda_{l}(\mathbf{L}))=\left|\lambda(\mathbf{L})_{ l}\right|^{\alpha}\exp(i\theta_{l}).\]

Now, equivalent to the proof of Theorem 5.3, we apply Lemma D.3. Therefore, the dominating frequency is given by the eigenvalue of \(\mathbf{W}\otimes\mathbf{L}^{\alpha}\) with the most negative real part. The eigenvalues of \(\mathbf{W}\otimes\mathbf{L}^{\alpha}\) are given by \(\lambda_{r}(\mathbf{W})\operatorname{f}_{\alpha}(\lambda_{l}(\mathbf{L}))\) for \(r=1,\ldots,K\), \(l=1,\ldots,N\). The corresponding real parts are given by

\[\Re(\lambda_{r}(\mathbf{W})\operatorname{f}_{\alpha}(\lambda_{l}(\mathbf{L}) ))=\lambda_{r}(\mathbf{W})\left|\lambda(\mathbf{L})_{i}\right|^{\alpha}\cos( \theta_{i})=\lambda_{r}(\mathbf{W})\left|\lambda(\mathbf{L})_{i}\right|^{ \alpha-1}\Re(\lambda(\mathbf{L})_{i}).\]

By Perron-Frobenius, the eigenvalue of \(\mathbf{L}\) with the largest eigenvalues is given by \(\lambda_{N}(\mathbf{L})\in\mathbb{R}\). Hence, for all \(l=1,\ldots,N\),

\[\left|\lambda(\mathbf{L})_{l}\right|^{\alpha}\cos(\theta_{l})\leq\left|\lambda( \mathbf{L})_{N}\right|^{\alpha}.\]

Similarly, for all \(l=1,\ldots,N\) with \(\Re(\lambda(\mathbf{L})_{l})<0\),

\[-\left|\lambda(\mathbf{L})_{l}\right|^{\alpha}\cos(\theta_{l})\leq-\left| \lambda(\mathbf{L})_{1}\right|^{\alpha}.\]

Thus, the frequency with the most negative real part is either given by \(\lambda_{K}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{1}(\mathbf{L})\right)\) or \(\lambda_{1}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{N}(\mathbf{L})\right)\). The remainder of the proof is analogous to the proof of Theorem D.7.

In the following, we provide the complete statement and proof for the claims made in Theorem 5.3 when the underlying \(\operatorname{\textsc{ODE}}\) is the Schrodinger equation as presented in (3).

**Theorem D.8**.: _Let \(\mathcal{G}\) be a undirected graph with SNA \(\mathbf{L}\). Consider the initial value problem in (3) with channel mixing matrix \(\mathbf{W}\in\mathbb{C}^{K\times K}\) and \(\alpha\in\mathbb{R}\). Suppose that \(\mathbf{W}\) has at least one eigenvalue with non-zero imaginary part and sort the eigenvalues of \(\mathbf{W}\) in ascending order with respect to their imaginary part. Then, for almost initial values \(\mathbf{x}_{0}\in\mathbb{C}^{N\times K}\), the following is satisfied._

1. _Solutions of (_3_) are_ \(\operatorname{\textsc{HFD}}\) _if_ \[\Im\left(\lambda_{K}(\mathbf{W})\right)\operatorname{f}_{\alpha}\left(\lambda _{1}(\mathbf{L})\right)<\Im\left(\lambda_{1}(\mathbf{W})\right)\,,\] _and_ \(\operatorname{\textsc{LFD}}\) _otherwise._
2. _Let_ \(\lambda_{+}(\mathbf{L})\) _and_ \(\lambda_{-}(\mathbf{L})\) _be the smallest positive and biggest negative non-zero eigenvalue of_ \(\mathbf{L}\)_, respectively. Solutions of (_3_) are_ \((1-\lambda_{-}(\mathbf{L}))\)_-FD if_ \[\Im\left(\lambda_{K}(\mathbf{W})\right)\operatorname{f}_{\alpha}\left( \lambda_{-}(\mathbf{L})\right)<\Im\left(\lambda_{1}(\mathbf{W})\right) \operatorname{f}_{\alpha}\left(\lambda_{+}(\mathbf{L})\right)\,.\] _Otherwise, solutions of (_3_) are_ \((1-\lambda_{+}(\mathbf{L}))\)_-FD._

Proof.: The proof follows the same reasoning as the proof for the heat equation in Theorem D.5. The difference is that we now apply Lemma D.4 instead of Lemma D.3.

Therefore, the dominating frequency is either \(\lambda_{K}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{1}(\mathbf{L})\right)\) or \(\lambda_{1}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{N}(\mathbf{L})\right)\) if \(\alpha>0\), and \(\lambda_{K}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{-}(\mathbf{L})\right)\) or \(\lambda_{1}(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{+}(\mathbf{L})\right)\) if \(\alpha<0\). 

### Frequency Dominance for Numerical Approximations of the Heat Equation

For \(n\in\mathbb{N}\) and \(h\in\mathbb{R}\), \(h>0\), the solution of (2) at time \(nh>0\) can be approximated with an explicit Euler scheme

\[\operatorname{vec}(\mathbf{x})(n\,h)=\sum_{k=0}^{n}\binom{n}{k}h^{k}(- \mathbf{W}\otimes\mathbf{L}^{\alpha})^{k}\mathrm{vec}(\mathbf{x}_{0})\,,\]

which can be further simplified via the binomial theorem as

\[\operatorname{vec}(\mathbf{x})(n\,h)=(\mathbf{I}-h\,(\mathbf{W}\otimes \mathbf{L}^{\alpha}))^{n}\operatorname{vec}(\mathbf{x}_{0})\,.\] (11)

Hence, it holds the representation formula

\[\operatorname{vec}(\mathbf{x})(n\,h)=\sum_{r,l}c_{r,l}\,\left(1-h\,\lambda_{r} \,(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{l}(\mathbf{L})\right) \right)^{n}\psi_{r}\,(\mathbf{W})\otimes\psi_{l}\,(\mathbf{L})\.\]

In this case, the dominating frequency maximizes \(|1-h\,\lambda_{r}\,(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{l}( \mathbf{L})\right)|\). When \(h<\left\|\mathbf{W}\right\|^{-1}\), the product \(h\,\lambda_{r}\,(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{l}( \mathbf{L})\right)\) is guaranteed to be in \([-1,1]\), and

\[|1-h\,\lambda_{r}\,(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{l}( \mathbf{L})\right)|=1-h\,\lambda_{r}\,(\mathbf{W})\operatorname{f}_{\alpha} \left(\lambda_{l}(\mathbf{L})\right)\in[0,2]\,.\]

Therefore, the dominating frequency minimizes \(h\,\lambda_{r}\,(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{l}( \mathbf{L})\right)\). This is the reasoning behind the next result.

**Proposition D.9**.: _Let \(h\in\mathbb{R}\), \(h>0\). Consider the fractional heat equation (2) with \(\alpha\in\mathbb{R}\). Let \(\left\{\mathbf{x}(n\,h)\right\}_{n\in\mathbb{N}}\) be the trajectory of vectors derived by approximating (2) with an explicit Euler scheme with step size \(h\). Suppose \(h<\left\|\mathbf{W}\right\|^{-1}\). Then, for almost all initial values \(\mathbf{x}_{0}\)_

\[\mathscr{E}\left(\frac{\mathbf{x}(n\,h)}{\left\|\mathbf{x}(n\,h)\right\|_{2}} \right)\xrightarrow{n\to\infty}\,\begin{cases}\frac{\lambda_{N}\,( \mathbf{I}-\mathbf{L})}{2}\,,&\text{ if }\lambda_{K}(\mathbf{W}) \operatorname{f}_{\alpha}\left(\lambda_{1}(\mathbf{L})\right)<\lambda_{1}( \mathbf{W})\,,\\ 0\,,&\text{ otherwise }.\end{cases}\]

Proof.: Define

\[(\lambda_{a},\lambda_{b})\coloneqq\operatorname*{arg\,max}_{r,l}\left\{|1-h \lambda_{r}\,(\mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{l}(\mathbf{L} )\right)|:r\in\left\{1,\ldots,K\right\},l\in\left\{1,\ldots,N\right\}\right\}\,.\]

By the hypothesis on \(h\), this is equivalent to

\[(\lambda_{a},\lambda_{b})=\operatorname*{arg\,min}_{r,l}\left\{\lambda_{r}\,( \mathbf{W})\operatorname{f}_{\alpha}\left(\lambda_{l}(\mathbf{L})\right):r\in \left\{1,\ldots,K\right\},l\in\left\{1,\ldots,N\right\}\right\}\,.\]

[MISSING_PAGE_EMPTY:33]

for all \(r\), \(l\). Denote by \(\varepsilon\) the gap

\[0<\varepsilon\coloneqq\min_{(r,l)\neq(a,b)}\left\{\mathrm{f}_{\alpha}\left( \lambda_{l}\left(\mathbf{L}\right)\right)\Im\left(\lambda_{r}\left(\mathbf{W} \right)\right)-\mathrm{f}_{\alpha}\left(\lambda_{b}\left(\mathbf{L}\right) \right)\Im\left(\lambda_{a}\left(\mathbf{W}\right)\right)\right\}\,.\]

Noting that

\[\left\{\begin{aligned} &\frac{h}{2}\left(\mathrm{f}_{\alpha}\left( \lambda_{l}\left(\mathbf{L}\right)\right)^{2}\left|\lambda_{r}\left(\mathbf{W} \right)\right|^{2}-\mathrm{f}_{\alpha}\left(\lambda_{b}\left(\mathbf{L}\right) \right)^{2}\left|\lambda_{a}\left(\mathbf{W}\right)\right|^{2}\right)& \leq h\left\|\mathbf{W}\right\|^{2}\left\|\mathbf{L}\right\|^{2 \alpha}=h\left\|\mathbf{W}\right\|^{2}\,,\\ &\frac{h}{2}\left(\mathrm{f}_{\alpha}\left(\lambda_{l}\left( \mathbf{L}\right)\right)^{2}\left|\lambda_{r}\left(\mathbf{W}\right)\right|^{2 }-\mathrm{f}_{\alpha}\left(\lambda_{b}\left(\mathbf{L}\right)\right)^{2}\left| \lambda_{a}\left(\mathbf{W}\right)\right|^{2}\right)&<\varepsilon \end{aligned}\right.\]

one gets that (12) is satisfied for \(h<\varepsilon\left\|\mathbf{W}\right\|^{-2}\). Therefore, for sufficiently small \(h\), the dominating frequencies are the ones with minimal imaginary part, i.e., either \(\mathrm{f}_{\alpha}\left(\lambda_{1}(\mathbf{L})\right)\Im\left(\lambda_{K}( \mathbf{W})\right)\) or \(\mathrm{f}_{\alpha}\left(\lambda_{N}(\mathbf{L})\right)\Im\left(\lambda_{1}( \mathbf{W})\right)\). If \(\mathrm{f}_{\alpha}\left(\lambda_{1}(\mathbf{L})\right)\Im\left(\lambda_{K}( \mathbf{W})\right)<\mathrm{f}_{\alpha}\left(\lambda_{N}(\mathbf{L})\right)\Im \left(\lambda_{1}(\mathbf{W})\right)\), then \(b=1\), and the normalized \(\mathrm{vec}\left(\mathbf{x}\right)\) converges to the eigenvector corresponding to the smallest frequency \(\lambda_{1}(\mathbf{L})\). By (10), this is also the eigenvector of \(\mathbf{I}\otimes\mathbf{I}-\mathbf{L}\) corresponding to the largest frequency \(1-\lambda_{1}(\mathbf{L})=\lambda_{N}(\mathbf{I}-\mathbf{L})\). An application of Lemma D.2 finishes the proof. 

Finally, we present a similar result for negative powers.

**Proposition D.12**.: _Let \(h\in\mathbb{R}\), \(h>0\). Consider the fractional Schrodinger equation (3) with \(\alpha<0\). Let \(\left\{\mathbf{x}(n\,h)\right\}_{n\in\mathbb{N}}\) be the trajectory of vectors derived by approximating the solution of (3) with an explicit Euler scheme with step size \(h\). Suppose that \(h\) is sufficiently small. Sort the eigenvalues of \(\mathbf{W}\) in ascending order with respect to their imaginary part. The approximated solution is \((1-\lambda_{+}(\mathbf{L}))\text{-FD}\) if_

\[\lambda_{1}(\mathbf{W})\,\mathrm{f}_{\alpha}\left(\lambda_{+}(\mathbf{L}) \right)<\lambda_{K}(\mathbf{W})\,\mathrm{f}_{\alpha}\left(\lambda_{-}(\mathbf{ L})\right)\,,\]

_and \((1-\lambda_{-}(\mathbf{L}))\text{-FD}\) otherwise._

Proof.: Similar to Proposition D.11, we can prove the statement by realizing that the dominating frequencies \((\lambda_{a},\lambda_{b})\) in (12) are either given by \((\lambda_{1}(\mathbf{W}),\lambda_{+}(\mathbf{L}))\) or \((\lambda_{K}(\mathbf{W}),\lambda_{-}(\mathbf{L}))\). 

## Appendix E Appendix for Section 5.2

We begin this section by describing the solution of general linear matrix ODEs of the form (6) in terms of the Jordan decomposition of \(\mathbf{M}\). This is required when \(\mathbf{M}\) is not diagonalizable. For instance, the SNA of a directed graph is not in general a symmetric matrix, hence, not guaranteed to be diagonalizable. We then proceed in Appendix E.1 with the proof of Theorem 5.6.

For a given matrix \(\mathbf{M}\in\mathbb{C}^{N\times N}\), the Jordan normal form is given by

\[\mathbf{M}=\mathbf{P}\mathbf{J}\mathbf{P}^{-1},\]

where \(\mathbf{P}\in\mathbb{C}^{N\times N}\) is an invertible matrix whose columns are the generalized eigenvectors of \(\mathbf{M}\), and \(\mathbf{J}\in\mathbb{C}^{N\times N}\) is a block-diagonal matrix with Jordan blocks along its diagonal. Denote with \(\lambda_{1},\dots,\lambda_{m}\) the eigenvalues of \(\mathbf{M}\) and with \(\mathbf{J}_{1},\dots,\mathbf{J}_{m}\) the corresponding Jordan blocks. Let \(k_{l}\) be the algebraic multiplicity of the eigenvalue \(\lambda_{l}\), and denote with \(\left\{\psi_{l}^{i}(\mathbf{M})\right\}_{i\in\{1,\dots,k_{l}\}}\) the generalized eigenvectors of the Jordan block \(\mathbf{J}_{l}\).

We begin by giving the following well-known result, which fully characterizes the frequencies for the solution of a linear matrix ODE.

**Lemma E.1**.: _Let \(\mathbf{M}=\mathbf{P}\mathbf{J}\mathbf{P}^{-1}\in\mathbb{C}^{N\times N}\) be the Jordan normal form of \(\mathbf{M}\). Let \(\mathbf{x}:[0,T]\rightarrow\mathbb{R}^{n}\) be a solution to_

\[\mathbf{x}^{\prime}(t)=\mathbf{M}\mathbf{x}(t)\,,\,\,\mathbf{x}(0)=\mathbf{x}_ {0}.\]

_Then, \(\mathbf{x}\) is given by_

\[\mathbf{x}(t)=\sum_{l=1}^{m}\exp\left(\lambda_{l}(\mathbf{M})t\right)\sum_{i=1 }^{k_{l}}c_{l}^{j}\sum_{j=1}^{i}\frac{t^{i-j}}{(i-j)!}\psi_{l}^{j}(\mathbf{M}),\]

_where_

\[\mathbf{x}_{0}=\sum_{l=1}^{m}\sum_{i=1}^{k_{l}}c_{l}^{i}\mathbf{P}\mathbf{e}_{l }^{i}\,,\]

_and \(\left\{\mathbf{e}_{l}^{i}:i\in\{1,\dots k_{l}\}\,\ l\in\{1,\dots,m\}\right\}\) is the standard basis satisfying \(\mathbf{P}\mathbf{e}_{l}^{i}=\psi_{l}^{i}(\mathbf{M})\)._Proof.: By (Perko, 2001, Section 1.8), the solution can be written as

\[\exp\left(\mathbf{M}\,t\right)\mathbf{x}_{0}=\mathbf{P}\exp\left(\mathbf{J}\,t \right)\mathbf{P}^{-1}\left(\sum_{l=1}^{m}\sum_{i=1}^{k_{l}}c_{l}^{i}\mathbf{ Pe}_{l}^{i}\right)=\mathbf{P}\exp\left(\mathbf{J}\,t\right)\left(\sum_{l=1}^{m} \sum_{i=1}^{k_{l}}c_{l}^{i}\mathbf{e}_{l}^{i}\right)\,,\]

where \(\exp\left(\mathbf{J}\,t\right)=\operatorname{diag}\left(\left\{\exp\left( \mathbf{J}_{l}\,t\right)\right\}_{l=1}^{m}\right)\) and

\[\exp\left(\mathbf{J}_{l}\,t\right)=\exp\left(\lambda_{l}(\mathbf{M})\,t \right)\begin{bmatrix}1&t&\frac{t^{2}}{2!}&\cdots&\frac{t^{k_{l}}}{\left(k_{l} -1\right)!}\\ 1&t&&\vdots\\ &1&\ddots&\frac{t^{2}}{2!}\\ &&\ddots&t\\ &&&1\end{bmatrix}\,.\]

Since \(\exp\left(\mathbf{J}\,t\right)=\bigoplus_{l=1}^{m}\exp\left(\mathbf{J}_{l}\,t\right)\), we can focus on a single Jordan block. Fix \(l\in\{1,\ldots,m\}\), it holds

\[\mathbf{P}\exp\left(\mathbf{J}_{l}\,t\right)\left(\sum_{i=1}^{k_ {l}}c_{l}^{i}\mathbf{e}_{l}^{i}\right)\] \[=\mathbf{P}\exp\left(\lambda_{l}(\mathbf{M})\,t\right)\left(c_{l }^{1}\mathbf{e}_{l}^{1}+c_{l}^{2}\left(t\,\mathbf{e}_{l}^{1}+\mathbf{e}_{l}^{ 2}\right)+c_{l}^{3}\left(\frac{t^{2}}{2!}\mathbf{e}_{l}^{1}+t\,\mathbf{e}_{l}^ {2}+\mathbf{e}_{l}^{3}\right)+\ldots\right)\] \[=\exp\left(\lambda_{l}(\mathbf{M})\,t\right)\left(c_{l}^{1} \psi_{l}^{1}(\mathbf{M})+c_{l}^{2}\left(t\,\psi_{l}^{1}(\mathbf{M})+\psi_{l}^ {2}(\mathbf{M})\right)\right.\] \[+c_{l}^{3}\left(\frac{t^{2}}{2!}\psi_{l}^{1}(\mathbf{M})+t\,\psi _{l}^{2}(\mathbf{M})+\psi_{l}^{3}(\mathbf{M})\right)+\ldots\right)\] \[=\exp\left(\lambda_{l}(\mathbf{M})\,t\right)\sum_{i=1}^{k_{l}}c_ {l}^{i}\sum_{j=1}^{i}\frac{t^{i-j}}{(i-j)!}\psi_{l}^{j}(\mathbf{M})\,.\]

Bringing the direct sums together, we get

\[\exp\left(\mathbf{M}\,t\right)\mathbf{x}_{0}=\sum_{l=1}^{m}\exp\left(\lambda_{ l}(\mathbf{M})\,t\right)\sum_{i=1}^{k_{l}}c_{l}^{i}\sum_{j=1}^{i}\frac{t^{i-j}}{(i -j)!}\psi_{l}^{j}(\mathbf{M})\,,\]

from which the thesis follows. 

In the following, we derive a formula for the solution of ODEs of the form

\[\mathbf{x}^{\prime}(t)=\mathbf{M}\mathbf{x}(t)\mathbf{W}\,,\;\mathbf{x}(0)= \mathbf{x}_{0}\,,\] (13)

for a diagonal matrix \(\mathbf{W}\in\mathbb{C}^{K\times K}\) and a general square matrix \(\mathbf{M}\in\mathbb{C}^{N\times N}\) with Jordan normal form \(\mathbf{P}\mathbf{J}\mathbf{P}^{-1}\). By vectorizing, we obtain the equivalent linear system

\[\operatorname{vec}(\mathbf{x})^{\prime}(t)=\mathbf{W}\otimes\mathbf{M} \operatorname{vec}(\mathbf{x})(t)\,,\;\operatorname{vec}(\mathbf{x})(0)= \operatorname{vec}(\mathbf{x}_{0})\,.\] (14)

Then, by properties of the Kronecker product, there holds

\[\mathbf{W}\otimes\mathbf{M}=\mathbf{W}\otimes(\mathbf{P}\mathbf{J}\mathbf{P}^ {-1})=(\mathbf{I}\otimes\mathbf{P})(\mathbf{W}\otimes\mathbf{J})(\mathbf{I} \otimes\mathbf{P}^{-1})=(\mathbf{I}\otimes\mathbf{P})(\mathbf{W}\otimes \mathbf{J})(\mathbf{I}\otimes\mathbf{P})^{-1}.\]

Note that \((\mathbf{I}\otimes\mathbf{P})(\mathbf{W}\otimes\mathbf{J})(\mathbf{I}\otimes \mathbf{P})^{-1}\) is not the Jordan normal form of \(\mathbf{D}\otimes\mathbf{M}\). However, we can characterize the Jordan form of \(\mathbf{W}\otimes\mathbf{M}\) as follows.

**Lemma E.2**.: _The Jordan decomposition of \(\mathbf{W}\otimes\mathbf{J}\) is given by \(\mathbf{W}\otimes\mathbf{J}=\tilde{\mathbf{P}}\tilde{\mathbf{J}}\tilde{ \mathbf{P}}^{-1}\) where \(\tilde{\mathbf{J}}\) is a block diagonal matrix with blocks_

\[\tilde{\mathbf{J}}_{j,l}=\begin{bmatrix}w_{j}\lambda_{l}(\mathbf{J})&1\\ &w_{j}\lambda_{l}(\mathbf{J})&1\\ &&\ddots\\ &&&w_{j}\lambda_{l}(\mathbf{J})&1\\ &&&&&w_{j}\lambda_{l}(\mathbf{J})\end{bmatrix}\,,\]

_and \(\tilde{\mathbf{P}}\) is a diagonal matrix obtained by concatenating \(\tilde{\mathbf{P}}_{j,l}=\operatorname{diag}\left(\left\{w_{j}^{-n+1}\right\}_{ n=1}^{k_{l}}\right)\)._Proof.: As \(\mathbf{J}=\bigoplus_{l=1}^{m}\mathbf{J}_{l}\), we can focus on a single Jordan block. Fix \(l\in\{1,\ldots,m\}\). We have

\[\mathbf{W}\otimes\mathbf{J}_{l}=\operatorname{diag}\left(\left\{w_{j}\, \mathbf{J}_{l}\right\}_{j=1}^{K}\right)=\bigoplus_{j=1}^{K}w_{j}\mathbf{J}_{l}\,,\]

hence, we can focus once again on a single block. Fix \(j\in\{1,\ldots,K\}\); the Jordan decomposition of \(w_{j}\mathbf{J}_{l}\) is given by \(\tilde{\mathbf{P}}_{l}=\operatorname{diag}\left(\left\{w_{j}^{-n}\right\}_{n=1 }^{k_{l}}\right)\) and

\[\tilde{\mathbf{J}}_{l}=\begin{bmatrix}w_{j}\lambda_{l}(\mathbf{J})&1&&&&\\ &w_{j}\lambda_{l}(\mathbf{J})&1&&\\ &&\ddots&\ddots&\\ &&&&w_{j}\lambda_{l}(\mathbf{J})&1\\ &&&&w_{j}\lambda_{l}(\mathbf{J})\end{bmatrix}\,.\]

To verify it, compute the \((n,m)\) element

\[\left(\tilde{\mathbf{P}}_{l}\tilde{\mathbf{J}}_{l}\tilde{\mathbf{P}}_{l}^{-1} \right)_{n,m}=\sum_{i,k}\left(\tilde{\mathbf{P}}_{l}\right)_{n,i}\left(\tilde {\mathbf{J}}_{l}\right)_{i,k}\left(\tilde{\mathbf{P}}_{l}^{-1}\right)_{k,m}\,.\]

Since \(\tilde{\mathbf{P}}_{l}\) is a diagonal matrix, the only non-null entries are on the diagonal; therefore, \(i=n\) and \(k=m\)

\[=\left(\tilde{\mathbf{P}}_{l}\right)_{n,n}\left(\tilde{\mathbf{J}}_{l}\right) _{n,m}\left(\tilde{\mathbf{P}}_{l}^{-1}\right)_{m,m}\]

and the only non-null entries of \(\tilde{\mathbf{J}}_{l}\) are when \(m=n\) or \(m=n+1\), hence

\[=\begin{cases}\left(\tilde{\mathbf{P}}_{l}\right)_{n,n}\left(\tilde{\mathbf{ J}}_{l}\right)_{n,n}\left(\tilde{\mathbf{P}}_{l}^{-1}\right)_{n,n}=w_{j} \lambda_{l}\left(\mathbf{J}\right)\,,&m=n\,,\\ \left(\tilde{\mathbf{P}}_{l}\right)_{n,n}\left(\tilde{\mathbf{J}}_{l}\right)_{ n,n+1}\left(\tilde{\mathbf{P}}_{l}^{-1}\right)_{n+1,n+1}=w_{j}^{-n+1}w_{j}^{n}=w_{j} \,,&m=n+1\,.\end{cases}\]

The thesis follows from assembling the direct sums back. 

Lemma E.2 leads to the following result that fully characterizes the solution of (14) in terms of the generalized eigenvectors and eigenvalues of \(\mathbf{M}\) and \(\mathbf{W}\).

**Proposition E.3**.: _Consider (14) with \(\mathbf{M}=\mathbf{P}\mathbf{J}\mathbf{P}^{-1}\) and \(\mathbf{W}\otimes\mathbf{J}=\tilde{\mathbf{P}}\tilde{\mathbf{J}}\tilde{ \mathbf{P}}^{-1}\), where \(\tilde{\mathbf{J}}\) and \(\tilde{\mathbf{P}}\) are given in Lemma E.2. The solution of (14) is_

\[\operatorname{vec}(\mathbf{x})(t)=\sum_{l_{1}=1}^{K}\sum_{l_{2}=1}^{m}\exp \left(\lambda_{l_{1}}(\mathbf{W})\lambda_{l_{2}}(\mathbf{M})t\right)\sum_{i=1} ^{k_{l_{2}}}c_{l_{1},l_{2}}^{i}\sum_{j=1}^{i}\frac{t^{i-j}}{(i-j)!}\left( \lambda_{l_{1}}(\mathbf{W})\right)^{1-j}\mathbf{e}_{l_{1}}\otimes\psi_{l_{2}}^ {j}(\mathbf{M})\,,\]

_where the coefficients \(c_{l_{1},l_{2}}^{i}\) are given by_

\[\operatorname{vec}(\mathbf{x}_{0})=\sum_{l_{1}=1}^{K}\sum_{l_{2}=1}^{m}\sum_{ i=1}^{k_{l_{2}}}c_{l_{1},l_{2}}^{i}(\mathbf{I}\otimes\mathbf{P})\tilde{ \mathbf{P}}\mathbf{e}_{l_{1}}\otimes\mathbf{e}_{l_{2}}^{i}\]

_where \(\left\{\mathbf{e}_{l_{2}}^{i}\ :\ l_{2}\in\{1,\ldots,m\}\,\ i\in\{1,\ldots,k_{l_{2}}\}\right\}\) is the standard basis satisfying \(\mathbf{P}\mathbf{e}_{l_{2}}^{i}=\psi_{l_{2}}^{i}(\mathbf{M})\)._

Proof.: By Lemma E.2, the eigenvalues of \(\mathbf{W}\otimes\mathbf{M}\) and the corresponding eigenvectors and generalized eigenvectors are

\[\lambda_{l_{1}}(\mathbf{W})\lambda_{l_{2}}(\mathbf{M})\,,\ \mathbf{e}_{l_{1}} \otimes\psi_{l_{2}}^{i}(\mathbf{M})\,,\ (\lambda_{l_{1}}(\mathbf{W}))^{-i+1}\mathbf{e}_{l_{1}} \otimes\psi_{l_{2}}^{i}(\mathbf{M})\]

for \(l_{1}\in\{1,\ldots,K\}\), \(l_{2}\in\{1,\ldots,m\}\) and \(i\in\{2,\ldots,k_{l}\}\). Hence, by Lemma E.1, the solution of (14) is given by

\[\operatorname{vec}(\mathbf{x})(t)=\sum_{l_{1}=1}^{K}\sum_{l_{2}=1}^{m}\exp \left(\lambda_{l_{2}}(\mathbf{M})\lambda_{l_{1}}(\mathbf{W})t\right)\sum_{i=1} ^{k_{l_{2}}}c_{l_{1},l_{2}}^{i}\sum_{j=1}^{i}\frac{t^{i-j}}{(i-j)!}(\lambda_{l_ {1}}(\mathbf{W}))^{1-j}(\mathbf{e}_{l_{1}}\otimes\psi_{l_{2}}^{j}(\mathbf{L}) )\,,\]

where the coefficients \(c_{l_{1},l_{2}}^{i}\) are given by

\[\operatorname{vec}(\mathbf{x}_{0})=\sum_{l_{1}=1}^{K}\sum_{l_{2}=1}^{m}\sum_{i=1 }^{k_{l_{2}}}c_{l_{1},l_{2}}^{i}(\mathbf{I}\otimes\mathbf{P})\tilde{\mathbf{P}}( \mathbf{e}_{l_{1}}\otimes\mathbf{e}_{l_{2}}^{i}(\mathbf{M}))\,.\]

### Proof of Theorem 5.6

In the following, we reformulate and prove Theorem 5.6.

**Corollary E.4**.: _Let \(\mathcal{G}\) be a strongly connected directed graph with \(\text{SNA}\ \mathbf{L}\in\mathbb{R}^{N\times N}\). Consider the initial value problem in (2) with diagonal channel mixing matrix \(\mathbf{W}\in\mathbb{R}^{K\times K}\) and \(\alpha=1\). Then, for almost all initial values \(\mathbf{x}_{0}\in\mathbb{R}^{N\times K}\), the solution to (2) is HFD if_

\[\lambda_{K}(\mathbf{W})\Re\lambda_{1}(\mathbf{L})<\lambda_{1}(\mathbf{W}) \lambda_{N}(\mathbf{L})\]

_and \(\lambda_{1}(\mathbf{L})\) is the unique eigenvalue that minimizes the real part among all eigenvalues of \(\mathbf{L}\). Otherwise, the solution is LFD._

Proof.: Using the notation from Proposition E.3 and its proof, we can write the solution of the vectorized form of (2) as

\[\mathrm{vec}(\mathbf{x})(t)=\sum_{l_{1}=1}^{K}\sum_{l_{2}=1}^{m}\exp\left(- \lambda_{l_{1}}(\mathbf{W})\lambda_{l_{2}}(\mathbf{L})t\right)\sum_{i=1}^{k_{ l_{2}}}c_{l_{1},l_{2}}^{i}\sum_{j=1}^{i}\frac{t^{i-j}}{(i-j)!}(\lambda_{l_{1}}( \mathbf{W}))^{1-j}(\mathbf{e}_{l_{1}}\otimes\psi_{l_{2}}^{j}(\mathbf{L})).\]

As done extensively, we separate the terms corresponding to the frequency with minimal real part. This frequency dominates as the exponential converges faster than polynomials for \(t\) going to infinity. Consider the case \(\lambda_{K}(\mathbf{W})\Re(\lambda_{1}(\mathbf{L}))<\lambda_{1}(\mathbf{W}) \Re(\lambda_{N}(\mathbf{L}))\). As \(\lambda_{1}(\mathbf{L})\) is unique, the product \(\lambda_{K}(\mathbf{W})\Re\left(\lambda_{1}(\mathbf{L})\right)\) is the unique most negative frequency. Assume without loss of generality that \(\lambda_{K}(\mathbf{W})\) has multiplicity one. The argument does not change for higher multiplicities as the corresponding eigenvectors are orthogonal since \(\mathbf{W}\) is diagonal. Then, \(\lambda_{K}(\mathbf{W})\lambda_{1}(\mathbf{L})\) has multiplicity one, and we calculate \(\mathrm{vec}(\mathbf{x})(t)\) as

\[\sum_{l_{1}=1}^{K}\sum_{l_{2}=1}^{m}\exp\left(-\lambda_{l_{1}}( \mathbf{W})\lambda_{l_{2}}(\mathbf{L})t\right)\sum_{i=1}^{k_{l_{2}}}c_{l_{1}, l_{2}}^{i}\sum_{j=1}^{i}\frac{t^{i-j}}{(i-j)!}(\lambda_{l_{1}}(\mathbf{W}))^{1-j}( \mathbf{e}_{l_{1}}\otimes\psi_{l_{2}}^{j}(\mathbf{L}))\] \[=c_{K,1}^{k_{1}}\exp\left(-t\lambda_{K}(\mathbf{W})\lambda_{1}( \mathbf{L})\right)\frac{t^{k_{1}-1}}{(k_{1}-1)!}(\mathbf{e}_{K}\otimes\psi_{1 }^{1}(\mathbf{L}))\] \[+c_{K,1}^{k_{1}}\exp\left(-t\lambda_{K}(\mathbf{W})\lambda_{1}( \mathbf{L})\right)\sum_{j=2}^{k_{1}}\frac{t^{k_{1}-j}}{(k_{1}-j)!}(\lambda_{K} (\mathbf{W}))^{1-j}(\mathbf{e}_{K}\otimes\psi_{1}^{j}(\mathbf{L}))\] \[+\exp\left(-t\lambda_{K}(\mathbf{W})\lambda_{1}(\mathbf{L})\right) t^{k_{1}-1}c_{K,1}^{i}\sum_{j=1}^{i}\frac{t^{i-j}}{(i-j)!}(\lambda_{K}( \mathbf{W}))^{1-j}(\mathbf{e}_{K}\otimes\psi_{1}^{j}(\mathbf{L}))\] \[+\sum_{l_{1}=1}^{K}\sum_{l_{2}=2}^{m}\exp\left(-\lambda_{l_{1}}( \mathbf{W})\lambda_{l_{2}}(\mathbf{L})t\right)\sum_{i=1}^{k_{l_{2}}}c_{l_{1}, l_{2}}^{i}\sum_{j=1}^{i}\frac{t^{i-j}}{(i-j)!}(\lambda_{l_{1}}(\mathbf{W}))^{1-j}( \mathbf{e}_{l_{1}}\otimes\psi_{l_{2}}^{j}(\mathbf{L}))\] \[=\exp\left(-t\lambda_{K}(\mathbf{W})\lambda_{1}(\mathbf{L})\right) t^{k_{1}-1}\Bigg{(}c_{K,1}^{k_{1}}\frac{1}{(k_{1}-1)!}(\mathbf{e}_{K}\otimes\psi_{1 }^{1}(\mathbf{L}))\] \[+c_{K,1}^{k_{1}}\sum_{j=2}^{k_{1}}\frac{t^{1-j}}{(k_{1}-j)!}( \lambda_{K}(\mathbf{W}))^{1-j}(\mathbf{e}_{K}\otimes\psi_{1}^{j}(\mathbf{L}))\] \[+\sum_{i=1}^{k_{1}-1}c_{K,1}^{i}\sum_{j=1}^{i}\frac{1}{(i-j)!}t^{ i-j-k_{1}+1}(\lambda_{K}(\mathbf{W}))^{1-j}(\mathbf{e}_{K}\otimes\psi_{1}^{j}( \mathbf{L}))\] \[+\sum_{l_{1}=1}^{K}\sum_{l_{2}=2}^{m}\exp\left(-t(\lambda_{l_{1}} (\mathbf{W})\lambda_{l_{2}}(\mathbf{L})-\lambda_{K}(\mathbf{W})\lambda_{1}( \mathbf{L}))\right)\sum_{i=1}^{k_{l_{2}}}c_{l_{1},l_{2}}^{i}\] \[\cdot\sum_{j=1}^{i}\frac{t^{i-j-k_{1}+1}}{(i-j)!}(\lambda_{l_{1}}( \mathbf{W}))^{1-j}(\mathbf{e}_{l_{1}}\otimes\psi_{l_{2}}^{j}(\mathbf{L})) \Bigg{)}.\]We can then write the normalized solution as

\[\left(\frac{c_{K,1}^{k_{1}}}{(k_{1}-1)!}(\mathbf{e}_{K}\otimes\psi_ {1}^{1}(\mathbf{L}))+c_{K,1}^{k_{1}}\sum_{j=2}^{k_{1}}\frac{t^{1-j}}{(k_{1}-j)!} (\lambda_{K}(\mathbf{W}))^{1-j}(\mathbf{e}_{K}\otimes\psi_{1}^{j}(\mathbf{L}))\right.\] \[\quad+\sum_{i=1}^{k_{1}-1}c_{K,1}^{i}\sum_{j=1}^{i}\frac{t^{i-j-k_ {1}+1}}{(i-j)!}(\lambda_{K}(\mathbf{W}))^{1-j}(\mathbf{e}_{K}\otimes\psi_{1}^{ j}(\mathbf{L}))\] \[\quad+\sum_{l_{1}=1}^{K}\sum_{l_{2}=2}^{m}e^{-t(\lambda_{K}( \mathbf{W})\lambda_{l_{2}}(\mathbf{L})-\lambda_{K}(\mathbf{W})\lambda_{1}( \mathbf{L}))}\sum_{i=1}^{k_{l_{2}}}c_{l_{1},l_{2}}^{i}\sum_{j=1}^{i}\frac{t^{i -j-k_{1}}}{(i-j)!}(\lambda_{l_{1}}(\mathbf{W}))^{1-j}(\mathbf{e}_{l_{1}}\otimes \psi_{l_{2}}^{j}(\mathbf{L}))\right)\] \[\quad\cdot\left\|\frac{c_{K,1}^{k_{1}}}{(k_{1}-1)!}\left( \mathbf{e}_{K}\otimes\psi_{1}^{1}(\mathbf{L})\right)+c_{K,1}^{k_{1}}\sum_{j=2 }^{k_{1}}\frac{t^{1-j}}{(k_{1}-j)!}(\lambda_{K}(\mathbf{W}))^{1-j}(\mathbf{e}_ {K}\otimes\psi_{1}^{j}(\mathbf{L}))\right.\] \[\quad+\sum_{i=1}^{k_{1}-1}c_{K,1}^{i}\sum_{j=1}^{i}\frac{t^{i-j-k _{1}+1}}{(i-j)!}(\lambda_{l_{1}}(\mathbf{W}))^{1-j}(\mathbf{e}_{K}\otimes\psi _{1}^{j}(\mathbf{L}))\] \[\quad+\sum_{l_{1}=1}^{K}\sum_{l_{2}=2}^{m}\exp\left(-t(\lambda_{l _{1}}(\mathbf{W})\lambda_{l_{2}}(\mathbf{L})-\lambda_{K}(\mathbf{W})\lambda_ {1}(\mathbf{L}))\right)\] \[\quad\cdot\sum_{i=1}^{k_{2}}c_{l_{1},l_{2}}^{i}\sum_{j=1}^{i} \frac{t^{i-j-k_{1}}}{(i-j)!}(\lambda_{l_{1}}(\mathbf{W}))^{1-j}(\mathbf{e}_{l _{1}}\otimes\psi_{l_{2}}^{j}(\mathbf{L}))\right\|_{2}^{-1}.\]

All summands, except the first, converge to zero for \(t\) going to infinity. Hence,

\[\frac{\operatorname{vec}(\mathbf{x})(t)}{\|\operatorname{vec}(\mathbf{x})(t) \|_{2}}\xrightarrow{t\to\infty}\left\|\frac{c_{K,1}^{k_{1}}}{(k_{1}-1)!}( \mathbf{e}_{K}\otimes\psi_{1}^{1}(\mathbf{L}))\right\|_{2}^{-1}\left(\frac{c_{ K,1}^{k_{1}}}{(k_{1}-1)!}(\mathbf{e}_{K}\otimes\psi_{1}^{1}(\mathbf{L})) \right)\right..\]

We apply Lemma D.2 to finish the proof for the HFD case. Note that \(\psi_{1}^{1}(\mathbf{L})\) is an eigenvector corresponding to \(\lambda_{1}(\mathbf{L})\). The LFD case is equivalent. By Perron-Frobenius for irreducible non-negative matrices, there is no other eigenvalue with the same real part as \(1-\lambda_{N}(\mathbf{L})=\lambda_{1}(\mathbf{I}-\mathbf{L})\). 

**Remark E.5**.: _If the hypotheses are met, the convergence result also holds for \(\mathbf{L}^{\alpha}\). With the same reasoning, we can prove that the normalized solution converges to the eigenvector corresponding to the eigenvalue of \(\mathbf{L}^{\alpha}\) with minimal real part. It suffices to consider the eigenvalues and generalized eigenvectors of \(\mathbf{L}^{\alpha}\). However, we do not know the relationship between the singular values of \(\mathbf{L}^{\alpha}\), where we defined the fractional Laplacian, and the eigenvalues of \(\mathbf{L}\). Hence, it is much more challenging to draw conclusions on the Dirichlet energy._

### Explicit Euler

In this subsection, we show that the convergence properties of the Dirichlet energy from Theorem 5.6 are also satisfied when (2) is approximated via an explicit Euler scheme.

As noted in (11), the vectorized solution to (2) can be written as

\[\operatorname{vec}(\mathbf{x})(n\,h)=\left(\mathbf{I}-h\left(\mathbf{W} \otimes\mathbf{L}\right)\right)^{n}\operatorname{vec}(\mathbf{x}_{0})\,,\]

when \(\alpha=1\). We thus aim to analyze the Jordan decomposition of \(\mathbf{L}^{n}\) for \(\mathbf{L}\in\mathbb{C}^{n\times n}\) and \(n\in\mathbb{N}\). Let \(\mathbf{L}=\mathbf{P}\mathbf{J}\mathbf{P}^{-1}\), where \(\mathbf{J}\) is the Jordan form, and \(\mathbf{P}\) is a invertible matrix of generalized eigenvectors.

Consider a Jordan block \(\mathbf{J}_{i}\) associated with the eigenvalue \(\lambda_{i}(\mathbf{M})\). For a positive integer \(n\), the \(n\)-th power of the Jordan block can be computed as:\[\mathbf{J}_{l}^{n}=\lambda_{l}(\mathbf{L})^{n}\begin{bmatrix}1&\binom{n}{1}\lambda_{l }(\mathbf{L})^{-1}&\binom{n}{2}\lambda_{l}(\mathbf{L})^{-2}&\cdots&\binom{n}{k_{ l}-1}\lambda_{l}(\mathbf{L})^{-k_{l}+1}\\ &1&\binom{n}{1}\lambda_{l}(\mathbf{L})^{-1}&&\binom{n}{k_{l}-2}\lambda_{l}( \mathbf{L})^{-k_{l}+2}\\ &&1&&\vdots\\ &&&&&\ddots&\binom{n}{1}\lambda_{l}(\mathbf{L})^{-1}\\ &&&&&&&1\end{bmatrix}\]

We compute the \(n\)-th power of \(\mathbf{L}\) as \(\mathbf{L}^{n}=(\mathbf{P}\mathbf{J}\mathbf{P}^{-1})^{n}=\mathbf{P}\mathbf{J}^ {n}\mathbf{P}^{-1}\), and we expand \(\mathbf{x}_{0}\) as

\[\mathbf{x}_{0}=\sum_{l=1}^{m}\sum_{i=1}^{k_{l}}c_{l}^{i}\mathbf{P}\mathbf{e}_{ l}^{i}\,,\]

where \(\left\{\mathbf{e}_{l}^{i}:i\in\left\{1,\ldots k_{l}\right\},l\in\left\{1, \ldots,m\right\}\right\}\) is the standard basis and \(\mathbf{P}\mathbf{e}_{l}^{i}=\psi_{l}^{i}(\mathbf{L})\) are the generalized eigenvectors of \(\mathbf{L}\). It is easy to see that

\[\mathbf{L}^{n}\mathbf{x}_{0}=\mathbf{P}\mathbf{J}^{n}\mathbf{P}^{-1}\left( \sum_{l=1}^{m}\sum_{i=1}^{k_{l}}c_{l}^{i}\mathbf{P}\mathbf{e}_{l}^{i}\right)= \mathbf{P}\mathbf{J}^{n}\left(\sum_{l=1}^{m}\sum_{i=1}^{k_{l}}c_{l}^{i} \mathbf{e}_{l}^{i}\right)\,.\]

As \(\mathbf{J}^{n}=\bigoplus_{l=1}^{m}\mathbf{J}_{l}^{n}\), we can focus on a single Jordan block. Fix \(l\in\left\{1,\ldots,m\right\}\), and compute

\[\mathbf{P}\mathbf{J}_{l}^{n}\left(\sum_{i=1}^{k_{l}}c_{l}^{i} \mathbf{e}_{l}^{i}\right) =\mathbf{P}\left(\lambda_{l}(\mathbf{M})^{n}c_{l}^{1}\mathbf{e}_ {l}^{1}\right)+\mathbf{P}\left(\binom{n}{1}\lambda_{l}(\mathbf{M})^{n-1}c_{l}^ {1}\mathbf{e}_{l}^{1}+\lambda_{l}(\mathbf{M})^{n}c_{l}^{2}\mathbf{e}_{l}^{2}\right)\] \[+\mathbf{P}\left(\binom{n}{2}\lambda_{l}(\mathbf{M})^{n-2}c_{l}^ {1}\mathbf{e}_{l}^{1}+\binom{n}{1}\lambda_{l}(\mathbf{L})^{n-1}c_{l}^{2} \mathbf{e}_{l}^{2}+\lambda_{l}(\mathbf{M})^{n}c_{l}^{3}\mathbf{e}_{l}^{3}\right)\] \[+\ldots.\]

We can summarize our findings in the following lemma.

**Lemma E.6**.: _For any \(\mathbf{L}=\mathbf{P}\mathbf{J}\mathbf{P}^{-1}\in\mathbb{R}^{N\times N}\) and \(\mathbf{x}_{0}=\sum\limits_{l=1}^{m}\sum\limits_{i=1}^{k_{l}}c_{l}^{i}\psi_{l }^{i}\left(\mathbf{L}\right)\), we have_

\[\mathbf{L}^{n}\mathbf{x}_{0}=\sum_{l=1}^{m}\sum_{i=1}^{\min\{k_{l},n-1\}}\sum \limits_{j=1}^{i}\binom{n}{i-j}\lambda_{l}(\mathbf{L})^{n-i+j}c_{l}^{j}\psi_{l }^{j}(\mathbf{L})\,.\]

We proceed with the main result of this subsection.

**Proposition E.7**.: _Let \(\mathcal{G}\) be a strongly connected directed graph with SNA \(\mathbf{L}\in\mathbb{R}^{N\times N}\). Consider the initial value problem in (2) with diagonal channel mixing matrix \(\mathbf{W}\in\mathbb{R}^{K\times K}\) and \(\alpha=1\). Approximate the solution to (2) with an explicit Euler scheme with a sufficiently small step size \(h\). Then, for almost all initial values \(\mathbf{x}_{0}\in\mathbb{C}^{N\times K}\) the following holds. If \(\lambda_{1}(\mathbf{L})\) is unique and_

\[\lambda_{K}(\mathbf{W})\Re\lambda_{1}(\mathbf{L})<\lambda_{1}(\mathbf{W})\Re \lambda_{N}(\mathbf{L}),\] (15)

_the approximated solution is HFD. Otherwise, the solution is LFD._

Proof.: As noted in (11), the vectorized solution to (2) with \(\alpha=1\), can be written as

\[\mathrm{vec}(\mathbf{x})(n\,h)=\left(\mathbf{I}-h\left(\mathbf{W}\otimes \mathbf{L}\right)\right)^{n}\mathrm{vec}(\mathbf{x}_{0}).\]

Consider the Jordan decomposition of \(\mathbf{L}=\mathbf{P}\mathbf{J}\mathbf{P}^{-1}\) and the Jordan decomposition of \(\mathbf{W}\otimes\mathbf{J}=\tilde{\mathbf{P}}\tilde{\mathbf{J}}\mathbf{P}^{-1}\), where \(\tilde{\mathbf{J}}\) and \(\tilde{\mathbf{P}}\) are specified in Lemma E.2. Then,

\[\mathrm{vec}(\mathbf{x})(n\,h) =\left(\mathbf{I}+h\mathbf{W}\otimes(\mathbf{P}\mathbf{J} \mathbf{P}^{-1})\right)^{n}\mathrm{vec}(\mathbf{x}_{0})\] \[=(\mathbf{I}\otimes\mathbf{P})(\mathbf{I}-h\mathbf{W}\otimes \mathbf{J})^{n}(\mathbf{I}\otimes\mathbf{P})^{-1}\mathrm{vec}(\mathbf{x}_{0})\]\[=(\mathbf{I}\otimes\mathbf{P})(\mathbf{I}-h\tilde{\mathbf{P}}\tilde{ \mathbf{J}}\tilde{\mathbf{P}}^{-1})^{n}(\mathbf{I}\otimes\mathbf{P})^{-1} \mathrm{vec}(\mathbf{x}_{0})\] \[=(\mathbf{I}\otimes\mathbf{P})\tilde{\mathbf{P}}(\mathbf{I}-h \tilde{\mathbf{J}})^{n}\tilde{\mathbf{P}}^{-1}(\mathbf{I}\otimes\mathbf{P})^{- 1}\mathrm{vec}(\mathbf{x}_{0})\] \[=(\mathbf{I}\otimes\mathbf{P})\tilde{\mathbf{P}}(\mathbf{I}-h \tilde{\mathbf{J}})^{n}((\mathbf{I}\otimes\mathbf{P})\tilde{\mathbf{P}})^{-1} \mathrm{vec}(\mathbf{x}_{0}).\]

Now, decompose \(\mathbf{x}_{0}\) into the basis of generalized eigenvectors, i.e.,

\[\mathrm{vec}(\mathbf{x}_{0})=\sum_{l_{1}=1}^{K}\sum_{l_{2}=1}^{m}\sum_{i=1}^{k _{l_{2}}}c_{l}^{i}((\mathbf{I}\otimes\mathbf{P})\tilde{\mathbf{P}})(\mathbf{e }_{l_{1}}\otimes\mathbf{e}_{l_{2}}^{i}(\mathbf{L})).\]

Then, by Lemma E.6, we have

\[\mathrm{vec}(\mathbf{x})(n\,h)=\sum_{l_{1}=1}^{K}\sum_{l_{2}=1}^{ m}\sum_{i=1}^{\min\{k_{l_{2}},t-1\}}\sum_{j=1}^{i}\binom{n}{i-j}\,(1-h\lambda_{l_{ 1}}(\mathbf{W})\lambda_{l_{2}}(\mathbf{L}))^{n-i+j}\,c_{l_{1},l_{2}}^{j}\] \[(\lambda_{l_{1}}(\mathbf{W}))^{1-j}\,\psi_{l_{1}}(\mathbf{W}) \otimes\psi_{l_{2}}^{j}(\mathbf{L}).\]

Now, consider the maximal frequency, i.e.,

\[L_{1},L_{2}=\operatorname*{arg\,max}_{l_{1},l_{2}}\left\{|1-h\lambda_{l_{1}}( \mathbf{W})\lambda_{l_{2}}(\mathbf{L})|\right\}.\]

Then, the solution \(\mathrm{vec}(\mathbf{x})(n\,h)\) can be written as

\[\sum_{l_{1}=1}^{K}\sum_{l_{2}=1}^{m}\sum_{i=1}^{\min\{k_{l_{2}}, n-1\}}\sum_{j=1}^{i}\binom{n}{i-j}\,(1-h\lambda_{l_{1}}(\mathbf{W})\lambda_{l_{ 2}}(\mathbf{L}))^{n-i+j}\,c_{l_{1},l_{2}}^{j}\psi_{l_{1}}(\mathbf{W})\otimes \psi_{l_{2}}^{j}(\mathbf{L})\] \[=(1-h\lambda_{L_{1}}(\mathbf{W})\lambda_{L_{2}}(\mathbf{L}))^{n}\] (16) \[\quad\cdot\sum_{l_{1}=1}^{K}\sum_{l_{2}=1}^{m}\sum_{i=1}^{\min\{ k_{l_{2}},t-1\}}\sum_{j=1}^{i}\binom{n}{i-j}\frac{(1-h\lambda_{l_{1}}(\mathbf{W}) \lambda_{l_{2}}(\mathbf{L}))^{n-i+j}}{(1-h\lambda_{L_{1}}(\mathbf{W})\lambda_ {L_{2}}(\mathbf{L}))^{n}}c_{l_{1},l_{2}}^{j}\psi_{l_{1}}(\mathbf{W})\otimes \psi_{l_{2}}^{j}(\mathbf{L}).\]

With a similar argument as in the proof of Theorem 5.6, we can then see that

\[\frac{\mathrm{vec}(\mathbf{x})(n\,h)}{\|\mathrm{vec}(\mathbf{x})(n\,h)\|_{2}} \xrightarrow[t\to\infty]{t\to\infty},\,\frac{c_{L_{1},L_{2}}^{L}\,\psi_{L_{1} }(\mathbf{W})\otimes\psi_{L_{2}}^{1}(\mathbf{L})}{\|c_{L_{1},L_{2}}^{1}\,\psi_ {L_{1}}(\mathbf{W})\otimes\psi_{L_{2}}^{1}(\mathbf{L})\|_{2}}\,,\]

where \(\psi_{L_{2}}^{1}(\mathbf{L})\) is the eigenvector corresponding to \(\lambda_{L_{2}}(\mathbf{L})\). Note that for almost all \(\mathbf{x}\), we have \(c_{L_{1},L_{2}}^{1}\neq 0\). Then \(\psi_{L_{2}}^{1}(\mathbf{L})\) is also an eigenvector of \(\mathbf{I}-\mathbf{L}\) corresponding to the eigenvalue \(1-\lambda_{L_{2}}(\mathbf{L})\). By Lemma D.2, we have that the approximated solution is \((1-\lambda_{L_{2}}(\mathbf{L}))\)-FD.

We finish the proof by showing that \(L_{2}=1\) if (15) is satisfied, and \(L_{2}=N\) otherwise. First, note that either \(\lambda_{K}(\mathbf{W})\Re\lambda_{1}(\mathbf{L})\) or \(\lambda_{1}(\mathbf{W})\Re\lambda_{N}(\mathbf{L})\) are the most negative real parts among all \(\{\lambda_{l}(\mathbf{W})\Re\lambda_{r}(\mathbf{L})\}_{l\in\{1,\dots,K\},r\in \{1\dots,N\}}\). Assume first that \(\lambda_{K}(\mathbf{W})\Re\lambda_{1}(\mathbf{L})\) has the most negative real part, i.e., (15) holds. Then, define

\[\varepsilon\coloneqq\max_{l,r}|\lambda_{K}(\mathbf{W})\Re\lambda_{1}(\mathbf{L })-\lambda_{l}(\mathbf{W})\Re\lambda_{r}(\mathbf{L})|\,\]

and assume \(h<\varepsilon\,\|\mathbf{W}\|^{2}\). Now it is easy to see that

\[2\lambda_{K}(\mathbf{W})\Re\lambda_{1}(\mathbf{L})-h\lambda_{K}(\mathbf{W})^{2 }|\lambda_{1}(\mathbf{L})|^{2}<2\lambda_{l}(\mathbf{W})\Re\lambda_{r}(\mathbf{L })-h\lambda_{l}(\mathbf{W})^{2}|\lambda_{r}(\mathbf{L})|^{2},\]

which is equivalent to \((K,1)=(L_{1},L_{2})\). Hence, the dynamics are \((1-\lambda_{1}(\mathbf{L}))\)-FD. As \((1-\lambda_{1}(\mathbf{L}))\) is highest frequency of \(\mathbf{I}-\mathbf{L}\), we get HFD dynamics. Similary, we can show that if \(\lambda_{1}(\mathbf{W})\Re\lambda_{N}(\mathbf{L})\) is the most negative frequency, we get LFD dynamics. Note that for the HFD argument, we must assume that \(\lambda_{1}(\mathbf{L})\) is the unique eigenvalue with the smallest real part. For the LFD argument, it is already given that \(\lambda_{N}(\mathbf{L})\) has multiplicity one by Perron-Frobenius Theorem.

### Gcn oversmooths

**Proposition E.8**.: _Let \(\mathcal{G}\) be a strongly connected and aperiodic directed graph with SNA \(\mathbf{L}\in\mathbb{R}^{N\times N}\). A GCN with the update rule_

\[\mathbf{x}_{t+1}=\mathbf{L}\mathbf{x}_{t}\mathbf{W},\]

_where \(\mathbf{x}_{0}\in\mathbb{R}^{N\times K}\) are the input node features, always oversmooths._

Proof.: The proof follows similarly to the proof of Proposition E.7. The difference is that instead of (16), we can write the node features after \(t\) layers as

\[\operatorname{vec}(\mathbf{x}_{t})=\sum_{l_{1}=1}^{K}\sum_{l_{2}=1}^{m}\sum_{ i=1}^{\min\left\{k_{l_{2}},t-1\right\}}\sum_{j=1}^{i}\binom{t}{i-j}\left(\lambda_{l_ {1}}(\mathbf{W})\lambda_{l_{2}}(\mathbf{L})\right)^{t-i+j}c_{l_{1},l_{2}}^{j} \psi_{l_{1}}(\mathbf{W})\otimes\psi_{l_{2}}^{j}(\mathbf{L}).\]

Now note that by Perron-Frobenius the eigenvalue \(\lambda_{N}(\mathbf{L})\) with the largest absolute value is real and has multiplicity one. Then, \(\max_{l_{1},l_{2}}|\lambda_{l_{3}}(\mathbf{W})\lambda_{l_{2}}(\mathbf{L})|\) is attained at either \(\lambda_{1}(\mathbf{W})\lambda_{N}(\mathbf{L})\) or \(\lambda_{K}(\mathbf{W})\lambda_{N}(\mathbf{L})\). Equivalently to the proof of Proposition E.7, we can show that the corresponding GCN is \(1-\lambda_{N}(\mathbf{L})\)-FD. Now \(1-\lambda_{N}(\mathbf{L})=\lambda_{1}(\mathbf{I}-\mathbf{L})\), and \(\lambda_{1}(\mathbf{I}-\mathbf{L})\)-FD corresponds to LFD, hence the GCN oversmooths. 

## Appendix F Appendix for the Cycle Graph Example

Consider the cycle graph with \(N\) nodes numbered from \(0\) to \(N-1\). Since each node has degree \(2\), the SNA \(\mathbf{L}=\mathbf{A}/2\) is a circulant matrix produced by the vector \(\mathbf{v}=\left(\mathbf{e}_{1}+\mathbf{e}_{N-1}\right)/2\). Denote \(\omega=\exp\left(2\pi i/N\right)\), the eigenvectors can be computed as

\[\mathbf{v}_{j}=\frac{1}{\sqrt{N}}\left(1,\omega^{j},\omega^{2j},\ldots,\omega ^{(N-1)j}\right)\]

associated to the eigenvalue \(\lambda_{j}=\cos(2\pi j/N)\). First, we can note that \(\lambda_{j}=\lambda_{N-j}\) for all \(j\in\{1,\ldots,N/2\}\); therefore, the multiplicity of each eigenvalue is \(2\) except \(\lambda_{0}\) and, if \(N\) is even, \(\lambda_{N/2}\). Since the original matrix is symmetric, there exists a basis of real eigenvectors. A simple calculation

\[\mathbf{L}\Re\mathbf{v}_{j}+i\,\mathbb{L}\Im\mathbf{v}_{j}=\mathbf{L} \mathbf{v}_{j}=\lambda_{j}\mathbf{v}_{j}=\lambda_{j}\Re\mathbf{v}_{j}+i \lambda_{j}\Im\mathbf{v}_{j}\]

shows that \(\Re\mathbf{v}_{j}\) and \(\Im\mathbf{v}_{j}\), defined as

\[\Re\mathbf{v}_{j}=\frac{1}{\sqrt{N}}\left(\cos\left(\frac{2\pi j\,n}{N}\right) \right)_{n=0}^{N-1}\,,\;\Im\mathbf{v}_{j}=\frac{1}{\sqrt{N}}\left(\sin\left( \frac{2\pi j\,n}{N}\right)\right)_{n=0}^{N-1}\]

are two eigenvectors of the same eigenvalue \(\lambda_{j}\). To show that they are linearly independent, we compute under which conditions

\[0=a\Re\mathbf{v}_{j}+b\Im\mathbf{v}_{j}\,.\]

We note that the previous condition implies that for all \(n\notin\{0,N/2\}\)

\[0 =a\cos\left(\frac{2\pi j\,n}{N}\right)+b\sin\left(\frac{2\pi j\,n }{N}\right)\] \[=\sqrt{a^{2}+b^{2}}\sin\left(\frac{2\pi j\,n}{N}+\arctan\left( \frac{b}{a}\right)\right)\]

Suppose \(a,b\neq 0\), then it must be

\[\frac{2\pi j\,n}{N}+\arctan\left(\frac{b}{a}\right)=k\pi\,,\;k\in\mathbb{Z}\]

which is equivalent to

\[2j\,n=\left(k-\frac{\arctan\left(\frac{b}{a}\right)}{\pi}\right)N\,,\;k\in \mathbb{Z}\]The left-hand side is always an integer, while the right-hand side is an integer if and only if \(b=0\). This reduces the conditions to

\[\begin{cases}a\cos\left(\frac{2\pi j\,n}{N}\right)=0\\ |a|\sin\left(\frac{2\pi j\,n}{N}\right)=0\end{cases}\]

which is true if and only if \(a=0\). Consider now an even number of nodes \(N\); the eigenspace of \(\lambda_{N/2}=-1\) is

\[\mathbf{v}_{N/2}=\frac{1}{\sqrt{N}}\left((-1)^{n}\right)_{n=0}^{N-1}\]

hence, the maximal eigenvector of \(\mathbf{I}-\mathbf{L}\) guarantees homophily \(0\). Consider now a number of nodes \(N\) divisible by \(4\); the eigenspace of \(\lambda_{N/4}=0\) has basis

\[\Re\mathbf{v}_{N/4}=\frac{1}{\sqrt{N}}\left(\cos\left(\frac{\pi n}{2}\right) \right)_{n=0}^{N-1}\,,\;\Im\mathbf{v}_{N/4}=\frac{1}{\sqrt{N}}\left(\sin\left( \frac{\pi n}{2}\right)\right)_{n=0}^{N-1}\]

Their sum is then equivalent to

\[\Re\mathbf{v}_{N/4}+\Im\mathbf{v}_{N/4} =\frac{1}{\sqrt{N}}\left(\cos\left(\frac{\pi n}{2}\right)+\sin \left(\frac{\pi n}{2}\right)\right)_{n=0}^{N-1}\] \[=\frac{\sqrt{2}}{\sqrt{N}}\left(\sin\left(\frac{\pi n}{2}+\frac{ \pi}{4}\right)\right)_{n=0}^{N-1}\] \[=\sqrt{\frac{2}{N}}\left(\sin\left(\frac{\pi}{4}(2n+1)\right) \right)_{n=0}^{N-1}\] \[=\frac{1}{\sqrt{N}}\left(1,1,-1,-1,\ldots\right)\]

hence, the mid eigenvector of \(\mathbf{L}\) guarantees homophily \(\nicefrac{{1}}{{2}}\). A visual explanation is shown in Figure 4.