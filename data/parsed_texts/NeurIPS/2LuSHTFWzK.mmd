# On the cohesion and separability of average-link for hierarchical agglomerative clustering

Eduardo S. Laber

Departmento de Informatica, PUC-RIO

laber@inf.puc-rio.br &Miguel Batista

Departmento de Informatica, PUC-RIO

miguel260503@gmail.com

###### Abstract

Average-link is widely recognized as one of the most popular and effective methods for building hierarchical agglomerative clustering. The available theoretical analyses show that this method has a much better approximation than other popular heuristics, as single-linkage and complete-linkage, regarding variants of Dasgupta's cost function [STOC 2016]. However, these analyses do not separate average-link from a random hierarchy and they are not appealing for metric spaces since every hierarchical clustering has a \(1/2\) approximation with regard to the variant of Dasgupta's function that is employed for dissimilarity measures [Moseley and Yang 2020]. In this paper, we present a comprehensive study of the performance of average-link in metric spaces, regarding several natural criteria that capture separability and cohesion, and are more interpretable than Dasgupta's cost function and its variants. We also present experimental results with real datasets that, together with our theoretical analyses, suggest that average-link is a better choice than other related methods when both cohesion and separability are important goals.

## 1 Introduction

Clustering is the task of partitioning a set of objects/points so that similar ones are grouped together while dissimilar ones are put in different groups. Clustering methods are widely used for exploratory analysis and for reducing the computational resources required to handle large datasets.

Hierarchical clustering is an important class of clustering methods. Given a set of \(\mathcal{X}\) of \(n\) points, a hierarchical clustering is a sequence of clusterings \((\mathcal{C}^{n},\mathcal{C}^{n-1},\ldots,\mathcal{C}^{1})\), where \(\mathcal{C}^{n}\) is a clustering with \(n\) unitary clusters, each of them corresponding to a point in \(\mathcal{X}\), and the clustering \(\mathcal{C}^{i}\), with \(i<n\), is obtained from \(\mathcal{C}^{i+1}\) by replacing two of its clusters with their union \(A^{i}\). A hierarchical clustering induces a strictly binary tree with \(n\) leaves, where each leaf corresponds to a point in \(\mathcal{X}\) and the \(i\)th internal node, with \(i<n\), is associated with the cluster \(A^{i}\); the points in \(A^{i}\) correspond to the leaves of the subtree rooted in \(A^{i}\). Hierarchical clustering methods are often taught in data science/ML courses, are implemented in many machine learning libraries, such as scipy, and have applications in different fields as evolution studies via phylogenetic trees [Eisen et al., 1998], finance [Tumminello et al., 2010] and detection of closely related entities [Kobren et al., 2017, Monath et al., 2021].

Average-link is widely considered one of the most effective hierarchical clustering algorithms. It belongs to the class of _agglomerative methods_, that is, methods that start with a set of \(n\) clusters, corresponding to the \(n\) input points, and iteratively use a linkage rule to merge two clusters. Due to its relevance, we can find some recent works dedicated to improving average-link' efficiency and scalability [Yu et al., 2021, Dhulipala et al., 2021, 2022, 2023] as well as recent theoretical work that try to understand its success in practice [Cohen-Addad et al., 2019, Charikar et al., 2019a, Moseley and Wang, 2023, Charikar et al., 2019b].

Most of the available theoretical works give approximation bounds for average-link regarding the cost function introduced by (Dasgupta, 2016) as well as for some variants of it. Let \(\mathcal{D}\) be the tree induced by a hierarchical clustering. Dasgupta's cost function and its variation for dissimilarities considered in (Cohen-Addad et al., 2019) are, respectively, given by

\[\texttt{Dasg}(\mathcal{D})=\sum_{a,b\in\mathcal{X}}\texttt{sim}(a,b)\cdot|D(a,b )|\;\;\text{and}\;\;\texttt{CKMM}(\mathcal{D})=\sum_{a,b\in\mathcal{X}} \texttt{diss}(a,b)\cdot|D(a,b)|,\] (1)

where \(\texttt{sim}(a,b)\) (\(\texttt{diss}(a,b)\)) is the similarity (dissimilarity) of points \(a\) and \(b\); \(D(a,b)\) is the subtree of \(\mathcal{D}\) rooted at the least common ancestor of the leaves corresponding to \(a\) and \(b\), and \(|D(a,b)|\) is the number of leaves in \(D(a,b)\). In general, the existing results show that average-link achieves constant approximation for variants of Dasgupta's function while other linkage methods do not.

However, there is significant room for further analysis due to the following reasons. First, Dasgupta's cost function, despite its nice properties, is less interpretable than traditional cost functions that measure compactness and separability. Second, although the analyses based on Dasg and its variants allow to separate average-link from other linkage methods as single-linkage and complete-linkage in terms of approximation, they do not separate average-link from a random hierarchy (Cohen-Addad et al., 2019; Moseley and Wang, 2023; Charikar et al., 2019). Moreover, for the case in which the points lie in a metric space every hierarchical clustering has \(1/2\) approximation for the maximization of CKMM(Wang and Moseley, 2020), so this cost function is less appealing in this relevant setting. Finally, to the best of our knowledge, Dasg does not reveal how good are the clusters generated for a specific range of \(k\). As an example, small \(k\) are important for exploratory analysis while large \(k\) is important for de-duplication tasks (Kobren et al., 2017).

### Our results

Motivated by this scenario, we present a comprehensive study of the performance of average-link in metric spaces, with regards to several natural criteria that capture separability and cohesion of clustering. In a nutshell, these results, as explained below, show that average link has much better global properties than other popular heuristics when these two important goals are taken into account.

Let \((\mathcal{X},\texttt{dist})\) be a metric space, where \(\mathcal{X}\) is a set of \(n\) points. The diameter \(\texttt{diam}(S)\) of a set of points \(S\) is given by \(\texttt{diam}(S)=\max\{\texttt{dist}(x,y)|x,y\in S\}\). For a cluster \(A\) and for two clusters \(A\) and \(B\), let

\[\texttt{avg}(A)=\frac{1}{\binom{|A|}{2}}\sum_{x,y\in A}\texttt{dist}(x,y)\;\; \text{and}\;\;\texttt{avg}(A,B)=\frac{1}{|A|\cdot|B|}\sum_{x\in A}\sum_{y\in B }\texttt{dist}(x,y)\]

Let \(\mathcal{C}=(C_{1},\ldots,C_{k})\) be a \(k\)-clustering for \((\mathcal{X},\texttt{dist})\). To study separability we consider the average (\(\texttt{sep}_{\texttt{av}}\)) and the minimum (\(\texttt{sep}_{\texttt{min}}\)) avg among clusters in \(\mathcal{C}\), that is,

\[\texttt{sep}_{\texttt{av}}(\mathcal{C}):=\frac{1}{\binom{k}{2}}\sum_{i\neq j} \texttt{avg}(C_{i},C_{j})\;\;\text{and}\;\;\texttt{sep}_{\texttt{min}}( \mathcal{C}):=\min_{i\neq j}\{\texttt{avg}(C_{i},C_{j})\},\] (2)

On the other hand, for studying cohesion, we consider the maximum diameter (max-diam) and the maximum average pairwise distance (max-avg) of the clusters in \(\mathcal{C}\). In formulae,

\[\texttt{max-diam}(\mathcal{C}):=\max\{\texttt{diam}(C_{i})|1\leq i\leq k\} \;\;\text{and}\;\;\texttt{max-avg}(\mathcal{C}):=\max\{\texttt{avg}(C_{i})|1 \leq i\leq k\}\] (3)

We also study natural optimization goals that capture both the separability and the cohesion of a clustering. We define the cs-ratio\({}_{\texttt{AV}}\) and cs-ratio\({}_{\texttt{DM}}\) of a clustering \(\mathcal{C}\) as

\[\texttt{cs-ratio}_{\texttt{AV}}(\mathcal{C}):=\frac{\texttt{max-avg}( \mathcal{C})}{\texttt{sep}_{\texttt{min}}(\mathcal{C})}\;\;\text{and}\;\; \texttt{cs-ratio}_{\texttt{DM}}(\mathcal{C}):=\frac{\texttt{max-diam}( \mathcal{C})}{\texttt{sep}_{\texttt{min}}(\mathcal{C})}\] (4)

Let \(\mathcal{A}^{k}\) be a \(k\)-clustering produced by average-link. We first prove through a simple inductive argument that cs-ratio\({}_{\texttt{AV}}(\mathcal{A}^{k})\leq 1\). This result does not assume that the points in \(\mathcal{X}\) lie in a metric space and it is tight in the sense that there are instances in which cs-ratio\({}_{\texttt{AV}}(\mathcal{C})=1\) for every \(k\)-clustering \(\mathcal{C}\). For the related cs-ratio\({}_{\tt DM}\) criterion, we present a more involved analysis which shows that cs-ratio\({}_{\tt DM}(\mathcal{A}^{k})\) as well as the approximation of average-link regarding OPT (the minimum possible cs-ratio\({}_{\tt DM}\)) are \(O(\log n)\); these bounds are nearly tight since there exists an instance for which cs-ratio\({}_{\tt DM}(\mathcal{A}^{k})\) and cs-ratio\({}_{\tt DM}(\mathcal{A}^{k})/\text{OPT}\) are \(\Omega(\frac{\log n}{\log\log n})\). Both cs-ratio\({}_{\tt AV}\) and cs-ratio\({}_{\tt DM}\) allow an exponential separation between average-link and other linkage methods, as single-linkage and complete-linkage. Interestingly, in contrast to CKMM (Eq. 1), our criteria also allow a very clear separation between average-link and the clustering induced by a random hierarchy.

Next, we focus on separability criteria. Let \(\text{OPT}_{\tt SEP}(k)\) be the maximum possible sep\({}_{\tt AV}\) of a \(k\)-clustering for \((\mathcal{X},\text{dist})\). We show that sep\({}_{\tt AV}(\mathcal{A}^{k})\) is at least \(\frac{\text{OPT}_{\tt NP}(k)}{k+2\ln n}\) and that this result is nearly tight. Furthermore, we argue that any hierarchical clustering algorithm that has bounded approximation regarding max-diam or max-avg does not have approximation better than \(1/k\) to sep\({}_{\tt AV}\). Regarding single-linkage and complete-linkage, we present instances that show that their approximation with respect to sep\({}_{\tt AV}\) are exponentially worse than that of average-link, for the relevant case that \(k\) is small.

We also investigate the cohesion of average-link. For a \(k\)-clustering \(\mathcal{C}\), let avg-diam be the average diameter of the \(k\) clusters in \(\mathcal{C}\). Let \(\text{OPT}_{\tt MF}(k)\) and \(\text{OPT}_{\tt MF}(k)\) be, respectively, the minimum possible max-diam and avg-diam of a \(k\)-clustering for \((\mathcal{X},\text{dist})\). We prove that for all \(k\), max-diam\((\mathcal{A}^{k})\leq\min\{k,1+4\ln n\}k^{\log_{2}3}\text{OPT}_{\tt MF }(k)\). This result together with the instance given by Theorem 3.4 of [23] allow to separate average-link from single-linkage, in terms of approximation, when \(k\) is \(\Omega(\log^{2.41}n)\). We also show that max-diam\((\mathcal{A}^{k})\) is \(\Omega(k)\text{OPT}_{\tt DM}(k)\), which is, to the best of our knowledge, the first lower bound on the maximum diameter of average-link.

Finally, to **complement** our study, we present some experiments with 10 real datasets in which we evaluate, to some extent, if our theoretical results line up with what is observed in practice. These experiments conform with our theoretical results since they also suggest that average-link performs better than other related methods when both cohesion and separability are taken into account.

### Related work

There is a vast literature about hierarchical agglomerative clustering methods. Here, we focus on works that provide provable guarantees for average-link and some other well-known linkage methods.

**Average-link**. There are works that present bounds on the approximation of average-link regarding some criteria [2, 2, 23, 24]. All these works but [23] analyse the approximation of average-link regarding variants of Dasgupta's cost function. [23] assumes that the proximity between the points in \(\mathcal{X}\) is given by a similarity matrix. They show that average-link is a \(1/3\)-approximation with respect to the "dual" of Dasgupta's cost function. [2], as in our work, assumes that the proximity between points in \(\mathcal{X}\) is given by a dissimilarity measure and shows that average-link has \(2/3\) approximation for the problem of maximizing CKMM (Eq. 1). [2] show that these approximation ratio for average-link are tight. These papers also show that a random hierarchy obtained by a divisive heuristic that randomly splits the set of points in each cluster matches the \(1/3\) and \(2/3\) bounds.

[23] presents an interesting approach to derive upper bounds on cohesion criteria for a certain class of linkage methods that includes average-link. They show that avg\((A)\leq k^{1.59}\text{OPT}_{\tt MF}(k)\) for every cluster \(A\in\mathcal{A}^{k}\). Our bound on the maximum diameter of a cluster in \(\mathcal{A}^{k}\) incurs an extra factor of \(\min\{k,1+4\ln n\}\) to this bound and its proof combines their approach with some new ideas/analyses.

**Other Linkage Methods**. There are also works that give bounds on the diameter of the clustering built by complete-linkage and single-linkage on metric spaces [23, 24, 25, 26]. Let \(\mathcal{C}\) and \(\mathcal{S}\) be the \(k\)-clustering built by these methods, respectively. [1] shows that max-diam\((\mathcal{C})\) is \(\Omega(k\text{OPT}_{\tt DM}(k))\) while [23]and Laber, 2024] shows that \(\texttt{max-diam}(\mathcal{C})\) is \(O(\min\{k^{1.30}\text{OPT}_{\texttt{DM}}(k),k^{1.59}\text{OPT}_{\texttt{AV}}(k)\})\). Regarding \(\texttt{single-linkage}\), \(\texttt{max-diam}(\mathcal{S})\) is \(\Theta(k\text{OPT}_{\texttt{DM}}(k))\)[Dasgupta and Long, 2005, Arutyunova et al., 2023] and \(\Omega(k^{2}\text{OPT}_{\texttt{AV}}(k))\)[Dasgupta and Laber, 2024]. [Ackermann et al., 2010, Grosswendt and Roglin, 2015] give bounds for the case in which dist is the Euclidean metric.

In terms of separability criteria, it is well known that \(\texttt{single-linkage}\) maximizes the minimum spacing of a clustering [Kleinberg and Tardos, 2006][Chap 4.7]. Recently, [Laber and Murtinho, 2023] observed that it also maximizes the cost of the minimum spanning tree spacing, a stronger criterion. These criteria, in contrast to ours, just take into account the minimum distance between points in different clusters and then they can be significantly impacted by noise.

[Grosswendt et al., 2019] shows that Ward's method gives a 2-approximation for \(k\)-means when the optimal clusters are well-separated.

## 2 Preliminaries

Algorithm 2 shows a pseudo-code for \(\texttt{average-link}\). The function \(\texttt{dist}_{AL}(A,B)\) at line 3 that measures the distance between clusters \(A\) and \(B\) is given by

\[\texttt{dist}_{AL}(A,B):=\frac{1}{|A||B|}\sum_{a\in A}\sum_{b\in B}\texttt{ dist}(a,b).\]

\(\texttt{single-linkage}\) and \(\texttt{complete-linkage}\) are obtained by replacing \(\texttt{dist}_{AL}\), in Algorithm 2, with \(\texttt{dist}_{SL}(A,B):=\min\{\texttt{dist}(a,b)|(a,b)\in A\times B\}\) and \(\texttt{dist}_{CL}(A,B):=\max\{\texttt{dist}(a,b)|(a,b)\in A\times B\},\) respectively.

```
1:\(\mathcal{A}^{n}\leftarrow\) clustering with \(n\) unitary clusters, each one containing a point of \(\mathcal{X}\)
2:For\(i=n-1\) down to 1
3:\((A,B)\leftarrow\) clusters in \(\mathcal{A}^{i+1}\) for which \(\texttt{dist}_{AL}(A,B)\) is minimum
4:\(\mathcal{A}^{i}\leftarrow\mathcal{A}^{i+1}-\{A\}-\{B\}\cup\{A\cup B\}\) ```

**Algorithm 2** Average Link

A version of the triangle inequality for averages will be employed a number of times in our analyses. Its proof can be found in Section A.

**Proposition 2.1** (Triangle Inequality for averages).: _Let \(A\), \(B\) and \(C\) be three clusters. Then,_

\[\texttt{avg}(A,C)\leq\texttt{avg}(A,B)+\texttt{avg}(B,C).\]

For two disjoint clusters \(A\) and \(B\), the following identity holds

\[\binom{(|A|+|B|)}{2}\texttt{avg}(A\cup B)=\binom{|A|}{2}\texttt{avg}(A)+|A||B| \texttt{avg}(A,B)+\binom{|B|}{2}\texttt{avg}(B).\]

Dividing both sides by \(\binom{(|A|+|B|)}{2}\), we conclude that \(\texttt{avg}(A\cup B)\) is a convex combination of \(\texttt{avg}(A),\texttt{avg}(B)\) and \(\texttt{avg}(A,B)\), a fact will be used a couple of times in our analyses.

The following notation will be used throughout the text. We use \(H_{p}=\sum_{i=1}^{p}\frac{1}{i}\) to denote the \(p\)th harmonic number and \(\mathcal{A}^{k}\) to refer to the \(k\)-clustering obtained by \(\texttt{average-link}\) for the instance under consideration, which will always be clear from the context.

## 3 Cohesion and separability

In this section, we analyze the performance of \(\texttt{average-link}\) with respect to both \(\texttt{cs-ratio}_{\texttt{AV}}\) and \(\texttt{cs-ratio}_{\texttt{DM}}\) (Eq. 4), criteria that simultaneously take into account the separability and the cohesion of a clustering. Moreover, we contrast its performance with that achieved by other linkage methods.

### The \(\mathsf{cs-ratio}_{\mathsf{0M}}\) criterion

We first show that \(\mathsf{cs-ratio}_{\mathsf{0M}}(\mathcal{A}^{k})\leq 1\). The proof of this result can be found in Section B.1, it uses induction on the number of iterations of \(\mathsf{average-link}\) together with a fairly simple case analysis.

**Theorem 3.1**.: _Let \(\mathcal{A}^{k}\) be a \(k\)-clustering built by \(\mathsf{average-link}\). Then, for every \(k\), \(\mathsf{cs-ratio}_{\mathsf{0M}}(\mathcal{A}^{k})\leq 1\)._

We note that the above result does not assume the triangle inequality and it is tight in the sense that for the instance \((\mathcal{X},\mathtt{dist})\), in which the \(n\) points of \(\mathcal{X}\) have pairwise distance 1, every clustering has \(\mathsf{cs-ratio}_{\mathsf{0M}}\) equal to \(1\).

In Section B.2, we present instances which show that \(\mathsf{cs-ratio}_{\mathsf{0M}}\) can be \(\Omega(n)\), \(\Omega(\sqrt{n})\) and unbounded in terms of \(n\) for \(\mathsf{single-linkage}\), \(\mathsf{complete-linkage}\) and a random hierarchy, respectively. Interestingly, all the \(k\)-clustering, with \(2<k\leq n/2\), induced by the hierarchical clustering obtained by these methods satisfy these bounds. Furthermore, since \(\mathsf{cs-ratio}_{\mathsf{0M}}(\mathcal{C})\geq\mathsf{cs-ratio}_{\mathsf{ 0M}}(\mathcal{C})\) for every clustering \(\mathcal{C}\), these bounds also hold for the \(\mathsf{cs-ratio}_{\mathsf{0M}}\) criterion.

A natural question that arises is whether \(\mathsf{average-link}\) has a "good" approximation with respect to \(\mathsf{cs-ratio}_{\mathsf{0M}}\). Unfortunately, the answer is no. In fact, in Section B.3 we show an instance where the approximation is unbounded in terms of \(n\). However, as we show in the next section, \(\mathsf{average-link}\) has a logarithmic approximation with respect to \(\mathsf{cs-ratio}_{\mathsf{0M}}\).

### The \(\mathsf{cs-ratio}_{\mathsf{0M}}\) criterion

We analyze the \(\mathsf{cs-ratio}_{\mathsf{0M}}\) of \(\mathsf{average-link}\). The results of this section will have an important role in the analysis of both the separability and cohesion of \(\mathsf{average-link}\) presented further.

First, we show that for every cluster \(X\) in \(\mathcal{A}^{k}\), the average distance of a point \(x\in X\) to the other points in \(X-x\) is at most a logarithmic factor of the average distance between any two clusters \(Y\) and \(Z\). The proof can be found in Section B.5. Let \(T_{i-1}\) be the cluster that contains \(x\) before the \(i\)th merge involving \(x\) and let \(S_{i}\) be the cluster that is merged with \(T_{i-1}\). We prove by induction that \(\mathsf{avg}(x,T_{i}-x)\leq\ln H_{|T_{i}|-1}\mathsf{avg}(Y,Z)\), which implies on the desired result because \(T_{t}=X\) for some \(t\). To establish the induction, we use the triangle inequality to write \(\mathsf{avg}(x,T_{i}-x)\) as a function of both \(\mathsf{avg}(x,T_{i-1}-x)\) and \(\mathsf{avg}(T_{i-1},S_{i})\), and also argue that \(\mathsf{avg}(T_{i-1},S_{i})\leq\mathsf{avg}(X,Y)\).

**Lemma 3.2**.: _Let \(X\), \(Y\) and \(Z\), with \(|X|\geq 2\) and \(Y\neq Z\), be clusters of \(\mathcal{A}^{k}\). Then, for every \(x\in X\), we have that \(\mathsf{avg}(x,X)\leq\mathsf{avg}(x,X-x)\leq H_{|X|-1}\mathsf{avg}(Y,Z)\)._

The next result is a simple consequence of the previous one.

**Theorem 3.3**.: _Let \(k\geq 2\) and let \(X\), \(Y\) and \(Z\), with \(Y\neq Z\), be clusters of a \(k\)-clustering built by \(\mathsf{average-link}\). Then, \(\mathtt{diam}(X)\leq 2H_{|X|-1}\mathsf{avg}(Y,Z)\)._

Proof.: If \(|X|=1\) the result holds because \(\mathtt{diam}(X)=0\). Thus, we assume that \(|X|>1\). Let \(x\) and \(x^{\prime}\) be such that \(\mathtt{dist}(x,x^{\prime})=\mathtt{diam}(X)\). We have that

\[\mathtt{dist}(x,x^{\prime})\leq\mathsf{avg}(x,X)+\mathsf{avg}(X,x^{\prime}) \leq 2H_{|X|-1}\mathsf{avg}(Y,Z)\]

where the first inequality follows from the triangle inequality and the second one due to Lemma 3.2. 

The next theorem shows that \(\mathsf{cs-ratio}_{\mathsf{0M}}(\mathcal{A}^{k})\leq 2H_{n}\) and that \(\mathsf{average-link}\) has a logarithmic approximation for the \(\mathsf{cs-ratio}_{\mathsf{0M}}\) criterion. The first upper bound is a simple consequence of Theorem 3.3. Let \(\mathsf{OPT}\) be the minimum possible \(\mathsf{cs-ratio}_{\mathsf{0M}}\). To prove the bound on the approximation we consider two cases. If \(\mathsf{OPT}\geq 1/3\) the result holds because \(\mathsf{cs-ratio}_{\mathsf{0M}}(\mathcal{A}^{k})\leq 2\ln n\leq 6\mathsf{OPT}\ln n\). If \(\mathsf{OPT}<1/3\), we argue that the clusters in the optimal clustering are "well separated" and, hence, \(\mathsf{average-link}\) builds the optimal clustering.

**Theorem 3.4**.: _For all \(k\), the \(k\)-clustering \(\mathcal{A}^{k}\) built by \(\mathsf{average-link}\) satisfies \(\mathsf{cs-ratio}_{\mathsf{0M}}(\mathcal{A}^{k})\leq 2H_{n}\). Furthermore, for all \(k\), \(\mathsf{cs-ratio}_{\mathsf{0M}}(\mathcal{A}^{k})\) is \(O(\log n)\cdot\mathsf{OPT}\) where \(\mathsf{OPT}\) is \(\mathsf{cs-ratio}_{\mathsf{0M}}\) of the \(k\)-clustering with minimum possible \(\mathsf{cs-ratio}_{\mathsf{0M}}\)._Proof.: The inequality \(\texttt{cs-ratio}_{\texttt{DM}}(\mathcal{A}^{k})\leq 2H_{n}\) is obtained by using Theorem 3.3, with \(X\) being the cluster with the largest diameter in \(\mathcal{A}^{k}\) and \(Y\) and \(Z\) being the clusters in \(\mathcal{A}^{k}\) that satisfy \(\texttt{avg}(Y,Z)=\texttt{sep}_{\texttt{min}}(\mathcal{A}^{k})\).

Now we prove that \(\mathcal{A}^{k}\) has logarithmic approximation. If \(\text{OPT}\geq 1/3\), then \(\texttt{cs-ratio}_{\texttt{DM}}(\mathcal{A}^{k})\leq 2H_{n}\leq 6\text{OPT}H_{n}\) and, hence, the desired result holds.

Thus, we assume \(\text{OPT}<1/3\), Let \(\mathcal{C}^{*}(k)\) be a \(k\)-clustering that satisfies \(\texttt{cs-ratio}_{\texttt{DM}}(\mathcal{C}^{*}(k))=\text{OPT}\). The following claim will be useful.

_Claim 1_.: Let \(C,C^{\prime}\) be two clusters in \(\mathcal{C}^{*}(k)\) and let \(a,b\) be two closest points in \(C\) and \(C^{\prime}\), that is, \(\texttt{dist}(a,b)=\min\{\texttt{dist}(x,y)|(x,y)\in C\times C^{\prime}\}\). Thus, \(\texttt{dist}(a,b)>\max\{\texttt{diam}(C),\texttt{diam}(C^{\prime})\}\).

Proof of the claim.: We assume w.l.o.g. that \(\texttt{diam}(C)\geq\texttt{diam}(C^{\prime})\). For the sake of reaching a contradiction, assume that \(\texttt{dist}(a,b)\leq\texttt{diam}(C)\). Then, it follows from the triangle inequality that the maximum distance between a point in \(C\) and \(C^{\prime}\) is at most \(3\texttt{diam}(C)\). Thus, \(\texttt{sep}_{\texttt{min}}(\mathcal{C}^{*}(k))\leq\texttt{avg}(C,C^{\prime} )\leq 3\texttt{diam}(C)\) and so \(\texttt{cs-ratio}_{\texttt{DM}}(\mathcal{C}^{*}(k))\geq\texttt{diam}(C)/3 \texttt{diam}(C)=1/3\), which contradicts our assumption. \(\square\).

Now, we argue that \(\texttt{average-link}\) constructs the clustering \(\mathcal{C}^{*}(k)\) when \(\texttt{cs-ratio}_{\texttt{DM}}(\mathcal{C}^{*}(k))<1/3\), so its approximation is 1 in this case. For the sake of reaching a contradiction, let us assume \(\mathcal{A}^{k}\neq\mathcal{C}^{*}(k)\). Hence, at some iteration \(\texttt{average-link}\) merges two clusters, say \(A\) and \(B\), that satisfy the following properties: \(A\subseteq C\) and \(B\subseteq C^{\prime}\), where \(C\) and \(C^{\prime}\) are two different clusters in \(\mathcal{C}^{*}(k)\). Let \(t\) be the first iteration of \(\texttt{average-link}\) when it occurs.

Case 1) \(A\subset C\) or \(B\subset C^{\prime}\). Let us assume w.l.o.g. that \(A\subset C\). In this case, there is a cluster \(A^{\prime}\) at the beginning of iteration \(t\) such that \(A^{\prime}\cup A\subseteq C\). We have that \(\texttt{avg}(A,A^{\prime})\leq\texttt{diam}(C)\) and by the above claim the minimum distance between \(A\) and \(B\) is larger than \(\max\{\texttt{diam}(C),\texttt{diam}(C^{\prime})\}\). Thus, \(\texttt{avg}(A,B)>\max\{\texttt{diam}(C),\texttt{diam}(C^{\prime})\}\geq \texttt{avg}(A,A^{\prime})\), which contradicts the choice of \(\texttt{average-link}\).

Case 2) \(A=C\) and \(B=C^{\prime}\). If \(k=2\) we are done. Otherwise, there exists a cluster \(C^{\prime\prime}\in\mathcal{C}^{*}(k)\) and two clusters \(X\) and \(Y\) at the beginning of iteration \(t\) such that \(X\cup Y\subseteq C^{\prime\prime}\). Thus, it follows from the condition \(\text{OPT}<1/3\) that \(\texttt{avg}(X,Y)\leq\texttt{diam}(C^{\prime\prime})<\frac{1}{3}\texttt{sep }_{\texttt{min}}(\mathcal{C}^{*}(k))\leq\frac{1}{3}\texttt{avg}(C,C^{\prime}) \leq\texttt{avg}(C,C^{\prime}),\) which again contradicts the choice of \(\texttt{average-link}\). 

It is noteworthy that, in contrast to Theorem 3.1, the assumption that the points lie in a metric space is necessary to prove Theorem 3.4. In Section B.4 we present an instance that supports this observation.

Now, we present an instance, denoted by \(\mathcal{I}^{CS}\), that shows that the above results are nearly tight. This instance with small modifications will also be used to investigate the tightness of our results regarding the separability (Section 4) and the cohesion (Section 5) of \(\texttt{average-link}\). We note that in most of the instances presented here, including \(\mathcal{I}^{CS}\), will have more than one possible execution for the methods we analyze. In these cases, we will always consider the execution that is more suitable for our purposes. These multiple executions can be avoided at the price of more complicated descriptions that involve the addition of small values \(\epsilon\) to the distance or points to break ties.

Let \(t\) be an integer that satisfies \(t!=n\); note that \(t=\Omega(\frac{\log n}{\log\log n})\). Moreover, let \(A_{0}\) be a set containing a single point located at position \(p_{0}\) in the real line and \(A_{i}\), for \(0<i\leq t-1\), be a set of \((i+1)!-i!\) points that are located at position \(p_{i}\) of the real line. We define \(B_{0}=A_{0}\) and \(B_{i}=B_{i-1}\cup A_{i}\), for \(i\geq 1\). Set \(p_{0}=0,p_{1}=1\) and, for \(i>1\), \(p_{i}=p_{i-1}+\texttt{avg}(A_{i-1},B_{i-2})\). The set of points for our instance \(\mathcal{I}^{CS}\) is \(B_{t-1}\) and the distance between a point in \(A_{i}\) and a point in \(A_{j}\) is \(|p_{i}-p_{j}|\). The following lemma gives properties of \(\mathcal{I}^{CS}\) and, in particular, how \(\texttt{average-link}\) behaves on it.

**Lemma 3.5**.: _For \(i\geq 0\), we have that \(|B_{i}|=(i+1)!\) and for \(i\geq 2\), we have \(\texttt{diam}(B_{i-2})=i(i-1)/2\), \(\texttt{avg}(B_{i-2},A_{i-1})=i+1\) and \(p_{i}=i(i+1)/2\). Furthermore, for \(k\leq t\), \(\texttt{average-link}\) obtains the \(k\)-clustering \(\mathcal{A}^{k}=(B_{t-k}\), \(A_{t-k+1},\ldots,A_{t-1})\) and, in particular, for \(k=2\) it obtains the clustering \(\mathcal{A}^{2}=(B_{t-2},A_{t-1})\)._

From Lemma 3.5, we have that \(\texttt{sep}_{\texttt{min}}(\mathcal{A}^{2})=\texttt{avg}(B_{t-2},A_{t-1})=t+1\) and \(\texttt{diam}(B_{t-2})=t(t-1)/2\), so \(\texttt{cs-ratio}_{\texttt{DM}}=\frac{t(t-1)}{2(t+1)}\), which is \(\Omega(\frac{\log n}{\log\log n})\).

Furthermore, for the clustering \(\mathcal{A}^{\prime}=(A_{0},B_{t-1}-A_{0})\) we have that

\[\texttt{sep}_{\texttt{min}}(\mathcal{A}^{\prime})=\texttt{avg}(A_{0},B_{t-1}-A_ {0})\geq\frac{|A_{t-1}|}{|B_{t-1}|}\texttt{avg}(A_{0},A_{t-1})=\left(\frac{t!-(t -1)!}{t!}\right)p_{t-1}=\frac{(t-1)^{2}}{2}\] (5)

and \(\texttt{max-diam}(\mathcal{A}^{\prime})\leq\texttt{diam}(B_{t-1})=(t+1)(t+2)/2\). Thus, \(\texttt{cs-ratio}_{\texttt{DM}}(\mathcal{A}^{\prime})=O(1)\) and the logarithmic approximation of average-link to \(\texttt{cs-ratio}_{\texttt{DM}}\) is also nearly tight.

## 4 Separability criteria

In this section, we investigate the separability of average-link. Recall that \(\text{OPT}_{\texttt{SEP}}(k)\) is the maximum possible \(\texttt{sep}_{\texttt{av}}\) of a \(k\)-clustering for \((\mathcal{X},\texttt{dist})\). We show that for average-link sepav is at least \(\frac{\text{OPT}_{\texttt{sep}}(k)}{k+2\ln n}\) and that this bound is nearly tight. We also show that there are instances in which the sepav of single-linkage and complete-linkage are exponentially smaller than that of average-link.

Theorem 4.2 gives an upper bound on \(\texttt{sep}_{\texttt{av}}\) for average-link and its complete proof can be found in Section D.2. Here, we give an overview of the proof for the case \(k>2\), which is the most involved one. The proof uses the fact established by Proposition 4.1 that there exists a set of \(k\) points \(P\subseteq\mathcal{X}\) that satisfies \(\texttt{avg}(P)\geq\text{OPT}_{\texttt{SEP}}(k)\). This holds because a set of \(k\) randomly selected points that intersect all clusters of a \(k\)-clustering with maximum sepav satisfies the the desired property (in expectation). Having this result in hands, it is enough to show that \(\texttt{avg}(P)\) is \(O((k+H_{n-1})\texttt{sep}_{\texttt{av}}(\mathcal{A}^{k}))\).

This bound on \(\texttt{avg}(P)\) is obtained by relating the distance of each pair of points \(p,p^{\prime}\in P\) with the average distance between clusters in \(\mathcal{A}^{k}\). Let \(p,p^{\prime}\in P\) and let \(A\) and \(A^{\prime}\) be clusters in \(\mathcal{A}^{k}\) such that \(p\in A\) and \(p^{\prime}\in A^{\prime}\). Moreover, let \(S\) be a cluster in \(\mathcal{A}^{k}\), with \(S\notin\{A,A^{\prime}\}\). From the triangle inequality we have that \(\texttt{dist}(p,p^{\prime})=\texttt{avg}(p,p^{\prime})\leq\texttt{avg}(p,A)+ \texttt{avg}(A,S)+\texttt{avg}(S,A^{\prime})+\texttt{avg}(A^{\prime},p^{ \prime})\). Then, by bounding both \(\texttt{avg}(p,A)\) and \(\texttt{avg}(A^{\prime},p^{\prime})\) via Lemma 3.2, with \(Y\) and \(Z\) satisfying \(\texttt{avg}(Y,Z)\leq\texttt{sep}_{\texttt{av}}(\mathcal{A}^{k})\), we conclude that \(\texttt{dist}(p,p^{\prime})\leq 2H_{n}\texttt{sep}_{\texttt{av}}(\mathcal{A}^{k})+ \texttt{avg}(A,S)+\texttt{avg}(S,A^{\prime})\). In general lines, the result is then established by averaging this inequality for all \(S\notin\{A,A^{\prime}\}\) and for all \(p,p^{\prime}\in P\).

**Proposition 4.1**.: _There is a set of points \(P\subseteq\mathcal{X}\) with the following properties: \(|P|=k\) and \(\texttt{avg}(P)\geq\text{OPT}_{\texttt{SEP}}(k)\)._

**Theorem 4.2**.: _For every \(k\), the \(k\)-clustering \(\mathcal{A}^{k}\) obtained by average-link satisfies \(\texttt{sep}_{\texttt{av}}(\mathcal{A}^{k})\geq\frac{\text{OPT}_{\texttt{ SEP}}(k)}{k+2H_{n}}\)._

We present two instances that, together, show that the previous theorem is nearly tight. The first is the instance \(\mathcal{I}^{CS}\) presented right after Theorem 3.4. For \(\mathcal{I}^{CS}\), the clustering \(\mathcal{A}^{2}=(A_{t-1},B_{t-2})\) built by average-link satisfies \(\texttt{sep}_{\texttt{av}}(\mathcal{A}^{2})=\texttt{avg}(A_{t-1},B_{t-2})=t+1\). On the other hand, Eq. (5) shows that \(\texttt{sep}_{\texttt{av}}(\mathcal{A}^{\prime})=\frac{(t-1)^{2}}{2}\), for the clustering \(\mathcal{A}^{\prime}=(A_{0},B_{t-1}-A_{0})\). Thus, for \(\mathcal{I}^{CS}\), \(\texttt{sep}_{\texttt{av}}(\mathcal{A}^{2})\) is \(O(\frac{\text{OPT}_{\texttt{SEP}}(k)\log\log n}{\log n})\).

Now, we present our second instance, denoted by \(\mathcal{I}^{sep}_{k}\). Let \(k\) be an odd number and let \(D\) and \(\epsilon\) be positive numbers. The set of points of \(\mathcal{I}^{sep}_{k}\) is given by \(S_{1}\cup S_{2}\cup S_{3}\), where \(|S_{1}|=|S_{2}|=(k-1)/2\) and \(S_{3}=\{s_{i}|1\leq i\leq k-2\}\). We have \(\texttt{dist}(x,y)=\epsilon\) for \(x,y\in S_{1}\), \(\texttt{dist}(x,y)=\epsilon\) for \(x,y\in S_{2}\), \(\texttt{dist}(x,y)=1\) for \(x,y\in S_{3}\) and \(\texttt{dist}(x,y)=D\) if \(x\) and \(y\) are not in the same set.

For \(\mathcal{I}^{sep}_{k}\), when \(D\) is sufficiently large and \(\epsilon\) is sufficiently small, \(\mathcal{A}^{k}=(S_{1},S_{2},s_{1},\ldots,s_{k-2})\) and \(\texttt{sep}_{\texttt{av}}(\mathcal{A}^{k})=O(D/k)\). On the other hand, the sepav of the \(k-\)clustering that has the cluster \(S_{3}\) and \(k-1\) singletons corresponding to the points in \(S_{1}\cup S_{2}\) is \(\Omega(D)\). Thus, \(\texttt{sep}_{\texttt{av}}(\mathcal{A}^{k})\) is \(O(\text{OPT}_{\texttt{SEP}}(k)/k)\).

We note that single-linkage and complete-linkage also obtain the \(k\)-clustering \(\mathcal{A}^{k}\) for \(\mathcal{I}^{sep}_{k}\), so the upper bound \(\text{OPT}_{\texttt{SEP}}(k)/k\) also holds for them. In Section D.3 we present instances that show that \(\texttt{sep}_{\texttt{av}}\) is \(O(\frac{\text{OPT}_{\texttt{SEP}}(k)}{\sqrt{n}})\) for both single-linkage and complete-linkage.

The instance \(\mathcal{I}^{sep}_{k}\) is particularly interesting because it also shows that natural cohesion and separability criteria can be conflicting. The key reason is that any method \(M\) with bounded approximation (in terms of \(n\)) regarding max-diam or to max-avg (Equation 3) has to build the \(k\)-clustering \(\mathcal{A}^{k}\) for \(\mathcal{I}^{sep}_{k}\). Thus, by analysing \(\mathcal{I}^{sep}_{k}\) we can conclude that the approximation factor of \(M\) to \(\mathtt{sep_{av}}\) is \(O(1/k)\) and to \(\mathtt{sep_{min}}\) is \(O(1/D)\). The details can be found in Section D.4.

## 5 On the cohesion of average-link

In this section, we prove that \(\mathtt{max\mbox{-}diam}(\mathcal{A}^{k})\leq\min\{k,1+4\ln n\}k^{1.59}\mbox{ OPT}_{\mathtt{AV}}(k)\) and we also present an instance which shows that \(\mathtt{max\mbox{-}diam}(\mathcal{A}^{k})\geq k\mbox{OPT}_{\mathtt{DM}}(k)\).

Dasgupta and Laber (2024) presented an interesting approach to devise upper bounds on cohesion criteria for a class of linkage methods that includes \(\mathtt{average\mbox{-}link}\). Although this approach was used to show that the maximum pairwise average distance of a cluster in \(\mathcal{A}^{k}\) is at most \(k^{1.59}\mbox{OPT}_{\mathtt{AV}}(k)\), it cannot be employed, at least directly, to bound the maximum diameter of a cluster in \(\mathcal{A}^{k}\). Thus, to obtain our \((1+4\ln n)k^{1.59}\mbox{OPT}_{\mathtt{AV}}(k)\) bound we combine the results of (Dasgupta and Laber, 2024) with Theorem 3.4 while for the \(k^{1+1.59}\mbox{OPT}_{\mathtt{AV}}(k)\) bound we add some new ideas/analysis on top of those from (Dasgupta and Laber, 2024).

The analysis in Dasgupta and Laber (2024) keeps a dynamic partition of the clusters produced by the linkage method under consideration. Each group in the partition is a set of clusters denoted by _family_. A point \(p\) belongs to a family \(F\) if it belongs to some cluster in \(F\). Thus, \(\mathtt{diam}(F)\) is given by the maximum distance among the points that belong to \(F\). The approach bounds the diameter of each family \(F\) as (essentially) a function of the clusters that \(F\) touches in a target \(k\)-clustering \(\mathcal{T}=(T_{1},\ldots,T_{k})\). The bound on \(\mathtt{diam}(F)\) is then used to upper bound the diameter of the clusters in \(F\). For a \(k\)-clustering \(\mathcal{C}\), let \(\mathtt{avg\mbox{-}diam}(\mathcal{C}):=\frac{1}{k}\sum_{i=i}^{k}\mathtt{diam} (C_{i})\). As in Dasgupta and Laber (2024), we use as the target clustering the one with minimum \(\mathtt{avg\mbox{-}diam}\).

We explain how the families evolve along the execution of a linkage method, in particular \(\mathtt{average\mbox{-}link}\). Initially, we have \(k\) families, \(F_{1},\ldots,F_{k}\), where \(F_{i}\) is a family that contains \(|T_{i}|\) clusters, each one being a point from \(T_{i}\). Furthermore, the families are organized in a directed forest \(D\) that initially consists of \(k\) isolated nodes, where the \(i\)th node corresponds to family \(F_{i}\).

We specify how the families and the forest \(D\) are updated when the linkage method merges the clusters \(g\) and \(g^{\prime}\) belonging to the families \(F\) and \(F^{\prime}\), respectively. Assume w.l.o.g. \(|F|\geq|F^{\prime}|\). We have the following cases:

* \(|F^{\prime}|=1\) and \(|F|>1\). In this case two new families are created, \(F^{new}:=F-\{g\}\) and \(F^{new^{\prime}}:=\{g\cup g^{\prime}\}\). Moreover, \(F^{new}\) and \(F^{new^{\prime}}\) become, respectively, parents of \(F\) and \(F^{\prime}\) in \(D\)
* \(|F^{\prime}|>1\) or \(|F|=1\). In this case, only one family is created, \(F^{new}:=(F\cup F^{\prime}\cup\{g\cup g^{\prime}\})-g-g^{\prime}\). Moreover, \(F^{new}\) becomes parent of both \(F\) and \(F^{\prime}\) in \(D\).

We say that a family \(F\) is _regular_ if \(|F|>1\).

**Proposition 5.1** (Proposition 3.1 of Dasgupta and Laber (2024)).: _At the beginning of each iteration of \(\mathtt{average\mbox{-}link}\) at least one of the roots of the forest \(D\) corresponds to a regular family._

Let \(\mathcal{M}\) be the class of linkage methods (Algorithm 2) whose function \(f\), employed to measure the distance between clusters \(A\) and \(B\) satisfies

\[\{\mathtt{dist}(a,b)|(a,b)\in A\times B\}\leq f(A,B)\leq\mathtt{diam}(A\cup B)\] (6)

**Proposition 5.2** (Proposition 5.1 of Dasgupta and Laber (2024)).: _The diameter of every regular family \(F\) produced along the execution of a linkage method in \(\mathcal{M}\) is at most \(k^{\log_{2}3}\mbox{OPT}_{\mathtt{AV}}(k)\)._

Note that the function \(\mathtt{dist}_{AL}\) employed by \(\mathtt{average\mbox{-}link}\) satisfies the condition given by (6) and, thus, the above proposition holds for \(\mathtt{average\mbox{-}link}\).

We are ready to establish the main result of this section.

**Theorem 5.3**.: _Every cluster \(S\) in \(\mathcal{A}^{k}\) satisfies \(\mathtt{diam}(S)\leq\min\{k,4\ln n+1\}k^{\log_{2}3}\mbox{OPT}_{\mathtt{AV}}(k)\)._

Proof.: Let \(V=\{T\in\mathcal{T}|S\cap T\neq\emptyset\}\) be the set of clusters of the target clustering \(\mathcal{T}\) that intersect \(S\). We build a graph \(G\) whose nodes correspond to the clusters in \(V\). At the beginning of \(\mathtt{average\mbox{-}link}\)'s execution, \(G\) contains the set of nodes \(V\) and no edges.

At each iteration, there are two possibilities for the clusters \(g\) and \(g^{\prime}\) that are merged by \(\mathtt{average\mbox{-}link}\): \((g\cup g^{\prime})\cap S=\emptyset\) or \((g\cup g^{\prime})\subseteq S\). We define how \(G\) is updated in each case:

Case 1) \((g\cup g^{\prime})\cap S=\emptyset\). In this case, \(G\) is not updated.

Case 2) \((g\cup g^{\prime})\subseteq S\). Let \(x\) and \(y\) be points in \(g\) and \(g^{\prime}\) such that \(\mathtt{dist}(x,y)\) is minimum and let \(T^{x}\) and \(T^{y}\) be the clusters in \(\mathcal{T}\) that contain \(x\) and \(y\), respectively. We add an edge of weight \(\mathtt{dist}(x,y)\) between \(T^{x}\) and \(T^{y}\). We say, in this case, that \(x\) and \(y\) are _associated_ with the edge that links \(T^{x}\) to \(T^{y}\).

We need the following two claims:

_Claim 2_.: \(\mathtt{dist}(x,y)\leq k^{\log_{2}3}\textsc{OPT}_{\mathtt{AV}}(k)\)_._

_Proof of the claim._ Let \(H\) be a regular family at the beginning of iteration \(t\) Such family does exist due to Proposition 5.1. Moreover, let \(h\) and \(h^{\prime}\) be two clusters in \(H\). We have that

\[\mathtt{dist}(x,y)\leq\mathtt{dist}_{\mathit{AL}}(g,g^{\prime})\leq\mathtt{ dist}_{\mathit{AL}}(h,h^{\prime})\leq\mathtt{diam}(h\cup h^{\prime})\leq\mathtt{ diam}(H)\leq k^{\log_{2}3}\textsc{OPT}_{\mathtt{AV}}(k),\]

where the second inequality holds by the choice of \(\mathtt{average\mbox{-}link}\) and the last inequality holds due to the Proposition 5.2. \(\square\)

_Claim 3_.: For a cluster \(C\), let \(V_{C}:=\{T\in\mathcal{T}|T\cap C\neq\emptyset\}\). Let \(S^{\prime}\) be a cluster generated by \(\mathtt{average\mbox{-}link}\) that is a subset of \(S\). Then, when \(S^{\prime}\) is created, the subgraph of \(G\) induced by \(V_{S^{\prime}}\) is connected.

_Proof of the claim_ If \(|S^{\prime}|=1\) the property holds. Let \(S^{\prime}\) be a cluster obtained by merging \(S_{1}\) and \(S_{2}\). By induction, the property holds for \(S_{1}\) and \(S_{2}\). Since an edge is added between nodes in \(V_{S_{1}}\) and \(V_{S_{2}}\) then the property also holds for \(S\). \(\square\)

Thus, at the end of the algorithm, \(G\) is connected and each of its edges has weight at most \(k^{\log_{2}3}\textsc{OPT}_{\mathtt{AV}}(k)\). Let \(x\) and \(y\) be points in \(S\) such that \(\mathtt{dist}(x,y)=\mathtt{diam}(S)\) and let \(T^{x}=v_{1}\ldots v_{\ell}=T^{y}\) be a path in \(G\) from \(T^{x}\) to \(T^{y}\).

Consider a sequence of points \(x=p_{1}p^{\prime}_{1}\ldots p_{\ell}p^{\prime}_{\ell}=y\), where \(p_{i}\) and \(p^{\prime}_{i}\) are the points in \(v_{i}\) associated with the edge \(v_{i-1}v_{i}\) and \(v_{i}v_{i+1}\), respectively. From the triangle inequality

\[\mathtt{dist}(x,y)\leq\sum_{i=1}^{\ell-1}\mathtt{dist}(p^{\prime }_{i},p_{i+1})+\sum_{i=1}^{\ell}\mathtt{dist}(p_{i},p^{\prime}_{i})\leq(k-1)k ^{\log_{2}3}\textsc{OPT}_{\mathtt{AV}}(k)+\sum_{i=1}^{k}\mathtt{diam}(T_{i}) \leq\\ (k-1)k^{\log_{2}3}\textsc{OPT}_{\mathtt{AV}}(k)+k\textsc{OPT}_{ \mathtt{AV}}(k)\]

For the logarithmic bound, let \(S_{1}\) and \(S_{2}\) be the two clusters that are merged to form \(S\). At the beginning of the iteration in which \(S_{1}\) and \(S_{2}\) are merged, Proposition 5.1 assures that there exists a regular family, say \(H\). Let \(h\) and \(h^{\prime}\) be two clusters in \(H\). By Proposition 5.2, \(\mathtt{avg}(h,h^{\prime})\leq\mathtt{diam}(H)\leq k^{\log_{2}3}\textsc{OPT}_{ \mathtt{AV}}(k)\). Thus, by Theorem 3.3, \(\mathtt{diam}(S_{1})\leq 2\ln n\cdot\mathtt{avg}(h,h^{\prime})\leq 2\ln n \cdot k^{\log_{2}3}\textsc{OPT}_{\mathtt{AV}}(k)\) and \(\mathtt{diam}(S_{2})\leq 2\ln n\cdot k^{\log_{2}3}\textsc{OPT}_{\mathtt{AV}}(k)\). Let \(s_{1}\in S_{1}\) and \(s_{2}\in S_{2}\) be such that \(\mathtt{dist}(s_{1},s_{2})=\min\{\mathtt{dist}(p,q)|(p,q)\in S_{1}\times S_{2}\}\). Since \(S_{1}\) and \(S_{2}\) are merged we have that \(\mathtt{dist}(s_{1},s_{2})\leq\mathtt{avg}(S_{1},S_{2})\leq\mathtt{avg}(h,h^{ \prime})\leq k^{\log_{2}3}\textsc{OPT}_{\mathtt{AV}}(k)\). Thus, \(\mathtt{diam}(S)\leq\mathtt{diam}(S_{1})+\mathtt{dist}(s_{1},s_{2})+\mathtt{ diam}(S_{1})\leq(1+4\ln n)k^{\log_{2}3}\textsc{OPT}_{\mathtt{AV}}(k)\). \(\square\)

Theorem 3.4 of Dasgupta and Laber [2024] presents an instance with \(n=2k-2\) points for which \(\mathtt{single\mbox{-}linkage}\) builds a \(k\)-clustering that has a cluster whose diameter is \(\Omega(k^{2}\textsc{OPT}_{\mathtt{AV}}(k))\). Thus, this result together with Theorem 5.3 show a separation between \(\mathtt{average\mbox{-}link}\) and \(\mathtt{single\mbox{-}linkage}\) when \(k\) is \(\Omega(\log^{2.41}n)\).

Our last theoretical result is a lower bound on the maximum diameter of the clustering built by \(\mathtt{average\mbox{-}link}\). Its proof can be found in the Section E and it employs an augmented version of instance \(\mathcal{I}^{CS}\), presented right after Theorem 3.4.

**Theorem 5.4**.: _There is an instance for which the \(k\)-clustering \(\mathcal{A}^{k}\) built by \(\mathtt{average\mbox{-}link}\) satisfies \(\mathtt{max\mbox{-}diam}(\mathcal{A}^{k})\in\Omega(k\textsc{OPT}_{\mathtt{ IN}}(k))\)_

## 6 Experiments

In this final section, we briefly present an experiment in which we evaluate whether average-link, in addition to having better theoretical bounds, it also has a better performance in practice for the studied criteria. We employed 10 datasets and used the Euclidean metric to measure distances. For each of them, we executed average-link, complete-linkage and single-linkage, for the following sets of values of \(k\): Small=\(\{k|2\leq k\leq 10\}\), Medium=\(\{k|\sqrt{n}-4\leq k\leq\sqrt{n}+4\}\) and Large=\(\{k|k=n/i\text{ and }2\leq i\leq 10\}\). More details, as well as the results of our experiment with other distances, can be found in Section F.

Table 6 shows the average ratio between the result of a method and that of the best one, grouped by criterion and set of \(k\). Each entry is the average of 90 ratios (\(9\)\(k\)'s and 10 datasets) and each of these ratios for a method \(M\) is a value between 0 and 1 that is obtained by dividing the minimum between the result of \(M\) and that of the best method by the maximum between them. The letters A, C and S are the initials of the evaluated methods.

Concerning separability criteria, single-linkage and average-link have the best results for \(\texttt{sep}_{\texttt{av}}\). The latter has some advantage when \(k\) is small, which is in line with its better worst-case bound for small \(k\) (results from Section 4). For \(\texttt{sep}_{\texttt{min}}\), average-link has a huge advantage, which is not surprising since its linkage rule tries to increase \(\texttt{sep}_{\texttt{min}}\) at each step by merging the the clusters \(A\) and \(B\) for which \(\texttt{avg}(A,B)=\texttt{sep}_{\texttt{min}}(\mathcal{C})\), where \(\mathcal{C}\) is the current clustering.

Regarding cohesion criteria, complete-linkage and average-link were the best methods. They had close results for max-avg while for max-diam the former had a strong dominance. These results align with ours and those from (Dasgupta and Laber, 2024), in the sense that they show that these linkage methods present better worst-case upper bounds than single-linkage when the comparison is made against \(\text{OPT}_{\texttt{av}}(k)\). Moreover, the advantage of complete-linkage for max-diam is also expected since it is the "natural" greedy rule to minimize the maximum diameter (See Proposition 2.1 of Dasgupta and Laber (2024)).

For cs-ratio\({}_{\texttt{DM}}\), average-link and complete-linkage present the best results, with the former being slightly superior for the small \(k\) and the latter being slightly superior when \(k\) is not small. average-link has a huge dominance for the cs-ratio\({}_{\texttt{AV}}\) criterion, which lines up with the theoretical results from Section 3.1.

In summary, these experiments, together with our theoretical results, provide evidence that average-link is a better choice when both cohesion and separability are relevant.

**Acknowledgements** The work of the first author is partially supported by CNPq (grant 310741/2021-1). This study was financed in part by the Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior - Brasil (CAPES) - Finance Code 001

**Limitations.** We have not identified a major limitation in our work. That said, the assumption that the points lie in a metric space used in our results (except Theorem 3.1) could be seen as a limitation. On the experimental side, having more than 10 datasets would give our conclusions more robustness.

## References

* Eisen et al. (2018) Michael B. Eisen, Paul T. Spellman, Patrick O. Brown, and David Botstein. Cluster analysis and display of genome-wide expression patterns. _Proceedings of the National Academy of Sciences

\begin{table}
\begin{tabular}{l c c c||c c c||c c c}  & \multicolumn{3}{c}{Small} & \multicolumn{3}{c}{Medium} & \multicolumn{3}{c}{Large} \\ \hline  & A & C & S & A & C & S & A & C & S \\ \hline \(\texttt{sep}_{\texttt{min}}\) & **0,99** & 0.82 & 0,76 & **1** & 0,81 & 0,68 & **1** & 0,81 & 0,72 \\ \(\texttt{sep}_{\texttt{av}}\) & **0,97** & 0.82 & 0,94 & 0,97 & 0,9 & **1** & 0,98 & 0,96 & **1** \\ max-diam & 0,85 & **1** & 0,72 & 0,8 & **1** & 0,48 & 0,76 & **1** & 0,38 \\ max-avg & 0,95 & **0,96** & 0,86 & **0,99** & 0,89 & 0,71 & **0,99** & 0,84 & 0,67 \\ cs-ratio\({}_{\texttt{DM}}\) & **0,96** & 0,92 & 0,63 & 0,95 & **0,97** & 0,4 & 0,93 & **0,99** & 0,33 \\ cs-ratio\({}_{\texttt{AV}}\) & **0,98** & 0,82 & 0,69 & **1** & 0,73 & 0,51 & **1** & 0,68 & 0,4 \\ \end{tabular}
\end{table}
Table 1: Average ratio between the result of a method and the best one for each criterion and each group of \(k\). The best results are bold-facedof the United States of America_, 95(25):14863-14868, December 1998. ISSN 0027-8424. doi: 10.1073/pnas.95.25.14863.
* Tumminello et al. (2010) Michele Tumminello, Fabrizio Lillo, and Rosario N.Mantegna. Correlation, hierarchies, and networks in financial markets. _Journal of Economic Behavior \(\&\) Organization_, 75(1):40-58, 2010. ISSN 0167-2681. doi: https://doi.org/10.1016/j.jebo.2010.01.004. Transdisciplinary Perspectives on Economic Complexity.
* 17, 2017_, pages 255-264. ACM, 2017. doi: 10.1145/3097983.3098079. URL https://doi.org/10.1145/3097983.3098079.
* Monath et al. (2021) Nicholas Monath, Kumar Avinava Dubey, Guru Guruganesh, Manzil Zaheer, Amr Ahmed, Andrew McCallum, Gokhan Mergen, Marc Najork, Mert Terzian, Bryon Tjanaka, Yuan Wang, and Yuchen Wu. Scalable hierarchical agglomerative clustering. In Feida Zhu, Beng Chin Ooi, and Chunyan Miao, editors, _KDD '21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021_, pages 1245-1255. ACM, 2021. doi: 10.1145/3447548.3467404. URL https://doi.org/10.1145/3447548.3467404.
* Yu et al. (2021) Shangdi Yu, Yigiu Wang, Yan Gu, Laxman Dhulipala, and Julian Shun. Parchain: A framework for parallel hierarchical agglomerative clustering using nearest-neighbor chain. _Proc. VLDB Endow._, 15(2):285-298, 2021. doi: 10.14778/3489496.3489509. URL http://www.vldb.org/pvldb/vol15/p285-yu.pdf.
* Dhulipala et al. (2021) Laxman Dhulipala, David Eisenstat, Jakub Lacki, Vahab S. Mirrokni, and Jessica Shi. Hierarchical agglomerative graph clustering in nearly-linear time. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 2676-2686. PMLR, 2021. URL http://proceedings.mlr.press/v139/dhulipala21a.html.
* December 9, 2022_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/909de96145d97514b143dfde03e6cd2b-Abstract-Conference.html.
* Dhulipala et al. (2023) Laxman Dhulipala, Jakub Lacki, Jason Lee, and Vahab Mirrokni. Terahac: Hierarchical agglomerative clustering of trillion-edge graphs. _Proc. ACM Manag. Data_, 1(3):221:1-221:27, 2023. doi: 10.1145/3617341. URL https://doi.org/10.1145/3617341.
* Cohen-Addad et al. (2019) Vincent Cohen-Addad, Varun Kanade, Frederik Mallmann-Trenn, and Claire Mathieu. Hierarchical clustering: Objective functions and algorithms. _J. ACM_, 66(4):26:1-26:42, 2019. doi: 10.1145/3321386. URL https://doi.org/10.1145/3321386.
* Charikar et al. (2019) Moses Charikar, Vaggos Chatziafratis, Rad Niazadeh, and Grigory Yaroslavtsev. Hierarchical clustering for euclidean data. In Kamalika Chaudhuri and Masashi Sugiyama, editors, _The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan_, volume 89 of _Proceedings of Machine Learning Research_, pages 2721-2730. PMLR, 2019a. URL http://proceedings.mlr.press/v89/charikar19a.html.
* Moseley and Wang (2023) Benjamin Moseley and Joshua R. Wang. Approximation bounds for hierarchical clustering: Average linkage, bisecting k-means, and local search. _J. Mach. Learn. Res._, 24:1:1-1:36, 2023. URL http://jmlr.org/papers/v24/18-080.html.
* Charikar et al. (2019) Moses Charikar, Vaggos Chatziafratis, and Rad Niazadeh. Hierarchical clustering better than average-linkage. In Timothy M. Chan, editor, _Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019_, pages 2291-2304. SIAM, 2019b. doi: 10.1137/1.9781611975482.139. URL https://doi.org/10.1137/1.9781611975482.139.
* Charikar et al. (2019)Sanjoy Dasgupta. A cost function for similarity-based hierarchical clustering. In Daniel Wichs and Yishay Mansour, editors, _Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016_, pages 118-127. ACM, 2016. doi: 10.1145/2897518.2897527. URL https://doi.org/10.1145/2897518.2897527.
* Wang and Moseley (2020) Yuyan Wang and Benjamin Moseley. An objective for hierarchical clustering in euclidean space and its connection to bisecting k-means. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 6307-6314. AAAI Press, 2020. doi: 10.1609/AAAI.V34104.6099. URL https://doi.org/10.1609/aaai.v34i04.6099.
* Dasgupta and Laber (2024) Sanjoy Dasgupta and Eduardo Sany Laber. New bounds on the cohesion of complete-link and other linkage methods for agglomerative clustering. In _Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024_. OpenReview.net, 2024. URL https://openreview.net/forum?id=gL5djEYLx2.
* Dasgupta and Long (2005) Sanjoy Dasgupta and Philip M. Long. Performance guarantees for hierarchical clustering. _Journal of Computer and System Sciences_, 70(4):555-569, 2005. ISSN 0022-0000. doi: https://doi.org/10.1016/j.jcss.2004.10.006. URL https://www.sciencedirect.com/science/article/pii/S0022000004001321. Special Issue on COLT 2002.
* Ackermann et al. (2010) Marcel R. Ackermann, Johannes Blomer, Daniel Kuntze, and Christian Sohler. Analysis of agglomerative clustering. _CoRR_, abs/1012.3697, 2010. URL http://arxiv.org/abs/1012.3697.
* ESA 2015
- 23rd Annual European Symposium, Patras, Greece, September 14-16, 2015, Proceedings_, volume 9294 of _Lecture Notes in Computer Science_, pages 656-667. Springer, 2015. doi: 10.1007/978-3-662-48350-3_55. URL https://doi.org/10.1007/978-3-662-48350-3_55.
* Arutyunova et al. (2023) Anna Arutyunova, Anna Grosswendt, Heiko Roglin, Melanie Schmidt, and Julian Wargalla. Upper and lower bounds for complete linkage in general metric spaces. _Machine Learning_, pages 1-30, 2023.
* Kleinberg and Tardos (2006) Jon M. Kleinberg and Eva Tardos. _Algorithm design_. Addison-Wesley, 2006. ISBN 978-0-321-37291-8.
* 16, 2023_, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/20f814ecdaa8c76131e21683447e347b-Abstract-Conference.html.
* Grosswendt et al. (2019) Anna Grosswendt, Heiko Roglin, and Melanie Schmidt. Analysis of ward's method. In Timothy M. Chan, editor, _Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019_, pages 2939-2957. SIAM, 2019. doi: 10.1137/1.9781611975482.182. URL https://doi.org/10.1137/1.9781611975482.182.
* Brooks et al. (2014) Pope D. Brooks, Thomas and Michael Marcolini. Airfoil Self-Noise. UCI Machine Learning Repository, 2014. DOI: https://doi.org/10.24432/C5VW2C.
* Lohweg (2013) Volker Lohweg. Banknote Authentication. UCI Machine Learning Repository, 2013. DOI: https://doi.org/10.24432/C55P57.
* Yeh (2007) I-Cheng Yeh. Concrete Compressive Strength. UCI Machine Learning Repository, 2007. DOI: https://doi.org/10.24432/C5PK67.
* Chapaydin et al. (1998) Kaynak Cenk Alpaydin, Ehem. Cascading classifiers. _Kybernetika_, 34(4):[369]-374, 1998. URL http://eudml.org/doc/33363.
* Zhou (2014) Fang Zhou. Geographical Origin of Music. UCI Machine Learning Repository, 2014. DOI: https://doi.org/10.24432/C5VK5D.
* Zhou et al. (2015)Gardiner Katheleen Higuera, Clara and Krzysztof Cios. Mice Protein Expression. UCI Machine Learning Repository, 2015. DOI: https://doi.org/10.24432/C50S3Z.
* Cassotti Matteo Consonni Viviana Ballabio (2019) Cassotti Matteo Consonni Viviana Ballabio, Davide and Roberto Todeschini. QSAR fish toxicity. UCI Machine Learning Repository, 2019. DOI: https://doi.org/10.24432/C5JG7B.
* Renjith (2018) Shini Renjith. Travel Reviews. UCI Machine Learning Repository, 2018. DOI: https://doi.org/10.24432/C56K6W.

Proof of proposition 2.1

Proof.: Let \(a\in A\) and \(c\in C\). Then, \(\mathtt{dist}(a,c)\leq\mathtt{dist}(a,b)+\mathtt{dist}(b,c)\) for every \(b\in B\). Thus,

\[|B|\mathtt{dist}(a,c)\leq\sum_{b\in B}(\mathtt{dist}(a,b)+\mathtt{dist}(b,c))\]

It follows that

\[|B|\sum_{a\in A}\sum_{c\in C}\mathtt{dist}(a,c)\leq\sum_{a\in A} \sum_{c\in C}(\sum_{b\in B}(\mathtt{dist}(a,b)+\mathtt{dist}(b,c)))=\] \[|C|\sum_{a\in A}\sum_{b\in B}\mathtt{dist}(a,b)+|A|\sum_{b\in B} \sum_{c\in C}\mathtt{dist}(b,c)\]

Dividing both sides by \(|A|\cdot|B|\cdot|C|\) we establish the inequality. 

## Appendix B Proofs of section 3

### Proof of Theorem 3.1

Proof.: When \(k=n\) the result is valid because \(\mathsf{avg}(A^{n})=0\) for every \(A\in\mathcal{A}^{n}\). We assume by induction that the result holds for \(k+1\) and we prove that it also holds for \(k\). Let \(A\) and \(B\) be the clusters in \(\mathcal{A}^{k+1}\) that are merged to obtain \(\mathcal{A}^{k}\), so \(\mathcal{A}^{k}=\mathcal{A}^{k+1}\cup(A\cup B)-\{A,B\}\). Let \(S,T\) and \(U\) be clusters in \(\mathcal{A}^{k}\), with \(T\neq U\). It is enough to prove that \(\mathsf{avg}(S)\leq\mathsf{avg}(T,U)\).

Case 1) \(A\cup B\notin\{S,T,U\}\). In this case, \(S,T,U\in\mathcal{A}^{k+1}\) and, then, by induction, \(\mathsf{avg}(S)\leq\mathsf{avg}(T,U)\).

Case 2) \(A\cup B=S\) and \(S\notin\{T,U\}\). Since \(A,B,T,U\in\mathcal{A}^{k+1}\), the induction hypothesis assures that \(\mathsf{avg}(A)\leq\mathsf{avg}(T,U)\) and \(\mathsf{avg}(B)\leq\mathsf{avg}(T,U)\) and the average-link rule ensures that \(\mathsf{avg}(A,B)\leq\mathsf{avg}(T,U)\). Since \(\mathsf{avg}(S)\) is a convex combination of \(\mathsf{avg}(A)\), \(\mathsf{avg}(B)\) and \(\mathsf{avg}(A,B)\), the above inequalities imply that \(\mathsf{avg}(S)=\mathsf{avg}(A\cup B)\leq\mathsf{avg}(T,U)\).

Case 3) \(A\cup B=S\) and \(S\in\{T,U\}\). We assume w.l.o.g. that \(S=T\). The induction hypothesis and the average-link rule guarantee that \(\max\{\mathsf{avg}(A),\mathsf{avg}(B),\mathsf{avg}(A,B)\}\leq\min\{\mathsf{ avg}(A,U),\mathsf{avg}(B,U)\}\) Since \(\mathsf{avg}(S,U)\) is a convex combination of \(\mathsf{avg}(A,U)\) and \(\mathsf{avg}(B,U)\) and \(\mathsf{avg}(S)\) is a convex combination of \(\mathsf{avg}(A)\), \(\mathsf{avg}(B)\) and \(\mathsf{avg}(A,B)\), the above inequality implies that \(\mathsf{avg}(S)=\mathsf{avg}(A\cup B)\leq\mathsf{avg}(T,U)\).

Case 4) \(S\neq A\cup B\) and \(A\cup B\in\{T,U\}\). We assume w.l.og. that \(T=A\cup B\). Since \(S,A,B,U\in\mathcal{C}^{k+1}\), the induction hypothesis assures that \(\mathsf{avg}(S)\leq\min\{\mathsf{avg}(A,U),\mathsf{avg}(B,U)\}\) Since \(\mathsf{avg}(T,U)\) is a convex combination of \(\mathsf{avg}(A,U)\) and \(\mathsf{avg}(B,U)\), the above inequality assures that \(\mathsf{avg}(S)\leq\mathsf{avg}(T,U)\). 

### Lower bounds on \(\mathtt{cs\text{-}ratio}_{\mathsf{AV}}\) for other methods

The following examples show that the \(\mathtt{cs\text{-}ratio}_{\mathsf{AV}}\) of complete-linkage, single-linkage and a random hierarchy can be much higher than that of average-link in metric spaces.

**single-linkage.** Consider the instance with \(n\) points \(x_{1},\dots,x_{n}\) in the real line, where \(x_{i}=1\), if \(i=1\), and \(x_{i}=x_{i-1}+1-i\epsilon\), for \(i>1\). For \(\epsilon\) sufficiently small, single-linkage builds the \(k\)-clustering \(\mathcal{C}=(x_{1},x_{2},\dots,x_{k-1},\{x_{k},\dots,x_{n}\})\). We have that \(\mathsf{avg}(\{x_{k},\dots,x_{n}\})\) is \(\Omega(n-k)\) while \(\mathsf{avg}(x_{1},x_{2})=1-\epsilon\), so that \(\mathtt{cs\text{-}ratio}_{\mathsf{AV}}(\mathcal{C})\) is \(\Omega(n-k)\).

**complete-linkage.** Let \(t=2^{m}-1\), where \(m\) is a positive integer and let \(p=2(t^{2}+t)\). We build an instance whose set of points \(\mathcal{X}=A\cup B\cup C\cup D\cup E\) has \(n=2p\) points, where \(A,B,C,D\) and \(E\) are sets of points in \(\mathbb{R}^{p+1}\) that satisfy the following properties:

* the first coordinate of the points in \(A\cup B\cup C\cup D\) is the only one that has a value different than 0;
* \(A=\{a_{1},\dots,a_{t}\}\) and the first coordinate of \(a_{i}\) is equal to \(i+1/2\);* \(B=\{b_{1},\ldots,b_{t}\}\) and the first coordinate of \(b_{i}\) is equal to \(-(i+1/2)\);
* \(C\) has \(t^{2}\) points and all have the first coordinate \(1/2\);
* \(D\) has \(t^{2}\) points and all have the first coordinate \(-1/2\);
* \(E=\{e_{1},\ldots,e_{p}\}\), where the value of the first coordinate of \(e_{i}\) is \(t^{2}\), the \((i+1)\)th coordinate has value \(1.5t\) and all other coordinates have value equal to \(0\).

The distance between any two points in \(\mathcal{X}\) is given by the \(\ell_{1}\) metric. Hence, the distance between any two points in \(E\) is \(3t\), the distance between points in \(A\cup B\cup C\cup D\) is at most \(2t+1\) and the distance between a point in \(A\cup B\cup C\cup D\) and a point in \(E\) is at least \(t^{2}\). For \(i\leq p\), let \(E_{i}=\{e_{i},\ldots,e_{p}\}\).

Thus, for \(2<k<p=n/2\), there is a way to break ties for which the \(k\)-clustering obtained by \(\mathtt{complete\mbox{-}linkage}\) is \(\mathcal{C}^{k}=(A\cup C,B\cup D,e_{1},e_{2},\ldots,e_{k-3},E_{k-2})\).

We have that \(\max\{\mathtt{dist}(a,d)\in A\times D\}\leq t+1\), \(\max\{\mathtt{dist}(b,c)\in B\times C\}\leq t+1\) and \(\max\{\mathtt{dist}(a,b)\in A\times B\}\leq 2t+1\). Thus, we get that

\[\mathtt{sep_{min}}(\mathcal{C}^{k})\leq\mathtt{avg}((A\cup C,B \cup D))\leq\] \[\frac{1}{(t^{2}+t)^{2}}\left(\sum_{x\in A}\sum_{y\in B}\mathtt{ dist}(x,y)+\sum_{x\in A}\sum_{y\in D}\mathtt{dist}(x,y)+\sum_{x\in C}\sum_{y\in B }\mathtt{dist}(x,y)+\sum_{x\in C}\sum_{y\in D}\mathtt{dist}(x,y)\right)\] \[\leq\frac{t^{2}(2t+1)+t^{3}(t+1)+t^{3}(t+1)+t^{4}}{(t^{2}+t)^{2} }\leq 3\]

Since \(\mathtt{max\mbox{-}avg}(\mathcal{C})\geq\mathtt{avg}(E_{k-2})=3t\), we get that \(\mathtt{cs\mbox{-}ratio}_{\mathtt{N}}(\mathcal{C}^{k})\) is \(\Omega(t)\) and, hence, \(\Omega(\sqrt{n})\).

**random hierarchy.** To analyze a random hierarchy, we first need to define how it is generated. We start with a random permutation of the points in \(\mathcal{X}\) and a clustering \(\mathcal{C}\) containing initially the cluster comprised by all points in \(\mathcal{X}\). Let \(x_{1},\ldots,x_{n}\) be the points in \(\mathcal{X}\) according to the order given by the permutation. Then, we perform the following steps until we have \(n\) clusters:

* \(j\leftarrow\) a randomly selected a number in the set \(\{1,2,\ldots,n-1\}\).
* If the points \(x_{j}\) and \(x_{j+1}\) are in the same cluster \(C\in\mathcal{C}\)
* split \(C\) into \(C_{\leq}=\{x_{i}\in C|i\leq j\}\) and the cluster \(C_{>}=C-C_{\leq}\).
* Update \(\mathcal{C}\) by replacing \(C\) with \(C_{\leq}\) and \(C_{>}\)

After \(t\) splits we have a clustering with \(n-t\) clusters.

Now, we consider an instance with \(n\) points and 3 groups \(X\), \(Y\) and \(Z\), that satisfy \(|X|=|Y|=(n-1)/2\) and \(Z=\{z\}\). The distance between any two points in \(X\) is \(1\) and the same holds for \(Y\). Moreover, the distance between points in \(X\) and \(Y\) is \(2\). The distance of \(z\) to any other point is \(D>>n^{2}\). Any \(k\)-clustering, with \(k\geq 3\), has \(\mathtt{sep_{min}}\leq 2\) because at least two clusters do not contain \(z\). Let \(k\leq n/2\). The probability that \(z\) is a singleton in the \(k\)-clustering when \(z\notin\{x_{1},x_{n}\}\) is

\[\frac{{n-3\choose k-3}}{{n-1\choose k-1}}=\frac{(k-1)(k-2)}{(n-1)(n-2)}<\frac {1}{4}\]

Then, with probability at least \(3/4\), there will be a cluster \(C\) that contains \(z\) and a point in \(X\cup Y\), which implies that \(E[\mathtt{avg}(C)]\geq D/4n^{2}\). Thus, with probability at least \(3/4\) the \(k\)-clustering induced by the random hierarchy has \(\mathtt{sep_{av}}\ \Omega(D/4n^{2})\), when \(z\notin\{x_{1},x_{n}\}\). Since the probability of \(z\notin\{x_{1},x_{n}\}\) is \((n-2)/n\), the same bound holds when we drop this constraint.

On the approximation of \(\mathtt{average\mbox{-}link}\) for \(\mathtt{cs\mbox{-}ratio}_{\mathtt{A}\mathtt{V}}\)

Let \(n\) be an even number, \(k=2\) and \(\epsilon\) a positive number very close to 0. Consider 4 set of points \(S_{1}\), \(S_{2}\), \(S_{3}\) and \(S_{4}\), where \(S_{1}=\{s_{1}\}\),\(S_{2}=\{s_{2}\}\) and \(S_{3}\) and \(S_{4}\) have \(n/2-1\) points each. We have \(\mathtt{dist}(x,y)=\epsilon\) for \(x,y\in S_{3}\), \(\mathtt{dist}(x,y)=\epsilon\) for \(x,y\in S_{4}\), \(\mathtt{dist}(s_{1},s_{2})=T\) and \(\mathtt{dist}(x,y)=T\) for \((x,y)\in S_{3}\times S_{4}\). In addition, we have \(\mathtt{dist}(s_{1},x)=2T\) for \(x\neq s_{2}\) and \(\mathtt{dist}(s_{2},y)=2T\) for \(y\neq s_{1}\).

Clearly, the \(4\)-clustering obtained by \(\mathtt{average-link}\) is \((S_{1},S_{2},S_{3},S_{4})\). Then, to obtain a 2-clustering, it merges the clusters \(S_{1}\) and \(S_{2}\) and, next, \(S_{3}\) and \(S_{4}\), so that the final 2-clustering is \(\mathcal{A}^{2}=(S_{1}\cup S_{2},S_{3}\cup S_{4})\), which satisfies \(\mathtt{max-avg}(\mathcal{A}^{2})=T\) and \(\mathtt{sep_{min}}(\mathcal{A})=2T\). On the other hand, for the clustering \(\mathcal{S}=(S_{1}\cup S_{3},S_{2}\cup S_{4})\), we have that \(\mathtt{max-avg}(\mathcal{S})\) is \(O(T/n^{2})\) and \(\mathtt{sep_{min}}(\mathcal{S})\geq T\). Thus, the approximation of \(\mathtt{average-link}\) is \(\Omega(n^{2})\)

### Triangle inequality is necessary for Theorem 3.4

We present an instance that shows that the assumption that points lie in a metric space is necessary to establish Theorem 3.4.

Let \(A\) and \(B\) be sets with \(n/2-1\) and \(n/2\) points, respectively. We have \(\mathtt{dist}(a,a^{\prime})=1\) if \(a,a^{\prime}\in A\); \(\mathtt{dist}(b,b^{\prime})=1\) if \(b,b^{\prime}\in B\) and \(\mathtt{dist}(a,b)=4\) if \((a,b)\in A\times B\). Moreover, let \(p\) be a point that is not in \(A\cup B\). There is a point \(a\in A\) for which \(\mathtt{dist}(a,p)=n/2-2\) and for all other points \(a^{\prime}\in A-\{a\}\), \(\mathtt{dist}(a^{\prime},p)=2\). Moreover, \(\mathtt{dist}(p,b)=4\) for \(b\in B\).

For this instance \(\mathtt{average-link}\) builds the \(2\)-clustering \(\mathcal{A}^{2}=(A\cup\{p\},B)\). We have that \(\mathtt{diam}(A\cup p)=n/2-2\) and \(\mathtt{avg}(A\cup p,B)=4\), Thus, \(\mathtt{cs-ratio_{0M}}(\mathcal{A}^{2})\) is \(\Omega(n)\). On the other hand, for the clustering \(\mathcal{A}^{\prime}=(A,B\cup p)\), \(\mathtt{cs-ratio_{0M}}(\mathcal{A}^{\prime})\) is \(O(1)\), so the approximation of \(\mathtt{average-link}\) is \(\Omega(n)\).

### Proof of Lemma 3.2

Proof.: The first inequality holds because \(\mathtt{avg}(x,X)=\frac{|X|-1}{|X|}\mathtt{avg}(x,X-x)\). Thus, we just need to prove the second one.

Let \(S_{1}\) be the first cluster merged with \(x\) by \(\mathtt{average-link}\) and let \(S_{i}\), for \(i>1\), be the cluster merged with \(S_{1}\cup\cdots\cup S_{i-1}\) by \(\mathtt{average-link}\). Define \(T_{0}:=\{x\}\) and, for \(i\geq 1\), \(T_{i}:=T_{i-1}\cup S_{i}\). Furthermore, define \(e_{i}\) and \(m_{i}\) as \(e_{i}:=\mathtt{avg}(T_{i-1},S_{i})\) and \(m_{i}:=\mathtt{avg}(x,T_{i}-x)\), respectively. Note that there is \(t\) for which \(T_{t}=X\) and, hence, \(m_{t}=\mathtt{avg}(x,X-x)\).

We have that

\[m_{i+1}=\frac{|T_{i}|-1}{|T_{i+1}|-1}\mathtt{avg}(x,T_{i}-x)+ \frac{|S_{i+1}|}{|T_{i+1}|-1}\mathtt{avg}(x,S_{i+1})\leq\] (7) \[\frac{|T_{i}|-1}{|T_{i+1}|-1}m_{i}+\frac{|S_{i+1}|}{|T_{i+1}|-1}( m_{i}+e_{i+1})=m_{i}+\frac{|S_{i+1}|}{|T_{i+1}|-1}e_{i+1},\] (8)

where the inequality follows from the triangle inequality.

Let us consider the beginning of the iteration in which \(T_{i-1}\) and \(S_{i}\) are merged. At this point we have \(\ell\geq 1\) clusters \(Y_{1},\ldots,Y_{\ell}\) such that \(Y=Y_{1}\cup\cdots\cup Y_{\ell}\) and \(\ell^{\prime}\) clusters \(Z_{1},\ldots,Z_{\ell^{\prime}}\) such that \(Z=Z_{1}\cup\cdots\cup Z_{\ell}\). Note that there exist \(i\) and \(j\) such that \(\mathtt{avg}(Y_{i},Z_{j})\leq\mathtt{avg}(Y,Z)\). Thus, we must have \(e_{i}\leq\mathtt{avg}(Y,Z)\), otherwise \(\mathtt{average-link}\) would merge \(Y_{i}\) and \(Z_{j}\) rather than \(T_{i-1}\) and \(S_{i}\).

To establish the result, we show by induction that \(m_{i}\leq\mathtt{avg}(Y,Z)\cdot H_{|T_{i}|-1},\) for \(i\geq 1\). The lemma is then established by taking \(i=t\), where \(t\) satisfies \(T_{t}=X\).

For \(i=1\), we have \(m_{1}=e_{1}\leq\mathtt{avg}(Y,Z)<\mathtt{avg}(Y,Z)\cdot H_{|T_{1}|-1}\). We assume by induction that \(m_{i-1}\leq\mathtt{avg}(Y,Z)\cdot H_{|T_{i-1}|-1}.\) By inequality ( 7)-(8),

\[m_{i}\leq m_{i-1}+e_{i}\frac{|S_{i}|}{|T_{i}|-1}\leq\mathtt{avg}(Y,Z)\left( \sum_{h=1}^{|T_{i-1}|-1}\frac{1}{h}\right)+\mathtt{avg}(Y,Z)\left(\sum_{h=|T_{i -1}|}^{|T_{i}|-1}\frac{1}{h}\right)=\mathtt{avg}(Y,Z)\cdot H_{|T_{i}|-1}\]Proof of Lemma 3.5

Proof.: First, we note that

\[|B_{i-1}|=\sum_{h=0}^{i-1}|A_{i}|=\sum_{h=0}^{i-1}(h+1)!-h!=i!,\]

for \(i\geq 1\).

Moreover, for \(i\geq 2\), we have that

\[\mathsf{avg}(A_{i},B_{i-1})=\frac{|A_{i-1}|}{|B_{i-1}|}\mathsf{ avg}(A_{i},A_{i-1})+\frac{|B_{i-2}|}{|B_{i-1}|}\mathsf{avg}(A_{i},B_{i-2})=\] (9) \[\frac{|A_{i-1}|}{|B_{i-1}|}\mathsf{avg}(A_{i},A_{i-1})+\frac{|B_{i -2}|}{|B_{i-1}|}\big{(}\mathsf{avg}(A_{i},A_{i-1})+\mathsf{avg}(A_{i-1},B_{i-2 })\big{)}=\] (10) \[\mathsf{avg}(A_{i},A_{i-1})+\frac{|B_{i-2}|}{|B_{i-1}|}\mathsf{ avg}(A_{i-1},B_{i-2})=\] (11) \[\left(1+\frac{|B_{i-2}|}{|B_{i-1}|}\right)\mathsf{avg}(A_{i-1},B_ {i-2}),\] (12)

where the last identity follows because \(\mathsf{avg}(A_{i},A_{i-1})=p_{i}-p_{i-1}=\mathsf{avg}(A_{i-1},B_{i-2})\).

By applying the above equation successively, we conclude that

\[\mathsf{avg}(A_{i},B_{i-1})=(i+1)\cdot\mathsf{avg}(A_{1},B_{0})=(i+1)\]

and, hence,

\[p_{i}=1+\sum_{h=1}^{i-1}(h+1)=\frac{i(i+1)}{2}.\]

Thus,

\[\mathtt{diam}(B_{i-1})=p_{i-1}-p_{0}=p_{i-1}=\frac{i(i-1)}{2}\]

Now we show that at the beginning of the step \((n-t)+i\) average-link keeps a clustering that contains the cluster \(B_{i-1}\) and the clusters \(A_{j}\), for \(i\leq j\leq t-1\). First, we observe that after \(n-t\) steps average-link produces a \(t\)-clustering \((A_{0},\ldots,A_{t-1})\) since points in the same group \(A_{i}\) are located at the same position. We analyze what happens in the remaining \(t-1\) steps.

For \(i=1\) the result holds because \(B_{0}=A_{0}\). We assume as an induction hypothesis that at beginning of the step \((n-t)+i\), we have the clusters \(B_{i-1}\) and \(A_{j}\), for \(j\geq i\). By construction, for \(i\leq r<s\),

\[\mathsf{avg}(A_{s},A_{r})=p_{s}-p_{r}>p_{i+1}-p_{i}=\mathsf{avg}(A_{i},B_{i-1 }),\]

Moreover,

\[i-1=\mathsf{avg}(A_{i},B_{i-1})<\mathsf{avg}(A_{j},B_{i-1}),\]

for \(j>i\). Thus, \(\mathsf{average-link}\) prefers merging \(A_{i}\) and \(B_{i-1}\) rather than any other pair of clusters, which completes the inductive step. 

## Appendix D Proofs from section 4

### Proof of Proposition 4.1

Proof.: Let \(\mathcal{C}^{*}=(C_{1}^{*},\ldots,C_{k}^{*})\) be a \(k\)-clustering that maximizes \(\mathsf{sep}_{\mathsf{av}}\). Let \(\mathcal{Q}\) be the family of sets of points \(Q\) such that \(|Q|=k\) and \(Q\) intersects all clusters \(C_{1}^{*},\ldots,C_{k}^{*}\). Let \(P=\{p_{1},\ldots,p_{k}\}\) be a set in \(\mathcal{Q}\) that satisfies \(\mathsf{avg}(P)\geq\mathsf{avg}(Q),\) for every \(Q\in\mathcal{Q}\). Moreover, let \(U=\{u_{1},\ldots,u_{k}\}\) be a set of \(k\) points where \(u_{i}\) is randomly selected from \(C^{*}_{i}\). It follows from the choice of \(P\) that

\[\frac{k(k-1)}{2}\mathsf{avg}(P)\geq\frac{k(k-1)}{2}E[\mathsf{avg}(U)]=\] \[E\left[\sum_{i=1}^{k-1}\sum_{j=i+1}^{k}\mathsf{dist}(u_{i},u_{j}) \right]=\sum_{i=1}^{k-1}\sum_{j=i+1}^{k}E\left[\mathsf{dist}(u_{i},u_{j}) \right]=\sum_{i=1}^{k-1}\sum_{j=i+1}^{k}\mathsf{avg}(C^{*}_{i},C^{*}_{j})\geq\] \[\frac{k(k-1)}{2}\mathsf{sep}_{\mathsf{av}}(C^{*})\]

### Proof of Theorem 4.2

Proof.: Let \(P=\{p_{i}|1\leq i\leq k\}\) be the \(k\) points given by Proposition 4.1 and let \(h\) be a function that maps each point \(p\in P\) into its cluster in \(\mathcal{A}^{k}\). Moreover, let \(Y\) and \(Z\) be clusters in \(\mathcal{A}^{k}\) that satisfy \(\mathsf{avg}(Y,Z)=\mathsf{sep}_{\mathsf{min}}(\mathcal{A}^{k})\).

Let \(p\) and \(p^{\prime}\) be distinct points in \(P\). We consider two cases:

Case 1) \(p\) and \(p^{\prime}\) belong to the same cluster \(A\) in \(\mathcal{A}^{k}\). From Theorem 3.3 we have that

\[\mathsf{dist}(p,p^{\prime})\leq\mathsf{diam}(A)\leq 2H_{|A|}\mathsf{avg}(Y,Z) =2H_{|A|}\mathsf{sep}_{\mathsf{min}}(\mathcal{A}^{k})\]

Thus,

\[\sum_{p,p^{\prime}\in P\cap A}\mathsf{dist}(p,p^{\prime})\leq\sum_{p,p^{\prime }\in P\cap A}2H_{|A|}\mathsf{sep}_{\mathsf{min}}(\mathcal{A}^{k}).\] (13)

By considering all clusters \(A\in\mathcal{A}^{k}\) we get

\[\sum_{\begin{subarray}{c}p,p^{\prime}\in P\\ h(p)=h(p^{\prime})\end{subarray}}\mathsf{dist}(p,p^{\prime})\leq\sum_{ \begin{subarray}{c}p,p^{\prime}\in P\\ h(p^{\prime})\equiv h(p^{\prime})\end{subarray}}2H_{n}\mathsf{sep}_{\mathsf{ min}}(\mathcal{A}^{k})\] (14)

Case 2) \(p\) and \(p^{\prime}\) belong, respectively, to different clusters \(A\) and \(A^{\prime}\) in \(\mathcal{A}^{k}\). We consider two subcases: _subcase 2.1)_\(k=2\). In this case, from the triangle inequality, we have that \(\mathsf{dist}(p,p^{\prime})=\mathsf{avg}(p,p^{\prime})\leq\mathsf{avg}(p,A)+ \mathsf{avg}(A,A^{\prime})+\mathsf{avg}(A^{\prime},p^{\prime})\). By using Lemma 3.2, we have that \(\mathsf{avg}(p,A)\leq H_{n-1}\mathsf{avg}(A,A^{\prime})=H_{n-1}\mathsf{sep}_{ \mathsf{min}}(\mathcal{A}^{k})\) and \(\mathsf{avg}(p^{\prime},A^{\prime})\leq H_{n-1}\mathsf{avg}(A,A^{\prime})=H_{ n-1}\mathsf{sep}_{\mathsf{min}}(\mathcal{A}^{k})\). Thus,

\[\sum_{\begin{subarray}{c}p,p^{\prime}\in P\\ h(p)\neq h(p^{\prime})\end{subarray}}\mathsf{dist}(p,p^{\prime})=\mathsf{dist}(p,p ^{\prime})\leq 2H_{n-1}\mathsf{sep}_{\mathsf{min}}(\mathcal{A}^{k})+\mathsf{ avg}(A,A^{\prime}),\] (15)

where the first identity holds because \(P=\{p,p^{\prime}\}\).

_subcase 2.2)_\(k>2\). Let \(S\) be a cluster in \(\mathcal{A}^{k}-\{A,A^{\prime}\}\). From the triangle inequality, we have that

\[\mathsf{dist}(p,p^{\prime})=\mathsf{avg}(p,p^{\prime})\leq\mathsf{avg}(p,A)+ \mathsf{avg}(A,S)+\mathsf{avg}(S,A^{\prime})+\mathsf{avg}(A^{\prime},p^{ \prime})\]

If \(|A|=1\), \(\mathsf{avg}(p,A)=0\leq H_{|A|}\cdot\mathsf{sep}_{\mathsf{min}}(\mathcal{A}^{k})\). Moreover, if \(|A|\geq 2\), it follows from Lemma 3.2 that \(\mathsf{avg}(p,A)\leq H_{|A|}\cdot\mathsf{avg}(Y,Z)=H_{|A|}\mathsf{sep}_{ \mathsf{min}}(\mathcal{A}^{k})\). Analogously, we have \(\mathsf{avg}(p^{\prime},A^{\prime})\leq H_{|A^{\prime}|}\mathsf{sep}_{\mathsf{ min}}(\mathcal{A}^{k})\). Thus,

\[\mathsf{dist}(p,p^{\prime})\leq H_{|A|}\mathsf{sep}_{\mathsf{min}}(\mathcal{A}^ {k})+\mathsf{avg}(A,S)+\mathsf{avg}(S,A^{\prime})+H_{|A^{\prime}|}\mathsf{sep} _{\mathsf{min}}(\mathcal{A}^{k}).\]

By averaging over all possible \(S\in\mathcal{A}^{k}-\{A,A^{\prime}\}\) we get that

\[\mathsf{dist}(p,p^{\prime})\leq\cdot 2H_{n}\mathsf{sep}_{\mathsf{min}}(\mathcal{A}^ {k})+\frac{1}{k-2}\sum_{S\notin\{A,A^{\prime}\}}(\mathsf{avg}(A,S)+\mathsf{ avg}(S,A^{\prime}))\]

By adding over all points \(p\in P\cap A\) and \(p^{\prime}\in P\cap A^{\prime}\) we get that \[\sum_{p\in P\cap A}\sum_{p^{\prime}\in P\cap A^{\prime}}2H_{n}\texttt{sep}_{ \texttt{min}}(\mathcal{A}^{k})+\frac{|P\cap A|\cdot|P\cap A^{\prime}|}{k-2} \sum_{S\notin\{A,A^{\prime}\}}(\texttt{avg}(A,S)+\texttt{avg}(S,A^{\prime}))\]

By adding the above inequalities for \(p,p^{\prime}\in P\), with \(h(p)\neq h(p^{\prime})\), we get that

\[\sum_{\begin{subarray}{c}p,p^{\prime}\in P\\ h(p)\neq h(p^{\prime})\end{subarray}}\texttt{dist}(p,p^{\prime})\leq\] (16) \[\sum_{\begin{subarray}{c}p,p^{\prime}\in P\\ h(p)\neq h(p^{\prime})\end{subarray}}2H_{n}\cdot\texttt{sep}_{\texttt{min}}( \mathcal{A}^{k})+\frac{1}{k-2}\sum_{\begin{subarray}{c}A,A^{\prime}\in A^{k}\\ A\neq A^{\prime}\end{subarray}}|P\cap A|\cdot|P\cap A^{\prime}|\sum_{S\notin\{A,A^{\prime}\}}(\texttt{avg}(A,S)+\texttt{avg}(S,A^{\prime})=\] (17) \[\sum_{\begin{subarray}{c}p,p^{\prime}\in P\\ h(p)\neq h(p^{\prime})\end{subarray}}2H_{n}\cdot\texttt{sep}_{\texttt{min}}( \mathcal{A}^{k})+\frac{1}{k-2}\sum_{\begin{subarray}{c}A,A^{\prime}\in A^{k}\\ A\neq A^{\prime}\end{subarray}}(|P\cap(A\cup A^{\prime})|)\cdot(k-|P\cap(A\cup A ^{\prime})|)\cdot\texttt{avg}(A,A^{\prime})\leq\] (18) \[\sum_{\begin{subarray}{c}p,p^{\prime}\in P\\ h(p)\neq h(p^{\prime})\end{subarray}}2H_{n}\cdot\texttt{sep}_{\texttt{min}}( \mathcal{A}^{k})+k\sum_{\begin{subarray}{c}A,A^{\prime}\in A^{k}\\ A\neq A^{\prime}\end{subarray}}\texttt{avg}(A,A^{\prime}),\] (19)

where the last inequality holds because \((|P\cap(A\cup A^{\prime})|)\cdot(k-|P\cap(A\cup A^{\prime})|)\leq k^{2}/4\).

If we compare inequalities (16)-(19) with inequality (15), we conclude that (16)-(19) also hold for the subcase \(k=2\).

Then, by adding inequality (14) with the inequalities (16)-(19) and also using the fact \(\texttt{sep}_{\texttt{min}}(\mathcal{A}^{k})\leq\texttt{sep}_{\texttt{av}}( \mathcal{A}^{k})\), we get that

\[\sum_{\begin{subarray}{c}p,p^{\prime}\in P\\ p\neq p^{\prime}\end{subarray}}\texttt{dist}(p,p^{\prime})\leq 2H_{n}\frac{k(k-1)}{2} \texttt{sep}_{\texttt{min}}(\mathcal{A}^{k})+k\sum_{\begin{subarray}{c}A,A^{ \prime}\in A^{k}\\ A\neq A^{\prime}\end{subarray}}\texttt{avg}(A,A^{\prime})\leq(2H_{n}+k)\frac{k(k -1)\texttt{sep}_{\texttt{av}}(\mathcal{A}^{k})}{2}\]

Proposition 4.1 ensures that

\[\frac{k(k-1)}{2}\texttt{OPT}_{\texttt{SEP}}(k)\leq\frac{k(k-1)}{2}\texttt{ avg}(P)=\sum_{p,p^{\prime}\in P}\texttt{dist}(p,p^{\prime})\]

Thus, from the two previous inequalities, we conclude that

\[\texttt{sep}_{\texttt{av}}(\mathcal{A}^{k})\geq\frac{\texttt{OPT}_{\texttt{ SEP}}(k)}{2H_{n}+k}.\]

### The \(\texttt{sep}_{\texttt{av}}\) criterion for other linkage methods

The following instances show that the separability of both single-linkage and complete-linkage can be much lower than \(\frac{\texttt{OPT}_{\texttt{SEP}}(k)}{\log n}\).

For single-linkage, consider the instance \(\mathcal{X}=A\cup B\cup\{p\}\), where \(A\) contains \(n-1-\sqrt{n}\) points and \(B\) contains \(\sqrt{n}\) points \(b_{1},\ldots,b_{\sqrt{n}}\). Moreover, we have \(\texttt{dist}(x,y)=\epsilon\), for \(x,y\in A\), \(\texttt{dist}(b_{i},x)=i\) for every point \(x\in A\) and \(\texttt{dist}(b_{i},b_{j})=|i-j|\). Moreover, \(\texttt{dist}(p,x)=1+\epsilon\), for every point \(x\in A\). and \(\texttt{dist}(p,b_{i})=1+\epsilon+i\) In this case, single-linkage builds the clustering \((A\cup B,\{p\})\). We have that \(\texttt{sep}_{\texttt{av}}(A\cup B,p)\leq 2\), while \(\texttt{sep}_{\texttt{av}}(A\cup p,B)\) is \(\Omega(\sqrt{n})\).

Regarding complete-linkage, we consider the instance presented at Section B.2, but without the set \(E\), that is, the set of points is \(\mathcal{X}=A\cup B\cup C\cup D\). When \(k=2\), complete-linkage builds the clustering \((A\cup C,B\cup D)\) that has \(\texttt{sep}_{\texttt{av}}\ O(1)\) while the clustering \((A,C\cup D\cup B)\) satisfies

\[\texttt{sep}_{\texttt{av}}(A,C\cup D\cup B)\geq\frac{\frac{t^{2}}{2}(2t^{2}+t )}{t(2t^{2}+t))}=\frac{t}{2}.\]

Since \(t=\Theta(\sqrt{n})\), we conclude that the separability of complete-linkage for this instance is \(O(\frac{\texttt{OPT}_{\texttt{sep}}(k)}{\sqrt{n}})\).

### Separability and cohesion can be conflicting

Recall that for instance \(\mathcal{I}_{k}^{sep}\) average-link builds the \(k\)-clustering \(\mathcal{A}^{k}=(S_{1},S_{2},s_{1},s_{2},\ldots,s_{k-2})\). Note that \(\texttt{max-diam}(\mathcal{A}^{k})=\texttt{max-avg}(\mathcal{A}^{k})=\epsilon\). Let \(\mathcal{A}^{\prime}\) be a \(k\)-clustering different from \(\mathcal{A}^{k}\). We argue that \(\texttt{max-diam}(\mathcal{A}^{\prime})\geq 1\) and \(\texttt{max-avg}(\mathcal{A}^{\prime})\) is \(\Omega(1/k^{2})\). In fact, if \(\mathcal{A}^{\prime}\) has a cluster \(A\) that satisfies \(|A|\geq 2\) and \(|A\cap S_{3}|\geq 1\), then \(\texttt{max-diam}(\mathcal{A}^{\prime})\geq 1\) and \(\texttt{max-avg}(\mathcal{A}^{\prime})\) is \(\Omega(1/k^{2})\). Otherwise, if \(\mathcal{A}^{\prime}\) does not have such a cluster, then all points in \(S_{3}\) must be singletons in \(\mathcal{A}^{\prime}\). Since \(\mathcal{A}^{\prime}\neq\mathcal{A}^{k}\), there is a cluster in \(\mathcal{A}^{\prime}\) that contains both a point in \(S_{1}\) and a point in \(S_{2}\). Thus, \(\texttt{max-diam}(\mathcal{A}^{\prime})=D\) and \(\texttt{max-avg}(\mathcal{A}^{\prime})\) is \(\Omega(D/k^{2})\).

Let \(\mathcal{M}\) be the class of methods with bounded approximation regarding \(\texttt{max-diam}\) or to \(\texttt{max-avg}\). Then any method \(M\in\mathcal{M}\) builds the clustering \(\mathcal{A}^{k}\). Since \(\texttt{sep}_{\texttt{av}}(\mathcal{A}^{k})\) is \(O(D/k)\) and there is a \(k\)-clustering for \(\mathcal{I}_{k}^{sep}\) whose \(\texttt{sep}_{\texttt{av}}\) is \(\Omega(D)\), we conclude that the approximation factor of any method \(M\in\mathcal{M}\) regarding \(\texttt{sep}_{\texttt{av}}\) is \(O(1/k)\).

Now, we consider \(\texttt{sep}_{\texttt{min}}\). We have that \(\texttt{sep}_{\texttt{min}}(\mathcal{A}^{k})=1\). Let \(\mathcal{B}=(B_{1},\ldots,B_{k})\) be a \(k\)-clustering with the following properties: (i) \(|B_{i}\cap S_{3}|\geq 1\) for each \(i\leq k-2\); (ii) each \(B_{i}\), with \(i\geq 2\), has exactly one point in \(S_{1}\cup S_{2}\) (iii) \(B_{k-1}\) has a point in \(S_{1}\) and \(B_{k}\) has a point in \(S_{2}\). We have that \(\texttt{sep}_{\texttt{min}}(\mathcal{B})\) is \(\Omega(D)\). Thus, any method \(M\in\mathcal{M}\) has approximation \(O(1/D)\) to \(\texttt{sep}_{\texttt{min}}\), that is, the approximation is unbounded in terms of \(n\).

## Appendix E Proof of Theorem 5.4

Proof.: Let \(\mathcal{I}\) be the instance obtained by augmenting the instance \(\mathcal{I}^{CS}\), presented right after Theorem 3.4, with the points \(x_{0},\ldots,x_{t-1}\), where \(\texttt{dist}(x_{i},A_{i})=t+1+\epsilon\) and for \(i\neq j\), \(\texttt{dist}(x_{i},x_{j})=|p_{j}-p_{i}|+2(t+1+\epsilon)\) and \(\texttt{dist}(x_{i},A_{j})=|p_{j}-p_{i}|+t+1+\epsilon\).

Consider \(t=k\). We argue that the \((k+1)\)-clustering obtained by average-link for \(\mathcal{I}\) consists of the clusters \((B_{k-1},\{x_{0}\},\ldots,\{x_{k-1}\})\). In fact, in its first steps average-link obtains the \(2k\)-clustering \((A_{0},\ldots,A_{k-1},x_{0},\ldots,x_{k-1})\) since the distance between points in \(A_{i}\) is 0. In the next \(k-1\) steps, average-link does not make a merge involving a point \(x_{i}\) because the average distance of \(x_{i}\) to any other cluster is larger \(k+1\) and, by Lemma 3.5, the average distance between \(B_{i-2}\) and \(A_{i}\) is \(i+1\leq k+1\). Thus, the execution of average-link for \(\mathcal{I}\) merges the same clusters that are merged in the instance \(\mathcal{I}^{CS}\) and, then, ends up with the \((k+1)\)-clustering \((B_{k-1},\{x_{0}\},\ldots,\{x_{k-1}\})\).

Thus, for instance \(\mathcal{I}\), the maximum diameter of a cluster in \(\mathcal{A}^{k}\) is at least \(\texttt{diam}(B_{k-1})\), which is \(\Omega(k^{2})\), while the \(k\)-clustering \((x_{0}\cup A_{0},\ldots,x_{k-1}\cup A_{k-1})\) has diameter \(k+\epsilon\). 

## Appendix F Experiments: extra details

Table 2 presents our datasets with their main characteristics.

Figures (1)-(6) show the results obtained by single-linkage, complete-linkage and average-link, for all datasets and the different criteria considered in the paper. For a given dataset \(D\), method \(M\) and criterion \(\alpha\), the height of the bar is given by the average of \(m_{k}\) for every \(k\) considered in our experiments, where \(m_{k}\) is the ratio between the value of criterion \(\alpha\) achieved by method \(M\) on dataset \(D\) divided by the best value for criterion \(\alpha\), among those achieved by single-linkage, average-link and complete-linkage

Regarding the cohesion criteria complete-linkage presents the best results for max-diam, followed by average-link. For max-avg, again complete-linkage and average-link are the best, with the latter having a slight advantage.

In terms of the separability criteria, average-linkis much better than the other methods for sepmin, while for sepav there is a balance between average-link and single-linkage.

For the criteria that combine cohesion and separability, average-linkis superior for cs-ratioAV, while there is a balance between average-link and complete-linkage for cs-ratioDM.

Table 3 and 4 show the results for the experiment described in Section 6, when the Euclidean distance is replaced with the \(\ell_{1}\) and \(\ell_{\infty}\) norm, respectively. The observations made in Section 6 also hold when these metrics are used.

Finally, we note that the variance of the results for average-link is small. Indeed, an entry (average) close to 1 (e.g. 0.96) cannot have an underlying large variance because 1 is the maximum possible value for an entry. Since most entries for average-link are close to 1, one can conclude that the variance of its results is usually small. In the supplemental material, we have.csv files with our full results.

\begin{table}
\begin{tabular}{c|c|c|c} Dataset & \(n\) & \(d\) & Source \\ \hline Airfoil & 1501 & 5 & Brooks and Marcolini (2014) \\ Banknote & 1371 & 5 & Lohweg (2013) \\ Collins & 1000 & 19 & OpenML \\ Concrete & 1028 & 8 & Yeh (2007) \\ Digits & 1797 & 64 & Alpaydin (1998) \\ Geographical Music & 1057 & 116 & Zhou (2014) \\ Mice & 552 & 77 & Higuera and Cios (2015) \\ Qsarfish & 906 & 10 & Ballabio and Todeschini (2019) \\ Tripdvisor & 979 & 10 & Renjith (2018) \\ Vowel & 990 & 10 & UCI \\ \end{tabular}
\end{table}
Table 2: Datasets

Figure 1: Results for the max-diam for the different datasets. For interpreting the bars, the lower the better

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Guidelines:

Figure 3: Results for the \(\texttt{sep}_{\texttt{min}}\) for the different datasets. For interpreting the bars, the higher the better

Figure 2: Results for the \(\texttt{max-avg}\) for the different datasets. For interpreting the bars, the lower the better* The answer NA means that the abstract and introduction do not include the claims made in the paper.
* The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
* The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.

Figure 4: Results for the \(\mathtt{sep_{av}}\) for the different datasets. For interpreting the bars, the higher the better

Figure 5: Results for the \(\mathtt{cs\_ratio}_{AV}\) for the different datasets and methods. For interpreting the bars, the lower the better

* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We included a section at the end of the paper.

\begin{table}
\begin{tabular}{l c c c||c c c c||c c}  & \multicolumn{3}{c}{Smal} & \multicolumn{3}{c}{Medium} & \multicolumn{3}{c}{Large} \\ \hline  & A & C & S & A & C & S & A & C & S \\ \hline sep\({}_{\texttt{min}}\) & **0,99** & 0,82 & 0,77 & **0,98** & 0,91 & 0,7 & **0,99** & 0,94 & 0,75 \\ sep\({}_{\texttt{av}}\) & **0,97** & 0,82 & 0,95 & **0,97** & 0,92 & **1** & 0,98 & 0,96 & **1** \\ max-diam & 0,94 & **1** & 0,9 & 0,87 & **1** & 0,7 & 0,85 & **1** & 0,56 \\ max-avg & 0,94 & **0,96** & 0,91 & **0,94** & 0,88 & 0,79 & **0,95** & 0,85 & 0,81 \\ cs-ratio\({}_{\texttt{DM}}\) & **0,97** & 0,86 & 0,74 & 0,91 & **0,98** & 0,52 & 0,89 & **0,99** & 0,45 \\ cs-ratio\({}_{\texttt{AV}}\) & **0,96** & 0,82 & 0,74 & **0,96** & 0,85 & 0,59 & **0,97** & 0,82 & 0,65 \\ \end{tabular}
\end{table}
Table 4: Average ratio between the result of a method and the best one for each criterion and each group of \(k\). The best results are bold-faced. Distances are computed using \(\ell_{\infty}\) norm

Figure 6: Results for the cs-ratio\({}_{\texttt{DM}}\) for the different datasets and methods. For interpreting the bars, the lower the better

\begin{table}
\begin{tabular}{l c c c||c c c||c c c}  & \multicolumn{3}{c}{Smal} & \multicolumn{3}{c}{Medium} & \multicolumn{3}{c}{Large} \\ \hline  & A & C & S & A & C & S & A & C & S \\ \hline sep\({}_{\texttt{min}}\) & **0,99** & 0,81 & 0,75 & **0,99** & 0,86 & 0,66 & **0,99** & 0,9 & 0,71 \\ sep\({}_{\texttt{av}}\) & **0,98** & 0,83 & 0,93 & **0,96** & 0,89 & **1** & 0,97 & 0,95 & **0,99** \\ max-diam & 0,86 & **0,99** & 0,72 & 0,85 & **1** & 0,5 & 0,81 & **1** & 0,41 \\ max-avg & **0,94** & **0,94** & 0,88 & **0,99** & 0,9 & 0,73 & **0,99** & 0,83 & 0,7 \\ cs-ratio\({}_{\texttt{DM}}\) & **0,96** & 0,91 & 0,62 & 0,96 & **0,98** & 0,38 & 0,88 & **0,99** & 0,32 \\ cs-ratio\({}_{\texttt{AV}}\) & **0,98** & 0,8 & 0,71 & **1** & 0,79 & 0,51 & **1** & 0,76 & 0,51 \\ \end{tabular}
\end{table}
Table 3: Average ratio between the result of a method and the best one for each criterion and each group of \(k\). The best results are bold-faced. Distances are computed using \(\ell_{1}\) norm Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ** Providing as much information as possible in the supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The details are in the paper and also in the supplemental material. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We have not included error bars because they do not help much in our case. However, from the tables and our analyses, the reader should have a clear idea of the variability of our results (see last paragraph of Section F). Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: This information is irrelevant to reproducing our experiments or reaching our conclusions. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our paper is mostly about theoretical results. We provide several new analyses for algorithms that are widely known. We do not see a clear societal impact that deserves to be mentioned. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the datasets we use in Appendix F Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Our supplementary material contains our codes. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects**Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.